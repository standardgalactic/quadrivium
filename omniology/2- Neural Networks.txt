
Neural Networks and
Learning Machines
Third Edition
Simon Haykin
McMaster University
Hamilton, Ontario, Canada
New York Boston San Francisco
London Toronto Sydney Tokyo Singapore Madrid
Mexico City Munich Paris Cape Town Hong Kong Montreal

Library of Congress Cataloging-in-Publication Data
Haykin, Simon 
Neural networks and learning machines / Simon Haykin.—3rd ed.
p. cm.
Rev. ed of: Neural networks. 2nd ed., 1999.
Includes bibliographical references and index.
ISBN-13: 978-0-13-147139-9
ISBN-10: 0-13-147139-2
1. Neural networks (Computer science)
2. Adaptive filters.
I. Haykin, Simon
Neural networks.
II.Title.
QA76.87.H39 2008
006.3--dc22
2008034079
Vice President and Editorial Director, ECS: Marcia J. Horton
Associate Editor: Alice Dworkin
Supervisor/Editorial Assistant: Dolores Mars
Editorial Assistant: William Opaluch
Director of Team-Based Project Management: Vince O’Brien
Senior Managing Editor: Scott Disanno
A/V Production Editor: Greg Dulles
Art Director: Jayne Conte
Cover Designer: Bruce Kenselaar
Manufacturing Manager: Alan Fischer
Manufacturing Buyer: Lisa McDowell
Marketing Manager: Tim Galligan
Copyright © 2009 by Pearson Education, Inc., Upper Saddle River, New Jersey 07458.
Pearson Prentice Hall. All rights reserved. Printed in the United States of America. This publication is
protected by Copyright and permission should be obtained from the publisher prior to any prohibited
reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic,
mechanical, photocopying, recording, or likewise. For information regarding permission(s), write to: Rights
and Permissions Department.
Pearson® is a registered trademark of Pearson plc
Pearson Education Ltd.
Pearson Education Australia Pty. Limited
Pearson Education Singapore Pte. Ltd.
Pearson Education North Asia Ltd.
Pearson Education Canada, Ltd.
Pearson Educación de Mexico, S.A. de C.V.
Pearson Education–Japan
Pearson Education Malaysia Pte. Ltd.
10 9 8 7 6 5 4 3 2 1
ISBN-13:
978-0-13-147139-9
ISBN-10:
0-13-147139-2

To my wife, Nancy, for her patience and tolerance,
and
to the countless researchers in neural networks for their original contributions,
the many reviewers for their critical inputs, and many of my graduate students for
their keen interest.

This page intentionally left blank 

v
Preface
x
Introduction
1
1.
What is a Neural Network?
1
2.
The Human Brain
6
3.
Models of a Neuron
10
4.
Neural Networks Viewed As Directed Graphs
15
5.
Feedback
18
6.
Network Architectures
21
7.
Knowledge Representation
24
8.
Learning Processes
34
9.
Learning Tasks
38
10.
Concluding Remarks
45
Notes and References
46
Chapter 1
Rosenblatt’s Perceptron
47
1.1
Introduction
47
1.2.
Perceptron
48
1.3.
The Perceptron Convergence Theorem
50
1.4.
Relation Between the Perceptron and Bayes Classifier for a Gaussian Environment
55
1.5.
Computer Experiment: Pattern Classification
60
1.6.
The Batch Perceptron Algorithm
62
1.7.
Summary and Discussion
65
Notes and References
66
Problems
66
Chapter 2
Model Building through Regression
68
2.1
Introduction
68
2.2
Linear Regression Model: Preliminary Considerations
69
2.3
Maximum a Posteriori Estimation of the Parameter Vector
71
2.4
Relationship Between Regularized Least-Squares Estimation 
and MAP Estimation
76
2.5
Computer Experiment: Pattern Classification
77
2.6
The Minimum-Description-Length Principle
79
2.7
Finite Sample-Size Considerations
82
2.8
The Instrumental-Variables Method
86
2.9
Summary and Discussion
88
Notes and References
89
Problems
89
Contents
MATLAB codes + solutions to Computer Experiments

vi
Contents
Chapter 3
The Least-Mean-Square Algorithm
91
3.1
Introduction
91
3.2
Filtering Structure of the LMS Algorithm
92
3.3
Unconstrained Optimization: a Review
94
3.4
The Wiener Filter
100
3.5
The Least-Mean-Square Algorithm
102
3.6
Markov Model Portraying the Deviation of the LMS Algorithm 
from the Wiener Filter
104
3.7
The Langevin Equation: Characterization of Brownian Motion
106
3.8
Kushner’s Direct-Averaging Method
107
3.9
Statistical LMS Learning Theory for Small Learning-Rate Parameter
108
3.10
Computer Experiment I: Linear Prediction
110
3.11
Computer Experiment II: Pattern Classification
112
3.12
Virtues and Limitations of the LMS Algorithm
113
3.13
Learning-Rate Annealing Schedules
115
3.14
Summary and Discussion
117
Notes and References
118
Problems
119
Chapter 4
Multilayer Perceptrons
122
4.1
Introduction
123
4.2
Some Preliminaries
124
4.3
Batch Learning and On-Line Learning
126
4.4
The Back-Propagation Algorithm
129
4.5
XOR Problem
141
4.6
Heuristics for Making the Back-Propagation Algorithm Perform Better
144
4.7
Computer Experiment: Pattern Classification
150
4.8
Back Propagation and Differentiation
153
4.9
The Hessian and Its Role in On-Line Learning
155
4.10
Optimal Annealing and Adaptive Control of the Learning Rate
157
4.11
Generalization
164
4.12
Approximations of Functions
166
4.13
Cross-Validation
171
4.14
Complexity Regularization and Network Pruning
175
4.15
Virtues and Limitations of Back-Propagation Learning
180
4.16
Supervised Learning Viewed as an Optimization Problem
186
4.17
Convolutional Networks
201
4.18
Nonlinear Filtering
203
4.19
Small-Scale Versus Large-Scale Learning Problems
209
4.20
Summary and Discussion
217
Notes and References
219
Problems
221
Chapter 5
Kernel Methods and Radial-Basis Function Networks
230
5.1
Introduction
230
5.2
Cover’s Theorem on the Separability of Patterns
231
5.3
The Interpolation Problem
236
5.4
Radial-Basis-Function Networks
239
5.5
K-Means Clustering
242
5.6
Recursive Least-Squares Estimation of the Weight Vector
245
5.7
Hybrid Learning Procedure for RBF Networks
249
5.8
Computer Experiment: Pattern Classification
250
5.9
Interpretations of the Gaussian Hidden Units
252

5.10
Kernel Regression and Its Relation to RBF Networks
255
5.11
Summary and Discussion
259
Notes and References
261
Problems
263
Chapter 6
Support Vector Machines
268
6.1
Introduction
268
6.2
Optimal Hyperplane for Linearly Separable Patterns
269
6.3
Optimal Hyperplane for Nonseparable Patterns
276
6.4
The Support Vector Machine Viewed as a Kernel Machine
281
6.5
Design of Support Vector Machines
284
6.6
XOR Problem
286
6.7
Computer Experiment: Pattern Classification
289
6.8
Regression: Robustness Considerations
289
6.9
Optimal Solution of the Linear Regression Problem
293
6.10
The Representer Theorem and Related Issues
296
6.11
Summary and Discussion
302
Notes and References
304
Problems
307
Chapter 7
Regularization Theory
313
7.1
Introduction
313
7.2
Hadamard’s Conditions for Well-Posedness
314
7.3
Tikhonov’s Regularization Theory
315
7.4
Regularization Networks
326
7.5
Generalized Radial-Basis-Function Networks
327
7.6
The Regularized Least-Squares Estimator: Revisited
331
7.7
Additional Notes of Interest on Regularization
335
7.8
Estimation of the Regularization Parameter
336
7.9
Semisupervised Learning
342
7.10
Manifold Regularization: Preliminary Considerations
343
7.11
Differentiable Manifolds
345
7.12
Generalized Regularization Theory
348
7.13
Spectral Graph Theory
350
7.14
Generalized Representer Theorem
352
7.15
Laplacian Regularized Least-Squares Algorithm
354
7.16
Experiments on Pattern Classification Using Semisupervised Learning
356
7.17
Summary and Discussion
359
Notes and References
361
Problems
363
Chapter 8
Principal-Components Analysis
367
8.1
Introduction
367
8.2
Principles of Self-Organization
368
8.3
Self-Organized Feature Analysis
372
8.4
Principal-Components Analysis: Perturbation Theory
373
8.5
Hebbian-Based Maximum Eigenfilter
383
8.6
Hebbian-Based Principal-Components Analysis
392
8.7
Case Study: Image Coding
398
8.8
Kernel Principal-Components Analysis
401
8.9
Basic Issues Involved in the Coding of Natural Images
406
8.10
Kernel Hebbian Algorithm
407
8.11
Summary and Discussion
412
Notes and References
415
Problems
418
Contents
vii

viii
Contents
Chapter 9
Self-Organizing Maps
425
9.1
Introduction
425
9.2
Two Basic Feature-Mapping Models
426
9.3
Self-Organizing Map
428
9.4
Properties of the Feature Map
437
9.5
Computer Experiments I: Disentangling Lattice Dynamics Using SOM
445
9.6
Contextual Maps
447
9.7
Hierarchical Vector Quantization
450
9.8
Kernel Self-Organizing Map
454
9.9
Computer Experiment II: Disentangling Lattice Dynamics Using 
Kernel SOM
462
9.10
Relationship Between Kernel SOM and Kullback–Leibler Divergence
464
9.11
Summary and Discussion
466
Notes and References
468
Problems
470
Chapter 10
Information-Theoretic Learning Models
475
10.1
Introduction
476
10.2
Entropy
477
10.3
Maximum-Entropy Principle
481
10.4
Mutual Information
484
10.5
Kullback–Leibler Divergence
486
10.6
Copulas
489
10.7
Mutual Information as an Objective Function to be Optimized
493
10.8
Maximum Mutual Information Principle
494
10.9
Infomax and Redundancy Reduction
499
10.10 Spatially Coherent Features
501
10.11 Spatially Incoherent Features
504
10.12 Independent-Components Analysis
508
10.13 Sparse Coding of Natural Images and Comparison with ICA Coding
514
10.14 Natural-Gradient Learning for Independent-Components Analysis
516
10.15 Maximum-Likelihood Estimation for Independent-Components Analysis
526
10.16 Maximum-Entropy Learning for Blind Source Separation
529
10.17 Maximization of Negentropy for Independent-Components Analysis
534
10.18 Coherent Independent-Components Analysis
541
10.19 Rate Distortion Theory and Information Bottleneck
549
10.20 Optimal Manifold Representation of Data
553
10.21 Computer Experiment: Pattern Classification
560
10.22 Summary and Discussion
561
Notes and References
564
Problems
572
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
579
11.1
Introduction
580
11.2
Statistical Mechanics
580
11.3
Markov Chains
582
11.4
Metropolis Algorithm
591
11.5
Simulated Annealing
594
11.6
Gibbs Sampling
596
11.7
Boltzmann Machine
598
11.8
Logistic Belief Nets
604
11.9
Deep Belief Nets
606
11.10 Deterministic Annealing
610

11.11 Analogy of Deterministic Annealing with Expectation-Maximization 
Algorithm
616
11.12 Summary and Discussion
617
Notes and References
619
Problems
621
Chapter 12
Dynamic Programming
627
12.1
Introduction
627
12.2
Markov Decision Process
629
12.3
Bellman’s Optimality Criterion
631
12.4
Policy Iteration
635
12.5
Value Iteration
637
12.6
Approximate Dynamic Programming: Direct Methods
642
12.7
Temporal-Difference Learning
643
12.8
Q-Learning
648
12.9
Approximate Dynamic Programming: Indirect Methods
652
12.10 Least-Squares Policy Evaluation
655
12.11 Approximate Policy Iteration
660
12.12 Summary and Discussion
663
Notes and References
665
Problems
668
Chapter 13
Neurodynamics
672
13.1
Introduction
672
13.2
Dynamic Systems
674
13.3
Stability of Equilibrium States
678
13.4
Attractors
684
13.5
Neurodynamic Models
686
13.6
Manipulation of Attractors as a Recurrent
Network Paradigm
689
13.7
Hopfield Model
690
13.8
The Cohen–Grossberg Theorem
703
13.9
Brain-State-In-A-Box Model
705
13.10 Strange Attractors and Chaos
711
13.11 Dynamic Reconstruction of a Chaotic Process
716
13.12 Summary and Discussion
722
Notes and References
724
Problems
727
Chapter 14
Bayseian Filtering for State Estimation of Dynamic Systems
731
14.1
Introduction
731
14.2
State-Space Models
732
14.3
Kalman Filters
736
14.4
The Divergence-Phenomenon and Square-Root Filtering
744
14.5
The Extended Kalman Filter
750
14.6
The Bayesian Filter
755
14.7
Cubature Kalman Filter: Building on the Kalman Filter
759
14.8
Particle Filters
765
14.9
Computer Experiment: Comparative Evaluation of Extended Kalman and Particle 
Filters
775
14.10 Kalman Filtering in Modeling of Brain Functions
777
14.11 Summary and Discussion
780
Notes and References
782
Problems 784
Contents
ix

x
Contents
Chapter 15
Dynamically Driven Recurrent Networks
790
15.1
Introduction
790
15.2
Recurrent Network Architectures
791
15.3
Universal Approximation Theorem
797
15.4
Controllability and Observability
799
15.5
Computational Power of Recurrent Networks
804
15.6
Learning Algorithms
806
15.7
Back Propagation Through Time
808
15.8
Real-Time Recurrent Learning
812
15.9
Vanishing Gradients in Recurrent Networks
818
15.10 Supervised Training Framework for Recurrent Networks Using Nonlinear Sequential
State Estimators
822
15.11 Computer Experiment: Dynamic Reconstruction of Mackay–Glass Attractor
829
15.12 Adaptivity Considerations
831
15.13 Case Study: Model Reference Applied to Neurocontrol
833
15.14 Summary and Discussion
835
Notes and References
839
Problems
842
Bibliography
845
Index
889

xi
In writing this third edition of a classic book, I have been guided by the same underly-
ing philosophy of the first edition of the book:
Write an up-to-date treatment of neural networks in a comprehensive, thorough, and read-
able manner.
The new edition has been retitled Neural Networks and Learning Machines, in order to
reflect two realities:
1. The perceptron, the multilayer perceptron, self-organizing maps, and neuro-
dynamics, to name a few topics, have always been considered integral parts of
neural networks, rooted in ideas inspired by the human brain.
2. Kernel methods, exemplified by support-vector machines and kernel principal-
components analysis, are rooted in statistical learning theory.
Although, indeed, they share many fundamental concepts and applications, there are
some subtle differences between the operations of neural networks and learning ma-
chines. The underlying subject matter is therefore much richer when they are studied 
together, under one umbrella, particularly so when
• ideas drawn from neural networks and machine learning are hybridized to per-
form improved learning tasks beyond the capability of either one operating on its
own, and
• ideas inspired by the human brain lead to new perspectives wherever they are of
particular importance.
Moreover, the scope of the book has been broadened to provide detailed treat-
ments of dynamic programming and sequential state estimation, both of which have 
affected the study of reinforcement learning and supervised learning, respectively, in
significant ways.
Organization of the Book
The book begins with an introductory chapter that is motivational, paving the way for
the rest of the book which is organized into six parts as follows:
1. Chapters 1 through 4, constituting the first part of the book, follow the classical 
approach on supervised learning. Specifically,
Preface

xii
Preface
• Chapter 1 describes Rosenblatt’s perceptron, highlighting the perceptron con-
vergence theorem, and the relationship between the perceptron and the
Bayesian classifier operating in a Gaussian environment.
• Chapter 2 describes the method of least squares as a basis for model building.
The relationship between this method and Bayesian inference for the special
case of a Gaussian environment is established.This chapter also includes a dis-
cussion of the minimum description length (MDL) principle for model selection.
• Chapter 3 is devoted to the least-mean-square (LMS) algorithm and its con-
vergence analysis.The theoretical framework of the analysis exploits two prin-
ciples: Kushner’s direct method and the Langevin equation (well known in
nonequilibrium thermodynamics).
These three chapters, though different in conceptual terms, share a common
feature:They are all based on a single computational unit. Most importantly, they
provide a great deal of insight into the learning process in their own individual
ways—a feature that is exploited in subsequent chapters.
Chapter 4, on the multilayer perceptron, is a generalization of Rosenblatt’s
perceptron.This rather long chapter covers the following topics:
• the back-propagation algorithm, its virtues and limitations, and its role as an
optimum method for computing partial derivations;
• optimal annealing and adaptive control of the learning rate;
• cross-validation;
• convolutional networks, inspired by the pioneering work of Hubel and Wiesel
on visual systems;
• supervised learning viewed as an optimization problem, with attention focused
on conjugate-gradient methods, quasi-Newton methods, and the Marquardt–
Levenberg algorithm;
• nonlinear filtering;
• last, but by no means least, a contrasting discussion of small-scale versus large-
scale learning problems.
2. The next part of the book, consisting of Chapters 5 and 6, discusses kernel meth-
ods based on radial-basis function (RBF) networks.
In a way, Chapter 5 may be viewed as an insightful introduction to kernel
methods. Specifically, it does the following:
• presents Cover’s theorem as theoretical justification for the architectural struc-
ture of RBF networks;
• describes a relatively simple two-stage hybrid procedure for supervised learn-
ing, with stage 1 based on the idea of clustering (namely, the K-means algo-
rithm) for computing the hidden layer,and stage 2 using the LMS or the method
of least squares for computing the linear output layer of the network;
• presents kernel regression and examines its relation to RBF networks.
Chapter 6 is devoted to support vector machines (SVMs), which are com-
monly recognized as a method of choice for supervised learning. Basically, the
SVM is a binary classifier, in the context of which the chapter covers the fol-
lowing topics:

Preface
xiii
• the condition for defining the maximum margin of separation between a pair
of linearly separable binary classes;
• quadratic optimization for finding the optimal hyperplane when the two classes
are linearly separable and when they are not;
• the SVM viewed as a kernel machine, including discussions of the kernel trick
and Mercer’s theorem;
• the design philosophy of SVMs;
• the -insensitive loss function and its role in the optimization of regression 
problems;
• the Representer Theorem, and the roles of Hilbert space and reproducing ker-
nel Hilbert space (RKHS) in its formulation.
From this description, it is apparent that the underlying theory of support
vector machines is built on a strong mathematical background—hence their com-
putational strength as an elegant and powerful tool for supervised learning.
3. The third part of the book involves a single chapter, Chapter 7.This broadly based
chapter is devoted to regularization theory, which is at the core of machine learn-
ing.The following topics are studied in detail:
• Tikhonov’s classic regularization theory, which builds on the RKHS discussed in
Chapter 6.This theory embodies some profound mathematical concepts:the Fréchet
differential of the Tikhonov functional, the Riesz representation theorem, the
Euler–Lagrange equation,Green’s function,and multivariate Gaussian functions;
• generalized RBF networks and their modification for computational tractability;
• the regularized least-squares estimator, revisited in light of the Representer 
Theorem;
• estimation of the regularization parameter, using Wahba’s concept of general-
ized cross-validation;
• semisupervised learning, using labeled as well as unlabeled examples;
• differentiable manifolds and their role in manifold regularization—a role that
is basic to designing semisupervised learning machines;
• spectral graph theory for finding a Gaussian kernel in an RBF network used for
semisupervised learning;
• a generalized Representer Theorem for dealing with semisupervised kernel 
machines;
• the Laplacian regularized least-squares (LapRLS) algorithm for computing the
linear output layer of the RBF network; here, it should be noted that when the
intrinsic regularization parameter (responsible for the unlabeled data) is
reduced to zero, the algorithm is correspondingly reduced to the ordinary least-
squares algorithm.
This highly theoretical chapter is of profound practical importance.First,it provides
the basis for the regularization of supervised-learning machines. Second, it lays
down the groundwork for designing regularized semisupervised learning machines.
4. Chapters 8 through 11 constitute the fourth part of the book, dealing with unsu-
pervised learning. Beginning with Chapter 8, four principles of self-organization,
intuitively motivated by neurobiological considerations, are presented:

(i) Hebb’s postulate of learning for self-amplification;
(ii) Competition among the synapses of a single neuron or a group of neurons
for limited resources;
(iii) Cooperation among the winning neuron and its neighbors;
(iv) Structural information (e.g., redundancy) contained in the input data.
The main theme of the chapter is threefold:
• Principles (i), (ii), and (iv) are applied to a single neuron, in the course of which
Oja’s rule for maximum eigenfiltering is derived; this is a remarkable result 
obtained through self-organization, which involves bottom-up as well as top-
down learning. Next, the idea of maximum eigenfiltering is generalized to
principal-components analysis (PCA) on the input data for the purpose of
dimensionality reduction; the resulting algorithm is called the generalized Heb-
bian algorithm (GHA).
• Basically, PCA is a linear method, the computing power of which is therefore
limited to second-order statistics.In order to deal with higher-order statistics,the
kernel method is applied to PCA in a manner similar to that described in Chap-
ter 6 on support vector machines, but with one basic difference: unlike SVM,
kernel PCA is performed in an unsupervised manner.
• Unfortunately, in dealing with natural images, kernel PCA can become un-
manageable in computational terms. To overcome this computational limita-
tion, GHA and kernel PCA are hybridized into a new on-line unsupervised
learning algorithm called the kernel Hebbian algorithm (KHA), which finds
applications in image denoising.
The development of KHA is an outstanding example of what can be accomplished
when an idea from machine learning is combined with a complementary idea
rooted in neural networks, producing a new algorithm that overcomes their 
respective practical limitations.
Chapter 9 is devoted to self-organizing maps (SOMs), the development
of which follows the principles of self-organization described in Chapter 8.The
SOM is a simple algorithm in computational terms, yet highly powerful in its
built-in ability to construct organized topographic maps with several useful
properties:
• spatially discrete approximation of the input space,responsible for data generation;
• topological ordering, in the sense that the spatial location of a neuron in the
topographic map corresponds to a particular feature in the input (data) space;
• input–output density matching;
• input-data feature selection.
The SOM has been applied extensively in practice; the construction of contextual
maps and hierarchical vector quantization are presented as two illustrative ex-
amples of the SOM’s computing power. What is truly amazing is that the SOM
exhibits several interesting properties and solves difficult computational tasks, yet
it lacks an objective function that could be optimized.To fill this gap and thereby
provide the possibility of improved topographic mapping, the self-organizing map
is kernelized. This is done by introducing an entropic function as the objective
xiv
Preface

Preface
xv
function to be maximized. Here again, we see the practical benefit of hybridizing
ideas rooted in neural networks with complementary kernel-theoretic ones.
Chapter 10 exploits principles rooted in Shannon’s information theory as
tools for unsupervised learning. This rather long chapter begins by presenting a 
review of Shannon’s information theory, with particular attention given to the con-
cepts of entropy,mutual information,and the Kullback–Leibler divergence (KLD).
The review also includes the concept of copulas, which, unfortunately, has been
largely overlooked for several decades. Most importantly, the copula provides a
measure of the statistical dependence between a pair of correlated random vari-
ables. In any event, focusing on mutual information as the objective function, the
chapter establishes the following principles:
• The Infomax principle, which maximizes the mutual information between the
input and output data of a neural system; Infomax is closely related to redun-
dancy reduction.
• The Imax principle, which maximizes the mutual information between the sin-
gle outputs of a pair of neural systems that are driven by correlated inputs.
• The Imin principle operates in a manner similar to the Imax principle, except
that the mutual information between the pair of output random variables is
minimized.
• The independent-components analysis (ICA) principle,which provides a power-
ful tool for the blind separation of a hidden set of statistically independent
source signals. Provided that certain operating conditions are satisfied, the ICA
principle affords the basis for deriving procedures for recovering the original
source signals from a corresponding set of observables that are linearly mixed
versions of the source signals.Two specific ICA algorithms are described:
(i) the natural-gradient learning algorithm, which, except for scaling and per-
mutation, solves the ICA problem by minimizing the KLD between a pa-
rameterized probability density function and the corresponding factorial
distribution;
(ii) the maximum-entropy learning algorithm, which maximizes the entropy of
a nonlinearly transformed version of the demixer output; this algorithm,
commonly known as the Infomax algorithm for ICA, also exhibits scaling
and permutation properties.
Chapter 10 also describes another important ICA algorithm, known as FastICA,
which,as the name implies,is computationally fast.This algorithm maximizes a con-
trast function based on the concept of negentropy,which provides a measure of the
non-Gaussianity of a random variable. Continuing with ICA, the chapter goes on
to describe a new algorithm known as coherent ICA, the development of which
rests on fusion of the Infomax and Imax principles via the use of the copula;coherent
ICA is useful for extracting the envelopes of a mixture of amplitude-modulated
signals. Finally, Chapter 10 introduces another concept rooted in Shannon’s infor-
mation theory,namely,rate distortion theory,which is used to develop the last con-
cept in the chapter:information bottleneck.Given the joint distribution of an input
vector and a (relevant) output vector, the method is formulated as a constrained 

xvi
Preface
optimization problem in such a way that a tradeoff is created between two amounts
of information, one pertaining to information contained in the bottleneck vector
about the input and the other pertaining to information contained in the bottle-
neck vector about the output.The chapter then goes on to find an optimal manifold
for data representation, using the information bottleneck method.
The final approach to unsupervised learning is described in Chapter 11, using
stochastic methods that are rooted in statistical mechanics; the study of statistical
mechanics is closely related to information theory.The chapter begins by review-
ing the fundamental concepts of Helmholtz free energy and entropy (in a statisti-
cal mechanics sense), followed by the description of Markov chains. The stage is
then set for describing the Metropolis algorithm for generating a Markov chain,
the transition probabilities of which converge to a unique and stable distribution.
The discussion of stochastic methods is completed by describing simulated an-
nealing for global optimization, followed by Gibbs sampling, which can be used as
a special form of the Metropolis algorithm.With all this background on statistical
mechanics at hand, the stage is set for describing the Boltzmann machine, which,
in a historical context, was the first multilayer learning machine discussed in the
literature. Unfortunately, the learning process in the Boltzmann machine is very
slow, particularly when the number of hidden neurons is large—hence the lack of
interest in its practical use.Various methods have been proposed in the literature
to overcome the limitations of the Boltzmann machine.The most successful inno-
vation to date is the deep belief net, which distinguishes itself in the clever way in
which the following two functions are combined into a powerful machine:
• generative modeling, resulting from bottom-up learning on a layer-by-layer
basis and without supervision;
• inference, resulting from top-down learning.
Finally, Chapter 10 describes deterministic annealing to overcome the excessive
computational requirements of simulated annealing; the only problem with 
deterministic annealing is that it could get trapped in a local minimum.
5. Up to this point, the focus of attention in the book has been the formulation of al-
gorithms for supervised learning,semisupervised learning,and unsupervised learn-
ing. Chapter 12, constituting the next part of the book all by itself, addresses
reinforcement learning, in which learning takes place in an on-line manner as the
result of an agent (e.g., robot) interacting with its surrounding environment. In re-
ality, however, dynamic programming lies at the core of reinforcement learning.
Accordingly, the early part of Chapter 15 is devoted to an introductory treatment
of Bellman’s dynamic programming,which is then followed by showing that the two
widely used methods of reinforcement learning: Temporal difference (TD) learn-
ing, and Q-learning can be derived as special cases of dynamic programming. Both
TD-learning and Q-learning are relatively simple, on-line reinforcement learning
algorithms that do not require knowledge of transition probabilities. However,
their practical applications are limited to situations in which the dimensionality
of the state space is of moderate size. In large-scale dynamic systems, the curse
of dimensionality becomes a serious issue,making not only dynamic programming,

Preface
xvii
but also its approximate forms, TD-learning and Q-learning, computationally in-
tractable. To overcome this serious limitation, two indirect methods of approxi-
mate dynamic programming are described:
• a linear method called the least-squares policy evaluation (LSPV) algorithm,and
• a nonlinear method using a neural network (e.g., multilayer perceptron) as a
universal approximator.
6. The last part of the book, consisting of Chapters 13, 14, and 15, is devoted to
the study of nonlinear feedback systems, with an emphasis on recurrent neural
networks:
(i) Chapter 13 studies neurodynamics, with particular attention given to the sta-
bility problem. In this context, the direct method of Lyapunov is described.
This method embodies two theorems, one dealing with stability of the system
and the other dealing with asymptotic stability. At the heart of the method
is a Lyapunov function, for which an energy function is usually found to be
adequate. With this background theory at hand, two kinds of associative
memory are described:
• the Hopfield model, the operation of which demonstrates that a complex
system is capable of generating simple emergent behavior;
• the brain-state-in-a-box model, which provides a basis for clustering.
The chapter also discusses properties of chaotic processes and a regularized
procedure for their dynamic reconstruction.
(ii) Chapter 14 is devoted to the Bayesian filter, which provides a unifying basis
for sequential state estimation algorithms, at least in a conceptual sense.The
findings of the chapter are summarized as follows:
• The classic Kalman filter for a linear Gaussian environment is derived with
the use of the minimum mean-square-error criterion; in a problem at the
end of the chapter, it is shown that the Kalman filter so derived is a spe-
cial case of the Bayesian filter;
• square-root filtering is used to overcome the divergence phenomenon that
can arise in practical applications of the Kalman filter;
• the extended Kalman filter (EKF) is used to deal with dynamic systems
whose nonlinearity is of a mild sort; the Gaussian assumption is
maintained;
• the direct approximate form of the Bayesian filter is exemplified by a new
filter called the cubature Kalman filter (CKF);here again,the Gaussian as-
sumption is maintained;
• indirect approximate forms of the Bayesian filter are exemplified by par-
ticle filters, the implementation of which can accommodate nonlinearity as
well as non-Gaussianity.
With the essence of Kalman filtering being that of a predictor–corrector,
Chapter 14 goes on to describe the possible role of “Kalman-like filtering”
in certain parts of the human brain.

The final chapter of the book, Chapter 15, studies dynamically driven recur-
rent neural networks. The early part of the chapter discusses different structures
(models) for recurrent networks and their computing power, followed by two al-
gorithms for the training of recurrent networks:
• back propagation through time, and
• real-time recurrent learning.
Unfortunately both of these procedures, being gradient based, are likely to suffer
from the so-called vanishing-gradients problem.To mitigate the problem, the use
of nonlinear sequential state estimators is described at some length for the super-
vised training of recurrent networks in a rather novel manner. In this context, the
advantages and disadvantages of the extended Kalman filter (simple, but deriva-
tive dependent) and the cubature Kalman filter (derivative free, but more com-
plicated mathematically) as sequential state estimator for supervised learning are
discussed.The emergence of adaptive behavior, unique to recurrent networks, and
the potential benefit of using an adaptive critic to further enhance the capability
of recurrent networks are also discussed in the chapter.
An important topic featuring prominently in different parts of the book is 
supervised learning and semisupervised learning applied to large-scale problems. The
concluding remarks of the book assert that this topic is in its early stages of development;
most importantly, a four-stage procedure is described for its future development.
Distinct Features of the Book
Over and above the broad scope and thorough treatment of the topics summarized under
the organization of the book, distinctive features of the text include the following:
1. Chapters 1 through 7 and Chapter 10 include computer experiments involving the
double-moon configuration for generating data for the purpose of binary classifi-
cation.The experiments range from the simple case of linearly separable patterns
to difficult cases of nonseparable patterns. The double-moon configuration, as a
running example, is used all the way from Chapter 1 to Chapter 7, followed by
Chapter 10, thereby providing an experimental means for studying and compar-
ing the learning algorithms described in those eight chapters.
2. Computer experiments are also included in Chapter 8 on PCA, Chapter 9 on SOM
and kernel SOM, and Chapter 14 on dynamic reconstruction of the Mackay–Glass
attractor using the EKF and CKF algorithms.
3. Several case studies, using real-life data, are presented:
• Chapter 7 discusses the United States Postal Service (USPS) data for semisu-
pervised learning using the Laplacian RLS algorithm;
• Chapter 8 examines how PCA is applied to handwritten digital data and de-
scribes the coding and denoising of images;
• Chapter 10 treats the analysis of natural images by using sparse-sensory coding
and ICA;
• Chapter 13 presents dynamic reconstruction applied to the Lorenz attractor by
using a regularized RBF network.
xviii
Preface

Preface
xix
Chapter 15 also includes a section on the model reference adaptive control system
as a case study.
4. Each chapter ends with notes and references for further study, followed by end-
of-chapter problems that are designed to challenge, and therefore expand, the
reader’s expertise.
The glossary at the front of the book has been expanded to include explanatory
notes on the methodology used on matters dealing with matrix analysis and proba-
bility theory.
5. PowerPoint files of all the figures and tables in the book will be available to 
Instructors and can be found at www.prenhall.com/haykin.
6. Matlab codes for all the computer experiments in the book are available on the
Website of the publisher to all those who have purchased copies of the book.These
are available to students at www.pearsonhighered.com/haykin.
7. The book is accompanied by a Manual that includes the solutions to all the end-
of-chapter problems as well as computer experiments.
The manual is available from the publisher, Prentice Hall, only to instructors who
use the book as the recommended volume for a course, based on the material
covered in the book.
Last, but by no means least, every effort has been expended to make the book
error free and, most importantly, readable.
Simon Haykin
Ancaster,Ontario

This page intentionally left blank 

I am deeply indebted to many renowned authorities on neural networks and learning
machines around the world, who have provided invaluable comments on selected parts
of the book:
Dr. Sun-Ichi Amari,The RIKEN Brain Science Institute,Wako City, Japan
Dr. Susanne Becker, Department of Psychology, Neuroscience & Behaviour,
McMaster University, Hamilton, Ontario, Canada
Dr. Dimitri Bertsekas, MIT, Cambridge, Massachusetts
Dr. Leon Bottou, NEC Laboratories America, Princeton, New Jersey
Dr. Simon Godsill, University of Cambridge, Cambridge, England
Dr. Geoffrey Gordon, Carnegie-Mellon University, Pittsburgh, Pennsylvania
Dr. Peter Grünwald, CWI,Amsterdam, the Netherlands
Dr. Geoffrey Hinton, Department of Computer Science, University of Toronto,
Toronto, Ontario, Canada
Dr.Timo Honkela, Helsinki University of Technology, Helsinki, Finland
Dr. Tom Hurd, Department of Mathematics and Statistics, McMaster University,
Ontario, Canada.
Dr. Eugene Izhikevich,The Neurosciences Institute, San Diego, California
Dr. Juha Karhunen, Helsinki University of Technology, Helsinki, Finland
Dr. Kwang In Kim, Max-Planck-Institut für Biologische Kybernetik, Tübingen,
Germany
Dr. James Lo, University of Maryland at Baltimore County, Baltimore, Maryland
Dr. Klaus Müller, University of Potsdam and Fraunhofer Institut FIRST, Berlin,
Germany
Dr. Erkki Oja, Helsinki University of Technology, Helsinki, Finland
Dr. Bruno Olshausen, Redwood Center for Theoretical Neuroscience, University
of California, Berkeley, California
Dr. Danil Prokhorov,Toyota Technical Center,Ann Arbor, Michigan
Dr. Kenneth Rose, Electrical and Computer Engineering, University of California,
Santa Barbara, California
Dr.Bernhard Schölkopf,Max-Planck-Institut für Biologische Kybernetik,Tübingen,
Germany
Dr. Vikas Sindhwani, Department of Computer Science, University of Chicago,
Chicago, Illinois
xxi
Acknowledgments

Dr. Sergios Theodoridis, Department of Informatics, University of Athens,Athens,
Greece
Dr. Naftali Tishby,The Hebrew University, Jerusalem, Israel
Dr.John Tsitsiklis,Massachusetts Institute of Technology,Cambridge,Massachusetts
Dr. Marc Van Hulle, Katholieke Universiteit, Leuven, Belgium
Several photographs and graphs have been reproduced in the book with permis-
sions provided by Oxford University Press and
Dr. Anthony Bell, Redwood Center for Theoretical Neuroscience, University of
California, Berkeley, California
Dr. Leon Bottou, NEC Laboratories America, Princeton, New Jersey
Dr. Juha Karhunen, Helsinki University of Technology, Helsinki, Finland
Dr. Bruno Olshausen, Redwood Center for Theoretical Neuroscience, University
of California, Berkeley, California
Dr. Vikas Sindhwani, Department of Computer Science, University of Chicago,
Chicago, Illinois
Dr. Naftali Tishby,The Hebrew University, Jerusalem, Israel
Dr. Marc Van Hulle, Katholieke Universiteit, Leuven, Belgium
I thank them all most sincerely.
I am grateful to my graduate students:
1. Yanbo Xue, for his tremendous effort devoted to working on nearly all the com-
puter experiments produced in the book, and also for reading the second page
proofs of the book.
2. Karl Wiklund,for proofreading the entire book and making valuable comments for
improving it.
3. Haran Arasaratnam, for working on the computer experiment dealing with the
Mackay–Glass attractor.
4. Andreas Wendel (Graz University of technology, Austria) while he was on leave
at McMaster University, 2008.
I am grateful to Scott Disanno and Alice Dworkin of Prentice Hall for their sup-
port and hard work in the production of the book.Authorization of the use of color in
the book by Marcia Horton is truly appreciated; the use of color has made a tremendous
difference to the appearance of the book from cover to cover.
I am grateful to Jackie Henry of Aptara Corp. and her staff, including Donald E.
Smith, Jr., the proofreader, for the production of the book. I also wish to thank Brian
Baker and the copyeditor,Abigail Lin, at Write With, Inc., for their effort in copy-editing
the manuscript of the book.
The tremendous effort by my Technical Coordinator, Lola Brooks, in typing sev-
eral versions of the chapters in the book over the course of 12 months, almost nonstop,
is gratefully acknowledged.
Last, but by no means least, I thank my wife, Nancy, for having allowed me the
time and space, which I have needed over the last 12 months, almost nonstop, to com-
plete the book in a timely fashion.
Simon Haykin
xxii
Acknowledgments

ABBREVIATIONS
AR
autoregressive
BBTT
back propagation through time
BM
Boltzmann machine
BP
back propagation
b/s
bits per second
BSB
brain-state-in-a-box
BSS
Blind source (signal) separation
cmm
correlation matrix memory
CV
cross-validation
DFA
deterministic finite-state automata
EKF
extended Kalman filter
EM
expectation-maximization
FIR
finite-duration impulse response
FM
frequency-modulated (signal)
GCV
generalized cross-validation
GHA
generalized Hebbian algorithm
GSLC
generalized sidelobe canceler
Hz
hertz
ICA
independent-components analysis
Infomax
maximum mutual information
Imax
variant of Infomax
Imin
another variant of Infomax
KSOM
kernel self-organizing map
KHA
kernel Hebbian algorithm
LMS
least-mean-square
LR
likelihood ratio
xxiii
Abbreviations and Symbols

LS
Least-squares
LS-TD
Least-squares, temporal-difference
LTP
long-term potentiation
LTD
long-term depression
LR
likelihood ratio
LRT
Likelihood ratio test
MAP
Maximum a posteriori
MCA
minor-components analysis
MCMC
Markov Chan Monte Carlo
MDL
minimum description length
MIMO
multiple input–multiple output
ML
maximum likelihood
MLP
multilayer perceptron
MRC
model reference control
NARMA
nonlinear autoregressive moving average
NARX
nonlinear autoregressive with exogenous inputs
NDP
neuro-dynamic programming
NW
Nadaraya–Watson (estimator)
NWKR
Nadaraya–Watson kernal regression
OBD
optimal brain damage
OBS
optimal brain surgeon
OCR
optical character recognition
PAC
probably approximately correct
PCA
principal-components analysis
PF
Particle Filter
pdf
probability density function
pmf
probability mass function
QP
quadratic programming
RBF
radial basis function
RLS
recursive least-squares
RLS
regularized least-squares
RMLP
recurrent multilayer perceptron
RTRL
real-time recurrent learning
SIMO
single input–multiple output
SIR
sequential importance resampling
SIS
sequential important sampling
SISO
single input–single output
SNR
signal-to-noise ratio
SOM
self-organizing map
SRN
simple recurrent network (also referred to as Elman’s recurrent 
network)
xxiv
Abbreviations and Symbols

SVD
singular value decomposition
SVM
support vector machine
TD
temporal difference
TDNN
time-delay neural network
TLFN
time-lagged feedforward network
VC
Vapnik–Chervononkis (dimension)
VLSI
very-large-scale integration
XOR
exclusive OR
IMPORTANT SYMBOLS
a
action
aTb
inner product of vectors a and b
abT
outer product of vectors a and b
binomial coefficient
unions of A and B
B
inverse of temperature
bk
bias applied to neuron k
cos(a,b)
cosine of the angle between vectors a and b
cu, v(u, v)
probability density function of copula
D
depth of memory
Kullback–Leibler divergence between 
probability density functions f and g
adjoint of operator D
E
energy function
Ei
energy of state i in statistical mechanics

statistical expectation operator
average energy
exp
exponential
eav
average squared error, or sum of squared errors
e(n)
instantaneous value of the sum of squared errors
etotal
total sum of error squares
F
free energy
f*
subset (network) with minimum empirical risk
H
Hessian (matrix)
H
-1
inverse of Hessian H
i
square root of -1, also denoted by j
I
identity matrix
I
Fisher’s information matrix
J
mean-square error
J
Jacobian (matrix)
E
D~
Dfg
A ´ B
a l
m b
Abbreviations and Symbols
xxv

xxvi
Abbreviations and Symbols
P1/2
square root of matrix P
PT/2
transpose of square root of matrix P
Pn,n1
error covariance matrix in Kalman filter theory
kB
Boltzmann constant
log
logarithm
L(w)
log-likelihood function of weight vector w
l(w)
log-likelihood function of weight vector w based on a single example
Mc
controllability matrix
Mo
observability matrix
n
discrete time
pi
probability of state i in statistical mechanics
pij
transition probability from state i to state j
P
stochastic matrix
P(e|c)
conditional probability of error e given that the input is drawn from
class c
P

probability that the visible neurons of a Boltzmann machine are in
state 
, given that the network is in its clamped condition (i.e.,
positive phase)
P

probability that the visible neurons of a Boltzmann machine are in state ,
given that the network is in its free-running condition (i.e., negative phase)
ˆrx(j, k;n)]
estimate of autocorrelation function of xj(n) and xk(n)
ˆrdx(k;n)
estimate of cross-correlation function of d(n) and xk(n)
R
correlation matrix of an input vector
t
continuous time
T
temperature
t
training set (sample)
tr
operator denoting the trace of a matrix 
var
variance operator
V(x)
Lyapunov function of state vector x
vj
induced local field or activation potential of neuron j
wo
optimum value of synaptic weight vector
wkj
weight of synapse j belonging to neuron k
w*
optimum weight vector
equilibrium value of state vector x
average of state xj in a “thermal” sense
ˆx
estimate of x, signified by the use of a caret (hat)
absolute value (magnitude) of x
x*
complex conjugate of x, signified by asterisk as superscript
Euclidean norm (length) of vector x
xT
transpose of vector x, signified by the superscript T
z1
unit-time delay operator
Z
partition function
δj(n)
local gradient of neuron j at time n
∆w
small change applied to weight w
gradient operator
§
x
x
xj
x


2
Laplacian operator
gradient of J with respect to w
divergence of vector F

learning-rate parameter

cumulant

policy
	k
threshold applied to neuron k (i.e., negative of bias bk)

regularization parameter

k
kth eigenvalue of a square matrix
k()
nonlinear activation function of neuron k
symbol for “belongs to”
symbol for “union of”
symbol for “intersection of”
*
symbol for convolution

superscript symbol for pseudoinverse of a matrix

superscript symbol for updated estimate
Open and closed intervals
• The open interval (a, b) of a variable x signifies that a  x  b.
• The closed interval [a, b] of a variable x signifies that a  x  b.
• The closed-open interval [a, b) of a variable x signifies that a  x  b; likewise for
the open-closed interval (a, b], a  x  b.
Minima and Maxima
• The symbol arg 
f(w) signifies the minimum of the function f(w) with respect
to the argument vector w.
• The symbol arg 
f(w) signifies the maximum of the function f(w) with respect 
to the argument vector w.
max
w
min
w
¨
´

  F
§wJ
§
Abbreviations and Symbols
xxvii

This page intentionally left blank 

GLOSSARY
NOTATIONS I: MATRIX ANALYSIS
Scalars: Italic lowercase symbols are used for scalars.
Vectors: Bold lowercase symbols are used for vectors.
A vector is defined as a column of scalars. Thus, the inner product of a pair of m-
dimensional vectors, x and y, is written as
where the superscript T denotes matrix transposition. With the inner product being a
scalar, we therefore have
yTx  xTy
Matrices: Bold uppercase symbols are used for matrices.
Matrix multiplication is carried out on a row multiplied by column basis.To illustrate,con-
sider an m-by-k matrix X and a k-by-l matrix Y.The product of these two matrices yields
the m-by-l matrix
Z  XY
More specifically, the ij-th component of matrix Z is obtained by multiplying the ith row
of matrix X by the jth column of matrix Y, both of which are made up of k scalars.
The outer product of a pair of m-dimensional vectors, x and y, is written as xyT,
which is an m-by-m matrix.
NOTATIONS II: PROBABILITY THEORY
Random variables: Italic uppercase symbols are used for random variables.The sample
value (i.e., one-shot realization) of a random variable is denoted by the corresponding
= a
m
i=1
xiyi
≥
y1
y2
o
ym
¥
xTy = [x1, x2, ..., xm]
xxix

xxx
Contents
italic lowercase symbol. For example, we write X for a random variable and x for its
sample value.
Random vectors: Bold uppercase symbols are used for random vectors. Similarly, the
sample value of a random vector is denoted by the corresponding bold lowercase sym-
bol. For example, we write X for a random vector and x for its sample value.
The probability density function (pdf) of a random variable X is thus denoted by
pX(x), which is a function of the sample value x; the subscript X is included as a reminder
that the pdf pertains to random vector X.

1
WHAT IS A NEURAL NETWORK?
Work on artificial neural networks, commonly referred to as “neural networks,” has
been motivated right from its inception by the recognition that the human brain com-
putes in an entirely different way from the conventional digital computer.The brain is
a highly complex, nonlinear, and parallel computer (information-processing system). It
has the capability to organize its structural constituents, known as neurons, so as to
perform certain computations (e.g., pattern recognition, perception, and motor con-
trol) many times faster than the fastest digital computer in existence today. Consider,
for example, human vision, which is an information-processing task. It is the function
of the visual system to provide a representation of the environment around us and,
more important, to supply the information we need to interact with the environment.
To be specific, the brain routinely accomplishes perceptual recognition tasks (e.g., rec-
ognizing a familiar face embedded in an unfamiliar scene) in approximately 100–200 ms,
whereas tasks of much lesser complexity take a great deal longer on a powerful
computer.
For another example, consider the sonar of a bat. Sonar is an active echolocation
system. In addition to providing information about how far away a target (e.g., a flying
insect) is, bat sonar conveys information about the relative velocity of the target, the
size of the target, the size of various features of the target, and the azimuth and eleva-
tion of the target.The complex neural computations needed to extract all this informa-
tion from the target echo occur within a brain the size of a plum. Indeed, an echolocating
bat can pursue and capture its target with a facility and success rate that would be the
envy of a radar or sonar engineer.
How, then, does a human brain or the brain of a bat do it? At birth, a brain already
has considerable structure and the ability to build up its own rules of behavior through
what we usually refer to as “experience.” Indeed, experience is built up over time, with
much of the development (i.e., hardwiring) of the human brain taking place during the
first two years from birth, but the development continues well beyond that stage.
A “developing” nervous system is synonymous with a plastic brain: Plasticity per-
mits the developing nervous system to adapt to its surrounding environment. Just as
plasticity appears to be essential to the functioning of neurons as information-processing
units in the human brain, so it is with neural networks made up of artificial neurons. In
1
Introduction

its most general form, a neural network is a machine that is designed to model the way
in which the brain performs a particular task or function of interest; the network is usu-
ally implemented by using electronic components or is simulated in software on a dig-
ital computer. In this book, we focus on an important class of neural networks that
perform useful computations through a process of learning. To achieve good perfor-
mance, neural networks employ a massive interconnection of simple computing cells
referred to as “neurons” or “processing units.” We may thus offer the following defini-
tion of a neural network viewed as an adaptive machine1:
A neural network is a massively parallel distributed processor made up of simple processing
units that has a natural propensity for storing experiential knowledge and making it available
for use. It resembles the brain in two respects:
1. Knowledge is acquired by the network from its environment through a learning process.
2. Interneuron connection strengths, known as synaptic weights, are used to store the ac-
quired knowledge.
The procedure used to perform the learning process is called a learning algorithm,
the function of which is to modify the synaptic weights of the network in an orderly
fashion to attain a desired design objective.
The modification of synaptic weights provides the traditional method for the de-
sign of neural networks. Such an approach is the closest to linear adaptive filter theory,
which is already well established and successfully applied in many diverse fields (Widrow
and Stearns, 1985; Haykin, 2002). However, it is also possible for a neural network to
modify its own topology, which is motivated by the fact that neurons in the human brain
can die and new synaptic connections can grow.
Benefits of Neural Networks
It is apparent that a neural network derives its computing power through, first, its
massively parallel distributed structure and, second, its ability to learn and therefore
generalize. Generalization refers to the neural network’s production of reasonable
outputs for inputs not encountered during training (learning). These two information-
processing capabilities make it possible for neural networks to find good approximate
solutions to complex (large-scale) problems that are intractable. In practice, however,
neural networks cannot provide the solution by working individually. Rather, they need
to be integrated into a consistent system engineering approach. Specifically, a complex
problem of interest is decomposed into a number of relatively simple tasks, and neural
networks are assigned a subset of the tasks that match their inherent capabilities. It
is important to recognize, however, that we have a long way to go (if ever) before we can
build a computer architecture that mimics the human brain.
Neural networks offer the following useful properties and capabilities:
1. Nonlinearity. An artificial neuron can be linear or nonlinear.A neural network,
made up of an interconnection of nonlinear neurons, is itself nonlinear. Moreover, the
nonlinearity is of a special kind in the sense that it is distributed throughout the net-
work. Nonlinearity is a highly important property, particularly if the underlying physical
2
Introduction

mechanism responsible for generation of the input signal (e.g., speech signal) is inher-
ently nonlinear.
2. Input–Output Mapping. A popular paradigm of learning, called learning with a
teacher, or supervised learning, involves modification of the synaptic weights of a neur-
al network by applying a set of labeled training examples, or task examples. Each example
consists of a unique input signal and a corresponding desired (target) response.The network
is presented with an example picked at random from the set, and the synaptic weights
(free parameters) of the network are modified to minimize the difference between the
desired response and the actual response of the network produced by the input signal in
accordance with an appropriate statistical criterion. The training of the network is re-
peated for many examples in the set,until the network reaches a steady state where there
are no further significant changes in the synaptic weights. The previously applied train-
ing examples may be reapplied during the training session, but in a different order.Thus
the network learns from the examples by constructing an input–output mapping for the
problem at hand. Such an approach brings to mind the study of nonparametric statistical
inference, which is a branch of statistics dealing with model-free estimation,or,from a bi-
ological viewpoint, tabula rasa learning (Geman et al., 1992); the term “nonparametric”
is used here to signify the fact that no prior assumptions are made on a statistical model
for the input data. Consider, for example, a pattern classification task, where the re-
quirement is to assign an input signal representing a physical object or event to one of
several prespecified categories (classes). In a nonparametric approach to this problem,
the requirement is to “estimate” arbitrary decision boundaries in the input signal space
for the pattern-classification task using a set of examples, and to do so without invoking
a probabilistic distribution model. A similar point of view is implicit in the supervised
learning paradigm,which suggests a close analogy between the input–output mapping per-
formed by a neural network and nonparametric statistical inference.
3. Adaptivity. Neural networks have a built-in capability to adapt their synaptic
weights to changes in the surrounding environment. In particular, a neural network
trained to operate in a specific environment can be easily retrained to deal with minor
changes in the operating environmental conditions. Moreover, when it is operating in a
nonstationary environment (i.e., one where statistics change with time), a neural net-
work may be designed to change its synaptic weights in real time.The natural architec-
ture of a neural network for pattern classification, signal processing, and control
applications, coupled with the adaptive capability of the network, makes it a useful tool
in adaptive pattern classification, adaptive signal processing, and adaptive control.As a
general rule, it may be said that the more adaptive we make a system, all the time en-
suring that the system remains stable, the more robust its performance will likely be
when the system is required to operate in a nonstationary environment. It should be
emphasized, however, that adaptivity does not always lead to robustness; indeed, it may
do the very opposite. For example, an adaptive system with short-time constants may
change rapidly and therefore tend to respond to spurious disturbances, causing a dras-
tic degradation in system performance.To realize the full benefits of adaptivity, the prin-
cipal time constants of the system should be long enough for the system to ignore
spurious disturbances, and yet short enough to respond to meaningful changes in the
Section 1
What Is a Neural Network?
3

environment;the problem described here is referred to as the stability–plasticity dilemma
(Grossberg, 1988).
4. Evidential Response. In the context of pattern classification, a neural network
can be designed to provide information not only about which particular pattern to select,
but also about the confidence in the decision made.This latter information may be used
to reject ambiguous patterns, should they arise, and thereby improve the classification
performance of the network.
5. Contextual Information. Knowledge is represented by the very structure and
activation state of a neural network. Every neuron in the network is potentially affected
by the global activity of all other neurons in the network. Consequently, contextual
information is dealt with naturally by a neural network.
6. Fault Tolerance. A neural network, implemented in hardware form, has the
potential to be inherently fault tolerant, or capable of robust computation, in the
sense that its performance degrades gracefully under adverse operating conditions.
For example, if a neuron or its connecting links are damaged, recall of a stored pat-
tern is impaired in quality. However, due to the distributed nature of information
stored in the network, the damage has to be extensive before the overall response of
the network is degraded seriously. Thus, in principle, a neural network exhibits a
graceful degradation in performance rather than catastrophic failure. There is some
empirical evidence for robust computation, but usually it is uncontrolled. In order to
be assured that the neural network is, in fact, fault tolerant, it may be necessary to take
corrective measures in designing the algorithm used to train the network (Kerlirzin
and Vallet, 1993).
7. VLSI Implementability.The massively parallel nature of a neural network makes
it potentially fast for the computation of certain tasks.This same feature makes a neural
network well suited for implementation using very-large-scale-integrated (VLSI) tech-
nology. One particular beneficial virtue of VLSI is that it provides a means of capturing
truly complex behavior in a highly hierarchical fashion (Mead, 1989).
8. Uniformity of Analysis and Design. Basically, neural networks enjoy universal-
ity as information processors.We say this in the sense that the same notation is used in
all domains involving the application of neural networks.This feature manifests itself in
different ways:
• Neurons, in one form or another, represent an ingredient common to all neural
networks.
• This commonality makes it possible to share theories and learning algorithms in
different applications of neural networks.
• Modular networks can be built through a seamless integration of modules.
9. Neurobiological Analogy. The design of a neural network is motivated by
analogy with the brain, which is living proof that fault-tolerant parallel processing
is not only physically possible, but also fast and powerful. Neurobiologists look to
(artificial) neural networks as a research tool for the interpretation of neurobiolog-
ical phenomena. On the other hand, engineers look to neurobiology for new ideas to
solve problems more complex than those based on conventional hardwired design
4
Introduction

techniques. These two viewpoints are illustrated by the following two respective
examples:
• In Anastasio (1993), linear system models of the vestibulo-ocular reflex (VOR)
are compared to neural network models based on recurrent networks, which are
described in Section 6 and discussed in detail in Chapter 15.The vestibulo-ocular
reflex is part of the oculomotor system.The function of VOR is to maintain visual
(i.e., retinal) image stability by making eye rotations that are opposite to head ro-
tations.The VOR is mediated by premotor neurons in the vestibular nuclei that re-
ceive and process head rotation signals from vestibular sensory neurons and send
the results to the eye muscle motor neurons.The VOR is well suited for modeling
because its input (head rotation) and its output (eye rotation) can be precisely
specified. It is also a relatively simple reflex, and the neurophysiological proper-
ties of its constituent neurons have been well described.Among the three neural
types, the premotor neurons (reflex interneurons) in the vestibular nuclei are the
most complex and therefore most interesting.The VOR has previously been mod-
eled using lumped, linear system descriptors and control theory. These models
were useful in explaining some of the overall properties of the VOR, but gave lit-
tle insight into the properties of its constituent neurons. This situation has been
greatly improved through neural network modeling. Recurrent network models 
of VOR (programmed using an algorithm called real-time recurrent learning,
described in Chapter 15) can reproduce and help explain many of the static, dy-
namic, nonlinear, and distributed aspects of signal processing by the neurons that
mediate the VOR, especially the vestibular nuclei neurons.
• The retina, more than any other part of the brain, is where we begin to put together
the relationships between the outside world represented by a visual sense,its physical
image projected onto an array of receptors, and the first neural images. The retina is
a thin sheet of neural tissue that lines the posterior hemisphere of the eyeball.The
retina’s task is to convert an optical image into a neural image for transmission down
the optic nerve to a multitude of centers for further analysis.This is a complex task,
as evidenced by the synaptic organization of the retina. In all vertebrate retinas, the
transformation from optical to neural image involves three stages (Sterling, 1990):
(i) photo transduction by a layer of receptor neurons;
(ii) transmission of the resulting signals (produced in response to light) by chem-
ical synapses to a layer of bipolar cells;
(iii) transmission of these signals, also by chemical synapses, to output neurons
that are called ganglion cells.
At both synaptic stages (i.e., from receptor to bipolar cells, and from bipolar to
ganglion cells), there are specialized laterally connected neurons called horizontal
cells and amacrine cells, respectively. The task of these neurons is to modify the
transmission across the synaptic layers.There are also centrifugal elements called
inter-plexiform cells; their task is to convey signals from the inner synaptic layer
back to the outer one. Some researchers have built electronic chips that mimic
the structure of the retina. These electronic chips are called neuromorphic inte-
grated circuits, a term coined by Mead (1989). A neuromorphic imaging sensor
Section 1
What Is a Neural Network?
5

consists of an array of photoreceptors combined with analog circuitry at each 
picture element (pixel).It emulates the retina in that it can adapt locally to changes
in brightness, detect edges, and detect motion. The neurobiological analogy,
exemplified by neuromorphic integrated circuits, is useful in another important
way:It provides a hope and belief,and to a certain extent an existence of proof,that
physical understanding of neurobiological structures could have a productive 
influence on the art of electronics and VLSI technology for the implementation of
neural networks.
With inspiration from neurobiology in mind, it seems appropriate that we take a
brief look at the human brain and its structural levels of organization.2
2
THE HUMAN BRAIN
The human nervous system may be viewed as a three-stage system,as depicted in the block
diagram of Fig.1 (Arbib,1987).Central to the system is the brain, represented by the neural
(nerve) net, which continually receives information, perceives it, and makes appropriate
decisions.Two sets of arrows are shown in the figure.Those pointing from left to right in-
dicate the forward transmission of information-bearing signals through the system.The
arrows pointing from right to left (shown in red) signify the presence of feedback in the sys-
tem.The receptors convert stimuli from the human body or the external environment into
electrical impulses that convey information to the neural net (brain).The effectors convert
electrical impulses generated by the neural net into discernible responses as system outputs.
The struggle to understand the brain has been made easier because of the pio-
neering work of Ramón y Cajál (1911), who introduced the idea of neurons as struc-
tural constituents of the brain. Typically, neurons are five to six orders of magnitude
slower than silicon logic gates; events in a silicon chip happen in the nanosecond range,
whereas neural events happen in the millisecond range. However, the brain makes up
for the relatively slow rate of operation of a neuron by having a truly staggering num-
ber of neurons (nerve cells) with massive interconnections between them. It is estimated
that there are approximately 10 billion neurons in the human cortex, and 60 trillion
synapses or connections (Shepherd and Koch, 1990). The net result is that the brain is
an enormously efficient structure. Specifically, the energetic efficiency of the brain is ap-
proximately 10-16joules (J) per operation per second, whereas the corresponding value
for the best computers is orders of magnitude larger.
Synapses, or nerve endings, are elementary structural and functional units that me-
diate the interactions between neurons.The most common kind of synapse is a chemical
synapse, which operates as follows: A presynaptic process liberates a transmitter sub-
stance that diffuses across the synaptic junction between neurons and then acts on a post-
synaptic process.Thus a synapse converts a presynaptic electrical signal into a chemical
6
Introduction
Receptors
Stimulus
Response
Neural
net
Effectors
FIGURE 1
Block diagram representation of nervous system.

signal and then back into a postsynaptic electrical signal (Shepherd and Koch, 1990). In
electrical terminology, such an element is said to be a nonreciprocal two-port device. In
traditional descriptions of neural organization,it is assumed that a synapse is a simple con-
nection that can impose excitation or inhibition, but not both on the receptive neuron.
Earlier we mentioned that plasticity permits the developing nervous system to
adapt to its surrounding environment (Eggermont, 1990; Churchland and Sejnowski,
1992).In an adult brain,plasticity may be accounted for by two mechanisms:the creation
of new synaptic connections between neurons, and the modification of existing synapses.
Axons, the transmission lines, and dendrites, the receptive zones, constitute two types of
cell filaments that are distinguished on morphological grounds; an axon has a smoother
surface, fewer branches, and greater length, whereas a dendrite (so called because of its
resemblance to a tree) has an irregular surface and more branches (Freeman, 1975).
Neurons come in a wide variety of shapes and sizes in different parts of the brain.
Figure 2 illustrates the shape of a pyramidal cell, which is one of the most common types
of cortical neurons. Like many other types of neurons, it receives most of its inputs
through dendritic spines; see the segment of dendrite in the insert in Fig. 2 for detail.
The pyramidal cell can receive 10,000 or more synaptic contacts, and it can project onto
thousands of target cells.
The majority of neurons encode their outputs as a series of brief voltage pulses.
These pulses, commonly known as action potentials, or spikes,3 originate at or close to
the cell body of neurons and then propagate across the individual neurons at constant ve-
locity and amplitude. The reasons for the use of action potentials for communication
among neurons are based on the physics of axons.The axon of a neuron is very long and
thin and is characterized by high electrical resistance and very large capacitance.Both of
these elements are distributed across the axon. The axon may therefore be modeled as
resistance-capacitance (RC) transmission line, hence the common use of “cable equa-
tion”as the terminology for describing signal propagation along an axon.Analysis of this
propagation mechanism reveals that when a voltage is applied at one end of the axon, it
decays exponentially with distance,dropping to an insignificant level by the time it reach-
es the other end. The action potentials provide a way to circumvent this transmission
problem (Anderson, 1995).
In the brain,there are both small-scale and large-scale anatomical organizations,and
different functions take place at lower and higher levels. Figure 3 shows a hierarchy of in-
terwoven levels of organization that has emerged from the extensive work done on the
analysis of local regions in the brain (Shepherd and Koch, 1990; Churchland and Sejnow-
ski, 1992). The synapses represent the most fundamental level, depending on molecules
and ions for their action.At the next levels, we have neural microcircuits, dendritic trees,
and then neurons.A neural microcircuit refers to an assembly of synapses organized into
patterns of connectivity to produce a functional operation of interest.A neural microcir-
cuit may be likened to a silicon chip made up of an assembly of transistors.The smallest size
of microcircuits is measured in micrometers (m), and their fastest speed of operation is
measured in milliseconds.The neural microcircuits are grouped to form dendritic subunits
within the dendritic trees of individual neurons.The whole neuron, about 100 m in size,con-
tains several dendritic subunits.At the next level of complexity,we have local circuits (about
1 mm in size) made up of neurons with similar or different properties; these neural
Section 2
The Human Brain
7

assemblies perform operations characteristic of a localized region in the brain.They are fol-
lowed by interregional circuits made up of pathways,columns,and topographic maps,which
involve multiple regions located in different parts of the brain.
Topographic maps are organized to respond to incoming sensory information.
These maps are often arranged in sheets, as in the superior colliculus, where the visual,
8
Introduction
Dendritic spines
Synaptic
inputs
Basal
dendrites
Cell
body
Apical
dendrites
Axon
Synaptic
terminals
Segment
of dendrite
FIGURE 2
The pyramidal cell.

auditory, and somatosensory maps are stacked in adjacent layers in such a way that
stimuli from corresponding points in space lie above or below each other. Figure 4
presents a cytoarchitectural map of the cerebral cortex as worked out by Brodmann
(Brodal, 1981). This figure shows clearly that different sensory inputs (motor, so-
matosensory, visual, auditory, etc.) are mapped onto corresponding areas of the cere-
bral cortex in an orderly fashion. At the final level of complexity, the topographic
maps and other interregional circuits mediate specific types of behavior in the central
nervous system.
It is important to recognize that the structural levels of organization described
herein are a unique characteristic of the brain.They are nowhere to be found in a digi-
tal computer, and we are nowhere close to re-creating them with artificial neural net-
works. Nevertheless, we are inching our way toward a hierarchy of computational levels
similar to that described in Fig. 3. The artificial neurons we use to build our neural
networks are truly primitive in comparison with those found in the brain. The neural
networks we are presently able to design are just as primitive compared with the local
circuits and the interregional circuits in the brain. What is really satisfying, however, is
the remarkable progress that we have made on so many fronts. With neurobiological
analogy as the source of inspiration, and the wealth of theoretical and computational
tools that we are bringing together, it is certain that our understanding of artificial
neural networks and their applications will continue to grow in depth as well as
breadth, year after year.
Section 2
The Human Brain
9
Central nervous system
Interregional circuits
Local circuits
Neurons
Dendritic trees
Neural microcircuits
Synapses
Molecules
FIGURE 3
Structural
organization of levels
in the brain.

3
MODELS OF A NEURON
A neuron is an information-processing unit that is fundamental to the operation of a
neural network.The block diagram of Fig. 5 shows the model of a neuron, which forms
the basis for designing a large family of neural networks studied in later chapters. Here,
we identify three basic elements of the neural model:
1. A set of synapses, or connecting links, each of which is characterized by a weight
or strength of its own. Specifically, a signal xj at the input of synapse j connected to
neuron k is multiplied by the synaptic weight wkj. It is important to make a note
of the manner in which the subscripts of the synaptic weight wkj are written. The
first subscript in wkj refers to the neuron in question, and the second subscript
refers to the input end of the synapse to which the weight refers. Unlike the weight
of a synapse in the brain, the synaptic weight of an artificial neuron may lie in a
range that includes negative as well as positive values.
2. An adder for summing the input signals,weighted by the respective synaptic strengths
of the neuron; the operations described here constitute a linear combiner.
3. An activation function for limiting the amplitude of the output of a neuron.The ac-
tivation function is also referred to as a squashing function, in that it squashes
(limits) the permissible amplitude range of the output signal to some finite value.
10
Introduction
7
5
2
1
3
4
6
8
9
10
11
38
20
37
19
19
18
18
17
21
22
40
39
47
45
44
42
41
43
FIGURE 4
Cytoarchitectural map of the cerebral cortex.The different areas are identified
by the thickness of their layers and types of cells within them. Some of the key sensory areas
are as follows: Motor cortex: motor strip, area 4; premotor area, area 6; frontal eye fields, area
8. Somatosensory cortex: areas 3, 1, and 2.Visual cortex: areas 17, 18, and 19.Auditory cortex:
areas 41 and 42. (From A. Brodal, 1981; with permission of Oxford University Press.)

Typically, the normalized amplitude range of the output of a neuron is written
as the closed unit interval [0,1], or, alternatively, [-1,1].
The neural model of Fig. 5 also includes an externally applied bias, denoted by bk. The
bias bk has the effect of increasing or lowering the net input of the activation function,
depending on whether it is positive or negative, respectively.
In mathematical terms, we may describe the neuron k depicted in Fig. 5 by writ-
ing the pair of equations:
(1)
and
(2)
where x1, x2, ..., xm are the input signals; wk1, wk2, ..., wkm are the respective synaptic
weights of neuron k; uk (not shown in Fig.5) is the linear combiner output due to the input
signals; bk is the bias; „(·) is the activation function; and yk is the output signal of the
neuron.The use of bias bk has the effect of applying an affine transformation to the out-
put uk of the linear combiner in the model of Fig. 5, as shown by
(3)
In particular, depending on whether the bias bk is positive or negative, the relationship
between the induced local field, or activation potential, vk of neuron k and the linear
combiner output uk is modified in the manner illustrated in Fig. 6; hereafter, these two
terms are used interchangeably. Note that as a result of this affine transformation, the
graph of vk versus uk no longer passes through the origin.
The bias bk is an external parameter of neuron k. We may account for its presence
as in Eq.(2).Equivalently,we may formulate the combination of Eqs.(1) to (3) as follows:
(4)
vk = a
m
j=0
wkjxj
vk = uk + bk
yk = (uk + bk)
uk = a
m
j=1
wkjxj
Section 3
Models of a Neuron
11
FIGURE 5
Nonlinear model
of a neuron, labeled k.
Σ
wk1
wk2
wkm
x1
x2
xm
w()
Activation
function
Output
yk
Summing
junction
Synaptic
weights
Input
signals
Bias
bk
yk
•
•
•
•
•
•

and
(5)
In Eq. (4), we have added a new synapse. Its input is
(6)
and its weight is
(7)
We may therefore reformulate the model of neuron k as shown in Fig. 7. In this figure,
the effect of the bias is accounted for by doing two things: (1) adding a new input signal
fixed at 1, and (2) adding a new synaptic weight equal to the bias bk. Although the
models of Figs. 5 and 7 are different in appearance, they are mathematically equivalent.
wk0 = bk 
x0 = +1
yk = (vk)
12
Introduction
0
Induced
local
field yk
Linear combiner
output uk
Bias bk  0
bk  0
bk  0
FIGURE 6
Affine transformation produced by the
presence of a bias; note that vk  bk at uk  0.
Σ
wk1
wk2
wk0 wk0  bk
wkm
x1
Fixed input x0  1
x2
xm
Summing
junction
Synaptic
weights
(including bias)
Inputs
•
•
•
•
•
•
w()
Activation
function
Output
yk
yk
FIGURE 7
Another nonlinear model of a neuron;
accounts for the bias bk.
wk0

Types of Activation Function
The activation function, denoted by (v), defines the output of a neuron in terms of
the induced local field v. In what follows, we identify two basic types of activation
functions:
1. Threshold Function. For this type of activation function, described in Fig. 8a,
we have
(8)
In engineering, this form of a threshold function is commonly referred to as a Heaviside
function. Correspondingly, the output of neuron k employing such a threshold function
is expressed as
(9)
where vk is the induced local field of the neuron; that is,
(10)
vk = a
m
j=1
wkjxj + bk
yk = e1 if vk  0
0 if vk 6 0
(v) = e1 if v  0
0 if v 6 0

Section 3
Models of a Neuron
13
2
1.5
1
0.5
0
0.5
1
1.5
2
w(v)
v
(a)
10 8
6
4
2
0
2
4
8
6
10
w(v)
v
(b)
Increasing
a
FIGURE 8
(a) Threshold function.
(b) Sigmoid function for varying
slope parameter a.

In neural computation, such a neuron is referred to as the McCulloch–Pitts model, in
recognition of the pioneering work done by McCulloch and Pitts (1943). In this model,
the output of a neuron takes on the value of 1 if the induced local field of that neuron
is nonnegative, and 0 otherwise.This statement describes the all-or-none property of the
McCulloch–Pitts model.
2. Sigmoid Function.4 The sigmoid function, whose graph is “S”-shaped, is by far
the most common form of activation function used in the construction of neural net-
works. It is defined as a strictly increasing function that exhibits a graceful balance be-
tween linear and nonlinear behavior. An example of the sigmoid function is the logistic
function,5 defined by
(11)
where a is the slope parameter of the sigmoid function. By varying the parameter a, we
obtain sigmoid functions of different slopes, as illustrated in Fig. 8b. In fact, the slope
at the origin equals a/4. In the limit, as the slope parameter approaches infinity, the sig-
moid function becomes simply a threshold function.Whereas a threshold function as-
sumes the value of 0 or 1, a sigmoid function assumes a continuous range of values from
0 to 1. Note also that the sigmoid function is differentiable, whereas the threshold
function is not. (Differentiability is an important feature of neural network theory, as
described in Chapter 4).
The activation functions defined in Eqs. (8) and (11) range from 0 to 1. It is
sometimes desirable to have the activation function range from -1 to 1, in which case,
the activation function is an odd function of the induced local field. Specifically, the
threshold function of Eq. (8) is now defined as
(12)
which is commonly referred to as the signum function. For the corresponding form of a
sigmoid function, we may use the hyperbolic tangent function, defined by
(13)
Allowing an activation function of the sigmoid type to assume negative values as pre-
scribed by Eq. (13) may yield practical benefits over the logistic function of Eq. (11).
Stochastic Model of a Neuron
The neural model described in Fig. 7 is deterministic in that its input–output behav-
ior is precisely defined for all inputs. For some applications of neural networks, it is de-
sirable to base the analysis on a stochastic neural model. In an analytically tractable
approach, the activation function of the McCulloch–Pitts model is given a probabilistic
interpretation. Specifically, a neuron is permitted to reside in only one of two states: 1
(v) = tanh(v) 
(v) = • 1 if v 7 0
 0 if v = 0
-1 if v 6 0
(v) =
1
1 + exp(-av) 
14
Introduction

or -1, say. The decision for a neuron to fire (i.e., switch its state from “off” to “on”) is
probabilistic. Let x denote the state of the neuron and P(v) denote the probability of
firing, where v is the induced local field of the neuron.We may then write
(14)
A standard choice for P(v) is the sigmoid-shaped function
(15)
where T is a pseudotemperature used to control the noise level and therefore the un-
certainty in firing (Little, 1974). It is important to realize, however, that T is not the phys-
ical temperature of a neural network, be it a biological or an artificial neural network.
Rather, as already stated, we should think of T merely as a parameter that controls the
thermal fluctuations representing the effects of synaptic noise.Note that when 
,the
stochastic neuron described by Eqs. (14) and (15) reduces to a noiseless (i.e., determin-
istic) form, namely, the McCulloch–Pitts model.
4
NEURAL NETWORKS VIEWED AS DIRECTED GRAPHS
The block diagram of Fig. 5 or that of Fig. 7 provides a functional description of the var-
ious elements that constitute the model of an artificial neuron.We may simplify the ap-
pearance of the model by using the idea of signal-flow graphs without sacrificing any of
the functional details of the model. Signal-flow graphs, with a well-defined set of rules,
were originally developed by Mason (1953, 1956) for linear networks. The presence of
nonlinearity in the model of a neuron limits the scope of their application to neural net-
works. Nevertheless, signal-flow graphs do provide a neat method for the portrayal of
the flow of signals in a neural network, which we pursue in this section.
A signal-flow graph is a network of directed links (branches) that are intercon-
nected at certain points called nodes. A typical node j has an associated node signal xj.
A typical directed link originates at node j and terminates on node k; it has an associ-
ated transfer function, or transmittance, that specifies the manner in which the signal yk
at node k depends on the signal xj at node j. The flow of signals in the various parts of
the graph is dictated by three basic rules:
Rule 1. A signal flows along a link only in the direction defined by the arrow on
the link.
Two different types of links may be distinguished:
• Synaptic links, whose behavior is governed by a linear input–output relation.Specif-
ically, the node signal xj is multiplied by the synaptic weight wkj to produce the
node signal yk, as illustrated in Fig. 9a.
• Activation links, whose behavior is governed in general by a nonlinear input–output
relation. This form of relationship is illustrated in Fig. 9b, where (·) is the non-
linear activation function.

TS 0
P(v) =
1
1 + exp(-vT) 
x = e+1
with probability P(v)
-1
with probability 1 - P(v)
Section 4
Neural Networks Viewed As Directed Graphs
15

Rule 2. A node signal equals the algebraic sum of all signals entering the pertinent
node via the incoming links.
This second rule is illustrated in Fig.9c for the case of synaptic convergence,or fan-in.
Rule 3. The signal at a node is transmitted to each outgoing link originating from
that node, with the transmission being entirely independent of the transfer
functions of the outgoing links.
This third rule is illustrated in Fig. 9d for the case of synaptic divergence, or fan-out.
For example, using these rules, we may construct the signal-flow graph of Fig. 10 as
the model of a neuron, corresponding to the block diagram of Fig. 7. The representation
shown in Fig. 10 is clearly simpler in appearance than that of Fig. 7, yet it contains all the
functional details depicted in the latter diagram.Note that in both figures,the input x0  1
and the associated synaptic weight wk0  bk, where bk is the bias applied to neuron k.
Indeed, based on the signal-flow graph of Fig. 10 as the model of a neuron, we may
now offer the following mathematical definition of a neural network:
A neural network is a directed graph consisting of nodes with interconnecting synaptic and
activation links and is characterized by four properties:
1. Each neuron is represented by a set of linear synaptic links, an externally applied bias,
and a possibly nonlinear activation link.The bias is represented by a synaptic link con-
nected to an input fixed at 1.
2. The synaptic links of a neuron weight their respective input signals.
16
Introduction
xj
yk  wkjxj
wkj
xj
yk  w(xj)
yk  yi  yj
yi
yj
xj
xj
xj
(a)
(b)
(c)
(d)
w()
FIGURE 9
lllustrating basic rules for
the construction of signal-flow graphs.

3. The weighted sum of the input signals defines the induced local field of the neuron in
question.
4. The activation link squashes the induced local field of the neuron to produce an output.
A directed graph, defined in this manner is complete in the sense that it describes
not only the signal flow from neuron to neuron, but also the signal flow inside each neu-
ron.When,however,the focus of attention is restricted to signal flow from neuron to neu-
ron, we may use a reduced form of this graph by omitting the details of signal flow inside
the individual neurons. Such a directed graph is said to be partially complete. It is char-
acterized as follows:
1. Source nodes supply input signals to the graph.
2. Each neuron is represented by a single node called a computation node.
3. The communication links interconnecting the source and computation nodes of the
graph carry no weight; they merely provide directions of signal flow in the graph.
A partially complete directed graph defined in this way is referred to as an architectural
graph, describing the layout of the neural network. It is illustrated in Fig. 11 for the
simple case of a single neuron with m source nodes and a single node fixed at 1 for the
bias. Note that the computation node representing the neuron is shown shaded, and the
source node is shown as a small square.This convention is followed throughout the book.
More elaborate examples of architectural layouts are presented later in Section 6.
Section 4
Neural Networks Viewed as Directed Graphs
17
FIGURE 10
Signal-flow graph of a neuron.
FIGURE 11
Architectural graph of a neuron.
vk
wk0  bk
wk1
wk2
wkm
x2
x0  1
xm
x1
Output
yk
•
•
•
•
•
•
w()
x0  1
x2
x1
xm
Output
yk
•
•
•

To sum up, we have three graphical representations of a neural network:
• block diagram, providing a functional description of the network;
• architectural graph, describing the network layout;
• signal-flow graph, providing a complete description of signal flow in the network.
5
FEEDBACK
Feedback is said to exist in a dynamic system whenever the output of an element in the
system influences in part the input applied to that particular element, thereby giving
rise to one or more closed paths for the transmission of signals around the system.
Indeed, feedback occurs in almost every part of the nervous system of every animal
(Freeman, 1975). Moreover, it plays a major role in the study of a special class of neural
networks known as recurrent networks. Figure 12 shows the signal-flow graph of a single-
loop feedback system, where the input signal xj(n), internal signal xj (n), and output
signal yk(n) are functions of the discrete-time variable n. The system is assumed to be
linear, consisting of a forward path and a feedback path that are characterized by
the “operators” A and B, respectively. In particular, the output of the forward channel
determines in part its own output through the feedback channel. From Fig. 12, we readily
note the input–output relationships
(16)
and
(17)
where the square brackets are included to emphasize that A and B act as operators.
Eliminating xj(n) between Eqs. (16) and (17), we get
(18)
We refer to A/(1 - AB) as the closed-loop operator of the system, and to AB as the
open-loop operator. In general, the open-loop operator is noncommutative in that
.
Consider,for example,the single-loop feedback system shown in Fig.13a,for which
A is a fixed weight w and B is a unit-delay operator z-1, whose output is delayed with
respect to the input by one time unit.We may then express the closed-loop operator of
the system as
= w(1 - wz-1)-1
A
1 - AB =
w
1 - wz-1
BA Z AB
yk(n) =
A
1 - AB [xj(n)]
xj¿(n) = xj(n) + B[yk(n)] 
yk(n) = A[x¿j (n)] 
18
Introduction
xj(n)
xj(n)
yk(n)
A
B

FIGURE 12
Signal-flow graph of a
single-loop feedback system.

Using the binomial expansion for (1 - wz-1)-1, we may rewrite the closed-loop operator
of the system as
(19)
Hence, substituting Eq. (19) into (18), we get
(20)
where again we have included square brackets to emphasize the fact that z-1 is an op-
erator. In particular, from the definition of z-1, we have
(21)
where xj(n - l) is a sample of the input signal delayed by l time units. Accordingly, we
may express the output signal yk(n) as an infinite weighted summation of present and
past samples of the input signal xj(n), as shown by
(22)
We now see clearly that the dynamic behavior of a feedback system represented by the
signal-flow graph of Fig. 13 is controlled by the weight w. In particular, we may distin-
guish two specific cases:
1.
, for which the output signal yk(n) is exponentially convergent; that is, the
system is stable. This case is illustrated in Fig. 14a for a positive w.
2.
, for which the output signal yk(n) is divergent; that is, the system is unstable.
If 
the divergence is linear, as in Fig. 14b, and if 
the divergence is
exponential, as in Fig. 14c.
w 7 1
w = 1
w  1
w 6 1
yk(n) = a
q
l=0
wl+1xj(n - l) 
z-l[xj(n)] = xj(n - l) 
yk(n) = w a
q
l=0
wlz-l[xj(n)] 
A
1 - AB = w a
q
l=0
wlz-l 
Section 5
Feedback
19
FIGURE 13
(a) Signal-flow graph of a first-order, infinite-duration impulse response (IIR)
filter. (b) Feedforward approximation of part (a) of the figure, obtained by truncating Eq. (20).
xj(n)
xj(n)
yk(n)
w
z1

(a)
(b)
xj(n  1)
. . .
. . .
wN1
wN2
z1
z1
z1
w2
w
w
yk(n)
xj(n)

The issue of stability features prominently in the study of closed-loop feedback systems.
The case of 
corresponds to a system with infinite memory in the sense that
the output of the system depends on samples of the input extending into the infinite
past. Moreover, the memory is fading in that the influence of a past sample is reduced
exponentially with time n. Suppose that, for some power N,
is small enough relative
to unity such that wN is negligible for all practical purposes. In such a situation, we may
approximate the output yk by the finite sum
In a corresponding way, we may use the feedforward signal-flow graph of Fig. 13b as
the approximation for the feedback signal-flow graph of Fig. 13a. In making this ap-
proximation, we speak of the “unfolding” of a feedback system. Note, however, that
the unfolding operation is of practical value only when the feedback system is stable.
= wxj(n) + w2xj(n - 1) + w3xj(n - 2) + ... + wNxj(n - N + 1)
yk(n) L a
N-1
l=0
wl+1xj(n - l)
w
w 6 1
20
Introduction
(a)
wxj(0)
yk(n)
n
w  1
0
1
2
3
4
(b)
wxj(0)
yk(n)
n
w  1
0
1
2
3
4
(c)
wxj(0)
yk(n)
n
w  1
0
1
2
3
4
FIGURE 14
Time response of 
Fig. 13 for three different values 
of feedforward weight 
(a) Stable.
(b) Linear divergence.
(c) Exponential divergence.
w.

The analysis of the dynamic behavior of neural networks involving the applica-
tion of feedback is unfortunately complicated by the fact that the processing units used
for the construction of the network are usually nonlinear. Further consideration of this
important issue is deferred to the latter part of the book.
6
NETWORK ARCHITECTURES
The manner in which the neurons of a neural network are structured is intimately linked
with the learning algorithm used to train the network.We may therefore speak of learn-
ing algorithms (rules) used in the design of neural networks as being structured. The
classification of learning algorithms is considered in Section 8. In this section, we focus
attention on network architectures (structures).
In general, we may identify three fundamentally different classes of network
architectures:
(i)
Single-Layer Feedforward Networks
In a layered neural network, the neurons are organized in the form of layers. In the sim-
plest form of a layered network, we have an input layer of source nodes that projects
directly onto an output layer of neurons (computation nodes), but not vice versa. In
other words, this network is strictly of a feedforward type. It is illustrated in Fig. 15 for
the case of four nodes in both the input and output layers. Such a network is called a
single-layer network, with the designation “single-layer” referring to the output layer of
computation nodes (neurons).We do not count the input layer of source nodes because
no computation is performed there.
Section 6
Network Architectures
21
FIGURE 15
Feedforward network
with a single layer of neurons.
Input layer
of source
nodes
Output layer
of neurons

(ii)
Multilayer Feedforward Networks
The second class of a feedforward neural network distinguishes itself by the presence of
one or more hidden layers, whose computation nodes are correspondingly called hidden
neurons or hidden units; the term “hidden” refers to the fact that this part of the neural
network is not seen directly from either the input or output of the network.The func-
tion of hidden neurons is to intervene between the external input and the network out-
put in some useful manner. By adding one or more hidden layers, the network is enabled
to extract higher-order statistics from its input. In a rather loose sense, the network ac-
quires a global perspective despite its local connectivity, due to the extra set of synap-
tic connections and the extra dimension of neural interactions (Churchland and
Sejnowski, 1992).
The source nodes in the input layer of the network supply respective elements of
the activation pattern (input vector), which constitute the input signals applied to the
neurons (computation nodes) in the second layer (i.e., the first hidden layer).The out-
put signals of the second layer are used as inputs to the third layer, and so on for the rest
of the network.Typically, the neurons in each layer of the network have as their inputs
the output signals of the preceding layer only. The set of output signals of the neurons
in the output (final) layer of the network constitutes the overall response of the net-
work to the activation pattern supplied by the source nodes in the input (first) layer.The
architectural graph in Fig.16 illustrates the layout of a multilayer feedforward neural net-
work for the case of a single hidden layer. For the sake of brevity, the network in Fig. 16
is referred to as a 10–4–2 network because it has 10 source nodes, 4 hidden neurons, and
2 output neurons.As another example, a feedforward network with m source nodes, h1
neurons in the first hidden layer, h2 neurons in the second hidden layer, and q neurons
in the output layer is referred to as an m–h1–h2–q network.
22
Introduction
Input layer
of source
nodes
Layer of
hidden
neurons
Layer of
output
neurons
FIGURE 16
Fully connected
feedforward network with one
hidden layer and one output layer.

The neural network in Fig. 16 is said to be fully connected in the sense that every
node in each layer of the network is connected to every other node in the adjacent for-
ward layer. If, however, some of the communication links (synaptic connections) are
missing from the network, we say that the network is partially connected.
(iii)
Recurrent Networks
A recurrent neural network distinguishes itself from a feedforward neural network in that
it has at least one feedback loop. For example, a recurrent network may consist of a sin-
gle layer of neurons with each neuron feeding its output signal back to the inputs of all
the other neurons, as illustrated in the architectural graph in Fig. 17. In the structure de-
picted in this figure, there are no self-feedback loops in the network; self-feedback refers
to a situation where the output of a neuron is fed back into its own input.The recurrent
network illustrated in Fig. 17 also has no hidden neurons.
In Fig. 18 we illustrate another class of recurrent networks with hidden neurons.
The feedback connections shown in Fig. 18 originate from the hidden neurons as well
as from the output neurons.
The presence of feedback loops, be it in the recurrent structure of Fig. 17 or in that
of Fig.18,has a profound impact on the learning capability of the network and on its per-
formance. Moreover, the feedback loops involve the use of particular branches com-
posed of unit-time delay elements (denoted by z-1), which result in a nonlinear dynamic
behavior, assuming that the neural network contains nonlinear units.
Section 6
Network Architectures
23
FIGURE 17
Recurrent network
with no self-feedback loops and no
hidden neurons.
Unit-time delay
operators
z1 z1 z1 z1

7
KNOWLEDGE REPRESENTATION
In Section 1, we used the term “knowledge” in the definition of a neural network with-
out an explicit description of what we mean by it.We now take care of this matter by
offering the following generic definition (Fischler and Firschein, 1987):
Knowledge refers to stored information or models used by a person or machine to interpret,
predict, and appropriately respond to the outside world.
The primary characteristics of knowledge representation are twofold: (1) what informa-
tion is actually made explicit; and (2) how the information is physically encoded for sub-
sequent use.By the very nature of it,therefore,knowledge representation is goal directed.
In real-world applications of “intelligent” machines, it can be said that a good solution
depends on a good representation of knowledge (Woods, 1986). So it is with neural
networks. Typically, however, we find that the possible forms of representation from
the inputs to internal network parameters are highly diverse, which tends to make the
development of a satisfactory solution by means of a neural network a real design
challenge.
A major task for a neural network is to learn a model of the world (environment)
in which it is embedded, and to maintain the model sufficiently consistently with the
real world so as to achieve the specified goals of the application of interest. Knowledge
of the world consists of two kinds of information:
1. The known world state, represented by facts about what is and what has been
known; this form of knowledge is referred to as prior information.
2. Observations (measurements) of the world,obtained by means of sensors designed
to probe the environment, in which the neural network is supposed to operate.
24
Introduction
Unit-time delay
operators
Outputs
Inputs
z1
z1
z1
z1
FIGURE 18
Recurrent network with hidden neurons.

Ordinarily, these observations are inherently noisy, being subject to errors due to
sensor noise and system imperfections. In any event, the observations so obtained
provide the pool of information, from which the examples used to train the neural
network are drawn.
The examples can be labeled or unlabeled. In labeled examples, each example rep-
resenting an input signal is paired with a corresponding desired response (i.e., target out-
put). On the other hand, unlabeled examples consist of different realizations of the input
signal all by itself.In any event,a set of examples, labeled or otherwise,represents knowl-
edge about the environment of interest that a neural network can learn through training.
Note, however, that labeled examples may be expensive to collect, as they require the
availability of a “teacher”to provide a desired response for each labeled example.In con-
trast, unlabeled examples are usually abundant as there is no need for supervision.
A set of input–output pairs, with each pair consisting of an input signal and the cor-
responding desired response,is referred to as a set of training data, or simply training sam-
ple.To illustrate how such a data set can be used,consider,for example,the handwritten-digit
recognition problem. In this problem, the input signal consists of an image with black or
white pixels,with each image representing one of 10 digits that are well separated from the
background.The desired response is defined by the “identity”of the particular digit whose
image is presented to the network as the input signal.Typically,the training sample consists
of a large variety of handwritten digits that are representative of a real-world situation.
Given such a set of examples, the design of a neural network may proceed as follows:
• An appropriate architecture is selected for the neural network, with an input layer
consisting of source nodes equal in number to the pixels of an input image, and an
output layer consisting of 10 neurons (one for each digit).A subset of examples is
then used to train the network by means of a suitable algorithm.This phase of the
network design is called learning.
• The recognition performance of the trained network is tested with data not seen
before.Specifically,an input image is presented to the network,but this time the net-
work is not told the identity of the digit which that particular image represents.The
performance of the network is then assessed by comparing the digit recognition
reported by the network with the actual identity of the digit in question.This sec-
ond phase of the network operation is called testing, and successful performance on
the test patterns is called generalization, a term borrowed from psychology.
Herein lies a fundamental difference between the design of a neural network and
that of its classical information-processing counterpart: the pattern classifier. In the lat-
ter case, we usually proceed by first formulating a mathematical model of environmen-
tal observations, validating the model with real data, and then building the design on the
basis of the model. In contrast, the design of a neural network is based directly on real-
life data, with the data set being permitted to speak for itself. Thus, the neural network not
only provides the implicit model of the environment in which it is embedded, but also
performs the information-processing function of interest.
The examples used to train a neural network may consist of both positive and
negative examples. For instance, in a passive sonar detection problem, positive examples
pertain to input training data that contain the target of interest (e.g., a submarine). Now,
Section 7
Knowledge Representation
25

in a passive sonar environment, the possible presence of marine life in the test data is
known to cause occasional false alarms. To alleviate this problem, negative examples
(e.g.,echos from marine life) are included purposely in the training data to teach the net-
work not to confuse marine life with the target.
In a neural network of specified architecture, knowledge representation of the sur-
rounding environment is defined by the values taken on by the free parameters (i.e.,synap-
tic weights and biases) of the network.The form of this knowledge representation constitutes
the very design of the neural network, and therefore holds the key to its performance.
Roles of Knowledge Representation
The subject of how knowledge is actually represented inside an artificial network is,
however, very complicated. Nevertheless, there are four rules for knowledge represen-
tation that are of a general commonsense nature, as described next.
Rule 1. Similar inputs (i.e., patterns drawn) from similar classes should usually
produce similar representations inside the network, and should therefore
be classified as belonging to the same class.
There is a plethora of measures for determining the similarity between inputs. A
commonly used measure of similarity is based on the concept of Euclidian distance.To
be specific, let xi denote an m-by-1 vector
all of whose elements are real; the superscript T denotes matrix transposition. The vec-
tor xi defines a point in an m-dimensional space called Euclidean space and denoted by
m.As illustrated in Fig. 19, the Euclidean distance between a pair of m-by-1 vectors xi
and xj is defined by
(23)
where xik and xjk are the kth elements of the input vectors xi and xj, respectively. Corre-
spondingly, the similarity between the inputs represented by the vectors xi and xj is de-
fined as the Euclidean distance d(xi, xj).The closer the individual elements of the input
vectors xi and xj are to each other, the smaller the Euclidean distance d(xi, xj) is and
therefore the greater the similarity between the vectors xi and xj will be.Rule 1 states that
if the vectors xi and xj are similar, they should be assigned to the same class.
Another measure of similarity is based on the idea of a dot product, or inner prod-
uct, which is also borrowed from matrix algebra. Given a pair of vectors xi and xj of the
same dimension, their inner product is xi
Txj, defined as the projection of the vector xi
onto the vector xj, as illustrated in Fig. 19.We thus write
(24)
= a
m
k=1
xikxjk
(xi, xj) = xi
Txj 
= c a
m
k=1
(xik - xjk)2d
12 
d(xi, xj) = 7xi - xj7
xi = [xi1, xi2, ..., xim]T
26
Introduction

The inner product (xi, xj) divided by the product 
is the cosine of the angle sub-
tended between the vectors xi and xj.
The two measures of similarity defined here are indeed intimately related to each
other, as illustrated in Fig. 19. This figure shows clearly that the smaller the Euclidean
distance 
, and therefore the more similar the vectors xi and xj are, the larger
7 xi - xj7
7xj7
7xi7
Section 7
Knowledge Representation
27
xi
0
xj
xi
Txj
xi – xj
FIGURE 19
Illustrating the
relationship between inner product
and Euclidean distance as measures
of similiarity between patterns.
We may then use Eq. (23) to write
(25)
Equation (25) shows that minimization of the Euclidean distance d(xi, xi) corresponds
to maximization of the inner product (xi, xj) and, therefore, the similarity between the
vectors xi, and xj.
The Euclidean distance and inner product described here are defined in deter-
ministic terms.What if the vectors xi and xj are stochastic, drawn from two different pop-
ulations, or ensembles, of data? To be specific, suppose that the difference between these
two populations lies solely in their mean vectors. Let i and j denote the mean values
of the vectors xi and xj, respectively.That is,
(26)
where  is the statistical expectation operator over the ensemble of data vectors xi. The
mean vector j is similarly defined.For a measure of the distance between these two pop-
ulations, we may use the Mahalanobis distance, denoted by dij. The squared value of this
distance from xi to xj is defined by
(27)
where C1 is the inverse of the covariance matrix C. It is assumed that the covariance
matrix is the same for both populations, as shown by
(28)
= [(xj - j)(xj - j)T] 
C = [(xi - i)(xi - i)T]
dij
2 = (xi - i)TC-1(xj - j)
i = [xi]
= 2 - 2xT
i xj 
d2(xi, xj) = (xi - xj)T(xi - xj)
7xi7 = 7xj7 = 1
the inner product xi
Txj will be.
To put this relationship on a formal basis, we first normalize the vectors xi and xj
to have unit length, that is,

Then, for a prescribed C, the smaller the distance dij is, the more similar the vectors
xi and xj will be.
For the special case when xj  xi, i  j  , and C  I, where I is the identity
matrix, the Mahalanobis distance reduces to the Euclidean distance between the sam-
ple vector xi and the mean vector .
Regardless of whether the data vectors xi and xj are deterministic or stochastic,
Rule 1 addresses the issue of how these two vectors are correlated to each other.
Correlation plays a key role not only in the human brain, but also in signal processing
of various kinds (Chen et al., 2007).
Rule 2. Items to be categorized as separate classes should be given widely
different representations in the network.
According to Rule 1,patterns drawn from a particular class have an algebraic mea-
sure (e.g., Euclidean distance) that is small. On the other hand, patterns drawn from dif-
ferent classes have a large algebraic measure. We may therefore say that Rule 2 is the
dual of Rule 1.
Rule 3. If a particular feature is important, then there should be a large
number of neurons involved in the representation of that item in the
network.
Consider, for example, a radar application involving the detection of a target (e.g.,
aircraft) in the presence of clutter (i.e., radar reflections from undesirable targets such
as buildings, trees, and weather formations).The detection performance of such a radar
system is measured in terms of two probabilities:
• probability of detection, defined as the probability that the system decides that a
target is present when it is;
• probability of false alarm, defined as the probability that the system decides that
a target is present when it is not.
According to the Neyman–Pearson criterion, the probability of detection is maximized,sub-
ject to the constraint that the probability of false alarm does not exceed a prescribed value
(Van Trees,1968).In such an application,the actual presence of a target in the received sig-
nal represents an important feature of the input. Rule 3, in effect, states that there should
be a large number of neurons involved in making the decision that a target is present
when it actually is.By the same token,there should be a very large number of neurons in-
volved in making the decision that the input consists of clutter only when it actually
does. In both situations, the large number of neurons assures a high degree of accuracy
in decision making and tolerance with respect to faulty neurons.
Rule 4. Prior information and invariances should be built into the design of a
neural network whenever they are available, so as to simplify the network
design by its not having to learn them.
Rule 4 is particularly important because proper adherence to it results in a neural
network with a specialized structure. This is highly desirable for several reasons:
1. Biological visual and auditory networks are known to be very specialized.
28
Introduction

2. A neural network with specialized structure usually has a smaller number of free
parameters available for adjustment than a fully connected network.Consequently,
the specialized network requires a smaller data set for training, learns faster, and
often generalizes better.
3. The rate of information transmission through a specialized network (i.e., the net-
work throughput) is accelerated.
4. The cost of building a specialized network is reduced because of its smaller size,
relative to that of its fully connected counterpart.
Note, however, that the incorporation of prior knowledge into the design of a neural
network restricts application of the network to the particular problem being addressed
by the knowledge of interest.
How to Build Prior Information into Neural Network Design
An important issue that has to be addressed, of course, is how to develop a specialized
structure by building prior information into its design. Unfortunately, there are cur-
rently no well-defined rules for doing this; rather, we have some ad hoc procedures that
are known to yield useful results. In particular, we may use a combination of two tech-
niques:
1. restricting the network architecture, which is achieved through the use of local con-
nections known as receptive fields6;
2. constraining the choice of synaptic weights, which is implemented through the use of
weight-sharing.7
These two techniques, particularly the latter one, have a profitable side benefit: The
number of free parameters in the network could be reduced significantly.
To be specific,consider the partially connected feedforward network of Fig.20.This
network has a restricted architecture by construction.The top six source nodes constitute
Section 7
Knowledge Representation
29
Input layer
of source
nodes
Layer of
hidden
neurons
Layer of
output
neurons
1
2
3
1
y1
2
4
x1
x10
x9
x8
x7
x6
x5
x4
x3
x2
y2
FIGURE 20
Illustrating the
combined use of a receptive field
and weight sharing.All four hidden
neurons share the same set of
weights exactly for their six synaptic
connections.

the receptive field for hidden neuron 1, and so on for the other hidden neurons in the
network.The receptive field of a neuron is defined as that region of the input field over
which the incoming stimuli can influence the output signal produced by the neuron.The
mapping of the receptive field is a powerful and shorthand description of the neuron’s
behavior, and therefore its output.
To satisfy the weight-sharing constraint, we merely have to use the same set of
synaptic weights for each one of the neurons in the hidden layer of the network. Then,
for the example shown in Fig.20 with six local connections per hidden neuron and a total
of four hidden neurons, we may express the induced local field of hidden neuron j as
(29)
where {wi}6
i  1 constitutes the same set of weights shared by all four hidden neurons, and
xk is the signal picked up from source node k  i  j - 1. Equation (29) is in the form of
a convolution sum. It is for this reason that a feedforward network using local connec-
tions and weight sharing in the manner described herein is referred to as a convolutional
network (LeCun and Bengio, 2003).
The issue of building prior information into the design of a neural network pertains
to one part of Rule 4; the remaining part of the rule involves the issue of invariances,
which is discussed next.
How to Build Invariances into Neural Network Design
Consider the following physical phenomena:
• When an object of interest rotates, the image of the object as perceived by an ob-
server usually changes in a corresponding way.
• In a coherent radar that provides amplitude as well as phase information about its
surrounding environment, the echo from a moving target is shifted in frequency,
due to the Doppler effect that arises from the radial motion of the target in rela-
tion to the radar.
• The utterance from a person may be spoken in a soft or loud voice, and in a slow
or quick manner.
In order to build an object-recognition system, a radar target-recognition system, and a
speech-recognition system for dealing with these phenomena, respectively, the system
must be capable of coping with a range of transformations of the observed signal.
Accordingly, a primary requirement of pattern recognition is to design a classifier that
is invariant to such transformations. In other words, a class estimate represented by an
output of the classifier must not be affected by transformations of the observed signal
applied to the classifier input.
There are at least three techniques for rendering classifier-type neural networks
invariant to transformations (Barnard and Casasent, 1991):
1. Invariance by Structure. Invariance may be imposed on a neutral network by
structuring its design appropriately. Specifically, synaptic connections between the
vj = a
6
i=1
wixi+j-1,  j = 1, 2, 3, 4 
30
Introduction

neurons of the network are created so that transformed versions of the same input are
forced to produce the same output. Consider, for example, the classification of an input
image by a neural network that is required to be independent of in-plane rotations of
the image about its center.We may impose rotational invariance on the network struc-
ture as follows: Let wji be the synaptic weight of neuron j connected to pixel i in the
input image. If the condition wji  wjk is enforced for all pixels i and k that lie at equal
distances from the center of the image, then the neural network is invariant to in-plane
rotations.However,in order to maintain rotational invariance,the synaptic weight wji has
to be duplicated for every pixel of the input image at the same radial distance from the
origin.This points to a shortcoming of invariance by structure:The number of synaptic
connections in the neural network becomes prohibitively large even for images of
moderate size.
2. Invariance by Training. A neural network has a natural ability for pattern clas-
sification.This ability may be exploited directly to obtain transformation invariance as
follows:The network is trained by presenting it with a number of different examples of
the same object, with the examples being chosen to correspond to different transfor-
mations (i.e., different aspect views) of the object. Provided that the number of exam-
ples is sufficiently large,and if the the network is trained to learn to discriminate between
the different aspect views of the object, we may then expect the network to generalize
correctly to transformations other than those shown to it. However, from an engineer-
ing perspective, invariance by training has two disadvantages. First, when a neural net-
work has been trained to recognize an object in an invariant fashion with respect to
known transformations, it is not obvious that this training will also enable the network
to recognize other objects of different classes invariantly.Second,the computational de-
mand imposed on the network may be too severe to cope with, especially if the dimen-
sionality of the feature space is high.
3. Invariant Feature Space. The third technique of creating an invariant classifier-
type neural network is illustrated in Fig. 21. It rests on the premise that it may be pos-
sible to extract features that characterize the essential information content of an input
data set and that are invariant to transformations of the input. If such features are used,
then the network as a classifier is relieved of the burden of having to delineate the
range of transformations of an object with complicated decision boundaries. Indeed,
the only differences that may arise between different instances of the same object
are due to unavoidable factors such as noise and occlusion. The use of an invariant-
feature space offers three distinct advantages. First, the number of features applied to
the network may be reduced to realistic levels. Second, the requirements imposed on
network design are relaxed. Third, invariance for all objects with respect to known
transformations is assured.
Section 7
Knowledge Representation
31
Invariant
feature
extractor
Input
Class
estimate
Classifier-
type
neural
network
FIGURE 21
Block diagram of an
invariant-feature-space type of
system.

EXAMPLE 1:
Autoregressive Models
To illustrate the idea of invariant-feature space, consider the example of a coherent radar system
used for air surveillance, where the targets of interest include aircraft, weather systems, flocks of
migrating birds, and ground objects. The radar echoes from these targets possess different spec-
tral characteristics. Moreover, experimental studies have shown that such radar signals can be
modeled fairly closely as an autoregressive (AR) process of moderate order (Haykin and Deng,
1991).An AR model is a special form of regressive model defined for complex-valued data by
(30)
where {ai}M
i  1 are the AR coefficients, M is the model order, x(n) is the input, and e(n) is the error
described as white noise. Basically, the AR model of Eq. (30) is represented by a tapped-delay-line
filter as illustrated in Fig. 22a for M  2. Equivalently, it may be represented by a lattice filter as
shown in Fig. 22b, the coefficients of which are called reflection coefficients. There is a one-to-one
correspondence between the AR coefficients of the model in Fig.22a and the reflection coefficients
of the model in Fig. 22b.The two models depicted here assume that the input x(n) is complex val-
ued, as in the case of a coherent radar, in which case the AR coefficients and the reflection coeffi-
cients are all complex valued. The asterisk in Eq. (30) and Fig. 22 signifies complex conjugation.
For now, it suffices to say that the coherent radar data may be described by a set of autoregressive
coefficients, or by a corresponding set of reflection coefficients. The latter set of coefficients has
x(n) = a
M
i=1
a*ix(n - i) + e(n) 
32
Introduction
x(n  1)
x(n)
x(n  2)
Σ
z1
z1
*
w1
*
w2
x(n)
ˆ
x(n)
Σ
Σ
Σ
Σ
z1
z1
1
*
1
2
*
2
ˆ
e(n)  x(n)  x(n)
(a)
(b)
FIGURE 22
Autoregressive model of order 2: (a) tapped-delay-line model; (b) lattice-filter
model. (The asterisk denotes complex conjugation.)

a computational advantage in that efficient algorithms exist for their computation directly from
the input data. The feature extraction problem, however, is complicated by the fact that moving
objects produce varying Doppler frequencies that depend on their radial velocities measured with
respect to the radar, and that tend to obscure the spectral content of the reflection coefficients as
feature discriminants.To overcome this difficulty, we must build Doppler invariance into the com-
putation of the reflection coefficients.The phase angle of the first reflection coefficient turns out
to be equal to the Doppler frequency of the radar signal. Accordingly, Doppler frequency
normalization is applied to all coefficients so as to remove the mean Doppler shift. This is done
by defining a new set of reflection coefficients {κm} related to the set of ordinary reflection coef-
ficients {κm} computed from the input data as:
(31)
where 	 is the phase angle of the first reflection coefficient.The operation described in Eq. (31) is
referred to as heterodyning.A set of Doppler-invariant radar features is thus represented by the nor-
malized reflection coefficients κ1,κ2,...,κM ,with κ1 being the only real-valued coefficient in the set.
As mentioned previously, the major categories of radar targets of interest in air surveillance are
weather, birds, aircraft, and ground.The first three targets are moving, whereas the last one is not.
The heterodyned spectral parameters of radar echoes from ground have echoes similar in charac-
teristic to those from aircraft.A ground echo can be discriminated from an aircraft echo because of
its small Doppler shift.Accordingly,the radar classifier includes a postprocessor as shown in Fig.23,
which operates on the classified results (encoded labels) for the purpose of identifying the ground
class (Haykin and Deng, 1991). Thus, the preprocessor in Fig. 23 takes care of Doppler-shift-
invariant feature extraction at the classifier input,whereas the postprocessor uses the stored Doppler
signature to distinguish between aircraft and ground returns.
■
EXAMPLE 2:
Echolocating Bat
A much more fascinating example of knowledge representation in a neural network is found in
the biological sonar system of echolocating bats. Most bats use frequency-modulated (FM, or
“chirp”) signals for the purpose of acoustic imaging; in an FM signal, the instantaneous frequency
of the signal varies with time. Specifically, the bat uses its mouth to broadcast short-duration FM
sonar signals and uses its auditory system as the sonar receiver. Echoes from targets of interest
are represented in the auditory system by the activity of neurons that are selective to different com-
binations of acoustic parameters. There are three principal neural dimensions of the bat’s audi-
tory representation (Simmons et al., 1992):
•
Echo frequency, which is encoded by “place”originating in the frequency map of the cochlea;
it is preserved throughout the entire auditory pathway as an orderly arrangement across cer-
tain neurons tuned to different frequencies.
¿m = me-jm	  for m = 1, 2, ..., M
Section 7
Knowledge Representation
33
FIGURE 23
Doppler-shift-invariant classifier of radar signals.
Labeled
classes
Doppler information
Aircraft
Birds
Weather
Ground
Radar data
Neural
network
classifier
Postprocessor
Feature
extractor
(preprocessor)

•
Echo amplitude, which is encoded by other neurons with different dynamic ranges; it is
manifested both as amplitude tuning and as the number of discharges per stimulus.
•
Echo delay, which is encoded through neural computations (based on cross-correlation)
that produce delay-selective responses; it is manifested as target-range tuning.
The two principal characteristics of a target echo for image-forming purposes are spectrum
for target shape and delay for target range.The bat perceives “shape” in terms of the arrival time
of echoes from different reflecting surfaces (glints) within the target. For this to occur, frequency
information in the echo spectrum is converted into estimates of the time structure of the target.
Experiments conducted by Simmons and coworkers on the big brown bat, Eptesicus fuscus, crit-
ically identify this conversion process as consisting of parallel time-domain and frequency-to-
time-domain transforms whose converging outputs create the common delay of range axis of a
perceived image of the target. It appears that the unity of the bat’s perception is due to certain
properties of the transforms themselves, despite the separate ways in which the auditory time
representation of the echo delay and frequency representation of the echo spectrum are initially
performed. Moreover, feature invariances are built into the sonar image-forming process so as to
make it essentially independent of the target’s motion and the bat’s own motion.
■
Some Final Remarks
The issue of knowledge representation in a neural network is directly related to that of
network architecture. Unfortunately, there is no well-developed theory for optimizing
the architecture of a neural network required to interact with an environment of inter-
est,or for evaluating the way in which changes in the network architecture affect the rep-
resentation of knowledge inside the network.Indeed,satisfactory answers to these issues
are usually found through an exhaustive experimental study for a specific application of
interest, with the designer of the neural network becoming an essential part of the struc-
tural learning loop.
8
LEARNING PROCESSES
Just as there are different ways in which we ourselves learn from our own surrounding
environments,so it is with neural networks.In a broad sense,we may categorize the learn-
ing processes through which neural networks function as follows:learning with a teacher
and learning without a teacher.By the same token,the latter form of learning may be sub-
categorized into unsupervised learning and reinforcement learning.These different forms
of learning as performed on neural networks parallel those of human learning.
Learning with a Teacher
Learning with a teacher is also referred to as supervised learning. Figure 24 shows a block
diagram that illustrates this form of learning. In conceptual terms, we may think of the
teacher as having knowledge of the environment, with that knowledge being repre-
sented by a set of input–output examples. The environment is, however, unknown to the
neural network. Suppose now that the teacher and the neural network are both exposed
to a training vector (i.e.,example) drawn from the same environment.By virtue of built-in
34
Introduction

knowledge, the teacher is able to provide the neural network with a desired response
for that training vector. Indeed, the desired response represents the “optimum” ac-
tion to be performed by the neural network. The network parameters are adjusted
under the combined influence of the training vector and the error signal. The error
signal is defined as the difference between the desired response and the actual re-
sponse of the network. This adjustment is carried out iteratively in a step-by-step
fashion with the aim of eventually making the neural network emulate the teacher;
the emulation is presumed to be optimum in some statistical sense. In this way,
knowledge of the environment available to the teacher is transferred to the neural
network through training and stored in the form of “fixed” synaptic weights, repre-
senting long-term memory. When this condition is reached, we may then dispense
with the teacher and let the neural network deal with the environment completely
by itself.
The form of supervised learning we have just described is the basis of error-
correction learning. From Fig. 24, we see that the supervised-learning process con-
stitutes a closed-loop feedback system, but the unknown environment is outside the
loop. As a performance measure for the system, we may think in terms of the mean-
square error, or the sum of squared errors over the training sample, defined as a func-
tion of the free parameters (i.e., synaptic weights) of the system. This function may
be visualized as a multidimensional error-performance surface, or simply error surface,
with the free parameters as coordinates. The true error surface is averaged over all
possible input–output examples. Any given operation of the system under the
teacher’s supervision is represented as a point on the error surface. For the system to
improve performance over time and therefore learn from the teacher, the operating
point has to move down successively toward a minimum point of the error surface;
the minimum point may be a local minimum or a global minimum. A supervised
learning system is able to do this with the useful information it has about the gradient
of the error surface corresponding to the current behavior of the system.The gradient
Section 8
Learning Processes
35
Vector describing
state of the
environment
Desired
response
Error signal
Actual
response
Environment
Teacher
Σ


Learning
system
FIGURE 24
Block diagram of
learning with a teacher; the part of
the figure printed in red
constitutes a feedback loop.

36
Introduction
of the error surface at any point is a vector that points in the direction of steepest de-
scent. In fact, in the case of supervised learning from examples, the system may use an
instantaneous estimate of the gradient vector, with the example indices presumed to be
those of time. The use of such an estimate results in a motion of the operating point
on the error surface that is typically in the form of a “random walk.” Nevertheless,
given an algorithm designed to minimize the cost function, an adequate set of
input–output examples, and enough time in which to do the training, a supervised
learning system is usually able to approximate an unknown input–output mapping
reasonably well.
Learning without a Teacher
In supervised learning, the learning process takes place under the tutelage of a teacher.
However, in the paradigm known as learning without a teacher, as the name implies,
there is no teacher to oversee the learning process.That is to say, there are no labeled ex-
amples of the function to be learned by the network. Under this second paradigm, two
subcategories are identified:
1.
Reinforcement Learning
In reinforcement learning,the learning of an input–output mapping is performed through
continued interaction with the environment in order to minimize a scalar index of per-
formance. Figure 25 shows the block diagram of one form of a reinforcement-learning
system built around a critic that converts a primary reinforcement signal received from the
environment into a higher quality reinforcement signal called the heuristic reinforcement
signal,both of which are scalar inputs (Barto et al.,1983).The system is designed to learn
under delayed reinforcement,which means that the system observes a temporal sequence
of stimuli also received from the environment, which eventually result in the generation
of the heuristic reinforcement signal.
Critic
State (input)
vector
Learning
system
Heuristic
reinforcement
signal
Primary
reinforcement
signal
Environment
Actions
FIGURE 25
Block diagram of
reinforcement learning; the learning
system and the environment are
both inside the feedback loop.

Section 8
Learning Processes
37
The goal of reinforcement learning is to minimize a cost-to-go function,defined as the
expectation of the cumulative cost of actions taken over a sequence of steps instead of sim-
ply the immediate cost. It may turn out that certain actions taken earlier in that sequence
of time steps are in fact the best determinants of overall system behavior. The function
of the learning system is to discover these actions and feed them back to the environment.
Delayed-reinforcement learning is difficult to perform for two basic reasons:
• There is no teacher to provide a desired response at each step of the learning
process.
• The delay incurred in the generation of the primary reinforcement signal implies
that the learning machine must solve a temporal credit assignment problem. By this
we mean that the learning machine must be able to assign credit and blame indi-
vidually to each action in the sequence of time steps that led to the final outcome,
while the primary reinforcement may only evaluate the outcome.
Notwithstanding these difficulties, delayed-reinforcement learning is appealing. It pro-
vides the basis for the learning system to interact with its environment, thereby devel-
oping the ability to learn to perform a prescribed task solely on the basis of the outcomes
of its experience that result from the interaction.
2.
Unsupervised Learning
In unsupervised, or self-organized, learning, there is no external teacher or critic to
oversee the learning process, as indicated in Fig. 26. Rather, provision is made for a
task-independent measure of the quality of representation that the network is required
to learn, and the free parameters of the network are optimized with respect to that mea-
sure. For a specific task-independent measure, once the network has become tuned to
the statistical regularities of the input data, the network develops the ability to form in-
ternal representations for encoding features of the input and thereby to create new
classes automatically (Becker, 1991).
To perform unsupervised learning, we may use a competitive-learning rule. For
example, we may use a neural network that consists of two layers—an input layer and
a competitive layer. The input layer receives the available data. The competitive layer
consists of neurons that compete with each other (in accordance with a learning rule)
for the “opportunity” to respond to features contained in the input data. In its simplest
form, the network operates in accordance with a “winner-takes-all” strategy. In such a
strategy, the neuron with the greatest total input “wins” the competition and turns on;
all the other neurons in the network then switch off.
Vector describing
state of the
environment
Learning
system
Environment
FIGURE 26
Block diagram 
of unsupervised learning.

38
Introduction
9
LEARNING TASKS
In the previous section, we discussed different learning paradigms. In this section, we de-
scribe some basic learning tasks.The choice of a particular learning rule, is of course, in-
fluenced by the learning task, the diverse nature of which is testimony to the universality
of neural networks.
Pattern Association
An associative memory is a brainlike distributed memory that learns by association. As-
sociation has been known to be a prominent feature of human memory since the time
of Aristotle, and all models of cognition use association in one form or another as the
basic operation (Anderson, 1995).
Association takes one of two forms: autoassociation and heteroassociation. In au-
toassociation, a neural network is required to store a set of patterns (vectors) by re-
peatedly presenting them to the network. The network is subsequently presented
with a partial description or distorted (noisy) version of an original pattern stored in
it, and the task is to retrieve (recall) that particular pattern. Heteroassociation dif-
fers from autoassociation in that an arbitrary set of input patterns is paired with an-
other arbitrary set of output patterns. Autoassociation involves the use of
unsupervised learning, whereas the type of learning involved in heteroassociation is
supervised.
Let xk denote a key pattern (vector) applied to an associative memory and yk de-
note a memorized pattern (vector). The pattern association performed by the network
is described by
(32)
where q is the number of patterns stored in the network.The key pattern xk acts as a stim-
ulus that not only determines the storage location of memorized pattern yk, but also
holds the key for its retrieval.
In an autoassociative memory, yk  xk, so the input and output (data) spaces of the
network have the same dimensionality. In a heteroassociative memory,
; hence,
the dimensionality of the output space in this second case may or may not equal the di-
mensionality of the input space.
There are two phases involved in the operation of an associative memory:
• storage phase, which refers to the training of the network in accordance with Eq.
(32);
• recall phase, which involves the retrieval of a memorized pattern in response
to the presentation of a noisy or distorted version of a key pattern to the net-
work.
Let the stimulus (input) x represent a noisy or distorted version of a key pattern xj.This
stimulus produces a response (output) y, as indicated in Fig. 27. For perfect recall, we
should find that y  yj,where yj is the memorized pattern associated with the key pattern
xj.When 
for x  xj,the associative memory is said to have made an error in recall.
y Z yj
yk Z xk
xk S yk,  k = 1, 2, ..., q

Section 9
Learning Tasks
39
The number of patterns q stored in an associative memory provides a direct
measure of the storage capacity of the network. In designing an associative memory,
the challenge is to make the storage capacity q (expressed as a percentage of the total
number N of neurons used to construct the network) as large as possible, yet insist that
a large fraction of the memorized patterns is recalled correctly.
Pattern Recognition
Humans are good at pattern recognition. We receive data from the world around us
via our senses and are able to recognize the source of the data. We are often able to
do so almost immediately and with practically no effort. For example, we can recog-
nize the familiar face of a person even though that person has aged since our last en-
counter, identify a familiar person by his or her voice on the telephone despite a bad
connection, and distinguish a boiled egg that is good from a bad one by smelling it.
Humans perform pattern recognition through a learning process; so it is with neural
networks.
Pattern recognition is formally defined as the process whereby a received pat-
tern/signal is assigned to one of a prescribed number of classes. A neural network per-
forms pattern recognition by first undergoing a training session during which the
network is repeatedly presented with a set of input patterns along with the category
to which each particular pattern belongs. Later, the network is presented with a new
pattern that has not been seen before, but which belongs to the same population of pat-
terns used to train the network. The network is able to identify the class of that par-
ticular pattern because of the information it has extracted from the training data.
Pattern recognition performed by a neural network is statistical in nature, with the
patterns being represented by points in a multidimensional decision space. The deci-
sion space is divided into regions, each one of which is associated with a class. The
decision boundaries are determined by the training process.The construction of these
boundaries is made statistical by the inherent variability that exists within and
between classes.
In generic terms, pattern-recognition machines using neural networks may take
one of two forms:
• The machine is split into two parts, an unsupervised network for feature extraction
and a supervised network for classification, as shown in the hybridized system of 
Fig. 28a. Such a method follows the traditional approach to statistical pattern
recognition (Fukunaga, 1990; Duda et al., 2001; Theodoridis and Koutroumbas,
2003). In conceptual terms, a pattern is represented by a set of m observables,
which may be viewed as a point x in an m-dimensional observation (data) space.
Pattern
associator
Input
vector
x
Output
vector
y
FIGURE 27
Input–output relation
of pattern associator.

40
Introduction
Feature extraction is described by a transformation that maps the point x into
an intermediate point y in a q-dimensional feature space with q < m, as indicated
in Fig. 28b. This transformation may be viewed as one of dimensionality
reduction (i.e., data compression), the use of which is justified on the grounds
that it simplifies the task of classification. The classification is itself described
as a transformation that maps the intermediate point y into one of the classes
in an r-dimensional decision space, where r is the number of classes to be dis-
tinguished.
• The machine is designed as a feedforward network using a supervised learning al-
gorithm.In this second approach,the task of feature extraction is performed by the
computational units in the hidden layer(s) of the network.
Function Approximation
The third learning task of interest is that of function approximation.Consider a nonlinear
input–output mapping described by the functional relationship
(33)
where the vector x is the input and the vector d is the output. The vector-
valued function f(·) is assumed to be unknown. To make up for the lack of knowledge
about the function f(·), we are given the set of labeled examples:
(34)
t = {(xi, di)}N
i=1
d = f(x)
FIGURE 28
Illustration of the classical approach to pattern classification.
Unsupervised
network for
feature
extraction
Feature
vector
y
Feature
extraction
Classification
m-dimensional
observation space
q-dimensional
feature space
r-dimensional
decision space
1
2
r
Supervised
network for
classification
Input
pattern
x
x
y
•
•
•
(a)
(b)
Outputs

Section 9
Learning Tasks
41
The requirement is to design a neural network that approximates the unknown function
f(·) such that the function F(·) describing the input–output mapping actually realized by
the network, is close enough to f(·) in a Euclidean sense over all inputs, as shown by
(35)
where is a small positive number. Provided that the size N of the training sample 
is
large enough and the network is equipped with an adequate number of free parameters,
then the approximation error can be made small enough for the task.
The approximation problem described here is a perfect candidate for supervised
learning, with xi playing the role of input vector and di serving the role of desired re-
sponse. We may turn this issue around and view supervised learning as an approxima-
tion problem.
The ability of a neural network to approximate an unknown input–output
mapping may be exploited in two important ways:
(i) System identification. Let Eq. (33) describe the input–output relation of an un-
known memoryless multiple input–multiple output (MIMO) system; by a “memo-
ryless” system, we mean a system that is time invariant. We may then use the set
of labeled examples in Eq. (34) to train a neural network as a model of the system.
Let the vector yi denote the actual output of the neural network produced in re-
sponse to an input vector xi.The difference between di (associated with xi) and the
network output yi provides the error signal vector ei, as depicted in Fig. 29. This
error signal is, in turn, used to adjust the free parameters of the network to mini-
mize the squared difference between the outputs of the unknown system and the
neural network in a statistical sense, and is computed over the entire training
sample
(ii) Inverse modeling. Suppose next we are given a known memoryless MIMO system
whose input–output relation is described by Eq. (33). The requirement in this
case is to construct an inverse model that produces the vector x in response to
the vector d. The inverse system may thus be described by
(36)
x = f-1(d) 
t.

t

7F(x) - f(x)7 6  for all x
FIGURE 29
Block diagram of
system identification:The neural
network, doing the identification, is
part of the feedback loop.
Unknown
system
Input
vector
xi
yi
di
ei
Σ
Neural
network
model



42
Introduction
where the vector-valued function f-1(·) denotes the inverse of f(·). Note, however,
that f-1(·) is not the reciprocal of f(·); rather, the use of superscript -1 is merely
a flag to indicate an inverse. In many situations encountered in practice, the
vector-valued function f(·) is much too complex and inhibits a straightforward
formulation of the inverse function f-1(·). Given the set of labeled examples in
Eq. (34), we may construct a neural network approximation of f-1(·) by using the
scheme shown in Fig. 30. In the situation described here, the roles of xi and di are
interchanged: The vector di is used as the input, and xi is treated as the desired
response. Let the error signal vector ei denote the difference between xi and the
actual output yi of the neural network produced in response to di.As with the sys-
tem identification problem, this error signal vector is used to adjust the free pa-
rameters of the neural network to minimize the squared difference between the
outputs of the unknown inverse system and the neural network in a statistical
sense, and is computed over the complete training set . Typically, inverse mod-
eling is a more difficult learning task than system identification, as there may
not be a unique solution for it.
Control
The control of a plant is another learning task that is well suited for neural networks; by
a “plant” we mean a process or critical part of a system that is to be maintained in a con-
trolled condition.The relevance of learning to control should not be surprising because,
after all,the human brain is a computer (i.e.,information processor),the outputs of which
as a whole system are actions. In the context of control, the brain is living proof that it is
possible to build a generalized controller that takes full advantage of parallel distributed
hardware,can control many thousands of actuators (muscle fibers) in parallel,can handle
nonlinearity and noise, and can optimize over a long-range planning horizon (Werbos,
1992).
Consider the feedback control system shown in Fig. 31. The system involves the
use of unity feedback around a plant to be controlled; that is, the plant output is fed
back directly to the input. Thus, the plant output y is subtracted from a reference signal
d supplied from an external source.The error signal e so produced is applied to a neural
controller for the purpose of adjusting its free parameters.The primary objective of the
controller is to supply appropriate inputs to the plant to make its output y track the
t
FIGURE 30
Block diagram of inverse system modeling. The neural network, acting
as the inverse model, is part of the feedback loop.
Input
vector
xi
System
output
di
Error
ei
xi
Model
output
yi
Σ
Inverse
model
Unknown


f()

Section 9
Learning Tasks
43
reference signal d. In other words, the controller has to invert the plant’s input–output
behavior.
We note that in Fig. 31, the error signal e has to propagate through the neural con-
troller before reaching the plant. Consequently, to perform adjustments on the free pa-
rameters of the plant in accordance with an error-correction learning algorithm,we need
to know the Jacobian, made up of a matrix of partial derivatives as shown by
(37)
where yk is an element of the plant output y and uj is an element of the plant input u.
Unfortunately, the partial derivatives 
for the various k and j depend on the oper-
ating point of the plant and are therefore not known.We may use one of two approaches
to account for them:
(i) Indirect learning. Using actual input–output measurements on the plant, we first
construct a neural model to produce a copy of it. This model is, in turn, used to
provide an estimate of the Jacobian J. The partial derivatives constituting this Ja-
cobian are subsequently used in the error-correction learning algorithm for com-
puting the adjustments to the free parameters of the neural controller (Nguyen and
Widrow, 1989; Suykens et al., 1996;Widrow and Walach, 1996).
(ii) Direct learning. The signs of the partial derivatives 
are generally known
and usually remain constant over the dynamic range of the plant. This suggests
that we may approximate these partial derivatives by their individual signs.Their
absolute values are given a distributed representation in the free parameters of
the neural controller (Saerens and Soquet, 1991; Schiffman and Geffers, 1993).
The neural controller is thereby enabled to learn the adjustments to its free pa-
rameters directly from the plant.
Beamforming
Beamforming is used to distinguish between the spatial properties of a target signal and
background noise.The device used to do the beamforming is called a beamformer.
The task of beamforming is compatible, for example, with feature mapping in the
cortical layers of auditory systems of echolocating bats (Suga, 1990a; Simmons et al.,
0yk0uj
0yk0uj
J = e 0yk
0uj
f
j, k
Controller
Plant
Σ
Reference
signal
d
Error
signal
e
Plant
input
u
Unity feedback
Plant
output
y


FIGURE 31
Block diagram of feedback control system.

44
Introduction
1992).The echolocating bat illuminates the surrounding environment by broadcasting short-
duration frequency-modulated (FM) sonar signals and then uses its auditory system (in-
cluding a pair of ears) to focus attention on its prey (e.g.,flying insect).The ears provide the
bat with a beamforming capability that is exploited by the auditory system to produce
attentional selectivity.
Beamforming is commonly used in radar and sonar systems where the primary
task is to detect and track a target of interest in the combined presence of receiver
noise and interfering signals (e.g., jammers). This task is complicated by two factors:
• the target signal originates from an unknown direction, and
• there is no prior information available on the interfering signals.
One way of coping with situations of this kind is to use a generalized sidelobe canceller
(GSLC), the block diagram of which is shown in Fig. 32.The system consists of the fol-
lowing components (Griffiths and Jim, 1982; Haykin, 2002):
• An array of antenna elements,which provides a means of sampling the observation-
space signal at discrete points in space.
• A linear combiner defined by a set of fixed weights 
the output of which
performs the role of a desired response. This linear combiner acts like a “spa-
tial filter,” characterized by a radiation pattern (i.e., a polar plot of the ampli-
tude of the antenna output versus the incidence angle of an incoming signal).
The mainlobe of this radiation pattern is pointed along a prescribed direction,
for which the GSLC is constrained to produce a distortionless response. The
output of the linear combiner, denoted by d(n), provides a desired response for
the beamformer.
• A signal-blocking matrix Ca, the function of which is to cancel interference that
leaks through the sidelobes of the radiation pattern of the spatial filter represent-
ing the linear combiner.
{wi}i=1
m ,
Signal-
blocking
matrix
Ca
Neural
network
Desired
response
d(n)
u1(n)
u2(n)
um(n)
Error
signal
e(n)
Output
y(n)
x(n)
Σ
Σ
w2
w1
wm
•
•
•
•
•
•
•
•
•


Linear combiner
Inputs
FIGURE 32
Block diagram of generalized sidelobe canceller.

Section 10
Concluding Remarks
45
• A neural network with adjustable parameters, which is designed to accommodate
statistical variations in the interfering signals.
The adjustments to the free parameters of the neural network are performed by an
error-correcting learning algorithm that operates on the error signal e(n), defined as the
difference between the linear combiner output d(n) and the actual output y(n) of the
neural network.Thus the GSLC operates under the supervision of the linear combiner
that assumes the role of a “teacher.”As with ordinary supervised learning, notice that
the linear combiner is outside the feedback loop acting on the neural network.A beam-
former that uses a neural network for learning is called a neuro-beamformer. This class
of learning machines comes under the general heading of attentional neurocomputers
(Hecht-Nielsen, 1990).
10
CONCLUDING REMARKS
In the material covered in this introductory chapter, we have focused attention on neur-
al networks, the study of which is motivated by the human brain. The one important
property of neural networks that stands out is that of learning, which is categorized as
follows:
(i) supervised learning, which requires the availability of a target or desired response
for the realization of a specific input–output mapping by minimizing a cost func-
tion of interest;
(ii) unsupervised learning, the implementation of which relies on the provision of a
task-independent measure of the quality of representation that the network is re-
quired to learn in a self-organized manner;
(iii) reinforcement learning, in which input–output mapping is performed through the
continued interaction of a learning system with its environment so as to minimize
a scalar index of performance.
Supervised learning relies on the availability of a training sample of labeled
examples, with each example consisting of an input signal (stimulus) and the corre-
sponding desired (target) response. In practice, we find that the collection of labeled
examples is a time-consuming and expensive task, especially when we are dealing with
large-scale learning problems; typically, we therefore find that labeled examples are in
short supply. On the other hand, unsupervised learning relies solely on unlabeled ex-
amples, consisting simply of a set of input signals or stimuli, for which there is usually a
plentiful supply. In light of these realities, there is a great deal of interest in another cat-
egory of learning: semisupervised learning, which employs a training sample that consists
of labeled as well as unlabeled examples.The challenge in semisupervised learning, dis-
cussed in a subsequent chapter, is to design a learning system that scales reasonably well
for its implementation to be practically feasible when dealing with large-scale pattern-
classification problems.
Reinforcement learning lies between supervised learning and unsupervised
learning. It operates through continuing interactions between a learning system
(agent) and the environment.The learning system performs an action and learns from

46
Introduction
the response of the environment to that action. In effect, the role of the teacher in
supervised learning is replaced by a critic, for example, that is integrated into the
learning machinery.
NOTES AND REFERENCES
1. This definition of a neural network is adapted from Aleksander and Morton (1990).
2. For a readable account of computational aspects of the brain,see Churchland and Sejnowski
(1992). For more detailed descriptions, see Kandel et al. (1991), Shepherd (1990), Kuffler
et al. (1984), and Freeman (1975).
3. For detailed treatment of spikes and spiking neurons, see Rieke et al. (1997). For a
biophysical perspective of computation and information-processing capability of single
neurons, see Koch (1999).
4. For a thorough account of sigmoid functions and related issues, see Mennon et al.
(1996).
5. The logistic function, or more precisely, the logistic distribution function, derives its name
from a transcendental “law of logistic growth” that has a huge literature. Measured in
appropriate units, all growth processes are supposed to be represented by the logistic
distribution function
where t represents time, and 
and 
are constants.
6. According to Kuffler et al. (1984), the term “receptive field” was coined originally by
Sherrington (1906) and reintroduced by Hartline (1940). In the context of a visual system,
the receptive field of a neuron refers to the restricted area on the retinal surface, which
influences the discharges of that neuron due to light.
7. The weight-sharing technique was originally described in Rumelhart et al. (1986b).


F(t) =
1
1 + et-

ORGANIZATION OF THE CHAPTER
The perceptron occupies a special place in the historical development of neural net-
works: It was the first algorithmically described neural network. Its invention by
Rosenblatt, a psychologist, inspired engineers, physicists, and mathematicians alike to
devote their research  effort to different aspects of neural networks in the 1960s and
the 1970s. Moreover, it is truly remarkable to find that the perceptron (in its basic form
as described in this chapter) is as valid today as it was in 1958 when Rosenblatt’s paper
on the perceptron was first published.
The chapter is organized as follows:
1. Section 1.1 expands on the formative years of neural networks, going back to the
pioneering work of McCulloch and Pitts in 1943.
2. Section 1.2 describes Rosenblatt’s perceptron in its most basic form. It is followed by
Section 1.3 on the perceptron convergence theorem. This theorem proves conver-
gence of the perceptron as a linearly separable pattern classifier in a finite number
time-steps.
3. Section 1.4 establishes the relationship between the perceptron and the Bayes clas-
sifier for a Gaussian environment.
4. The experiment presented in Section 1.5 demonstrates the pattern-classification
capability of the perceptron.
5. Section 1.6 generalizes the discussion by introducing the perceptron cost function,
paving the way for deriving the batch version of the perceptron convergence
algorithm.
Section 1.7 provides a summary and discussion that conclude the chapter.
1.1
INTRODUCTION
In the formative years of neural networks (1943–1958), several researchers stand out
for their pioneering contributions:
• McCulloch and Pitts (1943) for introducing the idea of neural networks as com-
puting machines.
47
C H A P T E R  1
Rosenblatt’s Perceptron

• Hebb (1949) for postulating the first rule for self-organized learning.
• Rosenblatt (1958) for proposing the perceptron as the first model for learning
with a teacher (i.e., supervised learning).
The idea of Hebbian learning will be discussed at some length in Chapter 8. In this
chapter, we discuss Rosenblatt’s perceptron.
The perceptron is the simplest form of a neural network used for the classifi-
cation of patterns said to be linearly separable (i.e., patterns that lie on opposite
sides of a hyperplane). Basically, it consists of a single neuron with adjustable synap-
tic weights and bias. The algorithm used to adjust the free parameters of this neural
network first appeared in a learning procedure developed by Rosenblatt (1958, 1962)
for his perceptron brain model.1 Indeed, Rosenblatt proved that if the patterns (vec-
tors) used to train the perceptron are drawn from two linearly separable classes,
then the perceptron algorithm converges and positions the decision surface in the
form of a hyperplane between the two classes. The proof of convergence of the al-
gorithm is known as the perceptron convergence theorem.
The perceptron built around a single neuron is limited to performing pattern
classification with only two classes (hypotheses). By expanding the output (compu-
tation) layer of the perceptron to include more than one neuron, we may corre-
spondingly perform classification with more than two classes. However, the classes
have to be linearly separable for the perceptron to work properly. The important
point is that insofar as the basic theory of the perceptron as a pattern classifier is con-
cerned, we need consider only the case of a single neuron.The extension of the the-
ory to the case of more than one neuron is trivial.
1.2
PERCEPTRON
Rosenblatt’s perceptron is built around a nonlinear neuron, namely, the McCulloch–Pitts
model of a neuron. From the introductory chapter we recall that such a neural modeling
consists of a linear combiner followed by a hard limiter (performing the signum func-
tion), as depicted in Fig. 1.1. The summing node of the neural model computes a lin-
ear combination of the inputs applied to its synapses,as well as incorporates an externally
applied bias. The resulting sum, that is, the induced local field, is applied to a hard
48
Chapter 1
Rosenblatt’s Perceptron
Inputs
Hard
limiter
Bias b
x1
x2
xm
wm
w2
w1
w()
v
Output
y
•
•
•
FIGURE 1.1
Signal-flow
graph of the perceptron.

limiter. Accordingly, the neuron produces an output equal to 1 if the hard limiter
input is positive, and -1 if it is negative.
In the signal-flow graph model of Fig. 1.1, the synaptic weights of the perceptron
are denoted by w1, w2, ..., wm. Correspondingly, the inputs applied to the perceptron are
denoted by x1, x2, ..., xm. The externally applied bias is denoted by b. From the model,
we find that the hard limiter input, or induced local field, of the neuron is
(1.1)
The goal of the perceptron is to correctly classify the set of externally applied stimuli x1,
x2, ..., xm into one of two classes, c1 or c2. The decision rule for the classification is to as-
sign the point represented by the inputs x1, x2, ..., xm to class c1if the perceptron output
y is +1 and to class c2 if it is -1.
To develop insight into the behavior of a pattern classifier, it is customary to plot
a map of the decision regions in the m-dimensional signal space spanned by the m input
variables x1, x2, ..., xm. In the simplest form of the perceptron, there are two decision re-
gions separated by a hyperplane, which is defined by
v = a
m
i=1
wixi + b
Section 1.3
The Perceptron Convergence Theorem
49
x2
x1
0
Class 2
Decision boundary
w1x1  w2x2  b  0
Class 1
FIGURE 1.2
Illustration of the
hyperplane (in this example, a
straight line) as decision boundary
for a two-dimensional, two-class
pattern-classification problem.
(1.2)
This is illustrated in Fig. 1.2 for the case of two input variables x1 and x2, for which the
decision boundary takes the form of a straight line. A point (x1, x2) that lies above the
boundary line is assigned to class c1,and a point (x1, x2) that lies below the boundary line
is assigned to class c2. Note also that the effect of the bias b is merely to shift the deci-
sion boundary away from the origin.
The synaptic weights w1,w2,...,wm of the perceptron can be adapted on an iteration-
by-iteration basis. For the adaptation, we may use an error-correction rule known as the
perceptron convergence algorithm, discussed next.
a
m
i=1
wixi + b = 0

1.3
THE PERCEPTRON CONVERGENCE THEOREM
To derive the error-correction learning algorithm for the perceptron, we find it more
convenient to work with the modified signal-flow graph model in Fig. 1.3. In this second
model, which is equivalent to that of Fig. 1.1, the bias b(n) is treated as a synaptic weight
driven by a fixed input equal to 1.We may thus define the (m  1)-by-1 input vector
where n denotes the time-step in applying the algorithm. Correspondingly, we define
the (m + 1)-by-1 weight vector as
Accordingly, the linear combiner output is written in the compact form
(1.3)
where, in the first line, w0(n), corresponding to i  0, represents the bias b. For fixed n,
the equation wTx = 0, plotted in an m-dimensional space (and for some prescribed bias)
with coordinates x1, x2, ..., xm, defines a hyperplane as the decision surface between two
different classes of inputs.
For the perceptron to function properly, the two classes c1 and c2must be linearly
separable. This, in turn, means that the patterns to be classified must be sufficiently sep-
arated from each other to ensure that the decision surface consists of a hyperplane.This
requirement is illustrated in Fig.1.4 for the case of a two-dimensional perceptron.In Fig.
1.4a, the two classes c1 and c2 are sufficiently separated from each other for us to draw
a hyperplane (in this case, a striaght line) as the decision boundary. If, however, the two
classes c1 and c2 are allowed to move too close to each other, as in Fig. 1.4b, they be-
come nonlinearly separable, a situation that is beyond the computing capability of the
perceptron.
Suppose then that the input variables of the perceptron originate from two lin-
early separable classes. Let h1 be the subspace of training vectors x1(1), x1(2), ... that be-
long to class c1, and let h2 be the subspace of training vectors x2(1), x2(2), ... that belong
to class c2.The union of h1 and h2 is the complete space denoted by h. Given the sets
= wT(n)x(n)
v(n) = a
m
i=0
wi(n)xi(n)
w(n) = [b, w1(n), w2(n), ..., wm(n)]T
x(n) = [+1, x1(n), x2(n), ..., xm(n)]T
50
Chapter 1
Rosenblatt’s Perceptron
Inputs
Hard
limiter
Linear
combiner
Fixed
input
x1
x2
xm
wm
w2
w1
x0  1
w0  b
w()
Output
y
v
•
•
•
FIGURE 1.3
Equivalent signal-flow graph
of the perceptron; dependence on time has
been omitted for clarity.

of vectors h1 and h2 to train the classifier, the training process involves the adjustment
of the weight vector w in such a way that the two classes c1 and c2 are linearly separa-
ble.That is, there exists a weight vector w such that we may state
(1.4)
In the second line of Eq. (1.4), we have arbitrarily chosen to say that the input vector x
belongs to class c2 if wTx  0. Given the subsets of training vectors h1 and h2, the train-
ing problem for the perceptron is then to find a weight vector w such that the two in-
equalities of Eq. (1.4) are satisfied.
The algorithm for adapting the weight vector of the elementary perceptron may
now be formulated as follows:
1. If the nth member of the training set, x(n), is correctly classified by the weight
vector w(n) computed at the nth iteration of the algorithm, no correction is made to the
weight vector of the perceptron in accordance with the rule:
(1.5)
2. Otherwise, the weight vector of the perceptron is updated in accordance with
the rule
(1.6)
where the learning-rate parameter η(n) controls the adjustment applied to the weight vec-
tor at iteration n.
If (n)   > 0, where  is a constant independent of the iteration number n, then
we have a fixed-increment adaptation rule for the perceptron.
In the sequel, we first prove the convergence of a fixed-increment adaptation rule
for which η  1. Clearly, the value of η is unimportant, so long as it is positive.A value
w(n + 1) = w(n) + (n)x(n)   if wT(n)x(n)  0 and x(n) belongs to class c1
w(n + 1) = w(n) - (n)x(n)   if wT(n)x(n) 7 0 and x(n) belongs to class c2
w(n + 1) = w(n)   if wT(n)x(n)  0 and x(n) belongs to class c2
w(n + 1) = w(n)   if wT(n)x(n) 7 0 and x(n) belongs to class c1
wTx  0 for every input vector x belonging to class c2
wTx 7 0 for every input vector x belonging to class c1
Section 1.3
The Perceptron Convergence Theorem
51
(a)
(b)
Decision
Boundary
Class 1
Class 2
Class 1
Class 2
FIGURE 1.4
(a) A pair of linearly separable patterns. (b) A pair of non-linearly separable
patterns.

of 
merely scales the pattern vectors without affecting their separability.The case
of a variable η(n) is considered later.
Proof of the perceptron convergence algorithm2 is presented for the initial condi-
tion w(0)  0. Suppose that wT(n)x(n) < 0 for n  1, 2, ..., and the input vector x(n)
belongs to the subset h1. That is, the perceptron incorrectly classifies the vectors x(1),
x(2), ..., since the first condition of Eq. (1.4) is violated. Then, with the constant (n)
 1, we may use the second line of Eq. (1.6) to write
(1.7)
Given the initial condition w(0) = 0, we may iteratively solve this equation for w(n + 1),
obtaining the result
(1.8)
Since the classes 
and 
are assumed to be linearly separable, there exists a solution
wo for which wTx(n) > 0 for the vectors x(1), ..., x(n) belonging to the subset h1. For a
fixed solution wo, we may then define a positive number  as
(1.9)
Hence, multiplying both sides of Eq. (1.8) by the row vector wT
o, we get
Accordingly, in light of the definition given in Eq. (1.9), we have
(1.10)
Next we make use of an inequality known as the Cauchy–Schwarz inequality. Given
two vectors w0 and w(n + 1), the Cauchy–Schwarz inequality states that
(1.11)
where 
denotes the Euclidean norm of the enclosed argument vector, and the inner
@ @@ @
@ @wo@ @ 2@ @ w(n + 1)@ @ 2  [wo
Tw(n + 1)]2
wT
ow(n + 1)  n
wT
ow(n + 1) = wT
ox(1) + wT
ox(2) + p + wT
ox(n)
 =
min
x(n)h1
wT
ox(n)
c2
c1
w(n + 1) = x(1) + x(2) + p + x(n)
w(n + 1) = w(n) + x(n)  for x(n) belonging to class c1
 Z 1
52
Chapter 1
Rosenblatt’s Perceptron
product wT
ow(n + 1) is a scalar quantity.We now note from Eq. (1.10) that [wT
ow(n  1)]2
is equal to or greater than n22. From Eq. (1.11) we note that 
is equal
to or greater than [wT
ow(n + 1)]2. It follows therefore that
or, equivalently,
(1.12)
We next follow another development route.In particular,we rewrite Eq.(1.7) in the form
(1.13)
By taking the squared Euclidean norm of both sides of Eq. (1.13), we obtain
(1.14)
@ @w(k + 1)@ @ 2 = @ @w(k)@ @ 2 + @ @x(k)@ @ 2 + 2wT(k)x(k)
w(k + 1) = w(k) + x(k)  for k = 1, ..., n and x(k)  h1
@ @w(n + 1)@ @ 2  n22
@ @wo@ @ 2
@ @wo@ @ 2@ @w(n + 1)@ @ 2  n22
@ @wo@ @ 2@ @w(n + 1)@ @ 2

But, wT(k)x(k)  0.We therefore deduce from Eq. (1.14) that
or, equivalently,
(1.15)
Adding these inequalities for k = 1, ..., n, and invoking the assumed initial condition
w(0) = 0, we get the inequality
(1.16)
where  is a positive number defined by
(1.17)
Equation (1.16) states that the squared Euclidean norm of the weight vector w(n  1)
grows at most linearly with the number of iterations n.
The second result of Eq. (1.16) is clearly in conflict with the earlier result of
Eq. (1.12) for sufficiently large values of n. Indeed, we can state that n cannot be larger
than some value nmax for which Eqs. (1.12) and (1.16) are both satisfied with the equality
sign.That is, nmax is the solution of the equation
Solving for nmax, given a solution vector wo, we find that
(1.18)
We have thus proved that for η(n) = 1 for all n and w(0) = 0, and given that a solution
vector wo exists, the rule for adapting the synaptic weights of the perceptron must ter-
minate after at most nmax interations. Surprisingly, this statement, proved for hypothe-
sis h1, also holds for huypothesis h2. Note however,
We may now state the fixed-increment covergence theorem for the perceptron as
follows (Rosenblatt, 1962):
Let the subsets of training vectors h1 and h2 be linearly separable. Let the inputs presented
to the perceptron originate from these two subsets. The perceptron converges after some
no iterations, in the sense that
is a solution vector for n0  nmax.
Consider next the absolute error-correction procedure for the adaptation of a single-
layer perceptron, for which (n) is variable. In particular, let (n) be the smallest integer
for which the condition
(n)xT(n)x(n) 7 @wT(n)x(n)@
w(no) = w(no + 1) = w(no + 2) = p
nmax =
@ @wo@ @ 2
2
n2
max2
@ @wo@ @ 2 = nmax
 =
max
x(k)h1 @ @x(k)@ @ 2
 n
@ @w(n + 1)@ @ 2  a
n
k=1
@ @x(k)@ @ 2
@ @w(k + 1)@ @ 2 - @ @w(k)@ @ 2  @ @x(k)@ @ 2,  k = 1, ..., n
@ @w(k + 1)@ @ 2  @ @w(k)@ @ 2 + @ @x(k)@ @ 2
Section 1.3
The Perceptron Convergence Theorem
53

holds.With this procedure we find that if the inner product wT(n)x(n) at iteration n has
an incorrect sign, then wT(n + 1)x(n) at iteration n + 1 would have the correct sign.This
suggests that if wT(n)x(n) has an incorrect sign, at iteration n, we may modify the train-
ing sequence at iteration n + 1 by setting x(n + 1) = x(n). In other words, each pattern is
presented repeatedly to the perceptron until that pattern is classified correctly.
Note also that the use of an initial value w(0) different from the null condition
merely results in a decrease or increase in the number of iterations required to converge,
depending on how w(0) relates to the solution wo. Regardless of the value assigned to
w(0), the perceptron is assured of convergence.
In Table 1.1, we present a summary of the perceptron convergence algorithm
(Lippmann, 1987). The symbol sgn(?), used in step 3 of the table for computing the
actual response of the perceptron, stands for the signum function:
(1.19)
We may thus express the quantized response y(n) of the perceptron in the compact form
(1.20)
Notice that the input vector x(n) is an (m + 1)-by-1 vector whose first element is fixed
at +1 throughout the computation. Correspondingly, the weight vector w(n) is an
y(n) = sgn[wT(n)x(n)]
sgn(v) = e+1   if v 7 0
-1   if v 6 0
54
Chapter 1
Rosenblatt’s Perceptron
TABLE 1.1
Summary of the Perceptron Convergence Algorithm
Variables and Parameters:
x(n) = (m + 1)-by-1 input vector
= [+1, x1(n), x2(n), ..., xm(n)]T
w(n) = (m + 1)-by-1 weight vector
= [b, w1(n), w2(n), ..., wm(n)]T
b = bias
y(n) = actual response (quantized)
d(n) = desired response
 = learning-rate parameter, a positive constant less than unity
1. Initialization. Set w(0) = 0. Then perform the following computations for time-step n = 1, 2, ....
2. Activation. At time-step n, activate the perceptron by applying continuous-valued input vector x(n) and desired
response d(n).
3. Computation of Actual Response. Compute the actual response of the perceptron as
where sgn(·) is the signum function.
4. Adaptation of Weight Vector. Update the weight vector of the perceptron to obtain
where
5. Continuation. Increment time step n by one and go back to step 2.
d(n) = e+1 if x(n) belongs to class c1
-1 if x(n) belongs to class c2
w(n + 1) = w(n) + [d(n) - y(n)]x(n)
y(n) = sgn[wT(n)x(n)]

(m + 1)-by-1 vector whose first element equals the bias b.One other important point to note
in Table 1.1 is that we have introduced a quantized desired response d(n), defined by
(1.21)
Thus, the adaptation of the weight vector w(n) is summed up nicely in the form of the
error-correction learning rule
(1.22)
where  is the learning-rate parameter and the difference d(n) - y(n) plays the role of
an error signal. The learning-rate parameter is a positive constant limited to the range
0 <   1.When assigning a value to it inside this range, we must keep in mind two con-
flicting requirements (Lippmann, 1987):
• averaging of past inputs to provide stable weight estimates, which requires a
small ;
• fast adaptation with respect to real changes in the underlying distributions of the
process responsible for the generation of the input vector x, which requires a large .
1.4
RELATION BETWEEN THE PERCEPTRON AND BAYES CLASSIFIER
FOR A GAUSSIAN ENVIRONMENT
The perceptron bears a certain relationship to a classical pattern classifier known as the
Bayes classifier. When the environment is Gaussian, the Bayes classifier reduces to a
linear classifier.This is the same form taken by the perceptron. However, the linear na-
ture of the perceptron is not contingent on the assumption of Gaussianity. In this sec-
tion, we study this relationship and thereby develop further insight into the operation
of the perceptron.We begin the discussion with a brief review of the Bayes classifier.
Bayes Classifier
In the Bayes classifier, or Bayes hypothesis testing procedure, we minimize the average
risk, denoted by r. For a two-class problem, represented by classes c1 and c2, the av-
erage risk is defined by Van Trees (1968) as
(1.23)
where the various terms are defined as follows:
 prior probability that the observation vector x (representing a realiza-
tion of the random vector X) corresponds to an object in class C1, with
i  1, 2, and p1 + p2 = 1
pi
+ c21p1 3h2
pX(x@c1)dx + c12p2 3h1
pX(x@c2)dx
r = c11p1 3h1
pX(x@c1)dx + c22p2 3h2
pX(x@c2)dx
w(n + 1) = w(n) + [d(n) - y(n)]x(n)
d(n) = e+1    if x(n) belongs to class c1
-1    if x(n) belongs to class c2
Section 1.4
Relation Between the Perceptron and Bayes Classifier 
55

 cost of deciding in favor of class ci represented by subspace hi when
class cj is true (i.e., observation vector x corresponds to an object in
class C1), with i, j  1, 2
 conditional probability density function of the random vector X,given that
the observation vector x corresponds to an object in class C1, with i  1, 2.
The first two terms on the right-hand side of Eq. (1.23) represent correct decisions
(i.e., correct classifications), whereas the last two terms represent incorrect decisions
(i.e., misclassifications). Each decision is weighted by the product of two factors: the
cost involved in making the decision and the relative frequency (i.e., prior probability)
with which it occurs.
The intention is to determine a strategy for the minimum average risk. Because we
require that a decision be made, each observation vector x must be assigned in the over-
all observation space 
to either 
or 
.Thus,
(1.24)
Accordingly, we may rewrite Eq. (1.23) in the equivalent form
(1.25)
where c11 < c21 and c22 < c12.We now observe the fact that
(1.26)
Hence, Eq. (1.25) reduces to
(1.27)
The first two terms on the right-hand side of Eq.(1.27) represent a fixed cost. Since
the requirement is to minimize the average risk 
, we may therefore deduce the fol-
lowing strategy from Eq.(1.27) for optimum classification:
1. All values of the observation vector x for which the integrand (i.e., the ex-
pression inside the square brackets) is negative should be assigned to subset 
(i.e., class 
), for the integral would then make a negative contribution to the
risk 
.
2. All values of the observation vector x for which the integrand is positive should be
excluded from subset 
(i.e., assigned to class 
), for the integral would then
make a positive contribution to the risk 
.
3. Values of x for which the integrand is zero have no effect on the average risk 
and
may be assingned arbitrarily. We shall assume that these points are assigned to
subset 
(i.e., class 
).
c2
x2
r
r
c2
x1
r
c1
x1
r
+
3x1
[p2(c12 - c22) pX(xc2) - p1(c21 - c11) pX(xc1)]dx
r = c21p1 + c22p2
3x
pX(xc1)dx =
3x
pX(xc2)dx = 1
+ c21p1 3x-x1
pX(xc1)x + c12p2 3x1
pX(xc2)dx
r = c11p1 3x1
pX(xc1)dx + c22p2 3x-x1
pX(xc2)dx
x = x1 + x2
x2
x1
x
pX(xci)
cij
56
Chapter 1
Rosenblatt’s Perceptron

On this basis, we may now formulate the Bayes classifier as follows:
If the condition
holds, assign the observation vector x to subspace 
(i.e., class 
). Otherwise assign x to
(i.e., class 
).
To simplify matters, define
(1.28)
and
(1.29)
The quantity 
,the ratio of two conditional probability density functions,is called the
likelihood ratio. The quantity is called the threshold of the test. Note that both 
and are always positive. In terms of these two quantities, we may now reformulate the
Bayes classifier by stating the following
If, for an observation vector x, the likelihood ratio 
is greater than the threshold , assign
x to class 
. Otherwise, assign it to class 
.
Figure 1.5a depicts a block-diagram representation of the Bayes classifier. The
important points in this block diagram are twofold:
1. The data processing involved in designing the Bayes classifier is confined entirely
to the computation of the likelihood ratio 
.
¶(x)
c2
c1

¶(x)

¶(x)

¶(x)
 =
p2(c12 - c22)
p1(c21 - c11)
¶(x) =
pX(xc1)
pX(xc2)
c2
x2
c1
x1
p1(c21 - c11) pX(xc1) 7 p2(c12 - c22) pX(xc2)
Section 1.4
Relation Between the Perceptron and Bayes Classifier 
57
FIGURE 1.5
Two equivalent implementations of the Bayes classifier: (a) Likelihood ratio
test, (b) Log-likelihood ratio test.
Assign x to class 1
if Λ(x)  Ô.
Otherwise, assign
it to class 2.
Comparator
Likelihood
ratio
computer
Λ(x)
(a)
(b)
x
Input vector
Input vector

Assign x to class 1
if logΛ(x)  logÔ.
Otherwise, assign
it to class 2.
Comparator
Log-likelihood
ratio
computer
logΛ(x)
x
logÔ

2. This computation is completely invariant to the values assigned to the prior prob-
abilities and costs involved in the decision-making process.These quantities merely
affect the value of the threshold .
From a computational point of view, we find it more convenient to work with
the logarithm of the likelihood ratio rather than the likelihood ratio itself. We are
permitted to do this for two reasons. First, the logarithm is a monotonic function.
Second, the likelihood ratio 
and threshold 
are both positive. Therefore, the
Bayes classifier may be implemented in the equivalent form shown in Fig. 1.5b. For
obvious reasons, the test embodied in this latter figure is called the log-likelihood
ratio test.
Bayes Classifier for a Gaussian Distribution
Consider now the special case of a two-class problem, for which the underlying distrib-
ution is Gaussian. The random vector X has a mean value that depends on whether it
belongs to class 
or class 
, but the covariance matrix of X is the same for both classes.
That is to say,
The covariance matrix C is nondiagonal, which means that the samples drawn from
classes 
and 
are correlated. It is assumed that C is nonsingular, so that its inverse
matrix C1 exists.
With this background, we may express the conditional probability density function
of X as the multivariate Gaussian distribution
(1.30)
where m is the dimensionality of the observation vector x.
We further assume the following:
1. The two classes 
and 
are equiprobable:
(1.31)
2. Misclassifications carry the same cost, and no cost is incurred on correct classifi-
cations:
(1.32)
We now have the information we need to design the Bayes classifier for the two-
class problem. Specifically, by substituting Eq. (1.30) into (1.28) and taking the natural
logarithm, we get (after simplifications)
c21 = c12 
and 
c11 = c22 = 0
p1 = p2 = 1
2
c2
c1
pX(xci) =
1
(2)m2(det(C))12 exp a - 1
2 (x - i)T C-1(x - i) b, i = 1, 2
c2
c1
[(X - 2)(X - 2)T] = C
Class c2: [X] = 2
[(X - 1)(X - 1)T] = C
Class c1: [X] = 1
c2
c1

¶(x)

58
Chapter 1
Rosenblatt’s Perceptron

(1.33)
By substituting Eqs. (1.31) and (1.32) into Eq. (1.29) and taking the natural logarithm,
we get
(1.34)
Equations (1.33) and (1.34) state that the Bayes classifier for the problem at hand is a
linear classifier, as described by the relation
(1.35)
where
(1.36)
(1.37)
(1.38)
More specifically, the classifier consists of a linear combiner with weight vector w and
bias b, as shown in Fig. 1.6.
On the basis of Eq. (1.35), we may now describe the log-likelihood ratio test for
our two-class problem as follows:
If the output y of the linear combiner (including the bias b) is positive, assign the observation
vector x to class 
. Otherwise, assign it to class 
.
The operation of the Bayes classifier for the Gaussian environment described
herein is analogous to that of the perceptron in that they are both linear classifiers; see
Eqs. (1.1) and (1.35). There are, however, some subtle and important differences be-
tween them, which should be carefully examined (Lippmann, 1987):
• The perceptron operates on the premise that the patterns to be classified are
linearly separable. The Gaussian distributions of the two patterns assumed in the
derivation of the Bayes classifier certainly do overlap each other and are there-
fore not separable.The extent of the overlap is determined by the mean vectors
c2
c1
b = 1
2 (2
TC-12 - 1
TC-11)
 w = C -1(1 - 2)
y = log¶(x)
y = wTx + b
log = 0
= (1 - 2)TC-1x + 1
2 (T
2C-12 - 1
TC-11)
 log¶(x) = - 1
2 (x - 1)TC-1(x - 1) + 1
2 (x - 2)TC-1(x - 2)
Section 1.4
Relation Between the Perceptron and Bayes Classifier 
59
FIGURE 1.6
Signal-flow
graph of Gaussian classifier.
x
Bias b
x1
x2
xm
wm
w2
w1
Output
y
•
•
•

1 and 2 and the covariance matrix C. The nature of this overlap is illustrat-
ed in Fig. 1.7 for the special case of a scalar random variable (i.e., dimension-
ality m = 1). When the inputs are nonseparable and their distributions overlap
as illustrated, the perceptron convergence algorithm develops a problem be-
cause decision boundaries between the different classes may oscillate contin-
uously.
• The Bayes classifier minimizes the probability of classification error. This mini-
mization is independent of the overlap between the underlying Gaussian distrib-
utions of the two classes. For example, in the special case illustrated in Fig. 1.7, the
Bayes classifier always positions the decision boundary at the point where the
Gaussian distributions for the two classes 
and 
cross each other.
• The perceptron convergence algorithm is nonparametric in the sense that it makes
no assumptions concerning the form of the underlying distributions. It operates
by concentrating on errors that occur where the distributions overlap. It may
therefore work well when the inputs are generated by nonlinear physical mech-
anisms and when their distributions are heavily skewed and non-Gaussian. In
contrast, the Bayes classifier is parametric; its derivation is contingent on the as-
sumption that the underlying distributions be Gaussian, which may limit its area
of application.
• The perceptron convergence algorithm is both adaptive and simple to imple-
ment; its storage requirement is confined to the set of synaptic weights and bias.
On the other hand, the design of the Bayes classifier is fixed; it can be made adap-
tive, but at the expense of increased storage requirements and more complex
computations.
1.5
COMPUTER EXPERIMENT: PATTERN CLASSIFICATION
The objective of this computer experiment is twofold:
(i) to lay down the specifications of a double-moon classification problem that will
serve as the basis of a prototype for the part of the book that deals with pattern-
classification experiments;
c2
c1
60
Chapter 1
Rosenblatt’s Perceptron
0
μ2
μ1
Class
2
Class
1
pX(x|1)
pX(x|2)
Decision
boundary
x
FIGURE 1.7
Two overlapping,
one-dimensional Gaussian
distributions.

(ii) to demonstrate the capability of Rosenblatt’s perceptron algorithm to correctly
classify linearly separable patterns and to show its breakdown when the condition
of linear separability is violated.
Specifications of the Classification Problem
Figure 1.8 shows a pair of “moons” facing each other in an asymmetrically arranged
manner.The moon labeled “Region A” is positioned symmetrically with respect to the 
y-axis, whereas the moon labeled “Region B” is displaced to the right of the y-axis by
an amount equal to the radius r and below the x-axis by the distance d. The two moons
have identical parameters:
The vertical distance d separating the two moons is adjustable; it is measured with re-
spect to the x-axis, as indicated in Fig. 1.8:
• Increasingly positive values of d signify increased separation between the two moons;
• increasingly negative values of d signify the two moons’ coming closer to each other.
The training sample t consists of 1,000 pairs of data points, with each pair consisting of
one point picked from region A and another point picked from region B, both randomly.
The test sample consists of 2,000 pairs of data points, again picked in a random manner.
 width of each moon, w = 6
 radius of each moon, r = 10
Section 1.5
Computer Experiment: Pattern Classification
61
x
y
Region B
Region A
w
d
r
FIGURE 1.8
The double-moon classification problem.

The Experiment
The perceptron parameters picked for the experiment were as follows:
The learning-rate parameter η was varied linearly from 10-1 down to 10-5.
The weights were initially all set at zero.
Figure 1.9 presents the results of the experiment for d = 1, which corresponds to
perfect linear separability. Part (a) of the figure presents the learning curve, where the
mean-square error (MSE) is plotted versus the number of epochs; the figure shows con-
vergence of the algorithm in three iterations. Part (b) of the figure shows the decision
boundary computed through training of the perceptron algorithm, demonstrating per-
fect separability of all 2,000 test points.
In Fig. 1.10, the separation between the two moons was set at d = -4, a condition
that violates linear separability. Part (a) of the figure displays the learning curve where
the perceptron algorithm is now found to fluctuate continuously, indicating breakdown
of the algorithm. This result is confirmed in part (b) of the figure, where the decision
boundary (computed through training) intersects both moons, with a classification error
rate of (186/2000)  100% = 9.3%.
1.6
THE BATCH PERCEPTRON ALGORITHM
The derivation of the perceptron convergence algorithm summarized in Table 1.1 was
presented without reference to a cost function. Moreover, the derivation focused on a
single-sample correction. In this section, we will do two things:
1. introduce the generalized form of a perceptron cost function;
2. use the cost function to formulate a batch version of the perceptron convergence
algorithm.
The cost function we have in mind is a function that permits the application of a
gradient search. Specifically, we define the perceptron cost function as
(1.39)
where 
is the set of samples x misclassified by a perceptron using w as its weight vec-
tor (Duda et al., 2001). If all the samples are classified correctly, then the set 
is empty,
in which case the cost function (w) is zero.In any event,the nice feature of the cost func-
tion (w) is that it is differentiable with respect to the weight vector w. Thus, differenti-
ating (w) with respect to w yields the gradient vector
(1.40)
§J(w) =
a
x(n)Hx
(-x(n)d(n))
J
J
J
x
x
J(w) =
a
x(n)Hx
(-wTx(n)d(n))
 = 50; see Eq. (1.17)
size of the input layer = 2
62
Chapter 1
Rosenblatt’s Perceptron

Section 1.6
The Batch Perceptron Algorithm
63
0
10
30
40
20
50
0
0.01
0.02
0.03
0.04
0.05
0.06
MSE
Number of epochs
(a) learning curve
10
10
5
0
5
10
15
20
5
10
5
0
y
x
(b) testing result
Classification using perceptron with distance  1, radius  10, and width  6
FIGURE 1.9
Perceptron with the double-moon set at distance d  1.

64
Chapter 1
Rosenblatt’s Perceptron
0
10
30
40
20
50
0.48
0.46
0.5
0.52
0.56
0.54
0.58
0.6
MSE
Number of epochs
(a) learning curve
8
6
4
10
5
0
5
10
15
20
2
10
12
6
8
0
2
4
y
x
(b) testing result
Classification using perceptron with distance  4, radius  10, and width  6
FIGURE 1.10
Perceptron with the double-moon set at distance d  4.

where the gradient operator
(1.41)
In the method of steepest descent, the adjustment to the weight vector w at each time-
step of the algorithm is applied in a direction opposite to the gradient vector 
.
Accordingly, the algorithm takes the form
(1.42)
which includes the single-sample correction version of the perceptron convergence algo-
rithm as a special case. Moreover, Eq. (1.42) embodies the batch perceptron algorithm for
computing the weight vector,given the sample set x(1),x(2),....In particular,the adjustment
applied to the weight vector at time-step n + 1 is defined by the sum of all the samples mis-
classified by the weight vector w(n), with the sum being scaled by the learning-rate para-
meter η(n).The algorithm is said to be of the “batch”kind because at each time-step of the
algorithm, a batch of misclassified samples is used to compute the adjustment.
1.7
SUMMARY AND DISCUSSION
The perceptron is a single-layer neural network, the operation of which is based on
error-correlation learning.The term “single layer” is used here to signify the fact that the
computation layer of the network consists of a single neuron for the case of binary clas-
sification.The learning process for pattern classification occupies a finite number of it-
erations and then stops. For the classification to be successful, however, the patterns
would have to be linearly separable.
The perceptron uses the McCulloch–Pitts model of a neuron. In this context, it is
tempting to raise the question,would the perceptron perform better if it used a sigmoidal
nonlinearity in place of the hard limiter? It turns out that the steady-state,decision-making
characteristics of the perceptron are basically the same, regardless of whether we use
hard limiting or soft limiting as the source of nonlinearity in the neural model (Shynk,
1990;Shynk and Bershad,1991).We may therefore state formally that so long as we limit
ourselves to the model of a neuron that consists of a linear combiner followed by a non-
linear element,then regardless of the form of nonlinearity used,a single-layer perceptron
can perform pattern classification only on linearly separable patterns.
The first real critique of Rosenblatt’s perceptron was presented by Minsky and
Selfridge (1961). Minsky and Selfridge pointed out that the perceptron as defined by
Rosenblatt could not even generalize toward the notion of binary parity, let alone make
general abstractions. The computational limitations of Rosenblatt’s perceptron were
subsequently put on a solid mathematical foundation in the famous book Perceptrons,
by Minsky and Papert (1969, 1988).After the presentation of some brilliant and highly
detailed mathematical analyses of the perceptron, Minsky and Papert proved that the
perceptron as defined by Rosenblatt is inherently incapable of making some global
= w(n) + (n) a
x(n)Hx
x(n)d(n)
w(n + 1) = w(n) - (n)§J(w)
§J(w)
 = c
0
0w1
,
0
0w2
, ..., 0
0wm
d
T
Section 1.7
Summary and Discussion
65

generalizations on the basis of locally learned examples.In the last chapter of their book,
Minsky and Papert make the conjecture that the limitations they had discovered for
Rosenblatt’s perceptron would also hold true for its variants—more specifically, multi-
layer neural networks. Section 13.2 of their book (1969) says the following:
The perceptron has shown itself worthy of study despite (and even because of!) its severe
limitations. It has many features to attract attention: its linearity; its intriguing learning
theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no
reason to suppose that any of these virtues carry over to the many-layered version. Never-
theless, we consider it to be an important research problem to elucidate (or reject) our in-
tuitive judgement that the extension to multilayer systems is sterile.
This conclusion was largely responsible for casting serious doubt on the computational ca-
pabilities of not only the perceptron,but also neural networks in general up to the mid-1980s.
History has shown,however,that the conjecture made by Minsky and Papert seems
to be unjustified in that we now have several advanced forms of neural networks and
learning machines that are computationally more powerful than Rosenblatt’s perceptron.
For example, multilayer perceptrons trained with the back-propagation algorithm dis-
cussed in Chapter 4, the radial basis-function networks discussed in Chapter 5, and the
support vector machines discussed in Chapter 6 overcome the computational limita-
tions of the single-layer perceptron in their own individual ways.
In closing the discussion, we may say that the perceptron is an elegant neural net-
work designed for the classification of linearly separable patterns. Its importance is not
only historical but also of practical value in the classification of linearly separable patters.
NOTES AND REFERENCES
1. The network organization of the original version of the perceptron as envisioned by
Rosenblatt (1962) has three types of units: sensory units, association units, and response
units.The connections from the sensory units to the association units have fixed weights,
and the connections from the association units to the response units have variable
weights. The association units act as preprocessors designed to extract a pattern from
the environmental input. Insofar as the variable weights are concerned, the operation of
Rosenblatt’s original perceptron is essentially the same as that for the case of a single
response unit (i.e., single neuron).
2. Proof of the perceptron convergence algorithm presented in Section 1.3 follows the
classic book of Nilsson (1965).
PROBLEMS
1.1 Verify that Eqs. (1.19)–(1.22), summarizing the perceptron convergence algorithm, are con-
sistent with Eqs. (1.5) and (1.6).
1.2 Suppose that in the signal-flow graph of the perceptron shown in Fig. 1.1, the hard limiter
is replaced by the sigmoidal nonlinearity
(v) = tanh a v
2 b
66
Chapter 1
Rosenblatt’s Perceptron

where v is the induced local field. The classification decisions made by the perceptron are
defined as follows:
Observation vector x belongs to class 
if the output y > , where is a threshold; otherwise, x
belongs to class 
.
Show that the decision boundary so constructed is a hyperplane.
1.3 (a) The perceptron may be used to perform numerous logic functions. Demonstrate the
implementation of the binary logic functions AND, OR, and COMPLEMENT.
(b) A basic limitation of the perceptron is that it cannot implement the EXCLUSIVE
OR function. Explain the reason for this limitation.
1.4 Consider two one-dimensional, Gaussian-distributed classes 
and 
that have a common
variance equal to 1.Their mean values are
These two classes are essentially linearly separable. Design a classifier that separates these
two classes.
1.5 Equations (1.37) and (1.38) define the weight vector and bias of the Bayes classifier for a
Gaussian environment. Determine the composition of this classifier for the case when the
covariance matrix C is defined by
where 2 is a constant and I is the identity matrix.
Computer Experiment
1.6 Repeat the computer experiment of Section 1.5, this time, however, positioning the two
moons of Figure 1.8 to be on the edge of separability, that is, d = 0. Determine the classifi-
cation error rate produced by the algorithm over 2,000 test data points.
C = 2I
 2 = +10
 1 = -10
c2
c1
c2


c1
Problems
67

68
ORGANIZATION OF THE CHAPTER
The theme of this chapter is how to use linear regression, a special form of function
approximation, to model a given set of random variables.
The chapter is organized as follows:
1. Section 2.1 is introductory, followed by Section 2.2 that sets the stage for the rest of
the chapter by describing the mathematical framework of linear regression models.
2. Section 2.3 derives the maximum a posteriori (MAP) estimate of the parameter vector
of a linear regression model.
3. Section 2.4 tackles the parameter estimation problem using the method of least
squares and discusses this method’s relationship to the Bayesian approach.
4. In Section 2.5, we revisit the pattern-classification experiment considered in 
Chapter 1, this time using the method of least squares.
5. Section 2.6 addresses the problem of model-order selection.
6. Section 2.7 discusses consequences of finite sample size in parameter estimation,
including the bias–variance dilemma.
C H A P T E R  2
Model Building 
through Regression
7. Section 2.8 introduces the notion of instrumental variables to deal with the “errors-
in-variables” problem.
Section 2.9 provides a summary and discussion that conclude the chapter.
2.1
INTRODUCTION
The idea of model building shows up practically in every discipline that deals with statistical
data analysis. Suppose, for example, we are given a set of random variables and the as-
signed task is to find the relationships that may exist between them, if any. In regression,
which is a special form of function approximation,we typically find the following scenario:
• One of the random variables is considered to be of particular interest; that random
variable is referred to as a dependent variable, or response.
• The remaining random variables are called independent variables, or regressors;
their role is to explain or predict the statistical behavior of the response.
• The dependence of the response on the regressors includes an additive error term,
to account for uncertainties in the manner in which this dependence is formulated;

Section 2.2
Linear Regression Model: Preliminary Considerations
69
the error term is called the expectational error,or explanational error,both of which
are used interchangeably.
Such a model is called the regression model.1
There are two classes of regression models: linear and nonlinear. In linear regres-
sion models, the dependence of the response on the regressors is defined by a linear func-
tion, which makes their statistical analysis mathematically tractable. On the other hand,
in nonlinear regression models, this dependence is defined by a nonlinear function,hence
the mathematical difficulty in their analysis. In this chapter, we focus attention on linear
regression models. Nonlinear regression models are studied in subsequent chapters.
The mathematical tractability of linear regression models shows up in this chapter
in two ways. First, we use Bayesian theory2 to derive the maximum a posteriori estimate
of the vector that parameterizes a linear regression model. Next, we view the parameter
estimation problem using another approach, namely, the method of least squares, which
is perhaps the oldest parameter-estimation procedure; it was first derived by Gauss in
the early part of the 19th century.We then demonstrate the equivalence between these
two approaches for the special case of a Gaussian environment.
2.2
LINEAR REGRESSION MODEL: PRELIMINARY CONSIDERATIONS
Consider the situation depicted in Fig. 2.1a, where an unknown stochastic environment
is the focus of attention.The environment is probed by applying a set of inputs, consti-
tuting the regressor
(2.1)
where the superscript T denotes matrix transposition. The resulting output of the envi-
ronment, denoted by d, constitutes the corresponding response, which is assumed to be
scalar merely for the convenience of presentation. Ordinarily, we do not know the func-
tional dependence of the response d on the regressor x, so we propose a linear regres-
sion model, parameterized as:
(2.2)
where w1, w2, ..., wM denote a set of fixed, but unknown, parameters, meaning that the en-
vironment is stationary. The additive term ε, representing the expectational error of the
model, accounts for our ignorance about the environment.A signal-flow graph depiction
of the input–output behavior of the model described in Eq.(2.2) is presented in Fig.2.1b.
Using matrix notation, we may rewrite Eq. (2.2) in the compact form
(2.3)
where the regressor x is defined in terms of its elements in Eq. (2.1). Correspondingly,
the parameter vector w is defined by
(2.4)
w = [w1, w2, ..., wM]T
d = wTx + ε
d = a
M
j=1
wjxj + ε
x = [x1, x2, ..., xM]T

whose dimensionality is the same as that of the regressor x; the common dimension
M is called the model order. The matrix term wTx is the inner product of the vectors w
and x.
With the environment being stochastic, it follows that the regressor x, the response
d, and the expectational error ε are sample values (i.e., single-shot realizations) of the
random vector X, the random variable D, and the random variable E, respectively.With
such a stochastic setting as the background, the problem of interest may now be stated
as follows:
Given the joint statistics of the regressor X and the corresponding response D, estimate the
unknown parameter vector w.
When we speak of the joint statistics, we mean the following set of statistical parameters:
• the correlation matrix of the regressor X;
• the variance of the desired response D;
• the cross-correlation vector of the regressor X and the desired response D.
It is assumed that the means of both X and D are zero.
In Chapter 1, we discussed one important facet of Bayesian inference in the con-
text of pattern classification. In this chapter, we study another facet of Bayesian infer-
ence that addresses the parameter estimation problem just stated.
70
Chapter 2
Model Building through Regression
FIGURE 2.1
(a) Unknown stationary stochastic environment. (b) Linear regression model
of the environment.
Desired response
(Output)
d
Regressor
(Input vector)
x
(a)
(b)
Unknown
stochastic
environment:
w
Expectational
error
Desired response
d
w1
w2
wM
e
xM
x2
x1
Regressor
x
•
•
•
•
•
•

2.3
MAXIMUM A POSTERIORI ESTIMATION OF 
THE PARAMETER VECTOR
The Bayesian paradigm provides a powerful approach for addressing and quantifying
the uncertainty that surrounds the choice of the parameter vector w in the linear re-
gression model of Eq. (2.3). Insofar as this model is concerned, the following two re-
marks are noteworthy:
1. The regressor X acts as the “excitation,” bearing no relation whatsoever to the
parameter vector w.
2. Information about the unknown parameter vector W is contained solely in the
desired response D that acts as the “observable” of the environment.
Accordingly,we focus attention on the joint probablity density function of W and D,con-
ditional on X.
Let this density function be denoted by  pW, D|X(w, d|x). From probability theory,
we know that this density function may be expressed as
(2.5)
Moreover, we may also express it in the equivalent form
(2.6)
In light of this pair of equations, we may go on to write
(2.7)
provided that 
. Equation (2.7) is a special form of Bayes’s theorem; it em-
bodies four density functions, characterized as follows:
1. Observation density:This stands for the conditional probability density function
pD |W, X(d | w, x), referring to the “observation” of the environmental response d
due to the regressor x, given the parameter vector w.
2. Prior: This stands for the probability density function pW(w), referring to infor-
mation about the parameter vector w, prior to any observations made on the en-
vironment. Hereafter, the prior is simply denoted by π(w).
3. Posterior density: This stands for the conditional probability density function
pW|D, X(w| d, x), referring to the parameter vector w “after” observation of the
environment has been completed. Hereafter, the posterior density is denoted by
π(w | d, x). The conditioning response–regressor pair (x, d) is the “observation
model,” embodying the response d of the environment due to the regressor x.
4. Evidence: This stands for the probability density function pD(d), referring to the
“information” contained in the response d for statistical analysis.
pD(d) Z 0
pW D, X(w d,  x) =
pD  W, X(dw, x)pW(w)
pD(d)
pW, D  X(w, dx) = p DW, X(dw, x)pW(w)
pW, D X(w, d x) = p W D, X(wd, x)pD(d)
Section 2.3
Maximum A Posteriori Estimation of the Parameter Vector
71
The observation density pD|W, X(d | w, x) is commonly reformulated mathematically as
the likelihood function, defined by
(2.8)
l(wd, x) = pDW, X(dw, x)

Moreover, insofar as the estimation of the parameter vector w is concerned, the evi-
dence pD(d) in the denominator of the right-hand side of Eq. (2.7) plays merely the role
of a normalizing constant. Accordingly, we may express Eq. (2.7) in words by stating the
following:
The posterior density of the vector w parameterizing the regression model is proportional to
the product of the likelihood function and the prior.
That is,
(2.9)
where the symbol  signifies proportionality.
The likelihood function l(w|d, x), considered on its own, provides the basis for the
maximum-likelihood (ML) estimate of the parameter vector w, as shown by
(2.10)
For a more profound estimate of the parameter vector w, however, we look to
the posterior density (w|d, x). Specifically, we define the maximum a posteriori (MAP)
estimate of the parameter vector w by the formula
(2.11)
We say that the MAP estimator is more profound than the ML estimator for two
important reasons:
1. The Bayesian paradigm for parameter estimation, rooted in the Bayes’ theo-
rem as shown in Eq. (2.7) and exemplified by the MAP estimator of Eq. (2.11),
exploits all the conceivable information about the parameter vector w. In contrast,
the ML estimator of Eq. (2.10) lies on the fringe of the Bayesian paradigm,
ignoring the prior.
2. The ML estimator relies solely on the observation model (d, x) and may there-
fore lead to a nonunique solution. To enforce uniqueness and stability on the
solution, the prior (w) has to be incorporated into the formulation of the estima-
tor; this is precisely what is done in the MAP estimator.
Of course,the challenge in applying the MAP estimation procedure is how to come up with
an appropriate prior, which makes MAP more computationally demanding than ML.
One last comment is in order. From a computational perspective, we usually find
it more convenient to work with the logarithm of the posterior density rather than the
posterior density itself. We are permitted to do this, since the logarithm is a monotoni-
cally increasing function of its argument. Accordingly, we may express the MAP esti-
mator in the desired form by writing
(2.12)
where “log”denotes the natural logarithm.A similar statement applies to the ML estimator.
wMAP = arg max
w  log((wd, x))
wMAP = arg max 
 w (wd, x)
wML = arg max 
w
l(w d, x)
(wd, x) r l(wd, x)(w)
72
Chapter 2
Model Building through Regression

Parameter Estimation in a Gaussian Environment
Let xi and di denote the regressor applied to the environment and the resulting response,
respectively, on the ith trial of an experiment performed on the environment. Let the
experiment be repeated a total of N times. We thus express the training sample, avail-
able for parameter estimation, as
(2.13)
To proceed with the task of parameter estimation,we make the following assumptions:
Assumption 1: Statistical Independence and Identical Distribution
The N examples, constituting the training sample, are statistically independent and iden-
tically distributed (iid).
Assumption 2: Gaussianity 
The environment, responsible for generation of the training sample t, is Gaussian
distributed.
More specifically,the expectational error in the linear regression model of Eq.(2.3)
is described by a Gaussian density function of zero mean and common variance σ2, as
shown by
(2.14)
Assumption 3: Stationarity 
The environment is stationary, which means that the parameter vector w is fixed, but
unknown, throughout the N trials of the experiment.
More specifically, the M elements of the weight vector w are themselves assumed
to be iid, with each element being governed by a Gaussian density function of zero mean
and common variance σ2
w.We may therefore express the prior for the kth element of the
parameter vector w as
(2.15)
Rewriting Eq. (2.3) for the ith trial of the experiment performed on the environ-
ment, we have
(2.16)
where di, xi, and εi are sample values (i.e., single-shot realizations) of the random
variable D, the random vector X, and the random variable E, respectively.
Let 
denote the statistical expectation operator. Since, under Assumption 2,
we have
(2.17)
[Ei] = 0,  for all i 

di = wTxi + εi,  i = 1, 2, ..., N 
(wk) =
1
12 w
 exp a- w2
k
22
w
b,  k = 1, 2, ..., M 
pE(εi) =
1
12  exp a- ε2
i
22 b,  i = 1, 2, ..., N 
t = {xi, di}N
i=1
Section 2.3
Maximum A Posteriori Estimation of the Parameter Vector
73

and
(2.18)
it follows from Eq. (2.16) that, for a given regressor xi,
(2.19)
(2.20)
We thus complete the Gaussian implication of Assumption 2 by expressing the likelihood
function for the ith trial, in light of Eq. (2.14), as
(2.21)
Next, invoking the iid characterization of the N trials of the experiment on the
environment under Assumption 1, we express the overall likelihood function for the
experiment as
(2.22)
which accounts for the total empirical knowledge about the weight vector w contained
in the training sample t of Eq. (2.13).
The only other source of information that remains to be accounted for is that con-
tained in the prior (w). Invoking the zero-mean Gaussian characterization of the kth
element of w described in Eq. (2.15), followed by the iid characterization of the M ele-
ments of w under Assumption 3, we write
(2.23)
where ||w|| is the Euclidean norm of the unknown parameter vector w, defined by
 =
1
(22w)M exp a- 1
22
w
 7w7 2 b 
 =
1
(22w)M exp a- 1
22
w a
M
i=1
w2
k b
 =
1
(22w)M q
M
k=1
exp a- w2
k
22
w
b
 (w) = q
M
k=1
(wk)
 =
1
(22)N exp a - 1
22 a
N
i=1
(di - wTxi)2 b 
 =
1
(22)N q
N
i=1
exp a- 1
22 (di - wTxi)2 b
 l(wd, x) = q
N
i=1
l(wdi, xi)
l(wdi, xi) =
1
12  exp a- 1
22 (di - wTxi)2 b ,  i = 1, 2, ..., N
 = 2
 = [E2
i]
 var[Di] = [(Di - [Di])2]
 [Di] = wTxi,  i = 1, 2, ..., N 
var[Ei] = [E2
i] = 2,  for all i 
74
Chapter 2
Model Building through Regression

(2.24)
Hence, substituting Eqs. (2.22) and (2.23) into Eq. (2.9), and then simplifying the result,
we get the posterior density
(2.25)
We are now fully equipped to apply the MAP formula of Eq. (2.12) to the esti-
mation problem at hand. Specifically, substituting Eq. (2.25) into this formula, we get
(2.26)
where we have introduced the new parameter
(2.27)
Now we define the quadratic function
(2.28)
Clearly, maximization of the argument in Eq. (2.26) with respect to w is equivalent to
minimization of the quadratic function e(w).Accordingly, the optimum estimate
is obtained by differentiating the function e(w) with respect to w and setting the result
equal to zero. In so doing, we obtain the desired MAP estimate of the M-by-1 parame-
ter vector as
(2.29)
where we have introduced two matrices and a vector:
1. the time-averaged M-by-M correlation matrix of the regressor x, which is defined by
(2.30)
where xixT
j is the outer product of the regressors xi and xj, applied to the environ-
ment on the ith and jth experimental trials;
2. the M-by-M identity matrix I whose M diagonal elements are unity and the re-
maining elements are all zero;
3. the time-averaged M-by-1 cross-correlation vector of the regressor x and the de-
sired response d, which is defined by
(2.31)
ˆrdx(N) = – a
N
j=1
xidi 
Rˆ xx(N) = – a
N
i=1 a
N
j=1
xixT
j  
wˆ MAP(N) = [Rxx(N) + 
I]-1rdx(N)
ˆwMAP
e(w) = 1
2 a
N
i=1
(di - wTxi)2 + 
2 7w7 2

 = 2
2
w
 
wˆ MAP(N) = max
w c- 1
2 a
N
i=1
(di - wTxi)2 - 
2 7w7 2d
(wd, x) r expc- 1
2 2 a
N
i=1
(di - wTxi)2 -
1
2 2
w
 7w7 2d 
7w7 = a a
M
k=1
w2
k b
12
 
Section 2.3
Maximum A Posteriori Estimation of the Parameter Vector
75

The correlations
(N) and
(N) are both averaged over all the N examples of the
training sample t—hence the use of the term “time averaged.”
Suppose we assign a large value to the variance σ2
w, which has the implicit effect
of saying that the prior distribution of each element of the parameter vector w is
essentially uniform over a wide range of possible values. Under this condition, the
parameter 
 is essentially zero and the formula of Eq. (2.29) reduces to the ML estimate
(2.32)
which supports the point we made previously: The ML estimator relies solely on the
observation model exemplified by the training sample t. In the statistics literature on
linear regression, the equation
(2.33)
is commonly referred to as the normal equation; the ML estimator 
is, of course, the
solution of this equation. It is also of interest that the ML estimator is an unbiased esti-
mator, in the sense that for an infinitely large training sample t, we find that, in the
limit,
converges to the parameter vector w of the unknown stochastic environment,
provided that the regressor x(n) and the response d(n) are drawn from jointly ergodic
processes, in which case time averages may be substituted for ensemble averages. Under
this condition, in Problem 2.4, it is shown that
In contrast, the MAP estimator of Eq. (2.29) is a biased estimator, which therefore
prompts us to make the following statement:
In improving the stability of the maximum likelihood estimator through the use of regular-
ization (i.e., the incorporation of prior knowledge), the resulting maximum a posteriori esti-
mator becomes biased.
In short, we have a tradeoff between stability and bias.
2.4
RELATIONSHIP BETWEEN REGULARIZED LEAST-SQUARES
ESTIMATION AND MAP ESTIMATION
We may approach the estimation of the parameter vector w in another way by focusing
on a cost function e0(w) defined as the squared expectational errors summed over the N
experimental trials on the environment. Specifically, we write
where we have included w in the argument of εi to stress the fact that the uncertainty in
the regression model is due to the vector w. Rearranging terms in Eq. (2.16), we
obtain
(2.34)
εi(w) = di - wTxi,  i = 1, 2, ..., N 
e0(w) = a
N
i=1
ε2
i(w)
limit
NS q ˆwML(N) = w
ˆwML
ˆwML
ˆRxx(N) ˆwML(N) = ˆrdx(N)
wˆ ML(N) = Rˆ -1
xx(N) rˆ dx(N)
ˆrdx
ˆRxx
76
Chapter 2
Model Building through Regression

Substituting this equation into the expression for e0(w) yields
(2.35)
which relies solely on the training sample t. Minimizing this cost function with respect
to w yields a formula for the ordinary least-squares estimator that is identical to the
maximum-likelihood estimator of Eq. (2.32), and hence there is a distinct possibility of
obtaining a solution that lacks uniqueness and stability.
To overcome this serious problem, the customary practice is to expand the cost
function of Eq. (2.35) by adding a new term as follows:
(2.36)
This expression is identical to the function defined in Eq. (2.28). The inclusion of the
squared Euclidean norm ||w||2 is referred to as structural regularization. Correspond-
ingly, the scalar 
 is referred to as the regularization parameter.
When 
  0, the implication is that we have complete confidence in the observa-
tion model exemplified by the training sample t. At the other extreme, when 
 
,
the implication is that we have no confidence in the observation model. In practice, the
regularization parameter 
 is chosen somewhere between these two limiting cases.
In any event, for a prescribed value of the regularization parameter 
, the solution
of the regularized method of least squares, obtained by minimizing the regularized cost
function of Eq. (2.36) with respect to the parameter vector w, is identical to the MAP
estimate of Eq. (2.29). This particular solution is referred to as the regularized least-
squares (RLS) solution.
2.5
COMPUTER EXPERIMENT: PATTERN CLASSIFICATION
In this section, we repeat the computer experiment performed on the pattern-
classification problem studied in Chapter 1, where we used the perceptron algorithm.As
before, the double-moon structure, providing the training as well as the test data, is that
shown in Fig. 1.8.This time, however, we use the method of least squares to perform the
classification.
Figure 2.2 presents the results of training the least squares algorithm for the
separation distance between the two moons set at d  1. The figure shows the deci-
sion boundary constructed between the two moons. The corresponding results
obtained using the perceptron algorithm for the same setting d  1 were presented
in Fig. 1.9. Comparing these two figures, we make the following interesting 
observations:
1. The decision boundaries constructed by the two algorithms are both linear, which
is intuitively satisfying. The least-squares algorithm discovers the asymmetric
q
 = 1
2 a
N
i=1
(di - wTxi)2 + 
2 7w7 2 
 e(w) = e0(w) + 
2 7w7 2
e0(w) = 1
2 a
N
i=1
(di - wTxi)2
Section 2.5
Computer Experiment: Pattern Classification
77

manner in which the two moons are positioned relative to each other, as seen by
the positive slope of the decision boundary in Fig.2.2.Interestingly enough,the per-
ceptron algorithm completely ignores this asymmetry by constructing a decision
boundary that is parallel to the x-axis.
2. For the separation distance d  1, the two moons are linearly separable.The per-
ceptron algorithm responds perfectly to this setting; on the other hand, in discov-
ering the asymmetric feature of the double-moon figure, the method of least
squares ends up misclassifying the test data,incurring a classification error of 0.8%.
3. Unlike the perceptron algorithm, the method of least squares computes the deci-
sion boundary in one shot.
Figure 2.3 presents the results of the experiment performed on the double-moon
patterns for the separation distance d  4, using the method of least squares. As
expected, there is now a noticeable increase in the classification error, namely, 9.5%.
Comparing this performance with the 9.3% classification error of the perceptron algo-
rithm for the same setting, which was reported in Fig. 1.10, we see that the classification
performance of the method of least squares has degraded slightly.
The important conclusion to be drawn from the pattern-classification computer
experiments of Sections 1.5 and 2.5 is as follows:
Although the perceptron and the least-squares algorithms are both linear, they operate dif-
ferently in performing the task of pattern classification.
78
Chapter 2
Model Building through Regression
x1
20
15
10
5
0
5
10
10
0
5
10
x2
5
Classification using least squares with dist  1, radius  10, and width  6
FIGURE 2.2
Least Squares classification of the double-moon of Fig. 1.8 with distance d  1.

2.6
THE MINIMUM-DESCRIPTION-LENGTH PRINCIPLE
The representation of a stochastic process by a linear model may be used for synthesis
or analysis. In synthesis, we generate a desired time series by assigning a formulated set
of values to the parameters of the model and feeding it with white noise of zero mean
and prescribed variance; the model so obtained is referred to as a generative model. In
analysis, on the other hand, we estimate the parameters of the model by processing a
given time series of finite length,using the Bayesian approach or the regularized method
of least squares. Insofar as the estimation is statistical, we need an appropriate measure
of the fit between the model and the observed data.We refer to this second problem as
that of model selection. For example, we may want to estimate the number of degrees
of freedom (i.e., adjustable parameters) of the model, or even the general structure of
the model.
A plethora of methods for model selection has been proposed in the statistics lit-
erature, with each one of them having a goal of its own.With the goals being different,
it is not surprising to find that the different methods yield wildly different results when
they are applied to the same data set (Grünwald, 2007).
In this section,we describe a well-proven method,called the minimum-description-
length (MDL) principle for model selection, which was pioneered by Rissanen (1978).
Inspiration for the development of the MDL principle is traced back to
Kolmogorov complexity theory. In this remarkable theory, the great mathematician
Section 2.6
The Minimum-Description-Length Principle
79
FIGURE 2.3
Least-squares classification of the double-moon of Fig. 1.8 with distance d  -4.
x2
8
6
4
10
5
20
15
10
5
0
2
0
2
4
6
8
10
12
x1
Classification using least squares with dist  4, radius  10, and width  6

Kolmogorov defined complexity as follows (Kolmogorov, 1965; Li and Vitányi, 1993;
Cover and Thomas, 2006; Grünwald, 2007):
The algorithmic (descriptive) complexity of a data sequence is the length of the shortest bi-
nary computer program that prints out the sequence and then halts.
What is truly amazing about this definition of complexity is the fact that it looks to the
computer, the most general form of data compressor, rather than the notion of proba-
bility distribution for its basis.
Using the fundamental concept of Kolmogorov complexity, we may develop a
theory of idealized inductive inference, the goal of which is to find “regularity” in a given
data sequence. The idea of viewing learning as trying to find “regularity” provided the
first insight that was used by Rissanen in formulating the MDL principle. The second
insight used by Rissanen is that regularity itself may be identified with the “ability to
compress.”
Thus, the MDL principle combines these two insights, one on regularity and the
other on the ability to compress, to view the process of learning as data compression,
which, in turn, teaches us the following:
Given a set of hypotheses,h, and a data sequence d, we should try to find the particular hypothesis
or some combination of hypotheses in h, that compresses the data sequence d the most.
This statement sums up what the MDL principle is all about very succinctly. The sym-
bol d for a sequence should not be confused with the symbol d used previously for de-
sired response.
There are several versions of the MDL principle that have been described in the
literature.We will focus on the oldest, but simplest and most well-known version, known
as the simplistic two-part code MDL principle for probabilistic modeling. By the term
“simplistic,” we mean that the codelengths under consideration are not determined in
an optimal fashion. The terms “code” and “codelengths” used herein pertain to the
process of encoding the data sequence in the shortest or least redundant manner.
Suppose that we are given a candidate model or model class
. With all the ele-
ments of 
being probabilistic sources, we henceforth refer to a point hypothesis as p
rather than h. In particular,we look for the probability density function 
that best
explains a given data sequence d. The two-part code MDL principle then tells us to look
for the (point) hypothesis 
that minimizes the description length of p, which we
denote by L1(p), and the description length of the data sequence d when it is encoded
with the help of p, which we denote as L2(d | p).We thus form the sum
and pick the particular point hypothesis 
that minimizes L12(p, d).
It is crucial that p itself be encoded as well here.Thus,in finding the hypothesis that
compresses the data sequence d the most, we must encode (describe or compress) the
data in such a way that a decoder can retrieve the data even without knowing the
hypothesis in advance.This can be done by explicitly encoding a hypothesis,as in the fore-
going two-part code principle; it can also be done in quite different ways—for example,
by averaging over hypotheses (Grünwald, 2007).
p  m
L12(p, d) = L1(p) + L2(d p)
p  m
p  m
m
m
80
Chapter 2
Model Building through Regression

Model-Order Selection
Let m(1),m(2),...,m(k),...,denote a family of linear regression models that are associated with
the parameter vector 
,where the model order k  1,2,...;that is,the weight spaces
w(1), w(2), ..., w(k), ..., are of increasing dimensionality.The issue of interest is to identify
the model that best explains an unknown environment that is responsible for generat-
ing the training sample {xi, di}N
i1, where xi is the stimulus and di is the corresponding re-
sponse.What we have just described is the model-order selection problem.
In working through the statistical characterization of the composite length L12(p, d),
the two-part code MDL principle tells us to pick the kth model that is the mimimizer
Error term
Complexity term
(2.37)
where (w(k)) is the prior distribution of the parameter vector w(k), and the last term
of the expression is of the order of model order k (Rissanen, 1989; Grünwald, 2007).
For a large sample size N, this last term gets overwhelmed by the second term of
the expression 
. The expression in Eq. (2.37) is usually partitioned into two
terms:
• the error term, denoted by -log(p(di|w(k)) (w(k)), which relates to the model and
the data;
• the hypothesis complexity term, denoted by
, which relates to the
model alone.
In practice, the O(k) term is often ignored to simplify matters when applying Eq. (2.37),
with mixed results. The reason for mixed results is that the O(k) term can be rather
large. For linear regression models, however, it can be explicitly and efficiently com-
puted, and the resulting procedures tend to work quite well in practice.
Note also that the expression of Eq. (2.37) without the prior distribution (w(k))
was first formulated in Rissanen (1978).
If it turns out that we have more than one minimizer of the expression in Eq.
(2.37), then we pick the model with the smallest hypothesis complexity term.And if this
move still leaves us with several candidate models, then we do not have any further
choice but to work with one of them (Grünwald, 2007).
Attributes of the MDL Principle
The MDL principle for model selection offers two important attributes (Grünwald,
2007):
1. When we have two models that fit a given data sequence equally well, the MDL
principle will pick the one that is the “simplest” in the sense that it allows the use
of a shorter description of the data. In other words, the MDL principle implements
a precise form of Occam’s razor, which states a preference for simple theories:
Accept the simplest explanation that fits the data.
k
2 log(N) + O(k)
k
2 log(N)
k = 1, 2, ...
i = 1, 2, ..., N
min
k
e- log p(di @ w(k))(w(k)) + k
2 log(N) + O(k) f ,
w k  wk
Section 2.6
The Minimum-Description-Length Principle
81
¯˚˚˚˚˘˚˚˚˚˙
¯˚˚˚˘˚˚˚˙

2. The MDL principle is a consistent model selection estimator in the sense that it con-
verges to the true model order as the sample size increases.
Perhaps the most significant point to note is that, in nearly all of the applications in-
volving the MDL principle, few, if any, anomalous results or models with undesirable
properties have been reported in the literature.
2.7
FINITE SAMPLE-SIZE CONSIDERATIONS
A serious limitation of the maximum-likelihood or ordinary least-squares approach to
parameter estimation is the nonuniqueness and instability of the solution, which is at-
tributed to complete reliance on the observation model (i.e., the training sample t); the
traits of nonuniqueness and instability in characterizing a solution are also referred to
as an overfitting problem in the literature.To probe more deeply into this practical issue,
consider the generic regressive model
(2.38)
where f(x, w) is a deterministic function of the regressor x for some w parameterizing
the model and ε is the expectational error. This model, depicted in Fig. 2.4a, is a
mathematical description of a stochastic environment; its purpose is to explain or predict
the response d produced by the regressor x.
Figure 2.4b is the corresponding physical model of the environment, where 
de-
notes an estimate of the unknown parameter vector w. The purpose of this second model
is to encode the empirical knowledge represented by the training sample t, as shown by
(2.39)
In effect, the physical model provides an approximation to the regression model of
Fig. 2.4a. Let the actual response of the physical model, produced in response to the
input vector x, be denoted by
wˆ
t S  
wˆ
d = f(x, w) +  
82
Chapter 2
Model Building through Regression
FIGURE 2.4
(a) Mathematical model of a stochastic environment, parameterized by the vector
w. (b) Physical model of the environment, where 
is an estimate of the unknown parameter
vector w.
wˆ
(a)
Response
d
Expectational error
e
Regressor
x
f(, w)
(b)
Output
y
Input
x
F(, w)
ˆ
Σ

(2.40)
where 
is the input–output function realized by the physical model; the y in
Eq. (2.40) is a sample value of random variable Y. Given the training sample t of
Eq. (2.39), the estimator 
is the minimizer of the cost function
(2.41)
where the factor has been used to be consistent with earlier notations. Except for the 
scaling factor , the cost function 
is the squared difference between the environ-
mental (desired) response d and the actual response y of the physical model, computed
over the entire training sample t.
Let the symbol t denote the average operator taken over the entire training sam-
ple t. The variables or their functions that come under the average operator t are
denoted by x and d; the pair (x, d) represents an example in the training sample t. In
contrast, the statistical expectation operator  acts on the whole ensemble of x and d,
which includes t as a subset.The difference between the operators  and t should be
very carefully noted in what follows.
In light of the transformation described in Eq. (2.39), we may interchangeably use 
and F(x, t) and therefore rewrite Eq. (2.41) in the equivalent form
(2.42)
By adding and then subtracting f(x, w) to the argument (d - F(x, t)) and next using
Eq. (2.38), we may write
By substituting this expression into Eq. (2.42) and then expanding terms, we may recast
the cost function e( ) in the equivalent form
(2.43)
However, the last expectation term on the right-hand side of Eq. (2.43) is zero, for two
reasons:
• The expectational error is uncorrelated with the regression function f(x, w).
• The expectational error pertains to the regression model of Fig. 2.4a, whereas the
approximating function
pertains to the physical model of Fig. 2.4b.
Accordingly, Eq. (2.43) reduces to
(2.44)
e( ˆw) = 1
2
 t[2] + 1
2
 t[(f(x, w) - F(x, t))2]
F(x, ˆw)


e(wˆ ) = 1
2 t[2] + 1
2 t[(f(x, w) - F(x, t))2] + t[f(x, w) - F(x, t)] 
wˆ
 =  + [f(x, w) - F(x, t)]
 d - f(x, t) = [d - f(x, w)] + [f(x, w) - F(x, t)]
e(wˆ ) = 1
2
 t[(d - F(x, t))2] 
F(x, wˆ )

e(wˆ )
1
2
1
2
e(wˆ ) = 1
2 a
N
i=1
(di - F(xi, wˆ ))2
wˆ
F(, wˆ )
y = F(x, wˆ )
Section 2.7
Finite Sample-size Considerations
83

The term 
t[
] on the right-hand side of Eq. (2.44) is the variance of the expectational
(regressive modeling) error , evaluated over the training sample t; here it is assumed
that 
has zero mean. This variance represents the intrinsic error because it is indepen-
dent of the estimate . Hence, the estimator 
that is the minimizer of the cost function
e( ) will also minimize the ensemble average of the squared distance between the re-
gression function f (x, w) and the approximating function 
. In other words, the
natural measure of the effectiveness of 
as a predictor of the desired response d
is defined as follows (ignoring the scaling factor 1⁄2):
(2.45)
This natural measure is fundamentally important because it provides the mathematical
basis for the tradeoff between the bias and variance that results from the use of 
as the approximation to f(x, w).
Bias–Variance Dilemma
From Eq. (2.38), we find that the function f (x, w) is equal to the conditional expectation
(d|x).We may therefore redefine the squared distance between f (x) and 
as
(2.46)
This expression may therefore be viewed as the average value of the estimation error
between the regression function f (x, w)  [d|x] and the approximating function 
,
evaluated over the entire training sample t. Notice that the conditional mean
[d|x] has a constant expectation with respect to the training sample t. Next we write
F(x, wˆ )
Lav(f(x, w), F(x, ˆw)) = t[( [d@x] - F(x, t))2]
F(x, wˆ )
F(x, wˆ )
Lav(f(x, w), F(x, wˆ )) = t[(f(x, w) - F(x, t))2]
F(x, ˆw)
F(x, wˆ )
wˆ
wˆ
wˆ


2

84
Chapter 2
Model Building through Regression
where we have simply added and then subtracted the average t[F(x, t)]. By proceed-
ing in a manner similar to that described for deriving Eq. (2.43) from Eq. (2.42), we
may reformulate Eq. (2.46) as the sum of two terms (see Problem 2.5):
(2.47)
where 
and 
are themselves respectively defined by
(2.49)
and
(2.49)
We now make two important observations:
1. The first term,
, is the bias of the average value of the approximating function
F(x, t), measured with respect to the regression function f(x, w) = [d | x]. Thus,
represents the inability of the physical model defined by the function F(x, wˆ )
B(wˆ )

B(wˆ )
V(wˆ ) = t[(F(x, t) - t[F(x, t])2]
B(wˆ ) = t[F(x, t)] - [d @x]
V(wˆ )
B(wˆ )
Lav(f(x), F(x, t)) = B2(wˆ ) + V(wˆ )
[dx] - F(x, t) = ([dx] - t[F(x, t)]) + (t[F(x, t)] - F(x, t))
to accurately approximate the regression function f(x, w)  [d|x].We may there-
fore view the bias
as an approximation error.
2. The second term,
, is the variance of the approximating function F(x, t),
measured over the entire training sample t. Thus,
represents the inadequacy
V(wˆ )
V(wˆ )
B(wˆ )

of the empirical knowledge contained in the training sample tabout the regres-
sion function f(x, w).We may therefore view the variance
as the manifesta-
tion of an estimation error.
Figure 2.5 illustrates the relations between the target (desired) and approximat-
ing functions; it shows how the estimation errors, namely, the bias and variance, accu-
mulate. To achieve good overall performance, the bias 
and the variance 
of
the approximating function 
would both have to be small.
Unfortunately, we find that in a complex physical model that learns by example and
does so with a training sample of limited size, the price for achieving a small bias is a large
variance. For any physical model, it is only when the size of the training sample becomes
infinitely large that we can hope to eliminate both bias and variance at the same time.Ac-
cordingly,we have a bias–variance dilemma, the consequence of which is prohibitively slow
convergence (Geman et al., 1992).The bias–variance dilemma may be circumvented if we
are willing to purposely introduce bias, which then makes it possible to eliminate the vari-
ance or to reduce it significantly.Needless to say,we must be sure that the bias built into the
design of the physical model is harmless. In the context of pattern classification, for exam-
ple, the bias is said to be harmless in the sense that it will contribute significantly to the
mean-square error only if we try to infer regressions that are not in the anticipated class.
Explanatory notes on what Fig. 2.5 is depicting:
1. The shaded inner space of the figure is a subset of the outer space:
The outer space represents the ensemble of regression functions 
.
The inner space represents the ensemble of approximating functions 
.
2. The figure shows three points, two fixed and one random:
fixed-point, is averaged over the outer space
second fixed-point, is averaged over the inner space
is randomly distributed inside the inner space
3. Statistical parameters, embodied in the figure:
B(w)  bias, denoting the distance between 
and 
.
t[F(x, t)]
[dx]
F(x, t)
t[F(x, t)],
[dx],
F(, wˆ )
f(, w)
F(x, wˆ ) = F(x, t)
V(wˆ )
B(wˆ )
V(wˆ )
Section 2.7
Finite Sample-size Considerations
85
F(x, )
f(x, w)  [d|x]
[F(x, )]
FIGURE 2.5
Decomposition of the natural measure 
defined in
Eq. (2.46), into bias and variance terms for linear regression models.
Lav(f(x, w), F(x, wˆ )),
V(w)  variance, denoting the squared distance between 
and
averaged over the training sample t.
t[F(x, t)],
F(x, t)
B2(w)  V(w)  squared distance between 
and 
averaged over the
training sample t.
[dx]
F(x, t)

86
Chapter 2
Model Building through Regression
In general, the bias must be designed for each specific application of interest. A
practical way of achieving such an objective is to use a constrained network architecture,
which usually performs better than a general-purpose architecture.
2.8
THE INSTRUMENTAL-VARIABLES METHOD
In studying the linear regression model, first from the perspective of Bayesian theory in
Section 2.3 and then from the perspective of the method of least squares in Section 2.4,
we pointed out that both approaches yield the same solution for the parameter vector
w of the unknown stochastic environment depicted in Fig. 2.1, namely, Eq. (2.29) for the
regularized linear regression model and Eq. (2.32) for the unregularized version. Both
of these formulas were derived for a Gaussian environment, on the premise that the re-
gressor (i.e., the input signal) x and the desired response d are both noiseless. What if,
however, we find that the regressor x can be observed only in the presence of additive
noise, as could happen in practice? That is, the noisy regressor is now defined by
(2.50)
where 
is the measurement noise associated with the observation of xi in the ith real-
ization of the training sample t. If we were to apply the unregularized formula of
Eq. (2.32), we would obtain a modified solution for the parameter vector w of the un-
known stochastic environment:
(2.51)
where
zz is the time-averaged correlation function of the noisy regressor z and
dz is
the corresponding time-averaged cross-correlation function between the desired response
d and z. To simplify matters, we have ignored the dependence of these two correlation
functions on the size of the training sample.Assuming that the measurement noise vec-
tor v is white noise with zero mean and correlation matrix 
,where I is the identity ma-
trix, we obtain the following correlation functions:
and
Correspondingly, the maximum-likelihood estimator assumes the new form
(2.52)
which, in mathematical terms, is identical to the MAP formula of Eq. (2.29) with the
regularization parameter 
 set equal to the noise variance σ2
v.This observation leads us
to make the following statement:
The presence of additive noise in the regressor z (with the right noise variance) has the ben-
eficial effect of stabilizing the maximum-likelihood estimator, but at the expense of intro-
ducing a bias into the solution.
This is quite an ironic statement:The addition of noise acts as a regularizer (stabilizer)!
wˆ ML = (Rˆ xx + 
2I)-1rˆdx
rˆ dz = rˆ dx
Rˆ zz = Rˆ xx + 
2I

2I
rˆ
ˆR
wˆ ML = ˆR-1
zz rˆ dz
i
zi = xi + i

Suppose, however, the requirement is to produce a solution for the unknown
parameter vector w that is desirably asymptotically unbiased. In such a situation, we
may resort to the method of instrumental variables (Young, 1984).This method relies
on the introduction of a set of instrumental variables, denoted by the vector that has
the same dimensionality as the noisy regressor z and satisfies the following two 
properties:
Property 1.
The instrumental vector is highly correlated with the noiseless
regressor x, as shown by
(2.53)
where xj is the jth element of the noiseless regressor x and 
is the kth element of the
instrumental vector .
Property 2.
The instrumental vector and the measurement noise vector v are
statistically independent, as shown by
(2.54)
Equipped with the instrumental vector that satisfies these two properties, we compute
the following correlation functions:
1. The noisy regressor z is correlated with the instrumental vector , obtaining the
cross-correlation matrix
(2.55)
where zi is the ith regressor of the noisy training sample {zi, di}N
i1, and 
is the
corresponding instrumental vector.
2. The desired response d is correlated with the instrumental vector , obtaining the
cross-correlation vector
(2.56)
Given these two correlation measurements, we then use the modified formula
(2.57)
for computing an estimate of the unknown parameter vector w (Young, 1984). Un-
like the ML solution of Eq. (2.51), the modified formula of Eq. (2.57), based on the
method of instrumental variables, provides an asymptotically unbiased estimate of
the unknown parameter vector w; see Problem 2.7.
 = a a
N
i=1
xˆizT
i b
-1
a a
N
i=1
xˆid i b
 wˆ (N) = R-1
zxˆ rdxˆ 
rˆdˆx = a
N
i=1
xˆ idi
xˆ
xˆi
ˆRzxˆ = a
N
i=1
xˆ izT
i
xˆ
xˆ
[j ˆxk] = 0  for all j and k
xˆ
xˆ
xˆ k
[xj ˆxk] Z 0  for all j and k
xˆ
xˆ
Section 2.8
The Instrumental-Variables Method
87

In applying the method of instrumental variables, however, the key issue is
how to obtain or generate variables that satisfy Properties 1 and 2. It turns out
that in time-series analysis, the resolution of this issue is surprisingly straight-
forward (Young, 1984).
2.9
SUMMARY AND DISCUSSION
In this chapter,we studied the method of least squares for linear regression,which is well
established in the statistics literature. The study was presented from two different, yet
complementary, viewpoints:
• Bayesian theory, where the maximum a posteriori estimate of a set of unknown
parameters is the objective of interest. This approach to parameter estimation
requires knowledge of the prior distribution of the unknown parameters.The pre-
sentation was demonstrated for a Gaussian environment.
• Regularization theory, where the cost function to be minimized with respect to the
unknown parameters consists of two components:the squared explanational errors
summed over the training data, and a regularizing term defined in terms of the
squared Euclidean norm of the parameter vector.
For the special case of an environment in which the prior distribution of the unknown
parameters is Gaussian with zero mean and variance σ2
w, it turns out that the regular-
ization parameter 
 is inversely proportional to σ2
w.The implication of this statement is
that when σ2
w is very large (i.e., the unknown parameters are uniformly distributed over
a wide range), the formula for finding the estimate of the parameter vector w is defined
by the normal equation
where 
xx is the time-averaged correlation matrix of the input vector x and 
is the cor-
responding time-averaged cross-correlation vector between the input vector x and the
desired response d. Both correlation parameters are computed using the training sam-
ple {xi, di}N
i1 and are therefore dependent on its sample size N. Furthermore, this for-
mula is identical to the solution obtained using the maximum-likelihood method that
assumes a uniform distribution for the prior.
We also discussed three other important issues:
• The minimum-description-length (MDL) criterion for model-order selection (i.e.,
the size of the unknown parameter vector in a linear regression model).
• The bias–variance dilemma, which means that in parameter estimation (involv-
ing the use of a finite sample size) we have the inevitable task of trading off the
variance of the estimate with the bias; the bias is defined as the difference be-
tween the expected value of the parameter estimate and the true value, and the
variance is a measure of the “volatility” of the estimate around the expected value.
• The method of instrumental variables, the need for which arises when the ob-
servables in the training sample are noisy; such a situation is known to arise in
practice.
rˆ dx
Rˆ
wˆ = Rˆ -1
xx rˆdx
88
Chapter 2
Model Building through Regression

Problems
89
NOTES AND REFERENCES
1. Regression models can be linear or nonlinear. Linear regression models are dis-
cussed in depth in the classic book by Rao (1973). Nonlinear regression models are
discussed in Seber and Wild (1989).
2. For a highly readable account of Bayesian theory, see Robert (2001).
3. For a detailed discussion of the method of least squares, see Chapter 8 of Haykin
(2002).
PROBLEMS
2.1. Discuss the basic differences between the maximum a posteriori and maximum-
likelihood estimates of the parameter vector in a linear regression model.
2.2. Starting with the cost function of Eq. (2.36), e(w), derive the formula of 
Eq. (2.29) by minimizing the cost function with respect to the unknown para-
meter vector w.
2.3. In this problem, we address properties of the least-squares estimator based on the
linear regression model of Fig. 2.1:
Property 1.
The least-squares estimate
is unbiased, provided that the expectational error ε in the linear regression
model of Fig. 2.1 has zero mean.
Property 2.
When the expectational error ε is drawn from a zero-mean white-
noise process with variance σ2, the covariance matrix of the least-squares estimate 
equals
Property 3.
The estimation error
produced by the optimized method of least squares is orthogonal to the estimate
of the desired response, denoted by ; this property is a corollary to the principle
of orthogonality. If we were to use geometric representations of d,
, and eo, then
we would find that the “vector” representing eo, is perpendicular (i.e., normal) to
that representing ; indeed it is in light of this geometric representation that the
formula
is called the normal equation.
Starting with the normal equation, prove each of these three properties under the
premise that 
and 
are time-averaged correlation functions.
rˆ dx
Rˆ
xx
Rˆ
xxwˆ = rˆ dx
dˆ
dˆ
dˆ
eo = d - wˆ Tx
2Rˆ -1
xx.
wˆ
wˆ = R-1
xxrdx

90
Chapter 2
Model Building through Regression
2.4. Let 
denote the ensemble-averaged correlation function of the regressor x, and let rdx de-
note the corresponding ensemble-averaged cross-correlation vector between the regressor
x and response d; that is,
Referring to the linear regression model of Eq. (2.3), show that minimization of the mean-
square error 
leads to the Wiener–Hopf equation
where w is the parameter vector of the regression model. Compare this equation with the
normal equation of Eq. (2.33).
2.5. Equation (2.47) expresses the natural measure of the effectiveness of the approximating
function 
as a predictor of the desired response d. This expression is made up of two
components, one defining the squared bias and the other defining the variance. Derive this
expression, starting from Eq. (2.46)
2.6. Elaborate on the following statement:
A network architecture, constrained through the incorporation of prior knowledge, ad-
dresses the bias–variance dilemma by reducing variance at the expense of increased bias.
2.7. The method of instrumental variables described in Eq. (2.57) provides an asymptotically
unbiased estimate of the unknown parameter vector 
; that is,
Prove the validity of this statement, assuming joint ergodicity of the regressor x and
response d.
COMPUTER EXPERIMENT
2.8. Repeat the pattern-classification experiment described in Section 2.5, this time setting the
two moons at the very edge of linear separability, that is, d  0. Comment on your results,
and compare them with those obtained in Problem 1.6, involving the perceptron.
2.9. In performing the experiments in Section 2.5 and Problem 2.8, there was no regularization
included in the method of least squares. Would the use of regularization have made a dif-
ference in the performance of the method of least squares?
To substantiate your response to this question,repeat the experiment of Problem 2.7,this
time using the regularized least-squares algorithm.
lim
NS qwˆ (N) = w
wˆ (N)
F(x, wˆ )
Rxxw = rdx
J(w) = [2]
rdx = [dx]
Rxx = [xxT]
Rxx

ORGANIZATION OF THE CHAPTER
In this chapter, we describe a highly popular on-line learning algorithm known as the
least-mean-square (LMS) algorithm, which was developed by Widrow and Hoff in 1960.
The chapter is organized as follows:
1. Section 3.1 is introductory,followed by Section 3.2 that sets the stage for the rest of the
chapter by describing a linear discrete-time filter of finite-duration impulse response.
2. Section 3.3 reviews two unconstrained optimization techniques:the method of steepest
descent and Newton’s method.
3. Section 3.4 formulates the Wiener filter, which is optimum in the mean-square-error
sense.Traditionally, the average performance of the LMS algorithm is judged against
the Wiener filter.
4. Section 3.5 presents the derivation of the LMS algorithm. Section 3.6 portrays a modi-
fied form of the LMS algorithm as a Markov model.Then,to prepare the way for study-
ing the convergence behavior of the LMS algorithm,Section 3.7 introduces the Langevin
equation, rooted in unstable thermodynamics.The other tool needed for convergence
analysis of the algorithm is Kushner’s method of direct averaging;this method is discussed
in Section 3.8. Section 3.9 presents a detailed statistical analysis of the algorithm; most
importantly, it shows that the statistical behavior of the algorithm (using a small learn-
ing-rate parameter) is, in fact, the discrete-time version of the Langevin equation.
5. Section 3.10 presents a computer experiment validating the small learning-rate theory
of the LMS algorithm. Section 3.11 repeats the pattern-classification experiment of
Section 1.5 on the perceptron, this time using the LMS algorithm.
6. Section 3.12 discusses the virtues and limitations of the LMS algorithm. Section 3.13
discusses the related issue of learning-rate annealing schedules.
Section 3.14 provides a summary and discussion that conclude the chapter.
3.1
INTRODUCTION
Rosenblatt’s perceptron, discussed in Chapter 1, was the first learning algorithm for
solving a linearly separable pattern-classification problem.The least-mean-square (LMS)
algorithm, developed by Widrow and Hoff (1960), was the first linear adaptive-filtering
91
C H A P T E R  3
The Least-Mean-Square
Algorithm

algorithm for solving problems such as prediction and communication-channel equal-
ization. Development of the LMS algorithm was indeed inspired by the perceptron.
Though different in applications, these two algorithms share a common feature: They
both involve the use of a linear combiner, hence the designation “linear.”
The amazing thing about the LMS algorithm is that it has established itself not
only as the workhorse for adaptive-filtering applications, but also as the benchmark
against which other adaptive-filtering algorithms are evaluated.The reasons behind this
amazing track record are multifold:
• In terms of computational complexity, the LMS algorithm’s complexity is linear
with respect to adjustable parameters, which makes the algorithm computationally
efficient, yet the algorithm is effective in performance.
• The algorithm is simple to code and therefore easy to build.
• Above all, the algorithm is robust with respect to external disturbances.
From an engineering perspective, these qualities are all highly desirable. It is therefore
not surprising to see that the LMS algorithm has withstood the test of time.
In this chapter, we derive the LMS algorithm in its most basic form and discuss its
virtues and limitations.Most importantly,the material presented herein sets the stage for
the back-propagation algorithm to be discussed in the next chapter.
3.2
FILTERING STRUCTURE OF THE LMS ALGORITHM
Figure 3.1 shows the block diagram of an unknown dynamic system that is stimulated
by an input vector consisting of the elements x1(i), x2(i), ..., xM(i), where i denotes the
instant of time at which the stimulus (excitation) is applied to the system.The time index
FIGURE 3.1
(a) Unknown dynamic 
system. (b) Signal-flow graph of
adaptive model for the system; the
graph embodies a feedback loop set in
color.
(a)
(b)
Unknown
dynamic
system
Output
 d(i)
Inputs
x1(i)
x2(i)
xM(i)
•••
•••
w1(i)
wM(i)
e(i)
y(i)
v(i)
x1(i)
x2(i)
xM(i)
1
d(i)
w2(i)
92
Chapter 3
The Least-Mean-Square Algorithm

i  1,2,..., n. In response to this stimulus,the system produces an output denoted by y(i).
Thus, the external behavior of the system is described by the data set
t:
(3.1)
where
(3.2)
The sample pairs composing t are identically distributed according to an unknown
probability law. The dimension M pertaining to the input vector x(i) is referred to as
the dimensionality of the input space, or simply as the input dimensionality.
The stimulus vector x(i) can arise in one of two fundamentally different ways, one
spatial and the other temporal:
x(i) = [x1(i), x2(i), ..., xM(i)]T
{x(i), d(i); i = 1, 2, ...., n, ...}
Section 3.2
Filtering Structure of the LMS Algorithm
93
• The M elements of x(i) originate at different points in space; in this case, we speak
of x(i) as a snapshot of data.
• The M elements of x(i) represent the set of present and (M - 1) past values of
some excitation that are uniformly spaced in time.
The problem we address is how to design a multiple-input–single-output model
of the unknown dynamic system by building it around a single linear neuron. The
neural model operates under the influence of an algorithm that controls necessary
adjustments to the synaptic weights of the neuron, with the following points in
mind:
• The algorithm starts from an arbitrary setting of the neuron’s synaptic weights.
• Adjustments to the synaptic weights in response to statistical variations in the sys-
tem’s behavior are made on a continuous basis (i.e time is incorporated into the
constitution of the algorithm).
• Computations of adjustments to the synaptic weights are completed inside an
interval that is one sampling period long.
The neural model just described is referred to as an adaptive filter. Although the
description is presented in the context of a task clearly recognized as one of system
identification, the characterization of the adaptive filter is general enough to have
wide application.
Figure 3.1b shows a signal-flow graph of the adaptive filter. Its operation consists
of two continuous processes:
1. Filtering process, which involves the computation of two signals:
• an output, denoted by y(i), that is produced in response to the M elements of
the stimulus vector x(i), namely, x1(i), x2(i), ..., xM(i);
• an error signal, denoted by e(i), that is obtained by comparing the output y(i)
with the corresponding output d(i) produced by the unknown system. In effect,
d(i) acts as a desired response, or target, signal.
2. Adaptive process, which involves the automatic adjustment of the synaptic weights
of the neuron in accordance with the error signal e(i).

Thus, the combination of these two processes working together constitutes a feedback
loop acting around the neuron, as shown in Fig. 3.1b.
Since the neuron is linear, the output y(i) is exactly the same as the induced local
field v(i); that is,
(3.3)
where w1(i), w2(i), ..., wM(i) are the M synaptic weights of the neuron, measured at
time i. In matrix form, we may express y(i) as an inner product of the vectors x(i)
and w(i) as
(3.4)
where
Note that the notation for a synaptic weight has been simplified here by not including
an additional subscript to identify the neuron, since we have only a single neuron to
deal with. This practice is followed throughout the book, whenever a single neuron is
involved. The neuron’s output y(i) is compared with the corresponding output d(i)
received from the unknown system at time i. Typically, y(i) is different from d(i); hence,
their comparison results in the error signal
(3.5)
The manner in which the error signal e(i) is used to control the adjustments to the neu-
ron’s synaptic weights is determined by the cost function used to derive the adaptive-
filtering algorithm of interest.This issue is closely related to that of optimization. It is
therefore appropriate to present a review of unconstrained-optimization methods.
The material is applicable not only to linear adaptive filters, but also to neural networks
in general.
3.3
UNCONSTRAINED OPTIMIZATION: A REVIEW
Consider a cost function e(w) that is a continuously differentiable function of some
unknown weight (parameter) vector w. The function e(w) maps the elements of w into
real numbers. It is a measure of how to choose the weight (parameter) vector w of an
adaptive-filtering algorithm so that it behaves in an optimum manner.We want to find
an optimal solution w* that satisfies the condition
e
e
(3.6)
That is, we need to solve an unconstrained-optimization problem, stated as follows:
e
.
The necessary condition for optimality is
e
(3.7)
(w*) = 0
§
(w) with respect to the weight vector w
Minimize the cost function
(w)
(w*) 
e(i) = d(i) - y(i)
w(i) = [w1(i), w2(i), ..., wM(i)]T
y(i) = xT(i)w(i)
y(i) = v(i) = a
M
k=1
wk(i)xk(i)
94
Chapter 3
The Least-Mean-Square Algorithm

where § is the gradient operator,
(3.8)
and 
is the gradient vector of the cost function,
(3.9)
(Differentiation with respect to a vector is discussed in Note 1 at the end of this chapter.)
A class of unconstrained-optimization algorithms that is particularly well suited for
the design of adaptive filters is based on the idea of local iterative descent:
Starting with an initial guess denoted by w(0), generate a sequence of weight vectors w(1),
w(2), . . ., such that the cost function e(w) is reduced at each iteration of the algorithm, as
shown by
(3.10)
where w(n) is the old value of the weight vector and w(n  1) is its updated value.
We hope that the algorithm will eventually converge onto the optimal solution w*. We
say “hope” because there is a distinct possibility that the algorithm will diverge (i.e.,
become unstable) unless special precautions are taken.
In this section, we describe three unconstrained-optimization methods that rely
on the idea of iterative descent in one form or another (Bertsekas, 1995).
Method of Steepest Descent
In the method of steepest descent, the successive adjustments applied to the weight vec-
tor w are in the direction of steepest descent,that is,in a direction opposite to the gradient
vector
. For convenience of presentation, we write
(3.11)
Accordingly, the steepest-descent algorithm is formally described by
(3.12)
where  is a positive constant called the stepsize, or learning-rate, parameter, and g(n) is
the gradient vector evaluated at the point w(n). In going from iteration n to n  1, the
algorithm applies the correction
(3.13)
Equation (3.13) is in fact a formal statement of the error-correction rule described in the
introductory chapter.
To show that the formulation of the steepest-descent algorithm satisfies the con-
dition of Eq. (3.10) for iterative descent, we use a first-order Taylor series expansion
around w(n) to approximate 
as
e(w(n + 1)) L e(w(n)) + gT(n)¢w(n)
e(w(n + 1))
= -g(n)
¢w(n) = w(n + 1) - w(n)
w(n + 1) = w(n) - g(n)
g = §e(w)
§e(w)
e(w(n + 1)) 6 e(w(n))
§e(w) = c 0e
0w1
, 0e
0w2
, ..., 0e
0wM
d
T
(w)
§e
§ = c
0
0w1
,
0
0w2
, ..., 
0
0wM
d
T
Section 3.3
Unconstrained Optimization: A Review
95

the use of which is justified for small . Substituting Eq. (3.13) into this approximate
relation yields
which shows that,for a positive learning-rate parameter ,the cost function is decreased
as the algorithm progresses from one iteration to the next.The reasoning presented here
is approximate in that this end result is true only for small enough learning rates.
The method of steepest descent converges to the optimal solution w* slowly. More-
over,the learning-rate parameter  has a profound influence on its convergence behavior:
• When  is small, the transient response of the algorithm is overdamped, in that
the trajectory traced by w(n) follows a smooth path in the w-plane, as illustrated
in Fig. 3.2a.
• When  is large,the transient response of the algorithm is underdamped, in that the
trajectory of w(n) follows a zigzagging (oscillatory) path, as illustrated in Fig. 3.2b.
• When  exceeds a certain critical value, the algorithm becomes unstable (i.e., it
diverges).
Newton’s Method
For a more elaborate optimization technique,we may look to Newton’s method, the basic
idea of which is to minimize the quadratic approximation of the cost function 
around the current point w(n); this minimization is performed at each iteration of the
algorithm. Specifically, using a second-order Taylor series expansion of the cost func-
tion around the point w(n), we may write
(3.14)
As before, g(n) is the M-by-1 gradient vector of the cost function 
evaluated at
the point w(n).The matrix H(n) is the m-by-m Hessian of 
, also evaluated at w(n).
The Hessian of 
is defined by
= ≥
02e
0w2
1
02e
0w10w2
. . .
02e
0w10wM
02e
0w20w1
02e
0w2
2
. . .
02e
0w20wM
o
o
o
o
02e
0wM0w1
02e
0wM0w2
. . .
02e
0w2
M
¥
H = §2e(w)
e(w)
e(w)
e(w)
L gT(n)¢w(n) + 1
2¢wT(n)H(n)¢w(n)
¢e(w(n)) = e(w(n + 1)) - e(w(n))
e(w)
= e(w(n)) - 7g(n)7 2
e(w(n + 1)) L e(w(n)) - gT(n)g(n)
96
Chapter 3
The Least-Mean-Square Algorithm
(3.15)

Section 3.3
Unconstrained Optimization: A Review
97
w2(n)
w1(n)
4.0
4.0
0.0
4.0
0.0
4.0
n  0
n  1
n  2
(a)
small h
w2(n)
w1(n)
4.0
4.0
0.0
4.0
0.0
4.0
n  0
n  1
n  2
(b)
large h
FIGURE 3.2
Trajectory of the method of steepest descent in a two-dimensional space for
two different values of learning-rate parameter: (a) small  (b) large .The coordinates
w1 and w2 are elements of the weight vector w; they both lie in the w-plane.

Equation (3.15) requires the cost function 
to be twice continuously differentiable
with respect to the elements of w. Differentiating1 Eq. (3.14) with respect to ∆w, we
minimize the resulting change 
when
Solving this equation for ∆w(n) yields
That is,
(3.16)
where H-1(n) is the inverse of the Hessian of 
.
Generally speaking, Newton’s method converges quickly asymptotically and does
not exhibit the zigzagging behavior that sometimes characterizes the method of steep-
est descent.However,for Newton’s method to work,the Hessian H(n) has to be a positive
definite matrix for all n. Unfortunately, in general, there is no guarantee that H(n) is
positive definite at every iteration of the algorithm. If the Hessian H(n) is not positive
definite, modification of Newton’s method is necessary (Powell, 1987; Bertsekas, 1995).
In any event, a major limitation of Newton’s method is its computational complexity.
Gauss–Newton Method
To deal with the computational complexity of Newton’s method without seriously
compromising its convergence behavior, we may use the Gauss–Newton method. To
apply this method, we adopt a cost function that is expressed as the sum of error
squares. Let
(3.17)
where the scaling factor is included to simplify matters in subsequent analysis.All the
error terms in this formula are calculated on the basis of a weight vector w that is fixed
over the entire observation interval 1  i  n.
The error signal e(i) is a function of the adjustable weight vector w. Given an oper-
ating point w(n), we linearize the dependence of e(i) on w by introducing the new term
Equivalently, by using matrix notation, we may write
(3.18)
where e(n) is the error vector
e(n) = [e(1), e(2), ..., e(n)]T
e¿(n, w) = e(n) + J(n) (w - w(n))
e¿(i, w) = e(i) + c
0e(i)
0w d
T
w=w(n)
* (w - w(n)),  i = 1, 2, ..., n
1
2
e(w) = 1
2 a
n
i=1
e2(i)
e(w)
= w(n) - H-1(n)g(n)
w(n + 1) = w(n) + ¢w(n)
¢w(n) = -H-1(n)g(n)
g(n) + H(n)¢w(n) = 0
¢e(w)
e(w)
98
Chapter 3
The Least-Mean-Square Algorithm

and J(n) is the n-by-m Jacobian of e(n):
(3.19)
The Jacobian J(n) is the transpose of the m-by-n gradient matrix 
, where
The updated weight vector w(n  1) is now defined by
(3.20)
Using Eq. (3.18) to evaluate the squared Euclidean norm of e(n, w), we get
Hence, differentiating this expression with respect to w and setting the result equal to
zero, we obtain
Solving this equation for w, we may thus write, in light of Eq. 3.20,
(3.21)
which describes the pure form of the Gauss–Newton method.
Unlike Newton’s method, which requires knowledge of the Hessian of the cost
function 
, the Gauss–Newton method requires only the Jacobian of the error
vector e(n). However, for the Gauss–Newton iteration to be computable, the matrix
product JT(n)J(n) must be nonsingular.
With regard to the latter point, we recognize that JT(n)J(n) is always nonnegative
definite. To ensure that it is nonsingular, the Jacobian J(n) must have row rank n; that
is, the n rows of J(n) in Eq. (3.19) must be linearly independent. Unfortunately, there is
no guarantee that this condition will always hold. To guard against the possibility that
J(n) is rank deficient, the customary practice is to add the diagonal matrix δI to the
matrix JT(n)J(n), where I is the identity matrix.The parameter δ is a small positive con-
stant chosen to ensure that
JT(n)J(n) + I is positive definite for all n
e(n)
w(n + 1) = w(n) - (JT(n)J(n))-1JT(n)e(n)
JT(n)e(n) + JT(n)J(n)(w - w(n)) = 0
+ 1
2 (w - w(n))TJT(n)J(n)(w - w(n))
1
2 7e¿(n, w)7 2 = 1
2 7e(n)7 2 + eT(n)J(n)(w - w(n))
w(n + 1) = arg min
W e 1
2 7e¿(n,w)7 2 f
§e(n) = [§e(1), §e(2), ..., §e(n)]
§e(n)
J(n) = ≥
0e(1)
0w1
0e(1)
0w2
. . .
0e(1)
0wM
0e(2)
0w1
0e(2)
0w2
. . .
0e(2)
0wM
o
o
o
0e(n)
0w1
0e(n)
0w2
. . .
0e(n)
0wM
¥
w=w(n)
Section 3.3
Unconstrained Optimization: A Review
99

On this basis, the Gauss–Newton method is implemented in the slightly modified form
(3.22)
The effect of the added term δI is progressively reduced as the number of iterations, n,
is increased. Note also that the recursive equation (3.22) is the solution of the modified
cost function
(3.23)
where w(n) is the current value of the weight vector w(i).
In the literature on signal processing, the addition of the term δI in Eq. (3.22) is
referred to as diagonal loading. The addition of this term is accounted for by expanding
the cost function 
in the manner described in Eq. (3.23), where we now have two
terms (ignoring the scaling factor ):
• The first term,
, is the standard sum of squared errors, which depends on
the training data.
• The second term contains the squared Euclidean norm,
, which
depends on the filter structure. In effect, this term acts as a stabilizer.
The scaling factor δ is commonly referred to as a regularization parameter, and the result-
ing modification of the cost function is correspondingly referred to as structural regu-
larization. The issue of regularization is discussed in great detail in Chapter 7.
3.4
THE WIENER FILTER
The ordinary least-squares estimator was discussed in Chapter 2, where the traditional
approach to minimization was used to find the least-squares solution from an observa-
tion model of the environment.To conform to the terminology adopted in this chapter,
we will refer to it as the least-squares filter. Moreover, we will rederive the formula for
this filter by using the Gauss–Newton method.
To proceed then, we use Eqs. (3.3) and (3.4) to define the error vector as
(3.24)
where d(n) is the n-by-1 desired response vector,
and X(n) is the n-by-M data matrix,
Differentiating the error vector e(n) with respect to w(n) yields the gradient matrix
§e(n) = -XT(n)
X(n) = [x(n), x(2), ..., x(n)]T
d(n) = [d(1), d(2), ..., d(n)]T
= d(n) - X(n)w(n)
e(n) = d(n) - [x(1), x(2), ... , x(n)]Tw(n)
7w - w(n)7 2
g
n
i=1
e2(i)
1
2
e(w)
e(w) = 1
2 e a
n
i=1
e2(i) + 7w - w(n)7 2 f
w(n + 1) = w(n) - (JT(n)J(n) + I)-1JT(n)e(n)
100
Chapter 3
The Least-Mean-Square Algorithm

Correspondingly, the Jacobian of e(n) is
(3.25)
Since the error equation (3.18) is already linear in the weight vector w(n), the
Gauss–Newton method converges in a single iteration, as shown here. Substituting Eqs.
(3.24) and (3.25) into (3.21) yields
(3.26)
The term (XT(n)X(n))-1XT(n) is called the pseudoinverse of the data matrix X(n); that is,2
(3.27)
Hence, we may rewrite Eq. (3.26) in the compact form
(3.28)
This formula represents a convenient way of stating the following:
The weight vector w(n + 1) solves the linear least-squares problem, defined over an observa-
tion interval of duration n,as the product of two terms:the pseudoinverse X(n) and the desired
response vector d(n).
Wiener Filter: Limiting Form of the Least-Squares 
Filter for an Ergodic Environment
Let wo denote the limiting form of the least-squares filter as the number of observations,n,
is allowed to approach infinity.We may then use Eq. (3.26) to write
(3.29)
Suppose now the input vector x(i) and the corresponding desired response d(i)
are drawn from a jointly ergodic environment that is also stationary.We may then sub-
stitute time averages for ensemble averages. By definition, the ensemble-averaged form
of the correlation matrix of the input vector x(i) is
(3.30)
and, correspondingly, the ensemble-averaged form of the cross-correlation vector
between the input vector x(i) and the desired response vector d(i) is
(3.31)
where  is the expectation operator. Therefore, under the ergodicity assumption, we
may now write
Rxx = lim
nS q
1
n X(n)XT(n)
rdx = [x(i)d(i)]
Rxx = [x(i)xT(i)]
= lim
nS q a 1
n XT(n)X(n) b
-1
*
lim
nS q
1
n XT(n)d(n)
= lim
nS q(XT(n)X(n))-1XT(n)d(n)
wo = lim
nS q w(n + 1)
w(n + 1) = X+(n)d(n)
X+(n) = (XT(n)X(n))-1XT(n)
= (XT(n)X(n))-1XT(n)d(n)
w(n + 1) = w(n) + (XT(n)X(n))-1XT(n)(d(n) - X(n)w(n))
J(n) = -X(n)
Section 3.4
The Wiener Filter
101

and
Accordingly, we may recast Eq. (3.29) in terms of ensemble-averaged correlation
parameters as
(3.32)
where R-1
xx is the inverse of the correlation matrix Rxx. The formula of Eq. (3.32) is the
ensemble-averaged version of the least-squares solution defined in Eq. (2.32).
The weight vector wo is called the Wiener solution to the optimum linear filtering
problem (Widrow and Stearns, 1985; Haykin, 2002). Accordingly, we may make the
statement:
For an ergodic process, the least-squares filter asymptotically approaches the Wiener filter as
the number of observations approaches infinity.
Designing the Wiener filter requires knowledge of the second-order statistics: the
correlation matrix Rxx of the input vector x(n), and the cross-correlation vector rxd
between x(n) and the desired response d(n). However, this information is not available
when the environment in which the filter operates is unknown.We may deal with such
an environment by using a linear adaptive filter, adaptive in the sense that the filter is
able to adjust its free parameters in response to statistical variations in the environment.
A highly popular algorithm for doing this kind of adjustment on a continuing-time basis
is the least-mean-square algorithm, discussed next.
3.5
THE LEAST-MEAN-SQUARE ALGORITHM
The least-mean-square (LMS) algorithm is configured to minimize the instantaneous
value of the cost function,
(3.33)
where e(n) is the error signal measured at time n. Differentiating 
with respect to
the weight vector 
yields
(3.34)
As with the least-squares filter, the LMS algorithm operates with a linear neuron, so we
may express the error signal as
(3.35)
Hence,
0e(n)
0wˆ (n) = -x(n)
e(n) = d(n) - xT(n)wˆ (n)
0e(wˆ )
0wˆ
= e(n)
0e(n)
0w
wˆ
e(wˆ )
e(wˆ ) = 1
2 e2(n)
wo = R-1
xxrdx
rdx = lim
nS qXT(n)d(n)
102
Chapter 3
The Least-Mean-Square Algorithm

and
Using this latter result as the instantaneous estimate of the gradient vector, we may write
(3.36)
Finally, using Eq. (3.36) for the gradient vector in Eq. (3.12) for the method of steepest
descent, we may formulate the LMS algorithm as follows:
(3.37)
It is also noteworthy that the inverse of the learning-rate parameter  acts as a measure
of the memory of the LMS algorithm: The smaller we make , the longer the memory
span over which the LMS algorithm remembers past data will be. Consequently, when 
is small, the LMS algorithm performs accurately, but the convergence rate of the algo-
rithm is slow.
In deriving Eq. (3.37), we have used 
in place of w(n) to emphasize the fact
that the LMS algorithm produces an instantaneous estimate of the weight vector that
would result from the use of the method of steepest-descent.As a consequence, in using
the LMS algorithm we sacrifice a distinctive feature of the steepest-descent algorithm.
In the steepest-descent algorithm, the weight vector w(n) follows a well-defined tra-
jectory in the weight space w for a prescribed . In contrast, in the LMS algorithm, the
weight vector 
traces a random trajectory. For this reason, the LMS algorithm is
sometimes referred to as a “stochastic gradient algorithm.”As the number of iterations
in the LMS algorithm approaches infinity,
performs a random walk (Brownian
motion) about the Wiener solution wo.The important point to note, however, is the fact
that, unlike the method of steepest descent, the LMS algorithm does not require knowl-
edge of the statistics of the environment. This feature of the LMS algorithm is impor-
tant from a practical perspective.
A summary of the LMS algorithm, based on Eqs. (3.35) and (3.37), is presented in
Table 3.1, which clearly illustrates the simplicity of the algorithm. As indicated in this
table, initialization of the algorithm is done by simply setting the value of the weight
vector 
.
wˆ (0) = 0
wˆ (n)
wˆ (n)
wˆ (n)
wˆ (n + 1) = wˆ (n) + x(n)e(n)
gˆ (n) = -x(n)e(n)
0e(wˆ )
0wˆ (n) = -x(n)e(n)
Section 3.5
The Least-mean-Square Algorithm
103
TABLE 3.1
Summary of the LMS Algorithm
Training Sample:
Input signal vector  x(n)
Desired response  d(n)
User-selected parameter: 
Initialization. Set 
.
wˆ (0) = 0
Computation. For 
1, 2, ..., compute
n =
e(n) = d(n) - wˆ T(n)x(n)
wˆ (n + 1) = wˆ (n) + x(n)e(n)

Signal-Flow Graph Representation of the LMS Algorithm
By combining Eqs. (3.35) and (3.37), we may express the evolution of the weight vector
in the LMS algorithm as
(3.38)
where I is the identity matrix. In using the LMS algorithm, we recognize that
(3.39)
where z-1 is the unit-time delay operator, implying storage. Using Eqs. (3.38) and (3.39),
we may thus represent the LMS algorithm by the signal-flow graph depicted in Fig. 3.3.
This signal-flow graph reveals that the LMS algorithm is an example of a stochastic feed-
back system.The presence of feedback has a profound impact on the convergence behav-
ior of the LMS algorithm.
3.6
MARKOV MODEL PORTRAYING THE DEVIATION OF THE LMS
ALGORITHM FROM THE WIENER FILTER
To perform a statistical analysis of the LMS algorithm, we find it more convenient to
work with the weight-error vector, defined by
(3.40)
where wo is the optimum Wiener solution defined by Eq. (3.32) and 
is the
corresponding estimate of the weight vector computed by the LMS algorithm. Thus,
wˆ (n)
(n) = wo - wˆ (n)
wˆ (n) = z-1[wˆ (n + 1)]
= [I - x(n)xT(n)]wˆ(n) + x(n)d(n)
wˆ(n + 1) = wˆ (n) + x(n)[d(n) - xT(n)wˆ(n)]
104
Chapter 3
The Least-Mean-Square Algorithm
hx(n) d(n)
hx(n) xT(n)
w(n  1)



w(n)
z1I
ˆ
ˆ
Σ
FIGURE 3.3
Signal-flow graph
representation of the LMS
algorithm.The graph embodies
feedback depicted in color.

in terms of 
, assuming the role of a state, we may rewrite Eq. (3.38) in the com-
pact form
(3.41)
Here, we have
(3.42)
where I is the identity matrix.The additive noise term in the right-hand side of Eq.(3.41)
is defined by
(3.43)
where
(3.44)
is the estimation error produced by the Wiener filter.
Equation (3.41) represents a Markov model of the LMS algorithm, with the model
being characterized as follows:
• The updated state of the model,denoted by the vector 
,depends on the old
state 
, with the dependence itself being defined by the transition matrix A(n).
• Evolution of the state over time n is perturbed by the intrinsically generated noise
f(n), which acts as a “driving force”.
Figure 3.4 shows a vector-valued signal-flow graph representation of this model. The
branch labeled z-1I represents the memory of the model, with z-1 acting as the unit-time
delay operator, as shown by
(3.45)
This figure highlights the presence of feedback in the LMS algorithm in a more compact
manner than that in Fig. 3.3.
The signal-flow graph of Fig. 3.4 and the accompanying equations provide the
framework for the convergence analysis of the LMS algorithm under the assumption
of a small learning-rate parameter . However, before proceeding with this analysis, we
will digress briefly to present two building blocks with that goal in mind: the Langevin
equation, presented in Section 3.7, followed by Kushner’s direct-averaging method,
presented in Section 3.8. With those two building blocks in hand, we will then go on
to study convergence analysis of the LMS algorithm in Section 3.9.
z-1[ (n + 1)] =  (n)
 (n)
 (n + 1)
eo(n) = d(n) - wT
ox(n)
f(n) = - x(n)eo(n)
A(n) = I - x(n)xT(n)
 (n + 1) = A(n) (n) + f(n)
 (n)
Section 3.6
Markov Model Portraying the Deviation of the LMS Algorithm
105
FIGURE 3.4
Signal-flow graph
representation of the Markov
model described in Eq. (3.41); the
graph embodies feedback depicted
in color.

 (n)
A(n)
(n
f
)
z
1I
 (n
 1)
d
d

3.7
THE LANGEVIN EQUATION: CHARACTERIZATION 
OF BROWNIAN MOTION
Restating the remarks made towards the end of Section 3.5 in more precise terms
insofar as stability or convergence is concerned, we may say that the LMS algorithm
(for small enough ) never attains a perfectly stable or convergent condition. Rather,
after a large number of iterations, n, the algorithm approaches a “pseudo-equilibrium”
condition, which, in qualitative terms, is described by the algorithm executing Brownian
motion around the Wiener solution.This kind of stochastic behavior is explained nicely
by the Langevin equation of nonequilibrium thermodynamics.3 So, we will make a brief
digression to introduce this important equation.
Let v(t) denote the velocity of a macroscopic particle of mass m immersed in a
viscous fluid. It is assumed that the particle is small enough for its velocity due to ther-
mal fluctuations deemed to be significant. Then, from the equipartition law of thermo-
dynamics, the mean energy of the particle is given by
(3.46)
where kB is Boltzmann’s constant and T is the absolute temperature.The total force exerted
on the particle by the molecules in the viscous fluid is made up of two components:
(i) a continuous damping force equal to -v(t) in accordance with Stoke’s law, where
 is the coefficient of friction;
(ii) a fluctuating force Ff(t), whose properties are specified on the average.
The equation of motion of the particle in the absence of an external force is therefore
given by
Dividing both sides of this equation by m, we get
(3.47)
where
(3.48)
and
(3.49)
The term Γ(t) is the fluctuating force per unit mass; it is a stochastic force because it depends
on the positions of the incredibly large number of atoms constituting the particle, which
are in a state of constant and irregular motion. Equation (3.47) is called the Langevin
equation, and Γ(t) is called the Langevin force. The Langevin equation, which describes
the motion of the particle in the viscous fluid at all times (if its initial conditions are spec-
ified), was the first mathematical equation describing nonequilibrium thermodynamics.
(t) =
Ff(t)
m
 = 
m
dv
dt = -v(t) + (t)
m dv
dt = -v(t) + Ff(t)
1
2 [v2(t)] = 1
2 kBT  for all continuous time t
106
Chapter 3
The Least-Mean-Square Algorithm

In Section 3.9, we show that a transformed version of the LMS algorithm has the
same mathematical form as the discrete-time version of the Langevin equation. But,
before doing that, we need to describe our next building block.
3.8
KUSHNER’S DIRECT-AVERAGING METHOD
The Markov model of Eq. (3.41) is a nonlinear stochastic difference equation. This equa-
tion is nonlinear because the transition matrix A(n) depends on the outer product
x(n)xT(n) of the input vector x(n). Hence, the dependence of the weight-error vector
on x(n) violates the principle of superposition, which is a requirement for lin-
earity. Moreover, the equation is stochastic because the training sample {x(n), d(n)} is
drawn from a stochastic environment. Given these two realities, we find that a rigorous
statistical analysis of the LMS algorithm is indeed a very difficult task.
However,under certain conditions,the statistical analysis of the LMS algorithm can
be simplified significantly by applying Kushner’s direct-averaging method to the model of
Eq.(3.41).For a formal statement of this method,we write the following (Kushner,1984):
Consider a stochastic learning system described by the Markov model
where, for some input vector x(n), we have
and the additive noise f(n) is linearly scaled by the learning-rate parameter . Provided that
• the learning-rate parameter  is sufficiently small, and
• the additive noise f(n) is essentially independent of the state 
, the state evolution of a
modified Markov model described by the two equations
(3.50)
(3.51)
is practically the same as that of the original Markov model for all n.
The deterministic matrix 
of Eq. (3.51) is the transition matrix of the modified
Markov model. Note also that we have used the symbol 
for the state of the mod-
ified Markov model to emphasize the fact that the evolution of this model over time is
identically equal to that of the original Markov model only for the limiting case of a
vanishingly small learning-rate parameter .
A proof of the statement embodying Eqs. (3.50) and (3.51) is addressed in Prob-
lem 3.7, assuming ergodicity (i.e., substituting time averages for ensemble averages).
For the discussion presented herein, it suffices to say the following:
1. As mentioned previously, when the learning-rate parameter  is small, the LMS
algorithm has a long memory. Hence, the evolution of the updated state
can be traced in time, step by step, all the way back to the initial condition
.
2. When  is small, we are justified in ignoring all second- and higher-order
terms in  in the series expansion of
.
 0(n + 1)
 (0)
0(n + 1)
 0(n)
A(n)
A(n) = I - [x(n)xT(n)]
 0(n + 1) = A(n) 0(n) + f0(n)
 (n)
A(n) = I - x(n)xT(n)
 (n + 1) = A(n)(n) + f(n)
 (n + 1)
Section 3.8
Kushner’s Direct-Averaging Method
107

3. Finally, the statement embodied in Eqs. (3.50) and (3.51) is obtained by invoking
ergodicity, whereby ensemble averages are substituted for time agerages.
3.9
STATISTICAL LMS LEARNING THEORY FOR SMALL 
LEARNING-RATE PARAMETER
Now that we are equipped with Kushner’s direct-averaging method, the stage is set for
a principled statistical analysis of the LMS algorithm by making three justifiable
assumptions:
Assumption I: The learning-rate parameter  is small
By making this assumption, we justify the application of Kushner’s direct-averaging
method—hence the adoption of the modified Markov model of Eqs. (3.50) and (3.51)
as the basis for the statistical analysis of the LMS algorithm.
From a practical perspective, the choice of small  also makes sense. In particular,
the LMS algorithm exhibits its most robust behavior with respect to external distur-
bances when  is small; the issue of robustness is discussed in Section 3.12.
Assumption II: The estimation error eo(n) produced 
by the Wiener filter is white.
This assumption is satisfied if the generation of the desired response is described by the
linear regression model
(3.52)
Equation (3.52) is simply a rewrite of Eq. (3.44), which, in effect, implies that the
weight vector of the Wiener filter is matched to the weight vector of the regression
model describing the stochastic environment of interest.
Assumption III: The input vector x(n) and the desired 
response d(n) are jointly Gaussian
Stochastic processes produced by physical phenomena are frequently mechanized
such that a Gaussian model is appropriate—hence the justification for the third
assumption.
No further assumptions are needed for the statistical analysis of the LMS algorithm
(Haykin, 2002, 2006). In what follows, we present a condensed version of that analysis.
Natural Modes of the LMS Algorithm
Let Rxx denote the ensemble-averaged correlation matrix of the input vector x(n),drawn
from a stationary process; that is,
(3.53)
Rxx = [x(n)xT(n)]
d(n) = wT
ox(n) + eo(n)
108
Chapter 3
The Least-Mean-Square Algorithm

Correspondingly, we may express the averaged transition matrix in Eq. (3.51) pertain-
ing to the modified Markov model as
(3.54)
We may therefore expand Eq. (3.50) into the form
(3.55)
where 
is the addition noise.Henceforth,Eq. (3.55) is the equation on which the sta-
tistical analysis of the LMS algorithm is based.
Natural Modes of the LMS Algorithm
Applying the orthogonality transformation of matrix theory4 to the correlation matrix
Rxx, we write
(3.56)
where Q is an orthogonal matrix whose columns are the eigenvectors of Rxx, and  is a
diagonal matrix whose elements are the associated eigenvalues. Extending the applica-
tion of this transformation to the difference equation Eq. (3.55) yields the  correspond-
ing system of decoupled first-order equations (Haykin, 2002, 2006)
(3.57)
where M is the dimensionality of the weight vector 
. Moreover, vk(n) is the kth
element of the transformed weight-error vector
(3.58)
and, correspondingly,
is the kth element of the transformed noise vector
(3.59)
More specifically,
is the sample function of a white-noise process of zero mean and
k(n)
	(n) = QTf0(n)
k(n)
v(n) = QT 0(n)
wˆ (n)
vk(n + 1) = (1 - 
k)vk(n) + k(n),  k = 1, 2, ..., M
QTRxxQ = 
f0(n)
 0(n + 1) = (I - Rxx) 0(n) + f0(n)
= [I - Rxx]
A = [I - x(n)xT(n)]
Section 3.9
Statistical LMS Learning Theory for Small Learning-Rate Parameter
109
variance µ2Jmin
k,where Jmin is the minimum mean-square error produced by the Wiener
filter. In effect, the variance of the zero-mean driving force for the kth difference
equation Eq. (3.57) is proportional to the kth eigenvalue of the correlation matrix Rxx,
namely, 
k.
Define the difference
(3.60)
We may then recast Eq. (3.57) in the form
(3.61)
The stochastic equation Eq. (3.61) is now recognized as the discrete-time version of the
Langevin equation Eq. (3.47). In particular, as we compare these two equations, term by
term, we construct the analogies listed in Table 3.2. In light of this table, we may now
make the following important statement:
¢vk(n) = -
kvk(n) + k(n)  for k = 1, 2, ..., M
¢vk(n) = vk(n + 1) - vk(n)  for k = 1, 2, ..., M

The convergence behavior of the LMS filter resulting from application of the orthogonality
transformation to the difference equation Eq. (3.55) is described by a system of M decou-
pled Langevin equations whose kth component is characterized as follows:
• damping force is defined by 
kvk(n);
• Langevin force k(n) is described by a zero-mean white-noise process with the variance
2Jmin
k.
Most important, the Langevin force 
k(n) is responsible for the nonequilibrium behavior
of the LMS algorithm, which manifests itself in the form of Brownian motion performed
by the algorithm around the optimum Wiener solution after a large enough number of
iterations n. It must, however, be stressed that the findings summarized in Table 3.2 and
the foregoing statement rest on the premise that the learning-rate parameter  is small.
Learning Curves of the LMS Algorithm
Following through the solution of the transformed difference equation Eq. (3.57), we
arrive at the LMS learning curve described by Haykin, (2002, 2006),
(3.62)
where
is the mean-square error and vk(0) is the initial value of the kth element of the trans-
formed vector v(n). Under the assumption that the learning-rate parameter  is small,
Eq. (3.62) simplifies to
(3.63)
The practical validity of the small-learning-rate-parameter theory presented in
this section is demonstrated in the computer experiment presented next.
3.10
COMPUTER EXPERIMENT I: LINEAR PREDICTION
The objective of this experiment is to verify the statistical learning theory of the LMS
algorithm described in Section 3.9, assuming a small learning-rate parameter .
J(n) L Jmin + Jmin
2
a
M
k=1

k + a
M
k=1

k a vk(0)2 - Jmin
2
b(1 - 
k)2n
J(n) = [e(n)2]
J(n) = Jmin + Jmin a
M
k=1

k
2 - 
k
+ a
M
k=1

k avk(0)2 -
Jmin
2 - 
k
b (1 - 
k)2n

110
Chapter 3
The Least-Mean-Square Algorithm
TABLE 3.2
Analogies between the Langevin equation (in continuous time)
and the transformed LMS evolution (in discrete time)
Langevin equation Eq. (3.47)
LMS evolution Eq. (3.61)
(acceleration)
dv(t)
dt
∆vk(n)
γv(t)
(damping force)

kvk(n)
k(n)
Γ(t)
(stochastic driving force)

For the experiment, we consider a generative model defined by
(3.64)
which represents an autoregressive (AR) process of order one. The model being of first
order, a is the only parameter of the model.The explanational error 
is drawn from
a zero-mean white-noise process of variance σ2
e.The generative model is parameterized
as follows:
To estimate the model parameter a, we use the LMS algorithm characterized by
the learning-rate parameter   0.001. Starting with the initial condition 
, we
apply the scalar version of Eq. (3.35), where the estimation error
and where 
is the estimate of a produced by the LMS algorithm at time n.Then,per-
forming 100 statistically independent application of the LMS algorithm, we plot the
ensemble-averaged learning curve of the algorithm.The solid (randomly varying) curve
plotted in Fig. 3.5 for 5,000 iterations is the result of this ensemble-averaging operation.
In Fig. 3.5, we have also included the result of computing the ensemble-averaged
learning curve by using the theoretically derived formula of Eq.(3.63),assuming a small .
It is remarkable to see perfect agreement between theory and practice, as evidenced by
aˆ (n)
e(n) = x(n) - aˆ(n)x(n - 1)
wˆ (0) = 0
2
x = 0.995
2
 = 0.02
a = 0.99
(n)
x(n) = ax(n - 1) + (n)
Section 3.10
Computer Experiment I: Linear Prediction
111
FIGURE 3.5
Experimental
verification of the small-
learning-rate-parameter
theory of the LMS algorithm
applied to an autoregressive
process of order one.
102
0
1,000
2,000
3,000
4,000
5,000
101
100
101
Number of iterations, n
Learning-rate parameter h  0.001
Mean–square error
Theory
Experiment

the results plotted in Fig.3.6.Indeed,this remarkable agreement should be viewed as the
confirmation of two important theoretical principles:
1. Kushner’s method may be used to tackle the theoretical analysis of the LMS learn-
ing behavior under the assumption of a small learning-rate parameter.
2. The LMS algorithm’s learning behavior may be explained as an instance of
Langevin’s equation.
3.11
COMPUTER EXPERIMENT II: PATTERN CLASSIFICATION
For the second experiment on the LMS algorithm, we study the algorithm’s application
to the double-moon configuration pictured in Fig. 1.8. To be more specific, the perfor-
mance of the algorithm is evaluated for two settings of the double-moon configuration:
(i) d  1, corresponding to linear separability;
(ii) d  -4, corresponding to nonlinear separability.
In doing so, in effect, we are repeating the experiment performed in Section 2.5 on the
method of least squares, except this time we use the LMS algorithm.
The results of the experiment pertaining to these two values of d are presented in
Figs. 3.6 and 3.7, respectively. Comparing these two figures with Figs. 2.2 and 2.3, we may
make the following observations:
(a) Insofar as classification performance is concerned, the method of least squares
and the LMS algorithm yield results that are identical for all practical purposes.
112
Chapter 3
The Least-Mean-Square Algorithm
x2
x1
10
5
–10
–5
0
5
10
15
20
0
–5
–10






















































































 








































 







































 


















 
















 
 























 





























 












 

































 




















































 





































































































 














































 











































 




















 



























 
 
























 






























 












 



































 












































 









 









Classification using LMS with distance  1, radius  10, and width  6
FIGURE 3.6
LMS classification
with distance 1, based on the
double-moon configuration of
Fig. 1.8.

(b) In terms of convergence, the LMS algorithm is much slower than the method of
least squares.This difference is attributed to the fact that the LMS algorithm is re-
cursive, whereas the method of least squares operates in a batch mode that in-
volves matrix inversion in one time-step.
As a matter of interest, in Chapter 5, we present a recursive implementation of
the method of least squares. On account of using second-order information, recursive
implementation of the method of least squares remains faster in its convergence behav-
ior than the LMS algorithm.
3.12
VIRTUES AND LIMITATIONS OF THE LMS ALGORITHM
Computational Simplicity and Efficiency
Two virtues of the LMS algorithm are computational simplicity and efficiency, both
of which are exemplified by the following summary of the algorithm presented in
Table 3.1:
• Coding of the algorithm is composed of two or three lines,which is as simple as any-
one could get.
• Computational complexity of the algorithm is linear in the number of adjustable
parameters.
From a practical perspective, these are important virtues.
Section 3.12
Virtues and Limitations of the LMS Algorithm
113
x2
8
6
4
10
5
20
15
10
5
0
2
0
2
4
6
8
10
12
x1
Classification using LMS with distance  4, radius  10, and width  6
FIGURE 3.7
LMS classification
with distance 4, based on the
double-moon configuration of 
Fig. 1.8.

Robustness
Another important virtue of the LMS algorithm is that it is model independent and
therefore robust with respect to disturbances. To explain what we mean by robustness,
consider the situation depicted in Fig. 3.8, where a transfer operator T maps a couple
of disturbances at its input into a “generic” estimation error at the output. Specifically,
at the input, we have the following:
• An initial weight-error vector defined by
(3.65)
where w is an unknown parameter vector and wˆ (0) is its “proposed” initial esti-
mate at time n  0. In the LMS algorithm, we typically set wˆ (0)  0, which, in a
way, is the worst possible initializing condition for the algorithm.
• An explanational error
that traces back to the regression model of Eq. (2.3),
reproduced here for convenience of presentation, where d is the model output
produced in response to the regressor x:
(3.66)
Naturally,the operator T is a function of the strategy used to construct the estimate 
(e.g., the LMS algorithm).We may now introduce the following definition:
The energy gain of the estimator is defined as the ratio of the error energy at the output of the
operator T to the total disturbance energy at the input.
To remove this dependence and thereby make the estimator “model independent,” we
consider the scenario where we have the largest possible energy gain over all conceivable
disturbance sequences applied to the estimator input. In so doing, we will have defined
the H
norm of the transfer operator T.
With this brief background,we may now formulate what the H
norm of the trans-
fer operator T is about:
Find a causal estimator that minimizes the H
norm of T, where T is a transfer operator
that maps the disturbances to the estimation errors.
The optimal estimator designed in accordance with the H
criterion is said to be
of a minimax kind. More specifically, we may view the H
optimal estimation problem
as a “game-theoretic problem” in the following sense: Nature, acting as the “opponent,”
has access to the unknown disturbances, thereby maximizing the energy gain. On the
other hand, the “designer” of the estimation strategy has the task of finding a causal
algorithm for which the error energy is minimized. Note that in introducing the idea of
the H
criterion, we made no assumptions about the disturbances indicated at the input
of Fig. 3.8.We may therefore say that an estimator designed in accordance with the H
criterion is a worst-case estimator.
q
q
q
q
q
q
q
wˆ (n)
d = wTx + 

w(0) = w - ˆw(0)
114
Chapter 3
The Least-Mean-Square Algorithm
Disturbance
e(n)
Generic
estimation
error
Initial
weight-error
vector w(0)
Transfer
operator
T
FIGURE 3.8
Formulation of the
optimal Hq estimation problem.The
generic estimation error at the
transfer operator’s output could be the
weight-error vector, the explanational
error, etc.

In precise mathematical terms, the LMS algorithm is optimal in accordance with the
H
(or minimax) criterion.5 The basic philosophy of optimality in the H
sense is to
cater to the worst-case scenario:
If you do not know what you are up against, plan for the worst scenario and optimize.
For a long time, the LMS algorithm was regarded as an instantaneous approximation to
the gradient-descent algorithm. However, the H
optimality of LMS algorithm provides
this widely used algorithm with a rigorous footing. Moreover, the H
theory of the LMS
algorithm shows that the most robust performance of the algorithm is attained when
the learning-rate parameter  is assigned a small value.
The model-independent behavior of the LMS algorithm also explains the ability
of the algorithm to work satisfactorily in both a stationary and a nonstationary envi-
ronment. By a “nonstationary” environment, we mean an environment in which the
statistics vary with time. In such an environment, the optimum Wiener solution takes on
a time-varying form, and the LMS algorithm has the additional task of tracking varia-
tions in the minimum mean-square error of the Wiener filter.
Factors Limiting the LMS Performance
The primary limitations of the LMS algorithm are its slow rate of convergence and its
sensitivity to variations in the eigenstructure of the input (Haykin, 2002).The LMS algo-
rithm typically requires a number of iterations equal to about 10 times the dimension-
ality of the input data space for it to reach a steady-state condition. The slow rate of
convergence of the LMS algorithm becomes particularly serious when the dimension-
ality of the input data space becomes high.
As for sensitivity to changes in environmental conditions, convergence behavior
of the LMS algorithm is particularly sensitive to variations in the condition number, or
eigenvalue spread, of the correlation matrix Rxx of the input vector x. The condition
number of Rxx, denoted by (R), is defined by
(3.67)
where 
max and 
min are the maximum and minimum eigenvalues of the correlation
matrix Rxx, respectively. The sensitivity of the LMS algorithm to variations in the con-
dition number (R) becomes particularly acute when the training sample to which the
input vector x(n) belongs is ill conditioned—that is, when the condition number of the
LMS algorithm is high.6
3.13
LEARNING-RATE ANNEALING SCHEDULES
The slow-rate convergence encountered with the LMS algorithm may be attributed to
the fact that the learning-rate parameter is maintained constant at some value 
throughout the computation, as shown by
(3.68)
(n) = 0  for all n
0
(R) = 
max

min
q
q
q
q
Section 3.13
Learning-Rate Annealing Schedules
115

This is the simplest possible form the learning-rate parameter can assume. In contrast,
in stochastic approximation, which goes back to the classic paper by Robbins and
Monro (1951), the learning-rate parameter is time varying. The particular time-
varying form most commonly used in the stochastic approximation literature is
described by
(3.69)
where c is a constant. Such a choice is indeed sufficient to guarantee convergence of the
stochastic approximation algorithm (Kushner and Clark, 1978). However, when the con-
stant c is large, there is a danger of parameter blowup for small n.
As an alternative to Eqs. (3.68) and (3.69), we may use the search-then-converge
schedule, described by Darken and Moody (1992), as 
(3.70)
where 0 and τ are user-selected constants. In the early stages of adaptation involving a
number of iterations n that is small compared with the search-time constant τ,the learning-
rate parameter (n) is approximately equal to 0, and the algorithm operates essentially
as the “conventional” LMS algorithm, as indicated in Fig. 3.9. Hence, by choosing a high
value for 0 within the permissible range, we hope that the adjustable weights of the fil-
ter will find and hover about a “good” set of values.Then, for a number n of iterations
that is large compared with the search-time constant τ, the learning-rate parameter (n)
(n) =
0
1 + (n)
(n) = c
n
116
Chapter 3
The Least-Mean-Square Algorithm
Search-then-converge
schedule
Slope  c
Stochastic
approximation
schedule
(log scale)
n
0.10
0
(n)
t
0.010
FIGURE 3.9
Learning-rate annealing
schedules:The horizontal axis, printed in
color, pertains to the standard LMS
algorithm.

approximates as c/n, where c  τ0, as illustrated in Fig. 3.9. The algorithm now oper-
ates as a traditional stochastic approximation algorithm, and the weights may converge
to their optimum values. Thus, the search-then-converge schedule has the potential to
combine the desirable features of the standard LMS algorithm with traditional sto-
chastic approximation theory.
3.14
SUMMARY AND DISCUSSION
In this chapter,we studied the celebrated least-mean-square (LMS) algorithm,developed
by Widrow and Hoff in 1960. Since its inception, this algorithm has withstood the test
of time for a number of important practical reasons:
1. The algorithm is simple to formulate and just as simple to implement, be it in hard-
ware or software form.
2. In spite of its simplicity, the algorithm is effective in performance.
3. Computationally speaking, the algorithm is efficient in that its complexity follows
a linear law with respect to the number of adjustable parameters.
4. Last, but by no means least, the algorithm is model independent and therefore
robust with respect to disturbances.
Under the assumption that the learning-rate parameter  is a small positive quan-
tity, the convergence behavior of the LMS algorithm—usually difficult to analyze—
becomes mathematically tractable, thanks to Kushner’s direct-averaging method. The
theoretical virtue of this method is that when  is small, the nonlinear “stochastic”
difference equation, which describes the convergence behavior of the LMS algorithm,
is replaced by a nonlinear “deterministic” version of the original equation. Moreover,
through the clever use of eigendecomposition, the solution of the resulting nonlinear
deterministic equation is replaced by a system of decoupled first-order difference
equations.The important point to note here is that the first-order difference equation
so derived is mathematically identical to the discrete-time version of the Langevin
equation of nonequilibrium thermodynamics. This equivalence explains the Brown-
ian motion executed by the LMS algorithm around the optimum Wiener solution after
a large  enough number of iterations. The computer experiment presented in Section
3.10 and other computer experiments presented in Haykin (2006) confirm the valid-
ity of Eq. (3.63), which describes the ensemble-averaged learning curve of the LMS
algorithm.
It is also noteworthy that the LMS algorithm exhibits its most robust performance
when the learning-rate parameter  is small.However,the price paid for this kind of prac-
tically important performance is a relatively slow rate of convergence.To some extent,
this limitation of the LMS algorithm can be alleviated through the use of learning-rate
annealing, as described in Section 3.13.
One last comment is in order.Throughout the chapter, we focused attention on the
ordinary LMS algorithm. Needless to say, the algorithm has several variants, each of
which offers a practical virtue of its own; for details, the interested reader is referred to
(Haykin, 2002).
Section 3.14
Summary and Discussion
117

NOTES AND REFERENCES
1. Differentiation with respect to a vector
Let f(w) denote a real-valued function of parameter vector w. The derivative of f(w) with
respect to w is defined by the vector
where m is the dimension of vector w. The following two cases are of special interest:
Case 1 The function f(w) is defined by the inner product:
Hence,
or, equivalently, in matrix form,
(3.71)
Case 2 The function f(w) is defined by the quadratic form:
Here, rij is the ij-th element of the m-by-m matrix R. Hence,
or, equivalently, in matrix form,
(3.72)
Equations (3.71) and (3.72) provide two useful rules for the differentiation of a real-valued
function with respect to a vector.
2. The pseudoinverse of a rectangular matrix is discussed in Golub and Van Loan (1996); see
also Chapter 8 of Haykin (2002).
3. The Langevin equation is discussed in Reif (1965). For a fascinating historical account of
the Langevin equation, see the tutorial paper on noise by Cohen (2005).
4. The orthogonality transformation described in Eq. (3.56) follows from the eigendecompo-
sition of a square matrix.This topic is described in detail in Chapter 8.
0f
0w = 2Rw
0f
0wi
= 2 a
m
j=1
rijwj,  i = 1, 2, ...., m
 = a
m
i=1 a
m
j=1
wi rij wj
 f(w) = wTRw
0f
0w = x
0f
0wi
= xi,  i = 1, 2, ...., m
 = a
m
i=1
xiwi
 f(w) = xTw
0f
0w = c 0f
0w1
, 0f
0w2
, p, 0f
0wm
d
T
118
Chapter 3
The Least-Mean-Square Algorithm

5. For an early (and perhaps the first) motivational treatment of H
control, the reader is
referred to Zames (1981).
The first exposition of optimality of the LMS algorithm in the H
sense was presented
in Hassibi et al. (1993). Hassibi et al. (1999) treat the H
theory from an estimation or
adaptive-filtering perspective. Hassibi also presents a condensed treatment of robustness
of the LMS algorithm in the H
sense in Chapter 5 of Haykin and Widrow (2005).
For books on H
theory from a control perspective, the reader is referred to Zhou
and Doyle (1998) and Green and Limebeer (1995).
6. Sensitivity of convergence behavior of the LMS algorithm to variations in the condition
number of the correlation matrix Rxx, denoted by (R), is demonstrated experimentally in
Section 5.7 of the book by Haykin (2002). In Chapter 9 of Haykin (2002), which deals with
recursive implementation of the method of least squares, it is also shown that convergence
behavior of the resulting algorithm is essentially independent of the condition number (R).
PROBLEMS
3.1 (a) Let m(n) denote the mean weight vector of the LMS algorithm at iteration n; that is,
Using the small-learning-rate parameter theory of Section 3.9, show that
where  is the learning-rate parameter, Rxx is the correlation matrix of the input vec-
tor x(n), and m(0) and 
are the initial and final values of m(n), respectively.
(b) Show that for convergence of the LMS algorithm in the mean, the learning-rate
parameter  must satisfy the condition
where 
is the largest eigenvalue of the correlation matrix Rxx.
3.2 Continuing from Problem 3.1, discuss why convergence of the LMS algorithm in the mean
is not an adequate criterion for convergence in practice.
3.3 Consider the use of a white-noise sequence of zero mean and variance σ2 as the input to the LMS
algorithm.Determine the condition for convergence of the algorithm in the mean-square sense.
3.4 In a variant of the LMS algorithm called the leaky LMS algorithm, the cost function to be
minimized is defined by
where w(n) is the parameter vector, e(n) is the estimation error, and 
 is a constant. As in
the ordinary LMS algorithm, we have
where d(n) is the desired response corresponding to the input vector x(n).
e(n) = d(n) - wT(n)x(n)
e(n) = 1
2 e(n)2 + 1
2 
7 w(n)7 2

max
O 6  6
2

max
m(q)
m(n) = (I - Rxx)n[m(0) - m(q)] + m(q)
m(n) = [wˆ (n)]
q
q
q
q
q
Problems
119

(a) Show that the time update for the parameter vector of the leaky LMS algorithm is
defined by
which includes the ordinary LMS algorithm as a special case.
(b) Using the small learning-rate parameter theory of Section 3.9, show that
where Rxx is the correlation matrix of x(n), I is the identity matrix, and rdx is the cross-
correlation vector between x(n) and d(n).
3.5 Continuing from Problem 3.4, verify that the leaky LMS algorithm can be “simulated” by
adding white noise to the input vector x(n).
(a) What should variance of this noise be for the condition in part (b) of Problem 3.4 to hold?
(b) When will the simulated algorithm take a form that is practically the same as the leaky
LMS algorithm? Justify your answer.
3.6 An alternative to the mean-square error (MSE) formulation of the learning curve that we
sometimes find in the literature is the mean-square deviation (MSD) learning curve. Define
the weight-error vector
where w is the parameter vector of the regression model supplying the desired response.This
second learning curve is obtained by computing a plot of the MSD
versus the number of iterations n.
Using the small-learning-rate-parameter theory of Section 3.9, show that 
where  is the learning-rate parameter, M is the size of the parameter vector , and Jmin is
the minimum mean-square error of the LMS algorithm.
3.7 In this problem, we address a proof of the direct-averaging method, assuming ergodicity.
Start with Eq. (3.41), which defines the weight-error vector
in terms of the transi-
tion matrix A(n) and driving force f(n), which are themselves defined in terms of the input
vector x(n) in Eqs. (3.42) and (3.43), respectively; then proceed as follows:
• Set n  0, and evaluate  
.
 (1)
 (n)
wˆ
= 1
2 MJmin
D(q) = lim
nS q D(n)
D(n) = [(n)2]
 (n) = w - wˆ (n)
lim
xS q[wˆ (n)] = (Rxx + 
I)-1rdx
wˆ (n + 1) = (1 - 
)wˆ (n) + x(n)e(n)
120
Chapter 3
The Least-Mean-Square Algorithm
• Set n  1, and evaluate  
.
• Continue in this fashion for a few more iterations.
 (2)
With these iterated values of 
at hand, deduce a formula for the transition matrix A(n).
 (n)
Next, assume that the learning-rate parameter  is small enough to justify retaining only
the terms that are linear in . Hence, show that
A(n) = I -  a
n
i=1
x(i)xT(i)

which, assuming ergodicity, takes the form
3.8 When the learning-rate parameter  is small, the LMS algorithm acts like a low-pass filter
with a small cutoff frequency. Such a filter produces an output that is proportional to the
average of the input signal.
Using Eq.(3.41),demonstrate this property of the LMS algorithm by considering the sim-
ple example of the algorithm using a single parameter.
3.9 Starting with Eq.(3.55) for a small learning-rate parameter,show that under steady-state con-
ditions, the Lyapunov equation
holds, where we have
and
for i  0, 1, 2, ....The matrix P0 is defined by 
, and eo(n) is the irreducible esti-
mation error produced by the Wiener filter.
Computer Experiments
3.10 Repeat the computer experiment of Section 3.10 on linear prediction for the following val-
ues of the learning-rate parameter:
(i)
  0.002;
(ii)   0.01;
(iii)   0.02.
Comment on your findings in the context of applicability of the small-learning-rate-
parameter theory of the LMS algorithm for each value of .
3.11 Repeat the computer experiment of Section 3.11 on pattern classification for the distance
of separation between the two moons of Fig. 1.8 set at d = 0. Compare the results of your
experiment with those in Problem 1.6 on the perceptron and Problem 2.7 on the method of
least squares.
3.12 Plot the pattern-classification learning curves of the LMS algorithm applied to the double-
moon configuration of Fig.1.8 for the following values assigned to the distance of separation:
d  1
d  0
d  4
Compare the results of the experiment with the corresponding ones obtained using Rosen-
blatt’s perceptron in Chapter 1.
[o(n)T
o (n)]
R(i) = [x(n)xT(n - i)]
J(i)
min = [eo(n)eo(n - i)]
RP0(n) + P0(n)R =  a
q
i=0
J(i)
min R(i)
A(n) = I - Rxx
Problems
121

122
ORGANIZATION OF THE CHAPTER
In this chapter, we study the many facets of the multilayer perceptron, which stands for
a neural network with one or more hidden layers.After the introductory material pre-
sented in Section 4.1, the study proceeds as follows:
1. Sections 4.2 through 4.7 discuss matters relating to back-propagation learning.We begin
with some preliminaries in Section 4.2 to pave the way for the derivation of the back-
propagation algorithm.This section also includes a discussion of the credit-assignment
problem. In Section 4.3, we describe two methods of learning: batch and on-line. In
Section 4.4, we present a detailed derivation of the back-propagation algorithm, using
the chain rule of calculus;we take a traditional approach in this derivation.In Section 4.5,
we illustrate the use of the back-propagation algorithm by solving the XOR problem,
an interesting problem that cannot be solved by Rosenblatt’s perceptron. Section 4.6
presents some heuristics and practical guidelines for making the back-propagation
algorithm perform better. Section 4.7 presents a pattern-classification experiment on
the multilayer perceptron trained with the back-propagation algorithm.
2. Sections 4.8 and 4.9 deal with the error surface. In Section 4.8, we discuss the fun-
damental role of back-propagation learning in computing partial derivatives of a
network-approximating function.We then discuss computational issues relating to
the Hessian of the error surface in Section 4.9. In Section 4.10, we discuss two
issues: how to fulfill optimal annealing and how to make the learning-rate pa-
rameter adaptive.
3. Sections 4.11 through 4.14 focus on various matters relating to the performance of a
multilayer perceptron trained with the back-propagation algorithm.In Section 4.11,we
discuss the issue of generalization—the very essence of learning.Section 4.12 addresses
the approximation of continuous functions by means of multiplayer perceptrons.The use
of cross-validation as a statistical design tool is discussed in Section 4.13.In Section 4.14,
we discuss the issue of complexity regularization,as well as network-pruning techniques.
4. Section 4.15,summarizes the advantages and limitations of back-propagation learning.
5. Having completed the study of back-propagation learning, we next take a different
perspective on learning in Section 4.16 by viewing supervised learning as an
optimization problem.
C H A P T E R  4
Multilayer Perceptrons

6. Section 4.17 describes an important neural network structure: the convolutional mul-
tilayer perceptron. This network has been successfully used in the solution of difficult
pattern-recognition problems.
7. Section 4.18 deals with nonlinear filtering, where time plays a key role.The discussion
begins with short-term memory structures, setting the stage for the universal myopic
mapping theorem.
8. Section 4.19 discusses the issue of small-scale versus large-scale learning problems.
The chapter concludes with summary and discussion in Section 4.20.
4.1
INTRODUCTION
In Chapter 1, we studied Rosenblatt’s perceptron, which is basically a single-layer neural
network.Therein, we showed that this network is limited to the classification of linearly
separable patterns. Then we studied adaptive filtering in Chapter 3, using Widrow and
Hoff’s LMS algorithm. This algorithm is also based on a single linear neuron with
adjustable weights, which limits the computing power of the algorithm. To overcome
the practical limitations of the perceptron and the LMS algorithm, we look to a neural
network structure known as the multilayer perceptron.
The following three points highlight the basic features of multilayer perceptrons:
• The model of each neuron in the network includes a nonlinear activation func-
tion that is differentiable.
• The network contains one or more layers that are hidden from both the input and
output nodes.
• The network exhibits a high degree of connectivity, the extent of which is deter-
mined by synaptic weights of the network.
These same characteristics, however, are also responsible for the deficiencies in
our knowledge on the behavior of the network. First, the presence of a distributed form
of nonlinearity and the high connectivity of the network make the theoretical analysis
of a multilayer perceptron difficult to undertake. Second, the use of hidden neurons
makes the learning process harder to visualize. In an implicit sense, the learning process
must decide which features of the input pattern should be represented by the hidden
neurons. The learning process is therefore made more difficult because the search has
to be conducted in a much larger space of possible functions, and a choice has to be
made between alternative representations of the input pattern.
A popular method for the training of multilayer perceptrons is the back-propagation
algorithm, which includes the LMS algorithm as a special case.The training proceeds in
two phases:
1. In the forward phase, the synaptic weights of the network are fixed and the input
signal is propagated through the network,layer by layer,until it reaches the output.
Thus, in this phase, changes are confined to the activation potentials and outputs
of the neurons in the network.
Section 4.1
Introduction
123

2. In the backward phase, an error signal is produced by comparing the output of the
network with a desired response.The resulting error signal is propagated through
the network,again layer by layer,but this time the propagation is performed in the
backward direction. In this second phase, successive adjustments are made to the
synaptic weights of the network.Calculation of the adjustments for the output layer
is straightforward, but it is much more challenging for the hidden layers.
Usage of the term “back propagation” appears to have evolved after 1985, when the
term was popularized through the publication of the seminal book entitled Parallel Dis-
tributed Processing (Rumelhart and McClelland, 1986).
The development of the back-propagation algorithm in the mid-1980s represented
a landmark in neural networks in that it provided a computationally efficient method for
the training of multilayer perceptrons, putting to rest the pessimism about learning in
multilayer perceptrons that may have been inferred from the book by Minsky and Papert
(1969).
4.2
SOME PRELIMINARIES
Figure 4.1 shows the architectural graph of a multiplayer perceptron with two hidden
layers and an output layer. To set the stage for a description of the multilayer percep-
tron in its general form,the network shown here is fully connected.This means that a neu-
ron in any layer of the network is connected to all the neurons (nodes) in the previous
layer.Signal flow through the network progresses in a forward direction,from left to right
and on a layer-by-layer basis.
124
Chapter 4
Multilayer Perceptrons
Output
signal
Input
signal
Input
layer
First
hidden
layer
Second
hidden
layer
Output
layer
•
•
•
•
•
•
•
•
•
FIGURE 4.1
Architectural graph of a multilayer perceptron with two hidden layers.

Figure 4.2 depicts a portion of the multilayer perceptron.Two kinds of signals are
identified in this network:
1. Function Signals. A function signal is an input signal (stimulus) that comes in at
the input end of the network, propagates forward (neuron by neuron) through the
network, and emerges at the output end of the network as an output signal. We
refer to such a signal as a “function signal” for two reasons. First, it is presumed to
perform a useful function at the output of the network. Second, at each neuron of
the network through which a function signal passes, the signal is calculated as a
function of the inputs and associated weights applied to that neuron.The function
signal is also referred to as the input signal.
2. Error Signals. An error signal originates at an output neuron of the network and
propagates backward (layer by layer) through the network. We refer to it as an
“error signal” because its computation by every neuron of the network involves an
error-dependent function in one form or another.
The output neurons constitute the output layer of the network. The remaining
neurons constitute hidden layers of the network.Thus, the hidden units are not part of
the output or input of the network—hence their designation as “hidden.” The first
hidden layer is fed from the input layer made up of sensory units (source nodes); the
resulting outputs of the first hidden layer are in turn applied to the next hidden layer;
and so on for the rest of the network.
Each hidden or output neuron of a multilayer perceptron is designed to perform
two computations:
1. the computation of the function signal appearing at the output of each neuron,
which is expressed as a continuous nonlinear function of the input signal and
synaptic weights associated with that neuron;
2. the computation of an estimate of the gradient vector (i.e., the gradients of the
error surface with respect to the weights connected to the inputs of a neuron),
which is needed for the backward pass through the network.
Section 4.2
Some Preliminaries
125
Function signals
Error signals
FIGURE 4.2
Illustration of
the directions of two basic
signal flows in a multilayer
perceptron: forward
propagation of function
signals and back propagation
of error signals.

Function of the Hidden Neurons
The hidden neurons act as feature detectors; as such, they play a critical role in the oper-
ation of a multilayer perceptron.As the learning process progresses across the multilayer
perceptron, the hidden neurons begin to gradually “discover” the salient features that
characterize the training data.They do so by performing a nonlinear transformation on
the input data into a new space called the feature space. In this new space, the classes of
interest in a pattern-classification task, for example, may be more easily separated from
each other than could be the case in the original input data space. Indeed, it is the for-
mation of this feature space through supervised learning that distinguishes the multilayer
perceptron from Rosenblatt’s perceptron.
Credit-Assignment Problem
When studying learning algorithms for distributed systems, exemplified by the multi-
layer perceptron of Figure 4.1, it is instructive to pay attention to the notion of credit
assignment. Basically, the credit-assignment problem is the problem of assigning credit
or blame for overall outcomes to each of the internal decisions made by the hidden com-
putational units of the distributed learning system, recognizing that those decisions are
responsible for the overall outcomes in the first place.
In a multilayer perceptron using error-correlation learning, the credit-assignment
problem arises because the operation of each hidden neuron and of each output neu-
ron in the network is important to the network’s correct overall action on a learning
task of interest. That is, in order to solve the prescribed task, the network must assign
certain forms of behavior to all of its neurons through a specification of the error-
correction learning algorithm. With this background, consider the multilayer percep-
tron depicted in Fig. 4.1. Since each output neuron is visible to the outside world, it is
possible to supply a desired response to guide the behavior of such a neuron. Thus, as
far as output neurons are concerned, it is a straightforward matter to adjust the synap-
tic weights of each output neuron in accordance with the error-correction algorithm.
But how do we assign credit or blame for the action of the hidden neurons when the
error-correction learning algorithm is used to adjust the respective synaptic weights of
these neurons? The answer to this fundamental question requires more detailed atten-
tion than in the case of output neurons.
In what follows in this chapter,we show that the back-propagation algorithm,basic
to the training of a multilayer perceptron,solves the credit-assignment problem in an ele-
gant manner. But before proceeding to do that, we describe two basic methods of super-
vised learning in the next section.
4.3
BATCH LEARNING AND ON-LINE LEARNING
Consider a multilayer perceptron with an input layer of source nodes, one or more
hidden layers, and an output layer consisting of one or more neurons; as illustrated in
Fig. 4.1. Let
(4.1)
t = {x(n), d(n)}N
n=1
126
Chapter 4
Multilayer Perceptrons

denote the training sample used to train the network in a supervised manner. Let yj(n)
denote the function signal produced at the output of neuron j in the output layer by the
stimulus x(n) applied to the input layer. Correspondingly, the error signal produced at
the output of neuron j is defined by
(4.2)
where dj(n) is the ith element of the desired-response vector d(n). Following the termi-
nology of the LMS algorithm studied in Chapter 3, the instantaneous error energy of
neuron j is defined by
(4.3)
Summing the error-energy contributions of all the neurons in the output layer,we express
the total instantaneous error energy of the whole network as
(4.4)
where the set C includes all the neurons in the output layer.With the training sample con-
sisting of N examples, the error energy averaged over the training sample, or the empirical
risk, is defined by
(4.5)
Naturally, the instantaneous error energy, and therefore the average error energy, are
both functions of all the adjustable synaptic weights (i.e., free parameters) of the mul-
tilayer perceptron. This functional dependence has not been included in the formulas
for e(n) and eav(N), merely to simplify the terminology.
Depending on how the supervised learning of the multilayer perceptron is actu-
ally performed, we may identify two different methods—namely, batch learning and on-
line learning, as discussed next in the context of gradient descent.
Batch Learning
In the batch method of supervised learning, adjustments to the synaptic weights of the
multilayer perceptron are performed after the presentation of all the N examples in the
training sample t that constitute one epoch of training. In other words, the cost function
for batch learning is defined by the average error energy eav. Adjustments to the synaptic
weights of the multilayer perceptron are made on an epoch-by-epoch basis. Correspond-
ingly, one realization of the learning curve is obtained by plotting eav versus the number
 =
1
2N a
N
n=1 a
jC
ej
2(n)
 eav(N) = 1
N a
N
n=1
e(n)
 = 1
2 a
jC
ej
2(n)
 e(n) = a
jC
ej(n)
ej(n) = 1
2
 ej
2(n)
ej(n) = dj(n) - yj(n)
Section 4.3
Batch Learning and On-Line Learning
127

of epochs, where, for each epoch of training, the examples in the training sample t are
randomly shuffled.The learning curve is then computed by ensemble averaging a large
enough number of such realizations, where each realization is performed for a different
set of initial conditions chosen at random.
With the method of gradient descent used to perform the training, the advantages
of batch learning include the following:
• accurate estimation of the gradient vector (i.e., the derivative of the cost function
eav with respect to the weight vector w), thereby guaranteeing, under simple con-
ditions, convergence of the method of steepest descent to a local minimum;
• parallelization of the learning process.
However, from a practical perspective, batch learning is rather demanding in terms of
storage requirements.
In a statistical context, batch learning may be viewed as a form of statistical infer-
ence. It is therefore well suited for solving nonlinear regression problems.
On-line Learning
In the on-line method of supervised learning, adjustments to the synaptic weights of the
multilayer perceptron are performed on an example-by-example basis. The cost function
to be minimized is therefore the total instantaneous error energy e(n).
Consider an epoch of N training examples arranged in the order {x(1), d(1)}, {x(2),
d(2)},
{x(N), d(N)}.The first example pair {x(1), d(1)} in the epoch is presented to the
network, and the weight adjustments are performed using the method of gradient
descent.Then the second example {x(2), d(2)} in the epoch is presented to the network,
which leads to further adjustments to weights in the network.This procedure is contin-
ued until the last example {x(N), d(N)} is accounted for.Unfortunately,such a procedure
works against the parallalization of on-line learning.
For a given set of initial conditions, a single realization of the learning curve is
obtained by plotting the final value e(N) versus the number of epochs used in the train-
ing session, where, as before, the training examples are randomly shuffled after each
epoch. As with batch learning, the learning curve for on-line learning is computed by
ensemble averaging such realizations over a large enough number of initial conditions
chosen at random. Naturally, for a given network structure, the learning curve obtained
under on-line learning will be quite different from that under batch learning.
Given that the training examples are presented to the network in a random man-
ner, the use of on-line learning makes the search in the multidimensional weight space
stochastic in nature; it is for this reason that the method of on-line learning is sometimes
referred to as a stochastic method. This stochasticity has the desirable effect of making
it less likely for the learning process to be trapped in a local minimum, which is a defi-
nite advantage of on-line learning over batch learning. Another advantage of on-line
learning is the fact that it requires much less storage than batch learning.
Moreover, when the training data are redundant (i.e., the training sample t con-
tains several copies of the same example), we find that, unlike batch learning, on-line
...,
128
Chapter 4
Multilayer Perceptrons

learning is able to take advantage of this redundancy because the examples are presented
one at a time.
Another useful property of on-line learning is its ability to track small changes in
the training data, particularly when the environment responsible for generating the data
is nonstationary.
To summarize,despite the disadvantages of on-line learning,it is highly popular for
solving pattern-classification problems for two important practical reasons:
• On-line learning is simple to implement.
• It provides effective solutions to large-scale and difficult pattern-classification
problems.
It is for these two reasons that much of the material presented in this chapter is devoted
to on-line learning.
4.4
THE BACK-PROPAGATION ALGORITHM
The popularity of on-line learning for the supervised training of multilayer perceptrons
has been further enhanced by the development of the back-propagation algorithm. To
describe this algorithm, consider Fig. 4.3, which depicts neuron j being fed by a set of
Section 4.4
The Back-Propagation Algorithm
129
y0  1
1
wj0(n)  bj(n)
wji(n)
vj(n)
yj(n)
yi(n)
y1(n)
ym(n)
dj(n)
ej(n)
w()
Neuron j
•
•
•
•
•
•
FIGURE 4.3
Signal-flow graph highlighting the details of output neuron j.

function signals produced by a layer of neurons to its left.The induced local field vj(n)
produced at the input of the activation function associated with neuron j is therefore
(4.6)
where m is the total number of inputs (excluding the bias) applied to neuron j. The
synaptic weight wj0 (corresponding to the fixed input y0  1) equals the bias bj applied
to neuron j. Hence, the function signal yj(n) appearing at the output of neuron j at
iteration n is
(4.7)
In a manner similar to the LMS algorithm studied in Chapter 3, the back-
propagation algorithm applies a correction wji(n) to the synaptic weight wji(n), which
is proportional to the partial derivative 
.According to the chain rule of
calculus, we may express this gradient as
(4.8)
The partial derivative 
represents a sensitivity factor, determining the
direction of search in weight space for the synaptic weight wji.
Differentiating both sides of Eq. (4.4) with respect to ej(n), we get
(4.9)
Differentiating both sides of Eq. (4.2) with respect to yj(n), we get
(4.10)
Next, differentiating Eq. (4.7) with respect to vj(n), we get
(4.11)
where the use of prime (on the right-hand side) signifies differentiation with respect to
the argument. Finally, differentiating Eq. (4.6) with respect to wji(n) yields
(4.12)
The use of Eqs. (4.9) to (4.12) in Eq. (4.8) yields
(4.13)
0e(n)
0wji(n) = -ej(n)¿j(vj(n))yi(n)
0vj(n)
0wji(n) = yi(n)
0yj(n)
0vj(n) = ¿j(vj(n))
0ej(n)
0yj(n) = -1
0e(n)
0ej(n) = ej(n)
0e(n)0wji(n)
0e(n)
0wji(n) =
0e(n)
0ej(n) 
0ej(n)
0yj(n) 
0yj(n)
0vj(n) 
0vj(n)
0wji(n)
0e(n)0wji(n)
¢
yj(n) = j(vj(n))
vj(n) = a
m
i=0
wji(n)yi(n)
130
Chapter 4
Multilayer Perceptrons

The correction wji(n) applied to wji(n) is defined by the delta rule, or
(4.14)
where  is the learning-rate parameter of the back-propagation algorithm. The use of
the minus sign in Eq. (4.14) accounts for gradient descent in weight space (i.e., seeking
a direction for weight change that reduces the value of 
). Accordingly, the use of
Eq. (4.13) in Eq. (4.14) yields
(4.15)
where the local gradient
is defined by
(4.16)
The local gradient points to required changes in synaptic weights.According to Eq.(4.16),
the local gradient j(n) for output neuron j is equal to the product of the corresponding
error signal ej(n) for that neuron and the derivative j(vj(n)) of the associated activa-
tion function.
From Eqs. (4.15) and (4.16), we note that a key factor involved in the calculation
of the weight adjustment !wji(n) is the error signal ej(n) at the output of neuron j. In this
context, we may identify two distinct cases, depending on where in the network neuron
j is located. In case 1, neuron j is an output node. This case is simple to handle because
each output node of the network is supplied with a desired response of its own, making
it a straightforward matter to calculate the associated error signal. In case 2, neuron j is
a hidden node. Even though hidden neurons are not directly accessible, they share
responsibility for any error made at the output of the network.The question, however,
is to know how to penalize or reward hidden neurons for their share of the responsibility.
This problem is the credit-assignment problem considered in Section 4.2.
Case 1 Neuron j Is an Output Node
When neuron j is located in the output layer of the network, it is supplied with a desired
response of its own. We may use Eq. (4.2) to compute the error signal ej(n) associated
with this neuron; see Fig. 4.3. Having determined ej(n), we find it a straightforward mat-
ter to compute the local gradient j(n) by using Eq. (4.16).
Case 2 Neuron j Is a Hidden Node
When neuron j is located in a hidden layer of the network, there is no specified desired
response for that neuron.Accordingly, the error signal for a hidden neuron would have
to be determined recursively and working backwards in terms of the error signals of
all the neurons to which that hidden neuron is directly connected; this is where the
 = ej(n)¿j(vj(n))
 =
0e(n)
0ej(n) 
0ej(n)
0yj(n) 
0yj(n)
0vj(n)
 j(n) =
0e(n)
0vj(n)
j(n)
¢wji(n) = j(n)yi(n)
e(n)
¢wji(n) = - 0e(n)
0wji(n)
¢
Section 4.4
The Back-Propagation Algorithm
131

development of the back-propagation algorithm gets complicated. Consider the situa-
tion in Fig. 4.4, which depicts neuron j as a hidden node of the network. According to
Eq. (4.16), we may redefine the local gradient j(n) for hidden neuron j as
(4.17)
where in the second line we have used Eq. (4.11). To calculate the partial derivative
, we may proceed as follows: From Fig. 4.4, we see that
(4.18)
which is Eq. (4.4) with index k used in place of index j. We have made this substitution
in order to avoid confusion with the use of index j that refers to a hidden neuron under
case 2. Differentiating Eq. (4.18) with respect to the function signal yj(n), we get
(4.19)
0e(n)
0yj(n) = a
k
ek
0ek(n)
0yj(n)
e(n) = 1
2 a
kHC
e2
k(n),  neuron k is an output node
0e(n)0yj(n)
 = - 
0e(n)
0yj(n)¿j(vj(n)),  neuron j is hidden
 j(n) = - 
0e(n)
0yj(n) 
0yj(n)
0vj(n)
132
Chapter 4
Multilayer Perceptrons
y0  1
Neuron j
Neuron k
yi(n)
wji(n)
wj0(n)  bj(n)
wkj(n)
vk(n)
vj(n)
w()
w()
yj(n)
yk(n)
1
1
dk(n)
ek(n)
•
•
•
•
•
•
•
•
•
•
•
•
FIGURE 4.4
Signal-flow graph highlighting the details of output neuron k connected to hidden neuron j.

Next we use the chain rule for the partial derivative 
and rewrite Eq.(4.19)
in the equivalent form
(4.20)
However, from Fig. 4.4, we note that
(4.21)
Hence,
(4.22)
We also note from Fig. 4.4 that for neuron k, the induced local field is
(4.23)
where m is the total number of inputs (excluding the bias) applied to neuron k. Here
again, the synaptic weight wk0(n) is equal to the bias bk(n) applied to neuron k, and the
corresponding input is fixed at the value 1. Differentiating Eq. (4.23) with respect to
yj(n) yields
(4.24)
By using Eqs. (4.22) and (4.24) in Eq. (4.20), we get the desired partial derivative
(4.25)
where, in the second line, we have used the definition of the local gradient k(n) given
in Eq. (4.16), with the index k substituted for j.
Finally, using Eq. (4.25) in Eq. (4.17), we get the back-propagation formula for the
local gradient j(n), described by
(4.26)
Figure 4.5 shows the signal-flow graph representation of Eq. (4.26), assuming that the
output layer consists of mL neurons.
The outside factor j(vj(n)) involved in the computation of the local gradient j(n) in
Eq.(4.26) depends solely on the activation function associated with hidden neuron j.
The remaining factor involved in this computation—namely,the summation over k—depends
on two sets of terms. The first set of terms, the k(n), requires knowledge of the error
j(n) = ¿j(vj(n)) a
k
k(n)wkj(n),  neuron j is hidden
 = - a
k
k(n)wkj(n)
 0e(n)
0yj(n) = - a
k
ek(n)¿k(vk(n))wkj(n)
0vk(n)
0yj(n) = wkj(n)
vk(n) = a
m
j=0
wkj(n)yj(n)
0ek(n)
0vk(n) = -¿k(vk(n))
 = dk(n) - k(vk(n)),  neuron k is an output node
 ek(n) = dk(n) - yk(n)
0e(n)
0yj(n) = a
k
ek(n) 
0ek(n)
0vk(n) 
0vk(n)
0yj(n)
0ek(n)0yj(n)
Section 4.4
The Back-Propagation Algorithm
133

signals ek(n) for all neurons that lie in the layer to the immediate right of hidden neu-
ron j and that are directly connected to neuron j; see Fig. 4.4. The second set of terms,
the wkj(n), consists of the synaptic weights associated with these connections.
We now summarize the relations that we have derived for the back-propagation
algorithm. First, the correction !wji(n) applied to the synaptic weight connecting neu-
ron i to neuron j is defined by the delta rule:
(4.27)
Second, the local gradient j(n) depends on whether neuron j is an output node or a
hidden node:
1. If neuron j is an output node, j(n) equals the product of the derivative j(vj(n))
and the error signal ej(n),both of which are associated with neuron j; see Eq.(4.16).
2. If neuron j is a hidden node, j(n) equals the product of the associated derivative
j(vj(n)) and the weighted sum of the s computed for the neurons in the next
hidden or output layer that are connected to neuron j; see Eq. (4.26).
The Two Passes of Computation
In the application of the back-propagation algorithm, two different passes of computa-
tion are distinguished. The first pass is referred to as the forward pass, and the second
is referred to as the backward pass.
In the forward pass, the synaptic weights remain unaltered throughout the net-
work, and the function signals of the network are computed on a neuron-by-neuron
basis.The function signal appearing at the output of neuron j is computed as
(4.28)
where vj(n) is the induced local field of neuron j, defined by
(4.29)
vj(n) = a
m
i=0
wji(n)yi(n)
yj(n) = (vj(n))
°
Weight
correction
¢wji(n)
¢ = °
learning-
rate parameter

¢ * °
local
gradient
j(n)
¢ * °
input signal
of neuron j,
yi(n)
¢
134
Chapter 4
Multilayer Perceptrons
Dj(n)
D1(n)
Dk(n) Wk
(vk(n))
W1
(v1(n))
W
mL(vmL(n))
DmL(n)
w1j(n)
wkj(n)
wmLj(n)
ek(n)
emL(n)
e1(n)
•
•
•
•
•
•
•
•
•
•
•
•
FIGURE 4.5
Signal-flow
graph of a part of the adjoint
system pertaining to back-
propagation of error signals.

where m is the total number of inputs (excluding the bias) applied to neuron j; wji(n) is
the synaptic weight connecting neuron i to neuron j; and yi(n) is an input signal of neu-
ron j, or, equivalently, the function signal appearing at the output of neuron i. If neuron
j is in the first hidden layer of the network, then m  m0 and the index i refers to the ith
input terminal of the network, for which we write
(4.30)
where xi(n) is the ith element of the input vector (pattern). If, on the other hand, neu-
ron j is in the output layer of the network, then m  mL and the index j refers to the jth
output terminal of the network, for which we write
(4.31)
where oj(n) is the jth element of the output vector of the multilayer perceptron.This out-
put is compared with the desired response dj(n), obtaining the error signal ej(n) for the
jth output neuron. Thus, the forward phase of computation begins at the first hidden
layer by presenting it with the input vector and terminates at the output layer by com-
puting the error signal for each neuron of this layer.
The backward pass, on the other hand, starts at the output layer by passing the
error signals leftward through the network,layer by layer,and recursively computing the
 (i.e., the local gradient) for each neuron. This recursive process permits the synaptic
weights of the network to undergo changes in accordance with the delta rule of Eq.(4.27).
For a neuron located in the output layer, the  is simply equal to the error signal of that
neuron multiplied by the first derivative of its nonlinearity. Hence, we use Eq. (4.27) to
compute the changes to the weights of all the connections feeding into the output layer.
Given the s for the neurons of the output layer, we next use Eq. (4.26) to compute the
s for all the neurons in the penultimate layer and therefore the changes to the weights
of all connections feeding into it.The recursive computation is continued, layer by layer,
by propagating the changes to all synaptic weights in the network.
Note that for the presentation of each training example,the input pattern is fixed—
that is, “clamped” throughout the round-trip process, which encompasses the forward
pass followed by the backward pass.
Activation Function
The computation of the  for each neuron of the multilayer perceptron requires knowl-
edge of the derivative of the activation function (·) associated with that neuron.For this
derivative to exist, we require the function (·) to be continuous. In basic terms, differ-
entiability is the only requirement that an activation function has to satisfy.An example
of a continuously differentiable nonlinear activation function commonly used in multi-
layer perceptrons is sigmoidal nonlinearity,1 two forms of which are described here:
1. Logistic Function. This form of sigmoidal nonlinearity, in its general form, is
defined by
(4.32)
j(vj(n)) =
1
1 + exp(-avj(n)),  a 7 0
yj(n) = oj(n)
yi(n) = xi(n)
Section 4.4
The Back-Propagation Algorithm
135

where vj(n) is the induced local field of neuron j and a is an adjustable positive parameter.
According to this nonlinearity, the amplitude of the output lies inside the range
Differentiating Eq. (4.32) with respect to vj(n), we get
(4.33)
With yj(n)  j(vj(n)),we may eliminate the exponential term exp(-avj(n)) from Eq.(4.33)
and consequently express the derivative j(vj(n)) as
(4.34)
For a neuron j located in the output layer, yj(n)  oj(n). Hence, we may express the local
gradient for neuron j as
(4.35)
where oj(n) is the function signal at the output of neuron j, and dj(n) is the desired
response for it. On the other hand, for an arbitrary hidden neuron j, we may express
the local gradient as
(4.36)
Note from Eq. (4.34) that the derivative j(vj(n)) attains its maximum value at yj(n) 
0.5 and its minimum value (zero) at yj(n)  0, or yj(n)  1.0. Since the amount of change
in a synaptic weight of the network is proportional to the derivative j(vj(n)), it follows
that for a sigmoid activation function, the synaptic weights are changed the most for
those neurons in the network where the function signals are in their midrange.Accord-
ing to Rumelhart et al. (1986a), it is this feature of back-propagation learning that con-
tributes to its stability as a learning algorithm.
2. Hyperbolic tangent function. Another commonly used form of sigmoidal non-
linearity is the hyperbolic tangent function, which, in its most general form, is defined by
(4.37)
where a and b are positive constants. In reality, the hyperbolic tangent function is just
the logistic function rescaled and biased. Its derivative with respect to vj(n) is given by
(4.38)
 = b
a [a - yj(n)][a + yj(n)]
 = ab(1 - tanh2(bvj(n)))
 ¿j(vj(n)) = ab sech2(bvj(n))
j(vj(n)) = a tanh(bvj(n))
 = ayj(n)[1 - yj(n)] a
k
k(n)wkj(n),  neuron j is hidden
 j(n) = ¿j(vj(n)) a
k
k(n)wkj(n)
 = a[dj(n) - oj(n)]oj(n)[1 - oj(n)],  neuron j is an output node
 j(n) = ej(n)¿j(vj(n))
¿j(vj(n)) = ayj(n)[1 - yj(n)]
¿j(vj(n)) =
a exp(-avj(n))
[1 + exp(-avj(n))]2
0  yj  1.
136
Chapter 4
Multilayer Perceptrons

For a neuron j located in the output layer, the local gradient is
(4.39)
For a neuron j in a hidden layer, we have
(4.40)
By using Eqs. (4.35) and (4.36) for the logistic function and Eqs. (4.39) and (4.40) for the
hyperbolic tangent function, we may calculate the local gradient j without requiring
explicit knowledge of the activation function.
Rate of Learning
The back-propagation algorithm provides an “approximation”to the trajectory in weight
space computed by the method of steepest descent.The smaller we make the learning-
rate parameter , the smaller the changes to the synaptic weights in the network will be
from one iteration to the next, and the smoother will be the trajectory in weight space.
This improvement, however, is attained at the cost of a slower rate of learning. If, on
the other hand, we make the learning-rate parameter  too large in order to speed up
the rate of learning, the resulting large changes in the synaptic weights assume such a
form that the network may become unstable (i.e., oscillatory). A simple method of
increasing the rate of learning while avoiding the danger of instability is to modify the
delta rule of Eq. (4.15) by including a momentum term, as shown by
(4.41)
where  is usually a positive number called the momentum constant. It controls the feed-
back loop acting around !wji(n), as illustrated in Fig. 4.6, where z-1 is the unit-time delay
operator. Equation (4.41) is called the generalized delta rule2; it includes the delta rule
of Eq. (4.15) as a special case (i.e.,   0).
In order to see the effect of the sequence of pattern presentations on the synap-
tic weights due to the momentum constant , we rewrite Eq. (4.41) as a time series with
index t. The index t goes from the initial time 0 to the current time n. Equation (4.41)
¢wji(n) = ¢wji(n - 1) + j(n)yi(n)
 = b
a [a - yj(n)] [a + yj(n)] a
k
k(n)wkj(n),  neuron j is hidden
 j(n) = ¿j(vj(n)) a
k
k(n)wkj(n)
 = b
a [dj(n) - oj(n)][a - oj(n)][a + oj(n)]
 j(n) = ej(n)¿j(vj(n))
Section 4.4
The Back-Propagation Algorithm
137
dj(n) yi(n)
a
h
!wji(n  1)
!wji(n)
z1
FIGURE 4.6
Signal-flow
graph illustrating the effect
of momentum constant ,
which lies inside the
feedback loop.

may be viewed as a first-order difference equation in the weight correction wji(n).
Solving this equation for wji(n), we have
(4.42)
which represents a time series of length n  1.From Eqs.(4.13) and (4.16),we note that the
product j(n)yi(n) is equal to 
.Accordingly, we may rewrite Eq. (4.42) in
the equivalent form
(4.43)
Based on this relation, we may make the following insightful observations:
1. The current adjustment wji(n) represents the sum of an exponentially weighted
time series. For the time series to be convergent, the momentum constant must be
restricted to the range 
When  is zero, the back-propagation algorithm
0   6 1.
¢
¢wji(n) = -  a
n
t=0
n-t 
0e(t)
0wji(t)
-0e(n)  0wji(n)
¢wji(n) =  a
n
t=0
n-tj(t)yi(t)
¢
¢
138
Chapter 4
Multilayer Perceptrons
operates without momentum.Also, the momentum constant  can be positive or nega-
tive, although it is unlikely that a negative  would be used in practice.
2. When the partial derivative 
has the same algebraic sign on con-
secutive iterations, the exponentially weighted sum wji(n) grows in magnitude, and
consequently the weight wji(n) is adjusted by a large amount.The inclusion of momen-
tum in the back-propagation algorithm tends to accelerate descent in steady downhill
directions.
3. When the partial derivative 
has opposite signs on consecutive
iterations, the exponentially weighted sum wji(n) shrinks in magnitude, and conse-
quently the weight wji(n) is adjusted by a small amount.The inclusion of momentum in
the back-propagation algorithm has a stabilizing effect in directions that oscillate in sign.
The incorporation of momentum in the back-propagation algorithm represents a
minor modification to the weight update; however, it may have some beneficial effects
on the learning behavior of the algorithm.The momentum term may also have the ben-
efit of preventing the learning process from terminating in a shallow local minimum on
the error surface.
In deriving the back-propagation algorithm, it was assumed that the learning-rate
parameter is a constant denoted by . In reality, however, it should be defined as ji;
that is, the learning-rate parameter should be connection dependent. Indeed, many inter-
esting things can be done by making the learning-rate parameter different for different
parts of the network.We provide more detail on this issue in subsequent sections.
It is also noteworthy that in the application of the back-propagation algorithm,
we may choose all the synaptic weights in the network to be adjustable, or we may con-
strain any number of weights in the network to remain fixed during the adaptation
process. In the latter case, the error signals are back propagated through the network
in the usual manner; however, the fixed synaptic weights are left unaltered.This can be
done simply by making the learning-rate parameter ji for synaptic weight wji equal to
zero.
¢
0e(t)0wji(t)
¢
0e(t)0wji(t)

Stopping Criteria
In general, the back-propagation algorithm cannot be shown to converge, and there are
no well-defined criteria for stopping its operation. Rather, there are some reasonable
criteria, each with its own practical merit, that may be used to terminate the weight
adjustments. To formulate such a criterion, it is logical to think in terms of the unique
properties of a local or global minimum of the error surface.3 Let the weight vector w*
denote a minimum, be it local or global.A necessary condition for w* to be a minimum
is that the gradient vector g(w) (i.e., first-order partial derivative) of the error surface
with respect to the weight vector w must be zero at w  w*. Accordingly, we may
formulate a sensible convergence criterion for back-propagation learning as follows
(Kramer and Sangiovanni-Vincentelli, 1989):
The back-propagation algorithm is considered to have converged when the Euclidean norm
of the gradient vector reaches a sufficiently small gradient threshold.
The drawback of this convergence criterion is that, for successful trials, learning times
may be long.Also, it requires the computation of the gradient vector g(w).
Another unique property of a minimum that we can use is the fact that the cost
function 
is stationary at the point w  w*.We may therefore suggest a different
criterion of convergence:
The back-propagation algorithm is considered to have converged when the absolute rate of
change in the average squared error per epoch is sufficiently small.
The rate of change in the average squared error is typically considered to be small
enough if it lies in the range of 0.1 to 1 percent per epoch. Sometimes a value as small
as 0.01 percent per epoch is used. Unfortunately, this criterion may result in a premature
termination of the learning process.
There is another useful, and theoretically supported, criterion for convergence:
After each learning iteration, the network is tested for its generalization performance.
The learning process is stopped when the generalization performance is adequate or
when it is apparent that the generalization performance has peaked; see Section 4.13 for
more details.
Summary of the Back-Propagation Algorithm
Figure 4.1 presents the architectural layout of a multilayer perceptron. The corre-
sponding signal-flow graph for back-propagation learning, incorporating both the for-
ward and backward phases of the computations involved in the learning process, is
presented in Fig. 4.7 for the case of L  2 and m0  m1  m2  3. The top part of the
signal-flow graph accounts for the forward pass.The lower part of the signal-flow graph
accounts for the backward pass, which is referred to as a sensitivity graph for comput-
ing the local gradients in the back-propagation algorithm (Narendra and Parthasarathy,
1990).
Earlier, we mentioned that the sequential updating of weights is the preferred
method for on-line implementation of the back-propagation algorithm. For this mode
eav(w)
Section 4.4
The Back-Propagation Algorithm
139

of operation, the algorithm cycles through the training sample {(x(n), d(n))}N
n1 as
follows:
1. Initialization. Assuming that no prior information is available, pick the synap-
tic weights and thresholds from a uniform distribution whose mean is zero and whose
variance is chosen to make the standard deviation of the induced local fields of the neu-
rons lie at the transition between the linear and standards parts of the sigmoid activa-
tion function.
2. Presentations of Training Examples. Present the network an epoch of training
examples.For each example in the sample,ordered in some fashion,perform the sequence
of forward and backward computations described under points 3 and 4, respectively.
3. Forward Computation. Let a training example in the epoch be denoted by (x(n),
d(n)), with the input vector x(n) applied to the input layer of sensory nodes and the
desired response vector d(n) presented to the output layer of computation nodes. Com-
pute the induced local fields and function signals of the network by proceeding forward
through the network, layer by layer.The induced local field 
for neuron j in layer l
is
(4.44)
vj
(l)(n) = a
i
w(l)
ji (n)y(l-1)
i
(n)
v(l)
j (n)
140
Chapter 4
Multilayer Perceptrons
x1
x2
x3
1
1
v1
(1)
d1
(1)
d1
(2)
d2
(1)
d2
(2)
d3
(1)
d3
(2)
v1
(1)
v2
(1)
v2
(1)
v3
(1)
v3
(2)
v3
(2)
v3
(1)
v2
(2)
v2
(2)
v1
(2)
v1
(2)
d1
d2
e1
e1
e2
e2
e3
e3
d3
w10  b(1)
(1)
w13
(1)
w13
(2)
w31
(2)
w10  b(2)
(2)
w() y2
(1)
w() y3
(1)
w() y1
(1)
w()
w()
w()
w()
w()
w()
w() o2  1
w() o1  1
w() o3  1
FIGURE 4.7
Signal-flow graphical summary of back-propagation learning.Top part of the
graph: forward pass. Bottom part of the graph: backward pass.

where yi
(l-1)(n) is the output (function) signal of neuron i in the previous layer l - 1 at
iteration n, and 
ji
(l)(n) is the synaptic weight of neuron j in layer l that is fed from neu-
ron i in layer l - 1. For i  0, we have y0
(l -1)(n)  1, and 
j0
(l)(n) 
(n) is the bias
applied to neuron j in layer l. Assuming the use of a sigmoid function, the output signal
of neuron j in layer l is
If neuron j is in the first hidden layer (i.e., l  1), set
where xj(n) is the jth element of the input vector x(n). If neuron j is in the output layer
(i.e., l  L, where L is referred to as the depth of the network), set
Compute the error signal
(4.45)
where dj(n) is the jth element of the desired response vector d(n).
4. Backward Computation. Compute the s (i.e., local gradients) of the network,
defined by
(4.46)
where the prime in j( ) denotes differentiation with respect to the argument. Adjust
the synaptic weights of the network in layer l according to the generalized delta rule
(4.47)
where  is the learning-rate parameter and  is the momentum constant.
5. Iteration. Iterate the forward and backward computations under points 3 and
4 by presenting new epochs of training examples to the network until the chosen stop-
ping criterion is met.
Notes: The order of presentation of training examples should be randomized from epoch to epoch.
The momentum and learning-rate parameter are typically adjusted (and usually decreased) as
the number of training iterations increases. Justification for these points will be presented later.
4.5
XOR PROBLEM
In Rosenblatt’s single-layer perceptron, there are no hidden neurons. Consequently, it
cannot classify input patterns that are not linearly separable. However, nonlinearly sep-
arable patterns commonly occur. For example, this situation arises in the exclusive-OR
(XOR) problem, which may be viewed as a special case of a more general problem,
namely, that of classifying points in the unit hypercube. Each point in the hypercube is
in either class 0 or class 1. However, in the special case of the XOR problem, we need
wji
(l)(n + 1) = w(l)
ji (n) + [¢wji
(l)(n - 1)] + (l)
j (n)yi
(l-1)(n)

 j
(l)(n) = •
e j
(L)(n)j¿(v j
(L)(n))
for neuron j in output layer L
j¿ (v j
(l)(n)) a
k
 k
(l+1)(n)w kj
(l+1)(n)
for neuron j in hidden layer l
ej(n) = dj(n) - oj(n)
y(L)
j
= oj(n)
yj
(0)(n) = xj(n)
y(l)
j
= j(vj(n))
b(l)
j
w
w
Section 4.5
XOR Problem
141

consider only the four corners of a unit square that correspond to the input patterns
(0,0), (0,1), (1,1), and (1,0),where a single bit (i.e.,binary digit) changes as we move from
one corner to the next.The first and third input patterns are in class 0, as shown by
0  0  0
and
1  1  0
where  denotes the exclusive-OR Boolean function operator.The input patterns (0,0)
and (1,1) are at opposite corners of the unit square, yet they produce the identical
output 0. On the other hand, the input patterns (0,1) and (1,0) are also at opposite
corners of the square, but they are in class 1, as shown by
0  1  1
and
1  0  1
We first recognize that the use of a single neuron with two inputs results in a
straight line for a decision boundary in the input space. For all points on one side of this
line, the neuron outputs 1; for all points on the other side of the line, it outputs 0. The
position and orientation of the line in the input space are determined by the synaptic
weights of the neuron connected to the input nodes and the bias applied to the neuron.
With the input patterns (0,0) and (1,1) located on opposite corners of the unit square,
and likewise for the other two input patterns (0,1) and (1,0), it is clear that we cannot
construct a straight line for a decision boundary so that (0,0) and (0,1) lie in one deci-
sion region and (0,1) and (1,0) lie in the other decision region. In other words, the single-
layer perceptron cannot solve the XOR problem.
However, we may solve the XOR problem by using a single hidden layer with two
neurons, as in Fig. 4.8a (Touretzky and Pomerleau, 1989). The signal-flow graph of the
network is shown in Fig. 4.8b.The following assumptions are made here:
• Each neuron is represented by a McCulloch–Pitts model, which uses a threshold
function for its activation function.
• Bits 0 and 1 are represented by the levels 0 and 1, respectively.
The top neuron, labeled as “Neuron 1” in the hidden layer, is characterized as
The slope of the decision boundary constructed by this hidden neuron is equal to -1
and positioned as in Fig. 4.9a.The bottom neuron, labeled as “Neuron 2” in the hidden
layer, is characterized as
 b2 = - 1
2
 w21 = w22 = + 1
 b1 = - 3
2
 w11 = w12 = +1
142
Chapter 4
Multilayer Perceptrons

The orientation and position of the decision boundary constructed by this second hid-
den neuron are as shown in Fig. 4.9b.
The output neuron, labeled as “Neuron 3” in Fig. 4.8a, is characterized as
The function of the output neuron is to construct a linear combination of the decision
boundaries formed by the two hidden neurons.The result of this computation is shown
in Fig.4.9c.The bottom hidden neuron has an excitatory (positive) connection to the out-
put neuron, whereas the top hidden neuron has an inhibitory (negative) connection to
the output neuron.When both hidden neurons are off, which occurs when the input pat-
tern is (0,0), the output neuron remains off. When both hidden neurons are on, which
occurs when the input pattern is (1,1), the output neuron is switched off again because
the inhibitory effect of the larger negative weight connected to the top hidden neuron
overpowers the excitatory effect of the positive weight connected to the bottom hidden
neuron.When the top hidden neuron is off and the bottom hidden neuron is on, which
occurs when the input pattern is (0,1) or (1,0), the output neuron is switched on because
of the excitatory effect of the positive weight connected to the bottom hidden neuron.
Thus, the network of Fig. 4.8a does indeed solve the XOR problem.
 b3 = - 1
2
 w32 = +  1
 w31 = -2
Section 4.5
XOR Problem
143
(a)
(b)
Input
layer
Hidden
layer
Output
layer
Neuron 1
Neuron 2
Neuron 3
w()
w()
w()
x1
x2
o3
1
1
1
1
1
1
1.5
0.5
2
1
1
0.5
FIGURE 4.8
(a) Architectural
graph of network for solving the
XOR problem. (b) Signal-flow
graph of the network.

4.6
HEURISTICS FOR MAKING THE BACK-PROPAGATION ALGORITHM
PERFORM BETTER
It is often said that the design of a neural network using the back-propagation algo-
rithm is more of an art than a science, in the sense that many of the factors involved in
the design are the results of one’s own personal experience.There is some truth in this
statement. Nevertheless, there are methods that will significantly improve the back-
propagation algorithm’s performance, as described here:
1. Stochastic versus batch update. As mentioned previously, the stochastic
(sequential) mode of back-propagation learning (involving pattern-by-pattern updat-
ing) is computationally faster than the batch mode. This is especially true when the
144
Chapter 4
Multilayer Perceptrons
(0,1)
(1,1)
(0,0)
Input
x2
Input x1
Output
 1
Output
 0
(a)
(1,0)
(0,1)
(1,1)
(0,0)
Input
x2
Input x1
Output
 0
Output
 1
(c)
(1,0)
(0,1)
(1,1)
(0,0)
Input
x2
Input x1
Output
 0
Output
 0
Output
 1
(b)
(1,0)
FIGURE 4.9
(a) Decision
boundary constructed by
hidden neuron 1 of the
network in Fig. 4.8.
(b) Decision boundary
constructed by hidden
neuron 2 of the network.
(c) Decision boundaries
constructed by the
complete network.

training data sample is large and highly redundant. (Highly redundant data pose
computational problems for the estimation of the Jacobian required for the batch
update.)
2. Maximizing information content. As a general rule, every training example pre-
sented to the back-propagation algorithm should be chosen on the basis that its infor-
mation content is the largest possible for the task at hand (LeCun, 1993). Two ways of
realizing this choice are as follows:
• Use an example that results in the largest training error.
• Use an example that is radically different from all those previously used.
These two heuristics are motivated by a desire to search more of the weight space.
In pattern-classification tasks using sequential back-propagation learning, a sim-
ple and commonly used technique is to randomize (i.e., shuffle) the order in which the
examples are presented to the multilayer perceptron from one epoch to the next. Ide-
ally,the randomization ensure that successive examples in an epoch presented to the net-
work rarely belong to the same class.
3. Activation function. Insofar as the speed of learning is concerned, the preferred
choice is to use a sigmoid activation function that is an odd function of its argument, as
shown by
This condition is satisfied by the hyperbolic function
as shown in Fig. 4.10, but not the logistic function. Suitable values for the constraints a
and b in the formula for (v) are as follows (LeCun, 1993):
and
The hyperbolic tangent function (v) of Fig. 4.10 has the following useful properties:
• (1)  1 and (-1)  -1.
• At the origin, the slope (i.e., effective gain) of the activation function is close to
unity, as shown by
• The second derivative of (v) attains its maximum value at v  1.
 = 1.1424
 = 1.7159 a 2
3 b
 (0) = ab
b = 2
3
a = 1.7159
(v) = a tanh(bv)
(-v) = -(v)
Section 4.6
Heuristics for Making the Back-Propagation Algorithm Perform Better
145

4. Target values. It is important that the target values (desired response) be cho-
sen within the range of the sigmoid activation function. More specifically, the desired
response dj for neuron j in the output layer of the multilayer perceptron should be offset
by some amount  away from the limiting value of the sigmoid activation function,
depending on whether the limiting value is positive or negative. Otherwise, the back-
propagation algorithm tends to drive the free parameters of the network to infinity and
thereby slow down the learning process by driving the hidden neurons into saturation.
To be specific, consider the hyperbolic tangent function of Fig. 4.10. For the limiting
value a, we set
and for the limiting value of -a, we set
where  is an appropriate positive constant.For the choice of a  1.7159 used in Fig.4.10,
we may set   0.7159, in which case the target value (desired response) dj can be con-
veniently chosen as "1, as indicated in the figure.
5. Normalizing the inputs. Each input variable should be preprocessed so that its
mean value, averaged over the entire training sample, is close to zero, or else it will be
small compared to its standard deviation (LeCun, 1993).To appreciate the practical sig-
nificance of this rule,consider the extreme case where the input variables are consistently
positive. In this situation, the synaptic weights of a neuron in the first hidden layer can
only increase together or decrease together. Accordingly, if the weight vector of that
dj = -a + 
dj = a - 
146
Chapter 4
Multilayer Perceptrons
a  1.7159
a  1.7159
1.0
0
1.0
1.0
1.0
w(v)
v
FIGURE 4.10
Graph of the hyperbolic tangent function (v)  atanh(bv) for a  1.7159 and
b  2/3.The recommended target values are 1 and -1.

neuron is to change direction, it can do so only by zigzagging its way through the error
surface, which is typically slow and should therefore be avoided.
In order to accelerate the back-propagation learning process, the normalization of
the inputs should also include two other measures (LeCun, 1993):
• The input variables contained in the training set should be uncorrelated; this can
be done by using principal-components analysis, to be discussed in Chapter 8.
• The decorrelated input variables should be scaled so that their covariances are
approximately equal, thereby ensuring that the different synaptic weights in the
network learn at approximately the same speed.
Figure 4.11 illustrates the results of three normalization steps: mean removal, decorre-
lation, and covariance equalization, applied in that order.
Section 4.6
Heuristics for Making the Back-Propagation Algorithm Perform Better
147
x2
x1
Mean
removal
Decorrelation
Original set of
data points
x2
x1
x2
x1
x1
Covariance
equalization
x2'
'
'''
'''
''
''
FIGURE 4.11
Illustrating the operation of mean removal, decorrelation, and covariance
equalization for a two-dimensional input space.

It is also of interest to note that when the inputs are transformed in the manner
illustrated in Fig. 4.11 and used in conjunction with the hyperbolic tangent function
specified in Fig. 4.10, the variance of the individual neural outputs in the multilayer
perceptron will be close to unity (Orr and Müller, 1998).The rationale for this state-
ment is that the effective gain of the sigmoid function over its useful range is roughly
unity.
6. Initialization. A good choice for the initial values of the synaptic weights and
thresholds of the network can be of tremendous help in a successful network design.The
key question is:What is a good choice?
When the synaptic weights are assigned large initial values, it is highly likely that
the neurons in the network will be driven into saturation. If this happens, the local
gradients in the back-propagation algorithm assume small values, which in turn will
cause the learning process to slow down. However, if the synaptic weights are assigned
small initial values, the back-propagation algorithm may operate on a very flat area
around the origin of the error surface; this is particularly true in the case of sigmoid
functions such as the hyperbolic tangent function. Unfortunately, the origin is a saddle
point, which refers to a stationary point where the curvature of the error surface across
the saddle is negative and the curvature along the saddle is positive. For these rea-
sons, the use of both large and small values for initializing the synaptic weights should
be avoided. The proper choice of initialization lies somewhere between these two
extreme cases.
To be specific, consider a multilayer perceptron using the hyperbolic tangent func-
tion for its activation functions. Let the bias applied to each neuron in the network be
set to zero.We may then express the induced local field of neuron j as
Let it be assumed that the inputs applied to each neuron in the network have zero mean
and unit variance, as shown by
and
Let it be further assumed that the inputs are uncorrelated, as shown by
and that the synaptic weights are drawn from a uniformly distributed set of numbers with
zero mean, that is,
and variance
w
2 = [(wji - w)2] = [w2
ji]  for all (j, i) pairs
w = [wji] = 0  for all (j, i) pairs
[yiyk] = e 1
for k = i
0
for k Z i
2
y = [(yi - i)2] = [yi
2] = 1  for all i
y = [yi] = 0  for all i
vj = a
m
i=1
wjiyi
148
Chapter 4
Multilayer Perceptrons

Accordingly, we may express the mean and variance of the induced local field vj as
and
where m is the number of synaptic connections of a neuron.
In light of this result, we may now describe a good strategy for initializing the
synaptic weights so that the standard deviation of the induced local field of a neuron lies
in the transition area between the linear and saturated parts of its sigmoid activation
function. For example, for the case of a hyperbolic tangent function with parameters a
and b used in Fig. 4.10, this objective is satisfied by setting v  1 in the previous equa-
tion, in which case we obtain the following (LeCun, 1993):
(4.48)
Thus, it is desirable for the uniform distribution, from which the synaptic weights are
selected, to have a mean of zero and a variance equal to the reciprocal of the number
of synaptic connections of a neuron.
7. Learning from hints. Learning from a sample of training examples deals with an
unknown input–output mapping function f(·). In effect, the learning process exploits
the information contained in the examples about the function f(·) to infer an approxi-
mate implementation of it.The process of learning from examples may be generalized
to include learning from hints, which is achieved by allowing prior information that we
may have about the function f(·) to be included in the learning process (Abu-Mostafa,
1995). Such information may include invariance properties, symmetries, or any other
knowledge about the function f(·) that may be used to accelerate the search for its
approximate realization and, more importantly, to improve the quality of the final esti-
mate.The use of Eq. (4.48) is an example of how this is achieved.
8. Learning rates. All neurons in the multilayer perceptron should ideally learn at
the same rate. The last layers usually have larger local gradients than the layers at the
front end of the network. Hence, the learning-rate parameter  should be assigned a
w = m-12 
 = m2
w
 = a
m
i=1
 [wji
2]
 = a
m
i=1 a
m
k=1
[wjiwjk][yiyk]
 = c a
m
i=1 a
m
k=1
wjiwjkyiykd
 2
v = [(vj - v)2] = [vj
2]
 v = [vj] = c a
m
i=1
wjiyid = a
m
i=1
[wji][yi] = 0
Section 4.6
Heuristics for Making the Back-Propagation Algorithm Perform Better
149

smaller value in the last layers than in the front layers of the multilayer perceptron.
Neurons with many inputs should have a smaller learning-rate parameter than neurons
with few inputs so as to maintain a similar learning time for all neurons in the network.
In LeCun (1993), it is suggested that for a given neuron, the learning rate should be
inversely proportional to the square root of synaptic connections made to that neuron.
4.7
COMPUTER EXPERIMENT: PATTERN CLASSIFICATION
In this computer experiment, we resume the sequence of pattern-classification experi-
ments performed first in Chapter 1 using Rosenblatt’s perceptron and then in Chapter 2
using the method of least squares. For both experiments, we used training and test data
generated by randomly sampling the double-moon structure pictured in Fig. 1.8. In each
of the experiments, we considered two cases, one employing linearly separable patterns
and the other employing nonlinearly separable patterns. The perceptron worked per-
fectly fine for the linearly separable setting of d  1, but the method of least squares
required a larger separation between the two moons for perfect classification. In any
event, they both failed the nonlinearly separable setting of d  -4.
The objective of the computer experiment presented herein is twofold:
1. to demonstrate that the multilayer perceptron, trained with the back-propagation
algorithm, is capable of classifying nonlinearly separable test data;
2. to find a more difficult case of nonlinearly separable patterns for which the mul-
tilayer perceptron fails the double-moon classification test.
The specifications of the multilayer perceptron used in the experiment are as follows:
Size of the input layer: m0  2
Size of the (only) hidden layer: m1  20
Size of the output layer: m2  1
Activation function: hyperbolic tangent function 
Threshold setting: zero
Learning-rate parameter : annealed linearly from 10-1 down to 10-5
The experiment is carried out in two parts, one corresponding to the vertical separation
d  -4, and the other corresponding to d  -5:
(a) Vertical separation d  4.
Figure 4.12 presents the results of the MLP experiment for the length of separa-
tion between the two moons of d  4. Part (a) of the figure displays the learn-
ing curve resulting from the training session. We see that the learning curve
reached convergence effectively in about 15 epochs of training. Part (b) of the
figure displays the optimal nonlinear decision boundary computed by the MLP.
Most important, perfect classification of the two patterns was achieved, with no
classification errors.This perfect performance is attributed to the hidden layer of
the MLP.
(v) =
1 - exp(-2v)
1 + exp(-2v)
150
Chapter 4
Multilayer Perceptrons

Section 4.7
Computer Experiment: Pattern Classification
151
0.4
0.35
0
10
30
40
20
50
0
0.05
MSE
Number of epochs
(a) Learning curve
Learning curve
0.1
0.15
0.2
0.25
0.3
12
8
10
5






































































































 






















































































































































































 











































































































































































































 






















































































































































































 






































































































0
5
10
15
20
6
4
2
0
2
x2
x1
Classification using MLP with distance  4, radius  10, and width  6
4
6
8
10
(b) Testing result
FIGURE 4.12
Results of the computer experiment on the back-propagation algorithm applied
to the MLP with distance d  4. MSE stands for mean-square error.

152
Chapter 4
Multilayer Perceptrons
0.45
0.35
0
10
30
40
20
50
0
0.05
MSE
Number of epochs
(a) Learning curve
Learning curve
0.1
0.15
0.2
0.25
0.4
0.3
12
10
5








 
































































































 






















































































































































































 











































































































































































































 






















































































































































































 






































































































0
5
10
15
20
6
4
2
0
2
x2
x1
Classification using MLP with distance  5, radius  10, and width  6
4
6
8
10
(b) Testing result
FIGURE 4.13
Results of the computer experiment on the back-propagation algorithm applied
to the MLP with distance d  -5.

(b) Vertical separation d  5.
To challenge the multilayer perceptron with a more difficult pattern-classification
task, we reduced the vertical separation between the two moons to d  -5.The re-
sults of this second part of the experiment are presented in Fig. 4.13.The learning
curve of the back-propagation algorithm, plotted in part (a) of the figure, shows a
slower rate of convergence, roughly three times that for the easier case of d  -4.
Moreover, the testing results plotted in part (b) of the figure reveal three classifi-
cation errors in a testing set of 2,000 data points, representing an error rate of 0.15
percent.
The decision boundary is computed by finding the coordinates 
and 
pertain-
ing to the input vector x, for which the response of the output neuron is zero on the
premise that the two classes of the experiment are equally likely. Accordingly, when a
threshold of zero is exceeded, a decision is made in favor of one class; otherwise, the
decision is made in favor of the other class.This procedure is followed on all the double-
moon classification experiments reported in the book.
4.8
BACK PROPAGATION AND DIFFERENTIATION
Back propagation is a specific technique for implementing gradient descent in weight
space for a multilayer perceptron.The basic idea is to efficiently compute partial deriv-
atives of an approximating function F(w, x) realized by the network with respect to all
the elements of the adjustable weight vector w for a given value of input vector x. Herein
lies the computational power of the back-propagation algorithm.4
To be specific, consider a multilayer perceptron with an input layer of m0 nodes,
two hidden layers, and a single output neuron, as depicted in Fig. 4.14.The elements of
the weight vector w are ordered by layer (starting from the first hidden layer), then by
neurons in a layer, and then by the number of a synapse within a neuron. Let w(l)
ji denote
the synaptic weight from neuron i to neuron j in layer l  1, 2, .... For l  1, corre-
sponding to the first hidden layer, the index i refers to a source node rather than to a
x2
x1
Section 4.8
Back Propagation and Differentiation
153
Input
layer
First
hidden
layer
Second
hidden
layer
Output
layer
x1
y
x2
xm0
•
•
•
•
•
•
•
•
•
FIGURE 4.14
Multilayer
perceptron with two hidden
layers and one output neuron.

neuron. For l  3, corresponding to the output layer in Fig. 4.14, we have j  1.We wish
to evaluate the derivatives of the function F(w, x) with respect to all the elements of the
weight vector w for a specified input vector 
.We have included the
weight vector w as an argument of the function F in order to focus attention on it. For
example, for l  2 (i.e., a single hidden layer and a linear output layer), we have
(4.49)
where w is the ordered weight vector and x is the input vector.
The multilayer perceptron of Fig. 4.14 is parameterized by an architecture a (rep-
resenting a discrete parameter) and a weight vector w (made up of continuous elements).
Let aj
(l) denote that part of the architecture extending from the input layer (l  0) to
node j in layer l  1, 2, 3.Accordingly, we may write
(4.50)
where 
is the activation function. However, a1
(3) is to be interpreted merely as an archi-
tectural symbol rather than a variable.Thus, adapting Eqs. (4.2), (4.4), (4.13), and (4.25)
for use in this new situation, we obtain the formulas
(4.51)
(4.52)
(4.53)
where  is the partial derivative of the nonlinearity 
with respect to its argument and
xi is the ith element of the input vector x. In a similar way, we may derive the equations
for the partial derivatives of a general network with more hidden layers and more neu-
rons in the output layer.
Equations (4.51) through (4.53) provide the basis for calculating the sensitivity of
the network function F(w, x) with respect to variations in the elements of the weight vec-
tor w. Let 
denote an element of the weight vector w. The sensitivity of F(w, x) with
respect to , is formally defined by
It is for this reason that we refer to the lower part of the signal-flow graph in Fig. 4.7 as
a “sensitivity graph.”
The Jacobian
Let W denote the total number of free parameters (i.e., synaptic weights and biases) of
a multilayer perceptron,which are ordered in a manner described to form the weight vec-
tor w. Let N denote the total number of examples used to train the network. Using back
SF
# = 0FF
0##
#
#


xic a
k
w(3)
1k ¿(a(2)
k )w(2)
kj d
 
0F(w, x)
0w(1)
ji
= ¿(a(3)
1 )¿(a(1)
j )
 
0F(w, x)
0w(2)
kj
= ¿(a(3)
1 )¿(a(2)
k )(a (1)
j )w(3)
1k
 
0F(w, x)
0w(3)
1k
= ¿(a(3)
1 )(a (2)
k )

F(w, x) = (a 1
(3))
F(w, x) = a
m1
j=0
woj  a a
m0
i=0
wjixi b
x = [x1, x2, ..., xm0]T
154
Chapter 4
Multilayer Perceptrons

propagation, we may compute a set of W partial derivatives of the approximating func-
tion F[w, x(n)] with respect to the elements of the weight vector w for a specific exam-
ple x(n) in the training sample. Repeating these computations for n  1, 2, ..., N, we end
up with an N-by-W matrix of partial derivatives.This matrix is called the Jacobian J of
the multilayer perceptron evaluated at x(n). Each row of the Jacobian corresponds to a
particular example in the training sample.
There is experimental evidence to suggest that many neural network training
problems are intrinsically ill conditioned, leading to a Jacobian J that is almost rank
deficient (Saarinen et al., 1991).The rank of a matrix is equal to the number of linearly
independent columns or rows in the matrix, whichever one is smallest. The Jacobian
J is said to be rank deficient if its rank is less than min (N, W). Any rank deficiency
in the Jacobian causes the back-propagation algorithm to obtain only partial infor-
mation of the possible search directions. Rank deficiency also causes training times
to be long.
4.9
THE HESSIAN AND ITS ROLE IN ON-LINE LEARNING
The Hessian matrix, or simply the Hessian, of the cost function 
, denoted by H, is
defined as the second derivative of 
with respect to the weight vector w, as shown
by
(4.54)
The Hessian plays an important role in the study of neural networks; specifically, we
mention the following points5:
1. The eigenvalues of the Hessian have a profound influence on the dynamics of
back-propagation learning.
2. The inverse of the Hessian provides a basis for pruning (i.e., deleting) insignifi-
cant synaptic weights from a multilayer perceptron; this issue will be discussed in
Section 4.14.
3. The Hessian is basic to the formulation of second-order optimization methods as
an alternative to back-propagation learning, to be discussed in Section 4.16.
In this section, we confine our attention to point 1.
In Chapter 3, we indicated that the eigenstructure of the Hessian has a pro-
found influence on the convergence properties of the LMS algorithm. So it is also
with the back-propagation algorithm, but in a much more complicated way.Typically,
the Hessian of the error surface pertaining to a multilayer perceptron trained with the
back-propagation algorithm has the following composition of eigenvalues (LeCun
et al., 1998):
• a small number of small eigenvalues,
• a large number of medium-sized eigenvalues, and
• a small number of large eigenvalues.
There is therefore a wide spread in the eigenvalues of the Hessian.
H =
02eav(w)
0w2
eav(w)
eav(w)
Section 4.9
The Hessian and Its Role in On-line Learning
155

The factors affecting the composition of the eigenvalues may be grouped as follows:
• nonzero-mean input signals or nonzero-mean induced neural output signals;
• correlations between the elements of the input signal vector and correlations
between induced neural output signals;
• wide variations in the second-order derivatives of the cost function with respect to
synaptic weights of neurons in the network as we proceed from one layer to the
next. These derivatives are often smaller in the lower layers, with the synaptic
weights in the first hidden layer learning slowly and those in the last layers learn-
ing quickly.
Avoidance of Nonzero-mean Inputs
From Chapter 3,we recall that the learning time of the LMS algorithm is sensitive to vari-
ations in the condition number max/min, where max is the largest eigenvalue of the Hes-
sian and min is its smallest nonzero eigenvalue. Experimental results show that a similar
situation holds for the back-propagation algorithm, which is a generalization of the LMS
algorithm. For inputs with nonzero mean, the ratio max/min is larger than its corre-
sponding value for zero-mean inputs: The larger the mean of the inputs, the larger the
ratio max/min will be. This observation has a serious implication for the dynamics of
back-propagation learning.
For the learning time to be minimized, the use of nonzero-mean inputs should be
avoided. Now, insofar as the signal vector x applied to a neuron in the first hidden layer
of a multilayer perceptron (i.e., the signal vector applied to the input layer) is concerned,
it is easy to remove the mean from each element of x before its application to the network.
But what about the signals applied to the neurons in the remaining hidden and output
layers of the network? The answer to this question lies in the type of activation function
used in the network. In the case of the logistic function, the output of each neuron is
restricted to the interval [0, 1]. Such a choice acts as a source of systematic bias for those
neurons located beyond the first hidden layer of the network.To overcome this problem,
we need to use the hyperbolic tangent function that is odd symmetric. With this latter
choice, the output of each neuron is permitted to assume both positive and negative val-
ues in the interval [-1, 1], in which case it is likely for its mean to be zero. If the network
connectivity is large, back-propagation learning with odd-symmetric activation functions
can yield faster convergence than a similar process with nonsymmetric activation functions.
This condition provides the justification for heuristic 3 described in Section 4.6.
Asymptotic Behavior of On-line Learning
For a good understanding of on-line learning,we need to know how the ensemble-averaged
learning curve evolves across time. Unlike the LMS algorithm, this calculation is unfor-
tunately much too difficult to perform. Generally speaking, the error-performance sur-
face may have exponentially many local minima and multiple global minima because of
symmetry properties of the network. Surprisingly, this characteristic of the error-
performance surface may turn out to be a useful feature in the following sense: Given
that an early-stopping method is used for network training (see Section 4.13) or the
156
Chapter 4
Multilayer Perceptrons

network is regularized (see Section 4.14), we may nearly always find ourselves “close”
to a local minimum.
In any event, due to the complicated nature of the error-performance surface, we
find that in the literature,statistical analysis of the learning curve is confined to its asymp-
totic behavior in the neighborhood of a local minimum. In this context, we may highlight
some important aspects of this asymptotic behavior, assuming a fixed learning-rate
parameter, as follows (Murata, 1998):
(i) The learning curve consists of three terms:
• minimal loss, determined by the optimal parameter w*,which pertains to a local
or global minimum;
• additional loss, caused by fluctuations in evolution of the weight-vector esti-
mator w(n) around the mean
• a time-dependent term, describing the effect of decreasing speed of error con-
vergence on algorithmic performance.
(ii) To ensure stability of the on-line learning algorithm, the learning-rate parameter
 must be assigned a value smaller than the reciprocal of the largest eigenvalue of
the Hessian, 1/max. On the other hand, the speed of convergence of the algorithm
is dominated by the smallest eigenvalue of the Hessian, min.
(iii) Roughly speaking, if the learning-rate parameter  is assigned a large value, then
the speed of convergence is fast,but there will be large fluctuations around the local
or global minimum, even if the number of iterations, n, approaches infinity.
Conversely, if  is small, then the extent of fluctuations is small, but the speed of
convergence will be slow.
4.10
OPTIMAL ANNEALING AND ADAPTIVE CONTROL 
OF THE LEARNING RATE
In Section 4.2, we emphasized the popularity of the on-line learning algorithm for two
main reasons:
(i) The algorithm is simple, in that its implementation requires a minimal amount of
memory, which is used merely to store the old value of the estimated weight vec-
tor from one iteration to the next.
(ii) With each example {x, d} being used only once at every time-step, the learning
rate assumes a more important role in on-line learning than in batch learning,
in that the on-line learning algorithm has the built-in ability to track statistical
variations in the environment responsible for generating the training set of
examples.
In Amari (1967) and, more recently, Opper (1996), it is shown that optimally annealed
on-line learning is enabled to operate as fast as batch learning in an asymptotic sense.This
issue is explored in what follows.
lim
nS q[wˆ (n)] = w*
Section 4.10
Optimal Annealing and Adaptive Control of the Learning Rate
157

Optimal Annealing of the Learning Rate
Let w denote the vector of synaptic weights in the network, stacked up on top of each
other in some orderly fashion.With 
(n) denoting the old estimate of the weight vector
w at time-step n, let 
(n1) denote the updated estimate of w on receipt of the “input-
wˆ
wˆ
158
Chapter 4
Multilayer Perceptrons
Updated
estimate
Old
estimate
Learning-
rate 
parameter
Error signal
Partial derivative of
the network function F
¯˚˘˚˙
¯˚˚˚˚˘˚˚˚˚˙
¯˘˙
¯˘˙ ¯˚˚˚˚˚˚˚˚˘˚˚˚˚˚˚˚˚˙
desired response”example {x(n1),d(n1)}.Correspondingly,let F(x(n1);
(n)) denote
wˆ
the vector-valued output of the network produced in response to the input x(n1); natu-
rally the dimension of the function F must be the same as that of the desired response
vector d(n). Following the defining equation of Eq. (4.3), we may express the instanta-
neous energy as the squared Euclidean norm of the estimation error, as shown by
(4.55)
The mean-square error, or expected risk, of the on-line learning problem is defined by
(4.56)
where x,d is the expectation operator performed with respect to the example {x, d}.The
solution
(4.57)
defines the optimal parameter vector.
The instantaneous gradient vector of the learning process is defined by
(4.58)
where
(4.59)
With the definition of the gradient vector just presented, we may now express the on-
line learning algorithm as
(4.60)
or, equivalently,
(4.61)
Given this difference equation, we may go on to describe the ensemble-averaged
dynamics of the weight vector w in the neighborhood of the optimal parameter w* by
the continuous differential equation
(4.62)
d
dt wˆ (t) = - (t)x,d[g(x(t), d(t); wˆ(t))]
wˆ (n + 1) = wˆ (n) + (n)[d(n + 1) - F(x(n + 1); wˆ (n))] F¿(x(n + 1); wˆ (n))
wˆ (n + 1) = wˆ (n) - (n)g(x(n + 1), d(n + 1); wˆ (n))
F¿(x; w) =
0
0w
 F(x; w)
 = - (d(n) - F(x(n); w)F¿(x(n); w)
g(x(n), d(n); w) =
0
0w e(x(n), d(n); w)
w* = arg min
w [J(w)]
J(w) = x,d[e(x, d; w)]
e(x(n), d(n); w) = 1
2
 7d(n) - F(x(n); w)7 2

where t denotes continuous time.Following Murata (1998),the expected value of the gra-
dient vector is approximated by
(4.63)
where the ensembled-averaged matrix K* is itself defined by
(4.64)
The new Hessian K* is a positive-definite matrix defined differently from the Hessian
H of Eq. (4.54). However, if the environment responsible for generating the training
examples {x, d} is ergodic, we may then substitute the Hessian H, based on time aver-
aging, for the Hessian K*, based on ensemble-averaging. In any event, using Eq. (4.63)
in Eq. (4.62), we find that the continuous differential equation describing the evolution
of the estimator 
may be approximated as
(4.65)
Let the vector q denote an eigenvector of the matrix K*, as shown by the defining
equation
(4.66)
where  is the eigenvalue associated with the eigenvector q.We may then introduce the
new function
(4.67)
which, in light of Eq. (4.63), may itself be approximated as
(4.68)
At each instant of time t, the function (t) takes on a scalar value, which may be viewed
as an approximate measure of the Euclidean distance between two projections onto the
eigenvector q, one due to the optimal parameter w* and the other due to the estimator
.The value of (t) is therefore reduced to zero if, and when, the estimator 
con-
verges to w*.
wˆ (t)
wˆ (t)
= -
qT(w* - wˆ (t))
(t) L -qTK*(w* - wˆ (t))
(t) = x,d[qTg(x, d; wˆ (t))]
K*q = 
q
d
dt wˆ (t) L - (t)K*(w* - wˆ (t))
wˆ (t)
 = x,dc 02
0w2 e(x, d; w)d
K* = x,dc 0
0w g(x, d; w)d
x,d[g(x, d; wˆ (t))] L -K*(w* - wˆ (t))
Section 4.10
Optimal Annealing and Adaptive Control of the Learning Rate
159
From Eqs. (4.65), (4.66), and (4.68), we find that the function (t) is related to the
time-varying learning-rate parameter (t) as follows:
(4.69)
This differential equation may be solved to yield
(4.70)
where c is a positive integration constant.
(t) = c exp(-
(t)dt)
d
dt (t) = -
(t)(t)

the exponent be large compared with unity, which may be satisfied by setting 0  /
for positive .
Now, there remains only the issue of how to choose the eigenvector q. From the
previous section, we recall that the convergence speed of the learning curve is domi-
nated by the smallest eigenvalue min of the Hessian H. With this Hessian and the new
Hessian K* tending to behave similarly, a clever choice is to hypothesize that for a
sufficiently large number of iterations,the evolution of the estimator (t) over time t may
be considered as a one-dimensional process, running “almost parallel” to the eigenvector
of the Hessian K* associated with the smallest eigenvalue min, as illustrated in Fig. 4.15.
We may thus set
(4.73)
where the normalization is introduced to make the eigenvector q assume unit Euclidean
length. Correspondingly, the use of this formula in Eq. (4.67) yields
(4.74)
We may now summarize the results of the discussion presented in this section by
making the following statements:
1. The choice of the annealing schedule described in Eq. (4.71) satisfies the two con-
ditions
(4.75)
a
t
(t) S q and a
t
2(t) 7 q, as t S q
(t) = 7x,d[g(x, d; wˆ (t))] 7
q =
x,d[g(x, d; wˆ  )]
7x,d[g(x, d; wˆ  )]7
wˆ
Following the annealing schedule due to Darken and Moody (1991) that was dis-
cussed in Chapter 3 on the LMS algorithm, let the formula
(4.71)
account for dependence of the learning-rate on time t, where  and 0 are positive tun-
ing parameters. Then, substituting this formula into Eq. (4.70), we find that the corre-
sponding formula tor the function (t) is
(4.72)
For (t) to vanish as time t approaches infinity, we require that the product term 0 in

(t) = c(t + )-
0

(t) =

t +  0
160
Chapter 4
Multilayer Perceptrons
Trajectory
of wˆ (t)
w  *
FIGURE 4.15
The evolution of the estimator 
over time t.The ellipses represent
contours of the expected risk for varying values of w, assumed to be two-dimensional.
wˆ (t)

In other words, (t) satisfies the requirements of stochastic approximation theory
(Robbins and Monro, 1951).
2. As time t approaches infinity, the function (t) approaches zero asymptotically. In
accordance with Eq. (4.68), it follows that the estimator 
approaches the opti-
mal estimator w* as t approaches infinity.
3. The ensemble-averaged trajectory of the estimator 
is almost parallel to the
eigenvector of the Hessian K* associated with the smallest eigenvalue min after a
large enough number of iterations.
4. The optimally annealed on-line learning algorithm for a network characterized by
the weight vector w is collectively described by the following set of three equations:
(4.76)
Here, it is assumed that the environment responsible for generating the training
examples {x, d} is ergodic, so that the ensemble-averaged Hessian K* assumes the
same value as the time-averaged Hessian H.
5. When the learning-rate parameter 0 is fixed in on-line learning based on stochas-
tic gradient descent, stability of the algorithm requires that we choose 0  l$max,
where max is the largest eigenvalue of the Hessian H. On the other hand, in the
case of optimally annealed stochastic gradient descent, according to the third line
of Eq. (4.76), the choice is 0  1$min, where min is the smallest eigenvalue of H.
6. The time constant nswitch, a positive integer, defines the transition from a regime of
fixed 0 to the annealing regime, where the time-varying learning-rate parameter
(n) assumes the desired form c/n, where c is a constant, in accordance with sto-
chastic approximation theory.
Adaptive Control of the Learning Rate
The optimal annealing schedule, described in the second line of Eq. (4.76), provides an
important step in improved utilization of on-line learning. However, a practical limita-
tion of this annealing schedule is the requirement that we know the time constant switch
a priori.A practical issue of concern, then, is the fact that when the application of inter-
est builds on the use of on-line learning in a nonstationary environment where the sta-
tistics of the training sequence change from one example to the next, the use of a
prescribed time constant nswitch may no longer be a realistic option. In situations of this
kind, which occur frequently in practice, the on-line learning algorithm needs to be
equipped with a built-in mechanism for the adaptive control of the learning rate. Such
0 =


min
,   = positive constant
(n) =
nswitch
n + nswitch
 0
wˆ(n + 1) = wˆ (n) + (n)(d(n + 1) - F(x(n) + 1; wˆ (n))F¿(x(n + 1); wˆ (n))
wˆ (t)
wˆ(t)

Section 4.10
Optimal Annealing and Adaptive Control of the Learning Rate
161
Updated
estimate
Old
estimate
Learning-
rate 
parameter
Error signal
Partial derivative of
the network function F
¯˚˘˚˙
¯˘˙
¯˘˙
¯˚˚˚˚˘˚˚˚˚˙
¯˚˚˚˚˚˚˚˘˚˚˚˚˚˚˚˙
¯˚˚˚˚˚˚˘˚˚˚˚˚˚˙

a mechanism was first described in the literature by Murata (1998), in which the so-
called learning of the learning algorithm (Sompolinsky et al., 1995) was appropriately
modified.
The adaptive algorithm due to Murata is configured to achieve two objectives:
1. automatic adjustment of the learning rate, which accounts for statistical variations
in the environment responsible for generation of the training sequence of examples;
2. generalization of the on-line learning algorithm so that its applicability is broad-
ened by avoiding the need for a prescribed cost function.
To be specific, the ensemble-averaged dynamics of the weight vector w, defined in
Eq. (4.62), is now rewritten as6
(4.77)
where the vector-valued function f(·,·;·) denotes flow that determines the change applied
to the estimator 
in response to the incoming example {x(t), d(t)}. The flow f is
required to satisfy the condition
(4.78)
where w* is the optimal value of the weight vector w, as previously defined in Eq. (4.57).
In other words, the flow f must asymptotically converge to the optimal parameter w*
across time t. Moreover, for stability, we also require that the gradient of f should be a
positive-definite matrix.The flow f includes the gradient vector g in Eq. (4.62) as a spe-
cial case.
The previously defined equations of Eqs. (4.63) through (4.69) apply equally well
to Murata’s algorithm.Thereafter, however, the assumption made is that the evolution
of the learning rate (t) across time t is governed by a dynamic system that comprises
the pair of differential equations
(4.79)
and
(4.80)
where it should be noted that (t) is always positive and  and  are positive constants.
The first equation of this dynamic system is a repeat of Eq. (4.69).The second equation
of the system is motivated by the corresponding differential equation in the learning of
the learning algorithm described in Sompolinsky et al. (1995).7
As before, the  in Eq. (4.79) is the eigenvalue associated with the eigenvector q
of the Hessian K*. Moreover, it is hypothesized that q is chosen as the particular eigen-
vector associated with the smallest eigenvalue min.This,in turn,means that the ensemble-
averaged flow f converges to the optimal parameter w* in a manner similar to that
previously described, as depicted in Fig. 4.15.

d
dt
 (t) = (t)((t) - (t))
d
dt
 (t) = -
(t)(t)
x,d[f(x, d; w*)] = 0
wˆ (t)
d
dt wˆ (t) = -(t)x,d[f(x(t), d(t); wˆ (t))]
162
Chapter 4
Multilayer Perceptrons

The asymptotic behavior of the dynamic system described in Eqs. (4.79) and (4.80)
is given by the corresponding pair of equations
(4.81)
and
(4.82)
The important point to note here is that this new dynamic system exhibits the desired
annealing of the learning rate (t)—namely,c/t for large t—which is optimal for any esti-
mator 
converging to w*, as previously discussed.
In light of the considerations just presented, we may now formally describe the
Murata adaptive algorithm for on-line learning in discrete time as follows (Murata, 1998;
M¨uller et al., 1998):
(4.83)
(4.84)
(4.85)
The following points are noteworthy in the formulation of this discrete-time system of
equations:
• Equation (4.83) is simply the instantaneous discrete-time version of the differen-
tial equation of Eq. (4.77).
• Equation (4.84) includes an auxiliary vector r(n), which has been introduced to
account for the continuous-time function (t). Moreover, this second equation
of the Murata adaptive algorithm includes a leakage factor whose value  controls
the running average of the flow f.
• Equation (4.85) is a discrete-time version of the differential equation Eq.(4.80).The
updated auxiliary vector r(n  1) included in Eq. (4.85) links it to Eq. (4.84); in so
doing,allowance is made for the linkage between the continuous-time functions (t)
and (t) previously defined in Eqs. (4.79) and (4.80).
Unlike the continuous-time dynamic system described in Eqs.(4.79) and (4.80),the
asymptotic behavior of the learning-rate parameter (t) in Eq. (4.85) does not converge
to zero as the number of iterations, n, approaches infinity, thereby violating the
requirement for optimal annealing. Accordingly, in the neighborhood of the optimal
parameter w*, we now find that for the Murata adaptive algorithm:
(4.86)
This asymptotic behavior is different from that of the optimally annealed on-line learn-
ing algorithm of Eq. (4.76). Basically, the deviation from optimal annealing is attributed
to the use of a running average of the flow in Eq. (4.77), the inclusion of which was moti-
vated by the need to account for the algorithm not having access to a prescribed cost
lim
nS q
 
wˆ (n) Z w*


(n + 1) = (n) + (n)( 7r(n + 1) 7 - (n))
r(n + 1) = r(n) + f(x(n + 1), d(n + 1); wˆ (n)),  0 6  6 1
wˆ (n + 1) = wˆ (n) - (n)f(x(n + 1), d(n + 1); wˆ (n))
wˆ (t)
(t) = c
t
,  c = 
-1
(t) = 1
 a 1

 - 1
 b 1
t,     7 
Section 4.10
Optimal Annealing and Adaptive Control of the Learning Rate
163

function, as was the case in deriving the optimally annealed on-line learning algorithm
of Eq. (4.76).
The learning of the learning rule is useful when the optimal 
varies with time n
slowly (i.e., the environment responsible for generating the examples is nonstationary)
or it changes suddenly. On the other hand, the 1/n rule is not a good choice in such an
environment, because n becomes very small for large n, causing the 1/n rule to lose its
learning capability.Basically,the difference between the optimally annealed on-learning
algorithm of Eq. (4.76) and the on-line learning algorithm described in Eqs. (4.83) to
(4.85) is that the latter has a built-in mechanism for adaptive control of the learning
rate—hence its ability to track variations in the optimal 
.
A final comment is in order: Although the Murata adaptive algorithm is indeed
suboptimal insofar as annealing of the learning-rate parameter is concerned, its impor-
tant virtue is the broadened applicability of on-line learning in a practically imple-
mentable manner.
4.11
GENERALIZATION
In back-propagation learning, we typically start with a training sample and use the back-
propagation algorithm to compute the synaptic weights of a multilayer perceptron by
loading (encoding) as many of the training examples as possible into the network.The
hope is that the neural network so designed will generalize well. A network is said to
generalize well when the input–output mapping computed by the network is correct (or
nearly so) for test data never used in creating or training the network; the term “gener-
alization” is borrowed from psychology. Here, it is assumed that the test data are drawn
from the same population used to generate the training data.
The learning process (i.e., training of a neural network) may be viewed as a “curve-
fitting”problem.The network itself may be considered simply as a nonlinear input–output
mapping. Such a viewpoint then permits us to look at generalization not as a mystical
property of neural networks, but rather simply as the effect of a good nonlinear inter-
polation of the input data.The network performs useful interpolation primarily because
multilayer perceptrons with continuous activation functions lead to output functions
that are also continuous.
Figure 4.16a illustrates how generalization may occur in a hypothetical network.
The nonlinear input–output mapping represented by the curve depicted in this figure is
computed by the network as a result of learning the points labeled as “training data.”
The point marked in red on the curve as “generalization” is thus seen as the result of
interpolation performed by the network.
A neural network that is designed to generalize well will produce a correct
input–output mapping even when the input is slightly different from the examples used
to train the network, as illustrated in the figure.When, however, a neural network learns
too many input–output examples, the network may end up memorizing the training
data. It may do so by finding a feature (due to noise, for example) that is present in the
training data, but not true of the underlying function that is to be modeled. Such a phe-
nomenon is referred to as overfitting or overtraining. When the network is overtrained,
it loses the ability to generalize between similar input–output patterns.
wˆ *
wˆ *
164
Chapter 4
Multilayer Perceptrons

Ordinarily,loading data into a multilayer perceptron in this way requires the use of
more hidden neurons than are actually necessary, with the result that undesired contri-
butions in the input space due to noise are stored in synaptic weights of the network.An
example of how poor generalization due to memorization in a neural network may occur
is illustrated in Fig. 4.16b for the same data as depicted in Fig. 4.16a.“Memorization” is
essentially a “look-up table,” which implies that the input–output mapping computed by
the neural network is not smooth.As pointed out in Poggio and Girosi (1990a), smooth-
ness of input–output mapping is closely related to such model-selection criteria as Occam’s
Section 4.11
Generalization
165







Nonlinear mapping
learned through 
training
Nonlinear mapping
learned through training
 Training data points
Generalization point
 Training data points
Generalization point
Output
Input
0
0
(a)







Output
Input
(b)
FIGURE 4.16
(a) Properly fitted nonlinear mapping with good generalization. (b) Overfitted
nonlinear mapping with poor generalization.

razor,the essence of which is to select the “simplest”function in the absence of any prior
knowledge to the contrary.In the context of our present discussion,the simplest function
means the smoothest function that approximates the mapping for a given error criterion,
because such a choice generally demands the fewest computational resources. Smooth-
ness is also natural in many applications, depending on the scale of the phenomenon
being studied.It is therefore important to seek a smooth nonlinear mapping for ill-posed
input–output relationships,so that the network is able to classify novel patterns correctly
with respect to the training patterns (Wieland and Leighton, 1987).
Sufficient Training-Sample Size for a Valid Generalization
Generalization is influenced by three factors: (1) the size of the training sample and
how representative the training sample is of the environment of interest, (2) the archi-
tecture of the neural network, and (3) the physical complexity of the problem at hand.
Clearly, we have no control over the lattermost factor. In the context of the other two
factors, we may view the issue of generalization from two different perspectives:
• The architecture of the network is fixed (hopefully in accordance with the physical
complexity of the underlying problem),and the issue to be resolved is that of deter-
mining the size of the training sample needed for a good generalization to occur.
• The size of the training sample is fixed, and the issue of interest is that of deter-
mining the best architecture of network for achieving good generalization.
Both of these viewpoints are valid in their own individual ways.
In practice, it seems that all we really need for a good generalization is to have the
size of the training sample, N, satisfy the condition
(4.87)
where W is the total number of free parameters (i.e., synaptic weights and biases) in the
network,  denotes the fraction of classification errors permitted on test data (as in pat-
tern classification), and O(·) denotes the order of quantity enclosed within. For exam-
ple, with an error of 10 percent, the number of training examples needed should be
about 10 times the number of free parameters in the network.
Equation (4.87) is in accordance with Widrow’s rule of thumb for the LMS algo-
rithm, which states that the settling time for adaptation in linear adaptive temporal fil-
tering is approximately equal to the memory span of an adaptive tapped-delay-line filter
divided by the misadjustment (Widrow and Stearns, 1985; Haykin, 2002). The misad-
justment in the LMS algorithm plays a role somewhat analogous to the error  in
Eq. (4.87). Further justification for this empirical rule is presented in the next section.
4.12
APPROXIMATIONS OF FUNCTIONS
A multilayer perceptron trained with the back-propagation algorithm may be viewed
as a practical vehicle for performing a nonlinear input–output mapping of a general
nature.To be specific, let m0 denote the number of input (source) nodes of a multilayer
N = O a W
 b 
166
Chapter 4
Multilayer Perceptrons

perceptron, and let M  mL denote the number of neurons in the output layer of the
network. The input–output relationship of the network defines a mapping from an
m0-dimensional Euclidean input space to an M-dimensional Euclidean output space,
which is infinitely continuously differentiable when the activation function is likewise.In
assessing the capability of the multilayer perceptron from this viewpoint of input–output
mapping, the following fundamental question arises:
What is the minimum number of hidden layers in a multilayer perceptron with an input–output
mapping that provides an approximate realization of any continuous mapping?
Universal Approximation Theorem
The answer to this question is embodied in the universal approximation theorem8 for a
nonlinear input–output mapping, which may be stated as follows:
Let (·) be a nonconstant, bounded, and monotone-increasing continuous function. Let Im0
Section 4.12
Approximations of Functions
167
denote the m0-dimensional unit hypercube
. The space of continuous functions on Im0
[0, 1]m0
is denoted by
. Then, given any function
and
 0, there exist an integer 

f  C(Im0)
C(Im0)
m1 and sets of real constants i, bi, and wij, where i  1,
m1 and j  1,
m0 such that we
may define
(4.88)
as an approximate realization of the function f(·); that is,
for all
that lie in the input space.
The universal approximation theorem is directly applicable to multilayer percep-
trons. We first note, for example, that the hyperbolic tangent function used as the non-
linearity in a neural model for the construction of a multilayer perceptron is indeed a
nonconstant, bounded, and monotone-increasing function; it therefore satisfies the con-
ditions imposed on the function (·) Next, we note that Eq. (4.88) represents the out-
put of a multilayer perceptron described as follows:
1. The network has m0 input nodes and a single hidden layer consisting of m1 neu-
rons; the inputs are denoted by 
.
2. Hidden neuron i has synaptic weights 
, and bias bi.
3. The network output is a linear combination of the outputs of the hidden neurons,
with 
defining the synaptic weights of the output layer.
The universal approximation theorem is an existence theorem in the sense that it
provides the mathematical justification for the approximation of an arbitrary continu-
ous function as opposed to exact representation. Equation (4.88), which is the back-
bone of the theorem, merely generalizes approximations by finite Fourier series. In
effect, the theorem states that a single hidden layer is sufficient for a multilayer percep-
tron to compute a uniform 
approximation to a given training set represented by the
set of inputs
and a desired (target) output 
. However, the theorem
f(x1, ..., xm0)
x1, ..., xm0

1, ..., m1
wi1, ..., wm0
x1, ... , xm0
x1, x2, ..., xm0
F(x1, ..., xm0) - f(x1, ..., xm0)
 6
  ε
F(x1, ... , xm0) = a
m1
i=1
i a a
m0
j=1
wijxj + bi b
...,
...,

does not say that a single hidden layer is optimum in the sense of learning time, ease of
implementation, or (more importantly) generalization.
Bounds on Approximation Errors
Barron (1993) has established the approximation properties of a multilayer perceptron,
assuming that the network has a single layer of hidden neurons using sigmoid functions
and a linear output neuron. The network is trained using the back-propagation algo-
rithm and then tested with new data. During training, the network learns specific points
of a target function f in accordance with the training data and thereby produces the
approximating function F defined in Eq. (4.88). When the network is exposed to test
data that have not been seen before,the network function F acts as an “estimator”of new
points of the target function; that is,
.
A smoothness property of the target function f is expressed in terms of its Fourier
representation. In particular, the average of the norm of the frequency vector weighted
by the Fourier magnitude distribution is used as a measure for the extent to which the
function f oscillates. Let 
denote the multidimensional Fourier transform of the
function f(x),
the m0-by-1 vector  is the frequency vector.The function f(x) is
defined in terms of its Fourier transform 
by the inverse formula
(4.89)
where 
. For the complex-valued function 
for which 
is integrable,
we define the first absolute moment of the Fourier magnitude distribution of the function
f as
(4.90)
where 
is the Euclidean norm of  and 
is the absolute value of 
. The
first absolute moment Cf quantifies the smoothness of the function f.
The first absolute moment Cf provides the basis for a bound on the error that
results from the use of a multilayer perceptron represented by the input–output map-
ping function F(x) of Eq. (4.88) to approximate f(x). The approximation error is mea-
sured by the integrated squared error with respect to an arbitrary probability measure
 on the ball 
of radius r  0. On this basis, we may state the follow-
ing proposition for a bound on the approximation error given by Barron (1993):
For every continuous function f(x) with finite first moment Cf and every m1  1, there exists
a linear combination of sigmoid-based functions F(x) of the form defined in Eq. (4.88) such
that when the function f(x) is observed at a set of values of the input vector x denoted by
{xi}N
i1 that are restricted to lie inside the prescribed ball of radius r, the result provides the fol-
lowing bound on the empirical risk:
(4.91)
where Cf¿ = (2rCf)2.
eav(N) = 1
N a
N
i=1
(f(xi) - F(xi))2 
C¿f
m1
Br = {x: 7x 7  r}
f~()
f~()
7 7
Cf =
3m0 f~() * 77 12 d 
f~()
f~()
j = 2-1
f(x) =
3m0 f~()exp(jTx) d 
f~()
x  m0;
f~()
F = fˆ
168
Chapter 4
Multilayer Perceptrons

In Barron (1992), the approximation result of Eq. (4.91) is used to express the bound on
the risk 
resulting from the use of a multilayer perceptron with m0 input nodes
and m1 hidden neurons as follows:
(4.92)
The two terms in the bound on the risk 
express the tradeoff between two con-
flicting requirements on the size of the hidden layer:
1. Accuracy of best approximation. For this requirement to be satisfied, the size of
the hidden layer, m1, must be large in accordance with the universal approxima-
tion theorem.
2. Accuracy of empirical fit to the approximation. To satisfy this second requirement,
we must use a small ratio m1/N. For a fixed size of training sample, N, the size of
the hidden layer, m1, should be kept small, which is in conflict with the first
requirement.
The bound on the risk 
described in Eq. (4.92) has other interesting impli-
cations. Specifically, we see that an exponentially large sample size, large in the dimen-
sionality m0 of the input space, is not required to get an accurate estimate of the target
function, provided that the first absolute moment Cf remains finite.This result makes
multilayer perceptrons as universal approximators even more important in practical
terms.
The error between the empirical fit and the best approximation may be viewed as
an estimation error. Let 0 denote the mean-square value of this estimation error.Then,
ignoring the logarithmic factor logN in the second term of the bound in Eq. (4.92), we
may infer that the size N of the training sample needed for a good generalization is
about m0m1/0. This result has a mathematical structure similar to the empirical rule
of Eq. (4.87), bearing in mind that m0m1 is equal to the total number of free para-
meters W in the network. In other words, we may generally say that for good gener-
alization, the number N of training examples should be larger than the ratio of the
total number of free parameters in the network to the mean-square value of the
estimation error.
Curse of Dimensionality
Another interesting result that emerges from the bounds described in (4.92) is that when
the size of the hidden layer is optimized (i.e., the risk 
is minimized with respect
to N) by setting
then the risk 
is bounded by 
.A surprising aspect of this result
O(Cf2m0(logNN)
eav(N)
m1 M Cf a
N
m0 log N b
12
eav(N)
eav(N)
eav(N)
eav(N)  O a
C2
f
m1
b + O a m0m1
N  log N b
eav(N)
Section 4.12
Approximations of Functions
169
is that in terms of the first-order behavior of the risk 
,the rate of convergence expressed
as a function of the training-sample size N is of order (1/N)1/2 (times a logarithmic factor).
In contrast, for traditional smooth functions (e.g., polynomials and trigonometric
eav(N)

functions), we have a different behavior. Let s denote a measure of smoothness, defined
as the number of continuous derivatives of a function of interest. Then, for traditional
smooth functions, we find that the minimax rate of convergence of the total risk eav(N)
170
Chapter 4
Multilayer Perceptrons
is of order (1/N)2s/(2smo).The dependence of this rate on the dimensionality of the input
space, m0, is responsible for the curse of dimensionality, which severely restricts the
practical application of these functions.The use of a multilayer perceptron for function
approximation appears to offer an advantage over the use of traditional smooth functions.
This advantage is, however, subject to the condition that the first absolute moment Cf
remains finite; this is a smoothness constraint.
The curse of dimensionality was introduced by Richard Bellman in his studies of
adaptive control processes (Bellman,1961).For a geometric interpretation of this notion,
let x denote an m0-dimensional input vector and {(xi, di)}, i  1, 2, ..., N, denote the
training sample. The sampling density is proportional to 
. Let a function f(x) rep-
resent a surface lying in the m0-dimensional input space that passes near the data
points {(xi, di)}N
i1. Now, if the function f(x) is arbitrarily complex and (for the most
part) completely unknown, we need dense sample (data) points to learn it well.
Unfortunately, dense samples are hard to find in “high dimensions”—hence the curse
of dimensionality. In particular, there is an exponential growth in complexity as a
result of an increase in dimensionality, which, in turn, leads to the deterioration of
the space-filling properties for uniformly randomly distributed points in higher-
dimension spaces. The basic reason for the curse of dimensionality is as follows
(Friedman, 1995):
A function defined in high-dimensional space is likely to be much more complex than a
function defined in a lower-dimensional space, and those complications are harder to
discern.
Basically, there are only two ways of mitigating the curse-of-dimensionality problem:
1. Incorporate prior knowledge about the unknown function to be approximated.
This knowledge is provided over and above the training data. Naturally, the acqui-
sition of knowledge is problem dependent. In pattern classification, for example,
knowledge may be acquired from understanding the pertinent classes (categories)
of the input data.
2. Design the network so as to provide increasing smoothness of the unknown func-
tion with increasing input dimensionality.
Practical Considerations
The universal approximation theorem is important from a theoretical viewpoint because
it provides the necessary mathematical tool for the viability of feedforward networks
with a single hidden layer as a class of approximate solutions.Without such a theorem,
we could conceivably be searching for a solution that cannot exist. However, the theo-
rem is not constructive; that is, it does not actually specify how to determine a multilayer
perceptron with the stated approximation properties.
The universal approximation theorem assumes that the continuous function to be
approximated is given and that a hidden layer of unlimited size is available for the
N1m0

approximation. Both of these assumptions are violated in most practical applications
of multilayer perceptrons.
The problem with multilayer perceptrons using a single hidden layer is that the neu-
rons therein tend to interact with each other globally. In complex situations, this inter-
action makes it difficult to improve the approximation at one point without worsening
it at some other point. On the other hand, with two hidden layers, the approximation
(curve-fitting) process becomes more manageable. In particular, we may proceed as fol-
lows (Funahashi, 1989; Chester, 1990):
1. Local features are extracted in the first hidden layer. Specifically, some neurons in
the first hidden layer are used to partition the input space into regions, and other
neurons in that layer learn the local features characterizing those regions.
2. Global features are extracted in the second hidden layer. Specifically, a neuron in
the second hidden layer combines the outputs of neurons in the first hidden layer
operating on a particular region of the input space and thereby learns the global
features for that region and outputs zero elsewhere.
Further justification for the use of two hidden layers is presented in Sontag (1992) in the
Section 4.13
Cross-Validation
171
context of inverse problems.
4.13
CROSS-VALIDATION
The essence of back-propagation learning is to encode an input–output mapping (repre-
sented by a set of labeled examples) into the synaptic weights and thresholds of a multilayer
perceptron. The hope is that the network becomes well trained so that it learns enough
about the past to generalize to the future. From such a perspective, the learning process
amounts to a choice of network parameterization for a given set of data.More specifically,
we may view the network selection problem as choosing, within a set of candidate model
structures (parameterizations), the “best” one according to a certain criterion.
In this context, a standard tool in statistics, known as cross-validation, provides an
appealing guiding principle9 (Stone, 1974, 1978). First the available data set is randomly
partitioned into a training sample and a test set. The training sample is further parti-
tioned into two disjoint subsets:
• an estimation subset, used to select the model;
• a validation subset, used to test or validate the model.
The motivation here is to validate the model on a data set different from the one used
for parameter estimation. In this way, we may use the training sample to assess the per-
formance of various candidate models and thereby choose the “best” one.There is, how-
ever, a distinct possibility that the model with the best-performing parameter values so
selected may end up overfitting the validation subset. To guard against this possibility,
the generalization performance of the selected model is measured on the test set, which
is different from the validation subset.
The use of cross-validation is appealing particularly when we have to design a
large neural network with good generalization as the goal. For example, we may use

cross-validation to determine the multilayer perceptron with the best number of hidden
neurons and to figure out when it is best to stop training, as described in the next two
subsections.
Model Selection
To expand on the idea of selecting a model in accordance with cross-validation, con-
sider a nested structure of Boolean function classes denoted by
(4.93)
In words, the kth function class 
encompasses a family of multilayer perceptrons with
similar architecture and weight vectors w drawn from a multidimensional weight space
. A member of this class, characterized by the function or hypothesis Fk  F(x, w),
, maps the input vector x into {0,1},where x is drawn from an input space 
with
some unknown probability P. Each multilayer perceptron in the structure described is
trained with the back-propagation algorithm,which takes care of training the parameters
of the multilayer perceptron.The model-selection problem is essentially that of choos-
ing the multilayer perceptron with the best value of w, the number of free parameters
(i.e., synaptic weights and biases). More precisely, given that the scalar desired response
for an input vector x is d  {0, 1}, we define the generalization error as the probability
We are given a training sample of labeled examples
The objective is to select the particular hypothesis F(x, w) that minimizes the general-
ization error g(F), which results when it is given inputs from the test set.
In what follows, we assume that the structure described by Eq. (4.93) has the prop-
erty that, for any sample size N, we can always find a multilayer perceptron with a large
enough number of free parameters Wmax(N) such that the training sample 
can be fitted
adequately.This assumption is merely restating the universal approximation theorem of
Section 4.12. We refer to Wmax(N) as the fitting number. The significance of Wmax(N) is
that a reasonable model-selection procedure would choose a hypothesis F(x, w) that
requires W
Wmax(N); otherwise, the network complexity would be increased.
Let a parameter r, lying in the range between 0 and 1, determine the split of the
training sample 
between the estimation subset and validation subset.With 
consist-
t
t

t
t = {(xi, di)}N
i=1
g(F) = P(F(x) Z d)  for x  x
x
w  wk
wk
fk
 = {F(x, w); w  wk},    k = 1, 2, ... , n
 fk = {Fk}
 f1 (  f2 ( p ( fn
172
Chapter 4
Multilayer Perceptrons
ing of N examples, (1  r)N examples are allotted to the estimation subset, and the
remaining rN examples are allotted to the validation subset. The estimation subset,
denoted by 
, is used to train a nested sequence of multilayer perceptrons, resulting in
the hypotheses 
of increasing complexity.With 
made up of (1 - r)N exam-
ples, we consider values of W smaller than or equal to the corresponding fitting number
Wmax((1  r)N).
t¿
f1, f2, ... , fn
t¿

The use of cross-validation results in the choice
(4.94)
where v corresponds to Wv  Wmax((1 - r)N), and 
is the classification error pro-
duced by hypothesis 
when it is tested on the validation subset 
, consisting of rN
examples.
The key issue is how to specify the parameter r that determines the split of the train-
ing sample 
between the estimation subset 
and validation subset 
.In a study described
in Kearns (1996) involving an analytic treatment of this issue and supported with detailed
computer simulations, several qualitative properties of the optimum r are identified:
• When the complexity of the target function, which defines the desired response d
in terms of the input vector x, is small compared with the sample size N, the per-
formance of cross-validation is relatively insensitive to the choice of r.
• As the target function becomes more complex relative to the sample size N, the
choice of optimum r has a more pronounced effect on cross-validation perfor-
mance, and the value of the target function itself decreases.
• A single fixed value of r works nearly optimally for a wide range of target-function
complexity.
On the basis of the results reported in Kearns (1996), a fixed value of r equal to 0.2
appears to be a sensible choice, which means that 80 percent of the training sample 
is assigned to the estimation subset and the remaining 20 percent is assigned to the
validation subset.
Early-Stopping Method of Training
Ordinarily, a multilayer perceptron trained with the back-propagation algorithm learns
in stages,moving from the realization of fairly simple to more complex mapping functions
as the training session progresses.This process is exemplified by the fact that in a typical
situation,the mean-square error decreases with an increasing number of epochs used for
training: It starts off at a large value, decreases rapidly, and then continues to decrease
slowly as the network makes its way to a local minimum on the error surface.With good
generalization as the goal, it is very difficult to figure out when it is best to stop training
if we were to look at the learning curve for training all by itself. In particular, in light of
what was said in Section 4.11 on generalization, it is possible for the network to end up
overfitting the training data if the training session is not stopped at the right point.
We may identify the onset of overfitting through the use of cross-validation, for
which the training data are split into an estimation subset and a validation subset. The
estimation subset of examples is used to train the network in the usual way, except for a
minor modification:The training session is stopped periodically (i.e.,every so many epochs),
and the network is tested on the validation subset after each period of training. More
specifically,the periodic “estimation-followed-by-validation process”proceeds as follows:
• After a period of estimation (training)—every five epochs, for example—the
synaptic weights and bias levels of the multilayer perceptron are all fixed, and the
t
t–
t¿
t
t–
fk
et–(fk)
fcv =
min
k=1, 2, p ,  {et–(fk)}
Section 4.13
Cross-Validation
173

network is operated in its forward mode. The validation error is thus measured
for each example in the validation subset.
• When the validation phase is completed, the estimation (training) is resumed for
another period, and the process is repeated.
This procedure is referred to as the early-stopping method of training, which is simple
to understand and therefore widely used in practice.
Figure 4.17 shows conceptualized forms of two learning curves, one pertaining to
measurements on the estimation subset and the other pertaining to the validation subset.
Typically, the model does not do as well on the validation subset as it does on the estima-
tion subset,on which its design was based.The estimation learning curve decreases monot-
onically for an increasing number of epochs in the usual manner.In contrast,the validation
learning curve decreases monotonically to a minimum and then starts to increase as the
training continues.When we look at the estimation learning curve, it may appear that we
could do better by going beyond the minimum point on the validation learning curve. In
reality, however, what the network is learning beyond this point is essentially noise con-
tained in the training data.This heuristic suggests that the minimum point on the valida-
tion learning curve be used as a sensible criterion for stopping the training session.
However, a word of caution is in order here. In reality, the validation-sample error
does not evolve over the number of epochs used for training as smoothly as the ideal-
ized curve shown in Fig. 4.17. Rather, the validation-sample error may exhibit few local
minima of its own before it starts to increase with an increasing number of epochs. In
such situations, a stopping criterion must be selected in some systematic manner. An
empirical investigation on multilayer perceptrons carried out by Prechelt (1998) demon-
strates experimentally that there is, in fact, a tradeoff between training time and gener-
alization performance. Based on experimental results obtained therein on 1,296 training
sessions, 12 different problems, and 24 different network architectures, it is concluded
that, in the presence of two or more local minima, the selection of a “slower” stopping
criterion (i.e., a criterion that stops later than other criteria) permits the attainment of
a small improvement in generalization performance (typically, about 4 percent, on aver-
age) at the cost of a much longer training time (about a factor of four, on average).
174
Chapter 4
Multilayer Perceptrons
0
Number of epochs
Validation-sample
error
Training-sample
error
Early-
stopping
point
Mean-
square 
error
FIGURE 4.17
Illustration of
the early-stopping rule based
on cross-validation.

Variants of Cross-Validation
The approach to cross-validation just described is also referred to as the holdout method.
There are other variants of cross-validation that find their own uses in practice, partic-
ularly when there is a scarcity of labeled examples. In such a situation, we may use
multifold cross-validation by dividing the available set of N examples into K subsets,
where K  1; this procedure assumes that K is divisible into N. The model is trained on
all the subsets except for one, and the validation error is measured by testing it on the
subset that is left out.This procedure is repeated for a total of K trials, each time using
a different subset for validation, as illustrated in Fig. 4.18 for K  4. The performance
of the model is assessed by averaging the squared error under validation over all the trials
of the experiment.There is a disadvantage to multifold cross-validation: It may require
an excessive amount of computation, since the model has to be trained K times, where
1  K  N.
When the available number of labeled examples, N, is severely limited, we may use
the extreme form of multifold cross-validation known as the leave-one-out method. In
this case, N - 1 examples are used to train the model, and the model is validated by test-
ing it on the example that is left out.The experiment is repeated for a total of N times,
each time leaving out a different example for validation.The squared error under vali-
dation is then averaged over the N trials of the experiment.
4.14
COMPLEXITY REGULARIZATION AND NETWORK PRUNING
In designing a multilayer perceptron by whatever method,we are in effect building a non-
linear model of the physical phenomenon responsible for the generation of the
input–output examples used to train the network. Insofar as the network design is sta-
tistical in nature, we need an appropriate tradeoff between reliability of the training
data and goodness of the model (i.e., a method for solving the bias–variance dilemma
discussed in Chapter 2). In the context of back-propagation learning,or any other super-
vised learning procedure for that matter, we may realize this tradeoff by minimizing the
total risk, expressed as a function of the parameter vector w, as follows:
(4.95)
The first term,
, is the standard performance metric, which depends on both the
network (model) and the input data. In back-propagation learning, it is typically defined
eav(w)
R(w) = eav(w) + 
ec(w)
Section 4.14
Complexity Regularization and Network Pruning
175
Trial 1
Trial 2
Trial 3
Trial 4
FIGURE 4.18
Illustration of the
multifold method of cross-
validation. For a given trial, the
subset of data shaded in red is used
to validate the model trained on
the remaining data.

as a mean-square error whose evaluation extends over the output neurons of the net-
work and is carried out for all the training examples on an epoch-by-epoch basis, see
Eq. (4.5). The second term,
, is the complexity penalty, where the notion of com-
plexity is measured in terms of the network (weights) alone; its inclusion imposes on the
solution prior knowledge that we may have on the models being considered. For the
present discussion,it suffices to think of  as a regularization parameter,which represents
the relative importance of the complexity-penalty term with respect to the performance-
metric term. When  is zero, the back-propagation learning process is unconstrained,
with the network being completely determined from the training examples.When  is
made infinitely large, on the other hand, the implication is that the constraint imposed
by the complexity penalty is by itself sufficient to specify the network, which is another
way of saying that the training examples are unreliable. In practical applications of
complexity regularization, the regularization parameter  is assigned a value some-
where between these two limiting cases. The subject of regularization theory is dis-
cussed in great detail in Chapter 7.
Weight-Decay Procedure
In a simplified, yet effective, form of complex regularization called the weight-decay
procedure (Hinton, 1989), the complexity penalty term is defined as the squared norm
of the weight vector w (i.e., all the free parameters) in the network, as shown by
(4.96)
where the set 
refers to all the synaptic weights in the network.This procedure oper-
ates by forcing some of the synaptic weights in the network to take values close to zero,
while permitting other weights to retain their relatively large values. Accordingly, the
weights of the network are grouped roughly into two categories:
(i) weights that have a significant influence on the network’s performance;
(ii) weights that have practically little or no influence on the network’s performance.
The weights in the latter category are referred to as excess weights. In the absence of com-
plexity regularization, these weights result in poor generalization by virtue of their high
likelihood of taking on completely arbitrary values or causing the network to overfit
the data in order to produce a slight reduction in the training error (Hush and Horne,
1993). The use of complexity regularization encourages the excess weights to assume
values close to zero and thereby improve generalization.
Hessian-Based Network Pruning: Optimal Brain Surgeon
The basic idea of an analytic approach to network pruning is to use information on
second-order derivatives of the error surface in order to make a trade-off between net-
work complexity and training-error performance. In particular, a local model of the
error surface is constructed for analytically predicting the effect of perturbations in
synaptic weights. The starting point in the construction of such a model is the local
ctotal
 =
a
iHctotal
w2
i
 ec(w) = 7w7 2
ec(w)
176
Chapter 4
Multilayer Perceptrons

approximation of the cost function 
by using a Taylor series about the operating
point, described as
(4.97)
where !w is a perturbation applied to the operating point w and g(w) is the gradient vec-
tor evaluated at w. The Hessian is also evaluated at the point w, and therefore, to be
correct, we should denote it by H(w).We have not done so in Eq. (4.97) merely to sim-
plify the notation.
The requirement is to identify a set of parameters whose deletion from the multi-
layer perceptron will cause the least increase in the value of the cost function 
.To solve
this problem in practical terms, we make the following approximations:
1. Extremal Approximation. We assume that parameters are deleted from the net-
work only after the training process has converged (i.e.,the network is fully trained).The
implication of this assumption is that the parameters have a set of values correspond-
ing to a local minimum or global minimum of the error surface. In such a case, the gra-
dient vector g may be set equal to zero, and the term gT!w on the right-hand side of
Eq. (4.97) may therefore be ignored;otherwise,the saliency measures (defined later) will
be invalid for the problem at hand.
2. Quadratic Approximation. We assume that the error surface around a local
minimum or global minimum is “nearly quadratic.” Hence, the higher-order terms in
Eq. (4.97) may also be neglected.
Under these two assumptions, Eq. (4.97) is simplified as
(4.98)
Equation (4.98) provides the basis for the pruning procedure called optimal brain sur-
geon (OBS), which is due to Hassibi and Stork (1993).
The goal of OBS is to set one of the synaptic weights to zero in order to minimize
the incremental increase in 
given in Eq.(4.98).Let wi(n) denote this particular synap-
tic weight.The elimination of this weight is equivalent to the condition
(4.99)
where 1i is the unit vector whose elements are all zero, except for the ith element, which
is equal to unity.We may now restate the goal of OBS as follows:
Minimize the quadratic form
!wTH!w with respect to the incremental change in
1
2
1i
T ¢w + wi = 0
eav
 = 1
2¢wT H¢w
 ¢eav = e(w + ¢w) - e(w)
eav
eav(w + ¢w) = eav(w) + gT(w)¢w + 1
2  ¢wTH¢w + O( 7¢w 7 3)
eav
Section 4.14
Complexity Regularization and Network Pruning
177
the weight vector, !w, subject to the constraint that
is zero, and then minimize the
result with respect to the index i.
There are two levels of minimization going on here. One minimization is over the
synaptic-weight vectors that remain after the ith weight vector is set equal to zero.The
second minimization is over which particular vector is pruned.
1T
i ¢w + wi

To solve this constrained-optimization problem, we first construct the Lagrangian
(4.100)
where  is the Lagrange multiplier.Then, taking the derivative of the Lagrangian S with
respect to !w, applying the constraint of Eq. (4.99), and using matrix inversion, we find
that the optimum change in the weight vector w is given by
(4.101)
and the corresponding optimum value of the Lagrangian S for element wi is
(4.102)
where H1 is the inverse of the Hessian H, and [H1]i,i is the ii-th element of this inverse
matrix. The Lagrangian Si optimized with respect to !w, subject to the constraint that
the ith synaptic weight wi be eliminated, is called the saliency of wt. In effect, the saliency
Si represents the increase in the mean-square error (performance measure) that results
from the deletion of wi. Note that the saliency Si, is proportional to 
. Thus, small
weights have a small effect on the mean-square error. However, from Eq. (4.102), we see
that the saliency Si, is also inversely proportional to the diagonal elements of the inverse
Hessian. Thus, if [H1]i,i is small, then even small weights may have a substantial effect
on the mean-square error.
In the OBS procedure, the weight corresponding to the smallest saliency is the
one selected for deletion. Moreover, the corresponding optimal changes in the remain-
der of the weights are given in Eq. (4.101), which show that they should be updated
along the direction of the i-th column of the inverse of the Hessian.
According to Hassibi and coworkers commenting on some benchmark problems,the
OBS procedure resulted in smaller networks than those obtained using the weight-decay
procedure.It is also reported that as a result of applying the OBS procedure to the NETtalk
multilayer perceptron, involving a single hidden layer and well over 18,000 weights, the
network was pruned to a mere 1,560 weights, a dramatic reduction in the size of the net-
work. NETtalk, due to Sejnowski and Rosenberg (1987), is described in Section 4.18.
Computing the inverse Hessian. The inverse Hessian H-1 is fundamental to the formu-
lation of the OBS procedure. When the number of free parameters, W, in the network
is large, the problem of computing H-1 may be intractable. In what follows, we describe
a manageable procedure for computing H-1, assuming that the multilayer perceptron is
fully trained to a local minimum on the error surface (Hassibi and Stork, 1993).
To simplify the presentation, suppose that the multilayer perceptron has a single
output neuron.Then, for a given training sample, we may redefine the cost function of
Eq. (4.5) as
eav(w) =
1
2N a
N
n=1
(d(n) - o(n))2
wi
2
Si =
wi
2
2[H-1]i,i
¢w =
-
wi
[H-1]i,i
H-11i
S = 1
2 ¢wT H¢w - 
(1i
T¢w + wi)
178
Chapter 4
Multilayer Perceptrons

where o(n) is the actual output of the network on the presentation of the nth example,
d(n) is the corresponding desired response, and N is the total number of examples in the
training sample.The output o(n) may itself be expressed as
where F is the input–output mapping function realized by the multilayer perceptron, x
is the input vector, and w is the synaptic-weight vector of the network.The first deriva-
tive of eav with respect to w is therefore
(4.103)
and the second derivative of eav with respect to w or the Hessian is
(4.104)
where we have emphasized the dependence of the Hessian on the size of the training
sample, N.
Under the assumption that the network is fully trained—that is, the cost function
eav has been adjusted to a local minimum on the error surface—it is reasonable to say
that o(n) is close to d(n). Under this condition, we may ignore the second term and
approximate Eq. (4.104) as
(4.105)
To simplify the notation, define the W-by-1 vector
(4.106)
which may be computed using the procedure described in Section 4.8. We may then
rewrite Eq. (4.105) in the form of a recursion as follows:
(4.107)
This recursion is in the right form for application of the so-called matrix inversion lemma,
also known as Woodbury’s equality.
Let A and B denote two positive-definite matrices related by
A = B-1 + CDCT
 = H(n - 1) + (n)T(n),  n = 1, 2, ... , N
 H(n) = a
n
k=1
(k)T(k)
(n) =
1
2N
 
0F(w, x(n))
0w
H(N) L 1
N a
N
n=1
a
0F(w, x(n))
0w
b a
0F(w, x(n))
0w
b
T
 
 - 
02F(w, x(n))
0w2
(d(n) - o(n)) f
 = 1
N a
N
n=1
e a
0F(w, x(n))
0w
b a
0F(w, x(n))
0w
b
T
 H(N) = 02eav
0w2
0eav
0w = - 1
N a
N
n=1
0F(w, x(n))
0w
 (d(n) - o(n))
o(n) = F(w, x)
Section 4.14
Complexity Regularization and Network Pruning
179

where C and D are two other matrices. According to the matrix inversion lemma, the
inverse of matrix A is defined by
For the problem described in Eq. (4.107) we have
Application of the matrix inversion lemma therefore yields the desired formula for
recursive computation of the inverse Hessian:
(4.108)
Note that the denominator in Eq.(4.108) is a scalar; it is therefore straightforward to cal-
culate its reciprocal. Thus, given the past value of the inverse Hessian, H-1(n – 1), we
may compute its updated value H-l(n) on the presentation of the nth example, repre-
sented by the vector (n).This recursive computation is continued until the entire set of
N examples has been accounted for.To initialize the algorithm, we need to make H-1(0)
large, since it is being constantly reduced according to Eq. (4.108). This requirement is
satisfied by setting
where  is a small positive number and I is the identity matrix. This form of initializa-
tion assures that H-l(n) is always positive definite.The effect of  becomes progressively
smaller as more and more examples are presented to the network.
A summary of the optimal-brain-surgeon algorithm is presented in Table 4.1.
4.15
VIRTUES AND LIMITATIONS OF BACK-PROPAGATION LEARNING
First and foremost, it should be understood that the back-propagation algorithm is not
an algorithm intended for the optimum design of a multilayer perceptron. Rather, the
correct way to describe it is to say:
The back-propagation algorithm is a computationally efficient technique for computing the
gradients (i.e., first-order derivatives) of the cost function e(w), expressed as a function of
the adjustable parameters (synaptic weights and bias terms) that characterize the multilayer
perceptron.
The computational power of the algorithm is derived from two distinct properties:
1. The back-propagation algorithm is simple to compute locally.
2. It performs stochastic gradient descent in weight space, when the algorithm is
implemented in its on-line (sequential) mode of learning.
H-1(0) = -1I
H-1(n) = H-1(n - 1) -
H-1(n - 1)(n)T(n)H-1(n - 1)
1 + T(n)H-1(n - 1)(n)
 D = 1
 C = (n)
 B-1 = H(n - 1)
 A = H(n)
A-1 = B - BC(D + CTBC)-1CTB
180
Chapter 4
Multilayer Perceptrons

Connectionism
The back-propagation algorithm is an example of a connectionist paradigm that relies on
local computations to discover the information-processing capabilities of neural networks.
This form of computational restriction is referred to as the locality constraint, in the sense
that the computation performed by each neuron in the network is influenced solely by those
other neurons that are in physical contact with it. The use of local computations in the
design of (artificial) neural networks is usually advocated for three principal reasons:
1. Neural networks that perform local computations are often held up as metaphors
for biological neural networks.
2. The use of local computations permits a graceful degradation in performance
caused by hardware errors and therefore provides the basis for a fault-tolerant net-
work design.
3. Local computations favor the use of parallel architectures as an efficient method
for the implementation of neural networks.
Replicator (Identity) Mapping
The hidden neurons of a multilayer perceptron trained with the back-propagation
algorithm play a critical role as feature detectors. A novel way in which this impor-
tant property of the multilayer perceptron can be exploited is in its use as a replicator
or identity map (Rumelhart et al., 1986b; Cottrel et al., 1987). Figure 4.19 illustrates
Section 4.15
Virtues and Limitations of Back-Propagation Learning
181
TABLE 4.1
Summary of the Optimal-Brain-Surgeon Algorithm
1. Train the given multilayer perceptron to minimum mean-square error.
2. Use the procedure described in Section 4.8 to compute the vector
where F(w, x(n)) is the input–output mapping realized by the multilayer perceptron with an overall
weight vector w, and x(n) is the input vector.
3. Use the recursion in Eq. (4.108) to compute the inverse Hessian H1.
4. Find the i that corresponds to the smallest saliency
where 
is the (i, i)th element of H-1. If the saliency Si is much smaller than the mean-square error
eav, then delete the synaptic weight wi and proceed to step 5. Otherwise, go to step 6.
5. Update all the synaptic weights in the network by applying the adjustment
Go to step 2.
6. Stop the computation when no more weights can be deleted from the network without a large increase
in the mean-square error. (It may be desirable to retrain the network at this point).
¢w = - 
wi
[H-1]i, i
 H-11i
[H-1]i, i
Si =
wi
2
2[H-1]i, i
(n) =
1
2N
 
0F(w, x(n))
0w

how this can be accomplished for the case of a multilayer perceptron using a single
hidden layer. The network layout satisfies the following structural requirements, as
illustrated in Fig. 4.19a:
• The input and output layers have the same size, m.
• The size of the hidden layer, M, is smaller than m.
• The network is fully connected.
A given pattern x is simultaneously applied to the input layer as the stimulus and to the
output layer as the desired response. The actual response of the output layer,
, is
xˆ
182
Chapter 4
Multilayer Perceptrons
Estimate of
input signal,
xˆ
Encoded
signal
s
(a)
Input
signal
x
Multilayer perceptron


(b)
xˆ
x
s
e
x
Σ
Multilayer
perceptron
(c)
Decoded
signal,
xˆ
s
FIGURE 4.19
(a) Replicator network (identity map) with a single hidden layer used as an
encoder. (b) Block diagram for the supervised training of the replicator network. (c) Part of
the replicator network used as a decoder.

Section 4.15
Virtues and Limitations of Back-Propagation Learning
183
intended to be an “estimate” of x. The network is trained using the back-propagation
algorithm in the usual way, with the estimation error vector (
) treated as the error
signal, as illustrated in Fig. 4.19b.The training is performed in an unsupervised manner
(i.e., without the need for a teacher). By virtue of the special structure built into the
design of the multilayer perceptron, the network is constrained to perform identity map-
ping through its hidden layer.An encoded version of the input pattern, denoted by s, is
produced at the output of the hidden layer, as indicated in Fig. 4.19a. In effect, the fully
trained multilayer perceptron performs the role of an “encoder.”To reconstruct an esti-
mate of the original input pattern x (i.e., to perform decoding), we apply the encoded
signal to the hidden layer of the replicator network, as illustrated in Fig. 4.19c. In effect,
this latter network performs the role of a “decoder.”The smaller we make the size M of the
hidden layer compared with the size m of the input–output layer, the more effective the
configuration of Fig. 4.19a will be as a data-compression system.10
Function Approximation
A multilayer perceptron trained with the back-propagation algorithm manifests itself as
a nested sigmoidal structure, written for the case of a single output in the compact form
(4.109)
where (·) is a sigmoid activation function;wok is the synaptic weight from neuron k in the
last hidden layer to the single output neuron o, and so on for the other synaptic weights;
and xi is the ith element of the input vector x. The weight vector w denotes the entire set
of synaptic weights ordered by layer, then neurons in a layer, and then synapses in a
neuron. The scheme of nested nonlinear functions described in Eq. (4.109) is unusual in
classical approximation theory.It is a universal approximator,as discussed in Section 4.12.
Computational Efficiency
The computational complexity of an algorithm is usually measured in terms of the num-
ber of multiplications,additions,and storage requirement involved in its implementation.
A learning algorithm is said to be computationally efficient when its computational com-
plexity is polynomial in the number of adjustable parameters that are to be updated
from one iteration to the next.On this basis,it can be said that the back-propagation algo-
rithm is computationally efficient, as stated in the summarizing description at the begin-
ning of this section. Specifically, in using the algorithm to train a multilayer perceptron
containing a total of W synaptic weights (including biases), its computational complex-
ity is linear in W. This important property of the back-propagation algorithm can be
readily verified by examining the computations involved in performing the forward and
backward passes summarized in Section 4.4. In the forward pass, the only computations
involving the synaptic weights are those that pertain to the induced local fields of the var-
ious neurons in the network. Here, we see from Eq. (4.44) that these computations are
all linear in the synaptic weights of the network. In the backward pass, the only compu-
tations involving the synaptic weights are those that pertain to (1) the local gradients of
the hidden neurons,and (2) the updating of the synaptic weights themselves,as shown in
Eqs. (4.46) and (4.47), respectively. Here again, we also see that these computations are

F(x, w) =  aa
k
wok aa
j
wkj a p  aa
i
wlixi b b b b
xˆ
x - xˆ

all linear in the synaptic weights of the network.The conclusion is therefore that the com-
putational complexity of the back-propagation algorithm is linear inW;that is,it is O(W).
Sensitivity Analysis
Another computational benefit gained from the use of back-propagation learning is the
efficient manner in which we can carry out a sensitivity analysis of the input–output
mapping realized by the algorithm.The sensitivity of an input–output mapping function
F with respect to a parameter of the function, denoted by #, is defined by
(4.110)
Consider then a multilayer perceptron trained with the back-propagation algorithm.Let
the function F(w) be the input–output mapping realized by this network; w denotes the
vector of all synaptic weights (including biases) contained in the network.In Section 4.8,
we showed that the partial derivatives of the function F(w) with respect to all the ele-
ments of the weight vector w can be computed efficiently. In particular, we see that the
complexity involved in computing each of these partial derivatives is linear in W, the total
number of weights contained in the network.This linearity holds regardless of where the
synaptic weight in question appears in the chain of computations.
Robustness
In Chapter 3, we pointed out that the LMS algorithm is robust in the sense that distur-
bances with small energy can give rise only to small estimation errors. If the underlying
observation model is linear, the LMS algorithm is an 
-optimal filter (Hassibi et al.,
1993, 1996).What this means is that the LMS algorithm minimizes the maximum energy
gain from the disturbances to the estimation errors.
If, on the other hand, the underlying observation model is nonlinear, Hassibi and
Kailath (1995) have shown that the back-propagation algorithm is a locally 
-optimal
filter.The term “local” means that the initial value of the weight vector used in the back-
propagation algorithm is sufficiently close to the optimum value w* of the weight vec-
tor to ensure that the algorithm does not get trapped in a poor local minimum. In
conceptual terms, it is satisfying to see that the LMS and back-propagation algorithms
belong to the same class of 
-optimal filters.
Convergence
The back-propagation algorithm uses an “instantaneous estimate”for the gradient of the
error surface in weight space. The algorithm is therefore stochastic in nature; that is, it
has a tendency to zigzag its way about the true direction to a minimum on the error sur-
face. Indeed, back-propagation learning is an application of a statistical method known
as stochastic approximation that was originally proposed by Robbins and Monro (1951).
Consequently, it tends to converge slowly.We may identify two fundamental causes for
this property (Jacobs, 1988):
1. The error surface is fairly flat along a weight dimension, which means that the
derivative of the error surface with respect to that weight is small in magnitude. In such
Hq
Hq
Hq
SF
# = 0FF
0##
184
Chapter 4
Multilayer Perceptrons

a situation, the adjustment applied to the weight is small, and consequently many itera-
tions of the algorithm may be required to produce a significant reduction in the error
performance of the network. Alternatively, the error surface is highly curved along a
weight dimension, in which case the derivative of the error surface with respect to that
weight is large in magnitude.In this second situation,the adjustment applied to the weight
is large, which may cause the algorithm to overshoot the minimum of the error surface.
2. The direction of the negative gradient vector (i.e., the negative derivative of
the cost function with respect to the vector of weights) may point away from the mini-
mum of the error surface: hence, the adjustments applied to the weights may induce the
algorithm to move in the wrong direction.
To avoid the slow rate of convergence of the back-propagation algorithm used to
train a multilayer perceptron, we may opt for the optimally annealed on-line learning
algorithm described in Section 4.10.
Local Minima
Another peculiarity of the error surface that affects the performance of the back-
propagation algorithm is the presence of local minima (i.e., isolated valleys) in addition
to global minima; in general, it is difficult to determine the numbers of local and global
minima.Since back-propagation learning is basically a hill-climbing technique,it runs the
risk of being trapped in a local minimum where every small change in synaptic weights
increases the cost function. But somewhere else in the weight space, there exists another
set of synaptic weights for which the cost function is smaller than the local minimum in
which the network is stuck. It is clearly undesirable to have the learning process termi-
nate at a local minimum, especially if it is located far above a global minimum.
Scaling
In principle, neural networks such as multilayer perceptrons trained with the back-
propagation algorithm have the potential to be universal computing machines.However,
for that potential to be fully realized, we have to overcome the scaling problem, which
addresses the issue of how well the network behaves (e.g., as measured by the time
required for training or the best generalization performance attainable) as the compu-
tational task increases in size and complexity. Among the many possible ways of mea-
suring the size or complexity of a computational task, the predicate order defined by
Minsky and Papert (1969, 1988) provides the most useful and important measure.
To explain what we mean by a predicate, let %(X) denote a function that can have
only two values. Ordinarily, we think of the two values of %(X) as 0 and 1. But by tak-
ing the values to be FALSE or TRUE, we may think of %(X) as a predicate—that is, a
variable statement whose falsity or truth depends on the choice of argument X. For
example, we may write
Using the idea of a predicate,Tesauro and Janssens (1988) performed an empirical
study involving the use of a multilayer perceptron trained with the back-propagation
%CIRCLE(X) = e1
if the figure X is a circle
0
if the figure X is not a circle
Section 4.15
Virtues and Limitations of Back-Propagation Learning
185

algorithm to learn to compute the parity function.The parity function is a Boolean pred-
icate defined by
and whose order is equal to the number of inputs.The experiments performed by Tesauro
and Janssens appear to show that the time required for the network to learn to compute
the parity function scales exponentially with the number of inputs (i.e., the predicate
order of the computation), and that projections of the use of the back-propagation algo-
rithm to learn arbitrarily complicated functions may be overly optimistic.
It is generally agreed that it is inadvisable for a multilayer perceptron to be fully
connected. In this context, we may therefore raise the following question: Given that a
multilayer perceptron should not be fully connected, how should the synaptic connec-
tions of the network be allocated? This question is of no major concern in the case of
small-scale applications, but it is certainly crucial to the successful application of back-
propagation learning for solving large-scale, real-world problems.
One effective method of alleviating the scaling problem is to develop insight into
the problem at hand (possibly through neurobiological analogy) and use it to put inge-
nuity into the architectural design of the multilayer perceptron. Specifically, the net-
work architecture and the constraints imposed on synaptic weights of the network should
be designed so as to incorporate prior information about the task into the makeup of the
network.This design strategy is illustrated in Section 4.17 for the optical character recog-
nition problem.
4.16
SUPERVISED LEARNING VIEWED AS AN OPTIMIZATION PROBLEM
In this section, we take a viewpoint on supervised learning that is quite different from
that pursued in previous sections of the chapter. Specifically, we view the supervised
training of a multilayer perceptron as a problem in numerical optimization. In this con-
text, we first point out that the error surface of a multilayer perceptron with supervised
learning is a nonlinear function of a weight vector w; in the case of a multilayer per-
ceptron, w represents the synaptic weight of the network arranged in some orderly fash-
ion. Let 
denote the cost function, averaged over the training sample. Using the
Taylor series, we may expand 
about the current operating point on the error sur-
face as in Eq. (4.97), reproduced here in the form:
(4.111)
where g(n) is the local gradient vector, defined by
(4.112)
g(n) =
0eav(w)
0w
†
w=w(n)
 + (third- and higher-order terms)
eav(w(n) + ¢w(n)) = eav(w(n)) + gT(n)¢w(n) + 1
2¢wT(n)H(n)¢w(n)
eav(w)
eav(w)
%PARITY(X) = e1
if X is an odd number
0
otherwise
186
Chapter 4
Multilayer Perceptrons

The matrix H(n) is the local Hessian representing “curvature” of the error performance
surface, defined by
(4.113)
The use of an ensemble-averaged cost function 
presumes a batch mode of learning.
In the steepest-descent method, exemplified by the back-propagation algorithm,
the adjustment !w(n) applied to the synaptic weight vector w(n) is defined by
(4.114)
where  is a fixed learning-rate parameter. In effect, the steepest-descent method oper-
ates on the basis of a liner approximation of the cost function in the local neighborhood
of the operating point w(n). In so doing, it relies on the gradient vector g(n) as the only
source of local first-order information about the error surface.This restriction has a ben-
eficial effect:simplicity of implementation.Unfortunately,it also has a detrimental effect:
a slow rate of convergence, which can be excruciating, particularly in the case of large-
scale problems. The inclusion of the momentum term in the update equation for the
synaptic weight vector is a crude attempt at using second-order information about the
error surface, which is of some help. However, its use makes the training process more
delicate to manage by adding one more item to the list of parameters that have to be
“tuned” by the designer.
In order to produce a significant improvement in the convergence performance of
a multilayer perceptron (compared with back-propagation learning), we have to use
higher-order information in the training process.We may do so by invoking a quadratic
approximation of the error surface around the current point w(n). We then find from
Eq. (4.111) that the optimum value of the adjustment !w(n) applied to the synaptic
weight vector w(n) is given by
(4.115)
where H-1(n) is the inverse of the Hessian H(n), assuming that it exists. Equation (4.115)
is the essence of Newton’s method. If the cost function 
is quadratic (i.e., the
third- and higher-order terms in Eq. (4.109) are zero), Newton’s method converges to
the optimum solution in one iteration. However, the practical application of Newton’s
method to the supervised training of a multilayer perceptron is handicapped by three
factors:
(i) Newton’s method requires calculation of the inverse Hessian H-1(n), which can
be computationally expensive.
(ii) For H-1(n) to be computable, H(n) has to be nonsingular. In the case where H(n)
is positive definite, the error surface around the current point w(n) is describable
by a “convex bowl.” Unfortunately, there is no guarantee that the Hessian of the
error surface of a multilayer perceptron will always fit this description. Moreover,
there is the potential problem of the Hessian being rank deficient (i.e., not all the
eav(w)
¢w*(n) = H-1(n)g(n)
¢w(n) = -g(n)
eav(w)
H(n) =
02eav(w)
0w2
†
w=w(n)
Section 4.16
Supervised Learning Viewed as an Optimization Problem
187

columns of H are linearly independent), which results from the intrinsically 
ill-conditioned nature of supervised-learning problems (Saarinen et al., 1992); this
factor only makes the computational task more difficult.
(iii) When the cost function 
is nonquadratic, there is no guarantee for conver-
gence of Newton’s method, which makes it unsuitable for the training of a multi-
layer perceptron.
To overcome some of these difficulties, we may use a quasi-Newton method, which
requires only an estimate of the gradient vector g.This modification of Newton’s method
maintains a positive-definite estimate of the inverse matrix H-1 directly without matrix
inversion. By using such an estimate, a quasi-Newton method is assured of going down-
hill on the error surface. However, we still have a computational complexity that is
O(W2), where W is the size of weight vector w. Quasi-Newton methods are therefore
computationally impractical, except for in the training of very small-scale neural net-
works.A description of quasi-Newton methods is presented later in the section.
Another class of second-order optimization methods includes the conjugate-
gradient method, which may be regarded as being somewhat intermediate between the
method of steepest descent and Newton’s method.Use of the conjugate-gradient method
is motivated by the desire to accelerate the typically slow rate of convergence experi-
enced with the method of steepest descent, while avoiding the computational require-
ments associated with the evaluation, storage, and inversion of the Hessian in Newton’s
method.
Conjugate-Gradient Method11
The conjugate-gradient method belongs to a class of second-order optimization meth-
ods known collectively as conjugate-direction methods. We begin the discussion of these
methods by considering the minimization of the quadratic function
(4.116)
where x is a W-by-1 parameter vector;A is a W-by-W symmetric,positive-definite matrix;
b is a W-by-1 vector; and c is a scalar. Minimization of the quadratic function f(x) is
achieved by assigning to x the unique value
(4.117)
Thus, minimizing f(x) and solving the linear system of equations Ax*  b are equivalent
problems.
Given the matrix A, we say that a set of nonzero vectors s(0), s(1),
s(W - 1) is
A-conjugate (i.e., noninterfering with each other in the context of matrix A) if the
following condition is satisfied:
(4.118)
If A is equal to the identity matrix, conjugacy is equivalent to the usual notion of
orthogonality.
sT(n)As(j) = 0  for all n and j such that n Z j
...,
x* = A-1b
f(x) = 1
2 xTAx - bTx + c
eav(w)
188
Chapter 4
Multilayer Perceptrons

Section 4.16
Supervised Learning Viewed as an Optimization Problem
189
EXAMPLE 1
Interpretation of A-conjugate vectors
For an interpretation of A-conjugate vectors, consider the situation described in Fig. 4.20a, per-
taining to a two-dimensional problem.The elliptic locus shown in this figure corresponds to a plot
of Eq. (4.116) for
at some constant value assigned to the quadratic function f(x). Figure 4.20a also includes a pair
of direction vectors that are conjugate with respect to the matrix A. Suppose that we define a
new parameter vector v related to x by the transformation
where A1/2 is the square root of A. Then the elliptic locus of Fig. 4.20a is transformed into a cir-
cular locus, as shown in Fig. 4.20b. Correspondingly, the pair of A-conjugate direction vectors
in Fig. 4.20a is transformed into a pair of orthogonal direction vectors in Fig. 4.20b.
■
An important property of A-conjugate vectors is that they are linearly independent.
We prove this property by contradiction. Let one of these vectors—say, s(0)—be
expressed as a linear combination of the remaining W - 1 vectors as follows:
Multiplying by A and then taking the inner product of As(0) with s(0) yields
However, it is impossible for the quadratic form sT(0)As(0) to be zero, for two reasons:
The matrix A is positive definite by assumption, and the vector s(0) is nonzero by def-
inition. It follows therefore that the A-conjugate vectors s(0), s(1),
s(W - 1) cannot
be linearly dependent; that is, they must be linearly independent.
...,
sT(0)As(0) = a
W-1
j=1
jsT(0)As(j) = 0
s(0) = a
W-1
j=1
js(j)
v = A12x
x = [x0, x1]T
x1
0
(a)
x0
v1
0
(b)
v0
FIGURE 4.20
Interpretation
of A-conjugate vectors.
(a) Elliptic locus in two-
dimensional weight space.
(b) Transformation of the
elliptic locus into a circular
locus.

For a given set of A-conjugate vectors s(0), s(1),
s(W - 1), the corresponding
conjugate-direction method for unconstrained minimization of the quadratic error func-
tion f(x) is defined by
(4.119)
where x(0) is an arbitrary starting vector and (n) is a scalar defined by
(4.120)
(Fletcher, 1987; Bertsekas, 1995). The procedure of choosing  so as to minimize the
function f(x(n)  s(n)) for some fixed n is referred to as a line search, which represents
a one-dimensional minimization problem.
In light of Eqs. (4.118), (4.119) and (4.120), we now offer some observations:
1. Since the A-conjugate vectors s(0), s(1),
s(W - 1) are linearly independent,
they form a basis that spans the vector space of w.
2. The update equation (4.119) and the line minimization of Eq. (4.120) lead to the
same formula for the learning-rate parameter, namely,
(4.121)
where e(n) is the error vector defined by
(4.122)
3. Starting from an arbitrary point x(0),the conjugate-direction method is guaranteed
to find the optimum solution x* of the quadratic equation f(x)  0 in at most W
iterations.
The principal property of the conjugate-direction method is described in the
following statement (Fletcher, 1987; Bertsekas, 1995):
At successive iterations, the conjugate-direction method minimizes the quadratic function f(x)
over a progressively expanding linear vector space that eventually includes the global mini-
mum of f(x).
In particular, for each iteration n, the iterate x(n  1) minimizes the function f(x) over
a linear vector space 
that passes through some arbitrary point x(0) and is spanned
by the A-conjugate vectors s(0), s(1),
s(n), as shown by
(4.123)
where the space 
is defined by
(4.124)
dn = ex(n)  x(n) = x(0) + a
n
j=0
(j)s(j) f
dn
x(n + 1) = arg min
xdn
 f(x)
...,
dn
e(n) = x(n) - x*
(n) = -
sT(n)Ae(n)
sT(n)As(n),  n = 0, 1, ..., W - 1
...,
f(x(n) + (n)s(n)) = min
  f(x(n) + s(n))
x(n + 1) = x(n) + (n)s(n),  n = 0, 1, ..., W - 1
...,
190
Chapter 4
Multilayer Perceptrons

Section 4.16
Supervised Learning Viewed as an Optimization Problem
191
For the conjugate-direction method to work, we require the availability of a set
of A-conjugate vectors s(0), s(1),
s(W - 1). In a special form of this method known
as the scaled conjugate-gradient method,12 the successive direction vectors are generated
as A-conjugate versions of the successive gradient vectors of the quadratic function f(x)
as the method progresses—hence the name of the method. Thus, except for n  0, the
set of direction vectors {s(n)} is not specified beforehand, but rather it is determined in
a sequential manner at successive steps of the method.
First, we define the residual as the steepest-descent direction:
(4.125)
Then, to proceed, we use a linear combination of r(n) and s(n - 1), as shown by
(4.126)
where (n) is a scaling factor to be determined. Multiplying this equation by A, taking
the inner product of the resulting expression with s(n - 1),invoking the A-conjugate prop-
erty of the direction vectors, and then solving the resulting expression for (n), we get
(4.127)
Using Eqs. (4.126) and (4.127), we find that the vectors s(0), s(1),
s(W - 1) so gen-
erated are indeed A-conjugate.
Generation of the direction vectors in accordance with the recursive equation
(4.126) depends on the coefficient (n).The formula of Eq. (4.127) for evaluating (n),
as it presently stands, requires knowledge of matrix A. For computational reasons, it
would be desirable to evaluate (n) without explicit knowledge of A.This evaluation can
be achieved by using one of two formulas (Fletcher, 1987):
1. the Polak–Ribi`ère formula, for which (n) is defined by
(4.128)
2. the Fletcher–Reeves formula, for which (n) is defined by
(4.129)
To use the conjugate-gradient method to attack the unconstrained minimization
of the cost function 
pertaining to the unsupervised training of multilayer per-
ceptron, we do two things:
• Approximate the cost function 
by a quadratic function.That is, the third-
and higher-order terms in Eq. (4.111) are ignored, which means that we are oper-
ating close to a local minimum on the error surface. On this basis, comparing
Eqs. (4.111) and (4.116), we can make the associations indicated in Table 4.2.
• Formulate the computation of coefficients (n) and (n) in the conjugate-gradi-
ent algorithm so as to require only gradient information.
eav(w)
eav(w)
(n) =
rT(n)r(n)
rT(n - 1)r(n - 1)
(n) =
rT(n)(r(n) - r(n - 1))
rT(n - 1)r(n - 1)
...,
(n) = -
sT(n - 1)Ar(n)
sT(n - 1)As(n - 1)
s(n) = r(n) + (n)s(n - 1),  n = 1, 2, ..., W - 1
r(n) = b - Ax(n)
...,

The latter point is particularly important in the context of multilayer perceptrons because
it avoids using the Hessian H(n), the evaluation of which is plagued with computational
difficulties.
To compute the coefficient (n) that determines the search direction s(n) without
explicit knowledge of the Hessian H(n), we can use the Polak–Ribière formula of
Eq. (4.128) or the Fletcher–Reeves formula of Eq.(4.129).Both of these formulas involve
the use of residuals only. In the linear form of the conjugate-gradient method, assum-
ing a quadratic function, the Polak–Ribière and Fletcher–Reeves formulas are equiva-
lent. On the other hand, in the case of a nonquadratic cost function, they are not.
For nonquadratic optimization problems, the Polak–Ribière form of the conju-
gate-gradient algorithm is typically superior to the Fletcher–Reeves form of the algo-
rithm, for which we offer the following heuristic explanation (Bertsekas, 1995): Due to
the presence of third- and higher-order terms in the cost function 
and possible
inaccuracies in the line search, conjugacy of the generated search directions is progres-
sively lost.This condition may in turn cause the algorithm to “jam” in the sense that the
generated direction vector s(n) is nearly orthogonal to the residual r(n).When this phe-
nomenon occurs,we have r(n)  r(n - 1),in which case the scalar (n) will be nearly zero.
Correspondingly, the direction vector s(n) will be close to r(n), thereby breaking the
jam.In contrast,when the Fletcher–Reeves formula is used,the conjugate-gradient algo-
rithm typically continues to jam under similar conditions.
In rare cases, however, the Polak–Ribière method can cycle indefinitely without
converging. Fortunately, convergence of the Polak–Ribière method can be guaranteed
by choosing
(4.130)
where PR is the value defined by the Polak–Ribière formula of Eq. (4.128) (Shewchuk,
1994). Using the value of  defined in Eq. (4.130) is equivalent to restarting the conju-
gate gradient algorithm if PR  0. To restart the algorithm is equivalent to forgetting
the last search direction and starting it anew in the direction of steepest descent.
Consider next the issue of computing the parameter (n), which determines the
learning rate of the conjugate-gradient algorithm. As with (n), the preferred method
for computing (n) is one that avoids having to use the Hessian H(n). We recall that
the line minimization based on Eq. (4.120) leads to the same formula for (n) as that
derived from the update equation Eq. (4.119). We therefore need a line search,12 the
purpose of which is to minimize the function 
with respect to . That is,
eav(w + s)
 = max{PR, 0}
eav(w)
192
Chapter 4
Multilayer Perceptrons
TABLE 4.2
Correspondence Between f(x) and eav(w)
Quadratic function f(x)
Cost function eav(w)
Parameter vector x(n)
Synaptic weight vector w(n)
Gradient vector 0f(x)0x
Gradient vector g = 0eav0w
Matrix A
Hessian matrix H
given fixed values of the vectors w and s, the problem is to vary  such that this func-
tion is minimized.As  varies, the argument w  s traces a line in the W-dimensional

Section 4.16
Supervised Learning Viewed as an Optimization Problem
193
vector space of w—hence the name “line search.”A line-search algorithm is an iterative
procedure that generates a sequence of estimates {(n)} for each iteration of the conjugate-
gradient algorithm. The line search is terminated when a satisfactory solution is found.
The computation of a line search must be performed along each search direction.
Several line-search algorithms have been proposed in the literature, and a good
choice is important because it has a profound impact on the performance of the conjugate-
gradient algorithm in which it is embedded. There are two phases to any line-search
algorithm (Fletcher, 1987):
• the bracketing phase, which searches for a bracket (that is, a nontrivial interval that
is known to contain a minimum), and
• the sectioning phase, in which the bracket is sectioned (i.e., divided), thereby gen-
erating a sequence of brackets whose length is progressively reduced.
We now describe a curve-fitting procedure that takes care of these two phases in a
straightforward manner.
Let 
denote the cost function of the multilayer perceptron, expressed as a
eav()
av(h)
av(h1)
av(h3)
h3
h
av(h2)
h2
h1
0
FIGURE 4.21
Illustration of the
line search.
function of . It is assumed that 
is strictly unimodal (i.e., it has a single minimum
in the neighborhood of the current point w(n)) and is twice continuously differentiable.
We initiate the search procedure by searching along the line until we find three points
1, 2, and 3 such that the following condition is satisfied, as illustrated in Fig. 4.21:
(4.131)
Since 
is a continuous function of , the choice described in Eq. (4.131) ensures
eav()
eav(1)  eav(3)  eav(2)  for 1 6 2 6 3
eav()
that the bracket [1, 3] contains a minimum of the function 
. Provided that the
function 
is sufficiently smooth, we may consider this function to be parabolic in
the immediate neighborhood of the minimum.Accordingly,we may use inverse parabolic
interpolation to do the sectioning (Press et al., 1988). Specifically, a parabolic function
is fitted through the three original points 1, 2, and 3, as illustrated in Fig. 4.22, where
the solid line corresponds to 
and the dashed line corresponds to the first iteration
eav()
eav()
eav()

of the sectioning procedure. Let the minimum of the parabola passing through the three
points 1, 2, and 3 be denoted by 4. In the example illustrated in Fig. 4.22, we have
and 
. Point 3 is replaced in favor of 4, making [1,
eav(4) 6 eav(1)
eav(4) 6 eav(2)
194
Chapter 4
Multilayer Perceptrons
av(h)
Parabolic
approximation
to av(h)
Mean-
squared
error
h4
h3
h
h2
h1
FIGURE 4.22
Inverse parabolic
interpolation.
4] the new bracket.The process is repeated by constructing a new parabola through the
points 1, 2, and 4.The bracketing-followed-by-sectioning procedure, as illustrated in
Fig. 4.22, is repeated several times until a point close enough to the minimum of 
is located, at which time the line search is terminated.
Brent’s method constitutes a highly refined version of the three-point curve-
fitting procedure just described (Press et al., 1988).At any particular stage of the com-
putation, Brent’s method keeps track of six points on the function 
, which may
not all be necessarily distinct.As before, parabolic interpolation is attempted through
three of these points. For the interpolation to be acceptable, certain criteria involving
the remaining three points must be satisfied. The net result is a robust line-search
algorithm.
Summary of the Nonlinear Conjugate-Gradient Algorithm
All the ingredients we need to formally describe the nonlinear (nonquadratic) form of
the conjugate-gradient algorithm for the supervised training of a multilayer perceptron
are now in place.A summary of the algorithm is presented in Table 4.3.
Quasi-Newton Methods
Resuming the discussion on quasi-Newton methods, we find that these are basically gra-
dient methods described by the update equation
(4.132)
where the direction vector s(n) is defined in terms of the gradient vector g(n) by
(4.133)
s(n) = -S(n)g(n) 
w(n + 1) = w(n) + (n)s(n) 
eav()
eav()

Section 4.16
Supervised Learning Viewed as an Optimization Problem
195
The matrix S(n) is a positive-definite matrix that is adjusted from one iteration to the
next. This is done in order to make the direction vector s(n) approximate the Newton
direction, namely,
Quasi-Newton methods use second-order (curvature) information about the error
surface without actually requiring knowledge of the Hessian. They do so by using two
successive iterates w(n) and w(n  1), together with the respective gradient vectors g(n)
and g(n  1). Let
(4.134)
and
(4.135)
¢w(n) = w(n + 1) - w(n) 
q(n) = g(n + 1) - g(n) 
-(02eav0w2)-1 (0eav0w)
TABLE 4.3
Summary of the Nonlinear Conjugate-Gradient Algorithm for the Supervised Training of a
Multilayer Perceptron
Initialization
Unless prior knowledge on the weight vector w is available, choose the initial value w(0) by using a proce-
dure similar to that described for the back-propagation algorithm.
Computation
1. For w(0), use back propagation to compute the gradient vector g(0).
2. Set s(0)  r(0)  -g(0).
3. At time-step n, use a line search to find (n) that minimizes 
sufficiently, representing the cost
function 
expressed as a function of  for fixed values of w and s.
eav
eav()
4. Test to determine whether the Euclidean norm of the residual r(n) has fallen below a specified value,
that is, a small fraction of the initial value 
.
7 r(0)7
5. Update the weight vector:
w(n + 1) = w(n) + (n)s(n)
6. For w(n  1), use back propagation to compute the updated gradient vector g(n  1).
7. Set r(n  1)  -g(n  1).
8. Use the Polak–Ribière method to calculate:
(n + 1) = max e
rT(n + 1)(r(n + 1) - r(n))
rT(n)r(n)
, 0 f
9. Update the direction vector:
s(n + 1) = r(n + 1) + (n + 1)s(n)
10. Set n  n  1, and go back to step 3.
Stopping criterion. Terminate the algorithm when the condition
7 r(n)7  7r(0)7
is satisfied, where  is a prescribed small number.

We may then derive curvature information by using the approximate formula
(4.136)
In particular, given W linearly independent weight increments !w(0), !w(1),...,
q(n) -' a 0
0w g(n) b ¢w(n)
196
Chapter 4
Multilayer Perceptrons
!w(W - 1) and the respective gradient increments q(0), q(1),
q(W - 1), we may
approximate the Hessian as
(4.137)
We may also approximate the inverse Hessian as follows13:
(4.138)
When the cost function eav(w) is quadratic, Eqs. (4.137) and (4.138) are exact.
In the most popular class of quasi-Newton methods, the updated matrix S(n  1)
is obtained from its previous value S(n), the vectors !w(n) and q(n), by using the follow-
ing recursion (Fletcher, 1987; Bertsekas, 1995):
(4.139)
where
(4.140)
and
(4.141)
The algorithm is initiated with some arbitrary positive- definite matrix S(0).The partic-
ular form of the quasi-Newton method is parameterized by how the scalar 
is
defined, as indicated by the following two points (Fletcher, 1987):
1. For (n)  0 for all n, we obtain the Davidon–Fletcher–Powell (DFP) algorithm,
which is historically the first quasi-Newton method.
2. For (n)  1 for all n, we obtain the Broyden–Fletcher–Goldfarb–Shanno (BFGS)
algorithm, which is considered to be the best form of quasi-Newton methods cur-
rently known.
Comparison of Quasi-Newton Methods with 
Conjugate-Gradient Methods
We conclude this brief discussion of quasi-Newton methods by comparing them with
conjugate-gradient methods in the context of nonquadratic optimization problems
(Bertsekas, 1995):
• Both quasi-Newton and conjugate-gradient methods avoid the need to use the
Hessian. However, quasi-Newton methods go one step further by generating an
(n)
0  (n)  1  for all n
v(n) =
¢w(n)
¢wT(n)¢w(n) -
S(n)q(n)
qT(n)S(n)q(n)
+ (n)[qT(n)S(n)q(n)] [v(n)vT(n)]
S(n + 1) = S(n) +
¢w(n)¢wT(n)
qT(n)q(n)
-
S(n)q(n)qT(n)S(n)
qT(n)S(n)q(n)
H-1 M [¢w(0), ¢w(1), ..., ¢w(W - 1)] [q(0), q(1), ..., q(W - 1)]-1
H M [q(0), q(1), ... , q(W - 1)] [¢w(0), ¢w(1), ..., ¢w(W - 1)]-1
...,

Section 4.16
Supervised Learning Viewed as an Optimization Problem
197
approximation to the inverse Hessian. Accordingly, when the line search is accu-
rate and we are in close proximity to a local minimum with a positive-definite Hes-
sian, a quasi-Newton method tends to approximate Newton’s method, thereby
attaining faster convergence than would be possible with the conjugate-gradient
method.
• Quasi-Newton methods are not as sensitive to accuracy in the line-search stage of
the optimization as the conjugate-gradient method.
• Quasi-Newton methods require storage of the matrix S(n), in addition to the
matrix-vector multiplication overhead associated with the computation of the
direction vector s(n).The net result is that the computational complexity of quasi-
Newton methods is O(W2). In contrast, the computational complexity of the
conjugate-gradient method is O(W). Thus, when the dimension W (i.e., size of
the weight vector w) is large, conjugate-gradient methods are preferable to quasi-
Newton methods in computational terms.
It is because of the lattermost point that the use of quasi-Newton methods is restricted,
in practice, to the design of small-scale neural networks.
Levenberg–Marquardt Method
The Levenberg–Marquardt method, due to Levenberg (1994) and Marquardt (1963), is
a compromise between the following two methods:
• Newton’s method, which converges rapidly near a local or global minimum, but
may also diverge;
• Gradient descent, which is assured of convergence through a proper selection of
the step-size parameter, but converges slowly.
To be specific, consider the optimization of a second-order function F(w), and let g be
its gradient vector and H be its Hessian.According to the Levenberg–Marquardt method,
the optimum adjustment 
applied to the parameter vector w is defined by
(4.142)
where I is the identity matrix of the same dimensions as H and  is a regularizing, or
loading, parameter that forces the sum matrix (H  I) to be positive definite and safely
well conditioned throughout the computation. Note also that the adjustment 
of
Eq. (4.142) is a minor modification of the formula defined in Eq. (4.115).
With this background, consider a multilayer perceptron with a single output neu-
ron.The network is trained by minimizing the cost function
(4.143)
where {x(i), d(i)}
is the training sample and F(x(i); w) is the approximating function
realized by the network; the synaptic weights of the network are arranged in some
orderly manner to form the weight vector w. The gradient and the Hessian of the cost
function eav(w) are respectively defined by
N
i=1
eav(w) =
1
2N a
N
i=1
[d(i) - F(x(i); w)]2 
¢w
¢w = [H + 
I]-1g 
¢w

(4.144)
and
(4.145)
Thus, substituting Eqs. (4.144) and (4.145) into Eq. (4.142), the desired adjustment 
is computed for each iteration of the Levenberg-Marquardt algorithm.
However, from a practical perspective, the computational complexity of 
Eq. (4.145) can be demanding, particularly when the dimensionality of the weight
vector w is high; the computational difficulty is attributed to the complex nature of
the Hessian H(w).To mitigate this difficulty, the recommended procedure is to ignore
the second term on the right-hand side of Eq. (4.145), thereby approximating the
Hessian simply as
(4.146)
This approximation is recognized as the outer product of the partial derivative
with itself, averaged over the training sample; accordingly, it is referred
to as the outer-product approximation of the Hessian.The use of this approximation is
justified when the Levenberg-Marquardt algorithm is operating in the neighborhood
of a local or global minimum.
Clearly,the approximate version of the Levenberg–Marquardt algorithm,based on
the gradient vector of Eq. (4.144) and the Hessian of Eq. (4.146), is a first-order method
of optimization that is well suited for nonlinear least-squares estimation problems.More-
over, because of the fact that both of these equations involve averaging over the train-
ing sample, the algorithm is of a batch form.
The regularizing parameter 
plays a critical role in the way the Levenberg-
Marquardt algorithm functions. If we set equal to zero, then the formula of Eq. (4.142)
reduces to Newton’s method. On the other hand, if we assign a large value to 
such
that 
overpowers the Hessian H, the Levenberg-Marquardt algorithm functions effec-
tively as a gradient descent method. From these two observations, it follows that at each
iteration of the algorithm, the value assigned to 
should be just large enough to main-
tain the sum matrix 
in its positive-definite form. In specific terms, the
recommended Marquardt recipe for the selection of 
is as follows (Press et al.,) 1988:
1. Compute 
at iteration 
2. Choose a modest value for 
say 
 = 10-3.

,
n - 1.
eav(w)

(H + 
I)


I



0F(w, x(i))0w
H(w) L 1
N a
N
i=1
c
0F(x(i); w)
0w
d c
0F(x(i); w)
0w
d
T
 
¢w
- 1
N a
N
i=1
[d(i) - F(x(i); w)] 
02F(x(i); w)
0w2
 = 1
N a
N
i=1
c
0F(x(i); w)
0w
d c
0F(x(i); w)
0w
d
T
 H(w) =
02eav(w)
0w2
 
 = - 1
N a
N
i=1
[d(i) - F(x(i); w)] 
0F(x(i); w)
0w
 g(w) =
0eav(w)
0w
 
198
Chapter 4
Multilayer Perceptrons

Section 4.16
Supervised Learning Viewed as an Optimization Problem
199
The second-order information contained in the Hessian (curvature) of the cost function is used
to improve the performance of supervised-learning algorithms.
A simple way of expanding on the performance of the optimally annealed on-line
learning algorithm considered in Section 4.10 is to replace the learning-rate parameter
(n) in Eq. (4.60) with the scaled inverse of the Hessian H, as shown by
(4.147)
The replacement of (n) with the new term 
is intended to accelerate the 
speed of convergence of the on-line algorithm in an optimally annealed fashion.It is assumed
that the Hessian H is known a priori and its inverse H-1 can therefore be precomputed.
Recognizing the fact that “there is no such thing as a free lunch,” the price paid for
the accelerated convergence is summarized as follows (Bottou, 2007):
(i) Whereas in the stochastic gradient descent of Eq. (4.60), the computation cost per
iteration of the algorithm is O(W), where W is the dimension of the weight vector
w being estimated,the corresponding computation cost per iteration of the second-
order stochastic gradient-descent algorithm in Eq. (4.147) is O(W2).
(ii) For each training example (x, d) processed by the algorithm of Eq. (4.147), the
algorithm requires multiplication of the W-by-l gradient vector g and the W-by-W
inverse matrix H-1 and storage of the product.
1
n
 H-1
wˆ (n + 1) =  wˆ (n) - 1
n H-1g(x(n + 1), d(n + 1); wˆ (n))
Updated
estimate
Old
estimate
Annealed
inverse 
of the
Hessian H
Gradient vector
g
¯˚˚˚˚˚˚˘˚˚˚˚˚˚˙
¯˚˘˚˙
¯˘˙
¯˘˙
3. Solve Eq. (4.142) for the adjustment 
at iteration and evaluate 
4. If 
increase 
by a factor of 10 (or any other substantial
factor) and go back to step 3.
5. If, on the other hand,
decrease by a factor of 10, update
the trial solution 
and go back to step 3.
For obvious reasons, a rule for stopping the iterative process is necessary. In Press et al.
(1998), it is pointed out that an adjustment in the parameter vector w that changes
by an incrementally small amount is never statistically meaningful.We may there-
fore use this insightful comment as a basis for the stopping rule.
One last comment is in order: To evaluate the partial derivative 
at
each iteration of the algorithm, we may use back-propagation in the manner described
in Section 4.8.
Second-Order Stochastic Gradient Descent for On-line Learning
Up to this point,this section has focused on second-order optimization techniques for batch
learning. Hereafter, we turn our attention to second-order stochastic gradient-descent
methods for on-line learning.Although these two families of techniques are entirely dif-
ferent, they do share a common purpose:
0F(x; w)0w
eav( w)
w S w + ¢w,

eav(w + ¢w) 6 eav(w),

eav(w + ¢w)  eav(w),
eav(w + ¢w).
n
¢w

(iii) In a general context, whenever some form of sparsity exists in the training sample,
the natural move is to exploit the sparsity for the purpose of improved algorith-
mic performance. Unfortunately, the Hessian H is typically a full matrix and there-
fore not sparse,which rules out the possibility of exploiting training-sample sparsity.
To overcome these limitations,we may resort to one of the following approximation
procedures:
(i) Diagonal approximation: (Becker and LeCun, 1989). In this procedure, only the
diagonal elements of the Hessian are retained,which means that the inverse matrix
H-1 will likewise be a diagonal matrix. Matrix theory teaches us that the matrix
product H-1g will consist of a sum of terms of the form 
where hii is the ith
h-1
ii gi,
200
Chapter 4
Multilayer Perceptrons
diagonal element of the Hessian H and gi is the corresponding element of the gra-
dient g for i  1, 2,
W.With the gradient vector g being linear in the weights, it
follows that the computational complexity of the approximated second-order on-
line learning algorithm is O(W).
(ii) Low-rank approximation: (LeCun et al., 1998). By definition, the rank of a matrix
equals the number of algebraically independent columns of the matrix. Given a
Hessian H, the use of singular value decomposition (SVD) provides an important
procedure for the low-rank approximation of the Hessian H. Let the rank of H
be denoted by p and a rank r approximation of H be denoted by Hr, where r  p.
The squared error between the Hessian and its approximation is defined by the
Frobenius norm
(4.148)
where tr[•] denotes the trace (i.e., sum of the diagonal components) of the square
matrix enclosed inside the square brackets. Applying the SVD to the matrices H
and Hr, we write
(4.149)
and
(4.150)
where the orthogonal matrices U and V define the common right and left singu-
lar vectors, respectively, and the rectangular matrix
(4.151)
defines the singular values of the low-rank approximation Hr.The new square matrix
(4.152)
provides the least-squares, rank r approximation to the Hessian H (Scharf, 1991).
Correspondingly, the use of the new matrix Hr in place of the Hessian H in the
on-line learning algorithm of Eq. (4.147) reduces the computational complexity of
the algorithm to somewhere between O(W) and O(W2).
(iii) BFGS approximation: (Schraudolph et al., 2007).As pointed out previously in this
section, the BFGS algorithm is considered to be the best form of a quasi-Newton
Hr = UgrVT
gr = diag[
1, 
2, ..., 
r, 0 ..., 0]
Hr = VgrUT
H = VgUT
e2 = tr[(H - Hr)T(H - Hr)]
...,

Section 4.17
Convolutional Networks
201
method. In the 2007 paper by Schraudolph et al., the BFGS algorithm is modified
in both its full and limited versions of memory such that it becomes usable for the
stochastic aproximation of gradients. The modified algorithm appears to provide
a fast,scalable,stochastic quasi-Newton procedure for on-line convex optimization.
In Yu et al. (2008), the BFGS quasi-Newton method and its limited-memory vari-
ant are extended to deal with non-smooth convex objective functions.
4.17
CONVOLUTIONAL NETWORKS
Up to this point, we have been concerned with the algorithmic design of multilayer per-
ceptrons and related issues. In this section, we focus on the structural layout of the mul-
tilayer perceptron itself. In particular, we describe a special class of multilayer
perceptrons known collectively as convolutional networks, which are well suited for
pattern classification.The idea behind the development of these networks is neurobio-
logically motivated, going back to the pioneering work of Hubel and Wiesel (1962, 1977)
on locally sensitive and orientation-selective neurons of the visual cortex of a cat.
A convolutional network is a multilayer perceptron designed specifically to rec-
ognize two-dimensional shapes with a high degree of invariance to translation, scaling,
skewing,and other forms of distortion.This difficult task is learned in a supervised manner
by means of a network whose structure includes the following forms of constraints
(LeCun and Bengio, 2003):
1. Feature extraction. Each neuron takes its synaptic inputs from a local receptive
field in the previous layer, thereby forcing it to extract local features. Once a feature
has been extracted, its exact location becomes less important, so long as its position rel-
ative to other features is approximately preserved.
2. Feature mapping. Each computational layer of the network is composed of mul-
tiple feature maps, with each feature map being in the form of a plane within which the
individual neurons are constrained to share the same set of synaptic weights. This sec-
ond form of structural constraint has the following beneficial effects:
• shift invariance, forced into the operation of a feature map through the use of
convolution with a kernel of small size, followed by a sigmoid function;
• reduction in the number of free parameters, accomplished through the use of weight
sharing.
3. Subsampling. Each convolutional layer is followed by a computational layer
that performs local averaging and subsampling, whereby the resolution of the feature
map is reduced. This operation has the effect of reducing the sensitivity of the feature
map’s output to shifts and other forms of distortion.
We emphasize that all weights in all layers of a convolutional network are learned
through training.Moreover,the network learns to extract its own features automatically.
Figure 4.23 shows the architectural layout of a convolutional network made up of
an input layer, four hidden layers, and an output layer.This network is designed to per-
form image processing (e.g., recognition of handwritten characters). The input layer,
made up of 28  28 sensory nodes, receives the images of different characters that have

been approximately centered and normalized in size.Thereafter, the computational lay-
outs alternate between convolution and subsampling:
1. The first hidden layer performs convolution. It consists of four feature maps, with
each feature map consisting of 24  24 neurons. Each neuron is assigned a recep-
tive field of size 5  5.
2. The second hidden layer performs subsampling and local averaging.It also consists
of four feature maps, but each feature map is now made up of 12  12 neurons.
Each neuron has a receptive field of size 2  2, a trainable coefficient, a trainable
bias, and a sigmoid activation function.The trainable coefficient and bias control
the operating point of the neuron; for example, if the coefficient is small, the neu-
ron operates in a quasilinear mode.
3. The third hidden layer performs a second convolution. It consists of 12 feature
maps, with each feature map consisting of 8  8 neurons. Each neuron in this
hidden layer may have synaptic connections from several feature maps in the
previous hidden layer. Otherwise, it operates in a manner similar to the first
convolutional layer.
4. The fourth hidden layer performs a second subsampling and local averaging. It
consists of 12 feature maps, but with each feature map consisting of 4  4 neu-
rons. Otherwise, it operates in a manner similar to the first subsampling layer.
5. The output layer performs one final stage of convolution. It consists of 26 neu-
rons, with each neuron assigned to one of 26 possible characters. As before, each
neuron is assigned a receptive field of size 4  4.
With the successive computational layers alternating between convolution and sub-
sampling, we get a “bipyramidal” effect. That is, at each convolutional or subsampling
layer, the number of feature maps is increased while the spatial resolution is reduced,
compared with the corresponding previous layer. The idea of convolution followed by
subsampling is inspired by the notion of “simple” cells followed by “complex” cells14
that was first described in Hubel and Wiesel (1962).
The multilayer perceptron described in Fig. 4.23 contains approximately 100,000
synaptic connections, but only about 2,600 free parameters.This dramatic reduction in
202
Chapter 4
Multilayer Perceptrons
INPUT
28  28
Convolution
Convolution
Convolution
Subsampling
Subsampling
OUTPUT
26@1  1
Feature maps
4@24  24
Feature maps
4@12  12
Feature maps
12@8  8
Feature maps
12@4  4
FIGURE 4.23
Convolutional network for image processing such as handwriting recognition.
(Reproduced with permission of MIT Press.)

Section 4.18
Nonlinear Filtering
203
the number of free parameters is achieved through the use of weight sharing.The capac-
ity of the learning machine is thereby reduced, which in turn improves the machine’s
generalization ability.What is even more remarkable is the fact that the adjustments to
the free parameters of the network are made by using the stochastic mode of back-
propagation learning.
Another noteworthy point is that the use of weight sharing makes it possible to
implement the convolutional network in parallel form.This is another advantage of the
convolutional network over a fully connected multilayer perceptron.
The lesson to be learned from the convolutional network of Fig.4.23 is twofold.First,
a multilayer perceptron of manageable size is able to learn a complex, high-dimensional,
nonlinear mapping by constraining its design through the incorporation of prior knowl-
edge about the task at hand. Second, the synaptic weights and bias levels can be learned
by cycling the simple back-propagation algorithm through the training sample.
4.18
NONLINEAR FILTERING
The prototypical use of a static neural network, exemplified by the multilayer per-
ceptron, is in structural pattern recognition; insofar as applications are concerned,
much of the material presented in this chapter has focused on structural pattern
recognition. In contrast, in temporal pattern recognition, or nonlinear filtering, the
requirement is to process patterns that evolve over time, with the response at a
particular instant of time depending not only on the present value of the input sig-
nal, but also on past values. Simply put, time is an ordered quantity that constitutes
an important ingredient of the learning process in temporal-pattern-recognition
tasks.
For a neural network to be dynamic,it must be given short-term memory in one form
or another. A simple way of accomplishing this modification is through the use of time
delays, which can be implemented at the synaptic level inside the network or externally
at the input layer of the network.Indeed,the use of time delays in neural networks is neu-
robiologically motivated, since it is well known that signal delays are omnipresent in the
brain and play an important role in neurobiological information processing (Braitenberg,
1967,1977,1986;Miller,1987).Time may therefore be built into the operation of a neural
network in two basic ways:
• Implicit representation. Time is represented by the effect it has on signal pro-
cessing in an implicit manner. For example, in a digital implementation of the
neural network, the input signal is uniformly sampled, and the sequence of
synaptic weights of each neuron connected to the input layer of the network
is convolved with a different sequence of input samples. In so doing, the tem-
poral structure of the input signal is embedded in the spatial structure of the
network.
• Explicit representation. Time is given its own particular representation inside the
network structure. For example, the echolocation system of a bat operates by emit-
ting a short frequency-modulated (FM) signal, so that the same intensity level is
maintained for each frequency channel restricted to a very short period within the

FM sweep. Multiple comparisons between several different frequencies encoded
by an array of auditory receptors are made for the purpose of extracting accurate
distance (range) information about a target (Suga and Kanwal, 1995). When an
echo is received from the target with an unknown delay, a neuron (in the auditory
system) with a matching delay line responds, thereby providing an estimate of the
range to the target.
In this section, we are concerned with the implicit representation of time, whereby a
static neural network (e.g., multilayer perceptron) is provided with dynamic properties
by external means.
Figure 4.24 shows the block diagram of a nonlinear filter consisting of the cascade
connection of two subsystems: short-term memory and a static neural network (e.g.,
multilayer perceptron).This structure provides for a clear-cut separation of processing
roles: The static network accounts for nonlinearity, and the memory accounts for time.
To be specific, suppose we are given a multilayer perceptron with an input layer of
size m. Then, in a corresponding way, the memory is a single-input, multiple-output
(SIMO) structure providing m differently delayed versions of the input signal for
stimulating the neural network.
Short-Term Memory Structures
Figure 4.25 shows the block diagram of a discrete-time memory structure consisting of p
identical sections connected in cascade. Each section is characterized by an impulse
response, denoted by h(n), where n denotes discrete time.The number of sections, p, is
204
Chapter 4
Multilayer Perceptrons
Input
x(n)
Output
y(n)
d(n)


Error
signal
Short-
term
memory
Static
neural
network
Σ
FIGURE 4.24
Nonlinear filter built on a static neural network.
h(n)
Input
signal
Unit 1
Output terminals
h(n)
Unit 2
h(n)
Unit p
FIGURE 4.25
Generalized tapped-delay-line memory of order p.

Section 4.18
Nonlinear Filtering
205
called the order of the memory. Correspondingly, the number of output terminals (i.e.,
taps) provided by the memory is p  1, which includes the direct connection from the
input to the output.Thus, with m denoting the size of the input layer of the static neural
network, we may set
The impulse response of each delay section of the memory satisfies two properties:
• causality, which means that h(n) is zero for n < 0;
• normalization, which means that 
.
On this basis, we may refer to h(n) as the generating kernel of the discrete-time
memory.
The attributes of a memory structure are measured in terms of depth and resolu-
tion (deVries and Principe, 1992). Let hoverall(n) denote the overall impulse response of
the memory.With p memory sections, it follows that hoverall(n) is defined by p successive
convolutions of the impulse response h(n).Accordingly, the memory depth D is defined
as the first time moment of hoverall(n), namely,
(4.153)
A memory of low depth D holds information content for a relatively short time
interval, whereas a high-depth memory holds it much further into the past. Memory res-
olution R is defined as the number of taps in the memory structure per unit of time. A
memory of high resolution is therefore able to hold information about the input sequence
at a fine level, whereas a low-resolution memory can do so only at a coarser level. For
a fixed memory order p, the product of memory depth D and memory resolution R is a
constant that turns out to be equal to p.
Naturally, different choices of the generating kernel h(n) result in different values
for the depth D and memory resolution R, as illustrated by the following two memory
structures:
1. Tapped-delay-line memory, for which the generating kernel is simply defined by the
unit impulse (n); that is,
(4.154)
Correspondingly, the overall impulse response is
(4.155)
Substituting Eq. (4.155) into Eq. (4.153) yields the memory depth D  p, which is
intuitively satisfying. Moreover, since there is only one tap per time unit, it follows
that the resolution R  1, yielding a depth-resolution product equal to p.
hoverall(n) = (n - p) = e1,
n = p
0,
n Z p
h(n) = (n) = e1,
n = 0
0,
n Z 0
D = a
q
n=0
nhoverall(n) 
a
q
n=0
h(n) = 1
m = p + 1

2. Gamma memory, for which the generating kernel is defined by
(4.156)
where  is an adjustable parameter (deVries and Principe, 1992). For h(n) to be
convergent (i.e., for the short-term memory to be stable), we require that
Correspondingly, the overall impulse response of the gamma memory is
(4.157)
where
is a binomial coefficient. The impulse response hoverall(n) for varying 
p represents a discrete version of the integrand of the gamma function (deVries
and Principe, 1992)—hence the name “gamma memory.” Figure 4.26 plots the
impulse response hoverall(n),normalized with respect to ,for varying memory order
p  1, 2, 3, 4 and   0.7. Note also that the time axis has been scaled by the 
parameter , which has the effect of positioning the peak value of hoverall(n) at 
n  p  1.
It turns out that the depth of gamma memory is (p/) and the resolution is , again
producing a depth-resolution product equal to p. Accordingly, by choosing  to be less
than unity, the gamma memory produces improvement in depth at the expense of res-
olution.For the special case of   1,the gamma memory reduces to an ordinary tapped-
delay-line memory where each section consists simply of a unit-time delay operator.
a
b
hoverall(n) = a n - 1
p - 1 bp(1 - )n-p,  n  p 
0 6  6 2
h(n) = (1 - )n-1,  n  1 
206
Chapter 4
Multilayer Perceptrons
10
p  1
n/
8
6
4
2
0
0.2
0.4
0.6
0.8
1
hoverall(n)
1

p  2
p  3
p  4
FIGURE 4.26
Family of impulse responses of the gamma memory for order p  1, 2, 3, 4 and
  0.7.

Section 4.18
Nonlinear Filtering
207
Universal Myopic Mapping Theorem
The nonlinear filter of Fig. 4.24 may be generalized to that shown in Fig. 4.27. This
generic dynamic structure consists of two functional blocks.The block labeled {hj}L
j1
represents multiple convolutions in the time domain, that is, a bank of linear filters
operating in parallel. The hj are drawn from a large set of real-valued kernels, each
one of which represents the impulse response of a linear filter.The block labeled N
represents a static (i.e., memoryless) nonlinear feedforward network such as the
multilayer perceptron. The structure of Fig. 4.27 is a universal dynamic mapper. In
Sandberg and Xu (1997a), it is shown that any shift-invariant myopic map can be
uniformly approximated arbitrarily well by a structure of the form depicted in Fig.
4.27 under mild conditions. The requirement that a map be myopic is equivalent to
“uniformly fading memory”; it is assumed here that the map is causal, which means
that an output signal is produced by the map at time 
only when the input
signal is applied at time n  0. By “shift invariant,” we mean the following: If y(n)
is the output of the map generated by an input x(n), then the output of the map gen-
erated by the shifted input x(n - n0) is y(n - n0), where the time shift n0 is an inte-
ger. In Sandberg and Xu (1997b), it is further shown that for any single-variable,
shift-invariant, causal, uniformly fading memory map, there is a gamma memory and
static neural network, the combination of which approximates the map uniformly and
arbitrarily well.
We may now formally state the universal myopic mapping theorem15 as follows
(Sandberg and Xu, 1997a, 1997b):
Any shift-invariant myopic dynamic map can be uniformly approximated arbitrarily well by
a structure consisting of two functional blocks: a bank of linear filters feeding a static neural
network.
As already mentioned, a multilayer perceptron may serve the role of the static network.
It is also noteworthy that this theorem holds when the input and output signals are func-
tions of a finite number of variables, as in image processing, for example.
n  0
Bank of
convolution
kernels
(linear
filters)
Static
nonlinear
network
Input
x(n)
Output
y(n)

h1(n)
h2(n)
hL(n)
•
•
•
FIGURE 4.27
Generic structure for
universal myopic mapping theorem.

Practical Implications of the Theorem
The universal myopic mapping theorem has profound practical implications:
1. The theorem provides justification for NETtalk,which was the first demonstration of
a massively parallel distributed network that converts English speech to phonemes;
a phoneme is a basic linguistic unit (Sejnowski and Rosenberg, 1987). Figure 4.28
shows a schematic diagram of the NETtalk system,based on a multilayer perceptron
with an input layer of 203 sensory (source) nodes, a hidden layer of 80 neurons, and
an output layer of 26 neurons.All the neurons used sigmoid (logistic) activation func-
tions. The synaptic connections in the network were specified by a total of 18,629
weights, including a variable threshold for each neuron; threshold is the negative of
bias. The standard back-propagation algorithm was used to train the network. The
network had seven groups of nodes in the input layer,with each group encoding one
letter of the input text.Strings of seven letters were thus presented to the input layer
at any one time.The desired response for the training process was specified as the cor-
rect phoneme associated with the center (i.e., fourth) letter in the seven-letter win-
dow.The other six letters (three on either side of the center letter) provided a partial
context for each decision made by the network. The text was stepped through the
window on a letter-by-letter basis.At each step in the process,the network computed
a phoneme, and after each word the synaptic weights of the network were adjusted
according to how closely the computed pronunciation matched the correct one.
The performance of NETtalk exhibited some similarities with observed
human performance, as summarized here (Sejnowski and Rosenberg, 1987):
• The training followed a power law.
• The more words the network learned, the better it was at generalizing and cor-
rectly pronouncing new words.
• The performance of the network degraded very slowly as synaptic connections
in the network were purposely damaged.
• Relearning after damage to the network was much faster than learning during
the original training.
208
Chapter 4
Multilayer Perceptrons
FIGURE 4.28
Schematic diagram of the NETtalk network architecture.
(     –              a             –             c             a              t              –    )
Output neurons
Hidden neurons
Source nodes
/ k /
TEACHER

Section 4.19
Small-Scale Versus Large-Scale Learning Problems
209
NETtalk was a brilliant illustration in miniature of many aspects of learning,starting
out with considerable “innate” knowledge of its input patterns and then gradually
acquiring competence at converting English speech to phonemes through practice.
2. The universal myopic theorem lays down the framework for the design of more
elaborate models of nonlinear systems.The multiple convolutions at the front end
of the structure in Fig. 4.27 may be implemented using linear filters with a finite-
duration impulse response (FIR) or infinite-duration impulse response (IIR). Most
importantly, the structure of Fig. 4.27 is inherently stable, provided that the linear fil-
ters are themselves stable.We thus have a clear-cut separation of roles as to how to
take care of short-term memory and memoryless nonlinearity in building a stable
dynamic system.
3. Given a stationary time series x(1), x(2),
x(n), we may use the universal
myopic mapping structure of Fig. 4.27 to build a predictive model of the
underlying nonlinear physical laws responsible for generating the time series
by setting y(n)  x(n  1), no matter how complex the laws are. In effect, the
future sample x(n  1) plays the role of desired response. When a multilayer
perceptron is used as the static network in Fig. 4.27 for such an application, it
is advisable to provide a linear neuron for the output unit in the network. This
provision will ensure that no amplitude limitation is placed on the dynamic
range of the predictive model.
4.19
SMALL-SCALE VERSUS LARGE-SCALE LEARNING PROBLEMS
At various points along the way in this chapter and other parts of the book,we have made
references to small-scale and large-scale learning problems. However, we did not elab-
orate rigorously on the meaning of these two kinds of supervised learning.The purpose
of this section is to sharpen the statistical and computational issues that distinguish them
from each other.
We begin the discussion with structural risk minimization (SRM), which is entirely
statistical in nature; SRM is adequate for dealing with small-scale learning problems.
Then we broaden the discussion by considering computational issues that assume a
prominent role in dealing with large-scale learning problems.
Structural Risk Minimization
The feasibility of supervised learning depends on the following key question:
Does a training sample consisting of N independent and identically distributed examples
contain sufficient information to construct a learning machine capable of good generaliza-
tion performance?
The answer to this fundamental question lies in the method of structural risk mini-
mization, described by Vapnik (1982, 1998).
(x1, d1), (x2, d2), ...,(xN, dN)
...,

To describe what we mean by this method, let the natural source or environment
responsible for generating the training sample be represented by the nonlinear regres-
sion model
(4.158)
where, following the terminology introduced in Chapter 2, the vector x is the regressor,
the scalar d is the response, and is the explanational (modeling) error.The function f
is the unknown, and the objective is to estimate it.To do this estimation, we define the
expected risk (i.e., the ensemble-averaged cost function) as
(4.159)
where the expectation is performed jointly with respect to the regressor–response pair
(x, d). In Chapter 5, we will show that the conditional mean estimator
(4.160)
is the minimizer of the cost function 
. Correspondingly, we write 
as
the minimum value of the cost function defined in Eq. (4.159); it serves as the absolute
optimum that is achievable.
Determination of the conditional mean estimator 
requires knowledge of the
underlying joint probability distribution of the regressor x and the response d.Typically,
however, we find that this knowledge is not available. To circumvent this difficulty,
we look to machine learning for a viable solution. Suppose, for example, we choose a
single-layer multilayer perceptron to do the machine learning. Let the function F(x; w)
denote the input–output relationship of the neural network parameterized by the weight
vector w.We then make our first approximation by setting
(4.161)
Correspondingly, we formulate the model’s cost function as
(4.162)
where, as before, the expectation is performed jointly with respect to the pair (x, d).This
second cost function is naturally formulated differently from the cost function 
pertaining to the source—hence the use of different symbols for them. In imposing the
equality of Eq. (4.161) on the neural network, we have in effect restricted the choice of
the approximating function F(x; w).
Let
(4.163)
be the minimizer of the cost function J(w). The reality, however, is that even if we can
find the minimizer 
, it is highly likely that the resulting cost function 
will be
worse than the minimized cost function 
. In any event, we cannot do better
than 
, and thus we write
(4.164)
J(wˆ *) 7 Jactual(fˆ*)
Jactual(fˆ*)
Jactual(fˆ*)
J(wˆ *)
wˆ *
wˆ * = arg min J(w)
w
Jactual(f)
J(w) = x,dc 1
2 (d - F(x; w))2d
f(x) = F(x; w)
fˆ*
Jactual(fˆ*)
Jactual(f)
fˆ* = [d @ x]
Jactual(f) = x,dc 1
2 (d - f(x))2d

d = f(x) + 
210
Chapter 4
Multilayer Perceptrons

4.19
Small-Scale Versus Large-Scale Learning Problems
211
Unfortunately, we are still faced with the same practical problem as before in that
we may not know the underlying joint probability distribution of the pair (x, d).To alle-
viate this difficulty, we make our second approximation by using the empirical risk (i.e.,
the time-averaged energy function)
(4.165)
whose minimizer is defined by
(4.166)
Clearly, the minimized cost function 
cannot be smaller than 
. Indeed, it is
highly likely to find that
(4.167)
With the two approximations that have been made,we may wonder why we should
compute the minimum 
exactly. Before addressing this question, let us examine what
happens when the example multilayer perceptron is changed by enlarging the size of the
hidden layer.
From Section 4.12, we recall that the multilayer perceptron is a universal approx-
imator of the unknown function f(x). In theory, the parameterized function F(x; w)
approximates the unknown function f(x) with any desired accuracy provided that the size
of the hidden layer is large enough. This, in turn, means that 
becomes closer to
the absolute optimum 
. However, by enlarging the size of the hidden layer, we
may compromise the generalization capability of the multilayer perceptron.In particular,
it is possible for the error 
to increase as a result of enlarging the
hidden layer, unless the size of the training sample, N, is correspondingly increased.The
issue just discussed is the essence of Vapnik’s structural risk minimization, which mani-
fests itself in the “approximation–estimation trade-off.”
To elaborate further on this trade-off, let the excess error
be decomposed into two terms as follows:
(4.168)
In this classical decomposition of errors, the following points are noteworthy:
(i) The estimation error provides a measure of how much performance is lost as a
result of using a training sample of some prescribed size N. Moreover, with 
being dependent on the training sample, the approximation error is therefore rel-
evant in the assessment of network training.
(ii) The approximation error provides a measure of how much performance is lost by
choosing a model characterized by the approximating function F(x, w). More-
over, with  
being a conditional estimator of the response d given the regres-
sor x, the estimation error is therefore relevant in the assessment of network
testing.
fˆ*
wˆ N
J(wˆ N) - Jactual(fˆ*) = J(wˆ N) - J(wˆ *) + J(wˆ *) - Jactual(fˆ*)
(J(wˆ N) - Jactual(fˆ*))
(J(wˆ *) - Jactual(fˆ*))
Jactual(fˆ*)
J(wˆ *)
wˆ N
J(wˆ N) 7 J(wˆ *) 7 Jactual(fˆ*)
J(wˆ *)
J(wˆ N)
wˆ N = arg min eav(N; w)
w
eav(N; w) =
1
2N a
N
n=1
(d(n) - F(x(n); w))2
Approximation error
¯˚˚˚˚˚˘˚˚˚˚˚˙
Excess error
¯˚˚˚˚˚˘˚˚˚˚˚˙
Estimation error
¯˚˚˚˚˘˚˚˚˚˙

212
Chapter 4
Multilayer Perceptrons
In Vapnik’s theoretical framework, the approximation and estimation errors are
formulated in terms of the VC dimension, commonly denoted by h. This new parame-
ter, short for the Vapnik– Chervonenkis dimension (Vapnik and Chervonenkis, 1971), is
a measure of the capacity, or expressive power, of a family of binary classification func-
tions realized by the learning machine.16 For the example of a single-layer multilayer per-
ceptron, the VC dimension is determined by the size of the hidden layer; the larger this
size is, the larger the VC dimension h will be.
To put Vapnik’s theory in a practical context, consider a family of nested approx-
imating network functions denoted by
(4.169)
such that we have
where the symbol 
means “is contained in.” Correspondingly, the VC dimensions of
the individual subsets of FK satisfy the condition
In effect, the size of FK is a measure of the machine capacity. Hereafter, we use the
definition of Eq. (4.169) in place of the VC dimension.
Figure 4.29 plots variations of the approximation and estimation errors versus the
size K of the family of approximating network functions FK. For the example of a single-
layer multilayer perceptron, the optimum size of the hidden layer is determined by the
point at which the approximation error and the estimation error assume a common
value. Before the optimum condition is reached, the learning problem is overdetermined,
which means that the machine capacity is too small for the amount of detail contained in
the training sample.Beyond the minimum point,the learning problem is underdetermined,
which means that the machine capacity is too large for the training sample.
h1 6 h2 6 p 6 hK
(
F1 ( F2 ( p ( FK
k = 1, 2, ..., K
Fk = {F(x; w)(w  Wk)},
Error
0
Optimum condition
Estimation
error
Bound on the
excess error
Approximation
error
Size of the approximating network functions, K
FIGURE 4.29
Variations of the approximation and estimation errors with the size K.

4.19
Small-Scale Versus Large-Scale Learning Problems
213
Computational Considerations
The neural network model (e.g., the single-layer multilayer perceptron) must be a
controlled variable, so that it can be freely adjusted to achieve the best test performance
on data not seen before. Another controlled variable is the number of examples to do
the training. In order to add practical realism to the supervised-training process, Bottou
(2007) has introduced the cost of computation by considering a new controlled vari-
able: optimization accuracy.
In practice, it is possible to find that the task of computing the minimizer 
is
rather costly. Moreover, in the course of coming up with a satisfactory network design,
we usually make many approximations. Suppose, then, we settle on a network model
characterized by the weight vector 
which is different from 
in so doing, we will
have made our third, and final, approximation. For example, the on-line learning algo-
rithm could be stopped long before convergence has been reached, due to limited com-
puting time. In any event,
is a suboptimal solution that satisfies the condition
(4.170)
where 
constitutes a new controlled variable; it provides a measure of computational
accuracy.
In light of this new practicality,we now have a more complicated problem than that
encountered in the method of structural risk minimization. Specifically, we must now
adjust three variables:
• the network model (for instance, through the number of hidden neurons in a mul-
tilayer perceptron),
• the number of training examples, and
• the optimization accuracy (for instance by prematurely terminating computation
of the minimizer 
and settling on the suboptimal solution 
).
In order to hit the best test performance, we have to satisfy budget constraints, which
define the maximum number of training examples that we can use and the maximum
computing time that we can afford. In a practical context, we are therefore confronted
with a trade-off that is rather complicated.In solving this constrained-optimization prob-
lem, the trade-off will depend on whether we first hit a limit on the number of examples
or impose a limit on the computing time.Which of these two limits is the active budget
constraint depends on whether the supervised-learning process is of a small-scale or
large-scale kind, as discussed next.
Definitions
According to Bottou (2007), small-scale and large-scale learning problems are respec-
tively defined as follows:
Definition I.
Small-scale learning
A supervised-learning problem is said to be of a small-scale kind when the size of the
training sample (i.e., the number of examples) is the active budget constraint imposed
on the learning process.
w
N
wˆ N
&
eav(N; w
N)  eav(N; wˆ N) + &
w
N
wˆ N;
w
N,
wˆ N

214
Chapter 4
Multilayer Perceptrons
Definition II.
Large-scale learning
A supervised-learning problem is said to be of a large-scale kind when the computing
time is the active budget constraint imposed on the learning process.
In other words, it is the active budget constraint that distinguishes one learning
problem from the other.
For an illustrative example of a small-scale learning problem, we may mention
the design of an adaptive equalizer, the purpose of which is to compensate for the
inevitable distortion of information-bearing data transmitted over a communica-
tion channel. The LMS algorithm, rooted in stochastic gradient descent and dis-
cussed in Chapter 3, is widely used for solving this on-line learning problem (Haykin,
2002).
For an illustrative example of a large-scale learning problem, we may mention
the design of a check reader where the training examples consist of joint pairs, each
of which describes a particular {image, amount} pair, where “image” pertains to a
check and “amount” pertains to the amount of money inscribed in the check. Such a
learning problem has strong structure that is complicated by the following issues
(Bottou, 2007):
• field segmentation;
• character segmentation;
• character recognition;
• syntactical interpretation.
The convolutional network,embodying differentiable modules as described in Section 4.17
and trained with a stochastic gradient algorithm for a few weeks, is widely used for solv-
ing this challenging learning problem (LeCun et al., 1998). Indeed, this novel network
has been deployed in industry since 1996, running billions of checks.
Small-Scale Learning Problems
Insofar as small-scale learning problems are concerned, there are three variables avail-
able to the designer of a learning machine:
• the number of training examples, N;
• the permissible size K of the family of approximating network functions F;
• the computational error & introduced in Eq. (4.170).
With the active budget constraint being the number of examples, the design options in
learning problems of the first kind are as follows (Bottou, 2007):
• Reduce the estimation error by making N as large as the budget permits.
• Reduce the optimization error by setting the computational error 
 0, which
means setting  

.
• Adjust the size of F to the extent deemed to be reasonable.
wˆ N
w
N
&

4.19
Small-Scale Versus Large-Scale Learning Problems
215
With 
 0, the method of structural risk minimization, involving the approximation–
estimation tradeoff illustrated in Fig. 4.29, is adequate for dealing with small-scale
learning problems.
Large-Scale Learning Problems
As pointed out previously, the active budget constraint in large-scale learning prob-
lems is the computing time. In tackling learning problems of this second kind, we
face more complicated trade-offs because we now have to account for the computing
time T.
In large-scale learning problems, the excess error is defined by the difference
, which is decomposed into three terms, as shown by the follow-
ing (Bottou, 2007):
(4.171)
The last two terms, constituting the approximation and estimation errors, are common
to both small-scale and large-scale learning problems. It is the first term in Eq. (4.171)
that distinguishes large-scale learning problems from small-scale ones. This new term,
called the optimization error, is obviously related to the computational error .
Computation of the bound on the approximation error, depicted in Fig. 4.29, is
reasonably well understood (in terms of the VC theory) for small-scale learning prob-
lems. Unfortunately, the constants involved in the formula for this bound are quite bad
when the formula is applied to large-scale learning problems. In these more difficult sit-
uations, it is therefore more productive to analyze Eq. (4.171) in terms of convergence
rates rather than bounds.
The requirement is to minimize the sum of the three terms in Eq. (4.171) by adjust-
ing the available variables:
• the number of examples, N;
• the permissible size K of approximating network functions, FK;
• the computational error , which is no longer zero.
Doing this minimization analytically is extremely difficult, due to the fact that the com-
puting time T is actually dependent on all three variables N, F, and .To illustrate the
consequences of this dependence, suppose we assign a small value to the error  so as
to reduce the optimization error. To realize this reduction, unfortunately, we must also
increase N, F, or both, any of which would have undesirable effects on the approxima-
tion and estimation errors.
Nevertheless, in some cases, it is possible to compute the exponents with respect
to which the three errors tend to decrease when  decreases and both F and N increase.
Similarly,it is possible to identify the exponents with respect to which the computing time
T increases when  decreases and both F and N increase. Putting these pieces together,
we have the elements for an approximate solution to trade-offs in tackling large-scale
J(w
N) - Jactual(fˆ*) = J(w
N)- J(wˆ N) +J(wˆ N) -J(wˆ *)+ J(wˆ *) - Jactual(fˆ*)
(J(w
N) - Jactual(fˆ*))

Estimation error
¯˚˚
˚˚˘˚˚
˚˚˙
Approximation error
¯˚˚˚˚˘˚˚˚˚˙
Excess error
¯˚˚˚˚˘˚˚˚˚˙
Optimization error
¯˚˚˚˘˚˚˚˙

216
Chapter 4
Multilayer Perceptrons
learning problems. Most importantly, in the final analysis, the trade-offs depend on the
choice of the optimization algorithm.
Figure 4.30 illustrates how a plot of log & versus log T is affected by the type of
optimization algorithm used to solve a large-scale learning problem. Three categories
of optimization algorithms are identified in this figure—namely, bad, mediocre, and
good—examples of which respectively include stochastic gradient descent (i.e., on-line
learning), gradient descent (i.e., batch learning), and second-order gradient descent
(e.g., quasi-Newton optimization algorithm of the BFGS kind or its extension). Table 4.4
summarizes the distinguishing features of these three categories of optimization
algorithms.
TABLE 4.4
Summary of Statistical Characteristics of Three Optimization Algorithms*
Algorithm 
Cost per
Time to
iteration
reach 
1. Stochastic
O(m)
gradient descent
(on-line learning)
2. Gradient descent
O(Nm)
(batch learning)
3. Second-order
O(m(m  N))
gradient descent
(on-line learning)
m : dimension of input vector x
N : number of examples used in training
: computational error
*This table is compiled from Bottou (2007).
&
O alog alog 1
& b b
O alog 1
& b
O a 1
& b
&
log &
log T
Setting for
best &
Mediocre optimization algorithm (linear), for 
which & decreases like exp (T)
Good optimization algorithm (superlinear), for 
which & decreases faster than exp (T)
Bad optimization algorithm, for 
which & decreases like 1/T
FIGURE 4.30
Variations of the computational error versus the computation time T for
three classes of optimization algorithm: bad, mediocre, and good. (This figure is reproduced
with the permission of Dr. Leon Bottou.)
&

4.20
Summary and Discussion
217
The message to take from the material presented in this section on supervised
learning may now be summed up as follows:
Whereas the study of small-scale learning problems is well-developed, the study of large-scale
learning problems is in its early stages of development.
4.20
SUMMARY AND DISCUSSION
The back-propagation algorithm has established itself as a computationally efficient
and useful algorithm for the training of multilayer perceptrons. The algorithm derives
its name from the fact that the partial derivatives of the cost function (performance
measure) with respect to the free parameters (synaptic weights and biases) of the net-
work are determined by back-propagating the error signals (computed by the output
neurons) through the network,layer by layer.In so doing,the algorithm solves the credit-
assignment problem in a most elegant fashion. The computing power of the algorithm
lies in its two main attributes:
•
the local method, for updating the synaptic weights and biases of the multilayer
perceptron;
•
the efficient method, for computing all the partial derivatives of the cost function
with respect to these free parameters.
Stochastic and Batch Methods of Training
For a given epoch of training data, the back-propagation algorithm operates in one of
two modes: stochastic or batch. In the stochastic mode, the synaptic weights of all neu-
rons in the network are adjusted in a sequential manner, pattern by pattern. Conse-
quently, estimation of the gradient vector of the error surface used in the computation
is stochastic in nature—hence the name “stochastic back-propagation learning.” On the
other hand, in the batch mode, the adjustments to all synaptic weights and biases are
made on an epoch-by-epoch basis, with the result that a more accurate estimate of the
gradient vector is utilized in the computation. Despite its disadvantages, the stochastic
form of back-propagation learning is most frequently used for the training of multilayer
perceptrons, particularly for large-scale problems.To achieve the best results, however,
careful tuning of the algorithm is required.
Pattern Classification and Nonlinear Filtering
The specific details involved in the design of a multilayer perceptron naturally depend
on the application of interest.We may, however, make two distinctions:
1. In pattern classification involving nonlinearly separable patterns,all the neurons
in the network are nonlinear. The nonlinearity is achieved by employing a sigmoid
function, two commonly used forms of which are (a) the logistic function, and (b) the
hyperbolic tangent function. Each neuron is responsible for producing a hyperplane of

218
Chapter 4
Multilayer Perceptrons
its own in decision space. Through a supervised learning process, the combination of
hyperplanes formed by all the neurons in the network is iteratively adjusted in order
to separate patterns drawn from the different classes and not seen before, with the
fewest classification errors on average. For pattern classfication, the stochastic back-
propagation algorithm is widely used to perform the training, particularly for large-scale
problems (e.g., optical character recognition).
2. In nonlinear filtering, the dynamic range at the output of the multilayer per-
ceptron should be sufficiently large to accommodate the process values; in this context,
the use of linear output neurons is the most sensible choice.As for learning algorithms,
we offer the following observations:
• On-line learning is much slower than batch learning.
• Assuming that batch learning is the desired choice, the standard back-propagation
algorithm is slower than the conjugate gradient algorithm.
The method of nonlinear filtering, discussed in this chapter, focused on the use of a
static network, exemplified by the multilayer perceptron; the input signal is applied
to the multilayer perceptron through a short-term memory structure (e.g., tapped
delay line or gamma filter) that provides for time, which is an essential dimension
of filtering. In Chapter 15, we revisit the design of nonlinear filters for which
feedback is applied to a multilayer perceptron, turning it into a recurrent neural
network.
Small-scale versus Large-scale Learning Problems
Generally speaking, there are three kinds of error that can arise in the study of machine-
learning problems:
1. Approximation error, which refers to the error incurred in the training of a
neural network or learning machine, given a training sample of some finite size N.
2. Estimation error, which refers to the error incurred when the training of the
machine is completed and its performance is tested using data not seen before; in effect,
estimation error is another way of referring to generalization error.
3. Optimization error, the presence of which is attributed to accuracy of the com-
putation involved in training the machine for some prescribed  computing time T.
In small-scale learning problems, we find that the active budget constraint is the size
of the training sample, which implicily means that the optimization error is usually
zero in practice.Vapnik’s theory of structural risk minimization is therefore adequately
equipped to handle small-scale learning problems. On the other hand, in large-scale
learning problems, the active budget constraint is the available computing time, T,
with the result that the optimization error takes on a critical role of its own. In par-
ticular, computational accuracy of the learning process and therefore the optimiza-
tion error are both strongly affected by the type of optimization algorithm employed
to solve the learning problem.

Notes and References
219
NOTES AND REFERENCES
1. Sigmoid functions are “S” shaped graphs; Mennon et al. (1996) present a detailed study of
two classes of sigmoids:
• simple sigmoids, defined to be odd, asymptotically bounded, and completely monotone
functions of one variable;
• hyperbolic sigmoids, representing a proper subset of simple sigmoids and a natural gen-
eralization of the hyperbolic tangent function.
2. For the special case of the LMS algorithm,it has been shown that use of the momentum con-
stant  reduces the stable range of the learning-rate parameter  and could thus lead to insta-
bility if  is not adjusted appropriately. Moreover, the misadjustment increases with
increasing ; for details, see Roy and Shynk (1990).
3. A vector w* is said to be a local minimum of an input—output function F if it is no worse
than its neighbors—that is, if there exists an  such that
(Bertsekas, 1995).The vector w* is said to be a global minimum of the function F if it is no
worse than all other vectors—that is,
where n is the dimension of w.
4. The first documented description of the use of back propagation for efficient gradient eval-
uation is attributed to Werbos (1974). The material presented in Section 4.8 follows the
treatment given in Saarinen et al. (1992); a more general discussion of the topic is presented
by Werbos (1990).
5. Battiti (1992) presents a review of exact and approximate algorithms for computing the
Hessian, with particular reference to neural networks.
6. Müller et al. (1998) have studied the application of the annealed on-line learning algorithm
of Eq. (4.77) to a nonstationary blind source separation problem,which illustrates the broad
algorithmic applicability of adaptive control of the learning rate due to Murata (1998).The
issue of blind source separation is discussed in Chapter 10.
7. The formulation of Eq. (4.80) follows a corresponding part of the optimally annealed on-
line learning algorithm due to Sompolinski et al. (1995) that deals with adaptation of the
learning-rate parameter. Practical limitations of this algorithm include the need to compute
the Hessian at each iteration and the need to know the minimal loss of the learning curve.
8. The universal approximation theorem may be viewed as a natural extension of the
Weierstrass theorem (Weierstrass, 1885; Kline, 1972).This theorem states 
Any continuous function over a closed interval on the real axis can be expressed in that
interval as an absolutely and uniformly convergent series of polynomials.
Research interest in the virtues of multilayer perceptrons as devices for the repre-
sentation of arbitrary continuous functions was perhaps first put into focus by Hecht-
Nielsen (1987), who invoked an improved version of Kolomogorov’s superposition
theorem due to Sprecher (1965). Then, Gallant and White (1988) showed that a single-
hidden-layer multilayer perceptron with monotone “cosine” squashing at the hidden
layer and no squashing at the output behaves like as a special case of a “Fourier network”
F(w*)  F(w)  for all w  n
F(w*)  F(w)  for all w with 7 w - w*7  6 

220
Chapter 4
Multilayer Perceptrons
that yields a Fourier series approximation to a given function as its output. However, in
the context of traditional multilayer perceptrons,it was Cybenko who demonstrated rigorously
for the first time that a single hidden layer is sufficient to uniformly approximate any con-
tinuous function with support in a unit hypercube; this work was published as a Uni-
versity of Illinois Technical Report in 1988 and republished as a paper one year later
(Cybenko, 1988, 1989). In 1989, two other papers were published independently on mul-
tilayer perceptrons as universal approximators, one by Funahashi (1989) and the other
by Hornik et al. (1990). For subsequent contributions to the approximation problem, see
Light (1992b).
9. The history of the development of cross-validation is documented in Stone (1974).The idea
of cross-validation had been around at least since the 1930s, but refinement of the tech-
nique was accomplished in the 1960s and 1970s.Two important papers from that era are by
Stone (1974) and Geisser (1975),who independently and almost simultaneously propounded
the idea of cross-validation. The technique was termed the “cross-validating method” by
Stone and the “predictive sample reuse method” by Geisser.
10. Hecht-Nielsen (1995) decribes a replicator neural network in the form of a  multilayer per-
ceptron with an input layer of source nodes, three hidden layers and an output layer:
• The activation functions of neurons in the first and third hidden layers are defined by the
hyperbolic tangent function
where v is the induced local field of a neuron in those layers.
• The activation function for each neuron in the second hidden layer is given by
where a is a gain parameter and v is the induced local filed of a neuron in that layer. The
function 
describes a smooth staircase activation function with N treadles, thereby
(2)(v) 
(2)(v) = 1
2 +
2
2(N - 1) a
N-1
j=1
tanh aa a v - j
N b b
(1)(v) = (3)(v) = tanh(v)
essentially quantizing the vector of the respective neural outputs into K  Nn, where n
is the number of neurons in the middle hidden layer.
• The neurons in the output layer are linear, with their activation functions defined by
• Based on this neural network structure, Hecht-Nielsen describes a theorem show-
ing that optimal data compression for arbitrary input data vector can be carried
out.
11. The classic reference for the conjugate-gradient method is Hestenes and Stiefel (1952). For
a discussion of the convergence behavior of the conjugate-gradient algorithm,see Luenberger
(1984) and Bertsekas (1995). For a tutorial treatment of the many facets of the conjugate-
gradient algorithm,see Shewchuk (1994).For a readable account of the algorithm in the con-
text of neural networks, see Johansson et al. (1990).
12. The conventional form of the conjugate-gradient algorithm requires the use of a line search,
which can be time consuming because of its trial-and-error nature. Møller (1993) describes
a modified version of the conjugate-gradient algorithm called the scaled conjugate-gradient
algorithm, which avoids the use of a line search. Essentially, the line search is replaced by a
one-dimensional Levenberg–Marquardt form of algorithm. The motivation for using such
methods is to circumvent the difficulty caused by nonpositive-definite Hessian matrices
(Fletcher, 1987).
(4)(v) = v

Problems
221
13. The so-called r-technique, due to Pearlmutter (1994), provides an efficient procedure for
computing a matrix-vector product; as such, this technique can be of practical use in com-
puting the inverse Hessian H1 in Eq.(4.138).The r-technique is addressed in Problem 4.6.
14. Hubel and Wiesel’s notion of “simple” and “complex” cells was first exploited in the neural
network literature by Fukushima (1980, 1995) in the design of a learning machine called the
neocognitron.This learning machine,however,operates in a self-organized manner,whereas
the convolutional network described in Fig. 4.23 operates in a supervised manner using
labeled examples.
15. For the origins of the universal myopic mapping theorem, see Sandberg (1991).
16. For a detailed account of the VC-dimension and the related bound on empirical risk, see
the classic book on statistical learning theory by Vapnik (1998). The VC-dimension is also
discussed in the books by Scho..lkopf and Smola (2002) and Herbrich (2002).A noteworthy
comment is in order: the VC-dimension is related to Cover’s separating capacity, which will
be discussed in the next chapter, Chapter 5.
PROBLEMS
Back-Propagation Learning
4.1 Figure P4.1 shows a neural network involving a single hidden neuron for solving the XOR
problem; this network may be viewed as an alternative to that considered in Section 4.5. Show
that the network of Fig. P4.1 solves the XOR problem by constucting (a) decision regions, and
(b) a truth table for the network.
4.2 Use the back-propagation algorithm for computing a set of synaptic weights and bias levels
for a neural network structured as in Fig. 4.8 to solve the XOR problem. Assume the use of
a logistic function for the nonlinearity.
4.3 The momentum constant  is normally assigned a positive value in the range 0  
1.
Investigate the difference that would be made in the behavior of Eq.(4.43) with respect to time
t if  were assigned a negative value in the range -1
  0.
4.4 Consider the simple example of a network involving a single weight, for which the cost func-
tion is
where 
, k1, and k2 are constants.A back-propagation algorithm with momentum  is used
to minimize 
.
e(w)
w0
e(w) = k1(w - w0)2 + k2


Output
Inputs
1
1
1
1
2
0.5
1.5
FIGURE P4.1

222
Chapter 4
Multilayer Perceptrons
Explore the way in which the inclusion of the momentum constant  influences the
learning process, with particular reference to the number of steps required for convergence
versus .
4.5 Equations (4.51) through (4.53) define the partial derivatives of the approximating function
F(w, x) realized by the multilayer perceptron in Fig. 4.14. Derive these equations from the sce-
nario described by the following conditions:
(a) Cost function:
(b) Output of neuron j:
where 
is the synaptic weight from neuron i to neuron j, and yi is the output of neuron i;
(c) Nonlinearity:
4.6 The r technique,developed by Pearlmutter (1994),provides a computationally fast procedure
for evaluating a matrix–vector product. To illustrate this procedure, consider a multilayer
perceptron with a single hidden layer; the forward-propagation equations of the network are
defined by
r[.] denotes an operator that acts on the quantity enclosed inside the brackets to produce the
following results for the example network at hand:
The r results are to be viewed as new variables. In effect, the operator r[.] follows the ordi-
nary rules of calculus in addition to the condition
where 
is the vector of weights connected to node j and aj is the associated vector resulting
from application of the r operator.
wj
r[wj] = aj
r[wkj] = akj
r[yk] = a
j
wkjr[zj] + a
i
ajizj,
¿(vj) =
0
0vj
 (vj)
r[vj] = ¿(vj)r[vj],
r[wji] = aji
r[vj] = a
i
ajixi,
 yk = a
j
wkjzj
 zj = (vj)
 vj = a
i
wjixi
(v) =
1
1 + exp(-v)
wji
yj =  aa
i
wji yi b
e(n) = 1
2 [d - F(w, x)]2

Problems
223
(a) Applying the r technique to the back-propagation algorithm, derive expressions for the
elements of the matrix–vector product Ha, identifying the new variables for the hidden
and output neurons, the matrix H is the Hessian. For this application, use the multilayer
perceptron described at the beginning of the problem.
(b) Justify the statement that the r technique is computationally fast.
Supervised Learning Issues
4.7 In this problem, we study the output representation and decision rule performed by a multi-
layer perceptron. In theory, for an M-class classification problem in which the union of the M
distinct classes forms the entire input space, we need a total of M outputs to represent all pos-
sible classification decisions, as depicted in Fig. P4.7. In this figure, the vector xj denotes the
jth prototype (i.e., unique sample) of an m-dimensional random vector x to be classified by a
multilayer perceptron.The kth of M possible classes to which x can belong is denoted by ck.
Let ykj be the kth output of the network produced in response to the prototype xj, as shown
by
where the function Fk(·) defines the mapping learned by the network from the input to the
k-th output. For convenience of presentation, let
where F(.) is a vector-valued function.The basic question we wish to address in this problem
is the following:
After a multilayer perceptron is trained, what should the optimum decision rule be for
classifying the M outputs of the network?
To address this problem, consider the use of a multilayer perceptron embodying a logistic
function for its hidden neurons and operating under the following assumptions:
•
The size of the training sample is sufficiently large to make a reasonably accurate esti-
mate of the probability of correct classification.
•
The back-propagation algorithm used to train the multilayer perceptron does not get
stuck in a local minimum.
Specifically, develop mathematical arguments for the property that the M outputs of the mul-
tilayer perceptron provide estimates of the a posteriori class probabilities.
4.8 In this problem, we revisit the adaptive control of the learning rate discussed in Section
4.10. The issue of interest is to demonstrate that the asymptotic behavior of the learning-
rate parameter 
in Eq. (4.85) does not converge to zero as the number of iterations
increases to infinity.
(n)
= F(xj)
= [F1(xj), F2(xj), ..., FM(xj)]T
yj = [y1j, y2j,  ..., yMj]T
ykj = Fk(xj),  k = 1, 2, ..., M
xj
Multilayer
perceptron
w
y1,j
y2,j
yM,j
•
•
•
FIGURE P4.7
Block diagram of a
pattern classifier for Problem 4.7.

224
Chapter 4
Multilayer Perceptrons
(a) Let -r(n) denote the expectation of the auxiliary vector r(n) with respect to the example
{x, d}. Show that if the estimator (n) is in the close vicinity of the optimal estimator w*,
we may then write
where w¯ (n) is the mean value of the estimator (n) and  is a small positive parameter.
(b) In Heskas and Kappen (1991),it is shown that the estimator (n) is closely approximated by
a Gaussian-distributed random vector. Hence, justify the following asymptotic behavior:
What does this condition teach us about the asymptotic behavior of the learning-
rate parameter (n)?
4.9 The composition of the minimum-description-length (MDL) criterion is described as follows
(see Eq. (2.37)):
lim
nS q wˆ (n) Z w(n)
wˆ
wˆ
r– (n + 1) L (1 - ) r– (n) + K*(wˆ (n) - w– (n))
wˆ
MDL = (Error term) + (Complexity term)
Discuss how the weight-decay method used for network pruning fits into the MDL formalism.
4.10 In the optimal-brain-damage (OBD) algorithm for network pruning, due to LeCun et al.
(1990b), the Hessian H is approximated by its diagonal version. Using this approximation,
derive the OBD procedure as a special case of the optimal-brain-surgeon (OBS) algorithm,
studied in Section 4.14.
4.11 In Jacobs (1988), the following heuristics are proposed to accelerate the convergence of 
on-line back-propagation learning:
(i) Every adjustable network parameter of the cost function should have its own learning-
rate parameter.
(ii) Every learning-rate parameter should be allowed to vary from one iteration to the next.
(iii) When the derivative of the cost function with respect to a synaptic weight has the same
algebraic sign for several consecutive iterations of the algorithm, the learning-rate para-
meter for that particular weight should be increased.
(iv) When the algebraic sign of the cost function with respect to a particular synaptic weight
alternates for several consecutive iterations of the algorithm, the learning-rate parame-
ter for that weight should be decreased.
These four heuristics satisfy the locality constraint of the back-propagation algorithm.
(a)
Use intuitive arguments to justify these four heuristics.
(b) The inclusion of a momentum in the weight update of the back-propagation algorithm
may be viewed as a mechanism for satisfying heuristics (iii) and (iv). Demonstrate the
validity of this statement.
Second-Order Optimization Methods
4.12 The use of a momentum term in the weight update described in Eq. (4.41) may be consid-
ered as an approximation to the conjugate-gradient method (Battiti, 1992). Discuss the valid-
ity of this statement.
4.13 Starting with the formula for (n) in Eq. (4.127), derive the Hesteness-Stiefel formula,
where s(n) is the direction vector and r(n) is the residual in the conjugate-gradient method.
Use this result to derive the Polak–Ribière formula of Eq. (4.128) and the Fletcher–Reeves
formula of Eq. (4.129).
(n) =
rT(n)(r(n) - r(n - 1))
sT(n - 1)r(n - 1)

Problems
225
Temporal Processing
4.14 Figure P4.14 illustrates the use of a Gaussian-shaped time window as a method for tempo-
ral processing, which is motivated by neurobiological considerations (Bodenhausen and
Waibel,1991).The time window associated with synapse i of neuron j is denoted by 	(n, ji, ji),
where ji and ji are measures of time delay and width of the windows, respectively, as shown
by
,
The output of neuron j is thus defined as
where ui(n) is the convolution of the input xi(n) and the time window 	(n, ji,ji).The require-
ment is for the weight 
, and time delay ji of synapse i belonging to neuron j are all to be
learned in a supervised manner.
This process of learning may be accomplished by using the standard back-propagation
algorithm. Demonstrate this learning process by deriving update equations for 
, ji, and ji.
Computer Experiments
4.15 Investigate the use of back-propagation learning employing a sigmoidal nonlinearity to
achieve one-to-one mappings, as described here:
1.
2.
3.
4.
0  x  
2
f(x) = sin x,
1  x  10
f(x) = exp(-x),
1  x  10
f(x) = log10x,
1  x  100
f(x) = 1
x,
wji
wji
yj(n) =  a a
m0
i=1
wjiui(n) b
i = 1, 2, ..., m0
	(n, ji, ji) =
1
22ji
 exp a-
1
22
ji
(n - ji)2 b
Input
x1(n)
Input
x2(n)
Move this window
to the left
Σ
Output of neuron j
vj(n)
•
•
•
Input
xm(n)
Make this
window
wider
Time n
FIGURE P4.14
The figure for Problem 4.14; the instructions appended
to the Gaussian windows are aimed at the learning algorithm.

226
Chapter 4
Multilayer Perceptrons
For each mapping, do the following:
(a) Set up two sets of data, one for network training, and the other for testing.
(b) Use the training data set to compute the synaptic weights of the network, assumed to
have a single hidden layer.
(c) Evaluate the computation accuracy of the network by using the test data.
Use a single hidden layer, but with a variable number of hidden neurons. Investigate how the
network performance is affected by varying the size of the hidden layer.
4.16 Repeat the computer experiment of Section 4.7 for the MLP classifier, where the distance
between the two moons is set at d  0. Comment on the findings of your experiment in light
of the corresponding experiment performed on the perceptron in Problem 1.6 for the same
setting.
4.17 In this computer experiment, we consider a pattern-classification experiment for which the
decision boundary is theoretically known.The primary objective of the experiment is to see
how the design of the multilayer perceptron could be optimized experimentally in relation
to the optimum decision boundary.
Specifically, the requirement is to distinguish between two equiprobable classes of “over-
lapping” two-dimensional Gaussian-distributed patterns, labeled c1 and c2.The conditional
probability density functions of the two classes are
(a) The optimum Bayesian decision boundary is defined by the likelihood ratio test
where
and  is the threshold determined by the prior probabilities of the two classes. Show
that the optimum decision boundary is a circle whose center is located at
and radius r  2.34.
xc = c -23
0
d
¶(x) =
px∑c1(x@c1)
px@c2(x@c2)
¶(x) ' 
c1
c2
2
2 = 4
2 = [2, 0]T
where
Class c2:  pXc2(xc2) =
1
22
2
  exp a - 1
22
2
 7 x - 27 2 b
2
1 = variance = 1
1 = mean vector = [0, 0]T
where
Class c1:  pXc1(xc1) =
1
22
1
  exp a - 1
21
2 7 x - 17 2 b

Problems
227
(b) Assume the use of a single hidden layer.The requirement is to experimentally determine
the optimal number of hidden neurons.
• Starting with a multilayer perceptron with two hidden neurons, and using the back-
propagation algorithm with learning-rate parameter   0.1 and momentum con-
stant   0 to train the network, calculate the probability of correct classification for
the following scenarios:
Training-sample size
Number of epochs
500
320
2,000
80
8,000
20
• Repeat the experiment, this time using four hidden neurons, with everything else
remaining the same as before. Compare the results of this second experiment with
those of the previous one and thereby select the network configuration, with two or
four hidden neurons, that you consider to be the optimal choice.
(c) For the “optimal” network selection made in part (b), we now turn to experimentally
find the optimal values of the learning-rate parameter  and momentum constant .To
do this, perform experiments using the following combination of parameters:
Hence, determine the values of  and  that yield the best probability of correct
classification.
(d) Having identified the optimum size of hidden layer and the optimum set of  and ,
perform one last experiment to find the optimum decision boundary and the corre-
sponding probability of correct classification. Compare the optimum performance so
obtained experimentally against the theoretical optimum, and comment on your
results.
4.18 In this problem, we use the standard back-propagation algorithm to solve a difficult non-
linear prediction problem and compare its performance with that of the LMS algorithm.
The time series to be considered is created using a discrete Volterra model that has the
form
where gi, gij,
are the Volterra coefficients; the v(n) are samples of a white, independently
distributed Gaussian noise sequence; and x(n) is the resulting output of the Volterra model.
The first summation term is the familiar moving-average (MA) time-series model, and the
remaining summation terms are nonlinear components of ever increasing order. In general,
the estimation of the Volterra coefficients is considered to be difficult, primarily because of
their nonlinear relationship to the data.
We consider the simple example
x(n) = (n) + (n - 1)(n - 2)
p ,
x(n) = a
i
giv(n - i) + a
i
a
j
gijv(n - i)v(n - j) + p
   [0.0, 0.1, 0.5]
   [0.01, 0.1, 0.5]

228
Chapter 4
Multilayer Perceptrons
The time series has zero mean, is uncorrelated, and therefore has a white spectrum. How-
ever, the time-series samples are not independent of each other, and therefore a higher-
order predictor can be constructed.The variance of the model output is given by
where 
is the white-noise variance.
(a) Construct a multilayer perceptron with an input layer of six nodes, a hidden layer of 16
neurons,and a single output neuron.A tapped-delay-line memory is used to feed the input
layer of the network.The hidden neurons use sigmoid activation functions limited to the
interval [0, 1], whereas the output neuron operates as a linear combiner. The network is
trained with the standard back-propagation algorithm having the following description:

2
2
x = 
2 + 2
4
Learning-rate parameter
  0.001
Momentum constant
  0.6
Total number of samples processed
100,000
Number of samples per epoch
1,000
Total number of epochs
2,500
The white-noise variance 
is set equal to unity. Hence, with   0.5, we find that the

2
output variance of the predictor is 
 1.25.
Compute the learning curve of the nonlinear predictor, with the variance of the
predictor output x(n) plotted as a function of the number of epochs of training samples
up to 2,500 epochs. For the preparation of each epoch used to perform the training,
explore the following two modes:
(i) The time ordering of the training sample is maintained from one epoch to the
next in exactly the same form as it is generated.
(ii) The ordering of the training sample is randomized from one pattern (state) to another.
Also, use cross-validation (described in Section 4.13) with a validation set of 1,000 sam-
ples to monitor the learning behavior of the predictor.
(b) Repeat the experiment, using the LMS algorithm designed to perform a linear predic-
tion on an input of six samples.The learning-rate parameter of the algorithm is set equal
to   10-5.
(c) Repeat the entire experiment for   1,
 2, and then for   2,
 5.
The results of each experiment should reveal that initially the back-propagation algorithm
and the LMS algorithm follow essentially a similar path, and then the back-propagation
algorithm continues to improve, finally producing a prediction variance approaching the
prescribed value of 
4.19 In this experiment, we use a multilayer perceptron trained with the back-propagation algo-
rithm to perform one-step prediction on the Lorenz attractor. The dynamics of this attrac-
tor are defined by three equations:
 
dz(t)
dt
= x(t)y(t) - bz(t)
 
dy(t)
dt
= -x(t)z(t) + rx(t) - y(t)
 
dx(t)
dt
= -x(t) + y(t)
x
2.

2

2
x
2

Problems
229
where , r, and b are dimensionless parameters. Typical values for these parameters are 
  10, b  , and r  28.
The specifications of the multilayer perceptron are as follows:
Number of source nodes:20
Number of hidden neurons:200
Number of output neurons:1
The particulars of the data sets are as follows:
Training sample:700 data points
Testing sample:800 data points
Number of epochs used for training:50
The parameters of the back-propagation algorithm are as follows:
The learning-rate parameter  is annealed linearly from 10–1 down to 10–5.
Momentum:  0
(a) Compute the learning curve of the MLP, plotting the mean-square error versus the
number of epochs used to do the training.
(b) Compute the one-step prediction to the Lorenz attractor; specifically, plot the results
obtained as a function of time, and compare the prediction against the evolution of the
Lorenz attractor.
8
3

230
ORGANIZATION OF THE CHAPTER
In this chapter, we study another approach to machine learning : a kernel method based
on clustering. After the introductory material presented in Section 5.1, the rest of the
chapter is organized as follows:
1. Section 5.2 deals with Cover’s theorem on the separability of patterns.This theorem
is illustrated by revisiting the XOR problem.
2. Section 5.3 discusses a solution of the interpolation problem that uses radial-basis
functions, setting the stage for the construction of radial-basis function (RBF) net-
works in Section 5.4; this latter section also includes practical considerations per-
taining to RBF networks.
3. The K-means algorithm, discussed in Section 5.5, provides a simple, yet highly popu-
lar, algorithm for clustering, which is well suited for training the hidden layer in an
unsupervised manner. Section 5.6 follows up on the K-means clustering algorithm
to describe a recursive implementation of least-squares estimation (discussed in
Chapter 2) for training the output layer of the RBF network in a supervised manner.
Section 5.7 addresses practical considerations pertaining to this two-stage procedure
for designing RBF networks.This procedure is illustrated in the computer experiment
presented in Section 5.8, where comparisons are made with the results of the same
computer experiment performed in Chapter 4 using the back-propagation algorithm.
4. Section 5.9 examines interpretations of Gaussian hidden units, followed by Section 5.10
on the relationship between kernel regression in statistics and RBF networks.
The chapter concludes with a summary and discussion in Section 5.11.
5.1
INTRODUCTION
The supervised training of a neural network may be approached in several different
ways.The back-propagation learning algorithm for multilayer perceptrons, described in
Chapter 4, may be viewed as the application of a recursive technique known in statis-
tics as stochastic approximation.
In this chapter, we take a completely different approach. Specifically, we solve the
problem of classifying nonlinearly separable patterns by proceeding in a hybrid manner,
involving two stages:
C H A P T E R  5
Kernel Methods and Radial-
Basis Function Networks

• The first stage transforms a given set of nonlinearly separable patterns into a new
set for which, under certain conditions, the likelihood of the transformed patterns
becoming linearly separable is high; the mathematical justification of this trans-
formation is traced to an early paper by Cover (1965).
• The second stage completes the solution to the prescribed classification problem
by using least-squares estimation that was discussed in Chapter 2.
Through a discussion of the interpolation problem, we first describe an imple-
mentation of this hybrid approach to pattern classification by using a radial-basis func-
tion (RBF) network,1 the structure of which consists of only three layers:
• The input layer is made up of source nodes (sensory units) that connect the net-
work to its environment.
• The second layer, consisting of hidden units, applies a nonlinear transformation
from the input space to the hidden (feature) space. For most applications, the
dimensionality of the only hidden layer of the network is high; this layer is trained
in an unsupervised manner using stage 1 of the hybrid learning procedure.
• The output layer is linear, designed to supply the response of the network to the
activation pattern applied to the input layer; this layer is trained in a supervised
manner using stage 2 of the hybrid procedure.
The nonlinear transformation from the input space to the hidden space and the high
dimensionality of the hidden space satisfy the only two conditions of Cover’s theorem.
Much of the theory developed on RBF networks builds on the Gaussian function,
an important member of the class of radial-basis functions.The Gaussian function may
also be viewed as a kernel—hence the designation of the two-stage procedure based on
the Gaussian function as a kernel method.
Speaking of kernels, in the latter part of the chapter, we also discuss the relation-
ship between kernel regression in statistics and radial-basis function networks.
5.2
COVER’S THEOREM ON THE SEPARABILITY OF PATTERNS
When a radial-basis function (RBF) network is used to perform a complex pattern-
classification task, the problem is basically solved by first transforming it into a high-
dimensional space in a nonlinear manner and then separating the classes in the output
layer. The underlying justification is found in Cover’s theorem on the separability of
patterns, which, in qualitative terms, may be stated as follows (Cover, 1965):
A complex pattern-classification problem, cast in a high-dimensional space nonlinearly,
is more likely to be linearly separable than in a low-dimensional space, provided that the
space is not densely populated.
From the work we did on single-layer structures in Chapter 1 through 3, we know that
once we have linearly separable patterns, the classification problem is easy to solve.
Accordingly, we may develop a great deal of insight into the operation of an RBF net-
work as a pattern classifier by studying the critical issue of separability of patterns.
Section 5.2
Cover’s Theorem on the Separability of Patterns
231

Consider a family of surfaces where each naturally divides an input space into two
regions. Let 
denote a set of N patterns (vectors) x1, x2,..., xN, each of which is assigned
to one of two classes 
and 
. This dichotomy (binary partition) of the points is said
to be separable with respect to the family of surfaces if a surface exists in the family that
separates the points in the class 
from those in the class 
. For each pattern 
,
x  x
x2
x1
x2
x1
x
232
Chapter 5
Kernel Methods and Radial-Basis Function Networks
define a vector made up of a set of real-valued functions {
i(x)|i  1, 2,..., m1}, as
shown by
(5.1)
Suppose that the pattern x is a vector in an m0-dimensional input space.The vector (x)
then maps the points in the m0-dimensional input space into corresponding points in a new
space of dimension m1.We refer to 
i(x) as a hidden function,because it plays a role sim-
ilar to that of a hidden unit in a feedforward neural network. Correspondingly, the space
spanned by the set of hidden functions 
is referred to as the feature space.
{i(x)}m1
i=1

(x) = [1(x), 2(x), p , m1(x)]T

A dichotomy
of 
is said to be separable if there exists an m1-dimensional
vector w such that we may write the following (Cover, 1965):
(5.2)
The hyperplane defined by the equation
describes the separating surface in the -space (i.e., feature space). The inverse image
of this hyperplane, that is,
(5.3)
defines the separating surface (i.e., decision boundary)in the input space.
Consider a natural class of mappings obtained by using a linear combination of
r-wise products of the pattern vector coordinates.The separating surfaces correspond-
ing to such mappings are referred to as rth-order rational varieties. A rational variety of
order r in a space of dimension m0 is described by an rth-degree homogeneous equation
in the coordinates of the input vector x, as shown by
(5.4)
where xi is the ith component of input vector x and x0 is set equal to unity in order
to express the equation in a homogeneous form. An rth-order product of entries xi of
x—that is,
—is called a monomial. For an input space of dimensionality m0,
there are
monomials in Eq.(5.4).Examples of the type of separating surfaces described by Eq.(5.4)
are hyperplanes (first-order rational varieties), quadrices (second-order rational vari-
eties), and hyperspheres (quadrics with certain linear constraints on the coefficients).
m0!
(m0 - r)! r!
xi1xi2 p xir
a
0i1i2pirm0
ai1i2pirxi1xi2 p xir = 0
x: wT(x) = 0
wT(x) = 0
wT(x) 6 0,  x  x2
wT(x) 7 0,  x  x1

x
{x1, x2}

These examples are illustrated in Fig. 5.1 for a configuration of five points in a two-
dimensional input space. In general, linear separability implies spherical separability,
which implies quadric separability; however, the converses are not necessarily true.
In a probabilistic experiment, the separability of a set of patterns becomes a ran-
dom event that depends on the dichotomy chosen and the distribution of the patterns
in the input space. Suppose that the activation patterns x1, x2, ..., xN are chosen inde-
pendently, according to a probability measure imposed on the input space. Suppose also
that all the possible dichotomies of
are equiprobable. Let P(N, m1) denote
x = {xi}N
i=1
Section 5.2
Cover’s Theorem on the Separability of Patterns
233








(a)
(b)
(c)
FIGURE 5.1
Three examples 
of -separable dichotomies of
different sets of five points in 
two dimensions: (a) linearly
separable dichotomy;
(b) spherically separable
dichotomy; (c) quadrically
separable dichotomy.

the probability that a particular dichotomy picked at random is  separable, where the
class of separating surfaces chosen has m1 degrees of freedom. Following Cover (1965),
we may then state that
1
(5.5)
where the binomial coefficients composing N -1 and m are themselves defined for all
integers l and m by
For a graphical interpretation of Eq. (5.5), it is best to normalize the equation by
setting N  m1 and plotting the probability P(m1, m1) versus  for various values of
m1.This plot reveals two interesting characteristics (Nilsson, 1965):
• a pronounced threshold effect around   2;
• the value P(2m1, m1)  1/2 for each value of m1.
Equation (5.5) embodies the essence of Cover’s separability theorem for random pat-
terns.2 It is a statement of the fact that the cumulative binomial distribution corresponding
to the probability that (N - 1) flips of a fair coin will result in (m1 - 1) or fewer heads.
a l
m b =
l!
(l - m)!m!
for N  m1 - 1
P(N, m1) =
(21-N) a
m1-1
m=0
aN - 1
m
b  for N 7 m1 - 1
•

Although the hidden-unit surfaces envisioned in the derivation of Eq. (5.5) are
of a polynomial form and therefore different from those commonly used in radial-
basis-function networks, the essential content of the equation has general applicability.
Specifically, the higher we make the dimension m1 of the hidden space, the closer the
probability P(N, m1) will be to unity.To sum up, Cover’s theorem on the separability of
patterns encompasses two basic ingredients:
1. nonlinear formulation of the hidden function defined by i(x), where x is the input vector
and i  1, 2, ..., m1;
2. high dimensionality of the hidden (feature) space compared with the input space, where the
dimensionality of the hidden space is determined by the value assigned to m1 (i.e., the num-
ber of hidden units).
In general, as stated previously, a complex pattern-classification problem cast in high-
dimensional space nonlinearly is more likely to be linearly separable than in a low-
dimensional space. We emphasize, however, that in some cases the use of nonlinear
mapping (i.e., point 1) may be sufficient to produce linear separability without having
to increase the dimensionality of the hidden-unit space, as illustrated in the following
example.
EXAMPLE 1
The XOR Problem
To illustrate the significance of the idea of  separability of patterns, consider the simple, yet
important, XOR problem. In the XOR problem, there are four points (patterns)—(1, 1), (0, 1),
(0, 0), and (1, 0)—in a two-dimensional input space, as depicted in Fig. 5.2a. The requirement is
to construct a pattern classifier that produces the binary output 0 in response to the input pat-
tern (1, 1), or (0, 0), and the binary output 1 in response to the input pattern (0, 1) or (1, 0).Thus,
points that are closest in the input space, in terms of the Hamming distance, map to regions that
are maximally apart in the output space. The Hamming distance of a sequence as defined is the
number of changes from symbol 1 to symbol 0, and vice versa, that are found in a binary sequence.
Thus, the Hamming distance of both 11 and 00 is zero, whereas the Hamming distance of both
01 and 10 is one.
Define a pair of Gaussian hidden functions as follows:
We may then construct the results summarized in Table 5.1 for the four different input patterns
of interest.The input patterns are mapped onto the (1, 2) plane as shown in Fig. 5.2b. Here, we
now see that the input patterns (0, 1) and (1, 0) are linearly separable from the remaining input
patterns (1, 1) and (0, 0).Thereafter, the XOR problem may be readily solved by using the func-
tions 1(x) and 2(x) as the inputs to a linear classifier such as the perceptron.
■
In this example, there is no increase in the dimensionality of the hidden space
compared with the input space. In other words, nonlinearity exemplified by the use of
Gaussian hidden functions is sufficient to transform the XOR problem into a linearly
separable one.
2(x) = exp(- 7x - t27 2),  t2 = [0, 0]T
1(x) = exp(- 7x - t17 2),  t1 = [1, 1]T
234
Chapter 5
Kernel Methods and Radial-Basis Function Networks

Separating Capacity of a Surface
Equation (5.5) has an important bearing on the expected maximum number of ran-
domly assigned patterns that are linearly separable in a multidimensional space. To
explore this issue, let x1, x2,..., xN be a sequence of random patterns (vectors) as previ-
ously described. Let N be a random variable defined as the largest integer such that this
sequence is  separable, where  has m1 degrees of freedom. Then, from Eq. (5.5), we
deduce that the probability that N  n is given by
(5.6)
 = a 1
2 b
n
a n - 1
m1 - 1 b , n = 0, 1, 2, p
 Prob(N = n) = P(n, m1) - P(n + 1, m1)
Section 5.2
Cover’s Theorem on the Separability of Patterns
235
0.2
0
0.2
0.4
0.6
0.8
1.0
1.2
0.4
0.6
0.8
1.0
(1, 1)
Decision
             boundary
(0, 1)
(1, 0)
(0, 0)
(b)
(0, 1)
(1, 1)
(0, 0)
(a)
(1, 0)
w1
w2
FIGURE 5.2
(a) The four patterns of
the XOR problem; (b) decision-making
diagram.
TABLE 5.1
Specification of the Hidden Functions for the XOR 
Problem of Example 1
Input Pattern 
x
First Hidden Function 
1(x)
Second Hidden Function 
2(x)
(1,1)
1
0.1353
(0,1)
0.3678
0.3678
(0,0)
0.1353
1
(1,0)
0.3678
0.3678

For an interpretation of this result, recall the definition of a negative binomial dis-
tribution. This distribution equals the probability that k failures precede the rth success
in a long, repeated sequence of Bernoulli trials. In such a probabilistic experiment, there
are only two possible outcomes for each trial—success or failure—and their probabili-
ties remain the same throughout the experiment. Let p and q denote the probabilities
of success and failure, respectively, with p  q  1. The negative binomial distribution
is defined by the following (Feller, 1968):
For the special case of p  q  (i.e.,success and failure are equiprobable) and k  r  n,
the negative binomial distribution reduces to
With this definition, we now see that the result described in Eq. (5.6) is just the nega-
tive binomial distribution, shifted by m1 units to the right, and with parameters m1 and
.Thus, N corresponds to the “waiting time” for the m1-th failure in a sequence of tosses
of a fair coin.The expectation of the random variable N and its median are, respectively,
(5.7)
and
(5.8)
We therefore have a corollary to Cover’s theorem in the form of a celebrated
asymptotic result that may be stated as follows (Cover, 1965):
The expected maximum number of randomly assigned patterns (vectors) that are linearly
separable in a space of dimensionality m1 is equal to 2m1.
This result suggests that 2m1 is a natural definition of the separating capacity of a family of
decision surfaces having m1 degrees of freedom. In a loose-sense,it can be argued that Cover’s
separating capacity is related to the VC-dimension, discussed previously in Chapter 4.
5.3
THE INTERPOLATION PROBLEM
The important point that emerges from Cover’s theorem on the separability of patterns
is that in solving a nonlinearly separable pattern-classification problem, there is usually
practical benefit to be gained by mapping the input space into a new space of high
enough dimension. Basically, a nonlinear mapping is used to transform a nonlinearly
separable classification problem into a linearly separable one with high probability. In
a similar way,we may use a nonlinear mapping to transform a difficult nonlinear filtering
problem into an easier one that involves linear filtering.
Consider then a feedforward network with an input layer,a single hidden layer,and
an output layer consisting of a single unit. We have purposely chosen a single output
unit to simplify the exposition without loss of generality. The network is designed to
perform a nonlinear mapping from the input space to the hidden space, followed by a
linear mapping from the hidden space to the output space. Let mo denote the dimension
median[N] = 2m1
[N] = 2m1
1
2
f ak; n - k, 1
2 b = a 1
2 b
n
an - 1
k
b , n = 0, 1, 2, p
1
2
f(k; r, p) = prqk a r + k - 1
k
b
236
Chapter 5
Kernel Methods and Radial-Basis Function Networks

of the input space.Then, in an overall fashion, the network represents a map from the
mo-dimensional input space to the single-dimensional output space, written as
(5.9)
We may think of the map s as a hypersurface (graph) 
, just as we think of the
 ( m0+1
s: m0 S 1
Section 5.3
The Interpolation Problem
237
For strict interpolation as specified here, the interpolating surface (i.e., function F) is
constrained to pass through all the training data points.
The radial-basis-functions (RBF) technique consists of choosing a function F that
has the form
(5.11)
where 
is a set of N arbitrary (generally nonlinear) func-
tions, known as radial-basis functions, and 
denotes a norm that is usually Euclidean
7 7
{(@ @x - xi@ @)@i = 1, 2, ..., N}
F(x) = a
N
i=1
wi(@ @x - xi@ @)
elementary map 
, where s(x)  x2, as a parabola drawn in 
space.The sur-
face  is a multidimensional plot of the output as a function of the input. In a practical
situation, the surface  is unknown and the training data are usually contaminated with
noise.The training phase and generalization phase of the learning process may be respec-
tively viewed as follows (Broomhead and Lowe, 1988):
• The training phase constitutes the optimization of a fitting procedure for the sur-
face , based on known data points presented to the network in the form of
input–output examples (patterns).
• The generalization phase is synonymous with interpolation between the data points,
with the interpolation being performed along the constrained surface generated
by the fitting procedure as the optimum approximation to the true surface .
Thus, we are led to the theory of multivariable interpolation in high-dimensional space,
which has a long history (Davis, 1963).The interpolation problem, in its strict sense, may
be stated as follows:
Given a set of N different points
and a corresponding set of N
real numbers
, find a function 
that satisfies the interpo-
lation condition:
(5.10)
F(xi) = di,  i = 1, 2, ..., N
F: N S 1
{diH1i = 1, 2, ..., N}
{xi  m0i = 1, 2, ..., N}
2
s: 1 S 1
(Powell,1988).The known data points 
are taken to be the centers
of the radial-basis functions.
Inserting the interpolation conditions of Eq. (5.10) into Eq. (5.11), we obtain a set
of simultaneous linear equations for the unknown coefficients (weights) of the expan-
sion {wi} given by
(5.12)
≥
d1
d2
o
dN
¥
=
≥
w1
w2
o
wN
¥
≥
11
12
p
1N
21
22
p
2N
o
o
o
o
N1
N2
p
NN
¥
xi  mO, i = 1, 2, ..., N

where
Let
The N-by-1 vectors d and w represent the desired response vector and linear weight vector,
respectively, where N is the size of the training sample. Let 
denote an N-by-N matrix
with elements ij:
(5.14)
We call this matrix the interpolation matrix. We may then rewrite Eq. (5.12) in the com-
pact form
(5.15)
Assuming that 
is nonsingular, and therefore that the inverse matrix 
exists, we
may go on to solve Eq. (5.15) for the weight vector w, obtaining
(5.16)
The vital question is:How can we be sure that the interpolation matrix 
is nonsingular?
It turns out that for a large class of radial-basis functions and under certain condi-
tions, the answer to this basic question is given in an important theorem discussed next.
Micchelli’s Theorem
In Micchelli (1986), the following theorem is proved:
Let
be a set of distinct points in
. Then the N-by-N interpolation matrix
, whose
ij-th element is
, is nonsingular.
There is a large class of radial-basis functions that is covered by Micchelli’s theorem; it
includes the following functions that are of particular interest in the study of RBF networks:
1. Multiquadrics:
(5.17)
2. Inverse multiquadrics:
(5.18)
3. Gaussian functions:
(5.19)
The multiquadrics and inverse multiquadrics are both due to Hardy (1971).
(r) = exp a - r2
22 b
 
for some  7 0 and r  
(r) =
1
(r2 + c2)12  for some c 7 0 and r  
(r) = (r2 + c2)12  for some c 7 0 and r  
ij = (7xi - xj7)

m0
{xi}N
i=1

w = -1x
-1

w = x
 = {ij}i,j=1
N

 w = [w1, w2, ..., wN]T
 d = [d1, d2, ..., dN]T
ij = (@ @xj - xj@ @),    i, j = 1, 2, ..., N
238
Chapter 5
Kernel Methods and Radial-Basis Function Networks

Section 5.4
Radial-Basis-Function Networks
239
For the radial-basis functions listed in Eqs. (5.17) to (5.19) to be nonsingular, the
points 
must all be different (i.e., distinct). This is all that is required for nonsin-
gularity of the interpolation matrix 
, whatever the values of size N of the data points
or dimensionality mo of the vectors (points) xi happen to be.
The inverse multiquadrics of Eq. (5.18) and the Gaussian functions of Eq. (5.19)
share a common property: They are both localized functions, in the sense that 
as 
. In both of these cases, the interpolation matrix 
is positive definite. By
contrast, the multiquadrics of Eq. (5.17) are nonlocal, in that (r) becomes unbounded as


r S q
(r) S 0

{xi}i=1
N
, and the corresponding interpolation matrix 
has N1 negative eigenvalues
and only one positive eigenvalue, with the result that it is not positive definite (Mic-
chelli, 1986). What is remarkable, however, is that an interpolation matrix 
based on
Hardy’s multiquadrics is nonsingular and therefore suitable for use in the design of RBF
networks.
What is even more remarkable is that radial-basis functions that grow at infinity,
such as multiquadrics, can be used to approximate a smooth input–output mapping with
greater accuracy than those that yield a positive-definite interpolation matrix. This
surprising result is discussed in Powell (1988).
5.4
RADIAL-BASIS-FUNCTION NETWORKS
In light of Eqs. (5.10) through (5.16), we may now envision a radial-basis-function (RBF)
network in the form of a layered structure, as illustrated in Fig. 5.3; specifically, we have
three layers:
1. Input layer, which consists of mo source nodes, where mo is the dimensionality of
the input vector x.
2. Hidden layer, which consists of the same number of computation units as the size
of the training sample,namely,N;each unit is mathematically described by a radial-
basis function
The jth input data point xj defines the center of the radial-basis function, and the
vector x is the signal (pattern) applied to the input layer.Thus, unlike a multilayer
perceptron,the links connecting the source nodes to the hidden units are direct con-
nections with no weights.
3. Output layer, which,in the RBF structure of Fig.5.3,consists of a single computational
unit.Clearly,there is no restriction on the size of the output layer,except to say that
typically the size of the output layer is much smaller than that of the hidden layer.
Henceforth, we focus on the use of a Gaussian function as the radial-basis func-
tion,in which case each computational unit in the hidden layer of the network of Fig. 5.3
is defined by
(5.20)
 = exp a- 1
22
j
@ @x - xj@ @ 2 b ,  j = 1, 2, ..., N
 j(x) = (x - xj)
j(x) = (7x - xj7),  j = 1, 2, ...  , N


r S q

where j is a measure of the width of the jth Gaussian function with center xj.Typically,
but not always, all the Gaussian hidden units are assigned a common width . In situa-
tions of this kind, the parameter that distinguishes one hidden unit from another is the
center xj. The rationale behind the choice of the Gaussian function as the radial-basis
function in building RBF networks is that it has many desirable properties, which will
become evident as the discussion progresses.
Practical Modifications to the RBF Network
Formulation of the RBF network of Fig. 5.3 via interpolation theory is rather neat. In
practice, however, we find that the training sample 
is typically noisy, be that
in the context of pattern classification or nonlinear regression. Unfortunately, the use of
interpolation based on noisy data could lead to misleading results—hence the need for
a different approach to the design of an RBF network.
There is another practical issue that needs attention: Having a hidden layer of the
same size as the input layer could be wasteful of computational resources, particularly
when dealing with large training samples. When the hidden layer of the RBF network
is specified in the manner described in Eq. (5.20), we find that any correlation existing
between adjacent data points in the training sample is correspondingly transplanted
into adjacent units in the hidden layer. Stated in another way, there is redundancy of
neurons in the hidden layer when they are chosen in accordance with Eq. (5.20) by
{xi, di}N
i=1
240
Chapter 5
Kernel Methods and Radial-Basis Function Networks
Σ
x1
x2
xm0
Output
y  F(x)
Hidden layer
of size N
Output layer
of size one
Input layer
of size m0
wN()
center xN
w2()
center x2
w1()
center x1
w1
w2
wN
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Input
vector
x
FIGURE 5.3
Structure of an RBF network, based on interpolation theory.

virtue of the redundancy that may inherently exist in the training sample. In situations
of this kind, it is therefore good design practice to make the size of the hidden layer a
fraction of the size of the training sample, as illustrated in Fig. 5.4. Note that although
the RBF networks of Figs. 5.3 and 5.4 are indeed different, they do share a common fea-
ture: Unlike the case for a multilayer perceptron, the training of an RBF network does
not involve the back propagation of error signals.
Moreover, the approximating function realized by both of these two RBF struc-
tures has the same mathematical form,
(5.21)
where the dimensionality of the input vector x (and therefore that of the input layer) is
m0 and each hidden unit is characterized by the radial-basis function (x, xj), where

F(x) = a
K
j=1
wj(x, xj)
Section 5.4
Radial-Basis-Function Networks
241
FIGURE 5.4
Structure of a practical RBF network. Note that this network is similar in
structure to that of Fig. 5.3.The two networks are different, however, in that the size of the
hidden layer in Fig. 5.4 is smaller than that in Fig. 5.3.
Σ
x1
x2
xm0
Output
y  F(x)
Hidden layer
of size K  N
Output layer
of size one
Input layer
of size m0
wK()
center xK
w2()
center x2
w1()
center x1
w1
w2
wK
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Input
vector
x
j  1, 2, ..., K, with K being smaller than N. The output layer, assumed to consist of a sin-
gle unit, is characterized by the weight vector w, whose dimensionality is also K. The
structure of Fig. 5.3 differs from that of Fig. 5.4 in two respects:
1. In Fig. 5.3, the dimensionality of the hidden layer is N, where N is the size of the
training set, whereas that of Fig. 5.4 is K  N.

2. Assuming that the training sample 
is noiseless, the design of the hidden
layer in Fig. 5.3 is solved simply by using the input vector xj to define the center of
the radial-basis function (x, xj) for j  1, 2, ..., N. On the other hand, to design the
hidden layer in Fig. 5.4, we have to come up with a new procedure.
The material covered in the next section addresses this latter point in a practical way for
the case when the hidden units use Gaussian functions.
5.5
K-MEANS CLUSTERING
In designing the RBF network of Fig. 5.4, a key issue that needs to be addressed is how
to compute the parameters of the Gaussian units that constitute the hidden layer by
using unlabeled data. In other words, the computation is to be performed in an unsu-
pervised manner. In this section, we describe a solution to this problem that is rooted in
clustering, by which we mean the following:
Clustering is a form of unsupervised learning whereby a set of observations (i.e., data points)
is partitioned into natural groupings or clusters of patterns in such a way that the measure of
similarity between any pair of observations assigned to each cluster minimizes a specified
cost function.
There is a plethora of clustering techniques to choose from.We have chosen to focus on
the so-called K-means algorithm,3 because it is simple to implement, yet effective in per-
formance, two features that have made it highly popular.
Let 
denote a set of multidimensional observations that is to be partitioned
into a proposed set of K clusters, where K is smaller than the number of observations,
N. Let the relationship.
(5.22)
denote a many-to-one mapper, called the encoder, which assigns the ith observation xi
to the jth cluster according to a rule yet to be defined. (The alert reader could wonder
why it is we have chosen the index j to refer to a cluster when the logical choice would
have been k; the reason for this choice is that the symbol k is used to refer to a kernel
function that will be discussed later in the chapter.) To do this encoding,we need a measure
of similarity between every pair of vectors xi and 
, which is denoted by d(xi,
).When
the measure d(xi,
) is small enough, both xi and
are assigned to the same cluster;
otherwise, they are assigned to different clusters.
To optimize the clustering process,we introduce the following cost function (Hastie
et al., 2001):
(5.23)
For a prescribed K, the requirement is to find the encoder C(i) j for which the cost func-
tion J(C) is minimized. At this point in the discussion, we note that the encoder C is
unknown—hence the functional dependence of the cost function J on C.
J(C) = 1
2 a
K
j=1
a
C(i)=j
 a
C(i¿)=j
d(xi, xi¿)
xi¿
xi¿
xi¿
xi¿
j = C(i),  i = 1, 2, ..., N
{xi}N
i=1

{xi, di}i=1
N
242
Chapter 5
Kernel Methods and Radial-Basis Function Networks

In K-means clustering, the squared Euclidean norm is used to define the measure
of similarity between the observations xi and 
, as shown by
(5.24)
Hence, substituting Eq. (5.24) into Eq. (5.23), we get
(5.25)
We now make two points:
1. The squared Euclidean distance between the observations xi and 
is symmetric;
that is,
2. The inner summation in Eq. (5.25) reads as follows: For a given 
, the encoder C
assigns to cluster j all the observations 
that are closest to xi. Except for a scal-
ing factor, the sum of the observations 
so assigned is an estimate of the mean vec-
tor pertaining to cluster j; the scaling factor in question is 1/Nj, where Nj is the
number of data points within cluster j.
On account of these two points,we may therefore reduce Eq.(5.25) to the simplified form
(5.26)
where 
denotes the “estimated” mean vector associated with cluster j.4 In effect, the
mean 
may be viewed as the center of cluster j. In light of Eq. (5.26), we may now
restate the clustering problem as follows:
Given a set of N observations, find the encoder C that assigns these observations to the K
clusters in such a way that, within each cluster, the average measure of dissimilarity of the
assigned observations from the cluster mean is minimized.
Indeed, it is because of the essence of this statement that the clustering technique
described herein is commonly known as the K-means algorithm.
For an interpretation of the cost function J(C) defined in Eq. (5.26), we may say
that, except for a scaling factor 1/Nj, the inner summation in this equation is an estimate
of the variance of the observations associated with cluster j for a given encoder C, as
shown by
(5.27)
Accordingly, we may view the cost function J(C) as a measure of the total cluster vari-
ance resulting from the assignments of all the N observations to the K clusters that are
made by encoder C.
With encoder C being unknown, how do we minimize the cost function J(C)? To
address this key question, we use an iterative descent algorithm, each iteration of which
ˆ 2
j =
a
C(i)=j
 7xi - ˆ j7 2
ˆ j
ˆ j
J(C) = a
K
j=1 a
C(i)=j
 7xi - ˆ j7 2
xi¿
xi¿
xi¿
7xi - xi¿7 2 = 7xi¿ - xi7 2
xi¿
J(C) = 1
2 a
K
j=1
a
C(i)=j
 a
C(i)=j
7xi - xi7 2
d(xi, xi¿) = 7xi - xi¿7 2
xi¿
Section 5.5
K-Means Clustering
243

involves a two-step optimization. The first step uses the nearest neighbor rule to mini-
mize the cost function J(C) of Eq. (5.26) with respect to the mean vector 
for a given
encoder C. The second step minimizes the inner summation of Eq. (5.26) with respect
to the encoder C for a given mean vector 
. This two-step iterative procedure is con-
tinued until convergence is attained.
Thus, in mathematical terms, the K-means algorithm proceeds in two steps:5
Step 1. For a given encoder C, the total cluster variance is minimized with respect to the
assigned set of cluster means 
; that is, we perform, the following
minimization:
for a given C
(5.28)
Step 2. Having computed the optimized cluster means
in step 1, we next opti-
mize the encoder as follows:
(5.29)
Starting from some initial choice of the encoder C, the algorithm goes back and forth
between these two steps until there is no further change in the cluster assignments.
Each of these two steps is designed to reduce the cost function J(C) in its own
way; hence, convergence of the algorithm is assured. However, because the algorithm
lacks a global optimality criterion, the result may converge to a local minimum, result-
ing in a suboptimal solution to the clustering assignment.Nevertheless,the algorithm has
practical advantages:
1. The K-means algorithm is computationally efficient, in that its complexity is linear
in the number of clusters.
2. When the clusters are compactly distributed in data space, they are faithfully
recovered by the algorithm.
One last comment is in order: To initialize the K-means algorithm, the recom-
mended procedure is to start the algorithm with many different random choices for the
means
for the proposed size K and then choose the particular set for which the
double summation in Eq. (5.26) assumes the smallest value (Hastie et al., 2001).
The K-Means Algorithm Fits within the Framework of Cover’s Theorem
The K-means algorithm applies a nonlinear transformation to the input signal x. We say
so because the measure of dissimilarity—namely, the squared Euclidean distance
, on which it is based—is a nonlinear function of the input signal x for a given
cluster center xj. Furthermore, with each cluster discovered by the K-means algorithm
defining a particular computational unit in the hidden layer,it follows that if the number
of clusters, K, is large enough, the K-means algorithm will satisfy the other requirement
7 x - xj7 2
{ˆ j}K
j=i
C(i) = arg min
1jK
7 x(i) - ˆ j7 2
{ˆ j}K
j=1
min
{ˆ }K
j=1
a
K
j=i
a
C(i)=j
7 xi - ˆ j 7 2
{ˆ j}K
j=1
ˆ j
ˆ j
244
Chapter 5
Kernel Methods and Radial-Basis Function Networks

of Cover’s theorem—that is, that the dimensionality of the hidden layer is high enough.
We therefore conclude that the K-means algorithm is indeed computationally powerful
enough to transform a set of nonlinearly separable patterns into separable ones in accor-
dance with this theorem.
Now that this objective has been satisfied, we are ready to consider designing the
linear output layer of the RBF network.
5.6
RECURSIVE LEAST-SQUARES ESTIMATION OF THE WEIGHT VECTOR
The K-means algorithm performs computation in a recursive manner. It would therefore
be desirable to recast the method of least squares—discussed in Chapter 2—for computing
the weight vector in the output layer of the RBF network to perform this computation in
a recursive manner, too.With this objective in mind, we recast Eq. (2.33) in the form
(5.30)
where all three quantities are expressed as functions of discrete time n. In writing this
equation, called the normal equation in statistics, we have introduced three terms:
1. the K-by-K correlation function of the hidden-unit outputs, which is defined by
(5.31)
where
(5.32)
and
(5.33)
2. the K-by-1 cross-correlation vector between the desired response at the output of
the RBF network and the hidden-unit outputs, which is defined by
(5.34)
3. the unknown weight vector
, which is optimized in the least-squares sense.
The requirement is to solve the normal equation in Eq. (5.30) for the weight vec-
tor w(n).Of course,we could do this computation by first inverting the correlation matrix
R(n) and then multiplying the resulting inverse matrix R-1(n) by the cross-correlation
vector r(n), which is what is done in the method of least squares. However, when the size
of the hidden layer, K, is large, which is often the case, computation of the inverse matrix
R-1(n) for n  K can be a demanding task.The proposed use of a recursive implemen-
tation of the method of least squares takes care of this computational difficulty. The
wˆ (n)
r(n) = a
n
i=1
(xi)d(i)
(xi, j) = exp a- 1
22
j
7 xi - j7 2 b,
j = 1, 2, ..., K
(xi) = [(xi, 1), (xi, 2), ...,  (xi, K)]T
R(n) = a
n
i=1
(xi)T(xi)
R(n)wˆ (n) = r(n),
n = 1, 2, ...,
Section 5.6
Recursive Least-Squares Estimation of the Weight Vector
245

resulting algorithm is called the recursive least-squares (RLS) algorithm,6 the derivation
of which is discussed next.
The RLS algorithm
We begin the derivation of the RLS algorithm by reformulating Eq. (5.34) for the cross-
correlation vector r(n), as shown by
(5.35)
where, in the first line, we isolated the term corresponding to i  n from the summation
in Eq. (5.34), and in the last line we used Eq. (5.30), replacing n with n - 1. Next, we add
the term 
to the right-hand side of Eq. (5.35) in a purposeful way
and subtract the same term from it in another part of the equation, leaving the equation
itself unchanged; we thus write (after factoring common terms)
(5.36)
The expression inside the first set of brackets on the right-hand side of Eq. (5.36) is
recognized as the correlation function
(5.37)
For the expression inside the second set of brackets on the right-hand side of Eq. (5.36),
we introduce the new term
(5.38)
This new term is called the prior estimation error, where the use of “prior” is intended
to emphasize the fact that the estimation error (n) is based on the old estimate
of the weight vector—that is,“before” the weight estimate was updated.The 
wˆ(n - 1)
 = d(n) - wT(n - 1)	(n)
 (n) = d(n) - 	T(n)w(n - 1)
R(n) = R(n - 1) + 	(n)	T(n)
r(n) = [R(n - 1) + 	(n)	T(n)]wˆ (n - 1) + 	(n)[d(n) - 	T(n)wˆ (n - 1)]
	(n)	T(n)wˆ (n - 1)
 = R(n - 1)wˆ (n - 1) + (xn)d(n)
 = r(n - 1) + 	(xn)d(n)
 r(n) = a
n-1
i=1
	(xi)d(i) + 	(xn)d(n)
246
Chapter 5
Kernel Methods and Radial-Basis Function Networks
(n) is also referred to as the innovation, because the input vector x(n) embedded in
the 	(n) and the corresponding desired response d(n) represent “new” information
available to the algorithm for estimation at time n.
Returning to Eq. (5.36), we may now make use of Eqs. (5.37) and (5.38) to simplify
matters as follows:
(5.39)
Accordingly, the use of this equation in Eq. (5.30) yields
(5.40)
which may be expressed in the desired form for updating the weight vector, as shown by
(5.41)
wˆ (n) = wˆ (n - 1) + R-1(n)	(n)(n)
R(n)wˆ (n) = R(n)wˆ (n - 1) + 	(n)(n)
r(n) = R(n)wˆ (n - 1) + 	(n)(n)

where we have multiplied both sides of Eq. (5.40) by the inverse matrix 
(n). To do
the update in a computationally efficient manner,however,we need a corresponding for-
mula for computing the inverse matrix 
(n), given its past value 
(n - 1); this issue
is discussed next.
Recursive Formula for Computing 
(n)
Referring back to Eq. (5.37), we see that we do have a formula for recursively updating
the correlation matrix (n).We capitalize on this recursion for the purpose of a recur-
sive formula for the inverse matrix 
(n) by using the matrix inverse lemma,which was
discussed in Section 4.14.
To recap, consider the matrix
(5.42)
where it is assumed that matrix B is nonsingular and matrix B-1 therefore exists. Matri-
ces A and B are of similar dimensions, matrix D is another nonsingular matrix with dif-
ferent dimensions, and matrix C is a rectangular matrix of appropriate dimensions.
According to the matrix inversion lemma, we have
(5.43)
For the problem at hand, we use Eq. (5.37) to make the following identifications:
Accordingly, the application of Eq. (5.43) to this special set of matrices yields
(5.44)
where, in the second term on the right-hand side of the equation, we have made use of
the symmetry property of the correlation matrix; that is,
To simplify the formulation of the RLS algorithm, we now introduce two new
definitions:
1. R-1(n)  P(n)
Hence, we may reformulate Eq. (5.44) as
(5.45)
where the denominator on the right-hand side of the equation is a quadratic form and
therefore a scalar.
P(n) = P(n - 1) -
P(n - 1)(n)T(n)P(n - 1)
P(n - 1)(n)T(n)PT(n - 1)
RT(n - 1) = R(n - 1)
R-1(n) = R-1(n - 1) -
R-1(n - 1)(n)T(n)R-1T(n - 1)
1 + T(n)R-1(n - 1)(n)
D = 1
C = (n)
B-1 = R(n - 1)
A = R(n)
A-1 = B - BC(D + CTBC)-1 CTB
A = B-1 + CDCT
R-1
R
R-1
R-1
R-1
R-1
Section 5.6
Recursive Least-Squares Estimation of the Weight Vector
247

To provide an interpretation for P(n), consider the linear regression model
as the generative model for the desired response d(n), with 	(n) as the regressor. The
additive noise term (n) is assumed to be white with zero mean and variance 
.Then,
viewing the unknown weight vector w as the state of the model and 
as the estimate
produced by the RLS algorithm, we define the state-error covariance matrix as follows:
(5.46)
Verification of this result is addressed in Problem 5.5.
2.
(5.47)
The new term g(n) is called the gain vector of the RLS algorithm, because, in light
of Eq. (5.41), we may view the prior estimation error (n) multiplied by g(n) as the cor-
rection needed for updating the old estimate 
to its new value 
, as
shown by
(5.48)
Summary of the RLS Algorithm7
With Eqs. (5.45), (5.47), (5.38) and (5.48) at hand, in that order, we may now summarize
the RLS algorithm as follows:
Given the training sample 
, do the following computations for
n  1, 2, ..., N:
To initialize the algorithm, set
and
Note that the 
 used in the initialization of the algorithm performs the role of a reg-
ularizing parameter in the cost function
When 
 is chosen relatively small, which is typically the case, then, indirectly, we are
confirming confidence in the quality of the training sample 
.
{x(i), d(i)}i=1
N
eav(w) = 1
2 a
N
i=1
(d(i) - wT	(i))2 + 1
2
 
7w7 2
 P(0) = 
-1I,  
 is a small positive constant
 wˆ (0) = 0
 wˆ (n) = wˆ (n - 1) + g(n)(n)
 (n) = d(n) - wˆ T(n - 1)	(n)
 g(n) = P(n)	(n)
 P(n) = P(n - 1) -
P(n - 1)	(n)	T(n)P(n - 1)
1 + 	T(n)P(n - 1)	(n)
{	(i), d(i)}i=1
N
wˆ (n) = wˆ (n - 1) + g(n)(n)
wˆ (n)
wˆ (n - 1)
 = P(n)	(n)
 g(n) = R-1(n)	(n)
[(w - wˆ (n))(w - wˆ (n))T] = 2
P(n)
wˆ (n)
2


d(n) = wT	(n) + (n)
248
Chapter 5
Kernel Methods and Radial-Basis Function Networks

5.7
HYBRID LEARNING PROCEDURE FOR RBF NETWORKS
Equipped with the K-means clustering algorithm described in Section 5.5 and the recur-
sive least-squares (RLS) algorithm derived in Section 5.6, we are now ready to describe
a hybrid learning procedure8 for the RBF network of Fig. 5.4. The K-means algorithm
for training the hidden layer is applied first; it is then followed by the RLS algorithm for
training the output layer.Hereafter,we refer to this hybrid learning procedure as the “K-
means, RLS” algorithm, aimed at training an RBF network with the following compo-
sition:
Input layer. The size of the input layer is determined by the dimensionality of the input
vector x, which is denoted by m0.
Hidden layer.
1. The size of the hidden layer, m1, is determined by the proposed number of clusters,
K. Indeed, the parameter K may by viewed as a degree of freedom under the
designer’s control.As such, the parameter K holds the key to the model-selection
problem and thereby controls not only the performance, but also the computa-
tional complexity of the network.
2. The cluster mean 
, computed by the K-means algorithm working on the unla-
ˆ j
Section 5.7
Hybrid Learning Procedure for RBF Networks
249
tion (.,xj) assigned to the hidden unit j  1, 2, ..., K.
3. To simplify the design, the same width, denoted by , is assigned to all the Gaussian
functions in accordance with the spread of the centers discovered by the K-means
algorithm, as shown by
(5.49)
where K is the number of centers and dmax is the maximum distance between them
(Lowe, 1989).This formula ensures that the individual Gaussian units are not too
peaked or too flat; both extreme conditions should be avoided in practice.
Output layer. Once the training of the hidden layer is completed, the training of the out-
put layer can begin. Let the K-by-1 vector
denote the outputs of the K units in the hidden layer.This vector is produced in response
to the stimulus xi,i  1,2,...,N.Thus,insofar as the supervised training of the output layer
is concerned, the training sample is defined by {(xi), di}N
i 1, where di is the desired
response at the overall output of the RBF network for input xi. This training is carried
out using the RLS algorithm. Once the network training is completed, testing of the
whole network with data not seen before can begin.
(xi) = ≥
(xi, 1)
(xi, 2)
o
(xi, K)
¥
 =
dmax
22K
beled sample 
of input vectors,determines the center xj in the Gaussian func-
{xi}i=1
N

An attractive feature of the “K-means, RLS” algorithm is its computational effi-
ciency, which follows from the fact that the K-means and RLS algorithms are both com-
putationally efficient in their own individual ways.The only questionable feature of the
algorithm is the absence of an overall optimality criterion that combines the training of
the hidden and output layers, assuring the whole system of optimality in some statisti-
cal sense.
5.8
COMPUTER EXPERIMENT: PATTERN CLASSIFICATION
In this section, we use a computer experiment to evaluate the pattern-classification per-
formance of the “K-means, RLS” algorithm used to train an RBF network.The exper-
iment uses data obtained by randomly sampling the double-moon configuration of
Fig. 1.8. A specific aim of the experiment is to compare the performance of the RBF
network trained in this way with that of the multilayer perceptron (MLP) trained using
the back-propagation algorithm, which was the focus of attention in the experiment
performed in Section 4.7.
The hidden layer of the RBF network was chosen to consist of 20 Gaussian units,
thereby making it of the same size as the hidden layer of the MLP that was investigated
in Section 4.7. For training of the RBF network, 1,000 data points were used; and for
testing, 2,000 data points were used. In a manner similar to that for the MLP experi-
ments, the RBF experiments were performed for two different settings of the double-
moon figure, d  - 5 and d  - 6, with the latter setting being the more difficult one of
the two.
(a) Vertical separation: d  - 5
For this vertical separation between the two moons, K  20 was assigned as the num-
ber of clusters (i.e., number of hidden units). By applying the K-means algorithm to
the unlabeled part of the training sample, the centers of the clusters, and therefore the
centers of the Gaussian units in the hidden layer, were determined. With the spread
of the centers known, the formula of Eq. (5.49) was then used to compute the com-
mon width   2.6 assigned to all the Gaussian units. The design of the hidden layer
of the RBF network was thus completed. Finally, the RLS algorithm was used to train
the output layer and thereby compute the decision boundary, preparing the way for
the testing session.
The results of the first part of the experiment are presented in Fig. 5.5. Part (a) of
the figure shows the learning curve of the RLS algorithm and part (b) shows the deci-
sion boundary learned by the RBF network. As can be seen from Fig. 5.5a, within two
epochs of training, the design of the output layer is completed. Part (b) of the figure
confirms almost perfect separability of the two moon-shaped patterns by the RBF network.
(b) Vertical separation: d  - 6
The pattern-classification experiment on the RBF network was then repeated for this
more difficult setting of the double-moon configuration of Fig. 1.8. This time, the com-
mon width   2.4 was assigned to the 20 Gaussian units,the assignment again being per-
formed in accordance with the formula of Eq. (5.49).
250
Chapter 5
Kernel Methods and Radial-Basis Function Networks

Section 5.8
Computer Experiment: Pattern Classification
251
0.024
0.006
0.004
0.008
0.01
0.012
0.014
MSE
Learning curve
0.016
0.018
0.02
0.022
0
10
20
30
40
50
Number of epochs
(a) Learning curve
12
6
4
2
0
2
y
Classification using RBF with distance  5, radius  10, and width  6
4
6
8
10
10
5
0
5
10
15
20
x
(b) Testing result
FIGURE 5.5
RBF network trained with K-means and RLS algorithms for distance d  -5.
The MSE in part (a) of the figure stands for mean-square error.

The results of the second part of the experiment are presented in Fig. 5.6, with
part (a) of the figure showing the learning curve of the RLS algorithm, and part (b)
showing the decision boundary learned by the RBF network as a result of training under
the “K-means, RLS” algorithm.A total of ten classification errors out of 2,000 test data
points were recorded, yielding a classification error rate of 0.5%.
For both parts (a) and (b) of the experiment, the classification threshold was set
at zero at the single output of the RBF network.
Comparison of the MLP and RBF Results
Comparing the results of experiments (a) and (b) performed on the RBF network in this
section with those of the corresponding experiment performed on the MLP in Section 4.7,
we draw the following conclusions:
1. The RBF network trained with the “K-means, RLS” algorithm outperforms the
MLP trained with the back-propagation algorithm.Specifically,the MLP failed per-
fect classification for the setting d  - 5 of the double moon,whereas the RBF net-
work reported almost perfect classification. The misclassification rate of 0.5%
produced by the RBF network for the difficult setting d  - 6 was slightly greater
than the 0.15% produced by the MLP for the easier setting d  - 5. Surely, the
MLP design could have been improved;however,the same thing could also be said
for the RBF network.
2. The training process performed on the RBF network was significantly faster than
that performed on the MLP.
5.9
INTERPRETATIONS OF THE GAUSSIAN HIDDEN UNITS
The Idea of Receptive Field
In a neurobiological context, a receptive field is defined as “that region of a sensory field
from which an adequate sensory stimulus will elicit a response” (Churchland and
Sejnowski, 1992). What is interesting to realize is that the receptive fields of cells in
higher areas of the visual cortex tend to be much larger than those of cells in the earlier
stages of the visual system.
Following this neurobiological definition of a receptive field,we may envision each
hidden unit of a neural network to have a receptive field of its own. Indeed, we may go
on to make the following corresponding statement:
The receptive field of a computational unit (e.g., hidden unit) in a neural network is, in gen-
eral, that region of the sensory field (e.g., input layer of source nodes) from which an adequate
sensory stimulus (e.g., pattern) will elicit a response.
This definition applies equally well to multilayer perceptrons and RBF networks. How-
ever, the mathematical delineation of the receptive field in an RBF network is easier to
determine than that in a multilayer perceptron.
252
Chapter 5
Kernel Methods and Radial-Basis Function Networks

Section 5.9
Interpretations of the Gaussian Hidden Units
253
0.05
0.01
0.005
0.015
0.02
0.025
0.03
MSE
Learning curve
0.035
0.04
0.045
0
10
20
30
40
50
Number of epochs
(a) Learning curve
12
6
4
2
0
2
y
4
6
8
10
10
5
0
5
10
15
20
x
(b) Testing result
Classification using RBF with distance  6, radius  10, and width  6
FIGURE 5.6
RBF network trained with K-means and RLS algorithms for distanced d  -6.
The MSE in part (a) stands for mean-square error.

Let (x, xj) denote the functional dependence of a computational unit on the input
vector x, given that the unit is centered on the point xj.According to Xu et al. (1994), the
receptive field of this computational unit is defined by
(5.50)
where a is some positive constant. In words, this equation states that the receptive field
of the function (x, xj) is that particular subset of the domain of the input vector x for
which the function (x, xj) takes sufficiently large values, all of them being equal to or
greater than the prescribed level a.
EXAMPLE 2
Receptive field of a Gaussian hidden unit
Consider a Gaussian computational unit defined by
According to Eq. (5.50), the receptive field of this unit is
where a < 1.The minimum permissible value of 
is zero, for which this equation yields
It follows therefore that the receptive field of the Gaussian function 
is defined by a mul-
tidimensional surface, which is symmetrically centered around the point xj in a spheroidal man-
ner. The spheroidal symmetric property of the receptive field is inherited naturally from the
Gaussian function itself.
Figure 5.7 depicts two special cases of this surface:
1. One-dimensional receptive field,
, for which the domain of the input x is confined to the
(x)
(x,xj)
   2 log(  )
1
a
7 x - xj7 =
(x)
(x) = exp a-
1
22 7 x - xj7 2 b - a
(x, xj) = exp a-
1
2 2 7 x - xj7 2 b
(x) = (x, xj) - a
2. Two-dimensional receptive field,
, for which the domain of the input x is confined to a
(x)
254
Chapter 5
Kernel Methods and Radial-Basis Function Networks
x2
x1
xi
0
(b)
(a)
x
0
2˙2 log(1/a)
˙2 log(1/a)
FIGURE 5.7
Illustrating the notion of receptive field for two special 
cases: (a) one-dimensional, and (b) two-dimensional.
closed interval 
, as shown in part (a) of the
figure.
[(xi - 22 log(1a)), (xi + 22 log(1a))]
circular disc of center 
and radius 
, as shown in part (b) of the
figure.
■
22 log(1a)
xi = [xi,1, xi,2]T

Section 5.10
Kernel Regression and Its Relation to RBF Networks
255
Interpretation of the Gaussian Function as a Kernel
One other important facet of the Gaussian function (x, xj) is that it may be interpreted
as a kernel, a term that is widely used in the statistics literature; it is also being increas-
ingly used in the machine-learning literature.
Consider a function dependent on an input vector x, with its center located at the
origin of the Euclidean space. Basic to the formulation of this function as a kernel,
denoted by k(x), is that the function has properties similar to those associated with the
probability density function of a random variable:
Property 1. The kernel k(x) is a continuous, bounded, and real function of x and
symmetric about the origin, where it attains its maximum value.
Property 2. The total volume under the surface of the kernel k(x) is unity; that is,
for an m-dimensional vector x, we have
Except for a scaling factor, the Gaussian function (x, xj) satisfies both of these prop-
erties for the center xj located at the origin. For a nonzero value of xj, properties 1 and 2
still hold except for the fact that xj replaces the origin.
It is because of the interpretation of the Gaussian function as a kernel that we
have used the term “kernel methods” in the title of this chapter.
5.10
KERNEL REGRESSION AND ITS RELATION TO RBF NETWORKS
The theory of RBF networks, presented in Section 5.3, is built on the notion of inter-
polation. In this section, we take another viewpoint—namely, kernel regression, which
builds on the notion of density estimation.
To be specific, consider a nonlinear regression model defined by
(5.51)
where 
is an additive white-noise term of zero mean and variance 
To avoid con-
fusion, we have used the symbol yi (instead of di as previously) to denote the model out-
put.As a reasonable estimate of the unknown regression function f (x), we may take the
mean of observables (i.e., values of the model output y) near a point x. For this approach
to be successful, however, the local average should be confined to observations in a
small neighborhood (i.e., receptive field) around the point x because, in general, obser-
vations corresponding to points away from x will have different mean values. More pre-
cisely, we find that the unknown function f(x) is equal to the conditional mean of the
observable y given the regressor x, as shown by
(5.52)
 =
3
q
-q
ypYX(yx)dy
 f(x) = [yx]

2.
(i)
yi = f(xi) + (i),  i = 1, 2, ..., N
3mk(x)dx = 1

where pY|X(y|x) is the conditional probability density function (pdf) of the random vari-
able Y given that the random vector X is assigned the value x.9 From probability theory,
we have
(5.53)
where pX(x) is the pdf of X and px, y(x, y) is the joint pdf of X and Y. Hence, using
Eq. (5.53) in Eq. (5.52), we obtain the following formula for the regression function:
(5.54)
Our particular interest is in a situation where the joint probability density func-
tion pX,Y(x, y) is unknown and all that we have available is the training sample
. To estimate pX,Y(x,y), and therefore pX(x), we may use a nonparametric
estimator known as the Parzen–Rosenblatt density estimator (Rosenblatt, 1956, 1970;
Parzen, 1962). Basic to the formulation of this estimator is the availability of a kernel
k(x). Assuming that the observations x1, x2, ..., xN are statistically independent and
identically distributed (iid), we may formally define the Parzen–Rosenblatt density
estimate of fX(x) as
(5.55)
where the smoothing parameter h is a positive number called bandwidth, or simply
width; h controls the size of the kernel.An important property of the Parzen–Rosenblatt
density estimator is that it is a consistent estimator10 (i.e., asymptotically unbiased) in
the sense that if h  h(N) is chosen as a function of N such that
then
For this latter equation to hold, x should be a point of continuity for 
.
In a manner similar to that described in Eq. (5.55), we may formulate the Parzen–
Rosenblatt density estimate of the joint probability density function pX,Y(x, y) as
(5.56)
Integrating 
with respect to y, we get the 
of Eq. (5.55), as we should.
Moreover,
3
q
-q
ypˆ X, Y(x, y)dy =
1
Nhm0+1 a
N
i=1
k a x - xi
h
b
3
q
-q
yk a y - yi
h
bdy
pˆ X(x)
pˆ X, Y(x, y)
pˆ X, Y(x, y) =
1
Nhm0+1 a
N
i=1
k a x - xi
h
b k a y - yi
h
b for x  m0 and y  
pˆ X(x)
lim
NS q [pˆ X(x)] = pX(x)
lim
NS q h(N) = 0,
pˆ X(x) =
1
Nhm0 a
N
i=1
k a x - xi
h
b  for x  m0
{(xi, yi)}N
i=1
f(x) = 3
q
-q
ypX, Y(x, y)dy
pX(x)
pYX(yx) =
pXY(xy)
pX(x)
256
Chapter 5
Kernel Methods and Radial-Basis Function Networks

Changing the variable of integration by setting  (y 2 yi)/h, and using Property 2 of
the kernel k(.), we obtain the result
(5.57)
Thus, using Eqs. (5.57) and (5.55) as estimates of the quantities in the numerator and
denominator of Eq. (5.54), respectively, we obtain the following estimate of the regres-
sion function f(x), after canceling common terms:
(5.58)
Here, in the denominator, for clarity of presentation, we have used j instead of i as the
index of summation.
There are two ways in which the approximating function F(x) of Eq. (5.58) may be
viewed:
1. Nadaraya–Watson regression estimator. For the first viewpoint, define the
normalized weighting function
(5.59)
with
(5.60)
We may then rewrite the kernel regression estimator of Eq.(5.58) in the simplified form
(5.61)
which describes F(x) as a weighted average of the observables {yi}N
i1.The particu-
lar form of weighting function WN(x, i) given in Eq. (5.61) was independently pro-
posed by Nadaraya (1964) and Watson (1964).Accordingly,the approximating function
of Eq. (5.61) is often called the Nadaraya–Watson regression estimator (NWRE).11
2. Normalized RBF network. For the second viewpoint, we assume spherical sym-
metry of the kernel k(x), in which case we may set
(5.62)
k a x - xi
h
b = k a 7x - xi7
h
b  for all i
F(x) = a
N
i=1
WN, i(x)yi
a
N
i=1
WN, i(x) = 1  for all x
WN, i(x) =
k a x - xi
h
b
a
N
j=1
k a
x - xj
h
b
,  i = 1, 2, ..., N
 =
a
N
i=1
yik a x - xi
h
b
a
N
j=1
k a
x - xj
h
b
 F(x) = fˆ(x)
3
q
-q
ypˆ X, Y(x, y) dy =
1
Nhm0 a
N
i=1
yik a x - xi
h
b

Section 5.10
Kernel Regression and Its Relation to RBF Networks
257

where, as usual,
denotes the Euclidean norm of the enclosed vector (Krzyzak
et al., 1996). Correspondingly, we define the normalized radial basis function
(5.63)
with
(5.64)
The subscript N in %N(x, xi) signifies the use of normalization.
For the regression problem considered under the second viewpoint, we rec-
ognize that the “linear weights,” wi, applied to the basic functions %N(x, xi) are sim-
ply the observables, yi of the regression model for the input data xi.Thus, letting
we may reformulate the approximating function of Eq. (5.58) in the general
form
(5.65)
Equation (5.65) represents the input–output mapping of a normalized radial-
basis-function (RBF) network (Moody and Darken,1989;Xu et al.,1994).Note that
(5.66)
Accordingly,
may be interpreted as the probability of an event described
by the input vector x, conditional on xi.
The basic difference between the normalized radial-basis function 
of
Eq. (5.63) and an ordinary radial-basis function is a denominator term that constitutes
the normalization factor. This normalization factor is an estimate of the underlying prob-
ability density function of the input vector x. Consequently, the basis functions 
for i  1, 2,..., N sum to unity for all x, as described in Eq. (5.64).
Multivariate Gaussian Distribution
A variety of kernel functions is possible in general. However, both theoretical and prac-
tical considerations limit the choice.A widely used kernel is the multivariate Gaussian
distribution
(5.67)
where m0 is the dimension of the input vector x. The spherical symmetry of the kernel
k(x) is clearly apparent in Eq. (5.67).Assuming the use of a common bandwidth  that
k(x) =
1
(2)m02 exp a- 7x7 2
2
b
%N(x, xi)
%N(x, xi)
%N(x, xi)
0  %N(x, xi)  1  for all x and xi
F(x) = a
N
i=1
wi%N(x, xi)
yi = wi,  i = 1, 2, ..., N
a
N
i=1
%N(x, xi) = 1  for all x
%N(x, xi) =
k a 7x - xi7
h
b
a
N
j=1
k a
7x - xj7
h
b
,  i = 1, 2, ..., N
7 . 7
258
Chapter 5
Kernel Methods and Radial-Basis Function Networks

plays the role of smoothing parameter h for a Gaussian distribution, and centering the
kernel on a data point xi, we may write
(5.68)
Thus, using Eq. (5.68), we find that the Nadaraya–Watson regression estimator takes
the form
(5.69)
where the denominator term, representing the Parzen–Rosenblatt density estimator,
consists of the sum of N multivariate Gaussian distributions centered on the data points
x1, x2, ..., xN (Specht, 1991).
Correspondingly, using Eq. (5.68) in Eq. (5.63) and then Eq. (5.65), we find that the
input–output mapping function of the normalized RBF network takes the form
(5.70)
In Eqs. (5.69) and (5.70), the centers of the normalized radial-basis functions coin-
cide with the data points {xi}N
i  1.As with ordinary radial-basis functions, a smaller num-
ber of normalized radial-basis functions can be used, with their centers treated as free
parameters to be chosen according to some heuristic (Moody and Darken, 1989), or
determined in a principled manner as discussed in Chapter 7.
5.11
SUMMARY AND DISCUSSION
In this chapter, we focused on radial-basis-function (RBF) networks as an alternative to
multilayer perceptrons. Like the multilayer perceptron, discussed in Chapter 4, the RBF
network is a universal approximator in its own right (Sandberg and Xu, 1997a, 1997b).
The basic structural difference between them is summed up as follows:
In a multilayer perceptron,function approximation is defined by a nested set of weighted sum-
mations, whereas in an RBF network, the approximation is defined by a single weighted sum.
Design Considerations
The design of an RBF network may follow interpolation theory, which, in mathemati-
cal terms, is elegant. However, from a practical perspective, this design approach has
two shortcomings. First, the training sample may be noisy, which could yield misleading
results by the RBF network. Second, when the size of the training sample is large, using
F(x) =
a
N
i=1
wi exp a - 7x - xi7 2
22
b
a
N
j=1
exp a- 
7x - xj7 2
22
b
F(x) =
a
N
i=1
yi exp a- 7x - xi7 2
22
b
a
N
j=1
exp a- 
7x - xj7 2
22
b
k a x - xi
h
b =
1
(22)m02 exp a - 7x - xi7 2
22
b ,  i = 1, 2, ..., N
Section 5.11
Summary and Discussion
259

an RBF network with a hidden layer of the same size as the training sample is wasteful
of computational resources.
A more practical approach to the design of RBF networks is to follow the hybrid
learning procedure described in Section 5.5 through 5.7. Basically, the procedure oper-
ates in two stages:
• Stage 1 applies the K-means clustering algorithm to train the hidden layer in an
unsupervised manner.Typically, the number of clusters, and therefore the number
of computational units in the hidden layer, is significantly smaller than the size of
the training sample.
• Stage 2 applies the recursive least-squares (RLS) algorithm to compute the weight
vector of the linear output layer.
This two-stage design procedure has a couple of desirable features: computational sim-
plicity and accelerated convergence.
Experimental Results
The results of the computer experiment on the double-moon “toy” problem, presented
in Section 5.8, reveal that the hybrid “K-means, RLS” classifier is capable of delivering
an impressive performance. When the results of this experiment are compared with
those of the same experiment using the support vector machine (SVM), to be discussed
in the next chapter, we find that the two classifiers perform very similarly. However, the
“K-means, RLS” classifier is much faster to converge and less demanding in computa-
tional effort than the SVM.
It is noteworthy that Rifkin (2002), in his doctoral thesis, made a detailed compari-
son between the RLS and the SVM for the classification of linearly separable patterns,
using a collection of toy examples.Here is a summary of this part of his experimental results:
• The RLS and SVM classifiers exhibit nearly identical performance.
• They are both susceptible to the presence of outliers in the training sample.
Rifkin (2002) also performed experiments on image classification, using two dif-
ferent data sets:
• The U.S. Postal Service (USPS) handwritten data set, consisting of 7,291 training
examples and 2,007 testing examples.The training set contains 6,639 negative exam-
ples and 652 positive ones, while the testing set contains 1,807 negative examples
and 200 positive ones.
• The MIT recognition set, referred to as faces.The training set contains 2,429 faces
and 4,548 nonfaces, and the testing set contains 572 faces and 23,573 nonfaces.
For the USPS data set, it is reported that the nonlinear RLS classifier performed as well
as or better than the SVM across the entire range of the receiver operating characteris-
tic (ROC) curve. The ROC curve plots the true-positive rate against the false-positive
rate for a varying decision threshold when a single network output is used; the term
“rate” is another way of referring to the probability of classification.The tests performed
on the faces set produced mixed results: For one set of design parameters, the SVM
260
Chapter 5
Kernel Methods and Radial-Basis Function Networks

performed substantially better than the nonlinear RLS classifier. For another set of
design parameters, the performances were close.We should also point out that the strat-
egy used by Rifkin (2002) for designing the hidden layer of the nonlinear RLS classifier
was quite different from the K-means clustering algorithm considered in this chapter.
An important message, pertaining to our own double-moon “toy” experiments
and the more extensive experiments reported in Rifkin (2002), is twofold:
1. The RLS algorithm has been thoroughly studied in the signal-processing and con-
trol literature (Haykin, 2002; Goodwin and Sin, 1984). Unfortunately, it has been
almost completely ignored in the machine-learning literature, except for Rifkin’s
(2002) thesis and a few other publications.
2. There is a need for more extensive experiments, using real-world data sets, to come
up with more definitive conclusions on how an RBF network based on the RLS
algorithm (for the design of its output layer) and the SVM compare with each
other, not only in terms of performance, but also with respect to rate of conver-
gence, as well as computational complexity.
Kernel Regression
The one other important topic studied in this chapter is kernel regression, which builds
on the notion of density estimation. In particular, we focused on a nonparametric esti-
mator known as the Parzen–Rosenblatt density estimator, the formulation of which
rests on the availability of a kernel.This study led us to two ways of viewing the approx-
imating function defined in terms of a nonlinear regression model:the Nadaraya–Watson
regression estimator and the normalized RBF network. For both of them, the multi-
variate Gaussian distribution provides a good choice for the kernel.
NOTES AND REFERENCES
1. Radial-basis functions were first introduced in the solution of the real multivariate inter-
polation problem.The early work on this subject is surveyed in Powell (1985). It is now one
of the main fields of research in numerical analysis.
Broomhead and Lowe (1988) were the first to exploit the use of radial-basis functions
in the design of neural networks.Another major contribution to the theory and design of ra-
dial-basis function networks is due to Poggio and Girosi (1990a).This latter paper emphasizes
the use of regularization theory applied to this class of networks as a method for improved
generalization to new data; regularization theory is discussed in detail in Chapter 10.
2. The proof of Cover’s theorem follows from two basic considerations (Cover, 1965):
•
Schalfi’s theorem, or the function-counting theorem, which states that the number of
homogeneously linearly separable dichotomies of N vectors, located in general position
in Euclidean-m1 space, is equal to
A set of vectors 
is said to be in “general position” in Euclidean-m1 space if
every subset on m1 or fewer vectors is linearly independent.
•
Reflection invariance of the joint probability distribution of h, which implies that the
probability (conditional on h) that a random dichotomy is separable is equal to the
h = {xi}N
i=1
C(N, m1) = 2 a
m1-1
m=0
a N - 1
m
b
Notes and References
261

262
Chapter 5
Kernel Methods and Radial-Basis Function Networks
unconditional probability that a particular dichotomy of h (all N vectors in one category)
is separable.
The function-counting theorem has been independently proved in different forms and ap-
plied to specific configurations of perceptrons (i.e., linear threshold units) by Cameron
(1960), Joseph (1960), and Winder (1961). In Cover (1968), this theorem was applied to
evaluate the capacity of a network of perceptrons in terms of the total number of adjustable
parameters, which is shown to be lower bounded by N/(1  log2/N), where N is the num-
ber of input patterns.
3. Clustering,in general,is discussed in several books,including Theodoridis and Koutroumbas
(2003); Duda et al. (2001); and Fukunaga (1990).
The K-means algorithm assumed this name after MacQueen (1967), who studied it
as a statistical clustering procedure, including the convergence properties of the algorithm.
The idea had been previously described in Foregey (1962) in the context of clustering.
Ding and He (2004) present a very interesting relationship between the K-means
algorithm for clustering and principal-components analysis for data reduction. In particu-
lar, it is shown that principal components represent a continuous (relaxed) solution of the
cluster membership indicators in K-means clustering. In a way, these two views are consis-
tent, in the sense that clustering of data is also a form of data reduction, both of which are,
of course,performed in an unsupervised manner.The subject of principal-components analy-
sis is presented in Chapter 8.
In the communications literature dealing with vector quantization, the K-means
algorithm is referred to as the generalized Lloyd algorithm, which is a generalization of
Lloyd’s original treatment that appeared in an unpublished 1957 report at Bell Laborato-
ries. Much later, in 1982, Lloyd’s report appeared in published form.
4. Fisher’s Linear Discriminant The cost function defined in Eq. (5.26) is nothing but the trace
of the so-called within-class covariance (scatter) matrix (Theodoridis and Koutroumbas,2003).
To understand the meaning of this statement, consider a variable y defined by an
inner product as follows:
(A)
The vector x is drawn from one of two populations 
and 
, which differ from each other
c2
c1
y = wTx
by virtue of the mean vectors 1 and 2, respectively, and w is a vector of adjustable para-
meters.The Fisher criterion for discriminating between these two classes is defined by
(B)
Where Cb is the between-class covariance matrix, defined by
(C)
and Ct is the total within-class covariance matrix, defined by
(D)
The within-class covariance matrix Ct is proportional to the sample covariance matrix of the
training sample. It is symmetric and nonnegative definite and is usually nonsingular if the
size of the training sample is large.The between-class covariance matrix Cb is also symmetric
Ct = a
nHc1
(xn - 1)(xn - 1)T + a
nHc2
(xn - 2)(xn - 2)T
Cb = (2 - 1)(2 - 1)T
J(w) = wTCbw
wTCtw

Problems
263
and nonnegative definite, but singular. A property of particular interest is that the matrix
product Cbw is always in the direction of the difference mean vector 12.This property
follows directly from the definition of Cb.
The expression defining J(w) is known as the generalized Rayleigh quotient.The vec-
tor w that maximized J(w) must satisfy the condition
(E)
where is a scaling factor. Equation (E) is a generalized eigenvalue problem. Recognizing
that, in our case, the matrix product Cbw is always in the direction of the difference vector
12, we find that the solution for Eq. (E) is simply
(F)
which is referred to as Fisher’s linear discriminant (Duda et al., 2001).
Taking the trace of the within-class covariance matrix Ct of Eq. (D), we do find that
the cost function of Eq. (5.26) is indeed the trace of this covariance matrix, as already
stated.
5. In philosophical terms, the two-step optimization procedure described in the text for the K-
means algorithm is similar to the two-step optimization involved in the EM algorithm,
where the first step is one of expectation,denoted by “E”,and the second step is one of max-
imization, denoted by “M”. The EM algorithm was originally developed in the context of
maximum-likelihood computation; it is described in Chapter 11.
6. In the literature, the acronym “RLS” is used as the abbreviation for the regularized least-
squares algorithm discussed in Chapter 2 as well as the recursive least-squares algorithm dis-
cussed in this chapter. From the context of the pertinent discussion, we are usually able to
discern which of these two algorithms the acroynm refers to.
7. The essence of the RLS algorithm summarized in Section 5.6, a classic, is described in the
books by Diniz (2002); Haykin (2002).
8. Hybrid learning procedures for RBF networks have been variously described in the liter-
ature,using different algorithms for the two stages of the procedure;see Moody and Darken
(1989) and Lippman (1989b).
9. The conditional mean estimator of Eq. (5.52) is also a minimum mean-square estimator; a
proof of this statement is presented in Note 7 of Chapter 14 under the umbrella of Bayes’s
estimation theory.
10. For a proof of the asymptotically unbiased property of the Parzen–Rosenblatt density esti-
mator, see Parzen (1962) and Cacoullos (1966).
11. The Nadaraya–Watson regression estimator has been the subject of extensive study in the
statistics literature.In a broader context,nonparametric functional estimation occupies a cen-
tral place in statistics; see Härdle (1990) and the collection of papers in Roussas (1991).
PROBLEMS
Cover’s Theorem
5.1
As suggested in Section 5.2, the best way to study Eq. (5.5) is to normalize it by setting N 

m1. Using this normalization, plot P(
m1, m1) versus 
 for N  1, 5, 15, and 25. Hence,
validate the two characteristics of Eq. (5.5) described in the section.
5.2
Identify the strong and weak points of Cover’s theorem as stated at the beginning of Sec-
tion 5.2.
w = Ct
-1(1 - 2)

Cbw = 
Ctw

5.3
The example given in Fig. 5.1b depicts a spherically separable dictomy. Assume that the
four data points outside the separating surface lie on a circle and that the only data point
inside lies at the center of the separating surface. Investigate how this sample of data points
is nonlinearly transformed, using
(a) the multiquadric
(b) the inverse multiquadric
K-means Clustering
5.4
Consider the following modification of the cost function defined in Eq. (5.26):
In this function, the weighting factor wij is defined as follows:
Show that the minimizing solution of this cost function is
How do you interpret the expressions in the numerator and denominator of this formula?
Contrast the conclusion from your two answers against that we have learned in the text in
the context of clustering.
Recursive Least-Squares Algorithm
5.5
In this problem, we address a statistical interpretation of the matrix P defined as the inverse
of the correlation matrix R.
(a) Using the linear regression model
show that the least-square optimized estimate of w is expressed as
wˆ = w + (T)-1T
di = wT	i + i,  i = 1, 2, ..., N
ˆ j =
a
N
i=1
wijxi
a
N
i=1
wij
,  j = 1, 2, ..., K
wij = e1
if the data point xi lies in cluster j
0
otherwise
J(j) = a
K
j=1 a
N
i=1
wij7 xi - j7 2
(x) =
1
(x2 + 1)12
(x) = (x2 + 1)12
264
Chapter 5
Kernel Methods and Radial-Basis Function Networks

where
and
Assume that the error 
is a sample of a white-noise process of variance 2.
(b) Hence, show that the covariance matrix
where
5.6
Starting with the regularized cost function
do the following:
(a) Show that the addition of the regularization term 
has no effect whatsoever on
the composition of the RLS algorithm, as summarized in Section 5.6.
(b) The only effect of introducing the regularization term is to modify the expression for
the correlation matrix of the input data into the form
where I is the identity matrix. Verify this new formulation of the correlation matrix
R(n), and justify the practical benefit gained by introducing regularization.
5.7
The least-mean-squares (LMS) algorithm for adaptive filtering was discussed in Chapter 3.
Compare the advantages and disadvantages of the recursive least-squares (RLS) algorithm
with those of the LMS algorithm.
Supervised Training of RBF Networks
5.8
The input–output relationship of a Gaussian-based RBF network is defined by
where j(n) is the center point of the jth Gaussian unit, the width (n) is common to all
the K units, and wj(n) is the linear weight assigned to the output of the jth unit; all these
y(i) = a
K
j=1
wj(n)exp a - 
1
22(n)
 7 x(i) - j(n)7 2 b,  i = 1, 2, ..., n
R(n) = a
n
i=1
	(i)	T(i) + 
I
1
2
7 w7 2
eav(w) = 1
2 a
N
i=1
(d(i) - wT	(i))2 + 1
2
 
7 w7 2
R = a
N
i=1
	i	i
T
 = 2P
 [(w -  wˆ )(w - wˆ )T] = 2R-1
i
 = [1, 2, ..., N]T
 = ≥
	T
1
	T
2
o
	T
N
¥
Problems
265

parameters are measured at time n.The cost function used to train the network is defined by
where
The cost function e is a convex function of the linear weights in the output layer, but non-
convex with respect to the centers and the width of the Gaussian units.
(a) Evaluate the partial derivatives of the cost function with respect to each of the network
parameters wj(n),
(n), and (n), for all i.
(b) Use the gradients obtained in part (a) to express the update formulas for all the network
parameters, assuming the learning-rate parameters w, µ, and  for the adjustable
parameters of the network, respectively.
(c) The gradient vector 
has an effect on the input data that is similar to clustering.
Justify this statement.
Kernel Estimation
5.9 Suppose that you are given a “noiseless”training sample 
,and that the requirement
is to design a network that generalizes to data samples that are corrupted by additive noise
and therefore not included in the training set. Let F(x) denote the approximating function
realized by such a network, which is chosen so that the expected squared error
is minimum, where 
is the probability density function of a noise distribution in the
input space 
. Show that the solution of this least-squares problem is given as follows
(Webb, 1994):
Compare this estimator with the Nadaraya–Watson regression estimator.
Computer Experiments
5.10 The purpose of this computer experiment is to investigate the clustering process performed
by the K-means algorithm. To provide insight into the experiment, we fix the number of
clusters at K  6, but vary the vertical separation between the two moons in Fig. 1.8. Specif-
ically, the requirement is to do the following, using an unlabeled training sample of 1,000
data points picked randomly from the two regions of the double-moon pictured in Fig. 1.8:
(a) Experimentally,determine the mean 
and variance 
for the sequence
ˆ 2
j, j = 1, 2, ..., 6,
ˆ j
F(x) =
a
N
i=1
f(xi)f(x - xi)
a
N
i=1
f(x - xi)
m0
f()
J(F) = 1
2 a
N
i=1 3m0[f(xi) - F(xi, )]2f()d
{f(xi)}i=1
N
0ej(n)
j
e(i) = d(i) - y(i)
e = 1
2 a
n
i=1
e2(i)
266
Chapter 5
Kernel Methods and Radial-Basis Function Networks
of eight uniformly spaced vertical separations starting at d  1 and reducing them by
one till the final separation d  - 6 is reached.
(b) In light of the results obtained in part (a), comment on how the mean 
of cluster j is
affected by reducing the separation d for j = 1, 2, and 3.
ˆ j

(c) Plot the variance 
versus the separation d for j = 1, 2, ..., 6.
ˆ j
2
Problems
267
(d) Compare the common 2 computed in accordance with the empirical formula of Eq.
(5.49) with the trends exhibited in the plots obtained in part (c).
5.11 The purpose of this second experiment is to compare the classification performance of two
hybrid learning algorithms:the “K-means,RLS”algorithm investigated in Section 5.8 and the
“K-means, LMS” algorithm investigated in this problem.
As in Section 5.8, assume the following specifications:
Number of hidden Gaussian units:20
Number of training samples:1,000 data points
Number of testing samples:2,000 data points
Let the learning-rate parameter of the LMS algorithm be annealed linearly from 0.6 down
to 0.01.
(a) Construct the decision boundary computed for the “K-means, LMS” algorithm for the
vertical separation between the two moons in Fig. 1.8 set at d = - 5.
(b) Repeat the experiment for d = - 6.
(c) Compare the classification results obtained using the “K-means, LMS” algorithm with
those of the “K-means, RLS” algorithm studied in Section 5.8.
(d) Discuss how, in general, the complexity of the “K-means, LMS” algorithm compares
with that of the “K-means, RLS” algorithm.

268
ORGANIZATION OF THE CHAPTER
This chapter is devoted to the study of support vector machines: a machine-learning
algorithm that is perhaps the most elegant of all kernel-learning methods. Following the
introductory section, Section 6.1, the rest of the chapter is organized as follows:
1. Section 6.2 discusses the construction of an optimal hyperplane for the simple case
of linearly separable patterns, which is followed by consideration of the more diffi-
cult case of nonseparable patterns in Section 6.3.
2. In Section 6.4, the idea of an inner-product kernel is introduced, thereby building
the framework for viewing the learning algorithm involved in the construction of a
support vector machine as a kernel method.In that section,we also introduce a widely
used notion known as the “kernel trick.”The design philosophy of a support vector
machine is summed up in Section 6.5, followed by a revisitation of the XOR prob-
lem in Section 6.6.The second part of the chapter concludes with a computer exper-
iment on pattern classification, presented in Section 6.7.
3. Section 6.8 introduces the concept of an -insensitive loss function, the use of which
in solving regression problems is discussed in Section 6.9.
4. Section 6.10 deals with the representer theorem, which provides insight into the for-
mulation of an approximating function in the context of Mercer’s kernels.
The chapter concludes with a summary and discussion in Section 6.11.
6.1
INTRODUCTION
In Chapter 4, we studied multilayer perceptrons trained with the back-propagation
algorithm. The desirable feature of this algorithm is its simplicity, but the algorithm
converges slowly and lacks optimality. In Chapter 5, we studied another class of feed-
forward networks known as radial-basis function networks, which we developed from
interpolation theory; we then described a suboptimal two-stage procedure for its design.
In this chapter, we study another category of feedforward networks known collectively
as support vector machines (SVMs).1
Basically,the support vector machine is a binary learning machine with some highly
elegant properties.To explain how the machine works, it is perhaps easiest to start with

C H A P T E R  6
Support Vector Machines

the case of separable patterns that arise in the context of pattern classification. In this
context, the main idea behind the machine may be summed up as follows:
Given a training sample, the support vector machine constructs a hyperplane as the decision
surface in such a way that the margin of separation between positive and negative examples
is maximized.
This basic idea is extended in a principled way to deal with the more difficult case of non-
linearly separable patterns.
A notion that is central to the development of the support vector learning algo-
rithm is the inner-product kernel between a “support vector” xi and a vector x drawn
from the input data space. Most importantly, the support vectors consist of a small sub-
set of data points extracted by the learning algorithm from the training sample itself.
Indeed, it is because of this central property that the learning algorithm, involved in the
construction of a support vector machine, is also referred to as a kernel method. How-
ever, unlike the suboptimal kernel method described in Chapter 5, the kernel method
basic to the design of a support vector machine is optimal, with the optimality being
rooted in convex optimization. However, this highly desirable feature of the machine is
achieved at the cost of increased computational complexity.
As with the design procedures discussed in Chapters 4 and 5, the support vector
machine can be used to solve both pattern-classification and nonlinear-regression prob-
lems. However, it is in solving difficult pattern-classification problems where support
vector machines have made their most significant impact.
6.2
OPTIMAL HYPERPLANE FOR LINEARLY SEPARABLE PATTERNS
Consider the training sample {(xi, di)}N
i1, where xi is the input pattern for the ith exam-
ple and di is the corresponding desired response (target output). To begin with, we
assume that the pattern (class) represented by the subset di  1 and the pattern rep-
resented by the subset di  1 are “linearly separable.” The equation of a decision
surface in the form of a hyperplane that does the separation is
(6.1)
where x is an input vector, w is an adjustable weight vector, and b is a bias.We may thus
write
(6.2)
The assumption of linearly separable patterns is made here to explain the basic idea
behind a support vector machine in a rather simple setting; this assumption will be
relaxed in Section 6.3.
For a given weight vector w and bias b, the separation between the hyperplane
defined in Eq. (6.1) and the closest data point is called the margin of separation, denoted
by &.The goal of a support vector machine is to find the particular hyperplane for which
the margin of separation, &, is maximized. Under this condition, the decision surface is
for di = - 1
wTxi + b 6 0
for di = + 1
wTxi + b   0
wTx + b = 0
Section 6.2
Optimal Hyperplane for Linearly Separable Patterns
269

referred to as the optimal hyperplane. Figure 6.1 illustrates the geometric construction
of an optimal hyperplane for a two-dimensional input space.
Let wo and bo denote the optimum values of the weight vector and bias, respec-
tively. Correspondingly, the optimal hyperplane, representing a multidimensional linear
decision surface in the input space, is defined by
(6.3)
which is a rewrite of Eq. (6.1).The discriminant function
(6.4)
gives an algebraic measure of the distance from x to the optimal hyperplane (Duda and
Hart, 1973). Perhaps the easiest way to see this is to express x as
where xp is the normal projection of x onto the optimal hyperplane and r is the desired
algebraic distance; r is positive if x is on the positive side of the optimal hyperplane and
negative if x is on the negative side. Since, by definition, g(xp)  0, it follows that
or, equivalently,
(6.5)
r =
g(x)
7wo7
g(x) = wT
ox + bo = r7wo7
x = xp + r wo
7wo7
g(x) = wT
ox + bo
wT
ox + bo = 0
270
Chapter 6
Support Vector Machines
x1
x2
x(s)
0
Optimal hyperplane
FIGURE 6.1
Illustration of the
idea of an optimal hyperplane
for linearly separable patterns:
The data points shaded in red
are support vectors.

In particular, the distance from the origin (i.e., x  0) to the optimal hyperplane is given 
by 
. If bo  0, the origin is on the positive side of the optimal hyperplane; if bo  0,
bo7 wo7
Section 6.2
Optimal Hyperplane for Linearly Separable Patterns
271
x1
bo
wo
0
x2
x
r
Optimal
hyperplane
FIGURE 6.2
Geometric
interpretation of algebraic
distances of points to the
optimal hyperplane for a 
two-dimensional case.
it is on the negative side. If bo  0, the optimal hyperplane passes through the origin.
A geometric interpretation of these algebraic results is given in Fig. 6.2.
The issue at hand is to find the parameters wo and bo for the optimal hyperplane,
given the training set 
. In light of the results portrayed in Fig. 6.2, we see
that the pair (wo, bo) must satisfy the following constraint:
(6.6)
Note that if Eq. (6.2) holds—that is, if the patterns are linearly separable—we can
always rescale wo and bo such that Eq. (6.6) holds; this scaling operation leaves Eq. (6.3)
unaffected.
The particular data points (xi, di) for which the first or second line of Eq. (6.6) is
satisfied with the equality sign are called support vectors—hence the name “support
vector machine.”All the remaining examples in the training sample are completely irrel-
evant. Because of their distinct property, the support vectors play a prominent role in
the operation of this class of learning machines. In conceptual terms, the support vectors
are those data points that lie closest to the optimal hyperplane and are therefore the most
difficult to classify. As such, they have a direct bearing on the optimum location of the
decision surface.
Consider a support vector x(s) for which d(s)  1.Then, by definition, we have
(6.7)
g(x(s)) = wT
ox(s) + bo = ( 1  for d(s) = ( 1
wT
oxi + bo  -1   for di = -1
wT
oxi + bo  1  
 for di = + 1
t = {(xi, di)}

From Eq. (6.5), the algebraic distance from the support vector x(s) to the optimal hyper-
plane is
(6.8)
where the plus sign indicates that x(s) lies on the positive side of the optimal hyperplane
and the minus sign indicates that x(s) lies on the negative side of the optimal hyperplane.
Let & denote the optimum value of the margin of separation between the two classes that
constitute the training sample .Then, from Eq. (6.8), it follows that
(6.9)
Equation (6.9) states the following:
Maximizing the margin of separation between binary classes is equivalent to minimizing the
Euclidean norm of the weight vector w.
In summary, the optimal hyperplane defined by Eq. (6.3) is unique in the sense
that the optimum weight vector wo provides the maximum possible separation between
positive and negative examples. This optimum condition is attained by minimizing the
Euclidean norm of the weight vector w.
Quadratic Optimization for Finding the Optimal Hyperplane
The support vector machine is cleverly formulated under the umbrella of convex
optimization2—hence the well-defined optimality of the machine. In basic terms, the
formulation proceeds along four major steps:
1. The problem of finding the optimal hyperplane starts with a statement of the prob-
lem in the primal weight space as a constrained-optimization problem.
2. The Lagrangian function of the problem is constructed.
3. The conditions for optimality of the machine are derived.
4. The stage is finally set for solving the optimization problem in the dual space of
Lagrange multipliers.
To proceed then, we first note that the training sample
is embodied in the two-line constraint of Eq. (6.6). It is instructive to combine the two
lines of this equation into the single line
(6.10)
di(wTxi + b)  1  for i = 1, 2, ..., N
t = {xi, di}i=1
N
 =
2
7wo7
 & = 2r
t
 = µ
1
7wo7  if d(s) = + 1
- 1
7wo7  if d(s) = - 1
 r =
g(x(s))
7wo7
272
Chapter 6
Support Vector Machines

With this form of the constraint at hand, we are now ready to formally state the
constrained-optimization problem as follows:
Given the training sample {(xi, di)}N
i1, find the optimum values of the weight vector w and bias
b such that they satisfy the constraints
and the weight vector w minimizes the cost function
The scaling factor is included here for convenience of presentation.This constrained-
optimization problem is called the primal problem. It is basically characterized as follows:
• The cost function )(w) is a convex function of w.
• The constraints are linear in w.
Accordingly, we may solve the constrained-optimization problem by using the method
of Lagrange multipliers (Bertsekas, 1995).
First, we construct the Lagrangian function
(6.11)
where the auxiliary nonnegative variables i are called Lagrange multipliers. The
solution to the constrained-optimization problem is determined by the saddle point
of the Lagrangian function J(w, b, ). A saddle point of a Lagrangian is a point
where the roots are real, but of opposite signs; such a singularity is always unstable.
The saddle point has to be minimized with respect to w and b; it also has to be
maximized with respect to . Thus, differentiating J(w, b, ) with respect to w and b
and setting the results equal to zero, we get the following two conditions of opti-
mality:
Application of optimality condition 1 to the Lagrangian function of Eq. (6.11) yields
the following (after the rearrangement of terms):
(6.12)
Application of optimality condition 2 to the Lagrangian function of Eq. (6.11) yields
(6.13)
a
N
i=1
idi = 0
w = a
N
i=1
idixi
Condition 2:  
0J(w, b, )
0b
= 0
Condition 1:  
0J(w, b, )
0w
= 0
J(w, b, ) = 1
2 wTw - a
N
i=1
i[di(wTxi + b) - 1]
1
2
£(w) = 1
2 wTw
di(wTxi + b)  1  for i = 1, 2, ..., N
Section 6.2
Optimal Hyperplane for Linearly Separable Patterns
273

The solution vector w is defined in terms of an expansion that involves the N training
examples. Note, however, that although this solution is unique by virtue of the convex-
ity of the Lagrangian, the same cannot be said about the Lagrange multipliers i.
It is also important to note that for all the constraints that are not satisfied as
equalities, the corresponding multiplier i must be zero. In other words, only those mul-
tipliers that exactly satisfy the condition
(6.14)
can assume nonzero values. This property is a statement of the Karush–Kuhn–Tucker
conditions3 (Fletcher, 1987; Bertsekas, 1995).
As noted earlier, the primal problem deals with a convex cost function and
linear constraints. Given such a constrained-optimization problem, it is possible to
construct another problem called the dual problem. This second problem has the
same optimal value as the primal problem, but with the Lagrange multipliers pro-
viding the optimal solution. In particular, we may state the following duality theorem
(Bertsekas, 1995):
(a) If the primal problem has an optimal solution, the dual problem also has an optimal solution,
and the corresponding optimal values are equal.
(b) In order for wo to be an optimal primal solution and o to be an optimal dual solution, it is
necessary and sufficient that wo is feasible for the primal problem, and
To postulate the dual problem for our primal problem, we first expand Eq. (6.11), term
by term, obtaining
(6.15)
The third term on the right-hand side of Eq.(6.15) is zero by virtue of the optimality con-
dition of Eq. (6.13). Furthermore, from Eq. (6.12), we have
Accordingly, setting the objective function J(w, b, )  Q(), we may reformulate
Eq. (6.15) as
(6.16)
where the i are all nonnegative. Note that we have changed the notation from J(w, b, )
to Q() so as to reflect the transformation from the primal optimization problem to its
dual.
We may now state the dual problem as follows:
Given the training sample t  {xi, di}N
i1, find the Lagrange multipliers {i}N
i1 that maximize
the objective function
Q() = a
N
i=1
i - 1
2 a
N
i=1 a
N
j=1
ijdidjxT
i xj
wTw = a
N
i=1
idiwTxi = a
N
i=1 a
N
j=1
ijdidjxT
i xj
J(w, b, ) = 1
2 wTw - a
N
i=1
idiwTxi - b a
N
i=1
idi + a
N
i=1
i
£(wo) = J(wo, bo, o) = min
w J(w, b, )
i[di(wTxi + b) - 1] = 0
274
Chapter 6
Support Vector Machines

subject to the constraints
Unlike the primal optimization problem based on the Lagrangian of Eq. (6.11), the dual
problem defined in Eq. (6.16) is cast entirely in terms of the training data. Moreover, the
function Q() to be maximized depends only on the input patterns in the form of a set
of dot products
Typically, the support vectors constitute a subset of the training sample, which
means that the solution vector is sparse.4 That is to say, constraint (2) of the dual prob-
lem is satisfied with the inequality sign for all the support vectors for which the ’s are
nonzero, and with the equality sign for all the other data points in the training sample,
for which the ’s are all zero. Accordingly, having determined the optimum Lagrange
multipliers, denoted by o,i, we may compute the optimum weight vector wo by using
Eq. (6.12) as
(6.17)
where Ns is the number of support vectors for which the Lagrange multipliers o,i are
all nonzero.To compute the optimum bias bo, we may use the wo thus obtained and take
advantage of Eq. (6.7), which pertains to a positive support vector:
(6.18)
Recall that the support vector x(s) corresponds to any point (xi, di) in the training sam-
ple for which the Lagrange multiplier o,i is nonzero. From a numerical (practical) per-
spective, it is better to average Eq. (6.18) over all the support vectors—that is, over all
the nonzero Lagrange multipliers.
Statistical Properties of the Optimal Hyperplane
In a support vector machine, a structure is imposed on the set of separating hyperplanes
by constraining the Euclidean norm of the weight vector w. Specifically, we may state
the following theorem (Vapnik, 1995, 1998):
Let D denote the diameter of the smallest ball containing all the input vectors x1, x2,
xN.
The set of optimal hyperplanes described by the equation
wT
ox + bo = 0
...,
= 1 - a
NS
i=1
o,idixT
i x(s)
bo = 1 - wT
ox(s)  for d(s) = 1
wo = a
NS
i=1
o,idixi
{xi
Txj}i, j=1
N
(2) i  0  for i = 1, 2, ..., N
(1) a
N
i=1
idi = 0
Q() = a
N
i=1
i - 1
2 a
N
i=1 a
N
j=1
ijdidjxT
i xj
Section 6.2
Optimal Hyperplane for Linearly Separable Patterns
275

has a VC dimension, h, bounded from above as
(6.19)
where the ceiling sign 
means the smallest integer greater than or equal to the number
<=
h  min e l D2
&2 m , m0 f + 1
276
Chapter 6
Support Vector Machines
enclosed within the sign, & is the margin of separation equal to 
, and m0 is the dimen-
sionality of the input space.
As mentioned previously in Chapter 4, the VC dimension, short for Vapnik–
Chervonenkis dimension, provides a measure of the complexity of a space of functions.
The theorem just stated tells us that we may exercise control over the VC dimension (i.e.,
complexity) of the optimal hyperplane, independently of the dimensionality m0 of the
input space, by properly choosing the margin of separation &.
Suppose, then, we have a nested structure made up of separating hyperplanes
described by
(6.20)
By virtue of the upper bound on the VC dimension h defined in Eq. (6.19), the nested
structure described in Eq. (6.20) may be reformulated in terms of the margin of sepa-
ration in the equivalent form
(6.21)
The ak and ck in Eqs. (6.20) and (6.21) are constants.
Equation (6.20) states that the optimal hyperplane is a hyperplane for which the mar-
gin of separation between the positive and negative examples is the largest possible.Equiv-
alently, Eq. (6.21) states that construction of the optimal hyperplane is realized by making
the squared Euclidean norm of the weight vector w the smallest possible. In a sense, these
two equations reinforce the statement we made previously in light of Eq. (6.9).
6.3
OPTIMAL HYPERPLANE FOR NONSEPARABLE PATTERNS
The discussion thus far has focused on linearly separable patterns. In this section, we
consider the more difficult case of nonseparable patterns. Given such a sample of train-
ing data,it is not possible to construct a separating hyperplane without encountering clas-
sification errors. Nevertheless, we would like to find an optimal hyperplane that
minimizes the probability of classification error, averaged over the training sample.
The margin of separation between classes is said to be soft if a data point (xi, di)
violates the following condition (see Eq. (6.10)):
This violation can arise in one of two ways:
• The data point (xi, di) falls inside the region of separation, but on the correct side
of the decision surface, as illustrated in Fig. 6.3a.
di(wTxi + b)  + 1,  i = 1, 2, ... , N
Sk = e l r2
&2 m + 1:  &2  ak f ,  k = 1, 2, ...
Sk = {wTx + b:  7w7 2  ck},  k = 1, 2, ...
27 wo7

• The data point (xi, di) falls on the wrong side of the decision surface, as illustrated
in Fig. 6.3b.
Note that we have correct classification in the first case, but misclassification in the
second.
To set the stage for a formal treatment of nonseparable data points, we introduce
a new set of nonnegative scalar variables,
into the definition of the separating
hyperplane (i.e., decision surface), as shown here:
(6.22)
The i are called slack variables; they measure the deviation of a data point from the ideal
condition of pattern separability. For 
the data point falls inside the region
of separation, but on the correct side of the decision surface, as illustrated in Fig. 6.3a.
For i  1, it falls on the wrong side of the separating hyperplane, as illustrated in
Fig. 6.3b.The support vectors are those particular data points that satisfy Eq. (6.22) pre-
cisely even if i  0. Moreover, there can be support vectors satisfying the condition 
i  0. Note that if an example with i  0 is left out of the training sample, the decision
surface will change.The support vectors are thus defined in exactly the same way for both
linearly separable and nonseparable cases.
Our goal is to find a separating hyperplane for which the misclassification error,
averaged over the training sample, is minimized.We may do this by minimizing the func-
tional
£() = a
N
i=1
I(i - 1)
0 6 i  1,
di(wTxi + b)  1 - i,  i = 1, 2, ..., N
{i}i=1
N ,
Section 6.3
Optimal Hyperplane for Nonseparable Patterns
277
FIGURE 6.3
Soft margin hyperplane (a) Data point xi (belonging to class 
, represented by
a small square) falls inside the region of separation, but on the correct side of the decision
surface. (b) Data point xi (belonging to class 
, represented by a small circle) falls on the
wrong side of the decision surface.
c2
c1
x1
x2
Optimal hyperplane
x1
x2
Optimal hyperplane

with respect to the weight vector w, subject to the constraint described in Eq. (6.22) and
the constraint on 
.The function I() is an indicator function, defined by
Unfortunately, minimization of )() with respect to w is a nonconvex optimization
problem that is NP complete.5
To make the optimization problem mathematically tractable, we approximate the
functional )() by writing
Moreover, we simplify the computation by formulating the functional to be minimized
with respect to the weight vector w as follows:
(6.23)
As before,minimizing the first term in Eq.(6.23) is related to the support vector machine.
As for the second term,
it is an upper bound on the number of test errors.
The parameter C controls the tradeoff between complexity of the machine and
the number of nonseparable points; it may therefore be viewed as the reciprocal of a
parameter commonly referred to as the “regularization” parameter.6 When the para-
meter C is assigned a large value, the implication is that the designer of the support
vector machine has high confidence in the quality of the training sample t. Conversely,
when C is assigned a small value, the training sample t is considered to be noisy, and less
emphasis should therefore be placed on it.
In any event, the parameter C has to be selected by the user. It may be determined
experimentally via the standard use of a training (validation) sample, which is a crude
form of resampling; the use of cross-validation for optimum selection of regularization
parameter (i.e., 1/C) is discussed in Chapter 7.
In any event,the functional )(w, ) is optimized with respect to w and {i}N
i1,subject
to the constraint described in Eq. (6.22), and i  0. In so doing, the squared norm of w is
treated as a quantity to be jointly minimized with respect to the nonseparable points rather
than as a constraint imposed on the minimization of the number of nonseparable points.
The optimization problem for nonseparable patterns just stated includes the opti-
mization problem for linearly separable patterns as a special case. Specifically, setting 
i  0 for all i in both Eqs. (6.22) and (6.23) reduces them to the corresponding forms
for the linearly separable case.
We may now formally state the primal problem for the nonseparable case as follows:
Given the training sample {(xi, di)}N
i1, find the optimum values of the weight vector w and bias
b such that they satisfy the constraint
(6.24)
(6.25)
i  0  for all i
di(wTxi + b)  1 - i  for i = 1, 2, ..., N
gi i,
£(w, ) = 1
2 wTw + C a
N
i=1
i
£() = a
N
i=1
i
I() = e 0
if   0
1
if  7 0
7w7 2
278
Chapter 6
Support Vector Machines

and such that the weight vector w and the slack variables i minimize the cost functional
(6.26)
where C is a user-specified positive parameter.
Using the method of Lagrange multipliers and proceeding in a manner similar to that
described in Section 6.2, we may formulate the dual problem for nonseparable patterns
as follows (see Problem 6.3):
Given the training sample {(xi, di)}N
i1, find the Lagrange multipliers {i}N
i1 that maximize
the objective function
(6.27)
subject to the constraints
where C is a user-specified positive parameter.
Note that neither the slack variables i nor their own Lagrange multipliers appear in the
dual problem. The dual problem for the case of nonseparable patterns is thus similar
to that for the simple case of linearly separable patterns, except for a minor, but impor-
tant, difference.The objective function Q() to be maximized is the same in both cases.
The nonseparable case differs from the separable case in that the constraint i  0 is
replaced with the more stringent constraint 
Except for this modification,
the constrained optimization for the nonseparable case and computations of the opti-
mum values of the weight vector w and bias b proceed in the same way as in the lin-
early separable case. Note also that the support vectors are defined in exactly the same
way as before.
Unbounded Support Vectors
For a prescribed parameter C, a data point (xi, di) for which the condition 0  i  C
holds is said to be an unbounded, or free support vector. When i  C, we find that
where F(xi) is the approximating function realized by the support vector machine for the
input xi. On the other hand, when i  0, we find that
In light of these two arguments, it follows that for unbounded support vectors, we have
diF(xi) = 1
diF(xi)  1,  i = 0
diF(xi)  1,  i = C
0  i  C.
(2) 0  i  C  for i = 1, 2, ..., N
(1) a
N
i=1
idi = 0
Q() = a
N
i=1
i - 1
2 a
N
i=1 a
N
j=1
ijdidjxT
i xj
£(w, ) = 1
2 wTw + C a
N
i=1
i
Section 6.3
Optimal Hyperplane for Nonseparable Patterns
279

Unfortunately, the converse argument does not hold; that is, even if we know that
diF(xi)  1 for a particular data point (xi, di), this condition does not necessarily tell us
anything about the corresponding Lagrange multiplier i.
Consequently, there is a distinct possibility of degeneracy (i.e., reduced optimality
conditions) in the solution to a pattern-classification problem computed by the support
vector machine. By this statement, we mean that a point (xi, di) that satisfies the margin
requirement exactly has no constraint on the possible value of the associated i.
In Rifkin (2002), it is argued that the number of unbounded support vectors is the
primary reason for how difficult, in a computational sense, the training of a support vec-
tor machine can be.
Underlying Philosophy of a Support Vector Machine 
for Pattern Classification
With the material on how to find the optimal hyperplane for nonseparable patterns
at hand, we are now in a position to formally describe the construction of a support
vector machine for a pattern-recognition task.
Basically, the idea of a support vector machine hinges on two mathematical oper-
ations summarized here and illustrated in Fig. 6.4:
1. nonlinear mapping of an input vector into a high-dimensional feature space that
is hidden from both the input and output;
280
Chapter 6
Support Vector Machines
w1(xi)
xi
w2(xi)
wm1(xi)
•
•
•
•
•
•
Input (data) space
Feature (hidden)
space
Output
space
wo
bo
Bias
1
This point
belongs to the
positive or
negative class
FIGURE 6.4
Illustrating the two mappings in a support vector machine for pattern classification:
(i) nonlinear mapping from the input space to the feature space; (ii) linear mapping from the
feature space to the output space.

2. construction of an optimal hyperplane for separating the features discovered in
step 1.
The rationale for each of these two operations is explained in the upcoming text.
One last important comment is in order.The number of features constituting the
hidden space in Fig. 6.4 is determined by the number of support vectors.Thus, SVM the-
ory provides an analytic approach for determining the optimum size of the feature
(hidden) space, thereby assuring optimality of the classification task.
6.4
THE SUPPORT VECTOR MACHINE VIEWED AS A KERNEL MACHINE
Inner-Product Kernel
Let x denote a vector drawn from the input space of dimension m0. Let {j(x)}*
j1 denote
a set of nonlinear functions that, between them, transform the input space of dimension
m0 to a feature space of infinite dimensionality.Given this transformation,we may define
a hyperplane acting as the decision surface in accordance with the formula
(6.28)
where {
j}*
j1 denotes an infinitely large set of weights that transforms the feature space
to the output space. It is in the output space where the decision is made on whether the
input vector x belongs to one of two possible classes, positive or negative. For conve-
nience of presentation, we have set the bias to zero in Eq. (6.28). Using matrix nota-
tion, we may rewrite this equation in the compact form
(6.29)
where 	(x) is the feature vector and w is the corresponding weight vector.
As in Section 6.3, we seek “linear separability of the transformed patterns” in the
feature space.With this objective in mind,we may adapt Eq.(6.17) to our present situation
by expressing the weight vector as
(6.30)
where the feature vector is expressed as
(6.31)
and Ns is the number of support vectors. Hence, substituting Eq. (6.29) into Eq. (6.30),
we may express the decision surface in the output space as
(6.32)
We now immediately see that the scalar term 	T(xi)	(x) in Eq. (6.32) represents an
inner product. Accordingly, let this inner-product term be denoted as the scalar
a
Ns
i=1
idi	T(xi)	(x) = 0
	(xi) = [1(xi), 2(xi), ...]T
w = a
NS
i=1
idi	(xi)
wT	(x) = 0
w
a
q
j=1
wjj(x) = 0
Section 6.4
The Support Vector Machine Viewed as a Kernel Machine
281

(6.33)
Correspondingly, we may express the optimal decision surface (hyperplane) in the out-
put space as
(6.34)
The function k(x, xi) is called the inner-product kernel,7 or simply the kernel, which is for-
mally defined as follows (Shawe-Taylor and Cristianini, 2004):
The kernel k(x, xi) is a function that computes the inner product of the images produced in
the feature space under the embedding 	 of two data points in the input space.
Following the definition of a kernel introduced in Chapter 5, we may state that
a
NS
i=1
idik(x, xi) = 0
= a
q
j=1
j(xi)j(x),  i = 1, 2, ..., Ns
k(x, xi) = 	T(xi)	(x)
282
Chapter 6
Support Vector Machines
the kernel k(x, xi) is a function that has two basic properties8:
Property 1. The function k(x, xi) is symmetric about the center point xi, that is,
and it attains its maximum value at the point x  xi.
Note, however, that the maximum need not exist; for example, k(x, xi)  xTxi, as a ker-
nel does not have a maximum.
Property 2. The total volume under the surface of the function k(x, xi) is a constant.
If the kernel k(x, xi) is appropriately scaled to make the constant under property 2 equal
to unity, then it will have properties similar to those of the probability density function
of a random variable.
The Kernel Trick
Examining Eq. (6.34), we may now make two important observations:
1. Insofar as pattern classification in the output space is concerned, specifying the
kernel k(x, xi) is sufficient; in other words, we need never explicitly compute the
weight vector wo; it is for this reason that the application of Eq. (6.33) is com-
monly referred to as the kernel trick.
2. Even though we assumed that the feature space could be of infinite dimensional-
ity, the linear equation of Eq. (6.34), defining the optimal hyperplane, consists of
a finite number of terms that is equal to the number of training patterns used in
the classifier.
It is in light of observation 1 that the support vector machine is also referred to as a
kernel machine. For pattern classification, the machine is parameterized by an
N-dimensional vector whose ith term is defined by the product idi for i = 1, 2, ..., N.
k(x, xi) = k(xi, x)  for all xi

We may view k(xi, xj) as the ij-th element of the symmetric N-by-N matrix
(6.35)
The matrix K is a nonnegative definite matrix called the kernel matrix; it is also referred
to simply as the Gram. It is nonnegative definite or positive semidefinite in that it satis-
fies the condition
for any real-valued vector a whose dimension is compatible with that of K.
Mercer’s Theorem
The expansion of Eq. (6.33) for the symmetric kernel k(x, xi) is an important special
case of Mercer’s theorem that arises in functional analysis.This theorem may be formally
stated as follows (Mercer, 1909; Courant and Hilbert, 1970):
Let k(x, x) be a continuous symmetric kernel that is defined in the closed interval
and likewise for x. The kernel k(x, x) can be expanded in the series
(6.36)
with positive coefficients 
i  0 for all i. For this expansion to be valid and for it to converge
absolutely and uniformly, it is necessary and sufficient that the condition
(6.37)
holds for all %(·), for which we have
(6.38)
where a and b are the constants of integretion.
The features i(x) are called eigenfunctions of the expansion, and the numbers 
i
are called eigenvalues. The fact that all of the eigenvalues are positive means that the
kernel k(x, x) is positive definite. This property, in turn, means that we have a com-
plex problem that can be solved efficiently for the weight vector w, as discussed
next.
Note, however, that Mercer’s theorem tells us only whether a candidate kernel
is actually an inner-product kernel in some space and therefore admissible for use in
a support vector machine. It says nothing about how to construct the functions i(x);
we have to do that ourselves. Nevertheless, Mercer’s theorem is important because
it places a limit on the number of admissible kernels. Note also that the expansion of
Eq. (6.33) is a special case of Mercer’s theorem, since all the eigenvalues of this expan-
sion are unity. It is for this reason that an inner-product kernel is also referred to as
a Mercer kernel.
3
a
b
%2(x)dx 6  q
3
a
b 3
a
b
k(x, x¿)%(x)%(x¿)dxdx¿  0
k(x, x) = a
q
i=1

ii(x)i(x)
a  x  b,
aTKa  0
K = {k(xi, xj)}i, j=1
N
Section 6.4
The Support Vector Machine Viewed as a Kernel Machine
283

6.5
DESIGN OF SUPPORT VECTOR MACHINES
The expansion of the kernel k(x, xi) in Eq. (6.33) permits us to construct a decision sur-
face that is nonlinear in the input space, but whose image in the feature space is linear.
With this expansion at hand, we may now state the dual form for the constrained opti-
mization of a support vector machine as follows:
Given the training sample
, find the Lagrange multipliers
that maximize
the objective function
(6.39)
subject to the constraints
where C is a user-specified positive parameter.
Constraint (1) arises from optimization of the Lagrangian Q() with respect to the bias
b, which is a rewrite of Eq. (6.13). The dual problem just stated is of the same form as
that for the case of nonseparable patterns considered in Section 6.3, except for the fact
that the inner product xT
i xj has been replaced by the Mercer kernel k(x, xi).
Examples of Support Vector Machines
The requirement on the kernel k(x, xi) is to satisfy Mercer’s theorem.Within this require-
ment, there is some freedom in how the kernel is chosen. In Table 6.1, we summarize the
kernels for three common types of support vector machines: polynomial learning
machine, radial-basis-function network, and two-layer perceptron.The following points
are noteworthy:
1. The Mercer kernels for polynomial and radial-basis-function types of support vec-
tor machines always satisfy Mercer’s theorem. In contrast, the Mercer kernel for
(2) 0  i  C  for i = 1, 2, ..., N
(1) a
N
i=1
idi = 0
Q() = a
N
i=1
i - 1
2 a
N
i=1 a
N
j=1
ijdidjk(xi, xj)
{i}i=1
N
{(xi, di)}i=1
N
284
Chapter 6
Support Vector Machines
TABLE 6.1
Summary of Mercer Kernels
Type of support 
vector machine
Mercer kernel 
k(x, xi), i = 1, 2, ... , N
Comments
Polynomial learning machine
(xTxi + 1)p
Power p is specified a priori
by the user
Radial-basis-function network
exp a- 1
22 x - xi2 b
The width 2, common to 
all the kernels, is specified 
a priori by the user
Two-layer perceptron
tanh(0xTxi + 1)
Mercer’s theorem is 
satisfied only for some 
values of 0 and 1

a two-layer perceptron type of support vector machine is somewhat restricted, as
indicated in the last row of Table 6.1.This latter entry is a testament to the fact that
the determination of whether a given kernel satisfies Mercer’s theorem can indeed
be a difficult matter.
2. For all three machine types, the dimensionality of the feature space is determined
by the number of support vectors extracted from the training data by the solution
to the constrained-optimization problem.
3. The underlying theory of a support vector machine avoids the need for heuristics
often used in the design of conventional radial-basis-function networks and mul-
tilayer perceptrons.
4. In the radial-basis-function type of a support vector machine,the number of radial-
basis functions and their centers are determined automatically by the number of
support vectors and their values, respectively.
Figure 6.5 displays the architecture of a support vector machine, where m1 denotes the
size of the hidden layer (i.e., feature space).
Regardless of how a support vector machine is implemented,it differs from the con-
ventional approach to the design of a multilayer perceptron in a fundamental way. In the
conventional approach, model complexity is controlled by keeping the number of fea-
tures (i.e.,hidden neurons) small.On the other hand,the support vector machine offers a
solution to the design of a learning machine by controlling model complexity indepen-
dently of dimensionality,as summarized here (Vapnik,1998;Schölkopf and Smola,2002):
• Conceptual problem. Dimensionality of the feature (hidden) space is purposely
made very large to enable the construction of a decision surface in the form of a
Section 6.5
Design of Support Vector Machines
285
Feature layer of
m1 inner-product
kernels
Input
layer of
size m0
Linear
weights
Output
neuron
x1
x2
xm0
w1
w2
wm1
•
•
•
•
•
•
•
•
•
Input
vector
x
Output
y
Bias b
k(x, x1)
k(x, x2)
k(x, xm1)
FIGURE 6.5
Architecture of support
vector machine, using a
radial-basis function
network.

hyperplane in that space. For good generalization performance, the model com-
plexity is controlled by imposing certain constraints on the construction of the
separating soft-margin hyperplane, which results in the extraction of a fraction of
the training data as support vectors.
• Computational problem. The need to compute the weight vector and the bias in the
output layer of the RBF network is avoided by using the kernel trick.
6.6
XOR PROBLEM
To illustrate the procedure for the design of a support vector machine, we revisit the
XOR (Exclusive OR) problem discussed in Chapters 4 and 5. Table 6.2 presents a sum-
mary of the input vectors and desired responses for the four possible states.
To proceed, define the following kernel (Cherkassky and Mulier, 1998):
(6.40)
With x  [x1, x2]T and xi  [xi1, xi2]T, we may thus express the kernel k(x, xi) in terms of
monomials of various orders as follows:
(6.41)
The image of the input vector x induced in the feature space is therefore deduced
to be
Similarly,
(6.42)
Using the definition of Eq. (6.35), we obtain the Gram
K = ≥
9
1
1
1
1
9
1
1
1
1
9
1
1
1
1
9
¥
	(xi) = [1, x2
i1, 22xi1xi2, x2
i2, 22xi1, 22xi2]T,  i = 1, 2, 3, 4
	(x) = [1, x2
1, 22x1x2, x2
2, 22x1, 22x2]T
k(x, xi) = 1 + x2
1x2
i1 + 2x1x2xi1xi2 + x2
2x2
i2 + 2x1xi1 + 2x2xi2
k(x, xi) = (1 + xTxi)2
286
Chapter 6
Support Vector Machines
TABLE 6.2
XOR Problem
Input vector x
Desired response d
(-1, -1)
-1
(-1, 1)
1
(1, -1)
1
(1, 1)
-1

The objective function for the dual form of optimization is therefore as follows (see
Eq. (6.39)):
(6.43)
Optimizing Q() with respect to the four Lagrange multipliers yields the following set
of simultaneous equations:
Hence, the optimum values of the Lagrange multipliers are
This result indicates that in this example,all four input vectors {xi}4
i  1 are support vectors.
The optimum value of Q() is
Correspondingly, we may write
or
From Eq. (6.30), we find that the optimum weight vector is
 = 1
8≥- ≥
1
1
22
1
- 22
- 22
¥ + ≥
1
1
- 22
1
- 22
22
¥ + ≥
1
1
- 22
1
22
- 22
¥ - ≥
1
1
22
1
22
22
¥ ¥
 wo = 1
8 [- (x1) + (x2) + (x3) - (x4)]
7wo7 =
1
22
1
2 7wo7 2 = 1
4
Qo() = 1
4
o, 1 = o, 2 = o, 3 = o, 4 = 1
8
91 - 2 - 3 + 4      = 1
- 1 + 92 + 3 - 4 = 1
- 1 + 2 + 93 - 4 = 1
 1 - 2 - 3 + 94 = 1
+ 92
2 + 223 - 224 + 93
2 - 234 + 94
2)
Q() = 1 + 2 + 3 + 4 - 1
2 (91
2 - 212 - 213 + 214
Section 6.6
XOR Problem
287

The first element of wo indicates that the bias b is zero.
The optimal hyperplane is defined by
Expanding the inner product wT
o 	(x) yields:
which reduces to
The polynomial form of support vector machine for the XOR problem is therefore as
shown in Fig.6.6a.For both x1  x2  1 and x1  x2  1,the output y  1,and for both
x1  1,x2  1 and x1  1 and x2  1, we have y  1.Thus, the XOR problem is
solved as indicated in Fig. 6.6b.
- x1x2 = 0
c0, 0, -1
22
, 0, 0, 0d ≥
1
x2
1
22x1x2
x2
2
22x1
22x2
¥ = 0
wT
o	(x) = 0
 = ≥
0
0
- 122
0
0
0
¥
288
Chapter 6
Support Vector Machines
(a)

1
x1
x2
y  x1x2
(b)
0
1.0
Decision
boundary
1.0
(1,1)
(1,1)
(1,1)
(1,1)
FIGURE 6.6
(a) Polynomial machine for solving the XOR problem. (b) Induced images in the
feature space due to the four data points of the XOR problem.

6.7
COMPUTER EXPERIMENT: PATTERN CLASSIFICATION
In this section,we continue the sequence of pattern-classification experiments based on the
double-moon problem depicted in Fig. 1.8.This time, we use the nonlinear support vector
machine with a single hidden layer.The experiment was repeated for two different settings
of the vertical separation between the two moons of Fig.1.8,namely,d  6.0 and d  6.5.
The parameter C was set equal to infinity for both parts of the experiment. The training
sample consisted of 300 data points, and the test sample consisted of 2,000 data points.The
training data were preprocessed in the same manner as described in Section 1.5.
The scenario corresponding to the distance d  6.0 was chosen for the first part
of the experiment so as to provide an illustrative way of comparing the SVM with the
“K-means, RLS” algorithm used to train the RBF network in Chapter 5, for which a
small number of classification errors was reported. Figure 6.7 presents the correspond-
ing results of the experiment using the SVM with d  6.0. Part (a) of the figure pre-
sents the training result, displaying the support vectors and the optimal decision
boundary computed by the algorithm. For this part of the experiment, there were no
classification errors when the machine was tested with data not seen before, as shown
in part (b) of the figure.
Figure 6.8 presents the results of the second part of the experiment,which used the
SVM for a more difficult scenario for which the vertical separation between the two
moons was reduced to d  6.5. Once again, part (a) of the figure presents the training
result, showing the support vectors and the decision boundary computed by the algo-
rithm. Part (b) of the figure shows the corresponding test result.This time, there were 11
classification errors in the test sample of 2,000 data points, representing a classification
error of 0.55%.
As already stated,both parts of the experiment were performed using the common
value C  *. In this context, the following two points are noteworthy:
1. For d  6.0, the two moons are perfectly nonlinearly separable; this observation
is confirmed by the complete absence of classification errors on test data,as demon-
strated in Fig. 6.7b.
2. For d  6.5, the two moons of Fig. 1.8 overlap slightly. Consequently, they are no
longer separable, which is confirmed by the small number of classification errors
that were found on test data in Fig. 6.8b. In this second part of the experiment, no
attempt was made to find the optimal value of parameter C so as to minimize the
classification error rate; this issue is addressed in Problem 6.24.
6.8
REGRESSION: ROBUSTNESS CONSIDERATIONS
Up to this point in the chapter, we have focused attention on the use of support vector
machines for solving pattern-recognition problems. In this section, we prepare the stage
for studying the use of support vector machines to solve regression problems.To do this,
we will first address the issue of a suitable optimization criterion with robustness as a pri-
mary objective.With this objective in mind, we need a model that is insensitive to small
changes in the model parameters, which is addressed next.
Section 6.8
Regression: Robustness Considerationss
289

290
Chapter 6
Support Vector Machines
12
6
4
2
0
2
y
Classification using SVM with distance  6, radius  10, and width  6
4
6
8
10
10
5
0
5
10
15
20
x
(a) Training result
12
6
4
2
0
2
y
Classification using SVM with distance  6, radius  10, and width  6
4
6
8
10
10
5
0
5
10
15
20
x
(b) Testing result
FIGURE 6.7
Experiment on SVM for the double-moon of Fig. 1.8 with distance d  6.

Section 6.8
Regression: Robustness Considerationss
291
12
6
4
2
0
2
y
Classification using SVM with distance  6.5, radius  10, and width  6
4
6
8
10
10
5
0
5
10
15
20
x
(a) Training result
12
6
4
2
0
y
Classification using SVM with distance  6.5, radius  10, and width  6
4
6
8
10
10
5
0
5
10
15
20
x
(b) Testing result
FIGURE 6.8
Experiment on SVM for the double-moon of Fig. 1.8 with distance d  6.5.

-Insensitive Loss Function
With robustness as a design goal, any quantitative measure of robustness should be con-
cerned with the maximum degradation of performance that is possible for an -deviation
from the nominal noise model.According to this viewpoint,an optimal robust estimation
procedure minimizes the maximum degradation and is therefore a minimax procedure9
of some kind (Huber, 1981). For the case when the additive noise has a probability den-
sity function that is symmetric about the origin, the minimax procedure for solving the
nonlinear regression problem uses the absolute error as the quantity to be minimized
(Huber, 1964).That is, the loss function has the form
(6.44)
where d is the desired response and 
is the corresponding estimator output.
To construct a support vector machine for approximating a desired response d,
we may use an extension of the loss function in Eq. (6.44), originally proposed in Vapnik
(1995, 1998), in the form
(6.45)
where is a prescribed parameter.The loss function 
is called the -insensitive loss
function. It is equal to zero if the absolute value of the deviation of the estimator output
y from the desired response d is less than or equal to zero;otherwise,it is equal to the absolute
value of the deviation minus . The loss function of Eq. (6.44) is a special case of the
-insensitive loss function for 
. Parts a and b of Figure 6.9 illustrate the -insensitive

 = 0



L(d, y)

L(d, y) = e∑d - y∑-   for ∑d - y∑ 
0
 otherwise
y = wT	(x)
L(d, y) = ∑d - y∑


292
Chapter 6
Support Vector Machines












0


(a)
Regressor
x
Response
d
0


d  y
0
(b)
Loss


 (d, y)
L
FIGURE 6.9
Linear regression (a) Illustrating an -insensitive tube of radius , fitted to the
data points shown as ’s. (b) The corresponding plot of the -insensitive loss function.



tube and the corresponding dependence of the loss 
on the error (d  y).
With the -insensitive loss function of Eq. (6.45) as a basis for robustification, the
stage is set for applying SVM theory to solve linear regression problems, as discussed
next.

L(d, y)

6.9
OPTIMAL SOLUTION OF THE LINEAR REGRESSION PROBLEM 
Consider a linear regression model, in which the dependence of a scalar observable d
on a regressor x is described by
(6.46)
where the parameter vector w and the bias b are both unknown.The problem is to com-
pute estimates of w and b, given the training sample t  {xi, di}N
i1, where the data are
statistically independent and identically distributed (iid).
Given the training sample t, consider the risk functional
(6.47)
where the summation accounts for the -insensitive training error and C is a constant
that determines the tradeoff between the training error and the penalizing term 
.
The yi is the estimator output produced in response to the input example xi.The require-
ment is to do the following:
Minimize the risk functional of Eq. (6.47) subject to the following constraints:
(6.48)
(6.49)
(6.50)
(6.51)
for i  1, 2, ..., N. The i and i are two sets of nonnegative slack variables that describe the
-sensitive loss function in Eq. (6.45).
To solve this optimization problem for the Lagrange multipliers i and i, we will
proceed in a manner similar to that pursued in Section 6.2 for the design of a support
vector machine for linearly separable patterns. First, we construct a Lagrangian function
(including the constraints), and then we go on introduce the corresponding dual set of
variables. Specifically, we first to write
- a
N
i=1
i(wTxi + b - di +  + i)
J(w, , i¿, , ¿, , ¿) = 1
2 7w7 2 + C a
N
i=1
(i + ¿i) - a
N
i=1
(ii + ¿i¿i)

 ¿i  0 
 i  0 
yi - di   + ¿i 
di - yi   + i 
7w7 2

1
2
 7w7 2 + C a
N
i=1
∑yi - di∑
d = wTx + b 
Section 6.9
Optimal Solution of the Linear Regression Problem
293
(6.52)
where, as before, the i and i are the Lagrange multipliers.The new multipliers i and i
are introduced in Eq. (6.52) to ensure that the optimality constraints on the Lagrange
multipliers i and i assume variable forms.The requirement is to minimize the Lagrangian
function of Eq. (6.52) with respect to the regression model’s parameters w and b, as well
as the slack variables  and .
- a
N
i=1
¿i(di - wTxi - b +  + i¿) 

294
Chapter 6
Support Vector Machines
Carrying out this optimization as just stated, and setting the resulting derivatives
equal to zero, we respectively obtain
(6.53)
(6.54)
(6.55)
(6.56)
i = 1, 2, ..., N
¿i + ¿i = C,
i = 1, 2, ..., N
i + i = C,
a
N
i=1
(i - ¿i) = 0
wˆ = a
N
i=1
(i - ¿i)x
The support-vector expansion of Eq. (6.53) defines the desired parameter estimate 
in
terms of the computed Lagrange multipliers i and i.To find the corresponding estimate
of the bias, denoted by , we exploit the Karush–Kuhn–Tucker conditions. From the dis-
cussion presented in Section 6.2,we infer that in order to conform to these conditions,for
all the constraints that are not satisfied as equalities,the corresponding variables of the dual
bˆ
wˆ
Accordingly, applying the Karush–Kuhn–Tucker conditions to these four constraints in
accordance with their pertinent dual variables, we respectively obtain
(6.57)
(6.58)
(6.59)
(6.60)
Examining these four equations, we draw three important conclusions:
1. Equations (6.59) and (6.60) tell us that the examples (xi, di) for which i  C
and i  C are the only ones that can lie outside the slack variables i  0 and
i  0;these slack variables correspond to points lying outside the -insensitive tube
centered around the regression function f(x)  wTx  b, as depicted in Fig. 6.10a.
2. Multiplying Eq. (6.57) by i, multiplying Eq. (6.58) by i, and then adding the
resulting equations, we obtain
Hence, for
, and with i  0 and i  0, we have the condition
i¿i = 0
	 7 0
i¿i(2	 + i + ¿i) = 0
	
(C - ¿i)¿i = 0
(C - i)i = 0
¿i(	 + ¿i - di + yi) = 0
i(	 + i + di - yi) = 0
problem must vanish. For the problem at hand we thus have two sets of constraints:
• One set is described by the inequalities of Eqs. (6.48) and (6.49), for which the
dual variables are i and i, respectively.
• The second set is described by the inequalities of Eqs. (6.50) and (6.51), for which
the dual variables are i and i, respectively; from Eqs. (6.55) and (6.56), we find
that i  C 
 i and i  C 
 i.

from which it follows that there can never be a situation where the pair of Lagrange
multipliers i and i are both simultaneously nonzero.
3. From Eqs. (6.59) and (6.60), we respectively observe that
Under these two conditions, the respective Eqs. (6.57) and (6.58) show us that
(6.61)
(6.62)
With Eqs.(6.61) and (6.62) at hand,we can now compute the bias estimate .First,we rec-
ognize that the output of the optimum estimator of the regression function is defined by
For the example xi as the input, we write
(6.63)
Substituting Eq. (6.63) into Eqs. (6.61) and (6.62) and then solving for , we respectively
obtain
(6.64)
and
(6.65)
Hence, knowing 
from Eq. (6.53) and given both 
and di, we may compute the bias
estimate .
For the computation of , in theory, we may use any Lagrange multiplier that lies
inside the range (0, C). However, in practice, it is prudent to use the average value com-
puted over all the Lagrange multipliers in that range.
Sparseness of the Support Vector Expansion
From Eqs.(6.57) and (6.58),we find that for all the examples that lie inside the -insensitive
tube, we have
Under this condition, the factors inside the parentheses of both equations are nonvan-
ishing; hence, for Eqs. (6.57) and (6.58) to hold (i.e., for the Karush–Kuhn–Tucker con-
ditions to be satisfied),we do not need all the examples xi to compute the desired estimate
. In other words, the computed support vector expansion of Eq. (6.53) is sparse.
The examples for which the Lagrange multipliers i and i are nonvanishing define
the support vectors. Insofar as the solution of Eq. (6.53) is concerned, it is geometrically
plausible that the examples that lie inside the -insensitive tube do not contribute to
this solution.The implication of this statement is that those particular examples do not
contain meaningful information about the solution (Schölkopf and Smola, 2002).

wˆ
∑di - yi∑ 

bˆ
bˆ

wˆ
bˆ = di - wˆ Txi +     for 0 6 i 6 C 
bˆ = di - wˆ Txi -     for 0 6 i 6 C 
bˆ
y = wˆ Txi + bˆ
y = wˆ Tx + bˆ
bˆ
for 0 6 ¿i 6 C 
 + di - yi = 0,
for 0 6 i 6 C 
 - di + yi = 0,
¿i = 0  for 0 6 ¿i 6 C
i = 0  for 0 6 i 6 C
Section 6.9
Optimal Solution of the Linear Regression Problem
295

6.10
THE REPRESENTER THEOREM AND RELATED ISSUES
We complete the discussion of kernel machines (inclusive of support vector machines),
be they linear or nonlinear,by establishing the Representer Theorem,which adds a great
deal of insight into our understanding of this important class of learning machines. To
pave the way for proving this theorem, we will first describe what is meant by a Hilbert
space and then by a reproducing-kernel Hilbert space.
Hilbert Space10
Let {xk}*
k1 be an orthonormal basis for an inner-product space f that is assumed to be
of infinite dimensionality.As a reminder,two vectors xj and xk are said to be orthonormal
if they satisfy the twofold condition
(6.66)
where the first part pertains to the normalization property and the second to the
orthogonality property.The space f so defined is called a pre-Hilbert space. The normed
space, in which every vector has a finite Euclidean norm (length), is a special case of the
pre-Hilbert space.
Let h be the largest and most inclusive space of vectors for which the infinite set
{xk}*
k1 is a basis. Then, vectors not necessarily lying in the space f represented in the
form
xT
j xk = e 1
for j = k
0
otherwise
296
Chapter 6
Support Vector Machines
(6.67)
are said to be spanned by the basis {xk}*
k1; the ak are the coefficients of the representa-
tion. Define the new vector
(6.68)
Another vector ym may be similarly defined. For n  m, we may express the squared
Euclidean distance between the vectors yn and ym as
(6.69)
where, in the last line, we invoked the twofold orthonormality condition of Eq. (6.66).
 =
a
n
k=m+1
a2
k 
 = g
a
n
k=m+1
akxkg
2
 7yn - ym7 2 = g a
n
k=1
akxk - a
m
k=1
akxkg
2
yn = a
n
k=1
akxk 
x = a
q
k=1
akxk 

In view of Eq. (6.69), we infer the following:
1.
2.
Moreover, for some positive 
we may pick an integer m large enough to satisfy the
inequality
Since
it therefore follows that
(6.70)
A sequence of vectors {yk}n
k1 in a normed space, for which the Euclidean distance
between yn and ym satisfies the condition
is a convergent sequence; such a sequence is called a Cauchy sequence. Note that
all convergent sequences are Cauchy sequences, but not all Cauchy sequences are
convergent.
Consequently, a vector x can be expanded on the basis {xk}*
k1 if, and only if, x
is a linear combination of the basis vectors and the associated coefficients {ak}*
k1
are square summable. Conversely, the square summability of the set of coefficients
{ak}*
k1 implies that the squared Euclidean distance 
approaches zero as
both n and m approach infinity, and the convergent sequence {yn}*
n1 is a Cauchy
sequence.
From this discussion, it is apparent that the space h is more “complete” than the
inner-product space f.We may therefore make the following important statement:
An inner-product space h is complete if every Cauchy sequence of vectors taken from the
space h converges to a limit in h; a complete inner-product space is called a Hilbert space.
Indeed, it is in view of this statement, that the inner-product space f, in terms of
which we started the discussion, is referred to as pre-Hilbert space.
Reproducing-Kernel Hilbert Space11
Consider a Mercer kernel k(x, .), where the vector 
, and let f be any vector
space of all real-valued functions of x that are generated by the kernel k(x, .).
x   x
7yn - ym7 2
7yn - ym7  6 ε  for any ε 7 0 and all m, n 7 M,
a
q
k=1
a2
k 6 q 
a
q
k=1
a2
k = a
m
k=1
a2
k +
a
q
k=m+1
a2
k
a
q
m+1
a2
k 6 ε
ε
a
m
k=1
a2
k 6 q
a
n
k=m+1
 a2
k S 0 as both n, m S q.
Section 6.10
The Representer Theorem and Related Issues
297

Suppose now two functions f(.) and g(.) are picked from the space f that are respec-
tively represented by
(6.71)
and
(6.72)
where the ai and the bj are expansion coefficients and both xi and 
for all i and j.
Given the functions f(.) and g(.), we now introduce the bilinear form
(6.73)
where K is the Gram, or kernel matrix, and in the first line of the equation we made use
of the relation
(6.74)
The first line of Eq. (6.73) may now be rewritten in the simplified form
(6.75)
where, in the second line, we used the symmetric property of the Mercer kernel. Simi-
larly, we may write
(6.76)
The definition of the bilinear form  f,g  introduced in Eq.(6.73) is independent of how
the functions f(.) and g(.) are represented.We say so because the summation 
in Eq. (6.75) is invariant with respect to changes in the index n, the coefficient vector b,
and the n-dimensional vector 
. A similar statement applies to the summation 
in Eq. (6.76).
Furthermore, from the definition of Eq. (6.73), we readily derive the following
three properties:
a
n
j=1
bjf(x~
j)
x~
j
a
l
i=1
aig(xi)
f, g = a
n
j=1
bjf(x~
j)
= a
l
i=1
aig(xi)
= a
l
i=1
ai a
n
j=1
bjk(x~
j, xi)
f, g = a
l
i=1
ai a
n
j=1
bjk(xi, x~
j)
k(xi, .), k(xj, .) = k(xi, xj)
= aTKb
f, g = a
l
i=1 a
n
j=1
aik(xi, x~
j)bj
x~
j  x
g(.) = a
n
j=1
bjk(x~
j, .)
f(.) = a
l
i=1
aik(xi, .)
298
Chapter 6
Support Vector Machines
g(xi)
⎧
⎪
⎨
⎪
⎩

Property 1. Symmetry For all functions f and g in the space f, the term  f, g is symmetric,
as shown by
(6.77)
Property 2. Scaling and distributive property For any pair of constants c and d and any set
of functions f, g, and h in the space f, we have
(6.78)
Property 3. Squared norm For any real-valued function f in the space f, if we evaluate
Eq. (6.73) for f acting all by itself,we obtain the following squared norm, or quadratic metric:
Since the Gram is nonnegative definite, the squared norm has the property
(6.79)
By virtue of the fact that for all real-valued functions f and g in the space f, the
bilinear term  f, g  satisfies the symmetry, scaling, and distributive properties as well as
the property that the norm 
is nonnegative, we may now formally state 
7f7 2 =  f, f
7f7 2  0 
 = aTKa
 7f7 2 = f, f
(cf + dg), h = c f, h + d g, h 
f, g = g, f 
Section 6.10
The Representer Theorem and Related Issues
299
that the f, g introduced in Eq. (6.73) is indeed an inner product; moreover it is an inner
product that must also satisfy the condition  f, g   0 if, and only if, f  0. In other
words, the space f embracing the functions f and g is an inner-product space.
There is one additional property that follows directly from Eq. (6.75). Specifically,
setting
g(.)  k(x, .) 
in Eq. (6.75), we obtain
(6.80)
For obvious reasons, this property of the Mercer kernel k(x, .) is known as the
reproducing property.
The kernel k(x, xi), representing a function of the two vectors 
, is called
a reproducing kernel of the vector space f if it satisfies the following two conditions
(Aronszajn, 1950):
1. For every 
as a function of the vector x belongs to f.
2. It satisfies the reproducing property.
These two conditions are indeed satisfied by the Mercer kernel, thereby endowing it
with the designation “reproducing kernel.” If the inner-product (vector) space f, in
which the reproducing kernel space is defined, is also complete, then we may go one
step further and speak of a “reproducing-kernel Hilbert space.”
xi  x, k(x, xi)
x, xi  x
 = f(x)
 = a
l
i=1
aik(xi, x),  k(x, xi) = k(xi, x)
 f, k(x, .) = a
l
i=1
aik(x, xi)

To justify the property of completeness, consider a fixed input vector x and a pair
of Cauchy sequences {fn(x)}*
n1 and {fm(x)}*
m1, where n  m.Then, applying the repro-
ducing property of Eq. (6.80) to both fn(x) and fm(x), we may write
where the right-hand side is an inner product. By invoking the Cauchy–Schwarz
inequality,12 we obtain
(6.81)
It follows, therefore, that fn(x) is a bounded Cauchy sequence, which converges toward
some real-valued function f in the space f. Finally, if we define the function
and complete the space f by adding to it all such convergent Cauchy sequences, we
obtain the Hilbert space h.We have thus demonstrated that each Mercer kernel k(x, .)
defines a Hilbert space h, where the value of the function f(x) is reproduced by the
inner product of f(x) with k(x, .).The Hilbert space so defined is called a reproducing-
kernel Hilbert space, for which we use the acronym RKHS hereafter.
The analytic power of RKHS is expressed in an important theorem considered next.
Formulation of the The Representer Theorem13
Define a space h as the RKHS induced by a Mercer kernel k(x, .). Given any real-
valued function 
, we may decompose it into the sum of two components, both
of which naturally lie in the space h:
• One component is contained in the span of the kernel functions k(x1, .), k(x2, .), ...,
k(xl, .); denoting this component by 
we may use Eq. (6.71) to represent it as
• The second component is orthogonal to the span of the kernel functions; it is
denoted by 
.
We may thus express the function f(.) as
(6.82)
Applying the distributive property of Eq. (6.78) to Eq. (6.82), we obtain
 = h a
l
i=1
aik(xi, .), k(xj, .) i
h
+ h k(xj, .), f› i
h
 f(xj) = f(.), k(xj, .)h
 = a
l
i=1
aik(xi, .) + f›(.)
 f(.) = f(.) + f›(.)
f›(x)
f(.) = a
l
i=1
aik(xi, .)
f(x),
f(.)  h
y(x) = lim
nS q fn(x)
k(x, x)
(fn(x) - fm(x))2   fn(.) - fm(.)2k(x, .)k(x, .)
fn(x) - fm(x) = fn(.) - fm(.)k(x, .)
300
Chapter 6
Support Vector Machines
s

With 
being orthogonal to the span of the kernel functions, the second term is zero;
this equation therefore reduces to
(6.83)
Equation (6.83) is a mathematical statement of the representer theorem:
Any function defined in an RKHS can be represented as a linear combination of Mercer
kernel functions.
However, there is more to be said.
Generalized Applicability of the Representer Theorem
An important property of the representer theorem is that the expansion given in Eq.(6.83)
is the minimizer of the regularized empirical risk (cost function)
(6.84)
where 
is the training sample, f is the unknown function to be estimated,
and 
is the regularizing function (Schölkopf and Smola, 2002). For the theorem
to hold, the regularizing function must be a strictly monotonic increasing function of its
argument;hereafter,this requirement is referred to simply as the monotonicity condition.
The first term on the right-hand side of Eq. (6.84) is the standard error term, which
is a quadratic function in f. Hence, the expansion of Eq. (6.83) is the minimizer of this
term through the use of fixed 
To prove that this expansion is also the minimizer of the regularized part of the
empirical risk 
we proceed in three steps as follows:
1. Let 
denote the orthogonal compliment to the span of the kernel functions
Then, since, according to Eq. (6.82), every function can be expressed
as a kernel expansion on the training data plus 
we may write
(6.85)
For mathematical convenience, we prefer to work with the new function
(6.86)
rather than the original regularizing function 
. This move is permissible
because a quadratic function is strictly monotonic on the infinite interval 
Hence,
is strictly monotonic on 
if, and only if,
also satis-
fies the monotonicity condition. For all 
we may thus write
(6.87)
+
~(7f7 h
2 ) = +
~ a g a
l
i=1
aik(xi, .) + f›(.) g
h
2
b
f›,
+(7f7 h)
[0, q)
+
~ ( 7f7 h
2 )
[0, q).
+(7f7 h)
+
~(7f7 h
2 ) = +( 7f7 h)
+( 7f7 h) = + a g a
l
i=1
aik(xi, .) + f›(.) g
h
b
f›,
{k(xi, .)}l
i=1.
f›
e(f),
ai  .
+(7f 7 h)
{x(n), d(n)}n=1
N
e(f) =
1
2N a
N
n=1
(d(n) - f(x(n))2 + +(7f7 h)
 = a
l
i=1
aik(xi, xj)
 f(xj) = h a
l
i=1
aik(xi, .), k(xj, .)i
h
f›
Section 6.10
The Representer Theorem and Related Issues
301

2. Applying the Pythagorean decomposition to the argument of 
on the right-hand
side of Eq. (6.87), we may go on to write
For the optimum condition, we must set 
the use of which in this equation
yields the equality
(6.88)
3. Finally,in light of the definition introduced in Eq.(6.86),we have the desired result
(6.89)
It follows therefore that, for fixed 
the representer theorem is also the minimizer
of the regularizing function 
provided that the monotonicity condition is
satisfied.
In treating the composition of the standard error and regularizing terms as one
whole entity,there will be a trade-off between these two terms.In any case,for some fixed
the representer theorem described by the expansion of Eq. (6.83) will serve as
the minimizer of the regularized empirical risk of Eq. (6.84), thereby establishing gen-
eral applicability of the representer theorem (Schölkopf and Smola, 2002).
In the next chapter, we will make extensive use of this important theorem in the
study of regularization theory.
6.11
SUMMARY AND DISCUSSION
The support vector machine is an elegant and highly principled learning method for the
design of a feedforward network with a single hidden layer of nonlinear units. Its deriva-
tion follows the method of structural risk minimization (SRM) that is rooted in VC
dimension theory, which makes its derivation even more profound; SRM was discussed
in Chapter 4. As the name implies, the design of the machine hinges on the extraction
of a subset of the training data that serves as support vectors and therefore represents
a stable characteristic of the data.The support vector machine includes the polynomial
learning machine, radial-basis-function network, and two-layer perceptron as special
cases.Thus, although these methods provide different models of intrinsic statistical reg-
ularities contained in the training data, they all stem from a common root in a support
vector machine setting.
One other distinctive property of the support vector machine is that it is a kernal
method of the batch-learning kind.14
ai  ,
+( 7f7 h),
ai  ,
+( 7f7 h) = + ag a
l
i=1
aik(xi, .)g
h
b
 +
~(7f7 h
2 ) = +
~ ag a
l
i=1
aik(xi, .)g
h
2
b
f› = 0,
  +
~ ag a
l
i=1
aik(xi, .)g
h
2
b
+ 7f› 7 h
2 b
 +
~(7f7 h
2 )  = +
~ a g a
l
i=1
aik(xi, .)g
h
2
+
~
302
Chapter 6
Support Vector Machines

Computational Considerations
The asymptotic behavior of a support vector machine grows linearly with the number of
training examples,N.It follows therefore that the computational cost of using the machine
for solving pattern recognition and regression problems has both a quadratic and a cubic
component. Specifically, when the parameter C is small, the computational cost grows
like N2;and when C is large,the computational cost grows like N3 (Bottou and Lin,2007).
To alleviate this problem, several commercial optimization libraries have been
developed to solve the quadratic-programming (QP) problem. However, these libraries
are of limited use.The memory requirements of the QP problem grow with the square
of the size of the training sample. Consequently, in real-life applications that may involve
several thousand data points, the QP problem cannot be solved by the straightforward
use of a commercial optimization library.The problem is complicated further by the fact
that, in general, the solution to an SVM problem is quite sparse, because the weight vec-
tor w in the output layer of the machine consists of few nonzero elements relative to the
number of data points in the training sample.Accordingly, direct attempts to solve the
QP problem in a support vector machine will not scale to large problem sizes. To miti-
gate this difficulty, several innovations have been described in the literature, as sum-
marized here:15
1. Osuna et al. (1997) have developed a novel decomposition algorithm that attains
optimality by solving a sequence of much smaller subproblems. In particular, the
decomposition algorithm takes advantage of the support vector coefficients that
are active on either side of their bounds defined by i  0 or i  C. It is reported
therein that the decomposition algorithm performs satisfactorily in applications
with as many as 100,000 data points.
2. Platt (1999) extended Osuna’s methodology by introducing an algorithm called
sequential minimal optimization (SMO), which breaks a large QP problem into a
series of very small QP subproblems that are solvable analytically, thereby elimi-
nating the need for a numerical QP library.The computation time of SMO is dom-
inated by kernel evaluation; hence, the use of kernel optimizations can be
accelerated.
3. Joachims (1999) introduced several key innovations of his own. Specifically, a large
SVM problem is decomposed into a series of smaller ones, but in a more princi-
pled manner than that of Osuna.Another important innovation introduced is the
notion of shrinking: If a point is not an unbounded support vector, has not been
for a long time, and there is little evidence for it becoming one, then, with high
probability,that point may be removed from further scruntiny,thereby saving com-
putation time.
4. Rifkin (2002) developed a computational procedure called the SvmFu algorithm,
which may be viewed as a synthesis of the ideas proposed by Osuna, Platt, and
Joachims. Specifically, the advantages of each of those three procedures were com-
bined with some new features. It is claimed that with SvmFu a large problem may
be solved as a sequence of subproblems which are small enough that their associ-
ated Hessian matrices can fit in memory.
Section 6.11
Summary and Discussion
303

5. Drineas and Mahoney (2005) have developed an algorithm to compute an easily
interpretable low-rank approximation to the N-by-N Gram, or kernel matrix, in
such a way that the computation of interest may be performed more rapidly.The
relationships of the new algorithm with the Nyström method from integral equa-
tion theory are discussed therein.
6. Hush et al. (2006) describe polynomial-time algorithms that produce approximate
solutions with guaranteed accuracy for a class of quadratic-programming prob-
lems that arise in the design of support vector machine classifiers.The algorithms
employ a two-stage process. The first stage produces an approximate solution to
a dual quadratic-programming problem, and the second stage maps this approxi-
mate dual solution to an approximate primal solution.
Curse of Dimensionality
As is the case for a multilayer perceptron, the intrinsic complexity of a support vector
machine as an approximating function increases exponentially with m0,where m0 denotes
the dimensionality of the input space. Moreover, the intrinsic complexity of the machine
decreases exponentially with s, where s denotes the smoothness index which measures
the number of constraints imposed on the approximating function. Accordingly, the
smoothness index of the approximating function acts as a corrective measure against the
curse of dimensionality. We may therefore say that a support vector machine will pro-
vide a good approximation to a dimensionally high function, provided that the function
of interest is correspondingly smooth.
Concluding Remarks
The support vector machine (SVM) has established itself as the most widely used kernel-
learning algorithm. Indeed, we may go on to say that in the machine-learning literature,
support vector machines represent the state-of-the-art by virtue of their good generl-
ization performance,relative ease of use,and rigorous theoretical foundations.Moreover,
in a practical context, they are capable of delivering robust performance in solving
pattern-recognition and regression problems.
However, the major limitation of support vector machines is the fast increase in
their computing and storage requirements with respect to the number of training
examples.These severe requirements tend to leave many large-scale learning problems
beyond the reach of support vector machines.The core of this practical limitation lies
in the quadratic programming routine that is an integral part of the SVM optimization
theory. To mitigate this practical difficulty, a great deal of effort has been devoted to
accelerate the SVM solver through a variety of parallel-implementation techniques
beyond the decomposition procedures described above (Durdanovic et al., 2007;
Yom-Tov (2007).
NOTES AND REFERENCES
1. The support vector machine was pioneered by Vapnik; the first description of the machine
was presented by Boser, Guyon, and Vapnik in 1992.The most comprehensive and detailed
304
Chapter 6
Support Vector Machines

description of this new class of learning machines appeared in Vapnik’s 1998 book entitled
“Statistical Learning Theory,” which is already a classic in the field.
The paper entitled “On the Mathematical Foundations of Learning,” by Cucker and
Smale (2001), presents a mathematically rigorous treatment of supervised learning theory,
with emphasis on the relationship of approximation to learning and the primary role of
inductive inference.
Comprehensive treatments of kernel machines,including support vector machines,are
presented in the books by Schölkopf and Smola (2002), Herbrich (2002), and Shawe-Taylor
and Cristianini (2004).
2. Convex optimization is a special class of optimization techniques that include least-squares
and linear programming, for which a complete theory is already available. Moreover, prob-
lems that lend themselves to convex optimization go beyond least-squares and linear pro-
grams. The advantages to be gained in formulating a problem as a convex-optimization
problem include
• solutions that are reliable and efficient, and
• theoretical advantages, exemplified by the formulation of a dual problem, the solution
of which is more computationally efficient and conceptually transparent than that of
the original problem.
For a detailed treatment of convex analysis and optimization, see the books by Boyd and
Vandenbergh (2004) and Bertsekas et al. (2003).
3. In any optimization problem with a differentiable objective function and constraints for
which duality applies, the primal and dual solutions must satisfy the Karush–Kuhn–Tucker
(KKT) conditions. These conditions are named for Karush (1939) and Kuhn and Tucker
(1951). The survey paper by Kuhn (1976) gives a historical account of solving inequality-
constrained problems, in which convex optimization plays a major role.
4. The relationship between sparse approximation and support vector expansion was first dis-
cussed in Girosi (1998) and Vapnik (1998).
Steinwart (2003) presents a detailed mathematical discussion of the sparseness that
arises in solving pattern-recognition problems by using support vector machines; in partic-
ular, this paper establishes (asymptotically) lower bounds on the number of support vectors.
Along the way, several results are proved that are of importance for the understanding of
support vector machines.The paper addresses three admissible loss functions:
i. the hinge loss, defined by 
;
ii. the squared hinge loss, defined by 
;
iii. the least-squares loss, defined by 
.
The corresponding SVM classifiers are denoted by L1, L2, and LS. The variables d and y
denote the desired response and the corresponding response computed by the support
vector machine for a given input example, respectively.
The design of support vector machines by using the least-squares loss is treated in a
great deal of detail in the book entitled “Least-Squares Support Vector Machines,”by Suykens
et al. (2002).
5. With computational complexity as the issue of interest, we may identify two classes of
algorithms:
• Polynomial time algorithms, which require a running time that a is polynomial function
of the problem size. For example, the fast Fourier transform (FFT) algorithm, commonly
used for spectrum analysis, is a polynomial time algorithm, as it requires a running time
of order n log n, where n is a measure of the problem size.
L(d, y) = (1 - dy)2
L(d, y) = [max(0, 1 - dy)]2
L(d, y) = max(0, 1 - dy)
Notes and References
305

• Exponential time algorithms, which require a running time that is an exponential func-
tion of the problem size. For example, an exponential time algorithm may take time 2n,
where n is a measure of the problem size.
On this basis, we may view polynomial time algorithms as efficient algorithms and expo-
nential time algorithms as inefficient algorithms.
There are many computational problems that arise in practice for which no efficient
algorithms have been devised. Many, if not all, of these seemingly intractable problems are
said to belong to a class of problems referred to as NP-complete problems. The term “NP”
stands for “nondeterministic polynomial.”
For more detailed discussion of NP-complete problems, see Cook (1971), Garey and
Johnson (1979), and Cormen et al. (1990).
6. The reciprocal of the parameter C plays exactly the same role as the regularization pa-
rameter in regularized least-squares estimation.We have adhered to the use of the parameter
C in describing the theory of support vector machines largely to be consistent with the early
development of this new class of kernel machines.
7. The idea of an inner-product kernel was first described in Aizerman et al. (1964a, 1964b)
in the formulation of the method of potential functions, which represents the forerunner
to radial-basis-function networks.At about the same time,Vapnik and Chervonenkis (1964)
developed the idea of an optimal hyperplane. The combined use of these two powerful
concepts in formulating the support vector machine first appeared in Boser et al. (1992).
8. For discussions on additional properties of kernels over and above those presented in
Section 6.4 under property 1 and property 2, see the books by Schölkopf and Smola (2002),
Herbrich (2002), and Shawe-Taylor and Cristianini (2004).
9. To describe minimax theory,consider a function f(x,z),where 
and 
.The require-
ment in this theory is either
to minimize sup f(x, z)
subject to 
or, alternatively,
to
The application of minimax theory arises, for example, in the study of worst-case designs,
which are of engineering importance.For a discussion of this theory,see Bertsekas et al.(2003).
Huber’s minimax theory is based on neighborhoods that are not global by virtue of
their exclusion of asymmetric distributions. Nevertheless, this theory deals successfully with
a large part of traditional statistics, particularly regression.
10. The Hilbert space is discussed in the books by Dorny (1975) and Debnath and Mikusi ´nski
(1990).
11. The original paper on reproducing-kernel Hilbert space (RKHS) is Aronszajn (1950),which
is a classic;it is also discussed in the books by Shawe-Taylor and Cristianini (2004),Schölkopf
and Smola (2002), and Herbrich (2002).
12. Let x and y be any two elements of an inner-product space f. According to the Cauchy–
Schwarz inequality, we have
x, y2  7 x7 2  7 y7 2
subject to z  z
x  x
maximize inf f(x, z)
x  x
z  z
z  z
x  x
306
Chapter 6
Support Vector Machines

the proof of which is straightforward.The inequality states that the squared inner product
of x and y is less than or equal to the product of the squared Euclidean length of x and that
of y. The version of the inequality presented in Eq. (6.81) is an adaptation of this statement
that is made to suit the problem considered in establishing the reproducing-kernel Hilbert
space.
13. In a historical context, the celebrated representer theorem was first described in Kimel-
dorf and Wahba (1971) for solving practical problems in statistical estimation based on
squared-loss (cost) functions; see also the book by Wahba (1990). Generalized applicabil-
ity of the representer theorem to regularized cost functions was addressed for the first-time
in Schölkopf and Smola (2002).
14. In contrast to the support vector machine that is of a batch-learning kind, the kernel LMS
algorithm, due to Liu et al. (2008), is of an on-line learning kind.This new algorithm embod-
ies ideas from the least-mean-square (LMS) algorithm, discussed in Chapter 3, and the
reproducting-kernel Hilbert space (RKHS), discussed in this chapter; these ideas are inte-
grated together in a composite fashion.In particular, the kernel trick is used to permit learn-
ing on iteration-by-iteration basis.
15. An overview of quadratic programming optimization methods is presented in Bottou and
Lin (2007).
PROBLEMS
Optimal separating hyperplane
6.1
Consider the case of a hyperplane for linearly separable patterns, which is defined by the
equation
where w denotes the weight vector, b denotes the bias, and x denotes the input vector.The
hyperplane is said to correspond to a canonical pair (w, b) if, for the set of input patterns
, the additional requirement
is satisfied. Show that this requirement leads to a margin of separation between the two
classes equal to 
.
6.2
Justify the following statement in the context of nonseparable patterns: Misclassification
implies nonseparability of patterns, but the converse is not necessarily true.
6.3
Starting with the primal problem for the optimization of the separating hyperplane for non-
separable patterns, formulate the dual problem as described in Section 6.3.
6.4
In this problem we explore the “leave-one-out method,” discussed in Chapter 4, for esti-
mating the expected test error produced by an optimal hyperplane for the case of nonsep-
arable patterns. Discuss the various possibilities that can arise in the use of this method by
eliminating any one pattern from the training sample and constructing a solution based on
the remaining patterns.
6.5
The location of the optimal hyperplane in the data space is determined by the data points
selected as support vectors. If the data are noisy, one’s first reaction might be to question
the robustness of the margin of separation to the presence of noise.Yet careful study of the
optimal hyperplane reveals that the margin of separation is actually robust to noise. Discuss
the rationale for this robust behavior.
27w7
min
i=1, 2, ..., N wTxi + b∑= 1
{xi}i=1
N
wTx + b = 0
Problems
307

Mercer Kernels
6.6
The Mercer kernel k(xi, xj) is evaluated over a training sample 
of size N, yielding the
N-by-N matrix
where 
.Assume that the matrix K is positive in that all of its elements have pos-
itive values. Using the similarity transformation
where  is a diagonal matrix made up of eigenvalues and Q is a matrix made up of the cor-
responding eigenvectors, formulate an expression for the Mercer kernel k(xi, xj) in terms of
the eigenvalues and eigenvectors of matrix K. What conclusions can you draw from this
representation?
6.7
(a) Demonstrate that all three Mercer kernels described in Table 6.1 satisfy the unitary
invariance property:
where Q is a unitary matrix defined by
(b) Does this property hold in general?
6.8
(a) Show that Mercer kernels are all positive definite.
(b) Consider a Mercer kernel,denoted by k(xi,xj).Such a kernel satisfies the Cauchy–Schwarz
inequality:
Demonstrate this property of Mercer kernels by considering the determinant of a two-
by-two Gram K.
6.9
Consider the Gaussian kernel
where no xi and xj are the same. Show that the Gram
has full rank—that is, any two columns of the matrix K are linearly independent in an alge-
braic sense.
6.10
The Mahalanobis kernel is defined by
where the M-dimensional input vector 
, and 
and the M-by-M
matrix
i = 1, 2, ..., N,
x  x
k(x, xi) = exp(-(x - xi)T a
-1(x - xi))
K = ≥
k(x1, x1)
k(x1, x2)
...
k(x1, xN)
k(x2, x1)
k(x2, x2))
...
k(x2, xN)
o
o
o
k(xN, x1)
k(xN, x2)
...
k(xN, xN)
¥
k(xi, xj) = exp a-
7 xi - xj7 2
22
b,  i, j = 1, 2, ..., N
k(xi, xj)k(xj, xi)  k(xi, xi)k(xj, xj)
Q-1 = QT
k(x, xi) = k(Qx, Qxi)
K = QQT
kij = k(xi,xj)
K = {kij}i, j=1
N
t
308
Chapter 6
Support Vector Machines

where 1, 2, ..., M are all positive. A distinct property of this kernel, compared with the
Gaussian kernel, is that each axis of the M-dimensional input space x has a “smoothing”
parameter (i.e., a particular ) of its own.
To illustrate this property, consider the function
which may be viewed as a density estimator (Herbrich, 2002). Given ai  1 and i   for
all i, M  2, and N  20, plot the function F(x) versus the coordinates x1 and x2 for the fol-
lowing values:
(i)
  0.5
(ii)   0.7
(iii)   1.0
(iv)   2.0
Comment on your results.
6.11
A joint probability density function 
over an x-by-x product space is said to
be a P-matrix provided that it satisfies the finitely nonnegative definite (i.e., positive semi-
definite) property (Shawe-Taylor and Cristianini, 2004).
By considering the simple case of a two-element set X  {X1, X2} of random variables,
demonstrate validity of the following statement:All P-kernels are joint distributions,but not
all joint distributions are P-kernels.
Pattern classification
6.12
The margin plays a key role in the design of support vector machines. Identify the impor-
tant properties of the margin in solving pattern-classification problems.
6.13
Using the formula of Eq. (6.17), show that the margin of linearly separable patterns can be
expressed in terms of the Lagrange multipliers as
where Ns is the number of support vectors.
6.14
Consider a training sample {xi, di}N
i  1 that consists of positive and negative examples that
are linearly separable. Justify the following statement:
The support vectors contain all the information needed to classify the positive and negative
examples.
6.15
Figure P6.15 shows a data set that consists of nonlinearly separable positive and nega-
tive examples. Specifically, the decision boundary separating the positive from negative
examples is an ellipse. Find the transformation that maps the input space into the fea-
ture space such that the positive and negative examples become linearly separable in
the feature space.
6.16
The Mercer kernel for a polynomial learning machine used to solve the XOR problem is
defined by
k(x, xi) = (1 + xTxi)p
 =
2
a a
NS
i=1
i b
12
pX1, X2(x1, x2)
F(x) = a
N
i=1
aiexp a- 7 x - xi7 2
22
i
b
a = diag(2
1, 2
2, ..., 2
M)
Problems
309

What is the minimum value of power p for which the XOR problem is solved?
Assume that p is a positive integer. What is the result of using a value for p larger than
the minimum?
6.17
Figure P6.17 shows the XOR function operating on a three-dimensional pattern x as
described by the relationship
where the symbol 
denotes the exclusive-OR Boolean function operator. Design a poly-
nomial learning machine to separate the two classes of points represented by the output of
this operator.
{
XOR(x1, x2, x3) = x1 { x2{ x3
310
Chapter 6
Support Vector Machines
0
a
Negative examples
b
a
b
x1
x2
Positive
examples
FIGURE P6.15



x2
0
x1
x3
FIGURE P6.17

Sparsity
6.18
Justify the following statement:
A support vector machine solver is sparse, but the Gram associated with the machine is
rarely sparse.
6.19
The quadratic programming routine in a support vector machine solver provides the basis
for splitting the training examples into three categories. Define these three categories, and
use a two-dimensional figure to illustrate how the splitting is performed.
Metrics
6.20
With different algortihms being developed for accelerating support vector machine solvers,
it is important that we formulate metrics for comparing the performance of these different
algorithms. Develop a set of metrics that could be used for dealing with this practical issue.
Reproducing-kernel Hilbert space
6.21
Let k(xi, ) and k(xj, ) denote a pair of kernels, where i, j  1, 2, ..., N. The vectors xi and xj
have the same dimensionality. Show that
where the expression on the left-hand side is an inner-product kernel.
6.22
Equations (6.77), (6.78), and (6.79) describe three important properties of the inner prod-
uct  f, g, defined in Eq. (6.75). Prove the properties described in those three equations.
6.23
Justify the following statement:
If a reproducing kernel k(x, x) exists, then that kernel is unique.
Computer experiments
6.24
This experiment investigates the scenario where the two moons in Fig. 1.8 overlap and are
therefore nonseparable.
(a) Repeat the second part of the experiment in Fig. 6.7, for which the vertical separation
between the two moons was fixed at d  6.5. Experimentally, determine the value of
parameter C for which the classification error rate is reduced to a minimum.
(b) Reduce the vertical separation between the two moons further by setting d  6.75,
for which the classification error rate is expected to be higher than that for d  6.5.
Experimentally, determine the value of parameter C for which the error rate is reduced
to a minimum.
Comment on the results obtained for both parts of the experiment.
6.25
Among the supervised-learning algorithms studied thus far, the support vector machine
stands out as the most powerful. In this problem, the performance of the support vector
machine is to be challenged by using it to classify the two multicircular regions that consti-
tute the “tightly fisted”structure shown in Fig.P6.24.The radii of the three concentric circles
in this figure are 
0.2,
0.5, and 
0.8.
(a) Generate 100 epochs, each of which consists of 200 randomly distributed training exam-
ples, and an equal number of test data for the two regions of Fig. P6.24.
(b) Train a support vector machine, assigning the value C  500. Hence, construct the deci-
sion boundary computed by the machine.
(c) Test the network and thereby determine the classification error rate.
(d) Repeat the experiment for C  100 and C  2,500.
Comment on your results.
d3 =
d2 =
d1 =
k(xi, .)k(xj, .) = k(xi, xj)
Problems
311

312
Chapter 6
Support Vector Machines
x2
x1
d3
d2
d1
0
FIGURE P6.25

313
ORGANIZATION OF THE CHAPTER
In this chapter, we focus attention on the many facets of regularization theory, which is
at the core of all neural-network and machine-learning algorithms. Following the moti-
vational material presented in Section 7.1,the rest of the chapter is organized as follows:
1. Section 7.2 addresses the issue of ill-posed inverse problems.
2. Section 7.3 develops Tikhonov’s regularization theory,which provides the mathematical
basis for the regularization of supervised-learning algorithms. This part of the chapter
also includes Section 7.4, which focuses on regularization networks whose hidden layer
has the same size as that of the training sample. Section 7.5 discusses a class of general-
ized radial-basis-function networks whose hidden layer is constrained to be a subset of
that characterizing regularization networks. The regularized least-squares estimator is
revisited in Section 7.6 as a special case of this class of generalized RBF networks.Then,
in Section 7.7 we show how the insightful ideas derived from regularized least-squares
estimation can be exploited in the regularization of other estimators that do not lend
themselves to the application of Tikhonov’s regularization theory.
3. Section 7.8 describes a procedure, based on cross-validation, for estimating the reg-
ularization parameter.
4. The last part of the chapter begins with a discussion of semisupervised learning in
Section 7.9. Then, the basic ideas behind manifold regularization are discussed in
Sections 7.10 through 7.12. Section 7.13 introduces spectral graph theory. Section
7.14 discusses generalization of the representer theorem in light of the manifold
regularization theory. Section 7.15 exploits spectral graph theory on the regularized
least-squares estimator (using labeled and unlabeled examples), as an illustrative
application of the generalized regularization theory. In Section 7.16, we present a
computer experiment on semisupervised learning, using least-squares estimation.
Section 7.17 concludes the chapter with a summary and discussion.
7.1
INTRODUCTION
In looking over the supervised-learning algorithms derived in previous chapters of
the book, we find that despite the differences in their compositions, they do share a 
C H A P T E R  7
Regularization Theory

314
Chapter 7
Regularization Theory
common viewpoint:
The training of a network by means of examples, designed to retrieve an output pattern when
presented with an input pattern, is equivalent to the construction of a hypersurface (i.e., mul-
tidimensional mapping) that defines the output pattern in terms of the input pattern.
Learning from examples as described here is an inverse problem, in the sense that its
formulation builds on knowledge obtained from examples of the corresponding direct
problem; the latter type of problem involves underlying physical laws that are unknown.
In real-life situations, however, we usually find that the training sample suffers from a
serious limitation:
The information content of a training sample is ordinarily not sufficient by itself to reconstruct
the unknown input–output mapping uniquely—hence the possibility of overfitting by a learn-
ing machine.
To overcome this serious problem, we may use the method of regularization, the
aim of which is to restrict the solution of the hypersurface reconstruction problem to
compact subsets by minimizing the augmented cost function:
Given a training sample,the empirical risk,or standard cost function,could,for example,
be defined by a sum of error squares.The addition of the regularizer is intended to smooth
the solution to the hypersurface reconstruction problem. Thus, through an appropriate
choice of the regularization parameter (which is under the designer’s control), the
regularized cost function provides a tradeoff between the “fidelity” of the training sam-
ple (involved in calculating the squared errors) and “smoothness” of the solution.
In this chapter, we study two issues of fundamental importance:
1. Classical regularization theory, which is rooted in the regularized cost function we
just described in words. This elegant theory, due to Tikhonov (1963), provides a
unified mathematical basis for the regularizers discussed in previous chapters;
moreover, it elaborates on them by presenting new ideas.
2. Generalized regularization theory, which expands the formulation of Tikhonov’s
classical regularization theory by including a third term; this new term, called the
manifold regularizer, due to Belkin et al. (2006), exploits the marginal probability
distribution of the input space responsible for generating unlabeled examples (i.e.,
examples without desired response). The generalized regularization theory pro-
vides a mathematical basis for semisupervised learning that relies on the combined
use of labeled and unlabeled examples.
7.2
HADAMARD’S CONDITIONS FOR WELL-POSEDNESS
The term “well posed”was introduced by Hadamard (1902) and has been used in applied
mathematics ever since. To explain this terminology, assume that we have a domain 
and a range 
which are related by a fixed, but unknown, mapping f. The problem of
reconstructing the mapping f is said to be well posed if Hadamard’s three conditions are
y,
x
°
Regularized
cost
function
¢ = °
Empirical
cost
function
¢ + aRegularization
parameter
b * (Regularizer)

Section 7.3
Tikhonov’s Regularization Theory
315
satisfied (Tikhonov and Arsenin, 1977; Morozov, 1993; Kirsch, 1996):
1. Existence. For every input vector 
, there exists an output y  f(x), where
2. Uniqueness. For any pair of input vectors 
, we have f(x)  f(t) if, and only
if, x  t.
x, t  x
y  y.
x  x
t
f(t)
f(x)
x
Mapping f()
Domain x
Range y
Î
´
FIGURE 7.1
Illustration of
the mapping of (input)
domain 
onto (output)
range .
y
x
3. Continuity. The mapping f is continuous; that is, for any  0, there exists   ( )
such that the condition &x(x, t)   implies that &y(f(x),f(t))  ,where 
is the
symbol for distance between the two arguments in their respective spaces.This cri-
terion is illustrated in Fig 7.1.The property of continuity is also referred to as stability.
&(, )



If any of these conditions is not satisfied, the problem is said to be ill posed. Basically,
ill posedness means that large data sets may contain a surprisingly small amount of
information about the desired solution.
In the context of supervised learning, Hadamard’s conditions are violated for the
following reasons:1 First, the existence criterion may be violated in that a distinct output
may not exist for every input.Second,there may not be as much information in the train-
ing sample as we really need for a unique reconstruction of the input–output mapping;
hence, the uniqueness criterion is likely to be violated.Third, the unavoidable presence
of noise or imprecision in real-life training data adds uncertainty to the reconstruction
process.In particular,if the noise level in the input is too high,it is possible for the neural
network or learning machine to produce an output outside of the range 
for a specified
input x in the domain ; in other words, it is possible for the continuity criterion to be
violated. If a learning problem lacks the property of continuity, then the computed
input–output mapping has nothing to do with the true solution to the learning problem.
There is no way to overcome these difficulties unless some prior information about the
input–output mapping is available.In this context,it is rather appropriate that we remind
ourselves of a statement made by Lanczos on linear differential operators (Lanczos,1964):
A lack of information cannot be remedied by any mathematical trickery.
7.3
TIKHONOV’S REGULARIZATION THEORY
In 1963,Tikhonov proposed a new method called regularization for solving ill-posed prob-
lems. In the context of a hypersurface reconstruction problem,the basic idea of regulariza-
tion is to stabilize the solution by means of some auxiliary nonnegative functional that
embeds prior information about the solution.The most common form of prior information
involves the assumption that the input–output mapping function (i.e.,solution to the recon-
struction problem) is smooth, in the following sense:
Similar inputs produce similar outputs for an input-output mapping to be smooth
x
y

316
Chapter 7
Regularization Theory
To be specific, let the set of input–output data (i.e., the training sample) available
for approximation be described by
Input signal:
Desired response:
(7.1)
Note that the output is assumed to be one dimensional.This assumption does not in any
way limit the general applicability of the regularization theory being developed here.Let
the approximating function be denoted by F(x), where (for convenience of presenta-
tion) we have omitted the weight vector w of the network from the argument of the
function F. Basically,Tikhonov’s regularization theory involves two terms:
1. Error function, denoted by 
which is defined in terms of the approximating
function F(xi) and the training sample {xi, di}N
i1. For example, for the least-squares
estimator, we have the standard cost (loss) function
(7.2)
where the subscript s in 
s stands for “standard.” For another example that is alto-
gether different—namely, the support vector machine—we have the margin loss
function
We could, of course, embrace both examples under a single formula, but these two
elemental loss functions are so different in their implications that their theoreti-
cal developments would have to be treated differently sooner or later. For the sake
of clarity of exposition, we henceforth will focus on the error function of Eq. (7.2).
2. Regularizer, denoted by the term 
, which is dependent on certain geometric
properties of the approximating function F(xi), as shown by
(7.3)
where the subscript c in 
stands for “complexity.” The D in Eq. (7.3) is a linear
differential operator. Prior information about the form of the solution [i.e., the
input–output mapping function F(x)] is embedded in the operator D, which nat-
urally makes the selection of D network dependent. We also refer to D as a
stabilizer because it stabilizes the solution to the regularization problem, making
it smooth and thereby satisfying the property of continuity. Note, however, that
while smoothness implies continuity, the reverse is not necessarily true. The ana-
lytic approach used to handle the situation described in Eq. (7.3) builds on the
concept of a Hilbert space, which was discussed in Chapter 6.In such a space,a con-
tinuous function is represented by a vector. By using this geometrical image, an
insightful link is established between matrices and linear differential operators.
The analysis of linear systems thereby becomes translatable to the analysis of lin-
ear differential equations (Lanczos,1964).Thus,the symbol 
in Eq.(7.3) denotes
a norm imposed on the Hilbert space to which DF(x) belongs. With the linear
7  7
ec
ec(F) = 1
2
 7DF7 2
ec(F)
es(F) = 1
N a
N
i=1
max(0, 1 - diF(xc)), di  {-1, + 1}
e
es(F) = 1
2 a
N
i=1
(di - F(xi))2
es(F),
i = 1, 2, ..., N
di  ,
i = 1, 2, ..., N
xi  m,

Section 7.3
Tikhonov’s Regularization Theory
317
differential operator D viewed as a map from the space of functions, to which F
belongs,into a Hilbert space,it is natural to take the norm in Eq.(7.3) as the L2 norm.
The training sample t  {xi, di}N
i1, generated by a physical process, is repre-
sented by the regression model:
where xi is the regressor, di is the response, and 
is the explanational error. Strictly
speaking,we require the function f(x) to be a member of a reproducing-kernel Hilbert
space (RKHS) with a reproducing kernel in the form of the Dirac delta distribution
(Tapia and Thompson, 1978); the need for this requirement will become apparent
later in the discussion.The concept of RKHS was also discussed in Chapter 6.
Let 
denote the standard cost (loss) function and +(F) denote the regular-
izing function.Then, assuming least-squares loss, the quantity to be minimized in regu-
larization theory is
(7.4)
where 
 is a positive real number called the regularization parameter and 
is called
the Tikhonov functional. A functional maps functions (defined in some suitable function
space) onto the real line.The minimizer of the Tikhonov functional 
(i.e., the solu-
tion to the regularization problem) is denoted by F
(x). It is noteworthy that Eq. (7.4)
may be viewed as a constrained-optimization problem: Minimize 
subject to a con-
es(F)
e(F)
e(F)
 = 1
2 a
N
i=1
[di - F(xi)]2 + 1
2 
 7DF7 2
 e(F) = es(F) + +(F)
es(F)
i
di = f(xi) + i,  i = 1, 2, ..., N
influence of which on the final solution is controlled by the regularization parameter 
.
Another way of viewing regularization is that it provides a practical solution to the
bias–variance dilemma, discussed in Chapter 2. Specifically, the optimum choice of the
regularization parameter 
 is designed to steer the solution to the learning problem
toward a satisfactory balance between model bias and model variance by incorporating
the right amount of prior information into it.
Applications of Tikhonov Regularization
In the discussion on the Tikhonov regularization theory presented thus far, the empha-
sis has been on regression, as implied by the use of 
in Eq. (7.1). However, it is
di  
iting case 
implies that the problem is unconstrained,with the solution F
(x) being
completely determined from the examples.The other limiting case,
,on the other
hand,implies that the prior smoothness constraint imposed by the differential operator D
is by itself sufficient to specify the solution F
(x), which is another way of saying that the
examples are unreliable.In practical applications,the regularization parameter 
 is assigned
a value somewhere between these two limiting conditions, so that both the training-
sample data and the prior information contribute to the solution F
(x). Thus, the regu-
larizing term 
represents a model complexity-penalty function, the
ec(F) = 1
27DF7 2

 S q

 S 0
straint imposed on +(F). In so doing, we emphasize an explicit constraint on the “com-
plexity” of the approximating function F.
Moreover, we may view the regularization parameter 
 as an indicator of the suf-
ficiency of the given training sample in specifying the solution F
(x).In particular,the lim-

318
Chapter 7
Regularization Theory
important to recognize that the Tikhonov regularization theory also applies to two other
topics:
1. Classification.This may be done, for example, by simply treating binary labels as real
values in standard least-squares regression. For another example, we may use empir-
ial risk (i.e., cost) functions, such as the hinge loss, that are better suited for pattern
classification,leading to support vector machines,which were discussed in Chapter 6.
2. Structured prediction. In some recent work, Tikhonov regularization has been
applied to structural prediction, where, for example, the output space may be a
sequence, a tree, or some other structured output space (Bakir et al., 2007).
The important point we wish to emphasize here is that the notion of regularization is at
the heart of almost all practical settings for which the requirement is to learn from a
training sample of some finite size.
Fréchet Differential of the Tikhonov Functional
The principle of regularization may now be stated as follows:
Find the approximating function F
(x) that minimizes the Tikhonov functional
, defined by
where
is the standard error term,
is the regularizing term, and 
 is the regular-
ization parameter.
To proceed with the minimization of the cost functional 
, we need a rule for eval-
uating the differential of 
.We can take care of this matter by using the Fréchet dif-
ferential. In elementary calculus, the tangent to a curve is a straight line that gives the
best approximation of the curve in the neighborhood of the point of tangency. Similarly,
the Fréchet differential of a functional may be interpreted as the best local linear approx-
imation.Thus, the Fréchet differential of the functional 
is formally defined by
(7.5)
where h(x) is a fixed function of the vector x (Dorny, 1975; Debnath and Mikusin´ski,
1990; de Figueiredo and Chen, 1993). In Eq. (7.5), the ordinary rules of differentiation
apply. A necessary condition for the function F(x) to be a relative extremum of the func-
tional 
is that the Fréchet differential 
must be zero at F(x) for all 
,
as shown by
(7.6)
where 
and 
are the Fréchet differentials of the functionals 
and
, respectively; h was used in place of h(x) in Eq. (7.5) to simplify the presentation.
Evaluating the Fréchet differential of the standard error term 
of Eq.(7.2),
we have
 des(F, h) = c d
des(F + h)d
=0
es(F, h)
ec(F)
es(F)
dec(F, h)
des(F, h)
de(F, h) = des(F, h) + 
dec(F, h) = 0
h  h
de(F, h)
e(F)
de(F, h) = c d
de(F + h)d
=0
e(F)
e(F)
e(F)
ec(F)
es(F)
e(F) = es(F) + 
ec(F)
e(F)

Section 7.3
Tikhonov’s Regularization Theory
319
(7.7)
The Riesz Representation Theorem
To continue with the treatment of the Fréchet differential formulated in the Hilbert
space, we find it helpful to invoke the Riesz representation theorem, which may be
stated as follows (Debnath and Mikusin´ski, 1990):
Let f be a bounded linear functional in a Hilbert space denoted by . There exists one 
such that
Moreover, we have
where it is agreed that h0 and f have norms in their respective spaces.
The symbol 
used here stands for the inner (scalar) product of two functions in
the 
space. Hence, in light of the Riesz representation theorem, we may rewrite the
Fréchet differential 
of Eq. (7.7) in the equivalent form
(7.8)
where 
denotes the Dirac delta distribution of x centered at xi; that is,
(7.9)
Consider next the evaluation of the Fréchet differential of the regularizing term 
of Eq. (7.3). Proceeding in a manner similar to that just described, we have (assuming
that 
)
(7.10)
 = Dh, DFh
 =
3m0DF Dh dx
 =
3m0D[F + h]Dh dx=0
 = 1
2 d
d 3m0(D[F + h])2dx=0
 dec(F, h) =
d
dec(F + h)=0
DF  L2(m0)
ec(F)
xi(x) = (x - xi)
xi
des(F, h) = - hh, a
N
i=1
(di - F)xi i
h
des(F, h)
h
  ,  h
7f7 = 7 h07 h
f(h) = h, h0 h for all h  h
h0  h
h
 = - a
N
i=1
[di - F(xi)]h(xi)
 = - a
N
i=1
[di - F(xi) - h(xi)]h(xi)=0
 = c 1
2 d
d a
N
i=1
[di - F(xi) - h(xi)]2d
=0

320
Chapter 7
Regularization Theory
where 
is the inner product of the two functions Dh(x) and DF(x) that results
from the action of the differential operator D on h(x) and F(x), respectively.
Euler–Lagrange Equation
Given a linear differential operator D, we can find a uniquely determined adjoint
operator, denoted by 
, such that for any pair of functions u(x) and v(x) that are suffi-
ciently differentiable and satisfy proper boundary conditions, we can write the following
(Lanczos, 1964):
(7.11)
Equation (7.11) is called Green’s identity; it provides a mathematical basis for defining
the adjoint operator 
in terms of the given differential D. If D is viewed as a matrix,
the adjoint operator 
plays a role similar to that of a matrix transpose.
Comparing the left-hand side of Eq.(7.11) with the fourth line of Eq.(7.10),we may
make the following identifications:
Using Green’s identity, we may rewrite Eq. (7.10) in the equivalent form
(7.12)
where 
is the adjoint of D.
Returning to the extremum condition described in Eq. (7.6) and substituting the
Fréchet differentials of Eqs. (7.8) and (7.12) into that equation, we may now express the
Fréchet differential 
as
(7.13)
Since the regularization parameter 
 is ordinarily assigned a value somewhere in the
open interval 
the Fréchet differential 
is zero for every h(x) in 
space
if, and only if, the following condition is satisfied by 
in the distributional sense:
Equivalently, we have
(7.14)
Equation (7.14) is the Euler–Lagrange equation for the Tikhonov functional 
; it
defines a necessary condition for the Tikhonov functional 
to have an extremum
at F
(x) (Debnath and Mikusin´ski, 1990).
e(F)
e(F)
D~DF
(x) = 1

 a
N
i=1
[di - F
(xi)](x - xi)
D~DF
 - 1

 a
N
i=1
(di - F
)xi = 0
F = F
h
de(F, h)
(0, q),
de(F, h) = hh, cD~DF - 1

 a
N
i=1
(di - F)xid i
x
de(F, h)
D~
 = h, D~DFx
 dec(F, h) =
3m0h(x)D~DF(x)dx
 Dv(x) = Dh(x)
 u(x) = DF(x)
D~D~
3mu(x)Dv(x)dx =
3mv(x)D~u(x)dx
D~
Dh, DFh

Section 7.3
Tikhonov’s Regularization Theory
321
Green’s Function
Equation (7.14) represents a partial differential equation in the function Fl. The solution
of this equation is known to consist of the integral transformation of the right-hand side
of the equation. We now digress briefly to introduce Green’s function and then con-
tinue the solution to Eq. (7.14).
Let G(x, ) denote a function in which both vectors x and  appear on equal foot-
ing but for different purposes: x as a parameter and  as an argument. For a given lin-
ear differential operator L, we stipulate that the function G(x, ) satisfies the following
conditions (Courant and Hilbert, 1970):
1. For a fixed ,G(x, ) is a function of x that satisfies the prescribed boundary conditions.
2. Except at the point x  , the derivatives of G(x, ) with respect to x are all con-
tinuous; the number of derivatives is determined by the order of the operator L.
3. With G(x, ) considered as a function of x, it satisfies the partial differential equation
(7.15)
everywhere, except at the point x  , where it has a singularity.That is, the func-
tion G(x, ) satisfies the partial differential equation (taken in the sense of distri-
butions)
(7.16)
where, as defined previously, (x - ) is the Dirac delta function positioned at the
point x  .
The function G(x, ) is called the influence function, or Green’s function, for the differ-
ential operator L (Courant and Hilbert, 1970). Green’s function plays a role for a 
linear differential operator which is similar to that for the inverse matrix for a matrix
equation.
Let (x) denote a continuous or piecewise-continuous function of 
. Then
the function
(7.17)
is a solution of the differential equation
(7.18)
where G(x, ) is Green’s function for the linear differential operator L.
To prove the validity of F(x) as a solution of Eq. (7.18), we apply the differential
operator L to Eq. (7.17), obtaining
(7.19)
 =
3m0LG(x, )()d
 LF(x) = L
3m0G(x, )()d()
LF(x) = (x)
F(x) =
3m0G(x, )()d
x  m0
LG(x, ) = (x - )
LG(x, ) = 0

322
Chapter 7
Regularization Theory
The differential operator L treats  as a constant, acting on the kernel G(x; ) only as a
function of x. By using Eq. (7.16) in Eq. (7.19), we get
Finally, using the sifting property of the Dirac delta function, namely,
we obtain LF(x)  (x), as described in Eq. (7.18).
Solution to the Regularization Problem
Returning to the issue at hand—namely, that of solving the Euler–Lagrange equation,
Eq. (7.14)—we set
(7.20)
and
(7.21)
Then, we may use Eq. (7.17) to write
where, in the last line, we have interchanged the order of integration and summation.
Finally, using the sifting property of the Dirac delta function, we get the desired solution
to the Euler–Lagrange equation, Eq. (7.14), as shown by
(7.22)
Equation (7.22) states that the minimizing solution F
(x) to the regularization problem
is a linear superposition of N Green’s functions.The xi represent the centers of the expan-
sion, and the weights [di - F(xi)]$
 represent the coefficients of the expansion. In other
words, the solution to the regularization problem lies in an N-dimensional subspace of
the space of smooth functions, and the set of Green’s functions {G(x, xi)} centered at xi,
i  1, 2, ..., N, constitutes a basis for this subspace (Poggio and Girosi, 1990a). Note that
the coefficients of expansion in Eq. (7.22), defining the regularized function Fl(x), are
• linear in the estimation error, defined as the difference between the desired
response di and the corresponding output F(xi) computed by the network, and
• inversely proportional to the regularization parameter 
.
F
(x) = 1

 a
N
i=1
[di - F(xi)]G(x, xi)
 = 1

 a
N
i=1
(di - F(xi))
3m0G(x, )( - xi)d
 F
(x) =
3m0G(x, ) e 1

 a
N
i=1
[di - F(xi)]( - xi) fd
() = 1

 a
N
i=1
[di - F(xi)]( - xi)
L = D~D
3m0()(x - )d() = (x)
LF(x) =
3m0(x - )()d

Section 7.3
Tikhonov’s Regularization Theory
323
Determination of the Expansion Coefficients
The next issue to be resolved is the determination of the unknown coefficients in the
expansion of Eq. (7.22). Let
(7.23)
We may then recast the minimizing solution of Eq. (7.22) simply as:
(7.24)
Evaluating Eq. (7.24) at xj, j  1, 2, ..., N, we get a system of N simultaneous equations:
(7.25)
We now introduce the following matrix definitions:
(7.26)
(7.27)
(7.28)
(7.29)
Accordingly,we may rewrite Eqs.(7.23) and (7.25) in matrix form as follows,respectively:
(7.30)
and
(7.31)
Eliminating F
 between Eqs. (7.30) and (7.31) and rearranging terms, we get
(7.32)
where I is the N-by-N identity matrix.We call matrix G the Green’s matrix.
The differential operator L defined in Eq. (7.20) is self-adjoint, in the sense that its
adjoint is equal to the operator L itself. It follows therefore that the associated Green’s
function G(x, xi) is a symmetric function, as shown by
(7.33)
Equation (7.33) states that the positions of any two points x and  can be interchanged
without affecting the value of the Green’s function G(x, ). Equivalently, the Green’s
matrix G defined in Eq. (7.28) is a symmetric matrix; that is,
G(xi, xj) = G(xj, xi)  for all i and j
(G + 
I)w = d
F
 = Gw
w = 1

 (d - F
)
w = [w1, w2, p , wN]T
G = ≥
G(x1, x1)
G(x1, x2)
p
G(x1, xN)
G(x2, x1)
G(x2, x2)
p
G(x2, xN)
o
o
o
G(xN, x1)
G(xN, x2)
p
G(xN, xN)
¥
d = [d1, d2, ..., dN]T
F
 = [F
(x1), F
(x2), ..., F
(xN)]T
F
(xj) = a
N
i=1
wiG(xj, xi),  j = 1, 2, ..., N
F
(x) = a
N
i=1
wiG(x, xi)
wi = 1

 [di - F(xi)],  i = 1, 2, ..., N

324
Chapter 7
Regularization Theory
(7.34)
We now invoke the interpolation theorem, which was described in Chapter 5 in the con-
text of the interpolation matrix .We first note that Green’s matrix G plays a role in reg-
ularization theory similar to that of  in RBF interpolation theory. Both G and  are
N-by-N symmetric matrices. Accordingly, we may state that the matrix G, for certain
classes of Green’s functions, is positive definite provided that the data points x1, x2, ...,
xN are distinct.The classes of Green’s functions covered by Micchelli’s theorem include
inverse multiquadrics and Gaussian functions, but not multiquadrics. In practice, we may
always choose 
 sufficiently large to ensure that the sum matrix G  
I is positive def-
inite and therefore invertible. This, in turn, means that the linear system of equations
(7.32) will have a unique solution given by the following (Poggio and Girosi, 1990a):
(7.35)
Thus, having selected the differential operator D and therefore having identified the
associated Green’s function G(xj, xi), where i  1, 2, ..., N, we may use Eq. (7.35) to com-
pute the weight vector w for a specified desired response vector d and an appropriate
value of regularization parameter 
.
The solution to the regularization problem is thus given by the expansion
(7.36)
Accordingly, we may now make the following threefold statement:
1. The approximating function Fl(x), which minimizes the regularized cost function (F) of Eq.
(7.4), is made up of an expansion of linearly weighted Green’s functions, with each Green’s
function depending only on the stablizer D.
2. The number of Green’s functions used in the expansion is equal to the size of the training sam-
ple, N.
3. The corresponding N weights of the expansion are defined in terms of the training sample
and regularization parameter l by Eq. (7.23).
If the stabilizer D is translationally invariant, the Green’s function G(x, xi) centered at
xi will depend only on the difference between the arguments x and xi; that is,
(7.37)
If the stabilizer D is both translationally and rotationally invariant, the Green’s func-
tion G(x, xi) will depend only on the Euclidean norm of the difference vector x - xi, as
shown by
(7.38)
Under these conditions, the Green’s function must be a radial-basis function. In such a
case, the regularized solution of Eq. (7.36) takes on the special form
(7.39)
F
(x) = a
N
i=1
wiG(7x - xi7)
G(x, xi) = G(7x - xi7)
G(x, xi) = G(x - xi)
{xi, di}N
i=1
F
(x) = a
N
i=1
wiG(x, xi)
w = (G + 
I)-1d
GT = G

Section 7.3
Tikhonov’s Regularization Theory
325
which constructs a linear function space that depends on the known data points accord-
ing to the Euclidean distance measure.
The solution described by Eq. (7.39) is termed strict interpolation, since all the
N data points available for training are used to generate the interpolating function
F(x). It is important, however, to realize that this solution differs from that of Eq. (5.11)
of Chapter 5 in a fundamental respect: The solution of Eq. (7.39) is regularized by
virtue of the definition given in Eq. (7.35) for the weight vector w. It is only when
we set the regularization parameter 
 equal to zero that the two solutions may
become one and the same.
Multivariate Gaussian Functions
The Green’s function G(x, xi) whose linear differential operator D is both translation-
ally and rotationally invariant and that satisfies the condition of Eq. (7.38) is of partic-
ular interest in practice. An example of such a Green’s function is the multivariate
Gaussian function defined by
(7.40)
where xi denotes the center of the function and i denotes its width. The self-adjoint
operator 
, which defines the Green’s function of Eq. (7.40) in accordance with
Eq. (7.16), is given by
(7.41)
where
(7.42)
and 
is the iterated Laplacian operator in m0 dimensions, with
(7.43)
With the number of terms permitted to go to infinity in Eq. (7.41), L ceases to be a dif-
ferential operator in the standard sense. For this reason, the operator L in Eq. (7.41) is
referred to as a pseudodifferential operator.
Since by definition,
, we deduce from Eq. (7.41) that the operator D and
its adjoint 
are as follows, respectively:
(7.44)
 =
a
a+b+ p +k=n
n
12 
0n
0x1
a0x2
b p 0xm0
k ,
 D = a
n
12
n
a 0
0x1
+
0
0x2
+ p +
0
0xm0
b
n
D~
L = D~D
§2 =
02
0x2
1
+
02
0x2
2
+ p +
02
0x2
m0
§2n
n = 2n
i
n!2n
L = a
q
n=0
(-1)nn§2n
L = D~D
G(x, xi) = exp a- 1
22
i
 7x - xi7 2 b

326
Chapter 7
Regularization Theory
and
(7.45)
Thus, the regularized solution described in Eq. (7.39) is attained by using a stabilizer
that includes all of its possible partial derivatives.
Using Eqs. (7.40) to (7.42) in Eq. (7.16) with  set equal to xi, we may write2
(7.46)
With the Green’s function G(x, xi) defined by the special form of Eq. (7.40), the regu-
larized solution given in Eq. (7.36) takes the form of a linear superposition of multivari-
ate Gaussian functions,
(7.47)
where, as before, the linear weights wi are themselves defined by Eq. (7.23).
In Eq. (7.47), the individual Gaussian members of the sum defining the approxi-
mating function Fl(x) are assigned different variances. To simplify matters, the condi-
tion i   for all i is often imposed on F(x). Even though the RBF networks thus
designed are of a somewhat restricted kind, they are still universal approximators (Park
and Sandberg, 1991).
7.4
REGULARIZATION NETWORKS
The expansion of the regularized approximating function F
(x) given in Eq. (7.36) in
terms of the Green’s function G(x, xi) centered at xi suggests the network structure
shown in Fig. 7.2 as a model for its implementation. For obvious reasons, this model
is called a regularization network (Poggio and Girosi, 1990a). The network consists of
three layers.The first layer is composed of input (source) nodes whose number is equal
to the dimension m0 of the input vector x (i.e., the number of independent variables of
the problem). The second layer is a hidden layer composed of nonlinear units that are
connected directly to all of the nodes in the input layer. There is one hidden unit for
each data point xi, i  1, 2, ..., N, where N is the size of the training sample.The activa-
tion functions of the individual hidden units are defined by a total of N Green’s func-
tions.Accordingly, the output of the ith hidden unit is G(x, xi).The output layer consists
of a single linear unit, being fully connected to the hidden layer. By “linearity,” we mean
that the output of the network is a linearly weighted sum of the outputs of the hidden
units. The weights of the output layer are the unknown coefficients of the expansion,
defined in terms of the Green’s functions G(x, xi) and the regularization parameter 
as seen in Eq. (7.23) or, equivalently, Eq. (7.35). Figure 7.2 depicts the architecture of the
F
(x) = a
N
i=1
wi exp a- 1
2i
2 x - xi2 b
a
q
n=0
(-1)n 2n
i
n!2n §2n exp a - 1
22
i
 x - xi2 b = (x - xi)
 =
a
a+b+p+k=n
(-1)n12
n
0n
0xa
10x2
b p 0xk
m0
 D~ = a
n
(-1)n12
n
a 0
0x1
+
0
0x2
+ p +
0
0xm0
b
n

Section 7.5
Generalized Radial-Basis-Function Networks
327
regularization network for a single output. Clearly, such an architecture can be readily
extended to accommodate any number of network outputs desired.
The regularization network shown in Fig. 7.2 assumes that Green’s function 
G(x, xi) is positive definite for all i. Provided that this condition is satisfied, which it is
in the case of the G(x, xi) having the Gaussian form given in Eq. (7.40), for example,
then the solution produced by this network will be an “optimal” interpolant in the
sense that it minimizes the functional 
. Moreover, from the viewpoint of approx-
imation theory, the regularization network has the following desirable properties (Pog-
gio and Girosi, 1990a):
(i) The regularization network is a universal approximator, in that it can approximate
arbitrarily well any multivariate continuous function on a compact subset of 
,
given a sufficiently large number of hidden units.
(ii) Since the approximation scheme derived from regularization theory is linear in
the unknown coefficients, it follows that the regularization network has the best
approximation property. This means that given an unknown nonlinear function f,
there always exists a choice of coefficients that approximates f better than all other
possible choices.The solution computed by the regularization network is therefore
optimal.
7.5
GENERALIZED RADIAL-BASIS-FUNCTION NETWORKS
The one-to-one correspondence between the training input vector xi and the Green’s
function G(x, xi) for i  1, 2, ..., N produces a regularization network that may some-
times be considered prohibitively expensive to implement in computational terms
m0
e(F)
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
G
G
Output
F(x)
Input
layer
Output
layer
Hidden
layer
of N Green’s
functions
x1
w1
wj
wN
x2
xm1
xm
G
Input
vector
x
FIGURE 7.2
Regularization network.

328
Chapter 7
Regularization Theory
for large N. Specifically, the computation of the linear weights of the network [i.e., the
coefficients of the expansion in Eq. (7.36)] requires the inversion of an N-by-N matrix,
which therefore grows polynomially with N (roughly as N3). Furthermore, the likeli-
hood of ill conditioning (i.e., large conditioning number) is higher for larger matrices;
the condition number of a matrix is defined as the ratio of the largest eigenvalue to the
smallest eigenvalue of the matrix. To overcome these computational difficulties, the
complexity of the network is reduced or the regularization parameter 
 is increased.
The reduced complexity of the RBF network, depicted in Fig. 7.3, is based on the
search for a suboptimal solution in a lower-dimensional space that approximates the
regularized solution of Eq. (7.36).This is done by using a standard technique known in
variational problems as Galerkin’s method. According to this technique, the approxi-
mated solution F*(x) is expanded on a finite basis, as shown by
(7.48)
where {(x, ti)	i  1, 2, ..., m1} is a new set of basis functions that we assume to be lin-
early independent without loss of generality (Poggio and Girosi, 1990a).Typically, the
number of basis functions is fewer than the number of data points (i.e., m1  N), and
the wi constitute a new set of weights.With radial-basis functions in mind, we set
(7.49)
This particular choice of basis functions is the only one which guarantees that in the
case of m1  N and
(7.49)
ti = xi,  i = 1, 2, ..., N
(x, ti) = G(7x - ti7),  i = 1, 2, ..., m1
F*(x) = a
m1
i=1
wi (x, ti)
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
„
„  1
„
„
Input
layer
Output
layer
Bias
Hidden layer
of m1 radial-
basis
functions
x1
w1
w0  b
wj
wm1
x2
xm1
xm
Input
vector
x
Output
F x(x)
FIGURE 7.3
Radial-basis-function network of reduced complexity.

Section 7.5
Generalized Radial-Basis-Function Networks
329
the correct solution of Eq. (7.39) is consistently recovered. Thus, using Eq. (7.49) in
Eq. (7.48), we may redefine F*(x) as
(7.50)
Given the expansion of Eq. (7.50) for the approximating function F*(x), the prob-
lem we now address is the determination of the new set of weights 
so as to min-
imize the new cost functional 
defined by
(7.51)
The first term on the right-hand side of Eq. (7.51) may be expressed as the squared
Euclidean norm 
, where
(7.52)
(7.53)
(7.54)
The desired-response vector d is N dimensional as before. However, the matrix G of
Green’s functions and the weight vector w have different dimensions; the matrix G
is now N by m1 and therefore no longer symmetric, and the vector w is m1 by 1. From
Eq. (7.50), we note that the approximating function F* is a linear combination of
the Green’s functions for the stabilizer D. Accordingly, in light of the material cov-
ered previously, we may express the second term on the right-hand side of Eq. (7.51)
as
(7.55)
where, in the second and third lines, we made use of the definition of an adjoint opera-
tor and Eq. (7.16), respectively. The new matrix G0 is a symmetric m1-by-m1 matrix,
 = wTG0w
 = a
m1
j=1 a
m1
i=1
wjwiG(tj, ti)
 = c a
m1
i=1
wiG(x, ti), a
m1
i=1
wtid
x
 = c a
m1
i=1
wiG(x, ti), D~D a
m1
i=1
wiG(x; ti)d
x
 7DF*7 2 = 6 DF*, DF*7x
 w = [w1, w2, ... , wm1]T
 G = ≥
G(x1, t1)
G(x1, t2)
p
G(x1, tmi)
G(x2, t1)
G(x2, t2)
p
G(x2, tmi)
o
o
o
G(xN, t1)
G(xN, t2)
p
G(xN, tmi)
¥
 d = [d1, d2, p , dN]T
7d - Gw7 2
e(F*) = a
N
i=1
adi - a
m1
j=1
wjG(7xi - tj7)b
2
+ 
7DF*7 2
e(F*)
{wi}m1
i=1
 = a
m1
i=1
wiG(7x - ti7)
 F*(x) = a
m1
i=1
wiG(x, ti)

330
Chapter 7
Regularization Theory
defined by
(7.56)
Thus, the minimization of Eq. (7.51) with respect to the weight vector w yields the fol-
lowing result (see Problem 7.4):
Solving this equation for the weight vector , we obtain
(7.57)
As the regularization parameter 
 approaches zero, the optimized weight vector
converges to the pseudoinverse (minimum-norm) solution to the underdetermined
least-squares data-fitting problem for m1  N, as shown by
(7.58)
where G is the pseudoinverse of matrix G (Golub and Van Loan, 1996); that is,
(7.59)
Weighted Norm
The norm in the approximated solution of Eq. (7.50) is ordinarily intended to be a
Euclidean norm. When, however, the individual elements of the input vector x belong
to different classes, it is more appropriate to consider a general weighted norm, the
squared form of which is defined by
(7.60)
where C is an m0-by-m0 norm-weighting matrix and m0 is the dimension of the input
vector x.
Using the definition of weighted norm, we may now rewrite the approximation to
the regularized solution given in Eq. (7.50) in the following more generalized form
(Lowe, 1989; Poggio and Girosi, 1990a):
(7.61)
The use of a weighted norm may be interpreted in two ways.We may simply view
it as applying an affine transformation to the original input space. In principle, allowing
for such a transformation cannot degrade results from the default case, since it actually
corresponds to an identity norm-weighting matrix. On the other hand, the weighted
F*(x) = a
m1
i=1
wiG(7x - ti7 C)
 = xTCTCx
 7x7 2
C = (Cx)T(Cx)
G+ = (GTG)-1GT
w = G+d,  
 = 0
wˆ
wˆ = (GTG + 
G0)-1GTd
wˆ
(GTG + 
G0)wˆ = GTd
G0 = ≥
G(t1, t1)
G(t1, t2)
p
G(t1, tm1)
G(t2, t1)
G(t2, t2)
p
G(t2, tm1)
o
o
o
o
G(tm1, t1)
G(tm1, t2)
p
G(tm1, tm1)
¥

Section 7.6
The Regularized Least-Squares Estimator: Revisited
331
norm follows directly from a slight generalization of the m0-dimensional Laplacian in the
definition of the pseudodifferential operator D in Eq.(7.44).The use of a weighted norm
may also be justified in the context of Gaussian radial-basis functions on the following
grounds: A Gaussian radial-based function 
centered at ti and with norm-
weighting matrix C may be expressed as
(7.62)
where the inverse matrix 
is defined by
The generalized multivariate Gaussian distribution of Eq. (7.62) has an exponent equal
to the Mahalanobis distance; see Eq. (27) of the introductory chapter. It is for this rea-
son that a kernel defined by Eq.(7.62) is referred to as the Mahalanobis kernel. This ker-
nel was also discussed in Problem 6.10 of Chapter 6.
The solution to the approximation problem given in Eq. (7.51) provides the frame-
work for the generalized radial-basis-function (RBF) network’s having the structure
shown in Fig.7.3.In this network,provision is made for a bias (i.e.,data-independent vari-
able) applied to the output unit.This is done simply by setting one of the linear weights
in the output layer of the network equal to the bias and treating the associated radial-
basis function as a constant equal to 1.
In structural terms, the generalized RBF network of Fig. 7.3 is similar to the
regularization RBF network of Fig. 7.2. However, they differ from each other in two
important ways:
1. The number of nodes in the hidden layer of the generalized RBF network of
Fig. 7.3 is m1, where m1 is ordinarily smaller than the number N of examples avail-
able for training. On the other hand, the number of hidden nodes in the regular-
ization RBF network of Fig. 7.2 is exactly N.
2. In the generalized RBF network of Fig. 7.3, the linear weights associated with the
output layer, the positions of the centers of the radial-basis functions, and the
norm-weighting matrix associated with the hidden layer are all unknown pa-
rameters that have to be learned. However, the activation functions of the hidden
layer in the regularization RBF network of Fig. 7.2 are known, being defined by a
set of Green’s functions centered at the training data points; the linear weights of
the output layer are the only unknown parameters of the network.
7.6
THE REGULARIZED LEAST-SQUARES ESTIMATOR: REVISITED
We first studied the least-squares estimator in Chapter 2, and then we made use of it in
Chapter 5 for computing the output layer of a suboptimal RBF network. In this section,
we revisit this relatively simple, yet effective, estimator one more time, with two points in
1
2 
- 1 = C TC
-1
 = expc- 1
2 (x - ti)T-1(x - ti)d
 G(7x - ti7 C = exp[-(x - ti)TCTC(x - ti)]
G(7x - ti7 C)

332
Chapter 7
Regularization Theory
mind: First, we will show that the formula of Eq. (7.57) includes the regularized least-
squares estimator as a special case.Second,we will show that,like any other kernel method,
regularized least-squares estimation is also governed by the Representer Theorem.
The Least-Squares Estimator Viewed as a Special Case of Eq. (7.57)
Given the training sample {xi, di}N
i1, the regularized cost function of a least-squares esti-
mator is defined by (see Chapter 2)
(7.63)
where the weight vector w is fixed throughout the training interval and  is the regular-
ization parameter.Comparing this cost function with that of Eq.(7.4),we readily see that
the regularizer is defined in terms of w simply as
in light of which we may immediately set the symmetric matrix G0 in Eq. (7.57) equal
to the identity matrix. Correspondingly, the line preceding Eq. (7.57) reduces to
Next,noting that since the least-squares estimator is linear and also lacks a hidden layer,
we may go on to express the transpose of the remaining matrix G of Eq. (7.53) simply as
(7.64)
Then, using this expression for GT and the expression of Eq. (7.52) for the desired
response d in the regularized solution of Eq. (7.57) for the weight vector , we get (after
some algebraic manipulations)
(7.65)
where
and
Equation (7.65) is a repeat of the formula defined in Eq. (2.29) for the maximum a pos-
teriori (MAP) estimator, which, as pointed out previously, also applies to the regularized
least-squares (RLS) estimator.
Using the expressions for the correlation matrix Rxx and the cross-correlation vec-
tor rdx, we may restate the formula of Eq. (7.65) directly in terms of the training sample
{xi, di}N
i1 as
rdx = a
N
i=1
xidi
Rxx = a
N
i=1
xixi
T
wˆ = (Rxx + I)-1rdx
wˆ
GT = [x1, x2, ..., xN]
(GTG + I)wˆ = GTd
= wTw
7 DF7 2 = 7 w7 2
e(w) = 1
2 a
N
i=1
(di - wTxi)2 + 1
2 7 w7 2

Section 7.6
The Regularized Least-Squares Estimator: Revisited
333
(7.66)
where X is the input data matrix
(7.67)
where the subscript N is the size of the training sample and the subscript M is the dimen-
sion of the weight vector 
. The vector d is the desired-response vector, defined in 
Eq. (7.52); it is reproduced here for convenience of presentation:
The Least-Squares Estimator Viewed in Light of the Representer Theorem
Next, viewing the least-squares estimator as a “kernel machine,” we may express its
kernel as the inner product
(7.68)
Then, invoking the Representer Theorem, discussed in Chapter 6, we may express the
approximating function realized by the regularized least-squares estimator as
(7.69)
where the expansion coefficients {ai,}N
i1 are uniquely defined by the training sample 
{xi, di}N
i1; the question is, how?
To address this question, we first use the identity
(7.70)
where X is the N-by-M input data matrix,d is the N-by-1 desired-response vector,
 is the
regularization parameter, and IN and IM are respective identity matrices of sizes N and
M; as a reminder, M is the size of the weight vector w. A proof of the matrix identity of
Eq. (7.70) is addressed in Problem 7.11. The right-hand side of this equation is recog-
nized as the formula for the optimized weight vector ; see Eq. (7.66). Using the identity
of Eq. (7.70), we may therefore express the approximating function realized by the regu-
larized least-squares estimator in terms of the weight vector 
and the input vector x as
(7.71)
which is now formulated as the inner product
(7.72)
 = aTk(x)
 F
(x) = kT(x)a
 = xTXT(XXT + 
IN)-1d
 F
(x) = xTwˆ
wˆ
wˆ
XT(XXT + 
IN)-1d = (XTX + 
IM)-1XTd
F
(x) = a
N
i=1
aik(x, xi)
 = xTxi,  i = 1, 2, ..., N
 k(x, xi) = x, xi
d = [d1, d2, ..., dN]T
wˆ
X = ≥
x11
x12
p
x1M
x21
x22
p
x2M
o
o
o
xN1
xN2
p
xNM
¥
wˆ = (XTX + 
I)-1XT d

334
Chapter 7
Regularization Theory
This formula is the matrix version of the representer theorem of Eq. (7.69), with the
following interpretations:
1. The row vector of kernels is defined in terms of the input vector x and the data
matrix X, as shown by
(7.73)
which is a 1-by-N row vector, as it should be.
2. The vector of expansion coefficients,a, is defined in terms of the estimator’s N-by-N
kernel matrix, or Gram, K, the regularization parameter 
, and the desired-
response vector d, as shown by
(7.74)
where
(7.75)
Two Equivalent Ways of Describing the Regularized
Least-Squares Estimator
From the discussion presented in this section, we see that, in fact, there are two ways of
describing the approximating function F
(x) realized by a regularized least-squares
estimator:
1. The formula of Eq. (7.71), which is defined in terms of the weight vector 
for a
given input vector x. Basically, this formulation is traced back to the normal equa-
tion for least-squares estimation, as discussed in Chapter 2.
2. The formula of Eq. (7.72), which is defined in terms of the estimator’s kernel.This
second formulation follows from the Representer Theorem derived in Chapter 6.
The important virtue of this formula is that it bypasses the need for computing
the weight vector of the RLS algorithm, which is the essence of the “kernel trick,”
discussed in Chapter 6.
The first viewpoint on regularized least-squares estimation, formulated in terms of the
normal equation,is well known in the statistics literature.However,the second viewpoint
formulated in terms of the representer theorem (well-known in the kernel-learning
literature) is new.
wˆ
 = ≥
x1
Tx1
x1
Tx2
p
x1
TxN
x2
Tx1
x2
Tx2
p
x2
Tx1
o
o
o
xN
Tx1
xN
Tx2
p
xN
TxN
¥
 K = XXT
 = (K + 
IN)-1d
 a = [a1, a2, ..., aN]T
 = xTXT = (Xx)T
 kT(x) = [k(x, x1), k(x, x2), ..., k(x, xN)]

Section 7.7
Additional Notes of Interest on Regularization
335
7.7
ADDITIONAL NOTES OF INTEREST ON REGULARIZATION
An attribute of Gaussian-based RBF networks is that they do lend themselves to rigorous
application of Tikhonov’s regularization theory,as demonstrated in Sections 7.4 and 7.5.The
same remark also applies to least-squares estimators, as shown in Section 7.6.
The aim of this section is to capitalize on lessons learned from least-squares estima-
tion and extend them to situations where the application of Tikhonov’s regularization
theory is a difficult undertaking.
Regression
Referring back to Eq. (7.63), it is insightful to reproduce this equation in the form:
(7.76)
Regularized
Empirical
Regularization
cost function
risk
term
e(w) = 1
2 a
N
i=1
(di - wT xi)2 + 1
2 
7w7 2
⎫⎬⎭
⎫
⎬
⎭
⎫
⎪⎪
⎪⎬
⎪⎪
⎪⎭
⎫⎬⎭
⎫
⎬
⎭
⎫
⎪
⎪
⎪⎬
⎪
⎪
⎪⎭
⎫⎬⎭
⎫⎪⎬⎪⎭
⎫
⎪
⎪
⎪⎬
⎪
⎪
⎪⎭
Viewed in the context of regression, the term 
has a certain intuitive appeal. Geo-
metrically speaking,in minimizing the cost function e(w),it turns out that inclusion of the
regularizer 
is helpful in finding the flattest function with desirable approximation 
properties. Indeed, it was with this aim in mind that in Section 4.14, we proposed mini-
mizing the cost function
Regularized
Empirical
Regularization
cost function
risk
term
as a plausible method for the regularization of a multilayer perceptron as a function
approximator.The fact of the matter is that it is mathematically diffcult to apply Tikhonov’s
regularization theory to a multilayer perceptron. Unlike a radial-basis function network,
the adjustable synaptic weights of a multilayer perceptron are distrubuted across the
hidden layer(s) as well as the output layer of the network.In practical terms,the adoption 
of 
as the regularizer is therefore a sensible option.
Maximum-Likelihood Estimation
Referring back to Chapter 2 dealing with the method of least-squares and Bayesian
estimation, therein we showed that the objective function of the maximum a posterior
(MAP) parameter estimator, operating in a Gaussian enviornment, may be expressed
as follows (see Eqs. (2.22) and (2.28)):
(7.77)
Log-
Log-
Log
posterior
likelihood
prior
L(w) = - 1
2 a
N
i=1
(di - wT xi)2 - 1
2 
7w7 2
1
2  w 2
e(w) = 1
2 a
N
i=1
(di - F(xi, w))2 + 1
2 
7w7 2
1
2 7 w 7 2
1
2 7 w7 2

336
Chapter 7
Regularization Theory
We may therefore view the term 
as prior information about the underlying 
structure of the MAP parameter estimator. The two equations, Eqs. (7.76) and (7.77),
respectively defining e(w) and L(w) have exactly the same mathematical structure,
except for the log-posterior L(w) being the negative of the regularized cost function
e(w). It follows therefore that the terms “regularization” and “prior information” play
the same role insofar as least-squares estimation, or equivalently, maximum-likehood
estimation in a Gaussian environment, is concerned.
Generalizing this insightful observation, on least-squares estimation, we may now
postulate a regularized maximum-likelihood estimator as the estimator whose objective
function is expressed as follows:
Regularized
Log-
Regularization
Log-likelihood likelihood
(penalty)
where w is the parameter vector to be optimized.Basically,what is being said here is that
when it is hard to formulate the prior for a maximum-likelihood estimation algorithm
aimed at estimating an unknown parameter vector w, subtraction  of the penalty term
from the logarithm of the likelihood function l(w) may provide a sensible option
for stabilizing the maximum-likelihood estimator.
7.8
ESTIMATION OF THE REGULARIZATION PARAMETER
The regularization parameter 
 plays a central role in the regularization theory of
radial-basis function networks, least-squares estimators, and support vector machines.
To derive the full benefit of this theory, we need an equally principled approach to the
estimation of 
.
To fix ideas, consider a nonlinear regression problem, described by a model whose
observable output yi at time-step i in response to an input vector xi is defined by
(7.78)
where f(xi) is a “smooth curve” and 
is a sample drawn from a white-noise process of
zero mean with covariance defined by
(7.79)
The problem is to reconstruct the underlying function of the model,
given the
training sample {(xi, yi)}N
i1.
Let F
(x) be the regularized estimate of 
for some value of the regularization
f(x)
f(xi),
[ik] = e 2
for k = i
0
otherwise
i
di = f(xi) + i,  i = 1, 2, ..., N

2 7 w7 2
L(w) = log l(w) - 1
2 
7w 7 2
1
2 7w7 2
⎫⎬⎭
⎫⎪⎬⎪⎭
⎫⎪⎬⎪⎭
parameter 
. That is, F
(x) is the minimizer of the Tikhonov functional formulated for
the nonlinear-regression problem as follows (see Eq. (7.4))
e(F) = 1
2 a
N
i=1
[di - F(xi)]2 + 
2 7DF(x)7 2

Section 7.8
Estimation of the Regularization Parameter
337
It is a nontrivial matter to choose a suitable value for 
, which controls the trade off
between two conflicting issues:
• “fidelity” to the data, measured by the empirical cost function 
;
• “smoothness” of the solution, measured by the regularizer 
.
A good choice for the regularization parameter 
 is the subject matter of this section.
Average Squared Error
Let R(
) denote the average squared error over a given data set between two functions—
the regression function 
pertaining to the model and the approximating function
f(x)
7DF(x)7 2
a
N
i=1
[di - F(xi)]2
Using this matrix notation, we may rewrite Eq. (7.80) in the form
(7.84)
where the N-by-1 vector f is given by
f = [f(x1), f(x2), ..., f(xN)]T
 = 1
N 7f - A(
)d7 2
 R(
) = 1
N 7f - F
7 2
F
(x) representing the solution for some 
—evaluated over the entire training sample.
That is,
(7.80)
The optimum  is the particular value of 
 that minimizes R(
).
Let F
(xk) be expressed as a linear combination of the given set of observables as
follows:
(7.81)
In matrix form, we may equivalently write
(7.82)
where d is the desired-response vector (i.e., the regression model’s response vector),
and
(7.83)
The N-by-N matrix A(
) is called the influence matrix.
A(
) = ≥
a11
a12
p
a1N
a21
a22
p
a2N
o
o
o
aN1
aN2
p
aNN
¥
F
 = [F
(x1), F
(x2), ..., F
(xN)]T
F
 = A(
)d
F
(xk) = a
N
i=1
aki(
)di
R(
) = 1
N a
N
i=1
[f(xi) - F
(xi)]2

338
Chapter 7
Regularization Theory
We can go one step further in our matrix formalism by rewriting Eq. (7.78) in the form
(7.85)
where
Hence, using Eq. (7.85) in Eq. (7.84) and then expanding terms, we obtain
(7.86)
where I is the N-by-N identity matrix. To find the expected value of R(
), we note the
following four points:
1. The first term on the right-hand side of Eq. (7.86) is a constant; it is therefore unaf-
fected by the expectation operator.
2. The expectation of the second term is zero, by virtue of the fact that the expecta-
tional error ei in Eq. (7.78) has zero mean for all i.
3. The expectation of the scalar 
is
(7.87)
where we first used the fact that the trace of a scalar is the same as the scalar itself
and then interchanged the order of expectation and trace operation.
4. We next use the following rule in matrix algebra: Given two matrices B and C of
compatible dimensions, the trace of BC is equal to the trace of CB. Thus, setting
we may rewrite Eq. (7.87) in the equivalent form
(7.88)
where in the last line we have made use of Eq. (7.79). Finally, noting that the trace
of AT(
)A(
) is the same as the trace of A2(
), we may write
(7.89)
Putting the results of these four points together, we may now express the expected value
of R(
) as
(7.90)
[R(
)] = 1
N 7I - A(
)f7 2 + 2
N tr[A2(
)]
[7A(
)f7 2] = 2tr[A2(
)]
 = 2tr[AT(
)A(
)]
 [7A(
)f7 2] = {tr[AT(
)A(
)  T]
B = T and C = AT(
) A(
) ,
 = [tr(TAT(
)A(
))]
 = tr([TAT(
)A(
)])
 [7A(
)7 2 = [TAT(
)A(
)]
7A(
) 7 2
 + 1
N 7A(
)7 2
 = 1
N 7(I - A(
))f7 2 - 2
N   TAT(
)(I - A(
))f
 R(
) = 1
N 7(I - A(
))f - A(
)   7 2
 = [1, 2, ..., N]T
d = f + 

Section 7.8
Estimation of the Regularization Parameter
339
The average squared error over a given training sample, R(
), however, is not a
practical measure, because it requires knowledge of the regression function f(x), the
function that is to be reconstructed.As an estimate of [R(
)], we introduce the follow-
ing definition (Craven and Wahba, 1979):
(7.91)
Subtraction of the last term in this formula is intended to make the estimate 
unbiased. Specifically, following a procedure similar to that described for deriving
Eq. (7.90) we may show the following:
(7.92)
Accordingly, the minimizer of the estimate 
can be taken as a good choice for the
regularization parameter 
.
Generalized Cross-Validation
A drawback of the estimate 
is that it requires knowledge of the noise variance 2.
Rˆ (
)
Rˆ (
)
[Rˆ (
)] = [R(
)]
Rˆ (
)
Rˆ (
) = 1
N 7(I - A(
))d7 2 + 2
N tr[A2(
)] - 2
N tr[(I - A(
))2]

In situations encountered in practice, 2 is usually not known.To deal with situations of
this kind, we may use the concept of generalized cross-validation that was originated by
Craven and Wahba (1979).
We begin by adapting the leave-one-out form of cross-validation to the problem at
hand. Specifically, let F 
[k](x) be the minimizer of the new functional
(7.93)
In this equation, the kth term [dk - F
(xk)] has been left out of the standard error func-
tion; by leaving out this term, we may use the ability of F 
[k](x), to “predict” the missing
data point dk as a measure of the goodness of 
; that is, the cross-validation is tested
on the single data point that is left out. Accordingly, we may introduce the measure of
goodness
(7.94)
which depends on the data alone. The ordinary cross-validation estimate of 
 is thus
defined to be the minimizer of V0(
) (Wahba, 1990).
A useful property of F 
[k] (xk) is that if the data point dk is replaced by the predic-
tion F 
[k](xk), and the original Tikhonov functional 
of Eq. (7.4) is minimized using
the data points d1, d2, ..., dk-1, dk, dk1, ..., dN, we get F
[k](xk) for the solution, since dk con-
tributes zero loss.This property, together with the fact that for each input vector x, the
minimizer F
(x), of 
depends linearly on dk, allows us to write
(7.95)
F[k]

 (xk) = F
(xk) + (F[k]

 (xk) - dk) 
0F
(xk)
0dk
e(F)
e(F)
V0(
) = 1
N a
N
k=1
[dk - F[k]

 (xk)]2
emodified(F) = 1
2 a
N 
i=1
 
iZ k
[di - F
(xi)]2 + 
2 7DF(x)7 2

340
Chapter 7
Regularization Theory
From Eq.(7.81),which defines the entries of the influence matrix A(
),we readily see that
(7.96)
where akk(
) is the kth diagonal element of A(
). Hence, using Eq. (7.96) in Eq. (7.95)
and solving the resulting equation for F 
[k] (xk), we obtain
(7.97)
Substituting Eq. (7.97) into Eq. (7.94), we may redefine V0(
) as
(7.98)
Typically, akk(
) is different for different k, which means that the data points in V0(
) are
not treated equally.To circumvent this undesirable feature of ordinary cross-validation,
Craven and Wahba (1979) introduced the generalized cross-validation (GCV), using
a rotation of coordinates.4 Specifically, the ordinary cross-validation function V0(
) of 
Eq. (7.98) is modified to
(7.99)
where the weights, #k, are themselves defined by
(7.100)
Then, the generalized cross-validation function V(
) becomes
(7.101)
Finally, using Eq. (7.81) in Eq. (7.101) yields the desired formula
(7.102)
which relies solely on quantities related to the training sample for its computation.
V(
) =
1
N
 7(I - A(
))d7 2
a 1
N
 tr[I - A(
)] b
2
V(
) =
1
N a
N
k=1
(dk - F
(xk))2
a 1
N
 tr[I - A(
)] b
2
#k = ±
1 - akk(
)
1
N
 tr[I - A(
)]
≤
2
V(
) = 1
N a
N
k=1
#k a
dk - F
(xk)
1 - akk(
) b
2
V0(
) = 1
N a
N
k=1
a
dk - F
(xk)
1 - akk(
) b
2
 =
F
(xk) - dk
1 - akk(
) + dk
 F[k]

 (xk) =
F
(xk) - akk(
)dk
1 - akk(
)
0F
(xk)
0dk
= akk(
)

Section 7.8
Estimation of the Regularization Parameter
341
An Optimal Property of the Generalized Cross-Validation Function V()
The expectation inefficiency of the method of generalized cross-validation is defined by
(7.103)
where R(
) is the average squared error over the training sample, given in Eq. (7.80).
Naturally, the asymptotic value of I* satisfies the condition
(7.104)
In other words, for large N, the average squared error R(
) with 
 estimated by mini-
mizing the generalized cross-validation function V(
) should be close to the minimum
possible value of R(
), which makes V(
) a good method for estimating 
.
Summarizing Comments
The general idea of cross-validation is to choose the regularization parameter 
 so as to
minimize the average squared error over the training sample, R(
). Unfortunately, this
cannot be accomplished directly, since R(
) involves the unknown regression function
f(x).With this being so, there are two possibilities that may be pursued in practice:
• If the noise variance 2 is known, we may use the minimizer of the estimate Rˆ (
)
lim
NS q I* = 1
I* =
[R(
)]
min

  [R(
)]
of Eq. (7.91) as the optimum choice of 
; it is optimum in the sense that it also
minimizes R(
).
• If 2 is not known, we may use the minimizer of the generalized cross-validation
function V(
) of Eq. (7.102) as a good choice of 
, which produces an expected
mean-square error that approaches its minimum possible value as the sample size
.
The important point to note here is that the theory justifying the use of generalized
cross-validation for estimating 
 is an asymptotic one. Good results can therefore be
expected only when the available training sample is large enough for the signal to be dis-
tinguishable from noise.
Practical experience with generalized cross-validation appears to show that it is
robust against nonhomogeneity of variances and non-Gaussian noise (Wahba, 1990).
However, the method is quite likely to produce unsatisfactory estimates of the regular-
ization parameter 
 if the noise process is highly correlated.
Some comments pertaining to the computation of the generalized cross-validation
function V(
) are in order. For given trial values of the regularization parameter 
, find-
ing the denominator term [tr[I  A(
)]/N]2 in the formula of Eq. (7.102) is the most
expensive part of the work involved in computing V(
).The “randomized trace method”
described in Wahba et al. (1995) may be used to compute tr[A(
)]; it is feasible to apply
this method to large-scale learning problems.
The material presented in this section has focused on cross-validation applied to
supervised learning for the purpose of estimating the regularization parameter 
.
N S q

342
Chapter 7
Regularization Theory
When we discuss semisupervised learning in Section 7.12, we find ourselves faced with
the estimation of two different regularization parameters. It would make for an inter-
esting study to extend the cross-validation theory covered herein to semisupervised
learning.
7.9
SEMISUPERVISED LEARNING
Starting from Chapter 1 on the perceptron and moving along up to this point in the
book, we have focused attention on supervised learning whose goal is to learn an
input–output mapping, given the training sample {xi di}N
i1. Such a data set is said to be
labeled, in the sense that each input vector xi is paired with a desired response or label
di for all i. From a practical perspective, the manual labeling of examples for the super-
vised training of a network is not only a time-consuming and expensive undertaking,
but also a process that is prone to errors. In contrast, the collection of unlabeled exam-
ples (i.e., examples without a desired response) is relatively inexpensive, and usually
there is a wealth of such examples readily available. Given these practical realities, how
do we exploit the availability of both labeled and unlabeled examples in the training of
networks? The answer to this challenging question lies in the use of semisupervised
learning.
In this new approach to learning,the input data set {xi}N
i1 is divided into two subsets:
1. a subset of data points denoted by {xi}l
i1, for which a corresponding set of labels
denoted by {di}l
i1 is supplied;
2. another subset of data points denoted by {xi}N
il1,for which the labels are unknown.
On this basis,we may view semisupervised learning as a new form of learning that resides
midway between supervised and unsupervised learning, which makes it a more difficult
learning task than supervised learning, but perhaps easier than unsupervised learning.
As a subject with many potential applications, semisupervised learning uses a wide
range of learning algorithms. In this chapter, we pursue the kernel approach based on
manifold regularization. By “manifold,” we mean a k-dimensional topological space
embedded in an n-dimensional Euclidean space where n is greater than k. If the func-
tions describing the manifold are partially differentiable, then the manifold is called a
differentiable manifold. We may thus view the concept of a manifold as the generaliza-
tion of the concept of a surface in 
and, by the same token, view the concept of a dif-
ferentiable manifold as the generalization of a differentiable surface in
.
The rationale for focusing on the kernel approach based on manifold regulariza-
tion is threefold:
1. The kernel approach for semisupervised learning fits naturally within the scope of
this chapter on regularization theory.
2. Manifold regularization provides a principled approach for the formulation of a
data-dependent, nonparametric kernel for semisupervised learning.
3. The use of manifold regularization has contributed encouraging results on some
classification tasks.
Simply put, kernel-based manifold regularization has the potential to make a signifi-
cant difference in semisupervised learning theory.
3
3

Section 7.10
Manifold Regularization: Preliminary Considerations
343
7.10
MANIFOLD REGULARIZATION: PRELIMINARY CONSIDERATIONS
A model of the semisupervised learning process is depicted in Fig. 7.4. To simplify the
presentation in this figure and the material to follow in the rest of the chapter, the term
“distribution”is used to refer to a “probability density function.” To proceed with the dis-
cussion, the model of Fig. 7.4 is abstracted in mathematical terms as follows:
1. The input space is denoted by x and assumed to be stationary; it supplies two sets
of inputs, one denoted by {xi}l
i1 and the other denoted by {xi}N
il1, both of which
are governed by a fixed distribution pX(x), which is assumed to pertain to a station-
ary process.
2. The teacher provides the label di as the desired response for every input vector x
in the set {xi}l
i1 received from the input space x in accordance with the conditional
distribution 
which is also fixed, but unknown.
3. The learning machine produces an output in response to the combined action of
the two subsets of data:
• Labeled data {xi, di}l
i1, received from the input space via the teacher, with the
joint distribution
(7.105)
According to this definition,pX(x) is the marginal distribution, obtained by inte-
grating out the dependence of the joint distribution pX,D(x, d) on the desired
response d.
pX,D(x, d) = pDX(dx)pX(x)
pDX(dx),
Input space x:
Distribution
pX(x)
Teacher
{xi}l
i1
Learning machine:
Weight vector
w  w
Machine output
F(x, w)
Joint distribution
pX,D(x, d)
Labeled data {xi, di}l
i1
Unlabeled data
{xi}N
il1
FIGURE 7.4
Model of the semisupervised learning process.

344
Chapter 7
Regularization Theory
• Unlabeled data {xi}N
il1, received directly from the input data space x, with the
distribution pX(x).
Unlike supervised learning, the training sample in semisupervised learning is therefore
made-up as follows:
(7.106)
Labeled
Unlabeled
Henceforth, it is assumed that there exists an identifiable relationship between
the distribution pX(x) and the conditional distribution 
for the principle of man-
ifold regularization to make a difference in improved function learning, be that in the
context of pattern classification or regression. Possible connections between these two
distributions may be formulated as two important assumptions (Chapelle et al., 2006);
1. The manifold assumption, which states the following:
The marginal distribution pX(x) underlying the input space x is supported on a
low-dimensional manifold.
The implication of this first assumption is that the conditional distribution function
varies smoothly (as a function of x) with respect to the underlying struc-
ture of the manifold.
A good question to raise here is: How can the manifold assumption be of
practical use? To answer this question, we have to remind ourselves of the curse-
of-dimensionality problem, discussed at some length in Chapter 4. Simply put, the
demand for a large number of examples needed for a learning task grows exponen-
tially with the dimensionality of the input space. If, however, the data are known
to lie in a low-dimensional manifold, then the curse-of-dimensionality can be
avoided by operating in a space of correspondingly low dimension.
The manifold assumption is justified for many physical processes. Consider,
for example, the speech production process, which may be viewed as a form of fil-
tering when seen as a sound source exciting a vocal tract filter. The vocal tract con-
sists of a tube of nonuniform cross-sectional area,beginning at the glottis and ending
at the lips.As the sound propagates along the local tract,the spectrum of the sound
signal is shaped by the frequency selectivity of the vocal tract; this effect is some-
what similar to the resonance phenomenon observed in organ pipes. The impor-
tant point to note here is that the space of sound (voice) signals is a low-dimensional
manifold, parameterized by the varying lengths and widths of the vocal tract.
2. The cluster assumption, which states the following:
The marginal distribution pX(x), according to which the examples for function learn-
ing are generated, is defined in such a way that if certain points are located in the same
“cluster,” then they are likely to have the same class or label.
This second assumption is considered to be a reasonable assumption to make.We
say so on the basis of the very existence of classes for a pattern-classification task
to be feasible.Specifically,if there are two objects belonging to two different classes,
then the likelihood of observing them both in the same cluster is relatively low.
pXD(xd)
pXD(xd)
(Training sample) = ({xi, di}l
i=1; {xi}N
i=l+1)

Section 7.11
Differentiable Manifolds
345
7.11
DIFFERENTIABLE MANIFOLDS
We begin the discussion on differentiable manifolds with the following intuitive notion:
A manifold is an abstract mathematical space in which every point has a local neighborhood
that resembles Euclidean space, but, in a global sense, has an underlying structure that is more
complicated than Euclidean space.
We may thus think of manifolds as an abstraction of the idea of smooth surfaces embed-
ded in Euclidean space.
In describing manifolds, the idea of dimension is extremely important. In general,
we may speak of a manifold of dimension n, or n-manifold, if the local neighborhood of
a point on the manifold is Euclidean of dimension n.
The local resemblance of a manifold to Euclidean space is typically assumed to be
close enough to permit the application of the customary rules of calculus to manifolds,
which makes the study of manifolds that much simpler. Expanding on this statement, let
denote the real numbers and 
denote their Cartesian product. In the study of man-
ifolds, the space 
is used in several senses: Sometimes 
merely means a topological
space; Sometimes 
denotes an n-dimensional vector space where operations are
performed in a continuous manner with respect to the topology; and sometimes 
is
identified simply with the Euclidean space.
Roughly speaking, a topological space is a geometric object. For a more precise
definition, we have to invoke set theory:
Let X be any set, and let t be a family of subsets of X.Then t is a topology on the set X pro-
vided that three conditions are satisfied:
(i) Both the set X and the empty set (i.e., a set with no elements) belong to t.
(ii) The intersection of any finite number of elements of t is an element of t.
(iii) The union of any collection of elements of t is an element of t.
If t is a topology as just defined, then the set X (involved in the definition) together with t
constitutes a topological space.
The elements of t are called the “open sets” of X. The essence of this definition
is that it enables us to define “continuous” mappings:A mapping (or function) 
between topological spaces is said to be continuous if the preimage
of any open
set A of Y is itself an open set of X.The mapping 
means that the set of points x
in X maps to A in Y.
With “differentiability” being an issue of special interest, let X, a subset of 
, be
an open set. An open set is defined as a set where the distance between any point in the
set and its edge is always greater than zero. Let 
with the ith component of the vec-
tor x being denoted by xi, and let the function f(x) map X onto . We may then make
the following statement:
The function 
is differentiable and said to be of class 
on the open set X, or simply f is
for a nonnegative integer k, if all the partial derivatives 
exist and are continuous
on X for 1  i  n and 0    k.
On this basis, for example, we may say that the function f is 
(i.e., infinitely differen-
tiable and therefore smooth) if it is 
for all k  0.
Ck
Cq
0f0x
i
Ck
Ck
f(x)

x  X,
n
f-1(A)
f-1(A)
f : X S Y
n
n
n
n
n


346
Chapter 7
Regularization Theory
We are still not quite ready for a formal definition of differentiable manifolds.
Rather, we need to introduce some other concepts, described as follows:
1. Homeomorphism
Consider a mapping 
between sets X and Y. If f has the property that for
every y in Y, there is exactly one x in X such that
as illustrated in Fig. 7.5, then f is called a bijection.
A bijection 
between two topological spaces X and Y is called a home-
omorphism if both f and its inverse 
are continuous mappings. Whenever such an f
can be found, X and Y are said to be homeomorphic to each other.
In physical terms, we may view homeomorphism as a continuous stretching and
bending of a topological space such that it is deformed into a new shape. For example,
a coffee mug and a doughnut are homeomorphic with respect to each other in that the
coffee mug can be continuously deformed into the doughnut, and vice versa.5 On the
other hand, a doughnut can never be deformed into a sphere, no matter what forms of
continuous stretching and bending are applied to it.
Intuitively, we may thus say that a homeomorphism maps points in one topologi-
cal space that are close together into an entirely different topological space such that
the corresponding points remain close together in the new topological space.
2. Diffeomorphism
For the definition of this concept, we require X and Y to be open sets in 
for some n.
Then, we say that 
is a diffeomorphism if the following two conditions hold:
(i) f is a homeomorphism, and
(ii) both f and 
are continuously differentiable.
In this case, X and Y are said to be diffeomorphic to each other. If f and 
are both
k-times continuously differentiable, then f is called a Ck-diffeomorphism.
3. Charts and Atlas
In studying the geography of the world, we find it convenient to use geographic maps
and charts as a substitute for examining the world as one whole entity. For a complete
f-1
f-1
f : X S Y
n
f-1
f : X S Y
f(x) = y
f : X S Y
FIGURE 7.5
A bijection 
.
f : X S Y
y1
x1
x2
x3
x4
f(x)  y
y2
y3
y4
Set Y
Set X

Section 7.11
Differentiable Manifolds
347
picture of the world, we use an atlas—namely, a family of geographic maps that covers
all the different parts of the world.
This nonmathematical view of geography of the world leads us to the following
intuitive procedure for constructing a topological manifold m:
(i) A family of overlapping “simple” spaces that covers the whole topological space
m is picked.
(ii) Each simple space is homeomorphic to an open set in 
. Each such homeomor-
phism is called a chart.
(iii) The charts are pieced together in a smooth manner.
Each chart therefore consists of a triple (X, Y, f ), where X is an open set in m, Y is an
open set in 
and 
is a homeomorphism.
A family of overlapping charts that covers all of m is called, for obvious reasons,
an atlas. Clearly, there is no unique way of constructing a manifold using this procedure.
In mathematical terms, we may follow up the definitions of charts and atlas with
the following pair of statements:
(i) Let the ith chart be denoted by the pair 
then the atlas is the intersection
of all the charts.
(ii) Any two charts 
and 
in the atlas must be compatible in the following
sense, as illustrated in Fig. 7.6:
• The part common to the two charts, shown as the shaded area in Fig. 7.6, must
be open.
• The shaded overlap map, denoted by fji, must be a C k diffeomorphism.
(Xj, fj)
(Xi, fi)
(Xi, fi);
f: X S Y
n
n
FIGURE 7.6
Illustrating the relationship between an atlas and its constituent charts. [This figure
is adapted from Abraham et al. (1988).]
0
y2
Yi
Yj
Xj
Xi
x2
Euclidean space
2
Euclidean space
2
Overlap map
fji
fi (Xi  Xj)
fj (Xi  Xj)
Map
fj
Manifold
m
Map
fi
0
y1
x1

348
Chapter 7
Regularization Theory
Note that fji is a mapping from the image set 
to the set 
, where
the symbol
denotes the product, or intersection, of two sets of interest. By requir-
ing each fji to be a C k-diffeomorphism, we are able to decide what is meant by a Ck-
differentiable function on m.
Differentiable Manifolds
At long last, we are now ready to define what we mean by a differentiable manifold:
A C k-differentiable manifold m of dimension n with atlas 
is a topo-
logical set where each Yi is an open set in 
, and such that the overlap mappings fji are all
C k-diffeomorphisms.
The implication of this definition is that for every n-dimensional point 
there is
an admissible chart (X, Y, f), where 
and f maps the open set X onto the open
subset Y of 
.
Why Be Interested in Manifolds for Learning?
To appreciate why the study of manifolds is important in the study of learning theory,
suppose we have a set of unlabeled examples denoted by 
where the dimen-
sionality of each example is n.These examples are represented as a set of data points in
an n-dimensional Euclidean space. Most unsupervised-learning algorithms operate only
on the ambient space, represented by the examples,
Suppose, however, that we
are able to construct a manifold of lower dimensionality than n, such that the true data
may reside on or around that manifold.Then, it may be possible to design a more effec-
tive semisupervised-learning algorithm by exploiting the underlying geometric proper-
ties of the manifold in addition to those of the ambient space. The idea we have just
described here is not just another method of data representation. Rather, it provides
the framework of a novel way of approaching problems of learning algorithms on man-
ifolds that are revealed through sampled data points (Belkin, 2003). However, for this
new approach to become a reality, we have to know the characterization of the mani-
fold that describes the intrinsic geometric structure of the input space;unfortunately,this
knowledge is rarely available in practice. To get around this difficulty, we try to con-
struct a model of the manifold, as described in the next couple of sections.
7.12
GENERALIZED REGULARIZATION THEORY
Tikhonov’s classical regularization theory, discussed in Section 7.3, incorporates a sin-
gle penalty function that reflects the ambient space, where the labeled examples are
generated. In this section, we extend this theory by incorporating a second penalty func-
tion that reflects the intrinsic geometric structure of the input space, where the unlabeled
examples are generated.In effect,this new theory,hereafter referred to as the generalized
regularization theory, applies to the framework of semisupervised function learning,
based on labeled as well as unlabeled examples. Moreover, it includes unsupervised
function learning, based on unlabeled examples alone, as a special case.
x1, x2, ... .
x1, x2, ... ,
n
x  X
x  m,
n
(Xi, Yi, fi), i = 1, p, I,
¨
fj(Xi ¨ Xj)
fi(Xi ¨ Xj)

Section 7.12
Generalized Regularization Theory
349
Appearing in pairs denoted by (x, d), the labeled examples are generated in accor-
dance with the joint distribution function pX,D(x, d),defined in Eq.(7.105).The unlabeled
examples,
, are generated in accordance with the marginal distribution function
pX(x). The underlying premise of the generalized regularization theory is the existence
of an identifiable relationship between these two distribution functions. Otherwise, it is
unlikely that the knowledge of the marginal pX(x) would be of practical use.Henceforth,
we make the following assumption:
If two input data points 
are close to each other in the intrinsic geometry of the mar-
ginal distribution function pX(x), then the conditional distribution function 
, eval-
uated at the data points x  xi and x  xj, behaves similarly.
To recast this assumption in a more tangible way that would lead to a practical solution,
we say the following:
If two data points xi and xj are very close to each other in the input space, the goal of semisu-
pervised function learning is to find a mapping, denoted by the function F(x), that forces the
corresponding output points F(xi) and F(xj) to lie on a real line in very close proximity to each
other with high likelihood.
To attain this goal, we need to introduce a new penalty term, over and above the penalty
term considered in the classical regularization theory.
To be specific, we extend the cost functional for the regularization of semisuper-
vised learning so a to include a new penalty term, giving us
(7.107)
where the two penalty terms are as follows:
1. The penalty term 
, under the control of the ambient regularization pa-
rameter 
A, reflects the complexity of the approximating function F in the ambi-
ent space. In particular, this penalty term is expressed in terms of the
reproducing-kernel Hilbert space (RKHS) representation of the feature space—
hence the subscript K.
2. The penalty term 
,under the control of the intrinsic regularization parameter 
I,
reflects the intrinsic geometric structure of the input space—hence the subscript I.
The subscript 
 in 
stands for the two regularization parameters, 
A and 
I. Note
also that in the first term on the right-hand side of Eq. (7.107), we have used l to denote
the number of labeled examples.
In the absence of the intrinsic penalty term 
, the minimizer of the cost func-
tional 
over the RKHS is defined by the classical representer theorem as
for 
(7.108)
according to which the problem is reduced to that of optimizing over a finite dimen-
sional space defined by the expansion coefficients 
We would like to extend this
theorem so as to include the intrinsic penalty term 
as well.
7F7 2
I
{ai}i=1
l
.

I = 0
F*

 (x) = a
l
i=1
aik(x, xi)
e
(F)
7F7 2
I
e
(F)
7F7 2
I
7F7 2
K
e
(F) = 1
2 a
l
i=1
(di - F(xi))2 + 1
2 
A7F7 2
K + 1
2 
I7F7 2
I
pXD(xd)
xi, xj  X
x  X

350
Chapter 7
Regularization Theory
With this aim in mind, we propose to model the intrinsic geometric structure of the
input space by a graph, for whose construction the unlabeled examples are sufficient, as
discussed next.
7.13
SPECTRAL GRAPH THEORY
Consider the training sample
which embodies N input data points, labeled as well as unlabeled. Given this training
sample, we proceed by constructing a weighted undirected graph consisting of N nodes
or vertices, one for each input data point, and a set of edges connecting adjacent nodes.
Any two nodes i and j are connected, provided that the Euclidean distance between
their respective data points xi and xj is small enough to satisfy the condition
(7.109)
for some prescribed constant 
The attractive feature of this adjancy criterion is twofold:
geometric insight and a naturally symmetric metric. However, we have to bear in mind
that the choice of a satisfactory value for the constant  may give rise to difficulty due
to the possibility of the graph having a multiplicity of connected components.
Let wij denote the weight of an undirected edge connecting nodes i and j. The
weights in the graph as a whole are usually real numbers, the selection of which must
satisfy three conditions:
1. symmetry, by which we mean
wij  wji
for all (i, j) pairs
2. connectivity, which means that the weight wij is nonzero if the pertinent nodes i and j are
connected, and zero otherwise;
3. nonnegativity, that is,
wij
0
for all (i, j) pairs
It follows therefore that the N-by-N weight matrix
is a symmetric, nonnegative-definite matrix, with all of its elements being nonnegative.
The rows and columns of the matrix W are indexed by the nodes of the graph, but their
ordering is unimportant. Hereafter, we refer to the undirected graph, characterized by
the weight matrix W, as graph G.
Let T denote an N-by-N diagonal matrix whose ii-th element is defined by
(7.110)
which is called the degree of node i. In words, the degree of a node i is equal to the sum
of all the elements in the ith row of the weight matrix W. The larger the degree tii is, the
tii = a
N
j=1
wij
W = {wij}

.
7xi - xj7 6 
X = {xi}N
i=1

Section 7.13
Spectral Graph Theory
351
more important the node i is. In the limiting case when tii is zero, the node i is said to be
isolated.
In terms of the weight matrix W and the diagonal matrix T, we now define the
Laplacian of the graph G as
(7.111)
If we assume that there are no self-loops—that is,wii  0 for all i—then for the ij-th element
of the Laplacian L, we have
(7.112)
It follows therefore that the Laplacian L is a symmetric matrix.
The graph Laplacian holds the key for the formulation of a suitable smoothing
functional to deal with the intrinsic penalty term 
, as described next.
Since the Laplacian L is a symmetric matrix, it has real eigenvalues.The subject of
eigendecomposition,involving the computation of the eigenvalues of a symmetric matrix
and their associated eigenvectors,is discussed in detail in Chapter 8.For now,we find it ade-
quate to focus on the Rayleigh coefficient of a symmetric matrix to evaluate the variational
characteristics of the eigenvalues of the Laplacian L.To this end, let f denote an arbitrary
vector-valued function of the input vector x, which assigns a real value to each node of the
graph G.We may then define the Rayleigh quotient of the Laplace operator L as the ratio
(7.113)
which expresses the ratio of two inner products:
1. the inner product of the function f and the matrix product Lf, where the Laplacian
L acts as an operator on the function f;
2. the inner product of the function f with itself, which is the squared Euclidean norm
of f.
Note that according to Eq. (7.113), the Laplacian L is a nonnegative-definite matrix,
which follows from Eq. (7.111).
With L being a N-by-N matrix, the Laplacian will have N real-valued eigenvalues,
arranged in increasing order, as shown by the set
which is called the eigenspectrum of the Laplacian L, or the eigenspectrum of the asso-
ciated graph G. It is not hard to show that the smallest eigenvalue 
0 is zero and that the
associated eignvector is the vector 1 whose N elements all have the value of 1. The
second smallest eigenvalue 
1 plays a critical part in spectral graph theory.
Notwithstanding the importance of 
1 and the other eigenvalues of the Laplacian
L, our primary interest in this chapter is to find a suitable measure for dealing with the
intrinsic penalty term 
.As we examine Eq. (7.113), the measure we are looking for
7F7 2
I

0  
1  p  
N-1

Rayleigh = f TLf
f Tf
7F7 2
I
lij = •
  tii
for j = i
- wij
for adjacent nodes i and j
  0
otherwise
L = T - W

352
Chapter 7
Regularization Theory
is to be found in the numerator of the Rayleigh quotient—namely, the quadratic term
fTLf. Accordingly, we introduce the smoothness functional
(7.114)
which is not only reasonable, but also intuitively satisfying.The vector-valued function
f is defined in terms of the training sample X as follows:
(7.115)
Hence, using Eqs. (7.112) and (7.115) in Eq. (7.114), we may also express the
smoothness functional by a double summation, as shown by
(7.116)
where wij is the weight of the edge connecting nodes i and j.
To complete the description of the smoothing function SG(f), we need a formula
for evaluating the edge weights of the graph G. In the spirit of kernel methods, we define
the weight wij, connecting nodes i and j, as a kernel function; that is,
(7.117)
This definition satisfies the symmetry, connectivity, and nonnegativity-preserving con-
ditions on the weight wij.An example of such a kernel is the Gaussian function
(7.118)
where 
the parameter under the designer’s control, is assumed to be the same for all
that is, all the kernels in the spectral graph.
The important point to note here in the context of semisupervised learning is
summed up as follows:
Application of spectral graph theory, through the combined use of Eqs. (7.117) and (7.118),
makes the learning machine for semisupervised learning into a kernel machine, whose hid-
den layer is determined by exploiting the intrinsic geometric structure of the input space that
is responsible for generating the unlabelled examples.
7.14
GENERALIZED REPRESENTER THEOREM
With the smoothing functional of Eq.(7.114) at hand,we are now ready to recast the cost
functional of Eq. (7.107) in the desired form
(7.119)
where the optimization is performed over a reproducing-kernel Hilbert space (i.e., F is in
the RKHS). Optimization of the cost functional 
admits an expansion of the form
(7.120)
F*
(x) = a
N
i=1
a*
i k(x, xi)
e(F)
e(F) = 1
2 a
l
i=1
(di - F(xi))2 + 1
2 A7 F7 2
K + 1
2 If TLf
i,
2,
k(xi, xj) = exp a-
7 xi - xj7 2
22
b
wij = k(xi, xj)
SG(F) = a
N
i=1 a
N
j=1
wij(F(xi) - F(xj))2
f = [F(x1), F(x2), ..., F(xN)]T
SG(F) = f TLf

Section 7.14
Generalized Representer Theorem
353
which incorporates both labeled and unlabeled examples (Belkin et al.,2006).As such,this
expansion may be viewed as the semisupervised generalization of the classical representer
theorem.
To prove this theorem, we proceed by first recognizing that any function F(x)
belonging to the reproducing-kernel Hilbert space may be decomposed into the sum
of two components: one component,
is contained in the span of the kernel func-
tions k(., x1), k(., x2), ..., k(., xN), and the other component,
is contained in the
orthogonal complement.That is to say, we write
(7.121)
where the ai are real coefficients. By invoking the reproducing property discussed in
Chapter 6, we find that the evaluation of the function F(x) at any data point xj, 1  j  N,
is independent of the orthogonal component, as shown by
(7.122)
We now note two points:
1. In the first term of Eq. (7.122), we have
2. The second term,
, is zero.
We may therefore go on to write
(7.123)
which shows that the empirical terms involving the standard regularized cost function
and the intrinsic norm in the minimization of Eq. (7.119) depend only on the expansion
coefficients 
and the Gram of the kernel function.
Next, we note that for all 
, this orthogonal component tends only to increase the
norm of the function in the reproducing-kernel Hilbert space—that is to say,
where the lower subscript K refers to the reproducing-kernel Hilbert space.
It follows, therefore, that for the minimization of the cost functional e
(F) to be
realized, we must have 
, proving the generalized representer theorem presented
in Eq. (7.120), where the use of a signifies the optimum setting.
F› = 0
 g a
N
i=1
aik(., xi)g
2
K
7F7 2
K = g a
N
i=1
aik(., xi)g
2
K
+ 7F›7 2
K
F›
{ai}i=1
N
F(xj) = a
N
i=1
aik(xi, xj)
F›, k(., xj)
k(., xi), k(., xj) = k(xi, xj)
= a
N
i=1
ai k(., xi), k(., xj) + F›, k(., xj)
= h a
N
i=1
aik(., xi), k(., xj) i + F›, k(., xj)
F(xj) = F, k(., xj)
= a
N
i=1
ai k(x, xi) + F›(x)
F(x) = F(x) + F› (x)
F›(x),
F(x),

354
Chapter 7
Regularization Theory
This simple form of the generalized representer theorem makes it possible to trans-
late an extrinsic-intrinsic regularization framework into a corresponding optimization
problem formulated over the finite-dimensional space of coefficients 
where N
is the total number of labeled and unlabeled examples (Belkin et al., 2006). In so doing,
we are enabled to invoke the machinery of kernel methods for solving difficult semi-
supervised learning problems, as illustrated in the next section.
7.15
LAPLACIAN REGULARIZED LEAST-SQUARES ALGORITHM
In Section 7.12, we introduced the concept of the smoothing functional, the formula-
tion of which embodies the Laplacian operator under the umbrella of spectral graph
theory. In particular, the defining formula of the smoothing functional is kernelized,
which makes the functional nonlinearly dependent on the input vector x, as shown in
Eqs. (7.116) through (7.118). Next we generalized the representer theorem so as to
accommodate the employment of both labeled and unlabeled examples.With these tools
at our disposal, the stage is now set for formulation of the Laplacian regularized least-
squares (LapRLS) algorithm (Belkin et al., 2006; Sindhwani et al., 2006). The practical
virtue of this new algorithm is twofold:
1. The training of the algorithm uses both labeled and unlabeled examples, thereby
enhancing applicability of the algorithm to a range of problems broader than that
attainable under supervised training alone.
2. Through kernelization, the algorithm can handle the recognition of nonlinearly
separable patterns,thereby broadening the applicability of least-squares estimation.
Basically, the LapRLS algorithm is derived by minimizing the cost functional of
Eq. (7.119) with respect to the function F(x). Using the representer theorem (for both
labeled and unlabeled examples), we have
in Eq. (7.119) and using matrix notations, we get
(7.124)
where we have introduced the following notations:
`
 = diag[1, 1, ... , 1, 0, 0, ... , 0]
J = N-by-N diagonal matrix, partially filled with l unity terms
 = [a1, a2, ... , aN]T
 a = N-by-1 expansion-coefficient vector
 = [d1, d2, ... , dl]T
 d = l-by-1 desired-response vector
e
(a) = 1
2
 (d - JKa)T(d - JKa) + 1
2
 
AaTKa + 1
2
 
IaTKLKa
F(x) = a
N
i=1
aik(x, xi)
{a*
i}N
i=1,

Section 7.15
Laplacian Regularized Least-Squares Algorithm
355
The l-by-l matrix K is the Gram, and L is the Laplacian graph matrix. Note that the
expression on the right-hand side of Eq. (7.124) is a quadratic function of the unknown
vector a—hence the designation of the cost function as e(a). Differentiating this
equation with respect to the vector a, collecting and simplifying terms, and then solv-
ing for the minimizer a*, we get
(7.125)
where we have used the symmetry of the Gram matrix K and the diagonal matrix J,
and I is the identity matrix; see Problem 7.16.
When we set the intrinsic regularization parameter I equal to zero (i.e.,l  N) and
note that under this condition the matrix J assumes the form of a standard diagonal
matrix, the formula of Eq. (7.125) reduces to that of Eq. (7.74) for the ordinary regular-
ized least-squares algorithm.
Table 7.1 presents a summary of the LapRLS algorithm, where we have four pa-
rameters under the designer’s control:
1. two regularization parameters A and I;
2. two graph parameters normal  and 
, where  is needed in the adjacency matrix
of Eq. (7.109) and 
is needed for the Gaussian kernel weight of Eq. (7.118).
Note that the algorithm does not require computation of the weight vector of the RLS
algorithm. Rather, we bypass the need for this computation by focusing on the param-
eter vector a in accordance with the representer theorem.
A distinguishing feature of the semisupervised-learning algorithm summarized in
Table 7.1 is the need to know the two regularization parameters A and I. As pointed
out previously, there is merit in extending the cross-validation theory of Section 7.8 to
cater to the estimation of A and I.
2
2
a* = (JK + AI + ILK)-1JTd
TABLE 7.1
Summary of the Laplacian Regularized Least-Squares Algorithm
Given quantities
Training samples 
, which are respectively labeled and unlabeled.
l is the number of the labeled examples and (N - l) is the number of unlabeled examples
Design parameters
 and 
:
spectral graph parameters
A and I:
regularization parameters, ambient and intrinsic
2
{xi, di}i=1
l
 and {xi}i=l+1
N
Computation
1. Construct the weighted undirected graph G with N nodes, using
• Eq. (7.109) for identifying the adjacent nodes of the graph, and
• Eqs. (7.117 and 7.118) for computing the edge weights.
2. Choose a kernel function k(x, .) and, using the training sample, compute the Gram
3. Compute the Laplacian matrix L of the graph G, using Eqs. (7.110) and (7.112).
4. Compute the optimum coefficient vector a*, using Eq. (7.125).
5. Finally, use the representer theorem of Eq. (7.120) to compute the optimized approximating 
function F
*(x).
K = {k(xi, xj)}i, j=1
N

356
Chapter 7
Regularization Theory
7.16
EXPERIMENTS ON PATTERN CLASSIFICATION USING
SEMISUPERVISED LEARNING
Pattern Classification Using Synthetic Data
To illustrate the pattern-classification capability of the Laplacian RLS algorithm,we per-
formed a toy experiment based on synthetic data extracted from the double-moon con-
figuration of Fig. 1.8. Specifically, two parameters of the experiment were held constant:
Vertical separation between the two moons, d  1.
Ambient regularization parameter, 
A  0.001.
The only variable parameter in the experiment was the intrinsic regularization parame-
ter 
I.
When 
I is set equal to zero exactly, the Laplacian RLS algorithm reduces to
the conventional RLS algorithm, for which the labeled data provide the only source
of information for learning. From an experimental point of view, the issue of inter-
est is therefore to see how the incorporation of unlabeled data in the semisuper-
vised learning process affects the decision boundary constructed by the Laplacian
RLS algorithm by varying 
I. To this end, the first part of the experiment explored
what happens to the decision boundary when the intrinsic regularization parameter

I is assigned a very small value. In the second part of the experiment, 
I was assigned
a large enough value to permit the unlabeled data sample to have its full impact on
the algorithm.
For both parts of the experiment,only two labeled data points were supplied to the
algorithm for each of the two classes, one class representing the top moon in Fig. 1.8
and the other class representing the bottom moon.The total size of the training sample,
consisting of both labeled and unlabeled data, was N  1000 data points; the size of the
testing sample was also 1000 data points.
(a) Intrinsic regularization parameter, 
I  0.0001. For this setting, Fig. 7.7 shows the
decision boundary constructed by the Laplacian RLS algorithm. Despite the very
small value assigned to 
I, it is still enough to markedly change the decision bound-
ary from that of the RLS algorithm (i.e.,
I  0).Recall from Figs.2.2 and 2.3 that the
RLS algorithm is characterized by a decision boundary in the form of a straight line
with a positive slope.
In terms of performance,there were a total of 107 misclassifications out of a total
of 1000 test data points; that is, the classification error rate was 10.7 percent.
(b) Intrinsic regularization parameter, 
I  0.1. In the second part of the experiment,
the intrinsic regularization parameter 
I was assigned the value 0.1, thereby en-
abling the Laplacian RLS algorithm to fully exploit the intrinsic information con-
tent of the unlabeled data.The locations of the labeled data points were exactly the
same as in the first part of the experiment.
To implement the Laplacian RLS algorithm, an RBF kernel for which we set
in Eq.(7.118).For construction of the Laplacian itself,a 20-nearest neighbor
graph was employed. In effect, the RBF network for implementing the Laplacian
RLS algorithm had a hidden layer of 20 computational nodes.
22 = 3

Section 7.16
Experiments on Pattern Classification
357
The results of the second part of the experiment, using this network configura-
tion, are shown in Fig. 7.8. Comparing this figure with Fig. 7.7, we see a dramatic
difference in the decision boundary constructed by the Laplacian RLS algorithm
for 
I  0.1 compared with 
I  0.0001. In particular, the two classes (i.e., the top
and bottom moons) are now separated from each other with no classification error.
This result is all the more impressive when it is recognized that for the setting
d  1,the two classes are not linearly separable,yet the Laplacian RLS algorithm
was able to separate them successfully with only two labeled data points per class.
This impressive performance of the Laplacian RLS algorithm is attributed to its
ability to fully use the intrinsic information contained about the two classes in the
unlabeled data.
The two parts of the experiment clearly demonstrate the tradeoffs between the
ambient and intrinsic forms of regularization, whereby a semisupervised-learning
process, exemplified by the Laplacian RLS algorithm, is enabled to generalize
from unlabeled examples with the aid of relatively few labeled examples.
10
5
0
5
10
15
20
10
5
0
5
10
x1
x2
FIGURE 7.7
Lapacian RLS classification of the double-moon of Fig. 1.8 with distance d  1
and two labeled data points per class that are denoted by the markers 
and 
. Intrinsic
regularization parameter 
I  0.0001.
¢

358
Chapter 7
Regularization Theory
Case Study: Pattern Classification Using USPS Data
Figure 7.9 shows learning curves for the RLS and Laplacian RLS algorithms for a real-
world image classification problem,using the United States Postal Service (USPS) data set.
The data set consists of 2,007 images of 10 classes of handwritten digits, each of which is
represented by a pixel vector of 256 dimensions. For each of the 10 classes, a separate
binary classifier was trained using the RLS and Laplacian RLS algorithms.Multiway clas-
sification was performed by taking the class with maximum output, by which we mean:
one-versus-rest multiclass classification. Figure 7.9 plots the mean classification error
rate and standard deviations of the two algorithms as a function of the number of labels
provided in the training set of 2,007 examples.Each point in Fig.7.9 was obtained by ran-
domizing over ten choices of labels.A Gaussian RBF kernel was used, for which, again
referring to the exponent in Eq.(7.118),we set 
equal to the mean pairwise Euelidean
distance between examples picked at random from the training sample. For the Lapla-
cian RLS, a ten-nearest-neighbor graph was used to construct the Laplacian; 
A  106
and 
I  0.01 were used as the regularization parameters. For the RLS, 
A was tuned
over a grid of values to return the optimized learning curve in Fig. 7.9. The results
22
10
5
0
5
10
15
20
10
5
0
5
10
x1
x2
FIGURE 7.8
Laplacian RLS classification of the double-moon of Fig. 1.8 with distance d  1
and two labeled data points that are denoted by the markers 
and 
.The intrinsic regularization
parameter 
I  0.1.
¢

Section 7.17
Summary and Discussion
359
presented in Fig. 7.9 demonstrate further that the use of unlabeled data significantly
boosts the performance of the Laplacian RLS as compared with the RLS algorithm.
7.17
SUMMARY AND DISCUSSION
Regularization theory is at the core of all learning algorithms.In this chapter,we presented
a detailed study of regularization theory, starting with Tikhonov’s classical regularization
theory for supervised learning using labeled examples,and finishing with the generalized reg-
ularization theory for semisupervised learning using labeled as well as unlabeled examples.
Tikhonov’s Regularization Theory
In its most basic form,the functional for Tikhonov’s regularization theory consists of two
terms, one being the empirical cost function, defined in terms of the labeled training
sample,and the other being the regularizing term,defined in terms of a differential oper-
ator applied to the approximating function.The differential operator acts as a smoothing
constraint imposed on the solution obtained by minimizing the cost functional with
40
60
80
100
120
140
10
15
20
25
30
35
Mean Error Rate
Number of Labeled Data Points
RLS
LapRLS
FIGURE 7.9
USPS data classification using (a) RLS algorithm and (b) Laplician RLS
algorithm. (Reproduced with the permission of Dr.Vikas Sindhwani.)

360
Chapter 7
Regularization Theory
respect to the unknown parameter (weight) vector of the approximating function. Cen-
tral to this optimal solution is the Green’s function,which serves as the kernel of a radial-
basis-function network. Keep in mind, however, that once the requirement of having a
fewer number of kernels than data points is imposed, the reduction in network com-
plexity becomes a critical factor in determining the smoothing regularizer.
Whatever the choice of the smoothing regularizer, in order to derive the full ben-
efits of Tikhonov’s regularization theory as applied to the design of regularized networks,
we need a principled approach for estimating the regularization parameter 
. The gen-
eralized cross-validation procedure described in Section 7.8 fills this special need.
Semisupervised Learning
With the study of regularization of supervised learning completed, we then turned our
attention to the regularization of semisupervised learning, the implementation of which
involves the use of both labeled and unlabeled examples.The cost functional now con-
sists of three terms:
• the empirical cost function, defined in terms of the labeled examples;
• the ambient regularization term, which reflects the complexity of the approximat-
ing function that requires the availability of labeled examples;
• the intrinsic regularization term, which reflects the intrinsic geometric structure of
the input space responsible for generating the unlabeled examples.
Correspondingly,there are now two regularization parameters,one for the ambient term
and the other for the intrinsic term.
As an important instance of the generalized regularization theory, we considered
the least-squares estimation problem, using labeled and unlabeled examples. Through
the use of a kernelized smoothing function that embodies a Laplacian operator and the
application of a generalized form of the representer theorem, we derived an algorithm
for the semisupervised version of regularized least-squares estimation; the algorithm,
called the Laplacian regularized least-squares algorithm, has two important practical
virtues:
1. For its training,the algorithm can handle both labeled and unlabeled examples,which
therefore broadens its application to more difficult pattern-recognition problems.
2. Through kernelization of the smoothing function, basic to the formulation of the
algorithm, the recognition of nonlinearly separable patterns becomes feasible with
least-squares estimation.
The practical feasibility of this algorithm was demonstrated by way of two insightful com-
puter experiments, one involving synthetic data and the other involving real-life data.
In Belkin et al.(2006),a semisupervised-learning algorithm is derived for Laplacian
support vector machines (LapSVM).The capability of this algorithm has been success-
fully tested on some real-life data sets. However, the algorithm requires the inver-
sion of a dense Gram, which, in turn, leads to a computational complexity on the
order of N3, where N is the size of the complete training sample (labeled as well as
unlabeled); moreover, as with the standard support vector machine, we still have

Notes and References
361
the quadratic-programming problem to solve, the complexity of which is also of the
order of N3. The LapRLS algorithm is computationally simpler than the LapSVM, as
there is no quadratic programming involved in its formulation. Most importantly, exper-
imental results appear to show that the performances of these two semisupervised-
learning machines are fairly close. From a practical perspective, it therefore appears that
the LapRLS is the better choice for solving semisupervised-learning problems.
Nevertheless, the computational complexity of the LapRLS is also of the order of
N3, which arises because of the inclusion of the intrinsic term in the cost functional.This
exceptionally high computational complexity could make it impractical to apply the
LapRLS to real-life problems involving large data sets. The development of scalable
semisupervised-learning machines is an active area of current research.
NOTES AND REFERENCES
1. Learning from Examples as an Ill-posed Inverse Problem. The fact that the task of machine
learning through examples would typically violate one or more of Hadamard’s conditions of
well-posedness may prompt us to view the learning process as an “ill-posed” inverse prob-
lem. However, from a rigorous mathematical perspective, the connection between learning
theory and the theory of ill-posed inverse problems, is not that straightforward.The mathe-
matical foundations of these two theories are different; basically, learning theory is intrinsi-
cally probabilistic in nature (regardless of whether we explicitly bring probability theory
into its formulation),while on the other hand,the theory of inverse problems could be viewed
as mostly deterministic.DeVito et al.(2005) present an insightful exposition of learning from
examples viewed as an ill-posed inverse problem.
2. Validation of Eq. (7.46). In basic terms, we may justify the validation of Eq. (7.46) by start-
ing with the unit Gaussian function
(A)
which is one-dimensional with 
. Basically, then, the requirement is to show that
(B)
where 
is the Dirac delta function centered at the origin x = 0.
To justify Eq. (B), the most expedient way is to exploit the fundamental properties of the
Fourier transform (Kammler, 2000). In particular, the differentiation property states
Differentiation of G(x) in the x-domain is equivalent to multiplication of 
, the Fourier
ˆG(s)
(x)
a
q
n=0
(-1)n
(2)-n
n!2n
02n
0x2n G(x) = (x)
2 = 12
G(x) = exp(-x2)
transform of G(x), by i2 s, where s is the spatial frequency and i is the square root of 1.
From Fourier-transform theory we also know that in mathematical terms, the unit Gauss-
ian function is its own Fourier transform. Specifically, for the G(x) of Eq. (A), we have
(C)
Therefore, taking the Fourier transform of the infinite summation on the left-hand side of
Eq. (B) yields (after the simplification of terms)
(D)
a
q
p=0
(-1)p (2)-p
p!2p
(i2s)2p exp(- s2) = exp(- s2) a
q
p=0
(2s)2p
p!
ˆG(s) = exp(- s2)


362
Chapter 7
Regularization Theory
The new infinite summation on the right-hand side of Eq. (D) is now recognized to be the
series expansion of the exponential exp(
). It follows therefore that the right-hand side
of Eq. (D) is equal to unity, the inverse transform of which is indeed the Dirac delta func-
tion 
. Justification of Eq. (B) is thereby established.
With Eq. (B) justified for the one-dimensional case, we may continue the justification of
Eq. (7.46) by invoking the method of induction by considering the two-dimensional case,
and so on.
3. Regularized Strict Interpolation. In Yee and Haykin (2001),a method for designing RBF net-
works is described, combining elements of two rigorous theories:
• the regularization theory of strict interpolation presented in Section 7.3, and
• the kernel regressive estimation theory presented in Chapter 5.
With respect to the latter theory,attention is focused on the Nadaraya-Watson regression esti-
mator. This method offers a principled approach for solving regression and pattern- 
classification problems in a way that is simple to encode and effective in performance. How-
ever,it is computationally demanding,particularly when the size of the training sample is large.
4. Generalized Cross-validation. To obtain generalized cross-validation from ordinary cross-
validation, we may consider a ridge regression problem described in Wahba (1990) as
(A)
where X is an N-by-N matrix of inputs, and the noise vector has a mean vector of zero and
a covariance matrix equal to 2I.Using the singular-value decomposition of X, we may write
where U and V are orthogonal matrices and D is a diagonal matrix. Let
and
(B)
We may then use U and V to transform Eq. (A) into the new form
The diagonal matrix D (not to be confused with a differential operator) is chosen to have
its singular values come in pairs. Then, there is an orthogonal matrix W for which WDWT
is a circulant matrix; that is,
which is constant along the main diagonal. Let
 = W
z = Wy~
 = £
a0
a1
p
aN-1
aN-1
a0
p
aN-2
aN-2
aN-1
p
aN-3
o
o
o
o
a1
a2
p
a0
§
 A = WDWT
y~ = D + ~
~ = UT
  = VT
 y~ = UTy
X = UDVT

y = X + 
(x)
s2

Problems
363
and
We may then use W to transform Eq. (B) into
(C)
The diagonal matrix D has “maximally uncoupled” rows, while the circulant matrix A has
“maximally coupled” rows.
With these transformations at hand,we may now state that generalized cross-validation
is equivalent to transforming the ridge regression problem of Eq. (A) into the maximally
coupled form of Eq. (C), then doing ordinary cross-validation on z, and finally transform-
ing the result back to the original coordinate system (Wahba, 1990).
5. Wikipedia Demonstration. To see the continuous deformation of a coffee mug into a dough-
nut and vice versa, visit the Wikipedia web site and search for “homeomorphism.”
PROBLEMS
Green’s Functions
7.1
The thin-plate-spline function is described by
.
.
Justify the use of this function as a translationally and rotationally invariant Green’s function.
7.2
The Gaussian function is the only radial-basis function that is factorizable. Using this prop-
erty of the Gaussian function, show that the Green’s function G(x, t) defined as a multivari-
ate Gaussian distribution may be factorized as
where xi and ti are the ith elements of the m-by-1 vectors x and t.
7.3
In Chapter 5, we identified three radial-basis functions—the Gaussian function, the inverse
multiquadric, and the multiquadric—all of which satisfy Micchelli’s theorem. However, the
class of Green’s functions includes only the first two radial-basis functions. Explain why
the multiquadric is excluded from the class of Green’s functions.
Regularized Networks
7.4
Consider the cost functional
which refers to the approximating function
Using the Frèchet differential, show that the cost functional 
is minimized when
(GTG + 
G0)wˆ = GTd
e(F*)
F*(x) = a
m1
i=1
wiG(7 x - ti7 )
e(F*) = a
N
i=1
cdi - a
m1
j=1
wiG(7 xj - ti7 )d
2
+ 
7 DF*7 2
G(x, t) = q
m
i=1
G(xi, ti)
for some  7  0 and r  
(r) = a r
 b
2
log a r
 b
z = A +  
 = W~

364
Chapter 7
Regularization Theory
where the N-by-m1 matrix G, the m1-by-m1 matrix G0, the m1-by-1 vector 
, and the N-by-1
vector d are defined by Eqs. (7.53), (7.56), (7.54), and (7.27), respectively.
7.5
Consider a regularizing term defined by
where
and the linear differential operator D is defined in terms of the gradient operator 
and the
Laplacian operator 
as
and
Show that
7.6
In Section 7.3, we derived the approximating function F
(x) of Eq. (7.47) by using the rela-
tionship of Eq. (7.46). In this problem, we wish to start with the relationship of Eq. (7.46)
and use the multidimensional Fourier transformation to derive Eq. (7.47). Perform this
derivation by using
as the definition of the multidimensional Fourier transform of the Green’s function G(x),
where 
and s is the m0-dimensional transform variable. Reference may be made
to a book on properties of the Fourier transform.
7.7
Consider the nonlinear regression problem described in Eq. (7.78). Let aik denote the ik-th
element of the inverse matrix (G  
I)-1. Hence, starting with Eq. (7.39), show that the esti-
mate of the regression function f(x) may be expressed as
where dk is the model output for the input xk, and
where 
is the Green’s function.
7.8
Spline functions are examples of piecewise polynomial approximators (Schumaker, 1981).
The basic idea behind the method of splines is as follows: An approximation region of
interest is broken up into a finite number of subregions via the use of knots; the knots can
G(77 )
%(x, xk) = a
N
i=1
G(7x - xi7 )aik,  k = 1, 2, ... , N
fˆ(x) = a
N
k=1
%(x, xk)dk
i = 2-1
G(s) =
3m0G(x) exp (-i sTx)dx
DF(x) = a
q
k=0
 2k
k!2k § 2kF(x)
D2k+1 = §(§
 2)k
D2k = (§
 2)k
§2
§
ak = 2k
k!2k
3m07DF(x) 7 2dx = a
q
k=0
ak 3m07DkF(x) 7 2dx
wˆ

Problems
365
be fixed, in which case the approximators are linearly parameterized, or they can be vari-
able, in which case the approximators are nonlinearly parameterized. In both cases, in each
region of the approximation a polynomial of at most degree n is used, with the additional
requirement that the overall function be n - 1 times differentiable. Polynomial splines are
relatively smooth functions that are easy to store, manipulate, and evaluate on a computer.
Among spline functions used in practice, cubic splines are perhaps the most popular.The
cost functional for a cubic spline, pertaining to a one-dimensional input, is defined by
where, in the language of splines,  denotes a smoothing parameter.
(a) Justify the following properties of the solution 
to this problem:
(1)
is a cubic polynomial between two successive values of x.
(2)
and its first two derivatives are all continuous, except at the boundary points,
where the second derivative of 
is zero.
(b) Since 
has a unique minimum, we must have
for any g drawn from the same class of twice-differentiable functions as f and for any real-
valued constant . This means that 
, interpreted as a function of , must have
e(f + g)
e(f + g)  e(f)
e(f)
f(x)
f(x)
f(x)
f(x)
e(f) = 1
2 a
N
i=1
[di - f(xi)]2 + 
2 3
xN
x1
c
d2f(x)
dx2
d
2
dx
a local minimum at   0. Hence, show that
which is the Euler–Lagrange equation for the cubic-spline problem.
7.9
Equation (7.75) defines the Gram, or kernel matrix, K of the method of least squares.
Demonstrate that the matrix K is nonnegative definite.
Regularized Least-Squares Estimation
7.10
Starting from Eq. (7.57), derive the normal equation of Eq. (7.65) for the regularized least-
squares estimator.
7.11
Justify the identity of Eq. (7.70), which involves the data matrix X and desired response
vector d.
Semisupervised Learning
7.12
Learning from labeled and unlabeled examples is an inverse problem. Justify the validity
of this statement.
Spectral Graph Theory
7.13
In Section 7.13, we made the statement that the smallest eigenvalue of the Laplacian L is
zero. Using the Rayleigh quotient of Eq. (7.113), justify this statement.
Generalized Representer Theorem
7.14
In the last line of Eq. (7.122), we used the following property of the representer theorem:
Prove this property.
h a
N
i=1
aik(., xi), k(., xj) i = a
N
i=1
aik(xi; xj)
3
xN
x1
a
d2f(x)
dx2
b a
d2g(x)
dx2
b dx = 1
2 a
N
i=1
[d - f(xi)]g(xi)

7.15
The representer theorem of Eq. (7.120) for both labeled and unlabeled examples and that
of Eq. (6.83) for labeled examples have the same mathematical form. Explain how the rep-
resenter theorem for semisupervised learning includes the representer theorem for super-
vised learning as a special case.
Laplacian Regularized Least-Squares Algorithm
7.16
(a) Derive the cost functional of Eq.(7.124).Then use this functional to derive the optimized
a* of Eq. (7.125).
(b) Show the details of how this minimizer includes that of Eq. (7.74) for labeled examples
as a special case.
7.17
Compare the computational complexity of the Laplacian regularized least-squares algo-
rithm with that of the regularized least-squares algorithm using labeled examples only.
7.18
In solving the method of least squares, we have the option of using either the normal equa-
tion or the representer theorem, as discussed in Section 7.6. However, in solving the semi-
supervised extension of this method, the representer theorem is the right choice. Explain
the rationale for this statement.
7.19
Implementation of the Laplacian RLS algorithm uses an RBF network. Discuss the dis-
tinctive roles of unlabeled and labeled examples in designing the hidden and output layers
of this network.
Computer Experiment
7.20
The small set of labeled data points may be viewed as the initializing condition of the
Laplacian RLS algorithm.As such, for a given unlabeled training sample, we would expect
the decision boundary constructed by the algorithm to depend on the locations of the labeled
data points. In this experiment, we explore this dependence, using synthetic data extracted
from the double-moon configuration of Fig. 1.8.
(a) One labeled data point per class. Repeat the computer experiment of Section 7.16 using
exactly the same specifications as before, but this time explore how the decision bound-
ary is affected by the locations of the two labeled data points, one per class.
(b) Two labeled data points per class. For the same settings as in part (a), repeat the exper-
iment using a pair of labeled data points per class.
Comment on the results of your experiments.
366
Chapter 7
Regularization Theory

ORGANIZATION OF THE CHAPTER
The purpose of this chapter is to describe how the implementation of principal-components
analysis can be accomplished through the use of unsupervised learning. The chapter is
organized as follows:
1. The introductory section, Section 8.1, highlights the essence of unsupervised
learning.
2. Section 8.2 describes the four principles of self-organization:self-reinforcement,com-
petition, cooperation, and structural information. These principles are particularly
important in the study of neural networks. The roles of these principles in the for-
mation of self-organized features in the visual system are illustrated in Section 8.3.
3. Section 8.4 develops the mathematical background of principal-components analysis
(PCA), using a perturbation-theoretic approach.
4. The next two sections address the formulation of two Hebbian-based on-line learn-
ing algorithms, with Section 8.5 focusing on Oja’s rule for maximum eigenfiltering
(i.e., extraction of the strongest principal component), followed by Section 8.6 on the
generalization of Oja’s rule. In Section 8.7, this generalized rule is applied to image
compression.
5. Section 8.8 discusses the kernelization of PCA, which makes it possible to extract
higher-order statistics of the input signal. Higher-order statistics constitute an intrin-
sic property of natural images, an issue discussed in Section 8.9. In order to tackle the
modeling of natural images in a computationally efficient manner, Section 8.10
describes an adaptive modification of kernelized PCA by building on the generalized
Hebbian algorithm.A case study on the denoising of a multipatch image is presented
in Section 8.10.
The chapter concludes with a summary and discussion in Section 8.11.
8.1
INTRODUCTION
An important property of neural networks is their ability to learn from their environ-
ment and,through training,to improve their performance in some statistical sense.Except
for the discussion on semisupervised learning in Chapter 7,the focus in previous chapters
367
C H A P T E R  8
Principal-Components
Analysis

has been on algorithms for supervised learning, for which a training sample is provided.
In supervised learning, the training sample embodies a set of examples on a desired
input–output mapping,which the network is required to approximate.In this and the next
three chapters, we take a new direction:We study algorithms for unsupervised learning.
In unsupervised learning, the requirement is to discover significant patterns, or
features, of the input data through the use of unlabeled examples. That it to say, the
network operates in accordance with the rule:
Learn from examples without a teacher.
The study of unsupervised learning may be pursued from two different perspectives:
(i) Self-organized learning, the formulation of which is motivated by neurobiological
considerations. In particular, the unsupervised-learning algorithm is supplied with
a set of rules of local behavior, and the requirement is to use the rules to compute
an input–output mapping with desirable properties. Here, the term “local” means
that the adjustments applied to the synaptic weights of each neuron in the network
are confined to the immediate local neighborhood of the neuron. In this context,
the modeling of a neural network used for self-organized learning tends to follow
neurobiological structures, recognizing that network organization is fundamental
to the brain.
(ii) Statistical learning theory, which is the approach that is traditionally pursued in
machine learning. The notion of locality of learning that is emphasized in neural
networks plays a lesser role in machine learning. Instead, in statistical learning
theory, a much greater emphasis is placed on well-established mathematical tools.
In this chapter, we study principal-components analysis (PCA)1 from both of these
two perspectives. PCA is a standard technique that is widely used for dimensionality
reduction is statistical pattern recognition and signal processing.
8.2
PRINCIPLES OF SELF-ORGANIZATION
Principle 1. Self-amplification
This first principle of self-organization states the following:
Modifications in the synaptic weights of a neuron tend to self-amplify in accordance with
Hebb’s postulate of learning, which is made possible by synaptic plasticity.
In a single neuron, the process of self-amplification, or self-reinforcement, is con-
strained by the requirement that modifications in the synaptic weights of the neuron
must be based on presynaptic and postsynaptic signals available at the local level. Basi-
cally, the requirements of self-reinforcement and locality specify a feedback mechanism,
by means of which a strong synapse leads to the coincidence of presynaptic and post-
synaptic signals. In turn, the synapse is increased in strength by such a coincidence.The
mechanism described here is the very essence of Hebbian learning.
Hebb’s postulate of learning is the oldest and most famous of all learning rules; it
is named in honor of the neuropsychologist Hebb (1949). Hebb’s book The Organiza-
tion of Behavior (1949) states the following (p. 62):
368
Chapter 8
Principal-Components Analysis

When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes
part in firing it, some growth process or metabolic changes take place in one or both cells
such that A’s efficiency as one of the cells firing B is increased.
Hebb proposed this change on the basis of associative learning (at the cellular level),
which would result in an enduring modification in the activity pattern of a spatially dis-
tributed “assembly of nerve cells.”
The statement on Hebb’s postulate of learning is made in a neurobiological context.
We may expand and rephrase it as a two-part rule (Stent, 1973; Changeux and Danchin,
1976):
1. If two neurons on either side of a synapse (connection) are activated simultaneously (i.e.,
synchronously), then the strength of that synapse is selectively increased.
2. If two neurons on either side of a synapse are activated asynchronolusly, then that synapse is
selectively weakened or eliminated.
Such a synapse is called a Hebbian synapse.2 (The original Hebbian rule did not
contain part 2.) More precisely, we define a Hebbian synapse as a synapse that uses a
time-dependent, highly local, and strongly interactive mechanism to increase synaptic effi-
ciency as a function of the correlation between the presynaptic and postsynaptic activities.
From this definition,we may deduce the following four key mechanisms (properties) that
characterize Hebbian learning (Brown et al., 1990):
1. Time-dependent mechanism. This mechanism refers to the fact that the modifica-
tions in a Hebbian synapse depend on the exact time of occurrence of the pre-
synaptic and postsynaptic signals.
2. Local mechanism. By its very nature, a synapse is the transmission site where
information-bearing signals (representing ongoing activity in the presynaptic and
postsynaptic units) are in spatiotemporal contiguity. This locally available infor-
mation is used by a Hebbian synapse to produce a local synaptic modification that
is input specific.
3. Interactive mechanism. The occurrence of a change in a Hebbian synapse depends
on signals on both sides of the synapse.That is,the Hebbian form of learning depends
on “true interaction”between presynaptic and postsynaptic signals in the sense that
we cannot make a prediction from either one of these two activities by itself. Note
also that this dependence or interaction may be deterministic or statistical in nature.
4. Conjunctional or correlational mechanism. One interpretation of Hebb’s postulate
of learning is that the condition for a change in synaptic efficiency is the conjunction
of presynaptic and postsynaptic signals.Thus,according to this interpretation,the co-
occurrence of presynaptic and postsynaptic signals (within a short interval of time)
is sufficient to produce the synaptic modification.It is for this reason that a Hebbian
synapse is sometimes referred to as a conjunctional synapse. For another interpreta-
tion of Hebb’s postulate of learning,we may think of the interactive mechanism char-
acterizing a Hebbian synapse in statistical terms.In particular,the correlation over time
between presynaptic and postsynaptic signals is viewed as being responsible for a
synaptic change.Accordingly,a Hebbian synapse is also referred to as a correlational
synapse. Correlation is indeed the basis of learning (Chen et al., 2007).
Section 8.2
Principles of Self-Organization
369

To formulate Hebbian learning in mathematical terms,consider a synaptic weight wkj
of neuron k with presynaptic and postsynaptic signals denoted by xj and yk, respectively.
The adjustment applied to the synaptic weight wkj at time-step n is expressed in the
general form
(8.1)
where f(.,.) is a function of both postsynaptic and presynaptic signals.The signals xj(n)
and yk(n) are often treated as dimensionless. The formula of Eq. (8.1) admits many
forms,3 all of which qualify as Hebbian. In what follows, we consider the simplest form
of Hebbian learning, described by
(8.2)
where  is a positive constant that determines the rate of learning. Equation (8.2) clearly
emphasizes the correlational nature of a Hebbian synapse. It is sometimes referred to
as the activity product rule. From the representation of Eq. (8.2), we see that the
repeated application of the input signal (presynaptic activity) xj leads to an increase in
yk and, therefore, exponential growth that finally drives the synaptic connection into
saturation. At that point, no new information will be stored in the synapse, and selec-
tivity is lost. Some mechanism is therefore needed to stabilize the self-organized behav-
ior of the neuron, which is taken care of by the second principle.
Principle 2. Competition
This second principle of self-organization states the following:
The limitation of available resources, in one form or another, leads to competition among the
synapses of a single neuron or an assembly of neurons, with the result that the most vigorously
growing (i.e., fittest) synapses or neurons, respectively, are selected at the expense of the oth-
ers.
This second principle is made possible by synaptic plasticity (i.e., adjustability of a synap-
tic weight).
For a given single neuron to stabilize, for example, there must be competition
among its synapses for limited resources (e.g., energy) in such a way that the increase
in strength of some synapses in the neuron is compensated for by a decrease in strength
in others. Accordingly, only the “successful” synapses can grow in strength, while the
less successful synapses tend to weaken and may eventually disappear altogether.
At the network level, a similar competitive process may prevail, by proceeding as
follows (Rumelhart and Zipser, 1985):
• To begin with, the neurons in the network are all the same, except for some ran-
domly distributed synaptic weights; hence, the neurons respond differently to a
given set of input patterns.
• A specific limit is imposed on the “strength” (e.g., the sum of synaptic weights) of
each neuron in the network.
• The neurons compete with each other in accordance with a prescribed rule for the
right to respond to a given subset of inputs; consequently, only one output neuron,
¢wkj(n) = yk(n)xj(n)
¢wkj(n) = f(yk(n), xj(n))
370
Chapter 8
Principal-Components Analysis

or one neuron per group, is active at a time.The neuron that wins the competition
is called a winner-takes-all neuron.
We thus find that, through this competitive-learning process, the individual neurons of
the network assume the role of feature detectors for different classes of input patterns.
Whereas in Hebbian learning, several output neurons of a neural network may be
active simultaneously, in competitive learning only a single output neuron, or one out-
put neuron per group, is active at any one time. It is this characteristic of competitive
learning that makes it highly suited to discovering statistically salient features that could
be used to classify a set of input patterns.
Principle 3. Cooperation
The third principle of self-organization states the following:
Modifications in synaptic weights at the neural level and in neurons at the network level tend
to cooperate with each other.
The cooperation may arise because of synaptic plasticity or because of simultaneous
stimulation of presynaptic neurons brought on by the existence of the right conditions
in the external environment.
Consider first the case of a single neuron:A single synapse on its own cannot effi-
ciently produce favorable events. Rather, there has to be cooperation among the neu-
ron’s synapses, making it possible to carry coincident signals strong enough to activate
that neuron.
At the network level,cooperation may take place through lateral interaction among
a group of excited neurons. In particular, a neuron that is firing tends to excite the neu-
rons in its immediate neighborhood more so than those farther away from it. Over the
course of time, we typically find that a cooperative system evolves through a sequence
of small changes from one configuration to another, until an equilibrium condition is
established.
It is also important to note that in a self-organizing system that involves both com-
petition and cooperation, competition always precedes cooperation.
Principle 4. Structural Information
The fourth, and last, principle of self-organization states the following:
The underlying order and structure that exist in an input signal represent redundant infor-
mation, which is acquired by a self-organizing system in the form of knowledge.
Structural information contained in the input data is therefore a prerequisite to self-
organized learning. It is also noteworthy that whereas self-amplification, competition,
and cooperation are processes that are carried out within a neuron or a neural network,
structural information, or redundancy, is an inherent characteristic of the input signal.
Consider, for example, a voice or video signal.When such a signal is sampled at a
high rate,the resulting sampled signal is correspondingly found to exhibit a higher degree
of correlation between adjacent samples.The meaning of this high correlation is that, on
Section 8.2
Principles of Self-Organization
371

average, the signal does not change rapidly from one sample to the next, which, in turn,
means that the signal contains structured, or redundant, information. In other words,
correlation is synonomous with structure and redundancy.
To appreciate the importance of structure, suppose that all the redundant infor-
mation contained in a signal is completely removed.What we are then left with is a com-
pletely nonredundant signal that is unpredictable and may therefore be indistinguishable
from noise. Given this kind of an input, no self-organizing or unsupervised-learning sys-
tem can function.
Summarizing Remarks
The neurobiologically motivated rules of self-organization hold for the unsupervised
training of neural networks, but not necessarily for more general learning machines that
are required to perform unsupervised-learning tasks. In any event, the goal of unsuper-
vised learning is to fit a model to a set of unlabeled input data in such a way that the
underlying structure of the data is well represented. For the model to be realizable, how-
ever, it is essential that the data be structured.
8.3
SELF-ORGANIZED FEATURE ANALYSIS
To illustrate the principles of self-organization just described, consider information pro-
cessing in the visual system, which is performed in stages. In particular, simple features
such as contrast and edge orientation are analyzed in the early stages of the system,
whereas more elaborate, complex features are analyzed in later stages. Figure 8.1 shows
the gross structure of a modular network that resembles the visual system. In Linsker’s
model of the mammalian visual system (Linsker, 1986), the neurons of the network in
Fig. 8.1 are organized into two-dimensional layers, with local forward connections from
one layer to the next; the layers are labeled A, B, and C in Fig. 8.1. Each neuron receives
information from a limited number of neurons located in an overlying region of the pre-
vious layer, which constitutes the receptive field of that neuron. The receptive fields of
the network play a crucial role in the synaptic development process because they make
it possible for neurons in one layer to respond to spatial correlations of the neuronal
activities (i.e., structural information) in the previous layer.Two assumptions of a struc-
tural nature are made in Linsker’s model:
1. The positions of the synaptic connections are fixed for the entire neural develop-
ment process once they have been chosen.
2. Each neuron acts as a linear combiner.
The model combines aspects of Hebb-like synaptic modification with competitive and
cooperative learning in such a way that the network’s outputs optimally discriminate
among an ensemble of inputs, with the self-organized learning proceeding on a layer-by-
layer basis. That is, the learning process permits the self-organized feature-analyzing
properties of each layer to develop fully before proceeding to the next layer; this form
of learning is an example of learning features of features.
372
Chapter 8
Principal-Components Analysis

In Linsker’s model, simulation results are presented that are qualitatively similar
to properties found in the early stages of visual processing in cats and monkeys. In light
of the highly complex nature of the visual system, it is indeed remarkable that the sim-
ple model considered by Linsker is capable of developing similar feature-analyzing neu-
rons.The point is not to imply that feature-analyzing neurons in the mammalian visual
system develop in exactly the manner described in Linsker’s model. Rather, such struc-
tures may be produced by a relatively simple layered network whose synaptic connec-
tions develop in accordance with a Hebbian form of learning,combined with competition
and cooperation, thereby providing practical justification for the principles of self-
organization.
8.4
PRINCIPAL-COMPONENTS ANALYSIS: PERTURBATION THEORY
A common problem in statistical pattern recognition is that of feature selection, or fea-
ture extraction. Feature selection refers to a process whereby a data space is transformed
into a feature space that, in theory, has exactly the same dimension as the original data
space. However, the transformation is designed in such a way that the data set may be
represented by a reduced number of “effective” features, yet retain most of the intrin-
sic information content of the original data; in other words, the data set undergoes a
dimensionality reduction. To be specific, suppose we have an m-dimensional vector x
and wish to transmit it using l numbers, where l  m, which implies that data compres-
sion is an intrinsic part of feature mapping. If we simply truncate the vector x, we will
Section 8.4
Principal-Components Analysis: Perturbation Theory
373
Layer
A
Layer
B
Layer
C
FIGURE 8.1
Layout of modular self-adaptive Linsker’s
model, with overlapping receptive fields.

cause a mean-square error equal to the sum of the variances of the elements eliminated
from x, so we ask the following question:
Does there exist an invertible linear transformation T such that the truncation of Tx is opti-
mum in the mean-square-error sense?
Clearly, the transformation T should have the property that some of its components
have low variance. Principal-components analysis (also known as the Karhunen–Loe`ve
transformation in communication theory) maximizes the rate of decrease of variance
and is therefore a right choice.
Let X denote an m-dimensional random vector representing the environment of
interest.We assume that the random vector X has zero mean, or
where is the statistical expectation operator. If X has a nonzero mean, we subtract the
mean from it before proceeding with the analysis. Let q denote a unit vector (i.e., a vec-
tor with a Euclidean norm of unity), also of dimension m, onto which the vector X is to
be projected. This projection is defined by the inner product of the vectors X and q, as
shown by
(8.3)
subject to the constraint
(8.4)
The projection A is a random variable with a mean and variance related to the statistics
of the random vector X. Under the assumption that the random vector X has zero mean,
it follows that the mean value of the projection A is zero, too:
The variance of A is therefore the same as its mean-square value, so we may write
(8.5)
The m-by-m matrix R is the correlation matrix of the random vector X, formally
defined as the expectation of the outer product of the vector X with itself, as shown by
(8.6)
We observe that the correlation matrix R is symmetric, which means that
From this property, it follows that if a and b are any m-by-1 vectors, then
(8.7)
aTRb = bTRa
RT = R
R = [XXT]
 = qTRq
 = qT[XXT]q
 = [(qTX)(XTq)]
 2 = [A2]
[A] = qT[X] = 0
7q7 = (qTq)12 = 1
A = XTq = qTX

[X] = 0
374
Chapter 8
Principal-Components Analysis

From Eq. (8.5), we see that the variance 2 of the projection A is a function of the
unit vector q; we may thus write
Section 8.4
Principal-Components Analysis: Perturbation Theory
375
(8.8)
on the basis of which we may think of %(q) as a variance probe.
Eigenstructure of Principal-Components Analysis
The next issue to be considered is that of finding those unit vectors q along which %(q)
has extremal or stationary values (i.e., local maxima or minima), subject to a constraint
on the Euclidean norm of q. The solution to this problem lies in the eigenstructure of the
correlation matrix R. If q is a unit vector such that the variance probe %(q) has an
extremal value, then for any small perturbation q of the unit vector q, we find that, to
a first order in q,
Now, from the definition of the variance probe given in Eq. (8.8), we have
where in the second line, we have made use of Eq. (8.7). Ignoring the second-order term
(q)TR q and invoking the definition of Eq. (8.8), we may therefore write
(8.9)
But %(q  q) is equal to %(q) to a first-order of approximation; it therefore follows
that we must have
(8.10)
Just any perturbations q of q are not admissible; rather, we are restricted to use
only those perturbations for which the Euclidean norm of the perturbed vector q  q
remains equal to unity; that is,
or, equivalently,
Hence, in light of Eq. (8.4), to a first order in q, we require that
(8.11)
This means that the perturbations q must be orthogonal to q, and therefore only a
change in the direction of q is permitted.
(q)Tq = 0
(q + q)T(q + q) = 1
7q + q7 = 1
(q)TRq = 0
 = %(q) + 2(q)TRq
 %(q + q) = qTRq + 2(q)TRq
 = qTRq + 2(q)TRq + (q)TRq
 %(q + q) = (q + q)TR(q + q)
%(q + q) = %(q)
 = qTRq
 %(q) = 2

By convention, the elements of the unit vector q are dimensionaless in a physical
sense. If, therefore, we are to combine Eqs. (8.10) and (8.11), we must introduce a scal-
ing factor 
 into the latter equation with the same dimensions as the entries in the core-
lation matrix R. We may then write
or, equivalently,
(8.12)
For the condition of Eq. (8.12) to hold, it is necessary and sufficient to have
(8.13)
This is the equation that governs the unit vectors q for which the variance probe %(q)
has extremal values.
Equation (8.13) is recognized as the eigenvalue problem, commonly encountered
in linear algebra (Strang, 1980).The problem has nontrivial solutions (i.e.,
) only
q Z 0
Rq = 
q
(q)T(Rq - 
q) = 0
(q)TRq - 
(q)Tq = 0
376
Chapter 8
Principal-Components Analysis
for special values of 
 that are called the eigenvalues of the correlation matrix R. The
associated values of q are called eigenvectors. Being symmetric, a correlation matrix is
characterized by real, nonnegative eigenvalues.The associated eigenvectors are unique,
assuming that the eigenvalues are distinct. Let the eigenvalues of the m-by-m matrix R
be denoted by 
1, 
2, ..., 
m, and the associated eigenvectors be denoted by q1, q2, ..., qm,
respectively.We may then write
(8.14)
Let the corresponding eigenvalues be arranged in decreasing order as
(8.15)
so that 
1  
max.Let the associated eigenvectors be used to construct the m-by-m matrix,
(8.16)
We may then combine the set of m equations represented in Eq. (8.14) into the single
equation
(8.17)
where  is a diagonal matrix defined by the eigenvalues of matrix R, that is,
(8.18)
The matrix Q is an orthogonal (unitary) matrix in the sense that its column vectors (i.e.,
the eigenvectors of R) satisfy the conditions of orthonormality:
(8.19)
Equation (8.19) requires distinct eigenvalues. Equivalently, we may write
QTQ = I
qT
i  qj = e1,
j = i
0,
j Z i
 = diag[
1, 
2, ..., 
j, ..., 
m]
RQ = Q
Q = [q1, q2, ..., qj, ..., qm]

1 7  
2 7  p 7  
j 7  p 7  
m
Rqj = 
jqj,  j = 1, 2, ..., m

from which we deduce that the inverse of matrix Q is the same as its transpose, as
shown by
(8.20)
This means that we may rewrite Eq. (8.17) in a form known as the orthogonal similarity
transformation:
(8.21)
In the expanded form, it is written as
(8.22)
The orthogonal similarity transformation of Eq. (8.21) transforms the correlation matrix
R into a diagonal matrix of eigenvalues.The correlation matrix R may itself be expressed
in terms of its eigenvalues and eigenvectors as
(8.23)
which is referred to as the spectral theorem. The outer product qiqi
T is of rank 1 for
all i. Equations (8.21) and (8.23) are two equivalent representations of the eigen-
decomposition of the correlation matrix R.
Principal-components analysis and eigendecomposition of matrix R are basically
one and the same, just different ways of viewing the problem.This equivalence follows
from Eqs. (8.8) and (8.22), where we see that the variance probes and eigenvalues are
indeed equal, as shown by
(8.24)
We may now summarize the two important findings we have made from the eigen-
structure of principal-components analysis:
• The eigenvectors of the correlation matrix R pertaining to the zero-mean random
vector X define the unit vectors qj, representing the principal directions along
which the variance probes %(qj) have their extremal values.
• The associated eigenvalues define the extremal values of the variance probes %(uj).
Basic Data Representations
Let the data vector x denote a realization (i.e., sample value) of the random vector X.
Let a denote a realization of the random variable A.
With m possible solutions for the unit vector q, we find that there are m possible
projections of the data vector x to be considered.Specifically,from Eq.(8.3),we note that
(8.25)
where the aj are the projections of x onto the principal directions represented by the unit
vectors qj.The aj are called the principal components;they have the same physical dimen-
sions as the data vector x. The formula in Eq. (8.25) may be viewed as one of analysis.
aj = qT
j x = xTqj,  j = 1, 2, ..., m
%(qj) = 
j,  j = 1, 2, ..., m
 = QQT
 R = a
m
i=1

iqi qT
i
qT
j Rqk = e 
j,
k = j
0,
k Z j
QTRQ = 
QT = Q-1
Section 8.4
Principal-Components Analysis: Perturbation Theory
377

To reconstruct the original data vector x exactly from the projections aj, we pro-
ceed as follows: First, we combine the set of projections {aj0 j  1, 2, ..., m} into a single
vector, as shown by
(8.26)
Next, we premultiply both sides of Eq. (8.26) by the matrix Q and then use the relation
QQT  I.Accordingly, the original data vector x may be reconstructed as
(8.27)
which may be viewed as the formula for synthesis. In this sense, the unit vectors qj rep-
resent a basis of the data space. Indeed, Eq. (8.27) is nothing but a coordinate transfor-
mation, according to which a point x in the data space is transformed into a
corresponding point a in the feature space.
Dimensionality Reduction
From the perspective of statistical pattern recognition, the practical value of principal-
components analysis is that it provides an effective technique for dimensionality reduc-
tion. In particular, we may reduce the number of features needed for effective data
representation by discarding those linear combinations in Eq. (8.27) that have small
variances and retain only those terms that have large variances. Let 
1, 
2, ..., 
l denote
the largest l eigenvalues of the correlation matrix R. We may then approximate the data
vector x by truncating the expansion of Eq. (8.27) after l terms as follows:
(8.28)
Given the original data vector x, we may use Eq. (8.25) to compute the set of principal
components retained in Eq. (8.28) as follows:
(8.29)
The linear projection of Eq. (8.29) from 
to 
(i.e., the mapping from the data space
to the feature space) represents an encoder for the approximate representation of the
l
m
≥
a1
a2
o
al
¥ = ≥
qT
1
qT
2
o
qT
l
¥  x,  l  m
 = [q1, q1, ..., qj]≥
a1
a2
o
al
¥ ,  l  m
 xˆ = a
l
j=1
ajqj
 = a
m
j=1
aj qj
 x = Qa
 = QTx
 = [xTq1, xTq2, ..., xTqm]T
 a = [a1, a2, ..., am]T
378
Chapter 8
Principal-Components Analysis

data vector x, as illustrated in Fig. 8.2a. Correspondingly, the linear projection of 
Eq.(8.28) from 
to 
(i.e.,the mapping from the feature space back to the data space)
represents a decoder for the approximate reconstruction of the original data vector x,
as illustrated in Fig. 8.2b. Note that the dominant (i.e., largest) eigenvalues 
1, 
2, ..., 
l
do not enter the computations described in Eqs.(8.28) and (8.29);they merely determine
the number of principal components used for encoding and decoding, respectively.
The approximation error vector e equals the difference between the original data
vector x and the approximating data vector , as shown by
(8.30)
Substituting Eqs. (8.27) and (8.28) into Eq. (8.30) yields
(8.31)
The error vector e is orthogonal to the approximating data vector
as illustrated in Fig.8.3.
In other words, the inner product of the vectors 
and e is zero.This property is shown
xˆ
xˆ ,
e =
a
m
i=l+1
aiqi
e = x - xˆ
xˆ
m
l
Section 8.4
Principal-Components Analysis: Perturbation Theory
379
(a)
Input
(data)
vector
Encoder
x1
x2...
xm
Vector of
principal
components
a1
a2...
al
qT
1
qT
2...
qT
l
(b)
xˆ1
xˆ2...
xˆl
Reconstructed
data
vector
Vector of
principal
components
Decoder
a1
a2...
al
[q1, q2, . . ., ql]
FIGURE 8.2
Illustration of two phases of
principal-components analysis: (a) Encoding.
(b) Decoding.
e
0
x
xˆ
FIGURE 8.3
Illustration of
the relationship between
vector x, its reconstructed
version 
and error vector e.
xˆ,

by using Eqs. (8.28) and (8.31) to obtain:
(8.32)
where we have made use of the second condition in Eq. (8.19). Equation (8.32) is a
mathematical statement of the principle of orthogonality.
The total variance of the m components of the data vector x is, via Eq. (8.8) and
the first line of Eq. (8.22),
(8.33)
where j
2 is the variance of the jth principal component aj. Correspondingly, the total
variance of the l elements of the approximating vector 
is
(8.34)
The total variance of the (l - m) elements in the approximation error vector 
is
therefore the difference between Eqs. (8.33) and (8.34):
(8.35)
The eigenvalues 
l1, ..., 
m are the smallest (m - l) eigenvalues of the correlation matrix
R; they correspond to the terms discarded from the expansion of Eq. (8.28) used to
construct the approximating vector . The closer all these eigenvalues are to zero, the
more effective the dimensionality reduction (resulting from the application of principal-
components analysis to the data vector x) will be in preserving the information content
of the original input data.Thus, to perform dimensionality reduction on some input data,
we do the following:
Compute the eigenvalues and eigenvectors of the correlation matrix of the input data vector,
and then project the data orthogonally onto the subspace spanned by the eigenvectors belong-
ing to the dominant eigenvalues.
This method of data representation is commonly referred to as subspace decomposition
(Oja, 1983).
EXAMPLE 1
Bivariate Data Set
To illustrate the application of principal-components analysis, consider the example of a bivari-
ate (two-dimensional) data set depicted in Fig. 8.4, where it is assumed that both feature axes are
approximately of the same scale.The horizontal and vertical axes of the diagram represent the nat-
ural coordinates of the data set. The rotated axes labeled 1 and 2 result from the application of
xˆ
a
m
j=l+1
2
j =
a
m
j=l+1

j
x - xˆ
a
l
j=1
2
j = a
l
j=1

j
xˆ
a
m
j=1
2
j = a
m
j=1

j
= 0
=
a
m
i=l+1 a
l
j=1
ai aj qT
i qj
for l 6 m
eTxˆ =
a
m
i=l+1
ai qT
i a
l
j=1
aj qj
380
Chapter 8
Principal-Components Analysis

principal-components analysis to this data set.From Fig.8.4,we see that projecting the data set onto
axis 1 captures the salient feature of the data—namely,the fact that the data set is bimodal (i.e.,there
are two clusters in its structure). Indeed, the variance of the projections of the data points onto axis 1
is greater than that for any other projection axis in the figure. By contrast, the inherent bimodal
nature of the data set is completely obscured when it is projected onto the orthogonal axis 2.
The important point to note from this simple example is that although the cluster structure
of the data set is evident from the two-dimensional plot of the raw data displayed in the frame-
work of the horizontal and vertical axes, this is not always the case in practice. In the more gen-
eral case of high-dimensional data sets, it is quite conceivable to have the intrinsic cluster structure
of the data concealed, and to see it we must perform a statistical analysis similar to principal-
components analysis (Linsker, 1988a).
■
Case Study
Digital Image Compression
Principal-components analysis provides a simple, yet efficient, method for compressing digital
images. A practical requirement for the storage, transmission, and feature extraction of digital
images is that the images be compressed. The PCA application of Fig. 8.5, using real-life data,
validates this statement (Holmström et al., 1997; Hyvärinen et al., 2001).
The leftmost column of Fig. 8.5 shows a set of 10 handwritten characters, namely, 0 to 9, each
of which is represented by a binary image consisting of a 32-by-32 matrix.When each of these images
is scanned on a row-by-row basis, a 1,024-by-1 vector is produced. For each one of the 10 charac-
ters,a set of about 1,700 handwritten samples was collected.The sample means (1,024-by-1 vectors)
Section 8.4
Principal-Components Analysis: Perturbation Theory
381
FIGURE 8.4
A cloud of data points is shown in two dimensions, and the density plots
formed by projecting this cloud onto each of two axes, 1 and 2, are indicated.The projection
onto axis 1 has maximum variance and clearly shows the bimodal, or clustered, character of
the data.
0
2
2
2
4
4
6
8
1
6
8

and the covariance matrices (1,024-by-1,024 matrices) were estimated with the use of standard pro-
cedures. For each of the 10 classes represented by a handwritten character, the first 64 principal
eigenvectors (components) of the covariance matrix were computed. The second column of the
figure presents the computed sample means.The remaining six columns show reconstructed images
for increasing values of the index l, which denotes the number of principal components in the
reconstruction formula of Eq. (8.28). In these images, the respective computed sample means
were added so as to properly scale the images for display.
Three important observations follow from the PCA results presented in Fig. 8.5:
•
As the reconstruction size l is progressively increased from 1 to 2, 5, 16, 32, and 64, the
reconstructed images increasingly resemble the original 10 handwritten characters.
•
By the time the reconstruction size l  64 is reached, every one of the reconstructed char-
acters is perfectly legible.
•
With a total of 1,024 possible principal components, the largest reconstruction size l  64
is a small percentage of the total.
Estimation of the Number of Principal Components
In the case study on digital image compression just presented, the number of principal
components (i.e., the size of the reduction in dimensionality) was determined experi-
mentally. For an analytical approach to this estimation problem, we may view it as a
model-selection problem.The minimum-description-length (MDL) criterion, discussed in
Chapter 2, provides a well-tested method for solving the problem.
382
Chapter 8
Principal-Components Analysis
FIGURE 8.5
Digital compression of
handwritten digits using principal-
components analysis. (This figure is
reproduced with the permission of 
Dr. Juha Karhunen.)

In Wax and Kailath (1985),the MDL criterion was applied to array-signal processing,
in which the primary motivation is that of ascertaining a signal’s direction of arrival in the pres-
ence of additive noise. To solve this problem, the MDL criterion was used to decompose
the input data space into two subspaces,one representing the signal subspace and the other
representing the noise subspace. In basic terms, decomposing the input data space into the
sum of a signal subspace and a noise subspace is nothing more than solving the dimension-
ality reduction problem, with the dimension of the signal subspace defining the underlying
number of principal eigenvectors (components) associated with the dominant eigenvalues.
8.5
HEBBIAN-BASED MAXIMUM EIGENFILTER
There is a close correspondence between the behavior of self-organized neural networks
and the statistical method of principal-components analysis. In this section, we demon-
strate this correspondence by establishing a remarkable result (Oja, 1982):
A single linear neuron with a Hebbian-type adaptation rule for its synaptic weights can evolve
into a filter for the first principal component of the input distribution.
To proceed with the demonstration, consider the simple neural model depicted in
Fig. 8.6a.The model is linear in the sense that the model’s output is a linear combination
of its inputs. The neuron receives a set of m input signals x1, x2, ..., xm through a corre-
sponding set of m synapses with weights w1, w2, ..., wm, respectively. The resulting out-
put y of the model is thus defined by
(8.36)
y = a
m
i=1
wixi 
Section 8.5
Hebbian-Based Maximum Eigenfilter
383
x1(n)
x2(n)
xm(n)
Output
y(n)
•
•
•
wm(n)
w2(n)
w1(n)
xi(n)
Óy(n)
y(n)
xi(n)
z1
wi(n  1)
wi(n)
(a)
(b)
Input
vector
x(n)
FIGURE 8.6
Signal-flow graph
representation of maximum eigenfilter.
(a) Graph of Eq. (8.36). (b) Graph of
Eqs. (8.41) and (8.42).

Note that in the situation described here, we have a single neuron to deal with, so there
is no need to use double subscripts to identify the synaptic weights of the neuron.
Derivation of the Maximum Eigenfilter
In accordance with Hebb’s postulate of learning, a synaptic weight 
varies with time,
growing strong when the presynaptic signal xi and postsynaptic signal y coincide with
each other. Specifically, we write
(8.37)
where n denotes discrete time and  is the learning-rate parameter. However, as pointed
out in Section 8.2, this learning rule in its basic form leads to unlimited growth of the
synaptic weight wi, which is unacceptable on physical grounds. We may overcome this
problem by incorporating some form of normalization in the learning rule for the adap-
tation of synaptic weights. The use of normalization has the effect of introducing com-
petition among the synapses of the neuron over limited resources, which, from Principle
2 of self-organization, is essential for stabilization. From a mathematical point of view,
a convenient form of normalization is to recast Eq. (8.37) in the new form:
(8.38)
where the summation in the denominator extends over the complete set of synapses
associated with the neuron. Assuming that the learning-rate parameter  is small, we
may expand the denominator of Eq. (8.38) as a power series, obtaining
(8.39)
In the third line of the right-hand side of Eq. (8.39), we used the constraint
for all n
and the input–output relation
Moreover, in the last line of Eq. (8.39), we used the approximation
assuming small .

(1 + 2y2(n))12 L 1 + y2(n)
y(n) = a
m
i=1
wi(n)xi(n)
a
m
i=1
w2
i(n) = 7w(n)7 2 = 1
= 1 + y2(n) + O(2)
= (1 + 2y2(n))12 + O(2)
= a a
m
i=1
w2
i(n) + 2y(n) a
m
i=1
wi(n)xi(n) b
12
+ O(2)
a a
m
i=1
(wi(n) + y(n)xi(n))2 b
12
= a a
m
i=1
(w2
i(n) + 2wi(n)y(n)xi(n)) b
12
+ O(2)
wi(n + 1) =
wi(n) + y(n)xi(n)
a a
m
i=1 (wi(n) + y(n)xi(n))2 b
12
wi(n + 1) = wi(n) + y(n)xi(n),  i = 1, 2, ..., m
wi
384
Chapter 8
Principal-Components Analysis

Next, dividing the numerator of Eq. (8.38) by the approximate expression for
the denominator given in Eq. (8.39), and again assuming small , we may go on to
write
Collecting common terms and ignoring second-order terms, we finally write
(8.40)
The term y(n)xi(n) on the right-hand side of Eq. (8.40) represents the usual Hebbian
modifications to synaptic weight wi and therefore accounts for the self-amplification
effect dictated by Principle 1 of self-organization. The inclusion of the negative term
y(n)wi(n) is responsible for stabilization in accordance with Principle 2, which requires
competition among the synapses of the neurons.The inclusion of this term modifies the
input xi(n) into a form that is dependent on the associated synaptic weight wi(n) and
the output y(n), as shown by
(8.41)
which may be viewed as the effective input of the ith synapse.We may now use the def-
inition given in Eq. (8.41) to rewrite the learning rule of Eq. (8.40) simply as
(8.42)
The overall operation of the neuron is represented by a combination of two signal-
flow graphs, as shown in Fig. 8.6. The signal-flow graph of Fig. 8.6a shows the depen-
dence of the output y(n) on the weights w1(n), w2(n), …, wm(n), in accordance with
Eq. (8.36). The signal-flow graph of Fig. 8.6b provides a portrayal of Eqs. (8.41) and
(8.42); the transmittance z-1 in the middle portion of the graph represents a unit-time
delay operator. The output signal y(n) produced in Fig. 8.6a acts as a transmittance in
Fig. 8.6b.The graph of Fig. 8.6b clearly exhibits the following two forms of internal feed-
back acting on the neuron:
• positive feedback for self-amplification and therefore growth of the synaptic weight wi(n),
influenced by the external input xi(n);
• negative feedback due to -y(n) for controlling the growth, thereby resulting in stabilization
of the synaptic weight wi(n) as it evolves across time.
The product term -y(n)wi(n) is related to a forgetting,or leakage factor, that is frequently
used in learning rules, but with a difference: The forgetting factor becomes more pro-
nounced with a stronger response y(n).This kind of control appears to have neurobio-
logical support (Stent, 1973).
wi(n + 1) = wi(n) + y(n)x¿i(n)
x¿i(n) = xi(n) - y(n)wi(n)
wi(n + 1) = wi(n) + y(n)(xi(n) - y(n)wi(n))
 = wi(n) + y(n)xi(n) - y2(n)wi(n) + O(2)
 = (wi(n) + y(n)xi(n))(1 - y2(n)) + O(2)
 = (wi(n) + y(n)xi(n))(1 + y2(n) + O(2))-1
 wi(n + 1) =
wi(n) + y(n)xi(n)
1 + y2(n) + O(2)
Section 8.5
Hebbian-Based Maximum Eigenfilter
385

Matrix Formulation of the Maximum Eigenfilter
For convenience of presentation, let
(8.43)
and
(8.44)
The input vector x(n) and the synaptic weight vector w(n) are typically both realiza-
tions of random vectors. Using this vector notation, we may rewrite Eq. (8.36) in the
form of an inner product as follows:
(8.45)
Similarly, we may rewrite Eq. (8.40) as
(8.46)
Hence, substituting Eq. (8.45) into Eq. (8.46) yields
(8.47)
The self-organized learning algorithm of Eq. (8.47) represents a nonlinear sto-
chastic difference equation, which makes convergence analysis of the algorithm mathe-
matically difficult.To pave the way for this convergence analysis, we will digress briefly
to introduce a general tool for convergence analysis of stochastic approximation algo-
rithms, assuming that the learning-rate parameter  is small.
Kushner’s Direct-Averaging Method
Examining the right-hand side of the self-organized learning algorithm of Eq. (8.47),
we make two observations:
1. The input vector x(n) appears in the form of an outer product x(n)xT(n),which rep-
resents the instantaneous value of the correlation matrix R—that is, Eq. (8.6) with-
out the expectation operator and with x(n) used as a realization of the random
vector X(n). Indeed, it is the term x(n)xT(n) that is responsible for the stochastic
behavior of the equation.
2. Since the algorithm is unsupervised,there is no external force acting on the algorithm.
From Eq. (8.47), it therefore follows that the characteristic matrix of the algorithm is
defined by
(8.48)
where I is the identity matrix. Post-multiplication of this characteristic matrix by the old
weight vector w(n) yields the updated weight vector w(n  1) of Eq. (8.47). Note that
since the term wT(n)(x(n)xT(n))w(n) is an inner product and therefore a scalar, we must
multiply it by the identity matrix I for matrix compatibility with the rest of the expres-
sion in Eq. (8.48).
I + [(x(n)xT(n)) - wT(n)(x(n)xT(n))w(n)I]
w(n + 1) = w(n) + [x(n)xT(n)w(n) - wT(n)(x(n)xT(n))w(n)w(n)]
w(n + 1) = w(n) + y(n)[x(n) - y(n)w(n)]
y(n) = xT(n)w(n) = wT(n)x(n)
w(n) = [w1(n), w2(n), ..., wm(n)]T
x(n) = [x1(n), x2(n), ..., xm(n)]T
386
Chapter 8
Principal-Components Analysis

Now, recall Kushner’s direct-averaging method from Chapter 3 on the least-mean-
square (LMS) algorithm, according to which we replace the characteristic matrix of
Eq. (8.48) by its expected value
(8.49)
This replacement is justified, provided that the learning-rate parameter  is small. In
effect, over the course of time the outer product x(n)xT(n) assumes the role of the cor-
relation matrix R.
We may thus state that the solution of the stochastic equation of Eq. (8.47) is effec-
tively close to the solution of the much simplified deterministic difference equation
(8.50)
for small .
Let
Then, using t to denote continuous time, we may now say that the incremental weight
change !w(n) in discrete time n is proportional to the rate of change of the weight w(t)
in continuous time t, as shown by the proportionality relationship
(8.51)
Thus,absorbing the learning-rate parameter  into the proportionality factor in Eq.(8.51),
and normalizing time t, accordingly, we may describe the evolution of the maximum
eigenfilter over time t by the ordinary nonlinear differential equation
(8.52)
where the quadratic term wT(t)Rw(t), being a scalar, makes the equation dimensionally
correct in matrix terms.
Asymptotic Stability of the Maximum Eigenfilter
Let w(t) be expanded in terms of the complete orthonormal set of eigenvectors of the cor-
relation matrix R, as shown by
(8.53)
where qk is the kth normalized eigenvector of the matrix R, and the coefficient 	k(t) is
the time-varying projection of the vector w(t) onto qk.Substituting Eq.(8.53) into Eq.(8.52),
and using the basic definitions from Section 8.4—namely,
and
qT
kRqk = 
k
Rqk = 
kqk
w(t) = a
m
k=1
	k(t)qk
dw(t)
dt
= Rw(t) - (wT(t)Rw(t))w(t)
dw(t)
dt
r ¢w(n)
¢w(n) = w(n + 1) - w(n)
w(n + 1) = w(n) + [R - wT(n)Rw(n)I]w(n)
I + [R - wT(n)Rw(n)I]
Section 8.5
Hebbian-Based Maximum Eigenfilter
387

where 
k is the eigenvalue associated with qk—we finally get
(8.54)
which, in turn, simplifies to
(8.55)
We have thus reduced the convergence analysis of the stochastic approximation algo-
rithm of Eq. (8.47) to the stability analysis of a nonlinear system of ordinary differen-
tial equations given by Eq. (8.55) involving the principal modes 	k(t).
Modified Langevin Equation
In light of the discussion presented in Chapter 3 on the adaptive LMS filter, we may
view Eq. (8.55), pertaining to the maximum eigenfilter, as a nonlinear modified form of
the Langevin equation without a driving force, as explained here:
(i) The Langevin equation is “modified” in the sense that we have the positive term

k	k(t) on the right-hand side of the equation, which provides amplification rather
than friction; this amplification term is Hebbian in origin.
(ii) The Langevin equation is “nonlinear” because of the second term 
,
which is attributed to competition among the synapses of the maximum eigenfilter.
(iii) The Langevin equation has no driving force because the maximum eigenfilter is
self-organized.
With no driving force, we therefore expect that, unlike the LMS filter, the maximum
eigenfilter will be absolutely convergent in an asymptotic sense. However, the non-
linearity of the maximum eigenfilter will make the study of its convergence behavior
more difficult in mathematical terms.
Convergence Analysis of the Langevin Equation
There are two cases to be considered in the convergence analysis,depending on the value
assigned to the index k. Case I corresponds to 1  k  m,and case II corresponds to k  1;
m is the dimension of both x(n) and w(n).These two cases are considered in turn.
Case I. 1  k  m.
For the treatment of this case, define
(8.56)
Hence,it is assumed that 
,which is true with probability 1 provided that the ini-
tial values w(0) are chosen at random. Then, differentiating both sides of Eq. (8.56) with
respect to time t, we get
	1(t) Z 0
k(t) = 	k(t)
	1(t),  1 6 k  m
-	k(t) a
l

l	l
2(t)
d	k(t)
dt
= 
k	k(t) - 	k(t) a
m
l=1

l	2
l(t),  k = 1, 2, ..., m
a
m
k=1
d	k(t)
dt
 qk = a
m
k=1

k	k(t)qk - c a
m
l=1

l	2
l(t)d a
m
k=1
	k(t)qk
388
Chapter 8
Principal-Components Analysis

(8.57)
Next, using Eq. (8.55) in Eq. (8.57), applying the definition of Eq. (8.56), and then sim-
plifying the result, we get
(8.58)
With the eigenvalues of the correlation matrix R assumed to be distinct and arranged
in decreasing order, we have
(8.59)
It follows therefore that the eigenvalue difference 
1 - 
k, representing the reciprocal
of a time constant in Eq. (8.58), is positive, so we find that for case I,
(8.60)
Case II. k  1.
From Eq. (8.57), this second case is described by the differential equation
(8.61)
Where in the last line we have used Eq.(8.56).However,from case I,we know that 
for 
as 
. Hence, the last term on the right-hand side of Eq. (8.61) approaches
zero as time t approaches infinity. Ignoring this term, we find that Eq. (8.61) simplifies to
(8.62)
It must be emphasized, however, that Eq. (8.62) holds only in an asymptotic sense.
Equation (8.62) represents an autonomous system (i.e., a system with no explicit
time dependence).The stability of such a system is best handled using a positive-definite
function called the Lyapunov function,a detailed treatment of which is deferred to Chapter
13. Let s denote the state vector of an autonomous system and V(t) denote a Lyapunov
function of the system.An equilibrium state of the system is asymptotically stable if
where 
is a small neighborhood around .s
u
d
dt V(t) 6 0  for s  u - s
s
d	1(t)
dt
= 
1	1(t)[1 - 	2
1(t)]  for t S q
t S q
l Z 1
l S 0
= 
1	1(t) - 
1	3
1(t) - 	3
1(t) a
m
l=2

l2
l(t)
= 
1	1(t) - 
1	3
1(t) - 	1(t) a
m
l=2

l	2
l(t)
d	1(t)
dt
= 
1	1(t) - 	1(t) a
m
l=1

l	2
l(t)
k(t) S 0  as t S q  for 1 6 k  m

1 7 
2 7 p 7 
k 7 p 7 
m 7 0
dk(t)
dt
= -(
1 - 
k)k(t),  1 6 k  m
=
1
	1(t)
d	k(t)
dt
- k(t)
	1(t)
d	1(t)
dt
,  1 6 k  m
dk(t)
dt
=
1
	1(t)
d	k(t)
dt
- 	k(t)
	2
1(t)
d	1(t)
dt
Section 8.5
Hebbian-Based Maximum Eigenfilter
389

For the problem at hand, we assert that the differential equation of Eq. (8.62) has
a Lyapunov function defined by
(8.63)
To validate this assertion, we must show that V(t) satisfies two conditions:
(8.64)
(8.65)
Differentiating Eq. (8.63) with respect to time, we get
(8.66)
where in the second line we have made use of Eq. (8.62). Since the eigenvalue 
1 is pos-
itive, we find from Eq. (8.66) that the condition of Eq. (8.64) is true for t approaching
infinity. Furthermore, from Eq. (8.66) we note that V(t) has minima [i.e., dV(t)/dt is zero]
at 	1(t) 
so the condition of Eq. (8.65) is also satisfied.We may therefore conclude
the analysis of case II by stating that
(8.67)
In light of the result described in Eq.(8.67) and the definition of Eq.(8.66),we may
restate the result of case I given in Eq. (8.60) in its final form:
(8.68)
The overall conclusion drawn from the analysis of cases I and II is twofold:
• The only principal mode of the stochastic approximation algorithm described in
Eq. (8.47) that will converge is 	1(t); all the other modes of the algorithm will decay
to zero.
• The mode 	1(t) will converge to 
Hence, in light of the expansion described in Eq. (8.53), we may formally state that
(8.69)
where q1 is the normalized eigenvector associated with the largest eigenvalue 
1 of the
correlation matrix R.
Finally,to establish that the solution of Eq.(8.69) is a locally asymptotically stable (in
the sense of Lyapunov) solution to the ordinary nonlinear differential equation of Eq.(8.52),
we have to satisfy the following condition, formulated in the discrete-time domain:
Let b(q) denote the basin of attraction surrounding the solution to Eq. (8.52); then the
parameter vector w(n) enters a compact subset a of the basin of attraction b(q) infinitely
often, with probability 1.
(The basin of attraction is defined in Chapter 13.)
w(t) S q1,  as t S q
;1.
	k(t) S 0  as t S q  for 1 6 k  m
	1(t) S ;1  as t S q
;1,
 = -4
1	2
1(t)[	2
1(t) - 1]2  as t S q
 
dV(t)
dt
= 4	1(t)[	1(t) - 1] 
d	1(t)
dt
2. V(t) has a minimum
1. 
dV(t)
dt
6 0  for all t
V(t) = [	2
1(t) - 1]2
390
Chapter 8
Principal-Components Analysis

To satisfy this condition,we have to show that there exists a subset a of the set of all pos-
sible vectors such that
(8.70)
To do so,we must first show that the sequence of parameter vectors w(n) is bounded with
probability 1. We do so by hard-limiting the entries of w(n) so that their magnitudes
remain below some threshold a. We may then define the norm of w(n) by writing
(8.71)
Let 
be the compact subset of 
defined by the set of vectors with norm less than or
equal to a. It is straightforward to show the following (Sanger, 1989b):
If 
, and the constant a is sufficiently large, then 
with prob-
ability 1.
Thus, as the number of iterations n increases, w(n) will eventually be within , and it will
remain inside 
(infinitely often) with probability 1. Since the basin of attraction 
includes all vectors with a bounded norm,we have 
.In other words,the afore-
mentioned condition for locally asymptotic stability, concerning the basin of attraction,
is satisfied.
We have thus shown that,subject to the use of a small learning-rate parameter ,the
stochastic approximation algorithm of Eq. (8.47) will cause the parameter weight vector
w(n) to converge with probability 1 to the eigenvector q1 associated with the largest eigen-
value 
1 of the correlation matrix R of the input vector x(n).Moreover,this solution is not
the only fixed point of the algorithm, but it is the only one that is asymptotically stable.
Summarizing Properties of the Hebbian-Based Maximum Eigenfilter
The convergence analysis just presented shows that a single linear neuron governed
by the self-organized learning rule of Eq. (8.40)—or equivalently, that of Eq. (8.46)—
adaptively extracts the first principal component of a stationary input.This first princi-
pal component corresponds to the largest eigenvalue 
1 of the correlation matrix of the
random vector X(n),a sample realization of which is denoted by x(n);in fact,
1 is related
to the variance of the model output y(n), as shown next.
Let 2(n) denote the variance of random variable Y(n), with a realization of it
denoted by y(n)—that is,
(8.72)
where the random variable Y(n) has zero mean for a zero-mean input.Letting 
in
Eq. (8.46) and using the fact that, in a corresponding way, w(n) approaches q1, we obtain
Using this relation, we can show that the variance 2(n) approaches 
1 as the number
of iterations n approaches infinity; see Problem 8.6.
In summary,a Hebbian-based linear neuron whose operation is described by Eq.(8.46)
converges with probability 1 to a fixed point, which is characterized as follows (Oja, 1982):
x(n) = y(n)q1  for n S q
n S q
2(n) = [Y2(n)]
a  b(q1)
b(q1)
a
a
7 w(n + 1)7 6 7 w(n)7
7w(n)7  a
m
a
 w(n) = max 
j wj(n)  a
lim
nS q w(n) = q1  infinitely often with probability 1
Section 8.5
Hebbian-Based Maximum Eigenfilter
391

1. The variance of the model output approaches the largest eigenvalue of the corre-
lation matrix R, as shown by
(8.73)
2. The synaptic-weight vector of the model approaches the associated eigenvector,
as shown by
(8.74)
with
(8.75)
These results assume that the correlation matrix R is positive definite, with the
largest eigenvalue 
1 having multiplicity 1. They also hold for a nonnegative definite
correlation matrix R provided that 
1 0 with multiplicity 1.
EXAMPLE 2
Matched Filter
Consider a random vector X, a realization of which is denoted by the sample vector x. Let
(8.76)
where the vector s, representing the signal component, is fixed with a Euclidean norm of one.The
random vector V,representing the additive noise component,has zero mean and covariance matrix
2I.The correlation matrix of X is given by
(8.77)
The largest eigenvalue of the correlation matrix R is therefore
(8.78)
The associated eigenvector q1 is equal to s. It is readily shown that this solution satisfies the eigen-
value problem
Hence, for the situation described in this example, the self-organized linear neuron (upon con-
vergence to its stable condition) acts as a matched filter in the sense that its impulse response
(represented by the synaptic weights) is matched to the signal component s.
■
8.6
HEBBIAN-BASED PRINCIPAL-COMPONENTS ANALYSIS
The Hebbian-based maximum eigenfilter discussed in the previous section extracts
the first principal component of the input. This single linear neural model may be
expanded into a feedforward network with a single layer of linear neurons for the
purpose of principal-components analysis of arbitrary size on the input (Sanger,
1989b).
Rq1 = 
1q1

1 = 1 + 2
 = s sT + 2I
 R = [X(n)XT(n)]
X = s + V
lim
nS q7w(n)7 = 1
lim
nS q w(n) = q1
lim
nS q 2(n) = 
1
392
Chapter 8
Principal-Components Analysis

Generalized Hebbian Algorithm
Consider the feedforward network shown in Fig. 8.7, for the operation of which the fol-
lowing two assumptions of a structural nature are made:
1. Each neuron in the output layer of the network is linear.
2. The network has m inputs and l outputs, both of which are specified. Moreover, the
network has fewer outputs than inputs (i.e., l  m).
The only aspect of the network that is subject to training is the set of synaptic weights
{wji} connecting source nodes i in the input layer to computational nodes j in the output
layer, where i  1, 2, ..., m and j  1, 2, ..., l.
The output yj(n) of neuron j at time n produced in response to the set of inputs
{xi(n) i  1, 2, ..., m} is given as follows (see Fig 8.8a):
(8.79)
The synaptic weight wji(n) is adapted in accordance with a generalized form of Hebbian
learning, as shown by
(8.80)
where !wji(n) is the change applied to the synaptic weight wji(n) at time n and  is the
learning-rate parameter (Sanger, 1989b). Note that in Eq. (8.80), the index i refers to the
input of the network in Fig. 8.7 and the index j refers to its output. The generalized
Hebbian algorithm (GHA)4 of Eq. (8.80) for a layer of l neurons includes the algorithm
of Eq. (8.40) for a single neuron as a special case—that is, j  1.
To develop insight into the behavior of the generalized Hebbian algorithm, we
rewrite Eq. (8.80) in the form
(8.81)
where (n) is a modified version of the ith element of the input vector x(n); it is a func-
tion of the index j, as shown by
(8.82)
xi¿(n) = xi(n) - a
j-1
k=1
wki(n)yk(n)
x¿i
¢wji(n) = yj(n)[xi(n) - wji(n)yj(n)],   i = 1, 2, ..., m
j = 1, 2, ..., l
¢wji(n) =  a yj(n)xi(n) - yj(n) a
j
k=1
wki(n)yk(n) b ,  i = 1, 2, ..., m
j = 1, 2, ..., l
yj(n) = a
m
i=1
wji(n)xi(n),  j = 1, 2, ..., l

Section 8.6
Hebbian-Based Principal-Components Analysis
393
•
•
•
•
•
•
x1
x2
x3
xm
y1
y2
yl
Input
vector
x
FIGURE 8.7
Feedforward
network with a single layer 
of computational nodes.

For a specified neuron j,the algorithm described in Eq.(8.81) has exactly the same math-
ematical form as that of Eq.(8.40),except for the fact that the input signal xi(n) is replaced
by its modified value 
defined in Eq.(8.82).We may go one step further and rewrite
Eq. (8.81) in a form that corresponds to Hebb’s postulate of learning, as shown by
(8.83)
where
(8.84)
Thus, noting that
(8.85)
and
(8.86)
where z-1 is the unit-time delay operator, we may construct the signal-flow graph of Fig.
8.8b for the generalized Hebbian algorithm. From this graph, we see that the algorithm
wji(n) = z-1[wji(n + 1)]
wji(n + 1) = wji(n) + ¢wji(n)
x–i(n) = x¿i - wji(n)yj(n)
¢wji(n) = yj(n)x–i(n)
x¿i(n)
394
Chapter 8
Principal-Components Analysis
x1(n)
x2(n)
xm(n)
yj(n)
•
•
•
wj1(n)
wj2(n)
wj,m(n)
(a)
(b)
•
•
•
xi(n)
Óyj(n)
yj(n)
y1(n)
y2(n)
yj1(n)
xi(n)
xi(n)
z1
wji(n  1)
w1i(n)
w2i(n)
wj1,i(n)
wji(n)
Input
vector
x(n)
FIGURE 8.8
The signal-flow graph representation of generalized Hebbian algorithm.
(a) Graph of Eq. (8.79). (b) Graph of Eqs. (8.80) through (8.81), where xi(n) and x0i (n) 
are defined in Eqs. (8.82) and (8.84).

lends itself to a local form of implementation,provided that it is formulated as in Eq.(8.85).
Note also that yj(n), responsible for feedback in the signal-flow graph of Fig. 8.8b, is
itself determined by Eq. (8.79); signal-flow graph representation of this latter equation
is shown in Fig. 8.8a.
For a heuristic understanding of how the generalized Hebbian algorithm actually
operates, we first use matrix notation to rewrite the version of the algorithm defined in
Eq. (8.81) as
(8.87)
where wj(n) is the synaptic weight vector of neuron j, and 
(8.88)
The vector x(n) represents a modified form of the input vector x(n). Based on the rep-
resentation given in Eq. (8.87), we make the following observations (Sanger, 1989b):
1. For the first neuron of the feedforward network shown in Fig. 8.7, we have
In this case, the generalized Hebbian algorithm reduces to that of Eq. (8.46) for a
single neuron. From the material presented in Section 8.5, we already know that
this neuron will discover the first principal component of the input vector x(n).
2. For the second neuron of the network in Fig. 8.7, we write
Provided that the first neuron has already converged to the first principal com-
ponent, the second neuron sees an input vector x(n) from which the first eigen-
vector of the correlation matrix R has been removed.The second neuron therefore
extracts the first principal component of x(n), which is equivalent to the second
principal component of the original input vector x(n).
3. For the third neuron, we write
Suppose that the first two neurons have already converged to the first and second
principal components, as explained in steps 1 and 2.The third neuron now sees an
input vector x(n),from which the first two eigenvectors have been removed.There-
fore, it extracts the first principal component of the vector x(n), which is equiva-
lent to the third principal component of the original input vector x(n).
4. Proceeding in this fashion for the remaining neurons of the feedforward network
in Fig. 8.7, it is now apparent that each output of the network trained in accor-
dance with the generalized Hebbian algorithm of Eq. (8.81) represents the re-
sponse to a particular eigenvector of the correlation matrix of the input vector,
and that the individual outputs are ordered by decreasing eigenvalue.
This method of computing eigenvectors is similar to a technique known as
Hotelling’s deflation technique (Kreyszig, 1988); it follows a procedure similar to
Gram–Schmidt orthogonalization (Strang, 1980).
j = 3:  x¿(n) = x(n) - w1(n)y1(n) - w2(n)y2(n)
j = 2:  x¿(n) = x(n) - w1(n)y1(n)
j = 1:  x¿(n) = x(n)
x¿(n) = x(n) - a
j-1
k=1
wk(n)yk(n)
¢wj(n) = yj(n)x(n) - y2
j(n)wj(n),  j = 1, 2, ..., l
Section 8.6
Hebbian-Based Principal-Components Analysis
395

Convergence Considerations
Let W(n)  {wji(n)} denote the l-by-m synaptic-weight matrix of the feedforward net-
work shown in Fig. 8.7; that is,
(8.89)
Let the learning-rate parameter of the generalized Hebbian algorithm of Eq. (8.81) take
a time-varying form (n) such that in the limit, we have
(8.90)
We may then rewrite this algorithm in the matrix form
(8.91)
where
and the operator LT[·] sets all the elements above the diagonal of its matrix argument
to zero, thereby making that matrix lower triangular. Under these conditions, and
invoking the assumptions made in Section 8.5, convergence of the GHA algorithm is
proved by following a procedure similar to that presented in the previous section for
the maximum eigenfilter. Thus, we may state the following theorem (Sanger, 1989b):
If the synaptic-weight matrix W(n) is assigned random values at time-step n  0, then with
probability 1, the generalized Hebbian algorithm of Eq. (8.91) will converge to a fixed point,
with WT(n) approaching a matrix whose columns are the first l eigenvectors of the m-by-m
correlation matrix R of the m-by-1 input vector, ordered by decreasing eigenvalue.
The practical significance of this theorem is that it guarantees the generalized
Hebbian algorithm to find the first l eigenvectors of the correlation matrix R, assuming
that the associated eigenvalues are distinct. Equally important is the fact that we do not
need to compute the correlation matrix R. Rather, the first l eigenvectors of R are com-
puted by the algorithm directly from the input data.The resulting computational savings
can be enormous, especially if the dimensionality m of the input space is very large and
the required number of the eigenvectors associated with the l largest eigenvalues of the
correlation matrix R is a small fraction of m. Just as importantly, the algorithm has a
built-in capability for adaptation; in other words, the algorithm can track statistical
variations in a nonstationary environment.
The convergence theorem is formulated in terms of a time-varying learning-rate
parameter (n). In practice, the learning-rate parameter is chosen to be a small con-
stant , in which case convergence is guaranteed with mean-square error in synaptic
weights of order .
In Chatterjee et al. (1998), the convergence properties of the GHA algorithm
described in Eq. (8.91) are investigated. The analysis presented therein shows that
increasing  leads to faster convergence and larger asymptotic mean-square error,which
is intuitively satisfying. In that paper, the tradeoff between the accuracy of computation
and speed of learning is made explicit.
y(n) = W(n)x(n)
¢W(n) = (n){y(n)xT(n) - LT[y(n)yT(n)]W(n)}
lim
nS q (n) = 0 and a
q
n=0
(n) = q
W(n) = [w1(n), w2(n), ..., wl(n)]T
396
Chapter 8
Principal-Components Analysis

Optimality of the Generalized Hebbian Algorithm
Suppose that in the limit we write
(8.92)
and that we have
(8.93)
Then the limiting values q1, q2, ..., ql of the synaptic-weight vectors of the neurons in the
feedforward network of Fig. 8.6 represent the normalized eigenvectors associated with l
dominant eigenvalues of the correlation matrix R, and which are ordered in descend-
ing eigenvalue.At equilibrium, we may therefore write
(8.94)
where 
1  
2  ...  
l.
For the output of neuron j, we have the limiting value
(8.95)
Let Yj(n) denote a random variable with a realization denoted by the output yj(n).The
cross-correlation between the random variables Yj(n) and Yk(n), at equilibrium, is given
by
(8.96)
Hence, we may state that at equilibrium the generalized Hebbian algorithm of Eq. (8.91)
acts as an eigenanalyzer of the input data.
Let 
denote the particular value of the input vector x(n) for which the limit-
xˆ (n)
 = e
j, k = j
0, k Z j
 = qT
j Rqk
 lim
nS q [Yj(n)Yk(n)] = [qT
j X(n)XT(n)qk]
lim
nS q yj(n) = xT(n)qj = qT
j x(n)
qT
j Rqk = e
j, k = j
0, k Z j
7wj(n)7 = 1  for all j
¢wj(n) S 0 and wj(n) S qj as n S q for j = 1, 2, ..., l
Section 8.6
Hebbian-Based Principal-Components Analysis
397
ing conditions of Eq. (8.92) are satisfied for j  l - 1. Hence, from the matrix form of
Eq. (8.80), we find that in the limit
(8.97)
This means that given two sets of quantities,the limiting values q1,q2,…,ql of the synaptic-
weight vectors of the neurons in the feedforward network of Fig. 8.6 and the corre-
sponding outputs y1(n), y2(n), …, yl(n), we may construct a linear least-squares estimate
of the input vector x(n). In effect, the formula of Eq. (8.97) may be viewed as one
of data reconstruction,as depicted in Fig.8.9.Note that in light of the discussion presented
in Section 8.4, this method of data reconstruction is subject to an approximation error
vector that is orthogonal to the estimate 
.
xˆ (n)
xˆ (n)
xˆ (n) = a
l
k=1
yk(n)qk

Summary of the GHA
The computations involved in the generalized Hebbian algorithm (GHA) are simple;
they may be summarized as follows:
1. Initialize the synaptic weights of the network, wji, to small random values at time
n  1.Assign a small positive value to the learning-rate parameter .
2. For n  1, j  1, 2, ..., l, and i  1, 2, ..., m, compute
,
where xi(n) is the ith component of the m-by-1 input vector x(n) and l is the desired
number of principal components.
3. Increment n by 1, go back to step 2, and continue until the synaptic weights wji
reach their steady-state values. For large n, the synaptic weight wji of neuron j con-
verges to the ith component of the eigenvector associated with the jth eigenvalue
of the correlation matrix of the input vector x(n).
8.7
CASE STUDY: IMAGE CODING
We continue our discussion of the generalized Hebbian learning algorithm by examin-
ing its use for solving an image-coding problem.
Figure 8.10a shows an image of Lena used for training; this image emphasizes edge
information. It was digitized to form a 256  256 image with 256 gray levels.The image
was coded using a linear feedforward network with a single layer of 8 neurons, each
with 64 inputs. To train the network, 8  8 nonoverlapping blocks of the image were
used.The experiment was performed with 2,000 scans of the picture and a small learning-
rate parameter   10-4.
Figure 8.10b shows the 8  8 masks representing the synaptic weights learned by
the network. Each of the eight masks displays the set of synaptic weights associated with
a particular neuron of the network. Specifically, excitatory synapses (positive weights)
are shown in white, whereas inhibitory synapses (negative weights) are shown in black;
j = 1, 2, ..., l
i = 1, 2, ..., m
 ¢wji(n) =  ayj(n)xi(n) - yj(n) a
j
k=1
wki(n)yk(n) b
 yj(n) = a
m
i=1
wji(n)xi(n),  j = 1, 2, ... , l
398
Chapter 8
Principal-Components Analysis
q1
q2
ql
y1(n)
y2(n)
yl(n)
•
•
•
x(n)
ˆ
FIGURE 8.9
Signal-flow
graph representation of how
the reconstructed vector is
computed in the GHA.
xˆ

Section 8.7
Case Study: Image Coding
399
Original Image
Weights
Using First 8 Components
11 to 1 compression
FIGURE 8.10
(a) An image of Lena used in the image-coding experiment. (b) 8  8
masks representing the synaptic weights learned by the GHA. (c) Reconstructed
image of Lena obtained using the dominant 8 principal components without
quantization. (d) Reconstructed image of Lena with an 11-to-1 compression ratio
using quantization.
gray indicates zero weights. In our notation, the masks represent the columns of the
64  8 synaptic-weight matrix WT after the generalized Hebbian algorithm has converged.
To code the image, the following procedure was used:
• Each 8  8 block of the image was multiplied by each of the 8 masks shown in
Fig. 8.10b, thereby generating 8 coefficients for image coding; Fig. 8.10c shows the
reconstructed image based on the dominant 8 principal components without
quantization.
• Each coefficient was uniformly quantized with a number of bits approximately
proportional to the logarithm of the variance of that coefficient over the image.
Thus, the first three masks were assigned 6 bits each, the next two masks 4 bits
each, the next two masks 3 bits each, and the last mask 2 bits. Based on this
representation, a total of 34 bits were needed to code each 8  8 block of pixels,
resulting in a data rate of 0.53 bits per pixel.
To reconstruct the image from the quantized coefficients, all the masks were weighted by
their quantized coefficients and then added to reconstitute each block of the image.The
reconstructed Lena’s image with 11-to-1 compression ratio is shown in Fig. 8.10d.
For a variation on the first image,we next applied the generalized Hebbian algorithm
to the image of peppers shown in Fig. 8.11a. This second image emphasizes textural

400
Chapter 8
Principal-Components Analysis
FIGURE 8.11
(a) Image of peppers. (b) 8  8 masks representing the synaptic weights
learned by the GHA applied to the peppers. (c) Reconstructed image of the peppers,
using 8 dominant principal components. (d) Reconstructed image of the peppers with
12-to-1 compression ratio, using masks of part (b) with quantization. (e) Reconstructed
image of the peppers, using the masks of Fig. 8.10(b) for encoding, with quantization for
a compression ratio of 12 to 1, the same as that in part (d). Part (f) reproduces the Lena
mask (i.e., weights) of Fig. 8.10b.
Original Image
Weights
Using First 8 Components
12 to 1 compression
12 to 1 compression
Weights From Lena

information. Figure 8.11b shows the 8  8 masks of synaptic weights learned by the net-
work by proceeding in the same manner described for Fig.8.10;note the difference between
these masks and those of Fig. 8.10b. Figure 8.11c shows the reconstructed image of the
peppers based on the dominant 8 principal components without quantization. To study
the effect of quantization,the outputs of the first 2 masks were quantized using 5 bits each,
the third using 3 bits, and the remaining 5 masks using 2 bits each.Thus, a total of 23 bits
were needed to code each 8  8 block of pixels,resulting in a bit rate of 0.36 bits per pixel.
Figure 8.11d shows the reconstructed image of the peppers, using its own masks quan-
tized in the manner just described.The compression ratio of this image was 12 to 1.
To test the “generalization” performance of the GHA, we finally used the masks of
Fig. 8.10b to decompose the peppers of Fig. 8.11a and then applied the same quantization
procedure that was used to generate the reconstructed image of Fig.8.11d.The result of this
image reconstruction is shown in Fig.8.11e with a compression ratio of 12 to 1,the same as
that in Fig. 8.11d.While the reconstructed images in Figs. 8.11d and 8.11e do bear a strik-
ing resemblance to each other,it can be seen that the image in Fig.8.11d possesses a greater
amount of “true” textural information and thus looks less “blocky” than that in Fig. 8.11e.
The reason for this behavior lies in the network weights.Comparing the masks (i.e.,weights)
of Fig.8.11b for the peppers image with those of Fig.8.10b for the Lena image,reproduced
in Fig. 8.11f for convenience of presentation, we make two observations:
(i) The first four weights of these two masks are very similar.
(ii) The final four weights for the Lena image encode edge information; but in the
case of the peppers image, the final four weights encode textural information.
It is therefore the cited differences under point (ii) that explain the blocky appearance
of the peppers image in part (e) compared with the corresponding image in part (d).
8.8
KERNEL PRINCIPAL-COMPONENTS ANALYSIS
The underlying theory of principal-components analysis, as presented thus far in this
chapter, is based on second-order statistics (i.e., correlations) of the input data; it is for
this reason that the standard PCA is referred to as a linear method of dimensionality
reduction. From a practical perspective, however, it would be highly desirable to expand
the data-reduction capability of PCA to encompass input data whose structure contains
higher-order statistics. This expanded capability requires that we make the PCA non-
linear. To that end, Schölkopf et al. (1998) have developed a nonlinear PCA called kernel
PCA. This new technique builds on the notion of a reproducing-kernel Hilbert space
(RKHS) studied previously in Chapter 6.
It is informative to compare the GHA and the kernel PCA in implementational terms:
1. The GHA uses a feedforward network composed simply of an input layer and an
output layer; the network is made up entirely of linear neurons. Kernel PCA also
uses a feedforward network, but the network includes a nonlinear hidden layer
and a linear output layer.
2. GHA is an on-line learning algorithm, whereas kernel PCA is a batch-learning
algorithm.
Section 8.8
Kernel Principal-Components Analysis
401

Insofar as the hidden layer is concerned, kernel PCA follows the theory embodied in the
design of support vector machines studied in Chapter 6.With regard to the output layer,
kernel PCA follows the dimensionality-reduction theory embodied in standard PCA—
hence the name “kernel PCA.”
Derivation of Kernel PCA
Let the vector 
denote the nonlinear mapping from the input space of
dimensionality m0 to the feature space of dimensionality m1.Correspondingly,let the vec-
tor 	(xi) denote the image of an input vector xi induced in the feature space.Then, given
the set of examples {xi}N
i1, we have a corresponding set of feature vectors {	(xi)}N
i1.
Accordingly,we may define an m1-by-m1 correlation matrix in the feature space,denoted
by , in terms of the outer product 	(xi)	T(xi) as
(8.98)
As with ordinary PCA, the first thing we have to do is to ensure that the set of feature
vectors {	(xi)}N
i1 has zero mean, that is,
To satisfy this condition in the feature space is a more difficult proposition than it is in
the input space;in Problem 8.15 we describe a procedure for catering to this requirement.
Proceeding, then, on the assumption that the feature vectors have been centered, we
may adapt the use of Eq. (8.14) to our present situation by writing
(8.99)
where is an eigenvalue of the correlation matrix 
and is the associated eigenvector.
q~
R~

~
R~q~ = 
~q~
1
N a
N
i=1
	(xi) = 0
R~ = 1
N a
N
i=1
	(xi)	T(xi)
R
	: m0 S m1
402
Chapter 8
Principal-Components Analysis
Now we note that all eigenvectors that satisfy Eq. (8.99) for 
lie in the span of the

~ Z 0
set of feature vectors {	(xj)}N
j1. Consequently, there does exist a corresponding set of
coefficients {j}N
j1 for which we can write
(8.100)
Thus, substituting Eqs. (8.98) and (8.100) into Eq. (8.99), we obtain
(8.101)
where k(xi, xj) is a Mercer kernel defined in terms of the feature vectors by an inner
product, as shown by
(8.102)
We need to go one step further with Eq. (8.101) so that the relationship is expressed
entirely in terms of the kernel.To do so, we premultiply both sides of Eq. (8.101) by the
transposed vector 	T(xs), thereby obtaining
k(xi, xj) = 	T(xi)	(xj)
a
N
i=1 a
N
j=1
j	(xi)k(xi, xj) = N
~
a
N
j=1
j	(xj)
q~ = a
N
j=1
j	(xj)

(8.103)
where the definitions of k(xs, xi) and k(xs, xj) follow Eq. (8.102)
We now introduce two matrix definitions:
• the N-by-N kernel matrix K, or Gram, whose ij-th element is the Mercer kernel
k(xi, xj);
• the N-by-1 vector , whose jth element is the coefficient j.
Accordingly, we may recast Eq. (8.103) in the compact matrix form
(8.104)
where the squared matrix K2 denotes the product of K with itself. Since premultiplica-
tion by K is common to both sides of Eq. (8.104), all the solutions of this eigenvalue
problem that are of interest are equally well represented in the dual eigenvalue problem
(8.105)
Let 
1  
2  ...  
N denote the eigenvalues of the Gram matrix K; that is,
(8.106)
where 
is the jth eigenvalue of the correlation matrix .Then Eq.(8.105) takes the stan-
dard form
(8.107)
where the coefficient vector  plays the role of the eigenvector associated with the
eigenvalue 
 of the Gram K. The vector  is normalized by requiring that the eigen-
vector 
of the correlation matrix 
is normalized to unit length; that is,
(8.108)
where it is assumed that the eigenvalues of K are arranged in decreasing order, with 
l
being the smallest nonzero eigenvalue of the Gram matrix K. Using Eq. (8.100) and
then invoking Eq. (8.107), we may show that the normalization condition of Eq. (8.108)
is equivalent to
(8.109)
For the extraction of principal components, we need to compute the projections
onto the eigenvectors 
in feature space, as shown by
(8.110)
= a
N
j=1
k, jk(xj, x), k = 1, 2, ..., l
qT
k	(x) = a
N
j=1
k, j	T(xj)	(x)
q
k
T
r r = 1

k
, r = 1, 2, ..., l
qT
k q
k = 1 for k = 1, 2, ..., l
R~
q
K = 

R~

~
j

j = N
~
j, j = 1, 2, ..., N
K = N
~
K2 = N
~K
a
N
i=1 a
N
j=1
jk(xs, xi)k(xi, xj) = N
~
a
N
j=1
jk(xs, xj), s = 1, 2, ..., N
Section 8.8
Kernel Principal-Components Analysis
403

where the vector x is a “test” point and k, j is the jth coefficient of eigenvector k asso-
ciated with the kth eigenvalue of the Gram K. The projections of Eq. (8.110) define the
nonlinear principal components in the m1-dimensional feature space.
Figure 8.12 illustrates the basic idea of kernel PCA, where the feature space is
nonlinearly related to the input space via the transformation 	(x). Parts (a) and (b) of
the figure refer to the input space and feature space, respectively. The contour lines
shown in Fig. 8.12b represent constant projections onto a principal eigenvector, which
is shown as a red arrow.In this figure,it is assumed that the transformation 	(x) has been
chosen in such a way that the images of the data points induced in the future space con-
gregate themselves essentially along the eigenvector. Figure 8.11a shows the nonlinear
contour lines in the input space that correspond to those in the feature space. Note that
we purposely have not shown a preimage of the eigenvector in the input space, as it may
not even exist (Schölkopf et al., 1998).
For kernels defined in accordance with Mercer’s theorem, we are basically per-
forming ordinary PCA in an m1-dimensional feature space, where the dimension m1 is
a design parameter.All the properties of ordinary PCA that are described in Section 8.4
carry over to kernel PCA. In particular, kernel PCA is linear in the feature space, but
nonlinear in the input space.
In Chapter 6, we presented three methods for constructing Mercer kernels that
were based on the use of polynomials,radial-basis functions,and hyperbolic functions;see
Table 6.1. The question of how to select the optimal kernel for a given task (i.e., the
appropriate feature space) remains an open problem.
404
Chapter 8
Principal-Components Analysis
x1
x2
Input (data)
space
(a)
(b)
Feature space
Eigenvector
w2(x)
w1(x)
x  
   
x1
x2
FIGURE 8.12
Illustration of kernel PCA. (a) Two-dimensional input space, displaying a set
of data points. (b) Two-dimensional feature space, displaying the induced images of the data
points congregating around a principal eigenvector.The uniformly spaced dashed lines in part
(b) represent contours of constant projections onto the eigenvector; the corresponding
contours are nonlinear in the input space.

Summary of the Kernel PCA
1. Given the unlabeled training sample {xi}N
i1,compute the N-by-N Gram K{k(xi,xj)},
where
where it is assumed that preprocessing has been performed to satisfy the zero-
mean condition of all feature vectors over the training sample, that is 
2. Solve the eigenvalue problem
where 
 is an eigenvalue of the Gram K and  is the associated eigenvector.
3. Normalize the eigenvectors so computed by requiring that
where 
l is the smallest nonzero eigenvalue of Gram K, assuming that the eigen-
values are arranged in decreasing order.
4. For the extraction of principal components of a test point x,compute the projections
where r, j is the jth element of eigenvector r.
EXAMPLE 3
ILLUSTRATIVE EXPERIMENT ON KERNEL PCA
To provide some intuitive understanding for the operation of kernel PCA, we show in Fig. 8.13
the results of a simple experiment described in Schölkopf et al. (1998). The two-dimensional
data, consisting of components x1 and x2, used in this experiment were generated as follows:The
x1-values have a uniform distribution in the interval [-1, 1].The x2-values are nonlinearly related
to the x1-values by the formula
where v is additive Gaussian noise of zero mean and variance 0.04.
The results of kernel PCA shown in Fig. 8.13 were obtained using kernel polynomials
where d  1 corresponds to linear PCA, and d  2, 3, and 4 correspond to kernel PCA. Linear
PCA, shown on the left-hand side of Fig. 8.13, results in only two eigenvalues, since the dimen-
sionality of the input space is two. In contrast, kernel PCA permits the extraction of higher-order
components, as shown by the results depicted in columns 2, 3, and 4 of Fig. 8.13, corresponding to
polynomial degree d  2, 3, and 4, respectively.The contour lines shown in each part of the figure
(except for the zero eigenvalue in the case of linear PCA) represent constant principal values
(i.e., constant projections onto the eigenvector associated with the eigenvalue in question).
k(x, xi) = (xTxi)d,  d = 1, 2, 3, 4
x2 = x2
1 + v
 = a
N
j=1
r, jk(xj, x),  r = 1, 2, ..., l
 ak = qT
r 	(x)
T
r r = 1

r
,  r = 1, 2, ..., l
K = 

1
N a
N
i=1
	(xi) = 0
k(xi, xj) = 	T(xi)	(xj),  i, j = 1, 2, ..., N
Section 8.8
Kernel Principal-Components Analysis
405

Based on the results shown in Fig. 8.13, we make the following observations:
•
As expected,linear PCA fails to provide an adequate representation of the nonlinear input data.
•
In all cases, the first principal component varies monotonically along a parabola that under-
lies the input data.
•
In kernel PCA, the second and third principal components exhibit a behavior that appears
somewhat similar for different values of polynomial degree d.
•
In the case of polynomial degree d  2,the third principal component of kernel PCA appears
to pick up the variance caused by the additive Gaussian noise v. By removing the contribution
from this component, we would in effect be performing some form of noise reduction.
■
8.9
BASIC ISSUES INVOLVED IN THE CODING OF NATURAL IMAGES
In the coding of natural images, there are basically two strategies, both of which, in their
own ways, exploit the inherent redundancy that characterizes the underlying structure
of such images so as to produce effective representations of the underlying scenes.The
two strategies are as follows:
406
Chapter 8
Principal-Components Analysis
Eigenvalue  0.709
0
0.5
0.5
1
1
0
1
Eigenvalue  0.621
0
0.5
0.5
1
1
0
1
Eigenvalue  0.570
0
0.5
0.5
1
1
0
1
Eigenvalue  0.552
0
0.5
0.5
1
1
0
1
Eigenvalue  0.291
0
0.5
0.5
1
1
0
1
Eigenvalue  0.345
0
0.5
0.5
1
1
0
1
Eigenvalue  0.395
0
0.5
0.5
1
1
0
1
Eigenvalue  0.418
0
0.5
0.5
1
1
0
1
Eigenvalue  0.000
0
0.5
0.5
1
1
0
1
Eigenvalue  0.034
0
0.5
0.5
1
1
0
1
Eigenvalue  0.026
0
0.5
0.5
1
1
0
1
Eigenvalue  0.021
0
0.5
0.5
1
1
0
1
FIGURE 8.13
Two-dimensional example illustrating kernel PCA. From left to right, the
polynomial degree of the kernel is d  1, 2, 3, and 4, respectively. From top to bottom, the
first three eigenvectors in the feature space are shown.The first column corresponds to linear
PCA, and the other three columns correspond to kernel PCA with polynomial degree d  2,
3, and 4, respectively. (Reproduced with permission from 
Dr. Klaus-Robert Müller.)

1. Compact coding. In this coding strategy,the image is transformed in such a way that
it is represented with a reduced number of vectors, subject to a prescribed level of
root-mean-square error. Principal-components analysis is a well-studied example
of compact coding.
2. Sparse-distributed coding. In this second coding strategy, the dimensionality of the
natural image is not reduced. Rather, the redundancy contained in the input image
is transformed in a special way that matches the redundancy of the firing patterns
of neuronal cells in the visual system.
In a classic paper (Field, 1994), these two coding strategies are contrasted. In partic-
ular, it is pointed out that the signature of a sparse-distributed code is found in the
fourth-order moment (i.e., kurtosis) of the underlying distribution of a natural image.
PCA is a linear coding scheme, relying on second-order statistics for its functional-
ity; it is therefore incapable of capturing the fourth-order statistics of a natural image,
which appears to be essential for an efficient coding strategy. Another important
point made in Field’s paper is that sparse-distributed-coding schemes, exemplified
by the wavelet transform,5 are effective because the response histograms of such codes
exhibit a high kurtosis when they are applied to natural images. Furthermore, the
paper points out that, to a first-order degree of approximation, natural images may
be considered as a sum of self-similar local functions (i.e., the inverse of a wavelet
transform).
Nowadays, it is generally agreed that the process involved in the generation of
natural images is nonlinear (Ruderman, 1997). One of the contributing factors is
occlusion, which is highly nonlinear.There are four primary sources of image contours,
which are responsible for occlusion in natural images (Richards, 1988):
• external occluding edges;
• crease or fold;
• shadow or illumination effects;
• surface markings or texture.
All four types of image contours provide information about surface shapes in their own
individual ways. However, the rules for inference as to what type of edge created the
image contour are quite different, which makes the encoding and decoding of natural
images a research challenge.
To capture higher-order statistics of a natural image, it is therefore apparent
that we must somehow introduce nonlinearity into PCA.6 In the next section, we dis-
cuss an adaptive method of achieving this objective in a computationally efficient
manner.
8.10
KERNEL HEBBIAN ALGORITHM
The discussion presented in the preceding section has taught us an important lesson:
Higher-order statistics are particularly important in the structural coding (i.e., modeling) of
natural images.
Section 8.10
Kernel Hebbian Algorithm
407

Moreover,natural images are highly complex,in the sense that the number of pixels con-
tained in the digital representation of a natural image can be arbitrarily high; this num-
ber of pixels defines the dimensionality of the image space, in which a sample image is
represented merely as a point.Hence,if a machine is required to learn the model of a nat-
ural image, then it usually would take a large number of examples to train the machine.
Now, recalling that the kernel PCA is a batch-learning algorithm, we find that the
storage and manipulation of the Gram will occupy a size equal to N2,where N is the num-
ber of training examples.Accordingly, the computational complexity of kernel PCA can
become unmanageable when it is required to model a natural image.
To alleviate this computational difficulty, Kim et al. (2005) have devised an itera-
tive method for computing the kernel PCA by exploiting the on-line unsupervised-
learning capability of the generalized Hebbian algorithm (GHA), studied in Section 8.6.
The resulting algorithm, called the kernel Hebbian algorithm (KHA), is capable of esti-
mating the kernel principal components with a memory complexity that is linear in the
number of training examples. Unlike the kernel PCA, the KHA is therefore applicable
to large-scale learning problems of an unsupervised kind.
Derivation of the KHA
Consider a training sample, denoted by {xi}N
i1. We may reformulate the update rule of
GHA, described in Eqs. (8.79) and (8.80) in the feature space, as
(8.111)
and
(8.112)
We have chosen p as a new index in place of k so as to avoid confusion with the symbol
k for kernel. As before, wj(n) and x(n) are, respectively, the changes applied to the
synaptic weight vector and the input vector selected from the training sample at time n;
 is the learning-rate parameter.The index l denotes the number of outputs. Due to the
possible high dimensionality of the feature space,we may not be able to apply Eq.(8.112)
directly. However, from the kernel PCA solution, it is known that wj is expanded in the
training sample in the feature space, as shown by
(8.113)
where ji are the coefficients of the expansion.The use of this formula in Eqs.(8.111) and
(8.112) leads to the following reformulation of the two update rules:
(8.114)
and
(8.115)
a
N
i=1
¢ji(n)(xi) =  c yj(n)(x(n)) - yj(n)a
N
i=1 a
j
p=1
yp(n)pi (xi)d , j = 1, 2, ..., l
yj(n) = a
N
i=1
ji(n)T(xi)(x(n)), j = 1, 2, ..., l
wj = a
N
i=1
ji(xi)
¢wj(n) =  c yj(n)(x(n)) - yj(n) a
j
p=1
wp(n)yp(n)d , j = 1, 2, ..., l
yj(n) = wT
j (n)(x(n)), j = 1, 2, ..., l
408
Chapter 8
Principal-Components Analysis

Invoking the definition of the Mercer kernel, we write
Moreover, we may identify two possible conditions:
(i) The index of the input vector x(n) in the training sample is i, in which case x(n)  xi.
(ii) Condition (i) does not hold, in which case x(n)  xi.
k(xi, x(n)) = T(xi)(x(n)),  i = 1, 2, ..., N
Section 8.10
Kernel Hebbian Algorithm
409
Accordingly, by removing the outer summation in Eq. (8.115) with respect to index i,
we finally obtain the update rule for the coefficients {ji} to be as follows (Kim et al.,
2005):
(8.116)
yj(n) = a
N
i=1
ji(n)k(xi, x(n)),  j = 1, 2, ..., l
and
(8.117)
where j  1, 2, ..., l and i  1, 2, ..., N. As with any other kernel method, implementa-
tions of the KHA are performed in the reproducing-kernel Hilbert space (RKHS).
With kernel PCA, we have to ensure that the set of feature vectors 
has
a zero mean. Problem 8.15 address a procedure for catering to this requirement for
batch processing. For on-line processing, which pertains to KHA, we would have to use
a sliding mean to adapt to changes in the input distribution.
One other comment concerning convergence of the KHA is in order: Since the
KHA builds on the GHA, we may say, in light of the convergence considerations pre-
sented in Section 8.6, that the KHA is locally convergent, provided that the learning-rate
parameter  is small enough.
Case Study: Denoising of Multipatch Image
When we speak of a complex image, a good example to consider is that of a patch taken
from the image of a natural scene. Modeling such an image becomes all the more chal-
lenging when the image has multiple patches.Indeed,the Lena image studied in Section 8.7
has multiple patches and therefore provides the basis of an insightful case study on
image denoising.
This case study is reported in Kim et al. (2005),7 who tested the KHA against six
other denoising algorithms. Specifically, two different versions of the Lena image were
constructed as follow:
(i) White Gaussian noise was added to the 256-by-256 Lena image, producing a signal-
to-noise ratio (SNR) of 7.72 dB.
(ii) Salt-and-pepper noise was added to the same image, producing an SNR of 4.94 dB.
{(xi)}N
i=1
¢ji(n) = μ
yj(n) - yj(n) a
j
p=1
pi(n)yp(n)  if x(n) = xi
- yj(n) a
j
p=1
pi(n)yp(n),
 if x(n) Z xi

From each of these two images, 12-by-12 overlapping images patches were then sampled
at a regular interval of two pixels.
The image model based on the kernel PCA, assuming a Gaussian kernel of fixed
width   1,was obtained by applying the KHA (with a learning-rate parameter   0.05)
to each training sample for around 800 sweeps through the noisy Lena image data.
Denoised reconstructions of the original Lena image were then obtained by retaining
the first r principal components from each kernel PCA model for varying r.
For comparison purposes,the denoised kernel PCA models were tested against the
median filter,8 Matlab’s Wiener filter,9 Wavelet-based methods, and the linear PCA.
Moreover, two state-of-the art methods were included in the comparative evaluations:
410
Chapter 8
Principal-Components Analysis
FIGURE 8.14
Denoising an image corrupted by white Gaussian noise: (a) Original image of
Lena. (b) Input noisy image. (c) Median filter. (d) Matlab’s wavelet denoising. (e) Matlab’s
Wiener filter. (f) Choi and Baraniuk’s method. (g) Pi´`zurica and Philips’s method. (h) PCA 
(r  20). (i) KHA (r  40). (This figure is reproduced with the permission of Dr. K. I. Kim.)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)

Section 8.10
Kernel Hebbian Algorithm
411
FIGURE 8.15
Denoising an image corrupted by salt and pepper type noise. (a) Original image
of Lena. (b) Input noisy image. (c) Median filter. (d) Matlab’s wavelet denoising. (e) Matlab’s
Wiener filter. (f) Choi and Baraniuk’s method. (g) Pi´`zurica and Philips’s method. (h) PCA 
(r  20). (i) KHA (r  20).( This figure is reproduced with the permission of Dr. K. I. Kim.)
• the Pizurica and Philips algorithm (Pizurica and Philips, 2006), which, assuming
additive Gaussian noise, estimates the probability that a given coefficient in the
wavelet subspace contains a noise-free component;
• the Choi and Baraniuk algorithm (Choi and Baraniuk, 1999), according to which
an estimate of the original signal is obtained by projecting the noisy signal onto the
Besov space10 in the wavelet domain.
The results of the experiments are reproduced in Figs. 8.14 and 8.15, in light of
which the following observations may be made Kim et al. (2005):
(i) The superior denoising performance produced by the Pi´`zurica and Philips algo-
rithm for the case of additive white Gaussian noise (AWGN) in Fig. 8.14 and the
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)

median filter for the case of salt-and-pepper noise in Fig. 8.15 are attributed to the
use of prior knowledge of the statistics of the pertinent noise sources.
(ii) The somewhat degraded performance of these two denoising methods,when tested
against the other type of noise (i.e.,salt and pepper noise in the Piˆzurica and Philips
algorithm, and the AWGN in the median filter) demonstrates the risk of relying on
prior knowledge.
(iii) The KHA performs well for both types of noise,as shown in Figs.8.14 and 8.15;this
result indicates that if we have no information about the additive noise charac-
teristics, then the KHA could be viewed as an alternative to existing methods.
Last, but by no means least, the KHA is an on-line unsupervised learning algorithm,
hence offering two additional advantages:
• Being of an on-line learning kind, it is computationally efficient.
• Being unsupervised,it avoids the need for labeled examples,the collection of which
(for supervised learning) can be costly in both time and effort.
8.11
SUMMARY AND DISCUSSION
An important issue in unsupervised learning is  how to formulate a performance mea-
sure or cost function for the learning process to generate an internal signal that plays a
supervisory role such that the network is enabled to predict or reconstruct its own input.
In principal-components analysis, the cost function is the mean-square value of the error
vector, defined as the difference between the input vector (assumed to be of zero mean)
and its reconstructed version that is to be minimized with respect to a set of adjustable
coefficients, subject to two orthonormality constraints:
(i) normalization, according to which each eigenvector has unit length;
(ii) orthogonality, according to which any two different eigenvectors are orthogonal to
each other.
Problem 8.3 explores this approach to the derivation of PCA, as a complement to the
perturbation theory presented in Section 8.4.
Dimensionality Reduction
The one notion that most aptly provides the motivation for using PCA is dimensionality
reduction, the essence of which is summarized in Eqs. (8.28) and (8.29).These two equa-
tions are reproduced here for convenience of the discussion:
(i) Representation of Data.
Given an m-dimensional data vector x, Eq. (8.29) states
that x can be represented by an l-dimensional vector of principal components,
denoted by
a = ≥
a1
a2
o
al
¥ = ≥
qT
1
qT
2
o
qT
l
¥ x,  l  m
412
Chapter 8
Principal-Components Analysis

where qi is the ith eigenvector of the m-by-m correlation matrix
and ai, the ith component of the vector a, is the projection of the data vector x
onto the ith eigenvector qi. If l  m, then the new vector a is a rotated version of
the original data vector x; the real difference between them is that a has uncorre-
lated components, whereas x does not. If l  m, then only a subset of the eigen-
vectors is retained, which makes the representation of data approximate. In the
latter case, we speak of dimensionality reduction.
(ii) Reconstruction of Data.
Given the vector of principal components, a, Eq. (8.28)
states that the original data vector x may be reconstructed by a linear combination
of the eigenvectors, as shown by the estimate
where the principal components 
are coefficients of the expansion.
a1, a2, ... , al
xˆ = a
l
i=1
aiqi,  l  m
R = [xxT]
Section 8.11
Summary and Discussion
413
Here again, the reconstruction is exact if l  m, and approximate if l  m. The
resulting error vector
satisfies the principle of orthogonality, which states that the error vector e is
orthogonal to the estimate .A consequence of this principle is that the estimator
is optimal in the minimum mean-square error sense (Haykin, 2002).An optimal
method for determining the reduced dimension l is the minimum-description-length
(MDL) criterion discussed in Chapter 2.
One application of PCA, with an emphasis on the principle of dimensionality
reduction, is in denoising. In one such application, the data vector x consists of a signal
component s and additive white Gaussian noise v; the objective is to minimize the effect
of the noise in some optimal sense. Let 
denote the m-dimensional data space in which
the vector x lies. Given x, PCA decomposes the space 
into two orthogonal subspaces:
• The signal subspace S.An estimate of the signal component, denoted by 
lies in
S; the estimate plays a role similar to that of 
in dimensionality reduction.
• The noise subspace n.An estimate of the noise component, denoted by 
lies in n;
the estimate 
plays a role similar to that of the error e in dimensionality reduction.
Another application of PCA is in data compression. In this application, the objec-
tive is to preserve as much information about an input set of data as possible.Given a data
vector x of dimension m, PCA achieves this objective through subspace decomposition
of the input data,whereby the first l (less than m) principal components of the input data
provides a linear mapping. The mapping is optimal in the sense that it permits recon-
struction of the original input data in the minimum mean-square error sense. Moreover,
a representation based on the first l principal components is preferable to an arbitrary sub-
space representation, because the principal components of the input data are naturally
ordered in decreasing eigenvalue or, equivalently, decreasing variance. Accordingly, we
vˆ
vˆ,
xˆ
sˆ
sˆ,
x
x
xˆ
xˆ
e = x - xˆ

may optimize the use of principal components analysis for data compression by employ-
ing the greatest numerical precision to encode the first principal component of the input
and progressively less precision to encode the remaining l - 1 components, as illustrated
in the image-coding case study presented in Section 8.7.
Two Views of Unsupervised Learning
1. The Bottom-up View. The notion of locality plays an essential role in the first three
principles of self-organization, namely, self-amplification, competition, and coop-
eration, as described in Section 8.2. These three principles represent bottom-up
learning, the motivation for which is to formulate a model of the learning process.
Such an approach to modeling is pursued in unsupervised neural networks, exem-
plified by the Hebbian maximum eigenfilter and the generalized Hebbian algo-
rithm studied in Sections 8.5 and 8.6, respectively.
On the other hand, as pointed out previously in the introductory section,
locality is not emphasized in machine learning. This lack of emphasis on self-
organization, in turn, means that the bottom-up view of computational intelligence
may play no role in unsupervised machine learning.
2. The Top-down View. Having formulated a model of an unsupervised learning
problem by following the principles of self-organization, we next tune the ad-
justable parameters (i.e.,weights) of the model in an analytical manner.Specifically,
given a set of unlabeled examples, we minimize the cost function subject to con-
straints imposed on the learning process. The underlying theory of this second
phase represents top-down learning, as pursued in neural networks. Iterative for-
mulations of the maximum eigenfiltering algorithm and generalized Hebbian al-
gorithm (GHA) are examples of this view of unsupervised learning.
On the other hand,machine learning confines itself essentially to the top-down
view of unsupervised learning.To make up for the lack of emphasis on self-organiza-
tion,effective use is made of analytical tools in statistical learning theory.This approach
to unsupervised learning is exemplified by the kernel PCA discussed in Section 8.9.
Irrespective of how the unsupervised learning is performed, it is in the top-
down learning where the intrinsic structural information contained in the input
data (i.e., Principle 4 of self-organization) is actually exploited.
Kernelization of Neurobiologically Inspired Algorithms
Kernel methods, exemplified by the kernel PCA,are computationally effective in that they
have a built-in capability to account for certain higher-order information contained in the
input data.Typically,however,these methods suffer from the curse of dimensionality,which
means that the computational complexity of the methods (for one reason or another)
increases exponentially with linearly increasing dimensionality of the input data space.
Consider, for example, the problem of image denoising. Unfortunately, the com-
putational complexity of the original form of kernel PCA severely limits its application to
real-life images (e.g., faces and natural scenes). However, by kernelizing the generalized
Hebbian algorithm (GHA), as in the kernel Hebbian algorithm (KHA) discussed in
Section 8.10, we have an iterative unsupervised algorithm that estimates the kernel
414
Chapter 8
Principal-Components Analysis

principal components with only linear computational (i.e., memory) complexity. Just as
important, the denoised images, presented in Section 8.10, demonstrate a performance
comparable to that of supervised-learning algorithms currently in use. It may therefore
be argued that,through the kernelization of an iterative PCA algorithm,we have not only
circumvented the curse-of-dimensionality problem in some measurable way, but also
solved an image denoising problem, using unlabeled examples only.
The message to take away from this discussion is indeed a profound one:
There is much to be gained by kernelizing (rooted in statistical learning theory) neurobio-
logically motivated unsupervised learning algorithms.
In the next chapter, on self-organizing maps that are motivated by neurobiological con-
siderations, we will describe another application of kernelization that does make a dif-
ference in its own right.
NOTES AND REFERENCES
1. Principal-components analysis (PCA) is perhaps the oldest and best known technique in
multivariate analysis (Jolliffe, 1986; Preisendorfer, 1988). It was first introduced by Pearson
(1901), who used it in a biological context to recast linear regression analysis into a new
form. It was then developed by Hotelling (1933) in work done on psychometry. It appeared
once again and quite independently in the setting of probability theory, as considered by
Karhunen (1947), and was subsequently generalized by Loe`ve (1963).
2. Synaptic Enhancement and Depression. We may generalize the concept of a Hebbian mod-
ification by recognizing that positively correlated activity produces synaptic strengthening,
and that either uncorrelated or negatively correlated activity produces synaptic weakening
(Stent, 1973). Synaptic depression may also be of a noninteractive type. Specifically, the
interactive condition for synaptic weakening may simply be noncoincident presynaptic or
postsynaptic activity.
We may go one step further by classifying synaptic modifications as Hebbian, anti-
Hebbian, and non-Hebbian (Palm, 1982). According to this scheme, a Hebbian synapse
increases its strength with positively correlated presynaptic and postsynaptic signals and
decreases its strength when these signals are either uncorrelated or negatively correlated.
Conversely, an anti-Hebbian synapse weakens positively correlated presynaptic and post-
synaptic signals and strengthens negatively correlated signals. In both Hebbian and anti-
Hebbian synapses, however, the modification of synaptic efficiency relies on a mechanism
that is time dependent, highly local, and strongly interactive in nature. In that sense, an anti-
Hebbian synapse is still Hebbian in nature, though not in function.A non-Hebbian synapse,
on the other hand, does not involve a Hebbian mechanism of either kind.
3. Covariance Hypothesis. One way of overcoming the limitation of Hebb’s hypothesis is to
use the covariance hypothesis introduced in Sejnowski (1977a, b). In this hypothesis, the
presynaptic and postsynaptic signals in Eq.(8.2) are replaced by the departure of presynaptic
and postsynaptic signals from their respective average values over a certain time interval.
Let 
and 
denote the time-averaged values of the presynaptic signal xj and postsynaptic
signal yk, respectively.According to the covariance hypothesis, the adjustment applied to the
synaptic weight wkj is defined by
(A)
¢wkj = (xj - x)(yk - y)
y
x
Notes and References
415

where  is the learning-rate parameter. The average values 
and 
constitute respective
presynaptic and postsynaptic thresholds, which determine the sign of synaptic modifica-
tion. In particular, the covariance hypothesis allows for the following:
• convergence to a nontrivial state, which is reached when 
or 
;
•
prediction of both synaptic potentiation (i.e., increase in synaptic strength) and synaptic
depression (i.e., decrease in synaptic strength).
Figure A illustrates the difference between Hebb’s hypothesis and the covariance hypoth-
esis. In both cases, the dependence of 
on yk is linear; however, the intercept with the
yk-axis in Hebb’s hypothesis is at the origin, whereas in the covariance hypothesis it is at
.
We make the following important observations from Eq. (A):
1. The synaptic weight wkj is enhanced if there are sufficient levels of presynaptic and post-
synaptic activities—that is, the conditions 
and 
are both satisfied.
2. The synaptic weight wkj is depressed if there is either
•
a presynaptic activation (i.e.,
) in the absence of sufficient postsynaptic acti-
vation (i.e.,
), or
•
a postsynaptic activation (i.e.,
) in the absence of sufficient presynaptic acti-
vation (i.e.,
).
This behavior may be regarded as a form of temporal competition between the incoming
patterns.
4. Historical Note. Long before the publication of Sanger’s GHA in 1989, Karhunen and Oja
(1982) published a conference paper that described a new algorithm, called the stochastic
gradient algorithm (SGA), derived for the purpose of computing the eigenvectors of PCA.
It turns out that the SGA is very close in its composition to the GHA.
5. Wavelets. The preface to the book by Mallat (1998) states the following:
Wavelets are based not on a “bright new idea,” but on concepts that already existed
under various forms in many different fields.The formalization and emergence of this
“wavelet theory” is the result of a multidisciplinary effort that brought together math-
ematicians, physicists and engineers, who recognized that they were independently
xj 6 x
yk 7 y
yk 6 y
xj 7 x
yk 7 y
xj 7 x
yk = y
¢wkj
yj = y
xk = x
y
x
416
Chapter 8
Principal-Components Analysis
0
!wkj
Ó(xj  − x)−y
slope  Ó(xj  − x)
Maximum
depression point
Postsynaptic
activity yk
Balance
point  −y
Covariance
hypothesis
slope  Óxj
Hebb’s hypothesis
FIGURE A
Illustration of Hebb’s
hypothesis and the covariance
hypothesis.

developing similar ideas. For signal processing, this connection has created a flow of
ideas that goes well beyond the construction of new bases or transforms.
Let %(t) denote a function of zero mean, as shown by
The function %(t) represents the impulse response of a band-pass filter; such a function is
called a wavelet. The wavelet is dilated with a scale parameter s and shifted in position by a
time parameter u; we thus write
Given a real-valued signal g(t) with Fourier transform G(f), the continuous wavelet trans-
form of g(t) is defined by the inner product in integral form:
According to this formula, the wavelet transform correlates the signal g(t) with %u,s(t).
Equivalently, we may write
where 
(f) is the Fourier transform of %u,s(t) and the asterisk denotes complex conjuga-
tion.We thus see that the wavelet transform Wg(u, s) depends on the values of the signal g(t)
and its Fourier transform G(f) in the time–frequency domain,where the energy of %u,s(t) and
that of its Fourier transform 
(f) are concentrated.
For authoritative treatment of the wavelet transform, the reader is referred to the books by
Mallat (1998) and Daubechies (1992,1993).The introductory book by Meyer (1993) includes
a historical perspective of the wavelet transform.
°u,s
°u,s
 =
3
q
-q
 G(f)°*u,s(f)df
 Wg(u, s) = °u,s(f), G(f)
 =
3
q
-q
 g(t)%u,s(t)dt
 Wg(u, s) = %u,s(t), g(t)
%u,s(t) =
1
2s
  % a t - u
s
b
3
q
-q
%(t)dt = 0
Notes and References
417
6. Nonlinear PCA methods.
These methods may be categorized into four classes:
(i)
Hebbian networks, where the linear neurons in the generalized Hebbian algorithms,
for example, are replaced with nonlinear neurons (Karhunen and Joutsensalo, 1995).
(ii)
Replicator networks or autoencoders, which are built around multilayer perceptrons
containing three hidden layers (Kramer, 1991):
• mapping layer;
• bottleneck layer;
• demapping layer.
Replicator networks were discussed in Chapter 4.
(iii) Principal curves, which are based on an iterative estimation of a curve or a surface
capturing the structure of the input data (Hastie and Stuetzle, 1989).The self-organizing
maps, studied in Chapter 9, may be viewed as a computational procedure for finding
a discrete approximation of principal curves.
(iv) Kernel PCA, originated by Schölkopf et al. (1998), was studied in Section 8.8 of this
chapter.

7. In Kim et al. (2005), the results of image-denoising experiments involving the KHA are
also presented for the following scenarios:
• superresolution and denoising of face (single-patch) images;
• multipatch superresolution images of natural scenes.
8. The median filter is a filter (estimator) that minimizes the Bayes risk for the absolute error
cost function
where e(n) is the error signal, defined as the difference between a desired response and the
actual response of the filter. It turns out that the result of this minimization is the median
of the posterior probability density function—hence the name of the filter.
9. Adaptive Wiener filters. The Wiener filter was studied in Chapter 3. In the adaptive Wiener
filter, the training sample {xi(n), di(n)}N
i1 is split into a successive sequence of windowed
batches of labeled data, and the filter parameters are computed using the normal equations
(or discrete form of the Wiener–Hopf equation) on a batch-by-batch basis. In effect, within
each batch, the data are viewed as pseudostationary, and the statistical variations in the
training sample show up in the corresponding changes in the filter parameters as the com-
putation proceeds from one batch to the next.
10. The Sobolev space is the space of all functions that contains all m derivatives in the space
Lm and in which the mth derivative is absolutely integrable (Vapnik, 1998).The Besov space
refines the condition for smoothness by including a third parameter for m  1 and 
PROBLEMS
Competition and Cooperation
8.1
In a self-organizing system that involves competition as well as cooperation, we find that
competition precedes cooperation. Justify the rationale behind this statement.
Principal-Components Analysis: Constrained-Optimization Approach
8.2
In Section 8.4, we used perturbation theory to derive the PCA. In this problem, we address
this same issue from the perspective of a constrained-optimization approach.
Let x denote an m-dimensional zero-mean data vector and w denote an adjustable para-
meter vector of the same dimension m. Let 2 denote the variance of the projection of the
data vector x onto the parameter vector w.
(a) Show that the Lagrangian for maximizing the variance 2, subject to the normalizing
condition 
is defined by
where R is the correlation matrix of the data vector x and  is the Lagrangian multiplier.
(b) Using the result of part (a),show that maximization of the Lagrangian J(w) with respect
to w yields the defining eigenequation
Hence,show that 
.In the eigendecomposition terminology,w is the 
2 = [(wTx)2] = 
Rw = w
J(w) = wTRw - (wTw - 1)
w = 1,
m = q.
R(e(n)) = e(n)
418
Chapter 8
Principal-Components Analysis
eigenvector and  is the associated eigenvalue.

Problems
419
eigenvector, and let the Lagrange multiplier 
ij represent the orthogonality condition
Show that the Lagrangian now assumes the expanded form
Hence, show that the maximization of J(wi) yields a set of m equations for which the opti-
mal solution is the eigenvalue 
i associated with the eigenvector wi.
8.3
Let the estimator of an m-dimensional zero-mean data vector x be defined by the expansion
where qi is the ith eigenvector of the correlation matrix
and a1, a2, ..., al are the coefficients of the expansion, subject to the condition
Show that minimization of the mean-square error
with respect to the adjustable coefficients a1, a2, ..., al yields the defining formula
as the ith principal component—that is, the projection of the data vector x onto the eigen-
vector qi.
8.4
Following on the constrained-optimization problem considered in Problem 8.2, consider
the Lagrangian
where (wTx)2 denotes the instantaneous value of the variance of a zero-mean data vector
x projected onto the weight vector w.
(a) Evaluating the gradient of the Lagrangian J(w) with respect to the adjustable weight
vector w, show that
(b) With the stochastic gradient ascent in mind for on-line learning, we may express the
weight-update formula as
where  is the learning-rate parameter. Hence, derive the iterative equation
which is a rewrite of Eq.(8.47) defining the evolution of the maximum eigenfilter across
discrete time n, with 
written in place of w(n).
wˆ (n)
wˆ (n + 1) = wˆ (n) + [(x(n)xT(n))wˆ (n) - wˆ T(n)(x(n)xT(n))wˆ (n)wˆ (n)]
wˆ (n + 1) = wˆ (n) + 1
2
 g(wˆ (n))
 = 2(wTx)x - 2
w
 g(w) = 0J(w)
0w
J(w) = (wTx)2 - 
(wTw - 1)
ai = q i
Tx, i = 1, 2, ..., l
J(xˆ i) = [x - xˆ i2]
e 1 for j = i
0 otherwise
qT
i  qj =
R = [xxT]
xˆ l = a
l
i=1
aiqi,  l  m
J(wi) = wi
TRwi - 
ii(wi
Tw - 1) - a
i-1
j=1

ijwT
i wj,  i = 1, 2, ..., m
wT
i wj = 0.
(c) Let the Lagrange multiplier 
i represent the normalizing condition
for the ith
7 wi7 = 1

420
Chapter 8
Principal-Components Analysis
Hebbian-Based Maximum Eigenfilter
8.5
For the matched filter considered in Example 2, the eigenvalue 
1 and associated eigen-
vector q1 are respectively defined by
and
Show that these parameters satisfy the basic relation
where R is the correlation matrix of the input vector.
8.6
Consider the maximum eigenfilter where the weight vector w(n) evolves in accordance
with Eq. (8.46). Show that the variance of the filter output approaches 
max as n approaches
infinity, where 
max is the largest eigenvalue of the correlation matrix of the input vector.
8.7
Minor-components analysis (MCA) is the opposite of principal-components analysis. In
MCA, we seek to find those directions that minimize the projection variance. The direc-
tions so found are the eigenvectors corresponding to the smallest (minimum) eigenvalues
of the correlation matrix R of the input random vector X(n).
In this problem, we explore how to modify the single neuron of Section 8.4 so as to
find the minor component of R. In particular, we make a change of sign in the learning rule
of Eq. (8.40), obtaining the following (Xu et al., 1992):
Show that if the smallest eigenvalue of the correlation matrix R is 
m with multiplicity 1,then
where w(n) is the weight vector whose ith component is wi(n) and qm is the eigenvector asso-
ciated with 
m.
Hebbian-Based Principal-Components Analysis
8.8
Construct a signal-flow graph to represent the vector-valued Eqs. (8.87) and (8.88).
8.9
The ordinary differential equation approach to convergence analysis described in Section
8.5 does not apply directly to the generalized Hebbian-learning algorithm (GHA).However,
by expressing the synaptic-weight matrix W(n) in Eq. (8.91) as a vector made up of the indi-
vidual columns of W(n), we may build on the asymptotic stability theory of the maximum
eigenfilter. Hence, in light of what has been said here, explore the convergence behavior of
the generalized Hebbian-learning algorithm.
8.10
In this problem, we explore the use of the generalized Hebbian-learning algorithm to study
two-dimensional receptive fields produced by a random input (Sanger, 1990).The random
input consists of a two-dimensional field of independent Gaussian noise with zero mean and
unit variance, which is convolved with a Gaussian mask (filter) and then multiplied by a
Gaussian window.The Gaussian mask has a standard deviation of 2 pixels, and the Gaussian
window has a standard deviation of 8 pixels. The resulting random input x(r, s) at position
(r, s) may thus be written as
x(r, s) = m(r, s)[g(r, s) * w(r, s)]
lim
nS q w(n) = qm
wi(n + 1) = wi(n) - y(n)(xi(n) - y(n)wi(n))
Rq1 = 
1q1
q1 = s

1 = 1 + 2

Problems
421
where w(r, s) is the field of independent and identically distributed Gaussian noise, g(r, s)
is the Gaussian mask, and m(r, s) is the Gaussian window function. The circular convolu-
tion of g(r, s) and w(r, s) is defined by
where g(r, s) and w(r, s) are both assumed to be periodic.
Use 2,000 samples of the random input x(r, s) to train a single-layer feedforward
network by means of the generalized Hebbian-learning algorithm. The network has 4,096
inputs arranged as a 64  64 grid of pixels and 16 outputs. The resulting synaptic weights
of the trained network are represented as 64  64 arrays of numbers. Perform the compu-
tations described herein and display the 16 arrays of synaptic weights as two-dimensional
masks. Comment on your results.
8.11
In situations where only the principal subspace (i.e., the space of the principal eigenvectors)
is required, we may use the symmetric algorithm, which is defined by
(a) Discuss the similarity and difference between the symmetric algorithm and the GHA.
(b) The principal subspace may be viewed as a generalization of Oja’s rule, defined in 
Eq. (8.46). Explain the rationale for this generalization.
Feature Extraction: Preamble for Problems 8.12 and 8.13
In the presentation of a data set made up of an aggregate of several clusters, we may say that, for
the clusters to be individually visible, the separation between them has to be larger than the inter-
nal scatter of the clusters. If it happens that there are only a few clusters in the data set, then the
leading principal axes found by PCA are to pick projections of the clusters with good separation,
thereby providing an effective basis for feature extraction.
8.12
In Section 4.19 of Chapter 4, we described structural risk minimization as a method of sys-
tematically realizing the best generalization performance by matching the capacity of a
learning machine to the available size of the training sample.
Given the principal-components analyzer as a preprocessor aimed at reducing the
dimension of the input data space, discuss how such a processor can embed structure into
the learning process by ranking a family of pattern classifiers.
8.13
Another application of the principal-components analyzer as a preprocessor is in the super-
vised training of a multilayer perceptron using the back-propagation algorithm.
The aim in this application is to speed up the convergence of the learning process by
decorrelating the input data. Discuss how this aim is realized.
Adaptive Principal-Components Extraction
8.14
The generalized Hebbian-learning algorithm (GHA) relies on the exclusive use of feed-
forward connections for principal-components analysis. In this problem, we address
another algorithm called the adaptive principal-components extraction (APEX) (Kung and
Diamantaras, 1990; Diamantaras and Kung, 1996).
The APEX algorithm uses both feedforward and feedback connections, as depicted in 
Fig. P8.14. The input vector x is m dimensional, and each neuron in the network is linear.
 xˆ (n) = a
l
j=1
wˆ j(n)yj(n)
 wˆ j(n + 1) = wˆ j(n) + yj[x(n) - xˆ j(n)]
g(r, s) * w(r, s) = a
N-1
p=0
 a
N-1
q=0
 g(p, q)w(r - p, s - q)

422
Chapter 8
Principal-Components Analysis
There are two kinds of synaptic connections in the network:
(i) Feedforward connections from the input nodes to each of the neurons 1, 2, ..., j, with j  m.
These connections are represented by the feedforward weight vector
where n denotes discrete time.
(ii) Lateral connections from the individual neural outputs 1, 2, ..., j  1 to neuron j; these
connections are represented by the feedback weight vector
The feedforward synaptic connections are Hebbian, but the feedback synaptic connections
are anti-Hebbian and therefore inhibitory. The output of neuron j is given by
For the analysis to follow, assume that all the neurons in the network have already con-
verged to their respective stable conditions, given as
where qk is the eigenvector associated with the kth eigenvalue of the correlation matrix
(a) Building on Eq. (8.40), write the update equations for wj(n) and aj(n) pertaining to
neuron j.
(b) Assume that the eigenvalues of the correlation matrix R are arranged in decreasing order,
with 
1 being the largest. Let qk be the eigenvector associated with the eigenvalue 
k.To
R = [x(n)xT(n)] at time step n = 0.
 ak(0) = 0,   k = 1, 2, ..., j - 1
 wk(0) = qk,  k = 1, 2, ..., j - 1
yj(n) = wT
j (n)x(n) + aT
j (n)yj-1(n)
aj(n) = [aj1(n), aj2(n), ..., aj, j-1(n)]T
wj(n) = [wj1(n), wj2(n), ..., wjm(n)]T
•
•
•
1
2
3
•
•
•
x1
x2
x3
xm
yj
aj3
aj2
aj1
Input
layer
wjm
j
Output
layer
y3
y2
y1
FIGURE P8.14
Network with
feedforward and lateral connections
for deriving the APEX algorithm.

express the time-varying behavior of the feedforward weight vector wj(n), you may use
the expansion
where 	jk(n) is a time-varying coefficient. Hence, show that
(i)
where  is the learning-rate parameter, ajk(n) is the kth element of the feedback
weight vector aj, and 
is the average output power of neuron j.
(ii)
where 1k is a vector whose j elements are all zero, except for the kth element, which
is equal to 1.
(c) To proceed further, there are two cases to be considered:
Case I:
For this case, show that
This two-by-two matrix has the double eigenvalue
Given that &jk  1, show that 	jk(n) and ajk(n) approach zero asymptotically for increas-
ing n. Explain the rationale for this asymptotic behavior.
Case II:
For this second case, the feedback weights ajk(n) have no influence on the modes of the
network; thus,
Hence, for every principal mode k  j, show that
and 	jk(n) will therefore asymptotically converge to zero for increasing n.
Expressing the average output power of neuron j as
finally show that
and
lim
nS q wj(n) = qj
lim
nS q 2
j(n) = 
j
2
j(n) = a
m
k=j

k	2
jk(n)
	jk(n + 1) = {1 + [
k - 2
j(n)]}	jk(n)
ajk(n) = 0 for j  k  m
j  k  m
&jk = [1 - 2
j(n)]2
c	jk(n + 1)
ajk(n + 1)d = c1 + (
k - 2
j(n))

k
-
k
1 - (
k + 2
j(n))d c 	jk(n)
ajk(n)d
1  k  j - 1
aj(n + 1) = - 
k	jk(n)1k + {1 - [
k + 2
j(n)]}aj(n)
j
2(n) = [yj
2(n)]
 +  a
j-1
k=1

kajk(n)qk
a
m
k=1
	jk(n + 1)qk = a
m
k=1
{1 + [
k - 2
j(n)]}	jk(n)qk
wj(n) = a
m
k=1
	jk(n)qk
Problems
423

Kernel PCA
8.15 Let 
denote the centered counterpart of the ij-th element kij of the Gram K. Derive the
following formula (Schölkopf, 1997):
Suggest a compact representation of this relation in matrix form.
8.16 Show that the normalization of eigenvector  of the Gram K is equivalent to the require-
ment that Eq. (8.109) be satisfied.
Computer Experiments
8.17 This problem continues with the computer experiment on image coding presented in
Section 8.7. Specifically, there are two issues of interest:
(a) Plot the learning curve of the GHA, where the algorithm is trained on the Lena image
(i.e., the mean-square error is plotted versus the number of epochs used for training).
(b) Correspondingly, plot the learning curve of the algorithm on the peppers image.
Hence,determine the number of epochs for the algorithm to converge in both cases (a) and (b).
8.18 In this experiment, we revisit Example 3 on kernel PCA.The requirement is to compute the
kernel PCA components for the two-dimensional data described by the formula
where  is an additive Gaussian noise of zero mean and variance 0.04. However, this time
the requirement is to perform the computation with the kernel Hebbian algorithm. Compare
the results of the experiment with those described in Example 3.
x2 = x2
1 + v
+
1
N2 a
N
m=1
 a
N
n=1
	T(xm)	(xn)
kij = kij - 1
N a
N
m=1
	T(xm)	(xj) - 1
N a
N
n=1
	T(xi)	(xn)
kij
424
Chapter 8
Principal-Components Analysis

ORGANIZATION OF THE CHAPTER
This chapter studies the generation of “topographic maps” using principles of self-
organization.Treatment of the subject is organized as follows:
1. The introductory section, Section 9.1, motivates interest in the use of self-organizing
maps.
2. Section 9.2 describes two basic feature models, both of which, in their own ways, are
motivated by neurobiological considerations.
3. Sections 9.3 and 9.4 deal with the highly popular and widely used self-organizing
(feature) map (SOM) and its properties. Section 9.5 presents computer experiments,
highlighting distinguishing features of the SOM. Section 9.6 illustrates application
of the SOM for constructing contextual maps.
4. Section 9.7 discusses hierarchical vector quantization, whose implementation is sim-
plified through the use of the self-organizing map.
5. Section 9.8 describes the kernel-based self-organizing map (kernal SOM), followed
by a computer experiment in Section 9.9 illustrating the improved topographic map-
ping capability of this new algorithm. Section 9.10 discusses the relationship between
kernel SOM and Kullback-Leibler divergence.
Section 9.10 concludes the chapter with a summary and discussion of the material
covered in the chapter.
9.1
INTRODUCTION
In this chapter, we continue our study of self-organizing systems by considering a special
class of artificial neural networks known as self-organizing maps.These networks are based
on competitive learning; the output neurons of the network compete among themselves to
be activated or fired,with the result that only one output neuron,or one neuron per group,
is on at any one time.An output neuron that wins the competition is called a winner-takes-
all neuron, or simply a winning neuron.1 One way of inducing a winner-takes-all competi-
tion among the output neurons is to use lateral inhibitory connections (i.e., negative
feedback paths) between them;such an idea was originally proposed by Rosenblatt (1958).
In a self-organizing map, the neurons are placed at the nodes of a lattice that is
usually one or two dimensional. Higher-dimensional maps are also possible but not as
425
C H A P T E R  9
Self-Organizing Maps

common. The neurons become selectively tuned to various input patterns (stimuli) or
classes of input patterns in the course of a competitive-learning process. The locations
of the neurons so tuned (i.e., the winning neurons) become ordered with respect to each
other in such a way that a meaningful coordinate system for different input features is
created over the lattice.A self-organizing map is therefore characterized by the forma-
tion of a topographic map of the input patterns, in which the spatial locations (i.e., co-
ordinates) of the neurons in the lattice are indicative of intrinsic statistical features contained
in the input patterns—hence, the name “self-organizing map.”
As a neural model, the self-organizing map provides a bridge between two levels
of adaptation:
• adaptation rules formulated at the microscopic level of a single neuron;
• formation of experientially better and physically accessible patterns of feature
selectivity at the microscopic level of neural layers.
The self-organizing map is inherently nonlinear.
The development of self-organizing maps as a neural model is motivated by a dis-
tinct feature of the human brain:
The brain is organized in many places in such a way that different sensory inputs are repre-
sented by topologically ordered computational maps.
In particular, sensory inputs such as tactile (Kaas et al., 1983), visual (Hubel and
Wiesel, 1962, 1977), and acoustic (Suga, 1985) inputs are mapped onto different areas of
the cerebral cortex in a topologically ordered manner. Thus, the computational map
constitutes a basic building block in the information-processing infrastructure of the
nervous system. A computational map is defined by an array of neurons representing
slightly differently tuned processors or filters,which operate on the sensory information-
bearing signals in parallel. Consequently, the neurons transform input signals into a
place-coded probability distribution that represents the computed values of parameters
by sites of maximum relative activity within the map (Knudsen et al., 1987). The
information so derived is of such a form that it can be readily accessed by higher-order
processors using relatively simple connection schemes.
9.2
TWO BASIC FEATURE-MAPPING MODELS
Anyone who examines a human brain cannot help but be impressed by the extent to which
the brain is dominated by the cerebral cortex, which obscures the other parts. In terms of
sheer complexity,the cerebral cortex probably exceeds any other known structure in the uni-
verse (Hubel and Wiesel, 1977). What is equally impressive is the way in which different
sensory inputs (motor, somatosensory, visual, auditory, etc.) are mapped onto correspond-
ing areas of the cerebral cortex in an orderly fashion; to appreciate this point, see the cyto-
architectural maps of the cerebral cortex in Fig.4 of the introductory chapter.Computational
maps offer four properties (Knudsen et al., 1987; Durbin and Michison, 1990):
1. In each map, neurons act in parallel and process pieces of information that are similar in
nature, but originate from different regions in the sensory input space.
2. At each stage of representation, each incoming piece of information is kept in its proper
context.
426
Chapter 9
Self-Organizing Maps

3. Neurons dealing with closely related pieces of information are close together so that they can
interact via short synaptic connections.
4. Contextual maps can be understood in terms of decision-reducing mappings from higher-
dimensional parameter spaces onto the cortical surface.
Our interest lies in building artificial topographic maps that learn through self-
organization in a neurobiologically inspired manner. In this context, the one important
point that emerges from this very brief discussion of computational maps of the brain
is the principle of topographic map formation, stated as follows (Kohonen, 1990):
The spatial location of an output neuron in a topographic map corresponds to a particular
domain or feature of data drawn from the input space.
This principle has provided the neurobiological motivation for two different feature-
mapping models2, described herein.
Figure 9.1 displays the layout of the two models. In both cases, the output neurons
are arranged in a two-dimensional lattice.This kind of topology ensures that each neu-
ron has a set of neighbors.The models differ from each other in the manner in which the
input patterns are specified.
Section 9.2
Two Basic Feature-Mapping Models
427
Winning
neuron
Two-dimensional array
of postsynaptic neurons
Two-dimensional array
of postsynaptic neurons
Bundle of synaptic
connections.
Bundle of synaptic
connections. (There is a
similar bundle of synaptic
connections originating from
other presynaptic neurons.)
(a) Willshaw–von der Malsburg’s model
(b) Kohonen model
Input
Winning
neuron
Activated
neuron
FIGURE 9.1
Two self-organized
feature maps.

The model in Fig. 9.1a was originally proposed by Willshaw and von der Malsburg
(1976) on biological grounds to explain the problem of retinotopic mapping from the
retina to the visual cortex (in higher vertebrates).Specifically,there are two separate two-
dimensional lattices of neurons connected together,one projecting onto the other.One lat-
tice represents presynaptic (input) neurons, and the other lattice represents postsynaptic
(output) neurons.The postsynaptic lattice uses a short-range excitatory mechanism as well
as a long-range inhibitory mechanism.These two mechanisms are local in nature and crit-
ically important for self-organization. The two lattices are interconnected by modifiable
synapses of a Hebbian type.Strictly speaking,therefore,the postsynaptic neurons are not
winner-takes-all neurons;rather,a threshold is used to ensure that only a few postsynaptic
neurons will fire at any one time. Moreover, to prevent a steady buildup in the synaptic
weights that may lead to network instability, the total weight associated with each
postsynaptic neuron is limited by an upper-boundary condition.3 Thus, for each neuron
some synaptic weights increase,while others decrease.The basic idea of the Willshaw–von
der Malsburg model is for the geometric proximity of presynaptic neurons to be coded in
the form of correlations in their electrical activity,and to use these correlations in the post-
synaptic lattice so as to connect neighboring presynaptic neurons to neighboring post-
synaptic neurons.A topologically ordered mapping is thereby produced through a process
of self-organization. Note, however, that the Willshaw–von der Malsburg model is spe-
cialized to mappings for which the input dimension is the same as the output dimension.
The second model of Fig.9.1b,introduced by Kohonen (1982),is not meant to explain
neurobiological details.Rather,the model captures the essential features of computational
maps in the brain and yet remains computationally tractable.It appears that the Kohonen
model is more general than the Willshaw–von der Malsburg model in the sense that it is
capable of performing data compression (i.e., dimensionality reduction on the input).
In reality, the Kohonen model belongs to the class of vector-coding algorithms.
The model provides a topological mapping that optimally places a fixed number of vec-
tors (i.e., code words) into a higher-dimensional input space, thereby facilitating data
compression.The Kohonen model may therefore be derived in two ways. First, we may
use basic ideas of self-organization, motivated by neurobiological considerations, to
derive the model, which is the traditional approach (Kohonen, 1982, 1990, 1997).Alter-
natively, we may use a vector quantization approach that uses a model involving an
encoder and a decoder; this way is motivated by considerations of communication the-
ory (Luttrell, 1989b, 1991a). In this chapter, we consider both approaches.
The Kohonen model has received much more attention in the literature than the
Willshaw–von der Malsburg model. It possesses certain properties, discussed later in
the chapter, that make it possible for the Kohonen model to capture the essential fea-
tures of cortical maps.
9.3
SELF-ORGANIZING MAP
The principal goal of the self-organizing map (SOM) is to transform an incoming sig-
nal pattern of arbitrary dimension into a one- or two-dimensional discrete map, and
to perform this transformation adaptively in a topologically ordered fashion. Figure 9.2
shows the schematic diagram of a two-dimensional lattice of neurons commonly used
428
Chapter 9
Self-Organizing Maps

as a discrete map. Each neuron in the lattice is fully connected to all the source
nodes in the input layer. This network represents a feedforward structure with a
single computational layer consisting of neurons arranged in rows and columns. A
one-dimensional lattice is a special case of the configuration depicted in Fig. 9.2: in
this special case, the computational layer consists simply of a single column or row
of neurons.
Each input pattern presented to the network typically consists of a localized region
or “spot” of activity against a quiet background.The location and nature of such a spot
usually varies from one realization of the input pattern to another. All the neurons in
the network should therefore be exposed to a sufficient number of different realiza-
tions of the input pattern in order to ensure that the self-organization process has a
chance to develop properly.
The algorithm responsible for the formation of the self-organizing map proceeds
first by initializing the synaptic weights in the network. This can be done by assigning
them small values picked from a random-number generator; in so doing, no prior order
is imposed on the feature map. Once the network has been properly initialized,there are
three essential processes involved in the formation of the self-organizing map, as sum-
marized here:
1. Competition. For each input pattern, the neurons in the network compute their
respective values of a discriminant function.This discriminant function provides the
basis for competition among the neurons. The particular neuron with the largest
value of discriminant function is declared winner of the competition.
Section 9.3
Self-Organizing Map
429
Layer
of source
nodes
Input
vector
Outputs
FIGURE 9.2
Two-dimensional lattice of neurons, illustrated for a three-dimensional input
and four-by-four dimensional output (all shown in red).

2. Cooperation. The winning neuron determines the spatial location of a topological
neighborhood of excited neurons, thereby providing the basis for cooperation
among such neighboring neurons.
3. Synaptic Adaptation. This last mechanism enables the excited neurons to increase
their individual values of the discriminant function in relation to the input pattern
through suitable adjustments applied to their synaptic weights. The adjustments
made are such that the response of the winning neuron to the subsequent appli-
cation of a similar input pattern is enhanced.
The processes of competition and cooperation are in accordance with two of the
four principles of self-organization described in Chapter 8.As for the principle of self-
amplification, it comes in a modified form of Hebbian learning in the adaptive process.
As explained in Chapter 8,the presence of redundancy in the input data,though not men-
tioned explicitly in describing the SOM algorithm, is essential for learning, since it pro-
vides knowledge about the underlying structure of the input activation patterns.Detailed
descriptions of the processes of competition, cooperation, and synaptic adaptation are
presented in what follows.
Competitive Process
Let m denote the dimension of the input (data) space. Let an input pattern (vector)
selected at random from the input space be denoted by
(9.1)
The synaptic-weight vector of each neuron in the network has the same dimension as the
input space. Let the synaptic-weight vector of neuron j be denoted by
(9.2)
where l is the total number of neurons in the network. To find the best match of the
input vector x with the synaptic-weight vectors wj, we compare the inner products wT
j x
wj = [wj1, wj2, ..., wjm]T, 
j = 1, 2, ..., l
x = [x1, x2, ..., xm]T
430
Chapter 9
Self-Organizing Maps
for j  1, 2, …, l and select the largest.This method assumes that the same threshold is
applied to all the neurons;the threshold is the negative of bias.Thus,by selecting the neu-
ron with the largest inner product 
in effect we will have determined the location
where the topological neighborhood of excited neurons is to be centered.
From the introductory chapter, we recall that the best-matching criterion, based on
maximizing the inner product 
is mathematically equivalent to minimizing the
Euclidean distance between the vectors x and wj, provided that wj has unit length for all j.
If we use the index i(x) to identify the neuron that best matches the input vector x, we
may then determine i(x) by applying the following condition, which sums up the essence
of the competition process among the neurons.4
(9.3)
where 
denotes the lattice of neurons. According to Eq. (9.3), i(x) is the subject of
attention because we want to find the identity of neuron i.The particular neuron i that
satisfies this condition is called the best-matching, or winning, neuron for the input vec-
tor x. Equation (9.3) leads to the following observation:
a
i(x) = arg min
j
7x - wj7, 
j  a
wT
j x,
wT
j x,

A continuous input space of activation patterns is mapped onto a discrete output space of
neurons by a process of competition among the neurons in the network.
Depending on the application of interest, the response of the network could be
either the index of the winning neuron (i.e., its position in the lattice) or the synaptic-
weight vector that is closest to the input vector in a Euclidean sense.
Cooperative Process
The winning neuron locates the center of a topological neighborhood of cooperating
neurons.The key question is: How do we define a topological neighborhood that is neu-
robiologically correct?
To answer this basic question, remember that there is neurobiological evidence
for lateral interaction among a set of excited neurons in the human brain. In particular,
a neuron that is firing tends to excite the neurons in its immediate neighborhood more
than those farther away from it, which is intuitively satisfying.This observation leads us
to introduce a topological neighborhood around the winning neuron i and make it decay
smoothly with lateral distance (Lo et al., 1991, 1993; Ritter et al., 1992).5 To be specific,
let hj,i denote the topological neighborhood centered on winning neuron i and encom-
passing a set of excited (cooperating) neurons, a typical one of which is denoted by j. Let
dj,i denote the lateral distance between the winning neuron i and the excited neuron j.
Then, we may assume that the topological neighborhood hj,i is a unimodal function of
the lateral distance dj,i, such that it satisfies two distinct requirements:
1. The topological neighborhood hj,i is symmetric about the maximum point defined
by dj,i  0; in other words, it attains its maximum value at the winning neuron i for
which the distance dj,i is zero.
2. The amplitude of the topological neighborhood hj,i decreases monotonically with
increasing lateral distance dj,i, decaying to zero for 
; this is a necessary
condition for convergence.
A good choice of hj,i that satisfies these requirements is the Gaussian function6
(9.4)
which is translation invariant (i.e., independent of the location of winning neuron i).The
parameter  is the “effective width” of the topological neighborhood, as illustrated in
Fig. 9.3; it measures the degree to which excited neurons in the vicinity of the winning
neuron participate in the learning process. In a qualitative sense, the Gaussian topolog-
ical neighborhood of Eq. (9.4) is more biologically appropriate than a rectangular one
that was used in the past. The use of a Gaussian topological neighborhood also makes
the SOM algorithm converge more quickly than a rectangular topological neighbor-
hood would (Lo et al., 1991, 1993; Erwin et al., 1992a).
For cooperation among neighboring neurons to hold, it is necessary that topolog-
ical neighborhood hj,i be dependent on the lateral distance dj,i between the winning
neuron i and excited neuron j in the output space, rather than on some distance mea-
sure in the original input space. This is precisely what we have in Eq. (9.4). In the case
hj, i(x) = exp a-
d2
j,i
22 b , 
j  a
dj,i S q
Section 9.3
Self-Organizing Map
431

of a one-dimensional lattice, dj,i is an integer equal to 
. On the other hand, in the
case of a two-dimensional lattice, it is defined by
(9.5)
where the discrete vector rj defines the position of excited neuron j and ri defines the
position of the winning neuron i, both of which are measured in the discrete output
space.
Another unique feature of the SOM algorithm is that the size of the topological
neighborhood is permitted to shrink with time.This requirement is satisfied by making
the width  of the topological neighborhood function hj,i decrease with time.A popular
choice for the dependence of  on discrete time n is the exponential decay described by
(9.6)
where 0 is the value of  at the initiation of the SOM algorithm and 1 is a time con-
stant to be chosen by the designer (Ritter et al., 1992; Obermayer et al., 1991). Corre-
spondingly, the topological neighborhood function assumes a time-varying form of its
own, as shown by
(9.7)
where (n) is defined by Eq. (9.6). Thus, as discrete time n (i.e., the number of itera-
tions) increases, the width (n) decreases at an exponential rate, and the topological
neighborhood shrinks in a corresponding manner.It is important to note,however,that the
neighborhood function will eventually still have a unity value for the winning neuron i,
since the distance dj,i for neuron j is calculated in the lattice space and compared to the
winning neuron i.
There is another useful way of viewing the variation of the neighborhood function
hj,i(x)(n) around a winning neuron i(x) across time n. The purpose of a wide hj,i(x)(n) is
hj, i(x)(n) = exp a-
d2
j,i
22(n)
b , 
n = 0, 1, 2, ...,
(n) = 0 exp a- n
1
b 
n = 0, 1, 2, ...,
d2
j, i = 7rj - ri7 2
j - i
432
Chapter 9
Self-Organizing Maps
1.0
0.61
2
hj,i
0
dj,i
FIGURE 9.3
Gaussian
neighborhood function.

essentially to correlate the directions of the weight updates of a large number of excited
neurons in the lattice.As the width of hj,i(x)(n) is decreased, so is the number of neurons
whose update directions are correlated.This phenomenon becomes particularly obvious
when the training of a self-organizing map is played out on a computer screen. It is
rather wasteful of computer resources to move a large number of degrees of freedom
around a winning neuron in a correlated fashion, as in the usual implementation of the
SOM algorithm. Instead, it is much better to use a renormalized SOM form of training,
according to which we work with a much smaller number of normalized degrees of free-
dom.This operation is easily performed in discrete form by having a neighborhood func-
tion hj,i(x)(n) of constant width, but gradually increasing the total number of neurons that
lie inside the neighborhood function.The new neurons are inserted halfway between the
old ones, and the smoothness properties of the SOM algorithm guarantee that the new
ones join the synaptic adaptation in a graceful manner (Luttrell, 1989a).A summary of
the renormalized SOM algorithm is presented in Problem 9.15.
Adaptive Process
Now we come to the last process, the synaptic adaptive process, in the self-organized
formation of a feature map. For the network to be self-organizing, the synaptic-weight
vector wj of neuron j in the network is required to change in relation to the input vec-
tor x.The question is how to make the change. In Hebb’s postulate of learning, a synap-
tic weight is increased with a simultaneous occurrence of presynaptic and postsynaptic
activities.The use of such a rule is well suited for associative learning (e.g., principal-
components analysis).For the type of unsupervised learning being considered here,how-
ever, the Hebbian hypothesis in its basic form is unsatisfactory for the following reason:
Changes in connectivities occur in one direction only, finally driving all the synaptic
weights into saturation.To overcome this problem, we modify the Hebbian hypothesis
by including the forgetting term g(yj)wj, where wj is the synaptic-weight vector of neu-
ron j and g(yj) is some positive scalar function of the response yj.The only requirement
imposed on the function g(yj) is that the constant term in the Taylor series expansion of
g(yj) be zero, so that we may write
(9.8)
The significance of this requirement will become apparent momentarily. Given such
a function, we may then express the change to the weight vector of neuron j in the
lattice as:
(9.9)
where  is the learning-rate parameter of the algorithm.The first term on the right-hand
side of Eq. (9.9) is the Hebbian term, and the second term is the forgetting term.To sat-
isfy the requirement of Eq. (9.8), we choose a linear function for g(yj), as shown by
(9.10)
For a winning neuron i(x), we may further simplify Eq. (9.9) by setting the response
(9.11)
yj = hj, i(x)
g(yj) = yj
¢wj = yjx - g(yj)wj
g(yj) = 0  for yj = 0
Section 9.3
Self-Organizing Map
433

Using Eqs. (9.10) and (9.11) in Eq. (9.9), we obtain
,
(9.12)
Finally, using discrete-time formalism, given the synaptic-weight vector wj(n) of
neuron j at time n, we define the updated weight vector wj(n  1) at time n  1 by
(9.13)
which is applied to all the neurons in the lattice that lie inside the topological neighbor-
hood of winning neuron i (Kohonen, 1982; Ritter et al., 1992; Kohonen, 1997). Equation
(9.13) has the effect of moving the synaptic-weight vector wi of winning neuron i toward
the input vector x. Upon repeated presentations of the training data, the synaptic-weight
vectors tend to follow the distribution of the input vectors because of the neighbor-
hood updating. The algorithm therefore leads to a topological ordering of
the feature map in the input space in the sense that neurons that are adjacent in the
lattice will tend to have similar synaptic-weight vectors. We have more to say on this
issue in Section 9.4.
Equation (9.13) is the desired formula for computing the synaptic weights of the
feature map. In addition to this equation, however, we need the heuristic of Eq. (9.7) for
selecting the neighborhood function hj,i(x)(n).
The learning-rate parameter (n) should also be time varying, as indicated in
Eq. (9.13), which is how it should be for stochastic approximation.In particular, it should
start at some initial value 0 and then decrease gradually with increasing time n. This
requirement can be satisfied by the following heuristic:
(9.14)
where 2 is another time constant of the SOM algorithm.According to this second heuris-
tic, the learning-rate parameter decays exponentially with time n. Even though the
exponential-decay formulas described in Eqs. (9.6) and (9.14) for the width of the neigh-
borhood function and the learning-rate parameter,respectively,may not be optimal,they
are usually adequate for the formation of the feature map in a self-organized manner.
Two Phases of the Adaptive Process: Ordering and Convergence
Starting from an initial state of complete disorder, it is amazing how the SOM algorithm
gradually leads to an organized representation of activation patterns drawn from the
input space, provided that the parameters of the algorithm are selected properly. We
may decompose the adaptation of the synaptic weights in the network, computed in
accordance with Eq. (9.13), into two phases: an ordering or self-organizing phase, fol-
lowed by a convergence phase.These two phases of the adaptive process are described
as follows (Kohonen, 1982, 1997a):
1. Self-organizing or ordering phase. It is during this first phase of the adaptive process
that the topological ordering of the weight vectors takes place.The ordering phase
may take as many as 1,000 iterations of the SOM algorithm, and possibly even
(n) = 0 exp a- n
2
b ,  n = 0, 1, 2, ...,
wj(n + 1) = wj(n) + (n)hj, i(x)(n)(x(n) - wj(n))
ei: winning neuron
j: excited (activated) neuron
¢wj = hj,i(x)(x - wj)
434
Chapter 9
Self-Organizing Maps

more. Careful consideration must therefore be given to the choice of the learning-
rate parameter and neighborhood function, as described here:
• The learning-rate parameter (n) should begin with a value close to 0.1; there-
after it should decrease gradually, but remain above 0.01 (i.e., it should never be
allowed to get to zero). These desirable values are satisfied by the following
choices in the formula of Eq. (9.14):
• The neighborhood function hj,i(n) should initially include almost all neurons in
the network centered on the winning neuron i and then shrink slowly with time.
Specifically, during the ordering phase—which, again, may occupy 1,000 iterations
or more—hj,i(n) is permitted to reduce to a small value of only a couple of neigh-
boring neurons around the winning neuron or to the winning neuron by itself.
Assuming the use of a two-dimensional lattice of neurons for the discrete map,
we may thus set the initial size 0 of the neighborhood function equal to the
“radius” of the lattice. Correspondingly, we may set the time constant 1 in the for-
mula of Eq. (9.6) as:
2. Convergence phase. This second phase of the adaptive process is needed to fine-
tune the feature map and therefore provide an accurate statistical quantification
of the input space. Moreover, the number of iterations needed for convergence
depends strongly on the dimensionality of the input space. As a general rule,
the number of iterations constituting the convergence phase must be at least 
500 times the number of neurons in the network. Thus, the convergence phase
may have to go on for thousands, and possibly even tens of thousands, of iterations.
The choice of learning-rate parameter and neighborhood function should be made
as follows:
• For good statistical accuracy, the learning-rate parameter (n) should be main-
tained during the convergence phase at a small value, on the order of 0.01. As
stated previously,
must not be allowed to decrease to zero; otherwise, it is
possible for the network to get stuck in a metastable state. A metastable state
belongs to a configuration of the feature map with a topological defect.The expo-
nential decay of Eq.(9.14) guarantees against the possibility of metastable states.
• The neighborhood function hj, i(x) should contain only the nearest neighbors of
a winning neuron, which may eventually reduce to one or zero neighboring
neurons.
One other comment is in order: In discussing the issues of ordering and convergence,
we have stressed the numbers of iterations needed to achieve them. In some software
packages, however, epochs (rather than iterations) are used to describe these two
issues.
(n)
1 = 1000
log0
2 = 1000
0 = 0.1
Section 9.3
Self-Organizing Map
435

Summary of the SOM Algorithm
The essence of Kohonen’s SOM algorithm is that it substitutes a simple geometric com-
putation for the more detailed properties of the Hebb-like rule and lateral interactions.
The essential ingredients and parameters of the algorithm are as follows:
• a continuous input space of activation patterns that are generated in accordance
with a certain probability distribution;
• a topology of the network in the form of a lattice of neurons, which defines a dis-
crete output space;
• a time-varying neighborhood function hj,i(x)(n) that is defined around a winning
neuron i(x);
• a learning-rate parameter (n) that starts at an initial value 0 and then decreases
gradually with time n, but never goes to zero.
For the neighborhood function and learning-rate parameter, we may use Eqs. (9.7) and
(9.14), respectively, for the ordering phase (i.e., the first 1,000 iterations or so). For good
statistical accuracy, (n) should be maintained at a small value (0.01 or less) during the
convergence for a fairly long period of time, which is typically thousands of iterations.
As for the neighborhood function, it should contain only the nearest neighbors of the
winning neuron at the start of the convergence phase and may eventually shrink to one
or zero neighboring neurons.
There are three basic steps involved in the application of the algorithm after ini-
tialization: sampling, similarity matching, and updating.These three steps are repeated
until formation of the feature map has been completed.The algorithm is summarized as
follows:
1. Initialization. Choose random values for the initial weight vectors wj(0).The only
restriction here is that the wj(0) be different for j  1, 2, ..., l, where l is the num-
ber of neurons in the lattice. It may be desirable to keep the magnitude of the
weights small.
Another way of initalizing the algorithm is to select the weight vectors
from the available set of input vectors 
in a random manner.The
advantage of this alternative choice is that the initial map will be in the range of
the final map.
2. Sampling. Draw a sample x from the input space with a certain probability; the
vector x represents the activation pattern that is applied to the lattice.The dimen-
sion of vector x is equal to m.
3. Similarity matching. Find the best-matching (winning) neuron i(x) at time-step n
by using the minimum-distance criterion
4. Updating. Adjust the synaptic-weight vectors of all excited neurons by using the
update formula
wj(n + 1) = wj(n) + (n)hj, i(x)(n)(x(n) - wj(n))
i(x) = arg min
j
7x(n) - wj7,  j = 1, 2, ..., l
{xi}i=1
N
{wj(0)}l
j=1
436
Chapter 9
Self-Organizing Maps

where (n) is the learning-rate parameter and hj,i(x)(n) is the neighborhood func-
tion centered around the winning neuron i(x); both (n) and hj,i(x)(n) are varied
dynamically during learning for best results.
5. Continuation. Continue with step 2 until no noticeable changes in the feature map
are observed.
9.4
PROPERTIES OF THE FEATURE MAP
Once the SOM algorithm has converged,the feature map computed by the algorithm dis-
plays important statistical characteristics of the input space.
To begin with, let 
denote a spatially continuous input (data) space, the topology of
which is defined by the metric relationship of the vectors 
. Let 
denote a spatially
discrete output space,the topology of which is endowed by arranging a set of neurons as the
computation nodes of a lattice. Let ) denote a nonlinear transformation called a feature
map, which maps the input space 
onto the output (i.e., lattice) space 
, as shown by
(9.15)
Equation (9.15) may be viewed as an abstraction of Eq. (9.3), which defines the location
of a winning neuron i(x) developed in response to an input vector x. For example, in a
neurobiological context, the input space 
may represent the coordinate set of
somatosensory receptors distributed densely over the entire body surface. Corre-
spondingly, the output space 
represents the set of neurons located in that layer of the
cerebral cortex to which the somatosensory receptors are projecting.
Given an input vector x, the SOM algorithm proceeds by first identifying a best-
matching, or winning neuron, i(x) in the output space 
, in accordance with the feature
map ).The synaptic-weight vector wi of neuron i(x) may then be viewed as a pointer for
that neuron into the input space .
Thus, as depicted in Fig. 9.4, the SOM algorithm embodies two ingredients that
define the algorithm:
• A projection from the continuous input data space 
onto the discrete output
neural space 
. In this way, an input vector is mapped onto a “winning neuron” in
the lattice structure in accordance with the similarity matching step (i.e., step 3) in
the algorithmic summary presented in Section 9.3.
• A pointer from the output space back to the input space.In effect,the pointer defined
by the weight vector of the winning neuron identifies a particular point in the input
data space as the “image”of the winning neuron;this operation is iteratively carried
out in accordance with the updating step (i.e., step 4) in the algorithmic summary.
In other words, there is communication, back and forth, between the output space where
the lattice of neurons resides and the input space where the examples are generated.
The SOM algorithm has some important properties that are discussed next.
Property 1. Approximation of the Input Space
The feature map ), represented by the set of synaptic weight vectors {wj} in the output space
,
provides a good approximation to the input space
.
x
a
a
x
x
a
a
x
£: x S a
a
x
a
x  x
x
Section 9.4
Properties of the Feature Map
437

The basic aim of the SOM algorithm is to store a large set of input vectors 
by finding a smaller set of prototypes 
so as to provide a good approximation to
the original input space . The theoretical basis of the idea just described is rooted in
vector quantization theory, the motivation for which is dimensionality reduction or data
compression (Gersho and Gray, 1992). It is therefore appropriate to present a brief dis-
cussion of this theory.
Consider Fig. 9.5, where c(x) acts as an encoder of the input vector x and x(c) acts
as a decoder of c(x).The vector x is selected at random from a training sample (i.e., input
space 
), subject to an underlying probability density function pX(x). The optimum
encoding–decoding scheme is determined by varying the functions c(x) and x(c) so as
to minimize the expected distortion, defined by
(9.16)
D = 1
2 3
q
-q
pX(x)d(x, x¿)dx
x
x
wj  a
x  x
438
Chapter 9
Self-Organizing Maps
Discrete
output space 
Continuous
input space 
Feature
map Φ
i(x)
x
wi
FIGURE 9.4
Illustration of
the relationship between
feature map ) and weight
vector wi of winning neuron i.
Encoder
c(x)
Code
c(x)
Decoder
x(c)
Input
vector
x
Reconstruction
vector
x(c)
FIGURE 9.5
Encoder–decoder
model for describing Property 1
of the SOM model.

where the factor has been introduced for convenience of presentation and d(x, x) is
a distortion measure.The integration is performed over the entire input space ,assumed
to be of dimensionality m, hence, the use of the differential variable dx in Eq. (9.16).A
popular choice for the distortion measure d(x, x) is the square of the Euclidean dis-
tance between the input vector x and the reconstruction vector x; that is,
(9.17)
Thus, we may rewrite Eq. (9.16) as
(9.18)
The necessary conditions for the minimization of the expected distortion D are embodied
in the generalized Lloyd algorithm7 (Gersho and Gray, 1992).The conditions are twofold:
Condition 1. Given the input vector x, choose the code c  c(x) to minimize the
squared-error distortion 
.
7x - x¿(c)7 2
D = 1
2 3
q
- q
pX(x)7x - x¿7 2dx
= (x - x¿)T(x - x¿)
d(x, x¿) = 7x - x¿7 2
x
1
2
Section 9.4
Properties of the Feature Map
439
Condition 2. Given the code c, compute the reconstruction vector x  x(c) as
the centroid of those input vectors x that satisfy condition 1.
Condition 1 is recognized as a nearest-neighbor encoding rule. Both conditions imply that
the average distortion D is stationary (i.e.,at a local minimum) with respect to variations in
the encoder c(x) and decoder x(c),respectively.To implement vector quantization,the gen-
eralized Lloyd algorithm operates in a batch training mode.Basically,the algorithm consists
of alternately optimizing the encoder c(x) in accordance with condition 1 and then opti-
mizing the decoder x(c) in accordance with condition 2 until the expected distortion D
reaches a minimum.In order to overcome the local-minimum problem,it may be necessary
to run the generalized Lloyd algorithm several times with different initial code vectors.
The generalized Lloyd algorithm is closely related to the SOM algorithm,as shown
in Luttrell (1989b). We may delineate the form of this relationship by considering the
scheme shown in Fig. 9.6, where we have introduced an additive signal-independent
“noise-term”following the encoder c(x).The noise,denoted by v, is associated with a fic-
titious “communication channel” between the encoder and the decoder, the purpose of
which is to account for the possibility that the output code c(x) may be distorted. On the
basis of the model shown in Fig. 9.6, we may consider a modified form of the expected
distortion given as
(9.19)
where (v) is the probability density function (pdf) of the additive noise v.The inner inte-
gration is over all possible realizations of this noise,hence the use of the incremental vari-
able dv in Eq. (9.19).
In accordance with the strategy described for the generalized Lloyd algorithm,
there are two separate optimizations to be considered for the model of Fig. 9.6, one per-
taining to the encoder and the other pertaining to the decoder. To find the optimum
D1 = 1
2 3
q
-q
pX(x)
3
q
-q
(v)7x - x¿(c(x) + v)7 2dv dx

encoder for a fixed x, we need the partial derivative of the expected distortion measure
D1 with respect to the decoded vector c. Using Eq. (9.19), we thus obtain
(9.20)
To find the optimum decoder for a fixed c, we need the partial derivative of the expected
distortion measure D1 with respect to the decoded vector x(c). Using Eq. (9.19), we
thus obtain
(9.21)
Hence, in light of Eqs. (9.20) and (9.21), conditions 1 and 2 stated earlier for the gener-
alized Lloyd algorithm are modified as follows (Luttrell, 1989b):
Condition I. Given the input vector x,choose the code c  c(x) to minimize the dis-
tortion measure
(9.22)
Condition II. Given the code c, compute the reconstruction vector x(c) to satisfy
the condition
(9.23)
Equation (9.23) is obtained by setting the partial derivative 
in Eq. (9.21)
equal to zero and then solving for x(c).
0D10x¿(c)
x¿(c) = 3
q
-q
pX(x)(c - c(x))xdx
3
q
-q
pX(x)(c - c(x))dx
D2 =
3
q
-q
(v)7x - x¿(c(x) + v)7 2dv
0D1
0x¿(c) = -
3
q
-q
pX(x)(c - c(x))(x - x¿(c))dx
0D1
0c
= 1
2 pX(x)
3
q
-q
(v) 0
0c 7x - x¿(c)7 2c = c(x) + vdv
440
Chapter 9
Self-Organizing Maps
Encoder
c(x)
Code
c(x)
Noise
v
Decoder
x(c)
Input
vector
x
Reconstruction
vector
x
Σ
FIGURE 9.6
Noisy encoder–
decoder model.

The encoder-decoder model described in Fig. 9.5 may now be viewed as a special
case of that shown in Fig. 9.6. In particular, if we set the probability density function
(v) of the noise v equal to a Dirac delta function (v),conditions I and II reduce to con-
ditions 1 and 2, respectively, for the generalized Lloyd algorithm.
To simplify condition I, we assume that (v) is a smooth function of v. It may then
be shown that, to a second-order of approximation, the distortion measure D2 defined
in Eq. (9.22) consists of two components (Luttrell, 1989b):
• the conventional distortion term, defined by the squared-error distortion
;
• a curvature term that arises from the noise model (v).
Assuming that the curvature term is small, condition I for the model in Fig. 9.6 may be
approximated by condition 1 for the noiseless model in Fig. 9.5.This approximation, in
turn, reduces condition I to a nearest-neighbor encoding rule as before.
As for condition II, we may realize it by using stochastic descent learning. In par-
ticular, we choose input vectors x at random from the input space , in accordance with
the 
and then update the reconstruction vector x(c) as
(9.24)
where  is the learning-rate parameter and c(x) is the nearest-neighbor encoding
approximation to condition 1.The update equation of Eq. (9.24) is obtained by inspec-
tion of the partial derivative in Eq. (9.21). This update is applied to all c, for which
we have
(9.25)
We may think of the gradient descent procedure described in Eq. (9.24) as a way of min-
imizing the distortion measure D1 of Eq. (9.19).That is, Eqs. (9.23) and (9.24) are essen-
tially of the same type, except for the fact that Eq. (9.23) is batch and Eq. (9.24) is
continuous (i.e., in flowthrough form).
The update equation of Eq. (9.24) is identical to the (continuous) SOM algorithm
of Eq. (9.13), bearing in mind the correspondences listed in Table 9.1. Accordingly, we
may state that the generalized Lloyd algorithm for vector quantization is the batch train-
ing version of the SOM algorithm with zero neighborhood size; for zero neighborhood,
(0)  1. Note that in order to obtain the generalized Lloyd algorithm from the batch
version of the SOM algorithm, we do not need to make any approximations, because
(c - c(x)) 7 0
x¿new(c) d x¿old(c) + (c - c(x))[x - x¿old(c)]
pX(x),
x
7 x - x¿(c)7 2
Section 9.4
Properties of the Feature Map
441
TABLE 9.1
Correspondence between the SOM Algorithm and the 
Model of Fig. 9.6
Encoding–Decoding
Model of Fig. 9.6
SOM Algorithm
Encoder c(x)
Best-matching neuron i(x)
Reconstruction vector x(c)
Synaptic-weight vector wj
Probability density function (c - c(x))
Neighborhood function hj,i(x)

the curvature terms (and all higher-order terms) make no contribution when the neighbor-
hood has zero width.
The important points to note from the discussion presented here are as follows:
1. The SOM algorithm is a vector quantization algorithm, which provides a good
approximation to the input space . This viewpoint provides another approach
for deriving the SOM algorithm, as exemplified by Eq. (9.24).
2. According to this viewpoint, the neighborhood function hj,i(x) in the SOM algo-
rithm has the form of a probability density function. In Luttrell (1991a), a zero-
mean Gaussian model is considered appropriate for the noise v in the model of
Fig.9.6.We thus also have theoretical justification for adopting the Gaussian neighbor-
hood function of Eq. (9.4).
The batch SOM8 is merely a rewrite of Eq. (9.23), with summations used to approxi-
mate the integrals in the numerator and denominator of the right-hand side of the equa-
tion.Note that in this version of the SOM algorithm,the order in which the input patterns
are presented to the network has no effect on the final form of the feature map, and
there is no need for a learning-rate schedule. But the algorithm still requires the use of
a neighborhood function.
Property 2. Topological Ordering
The feature map ) computed by the SOM algorithm is topologically ordered in the sense
that the spatial location of a neuron in the lattice corresponds to a particular domain or feature
of input patterns.
The topological ordering property9 is a direct consequence of the update equa-
tion of Eq. (9.13), which forces the synaptic-weight vector wi of the winning neuron i(x)
to move toward the input vector x. It also has the effect of moving the synaptic-weight
vectors wj of the closest excited neurons j along with the winning neuron i(x). We may
therefore visualize the feature map ) as an elastic or virtual net with the topology of a
one- or two-dimensional lattice as prescribed in the output space 
, and whose nodes
have weights as coordinates in the input space 
(Ritter, 2003). The overall aim of the
algorithm may thus be stated as follows:
Approximate the input space
by pointers or prototypes in the form of synaptic-weight vec-
x
x
a
x
442
Chapter 9
Self-Organizing Maps
tors 
in such a way that the feature map ) provides a faithful representation of the impor-
tant features that characterize the input vectors
in terms of a certain statistical criterion.
The feature map ) is usually displayed in the input space . Specifically, all the point-
ers (i.e., synaptic-weight vectors) are shown as dots, and the pointers of neighboring
neurons are connected with lines in accordance with the topology of the lattice.Thus, by
using a line to connect two pointers wi and wj, we are indicating that the corresponding
neurons i and j are neighboring neurons in the lattice.
Property 3. Density Matching
The feature map ) reflects variations in the statistics of the input distribution: Regions in the
input space
from which sample vectors x are drawn with a high probability of occurrence
x
x
x  x
wj

are mapped onto larger domains of the output space , and therefore with better resolution than
regions in
from which sample vectors x are drawn with a low probability of occurrence.
Let pX(x) denote the multidimensional pdf of the random input vector X, a sam-
ple realization of which is denoted by x. This pdf, integrated over the entire input space
, must by definition, equal unity:
Let m(x) denote the map magnification factor, defined as the number of neurons in a
small volume dx of the input space .The magnification factor, integrated over the input
space , must contain the total number l of neurons in the network, as shown by
(9.26)
For the SOM algorithm to match the input density exactly, we therefore require the fol-
lowing proportionality relationship (Amari, 1980):
(9.27)
This property implies that if a particular region of the input space contains frequently
occurring stimuli, it will be represented by a larger area in the feature map than a region
of the input space where the stimuli occur less frequently.
Generally speaking,in two-dimensional feature maps,the magnification factor m(x)
is not expressible as a simple function of the probability density function pX(x) of the input
vector x.It is only in the case of a one-dimensional feature map that it is possible to derive
such a relationship. For this special case, we find that, contrary to an earlier supposition
(Kohonen, 1982), the magnification factor m(x) is not proportional to pX(x). Two differ-
ent results are reported in the literature, depending on the encoding method advocated:
1. Minimum-distortion encoding, according to which the curvature terms and all
higher-order terms in the distortion measure of Eq. (9.22) due to the noise model
(v) are retained.This encoding method yields the result
(9.28)
which is the same as the result obtained for the standard vector quantizer (Luttrell,
1991a).
2. Nearest-neighbor encoding, which emerges if the curvature terms are ignored, as
in the standard form of the SOM algorithm.This second encoding method yields
the following result (Ritter, 1991):
(9.29)
Our earlier statement that a cluster of frequently occurring input stimuli is represented
by a larger area in the feature map still holds, albeit in a distorted version of the ideal
condition described in Eq. (9.27).
As a general rule (confirmed by computer simulations), the feature map computed
by the SOM algorithm tends to overrepresent regions of low input density and to
m(x) r pX
23(x)
m(x) r pX
13(x)
m(x) r pX(x)
3
q
-q
m(x)dx = l
x
x
3
q
- q
pX(x)dx = 1
x
x
Section 9.4
Properties of the Feature Map
443

underrepresent regions of high input density.In other words,the SOM algorithm fails to pro-
vide a faithful representation of the probability distribution that underlies the input space.10
Property 4. Feature Selection
Given data from an input space, the self-organizing map is able to select a set of best features
for approximating the underlying distribution.
This property is a natural culmination of Properties 1 through 3. In a loose sense,
Property 4 brings to mind the idea of principal-components analysis that was discussed in
the previous chapter,but with an important difference,as illustrated in Fig.9.7.Figure 9.7a
shows a two-dimensional distribution of zero-mean data points resulting from a linear
444
Chapter 9
Self-Organizing Maps
(a)
(b)
Output
Input
Output
0
Input
FIGURE 9.7
(a) Two-dimensional
distribution produced by a linear
input–output mapping. (b) Two-
dimensional distribution produced
by a nonlinear input–output
mapping.

input–output mapping corrupted by additive noise.In such a situation,principal-components
analysis works perfectly fine:It tells us that the best description of the “linear”distribution
in Fig. 9.7a is defined by a straight line (i.e., one-dimensional “hyperplane”) that passes
through the origin and runs parallel to the eigenvector associated with the largest eigenvalue
of the correlation matrix of the data. Consider next the situation described in Fig. 9.7b,
which is the result of a nonlinear input–output mapping corrupted by additive noise of zero
mean. In this second situation, it is impossible for a straight-line approximation computed
from principal-components analysis to provide an acceptable description of the data. On
the other hand, the use of a self-organizing map built on a one-dimensional lattice of neu-
rons is able to overcome this approximation problem by virtue of its topological-ordering
property.This latter approximation,illustrated in Fig.9.7b,works well only when the dimen-
sionality of the lattice matches the intrinsic dimensionality of the distribution.
9.5
COMPUTER EXPERIMENTS: DISENTANGLING LATTICE DYNAMICS
USING SOM
I. Two-dimensional lattice driven by two-dimensional stimulus
We illustrate the behavior of the SOM algorithm by using computer simulations to study
a network with 576 neurons,arranged in the form of a two-dimensional lattice with 24 rows
and 24 columns.The network is trained with a two-dimensional input vector x, whose
elements x1 and x2 are uniformly distributed in the region {x1, x2 in (-1, 1)}.To initialize
the network, randomly chosen values are assigned to the synaptic weights.
Figure 9.8 shows three stages of training as the network learns to represent the
input distribution, starting with Fig. 9.8a that shows the uniform distribution of data
used to train the feature map.Figure 9.8b shows the initial values of the synaptic weights,
randomly chosen. Figures 9.8c and 9.8d display the 24-by-24 maps computed by the
SOM algorithm, after completion of the ordering and convergence phases, respectively.
The lines drawn in Fig. 9.8 connect neighboring neurons (across rows and columns) in
the network, as discussed previously under Property 2.
The results shown in Fig. 9.8 demonstrate the ordering phase and the convergence
phase that characterize the learning process of the SOM algorithm. During the order-
ing phase,the map unfolds to form a mesh,as shown in Fig.9.8c.The neurons are mapped
in the correct order at the end of this phase. During the convergence phase, the map
spreads out to fill the input space. At the end of this second phase, shown in Fig. 9.8d,
the statistical distribution of the neurons in the map approaches that of the input vectors,
except for some distortion. Comparing the final state of the feature map in Fig. 9.8d
with the uniform distribution of the input in Fig. 9.8a, we see that the tuning of the map
during the convergence phase has captured the local irregularities that can be seen in
the input distribution.
The topological-ordering property of the SOM algorithm is well illustrated in
Fig. 9.8d. In particular, we observe that the algorithm (after convergence) captures the
underlying topology of the uniform distribution at the input.In the computer simulations
presented in Fig. 9.8, the input space 
and output space 
are both two-dimensional.
a
x
Section 9.5
Computer Experiments: Disentangling Lattice Dynamics Using SOM
445

II. One-dimensional lattice driven by two-dimensional stimulus
We now examine the case for which the dimension of the input space 
is greater than
x
446
Chapter 9
Self-Organizing Maps
(a) Input distribution
Time  0
(b) Initial weights
0.8
0.6
0.4
0.2
00
0.5
1
1
0.2
0.1
0
0.1
0.2
0.2
0
0.2
Time  160 K
(c) Ordering phase
0.8
0.6
0.4
0.2
00
0
0.5
1
1
Time  800 K
(d) Convergence phase
0.8
0.6
0.4
0.2
0
0.5
1
1
FIGURE 9.8
(a) Distribution of the input data. (b) Initial condition of the two-dimensional
lattice. (c) Condition of the lattice at the end of the ordering phase. (d) Condition of the
lattice at the end of the convergence phase. The times indicated under maps (b), (c), and (d)
represent the numbers of iterations.
the dimension of the output space 
. In spite of this mismatch, the feature map ) is
often able to form a topological representation of the input distribution.Figure 9.9 shows
three different stages in the evolution of a feature map initialized as in Fig. 9.9b and
trained with input data drawn from a uniform distribution inside a square as in Fig. 9.9a,
but this time the computation is performed with a one-dimensional lattice of 100 neu-
rons. Figures 9.9c and 9.9d show the feature map after the completion of the ordering
and convergence phases, respectively. Here, we see that the feature map computed by
the algorithm is very distorted in order to fill the square as densely as possible and
thereby provide a reasonably good approximation to the underlying topology of the two-
dimensional input space . The approximating curve shown in Fig. 9.9d resembles a
Peano curve (Kohonen, 1990a). An operation of the kind exemplified by the feature
map of Fig. 9.9, where an input space 
is represented by projecting it onto a lower-
dimensional output space 
, is referred to as dimensionality reduction.
a
x
x
a

9.6
CONTEXTUAL MAPS
There are two fundamentally different ways of visualizing a self-organizing feature map.
In one method of visualization, the feature map is viewed as an elastic net, with the
synaptic-weight vectors treated as pointers for the respective neurons,which are directed
into the input space. This method of visualization is particularly useful for displaying
the topological-ordering property of the SOM algorithm, as illustrated by the results of
the computer simulation experiments presented in Section 9.5.
In the second method of visualization, class labels are assigned to neurons in a
two-dimensional lattice (representing the output layer of the network), depending on
how each test pattern (not seen before) excites a particular neuron in the self-organized
network.As a result of this second stage of stimulation,the neurons in the two-dimensional
lattice are partitioned into a number of coherent regions, coherent in the sense that each
grouping of neurons represents a distinct set of contiguous symbols or labels (Ritter,
2003).This method assumes that the right conditions have been followed for the devel-
opment of a well-ordered feature map in the first place.
Section 9.6
Contextual Maps
447
(a) Input distribution
Time = 0 
(b) Initial weights
0.8
0.6
0.4
0.2
00
0.5
1
1
1
1
1
0.5
0
0
0.5
11
0
1
Time = 50 K
(c) Ordering phase
0.8
0.6
0.4
0.2
00
Time = 100 K
(d) Converging phase
0.8
0.6
0.4
0.2
0.5
1
0
0.5
1
FIGURE 9.9
(a) Distribution of the two-dimensional input data. (b) Initial condition of 
the one-dimensional lattice. (c) Condition of the one-dimensional lattice at the end of the
ordering phase. (d) Condition of the lattice at the end of the convergence phase. The times
included under maps (b), (c), and (d) represent the numbers of iterations.

Consider, for example, the set of data given in Table 9.2, which pertains to a num-
ber of 16 different animals. Each column of the table is a schematic description of an
animal, based on the presence ( 1) or absence ( 0) of some of the 13 different
attributes given on the left-hand side of the table. Some attributes, such as “feathers”
and “two legs,” are correlated, while many of the other attributes are uncorrelated. For
each animal given at the top of the table, we have an attribute code xa made up of 13
elements.The animal is itself specified by a symbol code xs, the composition of which
must not convey any information or known similarities between the animals. For the
example at hand, xs consists of a column vector whose kth element, representing ani-
mal k  1, 2, …, 16, is given a fixed value of a; the remaining elements are all set equal
to zero. The parameter a determines the relative influence of the symbol code com-
pared with that of the attribute code.To make sure that the attribute code is the dom-
inant one, a is chosen equal to 0.2. The input vector x for each animal is a vector of
29 elements, representing a concatenation of the attribute code xa and the symbol
code xs, as shown by
Finally, each data vector is normalized to unit length. The patterns of the data set
thus generated are presented to a two-dimensional lattice of 10  10 neurons, and the
synaptic weights of the neurons are adjusted in accordance with the SOM algorithm
summarized in Section 9.3. The training is continued for 2,000 iterations, after which
the feature map should have reached a steady state. Next, a test pattern defined by
x = c xs
xa
d = cxs
0 d + c 0
xa
d
448
Chapter 9
Self-Organizing Maps
TABLE 9.2
Animal Names and Their Attributes
Animal
is
small
1
1
1
1
1
1
0
0
0
0
1
0
0
0
0
0
medium
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
0
big
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
has
2 legs
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
4 legs
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
hair
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
hooves
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
mane
0
0
0
0
0
0
0
0
0
1
0
0
1
1
1
0
feathers
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
likes
to
hunt
0
0
0
0
1
1
1
1
0
1
1
1
1
0
0
0
run
0
0
0
0
0
0
0
0
1
1
0
1
1
1
1
0
fly
1
0
0
1
1
1
1
0
0
0
0
0
0
0
0
0
swim
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
Dove
Hen
Duck
Goose
Owl
Hawk
Eagle
Fox
Dog
Wolf
Cat
Tiger
Lion
Horse
Zebra
Cow
s
v
d

containing the symbol code of only one of the animals is presented to the
self-organized network,and the neuron with the strongest response is identified.This pro-
cedure is repeated for all 16 animals.
Proceeding in the manner just described, we obtain the map shown in Fig. 9.10,
where the labeled neurons represent those with the strongest responses to their respec-
tive test patterns; the unoccupied rectangular spaces in the figure represent neurons
with weaker responses.
Figure 9.11 shows the result of simulated electrode penetration mapping for the
same self-organized network.This time, however, each neuron in the network has been
marked by the particular animal for which it produces the best response. Figure 9.11
clearly shows that the feature map has essentially captured the “family relationships”
among the 16 different animals.There are three distinct clusters: the white (unshaded)
area representing “birds,” the grey shaded area representing “peaceful species,” and the
red area shaded representing animals that are “hunters.”
A feature map of the type illustrated in Fig. 9.11 is referred to as a contextual map
or semantic map (Ritter, 2003). Such a map resembles cortical maps (i.e., the computa-
tional maps formed in the cerebral cortex), which were discussed briefly in Section 9.2.
Contextual maps, which result from the use of the SOM algorithm, find applications in
such diverse fields as unsupervised categorization of phonemic classes from text, remote
sensing (Kohonen, 1997a), and data exploration or data mining (Kohonen, 1997b).
x = [xs, 0]T
Section 9.6
Contextual Maps
449
1
1
0
0
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
dog
wolf
tiger
zebra
horse
cow
fox
cat
goose
dove
lion
owl
hawk
eagle
hen
duck
FIGURE 9.10
Feature map containing labeled neurons with strongest responses to their
respective inputs.

9.7
HIERARCHICAL VECTOR QUANTIZATION
In discussing property 1 of the self-organizing feature map discussed in Section 9.4, we
pointed out that this map is closely related to the generalized Lloyd algorithm for
vector quantization. Vector quantization is a form of lossy data compression—lossy
in the sense that some information contained in the input data is lost as a result of 
the compression. Data compression is rooted in a branch of Shannon’s information
theory known as rate distortion theory (Cover and Thomas, 2002). For the purpose of
our present discussion dealing with hierarchical vector quantization, it is appropriate
to begin by stating the following fundamental result of rate distortion theory (Gray,
1984):
Better data compression performance can always be achieved by coding vectors instead of
scalars, even if the source of data is memoryless (e.g., it provides a sequence of independent
random variables) or if the data compression system has memory (i.e.,the action of an encoder
depends on past encoder inputs or outputs).
This fundamental result underlies the extensive research effort that has been devoted
to vector quantization over the course of decades.
However, conventional vector quantization algorithms require a prohibitive
amount of computation. The most time-consuming part of vector quantization is the
encoding operation. For encoding, the input vector must be compared with each code
450
Chapter 9
Self-Organizing Maps
1
1
0
0
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
dog
wolf
wolf
wolf
wolf
tiger
tiger
zebra
zebra
zebra
zebra
zebra
horse
horse
horse
horse
horse
horse
horse
horse
cow
cow
cow
cow
cow
cow
fox
fox
fox
cat
cat
cat
dog
fox
fox
fox
cat
cat
cat
fox
fox
cat
cat
cat
cat
cat
goose
goose
goose
goose
goose
goose
goose
dove
dove
dove
dove
dove
dove
dove
dove
lion
lion
lion
lion
lion
lion
tiger
tiger
lion
lion
lion
lion
lion
tiger
tiger
lion
lion
lion
lion
owl
owl
owl
hawk
owl
owl
hawk
hawk
eagle
eagle
eagle
eagle
eagle
eagle
hen
duck
duck
duck
hen
hen
FIGURE 9.11
Semantic map obtained through the use of simulated electrode penetration
mapping.The map is divided into three regions, representing birds (white), peaceful species
(grey), and hunters (red).

vector in the code book in order to determine which particular code yields the mini-
mum distortion. For a code book containing N code vectors, for example, the time taken
for encoding is on the order of N, which can therefore be large for large N. In Luttrell
(1989a), a multistage hierarchical vector quantizer is described that trades off accuracy
for speed of encoding.The multistage hierarchical vector quantizer attempts to factor-
ize the overall vector quantization into a number of suboperations,each of which requires
very little computation. Desirably, the factorization is reduced to a single table lookup
per suboperation. By clever use of the SOM algorithm to train each stage of the quan-
tizer, the loss in accuracy can be small (as low as a fraction of a decibel), while the gain
in speed of computation can be large.
Consider two vector quantizers VQ1 and VQ2, with VQ1 feeding its output into
VQ2. The output from VQ2 is the final encoded version of the original input signal
applied to VQ1. In performing its quantization, it is inevitable for VQ2 to discard some
information.As far as VQ1 is concerned, the sole effect of VQ2 is therefore to distort the
information output by VQ1. It thus appears that the appropriate training method for
VQ1 is the SOM algorithm, which accounts for the signal distortion induced by VQ2
(Luttrell, 1989a). In order to use the generalized Lloyd algorithm to train VQ2, we need
only assume that the output of VQ2 is not corrupted before we do the reconstruction.
Then we do not need to introduce any noise model (at the output of VQ2) with its asso-
ciated finite-width neighborhood function.
We can generalize this heuristic argument to a multistage vector quantizer. Each
stage must be designed to account for the distortion induced by all subsequent stages and
to model this distortion as noise. To do so, the SOM algorithm is used to train all the
stages of the quantizer, except for the last stage, for which the generalized Lloyd algo-
rithm is adequate.
Hierarchical vector quantization is a special case of multistage vector quantiza-
tion.As an illustration, consider the quantization of 4-by-1 input vector
In Fig. 9.12a, we show a single-stage vector quantizer for x.Alternatively, we may use a
two-stage hierarchical vector quantizer, as depicted in Fig. 9.12b.The significant differ-
ence between these two schemes is that the input dimension of the quantizer in Fig. 9.12a
is four, whereas for the quantizer in Fig. 9.12b it is two. Accordingly, the quantizer of
Fig. 9.12b requires a lookup table of smaller size and is therefore simpler to implement
than that of Fig. 9.12a.This is the advantage of a hierarchical quantizer over a conven-
tional quantizer.
Case Study. First-Order Autoregressive Model
Luttrell (1989a) has demonstrated the performance of a multistage hierarchical vector
quantizer applied to various stochastic time series, with little loss in encoding accuracy.
In Fig. 9.13, we have reproduced Luttrell’s results for the case of a correlated Gaussian
noise process generated using a first-order autoregressive (AR) model, given as
(9.30)
x(n + 1) = &x(n) + (n)
x = [x1, x2, x3, x4]T
Section 9.7
Hierarchical Vector Quantization
451

452
Chapter 9
Self-Organizing Maps
Two-input
vector
quantizer
Output
Two-input
vector
quantizer
x3
Two-input
vector
quantizer
x4
x1
x2
Output
x1
Single-stage
vector
quantizer
x2 x3 x4
(b)
(a)
FIGURE 9.12
(a) Single-stage
vector quantizer with four-
dimensional input. (b) Two-stage
hierarchical vector quantizer using
two-input vector quantizers. (From
S.P. Luttrell, 1989a, British Crown
copyright.)
Stage 2
Stage 1
(b)
(d)
Reconstruction
Original
(c)
Stage 1
Stage 2
(a)
FIGURE 9.13
Two-stage encoding–decoding results, using the binary tree shown in red in Fig. 9.12,
for the compression of correlated Gaussian noise input. Correlation coefficient & = 0.85. (From
S.P. Luttrell, 1989a, British Crown copyright.)

where & is the AR coefficient and v(n) is drawn from a set of statistically independent
and identically distributed (iid) Gaussian random variables of zero mean and unit vari-
ance. Hence, we may show that x(n) is statistically characterized as follows:
(9.31)
(9.32)
(9.33)
Thus, & may also be viewed as the correlation coefficient of the time series {x(n)}.To ini-
tiate the generation of the time series according to Eq. (9.30), a Gaussian random vari-
able of zero mean and variance 1/(1 - &2) was used for x(0), and the value &  0.85 was
used for the correlation coefficient.
For the vector quantization a hierarchical encoder with a four-dimensional input
space, like the binary tree, shown in red in Fig. 9.12b, was used. For the AR time series
{x(n)},translational symmetry implies that only two distinct lookup tables are needed.The
size of each table depends exponentially on the number of input bits and linearly on the
number of output bits. During training, a large number of bits is needed to represent
numbers for a correct computation of the updates described in Eq. (9.24); as a result, the
lookup tables are not used during training.Once training is complete,however,the num-
ber of bits may be reduced to their normal level and the table entries filled in as required.
For the encoder shown in Fig. 9.12b, the input samples were approximated by using four
bits per sample.For all stages of the encoder,N ( 17) code vectors were used,so the num-
ber of output bits from each lookup table was approximately four,too.Thus,the address-
space size of both the first-stage and second-stage lookup tables is 256 ( 24  4), which
means that the overall memory requirements for representing the tables are modest.
Figure 9.13 shows the encoding–decoding results obtained with x(n) as the input.
The lower half of Fig 9.13a shows the code vectors for each of the two stages as a curve
embedded in a two-dimensional input space; the upper half of Fig. 9.13a presents esti-
mates of the corresponding co-occurrence matrices using 16  16 bins. Figure 9.13b pre-
sents, as fragments of the time series, the following:
• the code vector computed by the first encoder stage;
• the reconstruction vector computed by the second stage that minimizes the mean-
square distortion while keeping all other variables fixed.
Figure 9.13c presents 512 samples of both the original time series (top curve) and its
reconstruction (bottom curve) from the output of the last encoder stage; the horizontal
scale in Fig. 9.13c is half that in Fig. 9.13b. Finally, Fig. 9.13d presents a co-occurrence
matrix created from a pair of samples: an original time-series sample and its corre-
sponding reconstruction.The width of the band in Fig. 9.13d indicates the extent of the
distortion produced by the hierarchical vector quantization.
Examining the waveforms in Fig. 9.13c, we see that the reconstruction is a good rep-
resentation of the original time series, except for some positive and negative peaks that
[x(n + 1)x(n)]
[x2(n)]
= &
[x2(n)] =
1
1 - &2
[x(n)] = 0
Section 9.7
Hierarchical Vector Quantization
453

were clipped. According to Luttrell (1989a), the normalized mean-square distortion was
computed to be almost as good as the corresponding result obtained with a single-stage four-
sample block encoder using one bit per sample that was reported in Jayant and Noll (1984).
9.8
KERNEL SELF-ORGANIZING MAP
Kohonen’s self-organizing map algorithm is a powerful tool for exploring large amounts
of high-dimensional data, exemplified by many large-scale visualization and data-
mining applications. However, from a theoretical perspective, the self-organizing map
algorithm has two fundamental limitations:
1. The estimate of the probability density function of the input space provided by
the algorithm lacks accuracy. Indeed, this shortcoming of the algorithm shows
itself up in the experimental results presented in Fig. 9.8.This shortcoming is also
found theoretically, be that in Eq. (9.28) or Eq. (9.29), in either one of which the
density-matching property of the algorithm is imperfect.
2. The formulation of the algorithm has no objective function that could be opti-
mized. Considering the nonlinear stochastic characterization of the algorithm, the
lack of an objective function makes the problem of developing a proof of conver-
gence that much more difficult.
Indeed, it is largely these two limitations of the self-organizing map, particularly the lat-
ter one, that have prompted many investigators to devise different ways of approaching
the formulation of feature-mapping models. In this section, we describe a kernel-based
formulation of the self-organizing map developed by Van Hulle (2002b), the motiva-
tion of which is improved topographic mapping.
Objective Function
In applications of the kernel method we have discussed previously, exemplified by the
support vector machine (SVM) and kernel principal-components analysis, the kernel
parameters are usually fixed. In contrast, in a kernel self-organizing map (SOM), each
neuron in the lattice structure of the map acts as a kernel.As such, the kernel parame-
ters are adjusted individually in accordance with a prescribed objective function,
which is maximized iteratively so as to facilitate the formation of a satisfactory topo-
graphic map.
In this section, we focus attention on the joint entropy of the kernel (i.e., neural)
outputs as the objective function. The notion of entropy is discussed in detail in
Chapter 10. For the present, it suffices to start with the definition of this new concept.
Consider a continuous random variable Yi, whose probability density function is
denoted by 
, where the sample value yi lies in the range 
. The
differential entropy of Yi is defined by
(9.34)
H(Yi) = -
3
q
-qpYi(yi) log pyi(yi)dyi
0  yi 6 q
pYi(yi)
454
Chapter 9
Self-Organizing Maps

where we have used log to denote the logarithm to be consistent with the terminology
of Chapter 10. For the kernel SOM, the random variable Yi refers to the output of the
ith kernel in the lattice, and yi refers to a sample value of Yi.
In what follows, we will proceed in a bottom-up manner:
• The differential entropy of a given kernel is first maximized.
• Then, when this maximization has been attained, the kernel parameters are
adjusted so as to maximize the “mutual information” between the kernel’s output
and input.We will have more to say on this second new concept later on.
Definition of the Kernel
Let the kernel be denoted by k(x, wi, i), where x is the input vector of dimensionality
m, wi is the weight (parameter) vector of the ith kernel, and i is its width; the index 
i  1, 2, ..., l, where l is the total number of neurons constituting the lattice structure of
the map. The rationale for assigning the index i to the kernel width, in addition to the
weight vector, is that both parameters will be iteratively adjusted.With the kernel being
radially symmetric around its center, defined by wi, we have
(9.35)
where 
is the Euclidean distance between the input vector x and the weight
vector wi, both of which naturally have the same dimension.
Now, just as in the case of SVM and kernel PCA,we look to a probability distribution—
namely,a Gaussian one—for the kernel definition.We will also look for a probability dis-
tribution but adopt a different definition for the kernel, as explained next.
Suppose the kernel output yi has “bounded” support.Then the differential entropy
H(Yi), defined in Eq. (9.34), will be maximized when Yi is uniformly distributed. (The
justification for this statement is that entropy is a measure of randomness, and a uniform
distribution is the extreme form of randomness.) The optimality condition just stated
occurs when the output distribution matches the cumulative distribution function of the
input space. For a Gaussian-distributed input vector x, we find that the cumulative dis-
tribution function of the corresponding Euclidean distance x - wi is the incomplete
gamma distribution. This distribution, to be defined, is the desired kernel definition.
Let the m elements of the input vector x be statistically independent and identi-
cally distributed (iid), with the jth element being Gaussian distributed with mean j and
variance 2. Let v denote the squared Euclidean distance between the input vector x
and the mean vector 
as shown by
(9.36)
The random variable V, represented by the sample value v, has a chi-square distribution,
as shown by (Abramowitz and Stegun, 1965)
(9.37)
pV() =
1
m2m2(m2) (m2)-1exp a- 
22 b ,    0
= a
m
j=1
(xj - j)2
 = 7x - 7 2
 = [1, 2, ..., m]T,
7 x - wi7
k(x, wi, i) = k(7x - wi7, i),  i = 1, 2, ..., l
Section 9.8
Kernel Self-Organizing Map
455

where m is the number of degrees of freedom of the distribution and (.) is the gamma
function, defined by
(9.38)
Let r denote the radial distance to the center of the kernel, defined by
(9.39)
which represents the sample value of a new random variable R.Then, using the rule for
the transformation of the random V into the random variable R, we write
(9.40)
Using this transformation, we find after some appropriate algebraic manipulations that
the probability density function of the random variable R, represented by the sample
value r, is given by the following (see Problem 9.8):
(9.41)
The continuous curves (printed in black) in Fig.9.14 are plots of the probability den-
sity function pR(r) versus the distance r for unit variance and increasing m  1,2,3,....From
these plots we see that as the input-space dimensionality m increases, pR(r) approaches a
Gaussian function rather rapidly. To be more specific, the second-order statistical para-
meters of the approximating Gaussian function are defined by (Van Hulle, 2002b)
for large m
(9.42)
Determination of the cumulative distribution function of the random variable R is
addressed in part (a) of Problem 9.9, the solution for which is defined by the incomplete
gamma distribution (Abramowitz and Stegun, 1965):
(9.43)
PR(r∑m) = 1 -
≠a m
2 , r2
22 b
≠a m
2 b
(R) L 2m
Var[R] L 2
2
∂
pR(r) = •
1
2(m2)-1(m2)
a r
b
m-1
 exp a- r2
22b ,  r  0
0,
  r 6 0
pR(r) =
pV()
` 0r
0 `
r = 12 = 7 x - 7
() =
3
q
0
z-1exp(-z)dz
456
Chapter 9
Self-Organizing Maps
Complement of incomplete 
gamma distribution
¯˚˚˚˘˚˚˚˙

The factor  
is the complement of the incomplete gamma dis-
 a m
2 , r2
22 b  a m
2 b
Section 9.8
Kernel Self-Organizing Map
457
FIGURE 9.14
Two different sets of plots versus the distance r are shown in
the figure for unit variance and increasing dimensionality m = 1, 2, 3, ...:
•
The continuous curves (printed in black) are plots of the probability
density function of Eq. (9.41).
•
The dashed curves (printed in red) are plots of the complement of the
incomplete gamma distribution or, equivalently, kernel k(r) of Eq. (9.44)
with 
.
(This figure is reproduced with the permission of Dr. Marc Van Hulle.)
r = 7x - w 7
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
distance r
pR(r), k(r)
tribution, plots of which versus the distance r are also included as the dashed curves
(printed in red) in Fig. 9.14 for unit variance and increasing m.These curves also provide
graphical plots of the desired kernel.Specifically,viewing r2 as the squared Euclidean dis-
tance between the input vector x and the weight vector wi for the ith neuron, the cor-
responding kernel k(x, wi,
) is finally defined as follows (Van Hulle, 2002b):
(9.44)
k(x, wi, i) =
1
 a m
2 b
 a m
2 , 7x - wi7 2
22
i
b ,  i = 1, 2, ..., l
i

Note that the kernel, centered on 
, is radially symmetric for all i. Most
importantly,the adoption of the incomplete gamma distribution ensures that the kernel’s
differential is maximized when the input distribution is Gaussian.
Learning Algorithm for Map Formation
With the kernel function of Eq. (9.44) at hand, we are now ready to formulate the algo-
rithm for self-organized topographic formation, using a kernel function to describe each
neuron in the map.
We begin by deriving formulas for the gradients of the objective function defined
in Eq. (9.34) with respect to the kernel parameters: the weight vector wi and kernel
width i for i = 1, 2, ..., l. As it stands now, however, the objective function 
is
defined in terms of the ith neural output
(9.45)
On the other hand, the distribution of Eq. (9.41) is defined in terms of the radial distance
r to the center of the kernel.We therefore need to make a change of random variables
from R to Yi, and correspondingly we write
(9.46)
where the denominator on the right-hand side accounts for the dependence of yi on r.
Hence, substituting Eq. (9.46) into Eq. (9.34), we may redefine the objective function
H(Yi) as
(9.47)
To proceed further, consider first the gradient of H(Yi) with respect to the weight
vector wi. The first term on the right-hand side of this equation is independent of wi.
The second term is the expectation of the partial derivative 
. We may
therefore express the derivative of H(Yi) with respect to wi as
(9.48)
Suppose now that for each kernel we start with a training sample of r’s to approximate
the probability density function pR(r) so as to maximize the differential entropy of the
kernel output yi(r). We may then replace the expectation on the right-hand side of
Eq. (9.48) with a deterministic quantity, as shown by
(9.49)
clog` 0yi(r)
0r
` d = log` 0yi(r)
0r
`
0H(Yi)
0wi
=
0
0wi
clog`
0yi(r)
0r
` d
log(0yi(r))dr
H(Yi) = -
3
q
0
pR(r)logpR(r)dr +
3
q
0
pR(r)log`
0yi(r)
0r
`dr
pYi(yi) = pR(r)
` dyi
dr `
yi = k(x, wi, i),  i = 1, 2, ..., l
H(Yi)
r = 7x - wi7
458
Chapter 9
Self-Organizing Maps

where 
is the value of yi(r) averaged over the training sample of r’s.Accordingly, we
may rewrite Eq. (9.48) in the simplified form
(9.50)
The avereged 
has the form of complement of incomplete gamma distribution
defined in Eq. (9.43), the use of which yields (see part (b) of Problem 9.9)
(9.51)
Recall that the kernel is symmetrically centered on the point
Thus, performing the partial differentiation 
of Eq. (9.51) with respect to wi
and substituting the result into Eq. (9.50), we obtain (after simplifying)
(9.52)
The following two remarks about Eq. (9.52) are noteworthy:
(i) Both terms on the right-hand side of the equation converge to the centroid of the
input vector x for a large number of iterations.
(ii) For a Gaussian-distributed input vector x of dimensionality m, we know from pre-
vious discussion that the expectation
(9.53)
The second term on the right-hand side of the equation is therefore expected to
be smaller than the first term for all m.
From a computational point of view, it would be highly desirable to simplify
Eq. (9.52) so that we end up with a single learning-rate parameter for the update rule
pertaining to the weight vector wi.11 To this end, we opt for a heuristic proposition:
Replace the squared Euclidean term 
with the expected value of Eq. (9.53),
thereby approximating Eq. (9.52) as follows:
(9.54)
With the objective function being maximized, the weight update is naturally applied in
the same direction as the gradient vector of Eq. (9.54), in accordance with gradient
ascent.We may thus write
¢wi = w a
0H(yi)
0wi
b
0H(yi)
0wi
L x - wi
m2
i      for all i
x - wi2
[x - wi2] = mi
2
0H(yi)
0wi
= x - wi
2
i
- (m - 1) a x - wi
x - wi2 b
0yi(r)0r
r = x - wi
0yi(r)
0r
=
- 22
(m2)(22i)m rm-1exp a- r2
22 b
yi(r)
= 0r
0wi
0
0r a log`
0yi(r)
0r
` b
0H(yi)
0wi
=
0
0wi
a log`
0yi(r)
0r
` b
yi(r)
Section 9.8
Kernel Self-Organizing Map
459

460
Chapter 9
Self-Organizing Maps
where w is a small learning-rate parameter.Absorbing the fixed dimensionality m of the
input vector x in w, we may finally express the weight adjustment as
(9.55)
The first update formula of the kernel SOM algorithm is therefore
(9.56)
where wi and w
i denote the old and updated values of the weight vector of neuron i,
respectively.
Consider next the gradient vector of the objective function 
with respect to
H(yi)
= wi + w a x - wi
i
2
b
wi
+ = wi + ¢wi
¢wi L w a x - wi
i
2
b
the kernel width i. Proceeding in a manner similar to that described for the gradient
vector 
, we obtain
(9.57)
We may thus define the kernel-width adjustment as
(9.58)
where  is the second learning-rate parameter. For the second update formula of the
kernel SOM algorithm, we therefore have
(9.59)
The two update rules given by Eqs. (9.56) and (9.59) work well for a single neuron.We
next consider their extensions for a network of multiple neurons.
Joint Maximization of the Objective Function
The maximization of the objective function 
on a neuron-by-neuron basis is not suf-
ficient for a workable algorithm.To see why this is so,consider a lattice made up of two neu-
rons whose respective kernel outputs are denoted by y1 and y2.When the update equations
of Eqs.(9.56) and (9.59) are used,assuming a Gaussian input distribution,for example,the
two neural kernels will eventually coincide; in other words, the two kernel outputs y1 and
y2 become statistically dependent.To guard against this unsatisfactory eventuality (i.e., to
maintain the statistical independence of y1 and y2 as much as possible),we should maximize
the objective function 
by putting the mechanism of kernel adaptation in a competitive-
learning framework, which is exactly what we did in deriving Kohonen’s SOM algorithm.
H(yi)
H(yi)
= i + 
i
a x - wi2
m2
i
- 1 b
i
+ = i + ¢i
= 
i
a x - wi2
m2
i
- 1 b
¢i = 
0H(yi)
0i
0H(yi)
0i
= 1
i
a x - wi2
m2
i
- 1 b
0H(yi)(0wi)

Then the kernel of the neuron winning the competition will decrease its range of interac-
tion with neighboring neurons, particularly when the winning neuron is strongly active;
thus,the overlap with neighboring neurons is diminished.Moreover,as we did with Kohonen’s
SOM algorithm, we impose a neighborhood function on the learning process in order to
topologically preserve the neural lattice with respect to the data distribution of the input
space. Accordingly,the combined use of competitive learning and a neighborhood function
enables us to use the two update rules with multiple neurons, as discussed next.
Topographic Map Formation
Consider a lattice a consisting of l neurons characterized by a corresponding set of
(complement of incomplete gamma distribution) kernels
(9.60)
With topographic map formation as the objective, we introduce an activity-based
competition among the l neurons in the lattice a, with the winning neuron being
defined by
(9.61)
Note that this similarity-matching criterion is different from that of Eq. (9.3), which
is based on a minimum distance-based neuronal competition. The two criteria of
Eqs. (9.3) and (9.61) are equivalent only when all the neural kernels have equal
widths (radii).
To supply the information needed for topological map formation, as in Kohonen’s
SOM, we introduce a neighborhood function hj, i(x), centered on the winning neuron i(x).
Moreover, following the discussion in Section 9.3, we adopt a monotonically decreasing
function of the lattice distance from the winning neuron i(x). In particular, we opt for
the Gaussian function of Eq. (9.4), reproduced here in the form
(9.62)
where  denotes the range of the neighborhood function 
; the neighborhood range
 should not be confused with the kernel width i.
Summary of the Kernel SOM Algorithm
We are now ready to describe the steps involved in the kernel self-organizing map:
1. Initialization. Choose random values for the initial weight vectors wi(0) and kernel
widths i(0) for i = 1, 2, ..., l, where l is the total number of neurons in the lattice
structure.The only restriction here is that the wi(0) and i(0) be different for the
different neurons.
2. Sampling. Draw a sample x from the input distribution with a certain probability.
3. Similarity matching. At time-step n of the algorithm, identify the winning neuron
i(x), using the criterion:
i(x) = arg max
i
yj(x),  j = 1, 2, ..., l
hj,i(x)
hj, i(x) = exp a-
xj - wi2
22
b , 
j  a 
i(x) = arg max 
i
yj(x),  for j  a 
k(x,wi,i), 
i = 1, 2, ..., l
Section 9.8
Kernel Self-Organizing Map
461

4. Adaptation. Adjust the weight vector and width of each kernel, using the respec-
tive update formulas
(9.63)
(9.64)
where 
and  are the two learning-rate parameters of the algorithm and hj, i(x) is the
neighborhood function centered on the winning neuron i(x), defined in accordance with
Eq. (9.61). As in Kohonen’s SOM, the neighborood range  is permitted to decay expo-
nentially over time.
9.9
COMPUTER EXPERIMENT II: DISENTANGLING LATTICE DYNAMICS
USING KERNEL SOM
In this experiment, we revisit a two-dimensional lattice, which was studied in part I of
the computer experiment of Section 9.5, except that this time we use the kernel SOM
in the experiment.The two learning-rate parameters of the algorithm were chosen to be
and
The two-dimensional lattice is a square lattice made up of 24-by-24 neurons, and the
input data are uniformly distributed.The weights were initialized by sampling from the
same input distribution, and the radii were initialized by sampling the uniform distrib-
ution [0, 0. 1].The neighborhood function used a Gaussian function with the width
(9.64)
where nmax denotes the maximum number of time-steps and 0 denotes the range spanned
by the neighborhood function at time n  0.The values used in the experiment are
and
These choices were made to ensure that the neighborhood function will vanish at the end
of the learning process, at which point it assumes the approximate value 4.5  10-10,
0 = 12
nmax = 2 * 106
(n) = 0exp a-20 a n
nmax
b b 
 = 10-4w
w = 0.01
w
j(n + 1) = µ
j(n) +
hj, i(x)
j(n) £
x(n) - wj(n)2
mj
2(n)
- 1§ ,
    j  a
j(n),
otherwise
wj(n + 1) = µ
wj(n) +
whj, i(x)
j
2
 (x(n) - wj(n)),
    j  a
wj(n),
   otherwise
462
Chapter 9
Self-Organizing Maps

which is practically zero.When this condition is finally reached, the neighborhood func-
tion encompasses the winning neuron alone.
Figure 9.15 presents two sequences illustrating the disentanglement of the topo-
graphic maps produced by the kernel SOM algorithm. Note that
• the pictures displayed in the left-hand column of the figure show the evolution of
the kernel weights over time n;
• the pictures displayed in the right-half column of the figure show the correspond-
ing evolution of the kernel widths over time n.
Section 9.9
Computer Experiment II: Disentangling Lattice Dynamics
463
Time = 0
Time = 0
Time = 1k
Time = 1k
Time = 10k
Time = 10k
Time = 2M
Time = 2M
FIGURE 9.15
The evolution of a 24-by-24 lattice
over time, the values of which (in terms of the
number of iterations) are given below each picture.
Left column: Evolution of the kernel weights. Right
column: Evolution of the kernel widths. Each box in
the figure outlines the result of a uniform input
distribution.The time given below each map
represents the number of iterations. (This figure is
reproduced with the permission of Dr. Marc Van
Hulle.)

Comparing the final form of the topographic map on the left-hand column of Fig. 9.15
with that of Fig. 9.8, computed by the kernel SOM and the conventional SOM on 24-by-
24 lattices and for roughly the same number of iterations, respectively, we may make
the following significant observation:
The distribution of the topographic map computed by the kernel SOM is much closer to the
uniform distribution assigned to the input data space than that of the topographic map com-
puted by the conventional SOM.
Accordingly, we may go on to say that the magnification factor m(x) computed by the
kernel SOM matches the input density pX(x) better than the conventional SOM does;
that is, the kernel SOM may come closer to the ideal condition of Eq. (9.27).
9.10
RELATIONSHIP BETWEEN KERNEL SOM 
AND KULLBACK–LEIBLER DIVERGENCE
We find it informative to discuss the relationship between the kernel SOM (using incom-
plete gamma distribution kernels) and Kullback–Leibler divergence (KLD). KLD, to be
discussed in detail in the next chapter, provides a formula for assessing the quality of a
density estimate measured against the true density. Let the true density be denoted by
pX(x) and its estimate be denoted by 
. Then we define KLD between these two
densities as
(9.65)
where we have followed the commonly used terminology in the literature on informa-
tion theory.So defined,KLD is always a nonnegative number that assumes the value zero
if, and only if,
matches pX(x) exactly.
For the present discussion, suppose the density estimate is expressed as a mixture
of Gaussian density functions with equal mixings, as shown by
(9.66)
which is conditional on the weight vector wi and the width i, for i = 1, 2, …, l.The opti-
mal density estimate, pX(x), is obtained by minimizing the KLD between it and the den-
sity estimate 
,In effect,the optimal density estimate pX(x) is viewed as the true
density. With optimality as the issue of interest, we need to differentiate the KLD of
Eq. (9.66) with respect to the adjustable parameters, wi and i.To this end, we obtain the
following pair of partial derivatives with respect to wi:
=
3
q
- q
0
0wi
 (pX(x)log pX(x) - pX(x)log pˆ X(xwi, i))dx
0
0wi
 (DpX pˆX) =
0
0wi 3
q
- q pX(x)log a
pX(x)
pˆ X(xwi, i) bdx
pˆ X(xwi, i)
pˆ X(xwi,i) = 1
l a
l
i=1
1
(2)m2m
i
 exp a- 1
22
i
x - wi2 b 
pˆ X(x)
DpXpˆX =
3
q
-q
pX(x)log a
pX(x)
pˆ X(x) b dx
pˆ X(x)
464
Chapter 9
Self-Organizing Maps

(9.67)
Similarly, we may express the partial derivative with respect to i as
(9.68)
Setting these two partial derivatives of the KLD equal to zero and then invoking sto-
chastic approximation theory (Robbins and Monro,1951),we obtain the pair of learning
rules (Van Hulle, 2002b)
(9.69)
and
(9.70)
for i = 1,2,...,l;the term 
is the conditional posterior density of the ith neuron
pˆ X(xwi, i)
¢i = wpˆ X(xwi, i)  m
i
a x - wi2
m2
i
- 1 b
¢wi = wpˆ X(xwi, i) a x - wi
2
i
b ,
0
0i
 (DpXpˆ X) =
-
3
q
- q
pX(x) a
1
pˆX(xwi, i)
0
0i
pˆ X(xwi, i) b dx
= -
3
q
- q
pX(x) a
1
pˆ X(xwi, i)
0
0wi
pˆ X(xwi, i) bdx
= -
3
q
- qpX(x)
0
0wi
 (log pˆ X(xwi, i))dx
Section 9.10
Relationship Between Kernel SOM and Kullback–leibler
465
characterized by the weight vector wi and width i.
Suppose we set the conditional posterior density
(9.71)
where
When this ideal condition is satisfied, we say that neuron i is the winning neuron in the
competition among the neurons j = 1, 2, ..., l. We may therefore view the conditional
posterior density function 
as playing the role of the topological neighborhood
function hj,i(x) introduced in the formulation of the kernel SOM. Indeed, setting
(9.72)
we find that the pair of update rules derived from the Kullback–Leibler divergence,
namely, Eqs. (9.69) and (9.70), has a mathematical form similar to the corresponding
pair of update rules, Eqs. (9.63) and (9.64), derived in Section 9.9 for the kernel SOM.
We may therefore make the following statement (Van Hulle, 2002b):
Minimization of the Kullback–Leibler divergence, assuming a Gaussian mixture model, is
equivalent to maximization of the joint entropy defined in terms of incomplete gamma distrib-
ution kernels and an activity-based neighborhood function,which are at the core of kernel SOM.
This statement is particularly important in the context of density estimation, where we
are given a data set 
and the requirement is to compute an estimate of the under-
lying distribution intrinsic to generation of the data.
{xi}N
i=1
pˆ X(xwj, j) = hj, i(x)
pˆ X(xwj, j)
ji = e1 for j = i
0 for j Z i
pˆ X(xjwi, i) = ji for j = 1, 2, ..., l

9.11
SUMMARY AND DISCUSSION
Self-Organizing Map
The self-organizing map due to Kohonen (1982) is a simple, yet powerful, algorithm that
is typically built around a one- or two-dimensional lattice of neurons for capturing the
important features contained in an input (data) space of interest. In so doing, it pro-
vides a structural representation of the input data by the neurons’ weight vectors as
prototypes. The SOM algorithm is neurobiologically inspired, incorporating the mech-
anisms that are basic to self-organization, as discussed in Chapter 8: competition, coop-
eration,self-amplification,and structural information.It may therefore serve as a generic,
though degenerate, model for describing the emergent behavior of collective-ordering
phenomena in complex systems after starting from total disorder. In other words, the
SOM has a built-in capability of producing “order out of disorder” in an evolutionary
process over the course of time.
The self-organizing map may also be viewed as a vector quantizer, thereby pro-
viding an analytic approach for deriving the update rule used to adjust the weight vec-
tors (Luttrell, 1989b). This latter approach clearly emphasizes the role of the
neighborhood function as a probability density function.
It should, however, be emphasized that this latter approach, based on the use of
average distribution D1 in Eq. (9.19) as the cost function to be minimized, can be justi-
fied only when the feature map is already well ordered. In Erwin et al. (1992b), it is
shown that the learning dynamics of a self-organizing map during the ordering phase of
the adaptive process (i.e.,during the topological ordering of a feature map that is initially
highly disordered) cannot be described by a stochastic gradient descent on a single cost
function. But in the case of a one-dimensional lattice, it may be described using a set of
cost functions, one for each neuron in the network, that are independently minimized
following a stochastic gradient descent.
Convergence Considerations of Self-Organizing Map
What is remarkable about Kohonen’s SOM algorithm is that it is so simple to implement,
yet mathematically so difficult to analyze its properties in a general setting. Some fairly
powerful methods have been used to analyze it by several investigators, but they have
only produced results of limited applicability. In Cottrell et al. (1997), a survey of results
on theoretical aspects of the SOM algorithm is given. In particular, the survey highlights
a result given by Forte and Pagés (1995, 1996) which states that in the case of a one-
dimensional lattice, we have a rigorous proof of the “almost sure” convergence of the
SOM algorithm to a unique state after completion of the self-organization phase.This
important result has been shown to hold for a general class of neighborhood functions.
However, the same cannot be said in a multidimensional setting.
Neurobiological Considerations
With the self-organizing map being inspired by ideas derived from cortical maps in the
brain, it seems natural to enquire whether such a model could actually explain the for-
mation of cortical maps. Erwin et al. (1995) have performed such an investigation.They
have shown that the self-organizing map is able to explain the formation of computational
466
Chapter 9
Self-Organizing Maps

maps in the primary visual cortex of the macaque monkey.The input space used in this
study had five dimensions: two dimensions for representing the position of a receptive
field in retinotopic space, and the remaining three dimensions for respectively repre-
senting orientation preference, orientation selectivity, and ocular dominance. The cor-
tical surface is divided into small patches that are considered as computational units
(i.e., artificial neurons) of a two-dimensional square lattice. Under certain assumptions,
it is shown that Hebbian learning leads to spatial patterns of orientation and ocular
dominance that are remarkably similar to those found in the macaque monkey.
Applications of The Self-Organizing Map
The combination of the simplicity of the SOM algorithm and its powerful capability for
insightful visualization has prompted the algorithm’s use for many large-scale applica-
tions.Typically, the algorithm is trained in an unsupervised mode, using a large train-
ing sample of data. In particular, if the data contain semantically related object groupings
(classes), the subsets of vectors belonging to the user-defined classes are mapped by the
SOM in such a way that the distribution of the data vectors over the map computed by
the algorithm provides a two-dimensional discrete approximation of the underlying dis-
tribution of the original data space. Building on this idea, in Laaksonen et al. (2004) and
Laaksonen and Viitaniemi (2007), the SOM is successfully used to detect and describe
ontological relations between semantic objects and object classes in a visual database con-
taining 2,618 images, each of which belongs to one or more predefined semantic classes.
The ontological relations used in the study include the following:
• simultaneous existence of objects from two or more object classes in one image;
• taxonomy of visual similarity;
• spatial relationships between different object types in one image.
In another application altogether different, Honkela et al. (1995) have used the
SOM algorithm to study the semantic roles of words in natural languages, where the
roles are reflected by the contexts in which they occur. The objective of the study was
to compute a contextual map for the explicit visualization of these roles. In the experi-
ments carried out in the study, the source database consisted of English translations of
fairy tales by the Brothers Grimm, without any prior syntactic or semantic categoriza-
tion of the words; the number of words was almost 250,000 in total, and the size of the
vocabulary was over 7,000 words. The SOM algorithm was able to create a contextual
map that seemed to comply reasonably well with the traditional syntactical categoriza-
tions and human intuition about the semantics of the words.The analysis of textual con-
tent has been extended to collections of millions of documents;in this kind of application,
the number of neurons in the lattice may reach hundreds of thousands, and the dimen-
sionality of the input data space may be as high as thousands (Honkela, 2007). It is large-
scale applications of this kind that make the self-organizing map such a powerful tool.
The Kernel SOM
In the latter part of the chapter,we described Van Hulle’s (2002b) kernel SOM algorithm,
the primary aim of which is to provide improved topographic mappings and approximate
distribution capabilities. A distinctive feature of the kernel SOM is that its derivation
Section 9.11
Summary and Discussion
467

468
Chapter 9
Self-Organizing Maps
begins with the formulation of an entropic objective function. Most importantly, the
kernel SOM is an on-line, stochastic-gradient-based algorithm.
Comparing the two self-organizing maps studied in the chapter, we may say that
the standard SOM and kernel SOM have similar update rules for the weight vectors in
the neural lattice. Also, they make the weight updates in the same direction, but use
different learning-rate parameters. Unlike the standard SOM, the kernel SOM has the
built-in capability to automatically adjust the kernel with i for each neuron i in the lat-
tice, so as to maximize the joint entropy of the kernel (neuron) outputs.
However, the kernel SOM requires careful tuning of the two learning-rate pa-
rameters
and  , in order to keep the weight and width updates from exploding.This
w
explosion happens if, and when, the inverse of the kernel-width variance i
2 becomes
greater than the learning-rate parameter w or . Such undesirable behavior is attrib-
uted to the fact that in the update rules of Eqs. (9.56) and (9.59), the learning-rate para-
meters w and  are divided by i
2 and i, respectively. To avoid the possibility of
explosive growth in wi and i, we may replace i
2 by the sum term i
2  , where  is a
prescribed small constant.
NOTES AND REFERENCES
1. There are other types of competitive learning where there is no winner, such as those dis-
cussed in Heskes (2001) and Van Hulle (2005).
2. The two feature-mapping models of Fig. 9.1 were inspired by the pioneering self-organizing
studies of von der Malsburg (1973), who noted that a model of the visual cortex could not
be entirely genetically predetermined; rather, a self-organizing process involving synaptic
learning may be responsible for the local ordering of feature-sensitive cortical cells. How-
ever, global topographic ordering was not achieved in von der Malsburg’s model, because
the model used a fixed (small) neighborhood. The computer simulation by von der Mals-
burg was perhaps the first to demonstrate self-organization.
3. Amari (1980) relaxes this restriction on the synaptic weights of the postsynaptic neurons
somewhat.The mathematical analysis presented by Amari elucidates the dynamic stability
of a cortical map formed by self-organization.
4. The competitive-learning rule described in Eq. (9.3) was first introduced into the neural
network literature in Grossberg (1969).
5. In the original form of the SOM algorithm derived by Kohonen (1982), the topological
neighborhood is assumed to have a constant amplitude. Let dj, i denote the lateral distance
between winning neuron i and excited neuron j inside the neighborhood function.The topo-
logical neighborhood for the case of a one-dimensional lattice was defined by
(A)
where 2K is the overall size of the one-dimensional neighborhood of excited neurons.Contrary
to neurobiological considerations, the implication of the model described in Eq. (A) is that all
the neurons located inside the topological neighborhood fire at the same rate,and the interac-
tion among those neurons is independent of their lateral distance from the winning neuron i.
6. In Erwin et al. (1992b), it is shown that metastable states, representing topological defects
in the configuration of a feature map, arise when the SOM algorithm uses a neighborhood
hj, i = e1,
-K  dj, i  K
0,  otherwise

function that is not convex.A broad, convex neighborhood function, such as a broad Gauss-
ian, leads to relatively shorter topological ordering times than a nonconvex one, due to the
absence of metastable states.
7. In Note 3 of Chapter 5, it was pointed out that the communications and information the-
ory literature, an early method known as the Lloyd algorithm was proposed for scalar quan-
tization. The algorithm was first described by Lloyd in an unpublished 1957 report at Bell
Laboratories (Lloyd, 1957) and then much later appeared in published form (Lloyd, 1982).
The Lloyd algorithm is also sometimes referred to as the “Max quantizer.”The generalized
Lloyd algorithm (GLA) for vector quantization is a direct generalization of Lloyd’s origi-
nal algorithm. The generalized Lloyd algorithm is sometimes referred to as the k-means
algorithm, after McQueen (1967), who used it as a tool for statistical clustering, as dis-
cussed in Chapter 5. In that previous chapter, we did point out that the k-means algo-
rithm operates in a manner similar to the expectation-maximization (EM) algorithm; the
only basic difference between them is that the objective function of the k-means algorithm,
and likewise that of the GLA, is minimized, whereas the objective function of the EM algo-
rithm is maximized.The EM algorithm is discussed in Chapter 11. For a historical account
of the Lloyd algorithm and generalized Lloyd algorithm, see Gersho and Gray (1992).
8. In Kohonen (1993), experimental results are presented showing that the batch version of
the SOM algorithm is faster than its on-line version. However, the adaptive capability of the
SOM algorithm is lost when using the batch version.
9. The topological property of a self-organizing map may be assessed quantitavely in differ-
ent ways. One such quantitative measure, called the topographic product, is described in
Bauer and Pawelzik (1992).It may be used to compare the faithful behavior of different fea-
ture maps pertaining to different dimensionalities. However, the measure is quantitative
only when the dimension of the lattice matches that of the input space.
10. The inability of the SOM algorithm to provide a faithful representation of the distribution
that underlies the input data has prompted modifications to the algorithm and the devel-
opment of new self-organizing algorithms that are faithful to the input.
Two types of modifications to the SOM algorithm have been reported in the literature:
(i) Modification to the competitive process.
In DeSieno (1988), a form of memory is used
to track the cumulative activities of individual neurons in the lattice. Specifically, a “con-
science” mechanism is added to bias the competitive-learning process of the SOM algo-
rithm.This is done in such a way that each neuron, regardless of its location in the lattice,
has the chance to win competition with a probability close to the ideal of 1/l, where l is
the total number of neurons. A description of the SOM algorithm with conscience is
presented in Problem 9.7.
(ii) Modification to the adaptive process.
In this second approach,the update rule for adjust-
ing the weight vector of each neuron under the neighborhood function is modified to
control the magnification properties of the feature map.In Bauer et al.(1996),it is shown
that through the addition of an adjustable step-size parameter to the update rule, it is
possible for the feature map to provide a faithful representation of the input distribu-
tion. Lin et al. (1997) follow a similar path by introducing two modifications to the SOM
algorithm:
• The update rule is modified to extract direct dependence on the input vector x and
weight vector wj of neuron j in question.
• The Voronoi partition is replaced with an equivariant partition designed specially for
separable input distributions.
Notes and References
469

This second modification enables the SOM algorithm to perform blind source separa-
tion. (Blind source separation is explained in detail in Chapter 10.)
The modifications mentioned build on the standard SOM algorithm in one form or
another. In Linsker (1989b), a completely different approach is taken. Specifically, a global
learning rule for topographic map formation is derived by maximizing the mutual infor-
mation between the output signal and the signal part of the input corrupted by additive
noise. (The notion of mutual information, rooted in Shannon’s information theory, is
discussed in Chapter 10.) Linsker’s model yields a distribution of neurons that matches the
input distribution exactly. The use of an information-theoretic approach to topographic
map formation in a self-organized manner is also pursued in Van Hulle (1996, 1997).
11. In Van Hulle (2002), ignoring the second term on the right-hand side of Eq. (9.52) is based
on the following arguments:
• The expected value of 
obtained for a Gaussian-distributed input vector x is
defined in Eq. (9.53).
• In an m-dimensional radially symmetric Gaussian distribution, the distribution can be
built up by taking m samples, with one sample for each input dimension.Then, in a one-
dimensional Gaussian distribution with the same radius, when the weight updates !
ij
are small (which presumes the use of a small learning-rate parameter w) and when the
update is performed along each input dimension separately (i.e., in random order), the
second term of Eq. (9.52) may be ignored.
PROBLEMS
SOM algorithm
9.1 The function g(yj) denotes a nonlinear function of the response yj, which is used in the SOM
algorithm as described in Eq. (9.9). Discuss the implication of what could happen if the con-
stant term in the Taylor series of g( yj) is nonzero.
9.2 Assume that (v) is a smooth function of the noise v in the model of Fig. 9.6. Using a Tay-
lor expansion of the distortion measure of Eq. (9.19), determine the curvature term that
arises from the noise model (v).
9.3 It is sometimes said that the SOM algorithm preserves the topological relationships that
exist in the input space. Strictly speaking, this property can be guaranteed only for an input
space of equal or lower dimensionality than that of the neural lattice. Discuss the validity of
this latter statement.
9.4 It is said that the SOM algorithm based on competitive learning lacks any tolerance against
hardware failure.However,the algorithm is error-tolerant in that a small perturbation applied
to the input vector causes the output to jump from the winning neuron to a neighboring
one. Discuss the implications of these two statements.
9.5 Consider the batch version of the SOM algorithm obtained by expressing Eq.(9.23) in its dis-
crete form, leading to the formula
Show that this version of the SOM algorithm can be expressed in a form similar to the
Nadaraya–Watson regression estimator (Cherkassky and Mulier, 1995); this estimator was
discussed in Chapter 5.
wj =
a
i
j,ixi
a
i
j,i
,  j = 1, 2, ..., l
w
x - wi2
470
Chapter 9
Self-Organizing Maps

Learning vector quantization
9.6 The update rules for both the maximum eigenfilter discussed in Chapter 8 and the self-
organizing map employ modifications of Hebb’s postulate of learning. Compare these two
modifications, highlighting the differences and similarities between them.
9.7 The conscience algorithm is a modification of the SOM algorithm that forces the density
matching to be exact (DeSieno, 1988). In the conscience algorithm, summarized in Table
P9.7, each neuron keeps track of how many times it has won the competition (i.e., how many
times its synaptic-weight vector has been the neuron closest to the input vector in Euclid-
ean distance). The notion used here is that if a neuron wins too often, it “feels guilty” and
therefore pulls itself out of the competition.
To investigate the improvement produced in density matching by the use of the con-
science algorithm, consider a one-dimensional lattice (i.e., linear array) made up of 20 neu-
rons that is trained with the linear input density plotted in Fig. P9.7.
(a) Using computer simulations, compare the density matching produced by the conscience
algorithm with that produced by the SOM algorithm.For the SOM algorithm use  = 0.05,
and for the conscience algorithm use B = 0.0001, C = 1.0, and  = 0.05.
(b) As frames of reference for this comparison,include the “exact”match to the input density.
Discuss the results of your computer simulations.
Kernel self-organizing map
9.8 Using the transformation formula of Eq. (9.40) applied to Eq. (9.37), derive the probability
density function of Eq. (9.41).
Problems
471
TABLE P9.7
Summary of the Conscience Algorithm
1. Find the synaptic-weight vector wi closest to the input vector x:
2. Keep a running total of the fraction of time, pj, that neuron j wins the competition, given as
where B is a small positive number, and
The pj are initialized to zero at the beginning of the algorithm.
3. Find the new winning neuron, using the conscience mechanism
where bj is a bias term introduced to modify the competition; it is defined as
where C is a bias factor and N is the total number of neurons in the network.
4. Update the synaptic-weight vector of the winning neuron to obtain
where  is the usual learning-rate parameter used in the SOM algorithm.
wnew
i
= wold
i
+ (x - wold
i )
bj = C a 1
N - pj b
7 x - wi7 = min
j
(7x - wj7 - bj)
yj = e 1
if neuron j is the winning neuron
0
otherwise
pnew
j
= pold
j
+ B(yj - pold
j )
7 x - wi7 = min
j
7 x - wj7, j = 1, 2, ..., N

9.9 This problem is in two parts, addressing the issues involved in deriving couple of equations
that pertain to the kernel SOM algorithm:
(a) The incomplete gamma distribution of a random variable X, with sample value x, is
defined by (Abramowitz and Stegun, 1965, p. 260):
where 
is the gamma function.The complement of the incomplete gamma distribution
is correspondingly defined by
Using these two formulas, derive the cumulative distribution function of random vari-
able R that is defined in Eq. (9.43).
(b) Using the formula of the incomplete gamma distribution as the definition of the aver-
aged neural output
derive Eq. (9.51) for the partial derivative       
9.10 In developing the approximate update formula of Eq. (9.55) for the weight vector of the kernel
SOM algorithm,we justified ignoring the second term in Eq.(9.52).Yet,in deriving the update for-
mula of Eq.(9.58) for the kernel width i,no approximation was made.Justify this latter choice.
Computer Experiments
9.11 In this experiment, we use computer simulations to investigate the SOM algorithm applied
to a one-dimensional lattice with a two-dimensional input. The lattice consists of 65 neu-
rons. The inputs consist of random points uniformly distributed inside the triangular area
shown in Fig.P9.11.Compute the map produced by the SOM algorithm after 0,20,100,1,000,
10,000, and 25,000 iterations.
9.12 Consider a two-dimensional lattice of neurons trained with a three-dimensional input dis-
tribution.The lattice consists of 10  10 neurons.
(a) The input is uniformly distributed in a thin volume defined by
{(0 6 x1 6 1), (0 6 x2 6 1), (0 6 x3 6 0.2)}
0y–
i(r)0r.
–yi,
(, x) =
1
() 3
q
x
t-1 exp(-t)dt
()
PX(x∑) =
1
() 3
x
0
t-1 exp(-t)dt
472
Chapter 9
Self-Organizing Maps
x2
x1
2.0
0.0
1.0
1.0
x2
x1
0
FIGURE P9.7
For Problem 9.7.
FIGURE P9.11
For Problem 9.11.

Use the SOM algorithm to compute a two-dimensional projection of the input space
after 50, 1,000, and 10,000 iterations of the algorithm.
(b) Repeat your computations for the case when the input is uniformly distributed inside a
wider parallelepiped volume defined by
(c) Repeat your computations one more time for the case when the input is uniformly dis-
tributed inside a cube defined by
Discuss the implications of the results of your computer simulations.
9.13 A problem that occasionally arises in the application of the SOM algorithm is the failure of
topological ordering by creating a “folded”map.This problem arises when the neighborhood
size is permitted to decay too rapidly.The creation of a folded map may be viewed as some
form of a “local minimum” of the topological-ordering process.
To investigate this phenomenon, consider a two-dimensional lattice of a 10  20 net-
work of neurons trained on a two-dimensional input uniformly distributed inside the square
{(-1  x1   1), (-1  x2   1)}. Compute the map produced by the SOM algorithm, per-
mitting the neighborhood function around the winning neuron to decay much faster than that
normally used. You may have to repeat the experiment several times in order to see a fail-
ure of the ordering process.
9.14 The topological-ordering property of the SOM algorithm may be used to form an abstract
two-dimensional representation of a high-dimensional input space.To investigate this form
of a representation, consider a two-dimensional lattice consisting of a 10  10 network of
neurons that is trained with an input consisting of four Gaussian clouds 
,
, and 
in an eight-dimensional input space.All the clouds have unit variance, but different centers.
The centers are located at the points (0, 0, 0, …, 0), (4, 0, 0, …, 0), (4, 4, 0, …, 0), and (0, 4,
0, ..., 0). Compute the map produced by the SOM algorithm, with each neuron in the map
being labeled with the particular class most frequently represented by the input points
around it.
9.15 Table P9.15 presents a summary of the renormalized SOM algorithm; a brief description of
the algorithm is given in Section 9.3. Compare the conventional and renormalized SOM
algorithms, keeping in mind the following two issues:
1. the coding complexity involved in algorithmic implementation;
2. the computer time taken to do the training.
Illustrate the comparison between these two algorithms using data drawn from a uniform dis-
tribution inside a square and the following two network configurations:
(a) a one-dimensional lattice of 257 neurons;
(b) a one-dimensional lattice of 2,049 neurons.
In both cases, start with an initial number of code vectors equal to two.
9.16 Consider the signal-space diagram shown in Fig. P9.16 corresponding to M-level pulse-
amplitude modulation (PAM) with M  8. The signal points correspond to Gray-encoded
data blocks. Each signal point is represented by a rectangular pulse signal with appropriate
amplitude scaling
p(t) =
; 7
2 ,
; 5
2 ,
; 3
2 ,
; 1
2 ,  0  t  T
c4
c3
c2
c1
{(0 6 x1 6 1), (0 6 x2 6 1), (0 6 x3 6 1)}
{(0 6 x1 6 1), (0 6 x2 6 1), (0 6 x3 6 0.4)}
Problems
473

where T is the signaling interval. At the receiver input, white Gaussian noise of zero mean
is added to the transmitted signal with varying signal-to-noise ratio (SNR). The SNR is
defined as the ratio of the “average” transmitted signal power to the average noise power.
(a) Using a random binary sequence as the transmitter input,generate data representing the
received signal for SNR  10, 20, and 30 dB.
(b) For each of these SNRs, set up a self-organizing map. For typical values, you may use the
following:
• An input vector made up of eight elements obtained by sampling the received signal
at a rate equal to eight times the signaling rate (i.e.,eight samples per signaling interval).
Do not assume knowledge of timing information.
• A one-dimensional lattice of 64 neurons (i.e., eight times the size of the input vector).
(c) Display the feature maps for each of the three SNRs, and thereby demonstrate the
topological-ordering property of the SOM algorithm.
474
Chapter 9
Self-Organizing Maps
Pulse
amplitude
Code
000
001
011
010
110
Midpoint
111
101
100
7
2

5
2

3
2

1
2

1
2

3
2

5
2

7
2

FIGURE P9.16
For Problem 9.16.
TABLE P9.15
Summary of Renormalized Training Algorithm (One-Dimensional Version)
1. Initialization. Set the number of code vectors to be some small number (e.g., use two for simplicity,
or some other value more representative of the problem at hand). Initialize their positions to be
those of a corresponding number of training vectors chosen randomly from the training sample.
2. Selection of an input vector. Choose an input vector randomly from the training sample.
3. Encoding of the input vector. Determine the “winning” code vector (i.e., the synaptic-weight vector
of the winning neuron).To do this, use either the nearest-neighbor or the minimum-distortion
encoding prescription as required.
4. Updating of the code book. Do the usual “winner and its topological neighbors” update.You may
find it sufficient to keep the learning-rate parameter  fixed (at 0.125, say) and to update the win-
ning neuron by using  and its nearest neighbors by using /2, for example.
5. Splitting of the code book.a Continue with the code-book update (step 4), each time using a new
input vector chosen randomly from the training sample, until the number of code-book updates is
about 10–30 times the number of code vectors.When this number is reached, the code book has
probably settled down, and it is time to split the code book.You may do so by taking the Peano
string of code vectors that you have and interpolating their positions to generate a finer grained
approximation to the Peano string; you may simply put an extra code vector halfway between each
two existing code vectors.
6. Completion of training. The code-book update and the code-book splitting are continued until the
total number of code vectors has reached some predetermined value (e.g., 100), at which time the
training is all over.
aThe splitting of the code book approximately doubles the number of code vectors after each epoch, so it
does not take too many epochs to get to any prescribed number of code vectors.

ORGANIZATION OF THE CHAPTER
The theme of this chapter is the study of unsupervised learning models whose formu-
lations, in one way or another, are rooted in information theory.
The chapter is organized as follows:
1. Section 10.1 presents motivational material on information theory and its profound
impact on neural processing.
2. Section 10.2 to 10.6 review fundamental concepts in Shannon’s information the-
ory. The review begins with Section 10.2 on the concept of entropy, followed by
Section 10.3 on the maximum-entropy principle. Section 10.4 discusses the con-
cept of mutual information between a pair of continuous random variables and
examines the associated properties. The related concept of Kullback–Leibler
divergence, providing a measure of how closely a pair of different probability
density functions are matched with each other, is discussed in Section 10.5, which
also discusses the relationship between mutual information and Kullback–Leibler
divergence. Section 10.6 completes the review by describing copulas, which is a
useful concept that has been known for several decades but largely overlooked.
3. Section 10.7 discusses the role of mutual information as an objective function for
unsupervised learning, thereby paving the way for discussions of the following five
principles and some of their applications, set forth in Sections 10.8 to 10.12.
• Maximum mutual information (Infomax) principle
• Principle of minimum redundancy
• Imax principle for dealing with spatially coherent features
• Imin principle for dealing with spatially incoherent features
• Principle of independent-components analysis (ICA)
4. Section 10.13 addresses the issue of sparseness, which is an inherent characteristic of
natural images; the section also motivates ICA theory by demonstrating its rela-
tionship to sparseness.
5. Section 10.14 to 10.17 describe different ICA algorithms, emphasizing their practical
virtues and limitations:
• Natural-gradient learning algorithm
• Maximum-likelihood estimation
475
C H A P T E R  1 0
Information-Theoretic
Learning Models

• Maximum-entropy learning algorithm
• FastICA through maximization of a non-Gaussian criterion known as negentropy
6. Section 10.18 discusses a relatively new concept known as coherent ICA, the devel-
opment of which builds on the use of copulas.
7. Section 10.19 introduces another new and fascinating method known as the information-
bottleneck (IB) method, which builds on another concept in Shannon’s information
theory: rate distortion theory.The IB method paves the way for the description of an
optimal manifold representation of data, which is discussed in Section 10.20, followed
by a computer experiment in Section 10.21.
The chapter concludes with a summary and discussion in Section 10.22.
10.1
INTRODUCTION
In a classic paper published in 1948, Claude Shannon laid down the foundations of
information theory. Shannon’s original work on information theory,1 and its refinement
by other researchers, was in direct response to the need of electrical engineers to design
communication systems that are both efficient and reliable. In spite of its practical ori-
gins, information theory as we know it today is a deep mathematical theory concerned
with the very essence of the communication process. The theory provides a framework
for the study of fundamental issues such as the efficiency of information representation
and the limitations involved in the reliable transmission of information over a com-
munication channel. Moreover, the theory encompasses a multitude of powerful the-
orems for computing ideal bounds on the optimum representation and transmission
of information-bearing signals.These bounds are important because they provide bench-
marks for the improved design of information-processing systems.
The main purpose of this chapter is to discuss information-theoretic models that
lead to self-organization in a principled manner. In this context, a model that deserves
special mention is the maximum mutual information principle due to Linsker (1988a,b).
This principle states the following:
The synaptic connections of a multilayered neural network develop in such a way as to
maximize the amount of information that is preserved when signals are transformed at each
processing stage of the network, subject to certain constraints.
The idea that information theory may offer an explanation for perceptual processing is
not new.2 For instance, in an early paper by Attneave (1954), the following information-
theoretic function is proposed for perceptual systems:
A major function of the perceptual machinery is to strip away some of the redundancy of
stimulation, to describe or encode information in a form more economical than that in which
it impinges on the receptors.
The main idea behind Attneave’s paper is the recognition that encoding of data from a
scene for the purpose of redundancy reduction is related to the identification of specific
features in the scene. This important insight is related to a view of the brain described
in Craik (1943), where a model of the external world is constructed so as to incorporate
the regularities and constraints of the world.
476
Chapter 10
Information-Theoretic Learning Models

10.2
ENTROPY
Consider a random variable X, each realization (presentation) of which may be regarded
as a message. Strictly speaking, if the random variable X is continuous in its amplitude
range, then it carries an infinite amount of information. However, on physical and bio-
logical grounds, we recognize that it is meaningless to think in terms of amplitude mea-
surements with infinite precision, which suggests that the value of X may be uniformly
quantized into a finite number of discrete levels.Accordingly,we may view X as a discrete
random variable, modeled as
(10.1)
where the sample value xk is a discrete number and (2K  1) is the total number of dis-
crete levels.The separation x between the discrete levels is assumed to be small enough
for the model of Eq. (10.1) to provide an adequate representation for the random vari-
able X.We may, of course, pass to the continuum limit by letting x approach zero and
K approach infinity, in which case we have a continuous random variable; then sums
become integrals.
To complete the model, let the event X  xk occur with probability
(10.2)
with the requirement that
(10.3)
Suppose that the event X  xk occurs with probability pk  1, which therefore
requires that pi  0 for all 
. In such a situation, there is no “surprise,” and there-
i Z k
0  pk  1  and  
a
K
k=-K
pk = 1
pk = P(X = xk)
X = {xkk = 0, ;1, ..., ;K}
Section 10.2
Entropy
477
fore no “information” is conveyed by the occurrence of the event X  xk, since we
know what the message must be. If, on the other hand, the various discrete levels
were to occur with different probabilities and, in particular, if the probability pk is low,
then there is more “surprise” and therefore “information” when X takes the value xk
rather than another value xi with higher probability 
. Thus, the concepts
“uncertainty,”“surprise,” and “information” are all related. Before the occurrence of
the event X  xk, there is an amount of uncertainty. When the event X  xk occurs,
there is an amount of surprise. After the occurrence of the event X  xk, there is an
increase in the amount of information.These three amounts are obviously the same.
Moreover, the amount of information is related to the inverse of the probability of
occurrence.
We define the amount of information gained after observing the event X  xk with
probability pk as the logarithmic function
(10.4)
where the base of the logarithm is arbitrary. When the natural logarithm is used, the
units for information are called nats, and when the base-2 logarithm is used, the units are
I(xk) = log a 1
pk
b = - log pk
pi, i Z k

called bits. In any case, the definition of information given in Eq. (10.4) exhibits the
following properties:
1.
(10.5)
Obviously, if we are absolutely certain of the outcome of an event, there is no
information gained by its occurrence.
2.
(10.6)
That is, the occurrence of an event X  xk either provides some or no information,
but it never results in a loss of information.
3.
(10.7)
That is, the less probable an event is, the more information we gain through its
occurrence.
The amount of information I(xk) is a discrete random variable with probability pk.
The mean value of I(xk) over the complete range of 2K  1 discrete values is given by
(10.8)
The quantity H(X) is called the entropy of a random variable X permitted to take a
finite set of discrete values; it is so called in recognition of the analogy between the def-
inition given in Eq. (10.8) and that of entropy in statistical thermodynamics.3 The
entropy H(X) is a measure of the average amount of information conveyed per mes-
sage. Note, however, that the X in H(X) is not an argument of a function, but rather
a label for a random variable. Note also that in the definition of Eq. (10.8), we take
0 log 0 to be 0.
The entropy H(X) is bounded as
(10.9)
where (2K  1) is the total number of discrete levels. Furthermore, we may make two
statements:
1. H(X)  0 if, and only if, the probability pk  1 for some k and the remaining prob-
abilities in the set are all zero; this lower bound on entropy corresponds to no
uncertainty.
2. H(X)  log(2K  1), if, and only if, pk  1/(2K  1) for all k (i.e., all the discrete
levels are equiprobable); this upper bound on entropy corresponds to maximum
uncertainty.
0  H(X)  log (2K + 1)
= - a
K
k=-K
pk log pk
=
a
K
k=-K
pkI(xk)
H(X) = [I(xk)]
I(xk) 7 I(xi)  for pk 6 pi
I(xk)  0  for 0  pk  1
I(xk) = 0  for pk = 1
478
Chapter 10
Information-Theoretic Learning Models

Differential Entropy of Continuous Random Variables
The discussion of information-theoretic concepts has thus far involved ensembles of
random variables that are discrete in their amplitude values. We now extend some of
these concepts to continuous random variables.
Consider next a continuous random variable X with the probability density func-
tion pX(x). By analogy with the entropy of a discrete random variable, we introduce the
definition:
(10.10)
We refer to h(X) as the differential entropy of X to distinguish it from the ordinary
entropy, or absolute entropy.
We justify the use of Eq. (10.10) as follows: We begin by viewing the continuous
random variable X as the limiting form of a discrete random variable that assumes the
value xk  k x, where k  0, ±1, ±2, …, and x approaches zero. By definition, the con-
tinuous random variable X assumes a value in the interval [xk, xk  x] with probabil-
ity pX(xk) x. Hence, permitting x to approach zero, the ordinary entropy of the
continuous random variable X may be written in the limit as
(10.11)
= h(X) - lim
xS0 logx
= -
3
q
-q
pX(x) log pX(x) dx - lim
xS0 log x
3
q
-q
pX(x) dx
= - lim
xS0 c a
q
k=-q
pX(xk)(log pX(xk)) x + log x a
q
k=-q
pX(xk) xd
H(X) = - lim
xS0 a
q
k=-q
pX(xk) x log(pX(xk) x)
= -[log pX(x)]
h(X) = -
3
q
-q
pX(x) log pX(x)dx
Section 10.2
Entropy
479
where, in the last line, we have made use of the first line of Eq. (10.10) and the fact that
the total area under the curve of the probability density function pX(x) is unity. In the
limit as x approaches zero, the term log x approaches infinity. This means that the
entropy of a continuous random variable is infinitely large. Intuitively, we would expect
this to be true because a continuous random variable may assume a value anywhere in
the open interval 
and the uncertainty associated with the variable is on the
(-q, q)
order of infinity.We avoid the problem associated with the term log x by adopting h(X)
as the differential entropy, with the term log x serving as a reference. Moreover, since
the information processed by a stochastic system as an entity of interest is actually the
difference between two entropy terms that have a common reference, the information
will be the same as the difference between the corresponding differential entropy terms.
We are therefore justified in using the term h(X), defined in Eq. (10.11), as the differ-
ential entropy of the continuous random variable X.
When we have a continuous random vector X consisting of n random variables
X1, X2, …, Xn, we define the differential entropy of X as the n-fold integral

(10.12)
where pX(x) is the joint probability density function of X and x is a sample value of X.
EXAMPLE 1
Uniform Distribution
Consider a random variable X uniformly distributed inside the interval [0, a] such that the prob-
ability density function
The differential entropy of X is
For a < 1, log a is negative, which means that the entropy h(X) is negative.We may therefore state
that,unlike the differential entropy of a discrete random variable,the differential entropy of a con-
tinuous random variable can assume a negative value.
For a  1, the differential entropy h(X) assumes the value zero.We may therefore say that
a uniformly distributed random variable contains the least amount of information among all
random variables.
■
Properties of Differential Entropy
From the definition of differential entropy h(X) given in Eq. (10.10), we readily see that
translation does not change its value; that is,
(10.14)
where c is constant.
Another useful property of h(X) is described by
(10.15)
where a is a scaling factor.To prove this property, we first recognize that since the area
under the curve of a probability density function is unity, then
(10.16)
Next, using the formula of Eq. (10.10), we may write
(10.17)
= - clog pY a y
a b d + log a
= - clog a 1
a pY a y
a b b d
h(Y) = - [log pY(y)]
pY(y) =
1
a pY a y
a b
h(aX) = h(X) + log  a
h(X + c) = h(X)
= log a
h(X) = -
3
a
0
1
a log a 1
a bdx
pX(x) = •
1
a,
0  x  a
0,
otherwise
= - [log pX(x)]
h(X) = -
3
q
-q
pX(x) log pX(x) dx
480
Chapter 10
Information-Theoretic Learning Models

By putting Y  aX in this equation, we obtain
(10.17)
from which Eq. (10.15) follows immediately.
Equation (10.15) applies to a scalar random variable. It may be generalized to the
case of a random vector X premultiplied by matrix A to yield
(10.18)
where det(A) is the determinant of matrix A.
10.3
MAXIMUM-ENTROPY PRINCIPLE
Suppose that we are given a stochastic system with a set of known states, but unknown
probabilities,and that somehow we learn some constraints on the probability distribution
of the states.The constraints can be certain ensemble average values or bounds on these
values.The problem is to choose a probability model that is optimum in some sense,given
this prior knowledge about the model.We usually find that there is an infinite number of
possible models that satisfy the constraints.Which model should we choose?
The answer to this fundamental question lies in the maximum-entropy (Max Ent)
principle 4 given by Jaynes (1957).The Max Ent principle may be stated as follows (Jaynes,
1957, 2003):
When an inference is made on the basis of incomplete information, it should be drawn
from the probability distribution that maximizes the entropy, subject to constraints on the
distribution.
In effect, the notion of entropy defines a kind of measure on the space of probability dis-
tributions such that those distributions of high entropy are favored over others.
From this statement, it is apparent that the Max Ent principle is a constrained-
optimization problem.To illustrate the procedure for solving such a problem, consider
the maximization of the differential entropy
over all probability density functions pX(x) of a random variable X, subject to the
following three constraints:
1. pX(x)
0, with equality outside the support of x
2.
3.
where gi(x) is some function of x. Constraints 1 and 2 simply describe two fundamental
properties of a probability density function. Constraint 3 defines the moments of X,
3
q
-q
pX(x)gi(x)dx = i  for i = 1, 2, ..., m
3
q
-q
pX(x) dx = 1

h(X) = -
3
q
-q
pX(x) log pX(x) dx
h(AX) = h(X) + log det(A)
h(aX) = -
3
q
-q
pX(x) log pX(x)dx + log a
Section 10.3
Maximum-Entropy Principle
481

depending on how the function gi(x) is formulated. In effect, constraint 3 sums up the
prior knowledge available about the random variable X. To solve this constrained-
optimization problem,we use the method of Lagrange multipliers, discussed in Chapter 6.
Specifically, we first formulate the Lagrangian
(10.19)
where 
0,
1,…,
m are the Lagrange multipliers.Differentiating the integrand of Eq.(10.19)
with respect to pX(x) and then setting the result equal to zero, we get
Solving this equation for the unknown pX(x) yields
(10.20)
The Lagrange multipliers in Eq.(10.20) are chosen in accordance with constraints 2 and 3.
Equation (10.20) defines the maximum-entropy distribution for this problem.
EXAMPLE 2
One-Dimensional Gaussian Distribution
Suppose the prior knowledge available to us is made up of the mean  and variance 2 of a ran-
dom variable X. By definition, the variance of the random variable X is given by
Comparing this equation with constraint 3, we readily see that
and
Hence, the use of Eq. (10.20) yields
Note that 
has to be negative if the integrals of pX(x) and (x  )2 pX(x) with respect to x are
to converge. Substituting this equation in equality constraints 2 and 3 and then solving for 
and
, we get
and

1 = - 1
22

0 = 1 - log (22)

1

0

1
pX(x) = exp [-1 + 
0 + 
1(x - )2]
1 = 2
g1(x) = (x - )2
3
q
-q
(x - )2pX(x)dx = 2 = constant
pX(x) = exp a - 1 + 
0 + a
m
i=1

igi(x) b
- 1 - log pX(x) + 
0 + a
m
i=1

igi(x) = 0
J(p) =
3
q
-q
c- pX(x) log pX(x) + 
0pX(x) + a
m
i=1

igi(x)pX(x)ddx
482
Chapter 10
Information-Theoretic Learning Models

The desired form for pX(x) is therefore described by
(10.21)
which is recognized as the probability density of a Gaussian random variable X of mean  and
variance 2.The maximum value of the differential entropy of such a random variable is given by
(10.22)
We may summarize the results of this example as follows:
1. For a given variance 2, the Gaussian random variable has the largest differential entropy
attainable by any random variable. That is, if X is a Gaussian random variable and Y is any
other random variable with the same mean and variance, then for all Y we have
with the equality holding only if the second random variable Y is also Gaussian.
2. The entropy of a Gaussian random variable X is uniquely determined by the variance of X
(i.e., it is independent of the mean of X).
■
EXAMPLE 3
Multidimensional Gaussian Distribution
In this second example, we want to build on the results of Example 2 to evaluate the differen-
tial entropy of a multidimensional Gaussian distribution. Since the entropy of a Gaussian
random variable X is independent of the mean of X, we may justifiably simplify the discussion
in this example by considering an m-dimensional vector X of zero mean. Let the second-order
statistics of X be described by the covariance matrix , defined as the expectation of the outer
product of X with itself. The joint probability density function of the random vector X is
given by
(10.23)
where 
is the determinant of
(Wilks, 1962). Equation (10.12) defines the differential
entropy of X.Therefore, substituting Eq. (10.23) into Eq. (10.12), we obtain the result
(10.24)
which includes Eq. (10.22) as a special case. In light of the Max Ent principle, we may thus state
the following:
For a given covariance matrix , the multivariate Gaussian distribution of Eq. (10.23) has the
largest differential entropy attainable by any random vector of zero mean, and that maxi-
mum differential entropy is defined by Eq. (10.24); the term “variate” is another way of refer-
ring to a component of random vector X.
■
h(X) = 1
2 [m + m log(2) + logdet()]

det()
pX(x) =
1
(2)m2(det())12 exp a- 1
2 xT-1x b

h(X)  h(Y)
h(X) = 1
2 [1 + log (22)]
pX(x) =
1
22
 exp a-
(x - )2
22
b
Section 10.3
Maximum-Entropy Principle
483

10.4
MUTUAL INFORMATION
Consider a pair of continuous random variables X and Y, which are correlated. From
probability theory, we may express the joint probability density function of X and Y as
(10.25)
Hence, invoking the definition of differential entropy, we may write
(10.26)
where h(X, Y) is called the joint differential entropy of X and Y, and h(YX) is called the
conditional differential entropy of Y given X. In words, we may say that the uncertainty
about X and Y is equal to the uncertainty about X plus the uncertainty about Y given
X. Similarly, we may say that the uncertainty about X and Y is equal to the uncertainty
about Y plus the uncertainty about X given Y, as shown by
(10.27)
Consider next a more structured situation that involves a stochastic neural system,
where the application of a continuous random variable X to the input of the system produces
a continuous random variable Y at the output of the system.By definition,the differential
entropy h(X) is the uncertainty about the system input X before observation of the system
output Y,and the conditional differential entropy H(XY) is the uncertainty about the sys-
tem input X after observation of the system output Y.The difference, H(X)  H(XY), is
therefore the uncertainty about the system input X that is resolved by observing the sys-
tem output Y.This entropic difference is called the mutual information between the system
input X and the system output Y; denoting it by I(X; Y), we may thus write
(10.28)
For the transition from line 1 to line 2 of Eq. (10.28), see Problem 10.2.The differential
entropy h(X) is a special case of the mutual information, since we have
The formula for the mutual information I(X;Y) in Eq.(10.28) is expressed in terms
of the differential entropy h(X). In a corresponding way, the mutual information I(Y; X)
may be expressed in terms of the differential entropy h(Y) as
(10.29)
where 
is the conditional differential entropy of Y given X.The mutual information
I(Y; X) is the uncertainty about the system output Y by observing the system input X.
The mutual information between two continuous random variables X and Y has
three important properties:
h(YX)
I(Y; X) = h(Y) - h(YX)
h(X) = I(X; X)
=
3
q
-q 3
q
-q
pXY(x y)pY(y) log a
pX Y(xy)
pY(y)
bdxdy
=
3
q
-q 3
q
-q
pX, Y(x, y) log a
pX, Y(x, y)
pX(x)pY(y) bdxdy
I(X; Y) = h(X) - h(XY)
h(X, Y) = h(Y) + h(XY)
h(X, Y) = h(X) + h(YX)
pX, Y(x, y) = pY(yx)pX(x)
484
Chapter 10
Information-Theoretic Learning Models
pX, Y(x, y)
¯˚˚˚˚˘˚˚˚˚˙

Property 1. Nonnegativity
The mutual information I(X; Y) is always nonnegative; that is,
(10.30)
This property states that, on average, we cannot lose information about the system input
X by observing the system output Y. Moreover, the mutual information I(X; Y) is zero
if, and only if, the input and output of the system are statistically independent.
Property 2. Symmetry
This second property states that
(10.31)
Properties 1 and 2 follow directly from the defining equation of Eq. (10.28).
Summing up Eqs. (10.26) to (10.31), we write
(10.32)
in light of which we may construct the picture presented in Fig. 10.1 (MacKay, 2003).The
differential entropy of the system input X is represented by the second rectangle in the
figure, and the differential entropy of the system output Y is represented by the third
rectangle. The mutual information between X and Y, shown as the shaded area in the
figure, is represented by the overlap between these two rectangles. The figure also
includes representations of the joint entropy h(X, Y), and the two conditional entropies
h(XY) and h(YX).
Property 3. Invariance
The mutual information is invariant under invertible transformations of random
variables.
= (h(X) + h(Y)) - h(X, Y)
= h(Y) - h(YX)
I(X; Y) = h(X) - h(XY)
I(Y; X) = I(X; Y)
I(X; Y)  0
Section 10.4
Mutual Information
485
h(XY)
h(YX)
h(X, Y)
h(X)
h(Y)
I(X; Y)
FIGURE 10.1
Relationships embodied in the three lines of 
Eq. (10.32), involving the mutual information I(X; Y).

Consider the invertible transformations
and
where x and y are sample values of the random variables X and Y, and u and v are sam-
ple values of the transformed random variables U and V. The invariance property of
mutual information states that
(10.33)
Since the transformations from x to u and from y to v are both invertible, there is no loss
of information in the course of these two transformations. Intuitively, this result vali-
dates the invariance property of mutual information.
Generalization of Mutual Information
The definition of mutual information I(X;Y) given in Eq.(10.28) applies to scalar random
variables X and Y.This definition may be readily extended to random vectors X and Y,
and we may thus write I(X; Y). Specifically, we define the mutual information I(X; Y) as
(10.34)
The mutual information I(X; Y) has properties that parallel those given in Eqs. (10.30)
and (10.31) for scalar random variables, which is intuitively satisfying.
10.5
KULLBACK–LEIBLER DIVERGENCE
The mutual information I(X; Y),defined in Eq.(10.34),applies to a stochastic neural sys-
tem whose input and output are denoted by the multidimensional vectors X and Y,
respectively. Consider next the same system, but this time we have two different prob-
ability density functions pX(x) and gX(x) as possible descriptors of the underlying dis-
tribution of the input vector X. We may then define the Kullback–Leibler divergence
(KLD) between pX(x) and gX(x) as follows (Kullback, 1968; Shore and Johnson, 1980):
(10.35)
where the expectation is with respect to the probability density function pX(x).
= clog a pX(x)
gX(x) b d
Dp 7 g =
3
q
-q
pX(x) log a
pX(x)
gX(x) bdx
pX, Y(x, y)
=
3
q
-q 3
q
-q
pXY(xy) PY(y) log a
pXY(xy)
pY(y)
b dxdy
=
3
q
-q 3
q
-q
pX,Y(x, y) log a
pX,Y(x, y)
pX(x)pY(y) b dxdy
I(X; Y) = h(X) - h(XY)
I(X; Y) = I(U; V)
v = g(y)
u = f(x)
486
Chapter 10
Information-Theoretic Learning Models
¯˚˚˚˚˘˚˚˚˚˙

The KLD has two unique properties of its own:
Property 1. Nonnegativity
This property states that
(10.36)
For the special case when gX(x)  pX(x), we have a perfect match between these two
distributions, in that the KLD between them is exactly zero.
Property 2. Invariance
Consider the invertible transformation
where x and y are samples of the random vectors X and Y,respectively.Correspondingly,
the KLD is invariant under this transformation, which means that
The 
is the KLD referring to the input vector X and the 
is the correspond-
ing KLD referring to the transformed output vector Y.
Relationship between the Kullback–Leibler Divergence 
and Mutual Information
The mutual information I(X; Y) between a pair of vectors X and Y has an interesting
interpretation in terms of the Kullback–Leibler divergence. Reproducing the second
line of Eq.(10.34) for convenience of presentation, we write
and comparing this formula with that of Eq. (10.35), we immediately deduce the fol-
lowing result:
(10.37)
In words, the mutual information I(X; Y) between X and Y is equal to the Kullback–
Leibler divergence between the joint probability density function pX,Y(x, y) and the
product of the marginal probability density functions pX(x) and pY(y).
Entropic Interpretation of the Kulback–Leibler Divergence
A special case of this latter result described in Eq. (10.37) is the Kullback–Leibler diver-
gence between the probability density function pX(x) of an m-by-1 random vector X
and the product of its m marginal probability density functions. Let 
denote the
ith marginal probability density function of component 
which is defined by
(10.38)
where x(i) is the (m  1)-by-1 vector left after removing the ith element from vector x.
pXi(xi) =
3
q
-q
pX(x)dx(i),  i = 1, 2, ..., m
Xi,
p~
Xi(xi)
I(X; Y) = DpX, Y7 pXpY
I(X; Y) =
3
q
-q 3
q
-q
pX, Y(x, y) log a
pX, Y(x, y)
pX(x)pY(y) bdxdy
DpY 7 gY
DpX 7 gX
DpX 7 gX = DpY 7 gY
y = f(x)
Dp 7 g  0
Section 10.5
Kullback–Leibler Divergence
487

Define the factorial distribution
which represents an independent set of random variables; the distribution of the ith
component Xi in this set is the same as the ith marginal distribution of the original ran-
dom vector X. The KLD between the ordinary probability density function pX(x) and
its factorial counterpart 
is given by
(10.39)
The first integral on the right-hand in the second line of side of Eq. (10.39) is, by defin-
ition, equal to h(X), where h(X) is the differential entropy of X.To deal with the sec-
ond term on the right-hand side of the equation,we first note that the differential dx may
be expressed as
Hence, we may write
(10.40)
where the inner integral on the right-hand side is with respect to the (m  1)-by-1 vector
x(i) and the outer integral is with respect to the scalar xi. But from Eq.(10.38),we see that
the inner integral is in fact equal to the marginal probability density function 
.
Accordingly, we may rewrite Eq. (10.40) in the equivalent form
(10.41)
where 
is the ith marginal entropy (i.e., the differential entropy based on the
h~(Xi)
= - h~(Xi),  i = 1, 2, ..., m
3
q
-q
pX(x) log p~
Xi(xi)dx =
3
q
- q
p~
Xi(xi) log p~
Xi(xi)dxi
p~
Xi(xi)
3
q
- q
pX(x) logp~
Xi(xi)dx =
3
q
-q
log p~
Xi(xi)
3
q
-q
pX(x)dx(i)dxi
dx = dx(i)dxi
=
3
q
- q
pX(x) log pX(x)dx - a
m
i=1 3
q
-q
pX(x) log p~
Xi(xi)dx
DpX7 p
X =
3
q
- q
pX(x) log °
pX(x)
q
m
i-1
p~
Xi(xi)
¢ dx
p~
X(x)
p~
X(x) = q
m
i=1
p~
Xi(xi)
488
Chapter 10
Information-Theoretic Learning Models
marginal probability density function 
).Finally,using Eq.(10.41) in Eq.(10.39) and
f~
Xi(xi)
noting that the first integral in Eq. (10.39) is equal to h(X), we may simplify the
Kullback–Leibler divergence of Eq. (10.39) to
(10.42)
Later in the chapter, we will use this formula in the study of independent-components
analysis.
DpX7p
X = -h(X) + a
m
i=1
h~(Xi)

Pythagorean Decomposition
Next, we consider the Kullback–Leibler divergence between the probability density
functions pX(x) and pU(x), where the vector x is a sample value common to both random
vectors X and U, and xi is the ith component of x.The m-by-1 random vector U consists
of independent variables, as shown by
The m-by-1 random vector X is defined in terms of U as
where A is a nondiagonal matrix. Let 
denote the marginal probability density
function of each Xi that is derived from pX(x). Then the Kullback–Leibler divergence
between pX(x) and pU(x) admits the following Pythagorean decomposition:
(10.43)
We refer to this classic relation as a Pythagorean decomposition because it has an
information-geometric interpretation (Amari, 1985).5
10.6
COPULAS
The mutual information I(X; Y) provides a measure of the statistical dependence
between two random variables X and Y. For a graphical interpretation of this depen-
dence, we may look to the picture depicted in Fig. 10.1, based on Eq. (10.32). However,
this equation lacks mathematical insight. Specifically, if the mutual information I(X; Y)
is zero, it tells us that the random variables X and Y are statistically independent. But,
if I(X;Y) is greater than zero,confirming statistical dependence between X and Y, it does
not provide us with a statistical measure of the dependence.
To elaborate,consider a pair of random variables X and Y whose sample values are
denoted by x and y,respectively.The issue of interest is to formulate a measure of statistical
dependence between X and Y that is not disturbed by their scaled versions or their vari-
ances.In order to achieve this objective,we transform X and Y into two new random vari-
ables U and V, respectively, such that the distributions of both U and V are uniform over
the interval [0,1].This transformation is one of nonlinear scaling expressed in terms of the
cumulative distribution functions 
and 
it is performed by setting
and
where u and v are sample values of the random variables U and V, respectively. The
joint distribution of the pair (U, V) is spread over the unit square [0, 1]  [0, 1]; the dis-
tribution is uniform if,and only if,the original random variables X and Y (or,equivalently,
the new random variables U and V) are statistically independent.The joint distribution
v = PY(y)
u = PX(x)
PY(y);
PX(x)
DpX7 pU = DpX7p
X + DpX7p~
U
p~
Xi(xi)
X = AU
pU(x) = q
m
i=1
pUi(xi)
Section 10.6
Copulas
489

of X and Y is thus transformed to that of U and V on the unit square, where the mar-
ginal distributions are uniform.
The new pair of random variables (U, V) is uniquely determined, and it is called a
copula.6 Formally,
The copula, involving the pair of random variables (U, V), is a function that models the sta-
tistical dependence between U and V in a distribution-free manner.
We may go on to state Sklar’s theorem on copulas as follows (Sklar, 1959):
Given the cumulative distribution functions PX,Y(x, y), PX(x), and PY(y), there exists a unique
copula CU, V(u, v) that satisfies the pair of relationships
(10.44)
and
(10.45)
where the two new random variables U and V are nonlinearly transformed versions of the orig-
inal random variables X and Y, respectively, and their sample values, u and v, are themselves
defined by
(10.46)
and
(10.47)
The joint distribution of the pair of random variables (U, V) is spread over the unit square.
Properties of Copulas
Property 1. Limiting values of copulas
With both sample values u and v limited to the range [0, 1], the copula values are
themselves limited as
Property 2. Joint density pX ,Y(x, y) expressed using the copula
The joint probability density function pX,Y(x, y) is informatively expressed in terms
of the copula as the product of three terms:
• the marginal probability density functions pX(x) and pY(y), and
• the copula’s joint probability density function cU,V(u,v).
To establish this relationship, we start with the basic definition of the joint probability
density function:
CU, V(1, v) = v
CU, V(u, 1) = u
CU, V(u, 0) = CU, V(0, v) = 0
v = PY(y)
u = PX(x)
CU, V(u, v) = P(P-1
X (x), P-1
Y (y))
PX, Y(x, y) = CU, V(PX(x), PY(y))
490
Chapter 10
Information-Theoretic Learning Models

Then, using Eq. (10.44), we write
where,in the last line,we used the definition PY(y)  v, and the prime in CU,V(PX(x),PY(y))
denotes differentiation of the copula with respect to PY(y). With the marginal PY(y)
being independent of x, we go on to write
where the second prime in C-U, V(PX(x), v) denotes differentiation of the derivative
CU, V(PX(x), v) with respect to PX(x). Finally, recognizing that PX(x)  u and that, by
definition, the joint probability density function of the copula is expressed as
(10.48)
we obtain the following relationship:
(10.49)
Equation (10.49) now leads us to make the following insightful statement:
If two random variables X and Y are statistically dependent, then the copula’s joint density
cU, V(u, v) distinctly accounts for the statistical dependence between X and Y.
This statement highlights the very essence of the copula.
EXAMPLE 4
The Copula of Two Statistically Independent Random Variables
Let the random variables X and Y be statistically independent.We then have
pX, Y(x, y) = pX(x)pY(y)
pX, Y(x, y) = pX(x)pY(y)cU, V(u, v)
cU, V(u, v) =
02
0u0v CU, V(u, v)
= pY(y)pX(x)C–U, V(PX(x), v)
= pY(y)
0PX(x)
0x
0
0PX(x) C¿U, V(PX(x), v)
pX, Y(x, y) = pY(y) 0
0x C¿U, V(PX(x), v)
=
0
0x CpY(y)C¿U, V(PX(x), v)D
=
0
0x c
0PY(y)
0y
0
0PY(y) CU, V(PX(x), PY(y))d
=
0
0x
0
0y CU, V(PX(x), PY(y))
pX, Y(x, y) =
02
0x0y CU, V(PX(x), PY(y))
pX, Y(x, y) =
02
0x0y PX, Y(x, y)
Section 10.6
Copulas
491

Under this condition, Eq. (10.49) reduces to
Correspondingly, we have
Thus, the product copula’s density CU ,V(u, v)  uv joins U and V together when the correspond-
ing random variables X and Y are statistically independent.
■
Relationship between Mutual Information and the Copula’s Entropy
With the background just presented on the copula, we are now ready to make another
statement:
The mutual information between two random variables X and Y is the negative of the cop-
ula’s joint entropy of the corresponding nonlinearly transformed pair of random variables U
and V.
To demonstrate this relationship, we may proceed as follows:
(i) Since the random variables U and V are the results of invertible transformations
applied to the original random variables X and Y,the invariance property of mutual
information, described in Section 10.4, immediately yields
(ii) Applying the last line of Eq. (10.32) to the mutual information I(U; V) gives
Since the random variables U and V are both uniformly distributed over the inter-
val [0, 1], it follows that the differential entropies h(U) and h(V) are both zero.
Hence, the formula for I(U; V) reduces to
(10.50)
which is the desired relationship.
The definition of mutual information in Eq. (10.50) is intuitively more satisfying
than any of the three standard formulas presented in Eq. (10.32), for two reasons:
1. Given a pair of random variables, the mutual information between them is
expressed directly as a function of the copula,which is part and parcel of the under-
lying distribution that matches the dependency between the two random variables.
= [log cU, V(u, v)]
I(U; V) = - hC(U, V)
I(U; V) = hC(U) + hC(V) - hC(U, V)
I(X; Y) = I(U; V)
= uv
=
3
u
0 3
v
0
1 du dv
CU, V(u, v) =
3
u
0 3
v
0
cU, V(u, v)du dv
cU, V(u, v) = 1,  for 0  u, v  1
492
Chapter 10
Information-Theoretic Learning Models

2. The mutual information is not a function of the marginal distributions of the two
random variables.
Furthermore, following on Eq. (10.49), we may make two more insightful remarks:
10.7
MUTUAL INFORMATION AS AN OBJECTIVE 
FUNCTION TO BE OPTIMIZED
Now that we have developed an adequate understanding of Shannon’s information the-
ory, we are ready to discuss its role in the study of self-organizing systems.
To proceed with the discussion, consider a neural system with multiple inputs and
multiple outputs. The primary objective here is for the system to be self-organizing,
designed for a specific task (e.g., modeling, extraction of statistically salient features, or
signal separation). This requirement can be satisfied by choosing the mutual informa-
tion between certain variables of the system as the objective function to be optimized.
This particular choice is justified by two considerations:
1. The mutual information has some unique properties, as discussed in Sections 10.4
to 10.6.
2. The mutual information can be determined without the need for a teacher, so the
provision for self-organization is naturally met.
The problem thus becomes one of adjusting the free parameters (i.e., synaptic weights)
of the system so as to optimize the mutual information.
Depending on the application of interest, we may identify the four different sce-
narios illustrated in Fig. 10.2.These scenarios are described as follows:
• In scenario 1, depicted in Fig. 10.2a, the input vector X is composed of the ele-
ments X1, X2, ..., Xm, and the output vector Y is composed of the elements
Y1, Y2, ..., Yl.The requirement is to maximize the information conveyed to the sys-
tem output Y about the system input X (i.e.,the information flow across the system).
• In scenario 2,depicted in Fig.10.2b,a pair of input vectors Xa and Xb is derived from
adjacent, but nonoverlapping, regions of an image.The inputs Xa and Xb produce
scalar outputs Ya and Yb, respectively. The requirement is to maximize the infor-
mation conveyed to Yb about Ya and vice versa.
• In scenario 3, depicted in Fig. 10.2c, the input vectors Xa and Xb are derived from
a corresponding pair of regions belonging to two separate, but related, images.The
outputs produced by these two input vectors are denoted by Ya and Yb, respec-
tively.The objective is to minimize the information conveyed to Yb about Ya and vice
versa.
• In scenario 4, depicted in Fig. 10.2d, the input vector X and the output vector Y are
defined in a manner similar to those in Fig. 10.2a, but with equal dimensionality
I(X; Y) 7 0 corresponds to cU, V(u, v) 7 1.
I(X; Y) = 0 corresponds to cU, V(u, v) = 1.
Section 10.7
Mutual Information as an Objective Function to be Optimized
493

(i.e., l  m). The objective here is for the statistical dependence between the com-
ponents of the output vector Y to be minimized.
In all four situations, mutual information plays a central role. However, the way in which
it is formulated depends on the particular situation being considered.In what follows,the
issues involved in these scenarios and their practical implications are discussed in the same
order just presented. Most importantly, it should be noted that scenario 4 encompasses
much of the material presented in the chapter in terms of theory, computing algorithms,
and applications, which reflect the practical realities of information-theoretic models.
10.8
MAXIMUM MUTUAL INFORMATION PRINCIPLE
The idea of designing a neural processor to maximize the mutual information I(Y; X)
is appealing as the basis for statistical signal processing.This method of optimization is
embodied in the maximum mutual information (Infomax) principle due to Linsker (1987,
1988a, 1989a), which may be stated formally as follows:
The transformation of a random vector X observed in the input layer of a neural system to a
random vector Y produced in the output layer of the system should be so chosen that the
activities of the neurons in the output layer jointly maximize information about the activities
in the input layer.The objective function to be maximized is the mutual information I(Y; X)
between the vectors X and Y.
494
Chapter 10
Information-Theoretic Learning Models
Minimize
statistical
dependence
between the
Y’s
Maximize information
conveyed to Y about X
Output
Y
Maximize
information conveyed
to Yb about Ya
and vice versa 
Minimize
information conveyed
to Yb about Ya
and vice versa 
Neural
model
Neural
model
•••
•••
•••
•••
X1
Y1
Y2
X2
Xm
Xa1
Xa2
Xb1
Xb2
Xam
Xbm
Ym
(d)
Neural
model
•••
•••
X1
Y1
Y2
X2
Xm
YO
Input
X
(a)
Neural
model
•••
•••
Xa1
Xb1
Xb2
Xa2
Ya
Xam
Xbm
Yb
Ya
Yb
Xa
Xb
(b)
(c)
FIGURE 10.2
Four basic scenarios that lend themselves to the application of information
maximization and its three variants.

The Infomax principle provides a mathematical framework for self-organization of the
signal transmission system described in Fig. 10.2a, assuming that the number of compo-
nents l in the output vector Y is smaller than the number of components m in the input
vector x.Also,this principle may be viewed as the neural network counterpart of the con-
cept of channel capacity, which defines the Shannon limit on the rate of information
transmission through a communication channel.
Next, we illustrate applications of the Infomax principle with two examples involv-
ing a single noisy neuron.In one example the noise appears at the output,and in the other
example it appears at the input.
EXAMPLE 5
Single Neuron Corrupted by Processing Noise
Consider the simple case of a linear neuron that receives its inputs from a set of m source nodes.
Let the output of this neuron in the presence of processing noise be expressed as
(10.51)
where wi is the ith synaptic weight and the random N is processing noise, as modeled in Fig. 10.3.
It is assumed that
•
the output Y of the neuron is a Gaussian random variable with zero mean and variance 2
Y;
•
the processing noise N is also a Gaussian random variable with zero mean and variance
2
N;
•
the processing noise is uncorrelated with any of the input components; that is,
The Gaussianity of the output Y can be satisfied in one of two ways. First, we start with X1, X2, ...,
Xm that are all Gaussian distributed. Then, with the additive noise N assumed to be Gaussian,
too, the Gaussianity of Y is assured by virtue of the fact that it is the weighted sum of a number
of Gaussian-distributed random variables.Alternatively, the inputs X1, X2, ..., Xm are statistically
independent, and, under mild conditions, their weighted sum approaches a Gaussian distribution
for large m by the central limit theorem of probability theory.
To proceed with the analysis, we first note from the second line of Eq. (10.32) that the
mutual information I(Y; X) between the output Y of the neuron and the input vector X is
(10.52)
I(Y; X) = h(Y) - h(YX)
[NXi] = 0  for all i
Y = a a
m
i=1
wiXi b + N
Section 10.8
Maximum Mutual Information Principle
495
Y
N
Noise
Output random
variable
Xm
X2
X1
w1
w2
wm
•••
Set of
input
random
variables
FIGURE 10.3
Signal-flow
graph of a noisy neuron.

In view of Eq. (10.51), we find that the probability density function of Y, given the input vector
X, is the same as the probability density function of a constant plus a Gaussian-distributed ran-
dom variable. Accordingly, the conditional entropy 
is the “information” that the output
neuron conveys about the processing noise N rather than about the signal vector X. We may
thus set
and therefore rewrite Eq. (10.52) simply as
(10.53)
By applying Eq. (10.22) for the differential entropy of a Gaussian random variable to the prob-
lem at hand, we obtain
(10.54)
and
(10.55)
After simplification, the use of Eqs. (10.54) and (10.55) in Eq. (10.53) yields
(10.56)
where 
depends on 2
N.
2
Y
I(Y; X) = 1
2 log a 2
Y
2
N
b
h(N) = 1
2 [1 + log(22
N)]
h(Y) = 1
2 [1 + log(22
Y)]
I(Y; X) = h(Y) - h(N)
h(YX) = h(N)
h(YX)
496
Chapter 10
Information-Theoretic Learning Models
The ratio 
may be viewed as a signal-to-noise ratio. Imposing the constraint that the
2
Y2
N
noise variance 
is fixed, we see from Eq. (10.56) that the mutual information I(Y; X) is maxi-
2
N
mized by maximizing the variance 
of the neuron output Y.We may therefore state that under
the condition that noise produced at the output of a neuron is due to a source independent of the
neural inputs, maximizing the output variance of the neuron maximizes the mutual information
between the output signal of that neuron and its inputs.
Lastly,the treatment of a single neuron corrupted by additive processing noise,based on min-
imizing the output’s variance, yields a solution that is just the PCA neuron trained on Oja’s rule,
which was described in Chapter 8.
■
EXAMPLE 6
Single Neuron Corrupted by Additive Input Noise
Suppose that the noise corrupting the behavior of a linear neuron originates at the input ends of
the synapses as shown in the model of Fig. 10.4.According to this second noisy model, we have
(10.57)
where each noise component 
is assumed to be an independent Gaussian random variable
with zero mean and common variance 
We may rewrite Eq. (10.57) in a form similar to that of
Eq. (10.51), as shown by
Y = a a
m
i=1
wiXi b + N¿
2
N.
Ni
Y = a
m
i=1
wi(Xi + Ni)
2
Y

where 
is a composite noise component defined by
The noise 
has a Gaussian distribution with zero mean and a variance equal to the sum of the
variances of its independent noise components; that is,
As before, we assume that the output Y of the neuron has a Gaussian distribution with vari-
ance 
The mutual information I(Y; X) between Y and X is still given by Eq. (10.52).This time,
however, the conditional entropy 
is defined by
(10.58)
Thus, using Eqs. (10.54) and (10.58) in (10.52) and then simplifying terms, we get
(10.59)
I(Y; X) = 1
2 log °
2
Y
2
N a
m
i=1w2
i
¢
= 1
2 c1 + 22
N a
m
i=1
w2
i d
= 1
2 (1 + 22
N¿)
h(YX) = h(N¿)
h(YX)
2
Y.
2
N¿ = a
m
i=1
w2
i 2
N
N¿
N¿ = a
m
i=1
wiNi
N¿
Section 10.8
Maximum Mutual Information Principle
497
Xm
Y
Output random
variable
N1
N2
Nm
X2
X1
•••
wm
w2
w1
Set of
input
random
variables
Set of
noise
variables
FIGURE 10.4
Another
noisy model of the neuron.
Under the constraint that the noise variance 
is maintained constant, the mutual information
2
N
I(Y; X) is now maximized by maximizing the ratio 
where 
is a function of wi.
■
2
Y
2
Y©m
i=1 w2
i

What can we infer from Examples 5 and 6?
First, we see from the material presented
in these two examples that the result of applying the Infomax principle is problem depen-
dent.The equivalence between maximizing the mutual information I(Y; X) and output
variance that applies to the model of Fig. 10.3, for a prescribed noise variance 
does
not carry over to the model of Fig. 10.4. It is only when we impose the constraint 
2
N,
498
Chapter 10
Information-Theoretic Learning Models
on the model of Fig. 10.4 that both models behave in a similar manner.
In general, the determination of the mutual information I(Y; X) between input
vector X and output vector Y is a difficult task. In Examples 5 and 6, we made the analy-
sis mathematically tractable by assuming that the noise distributions in a system with one
or more sources of noise are multivariate Gaussian. This assumption needs to be justi-
fied in practical applications of the Infomax principle.
In adopting a Gaussian noise model, we are in essence invoking a “surrogate”
mutual information computed on the premise that the output vector Y of a neuron has
a multivariate Gaussian distribution with the same mean vector and covariance matrix
as the actual distribution.In Linsker (1993),the Kullback–Leibler divergence is employed
to provide a principled justification for the use of such surrogate mutual information,
under the condition that the network has stored information about the mean vector and
covariance matrix of the output vector Y, but not about higher-order statistics.
Finally, the analysis presented in both Examples 5 and 6 was carried out in the con-
text of a single neuron.This was done purposely with a specific point in mind:For the Info-
max principle to be mathematically tractable,the optimization should be performed at the
local neural level; such optimization is consistent with the essence of self-organization.
EXAMPLE 7
Noiseless Network
In Examples 5 and 6, we considered noisy neurons. In this example, we consider a noiseless net-
work that transforms a random vector X of arbitrary distribution to a new random vector Y of
different distribution. Recognizing that I(X; Y)  I(Y; X) and extending the second line of
Eq. (10.32) to the situation described here, we may express the mutual information between the
input vector X and output vector Y as
where h(Y) is the entropy of Y and 
is the conditional entropy of Y given X. With the
mapping from X to Y assumed to be noiseless, the conditional differential entropy 
attains
its lowest possible value:It diverges to 
This result is due to the differential nature of the entropy
of a continuous random variable, discussed in Section 10.2. However, this difficulty is of no conse-
quence when we consider the gradient of the mutual information I(Y; X) with respect to a weight
matrix W that parameterizes the input–output mapping network. Specifically, we may write
(10.60)
because the conditional entropy 
is independent of W. Equation (10.60) states the following:
For a noiseless mapping network, maximizing the differential entropy of the network out-
put Y is equivalent to maximizing the mutual information between Y and the network input
X, with both maximizations being performed with respect to the weight matrix W of the
mapping network.
■
h(YX)
0I(Y; X)
0W
=
0h(Y)
0W
-q.
h(YX)
h(YX)
I(Y; X) = h(Y) - h(YX)
g i w2
i = 1

10.9
INFOMAX AND REDUNDANCY REDUCTION
In Shannon’s framework of information theory, order and structure represent
redundancy, which diminishes uncertainty via the receipt of information.The more order
and structure we have in the underlying process, the less information we receive by
observing that process. Consider for example, the highly structured and redundant
sequence of examples aaaaaa. On receiving the first example, a, we can immediately
say that the remaining five examples are all the same. The information conveyed by
such a sequence of examples is limited to that contained in a single example. In other
words, the more redundant a sequence of examples is, the smaller is the information
content of that sequence, but the greater is the structure of that information content.
From the definition of mutual information I(Y; X), we know mutual information
is a measure of the uncertainty about the output Y of a system that is resolved by observ-
ing the system input X. The Infomax principle operates by maximizing the mutual infor-
mation I(Y; X), as a result of which we are more certain about the system output Y by
observing the system input X. In light of the previously mentioned relationship between
information and redundancy, we may therefore say the following:
The Infomax principle leads to a reduction in redundancy in the output Y compared with that
in the input X.
The presence of noise is a factor that prompts the use of redundancy and the
related method of diversity, which we define as follows: By “diversity,” we mean the use
of two or more outputs with different properties being produced by a processor. More
to the point, when the additive noise in the input signal is high, we may use redundancy
to combat the degrading effects of the noise. In such an environment, more of the (cor-
related) components of the input signal are combined by the processor to provide an
accurate representation of the input.Also, when the output noise (i.e., processor noise)
is high, more of the output components are directed by the processor to provide redun-
dant information.The number of independent properties observed at the output of the
processor is thereby reduced, but the representation accuracy of each property is
increased.We may thus state that a high level of noise favors redundancy of representa-
tion. When, however, the noise level is low, diversity of representation is favored over
redundancy.
Modeling of a Perceptual System
Since the early days of information theory,it has been suggested that the redundancy of sen-
sory messages (stimuli) is important for understanding perception (Attneave,1954;Barlow,
1959).Indeed,the redundancy of sensory messages provides the knowledge that enables the
brain to build up its “cognitive maps” or “working models” of the environment around it.
Regularities in the sensory messages must somehow be recoded by the brain for it to know
what usually happens.However,redundancy reduction is the more specific form of Barlow’s
hypothesis, which states the following:
The purpose of early processing is to transform the highly redundant sensory input into a more
efficient factorial code.
Section 10.9
Infomax and Redundancy Reduction
499

In other words, the neural outputs become statistically independent when conditioned on
the input.
Inspired by Barlow’s hypothesis,Atick and Redlich (1990) postulated the principle
of minimum redundancy as the basis for an information-theoretic model of the perceptual
system shown in Fig.10.5.The model consists of three components:input channel,recoding
system, and output channel. The output of the input channel is described by
where S is an ideal signal received by the input channel and Ni is assumed to be the
source of all noise in the input.The signal X is subsequently transformed (recoded) by
a linear matrix operator A. It is then transmitted through the optic nerve, or output
channel, producing the output Y, as shown by
where No denotes the postencoding intrinsic noise. In the approach taken by Atick and
Redlich, it is observed that light signals arriving at the retina contain useful sensory
information in a highly redundant form. Moreover, it is hypothesized that the purpose
of retinal signal processing is to reduce or eliminate the redundant bits of data caused
by both correlations and noise before sending the signal along the optic nerve.To quan-
tify this notion, a redundancy measure is defined by
(10.61)
where I(Y; S) is the mutual information between Y and S and C(Y) is the channel capac-
ity of the optic nerve (output channel). Equation (10.61) is justified on the grounds that
the information the brain is interested in is the ideal signal S, while the physical chan-
nel through which this information must pass is in reality the optic nerve. It is assumed
that there is no dimensionality reduction in the input–output mapping performed by
the perceptual system, which means that C(Y)  I(Y; S).The requirement is to find an
input–output mapping (i.e., matrix A) that minimizes the redundancy measure R, sub-
ject to the constraint of no information loss, as shown by
where is some small positive parameter.The channel capacity C(Y) in Eq.(10.61) is defined
as the maximum rate of information flow possible through the optic nerve, ranging over
all probability distributions of inputs applied to it, with the average input power fixed.

I(Y; X) = I(X; X) - 
R = 1 -
I(Y; S)
C(Y)
Y = AX + No
X = S + Ni
500
Chapter 10
Information-Theoretic Learning Models
Recoding:
A
Input channel
Output channel
(optic nerve)
vi
s
x
Σ
vo
y
Σ
FIGURE 10.5
Model of a perceptual
system.The signal vector s and noise
vectors vi and vo are values of the random
vectors S, Ni, and No, respectively.

When the signal vector S and the output vector Y have the same dimensionality
and there is noise in the system, the principle of minimum redundancy and the Infomax
principle are mathematically equivalent, provided that a similar constraint is imposed
on the computational capability of the output neurons in both cases.To be specific, sup-
pose that the channel capacity is measured in terms of the dynamic range of the output
of each neuron in the model of Fig. 10.5. Then, according to the principle of minimum
redundancy, the quantity to be minimized is
for a given permissible information loss, and therefore for a given I(Y; S).Thus, for some
parameter 
, the quantity to be minimized is essentially
(10.62)
On the other hand, according to the Infomax principle, the quantity to be maximized in
the model of Fig. 10.5 is
(10.63)
Although the functions F1(Y; S) and F2(Y; S) are different, their optimizations yield
identical results:They are both formulations of the method of Lagrange multipliers, with
the roles of I(Y; S) and C(Y) being simply interchanged.
The important point to take from this discussion is that despite the difference in
formulations, these two information-theoretic principles lead to similar results7:
Maximization of the mutual information between the output and input of a neural system
does indeed lead to redundancy reduction.
10.10
SPATIALLY COHERENT FEATURES
The Infomax principle,as postulated in Section 10.8,applies to a situation where the mutual
information I(Y; X) between the output vector Y of a neural system and the input vector X
is the objective function to be maximized,as illustrated in Fig.10.2a.With appropriate changes
in terminology,we may extend this principle to deal with the unsupervised processing of the
image of a natural scene (Becker and Hinton,1992).An unprocessed pixel of such an image
contains a wealth of information about the scene of interest, albeit in complex structural
form. In particular, the intensity of each pixel is affected by such intrinsic parameters as
depth, reflectance, and surface orientation, as well as background noise and illumination.
The goal is to design a self-organizing system that is capable of learning to encode this com-
plex information in a simpler form.To be more specific,the objective is to extract higher-order
features that exhibit coherence across space in such a way that the representation of infor-
mation in one spatially localized region of the image makes it easy to produce the repre-
sentation of information in neighboring regions;a region refers to a collection of pixels in the
image.The situation described herein pertains to the second scenario illustrated in Fig.10.2b.
We may thus formulate the Imax principle for scenario 2 as follows (Becker, 1996;
Becker and Hinton, 1992):
The transformation of a pair of vectors Xa and Xb(representing adjacent, nonoverlapping
regions of an image by a neural system) should be so chosen that the scalar output Ya of the
F2(Y; S) = I(Y; S) + 
C(Y)
F1(Y; S) = C(Y) - 
I(Y; S)
1 -
I(Y; S)
C(Y)
Section 10.10
Spatially Coherent Features
501

system due to the input Xa maximizes information about the second scalar output Yb due to
Xb, and vice versa.The objective function to be maximized is the mutual information I(Ya; Yb)
between the outputs Ya and Yb.
Although the Imax principle is not equivalent to or derived from the Infomax principle,
it certainly functions in a similar spirit.
EXAMPLE 8
Coherent Image Processing
Consider the example of Fig. 10.6, which shows two neural networks (modules) a and b receiving
respective inputs Xa and Xb from adjacent, nonoverlapping regions of an image.The scalars Ya and
Yb denote the outputs of these two modules due to the respective input vectors Xa and Xb, respec-
tively. Let S denote a random signal component common to both Ya and Yb, which is representa-
tive of the spatial coherence across the two pertinent regions of the original image.We may express
Ya and Yb as noisy versions of the common signal S, as shown by
and
The Na and Nb are additive noise components, assumed to be statistically independent, zero-mean,
Gaussian-distributed random variables. The signal component S is also assumed to be Gaussian
with a distribution of its own. According to these two equations, the two modules a and b in
Fig. 10.6 make consistent assumptions about each other.
Using the last line of Eq. (10.32), we find that the mutual information between Ya and Yb
is defined by
(10.64)
According to the formula of Eq. (10.22) for the differential entropy of a Gaussian random vari-
able, the differential entropy h(Ya) of Ya is given by
(10.65)
h(Ya) = 1
2 [1 + log (22
a)]
I(Ya; Yb) = h(Ya) + h(Yb) - h(Ya, Yb)
Yb = S + Nb
Ya = S + Na
502
Chapter 10
Information-Theoretic Learning Models
Neural
network
a
Maximize
mutual information
I(Ya; Yb)
Region
a
Region
b
Neural
network
b
Ya
Xa
Xb
Yb
FIGURE 10.6
Processing of two neighboring regions of an image in accordance
with the Imax principle.

where 
is the variance of Ya. Similarly, the differential entropy of Yb is given by
(10.66)
where 
is the variance of Yb. As for the joint differential entropy h(Ya, Yb), we use the formula
of Eq. (10.24) with a value of two for the number m of outputs to write
The two-by-two matrix , denoting the covariance matrix of Ya and Yb, is defined by
(10.67)
and
The parameter 
is the correlation coefficient of Ya and Yb; that is,
Hence, we may reformulate the joint differential entropy of Ya and Yb as
(10.68)
Using Eqs. (10.65), (10.66), and (10.68) in Eq. (10.64) and then simplifying terms, we get
(10.69)
From Eq. (10.69), we immediately deduce that maximizing the mutual information I(Ya; Yb) is
equivalent to maximizing the correlation coefficient 
, which is intuitively satisfying. Note that,
by definition,
■
The result described in Eq.(10.69) was derived for the example of two random vari-
ables Ya and Yb produced at the outputs of the stochastic system in Fig. 10.6, both of
which are assumed to be both Gaussian distributed. In the more general case of non-
Gaussian distribution, however, the use of the correlation coefficient &ab does not serve
as an appropriate measure for the Imax principle.To generalize the utility of Imax, we
propose the use of copula, inspired by the formula of Eq. (10.50). To be more specific,
consider the scenario depicted in Fig. 10.2b. Let W denote the weight matrix of the sys-
tem responsible for generating the outputs Ya and Yb in response to the combined
influence of the respective input vectors Xa and Xb. We may then use the first line of
Eq. (10.50) to formulate the Imax principle simply as
(10.70)
max
W I(Ya;Yb) = min
W hC(Ua, Ub ; W)
&ab  1.
&ab
I(Ya ; Yb) = - 1
2 log (1 - &ab
2
a)
h(Ya ; Yb) = 1 + log (2) + 1
2 log [2
a 2
b(1 - &ab
2 )]
&ab =
[(Ya - [Ya]) (Yb - [Yb])]
ab
&ab
det () = 2
a2
b (1 - &ab
2 )
 = c
2
a
&abab
&abab
2
b
d

h(Ya ; Yb) = 1 + log (2) + 1
2 log det()
2
b
h(Yb) = 1
2 [1 + log (22
b)]
2
a
Section 10.10
Spatially Coherent Features
503

where, in terms of the pertinent cumulative probability distributions, we write
and
and hC(Ua, Ub; W) is the joint differential entropy of the random variables Ua and Ub,
whose respective sample values are ua and ub. Equivalently, in light of the second line
of Eq. (10.50), we may also write
(10.71)
where 
is the copula’s joint probability density function of the random
variables Ua and Ub. The formula of Eq. (10.71) includes the result of Eq. (10.69) as a
special case;the importance of this formula will be demonstrated later on in the chapter.
Relationship Between Imax and Canonical Correlation Analysis
Consider again the two input vectors Xa and Xb, which are not necessarily of the same
dimensionality. Let there be two corresponding weight (basis) vectors, denoted by wa and
wb,that are of similar dimensionality to Xa and Xb,respectively.The objective of canonical
correlation analysis (CCA), commonly used in statistics, is to find two linear combinations
and
that have maximum correlation between them. Comparing the problem stated herein with
that of Imax,we readily see that Imax is indeed the nonlinear counterpart of CCA.For a more
detailed treatment of CCA, the reader is referred to Note 8 under Notes and References.
10.11
SPATIALLY INCOHERENT FEATURES
The unsupervised processing of an image considered in the previous section deals with the
extraction of spatially coherent features from an image. We now consider the opposite
scenario.To be specific, consider the third scenario of Fig. 10.2c, where the objective is to
enhance the spatial differences between a pair of corresponding regions derived from two
separate,but correlated,images.Whereas we maximized the mutual information between
the outputs of the modules in Fig. 10.2b, we do the exact opposite in Fig. 10.2c.
We may thus state the Imin principle9 for scenario 3 as follows (Ukrainec and
Haykin, 1992, 1996):
The transformation of a pair of input vectors, Xa and Xb, representing data derived from cor-
responding regions in a pair of separate images, by a neural system should be so chosen that
the scalar output Ya of the system due to the input Xa minimizes information about the sec-
ond scalar output Yb due to Xb and vice versa.The objective function to be minimized is the
mutual information I(Ya; Yb) between the outputs Ya and Yb.
Yb = wT
bXb
Ya = wT
aXa
cUa, Ub(ua, ub; W)
max
W
I(Ya; Yb) = max
W
[log cUa, Ub(ua, ub; W)]
ub = PYb(yb)
ua = PYa(ya)
504
Chapter 10
Information-Theoretic Learning Models

Case Study: Radar Polarimetry
The Imin principle finds application in radar polarimetry, for example,where a surveillance
radar system produces a pair of images of an environment of interest by transmitting on
one polarization and receiving the backscatter from the environment on the same or a
different polarization. The polarization can be vertical or horizontal. For example, we
may have a pair of radar images, one image representing like polarization (horizontal–
horizontal, say), and the other image representing cross-polarization (horizontal on trans-
mit and vertical on receive).Such an application is described in Ukrainec and Haykin (1992,
1996), which pertains to the enhancement of a polarization target in a dual-polarized radar
system. The sample radar scene used in the study is described as follows: An incoherent
radar transmits in a horizontally polarized fashion and receives radar returns on both hori-
zontal and vertical polarization channels. The target of interest is a cooperative, polariza-
tion-twisting reflector designed to rotate the incident polarization through 90 degrees.In the
normal operation of a radar system,the detection of such a target is made difficult by imper-
fections in the system as well as reflections from unwanted polarimetric targets on the ground
(i.e.,radar clutter).We perceive that a nonlinear mapping is needed to account for the non-
Gaussian distribution common to radar returns.The target-enhancement problem is cast as
a variational problem,the objective of which is to minimize a quadratic cost functional with
constraints. The net result is a processed cross-polarized image that exhibits a significant
improvement in target visibility, this improvement is expected to be far more pronounced
than that attainable through the use of a linear technique such as principal-components
analysis.The model used by Ukrainec and Haykin assumes Gaussian statistics for the trans-
formed data, since a model-free estimate of the probability density function is a computa-
tionally challenging task.The mutual information between two Gaussian variables Ya and
Yb is defined by Eq.(10.69).To learn the synaptic weights of the two modules,a variational
approach is taken.The requirement is to suppress the radar clutter that is common to the
horizontally polarized and vertically polarized radar images.To satisfy this requirement,the
mutual information I(Ya; Yb) is minimized, subject to a constraint imposed on the synaptic
weights as shown by
where W is the overall weight matrix of the network and tr[·] is the trace of the enclosed
matrix product.A stationary point is reached when we have
(10.72)
where 
is the gradient operator with respect to the matrix W and 
 is the Lagrange
multiplier.A quasi-Newton optimization routine was used to find the minimum; quasi-
Newton’s methods were discussed in Chapters 3 and 4.
Figure 10.7 shows the architecture of the neural network used in Ukrainec and
Haykin (1992, 1996). A Gaussian radial-basis function (RBF) network was chosen for
each of the two modules because it has the advantage of providing a set of fixed-basis
functions (i.e., a nonadaptive hidden layer). The input data were expanded onto the
basis functions and then combined using layers of linear weights; the dashed lines shown
in Fig. 10.7 represent the cross-coupling connections between the two modules.The cen-
ters of the Gaussian functions were chosen at evenly spaced intervals to cover the entire
§W
§WI(Ya; Yb) + 
§WC = 0
C = (tr[WTW] - 1)2
Section 10.11
Spatially Incoherent Features
505

input domain, and their widths were chosen using a heuristic. Figure 10.8a shows the
raw horizontally polarized and vertically polarized (both on receive) radar images of a
parklike setting on the shore of Lake Ontario. The range coordinate is along the hori-
zontal axis of each image, increasing from left to right; the azimuth coordinate is along
the vertical axis, increasing down the image. Figure 10.8b shows the combined image
obtained by minimizing the mutual information between the horizontally and vertically
polarized radar images, in accordance with the Imin principle. The bright spot clearly
visible in this image corresponds to the radar return from a cooperative, polarization-
twisting reflector placed along the lakeshore. The case study discussed herein demon-
strates the practical benefit of applying the Imin principle to the processing of spatially
incoherent images.10
Generalizations of the Imax and Imin Principles
In formulating the Imax principle in Section 10.10 and the Imin principle in this sec-
tion, we treated the maximization or minimization of the mutual information I(Ya; Yb)
for a pair of output terminals. Both the Imax and Imin principles may be generalized for
any number of terminals, the outputs of which are denoted by Ya, Yb, Yc, ..., by maxi-
mizing or minimizing the multivariate mutual information I(Ya; Yb; Yc; ...), respectively.
506
Chapter 10
Information-Theoretic Learning Models
Ya
Xa
G
Yb
Xb
•••
•••
G
G
G
G
G
Minimize
mutual
information
I(Ya; Yb)
Linear
weights
Gaussian
radial-basis
functions
Cross-polarized
(horizontal–vertical)
radar input
Like-polarized
(horizontal–horizontal)
radar input
FIGURE 10.7
Block diagram of a neural processor, the goal of which is to suppress
background clutter using a pair of polarimetric, noncoherent radar inputs; clutter suppression
is attained by minimizing the mutual information between the outputs of the two modules.

Reflector
FIGURE 10.8
Application of the Imin
principle to radar polarimetry.
(a) Raw B-scan radar images (azimuth
plotted versus range) for horizontal–
horizontal polarization (top) and
horizontal–vertical (bottom)
polarization.
Reflector
(b) Composite image computed by
minimizing the mutual information
between the two polarized radar images
of part (a).
507

10.12
INDEPENDENT-COMPONENTS ANALYSIS
We now turn our attention to the scenario described in Fig. 10.2d. To add more speci-
ficity to the signal-processing problem depicted therein, consider the system of Fig. 10.9.
The system starts operating with a random source vector S defined by
Sample values of the m random variables constituting S are respectively denoted by
s1, s2, ..., sm. The random source vector S is applied to a mixer, whose input–output char-
acterization is defined by a nonsingular matrix A called the mixing matrix.The linear sys-
tem comprised of the source vector S and the mixer A is completely unknown to the
observer.The output of the system is defined by the random vector
(10.73)
where ai is the ith column vector of the mixing matrix A and Si is the random signal
produced by the ith source, i  1, 2, ..., m. The random vector X is correspondingly
denoted by
with a sample value of Xj being denoted by xj, where j  1, 2, ..., m.
The model described in Eq. (10.73) is called a generative model, in the sense that
it is responsible for generating the random variables X1, X2, ..., Xm. Correspondingly, the
random variables S1, S2, ..., Sm, constituting the source vector S, are called the latent
variables, meaning that they cannot be observed directly.
The Blind Source Separation Problem11
The block diagram of Fig.10.9 includes a demixer,described by an m-by-m demixing matrix
W. In response to the observation vector X, the demixer produces an output defined by
the random vector
in light of which we may now make the following statement:
Y = WX
X = [X1, X2, ..., Xm]T
= a
m
i=1
aiSi
X = AS
S = [S1, S2, ..., Sm]T
508
Chapter 10
Information-Theoretic Learning Models
Observation
vector
x
Unknown environment
Source
vector
s
Output
vector
y
Mixer:
A
Demixer:
W
FIGURE 10.9
Block diagram
of the processor for solving the
blind source separation
problem.The vectors s, x, and y
are values of the respective
random vectors S, X, and Y.

Given a set of independent realizations of the observation vector X resulting from an unknown
linear mixing of the latent (source) variables S1, S2, ..., Sm, estimate the demixing matrix W such
that the components of the resulting output vector Y are as statistically independent as possible;
here, the term “independence” should be understood in its strong statistical sense.
This statement describes the essence of the blind source separating problem. The prob-
lem is said to be blind to signify the fact that the estimation of the demixing matrix W
is carried out in an unsupervised manner.Moreover,the only information used to recover
the original source vector S is contained in the observation vector X.The underlying prin-
ciple involved in solving the blind source separation (BSS) problem is called independent-
components analysis (Comon, 1994). Independent-components analysis (ICA) may be
viewed as an extension of principal-components analysis (PCA),with the following basic
difference between them:Whereas PCA can only impose statistical independence up to
the second order while constraining the direction vectors to be orthogonal, ICA imposes
statistical independence on all the individual components of the output vector Y and
involves no orthogonality constraint.
Basic Assumptions
To simplify the study of principal-components analysis, the following four basic assump-
tions are made:
1. Statistical independence. The latent variables constituting the source vector S are
assumed to be statistically independent, Note, however, that since the observation
vector X is made up of a linear combination of the latent variables, the individual
components of the observation vector X are statistically dependent on each other.
2. Dimensionalities of the Mixing Matrix. The mixing matrix is a square matrix,which
means that the number of observations is the same as the number of sources.
3. Noise-free model. The generative model is assumed to be noise free, which means
that the only source of stochasicity in the model is the source vector S.
4. Zero mean. It is assumed that the source vector S has zero mean, which, in turn,
implies that the observation vector X has zero mean too. If not, then the mean
vector 
is subtracted from X to make it assume a zero-mean value.
One other assumption is sometimes invoked:
5. Whitening. It is also assumed that the observation vector X has been “whitened,”
which means that its individual components are uncorrelated, but not necessarily
independent.Whitening is achieved by linearly transforming the observation vec-
tor so that the correlation matrix 
is equal to the identity matrix.
It is also important to recognize that the solution of a BSS problem is feasible except
for an arbitrary scaling of the estimate of each source output (i.e., latent variable) and per-
mutation of indices.To elaborate,it is possible to find a demixing matrix W whose individual
rows are a rescaling and permutation of the mixing matrix A. In other words,the solution
to the BSS problem produced by the ICA algorithm may be expressed in the form
y = Wx = WAs = DPs
[XXT]
[X]
Section 10.12
Independent-Components Analysis
509

where D is a nonsingular diagonal matrix and P is a permutation matrix; s, x, and y are
respective realizations of the random vectors S, X, and Y.
Non-Gaussianity of Sources: A Necessary Requirement for ICA,
Except Possibly for One Source
For an ICA algorithm to be capable of separating a given set of source signals as inde-
pendently as possible at the demixer output, it is a requirement that there be sufficient
information in the observable vector X produced at the output of the generative model.
The key question is as follows:
How should the information content in the observation vector X manifest itself for the sepa-
rability of source signals to be feasible?
We will address this basic question by way of a simple, yet insightful, example.
EXAMPLE 9
Two Different Characterizations of a Pair
of Independent Sources
Consider a generative model involving a pair of independent random source signals S1 and S2,
both of which have zero mean and unit variance. The mixing matrix is defined by the nonsin-
gular matrix
The example is in two parts: In the first part, both sources are Gaussian distributed; in the
second part of the example, one source is Gaussian distributed and the other is uniformly
distributed.
From probability theory, we know the following two properties of Gaussian distributions
(Bertsekas and Tsitsiklis, 2002):
1. The higher-order moments of a zero-mean Gaussian random variable are all even and uniquely
defined by the variance (i.e., the second-order moment for the special case of zero mean).
2. The sum of two linearly scaled (weighted) Gaussian random variables is also Gaussian.
It follows therefore that when the source signals S1 and S2 are both Gaussian with zero mean, the
observables X1 and X2 are also both Gaussian with zero mean. Moreover, for the prescribed mix-
ing matrix, X1 has a variance
and X2 has the variance
where
 1 and 
 16.
Part (a) of Fig. 10.10 plots the histograms of the source signals S1 and S2, while part (b) of
the figure plots the corresponding two-dimensional distribution of the observables X1 and X2.
Examining Fig.10.10b,we find that the two-dimensional distribution is symmetric about the origin,
and its information content is insufficient to discriminate between the individual directions of the
original source signals S1 and S2.
2
2
2
1
(1)22
1 + (2)22
2 = 65,
(1)22
1 + (-1)22
2 = 17,
  A = c1
-1
1
2 d
510
Chapter 10
Information-Theoretic Learning Models

Section 10.12
Independent-Components Analysis
511
0
50
100
150
200
250
15
10
5
0
5
10
15
0
50
100
150
200
250
15
10
5
0
5
10
15
(a) Histograms of the two processes: The top histogram refers to Gaussian signal source
S1 of zero mean and veriance 1
2 = 1; the bottom one refers to Gaussian source signal S2 of
zero mean and veriance 2 = 16.
2
(b) Two-dimensional distribution of the linearly mixed signals X1 and X2.
x1
x2
30
20
10
0
10
20
30
15
10
5
0
5
10
15
FIGURE 10.10
Two Gaussian distributed processes.

Consider next the case when the source S1 is Gaussian distributed with zero mean and unit
variance, and the source S2 is uniformly distributed over the interval [2, 2]. Part (a) of Fig. 10.11
plots the histograms of S1 and S2,and part (b) of the figure plots the corresponding two-dimensional
distribution of the observables X1 and X2. Like the first case depicted in Fig. 10.10b, the two-
dimensional distribution of Fig. 10.11b is symmetric about the origin. However, a deeper examina-
tion of the distribution in Fig. 10.11b reveals two distinctive points:
1. The Gaussian-distributed source signal S1 (with infinite support) manifests itself along a
positive direction whose slope equals 1.
2. The uniformly distributed source signal S2 (with finite support) manifests itself along a neg-
ative direction whose slope equals 2.
Moreover, these two slopes do relate to the elemental values of the mixing matrix.
The conclusion to be drawn from the second case is that the two-dimensional distribution
of the observables X1 and X2 contains sufficient directional information about the original source
signals S1 and S2 for them to be linearly separable. This highly desirable condition arises only
when a single signal source is permitted to have a Gaussian distribution.
■
On the basis of the results of this example, we may now go on to answer the basic
question we raised on the feasible separability of source signals at the demixer output:
1. The observables X1,X1,...,Xm must have higher-order moments that are unrelated to
their respective second-order moments. Accordingly, the source signals S1, S1, ..., Sm
must be non-Gaussian.
2. At the very most, only a single source is permitted to have a Gaussian distribution.
To summarize, the necessary conditions for source separability are that the sources
be non-Gaussian and the mixing matrix be nonsinguar, both of which conditions must
be satisfied by the generative model. In particular, we may make the following succint
statement (Cardoso, 2003):
Independent components analysis (ICA) is the decomposition of a random vector into lin-
ear components that are as statistically independent as possible, where the term “indepen-
dence” is understood in its strongest statistical sense; ICA goes beyond (second-order)
decorrelation and therefore requires that the observations representing the data vector be
non-Gaussian.
Classification of ICA Algorithms
Now that we have established the necessary conditions for the separability of linearly
mixed source signals, we may go on to identify two broadly defined families of ICA
algorithms:
1. ICA Algorithms rooted in minimization of mutual information
Minimization of the mutual information between the demixer outputs in the block
diagram of Fig. 10.9 provides a natural basis for the design of ICA algorithms.This
first family of ICA algorithms includes the following:
1.1 The algorithm developed by Amari et al. (1996), which is based on the
Kullback–Leibler divergence.This algorithm is described in Section 10.14.
512
Chapter 10
Information-Theoretic Learning Models

Section 10.12
Independent-Components Analysis
513
(b) Two-dimensional distribution of the linearly mixed signals X1 and X2.
8
6
4
2
0
2
4
6
8
5
0
x1
5
x2
200
150
100
50
0
4
3
2
1
0
1
2
3
4
200
150
100
50
0
4
3
2
1
0
1
2
3
4
(a) Histograms of the two processes: The top histogram refers to Gaussian source signal
S1 of zero mean and veriance 1
2 ; the bottom one refers to uniformly distributed source
signal S2 uniformly distributed over the interval [2, 2].
FIGURE 10.11
Gaussian- and uniformly-distributed processes.

1.2 The algorithm developed by Pham et al. (1992), which is based on maximum-
likelihood estimation. This algorithm resides on the fringe of the Bayesian
paradigm in that it ignores prior information. It is described in Section 10.15.
1.3 The so-called Infomax algorithm, developed by Bell and Sejnowski (1995),
which is based on the maximum-entropy principle.This algorithm is described
in Section 10.16. In Cardoso (1997), it is shown that the Infomax algorithm is
equivalent to the maximum-likelihood-estimation algorithm.
In reality, although these ICA algorithms are different in their formulations, they
are all basically variants of the principle of minimizing mutual information.
2. ICA Algorithms rooted in maximization of non-Gaussianity
This second family of algorithms includes the fastICA algorithm (Hyvärinen and
Oja, 1997), which uses negentropy as the measure for non-Gaussianity. Moreover,
this algorithm not only sits in a class of its own, but also is computationally faster
compared with the other ICA algorithms. The fastICA algorithm is described in
Section 10.18.
Before discussing the aforementioned ICA algorithms, we will explore the signal-
processing power of ICA by considering natural images, as described next.
10.13
SPARSE CODING OF NATURAL IMAGES AND COMPARISON
WITH ICA CODING
In Chapter 8, we emphasized the importance of higher-order statistics of natural images
and the impact of those statistics on image modeling. In this section, we emphasize
another important characteristic of natural images—namely, sparseness—and the role
of ICA in capturing it. In so doing, we provide motivation for the practical importance
of ICA.
In Section 10.9, we discussed how the principle of minimum redundancy may be
applied to model visual systems (Atick and Redlich,1990).In Dong and Atick (1995) and
Dan et al. (1996), the application of this principle was extended to see how the proper-
ties of retinal ganglion cells in the visual system may be explained by whitening, or
decorrelating, a set of outputs produced by these cells in response to the 1/f amplitude
power spectra of natural images. Subsequently, Olshausen and Field (1997) pointed out
a basic limitation of the model studied by Atick and collaborators: The reduction of
redundancy considered therein was limited to linear pairwise correlations among the pix-
els of a natural image;these correlations can be captured by principal-components analy-
sis. In reality, however, natural images exhibit higher-order correlations due to oriented
lines and edges—especially those of the curved variety—all of which are prevalent in nat-
ural images.
In Olshausen and Field (1997) a probabilistic model is described for capturing the
higher-order correlation structure of a natural image. Most importantly, the model is
described in terms of a linear superposition of basis functions, as illustrated by
(10.74)
I(x) = a
i
ai%i(x)
514
Chapter 10
Information-Theoretic Learning Models

where the vector denotes a discrete spatial position within the two-dimensional images
the 
denote the basis functions, and the ai denote the mixing amplitudes.The
computed values of the ai constitute the output of the coding strategy.Moreover,the basis
functions were chosen to be adaptive, so as to account for the underlying structure of the
image in terms of a collection of statistically independent events in the best manner
possible.Thus, building on some prior work by Field (1994), Olshausen and Field (1997)
made the following conjecture:
Sparseness is an appropriate prior for the mixing amplitudes ai in Eq. (10.74), which is based
on the intuition that natural images may be described in terms of a relatively small number
of structural primitives exemplified by edges, lines, and other elementary features.
To validate this conjecture, Olshausen and Field carried out the following two tasks:
1. Formulation of a sparse-coding algorithm, aimed at maximization of spareseness
that is rooted in image processing and information theory. The algorithm was
designed to learn a set of basis functions for the image model based on Eq. (10.74)
that would best account for natural images in terms of sparse, statistically inde-
pendent components; the sparse-coding algorithm was shown to minimize the same
objective function as ICA, but by making an approximation due to the intractabil-
ity introduced by overcomplete representations.
2. Generation of data, taken from ten 512-by-512 pixel images of natural surround-
ings (trees,rocks,mountains,and so on);the data were applied to train the algorithm.
A stable solution computed by the sparse-coding algorithm was usually obtained after
approximately 2,000 updates (i.e., approximately 200,00 image presentations).The result
of the training process is shown in Fig. 10.12, in which the vast majority of basis functions
have become localized within individual pixels.
In an independent study, Bell and Sejnowski (1997) applied ICA to four natural
scenes involving trees, leaves, and so on, which were converted to greyscale byte values
ranging between 0 and 255.The Infomax algorithm for ICA, to be described in Section
10.16, was used in the study.The resulting solution is shown in Fig. 10.13.
Comparing the solution of Fig. 10.12, using the sparse-coding algorithm, and that
of Fig. 10.13, using the Infomax algorithm for ICA, it is remarkable to see how simi-
lar these two solutions are. The similarity is all the more remarkable when we recog-
nize that entirely different natural image data were used to train the two algorithms
independently.
The results of these two completely independent studies teach us the following
two important lessons:
1. Natural images are inherently sparse, in the sense that they may be described by
a relatively small number of distinctive structural primitives, examples of which
include edges and lines.
2. By its very essence, an algorithm for independent-components analysis has a built-
in capability of capturing these structural primitives.
So, indeed, the results presented in Figs. 10.12 ands 10.13 do provide the motivation to
study learning algorithms for ICA, which we do in the next four sections.
%i(x)
I(x),
x
Section 10.13
Sparse Coding of Natural Images and Comparison with ICA Coding
515

10.14
NATURAL-GRADIENT LEARNING 
FOR INDEPENDENT-COMPONENTS ANALYSIS
Consider the input–output relationship
(10.75)
where the random vector X denotes the observables (i.e., the demixer input), W denotes
the demixing matrix, and the random vector Y denotes the resulting response (i.e., the
demixer output). With statistical independence among the individual components of
the output Y as the desired property for blind source separation, what is a practical
Y = WX
516
Chapter 10
Information-Theoretic Learning Models
FIGURE 10.12
The result of applying the sparse-coding algorithm to a natural image. (The
figure is reproduced with the permission of Dr. Bruno Olshausen.)

Section 10.14
Natural-Gradient Learning for Independent-Components Analysis
517
FIGURE 10.13
The result of applying the Infomax algorithm for ICA to another natural
image. (The figure is reproduced with the permission of Dr.Anthony Bell.)
measure that we can use to achieve that property? To prepare ourselves for the answer
to this fundamental question, let pY(y, W) denote the probability density function of
the output Y, parameterized by the demixing matrix W, and let the corresponding
factorial distribution be defined by
(10.76)
where 
is the marginal probability density function of the random variable Yi (i.e.,
the ith component of Y); for obvious reasons, the factorial distribution 
is not
parameterized.In effect,Eq.(10.76) may be viewed as a constraint imposed on the learning
p~
Y(y)
p~
Yi(yi)
p~
Y(y) = q
m
i=1
p~
Yi(yi)

rule (to be developed), forcing it to contrast pY(y, W) against the factorial distribution
, which, ideally, should match that of the original sources. Focusing on the distrib-
utions pY(y, W) and 
as the only two distributions at our disposal, we may now
state the answer to our question, embodied in the principle of independent-components
analysis (ICA) which states (Comon, 1994),
Given an m-by-1 random vector X representing a linear combination of m independent source
signals, the transformation of the observation vector X into a new random vector Y should
be carried out in such a way that the Kullback–Leibler divergence between the parameterized
probability density function pY(y, W) and the corresponding factorial distribution
is
minimized with respect to the unknown parameter matrix W.
From this statement, it is clear that the Kullback–Leibler divergence is a natural
basis for the expected contrast function,the formulation of which constitutes the very first
step in deriving a learning algorithm for ICA. With the demixing matrix W being the
unknown parameter in ICA,the expected contrast function is a function of W.Hereafter,
we use R(W) to denote that contrast function, which, in light of the formula for the
Kullback–Leibler divergence presented in the first line of Eq. (10.39), may now be for-
mally defined as follows:
(10.77)
What is truly remarkable about this formula is the fact that it appears to be the
inspirational framework used to derive many of the learning algorithms proposed in
the literature for ICA and blind source separation (Cichocki and Amari, 2002).
Following the discussion of the Kullback–Leibler divergence presented in Section 10.5,
we may reformulate the expected contrast function R(W) in terms of the two entropies
at our disposal, as shown by
(10.78)
where h(Y) is the entropy of the random vector Y at the output of the demixer and
is the marginal entropy of the ith element of Y. R(W) is the objective function to
be minimized with respect to W.
Determination of the Differential Entropy h(Y)
The output vector Y is related to the input vector X by Eq. (10.75), where W is the
demixing matrix. In light of Eq. (10.18), we may express the differential entropy of Y as
(10.79)
where h(X) is the differential entropy of X and det(W) is the determinant of W. Using
this expression in Eq. (10.77), we may reformulate the expected contrast function one
more time, as
= h(X) + log|det(W)|
h(Y) = h(WX)
h(Yi)
R(W) = - h(Y) + a
m
i=1
h(Yi)
R(W) =
3
q
-q
pY(y, W)log °
pY(y, W)
q
m
i=1
pYi(yi)
¢ dy
pY(y)
pY(y)
pY(y)
518
Chapter 10
Information-Theoretic Learning Models

(10.80)
where, for the rightmost term in the second line of the equation, we used Eq. (10.10) and
the expectation in that term is with respect to Yi. Note that the entropy h(X) is inde-
pendent of the demixing matrix W; henceforth, we are justified to ignore this term in
deriving  the learning algorithm for ICA.
Derivation of Stochastic Gradient Algorithm for ICA
With stochastic gradient descent in mind,it is the usual practice to ignore the expectation
operator 
and focus attention on instantaneous values only. For the problem at hand,

= - h(X) - logdet(W) - a
m
i=1
[log p
Yi(yi)]
R(W) = - h(X) - logdet(W) + a
m
i=1
h~(Yi)
Section 10.14
Natural-Gradient Learning for Independent-Components Analysis
519
there is only one instantaneous value to be considered, namely,
Let &(W)
denote the instantaneous value of the expected contrast function R(W),which,henceforth,
we refer to simply as the contrast function; that is to say,
Thus, ignoring the entropy h(X), we may use Eq. (10.80) to write
(10.81)
The stochastic gradient matrix is defined by
(10.82)
where 
is the gradient operator with respect to the demixing matrix W.The two con-
tributions to this gradient matrix are considered separately:
1. The first contribution is defined by
(10.83)
where 
is the transpose of the inverse matrix 
2. The ith component of the second contribution to the stochastic gradient matrix is
defined by
(10.84)
where wi is the ith column vector of the demixing matrix W and yi is a sample
value of the output vector Yi.Thus, taking sample values of the ith components in
Eq. (10.75), we have
(10.85)
yi = wT
i x, i = 1, 2, ..., m
0
0wi
 log pYi(yi) = 0yi
0wi
0
0yi
 log pYi(yi)
W -1.
W-T
0
0W logdet(W) = W-T
§
§&(W) = -
0
0W logdet(W) -
0
0W a
m
i=1
log p~
Yi(yi)
&(W) = -
 logdet(W) - a
m
i=1
log p~
Yi(yi)
R(W) = [&(W)]
log pYi(yi).

where x is a sample value of the input vector X and 
is a sample value of 
Differentiating Eq. (10.85) with respect to wi, we obtain
(10.86)
Moreover,
(10.87)
where the partial derivative
At this point in the discussion, we find it convenient to introduce the activation
function
for the ith neuron used in constructing the demixer; specifically, we
define
(10.88)
Accordingly, using Eqs. (10.85) to (10.88), we may write
(10.89)
From this expression, we may express the contribution of the summation term in
Eq. (10.82) to the stochastic gradient matrix as
(10.90)
where the vector of activation functions, expressed as a function of the output
vector y, is
Next, substituting Eqs. (10.83) and (10.90) into Eq.(10.82), we get the desired stochas-
tic gradient matrix:
(10.91)
Now, let 
denote the learning-rate parameter, assumed to be a small positive con-
stant.Then, given the gradient matrix of Eq. (10.91), the incremental adjustment made
to the demixing matrix W is
(10.92)
= CW-T - 	(y)xTD
¢W = -§&(W)

§&(W) = - W-T + 	(y)xT
	(y) = [1(y1), 2(y2), ..., m(ym)]T
0
0W a
m
i=1
logpYi(yi) = -	(y)xT = - x	T(y)
0
0wi
 log pYi(yi) = - i(yi)x,  i = 1, 2, ..., m
i(yi) = -
p¿Yi(yi)
pYi(yi), i = 1, 2, ..., m

p¿Yi(yi) =
0
0yi
pYi(yi)
=
p¿Yi(yi)
pYi(yi)
0
0yi
 log pYi(yi) =
1
pYi(yi)
0
0yi
pYi(yi)
0yi
0wi
= x
Yi .
yi
520
Chapter 10
Information-Theoretic Learning Models

For reasons that will become apparent momentarily, we find it convenient to reformu-
late Eq. (10.92) by first taking the transpose of Eq. (10.85), which yields
Hence, we may rewrite Eq. (10.92) in the new, equivalent form
(10.93)
where I is the identity matrix. Correspondingly, the on-line learning rule for adapting the
demixing matrix takes the form
(10.94)
where the parameters are all shown in their time-varying forms.
An undesirable property of this algorithm is postmultiplication of the adjustment
term by the inverse of the transposed weight matrix W. Our next task is to find a method
for eliminating the computation of the inverse.
Equivariant Property
The purpose of the ICA algorithm is to update the demixing matrix W(n) such that the
output vector
is as close as possible to the original source vector s(n) in some statistical sense. To be
more specific, consider a global system characterized by the system matrix C(n) that is
obtained by multiplying the mixing matrix A and demixing matrix W(n); that is,
(10.95)
Ideally, this global system satisfies two conditions:
1. The algorithm responsible for adjusting C(n) converges to an optimum value equal
to the permutation matrix.(Note,however,that a signed permutation matrix,which
has 1 or 1 in each row and column only once, is also optimal.)
2. The algorithm is itself described by
(10.96)
where G(C(n)s(n)) is a matrix-valued function of the matrix product C(n)s(n).The
performance of the algorithm is completely characterized by the system matrix
C(n), not by the individual values of the mixing matrix A and demixing matrix
W(n).Such an adaptive system is said to be equivariant (Cardoso and Laheld,1996).
The on-line learning algorithm of Eq. (10.94) is certainly capable of satisfying the
first condition approximately. However, as it stands, it cannot satisfy the second condi-
tion.To see that this is so, we multiply Eq. (10.94) by the mixing matrix A and then use
Eq. (10.95) to write
(10.97)
C(n + 1) = C(n) + (n)G(C(n)s(n))W-T(n)A
C(n + 1) = C(n) + (n)G(C(n)s(n))C(n)
C(n) = W(n)A
y(n) = W(n)x(n) = W(n)As(n)
W(n + 1) = W(n) + (n)[I - 	(y(n))yT(n)]W-T(n)
= [I - 	(y)yTW-T
W = [I - 	(y)xTWT]W-T
yT = xTWT
Section 10.14
Natural-Gradient Learning for Independent-Components Analysis
521
521
v
Correction term

where
(10.98)
Clearly, the algorithm of Eq. (10.94) falls short of the equivariant condition described
in Eq. (10.96) in that the matrix-valued function G(C(n)s(n)) is postmultiplied by
WT(n)A,which,in general,is different from C(n).To correct this situation,we interpose
the matrix product WT(n)W(n) between the function G(C(n)s(n)) and the matrix prod-
uct WT(n)A in Eq. (10.97).The term WTW, being made up of the product of matrix W
and its transpose, is always positive definite. This is the reason that multiplication by
WTW does not change the sign of the minima of the learning algorithm.
The important question is:What is the implication of this modification that is made
in order to achieve the equivariant condition? The answer lies in how the gradient
descent in parameter space is formulated. Ideally, we should use the natural gradient12
of the contrast function 
, defined in terms of the usual gradient 
as
(10.99)
The ordinary gradient matrix is itself defined by Eq. (10.91). In an implicit sense, the
gradient 
is the optimum direction for descent only when the parameter space
is Euclidean with an orthonormal coordinate system. In a typical situation
involving neural networks, however, the parameter space 
has a coordinate system
that is not orthonormal.The natural gradient 
will provide the steepest descent
in this latter situation—hence the preference for using it instead of the usual gradient
in formulating the stochastic gradient algorithm for ICA. For the natural gradient space
to be definable, two conditions must be satisfied:
1. The parameter space 
is Riemannian.13The Riemannian structure is a differentiable
manifold. (The notion of a differentiable manifold was discussed in Chapter 7.)
2. The matrix W is nonsingular (i.e., invertible).
Both of these conditions are satisfied for the problem at hand.
Accordingly, the stage is set for us to modify the algorithm of Eq. (10.94) in the
manner just described, permitting us to write
Finally, recognizing that the matrix product WT(n) WT(n) equals the identity matrix,
we finally write
(10.100)
which leads to blind source separation with the desired equivariant property. Since the
derivation of the on-line learning algorithm of Eq. (10.100) is based on the natural
gradient, this algorithm is commonly referred to in the literature as the natural-gradient
learning algorithm for independent-components analysis (Cichocki and Amari, 2002).
Obviously,a complete picture of this algorithm must also include the matrix representation
W(n + 1) = W(n) + (n)[I - 	(y(n))yT(n)]W(n)
W(n + 1) = W(n) + (n)[I - 	(y(n))yT(n)]W(n)(WT(n)W-T(n))
w
*&(W)
w
w  {W}
&(W)
*&(W) = (&(W))WTW
&(W)
&(W)
G(C(n)s(n)) = I - 	(C(n)s(n))(C(n)s(n))T
522
Chapter 10
Information-Theoretic Learning Models

of the input-output relationship of Eq. (10.85) over the complete set of outputs:
This complete input-output picture of the algorithm is depicted in the signal-flow graph
of Fig. 10.14.
Important Virtues of the Natural-Gradient Learning Algorithm
In addition to possessing the equivariant property, the natural-gradient learning algo-
rithm, described in Eq. (10.100), has four important virtues:
1. The algorithm is computationally efficient, since it avoids the need for inverting
the demixing matrix W.
2. The convergence rate of the algorithm is relatively fast.
3. The algorithm lends itself to implementation in the form of an adaptive neural system.
4. Being a stochastic gradient algorithm, the algorithm has a built-in capability to
track statistical variations of a nonstationary environment.
Robustness of ICA Theory
The natural-gradient learning algorithm of Eq. (10.100) requires knowledge of the acti-
vation function 
defined in Eq. (10.88), which shows that 
is dependent on the
marginal distribution 
. Accordingly, for the algorithm to provide a satisfactory
pY(y)
(y)
(y)
 = Wx
y = {yi}i=1
M
Section 10.14
Natural-Gradient Learning for Independent-Components Analysis
523
523
I
Output vector
y(n)
Identity matrix
( )
Σ
h(n)
Input vector
x(n)
W(n
1)
W(n)
Σ
z
1I
FIGURE 10.14
Signal-flow
graph of the blind source
seperation learning
algorithm described in 
Eqs. (10.85) and (10.104):
The block labeled 
represents a bank of uni-time
delays.The graph embodies
a multiplicity of feedback
loops.
z-1I

solution to the blind source separation problem, any mathematical description of the
marginal distribution 
must be close to the true distribution of the original inde-
pendent component (i.e., source); otherwise, we have a serious model mismatch.
In practice,however,we find that it is sufficient to consider only two possible approx-
imations of the underlying probability distribution of each independent component:14
1. Super-Gaussian distribution. This distribution has a form similar to the Laplacian
distribution, defined by
which decays exponentially with respect to the absolute value 
at a rate . For
example, samples of the amplitude of a speech signal tend to follow the Laplacian
distribution.
2. Sub-Gaussian distribution. This second distribution is like a log-Gaussian distrib-
ution that is somewhat flattened around the origin.
The preceding statement on approximations is testimony to the robustness of ICA theory:
(i) Simple models of the underlying distributions are adequate for estimating the inde-
pendent components.
(ii) Small modeling errors are tolerated in testing the super-Gaussian and sub-Gaussian
approximations for each independent component.
More specifically, the robustness of ICA theory is substantiated by the following
important theorem (Hyvärinen et al., 2001):
Let 
denote the assumed probability density function of the ith independent com-
ponent (source signal) represented by the demixer output yi. Define the activation
function
Suppose that estimates of the independent components 
are constrained to be
uncorrelated with each other and the random variables Yi have unit variance for all i.
Then the natural-gradient estimator of the independent components is locally consistent,
provided that the assumed distributions satisfy the condition
for all i
(10.101)
where
This theorem, hereafter referred to as the ICA robustness theorem (Hyvärinen et al.,
2001),shows rigorously that,so long as the sign of the inequality condition of Eq.(10.101)
¿(yi) =
0
0yi
(yi)
[yi(yi) - ¿(yi)] 7 0
{yi}m
i=1
= -
p¿
Yi(yi)
pYi(yi)
,  p¿Yi(yi) =
0
0yi
pYi(yi)
(yi) = - 0
0yi
log pYi(yi)
pYi(yi)
y
pY(y) = 
2 exp(-y), -q 6  6 q
pY(y)
524
Chapter 10
Information-Theoretic Learning Models

remains unchanged for all i, small discrepancies in the approximate distributions
do not affect the local consistency of estimates of the independent components computed
with the use of the natural-gradient learning algorithm.15
The ICA robustness theorem described for natural-gradient learning applies
equally well to the maximum-likelihood estimation procedure described in the next
section. Moreover, the ICA robustness theorem teaches us how to construct a fam-
ily of functions based on the inequality of Eq. (10.101), with each pair in the family
consisting of the log-probability density function pertaining to a super-Gaussian dis-
tribution and its sub-Gaussian counterpart. In practice, we therefore have a simple
binary choice between two candidate distributions.The next example illustrates such
a choice.
EXAMPLE 10
Super-Gaussian and Sub-Gaussian Functions
Consider the pair of log-density functions
where 
and 
are positive constants included to make sure that each function satisfies the basic
properties of a probability density function. The plus and minus superscripts are intended to
signify whether the function in question refers to a super-Gaussian or sub-Gaussian probability
density function, respectively.
Applying the formula of Eq. (10.88) for the activation function to 
we get the hyper-
bolic tangent function
where we have ignored the multiplying factor 2 for mathematical convenience. Differentiating
this result with respect to y one more time, we get the gradient of the activation function:
Hence, for the super-Gaussian function, the left-hand side of Eq. (10.101) yields the result (except
for the scaling factor 2)
Performing the same two operations for 
, we get
Hence, for the sub-Gaussian function, the left-hand side of Eq. (10.101) yields
where we have invoked the assumption that the variance of the zero-mean random variable Y
(represented by sample value y) is unity—that is,
Examining the results just obtained for the pair of super-Gaussian and sub-Gaussian func-
tions, we see that they indeed have opposite algebraic signs.Accordingly, only one of them satisfies
the inequality of Eq. (10.101); the particular activation function that satisfies this inequality for the
[Y2] = 1.
[y2 - y tanh(y) -1 + sech2(y)] = [-y tanh(y) + sech2(y)]
 - œ(y) = 1 - sech2(y)
 -(y) = y - tanh(y)
p-
Y(y)
[y tanh(y) - sech2(y)]
 +¿ = sech2(y)
 +(y) = tanh(y)
p+
Y(y),
2
1
 log p-
Y(y) = 2 - a 1
2 y2 - log cosh(y) b
 log p+
Y(y) = 1 - 2 log cosh(y)
pYi(yi)
Section 10.14
Natural-Gradient Learning for Independent-Components Analysis
525
525

dataset under the ICA study is the one that should be used for the class of algorithms rooted in the
principle of independent-components analysis (e.g., the natural-gradient learning algorithm).
■
10.15
MAXIMUM-LIKELIHOOD ESTIMATION 
FOR INDEPENDENT-COMPONENTS ANALYSIS
The principle of independent-components analysis described in the previous section is
just one of many methods that have been proposed in the literature for blind source
separation.In the context of this principle,however,there are two other methods for per-
forming the task of source separation in an unsupervised manner: maximum likelihood
and maximum entropy. In this section, we discuss maximum likelihood, followed by
maximum entropy in the next section.
Maximum likelihood is a well-established procedure for statistical estimation with
some nice properties.16 In this procedure, we first formulate a log-likelihood function
and then optimize it with respect to the parameter vector of the probabilistic model
under consideration. From the discussion presented in Chapter 2, we recall that the like-
lihood function is the probability density function of a data set in a given model, but
viewed as a function of the unknown parameters of the model. Referring to Fig. 10.9, let
pS(s) denote the probability density function of the random source vector S whose sam-
ple value is s. Then the probability density function of the observation vector X  AS
at the output of the mixer is defined by 
(10.102)
where det(A) is the determinant of the mixing matrix A. Let 
denote a
training sample consisting of N independent realizations of the random vector X.
We may then write
(10.103)
We find it more convenient to work with the normalized (divided by the sample size N)
version of the log-likelihood function, as shown by
Let y  A1x be a realization of the random vector Y at the demixer output, and thus
we write
(10.104)
Let A1  W and let pY(y, W) denote the probability density function of Y parameter-
ized by W. Then, recognizing that the summation in Eq. (10.104) is the sample average
of log pS(yk) and invoking the law of large numbers that, with probability 1, as the sam-
ple size N approaches infinity, we may introduce the function
1
N log pX(t, A) = 1
N a
N
k=1
log pS(yk) - logdet(A)
= 1
N a
N
k=1
 log pS(A-1xk) - logdet(A)
1
N log pX(t, A) = 1
N a
N
k=1
 log pX(xk, A)
pX(t, A) = q
N
k=1
pX(xk, A)
t = {xk}N
k=1
pX(x, A) = det(A) -1 pS(A-1x)
526
Chapter 10
Information-Theoretic Learning Models

(10.105)
where the expectation in the second line is with respect to Y. The function L(W) is the
desired log-likelihood function. By writing
we may express L(W) in the equivalent form
(10.106)
where we have used the following definitions:
• the expected contrast function 
with the same formula as the Kullback-Leibler
divergence, as defined in Eq. (10.77);
• the differential entropy 
as defined in the first line of Eq. (10.12)
Next, using the formula of Eq. (10.78), we finally rewrite Eq. (10.79), in the desired form:
(10.107)
where h(X) is the differential entropy of the random vector X at the demixer input
(Cardoso, 1998a). The only quantity in Eq. (10.107) that depends on the weight vector
W of the demixer is the expected contrast function R(W).We therefore conclude from
Eq. (10.107) that maximizing the log-likelihood function L(W) is equivalent to mini-
mizing R(W), which requires matching the probability distribution of the demixer out-
put Y to that of the original source vector S.
Relationship between Maximum-Likelihood Estimation 
and the Principle of Independent-Components Analysis
Applying the Pythagorean decomposition described in Eq. (10.43) to the problem at
hand,we may express the expected contrast function for maximum likelihood as follows:
(10.108)
The first Kullback–Leibler divergence,
, on the right-hand side of Eq. (10.108) is
a measure of structural mismatch that characterizes the method of independent-com-
ponents analysis. The second Kullback–Leibler divergence,
, is a measure of the
marginal mismatch between the marginal distribution of the demixer output Y and the
Dp~
Y7pS
DpY7p~
Y
R(W) = DpY7p~
Y + Dp~
Y7pS
L(W) = -R(W) - h(X)
h(Y,w)
R(w)
= -R(W) - h(Y, W) + logdet(W)
+
3
q
-q
pY(y, W)log pY(y, W)dy + log ƒ det(W) ƒ
L(W) =
3
q
-q
pY(y, W)log a
pS(y)
pY(y, W) bdy
pS(y) = a
pS(y)
pY(y, W) bpY(y, W)
=
3
q
-q
pY(y, W)log pS(y)dy + logdet(W)
= [log pS(y)] + logdet(W)
L(W) = lim
NS q
1
N a
N
k=1
log pS(yk) + logdet(W)
Section 10.15
Maximum-Likelihood Estimation for Independent-Components
527

distribution of the original source vector S.In words,we may express the “global”distribution-
matching criterion for maximum likelihood as follows:
(10.109)
Insofar as the right-hand side of Eq. (10.109)  is concerned,“structural mismatch” refers
to the structure of a distribution pertaining to the set of independent variables, whereas
“marginal mismatch”refers to the mismatch between the individual marginal distributions.
Under the ideal condition W  A1 (i.e., perfect blind source separation), both the
structural mismatch and marginal mismatch vanish.At that point,the maximum-likelihood
method and the principle of independent-components analysis yield exactly the same solu-
tion.The idealized relationship between the maximum-likelihood method and the princi-
ple of independent-components analysis is depicted in Fig.10.15.In this figure,
is the set
s
DpY7 pS
DpY7 p
Y
a
Total
mismatch b = a Structural
mismatch b + a Marginal
mismatch b
528
Chapter 10
Information-Theoretic Learning Models
  {pY(y, W)}
Parameterized
demixer output
distribution
pY(y, W)
pS(s): True source
distribution
: Set of all the
independent
distributions
	:  The whole set of
probability distributions
{pY(y)}
W
pY(y): Marginal 
distribution of 
demixer output
~
FIGURE 10.15
Illustration of the relationship between the maximum-likelihood method
and independent-components analysis for blind source separation.The maximum-likelihood
method minimizes 
whereas independent-components analysis minimizes 
.
DpY7 p
Y
DpY7 pS
v
v

Section 10.16
Maximum-Entropy Learning For Blind Source Separation
529
FIGURE 10.16
Block diagram of the maximum-entropy principle for blind source
separation.The vectors s, x, y, and z are sample values of the random vectors S, X, Y,
and Z, respectively.
Nonlinearity
G(•)
Demixer:
W
Unknown environment
Mixer:
A
s
x
y
z
of all probability density functions pY(y) of the random vector Y at the demixer output;
is the set of all independent probability distributions—that is,those of the product form.
Both 
and are of infinite dimension.The set 
is the finite set of prob-
d = {pY(y, W )}
i
s
i
ability distributions measured at the demixer output.The set 
is m2 dimensional, where
m is the dimension of Y, and the weight matrix W is a coordinate system in it. From Fig.
d
10.15, we clearly see that both 
and 
are minimized at W  A1. Moreover, as
indicated in Fig 10.15, the sets 
and 
are indeed orthogonal at their intersection point,
defined by the true probability density function pS(s).
A blind source separation algorithm based on the maximum-likelihood method
must include a provision for estimating the underlying source distributions when they
are unknown, which is typically the case. The parameters for this estimation can be
adapted just as we adapt the demixing weight matrix W. In other words, we should per-
form a joint estimation of the mixing matrix and (some characteristics of) the source
distributions (Cardoso, 1997, 1998); an elegant and well-developed approach for this
joint estimation is presented in Pham et al. (1992, 1997).
10.16
MAXIMUM-ENTROPY LEARNING FOR BLIND SOURCE SEPARATION
In this section, we look to the maximum-entropy principle discussed in Section 10.3,
for another method for blind source separation. To this end, consider Fig. 10.16, which
shows the block diagram of the system based on this method. As before, the demixer
operates on the observation vector x to produce an output y = Wx that is an estimate
of the original source vector s. The vector y is transformed into a vector z by passing it
through a componentwise nonlinearity denoted by G(·), which is monotonic and
invertible.Thus, unlike y, the vector z is assured of a bounded differential entropy h(Z)
for an arbitrarily large demixer. For a prescribed nonlinearity G(·), the principle of max-
imum entropy produces an estimate of the original source vector s by maximizing the
entropy h(z) with respect to W. In light of Eq. (10.60), derived in Example 7, for a noise-
less network, we recall that the maximum-entropy principle is closely related to the
Infomax principle. Indeed, it is for this reason that the algorithm based on the scheme
of Fig. 10.16 is referred to in the literature as the Infomax algorithm for ICA (Bell and
Sejnowski, 1995).
The nonlinearity G is a diagonal map described by
(10.110)
≥
z1
z2
o
zm
¥
≥
g1(y1)
g2(y2)
o
gm(ym)
¥ =
G : ≥
y1
y2
o
ym
¥ S
i
d
DpY7pS
DpY7p
Y

We may thus write
(10.111)
Since the nonlinearity G(·) is invertible, we may express the original source vector s in
terms of the demixer output vector z as
(10.112)
where 
is the inverse nonlinearity:
(10.113)
The probability density function of the output vector z is defined in terms of that of the
source vector s by
(10.114)
where det(J(s)) is the determinant of the Jacobian J(s) (Papoulis,1984).The ij-th element
of this latter matrix is defined by
(10.115)
Hence, the entropy of the random vector Z at the output of the nonlinearity G is
(10.116)
We thus see that maximizing the differential entropy h(Z) is equivalent to minimizing
the Kullback–Leibler divergence between pS(s) and a probability density function of s
defined by det(J(s)); see the last line of Eq. (10.35).
Suppose now the random variable Zi (i.e., the ith element of Z) is uniformly dis-
tributed inside the interval [0, 1] for all i. According to Example 1, the entropy h(Z) is
then equal to zero. Correspondingly, we find from Eq. (10.116) that
(10.117)
Under the ideal condition W  A1, this relationship reduces to
(10.118)
pSi(si) = 0zi
0yi
zi=g(si)  for all i
pS(s) = det(J(s))
= - DpS7 detJ  evaluated at s = (z)
= - clog a
pS(s)
det(J(s)) b d
s=(z)
h(Z) = - [log pZ(z)]
Jij = 0zi
0sj
pZ(z) =
pS(s)
det(J(s)) `
s=(z)
≥
y1
y2
o
ym
¥
≥
g1
-1(z1)
g2
-1(z2)
o
gm
-1(zm)
¥ =
G-1 : ≥
z1
z2
o
zm
¥ S
G-1
= (z) 
 s = A-1W-1G-1(z)
= G(WAs) 
z = G(y)
530
Chapter 10
Information-Theoretic Learning Models

Conversely, we can say that if Eq. (10.118) is satisfied, then maximizing h(Z) yields
W  A1, and blind source separation is thereby achieved.
We may now summarize the idea of the maximum-entropy principle for blind
source separation as follows (Bell and Sejnowski, 1995):
Let the nonlinearity at the demixer output in Fig. 10.16 be defined in terms of the original
source distribution as
(10.119)
Maximizing the differential entropy of the random vector Z (the ith element of which has the
sample value zi) at the output of the nonlinearity G is then equivalent to W  A1, which
yields perfect blind source separation.
Equivalence of the Maximum-Entropy 
and Maximum-Likelihood Methods
The maximum-entropy and maximum-likelihood methods for blind source separation
are indeed equivalent under the condition that the random variable Zi is uniformly dis-
tributed inside the interval [0, 1] for all i (Cardoso, 1997). To prove this relationship,
we first use the chain rule of calculus to rewrite Eq. (10.115) in the equivalent form
(10.120)
where the partial derivative 
is to be defined. The Jacobian J may therefore be
expressed as
where D is the diagonal matrix
Hence,
(10.121)
In light of Eq. (10.121), an estimate of the probability density function pS(s) parame-
terized by the weight matrix W and the nonlinearity G may be written formally as fol-
lows (Roth and Baram, 1996):
(10.122)
pS(sW, G) = det(WA)q
m
i=1
0gi(yi)
0yi
det(J) = det(WA)q
m
i=1
0zi
0yi
D = diag a 0z1
0y1
, 0z2
0y2
, ..., 0zm
0ym
b
J = DWA
0zi0yi
= a
m
k=1
0zi
0yi
wikakj
Jij = a
m
k=1
0zi
0yi
0yi
0xk
0xk
0sj
=
3
z1
- q
pSi(si)dsi         for i = 1, 2, ..., m
zi = gi(yi)
Section 10.16
Maximum-Entropy Learning For Blind Source Separation
531

We thus see that under this condition,maximizing the log-likelihood function log pS(sW,G)
is equivalent to maximizing the entropy h(Z) for blind source separation.That is,the meth-
ods of maximum entropy and maximum likelihood are indeed equivalent.
The Learning Algorithm for Blind Source Separation
Referring to the second line of Eq. (10.116), we note that since the source distribution
is typically fixed, maximizing the entropy h(Z) requires maximizing the expectation of
the denominator term log det(J(s)) with respect to the weight matrix W. With an adap-
tive algorithm for doing this computation as our goal, we may thus consider the instan-
taneous objective function
(10.123)
Substituting Eq. (10.121) into Eq. (10.123) yields
(10.124)
Hence, differentiating 
with respect to the weight matrix W of the demixer, we get the
following (see Problem 10.20):
(10.125)
To proceed further with this formula, we need to specify the nonlinearity fed by the
demixer output.A simple form of nonlinearity that may be used here is the logistic function
(10.126)
Figure 10.17 presents plots of this nonlinearity and its inverse.The figure shows that the
logistic function satisfies the basic requirements of monotonicity and invertibility for
blind source separation. Substituting Eq. (10.126) into Eq. (10.125) yields
where x is the received signal vector, z is the nonlinear transformed output vector of
the demixer, and 1 is a corresponding vector of ones.
The objective of the learning algorithm is to maximize the differential entropy
h(Z). Accordingly, invoking the method of steepest ascent, we find that the change
applied to the weight matrix W is17
(10.127)
where  is the learning-rate parameter. As with the ICA natural gradient learning
algorithm described in Section 10.14, we may eliminate the need for inverting the
= AW -T + (1 - 2z)xTB
¢W =  0
0W
0
0W = W -T + (1 - 2z)xT
=
1
1 + e-yi,  i = 1, 2, ..., m
zi = g(yi)
0
0W = W -T + a
m
i=1
0
0W log a 0zi
0yi
b

 = logdet(A) + logdet(W) + a
m
i=1
log a 0zi
0yi
b
 = logdet(J)
532
Chapter 10
Information-Theoretic Learning Models

transposed weight matrix WT by using the natural gradient, which is equivalent to
multiplying Eq. (10.127) by the matrix product WTW. This optimal rescaling yields the
desired formula for weight adjustment as
(10.128)
where I is the identity matrix and the vector y is the demixer output.The learning algo-
rithm for computing the weight matrix W is therefore described by
(10.129)
The algorithm is initiated with W(0) selected from a uniformly distributed set of small
numbers. Referring to the block diagram of Fig. 10.16, we readily see that at time n the
output y(n) is defined in terms of the input x(n) by the matrix product W(n) x(n).Thus,
every time the demixing matrix W(n) is updated, we may correspondingly compute the
updated value of the demixer output y(n).
W(n + 1) = W(n) + (I + (1 - 2z(n))yT(n))W(n)
= AI + (1 - 2z)yTBW
= AI + (1 - 2z)(Wx)TBW
¢W = AW-T + (1 - 2z)xTBWTW
Section 10.17
Maximization of Negentropy For Independent-Components
533
FIGURE 10.17
(a) Logistic function:
(b) Inverse of logistic function:
yi = g-1(zi).
zi = g(yi) =
1
1 + e-yi.
1
0.8
0.6
zi  g(yi)
0.4
0.2
0
10
8
6
4
2
0
2
(a)
4
6
8
10
yi
yi
0
5
0
5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(b)
1
zi

10.17
MAXIMIZATION OF NEGENTROPY
FOR INDEPENDENT-COMPONENTS ANALYSIS
In one form or another, the ICA algorithm discussed in Sections 10.14, 10.15, and 10.16
is basically rooted in the principle of statistical independent components, which is itself
based on the Kullback–Leibler divergence, as discussed in Section 10.14. In this section,
we will depart from that principle by describing another ICA algorithm that is rooted
differently in information theory. The algorithm is called the FastICA algorithm and is
due to Hyvärinen and Oja (1997).
In more specific terms, the FastICA algorithm exploits the notion of non-
Gaussianity, which is a requirement of independent-components analysis, discussed pre-
viously in Section 10.12. An important measure for the non-Gaussianity of a random
variable is negentropy,which is based on differential entropy.It is therefore a propos that
we begin our discussion of the FastICA algorithm by describing what is meant by this
new concept.
Negentropy
In Example 2, we showed that a Gaussian random variable distinguishes itself from all
other random variables by having the largest possible differential entropy.Specifically,the
information content of a Gaussian random variable is confined to second-order statistics,
from which all higher-order statistics can be computed.To assess the non-Gaussianity of
a random variable, it would therefore be desirable to postulate a measure that satisfies
two properties:
1. The measure is nonnegative, assuming the limiting value of zero for a Gaussian
random variable.
2. For all other random variables, the measure is greater than zero.
The concept of negentropy satisfies both of these properties.
Consider a random vector X that is known to be non-Gaussian.The negentropy of
X is formally defined by
(10.130)
where H(X) is the differential entropy of X, and H(XGaussian) is the differential entropy
of a Gaussian random vector whose covariance matrix is equal to that of X.
In information-theoretic terms, negentropy is an elegant measure of non-
Gaussianity. But it is highly demanding in computational terms, which limits its practi-
cal use. To overcome this computational difficulty, we have to seek a simple
approximation to negentropy.Consider,then,a non-Gaussian random variable V of zero
mean and unit variance. Hyvärinen and Oja (2000) have proposed the approximation
(10.131)
where U is a Gaussian random variable also of zero mean and unit variance (i.e.,it is stan-
dardized). For all practical purposes, )(·) is a nonquadratic function; desirably, this func-
tion does not grow rapidly, so as to robustify the estimation procedure. According to
Hyvärinen and Oja (2000) two choices have proven their usefulness, given as
N(V) = [£(V)] - [£(U)]2
N(X) = H(XGaussian) - H(X)
534
Chapter 10
Information-Theoretic Learning Models

1.
(10.132)
2.
(10.133)
where is a sample value of the random variable V.We may therefore look to Eq.(10.131)
as the “contrast function” to be maximized for the purpose of independent-components
analysis.Except for scaling factors,the function ( ) may be viewed as a probability den-
sity function. Note that the function (·) used in Eqs. (10.132) and (10.133) should not
be confused with the matrix  used in Eq. (10.123).
Basic Learning Rule of the FastICA Algorithm
To pave the way for the development of the FastICA algorithm, we first consider a
single-unit version of the algorithm. The term “unit” refers to a neuron with an
adjustable-weight vector w. The neuron is to be designed in such a way that we develop
the basic learning rule of the FastICA algorithm.
Let x be the sample value of a prewhitened random vector of zero mean, X, that
is applied to the input of the neuron. We start the development of the basic learning
rule by doing the following:
Maximize the negentropy of the projection of the adjustable-weight vector w on to the random
vector X, subject to the constraint 
.
The projection is defined by the inner product wTX. With the random vector X
prewhitened, the constraint 
is equivalent to constraining the projection to have
a variance equal to unity, as shown by
(10.134)
In the first line of Eq. (10.134), we made use of the zero-mean assumption imposed
on X, and in the third line, we made use of the prewhiteneing assumption also imposed
on X.
For the basic learning rule to be computationally efficient, we look to the approx-
imation of Eq. (10.131) as the formula for computing the negentropy N(V), where
V = wTX. Since U is a standard Gaussian random variable of zero mean and unit vari-
ance, and therefore independent of w, it follows that maximizing N(V) with respect to
w is equivalent to maximizing the nonquadratic function (V) = (wTX).We may thus
reformulate the optimization problem of interest as follows:
Maximize the expectation 
, subject to the constraint 7 w7 = 1.
[£(wTx)]
= 1
= 7 w7 2
= wTw
= wT[XXT]w
= [wTXXTw]
 var[wTX] = [(wTX)2]
7 w7 = 1
7 w7 = 1
v
v
£(v) = -exp a - v2
2 b
£(v) = log (cosh (v))
Section 10.17
Maximization of Negentropy For Independent-Components
535

According to the Karush–Kuhn–Tucker conditions of optimization theory (dis-
cussed in Chapter 6), the solution to this constrained maximization problem is to be
found in the equation
(10.135)
where x is the sample value of the random vector X.The gradient vector of the expec-
tation 
with respect to the weight vector w is evaluated as
(10.136)
where (·) is the first derivative of the nonquadratic function )(·) with respect to its
argument—that is,
For example, for the function )(·) defined in Eq. (10.132), we have
and for the function )(v) defined in Eq. (10.33), we have
Thus, we may rewrite Eq. (10.135) in the equivalent form
(10.137)
What we are interested in is finding a computationally efficient iterative procedure
for implementing a basic learning rule whereby the optimized weight vector w points
along the direction of an independent component.To this end, we propose the applica-
tion of Newton’s method to the expression on the left-hand side of Eq. (10.137).
Let this expression be denoted by the vector-valued function
(10.138)
Newton’s method was discussed in Chapters 3 and 4. For the application of this method,
we need the Jacobian of the function f(w), which is defined by
f(w) = [x(wTx)] - 
w
[x(wTx)] - 
w = 0
= v exp a- v2
2 b
(v) = d
dv a - exp a- v2
2 b b
= tanh(v)
(v) = d
dv log (cosh(v))
(v) =
d£(v)
dv
= [x(wTx)]
= c
0(wTx)
0w
0
0wTx £(wTx)d
0
0w [£(wTx)] = c 0
0w £(wTx)d
[£(wTx)]
0
0w [£(wTx)] - 
w = 0
536
Chapter 10
Information-Theoretic Learning Models

(10.139)
where I is the identity matrix. The prime in (·) signifies differentiation of the function
(·) with respect to its argument. In other words, (·) is the second-order derivative of
the original function )(·) with respect to its argument. Now we can see why, earlier on,
we stated that (·) must be a nonquadratic function; otherwise, we would have ended up
with (·) equal to a constant in Eq. (10.139), which is clearly unacceptable.
Before proceeding, however, we wish to simplify development of the basic learn-
ing rule further. Since the input vector x has been prewhitened, it is reasonable to
assume that the outer product xxT is statistically independent of the term (wTx) in
Eq. (10.139). Under this assumption, we may go on to write
(10.140)
where, in the last line, we have made use of the whitening property of the input x—that
is,
Accordingly, we now find that the whole expression for the Jacobian J(w)
in Eq. (10.139) takes the form of a scalar multiplied by the identity matrix I, as shown
by
(10.141)
which is readily invertible.With this approximate formula at hand, we may express the
Newton iterative step as
(10.142)
where w is the old value of the weight vector and w is its updated value. Note also that
we have used a minus sign in the iterative step, since we are seeking the maxima of the
function f(w).Thus, substituting Eq. (10.141) into Eq. (10.142), we obtain
We may simplify this iterative step by multiplying both sides of this equation by the
scalar ( [(wTx)]  ), which yields
(10.143)
= [¿(wTx)]w - [x(wTx)]
w+ = ([¿(wTx)] - 
)w - ([x(wTx)] - 
w)


w+ = w - ([¿(wTx)] - 
)-1([x(wTx)] - 
w)
w+ = w - J-1(w)f(w)
J(w) L ([¿(wTx)] - 
)I
[xxT] = I.
= [¿(wTx)]I
[xxT¿(wTx)] L [xxT][¿(wTx)]
= [xxT¿(wTx)] - 
I
= c 0
0w x(wTx)d - 
I
=
0
0w [x(wTx)] -
0
0w (
w)
=
0
0w{[x(wTx)] - 
w}
J(w) =
0
0w f(w)
Section 10.17
Maximization of Negentropy For Independent-Components
537

where, on the left-hand side, we have absorbed the scaling factor 
in
([(wTx)] - 
)
538
Chapter 10
Information-Theoretic Learning Models
FIGURE 10.18
Model of the neuron featuring in the basic learning rule of the FastICA
algorithm.
Σ
x1
x2
xm
•
•
•
•
•
•
w()
w()  w(wTx)
where w  [w1, », wm]T
Input vector
x

w1
w2
wm
the new value of w. Note also that we do not need to know the value of the Lagragian
multiplier 
,as it has been eliminated algebraically from the iterative step of Eq.(10.143).
Equation (10.143) is at the core of the basic learning rule that we have been seek-
ing. Indeed, in light of this equation, we may now model the single neuron, around which
this equation is built, as shown in Fig. 10.18. According to this figure, we may view the
nonlinear function (·) as the activation function of the neuron.
With the iterative step of Eq. (10.143) at our disposal, we are finally ready to sum-
marize the Newton-based learning rule of the fastICA algorithm as follows:
1. Choose an initial value of the weight vector w, using a random-number generator,
subject to the constraint that the Euclidean norm of w is unity.
2. Using the old value of the weight vector w, compute the updated value
3. Normalize the updated weight vector w so that its Euclidean norm is equal to
unity, as shown by
4. If the algorithm has not converged, go back to step 2 and repeat the computation.
To compute the expectations in step 2 of the learning rule, we may invoke ergodicity
and replace the expectations by time averages, based on a sequence of independent sam-
ples (realizations) of the input vector x.
The learning rule is said to have converged—that is, the rule has reached an
equilibrium point—when the updated weight vector 
and the old weight vector w
point along a common direction.That is, the absolute value of the inner product wT
is close to unity. However, since an ICA algorithm can extract an independent compo-
nent only to within a multiplicative scaling factor, it is not necessary to seek an equilib-
rium point where the weight vectors w and w point in exactly the same direction. In
other words, it is acceptable for 
to be the negative of w.
w+
w+
w+
w =
w+
7w+ 7
w+ = [¿(wTx)]w - [x(wTx)]

One last comment is in order: Derivation, and therefore application, of the algo-
rithm is based on the premise that the mixer output has been prewhitened; the issue of
prewhitening was discussed in Section 10.12.
Multiunit Version of the FastICA Algorithm
Naturally,the Newton-based learning rule,built around a single neuron,can estimate only
one of the m independent components (sources) responsible for generating the obser-
vation vector x. To expand this rule so as to estimate all the m independent components,
we obviously need a network of m neurons or its equivalent.
To explore the conditions that this network must satisfy, let w1, w2, ..., wm denote
the weight vectors produced by m individual neurons of the network. For this set of vec-
tors to represent a correct solution to the blind source separation (BSS) problem, we
require two conditions:
1. Orthogonality. Suppose the random observation vector X is applied simultane-
ously to the m neurons, producing the set of outputs
In order to prevent all the m weight vectors from converging into the same inde-
pendent components,we require that the neural outputs be uncorrelated with each
other—that is,
(10.144)
Thus, with Vi =
X and Vj = wT
jX = XT wj, we write
where,in the last line,we made use of the whitening property of the observation vec-
tor X. It follows, therefore, that for the decorrelation property of Eq. (10.144) to be
satisfied, the weight vectors w1, w2, ..., wm must form an orthogonal set, as shown by
(10.145)
2. Normalization. To be in line with the Newton-based learning rule, we require that
each weight vector be normalized for its Euclidean norm to equal unity,as shown by
(10.146)
Putting conditions 1 and 2 together,we may now make the following formal statement:
For the weight vectors 
to provide estimates of the m independent components
(sources) responsible for the generation of the observation vector x, they must form an
orthonormal set, as shown by
(10.147)
wT
i wj = e 1
for j = i
0
otherwise
w1, w2, ..., wm
7wi7 = 1  for all i
wT
i wj = 0  for j Z i
= wT
i wj  for j Z i
= wT
i [XXT]wj
[ViVj] = [wT
i XXTwj]
wT
i
[ViVj] = 0  for j Z i
{Vi}m
i=1,  where Vi = wT
i X
Section 10.17
Maximization of Negentropy For Independent-Components
539

The Gram–Schmidt Orthogonalization Procedure
The twofold necessary condition of Eq. (10.147) imposed on the weight vectors leads us
to think in terms of a simple deflational method, based on the Gram–Schmidt orthogo-
nalization procedure,18 for estimating all the m independent components, one by one, as
originally proposed by Hyvärinen and Oja (1997, 2000).To be specific, suppose that we
first run the single-neuron, Newton-based learning rule on N independent realizations
(samples) of the observation vector x, obtaining the weight vector w1 as the estimate of
one of the m independent components. When running this rule on the next set of N
independent realizations of x, suppose the resulting weight vector is denoted by 2.The
reason for using a different symbol for this second weight vector is that the vector 
is unlikely to be orthogonal to w1. To correct for this deviation from the necessary
condition of orthogonality, we apply the Gram–Schmidt orthogonalization procedure,
obtaining
where we have subtracted from 2 the “projection”
By recognizing that
, it is a straightforward matter to show that 
is indeed orthogonal to w1—that
is,
All that remains to be done is to normalize 
by setting
Proceeding in this manner, suppose that on the next set of N samples of the
observation vector x, the Newton-based learning rule yields the weight vector 3,
where again it is unlikely for 3 to be orthogonal to w2 and w1.To correct for these devi-
ations, we apply the Gram–Schmidt orthogonalization procedure one more time,
obtaining
where we have subtracted from 3 the projections (T
3wj)wj, j = 1, 2. By recognizing that
and wT
1w2  0, it is a straightforward matter to show that 3 is orthog-
7 w17 = 7 w27 = 1
3 = 3 - (T
3w1)w1 - (T
3w2)w2
w2 =
2
727
2
T
2w1 = 0.
2
7 w17 = 1
(T
2w1)w1.
2 = 2 - (T
2w1)w1
2
540
Chapter 10
Information-Theoretic Learning Models
onal to both w1 and w2. Hence, all that remains is to normalize 3 by setting
We may continue in this manner until all the m independent components have been
accounted for.
We may now summarize the Gram–Schmidt orthogonalization procedure for com-
puting the desired set of m weight vectors as follows:
1. Given that w1 is the normalized weight vector produced by the single-unit Newton-
based learning rule on its first complete iteration, and given that 2, ..., i+1 are the
weight vectors produced by the rule on the next i complete iterations, compute
where the “projections”
have been subtracted from i +1 for j = 1, 2, ..., i.
(T
i+1wj)wj
i+1 = i+1 - a
i
j=1
(T
i+1wj)wj,  i = 1, 2, ..., m - 1
w3 =
3
737

2. Normalize i+1 by setting
The FastICA algorithm based on this procedure represents the single-unit defla-
tion version of the algorithm.19
Properties of the FastICA Algorithm
Compared with other ICA algorithms, the fast ICA algorithm has some desirable prop-
erties (Hyvärinen and Oja, 2000;Tichavsky et al., 2006):
1. Under the assumption of a noise-free, linear generative model, the convergence of the
FastICA algorithm is relatively fast—hence the name of the algorithm. Whereas the
gradient-based ICA algorithms discussed in Sections 10.14,10.15,and 10.16,tend to con-
verge in a linear manner, convergence of the FastICA is cubic (or at least quadratic).
2. Unlike the gradient-based ICA algorithms, the FastICA algorithm does not require
the use of a learning-rate parameter, making it simpler to design.
3. The FastICA algorithm has a built-in capability to find the independent components of
practically any non-Gaussian distributions,using any nonquadratic form of nonlinearity
This versatility of the algorithm is to be contrasted against gradient-based ICA
algorithms, where applicability is confined to sub-Gaussian or super-Gaussian distrib-
utions, and careful attention is given to the choice of nonlinearity.
4. Through proper selection of the nonquadratic function (·), exemplified by Eqs.
(10.132) and (10.133), robustness of the FastICA algorithm may be assured, even with
large data sets and under somewhat noisy conditions.
5. The independent components are computed by the FastICA algorithm sytematically,
one by one. This feature of the algorithm makes it a useful tool for exploratory data
analysis, where the estimation of a limited number of independent components may
be all that is required for the application of interest. The computational load of the
analysis is thereby reduced.
6. The FastICA algorithm has many attributes usually associated with neural networks:
parallelism, distributed computation, simplicity, and small memory requirement. On
the other hand,stochastic gradient-based ICA algorithms—exemplified by the natural-
gradient algorithm discussed in Section 10.14 —are the preferred choice for blind source
separation problems involving nonstationary environments, where there is a definite
need for fast adaptivity.
10.18
COHERENT INDEPENDENT-COMPONENTS ANALYSIS
Looking back at the material we have thus far covered in this chapter on the impact of
information theory on the development of learning models,we find that the maximization
of the mutual information principle—or the Infomax principle,for short—stands out promi-
nently. Not only does the Infomax principle play a significant role in our understanding of
redundancy reduction,the modeling of perception,and the extraction of independent com-
ponents, but also its relative the Imax principle plays a role of its own in the extraction of
spatially coherent features. In reality, Infomax and Imax play complementary roles:
Infomax deals with information flow across a network,whereas Imax deals with spatial coher-
ence across a pair of network outputs.
(v).
wi+1 =
i+1
7i+17 ,  i = 1, 2, ..., m - 1
Section 10.18
Coherent Independent-Components Analysis
541

Figure 10.19 depicts a scenario where these two principles are embodied together.Specif-
ically, we have two separate, but dimensionally similar neural networks: Network a is
characterized by the weight matrix Wa, and network b is characterized by the weight
matrix Wb.Both networks are assumed to be noiseless.The goal is to combine the Infomax
and Imax principles in such a way that the two aforementioned properties—information
flow in each network in accordance with the Infomax principle, and spatial coherence
across the neural outputs of the two networks treated on a pair-by-pair basis in accordance
with the Imax principle—are integrated into a composite learning principle.
Contributions of the Infomax Principle
Consider first the Infomax principle applied across the input–output of each network in
Fig. 10.19.Then, in light of Eq. (10.60), pertaining to the noiseless Example 7, the network
characterized by the weight matrix Wa may be described by the mutual information
where, to simplify the presentation, we have ignored an additive constant that is indepen-
dent of the weight matrix Wa; moreover, we have made use of Eq. (10.60) in a way that
involves the entropy of a random vector.With the elements constituting the output random
vector Ya being “independent,” we may express the probability density function of Ya as
where l is the number of output terminals.We may therefore go on to write
(10.148)
= -  c a
l
i=1
log pYa, i(ya, i)d,  i = 1, 2, ..., l
I(Ya; Xa) = -  clog q
l
i=1
pYa, i(ya, i)d
pYa(ya) = q
l
i=1
pYa, i(ya, i)
I(Ya; Xa) = - [log pYa(ya)]
542
Chapter 10
Information-Theoretic Learning Models
Network a:
Wa
Maximum I(Ya; Xa)
Maximum I(Yai; Ybi)
for i  1, 2, », l
Maximum I(Yb; Xb)
•
•
•
•
•
•
xa2
xal
xa
xa1
ya2
yal
ya1
Network b:
Wb
•
•
•
•
•
•
xa2
xbl
xb
xa1
yb2
ybl
yb1
FIGURE 10.19
The coupled-
network layout for coherent
ICA.

Similarly, for the second network, characterized by the weight matrix Wb, we may write
(10.149)
Contributions of the Imax Principle
Consider next the Imax principle applied across the outputs of these two networks,treated
on a pair-by-pair basis. In light of the second line of Eq. (10.50), we may express the
mutual information between the outputs Ya,i and Yb,i in terms of the copula as
Again, because the l outputs of each network in Fig. 10.19 are independent, these indi-
vidual mutual contributions are additive, yielding the sum
(10.150)
Overall Cost Function
Let J(Wa, Wb) denote the ensemble-averaged objective function, accounting for the
combined actions of the Infomax and Imax principles.Then,combining the mutual infor-
mation contributions of Eqs. (10.148) to (10.150), we write
(10.151)
where, in the last line, we have made use of Eq. (10.49), expressing the joint probability
density function of the output random variables Ya, i and Yb, i. The objective function
J(Wa, Wb) defines the sum of the joint entropies of the two sets of network outputs
{Ya, i}l
i = 1 and {Yb, i}l
i = 1, which are treated on an orderly pair-by-pair basis; these outputs
are respectively dependent on the weight matrices Wa and Wb. Indeed, it is precisely
with this definition in mind that we introduced the minus sign in combining the contri-
bution of copulas into the first line of Eq. (10.151). In so doing, the desired orderly sta-
tistical dependence between the two sets of network outputs has been enforced, and
hence we can make the following formal statement:
The coherent ICA principle maximizes the overall sum of the joint entropies of the two sets
of network outputs {Ya, i}l
i = 1 and {Yb, i}l
i = 1, treated on an orderly pair-by-pair basis, with the
maximization being performed with respect to the weight matrices Wa and Wb of the two
constituent networks.
= -  c a
l
i=1
log pYa, iYb, i(ya, i, yb, i)d
= -  c a
l
i=1
log ApYa, i(ya, i)pYb, i(yb, i)cYa, i, Yb, i(ya, i, yb, i)B d
J(Wa, Wb) = -  c a
l
i=1
logpYa, i(ya, i)d -  c a
l
i=1
logpYb, i(yb, i)d -  c a
l
i=1
log cYa, iYb, i(ya, iyb, i)d
a
l
i=1
I(Ya, i; Yb, i) =  c a
l
i=1
log cYa, i; Yb, i(ya, i; yb, i)d
I(Ya, i; Ya, b) = [log cYa, i; Yb, i(ya, i; yb, i)]  for i = 1, 2, ..., l
I(Yb; Xb) = - c a
l
i=1
log pYb, i(yb, i)d,  i = 1, 2, ..., l
Section 10.18
Coherent Independent-Components Analysis
543

To proceed further, we make two reasonable assumptions:
1. The two neural networks in Fig. 10.19 are both linear, as shown by
(10.152)
where wT
a, i and wT
b, i are the ith row vectors of the weight matrices Wa and Wb,
respectively.
2. With data drawn from natural scenes being typically sparse as discussed in Section
10.13, the distributions of the composite output vector yi may be described by the
zero-mean generalized Gaussian bivariate distribution with a two-by-two covariance
matrix , as shown by
(10.153)
where the parameter  controls the shape and sparseness of the copula. The covari-
ance matrix  is itself defined by
(10.154)
which is the variance-normalized form of the covariance matrix defined in Eq.(10.67)
for Imax. The correlation coefficient & controls the extent of correlation
between the paired network outputs ya, i and yb, i for all i. Increasing & does not
affect the shape or skew of the copula; rather, it affects the relative importance
of Imax over Infomax by favoring a greater coherence in learning across the two
networks.
For 
the distribution of Eq. (10.153) reduces to the Gaussian bivariate dis-
 = 2,
 = c 1
&
&
1d
pYi(yi) =
1
2det12() exp a- 1
2 (yT
i -1yi)2 b ,  i = 1, 2, ..., l
= c wT
a, i xa, i
wT
b, i xb, i
d,  i = 1, 2, ..., l
yi = c ya, i
yb, i
d
544
Chapter 10
Information-Theoretic Learning Models
tribution. For  less than 2, Eq. (10.153) begins to take on the form of a super-Gaussian
distribution, as illustrated in Fig. 10.20 for three different values of . In particular, for
the distribution of Eq. (10.153) assumes a form more like that of the Laplacian
distribution of a speech signal.
The vector yi embodies the two elements ya, i and yb, i.Hence,substituting Eq.(10.153)
into Eq. (10.151) and ignoring the constant term 2det1/2(), we get
(10.155)
where the ensemble averaging is performed with respect to yi. To simplify computa-
tional complexity, we bypass the need for ensemble averaging by using the instantaneous
values of the quadratic form yT
i  1yi, for all i. Thus, using the definition in Eq. (10.154)
for the covariance matrix , we may go on to write
J(Wa, Wb) = 1
2 c a
l
i=1
(yT
i -1yi)2d
 = 1.3,

(10.156)
where the hat in 
distinguishes it from its ensemble-averaged counterpart.
Formulation of the Learning Rules for the Two Networks
To formulate the adaption rule for the weight vector Wa, i, we start by differentiating
with respect to wa, i. Using the chain rule of calculus, we write
(10.157)
Differentiating Eq. (10.156) with respect to yi, a yields
(10.158)
Using Eq. (10.152), we find that differentiating ya, i = wT
a, i xa with respect to wa, i yields
(10.159)
0ya, i
0wa, i
= xa
0Jˆ(Wa, Wb)
0ya, i
=

(1 - &2) (ya, i - &yb, i)(y2
a, i - 2&ya, iyb, i + y2
b, i)(2) - 1
0Jˆ(Wa, Wb)
0wa, i
=
0Jˆ(Wa, Wb)
0ya, i
 
0ya, i
0wa, i
Jˆ(Wa, Wb)
Jˆ(Wa, Wb)
 =
1
2(1 - &2) a
l
i=1
(y2
i, a - 2&yi, ayi, b + y2
i, b)2
 Jˆ (Wa, Wb) = 1
2
 c a
l
i=1
(yT
i -1yi)2d
Section 10.18
Coherent Independent-Components Analysis
545
3
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
2
1
0
1
2
3
Probability density function
Signal Value
α  1.7
α  1.9
α  1.3
FIGURE 10.20
Generalized Gaussian distributions for varying values of parameter .

Therefore, using Eqs. (10.158) and (10.159) in Eq. (10.157), we obtain the gradient vector
(10.160)
The objective is to maximize the instantaneous objective function 
,which means
that we use gradient ascent for the iterative computation.Accordingly,the change applied to
the weight vector wa, i is defined by
(10.161)
Similarly, the change applied to the weight vector wb, i is defined by
(10.162)
where it is assumed that network b shares the same learning-rate parameter  with
network a.
The weight updates for networks a and b are thus respectively expressed as follows:
(10.163)
(10.164)
where
The two update rules of Eqs.(10.163) and (10.164)—building on the weight changes
and 
described in Eqs. (10.161) and (10.162) — constitute the coherent ICA
algorithm.
Interpretations of Eqs. (10.161) and (10.162)
It is instructive to examine the algebraic structures of the learning rules described in
Eqs. (10.161) and (10.162). Taking on Eq. (10.161) first, we see that the change 
applied to the ith column vector of the weight matrix 
pertaining to network a in
Fig. 10.19, is made up of three basic factors:
1. The scaling factor 
which may be viewed simply as a modified learning-
rate parameter that is common to the computation of 
as well as 
for
¢wb, i
¢wa, i
(1 - &2),
Wa,
¢wa, i
¢Wb, i
¢Wa, i
i = 1, 2, ..., l.
w+
b, i = wb, i + ¢wb, i
w+
a, i = wa, i + ¢wa, i
¢wb, i =

(1 - &2) (yb, i - &ya, i)(y2
a, i - 2&ya, iyb, i + y2
b, i)(2) -1xb
¢wa, i =

(1 - &2) (ya, i - &yb, i)(y2
a, i - 2&ya, iyb, i + y2
b, i)(2) -1xa
Jˆ(Wa, Wb)
0Jˆ(Wa, Wb)
0wa, i
=

(1 - &2) (ya, i - &yb, i)(y2
a, i - 2&ya, iyb, i + y2
b, i)(2)-1xa
546
Chapter 10
Information-Theoretic Learning Models
all i. A change in the parameter  merely affects the rate of adaptation of the
algorithm.
2. The factor 
which may be expressed as the difference between two
quadratic forms, as shown by
The first quadratic form 
involves network a alone, whereas the sec-
ond quadratic form 
involves both networks a and b. The important
point to note here is the fact the contribution made by the second factor
is independent of the parameter 
in other words, this factor is
;
(ya, i - &yb, i)xa
(xT
bwb, ixa)
(xT
awa, ixa)
(ya, i - &yb, i)xa = (xT
awa, ixa) - &(xb
Twb, ixa)
(ya, i - &yb, i)xa,

completely unaffected by whether or not the distribution of the output vector
deviates from Gaussianity.
3. The third and last factor 
may also be expressed in terms
of quadratic forms, as shown by
It is in this factor where the parameter  affects the operation of the algorithm in
the most significant manner. In particular, when 
, the power to which the fac-
tor is raised becomes zero,thereby nulling out its impact on the algorithm.It is when
which arises when dealing with super-Gaussian distributions, that the coher-
ent ICA algorithm performs its distinctive signal-processing role.
Similar remarks apply to the learning rule of Eq. (10.162), except for the fact that
the subscripts a and b interchange their respective roles.
Practical Considerations
In performing the coherent ICA learning process,it is assumed that the network inputs xa and
xb in Fig. 10.19 are both prewhitened, which is normal practice in ICA-related work. More-
over,after each iteration of the learning process,the weights are normalized,as shown by
(10.165)
and
(10.166)
These normalized values are then used in the next iteration of the algorithm.
For applications that involve the modeling of data where we have two streams
consisting of spatially shifted data, as exemplified by Fig. 10.19, it is useful to enforce a
weight-sharing constraint between the two data streams, in which case we set
(10.167)
A sensible way of satisfying this constraint is to use the average of 
and 
calculated
in Eqs. (10.165) and (10.166). Thus, by starting the weight-adaptation rule for coherent
ICA with the same initial weight matrices assigned to networks a and b, the weight-
sharing property is maintained in every step of the adaptation rule.
To illustrate an important practical use of the coherent ICA principle, we now dis-
cuss how the coherent ICA principle provides a computational tool for learning filters
involved in the auditory coding of natural sounds.
Auditory Encoding: Coherent ICA Applied to Natural Sounds
Time manifests itself in many structural and functional specializations of the audi-
tory system. With multiple time scales in acoustic stimuli, we find it informative to
wb, i
wa, i
wa, i = wb, i  for all i
wb, i =
w+
b, i
7w+
b, i7
wa, i =
w+
a, i
7w+
a, i7
 6 2,
 = 2
+ wT
b, ixbxT
bwb, iB
Ay2
a, i - 2&ya, iyb, i + y2
b, iB = AwT
a, i xaxT
awa, i - 2&wT
a, ixa xT
bwb, i
(y2
a, i - 2&ya, iyb, i + y2
b, i)
yi
Section 10.18
Coherent Independent-Components Analysis
547

distinguish between two specific components in the waveform of an acoustic stimulus
(Joris et al., 2004):
1. the carrier, represented by the fine structure of the waveform, which waxes and
wanes in an “amplitude-modulated” fashion;
2. the envelope, which is the contour of the amplitude-modulated waveform.
From amplitude modulation theory, we know that the information-bearing signal
(i.e., the modulating signal) is contained in the envelope of the modulated signal. From
a physiological viewpoint, interest in amplitude modulation is therefore motivated by
the desire to know whether envelope processing is actually embedded in the auditory
system.
Indeed,across multiple layers of the auditory system,there are neurons that respond
differently to an incoming amplitude-modulated speech signal. In particular, the succes-
sive layers of the auditory system distinguish themselves by responding to different lim-
ited ranges of amplitude-modulation rates:The lower layers are most responsive to fast
changes in the energy of incoming acoustic stimuli, with progressively slower changes
occurring in the higher layers.In light of this reality,it is not surprising that amplitude mod-
ulation is considered to be an important acoustic cue in the perception of sound.
With auditory processing as the issue of interest, the question that we would like
to address is the following:
1. Given an additive mixture of amplitude-modulated speech signals,how can we separate the
envelopes of the individual components, ignoring the associated carriers?
A related question is the following:
2. In a self-organized manner, can we learn the procedure by which the different processing
layers in the auditory system respond to an amplitude-modulated stimulus?
The answer to this basic question experimentally is to be found in coherent ICA (Haykin
and Kan, 2007).
In coherent ICA, the goal is to extract information that is maintained “coherently”
across separate sources while, at the same time, information flow across the networks
associated with the sources is maximized. Since in amplitude modulation, the envelope
varies slowly compared with the carrier,we may view amplitude modulation as a form of
temporal coherence insofar as the envelope is concerned; that is, across two time-steps !t
seconds apart, with !t assumed to be just short enough, we may set 
.
In Kan (2007) and Haykin and Kan (2007),the coherent ICA algorithm was applied
to a set of speech samples of English speakers taken from the TIMIT database.20 Therein,
it was demonstrated experimentally that the two sets of filters learned using coherent ICA
on the speech data for two layers of auditory processing are smooth and temporally local-
ized. Most importantly, the results of the experiments exhibit two important features:
1. The passband of the filters in both layers includes only frequencies within the mod-
ulation spectrum, ignoring the carrier frequencies.
2. The baseband (i.e., modulation-based) filters computed for the first layer of pro-
cessing have a cutoff frequency that is about 10 times that of the baseband filters
computed for the second layer of processing. In other words, the first layer of the
x(t + ¢t) L x(t)
548
Chapter 10
Information-Theoretic Learning Models

experimental model (based on coherent ICA) is most responsive to fast changes
in the input auditory signal, whereas the second layer of the model is responsive
to slower changes in the input.
In short,the filters learned by coherent ICA,when applied to natural sounds,are baseband
filters that appear to exhibit properties similar to those of biological neurons in the cochlear
nucleus and inferior colloculus.
10.19
RATE DISTORTION THEORY AND INFORMATION BOTTLENECK
Up to this point, we have focused on two fundamental concepts of information theory—
entropy and mutual information—as pillars in the study of information-theoretic learning. In
this section,we look to rate distortion theory for another insightful approach to information-
theoretic learning. The approach we have in mind, called the information bottleneck
method, was first described in the literature by Tishby et al. (1999).
Rate distortion theory, which is an integral part of Shannon’s information theory
(Shannon, 1948), deals with the compression of data with possible distortion, the pur-
poseful application of which results in a measurable amount of distortion in the data.The
motivation for compressing data is to produce a new stream of data that, on the aver-
age, requires a smaller number of bits for its representation or transmission than does
the original data stream.
To pave the way for a description of the information bottleneck method, we begin
the discussion with rate distortion theory.
Rate Distortion Theory
Given a stream of data produced by a source of information, the goal of rate distortion
theory is to find the minimum expected value of distortion that is achievable at a spec-
ified rate of information flow, or, equivalently, to find the minimum rate of information
flow achievable for a prescribed level of distortion.
To cast the theory in analytic terms, let X denote a random vector of probability
density function pX(x),produced by a source of information.Correspondingly,let the ran-
dom vector 
of probability density function qT(t) represent a compressed version of X.
(Note that we have used different symbols for the distributions of X and .) Following
the last line of Eq. (10.28), the mutual information between X and T is expressed as
where 
is the conditional probability density function of T given X. For the mea-
sure of “distance” between the vectors X and T, we use the symbol d(x, t), where x and
t denote sample values of X and T, respectively.The expected distortion is defined by
(10.168)
[d(x, t)] =
3
q
-q 3
q
-q
pX(x)qTX(tx)d(x, t)dxdt
qTX(tx)
I(X; T) =
3
q
- q 3
q
- q
pX(x)qTX(tx) log a
qTX(tx)
qT(t)
bdxdt
T
T
Section 10.19
Rate Distortion Theory and Information Bottleneck
549
Joint pdf
Joint pdf
µ
µ

Rate distortion theory is itself characterized by a function called the rate distortion func-
tion, denoted by R(D).
With this notational background at hand, we may now formally state rate distor-
tion theory as follows (Cover and Thomas, 2006):
Find the rate distortion function
subject to the distortion constraint
From this statement, it is apparent that the computation of the rate distortion function
R(D) involves the solution of the following constrained-optimization problem:
Minimize the mutual information between the source and its representation, subject to a pre-
scribed distortion constraint.
This optimization problem can be solved by the Blahut–Arimoto algorithm (Cover &
Thomas 2006), which amounts to alternating projections between two convex sets of
unknown distributions, as described in Section 10.21.
The main achievement of rate distortion theory is in showing that the rate distor-
tion function is an asymptotically achievable lower bound on the rate (code length) of
any description of the data with the given expected distortion.
Information Bottleneck Method
The information bottleneck method builds on rate distortion theory by replacing the
distortion term with information on a “relevant variable.”21 In many applications, the
“true distortion” measure is unknown or undefined, but there is another variable on
which we would like to preserve some given information.A good example is the prob-
lem of speech recognition. In this problem, it is notoriously difficult to formulate a dis-
tortion function that correctly captures human acoustic perception; it is much easier to
give many examples of spoken words together with their phonetic transcription. In such
a case, we seek a compression of the high-entropy speech signal that preserves as much
information as possible on the lower entropy phonetic sequence.Other important exam-
ples of this type of co-occurrence data are those for which distortion functions are not
directly available: words and topics, images and objects, gene expression and tissue sam-
ples, and stimuli and neural responses. The information bottleneck method has been
successfully applied to these types of data (Slonim et al., 2006).
The information bottleneck method is done through the introduction of an auxiliary
(relevant) random vector,denoted by Y.This new random vector is (stochastically) depen-
dent on the original,usually high-entropy,random vector X;hence,the mutual information
I(X; Y) is nonzero.
With X being the random vector that is to be compressed, Y is the random vector
that we would like to predict (or about which we would like to maintain as much infor-
mation as possible). By introducing the bottleneck random vector T as the compressed
representation of the original random vector X, in effect, we have created a tradeoff or
[d(x, t)]  D
R(D) = min
qTX(tx)I(X; T)
550
Chapter 10
Information-Theoretic Learning Models

bottleneck between two amounts of information: one is contained in T about X and the
other is contained in T about Y.
In particular, we would like to resolve the information bottleneck by satisfying
two objectives in a combined manner:
1. Partition the sample values of the original (high-entropy) random vector X in such
a way that as much mutual information as possible is preserved about the relevant
random vector Y.
2. Lose as much information as possible about the original random vector X to ob-
tain the simplest form of minimal partitioning.
Thus, among all the characterizations of X, the problem is to determine only those which
are most relevant to the prediction of Y.
Basically, the information bottleneck method is designed to find optimal relevant
data representations. This problem can be formulated as follows :
Given the joint probability density function of a random vector X and a relevant random
vector Y, extract minimal sufficient partitions over the sample values of X that are informa-
tive about Y, by finding the bottleneck random vector T through the unknown distribution
that minimizes the information bottleneck function
(10.169)
subject to the requirement that T is dependent on X and Y is dependent on T and subject to
normalization constraints.
The positive Lagrange multiplier  is the tradeoff parameter between compression
(minimal representation) and predictability (information preservation). By varying this
parameter between zero and infinity, we obtain a concave information curve, analogous
to the rate distortion function, that provides the optimal achievable tradeoff between
compression and prediction.
EXAMPLE 11
Gaussian Information Bottleneck
For an analytic treatment of the information bottleneck method, we may consider a coupled pair
of eigenvector problems for the logarithmic functional derivatives:
and
To get around the difficulty of solving these problems in general, we turn to the analytically
tractable case when the original random vector X and its compressed version Y are described by
joint multivariate Gaussian distributions, as in Chechik et al. (2004). In this Gaussian framework,
solving the coupled pair of eigenvector problems lends itself to the canonical correlation analy-
sis (CCA), which, as mentioned in Section 10.10, is a special case of the Imax principle. We thus
find that the issue to be resolved is that of finding a linear projection onto a subspace, the dimen-
sionality of which is determined by the tradeoff parameter . In particular, as the parameter 
increases, additional dimensions (i.e., eigenvalues) are added to the projection (bottleneck) vector
T; this addition manifests itself through a series of critical points, or structural phase transitions,
while at the same time the relative Euclidean norm of each basis vector is rescaled. The process
of dimension-expansion is continued until all the relevant information about the compressed vector


0
0t log pYT(yt)
0
0t log pXT(xt)
J(qTX(tx)) = I(X; T) - I(T; Y)
qTX(tx)
Section 10.19
Rate Distortion Theory and Information Bottleneck
551

Y is captured in the bottleneck vector T.The net result of the process is an insightful demonstra-
tion of how the information bottleneck method provides a continuous measure of model com-
plexity for varying 
in information-theoretic terms.
For the Gaussian framework studied in Chechik et al. (2004), Fig. 10.21 plots the mutual
information I(T; Y) versus the mutual information I(T; X) for varying .The information curve,
shown as the continuous and smooth curve in red in Fig. 10.21, was obtained for four eigenvalues:
Correspondingly, the critical points are designated by small circles in the
figure. The information curve (passing through these critical points) is constructed from several
segments, realizing that as the mutual information I(T; X) increases, additional eigenvectors are
used in the projection.For the purpose of comparison,Fig.10.21 also shows the information curves
calculated with small number of eigenvectors for each .
From the results plotted in Fig. 10.21, we see that the information curve for the Gaussian
information bottleneck method is indeed concave everywhere.At each value of the mutual infor-
mation I(T; X), the information curve is bounded by a tangent with a slope defined by the func-
tion
1(I(T; X)). At the origin, I(T; X)  0, the slope 
1(0)  1
1, where 
1 is the first
eigenvalue of the canonical correlation analysis of the original random vector X and its com-
pressed version Y. Note also that the asymptotic slope of the information curve is zero, as 
This asymptotic behavior simply reflects the reality of the law of diminishing returns:The addition
of more bits of information to the description of the original random vector X does not provide
increased accuracy for the bottleneck vector T.
■
 S q.




i = 0.1, 0.5, 0.7, 0.9.


552
Chapter 10
Information-Theoretic Learning Models
1  1
1
I(T; Y)
1.0
5
10
15
20
25
I(T; X)
FIGURE 10.21
The information curve for multivariate Gaussian variables.
The envelope (red curve) is the optimal compression–prediction tradeoff,
captured by varying the Lagrange multiplier  from zero to infinity.The
slope of the curve at each point is given by 1/.There is always a critical
lower value of  that determines the slope at the origin, below which there
are only trivial solutions.The suboptimal (black) curves are obtained when
the dimensionality of T is restricted to fixed lower values. (This figure is
reproduced with the permission of Dr. Naftali Tishby.)

Section 10.20
Optimal Manifold Representation of Data
553
I(T;Y)
I(X;T)
I(X;Y)
X
Y
T
qTX(tX)
qYT(yT)
FIGURE 10.22
An illustration of the information
bottleneck method.The bottleneck T captures the
relevant portion of the original random vector X with
respect to the relevant variable Y by minimizing the
information I(X;T) while maintaining I(T;Y) as high as
possible.The bottleneck T is determined by the three
distributions qTX(tX), qT(t), and qYT(Yt), which
represent the solution of the bottleneck equations
(10.170) to (10.172).
Information Bottleneck Equations
The solution of the information bottleneck optimization problem is given through the
following bottleneck equations for distributions of the vector T:
(10.170)
(10.171)
(10.172)
In Eq. (10.170), Dp||q denotes the Kullback–Leibler divergence between the two condi-
tional probability density functions pYX(yx) and qYT(yt), and Z(x,) is a normalization
(partition) function. Figure 10.22 illustrates the idea of information bottleneck in light
of these three equations.
The system of Eqs. (10.170) through (10.172) must be solved self-consistently for
the three unknown distributions qTX(tx), qT(t), and 
.Tishby et al. (1999) showed
that, by iterating these equations from a random starting distribution in a manner sim-
ilar to the Blahut–Arimoto iterations for rate distortion theory, the equations converge
to an optimal solution for any value of the parameter .
The information bottleneck problem can be solved to obtain relevant continuous
manifolds (dimension reduction), as was shown Chechik et al. (2004) for Gaussian vari-
ables or as discussed in the next section, in accordance with Chigirev and Bialek (2004).
10.20
OPTIMAL MANIFOLD REPRESENTATION OF DATA
In Chapter 7, we discussed the unsupervised manifold representation of data from the
perspective of regularization, using spectral graph theory. In this section, we revisit the
same problem, but this time we approach it from an information-theoretic perspective.
More specifically, the approach taken here follows that of Chigirev and Bialek (2004),
which builds on the following insight:
There is analytic benefit to be gained by treating dimensionality reduction as a data com-
pression problem.
The Chigirev–Bialek approach to data representation is, in fact, a clever application of
the information bottleneck method discussed in the previous section.
qY|T(yt)
qYT(yt) = a
X
qYT(yt)qTX(tx) a
pX(x)
qT(t) b
qT(t) = a
X
qTX(tx)pX(x)
qTX(tx) =
qT(t)
Z(x, ) exp(-Dpq)

Dimensionality Reduction Viewed as Data Compression:
The Basic Equations
From the discussion presented in Chapter 7, we recall that, in intuitive terms, a manifold
refers to a k–dimensional continuous region (e.g., a curved line or a surface) that is
embedded in an m-dimensional Euclidean space, where k is smaller than m. In the man-
ifold representation of data, we visualize a manifold of dimensionality lower than the
data.In particular,we say that the manifold describes the data “almost perfectly,”because
of the unavoidable presence of additive noise and other forms of data degradation.
Let m denote a manifold of dimensionality k and qm() denote the probability
density function of points on the manifold;  represents one such point. Let X denote
a random data vector of dimensionality m greater than k, which, in effect, implies that
the data set x represented by X is sparse. Indeed, it is the sparseness of the data set that
makes its unsupervised representation a challenging task. Let qmX(x) denote the con-
ditional probability density function of points on the manifold, given the data set x.
Thus, the stochastic map
(10.173)
describes the mapping of x into .
The manifold is described by the doublet {m, Pm}, which embodies a “less than
faithful representation” of the data set x, substantiating a similar remark made above.
Stated in another way, we may say that the vector , denoting a point on the manifold
m, is a distorted version of the data point x—hence the need for a distance measure that
we denote by d(x, ). To simplify matters, we adopt a Euclidean distance function for
this measure, as shown by
(10.174)
which is commonly used.The expected distortion is thus defined by the double multidi-
mensional integral
(10.175)
where px(x) is the probability density function of the data set x, whose sample value is
denoted by the data point x.
Equation (10.175) is one important aspect of the data compression problem.The
second important aspect is the mutual information between the manifold m and the
data set x, which is defined by
(10.176)
When the logarithm is to base 2,this mutual information defines the number of bits required
to encode a data point x into a point  on the manifold m. Furthermore, by viewing the
dimensionality reduction as a data compression problem,I(x; m) defines the “capacity”of
a channel required to transmit the compressed data ,given the data vector x treated as input.
I(x; m) =
3
q
-q 3
q
-q
px(x)qmx(x) log a
qx(x)
qm()
b dxd
[d(x, )] =
3
q
-q 3
q
-q
px(x)qmX(x)7x - 7 2dxd
d(x, ) = 7x - 7 2
Pm: x S qmx(x)
554
Chapter 10
Information-Theoretic Learning Models
µ
Joint pdf

When viewed together,Eqs.(10.175) and (10.176) present us with a tradeoff involv-
ing two basic issues:
1. For a “faithful” manifold representation of the data, we need to minimize the
expected distortion defined in Eq. (10.175).
2. On the other hand, for a “good” compression of the data into points on the man-
ifold, we need to maximize the mutual information defined in Eq. (10.176).
To resolve this tradeoff, we introduce the concept of an optimal manifold (Chigirev and
Bialek, 2004):
Given a data set x and a channel capacity I(x; m), the manifold m is said to be an optimal
representation of the data set x provided that two conditions are satisfied:
(i) The expected distortion
is minimized.
(ii) Only the number of bits defined by the channel capacity I(x; m) is required to represent
the data point x.
Another way of defining an optimal manifold is to say the following:
The manifold m is optimal if the channel capacity I(x; m) is maximized while the expected
distortion is fixed at some prescribed value.
Either way, we are faced with a problem in rate distortion theory. Since this prob-
lem is a constrained-optimization problem in light of the discussion presented in Section
10.19, we introduce a Lagrange multiplier 
 that accounts for the tradeoff between the
expected distortion and the channel capacity, as shown by the Lagrangian
(10.177)
To find the optimal manifold, this functional must be minimized.
To do this minimization in analytic terms, we need to parameterize the manifold.
Following the information bottleneck method described in Section 10.19, we introduce
the bottleneck vector T, a sample value of which is denoted by t
l, where the new
dimension l is less than or equal to the dimension m of the data vector x. We also intro-
duce a new vector-valued function
(10.178)
that maps the point t in the parameter space spanned by the bottleneck vector T onto
the manifold m. The vector-valued function (t) is therefore a “descriptor” of the man-
ifold m. We also assume that the dimension of (t) is the same as the dimension of the
data point x, so that we may use the squared Euclidean distance 
as the new
measure of the distortion incurred in using the manifold m as the representation of the
data set x.
In light of the manifold parameterization just described, we recast our two basic
equations of Eqs. (10.175) and (10.176) in the new respective forms
(10.179)
[d(x, (t))] =
3
q
-q 3
q
-q
pX(x)qTX(tx)7x - (t)7 2dxdt
7x - (t)7 2
(t): t S m
 
F(m, Pm) = [d(x, )] + 
I(x; m)
 [d(x, )]
Section 10.20
Optimal Manifold Representation of Data
555

556
Chapter 10
Information-Theoretic Learning Models
and
(10.180)
Correspondingly, the functional F of Eq. (10.177) is rewritten in the new form
(10.181)
The expected distortion and the channel capacity in this latter equation are both intrinsic
properties of the manifold described by the doublet {m, Pm}, and the properties are
invariant under reparameterization.
With Eqs. (10.179) to (10.181) at hand, the stage is now set to find the optimal
manifold.We do so by applying the following two conditions of optimization:
1.
(10.182)
2.
(10.183)
Thus, applying condition 1, we obtain
which leads to the following pair of equations that are consistent in probabilistic
terms:
(10.184)
(10.185)
The derivation of this pair of equations hinges only on the expected-distortion compo-
nent of the functional F, as it is only this component that depends on (t)—hence the
absence of the Lagrange multiplier 
.
However,when we go on to apply the second optimizing condition described in Eq.
(10.183), we have to recognize that this optimization involves all possible values of the
conditional qTX(tx) under the constraint
This constraint is merely the requirement that the area under the curve of qTX(tx) be
unity, which is a fundamental property of every probability density function.To satisfy
this additional constraint, we introduce the new Lagrangian multipliers (x) for all x
and thus expand the definition of the functional F to obtain
3
q
-q
qTX(tx)dt = 1  for all x
qT(t) =
3
q
-q
pX(x)qTX(tx)dx
(t) =
1
qT(t) 3
q
-q
xpX(x)qTX(tx)dx
3
q
-q
pX(x)qTX(tx)(-2x + 2(t))dx = 0
0F
qTX(tx) = 0   for (t) fixed
0F
0(t) = 0  for qTX(tx) fixed 
F((t), qTX(tx)) = [(tx, (t))] + 
I(X; T)
I(X; T) =
3
q
-q 3
q
-q
pX(x)qTX(tx) log a
qTX(tx)
qT(t)
bdxdt

Now, setting
(10.187)
and solving the resulting equation for the desired conditional qTX(tx), we get a second
pair of equations that are also consistent in probabilistic terms:
(10.188)
and
(10.189)
The function Z(x, 
) plays the role of a normalization (partition) function in that its
inclusion in Eq. (10.188) ensures that the constraint imposed on 
is satisfied, as it
should be.
The collectivity of Eqs. (10.184), (10.185), (10.188), and (10.189) describes the
optimal manifold for data representation in an unsupervised manner.This description nat-
urally requires knowledge of the continuous probability density function pX(x).
Discretization Process
In practice, however, we only have a training sample h denoted by {xi}N
i1, where N
is the sample size. In light of this practical reality, we introduce the discrete approxi-
mation
(10.190)
where (·) denotes the Dirac delta function. Correspondingly, we model the manifold m
by the discrete set
(10.191)
t = {tj}L
j=1
pX(x) L 1
N a
N
i=1
(x - xi)
qT(t)
Z(x, 
) =
3
q
-q
qT(t) exp a- 1

 7x - (t)7 2 bdt
qTX(tx) =
qT(t)
Z(x, 
) exp a- 1

 7x - (t)7 2 b
(x)

pX(x) = log Z(x, 
)
1

 7x - (t)7 2 + log a
qTX(tx)
qT(t)
b +
(x)

pX(x) = 0
Section 10.20
Optimal Manifold Representation of Data
557
(10.186)
where qT(t) is as previously defined in Eq. (10.185).
Thus, applying the second optimizing condition of Eq. (10.183) to this new formu-
lation of the functional F and simplifying terms through the use of Eq.(10.185),we obtain
+ 
pX(x)qTX(tx)log a
qTX(tx)
qT(t)
b + (x)qTX(tx)}dtdx f
F((t), qTX(tx)) =
3
q
-q 3
q
-q
epX(x)qTX(tx)7x - (t)7 2

Then, noting that the sample value t of the bottleneck vector T appears only in the argu-
ments of the function (t), the conditional qTX(tx), and the marginal qT(t), we may
replace these three continuous functions by their respective discrete counterparts j,
qj(xi),and qj,where the indices i and j are included to emphasize the discretization process.
To complete the discretization process, we introduce  to denote the coordinate index
in the Euclidean space m.
Now that we have a discrete model for the manifold,the goal is to develop an algo-
rithm for computing the model in an iterative manner. To this end, we first note that
Eqs. (10.188) and (10.189), defining qTX(tx) and Z(x, 
), respectively, are both convex
functions of their respective variables t and x; the Lagrange multiplier is a prescribed
parameter. In a computational context, these two equations are the difficult part of the
manifold’s discrete model.
To probe further into how we can mitigate this computational difficulty, consider
the two convex sets a and b depicted in Fig. 10.23. We would like to minimize the
Euclidean “distance” between them; the distance is denoted by d(x, y), where x and y
are any two points in the sets a and b, respectively.An intuitively obvious way of min-
imizing the Euclidean distance d(x, y) is to do the following (Csiszát and Tusnády, 1984):
Fix the point x in set a, and find the point y in set b that is closest to it. Then fix the newly
found point y, and find the point x in the set a that is closest to it.
If we continue this process by going back and forth between the sets a and b, as illus-
trated in Fig. 10.23, it would seem plausible that the distance d(x, y) would become
smaller after each iteration of this alternating process. This is exactly what is done in
the Blahut–Arimoto algorithm for minimizing the rate distortion function (Blahut, 1972;
Arimoto, 1972). Equations (10.188) and (10.189) have the same mathematical form as
those found in the characterization of the rate distortion function (Cover and Thomas,
2006). Moreover, in Csiszár and Tusnády (1984), it is shown that the alternating process
between the two convex sets a and b will converge, provided that they are both sets of
probability distributions and that the distance measure is the Kullback–Leibler diver-
gence between the two distributions.
Iterative Algorithm for Computing the Optimal Manifold
Representation of Data
In light of these reassuring findings, we may go on to formulate an iterative algorithm
for computing the discrete model of the manifold m. Let n denote the time-step in the
iterative algorithm.Then, using the discrete versions of Eqs. (10.184), (10.185), (10.188)

558
Chapter 10
Information-Theoretic Learning Models
Set
a
Set
b
FIGURE 10.23
Illustrating the alternating process of computing the distance between two
convex sets a and b.

(10.192)
(10.193)
(10.194)
(10.195)
where 
refers to the th element of the data vector .
To initialize the algorithm, we randomly pick L points from the data set x and let
(10.195)
To terminate the computation, let 
denote the precision with which the manifold
points are to be located.The algorithm is terminated at time step n once the condition
is reached.
The one remaining parameter that needs to be set is the Lagrange multiplier 
,
which determines the tradeoff between the expected distortion and the channel capac-
ity involved in the functional F. This parameter is under the designer’s control, depend-
ing on how this tradeoff is to be realized.
Practical Considerations
The algorithm for computing the optimal manifold representation of data, described in
Eqs.(10.192) to (10.195),is designed to constrain the mutual information between points
on the manifold and those in the original data space. This constraint is invariant with
respect to all invertible coordinate transformations in either of these two spaces—a
condition that may enforce smoothness of the manifold in an implicit sense (Chigirev and
Bialek, 2004). In a theoretical framework, it may be argued that the justification for a
smooth manifold using an information-theoretic approach is not as rigorous as that for
an approach rooted in regularization theory. Nevertheless, the optimal manifold repre-
sentation of data appears to work satisfactorily in practice.
Most importantly, unlike that for other dimensionality-reduction procedures (eg.,
the Belkin–Niyogi procedure based on regularization and spectral graph theory, as
described in Chapter 7), the convergence time of the information-theoretic algorithm
described in this section is linear in the sample size N. This highly desirable feature of the
max
j
j(n) - j(n - 1) 6 

j = 1, 2, ..., L
j = xi,j
pj(0) = 1
L
¶
xi

xi, 
pj(xi, n + 1) =
pj(n)
Z(xi,
,n) exp a- 1

 7xi - j(n)7 2 b
Z(xi, 
, n) = a
L
j=1
pj(n) exp a - 1

 7xi - j(n)7 2 b
j, (n) =
1
pj(n)  1
N a
N
i=1
xi, pj(xi, n),  = 1, 2, ..., m
pj(n) = 1
N a
N
i=1
pj(xi, n)
Section 10.20
Optimal Manifold Representation of Data
559
and (10.189) and adopting an L-point discrete set 
to model the manifold
represented by the continuous variable t, we may now formulate the desired algorithm
based on the following set of four equations, where the time-step n  0, 1, 2, ..., and
the index j  1, 2, ..., L. (Chigirev and Bialek, 2004):
{t1, t2, ..., tL}

algorithm, attributed to the inherently convex nature of the equations describing the
manifold, makes its application rather attractive, particularly when we are challenged to
tackle the difficult task of dimensionality reduction for large datasets in practice.
Other highly desirable features of the algorithm include the following include the
following two:
• Knowledge of the dimensionality of the manifold under consideration is not required.
• The algorithm is well suited for handling the dimensionality reduction of sparse
data, which is important because, in high-dimensional spaces, all datasets are
typically sparse.
10.21
COMPUTER EXPERIMENT: Pattern Classification
This computer experiment addresses the combined use of two algorithms: first, the opti-
mal manifold representation of the input data for unsupervised clustering and second,
the least-mean-square (LMS) algorithm for supervised classification that was discribed
in Chapter 3. Though different in application, these two algorithms share two useful
properties: effective performance and computational efficiency.
To study the performance of the combined “optimal manifold–LMS” algorithm, we
again use data drawn randomly from the double-moon configuration of Fig.1.8,with the ver-
tical separation between the two moons fixed at d = 6.Figure 10.24 presents the results of
560
Chapter 10
Information-Theoretic Learning Models
12
10
8
6
4
2
x2
0
2
4
6
10
5
0
5
x1
10
15
20
FIGURE 10.24
Pattern classification of the double-moon configuration of Fig. 1.8, using the
optimal manifold + LMS algorithm with distance d  6 and 20 centers.

the experiment, computed with 20 centers shared almost equally between the two moons.
The decision boundary, constructed by the algorithm under supervised training with 300
data points,separates data drawn from the two moons in an “almost flawless”manner.More
precisely, there were 6 classification errors out of 2,000 test data points, representing a mis-
classification error rate of 0.3 percent. This performance is close to the error-free perfor-
mance of the support vector machine (SVM) for the same setting of the double-moon
configuration, which was reported in Section 6.7. The important point to take away from
this comparison is that the close performance of the “optimal manifold–LMS” algorithm
was achieved at a fraction of the computational complexity experienced with the SVM.
10.22
SUMMARY AND DISCUSSION
In this rather long chapter, we established Shannon’s information theory as a basic sta-
tistical tool for the study of many important aspects of self-organized or unsupervised
learning—a truly remarkable achievement.
Mutual Information as the Objective Function of Self-organization
Shannon’s mutual information between input and output stochastic processes has some
unique properties that commend its adoption as the objective function to be optimized for
self-organized learning. Indeed, some important principles of self-organization have
emerged from the study presented in this chapter:
1. The Infomax principle, which involves maximization of the mutual information
between the multidimensional input and output vectors of a neural network.This
principle lays down the framework for the development of self-organized models
and feature maps.
2. The principle of minimum redundancy, which is basically another way of saying that
maximization of the mutual information between the input and output of a network
leads to the minimization of redundancy.
3. The Imax principle, which consists of maximizing the mutual information between
the single outputs of a pair of neural networks driven by two spatially shifted mul-
tidimensional input vectors. This second principle is well suited for image pro-
cessing in which the objective is the discovery of properties of a noisy sensory
input that exhibits coherence across both space and time.
4. The Imin principle, which consists of minimizing the mutual information between
the single outputs of a pair of neural networks driven by two spatially shifted mul-
tidimensional input vectors.This principle finds application in image processing in
which the objective is to minimize the spatiotemporal coherence between two cor-
relatal images of an environment that are obtained through the use of a pair of sen-
sors through orthogonal properties.
Two Fundamental Routes for Independent-Components Analysis
Another important topic discussed in the chapter was that of independent-components
analysis (ICA), which provides a mathematical basis on which the components of a
Section 10.22
Summary and Discussion
561

random vector are made as statistically independent as possible in a strong sense. This
principle finds application in solving the blind source separation (BSS) problem, the
necessary conditions for which are as follows:
• statistically independent sources of signals;
• non-Gaussian source signals, except for one which is permitted to be Gaussian
distributed;
• square mixing matrix, which means that the source signals and the observables
are equal in number;
• noise-free mixing model.
Basically, there are two routes for deriving ICA algorithms:
1. The principle of independent-components analysis (Comon, 1994). Building on the
Kullback–Leibler divergence;this principle leads to the formulation of an expected
cost function that depends on two distributions:
• parameterized probability density function of a demixer’s output;
• corresponding factorial distribution.
Application of the principle of independent-components analysis manifests itself
in two well-known algorithms:
(i)
The natural-gradient algorithm for ICA, which is due to Amari et al. (1996).
(ii)
The Infomax principle for the ICA algorithm, which is due to Bell and
Sejnowski (1995).
The main virtue of these two algorithms is their ability to adapt to statistical
variations of the environment.They are also capable of delivering a robust per-
formance, provided that the right type of activation function is used, depending
on whether the original sources are super-Gaussian or sub-Gaussian distribted.
2. The principle of maximum negentropy (Comon, 1994). The notion of negentropy
provides a measure of the non-Gaussianity of a random variable. The statistical
independence of a set of components is realized through the minimization of
negentropy. Application of this second principle leads to the formulation by the
FastICA algorithm due to Hyvärinen and Oja (1997). Attractive features of the
FastICA algorithm include the following:
• fast rate of convergence;
• absence of learning-rate parameter;
• robstness,regardless of whether the sources are super-Gaussian or sub-Gaussian
distributed;
• simplicity of implementation.
However,due to the lack of a learning-rate parameter,the FastICA algorithm does
not have the ability to track time-varying mixtures.
A question that begs itself in the context of the three different ICA algorithms
highlighted here is the following:
What are the connections among mutual information, entropy and non-Gaussianity in a large
ICA framework, without one’s resorting to a somewhat arbitrary decorrelation constraint?
562
Chapter 10
Information-Theoretic Learning Models

In addressing this fundamental question in ICA theory, Cardoso (2003) provides a
great deal of mathematical insight insofar as the issues of statistical dependence,
correlation, and Gaussianity are concerned. The following is a key result reported in
Cardoso’s paper:
When the prewhitening requirement is relaxed,the Kullback–Leibler divergence can be decom-
posed, under linear transforms, as the sum of two terms: One term expresses the decorrelation
of the components, and the other expresses their non-Gaussianity.
By restricting attention to linear transforms, ICA, in effect, permits the non-Gaussian
component to express itself only in the marginal distributions.
Some more comments on ICA and BSS are in order.The two concepts are closely
related to each other, so much so that frequently one of them is used when the other
is really meant. Most importantly, ICA and BSS constitute an ever-expanding field,
both in theoretical terms and practical applications.This statement is substantiated by
an impressive list of topics, each of which has a practical distinctive direction of its own.
(See Note 22 in the List of Notes and References.)
Coherent ICA
One other ICA-related principle discussed in this chapter was that of coherent ICA
(Kan, 2007; Haykin and Kan, 2007).This new principle combines properties of the Info-
max and Imax principles to maximize the spatiotemporal coherence across the outputs
of a pair of multiple-input, multiple output (MIMO) networks of similar dimensionali-
ties when the networks are driven by spatially shifted data streams. Using real-life data,
two important results in the context of auditory encoding of natural sounds have been
demonstrated:
(i) Coherent ICA is capable of exhibiting amplitude-modulation tuning, thereby sup-
porting the notion that envelope processing is embedded in the auditory system.
(ii) Coherent ICA is capable of learning the varying rates at which two successive pro-
cessing layers of filters respond to acoustic stimuli in a manner that mimics what
goes on in the hierarchical auditory system.
Information Bottleneck
In one form or another, all the information-theoretic principles of self-organization
summarized thus far build on the concepts of entropy and mutual information that are
basic to Shannon’s classic information theory. In the latter part of the chapter, we used
rate distortion theory—another concept basic to Shannon’s information theory—to
formulate our last principle of the chapter: the information bottleneck method (Tishby
et al., 1999; Slonim et al., 2006). Following are two important aspects of the method
that should be emphasized:
1. The information bottleneck method is not a statistical modeling algorithm; rather,
it is a method for finding relevant representations of complex data that can explain
the underlying structure and the statistical correlation of interest between a given
set of variables.
Section 10.22
Summary and Discussion
563

2. Although the method assumes knowledge of the joint distribution pX,Y(x,y) between
the input vector X and output vector Y, it is applied in practice to empirical distrib-
utions based on finite samples.This plug-in approach is fully justified in Shamir et al.
(2008), where theorems on learning, generalization, and consistency are presented.
Equipped with the information bottleneck method, we used it to derive the optimal
manifold representation of data (Chigirev and Bialek, 2004). The algorithm for imple-
menting this representation has some useful properties:
• The computational complexity of the algorithm is linear in the size of the training
sample.
• The algorithm does not require knowledge of the manifold’s dimensionality.
• The algorithm is well suited for dealing with high-dimensional data, which are typ-
ically sparse.
A concluding comment is in order: The breadth and depth of the material sum-
marized in this section is testimony to the remarkable impact that Shannon’s informa-
tion theory, originally developed for communication systems, has had on unsupervised
learning models and their applications.
NOTES AND REFERENCES
1. Shannon’s Information Theory
For detailed treatment of information theory, see the book by Cover and Thomas (2006).
For a collection of papers on the development of information theory (including the 1948
classic paper by Shannon), see Slepian (1973). Shannon’s paper is also reproduced, with
minor revisions, in the books by Shannon and Weaver (1949) and Sloane and Wyner (1993).
For a brief review of the important principles of information theory with neural pro-
cessing in mind, see Atick (1992). For a treatment of information theory from a biology
perspective, see Yockey (1992).
2. For a review of the literature on the relation between information theory and perception,
see Linsker (1990b) and Atick (1992).
3. Entropy
The term “entropy,” in an information-theoretic context, derives its name from analogy
with entropy in thermodynamics; the latter quantity is defined by
where kB is Boltzmann’s constant and p is the probability that the system is in state  (see
Chapter 11). Except for the factor kB, the formula for entropy H in thermodynamics has
exactly the same mathematical form as the definition of entropy given in Eq. (10.8).
4. Maximum-Entropy Principle
In Shore and Johnson (1980), it is proved that the maximum-entropy principle is correct in
the following sense:
Given prior knowledge in the form of constraints, there is only one distribution sat-
isfying these constraints that can be chosen by a procedure that satisfies the “consis-
tency axioms”; this unique distribution is defined by maximizing entropy.
H = -kB a

p log p
564
Chapter 10
Information-Theoretic Learning Models

The consistency axioms are fourfold:
I.
Uniqueness:The result should be unique.
II.
Invariance:The choice of coordinates should not affect the result.
III.
System independence: It should not matter whether independent information about
independent systems is accounted for separately in terms of different densities or
together in terms of a joint density.
IV.
Subset independence: It should not matter whether an independent subset of system
states is treated in terms of a separate conditional density or in terms of the full system
density.
In Shore and Johnson (1980), it is shown that the relative entropy or the Kullback–Leibler
divergence also satisfies the consistency axioms.
5. Pythagorean Decomposition
To prove the decomposition of Eq.(10.43),we may proceed as follows.By definition we have
(A)
From the definitions of 
and pU(x), we see that
Let I denote the integral in the last line of Eq. (A).We may then write
(B)
= a
m
i=1 3
q
-q
log a
p
Xi(xi)
pUi(xi) bp

Xi(xi) dxi
= a
m
i=1 3
q
-q
a log a
p
Xi(xi)
pUi(xi) b
3
q
-q
pX(x) dx(i) b dxi
=
3
q
-q
pX(x) log ±
q
m
i=1
pXi(xi)
q
m
i=1
pUi(xi)
≤dx
I =
3
q
-q
pX(x) log a
p
X(x)
pU(x) b dx
= a
m
i=1
 log a
p
Xi(xi)
pUi(xi) b
 log a
p
X(x)
pU(x) b = log ±
q
m
i=1
p
Xi(xi)
q
m
i=1
pUi(xi)
≤
p
X(x)
= DpX7p
X +
3
q
-q
pX(x) log a
p~
X(x)
pU(x) b dx
=
3
q
-q
pX(x) log a
pX(x)
p
X(x)
b dx +
3
q
-q
pX(x) log a
p~
X(x)
pU(x) b dx
=
3
q
-q
pX(x) log a
pX(x)
p
X(x)
b  a
p~
X(x)
pU(x) b dx
DpX7pU =
3
q
-q
pX(x) log a
pX(x)
pU(x) b dx
Notes and References
565

where, in the last line, we have made use of the defining equation (10.39). The integral in
Eq. (B) is the Kullback–Leibler divergence 
for i = 1, 2, ..., m. To put the expression
for Eq. (B) in its final form, we note that the area under 
is unity, and therefore write
(C)
where in the first line we have used the definition dx = dxidx(i) as described in Section 10.5.
Thus, substituting Eq. (C) into (A), we obtain the desired decomposition:
6. Copulas
The word copula is Latin for “link” or “bond”; it is also used in grammar and logic to refer
to the part of a proposition that connects a subject and predicate (Nelsen, 2006). In the
mathematical literature, the term was first used by Sklar (1959) in the theorem that bears
his name: Sklar’s theorem describes the formation of multivariate distribution functions by
“joining together” one-dimensional distribution functions. Nelsen’s book provides an inter-
esting historical perspective on copulas and also describes their basic properties, followed
by methods for constructing copulas and the role of copulas in modeling and the study of
statistical dependence. A detailed list of references on copulas and related issues is given
at the end of Nelsen’s book.
7. Nadal and Parga (1994, 1997) discuss the relationship between Infomax and redundancy
reduction, reaching a similar conclusion that maximization of the mutual information
between the input vector and output vector of a neural system leads to data reduction. Haft
and van Hemmen (1998) discuss the implementation of Infomax filters for the retina. It is
shown that redundancy is essential to the attainment of noise robustness of an internal rep-
resentation of the environment as it is produced by a sensory system such as the retina.
8. Canonical Correlation Analysis
The theory of canonical correlation analysis was originally developed by Hotelling (1935,
1936).To describe the theory, we follow the treatment presented in Anderson (1984).
Consider a zero-mean random vector X consisting of m components and characterized
by an m-by-m covariance matrix . Let X be partitioned into two subvectors Xa and Xb
consisting of components ma and mb, respectively. Correspondingly, the covariance matrix
is partitioned as
= c aa
ab
ba
bb
d
= c [XaXT
a]
[XaXT
b]
[XbXT
a]
[XbXT
b] d
=  c a Xa
Xb
b(Xa, Xb)Td
 =  CXXTD


DpXpU = DpXp
X + DpXp
U
= Dp~
XpU
=
3
q
-q
p
X(x)log ±
q
m
i=1
p
Xi(xi)
q
m
i=1
pUi(xi)
≤dx
I = a
m
i=1 3
q
-q q
m
j=1
p
Xj(xj) alog a
p
Xi(xi)
pUi(xi) b dxi b dx(i)
f

Xj(xj)
Dp
xi7 pui
566
Chapter 10
Information-Theoretic Learning Models

where
The objective of canonical correlation analysis (CCA) is to formulate linear transformations
of the subvectors Xa and Xb so as to clearly exhibit the intercorrelations between the trans-
formed random variables in a maximal manner.To this end,consider the linear transformations 
and
where Ya and Yb are both zero-mean random variables and the ma-by-1 vector wa and
mb-by-1 vector wb are the basis vectors to be determined. Since the cross-correlation func-
tion of a multiple of Ya and a multiple of Yb is the same as the cross-correlation function of
Ya and Yb by themselves,we may require that the weight vectors wa and wb be chosen in such
a way that both Ya and Yb have unit variance.This requirement leads to the following two
conditions:
(A)
and
(B)
With this introductory material at hand, we may now state the problem at hand:
Find the weight vectors wa and wb so as to maximize the cross-correlation function
subject to the two conditions expressed under Eqs. (A) and (B)
To solve this constrained-optimization problem, we use the method of Lagrange multi-
pliers, for which we write the Lagrangian
where a and b are Lagrange multipliers and the factors 1/2 are introduced to simplify the
presentation. Differentiating the Lagrangian J(wa, wb) with respect to wa and wb and setting
the results equal to zero yields the following pair of equations:
(C)
and
(D)
Multiplying the left-hand sides of Eqs. (C) and (D) by 
and 
, respectively, we obtain
(E)
and
(F)
wT
b bawa - bwT
b bbwb = 0
wT
a abwb - awT
a aawa = 0
wT
b
wT
a
bawa - bbbwb = 0
abwb - aaawa = 0
J(wa, wb) = wT
a abwb - 1
2 a(wT
a aawa - 1) - 1
2 b(wT
b bbwb - 1)
[YaYb] = [wa
TXaXa
Twb] = wT
a abwb
1 = [Y2
b] = [wb
TXbXb
Twb] = wb
Tbbwb
1 = [Y2
a] = [wa
TXaXa
Twa] = wa
Taawa
Yb = wT
bXb
Ya = wa
TXa
ba = T
ab
Notes and References
567

Next,invoking the conditions of Eqs.(A) and (B) in Eqs.(E) and (F),respectively,shows that
(G)
where we have made use of the relationship 
. Hence, the two Lagrange multi-
pliers in the Lagrangian J(wa, wb) assume a common value, hereafter denoted by .
Moreover, recognizing that the variances of both Ya and Yb are normalized to unity, it
follows from Eq. (G) that the Lagrange multiplier  is the canonical correlation between
these two random variables.
The key question now is: How do we determine the basis vectors wa and wb? Using Eqs.
(C) and (D), we may readily show that the basis vectors wa and wb are respectively defined
by the pair of eigenequations
(H)
and
(I)
where
(J)
We may therefore make two statements:
1. The eigenvalue 
 of the matrix Ca is equal to the squared value of the canonical corre-
lation, and the associated eigenvector defines the basis vector wa.
2. The eigenvalue 
 of the second matrix Cb is also equal to the squared value of the
canonical correlation, and the associated eigenvector defines the second basis
vector wb.
Note, however, that the number of meaningful solutions of the eigenequations (G), (H),
and (I) is limited by the dimension ma or mb, whichever is smaller.The largest eigenvalue,

1, yields the strongest canonical correlation; the next eigenvalue, 
2, yields the second-
strongest canonical correlation, and so on.
Canonical correlation analysis (CCA), as described herein, can be used to reveal second-
order statistical dependencies between two related, but different, datasets. Even though,
CCA does not include higher-order statistics, it often performs well in practice.
From Eqs. (H) and (I), it is also apparent that canonical correlation analysis includes
principal components as a special case that occurs when the matrices Ca and Cb are assigned
a common value—that is , when the subectors Xa and Xb are one and the same.
It is also of interest to note that, in Fyfe (2005), two different neural implementations of
canonical correlation analysis are presented, supported by simulations using artificial and
real-life data.
9. Uttley’s Informon
In Uttley (1970, a negative information pathway is considered by optimizing the negative of
the mutual information between the signals at the input and the output of the pathway. It
is shown that such a system adapts to become a discriminator of the more frequent pattern
occurring in the set of input signals during adaptation. The model is called “informon,”
which is loosely related to the Imin principle.

 = 2
Cb
-1
bb ba-1
aa ab wb = 
wb
Ca
-1
aa ab-1
bb ba wa = 
wa
ba = T
ab
a = b = wT
aabwb
568
Chapter 10
Information-Theoretic Learning Models
µ
µ

10. Fuzzy Imin Processor
The system described in Ukrainec and Haykin (1996) includes a postdetection processor that
uses a priori information about the reflector location along the water-land boundary of the
waterway.A fuzzy processor combines primary detection performance with the output from
a vision-based edge detector to effectively remove false alarms, thereby resulting in a fur-
ther improvement in system performance.
11. Historical Notes
Two seminal papers on blind source separation and independent-components analysis are
widely recognized in the literature:
• The paper by Herault et al. (1985) on blind source separation (BSS) using Hebbian
learning.
• The paper by Comon (1994) on independent-components analysis (ICA), where this term
was coined for the firs time.
For a detailed historical account of BSS and ICA,including several other early contributions,
see Jutten and Taleb (2000).
12. Natural Gradient
The idea of using 
instead of the usual gradient 
for solving the source
separation problem is described in Cardoso and Laheld (1996).Therein,
is referred to
as the relative gradient. This gradient is exactly the same as the natural gradient, the defin-
ition of which follows from an information-geometric perspective (Amari, 1998; Amari
et al. 1996).
13. Riemannian Space
In the Riemannian space of dimension n, for example, the squared norm of a vector a is
defined by
where the gij are functions of the coordinates x1, x2, …, xn of the Riemannian space, gij = gji,
and the right-hand side of this expression is always positive. This expression is a general-
ization of the Euclidean formula for a squared norm:
For a discussion of the Riemannian structure, see Amari (1987), Murray and Rice (1993),
and Rosenberg (1997).
14. Super-Gaussian and Sub-Gaussian Distributions
Consider a random variable X whose probability density function is denoted by pX(x),
where x is a sample value of X. Let pX(x) be expressible in the form exp(g(x)), where g(x)
is an even function of x that is differentiable with respect to x, except possibly at the origin;
the derivative of g(x) with respect to x is denoted by g(x).
If g(x)/x is strictly decreasing for 0 < x < 
, then the random variable X is said to be
super-Gaussian. For example, we may have g(x) = |x| with  < 2.
If, on the other hand, the random variable X is uniformly distributed, or g(x) and g(x)/x
are strictly increasing for 0 < x < 
, then the random variable X is said to be sub-Gaussian
For example, we may have g(x) = |x| with  > 2.
q
q
7a7 2 = a
n
i=1
ai
2
7a7 2 = a
n
i=1 a
n
j=1
aigijaj
*D
D
*D = (D)WTW
Notes and References
569

570
Chapter 10
Information-Theoretic Learning Models
Perhaps in an abusive way, sometimes we find that the sign of the kurtosis of a random
variable is used as an indicator of its super-Gaussianity or sub-Gaussianity property. The
kurtosis of random variable X is defined by
On this basis, the random variable X is said to be super-Gaussian or sub-Gaussian if the kur-
tosis K4 is positive or negative, respectively.
15. Another Historical Note
In a historical context, Cardoso (1997) was the first to justify theoretically that it is sufficient
to use the right type of nonlinear activation function in the natural-gradient algorithm for
it to achieve convergence to a blind source solution.
16. Maximum–likelihood Estimation
Maximum likelihood estimators have some desirable properties. Under quite general con-
ditions, the following asymptotic properties may be proved (Kmenta, 1971):
(i)
Maximum-likelihood estimators are consistent. Let L() denote the log-likelihood
function and 	i denote an element of the parameter vector . The partial derivative
is called a score. We say that a maximum-likelihood estimator is consistent in
the sense that the value of 	i, for which the score 
is identically zero, converges
in probability to the true value of 	i as the sample size used in the estimation
approaches infinity.
(ii)
Maximum-likelihood estimators are asymptotically efficient. That is,
where N is the sample size,
is the maximum-likelihood estimate of 	i, and Iii is the
ith diagonal element of the inverse of Fisher’s information matrix. Fisher’s infor-
mation matrix is defined by
where m is the dimension of parameter vector .
(iii)
Maximum-likelihood estimators are asymptotically Gaussian. That is, as the sample
size approaches infinity, each element of the maximum-likelihood estimate 
assumes a Gaussian distribution.
In practice, we find the large-sample (i.e., asymptotic) properties of maximum-likelihood
estimators hold rather well for sample size N
50.
17. Original Version of Infomax for ICA
Equation (10.127) describes the original version of the Infomax-for ICA algorithm as
derived in Bell and Sejnowski (1995).The original algorithm is very slow to converge, due
to the presence of the term WT denoting the inverse of the transposed demixing matrix W.
It was later discovered that,by using the natural gradient in place of the ordinary (Euclidean)

ˆ
J =
- ≥
Ec 02LL
0	2
1
d
Ec 02LL
0	10	2
d
p
Ec 02LL
0	10	m
d
Ec 02LL
0	20	1
d
Ec 02LL
0	2
2
d
p
Ec
02L
0	20	m
d
c
02L
0	m0	1
d
c
02L
0	m0	2
d
p
c 02L
0	2
m
d
¥
	ˆ
i
NS q
lim e
var[	i - 	ˆ
i]
Iii
f = 1  for all i
0L0	i
0L0	i
K4 =
[X4]
([X2])2 - 3

Notes and References
571
gradient, as described in the first line of Eq. (10.128), convergence of the algorithm would
be accelerated considerably.
18. The Gram–Schmidt orthogonalization procedure is described in Golub and Van Loan (1996).
19. Symmetric FastICA
In addition to the single-unit deflation version of the fast ICA algorithm described in Sec-
tion 10.17, there is another version of the algorithm, referred to as the symmetric FastICA
algorithm. This latter version estimates the components of the blind source separation
problem in a parallel manner. Specifically, the algorithm involves the parallel computa-
tion of single-unit updates for each component, followed by subsequent symmetric orthog-
onalization of the estimated demixing matrix after each iteration. In Tichavsky et al. (2006),
analytic closed-form expressions characterizing the separability of both versions of the algo-
rithm are derived in a “local” sense.
20. TIMIT Database
The TIMIT (Texas Instruments (TI) and Massachusetts Institute of Technology (MIT))
database is a standard in speech-recognition experiments. It consists of 8-kHz- bandwidth
read (not conversational) speech recorded in a quiet environment. The database contains
630 speakers (438 males and 192 females) with 10 utterances per speaker, each one of which
is 3 seconds long, on average.
21. Another viewpoint on Information Bottleneck
Another way of thinking about the information bottleneck method is to view it as a gen-
eralization of the classical notion of “minimal sufficient statistics.” A sufficient statistic for
parameter vector a in the sample probability density function pX|A(x1, x2, ..., xn|a) is a vec-
torial function of a sample, S(X), which preserves all the mutual information in the sam-
ple on the parameter a; that is, I(X; a)  I(S(X); a). A minimal sufficient statistic is the
simplest possible sufficient statistic, or one which is a function of any other sufficient statistic,
T(X) = f(S(X)). From a basic property of mutual information called the data-processing
inequality (Cover & Thomas 2006), it follows that T(X) is minimal if I(T; X)
I(S; X) for
any sufficient statistic S(X). Minimal sufficient statistics capture the notion of the relevant
part of the sample X about the parameter vector a. Unfortunately, exact (fixed-dimensional)
sufficient statistics exist only for distributions of exponential form. An attractive general-
ization of this important concept is achieved by the information bottleneck method, which
explicitly finds functions of X with minimum mutual information about X and maximal
information about a relevant variable Y (or a in the parametric statistic case).
22. Beyond Classical ICA Theory
Earlier in this chapter,we focused on classical ICA theory.Interest in the study of independent-
components analysis and blind source separation has expanded considerably on several fronts,
including the following:
• Separation of convolutive mixtures, in which attention is directed on the fact that convo-
lution plays a key role in the mixture of signals observed in practice.
• Nonlinear blind source separation, in which nonlinearity is an underlying characteristic of
the mixing process.
• Blind separation of nonindependent sources, in which it is recognized that one or more or
the source signals may not be statistically independent.
• Noisy independent-components analysis, in which the noise-free requirement imposed on
classical ICA theory is relaxed, thereby forcing us to confront the practical reality of
source signals being noisy.
• An underdetermined scenario, in which the number of source signals is greater than the
number of observables at the output of the mixing process, which may occur in practice.


572
Chapter 10
Information-Theoretic Learning Models
• Multually independent subspaces, in which the ICA theory is expanded to encompass a sit-
uation wherein the sources produce signals that occupy different subspaces that are sta-
tistically independent of each other, yet within each subspace the pertinent source signals
are dependent.
• Blind source separation techniques exploiting nonstationarity, in which the source signals are
no longer assumed to be stationary and the challenge is to build on the notion of nonstationarity.
• Blind source separation techniques, whose mathematical basis depends on the time-
frequency representation of source signals.
• Sparse-components analysis, in which the notion of the sparseness of source signals (e.g.,
natural images) plays a key role in their separation.
• Blind source separation techniques based on temporal dependencies, in which it is possi-
ble to separate even independent Gaussian sources under special conditions.
What we have listed here is an array of topics that not only relate to the practical reali-
ties of source signals, but also highlight the theoretical challenges involved in ICA and
BSS theory and their applications. For more detailed discussion of those topics, the inter-
ested reader is referred to the books by Hyvärinen et al., (2001), Roberts and Everson
(2001), and Cichocki and Amari (2002), and the review papers of Cardoso (2001) and
Choi et al. (2005).
PROBLEMS
MaxEnt Principle
10.1 The support of a random variable X (i.e., the range of values for which it is nonzero) is
defined by [a, b]; there is no other constraint imposed on this random variable.What is the
maximum entropy distribution for this random variable? Justify your answer.
Mutual Information
10.2 (a) Use the definitions of differential entropy h(X) and conditional differential entropy
h(X|Y) to go from the first line of Eq. (10.28) to the integral formula in the second line
of the equation, defining the mutual information I(X; Y) between a pair of continu-
ous random variables, X and Y.
(b) Use the derived integral formula for the mutual information I(X; Y) to prove the
properties described in Eqs. (10.30) to (10.32).
(c) Justify the second line of Eq. (10.35), expressing the Kullback-Leibler divergence 
as an expectational formula.
10.3 Consider a random input vector X made up of a primary component X1 and a contextual
component X2. Define
How is the mutual information between X1 and X2 related to the mutual information
between Yi and Zi? Assume that the probability model of X is defined by the multivariate
Gaussian distribution
where  is the mean of X and  is its covariance matrix.
10.4 In this problem, we explore the use of the Kullback–Leibler divergence (KLD) to
derive a supervised-learning algorithm for multilayer perceptrons (Hopfield, 1987;
pX(x) =
1
(2)m2(det)12 exp((x - )T-1(x - ))
Zi = bi
TX2
Yi = ai
TX1
Dp 7 g

Baum and Wilczek, 1988). To be specific, consider a multilayer perceptron consisting
of an input layer, a hidden layer, and an output layer. Given a case or example  pre-
sented to the input, the output of neuron k in the output layer is assigned the proba-
bilistic interpretation
Correspondingly, let qk denote the actual (true) value of the conditional probability that
the proposition k is true, given the input case . The KLD for the multilayer perceptron
is defined by
where p is the a priori probability of occurrence of case .
Using 
as the cost function to be optimized, derive a learning rule for training the
multilayer perceptron.
Copulas
10.5 Demonstrate the three limiting values of the copula CUV(u, v) listed under Property 1 in
Section 10.6.
10.6 An interesting application of copulas is the generation of new distributions (Genest and
MacKay, 1989). Parts (a) and (b) of this problem illustrate that application.
(a) Product copula
Each member of a statistically independent pair of random variables X and Y is uni-
formly distributed, as shown by the formulas
Plot the copula CU,V(u, v)
(b) Gaussian copula
Consider a pair of correlated Gaussian distributions with zero mean and unit vari-
ance. Plot the corresponding copula for the following two values of the correlation
coefficient:
(i)  = 0.9
(ii)  = 0.95
10.7 Consider a pair of random variables X and Y whose mutual information is denoted by
I(X; Y). Contrast the formula for I(X; Y) of Eq. (10.28) and that of Eq. (10.49) based on
copulas as measures of statistical dependence.
10.8 To derive Eq. (10.50) on the relationship between mutual information and a copula’s
entropy, we used a direct approach. Following a procedure similar to that described for
deriving Eq. (10.49), rederive Eq. (10.50).
pY(y) = •
1
2,
-1  y  1
0,
otherwise
pX(x) = •
1
2,
-1  x  +1
0,
otherwise
Dp 7 q
Dp 7 q = a

p a
k
aqklog a
qk
pk
b + (1 - qk) log a
1 - qk
1 - pk
b b
yk = pk
Problems
573

• The additive noise components N1 and N2 at the outputs of the network are Gauss-
ian distributed, with zero mean and common variance 2
N.They are also uncorrelated
with each other.
• Each noise source is uncorrelated with the input signals.
• The output signals Y1 and Y2 are both Gaussian random variables with zero mean.
(a) Determine the mutual information I(Y; X) between the output vector Y = [Y1,Y2]T
and the input vector X = [X1, X2, ..., Xm]T.
(b) Using the result derived in part (a), investigate the tradeoff between redundancy
and diversity under the following conditions:
(i) large noise variance, represented by 2
N being large compared with the vari-
ances of Y1 and Y2;
(ii) low noise variance, represented by 2
N being small compared with the vari-
ances of Y1 and Y2.
10.11 In the Imax principle, described in Section 10.10, the objective is to maximize the mutual
information I(Ya; Yb) between the outputs Ya and Yb of a noisy neural system due to the
input vectors Xa and Xb. In another approach, a different objective is used: Maximize the 
mutual information 
between the average of the outputs Ya and Yb and the 
underlying signal component S common to these two outputs.
I a Ya + Yb
2
; S b
574
Chapter 10
Information-Theoretic Learning Models
X1
X2
X3
Xm
•
•
•
w11
w21
w22
w23
w12
w13
w1m
w2m
1
2
Y1
N1
N2
Y2
FIGURE P10.10
Infomax Principle
10.9 Consider two channels whose outputs are represented by the random variables X and Y.
The requirement is to maximize the mutual information between X and Y. Show that this
requirement is achieved by satisfying two conditions:
(a) The probability of occurrence of X or that of Y is 0.5.
(b) The joint probability distribution of X and Y is concentrated in a small region of the
probability space.
10.10 Consider the noise model of Fig. P10.10, which shows m source nodes in the input layer of
a two-neuron network. Both neurons are linear. The inputs are denoted by X1, X2, ..., Xm,
and the resulting outputs are denoted by Y1 and Y2. You may make the following
assumptions:

Using the noisy model described in Example 8, do the following:
(a) Show that
where Na and Nb are the noise components in Ya and Yb, respectively.
(b) Demonstrate the interpretation of this mutual information as a ratio of signal-plus-
noise to noise.
Independent-Components Analysis
10.12 Make a detailed comparison between principal-components analysis (discussed in Chapter 8)
and independent-components analysis, discussed in Section 10.12.
10.13 Independent-components analysis may be used as a preprocessing step for approximate
data analysis before detection and classification (Comon, 1994). Discuss which property of
independent-components analysis can be exploited for this application.
10.14 Darmois’s theorem states that the sum of independent variables can be Gaussian distributed
if, and only if, these variables are themselves Gaussian distributed (Darmois, 1953). Use
independent-components analysis to prove this theorem.
10.15 In practice,an algorithmic implementation of independent-components analysis can go only
for “as statistically independent as possible.” Contrast the solution to the blind source sep-
aration problem using such an algorithm with the solution obtained using a decorrelation
method.Assume that the covariance matrix of the observation vector is nonsingular.
Natural-Gradient Learning Algorithm for ICA
10.16 Referring to the scheme described in Fig. 10.12, show that minimizing the mutual infor-
mation between any two components of the demixer output Y is equivalent to minimizing
the Kullback–Leibler divergence between the parameterized probability density func-
tion pY(y, W) and the corresponding factorial distribution 
.
10.17 The adaptive algorithm for blind source separation described in Eq.(10.100) has two impor-
tant properties: (1) the equivariant property, and (2) the property that the weight matrix W
is maintained as nonsingular. Property (1) is discussed in some detail in the latter part of
Section 10.14. In this problem, we consider the second property.
Provided that the initial value W(0) used in starting the algorithm of Eq. (10.100) sat-
isfies the condition 
, show that
This is the necessary and sufficient condition for ensuring that W(n) is nonsingular for all n.
10.18 In this problem, we formulate the batch version of the natural-gradient learning algorithm
for ICA described in Eq. (10.100). Specifically, we write
where
Y = ≥
y1(1)
y1(2)
p
y1(N)
y2(1)
y2(2)
p
y2(N)
o
o
o
o
ym(1)
ym(2)
p
ym(N)
¥
¢W =  aI - 1
N (Y)YT bW
det(W(n)) Z 0    for all n
det(W(0)) Z 0
pY(y, W)
I a Ya + Yb
2
; S b = log a
var[Ya + Yb]
var[Na + Nb] b
Problems
575

and
where N is the number of available data points. Justify the formulation of the adjustment
!W applied to the weight matrix W as described.
Infomax for ICA Algorithm
10.19 Consider Fig. 10.16, in which we have (using the symbols for random vectors)
where
and W is an m-by-m weight matrix. Let
where
(a) Show that the joint entropy of Z is related to the Kullback–Leibler divergence 
according to the relationship
where 
is the Kullback–Leibler divergence between (a) the probability density
function of the statistically independent (i.e., factorized) version of the output vector
Y and (b) a “probability density function” defined by .m
i = 1 q(yi).
(b) How is the formula for h(Z) modified for the case when q(yi) is equal to the probabil-
ity density function of the original source output Si, for all i?
10.20 (a) Starting with Eq. (10.124), derive the result given in Eq. (10.125).
(b) For the logistic function described in Eq. (10.126), show that the use of Eq. (10.125)
yields the formula given in Eq. (10.127).
(c) Construct a signal-flow graph of the Infomax algorithm for blind source separation
that builds on the learning algorithm of Eq. (10.129).
FastICA Algorithm
10.21 Given the functions )(v) defined in Eqs. (10.132) and (10.133)—that is,
1. )(v) = log(cosh(v))
2. £(v) = exp a- v2
2 b
Dpq
h(Z) = -Dpp - Dpq
Dpp
Zk = (Yk),  k = 1, 2, ..., m
Z = [Z1, Z2, ..., Zm]T
X = [X1, X2, ..., Xm]T
Y = [Y1, Y2, ..., Ym]T
Y = WX
(Y) = ≥
(y1(1))
(y1(2))
p
(y1(N))
(y2(1))
(y2(2))
p
(y2(N))
o
o
o
o
(ym(1))
(ym(2))
p
(ym(N))
¥
576
Chapter 10
Information-Theoretic Learning Models

derive the corresponding expressions for
and
Which of the functions (v), (v), and (v) for the examples under points 1 and 2 befit the
description of a neural activation function? Justify your answer.
10.22 The FastICA algorithm is claimed to be much faster than other ICA algorithms, namely, the
natural-gradient algorithm and the Infomax ICA algorithm. Identify the features of the
FastICA algorithm that are responsible for this important property.
Coherent ICA
10.23 In combining the Infomax and Imax contributions to the objective function J(Wa, Wb), we
bypassed the need for including regularization that provides a tradeoff between the Info-
max and Imax components.We did so in order to simplify the formulation of the ICA algo-
rithm. How could this objective function be modified to preserve the statistical dependence
between the outputs of networks a and b, yet include regularization in the objective func-
tion? What are the implications of this extension?
10.24 In computational terms, the coherent ICA algorithm shares two features that are similar to
two features of the FastICA algorithm.What are those features? Give details.
10.25 Contrast the distinctive features of coherent ICA with those of ICA.
Information Bottleneck Method
10.26 Consider the information curve obtained by plotting I(T; Y) versus I(X; T) as shown in
Fig.10.21.Show that,for the optimal information bottleneck solution,this curve is an increas-
ing concave curve with slope 1/
at every point.
10.27 The illustrative depiction of the information bottleneck method presented in Fig. 10.22 and
the replicator network (identity map) of Fig. 4.19a bear a strong resemblance to each other.
Elaborate on this statement and its relevant implications.
10.28 Equation (10.184) follows from Eq. (10.182).
(a) Prove Eq. (10.184).
(b) Justify the formulation of the companion equation (10.185).
10.29 In the course of applying the optimizing condition of Eq. (10.183) to the Lagrangian of
Eq.(10.186), we skipped through some critical steps.
(a) Starting from Eq. (10.183), develop all the steps involved in arriving at the result
(b) Hence, derive the consistent pair of equations presented in Eqs. (10.188) and
(10.189).
1
 7 x - (t)7 2 + log a
qTX(tx)
qT(t)
b +
(x)
pX(x) = 0

¿(v) =
d(v)
0v
(v) =
d£(v)
dv
Problems
577

Computer Experiments
10.30 Consider the system described in Fig.10.9 involving the following three independent sources:
The mixing matrix A is
(a) Plot the waveforms of the three source signals s1(n), s2(n), and s3(n).
(b) Use any of the three ICA algorithms described in Sections 10.14, 10.16, and 10.17 to
solve the blind source separation problem, involving the sources s1(n), s2(n), s3(n) and
the mixing matrix A. Plot the waveforms produced at the output of the demixer, and
check them against those plotted in part (a).
(c) Determine the demixing matrix W.
A = £
0.56
0.79
- 0.37
- 0.75
0.65
0.86
0.17
0.32
- 0.48
§
s3(n) = noise, uniformly distributed in the range [-1, 1]
s2(n) = 0.01sgn(sin(500n + 9 cos(40n)))
s1(n) = 0.1sin(400n)cos(30n)
578
Chapter 10
Information-Theoretic Learning Models
10.31 In the computer experiment described in Section 10.21, we used the optimal manifold (for
the unsupervised representation of data) and the least-mean-square (LMS) algorithm to
perform pattern classification.The data used for the classification was based on a specific
setting of the two-moon configuration depicted in Fig. 1.8.
(a) Repeat the computer experiment of Section 10.21, this time using the recursive least-
squares (RLS) algorithm in place of the LMS algorithm.
(b) Contrast the results of your experiment against those of Section 10.21 in terms of per-
formance convergence, and computational complexity.

ORGANIZATION OF THE CHAPTER
The theme of this chapter is the study of stochastic algorithms for simulation, opti-
mization, and learning by building on ideas rooted in statistical mechanics.
The chapter is organized as follows:
1. Section 11.1 is an introductory section that provides motivation for studying the subject
matter just described.
2. Section 11.2 gives an introductory treatment of statistical mechanics, with an emphasis
on the concepts of free energy and entropy, as viewed in thermodynamics.
3. Section 11.3 discusses a special kind of stochastic processes known as Markov chains,
the use of which is commonly encountered in the study of statistical mechanics.
4. Sections 11.4, 11.5, and 11.6 discuss the following three stochastic simulation/
optimization methods:
• The Metropolis algorithm 
• Simulated annealing
• Gibbs sampling
The Metropolis algorithm and Gibbs sampling provide tools for the simulation of sta-
tionary and nonstationary processes,respectively,and simulated annealing is oriented
toward optimization.
5. The next three sections, Sections 11.7, 11.8, and 11.9, address the following stochas-
tic machines rooted in statistical mechanics:
• The Boltzmann machine
• Logistic belief nets
• Deep belief nets
The last machine has some unique properties that overcome practical limitations of
the classical Boltzmann machine and logistic belief nets.
6. Section 11.10 describes deterministic annealing, which is an approximation to simu-
lated annealing; despite its name, deterministic annealing is a stochastic algorithm.
Section 11.11 introduces the expectation-maximization algorithm and discusses an
analogy of deterministic annealing with this algorithm.
The chapter concludes with a summary and discussion in Section 11.12.
579
C H A P T E R  1 1
Stochastic Methods Rooted 
in Statistical Mechanics

11.1
INTRODUCTION
For our last class of unsupervised (self-organized) learning systems, we turn to statisti-
cal mechanics as the source of ideas.The subject of statistical mechanics encompasses the
formal study of macroscopic equilibrium properties of large systems of elements that are
subject to the microscopic laws of mechanics.The main aim of statistical mechanics is to
derive the thermodynamic properties of macroscopic bodies starting from the motion
of microscopic elements such as atoms and electrons (Landau and Lifshitz, 1980; Parisi,
1988).The number of degrees of freedom encountered here is enormous, making the use
of probabilistic methods mandatory.As with Shannon’s information theory, the concept
of entropy plays a vital role in the study of statistical mechanics. In this context, we may
say the following:
The more ordered the system,or the more concentrated the underlying probability distribution,
the smaller the entropy will be.
By the same token, we can say that the more disordered the system, or the more uniform
the underlying probability distribution, the larger the entropy will be. In 1957, Jaynes
showed that entropy can be used not only as the starting point of formulating statistical
inference as described in the previous chapter, but also for generating the Gibbs dis-
tribution that is basic to the study of statistical mechanics.
Interest in the use of statistical mechanics as a basis for the study of neural networks
goes back to the early works of Cragg and Tamperley (1954) and Cowan (1968). The
Boltzmann machine (Hinton and Sejnowski,1983,1986;Ackley et al.,1985) is perhaps the
first multilayer learning machine inspired by statistical mechanics.The machine is named
in recognition of the formal equivalence between Boltzmann’s original work on statistical
thermodynamics and the network’s own dynamic behavior. Basically, the Boltzmann
machine is a stochastic system for modeling the underlying probability distribution of a
given data set, from which conditional distributions for use in tasks such as pattern com-
pletion and pattern classification can be derived.Unfortunately,the learning process in the
early version of the Boltzmann machine is painfully slow.This shortcoming has motivated
new developments in the Boltzmann machine and inspired the formulation of new sto-
chastic machines.These issues constitute the material presented in this chapter.
11.2
STATISTICAL MECHANICS
Consider a physical system with many degrees of freedom,which can reside in any one of
a large number of possible states. Let pi denote the probability of occurrence of state i of
a stochastic system with the following properties:
(11.1)
and
(11.2)
Let Ei denote the energy of the system when it is in state i. A fundamental result from
statistical mechanics tells us that when the system is in thermal equilibrium with its
surrounding environment, state i occurs with a probability defined by
a
i
pi = 1
pi  0  for all i
580
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

(11.3)
where T is the absolute temperature in kelvins, kB is Boltzmann’s constant, and Z is a
constant that is independent of all states. One degree kelvin corresponds to 273° on
the Celsius scale, and kB  1.38  1023 joules/kelvin.
Equation (11.2) defines the condition for the normalization of probabilities.Impos-
ing this condition on Eq. (11.3), we get
(11.4)
The normalizing quantity Z is called the sum over states, or the partition function. (The
symbol Z is commonly used because the German name for this term is Zustadsumme.)
The probability distribution of Eq. (11.3) is called the canonical distribution, or Gibbs
distribution1; the exponential factor exp(EikBT) is called the Boltzmann factor.
The following two points are noteworthy from the Gibbs distribution:
1. States of low energy have a higher probability of occurrence than states of high
energy.
2. As the temperature T is reduced,the probability is concentrated on a smaller subset
of low-energy states.
The parameter T may be viewed as a pseudotemperature that controls thermal
fluctuations representing the effect of “synaptic noise” in a neuron. Its precise scale is
therefore irrelevant.Accordingly, we may choose to measure it by setting the constant
kB equal to unity and thereby redefine the probability pi and partition function Z as fol-
lows, respectively:
(11.5)
and
(11.6)
Henceforth our treatment of statistical mechanics is based on these two definitions,
where T is referred to simply as the temperature of the system. From Eq. (11.5), we find
that logpi may be viewed as a form of “energy” measured at unit temperature.
Free Energy and Entropy
The Helmholtz free energy of a physical system, denoted by F, is defined in terms of the
partition function Z as
(11.7)
The average energy of the system is defined by
(11.8)
6E7
= a
i
piEi
F = -T log Z
Z = a
i
exp a - Ei
T b
pi = 1
Z exp a - Ei
T b
Z = a
i
exp a - Ei
kBT b
pi = 1
Z exp a - Ei
kBT b
Section 11.2
Statistical Mechanics
581

where <·> denotes the ensemble-averaging operation.Thus, using Eqs. (11.5) to (11.8),
we see that the difference between the average energy and free energy is given by
(11.9)
The quantity on the right-hand side of Eq. (11.9), except for the temperature T, is rec-
ognized as the entropy of the system, as shown by
(11.10)
(This definition is consistent with that introduced in Chapter 10 on information-theoretic
models.)
We may therefore rewrite Eq. (11.9) in the form
or, equivalently,
(11.11)
Consider two systems A and A placed in thermal contact with each other.Suppose
that system A is small compared with system A,so that A acts as a heat reservoir at some
constant temperature T. The total entropy of the two systems tends to increase in accor-
dance with the relation
where ∆H and ∆H denote the entropy changes of systems A and A, respectively
(Reif, 1965). The implication of this relation, in light of Eq. (11.11), is that the free
energy of the system, F, tends to decrease and become a minimum in an equilibrium
situation. From statistical mechanics, we find that the resulting probability distribution
is defined by the Gibbs distribution. We thus have an important principle called the
principle of minimal free energy, which may be stated as follows (Landau and Lifshitz,
1980; Parisi, 1988):
The minimum of the free energy of a stochastic system with respect to variables of the sys-
tem is achieved at thermal equilibrium, at which point the system is governed by the Gibbs
distribution.
Nature likes to find a physical system with minimum free energy.
11.3
MARKOV CHAINS
Consider a system whose evolution is described by a stochastic process {Xn, n  1, 2, ...},
consisting of a family of random variables. The value xn, assumed by the random vari-
able Xn at discrete time n, is called the state of the system at that time instant.The space
of all possible values that the random variables can assume is called the state space of
the system. If the structure of the stochastic process {Xn, n  1, 2, ...} is such that the
dependence of 
on the entire past is completely captured by the dependence on the
xn+1
¢H + ¢H¿  0
F =
6 E7
- TH
6E7
- F = TH
H = - a
i
pi logpi
6E7
- F = -T a
i
pi logpi
582
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

last sample 
, we say that the process is a Markov chain (Feller, 1950;Ash, 1965). More
precisely, we have
(11.12)
which is called the Markov property. In words, we say the following:
A sequence of random variables X1, X2, ..., Xn, Xn  1 forms a Markov chain, if the probabil-
ity that the system is in state xn  1, given the sequence of past states it has gone through,is exclu-
sively determined by the state of the system at time n.
We may therefore think of the Markov chain as a generative model, consisting of
a number of states linked together (on a pairwise basis) by possible transitions. Each time
a particular state is visited, the model outputs the symbol associated with that state.
Transition Probabilities
In a Markov chain, the transition from one state to another is probabilistic, but the
production of an output symbol is deterministic. Let
(11.13)
denote the transition probability from state i at time n to state j at time n  1. Since the
pij are conditional probabilities, all transition probabilities must satisfy two conditions:
(11.14)
(11.15)
We will assume that the transition probabilities are fixed and do not change with time;
that is, Eq. (11.13) is satisfied for all time n. In such a case, the Markov chain is said to be
homogeneous in time.
In the case of a system with a finite number of possible states K, for example, the
transition probabilities constitute the K-by-K matrix
(11.16)
whose individual elements satisfy the conditions described in Eqs. (11.14) and (11.15);
the latter condition says that each row of P must add to unity. A matrix of this type is
called a stochastic matrix. Any stochastic matrix can serve as a matrix of transition
probabilities.
The definition of one-step transition probability given in Eq. (11.13) may be gen-
eralized to cases where the transition from one state to another takes place in some
fixed number of steps. Let 
denote the m-step transition probability from state i to
state j:
(11.17)
p(m)
ij
= P(Xn+m = xjXn = xi),  m = 1, 2, ...
pij
(m)
P = ≥
p11
p12
p
p1K
p21
p22
p
p2K
o
o
o
pK1
pK2
p
pKK
¥
a
j
pij = 1  for all i
pij  0  for all i, j
pij = P(Xn + 1 = jXn = i)
P(Xn+1 = xn+1Xn = xn, ..., X1 = x1) = P(Xn+1 = xn+1Xn = xn)
xn
Section 11.3
Markov Chains
583

passes in its transition from state i to state j. Specifically,
is related to 
by the
recursive relation
(11.18)
with
Equation (11.18) may be generalized as
(11.19)
which is a special case of the Chapman–Kolmogorov identity (Feller, 1950).
Specification of a Markov Chain
With the notions of state and transition probability at hand, we may now summarize
how a Markov chain is specified:
(i) A stochastic model is identified in terms of the following:
•
a finite set of K possible states denoted by S  {1, 2, ..., K};
•
a corresponding set of probabilities {pij}, where pij is the transition probability from state
i to state j, subject to two conditions:
and
(ii) Given the stochastic model just described, a Markov chain is specified in terms of a sequence
of random variables X0, X1, X2, ... whose values are taken from the set S in accordance with
the Markov property
which holds for all times n, all states i, j
S, and all possible sequences i0, ..., in  1 pertaining
to earlier states.
Recurrent Properties
Suppose a Markov chain starts in state i. State i is said to be a recurrent state if the
Markov chain returns to state i with probability 1; that is,
If the probability pi is less than 1, state i is said to be a transient state (Leon-Garcia,1994).
If the Markov chain starts in a recurrent state, that state reoccurs an infinite num-
ber of times. If it starts in a transient state, that state reoccurs only a finite number of
pi = P(ever returning to state i) = 1

P(Xn+1 = j|Xn = i, Xn-1 = in-1, ..., X0 = i0) = P(Xn+1 = j|Xn = i)
a
j
pij = 1  for all i
pij  0
p(m+n)
ij
= a
k
p(m)
ik p(n)
kj ,  m, n = 1, 2, ...
p(1)
ik = pik
p(m+1)
ij
= a
k
p(m)
ik pkj,  m = 1, 2, ...
p(m)
ij
p(m+1)
ij
584
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
We may view 
as the sum over all intermediate states, k, through which the system
p(m)
ij

times, which may be explained as follows: We may view the reoccurrence of state i as a
Bernoulli trial2 with a probability of success equal to pi. The number of returns is thus a
geometric random variable with a mean of (1  pi
1). If pi < 1, it follows that the number
of an infinite number of successes is zero.Therefore,a transient state does not reoccur after
some finite number of returns.
If a Markov chain has some transient states and some recurrent states, then the
process will eventually move only among the recurrent states.
Periodicity
Figure 11.1 shows a Markov chain of the recurrent type. This Markov chain moves
through a sequence of substates, ending up in the same substate after three time-steps.
The figure illustrates a recurrent Markov chain that is periodic.
In light of Fig. 11.1, a recurrent Markov chain is said to be periodic if all of its states
can be grouped into d disjoint subsets S1, S2, ..., Sd, where d > 1, and in such a way that
all the transitions from one subset lead to the next subset; in the figure, d = 3. More pre-
cisely, a periodic recurrent Markov chain satisfies the following condition (Bertsekas
and Tsitsiklis, 2002):
A recurrent Markov chain is said to be aperiodic if it is not periodic.
Irreducible Markov Chains
The state j of a Markov chain is said to be accessible from state i if there is a finite sequence
of transitions from i to j with positive probability.If the states i and j are accessible to each
If i  Sk and pij 7 0, then ej  Sk+1, for k = 1, ... , d - 1
j  S1,
for k = d
Section 11.3
Markov Chains
585
FIGURE 11.1
A periodic
recurrent Markov chain with
d  3.
1
2
3
6
4
5

other,they are said to communicate with each other.This communication is described by
writing 
. Clearly, if state i communicates with state j and state j communicates with
state k (that is,
and 
), then state i communicates with state k (that is,
).
If two states of a Markov chain communicate with each other, they are said to
belong to the same class. In general, the states of a Markov chain consist of one or more
disjoint classes. If, however,all the states consist of a single class,the Markov chain is said
to be indecomposible, or irreducible. In other words, by starting at any state of an irre-
ducible Markov chain, we can reach any other state with positive probability. Reducible
chains are of little practical interest in most areas of application.Accordingly, we restrict
our attention to irreducible chains.
Consider an irreducible Markov chain that starts in a recurrent state i at time n  0.
Let Ti(k) denote the time that elapses between the (k  1)th and kth returns to state i.The
mean recurrence time of state i is defined as the expectation of Ti(k) over the returns k.
The steady-state probability of state i, denoted by i, is equal to the reciprocal of the mean
recurrence time 
as shown by
If 
—that is,
 0—the state i is said to be a positive recurrent
(persistent) state. If [Ti(k)]  q—that is, i  0—the state i is said to be a null recurrent
(persistent) state. The implication of i  0 is that the Markov chain will eventually reach
a point where a return to state i is impossible. Positive recurrence and null recurrence are
different class properties,which means that a Markov chain with positive recurrent and null
recurrent states is reducible.
Ergodic Markov Chains
In principle, ergodicity means that we may substitute time averages for ensemble aver-
ages. In the context of a Markov chain, ergodicity means that the long-term proportion
of time spent by the chain in state i corresponds to the steady-state probability i, which
may be justified as follows:The proportion of time spent in state i after k returns, denoted
by vi(k), is defined by
The return times Ti(l) form a sequence of statistically independent and identically
distributed (iid) random variables, since, by definition, each return time is statistically
independent of all previous return times. Moreover, in the case of a recurrent state i, the
chain returns to state i an infinite number of times. Hence, as the number of returns, k,
approaches infinity, the law of large numbers states that the proportion of time spent in
state i approaches the steady-state probability, as shown by
(11.20)
where K is the total number of states.
lim
kS q vi(k) = i  for i = 1, 2, ..., K
vi(k) =
k
a
k
l=1
Ti(l)

i
[Ti(k)] 6 q
i =
1
[Ti(k)]
[Ti(k)],
i4k
j 4k
i 4j
i 4j
586
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

A sufficient, but not necessary, condition for a Markov chain to be ergodic is for
it to be both irreducible and aperiodic.
Convergence to Stationary Distributions
Consider an ergodic Markov chain characterized by a stochastic matrix P. Let the row
vector (n  1) denote the state distribution vector of the chain at time n  1;the jth element
of (n 1) is the probability that the chain is in state xj at time n  1.The state distribution
vector at time n is defined by
(11.21)
By iteration of Eq. (11.21), we obtain
and finally we may write
(11.22)
where  (0) is the initial value of the state distribution vector. In words, we say the
following:
The state distribution vector of the Markov chain at time n is the product of the initial state
distribution vector (0) and the nth power of the stochastic matrix P.
Let p(n)
ij denote the ij-th element of Pn. Suppose that as time n approaches infinity,
p(n)
ij tends to j independent of i, where j is the steady-state probability of state j. Cor-
respondingly, for large n, the matrix Pn approaches the limiting form of a square matrix
with identical rows as shown by
(11.23)
where  is a row vector consisting of 1, 2, ..., K. We then find from Eq. (11.22) that,
after rearranging terms,
Since,by definition,
,this condition is satisfied by the vector  independent
of the initial distribution.
We may now state the ergodicity theorem for Markov chains as follows (Feller,
1950;Ash, 1965):
Let an ergodic Markov chain with states x1, x2, ..., xK and stochastic matrix P  {pij} be irre-
ducible. The chain then has a unique stationary distribution to which it converges from any
gK
j = 1 (0)
j
= 1
c a
K
j=1
 j
(0) - 1d = 0
= ≥


o

¥
lim
nS q Pn = ≥
1
2
p
K
1
2
p
K
o
o
o
1
2
p
K
¥
(n) = (0)Pn
(n) = (n - 1)P = (n - 2)P2 = (n - 3)P3 = p
(n) = (n-1)P
Section 11.3
Markov Chains
587

initial state; that is, there is a unique set of numbers {j}Κ
j  1 such that
1.
(11.24)
2.
(11.25)
3.
(11.26)
4.
(11.27)
Conversely,suppose that the Markov chain is irreducible and aperiodic and that there exist
numbers {j}Κ
j  1 satisfying Eqs. (11.25) through (11.27).Then the chain is ergodic,the j are given
by Eq. (11.24), and the mean recurrence time of state j is 1/j.
The probability distribution {j}K
j  1 is called an invariant, or stationary, distribution.
It is so called because it persists forever once it is established. In light of the ergodicity
theorem, we may thus make the following two-part statement:
1. Starting from an arbitrary initial distribution, the transition probabilities of a
Markov chain will converge to a stationary distribution provided that such a dis-
tribution exists.
2. The stationary distribution of the Markov chain is completely independent of the
initial distribution if the chain is ergodic.
EXAMPLE 1
An Ergodic Markov Chain
Consider a Markov chain whose state-transition diagram is depicted in Fig. 11.2. The chain has
two states x1 and x2.The stochastic matrix of the chain is
which satisfies the conditions of Eqs. (11.14) and (11.15).
Suppose the initial condition is
(0) = c 1
6 5
6 d
P = ≥
1
4
3
4
1
2
1
2
¥
j = a
K
i=1
ipij for j = 1, 2, ..., K
a
K
j=1
j = 1
j 7 0 for all j
lim
nS qpij
(n) = j for all i
588
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
x1
Up-to-date
Behind
x2
3
4
1
2
1
4
1
2
FIGURE 11.2
State-transition
diagram of Markov chain for
Example 1: The states 
and
may be identified as up-to-
date and behind, respectively.
x2
x1

From Eq. (11.21), we find that the state distribution vector at time n = 1 is
Raising the stochastic matrix P to power n = 2, 3, and 4, we have
Thus, 1  0.4000 and 2  0.6000. In this example, convergence to the stationary distribution is
accomplished essentially in n = 4 iterations.With both 1 and 2 being greater than zero,both states
are positive recurrent, and the chain is therefore irreducible. Note also that the chain is aperiodic,
since the greatest common divisor of all integers n
1 such that (Pn)jj  0 is equal to 1. We
therefore conclude that the Markov chain of Fig. 11.2 is ergodic.
■
EXAMPLE 2
An Ergodic Markov Chain with Stationary Distribution
Consider a Markov chain with a stochastic matrix, some of whose elements are zero, as
shown by
The state transition diagram of the chain is depicted in Fig. 11.3.
P = £
0
0
1
1
3
1
6
1
2
3
4
1
4
0
§

P4 = c0.4000
0.6000
0.4000
0.6000d
P3 = c0.4001
0.5999
0.3999
0.6001d
P2 = c0.4375
0.5625
0.3750
0.6250d
= c 11
24 13
24 d
= c 1
6 5
6 d ≥
1
4
3
4
1
2
1
2
¥
(1) = (0)P
Section 11.3
Markov Chains
589
x2
3
4
1
6
1
3
1
1
2
1
4
x1
x1
x2
x3
FIGURE 11.3
State-transition
diagram of Markov chain for
Example 2.

By applying Eq. (11.27), we obtain the following set of simultaneous equations:
By solving these equations for 1, 2, and 3, we get
The given Markov chain is ergodic with its stationary distribution defined by 1, 2, and 3.
■
Classification of States
On the basis of the material presented in this section, we may develop a summary of the
classes to which a state can belong as shown in Fig. 11.4 (Feller, 1950; Leon-Garcia,
1994).This figure also includes the associated long-term behavior of the state.
Principle of Detailed Balance
This principle is commonly used in statistical mechanics.In words,the principle of detailed
balance asserts the following:
At thermal equilibrium, the rate of occurrence of any transition equals the corresponding
rate of occurrence of the inverse transition, as shown by 
(11.28)
A Markov chain that satisfies the principle of detailed balance is said to be reversible.
ipij = jpji
3 = 0.4652
2 = 0.1395
1 = 0.3953
3 = 1 + 1
2 2
2 = 1
6 2 + 1
4 3
1 = 1
3 2 + 3
4 3
590
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
State
j
Transient
j  0
Aperiodic
lim pij  j
as n →*
(n)
(n)Periodic
lim pij  dj as n →*,
where d is an integer
greater than 1
Positive recurrent
j  0
Null recurrent
j  0
Recurrent
FIGURE 11.4
Classification of the
states of a Markov chain and their
associated long-term behavior.

For an illustrative application of this principle, we will use it to derive the relation
of Eq. (11.27), which is the definition of a stationary distribution. To this end, we may
manipulate the summation on the right-hand side of this equation as follows:
In the second line of this expression, we made use of the principle of detailed balance,
and in the last line we made use of the fact that the transition probabilities of a
Markov chain satisfy the following condition (see Eq. (11.15), with the roles of i and
j interchanged):
From this discussion, it follows that the principle of detailed balance implies that the
distribution {j} is a stationary distribution. Insofar as a stationary distribution is con-
cerned, the principle of detailed balance is therefore much stronger than Eq. (11.27),
in the sense that it is sufficient, but not necessary, for the existence of a stationary
distribution.
11.4
METROPOLIS ALGORITHM
Now that we understand the composition of a Markov chain, we will use it to formu-
late a stochastic algorithm for simulating the evolution of a physical system to ther-
mal equilibrium. The algorithm is called the Metropolis algorithm (Metropolis et al.,
1953). It is a modified Monte Carlo method, introduced in the early days of scientific
computation for the stochastic simulation of a collection of atoms in equilibrium at a
given temperature.
Because it is a modified Monte Carlo method, the Metropolis algorithm is com-
monly referred to as a Markov chain Monte Carlo (MCMC) method. In this context, we
may formally state the following definition (Robert and Casella, 1999):
A Markov Chain Monte Carlo method for the simulation of an unknown probability distribu-
tion is any method that produces an ergodic Markov chain whose own stationary distribution is
the unknown distribution.
The Metropolis algorithm fits this definition perfectly, and so does a generalization of
it: the Metropolis–Hastings algorithm.3
Statistical Analysis of the Metropolis Algorithm
Suppose that the random variable Xn, representing an arbitrary Markov chain, is in state
xi at time n. We randomly generate a new state xj, representing a realization of another
a
K
i=1
pji = 1  for all j
= j
= a
K
i=1
(pji)j
a
K
i=1
ipij = a
K
i=1
a i
j pij bj
Section 11.4
Metropolis Algorithm
591

random variable Yn. It is assumed that the generation of this new state satisfies the sym-
metry condition
Let ∆Ε denote the energy difference resulting from the transition of the system from state
Xn  xi to state Yn  xj. Given !E, we proceed as follows:
1. If the energy difference ∆Ε is negative, the transition leads to a state with lower
energy, and the transition is accepted.The new state is then accepted as the start-
ing point for the next step of the algorithm; that is, we put Xn+1  Yn.
2. If, on the other hand, the energy difference ∆Ε is positive, the algorithm proceeds
in a probabilistic manner at that point. First, we select a random number ξ uni-
formly distributed in the range [0, 1]. If ξ < exp(!ET), where T is the operat-
ing temperature, the transition is accepted and we put Xn+1 = Yn. Otherwise, the
transition is rejected and we put Xn+1 = Xn; that is, the old configuration is reused
for the next step of the algorithm.
Choice of Transition Probabilities
Let an arbitrary Markov chain have a proposed set of transition probabilities denoted by
ij, which satisfy three conditions:
1. Nonnegativity:
2. Normalization:
3. Symmetry:
Let i denote the steady-state probability that the Markov chain is in state xi, i = 1, 2, ...,
K. We may then use the symmetric ij and the probability distribution ratio ji, to be
defined,to formulate the desired set of transition probabilities as follows (Beckerman,1997):
(11.29)
To ensure that the transition probabilities are normalized to unity, we introduce an addi-
tional definition for the probability of no transition, given as
(11.30)
= 1 - a
jZi
ijij
pii = ii + a
jZ1
ij a1 -
j
i
b
pij = µ
ij a
j
i
b
for
j
i 6 1
ij
for
j
i  1
ji = ji  for all i, j
a
j
ij = 1  for all i
ij  0  for all i, j
P(Yn = xjXn = xi) = P(Yn = xiXn = xj)
592
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

where ij is the moving probability defined by
(11.31)
The only outstanding requirement is determining how to choose the ratio ji.To
cater to this requirement, we choose the probability distribution to which we want the
Markov chain to converge to be a Gibbs distribution, as shown by
in which case the probability distribution ratio ji takes the simple form
(11.32)
where
(11.33)
By using the ratio of probability distributions, we have eliminated dependence on the
partition function Z.
By construction, the transition probabilities are all nonnegative and normalized to
unity, as required by Eqs. (11.14) and (11.15). Moreover, they satisfy the principle of
detailed balance defined by Eq. (11.28).This principle is a sufficient condition for ther-
mal equilibrium. To demonstrate that the principle of detailed balance is indeed satis-
fied, we offer the following considerations:
Case 1: ∆E < 0.
Suppose that in going from state xi to state xj, the energy change ∆E
is negative. From Eq. (11.32), we find that (ji)  1, so the use of Eq. (11.29) yields
and
Hence, the principle of detailed balance is satisfied for ∆E < 0.
Case 2: ∆E  0.
Suppose next that the energy change ∆E in going from state xi to
state xj is positive. In this case, we find that (ji) < 1, and the use of Eq. (11.29) yields
and
Here again, we see that the principle of detailed balance is satisfied.
jpji = ipij
ipij = i a
j
i ij b = jij = jji
jpji = j a i
j ji b = iji
ipij = iij = iji
¢E = Ej - Ei
j
i = exp a - ¢E
T b
j = 1
Z exp a -
Ej
T b
ij = min a1,
j
i
b
Section 11.4
Metropolis Algorithm
593

To complete the picture, we need to clarify the use of the proposed set of transi-
tion probabilities denoted by ij. These transition probabilities are in fact the proba-
bilistic model of the random step in the Metropolis algorithm. From the description of
the algorithm presented earlier, we recall that the random step is followed by a random
decision.We may therefore conclude that the transition probabilities pij defined in Eqs.
(11.29) and (11.30) in terms of the proposed transition probabilities ij and the steady-
state probabilities j are indeed the correct choice for the Metropolis algorithm.
We therefore conclude that the Metropolis algorithm generates a Markov chain,4
the transition probabilities of which do indeed converge to a unique and stable Gibbs
distribution (Beckerman, 1997).
11.5
SIMULATED ANNEALING
Consider next the problem of finding a low-energy system whose states are ordered in a
Markov chain.From Eq.(11.11),we observe that as the temperature T approaches zero,the
free energy F of the system approaches the average energy E. With 
, we
observe from the principle of minimal free energy that the Gibbs distribution, which is the
stationary distribution of the Markov chain, collapses on the global minima of the average
energy E as 
. In other words, low-energy ordered states are strongly favored at
low temperatures.These observations prompt us to raise the question,Why not simply apply
the Metropolis algorithm for generating a population of configurations representative of
the stochastic system at very low temperatures? We do not advocate the use of such a strat-
egy because the rate of convergence of the Markov chain to thermal equilibrium is extremely
slow at very low temperatures. Rather, the preferred method for improved computational
efficiency is to operate the stochastic system at a high temperature where convergence to
equilibrium is fast, and then maintain the system at equilibrium as the temperature is care-
fully lowered.That is, we use a combination of two related ingredients:
1. a schedule that determines the rate at which the temperature is lowered;
2. an algorithm—exemplified by the Metropolis algorithm—that iteratively finds the
equilibrium distribution at each new temperature in the schedule by using the final
state of the system at the previous temperature as the starting point for the new
temperature.
The twofold scheme that we have just described is the essence of a widely used sto-
chastic relaxation technique known as simulated annealing5 (Kirkpatrick et al.,1983).The
technique derives its name from analogy with an annealing process in physics and chem-
istry in which we start the process at high temperature and then lower the temperature
slowly while maintaining thermal equilibrium.
The primary objective of simulated annealing is to find the global minimum of a
cost function that characterizes large and complex systems. As such, this technique pro-
vides a powerful tool for solving nonconvex optimization problems, motivated by the
following simple idea:
When optimizing a very large and complex system (i.e.,a system with many degrees of freedom),
instead of always going downhill, try to go downhill most of the time.
T S 0
F S 6E7
594
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

Simulated annealing differs from conventional iterative optimization algorithms in two
important respects:
1. The algorithm need not get stuck, since transition out of a local minimum is always
possible when the system operates at a nonzero temperature.
2. Simulated annealing is adaptive in that gross features of the final state of the system
are seen at higher temperatures, while fine details of the state appear at lower
temperatures.
Annealing Schedule
As already mentioned, the Metropolis algorithm is the basis for the simulated-annealing
process, in the course of which the temperature T is decreased slowly. That is, the tem-
perature T plays the role of a control parameter.The simulated-annealing process will con-
verge to a configuration of minimal energy provided that the temperature is decreased no
faster than logarithmically.Unfortunately,such an annealing schedule is extremely slow—
too slow to be of practical use. In practice, we must resort to a finite-time approximation
of the asymptotic convergence of the algorithm.The price paid for the approximation is
that the algorithm is no longer guaranteed to find a global minimum with a probability of
one. Nevertheless, the resulting approximate form of the algorithm is capable of produc-
ing near-optimal solutions for many practical applications.
To implement a finite-time approximation of the simulated-annealing algorithm,
we must specify a set of parameters governing the convergence of the algorithm.
These parameters are combined in a so-called annealing schedule, or cooling sched-
ule. The annealing schedule specifies a finite sequence of values of the temperature
and a finite number of transitions attempted at each value of the temperature. The
annealing schedule due to Kirkpatrick et al. (1983) specifies the parameters of inter-
est as follows6:
1. Initial Value of the Temperature. The initial value T0 of the temperature is chosen
high enough to ensure that virtually all proposed transitions are accepted by the
simulated-annealing algorithm
2. Decrement of the Temperature. Ordinarily, the cooling is performed exponentially,
and the changes made in the value of the temperature are small. In particular, the
decrement function is defined by
(11.34)
where  is a constant smaller than,but close to,unity.Typical values of  lie between
0.8 and 0.99.At each temperature, enough transitions are attempted so that there
are 10 accepted transitions per experiment, on average.
3. Final Value of the Temperature. The system is fixed and annealing stops if the de-
sired number of acceptances is not achieved at three successive temperatures.
The latter criterion may be refined by requiring that the acceptance ratio, defined as the
number of accepted transitions divided by the number of proposed transitions, is smaller
than a prescribed value (Johnson et al., 1989).
Tk = Tk-1,  k = 1, 2, ...
Section 11.5
Simulated Annealing
595

Simulated Annealing for Combinatorial Optimization
Simulated annealing is particularly well suited for solving combinatorial-optimization
problems. The objective of combinatorial optimization is to minimize the cost func-
tion of a finite, discrete system characterized by a large number of possible solutions.
Essentially, simulated annealing uses the Metropolis algorithm to generate a sequence
of solutions by invoking an analogy between a physical many-particle system and a
combinatorial-optimization problem.
In simulated annealing, we interpret the energy Ei in the Gibbs distribution of Eq.
(11.5) as a numerical cost and the temperature T as a control parameter. The numeri-
cal cost assigns to each configuration in the combinatorial-optimization problem a scalar
value that describes how desirable that particular configuration is to the solution. The
next issue to be considered in the simulated-annealing procedure is how to identify con-
figurations and generate new configurations from previous ones in a local manner.This
is where the Metropolis algorithm performs its role. We may thus summarize the cor-
respondence between the terminology of statistical physics and that of combinatorial
optimization as shown in Table 11.1 (Beckerman, 1997).
11.6
GIBBS SAMPLING
Like the Metropolis algorithm, the Gibbs sampler7 generates a Markov chain with the
Gibbs distribution as the equilibrium distribution. However, the transition probabili-
ties associated with the Gibbs sampler are nonstationary (Geman and Geman, 1984). In
the final analysis, the choice between the Gibbs sampler and the Metropolis algorithm
is based on technical details of the problem at hand.
To proceed with a description of this sampling scheme, consider a K-dimensional
random vector X made up of the components X1,X2,...,XK. Suppose that we have knowl-
edge of the conditional distribution of Xk, given values of all the other components of X
for k = 1, 2, ..., K. The problem we wish to address is how to obtain a numerical estimate
of the marginal density of the random variable Xk for each k.The Gibbs sampler proceeds
by generating a value for the conditional distribution for each component of the random
vector X, given the values of all other components of X. Specifically, starting from an
596
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
TABLE 11.1
Correspondence between Statistical Physics and
Combinatorial Optimization
Statistical physics
Combinatorial optimization
Sample
Problem instance
State (configuration)
Configuration
Energy
Cost function
Temperature
Control parameter
Ground-state energy
Minimal cost
Ground-state configuration
Optimal configuration

arbitrary configuration {x1(0), x2(0), ..., xK(0)}, we make the following drawings on the
first iteration of Gibbs sampling:
We proceed in this same manner on the second iteration and every other iteration of the
sampling scheme.The following two points should be carefully noted:
1. Each component of the random vector X is “visited” in the natural order, with the
result that a total of K new variates are generated on each iteration.
2. The new value of component Xk1 is used immediately when a new value of Xk is
drawn for k  2, 3, ..., K.
From this discussion, we see that the Gibbs sampler is an iterative adaptive scheme.
After n iterations of its use, we arrive at the K variates X1(n), X2(n), ..., XK(n). Under
mild conditions, the following three theorems hold for Gibbs sampling (Geman and
Geman, 1984; Gelfand and Smith, 1990):
1. Convergence theorem. The random variable Xk(n) converges in distribution to the true
probability distributions of Xk for k  1, 2, ..., K as n approaches infinity; that is,
(11.35)
where
is marginal cumulative distribution function of Xk.
In fact, a stronger result is proven in Geman and Geman (1984). Specifically, rather than
requiring that each component of the random vector X be visited in repetitions of the nat-
ural order, convergence of Gibbs sampling still holds under an arbitrary visiting scheme
provided that this scheme does not depend on the values of the variables and that each
component of X is visited on an “infinitely often” basis.
2. Rate-of-convergence theorem. The joint cumulative distribution of the random variables
X1(n), X2(n), ..., XK(n) converges to the true joint cumulative distribution of X1, X2, ...,
XK at a geometric rate in n.
This theorem assumes that the components of X are visited in the natural order. When,
however, an arbitrary, but “infinitely often,” visiting approach is used, then a minor adjust-
ment to the rate of convergence is required.
3. Ergodic theorem. For any measurable function g of the random variables X1, X2, ..., XK
whose expectation exists, we have
(11.36)
with probability 1 (i.e., almost surely).
lim
nS q
1
n a
n
i=1
g(X1(i), X2(i), ..., XK(i)) S [g(X1, X2, ..., XK)]
PXk(x)
lim
nS q P(X(n)
k
 xxk(0)) = PXk(x)  for k = 1, 2, ..., K
xK(1) is drawn from the distribution of XK, given x1(1), x2(1), ..., xK-1(1).
o
xk(1) is drawn from the distribution of Xk, given x1(1), ..., xk-1(1), xk+1(0), ..., xK(0).
o
x2(1) is drawn from the distribution of X2, given x1(1), x3(0), ..., xK(0).
x1(1) is drawn from the distribution of X1, given x2(0), x3(0), ..., xK(0).
Section 11.6
Gibbs Sampling
597

The ergodic theorem tells us how to use the output of the Gibbs sampler to obtain numerical
estimations of the desired marginal densities.
Gibbs sampling is used in the Boltzmann machine to sample from distributions
over hidden neurons; this stochastic machine is discussed in the next section. In the con-
text of a stochastic machine using binary units (e.g., the Boltzmann machine), it is note-
worthy that the Gibbs sampler is exactly the same as a variant of the Metropolis
algorithm. In the standard form of the Metropolis algorithm, we go downhill with a
probability of unity. In contrast, in the alternative form of the Metropolis algorithm, we
go downhill with a probability equal to unity minus the exponential of the energy gap
(i.e., the complement of the uphill rule). In other words, if a change lowers the energy
E or leaves it unchanged, that change is accepted; if the change increases the energy, it
is accepted with probability exp(!E) and is rejected otherwise, with the old state then
being repeated (Neal, 1993).
11.7
BOLTZMANN MACHINE
The Boltzmann machine is a stochastic binary machine whose composition consists of
stochastic neurons.A stochastic neuron resides in one of two possible states in a proba-
bilistic manner.These two states may be designated as 1 for the “on” state and 1 for
the “off” state or as 1 and 0, respectively.We will adopt the former designation.Another
distinguishing feature of the Boltzmann machine is the use of symmetric synaptic con-
nections between its neurons.The use of this form of synaptic connections is also moti-
vated by statistical physics considerations.
The stochastic neurons of the Boltzmann machine are partitioned into two func-
tional groups, visible and hidden, as depicted in Fig. 11.5. The visible neurons8 provide
an interface between the network and the environment in which it operates. During the
training phase of the network, the visible neurons are all clamped onto specific states
determined by the environment. The hidden neurons, on the other hand, always oper-
ate freely; they are used to explain underlying constraints contained in the environ-
mental input vectors.The hidden neurons accomplish this task by capturing higher-order
598
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
•
•
•
1
2
3
L
Hidden neurons
•
•
•
1
2
3
K
Visible neurons
FIGURE 11.5
Architectural graph
of Botzmann machine; K is the
number of visible neurons, and L is
the number of hidden neurons. The
distinguishing features of the
machine are:
1. The connections between the
visible and hidden neurons are
symmetric.
2. The symmetric connections are
extended to the visible and
hidden neurons.

statistical correlations in the clamping vectors.The network described here represents
a special case of the Boltzmann machine. It may be viewed as an unsupervised-learning
procedure for modeling a probability distribution that is specified by clamping pat-
terns onto the visible neurons with appropriate probabilities. By so doing, the network
can perform pattern completion. Specifically, when a partial information-bearing vec-
tor is clamped onto a subset of the visible neurons, the network performs completion
on the remaining visible neurons, provided that it has learned the training distribution
properly.
The primary goal of Boltzmann learning is to produce a neural network that cor-
rectly models input patterns according to the Boltzmann distribution. In applying this
form of learning, two assumptions are made:
1. Each environmental input vector (pattern) persists long enough to permit the net-
work to reach thermal equilibrium.
2. There is no structure in the sequential order in which the environmental vectors
are clamped onto the visible units of the network.
A particular set of synaptic weights is said to constitute a perfect model of the envi-
ronmental structure if it leads to exactly the same probability distribution of the
states of the visible units (when the network is running freely) as when these units
are clamped by the environmental input vectors. In general, unless the number of hid-
den units is exponentially large compared with the number of visible units, it is
impossible to achieve such a perfect model. If, however, the environment has a reg-
ular structure, and the network uses its hidden units to capture these regularities, it
may achieve a good match to the environment with a manageable number of hidden
units.
Gibbs Sampling and Simulated Annealing for the Boltzmann Machine
Let x denote the state vector of the Boltzmann machine, with its component xi denot-
ing the state of neuron i. The state x represents a realization of a random vector X. The
synaptic connection from neuron i to neuron j is denoted by wji, with
(11.37)
and
(11.38)
Equation (11.37) describes symmetry, and Eq. (11.38) emphasizes the absence of self-
feedback. The use of a bias is permitted by using the weight wj0 from a fictitious node
maintained at 1 and by connecting it to neuron j for all j.
From an analogy with thermodynamics, the energy of the Boltzmann machine is
defined as follows9:
(11.39)
E(x) = - 1
2 a
i
a
j
wjixixj
iZj
wii = 0  for all i
wji = wij  for all i, j
Section 11.7
Boltzmann Machine
599

Invoking the Gibbs distribution of Eq. (11.5), we may define the probability that the
network (assumed to be in equilibrium at temperature T ) is in state x as
(11.40)
where Z is the partition function.
To simplify the presentation, define the single event A and joint events B and C
as follows:
with i Z j
In effect, the joint event B excludes A, and the joint event C includes both A and B. The
probability of B is the marginal probability of C with respect to A. Hence,using Eqs.(11.39)
and (11.40), we may write
(11.41)
and
(11.42)
The exponent in Eqs.(11.41) and (11.42) may be expressed as the sum of two components,
one involving xj and the other being independent of xj. The component involving xj is
given by
Accordingly, by setting xj = xi = ±1, we may express the conditional probability of A
given B, as follows:
=
1
1 + exp a-
xj
T a
i
wjixib
iZj
P(AB) =
P(A, B)
P(B)
xj
2T a
i
iZj
wjixi
= 1
Z a
xj
exp a 1
2T a
i
a
j
wjixixj b
iZj
P(B) = a
A
P(A, B)
= 1
Z exp a 1
2T a
i
a
j
wjixixjb
iZj
P(C) = P(A, B)
C: {Xi = xi}i=1
K
B: {Xi = xi}i=1
K
A: Xj = xj
P(X = x) = 1
Z exp a -
E(x)
T
b
600
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

That is, we may write
(11.43)
where (·) is the logistic function
(11.44)
Note that although x varies between 1 and 1, the whole argument 
for large K may vary between 
and 
, as depicted in Fig. 11.6. Note also that in
deriving Eq. (11.43), the need for the partition function Z has been eliminated. This
condition is highly desirable, since a direct computation of Z is infeasible for a network
of large complexity.
The use of Gibbs sampling exhibits the joint distribution P(A, B). Basically, as
explained in Section 11.6, this stochastic simulation starts with the network assigned an
arbitrary state, and the neurons are all repeatedly visited in their natural order. On each
visit, a new value for the state of each neuron is chosen in accordance with the proba-
bility distribution for that neuron,conditional on the values for the states of all other neu-
rons in the network. Provided that the stochastic simulation is performed long enough,
the network will reach thermal equilibrium at temperature T.
Unfortunately, the time taken to reach thermal equilibrium can be much too long.
To overcome this difficulty, simulated annealing for a finite sequence of temperatures
T0,T1,...,Tfinal is used,as explained in Section 11.5.Specifically,the temperature is initially
set to the high value T0,thereby permitting thermal equilibrium to be reached fast.There-
after, the temperature T is gradually reduced to the final value Tfinal, at which point the
neural states will have (hopefully) reached their desired marginal distributions.
Boltzmann Learning Rule
Since the Boltzmann machine is a stochastic machine, it is natural to look to probabil-
ity theory for an appropriate index of performance. One such criterion is the likelihood
+q
-q
v = x
Tg i Z j wjixi
(v) =
1
1 + exp(-v)
PAXj = x{Xi = xi}K
i=1, iZjB =  a x
T a
K
i
wjixib
iZj
Section 11.7
Boltzmann Machine
601
P(v)
1.0
0.5
0
v
FIGURE 11.6
Sigmoid-shaped
function P(v).

function.10 On this basis, the goal of Boltzmann learning is to maximize the likelihood
function—or, equivalently, the log-likelihood function—in accordance with the
maximum-likelihood principle; this principle was described in Chapter 10.
Let 
denote a training sample drawn from the probability distribution of inter-
est. It is assumed that the examples are all two-valued. Repetition of training exam-
ples is permitted in proportion to how common certain cases are known to occur. Let
a subset of the state vector x—say, x—denote the state of the visible neurons. The
remaining part of the state vector x—say, x—represents the state of the hidden neu-
rons. The state vectors x, x, and x are realizations of the random vectors X, X,
and X, respectively. There are two phases to the operation of the Boltzmann
machine:
1. Positive phase. In this phase, the network operates in its clamped condition (i.e.,
under the direct influence of the training sample ).
2. Negative phase. In this second phase, the network is allowed to run freely, and
therefore with no environmental input.
Given the synaptic-weight vector w for the whole network, the probability that
the visible neurons are in state x is P(X  x).With the many possible values of x con-
tained in the training sample 
assumed to be statistically independent, the overall prob-
ability distribution is the factorial distribution 
. To formulate the
log-likelihood function L(w), we take the logarithm of this factorial distribution and
treat w as the unknown parameter vector.We may thus write
(11.45)
To formulate the expression for the marginal probability P(X  x) in terms of the
energy function E(x), we follow two points:
1. The probability P(X  x) is equal to 
exp(E(x)T),in accordance with Eq.(11.40).
2. By definition, the state vector x is the joint combination of x pertaining to the
visible neurons and x pertaining to the hidden neurons. Hence, the probability of
finding the visible neurons in state x, for any x, is given by
(11.46)
where the random vector X is a subset of X. From Eq. (11.6), we find that the par-
tition function Z is itself defined by
P(X = x) = 1
Z a
x
exp a -
E(x)
T
b
1
Z
= a
x t
logP(X = x)
L(w) = log q
xt
P(X = x)
ßxtP(X = x)
t
t
t
602
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
(11.47)
Z = a
x
exp a-
E(x)
T
b

Thus, substituting Eqs. (11.46) and (11.47) into Eqs. (11.45), we obtain the desired
expression for the log-likelihood function:
(11.48)
The dependence on w is contained in the energy function E(x),as shown in Eq.(11.39).
Differentiating L(w) with respect to wji and using Eq. (11.39), we obtain the fol-
lowing result after some manipulation of terms (see Problem 11.9):
(11.49)
To simplify matters, we introduce two definitions:
1.
(11.50)
2.
(11.51)
In a loose sense, we may view the first average, &+
ji, as the mean firing rate, or correlation,
between the states of neurons i and j when the network is operating in its clamped, or
positive, phase. Similarly, we may view the second average,&
ji, as the correlation between
the states of neurons i and j when the network is operating in its free-running, or nega-
tive, phase.With these definitions, we may simplify Eq. (11.49) to
(11.52)
The goal of Boltzmann learning is to maximize the log-likelihood function L(w).
We may use gradient ascent to achieve that goal by writing
(11.53)
where  is a learning-rate parameter; it is defined in terms of , the constant introduced
in the first line of Eq. (11.53), and the operating temperature T as
(11.54)
The gradient ascent rule of Eq. (11.53) is called the Boltzmann learning rule. The learn-
ing described here is performed in batch; that is, changes to the synaptic weights are
made on the presentation of the entire training sample.
 = 
T
= (& +
ji - & -
ji )
¢wji = 
0L(w)
0wji
0L(w)
0wji
= 1
T (& +
ji - & -
ji )
= a
xt a
x
P(X = x)xjxi
& -
ji = 6 xjxi7-
= a
xt a
x
P(X = xX = x)xjxi
& +
ji =
6xjxi7 +
0L(w)
0wji
= 1
T a
xt
aa
x
P(X = xX = x)xjxi - a
x
P(X = x)xjxi b
L(w) = a
xt
a log a
x
exp a-
E(x)
T
b - log a
x
exp a-
E(x)
T
b b
Section 11.7
Boltzmann Machine
603

Summarizing Remarks
The simplicity of the Boltzmann learning rule described in Eq. (11.53) is attributed to
the fact that only locally available observations are used under two different operating
conditions of neurons, one being clamped and the other free running.Another interest-
ing feature of the rule, which may come as a surprise, is that adjustment of the synaptic
weight wji from neuron i to neuron j is independent of whether the two neurons are
both visible, both hidden, whether they are both free running, or whether they are one
of each.These desirable features of the Boltzmann machine stem from a key insight by
Hinton and Sejnowski (1983, 1986) that ties the machine’s abstract mathematical model
to neural networks by combining two concepts:
• the Gibbs distribution for describing the stochasticity of neurons, and
• the statistical physics-based energy function of Eq. (11.39) for defining the Gibbs
distribution.
However, from a practical perspective, we typically find that the learning process
in the Boltzmann machine is very slow, particularly when the number of hidden neurons
used in the machine is large.The reason for this undesirable behavior is that the machine
takes a long time to reach an equilibrium distribution, which usually happens when the
visible units are unclamped.
Nevertheless, over the years, interest has continued in the search for a stochastic
machine that would share with the classical Boltzmann machine the capacity to learn
probability distributions over binary vectors, but would also be capable of performing
the following two functions:
1. Ignore the negative phase of the Boltzmann machine that is responsible for
increased computation time, and find some other means for exercising control
over the learning process.
2. Operate efficiently in densely connected networks.
In the next two sections, we describe a pair of stochastic machines that have tried to
address these two practical issues in their own individual ways.
11.8
LOGISTIC BELIEF NETS
In the first generation of logistic belief networks, devised by Neal (1992),the symmetric con-
nections of the Boltzmann machine are replaced with direct connections,forming an acyclic
graph. It is for this reason that Neal’s logistic belief net is also referred to as a directed
belief net; hereafter,both terminologies will be used interchangeably.To be more specific,
the network is of a multilayer kind, as illustrated in Fig. 11.7.The acyclic property of the
network makes it easy to perform probabilistic calculations. In analogy with the classical
Boltzmann machine, the network uses the logistic function of Eq. (11.43) to calculate the
conditional probability of a neuron being active in response to its induced local field.
Let the random vector X, consisting of the random variables X1, X2, ..., Xn, denote
the behavior of a logistic belief network comprising N stochastic neurons. The parents
of element Xj (i.e., the parents of node j in Fig. 11.7) in the vector X are defined by
(11.55)
pa(Xj) 8 {X1, X2, ..., Xj-1}
604
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

where 
is the smallest subset of the random vector X that excites nodes;
and for which the conditional probability
(11.56)
Referring to Fig. 11.7, for example, node i is a parent of node j as there is a direct link
from node i to node j. An important virtue of the logistic belief network is its ability to
clearly exhibit the conditional dependencies of the underlying probabilistic model of
the input data,with the probability that the jth neuron is active being defined by the logis-
tic function where wji is the synaptic weight from neuron i to neuron j.This conditional
probability depends on pa(Xj) solely through a sum of weighted inputs.Thus, Eq. (11.56)
provides the basis for the propagation of beliefs through the network.
Calculations of conditional probabilities are performed under two different null
conditions:
1. wji  0 for all Xi not belonging to pa(Xj),which follows from the definition of a parent.
2. wji  0 for i
j, which follows from the fact that the network is acyclic.
As with the Boltzmann machine,the learning rule of the logistic belief net is derived
by maximizing the log-likelihood function L(w) of Eq. (11.45), computed for the training
sample
. It turns out that this maximization is accomplished through the use of gradient
ascent in probability space by defining the change in the synaptic weight wji as
where  is the learning-rate parameter, and the weight vector w is for the whole net.
However, a serious limitation of the logistic belief learning procedure is that when
it is applied to densely connected networks, computation of the posterior distribution
over the hidden neurons becomes computationally intractable, except in some simple
applications such as linear models with additive Gaussian noise.As with the Boltzmann
machine, Gibbs sampling can be used to approximate the posterior distribution, but the
use of Gibbs sampling is considerably more complicated in a directed belief net.
¢wji = 
0
0wji
L(w)
t

P(Xj = xjX1 = x1, ..., Xj - 1 = xj - 1) = P(Xj = xjpa(Xj))
{x1, x2, ..., xj}
Section 11.8
Logistic Belief Nets
605
FIGURE 11.7
Directed
(logistic) belief network.
i wji
j
Outputs
Output
layer
Hidden
layer
Input
layer
Inputs
Bias
Bias

11.9
DEEP BELIEF NETS
To overcome the difficulty in performing inference in logistic (directed) belief nets,
Hinton et al. (2006) have devised a new way of learning logistic belief nets so that infer-
ence is easy to accomplish. The models that are learned in this new way are identical
to those in logistic belief nets, except for the fact that they differ in their top layers,
which (in the new way) form an undirected associative memory. Indeed, it is in virtue
of this difference that the new belief networks are called “deep belief nets.”
Deep belief nets build on a neural network structure that was first described in
Smolensky (1986);at that time,this structure was referred to as a “harmonium.”A distinc-
tive feature of the harmonium is that there are no connections among the visible or hidden
neurons; otherwise, it is similar to the classical Botzmann machine in that it uses symmet-
ric connections between the visible neurons and hidden ones.On account of the difference
cited,the harmonium has been renamed a restricted Boltzmann machine (RBM) in Hinton
et al. (2006).At first sight, it may appear surprising to find that a symmetrically connected
module such as a restricted Boltzmann machine could learn a directed generative model
like a logistic belief net.
Since there are no connections between the hidden neurons in the RBM,and since
the connections between the visible layer and the hidden layer are undirected,as depicted
in Fig. 11.8, it follows that the states of the hidden neurons are conditionally independent
of each other, given the visible states. Hence, the RBM is capable of extracting an unbi-
ased sample from the posterior distribution, given a data vector clamped onto the visi-
ble neurons.This property of the RBM gives it a big advantage over the corresponding
directed belief net (Hinton, 2007).
Another point of interest is that learning with an infinitely deep directed belief net,
all of whose weight vectors are tied in the manner described in Fig. 11.9, is equivalent
to learning with the single restricted Boltzmann machine shown in Fig. 11.8.
Maximum-Likelihood Learning in a Restricted Boltzmann Machine
The probability that a hidden neuron in the RBM is active is defined by the logistic
function of Eq. (11.44). Let x(0)
 denote a data vector clamped onto the visible layer of the
RBM at time 0. Then the learning process alternates back and forth between the fol-
lowing two operations:
• Updating the hidden states all in parallel, given the visible states
• Doing the same, but in reverse: updating the visible states all in parallel, given the
hidden states
606
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
Hidden layer
Visible layer
FIGURE 11.8
Neural structure of
restricted Boltzmann machine
(RBM). Contrasting this with that of
Fig. 11.6, we see that unlike the
Boltzmann machine, there are no
connections among the visible
neurons and the hidden neurons in
the RBM.

Let w denote the weight vector for the whole net. Accordingly, we find that the gradient
of the log-likelihood function L(w) with respect to the weight wji, symmetrically con-
necting the visible neuron i and the hidden neuron j, is given by
(11.57)
where &(0)
ji and &(q)
ji are the average correlations between the states of neurons i and j at
zero time and an infinitely long time, respectively (Hinton et al., 2006; Hinton, 2007).
Except for insignificant changes in terminology, Eq. (11.57) is of the same mathemati-
cal form as that of Eq. (11.52) in classical Boltzmann learning. However, since we do not
anneal in an RBM, Eq. (11.57) does not involve the use of temperature as a parameter.
Training of a Deep Belief Net
The training of a deep belief net progresses on a layer-by-layer basis, as follows (Hinton,
et al., 2006; Hinton, 2007):
1. A restricted Boltzmann machine (RBM) is trained directly on the input data,
thereby making it possible for the stochastic neurons in the hidden layer of the
RBM to capture the important features that characterize the input data. Here-
after, we refer to this hidden layer as the first hidden layer of the deep belief net.
2. The activations of the trained features are then treated as “input data,” which, in
turn, are used to train a second RBM. In effect, the learning process just described
0L(w)
0wji
= &(0)
ji
- &(q)
ji
Section 11.9
Deep Belief Nets
607
W
WT
•
•
•
•
•
•
Hidden layer *
Visible layer *
Hidden layer 1
Visible layer 1
Hidden layer 0
Visible layer 0
W
W
WT
FIGURE 11.9
Top-down learning, using logistic belief network
of infinite depth.

may be viewed as one of learning features of features. The origin of this idea may
be traced to an early paper by Selfridge (1958), who proposed a pattern-recognition
system called the “pandemonium.”
3. The process of learning features of features is continued until a prescribed number
of hidden layers in the deep belief net have been trained.
An important point to note here is the fact that every time a new layer of features is
added to the deep belief net,a variational lower bound on the log-probability of the orig-
inal training data is improved (Hinton et al., 2006).
Generative Model
Figure 11.10 pictures a deep belief net after three hidden layers of the net have been
trained.The upward arrows indicate the weights computed as a result of learning the fea-
tures of features. The function of these weights is to infer the binary feature values in
each hidden layer of the belief net when a data vector is clamped into the neurons of the
visible layer.
The generative model is identified by the unshaded arrows in Fig. 11.10. Note that
the generative model does not include the bottom-up connections represented by the
red shaded upward arrows; but most importantly, it does include the up-and-down con-
nections in the top-level RBM (i.e.,layers 2 and 3),which plays the dual role of a bipartite
associative memory. When bottom-up training is performed, the top-level RBM learns
from the hidden layer below. When top-down generation is performed, the top-level
RBM is the initiator of generative modeling.
With the picture depicted in Fig. 11.10 in mind, data generation proceeds along
the following lines:
1. An equilibrium sample is taken from the top-level RBM by the performance
of alternating Gibbs sampling for many time-steps in the manner described in
Fig. 11.11; this operation is permitted to go on for a sufficiently long time until
equilibrium is reached.
2. A single top-down pass starting with the “visible”units of the top-level RBM is then
used to stochastically pick the states of all the other hidden layers of the net.
608
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
Hidden layer 3
W3
T
W1
T
W2
T
W3
Hidden layer 2
Input data
W2
Hidden layer 1
W1
Visible layer 
FIGURE 11.10
A hybrid generative model in which
the two top layers form a restricted Boltzmann
machine and the lower two layers form a directed
model.The weights shown with red shaded arrows
are not part of the generative model; they are used to
infer the feature values given to the data, but they are
not used for generating data.

Data generation is slow because, first of all, the top-level RBM must reach its equilib-
rium distribution. Fortunately, generation is not required for perceptual inference or
learning.
Hybrid Learning Process
Each RBM in the deep belief net divides the task of modeling its own “visible” data
into two subtasks, as portrayed in Fig. 11.12 (Hinton, 2007):
Subtask 1. The machine learns generative weights w that convert the aggregated
posterior distribution over the hidden neurons into a close approximation to the
data distribution over the visible neurons.
Subtask 2. The same set of weights, denoted by the vector w, also define a prior
distribution over the hidden state vectors. Sampling for this prior distribution
requires extensive alternating Gibbs sampling,as illustrated in Fig.11.11.However,
it is the very existence of this complicated prior that is responsible for making infer-
ence so simple in an RBM.When the next RBM is learned under subtask 2,that par-
ticular RBM replaces the complicated prior (defined by w) with a new prior that
Section 11.8
Logistic Belief Nets
609
Visible
layer
Hidden layer
Time t  0
t  1
Time t  0
t  1
t  2
rji
(0)
rji
(01)
rji
(11)
rji
(12)
•
•
•
•
•
•
i
•
•
•
•
•
•
i
•
•
•
•
•
•
j
j
•
•
•
•
•
•
•
•
•
•
•
•
i
t  *
j
•
•
•
•
•
•
t  *
•
•
•
•
•
•
•
•
•
•
•
•
i
FIGURE 11.11
Illustrating the progression of alternating Gibbs sampling in an RBM.After
sufficiently many steps, the visible and hidden vectors are sampled from the stationary
distribution defined by the current parameters of the model.
Probability of generating the
visible states, given the hidden
states and the weight vector w
Probability of generating 
a hidden state, given the
weight vector w
Subtask 2
Subtask 1
Aggregated
posterior distribution over
the hidden neurons
Learned data distribution
over the visible neurons
FIGURE 11.12
The task of modeling
the sensory data is divided into two
subtasks.

better approximates the aggregated posterior distribution over the hidden units of
the lower-level RBM.
Concluding Remarks
1. Except for the top two layers, a deep belief net is a multilayer logistic belief network
that has directed connections from one layer of the network to the next one down.
2. Learning proceeds bottom up on a layer-by-layer basis without any supervision.
Because of the way the learning process is performed, perceptual inference is very
simple in a deep belief net: Simply put, the inference process consists of a single
bottom-up pass.
3. Deep belief nets provide the designer a great deal of freedom. The challenge for
the designer is how to exploit this freedom in a creative way.
11.10
DETERMINISTIC ANNEALING
We now come to the final topic of the chapter,deterministic annealing.In Section 11.5,we
discussed simulated annealing, which is a stochastic relaxation technique that provides a
powerful method for solving nonconvex optimization problems. However, in the applica-
tion of simulated annealing, care must be exercised in choosing the annealing schedule. In
particular, a global minimum is achieved only if the temperature is decreased at a rate no
faster than logarithmically.This requirement makes the use of simulated annealing imprac-
tical in many applications.Simulated annealing operates by making random moves on the
energy surface (landscape). By contrast, in deterministic annealing, some form of random-
ness is incorporated into the energy or cost function itself, which is then deterministically
optimized at a sequence of decreasing temperatures (Rose et al., 1990; Rose, 1998).
In what follows, we describe the idea of deterministic annealing in the context of
an unsupervised-learning task: clustering.11
Clustering via Deterministic Annealing
The idea of clustering was discussed in Chapter 5. In light of the discussion presented
therein, we say that clustering is the partitioning of a given set of data points into sub-
groups, each one of which is as similar or homogeneous as possible. Clustering is typi-
cally a nonconvex-optimization problem, since most distortion functions used in
clustering are nonconvex functions of the input data. (The optimal manifold represen-
tation of data described in Chapter 10 is an exceptional case.) Moreover, a plot of the
distortion function versus the input is riddled with local minima, making the task of
finding the global minimum even more difficult.
In Rose (1991, 1998), a probabilistic framework is described for clustering by ran-
domization of the partition, or equivalently,randomization of the encoding rule.The main
principle used here is that each data point is associated in probability with a particular
cluster (subgroup).To be specific, let the random vector X denote a source (input) vec-
tor, and let the random vector Y denote the best reconstruction (output) vector from a
codebook of interest. Individual realizations of these two vectors are denoted by vec-
tors x and y, respectively.
610
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

To perform clustering, we need a distortion measure, denoted by d(x, y). It is
assumed that d(x, y) satisfies two desirable properties:
1. It is a convex function of y for all x.
2. It is finite whenever its two arguments x and y are both finite.
These two mild properties are satisfied,for example,by the squared Euclidean distortion
measure:
(11.58)
which was also used in Chapters 5 and 10. The expected distortion for the randomized
pattern is defined by
(11.59)
where P(X  x, Y  y) is the probability of the joint event X  x and Y  y. In the sec-
ond line of Eq. (11.59), we have used the formula for the probability of a joint event:
(11.60)
The conditional probability
is referred to as the association probability—
that is, the probability of associating the code vector y with the source vector x.
The expected distortion D is traditionally minimized with respect to the free pa-
rameters of the clustering model:the reconstruction vector y and the association probability
.This form of minimization produces a “hard”clustering solution—hard
in the sense that a source vector x is assigned to the nearest code vector y. In determinis-
tic annealing,on the other hand,the optimization problem is reformulated as that of seek-
ing the probability distribution that minimizes the expected distortion,subject to a specified
level of randomness. For a principled measure of the level of randomness, we use the joint
entropy, defined in the Shannon sense, by the following (see Section 10.2):
(11.61)
The constrained optimization of the expected distortion is then expressed as the mini-
mization of the Lagrangian, defined by
(11.62)
where T is treated as the Lagrange multiplier. From Eq. (11.62), we observe the follow-
ing points:
• For large values of T, the joint entropy H is maximized.
• For small values of T, the expected distortion D is minimized, resulting in a hard
(nonrandom) clustering solution.
• For intermediate values of T, the minimization of F provides a tradeoff between
an increase in the entropy H and a reduction in the expected distortion D.
F = D - TH
H(X, Y) = - a
x
a
y
P(X = x, Y = y) logP(X = x, Y = y)
P(Y = yX = x)
P(Y = yX = x)
P(X = x, Y = y) = P(Y = yX = x)P(X = x)
= a
x
P(X = x) a
y
P(Y = yX = x)d(x, y)
D = a
x
a
y
P(X = x, Y = y)d(x, y)
d(x, y) = 7x - y7 2
Section 11.10
Deterministic Annealing
611

Most importantly, by comparing Eq. (11.62) with Eq. (11.11), we may identify the
correspondence between the terms of the constrained-clustering optimization problem
and the terms of statistical mechanics, as shown in Table 11.2. In light of this analogy, we
henceforth refer to T as the temperature.
To develop further insight into the Lagrangian F, following the formula of Eq.(10.26),
we decompose the joint entropy H(X, Y) into two terms:
H(X) is the source entropy, and 
is the conditional entropy of the reconstruction
vector Y given the source vector X. The source entropy H(X) is independent of clus-
tering. Accordingly, we may drop the source entropy H(X) from the definition of the
Lagrangian F and thereby focus on the conditional entropy
(11.63)
This expression highlights the role of the association probability P(Y = yX = x). Hence,
keeping in mind the correspondence between the constrained-clustering optimization
problem and statistical physics, and invoking the principle of minimal free energy
described in Section 11.2, we find that minimizing the Lagrangian F with respect to the
association probabilities results in the Gibbs distribution
(11.64)
where Zx is the partition function for the problem at hand. It is defined by
(11.65)
As the temperature T approaches infinity, we find from Eq. (11.64) that the association
probability approaches a uniform distribution.The implication of this statement is that
at very high temperatures, each input vector is equally associated with all clusters. Such
associations may be viewed as “extremely fuzzy.”At the other extreme, as the temper-
ature T approaches zero,the association probability approaches a delta function.Accord-
ingly, at very low temperatures, the classification is hard, with each input sample being
assigned to the nearest code vector with probability 1.
Zx = a
y
exp a-
d(x, y)
T
b
P(Y = yX = x) = 1
Zx
exp a-
d(x, y)
T
b
H(YX) = - a
x
P(X = x) a
y
P(Y = yX = x)logP(Y = yX = x)
H(YX)
H(X, Y) = H(X) + H(YX)
612
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
TABLE 11.2
Correspondence between Constrained-Clustering 
Optimization and Statistical Physics
Constrained-clustering optimization
Statistical physics
Lagrangian, F
Free energy, F
Expected distortion, D
Average energy, <E>
Joint entropy in the Shannon sense, H
Entropy, H
Lagrange multiplier, T
Temperature, T

To find the minimum value of the Lagrangian F, we substitute the Gibbs distrib-
ution of Eq. (11.64) into Eqs. (11.59) and (11.63) and then use the resulting expressions
in the formula for the Lagrangian F in Eq. (11.62).The result obtained is as follows (see
Problem 11.16):
(11.66)
To minimize the Lagrangian with respect to the remaining free parameters—namely,
the code vectors y—we set the gradients of F* with respect to y to zero.Hence,we obtain
the condition
(11.67)
where 
is the set of all code vectors. Using the formula of Eq. (11.60) and normalizing
with respect to P(X = x), we may redefine this minimizing condition as
(11.68)
where the association probability P(Y  yX  x) is itself defined by the Gibbs distri-
bution of Eq. (11.64). In Eq. (11.68), we have included the scaling factor 1/N merely for
completeness, where N is the number of available examples.
We may now describe the deterministic-annealing algorithm for clustering (Rose,
1998):
The deterministic-annealing algorithm consists of minimizing the Lagrangian F* with respect
to the code vectors at a high value of temperature T and then tracking the minimum while the
temperature T is lowered.
In other words, deterministic annealing operates with a specific annealing schedule
whereby the temperature is lowered in an orderly fashion. At each value of the tem-
perature T, a two-step iteration central to the algorithm is performed:
1. The code vectors are fixed, and the Gibbs distribution of Eq. (11.64) for a specific
distortion measure d(x, y) is used to calculate the association probabilities.
2. The associations are fixed, and Eq. (11.68) is used to optimize the distortion mea-
sure d(x, y) with respect to the code vector y.
This two-step iterative procedure is monotonically nonincreasing in F* and is therefore
assured of converging to a minimum.At high values of temperature T, the Lagrangian
F* is fairly smooth and is a convex function of y under the mild assumptions previously
made on the distortion measure d(x, y).A global minimum of F* can be found at high
temperatures. As the temperature T is lowered, the association probabilities become
hard, resulting in a hard-clustering solution.
As the temperature T is lowered in the course of going through the annealing
schedule, the system undergoes a sequence of phase transitions, which consists of nat-
ural cluster splits through which the clustering model grows in size (i.e., number of
1
N a
x
P(Y = yX = x) 0
0y d(x, y) = 0   for all y  y
y
a
x
P(X = x, Y = y) 0
0y d(x, y) = 0   for all y  y
= -T a
x
P(X = x)logZx
F* = min
P(Y=yX=x)F
Section 11.10
Deterministic Annealing
613

614
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
(a)
(b)
(c)
(d)
(e)
(f)
(g)
FIGURE 11.13
Clustering at
various phases.The lines are
equiprobability contours,
in (b), and 
elsewhere:
(a) 1 cluster (B = 0),
(b) 2 clusters (B = 0.0049),
(c) 3 clusters (B = 0.0056),
(d) 4 clusters (B = 0.0100),
(e) 5 clusters (B = 0.0156),
(f) 6 clusters (B = 0.0347), and 
(g) 19 clusters (B = 0.0605).
p = 1
3
p = 1
2

clusters) (Rose et al., 1990; Rose, 1991).This phenomenon is significant for the follow-
ing reasons:
1. The sequence of phase transitions provides a useful tool for controlling the size of
the clustering model.
2. As in ordinary physical annealing,the phase transitions are the critical points of the
deterministic-annealing process during which care has to be exercised with the
annealing.
3. The critical points are computable, thereby providing information that can be used
to accelerate the algorithm in between phase transitions.
4. An optimum model size may be identified by coupling a validation procedure with
the sequence of solutions produced at various phases, which represent solutions of
increasing model size.
Case Study: Mixture of Gaussian Distributions
Figures 11.13 and 11.14 illustrate the evolution of the clustering solution via determinis-
tic annealing at various phases as the temperature T is decreased or the reciprocal of
temperature, B  1/T, is increased (Rose, 1991).The data set used to generate these fig-
ures is a mixture of six Gaussian distributions whose centers are marked with an “X” in
Fig. 11.13.The centers of the computed clusters are marked with an “o.” Since the clus-
tering solutions at nonzero temperatures are not hard, this random partition is depicted
by contours of equal probability—for example, probability of belonging to a particular
cluster. This process starts with one natural cluster containing the training sample (Fig.
11.13a).At the first phase transition,it splits into two clusters (Fig.11.13b) and then passes
1
3
Section 11.10
Deterministic Annealing
615
Log(D$Dmin)
Log[B$Bmin]
1
2
3
4
5
6
19
FIGURE 11.14
Phase diagram
for the Case Study in deterministic
annealing.The number of effective
clusters is shown for each phase.

through a sequence of phase transitions until it reaches the “natural” set of six clusters.
The next phase transition results in an “explosion” when all clusters split. Figure 11.14
shows the phase diagram, displaying the behavior of the average distortion throughout
the annealing process and the number of natural clusters at each phase.In this figure,the
average distortion (normalized with respect to its minimum value) is plotted versus the
reciprocal of temperature, B, normalized with respect to its minimum value Bmin. Both
axes are labeled in their relative logarithmic forms.
11.11
ANALOGY OF DETERMINISTIC ANNEALING WITH THE
EXPECTATION-MAXIMIZATION ALGORITHM
For another important aspect of the deterministic-annealing algorithm, suppose we view
the association probability P(Y  yX  x) as the expected value of a random binary
variable Vxy defined as
(11.69)
Then, from such a perspective, we recognize the two-step iteration of the deterministic-
annealing algorithm to be a form of the expectation-maximization (EM) algorithm. In
order to appreciate the relevance of this point, we will digress briefly to describe the
underlying theory of the EM algorithm.
The EM Algorithm12
Let the vector z denote some missing or unobservable data. Let r denote the complete-
data vector, made up of some observable data point d and the missing data vector z.
There are therefore two data spaces,
and , to be considered, with the mapping from
to 
being many-to-one. However, instead of observing the complete data vector r,
we are actually able to observe only the incomplete data d = d(r) in 
Let 
denote the conditional probability density function (pdf) of r, given a
pc(r)
d.
d
r
d
r
Vxy = e1
if the source vector x is assigned to code vector y
0
otherwise
616
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
parameter vector . It follows that the conditional pdf of random variable D, given , is
defined by
(11.70)
where 
is the subspace of 
that is determined by d = d(r).The EM algorithm for
r
r(d)
pD(d) =
3r(d)
pc(r)dr
parameter estimation is directed at finding a value of  that maximizes the incomplete-
data log-likelihood function
This problem, however, is solved indirectly by working iteratively with the complete-data
log-likelihood function
(11.71)
which is a random variable because the missing data vector z is unknown.
Lc() = log pc(r)
L() = log pD(d)

To be more specific, let 
denote the value of the parameter vector  on iter-
ation n of the EM algorithm. In the E-step of this iteration, we calculate the expectation
(11.72)
where the expectation is performed with respect to 
. In the M-step of this same
ˆ (n)
Q(, ˆ (n)) = [Lc()]
ˆ (n)
Section 11.12
Summary and Discussion
617
iteration, we maximize 
with respect to  over the parameter (weight) space
Q(, ˆ (n))
and therefore find the updated parameter estimate 
, as shown by
(11.73)
The algorithm is started with some initial value 
of the parameter vector . The
E-step and M-step are then alternately repeated in accordance with Eqs. (11.72) and
(11.73), respectively, until the difference between 
and 
drops to
some arbitrary small value; at that point, the computation is terminated.
Note that after an iteration of the EM algorithm,the incomplete-data log-likelihood
function is not decreased, as shown by
where equality usually means that we are at a stationary point of the log-likelihood
function.
Discussion of the Analogy (Resumed)
Returning to the analogy between deterministic annealing and the EM algorithm, we
now make two highly relevant observations:
(i) In step 1 of the deterministic-annealing algorithm, which computes the association
probabilities, we have the equivalent of the expectation step of the EM algorithm.
(ii) In step 2 of the deterministic-annealing algorithm, which optimizes the distortion
measure d(x, y) with respect to the code vector y, we have the equivalent of the
maximization step of the EM algorithm.
Note, however, that in making this analogy, the task of deterministic annealing is
more general than maximum-likelihood estimation.We say so because,unlike maximum-
likelihood estimation, deterministic annealing does not make any assumption on the
underlying probability distribution of the data.The association probabilities are, in fact,
derived from the Lagrangian F* to be minimized.
11.12
SUMMARY AND DISCUSSION
In this chapter, we have discussed the use of ideas rooted in statistical mechanics as the
mathematical basis for the formulation of stochastic simulation/optimization methods
and learning machines.
Three stochastic methods were discussed:
1. The Metropolis algorithm, which is a Markov chain Monte Carlo (MCMC)
method aimed at the simulation of an unknown probability distribution.
L(ˆ (n + 1)  Lˆ (n))   for n = 0, 1, 2, ...,
L(ˆ (n))
L(ˆ (n + 1))
ˆ (0)
ˆ (n + 1) = arg max Q(, ˆ (n))

ˆ (n + 1)
w

2. Simulated annealing, which is an adaptive procedure in the sense that gross fea-
tures of the final state of a system under study are seen at higher temperatures,
while the fine details of the state appear at lower temperatures.As an optimiza-
tion algorithm, simulated annealing can avoid getting stuck at a local minimum.
3. Gibbs sampling, which generates a Markov chain with the Gibbs distribution as
the equilibrium distribution. Unlike the Metropolis algorithm, the transition
probabilities associated with the Gibbs sampler are nonstationary.
Much of the next part of the chapter, dealing with stochastic learning machines,
focused on two topics:
1. The classical Boltzmann machine, which uses hidden and visible neurons that
are in the form of stochastic binary state units.The machine cleverly exploits the
beautiful properties of the Gibbs distribution, thereby offering some appeal-
ing properties:
• Through training, the probability distribution exhibited by the neurons is
matched to that of the environment.
• The machine offers a generalized approach that is applicable to the basic
issues of search, representation, and learning.
• The machine is guaranteed to find the global minimum of the energy surface
with respect to the states, provided that the annealing schedule in the learn-
ing process is performed slowly enough.
Unfortunately,the Boltzmann machine takes a long time to reach an equilibrium
distribution, thereby limiting its practical usefulness.
2. The deep belief net (DBN), which uses the restricted Boltzmann machine
(RBM) as a basic building block. A distinctive feature of the RBM is that
there are no connections among the hidden neurons; otherwise, it is similar
to the classical Boltzmann machine in that it uses symmetric connections
between the visible and hidden neurons. The DBN also builds on the very
old idea of learning features of features:
• The machine starts the learning process by focusing on some features of the
raw sensory input data, which should capture interesting irregularities in the
input data.
• It learns another layer of features by treating the previous layer of features as
“new” sensory data.
• Then,it continues the learning process in this manner,one layer after another
until the highest level of features learned is so complex that it is very easy to
recognize objects of interest in the original raw sensory input data.
Through a clever use of top-down learning for generative modeling and bottom-
up learning for inference, the DBN acquires the capability of learning the den-
sity model of unlabeled digit images with an impressive accuracy.
Simulated annealing, discussed early on in the chapter, distinguishes itself by per-
forming random moves on the energy surface, which can make the annealing schedule
very slow,with the result that its use is unrealistic for many applications.By contrast,deter-
ministic annealing, discussed in the last part of the chapter, incorporates randomness
618
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

into the cost function, which is then deterministically optimized at each temperature
sequentially, starting at a high temperature and then cooling gradually. Whereas simu-
lated annealing is guaranteed to reach a global minimum, no such guarantee has yet
been found for the case of deterministic annealing.
NOTES AND REFERENCES
1. The term “canonical distribution” as a description of Eq. (11.3) was coined by J. Willard
Gibbs (1902). On page 33 of Part One (Elementary Principles in Statistical Mechanics) of his
collected works, he wrote,
“The distribution represented by...
where H and % are constants, and H positive, seems to represent the most simple case
conceivable, since it has the property that when the system consists of parts with sepa-
rate energies, the laws of the distribution in phase of the separate parts are of the same
nature—a property which enormously simplifies the discussion, and is the foundation
of extremely important relations to thermodynamics....
When an ensemble of systems is distributed in phase in the manner described,i.e.,
when the index of probability (P) is a linear function of the energy ( ), we shall say that
the ensemble is canonically distributed, and shall call the divisor of the energy (H) the
modulus of distribution.”
In the physics literature, Eq. (11.3) is commonly referred to as the canonical distri-
bution (Reif, 1965) or Gibbs distribution (Landau and Lifschitz, 1980). In the neural net-
work literature, it has been referred to as the Gibbs distribution, Boltzmann distribution,
and Boltzmann–Gibbs distribution.
2. Bernoulli Trial
Consider an experiment involving a sequence of independent, but identical, stages—a
sequence of independent trials. Suppose that there are only two possible outcomes at each
stage of the experiment. In such a case, we say that we have a sequence of Bernoulli trials.
For example, in a coin-tossing experiment, the outcomes are “heads” and “tails.”
3. Metropolis–Hastings Algorithm
The original Metropolis algorithm was introduced in 1953 for the purpose of optimization
in a discrete state space with statistical physics in mind. Later on in 1970, Hastings gener-
alized the Metropolis algorithm for use in statistical simulation under the assumption of a
set of nonsymmetric transition probabilities—that is,
Correspondingly, the moving probability is defined by
The associated Markov chain still satisfies the principle of detailed balance. The Markov
chain Monte Carlo method generalized in this manner is referred to as the
Metropolis–Hastings algorithm (Robert and Casella, 2004). The Metropolis algorithm, for
which ji  ij, is naturally a special case of the Metropolis–Hastings algorithm.
ij = min a1,
jji
iij
b
ji Z ij

P = exp a % - 
H
b
Notes And References
619

4. In Tu et al. (2005), an algorithm rooted in Bayesian theory is described for the parsing of
an image into its constituent parts. The image-parsing algorithm optimizes the posterior
distribution, thereby outputting a representation of the scene of interest much like the way
in which a sentence is passed in a speech and natural language.
The computational framework of the algorithm integrates two popular approaches to
inference:
• generative (top-down) approach, which formulates the posterior distribution;
• discriminative (bottom-up) approach, which computes the discriminative distribution,
using a sequence of bottom-up filters (tests).
In the algorithm design devised by Tu et al., the posterior distribution defined by the gener-
ative model provides the target distribution for a Markov chain, and the discriminative dis-
tribution is used to construct the posterior distribution that drives the Markov chain.In other
words,the Markov chain Monte Carlo method is at the heart of the image-parsing algorithm.
5. The idea of introducing temperature and simulating annealing into combinatorial-optimization
problems is due to Kirkpatrick et al. (1983) and independently to Cerny (1985).
In a physical context, annealing is a delicate process by nature. In their 1983 paper,
Kirkpatrick et al. discuss the notion of “melting” a solid, which involves raising the
temperature to a maximum value at which all particles of the solid arrange themselves “ran-
domly” in the liquid phase. Then the temperature is lowered, permitting all particles to
arrange themselves in the low-energy ground state of a corresponding lattice. If the cool-
ing is too rapid—that is, the solid is not allowed enough time to reach thermal equilibrium
at each temperature value—the resulting crystal will have many defects, or the substance
may form a glass with no crystalline order and only metastable locally optimal structures.
The notion of “melting” may be right when thinking about glass, and perhaps com-
binatorial-optimization problems in a corresponding computational context. However, it is
misleading when discussing many other application domains (Beckerman, 1997). For exam-
ple,in image processing,if we raise the “temperature”so that all particles arrange themselves
randomly, we lose the image; it becomes uniformly gray. In a corresponding metallurgical
sense,when we anneal either iron or copper,we must keep the annealing temperature below
the melting point; otherwise, we ruin the sample.
There are several important parameters that govern metallurgical annealing:
•
annealing temperature, which specifies the temperature to which the metal or alloy is heated;
•
annealing time, which specifies the duration of time for which the elevated temperature
is maintained;
•
cooling schedule, which specifies the rate at which the temperature is lowered.
These parameters have their counterparts in simulated annealing, as described in the sub-
section on annealing schedule.
6. For more elaborate and theoretically oriented annealing schedules, see the books by Aarts
and Korst (1989) and van Laarhoven and Aarts (1988).
7. Gibbs sampling is referred to in statistical physics as a “heat bath” version of the Metropo-
lis algorithm. It is widely used in image processing, neural networks, and statistics, following
its formal exposition in the literature by Geman and Geman (1984) and Gelfand and Smith
(1990).The latter paper also discusses other variations of sampling-based (or Monte Carlo)
approaches to the numerical calculation of estimates of marginal probability distributions.
8. The visible neurons of a Boltzmann machine may also be subdivided into input and output
neurons. In this second configuration, the Boltzmann machine performs association under
the supervision of a teacher.The input neurons receive information from the environment,
and the output neurons report the outcome of the computation to an end user.
ˆ
620
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics

Problems
621
9. The formula of Eq. (11.39) applies to a Boltzmann machine whose “on” and “off” states
are denoted by 1 and 1, respectively. In the case of a machine using 1 and 0 to denote
its “on” and “off” states, respectively, we have
10. Traditionally, the Kullback–Leibler divergence has been used as the index of performance
for the Boltzmann machine (Ackley et al., 1985; Hinton and Sejnowski, 1986). This crite-
rion was discussed in Chapter 10. In that chapter, we also showed that minimization of the
Kullback–Leibler divergence is equivalent to maximum-likelihood estimation.
11. Deterministic annealing has been successfully applied to many learning tasks, including:
• vector quantization (Rose et al., 1992; Miller and Rose, 1994);
• statistical classifier design (Miller et al., 1996).
12. The paper by Newcomb (1886), considering the estimation of parameters of a mixture of
two univariate Gaussian distributions, appears to be the earliest reference to an EM type
of process reported in the literature.
The name “EM algorithm”was coined by Dempster et al.in their fundamental 1997 paper.
In that paper,formulation of the EM algorithm for computing maximum-likelihood estimates
from incomplete data at various levels of generality was presented for the first time.
The first unified account of the theory, methodology, applications, history, and exten-
sions of the EM algorithm was presented in book form by McLachlan and Krishnan (1997).
PROBLEMS
Markov Chains
11.1 The n-step transition probability from state i to state j is denoted by p(n)
ij. Using the method
of induction, show that
11.2 Figure P11.2 shows the state transition diagram for the random walk process, where the
transition probability p is greater than zero. Is the infinitely long Markov chain depicted
here irreducible? Justify your answer.
11.3 Consider the Markov chain depicted in Fig. P11.3, which is reducible. Identify the classes of
states contained in this state transition diagram.
11.4 Calculate the steady-state probabilities of the Markov chain shown in Fig. P11.4.
11.5 Consider the example of a Markov chain shown in Fig. P11.5. Using this example, verify the
validity of the Chapman–Kolmogorov identity.
Simulation techniques
11.6 The Metropolis algorithm and the Gibbs sampler represent two alternative techniques for
simulating a large-scale problem of interest. Discuss the basic similarities and differences
between them.
p(1+n)
ij
= a
k
pikp(n)
kj
E(x) = - a
i
a
j
iZj
wjixixj
FIGURE P11.2
p
p
p
p
1  p
1  p
1  p
1  p
x2
x1
x0
x1
x2

622
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
FIGURE P11.3
FIGURE P11.4
x1
x3
x2
1
3
2
3
3
4
1
2
1
2
1
4
x1
x3
x2
3
4
2
3
3
4
1
3
1
4
1
4
11.7 In this problem, we consider the use of simulated annealing for solving the traveling-sales-
man problem (TSP).You are given the following:
•
N cities;
•
the distance between each pair of cities, d;
•
a tour represented by a closed path visiting each city once, and only once.

Problems
623
FIGURE P11.5
The objective is to find a tour (i.e., permutation of the order in which the cities are visited)
that is of minimal total length L. In this problem, the different possible tours are the con-
figurations, and the total length of a tour is the cost function to be minimized.
(a) Devise an iterative method of generating valid configurations.
(b) The total length of a tour is defined by
where P denotes a permutation with P(N + 1)  P(1). Correspondingly, the partition
function is
where T is a control parameter. Set up a simulated-annealing algorithm for the TSP.
Boltzmann machine
11.8 Consider a stochastic, two-state neuron j operating at temperature T. This neuron flips from
state xj to state xj with probability
where ∆Ej is the energy change resulting from such a flip. The total energy of the Boltz-
mann machine is defined by
where 
is the synaptic weight from neuron i to neuron j, with 
and 
(a) Show that
where 
is the induced local field of neuron j.
vj
¢Ej = -2xjvj
wji = 0.
wji = wij
wji
E = - 1
2 a
i
a
j
wjixixj
iZj
P(xj S -xj) =
1
1 + exp(-¢EjT)
Z = a
P
e-LPT
LP = a
N
i=1
dP(i)P(i+1)
p1j
pkj
pik
plj
pil
pi1
1
k
i
j
•
•
•
l
•
•
•
Time n – 1
Time n
Time 0
(n – 1)
(n – 1)
(n – 1)
(b) Hence, show that for an initial state xj  1, the probability that neuron j is flipped into
state 1 is 1/(1  exp(
j/T)).
2v

624
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
(c) Show that the same formula in part (b) holds for neuron j flipping into state 1 when it
is initially in state 1.
11.9 Derive the formula given in Eq. (11.49) that defines the derivative of the log-likelihood func-
tion L(w) with respect to the synaptic weight wji for the Boltzmann machine.
11.10 The Gibbs distribution may be derived using a self-contained mathematical approach that
does not rely on concepts from statistical physics. In particular, a two-step Markov chain
model of a stochastic machine may be used to formalize the assumptions that yield the
unique properties of the Boltzmann machine (Mazaika, 1987). This should not come as a
surprise, since simulated annealing, basic to the operation of the Boltzmann machine, is
known to have a Markov property of its own (van Laarhoven and Aarts, 1988).
Consider, then, a transition model between states of a neuron in a stochastic machine
that is composed of two random processes:
•
The first process decides which state transition should be attempted.
•
The second process decides whether the transition succeeds.
(a) Expressing the transition probability pji as the product of two factors, that is,
show that
(b) Assume that the attempt-rate matrix is symmetric:
Also assume that the probability of a successful attempt satisfies the property of com-
plementary conditional transition probability:
By invoking these two assumptions, show that
(c) Given that 
, use the result of part (a) to show that
(d) Finally, make a change of variables to obtain
where T and T* are arbitrary constants. Hence, derive the following results, where
∆E = Ej  Ei.
(i)
(ii)
(iii)
(e) What conclusions can you draw from these results?
qji =
1
1 + exp(-¢ET)
Z = a
j
exp a-
Ej
T b
i = 1
Z exp a- Ei
T b
Ei = - T log i + T*
qij =
1
1 + (ij)
ji Z 0
a
j
ji(qijj + qiji - j) = 0
qji = 1 - qij
ji = ij
pii = 1 - a
jZi
jiqji
pji = jiqji  for j Z i

Problems
625
11.11 In Section 11.7, we used maximum likelihood as the criterion for deriving the Boltzmann
learning rule, described in Eq. (11.53). In this problem, we revisit this learning rule with
another criterion. From the discussion presented in Chapter 10, the Kullback–Leibler diver-
gence between two probability distributions p+
 and p-
 is defined by
where the summation is over all possible states .The probability p+
 denotes the probabil-
ity that the visible neurons are in state  when the network is in its clamped (positive) con-
dition, and the probability p-
 denotes the probability that the same neurons are in a state α
when the network is in its free-running (negative) condition.Using 
,rederive the Boltz-
mann learning rule of Eq (11.53).
11.12 Consider a Boltzmann machine whose visible neurons are divided into input neurons and out-
put neurons. The states of these neurons are denoted by  and , respectively. The state of
the hidden neurons is denoted by . The Kullback–Leibler divergence for this machine is
defined by
where p+
 is the probability of state α over the input neurons;
is the conditional proba-
bility that the output neurons are clamped in state α given an input state α; and 
is the
conditional probability that the output neurons are in thermal equilibrium in state γ given
that only the input neurons are clamped in state . As before, the plus and minus super-
scripts denote the positive (clamped) and negative (free-running) conditions, respectively.
(a) Derive the formula 
for a Boltmann machine that includes input, hidden, and out-
put neurons.
(b) Show that the Boltzmann learning rule for adjusting the synaptic weight 
in this net-
work configuration may still be expressed in the same form as that described in Eq.
(11.53), with new interpretations for the correlations &+
ji and &-
ji.
Deep Belief Nets
11.13 After learning,what is the main difference between a deep belief net and a logistic belief net?
Explain your answer.
11.14 An infinitely deep logistic belief net, shown in Fig. 11.9, is equivalent to a single RBM, shown
in Fig. 11.8. Justify this statement.
Deterministic Annealing
11.15 In Section 11.10 we developed the idea of deterministic annealing using an information-
theoretic approach.The idea of deterministic annealing may also be developed in a princi-
pled manner using the maximum-entropy principle discussed in Chapter 10. Justify the
rationale of this second approach (Rose, 1998).
11.16 (a) Using Eqs. (11.59), (11.64), and (11.63), derive the result given in Eq. (11.66), which
defines the Lagrangian F* that results from use of the Gibbs distribution for the asso-
ciation probability.
(b) Using the result from part (a) of this problem, derive the condition given in Eq. (11.68)
for the minimum of F* with respect to the code vector y.
(c) Apply the minimizing condition of Eq. (11.68) to the squared distortion measure of Eq.
(11.58), and comment on your result.
wji
Dp+7 p-
p-

p+

Dp + 7 p - = a

p+
 a

p+
log a
p+

p-

b
Dp+7 p-
Dp + 7 p - = a

p+
 log a p+

p-

b

626
Chapter 11
Stochastic Methods Rooted in Statistical Mechanics
11.17 Consider a data set that is a mixture of Gaussian distributions. In what way does the use of
deterministic annealing offer an advantage over maximum-likelihood estimation in such a
situation?
11.18 In this problem, we explore the use of deterministic annealing for pattern classification using
a neural network (Miller et al., 1996).The output of neuron j in the output layer is denoted
by Fj(x), where x is the input vector. The classification decision is based on the maximum
discriminant Fj(x).
(a) For a probabilistic objective function, consider the expression
where 
is a training sample of labeled vectors, with x denoting an input vector and 
its class label, and 
is the probability of association between input vector x
and class region 
. Using the maximum-entropy principle discussed in Chapter 10, for-
mulate the Gibbs distribution for 
.
P(x  rj)
rj
P(x  rj)
c
t
F = 1
N
a
(x,c)t a
j
P(x  rj)Fj(x)
(b) Let Pe denote the average misclassification cost. Formulate the Lagrangian for min-
imization of Pe subject to the constraint that the entropy corresponding to the asso-
ciation probabilities 
is equal to some constant value H.
P(x  rj)

ORGANIZATION OF THE CHAPTER
The purpose this chapter is threefold: (i) to discuss the development of dynamic pro-
gramming as the mathematical basis of planning a multistage course of action by an
agent operating in a stochastic environment, (ii) to give a direct derivation of rein-
forcement learning as an approximate form of dynamic programming, and (iii) to
present indirect methods of approximate dynamic programming to deal with the
“curse” of dimensionality.
The chapter is organized as follows:
1. Section 12.1, the introductory section, motivates the study of dynamic programming
by discussing Markov decision processes, which is done in Section 12.2.
2. Sections 12.3 through 12.5 discuss Bellman’s theory of dynamic programming and
the two related methods: policy iteration and value iteration.
3. Section 12.6 describes the rationale behind the direct learning-based approximation
of dynamic programming,thereby leading to the developments of temporal-difference
learning and Q-learning in Sections 12.7 and 12.8, respectively.
4. Section 12.9 describes the rationale behind the indirect approximation of dynamic
programming to deal with the “curse”-of-dimensionality problem, thereby leading to
the discussion of least squares policy evaluation and approximate value iteration pre-
sented in Sections 12.10 and 12.11, respectively.
The chapter concludes with a summary and discussion in Section 12.12.
12.1
INTRODUCTION
In the introductory chapter, we identified two main paradigms of learning: learning with
a teacher and learning without a teacher.The paradigm of learning without a teacher is
subdivided into self-organized (unsupervised) learning and reinforcement learning.
Different forms of learning with a teacher, or supervised learning, were covered in
Chapters 1 through 6, and different forms of unsupervised learning were discussed in
Chapters 9 through 11.Semisupervised learning was discussed in Chapter 7.In this chap-
ter, we discuss reinforcement learning.
Supervised learning is a “cognitive”learning problem performed under the tutelage
of a teacher.It relies on the availability of an adequate set of input–output examples that
627
C H A P T E R  1 2
Dynamic Programming

are representative of the operating environment. In contrast, reinforcement learning is
a “behavioral” learning problem. It is performed through interaction between an agent
and its environment,in which the agent or decision maker seeks to achieve a specific goal
despite the presence of uncertainties (Barto et al., 1983; Sutton and Barto, 1998). The
fact that this interaction is performed without a teacher makes reinforcement learning par-
ticularly attractive for dynamic situations,where it is costly or difficult (if not impossible)
to gather a satisfactory set of input–output examples.
There are two approaches to the study of reinforcement learning,1 summarized as
follows:
1. the classical approach, in which learning takes place through a process of punish-
ment and reward, with the goal of achieving a highly skilled behavior;
2. the modern approach, which builds on a mathematical technique known as dynamic
programming to decide on a course of action by considering possible future stages
without actually experiencing them; the emphasis here is on planning.
Our discussion focuses on modern reinforcement learning.
Dynamic programming2 is a technique that deals with situations where decisions are
made in stages,with the outcome of each decision being predictable to some extent before
the next decision is made. A key aspect of such situations is that decisions cannot be
made in isolation.Rather,the desire for a low cost at the present must be balanced against
the undesirability of high cost in the future.This is a credit assignment problem, because
credit or blame must be assigned to each one of a set of interacting decisions. For opti-
mal planning, it is necessary to have an efficient tradeoff between immediate and future
costs. Such a tradeoff is indeed captured by the formalism of dynamic programming. In
particular, dynamic programming addresses the following fundamental problem:
How can an agent or decision maker improve its long-term performance in a stochastic envi-
ronment when the attainment of this improvement may require having to sacrifice short-term
performance?
Bellman’s dynamic programming provides an optimal solution to this fundamental prob-
lem in an elegant and principled manner.
In the art of mathematical model building, the challenge is to strike the right bal-
ance between two entities, one practical and the other theoretical. Respectively, the two
entities are
• the realistic description of a given problem and
• the power of analytic and computational methods to apply to the problem.
In dynamic programming, an issue of particular concern is that of decision making by an
agent that operates in a stochastic environment.To address this issue,we build our model
around Markov decision processes. Given the initial state of a dynamic system, a Markov
decision process provides the mathematical basis for choosing a sequence of decisions
that will maximize the returns from an N-stage decision-making process.What we have
just described is the essence of Bellman’s dynamic programming. It is therefore fitting
that we begin the study of dynamic programming with a discussion of Markov decision
processes.
628
Chapter 12
Dynamic Programming

12.2
MARKOV DECISION PROCESS
Consider an agent or decision maker that interacts with its environment in the manner
illustrated in Fig. 12.1. The agent operates in accordance with a finite-discrete-time
Markovian decision process that is characterized as follows:
• The environment evolves probabilistically, occupying a finite set of discrete states.
However, the state does not contain past statistics, even though these statistics
could be useful to the agent.
• For each environmental stage, there is a finite set of possible actions that may be
taken by the agent.
• Every time the agent takes an action, a certain cost is incurred.
• States are observed, actions are taken, and costs are incurred at discrete times.
In the context of our present discussion, we introduce the following definition:
The state of the environment is a summary of the entire past experience of an agent gained
from its interaction with the environment, such that the information necessary for the agent to
predict the future behavior of the environment is contained in that summary.
The state at time-step n is denoted by the random variable Xn, and the actual state at
time-step n is denoted by in.The finite set of states is denoted by x. A surprising aspect
of dynamic programming is that its applicability depends very little on the nature of the
state. We may therefore proceed without any assumption on the structure of the state
space.Note also that the complexity of the dynamic-programming algorithm is quadratic
in the dimension of the state space and linear in the dimension of the action space.
For state i, for example, the available set of actions (i.e., inputs applied to the envi-
ronment by the agent) is denoted by 
, where the second subscript k in action
aik taken by the agent merely indicates the availability of more than one possible action
when the environment is in state i. The transition of the environment from the state i to
the new state j, for example, due to action aik is probabilistic in nature. Most importantly,
however, the transition probability from state i to state j depends entirely on the current
state i and the corresponding action aik.This is the Markov property, which was discussed
in Chapter 11. This property is crucial because it means that the current state of the
environment provides the necessary information for the agent to decide what action
to take.
The random variable denoting the action taken by the agent at time-step n is
denoted by An. Let pij(a) denote the transition probability from state i to state j due to
ai = {aik}
Section 12.2
Markov Decision Process
629
Agent
State
Environment
Cost
Action
FIGURE 12.1
Block diagram of an
agent interacting with its environment.

action taken at time-step n, where An  a. By virtue of the Markov assumption on the
state dynamics, we have
(12.1)
The transition probability pij(a) satisfies two conditions that are imposed on it by prob-
ability theory, namely,
(12.2)
(12.3)
where the i and j reside in the state space.
For a given number of states and given transition probabilities, the sequence of
environmental states resulting from the actions taken by the agent over time forms a
Markov chain. Markov chains were discussed in Chapter 11.
At each transition from one state to another, a cost is incurred by the agent.Thus,
at the nth transition from state i to state j under action aik, the agent incurs a cost denoted
by ng(i, aik, j), where g(., ., .) is a prescribed function and  is a scalar called the discount
factor, confined to the range 0
 < 1.The discount factor reflects intertemporal pref-
erences. By adjusting , we are able to control the extent to which the agent is concerned
with long-term versus short-term consequences of its own actions. In the limit, when
  0, the agent is myopic, in the sense that it is concerned only with the immediate
consequences of its actions. In what follows, we will ignore this limiting value; that is, we
confine the discussion to 0 <  < 1.As  approaches 1, future costs become more impor-
tant in determining optimal actions.
Our interest is in the formulation of a policy, defined as a mapping of states into
actions. In other words;
Policy is a rule used by the agent to decide what to do, given knowledge of the current state
of the environment.
The policy is denoted by
(12.4)
where n is a function that maps the state Xn  i into an action An  a at time-step n  0,
1, 2, .... This mapping is such that
where 
denotes the set of all possible actions taken by the agent in state i. Such policies
are said to be admissible.
A policy can be nonstationary or stationary. A nonstationary policy is time vary-
ing, as indicated in Eq. (12.4).When, however, the policy is independent of time,
the policy is said to be stationary. In other words, a stationary policy specifies exactly the
same action each time a particular state is visited. For a stationary policy, the underly-
ing Markov chain may be stationary or nonstationary; it is possible to use a stationary
 = {, , , ...}
ai
n(i)  ai  for all states i   x
 = {0, 1, 2, ...}

 2.   a
j
pij(a) = 1   for all i
1. pij(a)  0         for all i and j
pij(a) = P(Xn+1 = j  Xn = i, An = a)
630
Chapter 12
Dynamic Programming

policy on a nonstationary Markov chain, but this is not a wise thing to do. If a station-
ary policy  is employed, then the sequence of states {Xn, n  0, 1, 2, ...} forms a Markov
chain with transition probabilities pij((i)), where (i) signifies an action. It is for this
reason that the process is referred to as a Markov decision process.
The Basic-Problem
A dynamic-programming problem can be of a finite-horizon or infinite-horizon kind. In
a finite-horizon problem, the cost accumulates over a finite number of stages. In an
infinite-horizon problem, the cost accumulates over an infinite number of stages.Infinite-
horizon problems provide a reasonable approximation to problems involving a finite,but
very large, number of stages. They are also of particular interest because discounting
ensures that the costs for all states are finite for any policy.
Let g(Xn, n(Xn), Xn1) denote the observed cost incurred as a result of the tran-
sition from state Xn to state Xn1 under the action of policy n(Xn). The total expected
cost in an infinite-horizon problem, starting from an initial state X0  i and using a policy
  {n}, is defined by
Section 12.3
Bellman’s Optimality Criterion
631
(12.5)
where the expected value is taken with respect to the Markov chain {X1, X2, ...} and 
is the discount factor. The function J(i) is called the cost-to-go function for policy ,
starting from state i. Its optimal value, denoted by J*(i), is defined by
(12.6)
The policy  is optimal if, and only if, it is greedy with respect to J*(i).The term “greedy”
is used to describe the case when an agent seeks to minimize the immediate next cost
without paying any attention to the possibility that such an action may do away with
access to better alternatives in the future.
When the policy  is stationary—that is,   {, , ...}—we use the notation J(i)
in place of J*(i) and say that  is optimal if
(12.7)
We may now sum up the basic problem in dynamic programming as follows:
Given a stationary Markovian decision process describing the interaction between an agent
and its environment,find a stationary policy  = {,,  ...} that minimizes the cost-to-go func-
tion Jµ(i) for all initial states i.
Note that during learning, the behavior of an agent may change with time. However,
the optimal policy that the agent seeks will be stationary.
12.3
BELLMAN’S OPTIMALITY CRITERION
The dynamic-programming technique rests on a very simple idea known as the principle
of optimality, due to Bellman (1957). Simply stated, this principle says the following
(Bellman and Dreyfus, 1962):
J(i) = J*(i)  for all initial states i 
J*(i) = min
  J(i) 
J(i) = c a
q
n=0
ng(Xn, n(Xn), Xn+1)X0 = id 

An optimal policy has the property that whatever the initial state and initial decision are, the
remaining decisions must constitute an optimal policy starting from the state resulting from
the first decision.
As used here, a “decision” is a choice of control at a particular time, and a “policy” is the
entire control sequence or control function.
To formulate the principle of optimality in mathematical terms, consider a finite-
horizon problem for which the cost-to-go function is defined by
(12.8)
where K is the planning horizon (i.e., number of stages) and gK(XK) is the terminal cost.
Given X0, the expectation in Eq. (12.8) is with respect to the remaining states X1, ..., XK -1.
With this terminology, we may now formally state the principle of optimality as follows
(Bertsekas, 2005, 2007):
Let *  {*0, *1, ..., *k-1} be an optimal policy for the basic finite-horizon problem.Assume
that when using the optimal policy *, a given state Xn occurs with positive probability. Con-
sider the subproblem where the environment is in state Xn at time n, and suppose we wish to
minimize the corresponding cost-to-go function
(12.9)
for n  0, 1, ..., K - 1. Then the truncated policy {*n, *n1, ..., *K-1} is optimal for the
subproblem.
We may intuitively justify the principle of optimality by the following argument:
If the truncated policy {*n, *n1, ..., *K-1} was not optimal as stated, then once the state
Xn is reached at time n, we could reduce the cost-to-go function Jn(Xn) simply by switch-
ing to a policy that is optimal for the subproblem.
The principle of optimality builds on the engineering notion of “divide and con-
quer.” Basically, an optimal policy for a complex multistage planning or control prob-
lem is constructed by proceeding as follows:
1. Construct an optimal policy for the “tail subproblem” involving only the last stage
of the system.
2. Extend the optimal policy to the “tail subproblem” involving the last two stages of
the system.
3. Continue the procedure in this fashion until the entire problem has been dealt with.
Dynamic-Programming Algorithm
On the basis of the procedure just described, we may now formulate the dynamic-
programming algorithm, which proceeds backward in time from period N - 1 to period
0. Let   {0, 1, ..., K-1} denote an admissible policy. For each n  0, 1, ..., K - 1,
let n  {n, n1, ..., K-1}, and let J*n(Xn) be the optimal cost for the (K - n)-stage prob-
lem that starts at state Xn and time n and ends at time K; that is, we write
Jn(Xn) = cgK(XK) + a
K-1
k=n
gk(Xk, k(Xk), Xk+1)d 
J0(X0) = cgK(XK) + a
K-1
n=0
gn(Xn, n(Xn), Xn+1)d 
632
Chapter 12
Dynamic Programming

(12.10)
which represents the optimal form of Eq. (12.9). Recognizing that n  (n,n1) and
partially expanding the summation on the right-hand side of Eq. (12.10), we may write
(12.11)
where, in the last line, we have made use of the defining equation of Eq. (12.10), with
n  1 used in place of n. Accordingly, we deduce from Eq. (12.11) that
(12.12)
We may now formally state the dynamic-programming algorithm as follows
(Bertsekas, 2005, 2007):
For every initial state X0, the optimal cost J*(X0) of the basic finite-horizon problem is equal
to J0(X0), where the function J0 is obtained from the last step of the algorithm
(12.13)
which runs backward in time, with
(12.14)
Furthermore, if *n minimizes the right-hand side of Eq. (12.13) for each Xn and n, then the
policy *  {*0, *1, ..., *K-1} is optimal.
Bellman’s Optimality Equation
In its basic form, the dynamic-programming algorithm deals with a finite-horizon prob-
lem. We are interested in extending the use of this algorithm to deal with the infinite-
horizon discounted problem described by the cost-to-go function of Eq. (12.5) under a
stationary policy   {, , , ...}.With this objective in mind, we do two things:
1. Reverse the time index of the algorithm.
2. Define the cost gn(Xn, (Xn), Xn1) as
(12.15)
gn(Xn, (Xn), Xn+1) = ng(Xn, (Xn), Xn+1)
JK(XK) = gK(XK)
Jn(Xn) = min 
n
 
Xn + 1
 [gn(Xn, n(Xn), Xn+1) + Jn+1(Xn+1)]
Jn(Xn) = min
n   
Xn + 1
 [gn(Xn, n(Xn), Xn+1) + Jn+1(Xn+1)] 
 = min
n
 
 
Xn + 1
 £gn(Xn, n(Xn), Xn+1) + J*n+1(Xn+1)§
+ min
n + 1

 
(Xn + 2, ..., XK - 1)cgK(XK) +
a
K-1
k=n+1
gk(Xk, k(Xk), Xk+1)d f
 = min 
n

Xn + 1
egn(Xn), n(Xn), Xn+1) 
+ gK(XK) +
a
K-1
k=n+1
gk(Xk, k(Xk), Xk+1)d
 J*n(Xn) =
min
(n, n + 1)

 
(Xn + 1,..., XK - 1)cgn(Xn, n(Xn), Xn+1)
J*n(Xn) = min
n  

(Xn + 1, ..., XK - 1) cgK(XK) + a
K-1
k=n
gk(Xk, k(Xk), Xk+1)d 
Section 12.3
Bellman’s Optimality Criterion
633

We may now reformulate the dynamic-programming algorithm as
(12.16)
which starts from the initial conditions
The state X0 is the initial state, X1 is the new state that results from the action of policy
, and  is the discount factor.
Let J*(i) denote the optimal infinite-horizon cost for the initial state X0  i. We
may then view J*(i) as the limit of the corresponding K-stage optimal cost JK(i) as the
horizon K approaches infinity; that is,
(12.17)
This relation is the connecting link between the finite-horizon and infinite-horizon dis-
counted problems. Substituting n  1  K and X0  i into Eq. (12.16) and then applying
Eq. (12.17), we obtain
(12.18)
To rewrite this equation for the optimal infinite-horizon cost J*
, we proceed in two
stages:
1. Evaluate the expectation of the cost g(i, (i), X1) with respect to X1 by writing
(12.19)
where N is the number of states of the environment and pij is the transition prob-
ability from the initial state X0  i to the new state X1  j. The quantity defined
in Eq. (12.19) is the immediate expected cost incurred at state i by following the
action recommended by the policy . Denoting this cost by c(i, (i)), we write
(12.20)
2. Evaluate the expectation of J*(X1) with respect to X1. Here, we note that if we
know the cost J*(X1) for each state X1 of a finite-state system, then we may read-
ily determine the expectation of J*(X1) in terms of the transition probabilities of
the underlying Markov chain by writing
(12.21)
Thus, using Eqs. (12.19) to (12.21) in Eq. (12.18) and 
expressed as a function of the
policy 
for the purpose of generality, we obtain the desired result
(12.22)
J*(i) = min

ac(i, (i)) +  a
N
j=1
pij()J*(j) b  for i = 1, 2, ..., N

pij
[J*(X1)] = a
N
j=1
pijJ*(j)
c(i, (i)) = a
N
j=1
pijg(i, (i), j)
[g(i), (i), X1] = a
N
j=1
pijg(i, (i), j)
(i)
J*(i) = min


X1
[g(i, (i), X1) + J*(X1)]
J*(i) = lim
KS q JK(i)  for all i
J0(X) = 0  for all X
Jn+1(X0) = min


X1
[g(X0, (X0), X1) + Jn(X1)]
634
Chapter 12
Dynamic Programming

Equation (12.22) is called Bellman’s optimality equation. It should not be viewed as an
algorithm. Rather, it represents a system of N equations with one equation per state.The
solution of this system of equations defines the value of the optimal cost-to-go function
for the N states of the environment.
There are two basic methods for computing an optimal policy.They are called pol-
icy iteration and value iteration.These two methods are described in Sections 12.4 and
12.5, respectively.
12.4
POLICY ITERATION
To set the stage for a description of the policy iteration algorithm, we begin by introduc-
ing a concept called the Q-factor,due to Watkins (1989).Consider an existing policy  for
which the cost-to-go function J(i) is known for all states i. The Q-factor for each state
and action 
is defined as the immediate cost plus the sum of the discounted costs 
of all successor states under policy , as shown by
(12.23)
where the action a  (i). Note that the Q-factors denoted by Q(i, a) contain more
information than the cost-to-go function J(i).For example,actions may be ranked on the
basis of Q-factors alone, whereas ranking on the basis of the cost-to-go function also
requires knowledge of the state-transition probabilities and costs. Note also that J*(i) in
Eq. (12.22) is obtained as min
 Q(i, a).
We may develop insight into the meaning of the Q-factor by visualizing a new
system whose states are made up of the original states 1, 2, ..., N and all the possible
state–action pairs (i, a), as portrayed in Fig. 12.2.There are two distinct possibilities that
can occur:
1. The system is in state (i, a),in which case no action is taken.Transition is made auto-
matically to state j, say, with probability pij(a), and a cost g(i, a, j) is incurred.
2. The system is in state i, say, in which case action 
is taken.The next state is
(i, a), deterministically.
a  ai
Q(i, a) = c(i, a) +  a
n
j=1
pij(a)J(j)
a  ai
i  x
Section 12.4
Policy Iteration
635
pij(a)
i
i, a
j
k
j, b
FIGURE 12.2
Illustration of two possible
transitions:The transition from state (i, a)
to state j is probabilistic, but the transition
from state i to (i, a) is deterministic.

In light of what was said previously in Section 12.2,the policy  is greedy with respect
to the cost-to-go function J(i) if, for all states, (i) is an action that satisfies the condition
(12.24)
The following two observations about Eq. (12.24) are noteworthy:
1. It is possible for more than one action to minimize the set of Q-factors for some
state, in which case there can be more than one greedy policy with respect to the
pertinent cost-to-go function.
2. A policy can be greedy with respect to many different cost-to-go functions.
Moreover, the following fact is basic to all dynamic-programming methods:
(12.25)
In this expression, * is an optimal policy.
With the notions of Q-factor and greedy policy at our disposal, we are ready to
describe the policy iteration algorithm. Specifically, the algorithm operates by alternat-
ing between two steps:
1. a policy evaluation step, in which the cost-to-go function for some current policy
and the corresponding Q-factor are computed for all states and actions;
2. a policy improvement step, in which the current policy is updated in order to be
greedy with respect to the cost-to-go function computed in step 1.
These two steps are illustrated in Fig. 12.3.To be specific, we start with some initial pol-
icy 0 and then generate a sequence of new policies 1, 2, .... Given the current policy
n, we perform the policy evaluation step by computing the cost-to-go function Jn(i) as
the solution of the linear system of equations (see Eq. (12.22))
(12.26)
Jn(i) = c(i, n(i)) +  a
N
j=1
pij(n(i))Jn(j),  i = 1, 2, ..., N
Q*(i, *(i)) = min
a Ai
Q*(i, a)
Q(i, (i)) = min
a Ai
Q(i, a)  for all i
636
Chapter 12
Dynamic Programming
Q-factor:
Q(i, a)
Policy
evaluation
Transition
probabilities
Policy
improvement
Policy

Cost-to-go
function
J
FIGURE 12.3
Block diagram of the policy 
iteration algorithm.

in the unknowns 
. Using these results, we then compute the
Q-factor for state–action pair (i, a) as follows (see Eq. (12.23)):
(12.27)
Next, we perform the policy improvement step by computing a new policy n1 defined
by (see Eq. (12.24))
(12.28)
The two-step process just described is repeated with policy n1 used in place of n,
until we have
in which case the algorithm is terminated with policy n. With 
, we may
then say that the policy iteration algorithm will terminate after a finite number of iter-
ations because the underlying Markov decision process has a finite number of states.
Table 12.1 presents a summary of the policy iteration algorithm based on Eqs. (12.26)
to (12.28).
In the reinforcement-learning literature, the policy iteration algorithm is referred
to as an actor–critic architecture (Barto et al., 1983). In this context, the policy improve-
ment assumes the role of actor, because it is responsible for the way in which an agent
acts. By the same token, the policy evaluation assumes the role of critic, because it is
responsible for criticizing the action taken by the agent.
12.5
VALUE ITERATION
In the policy iteration algorithm, the cost-to-go function has to be recomputed entirely
at each iteration of the algorithm, which is expensive. Even though the cost-to-go func-
tion for the new policy may be similar to that for the old policy, unfortunately there is
no dramatic shortcut for this computation.There is, however, another method for find-
ing the optimal policy that avoids the burdensome task of repeatedly computing the
cost-to-go function. This alternative method, based on successive approximations, is
known as the value iteration algorithm.
Jn + 1  Jn
Jn + 1(i) = Jn(i)  for all i
n+1(i) = arg min
aHAi
Qn(i, a),  i = 1, 2, ..., N
Qn(i, a) = c(i, a) +  a
N
j=1
pij(a)Jn(j),  a  ai and i = 1, 2, ..., N
Jn(1), Jn(2), ..., Jn(N)
Section 12.5
Value Iteration
637
TABLE 12.1
Summary of the Policy Iteration Algorithm
1. Start with an arbitrary initial policy 0.
2. For n  0, 1, 2, ..., compute 
and 
for all states 
and actions 
.
3. For each state i, compute
4. Repeat steps 2 and 3 until n1 is not an improvement on n, at which point the algorithm terminates
with n as the desired policy.
n+1(i) = arg min
a ai
Qn(i, a)
a  ai
i  x
Qn(i, a)
Jn(i)

The value iteration algorithm involves solving Bellman’s optimality equation,
given in Eq. (12.22), for each of a sequence of finite-horizon problems. In the limit,
the cost-to-go function of the finite-horizon problem converges uniformly over all
states to the corresponding cost-to-go function of the infinite-horizon problem as the
number of iterations of the algorithm approaches infinity (Ross, 1983; Bertsekas,
2007).
Let Jn(i) denote the cost-to-go function for state i at iteration n of the value iter-
ation algorithm. The algorithm begins with an arbitrary guess J0(i) for i  1, 2, ..., N. If
some estimate of the optimal cost-to-go function J*(i) is available, it should be used as
the initial value J0(i). Once J0(i) has been chosen, we may compute the sequence of cost-
to-go functions J1(i), J2(i), ..., using the value iteration algorithm
(12.29)
Application of the update to the cost-to-go function, described in Eq. (12.29) for state i,
is referred to as backing up of i’s cost. This backup is a direct implementation of
Bellman’s optimality equation, given in Eq. (12.22). Note that the values of the cost-to-
go functions in Eq. (12.29) for states i  1, 2, ..., N are backed up simultaneously on each
iteration of the algorithm. This method of implementation represents the traditional
synchronous form of the value iteration algorithm.3 Thus, starting from arbitrary initial
values J0(1), J0(2), ..., J0(N), the algorithm described by Eq. (12.29) converges to the
corresponding optimal values J*(1), J*(2), ..., J*(N) as the number of iterations, n,
approaches infinity. In other words, value iteration requires an infinite number of
iterations.
Unlike the policy iteration algorithm, an optimal policy is not computed directly
in the value iteration algorithm.Rather,the optimal values J*(1),J*(2),...,J*(N) are first
computed using Eq. (12.29). Then a greedy policy with respect to that optimal set is
obtained as an optimal policy.That is,
(12.30)
where
(12.31)
A summary of the value iteration algorithm, based on Eqs. (12.29) to (12.31), is
presented in Table 12.2.This summary includes a stopping criterion for Eq. (12.29).
EXAMPLE 1  Relationship Between Value Iteration and Policy Iteration
To understand how value iteration works in relation to policy iteration, consider the illustrative
example of Fig. 12.4. Part (a) of the figure depicts the backup operations involved in computing
the Q-factor Q(i,a) for policy iteration,and part (b) depicts the corresponding backup operations
for computing the Q-factor Q*(i, a) for value iteration. Each unshaded small circle in the figure
represents a state and each red-shaded small circle represents a state-action pair. Suppose we
start from state j. The agent may take any one of three possible actions, and the environment
Q*(i, a) = c(i, a) +  a
N
j=1
pij(a)J*(j),  i = 1, 2, ..., N
*(i) = arg min
aai
 Q*(i, a),  i = 1, 2, ..., N
Jn+1(i) = min
   
aAi
 ec(i, a) +  a
N
j=1
pij(a)Jn(j) f,  i = 1, 2, ..., N
638
Chapter 12
Dynamic Programming

could respond with any one of six possible state-action pairs; (i, a) is one such state-action pair,
for which the transition cost is denoted by g(i, j).
Examining Fig. 12.4, we see that the backup operations for policy iteration and those for
value iteration are identical, except for one basic difference:Value iteration requires that the max-
imum be taken over all possible state-action pairs as indicated in Fig. 12.4(b).
■
Section 12.5
Value Iteration
639
TABLE 12.2
Summary of the Value Iteration Algorithm
1. Start with arbitrary initial value J0(i) for state i  1, 2, ..., N.
2. For n  0, 1, 2, ..., compute
Continue this computation until
where 
is a prescribed tolerance parameter. It is presumed that 
is sufficiently small for Jn(i) to be
close enough to the optimal cost-to-go function J*(i).We may then set
3. Compute the Q-factor
Hence, determine the optimal policy as a greedy policy for J*(i):
*(i) = arg min
aai
Q*(i, a)
Q*(i, a) = c(i, a) +  a
N
j=1
pij(a)J*(j) for a  ai and
i = 1, 2, ..., N
Jn(i) = J*(i)       for all states i


Jn+1(i) - Jn(i) 6   for each state i
Jn+1(i) = min
aai
ec(i, a) +  a
N
j=1
pij(a)Jn(j), f ,  a  ai
i = 1, 2, ..., N
FIGURE 12.4
Illustrative backup diagrams for (a) policy iteration and (b) value iteration.
(a)
(b)
g(i, j)
State
i
State
j
g(i, j)
State
i
State
j
Action
a
Action
a
Take the
minimum
over all
possible
state-action
pairs

EXAMPLE 2
The Stagecoach Problem
To illustrate the usefulness of the Q-factor in dynamic programming, we consider the stagecoach
problem: A fortune seeker in Missouri decided to go west to join the gold rush in California in the
mid-nineteenth century (Hiller and Lieberman, 1995). The journey required traveling by stage-
coach through unsettled country, which posed a serious danger of attack by marauders along the
way.The starting point of the journey (Missouri) and the destination (California) were fixed, but
there was considerable choice as to which other eight states to travel through en route, as shown
in Fig. 12.5. In this figure, we have the following conditions:
•
There is a total of 10 states, with each state represented by a letter.
640
Chapter 12
Dynamic Programming
•
The direction of travel is from left to right.
•
There are four stages (i.e., stagecoach runs) from the point of embarkation in state A
(Missouri) to the destination in state J (California).
•
In moving from one state to the next, the action taken by the fortune seeker is to move up,
straight, or down.
•
There is a total of 18 possible routes from state A to state J.
Figure 12.5 also includes the cost of a life insurance policy for taking any stagecoach run, based
on a careful evaluation of the safety of that run.The problem is to find the route from state A to
state J with the cheapest insurance policy.
To find the optimum route, we consider a sequence of finite-horizon problems, starting from
the destination in state J and working backward.This procedure is in accordance with Bellman’s
principle of optimality, described in Section 12.3.
Calculating the Q-factors for the last stage before the destination,we readily find in Fig.12.6a
that the terminal Q-values are as follows:
These numbers are indicated on states H and I, respectively, in Fig. 12.6a.
 Q(I, up)  
 =   4
 Q(H, down)   =   3
B
2
2
4
4
4
4
4
3
3
3
3
1
3
5
3
6
6
4
7
1
A
C
D
G
I
F
J
E
H
FIGURE 12.5
Flow graph for stagecoach problem.

Section 12.5
Value Iteration
641
(a)
(b)
(c)
(d)
4
4
4
4
4
4
4
4
4
4
4
1
4
4
4
7
7
7
7
6
6
11
11
11
7
7
8
8
4
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
7
6
1
1
1
1
4
3
H
H
H
H
E
E
E
B
B
C
C
A
D
D
F
F
F
G
G
G
I
I
I
I
J
J
J
J
FIGURE 12.6
Steps involved in calculating the Q-factors for the stagecoach problem.
The routes (printed in red) from A to J are the optimal ones.

Next, moving back by one more stage and using the Q-values in Fig. 12.6a, we have the
following Q values:
Since the requirement is to find the route with the smallest insurance policy, the Q-values indi-
cate that only the stage runs 
, and 
should be retained and the remain-
ing ones should be pruned, as indicated in Fig. 12.6b.
Moving back one further stage, we repeat the calculations of the Q-factors for states B, C,
and D in the manner described and retain only those stage runs from the states B, C, and D that
are covered by the lowest insurance costs, obtaining the picture depicted in Fig. 12.6c.
Finally, moving back one last stage and proceeding in the same way as before, we obtain the
picture depicted in Fig. 12.6d. From this figure, we see that there are indeed three optimal routes:
all three of which are printed in red in Fig. 12.6d. They all yield a total cost of 11. Note also that
all three optimal routes bypass B even though the immediate cost from A to B is the smallest
of all three possible choices in moving forward.
■
12.6
APPROXIMATE DYNAMIC PROGRAMMING: DIRECT METHODS
Bellman’s dynamic programming is elegant. However, its development assumes the
availability of an explicit model that encompasses the transition probability from one
state to another. Unfortunately, in many practical situations, such a model is not avail-
able. However, provided that a dynamic system is well structured and its state space has
a manageable size, we may use Monte Carlo simulation to explicitly estimate the tran-
sition probabilities and associated transition costs; by its very nature, the estimation so
performed is approximate. We refer to this approach to approximate dynamic pro-
gramming as direct, because the use of simulation as described herein facilitates the
direct application of dynamic programming methods.
For an illustrative example, consider a multiuser communication network about
which the issue of interest is dynamic channel allocation. Suppose that costs are assigned
to patterns of channel use, depending on the distances between calls across a given chan-
nel.To be specific,patterns in which channel-sharing calls involve users close to each other
are favored over patterns in which channel-sharing calls are far apart from each other. In
other words,the communication network is equipped with a well-developed cost structure
for servicing calls made by users operating in a disciplined manner within the network.
Given such a dynamic system, it is possible to use Monte Carlo simulation to permit the
direct approximation of dynamic programming to the network (Nie and Haykin, 1998).
A S D S F S I S J
A S D S E S H S J
A S C S E S H S J
G S H
E S H, F S I
 Q(G, straight)   =   3 + 4 = 7
 Q(G, up)  
 =   3 + 3 = 6
 Q(F, down)  
 =   3 + 4 = 7
 Q(F, up)  
 =   6 + 3 = 9
 Q(E, down)  
 =   4 + 4 = 8
 Q(E, straight)   =   1 + 3 = 4
642
Chapter 12
Dynamic Programming

Basically, the rationale behind the direct approximation of dynamic programming
is to use computer simulation to generate multiple system trajectories, which lead to the
construction of a look-up table with a separate entry for the value of each state; the
larger we make the number of system trajectories, the more reliable the simulation
results will naturally be. In particular, a separate variable J(i) is kept in memory every
time state i is visited by a trajectory of the simulated system. In so doing, we will have
simulated a dynamic system with probabilistic transitions from state i to state j and
incurring the immediate transition cost g(i, j).
The stage is thus set for direct approximation of the two basic dynamic program-
ming methods: value iteration and policy iteration. In particular,
Section 12.7
Temporal-Difference Learning
643
• in the case of value iteration, we obtain temporal-difference learning;
• in the case of policy iteration, we obtain Q-learning.
These two algorithms, respectively discussed in Sections 12.7 and 12.8, are well known
in the reinforcement learning literature.We may therefore view reinforcement learning
as the direct approximation of dynamic programming.
One final comment is in order: Naturally, the construction of a look-up table is
memory limited. It follows therefore that the practical use of temporal-difference and Q-
learning is limited to situations in which the state-space is of moderate size.
12.7
TEMPORAL-DIFFERENCE LEARNING
The idea of temporal-difference learning was first described in Sutton (1988).We begin
the discussion by considering the simplest version of this approximate form of dynamic
programming, called the TD(0) algorithm;“TD” stands for “temporal difference.”
TD(0) Learning Algorithm
Let  be a policy that leads to the evolution of the states of a Markov decision process.
The states are described by the sequence {in}N
n0; the total number of state transitions
is N, and the terminal state iN  0. Let g(in, in1) be the immediate cost incurred in the
transition from state in to state in1, where the index n  0, 1, ..., N - 1.Then according
to the Bellman equation, the cost-to-go function is defined by
(12.32)
where, for each n, the ensemble averaging is performed over all possible occurrences of
the state in1. From a practical perspective, what we need is an iterative algorithm that
avoids the need for ensemble averaging.To this end, we may invoke the Robbins–Monro
stochastic approximation, which was discussed in Chapter 3.
To recall the essence of this stochastic approximation, consider the relationship
where r is the old value,  is a small positive step-size parameter that could change from
one iteration to the next, and the new variable is a random variable generated accord-
ing to the distribution 
;as in previous chapters,the plus sign used as a superscript
pVR(vr)
v
r+ = (1 - )r + g(r, v)
J(in) = [g(in, in+1) + J(in+1)],  n = 0, 1, ..., N - 1
in r signifies “updating.”

Thus,applying the Robbins–Monro stochastic approximation to the Bellman equa-
tion given in Eq. (12.32), we obtain
(12.33)
where J(in) on the left-hand side is an updated estimate that is computed every time the
state in is visited.To simplify matters,we now introduce the temporal difference, defined by
(12.34)
which represents the difference between two quantities:
• the overall cost-to-go function based on the simulated outcome of the current
state—namely, g(in, in1)  J (in1);
• the current estimate J(in).
In effect, the temporal difference dn provides the signal to determine whether the cur-
rent estimate J(in) should be increased or decreased. Using the definition of Eq. (12.34),
we may now rewrite the iterative algorithm of Eq. (12.33) in the simplified form
(12.35)
where J(in) is the current estimate, J(in) is the updated estimate, and the product term 
nd(n)
is the correction applied to the current estimate in order to produce the updated one.
The one-step update rule of Eq. (12.35) is commonly referred to as the TD(0)
algorithm; the rationale for this terminology will become apparent later in the section.
This update is performed every time the state in is visited and the temporal difference dn
becomes available.
Monte Carlo Simulation Algorithm
Equation (12.35) describes one particular iterative algorithm,derived from Bellman’s equa-
tion. For another point of view and a different algorithm, consider the cost-to-go-function
(12.36)
where, this time, the expectation operator is applied across the individual costs pertain-
ing to the entire sequence of state transition. Here again, applying the Robbins–Monro
stochastic approximate to Eq. (12.36), we obtain (after collecting common terms)
(12.37)
where k is a time-varying step-size (learning-rate) parameter.This update formula may
be expressed in the equivalent form
o
+ g(in+1, in+2) + J(in+2) - J(in+1)
J+(in) = J(in) + k[g(in, in+1) + J(in+1) - J(in)
J+(in) = J(in) + k a
a
N-n-1
k=0
g(in+k, in+k+1) - J(in) b
J(in) = c
a
N-n-1
k=0
g(in+k, in+k+1)d,  n = 0, 1, ..., N - 1

J+(in) = J(in) + dn
dn = g(in, in+1) + J(in+1) - J(in),  n = 0, 1, ..., N - 1
= J(in) + [g(in, in+1) + J(in+1) - J(in)]
J+(in) = (1 - )J(in) + [g(in, in+1) + J(in+1)]
644
Chapter 12
Dynamic Programming

where, in the last line, we have used the property that the terminal state iN  0, which,
correspondingly, means that the cost 
.Accordingly, invoking the definition of
temporal difference introduced in Eq. (12.34), we find that the iterative algorithm of
Eq. (12.37) assumes the simplified form
(12.38)
which embodies the entire sequence of temporal differences.
In fact, Eq. (12.38) is an iterative implementation of Monte Carlo simulation of the
trajectory {in, in1,...,iN},where iN  0—hence the reference to this equation as the Monte
Carlo simulation algorithm.To justify this statement, we make two assumptions:
1. The different simulated system trajectories are statistically independent.
2. Each trajectory is generated in accordance with a Markov decision process under
policy .
To proceed with the justification,let c(in) denote the total sum of the costs incurred by the
sequence {in, in1, ..., iN} when the state in is encountered at time n of the simulation;that is,
(12.39)
Then, we may use
(12.40)
which is computed after having visited the state in for a total of T simulation trials.Hence,
the estimate of the ensemble-averaged cost-to-go function is
(12.41)
It is a straightforward matter to show that the sample mean of Eq. (12.40) may be cal-
culated using the iterative formula
(12.42)
Starting with the initial condition
and setting the step-size parameter
(12.43)
we find that Eq. (12.42) is simply a rewrite of the iterative algorithm of Eq. (12.38), using
the new notations introduced for treating the Monte Carlo simulation viewpoint of tem-
poral differences.
n = 1
n,  n = 1, 2, ...
J(in) = 0
J+(in) = J(in) + n(c(in) - J(in))
J(in) = [c(in)]  for all n
J(in) = 1
T a
T
n=1
c(in)
c(in) =
a
N-n-1
k=0
g(in+k, in+k+1),  n = 0, 1, ..., N - 1
J+(in) = J(in) +
a
N-n-1
k=0
dn+k
J(iN) = 0
+ g(iN-1, iN) + J(iN) - J(iN-1)]
+ g(iN-2, iN-1) + J(iN-1) - J(iN-2)
Section 12.7
Temporal-Difference Learning
645

Unified View of Temporal-Difference Learning: TD()
In the discussion just presented on temporal-difference learning, we derived two limit-
ing forms of iterative algorithms:
• The iterative algorithm of Eq.(12.35),derived from the Bellman equation,accounts
for the immediate cost of transition from state in to state in1.
• The iterative algorithm of Eq. (12.38), rooted in Monte Carlo simulation,
accounts for the cumulative cost incurred over the entire sequence of state
transitions.
Clearly, there must be a middle ground between these two iterative procedures, which
deserves consideration of its own.To pursue this middle ground, we introduce two mod-
ifications (Bertsekas and Tsitsiklis, 1996):
1. We expand the Bellman equation so as to account for the individual costs incurred
in the first l  1 state transitions for some fixed l:
(12.44)
2. With no prior knowledge that would favor one desirable value of l over another,
we form a weighted average over all the possible multistep Bellman equations by
multiplying the right-hand side of Eq. (12.44) by (1 - 
)
l and then summing over
l for some fixed 
 < 1:
Since we are dealing with linear equations,we may interchange the order of summations:
(12.45)
We now take note of the following two equalities:
1.
2.
Accordingly, we may recast Eq. (12.45) in the equivalent form
(12.46)
J(in) = c a
q
k=0

k(g(in+k, in+k+1) + 
kJ(in+k+1) - 
kJ(in+k))d + J(in)
= a
q
l=0

lJ(in+l+1) - a
q
l=0

lJ(in+l) + J(in)
(1 - 
) a
q
l=0

lJ(in+l+1) = a
q
l=0

lJ(in+l+1) - a
q
l=0

l+1J(in+l+1)
= 
k
(1 - 
) a
q
l=k

l = a
q
l=k

l - a
q
l=k

l+1
J(in) = c(1 - 
) a
l
k=0
g(in+k, in+k+1) a
q
l=k

l + (1 - 
) a
q
l=0

lJ(in+l+1)d
J(in) = (1 - 
)c a
q
l=0

l a a
l
k=0
g(in+k, in+k+1) + J(in+l+1) b d
J(in) = c a
l
k=0
g(in+k, in+k+1) + J(in+l+1)d
646
Chapter 12
Dynamic Programming

where, for the three terms inside the square brackets on the right-hand side, we have
simply used k in place of l for compactness of the presentation.
The stage is now set for us to simplify matters by making use of the definition of
the temporal difference introduced in Eq. (12.34).To do so, we rewrite Eq. (12.46) one
more time in the simplified form
(12.47)
Recognizing that for some fixed value of 
, we have 
for all k in accordance
with Bellman’s equation, we hardly find Eq. (12.47) surprising. In a sense, we may sum
up the net result of the analysis following the modifications under points 1 and 2 as 
merely adding the expectation 
to the right-hand side of the equality 
for all n. Nevertheless, this result does have significant implica-
tions when we go on to apply the Robbins–Monro stochastic approximation, as shown
next.
Specifically, the application of this approximation to Eq. (12.47) yields the itera-
tive algorithm
which, after the cancellation of terms, simplifies to
(12.48)
The iterative algorithm of Eq. (12.48) is commonly referred to as TD(
); as men-
tioned previously,TD stands for “temporal difference”.This algorithm was originated by
Sutton (1988).It is noteworthy that for the derivation of this algorithm,we used ideas from
Bellman’s dynamic programming,Monte Carlo simulation,and stochastic approximation.
Moreover, TD(
) includes the iterative algorithms of Eqs. (12.35) and (12.38) as
two special cases:
1. If we let 
  0 and use the convention that 00  1, then Eq. (12.48) reduces to
which is a repeat of Eq. (12.35), derived using the dynamic-programming method.
Indeed, it is for this reason that the algorithm of Eq. (12.35) is referred to as TD(0),
as pointed out earlier.
2. For the other limiting case, if we let 
  1, then Eq. (12.48) reduces to
J+(in) = J(in) +  a
N-n-1
k=0
dn+k
J+(in) = J(in) + dn
J+(in) = J(in) +  a
q
k=n

k-ndk
J+(in) = (1 - )J(in) +  a a
q
k=n

k-ndk + J(in) b
[J(in)] = [J(in)]
c a
q
k=n

k-ndkd = 0
[dk] = 0
 = c a
q
k=n

k-ndkd + J(in),  for n = 0, 1, ..., N - 1
 J(in) = c a
q
k=0

kdn+kd + J(in) 
Section 12.7
Temporal-Difference Learning
647

which, except for the scaling factor , is a repeat of Eq. (12.38), derived using the
Monte Carlo evaluation method. Here it is noted that the temporal difference dn
is zero for n equal to or greater than the planning horizon N.
In summary, we may state:
The TD methods, described in Eq. (12.48), are on-line prediction methods that learn how to
compute their estimates, partly, on the basis of other estimates.
In other words, TD methods are of a bootstraping kind. Most importantly, they do not
require a model of the environment.
Practical Considerations
According to Bertsekas and Tsitsiklis (1996), the estimate J(in), for some state in, pro-
duced by the TD(
) algorithm is guaranteed to converge to the ensemble-averaged
value J(
) for policy , provided that two conditions are satisfied:
1. The state in is visited by trajectories infinitely often for all n.
2. The step-size parameter  is allowed to diminish toward zero at an appropriate rate.
The proof of convergence presented in Bertsekas and Tsitsiklis (1996) appears to show
that there is no theoretical obstacle to changing the parameter 
 in the course of learning
performed by the TD(
) algorithm.The theoretical considerations presented therein suggest
that a sensible strategy for choosing a suitable value for 
 is to start the operation of the
TD(
) algorithm with a large value of 
 close to unity (i.e., initially favoring Monte Carlo
estimation of the ensemble-averaged cost-to-go function) and then allow 
 to decrease
slowly to zero (i.e.,moving toward estimates produced in accordance with the Bellman equa-
tion). In a loose sense, 
 is put through a form of annealing over the course of time.
12.8
Q-LEARNING
The TD(
) algorithm, derived in the previous section as a stochastic approximation of
dynamic programming, is a model-free algorithm. In this section, we describe another
stochastic algorithm, known as Q-learning, that also does not require explicit knowl-
edge of the environment. Q-learning was first derived in Watkins (1989). The letter Q
in Q-learning does not signify anything special; it is merely a notation that was adopted
by Watkins in his original derivation of the algorithm.
To motivate the discussion of Q-learning, consider the reinforcement-learning sys-
tem depicted in Fig. 12.1. The behavioral task of this system is to find an optimal (i.e.,
minimal-cost) policy after trying out various possible sequences of actions and observ-
ing the costs incurred and the state transitions that occur. The policy used to generate
behavior is called the behavior policy. This policy is separated from the estimation pol-
icy, the purpose of which is to estimate the value of a policy. With these two policies
being separated from each other, Q-learning is said to be an off-policy method for con-
trol.A side benefit gained from this separation is that the estimation policy can be greedy,
while the behavior policy is left to sample all possible actions. An off-policy method is
to be distinguished from an on-policy method, where the value of a policy is being esti-
mated while it is being used for control at the same time.

648
Chapter 12
Dynamic Programming

The Q-learning Algorithm
To proceed with the derivation of the Q-learning algorithm, let 
(12.49)
denote a four-tuple sample consisting of a trial action an performed on state in that results
in a transition to the new state jn  in  1 at a cost defined by
(12.50)
where n denotes discrete time. Given such a scenario, we may now raise the following
fundamental question:
Is there any on-line procedure for learning an optimal control policy through experience that is
gained solely on the basis of observing samples of the form sn defined in Eqs.(12.49) and (12.50)?
The answer to this question is an emphatic yes, and it is to be found in Q-learning.4
The Q-learning algorithm is an incremental dynamic-programming procedure that
determines the optimal policy in a step-by-step manner. It is highly suited for solving
Markov decision problems without explicit knowledge of the transition probabilities.
However, in a manner similar to TD(
), successful use of Q-learning hinges on the
assumption that the state of the environment is fully observable, which, in turn, means
that the environment is a fully observable Markov chain.
We recall from Section 12.4 that the Q-factor Q(i, a) for state–action pair (i, a) is
defined by Eq. (12.23), and Bellman’s optimality equation is defined by Eq. (12.22). By
combining these two equations and using the definition of the immediate expected cost
c(i, a) given in Eq. (12.20), we obtain
(12.51)
which can be viewed as a two-step version of Bellman’s optimality equation.The solu-
tions to the linear system of equations in Eq.(12.51) define the optimal Q-factors Q*(i, a)
uniquely for all state–action pairs (i, a).
We may use the value iteration algorithm formulated in Section 12.4 in terms of the
Q-factors to solve this linear system of equations.Thus,for one iteration of the algorithm,
we have
The small step-size version of this iteration is described by
(12.52)
where  is a small learning-rate parameter that lies in the range 0 <  < 1.
As it stands, an iteration of the value iteration algorithm described in Eq. (12.52)
requires knowledge of the transition probabilities. We may eliminate the need for this
prior knowledge by formulating a stochastic version of this equation. Specifically, the
Q* (i, a) = (1 - )Q(i, a) +  a
N
j=1
pij(a) ag(i, a, j) +  min
bai
 Q(j, b) b for all (i, a)
Q*(i, a) = a
N
j=1
pij(a) a g(i, a, j) +  min
bai
 Q(j, b) b  for all (i, a)
Q*(i, a) = a
N
j=1
pij(a) a g(i, a, j) +  min
bai
 Q*(j, b) b  for all (i, a)
gn = g(in, an, jn)
sn = (in, an, jn, gn)
Section 12.8
Q-Learning
649

averaging performed in an iteration of Eq. (12.52) over all possible states is replaced by
a single sample, thereby resulting in an update for the Q-factor given as
(12.53)
where
(12.54)
and j is the successor state to the state i and n(i, a) is the learning-rate parameter at time-
step n for the state–action pair (i, a).The update equation of Eq. (12.53) applies to the
current state–action pair (in, an), for which j  jn in accordance with Eq. (12.49). For all
other admissible state–action pairs, the Q-factors remain unchanged, as shown by
(12.55)
Equations (12.53) to (12.55) constitute one iteration of the Q-learning algorithm.
Convergence Theorem5
Suppose that the learning-rate parameter n(i, a) satisfies the conditions
(12.56)
Then,the sequence of Q-factors {Qn(i, a)} generated by the Q-learning algorithm converges with
probability 1 to the optimal value Q*(i, a) for all state–action pairs (i, a) as the number of iter-
ations n approaches infinity, provided that all state–action pairs are visited infinitely often.
An example of a time-varying learning parameter that guarantees convergence of the
algorithm is
(12.57)
where  and  are positive numbers.
Summarizing Remarks
The Q-learning algorithm may be viewed in one of two equivalent ways:
as a Robins-Monro stochastic approximation algorithm, or
as a combination of value iteration and Monte Carlo simulation.
The algorithm backs up the Q-factor for a single state-action pair at each iteration. Most
importantly, in the limit, the algorithm converges to the optimal Q-values without hav-
ing to form an explicit model of the underlying Markovian decision process. Once the
optimal Q-values have been computed, an optimal policy is determined with relatively
small computational effort by using the formula of Eq. (12.30).
The convergence of Q-learning to an optimal policy assumes the use of a look-up
table representation of the Q-factors Qn(i, a) for the state-action pair (i, a).This method
n =

 + n,  n = 1, 2, ...
a
q
n=0
n(i, a) = q  and  a
q
n=0
2
n(i, a) 6 q  for all (i, a) 
Qn+1(i, a) = Qn(i, a)  for all (i, a) Z (in, an)
Jn(j) = min
bai
 Qn(j, b)
Qn+1(i, a) = (1 - n(i, a))Qn(i, a) + n(i, a)[g(i, a, j) + Jn(j)] for (i, a) = (in, an)
650
Chapter 12
Dynamic Programming

of representation is straightforward and computationally efficient;however,it only works
provided that the joint input space consisting of state-action pairs is of a moderate size.
Exploration
In policy iteration, all potentially important parts of the state space should be explored.
In Q-learning, we have an additional requirement: All potentially profitable actions
should be tried as well. In particular, all admissible state–action pairs should be explored
often enough to satisfy the convergence theorem. For a greedy policy denoted by ,
only the state–action pairs (i,(i)) are explored.Unfortunately,there is no guarantee that
all profitable actions would be tried, even if the entire state space is explored.
What we need is a strategy that expands on Q-learning by providing a compromise
between two conflicting objectives (Thrun, 1992):
• exploration, which ensures that all admissible state–action pairs are explored often
enough to satisfy the Q-learning convergence theorem;
• exploitation, which seeks to minimize the cost-to-go function by following a greedy
policy.
One way to achieve this compromise is to follow a mixed nonstationary policy that
switches between an auxiliary Markov process and the original Markov process con-
trolled by a stationary greedy policy determined by Q-learning (Cybenko,1995).The aux-
iliary process has the following interpretation: The transition probabilities between
possible states are determined by the transition probabilities of the original controlled
process, with the added ingredient that the corresponding actions are uniformly ran-
domized. The mixture policy starts in any state of the auxiliary process and chooses
actions by following it; then it switches to the original controlled process, back and forth
between the two processes in the manner illustrated in Fig. 12.7. The time spent oper-
ating on the auxiliary process occupies a fixed number of steps, L, defined as twice the
longest expected time to visit all states of the auxiliary process.The time spent operat-
ing on the original controlled process increases progressively with every switch. Let nk
denote the times at which we switch from the auxiliary process to the original controlled
process, and mk denote the times at which we switch back to the auxiliary process, with
nk and mk defined as follows, respectively:
(12.58)
and
mk = nk + kL,  k = 1, 2, ...
nk = mk-1 + L,  k = 1, 2, ..., and m0 = 1
Section 12.8
Q-Learning
651
FIGURE 12.7
The time slots
pertaining to the auxiliary and
original control processes.
Auxiliary
process
m0  1
n1
n2
n3
m1
m2
Original
control
process

The auxiliary process is constructed in such a way that as 
, there is an infinite
number of visits to all states with probability 1, thereby guaranteeing convergence to the
optimal Q-factors. Moreover, as 
, the time spent by the mixed policy operating
in the auxiliary process becomes an asymptotically small fraction of the time spent oper-
ating in the original controlled process,which in turn means that the mixed policy asymp-
totically converges to a greedy policy. Hence, given the convergence of the Q-factors to
their optimal values, the greedy policy must indeed be optimal, provided that the pol-
icy becomes greedy slowly enough.
12.9
APPROXIMATE DYNAMIC PROGRAMMING: INDIRECT METHODS
Typically, large-scale dynamic systems have a state space of high dimensionality. Con-
sequently,when we deal with such a system,we experience the “curse”-of-dimensionality
problem, which refers to the exponential growth in computational complexity with the
dimension of the state space. Unfortunately, the curse of dimensionality renders not
only Bellman’s dynamic programming,but,naturally,its two direct approximate forms—
temporal difference learning and Q-learning—intractable. To illustrate this important
practical problem, consider a dynamic-programming problem involving a total of N pos-
sible states and M admissible actions for each state; in such a system, each iteration of
the value-iteration algorithm, for example, requires N2M operations for a stationary
policy.This computational level of operations makes it impossible to complete even one
iteration of the algorithm when N is very large.
In order to deal with difficult real-world problems involving a large number of
states, we may look to some appropriate form of approximate dynamic programming,
which is different from the direct approach discussed in Section 12.6.In particular,rather
than explicitly estimate the transition probabilities and associated transition costs as we
did in Section 12.6, we now do the following:
Use Monte Carlo simulation to generate one or more system trajectories, so as to approximate
the cost-to-go function of a given policy,or even the optimal cost-to-go function,and then opti-
mize the approximation in some statistical sense.
We refer to this approach to approximate dynamic programming as indirect,6 to distin-
guish it from the direct approach discussed in Section 12.6. In any event, it is presumed
that the state space of the simulated dynamic system has a dimensionality lower than that
of the original dynamic system.
Thus, having abandoned the notion of optimality, we may capture the goal of the
indirect approach to approximate dynamic programming in the following simple state-
ment (Werbos, 2003):
Do as well as possible, and not more.
In effect, performance optimality is traded off for computational tractability. This kind of
strategy is precisely what the human brain does on a daily basis: Given a difficult deci-
sion-making problem,the brain provides a suboptimal solution that is the “best”in terms
of reliability and available resource allocation.
k S q
k S q
652
Chapter 12
Dynamic Programming

With Bellman’s theory of dynamic programming as the frame of reference, the
goal of approximate dynamic programming may now be stated as follows:
Find a function 
that approximates the optimal cost-to-go function 
for state i, such
that the cost difference 
is minimized according to some statistical criterion.
With this goal in mind, we may now raise two basic questions:
Question 1: How do we choose the approximation function 
in the first place?
Question 2: Having chosen an appropriate approximation function 
how do
we adapt the weight vector w so as to provide the “best fit” to Bellman’s equation of
optimality?
To answer question 1, we have the option of a linear or nonlinear approximating func-
tion, which in turn will determine the answer to question 2. In what follows, we will con-
sider the linear approach first, followed by the nonlinear approach.
Linear Approach to Approximate Dynamic Programming
In this approach, the common way to proceed is to express the approximating function
as a linear function in the parameter vector w; that is,
(12.59)
where the 
are preprogrammed basis functions, or features, chosen by the designer of
the approximating scheme.The approximation of Eq. (12.59) is illustrated in Fig. 12.8.
The linear approach to approximate dynamic programming has a number of
appealing virtues:
(i) A linear function approximator is easy to formulate and analyze in mathematical
terms; consequently, the approximator’s underlying behavior is equally easy to
understand.
(ii) Ordinarily, mathematical formalism of a linear approximator provides insight into
what could go wrong in its actual operation, thereby making it feasible to fix what-
ever may have gone wrong.
(iii) Nonlinearities in the true cost-to-go function may be captured approximately in
specially chosen basis functions that can be constructed in light of intuition about
the dynamic programming problem at hand.
(iv) Above all, a linear approximator is relatively easy to implement.
With respect to point (iii), it should be noted that the choice of good basis functions
may pose serious difficulty in practice.
	i
= 	T
i w for all i
J~(i, w) = a
j
ijwj
J~(i, w)
J~(i, w),
J~(i, w)
J*(i) - J~(i, w)
J*(i)
J~(i, w)
Section 12.9
Approximate Dynamic Programming: Indirect Methods
653
FIGURE 12.8
Architectural layout of the linear approach to approximate dynamic programming.
Cost
function
of state i:
J(i)
Feature
extractor
Linear mapper:
weight vector w
Feature vector
(i)
Approximate cost
T(i)w

The choice of Eq. (12.59) provides the answer to question 1 for the linear approach.
As for the answer to question 2,the method most commonly used for providing the best fit
to the Bellman equation of optimality is the method of least squares, which was discussed
in Chapter 2. In Section 12.10, we will describe one way in which this issue is formulated.7
Nonlinear Approach to Approximate Dynamic Programming
Despite its appeal, the linear approach to approximate dynamic programming is con-
sidered to be a useful stepping-stone toward a still higher goal, expressed by consider-
ing the general case:
Recognizing that many of the dynamic environments encountered in practice are inherently
nonlinear,the approximate dynamic programming should not only be nonlinear itself,but also
be required to approximate “any” nonlinear dynamic environment to “any desired degree of
accuracy.”
In other words,what we are advocating here as the answer to question 1 for the nonlinear
approach is an approximating function that is a universal approximator.
From previous discussions presented on multilayer perceptrons and radial-basis
functions (RBFs), we know that both of these networks are universal approximators.
Moreover, recurrent multilayer perceptrons, to be discussed later in Chapter 15, are also
universal approximators. Given such a wide choice of networks, we assert that recur-
rent multilayer perceptrons provide the practical basis for the optimal design of non-
linear, approximate dynamic programming systems. Insofar as the issue of universal
approximation is concemed, we say so for two important reasons:
1. Unlike shallow architectures (exemplified by RBF networks), which have a single
nonlinear hidden layer and a linear output layer, recurrent multilayer perceptrons
can be designed to have two or more hidden layers. With one layer feeding the
other, recurrent multilayer perceptrons have the property of “learning features of
features,” whereby lower level features are progressively combined into more
abstract and higher level representations.In Bengio and LeCun (2007),it is argued
that deep architectures have the potential to generalize in nonlocal ways (i.e.,beyond
intermediate neighbors) and that such a property is crucial to making progress in
the design of machine learning algorithms applicable to highly complex tasks.
2. Recurrent multilayer perceptrons have built-in global feedback that could be
of a multiple kind (i.e., encompassing two or more network layers). Here, we
need to remind ourselves that brain systems have an abundance of global feed-
back built into them. In particular, there are almost always feedback connec-
tions between different areas in the brain that are at least as rich in number as
the feedforward connections (Churchland and Sejnowski, 1992). For example,
the recurrent projections from the primary visual cortex back to the lateral
geniculate nucleus (LGN) are about 10 times as numerous as the forward pro-
jections from the LGN to the primary visual cortex.8It is therefore no wonder
that the visual system is as powerful as it, and so it is with motor control, audi-
tory, and other parts of the brain. Given what we know about brain systems, we
are emboldened to say that global feedback is a facilitator of computational
intelligence—hence the practical importance of recurrent neural networks as
654
Chapter 12
Dynamic Programming

the candidate neural network in the simulation of approximate dynamic pro-
gramming systems.
Comparing recurrent multilayer perceptrons with ordinary multilayer perceptrons, we
see that they both share point 1 insofar as architectural depth is concerned. However,
it is property 2 on global feedback that gives recurrent multilayer perceptrons the advan-
tage over ordinary multilayer perceptrons. Given that we opt for the use of a recurrent
multilayer perceptron, the challenge is how to configure the feedforward and feedback
connections of the network in the most effective and efficient manner.
Now that we have answered question 1 for the nonlinear approach to approxi-
mate dynamic programming, we move on to address question 2 on how to adapt the
weight vector w in the approximating function 
so as to provide the best fit to
Bellman’s equation of optimality. For the present, it suffices to say the following:
The supervised training of a recurrent multilayer perceptron can be accomplished most effec-
tively with the use of a nonlinear sequential-state estimation algorithm that is derivative free.
By adopting such an approach to supervised learning,we need no longer be concerned with
how nonlinear the decision-making system happens to be. It is in situations of this kind
where derivative-free, nonlinear sequential state-estimation algorithms, to be discussed
in Chapter 14,become particularly important.The use of sequential state-estimation algo-
rithms for the supervised training of recurrent multilayer perceptrons (or ordinary mul-
tilayer perceptrons for that matter) is discussed in Chapter 15.
12.10
LEAST-SQUARES POLICY EVALUATION
For our first indirect method of approximate dynamic programming, we discuss an algo-
rithm called the least-squares policy evaluation algorithm, or the LSPE(
) algorithm for
short.The 
 plays a role in LSPE(
) similar to that in the TD(
) algorithm.
The basic idea behind the LSPE(
) algorithm may be summed up as follows:
Perform value iteration within a lower dimensional subspace spanned by a set of basis functions.
To be specific, let s denote the dimension of feature vector 
representing state i. We
may than define the N-by-s matrix
(12.60)
Let T denote a mapping that has the cost J as a unique fixed point for policy , and let 
denote a projection (of suitable norm) onto the subspace of vectors defined by the matrix
product 
w, where w is a parameter vector of dimension s. With simulation as the basis

ß
 = ≥
	T
1
	T
2
o
	T
N
¥
	i
J(i, w)
Section 12.10
Least-Squares Policy Evaluation
655
for the LSPE(
) algorithm, we may analytically describe it as follows (Bertsekas, 2007):
(12.61)
The algorithm is formulated in such a way that the additive simulation noise converges
to zero as the number of iterations, n, approaches infinity.
wn+1 = ßT(wn) + (simulation noise)

Background and Assumptions
Consider a finite-state Markov chain whose states, denoted by i  1, 2, ..., N, are con-
trolled by a stationary policy .We may recast Eq. (12.5) in the form
where in is the ith state at time n, is the discount factor,and g(in, in1) is the transition cost
from state in to in1.With a linear architecture in mind, the cost J(i) is approximated as
(12.62)
With the feature vector 	(i) assumed to be dimension s, it follows that the weight vec-
tor w must also have the same dimension. The issue of interest is to approximate the
parameterized cost
within the subspace
(12.63)
that is spanned by the columns of the matrix . Note that dimensionality of the matrix
product w is equal to the number of possible states, N.
Forthwith, we make two assumptions:
1. The Markov chain has positive steady-state probabilities; that is,
(12.64)
The implication of this assumption is that the Markov chain has a single recurrent
class with no transient states.
2. The rank of matrix  is s.
The implication of this second assumption is that the columns of the feature matrix
, and therefore the basis functions represented by w, are linearly independent.
Projected Value Iteration for Policy Evaluation
With value iteration in mind, we may use Eqs. (12.20) and (12.29) to write
(12.65)
where T denotes a mapping. Now, let
(12.66)
g = ≥
a
j
p1jg(1, j)
a
j
p2jg(2, j)
o
a
j
pNjg(N, j)
¥
TJ(i) = a
N
j=1
pij(g(i, j) + J(i)),    i = 1, 2, ..., N
lim
nS q 1
n a
n
k=1
P(ik = j|i0 = i) = j 7 0     for all i
s = {w|w  s}
J~(i, w)
 = 	T(i)w
 J(i) L J~(i, w)
J(i) = c a
q
n=0
ng(in, in+1)i0 = id
656
Chapter 12
Dynamic Programming

(12.67)
and
(12.68)
where we used the approximate formula of Eq. (12.62).We may then rewrite Eq. (12.65)
in terms of the vectors g, J, and the stochastic matrix P in the compact matrix form
(12.69)
We are interested in an approximate form of the value iteration
which is confined within the subspace 
and which involves the projection of the value
iterate onto . Specifically, in light of Eq. (12.68), we may write
(12.70)
where,as pointed our earlier,. denotes projection onto the subspace s.Equation (12.70)
is referred to as the projected value iteration (PVI) method, the essence of which may be
stated as follows:
At iteration n, the current iterate wn is operated on by the mapping T and the new vector
is projected onto the subspace s, thereby yielding the updated iterate wn1.
The PVI method is illustrated in Fig. 12.9.
The PVI method may be viewed as a projected or approximate form of the value
iteration method for solving the Bellman equation. In Bertsekas (2007) the following
findings are demonstrated:
1. The mappings T and 
are contractions of modulus with respect to the
ßT
T(wn)
wn+1 = ßT(wn),   n = 0, 1, 2, ...
s
s
Jn+1 = TJn
TJ = g + PJ
L w
J = ≥
J(1)
J(2)
o
J(N)
¥
P = ≥
p11
p12
p
p1N
p21
p22
p
p2N
o
o
o
pN1
pN2
p
pNN
¥
Section 12.10
Least-Squares Policy Evaluation
657
Subspace s
Value iterate
T(wn)  g  Pwn
wn1
wn
Projection onto s
0
FIGURE 12.9
Illustration
of the projected value
iteration (PVI) method.
weighted Euclidean norm 
, where, 1, 2, ..., N (representing the steady-state
7  7 

probabilities of the Markov chain) play the role of scaling factors in defining the
Euclidean norm.
2. The matrix product w* is the unique fixed point of the mapping 
for the weight
vector w*. (In the context of the present discussion,when we speak of a fixed point,
we mean a solution, that is, a vector w* that satisfies the condition 
.)
We may therefore say that the PVI method is an analytic method for approximating the
Bellman equation.
Despite its good points, however, the PVI method has two serious drawbacks:
1. With w having the dimensionality N,it follows that the transformed vector T(wn)
is an N-dimensional vector, so, for large-scale applications for which N is large, the
computational complexity of the method becomes unmanageable.
2. The projection of the vector 
onto the subspace s requires knowledge of
the steady-state probabilities 1, 2, ..., N. In general, these probabilities are not
known.
Fortunately, both of these drawbacks can be alleviated through the use of Monte Carlo
Simulation.
From Projected Value Iteration to Least-Squares Policy Evaluation
Using least-squares minimization for the projection ., we may express Eq. (12.70) in the
form
(12.71)
Equivalently, we may express the least-squares version of the PVI algorithm in the form
(12.72)
To perform the optimization in Eq.(12.72) in practical terms,we propose to approx-
imate it through the use of Monte Carlo simulation by generating an infinitely long tra-
jectory (i0, i1, i2, ...) for state i and updating the weight vector wn after each iteration (in,
in1) in accordance with the formula
(12.73)
For obvious reasons, this recursion is called least-squares policy evaluation, or LSPE for
short. As illustrated in Fig. 12.10, LSPE may be viewed as the PVI with an additive
simulation noise that accounts for the least-squares approximation.
Moreover,due to the contraction property of the joint mapping .T and the asymp-
totically diminishing nature of the simulation noise,the LSPE converges to the same limit
as the PVI, namely, the unique weight vector w* that satisfies the fixed-point equation
(12.74)
w* = ßT(w*)
wn+1 = arg min
W  a
n
k=1
(	T(ik)w - g(ik, ik+1) - 	T(ik+1)wn)2
wn+1 = arg min
W  a
N
i=1
i °	T(i)w - a a
N
j=1
pijg(i, j) + 	T(j)wn b ¢
wn+1 = arg min
W 7w - T(wn) 7 2

T(wn)
ßTw* = w*
ßT
658
Chapter 12
Dynamic Programming

The LSPE ()
In a manner similar to the way we introduced TD(
) in Section 12.7, let us introduce the
temporal difference (see Eq. (12.34))
(12.75)
Accordingly, we may express the simulation-based LSPE(
) algorithm as follows:
(12.76)
where (i0, i1, i2, ...) is an infinitely long trajectory generated by Monte Carlo simulation.
In words,
At iteration n  1 of the LSPE(
) algorithm, the updated weight vector wn1 is computed as
the particular value of the weight vector w that minimizes the least-squares difference between
the following two quantities:
•
the inner product 	T(ik)w approximating the cost function J(ik);
•
its temporal difference counterpart
which is extracted from a single simulated trajectory for k  0, 1, ..., n.
Note that the current value of the weight vector wn is maintained constant in perform-
ing each iteration of the least-squares minimization in Eq. (12.76).
The approximate nature of the LSPE(
) algorithm is attributed to two factors:
1. The use of simulation-based empirical frequencies for estimating the steady-state
probabilities i and transition probabilities pij.
2. The use of the finite discounted sum of temporal differences in Eq. (12.76) for ap-
proximating the PVI method.
Nevertheless, as the number of iterations, n, approaches infinity, the empirical frequen-
cies converge to the true probabilities and the finite discounted sum converges to an
infinitely discounted sum. Consequently, the LSPE(
) algorithm converges to its PVI
counterpart in an asymptotic sense.
	T(ik)wn + a
n
m=k
(
)m-k dn(im, im+1)
wn+1 = arg min
W
a
n
k=0
a	T(ik)w - 	T(ik)wn - a
n
m=k
(
)m-kdn(im, im+1) b
2
dn(ik, ik+1) = g(ik, ik+1) + 	T(ik+1)wn - 	T(ik)wn 
Section 12.10
Least-Squares Policy Evaluation
659
Subspace s
wn1
wn
Projection onto s
Simulation noise
0
Value iterate
T(wn)  g  Pwn
FIGURE 12.10
Illustration of the least-
squares policy evaluation
(LSPE) as a stochastic
version of the projected
value iteration (PVI).

The following insightful comment on the convergence behavior of the LSPE(
)
algorithm is particularly noteworthy:
The LSPE(
) algorithm consists of a deterministic component that converges rapidly and a
stochastic component that converges slowly towards zero, with the deterministic component
dominating the stochastic fluctuations in the early iterations of the algorithm.
This statement is borne out by the results of computer simulations presented in Bertsekas
et al. (2004). In particular, the results presented therein demonstrate that the LSPE(
)
algorithm for 0

1 is indeed a sound algorithm, in that it is fast in convergence
and reliable in performance. Generally speaking, a choice of 
 close to unity improves
the computational accuracy (i.e., it makes the matrix product 
closer to J(i)),
	T(i)w*


660
Chapter 12
Dynamic Programming
Q-factor:
Q(i, a, w)
Neural network
for approximate
policy evaluation:
w
Transition
probabilities
Policy
improvement
Policy

Approximate
cost-to-go
J~(i,  w)
FIGURE 12.11
Block diagram of the
approximate policy iteration algorithm.
but increases the effect of simulation noise, so more samples and longer trajectories are
required to achieve convergence.
12.11
APPROXIMATE POLICY ITERATION
The LSPE algorithm provides a powerful linear approach for approximate dynamic pro-
gramming. In this section, we describe the use of neural networks as a tool for the non-
linear approach to approximate dynamic programming. To this end, suppose then we
have a dynamic-programming problem for which the numbers of possible states and
admissible actions are very large, making the use of a traditional approach impractical.
It is assumed that we do have a model of the system; that is, the transition probabilities
pij(a) and the observed costs g(i, a, j) are all known. To deal with this situation, we pro-
pose to use an approximation to policy iteration, based on Monte Carlo simulation and
the method of least squares, as described next.
Figure 12.11 shows a simplified block diagram of the approximate policy-iteration
algorithm, where the policy evaluation step in Fig. 12.3 has been replaced with an
approximate one.Thus,the approximate policy-iteration algorithm proceeds by alternating
between an approximate policy evaluation step and a policy improvement step as follows:
1. Approximate policy evaluation step. Given the current policy ,we compute a cost-
to-go function 
approximating the actual cost-to-go function J(i) for all
J~(i, w)

states i. The vector w is the weight vector of the neural network used to perform
the approximation.
2. Policy improvement step. Using the approximate cost-to-go function 
, we
generate an improved policy . This new policy is designed to be greedy with re-
spect to 
for all i.
For the approximate policy-iteration algorithm to yield satisfactory results, it is
important to carefully choose the policy used to initialize the algorithm. This can be
done through the use of heuristics.Alternatively, we may start with some weight vector
w and use it to derive a greedy policy, which is, in turn, used as the initial policy.
Suppose,then,in addition to the known transition probabilities and observed costs,
we have the following items:
• a stationary policy  as the initial policy;
• a set of states 
representative of the operating environment;
• a set of M(i) samples of the cost-to-go function J(i) for each state 
; one such
sample is denoted by k(i, m), where m  1, 2, ..., M(i).
The weight vector w of the neural network is determined by using the method of least
squares—that is, by minimizing the cost function
(12.75)
Having determined the optimal value of weight vector w, and therefore the approxi-
mate cost-to-go function 
, we next determine the approximate Q-factors.To this
end, we use Eqs. (12.20) and (12.23) to approximate the Q-factor as
(12.76)
where the pij(a) is the transition probability from state i to state j under action a (known),
g(i, a, j) is the observed cost (also known), and  is a prescribed discount factor. The
iteration is completed by using the approximate Q-factor to determine an improved
policy based on the following formula (see Eq. (12.28)):
(12.77)
It is important to note that Eqs. (12.76) and (12.77) are used by the simulator to generate
actions only at the states that are actually visited by the simulation, rather than for all
states.As such, these two equations need not suffer from the curse of dimensionality.
The block diagram of Fig. 12.12 presents a more detailed picture of the approxi-
mate policy-iteration algorithm.This diagram consists of four interconnected functional
modules (Bertsekas and Tsitsiklis, 1996):
1. Simulator, which uses the given state-transition probabilities and observed one-step
costs to construct a surrogate model of the environment.The simulator generates
two things: (a) states that mimic the environment’s response to actions, and (b)
samples of the cost-to-go function for a given policy .
(i) = arg min
aai
Q(i, a, w)
Q(i, a, w) = a
jx
pij(a)(g(i, a, j) + J(j, w))
J~(i, w)
e(w) = a
ix a
M(i)
m=1
(k(i, m) - J~(i, w))2
i  x
x
J~(i, w)
J~(i, w)
Section 12.11
Approximate Policy Iteration
661

2. Action generator, which generates an improved policy (i.e., sequence of actions) in
accordance with Eq. (12.77).
3. Cost-to-go approximator, which generates the approximate cost-to-go function
for state i and parameter vector w, for use in Eqs. (12.76) and (12.77).
4. Least-squares solver, which takes samples of the cost-to-go function J(i) supplied
by the simulator for policy  and state i, and then computes the optimum para-
meter vector w that minimizes the cost function of Eq. (12.75).The link from the
least-squares solver to the cost-to-go approximator is switched on only after a pol-
icy has been fully evaluated and an optimal weight vector w* has been determined.
At that point, the cost-to-go approximation 
is replaced by 
.
Table 12.3 presents a summary of the approximate policy-iteration algorithm.
J~(i, w*)
J~(i, w)
J~(i, w)
662
Chapter 12
Dynamic Programming
Simulator
Actions
States
State
j
w
w*
Action
generator
Cost-to-go
approximator
Least-squares
solver
Approximate
cost-to-go
J(j, w)
~
FIGURE 12.12
Block diagram of the approximate policy-
iteration algorithm.
TABLE 12.3
Summary of the Approximate Policy-Iteration Algorithm
Known parameters: transition probabilities pij(a) and costs g(i, a, j).
Computation:
1. Choose a stationary policy  as the initial policy.
2. Using a set of samples {k(i, m)}m  1
M(i)
of the cost-to-go function J(i) generated by the simulator, determine the
parameter vector w of the neural network employed as the least-squares solver:
3. For the optimized vector w* determined in step 2, compute the approximate cost-to-go function 
for
the states visited. Determine the approximate Q-factors:
4. Determine the improved policy
5. Repeat steps 2 through 4.
(i) = arg min
aai
 Q(i, a, w*)
Q(i, a, w*) = a
j x
pij(a)(g(i, a, j) + J~(j, w*))
J~(i, w*)
 = min
w  a
ix
  a
M(i)
m=1
(k(i, m) - J~(i, w))2
 w* = min
w  e(w)
Note: Steps 3 and 4 apply only to actions at the states that are actually visited, rather than all states.

Naturally, the operation of this algorithm is subject to errors due to unavoidable
imperfections in the design of the simulator and least-squares solver.The neural network
used to perform the least-squares approximation of the desired cost-to-go function may
lack adequate computing power—hence the first source of error. Optimization of the
neural network approximator, and therefore tuning of the weight vector w, is based on
a desired response provided by the simulator—hence the second source of error.Assum-
ing that all policy evaluations and all policy improvements are performed within certain
error tolerances of  and ,respectively,it is shown in Bertsekas and Tsitsiklis (1996) that
the approximate policy-iteration algorithm will produce policies whose performances dif-
fer from the optimal policies by a factor that decreases to zero as  and  are reduced.
In other words, the approximate policy-iteration algorithm is sound with minimal per-
formance guarantees. According to Bertsekas and Tsitsiklis (1996), the approximate
policy-iteration algorithm tends initially to make rapid and fairly monotonic progress,
but a sustained policy oscillation of a random nature may result as a limiting condition.
This oscillatory behavior occurs after the approximating cost-to-go function 
gets
within a zone of O(  2)/(1 - )2) of the optimal value J*, where  is the discount
factor.Apparently, there is a fundamental structure common to all variants of approxi-
mate policy iteration that causes an oscillatory behavior.
12.12
SUMMARY AND DISCUSSION
The early part of this chapter presented a detailed discussion of Bellman’s theory of dynamic
programming for multistage decision making. Building on Markov decision processes for
stationary policies,the theory rests its case on the availability of an explicit model of the envi-
ronment that embodies transition probabilities and associated costs.We also discussed the
two methods of policy iteration and value iteration for solving the Bellman equation of
optimality.
Approximate Dynamic Programming: Direct Methods
Dynamic programming lies at the core of reinforcement learning.This statement was con-
firmed in this chapter by using dynamic programming to derive two model-free, on-line
learning algorithms widely known in the reinforcement learning literature:
• Temporal difference (TD) learning, due to Sutton (1988).
• Q-learning, due to Watkins (1989).
Being model free, both algorithms bypass the need for transition probabilities. How-
ever, memory limitations restrict their practical use to decision-making problems in
which the state space is of a moderate size.
Approximate Dynamic Programming: Indirect Methods
In the latter part of the chapter, we addressed an issue of practical importance: the so-
called curse of dimensionality. This issue, encountered in solving large-scale decision-
making problems,renders Bellman’s dynamic programming intractable.To overcome the
difficulty, we may resort to indirect approximate dynamic programming, which builds on
J~
Section 12.12
Summary and Discussion
663

Bellman’s theory. The implementation of indirect approximate dynamic programming
may proceed in one of two ways:
1. The linear structural approach, which involves two steps:
• feature extraction of the state i;
• least-squares minimization of the cost 
(i, w), where w is the weight vector
associated with state i.
We illustrated the applicability of this approach by deriving the least-squares pol-
icy evaluation (LSPE) algorithm.
2. The nonlinear structural approach, the development of which relies on the use of
a universal approximator that can approximate any nonlinear function to any de-
sired degree of accuracy. Neural networks have established themselves as univer-
sal approximators.
Despite the significant progress we have made in approximate dynamic programming,9
much remains to be done in terms of building systems that are capable of making high-
level decisions for large-scale applications, reliably and in a computationally tractable
manner. In this context, the issue of partial observability stands out as perhaps the most
challenging of all practical problems that afflict dynamic programming.
Partial Observability
Bellman’s dynamic programming theory assumes a fully observable system. To be more
precise, in order to solve the dynamic programming problem for an optimal policy, it is
assumed that the states of the environment obey the Markov property:The state at time
n  1 depends only on the state and policy at time n and is therefore independent of
everything that may have occurred prior to time n.This stringent assumption is frequently
violated in practice due to the unavoidable presence of unobservable states.Accordingly,
instead of a model based on Markov decision processes (MDPs) that is the cornerstone
of Bellman’s dynamic programming theory, we have to deal with partially observable
Markov decision processes (POMDPs) if we are to bring the theory of approximate
dynamic programming closer to practical reality. In a sense, partial observability may be
viewed as the second “curse”of dynamic programming,namely,the curse of modeling, in
that the observables contain incomplete information about the underlying dynamics of
the environment. We may therefore describe dynamic programming as a global opti-
mization methodology that suffers from the twin curses of modeling and dimensionality.
The POMDP problem has been recognized as a serious problem in the literature
for many years, posing a major obstacle to progress in applications that involve planning
under uncertainty (e.g., robotics).The problem is difficult because of the need to learn
strategies of action selection that would be contingent under all possible types of uncer-
tainty. The list of references presented in Note 10 under the Notes and References is
intended to provide a sense of research directions on how the POMDP problem is
being addressed in the literature.
Relationship between Dynamic Programming and Viterbi Algorithm
Dynamic programming has featured prominently throughout this chapter.As such,a study
of dynamic programming would be incomplete without discussion of its relationship to the
J
664
Chapter 12
Dynamic Programming

Viterbi algorithm, so named after its originator Viterbi (1968). In actual fact, Bellman’s
dynamic programming (Bellman,1957;Bellman and Dreyfus,1962) preceded Viterbi’s paper
by many years.The equivalence between these two algorithms was found by Omura (1969).
In the context of optimization,dynamic programming seeks to find the shortest path
across a weighted graph (e.g., the graph depicted in Fig. 12.5 for the stagecoach problem)
by starting from the destination and working backward to the starting point in a method-
ical stage-by-stage manner. In the context of convolutional decoding, on the other hand,
the Viterbi algorithm works on a weighted graph of its own,called the trellis diagram.This
diagram represents a graphical description of the convolutional encoder,viewed as a finite
state machine (Lin and Costello, 2004). Optimality of the Viterbi algorithm for convolu-
tional decoding in the maximum likelihood sense was recognized by Forney (1973).
NOTES AND REFERENCES
1. The classical approach to reinforcement learning is rooted in psychology, going back to the
early work of Thorndike (1911) on animal learning and that of Pavlov (1927) on condi-
tioning. Contributions to classical reinforcement learning also include the work of Widrow
et al. (1973); in that paper, the notion of a critic was introduced. Classical reinforcement
learning is discussed in book form in Hampson (1990).
Major contributions to reinforcement learning include the works of Samuel (1959) on his
celebrated checkers playing program,Barto et al.(1983) on adaptive critic systems,Sutton (1988)
on temporal difference methods, and Watkins (1989) on Q-learning. Reinforcement learning
is treated in great detail in the book by Sutton and Barto (1998).
In a neurobiological context,reward signals are processed by midbrain neurons known
as dopamine neurons. To elaborate, in a series of experiments reported in Schultz (1998),
instrumental conditioning was used to train a monkey to respond to stimuli (e.g., light and
sound). To get a reward, in the form of food or drink, the monkey had to release a resting
key followed by pressing another key. The resulting activities of dopamine neurons were
averaged over a total of 20 trials of each experiment. The results obtained by Schultz
revealed that the dopamine neurons did fire after the stimulus presentation and reward
delivery. Given Schultz’s remarkable findings, how do we model them? Well, viewing the
dopamine neurons as “a retina of the rewards system,” we may consider the responses pro-
duced by dopamine neurons as teaching signals for Pavlovian conditioning and TD-learning
(Schultz,2007;Iszhikevich,2007b);it should however be noted that the relevant form of TD-
learning is TD( ) and not TD(0), both of which are discussed in Section 12.7.
One last comment is in order: When considering TD-learning in the reinforcement-
learning literature, rewards are maximized. In contrast, when the same algorithm is con-
sidered in dynamic programming, cost-to-go functions are minimized.
2. In this book, we have discussed dynamic programming in the general context of stochastic
environments. It may therefore be tempting to retitle the chapter as “Stochastic Dynamic
Programming.” However, we have resisted this temptation, because “Dynamic Program-
ming” describes the field adequately for researchers working in it.
3. Policy iteration and value iteration are two principal methods of dynamic program-
ming. There are two other dynamic programming methods that deserve to be men-
tioned: the Gauss–Seidel method and asynchronous dynamic programming (Barto et al.,
1995; Bertsekas, 1995). In the Gauss-Seidel method, the cost-to-go function is updated at
one state at a time in a sequential sweep of all the states, with the competition for each state
based on the most recent costs of the other states. Asynchronous dynamic programming

Notes and References
665

differs from the Gauss–Seidel method in that it is not organized in terms of systematic
successive sweeps of the set of states.
4. On page 96 of his Ph.D. thesis,Watkins (1989) makes the following remarks on Q-learning:
“Appendix 1 presents a proof that this learning method does work for finite Markov
decision processes. The proof also shows that the learning method will converge
rapidly to the optimal action-value function.Although this is a very simple idea, it has
not, as far as I know, been suggested previously. However, it must be said that finite
Markov decision processes and stochastic dynamic programming have been exten-
sively studied for use in several different fields for over thirty years, and it is unlikely
that nobody has considered the Monte-Carlo method before.”
In a footnote commentary on these remarks, Barto et al. (1995) point out that although the
idea of assigning values to state–action pairs formed the basis of the approach to dynamic
programming taken in Denardo (1967), they have not seen algorithms like Q-learning for
estimating these values that predate Watkins’s 1989 thesis.
5. The outline of a proof of the convergence theorem for Q-learning was presented in
Watkins (1989); it was refined later in Watkins and Dayan (1992). More general results on
the convergence of Q-learning were presented in Tsitsiklis (1994); see also Bertsekas and
Tsitsiklis (1996).
6. The early development of approximate dynamic programming may be traced to Werbos’s
1977 paper, in which the idea of heuristic dynamic programming to bypass the curse of
dimensionality was described for the first time. The idea of heuristic dynamic
programming was proposed therein as a simple way of approximating the iterative
procedure due to Howard (1960) through the supervised training of a network with
adjustable weights.
Nowadays,“approximate dynamic programming” is commonly used to refer to meth-
ods that use approximation to overcome limitations of Bellman’s dynamic programming.
Volume 2 of the book by Bertsekas (2007) has a chapter on approximate dynamic pro-
gramming that identifies direct and indirect methods of approximation.
7. Least-Squares Temporal-Difference (LSTD) Algorithm
The LSTD algorithm, due to Bradtke and Barto (1996), provides another method for a lin-
ear architectural approach to the indirect approximation of dynamic programming. Devel-
opment of the LSTD algorithm proceeds as follows:
• With a basis function used for the representation of each state, the Bellman equation is
first approximated in such a way that the input and output observations appear as noisy
variables.
• Then, clever use is made of the method of instrumental variables discussed in Chapter 2,
so as to avoid the asymptotic bias introduced by the “errors-in-variables” problem; the
stage is thereby set for the application of the method of least squares.
• With a procedure similar to that used for the recursive least-squares (RLS) algo-
rithm in Chapter 5, a similar recursive implementation of the LSTD algorithm is
derived.
The original version of the LSTD algorithm was derived for 
  0. Building on the work of
Bradtke and Barto, Boyan (2002) extended the LSTD algorithm to 
 > 0.The LSTD algo-
rithm has also been discussed by Lagoudakis and Parr (2003) in the context of approxi-
mate policy iteration.
The relationship between LSTD and LSPE algorithms is discussed in Bertsekas
(2007).
666
Chapter 12
Dynamic Programming

8. Feedback in the Visual Cortex
The primary Visual Cortex—visual area 1, commonly abbreviated as V1—has distinct
anatomical layers, each having characteristic functions of its own. V1 is adjacent to and
interconnects with higher order visual areas that are concerned with more detailed analy-
ses of sensation (Kandel et al., 1991).
The lateral geniculate nucleus (LGN) is the part of the brain where visual informa-
tion is processed (Kandel et al., 1991).
9. Books on Approximate Dynamic Programming
The classic book Neuro-dynamic Programming by Bertsekas and Tsitsiklis (1996) was the
first book devoted to approximate dynamic programming. The edited volume by Si et al.
(2004) presents a broad coverage of topics under the umbrella of learning and approximate
dynamic programming (ADP), technical advances in ADP, and their applications.
10. Partial Observability
The problem of having to plan in a partially observable environment is notoriously difficult.
The following short list of references is intended to provide some sense of interesting direc-
tions of research in this highly challenging field:
(i)
Hierarchical Approach
Planning in a partially observable environment may be sim-
plified by decomposing a difficult task into a hierarchy of simple planning problems, a
technique that may be viewed as an application of the well-known engineering “divide
and conquer” paradigm. Charlin et al. (2007) investigate the problem of automatically
discovering the hierarchy by framing the optimization of a hierarchical policy as a non-
convex optimization problem that can be tackled with general nonlinear solvers.
In Guestrin and Gordon (2002), another approach to the hierarchical decomposi-
tion of POMDPs is described for collaborative multiagent dynamic systems.During both
the planning and execution phases,the computation is distributed among the agents,with
each agent needing to model and plan only a small part of the system.The subsystems are
connected together through a hierarchy that takes care of coordination and communica-
tion among the agents via a message-passing algorithm;a globally consistent plan is thereby
achieved.Another message-passing algorithm allows for execution of the resulting policy.
(ii) POMDP value iteration
An optimal policy for a POMDP can be represented via its
cost-to-go function, denoted by J(b).This function maps a belief state b (representing
a posterior distribution over the possibly true, but unobserved, configurations of the
world) to an estimate of the total return that an optimal policy can achieve, given that
b is the correct belief state.Although it is not possible to compute the cost-to-go func-
tion exactly (Sondik, 1971), many authors have proposed algorithms to approximate it.
In particular, the so-called point-based algorithms have shown substantial promise
(Smith, 2007). These algorithms estimate both the value and the gradient of J(b) at a
discrete sample of beliefs and generalize to arbitrary beliefs by using the convexity of
J(b).The belief samples are collected either by simulating the POMDP to get a tree of
reachable beliefs or by filling up the simplex of possible beliefs with the use of samples
chosen randomly or placed in a grid.
(iii) Belief Compression
In real POMDP problems, most “belief” states are unlikely. Most
importantly, there is a structured low-dimensional manifold of plausible beliefs embed-
ded in a high-dimensional belief space.Roy and Gordon (2003) introduce a new method,
termed “belief compression,”for solving large-scale POMDP problems by taking advan-
tage of the sparsity of the belief space. In particular, the dimensionality of the belief
space is reduced by using exponential family principal-components analysis (Collins
et al., 2002). (Differentiable manifolds were discussed in Chapter 10.)
Notes and References
667

(iv) Natural Policy Gradient
In direct policy-gradient methods for approximate planning
in large- scale MDPs, the motivation is to find a good policy  among a restricted class
of policies by following the gradient of future returns. Kakade (2002) describes a nat-
ural gradient method that represents the direction of steepest descent, based on the
underlying structure of the parameter space.A connection to policy iteration is estab-
lished by showing that the natural gradient moves toward choosing a greedy policy
action. (Amari’s natural gradient was discussed in Chapter 10.)
PROBLEMS
Bellman’s optimality criterion
12.1 When the discount factor  approaches 1, computation of the cost-to-go function in Eq.
(12.22) becomes longer.Why? Justify your answer.
12.2 In this problem, we present another proof of Bellman’s optimality equation of Eq. (12.22),
due to Ross (1983).
(a) Let  be any arbitrary policy, and suppose that  chooses action a at time-step 0 with
probability pa and 
.Then, we have
where W(j) represents the expected cost-to-go function from time-step 1 onward,given
that policy  is being used and that j is the state at time-step 1. Hence, show that
where
(b) Let  be the policy that chooses action a0 at time-step 0. If the next state is j, it views
the process as originating in state j, following a policy j such that
where  is a small positive number. Hence, show that
(c) Using the results derived in parts (a) and (b), prove Eq. (12.22).
12.3 Equation (12.22) represents a linear system of N equations, with one equation per state. Let
 P() = ≥
p11()
p12()
p
p1N()
p21()
p22()
p
p2N()
o
o
o
o
pN1()
pN2()
p
pNN()
¥
 c() = [c(1, ), c(2, ), ..., c(N, )]T
 J = [J(1), J(2), ..., J(N)]T
J(i)  min
aai
 ac(i, a) +  a
N
j=1
pij(a)J(j) b + 
Jj(j)  J(j) + 
W(j)  J(j)
J(i)  min
aai 
ac(i, a) +  a
N
j=1
pij(a)J(j) b
J(i) = a
aai
   pa a c(i, a) + a
N
j=1
pij(a)W(j) b
a  ai
668
Chapter 12
Dynamic Programming

Show that Eq. (12.22) may be reformulated in the equivalent matrix form
where I is the identity matrix. Comment on the uniqueness of the vector Jµ, representing the
cost-to-go functions for the N states.
12.4 In Section 12.3, we derived the dynamic-programming algorithm for a finite-horizon prob-
lem. In this problem, we rederive this algorithm for a discounted problem for which the
cost-to-go function is defined by
In particular, show that
Policy iteration
12.5 In Section 12.4, we said that the cost-to-go function satisfies the statement
Justify this assertion.
12.6 Discuss the significance of the statement embodied in Eq. (12.25).
12.7 Using a controller-critic system, illustrate the interaction between the policy update and
policy evaluation in the policy-iteration algorithm.
Value iteration
12.8 A dynamic-programming problem involves a total of N possible states and M admissible
actions.Assuming the use of a stationary policy, show that a single iteration of the value iter-
ation algorithm requires on the order of N 2M operations.
12.9 Table 12.2 presents a summary of the value iteration algorithm formulated in terms of the
cost-to-go function Jµ(i) for states 
.Reformulate this algorithm in terms of the Q-factors
Q(i, a).
12.10 Policy iteration always terminates after a finite number of steps, whereas value iteration
may require an infinite number of iterations. Discuss other differences between these two
methods of dynamic programming.
Temporal-difference learning
12.11 (a) Construct a signal-flow graph representation of the TD(0) algorithm described in
Eqs. (12.34) and (12.35).
(b) The TD(0) algorithm has a mathematical composition similar to that of the LMS algo-
rithm described in Chapter 3. Discuss the similarities and differences between these
two algorithms.
12.12 Justify the statement that the sample mean of Eq. (12.40) may be calculated from the iter-
ative formula of Eq. (12.42).
12.13 (a) Prove the equalities 1 and 2 that were used to go from Eq. (12.45) to Eq. (12.46).
(b) Construct a signal-flow graph representation of Eq. (12.48), describing the TD(
)
algorithm.
i  x
Jn + 1(i)  Jn(i) for all i
JK(X0) = min
  E
X1
 [g(X0, (X0), X1) + JK-1(X1)]
J(X0) = lim
KS q c a
K-1
n=0
ng(Xn, (Xn), Xn+1)d
(I - P())J = c()
Problems
669

Q-learning
12.14 Show that
12.15 The Q-learning algorithm is sometimes referred to as an adaptive form of the value itera-
tion policy. Justify the validity of this description.
12.16 Construct a signal-flow graph for the approximate Q-learning algorithm summarized in
Table P12.16.
12.17 The approximate Q-learning algorithm summarized in Table P12.16 assumes lack of knowl-
edge of the state-transition probabilities. Reformulate this algorithm assuming the avail-
ability of these probabilities.
Approximate Dynamic Programming: Indirect Methods
12.18 Equation (12.70) is the least-squares version of the projected value iteration (PVI) algo-
rithm. To implement this algorithm in practical terms, we proposed to apply Monte Carlo
simulation to approximate it with the use of the least-squares policy evaluation (LSPE)
algorithm described in Eq. (12.71).
(a) By setting the gradient of the cost function in Eq. (12.70) to zero, derive a closed for-
mula for wn+1.
(b) Do likewise for Eq. (12.71). Find empirical frequencies of state i and transition (i, j)
(i.e., estimates of the steady-state probability i and transition probability pij) to demon-
strate that the PVI and LSPE algorithms coincide asymptotically.
J*(i) = min
aai
Q(i, a)
670
Chapter 12
Dynamic Programming
TABLE P12.16
Summary of the Approximate Q-Learning Algorithm
1. Start with an initial weight vector w0, resulting in the Q-factor Q(i0, a0, w0); the weight vector w0
refers to a neural network used to perform the approximation.
2. For iteration n  1, 2, ..., do the following:
(a)
For the setting w of the neural network, determine the optimal action
(b)
Determine the target Q-factor
(c)
Update the Q-factor
where
(d)
Apply (in, an) as input to the neural network producing the output 
as an approxi-
mation to the target Q-factor Qn
target(in, an, w). Change the weight vector w slightly in a way that
brings 
closer to the target value Qn
target(in, an, w).
(e)
Go back to step (a) and repeat the computation.
Qˆ
n(in, an, w)
Qˆ
n(in, an, w)
¢Qn(i, a, w) = en(in, an)(Qtarget
n
(in, an, w) - Qn(in, an, w)), (i, a) = (in, an)
0,
   
Qn+1(in, an, w) = Qn(in, an, w) + ¢Qn(in, an, w)
Qtarget
n
(in, an, w) = g(in, an, jn) +  min
bajn
Qn(jn, b, w), jn = in+1
an = min
aajn
Qn(in, a, w)
otherwise

12.19 The LSPE(
) algorithm has a faster rate of convergence than the TD(
) algorithm. Justify
this statement.
12.20 Figure P12.20 depicts a neural-network-based scheme for approximating the target Q-factor,
denoted by Qtarget (i, a, w), where i denotes the state of the network, a denotes the action to
be taken,and w denotes the weight vector of the neural network used in the approximation.
Correspondingly,Table P12.16 presents a summary of the approximate Q-learning algorithm.
Explain the operation of the approximate dynamic programming scheme of Fig. P12.20 to
justify the summary presented in Table P12.16.
Problems
671
Qtarget(i, a, w)
State
i
Action
a
Q^(i, a, w)


Neural
network:
w
Error signal
Σ
FIGURE P12.20

ORGANIZATION OF THE CHAPTER
This chapter studies recurrent neural networks, with particular emphasis on the direct
method of Laypunov for solving the stability problem.
The material in the chapter is organized as follows:
1. The introductory Section 13.1 motivates the study of stability in deterministic neu-
rodynamic systems, pointing out some historical aspects of the problem.
2. Sections 13.2 through 13.6 provide background material. In particular, Section 13.2
introduces some fundamental concepts in dynamic systems, followed by a discussion
of the stability of equilibrium points in Section 13.3. In Section 13.4, we describe var-
ious types of attractors that arise in the study of dynamic systems. In Section 13.5, we
revisit the additive model of a neuron. Section 13.6 discusses the manipulations of
attractors as a neural network paradigm.
3. The next part of the chapter, consisting of Sections 13.7 through 13.9, deals with asso-
ciative memories.Section 13.7 is devoted to the Hopfield model and use of its discrete
form as a content-addressable memory. Section 13.8 presents the Cohen–Grossberg
theorem for nonlinear dynamic systems, including the Hopfield network and other
associative memories as special cases. In Section 13.9, we describe another neuro-
dynamic model known as the brain-state-in-a-box model, which is well suited for
clustering.
4. The last part of the chapter, consisting of Sections 13.10 and 13.11, deals with the
topic of chaos.Section 13.10 discusses the invariant characteristics of a chaotic process,
followed by a discussion of the closely related topic of dynamic reconstruction of a
chaotic process in Section 13.11.
The chapter concludes with some final remarks in Section 13.12.
13.1
INTRODUCTION
In one form or another, time plays a critical role in learning, a concept that is exempli-
fied by much of the material presented in preceding chapters of this book. Basically,
there are two ways in which time manifests itself in the learning process:
672
C H A P T E R  1 3
Neurodynamics

Section 13.1
Introduction
673
1. A static neural network (e.g.,multilayer perceptron,studied in Chapter 4) is made into
a dynamic mapper by stimulating it via a memory structure, short term or long term.
2. Time is built into the operation of a neural network through the use of feedback.
In the context of a neural network, there are two basic ways of applying feedback:
1. local feedback, which is applied to a single neuron inside the network;
2. global feedback, which encompasses one or more layers of hidden neurons—or
better still, the whole network.
Local feedback is a relatively simple matter to deal with, but global feedback has much
more profound implications. In the neural network literature, neural networks with one
or more global feedback loops are referred to as recurrent networks.
Basically, there are two functional uses of recurrent networks:
1. associative memories;
2. input–output mapping networks.
The use of recurrent networks as associative memories is considered in this chapter,
and their use as mappers is deferred to Chapter 15. Whichever one of these two is the
application of interest, an issue of particular concern is that of stability, which is con-
sidered in this chapter.
Feedback is like a double-edged sword in that when it is applied improperly, it can
produce harmful effects.In particular,the application of feedback can cause a system that
is originally stable to become unstable. Our primary interest in this chapter is in the
stability of recurrent networks.
The subject of neural networks viewed as nonlinear dynamic systems,with particular
emphasis on the stability problem,is referred to as neurodynamics.An important feature of
the stability (or instability) of a nonlinear dynamic system is that it is a property of the whole
system.As a corollary we may make the following statement:
The presence of stability always implies some form of coordination between the individual
parts of the system.
It appears that the study of neurodynamics began in 1938 with the work of Nicholas
Rashevsky, in whose visionary mind the application of dynamics to biology came into
view for the first time.
The stability of a nonlinear dynamic system is a difficult mathematical issue to deal
with.When we speak of the stability problem, those with an engineering background usu-
ally think in terms of the bounded-input–bounded-output (BIBO) stability criterion.Accord-
ing to this criterion, stability means that the output of a system must not grow without
bound as a result of a bounded input,initial condition,or unwanted disturbance.The BIBO
stability criterion is well suited for a linear dynamic system. However, it is useless to apply
it to recurrent neural networks;all such nonlinear dynamic systems are BIBO stable because
of the saturating nonlinearity built into the constitution of a neuron.
When we speak of stability in the context of a nonlinear dynamic system, we usu-
ally mean stability in the sense of Lyapunov. In a celebrated mémoire dated 1892,
Lyapunov, a Russian mathematician and engineer, presented the fundamental concepts

of stability theory known as the direct method of Lyapunov. This method is widely used
for the stability analysis of linear and nonlinear systems, both time invariant and time
varying. As such, it is directly applicable to the stability analysis of neural networks.
Indeed, much of the material presented in this chapter is concerned with the direct
method of Lyapunov. However, its application is no easy task.
The study of neurodynamics may follow one of two routes,depending on the appli-
cation of interest:
• Deterministic neurodynamics, in which the neural network model has a determin-
istic behavior.In mathematical terms,it is described by a set of nonlinear differential
equations that define the exact evolution of the model as a function of time
(Grossberg, 1967; Cohen and Grossberg, 1983; Hopfield, 1984).
• Statistical neurodynamics, in which the neural network model is perturbed by the
presence of noise. In this case, we have to deal with stochastic nonlinear differen-
tial equations, thereby expressing the solution in probabilistic terms (Amari et al.,
1972; Peretto, 1984; Amari, 1990). The combination of stochasticity and nonlin-
earity makes the subject more difficult to handle.
In this chapter, we restrict ourselves to deterministic neurodynamics.
13.2
DYNAMIC SYSTEMS
In order to proceed with the study of neurodynamics, we need a mathematical model
for describing the dynamics of a nonlinear system.A model most naturally suited for
this purpose is the state-space model. According to this model, we think in terms of a
set of state variables whose values (at any particular instant of time) are supposed to
contain sufficient information to predict the future evolution of the system. Let x1(t),
x2(t), ..., xN(t) denote the state variables of a nonlinear dynamic system, where
continuous time t is the independent variable and N is the order of the system. For
convenience of notation, these state variables are collected into an N-by-1 vector x(t)
called the state vector, or simply state, of the system. The dynamics of a large class of
nonlinear dynamic systems may then be cast in the form of a system of first-order dif-
ferential equations written as
(13.1)
where the function Fj(·) is,in general,a nonlinear function of its argument.We may express
this system of equations in a compact form by using vector notation, as shown by
(13.2)
where the nonlinear function F is vector valued, each element of which operates on a
corresponding element of the state vector:
(13.3)
x(t) = [x1(t), x2(t), ..., xN(t)]T
d
dt
 x(t) = F(x(t))
d
dt xj(t) = Fj(xj(t)),  j = 1, 2, ..., N
674
Chapter 13
Neurodynamics

Section 13.2
Dynamic Systems
675
A nonlinear dynamic system for which the vector function F(x(t)) does not depend
explicitly on time t, as in Eq. (13.2), is said to be autonomous; otherwise, it is nonau-
tonomous.1 We will concern ourselves with autonomous systems only.
Regardless of the exact form of the nonlinear function F(·), the state x(t) must
vary with time t; otherwise, x(t) is constant, and the system is no longer dynamic. We
may therefore formally define a dynamic system as follows:
A dynamic system is a system whose state varies with time.
Moreover, we may think of dx/dt as a “velocity” vector—not in a physical sense, but
rather in an abstract one.Then, according to Eq. (13.2), we may refer to the vector func-
tion F(x) as a velocity vector field, or simply a vector field.
State Space
It is informative to view the state-space equation of Eq. (13.2) as describing the motion
of a point in an N-dimensional state space. The state space can be a Euclidean space or
a subset thereof. It can also be a non-Euclidean space such as a circle, a sphere, a torus,
or some other differentiable manifold. Our interest in this chapter is confined to Euclidean
spaces. (Differential manifolds were discussed in Chapter 7.)
The state space is important because it provides us with a visual and conceptual
tool for analyzing the dynamics of a nonlinear system described by Eq. (13.2). It does
so by focusing attention on the global characteristics of the motion rather than the
detailed aspects of analytic or numeric solutions of the equation.
At a particular instant of time t, the observed state of the system (i.e., the state
vector x(t)) is represented by a single point in the N-dimensional state space. Changes
in the state of the system with time t are represented as a curve in the state space, with
each point on the curve carrying (explicitly or implicitly) a label that records the time
of observation. This curve is called a trajectory or orbit of the system. Figure 13.1
FIGURE 13.1
A two-dimensional
trajectory (orbit) of a dynamic system.
t0
t1
t2
t3
t4
t5
0
x1
x2

illustrates the trajectory of a two-dimensional system. The instantaneous velocity of
the trajectory (i.e., the velocity vector dx(t)/dt) is represented by the tangent vector,
shown as a red line in Fig. 13.1 for time t  t0. We may thus derive a velocity vector
for each point of the trajectory.
The family of trajectories for different initial conditions is referred to as the state
portrait of the system.The state portrait includes all those points in the state space where
the vector field F(x) is defined. Note that for an autonomous system, there will be only
one trajectory passing through an initial state.A useful idea that emerges from the state
portrait is the flow of a dynamic system, defined as the motion of the space of states
within itself. In other words, we may imagine the space of states to flow, just like a fluid,
around in itself, with each point (state) following a particular trajectory.The idea of flow
as described here is vividly illustrated in the state portrait in Fig. 13.2.
Given a state portrait of a dynamic system,we may construct a field of velocity (tan-
gent) vectors, one for every point of the state space.The picture so obtained in turn pro-
vides a portrayal of the vector field of the system.Figure 13.3 shows a number of velocity
vectors to help us develop a feeling for what a full field looks like. The usefulness of a
vector field thus lies in the fact that it gives us a visual description of the inherent ten-
dency of a dynamic system to move with a habitual velocity at each specific point of a
state space.
Lipschitz Condition
For the state-space equation of Eq. (13.2) to have a solution and for that solution to be
unique, we must impose certain restrictions on the vector function F(x). For conve-
nience of presentation, we have dropped dependence of the state x on time t, a prac-
tice that we follow from time to time. For a solution to exist, it is sufficient that F(x) be
676
Chapter 13
Neurodynamics
0
x1
x2
FIGURE 13.2
A two-dimensional state
(phase) portrait of a dynamic system.

Section 13.2
Dynamic Systems
677
continuous in all of its arguments. However, this restriction by itself does not guaran-
tee uniqueness of the solution. To do so, we must impose a further restriction known
as the Lipschitz condition. Let 
denote the norm, or Euclidean length, of the vector
x. Let x and u be a pair of vectors in an open set 
in a normal vector (state) space.Then,
according to the Lipschitz condition, there exists a constant K such that
(13.4)
for all x and u in 
(Hirsch and Smale, 1974; Jackson, 1989).A vector function F(x) that
satisfies Eq. (13.4) is said to be Lipschitz, and K is called the Lipschitz constant for F(x).
Equation (13.4) also implies the continuity of the function F(x) with respect to x.It follows,
therefore,that in the case of autonomous systems,the Lipschitz condition guarantees both
the existence and uniqueness of solutions for the state-space equation of Eq. (13.2). In
particular,if all partial derivatives 
are finite everywhere,then the function F(x) sat-
isfies the Lipschitz condition.
Divergence Theorem
Consider a region of volume V and surface S in the state space of an autonomous sys-
tem, and assume a “flow” of points from this region. From our earlier discussion, we
recognize that the velocity vector dx/dt is equal to the vector field F(x). Provided that
the vector field F(x) within the volume V is “well behaved,”we may apply the divergence
theorem from vector calculus (Jackson, 1975). Let n denote a unit vector normal to the
elemental surface dS pointing outward from the enclosed volume.Then,according to the
divergence theorem, the relation
(13.5)
3S
(F(x)  n)dS =
3V
(  F(x))dV
0Fi0xj
m
7F(x) - F(u)7  K7x - u7
m
7x7
0
x1
x2
FIGURE 13.3
A two-dimensional vector
field of a dynamic system.

holds between the volume integral of the divergence of F(x) and the surface integral of
the outwardly directed normal component of F(x).The quantity on the left-hand side of
Eq. (13.5) is recognized as the net flux flowing out of the region surrounded by the
closed surface S. If this quantity is zero, the system is conservative; if it is negative, the
system is dissipative. In light of Eq. (13.5), we may formally make the statement:
If the divergence 
(which is a scalar) is zero, the system is conservative, and if it is neg-
ative, the system is dissipative.
13.3
STABILITY OF EQUILIBRIUM STATES
Consider an autonomous dynamic system described by the state-space equation of
Eq. (13.2).A constant vector 
is said to be an equilibrium (stationary) state of the
system if the condition
(13.6)
is satisfied, where 0 is the null vector.The velocity vector dx/dt vanishes at the equilib-
rium state , and therefore the constant function 
is a solution of Eq. (13.2).
Furthermore, because of the uniqueness property of solutions, no other solution curve
can pass through the equilibrium state . The equilibrium state is also referred to as a
singular point, signifying the fact that in the case of an equilibrium point, the trajectory
will degenerate into the point itself.
In order to develop a deeper understanding of the equilibrium condition, suppose
that the nonlinear function F(x) is smooth enough for the state-space equation of
Eq.(13.2) to be linearized in the neighborhood of the equilibrium state .Specifically,let
(13.7)
where ∆x(t) is a small deviation from .Then, retaining the first two terms in the Taylor
series expansion of F(x), we may approximate it as follows:
(13.8)
The matrix A is the Jacobian of the nonlinear function F(x), evaluated at the point 
,
as shown by
(13.9)
Using Eqs. (13.7) and (13.8) in Eq. (13.2), followed by the definition of an equilibrium
state, we get
(13.10)
Provided that the Jacobian A is nonsingular—that is, the inverse matrix A1 exists—
the approximation described in Eq. (13.10) is sufficient to determine the local behavior
of the trajectories of the system in the neighborhood of the equilibrium state . If A is
nonsingular, the nature of the equilibrium state is essentially determined by its
x
d
dt ¢x(t) L A ¢x(t)
A = 0
0x F(x)x=x–
x = x
F(x) L x + A ¢x(t)
x
x(t) = x + ¢x(t)
x
x
x(t) = x
x
F(x) = 0
x  m
§  F(x)
678
Chapter 13
Neurodynamics

Section 13.3
Stability of Equilibrium States
679
TABLE 13.1
Classification of the Equilibrium State of a Second-Order
System
Type of Equilibrium
State x
Eigenvalues of the Jacobian A
Stable node
Real and negative
Stable focus
Complex conjugate with negative real parts
Unstable node
Real and positive
Unstable focus
Complex conjugate with positive real parts
Saddle point
Real with opposite signs
Center
Conjugate purely imaginary
eigenvalues and may therefore be classified in a corresponding fashion. In particular,
when the Jacobian matrix A has m eigenvalues with positive real parts, we say that the
equilibrium state is of type m.
For the special case of a second-order system, we may classify the equilibrium
state as summarized in Table 13.1 and illustrated in Fig. 13.4 (Cook, 1986;Arrowsmith
and Place, 1990). Without loss of generality, the equilibrium state is assumed to be at
the origin of the state space—that is, x  0. Note also that in the case of a saddle point,
shown in Fig. 13.4e, the trajectories going to the saddle point are stable, whereas the tra-
jectories coming out from the saddle point are unstable.
x
0
Real
Imaginary
x1
x2
0
Real
Imaginary
x2
x1
(a)
(b)
FIGURE 13.4
(a) Stable node.
(b) Stable focus. (c) Unstable node.
(d) Unstable focus. (e) Saddle point.
(f) Center.
(The figure is continued on the next
page.)

680
Chapter 13
Neurodynamics
0
Real
Imaginary
x1
x2
0
Real
Imaginary
x2
x1
0
Real
Imaginary
x1
x2
x1
x2
0
Real
Imaginary
(c)
(d)
(e)
(f)
FIGURE 13.4
(continued)

Section 13.3
Stability of Equilibrium States
681
Definitions of Stability
Linearization of the state-space equation, as outlined previously, provides useful
information about the local stability properties of an equilibrium state. However, for
us to be able to investigate the stability of a nonlinear dynamic system in a more
detailed fashion, we need precise definitions of the stability and convergence of an
equilibrium state.
In the context of an autonomous nonlinear dynamic system with equilibrium state
, the definitions of stability and convergence are as follows (Khalil, 1992):
Definition 1. The equilibrium state is said to be uniformly stable if, for any posi-
x
x
tive constant , there exists another positive constant   () such that the condition
implies that
for all t > 0.
In effect, this definition states that a trajectory of the system can be made to
stay within a small neighborhood of the equilibrium state if the initial state x(0)
is close to . Otherwise, the system is unstable.
Definition 2. The equilibrium state is said to be convergent if there exists a posi-
tive constant δ such that the condition
implies that
The meaning of this second definition is that if the initial state x(0) of a trajectory
is close enough to the equilibrium state , then the trajectory described by the state
vector x(t) will approach as time t approaches infinity.
Definition 3. The equilibrium state is said to be asymptotically stable if it is both
stable and convergent.
Here, we note that stability and convergence are independent properties. It is
only when both properties are satisfied that we have asymptotic stability.
Definition 4. The equilibrium state is said to be globally asymptotically stable
if it is stable and all trajectories of the system converge to as time t approaches
infinity.
This last definition implies that the system cannot have other equilibrium states,
and it requires that every trajectory of the system remain bounded for all time t > 0.
In other words, global asymptotic stability implies that the system will ultimately settle
down to a steady state for any choice of initial conditions.
x
x
x
x
x
x(t) S x  as t S q
7x(0) - x7 6 
x
x
x
7x(t) - x7 6 
7x(0) - x7 6 

EXAMPLE 1
Uniform Stability
Let a solution u(t) of the nonlinear dynamic system described by Eq.(13.2) vary with time t as indi-
cated in Fig. 13.5. For the solution u(t) to be uniformly stable, we require that u(t) and any other
solution v(t) remain close to each other for the same values of t (i.e., time “ticks”), as illustrated
in Fig. 13.5.This kind of behavior is referred to as an isochronous correspondence of the two solu-
tions v(t) and u(t).The solution u(t) is convergent provided that, for every other solution v(t) for
which 
at time t  0, the solutions v(t) and u(t) converge to an equilibrium
state as t approaches infinity.
■
Lyapunov’s Theorems
Now that we have defined stability and asymptotic stability for an equilibrium state of a
dynamic system, the next issue to be considered is that of determining stability. Obviously,
we may do so by actually finding all possible solutions to the state-space equation of the
system; however, such an approach is often difficult, if not impossible. A more elegant
approach is to be found in modern stability theory, founded by Lyapunov (1892).Specifically,
we may investigate the stability problem by applying the direct method of Lyapunov, which
makes use of a continuous scalar function of the state, called a Lyapunov function, to be
defined.
Lyapunov’s theorems on the stability and asymptotic stability of the state-space
equation of Eq. (13.2), describing an autonomous nonlinear dynamic system with state
vector x(t) and equilibrium state , may be stated as follows (Khalil, 1992):
Theorem 1. The equilibrium state is stable if, in a small neighborhood of , there exists a
positive-definite function V(x) such that its derivative with respect to time is negative semidefinite
in that region.
Theorem 2. The equilibrium state is asymptotically stable if, in a small neighborhood of
, there exists a positive-definite function V(x) such that its derivative with respect to time is nega-
tive definite in that region.
A scalar function V(x) that satisfies the requirements of these two theorems is called a
Lyapunov function for the equilibrium state .x
x
x
x
x
x
7 v(0) - u(0)7  ()
682
Chapter 13
Neurodynamics
v(0)
u(0)
t  0

u(t)
u(0)
v(t)
v(0)
FIGURE 13.5
Illustration of the
notion of uniform stability of a
state vector.

Section 13.4
Attractors
683
Theorems 1 and 2 require the Lyapunov function V(x) to be a positive-definite
function. Such a function is defined as follows:
1. The function V(x) has continuous partial derivatives with respect to the elements of the
state x.
2.
3.
where 
is a small neighborhood around .
Given that V(x) is a Lyapunov function, then according to Theorem 1, the equi-
librium state is stable if the following condition holds2:
(13.11)
Furthermore, according to Theorem 2, the equilibrium state is asymptotically stable if
(13.12)
The important point of this discussion is that Lyapunov’s theorems can be applied
without having to solve the state-space equation of the system. Unfortunately, the the-
orems give no indication of how to find a Lyapunov function; it is a matter of ingenuity
and trial and error in each case. In many problems of interest, the energy function can
serve as a Lyapunov function.The inability to find a suitable Lyapunov function does not,
however, prove instability of the system.The existence of a Lyapunov function is a suf-
ficient, but not necessary, condition for stability.
The Lyapunov function V(x) provides the mathematical basis for stability analy-
sis of the nonlinear dynamic system described by Eq. (13.2). On the other hand, the use
of Eq. (13.10), based on the Jacobian A, provides the basis for local stability analysis of
the system. Simply put, the Lyapunov stability analysis is much more powerful in its
conclusions than the local stability analysis.
Lyapunov Surface
For an intuitive understanding of the two Lyapunov theorems, we introduce the notion
of a Lyapunov surface, which is formally defined by
V(x)  c
for some constant c  0
Under Theorem 1, the condition
implies that once a trajectory crosses a Lyapunov surface for some positive constant c,
the trajectory moves inside a set of points defined by
and can never come out of the Lyapunov surface. It is in this sense that we speak of the
stability of the system under Theorem 1.
x  N given V(x)  0
d
dt V(x)  0
d
dt V(x) 6 0  for x  u - x
x
d
dt V(x)  0  for x  u - x
x
x
u
V(x) 7 0 if x  u - x.
V(x) = 0.

On the other hand, under Theorem 2, the condition
implies that the trajectory will move from one Lyapunov surface to an inner Lyapunov
surface with a smaller constant c, as illustrated in Fig. 13.6. In particular, as the constant
c decreases in value, the Lyapunov surface moves closer to the equilibrium state in a
corresponding fashion, the implication being that as time t progresses the trajectory
approaches the equilibrium state . However, we cannot be sure that the trajectory will
actually converge onto as 
. Nevertheless, we can conclude that the equilibrium
state is stable in the restricted sense that the trajectory is contained inside any ball 
of some small radius ,requiring that the initial condition x(0) lies inside a Lyapunov sur-
face contained in that ball (Khalil,1992).As a matter of interest,this is the condition that
we referred to in Section 8.5 on the asymptotical stability of the maximum eigenfilter.
13.4
ATTRACTORS
Dissipative systems are generally characterized by the presence of attracting sets or man-
ifolds of dimensionality lower than that of the state space.The notion of a manifold was dis-
cussed at some length in Chapter 7. In short, by a “manifold,” we mean a k-dimensional
surface embedded in the N-dimensional state space,which is defined by the set of equations
(13.13)
where x1, x2, ..., xN are elements of the N-dimensional state of the system and Mj is
some function of these elements. These manifolds are called attractors3 in that they
Mj(x1, x2, ..., xN) = 0,  e j = 1, 2, ..., k
k 6 N

b
x
t S q
x
x
x
d
dt V(x) 6 0
684
Chapter 13
Neurodynamics
x1
c1
c2
c3
x2
0
x
FIGURE 13.6
Lyapunov surfaces for decreasing value of constant c, with c1  c2  c3.
The equilibrium state is denoted by the point .
x–

Section 13.5
Neurodynamic Models
685
Equilibrium
point
Basin of
attraction
T1
T2
Q
P
FIGURE 13.7
Illustration of the notion of a basin of attraction and the idea of a separatrix.
are bounded subsets to which regions of initial conditions of a nonzero state-space
volume converge as time t increases.
The manifold may consist of a single point in the state space, in which case we
speak of a point attractor. Alternatively, it may be in the form of a periodic orbit, in
which case we speak of a stable limit cycle—stable in the sense that nearby trajectories
approach it asymptotically. Figure 13.7 illustrates these two types of attractors. Attrac-
tors represent only the equilibrium states of a dynamic system that may be observed
experimentally. Note, however, that in the context of attractors, an equilibrium state
does not imply a static equilibrium, nor does it imply a steady state. For example, a limit
cycle represents a stable state of an attractor, but it varies continuously with time.
In Fig. 13.7, we note that each attractor is encompassed by a distinct region of its
own. Such a region is called a basin (domain) of attraction. Note also that every initial
state of the system is in the basin of some attractor.The boundary separating one basin of
attraction from another is called a separatrix. In the case of Fig. 13.7, the basin boundary
is represented by the union of the trajectory T1, the saddle point Q, and the trajectory T2.
A limit cycle constitutes the typical form of an oscillatory behavior that arises
when an equilibrium point of a nonlinear system becomes unstable.As such, it can arise
in nonlinear systems of any order. Nevertheless, limit cycles are particularly character-
istic of second-order systems.
Hyperbolic-Attractors
Consider a point attractor whose nonlinear dynamic equations are linearized around
the equilibrium state in the manner described in Section 13.2.Let A denote the Jacobian
of the system evaluated at 
.The attractor is said to be a hyperbolic attractor if the
eigenvalues of the Jacobian A all have an absolute value less than 1 (Ott, 1993). For
example, the flow of a second-order hyperbolic attractor may have the form shown in
Fig. 13.4a or that of Fig. 13.4b; in both cases, the eigenvalues of the Jacobian A have
negative real parts. Hyperbolic attractors are of particular interest in the study of the
“vanishing-gradients problem,” which arises in dynamically driven recurrent networks;
this problem will be discussed in Chapter 15.
x = x
x

13.5
NEURODYNAMIC MODELS
Having familiarized ourselves with the behavior of nonlinear dynamic systems, we are
now ready to discuss some of the important issues involved in neurodynamics, which
we do in this section and the ones to follow. We emphasize that there is no universally
agreed-upon definition of what we mean by neurodynamics. Rather than try to present
such a definition, we will instead define the most general properties of the neurody-
namic systems considered in this chapter. In particular, the discussion is limited to neu-
rodynamic systems whose state variables are continuous valued and whose equations of
motion are described by differential equations or difference equations. The systems of
interest possess four general characteristics (Peretto and Niez, 1986; Pineda, 1988a):
1. A large number of degrees of freedom. The human cortex is a highly parallel, dis-
tributed system that is estimated to possess about 10 billion neurons, with each
neuron modeled by one or more state variables. It is generally believed that both
the computational power and the fault-tolerant capability of such a neurodynamic
system are the result of the collective dynamics of the system.The system is char-
acterized by a very large number of coupling constants represented by the strengths
(efficacies) of the individual synaptic junctions.
2. Nonlinearity. A neurodynamic system is inherently nonlinear. In fact, nonlinear-
ity is essential for creating a universal computing machine.
3. Dissipation. A neurodynamic system is dissipative. It is therefore characterized by
the convergence of the state-space volume onto a manifold of lower dimension-
ality as time goes on.
4. Noise. Finally, noise is an intrinsic characteristic of neurodynamic systems. In real-
life neurons, membrane noise is generated at synaptic junctions (Katz, 1966).
The presence of noise necessitates the use of a probabilistic treatment of neural
activity, adding another level of complexity to the analysis of neurodynamic systems.A
detailed treatment of stochastic neurodynamics is beyond the scope of this book, as pre-
viously noted.The effect of noise is therefore ignored in the material that follows.
Additive Model
Consider the noiseless, dynamic model of a neuron shown in Fig. 13.8. In physical terms,
the synaptic weights wj1, wj2, ..., wjN represent conductances, and the respective inputs
x1(t), x2(t), ..., xN(t) represent potentials; N is the number of inputs. These inputs are
applied to a current-summing junction, characterized as follows:
• low input resistance;
• current gain of unity;
• high output resistance.
The current-summing junction thus acts as a summing node for the input currents. The
total current flowing toward the input node of the nonlinear element (activation function)
in Fig. 13.8 is therefore
a
N
i=1
wjixi(t) + Ij
686
Chapter 13
Neurodynamics

Section 13.5
Neurodynamic Models
687
wj1
wj1x1(t)
wj2x2(t)
wj3x3(t)
wjNxN(t)
wj2
wj3
wjN
x1(t)
x2(t)
xj(t)
x3(t)
xN(t)
Nonlinearity
Neural
output
Ij
Cj
Rj
Current
source
Current-
summing
junction
•
•
•
vj
w()
Σ wjixi(t)
i
Synaptic
inputs
FIGURE 13.8
Additive model of a neuron, labeled j.
where the first (summation) term is due to the stimuli x1(t), x2(t), ..., xN(t) acting on the
synaptic weights (conductances) wj1,wj2,...,wjN, respectively,and the second term is due to
the current source Ij representing an externally applied bias.Let vj(t) denote the induced local
field at the input of the nonlinear activation function (·).We may then express the total cur-
rent flowing away from the input node of the nonlinear element as the sum of two terms:
where the first term is due to leakage resistance Rj and the second term is due to leak-
age capacitance Cj. From Kirchoff’s current law, we know that the total current flowing
toward any node of an electrical circuit is zero. By applying Kirchoff’s current law to the
input node of the nonlinearity in Fig. 13.8, we get
(13.14)
The capacitive term Cjdvj(t)/dt on the left-hand side of Eq. (13.14) is the simplest way
to add dynamics (memory) to the model of a neuron. Given the induced local field vj(t),
we may determine the output of neuron j by using the nonlinear relation
(13.15)
The RC model described by Eq. (13.14) is commonly referred to as the additive model;
this terminology is used to distinguish the model from multiplicative (or shunting)
models, in which wji is dependent on xi.
A characteristic feature of the additive model described by Eq. (13.14) is that the
signal xi(t) applied to neuron j by adjoining neuron i is a slowly varying function of time
t. The model thus described constitutes the basis of classical neurodynamics.4
xj(t) = (vj(t))
Cj
dvj(t)
dt
+
vj(t)
Rj
= a
N
i=1
wjixi(t) + Ij
vj(t)
Rj
+ Cj
dvj(t)
dt

To proceed further, consider a recurrent network consisting of an interconnection
of N neurons, each of which is assumed to have the same mathematical model described
in Eqs. (13.14) and (13.15).Then, ignoring interneuron propagation time delays, we may
define the dynamics of the network by the following system of coupled first-order dif-
ferential equations:
(13.16)
This system of equations has the same mathematical form as the state equations of
Eq. (13.1); it follows from a simple rearrangement of terms in Eq. (13.14). It is assumed
that the activation function (·) relating the output xj(t) of neuron j to its induced local
field vj(t) is a continuous function and therefore differentiable with respect to time t.A
commonly used activation function is the logistic function
(13.17)
A necessary condition for the learning algorithms described in Sections 13.6 through
13.11 to exist is that a recurrent network described by Eqs. (13.15) and (13.16) possesses
fixed points (i.e., point attractors).
Related Model
To simplify the exposition, we assume that the time constant j  RjCj of neuron j in
Eq. (13.16) is the same for all j. Then, by normalizing time t with respect to the common
value of this time constant, and normalizing wji and Ij with respect to Rj, we may recast
the model of Eq. (13.16) in the simplified form
(13.18)
where we have also incorporated Eq.(13.15).The attractor structure of the system of cou-
pled first-order nonlinear differential equations given in Eq. (13.18) is basically the same
as that of a closely related model described in Pineda (1987):
(13.19)
In the additive model described by Eq. (13.18), the induced local fields v1(t), v2(t), ..., vN(t)
of the individual neurons constitute the state vector.On the other hand,in the related model
of Eq. (13.19), the outputs of the neurons x1(t), x2(t), ..., xN(t) constitute the state vector.
These two neurodynamic models are in fact related to each other by a linear,
invertible transformation. Specifically, by multiplying both sides of Eq. (13.19) by wkj,
summing with respect to j, and then substituting the transformation
vk(t) = a
j
wkjxj(t)
dxj(t)
dt
= -xj(t) +  a a
i
wjixi(t) b + Kj,  j = 1, 2, ..., N
dvj(t)
dt
= - vj(t) + a
i
wji(vi(t)) + Ij,  j = 1, 2, ..., N
(vj) =
1
1 + exp(- vj),  j = 1, 2, ..., N
Cj
dvj(t)
dt
= -
vj(t)
Rj
+ a
N
i=1
wjixi(t) + Ij,  j = 1, 2, ..., N
688
Chapter 13
Neurodynamics

Section 13.6
Manipulation of Attractors as a Recurrent Network Paradigm
689
we obtain a model of the type described by Eq. (13.18) and thereby find that the bias
terms of the two models are related by
The important point to note here is that results concerning the stability of the additive
model of Eq. (13.18) are applicable to the related model of Eq. (13.19).
For block-diagram descriptions of the neurodynamic models of Eqs. (13.18) and
(13.19), the reader is referred to Problem 13.2.
13.6
MANIPULATION OF ATTRACTORS AS A RECURRENT
NETWORK PARADIGM
When the number of neurons, N, is very large, the neurodynamic noiseless model
described by Eq. (13.16) possesses the general properties outlined in Section 13.5: very
many degrees of freedom, nonlinearity, and dissipation. Accordingly, such a neurody-
namic model can have complicated attractor structures and may therefore exhibit use-
ful computational capabilities.
The identification of attractors with computational objects (e.g., associative mem-
ories and input–output mappers) is one of the foundations of neural network paradigms.
In order to implement this idea,we must exercise control over the locations of the attrac-
tors in the state space of the system.A learning algorithm then takes the form of a non-
linear dynamic equation that manipulates the locations of the attractors for the purpose
of encoding information in a desired form or learning temporal structures of interest. In
this way, it is possible to establish an intimate relationship between the physics of the
machine and the algorithms of the computation.
One way in which the collective properties of a neural network may be used to
implement a computational task is by way of the concept of energy minimization. The
Hopfield network and the brain-state-in-a-box model, to be considered in Sections
13.7 and 13.9, respectively, are well-known examples of such an approach. Both of
these models are energy-minimizing networks; they differ from each other in their
areas of application. The Hopfield network could be used as a content-addressable
memory or an analog computer for solving combinatorial-type optimization prob-
lems.The brain-state-in-a-box model, on the other hand, is useful for clustering types
of applications. More will be said about these applications in subsequent sections of
the chapter.
The Hopfield network and brain-state-in-a-box model are examples of an asso-
ciative memory with no hidden neurons; an associative memory is an important resource
for intelligent behavior.Another neurodynamic model is that of an input–output map-
per, the operation of which relies on the availability of hidden neurons. In this latter
case, the method of steepest descent is often used to minimize a cost function defined
in terms of the network parameters, and thereby to change the attractor locations.This
latter application of a neurodynamic model is exemplified by the dynamically driven
recurrent networks discussed in Chapter 15.
Ik = a
j
wkjKj

13.7
HOPFIELD MODEL
The Hopfield network (model) consists of a set of neurons and a corresponding set of
unit-time delays, forming a multiple-loop feedback system, as illustrated in Fig. 13.9.The
number of feedback loops is equal to the number of neurons.Basically,the output of each
neuron is fed back,via a unit-time delay element, to each of the other neurons in the net-
work. In other words, there is no self-feedback in the model; the reason for avoiding the
use of self-feedback is explained later.
To study the dynamics of the Hopfield network, we use the neurodynamic model
described in Eq. (13.16), which is based on the additive model of a neuron.
Recognizing that xi(t)  i(vi(t)), we may rewrite Eq. (13.16) in the form
(13.20)
To study the stability of this system of differential equations,we make three assumptions:
1. The matrix of synaptic weights is symmetric—that is,
(13.21)
2. Each neuron has a nonlinear activation of its own—hence the use of i() in 
Eq. (13.20).
wji = wij  for all i and j
Cj
d
dt vj(t) = -
vj(t)
Rj
+ a
N
i=1
wjii(vi(t)) + Ij,  j = 1, ..., N
690
Chapter 13
Neurodynamics
Neurons
Unit-time delay
operators
z1
z1
z1
z1
FIGURE 13.9
Architectural graph of a Hopfield
network consisting of N  4 neurons.

Section 13.7
Hopfield Model
691
3. The inverse of the nonlinear activation function exists, so we may write
(13.22)
Let the sigmoid function i(v) be itself defined by the hyperbolic tangent function
(13.23)
which has a slope of ai/2 at the origin, as shown by
(13.24)
Henceforth, we refer to ai as the gain of neuron i.
Based on the sigmoid function of Eq. (13.23), the inverse output–input relation of
Eq. (13.22) may be expressed as
(13.25)
The standard form of the inverse output–input relation for a neuron of unity gain is
defined by
(13.26)
We may rewrite Eq. (13.25) in terms of this standard relation as
(13.27)
Figure 13.10a shows a plot of the standard sigmoidal nonlinearity (v), and Fig. 13.10b
shows the corresponding plot of the inverse nonlinearity -1(x).
The energy (Lyapunov) function of the Hopfield network in Fig. 13.9 is defined as
follows:
(13.28)
The energy function E defined by Eq. (13.28) may have a complicated landscape with
many minima. The dynamics of the network are described by a mechanism that seeks
out those minima.
With minima in mind, differentiating E with respect to time t, we get
(13.29)
dE
dt = - a
N
j=1
a a
N
i=1
wjixi -
vj
Rj
+ Ij b
dxj
dt
E = - 1
2 a
N
i=1 a
N
j=1
wjixixj + a
N
j=1
1
Rj 3
xj
0
-1
j (x)dx - a
N
j=1
Ijxj
-1
i (x) = 1
ai
-1(x)
-1(x) = - log a 1 - x
1 + x b
v = -1
i (x) = - 1
ai
 log a 1 - x
1 + x b
ai
2 = di
dv  ` 
v=0
x = i(v) = tanh a aiv
2 b =
1 - exp(-aiv)
1 + exp(-aiv)
v = -1
i (x)

692
Chapter 13
Neurodynamics
0
1
1
(a)
x  w(v)
v
0
1
1
(b)
v  w1(x)
x
FIGURE 13.10
Plots of (a) the standard
sigmoidal nonlinearity, and (b) its inverse.
The quantity inside the parentheses on the right-hand side of Eq. (13.29) is recognized
as Cj dvj/dt by virtue of Eq. (13.20).We may thus simplify Eq. (13.29) to
(13.30)
We now recognize the inverse relation that defines vj in terms of xj. The use of
Eq. (13.22) in Eq. (13.30) yields
dE
dt = - a
N
j=1
Cj a
dvj
dt b 
dxj
dt

Section 13.7
Hopfield Model
693
(13.31)
From Fig. 13.10b, we see that the inverse output–input relation j
-1(xj) is a monotoni-
cally increasing function of the output xj. It therefore follows that
(13.32)
We also note that
(13.33)
Hence, all the factors that make up the sum on the right-hand side of Eq. (13.31) are
nonnegative. In other words, for the energy function E defined in Eq. (13.28), we
have
From the definition of Eq. (13.28), we note that the function E is bounded.Accordingly,
we may now make two statements:
1. The energy function E is a Lyapunov function of the continuous Hopfield model.
2. The model is stable in accordance with Lyapunov’s theorem 1.
In other words, the time evolution of the continuous Hopfield model described by the
system of nonlinear first-order differential equations given in Eq. (13.20) represents a
trajectory in state space that seeks out the minima of the energy (Lyapunov) function
E and comes to a stop at such fixed points. From Eq. (13.31), we also note that the deriv-
ative dE/dt vanishes only if
We may thus go one step further and write
(13.34)
Equation (13.34) provides the basis for the following statement:
The (Lyapunov) energy function E of a Hopfield network is a monotonically decreasing
function of time.
Accordingly, the Hopfield network is asymptotically stable in the Lyapunov sense; the
attractor fixed points are the minima of the energy function, and vice versa.
dE
dt 6 0  except at a fixed point
d
dt xj(t) = 0  for all j
dE
dt  0   for all t
a
dxj
dt b
2
 0   for all xj
d
dxj
-1
j (xj)  0   for all xj
 = - a
N
j=1
Cj a
dxj
dt b
2
c d
dxj
-1
j (xj)d
 dE
dt = - a
N
j=1
Cjc d
dt -1
j (xj)d 
dxj
dt

Relation between the Stable States of the Discrete and 
Continuous Versions of the Hopfield Model
The Hopfield network may be operated in a continuous mode or a discrete mode,
depending on the model adopted for describing the neurons. The continuous mode
of operation is based on an additive model, as previously described. On the other
hand, the discrete mode of operation is based on the McCulloch–Pitts model. We
may readily establish the relationship between the stable states of the continuous
Hopfield model and those of the corresponding discrete Hopfield model by redefin-
ing the input–output relation for a neuron such that we may satisfy two simplifying
characteristics:
1. The output of neuron j has the asymptotic values
(13.35)
2. The midpoint of the activation function of the neuron lies at the origin,as shown by
(13.36)
Correspondingly, we may set the bias Ij equal to zero for all j.
In formulating the energy function E for a continuous Hopfield model,the neurons
are permitted to have self-loops.A discrete Hopfield model, on the other hand, need not
have self-loops. We may therefore simplify our discussion by setting wjj  0 for all j in
both models.
In light of these observations, we may redefine the energy function of a continu-
ous Hopfield model given in Eq. (13.28) as
(13.37)
The inverse function j
1 (x) is defined by Eq. (13.27).We may thus rewrite the energy
function of Eq. (13.37) in the form
(13.38)
The integral
has the standard form plotted in Fig. 13.11. Its value is zero for xj  0, and positive other-
wise. It assumes a very large value as xj approaches "1. If, however, the gain aj of neuron
j becomes infinitely large (i.e., the sigmoidal nonlinearity approaches the idealized hard-
limiting form), the second term of Eq. (13.38) becomes negligibly small. In the limiting
case when aj 
for all j, the maxima and minima of the continuous Hopfield model
q
3
xj
0
-1(x)dx
E = - 1
2 a
N
i=1 a
N
j=1
iZj
wjixixj + a
N
j=1
1
ajRj 3
xj
0
-1(x)dx
E = - 1
2 a
N
i=1 a
N
j=1
iZj
wjixixj + a
N
j=1
1
Rj 3
xj
0
j
-1(x)dx
j(0) = 0
xj = e +1  for vj = q
-1  for vj = -q
694
Chapter 13
Neurodynamics

Section 13.7
Hopfield Model
695
0.8
0.6
0.4
0.2
1
xj
1
0
xj
w1(x)dx
⌠
⌡0
FIGURE 13.11
Plot of the
integral 
.
1
xj
0 -1(x)dx
become identical to those of the corresponding discrete Hopfield model.In the latter case,
the energy (Lyapunov) function is defined simply by
(13.39)
where the jth neuron’s state xj  "1.We conclude, therefore, that the only stable points
of the very high-gain, continuous, deterministic Hopfield model correspond to the sta-
ble points of the discrete stochastic Hopfield model.
When, however, each neuron j has a large, but finite, gain aj, we find that the second
term on the right-hand side of Eq.(13.38) makes a noticeable contribution to the energy func-
tion of the continuous model.In particular,this contribution is large and positive near all sur-
faces,edges,and corners of the unit hypercube that defines the state space of the model.On
the other hand, the contribution is negligibly small at points that are far removed from the
surface.Accordingly,the energy function of such a model has its maxima at corners,but the
minima are displaced slightly toward the interior of the hypercube.
Figure 13.12 depicts the energy contour map, or energy landscape, for a continuous
Hopfield model using two neurons.The outputs of the two neurons define the two axes
of the map. The lower left- and upper right-hand corners of Fig. 13.12 represent stable
minima for the limiting case of infinite gain; the minima for the case of finite gain are
displaced inward. The flow to the fixed points (i.e., stable minima) may be interpreted
as the solution to the minimization of the energy function E defined in Eq. (13.28).
The Discrete Hopfield Model as a Content-Addressable Memory
In the application of the Hopfield network as a content-addressable memory, we know
a priori the fixed points of the network in that they correspond to the patterns to be
stored. However, the synaptic weights of the network that produce the desired fixed
points are unknown, and the problem is how to determine them.The primary function
of a content-addressable memory is to retrieve a pattern (item) stored in memory in
response to the presentation of an incomplete or noisy version of that pattern.To illustrate
E = - 1
2 a
N
i=1 a
N
j=1
iZj
wjixixj

the meaning of this statement in a succinct way, we can do no better than to quote from
Hopfield’s 1982 paper:
Suppose that an item stored in memory is “H.A. Kramers & G.H. Wannier Physi Rev. 60,
252 (1941).” A general content-addressable memory would be capable of retrieving this entire
memory item on the basis of sufficient partial information. The input “& Wannier (1941)”
might suffice.An ideal memory could deal with errors and retrieve this reference even from
the input “Wannier, (1941).”
An important property of a content-addressable memory is therefore the ability to
retrieve a stored pattern, given a reasonable subset of the information content of that
pattern. Moreover, a content-addressable memory is error correcting in the sense that
it can override inconsistent information in the cues presented to it.
696
Chapter 13
Neurodynamics
FIGURE 13.12
An energy contour map for a two-neuron, two-stable-state system.The ordinate
and abscissa are the outputs of the two neurons. Stable states are located near the lower left and
upper right corners, and unstable extrema are located at the other two corners.The arrows show
the motion of the state.This motion is not generally perpendicular to the energy contours. (From
J.J. Hopfield, 1984, with permission of the National Academy of Sciences of the U.S.A.)

Section 13.7
Hopfield Model
697
Encoding
Decoding
Space of
fundamental
memories
Space of
stored vectors
m
xm
FIGURE 13.13
Illustration of the encoding–decoding performed by a recurrent network.
The essence of a content-addressable memory (CAM) is to map a fundamental
memory 
onto a fixed (stable) point xµ of a dynamic system, as illustrated in Fig. 13.13.
Mathematically, we may express this mapping as
The arrow from left to right describes the encoding operation, whereas the arrow
from right to left describes the decoding operation.The attractor’s fixed points of the
state space of the network are the fundamental memories, or prototype states, of the
network. Suppose now that the network is presented a pattern containing partial, but
sufficient, information about one of the fundamental memories.We may then repre-
sent that particular pattern as a starting point in the state space. In principle, pro-
vided that the starting point is close to the fixed point representing the memory being
retrieved (i.e., it lies inside the basin of attraction belonging to the fixed point), the
system should evolve with time and finally converge onto the memory state itself.At
that point, the entire memory is generated by the network. Consequently, the Hopfield
network has an emergent property, which helps it retrieve information and cope with
errors.
With the Hopfield model’s use of the formal neuron of McCulloch and Pitts
(1943) as its basic processing unit, each such neuron has two states determined by the
level of the induced local field acting on it. The “on,” or “firing,” state of neuron i is
denoted by the output xi  1, and the “off,” or “quiescent,” state is represented by
xi  1. For a network made up of N such neurons, the state of the network is thus
defined by the vector
With xi  "1,the state of neuron i represents one bit of information,and the N-by-1 state
vector x represents a binary word of N bits of information.
x = [x1, x2, ..., xN]T
 L x


The induced local field vj of neuron j is defined by
(13.40)
where bj is a fixed bias applied externally to neuron j. Hence, neuron j modifies its state
xj according to the deterministic rule
This relation may be rewritten in the compact form
where sgn is the signum function. What if vj is exactly zero? The action taken here can
be quite arbitrary. For example, we may set xj  "1 if vj  0. However, we will use the
following convention: If vj is zero, neuron j remains in its previous state, regardless of
whether it is on or off. The significance of this assumption is that the resulting flow dia-
gram is symmetrical, as will be illustrated later.
There are two phases to the operation of the discrete Hopfield network as a content-
addressable memory,namely,the storage phase and the retrieval phase,as described here:
1. Storage Phase. Suppose that we wish to store a set of N-dimensional vectors
(binary words) denoted by 
We call these M vectors fundamental
{ ƒ  = 1, 2, ...., M}.
xj = sgn(vj)
xj = e +1  if vj 7 0
-1  if vj 6 0
vj = a
N
i=1
wjixi + bj
698
Chapter 13
Neurodynamics
memories, representing the patterns to be memorized by the network. Let µ,i denote the
ith element of the fundamental memory 
, where the class  
According
to the outer-product rule of storage—that is, the generalization of Hebb’s postulate of
learning—the synaptic weight from neuron i to neuron j is defined by
(13.41)
The reason for using 1/N as the constant of proportionality is to simplify the mathemat-
ical description of information retrieval. Note also that the learning rule of Eq. (13.41) is
a “one shot” computation. In the normal operation of the Hopfield network, we set
(13.42)
which means that the neurons have no self-feedback, as pointed out previously. Let W
denote the N-by-N synaptic-weight matrix of the network, with wji as its ji-th element.We
may then combine Eqs.(13.41) and (13.42) into a single equation written in matrix form as
(13.43)
where µµ
T represents the outer product of the vector µ with itself and I denotes the
identity matrix.From these defining equations of the synaptic weights and weight matrix,
we may reconfirm the following:
W = 1
N a
M
=1
T
 - MI
wii = 0  for all i
wji = 1
N a
M
=1
, j, i
 = 1, 2, ...., M.


Section 13.7
Hopfield Model
699
• The output of each neuron in the network is fed back to all other neurons.
• There is no self-feedback in the network (i.e., wii  0).
• The weight matrix of the network is symmetric, as shown by the following relation
(see Eq. (13.21)):
(13.44)
2. Retrieval Phase. During the retrieval phase, an N-dimensional vector probe,
called a probe, is imposed on the Hopfield network as its state.The probe vector has ele-
ments equal to "1. Typically, it represents an incomplete or noisy version of a funda-
mental memory of the network. Information retrieval then proceeds in accordance with
a dynamic rule, in which each neuron j of the network randomly, but at some fixed rate,
examines the induced local field vj (including any nonzero bias bj) applied to it. If, at that
instant of time, vj is greater than zero, neuron j will switch its state to 1 or remain in
that state if it is already there. Similarly, if vj is less than zero, neuron j will switch its
state to 1 or remain in that state if it is already there. If vj is exactly zero, neuron j is
left in its previous state, regardless of whether it is on or off. The state’s update from
one iteration to the next is therefore deterministic, but the selection of a neuron to per-
form the updating is done randomly. The asynchronous (serial) updating procedure
described here is continued until there are no further changes to report. That is, start-
ing with the vector probe, the network finally produces a time-invariant state vector y
whose individual elements satisfy the following condition for stability:
(13.45)
In matrix form, Eq. (13.45) is expressed as
(13.46)
where W is the synaptic-weight matrix of the network and b is the externally applied bias
vector. The stability condition described here is also referred to as the alignment condition.
The state vector y that satisfies the alignment condition is called a stable state or fixed point
of the state space of the system.We may therefore say that the Hopfield network will always
converge to a stable state when the retrieval operation is performed asynchronously.5
Table 13.2 presents a summary of the steps involved in the storage phase and
retrieval phase of operating a Hopfield network.
EXAMPLE 2
Emergent Behavior of a Hopfield Model with Three Neurons
To illustrate the emergent behavior of the Hopfield model, consider the network of Fig. 13.14a,
which consists of three neurons.The weight matrix of the network is
The weight matrix W is legitimate because it satisfies the necessary conditions of Eqs. (13.42) and
(13.44).The bias applied to each neuron is assumed to be zero.With three neurons in the network,
W = 1
3 £
0
-2
+2
-2
0
-2
+2
-2
0
§
y = sgn(Wy + b)
yj = sgn a a
N
i=1
wjiyi + bj b,  j = 1, 2, ..., N
WT = W

there are 23  8 possible states to consider. Of these eight states, only the two states (1, 1, 1) and
(1, 1, 1) are stable; the remaining six states are all unstable. We say that these two particular
states are stable because they both satisfy the alignment condition of Eq. (13.46). For the state vec-
tor (1, 1, 1), we have
for which the use of hard-limiting yields the result
Similarly, for the state vector (1, 1, 1), we have
Wy = 1
3 £
0
-2
+2
-2
0
-2
+2
-2
0
§ £
-1
+1
-1
§ = 1
3 £
-4
+4
-4
§
sgn(Wy) = £
+1
-1
+1
§ = y
Wy = 1
3 £
0
-2
+2
-2
0
-2
+2
-2
0
§ £
+1
-1
+1
§ = 1
3 £
+4
-4
+4
§
700
Chapter 13
Neurodynamics
TABLE 13.2
Summary of the Hopfield Model
1. Learning. Let 1, 2, ...,  denote a known set of N-dimensional fundamental memories. Use the outer-
product rule (i.e., Hebb’s postulate of learning) to compute the synaptic weights of the network as
where wji is the synaptic weight from neuron i to neuron j. The elements of the vector µ equal "1.
Once they are computed, the synaptic weights are kept fixed.
2. Initialization. Let probe denote an unknown N-dimensional input vector (probe) presented to the
network.The algorithm is initialized by setting
where xj(0) is the state of neuron j at time n  0 and j,probe is the jth element of the probe probe.
3. Iteration Until Convergence. Update the elements of state vector x(n) asynchronously (i.e., ran-
domly and one at a time) according to the rule
Repeat the iteration until the state vector x remains unchanged.
4. Outputting. Let xfixed denote the fixed point (stable state) computed at the end of step 3.The result-
ing output vector y of the network is
Step 1 is the storage phase, and steps 2 through 4 constitute the retrieval phase.
y = xfixed
xj(n + 1) = sgn a a
N
i=1
wjixi(n) b,  j = 1, 2, ..., N
xj(0) = j, probe,  j = 1, ..., N
wji = µ
1
N a
M
=1
,j ,i,
j Z i
0,
j = i

Section 13.7
Hopfield Model
701
2
3

2
3

2
3

2
3

2
3

2
3

1
2
3
y2
y3
y1
(1, 1, 1)
(1, 1, 1)
(1, 1, 1)
(1, 1, 1)  0
(1, 1, 1)
Stable state
(1, 1, 1)
Stable state
(1, 1, 1)
(1, 1, 1)
(a)
(b)
FIGURE 13.14
(a) Architectural graph of
Hopfield network for N  3
neurons. (b) Diagram
depicting the two stable
states and flow of the
network.
which, after hard limiting, yields
Hence, both of these state vectors satisfy the alignment condition. Note that the two stable states
of the model are the negative of each other.
sgn(Wy) = £
-1
+1
-1
§ = y

702
Chapter 13
Neurodynamics
Moreover, following the asynchronous updating procedure summarized in Table 13.2, we
get the flow described in Fig. 13.14b. This flow map exhibits symmetry with respect to the two
stable states of the network, which is intuitively satisfying.This symmetry is the result of leaving
a neuron in its previous state if the induced local field acting on it is exactly zero.
Figure 13.14b also shows that if the network of Fig. 13.14a is in the initial state (1, 1, 1),
(1, 1, 1), or (1, 1, 1), it will converge onto the stable state (1, 1, 1) after one iteration. If,
on the other hand, the initial state is (1, 1, 1), (1, 1, 1), or (1, 1, 1), it will converge onto
the second stable state (1, 1, 1).
The network therefore has two fundamental memories, (1, 1, 1) and (1, 1, 1), repre-
senting the two stable states.The application of Eq. (13.43) yields the synaptic-weight matrix
which checks exactly with the synaptic weights shown in Fig. 13.14a.
The error-correcting capability of the Hopfield network is readily seen by examining the flow
map of Fig. 13.14b:
1. If the probe probe applied to the network equals (1, 1, 1), (1, 1, 1), or (1, 1, 1), the
resulting output is the fundamental memory (1, 1, 1). Each of these values of the probe
represents a single error, compared with the stored pattern.
2. If the probe probe equals (1, 1, 1), (1, 1, 1), or (1, 1, 1), the resulting network out-
put is the fundamental memory (1, 1, 1). Here again, each of these values of the
probe represents a single error, compared with the stored pattern.
■
Spurious States
The weight matrix W of a discrete Hopfield network is symmetric, as indicated in
Eq. (13.44).The eigenvalues of W are therefore all real. However, for large M, the eigen-
values are ordinarily degenerate, which means that there are several eigenvectors with
the same eigenvalue.The eigenvectors associated with a degenerate eigenvalue form a
subspace. Furthermore, the weight matrix W has a degenerate eigenvalue with a value
of zero, in which case the subspace is called the null space. The null space exists by virtue
of the fact that the number of fundamental memories, M, is smaller than the number of
neurons, N, in the network. The presence of a null subspace is an intrinsic characteris-
tic of the Hopfield network.
An eigenanalysis of the weight matrix W leads us to take the following view-
point of the discrete Hopfield network used as a content-addressable memory (Aiyer
et al., 1990):
1. The discrete Hopfield network acts as a vector projector in the sense that it projects
a probe onto a subspace 
spanned by the fundamental memory vectors.
2. The underlying dynamics of the network drive the resulting projected vector to one
of the corners of a unit hypercube where the energy function is minimized.
m
 = 1
3 £
0
-2
+2
-2
0
-2
+2
-2
0
§
 W = 1
3 £
+1
-1
+1
§ [+1, -1, +1] + 1
3 £
-1
+1
-1
§ [-1, +1, -1] - 2
3 £
1
0
0
0
1
0
0
0
1
§

Section 13.8
The Cohen–Grossberg Theorem
703
The unit hypercube is N-dimensional. The M fundamental memory vectors,
spanning the subspace 
, constitute a set of fixed points (stable states) represented by
certain corners of the unit hypercube. The other corners of the unit hypercube that
lie in or near subspace 
are potential locations for spurious states, also referred to
as spurious attractors. Unfortunately,spurious states represent stable states of the Hopfield
network that are different from the fundamental memories of the network
In the design of a Hopfield network as a content-addressable memory, we are
therefore faced with a tradeoff between two conflicting requirements:
• the need to preserve the fundamental memory vectors as fixed points in the state
space;
• the desire to have few spurious states.
However, the fundamental memories of a Hopfield network are not always stable.
Moreover,spurious states representing other stable states that are different from the fun-
damental memories can arise.These two phenomena tend to decrease the efficiency of
the Hopfield network as a content-addressable memory.
13.8
THE COHEN–GROSSBERG THEOREM
In Cohen and Grossberg (1983),a general principle for assessing the stability of a certain
class of neural networks is described by a system of coupled nonlinear differential equa-
tions, given as
(13.47)
which admits a Lyapunov function defined as
(13.48)
where j (
) is the derivative of j(
) with respect to 
. For the definition of Eq. (13.48)
to be valid, however, we require three conditions to hold:
1. The synaptic weights of the network are symmetric—that is,
(13.49)
2. The function aj(uj) satisfies the nonnegativity condition—that is,
(13.50)
3. The nonlinear input–output function j(uj) satisfies the monotonicity condition—
that is,
(13.51)
¿j(uj) =
d
duj
j(uj)  0
aj(uj)  0
cij = cji
 E = 1
2 a
N
i=1 a
N
j=1
cjii(ui)j(uj) - a
N
j=1 3
uj
0
bj(
)¿j(
)d
d
dt uj = aj(uj)cbj(uj) - a
N
i=1
cjii(ui)d,  j = 1, ..., N
m
m

With this background, we may now formally state the Cohen–Grossberg theorem:
Provided that the system of nonlinear differential equations (13.47) satisfies the conditions of sym-
metry, nonegativity, and monotonicity, the Lyapunov function E of the system defined by Eq.
(13.48) satisfies the condition
Once this basic property of the Lyapunov function E is in place, stability of the system fol-
lows from Lyapunov’s theorem 1.
The Hopfield Model as a Special Case of the Cohen–Grossberg Theorem
By comparing the general system of Eq. (13.47) with the system of Eq. (13.20) for a
continuous Hopfield model,we may make the correspondences between the Hopfield model
and the Cohen–Grossberg theorem that are summarized in Table 13.3.The use of this table
in Eq. (13.48) yields a Lyapunov function for the continuous Hopfield model given as
(13.52)
where the nonlinear activation function j(·) is defined by Eq. (13.23).
We next make the following observations:
1. i(vi)  xi
2.
3.
Basically, relations 2 and 3 result from the use of x  i(v).Thus, the use of these obser-
vations in the Lyapunov function of Eq. (13.52) yields a result identical to the one we
defined earlier; see Eq. (13.28). Note, however, that although i(v) must be a nonde-
creasing function of the input v, it does not need to have an inverse in order for the gen-
eralized Lyapunov function of Eq. (13.52) to hold.
The Cohen–Grossberg theorem is a general principle of neurodynamics with a
wide range of applications (Grossberg, 1990). In the next section, we consider another
application of this important theorem.
1
vj
0 v¿j(v)dv = 1
xj
0 vdx = 1
xi
0 -1
j (x)dx
1
vj
0 ¿j(v)dv = 1
xj
0 dx = xj
E = - 1
2 a
N
i=1 a
N
j=1
wjii(vi)j(vj) + a
N
j=1 3
vj
0
a
vj
Rj
- Ij b ¿j(v)dv
dE
dt  0
704
Chapter 13
Neurodynamics
TABLE 13.3
Correspondences between the
Cohen–Grossberg Theorem and
the Hopfield Model
Cohen–Grossberg Theorem
Hopfield Model
uj
Cjvj
aj(uj)
1
bj(uj)
(vj/Rj)  Ij
cji
wji
i(ui)
i(vi)

Section 13.9
Brain-State-in-a-Box Model
705
13.9
BRAIN-STATE-IN-A-BOX MODEL
In this section, we continue the neurodynamic analysis of an associative memory 
by studying the brain-state-in-a-box (BSB) model, which was first described by
Anderson et al. (1977). The BSB model is basically a positive feedback system with
amplitude limitation. It consists of a highly interconnected set of neurons that feed
back upon themselves.This model operates by using the built-in positive feedback to
amplify an input pattern until all the neurons in the model are driven into saturation.
The BSB model may thus be viewed as a categorization device in that an analog input
pattern is given a digital representation, defined by a stable state of the model.
Let W denote a symmetric-weight matrix whose largest eigenvalues have positive
real components. Let x(0) denote the initial state vector of the model, representing an
input activation pattern. Assuming that there are N neurons in the model, the state
vector of the model has dimension N, and the weight matrix W is an N-by-N matrix.
The BSB model is then completely defined by the pair of equations
(13.53)
and
(13.54)
where  is a small positive constant called the feedback factor and x(n) is the state
vector of the model at discrete time n. Figure 13.15a shows a block diagram of the
combination of Eqs. (13.53) and (13.54); the block labeled W represents a single-layer
x(n + 1) = (y(n))
y(n) = x(n) + Wx(n)
FIGURE 13.15
(a) Block diagram
of the brain-state-in-a-box (BSB)
model. (b) Signal-flow graph of
the linear associator represented
by the weight matrix W.
(a)
(b)
Feedback
factor
ıx(n)
y(n)
x(n)
x(n  1)
Weight
matrix
Unit delays
Nonlinearity
Σ
W
ı
z1I
Outputs
x1(n)
x2(n)
Input
vector
x(n)
xN(n)
•
•
•
•
•
•
•
•
•
w()

linear neural network, as depicted in Fig. 13.15b. The activation function  is a
piecewise-linear function that operates on yj(n), the jth component of the vector y(n),
as follows (see Fig. 13.16):
(13.55)
Equation (13.55) constrains the state vector of the BSB model to lie within an N-dimen-
sional unit cube centered on the origin.
The model thus proceeds as follows:An activation pattern x(0) is input into the BSB
model as the initial state vector,and Eq.(13.53) is used to compute the vector y(0).Equa-
tion (13.54) is then used to truncate y(0),obtaining the updated state vector x(1).Next,x(1)
is cycled through Eqs. (13.53) and (13.54), thereby obtaining x(2). This procedure is
repeated until the BSB model reaches a stable state represented by a particular corner of
the unit hypercube.Intuitively,positive feedback in the BSB model causes the initial state
vector x(0) to increase in Euclidean length (norm) with an increasing number of itera-
tions until it hits a wall of the box (unit hypercube) and then slides along the wall, even-
tually ending up in a stable corner of the box,where it keeps on “pushing,”but cannot get
out of the box (Kawamoto and Anderson, 1985)—hence the name of the model.
Lyapunov Function of the BSB Model
The BSB may be redefined as a special case of the neurodynamic model described in
Eq. (13.16) (Grossberg, 1990).To see this, we first rewrite the jth component of the BSB
algorithm described by Eqs. (13.53) and (13.54) in the form
 = •
+1
if yj(n) 7 +1
yj(n)
if -1  yj(n)  +1
-1
if yj(n) 6 -1
 xj(n + 1) = (yj(n))
706
Chapter 13
Neurodynamics
y)
y
1
1
1
1
0
w(
FIGURE 13.16
Piecewise-linear
activation function used in the
BSB model.

Section 13.9
Brain-State-in-a-Box Model
707
(13.56)
The coefficients cji are defined by
(13.57)
where ji is the Kronecker delta equal to 1 if j  i and 0 otherwise, and wji is the ji-th
element of the weight matrix W. Equation (13.56) is written in discrete-time form. To
proceed further, we need to reformulate it in a continuous-time form, as shown by
(13.58)
where the bias Ij is zero for all j. However, for us to apply the Cohen–Grossberg theo-
rem, we have to go one step further and transform Eq. (13.58) into the same form as the
additive model.We may do so by introducing a new set of variables,
(13.59)
Then, by virtue of the definition of cji given in Eq. (13.57), we find that
(13.60)
Correspondingly, we may recast the model of Eq. (13.58) in the equivalent form
(13.61)
We are now ready to apply the Cohen–Grossberg theorem to the BSB model. By
comparing Eq. (13.61) with Eq. (13.47), we may deduce the correspondences listed in
Table 13.4 between the BSB model and the Cohen–Grossberg theorem.Therefore, using
d
dt vj(t) = - vj(t) + a
N
i=1
cji (vi(t)),  j = 1, 2, ..., N
xj(t) = a
N
i=1
cjivi(t)
vj(t) = a
N
i=1
cjixi(t)
d
dt xj(t) = -xj(t) +  a a
N
i=1
cjixi(t) b,  j = 1, 2, ..., N
cji = ji + wji
xj(n + 1) =  a a
N
i=1
cjixi(n) b ,  j = 1, 2, ..., N
TABLE 13.4
Correspondences between the
Cohen–Grossberg Theorem 
and the BSB Model
Cohen–Grossberg Theorem
BSB Model
uj
vj
aj(uj)
1
bj(uj)
vj
cji
cji
j(vj)
j(uj)

the results of Table 13.4 in Eq. (13.48), we find that the Lyapunov function of the BSB
model is given by
(13.62)
where (v) is the derivative of the piecewise linear function (v) with respect to its
argument.Finally,substituting the definitions of Eqs.(13.55),(13.57) and (13.59) into Eq.
(13.62), we can define the Lyapunov (energy) function of the BSB model in terms of the
original state variables as follows (Grossberg, 1990):
(13.63)
The evaluation of the Lyapunov function of the Hopfield network presented in
Section 13.7 assumes the existence of the derivative of the inverse of the model’s sig-
moidal nonlinearity, which is satisfied by the use of a hyperbolic tangent function. In
contrast, this condition is not satisfied in the BSB model when the state variable of
the jth neuron in the model is either 1 or 1. Despite this difficulty, the Lyapunov
function of the BSB model can be evaluated via the Cohen–Grossberg theorem,
which clearly illustrates the general applicability of this important theorem.
Dynamics of the BSB Model
In a direct analysis carried out by Golden (1986), it is demonstrated that the BSB model
is in fact a gradient descent algorithm that minimizes the energy function E, defined by
Eq. (13.63).This property of the BSB model, however, presumes that the weight matrix
W satisfies two conditions:
1. The weight matrix W is symmetric:
2. The weight matrix W is positive semidefinite; that is, in terms of the eigenvalues of
W, we have
where 
min is the smallest eigenvalue of W.
Accordingly, the energy function E of the BSB model decreases with increasing n (num-
ber of iterations) whenever the state vector x(n  1) at time n  1 is different from the
state vector x(n) at time n. Moreover, the minimum points of the energy function E
define the equilibrium states of the BSB model that are characterized by
x(n + 1) = x(n)

min  0
W = WT
 = - 
2 xTWx
 E = - 
2 a
N
i=1 a
N
j=1
wjixjxi
E = - 1
2 a
N
j=1 a
N
i=1
cji(vj)(vi) + a
N
j=1 3
vj
0
v¿(v)dv
708
Chapter 13
Neurodynamics

Section 13.9
Brain-State-in-a-Box Model
709
In other words, like the Hopfield model, the BSB model is an energy-minimizing
network.
The equilibrium states of the BSB model are defined by certain corners of the unit
hypercube and its origin. In the latter case, any fluctuation in the state vector, no mat-
ter how small, is amplified by positive feedback in the model and therefore causes the
state of the model to shift away from the origin in the direction of a stable configura-
tion; in other words, the origin is a saddle point. For every corner of the hypercube to
serve as a possible equilibrium state of the BSB model, the weight matrix W has to sat-
isfy a third condition (Greenberg, 1988):
• The weight matrix W is diagonal dominant, which means that
(13.64)
where wij is the ij-th element of W.
For an equilibrium state x to be stable—that is, for a certain corner of the unit
hypercube to be a fixed point attractor—there has to be a basin of attraction 
(x)
in the unit hypercube such that for all initial-state vectors x(0) in 
, the BSB
model converges onto x. For every corner of the unit hypercube to be a possible
point attractor, the weight matrix W has to satisfy a fourth condition (Greenberg,
1988):
• The weight matrix W is strongly diagonal dominant, as shown by
(13.65)
where α is a positive constant.
The important point to take from this discussion is that in the case of a BSB model
for which the weight matrix W is symmetric and positive semidefinite, as is often the
case, only some (but not all) of the corners of the unit hypercube act as point attractors.
For all the corners of the unit hypercube to act as potential point attractors, the weight
matrix W has to satisfy Eq. (13.65) as well, which, of course, subsumes the condition of
Eq. (13.64).
Clustering
A natural application for the BSB model is clustering (Anderson, 1995). This follows
from the fact that the stable corners of the unit hypercube act as point attractors with
well-behaved basins of attraction, which therefore divide the state space into a corre-
sponding set of well-defined regions. Consequently, the BSB model may be used as an
unsupervised clustering algorithm, with each stable corner of the unit hypercube repre-
senting a “cluster” of related data.The self-amplification provided by positive feedback (in
conformity with Principle 1 of self-organization described in Chapter 8) is an important
ingredient of this clustering property.
wjj  a
iZj
wij +   for j = 1, 2, ..., N
n(x)
n
wjj  a
iZj
wij  for j = 1, 2, ..., N

EXAMPLE 3
Autoassociation
Consider a BSB model containing two neurons.The two-by-two matrix of the model is defined by
which is symmetric and positive definite and therefore satisfies Eq. (13.65).
The four parts of Fig. 13.17 correspond to different settings of the initial state of the model:
(a) x(0)  [0.1, 0.2]T
(b) x(0)  [0.2, 0.3]T
W = c 0.035
-0.005
-0.005
0.035 d
710
Chapter 13
Neurodynamics
0
(1, 1)
(a)
(1, 1)
(1, 1)
(1, 1)
0
(1, 1)
(b)
(1, 1)
(1, 1)
(1, 1)
0
(1, 1)
(1, 1)
(1, 1)
(1, 1)
0
(1, 1)
(c)
(1, 1)
(1, 1)
(1, 1)
(d)
FIGURE 13.17
Illustrative example of a two-neuron BSB model, operating under four different
initial conditions:
• the four shaded areas of the figure represent the model’s basins of attraction;
• the corresponding trajectories of the model are plotted in red;
• the four corners, where the trajectories terminate, are printed as black dots.

Section 13.10
Strange Attractors and Chaos
711
(c) x(0)  [0.8, 0.4]T
(d) x(0)  [0.6, 0.1]T
The areas shown shaded in the figure are the four basins of attraction characterizing the model.
The figure clearly illustrates that when the initial state of the model lies in a particular basin
of attraction, the underlying dynamics of the model continually drive the weight matrix W(n)
with increasing number of iterations n, until the state x(n) of the model terminates on a fixed
point attractor, exemplified by the corresponding corner of the two-by-two square. A case of
particular interest is shown in part (d) of the figure. In this case, the initial condition x(0) lies in
the first quadrant, yet the trajectory terminates on the corner (1, 1) in the fourth quadrant,
which is where the pertinent fixed point attractor resides.
In this example, the square state space of the two-neuron BSB model is partitioned com-
pletely into four distinct basins of attraction; each basin embraces a corner of the square, repre-
senting a stable state with minimum energy.Thus, the BSB model may be viewed as an example
of an autoassociative network, in the sense that all the points lying in any one of the basins of
attraction are associated with a minimum-energy stable-state point of its own.
■
13.10
STRANGE ATTRACTORS AND CHAOS
Up to this point in our discussion of neurodynamics, we have focused attention on
the kind of behavior exhibited by nonlinear dynamic systems characterized as fixed-
point attractors. In this section, we consider another class of attractors called strange
attractors, which characterize certain nonlinear dynamic systems of order greater
than two.
A strange attractor exhibits a chaotic behavior that is highly complex. What
makes the study of strange attractors and chaos particularly interesting is the fact that
the system in question is deterministic in the sense that its operation is governed by
fixed rules, yet, with only a few degrees of freedom, can exhibit a behavior so compli-
cated that it looks random. Indeed, the randomness is fundamental in the sense that
the second-order statistics of a chaotic time series seem to indicate that it is random.
However, unlike a true random phenomenon, the randomness exhibited by a chaotic
system does not go away by gathering more information! In principle, the future behav-
ior of a chaotic system is completely determined by the past, but, in practice, any uncer-
tainty in the choice of initial conditions, no matter how small, grows exponentially
with time. Consequently, even though the dynamic behavior of a chaotic system is pre-
dictable in the short term, it is impossible to predict the long-term behavior of the sys-
tem. A chaotic time series is therefore paradoxical in the sense that its generation is
governed by a deterministic dynamic system, yet its evolution as a function of time has
a randomlike appearance. It is this attribute of a chaotic phenomenon that was orig-
inally emphasized by Lorenz with the discovery of an attractor that bears his name
(Lorenz, 1963).
In a nonlinear dynamic system, when the orbits in an attractor with neighboring
initial conditions tend to move apart with increasing time, the system is said to possess
a strange attractor, and the system itself is said to be chaotic. In other words, a funda-
mental property that makes an attractor “strange” is the sensitive dependence on initial
conditions. Sensitivity in this context means that if two identical nonlinear systems are
started at slightly different initial conditions—namely, x and x  , where  is a very

small vector—their dynamic states will diverge from each other in state space, and their
separation will increase exponentially on the average.
Invariant Characteristics of Chaotic Dynamics
Two major features—namely, fractal dimensions and Lyapunov exponents—have
emerged as the classifiers of a chaotic process. Fractal dimensions characterize the geo-
metric structure of a strange attractor. The term “fractal” was coined by Mandelbrot
(1982). Unlike integer dimensions (as in a two-dimensional surface or a three-dimen-
sional object), fractal dimensions are not integers. As for Lyapunov exponents, they
describe how orbits on the attractor move under the evolution of the system dynamics.
These two invariant characteristics of chaotic dynamics are discussed in what follows.The
term “invariant” signifies the fact that both fractal dimensions and Lyapunov exponents
of a chaotic process remain unchanged under smooth nonlinear changes of the coordi-
nate system of the process.
Fractal Dimensions
Consider a strange attractor whose dynamics in d-dimensional state space are described by
(13.66)
which is the discrete-time version of Eq. (13.2).This correspondence is readily seen by
setting t  n!t, where !t is the sampling period. Assuming that !t is sufficiently small,
we may correspondingly set
We may thus formulate the discrete-time version of Eq. (13.2) as
Setting !t  1 for convenience of presentation and rearranging terms, we get
which may be cast into the form shown in Eq. (13.66) simply by absorbing x(n) in a
redefined version of the vector-valued function F(·).
Returning to Eq. (13.66), suppose we construct a small sphere of radius r around
some location y on or near an orbit of the attractor. We may then define a natural dis-
tribution of points for the attractor as
(13.67)
where (·) is a d-dimensional delta function and N is the number of data points.
Note the change of notation concerning the use of N. The natural distribution &(y)
plays a role for a strange attractor that is analogous to that of the probability 
&(y) = lim
NS q 1
N a
N
n=1
(y - x(n))
x(n + 1) = x(n) + F(x(n))
1
¢t [x(n¢t + ¢t) - x(n¢t)] = F(x(n¢t)) for small ¢t
d
dt x(t) = 1
¢t [x(n¢t + ¢t) - x(n¢t)]
x(n + 1) = F(x(n)),  n = 0, 1, 2, ...
712
Chapter 13
Neurodynamics

Section 13.10
Strange Attractors and Chaos
713
density function of a random variable. Accordingly, we may define an invariant 
with respect to a function f(y) under the evolution of the dynamics described as the
multifold integral
(13.68)
A function f(y) of interest is one that gives us a measure of how the number of points
within the small sphere scales as the radius r of the sphere is reduced to zero.Recognizing
that the volume occupied by the d-dimensional sphere is proportional to rd, we may get
a sense of attractor dimension by seeing how the density of points on the attractor
behaves at small distances in state space.
The Euclidean distance between the center y of the sphere and the point x(n)
at time-step n is 
. Hence, the point x(n) lies inside the sphere of radius r,
provided that
or, equivalently,
Thus,the function f(x) for the situation described here may be written in the general form
(13.69)
where q is an integer, and 	(·) is the Heaviside function, defined by
On substituting Eqs. (13.67) and (13.69) into Eq. (13.68), we get a new function
C(q, r) that depends on q and r, as shown by
(13.70)
Hence, using the sifting property of the delta function, namely,
for some function g(·) and interchanging the order of summation, we may redefine the
function C(q, r) in Eq. (13.70) as
(13.71)
C(q, r) = 1
N a
N
n=1
a
1
N - 1 a
N
k=1
kZn
	(r - 7x(n) - x(k)7) b
q-1
3
q
-q
g(y)(y - x(n))dy = g(x(n))
C(q, r) =
3
q
-q
a
1
N - 1 a
N
k=1
kZn
	(r - 7y - x(k)7) b
q-1
a 1
N a
N
n=1
(y - x(n)) bdy
	(z) = e1  for z 7 0
0  for z 6 0
f(x) = a
1
N - 1 a
N
k=1
kZn
	(r - 7y - x(k)7) b
q-1
r - 7y - x(n)7 7 0
7y - x(n)7 6 r
7y - x(n)7
f =
3
q
-q
f(y)&(y)dy
f

The function C(q, r) is called the correlation function.6 In words, it is defined as follows:
The correlation function of an attractor, denoted by C(q, r), is a measure of the probability
that any two points on the attractor, x(n) and x(k), are separated by a distance r for some
integer q.
It is assumed that the number of data points,N,in the defining equation of Eq.(13.71) is large.
The correlation function C(q, r) is an invariant of the attractor in its own right.
Nevertheless, the customary practice is to focus on the behavior of C(q, r) for small r.
This limiting behavior is described by
(13.72)
where Dq, called a fractal dimension of the attractor, is assumed to exist.Taking the log-
arithm of both sides of Eq. (13.72), we may formally define Dq as
(13.73)
However, since we usually have a finite number of data points, the radius r must be just
small enough to permit a sufficient number of points to fall inside the sphere. For a pre-
scribed q, we may then determine the fractal dimension Dq as the slope of the part of
the function C(q, r) that is linear in log r.
For q  2, the definition of the fractal dimension Dq assumes a simple form that
lends it to reliable computation. The resulting dimension D2 is called the correlation
dimension of the attractor (Grassberger and Procaccia,1983).The correlation dimension
reflects the complexity of the underlying dynamic system and bounds the degrees of
freedom required to describe the system.
Lyapunov Exponents
The Lyapunov exponents are statistical quantities that describe the uncertainty about
the future state of an attractor. More specifically, they quantify the exponential rate at
which nearby trajectories separate from each other while moving on the attractor. Let
x(0) be an initial condition and {x(n), n  0, 1, 2, ...} be the corresponding orbit of the
attractor. Consider an infinitesimal displacement from the initial condition x(0) in the
direction of a vector y(0) tangential to the orbit.Then, the evolution of the tangent vec-
tor determines the evolution of the infinitesimal displacement of the perturbed orbit
{y(n), n  0, 1, 2, ...} from the unperturbed orbit {x(n), n  0, 1, 2, ...}. In particular, the
ratio 
defines the infinitesimal displacement of the orbit from x(n), and the
ratio 
is the factor by which the infinitesimal displacement grows if
or shrinks if 
.For an initial condition x(0) and initial dis-
placement 
, the Lyapunov exponent is defined by
(13.74)
A d-dimensional chaotic process has a total of d Lyapunov exponents that can be
positive, negative, or zero. Positive Lyapunov exponents account for the instability of

(x(0), ) = lim
nS q
1
n log a
7y(n)7
7y(0)7 b
0 = y(0)7y(0)7
7y(n)7 6 7y(0)7
7 y(n)7 7 7 y(0)7
7 y(n)7 7y(0)7
y(n)7 y(n)7
Dq = lim
rS0
log C(q, r)
(q - 1)log r
C(q, r) L r(q-1)Dq
714
Chapter 13
Neurodynamics

Section 13.10
Strange Attractors and Chaos
715
an orbit throughout the state space. This condition may be stated in another way as
follows:
•
Positive Lyapunov exponents are responsible for the sensitivity of a chaotic process to ini-
tial conditions.
•
Negative Lyapunov exponents, on the other hand, govern the decay of transients in the
orbit.
•
A zero Lyapunov exponent signifies the fact that the underlying dynamics responsible for
the generation of chaos are describable by a coupled system of nonlinear differential equa-
tions: that is, the chaotic process is a flow.
A volume in d-dimensional state space behaves as exp(L(
1  
2  ... 
d)), where L
is the number of time-steps into the future. It follows, therefore, that for a dissipative
process, the sum of all Lyapunov exponents must be negative. This is a necessary con-
dition for a volume in state space to shrink as time progresses, which is a requirement
for physical realizability.
Lyapunov Dimension
Given the Lyapunov spectrum defined by the set of exponents 
1, 
2, ..., 
d, Kaplan and
Yorke (1979) proposed a Lyapunov dimension for a strange attractor, given as
(13.75)
where K is an integer that satisfies the two conditions
Ordinarily, the Lyapunov dimension DL is about the same size as the correlation dimen-
sion D2.This is an important property of a chaotic process.That is,although the Lyapunov
and correlation dimensions are defined in entirely different ways, their values for a
strange attractor are usually quite close to each other.
Definition of a Chaotic Process
Throughout this section, we have spoken of a chaotic process without providing a for-
mal definition of it. In light of what we now know about Lyapunov exponents, we can
offer the following definition:
A chaotic process is generated by a nonlinear deterministic system with at least one positive
Lyapunov exponent.
The positivity of at least one Lyapunov exponent of the attractor is a necessary condi-
tion for sensitivity to initial conditions, which is the hallmark of a strange attractor.
a
K
i=1

i 7 0 and a
K+1
i=1

i 6 0
DL = K +
a
K
i=1

i

K+1

The largest Lyapunov exponent also defines the horizon of predictability of a
chaotic process. Specifically, the short-term predictability of a chaotic process is
approximately equal to the reciprocal of the largest Lyapunov exponent (Abarbanel,
1996).
13.11
DYNAMIC RECONSTRUCTION OF A CHAOTIC PROCESS
Dynamic reconstruction may be defined as the identification of a mapping that provides
a model for an unknown dynamic system of dimensionality m. Our interest here is in the
dynamic modeling of a time series produced by a physical system that is known to be
chaotic. In other words, given a time series {y(n)}N
n  1, we wish to build a model that cap-
tures the underlying dynamics responsible for generation of the observable y(n).As we
pointed out earlier in the previous section, N denotes the sample size.The primary moti-
vation for dynamic reconstruction is to make physical sense from such a time series,
hopefully bypassing the need for a detailed mathematical knowledge of the underlying
dynamics.The system of interest is typically much too complex to characterize in math-
ematical terms.The only information available to us is contained in a time series obtained
from measurements on one of the observables of the system.
A fundamental result in dynamic-reconstruction theory7 is a geometric theorem
called the delay-embedding theorem, due to Takens (1981).Takens considered a noise-
free situation, focusing on delay coordinate maps or predictive models that are con-
structed from a time series, representing an observable from a dynamic system. In
particular,Takens showed that under certain conditions, the delay coordinate map from
a d-dimensional smooth compact manifold to 
is a diffeomorphism on that mani-
fold,where d is the dimension of the state space of the dynamic system.(Diffeomorphism
was discussed in Chapter 7.)
For an interpretation of Takens’s theorem in signal-processing terms, first consider
an unknown dynamic system whose evolution in discrete time is described by the non-
linear difference equation
(13.76)
where x(n) is the d-dimensional state of the system at time n and F(·) is a vector-valued
function. It is assumed here that the sampling period is normalized to unity. Let the time
series {y(n)} observable at the output of the system be defined in terms of the state vec-
tor x(n) as
(13.77)
where g(·) is a scalar-valued function and v(n) denotes additive noise. The noise v(n)
accounts for the combined effects of imperfections and imprecisions in the observable
y(n). Equation (13.76) and (13.77) describe the state-space behavior of the dynamic sys-
tem.According to Takens’s theorem,the geometric structure of the multivariable dynam-
ics of the system can be unfolded from the observable y(n) with v(n)  0 in a
D-dimensional space constructed from the new vector
(13.78)
yR(n) = [y(n), y(n - ), ..., y(n - (D - 1))]T
y(n) = g(x(n)) + (n)
x(n + 1) = F(x(n))
2d+1
716
Chapter 13
Neurodynamics

Section 13.11
Dynamic Reconstruction of a Chaotic Process
717
where  is a positive integer called the normalized embedding delay.That is,given the observ-
able y(n) for varying discrete time n, which pertains to a single observable (component) of
an unknown dynamic system, dynamic reconstruction is possible using the D-dimensional
vector yR(n) provided that D  2d  1, where d is the dimension of the state space of the
system.Hereafter,we refer to this statement as the delay-embedding theorem.The condition
D  2d  1 is a sufficient,but not necessary,condition for dynamic reconstruction.The pro-
cedure for finding a suitable D is called embedding, and the minimum integer D that achieves
dynamic reconstruction is called the embedding dimension; it is denoted by DE.
The delay-embedding theorem has a powerful implication: Evolution of the points
in the reconstruction space follows that of the unknown dynamics
in the original state space.That is, many important properties of the
unobservable state vector x(n) are reproduced without ambiguity in the reconstruction
space defined by yR(n). However, for this important result to be attainable, we need
reliable estimates of the embedding dimension DE and the normalized embedding delay
, as summarized here:
1. The sufficient condition D  2d  1 makes it possible to undo the intersections of an
orbit of the attractor with itself, which arise from projection of that orbit to lower
dimensions.The embedding dimension DE can be less than 2d  1.The recommended
procedure is to estimate DE directly from the observable data.A reliable method for
estimating DE is the method of false nearest neighbors, described in Abarbanel (1996).
In this method,we systematically survey the data points and their neighbors in dimen-
sion d  1, then d  2, and so on.We thereby establish the condition when apparent
neighbors stop being “unprojected” by the addition of more elements to the recon-
struction vector yR(n),and thus obtain an estimate for the embedding dimension DE.
2. Unfortunately, the delay-embedding theorem has nothing to say on the choice of
the normalized embedding delay . In fact, it permits the use of any  so long as the
available time series is infinitely long.In practice,however,we always have to work
with observable data of finite length N. The proper prescription for choosing  is
to recognize that the normalized embedding delay  should be large enough for
y(n) and y(n  ) to be essentially independent of each other so as to serve as
coordinates of the reconstruction space, but not so independent as to have no cor-
relation with each other.This requirement is best satisfied by using the particular
 for which the mutual information between y(n) and y(n  ) attains its first min-
imum (Fraser, 1989). (Mutual information was discussed in Chapter 10.)
Recursive Prediction
From the discussion presented, the dynamic-reconstruction problem may be interpreted
as one of representing the signal dynamics properly (the embedding step), as well as of
constructing a predictive mapping (the identification step).Thus, in practical terms, we
have the following network topology for dynamic modeling:
• a short-term memory (e.g.,delay-line memory) structure to perform the embedding,
whereby the reconstruction vector yR(n) is defined in terms of the observable y(n)
and its delayed versions (see Eq. (13.78));
x(n) S x(n + 1)
yR(n) S yR(n + 1)

• a multiple-input,single-output (MISO) adaptive nonlinear dynamic system trained
as a one-step predictor (e.g., neural network) to identify the unknown mapping
, which is defined by
(13.79)
The predictive mapping described in Eq. (13.79) is the centerpiece of dynamic model-
ing: Once it is determined, the evolution 
becomes known, which, in
turn, determines the unknown evolution 
.
Presently, we do not have a rigorous theory to help us decide if the nonlinear
predictor has successfully identified the unknown mapping f. In linear prediction,
minimizing the mean-square value of the prediction error may lead to an accurate model.
However, a chaotic time series is different. Two trajectories in the same attractor 
are vastly different on a sample-by-sample basis, so minimizing the mean-square
value of the prediction error is a necessary, but not sufficient, condition of a success-
ful mapping.
The dynamic invariants—namely,correlation dimension and Lyapunov exponents—
measure global properties of the attractor, so they should gauge the success of
dynamic modeling. Hence, a pragmatic approach for testing the dynamic model is to
seed it with a point on the strange attractor and to feed the output back to its input,
forming an autonomous system, as illustrated in Fig. 13.18. Such an operation is called
iterated prediction or recursive prediction. Once the initialization is completed, the out-
put of the autonomous system is a realization of the dynamic-reconstruction process.
This method, of course, presumes that the predictor has been designed properly in the
first place.
For a reliable dynamic reconstruction, we may define the reconstruction vector
yR(n) as a full m-dimensional vector
(13.80)
where m is an integer defined by
m  DE
(13.81)
This formulation of the reconstruction vector yR(n) supplies more information to the pre-
dictive model than that provided by Eq. (13.78) and may therefore yield a more accu-
rate dynamic reconstruction.However,both formulations share a common feature:Their
compositions are uniquely defined by knowledge of the embedding dimension DE. In
any event, it is wise to use the minimum permissible value of D—namely, DE—to min-
imize the effect of additive noise v(n) on the quality of dynamic reconstruction.
yR(n) = [y(n), y(n - 1), ..., y(n - m + 1)]T
x(n) S x(n + 1)
yR(n) S yR(n + 1)
yˆ (n + 1) = f(yR(n))
f:D S 1
718
Chapter 13
Neurodynamics
FIGURE 13.18
Feedback system 
used as an iterated predictor 
for the dynamic reconstruction 
of a chaotic process.
Trained
predictor
y(n)
Unit delay
y(n + 1)
ˆ
ˆ

Section 13.11
Dynamic Reconstruction of a Chaotic Process
719
Dynamic Reconstruction Is an Ill-Posed Filtering Problem
In reality, the dynamic-reconstruction problem is an ill-posed inverse problem.We say so
because it is highly likely that one or more of Hadamard’s three conditions for the well-
posedness of an inverse problem—which were formulated in Chapter 7—may be violated:
1. For some unknown reason, the existence condition may be violated.
2. There may not be sufficient information in the observable time series to reconstruct
the nonlinear dynamics uniquely; hence, the uniqueness criterion is violated.
3. The unavoidable presence of additive noise or some form of imprecision in the
observable time series adds uncertainty to the dynamic reconstruction.
In particular, if the noise level is too high, it is possible for the continuity criterion to be
violated.
How,then,do we make the dynamic-reconstruction problem well posed? The answer
lies in the inclusion of some form of prior knowledge about the input–output mapping as
an essential requirement. In other words, some form of constraints (e.g., smoothness of
input–output mapping) would have to be imposed on the predictive model designed for
solving the dynamic-reconstruction problem.One effective way in which this requirement
can be satisfied is to invoke Tikhonov’s regularization theory, which was also discussed in
Chapter 7. Simply put, without regularization, the iterated predictor may well not work.
Another issue that needs to be considered is the ability of the predictive model to solve
the inverse problem with sufficient accuracy.In this context,the use of a neural network to
build the predictive model is appropriate.In particular,the universal approximation prop-
erty of a multilayer perceptron or that of a radial-basis function network means that we can
take care of the issue of reconstruction accuracy by using one or the other of these networks
with an appropriate number of hidden neurons.In addition,however,we need the solution
to be regularized for the reasons already explained.In theory,both multilayer perceptrons
and radial-basis function networks lend themselves to the use of regularization, but as
explained in Chapter 7,it is in radial-basis function networks that we find regularization the-
ory included in a mathematically tractable manner as an integral part of their design.
Case Study: Dynamic Reconstruction of Lorenz Attractor
To illustrate the idea of dynamic reconstruction, consider the system of three coupled
ordinary differential equations, abstracted by Lorenz (1963) from the Galerkin approx-
imation to the partial differential equations of thermal convection in the lower atmos-
phere; the Lorenz attractor provides a workhorse set of equations for testing ideas in
nonlinear dynamics.The equations for the Lorenz attractor are
(13.82)
dz(t)
dt
= x(t)y(t) - bz(t)
dy(t)
dt
= - x(t)z(t) + rx(t) - y(t)
dx(t)
dt
= - x(t) + y(t)

where , r, and b are dimensionless parameters.Typical values for these parameters are 
  10, b  8/3, and r  28.
Figure 13.19 shows the results of iterated prediction performed on a regularized
RBF network with 400 centers using a “noisy” time series based on the component x(t)
of the Lorenz attractor.The signal-to-noise ratio was 25 dB. For the design of the reg-
ularized RBF network, we used the following parameters:
size of the input layer, m  20
regularization parameter 
  102
The size of the input layer was determined using Eq.(13.81);the regularization parameter

 was determined using the generalized cross-validation procedure described in Chapter 7.
The solution to the dynamic-reconstruction problem, presented in Fig. 13.19 using
a regularized form of the RBF network, has learned the dynamics, in the sense that the
output of the network under iterated prediction closely approximates the actual
trajectory of the Lorenz attractor in the short term. This outcome is borne out by the
results presented in Table 13.5, where we have a summary of Lorenz data for two cases:
(a) Noisy Lorenz system with signal-to-noise ratio SNR  25 dB.
(b) Reconstructed data, using the noisy version of Lorenz time series as described in
Table 13.5.
The invariants of the reconstructed time series using noisy data are close to the corre-
sponding ones pertaining to the noise-free Lorenz data.The deviations in absolute val-
ues are due to the residual effect of noise embedded in the reconstructed attractor and
to inaccuracies in the estimation procedure.Figure 13.19 clearly shows that there is more
to dynamic modeling than just prediction. This figure, and many others not included
720
Chapter 13
Neurodynamics
40
30
30
20
20
10
Amplitude
10
0
0
50
100
150
Time, n
200
250
300
350
400
FIGURE 13.19
Regularized iterated prediction (N  400, m  20) on Lorenz data at SNR 
25 dB; the solid curve is the actual chaotic signal, and the red curve is the reconstructed signal.

Section 13.11
Dynamic Reconstruction of a Chaotic Process
721
here, demonstrates the “robustness” of the regularized RBF solution with respect to the
point on the attractor that is used to initialize the iterated prediction process.
The following two observations from Fig. 13.19, pertaining to the use of regular-
ization, are particularly noteworthy:
1. The short-term predictability of the reconstructed time series in Fig. 13.19 is about
60 samples.The theoretical horizon of predictability computed from the Lyapunov
spectrum of the noiseless Lorenz attractor is approximately 100 samples.The exper-
imental deviation from the horizon of predictability of the noise-free Lorenz attrac-
tor is merely a manifestation of the presence of noise in the actual data used to
perform the dynamic reconstruction.The theoretical horizon of predictability com-
puted from the reconstructed data was 61 (see Table 13.5), which is quite close to
the experimentally observed value of short-term predictability.
2. Once the period of short-term predictability is over, the reconstructed time series
in Fig. 13.19 begins to deviate from the noiseless realization of the actual Lorenz
attractor.This result is basically a manifestation of chaotic dynamics—namely, sen-
sitivity to initial conditions.As mentioned previously,sensitivity to initial conditions
is a hallmark of chaos.
TABLE 13.5
Summary of Parameters for Experiment on
Dynamic Reconstruction Using the Lorenz System
(a)
Noisy Lorenz system: 25 dB SNR
Number of samples (data points) used: 35,000
1.
Normalized embedding delay   4
2.
Embedding dimension DE  5
3.
Lyapunov exponents:

5 = -47.0572

4 = -18.0082

3 = -3.1447

2 = 5.8562

1 = 13.2689
Notes: All of the Lyapunov exponents are expressed in nats per sec-
ond; a nat is a natural unit for measuring information as discussed in
Chapter 10. Note also that in case (b), dynamic reconstruction
restores the Lyapunov spectrum to its correct size of three (equal to
the number of equations) with only one positive Lyapunov exponent.
4.
Horizon of predictability
100 samples
(b)
Reconstructed system using the noisy Lorenz data of Fig. 13.19
Number of samples generated (recursively): 35,000
1.
Normalized embedding delay   4
2.
Embedding dimension DE  3
3.
Lyapunov exponents:

3 = -15.0342

2 = -0.6275

1 = 2.5655
L
4.
Horizon of predictability
61 samples
L

722
Chapter 13
Neurodynamics
13.12
SUMMARY AND DISCUSSION
The Stability Problem in Recurrent Networks
In this chapter,we presented an introductory treatment of the mathematical foundations
of deterministic neurodynamic systems, described by Eq. (13.2), reproduced here for
convenience of presentation:
where t denotes continuous-time,x(t) is the state of the system;and F(·) is a vector-valued
function, each element of which operates on a corresponding element of the state x(t).
Much of the discussion in the early part of the chapter focused on the issue of stabil-
ity of the system.In particular,we described the direct method of Lyapunov,which provides
a powerful mathematical tool for investigating the stability problem in terms of a continu-
ous scalar function of the state x(t), which is called a Lyapunov function. The method
embodies two theorems that enable us to establish whether a given autonomous nonlinear
dynamic system is stable or asymptotically stable.A cautionary word is in order here:The
method does not show us how to find a Lyapunov function;rather,the task is left to the inves-
tigator’s ingenuity to do the finding. In many problems of practical interest, however, the
energy function can serve as a Lyapunov function.
Models of Associative Memory
In the next part of the chapter, we discussed two models of an associative memory: the
Hopfield model and the brain-state-in-a-box (BSB) model. These two models share
some common points:
• they both employ positive feedback in accordance with Hebb’s postulate of learning;
• they both have an energy (Lyapunov) function, and their underlying dynamics
tend to minimize it in an iterative fashion;
• they are both capable of performing computation using attractor dynamics.
Naturally, they differ in their areas of application. The BSB model has an inherent
capability to perform clustering of data. On the other hand, the Hopfield model has a
capability of its own to operate as a content-addressable memory; however, its error-
correcting capability is not as good as that of the well-established error-correcting codes
in the digital communications literature.8 The analog version of the Hopfield network
has also been considered as a model for solving the traveling-salesman problem.9
The Hopfield Model Discussed Further
Hopfield’s 1982 paper has had a significant impact on the neural network literature.
Indeed, it was one of the catalysts responsible for reviving sustained interest in the study
of neural networks in the 1980s.
Most importantly, by doing the following in that classic paper:
• considering a recurrent neural network, artificially configured to have its synap-
tic weights satisfy the symmetry condition of Eq. (13.21);
d
dt x(t) = F(x(t))

Section 13.12
Summary and Discussion
723
• formulating an energy function E, as defined in Eq. (13.28);
• proving that the energy function E is a Lyapunov function;
• then minimizing the energy function in an iterative fashion to demonstrate that the
network is capable of exhibiting emergent behavior with a set of stable points;
and doing all of this in a relatively short paper, it made Hopfield’s paper (1982) all the
more elegant and impressive. Indeed, it was responsible for generating a great deal of
excitement among physicists and mathematicians for over a decade.
In short, Hopfield showed us that it is indeed possible for a simple, structured
behavior to emerge from the evolution of a complex, nonlinear dynamic system over time.
The possibility of this kind of dynamic behavior had been studied previously by other
investigators, but it was in Hopfield’s paper that the underpinnings of the emergent
behavior of recurrent neural networks were brought together for the first time in a vis-
ible and convincing manner.
A word of caution is in order: It would be somewhat naive to think of the Hopfield
model and other models of associative memories described in the literature on neural
networks, as useful as they are, to be applicable to human memory (Anderson, 1995).
Large-Scale Computer Model as a Facilitator of Understanding
Mammalian Brains
With modeling of some function of the brain or,to be even more ambitious,modeling the
whole brain itself,as a challenging task,it is very inspiring to refer to the pioneering work
done by Izhikevich and Edelman on the structural and dynamic complexity of mam-
malian brains. In their 2008 paper, a large-scale computer model of mammalian thalamo-
cortical systems is described. It is known that the thalamo-cortical system is essential for
onsciousness in the sense that losing a thalamus or cortex abolishes consciousness; on
the other hand, for example, losing the hippocampus or cerebellum impairs the function
of the brain, but spares consciousness. The focus on the thalamo-cortical system makes
the Izhikevich-Edelman model all the more interesting.
Major characteristics of the model include the following:
1. The simulation of one million multicompartmental spiking neurons. For the sim-
ulation, the neurons were calibrated to reproduce known responses in vitro in rats; pre-
vious work done by Izhikevich (2007a) on the spiking dynamics of neurons featured
prominently in the simulation.
2. Almost half a billion synapses. This large-scale synaptic model exhibits
autonomouly three highly relevant neural activities:
(i) Neurodynamics. The simulated spiking dynamics of each neuron and each
dendritic compartment are described by the following pair of differential
equations:
(13.83)
(13.84)
du
dt = a[b(v - vr) - u]
C dv
dt = k(v - vr)(v - vthr) - u + I

where C  membrane capacitance,
v  membrane potential,
vr  resting potential,
vthr  instantaneous threshold potential,
u  recovery variable that defines the difference between all inward
and outward voltage-gated currents,
I  dendritic and synaptic current,
and a and b are constants. When the membrane potential assumes a value
greater than the peak of the spike, the neural model fires a spike (i.e., action
potential), and all variables of the model are reset.
(ii) Short-term synaptic plasticity. In the model, the conductance (i.e., strength)
of each synapse can be scaled down or up,respectively representing depres-
sion or facilitation, on a short time-scale.
(iii) Long-term spike-timing-dependent plasticity. For this second plastic fea-
ture of the model, each synapse is potentiated or depressed, depending on
the order of firing the presynaptic neuron and the corresponding dendritic
compartment of the postsynaptic neuron.
3. Generalization performance. This performance was demonstrated by having the
model exhibit behavioral regimes of normal brain activity that were not built into the
model.
A large-scale computer model, endowed with these neurobiologically motivated per-
formances, demonstrates that we are gradually but surely moving closer to the building
of large-scale computer models of mammalian brains that will be capable of real-time
operations.
NOTES AND REFERENCES
1. A nonautonomous dynamic system is defined by the state equation
with the initial condition x(t0)  x0. For a nonautonomous system, the vector field F(x(t), t)
depends on time t. Therefore, unlike the case of an autonomous system, we generally cannot
set the initial time equal to zero (Parker and Chua, 1989).
2. In addition to Eq. (13.11), global stability of a nonlinear dynamic system generally requires
that the condition of radial unboundedness
holds (Slotine and Li, 1991).This condition is usually satisfied by Lyapunov functions con-
structed for neural networks with sigmoid activation functions.
3. For a rigorous definition of an attractor, we offer the following (Lanford, 1981; Lichtenberg
and Lieberman, 1992):
V(x) S q  as 7 x7 S q
d
dt x(t) = F(x(t), t)
724
Chapter 13
Neurodynamics

Notes and References
725
A subset (manifold)
of the state space is called an attractor if
•
is invariant under the flow;
•
there is an (open) neighborhood around
that shrinks down to
under the flow;
•
no part of
is transient;
•
cannot be decomposed into two nonoverlapping invariant pieces.
m
m
m
m
m
m
(A)
where
An action potential is generated each time the interior potential u(t) reaches a threshold value.
The action potentials are treated as Dirac delta (impulse) functions as shown by
(B)
where tk,n, n,  1, 2, 3, ..., denotes the times at which neuron k fires action potentials.These
times are defined by Eq. (A).
The behavior of total current ik(t) flowing into neuron k is modeled as
(C)
where wkj is the synaptic weight from neuron j to neuron k,  is a characteristic time con-
stant of neuron k, and the function gj(t) is defined in accordance with Eq. (B).
The additive model of Eq. (13.14) may be viewed as a special case of Eq. (C).
Specifically, the spiky nature of gj(t) is ignored by replacing it with the convolution of gj(t)
with a smoothing function. Such a move is justified if, during a reasonable time interval,
there are many contributions to the sum on the right-hand side of Eq. (C) due to high con-
nectivity, and all that we are really interested in is the short-term behavior of the firing rate
of neuron k.
5. The Little model (Little, 1974; Little and Shaw, 1978) uses the same synaptic weights as the
Hopfield model. However, they differ from each other in that the Hopfield model uses
asynchronous (serial) dynamics, whereas the Little model uses synchronous (parallel)
d
dt ik(t) = - 1
 ik(t) + a
j
wkjgj(t)
gk(t) = a
n
(t - tk,n)
u0 = potential to which the neuron is reduced when i(t) vanishes.
i(t) = electrical current injected into the neuron by another neuron,
R = leakage resistance of the membrane,
C = capacitance of the membrane surrounding the neuron,
u(t) = interior potential of the neuron,
C d
dt u(t) = - 1
R (u(t) - u0) + i(t)
4. Integrate-and-Fire Neuron
The additive model of Eq. (13.14) does not fully capture the essence of what a biological
neuron does. In particular, it ignores the timing information encoded into action potentials;
action potentials are described briefly in qualitative terms in the introductory chapter.
Hopfield (1994) describes a dynamic model that accounts for action potentials by consid-
ering an integrate-and-fire neuron. The operation of such a neuron is described by the
first-order differential equation

dynamics. Accordingly, they exhibit different convergence properties (Bruck, 1990; Goles
and Martinez, 1990):The Hopfield network will always converge to a stable state, whereas
the Little model will always converge to a stable state or a limit cycle of length two, at most.
By such a “limit cycle,” we mean that the cycles in the state space of the network are of a
length less than or equal to two.
6. The idea of a correlation function C(q, r) as defined in Eq. (13.71) was known in statistics
from the work of Rényi (1970). However, the use of it to characterize a strange attractor is
due to Grassberger and Procaccia (1983).They originally discussed the use of C(q, r) in the
context of correlation dimension for q  2.
7. The construction of dynamics using independent coordinates from a time series was first
advocated by Packard et al. (1980). However, this paper does not give proof, and it uses
“derivative” embeddings rather than time-delay embeddings. The idea of time-delay, or
delay-coordinate, embeddings is attributed to Takens. Specifically, in 1981, Takens
published a mathematically profound paper on time-delay embeddings, which applies to
attractors that are surfaces, or like a torus; see also the paper by Mañé (1981) on the
same subject published in the same issue. Takens’s paper is difficult to read for non-
mathematicians, and Mañé’s paper is even more difficult to read. The idea of delay-
coordinate mapping was refined in 1991 by Sauer et al. The approach taken in this latter
paper integrates and expands on previous results due to Whitney (1936) and Takens
(1981).
8. Spurious states disturb the retrieval phase of the Hopfield model as they tend to resem-
ble mixtures of stored patterns.Accordingly, the error-correcting capability of the Hopfield
model is degraded by the generation of superious states.The net result is that as an error-
correcting system, the Hopfield model is not that good. This is particularly so when the
Hopfield model is compared with well-established error-correcting codes in the digital
communications literature (Lin and Costello, 2004). These latter codes are impressive in
that—through the insertion of parity-check bits in accordance with cleverly formulated
encodinag schemes—they are capable of approaching the so-called Shannon limit, a chal-
lenge that has occupied the attention of coding theorists ever since Shannon’s 1948 clas-
sic paper on information theory.
9. Combinatorial-optimization problems rank among the most difficult known to mathe-
maticians. This class of optimization problems includes the traveling-salesman problem
(TSP), considered to be a classic. Given the positions of a specified number of cities,
assumed to lie in a plane, the problem is to find the shortest tour that starts and finishes
at the same city. The TSP is simple to state, but hard to solve exactly in that there is no
known method of finding the optimum tour, short of computing the length of every possible
tour and then selecting the shortest one. It is said to be NP complete (Hopcroft and
Ullman, 1979).
In a 1985 paper, Hopfield and Tank proposed the use of an analog network, based on
the system of N coupled first-order differential equations in Eq. (13.20), for representing
a solution of the TSP. Specifically, the synaptic weights of the network were determined by
distances between the cities visited on the tour, and the optimum solution to the problem
was treated as a fixed point of the equations in Eq. (13.20). Herein lie the difficulties
encountered with “mapping” combinatorial-optimization problems onto the continuous
(analog) Hopfield model.The model acts to minimize a single energy (Lyapunov) function,
performing the role of an objective function subject to some hard constraints. If any of these
constraints are violated, the solution is considered to be invalid. In Gee et al. (1993), it was
726
Chapter 13
Neurodynamics

Problems
727
shown that the success of the Hopfield model is highly sensitive to the way in which the
Lyapunov function for the system of coupled equations is constructed.
PROBLEMS
Dynamic Systems
13.1 Restate Lyapunov’s theorems for the state vector x(0) as the equilibrium state of a nonlinear
dynamic system.
13.2 Parts (a) and (b) of Fig. P13.2 are the block-diagram representations of the neurodynamic
equations described in Eqs. (13.18) and (13.19), respectively. Using this pair of equations,
verify the validity of the two block-diagrams in Fig. P13.2.
13.3 Consider a general neurodynamic system with an unspecified dependence on internal
dynamic parameters, external dynamic stimuli, and state variables.The system is defined by
the state equations
where the matrix W represents the internal dynamic parameters of the system, the vec-
tor u represents the external dynamic stimuli, and x is the state vector whose jth ele-
ment is denoted by xj. Assume that trajectories of the system converge onto point
attractors for values of W, u, and initial states x(0) in some operating region of the state
space (Pineda, 1988b). Discuss how the system described here may be used for the fol-
lowing applications:
(a) a continuous mapper, with u as input and 
as output;
(b) an autoassociative memory, with x(0) as input and 
as output.
x(q)
x(q)
dxj
dt = j(W, u, x),  j = 1, 2, ..., N
Set of nonlinearities
operating on the individual
elements of the input vector
Integrator
Matrix of
synaptic weights
Vector of
biases
(a)



I
Σ
w()
W
v(t)
∫ dt
dv
dt
( )
(b)
Set of nonlinearities
operating on the individual
elements of the input vector
Integrator
Matrix of
synaptic weights
Vector of
biases



K
Σ
w()
W
x(t)
∫ dt
dx
dt
FIGURE P13.2

Hopfield Models
13.4 Consider a Hopfield network made up of five neurons, which is required to store the fol-
lowing three fundamental memories:
(a) Evaluate the 5-by-5 synaptic-weight matrix of the network.
(b) Use asynchronous updating to demonstrate that all three fundamental memories 1, 2,
and 3 satisfy the alignment condition.
(c) Investigate the retrieval performance of the network when it is presented with a noisy
version of 1 in which the second element is reversed in polarity.
13.5 Investigate the use of synchronous updating for the retrieval performance of the Hopfield
network described in Problem 13.4.
13.6 (a) Show that
are also fundamental memories of the Hopfield network described in Problem 13.4.
How are these fundamental memories related to those of Problem 13.4?
(b) Suppose that the first element of the fundamental memory 3 in Problem 13.4 is masked
(i.e., reduced to zero). Determine the resulting pattern produced by the Hopfield net-
work. Compare this result with the original form of 3.
13.7 Consider a simple Hopfield network made up of two neurons.The synaptic-weight matrix
of the network is
The bias applied to each neuron is zero.The four possible states of the network are
(a) Demonstrate that states x2 and x4 are stable,whereas states x1 and x3 exhibit a limit cycle.
Do this demonstration using the following tools:
1. the alignment (stability) condition;
2. the energy function.
(b) What is the length of the limit cycle characterizing the states x1 and x3?
13.8 Show that the energy function of a Hopfield network may be expressed as
E = - N
2 a
M
=1
m2

 x4 = [+1, -1]T
 x3 = [-1, -1]T
 x2 = [-1, +1]T
 x1 = [+1, +1]T
W = c
0
-1
-1
0d
 3 = [+1, -1, +1, -1, -1]T
 2 = [-1, +1, +1, -1, +1]T
 1 = [-1, -1, -1, -1, -1]T
 3 = [-1, +1, -1, +1, +1]T
 2 = [+1, -1, -1, +1, -1]T
 1 = [+1, +1, +1, +1, +1]T
728
Chapter 13
Neurodynamics

Problems
729
where mv denotes overlaps defined by
where xj is the jth element of the state vector x, v, j is the jth element of the fundamental
memory v, and M is the number of fundamental memories.
13.9 It can be argued that the Hopfield network is robust with respect to perturbations, such
as synaptic noise. Demonstrate the validity of this statement, using an illustrative
example.
13.10 The Boltzmann machine, studied in Chapter 11, may be viewed as an extension of the Hop-
field network. Make up a list of the similarities and differences between these two unsu-
pervised learning systems.
Cohen–Grossberg Theorem
13.11 Consider the Lyapunov function E defined in Eq. (13.48). Show that
provided that the conditions in Eqs. (13.49) to (13.51) are satisfied.
13.12 In Section 13.9, we derived the Lyapunov function of the BSB model by applying the
Cohen–Grossberg theorem. In carrying out the derivation, we omitted some of the details
leading to Eq. (13.63). Fill in the details.
13.13 Figure P13.13 shows a plot of the nonmonotonic activation function due to Morita (1993)
discussed in Note 6 under the Notes and References. This activation function is used in
place of the hyperbolic tangent function in the construction of a Hopfield network. Is the
Cohen–Grossberg theorem applicable to the associative memory so constructed? Justify
your answer.
dE
dt  0
m = 1
N a
N
j=1
xj, j,   = 1, 2, ..., M
FIGURE P13.13
0.8
1.0
1.0
0
0.8
()

h
h



Data Representation
13.14 In Chapter 10, we described an algorithm for data representation using the idea of an opti-
mal manifold, due to Chigirev and Bialek (2005). Given a set of unlabeled data as input
applied to the algorithm, two results are produced by the algorithm as follows:
• a set of manifold points, around which the input data are clustered;
• a stochastic map, which projects the input data onto the manifold.
Using the idea of the Grassberger-Procacia correlation dimension described in Section
13.10, outline an experiment for validating the Chigirev-Bialek algorithm as a possible esti-
mator of manifold-dimensional complexity.
730
Chapter 13
Neurodynamics

ORGANIZATION OF THE CHAPTER
This chapter focuses on an issue of fundamental importance: the estimation of the hid-
den state of a dynamic system, given a set of observations.
The chapter is organized as follows:
1. The introductory section, Section 14.1, motivates interest in the study of sequential
state estimation.
2. Section 14.2 discusses the notion of a state space and the different ways of modeling it.
3. The celebrated Kalman filter is derived in Section 14.3, which is followed by treat-
ment of square-root implementation of the filter for assured numerical stability in Sec-
tion 14.4. Section 14.5 derives the extended Kalman filter for dealing with situations
where the nonlinearity is of a “mild” sort.
4. Section 14.6 discusses Bayesian filtering,which provides a unifying framework for the
state estimation of dynamic systems,at least conceptually;this filtering model includes
the Kalman filter as a special case.
5. Section 14.7 presents a description of the cubature rule for direct numerical approx-
imation of the Bayesian filter, paving the way for the description of a new filter, the
cubature Kalman filter, which builds on ideas from Kalman filter theory.
6. Section 14.8 addresses another approach for approximating the Bayesian filter;this one
is rooted in Monte Carlo simulation.In particular,a detailed treatment of particle filters
is presented. A computer experiment comparing the performance of the extended
Kalman filter and a particular form of the particle filter is presented in Section 14.9.
7. Section 14.10 discusses the role of Kalman filtering in modeling different parts of
the human brain.
The chapter concludes with a summary and discussion in Section 14.11.
14.1
INTRODUCTION
In the neurodynamic systems studied in Chapter 13, the main issue of concern was sta-
bility. In this chapter, we consider another important issue: estimation of the state of a
dynamic system,given a sequence of observations dependent on the state in some fashion.
731
C H A P T E R  1 4
Bayesian Filtering for State
Estimation of Dynamic
Systems

The observations take place in discrete time, not for mathematical convenience, but
because that is how they arise naturally.Moreover,the state is not only unknown,but also
hidden from the observer. We may therefore view the state-estimation problem as an
inverse problem.
As an illustrative example, consider a dynamically driven multiplayer perceptron
with feedback loops from one layer of the network to a preceding one (e.g., from a hid-
den layer to the input layer).The state of the network could be viewed as a vector made
up of all the synaptic weights of the network, arranged in some orderly fashion. What
we would like to do is to use sequential state-estimation theory to adjust the weight vec-
tor of the network in a supervised manner, given a training sample. This application is
discussed in detail in the next chapter. For this application, however, we need a sequen-
tial procedure for state estimation, the rationale for which is deferred to that chapter.
The first rigorous treatment of sequential state-estimation theory appeared in
Kalman’s classic paper, published in 1960. Kalman’s exposition was based on two sim-
plifying assumptions for mathematical tractability:
1. The dynamic system is entirely linear.
2. The noise processes perturbing the state of the dynamic system and the observables
are additive and Gaussian.
In making these assumptions, Kalman derived an optimal estimate of the unknown state
of the system, the computation of which was performed recursively. Within its domain
of applicability, the Kalman filter has undoubtedly withstood the test of time.
Sequential state-estimation theory remains an active area of research. Much of
this research has focused on how to deal with the practical issues of nonlinearity and non-
Gaussianity. Under one or both of these conditions, optimal estimation of the state is no
longer an option.Rather,we have to settle on the realization of an approximate estimator.
The challenge is how to derive such an estimator that is both principled and computa-
tionally efficient.
14.2
STATE-SPACE MODELS
All dynamic systems share a basic feature:the state of the system.We formally define this
feature as follows:
The state of a stochastic dynamic system is defined as the minimal amount of information
about the effects of past inputs applied to the system that is sufficient to completely describe
the future behavior of the system.
Typically, the state is not measurable directly. Rather, in an indirect manner, the
state makes its effect on the outside world measurable through a set of observables. As
such, the characterization of an unknown dynamic system is described by a state-space
model, which embodies a pair of equations:
1. The system (state) model, which,formulated as a first-order Markov chain,describes
the evolution of the state as a function of time, as shown by
(14.1)
xn+1 = an(xn, n)
732
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

Section 14.2
State-Space Models
733
where n denotes discrete time,the vector xn denotes the current value of the state,and
xn1 denotes the subsequent value of the state;the vector n denotes dynamic noise,
or process noise, and an(,) is a vectorial function of its two arguments.
2. The measurement (observation) model, which is formulated as
(14.2)
where the vector yn denotes a set of observables,the vector n denotes measurement
noise, and bn(,) denotes another vectorial function.
The subscript n in both an and bn is included to cover situations where these
two functions are time varying. For the state-space model to be of practical value,
it must closely describe the underlying physics of the system under study.
Figure 14.1 depicts a signal-flow graph representation of the state-space model
defined by Eqs. (14.1) and (14.2), and Fig. 14.2 depicts the state’s evolution across time
as a Markov chain. The time-domain representation of the model depicted in these two
figures offers certain attributes:
• mathematical and notational convenience;
• a close relationship of the model to physical reality;
• a meaningful basis of accounting for the statistical behavior of the system.
yn = bn(xn, n)
xn1
xn
an(.,.)
bn(.,.)
z1I
System model
Measurement model
yn
Observable
n
n
FIGURE 14.1
Generic state-space model of a time-varying, nonlinear dynamic system, where
z-1I denotes a block of unit-time delays.
FIGURE 14.2
Evolution of the state across time, viewed as a first-order Markov chain.
xn1
a0(., 0)
x0
x1
y1
y2
yn
a1(., 1)
an(., n)
bn(., Nn)
b2(., N2)
b1(., N1)
x2
xn
•
•
•
•
•
•
•
•
•
•
•
•

734
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Justifiably, the following assumptions are made:
1. The initial state x0 is uncorrelated with the dynamic noise n for all n.
2. The two sources of noise,n and n,are statistically independent,which means that
(14.3)
This equation is a sufficient condition for independence when n on n are jointly
Gaussian.
It is noteworthy that the Markovian model of Fig. 14.2 is fundamentally different
from the Morkovian model considered in Chapter 12, which covered dynamic program-
ming.Whereas in dynamic programming the state is directly accessible to the observer,
in sequential state estimation the state is hidden from the observer.
Statement of the Sequential State-Estimation Problem
Given an entire record of observations consisting of y1, y2, ..., yn, compute an estimate of the
hidden state xk that is optimal in some statistical sense, with the estimation being performed
in a sequential manner.
In a way, this statement embodies two systems:
• the unknown dynamic system,whose observable yn is a function of the hidden state;
• the sequential state estimator or filter, which exploits the information about the
state that is contained in the observables.
In a loose sense, we may view this problem as an “encoding–decoding” problem, with
the observables representing an encoded version of the state and the state estimate pro-
duced by the filter representing a decoded version of the observables.
In any event, the state-estimation problem is called prediction if k > n, filtering if
k  n, and smoothing if k < n. Typically, a smoother is statistically more accurate than
both the predictor and filter, as it uses more observables. On the other hand, both pre-
diction and filtering can be performed in real time, whereas smoothing cannot.
Hierarchy of State-Space Models
The mathematical difficulty of solving the state-estimation problem is highly dependent
on how the state-space model is actually described, leading to the following hierarchy
of models:
1. Linear, Gaussian model. In this model,which is the simplest of state-space mod-
els, Eqs. (14.1) and (14.2) respectively reduce to
(14.4)
and
(14.5)
where An1,n is the transition matrix from state xn to state xn1 and Bn is the measurement
matrix. The dynamic noise n and measurement noise n are both additive and assumed
to be statistically independent zero-mean Gaussian processes1 whose covariance matrices
 yn = Bnxn + n
 xn+1 = An+1,nxn + n
[nk
T] = 0   for all n and k

are respectively denoted by Q#,n and Q,n.The state-space model defined by Eqs. (14.4)
and (14.5) is indeed the model that was used by Kalman to derive his recursive filter,
which is mathematically elegant and devoid of any approximation.Kalman filters are dis-
cussed in Section 14.3.
2. Linear, non-Gaussian model. In this second model, we still use Eqs. (14.4) and
(14.5), but the dynamic noise n and measurement noise n are now assumed to be addi-
tive, statistically independent, non-Gaussian processes. The non-Gaussianity of these
two processes is therefore the only source of mathematical difficulty. In situations of
this kind, we may extend the application of the Kalman filter by using the Gaussian-
sum approximation, summarized as follows:
Any probability density function p(x) describing a multidimensional non-Gaussian vec-
tor, represented by the sample value x, can be approximated as closely as desired by the
Gaussian-sum formula
(14.6)
for some integer N and positive scalers ci, with 
. The term 
stands for a 
n(xi, i)
a
N
i=1
ci = 1
p(x) = a
N
i=1
cin(xi,i)
Section 14.2
State-Space Models
735
Gaussian (normal) density function with mean 
and covariance matrix i for i = 1, 2, ..., N.
xi
The Gaussian sum on the right-hand side of Eq. (14.6) converges uniformly to the given
probability density function pX(x) as the number of terms, N, increases and the covari-
ance matrices i approach zero for all i (Anderson and Moore, 1971). To compute the
Gaussian-sum approximation of Eq. (14.6) for a prescribed probability density function
p(x), we may, for example, use a procedure based on the expectation-maximization (EM)
algorithm;this algorithm was described in Chapter 11.Then,having computed this approx-
imation, we may use a bank of Kalman filters to solve the sequential state-estimation
problem described by a linear,non-Gaussian model (Alspach and Sorenson,1972).Note,
however, that the terms in a Gaussian-sum model tend to grow exponentially over the
course of time, which may therefore require the use of a pruning algorithm.
3. Nonlinear, Gaussian model. The third model in the hierarchy of state-space
models of increasing complexity is formulated as
(14.7)
and
(14.8)
where the dynamic noise n and measurement noise n are both assumed to be additive
and Gaussian.This is where we start to experience mathematical difficulty in solving a
sequential state-estimation problem. There are basically two radically different
approaches for computing an approximate solution to the problem:
(i) Local approximation.
In this first approach to nonlinear filtering, the nonlinear
function an() in the system model of Eq. (14.7) and the nonlinear function bn()
in the measurement model of Eq.(14.8) are approximated around localized estimates
yn = bn(xn) + n
xn+1 = an(xn) + n

of the state, whereby both equations are linearized.The stage is then set for apply-
ing the Kalman filter to compute the approximate solution.The extended Kalman
filter discussed in Section 14.5 is an example of the local-approximation approach
to nonlinear filtering.
(ii) Global approximation.
In this second approach to nonlinear filtering, the solu-
tion is formulated in a Bayesian estimation framework in such a way that difficult
interpretations inherent to the problem are made mathematically tractable.
Particle filters, discussed in Section 14.7, belong to this second approach to
nonlinear filtering.
4. Nonlinear, non-Gaussian model. This last class of state-space models is
described by Eqs. (14.1) and (14.2), where both the system model and the measurement
model are nonlinear, and the dynamic noise n and measurement noise n are not only
non-Gaussian, but may also be nonadditive. In this kind of scenario, particle filters are
currently the method of choice, but not necessarily the only method, for solving the
sequential state-estimation problem.
14.3
KALMAN FILTERS
The state-space model for the Kalman filter is defined by Eqs. (14.4) and (14.5). This
linear Gaussian model is parameterized as follows:
• the transition matrix An1, n, which is invertible;
• the measurement matrix Bn, which, in general, is a rectangular matrix;
• the Gaussian dynamic noise n, which is assumed to have zero mean and covari-
ance matrix Q#, n;
• the Gaussian measurement noise n, which is assumed to have zero mean and
covariance matrix Q,n.
All these parameters are assumed to be known.We are also given the sequence of observ-
ables 
.The requirement is to derive an estimate of the state xk that is optimized
in the minimum mean-square-error sense. We will confine the discussion to filtering for
which k = n, and one-step prediction for which k = n  1.
The Innovations Process
An insightful way of deriving this optimum estimate is to use the so-called innovations
process associated with the observable yn, which is defined by
(14.9)
where 
is the minimum mean-square-error estimate of yn, given all the observables
up to and including time n - 1. In effect, we can say the following:
The innovations process n is that part of the observable yn that is new, since the predictable
part of yn—namely,
—is completely determined by the sequence 
.
{yk}k=1
n-1
yˆnn-1
yˆnn-1
n = yn - yˆ nn-1
{yk}n
k=1
736
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

The innovations process has some important properties:
Property 1.The innovations process n associated with the observable yn is orthog-
onal to all past observables y1, y2, ..., yn-1, as shown by
(14.10)
Property 2. The innovations process consists of a sequence of random vectors that
are orthogonal to each other, as shown by
(14.11)
Property 3. There is a one-to-one correspondence between the sequence of ran-
dom vectors {y1, y2, ..., yn}, representing the observed data, and the sequence {1,
2, ..., n}, representing the innovations process, in that the one sequence may be
obtained from the other by means of linear stable operators without any loss of
information. Thus, we may write
(14.12)
In light of these properties, we now see why it is easier to work with the innova-
tions process rather than the observables themselves: In general, the observables
are correlated, whereas the corresponding elements of the innovations processes
are not.
Covariance Matrix of the Innovations Process
Starting with the initial condition x0, we may use the system model of Eq. (14.4) to
express the state at time k as
(14.13)
Equation (14.13) indicates that the state xk is a linear combination of x0 and 1, 2, ...,
k-1.
By hypothesis, the measurement noise n is uncorrelated with both the initial state
x0 and the dynamic noise i. Accordingly, postmultiplying both sides of Eq. (14.13) by
T
n and taking expectations, we obtain
(14.14)
Correspondingly, we find from the measurement equation of Eq. (14.5) that
(14.15)
and
(14.16)
Given the past observations y1, ..., yn-1, we also find from the measurement equa-
tion of Eq. (14.5) that the minimum mean-square estimate of the current observation
yn is
[ykn
T] = 0,  0  k  n
[ykk
T] = 0,  0  k  n - 1
[xkn
T] = 0,  k, n  0
xk = Ak,0 x0 + a
k-1
i=1
Ak,i i
{y1, y2, ..., yn} ∆{1,2, ..., n}
[nk
T] = 0,  1  k  n - 1
nyk
T] = 0,  1  k  n - 1
Section 14.3
Kalman Filters
737

(14.17)
where 
is the corresponding estimate of the measurement noise, given the past
ˆ nn-1
yˆnn-1 = Bnxˆnn-1 + ˆ nn-1
738
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
observations y1, ..., yn-1. The estimate 
is zero, since n is orthogonal to the past
observations in light of Eq. (14.15).We may therefore reduce Eq. (14.17) to
(14.18)
Substituting Eqs. (14.5) and (14.18) into Eq. (14.9) and then collecting terms, we obtain
(14.19)
where the new term 
is the state prediction-error vector, defined by
(14.20)
In Problem 14.1, it is shown that 
is orthogonal to both the dynamic noise n
and the measurement noise n. Then, defining the covariance matrix of the zero-mean
innovations process n as
(14.21)
and using Eq. (14.19), we may readily show that
(14.22)
where Qv,n is the covariance matrix of the measurement noise n and the new term
(14.23)
is the prediction-error covariance matrix. Equation (14.22) is our first entry into the
Kalman filtering algorithm.
Estimation of the Filtered State Using the Innovations Process: 
The predictor-corrector formula
Our next task is to derive the minimum mean-square-error estimate of the state xi at
some time i,based on the innovations process.To this end,given the innovations sequence
1, 2, ..., m, we first express the corresponding estimate of xi as the linear expansion
(14.24)
where 
is a set of matrices assuming the role of coefficients of the expansion for
time i.The state-prediction error and the innovations process satisfy the following orthog-
onality condition (see Problem 14.3):
(14.25)
Hence, substituting Eq. (14.24) into Eq. (14.25) and using the orthogonality property of
the innovations process described in Eq. (14.11), we obtain
[xik
T] = Ci,kRk
[inT
k] = 0  for k = 1, 2, ..., n
 and i  n
{Ci,k}n
k=1
xˆ in = a
n
k=1
Ci,kk
Pnn-1 = [nn-1nn-1
T
]
Rn = BnPnn-1Bn
T + Qv,n
Rn = [n n
T]
nn-1
n,n-1 = xn - xˆ nn-1
nn-1
n = Bnnn-1 + n
yˆ nn-1 = Bnxˆ nn-1
ˆ nn-1

where, as defined previously, Rk is the convariance matrix of the innovations process.
Solving this equation for the coefficient matrix Ci,k, we thus have
The use of this expression in Eq. (14.24) yields
(14.26)
For i = n, corresponding to the process of filtering, we may use Eq. (14.26) to
express the filtered estimate of the state as
(14.27)
where, in the second line, the term corresponding to k = n has been isolated from the
summation.In order to put Eq.(14.27) into an interpretable form,we first use Eq.(14.26)
to write
(14.28)
To simplify the second term in Eq. (14.27), we introduce the following definition:
(14.29)
Accordingly, we may now express the filtered estimate of the state as the recursion:
(14.30)
The two terms comprising the right-hand side of Eq. (14.30) may now be interpreted as
follows:
1. The term 
represents one-step prediction: It represents a predicted estimate
of the state xn, given all the observations up to and including time n - 1.
2. The product term Gnn represents a correction term:The innovations process n,
representing new information brought to the filtering process by the observation yn,
is multiplied by a “gain factor” Gn. For this reason, Gn is commonly referred to as
the Kalman gain, in recognition of the pioneering work done by Kalman in his
classic 1960 paper.
In light of these two insightful points, Eq. (14.30) is known as the predictor-corrector
formula in Kalman filter theory.
Computation of the Kalman Gain
In Eq. (14.30), we now have our second equation for the recursive computation of the
Kalman filter. However, for this equation to be of practical value, we need a formula for
computing the Kalman gain that befits a recursive procedure for estimating the state.
xˆ nn-1
xˆ nn = xˆ nn-1 + Gnn
Gn = [xnn
T]R-1
n
xˆ nn-1 = a
n-1
k=1
[xnk
T]Rk
-1k
 = a
n-1
k=1
[xnk
T]Rk
-1k + [xnn
T]Rn
-1n
 xˆ nn = a
n
k=1
[xnk
T]Rk
-1k
xˆ in = a
n
k=1
[xik
T]Rk
-1k
Ci,k = [xik
T]Rk
-1
Section 14.3
Kalman Filters
739

With this objective in mind, we use Eq. (14.19) to write
where, in the second line, we used the fact that the state xn and measurement noise n
are uncorrelated. Next, we note that the state-prediction error vector 
is orthogo-
nal to the state estimate 
in accordance with the principle of orthogonality. There-
fore, the expectation of the outer product of 
and 
is zero, so the expectation
is unaffected if we replace xn with 
.We may thus write
Therefore, using this formula for the expectation 
in Eq. (14.29), we may express
the Kalman gain Gn in terms of the prediction-error covariance matrix 
as
(14.31)
which is the third equation for the recursive computation of the Kalman filter.
Riccati Difference Equation for Updating the Prediction-Error
Covariance Matrix
To complete the recursive procedure for computing the Kalman filter,we need a recursive
formula to update the prediction-error covariance matrix from one iteration to the next.
To tackle this last step of the state-estimation procedure, we first replace n with
n  1 in Eq. (14.20):
Next, we find it instructive to express the predicted estimate of the state in terms of its
filtered estimate.To this end, replacing n with n  1 in Eq. (14.28) and using Eq. (14.4),
we write
(14.32)
In the third line of Eq. (14.32), we used the fact that the dynamic noise n is indepen-
dent of the observations, and therefore the expectation 
is zero; and finally, we
used the first line of the defining formula of Eq.(14.27) for the filtered estimate 
.With
the relationship of Eq. (14.32) between the predicted and filtered estimates of the state
xn at hand, we now use the formula for 
to write
n+1n
xˆ nn
[nk
T]
 = An+1,nxˆ nn
 = An+1,n a
n
k=1
[xnn
T]Rk
-1k
 = a
n
k=1
[(An+1,nxn + n)k
T]Rk
-1k
 xˆ n+1n = a
n
k=1
[xn+1k
T]Rk
-1k
n+1n = xn+1 - xˆn+1n
Gn = Pnn-1Bn
T Rn
-1
Pnn-1
[xnn
T]
 = Pnn-1Bn
T
 [xnn
T] = [nn-1T
nn-1]Bn
T
nn-1
[xnn
T]
nn-1
xˆ nn-1
xˆ nn-1
nn-1
 = [xnnn-1
T
]Bn
T
 [xnn
T] = [xn(Bnnn-1 + n)T]
740
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

(14.33)
where the state-filtering-error vector is defined by
(14.34)
Hence, recognizing that the state-filtering-error vector 
and the dynamic noise n
are uncorrelated, we may express the prediction-error covariance matrix as
(14.35)
where Q,n is the covariance matrix of the dynamic noise n. In Eq. (14.35), we have
introduced our last parameter, namely, the filtering-error covariance matrix, which is
defined by
(14.36)
To complete the recursion cycle in the Kalman filtering algorithm, we need a for-
mula for computing the filtering-error covariance matrix 
.To this end,we first use Eq.
(14.30) in Eq. (14.34), obtaining
Hence, using the definition of Eq. (14.36), we obtain
(14.37)
Next, we note that since the estimate 
is orthogonal to the innovations process n,
we have
Similarly,
By using this pair of relationships and the defining formula of Eq.(14.29) for the Kalman
gain, it is a straightforward matter to show that
Gn[nnn-1
T
] = [nn-1n
T]Gn
T = GnRnGn
T
[nnn-1
T
] = [nxn
T]
= [xnn
T]
[nn-1n
T] = [(xn - xˆ nn-1)n
T]
xˆ nn-1
= Pnn-1 - Gn[nnn-1
T
] - [nn-1n
T]Gn
T + GnRnGn
T
= [nn-1nn-1
T
] - Gn[nnn-1
T
] - [nn-1n
T]Gn
T + Gn[nn
T]Gn
T
Pnn = [(nn-1 - Gnn)(nn-1 - Gnn)T]
= nn-1 - Gnn
nn = xn - xˆ nn-1 - Gnn
Pnn
Pnn = [nnnn
T ]
= An+1,nPnnAn+1,n
T
+ Q,n
Pn+1n = [n+1nn+1n
T
]
nn
nn = xn - xˆ nn
= An+1,nnn + n
= An+1,n(xn - xˆ nn) + n
n+1n = (An+1,n xn + n) - An+1,n xˆ nn
Section 14.3
Kalman Filters
741
State xn1
¯˚˚˚˘˚˚˚˙
Predicted estimate
xˆ n+1n
¯˚˚˘˚˚˙

Accordingly, Eq. (14.37) is reduced to
Finally, using the formula of Eq. (14.31) for the Kalman gain and invoking the symmet-
ric properties of the covariance matrices Rn and 
, we write
(14.38)
Thus, the pair of equations in Eqs. (14.38) and (14.35) provides the means of updat-
ing the prediction-error covariance matrix. In particular, Eq. (14.38) is commonly
referred to as the discrete form of the Riccati equation, which is well known in control
theory.
Together with Eq. (14.32), this pair of equations completes the formulation of the
Kalman filtering algorithum.
Summary of the Kalman Filter
Table 14.1 presents a summary of the variables and parameters used to formulate the
solution of the Kalman filtering problem.The input of the filter is the sequence of observ-
ables y1, y2, ..., yn, and the output of the filter is the filtered estimate 
. The compu-
tational procedure is recursive, as summarized in Table 14.2.The summary also includes
the initial conditions needed to start the recursive computation. Note that the formula
for the innovation 
in Table 14.2 follows from Eqs. (14.9) and (14.18).
The version of the Kalman filter summarized in Table 14.2 is commonly referred
to as the covariance (Kalman) filtering algorithm.2 This terminology follows from the
fact that the algorithm propagates the covariance matrix
across one complete cycle
of the recursive computation, where 
refers to the prediction.
Pnn-1
Pnn-1
n
xˆ nn
Pnn = Pnn-1 - GnBnPnn-1
Pnn-1
Pnn = Pnn-1 - GnRnGn
T
742
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
TABLE 14.1
Summary of the Kalman Variables and Parameters
Variable
Definition
Dimension
xn
State at time n
M by 1
yn
Observation at time n
L by 1
An1,n
Invertible transition matrix from state at time n to state 
at time n1
M by M
Bn
Measurement matrix at time n
L by M
Q#,n
Covariance matrix of dynamic noise n
M by M
Q,n
Covariance matrix of measurement noise n
L by L
xˆ nn-1
Predicted estimate of the state at time n, given the observations
y1, y2, ..., yn-1
M by 1
xˆ nn
Filtered estimate of the state at time n, given the observations
y1, y2, ..., yn
M by 1
Gn
Kalman gain at time n
M by L
n
Innovations process at time n
L by 1
Rn
Covariance matrix of the innovations process n
L by L
Pnn-1
Prediction-error covariance matrix
M by M
Pnn
Filtering-error covariance matrix
M by M

Figure 14.3 depicts a signal-flow graph of the Kalman filter, where z-1 I represents
a bank of unit-time delays.This figure clearly shows that the Kalman filter is a double-
loop feedback system. One feedback loop, embodying the transition matrix An,n-1 of the
system (state) model, acts as the predictor. The second feedback loop, embodying the
matrix Bn of the measurement model, acts as the corrector. These two feedback loops
work together to generate the filtered estimate of the state xn—namely,
—in response
to the observation yn. It follows, therefore, that the Kalman filter, as depicted in Figure
14.3, is indeed a causal system in that it is capable of operating in real time. In fact, we
also have an overall feedback loop that encompasses these two feedback loops.
The Kalman gain Gn,central to the operation of the Kalman filter,varies with time n.
Thus, we say that the Kalman filter is a time-varying filter. This property holds even if the
state-space model of the original dynamic system is time invariant.
xˆ nn
Section 14.3
Kalman Filters
743
TABLE 14.2
Summary of the Kalman Filter Based on Filtered Estimate of the State
Observations = {y1, y2, ..., yn}
Known parameters
Transition matrix = An1,n
Measurement matrix = Bn
Covariance matrix of dynamic noise = Q#,n
Covariance matrix of measurement noise = Q,n
Computation: n = 1, 2, 3, ...
Gn = Pnn-1Bn
T[BnPnn-1Bn
T + Q,n]-1
n = yn - Bnxˆ nn-1
xˆ nn = xˆ nn-1 + Gnn
xˆ n+1n = An+1,nxˆ nn
Pnn = Pnn-1 - GnBnPnn-1
Pn+1n = An+1,nPnnAn+1,n
T
+ Q#,n
Initial conditions
xˆ 10 = [x1]
P1,0 = [(x1 - [x1])(x1 - [x1])T] = 0
The matrix 0 is a diagonal matrix with diagonal elements all set equal to -1, where  is a small
number.
Observation
yn
Filtered
estimate 
of the state
xn
Gn
Bn
An,n1
Gnn
n




xn|n
ˆ
xn|n1
ˆ
yn|n1
ˆ
xn1|n1
ˆ
z1I
FIGURE 14.3
Signal-flow graph of the Kalman filter, depicting it as a double-loop feedback
system.

14.4
THE DIVERGENCE PHENOMENON AND SQUARE-ROOT FILTERING
The covariance filtering algorithm summarized in Table 14.2 is prone to serious numer-
ical difficulties that are well documented in the literature (Kaminski et al., 1971; Bier-
man and Thornton, 1977).
In practice, numerical difficulties can arise in two basic ways. One way is through
numerical imprecision. To be specific, the matrix 
is computed as the difference
between two nonnegative-definite matrices, as shown in Eq. (14.38). Hence, unless the
numerical accuracy employed at every iteration of the algorithm is high enough, there
is a possibility that the matrix resulting from this computation will violate the proper-
ties of symmetry and nonnegative definiteness. But, according to Eq. (14.36),
is a
covariance matrix and must therefore be nonnegative definite.We thus have a conflict-
ing situation between theory and practice, with the result that the presence of numer-
ical inaccuracies in the computation leads to “unstable” behavior of the Kalman filter.
This undesirable behavior of the Kalman filter is commonly referred to as the divergence
phenomenon.
The divergence phenomenon may also arise in practice in another way. The
derivation of the Kalman filter is based on the linear, Gaussian state-space model,
described in Eqs. (14.4) and (14.5). Serious deviations of this model from the under-
lying physics of the dynamic system under study may also contribute to unstable
behavior of the algorithm. After all, the algorithm is driven by a real-life sequence
of observables, whereas mathematical derivation of the algorithm is based on a
hypothesized state-space model. Here, again, we have another conflicting situation
between theory and practice, which, in its own way, could lead to divergence of the
algorithm.
Given these practical relations, we may now pose the following question:
How do we overcome the divergence phenormenon so as to assure stable operation of the
Kalman filter in practice?
A practical answer to this important question is discussed next.
Square-Root Filtering
A mathematically elegant and computationally plausible method of resolving the diver-
gence problem is to use square-root filtering.Basically,in this modification of the Kalman
filter, we use numerically stable orthogonal transformations at every iteration of the
algorithm. Specifically, the matrix 
is propagated in its square-root form by applying
the Cholesky factorization, according to which we may write
(14.39)
where the term 
is reserved for a lower triangular matrix and 
is the transposed
term. In linear algebra, the Cholesky factor
is commonly referred to as the square
root of the matrix Pn,n.The very important point to note here is that the matrix product
is not likely to become indefinite,because the product of any square matrix and
its transpose is always nonnegative definite. Indeed, even in the presence of numerical
errors, the matrix conditioning of the Cholesky factor 
is generally better than that
of Pn|n itself.
Pnn
12
Pnn
12Pnn
T2
Pnn
12
Pnn
T2
Pnn
12
Pnn = Pnn
12Pnn
T2
Pnn
Pnn
Pnn
744
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

Square-Root Implementation of the Kalman Filter
A lemma in matrix algebra, called the matrix factorization lemma, is pivotal to the
derivation of square-root filtering algorithms. Consider any two L-by-M matrices X and
Y with the dimension L  M. The matrix factorization lemma states the following
(Stewart, 1973; Golub and Van Loan, 1996):
The matrix equality XXT = YYT holds if, and only if, there exists an orthogonal matrix 
 such that
(14.40)
To prove this lemma, we express the matrix product YYT as
In the last line of this equation, we invoked the defining property of the orthogonal
matrix :
The product of an orthogonal matrix with its transpose is equal to the identity matrix.
As a corollary to this property, we may equivalently write
(14.41)
That is, the inverse of an orthogonal matrix is equal to its own transpose.
With the matrix factorization lemma at our disposal, we may now proceed with the
derivation of the square-root covariance implementation of the Kalman filter.To begin,
we first use Eq. (14.31), defining the gain matrix G(n), in Eq. (14.38), obtaining
(14.42)
where the matrix Rn is itself defined by Eq. (14.22), reproduced here for convenience of
presentation:
Examining the reformulated Riccati equation of Eq. (14.42), we find that the expression
on its right-hand side consists of three distinct matrix terms:
M-by-M matrix: covariance matrix of the predicted state Pn|n-1;
L-by-M matrix: measurement matrix Bn multiplied by Pn|n-1;
L-by-L matrix: covariance matrix Rn of the innovations process.
Keeping in mind the different dimensionalities of these three matrix terms, we may
order all three of them in a compatible way in the N-by-N block matrix
(14.43)
 = £
Qv,n + BnPnn-1Bn
T
BnPnn-1
Pnn-1BT
n
Pnn-1
§
Hn = £
Rn
BnPnn-1
Pnn-1BT
n
Pnn-1
§
Rn = BnPnn-1Bn
T + Q,n
Pnn = Pnn-1 - Pnn-1Bn
TRn
-1BnPnn-1
-1 = T
 = XXT
 = XTXT
 YYT = X(X)T
Y = X
Section 14.4
The Divergence Phenomenon and Square-Root Filtering
745

where, in the second line, we inserted the formula for Rn. The size of the matrix in Eq.
(14.43), denoted by N, equals L  M. The new block matrix Hn is nonnegative-definite
by definition.We may therefore apply the Cholesky factorization to it, obtaining
(14.44)
where 
is the square root of the covariance matrix Pn|n-1 and O is a null matrix.
The matrix product on the right-hand side of Eq. (14.44) may be interpreted as the
product of matrix Xn,introduced earlier,and its transpose Xn
T.The stage is therefore set for
invoking the matrix factorization lemma, according to which the use of Eq. (14.40) yields
(14.45)
where the matrix n is an orthogonal matrix.To be more specific, n is an orthogonal
matrix that operates on Xn in such a way that the resulting matrix Yn is a lower trian-
gular matrix; that is, all the elements of Yn above its main diagonal are zero. It is because
of this action that the matrix n is also referred to as an orthogonal rotation. Invoking
the orthogonality property of n, we may expand on Eq. (14.45) by writing
(14.46)
Expanding the matrix products XnXT
n and YnYT
n and then equating corresponding terms
in the two sides of Eq. (14.46), we get three identifies:
(14.47)
(14.48)
(14.49)
The left-hand side of Eq. (14.47) is recognized as the covariance matrix Rn, which is fac-
torizable into 
.The identity in Eq.(14.47) is therefore satisfied by setting the first
unknown as follows:
(14.50)
Next, substituting this value of Y11, n into the identify in Eq. (14.48) and solving for Y21, n,
we find the second unknown:
(14.51)
In light of the definition of the Kalman gain Gn, developed previously in Eq. (14.31), we
may also express Y21,n as
(14.52)
Y21,n = GnRn
12
Y21,n = Pnn-1Bn
TRn
-T2
Y11,n = Rn
12
Rn
12Rn
T2
Pnn-1 = Y21,nY21,n
T
+ Y22,nY22,n
T
BnPnn-1 = Y11,nY21,n
T
 Q,n + BnPnn-1Bn
T = Y11,nY11,n
T
= £
Y11, n
OT
Y21, n
Y22, n
§ £
YT
11, n
YT
21, n
OT
YT
22, n
§
£
Q12
v,n
BnP12
nn-1
O
P12
nn-1
§  £
Q12
v,n
OT
P12
nn-1Bn
T
PT2
nn-1
§  
£
Q12
v,n
BnP12
nn-1
O
P12
nn-1
§  n = £
Y11, n
OT
Y21, n
Y22, n
§
Pnn-1
12
Hn = £
Q12
v,n
BnP12
nn-1
O
P12
nn-1
§ £
Q12
v,n
OT
P12
nn-1Bn
T
P12
nn-1
§
746
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Xn
¯˚˚˚˚˘˚˚˚˚˙
Yn
¯˚˚˚˘˚˚˚˙
Xn
¯˚˚˚˚˘˚˚˚˚˙
XT
n
¯˚˚˚˚˘˚˚˚˚˙
Yn
¯˚˚˚˘˚˚˚˙
YT
n
¯˚˚˘˚˚˙

Moreover, substituting the value of Y21,n given in Eq. (14.51) into Eq. (14.49), solving for
the matrix product Y22,nYT
22,n, and then using Eq. (14.42), we get
Factorizing the covariance matrix Pn,n into 
, we find the third unknown:
(14.53)
With all three nonzero submatrices of Yn determined, we may now fill in the unknowns
in Eq. (14.45), obtaining
(14.54)
In the final solution derived in Eq. (14.54), we may now distinguish between two
well-defined arrays of numbers that deserve close scrutiny:
1. Prearray. This array of numbers, on the left-hand side of Eq. (14.54), is operated
on by the orthogonal rotation n, which is designed to annihilate the submatrix
, element by element. The measurement matrix Bn and the covariance
matrix of the measurement noise, Q,n, are both given parameters.The square root
, being an old value that is being updated, is also known.Therefore, the sub-
matrices constituting the prearray are all known at time n.
2. Postarray. This second array of numbers, on the right-hand side of Eq. (14.54), is
a lower triangular matrix that results from the annihilation performed by the
orthogonal rotation on the prearray. In particular, the inclusion of the square root
in the prearray induces the generation of two useful matrices:
• the matrix Rn
1/2, representing the square root of the covariance matrix of the
innovations process n;
• the matrix product GnRn
1/2,which makes it possible to compute the Kalman gain.
One other important matrix resulting from computing the postarray is the square
root of the filtering-error covariance matrix,
.
With all of this information extracted from the postarray, we are ready to summarize the
computations involved in the square-root covariance filtering algorithm, as listed in
Table 14.3. A complete recursion cycle of the algorithm consists of the transformation
of the prearray into the postarray and the computation of updated parameters, which
are respectively listed under items 3 and 4 of the table. From this table, it is apparent that
the algorithm does indeed propagate the square root of the prediction-error covariance
matrix—namely,
.
Givens Rotations
Thus far in formulating the square-root covariance filtering algorithm, we have not paid
attention to the way in which the orthogonal matrix  is to be specified,other than to require
Pnn-1
12
Pnn
12
Q,n
12
Pnn-1
12
BnPnn-1
12
£
Q12
v, n
BnP12
n, n-1
O
P12
nn-1
§ n = £
Rn
12
OT
GnR12
n
P12
nn
§
Y22,n = Pnn
12
Pnn
12Pnn
T2
 = Pnn
 Y22,nY22,n
T
= Pnn-1 - Pnn-1Bn
TRn
-1BnPnn-1
Section 14.4
The Divergence Phenomenon and Square-Root Filtering
747

that the prearray should be transformed into a lower triangular postarray through a process
of annihilations. An elegant way of performing this process is to use Givens rotations, the
application of which proceeds in a step-by-step manner (Golub and Van Loan, 1996).
Under this procedure, the orthogonal matrix  is expressed as a product of N
orthogonal rotation components, as shown by
where we have ignored reference to discrete time n to simplify the presentation. The
characteristics of each rotation component are as follows:
1. Except for four strategic elements,the diagonal elements of k are all unity,and the
off-diagonal elements are all zero.
2. The subscript k in k refers to a pivotal point, around which the four strategic ele-
ments of k are located.As a rule, the pivotal point is always located on the main
diagonal of the prearray.
3. Two of the strategic elements of k are cosine parameters, and the remaining two
are sine parameters. To add mathematical significance to these cosine and sine
 = q
N
k=1
k
748
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
TABLE 14.3
Summary of Computations in the Square-Root Filtering Algorithm
1. Given parameters:
Transition matrix: An1,n
Measurement matrix: Bn
Covariance matrix of measurement noise: Q, n
Covariance matrix of dynamic noise: Q#, n
2. Old values of parameters to be updated:
Predicted estimate of the state:
Square root of the prediction-error covariance matrix:
3. Orthogonal rotation of the prearray into the postarray:
4. Updated parameters:
Notes:
1. Under point 4, all the matrices inside the brackets are extracted from the postarray and known
parameters.
2. In writing the updated parameters, we have made use of the corresponding computational formulas
of Table 14.2.
£P
T2
nn AT
n+1n
Q
T2
, n
§
 Pn+1n = [An+1nP12
nn
Q12
#, n]
 Pnn = Pnn
12[Pnn
12]T
 xˆ n+1n = An+1n xˆ nn
 xˆ nn = xˆ nn-1 + Gnn
 n = yn - Bnxˆ nn-1
 Gn = [GnRn
12][Rn
12]-1
£
Q12
v, n
BnP12
n, n-1
O
P12
nn-1
§n = £
Rn
12
OT
GnR12
n
P12
nn
§
Pnn-1
12
xˆ nn-1

parameters,suppose that the requirement is to annihilate the kl-th element of the pre-
array, where k refers to row and l refers to column.Then, the corresponding cosine
(diagonal) parameters 	kk and 	ll are assigned the same value,but one of the sine (off-
diagonal) parameters is assigned a negative value,as shown by the two-by-two matrix
(14.55)
All four parameters are real numbers, which is a requirement for satisfying the
constraint
(14.56)
The following example illustrates the steps involved in the transformation of a
prearray into a lower triangular postarray.
EXAMPLE 1.
Givens rotations for 3-by-3 prearray
Consider the 3-by-3 prearray X, which is to be transformed into a lower triangular 3-by-3 postar-
ray Y.The transformation will proceed in three steps.
Step 1: For this first step, we write
(14.57)
where the two zeros in the prearray follow from Eq. (14.54), and 
The requirement is to set u12 = 0, for which the following condition must hold:
Hence, by setting c2
1  s2
1 = 1 and solving for c1 and s1, we define the first orthogonal rota-
tion used in Eq. (14.57) as
(14.58)
Step 2: For this second step, we write
(14.59)
£
u11
0
u13
u21
u22
u23
u31
u32
u33
§ £
c2
0
-s2
0
1
0
s2
0
c2
§ = £
v11
0
v13
v21
v22
v23
v31
v32
v33
§
 s1 =
x12
2x11
2 + x12
2
 c1 =
x11
2x11
2 + x12
2
s1 = x12
x11
 c1
u12 = -x11s1 + x12c1
£
x11
x12
x13
0
x22
x23
0
x32
x33
§ £
c1
-s1
0
s1
c1
0
0
0
1
§ = £
u11
u12
u13
u21
u22
u23
u31
u32
u33
§
ck
2 + sk
2 = 1    for all k
c	kk
	kl
	lk
	ll
d = cck
-sk
sk
ck
d
Section 14.4
The Divergence Phenomenon and Square-Root Filtering
749
Postarray of step 1
¯˚˚˚˘˚˚˚˙
Prearray of step 1
¯˚˚˚˘˚˚˚˙
1st Givens
rotation
¯˚˚˚˘˚˚˚˙
Postarray of step 2
¯˚˚˚˘˚˚˚˙
2nd Givens
rotation
¯˚˚˚˘˚˚˚˙
Prearray of step 2
¯˚˚˚˘˚˚˚˙

where
The requirement is to set 13 = 0, for which the following condition must hold:
Hence, by setting s2
2  c2
2 = 1 and solving for s2 and c2, we define the second orthogonal
rotation used in Eq. (14.59) as
(14.60)
Step 3:
For this third and final, step, we write
(14.61)
where
The requirement is to set y23 = 0, for which the following condition must hold:
Hence, by setting s2
3  c2
3 = 1 and solving for s3 and c3, we define the third orthogonal
rotation used in Eq. (14.61) as
(14.62)
The final product of the three-step transformation is the lower triangular postarray
which is the desired result.
■
14.5
THE EXTENDED KALMAN FILTER
The Kalman filtering problem, studied in Section 14.3, addressed the state estimation of
a dynamic system described by the linear state-space model of Eqs. (14.4) and (14.5). If,
however, the dynamic system is intrinsically nonlinear, but Gaussian, as described in
Eqs. (14.7) and (14.8), we may extend the use of the Kalman filter through linearization
Y = £
y11
0
0
y21
y22
0
y31
y32
y33
§
 s3 =
23
222
2 + 23
2
 c3 =
22
222
2 + 23
2
s3 = 23
22
 c3
y23 = -22s3 + 23c3
£
v11
0
0
v21
v22
v23
v31
v32
v33
§ £
1
0
0
0
c3
-s3
0
s3
c3
§ = £
y11
0
0
y21
y22
y23
y31
y32
y33
§
 s2 =
u13
2u11
2 + u13
2
 c2 =
u11
2u11
2 + u13
2
s2 = u13
u11
c2
13 = -u11s2 + u13c2
750
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Postarray of step 3
¯˚˚˚˘˚˚˚˙
Prearray of step 3
¯˚˚˚˘˚˚˚˙
3rd Givens rotation
¯˚˚˚˘˚˚˚˙

of the nonlinear state-space model of the system.The resulting state estimator is corre-
spondingly referred to as the extended Kalman filter. Such an extension is feasible by
virtue of the fact that the Kalman filter is described in terms of difference equations in
the case of discrete-time systems.
To set the stage for derivation of the extended Kalman filter, we will first reformu-
late the equations defining the Kalman filter into a slightly different form that is more
convenient for our present discussion.
Reformulation of the Kalman Filter
We begin the reformulation by using Eqs. (14.9) and (14.18) to redefine the innovation
process as
(14.63)
Next, we make the following observation: Suppose that instead of the state equations of
Eqs. (14.4) and (14.5) used to derive the Kalman filter, we are given the following alter-
native form of the state-space model:
(14.64)
and
(14.65)
The measurement model of Eq. (14.65) is exactly the same as that of Eq. (14.5). How-
ever, the system model of Eq. (14.64) differs from that of Eq. (14.4) by virtue of the new
term n, which is assumed to be a known (i.e., non-random) vector. In this case, we read-
ily find that the Kalman filter equations apply, except for a modification of Eq. (14.32),
which now takes the form
(14.66)
The need for this modification arises in the derivation of the extended Kalman filter, to
be discussed next.
Preliminary Steps Leading to Derivation of the Extended Kalman Filter
As mentioned previously,the extended Kalman filter (EKF) is an approximate solution that
allows us to extend the Kalman filtering idea to nonlinear state-space models (Jazwinski,
1970; Maybeck, 1982). The nonlinear state-space model considered here has the form
described in Eqs.(14.7) and (14.8),reproduced here merely for convenience of presentation:
(14.67)
and
(14.68)
As before, the dynamic noise n and measurement noise n are uncorrelated zero-mean
Gaussian-noise processes with covariance matrices Q#,n and Q,n, respectively. More-
over, the nonlinear model may vary with time, as signified by the subscript n in the vec-
torial functions an() and bn()
yn = bn(xn) + n
xn+1 = an(xn) + n
xˆ n+1n = An+1,nxˆ nn + n
yn = Bnxn + n
xn+1 = An+1,nxn + n + n
n = yn - bn(xˆnn-1)
Section 14.5
The Extended Kalman Filter
751

The basic idea of the EKF is to linearize the state-space model of Eqs. (14.67) and
(14.68) at each time instant around the most recent state estimate.This particular estimate
is taken to be either a filtered-estimate or a predicted estimate, depending on which
functional is being considered in the course of linearization. Once a linearized model is
obtained, we are ready to apply the Kalman filter equations.
The approximation proceeds in two stages:
Stage I. Construction of new matrices
Through partial differentiations, the following two matrices are constructed:
(14.69)
and
(14.70)
In more specific terms, the ij-th entry of the transition matrix An1,n is equal to the par-
tial derivative of the ith component of the vector-valued functional an(x) with respect to
the jth component of x. Likewise, the ij-th entry of the measurement matrix Bn is equal
to the partial derivative of the ith component of the vector-valued function bn(x) with
respect to the jth component of x. In the former case, the derivatives are evaluated at
the filtered state 
, whereas in the latter case, the derivatives are evaluated at the pre-
dicted estimate 
.The entries of both matrices An1,n and Bn are computable, given
the availability of 
and 
.
EXAMPLE 2
Two-dimensional nonlinear model
Consider a dynamic system described by the following two-dimensional nonlinear state-space
model:
In this example, we have
and
Applying the definitions of Eqs. (14.69) and (14.70), we readily obtain
and
■
Bn = [xˆ 2,nn-1
2
   
2xˆ1,nn-1xˆ 2,nn-1]
An+1,n = £ 1
2xˆ2,nn
n - xˆ2,nn
-xˆ1,nn
§
bn(xn) = x1,nx2,n
2
an(xn) = cx1,n + x2
2,n
nx1,n - x1,n x2,n
d
 yn = x1,nx2,n
2
+ n
 cx1,n+1 
x2,n+1
d = c x1,n + x2
2,n
nx1,n - x1,n x2,n
d + c#1,n
#2,n
d
xˆ nn-1
xˆ nn
xˆ nn-1
xˆ nn
Bn =
0bn(x)
0x
`
x=xˆnn - 1
An+1,n =
0an(x)
0x
`
x=xˆnn
752
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

Stage II. Linearization of the state-space model
Once the transition matrix An1,n and the measurement matrix Bn have been con-
structed, they are used in a first-order Taylor approximation of the nonlinear function-
als an(xn) and bn(xn) around the state estimates 
and 
, respectively. Specifically,
we write
(14.71)
and
(14.72)
With the foregoing approximate expressions at hand, we may now proceed to
approximate the nonlinear state equations of Eqs. (14.64) and (14.65). The respective
results of the approximations are
(14.73)
and
(14.74)
where we have introduced two new quantities: n in the system model, and 
in the
measurement model.These two new quantities are themselves defined as
(14.75)
and
(14.76)
where
and
are the values of the given nonlinear functions an(xn)
and bn(xn) evaluated at 
and 
, respectively. If we recall that we also
xn = xˆ nn-1
xn = xˆ nn
bn(xˆ nn-1)
an(xˆ nn)
yn = yn - [bn(xˆ nn-1) - Bnxˆ nn-1]
n = an(xˆ nn) - An+1,nxˆ nn
yn
yn L Bnxn + n
xn+1 L An+1,nxn + n + n
bn(xn) L bn(xˆ nn-1) + Bn[xn - xˆ nn-1]
an(xn) L an(xˆ nn) + An+1,n[xn - xˆ nn]
xˆ nn
xˆ n+1, n
Section 14.5
The Extended Kalman Filter
753
know An1,n from Eq. (14.69), it therefore follows that the entries in the new additive
term n are all known at time n, which confirms the validity of our previous observation.
Likewise, since Bn is known from Eq. (14.70), all the entries in the second new term 
are also known at time n; we may therefore regard 
as the effective observation vector
of the linearized model at time n.
Derivation of the Extended Kalman Filter
The approximate state-space model of Eqs. (14.73) and (14.74) is a linear model of
the same mathematical form as that described in Eqs. (14.64) and (14.65), with only
one minor difference: The observation y(n) in Eq. (14.65) is replaced with the new
observation 
for the linearized model. Indeed, it is with this objective in mind that
we had previously formulated the state-space model of Eqs. (14.64) and (14.65) in the
first place.
Hence, the defining equations for the EKF follow simply by modifying the sec-
ond and fourth equations of Table 14.2 on the Kalman filter in the manner described in
Table 14.4.
yn
yn
yn

Summarizing Remarks on the Extended Kalman Filter
The extended Kalman filter is attractive for nonlinear state estimation for two reasons:
1. It builds on the framework of Kalman filter theory in a principled way.
2. It is relatively simple to understand and therefore straightforward to put into prac-
tical use, for which it has established a long track record.
However, it has two fundamental drawbacks that tend to limit its usefulness:
1. For the extended Kalman filter to function satisfactorily, the nonlinearity of the
state-space model has to be of a mild sort, so as to justify the use of the first-order
Taylor series expansion, upon which its theory is built.
2. Its derivation requires knowledge of first-order partial derivatives (i.e., the
Jacobians) of the state-space model of the nonlinear dynamic system under study;
however, for many practical applications, the computation of Jacobians is un-
desirable or simply not feasible.
To address the limitations of the extended Kalman filter, we find it instructive to
describe the Bayesian approach to state estimation in the next section.
754
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
TABLE 14.4
Summary of the Extended Kalman Filter
Input process:
Observations = {y1, y2, ..., yn}
Known parameters:
Nonlinear state vectorial function = an(xn)
Nonlinear measurement vectorial function = bn(xn)
Covariance matrix of process noise vector = Q#, n
Covariance matrix of measurement noise vector = Q, n
Computation: n = 1, 2, 3, ...
Notes:
1.
The linearized matrices An1,n and Bn are computed from their nonlinear counterparts an(xn) and
bn(xn) by using Eqs. (14.69) and (14.70), respectively.
2.
The values 
and 
are obtained by substituting the filtered state estimate 
and the
predicted state estimate 
for the state xn in the nonlinear vectorial functions an(xn) and bn(xn),
respectively.
3.
Examining the order of iterations in Table 14.4, we now see the reason for evaluating 
and Bn
in the manner described in Eqs. (14.69) and (14.70).
An+1,n
xˆ nn-1
xˆ nn
bn(xˆ nn-1)
an(xˆ nn)
 Pn+1n = An+1,nPnnAn+1,n
T
+ Q#,n
 Pnn = Pnn-1 - GnBnPnn-1
 xˆ n+1n = an(xˆ nn)
 xˆ nn = xˆ nn-1 + Gnn
 n = yn - bn(xˆ nn-1)
 Gn = Pn,n-1Bn
T[BnPn,n-1Bn
T + Q,n]-1
Initial conditions:
where 0 = -1I, and  is a small positive constant and I is the identity matrix.
 P1,0 = [(x1 - [x1])(x1 - [x1])T] = 0
 xˆ 10 = [x1]

14.6
THE BAYESIAN FILTER
The adoption of a Bayesian filter to solve the state estimation of a dynamic system, be it
linear or nonlinear,is motivated by the fact that it provides a general unifying framework
for sequential state estimation,at least in a conceptual sense,hence the title of the chapter.
Naturally, probability theory is central to the Bayesian approach to state esti-
mation. To simplify the presentation, henceforth, we use the term “distribution” to
refer to a probability density function. Moreover, referring back to the system (state)
model of Eq. (14.1) and the measurement model of Eq. (14.2), we use the following
notation3:
= predictive distribution of the state xn at the current time n, given the
entire sequence of observations up to and including yn-1.
= posterior distribution of the current state xn, given the entire sequence
of observations up to and including the current time n; this distribu-
tion is commonly referred to simply as the “posterior.”
= transition-state distribution of the current state xn, given the immedi-
ate past state xn-1; this distribution is commonly referred to as the
“transition prior” or simply “prior.”
= likelihood function of the current observation yn, given the current
state xn.
For the derivation of the Bayesian filter, the only assumption that we will make is
that the evolution of the state is Markovian; this assumption is also implicitly embodied
in the formulation of the Kalman filter and its variants discussed in preceding sections of
the chapter. Basically, the assumption embodies the combination of two conditions:
1. Given the sequence of states x0, x1, ..., xn-1, xn, the current state xn depends only on
the immediate past state xn-1 through the state-transition distribution p(xnxn-1).
The initial state x0 is distributed according to
2. The observations y1, y2, ..., yn are conditionally dependent only on the
corresponding states x1, x2, ..., xn; this assumption implies that the conditional
joint likelihood function of the observations (i.e., the joint distribution of all
the observations conditional upon all the states up to and including time n)
factors as
(14.77)
The posterior distribution p(xnYn) plays a key role in Bayesian analysis in that it
embodies the entire knowledge that we have about the state xn at time n after having
received the entire observation sequence Yn.Accordingly, p(xnYn) contains all the infor-
mation necessary for state estimation. Suppose, for example, we wish to determine the
l(y1, y2, ..., ynx1, x2, ..., xn) = q
n
i=1
l(yixi)
p(x0y0) = p(x0)
l(ynxn)
p(xnxn-1)
p(xnYn)
p(xnYn-1)
Yn             = sequence of observations, denoting {yi}n
i=1.
Section 14.6
The Bayesian Filter
755

filtered estimate of the state xn, optimized in the minimum mean-square error (MMSE)
sense; according to the Bayes estimator, 4 the desired solution is
(14.78)
Correspondingly, for an assessment of accuracy of the filtered estimate 
, we com-
pute the covariance matrix
(14.79)
With computational efficiency being a compelling practical factor, there is a strong
desire to compute the filtered estimate 
and related parameters in a recursive man-
ner. Suppose that we have the posterior distribution of the state xn-1 at time n - 1 as
p(xn-1Yn-1).Then the updated value of the posterior distribution of the state at time n
is governed by two basis time-steps:
1. Time update, which involves computing the predictive distribution of xn, given the
observations sequence Yn-1, as shown by
(14.80)
This formula is justified as follows by the basic laws of probability theory: Multi-
plication of the old posterior distribution p(xn-1Yn-1) by the prior p(xnxn-1) results
in a joint distribution of the old state xn-1 and the current state xn conditional
upon Yn-1. Integrating this joint distribution with respect to xn-1 yields the predic-
tive distribution p(xnYn-1).
2. Measurement update, which computes the updated posterior distribution p(xnYn)
by exploiting information about the current state xn that is contained in the new
observation yn. In particular, applying the well-known Bayes theorem to the pre-
dictive distribution p(xnYn - 1) yields
(14.81)
where
(14.82)
is a normalizing constant (also referred to as the partition function); it ensures that
the total volume under the multidimensional curve of the posterior distribution
p(xnYn) is unity, as it should be. The sequence of normalization constants {Zi}n
i=1
 =
3
l(ynxn)p(xnYn-1)dxn
 Zn = p(ynYn-1)
p(xnYn) =
1
Zn
 p(xnYn-1)l(ynxn)
p(xnYn-1) =
3
p(xnxn-1)p(xn-1Yn-1)dxn-1
xˆ nn
 =
3
(xn - xˆ nn)(xn - xˆ nn)Tp(xnYn)dxn
 Pnn = p[(xn - xˆ nn)(xn - xˆ nn)T]
xˆ nn
 =
3
xnp(xnYn)dxn
 xˆ nn = p[xnYn]
756
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Predictive
distribution
¯˚˘˚˙
Prior
¯˚˘˚˙
Old posterior
¯˚˚˘˚˚˙
Updated
posterior
¯˚˘˚˙
Predictive
distribution
¯˚˘˚˙
Likelihood
function
¯˘˙

produces the joint log likelihood of the corresponding sequence of observations
{Yi}N
i=1, as shown by
(14.83)
The formulas of Eqs. (14.80) through (14.83) are all consequences of the Markov-
ian assumption described previously.
The time update and measurement update are both carried out at every time-step
throughout the computation of the Bayesian model.In effect,they constitute a recursion,
or cycle, of the computation, as depicted in Fig. 14.4; the factor Zn has been left out for
convenience of presentation.
Approximate Bayesian Filtering
The Bayesian filter of Fig. 14.4 is optimal in a conceptual sense, with two interesting
properties:
1. The model operates in a recursive manner by propagating the posterior distribution
p(xnYn).
2. Knowledge of the model about the state xn, extracted from the entire observa-
tions process Yn, is completely contained in the posterior distribution p(xnYn).
With this distribution as the focus of attention, we now lay down the groundwork for our
filtering objective.To be specific,consider an arbitrary function of the state xn,denoted by
h(xn). In practical filtering applications, we are interested in the on-line estimation of sig-
nal characteristics of the function h(xn).These characteristics are embodied in the Bayes
estimator, defined by the ensemble average of the function h(xn), namely,
(14.84)
where 
is the expectation operator with respect to the posterior distribution p(xnYn)
that pertains to a dynamic system, be it linear or nonlinear.The formula of Eq. (14.84)
includes Eq.(14.78) for the filtered estimate of the state and Eq.(14.79) for the covariance
p
=
3
h(xn)p(xnYn)dxn
hn = p[h(xn)]
log(p(y1, y2, ..., yn)) = a
n
i=1
log(Zi)
Section 14.6
The Bayesian Filter
757
∫dxn1
z1
Unit-time delay
Predictive
distribution
p(xn|Yn1)
Prior
p(xn|xn1)
Old posterior
p(xn1|Yn1)
Updated
posterior
p(xn|Yn)
Likelihood
function
l(yn|xn)
FIGURE 14.4
Block diagram of the Bayesian filter,with its updated posterior 
as the output of interest.
p(xnYn)
Arbitrary
function
¯˘˙
Posterior
¯˚˘˚˙

matrix of the estimate as two special cases, illustrating the general unifying framework
of the Bayesian model. For Eq. (14.78) we have h(xn) = xn, and for Eq. (14.79) we have
where h now assumes the form of a vectorial function.
For the special case of a dynamic system described by the linear, Gaussian model
of Eqs. (14.4) and (14.5), the recursive solution of Eq. (14.84) is realized exactly through
the Kalman filter; see Problem 14.10. However, when the dynamic system is nonlinear
or non-Gaussian, or both, then the product distribution constituting the integrand of
Eq. (14.84) is no longer Gaussian, which makes computation of the optimal Bayes
estimator 
a difficult proposition. In situations of this latter kind, we have no option
but to abandon the notion of optimality in the Bayesian sense and seek an approximate
estimator that is computationally feasible.
In light of this practical reality, we are now ready to formally state our nonlinear-
filtering objective:
Given the entire observations sequence Yn at time n pertaining to the nonlinear state-space
model of Eqs. (14.7) and (14.8), derive an approximate realization of the Bayes estimator
, defined in Eq. (14.84), that is subject to two practical requirements:
1. computational plausibility;
2. recursive implementability.
Suboptimal solutions of the nonlinear-filtering problem, obtained by approximat-
ing the Bayesian filter, may be derived via one of two routes, depending on the way in
which the approximation is made:
1. Direct Numerical Approximation of the Posterior. The rationale behind this direct
approach to nonlinear filtering is summed up as follows:
In general, it is easier to approximate the posterior distribution p(xnYn) directly and in a local
sense than it is to approximate the nonlinear function characterizing the system (state) model
of the filter.
To be specific, the posterior distribution p(xnYn) is approximated locally around
the point 
, where 
is the filtered estimate of the state xn, given all the
observables up to and including time n; the emphasis on locality makes the design
of the filter computationally simple and fast to execute. The objective of the
approximation is to facilitate the subsequent application of Kalman filter theory.
In fact, the widely used extended Kalman filter is an example of approximate
Bayesian filtering via the direct use of numerical methods. Most importantly, in
Section 14.7, we describe a new approximate Bayesian filter called the cubature
Kalman filter, which is much more powerful than the extended Kalman filter.
2. Indirect Numerical Approximation of the Posterior. The rationale behind this sec-
ond approach to nonlinear filtering is summed up as follows:
The posterior distribution p(xnYn) is approximated indirectly and in a global sense through
the use of Monte Carlo simulation, so as to make the Bayesian framework for nonlinear fil-
tering computationally tractable.
xˆ nn
xn = xˆ nn
h(xn)
hn
h(xn) = (xn - xˆ nn)(xn - xˆ nn)T
758
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

Particle filters, to be discussed in Section 14.8, are a popular example of this sec-
ond approach to nonlinear filtering. To be more specific, particle filters rely on a
technique called the sequential Monte Carlo (SMC) method,which uses a set of ran-
domly chosen samples with associated weights to approximate the posterior dis-
tribution p(xnYn).As the number of samples used in the simulation becomes larger,
the Monte Carlo computation of the posterior distribution becomes more accurate,
which is a desirable objective. However, the increased number of samples makes
the use of the SMC method computationally more expensive. In other words, com-
putational cost is traded for improved filtering accuracy.
From this brief discussion, it is apparent that the locally direct approach to
approximate Bayesian filtering builds on Kalman filter theory, whereas the glob-
ally indirect approach charts a path of its own by departing from that theory. Gen-
erally speaking, the globally indirect approach to nonlinear filtering is more
demanding in computational terms than the locally direct approach.
14.7
CUBATURE KALMAN FILTER: BUILDING ON THE KALMAN FILTER
By now, we know that the Bayesian filter is rendered computationally tractable when
all conditional distributions are assumed to be Gaussian. In this special case, approxima-
tion of the Bayesian filter reduces to computing multidimensional integrals of a special
form described as
Specifically, given an arbitrary nonlinear function f(x) of the vector 
and using
a Gaussian function, we consider an integral of the form
(14.85)
which is defined in the Cartesian coordinate system. For the numerical approximation
of the nonlinear function h(f), we propose the use of a third-degree spherical–radial
cubature rule (Stroud, 1971; Cools, 1997). The cubature rule is constructed by forcing
cubature points to obey some form of symmetry. In so doing, the complexity in solving
a set of nonlinear equations for a set of desired weights and cubature points is reduced
markedly. Before going into detail about the cubature rule, we introduce a number of
notations and definitions:
• Using d to denote the region of integration,we say that the weighting function w(x)
defined on d is fully symmetric if the following two conditions hold:
1.
implies 
, where y is any point obtainable from x by permuta-
tions and changes of sign of the coordinates of x.
2. w(x) = w(y) on d.
• In a fully symmetric region, we call a point u a generator if 
, where 
.
ui  ui+i 7 0  for i = 1, 2, ..., (r - 1)
0, ..., 0)  M
u = (u1, u2, ...., ur,
y  d
x  d
h(f) =
3Mf(x)exp(-xTx)dx
x  M
(nonlinear function)  (Gaussian function)
Section 14.7
Cubature Kalman Filter: Building on the Kalman Filter
759
Arbitrary
function
¯˘˙
Gaussian
function
¯˚˘˚˙

• We use the notation [u1, u2, ..., ur] to represent the complete set of points that can
be obtained by permuting and changing the signs of the generator u in all possi-
ble ways. For the sake of brevity, we suppress the (n - r) zero nodes in the nota-
tion. For example [1]
represents the following set of points:
• We use the notation [u1, u2, ..., ur]i to denote the ith point from the generator u.
Converting to Spherical–Radial Integration
The key step in this conversion is a change of variables from the Cartesian vector 
to a spherical–radial vector defined by a radius r and direction vector z, as outlined here:
Let x = rz with zTz = 1, so that xTx = r2 for 
.
Then the integral of Eq. (14.85) can be rewritten in a “spherical–radial” coordinate sys-
tem as shown by the double integral
(14.86)
where 
is the region defined by 
, and () is the spherical surface
measure on 
in the integral
(14.87)
The integral of Eq. (14.87) is computed numerically by the spherical rule.Then, having
computed S(r), we find that the radial integral
(14.88)
is computed numerically by using the Gaussian quadrature.With the calculation of h,the
computation of Eq. (14.85) is accomplished. Both of these rules are described next, in
that order.
Spherical Rule
We first derive a third-degree spherical rule that takes the form
(14.89)
The rule in Eq. (14.89) entails a total of 2M cubature points from the generator [u]; the
cubature points are located at the intersections of an M-dimensional sphere and its axes.
To find the unknown parameters u and w, it suffices to consider monomials f(z) = 1 and
f(z) = z1
2 due to the fully symmetric generators, given as
3uM
f(z)d(z) L w a
2M
i=1
f[u]i
h =
3
q
0
S(r)rM-1 exp(-r2)dr
S(r) =
3uM
f(rz)d(z)
uM
uM = {z; zTz = 1}
uM
h(f) =
3
q
0
3uM
f(rz)rM-1 exp(-r2)d(z)dr
r  [0, q)
x  M
e a 1
0 b, a0
1 b, a-1
0 b, a 0
-1 b f
= 2
760
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

(14.90)
(14.91)
where M is the dimension of the vector x, and the surface area of the unit hypersphere
is defined by
where
is the gamma function. Given AM as just defined, solving Eqs. (14.90) and (14.91) for w
and u yields
Radial Rule
For the radial rule, we propose the use of a Gaussian quadrature, which is known to be
the most efficient numerical method for computing an integral in a single dimension.
An m-point Gaussian quadrature is exact up to polynomials of degree (2M - 1) and
constructed as
(14.92)
where w(x) denotes a weighting function (Press et al., 1988). Respectively, the xi and
the wi are quadrature points and associated weights to be determined. Comparison of the
integrals in Eqs (14.88) and (14.92) yields the weighting function and the region of inte-
gration to be w(x) = xM-1 exp(- x2) and 
respectively.Thus, using t = x2 as a final
change of variables, we obtain the desired radial integral
(14.93)
where 
. The integral on the right-hand side of Eq. (14.93) is now in the
form of the well-known generalized Gauss–Laguerre formula (Stroud, 1966; Press and
Teukolsky, 1990).
A first-degree Gauss–Laguerre rule is exact for 
. Correspondingly, the
rule is exact for f(x) = 1, x2; it is not exact for odd-degree polynomials, such as that in
f(x) = x, x3. Fortunately, when the radial rule is combined with the spherical rule to
compute the integral Eq. (14.85), the resulting spherical–radial rule vanishes for all
odd-degree polynomials. The reason for this nice result is that the spherical rule van-
ishes by virtue of the symmetry for any odd-degree polynomial; see Eq. (14.86). Hence,
the spherical–radial rule for computing Eq.(14.85) is exact for all odd-degree polynomials.
f
~(t) = 1, t
f
~(t) = f(2t)
3
q
0
f(x)xM-1 exp(-x2)dx = 1
2 3
q
0
 f
~(t)t(M2)-1 exp(-t)dt
[0, q),
3d
 f(x)w(x)dx L a
m
i=1
wif(xi)
w = AM
2M  and  u2 = 1
(M) =
3
q
0
xM-1 exp(-x)dx
AM = 22M
(M2)
 f(z) = z1
2:    2wu2 =
3uM
z1
2d(z) = AM
M
 f(z) = 1:    2Mw =
3uM 
d(z) = AM
Section 14.7
Cubature Kalman Filter: Building on the Kalman Filter
761

Following this argument, for a spherical–radial rule to be exact for all third-degree poly-
nomials in 
, it suffices to consider the first-degree generalized Gauss–Laguerre
rule, which entails the use of a single point and a single weight.We may thus write
where
Spherical–Radial Rule
In this final subsection,we describe two useful results that are used to (i) combine the spher-
ical and radial rules, and (ii) extend the spherical–radial rule for a Gaussian-weighted inte-
gral.The respective results are presented as two theorems (Arasaratnam and Haykin,2009):
Theorem 1:
Let the radial integral be computed numerically by an mr-point Gaussian quadra-
ture rule:
Let the spherical integral be computed numerically by an ms-point spherical rule:
Then, an (ms  mr)-point spherical–radial cubature rule is approximately given by the double
summation
Theorem 2:
Let two weighting functions be denoted by w1(x) = exp(-xTx) and w2(x) = n(x;
, ), where, for a given vector x, the term 
denotes a Gaussian distribution with mean
 and covariance matrix . Then, for every square-root matrix 1/2 such that 1/2 T/2 = /, we have
n(x; , )
3
Mf(x)exp(-xTx)dx L a
ms
j=1 a
mr
i=1
aibj f(risj)
3uM
f(rs)d(s) = a
ms
j=1
bj f(rsj)
3
q
0
f(r)rM-1 exp(-r2)dr = a
mr
i=1
aif(ri)
w1 = 1
2  a M
2 b  and  x1 = 2M2
3
q
0
f(x)xM-1 exp(-x2)dx L w1 f(x1)
x  RM
762
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
For the third-degree spherical–radial rule, mr = 1 and ms = 2M. Accordingly, we
require a total of only 2M cubature points. Moreover, the rule is exact for integrands that
can be written as a linear combination of polynomials of degree up to three and all other
odd-degree polynomials.Invoking Theorems 1 and 2,we may now extend this third-degree
spherical–radial rule to numerically compute the standard Gaussian-weighted integral
(14.94)
hN(f) =
3M
 f(x)n(x; 0, I)dx L a
m
i=1
wi f(i)
3M f(x)w2(x)dx =
1
2M 
3M f(22x + )w1(x)dx

where
In effect, the i are the cubature-point representations of the M-dimensional vector x.
Derivation of the Cubature Kalman Filter
The formula of Eq.(14.94) is the cubature rule we have been seeking for the numerical approx-
imation of the moment integral of Eq.(14.85).Indeed,the cubature rule is central to the com-
putation of all the integrals contained in the Bayesian framework for nonlinear filtering.As
with the extended Kalman filter, we assume that the dynamic noise n and measurement
noise n are jointly Gaussian.This assumption may be justified on the following grounds:
1. From a mathematical perspective, Gaussian processes are simple and mathemat-
ically easy to handle.
2. Noise processes encountered in many real-world problems may be modeled as
Gaussian processes, due to the central limit theorem of probability theory.
Under the assumption of Gaussianity, we may now approximate the Bayesian filter by
using the cubature rule as follows:
1. Time update. Suppose that the prior distribution p(xn-1Yn-1) is approximated by
a Gaussian distribution whose mean is 
n-1n-1 and whose covariance matrix is
equal to the filtering-error covariance matrix Pn-1n-1.Then, using the formula for
the Bayes estimator, we may express the predicted estimate of the state as
(14.95)
where we have used knowledge of the system model of Eq. (14.7) and the fact that
the dynamic noise n-1 is uncorrelated with the sequence of observations Yn-1.
Similarly, we obtain the prediction-error covariance matrix
(14.96)
2. Measurement update. Equation (14.95) is an approximate formula for the time up-
date. Next, to find a formula for the measurement update, suppose that the joint
distribution of the state xn and the observation yn, conditional on the sequence
Yn-1, is also Gaussian, as shown by
(14.97)
n = a cxn
yn
d; c xˆ nn-1
yˆ nn-1
d, cPnn-1
Pxy, nn-1
Pyx, nn-1
Pyy, nn-1
d b
- xˆ n∑n-1xˆ n∑n-1
T
+ Qw,n
Pn@n-1 =
3M a(xn-1) aT(xn-1)nAxn-1; xˆ n-1∑n-1, Pn-1, n-1)dxn-1
=
3M a(xn-1)n(xn-1; xˆ n-1n-1, Pn-1n-1)dxn-1
xˆ nn-1 = [xnYn-1]
xˆ
 i =    m
2  [1]i and wi = 1
m,  i = 1, 2, ..., m = 2M
Section 14.7
Cubature Kalman Filter: Building on the Kalman Filter
763
Nonlinear
state-
transition
function
¯˚˘˚˙
Gaussian distribution
¯˚˚˚˚˚˚˘˚˚˚˚˚˚˙
Joint
variables
¯˘˙
Joint covariance matrix
¯˚˚˚˚˚˘˚˚˚˚˚˙
Joint
mean
¯˚˘˚˙
6

where 
is defined in Eq. (14.95) and 
is the predicted estimate of the
observation yn given the sequence Yn-1, as shown by
(14.98)
The innovations covariance matrix is defined by
(14.99)
Lastly,the cross-covariance matrix of the state xn and the observation yn is given by
(14.100)
The five integral formulas of Eqs. (14.95), (14.96) and (14.98) through (14.100) address
different aspects of approximating the Bayesian filter. However, as different as these
formulas are, their integrands have a common form: the product of a nonlinear function
and a corresponding Gaussian function of known mean and covariance matrix. There-
fore, all five integrals lend themselves to approximation by means of the cubature rule.
Most importantly, recursive computation of the filtered estimate of the state builds
on linear Kalman filter theory by proceeding as follows:
• The Kalman gain is computed as
(14.101)
where P-1
yy, n n-1 is the inverse of the covariance matrix 
.
• Upon receipt of the new observation yn, the filtered estimate of the state xn is com-
puted in accordance with the predictor-corrector formula:
(14.102)
• Correspondingly, the covariance matrix of the filtered state-estimation error is
computed as follows:
(14.103)
Pnn = Pnn-1 - GnPyy, nn-1Gn
T
xˆ nn = xˆ nn-1 + Gn(yn - yˆ nn-1)
Pyy, nn-1

Gn = Pxy, nn-1Pyy, nn-1
-1
 =
3M xnbT(xn)n(xn; xˆ nn-1, Pnn-1)dxn - xˆ nn-1yˆ T
nn-1
 Pxy, nn-1 = PT
yx, nn-1
Pyy, nn-1 =
3M b(xn)bT(xn)n(xn; xˆ nn-1, Pnn-1)dxn - yˆ nn-1yˆ T
nn-1 + Qv, n
yˆ nn-1 =
3M b(xn)n(xn; xˆ nn-1, Pnn-1)dxn
yˆnn-1
xˆ nn-1
764
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Outer product
of the
nonlinear
measurement
function with
itself
¯˚˚˘˚˚˙
Gaussian distribution
¯˚˚˚˚˘˚˚˚˚˙
Outer product
of the
estimate 
with itself
yˆ nn-1
¯˚˚˘˚˚˙
Covariance
matrix of 
measurement
noise
¯˘˙
Outer
product 
of xn with
b(xn)
¯˚˘˚˙
Gaussian distribution
¯˚˚˚˚˘˚˚˚˚˙
Outer product
of the estimates
and yˆ nn-1
xˆ nn-1
¯˚˚˘˚˚˙
Updated
estimate
¯˘˙
Old
estimate
¯˘˙
Kalman
gain
¯˘˙
Innovations
process
¯˚˚˘˚˚˙
Nonlinear
measurement
function
¯˘˙
Gaussian distribution
¯˚˚˚˚˘˚˚˚˚˙

Note the correspondences between Eqs. (14.101), (14.102), and (14.103) for the new
nonlinear filter and Eqs. (14.31), (14.30), and the unnumbered equation preceding
(14.38) for the Kalman filter, respectively. In any event, the posterior distribution may
finally be computed as a Gaussian distribution defined by
(14.104)
where the mean 
is defined by Eq. (14.102) and the covariance matrix Pnn is defined
by Eq. (14.103).
Thus, having started the computation with the prior distribution p(xn-1Yn-1) under
the time update, the recursion cycle has moved forward systematically through the mea-
surement update, culminating in the computation of the posterior distribution p(xnYn);
the cycle may then be repeated as required.
For obvious reasons, this new nonlinear filter is called the cubature Kalman filter
(Arasaratnam and Haykin, 2009). Important properties of this new nonlinear filter are
summarized as follows:
1. The cubature Kalman filter (CKF) is a derivative-free on-line sequential-state estimator.
2. The approximations of the moment integrals resulting from the use of the cuba-
ture rule are all linear in the number of function evaluations. Moreover, the points
and associated weights in the cubature rule are independent of the nonlinear func-
tion f(x) in Eq. (14.84); hence, they can be computed off-line and stored to speed
up the filtering execution.
3. As with the EKF, the computational complexity of the CKF, measured in terms of
flops, grows as M 3, where M is the dimension of the state space.
4. In a principled way, the CKF builds on the Kalman filter theory, including the use
of square-root filtering for the attainment of improved numerical accuracy; the
resulting filter is called the square-root cubature Kalman filter (SCKF),which prop-
agates the square roots of the predictive and posterior error covariance matrices
(Arasaratnam and Haykin, 2009).
5. Most importantly, the second-order moments in the prior distribution are com-
pletely preserved in the posterior distribution.Since the information we have about
the state is actually contained in the observations,we may go on to say that the CKF
completely preserves second-order information about the state that is contained
in the sequence of observations, thereby outperforming the EKF in terms of accu-
racy and reliability.
6. The CKF is the closest known direct approximation to the Bayesian filter,in that it eases
the curse-of-dimensionality problem the most but, by itself, does not overcome it.
It is the combination of these properties that makes the cubature Kalman filter an
attractive choice for the supervised training of recurrent multilayer perceptrons, as dis-
cussed in Chapter 15.In that chapter,we also present a computer experiment that clearly
demonstrates the practicality of this new powerful tool.
14.8
PARTICLE FILTERS
In this section, we continue the discussion on nonlinear filtering by describing the
indirect global approximation of the Bayesian filter. Much, if not all, of the underly-
ing theory involved in this second approach to nonlinear filtering resides in the literature
xˆ nn
p(xnYn) = n(xn; xˆ nn, Pnn)
Section 14.8
Particle Filters
765

on Monte Carlo statistical methods (Robert and Casella, 2004).This new class of nonlin-
ear filter is best exemplified by particle filters. Most importantly, particle filters have
become an important tool for solving nonlinear filtering problems because of their gen-
eral applicability in a variety of fields such as signal processing,tracking of targets in radar
and acoustic media, computer vision, and neural computation, just to name a few.
Before going into a detailed description of particle filters, we introduce some new
notations and definitions. Let Xn denote the sequence of all target states 
. As
before, Yn denotes the sequence of all observations 
. Correspondingly, we may
express the joint posterior distribution of all the states, Xn, given the sequence of obser-
vations Yn as p(XnYn). Since the sequence of states represented by Xn is hidden from
the observer, it is not usually feasible to obtain random samples directly from the pos-
terior distribution p(XnYn) for computing the integral of Eq. (14.84).To get around this
practical difficulty, we sample from another distribution called the instrumental, or
importance, distribution. Henceforth, this new distribution is denoted by q(XnYn).
Naturally, for the importance distribution to be an effective replacement for the poste-
rior distribution, q(XnYn) must have a support broad enough to completely include the
support of p(XnYn).
Monte Carlo Integration
Following the so-called method of importance sampling, we randomly draw a set of N
statistically independent and identically distributed (iid) samples from the importance
distribution q(XnYn). Let the randomly drawn samples at time n be denoted by x n
(i), i =
1, 2, ..., N. Starting from time 0 and moving forward to time n, step by step, the N sam-
ples trace individual “trajectories” of their own in the state space in accordance with
the importance distribution q(XnYn).These trajectories, denoted by Xn
(i), where i = 1, 2,
..., N, are called particles—hence the name “particle filtering.”
Next, we define the importance function
(14.105)
Then, using this definition in Eq. (14.84), we may reformulate the Bayes estimator as
(14.106)
where we have used h(Xn) as the arbitrary function in order to be consistent with the
particle-filtering terminology.
Applying the method of importance sampling to the Bayes estimator of Eq.(14.106),
we obtain the corresponding Monte Carlo estimator
(14.107)
hˆ n(N) L 1
N a
N
i=1
w
~
n
(i)h(Xn
(i))
 =
3
h(Xn)r(XnYn)q(XnYn)dxn
 hn =
3
h(Xn) a
p(XnYn)
q(XnYn) bq(XnYn)dxn
r(XnYn) =
p(XnYn)
q(XnYn)
{yi}n
i=1
{xi}n
i=1
766
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

where the 
are importance weights, defined by
(14.108)
To ensure that the Monte Carlo estimator
does not need to know the normaliz-
ˆhn(N)
 =
p(Xn
(i)Yn)
q(Xn
(i)Yn),    i = 1, 2, ...., N
 w
~
n
(i) = r(Xn
(i)Yn)
w
~
n
(i)
Section 14.8
Particle Filters
767
ing constant of the distribution p(X n
(i)Yn),which might be troublesome or perhaps impos-
sible to compute, it is customary to normalize the importance weights so that they sum
to unity.To this end, we reformulate the estimator of Eq. (14.107) as
(14.109)
where
(14.110)
are normalized importance weights.
For a finite number of particles, N, the estimator 
is “biased.” But, in an
asymptotic sense, we find that the following is the case (Doucet et al., 2001):
(14.111)
To improve on the method of importance sampling, we may follow it up with a
second stage of resampling, as in the sampling–importance-resampling (SIR) method,
due to Rubin (1988). In the first stage of the SIR method, a set of iid samples {X(i)
n }N
i=1 is
randomly drawn from the importance distribution q(XnYn) at iteration n in the usual
way, and the corresponding set of normalized importance weights {wn
(i)}N
i=1 is then com-
puted in accordance with Eq. (14.110). In the second stage of the SIR method, a sec-
lim
NS qhˆn(N) S hn
ˆhn(N)
wn
(i) =
w
~
n
(i)
a
N
j=1
w
~
n
(j)
,    i = 1, 2, p, N
hˆ n(N) L a
N
i=1
wn
(i)h(Xn
(i))
ond set of samples, denoted by 
, is drawn from the intermediate set {Xn
(i)}N
i=1,
taking into account the relative strengths of the normalized importance weights wn
(i); in
effect,each of these weights is viewed as a probability of occurrence of the pertinent sam-
ple.The rationale behind the second stage of sampling is as follows:
A sample 
picked in the second stage of resampling, for which the normalized importance
weight wn
(i) is large, is most likely to be under the joint posterior distribution
p(XnYn); such a sample should therefore be selected with higher probability than a sample
for which the normalized importance weight is small.
There are several ways of implementing SIR. In one particular method described
in Cappé et al. (2005), at each iteration we proceed as follows:
1. Sampling. Randomly draw an iid set of N samples {X(i)}N
i=1 from the importance dis-
tribution q(XY).
2. Weighting. Using Eq. (14.110), compute the corresponding set of normalized
weights {w(i)}N
i=1.
X (i)
n
{X
 (i)
n }M
i=1

3. Resampling.
(i) Given the intermediate samples X(1), X(2), ..., X(N), conditionally and indepen-
dently draw a set of L discrete random variables {I(1),I(2),...,I(L)} that take val-
ues in the set {1, 2, ..., N} with probabilities (w(1), w(2), ..., w(N)) as shown by,
for example,
P(I(1) = j) = w (j)
for j = 1, 2, ..., N
and so on for I(2), ..., I(L); typically, we have L  N.
(ii) Set 
for i = 1, 2, ..., L
The set {I(1), I(2), ..., I(L)} is recognized as a multinomial trial process.Accordingly, the SIR
method just described is said to be of a multinomial kind, which is illustrated in Fig. 14.5
for the example case of L = N = 6.
Later on in this section, we will discuss the role of resampling in overcoming the
effects of a problem known as “degeneracy”of the importance weights.However,the use
of resampling introduces some practical limitations of its own:
1. Resampling limits the scope of parallel implementation of particle filters, due to
the very nature of the process.
2. Particles associated with large importance weights are selected several times in
the course of resampling, which results in a loss of diversity among the particles;
this phenomenon is referred to as sample improvishment or weight degeneracy.
When, for example, the dynamic noise in the state-space model is relatively small,
all the particles may end up collapsing to a single one in a matter of few iterations,
which is obviously undesirable.
3. Invariably, resampling increases the variance of the Monte Carlo estimator.
Sequential Importance Sampling
The Monte Carlo estimator 
of Eq. (14.109), produced by the method of impor-
tance sampling, provides a computationally viable solution for approximating the
ˆhn(N)
X
 (i) = X(Ii)
768
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Importance (instrumental)
distribution
Target (posterior)
distribution
Resampled Particles
FIGURE 14.5
Illustration of the resampling process for the example of the number of samples
and resamples being equal to six.

Bayesian estimator
of the arbitrary function h(Xn), thereby fulfilling the first practi-
cal requirement of our nonlinear filtering objective, stated previously on page 758. How-
ever, we still have to fulfill the second requirement: recursive implementation of the
Monte Carlo estimator.
Unfortunately, the method of importance sampling in its simple form does not
cater to recursive computation. This is because we require the entire observation
sequence denoted by Yn before we are in a position to estimate the posterior distribu-
tion p(XnYn). In particular, as each new observation yn becomes available, we need to
compute the importance weights 
over the entire state sequence Xn.To satisfy
this need,the computational complexity of the importance-sampling process would have
to continue increasing with time n, which is impractical for obvious reasons. To over-
come this computational difficulty, we resort to a sequential implementation of impor-
tance sampling, commonly referred to as sequential-importance-sampling (SIS).
To describe the underlying theory of the SIS procedure,we first use the time update
of Eq.(14.80) and the measurement update of Eq.(14.81) to eliminate the predictive dis-
tribution, where we now write p(XnYn-1) and p(Xn-1Yn-1) in place of p(xnYn-1) and
p(xn-1Yn-1), respectively, so as to fit into the particle-filtering terminology. We thus
obtain
(14.112)
In the first line of this equation, we moved the likelihood function l(ynxn) inside the
integral, since, under the Markovian assumption, it is independent of the previous
value of the state—namely, xn-1; in the second line of the equation, we introduced the
importance distribution q(Xn|Yn). In the importance-sampling framework, the multiple-
product term
accounts for the importance weights associated with the importance distribution
q(Xn|Yn) at time n. Specifically, with Zn being a constant, we may write
(14.113)
where r denotes proportionality.
Suppose now the importance distribution is chosen in such a way that in the
denominator of Eq. (14.113), the factorization
(14.114)
q(Xn
(i)Yn) = q(Xn-1
(i) Yn-1)q(xn
(i)Xn-1
(i) , yn)
wn
(i) r
p(xn
(i)Xn-1
(i) )l(ynxn
(i))p(Xn-1
(i) Yn-1)
q(Xn
(i)Yn)
1
Zn
 p(xnxn-1)l(ynxn)
p(Xn-1Yn-1)
q(XnYn)
 =
3
 1
Zn
 p(xnxn-1)l(ynxn) 
p(Xn-1Yn-1)
q(XnYn)
q(XnYn)dxn-1
 p(XnYn) =
3
 1
Zn
 p(xnxn-1)l(ynxn)p(Xn-1Yn-1)dxn-1
{w
~ (i)
n }N
i=1
ˆhn
Section 14.8
Particle Filters
769
updated
posterior
¯˚˚˘˚˚˙
Prior
¯˚˚˘˚˚˙
Likelihood
function
¯˚˘˚˙
Importance
distribution
¯˚˘˚˙

holds for all i.Then, the updated sequence of samples from the importance distribution
q(Xn
(i)Yn) is obtained simply by augmenting the old sequence of samples drawn from the
importance distribution q(X(i)
n-1Yn-1) with a sequence of samples drawn from the new
importance distribution q(x(i)
n X(i)
n-1, yn), receipt upon the new observation yn. Thus, Eq.
(14.114) may be viewed as the “trick” behind sequential importance sampling. In any
event, using the decomposition of Eq. (14.114) in the formula of Eq. (14.113), we obtain
(14.115)
A case of practical interest is when only a filtered estimate of the posterior distri-
bution p(XnYn) is required at each time-step n. In such a case, we may set
and similarly for 
In a scenario of this kind, we require only that the cur-
rent state xn
(i) be stored, and therefore we discard the old trajectory X(i)
n-1 and the cor-
responding history of observations Yn-1. Accordingly, the formula of Eq. (14.115) for
updating the importance weights simplifies to
(14.116)
where
is the symbol for proportionality. Equation (14.116) is the desired formula
for evaluating the normalized importance weights recursively in time; it satisfies the
second requirement of the nonlinear filtering objective on recursive implementability
of particle filters. In particular, the SIS proceduce propagates the importance weights
at each time-step as a new observation is received. The multiplicative factor on the
right-hand side of Eq. (14.116), permitting the “old” importance weight 
to be
updated when the new observation yn is received at time-step n, is called the
incremental correction factor.
Clearly, sequential importance sampling applies equally well to the Monte Carlo
estimation of the posterior distribution 
; in light of Eqs. (14.112) and (14.116),
we may write
(14.117)
where (xn - xn
(i)) is the Dirac delta function positioned at xn = xn
(i) for i = 1, 2, ..., N and
the weights are updated in accordance with Eq. (14.116) for the filtering scenario.As the
number of particles, N, approaches infinity, the estimator of Eq. (14.117) approaches
the true posterior distribution 
.
The Weight-Degeneracy Problem
The importance distribution 
plays a pivotal role in the design of particle filters.
With it being invariably different from the posterior distribution 
, we find that
p(XnYn)
q(XnYn)
p(xnYn)
p(xnYn) L a
N
i=1
wn
(i)(xn - xn
(i))
p(xnYn)
w

n-1
(i)
r
wn
(i) r wn-1
(i)
*
p(xn
(i)xn-1
(i) )l(ynxn
(i))
q(xn
(i)xn-1
(i) , yn)
    for all i
p(x(i)
n @X(i)
n-1).
q(xn
(i)Xn-1
(i) , yn) = q(xn
(i)xn-1
(i) , yn)    for all i
w
~
n
(i) r
p(X(i)
n-1Yn-1)
q(X(i)
n-1Yn-1) *
p(xn
(i)Xn-1
(i) )l(ynxn
(i))
q(xn
(i)Xn-1
(i) , yn)
770
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Incremental correction
factor
¯˚˚˚˚˘˚˚˚˚˙
Old
importance
weight
¯˘˙
Updated
importance
weight
¯˘˙

the variance of the importance weights, defined in Eq. (14.108), can only increase over
the course of time. This phenomenon, experienced in the use of sequential importance
sampling, leads to the weight-degeneracy problem mentioned previously.
For an intuitive explanation of the weight-degeneracy problem, consider a parti-
cle Xn
(i) with a small normalized importance weight wn
(i) at time-step n. By definition, a
small weight implies that the particle wn
(i) has been drawn from the importance distrib-
ution 
a good distance away from the main body of the posterior distribution
, which therefore means that the contribution of this particular particle to the
Monte Carlo estimator
of Eq. (14.109) is rather ineffective.When the degeneracy
problem is severe, there is a large number of ineffective particles, with the result that the
Monte Carlo estimator 
is inefficient in statistical as well as computational terms.
In such a situation, a small number of particles carries the computational burden. Most
seriously, however, as the time-step n increases, we find that diversity among the ensem-
ble of particles is reduced and the variance of the estimator 
is increased, consti-
tuting a bad situation to be in.
To guard against the weight-degeneracy problem in sequential importance sam-
pling, we clearly need a degeneracy measure. With such a measure in mind, Liu (1996)
defined an effective sample size as
(14.118)
where wn
(i) is the normalized importance weight of Eq. (14.110).There are two extreme
cases to be considered in applying this simple formula:
1. The N weights are all uniformly distributed with wn
(i) = 1/N for all i, in which case
Neff = N.
2. All the N weights are zero, except for one weight whose value is unity; in this case,
Neff = 1.
It follows, therefore, that Neff lies inside the range [1, N]. In particular, a small Neff implies
a severe case of weight degeneracy, and vice versa.
The key question is therefore as follows:
Recognizing that the weight-degeneracy problem in sequential importance sampling is the
rule rather than the exception, how do we overcome it?
The answer to this fundamental question lies in the use of resampling described earlier
on in this section. For example, the algorithmic formulation of a particle filter could
include a prescribed threshold,denoted by Nthr.When the effective sample size Neff drops
below the threshold Nthr, the SIS procedure is momentarily stopped and a resampling
step is applied, after which the SIS procedure is resumed; this process is repeated until
the filtering is terminated.
The Sampling–Importance-Resampling Particle Filter
The first practical implementation of a particle filter was demonstrated by Gordon,
Salmond, and Smith (1993) under the name “bootstrap filter.” Prior to the publication
Neff = c a
N
i=1
(wn
(i))2d
-1
ˆhn(N)
ˆhn(N)
ˆhn(N)
p(XnYn)
q(XnYn)
Section 14.8
Particle Filters
771

of the Gordon–Salmond–Smith paper, the serious problem of weight degeneracy in
sequential importance sampling was neither clearly identified nor satisfactorily cured.
In this 1993 paper, the weight-degeneracy problem was solved through a rejuvenation
process whereby particles associated with small weights are pruned away, and particles
with large weights are not only retained, but also replicated, much in the same way as
in the traditional nonsequential sampling procedure. Indeed, it is for this reason that
nowadays the bootstrap filter is commonly referred to as the sampling–importance-
resampling (SIR) filter. The important point to take from this brief historical account is
that the SIR filter was the first successful demonstration of nonlinear filtering using
Monte Carlo simulation.
The SIR filter is simple to implement—hence its popular use for solving nonlin-
ear filtering problems.The distinctive features of the filter are twofold:
1. The prior as the importance distribution. Examining the recursive formula of
Eq. (14.116) for updating the weights, we see that the importance distribution is
defined by how we choose the denominator q(xn
(i)xn-1
(i) , yn) on the right-hand side
of the equation. In the SIR filter, this choice is made by setting
(14.119)
where, on the right-hand side of the equation, p(xnxn-1) is the prior, or state-
transition distribution. In effect, the SIR filter blindly samples from the prior
p(xn|xn-1), completely ignoring the information about the state xn contained in the
observation yn. Equation (14.119) follows from the Markovian assumption.
2. Sampling importance resampling. In the SIR filter, resampling is applied at
every time-step of the nonlinear filtering process; consequently, in Eq. (14.116)
we have
(14.120)
Because 1/N is a constant, it may be ignored. Thus, the need for an accumulation
over time of the incremental correction factor in Eq. (14.116) is no longer needed.
Accordingly, the use of Eqs. (14.119) and (14.120) in Eq. (14.116) yields the
much simplified formula
(14.121)
where l(ynxn
(i)) is the likelihood function of the observation yn, given the state xn
(i)
for particle i. Naturally, normalization of the importance weights calculated using
the proportionality equation of Eq. (14.121) is carried out after each resampling
step of the SIR filtering algorithm.
Table 14.5 presents a summary of the SIR filter.
From the discussion just presented, it is apparent that the assumptions made in
formulating the SIR filter are of a mild sort, as summarized here:
1. The nonlinear function an(, ) in the process model of Eq. (14.1) and the non-
linear function bn(, ) in the measurement model of Eq. (14.2) must be both
known.
w
~ (i)
n
r l(ynxn
(i))   for i = 1, 2, ...., N
wn-1
(i)
= 1N    for i = 1, 2, ...., N
q(xnxn-1, yn) = p(xnxn-1)
772
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

Section 14.8
Particle Filters
773
2. Determining the prior p(xnxn-1) requires knowledge of the statistics of the dynamic
noise n in Eq. (14.1); drawing samples (particles) from the underlying distribu-
tion of the dynamic noise n must therefore be permissible.
3. The likelihood function l(ynxn), involved in formulating Eq. (14.121), must be
known, which, in turn, means that the statistics of the measurement noise n in
Eq. (14.2) are available.
TABLE 14.5
Summary of the SIR Algorithm for Particle Filtering
Notation
The particles are denoted by i = 1, 2, ..., N, where N is the total number of particles.
Initialization
Given the state distribution p(x) and x0 as the initial value of x, randomly sample
where the notation “x ~ p” is short for “x is an observation from the distribution p,” and set the initial weight
where i = 1, 2, ..., N.
w0
(i) = 1
N
~ p(x0)
x(i)
0
Recursions
For each time-step n = 1, 2, 3, ..., followed by the index i = 1, 2, ..., N, do the following:
1. With the importance distribution defined by
where the prior 
is assumed to be known, draw samples
2. Compute the importance weights
where the likelihood l(ynxn
(i)) is also assumed to be known. Hence, compute the normalized weights
3. To resample, draw a set of N discrete random variables {I(1), I(2), ..., I(N)} that take values in the corresponding
set {1, 2, ..., N} with probabilities
Hence, set
and
4. Continue the computation until the filtering is completed.
wn
(i) = 1
N
x(i)
n = x(i)
n
P(I(s) = i) = wn
(i)
w(i)
n =
w(i)
n
a
N
j=1
w(j)
n
w(i)
n = l(yn ∑x(i)
n )
x(i)
n  ~ p(xn ∑x(i)
n-1)
p(xn∑x(i)
n-1)
q(xnxn-1
(i) , yn) = p(xnx(i)
n-1)

774
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
One other issue that needs to be addressed in designing the SIR filter (and,for that
matter,any particle filter) is the selection of a suitable value for the number of particles,N.
On one hand, N should be large enough to satisfy the asymptotic result of Eq. (14.111).
On the other hand, N should be small enough to keep the total computational burden
at a manageable level, since the particles are acting in parallel at each time-step of the
filtering. (Here, we are assuming that the number of particles is maintained at the same
value N after the importance sampling and resampling operations.) The value selected
for N must therefore be a “compromise” between these two conflicting situations, which
can be resolved only on a problem-by-problem basis.
The Optimal Choice of Importance Distribution
The prior distribution p(xnxn-1) provides an appealing way of choosing the importance
distribution, as is the case in the SIR filter. However, such a choice in the design of par-
ticle filters could lead to a poor performance under unfavorable conditions. For exam-
ple, if the input data are corrupted by outliers, we have “noninformative” observations,
and if the variance of measurement noise is small then we have “highly informative”
observations. In such situations, there is the potential for a mismatch between the pre-
dictive prior distribution and the posterior distribution of the state given the observa-
tions. In order to mitigate this mismatch in an “optimal” fashion, the particles should be
chosen to move in the state space under an importance distribution, defined by (Doucet
et al., 2000; Cappé et al., 2007) as
(14.122)
This particular choice of the importance distribution is optimal in the sense that the
conditional variance of the weights is zero, given the prior history of the particles.
Substituting Eq. (14.122) into the SIS formula of Eq. (14.116) yields the weight
update formula
(14.123)
where we now see that the incremental correction factor (i.e., the integral term)
depends only on the “past” position of the proposed particle x(i)
n-1 and the current
observation yn.
An important difference between the optimal formula of Eq. (14.123) and the SIR
formula of Eq. (14.121) should be noted: In the SIR filter, the particles are allowed to
move blindly in the state space, whereas under the optimal importance distribution of
Eq. (14.122), the particles are allowed to cluster in locations where the posterior distri-
bution has a mass of high probability, which is clearly a desirable situation.
However, computation of the optimal importance distribution defined in Eq.
(14.122) may not be a straightforward matter, except in some special cases. For example,
w(i)
n r w(i)
n-1 3
p(xnx(i)
n-1) l(ynxn)dxn
q(xn∑xn-1, yn)opt =
p(xn∑xn-1) l(yn∑xn)
1pAxn∑xn-1) l(yn∑xn) dxn
Old
weight
¯˘˙
Updated
weight
¯˘˙
Prior
¯˚˚˘˚˚˙
Likelihood
¯˚˘˚˙

in a class of state-space models for which the conditional distribution p(xn
, yn) is
Gaussian, choosing the optimal importance distribution to design a particle filter is
indeed possible (Doucet et al., 2000).
14.9
COMPUTER EXPERIMENT: COMPARATIVE EVALUATION 
OF EXTENDED KALMAN AND PARTICLE FILTERS
The experimental setup for this comparative evaluation is based on the state-space
model of a nonlinear, Gaussian dynamic system described by the following pair of
equations:
System (state) model:
Measurement (observation) model:
In this system, the dynamic noise #n is Gaussian 
, and the measurement noise 
n(0, 1)
yn = 1
20 xn
2 + vn
xn = 0.5xn-1 +
25xn-1
1 + x2
n-1
+ 8 cos(1.2(n - 1)) + #n
xn-1
(i)
Section 14.8
Computer Experiment
775
n is also Gaussian 
.The true initial value of the state is x0 = 0.1.
The SIR version of the particle filter was used in the experiment. The following
experimental conditions were applied to both the EKF and SIR filters:
Simulated state trajectory: 50 time-steps long
Number of independent Monte Carlo runs: 100
Intial value of the filtered estimate:
The specifications of the SIR particle filter were as follows:
• The number of particles, N, was 100.
• At each time-step of the filtering process, resampling was applied, followed by
normalization of the importance weights.
• The prior (i.e., state transition) distribution was used as the importance
distribution.
The results of the experiment are plotted in Figs.14.6 and 14.7 for the EKF and SIR
particle filter, respectively. In each figure, the solid curve is the true state, and the points
shown as asterisks (printed in red) are the averaged results of 50 such runs. The upper
and lower dotted curves in Figs. 14.6 and 14.7 define the confidence intervals of the state
estimates produced by the EKF and the PF, respectively.
Examination of these two figures reveals the following observations:
• For the EKF, the averaged trajectory of the filtered estimate of the state deviates
markedly from the true trajectory.
xˆ 00 = n(x0, 2)
n(0, 10)

776
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
**
*
**
*
*
*
*
**
**
*
* *
*
*
*
* *
**
**
*
*
*
**
*
*
*
*
**
*
* *
*
*
* *
*
*
*
*
*
*
*
30
20
5
10
15
20
25
30
35
40
45
50
Time-step n
10
0
xn
10
20
30
*
True
EKF
ˆ
FIGURE 14.6
Plots of the ensemble-averaged state estimate 
produced by the extended
Kalman filter (EKF), shown as a sequence of points *.The upper and lower dotted curves
(around the estimates) define the confidence interval of the state estimate produced by the
extended Kalman filter.The continuous line is the actual evolution of the state across time n.
xˆ n
FIGURE 14.7
Plots of the ensemble-averaged state estimate 
produced by the SIR parti-
cle filter, shown as a sequence of points *.The upper and lower dotted curves (around the
estimates) define the confidence interval of the state estimate produced by the particle filter
(PF).The continuous line is the actual evolution of the state across time n.
xˆ n
30
20
5
10
15
20
25
30
35
40
45
50
Time-step n
10
0
10
20
30
**
**
*
*
*
*
*
*
**
*
**
**
*
**
*
*
*
**
*
**
*
**
*
*
**
*
**
*
*
*
**
*
*
*
*
*
**
*
True
PF
xnˆ

• On the other hand, the corresponding averaged trajectory computed by the SIR
particle filter follows the true trajectory quite closely.
One other experimental result, pertaining to the particle filter, is shown in Fig 14.8,
where the root-mean-square error (RMSE) of the filtered estimate of the state is plot-
ted against the number of particles used in the SIR particle filter.We see that the RMSE
is initially high, progressively decreasing as the number of particles is increased. Beyond
N = 100 particles, there is no significant change in the RMSE; the choice of N = 100
particles for the SIR filter in the experiment is therefore justified.
14.10
KALMAN FILTERING IN MODELING OF BRAIN FUNCTIONS
The discussion presented thus far in the chapter has focused on Kalman filter theory, fol-
lowed by the Bayesian filter and its approximate forms. In so doing, we have emphasized
the practical virtues of these filters as sequential state estimators, each in its own way.
In this section, we present an overview of “Kalman-like filtering” for modeling differ-
ent brain functions (Chen et al., 2007).
Dynamic Model of Visual Recognition
The visual cortex contains a hierarchically layered structure (from V1 to V5) and mas-
sive interconnections within the cortex and between the cortex and the visual thalamus
(i.e., lateral geniculate nucleus, or LGN); for a brief description of these parts of the
visual system, see Note 8 under the Notes and References of Chapter 12. Specifically,
the visual cortex is endowed with two key anatomical properties (Chen et al., 2007):
Section 14.10
Kalman Filtering in Modeling of Brain Functions
777
1.3
1.2
1.1
1
0.9
RMSE
0.8
0.7
0.6
0.5
0
100
200
Number of particles
300
400
500
FIGURE 14.8
Plot of the root mean-square error (RMSE) versus the number of particles pro-
duced by the SIR particle filter; the points • are experimentally computed.

• Abundant Use of Feedback. The connections between any two areas of the visual
cortex are bilateral, thereby accommodating the transmission of forward as well
as feedback signals.
• Hierarchical Multiscale Structure. The receptive fields of lower area cells in the
visual cortex span only a small fraction of the visual field, whereas the receptive
fields of higher area cells increase in size until they span almost the entire visual
field.It is this constrained network structure that makes it possible for the fully con-
nected visual cortex to perform prediction in a high-dimensional data space with
a reduced number of free parameters, and therefore in a computationally efficient
manner.
In a series of studies over a period extending from 1997 to 2003, Rao and collab-
orators exploited these two properties of the visual cortex to build a dynamic model of
visual recognition, with the understanding that vision is fundamentally a nonlinear
dynamic process.The Rao–Ballard model of visual recognition is a hierarchically orga-
nized neural network, with each intermediate level of the hierarchy receiving two kinds
of information: bottom-up information from the preceding level and top-down informa-
tion from the higher level. For its implementation, the model uses a multiscale estima-
tion algorithm that may be viewed as a hierarchical from of the extended Kalman filter.
In particular, the EKF is used to simultaneously learn the feedforward, feedback, and
prediction parameters of the model, using visual experiences in a dynamic environment.
The resulting adaptive processes operate on two different time scales:
• The fast dynamic state-estimation process allows the dynamic model to anticipate
incoming stimuli.
• The slow Hebbian learning process provides for synaptic-weight adjustments in the
model.
Specifically, the Rao–Ballard model can be viewed as a neural network implementa-
tion of the EKF that employs top-down feedback between layers and is able to learn
the visual receptive fields for both static images and time-varying image sequences.The
model is very appealing in that it is simple and flexible, yet powerful. Above all, it
allows a Bayesian interpretation of visual perception (Knill and Richards, 1995; Lee
and Mumford, 2003).
Dynamic Model of Sound-Stream Segregation
It is well known in the computational neuroscience literature that auditory perception
shares many common features with visual perception (Shamma, 2001). Specifically,
Elhilali (2004) addressed the problem of sound-stream segregation within the framework
of computational auditory scene analysis (CASA).In the computational model described
therein, the hidden vector contains an internal (abstract) representation of sound
streams; the observation is represented by a set of feature vectors or acoustic cues (e.g.,
pitch and onset) derived from the sound mixture. Since temporal continuity in sound
streams is an important characteristic, it can be used to construct the system (state)
model.The measurement model describes the cortical filtering process with the cortical
778
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

model’s parameters. The basic component of dynamic sound-stream segregation is
twofold: First, infer the distribution of sound patterns into a set of streams at each time
instant; second, estimate the state of each cluster, given the new observations.The sec-
ond estimation problem is solved by a Kalman filtering operation, and the first cluster-
ing problem is solved by a Hebbian-like competitive-learning operation.
The dynamic nature of the Kalman filter is important not only for sound-stream
segregation, but also for sound localization and tracking, all of which are regarded as the
key ingredients for active audition (Haykin and Chen, 2006).
Dynamic Models of Cerebellum and Motor Learning
The cerebellum has an important role to play in the control and coordination of move-
ments, which are ordinarily carried out in a very smooth and almost effortless manner.
In the literature, it has been suggested that the cerebellum plays the role of a controller
or the neural analog of a dynamic state estimator. The key point in support of the
dynamic state-estimation hypothesis is embodied in the following statement, the valid-
ity of which has been confirmed by decades of work on the design of automatic track-
ing and guidance systems:
Any system, be it a biological or artificial system, required to predict or control the trajectory
of a stochastic multivariate dynamic system can do so only by using or invoking the essence
of Kalman filtering in one way or another.
Building on this key point, Paulin (1997) presents several lines of evidence which
favor the hypothesis that the cerebellum is a neural analog of a dynamic state estimator.
A particular line of evidence presented by Paulin relates to the vestibular–ocular reflex
(VOR), which is part of the oculomotor system.The function of the VOR is to maintain
visual (i.e.,retinal) image stability by making eye rotations that are opposite to head rota-
tions, as discussed in the introductory chapter.This function is mediated by a neural net-
work that includes the cerebellar cortex and vestibular nuclei. From the discussion
presented in Section 14.3, we know that a Kalman filter is an optimum linear system with
minimum variance for predicting the state trajectory of a dynamic system using noisy mea-
surements; it does so by estimating the particular state trajectory that is most likely, given
an assumed model for the underlying dynamics of the system.A consequence of this strat-
egy is that, when the dynamic system deviates from the assumed model, the Kalman fil-
ter produces estimation errors of a predictable kind, which may be attributed to the filter
“believing in”the assumed model rather than the actual sensory data.According to Paulin
(1997), estimation errors of this kind are observed in the behavior of the VOR.
Summarizing Comments
To summarize, the predictor–corrector property of the Kalman filter lends itself as a
potentially useful candidate for predictive coding in computational neural modeling,
which is a fundamental property of the autonomous brain functions in a dynamic envi-
ronment. It is also important to note that in the aforementioned examples, the hypoth-
esis that the neural system (e.g., cerebellum or neocortex) is a neural analog of a Kalman
filter is not to be taken to imply that, in physical terms, the neural system resembles a
Section 14.10
Kalman Filtering in Modeling of Brain Functions
779

Kalman filter. Rather, in general, biological systems do perform some form of state esti-
mation, and the pertinent neural algorithms may have the general “flavor” of a Kalman
filter. Moreover, it is plausible that some form of state estimation is broadly distributed
throughout other parts of the central nervous system.5
14.11
SUMMARY AND DISCUSSION
The theme throughout the material presented in this chapter has been that of estimat-
ing the unknown (hidden) state of a dynamic system, given a sequence of observations
that are dependent on the state. Basic to the solution of this problem is a state-space
model that consists of a pair of equations: One equation models the evolution of the
state across time with a dynamic noise driving that evolution, and the other models the
noisy version of observations on the state. It is assumed that the state-space model is
Markovian.
Kalman Filter Theory
When the dynamic system is linear and Gaussian, the optimal estimator of the state is
the celebrated Kalman filter.When the dynamic system is nonlinear and Gaussian, we
may use the extended Kalman filter by using first-order Taylor-series approximations of
the state-space model. This approximate approach to nonlinear filtering yields accept-
able results, provided that the nonlinearity is of a mild sort.
Bayesian Filter
In theory,the Bayesian filter is the most generic nonlinear filter and includes the Kalman
filter as a special case. However, to implement the Bayesian filter in practice, it has to
be approximated.The approximation may follow one of two routes:
1. Direct numerical approximation of the posterior. The idea behind this first approach
is summed up as follows:
Use numerical methods to facilitate the approximate estimation of the state of a non-
linear dynamic system through linear Kalman filter theory.
Examples of this approach to nonlinear filtering include the extended Kalman fil-
ter, the unscented Kalman filter (Julier et al., 2000), the quadrature Kalman filter
(Ito and Xing, 2000; Arasaratnam et al., 2007), and the cubature Kalman filter
(Arasaratnam and Haykin, 2009). Among these nonlinear filters, the extended
Kalman filter is the simplest, and the cubature Kalman filter is the most powerful.
Simply put, increased computational complexity is progressively traded for
increased reliability.
2. Indirect numerical approximation of the posterior. The most prominent and widely
used example of this second approach to nonlinear filtering is the particle filter.
With the posterior distribution of the Bayesian filter being inaccessible, we resort
to drawing samples randomly from an importance, or instrumental, distribution
whose support must include that of the posterior distribution. Moreover, recursive
780
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

implementation of the particle filter is realized through the sequential importance-
sampling (SIS) procedure.To circumvent the likelihood of the filter running into
the undesirable situation of weight degeneracy, the common practice is to follow
the importance sampling with resampling, whereby relatively weak normalized
weights are pruned away and the remaining normalized weights are replicated in
accordance with their probability of occurrence.
Although the Kalman filter and its variants and approximate extensions on
one hand, and particle filters on the other hand, are radically different in their ana-
lytic derivations and practical implementations,they do share an important property:
The predictor–corrector property.
Computational Considerations
(i) Kalman Filters. Whenever we develop a filtering algorithm, it is customary to
examine the convergence behavior of the algorithm. In particular, a user of the
algorithm would like to know the conditions under which the algorithm is likely
to diverge, and how to fix the divergence problem. For example, it is well known
that the Kalman filter suffers from the divergence phenomenon, which can arise
because of the following two factors:
• model mismatch between the state-space model (on which the derivation of
the Kalman filter is based) and the underlying physics of the actual dynamic
environment responsible for generation of the observations;
• inadequate arithmetic precision used in actual implementation of the Kalman
filter.
The root of the divergence phenomenon may be traced to the matrix
violat-
ing the nonnegative-definite property of a covariance matrix. Square-root filter-
ing provides a method for mitigating the divergence phenomenon.
(ii) Particle Filters. Turning next to the computational aspects of particle filters, we
find that coverage of this topic in the literature has been somewhat controversial.
This observation may not be surprising, given the Monte Carlo-based roots of
particle filtering.In any event,here we summarize some important results reported
in the literature:
1. For a prescribed number of particles, N, the error incurred in the Monte Carlo
estimate of the integral in Eq. (14.84) is on the order of O(N-1/2), which is inde-
pendent of the dimension of the state vector (Ristic et al., 2004). This result is
based on two assumptions:
• The posterior distribution p(xnYn) in the integrand of Eq.(14.84) is known exactly.
• The particles (i.e., samples) are statistically independent.
However, both of these assumptions are violated in particle filtering: Exact
knowledge of p(xnYn) is not available, and in a particle filter using resampling,
the particle trajectories become actually dependent.
2. In Crisan and Doucet (2002), an upper bound on the variance of the estimation
error produced by a particle filter is shown to be O(N-1/2) multiplied by a con-
stant scaling factor c.
Pnƒn
Section 14.11
Summary and Discussion
781

Unfortunately, this result has led to the erroneous conclusion that the esti-
mation error produced by a particle filter is independent of the dimension of
the state vector and therefore immune from the curse of dimensionality. In
Daum and Huang (2003), it is argued that the multiplying factor is not a con-
stant; rather, it increases exponentially with time n—hence the notation cn.
Moreover, it depends strongly on the dimension of the state vector, which
means that particle filters do suffer from the curse of dimensionality.
3. In an independent study reported in Bengtsson et al. (2008), it is demonstrated
that “brute-force-only” implementations of a particle filter to describe high-
dimensional posterior distributions will fail,due to the curse of dimensionality.The
recommended remedy to this phenomenon is to achieve some form of
dimensionality reduction prior to particle filtering; as pointed out in Chapter 10,
high-dimensional data are usually sparse and therefore open to dimensionality
reduction.
NOTES AND REFERENCES
1. Correlated Dynamic and Measurement Noise. In a linear Gaussian state-space model, cor-
relation between the dynamic noise n and measurement noise n is sometimes permitted.
This condition is used in econometrics. Specifically, we now have
where Cn is a known matrix.According to this equation, the two noise processes n and n
are contemporaneously correlated, but they remain uncorrelated at nonzero lags. In such
a situation, formulation of the Kalman filter has to be modified. It appears that this issue
was first discussed in Jazwinski (1970); see also Harvey (1989).
2. Information Filtering Algorithm. The covariance-filtering algorithm is one way of imple-
menting the Kalman filter. In another form called the information-filtering algorithm, the
Kalman filter is implemented by propagating the inverse of the covariance matrix 
; this
inverse is related to Fisher’s information matrix, which permits an interpretation of the fil-
ter in information-theoretic terms. For more detail on the information-filtering algorithm,
see Chapter 10 of Haykin (2002).
3. Notation. To be rigorously correct in Eq. (14.6) and consistent with the notation used ear-
lier in the book, we should write pX(x) in place of p(x), where the subscript X in pX(x) stands
for the random vector X whose sample value is denoted by x. We have used the notation
p(x) in Eq. (14.6) and other similar situations in the chapter for two reasons:
•
to simplify the presentation, as this chapter is rather rich in the probabilistic character-
ization of stochastic processes, and
•
most importantly, to avoid confusion in the latter part of the chapter, where the symbol
X is used to represent a sequence of states.
4. Bayes Estimation. A classic problem in estimation theory is that of the Bayes estimation of
a random parameter.There are different answers to this problem,depending on how the cost
function in the Bayes estimation is formulated.A particular type of the Bayes estimator of
interest to us is the so-called conditional mean estimator. In this note, we do two things:
Pnn
[nT
k] = eCn
for k = n
0
for k Z n
782
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

(1) derive the formula for the conditional mean estimator from first principles, and (2)
show that such an estimator is the same as a minimum mean-square-error estimator.
Toward those ends,consider a random parameter x.We are given an observation y that
depends on x, and the requirement is to estimate x. Let 
denote an estimate of the
parameter x; the symbol 
emphasizes the fact that the estimate is a function of the
observation y. Let R denote a cost function that depends on both x and its estimate.Then,
according to Bayes’s estimation theory, we may define the Bayes risk as
R = [C(x, xˆ (y))]
xˆ (y)
xˆ (y)
Notes and References
783
(A)
where p(x,y) is the joint probability density function of x and y. For a specifie cost function
, the Bayes estimate is defined as the estimate 
that minimizes the risk R.
A cost function of particular interest (and that is very much in the spirit of the mate-
rial covered in this book) is the mean-square error, specified as the square of the estimation
error, which is itself defined as the difference between the actual parameter value x and
the estimate 
; that is,
Correspondingly, we write
or, more simply,
We may therefore rewrite Eq. (A) as
(B)
where the subscripts in the risk Rms indicate the use of the mean-square error as its basis.
From probability theory, we have
(C)
where p(xy) is the conditional probability density function of x given y, and p(y) is the
(marginal) probability density function of y. Hence, using Eq. (C) in Eq. (B), we get
(D)
We now recognize that the inner integral (inside the square brackets) and p(y) in Eq.
(D) are both nonnegative. We may therefore minimize the risk Rms simply by minimizing
the inner integral. Let the estimate so obtained be denoted by 
.We find 
by dif-
ferentiating the inner integral with respect to 
and then setting the result equal to zero.
To simplify the presentation, let I denote the inner integral in Eq. (D). Then differ-
entiating I with respect to 
yields
(E)
dI
dxˆ = -2
3
q
- q xp(xy)dx + 2xˆ(y)
3
q
- q p(xy)dx
xˆ(y)
xˆ(y)
xˆ ms(y)
xˆ ms(y)
Rms =
3
q
- q c
3
q
- q (x - xˆ(y))2 p(xy)dxdp(y)dy
p(x, y) = p(x∑y)p(y)
Rms =
3
q
- q 3
q
- q (x - xˆ(y))2 p(x, y)dx dy
C() = 2
C(x, xˆ(y)) = C(x - xˆ(y))
 = x - xˆ (y)
xˆ (y)
xˆ (y)
C(x, xˆ (y))
=
3
q
- q 3
q
- q
C(x, xˆ (y))p(x, y)dx dy

The second integral on the right-hand side of Eq. (E) represents the total area under a
probability density function and therefore equals unity. Hence, setting the derivative 
equal to zero, we obtain
(F)
The solution defined by Eq. (F) is a unique minimum.
The estimator 
defined in Eq. (F) is naturally a minimum mean-square-error
estimator. For another interpretation of this estimator, we recognize that the integral on
the right-hand side of the equation is just the conditional mean of the parameter x, given
the observation y.
We therefore conclude that the minimum mean-square-error estimator and the con-
ditional mean estimator are indeed one and the same. In other words, we have
(G)
Substituting Eq. (G) for the estimate 
into Eq. (D), we find that the inner integral is just
the conditional variance of the parameter x, given y. Accordingly, the minimum value of
the risk Rms is just the average of this conditional variance over all observations y.
5. Bayesian Filtering Based on Spike Trains. In the discussion presented in Section (14.10) on
the dynamic modeling of brain functions, we followed a traditional signal-processing frame-
work, with emphasis on the role of Kalman filter theory.
In fact,however,cortical neural networks observe an uncertain dynamic environment
through spike trains received from sensory afferents,and not directly from the environment.
Spike trains provide the principal communication channels among neurons in the brain;they
are represented in terms of the arrival times of the spikes (Koch, 1999; Rieke et al., 1997).
Bobrowski et al. (2007) considered the problem of optimally estimating the probability dis-
tribution of the hidden state of a dynamic environment, given noisy observations in the
form of spike trains. Most importantly, they described a linear recurrent network model
that is capable of exactly implementing Bayesian filtering in real time.The input may be mul-
timodal, consisting of two different subsets—one visual and the other auditory, for exam-
ple.Moreover,synthetic examples were presented to demonstrate the operation of the system.
It is noteworthy that nonlinear filtering in continuous time, based on point-process
observations, was first described by Snyder (1972); see also Snyder’s 1975 book on random
point processes.
PROBLEMS
Kalman Filters
14.1 The predicted state-error vector is defined by
where 
is the minimum mean-square estimate of the state xn, given the observed data
sequence y1, ..., yn-1. Let n and n denote the dynamic-noise and measurement-noise vec-
tors, respectively. Show that 
nn-1 is orthogonal to both n and n; that is, show that
and
[nn-1n
T] = 0
[nn-1n
T] = 0

xˆ nn-1
nn-1 = xn - xˆ nn-1
xˆ(y)
xˆ ms(y) = [xy]
xˆ ms(y)
xˆ ms(y) =
3
q
- q xp(xy)dx
dIdxˆ
784
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems

14.2 Consider a set of scalar observations yn of zero mean that is transformed into the corre-
sponding set of innovation process n of zero mean and variance 2
, n. Let the estimate of
the state vector xi, given this set of data, be expressed as
where bi, k, k = 1, 2, ..., n, is a set of vectors to be determined.The requirement is to choose
the bi, k so as to minimize the expected value of the squared norm of the estimated state-
error vector
Show that this minimization yields the result
where
is the normalized innovation. This result may be viewed as a special case of Eqs. (14.24)
and (14.26).
14.3 Prove Eq. (14.25), which states that the innovations process k and the state-estimation
error 
are uncorrelated for k = 1, 2, ..., n and i  n.
14.4 Show that in Kalman filter theory, the filtered state-estimation error vector 
is a zero-
mean, Gaussian, and first-order Markov process.
14.5 The Kalman gain Gn, defined in Eq. (14.31), involves the inverse matrix Rn
-1.The matrix Rn
is itself defined in Eq. (14.22). The matrix Rn is nonnegative definite, but not necessarily
nonsingular.
(a) Why is Rn nonnegative definite?
(b) What prior condition would you impose on the matrix Q, n to ensure that the inverse
matrix Rn
-1 exists?
14.6 In many cases, the prediction-error covariance matrix
converges to the steady-state
value P as the number of iterations, n, approaches infinity. Show that the limiting value P
satisfies the algebraic Riccati equation
where it is assumed that the state-transition matrix equals the identity matrix and the matri-
ces B, Q#, and Q are the limiting values of Bn, Q#, n, and Q, n, respectively.
14.7 It can be argued that a state-space model of the original dynamic system is embedded within
the structure of the Kalman filter. Justify this statement.
14.8 Examination of the predictor–corrector framework in the Kalman filter reveals the follow-
ing two properties:
(a) Computation of the predicted state 
and the prediction-error covariance matrix
Pn1n relies only on information extracted from the system (state) model.
(b) Computation of the filtered stated 
and the filtering-error covariance matrix 
relies only on information extracted from the measurement model.
Justify these two properties of the Kalman filter.
Pnn
xˆ nn
xˆ n+1n
PBT(BPBT + Q)-1(BP - Q#) = 0
Pn+1n
in
in
k = k
,k
xˆ in = a
n
k=1
[xik]k,
i, n = xi - xˆ in.
xˆ in = a
n
k=1
bi, k k,
Problems
785

14.9 The prediction-error covariance matrix Pn1 n and the filtering-error covariance matrix 
can never assume a common value.Why?
14.10 The derivation of the Kalman filter presented in Section 14.3 is based on the notion of min-
imum mean-square-error estimation. In this problem, we explore another derivation of the
Kalman filter, based on the maximum a posteriori (MAP) probability criterion. For this
derivation, it is assumed that the dynamic noise n and measurement noise n are both
zero-mean Gaussian processes with covariance matrices Q, n and Q, n respectively. Let
p(xYn) denote the conditional probability distribution of xn, given that Yn denotes the set
of observations y1, ..., yn.The MAP estimate of xn, denoted by 
, is defined as that par-
xˆ MAP,n
Pnn

786
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
ticular value of xn which maximizes p(xnYn), or, equivalently, the logarithm of p(xnYn).
This evaluation requires that we solve for the condition
(A)
Show that
(B)
(a) We may express the distribution p(xnYn) as
which, in light of the definition of a joint distribution, may also be expressed as
Hence, show that
(b) Using the Gaussian characterizations of the dynamic noise n and measurement noise
n, derive expressions for p(ynxn) and p(xnYn-1). Next recognizing that p(ynYn-1) may
be treated as a constant, since it does not depend on the state xn, formulate the expres-
sion for p(xnYn).
(c) Using the results of part (b) in Eq. (A), followed by the matrix inversion lemma (dis-
cussed in Chapter 5), derive the formula for 
and show that it is exactly the same
as the Kalman filter derived in Section 14.3.
(d) Finally,show that the MAP estimate 
derived in part (c) does indeed satisfy Eq.(B).
14.11 Consider a linear dynamic system described by the noiseless state-space model
and
yn = Bxn
xn+1 = Axn
xˆ MAP, n
xˆ MAP, n
p(xnYn) =
p(ynxn)p(xnYn-1)
p(yn,Yn-1)
p(xnYn) =
p(xn, yn,Yn-1)
p(yn, Yn-1)
p(xnYn) =
p(ynYn-1)
p(Yn)
02 log p(xnYn)
02xn
`
xn=xˆMAP, n 6 0
0 log p(xnYn)
0xn
`
xn=xˆMAP, n = 0

where xn is the state, yn is the observation, A is the transition matrix, and B is the measure-
ment matrix.
(a) Show that
where Gn is the Kalman gain and n denotes the innovations process.How is Gn defined?
(b) Using the results of part (a), justify the statement that the Kalman filter is a whitening
filter in that it produces a “white” estimation error in response to yn.
14.12 Table 14.2 presents a summary of the Kalman filter based on filtered estimation of the state.
Develop another summary of the Kalman filter, this time using the predicted estimate of
the state as its basis, and depict the corresponding signal-flow graph of the Kalman filter.
Square-Root Kalman Filter
14.13 The identities of Eqs. (14.47) through (14.49) follow from equating corresponding terms in
the two sides of Eq. (14.46). There is actually a fourth identity to be considered. Find this
identity and show that it is the transpose of one of the identities already known.
Extended Kalman Filter
14.14 Starting with the modified system (state) model of Eq. (14.64), show that 
a known (i.e.,
nonrandom) vector, is defined by Eq. (14.75).
14.15 Let Pxy, n denote the cross-covariance matrix between the state-error vector 
and the measurement-error vector 
. Let Pyy, n denote the covariance matrix of
the measurement-error vector 
. Show that the modified Kalman gain in
may be expressed in terms of these two covariance matrices as
Bayesian Filter
14.16 (a) Prove Eq. (14.77)
(b) Prove Eq. (14.83).
Particle Filters
14.17 The extended Kalman filter and particle filter represent two different examples of nonlin-
ear filters in the following sense:
• Derivation of the extended Kalman filter is based on a local approach under statistical
distribution constraints.
• On the other hand, derivation of particle filters is based on a global approach with no
statistical constraints.
Elaborate on these two statements.
14.18 Figure 14.5 illustrates the resampling process for the case when the number of samples and
resamples is equal to six; that is, the number of particles after resampling is the same as the
number of particles before sampling. Explain how this figure is produced.
14.19 Consider a nonlinear dynamic system whose state-space model is defined by
and
yn = bn(xn) + n
xn+1 = an(xn) + n
Gf, n = Pxy, n Pyy, n
-1
Gf, n = A-1
n+1, n Gn
yn - yˆ nn-1
yn - yˆ nn-1
xn - xˆ nn-1
n,
n = yn - Bxˆ nn-1
xˆ nn = (I - GnB)xˆ nn-1 + Gn yn
Problems
787

where the dynamic noise n and the measurement noise n are both zero-mean, white-
noise Gaussian processes with covariance matrices Q#, n and Q, n, respectively.
Determine the following distributions:
(a) The prior predictive distribution p(xnYn1).
(b) The likelihood p(ynxn).
(c) The posterior p(xnYn), where Yn denotes the observation sequence y1, y2, ..., yn.
14.20 Continuing from Problem 14.19, show that the optimal importance density p(xnxn-1, yn) is
Gaussian.
Computer experiment
14.21 In this problem, we use a particle filter to solve a nonlinear-tracking problem in com-
puter vision.An object that is made up of 5  5 pixels moves along a trajectory defined by
the following pair of equations:
where xn and yn are the image coordinates for step n and N is the total number of frames.
The scene of size 300  300 pixels is visualized in Fig. P14.21. The white background area
is divided by four equally spaced bars of height h = 10 pixels in black, which indicate the
foreground areas.The object can be distinguished by its red color.
(a) Simulate the trajectory shown in light red as an image sequence using N = 150 frames.
Make sure to show the object if it moves over background areas and to hide it if it is
occluded by foreground areas.
 yn = 100sin a 3.5n
N
b + 150
 xn = 200` sin a 2n
N b ` + 50
788
Chapter 14
Bayesian Filtering for State Estimation of Dynamic Systems
Trajectory
Object
Foreground area
FIGURE P14.21
Scene and trajectory for Problem 14.21.

(b) Using the simulated data as input, implement a particle filter to track the object. In
areas where the object is visible, you can use the color information to get a position
measurement, but where it is occluded, you have to rely on the filter estimates.What
assumptions do you need to make when setting up the state-space model? Visualize
the true and estimated trajectory in the scene.
(c) Now continually increase the height h of the foreground areas for different trials.
Explain the trade-off necessary to keep track of the object throughout the image
sequence.What influence do the frame rate and the number of particles have?
(d) The information gathered during this tracking process can be used to estimate fore-
ground and background parts of the scene—that is, to get the depth of the object rel-
ative to the parts it interacts with. Discuss possible approaches to this task.
Problems
789

ORGANIZATION OF THE CHAPTER
This chapter studies the many facets of a dynamically driven recurrent network as an
input–output mapper.
The subject matter of the chapter is organized as follows:
1. By linking up with Chapters 13 and 14, the introductory section, Section 15.1, moti-
vates the study of dynamically driven recurrent networks.
2. Section 15.2 deals with different recurrent network architectures.
3. Sections 15.3 and 15.4 discuss theoretical aspects of recurrent networks,with an empha-
sis on the universal approximation theorem and on controllability and observability.
4. Section 15.5 addresses the computational power of recurrent networks.
5. Sections 15.6 to 15.8 are devoted to learning algorithms, with introductory remarks
in Section 15.6, followed by a treatment of two gradient-based algorithms: the back-
propagation-through-time algorithm in Section 15.7 and the real-time recurrent learn-
ing algorithm in Section 15.8.
6. Section 15.9 discusses the vanishing-gradient problem, which limits the practical
applicability of gradient-based recurrent learning algorithms; also discussed is how
to mitigate the problem through the use of second-order methods.
7. Section 15.10 describes the framework for solving the supervised training of a recur-
rent neural network (i.e.,estimating its synaptic weights) through the use of a sequen-
tial state estimator.A computer experiment follows in Section 15.11.
8. Section 15.12 addresses a limited form of adaptive behavior that is observed only in
recurrent neural networks after fixing the weights on the completion of supervised
training. To enhance this adaptive behavior, the structure of the network is corre-
spondingly expanded by including an adaptive critic.
9. Section 15.13 highlights a case study involving the use of a model-reference
neurocontroller.
The chapter concludes with a summary and discussion in Section 15.14.
15.1
INTRODUCTION
We begin the last chapter of the book with the following statement:
Global feedback is a facilitator of computational intelligence.
790
C H A P T E R  1 5
Dynamically Driven Recurrent
Networks

This statement is well illustrated by the study of recurrent networks as associative mem-
ories,which was presented in Chapter 13.Therein,we demonstrated how the use of global
feedback in a recurrent network makes it possible to achieve some useful tasks:
• content-addressable memory, exemplified by the Hopfield network;
• autoassociation, exemplified by Anderson’s brain-state-in-a-box model;
• dynamic reconstruction of a chaotic process, using feedback built around a regu-
larized one-step predictor.
In this chapter, we study the other important application of recurrent networks:
input–output mapping, the study of which naturally benefits from Chapter 14 on sequen-
tial state estimation. Consider, for example, a multilayer perceptron with a single hid-
den layer as the basic building block of a recurrent network. The application of global
feedback around the multilayer perceptron can take a variety of forms. We may apply
feedback from the outputs of the hidden layer of the multilayer perceptron to the input
layer.Alternatively, we may apply the feedback from the output layer to the input of the
hidden layer.We may even go one step further and combine all these possible feedback
loops in a single recurrent network structure. We may also, of course, consider other
neural network configurations as the building blocks for the construction of recurrent
networks.The important point is that recurrent networks have a very rich repertoire of
architectural layouts, which makes them all the more powerful in computational terms.
By definition, the input space of a mapping network is mapped onto an output
space. For this kind of application, a recurrent network responds temporally to an exter-
nally applied input signal.We may therefore speak of the recurrent networks considered
in this chapter as dynamically driven recurrent networks—hence the title of the chapter.
Moreover, the application of feedback enables recurrent networks to acquire state rep-
resentations, which makes them desirable tools for such diverse applications as nonlin-
ear prediction and modeling, adaptive equalization of communication channels, speech
processing, and plant control, to name just a few.
15.2
RECURRENT NETWORK ARCHITECTURES
As mentioned in the introduction, the architectural layout of a recurrent network takes
many different forms.In this section,we describe four specific network architectures,each
of which highlights a specific form of global feedback.1 They share the following com-
mon features:
• They all incorporate a static multilayer perceptron or parts thereof.
• They all exploit the nonlinear mapping capability of the multilayer perceptron.
Input–Output Recurrent Model
Figure 15.1 shows the architecture of a generic recurrent network that follows naturally
from a multilayer perceptron. The model has a single input that is applied to a tapped-
delay-line memory of q units.It has a single output that is fed back to the input via another
tapped-delay-line memory, also of q units. The contents of these two tapped-delay-line
memories are used to feed the input layer of the multilayer perceptron.The present value
Section 15.2
Recurrent Network Architectures
791

of the model input is denoted by un, and the corresponding value of the model output is
denoted by yn+1; that is, the output is ahead of the input by one time unit. Thus, the sig-
nal vector applied to the input layer of the multilayer perceptron consists of a data win-
dow made up of the following components:
• present and past values of the input, namely, un, un-1, ..., un-q+1, which represent
exogenous inputs originating from outside the network;
792
Chapter 15
Dynamically Driven Recurrent Networks
z1
z1
z1
z1
z1
z1
Multilayer
perceptron
Output
yn1
un1
un2
unq2
unq1
Input
un
ynq1
ynq2
yn1
yn
FIGURE 15.1
Nonlinear autoregressive
with exogenous inputs (NARX) model;
the feedback part of the network is shown
in red.

• delayed values of the output, namely, yn, yn1, ..., ynq1, on which the model out-
put yn1 is regressed.
The recurrent network of Fig. 15.1 is referred to as a nonlinear autoregressive with
exogenous inputs (NARX) model.2 The dynamic behavior of the NARX model is
described by
(15.1)
where F is a nonlinear function of its arguments. Note that in Fig. 15.1 we have assumed
that the two delay-line memories in the model are both of size q; they are generally dif-
ferent, however.
State-Space Model
Figure 15.2 shows the block diagram of another generic recurrent network,called a state-
space model, the basic idea of which was discussed in Chapter 14. The hidden neurons
define the state of the network. The output of the hidden layer is fed back to the input
layer via a bank of unit-time delays.The input layer consists of a concatenation of feed-
back nodes and source nodes.The network is connected to the external environment via
the source nodes.The number of unit-time delays used to feed the output of the hidden
layer back to the input layer determines the order of the model. Let the m-by-1 vector
un denote the input vector and the q-by-1 vector xn denote the output of the hidden
layer at time n.We may then describe the dynamic behavior of the model in Fig. 15.2 by
the following pair of equations:
(15.2)
(15.3)
where a( , ) is a nonlinear function characterizing the hidden layer and B is the matrix
of synaptic weights characterizing the output layer.The hidden layer is nonlinear,but the
output layer is linear.
The recurrent network of Fig. 15.2 includes several recurrent architectures as spe-
cial cases. For example, the simple recurrent network (SRN) described in Elman (1990,
1996), and depicted in Fig. 15.3, has an architecture similar to that of Fig. 15.2, except for
 yn = Bxn
 xn+1 = a(xn, un)
yn+1 = F(yn, ...., yn-q+1; un, ...., un-q+1)
Section 15.2
Recurrent Network Architectures
793
Bank of
q
unit-time
delays
Bank of
p
unit-time 
delays
Multilayer
perceptron
Linear
output
layer
xn
un
Input
vector
Output
vector
Multilayer perceptron with
single hidden layer
xn1
yn1
yn
FIGURE 15.2
State-space model; the feedback part of the model is shown in red.

the fact that the output layer may be nonlinear and the bank of unit-time delays at the
output is omitted. It is commonly referred to in the literature as a simple recurrent net-
work in the sense that the error derivatives computed by the recurrent network are
“simply” delayed by one time-step back into the past; however, this simplification does
not prevent the network from storing information from the distant past.
Elman’s network contains recurrent connections from the hidden neurons to a
layer of context units consisting of unit-time delays.These context units store the outputs
of the hidden neurons for one time-step and then feed them back to the input layer.
The hidden neurons thus have some record of their prior activations, which enables the
network to perform learning tasks that extend over time.The hidden neurons also feed
the output neurons that report the response of the network to the externally applied stim-
ulus. Due to the nature of the feedback around the hidden neurons, these neurons may
continue to recycle information through the network over multiple time-steps and
thereby discover abstract representations of time, hence the power of feedback.
Recurrent Multilayer Perceptrons
The third recurrent architecture considered here is known as a recurrent multilayer per-
ceptron (RMLP) (Puskorius et al.,1996).It has one or more hidden layers,basically for the
same reasons that static multilayer perceptrons are often more effective and parsimonious
than those using a single hidden layer. Each computation layer of an RMLP has feedback
around it, as illustrated in Fig. 15.4 for the case of an RMLP with two hidden layers.3
Let the vector xI,n denote the output of the first hidden layer, xII,n denote the out-
put of the second hidden layer, and so on. Let the vector xo,n denote the ultimate out-
put of the output layer. Then the general dynamic behavior of the RMLP in response
to an input vector un is described by a system of coupled equations given as
(15.4)
xo,n+1 = 	o(xo,n, xK,n+1)
o
xII,n+1 = 	II(xII,n, xI,n+1)
xI,n+1 = 	I(xI,n, un)
794
Chapter 15
Dynamically Driven Recurrent Networks
Bank of
unit-time
delays
Hidden
layer
Output
layer
Input
vector
Output
vector
Context
units
Multilayer perceptron with
single hidden layer
FIGURE 15.3
Simple recurrent network (SRN); the feedback part of the network is shown in red.

where 	I( , ), 	II( , )
	 o( , ) denote the activation functions characterizing the
first hidden layer, second hidden layer
and output layer of the RMLP, respectively,
, ...,
, ...,
Section 15.2
Recurrent Network Architectures
795
Multilayer perceptron with
multiple hidden layers
z1I
z1I
z1I
First
hidden
layer
Input
vector
Output
vector
Bank of
unit-time delays
Second
hidden
layer
Output
layer
xI, n1
xII, n1
xo, n1
xII, n
xo, n
xI, n
un
FIGURE 15.4
Recurrent multilayer perceptron; feedback paths in the network are printed in red.
and K denotes the number of hidden layers in the network; in Fig. 15.4, K  2.
The RMLP described herein subsumes the Elman network of Fig. 15.3 and the
state-space model of Fig. 15.2, since neither the output layer of the RMLP nor any of its
hidden layers is constrained to have a particular form of activation function.
Second-Order Network
In describing the state-space model of Fig. 15.2, we used the term “order” to refer to
the number of hidden neurons whose outputs are fed back to the input layer via a bank
of time-unit delays.
In yet another context, the term “order” is sometimes used to refer to the way in
which the induced local field of a neuron is defined. Consider, for example, a multilayer
perceptron where the induced local field vk of neuron k is defined by
(15.5)
where xj is the feedback signal derived from hidden neuron j and ui is the source signal
applied to node i in the input layer; the w’s represent the pertinent synaptic weights in
the network.We refer to a neuron described in Eq. (15.5) as a first-order neuron.When,
however, the induced local field vk is combined using multiplications, as shown by
(15.6)
we refer to the neuron as a second-order neuron.The second-order neuron k uses a sin-
gle weight 
that connects it to the input nodes i and j.
Second-order neurons constitute the basis of second-order recurrent networks (Giles
et al.,1990),an example of which is shown in Fig.15.5.The network accepts a time-ordered
sequence of inputs and evolves with dynamics defined by the pair of equations:
(15.7)
vk, n = bk + a
i
a
j
wkijxi, nuj,n
wkji
vk = a
i
a
j
wkijxiuj
vk = a
j
wa, kjxj + a
i
wb, kiui

(15.8)
where vk,n is the induced local field of hidden neuron k, bk is the associated bias, xk,n is
the state (output) of neuron k, uj,n is the input applied to source node j, and wkij is a
weight of second-order neuron k.
Unique features of the second-order recurrent network in Fig. 15.5 are that the
product xj,nuj,n represents the pair {state, input} and that a positive weight 
repre-
sents the presence of the state transition, {state, input}
{next state}, while a negative
weight represents the absence of the transition.The state transition is described by
(15.9)
(xi, uj) = xk
S
wkij
 =
1
1 + exp(-vk, n)
 xk, n + 1 = (vk, n)
796
Chapter 15
Dynamically Driven Recurrent Networks
Inputs
Second-order
weights wkji
Unit-time delays
Multipliers
Neurons
Output
x1, n1
x2, n1
x3, n1
z1
z1
z1
u1, n
u2, n
FIGURE 15.5
Second-order recurrent network; bias connections to the neurons are omitted
to simplify the presentation.The network has 2 inputs and 3 state neurons, hence the need for
3  2  6 multipliers. The feedback links in the figure are printed in red to emphasize their
global role.

In light of this relationship, second-order networks are readily used for representing
and learning deterministic finite-state automated (DFA)4; a DFA is an information-
processing system with a finite number of states. More information on the relationship
between neural networks and automata is given in Section 15.5.
15.3
UNIVERSAL APPROXIMATION THEOREM
The notion of state plays a vital role in the mathematical formulation of a dynamic sys-
tem, as explained in detail in Chapter 14. To recap, the state of a dynamic system is
defined as a set of quantities that summarizes all the information about the past behav-
ior of the system that is needed to uniquely describe its future behavior, except for the
purely external effects arising from the applied input (excitation). Let the q-by-1 vector
xn denote the state of a nonlinear discrete-time system, the m-by-1 vector un denote the
input applied to the system, and the p-by-1 vector yn denote the corresponding output
of the system. Consider a recurrent network whose dynamic behavior, assumed to be
noise free, is described by the pair of nonlinear equations
(15.10)
(15.11)
where Wa is a q-by-q matrix, Wb is a q-by-m matrix, Wc is a p-by-q matrix, and 
q
q is a diagonal map described by
(15.12)
for some memoryless, componentwise nonlinearity 
. The spaces 
m,
q, and
p are called the input space, state space, and output space, espectively.The dimension-
ality of the state space—namely, q—is the order of the system. Thus, the state-space
model of Fig. 15.2 is an m-input, p-output recurrent model of order q. Equation (15.10)
is the system (state) equation of the model, and Eq. (15.11) is the measurement equation.
The system (state) equation of Eq. (15.10) is a special form of Eq. (15.2).
The recurrent network of Fig. 15.2, based on the use of a static multilayer per-
ceptron and two delay-line memories, provides a method for implementing the non-
linear feedback system described by Eqs. (15.10) to (15.12). Note that in Fig. 15.2,
only those neurons in the multilayer perceptron that feed back their outputs to the input
layer via delays are responsible for defining the state of the recurrent network. This
statement therefore excludes the neurons in the output layer from the definition of
the state.
For the interpretation of matrices Wa, Wb, and Wc and nonlinear vectorial function
	(·), we may say the following:
• The matrix Wa represents the synaptic weights of the q neurons in the hidden layer
that are connected to the feedback nodes in the input layer. The matrix Wb rep-
resents the synaptic weights of these hidden neurons that are connected to the




S

:
	: ≥
x1
x2
o
xq
¥ S ≥
(x1)
(x2)
o
(xq)
¥

S

	:
yn = Wcxn
 xn+1 = 	(Waxn + Wbun)
Section 15.3
Universal Approximation Theorem
797

source nodes in the input layer.To simplify the composition of Eq. (15.10), the use
of bias has been excluded from the state model.
• The matrix Wc represents the synaptic weights of the p linear neurons in the out-
put layer that are connected to the hidden neurons. Here again, the use of bias in
the output layer has been ignored to simplify the presentation.
• The nonlinear function (·) represents the sigmoidal activation function of a hid-
den neuron.This activation function typically takes the form of a hyperbolic tan-
gent function,
(15.13)
or a logistic function,
(15.14)
An important property of a recurrent neural network described by the state-space
model of Eqs. (15.10) and (15.11) is that it is a universal approximator of all nonlinear
dynamic systems. Specifically, we may make the following statement (Lo, 1993):
Any nonlinear dynamic system may be approximated by a recurrent neural network to
any desired degree of accuracy and with no restrictions imposed on the compactness of the
state space, provided that the network is equipped with an adequate number of hidden
neurons.
Indeed, this profound statement on universal approximation is testimony to the
computing power of recurrent neural networks for signal-processing and control
applications.
EXAMPLE 1
Fully Connected Recurrent Network
To illustrate the compositions of matrices Wa, Wb and Wc, consider the fully connected recurrent
network shown in Fig. 15.6, where the feedback paths originate from the hidden neurons In this
example, we have m  2, q  3, and p  1.The matrices Wa and Wb are defined as
and
where the first column of Wb, consisting of b1, b2, and b3, represents the bias terms applied to neu-
rons 1, 2, and 3, respectively.The matrix Wc is a row vector defined by
■
Wc = [1, 0, 0]
Wb = £
b1
w14
w15
b2
w24
w25
b3
w34
w35
§
Wa = £
w11
w12
w13
w21
w22
w23
w31
w32
w33
§
(x) =
1
1 + e-x
(x) = tanh(x) = 1 - e-2x
1 + e-2x

798
Chapter 15
Dynamically Driven Recurrent Networks

15.4
CONTROLLABILITY AND OBSERVABILITY
As mentioned earlier, many recurrent networks can be represented by the state-space
model shown in Fig. 15.2, where the state is defined by the output of the hidden layer
fed back to the input layer via a set of unit-time delays. In this context, it is insightful to
know whether the recurrent network is controllable and observable or not. Controlla-
bility is concerned with whether we can control the dynamic behavior of the recurrent
network. Observability is concerned with whether we can observe the result of the con-
trol applied to the recurrent network.
Formally,a dynamic system is said to be controllable if any initial state of the system
is steerable to any desired state within a finite number of time-steps;the output of the sys-
tem is irrelevant to this definition. Correspondingly, the system is said to be observable if
the state of the system can be determined from a finite set of input–output measurements.
Section 15.4
Controllability and Observability
799
z1
z1
z1
z1
Bias
Input
layer
Computation
layer
1
2
3
Inputs
u1, n
x1, n
x2, n
x3, n
u2, n
x1, n1
x2, n1
x3, n1
Output
yn
Unit-time delays
FIGURE 15.6
Fully connected recurrent network with two inputs, two hidden neurons, and one
output neuron. The feedback connections are shown in red to emphasize their global role.

The concepts of controllability and observability have been thoroughly studied in linear
system theory.5 Accordingly, in the study of recurrent neural networks presented herein,
we confine our attention to local controllability and local observability—local in the sense
that both properties apply in the neighborhood of an equilibrium state of the network;the
equilibrium state was discussed in detail in Chapter 13.
A state 
is said to be an equilibrium state of Eq. (15.10) if, for an input u, and a
matrix A1 to be defined, it satisfies the following condition:
(15.15)
To simplify the exposition, the equilibrium state is described by the condition
for x  0,
In other words, the equilibrium point is represented by the origin, (0, 0).
Also without loss of generality, we may simplify the exposition by limiting our
selves to a single-input–single-output (SISO) system.We may then rewrite Eqs. (15.10)
and (15.11) as, respectively,
(15.16)
and
(15.17)
where both wb and wc are q-by-1 vectors, un is the scalar input, and yn is the scalar out-
put. Since 
is continuously differentiable for the sigmoid function of Eq. (15.13) or that
of Eq. (15.14), we may linearize Eq. (15.16) by expanding it as a Taylor series around the
equilibrium point 
and 
and retaining first-order terms, as shown by
(15.18)
where 
and 
are small displcements applied to the state and input,respectively,and
un
xn
xn+1 = (0)Waxn + (0)wbun
u = 0
x = 0

yn = wc
Txn
xn+1 = 	(Waxn + wbun)
0 = 	(0) 
x = A1x
x
800
Chapter 15
Dynamically Driven Recurrent Networks
the q-by-q matrix 
is the Jacobian of (v) with respect to its argument v, evaluated
at v = 0.We may thus describe the linearized system by writing
(15.19)
and
(15.20)
where the q-by-q matrix A1, and the q-by-1 vector a2 are respectively defined by
(15.21)
and
(15.22)
The state equations of Eqs. (15.19) and (15.20) are in the standard linear form.We may
therefore make use of well-known results on the controllability and observability of lin-
ear dynamic systems that are a standard part of mathematical control theory.
a2 = (0)wb
A1 = (0)Wa
yn = wc
Txn
xn+1 = A1xn + a2un
	
(0)

Local Controllability
We readily find that repeated use of the linearized equation of Eq.(15.19) yields the following
equations
where q is the dimensionality of the state space.Accordingly, we may make the follow-
ing statement (Levin and Narendra, 1993):
The linearized system represented by Eq. (15.19) is controllable if the matrix
(15.23)
is of rank q—that is, full rank—because then the linearized system of Eqs. (15.23) has a
unique representation of xnq in terms of un, un1
unq1, given A1, a2, and xn.
The matrix Mc is called the controllability matrix of the linearized system.
Let the recurrent network described by Eqs. (15.16) and (15.17) be driven by a
sequence of inputs uq,n defined by
(15.24)
Hence, we may consider the mapping
(15.25)
where G:
2q S
2q. In Problem 15.4, it is shown that:
• the state xn+q is a nested nonlinear function of its past value xn and the inputs un,
un+1
un+q-1, and
• the Jacobian of xn+q with respect to uq,n, evaluated at the origin, is equal to the con-
trollability matrix Mc of Eq. (15.23).
We may express the Jacobian of the mapping G, defined in Eq. (15.25), with respect to
xn and uq,n, evaluated at the origin (0, 0), as
(15.26)
where I is the identity matrix, 0 is the null matrix, and the entry X is of no interest.
Because of its special form, the determinant of the Jacobian J(c)
(0,0) is equal to the prod-
uct of the determinant of the identity matrix I (which equals 1) and the determinant of
the controllability matrix Mc. If Mc is of full rank, then so is J(c)
(0, 0).
 = c I
X
0
Mc
d
 J (0, 0)
(c)
= ≥
axn
xn
b
(0, 0)
a
xn+q
xn
b
(0, 0)
a xn
uq,n
b
(0, 0)
a
xn+q
uq,n
b
(0, 0)
¥
, ...,


G(xn, uq,n) = (xn, xn+q)
uq,n = [un, un+1, ..., un+q-1]T
, ...,
Mc = [A1
q-1a2, ..., A1 a2, a2]
 xn+q = A1
qxn + A1
q-1a2un + p + A1a2un+q-2 + a2un+q-1
 o
 xn+2 = A1
2xn + A1a2un + a2un+1
 xn+1 = A1xn + a2un
Section 15.4
Controllability and Observability
801

To proceed further, we need to invoke the inverse function theorem, which may be
stated as follows (Vidyasagar, 1993):
Consider the mapping f:
q S
q. Suppose that each component of the mapping f is differ-
entiable with respect to its argument at the equilibrium point x0 
q, and let y0 = f(x0).Then
there exist open sets 
containing x0 and 
containing y0 such that f is a diffeo-
morphism of 
onto 
. If, in addition, f is smooth, then the inverse mapping f -1:
S
is
also smooth—that is, f is a smooth diffeomorphism.
The mapping 
is said to be a diffeomorphism of 
onto 
if it satisfies the
following three conditions (see Chapter 7):
1.
.
2. The mapping 
is one to one (i.e., invertible).
3. Each component of the inverse mapping 
is continuously differen-
tiable with respect to its argument.
Returning to the issue of controllability, we may identify 
in the inverse
function theorem with the mapping defined in Eq. (15.25). By using the inverse function
theorem, we may say that if the controllability matrix Mc is of rank q, then locally there
exists an inverse mapping defined by
(15.27)
Equation (15.27),in effect,states that there exists an input sequence (uq,n) that can locally
drive the network from state xn to state xn + q in q time-steps.Accordingly,we may formally
state the local controllability theorem as follows (Levin and Narendra, 1993):
Let a recurrent network be defined by Eqs. (15.16) and (15.17), and let its linearized version
around the origin (i.e., equilibrium point) be defined by Eqs.(15.19) and (15.20).If the linearized
system is controllable, then the recurrent network is locally controllable around the origin.
Local Observability
Using the linearized equations of Eqs. (15.19) and (15.20) repeatedly, we may write
where q is the dimensionality of the state space.Accordingly, we may make the follow-
ing statement (Levin and Narendra, 1993):
The linearized system described by Eqs. (15.19) and (15.20) is observable if the matrix
(15.28)
is of rank q—that is, full rank.
Mo = [wc, wcA1
T, ...., wc(A1
T)q-1]
 + wc
Ta2un+q-2
 yn+q-1 = wc
TA1
q-1xn + wc
TA1
q-2a2un + p + wc
TA1a2un+q-3
 o
 = wc
TA1xn + wc
Ta2un
 yn+1 = wc
Txn+1
 yn = wc
Txn
(xn, xn+q) = G-1(xn, uq,n)
f(u) = v
f-1: v S u
f: u S v
f(u) = v
v
u
f: u S v
q
q
v
u
v 8 q
u 8 q



802
Chapter 15
Dynamically Driven Recurrent Networks

The matrix Mo is called the observability matrix of the linearized system.
Let the recurrent network described by Eqs. (15.16) and (15.17) be driven by a
sequence of inputs defined by
(15.29)
Correspondingly, let
(15.30)
denote the vector of outputs produced by the initial state xn and the sequence of inputs
uq-1, n.We may then consider the mapping
(15.31)
where H:
2q-1
2q-1.In Problem 15.5,it is shown that the Jacobian of yq,n with respect to
xn,evaluated at the origin,is equal to the observability matrix Mo of Eq.(15.28).We may thus
express the Jacobian of H with respect to uq-1,n and xn,evaluated at the origin,(0, 0),as
(15.32)
where again the entry X is of no interest.The determinant of the Jacobian J(o)
(0,0) is equal
to the product of the determinant of the identity matrix I (which equals 1) and the deter-
minant of Mo. If Mo is of full rank, then so is J(o)
(0,0). Invoking the inverse function theo-
rem one more time, we may therefore say that if the observability matrix Mo of the
linearized system is of full rank, then locally there exists an inverse mapping defined by
(15.33)
In effect, this equation states that in the local neighborhood of the origin, xn is some
nonlinear function of both uq-1,n and yq,n, and that nonlinear function is an observer of
the recurrent network.We may therefore formally state the local observability theorem
as follows (Levin and Narendra, 1993):
Let a recurrent network be defined by Eqs. (15.16) and (15.17), and let its linearized version
around the origin (i.e., equilibrium point) be defined by Eqs.(15.19) and (15.20).If the linearized
system is observable, then the recurrent network is locally observable around the origin.
EXAMPLE 2
Controllability and Observability of a Simple State-Space Model
Consider a state-space model with matrix A1 = aI, where a is a scalar and I is the identity matrix.
Then the controllability matrix Mc of Eq. (15.23) reduces to
The rank of this matrix is 1. Hence, the linearized system with this value of matrix A1 is not
controllable.
Mc = a[a2, ...., a2, a2]
(uq-1, n, xn) = H-1(uq-1, n, yq, n)
 = c I
X
0
Mo
d
 J(o)
(0,0) = ≥
a
0uq-1,n
0uq-1,n
b
(0, 0)
a
0yq,n
0uq-1,n
b
(0, 0)
a
0uq-1,n
xn
b
(0, 0)
a
0yq,n
xn
b
(0, 0)
¥
S 

H(uq-1,n, xn) = (uq-1,n, yq,n)
yq, n = [yn, yn+1, ...., yn+q-1]T
uq-1, n = [un, un+1, ...., un+ q-2]T
Section 15.4
Controllability and Observability
803

Putting A1 = aI in Eq. (15.28), we obtain the observability matrix
whose rank is also 1.The linearized system is also not observable.
■
15.5
COMPUTATIONAL POWER OF RECURRENT NETWORKS
Recurrent networks, exemplified by the state-space model of Fig. 15.2 and the NARX
model of Fig.15.1,have an inherent ability to simulate finite-state automata.Automata rep-
resent abstractions of information-processing systems such as computers.Indeed,automata
and neural networks share a long history.6 On p.55 of his 1967 book,entitled Computation:
Finite and Infinite Machines, Minsky makes the following consequential statement:
“Every finite-state machine is equivalent to, and can be ‘simulated’ by, some neural net.That
is, given any finite-state machine 
, we can build a certain neural net 
which, regarded as
a black-box machine, will behave precisely like 
!”
The early work on recurrent networks used hard threshold logic for the activation func-
tion of a neuron rather than soft sigmoid functions.
Perhaps the first experimental demonstration of whether a recurrent network could
learn the contingencies implied by a small finite-state grammar was reported in Cleere-
mans et al. (1989). Specifically, the simple recurrent network (see Fig. 15.3) was presented
with strings derived from the grammar and required to predict the next letter at every step.
The predictions were context-dependent,since each letter appeared twice in the grammar
and was followed in each case by different successors. It was shown that the network is
able to develop internal representations in its hidden neurons that correspond to the states
of the automaton (finite-state machine).In Kremer (1995),it is formally proved that the sim-
ple recurrent network has a computational power as great as that of any finite-state machine.
In a generic sense, the computational power of a recurrent network is embodied
in two main theorems:
Theorem I
(Siegelmann and Sontag, 1991)
All Turing machines may be simulated by fully connected recurrent networks built on neu-
rons with sigmoidal activation functions.
The Turing machine is an abstract computing device invented by Turing in 1936; its
mathematical model is more general than that of finite-state automata. Hence, simulation
of the Turing machine by a recurrent network is a more challenging proposition.The machine
consists of three functional blocks, as depicted in Fig. 15.7 (Fischler and Firschein, 1987):
1.
a control unit, which can assume any one of a finite number of possible states;
2.
linear tape, assumed to be infinitely long in both directions, which is marked off into
discrete squares, where each square is available to store a single symbol taken from a
finite set of symbols;
3.
a read–write head, which moves along the tape and transmits information to and from
the control unit.
A function f(x) is said to be computable if there exists a Turing machine that,given a tape
representing the argument x, eventually comes to a stop when the tape represents the
m
nm
m
Mo = a[wc, wc, ..., wc]
804
Chapter 15
Dynamically Driven Recurrent Networks

value f(x). However, this idea is rather problematic, because the idea of computation
lacks a formal definition. Nevertheless, the Church–Turing thesis, which states that the
Turing machine is capable of computing any computable function, is generally accepted
as a sufficient condition (Russell and Norvig, 1995).
Theorem II
(Siegelmann et al., 1997)
NARX networks with one layer of hidden neurons with bounded, one-sided saturated activa-
tion functions and a linear output neuron can simulate fully connected recurrent networks
with bounded, one-sided saturated activation functions, except for a linear slowdown.
A “linear slowdown” means that if the fully connected recurrent network with N neu-
rons computes a task of interest in time T, then the total time taken by the equivalent
NARX network is (N  1)T.A function () is said to be a bounded, one-sided saturated
(BOSS) function if it satisfies the following three conditions:
1. The function () has a bounded range; that is, a  (x)  b, a Z b, for all x
.
2. The function () is saturated on the left side; that is, there exist values s and S such that
(x)  S for all x  s.
3. The function () is nonconstant; that is, (x1) Z (x2) for some x1 and x2.
The threshold (Heaviside) and piecewise-linear functions satisfy the BOSS conditions.
However, in a strict sense, a sigmoid function is not a BOSS function because it does not
satisfy condition 2. Nevertheless, with a minor modification, it can be made into a BOSS
function by writing (in the case of a logistic function)
where x
. In effect, the logistic function is truncated for x  s.
As a corollary to Theorems I and II, we may state the following (Giles, 1996):
NARX networks with one hidden layer of neurons with BOSS activations functions and a
linear output neuron are Turing equivalent.
Figure 15.8 presents a portrayal of Theorems I and II and this corollary. It should,
however, be noted that when the network architecture is constrained, the computational


(x) = •
1
1 + exp (-x) 
 for x 7 s
0
for x  s


Section 15.5
Computational Power of Recurrent Networks
805
Linear tape
Control
unit
Read-write
head
Square for
storing a
symbol
Movement
of head
FIGURE 15.7
Turing machine.

power of a recurrent network may no longer hold, as described in Sperduti (1997). Ref-
erences to examples of constrained network architectures are presented in Note 7 under
Notes and References.
15.6
LEARNING ALGORITHMS
We now turn to the issue of training recurrent networks. From Chapter 4, we recall that
there are two modes of training an ordinary (static) multilayer perceptron: batch mode
and stochastic (sequential) mode. In the batch mode, the sensitivity of the network is
computed for the entire training sample before adjusting the free parameters of the net-
work. In the stochastic mode, on the other hand, parameter adjustments are made after
the presentation of each pattern in the training sample. Likewise, we have two modes
of training a recurrent network, described as follows (Williams and Zipser, 1995):
1. Epochwise training. For a given epoch, the recurrent network uses a temporal
sequence of input–target response pairs and starts running from some initial state until it
reaches a new state, at which point the training is stopped and the network is reset to an
initial state for the next epoch.The initial state doesn’t have to be the same for each epoch
of training. Rather, what is important is for the initial state for the new epoch to be differ-
ent from the state reached by the network at the end of the previous epoch. Consider, for
example,the use of a recurrent network to emulate the operation of a finite-state machine.
In such a situation, it is reasonable to use epochwise training, since there is a good possi-
bility that a number of distinct initial states and a set of distinct final states in the machine
will be emulated by the recurrent network. In epochwise training for recurrent networks,
the term “epoch” is used in a sense different from that for an ordinary multilayer percep-
tron.Although an epoch in the training of a multilayer perceptron involves the entire train-
ing sample of input–target response pairs, an epoch in the training of a recurrent neural
network involves a single string of temporally consecutive input–target response pairs.
2. Continuous training. This second method of training is suitable for situations
where there are no reset states available or on-line learning is required.The distinguishing
feature of continuous training is that the network learns while performing signal process-
ing.Simply put,the learning process never stops.Consider,for example,the use of a recur-
rent network to model a nonstationary process such as a speech signal. In this kind of
situation,continuous operation of the network offers no convenient times at which to stop
the training and begin anew with different values for the free parameters of the network.
806
Chapter 15
Dynamically Driven Recurrent Networks
Turing
machine
Fully connected
recurrent
network
NARX
network
FIGURE 15.8
Illustration of Theorems I and II,
and corollary to them.

Keeping these two modes of training in mind, in the next two sections we will
describe two different learning algorithms for recurrent networks,summarized as follows:
• The back-propagation-through-time (BPTT) algorithm, discussed in Section 15.7,
operates on the premise that the temporal operation of a recurrent network may
be unfolded into a multilayer perceptron.This condition would then pave the way
for application of the standard back-propagation algorithm.The back-propagation-
through-time algorithm can be implemented in the epochwise mode, continuous
(real-time) mode, or a combination thereof.
• The real-time recurrent learning (RTRL) algorithm, discussed in Section 15.8, is
derived from the state-space model described by Eqs. (15.10) and (15.11).
Basically, BPTT and RTRL involve the propagation of derivatives, one in the backward
direction and the other in the forward direction.They can be used in any training process that
requires the use of derivatives. BPTT requires less computation than RTRL does, but the
memory space required by BPTT increases fast as the length of a sequence of consecutive
input–target response pairs increases.Generally speaking,we therefore find that BPTT is bet-
ter for off-line training, and RTRL is more suitable for on-line continuous training.
In any event, these two algorithms share many common features. First, they are
both based on the method of gradient descent,whereby the instantaneous value of a cost
function (based on a squared-error criterion) is minimized with respect to the synaptic
weights of the network. Second, they are both relatively simple to implement, but can
be slow to converge.Third, they are related in that the signal-flow graph representation
of the back-propagation-through-time algorithm can be obtained from transposition of
the signal-flow graph representation of a certain form of the real-time recurrent learn-
ing algorithm (Lefebvre, 1991; Beaufays and Wan, 1994).
Some Heuristics
Before proceeding to describe these two learning algorithms, we list some heuristics for
the improved training of recurrent networks that involve the use of gradient-descent
methods (Giles, 1996):
• Lexigraphic order of training samples should be followed, with the shortest strings
of symbols being presented to the network first.
• The training should begin with a small training sample. The size of the training
sample should be incrementally increased as the training proceeds.
• The synaptic weights of the network should be updated only if the absolute error
on the training sample currently being processed by the network is greater than
some prescribed criterion.
• The use of weight decay during training is recommended; weight decay, a crude
form of complexity regularization, was discussed in Chapter 4.
The first heuristic is of particular interest. If implementable, it may provide a pro-
cedure for alleviating the vanishing-gradients problem that arises in recurrent networks
trained by means of gradient-descent methods. Detailed discussion of this problem is
deferred to Section 15.9.
Section 15.6
Learning Algorithms
807

15.7
BACK PROPAGATION THROUGH TIME
The back-propagation-through-time (BPTT) algorithm for training a recurrent network
is an extension of the standard back-propagation algorithm.8 It may be derived by
unfolding the temporal operation of the network into a layered feedforward network,
the topology of which grows by one layer at every time-step.
To be specific, let 
denote a recurrent network required to learn a temporal task,
starting from time n0 all the way up to time n. Let 
denote the feedforward network
that results from unfolding the temporal operation of the recurrent network 
. The
unfolded network 
is related to the original network 
as follows:
1. For each time-step in the interval (n0, n], the network 
has a layer containing K
neurons, where K is the number of neurons contained in the network 
.
2. In every layer of the network 
, there is a copy of each neuron in the network .
3. For each time-step l
[n0, n], the synaptic connection from neuron i in layer l to
neuron j in layer l + 1 of the network 
is a copy of the synaptic connection from
neuron i to neuron j in the network 
.
These points are illustrated in the following example.
EXAMPLE 3
Unfolding of two-neuron recurrent network
Consider the two-neuron recurrent network 
shown in Fig. 15.9a.To simplify the presentation,
we have omitted the unit-time delay operators z-1 that should be inserted in each of the synaptic
connections (including the self-loops) in Fig. 15.9a. By unfolding the temporal operation of this
network in a step-by-step manner, we get the signal-flow graph shown in Fig. 15.9b, where the
starting time n0 = 0.The graph in Fig.15.9b represents the layered feedforward network 
, where
a new layer is added at each step of the temporal operation.
■
n*
n
n
n*

n
n*
n
n*
n
n*
n
n*
n
808
Chapter 15
Dynamically Driven Recurrent Networks
(a)
(b)
w21
w12
w22
w11
Time-step
0
1
2
n
x1, 0
x1
x2
x2, 0
x2, 1
x2, 2
x1, 1
x1, 2
w11
w12
w21
w12
w21
w22
w22
w11
x1, n
w12
w21
w22
x2, n
x2, n1
x1, n1
w11
n  1
FIGURE 15.9
(a) Architectural graph of a two-neuron recurrent network 
. (b) Signal-flow
graph of the network 
unfolded in time.
n
n

Application of the unfolding procedure leads to two basically different imple-
mentations of back propagation through time, depending on whether epochwise train-
ing or continuous (real-time) training is used.These two methods of recurrent learning
are now described in that order.
Epochwise Back Propagation Through Time
Let the data set used to train a recurrent network be partitioned into independent
epochs, with each epoch representing a temporal pattern of interest. Let n0 denote the
start time of an epoch and n1 denote its end time. Given this epoch, we may define the
cost function
(15.34)
where 
is the set of indices j pertaining to those neurons in the network for which
desired responses are specified, and ej,n is the error signal at the output of such a neu-
ron measured with respect to some desired (target) response.We wish to compute sen-
sitivity of the network—that is, the partial derivatives of the cost function 
with respect to synaptic weights of the network. To do so, we may use the epochwise
back-propagation-through-time (BPTT) algorithm, which builds on the batch mode of
standard back-propagation learning that was described in Chapter 4. The epochwise
BPTT algorithm proceeds as follows (Williams and Peng, 1990):
• First, a single forward pass of the data through the network for the interval
(n0, n1) is performed. The complete record of input data, network state (i.e.,
synaptic weights of the network), and desired responses over this interval is
saved.
• A single backward pass over this past record is performed to compute the values
of the local gradients
(15.35)
for all 
and 
.This computation is performed by using the formula
(15.36)
where 
(·) is the derivative of an activation function with respect to its argu-
ment and 
j,n is the induced local field of neuron j. It is assumed that all neurons
in the network have the same activation function (·). The use of Eq. (15.36) is
repeated, starting from time n1 and working back, step by step, to time n0; the
number of steps involved here is equal to the number of time-steps contained in
the epoch.
• Once the computation of back propagation has been performed back to time n0 + 1,
an adjustment is applied to the synaptic weight 
ji of neuron j, given by
w

v
¿
j,n = •
¿(vj,n) ej,n
¿(vj,n)cej,n + a
ka
wjkk,n+1d
   for n = n1
   for n0 6 n 6 n1
n0 6 n  n1
j  a
j,n = - 0etotal
0vj,n
etotal(n0, n1)
a
etotal = 1
2 a
n1
n=n0 a
jHa
ej,n
2
Section 15.7
Back Propagation Through Time
809

(15.37)
where  is the learning-rate parameter and xi,n-1 is the input applied to the ith
synapse of neuron j at time n - 1.
Comparing the procedure just described for epochwise BPTT with the batch mode of
standard back-propagation learning,we see that the basic difference between them is that
in the former case, the desired responses are specified for neurons in many layers of the
network because the actual output layer is replicated many times when the temporal
behavior of the network is unfolded.
Truncated Back Propagation Through Time
To use back propagation through time in a real-time fashion, we use the instantaneous
value of the sum of squared errors, namely,
as the cost function to be minimized. As with the stochastic mode of standard back-
propagation learning, we use the negative gradient of the cost function 
to com-
pute the appropriate adjustments to the synaptic weights of the network at each time
instant n. The adjustments are made on a continuous basis while the network is run-
ning. However, in order to do this in a computationally feasible manner, we save the
relevant history of input data and network state only for a fixed number of time-steps,
called the truncation depth. Henceforth, the truncation depth is denoted by h.Any infor-
mation older than h time-steps into the past is considered irrelevant and may therefore
be ignored. If we were not to truncate the computation, thereby permitting it to go back
to the starting time, the computation time and storage requirement would grow linearly
with time as the network runs, eventually reaching a point where the whole learning
process becomes impractical.
This second form of the algorithm is called the truncated back-propagation-through-
time (BPTT(h)) algorithm (Williams and Peng, 1990).The local gradient for neuron j is
now defined by
(15.38)
which, in turn, leads to the formula
(15.39)
j, l = •
¿(vj,l)ej,l
¿(vj,l) a
ka
wjk,lk, l+1
   
for l = n
for n - h 6 l 6 n
j, l = - 0el
0vj,l
   for all j  a
and n - h 6 l  n
en
en = 1
2 a
ja
ej,n
2
= 
a
n1
n = n0+1
j,nxi,n-1
¢wji = - 0etotal
0wji
810
Chapter 15
Dynamically Driven Recurrent Networks

Once the computation of back propagation has been performed back to time n - h + 1,
the following adjustment is applied to the synaptic weight 
ji of neuron j, where  and
xi,l - 1 are as defined previously:
(15.40)
Note that the use of 
jk,l in Eq. (15.39) requires that a history of weight values be main-
tained. The use of 
jk,l in this equation may be justified only if the learning-rate 
parameter  is small enough to ensure that the weight values do not change significantly
from one time-step to the next.
In comparing Eq. (15.39) with (15.36), we see that, unlike the epochwise BPTT algo-
rithm, the error signal is injected into the computation only at the current time n. This
explains the reason for not keeping a record of past values of the desired responses.In effect,
the truncated back-propagation-through-time algorithm treats the computation for all ear-
lier time-steps similar to the way in which the stochastic back-propagation algorithm (dis-
cussed in Chapter 4) treats the computations for hidden neurons in a multilayer perceptron.
Some Practical Considerations
In real-life applications of BPTT(h), the use of truncation is not as artificial as it may
sound. Unless the recurrent network is unstable, there should be a convergence of the
derivatives 
because computations farther back in time correspond to higher
powers of feedback strengths (roughly equal to sigmoid slopes multiplied by weights).
In any event, the truncation depth h must be large enough to produce derivatives that
closely approximate the actual values. This requirement places a lower bound on the
value of h. For example, in the application of dynamically driven recurrent networks to
engine idle-speed control, the value h = 30 is considered to be a reasonably conserva-
tive choice for that learning task to be accomplished (Puskorius et al., 1996).
The Ordered Derivative Approach
One other practical matter needs to be discussed.The unfolding procedure described in this
section for back propagation through time provides a useful tool for picturing the algorithm
in terms of a cascade of similar layers progressing forward in time, thereby helping us to
develop an understanding of how the procedure functions. This strong point is, unfortu-
nately, also the cause of its weakness. The procedure works perfectly fine for relatively
simple recurrent networks consisting of a few neurons.However,the underlying formulas,
particularly Eq.(15.39),become unwieldy when the unfolding procedure is applied to more
general architectures that are typical of those encountered in practice.In situations of this
kind, the preferred procedure is to use the more general approach described in Werbos
(1990), in which each expression in the forward propagation of a layer gives rise to a cor-
responding set of back-propagation expressions.An advantage of this approach is its homo-
geneous treatment of forward and recurrent (feedback) connections.
To describe the mechanics of this latter form of BPTT(h), let 
denote an
ordered derivative of the network output at node l with respect to x.To derive the back-
propagation equations, the forward-propagation equations are considered in reverse
F l
-x
0el0vj,l
w
w
¢wji,n = 
a
n
j=n-h+1
j, l xi,l-1
w
Section 15.7
Back Propagation Through Time
811

order. From each equation, we derive one or more back-propagation expressions
according to the following principle:
(15.41)
Example 4
Illustration of Eq. (15.41)
To clarify the notion of ordered derivatives, consider a nonlinear system described by the fol-
lowing pair of equations:
The variable x2 influences the output y in two ways:directly via the second equation,and indirectly
via the first equation.The ordered derivative of y with respect to x2 is defined by the total causal
impact that includes the direct and indirect effects of x2 on y, as shown by
■
Other Desirable Features of the Ordered Derivative Approach
In programming the ordered derivatives for BPTT(h),the quantity on the right-hand side
of each ordered derivative in Eq. (15.41) is added to the previous value of the left-hand
side. In this way, the appropriate derivatives are distributed from a given node in the net-
work to all the nodes and synaptic weights that feed it in the forward direction, with
due allowance being made for any delays that may be present in each connection.
The simplicity of the ordered-derivative formulation described in Eq. (15.41) also
reduces the need for visualizations such as unfolding in time or signal-flow graphs. In
Feldkamp and Puskorius (1998) and Puskorius et al. (1996), this procedure is used to
develop a pseudocode for implementing the BPTT(h) algorithm.
15.8
REAL-TIME RECURRENT LEARNING
In this section, we describe the second learning algorithm, real-time recurrent learning
(RTRL),9 which was briefly described in Section 15.6. The algorithm derives its name
from the fact that adjustments are made to the synaptic weights of a fully connected
recurrent network in real time—that is, while the network continues to perform its 
signal-processing function (Williams and Zipser, 1989). Figure 15.10 shows the layout of
such a recurrent network. It consists of q neurons with m external inputs.The network
has two distinct layers: a concatenated input-feedback layer and a processing layer of
computation nodes. Correspondingly, the synaptic connections of the network are made
up of feedforward and feedback connections; the feedback connections are shown in
red in Fig. 15.10.
 = 3 + 6x1x2
2
 = 3 + (2x1)(3x2
2)
 F-x2 = 0y
0x2
+ 0y
0x1
* 0x1
0x2
 y = x1
2 + 3x2
 x1 = log u + x2
3
If a = (b, c), then F-b
l
= 0
0b F-a
l  and F-c
l
= 0
0c F-a
l
812
Chapter 15
Dynamically Driven Recurrent Networks

The state-space description of the network is defined by Eqs. (15.10) and (15.11).
The system (state) equation of Eq. (15.10) is reproduced here in the expanded form
(15.42)
where it is assumed that all the neurons have a common activation function (·). The
(q + m + 1)-by-1 vector wj is the synaptic-weight vector of neuron j in the recurrent

xn+1 = £ 
(wT
1n)
o
(wT
j n)
o
(wT
qn)
§
Section 15.8
Real-Time Recurrent Learning
813
Output vector
yn1
of dimension
p
Input vector
un
of dimension
m
State vector
xn
of dimension
q
Bias
z1
z1
z1
z1
FIGURE 15.10
Fully connected recurrent network for formulation of the RTRL algorithm; the
feedback connections are all shown in red.

network—that is,
(15.43)
where 
a, j and 
b, j are the jth columns of the transposed weight matrices Wa
T and Wb
T,
respectively.The (q + m + 1)-by-1 vector n is defined by
(15.44)
where xn is the q-by-1 state vector and un is the (m + 1)-by-1 input vector.The first ele-
ment of un is + 1, and, in a corresponding way, the first element of wb,j is equal to the bias
bj applied to neuron j.
To simplify the presentation, we introduce three new matrices j,n, Uj,n, and n,
described as follows:
1. i,n is a q-by-(q + m + 1) matrix defined as the partial derivative of the state vec-
tor xn with respect to the weight vector wj:
(15.45)
2. Uj, n is a q-by-(q + m + 1) matrix whose rows are all zero, except for the jth row,
which is equal to the transpose of vector n:
(15.46)
3. n is a q-by-q diagonal matrix whose jth diagonal element is the partial deriva-
tive of the associated activation function with respect to its argument; we thus
write:
(15.47)
With these definitions, we may now differentiate Eq. (15.42) with respect to wj. Then,
using the chain rule of calculus, we obtain the following recursive equation:
(15.48)
This recursive equation describes the nonlinear state dynamics (i.e., evolution of the
state) of the real-time recurrent learning process.
To complete the description of this learning process, we need to relate the matrix
j, n to the gradient of the error surface with respect to wj.To do this,we first use the mea-
surement equation of Eq. (15.11) to define the p-by-1 error vector:
(15.49)
 = dn - Wcxn
 en = dn - yn
j, n+1 = n(Wa, nj, n + Uj, n),    j = 1, 2, ..., q
n = diag(¿(w1
Tn), ..., ¿(wj
Tn), ..., ¿(wq
Tn))
Uj, n = £
0
T
n
0
§ d jth row,    j = 1, 2, ..., q
j, n = 0xn
0wj
,    j = 1, 2, ..., q
n = c xn
un
d

w
w
wj = c wa, j
wb, j
d,    j = 1, 2, ..., q
814
Chapter 15
Dynamically Driven Recurrent Networks

where p is the dimension of the output vector yn. The instantaneous sum of squared
errors at time-step n is defined in terms of en by
(15.50)
The objective of the learning process is to minimize a cost function obtained by summing
over all time-steps n; that is,
To accomplish this objective, we may use the method of steepest descent, which requires
knowledge of the gradient matrix, written as
where 
is the gradient of 
with respect to the weight matrix W = {wk}. We may,
if desired,continue with this process and derive update equations for the synaptic weights
of the recurrent network without invoking approximations.However,in order to develop
a learning algorithm that can be used to train the recurrent network in real time, we
must use an instantaneous estimate of the gradient—namely,
—which results in an
approximation to the method of steepest descent. In a sense, we follow an approach
similar to that we used for the least-mean-square (LMS) algorithm in Chapter 3.
Returning to Eq. (15.50) as the cost function to be minimized, we differentiate it
with respect to the weight vector wj, obtaining
(15.51)
The adjustment applied to the synaptic-weight vector wj, n of neuron j is therefore deter-
mined by
(15.52)
 = Wcj, nen,    j = 1, 2, ..., q
 ¢wj,n = -  0en
0wj
 = - Wcj, nen,    j = 1, 2, ..., q
 = - Wc a 0xn
0wj
ben
 0en
0wj
= a 0en
0wj
ben
§W
en
§Wen
 = a
n
§Wen
 = a
n
0en
0W
 §Wetotal = 0etotal
0W
etotal = a
n
en
en
en = 1
2 eT
nen
Section 15.8
Real-Time Recurrent Learning
815
where  is the learning-rate parameter and j, n is the matrix governed by Eq. (15.48).
The only remaining task is that of specifying the initial conditions to start the learn-
ing process. For this purpose, we set
(15.53)
the implication of which is that the recurrent network initially resides in a constant state.
j,0 = 0    for all j

Table 15.1 presents a summary of the real-time recurrent learning algorithm. The
formulation of the algorithm as described here applies to an arbitrary activation func-
tion (·) that is differentiable with respect to its argument. For the special case of a sig-
moidal nonlinearity in the form of a hyperbolic tangent function, for example, we have
and
(15.54)
where 
is the induced local field of neuron j and xj,n+1 is its state at n + 1.
Deviation From the True Gradient Behavior
The use of the instantaneous gradient 
means that the real-time recurrent-learning
(RTRL) algorithm deviates from a non-real-time one based on the true gradient
. However, this deviation is exactly analogous to that encountered in the stan-
dard back-propagation algorithm used in Chapter 4 to train an ordinary multilayer
perceptron, where weight changes are made after each pattern presentation.While the
real-time recurrent-learning algorithm is not guaranteed to follow the precise nega-
tive gradient of the total errror function 
with respect to the weight matrix W,
the practical differences between the real-time and non-real-time versions are often
etotal(W)
§Wetotal
§Wen
vj,n
 = 1 - xj,n+1
2
 = sech2(vj,n)
 ¿(vj,n) =
0(vj,n)
0vj,n
 = tanh (vj,n)
 xj,n+1 = (vj,n)
816
Chapter 15
Dynamically Driven Recurrent Networks
TABLE 15.1 Summary of the Real-Time Recurrent Learning Algorithm
Parameters:
m = dimensionality of the input space
q = dimensionality of the state space
p = dimensionality of the output space
wj = synaptic-weight vector of neuron j, j = 1, 2, ..., q
Initialization:
1. Set the synaptic weights of the algorithm to small values selected from a uniform distribution.
2. Set the initial value of the state vector x(0) = 0.
3. Set 
.
Computations: Compute the following for n = 0, 1, 2, ...;
The definitions of xn, n, Uj,n and 
n are given in Eqs. (15.42). (15.45), (15.46), and (15.47), respectively.

 j,n+1 = n(Wa,nj,n + Uj,n),     j = 1, 2, ..., q
 ¢wj,n = Wcj,nen
 en = dn - Wcxn
j, 0 = 0 for j = 1, 2, ..., q

slight; these two versions become nearly identical as the learning-rate parameter  is
reduced.The most severe potential consequence of this deviation from the true gradient-
following behavior is that the observed trajectory (obtained by plotting 
versus the
elements of the weight matrix W) may itself depend on the weight changes produced by
the algorithm,which may be viewed as another source of feedback and therefore become
a cause of instability in the system. We can avoid this effect by using a learning-rate
parameter  that is small enough to make the time scale of the weight changes much
smaller than the time scale of the network operation. Basically, this is the same recipe
for algorithmic stability that was proposed for the LMS algorithm in Chapter 3.
Example 5
Illustration of the RTRL Algoithm
In this example,we formulate the RTRL algorithm for the fully recurrent network shown in Fig.15.6
with two inputs and a single output.The network has three neurons,with the composition of matrices
Wa, Wb, and Wc as described in Example 1.
With m = 2, q = 3, and p  1, we find from Eq. (15.44) that
Let
j, kl, n denote the kl-th element of matrix j, n at time-step n.The use of Eqs. (15.48) and (15.52)
then yields, respectively,
where 
is the Kronecker delta, which is equal to 1 for k = j and zero otherwise;
= 1, 2, 3,
and l = 1, 2, ..., 6. Figure 15.11 presents a sensitivity graph determining the evolution of the
weight adjustment
kl,n. Note that Wa = {
ji} for 
= 1, 2, 3, and Wb = {
jl} for j = 1, 2, 3 and
l = 4, 5, 6. Also, the Kronecker delta is not to be confused with the local gradient in Section 15.7
on the BPTT.
■
Teacher Forcing
A strategy that is frequently used in the training of recurrent networks is teacher forc-
ing (Williams and Zipser, 1989, 1995); in adaptive filtering, teacher forcing is known as
the equation-error method (Mendel, 1995). Basically, teacher forcing involves replacing
the actual output of a neuron, during training of the network, with the corresponding
desired response (i.e., target signal) in subsequent computation of the dynamic behav-
ior of the network,whenever that desired response is available.Although we are describ-
ing teacher forcing in a section on the RTRL algorithm, its use applies to any other
learning algorithm. For it to be applicable, however, the neuron in question must feed
its output back to the network input.
w
(j, i)
w
¢w
j, k
kj

j,kl,n+1 = ¿(vj,n) a a
3
i=1
wa,ji
i,kl,n + kjl,n b
¢wkl,n = (d1,n - x1,n)
1,kl,n

n = ≥
x1, n
x2, n
x3, n
1
u1, n
u2, n
¥
en
Section 15.8
Real-Time Recurrent Learning
817

Beneficial effects of teacher forcing include the following (Williams and Zipser,
1995):
• Teacher forcing may lead to faster training. The reason for this improvement is that
the use of teacher forcing amounts to the assumption that the network has correctly
learned all the earlier parts of the task that pertain to the neurons where teacher
forcing has been applied.
• Teacher forcing may serve as a corrective mechanism during training. For exam-
ple, the synaptic weights of the network may have the correct values, but somehow
the network is currently operating in the wrong region of the state space. Clearly,
adjusting the synaptic weights is the wrong strategy in such a situation.
A gradient-based learning algorithm that uses teacher forcing is, in fact, optimiz-
ing a cost function different from its unforced counterpart. The teacher-forced and 
-unforced versions of the algorithm may therefore yield different solutions, unless the
pertinent error signals are zero, in which case learning is unnecessary.
15.9
VANISHING GRADIENTS IN RECURRENT NETWORKS
A problem that requires attention in practical applications of a recurrent network is the
vanishing-gradients problem, which arises in the training of the network to produce a
desired response at the current time that depends on input data in the distant past.Because
of the combined nonlinearities, an infinitesimal change of a temporally distant input may
have almost no effect on network training.The problem may arise even if a large change
818
Chapter 15
Dynamically Driven Recurrent Networks
Ò1,kl,n
Ò2,kl,n
Ò3,kl,n
Ò1,kl,n
Ò1,kl,n1
Ò2, kl, n1
Ò3,kl, n1
Óe1, n
!wkl,n
jl, n
jl, n
jl, n
wa,11,n
wa,12,n
wa,13,n
dk1
dk2
dk3
w(v1,n)
w(v2,n)
w(v3,n)
z1
z1
z1
FIGURE 15.11
Sensitivity graph of the fully recurrent network of Fig. 15.6. Note:The three
nodes, labeled 
are all to be viewed as a single input.
l,n

in the temporally distant input has an effect, but the effect is not measurable by the gra-
dient.This vanishing-gradients problem makes the learning of long-term dependencies in
gradient-based training algorithms difficult, if not virtually impossible, in certain cases.
In Bengio et al. (1994), it is argued that for many practical applications, it is nec-
essary that a recurrent network be able to store state information for an arbitrary dura-
tion and to do so in the presence of noise. The long-term storage of definite bits of
information in the state variables of the recurrent network is referred to as information
latching.The information latching must be robust so that the stored state information can-
not be easily deleted by events that are unrelated to the learning task at hand. In spe-
cific terms, we may state the following (Bengio et al., 1994):
Robust information latching in a recurrent network is accomplished if the states of the net-
work are contained in the reduced attracting set of a hyperbolic attractor.
The notion of a hyperbolic attractor was discussed in Chapter 13. The reduced attract-
ing set of a hyperbolic attractor is the set of points in the basin of attraction for which
all the eigenvalues of the associated Jacobian have an absolute value less than 1. The
implication here is that if a state xn of the recurrent network is in the basin of attraction
of a hyperbolic attractor, but not in the reduced attracting set, then the size of a ball
of uncertainty around xn will grow exponentially with increasing time n, as illustrated in
Fig. 15.12a.Therefore, small perturbations (noise) in the input applied to the recurrent
Section 15.9
Vanishing Gradients in Recurrent Networks
819
P: hyperbolic
 
attractor
ı: basin of
 
attraction
 
of P
˝:  reduced
 
attraction
 
set of P
ı
Domain of state xn
(a)
ı
Domain of state xn
(b)
˝
P
˝
P
FIGURE 15.12
Illustration of
the vanishing-gradient problem:
(a) State xn resides in the basin
of attraction, , but outside the
reduced attration set . (b) State xn
resides inside the reduced attraction
set .

network could push the trajectory toward another (possibly wrong) basin of attraction.
If, however, the state xn remains in the reduced attracting set of the hyperbolic attrac-
tor, a bound on the input can be found that guarantees xn to remain within a certain
distance of the attractor, as illustrated in Fig. 15.12b.
Long-Term Dependencies
To appreciate the impact of robust information latching on gradient-based learning, we
note that the adjustment applied to the weight vector w of a recurrent network at time-
step n is defined by
where  is the learning-rate parameter and 
is the gradient of the cost function
with respect to w.The cost function 
is typically defined by
where the vector di,n is the desired response and the corresponding vector yi,n is the
actual response of the network at time-step n for the ith pattern. Hence, using these two
equations, we may go on to write
(15.55)
where, in the second line, we have used the chain rule of calculus; the state vector xi,n
pertains to the ith pattern (example) in the training sample. In applying algorithms such
as back propagation through time, the partial derivatives of the cost function are com-
puted with respect to independent weights at different time indices.We may expand on
the result in Eq. (15.55) by writing
Applying the chain rule of calculus a second time yields
(15.56)
We now recognize that in light of the state equation of Eq. (15.2), we have
Hence, we may interpret 
as the Jacobian of the nonlinear function 	(
)
expanded over n - k time-steps, as shown by

 ,
0xi, n0xi, k
xi, n = 	(xi, k, un),    1  k 6 n
¢wn =  a
i
a
0yi, n
0xi, n a  
n
k=1
a
0xi,n
0xi,k
*
0xi,k
0wk
bb(di, n - yi, n)
¢wn =  a
i
a
0yi, n
0xi, n a  
n
k=1
0xi, n
0wk
b(di, n - yi, n)
 =  a
i
a
0yi,n
0xi,n
*
0xi,n
0w b(di, n - yi, n)
 ¢wn =  a
i
a
0yi, n
0w b (di, n - yi, n)
etotal = 1
2 a
i
di, n - yi, n2
etotal
etotal
0etotal0w
¢wn = - 0etotal
0w
820
Chapter 15
Dynamically Driven Recurrent Networks

(15.57)
In Bengio et al. (1994), it is shown that if the input un is such that the recurrent network
remains robustly latched to a hyperbolic attractor after time n = 0, then the Jacobian
Jx,n,k is an exponentially decreasing function of k so that
(15.58)
The implication of Eq. (15.58) is that a small change in the weight vector w of the net-
work is experienced mostly in the near past (i.e., values of k close to the current time-
step n).There may exist an adjustment !w to the weight vector w at time n that would
permit the current state xn to move to another, possibly better, basin of attraction, but
the gradient of the cost function 
with respect to w does not carry that information.
To conclude, assuming that hyperbolic attractors are used to store state informa-
tion in a recurrent network by means of gradient-based learning, we find that either
• the network is not robust to the presence of noise in the input signal, or else
• the network is unable to discover long-term dependencies (i.e., relationships between target
outputs and inputs that occur in the distant past).
Second-Order Methods for Mitigating the Vanishing-Gradients Problem
Gradient-based learning algorithms rely entirely on first-order information—namely,
the Jacobian—for their operation. They are therefore inefficient in using the informa-
tion content of the training data.To improve the utilization of information contained in
the training data and thereby provide a remedy for the vanishing-gradients problem, we
need to look to second-order methods. In this context, we have two options:
1. We can use second-order optimization techniques, such as the quasi-Newton,
Levenberg–Marquardt, and conjugate gradient algorithms, which were discussed
in Chapters 2 and 4. Although these nonlinear optimization techniques have shown
promise, they are frequently plagued by convergence to poor local minima.10
2. We can use nonlinear sequential state-estimation procedures, which were discussed
in Chapter 14.During the training of a neural network,two functions are performed:
• The evolution of the weights in the neural network is tracked in a sequential
manner.
• Second-order information about the training data is provided in the form of a
prediction-error covariance matrix, which is also maintained and evolved
sequentially.
The extensive works reported in Puskorius and Feldkamp (2001),Feldkamp et al.(2001),
and Prokhorov (2006, 2007) have demonstrated that nonlinear sequential state-estima-
tion procedures form the basis of a second-order neural network training method that
is practical and effective as an alternative method to the batch-oriented nonlinear opti-
mization techniques mentioned previously. Accordingly, hereafter we will focus our
attention on the use of nonlinear sequential state-estimate procedures for the training
of recurrent multilayer perceptrons.
etotal
det(Jx,n,k) S 0 as k S q    for all n
= Jx,n,k
0xi,n
0xi,k
=
0	(xi,k, un)
0xi,k
Section 15.9
Vanishing Gradients in Recurrent Networks
821

15.10
SUPERVISED TRAINING FRAMEWORK FOR RECURRENT NETWORKS
USING NONLINEAR SEQUENTIAL STATE ESTIMATORS
To describe how a nonlinear sequential state estimator can be used to train a recurrent
network in a supervised manner, consider such a recurrent network built around a multi-
layer perceptron with s synaptic weights and p output nodes.With n denoting a time-step
in the supervised training of the network,let the vector wn denote the entire set of synap-
tic weights in the network computed at time-step n. For example, we may construct the
vector wn by stacking the weights associated with neuron 1 in the first hidden layer on
top of each other,followed by those of neuron 2,carrying on in this manner until we have
accounted for all the neurons in the first hidden layer; then we do the same for the sec-
ond and any other hidden layer in the network, until all the weights in the network have
been accounted for in the vector wn in the orderly fashion just described.
With sequential state estimation in mind, the state-space model of the network
under training is defined by the following pair of models (see Fig. 15.13):
1. System (state) model, which is described by the random-walk equation
(15.59)
The dynamic noise
n is a white Gaussian noise of zero mean and covariance matrix
,which is purposely included in the system model to anneal the supervised train-
ing of the network over time. In the early stages of the training, the covariance
matrix 
is large in order to encourage the supervised-learning algorithm to escape
local minima, and then it is gradually reduced to some finite, but small, value.
2. Measurement model, which is described by the equation
(15.60)
where the new entities are defined as follows:
• dn is the observable.
• vn is the vector representing the recurrent node activities inside the network,
with its elements listed in an order consistent with those of the weight vector wn;
hereafter, vn is referred to as the internal state.
dn = b(wn, vn, un) + n
Q#
Q#

wn+1 = wn + n
822
Chapter 15
Dynamically Driven Recurrent Networks
Network
nonlinearity
b(., ., .)
n
z1I
Dynamic
noise
wn1
Weight
(state)
vector
wn
Input vector
un
b(wn, vn, un)
Observable
dn
Recurrent
network activities
vn


Measurement
noise
n


FIGURE 15.13
Nonlinear state-space model depicting the underlying dynamics of a recurrent
network undergoing supervised training.

• un is the vector denoting the input signal applied to the network; that is, un is a
driving force applied to the network.
•
n is the vector denoting measurement noise corrupting the vector dn; it is
assumed to be a multivariate white-noise process of zero mean and diagonal
covariance matrix Rn.The source of this noise is attributed to the way in which
dn is actually obtained.
The vector-valued measurement function b(, , ) in Eq. (15.60) accounts for the overall
nonlinearity of the multilayer perceptron from the input to the output layer; it is the
only source of nonlinearity in the state-space model of the recurrent network.
Insofar as the notion of state is concerned,there are two different contexts in which
this notion naturally features in the supervised training of the network:
1. An externally adjustable state, which manifests itself in adjustments applied to the
network’s weights through supervised training—hence the inclusion of the weight
vector wn in the state-space model described by both Eq. (15.59) and Eq. (15.60).
2. An internally adjustable state, which is represented by the vector of recurrent node
activities, vn; these activities are outside the scope of the presently configured
supervised-training process, and it is for this reason that the vector vn is included
only in the measurement model of Eq. (15.60).The externally applied driving force
(input vector) un, the dynamic noise 
n, and the global feedback around the mul-


Section 15.10
Supervised Training Framework for Recurrent Networks
823
Description of the Supervised-Training Framework using the Extended
Kalman Filter
Given the training sample 
,the issue of interest is how to undertake the super-
vised training of the recurrent multilayer perceptron (RMLP) by means of a sequential
state estimator. Since the RMLP is nonlinear by virtue of the nonlinear measurement
model of Eq. (15.60), the sequential state estimator would have to be correspondingly
nonlinear. With this requirement in mind, we begin the discussion by considering how
the extended Kalman filter (EKF), studied in Chapter 14, can be used to fulfill this role.11
For the purpose of our present discussion, the relevant equations in the EKF algo-
rithm summarized in Table 15.2 are the following two, using the terminology of the
state-space model of Eqs. (15.59) and (15.60):
1. the innovations process, defined by
(15.61)
where the desired (target) response dn plays the role of the “observable” for the
EKF;
2. the weight (state) update, defined by
(15.62)
where 
is the predicted (old) estimate of the RMLP’s weight vector w at time
n, given the desired response up to and including time n - 1, and 
is the filtered
(updated) estimate of w on receipt of the observable dn.The matrix Gn is the Kalman
gain, which is an integral part of the EKF algorithm.
wˆ nn
wˆ nn-1
wˆ nn = wˆ nn-1 + Gnn
n = dn - b(wˆ nn-1, vn, un)
{un, dn}N
n=1
tilayer perceptron account for the evolution of vn over time n.

Examining the underlying operation of the RMLP, we find that the term
is the actual output vector yn produced by the RMLP with its “old”weight
b(wˆ nn-1, vn, un)
824
Chapter 15
Dynamically Driven Recurrent Networks
TABLE 15.2
Summary of the EKF algorithm for Supervised Training of the RMLP
Training sample:
where un is the input vector applied to the RMLP and dn is the corresponding desired response.
t = {un, dn}N
n=1
RMLP and Kalman filter: parameters and variables
b(, , )
:
vector-valued measurement function
B
:
linearized measurement matrix
wn
:
weight vector at time-step n
wˆ n|n1
:
predicted estimate of the weight vector
wˆ n|n
:
filtered estimate of the weight vector
vn
:
vector of recurrent node activities in the RMLP
yn
:
output vector of the RMLP produced in response to the input vector un
Q#
:
covariance matrix of the dynamic noise 
n
Q
:
covariance matrix of the measurement noise n
Gn
:
Kalman gain
Pn|n-1
:
prediction-error covariance matrix
Pn|n
:
filtering-error covariance matrix

Computation:
For n = 1, 2, ..., compute the following:
Pn+1n = Pnn + Q#,n
Pnn = Pnn-1 - GnBnPnn-1
wˆ n+1n = wˆ nn
wˆ nn = wˆ nn-1 + Gnn
n = dn - bn(wˆ nn-1, vn, un)
Gn = Pnn-1Bn
T[BnPnn-1Bn
T + Q,n]-1
Initilization:
P1|0 =
-1I, where 
is a small positive constant and I is the identity matrix
!
!
wˆ 10 = [w1]
vector 
and internal state 
in response to the input vector un. We may therefore
rewrite the combination of Eqs. (15.61) and (15.62) as a single equation:
(15.63)
On the basis of this insightful equation, we may now depict the supervised training of
the RMLP as the combination of two mutually coupled components forming a closed-
loop feedback system, such as that shown in Fig. 15.14:
1. The top part of the figure depicts the supervised-learning process as viewed partly
from the network’s perspective. With the weight vector set at its old (predicted)
value 
, the RMLP computes the actual output vector yn in response to the
input vector un.Thus,the RMLP supplies the EKF with yn as the predicted estimate
of the observable—namely,
.
2. The bottom part of the figure depicts the EKF in its role as the facilitator of the
training process.Supplied with 
,the EKF updates the old estimate of the
dˆnn-1 = yn
dˆ nn-1
wˆ nn-1
wˆ nn = wˆ nn-1 + Gn(dn - yn)
vn
wˆ nn-1

weight vector by operating on the current desired response dn. The filtered esti-
mate of the weight vector—namely,
—is thus computed in accordance with
Eq. (15.63).The 
so computed is supplied by the EKF to the RMLP via a bank
of unit-time delays.
With the transition matrix being equal to the identity matrix, as evidenced by Eq.
(15.59), we may set 
equal to 
for the next iteration.This equality permits the
supervised-training cycle to be repeated until the training is terminated.
Note that in the supervised-training framework of Fig. 15.14, the training sample
is split between the RMLP and EKF:The input vector un is applied to the
RMLP as the excitation,and the desired response dn is applied to the EKF as the observ-
able, which is dependent on the hidden weight (state) vector wn.
In Chapter 14,we emphasized the predictor–corrector property as an intrinsic prop-
erty of the Kalman filter, its variants, and extensions. In light of this property, examina-
tion of the block diagram of Fig.15.14 leads us to make the following insightful statement:
The recurrent neural network, undergoing training, performs the role of the predictor; and the
extended Kalman filter, providing the supervision, performs the role of the corrector.
Thus, whereas in traditional applications of the Kalman filter for sequential state esti-
mation, the roles of predictor and corrector are embodied in the Kalman filter itself, in
supervised-training applications these two roles are split between the recurrent neural
t = {un, dn}
wˆ nn
wˆ n+1n
wˆ nn
wˆ nn
Section 15.10
Supervised Training Framework for Recurrent Networks 
825
FIGURE 15.14
Closed-loop feedback system embodying the RMLP and the EKF: (a) The
RMLP, with weight vector 
, operates on the input vector un to produce the output vector
yn. (b) The EKF, supplied with the prediction 
nn - 1 = yn, operates on the desired response dn
to produce the filtered weight vector 
, thereby preparing the closed-loop
feedback system for the next iteration.
wˆ nn = wˆ n+1n
dˆ
wˆ nn-1
Bank of unit-time delays
Input
vector
un
Actual
output vector
yn
Desired
response
dn
Recurrent multilayer perceptron (RMLP):
weight vector  wn|n1
ˆ
Extended Kalman filter (EKF):
Predicted desired response dn|n1  yn
ˆ
wn|n
ˆ
ˆ
ˆ
wn1|n1  wn|n1
(a)
(b)
Training
sample:
t  {un, dn}

network and the extended Kalman filter. Such a split of responsibilities in supervised
learning is in perfect accord with the way in which the input and desired response ele-
ments of the training sample 
are split in Fig. 15.14.
The EKF Algorithm
For us to be able to apply the EKF algorithm as the facilitator of the supervised-
learning task, we have to linearize the measurement equation of Eq. (15.60) by retain-
ing first-order terms in the Taylor-series expansion of the nonlinear part of the
equation. With b(wn, vn, un) as the only source of nonlinearity, we may approximate
Eq. (15.60) as
(15.64)
where Bn is the p-by-s measurement matrix of the linearized model. The linearization
process involves computing the partial derivatives of the p outputs of the RMLP with
respect to its s weights, obtaining the matrix
(15.65)
the dimensions of which are p-by-s. Recognizing that dimensionality of the weight vec-
tor w is s, it follows that the matrix product Bw is a p-by-1 vector, which is in perfect
agreement with the dimensionality of the observable d.
The vector vn in b(w, vn, un) is maintained constant at some value; reference to
the time-step n has been omitted in Eq. (15.65) to simplify the presentation. The
in this equation denotes the ith element of the vectorial function
bi, i = 1, 2, p, p,
B = ≥
0b1
0w1
0b1
0w2
p
0b1
0ws
0b2
0w1
0b2
0w2
p
0b2
0ws




0bp
0w1
0bp
0w2
p
0bp
0ws
¥
dn = Bnwn + n
t
826
Chapter 15
Dynamically Driven Recurrent Networks
b(wn,vn,un).In accordance with Eq.(14.70) of Chapter 14,partial derivatives on the right-
hand side of the equation are evaluated at 
, where 
is the prediction of
the weight vector wn at time n, given the desired response up to and including time n - 1.
In practice, the partial derivatives in Eq. (15.65) are computed by using the back-
propagation-through-time (BPTT) or the real-time recurrent learning (RTRL) algo-
rithm. In effect, the EKF builds on one or the other of these two algorithms, which are
described in Sections 15.7 and 15.8, respectively.The implication here is that b must be
a function of the recurrent node activities, which it is, as shown in the measurement
model of Eq. (15.60).
The state-evolution equation of Eq. (15.59) is linear to begin with; it is therefore
unaffected by the linearization of the measurement equation.Thus, the linearized state-
space model of the recurrent network, permitting the application of the EKF, is defined
by Eqs. (15.59) and (15.64).
wˆ nn-1
wn = wˆ nn-1

Decoupled Extended Kalman Filter
The computational requirements of the extended Kalman filter (EKF), summarized in
Table 15.2, are dominated by the need to store and update the filtering-error covariance
matrix Pn|n at each time-step n. For a recurrent neural network containing p output
nodes and s weights, the computational complexity of the EKF is O(ps2) and its stor-
age requirement is O(s 2). For large s, these requirements may be highly demanding. In
such situations, we may look to the decoupled extended Kalman filter (DEKF) as a prac-
tical remedy for the proper management of computational resources (Puskorius and
Feldkamp, 2001).
The basic idea behind the DEKF is to ignore the interactions between the esti-
mates of certain weights in the recurrent neural network. In so doing, a controllable
number of zeros is introduced into the covariance matrix Pn|n. More specifically, if the
weights in the network are decoupled in such a way that we create mutually exclusive
weight groups, then the covariance matrix Pn|n is structured into a block-diagonal form,
as illustrated in Fig. 15.15.
Let g denote the designated number of disjoint weight groups created in the man-
ner just described.Also, for i = 1, 2, ..., g, let
weight vector for group i,
of the filtering-error covariance matrix for group i,
gain matrix for group i,
and so on for the other entries in the DEKF. The concatenation of the filtered weight
vectors 
forms the overall filtered weight vector 
; similar remarks apply to
, and the other entries in the DEKF algorithm. In light of these new notations,
we may now rewrite the DEKF algorithm for the ith weight group as follows:
wˆ nn
(i) = wˆ nn-1
(i)
+ Gn
(i)n
(i)
n
(i) = dn
(i) - bn
(i)(wˆ nn-1
(i)
, vn
(i), un
(i))
Gn
(i) = Pnn-1
(i)
(Bn
(i))Tc a
g
j=1
Bn
(j)Pnn-1
(j)
(B(j)
n )T + Q, n
(i) d
-1
Pnn
(i), Gn
(i)
wˆ nn
wˆ nn
(i)
G(i)
n
= Kalman
Pnn
(i) = subset
wˆ nn
(i) = filtered
Section 15.10
Supervised Training Framework for Recurrent Networks 
827
FIGURE 15.15
Block-diagonal representation of the filtering-
error covariance matrix 
pertaining to the decoupled
Kalman filter (DEKF).The shaded parts of the square
represent nonzero values of 
, where i = 1, 2, 3, 4 for the
example illustrated in the figure.As we make the number of
disjoint weight groups, g, larger, more zeros are created in the
covariance matrix Pn|n; in other words, the matrix Pn|n becomes
more sparse.The computational burden is therefore reduced,
but the numerical accuracy of the state estimation becomes
degraded.
Pnn
(i)
Pnn
(i)

Initialization of the DEKF algorithm proceeds in the manner described previously in
Table 15.2 for the EKF algorithm.
The computational requirements of the DEKF assume the following orders:
Computational complexity:
Storage requirement:
where si is the size of the state in group i and s in the total state size; p is the number of
output nodes. Depending on the number of disjoint groups,g,the computational require-
ments of the DEKF can be made significantly smaller than those of the EKF.
Summarizing Remarks on the EKF
An attractive feature of using the EKF as the sequential state estimator for the super-
vised training of a recurrent neural network is that its basic algorithmic structure (and
therefore its implementation) is relatively simple, as evidenced by the summary pre-
sented in Table 15.2. However, it suffers from two practical limitations:
1. The EKF requires linearization of the recurrent neural network’s vectorial meas-
urement function b(wn, vn, un).
2. Depending on the size of the weight vector w (i.e., the dimensionality of the state
space), we may have to resort to the use of the DEKF to reduce computational
complexity and storage requirements.The practical issue,however,is that we there-
by sacrifice computational accuracy.
We may bypass the first limitation by using a derivative-free nonlinear sequential state
estimator, as discussed next.
Supervised Training of Neural Networks with the Use of a Derivative-free
Sequential State Estimator
In Chapter 14, we discussed the cubature Kalman filter (Arasaratnam and Haykin,
2009), the formulation of which rests on applying a numerical method known as the
cubature rule (Stroud, 1971; Cools, 1997). Like the EKF, the cubature Kalman filter
(CKF) is an approximate realization of the Bayesian filter; however, in a theoretical
context the CKF is the optimum nonlinear filter for sequential state estimation. The
CKF has some unique properties:
1. The CKF is a more numerically accurate approximator of the Bayesian filter than
the EKF, in that it completely preserves second-order information about the state
that is contained in the observations.
O a a
g
i=1
si
2 b
O ap2s + p a
g
i=1
si
2 b
Pn+1n
(i)
= Pnn
(i) + Q#,n
(i)
Pnn
(i) = Pnn-1
(i)
- Gn
(i)Bn
(i)Pnn-1
(i)
wˆ n+1n
(i)
= wˆ nn
(i)
828
Chapter 15
Dynamically Driven Recurrent Networks

2. The CKF is derivative free; hence, there is no need for linearizing the measure-
ment matrix of the recurrent neural network.
3. Last,but by no means least,the cubature rule is used to approximate the time-update
integral that embodies the posterior distribution and all the other integral formulas
involved in the formulation of the Bayesian filter operating in a Gaussian
environment; as a rule, integration is preferred over differentiation because of its
“smoothing” property.
In light of these properties, it can be argued that the CKF is a highly attractive
choice for the supervised training of a recurrent neural network.The experiment to be
described in Section 15.11, involving the dynamic reconstruction of a chaotic attrac-
tor, demonstrates the superior performance of the CKF over the EKF and of another
derivative-free sequential state estimator known as the central-difference Kalman filter
(CDKF).12 The CDKF, due to Nörgaard et al. (2000), is derived by replacing a Taylor
series expansion of the nonlinear measurement equation in the vicinity of the current
estimate of the weight vector with a corresponding expansion based on Stirling’s formula
for interpolating an analytic function over a specified interval. In a one-dimensional set-
ting,Stirling’s formula may be obtained from the Taylor expansion by respectively replac-
ing first- and second-order partial derivatives with first- and second-order central
differences, commonly used in numerical analysis.13 Then, once the approximate lin-
earization of the measurement equation has been derived in the multidimensional set-
ting of interest,the formulation of the CDKF algorithm follows Kalman filter theory.The
original CDKF algorithm described in Nørgaard et al. (2000) employs square-root fil-
tering for improved numerical accuracy; such a procedure was described in Chapter 14
in the context of Kalman filtering.
15.11
COMPUTER EXPERIMENT: DYNAMIC RECONSTRUCTION 
OF MACKEY–GLASS ATTRACTOR
The Mackey–Glass attractor was originally formulated by Mackey and Glass (1977) for
modeling the dynamic formation of blood cells in the body. It is described by a single
continuous-time differential equation, namely,
(15.66)
where t denotes continuous time, the coefficients a = 0.2 and b = 0.1, and the time
delay !t = 30. In a formal sense, the Mackey–Glass attractor has an infinite number
of degrees of freedom, because we require knowledge of the initial value of the func-
tion x(t) across a continuous-time interval.Yet, it behaves like a strange attractor with
a finite dimension.
To solve Eq. (15.66) numerically, we used the fourth-order Runge-Kutta method
(Press et al., 1988) with a sampling period of 6s and an initial condition xn = 0.9 for
0  n  !t, where, as usual, n denotes discrete time. We thus obtained a time series of
length 1000, of which the first half was used for training and the rest for testing. Given
a chaotic attractor, we recall from Chapter 13 that the next data sample xn+ can be 
d
dt xt = - bxt +
axt-¢t
1 + xt- ¢t
10
Section 15.11
Computer Experiment: Dynamic Reconstruction
829

predicted from a properly chosen time series 
where
dE and are called the embedding dimension and embedding delay, respectively. For the
chaotic Mackey–Glass system, dE and  were chosen to be 7 and 1, respectively.
Recurrent multilayer perceptrons (RMLPs) have proven to be numerically robust
in learning time-correlated signals. For this experiment, we implemented an RMLP hav-
ing seven inputs, representing an embedding of the observed time series, one output,
and one self-recurrent hidden layer with five neurons. Hence, the RMLP has a total of
71 synaptic weights (with bias parameters included).The output neuron uses a linear acti-
vation function, whereas all the hidden neurons use the hyperbolic tangent function
The square-root versions of three algorithms were used to train the RMLP: The
extended Kalman filter, the central-difference Kalman filter, and the cubature Kalman
filter. To unfold the recurrent loop of the neural network, we used a truncation depth 
h = 1, which was found to be adequate for the experiment. Moreover, for the EKF algo-
rithm, we used the back-propagation algorithm to compute the partial derivatives of
the nonlinear measurement function bn, in accordance with the procedure described in
Section 15.7.
For all three algorithms, 10 epochs per run were used to train the RMLP. Each
epoch was obtained from a long subsequence involving 107 time-steps, starting from a
randomly selected point.To be more precise, each epoch was made up of 100 examples
gleaned by a sliding window of length 8 over the subsequence. The weights of the RMLP
were initialized to zero-mean Gaussian with a diagonal covariance matrix of 
where 
is an s-by-s identity matrix.
To compare the performance of CKF-trained RMLPs against the CDKF and
against EKF-trained RMLPs in a fair-minded way, we made 50 independent training
runs. To measure the performance in predicting 100 time-steps ahead starting from
a time index of 500, we used the ensemble-averaged cumulative absolute error,
defined by
where di
(r) is the desired response at time i for the rth run and 
is the estimate computed
at the output of the RMLP.The long-term accumulative prediction error is an increas-
ing function of time n.
As has already been pointed out, three different approximations of the Bayesian
filter were used in the experiment:
• Extended Kalman filter (EKF)
• Central-difference Kalman filter (CDKF)
• Cubature Kalman filter (CKF)
Results of the experiment are presented in Fig. 15.16, where the ensemble-averaged
cumulative absolute error in dynamic reconstruction is plotted against the number of
dˆi
(r)
en = 1
50 a
50
r=1 a
n
i=1
d(r)
i
- dˆ (r)
i ;    n = 1, 2, ..., 100
Is
10-2 * Is
(v) = tanh(v)

{xn, xn-, ..., xn-[dE-2], xn-[dE-1]},
830
Chapter 15
Dynamically Driven Recurrent Networks

predictive time-steps that were used in the dynamic reconstruction. As expected, the
results of the experiment provide clear evidence for the superior performance, and
therefore improved computational accuracy, of the CKF over the CDKF and the EKF
in that order.
15.12
ADAPTIVITY CONSIDERATIONS
An interesting property of a recurrent neural network (e.g., RMLP), observed after the
network has been trained in a supervised manner, is the emergence of an adaptive behav-
ior.14 This phenomenon occurs despite the fact that the synaptic weights in the network
have been fixed.The root of this adaptive behavior may be traced to a fundamental the-
orem, which is stated as follows (Lo and Yu, 1995b):
Consider a recurrent neural network embedded in a stochastic environment with relatively
small variability in its statistical behavior. Provided that that the underlying probability dis-
tribution of the environment is fully represented in the supervised-training sample supplied
to the network, it is possible for the network to adapt to the relatively small statistical varia-
tions in the environment without any further on-line adjustments being made to the synaptic
weights of the network.
This fundamental theorem is valid only for recurrent networks. We say so because the
dynamic state of a recurrent network actually acts as a “short-term memory” that 
carries an estimate or statistic of the uncertain environment for adaptation, in which
the network is embodied.
Section 15.12
Adaptivity Considerations
831
0
0
5
10
15
20
25
20
40
60
80
100
EKF
Prediction time-step
Cumulative absolute error
CDKF
CKF
FIGURE 15.16
Ensemble-averaged cumulative absolute error curves during the
autonomous prediction phase of dynamic reconstruction of the Mackey-Glass attractor.

This adaptive behavior has been referred to differently in the literature. In Lo
(2001), it is referred to as accommodative learning. In another paper published in the
same year (Younger et al., 2001), it is referred to as meta-learning, meaning “learn-
ing how to learn.” Hereafter, we will refer to this adaptive behavior as meta-learning
as well.
Regardless of how this adaptive behavior is termed, it is not expected that it will
work as effectively as a truly adaptive neural network, where provision is made for auto-
matic on-line weight adjustments if the environment exhibits large statistical variabil-
ity.This observation has been confirmed experimentally in Lo (2001),where comparative
performance evaluations were made between a recurrent neural network with meta-
learning and an adaptive neural network with long-term as well as short-term memories;
the comparative evaluations were performed in the context of system identification.
Nevertheless, the meta-learning capability of recurrent neural networks should be
viewed as a desirable property in control and signal-processing applications, particu-
larly where on-line adjustments of synaptic weights are not practically feasible or they
are too costly to perform.
Adaptive Critic
If a desired response is not available for the supervised training of a recurrent neural
network and existing unsupervised training methodology does not converge fast enough
for the application of interest, then reinforcement learning (i.e., approximate dynamic
programming) may be the only viable option. From Chapter 12, we recall that in approx-
imate dynamic programming, an agent (i.e., learning system) requires, from the envi-
ronment in which it is embedded, merely a response to the action taken by the agent.
Basically, the interaction between the agent and its environment in real time is all
that we need for the construction of a short-term memory that could permit the
internal states of the recurrent neural network to adapt to statistical variations of the
environment.
With the synaptic weights of the recurrent neural network fixed, the only way in
which its internal states can be adapted is through adjustments applied to the network’s
internal recurrent node activities, denoted by the vector vn in the measurement equation
of Eq. (15.60).Thus, unlike the supervised adjustment applied to the hidden weight vec-
tor wn, the adjustments to the vector vn are applied directly to the measurement equa-
tion of Eq. (15.60).
The block diagram of Fig. 15.17 depicts a scheme built around a recurrent neural
network with fixed weights whereby the recurrent node activities can be adapted in real
time. Specifically, we have an adaptive critic that receives two inputs, one from the net-
work and the other from the environment in response to some relevant action taken by
the network (acting as the agent).In response to these two inputs,the adaptive critic com-
putes the appropriate adjustments to the network’s internal recurrent node activities.
To summarize, we may say that through the use of an adaptive critic, a recurrent
neural network becomes equipped with two forms of memory:
1. long-term memory, which is acquired by the network itself through supervised
training that results in a set of fixed weights;
832
Chapter 15
Dynamically Driven Recurrent Networks

2. short-term memory, which enables the network to adapt its internal state (i.e.,
recurrent node activities) to statistical variations in the environment without
disturbing the fixed weights.
It is also noteworthy that through continuing interactions with the environment,
the short-term memory can be developed in a model-free setting, as described in
Prokhorov (2007).
15.13
CASE STUDY: MODEL REFERENCE APPLIED TO NEUROCONTROL
In this last topic covered in this chapter, we discuss a case study that not only fits quite
nicely into the chapter, but also brings together several topics discussed in previous
chapters of the book.
To be specific, we discuss an important application of recurrent neural networks
in the design of feedback control systems, where the states of a plant are coupled non-
linearly with imposed controls.The design of the system is further complicated by other
factors such as the presence of unmeasured and random disturbances, the possibility of
a nonunique plant inverse, and the presence of plant states that are unobservable.
A control strategy well suited for the use of recurrent neural networks is the model-
reference control (Narendra and Annaswamy, 1989; Puskorius and Feldkamp, 2001;
Prokhorov, 2006).As depicted in Fig. 15.18, the model-reference control system consists
of five functional components:
1. The plant, which is to be controlled so as to compensate for changes in the plant
dynamics.The plant’s output evolves over time n as a function of a control signal
and its own parameter vector 
k, where the time index k in
k changes far less fre-
quently than the time index n. For example,
k could be piecewise constant, where
it switches from one constant level to another with varying k.



Section 15.13
Case Study: Model Reference Applied to Neurocontrol
833
Input signal
vector
un
Control of
recurrent node
activities
vn
Output
signal
Response from the
environment
Recurrent neural network
with
fixed weight vector wn
and
adjustable recurrent node
activities vn
Adaptive
critic
Unit-time
delay
FIGURE 15.17
Block diagram illustrating the use of an adaptive critic for the control of
recurrent node activities vn in a recurrent neural network (assumed to have a single output);
the part of the figure involving the critic is shown in red.

2. The neurocontroller, which consists of a recurrent network exemplified by a recur-
rent multilayer perceptron. It supplies the control signal, applied to the plant input.
This signal varies as a function of the reference signal, feedback signal, and the
controller’s weight vector denoted by w.
3. The model reference, which is assumed to be stable.The model reference supplies
a desired signal in response to the reference signal as input.
4. The comparator,represented by the summing unit,which compares the plant output
against the desired response from the model reference to produce an error signal.
5. The bank of unit-time delays, represented by 
, which closes the feedback loop
around the plant by aligning the elements of the plant output vector with those of
the reference signal; in effect, an external recurrent network is realized by the feed-
back loop.
From this description, it is apparent that the plant output is an indirect function
of the neurocontroller’s weight vector w through the control signal and a direct func-
tion of the plant’s own parameter vector 
k.We may therefore express the plant output
as yi,p(n, w,
k), where the subscript i refers to a particular instance of the plant’s oper-
ation. The explicit dependence of the plant output on time n is included to emphasize
the nonstationary behavior of the plant. Correspondingly, let yi,r(n) denote the output
of the model reference for the same instance i.The reference signal is common to both
forward paths of the model-reference adaptive-control system; we have simplified mat-
ters by not including the dependence on the reference signal in the plant output or in
the model-reference output.
The error signal is defined by the difference between the model-reference output
and the plant output for each instance i.We may therefore formulate the mean-square error
(15.66)
where the inner summation is taken over the entire set of instances used in training the
neurocontroller, and the outer summation is taken over the entire duration of training
J(w, k) = 1
T a
T
n=1 a
i
7yi,r(n) - yi,p(n, w, k)7 2


z-1I
834
Chapter 15
Dynamically Driven Recurrent Networks
Neurocontroller
w
Plant
k
Model
reference
Control
signal
Feedback
signal
Reference
signal
Desired
response
yr
Error
signal
Plant output yp
z1I
Σ


FIGURE 15.18
Model-reference adaptive control system; the feedback loop of the system is
printed in red.

1
n
T.To provide for a design of the neurocontroller that is robust with respect to
parameter changes and external disturbances (the latter are not shown in Fig. 15.18),
adjustments to the neurocontroller’s weight vector w are made in such a way that the
root mean-square error J(w, k) and its maximum value are both reduced over all pos-
sible values of the plant’s parameter vector k (Prokhorov, 2006). This optimization
makes the plant output track the model-reference output.
The block labeled “Plant” in the model-reference control system of Fig. 15.18 has a
double meaning, depending on how it is seen from the perspective of the neurocontroller:
• One meaning refers to the actual system being controlled as the plant.
• The other meaning refers to a model of that actual system.
Accordingly, we may compensate for uncertainties in the plant dynamics by using direct
control in which the actual plant is used in the control system,or indirect control in which
the plant model is used in the control system (Adetona et al., 2000).
In many cases, we find that a physics-based model of the plant (i.e., the actual sys-
tem being controlled) is already in place; the availability of such a model is common in
industry, resulting from the investment of a great amount of both time and effort.Alter-
natively, we may use the principle of system identification, discussed in the introductory
chapter to build a neural-network-based model of the plant.Typically, however, we find
the following situation (Prokhorov, 2006):
1. The physics-based model is more accurate than the neural-network-based model.
2. The physics-based model does not include exclusively differentiable elements.
The method used to train the neurocontroller in the study reported by Prokhorov (2006)
is a modified version of a square-root state-estimation algorithm originally derived by
Nørgaard et al.(2000).As pointed out previously,this algorithm is appropriately referred
to as the central-difference Kalman filter (CDKF).
In Prokhorov (2006), experimental results are presented that not only validate the
training of a neurocontroller via the nonlinear sequential state-estimation framework,
but also demonstrate the superior accuracy obtained with the derivative-free CDKF
algorithm versus the derivative-dependent EKF algorithm.
15.14
SUMMARY AND DISCUSSION
Recurrent Network Models
In this chapter, we discussed recurrent neural networks that involve the use of global
feedback applied to a static (memoryless) multilayer perceptron. The application of
feedback enables neural networks to acquire state representations, making them suit-
able systems for diverse applications in signal processing and control.We identified four
main network models belonging to the class of recurrent networks with global feedback:
• nonlinear autoregressive networks with exogenous inputs (NARX networks),
which use feedback from the output layer to the input layer;
• fully connected recurrent networks, which use feedback from the hidden layer to
the input layer;


Section 15.14
Summary and Discussion
835

• recurrent multilayer perceptrons with more than one hidden layer, which use feed-
back from the output of each computation layer to its own input, and possibly all
the way from the output to the input;
• second-order recurrent networks, which use second-order neurons.
In all of these recurrent networks,the feedback is applied via tapped-delay-line memories.
The first three recurrent networks permit the use of a state-space framework for study-
ing their dynamic behavior. This approach, rooted in modern control theory, provides a
powerful method for studying the nonlinear dynamics of recurrent neural networks.
Properties of Recurrent Neural Networks
The following are some important properties of recurrent neural networks:
1. They are universal approximators of nonlinear dynamic systems, provided that
they are equipped with an adequate number of hidden neurons.
2. They are locally controllable and locally observable, provided that their linearized
versions satisfy certain conditions around the equilibrium point.
3. Given any finite-state machine, we can build a recurrent neural network which,
regarded as a black-box machine, will behave like that finite-state machine.
4. Recurrent neural networks exhibit a meta-learning (i.e., learning to learn) capability.
Indeed, it is precisely these properties that befit recurrent neural networks for applica-
tions in computing, control, and signal-processing.
Gradient-based Learning Algorithms
In this chapter, we also discussed two basic supervised-learning algorithms for the train-
ing of recurrent neural networks: back propagation through time (BPTT) and real-time
recurrent learning (RTRL). Both of these algorithms are gradient based, which makes
them computationally simple to implement.The BPTT is most suitable for off-line learn-
ing, whereas, by definition, the RTRL is designed for on-line learning. However, a prac-
tical limitation of both algorithms is the vanishing-gradients problem, which arises due
to their inability to use second-order information contained in the training data.
Supervised-learning Algorithms Based on Nonlinear Sequential State Estimation
An effective method for overcoming the vanishing-gradients problem is to use a non-
linear sequential state estimator to provide for the supervised training of a recurrent
multilayer perceptron. Here we have two choices available to us:
1. We can use the extended Kalman filter (EKF) on account of its computational sim-
plicity.However,we then have to provide for linearization of the measurement model
pertaining to the recurrent neural network by using the BPTT or RTRL algorithm.
2. We can use a derivative-free nonlinear sequential state estimator, exemplified by
the cubature Kalman filter (CKF) described in Chapter 14 or the central-difference
Kalman filter (CDKF) described briefly in this chapter. In so doing, we not only
broaden the applicability of this novel approach to supervised learning, but also
improve numerical accuracy. However, the price to be paid for these benefits is
increased computational requirements.
836
Chapter 15
Dynamically Driven Recurrent Networks

Among these three nonlinear filters, the CKF stands out not only as the closest approx-
imation to the Bayesian filter (that is optimal, at least, in a conceptual sense) but also
the most powerful of them all. Assuming Gaussianity, formulation of the CKF is influ-
enced by Kalman filter theory (e.g.,the innovations process),as discussed in Chapter 14.
In any event, this new approach to supervised learning is elegant, as evidenced by
the block diagram of Fig. 15.14 for EKF. Most importantly, the procedure is applicable
to recurrent neural networks as well as other neural networks (e.g., multilayer percep-
trons). Indeed, it is because of this universal applicability that we may look to the class
of nonlinear sequential state-estimation algorithms (embodying the EKF, CDKF, and
CKF) for supervised learning as an enabling technology, which makes them capable of
solving difficult signal-processing and control problems, particularly large-scale learn-
ing problems where the use of second-order information may very well be a “must.”
In theory, a recurrent network with global feedback (e.g., a recurrent multilayer
perceptron trained with the EKF algorithm) can learn the underlying dynamics of a
nonstationary environment by storing the knowledge gained from the training sample
in a fixed set of weights. Most importantly, the network can track the statistical variations
of the environment, provided that two conditions are satisfied:
• The recurrent network does not suffer from underfitting or overfitting.
• The training sample is representative of an environment that exhibits small 
statistical variations.
Multistream Training
The approach to supervised training of recurrent networks described in Fig. 15.14 may
benefit from a procedure known as multistream training. This procedure applies to sit-
uations, in which a coordinated weight update could be advantageous by virtue of using
multiple training patterns (Puskorius and Feldkamp, 2001).
In the supervised training of a neural network, two scenarios may arise, depend-
ing on the nature of the training sequence of input–target response pairs:
1. Homogeneous sequences, where one or more passes through the training data may
well produce satisfactory results.
2. Heterogenous sequences, where,for example,there may be regions of rapid changes
in the input–target response pairs followed by regions of slow changes.
In the latter scenario, there is a tendency in standard training processes for the network
weights to be adapted unduly in favor of the currently presented training data, so we
speak of the recency effect. For feedforward networks, the effective solution is to shuffle
the order in which the training data are presented to the network or to use the batch form
of training; both of these procedures were discussed in Chapter 4. For recurrent neural
networks, the direct analog of shuffling the order of data presentation is to present the
network with randomly selected subsequences; this has the effect of making a weight
update only for the last input–target response pair of the subsequence. In the case of a
training procedure using the EKF algorithm, for example, a full batch update involves
running the recurrent network through the entire training sample, computing the nec-
essary partial derivatives for each input–target response pair,and then making an update
of the network weights on the basis of the entire set of estimation errors.
Section 15.14
Summary and Discussion
837

838
Chapter 15
Dynamically Driven Recurrent Networks
The multistream training procedure overcomes the recency effect through the
combined use of shuffling (i.e., presentation of randomly selected subsequences) and
batch updating. In particular, multistream training is based on the principle that each
weight update should account for the information contents of multiple input–target
response pairs in a simultaneous manner.
One last comment is in order:Multistream training is applicable not only to the use
of the EKF algorithm, but also to the use of derivative-free nonlinear sequential state
algorithms (e.g., the CDKF and CKF).
The Final Concluding Remarks: Large-scale Learning Problems
With this subsection being the very last one of this chapter, and the chapter itself being
the very last chapter of the whole book, it is a propos that we devote the subsection to
the issue of large-scale learning problems. In particular, this issue was discussed at some
length in three previous chapters:
• in Chapter 4 on multilayer perceptrons, where the study of large-scale learning
problems was contrasted with small-scale learning problems;
• in Chapter 7 on regularization theory, where we used differentiable manifolds to
formulate a semi-supervised learning strategy capable of exploiting information
contained in the training data of labeled as well as unlabeled kinds;
• then, again, in Chapter 12 on dynamic programming, where the curse-of-
dimensionality problem was raised as an issue of serious concern in dealing with
large-scale dynamic environments.
In the context of supervised learning problems on pattern classification and  nonlinear
regression that are of a small-scale kind, the procedures for solving these problems are
well-understood, as evidenced by the material presented in this book. On the other
hand, it can be justifiably asserted that the study of large-scale learning problems is in
its early stages of development.
Indeed, we may view problems on large-scale learning as a window on the future
of learning. This window leads us directly into the real world.Accordingly, we may iden-
tify four specific stages involved in tackling large-scale learning problems:
1. Development of an inventory of resources for use as training data. This first stage
is highly critical because, after all, the training data provide the linkage between
the real world pertaining to the problem at hand and the design of the learning
machine that is being researched for solving the problem. The inventory of the
available resources may include
• high-quality labeled data;
• labeled data of not as high a quality;
• an abundance of unlabeled data.
Given such a mixture of training data, the challenge is how to formulate different
scenarios for training strategies that deserve to be pursued, realizing that compu-
tational resources are limited.

Notes and References
839
2. Modeling of the environment responsible for generation of the training data. In this
second stage, the challenge is to formulate a network model, which has a large
enough number of degrees of freedom that are also of the right kind. In this for-
mulation, the goal is to capture the underlying statistical physics (nature) of the
environment responsible for the data generation. The fact of the matter is that
unless this issue is tackled properly, there will inevitably be a mismatch between
the physical reality of the data generation and the theoretical basis being proposed
for the network model. If this model mismatch is serious, then no matter what is
done thereafter will not cure the shortcomings of the model.
3. Selection of the algorithm for estimating the adjustable parameters of the network model.
This third stage is also challenging in its own right in that we have to select an algorithm
that is well-suited for estimating the unknown parameters of the model in a compu-
tationally effective manner. More precisely, a network model should have sufficient
depth,extending from the input to the output,in order to tackle the problem effectively.
4. Optimal estimation of the adjustable parameters. The final challenge is to select an
optimization algorithm that has the built-in capability to reliably extract the infor-
mation content of the training data. Typically, second-order information is con-
sidered to be adequate. Most importantly, the optimization algorithm should be
computationally efficient. In this context, the two potential candidates are:
• nonlinear sequential estimation algorithms,exemplified by the cubature Kalman
filter;
• second-order optimization algorithms, exemplified by refined on-line versions
of the Gauss-Newton and Levenberg-Marquardt algorithms, where ways are
found to dispense with exact calculation of the Hessian while estimation accu-
racy is being reasonably well maintained.
We conclude the book by saying that in solving real-world,large-scale learning prob-
lems,it is only when careful attention has been given to all the four stages described herein
that we can be assured of realizing successful solutions, which will make a difference.
NOTES AND REFERENCES
1. For other recurrent network architectures, see Jordan (1986), Back and Tsoi (1991),
and Frasconi et al. (1992).
2. The NARX model encompasses an important class of discrete-time nonlinear systems
(Leontaritis and Billings, 1985). In the context of neural networks, it is discussed in Chen et
al. (1990), Narendra and Parthasarathy (1990), Lin et al. (1996), and Siegelmann et al., (1997).
It has been demonstrated that the NARX model is well suited for modeling nonlin-
ear systems such as heat exchangers (Chen et al.,1990),wastewater treatment plants (Su and
McAvoy, 1991; Su et al., 1992), catalytic reforming systems in a petroleum refinery (Su 
et al., 1992), nonlinear oscillations associated with multilegged locomotion in biological sys-
tems (Venkataraman, 1994), and grammatical inference (Giles and Horne, 1994).
The NARX model is also referred to as the nonlinear autoregressive-moving aver-
age (NARMA) model, with “moving average” referring to the inputs.
3. Recurrent multilayer perceptrons are a special case of time-lagged recurrent neural net-
works (TLRNN).This generic class of recurrent networks admits the use of an arbitrary

840
Chapter 15
Dynamically Driven Recurrent Networks
pattern of connectivity between the nodes of a neural network; on the other hand, recurrent
multilayer perceptrons have a layered pattern of connectivity.TLRNNs offer the following
important characteristics (Lo, 1993):
(i)
They include conventional structures such as finite-duration impulse response (FIR).
(ii)
They have the built-in ability to account for strongly hidden states of nonlinear
dynamic systems.
(iii)
They are universal approximations of nonlinear dynamic systems.
4. Omlin and Giles (1996) show that any known finite-state automata can be mapped into
second-order recurrent networks, and the correct classification of temporal sequences of
finite length is guaranteed.
5. For a rigorous treatment of controllability and observability, see Zadeh and Desoer
(1963), Kailath (1980), and Sontag (1990).
6. The first work on neural networks and automata (actually, sequential machines and
automata implementations), also referenced as the first paper on finite-state automata,
artificial intelligence, and recurrent neural networks was the classic paper by McCulloch
and Pitts (1943).The recurrent network (with instantaneous feedback) in the second part
of this paper was interpreted as a finite-state automaton in Kleene (1956). Kleene’s paper
appeared in the book Automata Studies, edited by Shannon and McCarthy, (1956);
authors in this amazing book include Moore, Minsky, von Neumann, Uttley, McCarthy,
and Shannon, among others. Sometimes Kleene’s paper is cited as the first article on
finite-state machines (Perrin, 1990). Minsky (1967) discussed automata and neural net-
works in his book entitled Computation: Finite and Infinite Machines.
All of the early work on automata and neural networks was concerned with 
synthesis—that is, how automata are built or designed into neural networks. Because most
automata (when implemented as sequential machines) require feedback, the neural 
networks were necessarily recurrent ones. Note that the early work (with the exception of
Minsky) did not make a clear distinction between automata (directed, labeled, and acyclic
graphs) and sequential machines (logic and feedback delays) and was concerned mostly
with finite-state automata.There was little interest (with the exception of Minsky) in mov-
ing up the automata hierarchy to push down automata and Turing machines.
After the “Dark Ages” of neural networks, research on automata and neural 
networks started again in the 1980s. This work could be broadly classified into three areas:
(1) learning automata; (2) automata synthesis, extraction, and refinement of knowledge; and
(3) representation.The first mention of automata and neural networks was in Jordan (1986).
7. A single-layer recurrent network using McCulloch–Pitts neurons cannot simulate any
finite-state machines (Goudreau et al., 1994), but Elman’s simple recurrent network can
(Kremer, 1995). Recurrent networks with only local feedback cannot represent all finite-
state machines (Frasconi and Gori, 1996; Giles et al., 1995; Kremer, 1996). In other words,
the use of global feedback is a necessary requirement for the simulation of finite-state
machines by neural networks.
8. The idea behind back propagation through time is that for every recurrent network, it is
possible to construct a feedforward network with identical behavior over a particular
time interval (Minsky and Papert, 1969). Back propagation through time was first
described in the Ph.D. dissertation of Werbos (1974); see also Werbos (1990).The 
algorithm was rediscovered independently by Rumelhart et al. (1986b).A variant of the
back-propagation-through-time algorithm is described in Williams and Peng (1990). For a
review of the algorithm and related issues, see Williams and Zipser (1995).
9. The real-time recurrent learning algorithm was described in the neural network literature
for the first time by Williams and Zipser (1989). Its origin may be traced to an earlier

x
f
x0
f0
f1/2
x1
f1
2f1
f3/2
2f3/2
x2
f2
2f2
4f2
f5/2
2f5/2
x3
f3
2f3
f7/2
x4
f4
Notes and References
841
paper by McBride and Narendra (1965) on system identification for tuning the parame-
ters of an arbitrary dynamic system.
The derivation given in Williams and Zipser is for a single layer of fully recurrent
neurons. It has since been extended to more general architectures; see, for example,
Kechriotis et al. (1994) and Puskorius and Feldkamp (1994).
10. Schraudolph (2002) describes an algorithm called the stochastic meta-descent (SMD) algo-
rithm, in which the notion of calculating the exact Hessian is abandoned in favor of an itera-
tive approximation. In particular, a special curvature matrix–vector product is introduced to
iteratively approximate second-order gradient methods such as the Gauss–Newton and
Levenberg–Marquardt methods, resulting in improved stability and performance.
11. Singhal and Wu (1989) were perhaps the first to demonstrate the improved mapping perfor-
mance of a supervised neural network using the extended Kalman filter. Unfortunately, the
training algorithm described therein is limited by its computational complexity.To overcome
this limitation, Kollias and Anastassiou (1989) and Shah and Palmieri (1990) tried to simplify
the application of extended Kalman filtering by partitioning the global problem into a num-
ber of subproblems, each of which addresses a single neuron. However, the treatment of each
neuron as an identification problem does not rigorously adhere to Kalman filter theory. More-
over, such an approach may lead to unstable behavior during training and may result in solu-
tions that are inferior to those obtained by other methods (Puskorius and Feldkamp, 1991).
12. In Prokhorov (2006, 2007) and related papers, the sequential state-estimation algorithm
due to Nørgaard, Poulsen, and Ravn (2000) is referred to as the nprKF algorithm, where
“npr” is taken from the first letters in the names of the algorithm’s three co-authors. In
this chapter, we have expressed preferance for naming the algorithm as the central-difference
Kalman filter (CDKF), which is more descriptive of what the algorithm is based on.
13. Consider a function of the variable x, denoted by f(x). Let fk denote the value of this
function for x = xk.The central difference is defined by
for every k
where the subscript on the left is the average of the two subscripts on the right. The fol-
lowing table illustrates how higher-order central differences can be formulated:
fk + 1
2 = fk+1 - fk
Note that elements of the table with the same subscript always lie on lines extending hori-
zontally, or centrally, into the table (Wylie and Barrett, 1982).
14. The emergence of adaptive behavior in recurrent neural networks, exemplified by recurrent
multilayer perceptrons, was first discussed in Lo and Yu (1995). For additional references
on this phenomenon, see the overview paper by Prokhorov et al. (2002).

842
Chapter 15
Dynamically Driven Recurrent Networks
PROBLEMS
State-space Model
15.1 Formulate the state-space equations for Elman’s simple recurrent network shown in 
Fig. 15.3.
15.2 Show that the recurrent multilayer perceptron of Fig. 15.4 can be represented by the state-
space model:
where un denotes the input, yn denotes the output, xn denotes the state, and f(,) and
g(,) denote vector-valued nonlinear functions.
15.3 Is it possible for a dynamic system to be controllable and unobservable, and vice versa—
that is to say, the system is uncontrollable and observable? Justify your answers.
15.4 Referring to the problem of local controllability discussed in Section 15.4, show that
a. the state xn+q is a nested nonlinear function of its past value xn and the input vector uq,n
of Eq. (15.24), and
b. the Jacobian of xn+q with respect to uq,n, evaluated at the origin, is equal to the control-
lability matrix Mc of Eq. (15.23).
15.5 Referring to the problem of local observability discussed in Section 15.4, show that the
Jacobian of the observation vector yq,n defined in Eq. (15.30) with respect to the state xn,
evaluated at the origin, is equal to the observability matrix Mo of Eq. (15.28).
15.6 The system equation of a nonlinear dynamic system is described by
where un is the input vector at time n and xn is the corresponding state of the system.The input
un appears in the system equation in a nonadditive manner. In this problem, we wish to
reformulate the process equation so that the input un appears additively. This is done by
writing
Formulate definitions for the vectors xn and un and the function fnew().
15.7 Figure P15.7 presents two examples of recurrent network architectures using local feedback
at the neural level. The architectures shown in parts (a) and (b) of the figure are called
local activation feedback and local output feedback, respectively (Tsoi and Back, 1994).
Formulate state-space models for these two recurrent network architectures, and comment
on their controllability and observability.
Nonlinear Antoregressive with Exogenous Inputs (NARX) Model
15.8 Considering the NARX network of Fig. P15.8, do the following:
a. Construct the state-space model equivalent of this single-input, single-output recurrent
network.
b. Repeat part (a) of the problem for the case when the network of Fig. P15.8 is expanded
to embody two inputs and single output.
x¿n+1 = fnew(x¿n) + u¿n
xn+1 = f(xn, un)
yn = g(xn, un)
xn+1 = f(xn, un)

Problems
843
(a) Local activation feedback architecture
Σ
Activation
function
Bias
Model of neuron
w()
Output
yn
Input
un
xn
Linear
dynamic
system
•
•
•
(b) Local output feedback architecture
Σ
Activation
function
Bias
Model of neuron
w()
Output
yn
Input
un
xn
Linear
dynamic
system
•
•
•
FIGURE P15.7
z1
z1
z1
z1
z1
Input
un
Output
yn
un1
un2
yn2
yn1
yn
Bias
Hidden
neurons
Output
neuron
yn1
FIGURE P15.8
NARX network with q = 3 hidden neurons.

15.9 Construct the NARX equivalent for the fully recurrent network shown in Fig. P15.9.
15.10 Any state-space model can be represented by a NARX model.What about the other way
around? Can any NARX model be represented by a state-space model of the form described
in Section 15.2? Justify your answer.
Back Propagation Through Time
15.11 Unfold the temporal behavior of the state-space model shown in Fig. 15.3.
15.12 The truncated BPTT(h) algorithm may be viewed as an approximation to the epochwise
BPTT algorithm.The approximation can be improved by incorporating aspects of epochwise
BPTT into the truncated BPTT(h) algorithm. Specifically, we may let the network go
through h additional steps before performing the next BPTT computation, where h h.
The important feature of this hybrid form of back propagation through time is that the
next backward pass is not performed until time step n + h. In the intervening time, past val-
ues of the network input, network state, and desired responses are stored in a buffer, but
no processing is performed on them. Formulate the local gradient for neuron j in this hybrid
algorithm.
Real-Time Recurrent Learning Algorithm
15.13 The dynamics of a teacher-forced recurrent network during training are described in 
the following manner:
 i,n = •
ui,n 
if i   a
di,n
if i   c
yi,n
if i   b - c
844
Chapter 15
Dynamically Driven Recurrent Networks
Input
un
Bias
Output
yn1
z1
z1
z1
z1
FIGURE P15.9

where a denotes the set of indices i for which i is an external input,b denotes the set of indices
i for which i is the output of a neuron,and c denotes the set of output neurons that are visible.
a. Show that for this scheme, the partial derivative 
is given by
b. Derive the training algorithm for a teacher-forced recurrent network.
Nonlinear Sequential State Estimators
15.14 Describe how the DEKF algorithm can be used to train the simple recurrent network shown
in Fig. 15.3 You may also invoke the BPTT algorithm for this training.
15.15 Table 15.2 presents a summary of the EKF algorithm for the supervised training of an
RMLP.Using the square-root filtering theory described in Chapter 14,formulate the square-
root modification of this algorithm.
15.16 The sampling–importance-resampling (SIR) particle filter was described in Chapter 14.
This filter is derivative free;it would therefore be tempting to suggest its use as an alternative
to the EKF algorithm for supervised training of a recurrent multilayer perceptron. Discuss
the possible difficulty of such an approach.
Computer Experiment
15.17 In this problem, we continue with the computer experiment described in Problem 6.25 
on support vector machines,page 312.Specifically,we address a difficult pattern-classification
experiment involving the tightly fisted multicircular structure of Fig. P6.25 reproduced here
as Fig. P15.18 for convenience of presentation.This time however, we study the supervised
0yj,n+1
0wkl,n
= ¿(vj,n) a
a
ib-c
wji,n a
0yi,n
0wkl,n
b + kjl,n b
0yj,n+10wkl,n


Problems
845
y
x
d3
d2
d1
FIGURE P15.17
Diameters of the three circles: d1 = 3, d2 = 6, d3 = 9.

training of a multilayer perceptron, based on the extended Kalman filtering algorithm,
along the lines described in Section 15.10.
For the multilayer perceptron, use the following structure:
• Two hidden layers, with four neurons in the first hidden layer and three in the second
hidden layer; the activation function 
is to be used for all the hidden
neurons.
• Linear output layer.
To perform the pattern classification, generate 100 epochs with each one consisting of 200
randomly distributed training examples and an equal size of test data for the two regions
of Fig. P15.17. Hence, do the following:
1. For a varying number of epochs, construct the decision boundary computed by the
EKF algorithm so as to determine the “best” classification performance.
2. For the classification performance considered to be the “best,” determine the mis-
classification error.
Finally, compare the results of your findings for the EKF algorithm against the corre-
sponding results obtained for the support vector machine in Problem 6.25.
(v) = tanh(v)
846
Chapter 15
Dynamically Driven Recurrent Networks

Aarts,E.,and J.Korst,1989.Simulated Annealing and Boltzmann Machines:A Stochastic Approach
to Combinatorial Optimization and Neural Computing, New York:Wiley.
Abarbanel, H.D.I., 1996. Analysis of Observed Chaotic Data, New York: Springer-Verlag.
Abraham, R., J.E. Marsden, and T. Ratiu, 1988. Manifolds, Tensor Analysis, and Applications,
2d ed., New York: Springer-Verlag.
Abraham, R.H., and C.D. Shaw, 1992. Dynamics of the Geometry of Behavior, Reading, MA:
Addison-Wesley.
Abramowitz, M., and I.A. Stegun, 1965. Handbook of Mathematical Functions with Formulas,
Graphs and Mathematical Tables, New York: Dover Publications.
Abu-Mostafa,Y.S., 1995.“Hints,” Neural computation, vol. 7, pp. 639–671.
Ackley, D.H., G.E. Hinton, and T.J. Sejnowski, 1985. “A Learning Algorithm for Boltzmann
Machines,” Cognitive Science, vol. 9, pp. 147–169.
Adetona, O., E. Garcia, and L.H. Keel, 2000. “A new method for control of discrete nonlin-
ear dynamic systems using neural networks,” IEEE Trans. Neural Networks, vol. 11, pp.
102–112.
Aiyer, S.V.B., N. Niranjan, and F. Fallside, 1990.“A theoretical investigation into the performance
of the Hopfield model,” IEEE Transactions on Neural Networks, vol. 15, pp. 204–215.
Aizerman, M.A., E.M. Braverman, and L.I. Rozonoer, 1964a. “Theoretical foundations of the
potential function method in pattern recognition learning,” Automation and Remote Control,
vol. 25, pp. 821–837.
Aizerman, M. A., E.M. Braverman, and L.I. Rozonoer, 1964b. “The probability problem of pat-
tern recognition learning and the method of potential functions,” Automation and Remote
Control, vol. 25, pp. 1175–1193.
Alspach, D.L. and H.W. Sorenson, 1972. “Nonlinear Bayesian estimation using Gaussian sum
approximations,” IEEE Trans.Automatic Control, vol. 17, pp. 439–448.
Aleksander, I., and H. Morton, 1990, An Introduction to Neural Computing, London: Chapman
and Hall.
Amari, S., 1998. “Natural gradient works efficiently in learning.” Neural Computation, vol. 10,
pp. 251–276.
Amari, S., 1993.“A universal theorem on learning curves,” Neural Networks, vol. 6, pp. 161–166.
Amari, S., 1990.“Mathematical foundations of neurocomputing,” Proceedings of the IEEE, vol. 78,
pp. 1443–1463.
Bibliography
847

Amari, S., 1987.“Differential geometry of a parametric family of invertible systems—Riemanian
metric,dual affine connections and divergence,”Mathematical Systems Theory, vol.20,pp.53–82.
Amari, S., 1985. Differential-Geometrical Methods in Statistics, New York: Springer-Verlag.
Amari, S., 1983.“Field theory of self-organizing neural nets,” IEEE Transactions on Systems, Man,
and Cybernetics vol. SMC-13, pp. 741–748.
Amari, S., 1980. “Topographic organization of nerve fields,” Bulletin of Mathematical Biology,
vol. 42, pp. 339–364.
Amari, S., 1977a. “Neural theory of association and concept-formation,” Biological Cybernetics,
vol. 26, pp. 175–185.
Amari,S.,1977b.“Dynamics of pattern formation in lateral-inhibition type neural fields,”Biological
Cybernetics, vol. 27, pp. 77–87.
Amari, S., 1972.“Characteristics of random nets of analog neuron-like elements,” IEEE Transac-
tions on Systems, Man, and Cybernetics, vol. SMC-2, pp. 643–657.
Amari, S., 1967. “A theory of adaptive pattern classifiers,” IEEE Trans. Electronic Computers,
vol. EC-16, pp. 299–307.
Amari, S., and M.A.Arbib, 1977.“Competition and cooperation in neural nets,” in J. Metzler, ed.,
Systems Neuroscience, pp. 119–165, New York:Academic Press.
Amari,S.,and J.-F.Cardoso,1997.“Blind source separation—Semiparametric statistical approach,”
IEEE Transactions on Signal Processing, vol. 45, pp. 2692–2700.
Amari, S.,T.-P. Chen, and A. Cichoki, 1997. “Stability analysis of learning algorithms for blind
source separation,” Neural Networks, vol. 10, pp. 1345–1351.
Amari, S.,A.Cichoki, and H.H.Yang, 1996.“A new learning algorithm for blind signal separation.”
Advances in Neural Information Processing Systems, vol. 8, pp. 757–763, Cambridge, MA: MIT
Press.
Amari, S., and K. Maginu, 1988. “Statistical neurodynamics of associative memory,” Neural
Networks, vol. 1, pp. 63–73.
Amari, S., K. Yoshida, and K.-I. Kanatani, 1977. “A mathematical foundation for statistical neu-
rodynamics,” SIAM Journal of Applied Mathematics, vol. 33, pp. 95–126.
Ambros-Ingerson, J., R. Granger, and G. Lynch, 1990.“Simulation of paleo-cortex performs hier-
archical clustering,” Science, vol. 247, pp. 1344–1348.
Amit, D.J., 1989. Modeling Brain Function: The World of Attractor Neural Networks, New York:
Cambridge University Press.
Anastasio T.J., 2003. “Vestibulo-ocular reflex,” In M.A. Arbib, ed., The Handbook of Brain The-
ory and Neural Networks, 2d ed., pp. 1192–1196, Cambridge, MA: MIT Press.
Anastasio,T.J., 1993.“Modeling vestibulo-ocular reflex dynamics: From classical analysis to neural
networks,” in F. Eeckman, ed., Neural Systems: Analysis and Modeling, pp. 407–430, Norwell,
MA: Kluwer.
Anderson,B.D.O.,and J.B.Moore,1971.Linear Optimal Control, Englewood Cliffs,NJ:Prentice-Hall.
Anderson, J.A., 1995. Introduction to Neural Networks, Cambridge, MA: MIT Press.
Anderson, J.A., 1993.“The BSB model:A simple nonlinear autoassociative neural network,” in
Associative Neural Memories (M. Hassoun, ed.) pp. 77–103, Oxford: Oxford University Press.
Anderson, J.A., and E. Rosenfeld, eds., 1988. Neurocomputing: Foundations of Research,
Cambridge, MA: MIT Press.
Anderson, J.A., A. Pellionisz, and E. Rosenfeld, eds., 1990a. Neurocomputing 2: Directions for
Research, Cambridge, MA: MIT Press.
848
Bibliography

Anderson, J.A., J.W. Silverstein, S.A. Ritz, and R.S. Jones, 1977.“Distinctive features, categorical
perception, and probability learning: Some applications of a neural model,” Psychological
Review, vol. 84, pp. 413–451.
Anderson, J.A., and J.P. Sutton, 1995. “A network of networks: Computation and neurobiology,”
World Congress on Neural Networks, vol. I, pp. 561–568.
Anderson,T.W., 1984. An Introduction to Multivariate Statistical Analysis, 2d ed., New York:Wiley.
Ansari, N., and E. Hou, 1997. Computational Intelligence for Optimization, Norwell, MA: Kluwer.
Arasaratnam, I., and S. Haykin, 2009.“Cubature Kalman filters,” IEEE Trans.Automatic Control,
vol. 54, June.
Arasaratnam, I., S. Haykin, and R.J. Elliott, 2007. “Discrete-time nonlinear-filtering algorithms
using Gauss–Hermite quadrature,” Proc. IEEE, vol. 95, pp. 953–977.
Arbib, M.A., 1989. The Metaphorical Brain, 2d ed., New York:Wiley.
Arbib, M.A., 1987. Brains, Machines, and Mathematics, 2d ed., New York: Springer-Verlag.
Arbib, M.A., 2003. The Handbook of Brain Theory and Neural Networks, 2d ed., Cambridge, MA:
MIT Press.
Arimoto, S., 1972.“An algorithm for calculating the capacity of an arbitrary memoryless channel,”
IEEE Trans. Information Theory, vol. IT-18, pp. 14–20.
Aronszajn, N., 1950. “Theory of reproducing kernels,” Trans. American Mathematical Society,
vol. 68, pp. 337–404.
Arrowsmith, D.K., and C.M. Place, 1990. An Introduction to Dynamical Systems, Cambridge,
U.K.: Cambridge University Press.
Ash, R.E., 1965. Information Theory, New York:Wiley.
Ashby,W.R., 1960. Design for a Brain, 2d ed., New York:Wiley.
Ashby,W.R., 1952. Design for a Brain, New York:Wiley.
Aspray, W., and A. Burks, 1986. Papers of John von Neumann on Computing and Computer
Theory, Charles Babbage Institute Reprint Series for the History of Computing, vol. 12.
Cambridge, MA: MIT Press.
Atick, J.J., 1992.“Could information theory provide an ecological theory of sensory processing?”
Network: Computation in Neural Systems, vol. 3, pp. 213–251.
Atick, J.J., and A.N. Redlich, 1990.“Towards a theory of early visual processing,” Neural Compu-
tation, vol. 2, pp. 308–320.
Atiya,A.F.,1987,“Learning on a general network,”In Neural Information Processing Systems, D.Z.
Anderson, ed., pp. 22–30, New York:American Institute of Physics.
Attneave, F., 1954. “Some informational aspects of visual perception,” Psychological Review,
vol. 61, pp. 183–193.
Back,A.D.,and A.C.Tsoi,1991.“FIR and IIR synapses,a new neural network architecture for time
series modeling,” Neural Computation, vol. 3, pp. 375–385.
Bakir, G.H.,T. Hofmann, B. Schölkopf,A.J. Smola, B.Taskar, and S.V.N.Vishwanathan, eds., 2007.
Predicting Structured Data, Cambridge, MA: MIT Press.
Barlow, H.B., 1989.“Unsupervised learning,” Neural Computation, vol. 1, pp. 295–311.
Barlow, H.B., 1959.“Sensory mechanisms, the reduction of redundancy, and intelligence,” in The
Mechanisation of Thought Processes, National Physical Laboratory Symposium No. 10, Her
Majesty’s Stationary Office, London.
Barlow, H., and P. Földiák, 1989.“Adaptation and decorrelation in the cortex,” in The Computing
Neuron, R. Durbin, C. Miall, and G. Mitchison, eds., pp. 54–72. Reading, MA:Addison-Wesley.
Bibliography
849

Barnard, E., and D. Casasent, 1991. “Invariance and neural nets,” IEEE Transactions on Neural
Networks, vol. 2, pp. 498–508.
Barron,A.R., 1993.“Universal approximation bounds for superpositions of a sigmoidal function,”
IEEE Transactions on Information Theory, vol. 39, pp. 930–945.
Barron,A.R., 1992.“Neural net approximation,” in Proceedings of the Seventh Yale Workshop on
Adaptive and Learning Systems, pp. 69–72, New Haven, CT:Yale University.
Barto, A.G., S.J. Bradtke, and S. Singh, 1995. “Learning to act using real-time dynamic program-
ming,” Artificial Intelligence, vol. 72, pp. 81–138.
Barto,A.G., R.S. Sutton, and C.W.Anderson, 1983.“Neuronlike adaptive elements that can solve
difficult learning control problems,” IEEE Transactions on Systems, Man, and Cybernetics,
vol. SMC-13, pp. 834–846.
Battiti, R., 1992. “First- and second-order methods for learning: Between steepest descent and
Newton’s method,” Neural Computation, vol. 4, pp. 141–166.
Bauer, H.-U., and K.R. Pawelzik, 1992. “Quantifying the neighborhood preservation of self-
organizing feature maps,” IEEE Transactions on Neural Networks, vol. 3, pp. 570–579.
Bauer, H.-U., R. Der, and M. Hermman, 1996. “Controlling the magnification factor of self-
organizing feature maps,” Neural Computation, vol. 8, pp. 757–771.
Baum, E.B., and F.Wilczek, 1988.“Supervised learning of probability distributions by neural net-
works,” in D.Z.Anderson, ed., pp. 52–61, New York:American Institute of Physics.
Beaufays, F., and E.A. Wan, 1994. “Relating real-time backpropagation and backpropagation-
through-time: An application of flow graph interreciprocity,” Neural Computation, vol. 6,
pp. 296–306.
Becker,S.,1996.“Mutual information maximization:models of cortical self-organization,”Network:
Computation in Neural Systems, vol. 7, pp. 7–31.
Becker, S., 1991. “Unsupervised learning procedures for neural networks,” International Journal
of Neural Systems, vol. 2, pp. 17–33.
Becker, S., and G.E. Hinton, 1992. “A self-organizing neural network that discovers surfaces in
random-dot stereograms,” Nature (London), vol. 355, pp. 161–163.
Becker,S.,and Y.LeCun,1989.“Improving the convergence of back-propagation learning with sec-
ond order methods,” In D.Touretzky, G.E. Hinton, and T.J. Sejnowski, eds., Proceedings of the
1988 Connectionist Models Summer School, pp. 29–37, San Fransisco: Morgan Kaufmann.
Beckerman, M., 1997. Adaptive Cooperative Systems, New York:Wiley (Interscience).
Belkin,M.,2003.Problems of Learning on Manifolds, Ph.D.thesis,The University of Chicago.
Belkin, M., P. Niyogi, and V. Sindhwani, 2006. “Manifold regularization: A geometric framework
for learning from labeled and unlabeled examples,” J. Machine Learning Research, vol. 7,
pp. 2399–2434.
Bell, A.J. and T.J. Sejnowski, 1997. “The ‘Independent Components’ of natural scenes are edge
filters,” Vision Research, vol. 37, pp. 3327–3338.
Bell, A.J., and T.J. Sejnowski, 1995. “An information-maximization approach to blind separation
and blind deconvolution,” Neural Computation, vol. 6, pp. 1129–1159.
Bellman, R., 1961. Adaptive Control Processes:A Guided Tour, Princeton, NJ: Princeton University
Press.
Bellman, R., 1957. Dynamic Programming, Princeton, NJ: Princeton University Press.
Bellman, R., and S.E. Dreyfus, 1962. Applied Dynamic Programming, Princeton, NJ: Princeton
University Press.
850
Bibliography

Bengio,Y., and Y. LeCun, 2007.“Scaling learning algorithms toward AI,” in L. Bottou, O. Chapelle,
D. DeCosta, and J. Weston, eds., Large-Scale Kernel Machines, pp. 321–359, Cambridge, MA:
MIT Press.
Bengio, Y., P. Simard, and P. Frasconi, 1994. “Learning long-term dependencies with gradient
descent is difficult,” IEEE Transactions on Neural Networks, vol. 5, pp. 157–166.
Bengtsson,T., P. Bickel, and B. Li, 2008.“Curse-of-dimensionality revisited: Collapse of the parti-
cle filter in very large scale systems,” IMS Collections, Probability and Statistics: Essays in
Honor of David A. Freedman, vol. 2, pp. 316–334.
Benveniste, A., M. Métivier, and P. Priouret, 1987. Adaptive Algorithms and Stochastic Approxi-
mation, New York: Springer-Verlag.
Bertsekas, D.P., 2007. Dynamic Programming and Optimal Control, vol. II, 3d ed., Nashua, NH:
Athena Scientific.
Bertsekas, D.P., 2005. Dynamic Programming and Optimal Control, vol. I, 3d ed., Nashua, NH:
Athena Scientific.
Bertsekas,D.P.,A.Nedich,and V.S.Borkar,2004.“Improved temporal difference methods with linear
function approximation,” in J. Si, A.G. Barto, W.B. Powell, and D. Wunsch II, eds., Handbook of
Learning and Approximate Dynamic Programming,pp.235–259,Hobken,NJ:Wiley-Interscience.
Bertsekas, D.P., with A. Nedich and A.E. Ozdaglar, 2003. Convex Analysis and Optimization,
Nashua, NH:Athena Scientific.
Bertsekas, D.P., and J.N.Tsitsiklis, 2002. Introduction to Probability, Nashua, NH:Athena Scientific.
Bertsekas, D.P.,1995. Nonlinear Programming. Belmont, MA:Athenas Scientific.
Bertsekas,D.P.,and J.N.Tsitsiklis,1996.Neuro-Dynamic Programming, Belmont,MA:Athena Scientific.
Bierman, G.J., and C.L.Thornton, 1977.“Numerical comparison of Kalman filter algorithms: Orbit
determination case study,” Automatica, vol. 13, pp. 23–35.
Bishop, C.M., 1995. Neural Networks for Pattern Recognition. Oxford, U.K.: Clarendon Press.
Blahut, R., 1972. “Computation of channel capacity and rate distortion functions,” IEEE Trans.
Information Theory, vol. IT-18, pp. 460–473.
Bobrowski,O.,R.Meir,and Y.C.Eldor,2007.“Bayesian filtering in spiking neural networks:Noise,
adaptation,and multisensory integration,”Neural Information Processing Systems (NIPS) Con-
ference,Vancouver: December.
Bodenhausen, U., and A.Waibel, 1991.“The tempo 2 algorithm:Adjusting time-delays by super-
vised learning,” Advances in Neural Information Processing Systems, vol. 3, pp. 155–161, San
Mateo, CA: Morgan Kaufmann.
Boltzmann, L., 1872. “Weitere studien über das Wärmegleichgewicht unter gasmolekülen,”
Sitzungsberichte der Mathematisch-Naturwissenschaftlichen Classe der Kaiserlichen Akademie
der Wissenschaften, vol. 66, pp. 275–370.
Boothby, W.M., 1986. An Introduction to Differentiable Manifolds and Riemannian Geometry,
2d ed., Orlando, FL:Academic Press.
Boser, B., I. Guyon, and V.N. Vapnik, 1992. “A training algorithm for optimal margin classifiers,”
Fifth Annual Workshop on Computational Learning Theory, pp. 144–152. San Mateo, CA:
Morgan Kaufmann.
Bottou, L., 2007. Learning Using Large Datasets, NIPS 2007 Conference Tutorial Notes, Neural
Information Processing Systems (NIPS) Conference,Vancouver: December.
Bottou, L., and C. Lin, 2007. “Support vector machine solvers,” in L. Bottou, O. Chapelle,
D. DeCosta, and J.Weston, eds., Large-Scale Kernel Machines, pp. 1–27, Cambridge, MA: MIT
Press.
Bibliography
851

Boyan,J.A.,2002.“Technical update:Least-squares temporal difference learning,”Machine Learning,
vol. 49, pp. 1–15.
Boyd, S., and L. Vandenberghe, 2004. Convex Optimization. Cambridge, U.K., and New York:
Cambridge University Press.
Bradtke, S.J., and A.G. Barto, 1996.“Linear least-squares algorithms for temporal difference learn-
ing,” Machine Learning, vol. 22, pp. 33–57.
Braitenberg,V., 1990.“Reading the structure of brains,” Network: Computation in Neural Systems,
vol. 1, pp. 1–12.
Braitenberg,V., 1986.“Two views of the cerebral cortex,” in Brain Theory, G. Palm and A.Aertsen,
eds., pp. 81–96. New York: Springer-Verlag.
Braitenberg,V., 1984. Vehicles: Experiments in Synthetic Psychology, Cambridge, MA: MIT Press.
Braitenberg,V., 1977. On the Texture of Brains, New York: Springer-Verlag.
Braitenberg,V., 1967.“Is the cerebella cortex a biological clock in the millisecond range?” in The
Cerebellum. Progress in Brain Research, C.A. Fox and R.S. Snider, eds., vol. 25 pp. 334–346,
Amsterdam: Elsevier.
Bregman,A.S., 1990. Auditory Scene Analysis:The Perceptual Organization of Sound, Cambridge,
MA: MIT Press.
Brodal,A., 1981. Neurological Anatomy in Relation to Clinical Medicine, 3d ed., New York: Oxford
University Press.
Brogan,W.L., 1985. Modern Control Theory, 2d ed., Englewood Cliffs, NJ: Prentice-Hall.
Broomhead, D.S., and D. Lowe, 1988. “Multivariable functional interpolation and adaptive net-
works,” Complex Systems, vol. 2, pp. 321–355.
Brown,T.H.,E.W.Kairiss,and C.L.Keenan,1990.“Hebbian synapses:Biophysical mechanisms and
algorithms,” Annual Review of Neuroscience, vol. 13, pp. 475–511.
Bruck, J., 1990.“On the convergence properties of the Hopfield model,” Proceedings of the IEEE,
vol. 78, pp. 1579–1585.
Bryson,A.E., Jr., and Y.C. Ho, 1969. Applied Optimal Control, Blaisdell. (Revised printing, 1975,
Hemisphere Publishing,Washington, DC).
Cacoullos, T., 1966. “Estimation of a multivariate density,” Annals of the Institute of Statistical
Mathematics (Tokyo), vol. 18, pp. 179–189.
Caianiello, E.R., 1961.“Outline of a theory of thought-processes and thinking machines,” Journal
of Theoretical Biology, vol. 1, pp. 204–235.
Cameron, S.H., 1960. Tech. Report 60–600, Proceedings of the Bionics Symposium, pp. 197–212,
Wright Air Development Division, Dayton, Ohio.
Cappé, O., S.J. Godsill, and E. Moulines, 2007. “An overview of existing methods and recent
advances in sequential Monte Carlo,” Proc. IEEE, vol. 95, pp. 899–924.
Cappé, O., E. Moulines, and T. Rydén, 2005. Inference in Hidden Markov Models, New York and
London: Springer.
Cardoso, J.F., 2003.“Dependence, correlation and Gaussianity in independent component analy-
sis,” J. Machine Learning Research, vol. 4, pp. 1177–1203.
Cardoso, J.F., 2001. “The three easy routes to independent component analysis: Contrasts and
geometry,” Proceedings of 3rd International Conference on Independent Component Analysis
and Blind Source Separation, San Diego, December.
Cardoso, J.-F., 1998. “Blind signal separation: A review,” Proceedings of the IEEE, vol. 86,
pp. 2009–2025.
852
Bibliography

Cardoso, J-F., 1997.“Infomax and maximum likelihood for blind source separation,” IEEE Signal
Processing Letters, vol. 4, pp. 112–114.
Cardoso, J.-F., and B. Laheld, 1996.“Equivariant adaptive source separation,” IEEE Transactions
on Signal Processing, vol. 44, pp. 3017–3030.
Carpenter, G.A., M.A. Cohen, and S. Grossberg, 1987.Technical comments on “Computing with
neural networks,” Science, vol. 235, pp. 1226–1227.
Cˇ erny,V.,1985.“Thermodynamic approach to the travelling salesman problem,”Journal of Optimiza-
tion Theory and Applications, vol. 45, pp. 41–51.
Changeux, J.P., and A. Danchin, 1976. “Selective stabilization of developing synapses as a mech-
anism for the specification of neural networks,” Nature, vol. 264, pp. 705–712.
Chapelle,O.,B.Schölkopf,and A.Zien,2006.Semi-Supervised Learning, Cambridge,MA:MIT Press.
Charlin, L., P. Poupart, and R. Shoida, 2007.“Automated hierarchy discovery for planning in par-
tially observable environments,” Advances in Neural Information Processing Systems, vol. 19,
pp. 225–232.
Chatterjee, C.,V.P. Roychowdhhury, and E.K.P. Chong, 1998.“On relative convergence properties
of principal component algorithms,”IEEE Transactions on Neural Networks, vol.9,pp.319–329.
Chechik,G.,A.Globerson,N.Tishby,and Y.Weiss,2004.“Information bottleneck for Gaussian vari-
ables,” Advances in Neural Information Processing Systems, vol. 16, pp. 1213–1220.
Chen, S., S. Billings, and P. Grant, 1990.“Non-linear system identification using neural networks,”
International Journal of ‘Control, vol. 51, pp. 1191–1214.
Chen, Z., S. Haykin, J.J. Eggermont, and S. Becker, 2007. Correlative Learning:A Basis for Brain
and Adaptive Systems, New York:Wiley-Interscience.
Cherkassky, V., and F. Mulier, 1998. Learning from Data: Concepts, Theory and Methods,
New York:Wiley.
Cherry, E.G., 1953.“Some experiments on the recognition of speech, with one and with two ears,”
Journal of the Acoustical Society of America, vol. 25, pp. 975–979.
Cherry, E.C., and W.K.Taylor, 1954.“Some further experiments upon the recognition of speech,
with one and with two ears,” Journal of Acoustical Society of America, vol. 26, pp. 554–559.
Chester, D.L., 1990.“Why two hidden layers are better than one,” International Joint Conference
on Neural Networks, vol. I, pp. 265–268,Washington, D.C.
Chigirev, D. and W. Bialek, 2004. “Optimal manifold representation of data: An information-
theoretic approach,” Advances in Neural Information Processing Systems, vol. 16, pp. 161–168.
Choi, H., and R.G. Baraniuk, 1999. “Multiple basis wavelet denoising using Besov projections,”
Proceedings of IEEE International Conference on Image Processing, pp. 595–599.
Choi, S., A. Cichoki, H.M. Park, and S.Y. Lee, 2005. “Blind source separation and independent
component analysis: A review,” Neural Information Processing-Letters and Reviews, vol. 6,
pp. 1–57.
Chung, R.K., 1997. Spectral Graph Theory, Regional Conference Series in Mathematics,
Number 92, Providence, RI:American Mathematical Society.
Churchland, P.S., and T.J. Sejnowski, 1992. The Computational Brain, Cambridge, MA: MIT Press.
Cichocki,A.,and S.Amari,2002.Adaptive Blind Signal and Image Processing:Learning Algorithms
and Applications, Chichester, NY:Wiley-Interscience.
Cleeremans,A.,D.Servan-Schreiber,and J.L.McClelland,1989.“Finite state automata and simple
recurrent networks,” Neural Computation, vol. 1, pp. 372–381.
Bibliography
853

Cohen,L.,2005.“The history of noise [on the 100th anniversary of its birth],”IEEE Signal Process-
ing Magazine, vol. 22, issue 6, pp. 20–45, November.
Cohen, M. A., and S. Grossberg, 1983. “Absolute stability of global pattern formation and paral-
lel memory storage by competitive neural networks,” IEEE Transactions on Systems, Man, and
Cybernetics, vol. SMC-13, pp. 815–826.
Collins, M., S. Dasgupta, and R.E. Schapire, 2002. “A generation of principal components analy-
sis to the exponential family,” Advances in Neural Information Processing Systems, vol. 14-1,
pp. 617–624, Cambridge, MA: MIT Press.
Comon, P., 1994. “Independent component analysis: A new concept?” Signal Processing, vol. 36,
pp. 287–314.
Comon, P., 1991.“Independent component analysis,” Proceedings of International Signal Process-
ing Workshop on Higher-order Statistics, pp. 111–120, Chamrousse, France.
Cook,A.S., 1971.“The complexity of theorem-proving procedures,” Proceedings of the 3rd Annual
ACM Symposium on Theory of Computing, pp. 151–158, New York.
Cook, P.A., 1986. Nonlinear Dynamical Systems, London: Prentice-Hall International.
Cools,R.,2002.“Advances in multidimensional integration,”J.Comput.and Applied Math., vol.149,
pp. 1–12.
Cools, R., 1997.“Computing cubature formulas:The science behind the art,” Acta Numerica, vol. 6,
pp. 1–54, Cambridge, U.K.: Cambridge University Press.
Cormen,T.H., C.E. Leiserson, and R.R. Rivest, 1990. Introduction to Algorithms. Cambridge, MA:
MIT Press.
Cortes, C, and V.Vapnik, 1995.“Support vector networks,” Machine Learning, vol. 20, pp. 273–297.
Cottrell,M.,J.C.Fort,and G.Pagés,1997.“Theoretical aspects of the SOM algorithm,”Proceedings
of the Workshop on Self-Organizing Maps, Espoo, Finland.
Courant, R., and D. Hilbert, 1970. Methods of Mathematical Physics, vol. I and II, New York:Wiley
Interscience.
Cover, T.M., and J.A. Thomas, 2006. Elements of Information Theory, 2d ed., Hoboken, NJ:
Wiley-Interscience.
Cover,T.M., 1968.“Capacity problems for linear machines,” In L. Kanal, ed., Pattern Recognition,
pp. 283–289,Washington, DC:Thompson Book Co.
Cover, T.M., 1965. “Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition,” IEEE Transactions on Electronic Computers, vol. EC-14,
pp. 326–334.
Cover,T.M., and P.E. Hart, 1967.“Nearest neighbor pattern classification,” IEEE Transactions on
Information Theory, vol. IT-13, pp. 21–27.
Cowan,J.D.,1968.“Statistical mechanics of nervous nets,”in Neural Networks, E.R.Caianiello,ed.,
pp. 181–188, Berlin: Springer-Verlag.
Cragg, B.G., and H.N.V.Tamperley, 1954.“The organization of neurons:A cooperative analogy,”
EEG Clinical Neurophysiology, vol. 6, pp. 85–92.
Craik, K.J.W., 1943. The Nature of Explanation, Cambridge, U.K.: Cambridge University Press.
Craven,P.,and G.Wahba,1979.“Smoothing noisy data with spline functions:Estimating the correct
degree of smoothing by the method of generalized cross-validation,” Numerische Mathematik.,
vol. 31, pp. 377–403.
854
Bibliography

Crisan, D., and A. Doucet, 2002.“A survey of convergence results on particle filtering methods for
practitioners,” IEEE Trans. Signal Processing, vol. 50, pp. 736–746.
Crites, R.H., and A.G. Barto, 1996.“Improving elevator performance using reinforcement learn-
ing,” Advances in Neural Information Processing Systems, vol. 8, pp. 1017–1023, Cambridge,
MA: MIT Press.
Csiszár, I., and G. Tusnády, 1984. “Information geometry and alternating minimization proce-
dures,” Statistics and Decisions, Supplement Issue, vol. I, 205–237.
Cucker, F., and S. Smale, 2001. “On the mathematical foundations of learning,” Bulletin (New
Series) of the American Mathematical Society, vol. 39, pp. 1–49.
Cybenko, G., 1995.“Q-learning:A tutorial and extensions.” Presented at Mathematics of Artificial
Neural Networks, Oxford University, Oxford, U.K., July 1995.
Cybenko, G, 1989. “Approximation by superpositions of a sigmoidal function,” Mathematics of
Control, Signals, and Systems, vol. 2, pp. 303–314.
Cybenko, G., 1988. “Approximation by superpositions of a sigmoidal function,” Urbana, IL.:
University of Illinois.
Dan,Y., J.J.Atick, and R.C. Reid, 1996.“Efficient coding of natural scenes in the lateral genic-
ulate nucleus: Experimental test of a computational theory,” J. of Neuroscience, vol. 16,
pp. 3351–3362.
Darken, C., and J. Moody, 1992.“Towards faster stochastic gradient search,” Advances in Neural
Information Processing Systems, vol. 4, pp. 1009–1016, San Mateo, CA: Morgan Kaufmann.
Darken, C., and J. Moody, 1991.“Note on learning rate schedules for stochastic optimization,” in
R.P. Lippmann, J.E. Moody, and D.S. Touretzky, Advances in Neural Information Processing
Systems, pp. 832–838, San Mateo, CA: Morgan Kaufmann.
Darmois, G., 1953.“Analyse générale des liaisons stochastiques,” Rev. Inst. Internal. Stat., vol. 21,
pp. 2–8.
Daubechies, I.,ed.,1993.Different Perspectives on Wavelets,American Mathematical Society Short
Course, San Antonio, January 11–12.
Daubechies, I., 1992. Ten Lectures on Wavelets, SIAM.
Daubechies, I., 1990.“The wavelet transform, time-frequency,” IEEE Transactions on Information
Theory, vol. IT-36, pp. 961–1005.
Daum, F. and J. Huang, 2003. “Curse of dimensionality and particle filters,” Proceedings, IEEE
Aerospace Conference, vol. 4, pp. 1979–1993, March.
Davis, P.J., 1963. Interpolation and Approximation, New York: Blaisdell.
Debnath, L., and P. Mikusi ´nski, 1990. Introduction to Hilbert Spaces with Applications, New York:
Academic Press.
de Figueiredo,R.J.P.,and G.Chen,1993.Nonlinear Feedback Control Systems, New York:Academic
Press.
Dempster,A.P.,N.M.Laird,and D.B.Rubin,1977.“Maximum likelihood from incomplete data via
the EM algorithm,”(with discussion),Journal of the Royal Statistical Society., B,vol.39,pp.1–38.
Denardo, E.V., 1967. “Contraction mappings in the theory underlying dynamic programming,”
SIAM, Review, vol. 9, pp. 165–177.
DeSieno, D., 1988. “Adding a conscience to competitive learning,” IEEE International Confer-
ence on Neural Networks, vol. I, pp. 117–124, San Diego.
deSilva, C.J.S., and Y. Attikiouzel, 1992. “Hopfield networks as discrete dynamical systems,”
International Joint Conference on Neural Networks, vol. III, pp. 115–120, Baltimore.
Bibliography
855

DeVito, E., L. Rosasco, A. Caponnetto, U. DeGiovannini, and F. Odone, 2005. “Learning from
examples as an inverse problem.” J. Machine Learning Research, vol. 6, pp. 883–904.
deVries, B., and J.C. Principe, 1992.“The gamma model—A new neural model for temporal pro-
cessing,” Neural Networks, vol. 5, pp. 565–576.
Diamantaras, K.I., and S.Y. Kung, 1996. Principal Component Neural Networks:Theory and Appli-
cations, New York:Wiley.
Ding, C., 2004. Spectral Clustering, Tutorial Notes, ICML, Banff,Alberta, Canada, July.
Ding,C.,and X.He,2004.“K-means clustering via principal component analysis,”Proceedings of the
Twenty-first International Conference on Machine Learning, pp.225–240,Banff,Alberta,Canada.
Diniz, P.S.R., 2002. Adaptive Filtering: Algorithms and Practical Implementation, 2d ed., Boston:
Kluwer Academic Publishers.
Dong,D.W.,and J.J.Atick,1995.“Temporal decorrelation:A theory of lagged and non-lagged responses
in the lateral geniculate nucleus,” Network: Computation in Neural Systems, vol. 6, pp. 159–178.
Dony, R.D., and S. Haykin, 1995. “Optimally adaptive transform coding,” IEEE Transactions on
Image Processing, vol. 4, pp. 1358–1370.
Dorny, C.N., 1975. A Vector Space Approach to Models and Optimization, New York:Wiley (Inter-
science).
Doucet,A., N. deFreitas, and N. Gordon, eds., 2001. Sequential Monte Carlo Methods in Practice,
New York: Springer.
Doucet,A., S. Godsill, and C.Andrieu, 2000.“On sequential Monte Carlo sampling methods for
Bayesian filtering,” Statistics and Computing, vol. 10, pp. 197–208.
Doya, K., S. Ishi,A. Pouget, and R.P.N. Rao, eds., 2007. Bayesian Brain: Probabilistic Approaches
to Neural Coding, Cambridge, MA: MIT Press.
Drineas, P., and M.W. Mahoney, 2005.“On the Nyström method for approximating a Gram matrix
for improved kernel-based learning,” J. Machine Learning Research, vol. 6, pp. 2153–2175.
Duda, R.O., P.E. Hart, and D.G. Stork, 2001. Pattern Classification, 2d ed., New York: Wiley-
Interscience.
Duda, R.O., and P.E. Hart, 1973. Pattern Classification and Scene Analysis, New York:Wiley.
Durbin, R., and G. Michison, 1990.“A dimension reduction framework for understanding cortical
maps,” Nature, vol. 343, pp. 644–647.
Durbin, R., C. Miall, and G. Mitchison, eds, 1989. The Computing Neuron, Reading, MA.Addison-
Wesley.
Durdanovic, I., E. Cosatto, and H. Graf, 2007.“Large-scale Parallel SVM implementation”. In L.
Bottou,O.Chapelle,D.DeCosta,and J.Weston,editors,Large-Scale Kernel Machines,pp.105–138,
MIT Press.
Eggermont, J.J., 1990. The Correlative Brain: Theory and Experiment in Neural Interaction, New
York: Springer-Verlag.
Elhilali, M., 2004. Neural Basis and Computational Strategies for Auditory Processing, Ph.D. the-
sis, University of Maryland.
Elliott, R.J., L.Aggoun, and J.B. Moore, 1995. Hidden Markov Models: Estimation and Control, New
York: Springer-Verlag.
Elman,J.E.,E.A.Bates,M.H.Johnson,A.Karmiloff-Smith,D.Parisi,and K.Plinket,1996.Rethinking
Innateness:A Connectionist Perspective on Development, Cambridge, MA: MIT Press.
Elman, J.L., 1990.“Finding structure in time,” Cognitive Science, vol. 14, pp. 179–211.
Erwin, E., K. Obermayer, and K. Schulten, 1995. “Models of orientation and ocular dominance
columns in the visual cortex:A critical comparison,” Neural Computation, vol. 7, pp. 425–468.
856
Bibliography

Erwin, E., K. Obermayer, and K. Schulten, 1992a. “I: Self-organizing maps: Stationary states,
metastability and convergence rate,” Biological Cybernetics, vol. 67, pp. 35–45.
Erwin, E., K. Obermayer, and K. Schulten, 1992b. “II: Self-organizing maps: Ordering, conver-
gence properties and energy functions,” Biological Cybernetics, vol. 67, pp. 47–55.
Feldkamp,L.A.,T.M.Feldkamp,and D.V.Prokhorov,2001.“Neural network training with the nprKF,”
Proceedings of the International Joint Conference on Neural Networks, Washington, DC.
Feldkamp, L., and G., Puskorius, 1998.“A signal processing framework based on dynamic neural
networks with application to problems in adaptation, filtering, and classification,” Proc. IEEE,
vol. 86, pp. 2259–2277.
Feldkamp, L.A., G.V. Puskorius, and P.C. Moore, 1997.“Adaptation from fixed weight networks,”
Information Sciences, vol. 98, pp. 217–235.
Feller, W., 1968. An Introduction to Probability Theory and its Applications, vol. 1, 3d ed., New
York: John Wiley; 1st printing, 1950.
Feller,W.,1971.An Introduction to Probability Theory and its Applications, 3d ed.,vol.II,New York:
Wiley; 1st printing, 1967.
Field, D.J., 1994.“What is the goal of sensory coding?” Neural computation, vol. 6, pp. 559–601.
Fischler,M.A.,and O.Firschein,1987.Intelligence:The Eye,The Brain,and The Computer, Reading,
MA:Addison-Wesley.
Fisher, R.A., 1925. “Theory of statistical estimation,” Proceedings of the Cambridge Philosophi-
cal Society, vol. 22, pp. 700–725.
Fletcher, R., 1987. Practical Methods of Optimization, 2d ed., New York:Wiley.
Forgey,E.,1965.“Cluster analysis of multivariate data:Efficiency vs.interpretability of classification,”
Biometrics, vol. 21, p. 768 (abstract).
Földiak, P., 1989. “Adaptive network for optimal linear feature extractions,” IEEE International
Joint Conference on Neural Networks, vol. I, pp. 401–405,Washington, DC.
Forney, G.D., Jr., 1973.“The Viterbi algorithm,” Proceedings of the IEEE, vol. 61, pp. 268–278.
Forte, J.C., and G. Pagés, 1996. “Convergence of stochastic algorithm: From the Kushner 
and Clark theorem to the Lyapunov functional,” Advances in Applied Probability, vol. 28,
pp. 1072–1094.
Forte, J.C., and G. Pagés, 1995.“On the a.s. convergence of the Kohonen algorithm with a general
neighborhood function,” Annals of Applied Probability, vol. 5, pp. 1177–1216.
Frasconi, P., and M. Gori, 1996.“Computational capabilities of local-feedback recurrent networks
acting as finite-state machines,” IEEE Transactions on Neural Networks, vol. 7, pp. 1521–1524.
Frasconi, P., M. Gori, and G. Soda, 1992. “Local feedback multilayered networks,” Neural Com-
putation, vol. 4, pp. 120–130.
Fraser,A.M., 1989.“Information and entropy in strange attractors,” IEEE Transactions on Infor-
mation Theory, vol. 35, pp. 245–262.
Freeman,W.J., 1975. Mass Action in the Nervous System, New York:Academic Press.
Friedman, J.H., 1995. “An overview of prediction learning and function approximation,” In 
V.Cherkassky,J.H.Friedman,and H.Wechsler,eds.,From Statistics to Neural Networks:Theory
and Pattern Recognition Applications, New York: Springer-Verlag.
Fukunaga, K., 1990. Statistical Pattern Recognition, 2d ed., New York:Academic Press.
Fukushima, K., 1995.“Neocognitron:A model for visual pattern recognition,” in M.A.Arbib, ed.,
The Handbook of Brain Theory and Neural Networks, Cambridge, MA: MIT Press.
Fukushima, K., 1980.“Neocognitron:A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position,” Biological Cybernetics, vol. 36,193–202.
Bibliography
857

Funahashi, K., 1989. “On the approximate realization of continuous mappings by neural net-
works,” Neural Networks, vol. 2, pp. 183–192.
Fyfe, C., 2005. Hebbian Learning and Negative Feedback Networks, New York: Springer.
Gabor, D., 1954. “Communication theory and cybernetics,” IRE Transactions on Circuit Theory,
vol. CT-1, pp. 19–31.
Gabor, D.,W.P.L.Wilby, and R.Woodcock, 1960.“A universal non-linear filter, predictor, and sim-
ulator which optimizes itself by a learning process,” Proceedings of the Institution of Electrical
Engineers, London, vol. 108, pp. 422–435.
Galland, C.C., 1993. “The limitations of deterministic Boltzmann machine learning,” Network,
vol. 4, pp. 355–379.
Gallant,A.R.,and H.White,1988.“There exists a neural network that does not make avoidable mis-
takes,” IEEE International Conference on Neural Networks, vol. I, pp. 657–664, San Diego.
Garey, M.R., and D.S. Johnson, 1979. Computers and Intractability, New York:W.H. Freeman.
Gee,A.H., 1993.“Problem solving with optimization networks,” Ph.D. dissertation, University of
Cambridge.
Gee,A.H., S.V.B.Aiyer, and R. Prager, 1993.“An analytical framework for optimizing neural net-
works.” Neural Networks, vol. 6, pp. 79–97.
Geisser, S., 1975.“The predictive sample reuse method with applications,” Journal of the American
Statistical Association, vol. 70, pp. 320–328.
Gelfand, A.E., and A.F.M. Smith, 1990. “Sampling-based approaches to calculating marginal
densities,” Journal of the American Statistical Association, vol. 85, pp. 398–409.
Geman, S., and D. Geman, 1984. “Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. PAMI-6, pp. 721–741.
Geman, S., E. Bienenstock, and R. Doursat, 1992. “Neural networks and the bias/variance
dilemma,” Neural Computation, vol. 4, pp. 1–58.
Genest, C., and J. MacKay, 1989.“The joy of copulas: Bivariate distributions with uniform margin-
als,” The American Statistician, vol. 40, pp. 280–285.
Gersho, A., and R.M. Gray, 1992. Vector Quantization and Signal Compression, Norwell, MA:
Kluwer.
Gibbs,J.W.,1902.“Elementary principles in statistical mechanics,”reproduced in vol.2 of Collected
Works of J.Willard Gibbs in Two Volumes, New York: Longmans, Green and Co., 1928.
Gidas, B., 1985.“Global optimization via the Langevin equation,” Proceedings of 24th Conference
on Decision and Control, pp. 774–778, Ft. Lauderdale, FL.
Giles, C.L., 1996. “Dynamically driven recurrent neural networks: Models, learning algorithms,
and applications,”Tutorial #4, International Conference on Neural Networks, Washington, DC.
Giles, C.L., D. Chen, G.Z. Sun, H.H. Chen, Y.C. Lee, and M.W. Goudreau, 1995. “Constructive
learning of recurrent neural networks: Limitations of recurrent cascade correlation with a sim-
ple solution,” IEEE Transactions on Neural Networks, vol. 6, pp. 829–836.
Giles, C.L., and B.G. Horne, 1994. “Representation of learning in recurrent neural network
architectures,” Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems,
pp. 128–134, Yale University, New Haven, CT.
Giles, C.L., G.Z. Sun, H.H. Chen,Y.C. Lee, and D. Chen, 1990.“Higher order recurrent networks
and grammatical inference,” Advances in Neural Information Processing Systems, vol. 2,
pp. 380–387, San Mateo CA: Morgan Kaufmann.
858
Bibliography

Girosi, F., 1998. “An equivalence between sparse approximation and support vector machines,”
Neural Computation, vol. 10, pp. 1455–1480.
Girosi, F, M. Jones, and T. Poggio, 1995. “Regularization theory and neural networks architec-
tures,” Neural Computation, vol. 7, pp. 219–269.
Glauber, R.J., 1963. “Time-dependent statistics of the Ising model,” Journal of Mathematical
Physics, vol. 4, pp. 294–307.
Golden, R.M., 1986. “The ‘Brain-State-in-a-Box’ neural model is a gradient descent algorithm,”
Journal of Mathematical Psychology, vol. 30, pp. 73–80.
Goles,E.,and S.Martinez,1990.Neural and Automata Networks, Dordrecht,The Netherlands:Kluwer.
Golub, G.H., and C.G. Van Loan, 1996. Matrix Computations, 3d ed., Baltimore: Johns Hopkins
University Press.
Goodwin, G.C., and K.S. Sin, 1984. Adaptive Filtering Prediction and Control, Englewood Cliffs,
NJ: Prentice-Hall.
Gordon, N.J., D.J. Salmond, and A.F.M. Smith, 1993.“Novel approach to nonlinear/non-Gaussian
Bayesian state estimation,” IEE Proceedings-F, vol. 140, pp. 107–113.
Goudreau,M.W.,C.L.Giles,S.T.Chakradhar,and D.Chen,1994.“First-order vs.second-order sin-
gle-layer recurrent neural networks,”IEEE Transactions on Neural Networks, vol.5,pp.511–513.
Grassberger, I., and I. Procaccia, 1983.“Measuring the strangeness of strange attractors,” Physica
D, vol. 9, pp. 189–208.
Gray, R.M., 1984.“Vector quantization,” IEEE ASSP Magazine, vol. 1, pp. 4–29.
Green,M.,and D.J.N.Limebeer,1995.Linear Robust Control, Englewood Cliffs,NJ:Prentice Hall.
Greenberg,H.J.,1988.“Equilibria of the brain-state-in-a-box (BSB) neural model,”Neural Networks,
vol. 1, pp. 323–324.
Grenander, U., 1983. Tutorial in Pattern Theory, Brown University, Providence, RI.
Griffiths, L.J., and C.W. Jim, 1982. “An alternative approach to linearly constrained optimum
beamforming,” IEEE Transactions on Antennas and Propagation, vol.AP-30, pp. 27–34.
Grossberg, S., 1990.“Content-addressable memory storage by neural networks:A general model
and global Liapunov method,” In Computational Neuroscience, E.L. Schwartz, ed., pp. 56–65,
Cambridge, MA: MIT Press.
Grossberg, S., 1988. Neural Networks and Natural Intelligence, Cambridge, MA: MIT Press.
Grossberg, S., 1982. Studies of Mind and Brain, Boston: Reidel.
Grossberg, S., 1969.“On learning and energy-entropy dependence in recurrent and nonrecurrent
signed networks,” Journal of Statistical Physics, vol. 1, pp. 319–350.
Grossberg, S., 1968. “A prediction theory for some nonlinear functional-difference equations,”
Journal of Mathematical Analysis and Applications, vol. 21, pp. 643–694, vol. 22, pp. 490–522.
Grossberg,S.,1967.“Nonlinear difference—differential equations in prediction and learning theory,”
Proceedings of the National Academy of Sciences, USA, vol. 58, pp. 1329–1334.
Grünwald, P.D., 2007. the Minimum Description Length principle, Cambridge, MA: MIT Press.
Guestrin, C., and G. Gordon, 2002. “Distributed planning in hierarchical factored MDPs,”
Proceedings of 18th Conference on Uncertainty in Artificial Intelligence, pp. 197–206,August.
Hadamard, J., 1902. “Sur les problèmes aux derivées partielles et leur signification physique,”
Bulletin, Princeton University, vol. 13, pp. 49–52.
Haft, M., and I.L. van Hemmen, 1998. “Theory and implementations of infomax filters for the
retina.” Network: Computations in Neural Systems, vol. 9, pp. 39–71.
Hagiwara, M., 1992. “Theoretical derivation of momentum term in back-propagation,”
International Joint Conference on Neural Networks, vol. I, pp. 682–686, Baltimore.
Bibliography
859

Hampson,S.E.,1990.Connectionistic Problem Solving:Computational Aspects of Biological Learn-
ing, Berlin: Birkhäuser.
Härdle,W., 1990. Applied Nonparametric Regression, Cambridge: Cambridge University Press.
Hardy, R.L., 1971. “Multiquadric equations of topography and other irregular surfaces,” Journal
of Geophysics Research, vol. 76, pp. 1905–1915.
Harel, D., 1987. ”Algorithmics:The Spirit of Computing,” Reading, MA:Addison-Wesley.
Hartline, H.K., 1940.“The receptive fields of optic nerve fibers,” American Journal of Physiology,
vol. 130, pp. 690–699.
Harvey, A., 1989. Forecasting, Structural Time Series Models and the Kalman Filter, Cambridge,
U.K., and New York: Cambridge University Press.
Hassibi, B., A.H. Sayed, and T. Kailath, 1999. Indefinite-Quadratic Estimation and Control: A
Unified Approach to H 2 and H q Theories, Studies in Applied and Numerical Mathematics,
SIAM, Philadelphia: Society for Industrial and Applied Mathematics.
Hassibi, B., A.H. Sayed, and T. Kailath, 1996. “The Hq optimality of the LMS algorithm,” IEEE
Transactions on Signal Processing, vol. 44, pp. 267–280.
Hassibi, B.,A.H. Sayed, and T. Kailath, 1993.“LMS is Hq optimal,” Proceedings of the IEEE Con-
ference on Decision and Control, pp. 74–79, San Antonio.
Hassibi, B., and D.G. Stork, 1993.“Second-order-derivatives for network pruning: Optimal brain
surgeon,” in S.H. Hanson, J.D. Cowan, and C.L. Giles, eds. Advances in Neural Information
Processing Systems, vol. 5, pp. 164–171, San Francisco: Morgan Kaufmann.
Hassibi, B., D.G. Stork, and G.J. Wolff, 1992. “Optimal brain surgeon and general network
pruning,” IEEE International Conference on Neural Networks, vol. 1, pp. 293–299, San
Francisco.
Hassibi, B., and T. Kailath, 1995.“Hq optimal training algorithms and their relation to back prop-
agation,” Advances in Neural Information Proccessing Systems, vol. 7, pp. 191–198.
Hastie,T., R.Tibshirani, and J. Friedman, 2001. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, New York: Springer.
Hastie,T., and W. Stuetzle, 1989.“Principal curves,” Journal of the American Statistical Association,
vol. 84, pp. 502–516.
Hastings,W.K.,1970.“Monte Carlo sampling methods using Markov chains and their applications,”
Biometrika, vol. 87, pp. 97–109.
Haykin, S., and K. Kan, 2007. “Coherent ICA: Implications for Auditory Signal Processing,”
WASPA’07, New Paltz, NY, October.
Haykin, S., 2006.“Statistical Learning Theory of the LMS Algorithm Under Slowly Varying Condi-
tions, Using the Langevin Equation,” 40th Asilomar Conference on Signals, Systems, and Com-
puters, Pacific Grove, CA.
Haykin, S., and Z. Chen, 2006.“The machine cocktail party problem,” in S. Haykin, J.C. Principe,
T.J. Sejnowski, and J. McWhirter, eds., New Directions in Statistical Signal Processing: From
Systems to Brain, pp. 51–75, Cambridge, MA: MIT Press.
Haykin,S.,and B.Widrow,2003.Least-Mean-Square Adaptive Filters, New York:Wiley-Interscience.
Haykin, S., 2002. Adaptive Filter Theory, 4th ed., Englewood Cliffs, NJ: Prentice Hall.
Haykin, S., and C. Deng, 1991.“Classification of radar clutter using neural networks,”IEEE Trans-
actions on Neural Networks, vol. 2, pp. 589–600.
Haykin, S., and B.Van Veen, 1998. Signals and Systems, New York:Wiley.
Hebb, D.O., 1949. The Organization of Behavior:A Neuropsychological Theory, New York:Wiley.
860
Bibliography

Hecht-Nielsen,R.,1995.“Replicator neural networks for universal optimal source coding,”Science,
vol. 269, pp. 1860–1863.
Hecht-Nielsen, R., 1990. Neurocomputing, Reading, MA:Addison-Wesley.
Hecht-Nielsen, R., 1987.“Kolmogorov’s mapping neural network existence theorem,” First IEEE
International Conference on Neural Networks, vol. III, pp. 11–14, San Diego.
Helstrom, C.W., 1968. Statistical Theory of Signal Detection, 2nd edition, Pergamon Press.
Herault,J.,and C.Jutten,1986.“Space or time adaptive signal processing by neural network models,”
in J.S.Denker,ed.,Neural Networks for Computing. Proceedings of the AIP Conference,Amer-
ican Institute of Physics, New York, pp. 206–211.
Herault, J., C. Jutten, and B.Ans, 1985.“Detection de grandeurs primitives dans un message com-
posite par une architecture de calcul neuromimetique un apprentissage non supervise.”
Procedures of GRETSI, Nice, France.
Herbrich,R.,2002.Learning Kernel Classifiers:Theory and Algorithms, Cambridge,MA:MIT Press.
Hertz, J., A. Krogh, and R.G. Palmer, 1991. Introduction to the Theory of Neural Computation,
Reading, MA:Addison-Wesley.
Heskes, T.M., 2001. “Self-organizing maps, vector quantization, and mixture modeling,” IEEE
Trans. on Neural Networks, vol. 12, pp. 1299–1305.
Heskes, T.M. and B. Kappen, 1991. “Learning processes and neural networks,” Phys. Rev., A44,
pp. 2718–2726.
Hestenes, M.R., and E. Stiefel, 1952.“Methods of conjugate gradients for solving linear systems,”
Journal of Research of the National Bureau of Standards, vol. 49, pp. 409–436.
Hiller, F.S., and G.J. Lieberman, 1995. Introduction to Operations Research, 6th ed., New York:
McGraw-Hill.
Hinton, G.E., 2007. Deep Belief Nets, 2007 NIPS Tutorial Notes, Neural Information Processing
Systems Conference, Vancouver, BC, December.
Hinton, G.E., S. Osindero, and Y.Teh, 2006.“A fast learning algorithm for deep belief nets,” Neural
Computation, vol. 18, pp. 1527–1554.
Hinton,G.E.,1989.“Connectionist learning procedures,”Artificial Intelligence, vol.40,pp.185–234.
Hinton, G.E., and T.J. Sejnowski, 1986. “Learning and relearning in Boltzmann machines,” in
Parallel Distributed Processing: Explorations in Microstructure of Cognition, D.E. Rumelhart
and J.L. McClelland, eds., Cambridge, MA: MIT Press.
Hinton, G.E., and T.J. Sejnowski, 1983. “Optimal perceptual inference,” Proceedings of IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, pp. 448–453,
Washington, DC.
Hirsch,M.W.,1989.“Convergent activation dynamics in continuous time networks,”Neural Networks,
vol. 2, pp. 331–349.
Hirsch, M.W., and S. Smale, 1974. Differential Equations, Dynamical Systems, and Linear Algebra,
New York:Academic Press.
Ho,Y.C., and R.C.K. Lee, 1964.“A Bayesian approach to problems in stochastic estimation and
control,” IEEE Trans.Automatic Control, vol.AC-9, pp. 333–339.
Hochreiter,S.,1991.Untersuchungen zu dynamischen neuronalen Netzen, diploma thesis,Technische
Universität Munchen, Germany.
Hodgkin, A.L., and A.F. Huxley, 1952. “A quantitative description of membrane current and its
application to conduction and excitation in nerve,” Journal of Physiology, vol. 117, pp. 500–544.
Holland, J.H., 1992. Adaptation in Natural and Artificial Systems, Cambridge, MA: MIT Press.
Bibliography
861

Holmström, L., P. Koistinen, J. Laaksonen, and E. Oja, 1997. “Comparison of neural and statis-
tical classifiers—taxonomy and two case studies,” IEEE Trans. on Neural Networks, vol. 8,
pp. 5–17.
Honkela, T., 2007. “Philosphical aspects of neural, probabilistic and fuzzy modeling of language
use and translation,”Proceedings of IJCNN,International Joint Conference on Neural Networks,
pp. 2881–2886, Orlando, FL,August.
Honkela, T., V. Pulkki, and T. Kohonen, 1995. “Contextual relations of words in Grimm tales,
analyzed by self-organizing maps,” Proceedings of International Conference on Artificial Neural
Networks, ICANN-95, vol. II, pp. 3–7, Paris.
Hopcroft, J., and U. Ullman, 1979. Introduction to Automata Theory, Languages and Computation,
Reading MA:Addison-Wesley.
Hopfield, J.J., 1995. “Pattern recognition computation using action potential timing for stimulus
representation,” Nature, vol. 376, pp. 33–36.
Hopfield,J.J.,1994.“Neurons,dynamics and computation,”Physics Today, vol.47,pp.40–46,February.
Hopfield, J.J., 1987.“Learning algorithms and probability distributions in feed-forward and feed-
back networks,” Proceedings of the National Academy of Sciences, USA, vol. 84, pp. 8429–8433.
Hopfield, J.J., 1984.“Neurons with graded response have collective computational properties like
those of two-state neurons,” Proceedings of the National Academy of Sciences, USA, vol. 81,
pp. 3088–3092.
Hopfield, J.J., 1982. “Neural networks and physical systems with emergent collective computa-
tional abilities,” Proceedings of the National Academy of Sciences, USA, vol. 79, pp. 2554–2558.
Hopfield, J.J., and T.W.Tank, 1985.“‘Neural’ computation of decisions in optimization problems,”
Biological Cybernetics, vol. 52, pp. 141–152.
Hornik,K.,M.Stinchcombe,and H.White,1990.“Universal approximation of an unknown mapping
and its derivatives using multilayer feedforward networks,” Neural Networks, vol. 3, pp. 551–560.
Hornik, K., M. Stinchcombe, and H.White, 1989.“Multilayer feedforward networks are universal
approximators,” Neural Networks, vol. 2, pp. 359–366.
Hotelling, H., 1936.“Relations between two sets of variates,” Biometrika, vol. 28, pp. 321–377.
Hotelling,H.,1935.“The most predictable criterion,”J.Educational Psychology, vol.26,pp.139–142.
Hotelling, H., 1933. “Analysis of a complex of statistical variables into principal components,”
Journal of Educational Psychology, vol. 24, pp. 417–441, 498–520.
Howard, R.A., 1960. Dynamic Programming and Markov Processes, Cambridge, MA: MIT Press.
Hubel, D.H., 1988. Eye, Brain, and Vision, New York: Scientific American Library.
Hubel,D.H.,and T.N.Wiesel,1977.“Functional architecture of macaque visual cortex,”Proceedings
of the Royal Society, B, vol. 198, pp. 1–59, London.
Hubel, D.H., and T.N.Wiesel, 1962.“Receptive fields, binocular interaction and functional archi-
tecture in the cat’s visual cortex,” Journal of Physiology, vol. 160, pp. 106–154, London.
Huber, P.J., 1985.“Projection pursuit,” Annals of Statistics, vol. 13, pp. 435–475.
Huber, P.J., 1981. Robust Statistics, New York:Wiley.
Huber, P.J., 1964.“Robust estimation of a location parameter,” Annals of Mathematical Statistics,
vol. 35, pp. 73–101.
Hush, D., P. Kelly, C. Scovel, and I. Steinwart, 2006.“QP algorithms with guaranteed accuracy and
run time for support vector machines,” J. Machine Learning Research, vol. 7, pp. 733–769.
Hush, D.R., and B.G. Home, 1993. “Progress in supervised neural networks: What’s new since
Lippmann?” IEEE Signal Processing Magazine, vol. 10, pp. 8–39.
862
Bibliography

Hush, D.R., and J.M. Salas, 1988.“Improving the learning rate of back-propagation with the gra-
dient reuse algorithm,” IEEE International Conference on Neural Networks, vol. I, pp. 441–447,
San Diego.
Hyvärinen,A., J. Karhunen, and E. Oja, 2001. Independent Component Analysis, New York:Wiley-
Interscience.
Hyvärinen,A. and E. Oja, 2000.“Independent component analysis:Algorithms and applications,”
Neural Networks, vol. 13, pp. 411–430.
Hyvärinen,A.and E.Oja,1997.“A fast fixed-point algorithm for independent component analysis,”
Neural Computation, vol. 9, pp. 1483–1492.
Ito,K.,and K.Xing,2000.“Gaussian filters for nonlinear filtering problems,”IEEE Trans.Automatic
Control, vol. 45, pp. 910–927.
Izhikevich, E., and G.M. Edelman, 2008. “Large-scale model of mammalian thalamocortical
systems,” Proceedings of the National Academy of Sciences, vol. 105, pp. 3593–3598.
Izhikevich, E.M., 2007a. Dynamical Systems in Neuroscience: The Geometry of Excitability and
Bursting, Cambridge, MA: MIT Press.
Izhikevich, E.M., 2007b. “Solving the distal reward problem through linkage of STDP and
dopamine signaling”, Cerebral Cortex, vol. 17, pp. 2443–2452.
Jackson, E.A., 1989. Perspectives of Nonlinear Dynamics, vol. 1, Cambridge, U.K.: Cambridge
University Press.
Jackson, E.A., 1990. Perspectives of Nonlinear Dynamics, vol. 2, Cambridge, U.K.: Cambridge
University Press.
Jackson, J.D., 1975. Classical Electrodynamics, 2d ed., New York:Wiley.
Jacobs, R.A., 1988. “Increased rates of convergence through learning rate adaptation,” Neural
Networks, vol. 1, pp. 295–307.
Jayant, N.S., and P. Noll, 1984. Digital Coding of Waveforms, Englewood Cliffs, NJ: Prentice-Hall.
Jaynes, E.T., 2003. Probability Theory: The Logic of Science, Cambridge, U.K., and New York:
Cambridge University Press.
Jaynes, E.T., 1982. “On the rationale of maximum-entropy methods,” Proceedings of the IEEE,
vol. 70, pp. 939–952.
Jaynes,E.T.,1957.“Information theory and statistical mechanics,”Physical Review, vol.106,pp.620–
630;“Information theory and statistical mechanic II,” Physical Review, vol. 108, pp. 171–190.
Jazwinski,A.H., 1970. Stochastic Processes and Filtering Theory, New York:Academic Press.
Jelinek, F., 1997. Statistical Methods for Speech Recognition, Cambridge, MA: MIT Press.
Joachims, T., 1999. “Making large-scale SVM learning practical,” in B. Schölkopf, C.J.C. Burges,
and A.J. Smola, eds., Advances in Kernel Methods—Support Vector Learning, pp. 169–184,
Cambridge, MA: MIT Press.
Johansson,E.M.,F.U.Dowla,and D.M.Goodman,1990.“Back-propagation learning for multi-layer
feedforward neural networks using the conjugate gradient method,”Report UCRL-JC-104850,
Lawrence Livermore National Laboratory, CA.
Johnson, D.S., C.R. Aragon, L.A. McGeoch, and C. Schevon, 1989. “Optimization by simulated
annealing:An experimental evaluation,” Operations Research, vol. 37, pp. 865–892.
Jolliffe, I.T., 1986. Principal Component Analysis, New York: Springer-Verlag.
Jordan,M.I.,1986.“Attractor dynamics and parallelism in a connectionist sequential machine,”The
Eighth Annual Conference of the Cognitive Science Society, pp. 531–546,Amherst, MA.
Jordan, M.I., ed., 1998. Learning in Graphical Models, Boston: Kluwer.
Bibliography
863

Joseph, R.D., 1960.“The number of orthants in n-space intersected by an s-dimensional subspace,”
Technical Memo 8, Project PARA, Cornell Aeronautical Lab., Buffalo.
Julier, S.J., and J.K. Ulhmann, 2004.“Unscented filtering and nonlinear estimation,” Proc. IEEE,
vol. 92, pp. 401–422.
Julier, S.J., J.K. Ulhmann, and H.F. Durrent-Whyte, 2000.“A new method for nonlinear transfor-
mation of means and covariances in filters and estimation,” IEEE Trans. Automatic Control,
vol. 45, pp. 472–482.
Jutten, C. and A.Taleb, 2000.“Source separation: from dusk till dawn,” Proceedings of 2nd Interna-
tional Workshop on Independent Component Analysis and Blind Source Separation, Helsinki,June.
Jutten, C., and J. Herault, 1991.“Blind separation of sources, Part I:An adaptive algorithm based
on neuromimetic architecture,” Signal Processing, vol. 24, pp. 1–10.
Kaas, J.H., M.M. Merzenich, and H.P. Killackey, 1983.“The reorganization of somatosensory cor-
tex following peripheral nerve damage in adult and developing mammals,” Annual Review of
Neurosciences, vol. 6, pp. 325–356.
Kailath,T., 1980. Linear Systems, Englewood Cliffs, NJ: Prentice-Hall.
Kailath,T., 1971.“RKHS approach to detection and estimation problems—Part I: Deterministic
signals in Gaussian noise,” IEEE Transactions of Information Theory, vol. IT-17, pp. 530–549.
Kailath,T., 1968.“An innovations approach to least-squares estimation: Part 1. Linear filtering in
additive white noise,” IEEE Transactions of Automatic Control, vol.AC-13, pp. 646–655.
Kakade, S., 2002.“A natural policy gradient,” Advances in Neural Information Processing Systems,
vol. 14-2, pp. 1531–1538, Cambridge, MA: MIT Press.
Kalman, R.E., 1960. “A new approach to linear filtering and prediction problems,” Transactions
of the ASME, Journal of Basic Engineering, vol. 82, pp. 35–45.
Kaminski, P.G.,A.E. Bryson, Jr., and S.F. Schmidt, 1971.“Discrete square root filtering:A survey
of current techniques,” IEEE Trans.Automatic Control, vol.AC-16, pp. 727–735.
Kammler, D.W., 2000, A First Course in Fourier Analysis, Prentice-Hall
Kan, K., 2007. Coherent Independent Component Analysis:Theory and Applications, M.Eng. thesis,
McMaster University, Hamilton, Ontario, Canada.
Kandel, E.R., J.H. Schwartz, and T.M. Jessell, eds., 1991. Principles of Neural Science, 3d ed.,
Norwalk, CT:Appleton & Lange.
Kaplan, J.L., and J.A.Yorke, 1979.“Chaotic behavior of multidimensional difference equations,”
in H.-O. Peitgen and H.-O.Walker, eds., Functional Differential Equations and Approximations
of Fixed Points, pp. 204–227, Berlin: Springer.
Kappen, H.J., and F.B. Rodriguez, 1998. “Efficient learning in Boltzmann machines using linear
response theory,” Neural Computation, vol. 10, pp. 1137–1156.
Karhunen, J., and J. Joutsensalo, 1995.“Generalizations of principal component analysis, optimiza-
tion problems, and neural networks,” Neural Networks, vol. 8, pp. 549–562.
Karhunen, J. and E. Oja, 1982. “New methods for stochastic approximation of truncated
Karhunen–Loève expansions,”IEEE Proceedings of the 6th International Conference on Pattern
Recognition, pp. 550–553, October.
Karhunen,K.,1947.“Über lineare methoden in der Wahrscheinlichkeitsrechnung,”Annales Academiae
Scientiarum Fennicae, Series AI: Mathematica-Physica, vol. 37, pp. 3–79, (Transl.: RAND Corp.,
Santa Monica, CA, Rep.T-131,Aug. 1960).
Karush,W., 1939.“Minima of functions of several variables with inequalities as side conditions,”
master’s thesis, Department of Mathematics, University of Chicago.
864
Bibliography

Katz, B., 1966. Nerve, Muscle and Synapse, New York: McGraw-Hill.
Kawamoto,A.H., and J.A.Anderson, 1985.“A neural network model of multistable perception,”
Acta Psychologica, vol. 59, pp. 35–65.
Kearns, M., 1996.“A bound on the error of cross validation using the approximation and estima-
tion rates, with consequences for the training-test split,” Advances in Neural Information Pro-
cessing Systems, vol. 8, pp. 183–189, Cambridge, MA: MIT Press.
Kearns,M.J.,and U.V.Vazirani,1994.An Introduction to Computational Learning Theory, Cambridge,
MA: MIT Press.
Kechriotis, G., E. Zervas, and E.S. Manolakos, 1994.“Using recurrent neural networks for adap-
tive communication channel equalization,” IEEE Transactions on Neural Networks, vol. 5,
pp. 267–278.
Kerlirzin,P.,and F.Vallet,1993.“Robustness in multilayer perceptrons,” Neural Computation, vol.5,
pp. 473–482.
Khalil, H.K., 1992. Nonlinear Systems, Englewood Cliffs, NJ: Prentice Hall.
Kim, K.I., M.O. Franz, and B. Schölkopf, 2005.“Iterative kernel principal component analysis for
image denoising,”IEEE Trans.Pattern Analysis and Machine Intelligence, vol.27,pp.1351–1366.
Kimeldorf, G.S., and G.Wahba, 1971.“Some results on Tchebycheffian spline functions,” J. Math.
Apal.Appli., vol. 33, pp. 82–95.
Kimeldorf, G.S., and G.Wahba, 1970,“A correspondence between Bayesian estimation on stochas-
tic processes and smoothing by splines,” Annals of Mathematical Statistics, vol. 41, pp. 495–502.
Kirkpatrick, S., 1984. “Optimization by simulated annealing: Quantitative studies,” Journal of
Statistical Physics, vol. 34, pp. 975–986.
Kirkpatrick,S.,and D.Sherrington,1978.“Infinite-ranged models of spin-glasses,” Physical Review,
Series B, vol. 17, pp. 4384–4403.
Kirkpatrick, S., C.D. Gelatt, Jr., and M.P. Vecchi, 1983. “Optimization by simulated annealing,”
Science, vol. 220, pp. 671–680.
Kirsch, A., 1996. An Introduction to the Mathematical Theory of Inverse Problems, New York:
Springer-Verlag.
Kleene, S.C., 1956.“Representation of events in nerve nets and finite automata,” in C.E. Shannon
and J. McCarthy, eds., Automata Studies, Princeton, NJ: Princeton University Press.
Kline, M., 1972. Mathematical Thought from Ancient to Modern Times, Oxford University Press.
Kmenta, J., 1971. Elements of Econometrics, New York: Macmillan.
Knill,D.C.,and W.Richards,eds.,1996.Perception as Bayesian Inference, Cambridge,U.K.,and New
York: Cambridge University Press.
Knudsen, E.I., S. duLac, and S.D. Esterly, 1987.“Computational maps in the brain,” Annual Review
of Neuroscience, vol. 10, pp. 41–65.
Koch, C., 1999. Biophysics of Computation: Information Processing in Single Neurons, New York:
Oxford University Press.
Kohonen,T., 1997a.“Exploration of very large databases by self-organizing maps,” 1997 Interna-
tional Conference on Neural Networks, vol. I, pp. PL1–PL6, Houston.
Kohonen,T., 1997b. Self-Organizing Maps, 2d ed., Berlin: Springer-Verlag.
Kohonen,T., 1993.“Things you haven’t heard about the self-organizing map,” Proceedings of the
IEEE International Conference on neural networks, pp. 1147–1156, San Francisco.
Kohonen, T., 1990. “The self-organizing map,” Proceedings of the Institute of Electrical and
Electronics Engineers, vol. 78, pp. 1464–1480.
Bibliography
865

Kohonen, T., 1982. “Self-organized formation of topologically correct feature maps,” Biological
Cybernetics, vol. 43, pp. 59–69.
Kohonen,T., 1972.“Correlation matrix memories,” IEEE Transactions on Computers, vol. C-21,
pp. 353–359.
Kolen,J.,and S.Kremer,eds.,2001.A Field Guide to Dynamical Recurrent Networks, New York:IEEE
Press.
Kollias, S., and D. Anastassiou, 1989. “An adaptive least squares algorithm for the efficient
training of artificial neural networks,” IEEE Transactions on Circuits and Systems, vol. 36,
pp. 1092–1101.
Kolmogorov,A., 1965.“Three approaches to the quantitative definition of information,” Problems
of Information Transmission, vol. 1, issue 1, pp. 1–7.
Kolmogorov,A.N., 1942.“Interpolation and extrapolation of stationary random sequences,” trans-
lated by the Rand Corporation, Santa Monica, CA.,April 1962.
Kramer,M.A.,1991.“Nonlinear principal component analysis using autoassociative neural networks,”
AIChE Journal, vol. 37, pp. 233–243.
Kramer, A.H., and A. Sangiovanni-Vincentelli, 1989. “Efficient parallel learning algorithms
for neural networks,” Advances in neural Information Processing Systems, vol. 1, pp. 40–48,
San Mateo, CA: Morgan Kaufmann.
Kremer,S.C.,1996.“Comments on constructive learning of recurrent neural networks:Limitations
of recurrent cascade correlation and a simple solution,”IEEE Transactions on Neural Networks,
vol. 7, pp. 1047–1049.
Kremer, S.C., 1995. “On the computational power of Elman-style recurrent networks,” IEEE
Transactions on Neural Networks, vol. 6, pp. 1000–1004.
Kreyszig, E., 1988. Advanced Engineering Mathematics, 6th ed., New York:Wiley.
Krzy ak, A., T. Linder, and G. Lugosi, 1996. “Nonparametric estimation and classification using
radial basis functions,” IEEE Transactions on Neural Networks, vol. 7, pp. 475–487.
Kuffler, S.W., J.G. Nicholls, and A.R. Martin, 1984. From Neuron to Brain:A Cellular Approach to
the Function of the Nervous System, 2d ed., Sunderland, MA: Sinauer Associates.
Kullback, S., 1968. Information Theory and Statistics, Gloucester, MA: Peter Smith.
Kuhn, H.W., 1976. “Nonlinear programming: A historical view,” in R.N. Cottle and C.E. Lemke,
eds., SIAM-AMS Proceedings, vol. IX,American Mathematical Society, pp. 1–26.
Kuhn, H.W., and A.W. Tucker, 1951. “Nonlinear programming,” in J. Neyman, ed., Proceedings
of the 2nd Berkley Symposium on Mathematical Statistics and Probabilities, pp. 481–492,
Monterey CA: University of California Press.
Kung, S.Y., and K.I. Diamantaras, 1990.“A neural network learning algorithm for adaptive prin-
cipal component extraction (APEX).”IEEE International Conference on Acoustics,Speech,and
Signal Processing, vol. 2, pp. 861–864,Albuquerque, NM.
Kushner, H.J., and D.S. Clark, 1978. Stochastic Approximation Methods for Costrained and Uncon-
strained Systems, New York: Springer-Verlag.
Laaksonen, J., and V.Viitanieni, 2006.“Emergence of ontological relations from visual data with
self-organizing maps,” Proceedings of SCAI’06, the 9th Scandanavian Conference on Artificial
Intelligence, pp. 31–38, Espoo, Finland, October.
Laaksonen, J.T., J.M. Koskela, and E. Oja, 2004.“Class distributions on SOM surfaces for feature
extraction and object retrieval,” Neural Networks, vol. 17, pp. 1121–1133.
Lagoudakis,M.G.,and R.Parr,2003.“Least-squares policy iteration,”J.Machine Learning Research,
vol. 4, pp. 1107–1149.
z#
866
Bibliography

Lanczos, C., 1964. Linear Differential Operators, London:Van Nostrand.
Landau,Y.D., 1979. Adaptive Control:The Model Reference Approach, New York: Marcel Dekker.
Landau, L.D., and E.M. Lifshitz, 1980. Statistical Physics: Part 1, 3d ed., London: Pergamon Press.
Lanford, O.E., 1981. “Strange attractors and turbulence,” in H.L. Swinney and J.P. Gollub, eds.,
Hydrodynamic Instabilities and the Transition to Turbulence, New York: Springer-Verlag.
Lang, S. 2002. Introduction to Differentiable Manifolds, New York: Springer.
LeCun,Y., 1993. Efficient Learning and Second-order Methods,A Tutorial at NIPS 93, Denver.
LeCun, Y., 1989. “Generalization and network design strategies,” Technical Report CRG-TR-89-4,
Department of Computer Science, University of Toronto, Ontario, Canada.
LeCun, Y., 1985. “Une procedure d’apprentissage pour reseau a seuil assymetrique.” Cognitiva,
vol. 85, pp. 599–604.
LeCun,Y., and Y., Bengio, 2003.“Convolutional Networks for Images, Speech, and Time Series,”
in M.A.Arbib, ed., The Handbook of Brain Theory and Neural Networks, 2d ed., Cambridge,
MA: MIT Press.
LeCun,Y., B. Boser, J.S. Denker, D. Henderson, R.E. Howard,W. Hubbard, and L.D. Jackel, 1990.
“Handwritten digit recognition with a back-propagation network,” Advances in Neural Infor-
mation Processing, vol. 2, pp. 396–404, San Mateo, CA: Morgan Kaufmann.
LeCun,Y.,L.Bottou,and Y.Bengio,1997.“Reading checks with multilayer graph transformer net-
works,”IEEE International Conference on Acoustics,Speech and Signal Processing, pp.151–154,
Munich, Germany.
LeCun,Y., L. Bottou,Y. Bengio, and R. Haffner, 1998.“Gradient-based learning applied to docu-
ment recognition,” Procedings of the IEEE, vol. 86, pp. 2278–2324.
LeCun,Y., J.S. Denker, and S.A. Solla, 1990.“Optimal brain damage,” Advances in Neural Infor-
mation Processing Systems, vol. 2, pp. 598–605, San Mateo, CA: Morgan Kaufmann.
LeCun,Y,I.Kanter,and S.A.Solla,1991.“Second order properties of error surfaces:Learning time
and generalization,” Advances in Neural Information Processing Systems, vol. 3, pp. 918–924,
Cambridge, MA: MIT Press.
Lee,T.S., and D. Mumford, 2003.“Hierarchical Bayesian inference in the visual cortex,” J. Optical
Society of America, vol. 20, pp. 1434–1448.
Lefebvre,W.C., 1991. An Object Oriented Approach for the Analysis of Neural Networks, master’s
thesis, University of Florida, Gainesville, FL.
Leon-Garcia, A., 1994. Probability and Random Processes for Electrical Engineering, 2d ed.,
Reading, MA:Addison-Wesley.
Leontaritis, I., and S. Billings, 1985. “Input–output parametric models for nonlinear systems:
Part I: Deterministic nonlinear systems,” International Journal of Control, vol. 41, pp. 303–328.
Levenberg, K., 1944.“A method for the solution of certain non-linear problems in least squares,”
Quart.Appl. Math., vol. 12, pp. 164–168.
Levin, A.V., and K.S. Narendra, 1996. “Control of nonlinear dynamical systems using neural
networks—Part II: Observability, identification, and control,” IEEE Transactions on Neural
Networks, vol. 7, pp. 30–42.
Levin, A.V., and K.S. Narendra, 1993. “Control of nonlinear dynamical systems using neural
networks—Controllability and stabilization,” IEEE Transactions on Neural Networks, vol. 4,
pp. 192–206.
Lewis, F.L., and V.L. Syrmas, 1995. Optimal Control, 2d ed., New York:Wiley (Interscience).
Li, M., and P.Vitányi, 1993. An Introduction to Kolmogorov Complexity and Its Applications, New
York: Springer-Verlag.
Bibliography
867

Lichtenberg,A.J., and M.A. Lieberman, 1992. Regular and Chaotic Dynamics, 2d ed., New York:
Springer-Verlag.
Light,W.A.,1992a.“Some aspects of radial basis function approximation,”in Approximation Theory,
Spline Functions and Applications, S.P.Singh,ed.,NATO ASI vol.256,pp.163–190,Boston:Kluwer
Academic Publishers.
Light,W.,1992b.“Ridge functions,sigmoidal functions and neural networks,”in E.W.Cheney,C.K.
Chui, and L.L. Schumaker, eds., Approximation Theory VII, pp. 163–206, Boston: Academic
Press.
Lin,S.,and D.J.Costello,2004.Error Control Coding, 2d ed.,Upper Saddle River,NJ:Prentice Hall.
Lin, J.K., D.G. Grier, and J.D. Cowan, 1997. “Faithful representation of separable distributions,”
Neural Computation, vol. 9, pp. 1305–1320.
Lin, T., B.G. Horne, P. Tino, and C.L. Giles, 1996. “Learning long-term dependencies in NARX
recurrent neural networks,” IEEE Transactions on Neural Networks, vol. 7, pp. 1329–1338.
Linsker, R., 1993. “Deriving receptive fields using an optimal encoding criterion,” Advances
in Neural Information Processing Systems, vol. 5, pp. 953–960, San Mateo, CA: Morgan
Kaufmann.
Linsker, R., 1990a. “Designing a sensory processing system: What can be learned from principal
components analysis?”Proceedings of the International Joint Conference on Neural Networks, vol.
2, pp. 291–297,Washington, DC.
Linsker, R., 1990b.“Perceptual neural organization: Some approaches based on network models
and information theory,” Annual Review of Neuroscience, vol. 13, pp. 257–281.
Linsker, R., 1989a.“An application of the principle of maximum information preservation to linear
systems,”Advances in Neural Information Processing Systems, vol.1,pp.186–194,San Mateo,CA:
Morgan Kaufmann.
Linsker,R.,1989b.“How to generate ordered maps by maximizing the mutual information between
input and output signals,” Neural computation, vol. 1, pp. 402–411.
Linsker, R., 1988a.“Self-organization in a perceptual network,” Computer, vol. 21, pp. 105–117.
Linsker, R., 1988b.“Towards an organizing principle for a layered perceptual network,” in Neural
Information Processing Systems, D.Z,Anderson,ed.,pp.485–494,New York:American Institute
of Physics.
Linsker, R., 1987. “Towards an organizing principle for perception: Hebbian synapses and the
principle of optimal neural encoding,”IBM Research Report RC12820, IBM Research,Yorktown
Heights, NY.
Linsker, R., 1986.“From basic network principles to neural architecture” (series), Proceedings of
the National Academy of Sciences, USA, vol. 83, pp. 7508–7512, 8390–8394, 8779–8783.
Lippmann, R.P., 1987. “An introduction to computing with neural nets,” IEEE ASSP Magazine,
vol. 4, pp. 4–22.
Lippmann, R.P., 1989a.“Review of neural networks for speech recognition,” Neural Computation,
vol. 1, pp. 1–38.
Lippmann, R.P., 1989b. “Pattern classification using neural networks,” IEEE Communications
Magazine, vol. 27, pp. 47–64.
Little,W.A.,1974.“The existence of persistent states in the brain,”Mathematical Biosciences, vol.19,
pp. 101–120.
Little,W.A.,and G.L.Shaw,1978.“Analytic study of the memory storage capacity of a neural network,”
Mathematical Biosciences, vol. 39, pp. 281–290.
868
Bibliography

Little,W.A., and G.L. Shaw, 1975.“A statistical theory of short and long term memory,”Behavioral
Biology, vol. 14, pp. 115–133.
Littman, M.L., R.S. Sutton, and S. Singh, 2002.“Predictive representations of state,” Advances in
Neural Information Processing Systems, vol. 14, pp. 1555–1561.
Liu, J.S., and R. Chen, 1998.“Sequential Monte Carlo methods for dynamical systems,” J.American
Statistical Association, vol. 93, pp. 1032–1044.
Liu, J.S., 1996.“Metropolized independent sampling with comparisons to rejection sampling and
importance sampling,” Statistics and Computing, vol. 6, pp. 113–119.
Liu, W., P.P. Pokharel, and J.C. Principe, 2008. “The kernel least-mean-square algorithm,” IEEE
Trans. Signal Processing, vol. 56, pp. 543–554.
Livesey, M., 1991. “Clamping in Boltzmann machines,” IEEE Transactions on Neural Networks,
vol. 2, pp. 143–148.
Ljung, L., 1987. System Identification:Theory for the User. Englewood Cliffs, NJ: Prentice-Hall.
Ljung, L., 1977. “Analysis of recursive stochastic algorithms,” IEEE Transactions on Automatic
Control, vol. AC-22, pp. 551–575.
Ljung, L., and T. Glad, 1994. Modeling of Dynamic Systems, Englewood Cliffs, NJ: Prentice 
Hall.
Lloyd, S.P., 1957.“Least squares quantization in PCM,” unpublished Bell Laboratories technical
note.Published later under the same title in IEEE Transactions on Information Theory, vol.IT-28,
pp. 127–135, 1982.
Lo, J.T., 2001.“Adaptive vs. accommodative neural networks for adaptive system identification,”
Proceedings of the International Joint Conference on Neural Networks, pp. 2001–2006,
Washington, DC.
Lo, J.T., and L. Yu, 1995a. “Recursive neural filters and dynamical range transformers,” Proc.
IEEE, vol. 92, pp. 514–535.
Lo, J.T., and L.Yu, 1995b.Adaptive neural filtering by using the innovations process, Proceedings
of the 1995 World Congress on Neural Networks, vol. II, pp. 29–35, July.
Lo, J.T., 1993.“Dynamical system identification by recurrent multilayer perceptrons,” Proceedings
of the 1993 World Congress on Neural Networks, Portland, OR.
Lo, Z.-P., M. Fujita, and B. Bavarian, 1991. “Analysis of neighborhood interaction in Kohonen
neural networks,” 6th International Parallel Processing Symposium Proceedings, pp. 247–249,
Los Alamitos, CA.
Lo, Z.-P., Y. Yu and B. Bavarian, 1993. “Analysis of the convergence properties of topology pre-
serving neural networks,” IEEE Transactions on Neural Networks, vol. 4, pp. 207–220.
Lockery, S.R., Y. Fang, and T.J. Sejnowski, 1990. “A dynamical neural network model of sensori-
motor transformations in the leech,” International Joint Conference on Neural Networks,
vol. I, pp. 183–188, San Diego, CA.
Loève, M., 1963. Probability Theory, 3d ed., New York:Van Nostrand.
Lorentz, G.G., 1976.“The 13th problem of Hilbert,” Proceedings of Symposia in Pure Mathematics,
vol. 28, pp. 419–430.
Lorentz, G.G., 1966. Approximation of Functions, Orlando, FL: Holt, Rinehart & Winston.
Lorenz, E.N., 1963.“Deterministic non-periodic flows,” Journal of Atmospheric Sciences, vol. 20,
pp. 130–141.
Lowe, D., 1989.“Adaptive radial basis function nonlinearities, and the problem of generalisation,”
First IEE International Conference on Artificial Neural Networks, pp. 171–175, London.
Bibliography
869

Lowe, D., 1991a.“What have neural networks to offer statistical pattern processing?” Proceedings
of the SPIE Conference on Adaptive Signal Processing, pp. 460–471, San Diego.
Lowe,D.,1991b.“On the iterative inversion of RBF networks:A statistical interpretation,”Second
IEE International Conference on Artificial Neural Networks, pp. 29–33, Bournemouth, U.K.
Lowe, D., and A.R.Webb, 1991a.“Time series prediction by adaptive networks:A dynamical sys-
tems perspective,” IEE Proceedings (London), Part F, vol. 138, pp. 17–24.
Lowe, D., and A.R. Webb, 1991b. “Optimized feature extraction and the Bayes decision in feed-
forward classifier networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
PAMI-13, 355–364.
Luenberger, D.G., 1984. Linear and Nonlinear Programming, 2d ed., Reading, MA: Addison-
Wesley.
Luttrell, S.P., 1994. “A Bayesian analysis of self-organizing maps,” Neural Computation, vol. 6,
pp. 767–794.
Luttrell, S.P., 1991.“Code vector density in topographic mappings: Scalar case,” IEEE Transactions
on Neural Networks, vol. 2, pp. 427–436.
Luttrell, S.P., 1989a.“Hierarchical vector quantization,” IEE Proceedings (London), vol. 136 (Part I),
pp. 405–413.
Luttrell, S.P., 1989b.“Self-organization:A derivation from first principle of a class of learning algo-
rithms,” IEEE Conference on Neural Networks, pp. 495–498,Washington, DC.
Lyapunov,A.M.,1892.The General Problem of Motion Stability (in Russian).(Translated in English
by F. Abramovici and M. Shimshoni, under the title Stability of Motion, New York: Academic
Press, 1966.)
Maass,W., 1993.“Bounds for the computational power and learning complexity of analog neural
nets,”Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, pp.335–344,
New York:ACM Press.
MacKay, D.J.C., 2003. Information Theory, Inference, and Learning Algorithms, Cambridge, U.K., and
New York:Cambridge University Press.
Mackey,M.C.,and L.Glass,1977.“Oscillations and chaos in physiological control systems,”Science,
vol. 197, pp. 287–289.
MacQueen, J., 1967. “Some methods for classification and analysis of multivariate observa-
tion,” in Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Prob-
ability, L.M. LeCun and J. Neyman, eds., vol. 1, pp. 281–297, Berkeley: University of
California Press.
Mahowald,M.A.,and C.Mead,1989.“Silicon retina,”in Analog VLSI and Neural Systems (C.Mead),
Chapter 15. Reading, MA:Addison-Wesley.
Mallat, S., 1998. A Wavelet tour of signal processing, San Diego:Academic Press.
Mandelbrot, B.B., 1982. The Fractal Geometry of Nature, San Francisco: Freeman.
Ma é, R., 1981. “On the dimension of the compact invariant sets of certain non-linear maps,” in
D. Rand and L.S.Young, eds., Dynamical Systems and Turbulence, Lecture Notes in Mathemat-
ics, vol. 898, pp. 230–242, Berlin: Springer-Verlag.
Marquardt, D.W. 1963. “An algorithm for least-squares estimation of nonlinear parameters,”
J. Soc. Indust. Appli. Math., vol. 11, no. 2, pp. 431–441, June.
Marr, D., 1982. Vision, New York:W.H. Freeman and Company.
Mason, S.J., 1953.“Feedback theory—Some properties of signal-flow graphs,” Proceedings of the
Institute of Radio Engineers, vol. 41, pp. 1144–1156.
n
870
Bibliography

Mason, S.J., 1956.“Feedback theory—Further properties of signal-flow graphs,”Proceedings of the
Institute of Radio Engineers, vol. 44, pp. 920–926.
Maybeck, P.S., 1982. Stochastic Models, Estimation, and Control, vol. 2, New York:Academic Press.
Maybeck, P.S., 1979. Stochastic Models, Estimation, and Control, vol. 1, New York:Academic Press.
Mazaika, P.K., 1987.“A mathematical model of the Boltzmann machine,” IEEE First International
Conference on Neural Networks, vol. III, pp. 157–163, San Diego.
McBride, L.E., Jr., and K.S. Narendra, 1965.“Optimization of time-varying systems,” IEEE Trans-
actions on Automatic Control, vol.AC-10, pp. 289–294.
McCulloch,W.S., 1988. Embodiments of Mind, Cambridge, MA: MIT Press.
McCulloch,W.S., and W. Pitts, 1943.“A logical calculus of the ideas immanent in nervous activity,”
Bulletin of Mathematical Biophysics, vol. 5, pp. 115–133.
McLachlan, G.J., and T. Krishnan, 1997. The EM Algorithm and Extensions, New York: Wiley
(Interscience).
McQueen, J., 1967. “Some methods for classification and analysis of multivariate observations,”
Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, vol. 1,
pp. 281–297, Berkeley, CA: University of California Press.
Mead, C.A., 1989. Analog VLSI and Neural Systems, Reading, MA:Addison-Wesley.
Mendel, J.M., 1995. Lessons in Estimation Theory for Signal Processing, Communications, and
Control. Englewood Cliffs, NJ: Prentice Hall.
Mennon,A., K. Mehrotra, C.K. Mohan, and S. Ranka, 1996.“Characterization of a class of sigmoid
functions with applications to neural networks,” Neural Networks, vol. 9, pp. 819–835.
Mercer, J., 1909.“Functions of positive and negative type, and their connection with the theory of
integral equations,”Transactions of the London Philosophical Society (A), vol.209,pp.415–446.
Metropolis, N.,A. Rosenbluth, M. Rosenbluth,A.Teller, and E.Teller, 1953. Equations of state cal-
culations by fast computing machines, Journal of Chemical Physics, vol. 21, pp. 1087–1092.
Meyer,Y.,1993.Wavelets:Algorithms and Applications, SIAM (translated from French and revised
by R.D. Ryan), Philadelphia: Society for Industrial and Applied Mathematics.
Micchelli, C.A., 1986.“Interpolation of scattered data: Distance matrices and conditionally posi-
tive definite functions,” Constructive Approximation, vol. 2, pp. 11–22.
Miller, D.,A.V. Rao, K. Rose, and A. Gersho, 1996.“A global optimization technique for statisti-
cal classifier design,” IEEE Transactions on Signal Processing, vol. 44, pp. 3108–3122.
Miller, D., and K. Rose, 1994.“Combined source-channel vector quantization using deterministic
annealing,” IEEE Transactions on Communications, vol. 42, pp. 347–356.
Miller,R.,1987.“Representation of brief temporal patterns,Hebbian synapses,and the left-hemisphere
dominance for phoneme recognition,” Psychobiology, vol. 15, pp. 241–247.
Minsky, M.L., 1986. Society of Mind, New York: Simon and Schuster.
Minsky, M.L., 1967. Computation: Finite and Infinite Machines. Englewood Cliffs, NJ: Prentice-
Hall.
Minsky, M.L., 1954. “Theory of neural-analog reinforcement systems and its application to the
brain-model problem,” Ph.D. thesis, Princeton University, Princeton, NJ.
Minsky, M.L., and S.A. Papert, 1988. Perceptrons, expanded edition, Cambridge, MA: MIT Press.
Minsky, M.L., and S.A. Papert, 1969. Perceptrons, Cambridge, MA: MIT Press.
Minsky, M.L., and O.G. Selfridge, 1961. “Learning in random nets,” Information Theory, Fourth
London Symposium, London: Buttenvorths.
Bibliography
871

Møller, M.F., 1993. “A scaled conjugate gradient algorithm for fast supervised learning,” Neural
Networks, vol. 6, pp. 525–534.
Moody, J., and C.J. Darken, 1989. “Fast learning in networks of locally-tuned processing units,”
Neural Computation, vol. 1, pp. 281–294.
Morita, M., 1993. “Associative memory with nonmonotonic dynamics,” Neural Networks, vol. 6,
pp. 115–126.
Morozov,V.A., 1993. Regularization Methods for Ill-Posed Problems, Boca Raton, FL: CRC Press.
Müller, K.,A. Ziehe, N. Murata, and S.Amari, 1998.“On-line learning in switching and drifting
environments with application to blind source separation,” in D. Saad, ed., On-line Learning
in Neural Networks, pp. 93–110, Cambridge, U.K., and New York: Cambridge University
Press.
Mumford, D., 1994.“Neural architectures for pattern-theoretic problems,” in C. Koch and J. Davis,
eds., Large-Scale Theories of the Cortex, pp. 125–152, Cambridge, MA: MIT Press.
Murata, N., 1998. “A statistical study of on-line learning,” in D. Saad, ed., On-line Learning in
Neural Networks, pp. 63–92, Cambridge, U.K., and New York: Cambridge University Press.
Murray,M.K.,and J.W.Rice,1993.Differential Geometry and Statistics, New York:Chapman and Hall.
Nadal, J.-P., and N. Parga, 1997, “Redundancy reduction and independent component analysis:
Conditions on cumulants and adaptive approaches,” Neural Computation, vol. 9, pp. 1421–1456.
Nadal, J.-P., and N. Parga, 1994,“Nonlinear neurons in the low-noise limit:A factorial code max-
imizes information transfer, Network, vol. 5, pp. 565–581.
Nadaraya, E.A. 1965.“On nonparametric estimation of density functions and regression curves,”
Theory of Probability and its Applications, vol. 10, pp. 186–190.
Nadaraya, É.A., 1964. “On estimating regression,” Theory of Probability and its Applications,
vol. 9, issue 1, pp. 141–142.
Narendra, K.S., and A.M. Annaswamy, 1989. Stable Adaptive Systems, Englewood Cliffs, NJ:
Prentice Hall.
Narendra, K.S., and K. Parthasarathy, 1990.“Identification and control of dynamical systems using
neural networks,” IEEE Transactions on Neural Networks, vol. 1, pp. 4–27.
Neal, R.M., 1995. Bayesian Learning for Neural Networks, Ph.D. Thesis, University of Toronto,
Canada.
Neal, R.M., 1993. “Bayesian learning via stochastic dynamics,” Advances in Neural Information
Processing Systems, vol. 5, pp. 475–482, San Mateo, CA: Morgan Kaufmann.
Neal, R.M., 1992. “Connectionist learning of belief networks,” Artificial Intelligence, vol. 56,
pp. 71–113.
Nelsen, R.B., 2006. An Introduction to Copulas, 2d ed., New York: Springer.
Newcomb, S., 1886.“A generalized theory of the combination of observations so as to obtain the
best result,” American Journal of Mathematics, vol. 8, pp. 343–366.
Newell,A., and H.A. Simon, 1972. Human Problem Solving, Englewood Cliffs, NJ: Prentice-Hall.
Nguyen, D., and B.Widrow, 1989.“The truck backer-upper:An example of self-learning in neural
networks,”International Joint Conference on Neural Networks, vol.II,pp.357–363,Washington,
DC.
Nie,J.,and S.Haykin,1999.“A Q-learning-based dynamic channel assignment technique for mobile
communication systems,” IEEE Transactions on Vehicular Technology, vol. 48, p. 1676–1687.
Nilsson, N.J., 1980. Principles of Artificial Intelligence, New York: Springer-Verlag.
Nilsson, N.J., 1965. Learning Machines: Foundations of Trainable Pattern-Classifying Systems, New
York: McGraw-Hill.
872
Bibliography

Niyogi, P., and F. Girosi, 1996. “On the relationship between generalization error, hypothesis
complexity, and sample complexity for radial basis functions,” Neural Computation, vol. 8,
pp. 819–842.
Novikoff,A.B.J.,1962.“On convergence proofs for perceptrons,”in Proceedings of the Symposium on
the Mathematical Theory of Automata, pp.615–622,Brooklyn,NY:Polytechnic Institute of Brooklyn.
Nørgaard, M., N.K. Poulsen, and O. Ravn, 2000.“New developments in state estimation for non-
linear systems,” Automatica, vol. 36, pp. 1627–1638.
Obermayer, K., H. Ritter, and K. Schulten, 1991. “Development and spatial structure of cortical
feature maps: A model study,” Advances in Neural Information Processing Systems, vol. 3,
pp. 11–17, San Mateo, CA: Morgan Kaufmann.
Oja, E., 1992.“Principal components, minor components, and linear neural networks,”Neural Net-
works, vol. 5, 927–936.
Oja,E.,1983.Subspace Methods of Pattern Recognition, Letchworth,England:Research Studies Press.
Oja, E., 1982.“A simplified neuron model as a principal component analyzer,” Journal of Mathe-
matical Biology, vol. 15, pp. 267–273.
Oja, E., and J. Karhunen, 1985.“A stochastic approximation of the eigenvectors and eigenvalues
of the expectation of a random matrix,”Journal of Mathematical Analysis and Applications, vol.
106, pp. 69–84.
Olshausen, B.A., and D.J. Field, 1997. Sparse coding with an overcomplete basis set: A strategy
employed by VI? Vision Research, vol. 37, pp. 3311–3325.
Olshausen, B.A., and D.J. Field, 1996. “Emergence of simple-cell receptive field properties by
learning a sparse code for natural images,” Nature, vol. 381, pp. 607–609.
Omlin, C.W., and C.L. Giles, 1996.“Constructing deterministic finite-state automata in recurrent
neural networks,” Journal of the Association for Computing Machinery, vol. 43, pp. 937–972.
Omura, J.K., 1969. “On the Viterbi decoding algorithm,” IEEE Trans. Information Theory, vol.
IT-15, pp. 177–179.
Opper, M., 1996. “Online versus offline learning from random examples: General results,” Phys.
Rev. Lett., vol. 77, pp. 4671–4674.
Orr, G.B., and K. Müller, 1998. Neural Networks: Tricks of the Trade (Outgrowth of a 1996 NIPS
Workshop), Berlin and New York: Springer.
Osuna, E., R. Freund, and F. Girosi, 1997. “An improved training algorithm for support vector
machines,” Neural Networks for Signal Processing VII, Proceedings of the 1997 IEEE Work-
shop, pp. 276–285,Amelia Island, FL.
Ott, E., 1993. Chaos in Dynamical Systems, Cambridge, MA: Cambridge University Press.
Packard, N.H., J.P. Crutchfield, J.D. Farmer, and R.S. Shaw, 1980.“Geometry from a time series,”
Physical Review Letters, vol. 45, pp. 712–716.
Palm, G., 1982. Neural Assemblies:An Alternative Approach, New York: Springer-Verlag.
Papoulis, A., 1984. Probability, Random Variables, and Stochastic Processes, 2d ed., New York:
McGraw-Hill.
Parisi, G., 1988. Statistical Field Theory, Reading, MA:Addison-Wesley.
Park, J., and I.W. Sandberg, 1991.“Universal approximation using radial-basis-function networks,”
Neural Computation, vol. 3, pp. 246–257.
Parker, D.B., 1987. “Optimal algorithms for adaptive networks.” Second order back propaga-
tion, second order direct propagation and second order Hebbian learning.” IEEE 1st Inter-
national Conference on Neural Networks, vol. 2, pp. 593–600, San Diego, CA.
Parker,T.S., and L.O., Chua, 1989. Practical Numerical Algorithms for Chaotic Systems, New York:
Springer.
Bibliography
873

Parzen, E., 1962. “On estimation of a probability density function and mode,” Annals of Mathe-
matical Statistics, vol. 33, pp. 1065–1076.
Paulin, M.G., 1997. “Neural representations of moving systems,” International Journal of Neuro-
biology, vol. 41, pp. 515–533.
Pavlov,I.P.,1927.Conditional Reflexes:An Investigation of the Physiological Activity of the Cerebral
Cortex, (Translation from the Russian by G.V.Anrep), New York: Oxford University Press.
Pearl, J., 1988. Probabilistic Reasoning in Intelligent Systems, San Mateo, CA: Morgan Kaufmann.
(revised 2nd printing, 1991).
Pearlmutter, B.A., 1994. “Fast exact multiplication by the Hessian,” Neural Computation, vol. 6,
issue 1, pp. 147–160.
Pearson, K., 1901.“On lines and planes of closest fit to systems of points in space,” Philosophical
Magazine, vol. 2, pp. 559–572.
Peretto, P. 1984. “Collective properties of neural networks: A statistical physics approach,”
Biological Cybernetics, vol. 50, pp. 51–62.
Peretto, P., and J.-J Niez, 1986. “Stochastic dynamics of neural networks,” IEEE Transactions on
Systems, Man, and Cybernetics, vol. SMC-16, pp. 73–83.
Perrin, D., 1990.“Finite automata,” in J. van Leeuwen, ed.,Handbook of Theoretical Computer Sci-
ence,Volume B:Formal Models and Semantics, Chapter 1,pp.3–57,Cambridge,MA:MIT Press.
Pham, D.T., and P. Garrat, 1997. “Blind separation of mixture of independent sources through a
quasi-maximum likelihood approach,” IEEE Transactions on Signal Processing, vol. 45,
pp. 1712–1725.
Pham,D.T.,P.Garrat,and C.Jutten,1992.“Separation of a mixture of independent sources through
a maximum likelihood approach,” Proceedings of EUSIPCO, pp. 771–774.
Phillips, D., 1962.“A technique for the numerical solution of certain integral equations of the first
kind,” Journal of Association for Computing Machinery, vol. 9, pp. 84–97.
Pineau, J., G. Gordon, and S. Thrun, 2006. “Anytime point-based approximations for large
POMDPs.” Journal of Artificial Intelligence Research, Vol. 27, pp. 335–380.
Pineda, F.J., 1988a.“Generalization of backpropagation to recurrent and higher order neural net-
works,” in Neural Information Processing Systems, D.Z.Anderson, ed., pp. 602–611, New York:
American Institute of Physics.
Pineda, F.J., 1988b. “Dynamics and architecture in neural computation,” Journal of Complexity,
vol. 4, pp. 216–245.
Pineda, F.J., 1987. “Generalization of back-propagation to recurrent neural networks,” Physical
Review Letters, vol. 59, pp. 2229–2232.
Pitts, W., and W.S. McCulloch, 1947. “How we know universals: The perception of auditory and
visual forms,” Bulletin of Mathematical Biophysics, vol. 9, pp. 127–147.
Pizurica, A., and W. Phillips, 2006. “Estimating the probability of the presence of a signal of
interest in multiresolution single- and multiband image denoising,” IEEE Trans. on Image
Processing, vol. 15, pp. 654–665.
Platt, J., 1999.“Fast training of support vector machines using sequential minimal optimization,”
in B. Schölkopf, C.J.C. Burges, and A.J. Smola, eds., Advances in Kernel Methods—Support Vec-
tor Learning, pp. 185–208, Cambridge, MA: MIT Press.
Poggio, T., and F. Girosi, 1990a. “Networks for approximation and learning,” Proceedings of the
IEEE, vol. 78, pp. 1481–1497.
Poggio,T., and F. Girosi, 1990b.” Regularization algorithms for learning that are equivalent to mul-
tilayer networks,” Science, vol. 247, pp. 978–982.
Poggio, T., and C. Koch, 1985. “Ill-posed problems in early vision: From computational theory 
to analogue networks,”Proceedings of the Royal Society of London, Series B,vol.226,pp.303–323.
874
Bibliography

Poggio,T.,V.Torre, and C. Koch, 1985.“Computational vision and regularization theory,” Nature,
vol. 317, pp. 314–319.
Polak, E., and G. Ribiére, 1969.“Note sur la convergence de méthodes de directions conjuguées,”
Revue Française d’ Informatique et de Recherche Opérationnelle vol. 16, pp. 35–43.
Powell, M.J.D., 1992.“The theory of radial basis function approximation in 1990,” in W. Light, ed.,
Advances in Numerical Analysis Vol. II: Wavelets, Subdivision Algorithms, and Radial Basis
Functions, pp. 105–210, Oxford: Oxford Science Publications.
Powell, M.J.D., 1988.“Radial basis function approximations to polynomials,” Numerical Analysis
1987 Proceedings, pp. 223–241, Dundee, UK.
Powell, M.J.D., 1987. “A review of algorithms for nonlinear equations and unconstrained opti-
mization,” ICIAM’87: Proceedings of the First International Conference on Industrial and
Applied Mathematics,Philadelphia,Society for Industrial and Applied Mathematics,pp.220–264.
Powell, M.J.D., 1985.“Radial basis functions for multivariable interpolation:A review,” IMA Con-
ference on Algorithms for the Approximation of Functions and Data, pp. 143–167, RMCS,
Shrivenham, U.K.
Prechelt, L., 1998. Early Stopping—But When? in Neural Networks:Tricks of the Trade, ed. G. Orr
and K. Müller, Lecture Notes in Computer Science, no. 1524. Berlin: Springer, pp. 55–69.
Preisendorfer, R.W., 1988. Principal Component Analysis in Meteorology and Oceanography, New
York: Elsevier.
Press, W.H., and S.A. Teukolsky, 1990. “Orthogonal polynomials and Gaussian quadrature with
nonclassical weighting functions,” Computers in Physics, pp. 423–426.
Press,W.H., P.B. Flannery, S.A.Teukolsky, and W.T.Vetterling, 1988. Numerical Recipes in C: The
Art of Scientific Computing, Cambridge and New York: Cambridge University Press.
Prokhorov, D.V., 2007. “Training recurrent neurocontrollers for real-time applications,” IEEE
Trans. Neural Networks, vol. 16, pp. 1003–1015.
Prokhorov, D.V., 2006. “Training recurrent neurocontrollers for rubustness with derivation-free
Kalman filter,” IEEE Trans. Neural Networks, vol. 17, pp. 1606–1616.
Prokhorov,D.V.,L.A.Feldkamp,and I.Y.Tyukin,2002.“Adaptive behavior with fixed weights in RNN:
An overview,” Proceedings of the International Joint Conference on Neural Networks, Hawaii.
Prokhorov, D., G. Puskorius, and L. Feldkamp, 2001.“Dynamical neural networks for control,” in
J.Kolen and S.Kremer,eds.,A Field Guide to Dynamical Recurrent Networks, New York:IEEE
Press, pp. 257–289.
Prokhorov,D.V.,and D.C.Wunsch,II,1997.“Adaptive critic designs,”IEEE Transactions on Neural
Networks, vol. 8, pp. 997–1007.
Puskorius, G., and L. Feldkamp, 2001.“Parameter-based Kalman filter training:Theory and imple-
mentation,” in S. Haykin, ed., Kalman Filtering and Neural Networks, New York:Wiley.
Puskorius, G.V., L.A. Feldkamp, and L.I. Davis, Jr., 1996. “Dynamic neural network methods
applied to on-vehicle idle speed control,” Proceedings of the IEEE, vol. 84, pp. 1407–1420.
Puskorius,G.V.,and L.A.Feldkamp,1994.“Neurocontrol of nonlinear dynamical systems with Kalman
filter-trained recurrent networks,” IEEE Transactions on Neural Networks, vol. 5, pp. 279–297.
Puskorius,G.V.,and L.A.Feldkamp,1991.“Decoupled extended Kalman filter training of feedforward
layered networks,”International Joint Conference on Neural Networks, vol.1,pp.771–777,Seattle.
Rabiner, L.R., 1989. “A tutorial on hidden Markov models,” Proceedings of the IEEE, vol. 73,
pp. 1349–1387.
Rabiner, L.R., and B.H. Juang, 1986.“An introduction to hidden Markkov models,” IEEE ASSP
Magazine, vol. 3, pp. 4–16.
Bibliography
875

Ramón y Cajál,S.,1911,Histologie du Systéms Nerveux de l’homme et des vertébrés, Paris:Maloine.
Rao, A., D. Miller, K. Rose, and A. Gersho, 1997a. “Mixture of experts regression modeling by
deterministic annealing.” IEEE Transactions on Signal Processing, vol. 45, pp. 2811–2820.
Rao,A.,K.Rose,and A.Gersho,1997b.“A deterministic annealing approach to discriminative hidden
Markov model design,”Neural Networks for Signal Processing VII,Proceedings of the 1997 IEEE
Workshop, pp. 266–275,Amelia Island, FL.
Rao, C.R., 1973. Linear Statistical Inference and Its Applications, New York:Wiley.
Rao, R.P.N., B.A. Olshausen, and M.S. Lewicki, eds., 2002. Probabilistics Models of the Brain,
Cambridge, MA: MIT Press.
Rao, R.P.N., and T.J. Sejnowski, 2003. “Self-organizing neural systems based on predictive learn-
ing,” Philosophical Transactions of the Royal Society of London, vol.A.361, pp. 1149–1175.
Rao, R.P.N., and D.H. Ballard, 1999.“Predictive coding in the visual cortex:A functional inter-
pretation of some extra-classical receptive-field effects,” Nature Neuroscience, vol. 3,
pp. 79–87.
Rao,R.P.N.,and D.H.Ballard,1997.“Dynamic model of visual recognition predicts neural response
properties in the visual cortex,” Neural Computation, vol. 9, pp. 721–763.
Rashevsky, N., 1938. Mathematical Biophysics, Chicago: University of Chicago Press.
Reed,R.D.,and R.J.Marks,II,1999.Neural Smithing:Supervised Learning in Feedforward Artificial
Neural Networks, Cambridge, MA: MIT Press.
Reif, F., 1965. Fundamentals of Statistical and Thermal Physics, New York: McGraw-Hill.
Renals, S., 1989.“Radial basis function network for speech pattern classification,” Electronics Letters,
vol. 25, pp. 437–439.
Rényi,A., 1970. Probability Theory, North-Holland,Amsterdam.
Rényi, A. 1960. “On measures of entropy and information,” Proceedings of the 4th Berkeley
Symposium on Mathematics, Statistics, and Probability, pp. 547–561.
Richards,W., ed., 1988. Natural Computation, Cambridge, MA: MIT Press.
Rieke, F., D.Warland, R. van Steveninck, and W. Bialek, 1997. Spikes: Exploring the Neural Code,
Cambridge, MA: MIT Press.
Rifkin, R.M., 2002. Everything old is new again: A fresh look at historical approaches in machine
learning, Ph.D. thesis, MIT.
Rissanen, J., 1989. Stochastic Complexity in Statistical Inquiry, Singapore:World Scientific.
Rissanen, J., 1978.“Modeling by shortest data description,” Automatica, vol. 14, pp. 465–471.
Ristic, B., S. Arulampalam, and N. Gordon, 2004. Beyond the Kalman Filter: Particle Filters for
Tracking Applications, Boston: Artech House.
Ritter, H., 2003. “Self-organizing feature maps.” In M.A. Arbib, editor, The Handbook of Brain
Theory and Neural Networks, 2nd edition, pp. 1005–1010.
Ritter, H., 1991. “Asymptotic level density for a class of vector quantization processes,” IEEE
Transactions on Neural Networks, vol. 2, pp. 173–175.
Ritter, H., and T. Kohonen, 1989.“Self-organizing semantic maps,” Biological Cybernetics, vol. 61,
pp. 241–254.
Ritter, H.,T. Martinetz, and K. Schulten, 1992. Neural Computation and Self-Organizing Maps:An
Introduction, Reading, MA:Addison-Wesley.
Robbins, H., and S. Monro, 1951.“A stochastic approximation method,” Annals of Mathematical
Statistics, vol. 22, pp. 400–407.
Robert, C.P., 2001. The Bayesian Choice, New York: Springer.
Robert, C.P., and G. Casella, 1999. Monte Carlo Statistical Methods, New York: Springer.
876
Bibliography

Roberts,S.,and R.Everson,editors,2001.Independent Component Analysis:Principles and Practice,
Cambridge, U.K., and New York: Cambridge University Press.
Rochester,N.,J.H.Holland,L.H.Haibt,and W.L.Duda,1956.“Tests on a cell assembly theory of the
action of the brain, using a large digital computer,” IRE Transactions on Information Theory,
vol. IT-2, pp. 80–93.
Rose, K., 1998. “Deterministic annealing for clustering, compression, classification, regression,
and related optimization problems,” Proceedings of the IEEE, vol. 86, pp. 2210–2239.
Rose, K., 1991. Deterministic Annealing, Clustering, and Optimization, Ph.D. Thesis, California
Institute of Technology, Pasadena, CA.
Rose, K., E. Gurewitz, and G.C. Fox, 1992.“Vector quantization by deterministic annealing,”IEEE
Transactions on Information Theory, vol. 38, pp. 1249–1257.
Rose, K., E. Gurewitz, and G.C. Fox, 1990.“Statistical mechanics and phase transitions in cluster-
ing,” Physical Review Letters, vol. 65, pp. 945–948.
Rosenberg, S., 1997. The Laplacian on a Riemannian Manifold, Cambridge, U.K., and New York:
Cambridge University Press.
Rosenblatt, F., 1962. Principles of Neurodynamics, Washington, DC: Spartan Books.
Rosenblatt, F., 1958.“The Perceptron:A probabilistic model for information storage and organi-
zation in the brain,” Psychological Review, vol. 65, pp. 386–408.
Rosenblatt, M., 1970.“Density estimates and Markov sequences,” in M. Puri, ed., Nonparametric
Techniques in Statistical Inference, pp. 199–213, London: Cambridge University Press.
Rosenblatt, M., 1956.“Remarks on some nonparametric estimates of a density function,” Annals
of Mathematical Statistics., vol. 27, pp. 832–837.
Ross, S.M., 1983. Introduction to Stochastic Dynamic Programming, New York:Academic Press.
Roth,Z.,and Y.Baram,1996.“Multi-dimensional density shaping by sigmoids,”IEEE Transactions
on Neural Networks, vol. 7, pp. 1291–1298.
Roussas, G., ed., 1991. Nonparametric Functional Estimation and Related Topics,The Netherlands:
Kluwer.
Roy, N., and G. Gordon, 2003. “Exponential family PCA for belief compression in POMDPs,”
Advances in Neural Information Processing Systems, vol. 15, pp. 1667–1674.
Roy, S., and J.J. Shynk, 1990.“Analysis of the momentum LMS algorithm,” IEEE Transactions on
Acoustics, Speech, and Signal Processing, vol.ASSP-38, pp. 2088–2098.
Rubin, D.B., 1988.“Using the SIR algorithm to simulate posterior distribution,” in J.M. Bernardo,
M.H. DeGroot, D.V. Lindley, and A.F.M. Smith, eds., Bayesian Statistics, vol. 3, pp. 395–402,
Oxford, U.K.: Oxford University Press.
Rubner, J., and K. Schulten, 1990. “Development of feature detectors by self-organization,”
Biological Cybernetics, vol. 62, pp. 193–199.
Rubner, J., and P. Tavan, 1989. “A self-organizing network for principal component analysis,”
Europhysics Letters, vol. 10, pp. 693–698.
Ruderman,D.L.,1997.“Origins of scaling in natural images,”Vision Research, vol.37,pp.3385–3395.
Rueckl, J.G., K.R. Cave, and S.M. Kosslyn, 1989.“Why are ‘what’ and ‘where’ processed by separate
cortical visual systems? A computational investigation,”J.Cognitive Neuroscience, vol.1,pp.171–186.
Rumelhart, D.E., and J.L. McClelland, eds., 1986. Parallel Distributed Processing: Explorations in
the Microstructure of Cognition, vol. 1, Cambridge, MA: MIT Press.
Rumelhart,D.E.,and D.Zipser,1985.“Feature discovery by competitive learning,”Cognitive Science,
vol. 9, pp. 75–112.
Bibliography
877

Rumelhart, D.E., G.E. Hinton, and R.J. Williams, 1986a. “Learning representations of back-
propagation errors,” Nature (London), vol. 323, pp. 533–536.
Rumelhart,D.E.,G.E.Hinton,and R.J.Williams,1986b.“Learning internal representations by error
propagation,” in D.E. Rumelhart and J.L. McCleland, eds., vol 1, Chapter 8, Cambridge, MA:
MIT Press.
Russell, S.J., and P. Novig, 1995. Artificial Intelligence:A Modem Approach, Upper Saddle River,
NJ: Prentice Hall.
Saarinen, S., R.B. Bramley, and G. Cybenko, 1992.“Neural networks, backpropagation, and auto-
matic differentiation,” in Automatic Differentiation of Algorithms: Theory, Implementation,
and Application, A. Griewank and G.F. Corliss, eds., pp. 31–42, Philadelphia: SIAM.
Saarinen, S., R. Bramley, and G. Cybenko, 1991.“The numerical solution of neural network train-
ing problems,”CRSD Report No.1089, Center for Supercomputing Research and Development,
University of Illinois, Urbana, IL.
Saerens, M., and A. Soquet, 1991.“Neural controller based on back-propagation algorithm,” IEE
Proceedings (London), Part F, vol. 138, pp. 55–62.
Salomon, R., and J.L. van Hemmen, 1996.“Accelerating backpropagation through dynamic self-
adaptation,” Neural Networks, vol. 9, pp. 589–601.
Samuel,A.L., 1959.“Some studies in machine learning using the game of checkers,” IBM Journal
of Research and Development, vol. 3, pp. 211–229.
Sandberg, I.W., 1991.“Structure theorems for nonlinear systems,” Multidimensional Systems and
Signal Processing, vol. 2, pp. 267–286.
Sandberg, I.W., L. Xu, 1997a.“Uniform approximation of multidimensional myopic maps,” IEEE
Transactions on Circuits and Systems, vol. 44, pp. 477–485.
Sandberg,I.W.,and L.Xu,1997b.“Uniform approximation and gamma networks,”Neural Networks,
vol. 10, pp. 781–784.
Sanger,T.D.,1990.“Analysis of the two-dimensional receptive fields learned by the Hebbian algorithm
in response to random input,” Biological Cybernetics, vol. 63, pp. 221–228.
Sanger,T.D.,1989a.“An optimality principle for unsupervised learning,”Advances in Neural Infor-
mation Processing Systems, vol. 1, pp. 11–19, San Mateo, CA: Morgan Kaufmann.
Sanger,T.D., 1989b.“Optimal unsupervised learning in a single-layer linear feedforward neural
network,” Neural Networks, vol. 12, pp. 459–473.
Sauer,T., J.A.Yorke, and M. Casdagli, 1991.“Embedology,” Journal of Statistical Physics, vol. 65,
pp. 579–617.
Saul, L.K., and M.I. Jordan, 1995. “Boltzmann chains and hidden Markov models,” Advances in
Neural Information Processing Systems, vol. 7, pp. 435–442.
Scharf, L.L., 1991. Statistical Signal Processing: Detection, Estimation, and Time Series Analysis,
Reading, MA:Addison-Wesley.
Schei,T.S., 1997.“A finite-difference method for linearization in nonlinear estimation algorithms,”
Automatica, vol. 33, pp. 2051–2058.
Schiffman, W.H., and H.W. Geffers, 1993. “Adaptive control of dynamic systems by back propa-
gation networks,” Neural Networks, vol. 6, pp. 517–524.
Schölkopf, B., and A.J. Smola, 2002. Learning with Kernels: Support Vector Machines, Regulariza-
tion, Optimization, and Beyond, Cambridge, MA: MIT Press.
Schölkopf,B.,A.J.Smola,and K.Müller,1998.“Nonlinear component analysis as a kernel eigenvalue
problem,” Neural Computation, vol. 10, pp. 1299–1319.
878
Bibliography

Schölkopf, B., 1997. Support Vector Learning, Munich, Germany: R. Oldenbourg Verlag.
Schraudolf, N.N., J.Yu, and S. Günter, 2007.“A stochastic quasi-Newton method for on-line con-
vex optimization,” Proceedings of 11th Intl. Conf. Artificial Intelligence and Statistics, Puerto
Rico, pp. 433–440.
Schraudolph, N.N., 2002. “Fast curvature matrix–vector products for second-order gradient
descent,” Neural Computation, vol. 4, pp. 1723–1738.
Schultz,W., 2007.“Reward signals”, Scholarpedia, vol. 2, issue 6, 16 pages.
Schultz, W., 1998. “Predictive reward signal of dopamine neurons”, J. Neurophysiology, vol. 80,
pp. 1–27.
Schumaker, L.L., 1981, Spline Functions: Basic Theory, New York:Wiley.
Seber, G.A.F., and C.J.Wild, 1989. Nonlinear Regression, New York:Wiley.
Sejnowski, T.J., 1977a. “Strong covariance with nonlinearly interacting neurons,” Journal of
Mathematical Biology, vol. 4, pp. 303–321.
Sejnowski,T.J.,1977b.“Statistical constraints on synaptic plasticity,”Journal of Theoretical Biology,
vol. 69, pp. 385–389.
Sejnowski, T.J., and C.R. Rosenberg, 1987. “Parallel networks that learn to pronounce English
text,” Complex Systems, vol. 1, pp. 145–168.
Selfridge, O.G., 1958. “Pandemonium: A paradigm for learning,” Mechanization of Thought
Processes, Proceedings of a Symposium held at the National Physical Laboratory, pp. 513–526,
London,November.(Reproduced in J.A.Anderson and E.Rosenfeld,editors,Neurocomputing,
pp. 117–122, Cambridge, MA: MIT Press, 1988.)
Shah,S.,and F.Palmieri,1990.“MEKA—A fast,local algorithm for training feedforward neural net-
works,” International Joint Conference on Neural Networks, vol. 3, pp. 41–46, San Diego.
Shamma, S.A., 2001.“On the role of space and time in auditory processing,” Trends in Cognitive
Sciences, vol. 5, pp. 340–348.
Shamma, S., 1989.“Spatial and temporal processing in central auditory networks,” in Methods in
Neural Modeling, C. Koch and I. Segev, Eds., Cambridge, MA: MIT Press.
Shanno, D.F., 1978. “Conjugate gradient methods with inexact line searches,” Mathematics of
Operations Research, vol. 3, pp. 244–256.
Shannon, C.E., 1948.“A mathematical theory of communication,” Bell System Technical Journal,
vol. 27, pp. 379–423, 623–656.
Shannon, C.E., and W. Weaver, 1949. The Mathematical Theory of Communication, Urbana, IL:
The University of Illinois Press.
Shannon,C.E.,and J.McCarthy,eds.,1956.Automata Studies, Princeton,NJ:Princeton University Press.
Shawe-Taylor, J., and N. Cristianini, 2004. Kernel Methods for Pattern Analysis, Cambridge, U.K.,
and New York: Cambridge University Press.
Shepherd, G.M., 1988. Neurobiology, 2d ed., New York: Oxford University Press.
Shepherd, G.M., ed., 1990. The Synoptic Organization of the Brain, 3d ed., New York: Oxford
University Press.
Shepherd, G.M., and C. Koch, 1990.“Introduction to synaptic circuits,” in The Synaptic Organiza-
tion of the Brain, G.M. Shepherd, ed., pp. 3–31. New York: Oxford University Press.
Sherrington, C.S., 1933. The Brain and Its Mechanism, London: Cambridge University Press.
Sherrington,C.S.,1906.The Integrative Action of the Nervous System, New York:Oxford University
Press.
Sherrington, D., and S. Kirkpatrick, 1975.“Spin-glasses,” Physical Review Letters, vol. 35, p. 1972.
Shewchuk, J.R., 1994. An Introduction to the Conjugate Gradient Method Without the Agonizing
Pain, School of Computer Science, Carnegie Mellon University, Pittsburgh,August 4, 1994.
Bibliography
879

Shore, J.E., and R.W. Johnson, 1980.“Axiomatic derivation of the principle of maximum entropy
and the principle of minimum cross-entropy,” IEEE Transactions on Information Theory,
vol. IT-26, pp. 26–37.
Shynk,J.J.,1990.“Performance surfaces of a single-layer perceptron,”IEEE Transactions or Neural
Networks, 1, 268–274.
Shynk,J.J.and N.J.Bershad,1991.Steady-state analysis of a single-layer perceptron based on a system
identification model with bias terms,” IEEE Transactions on Circuits and Systems, vol. CAS-38,
pp. 1030–1042.
Si, J.,A.G. Barto,W.B. Powell, and D.Wunsch II, eds., 2004. Handbook of Learning and Approxi-
mate Dynamic Programming, Hoboken, NJ:Wiley-Interscience.
Siegelmann,H.T.,B.G.Home,and C.L.Giles,1997.“Computational capabilities of recurrent NARX
neural networks,” Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 27, pp. 208–215.
Siegelmann,H.T.,and E.D.Sontag,1991.“Turing computability with neural nets,”Applied Mathemat-
ics Letters, vol. 4, pp. 77–80.
Silver, D., R.S. Sutton, and M. Müller, 2008. “Sample-based learning and search with permanent
and transient memories,” Proceedings of the 25th International Conference on Machine Learning,
Helsinki, Finland.
Simmons, J.A., P.A. Saillant, and S.P. Dear, 1992. “Through a bat’s ear,” IEEE Spectrum, vol. 29,
issue 3, pp. 46–48, March.
Sindhwani,V.,M.Belkin,and P.Niyogi,2006.“The geometric basis of semi-supervised learning,”in O.
Chapelle,B.Schölkopf,and A.Zien,eds.,Semi-Supervised Learning, pp.217–235,Cambridge,MA:
MIT Press.
Sindhwani,V., P. Niyogi, and M. Belkin, 2005.“Beyond the point cloud: From transductive to semi-
supervised learning,” Proceedings of the 22nd International Conference on Machine Learning,
Bonn, Germany.
Singh, S., and D. Bertsekas, 1997. “Reinforcement learning for dynamic channel allocation in
cellular telephone systems,” Advances in Neural Information Processing Systems, vol. 9,
pp. 974–980, Cambridge, MA: MIT Press.
Singhal, S., and L. Wu, 1989. “Training feed-forward networks with the extended Kalman filter,”
IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 1187–1190,
Glasgow, Scotland.
Sklar, A. (1959), “Fonctions de repartition ‘a n dimensions et leurs marges,” Publ. Inst. Statist.
Univ. Paris 8, pp. 229–231.
Slepian, D., 1973. Key papers in the development of information theory, New York: IEEE Press.
Sloane, N.J.A., and A.D. Wyner, 1993. Claude Shannon: Collected Papers, New York: IEEE
Press.
Slonim, N., N. Friedman, and N.Tishby, 2006.“Multivariate information bottleneck,” Neural Com-
putation, vol. 18, pp. 1739–1789.
Slotine, J.-J., and W. Li, 1991.Applied Nonlinear Control, Englewood Cliffs, NJ: Prentice Hall.
Smith, T. 2007. Probabilistic Planning for Robotic Exploration, Ph.D. thesis, Carnegie Mellon
University, Pittsburgh.
Smolensky, P., 1986. “Information processing in dynamical systems: Foundations of Informa-
tion Theory,” in D.E. Rumelhart, J.L. McLelland, and the PDP Research Group, Parallel Dis-
tributed Processing, Volume 1: Foundations, Chapter 6, pp. 194–281, Cambridge, MA: MIT
Press.
880
Bibliography

Snyder, D.L., 1975. Random Point Processes, New York:Wiley-Interscience.
Snyder, D.L., 1972.“Filtering and detection for doubly stochastic Poisson processes,” IEEE Trans.
Information Theory, vol. IT-18, pp. 91–102.
Sompolinsky, H., N. Barkai, and H.S. Seung, 1995. “On-line learning and dicotomies: Algorithms
and learning curves,” in J.-H. Oh, C. Kwon, and S. Cho, eds., Neural Networks: The Statistical
Mechanics Perspective, pp. 105–130, Singapore and River Edge, NJ:World Scientific.
Sondik, E.J., 1971. The Optimal Control of Partially Observable Markov Processes, Ph.D. thesis,
Stanford University.
Sontag, E.D., 1992. “Feedback stabilization using two-hidden-layer nets,” IEEE Transactions on
Neural Networks, vol. 3, pp. 981–990.
Sontag, E.D., 1990. Mathematical Control Theory: Deterministic Finite Dimensional Systems, New
York: Springer-Verlag.
Sontag, E.D., 1989.“Sigmoids distinguish more efficiently than Heavisides,” Neural Computation,
vol. 1, pp. 470–472.
Southwell,R.V.,1946.Relaxation Methods in Theoretical Physics, New York:Oxford University Press.
Specht,D.F.,1991.“A general regression neural network,”IEEE Transactions on Neural Networks,
vol. 2, pp. 568–576.
Sperduti, A., 1997. “On the computational power of recurrent neural networks for structures,”
Neural Networks, vol. 10, pp. 395–400.
Sprecher, D.A., 1965.“On the structure of continuous functions of several variables,” Transactions
of the American Mathematical Society, vol. 115, pp. 340–355.
Steinbuch, K., 1961.“Die Lernmatrix.” Kybernetik, vol. 1, pp. 36–45.
Steinwart, I., 2003.“Sparseness of support vector machines,” J. Machine Learning Research, vol. 4,
pp. 1071–1105.
Stent, G.S., 1973. “A physiological mechanism for Hebb’s postulate of learning,” Proceedings of
the National Academy of Sciences, USA, vol. 70, pp. 997–1001.
Sterling, P., 1990.“Retina,” in The Synoptic Organization of the Brain, G.M. Shepherd, ed., 3d ed.,
pp. 170–213, New York: Oxford University Press.
Stewart, G.W., 1973. Introduction to Matrix Computations, New York: Academic Press.
Stone, M., 1978. “Cross-validation: A review,” Mathematische Operationsforschung Statistischen,
Serie Statistics, vol. 9, pp. 127–139.
Stone, M., 1974.“Cross-validatory choice and assessment of statistical predictions,” Journal of the
Royal Statistical Society, vol. B36, pp. 111–133.
Strang, G., 1980. Linear Algebra and its Applications, New York:Academic Press.
Stroud,A.H.,1971.Approximate Calculation of Multiple Integrals, Englewood Cliffs,NJ:Prentice-Hall.
Stroud,A.H., 1966. Gaussian Quadrature Formulas, Englewood Cliffs, NJ: Prentice-Hall.
Su, H.-T., and T. McAvoy, 1991. “Identification of chemical processes using recurrent networks,”
Proceedings of the 10th American Controls Conference, vol. 3, pp. 2314–2319, Boston.
Su, H.-T., T. McAvoy, and P. Werbos, 1992. “Long-term predictions of chemical processes using
recurrent neural networks:A parallel training approach,” Industrial Engineering and Chemical
Research, vol. 31, pp. 1338–1352.
Suga,N.,1990a.“Cortical computational maps for auditory imaging,”Neural Networks, vol.3,pp.3–21.
Suga,N.,1990b.“Computations of velocity and range in the bat auditory system for echo location,”
in Computational Neuroscience, E.L. Schwartz, ed., pp. 213–231, Cambridge, MA: MIT Press.
Suga,N.,1990c.“Biosonar and neural computation in bats,” Scientific American, vol.262,pp.60–68.
Bibliography
881

Suga,N.,1985.“The extent to which bisonar information is represented in the bat auditory cortex,”
in Dynamic Aspects of Neocortical Function, G.M. Edelman,W.E. Gall, and W.M. Cowan, eds.
pp. 653–695, New York:Wiley (Interscience).
Suga,N., 2003,“Echolocation:Chocleotopic and computational maps,”in M.A.Arbib,ed.,The Hand-
book of Brain Theory and Neural Networks, 2d edition,pp.381–387,Cambridge,MA:MIT Press.
Sutton,R.S.,1988.“Learning to predict by the methods of temporal differences,”Machine Learning,
vol. 3, pp. 9–44.
Sutton, R.S., 1984. “Temporal credit assignment in reinforcement learning,” Ph.D. dissertation,
University of Massachusetts,Amherst, MA.
Sutton, R.S., ed., 1992. Special Issue on Reinforcement Learning, Machine Learning, vol. 8,
pp. 1–395.
Sutton, R.S., and A.G. Barto, 1998. Reinforcement Learning: An Introduction, Cambridge, MA:
MIT Press.
Sutton, R.S., and B.Tanner, 2005.“Temporal Difference Networks,” Advances in Neural Informa-
tion Processing Systems, vol. 17, pp. 1377–1384.
Suykens, J.A.,T.Van Gestel, J. DeBrabanter, B. DeMoor, and J.Vanderwalle, 2002. Least-Squares
Support Vector Machines, River Edge, NJ:World Scientific.
Suykens,J.A.K., J.P.L.Vandewalle,and B.L.R.DeMoor,1996.Artificial Neural Networks for Model-
ing and Control of Non-Linear Systems, Dordrecht,The Netherlands: Kluwer.
Takens, F., 1981.“On the numerical determination of the dimension of an attractor,” in D. Rand
and L.S. Young, eds., Dynamical Systems and Turbulence, Annual Notes in Mathematics,
vol. 898, pp. 366–381, Berlin: Springer-Verlag.
Tapia, R.A., and J.R.Thompson, 1978. Nonparametric Probability Density Estimation, Baltimore:
The Johns Hopkins University Press.
Tesauro, G., 1995.“Temporal difference learning and TD-gamma,” Communications of the Associ-
ation for Computing Machinery, vol. 38, pp. 58–68.
Tesauro, G., 1994. “TD-Gammon, A self-teaching Backgammon program, achieves master-level
play,” Neural Computation, vol. 6, pp. 215–219.
Tesauro, G., 1992. “Practical issues in temporal difference learning,” Machine Learning, vol. 8,
pp. 257–277.
Tesauro,G.,1989.“Neurogammon wins computer olympiad,”Neural Computation, vol.1,pp.321–323.
Tesauro, G., and R. Janssens, 1988.“Scaling relationships in back-propagation learning,” Complex
Systems, vol. 2, pp. 39–44.
Theodoridis,S.,and K.Koutroumbas,2003.Pattern Recognition, 2d ed.,Amsterdam and Boston:Acad-
emic Press.
Thorndike, E.L., 1911. Animal Intelligence, Darien, CT: Hafner.
Thrun,S.B.,1992.“The role of exploration in learning control,”in Handbook of Intelligent Control,
D.A.White and D.A. Sofge, eds., pp. 527–559, New York:Van Nostrand Reinhold.
Tichavsky, P., Z. Koldovsky, and E. Oja, 2006. “Performance analysis of FastICA algorithm and
Cramér–Rao bounds for linear independent component analysis,”IEEE Trans.Signal Processing,
vol. 54, pp. 1189–1203.
Tikhonov,A.N., 1973.“On regularization of ill-posed problems,” Doklady Akademii Nauk USSR,
vol. 153, pp. 49–52.
Tikhonov, A.N., 1963. “On solving incorrectly posed problems and method of regularization,”
Doklady Akademii Nauk USSR, vol. 151, pp. 501–504.
882
Bibliography

Tikhonov, A.N., and V.Y. Arsenin, 1977. Solutions of Ill-posed Problems, Washington, DC: W.H.
Winston.
Tishby, N., and N. Slonim, 2001. “Data Clustering by Markovian relaxation and the information
bottleneck method,” Advances in Neural Information Processing Systems, vol. 13, pp. 640–646.
Tishby, N., F.C. Pereira, and W. Bialek, 1999.“The information bottleneck method,” Proceedings of
the 37th Annual Allerton Conference on Communications,Control and Computing, pp.368–377.
Touretzky, D.S., and D.A. Pomerleau, 1989. “What is hidden in the hidden layers?” Byte, vol. 14,
pp. 227–233.
Tsitsiklis,J.N.,1994.“Asynchronous stochastic approximation and Q-learning,”Machine Learning,
vol. 16, pp. 185–202.
Tsoi, A.C., and A.D. Back, 1994. “Locally recurrent globally feedforward networks: A critical
review,” IEEE Transactions on Neural Networks, vol. 5, pp. 229–239.
Tu, Z.W., X. R. Chen, A.L. Yiulle, and S.C. Zhu, 2005. “Image parsing: Unifying segmentation,
detection, and recognition,” International J. Computer Vision, vol. 63, pp. 113–140.
Turing, A.M., 1952. “The chemical basis of morphogenesis,” Philosophical Transactions of the
Royal Society, B, vol. 237, pp. 5–72.
Turing,A.M., 1950.“Computing machinery and intelligence,” Mind, vol. 59, pp. 433–460.
Turing,A.M., 1936.“On computable numbers with an application to the Entscheidungs problem,”
Proceedings of the London Mathematical Society, Series 2, vol. 42, pp. 230–265. Correction pub-
lished in vol. 43, pp. 544–546.
Ukrainec,A.M., and S. Haykin, 1996.“A modular neural network for enhancement of cross-polar
radar targets,” Neural Networks, vol. 9, pp. 143–168.
Ukrainec,A., and S. Haykin, 1992.“Enhancement of radar images using mutual information based
unsupervised neural networks,”Canadian Conference on Electrical and Computer Engineering,
pp. MA6.9.1–MA6.9.4,Toronto, Canada.
Uttley,A.M., 1979. Information Transmission in the Nervous System, London:Academic Press.
Uttley, A.M., 1970. “The informon: A network for adaptive pattern recognition,” Journal of
Theoretical Biology, vol. 27, pp. 31–67.
Uttley,A.M., 1966.“The transmission of information and the effect of local feedback in theoret-
ical and neural networks,” Brain Research, vol. 102, pp. 23–35.
Uttley, A.M., 1956. “A theory of the mechanism of learning based on the computation of condi-
tional probabilities,” Proceedings of the First International Conference on Cybernetics, Namur,
Gauthier-Villars, Paris.
Valiant, L.G., 1984.“A theory of the learnable,” Communications of the Association for Comput-
ing Machinery, vol. 27, pp. 1134–1142.
Van Essen, D.C., C.H.Anderson, and D.J. Felleman, 1992.“Information processing in the primate
visual system:An integrated systems perspective,” Science, vol. 255, pp. 419–423.
Van Hulle, M.M., 2005.“Maximum likelihood topographic map formation,” Neural Computation,
vol. 17, pp. 503–513.
Van Hulle, M.M., 2002a. “Kernel-based topographic map formation by local density modeling,”
Neural Computation, vol. 14, pp. 1561–1573.
Van Hulle, M.M., 2002b.“Joint entropy maximization in kernel-based topographic maps,” Neural
Computation, vol. 14, pp. 1887–1906.
Van Hulle, M.M., 1997. “Nonparametric density estimation and regression achieved with topo-
graphic maps maximizing the information-theoretic entropy of their outputs,”Biological Cyber-
netics, vol. 77, pp. 49–61.
Bibliography
883

Van Hulle, M.M., 1996. “Topographic map formation by maximizing unconditional entropy: A
plausible strategy for “on-line” unsupervised competitive learning and nonparametric density
estimation,” IEEE Transactions on Neural Networks, vol. 7, pp. 1299–1305.
van Laarhoven, P.J.M., and E.H.L. Aarts, 1988. Simulated Annealing: Theory and Applications,
Boston: Kluwer Academic Publishers.
Van Trees, H.L., 1968. Detection, Estimation, and Modulation Theory, Part I, New York:Wiley.
Vapnik,V.N., 1998. Statistical Learning Theory, New York:Wiley.
Vapnik,V.N., 1995. The Nature of Statistical Learning Theory, New York: Springer-Verlag.
Vapnik,V.N., 1992.“Principles of risk minimization for learning theory,” Advances in Neural Infor-
mation Processing Systems, vol. 4, pp. 831–838, San Mateo, CA: Morgan Kaufmann.
Vapnik,V.N.,1982.Estimation of Dependences Based on Empirical Data, New York:Springer-Verlag.
Vapnik,V.N., and A.Ya. Chervonenkis, 1971.“On the uniform convergence of relative frequencies
of events to their probabilities,”Theoretical Probability and Its Applications, vol.17,pp.264–280.
Vapnik,V.N., and A.Ya. Chervonenkis, 1964.“A note on a class of perceptrons,” Automation and
Remote Control, vol. 25, pp. 103–109.
Venkataraman, S., 1994. “On encoding nonlinear oscillations in neural networks for locomotion,”
Proceedings of the 8th Yale Workshop on Adaptive and Learning Systems, pp. 14–20, New Haven,
CT.
Vidyasagar, M., 1997. A Theory of Learning and Generalization, London: Springer-Verlag.
Vidyasagar, M., 1993. Nonlinear Systems Analysis, 2d ed., Englewood Cliffs, NJ: Prentice Hall.
Viterbi,A.J., 1967.“Error bounds for convolutional codes and an asymptotically optimum decod-
ing algorithm,” IEEE Transactions on Information Theory, vol. IT-13, pp. 260–269.
von der Malsburg,C.,1990a.“Network self-organization,”in An Introduction to Neural and Electronic
Networks, S.F. Zornetzer, J.L. Davis, and C. Lau, eds., pp. 421–432, San Diego:Academic Press.
von der Malsburg, C., 1990b. “Considerations for a visual architecture,” in Advanced Neural
Computers, R. Eckmiller, ed., pp. 303–312, Amsterdam: North-Holland.
von der Malsburg,C.,1981.“The correlation theory of brain function,”Internal Report 81–2, Depart-
ment of Neurobiology, Max-Plak-Institute for Biophysical Chemistry, Göttingen, Germany.
von der Malsburg, C., 1973.“Self-organization of orientation sensitive cells in the striate cortex,”
Kybernetik, vol. 14, pp. 85–100.
von der Malsburg, C., and W. Schneider, 1986. “A neural cocktail party processor,” Biological
Cybernetics, vol. 54, pp. 29–40.
von Neumann, J., 1986. Papers of John von Neumann on Computing and Computer Theory,
W. Aspray and A. Burks, eds., Cambridge, MA: MIT Press.
von Neumann, J., 1958. The Computer and the Brain, New Haven, CT:Yale University Press.
von Neumann,J.,1956.“Probabilistic logics and the synthesis of reliable organisms from unreliable
components,”in Automata Studies, C.E.Shannon and J.McCarthy,eds.,pp.43–98,Princeton,NJ:
Princeton University Press.
Wahba, G., 1990. Spline Models for Observational Data, SIAM.
Wahba, G., D.R. Johnson, F. Gao, and J. Gong, 1995.“Adaptive tuning of numerical weather pre-
diction models: Randomized GCV in three and four dimensional data assimilation,” Monthly
Weather Review, vol. 123, pp. 3358–3369.
Watkins, C.J.C.H., 1989. Learning from Delayed Rewards, Ph.D. thesis, University of Cambridge,
Cambridge, U.K.
Watkins, C.J.C.H., and P. Dayan, 1992.“Q-leaming,” Machine Learning, vol. 8, pp. 279–292.
884
Bibliography

Watrous, R.L. 1987. “Learning algorithms for connectionist networks: Applied gradient meth-
ods of nonlinear optimization,” First IEEE International Conference on Neural Networks,
vol. 2, pp. 619–627, San Diego.
Watson,G.S.,1964.“Smooth regression analysis,”Sankhy :The Indian Journal of Statistics,Series A,
vol. 26, pp. 359–372.
Wax,W., and T. Kailath, 1985.“Detection of signals by information theoretic criteria,” IEEE Trans.
Acoustics, Speech and Signal Processing, vol.ASSP32, pp. 387–392.
Webb,A.R.,1994.“Functional approximation by feed-forward networks:A least-squares approach
to generalisation,” IEEE Transactions on Neural Networks, vol. 5, pp. 480–488.
Webb,A.R., and D. Lowe, 1990.“The optimal internal representation of multilayer classifier net-
works performs nonlinear discriminant analysis,” Neural Networks, vol. 3, pp. 367–375.
Weierstrass, K., 1885.“Uber die analytische Darstellbarkeit sogenannter willkurlicher Funktionen
einer reellen veranderlichen,” Sitzungsberichte der Akademie der Wissenschaften, Berlin,
pp. 633–639, 789–905.
Werbos, P., 2004. “ADP: Goals, opportunities and principles,” in J. Si, A.G. Barto, W.B. Powell,
and D. Wunsch II, eds., Handbook of Learning and Approximate Dynamic Programming,
Hoboken, NJ:Wiley-Interscience.
Werbos,P.J.,1992.“Neural networks and the human mind:New mathematics fits humanistic insight,”
IEEE International Conference on Systems, Man, and Cybernetics, vol. 1, pp. 78–83, Chicago.
Werbos, P.J., 1990. “Backpropagation through time: What it does and how to do it,” Proceedings
of the IEEE, vol. 78, pp. 1550–1560.
Werbos, P.J., 1989. “Backpropagation and neurocontrol: A review and prospectus,” International
Joint Conference on Neural Networks, vol. I, pp. 209–216,Washington, DC.
Werbos, P., 1977. “Advanced forecasting for global crisis warning and models of intelligence,”
General Systems Yearbook, vol. 22, pp. 25–38.
Werbos, P.J., 1974. “Beyond regression: New tools for prediction and analysis in the behavioral
sciences,” Ph.D. thesis, Harvard University, Cambridge, MA.
Whitney, H., 1936.“Differentiable manifolds,” Annals of Mathematics, vol. 37, pp. 645–680.
Whittaker, E.T., 1923. “On a new method of graduation,” Proceedings of the Edinburgh Mathe-
matical Society, vol. 41, pp. 63–75.
Widrow, B., N.K. Gupta, and S. Maitra, 1973. “Punish/reward: Learning with a critic in adap-
tive threshold systems,” IEEE Transactions of Systems, Man, and Cybernetics, vol. SMC-3,
pp. 455–465.
Widrow, B., and M.E. Hoff, Jr., 1960.“Adaptive Switching Circuits,” IRE WESCON Conv. Rec.,
Pt. 4, pp. 96–104.
Widrow,B.,and M.A.Lehr,1990.“30 years of adaptive neural networks:Perceptron,madaline,and
back-propagation,” Proceedings of the Institute of Electrical and Electronics Engineers, vol. 78,
pp. 1415–1442.
Widrow, B., P.E. Mantey, L.J. Griffiths, and B.B. Goode, 1967. “Adaptive antenna systems,”
Proceedings of the IEEE, vol. 55, pp. 2143–2159.
Widrow,B.,and S.D.Stearns,1985.Adaptive Signal Processing, Englewood Cliffs,NJ:Prentice-Hall.
Widrow,B.,and E.Walach,1996.Adaptive Inverse Control, Upper Saddle River,NJ:Prentice Hall.
Wieland, A., and R. Leighton, 1987. “Geometric analysis of neural network capabilities,” first
IEEE International Conference on Neural Networks, vol. III, pp. 385–392, San Diego.
Wiener, N., 1961. Cybernetics, 2d ed., New York:Wiley.
a
Bibliography
885

Wiener, N., 1958. Nonlinear Problems in Random Theory, New York:Wiley.
Wiener, N., 1949. Extrapolation, Interpolation, and Smoothing of Stationary Time Series with Engi-
neering Applications, Cambridge, MA: MIT Press. (This was originally issued as a classified
National Defense Research Report, February 1942).
Wiener, N., 1948. Cybernetics: Or Control and Communication in the Animal and the Machine,
New York:Wiley.
Wilks, S.S., 1962. Mathematical Statistics, New York:Wiley.
Williams, R.J., and J. Peng, 1990. “An efficient gradient-based algorithm for on-line training of
recurrent network trajectories,” Neural Computation, vol. 2, pp. 490–501.
Williams, R.J., and D. Zipser, 1995. “Gradient-based learning algorithms for recurrent networks
and their computational complexity,”in Y.Chauvin and D.E.Rumelhart,eds.,Backpropagation:
Theory,Architectures, and Applications, pp. 433–486, Hillsdale, NJ: Lawrence Erlbaum.
Williams, R.J., and D. Zipser, 1989.“A learning algorithm for continually running fully recurrent
neural networks,” Neural Computation, vol. 1, pp. 270–280.
Willshaw, D.J., O.P. Buneman, and H.C. Longuet-Higgins, 1969. “Non-holographic associative
memory,” Nature (London), vol. 222, pp. 960–962.
Willshaw, D.J., and C. von der Malsburg, 1976.“How patterned neural connections can be set up
by self-organization,”Proceedings of the Royal Society of London Series B, vol.194,pp.431–445.
Wilson, G.V., and G.S. Pawley, 1988. “On the stability of the travelling salesman problem algo-
rithm of Hopfield and Tank,” Biological Cybernetics, vol. 58, pp. 63–70.
Wilson, H.R., and J.D. Gowan, 1972.“Excitatory and inhibitory interactions in localized popula-
tions of model neurons,” Journal of Biophysics, vol. 12, pp. 1–24.
Winder, R.O., 1961.“Single stage threshold logic,” Switching Circuit Theory and Logical Design,
AIEE Special Publications, vol. S-134, pp. 321–332.
Winograd, S., and J.D. Cowan, 1963. Reliable Computation in the Presence of Noise, Cambridge,
MA: MIT Press.
Wood,N.L.,and N.Cowan,1995.“The cocktail party phenomenon revisited:Attention and memory
in the classic selective listening procedure of Cherry (1953),”Journal of Experimental Psychology:
General, vol. 124, pp. 243–262.
Woods, W.A., 1986. “Important issues in knowledge representation,” Proceedings of the Institute
of Electrical and Electronics Engineers, vol. 74, pp. 1322–1334.
Wu,C.F.J.,1983.“On the convergence properties of the EM algorithm,”Annals of Statistics, vol.11,
pp. 95–103.
Wylie, C.R., and L.C. Barrett, 1982. Advanced Engineering Mathematics, 5th ed., New York:
McGraw-Hill.
Xu, L., A. Krzyzak, and A. Yuille, 1994. “On radial basis function nets and kernel regression:
Statistical consistency, convergency rates, and receptive field size,” Neural Networks, vol. 7,
pp. 609–628.
Xu, L., E. Oja, and C.Y. Suen, 1992. “Modified Hebbian learning for curve and surface fitting,”
Neural Networks, vol. 5, pp. 441–457.
Yee,P.,and S.Haykin,2001.Regularized Radial Basis Function Networks:Theory and Applications,
New York: Wiley-Interscience.
Yockey, H.P., 1992. Information Theory and Molecular Biology, Cambridge, U.K.: Cambridge
University Press.
886
Bibliography

Yom-Tov, E., 2007. “A distributed sequential solver for large-scale SVMs.” In L. Bottou,
O. Chapelle, D. DeCosta, and J. Weston, editors, Large-Scale Kernel Machines, pp.139–154,
Cambridge: MIT Press.
Yoshizawa, S., M. Morita, and S.Amari, 1993.“Capacity of associative memory using a nonmonot-
onic neuron model,” Neural Networks, vol. 6, pp. 167–176.
Young, P.C., 1984. Recursive Estimation and Time-Series Analysis, Berlin and New York: Springer-
Verlag.
Younger,S.,S.Hockreiter,and P.Conwell,2001.“Meta-learning with backpropagation,”Proceedings
of the International Joint Conference on Neural Networks, pp. 2001–2006,Washington, DC.
Younger, S., P. Conwell, and N. Cotter, 1999.“Fixed-weight on-line learning,” IEEE Trans. Neural
Networks, vol. 10, pp. 272–283.
Zadeh, L.A., and C.A. Desoer, 1963. Linear System Theory:The State Space Approach, New York:
McGraw-Hill.
Zames, G., 1981.“Feedback and optimal sensitivity: Model reference transformations, multiplica-
tive seminorms, and approximate inverses,” IEEE Transactions on Automatic Control, vol.AC-
26, pp. 301–320.
Zames, G., and B.A. Francis, 1983.“Feedback, minimax, sensitivity, and optimal robustness,” IEEE
Transactions on Automatic Control, vol.AC-28, pp. 585–601.
Zhou, K., and J.C. Doyle, 1998. Essentials of Robust Control, Englewood Cliffs, NJ: Prentice Hall.
Bibliography
887

-insensitive loss function, 292–295
defined, 292
risk functional, 293
-insensitive tube, 292
A
Absolute error-correction procedure, 53
Absolute temperature, 106, 581
Acceptance ratio, 595
Accommodative learning, 832
Accuracy
approximation, 169
computational, 213
optimization, 213
Action potentials, 7
Activation function, 520
back-propagation algorithm, 135–137,
145–146
brain-state-in-a-box (BSB) model, 706
defined, 10–11
differentiability, 135
McCulloch-Pitts model, 14–15
nonlinear, 691
piecewise-linear, 706
sigmoid, 13, 14
threshold, 13–14
types, 13–14
Activation links
defined, 15
in signal-flow graphs, 15–16
Activation potential, 11
Active audition, 779
Active budget constraints, 213, 214, 218
large-scale problems, 215
small-scale problems, 214
Activity product rule, 370
Actor-critic architecture, 637
Adaptive behavior, 831–833
Adaptive control
automatic adjustment, 162
defined, 161–162
dynamic system, 162–163
generalization, 162
of learning rate, 161–164
Murata adaptive algorithm, 163–164
objectives, 162
as suboptimal, 164
Adaptive critic, 832–833
block diagram, 833
defined, 832
memory, 832–833
Adaptive equalizers, 214
Adaptive filters
adaptive process, 93
defined, 93
design, 95
feedback loop, 94
filtering process, 93
linear, 102
operation, 93
signal-flow graph, 92
Adaptive process
convergence phase, 435
defined, 430
ordering phase, 434–435
phases, 434–435
Adaptivity, 3–4
Adders, 10
Additive model, 686–687
Additive simulation noise, 658
Additive white Gaussian noise (AWGN),
411, 412
Adjancy criterion, 350
Affine transformation, 11, 12, 330
Algebraic distance, 272
Alignment condition, 699
Amacrine cells, 5
Ambient regularization parameter,
349, 360
Ambient space, 348
Amplitude modulation theory, 548
Annealing schedules, 595
Approximate Bayesian filtering, 757–759
Approximate dynamic programming,
642–643, 652–655. See also
Dynamic programming
direct, 642–643, 663
indirect, 652–653, 663–664
linear approach, 653–654
nonlinear approach, 654–655
summary, 663–664
Approximate policy iteration algorithm,
660–663
action generator, 662
block diagram, 660, 662
cost-to-go approximator, 662
defined, 660
least-squares solver, 662
policy evaluation step, 660–661
policy improvement step, 661
simulator, 661
summary, 662
Approximating function
bias, 84
partial derivatives, 153
variance, 85
Approximation
best, accuracy of, 169
BFGS, 200–201
curse of dimensionality and, 169–170
diagonal, 200
direct, 643
empirical fit, accuracy of, 169
extremal, 177
finite-time, 595
of functions, 166–171
Gaussian-sum, 735
indirect, 652–655
input space, 437–442
liner, 187
low-rank, 200
network pruning, 176–177
numerical, 763
outer-product, 198
properties, 168
quadratic, 177, 187
smoothness property, 168
stochastic, 184, 230
Taylor, 753
universal theorem, 167–168
Approximation errors, 85
bound computation, 215
bounds, 168–169
defined, 218
estimation of, 212
in lost performance, 211
vector, 379, 397
Arbitrary setting, 93
Architectural graphs, 17
Artificial neurons, 9, 10
Associative memory. See also
Memory
bipartite, 608
defined, 38
examples, 689
phases, 38
recurrent networks for, 673
undirected, 606
Atlases, 346–347
Attentional neurocomputers, 45
Attentional selectivity, 44
Attractors, 684–685
control over locations, 689
defined, 684–685
fixed points, 697
hyperbolic, 685
Lorenz, 719–721
Mackay-Glass, 829–831
point, 685
as recurrent network paradigm, 689
spurious, 703
strange, 711–716
Attribute codes, 448
Auditory encoding, 547–549
carrier, 548
envelope, 548
questions, 548
Autoassociation, 38
BSB model, 710–711
defined, 38
Autonomous systems, 389
Autoregressive (AR)
coefficients, 32, 453
first-order models, 451–454
models, 32–33
process, 111
time series, 453
Average correlations, 607
Average energy, 581
Average operator, 83
Averaged-squared error,
337–339
Axons, 7
Index
888

Index
889
B
Back propagation, 153–155
Back-propagation algorithm, 129–141,
180–186
activation function, 135–137, 145–146
backward computation, 141
backward pass, 135
batch methods, 217
computational efficiency, 180, 183
connectionism, 181
connectionist paradigm, 181
convergence, 139, 184–185
correction, 130
efficient method, 217
formula, 133
forward computation, 140–141
forward pass, 134–135
functional approximation, 183
heuristics, 144–150
information content, maximizing, 145
initialization, 140, 148–149
inputs, normalizing, 146–148
iteration, 141
learning from hints, 149
learning rate, 131, 137–139, 149–150
local computation, 181
local method, 217
local minima, 185
neuron is a hidden node, 131–134
neuron is an output node, 131
performance, improving, 144–150
presentation of training examples, 140
properties, 181
replicator (identity) mapping, 181–183
robustness, 184
scaling, 185–186
sensitivity analysis, 184
sensitivity factor, 130
sequential updating of weights,
139–141
signal-flow graphical summary, 140
stabilizing effect, 138
stochastic, 217
stochastic versus batch update, 144–145
stopping criteria, 139
summary, 139–141
target values, 146
Back-propagation-through-time (BPTT)
algorithm, 807, 808–812. See also
Learning algorithms
defined, 807, 808
epochwise, 809–810
ordered derivatives, 811–812
real-life applications, 811
truncated, 810–811
Backward computation, 141
Backward phase, 124
Barlow’s hypothesis, 499
Basin (domain) of attraction, 685
Batch learning, 127–128
advantages, 128
kernel PCA, 401
storage requirements, 128
Batch perceptron algorithm, 62–65, 65
Batch self-organizing maps (SOMs), 442
Bayes classifier
average risk, 55
block diagram, 57–58
classification error and, 60
defined, 55
formulation, 57
for Gaussian distribution, 58–60
as linear classifier, 59
minimum average risk, 56
parametric, 60
perceptron relation, 55–60
for two-class problem, 58–59
Bayes estimator, 756
Bayes theorem, 756
Bayesian estimation framework, 736
Bayesian filtering, 755–759
approximate, 757–759
derivation, 755
direct numerical approximation of pos-
terior, 758, 780
illustration, 757
indirect numerical approximation of
posterior, 758–759, 780–781
summary, 780–781
Bayesian paradigm, 88
Bayesian theory, 69
Beamformers, 43–45
defined, 43
neuro, 45
Behavior policy, 648
Belief nets
deep, 606–610
directed, 604
logistic, 604–605
Bellman’s optimality equation, 633–635
Bernoulli trials, 236, 585
Besov space, 411
Best-matching neurons, 430
Bias, 11
average value of approximating 
function, 84
estimate computation, 295
externally applied, 49
synaptic weights and, 50, 133
systematic, 156
vector, 699
Bias-variance dilemma, 84–86
defined, 85
local receptive fields, 85
summary, 88
weight sharing, 85
Bipyramidal effect, 202
Bits, 478
Bivariate data set, 381
Blahut-Arimoto algorithm
alternating projections, 550
defined, 558
for rate distortion theory, 553
Blind source separation (BSS)
invertibility, 532
learning algorithm, 523, 529, 532–533
maximum-entropy learning for,
529–533
problem, 509, 562
Block diagrams
adaptive critic, 833
agent interacting with environ-
ment, 629
approximate policy iteration algorithm,
660, 662
Bayes classifier, 57–58
brain-state-in-a-box (BSB) model, 705
decoupled extended Kalman filter 
(DEKF), 827
defined, 15
feedback control system, 42–43
generalized sidelobe canceller, 44
invariant-feature-space system, 31
inverse system modeling, 42
learning with a teacher, 35
maximum-entropy principle for 
BSS, 529
neural processor, 506
policy iteration algorithm, 636
reinforcement learning, 36
state-space model, 793
system identification, 41
unsupervised learning, 37
Boltzmann factor, 581
Boltzmann learning
goal, 602, 603
learning-rate parameter, 603
maximum-likelihood principle, 602
rule, 601–604
simplicity, 604
Boltzmann machine, 598–604, 618
defined, 580, 598
energy for, 599
Gibbs sampling in, 598, 599–601
goal, 599
hidden neurons, 598
negative phase, 602
positive phase, 602
simulated annealing for, 599–601
as stochastic machine, 601
stochastic neurons, 598
visible neurons, 598
Boltzmann’s constant, 106, 581
Bottleneck vectors, 555
Bottom-up learning, 414
Bounded, one-sided saturated (BOSS) 
function, 805
Bounded-input-bounded-output (BIBO)
stability criterion, 673
Brain, 6–10
anatomical organizations, 7, 9
cytoarchitectural map, 10
energetic efficiency, 6
neural (nerve) net, 6
neural microcircuits, 7
neurons, 6, 7
synapses, 6, 7
Brain-state-in-a-box (BSB) model, 689,
705–711
activation function, 706
autoassociation, 710–711
block diagram, 705
clustering, 709, 722
Cohen-Grossberg theorem and, 707
defined, 705

Brain-state-in-a-box (BSB) model (Contd.)
dynamics, 708–709
as energy-minimizing network, 709
equilibrium states, 708
feedback factor, 705
Lyapunov function, 706–708
state vector, 706
symmetric-weight matrix, 705
two-neuron, 710
Brent’s method, 194
Brownian motion, 110
characterization of, 106–107
Wiener solution, 106
Broyden-Fletcher-Goldfarb-Shanno
(BFGS), 196, 200–201
Budget constraints
active, 213, 214, 218
defined, 213
C
Canonical correlation analysis (CCA), 551
Imax relationship, 504
objective, 504
Canonical distribution, 581
Cauchy sequences, 299
bounded, 300
convergent, 300
defined, 297
Cauchy-Schwarz inequality, 52, 300
Cells
amacrine, 5
horizontal, 5
inter-plexiform, 5
pyramidal, 7, 8
Central nervous system, topographical
maps and, 9
Central-difference Kalman filter (CDKF),
829, 830. See also Kalman filters
defined, 829
derivative-free, 835
Cerebellum, dynamic models, 779
Channel capacity, 500
Chaotic processes, 711–716
dynamic reconstruction, 716–721
horizon of predictability, 716
Chapman-Kolmogorov identity, 584
Charts, 347
Chigirev-Bialek approach, 553
Chi-squared distribution, 455
Cholesky factor, 744
Cholesky factorization, 744, 745
Classes, 467
Closed-loop feedback system, 824, 825
Clustering
anatomical connectivity, 722
BSB model, 709
constrained, optimization, 612
defined, 242
deterministic annealing for, 610–615
functional connectivity, 722
K-means, 242–245
as nonconvex-optimization 
problem, 610
unsupervised, 709
of various phases, 614
Coefficient vectors, 403
Cohen-Grossberg theorem, 672, 703–704
BSB model and, 707
defined, 704
Hopfield model as case of, 704
Coherence, across space, 501
Coherent independent-component analy-
sis (ICA), 541–549, 576
algorithm, 546, 547
auditory encoding, 547–549
cost function, 543–545
coupled-network layout, 542
Imax principle contributions, 543
Infomax principle contributions,
542–543
Infomax/Imax roles, 541
joint entropies and, 543
performing, 547
results, 548–549
summary, 563
weight-sharing constraint, 547
Coherent regions, 447
Combinational optimization
problems, 723
simulated annealing for, 596
Compact coding, 407
Competition
cooperation and, 371
as self-organization principle, 370–371
self-organizing maps (SOMs), 429,
430–431
Competitive learning
defined, 425
framework, 460
process, 371
Complete-data log-likelihood 
function, 616
Complex conjugation, 32
Complexity penalty, 176, 317
Complexity regularization, 175–176
Computational accuracy, 213
Computational auditory scene analysis 
(CASA), 778
Computational efficiency, 756
Computational errors, 216
Computational tractability, 652
Computationally efficient 
algorithms, 183
Condition number, 328
Conditional differential entropy, 484
Conditional entropy, 612
Conjugate-gradient method, 188–194
Fletcher-Reeves formula, 191, 192
interpretation of A-conjugate vectors,
189–190
line search, 190, 192
Polak-Ribiére method, 191, 192
quadratic function minimization, 188
successive direction vectors, 191
summary, 194, 195
Conjunctional synapse, 369
Connectionist paradigm, 181
Consistent estimators, 256
Constrained-optimization problem,
481–482
Content-addressable memory, 722–723.
See also Memory
discrete Hopfield model as, 695–702
as error correcting, 696
essence, 697
primary function, 695
retrieving stored pattern, 696
Context units, 794
Contextual information, 4
Contextual maps, 447–450
computation, 467
defined, 449
illustrated, 450
Continuous functions, 168
Continuous Hopfield network, 693,
694–695
Continuous training, 806
Contrast function, 519
Control, 42–44
feedback system, 42–43
plant, 42
Controllability, 799–800
defined, 799
local, 801–802
matrix, 801
Convergence phase (adaptive 
process), 435
Convergence theorem, 650
Convex optimization, 272
Convolutional networks, 201–203
constraints, 201
defined, 30, 201
for image processing, 202
parallel implementation, 203
weight sharing and, 203
Convolutional sum, 30
Cooperation
basis, 430
neuron, 430, 431
as self-organization principle, 371
self-organizing maps (SOMs), 431–433
Cooperative polarization-twisting reflec-
tor, 505
Copulas, 489–493
defined, 490
differentiation, 491
joint density expression, 490
limiting values of, 490
mutual information relationship,
492–493
properties, 490–491
Sklar’s theorem, 490
of statistically independent random
variables, 491–492
Correlation, 28
coefficient, 453, 503
dimension, 714
Correlation matrix, 332–333
eigenstructure, 375
eigenvalues, 380, 403
eigenvectors, 380
instantaneous value, 386
m-by-m matrix as, 374
time-averages M-by-M, 75
Correlational synapse, 369
890
Index

Index
891
Cost function
coherent ICA, 543–545
empirical, 360
regularized, 314
single, 466
Cost-to-go function, 36, 637, 638
Covariance matrix, 27
computation, 756
cross, 764
filtering-error, 741
Gaussian random vector, 534
innovations process and, 737–738
inverse, 27
normalized form, 544
prediction-error, 738, 740–742, 764
state-error, 248
of zero-mean innovations process, 738
Cover’s theorem, 231–236
defined, 231
equation, 233
ingredients, 234
k-means algorithm and,
244–245
monomial, 232
for random patterns, 233
separable dichotomies, 233
separating capacity of a surface,
235–236
separating surface, 232
Credit-assignment problem, 126, 628
Cross-correlation vectors, 101, 245
Cross-covariance matrix, 764
Cross-validation, 171–176
defined, 172
early-stopping method, 173–175
generalized, 339–341
holdout method, 175
leave-one-out method, 175, 339
model selection, 172–173
multifold, 175–176
testing, 339
training samples, 172
use choice, 173
variants, 175–176
Cubature Kalman filter, 759–765. See also
Kalman filters
as approximate realization, 828
Baysian filter approximator, 828
derivation of, 763–765
as derivative free, 829
as derivative-free on-line sequential-
state estimator, 765
properties, 828–829
radial rule, 761–762
spherical rule, 760–761
spherical-radial integration 
conversion, 760
spherical-radial rule, 762–763
square-root (SCKF), 765
Cubature rule, 763
Curse of dimensionality, 169–170
defined, 652
reason for, 170
SVM, 304
Curve-fitting procedure, 193
D
Damping force, 106
Data compression, 413
dimensionality reduction as, 554–557
in feature mapping, 373
system, 183
Data reconstruction, 397
Data vectors, 377, 378, 379
Davidon-Fletcher-Powell (DFP) algo-
rithm, 196
Decision boundaries
illustrated, 144
orientation and position, 143
slope, 142
Decoders, optimum, 440
Decoupled extended Kalman filter
(DEKF), 827–828. See also
Extended Kalman filter (EKF)
block diagram representation, 827
computational requirements, 828
defined, 827
Decoupled first-order equation, 109
Deep belief nets, 606–610, 618
conclusions, 610
generative model, 608–609
hybrid learning process, 609–610
RBM, 606, 607, 608
training of, 607–608
Deflational method, 540
Degrees of freedom
normalized, 433
number of, 456
Delay
coordinate maps, 716
echo, 34
unit-time, 825
Delayed reinforcement, 36–37
defined, 36
difficulty, 37
Delay-embedding theorem, 716, 717
Demixers, 508
marginal mismatch, 527–528
weight vector, 527
Demixing matrix
column vector, 519
in ICA, 518
incremental adjustment, 520
update, 521
Dendrites, 7
Dendritic trees, 7
Density
matching, 442–444
observation, 71
posterior, 71, 72
sampling, 170
Desired-response vectors, 333, 337
Detailed balance, principle of, 590–591
Deterministic annealing, 610–616
algorithm, 613
analogy, 616–617
case study, 615–616
clustering via, 610–615
defined, 610
Deterministic finite-stat automated
(DFA), 797
Deterministic neurodynamics, 674
Deterministic rule, 698
Diagonal map, 529
Diagonal matrix, 530
Diagwonal approximation, 200
Diffeomorphism, 346, 348, 716, 802
Differentiability, 135
Differentiable manifolds, 342, 345–348
Differential entropy, 454–455
bounded, 529
conditional, 484
defined, 479
determination of, 518–519
of Gaussian random variable, 496
joint, 484, 504
of n-fold integral, 479–480
properties, 480–481
random vector, 531
uniform distribution, 480
Differential equations, 688
Differential manifolds, 675
Differential operator, 359
Digital image compression, 381–382
Dimensionality
curse of, 169–170, 304
input space, 93
reduction, 782
Dimensionality reduction, 373, 378–382
bivariate date set example, 380–381
defined, 378, 446
digital image compression example,
381–382
linear method of, 401
reconstruction of data, 413
representation of data, 412–413
summary, 412–414
Dirac delta distribution, 317, 319, 322
Dirac delta function, 441, 557, 770
Direct approximation, 643, 663
Direct learning, 43
Direct method of Lyapunov, 674
Direct problem, 314
Directed belief nets, 604–605
Directed graphs
architectural graph, 17
block diagram, 15
complete, 17
neural networks as, 15–18
partially complete, 17
signal-flow, 15–17
Discrete approximation, 557
Discrete Hopfield model, 694–702
as content-addressable memory,
695–702
retrieval phase, 699
stable state, 694–695
storage phase, 698–699
as vector projector, 702
weight matrix, 702
Discrete-time memory structure, 204
Dissipative systems, 684
Divergence phenomenon, 744
Divergence theorem, 677–678
Doppler frequency, 33
Doppler invariance, 33

892
Index
Dot product, 26
Double-moon classification problem,
60–62. See also Pattern
classification
distance, 63, 64
experiment, 62
illustrated, 61
LapRLS, 357, 358
specifications, 61
Dual problem, 274
Duality theorem, 274
Dynamic models
of cerebellum and motor learning, 779
of sound-stream segregation,
778–779
of visual recognition, 777–778
Dynamic programming, 627–665
algorithm, 632–633
approximate, 642–643, 652–655
approximate policy iteration algorithm,
660–663
background theory, 663
complexity, 629
for credit assignment problem, 628
defined, 628
direct approximation, 643
finite-horizon problem and, 633–634
indirect approximation, 652–655
least-squares policy evaluation
(LSPE(?)), 655–660
Markov decision processes, 628,
629–631
optimization and, 665
partial observability, 664
principle of optimality, 631
Q-learning, 648–649
reformulation, 634
summary, 663–665
Viterbi algorithm relationship, 664–665
Dynamic reconstruction, 716–721
defined, 716
delay-embedding theorem, 716, 717
embedding, 717
embedding dimension, 717
feedback system, 718
as ill-posed filtering problem, 719
of Lorenz attractor, 719–721
reconstruction vector, 718
recursive prediction, 717–718
Dynamic rule, 699
Dynamic systems, 674–678
controllability, 799–802
defined, 675
divergence theorem, 677–678
linear, 732
Lipschitz condition, 676–677
nonlinear, 798
observability, 799–800, 802–803
state estimation, 731–782
state space, 675–676
two-dimensional state portrait, 676
two-dimensional trajectory, 675
two-dimensional vector field, 677
Dynamically driven recurrent networks,
790–839
E
Early-stopping method, 173–175
Echo
amplitude, 34
delay, 34
frequency, 33
Eigenanalyzer, 397
Eigendecomposition, 377
Eigenspectrum, 351
Eigenstructure, correlation matrix, 375
Eigenvalues, 376, 679
composition factors, 156
defined, 283
Gram matrix, 403
largest, 391
problem, 376
spread, 115
Eigenvectors
complete orthonormal set of, 387
defined, 376
in feature space, 403
normalized, 397
problems, 551
Embedding, 717
delay, 716, 717
dimension, 717
Encoder-decoder model
correspondence, 441
illustrated, 438
noisy, 440
two-stage results, 452
Encoders, 242
Energy function
of BSB model, 706–708
discrete Hopfield model, 695
Hopfield network, 691
minima, 693
as monotonically decreasing, 693
Energy minimization, 689
Energy-minimizing networks, 709
Ensemble averaging, 128
Ensemble-averaged learning curve, 111, 158
Ensembled-averaged dynamics, 158, 162
Ensembled-averaged matrix, 159
Entropy, 477–481
absolute, 479
bounds, 478
conditional, 612
defined, 478
demixing matrix independence, 519
differential, 454–455, 479–481
joint, 454
marginal, 488
maximum, principle,, 481–483
source, 612
system, 582
Epochwise back-propagation-through-
time (BPTT) algorithm, 809–810
Epochwise training, 806
Equation-error method, 817
Equilibrium point, 538
Equilibrium states, 678–684
of BSB model, 708
defined, 678
stability of, 678–684
Equipartition law of thermodynamics, 106
Ergodic environment, 101
Ergodic Markov chains, 586–587
Ergodicity, 586
invoking, 538
theorem, 587
Error energy, 127
Error signals, 125
back propagation of, 134
defined, 35
learning-rate parameter, 55
origination, 125
Error surface, 35–36
Error vectors
approximation, 379, 397
mean-square value, 412
orthogonal, 379
Error-correction learning, 35
multilayer perceptrons, 126
perceptron convergence algorithm, 55
perceptrons, 50
Error-performance surface, 35
Estimation errors, 85, 169
defined, 218
in lost performance, 211
prior, 246
variations of, 212
Estimation policy, 648
Euclidean distance, 26, 159
in deterministic terms, 27
Mahalanobis distance reduction, 28
minimization of, 27
squared, 297
Euclidean norm, 392
of difference vectors, 324
unknown parameter vector, 74–75
Euclidean space, 26, 675
Euler-Lagrange equation, 320, 322
Evidential response, 4
Example-by-example basis, 128
Excess weights, 176
Excitatory-inhibitory network, 724
Exclusive-OR (XOR) problem,
141–144
architectural graph for solving, 143
decision boundaries, 142, 143, 144
defined, 141
hidden functions, 235
output neuron, 143
polynomial machine for solving, 288
separability of patterns and, 234–235
single-layer perceptron and, 142
solving, 142–144
support vector machines, 286–288
unit square, 142
Expansion coefficients, 322
defined by training sample, 333
determining, 323–325
vector of, 334
Expectation maximization (EM) algo-
rithm, 735
Expectational error
defined, 69
in finite sample-size considerations, 83
variance, 84

Index
893
Expectation-maximization (EM)
algorithm, 616–617
defined, 616
deterministic annealing and, 616–617
for parameter estimation, 616
Expected cost, immediate, 634
Expected distortion, 438, 554
Explanational error, 114
Extended Kalman filter (EKF)
algorithm, 826
approximate state-space model, 753
averaged trajectory, 775
comparative evaluation, 775–777
decoupled, 827–828
defined, 751
derivation, 753
ensemble-averaged state estimate, 776
new matrices construction, 752
preliminary steps, 751–752
reformulation, 751
state-space model linearization,
752, 753
summarizing remarks, 828
summary, 754
supervised-training framework using,
823–826
Extremal approximation, 177
F
Factorial distribution, 488, 517
FastICA algorithm
basic learning rule, 535–539
defined, 534
independent component computation,
541
multiunit version, 539
Newton-based learning rule, 538
non-Gaussianity, 534
properties, 541
single-unit deflation, 541
Fault tolerance, 4
Fault-tolerant network design, 181
Feature detectors, 126, 371
Feature extraction, 39, 201
Feature maps
approximation of input space, 437–442
computation, 437
defined, 437
density matching, 442–444
as elastic or virtual net, 442
feature selection, 444–445
input space display, 442
labeled neurons, 449
magnification factor, 443
properties, 437–445
self-organized, 427
synaptic-weight vector representation,
437
topological ordering, 442, 445
two-dimensional, 443
weight vector relationship, 438
Feature selection, 373, 444–445
Feature space, 40
defined, 126, 232
dimension, 373
eigenvectors in, 403
kernel PCA linearity, 404
Feature vectors, 281
Feature-mapping models, 201, 426–428
Kohonen model, 427
Willshaw-von der Malsburg model, 428
Feedback
factor, 705
global, 673
local, 673
in visual recognition modeling, 778
Feedback loops
adaptive filter, 94
closed, 824, 825
multiple, 690
presence of, 23
single, 18–19
Feedback system, 18–21
closed-loop, 824, 825
control, 42–43
double loop, 743
dynamic behavior, 19
dynamic reconstruction, 718
multiple-loop, 690
single-loop, 18–19
stochastic, 104
unfolding, 20
Feedforward networks
convolutional, 30
GHA, 401
kernel PCA, 401
multilayer, 22–23
single-layer, 21, 393
Feedforward signal-flow graph, 19
Filtering-error covariance matrix, 741
Finite sample size, 82–86
Finite-duration impulse response (FIR),
209
Finite-horizon problem, 631
dynamic programming and,
633–634
infinite-horizon problem link, 634
Finite-state automata, 804
Finite-time approximation, 595
First-order autoregressive (AR) model,
451–454
First-order neurons, 795
First-order Taylor approximation, 753
Fixed-increment adaptation rule, 51
Fixed-increment convergence theorem, 53
Fletcher-Reeves formula, 191, 192
Fluctuating force, 106
Forgetting factor, 385
Forgetting term, 433
Forward computation, 140
Forward phase, 123
Four-tuple samples, 649
Fractal dimensions, 712–714
Fréchet differential
defined, 318
evaluating, 318–319
of Tikhonov functional, 318–319
Free energy, 581, 582
Frobenius norm, 200
Functional approximation, 40–42
Functional signals, 125
Fundamental memories, 698
G
Gain
Kalman, 739–740, 764, 823
neuron, 691
unity, 691
Gain vector, 248
Gamma function, 456, 761
Gamma memory, 206
Gaussian distribution
Bayes classifier for, 58–60
generalized, 544, 545
input, 457
mixture, 615–616
multidimensional, 483
multivariate, 258–259
one-dimensional, 482–483
overlapping, one-dimensional, 60
smoothing parameter, 259
of two patterns, 59
zero-mean generalized bivariate, 544
Gaussian functions, 238
interpretation as kernel, 255
multivariate, 325–326
neighborhood function use, 462
translation invariant, 431
Gaussian hidden units, 254
Gaussian information bottleneck, 551–552
Gaussian neighborhood function, 432
Gaussian noise model, 498
Gaussian quadrature, 761
Gaussian random variables
differential entropy of, 496
independent, 496, 502
Gaussian-sum approximation, 735
Gauss-Newton method, 98–100
convergence, 101
defined, 98
implementation, 100
loading, 100
pure form, 99
Generalization, 164–166
adaptive control, 162
defined, 2, 25, 164
error, 172
GHA performance, 401
good versus poor, 165
Imax, 506
Imin, 506
learning-rate parameter, 396
memorization and, 165
multilayer perceptrons, 164–166
of mutual information, 486
nonlinear mapping with, 165
of on-line learning algorithm, 162
performance, 286
representer theorem, 313, 352–353
sufficient training-sample size for, 166
Generalized cross-validation (GCV),
339–341
computation, 341
defined, 340
optimal property, 341

894
Index
Generalized delta rule, 137
Generalized Gauss-Laguerre formula, 761
Generalized Hebbian algorithm (GHA),
393–398
assumptions, 393
behavior, 393
computations, 398
convergence considerations, 396
as eigenanalyzer, 397
as equilibrium, 397
feedforward network, 401
generalization performance, 401
heuristic understanding, 395
imaging coding example, 398–401
kernel PCA comparison, 401–402
kernelizing, 414
as on-line learning algorithm, 401
optimality of, 397–398
signal-flow graph, 394
summary of, 398
synaptic weights, 400
Generalized Lloyd algorithm, 439, 441, 450
Generalized RBF networks, 239–242,
327–331, 654. See also Radial-
basis function (RBF) networks
basis function choice, 329
framework, 331
hidden layer nodes, 331
linear weights, 331
regularization RBF network versus, 331
weighted norm, 330–331
Generalized regularization 
theory, 348–350. See also
Regularization theory
defined, 348
penalty terms, 349
Generalized representer theorem,
352–353
defined, 352
reproducing property, 353
simple form, 353–354
validity, 353
Generative model, 248, 508
deep belief net, 608–609
defined, 79
Gibbs distribution, 581, 593, 596, 612, 613
Gibbs sampling, 596–598, 618
for Boltzmann machine, 598, 599–601
defined, 596
in directed belief nets, 605
as iterative adaptive scheme, 597
joint distribution, 601
theorems, 597–598
Givens rotations, 747–749
Global features, 171
Global feedback, 673
Global system, 521
Gradient matrix, 521, 815
Gradient operators, 65, 95
Gradient vectors, 62
cost function, 95
instantaneous, 158
instantaneous estimate, 103
Gradient-based learning algorithms,
818, 836
Gram matrix, 298, 403
defined, 283
dense, inversion, 360
eigenvalues, 403
of kernel function, 353
N-by-N, 304
negative definite, 299
Gram-Schmidt orthogonalization, 395,
540–541
Greedy policy, 636, 651
Green’s function
classes, 324
defined, 321
for linear differential operator, 321
positive definite, 327
as radial-basis function, 324
as symmetric function, 323
Green’s identity, 320
Green’s matrix, 323
H
Hadamard’s conditions for 
well-posedness, 314–315
Handwritten-digit recognition 
problem, 25
Hard-limiting, 391
Heaviside function, 13, 713
Hebbian learning, 368–370
Hebbian synapse, 369
Hebbian-based maximum eigenfilter,
383–392
asymptotic stability of, 387–391
derivation of, 384–385
Kushner’s direct-averaging method,
386–387
matched filter example, 392
matrix formulation, 386
properties summary, 391–392
signal-flow graph, 383
Hebbian-based PCA, 392–398
Hebb’s postulate of learning, 698
Hessian
defined, 155
eigenstructure, 155
inverse, 155
inverse computing, 178–180
local, 187
on-line learning role, 155–157
outer-product approximation, 198
positive-definite matrix, 159
Hessian-based network pruning, 176–180
Heteroassociation, 38
Heterodyning, 33
Heterogeneous sequences, 837
Heuristic reinforcement signal, 36
Hidden functions
defined, 232
in XOR problem, 235
Hidden layers, 22
generalized RBF networks, 331
multilayer perceptrons, 153
RBF networks, 231, 239
redundancy of neurons, 240
Hidden neurons. See also Neurons
back-propagation algorithm, 131–134
Boltzmann machine, 599
defined, 22
feature detectors, 126
RBM, 606
recurrent network with, 24
Hidden units, 231
Gaussian, 254
interpretations of, 252–255
receptive field of, 254
Hierarchical vector quantization, 450–454
defined, 451
distortion, 453
first-order AR model, 451–454
two-stage, 452
Hilbert space, 296–297, 316
obtaining, 300
pre, 296–297
reproducing-kernel, 297–300
Hints, learning from, 149
Holdout method, 175
Homeomorphism, 346, 347
Homogeneous sequences, 837
Hopfield network, 689, 690–703
architectural graph, 690, 701
Cohen-Grossberg theorem and, 704
computational problems, 722–723
as content-addressable memory,
695–702
continuous, 693, 694–695
defined, 690
discrete, 694–702
dynamics, 690, 691
emergent behavior example, 699–702
emergent property, 697
energy (Lyapunov) function, 691
error correcting capability, 702
fundamental memories, 703
multiple-loop feedback system, 690
neurons, 697
retrieval phase, 699
spurious states, 702–703
stable states, 694–695
storage phase, 698–699
summary, 700
Horizontal cells, 5
Hotelling’s deflation technique, 395
Hybrid learning procedure, 249–250
Hyperbolic attractors, 685
Hyperbolic tangent function, 136–137
defined, 14
general form, 136
graph, 146
properties, 145
use of, 148
Hyperplanes, 49, 232
margin of separation, 269, 272
optimal, 269–281, 288
soft margin, 277
SVM construction, 269
Hyperspheres, 232
Hypersurface, 237
I
Identity mapping, 181–183
Identity matrix, 332, 386, 521

Index
895
Ill conditioned training problem, 155
Ill conditioning, 328
Ill-posed problems, 315
Image denoising, 414
application, 413
multipatch, 409–412
Image-coding problem, 398–401
Imax principle
canonical correlation analysis 
relationship, 504
coherent ICA and, 543
defined, 501–502
examples, 502–503
formulation, 503–504
generalization, 506
neighboring region processing, 502
summary, 561
Imin principle
defined, 504
generalization, 506
radar polarimetry, 505–506
summary, 561
Immediate expected cost, 634
Importance distribution
computation, 774–775
optimal choice, 774–775
state space, 774
Importance sampling
method, 766
sequential (SIS), 768–770
Importance weights, 767
Incomplete gamma distribution, 455,
456, 458
Incomplete-data log-likelihood 
function, 616
Incremental correction factor, 770
Independent-components analysis (ICA),
475, 508–514, 518
algorithm classification, 512–514
coherent, 476, 541–549
decomposition of random vector, 512
demixing matrix, 518
equivariant property, 521–522
Infomax algorithm for, 515
learning algorithm for, 518
in maximization of non-Gaussianity,
514
maximum-likelihood estimation,
526–529
in minimization of mutual information,
512
natural-gradient learning algorithm,
522
negentropy maximization for, 534–541
non-Gaussianity of sources, 510–512
as PCA extension, 509
principle, 562
principle, maximum-likelihood 
estimation relationship, 527–529
robustness theorem, 523–526
sparse coding comparison, 514–516
stochastic gradient algorithm for,
519–521
summary, 561–563
Indicator function, 278
Indirect approximation, 652–655, 663–664
Indirect learning, 43
Infinite-duration impulse response (IIR),
209
Infinite-horizon problem, 631, 634
Influence function. See Green’s function
Infomax. See Maximum information 
principle (Infomax)
Information bottleneck method, 549
bottleneck random vector, 550, 551
defined, 550
equations, 553
formulation, 551
Gaussian information bottleneck
example, 551–552
illustrated, 553
relevant random vector, 550, 551
summary, 563–564
Information curve
concave, 551
for multivariate Gaussian variables, 552
Information latching, 819
Information-theoretic learning, 549
models, 475–564
pattern classification, 560–561
Inner product, 26, 299
scalar, 319
space, 296, 297
vectors, 70
Inner-product kernel, 269, 281–282
defined, 282
properties, 282
Innovations process, 736–739, 823
covariance matrix, 737–738
defined, 736
filter state estimation with, 738–739
zero-mean, 738
Input
back-propagation algorithm,
146–148
dimensionality, 93
nonzero-mean, avoidance, 156
Input space, 343, 797
approximation, 437–442
display, 442
Input variables
covariances, 147
decorrelated, 147
normalizing, 146–148
training set, 147
Input-output examples, 34, 36
Input-output mapping, 3, 791
approximating, 41–42
continuous mapping and, 167
nonlinear, 166
of normalized RBF network, 258
Occam’s razor and, 165–166
recurrent networks for, 673
sensitivity of, 184
smooth, 315
Instantaneous error energy, 127
Instrumental distribution, 766
Instrumental-variables method, 86–88
Integrated squared error, 168
Inter-plexiform cells, defined, 5
Interpolation
matrix, 238
Micchelli’s theorem, 238–239
multivariable, 237
problem, 236–239
strict, 325
Interregional circuits, 8
Intrinsic error, 84
Intrinsic regularization parameter, 349, 360
Invariances, 28
building into neural network design,
30–31
Doppler, 33
feature space, 31
by structure, 30–31
by training, 31
Invariant feature space, 31
Inverse modeling, 41–42
Inverse multiquadratics, 238, 239
Inverse nonlinearity, 530
Inverse parabolic interpolation, 193, 194
Inverse problem, 314
Irreducible Markov chains, 585–586
Iterated Laplacian operator, 325
Iterative descent algorithm, 243–244
J
Jacobian matrix, 43
defined, 155
n-by-m, 99
rank deficient, 155
Joint entropy, 454
coherent ICA and, 543
differential, 484
Joint estimation, 529
Joint posterior distribution, 766
Joint probability density function, 484
K
Kalman filters, 736–743
in brain function modeling, 777–780
central-difference (CDKF), 829
computational considerations, 781
covariance algorithm, 742
cubature, 759–765
as double loop feedback system, 743
dynamic nature, 779
extended, 750–754
filtered estimate of state, 743
innovations process, 736–739
linear, theory, 764
predicator-corrector formula, 739,
781, 825
prediction-error covariance matrix,
738, 740–742
reformulation, 751
signal-flow graph, 743
square-root implementation,
745–747
state-space model for, 736
summary, 742–743
theory, 780
as time-varying filter, 743
unstable behavior, 744
variables and parameters, 742

896
Index
Kalman gain, 823
computation of, 739–740, 764
defined, 739
Karhunen-Loève transformation. See
Principal-components analysis
(PCA)
Karush-Kuhn-Tucker conditions, 274, 294,
295, 536
K-by-K correlation function, 245
Kernel Hebbian algorithm (KHA),
407–412
case study, 409–412
defined, 408
denoising of multipatch image, 409–412
derivation of, 408–409
implementation of, 409
in large-scale learning problems, 408
local convergence, 409
as on-line unsupervised learning 
algorithm, 412
Kernel machine
defined, 282
SVM viewed as, 281–283
Kernel matrix, 298, 403
defined, 283
N-by-N, 304
Kernel methods, 302
computation effectiveness, 414
defined, 231, 269
Kernel PCA. See also Principal-
components analysis (PCA)
as batch-learning algorithm, 401, 408
defined, 401, 402
derivation of, 402–404
example, 405–406
feedforward network, 401
GHA comparison, 401–402
illustrated, 404
summary of, 405
two-dimensional, 406
Kernel self-organizing map. See also
Self-organizing maps (SOMs)
adaptation, 461–462
automatic kernel adjustment, 467
defined, 454
first update formula, 459
initialization, 461
joint maximization of objective 
function, 460
kernel definition, 455–457
Kullback-Leibler divergence relation-
ship, 464–465
lattice dynamics with, 462–463
learning algorithm for map formation,
457–460
objective function, 454–455
as on-line stochastic-gradient-based
algorithm, 467
sampling, 461
second update formula, 460
similarity matching, 461
steps, 461–462
summary, 461–462, 467–468
topographic map formation, 460–461
tuning, 468
Kernel trick, 282–283
Kernelization, 414–415
Kernels
availability, 256
continuous symmetric, 283
definition of, 455–457
differential entropy, 455
Gaussian function interpretation 
of, 255
inner-product, 269, 281–282
Mahalanobis, 331
Mercer, 283, 284, 298, 402
multivariate Gaussian distribution,
258–259
positive definite, 283
regression, 255–259
reproducing, 299
row vector of, 334
Key pattern, 38
Kirchoff’s current law, 687
K-means algorithm, 242–245
computational efficiency, 244, 245
in Cover’s Theorem framework,
244–245
defined, 242, 243
initializing, 244
linear transformation, 244
steps, 244
Knowledge, 24–25
Knowledge representation, 24–34
characteristics, 24
free parameters, 26
importance, 28
input similarity, 26–28
invariances, 28, 30–31
prior information, 24, 28, 29–30
roles of, 26–29
rules, 26–29
Kohonen model, 427
Kolmogorov complexity theory, 79–80
Kronecker delta, 707, 817
Kullback-Leibler divergence (KLD),
527, 534
between two distributions, 558
decomposition, 563
defined, 464, 486
entropic interpretation, 487–488
in information-theoretic learning,
486–489
invariance, 487
kernel SOM relationship, 464–465
mutual information relationship, 487
nonnegativity, 487
partial derivatives, 464
properties, 487
Kushner’s direct-averaging method,
107–108, 117, 386–387
L
Labeled examples, 25, 45
Lagrange multipliers, 178, 287, 556
defined, 273
optimization problems solution, 293
in solving constrained-optimization 
problem, 482
support vectors for, 275
as tradeoff parameter, 551
Lagrangian function, 273
complexity of, 274
optimality condition, 273
Langevin equation, 106–107
convergence analysis, 388–391
defined, 106
modified, 388
of nonequilibrium thermodynamics,
117
nonlinear modified form, 388
without driving force, 388
Langevin force, 110
defined, 106
discrete-time version, 107, 109
Laplacian distribution, 524
Laplacian graph matrix, 355
Laplacian operator
iterated, 325
Rayleigh quotient, 351
Laplacian regularized least-squares
(LapRLS) algorithm, 354–355,
360
computational complexity, 360, 361
defined, 354
of double-moon, 357, 358
performance, 357
practical virtues, 360
reduction to RLS algorithm, 356
summary, 355
weight vector computation and, 355
Laplacian support vector machines
(LapSVMs), 360
Large-scale learning
defined, 214
optimization algorithm, 216, 217
optimization error, 215
problems, 215–217, 838–839
trade-offs, 215
Latent variables, 508
Lateral geniculate nucleus (LGN), 654,
777
Law of large numbers, 586
Leakage factor, 385
Learning
accommodative, 832
back-propagation, 180–186
batch, 127–128, 401
Boltzmann, 601–604
bottom-up, 414
competitive, 371, 425, 460
defined, 25
delayed-reinforcement, 36–37
direct, 43
error-correction, 35
features of features, 372, 608
future of, 838
Hebbian, 368–370
Hebb’s postulate, 698
from hints, 149
hybrid procedure, 249–250
indirect, 43
information-theoretic, 549
large-scale, 214, 215–217, 838–839

Index
897
of learning algorithm, 162, 164
manifolds for, 348
map formation algorithm, 457–460
motor, 779
on-line, 128–129
processes, 34–37
Q-learning, 648–652
reinforcement, 36–37, 45–46, 627
rules, formulation, 545–546
self-organized, 368
semisupervised, 45, 314, 356–359
small-scale, 213, 214
supervised, 3, 34–36, 45, 186–201,
627–628
tabula rasa, 3
temporal-difference, 643–648
time role in, 672
top-down, 414
unsupervised, 37, 45, 368
Learning algorithms
back-propagation-through-time
(BPTT), 807, 808–812
defined, 2
gradient-based, 818, 836
real-time recurrent learning (RTRL),
807, 812–818
recurrent network, 806–807
supervised, 836–837
unsupervised, 348
Learning curves
double-moon classification, 62
ensemble-averages, 111
estimation, 175
least-mean-square (LMS) algorithm,
110
on-line learning, 157
validation, 175
Learning machines, 343
Learning rate
adaptive control of, 161–164
back-propagation algorithm, 137–139,
149–150
determination, 370
optimal annealing of, 158–161
tuning parameters, 160
Learning tasks, 38–45
beamforming, 43–45
control, 42–43
function approximation, 40–42
pattern association, 38–39
pattern recognition, 39–40
Learning-rate annealing schedules,
115–117
defined, 115
illustrated, 116
search-then-coverage, 116, 117
Learning-rate parameter, 95, 96,
108, 433
back-propagation algorithm,
131, 138
Boltzmann learning, 603
generalized Hebbian algorithm, 396
Q-learning, 649, 650
statistical LMS learning theory for,
108–110
Least-mean-square (LMS) algorithm,
91–117, 815
classification, 112, 113
computational simplicity and efficiency,
113
condition number variation, 115
configuration, 102
convergence, 113, 115
defined, 91–92
development, 92
deviation from Wiener filter, 104–105
ensemble-average learning curve, 111
error signal, 102
explanation error, 114
filtering structure, 92–94
initial weight-error vector, 114
initialization, 103
instantaneous estimate, 103
learning curves, 110
learning-rate annealing schedules,
115–117
linear combiner, 92
memory, 103
model-independent behavior, 115
natural modes, 108–110
optimal manifold, 561
optimality, 115
performance-limiting factors, 115
robustness, 114–115
signal-flow graph representation, 104
small-learning-rate-parameter
theory, 111
statistical analysis, 108–110
statistical learning theory, 110
summary, 103, 117
supervised classification, 560
uses, 92
virtues and limitations, 113–115
Widrow’s rule of thumb, 166
Least-squares classification, 77–79
distance, 78, 79
finite sample-size considerations, 82–86
as linear, 78
Least-squares estimator
linearity, 332, 397
regularized, 331–334
representer theorem and, 333–334
as special case, 332–333
Least-squares filter
defined, 100
limiting form of, 101–102
Least-squares policy evaluation
(LSPE(
)), 655–660
approximate nature, 659
assumptions, 656
background, 656
convergence, 658, 660
defined, 655
illustrated, 659
mapping, 655
projected value iteration, 656–658
as PVI with additive simulation 
noise, 658
Leave-one-out method, 175
Left singular vectors, 200
Levenberg-Marquardt method, 197–199
approximate version, 198
defined, 197
efficiency, 198
iterations, 198
regularizing parameter, 198
Likelihood function, 71, 74
Likelihood ratio, 57
Line search, 190
algorithms, 193
illustrated, 193
purpose, 192
Linear adaptive filters, 102
Linear combiners
Bayes classifier, 59
defined, 10
LMS algorithm, 92
output, 11, 50, 59
Linear mapping, 236, 413
Linear prediction, 110–112
Linear regression models, 69–70, 108
defined, 69
illustrated, 70
model order, 70
normal equation, 76, 88
Linearly separable patterns, 48
Liner approximation, 187
Links
activation, 15, 16
directed, 15
synaptic, 15, 16
Linsker’s model, 372, 373
Lipschitz condition, 676–677
Lipschitz constant, 677
Lloyd algorithm, 439–440, 441, 450
Loading, 100
Local averaging, 201
Local circuits, 7–8
Local controllability, 801–802
Local features, 171
Local feedback, 673
Local gradients, 131
Local minima, 185
Local observability, 802–803
Locality constraint, 181
Logistic belief nets, 604–605
Logistic function, 14, 135–136
Log-likelihood function, 526, 527
complete-data, 616
incomplete-data, 616
maximizing, 527
Log-likelihood ratio test
defined, 58
for two-class problem, 59
Long memory, 107
Long-range inhibitory mechanism, 428
Long-term memory, 35, 832
Look-up tables, 643
Lorenz attractor
dynamic reconstruction of,
719–721
equations, 719–720
summary of parameters, 721
Low-rank approximation, 200
Lyapunov exponents, 714–715

898
Index
Lyapunov function, 389
of BSB model, 706–708
discrete Hopfield model, 695
Hopfield network, 691, 708
minima, 693
as monotonically decreasing, 693
Lyapunov surface, 683–684
Lyapunov’s theorems, 682–683
M
McCulloch-Pitts model, 47–48
activation function, 14–15
defined, 14
impact, 48
perceptron use, 65
Mackay-Glass attractor, 829–831
Magnification factor, 443, 463
Mahalanobis distance
defined, 27
reduction to Euclidean distance, 28
Mahalanobis kernel, 331
Mainstream training, 837–838
Manifold regularization. See also
Regularization
cluster assumption, 344
defined, 342
kernel-based, 342
manifold assumption, 344
preliminary considerations, 343–344
Manifold regularizer, 314
Manifolds
attractors, 684–685
defined, 554
differentiable, 345–348, 675
of dimension n, 345
intrinsic properties, 556
for learning, 348
lower dimensionality, 348
optimal representation, 553–560, 564
parameterization, 555
smoothness, 559
topographical, 347
Margin separation
defined, 269
optimum value, 272
Marginal entropy, 488
Marginal mismatch, 527
Markov chain Monte Carlo (MCMC), 591
Markov chains, 582–591, 630, 733
defined, 583
ergodic, 586–587
ergodicity theorem, 587
formation, 583
as generative model, 583
homogeneous in time, 583
irreducible, 585–586
periodicity, 585–586
principle of detailed balance, 590–591
recurrent properties, 584–585
specification, 584
state-transition diagrams, 588, 589
stationary distributions, 587–590
transition probabilities, 583–584
Markov decision processes,628,629–631,664
agent, 629
defined, 631
finite-discrete-time, 629
finite-horizon problem, 631
infinite-horizon problem, 631
partially observable (POMDPs), 664
Markov model, 107
LMS algorithm deviation, 104–105
modified, 109
signal-flow graph representation, 105
updated state, 105
Markov properties, 583, 629
Marquardt recipe, 198–199
Matched filter, 392
Matrix factorization lemma, 745
Matrix inversion lemma, 180, 247
Matrix transposition, 26, 69
Maximum a posteriori (MAP) 
estimation, 69
finite sample-size considerations, 82–86
parameter vector, 68, 71–76
procedure application, 72
regularized least-squares estimation, 76
relationship, 76–77
RLS estimator application, 332
Maximum eigenfilter
asymptotic stability, 387–391
derivation of, 384–385
Hebbian-based, 383–392
Kushner’s direct-averaging method,
386–387
matrix formulation, 386
properties summary, 391–392
signal-flow graph, 383
Maximum information principle 
(Infomax), 476, 529
application results, 517
coherent ICA and, 542–543
defined, 494–495
for ICA, 515
noiseless network, 498
redundancy reduction and, 499–501
scenarios, 494
single neuron corrupted by processing
noise, 495–496
summary, 561
Maximum-entropy principle, 481–483
BSS block diagram, 529
as constrained-optimization problem,
481
defined, 481
equivalence, 531–532
method of Lagrange multipliers, 482
Maximum-likelihood (ML) estimator, 72,
86, 335–336
regularized, 336
underlying structure, 336
Maximum-likelihood estimation
defined, 526
distribution-matching criterion, 528
equivalence, 531–532
ICA, 526–529
ICA principle relationship, 527–529
M-by-M demixing matrix, 508
M-by-M identity matrix, 75
Mean-square error, 35, 158
Memorization, 165
Memorized pattern, 38
Memory
associative, 38, 606
autoassociative, 38
discrete-time structure, 204
fading, 20
gamma, 206
heteroassociative, 38
impulse response, 205
infinite, 20
LMS algorithm, 103
long-term, 35, 107, 832
low depth, 205
order of, 205
resolution, 205
short-term, 204–206, 833
structures, 673
tapped-delay-line, 205
Mercer kernels, 283, 284,
298, 402
Mercer’s theorem, 283
Method of least squares, 69, 661
Method of steepest descent, 65,
95–96
defined, 95
overdamped, 96
trajectory, 97
underdamped, 96
Metropolis algorithm, 591–594, 617
alternative form, 598
defined, 591
statistical analysis, 591–592
transition probabilities, 592–594
M-h1-h2-q network, 22
Micchelli’s theorem, 238–239
Minimax, 114, 292
Minimum average risk, 56
Minimum mean-square error 
(MMSE), 756
Minimum redundancy principle, 561
Minimum-description-length (MDL)
principle, 79–82, 382–383, 413
attributes, 81–82
as consistent model selector 
estimator, 82
defined, 79
model-order selection, 81
Occam’s razor implementation, 81
simplistic two-part code, 80
Minimum-distortion encoding, 443
Mixers, 508
Mixing matrix, 508, 521
joint estimation, 529
nonsingular, 512
Model complexity-penalty function, 317
Model order
defined, 70
selection problem, 81
Model selection
cross-validation, 172–173
defined, 79
MDL principle for, 79
order, 81
problem, 382

Index
899
Model-reference control system, 833–835
bank of unit-time delays, 834
comparator, 834
defined, 833
illustrated, 834
model reference, 834
neurocontroller, 834
plant, 833, 834, 835
Momentum constant, 137
Monomials, 286
Monotonicity condition, 301
Monte Carlo integration, 766–768
Monte Carlo simulation, 642, 644–645, 658
defined, 645
of temporal differences, 645
Monte Carlo statistical methods, 766
Motor learning, 779
Multifold cross-validation, 175–176
Multilayer feedforward networks, 22–23
fully connected, 23
hidden layers, 22
partially connected, 23
Multilayer perceptrons, 122–217
approximation of functions, 166–171
architectural graph, 124
back propagation and differentiation,
153–155
back-propagation learning,180–186
backward phase, 124
basic signal flows, 125
batch learning, 127–128
complexity regularization, 175–176
computer experiment, 150–153
convolutional network, 201–203
credit-assignment problem, 126
cross-validation, 171–176
defined, 123
error signals, 125
error-correlation learning, 126
feature detectors, 126
forward phase, 123
free parameters, 154
functional signals, 125
generalization, 164–166
network pruning, 176–180
nonlinear filtering, 203–209
offset, 146
on-line learning, 128–129
pattern classification, 150–153
sigmoidal nonlinearity, 135
small-scale versus large-scale learning
problems, 209–217
specifications, 150
training, 123–124
two hidden layers,one output neuron,153
vertical separation, 150, 153
XOR problem, 141–144
Multiple input-multiple output (MIMO)
systems, 41, 563
Multiple system trajectories, 643
Multiquadratics, 238, 239
Multistage vector quantization, 451
Multivariable interpolation, 237
Multivariate Gaussian distribution,
258–259
Multivariate Gaussian functions, 325–326
Murata adaptive algorithm, 163–164
Mutual information, 484–486
copula entropy relationship, 492–493
defined, 484
generalization of, 486
gradient, 498
invariance, 485–486
Kullback-Leibler divergence (KLD)
relationship, 487
maximum principle, 494–498
minimization of, 512
nonnegativity, 485
as objective function, 493–494
as objective function of self-organiza-
tion,
561
statistical dependence, 489
summary, 561
symmetry, 485
uncertainty measurement, 499
N
Nadaraya-Watson regression estimator
(NWRE), 257
Nats, 477
Natural distribution, 712
Natural gradients, 533
Natural images
coding issues, 406–407
compact coding, 407
sparse coding of, 514–516
sparse-distributed coding, 407
Natural-gradient learning algorithm,
522–523
as computationally efficient, 523
defined, 522
virtues, 523
N-by-N diagonal matrix, 350
Nearest-neighbor encoding, 439, 443
Negative binomial distribution, 236
Negative examples, 25–26
Negentropy, 534–535
maximization for ICA, 534–541
maximum principle, 562
for non-Gaussianity measure, 514
Neighborhood function, 461
Nervous system, block diagram represen-
tation, 6
Nested sigmoidal scheme, 183
NETtalk
justification, 208
multilayer perceptron, 178
performance, 208
schematic diagram, 208
Network architectures
multilayer feedforward, 22–23
recurrent, 18, 23–24
restricting, 29
single-layer feedforward, 21
Network pruning, 176–180
approximations, 177–178
inverse Hessian computation,
178–180
Neural microcircuits, 7
Neural networks
adaptivity, 3–4
benefits of, 21–26
contextual information, 4
dynamic behavior, 21
evidential response, 4
fault tolerance, 4
fully connected, 23
input-output mapping, 3
neurobiological analogy, 4–6
nonlinearity, 2–3
overtrained, 164
partially connected, 23
performance, 2
specialized structures, 28–29
time in, 203–204
uniformity of analysis and design, 4
VLSI implementability, 4
Neuro-beamformers, 45
Neurodynamics, 672–724
additive model, 686–688
attractors, 684–685
BSB model, 689, 705–711
classical, 687
Cohen-Grossberg theorem, 703–704
degrees of freedom, 686
deterministic, 674
dissipation, 686
dynamic systems, 674–678
Hopfield model, 690–703
models, 686–689
noise, 686
nonlinearity, 686
related model, 688–689
stability, 678–684
statistical, 674
Neuromorphic integrated circuits, 5–6
Neuronal models
activation function, 10–11, 13–14
adaptive filter, 93
adder, 10
nonlinear, 11, 12
stochastic, 14–15
synapses, 10
Neurons
activation potential, 11
additive model, 687
architectural graph, 17
artificial, 9, 10
basic learning rule of FastICA 
algorithm, 538
best-matching, 430
bias, 11
coherent regions, 447
cooperative, 430, 431
defined, 2
dentritic trees, 7
excited, 371
in feature map, 449
first-order, 795
gain, 691
hidden, 22, 24, 126, 131–134
Hopfield network, 697
human brain, 6, 7
induced local field, 698

900
Index
Neurons (Contd.)
instantaneous error energy, 127
internal feedback acting on, 385
nonlinear, 2–3
nonlinear activation, 690
output, 131, 143
pyramidal cells, 7, 8
receptive field, 29, 30
second-order, 795
in self-organizing maps, 425–426
stochastic, 598
two-dimensional lattice, 429, 447
winner-takes-all, 371, 425
winning, 461
Newton direction, 195
Newton iterative step, 537
Newton-based learning rule, 538, 539, 540
Newton’s method, 96–98, 187, 536
Neyman-Pearson criteria, 28
Noise
Gaussian model, 498
neurodynamics, 686
salt-and-pepper, 409–410
subspace, 383, 413
white Gaussian, 409, 410
Noiseless network, 498
Noisy encoder-decoder model, 440
Nonline stochastic difference equation, 386
Nonlinear autoregressive with exogenous
(NARX) model, 792, 793
Nonlinear filtering, 203–209
dynamic range, 218
global approximation, 736
local approximation, 735–736
objective, 758
pattern processing, 203
short-term memory structures, 204–206
on static neural network, 204
universal myopic mapping theorem,
207–209
Nonlinear mapping, 165, 236
Nonlinear state dynamics, 814
Nonlinear stochastic difference equation,
107, 386
Nonlinearity, 2–3
Nonlinear-regression models, 69
Nonlinear-regression problem, 336
batch learning for, 128
formulation, 336
Nonparametric statistical inference, 3
Nonreciprocal two-port devices, 7
Nonseparable patterns
dual problem for, 279
optimal hyperplane for, 276–281
optimization problem for, 278
unbounded support vectors, 279–280
Nonstationary policy, 630, 631, 651
Nonzero-mean inputs, 156
Normal equation, 76, 245
Normalization, 258, 384
factor, 258
multiunit FastICA algorithm, 539
of probability, 581
property, 296
Normalized embedding delay, 717
Normalized weighting function, 257
Normalizing constant, 756
Norm-weighted matrix, 330, 331
Numerical approximation, 763
Numerical imprecision, 744
Numerical optimization, 186
O
Objective function, 454–455
gradients, 457
maximization, 459, 460
mutual information as, 493–494
outputs joint entropy, 454
Observability, 799–800
defined, 799
local, 802–803
matrix, 803
Observations, 24–25
density, 71
space, 39
Occam’s razor, 81, 165–166
Off-policy method, 648
On-line learning, 128–129
asymptotic behavior, 156–157
example-by-example basis, 128
generalization, 162
GHA, 401
Hessian role, 155–157
learning curve, 157
operation speed, 157
optimally annealed, 161
for pattern-classification problems, 129
popularity, 157
second-order stochastic gradient
descent, 199–201
small change tracking, 129
stability, 157
On-policy method, 648
Ontological relations, 467
Open-loop operator, 18
Optimal annealing
of learning rate, 158–161
on-line learning algorithm, 161
schedule, 161
Optimal hyperplane. See also Hyperplanes
algebraic distances of points, 271
defined, 270, 288
illustrated, 270
for linearly separable patterns, 269–276
for nonseparable patterns, 276–281
quadratic optimization for finding,
272–275
statistical properties, 275–276
Optimal manifolds. See also Manifolds
defined, 555
discrete model, 558
discretization process, 557–558
intrinsic properties, 556
iterative algorithm for computing,
558–559
LMS algorithm, 561
representation, 553–560, 564
smoothness, 559
in unsupervised manner, 557
Optimal parameter, 158
Optimal policies, 632
with policy iteration, 635–637
with value iteration, 637–642
Optimal robust estimation procedure, 292
Optimal-brain-surgeon algorithm, 181
Optimality
Lagrangian function, 273
LMS algorithm, 115
performance, 652
principle of, 631–632
Optimization
accuracy, 213
combinational, 596
conjugate-gradient method, 188–194
constrained clustering, 612
convex, 272
dual form, 287
dynamic programming and, 665
Karush-Kuhn-Tucker conditions, 536
for Lagrange multipliers, 293
neural network approximator, 663
numerical, 186
quasi-Newton methods, 194–197
robustness and, 289
supervised learning and, 186–201
support vector machine (SVM), 304
unconstrained, 94–100
Optimization errors, 215, 218
Ordered derivatives, 811–812
Ordering phase (adaptive process), 434–435
Orderly pair-by-pair basis, 543
Ordinary least-squares estimator, 77
Ordinary nonlinear differential equation,
387
The Organization of Behavior (Hebb), 368
Orthogonal rotation, 746
Orthogonal similarity transformation, 377
Orthogonality
multiunit FastICA algorithm, 539
principle of, 380, 740
property, 296
transformation, 109
Orthonormality
conditions of, 376
constraints, 412
Output neurons, 131, 143
Output space, 797
pointer, 437
representation, 437
Overdamped response, 96
Overdetermined learning problem, 212
Overfitting
defined, 164
illustrated, 165
problem, 82
Overtrained neural networks, 164
P
Parallel architectures, 181
Parallel Distributed Processing (Rumelhart
and McClelland), 124
Parameter estimation
assumptions, 73
in Gaussian environment, 73–76
problem, 70

Index
901
Parameter vector, 68
asymptotically unbiased, 87
defined, 69
Euclidean norm, 74–75
MAP estimation, 71–76
ML estimate, 72
total risk, 175
Partially observable Markov decision
processes (POMDPs), 664
Particle filters, 765–776
comparative evaluation, 775–777
computational considerations, 781–782
Monte Carlo integration, 766–768
sampling-importance-resampling,
771–774, 775
sequential importance sampling,
768–770
weight-degeneracy problem, 770–771
Partition function, 756
Parzen-Rosenblatt density estimator, 256,
259, 261
Pattern association, 38–39
Pattern classification
classical approach, 40
complex task, 231
computer experiment, 60–62, 77–79,
112–113, 150–153, 289
double-moon, 60–62
information-theoretic learning models,
560–561
k-means, RLS algorithm, 250–252
least-mean-square (LMS) algorithm,
112–113
least-squares, 77–79
multilayer perceptrons, 150–153
on-line learning for, 129
problems, solving, 129
with semisupervised learning, 356–359
support vector machine for, 280–281,
289
with synthetic data, 356–357
two-dimensional, two-class problem, 49
with USPS data, 358–359
Pattern recognition, 39–40
defined, 39
forms, 39–40
illustrated, 40
structural, 203
temporal, 203
Patterns
linear separable, 269–276
nonseparable, 276–281
separability of, 231–236
Perceptron convergence theorem, 50–55
absolute error-correction procedure, 53
as adaptive, 60
Cauchy-Schwarz inequality, 52
defined, 48, 49
error-correction learning rule, 55
fixed-increment, 53
fixed-increment adaptation rule,
51–52
learning-rate parameter, 51
as nonparametric, 60
proof, 52
quantized desired response, 54
summary, 54
Perceptron cost function, 62
Perceptrons
batch algorithm, 62–65
Bayes classifier relation, 55–60
built around single neuron, 48
defined, 47, 48
equivalent signal-flow graph, 50
error-correction learning algorithm, 50
fixed-increment adaptation rule, 51
goal, 49
hyperplane, 49
linearly separable classes, 50
multilayer, 122–217
parameters, 62
as pattern classifiers, 48
Rosenblatt’s, 47–66
single-flow graphs, 48
synaptic weights, 49
weight vector, 51
Perceptrons (Minsky and Papert), 65
Performance optimality, 652
Physics-based model, 835
Place-coded probability distribution, 426
Plasticity
defined, 1
stability, 4
synapses, 370
Point attractors, 685
Polak-Ribiére method, 191, 192
Policies
behavior, 648
cost-to-go function, 637
defined, 630
estimation, 648
greedy, 636, 651
mixed, 652
nonstationary, 630, 631, 651
optimal, 632, 635–642
stationary, 630–631
Policy iteration algorithm, 635–637
as actor-critic architecture, 637
approximate, 660–663
block diagram, 636
defined, 636
illustrative backup diagram, 639
steps, 636
summary, 637
value iteration relationship, 638–639
Polynomial-time algorithms, 304
Positive examples, 25–26
Postarray, 747
Prearray, 747, 749
Predicator-corrector formula, 739
Prediction-error covariance matrix, 738,
764
defined, 738
updating, 740–742
Predictive coding, 779
Predictive distribution, 756
Pre-Hilbert space, 296–297
Primal problem, 273
Principal components
defined, 377
estimation of number, 382–383
I-dimensional vector, 412
nonlinear, 404
Principal-components analysis (PCA),
367–415
assumptions, 509
basic data representations, 377–378
correlation matrix, 374
in data compression, 413
decoding, 379
defined, 368, 374
dimensionalities of mixing matrix, 509
dimensionality reduction, 378–382
eigenstructure, 375–377
encoding, 379
kernel Hebbian algorithm, 407–412
kernelization, 415
noise-free model, 509
random variable, 374
statistical independence, 509
whitening, 509
zero mean, 509
Prior information, 28
building into neural network design,
29–30
defined, 24
Probability of detection, 28
Probability of false alarm, 28
Probes, 699
Projected value iteration (PVI) method,
656–657
Prototype states, 697
Pseudodifferential operator, 325
Pseudoinverse, 101
Pseudotemperature, 581
Pyramidal cells
defined, 7
illustrated, 8
Pythagorean decomposition, 302, 489
Q
Q-factors, 635
optimal, 652
stagecoach problem, 640–642
Q-learning, 648–649
algorithm, 649–650
behavior policy, 648
convergence theorem, 650
defined, 648
exploration, 651–652
fully observable state, 649
learning-rate parameter, 649, 650
as off-policy method, 648
successful use, 649
Quadratic approximation, 178, 187
Quadratic metric, 299
Quadratic-programming (QP) problem,
303
Quadrature points, 761
Quasi-Newton methods, 194–196
conjugate-gradient method compari-
son, 196–197
defined, 194
scalar definition, 196
second-order information, 195

902
Index
R
Radar polarimetry, 505–506
Radial-basis function (RBF) networks,
239–242, 654
defined, 231
design, 259–260
function form, 237
Gaussian functions, 238
generalized, 327–331
hidden layer, 231, 239, 250
hybrid learning procedure, 249–250
ill conditioning, 328
illustrated, 328
input layer, 231, 239
inverse multiquadratics, 238, 239
layers, 231, 239
multiquadratics, 238, 239
normalized, 257–258
output layer, 231, 239, 250
pattern classification, 250–252
practice modifications, 240–242
shallow architectures, 654
structural illustration, 240, 241
summary, 259–261
training, 241, 250
as universal approximator, 259
Radial-basis functions, 237
Random variables, 374
Gaussian, 534, 535
non-Gaussianity, 534
uniformly distributed, 531
Random vectors, 256, 392
bottleneck, 550, 551
differential entropy, maximizing, 530
prewhitened, 535
relevant, 550, 551
Randomization
of encoding rule, 610
expected distortion, 611
of partition, 610
specified level, 611
Random-number generator, 429
Rank deficiency, 155
Rao-Ballard model, 778
Rate distortion theory, 450, 549–550, 563
defined, 550
expected distortion, 549
rate distortion function, 550
Rayleigh quotient, 351
Real-time recurrent learning (RTRL)
algorithm, 807, 812–818
concatenated input-feedback layer, 812
defined, 807, 812
derivation from true gradient behavior,
816–817
example, 817
formulation illustration, 813
processing layer of computational
nodes, 812
summary, 816
teacher forcing, 817–818
Receiver operating characteristic (ROC)
curve, 260
Receptive fields, 372
defined, 30, 252
of Gaussian hidden units, 254
in three-dimensional space, 254
weight-sharing and, 29
Reconstruction vector, 610, 718
Recurrent Markov chains, 584–585
Recurrent multilayer perceptrons
(RMLPs), 794–795
comparison, 655
defined, 794
dynamic behavior, 794
global feedback, 654
illustrated, 795
supervised training, 655, 823
Recurrent networks, 23–24, 688
adaptivity considerations, 831–833
architectures, 791–797
for associative memories, 673
attractor manipulation and, 689
back-propagation-through-time
(BPTT) algorithm, 807, 808–812
computational power, 804–806
continuous training, 806
defined, 18
dynamically driven, 790–839
encoding-decoding illustration, 697
epochwise training, 806
feedback loops, 23
fully connected, 798–799
with hidden neurons, 24
illustrated, 23
input/output model, 673, 791–793
learning algorithms, 806–807
meta-learning capability, 832
models, 835–836
multilayer perceptrons, 794–795
properties, 836
real-time recurrent learning (RTRL)
algorithm, 808, 812–818
second-order, 795–797
simple (SRN), 793–794
state, 797
state-space model, 793–794
supervised training framework,
822–829
Turing machine, 804
two-neuron, 808–809
vanishing gradients in, 818–821
Recursive least-squares (RLS) algorithm,
246–247, 249
classifiers, 260, 261
derivation, 246
formulation, 247
gain vector, 248
summary, 248
Reduced attracting set, 819
Redundancy measures, 500
Redundancy reduction, 499–501
Redundant information, 371
Reference signals, 42
Reflection coefficients, 32
Regression, 289–295, 335
-insensitive loss function, 292–295
defined, 68
kernels, 255–259
model building through, 68–89
nonlinear, 128, 336
optimum estimator, 295
Regression models
defined, 69
linear, 69–70
nonlinear, 69
Regressors
additive noise, 86
defined, 68
joint statistics, 70
noisy, 87
Regularization, 335–336
complexity, 175–176
defined, 315
for ill-posed problems, 315
manifold, 342–344
method of, 324
networks, 313
principle of, 318
structural, 77, 100
Regularization networks, 326–327
defined, 326
illustrated, 327
as universal approximator, 327
Regularization parameter
ambient, 349, 360
complexity-penalty term representa-
tion, 175
defined, 77, 100
estimation of, 336–342
intrinsic, 349, 356, 360
in regularization theory, 336
selection, 324
Regularization theory, 88, 313–360, 719
applications, 317–318
approximation scheme, 327
classical, 314
in classification, 318
error function, 316
Euler-Lagrange equation, 320
expansion coefficients determination,
323–325
Fréchet differential, 318–319
generalized, 314, 348–350
Green’s function, 321, 323, 324
multivariate Gaussian functions,
325–326
problem solution, 322
regularization parameter role, 336
regularizer, 316–317
Riesz representation theorem, 319–320
in structured prediction, 318
Tikhonov’s, 315–326
Regularized cost function, 314
Regularized least-squares (RLS) 
solution, 77
Regularized least-squares estimation,
331–334
defined, 77
descriptions, 334
input vector, 333
Laplacian, 354–355
maximum a posteriori (MAP)
estimation, 76
weight vector, 333

Index
903
Reinforcement learning, 627
block diagram, 36
classical approach, 628
delayed, 36–37
modern approach, 628
operation, 45–46
Renormalized SOM, 433
Replicator mapping, 181–183
Replicator network, 181–183
Representer theorem, 300–302, 332
classical, 349
generalization, 313, 352–353
generalized applicability, 301–302
least-squares estimator and, 333–334
mathematical statement, 301
Reproducing property, 299, 353
Reproducing-kernel Hilbert space
(RKHS), 297–300, 317
defined, 300
function norm increase, 353
functions defined in, 301
GHA comparison, 401–402
kernel PCA and, 401
Resampling, 768
Resistance-capacitance (RC) transmission
line, 7
Restricted Boltzmann machine (RBM)
defined, 606
hidden neurons, 606
top-level, 608, 609
training, 607
Retina, 5–6
Riccati difference equation, 742
Riemannian structure, 522
Riesz representation theorem, 319–320
Right singular vectors, 200
Risk functional, 293
Robbins-Monro stochastic approxi-
mation, 643–644
Robustness
back-propagation algorithm, 184
LMS algorithm, 114–115
optimization and, 289
Root mean-square error (RMSE), 777
Rosenblatt’s perceptron, 47–66
Rth-order rational varieties, 232
S
Saddle point, 148, 679
Saliency, 178
Salt-and-pepper noise, 409–410
Sampling, 767
density, 170
importance, 766
sequential importance, 768–770
Sampling-importance-resampling (SIR)
filter, 771–774, 775. See also
Particle filters
blind sampling, 772
defined, 772
ensemble-averaged state estimate, 776
resampling, 772
summary, 773
Scalable semisupervised-learning
algorithms, 361
Scaling
back-propagation algorithm, 185–186
factors, 538, 546
Search-then-convergence schedule,
116, 117
Second-order networks, 795–797
Second-order neurons, 795
Second-order stochastic gradient descent,
199–201
Self-amplification, 368–370
Self-organization
competition, 370–371
cooperation, 371
feature analysis, 372–373
principles of, 368–372
self-amplification, 368–370
structural information, 371–372
summary, 372
Self-organized learning
defined, 368
layer-by-layer basis, 372
Self-organized learning. See Unsupervised
learning
Self-organizing maps (SOMs), 425–468
adaptive process, 430, 433–435
algorithm summary, 436–437
applications, 467
approximation of input space, 437–442
batch training version, 441
competitive process, 429, 430–431
computer simulations, 445
contextual maps, 447–450
continuation, 437
convergence considerations, 466
cooperative process, 431–433
defined, 425–426, 465
density matching, 442–444
essential ingredients and parameters,
436
feature maps, 437–445
feature selection, 444–445
feature-mapping models, 426–428
hierarchical vector quantization,
450–454
initialization, 436
kernel, 454–462, 467–468
lattice dynamics with, 445–447
neurobiological considerations, 466
as nonlinear, 426
one-dimensional lattice, 446–447
pointer, 437
principle goal, 428
projection, 437
renormalization, 433
sampling, 436
similarity matching, 436
summary, 465–466
synaptic weights initialization, 429
topographic map, 426
topographical neighborhood, 431, 432
topological ordering, 442, 445
two-dimensional lattice, 445–446
updating, 436–437
user-defined classes mapped by, 467
as vector quantization algorithm, 442
as vector quantizer, 466
Semantic maps, 449, 450
Semisupervised learning, 45
effectiveness, 348
mathematical basis, 314
pattern classification experiments with,
356–359
scalable, 361
use of, 342
Sensitivity factor, 130
Sensitivity graphs, 154
defined, 139
fully recurrent network, 818
Separatrix, 685
Sequential importance sampling (SIS),
768–770
defined, 769
weight degeneracy, 772
Sequential minimal optimization (SMO),
303
Sequential Monte Carlo (SMC) method,
759
Sequential state-estimation
problem statement, 734
theory, 732
Shallow architectures, 654
Short-range excitatory mechanism, 428
Short-term memory, 204–206, 833
Shrinking, 303
Sigmoid function, 691
defined, 14
illustrated, 13
logistic function, 14
slope parameter, 14
Sigmoidal nonlinearity, 135
hyperbolic tangent function,
136–137
logistic function, 135–136
Signal subspace, 413
Signal-flow graphs
adaptive model for system, 92
back-propagation learning, 140
blind source separation learning
algorithm, 523
defined, 15
feedforward, 19
forgetting factor, 385
Gaussian classifier, 59
generalized Hebbian algorithm
(GHA), 394
IIR filter, 19
Kalman filter, 743
LMS algorithm, 104
maximum eigenfilter, 383
output neuron connected to hidden
neuron, 132
perceptron, 48, 50
reconstructed vector computation, 398
rules for construction, 15–16
single-loop feedback system, 18
synaptic divergence (fan-out), 16
Signal-to-noise ratio (SNR), 409
Signum function, 698
defined, 14
percetron computation, 54

904
Index
Similarity
matching, 377, 436, 461
measure, 26
orthogonal transformation, 377
Simple recurrent networks (SRNs),
793–794
Simulated annealing, 594–596, 618
annealing schedule, 595
for Boltzmann machine, 599–601
for combinational optimization, 596
defined, 594
iterative optimization algorithms 
versus, 595
objective, 594
Simulated electrode penetrating 
mapping, 449
Single-input, multiple-output (SIMO)
structure, 204
Single-input, single-output (SISO) 
system, 800
Single-layer feedforward networks, 21
Single-loop feedback system, 18
Singular value decomposition (SVD), 200
Sklar’s theorem, 490
Slack variables, 277
Small-scale learning
defined, 213
problems, 214
Smoothing constraints, 359
Smoothing regularizer, 360
Smoothness functional, 352
Soft margin hyperplane, 277
Sonar, 1
Sound-stream segregation, dynamic
model, 778–779
Source entropy, 612
Sparse coding
application results, 516
formulation, 515
ICA coding comparison, 514–516
of natural images, 514–516
Sparse-distributed coding, 407
Specialized networks, 28–29
Spectral graph theory, 350–352
eigenspectrum, 351
smoothness functional, 352
weighted undirected graph, 350
Spectral theorem, 377
Spectrum, in target echo, 34
Spherical surface measure, 760
Spherical-radial rule, 762–763
Spurious states, 702–703
Squared norm, 299
Square-root cubature Kalman filter
(SCKF), 765
Square-root filtering, 744, 748
Stability, 678–684
additive model, 689
alignment condition, 699
asymptotic, 387–388
condition for, 699
definitions, 681–682
differential equations, 690
of equilibrium states, 678–684
global, 683
Lyapunov surface, 683–684
Lyapunov theorems, 682–683
modern theory, 682
on-line learning, 157
plasticity, 4
synaptic weights matrix, 690
Stabilizers, 324
Stable states
continuous Hopfield model, 694–695
defined, 699
discrete Hopfield model, 694–695
Stagecoach problem, 640–642
defined, 640
flow graph, 640
Q-factors calculation, 640–642
Standard Gaussian-weighted integral,
762–763
State distribution vector, 587
State portrait, 676
State space, 675–676
defined, 582, 797
importance distribution, 774
State variables, 674
State vectors, 700
alignment condition satisfaction, 701
BSB model, 706
defined, 674
State-error covariance matrix, 248
State-filtering-error vector, 741
States
classification of, 590
cost-to-go function, 638
defined, 582, 732
deterministic transition, 635
equilibrium, 678, 679, 708
evolution, across time, 733
externally adjustable, 823
hidden, 732
intermediate, 584
internally adjustable, 823
mapping into actions, 630
measure, 732
null recurrent, 586
positive recurrent, 586
predication-error vector, 738
probabilistic transition, 635
prototype, 697
Q-factor, 635
recurrent, 584
recurrent network, 797
spurious, 702–703
stable, 699
steady-state probability, 586
as synaptic weights, 732
transition probability from, 583
unknown, 732
State-space model
approximate, 753
assumptions, 734
attributes, 733–734
block diagram, 793
hierarchy, 734–736
linear, Gaussian, 734–735
linear, non-Gaussian, 735
linearized, 826
measurement (observation) model, 733
network under training, 822
nonlinear, 822
nonlinear, Gaussian, 735–736
system (state) model, 732–733
Stationary policy, 630–631
Stationary stochastic environment, 69, 70
Statistical data analysis, 68
Statistical dependence, 489
Statistical expectation operator, 27
Statistical learning theory, 368, 414
Statistical mechanics, 580–582
Boltzmann machine, 580
defined, 580
stochastic methods rooted in, 579–618
Statistical neurodynamics, 674
Steepest-descent algorithm, 522
Stepsize parameter, 95
Stochastic approximation, 116
defined, 230
development, 184
Robbins-Monro, 643–644
theory, 161
Stochastic feedback system, 104
Stochastic gradient descent, 180
second-order, for on-line learning,
199–201
test error, 216
Stochastic maps, 554
Stochastic matrix
defined, 583
gradient, 519
raising, 589
Stochastic methods
defined, 128
Gibbs sampling, 596–598, 618
Metropolis algorithm, 591–594, 617
rooted in statistical mechanics, 579–618
simulated annealing, 594–596, 618
Stochastic neuronal model, 14–15
Stochastic neurons, 598
Stopping criteria, back-propagation 
algorithm, 139
Storage capacity, 39
Strange attractors, 711–716
chaotic behavior, 711
defined, 711
sensitive independence, 711
Strict interpolation, 325
Structural mismatch, 528
Structural pattern recognition, 203
Structural regularization, 77, 100
Structural risk minimization (SRM),
209–212, 302
approximation-estimation trade-off, 211
for small-scale learning problems, 209
Sub-Gaussian distribution
defined, 524
example, 525–526
Subsampling, 201, 202
Subspace decomposition, 380, 413
Subspace noise, 383, 413
Super-Gaussian distribution
defined, 524
example, 525–526

Index
905
Supervised learning, 34–36, 627–628
algorithms, 836–837
block diagram, 35
classification, 39
defined, 3, 34, 45
error signal, 35
error-correction, 35
error-performance surface, 35
feasibility, 209
input-output examples, 34
instantaneous estimate, 36
labeled examples, 45
as optimization problem, 186–201
Supervised training framework, 822–829
with extended Kalman filter, 823–826
externally adjustable state, 823
internally adjustable state, 823
for recurrent networks, 822–829
Support vector machines (SVMs), 260,
268–304, 454
architecture, 285
classifiers, 260
computational considerations, 303–304
curse of dimensionality, 304
defined, 268–269
design of, 284–286
error-free performance, 561
examples of, 284–286
as kernel machine, 281–283
Laplacian (LapSVMs), 360
optimal hyperplane (linearly separable
patterns), 269–276
optimal hyperplane (nonseparable 
patterns), 276–281
optimization theory, 304
for pattern classification, 280–281, 289
regression, 289–295
summary, 302–304
underlying philosophy, 280–281
XOR problem, 286–288
Support vectors
algebraic distance from, 272
defined, 271
expansion, 294, 295
unbounded, 279–280
SvmFu algorithm, 303
Symbol codes, 448
Symmetric function, 323
Symmetric matrix, 323–324
Synapses
conjunctional, 369
correlational, 369
defined, 6, 10
excitation/inhibition, 7
excitatory, 398
Hebbian, 369
neuronal model, 10
plasticity, 370
Synaptic adaptation, 430
Synaptic links, 15, 16
Synaptic weights
artificial neuron, 10
bias as, 50
defined, 10
equal to bias, 133
initializing, 429
modification of, 2, 371
N-by-N matrix, 698
stability, 690
vector, 392, 395, 437, 813
System identification, 41
Systematic bias, 156
T
Tabula rasa learning, 3
Tangent vector, 676
Tapped-delay-line filters, 32
Tapped-delay-line memory, 204, 205
Target values, back-propagation algo-
rithm, 146
Taylor series, 177
TD(
), 646–648
convergence, 648
iterative algorithms, 647
TD(0) learning algorithm, 643–644
Teacher forcing, 817–818
Teachers, 343
Temperature
decrement, 595
final value, 595
initial value, 595
of the system, 581
Temporal credit assignment problem, 37
Temporal pattern recognition
explicit representation, 204
implicit representation, 203
pattern processing, 203
Temporal-difference learning, 643–648
Monte Carlo simulation algorithm,
644–645
TD(
) algorithm, 646–648
TD(0) algorithm, 643–644
unified view, 646–648
Test threshold, 57
Testing, 25
Theory of idealized inductive 
inference, 80
Third-degree spherical-radial cubature 
rule, 759
Threshold, 13–14, 57
Tikhonov functional, 339
defined, 317
Fréchet differential of, 318–319
Tikhonov’s regularization theory,
315–326, 335, 719
in classification, 318
error function, 316
Euler-Lagrange equation, 320
expansion coefficients determination,
323–325
Fréchet differential, 318–319
Green’s function, 321, 323, 324
multivariate Gaussian functions,
325–326
regularization problem solution, 322
regularizer, 316–317
Riesz representation theorem, 319–320
in structured prediction, 318
Time averages, 75, 538
Top-down learning, 414
Topographic maps, 8–9
disentanglement, 462
formation, 460–461
self-organizing maps (SOMs), 426
Topographical manifolds, 347
Topographical neighborhoods, 431, 432
Topological ordering, 434, 442, 445
Training
continuous, 806
deep belief nets, 607–608
early-stopping method, 173–175
epochs, 127
epochwise, 806
invariance by, 31
mainstream, 837–838
multilayer perceptrons, 123–124
radial-basis function (RBF) networks,
241, 250
RBF networks, 241
sample, 127
Training samples
cross-validation, 172
defined, 25
expansion coefficients defined by, 333
sparsity, 200
Transfer functions, 15
Transition probabilities, 583–584
choice of, 592–594
normalized to unity, 592
Traveling-salesman problem (TSP), 723
Truncated back-propagation-through-
time (BPTT) algorithm, 810–811
Tuning parameters, 160
Turing machine, 804–805
defined, 804
illustrated, 805
Two-part code MDL principle, 80
U
Unbounded support vectors, 279–280
Unconstrained optimization, 94–100
Gauss-Newton method, 98–100
method of steepest descent, 95–96
Newton’s method, 96–98
problem, 94
Underdamped response, 96
Underdetermined learning 
problem, 212
Unit vectors, 177
Unit-delay operator, 18, 104, 105, 394
United States Postal Service (USPS) data set,
358–359
Unit-time delays, 825
Universal approximation theorem,
167–168, 183, 797–799
assumption, 170–171
defined, 167
as existence theorem, 167
theoretical viewpoint, 170
Universal dynamic mapper, 207
Universal myopic mapping theorem,
207–209
defined, 207
generic structure, 207
practical implications, 208–209

906
Index
Unlabeled examples, 45, 368, 414
defined, 25
generation, 314
Unsupervised learning
block diagram, 37
bottom-up view, 414
defined, 45
perspectives, 368
self-organized, 368
statistical learning theory, 368
top-down view, 414
unlabeled examples, 45, 368
views, 414
V
Value iteration algorithm, 637–642
defined, 637–638
illustrative backup diagram, 639
policy iteration relationship, 638–639
summary, 639
Vanishing gradients problem, 818–821
defined, 818
illustrated, 819
in recurrent networks, 818–821
second-order methods for mitigating,
821
Vapnik-Chervonenkis dimension, 276
Vector quantization
computation time, 450–451
hierarchical, 450–454
multistage, 451
theory, 438
Vector quantizers, 451, 452
Vector-coding algorithms, 428
Vectors
A-conjugate, 189–190
bias, 699
coefficient, 403
cross-correlation, 101, 245
data, 377, 378, 379
desired-response, 333, 337
error, 379, 397
feature, 281
gain, 248
gradient, 62, 95, 103, 158
inner product, 70
left singular, 200
parameter, 68, 69
projection, 26
random, 256, 392
right singular, 200
stochastic, 27
support, 271, 272, 279–280
synaptic-weight, 392, 395, 437
unit, 177
weight, 51, 99, 154, 158, 245–248
weight-error, 104, 114
Vector-valued measurement function, 823
Very-large-scale-integrated (VLSI), 4, 6
Vestibular-ocular reflex (VOR), 5, 779
Visual recognition
dynamic model, 777–778
Rao-Ballard model, 778
Viterbi algorithm, 664–665
Vocal tract filters, 344
W
Wavelet transforms, 407
Weight matrix
diagonal dominant, 709
discrete Hopfield network, 702
eigenanalysis, 702
positive semidefinite, 708
strongly diagonal dominant, 709
symmetric, 705, 708
transposed, 814
Weight sharing, 29
constraint, satisfying, 30
in convolutional networks, 203
Weight update, 823
Weight vectors, 51
demixer, 527
ensembled-averaged dynamics of,
158, 162
feature map relationship, 438
fixed throughout training interval, 332
Gram-Schmidt orthogonalization
procedure for, 540
optimum, 272, 287, 333
ordered, 154
parameter, 455
recursive least-squares estimation,
245–248
regularized solution for, 332
unknown, 245
updated, 99
Weight-decay procedure, 176
Weight-degeneracy problem, 770–771, 771
Weighted averages, 646
Weighted norm, 330–331
Weighted undirected graph, 350
Weight-error vectors, 104, 114
Weights, excess, 176
Weiner filter, 410
Well-posedness conditions,
314–315
White Gaussian noise, 409, 411
Widrow’s rule of thumb, 166
Wiener filter, 100–102
designing, 102
LMS algorithm deviation from,
104–105
Wiener solution, 117
defined, 102
optimum, 104
Willshaw-von der Malsburg model, 428
Winner-takes-all neurons, 371, 425
Winning neurons, 461
Woodbury’s equality, 179
Z
Zero-mean generalized Gaussian
bivariate distribution, 544



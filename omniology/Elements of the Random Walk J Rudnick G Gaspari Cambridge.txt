
This page intentionally left blank

ELEMENTS OF THE RANDOM WALK
An Introduction for Advanced Students and Researchers
Random walks have proven to be a useful model in understanding processes across
a wide spectrum of scientiﬁc disciplines. Elements of the Random Walk is an intro-
duction to some of the most powerful and general techniques used in the application
of these ideas.
The mathematical construct that runs through the analysis of each of the topics
covered in this book, and which therefore uniﬁes the mathematical treatment, is the
generating function. Although the reader is introduced to modern analytical tools,
such as path integrals and ﬁeld-theoretical formalism, the book is self-contained in
that basic concepts are developed and relevant fundamental ﬁndings fully discussed.
The book also provides an excellent introduction to frontier topics such as fractals,
scaling and critical exponents, path integrals, application of the GLW Hamiltonian
formalism, and renormalization group theory as they relate to the random walk
problem. Mathematical background is provided in supplements at the end of each
chapter, when appropriate.
This self-contained text will appeal to graduate students across science, engineer-
ing, and mathematics who need to understand the application of random walk
techniques, as well as to established researchers.
Joseph Rudnick earned his Ph.D. in 1970. He has held faculty positions at
Tufts University and the University of California, Santa Cruz, as well as a visiting
position at Harvard University. He is currently a Professor in the Department of
Physics and Astronomy at the University of California, Los Angeles.
George Gaspari is currently Emeritus Professor at the University of California,
Santa Cruz. He has held visiting positions at the University of Bristol, UK; Stanford
University; and the University of California Los Angeles. He has been a Sloan
Foundation Fellow.


ELEMENTS OF THE RANDOM WALK
An Introduction for Advanced Students and Researchers
JOSEPH RUDNICK
Department of Physics and Astronomy University of California,
Los Angeles
GEORGE GASPARI
Department of Physics University of California, Santa Cruz

cambridge university press
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge cb2 2ru, UK
First published in print format 
isbn-13    978-0-521-82891-8
isbn-13    978-0-511-18657-8
© J. Rudnick and G. Gaspari 2004
2004
Information on this title: www.cambridge.org/9780521828918
This publication is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
isbn-10    0-511-18657-6
isbn-10    0-521-82891-0
Cambridge University Press has no responsibility for the persistence or accuracy of urls
for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
eBook (EBL)
eBook (EBL)
hardback

For Alice and Nancy


Contents
Preface
page xi
1
Introduction to techniques
1
1.1
The simplest walk
1
1.2
Some very elementary calculations on the simplest walk
5
1.3
Back to the probability distribution
10
1.4
Recursion relation for the one-dimensional walk
13
1.5 Backing into the generating function for a random walk 
15
1.6
Supplement: method of steepest descents
20
2
Generating functions I
25
2.1
General introduction to generating functions
25
2.2
Supplement 1: Gaussian integrals
41
2.3
Supplement 2: Fourier expansions on a lattice
42
2.4
Supplement 3: asymptotic coefﬁcients of power series
47
3
Generating functions II: recurrence, sites visited, and the role of
dimensionality
51
3.1
Recurrence
51
3.2
A new generating function
51
3.3
Derivation of the new generating function
52
3.4
Dimensionality and the probability of recurrence
55
3.5
Recurrence in two dimensions
58
3.6
Recurrence when the dimensionality, d, lies between 2 and 4
60
3.7
The probability of non-recurrence in walks on different cubic
lattices in three dimensions
62
3.8
The number of sites visited by a random walk
63
4
Boundary conditions, steady state, and the electrostatic analogy
69
4.1
The effects of spatial constraints on random walk statistics
70
4.2
Random walk in the steady state
82
4.3
Supplement: boundary conditions at an absorbing boundary
93
vii

viii
Contents
5
Variations on the random walk
95
5.1
The biased random walk
95
5.2
The persistent random walk
98
5.3
The continuous time random walk
118
6
The shape of a random walk
127
6.1
The notion and quantiﬁcation of shape
127
6.2
Walks in d ≫3 dimensions
135
6.3
Final commentary
154
6.4
Supplement 1: principal radii of gyration and rotational motion
154
6.5
Supplement 2: calculations for the mean asphericity
159
6.6
Supplement 3: derivation of (6.21) for the radius of gyration
tensor,
↔
T , and the eigenvalues of the operator
165
7
Path integrals and self-avoidance
167
7.1
The unrestricted random walk as a path integral
168
7.2
Self-avoiding walks
171
8
Properties of the random walk: introduction to scaling
193
8.1
Universality
193
9
Scaling of walks and critical phenomena
203
9.1
Scaling and the random walk
203
9.2
Critical points, scaling, and broken symmetries
204
9.3
Ginzburg–Landau–Wilson effective Hamiltonian
215
9.4
Scaling and the mean end-to-end distance; ⟨R2⟩
218
9.5
Connection between the O(n) model and the self-avoiding walk
219
9.6
Supplement: evaluation of Gaussian integrals
227
10
Walks and the O(n) model: mean ﬁeld theory and spin waves
233
10.1 Mean ﬁeld theory and spin waves contributions
233
10.2 The mean ﬁeld theory of the O(n) model
236
10.3 Fluctuations: low order spin wave theory
239
10.4 The correlation hole
251
11
Scaling, fractals, and renormalization
255
11.1 Scale invariance in mathematics and nature
255
11.2 More on the renormalization group: the real space method
264
11.3 Recursion relations: ﬁxed points and critical exponents
277
12
More on the renormalization group
285
12.1 The momentum-shell method
285
12.2 The effective Hamiltonian when there is fourth order
interaction between the spin degrees of freedom
286

Contents
ix
12.3
The O(n) model: diagrammatics
300
12.4
O(n) recursion relations
302
12.5
The diagrammatic method
304
12.6
Diagrammatic analysis: the two-point correlation function
306
12.7
Supplement: linked cluster expansion
317
References
323
Index
327


Preface
We begin this preface by reporting the results of an experiment. On April 23, 2003,
we logged onto INSPEC – the physical science and engineering online literature
service – and entered the phrase “random walk.” In response to this query, INSPEC
delivered a list of 5010 articles, published between 1967 and that date. We then
tried the plural phrase, “random walks,” and were informed of 1966 more papers.
Some redundancy no doubt reduces the total number of references we received to
a quantity less than the sum of those two ﬁgures. Nevertheless, the point has been
made. Random walkers pervade science and technology.
Why is this so? Think of a system – by which we mean just about anything – that
undergoes a series of relatively small changes and that does so at random. It is more
likely than not that important aspects of this system’s behavior can be understood
in terms of the random walk. The canonical manifestation of the random walk is
Brownian motion, the jittering of a small particle as it is knocked about by the
molecules in a liquid or a gas. Chitons meandering on a sandy beach in search of
food leave a random walker’s trail, and the bacteria E. coli execute a random walk
as they alternate between purposeful swimming and tumbling. Go to a casino, sit
at the roulette wheel and see what kind of luck you have. The height of your pile of
chips will follow the rules governing a random walk, although in this case the walk
is biased (see Chapter 5), in that, statistically speaking, your collection of chips
will inevitably shrink.
We could go on. Random walks play a role in the analysis of the movements of
stock prices. A Random Walk down Wall Street, by Burton Malkiel has just been
published completely revised, following eight previous editions. Random Walks,
CriticalPhenomena,andTrivialityinQuantumFieldTheory,byRobertoFernandez
et al. focuses on the behavior of quantum ﬁeld theory in higher dimensions. There
are also Random Walks and Other Essays: Ruminations of a so-so Manager,
by Ren´e Azurin; Random Walk: A Novel for a New Age, by Lawrence Block,
and the record Random Walks Piano Music, by David Kraehenbuehl and Martha
xi

xii
Preface
Braden. Which is to say, the idea of the random walk has seeped into our collective
unconscious.
In this book, we hope to acquaint the reader with powerful techniques for the
analysis of random walks. The book is intended for the interested student or re-
searcher in physics, chemistry, engineering, or mathematics. It is our hope that
the level, style, and content of the book will be appealing and useful to advanced
undergraduate students, graduate students, and research scientists in all disciplines.
The mathematical techniques used in developing the theory are either explained
in the text proper or relegated to supplements at the end of each chapter when it
was thought that their inclusion would interrupt the ﬂow of the discussion. We are
hopeful that a student with a good understanding of calculus ought to be able to
follow much of the analytical manipulations. However, there are instances where
more advanced mathematical familiarity would be helpful.
The ﬁrst ﬁve chapters of this book focus on features of a variety of unrestricted
walks – that is to say, the trails left by walkers that retain no memory of where they
have visited previously – including biased walks, persistent walks, continuous time
walks, continuous ﬂow walks, and walks conﬁned to restricted regions of space.
The treatment is standard for the most part. However, we attempt to introduce a
language and a point of view based on generating functions, which is consistent
with a more modern ﬁeld-theoretical approach to the subject. This method will be
fully developed in the later chapters, when we must confront the complications
introduced by requiring the walk to be self-avoiding, meaning that the walker’s
path can never intersect itself. The generating function not only provides for a
ﬁeld-theoretical representation of the walker’s statistical behavior, but also allows
fortheconnectiontoastatisticalmechanicalmodelofmagnetism.Theidentiﬁcation
of random walks and magnetism has led to a quantum jump in our understanding
of the effects of self-avoidance; it makes available to the theorist the full arsenal of
analytical techniques that proved so successful in unraveling the complex properties
of systems that undergo continuous phase transitions.
A brief overview of the subjects covered in this book is as follows. Chapter 1
begins with a discussion of the properties of a one-dimensional walk. The chapter
is intended as a sort of overture, in that points of view and tricks are introduced that
we develop more fully in later chapters. Chapter 2 contains a serious discussion of
the meaning, nature, and implementation of the generating function in the context
of the random walk. In Chapter 3, we utilize the generating function to investigate
various aspects of unrestricted walks, including recurrence, mean number of sites
visited, and ﬁrst passage times. Chapter 4 – which relies heavily on the wonderful
little book Random Walks in Biology, by Berg, and contains discussions of the
effects of boundary conditions on walks – introduces the electrostatic analogy for
the analysis of a walk in the steady state. Biased and persistent walks make their

Preface
xiii
appearance in Chapter 5. We generalize the method of treating persistent walks
in one dimension to higher dimensional walks and present complete solutions for
persistent walks in two and three spatial dimensions. Chapter 6 is devoted entirely
to the problem of characterizing the average shape of the trail left by a random
walker. We focus on a particular quantitative measure of the shape of an object
that is unusually well suited to the kind of analytical tools that now exist for the
characterization of the properties of a random walk.
It should be mentioned that, in each of these chapters, we attempt to point out the
usefulness of the concepts of models of actual physical and biological processes
as the subject is developed. No attempt is made at a comprehensive comparison
between predictions of the model and experiments on particular systems. We direct
the reader to Weiss’ book Aspects and Applications of the Random Walk for such
detailed comparisons.
The random walk is one of the most important and intuitively appealing examples
of a statistical ﬁeld theory. It is a useful pedagogical model with which to introduce
someone to the latest techniques of such a theory, such as Ginzburg–Landau–Wilson
effective Hamiltonians, renormalization group theory, and graphical techniques.
Findingandunderstandingtheoriginalliterature,particularlywhenoneisbranching
out beyond his or her ﬁeld of specialty, can be a daunting task. We have tried
to reorganize and synthesize the most recent advances in the subject, which in
many cases are quite formidable in formulation. In so doing, we intended to make
these theories of random walks accessible to those who will ﬁnd the model useful
but are not well versed in the mathematical techniques upon which many recent
theoretical developments are based. We set out to accomplish this task in Chapters 7
through 12.
A reading of the table of contents clearly indicates what each of these chapters
entails.Hereweonlypointoutafewofthefeatureswhichwefoundtobeparticularly
interesting. In Chapter 7 we embark on a ﬁeld theory formulation of the random
walk problem `a la S.F. Edwards, by establishing a path integral expression for the
generating function. Once this is accomplished, it is straightforward to generate a
perturbation expansion in a quantity which measures “self-avoidance.” Doing this
allows for a gentle introduction to Feynman-like graphs and an exposition of the
associated graphical algebraic techniques. Using rather simple scaling arguments,
we clearly demonstrate the crucial role played by dimensionality in determining
the behavior of the walker, a feature that is stressed throughout the book. Finally,
a mean ﬁeld theory of self-avoidance is identiﬁed which then permits the inﬁnite
perturbation series to be summed. The mean ﬁeld generating function yields an
expression for statistical properties of the walk which shows it is exactly equivalent
to Flory’s treatment of self-avoidance. Chapter 8 contains a brief review of general
scaling notions as they apply to the random walk.

xiv
Preface
In Chapter 9, we establish the connection between the generating function and
the correlation function of a ﬁctional magnetic system, the O(n) model. This is an
extremely important result, for it brings to the theorist a new set of mathematical
tools developed over years by statistical physicists in their study of critical phe-
nomena, which can now be applied to the random walk problem. Critical point
scaling, critical exponents, universality, effective Hamiltonians, and renormaliza-
tion group theory are now at our disposal. These topics are covered in the remaining
chapters.
Once the connection between magnetism and random walks has been established,
mean ﬁeld theory and its extensions can be studied in well-known ways. This
is done in Chapters 9 and 10. The mean ﬁeld result is shown to be identical to
that found previously, thereby independently demonstrating the correctness of the
O(n) representation of random walks. Fluctuations are incorporated in a spin wave
approximation, leading to a reasonable physical rendering of the condensed state of
the magnetic system as it relates to the random walker. We outline the conceptual
underpinnings of the renormalization group approach and present some simple
realizations of the method in Chapter 11. Chapter 12 contains a full treatment of
the renormalization group as it applies to self-avoiding random walks.
We have interspersed problems throughout each chapter. These are intended to be
an aid in understanding the material and to provide a way for the reader to participate
in the exploration of the subject. They were not designed to be excessively long or
difﬁcult. We suggest students attempt their solution as they work their way through
the chapter as a way of gauging their understanding of the material. This book is
intended to be a textbook, appropriate for a stand-alone course on random walks
or as a supplemental text in a ﬁeld in which an understanding of random walks
is required. For example, this text might prove useful in a course on polymers,
or one on advanced topics in statistical mechanics, or even quantum ﬁeld theory.
Since our purpose here is to create a textbook, we have decided not to encumber
the presentation with a plethora of footnotes and an associated comprehensive
bibliography. It is our hope that the references we have included can be used to
track down the original articles dealing with the various aspects of the book. We
apologize to all those researchers who have made major contributions to the ﬁeld
and whose work is not cited herein.
This book took shape over several years, and the authors have beneﬁted from the
contributions of a number of people. We would like to express our gratitude to
Professor Fereydoon Family, who stimulated our initial interest in the subject of
random walks, and to Arezki Beldjenna whose contributions to the joint research
that underlies much of our chapter on shapes were especially important. We are
grateful to the students who sat in on the graduate seminar on random walks at

Preface
xv
UCLA for their enthusiasm and useful comments. Special thanks go to Maria R.
D’Orsogna for her careful reading of the notes that eventually became the text of
this book. The problem of the shape of a random walk was brought to our attention
by Professor Vladimir Privman, and for this we extend our heartfelt thanks.
The possibility of our writing a book on random walks was initially raised by
Professor Lui Lam. Our decision to publish with Cambridge University Press arose
from discussions with Rufus Neal. We thank him for brokering what has turned
out to be an enjoyable relationship with CUP, and for introducing us to Simon
Capelin, who has proven to be everything we could want in an editor. We thank
Fiona Chapman for her careful, and most cheerful, efforts as copy editor. We are
also grateful to Professor Warren Esty for permission to reproduce the images used
in Figures 1.1 and 1.2.
Finally, we are especially indebted to Professor Peter Young, who carefully read
the next-to-ﬁnal version of this manuscript. His queries, comments, and suggestions
resulted in a greatly improved ﬁnal version.
One of the authors (GG) expresses his appreciation to Tara and Bami Das for
making the early years among the best. He is also indebted to Nancy for her loving
support from the beginning to the end of this project. The other author (JR) thanks
his wife Alice for support, love, advice, and forbearance.


1
Introduction to techniques
This entire book is, in one way or another, devoted to a single process: the random
walk. As we will see, the rules that control the random walk are simple, even when
we add elaborations that turn out to have considerable signiﬁcance. However, as
often occurs in mathematics and the physical sciences, the consequences of simple
rulesarefarfromelementary.Wewillalsodiscoverthatrandomwalks,asinteresting
as they are in themselves, provide a basis for the understanding of a wide range
of phenomena. This is true in part because random walk processes are relevant to
so many processes in such a wide range of contexts. It also follows from the fact
that the solution of the random walk problem requires the use of so many of the
mathematical techniques that have been developed and applied in contemporary
twentieth-century physics. We’ll start out simply, but it won’t be long before we
enounter aspects to the problem that invite – indeed require – intense scrutiny.
We begin our investigations by looking at the random walk in its most elementary
manifestation. The reader may ﬁnd that most of what follows in this chapter is
familiar material. It is, nevertheless, useful to read through it. For one thing, review
is always helpful. More importantly, connections that are hinted at in the early
portions of this book will play an important role in later discussion.
1.1 The simplest walk
In the simplest example of a random walk the walker is conﬁned to a straight line.
This kind of walk is called, appropriately enough, a one-dimensional walk. In this
case, steps take the walker in one direction or the other. We will call those two
directions “right” and “left.” This makes everything easy, as we can now describe
the location of the walker by drawing a horizontal line on the page and showing
where on the line the walker happens to be. Let’s imagine that the walker decides
where its next step takes it by ﬂipping a coin. If the coin falls heads up the walker
takes a step to the right; if the coin falls tails up the walker takes a step to the left.
1

2
Introduction to techniques
The outcome of a ﬂip of the coin is equally likely to be heads or tails, so the walk is
clearly unbiased, in that there is no preference for progress to the left or the right.
Suppose the walker has taken N steps. It will have ﬂipped the coin N times. If
there were n heads and N −n tails, the walker will have taken n steps to the right
and N −n steps to the left. Suppose that each step is l meters long. Then the walker
will have moved a distance
d = nl −(N −n)l
= l(2n −N)
(1.1)
to the right. The walker will thus end up Nl meters to the left of where it started,
Nl meters to the right, or somewhere in between.
Before proceeding with the analysis of the behavior of the one-dimensional
walker, it is useful to inquire as to the relevance of the notion of such a walker to the
real world. As it turns out, the one-dimensional walk models a number of interesting
physical and mathematical processes. There is, for example, the diffusive spreading,
in one dimension, of a group of molecules or small particles as the result of thermal
motion. The one-dimensional walk also represents an idealization of a chain-like
polymer whose monomeric units can take on one of two possible conformations.
The outcome of a simple game of chance – for instance, one governed by the
ﬂip of a coin – can also be described in terms of the eventual location of a one-
dimensional random walker. In this last context, one of the ﬁrst applications of
notions eventually associated with the random walk is due to the mathematician de
Moivre in the solution of the “gambler’s ruin” problem (Montroll and Shlesinger,
1983).
An immediate and fairly obvious question about the walker is the sort one gener-
ally asks about the outcome of a random process, and that is with what probability
the walker ends up at a given location. That question is equivalent to asking with
what probability the walker throws a certain number of heads and tails in N tosses
of the coin. Another way to visualize this problem is to consider the act of ﬂipping
a coin a “trial” and to call all ﬂips that lead to heads a success. Then, clearly, the
above probability is the same as the probability of obtaining n successes in N trials.
Note that this interpretation applies to trials with more than two outcomes.
Back to the random walker. Suppose we want to know the probability that the
walker has gone a distance d to the right of its original position. In terms of the net
distance traveled, d = l(2n −N), the number of heads that were thrown is given
by
n = 1
2
d
l + N

(1.2)

1.1 The simplest walk
3
Fig. 1.1. A particular outcome of three ﬂips of a Roman coin displaying an image
of Emperor Septimius Severus (AD 193–211). Shown, left to right, is a head, then
a tail, then a head.
and the number of tails is
N −n = 1
2

N −d
l

(1.3)
Now, the probability of throwing a speciﬁc sequence that consists of n heads and
N −n tails in N coin tosses is equal to (1/2)N. See Figure 1.1. We arrive at the
result (1/2)N for this probability by noting that the probability of either result is one
half. Specifying the exact sequence of heads and tails is the same as specifying the
sequence of outcomes in a set of N trials, each of which has two possible results.
To obtain the probability of this sequence of outcomes, we multiply together the
probabilities of each outcome in the sequence. We obtain the probability in this
way because each toss of the coin is statistically independent of all other coin ﬂips.
That is, the probability of a given ﬂip yielding a head is 1/2, regardless of how all
previous tosses turned out.
The probability of throwing n heads and N −n tails in any order is (1/2)N
multiplied by the number of sequences of n heads and N −n tails. See, for example,
Figure 1.2. This number is simply the binomial coefﬁcient:
 N
n

=
N!
n!(N −n)!.
(1.4)
To derive the combinatorial factor in (1.4) in the case of the coin ﬂips depicted
in Figures 1.1 and 1.2, imagine the sequence of ﬂips in Figure 1.1 as an array of
coins. Then shufﬂe the coins in all possible ways. There are 3 × 2 × 1 = 3! ways
of doing this (three possibilities for the leftmost coin, two for the next in line and
only one left to place at the far right). However, in shufﬂing the coins, you have
overcounted the number of ways in which heads and tails can turn out. Switching
the ﬁrst and third coins in Figure 1.1 does not change the sequence of heads and
tails as both are heads. To compensate for this overcounting, we divide 3! by 2!, the
number of ways of shufﬂing, or permuting, the two heads. This leaves us with three

4
Introduction to techniques
Fig. 1.2. All outcomes of three ﬂips of the Roman coin in Figure 1.1 in which one
of the ﬂips turns up tails and the other two turn up heads.
distinct ways of having two heads and a tail turning up. In general, one computes
the number ways in which one can end up with n heads and N −n tails in N ﬂips of
a coin by imagining the results of the ﬂip being lined up as in Figure 1.1. Then one
shufﬂes the coins in all possible ways, leading to the factor N!, which one divides
by the number of ways of shufﬂing the n heads among themselves and the number
of ways of shufﬂing the N −n tails among themselves (Boas, 1983).
The factor in (1.4) is clearly the one that accounts for all distinct walks. It is not
hard to see that the combinatorial factor N!/n!(N −n)! is also equal to the number
of different ways that the walker can take n steps to the right and N −n steps to
the left. Put another way, the factor N!/n!(N −n)! is equal to the number of walks
that consist of n steps to the right and N −n steps to the left.
All this leads to the result that the likelihood that the one-dimensional walker
will take n steps to the right and N −n steps to the left is
1
2N
N!
n!(N −n)!
(1.5)
Exercise 1.1
How does the result (1.5) change when the coin is “biased” and the probability of a
heads at each toss is p ̸= 1/2? Assume that p does not change from one coin toss
to the next.
We can recast our expressions in terms of the location of the walker. Using (1.2)
and (1.3), we have for the number of N-step walks that take the walker a distance

1.2 Some very elementary calculations on the simplest walk
5
d to the right of its original location
C(N, d) = N!
 N + d/l
2

!
 N −d/l
2

!
(1.6)
and for the probability that the walker ends up a distance d to the right of its starting
point
P(N, d) = 1
2N C(N, d)
= 1
2N N!
 N + d/l
2

!
 N −d/l
2

!
(1.7)
The quantity P(N, d) in (1.7) is called the binomial probability distribution.
The results above allow one to calculate a good deal about the one-dimensional
random walk. However, there is much that can be found out without direct recourse
to them. In the next few sections we will see how much information can be extracted
from fairly simple and general arguments.
1.2 Some very elementary calculations on the simplest walk
The ﬁrst question that we will answer about the walker is where, on the average, it
ends up. Now, the answer to that question is one that you can come up with without
having to do an actual calculation. On the average, the walker will take as many
steps to the right as it does to the left. The mean distance to the right from the point
of departure is equal to zero.
We can do a little better than the above argument. We imagine an ensemble of
walkers, performing their walks in lockstep. We note the location of each of them,
and we calculate the average position by adding up the locations of all the walkers
and dividing by the number of walkers in the ensemble. If the position of the ith
walker is xi, then the mean position of a set of M walkers is
x = 1
M
M

i=1
xi
(1.8)
If we denote by w(x) the number of walkers who have ended up at x, then x as
given by the above equation is also equal to
x = 1
M

x
xw(x)
(1.9)
Given that it is equally likely that a walker will take a step to the right as to the left,
we know that there will be as many walkers at −x as at x, at least on the average.
This means that the two terms xw(x) and −xw(−x) will cancel each other out in
the sum in (1.9).

6
Introduction to techniques
Of course, the cancellation will not be perfect in an actual ensemble of walkers.
However, if we consider an enormous number of such ensembles, and take a sort
of “super” average, then such cancellation is, indeed, achieved.
This doesn’t mean that a given walker inevitably ends up where it started, or even
that it ends up near its starting point. To reﬁne our picture of the random walk, let’s
calculate the mean square displacement from the point of departure. This quantity,
x2, is given for a particular walk by
x2 =
 N

j=1
 j
2
(1.10)
Here,  j is the displacement at the jth step.1 That is to say that at the jth step the
walker moves a distance  j to the right. Using
x =
N

j=1
 j
(1.11)
one can argue that x = 0 by pointing out that  j = 0. This is an alternative deriva-
tion of the result immediately above. In the case of x2, we expand the right hand
side of (1.10) and then average.
x2 =
 N

j=1
 j
2
=

j
2
j +

j̸=k
 jk
=

j
2
j +

j̸=k
 j × k
(1.12)
The last line in Equation (1.12) expresses the fact that each decision to take a step
to the right or left is independent of every other decision. Because  j = 0 for all
 j, the contribution of the cross terms is equal to zero. We are left with 
j 2
j. We
suppose that the length of each step is the same, so the square of the displacement
at each step is equal to a ﬁxed number, which we will call l. This means that
x2 = Nl2.
(1.13)
The root mean square displacement, which measures how far away from its starting
point the walker has gotten, on the average, is, then given by

x2 = l
√
N
(1.14)
1 If the walker takes a step to the left, then  j is negative

1.2 Some very elementary calculations on the simplest walk
7
The distance that the one-dimensional random walker has wandered away from its
starting point goes as the square root of the number of steps it has taken.
Note that (1.14) implies that the net displacement of a random walker from the
origin scales as a fractional power of the number of steps that the walker has taken.
We will see that power laws pervade any quantitative discussion of the average
behavior of a random walker.
Worked-out example
Generate a formula for xn for arbitrary values of n.
Solution
The quantities on the left hand sides of (1.8) – (1.14) are known as moments of the
random walk distribution. The quantity xn is referred to as the nth moment of the
distribution. The general form of this quantity is
xn = 1
2N
N

m=0
((2m −N)l)n
N!
m!(N −m)!
(1.15)
Here, we have made use of (1.1) and (1.5). Suppose we were interested in one of
the higher moments of the distribution. For example, suppose we wanted to ﬁnd
x4. How would we go about doing that? We might expand the sum of the i’s,
raised to the fourth power, in a version of the calculation indicated in (1.12). This
would lead, in due course, to an answer. In fact, calculations of x3 and x4 using
(1.11) are posed as a problem later on in this chapter. However, there is another
approach, based on the notion of a generating function (Wilf, 1994), that yields a
straightforward algorithm for obtaining all moments of the distribution. What we
do is note that the quantity N!/m!(N −m)! is a binomial coefﬁcient. That is, this
quantity appears as the coefﬁcient of the term wm in the expansion of (1 + w)N in
powers of w:
(1 + w)N =
N

m=0
N!
m!(N −m)!wm
(1.16)
Replace w by ey, and divide by 2N. We have
1
2N
	
1 + ey
N = 1
2N
N

m=0
N!
m!(N −m)!emy
(1.17)
Let’s call the function on the left hand side of (1.17) g(y). Suppose we set y = 0
in (1.17). We generate the equality g(0) = (1/2N) N
m=0 N!/m!(N −m)! = (1 +
1)N/2N = 1.

8
Introduction to techniques
To ﬁnd the moments, we take derivatives. For example,
d
dy g(y)

y=0
=
1
2N
N

m=0
N!
m!(N −m)!memy

y=0
= 1
2N
N

m=0
N!
m!(N −m)!m
≡m
= d
dy
1
2N (1 + ey)N

y=0
= Ney (1 + ey)N−1
2N

y=0
= N (1 + 1)N−1
2N
= N
2
(1.18)
This tells us both that m = N/2 and that m = dg(y)/dy|y=0. We can readily gen-
eralize this result to
mn = dn
dyn g(y)

y=0
(1.19)
Making use of this result – and noting that x = N −2m – we can rewrite the
expression for xn as follows:
xn = ln

N −2 d
dy
n
g(y)

y=0
(1.20)
We can do a bit more. We rewrite the function g(y) as follows:
g(y) = eNy/2
ey/2 + e−y/2
2
N
= eNy/2 cosh(y/2)N
(1.21)
Then, we note that

N −2 d
dy

eNy/2 cosh(y/2)N
= cosh(y/2)N

N −2 d
dy

eNy/2 −2 d
dy cosh(y/2)N
= 0 −2 d
dy cosh(y/2)N
(1.22)

1.2 Some very elementary calculations on the simplest walk
9
We can carry this calculation out for the case of higher powers of N −2d/dy as
applied to the function g(y), and we ﬁnd in general that

N −2 d
dy
n
g(y) = (−2)n dn
dyn cosh(y/2)N
(1.23)
Exercise 1.2
Prove (1.23) by induction, or any other method you like.
This means that
xn = (−2l)n dn
dyn cosh(y/2)N

y=0
(1.24)
Then,
x2 = 4l2 d2
dy2 cosh(y/2)N

y=0
= 4
 N cosh(y/2)N
4
+ (−1 + N)N cosh(y/2)−2+N sinh(y/2)2
4

y=0
= Nl2
(1.25)
as found earlier (see (1.13)).
The next non-zero moment is x4. We ﬁnd
x4 = 16l4 d4
dy4 cosh(y/2)N

y=0
= l4
8 N cosh
 y
2
−4+N 	
(−4 + N) (8 + 3(−4 + N)N)
−4 (−4 + (−4 + N) (−2 + N)N) cosh(y) + N 3 cosh(2 y)


y=0
= l4N(−2 + 3N)
(1.26)
Note that the average of the fourth power of the distance of a one-dimensional
random walker from its point of origin has a term going as the square of the number
of steps, N, and also a term going linearly in N. At large values of N, the term
going as N 2 dominates the expression. Comparing (1.26) and (1.25), we see that
when N is very large
x4/(x2)2 ≈3
(1.27)

10
Introduction to techniques
Exercise 1.3
Use the relationship x = N
j=1  j for a one-dimensional walk – where  j is the
displacement to the right of the walker at the ith step – to ﬁnd x3 and x4. Make use of
the fact that n
i is equal to zero when n is odd and also that n
i = ln when n is even.
You will also make use of the fact that n1
j1 n2
j2 · · · nm
jm = n1
j1 × n2
j2 × · · · × nm
jm
when j1 ̸= j2 ̸= · · · ̸= jm.
1.3 Back to the probability distribution
Let’sreturntothecombinatorialfactorin(1.4).Althoughtheexpressioniscomplete,
in that we know perfectly well how to calculate each term in it, it is not of immediate
analytical use, especially when N, the number of random walk steps, is large. We
will now remedy this shortcoming by making use of Stirling’s formula for the
factorial to produce an expression more amenable to calculation. Stirling’s formula
is
ln n! ≈n ln
n
e

+ 1
2 ln (2πn)
(1.28)
An approximation that holds with greater accuracy as n is increased.
1.3.1 Derivation of Stirling’s formula
The approximate form that we will use follows from the well-known expression
for the gamma function
(x) =
 ∞
0
wx−1e−w dw
(1.29)
The relationship between the gamma function and the factorial is
N! = (N + 1)
(1.30)
This means
N! =
 ∞
0
wNe−w dw
(1.31)
The proof of the equality is readily established by integration by parts.
Figure 1.3 is a plot of the integrand in (1.31) when N = 10. Superimposed on
that plot is a Gaussian, shown as a dashed curve, which will be used to approx-
imate the integrand in the derivation of Stirling’s formula. How do we arrive at
the approximation by a Gaussian? First, we notice that the integrand is maximized

1.3 Back to the probability distribution
11
5
10
15
20
25
30
100000
200000
300000
400000
Fig. 1.3. The integrand wNe−w, in (1.31), when N = 10, along with the Gaussian
curve, shown dashed here, which will be used to approximate that integrand, in
the derivation of Stirling’s formula.
with w = N. We notice this by replacing w in the integrand by N + δ, and then by
exponentiating everything in the integrand. This exponentiation yields
N! =
 ∞
−N
dw exp [N ln(N + δ) −N −δ]
(1.32)
Focusing on the exponent and expanding in powers of δ:
N ln(N + δ) −(N + δ) = N ln N + N ln (1 + δ/N) −N −δ
= (N ln N −N) + N
 δ
N −1
2
δ2
N 2 + · · ·

−δ
= (N ln N −N) −δ2
2N + O
	
δ3
≡(N ln N −N) −(w −N)2
2N
+ · · ·
(1.33)
The Gaussian curve in Figure 1.3 is the function exp[10 ln 10 −10 −(w −
10)2/20]. Suppose we replace the integrand by that Gaussian approximation. We
then get the following result for the integration, leading to the factorial
N! ≈
√
2π N exp [N(ln N −N)]
(1.34)
To see how good an approximation it is, let’s compare the natural logarithm of 10!
with the natural logarithm of the right hand side of (1.34).
ln 10! = 15.1 044
(1.35)
ln
√
2π N exp [N(ln N −N)]

= 15.0 961
(1.36)

12
Introduction to techniques
The fractional difference between the right hand side of (1.35) and the right hand
side of (1.36) is about ﬁve parts in 104. The approximation gets even better as
N increases. When N = 100, the fractional difference between the logarithm of
Stirling’s formula and the log of the exact factorial is two parts in 106.
The formula works if n is large compared to 1. This means that the combinatorial
factor in (1.4) is well-approximated by
exp

N ln
 N
e

+ 1
2 ln (2π N) −n ln
n
e

+ 1
2 ln (2πn)
−(N −n) ln
 N −n
e

+ 1
2 ln (2π(N −n))

(1.37)
Let n = N
2 + m with m ≪N. Then the exponent in (1.37) can be expanded as
follows. We start with
ln
 N
2 + m

= ln
 N
2

+ 2m
N + 1
2
2m
N
2
+ · · ·
(1.38)
With the use of this equation, one obtains
N ln 2 −n2
2N −1
2 ln 2π N + O
 n3
N 2 , n
N

(1.39)
so the combinatorial factor has the form
2N
√
2π N
exp

−n2
2N + O
 n3
N 2 , n
N

(1.40)
The number m is equal to n −N/2, and since by (1.2) n = d/2l + N/2, we have
m = d/2l. This means that the likelihood that a walker will end up a distance d
from its point of departure is given by
1
√
2π N
exp

−d2
2Nl2

.
(1.41)
In arriving at (1.41), we have divided by the requisite factor of 2N to arrive at a
probability density that is normalized to one.
The expression in (1.41) is a Gaussian. We will encounter this ubiquitous form
repeatedly in the course of our investigation of random walk statistics. It reﬂects the
consequences of the central limit theorem of statistics (Feller, 1968), as it applies
to the random walk process.

1.4 Recursion relation for the 1D walk
13
Exercise 1.4
If N is large, then we can approximate the derivative of the log of N! as follows:
d
dN ln N! ≈ln N! −ln(N −1)!
N −(N −1)
Use this approximation to derive the leading contribution to Stirling’s formula for
N! (the ﬁrst term on the right hand side of (1.28)).
1.4 Recursion relation for the one-dimensional walk
There is another way to investigate the one-dimensional random walk. The number
of N-step walks that begin at a given location and end up at another one can be
related to the number of N −1-step walks that start at the same location and end
up nearby. If C(N; x, y) is equal to the number of walks that start at x and end up
at y then
C(N; x, y) = C(N −1; x, y −l) + C(N −1; x, y + l)
(1.42)
The formula (1.42) states mathematically that the number of N-step walks starting
at x and ending at y is equal to the sum of the number of N −1-step walks that
start at x and end up at all points adjacent to y. This statement reﬂects the fact
that the last step that a walker takes before ending up at the point y is from a
neighboring location. This fact tells us that the sequence of steps taken by our
walker, considered as a sequence of random events has the form of a Markovian
process of the ﬁrst order (Boas, 1983; Feller, 1968). That is, the probability of
occurrence of a given event is independent of the history consisting of all previous
events. In future chapters we will encounter higher order Markovian chains, and
even some that are non-Markovian.
The recursion relation (1.42) can be utilized to derive a familiar formula for the
combinatorial factors. Recall (1.4). This expression is for the number of N-step
walks that consist of n steps to the right and N −n steps to the left. Replacing the
terms in (1.42) by the equivalent expressions in terms of the combinatorial factors,
we have
N!
n!(N −n)! =
(N −1)!
(n −1)!(N −n)! +
(N −1)!
n!(N −n −1)!
(1.43)
That (1.43) is true can be readily veriﬁed. It is also the relation between combina-
torial factors that leads to Pascal’s triangle.
There is more that can be done with this recursion relation. If one assumes a
gentle dependence on the end point y – which is, in fact, the case when N is

14
Introduction to techniques
large – the recursion relation can be approximated by a differential equation. This
is accomplished by rewriting (1.42) as follows:
C(N + 1; x, y) = (C(N; x, y −l) + C(N; x, y + l) −2C(N; x, y))
+ 2C(N; x, y)
= l2
C(N; x, y −l) + C(N; x, y + l) −2C(N; x, y)
l2

+ 2C(N; x, y)
≈l2 ∂2C(N; x, y)
∂y2
+ 2C(N; x, y)
(1.44)
Another way to write this equation is
C(N + 1; x, y) −2C(N; x, y) = l2 ∂2C(N; x, y)
∂y2
(1.45)
Suppose we replace C(N; x, y) by 2N P(N; x, y), where P(N; x, y) is the proba-
bility that a walker starting out at y ends up at x after N steps. Equation (1.45)
becomes
P(N + 1, x, y) −P(N; x, y) = l2
2
∂2P(N; x, y)
∂y2
(1.46)
Again, imagine that N is large and that P(N; x, y) is a slowly varying function of
N. Then, we approximate the right hand side of (1.46) by ∂P(N; x, y)/∂N, and
we are left with the equation
∂P(N; x, y)
∂N
= l2
2
∂2P(N; x, y)
∂y2
(1.47)
This equation occupies a place of central importance, not only in the study of the
random walk, but also in physical and biological sciences, as well as in engineering.
It is the diffusion equation. While there are a variety of ways in which it can be
solved, we will write down a solution with the understanding that one can verify that
it works. We assume that the reader has encountered the equation and its solution
previously, and assert that it is
P(N; x, y) =
α
√
N
exp
(x −y)2
2l2N

(1.48)
Exercise 1.5
Verify (1.48) by direct substitution into (1.47).

1.5 The generating function for a random walk
15
1.5 Backing into the generating function for a random walk
So far our analysis of the statistics of the one-dimensional random walk problem is
the standard introduction to the subject that one ﬁnds in any elementary exposition
of the process. In the remaining portion of the section we would like to play some
games with the solution that we derived in (1.6) as a gentle way of initiating the
reader to the more advanced analytical techniques presented in subsequent sections.
It may seem at ﬁrst that the development represents a retreat from the results
obtained earlier on, in that the solution is, in a sense, “hidden” in the expressions
to be derived. However, what we will have at the end is a set of deﬁnitions and
relationships that can be generalized into a powerful approach to the properties of
more generally deﬁned random walks.
Recall that the expression on the right hand side of that equation is the combi-
natorial factor in (1.4), with n, the number of steps to the right, expressed in terms
of d through (1.2). The factor N!/n!(N −n)! also appears in the expansion of the
expression (1 + w)N in powers of w. That is
(1 + w)N =
N

n=0
N!
n!(N −n)!wn
(1.49)
It is a straightforward exercise to verify that the right hand side of (1.6) is the
coefﬁcient of eiqd in
	
eiql + e−iql
N ≡χ(q)N
(1.50)
The above means that the number of N-step walks that take the walker a distance
d to the right of its starting point is the coefﬁcient of eiqd in the expansion in terms
of eiq of the expression (2 cos q)N. There is a simple way to obtain that term. One
merely performs the integral
l
2π
 π/l
−π/l
e−iqdχ(q)N dq
(1.51)
In other words, the number of walks of interest is obtained by taking the inverse
Fourier transform of χ(q) raised to the Nth power. How (1.51) comes about is more
fully explained in the next section.
It is possible to regain the Gaussian form for the number of N-step walks by
performing the integral above, with the use of a couple of tricks and approximations.
First, we rewrite the expression for χ(q) as follows.
χ(q) = 2 cos ql
= eln(2 cos ql)
= 2eln(cos ql)
= 2eln(1−q2l2/2+··· )
= 2e−q2l2/2+O(q4)
(1.52)

16
Introduction to techniques
If we neglect the terms of order q4 in the exponent,
χ(q)N ≈2Ne−Nq2l2/2
(1.53)
Plugging this result into (1.51), we obtain
l
2π
 π/l
−π/l
e−iqd2Ne−Nq2l2/2 dq ≈
2N
√
2π N
e−d2/2Nl2
(1.54)
This is the same result as is given in (1.41). The integral was evaluated by assuming
that the upper and lower limits can be taken to plus and minus inﬁnity. The error
that this assumption entails can be shown to be negligible.
There is more. Suppose we perform the geometrical sum
g(z, q) =
∞

N=0
χ(q)NzN =
1
1 −zχ(q)
(1.55)
The quantity χ(q)N, entering into (1.51), is the coefﬁcient of zN in the expansion
of the right hand side of (1.55) in powers of the quantity z. The functions deﬁned
by expansions of the kind given in (1.55) are called generating functions. A large
portion of this book is devoted to the exploration of their properties. If we perform
a further expansion of the “structure function” χ(q) in powers of q, we have
χ(q) ≈2e−q2l2/2 ≈2

1 −q2l2
2

(1.56)
This expansion is accurate for our purposes as long as the number of steps in the
walk, N, is large and the end-to-end distance, d, is not too great. As a practical
matter, we require d ≪N.
The right hand side of (1.55) is, then, replaced by
1
1 −2z + zq2l2
(1.57)
and the number of N-step walks that displace the walker a distance d from its
starting point is equal to the inverse Fourier transform of the coefﬁcient of zN of
the expression in (1.57).
At this point, it may have seemed as if the transformations that have been per-
formed have had the effect of complicating, rather than simplifying, the problem
at hand. After all, the inverse Fourier transform is bad enough. The extraction of
a coefﬁcient in a power series can be an arbitrarily difﬁcult procedure. In the case
at hand, one has the right hand side of (1.55). As it turns out, there are also some
general prescriptions and a class of tricks that ease the difﬁculty of the latter pro-
cedure. To ﬁnd the coefﬁcient of zN in the function f (z), one simply performs the

1.5 The generating function for a random walk
17
z = 1/χ(q)
Fig. 1.4. Contour for the integral in (1.58). The pole at z = 1/χ(q) when f (z) is
given by (1.55) is indicated.
following contour integral
1
2πi

c
f (z)
zN+1 dz.
(1.58)
The contour encircles the origin, as shown in Figure 1.4. It is assumed that there is no
signiﬁcant singularity of the function f (z) inside the closed contour. The alert reader
will recognize this as Cauchy’s formula (Jeffreys, 1972) for N!dN/dzN f (z)|z=0.
The evaluation of the contour integral when f (z) looks like the right hand side of
(1.55) is relatively straightforward. One deforms the contour so that it encloses the
pole at z = 1/χ(k). Applying the formulas that apply to integration around simple
poles, one recovers the appropriate coefﬁcient.
There is another way to recover the result for the number of walks from the
generating function. This method simpliﬁes the extraction of the desired expression
when the number of steps, N, is large. Let the generating function be given by the
approximate form in (1.57). The integral over z is accomplished by exponentiating
the generating function and the denominator zN+1. The integral is now over the
function
exp

−(N + 1) ln z −ln
	
1 −2z + zq2l2

(1.59)
The integral is evaluated by looking for an extremum in the exponent of the expres-
sion above. The equation for the extremum is
−N + 1
z
+
(2 −q2l2)
1 −2z + zq2l2 = 0
(1.60)
Because N is large, the denominator of the second term on the left hand side of
(1.60) will be small. Writing z = 1/(2 −q2l2) + δ, we ﬁnd δ of order 1/(N + 1).

18
Introduction to techniques
Substituting into the expression in (1.59), we ﬁnd for the integrand
exp

(N + 1) ln
	
2 −q2l2
+ O(ln N)

= exp

(N + 1) ln 2 −(N + 1)q2l2
2
+ O(ln N)

(1.61)
Neglecting the terms of order ln N, and replacing (N + 1) with N, we obtain for
the integrand
2Ne−Nq2l2/2
(1.62)
This is just the result for χ(q)N that is displayed in (1.53). Of course, there is still an
integrationtodo,butthemethodthatisutilizedhere,knownasthemethodofsteepest
descents, produces the leading order contribution in the asymptotic expansion, i.e.
the expansion in powers of 1/N, of the integral (Jeffreys, 1972). We will comment
more extensively on the method of steepest descents in the supplement at the end
of this chapter.
1.5.1 More on generating functions: analytical structure
We deﬁned the generating function g(z, q) by (1.55). The Fourier integral of this
function deﬁnes another generating function
G(z, d) = 1
2π
 π/l
−π/l
eiqdg(z, q) dq
=
∞

N=0
 1
2π
 π/l
−π/l
eiqdχ(q)N dq

zN
=
∞

N=0
C(N; d)zN
(1.63)
Obviously, G(z, d) “generates” the number of N-step walks that cover a distance
d. Now, d ranges from −N to N, so if we sum on d, the left hand side becomes
g(z, 0), which will then generate the total number of N-step walks:
g(z, 0) =
∞

N=0
(N)zN
(1.64)
where (N), the total number of N-step walks, is equal to 2N in the case of the
one-dimensional walker. On summing the series on the right hand side of (1.64),
we ﬁnd
g(z, 0) =
1
1 −2z
(1.65)
As z →1/2, the function g(z, 0) diverges.

1.5 The generating function for a random walk
19
One might expect that for more complicated walks this singularity persists, and
the analytic behavior of g(z, 0) generalizes to
g(z, 0) ∼(zc −z)−γ
(1.66)
as z →zc, hinting at an analogy to the behavior of a system undergoing a phase
transition in the vicinity of a thermodynamic critical point. There will be more on
this subject later on.
1.5.2 Moments of the random walk
Here we show that the moments of the distribution of random walks follow from
the generating function. The key to the calculation of moments lies in the relation of
those moments to the Fourier transform of the generating function. From the inverse
transform of g(z, q), we can easily derive a general expression for the moments of
the random walk. As an example, we carry through the calculation of the second
moment, d2
N, for an N-step walk. Recall
g(z, q) =
∞

N=0
χ(q)NzN
=
∞

N=0
N

d=−N
C(N, d)eiqdzN
(1.67)
Taking two derivatives with respect to q:
−d2g
dq2 =
∞

N=0
N

d=−N
d2C(N, d)eiqdzN
(1.68)
This implies
−d2g
dq2

q=0
=
∞

N=0
d2zN
(1.69)
A general expansion for d2 follows from Cauchy’s formula
d2 = −1
2πi
 	
d2g/dq2
q=0
zN+1
dz
(1.70)
Two points should be noted about (1.70). First, the expression is quite general
and applies to more complicated paths than are considered here. For instance, the
formula is also valid when the walker suffers restrictions, such as the requirement

20
Introduction to techniques
that it not cross its own path. Second, the result for d2 is trivially generalized to
walks taking place in higher spatial dimensions.
Exercise 1.6
Calculate d2 in the limit of large N.
Exercise 1.7
Calculate d4 in the limit of large N. Show that d4 = 3(d2
N)2.
1.6 Supplement: method of steepest descents
There is a general method for extracting the coefﬁcient of zN in the power series
expansion of a function of z that is free of singularities in the vicinity of z = 0.
Let’s see how it works. If f (z) is the function of interest, and if it admits the power
series expansion
f (z) =
∞

N=0
CNzN
(1:S-1)
then a fundamental theorem of complex analysis tell us that
CN =
1
2πi

f (z)
zN+1 dz
(1:S-2)
where the closed integration contour encircles the origin and does not enclose any
singularityof f (z).Inthelimitingregime N ≫1weextracttheleadingcontribution
to the integral above by rewriting the integral as follows
CN =
1
2πi

eln( f (z))−(N+1) ln(z) dz
(1:S-3)
Then, we imagine that we can deform the contour so that it passes through an
extremum of the real part of the exponential, at z = z∗along a path of steepest
descent. The actual requirement on the path is that the imaginary part of the function
in the exponential in (1:S-3) is ﬁxed and equal to its value at the extremum point
(Jeffreys, 1972). If we are smart enough to have chosen the correct extremum,
and if N is large enough, the dominant contribution to the integral will be given
by the integrand at the extremum, multiplied by the result of an integration in the
immediate vicinity. Let’s apply the method to an integral of the general form
1
2πi

h(z)eNp(z) dz,
N ≫1
(1:S-4)

1.6 Supplement
21
saddle point
Fig. 1.5. The saddle point.
where dp(z)/dz = 0 at z = z∗, and h(z) varies slowly in the vicinity of the ex-
tremum. The quantity z∗is actually a saddle point. Figure 1.5 displays the general
behavior of the real part of a function in the neighborhood of its saddle point and
a path of steepest descent. It is also assumed that d2 p(z)/dz2 ̸= 0 at z = z∗. The
method applies equally well when the second derivative vanishes at the extremum
point, but is a bit more complicated. The major contribution to the integral comes in
the vicinity of the extremum when the path can be deformed to pass along a line of
steepest descent – that is, valley to valley through z∗. Then the integral is dominated
by the factor eN Rep(z∗) in the integral as N →∞. All other contributions along the
path are vanishingly small. In this region we can write
p(z) = p(z∗) + 1
2
d2 p
dz2∗
	
z −z∗
2
(1:S-5)
and
h(z) = h(z∗)
(1:S-6)
Then, the leading term in the asymptotic expansion of the integral is given by
h(z∗)
2πi eNp(z∗)

c
e
N
2 p′′(z∗)(z−z∗)2 dz
(1:S-7)
and the remaining integral is easily evaluated along the steepest descent line. Let’s
see how it works. In anticipation that the second derivative of p(z) is complex at
z∗, we write
p′′(z∗) = −
p′′(z∗)
 e2i(β−π/2)
(1:S-8)
and the integrand becomes
exp
	
N
p′′(z∗)
 e2i(β−π/2)(z −z∗)2
(1:S-9)

22
Introduction to techniques
β − π/2
z*
path of steepest
descent
valley
valley
y
x
V
U
Fig. 1.6. The new path of integration.
This reduces rather nicely by introducing the new variable
χ = (z −z∗)ei(β−π/2)
≡U + iV
(1:S-10)
The relation between the two coordinate systems can be seen in Figure 1.6.
In terms of the new variable χ, our integral looks like
e−i(β−π/2)
 ∞
−∞
e−N|p′′|χ2/2 dχ
(1:S-11)
with the steepest descent path being along the real axis of the new coordinate
system. Putting everything together, we obtain for the integral
1
2πi

h(z)eNp(z) dz ∼h(z∗)
2πi ep(z∗)

2π
N |p′′|ei(π/2−β)
(1:S-12)
As an example of how the method works, take f (z) to be given by f (z) =
(zc −z)−α. Identifying corresponding terms in (1:S-3) and our general form we see
that h(z) = 1 and p(z) = −α ln(zc −z) −(N + 1) ln z. The extremum equation is
dp(z)
dz
= 0 =
α
zc −z −N + 1
z
(1:S-13)
When N is large, we can write z∗= zc −δ, where the quantity δ is small. Then, to
a good approximation, δ = αzc/(N + 1), and the integrand at the extremum point

1.6 Supplement
23
is equal to
exp

−α ln

α
N + 1zc

−(N + 1) ln(zc) + α

+ · · ·
= (N + 1)αz−N−α−1
c
α−αeα + · · ·
(1:S-14)
where the ellipses refer to corrections that are symptotically smaller than the terms
displayed. To complete the identiﬁcation note that
p′′(z∗) = (N + 1)2
αz2c
Combining this result with (1:S-14), we ﬁnd for the coefﬁcient of zN in (zc −z)−α
1
2π α−α+1/2eα(N + 1)α−1z−N−α
c
(1:S-15)
This result is close to the exact one, which will be derived in section 2.4.


2
Generating functions I
2.1 General introduction to generating functions
This book makes extensive use of generating functions. In that respect the discus-
sions here are consistent with the approach that condensed matter physicists gener-
ally take when calculating properties of the random walk as it relates to problems
of contemporary interest. This chapter is devoted to a discussion of the generating
function and to an exploration of some of the ways in which the generating function
method can be put to use in the study of the random walk. Many of the arguments
in later chapters will call upon techniques and results that will be developed in the
pages to follow. Thus, the reader is strongly urged to pay close attention to the
discussion that follows, as topics and techniques that are introduced here will crop
up repeatedly later on.
2.1.1 What is a generating function?
The generating function is a mathematical stratagem that simpliﬁes a number of
problems. Its range of applicability extends far beyond the mathematics of the ran-
dom walk. Readers who have had an introduction to ordinary differential equations
will have already seen examples of the use of the method of the generating function
in the study of special functions. The generating function also plays a central role
in graph theory and in the study of combinatorics, percolation theory, classical and
quantum ﬁeld theory and a myriad of other applications in physics and mathematics.
Brieﬂy, a generating function is a mathematical expression, depending on one
or more variables, that admits a power series expansion. The coefﬁcients of the
expansion are the members of a family, or sequence, of numbers or functions. The
expansion can thus be said to “generate” the sequence of functions or numbers. A
particularly simple example of a generating function is the exponential ex = 1 +
x/1! + x2/2! + · · · + xn/n! · · · . The coefﬁcient of xn is 1/n!, so the exponential
generates the sequence of numbers 1/n!. Now, unless we do something clever
25

26
Generating functions I
with this result, it is not particularly useful or interesting. A far more powerful
relationship is the following:
1
√
z2 −2z x + 1
=
∞

n=0
Pn(x)zn
(2.1)
where Pn(x) is the nth order Legendre polynomial. The expression on the left hand
side of (2.1) is, thus, a generating function for Legendre polynomials. Similar iden-
tities allow one to generate other special functions that are solutions of important
linear differential equations of mathematical physics. Many of the key properties
of special functions are easily demonstrated with the use of generating functions.
Why use generating functions?
Equation (2.1) provides a clear indication of one of the advantages of the generating
function, in that it represents a prescription for the construction of the special
function that it generates. If you need to know the ﬁfth order Legendre polynomial,
P5(x), you can ﬁnd out what its form is by expanding the right hand side to ﬁfth
order in y. In this sense, the generating function encapsulates all information with
regard to the functions that it generates. Furthermore, it contains this information
in an exremely compact form.
In addition, the generating function can often be derived with relative ease. This
will certainly be true in the case of the generating function of ordinary random
walks. Finally, as we will soon see, there are cases in which the generating function
is the quantity of ultimate interest.
Exercise 2.1
Use (2.1) to derive the expression for P2(x).
Exercise 2.2
Use (2.1) to verify that
 1
−1 Pn(x) dx is equal to 2 if n = 0 and that the integral is
equal to zero if n ̸= 0.
Exercise 2.3
Making use of the result
 1
−1
dx

y2 −2yx + 1
 
z2 −2zx + 1
 =
1
√yz ln
1 + √yz
1 −√yz

ﬁnd
 1
−1
Pn(x)Pm(x) dx
for arbitrary integers, m and n.

2.1 General introduction to generating functions
27
Exercise 2.4
The integral order Bessel functions satisfy
e(z−1/z)ρ/2 =
∞

n=−∞
Jn(ρ)zn
Show that
Jn(ρ) = 1
2π
 π
−π
ei(ρ sin θ−nθ) dθ
Statistical mechanics: the generating function as the ultimate goal
of an investigation
Generating functions play an important role in statistical mechanics, though they
are not usually identiﬁed as such. They appear in relations connecting partition
functions of the various ensembles. For instance, the grand canonical partition
function (also called the grand partition function), Q, can be expressed in the
following way:
Q(α) =

N
Z(N)e−αN
(2.2)
In this equation, the quantity N is the number of particles a system contains, and
Z(N) is the partition function of that system in the canonical ensemble: Z(N) =
	
r e−Er/kT . The quantity α is given in terms of thermodynamic variables by α =
−µ/kT , µ being the chemical potential. Deﬁning the fugacity z via z = e−α one
is led to the identiﬁcation of Q as a generating function having the form Q(z) =
	
N Z(N)zN. This function generates the canonical partition function for a system
of N particles.
Notice that in the previous example one’s goal might well have been the gener-
ating function itself – in this case the grand partition function – and not necessarily
the sequence it generates. There are other physical systems whose properties are
most naturally characterized in terms of an appropriate generating function, which
thus becomes the object of primary focus. For example, consider a collection of
long chain polymers in chemical equilibrium with a reservoir of monomers. The
monomer bath controls the molecular weights of the polymers by establishing a
chemical potential, µ – or, alternatively, a fugacity z – per monomeric unit. The
ensemble of interest then consists of a distribution of polymers having a variety
of molecular weights, and the Boltzmann factor associated with an n-unit poly-
mer contains the factor zn. Random walk statistics are appropriate to the statistical
mechanics of this ensemble of polymers. These statistics derive directly from the
generating function.

28
Generating functions I
Fig. 2.1. The trail left by a random walker.
One may also have occasion to consider an ensemble of dynamical walkers
(“Brownian particles”) that face the possibility of extinction at each step. If the
probability of surviving to take a subsequent step is equal to z, then the probability
that a given walker will make exactly n steps before changing or losing its identity
is equal to zn(1 −z). The properties of an ensemble of such walkers also follows
directly from an appropriate generating function.
Exercise 2.5
The partition function of the monatomic ideal gas is given by
Z(N) = 1
N!

CT 3/2V
N
where T is the absolute temperature, V is the volume occupied by the gas and C is a
collection of physical constants. Find the grand partition function of the monatomic
ideal gas.
2.1.2 Generating function for a random walk on a lattice
Let’s turn our attention back to the process of primary interest and take a look at
a quantity directly relavant to random walks. Suppose C(N; ⃗x, ⃗y) is equal to the
number of N-step walks that start out from the point ⃗x and end up at the point ⃗y.
Figure 2.1 illustrates the trail left by such a walker. Then, consider the following
function of the variables z, ⃗x and ⃗y:
G(z; ⃗x, ⃗y) =
∞

N=0
zNC(N; ⃗x, ⃗y)
(2.3)
By the deﬁnition of the function G(z; ⃗x, ⃗y), the coefﬁcient of zN in its expansion as
a function of z is clearly C(N; ⃗x, ⃗y). Thus, G(z; ⃗x, ⃗y) is the generating function for

2.1 General introduction to generating functions
29
the family of functions C(N; ⃗x, ⃗y). Note that the probability that an N-step walk
begins at ⃗x and ends at ⃗y is simply C(N; ⃗x, ⃗y) divided by the total number of walks.
Thus, the family of functions generated by G(z; ⃗x, ⃗y) is also proportional to the
probability, PN(⃗x, ⃗y), that an N-step walk starting at ⃗x ends up at ⃗y.
If what we have been saying about the advantages of generating functions holds
in the case at hand, we ought to be able to derive an explicit expression for the
generating function G(z; ⃗x, ⃗y). As we will see, this task is well within our abilities.
Indeed, in the previous chapter we derived explicit expressions for both G(z; x, y)
and C(N; x, y) in the case of a walker proceeding along a straight line. There, we
found
C(N, ⃗x, ⃗y) = 1
2π
 π
−π
e−iq(x−y)χ(q)N dq
(2.4)
and
G(z; x, y) =
∞

N=0
C(N; x, y)zN
= 1
2π
 π
−π
dq
1 −χ(q)z e−iq(x−y)
(2.5)
where
χ(k) = 2 cos q
(2.6)
Here, q is measured in units of the inverse of the step length, l.
Not surprisingly, the results for the one-dimensional walk are easily extended
to walks taking place in d spatial dimensions. The trick is to utilize a recursion
relation that relates the statistics of an N-step walk to those of a walk consisting
of N + 1 steps. To keep the discussion simple we will assume that the walker is
conﬁned to the vertices of a three-dimensional, simple cubic lattice and that steps
are along the bonds joining nearest neighbor sites (see Figure 2.2 for a picture of a
walker on such a lattice (P´olya, 1919)). Such walkers are called “P´olya walkers,”
as he was the ﬁrst to investigate the properties of walkers on a lattice (P´olya, 1921).
Consider, now, an N-step walk on this lattice. The next-to-last step in this walk
must have left the walker at a site adjacent to its ﬁnal destination. This means that
the number of walks that start at site ⃗x and end at site ⃗y is equal to the sum of
the numbers of N −1-step walks starting at ⃗x and ending at sites adjacent to ⃗y.1
This implies the following relationship between C(N; ⃗x, ⃗y) and the corresponding
1 Actually, the desired quantity is the number of N −1-step walks that start at ⃗x, land at points adjacent to ⃗y and
then go on to ⃗y at the Nth step. We assume that any walker that is at an adjacent site can move to y in one step,
so the two numbers are the same.

30
Generating functions I
Fig. 2.2. A walker on a two-dimensional square lattice.
w1
w2
w3
w4
y
Fig. 2.3. Pictorial representation of the recursion relation for random walks.
quantities for walks starting at ⃗x and ending at sites ⃗wi, where the subscript indexes
the sites that are nearest neighbors to ⃗y.
C(N; ⃗x, ⃗y) =

i
C(N −1; ⃗x, ⃗wi)
(2.7)
Figure 2.3 is a pictorial representation of the process quantiﬁed in (2.7). Equation
(2.7) holds for all positive, non-zero values of N. When N = 0, (2.7) is replaced
by the identity C(0, ⃗x, ⃗y) = δ⃗x,⃗y, where δ is the discrete delta function.
As the next step, we convert (2.7) to a relation between generating functions. We
do this by multiplying (2.7) by zN and summing over N, from N = 0 to N = ∞.

2.1 General introduction to generating functions
31
This yields
∞

N=1
zNC(N; ⃗x, ⃗y) =
∞

N=1
zN


i
C(N −1; ⃗x, ⃗wi)

= z

i
∞

N=0
zNC(N; ⃗x, ⃗wi)
= z

i
G(z; ⃗x, ⃗wi)
(2.8)
The left hand side of (2.8) is almost equal to G(z; ⃗x, ⃗y). It is simply missing the
term z0C(0; ⃗x, ⃗y) = δ⃗x,⃗y. If we add this term to the equation, we end up with
G(z; ⃗x, ⃗y) = z

i
G(z; ⃗x, ⃗wi) + δ⃗x,⃗y
(2.9)
Notice that the new equation involves only the argument z. We are, however, not
quite at the desired point. The relation above is non-local in space, in that the
left hand side of (2.9) depends on the position vector ⃗y, of the end-point of the
walk, while the right hand side of the equation contains the locations, ⃗wi, of the
neighboring sites. This means that it is not yet possible to solve the equation by
simple algebraic means. Nevertheless, the recursion relations summed up in (2.7)
and (2.9) have a number of interesting and useful features, which we will explore
at the end of this section.
Translational symmetry and spatial Fourier transform
The recursion relation for the generating function of random walks yields readily
to an analysis based on the translational symmetry of the lattice (Montroll, 1956).
To take advantage of this symmetry, we ﬁrst note that both C(N; ⃗x, ⃗y) and the
generating function G(z; ⃗x, ⃗y) depend on the position vectors ⃗x and ⃗y only through
their difference, ⃗x −⃗y. This means that we can write the generating function in the
form G(z; ⃗x −⃗y). Next, we perform a spatial Fourier expansion of (2.9). If
g(z; ⃗q) ≡

⃗x
G(z; ⃗x −⃗y)ei⃗q·(⃗x−⃗y),
(2.10)
then, multiplying (2.9) by ei⃗q·(⃗x−⃗y) and summing over ⃗x,
g(z; ⃗q) = z

⃗x, ⃗wi
ei⃗q·(⃗x−⃗y)G(z; ⃗x −⃗wi) + 1
= z

⃗x−⃗wi, ⃗wi
ei⃗q·(⃗x−⃗wi)G(z; ⃗x −⃗wi)ei⃗q·( ⃗wi−⃗y) + 1
= z g(z; ⃗q)

⃗wi
ei⃗q·( ⃗wi−⃗y) + 1
≡z g(z; ⃗q)χ(⃗q) + 1.
(2.11)

32
Generating functions I
Fig. 2.4. A site and its nearest neighbors in a simple cubic lattice.
The last line introduces the structure function of the lattice, χ(⃗q). Notice that the
extraction of an explicit expression for the generating function g(z; ⃗q) requires
nothing more than simple algebra.
Simple cubic lattice
Up to this point, we have not referred to the speciﬁc structure of the lattice on
which the walker moves. It works for walks in one dimension, where we previously
encountered χ(q), or in d dimensions and a lattice of any symmetry. If the walk
takes place on a three-dimensional cubic lattice, then the six nearest neighbors are
as shown in Figure 2.4, and the ﬁnal sum in (2.11) is easy to perform. We ﬁnd for
the structure function
χ(⃗q) = 2

cos(qxa) + cos(qya) + cos(qza)

(2.12)
The quantity a in the above equation is the distance between adjacent points on the
cubic lattice. The sum above is over nearest neighbors to the site ⃗x = 0.
Exercise 2.6
Calculate the structure function, χ(⃗q), for body-centered cubic and face-centered
cubiclattices.Forinformationthatwillhelpyoutoobtainasolution,seeSupplement
2 at the end of this chapter.
If we now perform the algebraic steps necessary to solve (2.11) we obtain
g(z; ⃗q) =
1
1 −zχ(⃗q)
(2.13)
And, we’re done. The right hand side of (2.13), with χ(⃗q) as given by the right
hand side of (2.12), contains all the information there is to be had about the number

2.1 General introduction to generating functions
33
of random walks between any two vertices of a simple cubic lattice – assuming, of
course, that all of the walker’s steps are along the bonds joining nearest neighbors,
and they occur with equal probability. That is, the walk is unbiased.
In this way, we have obtained, in a reasonably small number of relatively simple
steps, the function that generates an important piece of information regarding the
statistics of random walks. The advertised advantages of the generating function
are not illusory.
2.1.3 Asymptotic behavior of C(N; ⃗x, ⃗y) for long walks: N ≫1
Now, let’s see what we can do with the expression we’ve obtained. First, we can
immediatelyextractthespatialFouriertransformofC(N; ⃗x −⃗y).Itisthecoefﬁcient
of zN in the power series expansion of the left hand side of (2.13), and expressing
1/(1 −z χ(⃗q)) in powers of z is easy:
1
1 −z χ(⃗q) = 1 + zχ(⃗q) + · · · +

zχ(⃗q)
n + · · ·
(2.14)
The spatial fourier coefﬁcient
c(N; ⃗q) =

⃗x−⃗y
ei⃗q·(⃗x−⃗y)C(N; ⃗x −⃗y)
(2.15)
is given by
c(N; ⃗q) =

χ(⃗q)
N
=

2

cos(qxa) + cos(qya) + cos(qza)
N
(2.16)
We want a result for the number of walks between two given points on the real
lattice, and the expression on the right hand side of (2.16) only takes us part of
the way towards our goal. Note that we take the lattice to possess cubic symmetry
and explicitly introduce the nearest neighbor lattice distance, a. The next step is to
extract C(N; ⃗x −⃗y) from its inverse fourier transform.
C(N; ⃗x −⃗y) =
 a
2π
3 
⃗q∈BZ
e−i⃗q·(⃗x−⃗y)c(N; ⃗q) d⃗q
(2.17)
where the integration over ⃗q is conﬁned to the ﬁrst Brillouin zone2 of the simple
cubic lattice, e.g.−π
a < qi ≤π
a , i = x, y, z. Now, the integral is not particularly
easy to carry out if N is large and one wants an exact result. However, the most
important contributions to the integral can be extracted with the use of a few simple
2 Supplement 2 at the end of this chapter.

34
Generating functions I
tricks. First, we exponentiate a portion of c(N; ⃗q) as follows:

cos(qxa) + cos(qya) + cos(qza)
N
= exp

N ln

cos(qxa) + cos(qya) + cos(qza)

(2.18)
Then, we expand the exponent as a power series in the components of ⃗q:
exp

N ln

cos(qxa) + cos(qya) + cos(qza)

= exp

N ln

3 −1
2

a2 
q2
x + q2
y + q2
z

+ O(⃗q 4)

= 3Ne−N
6 (a2|⃗q|2)+···
(2.19)
We are now ready to proceed.
The Gaussian limit
We will start out by dropping the terms of order ⃗q 4 and higher in the exponent. The
consequences of this are far-reaching. As we will soon see, the fact that the exponent
can be truncated at quadratic order in ⃗q leads to the result that long random walks
have universal behavior. That is, the statistics of walks with N steps, as exempliﬁed
by the quantity C(N; ⃗x −⃗y), are, when N ≫1, independent of details, such as on
what kind of lattice the walker must take its steps, or even whether or not the walker
is conﬁned to a lattice. This is a particular example of the type of universality that has
played such an important role in the study of critical-point behavior in equilibrium
thermodynamics.
Before discussing in more detail the approximations that have been made in
the truncation, we will use the exponential in the last line of (2.19), without the
ellipses, to extract the leading order behavior of the quantity C(N; ⃗x −⃗y) when
N ≫1. The integration we have to perform is over a simple Gaussian. Since the
Gaussian decays to zero when ⃗q has a magnitude signiﬁcantly greater than 1/
√
N,
and because we are interested in the limit N ≫1, we can ignore the restriction of ⃗q
to the ﬁrst Brillouin zone. The Gaussian factor ensures that there is no contribution
to the integral of any consequence outside of a small region in the center of the
zone. Inserting (2.19) into (2.17) we are left with
C(N; ⃗x −⃗y) =
 a
2π
3 
6Ne−i⃗q·(⃗x−⃗y)−N
6 (a2|⃗q|2) d⃗q
= 6N

3
2π N
3/2
e−3|⃗x−⃗y|2/2Na2
(2.20)
The expression for the number of random walks between two locations has a Gaus-
sian form. This result is correct as long as the distance from the starting point to the
location at which the walker ﬁnds itself at last is not too large compared to
√
Na.

2.1 General introduction to generating functions
35
The last line in (2.20) is straightforwardly obtained by completing squares in the
exponent3 and utilizing standard results for the Gaussian integral in the integration
over ⃗q.
2.1.4 Sources of universality
Let’s see just how much of a mistake we are making in discarding the term of order
|⃗q|4 in (2.19). To keep things simple we will imagine that ⃗q is a scalar. This does
not compromise the validity of our arguments. We now assess the consequences of
neglecting the term q4 in the integral
 ∞
−∞
e−iqr−a2 N
6 q2−bNq4 dq
(2.21)
We can do this in a variety of ways. The simplest is to expand the exponent with
respect to the quartic term. If we retain the ﬁrst order term in this expansion, the
integral is of the form
 ∞
−∞
e−iqr−a2 N
6 q2 
1 −bNq4
dq
=

6π
Na2 e
−3r2
2aN

1 −27b
Na4

1 −6 r2
Na2 + 3
 r2
Na2
2
(2.22)
The error we have made is, apparently, unimportant as long as the ratio b/a4 is small
compared to N, and we are interested in what happens when N is very large. A more
systematic investigation supports the conclusion that we just reached. In the limit
N ≫1 the number of walks between two points is given in terms of a Gaussian.
Note also that according to (2.22) the Gaussian form for C(N; ⃗x, ⃗y) holds under
the assumption that r = |⃗x −⃗y| is not large compared to
√
N. This relationship
is constistent with r ∼
√
N and N ≫1. As the separation between the end-points
approaches the number of steps, a different limiting form applies. The new form
reﬂects the fact that there can be no paths longer than the one left by a walker who
has gone in a straight line.
Notice that in the integration we performed in (2.20) we were able to ignore
the fact that the lattice has a Brillouin zone. In discarding terms in the exponent
beyond those of quadratic order in the wave-vector ⃗q, we also ignored the other
consequence of the crystalline structure of the lattice – the detailed dependence
on ⃗q of χ(⃗q). In fact, the only property of χ(⃗q) that we made use of is the fact
that it has a power-series expansion in ⃗q, and that the two lowest order terms in
3 For details, see Supplement 1 at the end of this chapter.

36
Generating functions I
that expansion are the zeroth and second order ones. Any lattice that has this last
property will yield a result for C(N; ⃗x −⃗y) that, in the limit N ≫1, has the same
form as the second part of (2.20).
To recapitulate, the source of the universal Gaussian form for the number of
random walks is two-fold. First, there is the fact that the structure function χ(⃗q) has
the power-series expansion c0 + c2q2 + · · · . Second, there is the fact that at large
number of steps, N, the only terms in that expansion that matter are the two lowest
ones. If χ(⃗q) expressed as a power series had different terms at lowest order, i.e.
χ(⃗q) = c0 + c′q1.3 + · · · , then the number of walks would not be expressible as a
Gaussian.
Off-lattice walkers
We can go even further. The walker need not be restricted to a lattice. Imagine that
the probability of a step having a length and direction described by the displacement
vector ⃗r is described by a distribution that does not conﬁne the walker to the lattice.
If, for example, the probability that ⃗r lies in the inﬁnitesimal volume d3r ≡dx dy dz
centered at ⃗r0 is equal to p(|⃗r0|) d3r ≡p(r0) d3r, then we can derive a result for
C(N; ⃗x −⃗y) in exactly the same way as we did for the case of a walker on a lattice.
Following a procedure similar to that taken for walks on a lattice, but allowing the
steps to be taken in a continuum, the quantity χ(⃗q) will be given by
χ(⃗q) =

ei⃗q·⃗r p(r) d3r
(2.23)
Suppose the probability density p(r) admits of a moment expansion. That is, write

ei⃗q·⃗r p

| ⃗r |

d⃗q =
 
1 + i⃗q · ⃗r + 1
2!

i⃗q · ⃗r
2 + · · ·

p(r) d3r
(2.24)
and suppose that the integrals involving the ﬁrst several terms in the expansion
of the exponential converge. Then, because the integral of i⃗q · ⃗r p(r) vanishes by
symmetry, we have the same sort of expansion for χ(⃗q) as was the case for the
walker on a lattice, and the Gaussian form for C(N; ⃗x −⃗y) follows.
An important consideration in the speciﬁcation of p(r) is that this probability is
normalized, in that

p(r) d3r = 1
(2.25)
This tells us that χ(0) = 1.

2.1 General introduction to generating functions
37
Exercise 2.7
Suppose
p(r) =

2
π r−3
0 e−r2/2r2
0
Find χ(⃗q) and use this result to obtain the probability distribution for the end-to-end
distance of an N-step off-lattice random walk.
Worked-out example
Suppose
p(r) =
r0
π2(r2 + r2
0)2
(2.26)
Find χ(⃗q) and use this result to obtain the probability distribution for the end-to-end
distance of an N-step off-lattice random walk.
Solution
Here, we have the case of a walker in which the likelihood of very long steps falls
off sufﬁciently slowly that standard results do not follow. In particular

r2 p(r) d3r
is not a ﬁnite quantity. As we will see, we do not end up in this case with a Gaussian
distribution for walks with N steps in the regime N ≫1.
Our ﬁrst task is to ﬁnd the function χ(q). Making use of (2.23), we have
χ(q) =

ei⃗q·⃗r
r0
π2(r2 + r2
0)2 dr
= 4
π
 ∞
0
sin qr
q
rr0
(r2 + r2
0)2 dr
(2.27)
The last line in (2.27) follows from the integration over θ and φ in

ei⃗q·⃗r f (r) d3r =
 2π
0
dφ
 π
0
sin θ dθ
 ∞
0
r2 dreiqr cos θ f (r)
(2.28)
The integration over r in (2.27) can be worked out with the use of contour methods.
For example, we write
4
π
 ∞
0
rr0 sin qr
q
1
(r2 + r2
0)2 dr = −4r0
π
1
q
d
dq
 ∞
0
cos qr
(r2 + r2
0)2 dr
= −2r0
π
1
q
d
dq
 ∞
−∞
eiqr
(r2 + r2
0)2 dr
(2.29)

38
Generating functions I
To ﬁnd the result of the integration, one closes the integration contour in the upper
half r-plane, around the second order pole at r = ir0. This yields the result
χ(q) = e−qr0
(2.30)
The next step in the calculation is to reconstruct the probability that an N-step
walk takes the walker a distance r from its point of origin. We know that this
quantity is given by
1
(2π)3

χ(q)Nei⃗q·⃗rd3q =
1
(2π)3 4π
 ∞
0
e−Nqr0 q sin qr
r
dq
=
Nr0
π2(r2 + N 2r2
0)2
(2.31)
The right hand side of the top line in (2.31) follows from (2.28). The integration
leading to the last line in (2.31) is easily carried with the use of a table of integrals
or by exploiting de Moivre’s theorem and replacing the sine function by complex
exponentials.
The end result for the probability distribution of the N-step off-lattice walk
controlled by the single-step probability distribution (2.26) looks very much like the
original probability distribution. It is clear that in this case, the “average” distance
of the walker from the point of origin scales like Nr0. That is, the “average” distance
away from its point of origin of the walker in question scales as N, rather than as
N 1/2. This average distance cannot be obtained in terms of a second moment of the
distribution P(N; 0, ⃗r) as given by the last line of (2.31) because that distribution
function does not have a ﬁnite second moment.
2.1.5 Gaussian walks, the continuum limit, and diffusion
There are other ways to derive the Gaussian distribution for long random walks.
Those who have some familiarity with the central limit theorem of statistics will
recognize the Gaussian form as a special application of that theorem. The form
is most revealing in that it also makes clear the connection with diffusion. To see
this, recall (2.7), the recursion relation we derived previously for C(N; ⃗x, ⃗y). This
equation can be turned into a recursion relation for the probability that the walker
that starts out at ⃗x and takes N steps will end up at ⃗y. If we call this probability
P(N; ⃗x, ⃗y), then the new recursion relation is
P(N; ⃗x, ⃗y) = 1
2d

nns
P(N −1; ⃗x, ⃗y −⃗wi)
(2.32)

2.1 General introduction to generating functions
39
where nns means nearest neighbor sites. The above equation holds for unbiased
walks on the simple cubic lattice in d dimensions. The unbiased nature of the
walker is manifested in the factor 1/2d in (2.32); the quantity 2d is the coordination
number of the lattice, e.g. the number of nearest neighbors. What is implied is that
the probability of the walker’s landing on any nearest neighbor site in one step is
equallylikely.Itisalsoclearfrom(2.32)howtogeneralizetherecursionrelationship
for biased walks, that is walks for which the probability of landing on neighboring
sites is pi instead of 1/2d, then the correct equation to represent this situation would
be
P(N; ⃗x, ⃗y) =

nns
P(N −1; ⃗x, ⃗y −⃗wi)pi
(2.33)
Back to the unbiased walkers on a simple cubic lattice: the recursion relation can
also be written as a difference equation
P(N; ⃗x, ⃗y) = 1
6
3

j=1

P(N −1; ⃗x, ⃗y −ˆe ja) + P(N −1; ⃗x, ⃗y + ˆe ja)

(2.34)
where the ˆe j’s are unit vectors along the positive x, y and z axes. Adding and
subtracting P(N −1; ⃗x, ⃗y) casts the recursion relation in a very suggestive form:
P(N; ⃗x, ⃗y) −P(N −1, ⃗x, ⃗y) = 1
6
3

j=1

P(N −1; ⃗x, ⃗y −ˆe ja)
+ P(N −1; ⃗x, ⃗y + ˆe ja) −2P(N −1; ⃗x, ⃗y)

(2.35)
This ﬁnal result is exact for the speciﬁc random walk we are considering. However,
when the number of steps is very large (N ≫1) the above functions are very slowly
varying, P(N −1; ⃗x, ⃗y) ≈P(N; ⃗x, ⃗y), and can therefore be approximated by the
leading terms in a Taylor series expansion, i.e.
P(N; ⃗x, ⃗y) −P(N −1; ⃗x, ⃗y) ≈
∂
∂N P(N; ⃗x, ⃗y)
(2.36)
Carrying this procedure out for both sides of (2.35) transforms the difference equa-
tion into a partial differential equation
∂
∂N P(N; ⃗x, ⃗y) = a2
6 ∇2P(N; ⃗x, ⃗y)
(2.37)
which, as the reader might well realize, is the diffusion equation. This is not an
unexpected result. The well-known solution to this equation is given by the left
hand side of our (2.20), with the factor 6N removed. It is now abundantly clear that
the approximations leading to the Gaussian form for the number of walks washes

40
Generating functions I
out the discrete nature of the lattice and produces the continuum limit. It is worth
noting that the generating function itself also satisﬁes a partial differential equation
in the continuum limit. Applying the same arguments as above reduces (2.9) to the
following partial differential equation for G(z; ⃗x, ⃗y):
za2∇2G(z; ⃗x, ⃗y) −(2dz −1) G(z; ⃗x, ⃗y) = δ⃗x,⃗y
(2.38)
which rightfully earns the function G the status of a Green’s function. We will have
more to say about the Green’s function nature of G in a subsequent chapter, where
we develop the random walk problem along the lines of a ﬁeld theory similar to
what has been done in statistical mechanics in the study of critical phenomena.
Exercise 2.8
Verify by direct substitution that the function
P(N; ⃗x, ⃗y) =
3
2
3/2
1
a3π3/2N 3/2 exp

−3r2
2Na2

where r = |⃗x −⃗y|, is a solution to (2.37). Furthermore, show that this probability
distribution is normalized, in that

P(N; ⃗x, ⃗y) d3x = 1
Exercise 2.9
Derive a diffusion equation for a biased one-dimensional walk. Under what condi-
tions will the probability density be of the Gaussian form?
2.1.6 Summing up
Let’s review what we have managed to achieve. Accepting that the number of N-
step walks that start and end at speciﬁed points is a quantity worth knowing (and it
most deﬁnitely is), we have shown that the generating function provides a measure
of random walk statistics that contains all information with regard to this quantity
for a walk with any number of steps. Furthermore, we have shown that this latter
mathematical entity admits of a relatively straightforward derivation. Its form has
been explicitly derived in the special case of a walker stepping randomly along the
edges of a cubic structure. While we have made some restrictive assumptions about
the walk – for instance, that it takes place on a certain type of lattice, that it is unbi-
ased – most of the restrictions are easily relaxed within the context of the generating

2.2 Supplement 1
41
function calculation at no great cost in the form of signiﬁcant complications to the
analysis.
Having obtained an explicit expression for the generating function, we show how
to extract the number of N-step walks with speciﬁed starting and ending points.
Finally, we investigate the asymptotic behavior of this function in the limit of large
N, rediscovering features already demonstrated in the previous chapter.
The remainder of this chapter consists of three supplements, in which techniques
utilized in the exploitation of the generating function method are reviewed. It is
not necessary to read them until those methods are called upon. On the other
hand, the reader may wish to be forearmed with some information concerning the
mathematics that underlie the discussions to follow.
2.2 Supplement 1: Gaussian integrals
Gaussian functions and Gaussian integrals play an important role in the statistics
of random walks. Here, we review some basic results associated with integrals of
Gaussians. The ﬁrst is the well-known formula for the integral of a simple Gaussian:
 ∞
−∞
e−ax2dx =
π
a
(2:S1-1)
The next set of integrals are moments of the Gaussian distribution. The nth moment
of the Gaussian e−ax2 is the integral
 ∞
−∞
xne−ax2 dx
(2:S1-2)
The most efﬁcient way to ﬁnd the value of the integral above for general values of
the power n is, appropriately enough, through a generating function. Consider the
integral
 ∞
−∞
e−ax2+bx dx
(2:S1-3)
Expanding (2:S1-3) in powers of b, we see that the coefﬁcient of bn/n! is the nth
moment of the Gaussian. This means that (2:S1-3) is a generating function for
moments of the Gaussian e−ax2.
Given this useful fact, let’s evaluate the integral in (2:S1-3). We do this by
completing squares in the exponent. Writing
ax2 −bx = a

x −b
2a
2
−b2
2a
(2:S1-4)

42
Generating functions I
we have
 ∞
−∞
e−ax2+bx =
 ∞
−∞
e−a(x−b/2a)+b2/4a dx
= eb2/4a
 ∞
−∞
e−ay2 dy
= eb2/4a
π
a
(2:S1-5)
performing the expansion and comparing terms, we see that
 ∞
−∞
xne−ax2 dx =
 π
a
n!
(n/2)!(4a)n/2
n is even
0
otherwise
(2:S1-6)
2.3 Supplement 2: Fourier expansions on a lattice
2.3.1 On Fourier transforms
The reader is, no doubt, familiar with the expansion of functions in plane waves
of the form ei⃗k·⃗r. Here, we recall some of the basic features of such expansions.
A function of the variable x deﬁned on the interval (−∞, ∞), satisfying certain
integrability criteria, admits an expansion of the form
f (x) = 1
2π
 ∞
−∞
g(k)e−ikx dk
(2:S2-1)
This transform can be inverted as follows:
g(k) =
 ∞
−∞
f (x)eikx dx
(2:S2-2)
The function g(k) is the fourier transform of the function f (x), and f (x) is the
inverse fourier transform of the function g(k). In three dimensions the equations
above generalize to
f (⃗r) =
1
(2π)3

g(⃗k)e−i⃗k·⃗r d3k
(2:S2-3)
and
g(⃗k) =

f (⃗r)ei⃗k·⃗r d3r
(2:S2-4)
respectively.
If the function f (x) is deﬁned on a discrete set of points, (i.e. a lattice), then the
expansions above must be modiﬁed. Here we present, in an abbreviated and non-
rigorous fashion, some results pertaining to the fourier representation of functions
deﬁned on a lattice, where the lattice can be either unbounded or of ﬁnite extent.

2.3 Supplement 2
43
We assume that the reader is acquainted with the standard notation and results of
lattice theory. If not, a good introductory text on condensed matter physics ought
to provide a suitable background (Ashcroft and Mermin, 1976; Ziman, 1979). For
ease of presentation, we restrict our analysis to simple lattices, ones without a
basis. These lattices are called Bravais lattices. They can be generated by adding
and subtracting integral multiples of three non-coplanar primitive vectors, ⃗a1, ⃗a2,
and ⃗a3. These vectors are often referred to as basis vectors, or primitive vectors.
All the lattice points are thus connected to a central point, also on the lattice, by
position vectors, called direct lattice vectors, of the form
⃗R = n1⃗a1 + n2⃗a2 + n3⃗a3
(2:S2-5)
The ni’s take on all positive and negative integral values. The volume in which
the lattice sits can be ﬁlled by translations through the lattice vectors ⃗R of small
volumes, the smallest of which is called the primitive unit cell. The shape of the
primitive unit cell is not unique, although its volume is. One of the commonly used
unit cells is a parallelepiped whose edges are the basis vectors ⃗a1, ⃗a2, and ⃗a3. The
volume, 	, of the primitive unit cell is then seen to be
	 = ⃗a1 · ⃗a2 × ⃗a3
(2:S2-6)
For every Bravais lattice, one can construct another lattice, called the reciprocal
lattice. The basis vectors spanning the reciprocal lattice are denoted ⃗b1, ⃗b2, and ⃗b3.
One constructs them from the basis vectors of the direct lattice according to the
prescription
⃗bi = 2π
	 ⃗a j × ⃗ak
(2:S2-7)
where i, j and k are cyclic permutations of x, y and z. This construction guarantees
that ⃗ai · ⃗b j = 2πδi j. The reciprocal lattice will play a crucial role in the development
of plane wave representations of functions deﬁned on a lattice. All the points of the
reciprocal lattice can be reached by position vectors of the form
⃗K = l1⃗b1 + l2⃗b2 + l3⃗b3
with l2, l2 and l3 taking on all positive and negative integral values. The vectors ⃗K
are called reciprocal lattice vectors, and by construction, they have the property
⃗K · ⃗R = 2π × (an integer)
(2:S2-8)
for any ⃗K and ⃗R drawn from the reciprocal of the direct lattice, respectively. This
follows immediately on recalling that ⃗ai · ⃗b j = 2πδi j. The primitive unit cell of the
reciprocal lattice is called the ﬁrst Brillouin zone. It is straightforward to show that
	BZ = ⃗b1 · ⃗b2 × ⃗b3 = (2π)3/	.

44
Generating functions I
A trivial example, but one pertinent to our previous discussion, is the simple
cubic lattice. For a lattice spacing a, we ﬁnd that the basis vectors
⃗a1 = aˆi
(2:S2-9)
⃗a2 = a ˆj
(2:S2-10)
⃗a3 = aˆk
(2:S2-11)
generate the cubic lattice. The primitive cell is a cube of side a and volume a3.
The basis vectors of the associated reciprocal lattice are found to be
⃗b1 = 2π
a
ˆi
(2:S2-12)
⃗b2 = 2π
a
ˆj
(2:S2-13)
⃗b3 = 2π
a
ˆk
(2:S2-14)
by straightforward application of (2:S2-7). The Brillouin zone is again a cube of
length 2π/a, on each side. The origin of the Brillouin zone is taken to be at the
center of the cube, and, of course, its volume is 	 = (2π)3/a3.
Two other lattices with cubic symmetry are the body-centered cubic lattice and
the face-centered cubic lattice. Three basis vectors for the body-centered cubic
(bcc) lattice are
⃗a1 = a
√
3
ˆi + ˆj + ˆk

(2:S2-15)
⃗a2 = a
√
3
ˆi + ˆj −ˆk

(2:S2-16)
⃗a3 = a
√
3
ˆi −ˆj + ˆk

(2:S2-17)
Three basis vectors for the face-centered cubic (fcc) lattice are
⃗a1 = a
√
2
ˆi + ˆj

(2:S2-18)
⃗a2 = a
√
2
ˆi + ˆk

(2:S2-19)
⃗a3 = a
√
2
ˆj + ˆk

(2:S2-20)
In all cases above, a is the spacing between nearest neighbor vertices on the lattice.
2.3.2 Inﬁnite lattices; Fourier expansions
The discussion above lays the foundation for plane wave expansions of functions
deﬁned on any one of the Bravais lattices. Let such a function be denoted F(⃗R).

2.3 Supplement 2
45
Here the function F(⃗R) is deﬁned for all direct lattice vectors ⃗R spanning the points
of our lattice. A function of this type is C(N; ⃗x, ⃗y), the number of walks beginning
at the lattice point ⃗x and ending at the lattice point ⃗y, with end-to-end displacement
vector ⃗R = ⃗y −⃗x, which is also a lattice vector. We desire an appropriate, and, we
hope, convergent expansion of such a function in a set of plane waves. As a ﬁrst
step toward achieving this goal, deﬁne the function f (⃗k) as follows:
f (⃗k) =

⃗R
F(⃗R)ei⃗k·⃗R
(2:S2-21)
The ﬁrst thing to note is that the function f (⃗k) is periodic in reciprocal space e.g.
f (⃗k + ⃗K) = f (⃗k)
(2:S2-22)
This follows from Equation (2:S2-8). Thus, knowledge of f (⃗k) in the ﬁrst Brillouin
zone sufﬁces to specify the functions for all values of ⃗k. Another important feature
of (2:S2-21) is that the equation can easily be inverted. To accomplish this, multiply
bothsidesoftheequationbye−i⃗k·⃗R′ andintegratethroughthevolumeoftheBrillouin
zone.

BZ
e−i⃗k·⃗R′ f (⃗k) d3k =

⃗R
f (⃗R)

BZ
ei⃗k·(⃗R−⃗R′) d3k
(2:S2-23)
Using the following result (see Ashcroft and Mermin, 1976; Ziman, 1979):

BZ
ei⃗k·(⃗R−⃗R′) d3k = 	BZδ(⃗R −⃗R′)
= (2π)3
	
δ(⃗R −⃗R′)
(2:S2-24)
allows one to insert (2:S2-21) and solve for F(⃗R). The pair of equations
F(⃗R) =
	
(2π)3

BZ
f (⃗k)e−i⃗k·⃗R d3k
(2:S2-25)
and
f (⃗k) =

⃗R
F(⃗R)ei⃗k·⃗R
(2:S2-26)
are the appropriate fourier representation for a function deﬁned on a Bravais lattice
speciﬁed by lattice vectors ⃗R. These equations are the generalization to the fourier
transform equations for functions deﬁned throughout all space.

46
Generating functions I
2.3.3 Plane wave expansions; ﬁnite lattices
Our interest in ﬁnite size lattices is not purely academic. Walks conﬁned to ﬁnite
regions of space have wide application to the properties of polymers and will occupy
our attention at a later point. The role of boundary conditions is an important feature
of constrained walks and could signiﬁcantly alter their statistical properties, but a
discussion of these matters is best deferred until we are confronted with a problem.
Here, as a way of establishing an appropriate language useful for analyzing the
behavior of walks with bounding surfaces, we introduce lattices of ﬁnite extent
through the artifact of invoking periodic boundary conditions. The ﬁnite lattice is
imagined to be in the shape of a large parallelepiped of edges N1a1, N2a2, and
N3a3, with a total number of lattice points N0 = N1N2N3.
The function F(⃗R) is now required to be periodic in the displacements N1⃗a1,
N2⃗a2, and N3⃗a3. Hence,
F(⃗R) = F(⃗R + N1⃗a1)
= F(⃗R + N2⃗a2)
= F(⃗R + N3⃗a3)
(2:S2-27)
Plane waves of the type ei⃗k·⃗R are again natural candidates, but, in order to guarantee
periodicity, the wave-vector ⃗k no longer varies continuously within the Brillouin
zone. It takes on discrete values. Indeed, the boundary conditions require
⃗k · Ni ⃗ai = 2πmi, i = 1, 2, 3
(2:S2-28)
where the mi takes on integral values.
The vector ⃗k satisfying the above constraints can be written in terms of the
reciprocal basis set as follows
⃗k = m1
N1
⃗b1 + m2
N2
⃗b2 + m3
N3
⃗b3
(2:S2-29)
Vectors of this sort generate a lattice with primitive basis vectors b1/N1, b2/N2,
and b3/N3, and primitive cell volume 	k = 	BZ/N0. There are exactly N0 ⃗k points
within the Brillouin zone. The density of these points is
ρk = 1
	k
= N0
	BZ
= N0	
(2π)3
=
V
(2π)3
(2:S2-30)

2.4 Supplement 3
47
In the above, V is the volume of the direct lattice. For macroscopically large lattices
the number N0 is of the order of Avogadro’s number. The ⃗k points are inﬁnitesimally
close and the wave-vector ⃗k can therefore be treated as a continuous variable, with
	
⃗k →

ρk d3k. Keeping this in mind, the appropriate plane wave expansion for
functions deﬁned on a lattice with periodic boundary conditions are the pair of
equations
F(⃗R) = 1
N0

⃗k
f (⃗k)ei⃗k·⃗R
(2:S2-31)
f (⃗k) =

⃗R
F(⃗R)e−i⃗k·⃗R
(2:S2-32)
with the discretely valued wave-vector ⃗k given by (2:S2-29).
Before ending the discussion, one further point needs mentioning, and that is
that in order to invert the above equation and solve for the fourier coefﬁcient f (⃗k)
in (2:S2-32) we made use of the property

⃗R
ei⃗k·⃗R = N0δ⃗k,0
(2:S2-33)
The reader can most directly see that this relation is true by explicitly doing the
sums, which we rewrite as
N1−1

n1=0
N2−1

n2=0
N3−1

n3=0
e
2πi
 m1
N1 n1+ m2
N2 n2
m3
N3 n3

=
 1 −e2πm1i
1 −e2πim1/N1
  1 −e2πm2i
1 −e2πim2/N2
  1 −e2πm3i
1 −e2πim3/N3

(2:S2-34)
Since m1, m2, and m3 are integers not equal to N1, N2, and N3, respectively, the
sum is equal to zero unless m1 = m2 = m3 = 0, in which case it is just the number
of lattice points, N0. Thus we recover (2:S2-33).
2.4 Supplement 3: asymptotic coefﬁcients of power series
Once we have managed to calculate a generating function, we need to invert the
process. That is, we need to determine the coefﬁcients in the power series. Recall the
relationship between the generating function G(z; ⃗x, ⃗y) and C(N; ⃗x, ⃗y), the latter
quantity being the number of N-step walks starting out at point ⃗x and ending up at
point ⃗y:
G(z; ⃗x, ⃗y) =
∞

N=0
zN C(N; ⃗x, ⃗y)
(2:S3-1)

48
Generating functions I
This means that, given G(z; ⃗x, ⃗y), we can ﬁnd C(N; ⃗x, ⃗y) if we know how to extract
the coefﬁcient of zN in the expansion of G(z; ⃗x, ⃗y) as a power series in z.
In most cases of interest the functional dependence of G(z; ⃗x, ⃗y) on z will not be
sufﬁciently simple that the behavior of coefﬁcients in the power series expansion
is immediately evident. In this supplement we will look at a few cases that turn
out to be particularly important for applications to random walk problems. We will
also introduce the method of steepest descents, a remarkably powerful technique
that allows us to extract the leading behavior of high order coefﬁcents in the power
series expansion of a large class of functions.
2.4.1 Simple pole
Let’s start with one of the simplest examples of a function with an inﬁnite power-
series expansion in z: f (z) = 1/(zc −z). If |z| < |zc|, we have
1
zc −z = 1
zc

1 + z
zc
+
 z
zc
2
+ · · ·

= 1
zc
∞

n=0
 z
zc
n
(2:S3-2)
so the coefﬁcient of zN is z−(N+1)
c
.
2.4.2 Two or more simple poles
Now, suppose zc and zd are both real, positive numbers and zc < zd. Furthermore,
let
f (z) =
a
zc −z +
b
zd −z
(2:S3-3)
Then, if z is suﬁciently small(z < zc)
f (z) = a
zc
∞

n=0
 z
zc
n
+ b
zd
∞

n=0
 z
zd
n
(2:S3-4)
so the coefﬁcient of zN is cN = az−(N+1)
c
+ bz−(N+1)
d
.
We can also write
cN = az−(N+1)
c

1 + b
a
zc
zd
N+1

2.4 Supplement 3
49
Now, let zc = zd(1 −
), where 
 > 0. Then
cN = az−(N+1)
c

1 + b
a (1 −
)N+1

= az−(N+1)
c

1 + b
a e(N+1) ln(1−
)

= az−(N+1)
c

1 + b
a e−δ(N+1)

(2:S3-5)
where δ = −ln(1 −
) and δ > 0. As N gets larger and larger the second term in
brackets in (2:S3-5) vanishes exponentially. Thus, for very large N the coefﬁcient of
zN in a/(zc −z) + b/(zd −z) is essentially equal to the coefﬁcient of a/(zc −z).
We will eventually generalize this result as follows:
If the functions fa(z) and fb(z) have poles or branch points at za and zb, respectively,
and if zb > za > 0 (za and zb both real), then, when N is large, the coefﬁcient of
zN in Afa(z) + B fb(z) is, for all practical purposes, equal to the coefﬁcient of zN
in Afa(z).
2.4.3 Higher order poles and branch points
What about the more general case f (z) = (zc −z)−α, where the exponent α need
not be an integer? One way of ﬁnding the coefﬁcient of zN is to use the binomial
expansion formula. Another way is the use the following identity:
 ∞
0
t Ae−xt dt = x−A−1(A + 1)
(2:S3-6)
where (A) is the gamma function. When A is an integer, n, (n + 1) = n!. With
the use of (2:S3-6) we have
(zc −z)−α =
1
(α)
 ∞
o
tα−1e−t(zc−z) dt
(2:S3-7)
To ﬁnd the coefﬁcient ofzN in (zc −z)−α we expand the right hand side of (2:S3-7)
with respect to z. The coefﬁcient of zN in that expansion is
1
(α)
1
N!
 ∞
0
tα−1+Ne−tzc dt =
1
(α)
1
N!z−(N+α)
c
(α + N)
=
(α + N)
(α)(N + 1)z−(N+α)
c
(2:S3-8)

50
Generating functions I
Now, we use Stirling’s formula for the gamma function of a large argument
(N) = exp[(N −1) ln(N −1) −(N −1)]
N ≫1
(2:S3-9)
When N is large, the coefﬁcient of interest is
z−N−α
c
(α) exp[(α + N −1) ln(α + N −1) −(α + N −1) −N ln N + N]
= z−N−α
c
(α) e(α−1) ln(N)
= z−N−α
c
(α) N α−1
(2:S3-10)
2.4.4 Logarithmic singularities
One more complication: suppose the function is of the form −(zc −z)−α ln(zc −z).
We obtain the coefﬁcient of zN by noting that this function is just d/dα(zc −z)−α.
Taking the derivative with respect to α of the last term in (2:S3-10), we have for
the desired coefﬁcient
d
dα
z−N−α
c
(α) N α−1 = ln(zc)z−N−α
c
N α−1
(α)
−′(α)z−N−α
c
(α)2
N α−1 + ln(N)z−N−α
c
N α−1
(α)
= ln(N)z−N−α
c
(α) N α−1

1 + O

1
ln(N)

(2:S3-11)
By the same token, we can ﬁnd the coefﬁcient of zN in −(zc −z)−α/ ln(zc −z) by
extracting the coefﬁcient of zN in the integral
 ∞
−α
(zc −z)y dy
(2:S3-12)
Using (2:S3-10) to ﬁnd the coefﬁcient of zN in the integrand, one is left with
 ∞
−α
y−N+y
c
(−y) N y−1 dy = −N y−1
ln N
y−N+y
c
(−y)

∞
−α
−
1
ln N
 ∞
−α
N y−1 d
dy

y−N+y
c
(−y)

dy
= N α−1y−N−α
c
ln(N)(α)

1 + O
 1
ln N

(2:S3-13)
The ﬁrst equality in (2:S3-13) results from an integration by parts. A further inte-
gration by parts establishes the second equality.

3
Generating functions II: recurrence, sites visited, and
the role of dimensionality
We’ve now had an introduction to the random walk. There has also been an intro-
duction to the generating function and its utilization in the analysis of the process.
Here we investigate some aspects of the random walk, both because they are in-
teresting in their own right and because they further demonstrate the usefulness
of the generating function as a theoretical technique in discussing this problem. In
particular, we will apply the generating function method to study the problem of
“recurrence” in the random walk. That is, we will address the question of whether
the walker will ever return to its starting point, and, if so, with what probability. We
will ﬁnd that the spatial dimensions in which the walk takes place play a crucial role
in determining this probability. Using an almost identical approach, the number of
distinct points visited by the walker will also be determined.
We present the two studies below as a display of the power of the generating
function technique in the study of the random walk process. For the skeptic, other
examples will be provided in subsequent chapters.
3.1 Recurrence
We begin this chapter with a discussion of the question of recurrence of an un-
restricted random walk. We will utilize the generating function, and an important
statistical identity, to see how the dimensionality of the random walker’s environ-
ment controls the probability of its revisiting the site from which it has set out. This
study complements our earlier discussion and provides evidence for the power of
the generating function approach.
3.2 A new generating function
A useful – but apparently little known1 – quantity enables one to obtain some key
results with remarkably little effort. This quantity is an expanded version of the
1 Little known among practising solid state physicists, that is.
51

52
Generating functions II
generating function we’ve been utilizing, and it refers to a walk that may or may
not visit a special site. Suppose the quantity C(N, M; ⃗x, ⃗y, ⃗w) is the number of
N-step walks that start at the location ⃗x, end at the location ⃗y and visit the site at
location ⃗w exactly M times in the process of getting from ⃗x to ⃗y. The quantity of
interest is
C′(N, t; ⃗x, ⃗y, ⃗w) =
∞

M=0
C(N, M; ⃗x, ⃗y, ⃗w)(1 −t)M
(3.1)
Clearly, terms in the sum on the right hand side of the above equation for which
M > N + 1 will not count, as there is no walk that visits a site more times than
it leaves footprints. That is C(N, M; ⃗x, ⃗y, ⃗w) = 0 if M > N + 1. We obtain the
coefﬁcients of the power series expansion in (1 −t)n that produces this generating
function in the standard way. It is easy to show that
C(N, M; ⃗x, ⃗y, ⃗w) = (−1)M 1
M!
dM
dt M C′(N, t; ⃗x, ⃗y, ⃗w)

t=1
(3.2)
Note that in this case the value to which the expansion parameter is set is not zero,
but one.
3.3 Derivation of the new generating function
We derive the new generating function by introducing a weighting factor. The
weighting factor W has the following form for an N-step walk that visits the site
si at the ith step
W =
N

i=1
w(si)
(3.3)
where factor w(si) is given by
w(si) = 1 −tδsi,S
(3.4)
Here, S is the special site of interest. The overall weighting factor for a given walk
is then
N

i=1

1 −tδsi,S

(3.5)
Suppose t = 1. Then, the weighting factor will have the effect of excluding any
walk that visits the special site S; all other walks will have a weighting factor of
one. This means that if we multiply all walks by the weighting factor above, set
t = 1 and sum, we will obtain the number of walks that never visit the site S. In the
case of N-step walks that start at ⃗x and end up at ⃗y, this is just C(N, 0; ⃗x, ⃗y, ⃗w),

3.3 Derivation of the new generating function
53
where ⃗w is the position vector of the site S. Suppose we take the derivative of the
weighted sum with respect to t, and then set t = 1. In that case, we will end up
with −1 times the number of walks that visit the site only once. Next, take the nth
derivative of the weighted sum over walks with respect to t, multiply by 1/n!, and
then set t = 1. This yields (−1)n times the number of walks that visit the special
site exactly n times. This is because each derivative generates a factor equal to
−δs j,S and all terms that “escape” the derivative become (1 −δsk,S) when t = 1.
The factor 1/n! compensates for the n! ways in which the n derivatives with respect
to t operate on the product in (3.5).
Now, we can evaluate the weighted walk by expanding the weighting factor,
(3.5), in powers of t.2 At ﬁrst order we generate the quantity
−t
N

i=1
δsi,S
(3.6)
When walks are multiplied by this weighting factor and summed, we end up with
−t times the sum of all N-step walks that visit the site S at one step, with no
restriction on what happens either before or after that step. At second order in the
expansion we have
t2
N

i=1
i−1

j=1
δsi,Sδs j,S
(3.7)
When walks are multiplied by this second order weighting factor and summed, we
end up with t2 times the sum of all N-step walks that visit the site S twice with
no restriction on what happens before, after or between those two visitations. A
graphical representation for the expansion in t of the new generating function is
shown in Figure 3.1. If the starting and end-point of the walk are ﬁxed, and if the
site in question is at the location v, then the sum in Figure 3.1 is given by
C(N; ⃗x, ⃗y) −t

N1+N2=N
C(N1; ⃗x, ⃗w)C(N2; ⃗w, ⃗y)
+ t2

N1+N2+N3=N
N2≥1
C(N1; ⃗x, ⃗w)C(N2; ⃗w, ⃗w)C(N3; ⃗w, ⃗y) + · · ·
(3.8)
The inequality that applies to N2 in the sum above simply requires the walker to
take at least one step before revisiting the site at ⃗w. Otherwise, we would count
zero step “walks” in the sum.
2 We are thus treating t as if it were a small quantity and expanding in it. This technique will be used later
for a different expansion parameter in the case of self-avoiding walks, in which case we will generate a virial
expansion.

54
Generating functions II
+t2
-t3
-t
Fig.3.1. Graphicalrepresentationofthevirialexpansionofthegeneratingfunction
deﬁned in (3.1). The large dots in the diagrams lie at the location ⃗w.
Now, we take the step of multiplying our new function by zN and summing. This
has the effect of removing the convolution over the Ni’s, and we obtain
G(z, t; ⃗x, ⃗y, ⃗w) =
∞

N=0
zNC′(N, t; ⃗x, ⃗y, ⃗w)
= G(z; ⃗x, ⃗y) −tG(z; ⃗x, ⃗w)G(z; ⃗w, ⃗y)
+ t2G(z; ⃗x, ⃗w)G1(z; ⃗w, ⃗w)G(z; ⃗w, ⃗y) + · · ·
= G(z; ⃗x, ⃗y) −t G(z; ⃗x, ⃗w)G(z; ⃗w, ⃗y)
1 + tG1(z; ⃗w, ⃗w)
(3.9)
The subscript 1 in G1(z; ⃗w, ⃗w) indicates that it is a generating function for walks
of at least one step.
The quantity
(−1)M 1
M!
dM
dt M G(z, t; ⃗x, ⃗y, ⃗w)

t=1
= (−1)M 1
M!
dM
dt M

G(z; ⃗x, ⃗y) −t G(z; ⃗x, ⃗w)G(z; ⃗w, ⃗y)
1 + tG1(z; ⃗w, ⃗w)

t=1
(3.10)
is, then, the generating function for all walks that start at ⃗x, end up at ⃗y and visit
the site at ⃗w exactly M times.
Exercise 3.1
Fill in the steps in the derivation of (3.9).

3.4 Dimensionality and the probability of recurrence
55
3.4 Dimensionality and the probability of recurrence
Our result allows us to determine whether or not a random walk is recurrent. If it
is, then almost all long random walks that start out at a given point will revisit that
point. If it is not, then only a ﬁnite fraction of those walks do so, and the walk is
called transient. Recurrent walks are also referred to as “persistent.” P´olya (1919)
was the ﬁrst to demonstrate that walks occurring in one and two dimensions return
to their starting point with absolute certainty, if they consist of an inﬁnite number of
steps and that in higher dimensions the walker has a non-zero probability of never
revisiting its starting point, no matter how long its walk. Let’s see what (3.9) tells
us about the recurrence of random walks on a lattice. We want to know how many
of the walks that start out at ⃗x revisit their point of origin, so we set ⃗w in (3.9) equal
to ⃗x. The generating function takes the form
G(z; ⃗x −⃗y) −t G1(z; 0)G(z; ⃗x −⃗y)
1 + tG1(z; 0)
= G(z; ⃗x −⃗y)
1 + tG1(z; 0)
(3.11)
The reason for the subscript 1 in the numerator of the second term on the left hand
side of (3.11) is that we want to count only those walks that take at least one step
from the starting point at x before revisiting that point. Otherwise, we count walks
that “revisit” their point of origin after zero steps.
To ﬁnd out how many N-step walk start out at ⃗x and end up at ⃗y, never having
revisited ⃗x, we need to calculate the coefﬁcient of zN in the function
G(z; ⃗x −⃗y)
1 + G1(z; 0) = G(z; ⃗x −⃗y)
G(z; 0)
(3.12)
The right hand side of the equality in (3.12) follows from the fact that the contri-
bution to the generating function G(z; 0) of walks consisting of no steps is exactly
one, by convention.
Let’s be even less restrictive and ask how many of the walks that start out at ⃗x and
end up anywhere never revisit the starting point ⃗x. We simply sum the expression
above over all possible end-points ⃗y – excluding ⃗x – and see what we have. Using
the relation between G(z; ⃗x −⃗y) and its spatial Fourier transform, g(z; ⃗q) we have

⃗y̸=⃗x
G(z; ⃗x −⃗y)
G(z; 0)
= g(z; 0)
G(z; 0) −G(z; 0)
G(z; 0)
= g(z; 0)
G(z; 0) −1
(3.13)
where we have used the identity
g(z, 0) =

⃗y
G(z, ⃗x −⃗y)
(3.14)

56
Generating functions II
for the Fourier coefﬁcient g(z, ⃗q). From Chapter 2, we know that
g(z, ⃗q = 0) =
1
1 −zχ(⃗q = 0)
≡
1
1 −z/zc
(3.15)
The last line of (3.15) serves as a deﬁnition of the quantity zc. This tells us that the
number of N-step walks that start out at ⃗x and end up anywhere is the coefﬁcient of
zN in (1 −z/zc)−1, while the number of walks that start at ⃗x and end up anywhere,
not having ever revisited the point of origin, ⃗x, is the coefﬁcient of zN in
zc
zc −z
1
G(z; 0)
(3.16)
As it turns out, the z-dependence of the generating function G(z; 0) in two dimen-
sions or less is different in a very important respect from its behavior in three and
higher dimensions. This will lead to fundamentally different results for the “recur-
rence” of walks in two and one dimensions from what we will ﬁnd in the case of
three-dimensional walks.
Worked-out example
Compute the probability that the walker never revisits its starting point when the
walk takes place in one dimension.
Solution
First, note that in one dimension χ(q) = eiq + e−iq. This means that
zc = 1/χ(q = 0)
= 1/2
(3.17)
For one-dimensional walks
G(z, 0) = 1
2π
 π
−π
dq
1 −z

eiq + e−iq
=
 2π
0
dq
1 −z

eiq + e−iq
(3.18)
The integral reduces to an contour integral around the unit circle if one makes the
change of variables α = eiq. The integral in (3.18) is, then
G(z, 0) = 1
2π
	
dα
iα

1 −z

α + α−1
= −
1
2πiz
	
dα
α2 −α/2 + 1/z
(3.19)
The path of integration in (3.19) is counter-clockwise around the unit circle.

3.4 Dimensionality and the probability of recurrence
57
The poles of the integrand are located at
α± = 1
2



1
z ±
1
z
2
−4



(3.20)
Since z < 1, α−is the only pole lying inside the unit circle. Using Cauchy’s formula
we ﬁnd
G(z, 0) = −2πi
2πiz

1
α−−α+

=
1
√
1 −4z2
=
zc
√zc −z√zc + z
(3.21)
The behavior of interest is related to the analytic properties of the function of z
that will be constructed in the vicinity of z = zc. If we insert our result for G(z, 0)
into the expression (3.13) for the number of walks that do not revisit their point of
origin we ﬁnd that the number of such N-step walks is the coefﬁcient of zN in
g(z; 0)
G(z; 0) →
zc
zc −z
√zc −z
zc
=
1
√zc −z
(3.22)
The next step is to extract the coefﬁcient of zN in the expression on the last line of
(3.22). Making use of the results derived in Supplement 3 of Chapter 2, we ﬁnd for
that coefﬁcient
z−N
c
(1/2) N −1/2
(3.23)
The total number of N-step walks goes as z−N
c
(the coefﬁcient of zN in the expansion
of zc/(zc −z)). Dividing by this quantity we ﬁnd that the probability that a one-
dimensional walker “escapes” without revisiting the point from which it started
is
N −1/2
1
(1/2)
(3.24)
which is vanishingly small in the limit N →∞.
Exercise 3.2
Consider a biased walker, for which the probability p, of a step to the right is not
equal to 1/2. Show in this case that the probability of a walker’s not revisiting its
site of origin is not equal to zero in the limit of an inﬁnite number of steps.

58
Generating functions II
Exercise 3.3
In the case above, show that the probability of escape (that is, the probability of
never returning to the point of origin) is equal to one when p = 1. There is an
obvious intuitive argument for this result, but the solution desired is based on an
analysis of the formula for the probability of return utilizing generating functions.
Exercise 3.4
In the case of the biased one-dimensional walk presented in Exercise 3.2, ﬁnd the
general formula for the probability of eventual return to the site of origin when
p ̸= 1/2.
3.5 Recurrence in two dimensions
In the above worked-out example, we found that a one-dimensional walker that has
taken enough steps will inevitably revisit its point of origin. In fact, it is possible to
show that if the one-dimensional walker takes enough steps, it will eventually visit
any given site. This will also turn out to be the case in two dimenensions. And it is
to two dimensions that we now turn.
As in the case of the one-dimensional walk, the key to the calculation of the
probability of eventual return to the point of origin for walks in two dimensions
is the extraction of the coefﬁcient of zN in the power-series expansion of the right
hand side of (3.13). As already noted, when N is large, the dominant contribution
comes from the ﬁrst term on the right hand side of (3.13), because of the singular
behavior of g(z; 0). First, recall that g(z; 0) = zc/(zc −z) (see (3.15)). In the case of
a walk on the d-dimensional version of a simple cubic lattice with nearest neighbor
distance equal to one,3 χ(⃗q) →2d as ⃗q →0. Thus, the above relationship holds
with zc = 1/2d.
The quantity of interest is, then, the coefﬁcient of zN in
zc
(zc −z)
1
G(z; 0)
(3.25)
In general
G(z; 0) =
1
(2π)d

ddq
1 −zχ(⃗q)
(3.26)
where
χ(⃗q) = c1 −c2|⃗q|2 + O(|⃗q|4)
(3.27)
3 In two dimensions, this is a square lattice.

3.5 Recurrence in two dimensions
59
where c1 = 2d and c2 = 1/2 for a simple cubic lattice. Truncating the structure
function χ(⃗q) to lowest order in |⃗q| allows us to integrate over the angular coordi-
nates in q-space, yielding the following simpliﬁed form for G(z; 0)
G(z; 0) =
1
(2π)d

1
c1
ddq
zc −z +

c2/c2
1

|⃗q|2
=
Kd
(2π)d

1
c1
qd−1 dq
(zc −z) + c2q2/c2
1
(3.28)
with the quantity Kd being the result of the angular integration, given by
Kd = 2πd/2
(d/2)
(3.29)
Exercise 3.5
Equation (3.29) follows from the equation for the area, A, of a d-dimensional sphere
of radius r:
A = Kdrd−1
Derive (3.29) by evaluating the integral for the surface area of such a sphere:
A =

· · ·

δ

x2
1 + · · · + x2
d −r

dx1 dx2 . . . dxd
=

· · ·

2rδ

x2
1 + · · · + x2
d −r2
dx1 dx2 . . . dxd
The result of the integral over q = |⃗q| clearly depends on the dimensionality of
q-space. When d = 2, we have
G(z; 0) ∝
 q0
0
q dq
zc −z + c2q2/c2
1
= c2
1
2c2
ln
zc −z + c2q2
0/c2
1
zc −z

.
(3.30)
Equation (3.30) tells us that, as z →zc, G(z; 0) ∝−ln(zc −z), so the number of
two-dimensional non-recurring random walks is proportional to the coefﬁcient of
zN in 1/(zc −z) ln(zc −z). The asymptotic form of this coefﬁcient is worked out

60
Generating functions II
zc − δ
z-plane
Fig. 3.2. The path of steepest descents.
in Supplement 3 of Chapter 2. Here, we will show how it can be obtained with the
use of steepest descents.4
The coefﬁcient of interest is given by Cauchy’s formula as
1
2πi
	
1
(zc −z) ln(zc −z)
dz
zN+1 ≡
	
h(z)e f (z) dz
where h(z) = 1/ ln(zc −z), and f (z) = −ln(zc −z) −(N + 1) ln(z). The function
f (z) has an extremum on the positive real axis at z = zc(1 −1/N) = zc −δ, with
f ′′(zc) = 1/δ2 > 0. Thus, the contour of integration can be nicely distorted to
pass through the steepest descents path which, for the case at hand, is parallel to
the imaginary axis passing through the extremum point zc −δ (see Figure 3.2),
yielding a value of a constant times z−(N+1)
c
/ ln N for the integral.
The total number of walks leaving ⃗x is equal to z−N
c
. The number of non-recurring
walks is smaller than the total number of walks by the factor 1/ ln N. As N →∞,
non-recurring walks become an inﬁnitesimally small subset of all walks.
3.6 Recurrence when the dimensionality, d, lies between 2 and 4
When the dimensionality falls between 2 and 4 we can reduce the integral for
G(z; 0) as follows:
4 See the supplement at the end of Chapter 1.

3.6 Recurrence when d is between 2 and 4
61
G(z; 0) ∼
 q0
0
qd−1 dq
zc −z + c2q2/c2
1
(3.31a)
= c2
1
c2
 q0
0
qd−3
zc −z + c2q2
c2
1

dq
zc −z + c2q2
c2
1
−c2
1
c2
 q0
0
(zc −z)qd−3 dq
zc −z + c2q2/c2
1
(3.31b)
= c2
1
c2
qd−2
0
d −2 −c2
1
c2
 ∞
0
(zc −z)qd−3 dq
zc −z + c2q2
c2
1
+ c2
1
c2
 ∞
q0
(zc −z)qd−3 dq
zc −z + c2q2/c2
1
(3.31c)
The reason that (3.31a) does not contain an equality is that we have approximated
the denominator in the integral by the two lowest powers of q that appear in it. This
tells us in general how the integral behaves, and we are able to properly reproduce
the leading order singularity structure of G(z; 0) at z = zc. However, the function
itself does not emerge from the integration with complete accuracy.
In the second integral in (3.31c) we can rescale the integration variable q by
deﬁning q = Qa(zc −z)1/2/c1/2
2 . Furthermore, let the ﬁnal contribution to (3.31c)
be equal to R(zc −z). Then,
G(z; 0) ∝c2
1
c2
qd−2
0
d −2 −
a2
c2
d/2
(zc −z)(d−2)/2
 ∞
0
Qd−3 dQ
1 + Q2 + R(zc −z)
(3.32)
The result on the right hand side of (3.32) tells us that the generating function
G(z; 0) behaves non-analytically as a function of the fugacity z. There is, to be
precise, a mathematical singularity in G(z; 0) at z = zc. However, when 2 < d < 4,
the singularity appears as a positive, but non-integral, power of the quantity zc −z.
The ﬁrst derivative with respect to z of the generating function is inﬁnite at z = zc.
On the other hand, when 2 < d < 4, the quantity G(zc; 0) is ﬁnite. The number of
walks that start at ⃗x and end up anywhere, never having revisited the point of origin
is then, in the limit of very large N, equal to the coefﬁcient of zN in the power-series
expansion of
zc
zc −z
1
G(zc; 0)
(3.33)
That coefﬁcient is z−N
c
/G(zc; 0). Dividing by the total number of N-step walks, we
ﬁnd that the probability of a walker’s starting out at ⃗x and ending up anywhere –
never having revisited the point of origin – is equal to G(zc; 0)−1.
In order to obtain precise results for this probability, we must attempt an accurate
evaluation of the quantity G(zc; 0).

62
Generating functions II
Exercise 3.6
In the vicinity of z = zc, we now know from (3.32) that a walk in three dimensions
will have a generating function G(z; 0) of the form
G(z; 0) = B0 −B1(zc −z)1/2
This means that to leading order in N, the number of walks that never revisit their
point of origin is a ﬁnite fraction of all walks that start at that same point. Use the
above expression to ﬁnd the most signiﬁcant correction to this leading order result
when N is large but ﬁnite.
3.7 The probability of non-recurrence in walks on different cubic lattices
in three dimensions
As noted above, a walker on a three-dimensional lattice will, with a probability that
is not equal to zero, “escape” to inﬁnity without ever having revisited the lattice
point from which it started. We can make use of the formulas that we now have at
our disposal to ﬁnd out what this probability is in the case of walkers on various
lattices. We will concentrate here on the three types of cubic lattices that were
introduced earlier in this book: the simple cubic lattice, the body-centered cubic
lattice and the face-centered cubic lattice.
Recall what we just found for the probability that an N-step walker, having left
its point of origin, will never revisit that point. This probability is equal to
G(zc; 0)−1
(3.34)
Now,
G(zc; 0) =
1
(2π)d

ddq
1 −zcχ(⃗q)
=
1
(2π)d

ddq
1 −χ(⃗q)/χ(0)
(3.35)
The structure factor χ(⃗q) is, in the three lattice cases we will talk about here, as
given below.
r Simple cubic (sc)
χ(⃗q) = 2

cos qx + cos qy + cos qz

(3.36)
r Body-centered cubic (bcc)
χ(⃗q) = 8 cos qx cos qy cos qz
(3.37)

3.8 Number of sites visited by a random walk
63
r Face-centered cubic lattice (fcc)
χ(⃗q) = 4

cos qx cos qy + cos qy cos qz + cos qx cos qz

(3.38)
The three-fold integrations that lead to the probabilities of escape without return
to the starting point are now known as Watson integrals (Watson, 1939). Their
evaluation is detailed in the literature (Hughes, 1995). The results of the integrations
can be expressed in terms of gamma functions and elliptic integrals. After some
highly non-trivial analysis, one obtains the numerical results
Gsc(zc; 0) ≈1.516 386 . . .
(3.39)
Gbcc(zc; 0) ≈1.393 203 . . .
(3.40)
Gfcc(zc; 0) ≈1.344 661 . . .
(3.41)
This means that the probability that a walker on a simple cubic lattice will never
revisit its point of origin is approximately 0.659 463. The corresponding proba-
bilities for the body-centered and face-centered cubic lattices are ≈0.71 777 and
≈0.743 682, respectively.
Exercise 3.7
In the case of a three-dimensional simple cubic lattice, construct an expression
for the fraction of walks that escape without revisiting their point of origin. If the
expression is in terms of an integral over q-space, it is not necessary to perform the
integration.
Exercise 3.8
In the case of a three-dimensional simple cubic lattice, construct an expression for
the fraction of walks that escape after visiting the point of origin exactly m times.
Call that fraction fm. Show that your expression leads to the equality
∞

m=0
fm = 1
3.8 The number of sites visited by a random walk
Here we will apply some of the results just derived to see how many distinct sites
are visited by a random walker, on average. To do this, we will start by ﬁnding out
what fraction of all the walks of n steps end up on a site that was never visited
before. We will then use the answer to this question to ﬁnd out how many new
sites are visited by a walker of arbitrary length. The connection between the two

64
Generating functions II
+
+
+...
Fig. 3.3. Graphical representation of the set of terms that tell us how many walks
that end up at a given site have never visited that site before.
quantities is fairly straightforward. Suppose a walker that has taken n steps ends
up, on average, on n sites that have never been visited before. This means that at
the nth step, n sites are, on the average, added to the total number visited by the
walker. To ﬁnd the total number of sites, we simply add up the new sites visited at
each step leading up to the last one. Thus, if δn is the total number of distinct sites
visited by the walker in n steps, then
δn =
n

n′=0
n′
(3.42)
Clearly, 0 = 1 and n = δn −δn−1. Now, the average number of new sites visited
at the nth step is just the probability that the walker has landed on a site not
previously visited. This probability is related to what is known as the probability of
ﬁrst passage (Barber and Ninham, 1970). To see how this goes, deﬁne Fn(⃗x −⃗y)
as the probability that the walker, starting at ⃗x, reaches ⃗y for the ﬁrst time at its nth
step. Then, the sum 
⃗y̸=⃗x Fn(⃗x −⃗y) is the probability that the walker reaches any
site for the ﬁrst time at the nth step. This is another way of saying that the walker
has reached a “new” site at the nth step. Thus,
n =

⃗y̸=⃗x
Fn(⃗x −⃗y)
(3.43)
The connection is thus established. The next stage of our calculation is the
determination of the probability, n. Here is how we do it. We make use of the
method that was utilized to calculate recurrence in a random walk to ﬁnd out how
many walks that end at a given site have never visited that site before. This quantity
is depicted in Figure 3.3. What we do is eliminate from the walks that end at the
site of interest all walks that have visited that site previously, in the same way as
we accomplished that elimination when we investigated the question of recurrence.

3.8 Number of sites visited by a random walk
65
Calling the site of origin A and the site at which the walk ends up B, the sum
represented by the diagrams in Figure 3.3 is given by
G(z; ⃗rB −⃗rA) −G(z; ⃗rB −⃗rA)G1(z; 0) + · · · = G(z; ⃗rB −⃗rA)
1
1 + G1(z; 0)
(3.44)
As previously noted, the quantity G1(z; 0) is related to the generating function
for walks that start and end at the same step. The difference between G1 and the
standard generating function is that there must be at least one step in G1. That is
G1(z; 0) =
∞

N=1
zNC(N; 0)
(3.45)
where C(N; 0) is the sum of all N-step walks that start and end at the same point.
This tells us that the spatial Fourier transform of G1 is given by5
g1(z; ⃗q) =
zχ(⃗q)
1 −zχ(⃗q)
(3.46)
It also tells us that
1 + G1(z; 0) = G(z; 0)
(3.47)
If we now sum over all possible end-points, we obtain the generating function for
all walks that end up at a point that has not been previously visited. Summing over
all end-points picks out the ⃗q = 0 Fourier transform of the generating function G,
which means that the generating function that tells us how many walks end at a
newly visited site is
g(z; 0)
1
1 + G1(z; 0) =
1
1 −zχ(0)
1
1 + G1(z; 0)
=
zc
zc −z
1
1 + G1(z; 0)
(3.48)
Now, let’s evaluate the function G1(z; 0). We have
G1(z; 0) =
1
(2π)d

zχ(⃗q)
1 −zχ(⃗q) ddq
=
1
(2π)d

1
1 −zχ(⃗q) ddq −
1
(2π)d

ddq
=
1
(2π)d

1
1 −zχ(⃗q) ddq −1
(3.49)
We’ve done this integral before. Using results that we have already, we ﬁnd the
following.
5 For convenience, the BZ is set equal to (2π)d.

66
Generating functions II
(1) In one dimension
G1(z; 0) ∝(zc −z)−1/2
(3.50)
(2) In two dimensions
G1(z; 0) ∝ln (zc −z)
(3.51)
(3) In three dimensions
G1(z; 0) ∝

ddq

1
1 −χ(⃗q)/χ(0) −1

−K (zc −z)1/2 + · · ·
(3.52)
The integral in (3.52), which represents the z = zc limit of the quantity G1, is ﬁnite.
In one and two dimensions, that limit yields an inﬁnite result for G1.
Let’s see what we get for the asymptotic behavior of the generating function for
the number of walks that end at a never-before-visited site. In one dimension, as
z →zc, the generating function of interest goes as
1
zc −z
1
(zc −z)−1/2 →(zc −z)−1/2
(3.53)
We ﬁnd (see Supplement 3 at the end of Chapter 2) that the coefﬁcient of zn in
(zc −z)1/2, when n is large, goes as a constant time √2/πn−1/2z−n
c . If we divide
by the number of all n-step walks, which goes strictly as z−n
c , then, we have a
probability of landing on a new site that decays away as n−1/2 for large n. To ﬁnd
the average number of sites visited in n steps, we sum over this quantity for all n′
up to n. The quantity of interest is, then,

2
π
n

n′=1

n′−1/2 = 2

2
π n1/2
(3.54)
We obtain the right hand side of (3.54) by approximating the sum on the left hand
side by an integral, which is reasonable as long as n is large.
In two dimensions, we ﬁnd the limiting behavior of the generating function for
walks that end at a newly visited site is
1
(zc −z) ln(zc −z)
(3.55)
Again, from Supplement 3 of Chapter 2, we ﬁnd the coefﬁcient of zn in this sum,
and we ﬁnd that when n is large, this coefﬁcient goes as πz−n
c / ln n. The sum that
tells us how many distinct sites are visited, on the average, by the two-dimensional
walker after n steps is, then, of the form
π
∞

n′=2
1
ln n′ = π n
ln n
(3.56)

3.8 Number of sites visited by a random walk
67
The lower limit of the sum has been set equal to two so as to avoid the singularity
at n′ = 1. This singularity is an artifact of the use of an expression that is asymptot-
ically correct for large argument in a regime in which it does not apply. The right
hand side of (3.56) follows from the replacment of the sum on the right hand side
by an integral, and then by an integration by parts. To be speciﬁc, we use
 n
1
dn′
ln n′ =
n′
ln n′

n
1
+
 n
1
dn′
(ln n′)2
=
n
ln n + · · ·
(3.57)
where the ellipsis in (3.57) represents terms that are asymptotically smaller than
the leading order one displayed there.
All this means that the number of distinct sites visited by a two-dimensional
walker grows as the number of steps taken, divided by the log of the number of
steps. When n is extremely large, this ratio is a negligible fraction of n. The results
derived here are in agreement with the asymptotic formulas for δn derived originally
with other methods (Montroll and Weiss, 1965).
Finally, in the case of the three-dimensional walker, the generating function of
interest has the form, in the vicinity of z = zc
zc
zc −z
1
G(zc; 0)
(3.58)
The coefﬁcient of zn for this walk goes as z−n
c /(G(zc; 0)). If we divide by the number
of all n-step walks, we end up with the n-independent ratio 1/(G(zc; 0)). This tells
us that the number of sites visited by an n-step walk goes as n/(G(zc; 0)). In one
and two dimensions, the number of distinct sites visited by a walker who goes on
a long walk is an inﬁnitesimal fraction of the total number of steps taken. In three
dimensions, the number of distinct sites visited is, asymptotically, a determinable
and ﬁnite fraction of the total number of steps taken in the course of the walk.
Though the results just obtained for recurrent walks and the number of distinct
sites visited can also be arrived at by other means (Barber and Ninham, 1970), our
goal was to demonstrate that these results follow relatively quickly and easily with
the use of generating functions.
Exercise 3.9
Find the mean number of distinct sites visited by an N-step walk on a body-centered
cubic lattice when N is very large. If your answer is in the form of an integral over
q-space, it is not necessary to evaluate that integral.

68
Generating functions II
Exercise 3.10
For the case of a biased one-dimensional walk, in which the probability of a step
to the right, p, is not equal to 1/2, ﬁnd the mean number of new sites visited by an
N-step walk when N is very large. The leading order term in your result will be
proportional to N. Can you justify the constant of proportionality qualitatively?
Exercise 3.11
In the case of an N-step walk on a three-dimensional, simple cubic lattice, the
next-to-leading order term in the result for the mean number of distinct sites visited
when N is very large goes as a power of N. What is that power?
Exercise 3.12
In the case of a simple cubic lattice, ﬁnd out how many sites are visited, approxi-
mately and on the average, in an N-step walk, where N is large.

4
Boundary conditions, steady state, and the
electrostatic analogy
The modiﬁcation of random walk statistics resulting from the imposition of con-
straintsonthewalkerswillberepeatedlyvisitedinthisbook.Infact,severalchapters
will be dedicated to the treatment of walkers that are forbidden to step on points
they have previously visited. This kind of constraint represents the inﬂuence of a
walk on itself. As we will see, calculations of the properties of the self-avoiding
random walk require the use of fairly sophisticated techniques. The payoff is the
ability to extract the fundamental properties of a model that accurately represents
the asymptotic statistics of important physical systems, particularly long linear
polymers.
The discussion referred to above will take place in later chapters. Here, we focus
on other constraints, which embody the inﬂuence of a static environment on a
walker. In the ﬁrst part of this chapter, we will address the effect on a random
walker of the requirement that it remain in a speciﬁed region of space. Speciﬁcally,
we will look at walkers conﬁned to a half-space, a linear segment in one dimension
and a spherical volume in three dimensions. Then we will look at the case of walkers
that are conﬁned to the region outside a spherical volume. This case turns out to
be relevant to issues relating to the intake of nutrients and the excretion of wastes
by a simple, stationary organism, such as a cell. In fact, the ﬁnal section of this
chapter utilizes random walk statistics to investigate the limits on the size of a cell
that survives by ingesting nutrients that diffuse to it from its surroundings.
In some important cases the study of the effects of environmental inﬂuences
on random walk statistics is facilitated if the walkers in question are members
of an ensemble that is continuously replenished at various points in the system.
Because of this replenishment, the number of such walkers at a given point does
not change with time. In other words, the system has achieved steady state. This
stratagem allows for the calculation of time-averaged quantities in many random
walk problems. We will derive the equation that is satisﬁed by the function that
describes the number of steady state walkers in a couple of ways. The equation
69

70
Boundary conditions, steady state, and the electrostatic analogy
is, in the approximations that are applicable here, equivalent to the equation for
the electrostatic potential due to a collection of charges. That is, the calculation of
the number of walkers in steady state is equivalent to the solution of the equation
for the electrostatic potential due to a set of charges. In addition, we will see how
boundaries that surround the region in which the walkers propagate can mimic
the effects of the boundaries enclosing the region containing static charges and
the electrostatic potential that they generate. This allows us to make use of the
powerful techniques that have been developed to calculate electrostatic potentials
(Jackson, 1999) to answer interesting questions about steady state random walks
and diffusion. Finally, we will make use of the mathematical connection between
steady state walks and electrostatics to look at diffusion in a region that surrounds
a compact volume, such as a sphere. We will obtain results that are relevant to the
process by which simple organisms absorb nutrients and eliminate waste.
4.1 The effects of spatial constraints on random walk statistics
The ﬁrst question we ask is: what is the change in the total number of walks when
a particular kind of barrier is present? Such a barrier will eliminate all walkers that
pass through it. It represents a particularly effective (if ruthless) means of keeping
walkers out of a “forbidden” region. This kind of partition has direct relevance
to the actual, physical, barrier that surrounds the region to which a polymer may
be conﬁned. As we will see, the real conﬁning surface that keeps a polymer in a
particular region has the same mathematical properties as the absorbing boundary
that does away with walkers who step across it.
4.1.1 The image walker
To be speciﬁc, imagine a set: of walkers who are free to wander in the half-space
x > 0. However, any walker that steps on the boundary at x = 0 ﬁnds its path
eliminated. One way of accomplishing this elimination is with the device of the
“image walker” (see Figure 4.1). This walker starts out at a point that is the mirror
image with respect to the x = 0 plane of the point of origin of the original walker.
Thewayinwhichonedeterminesthenumberofallowedwalksbetweentheconﬁned
walker’s starting point and some ultimate destination is by counting all paths that
the walker might have taken between those two points – including paths that enter
the forbidden region x ≤0 – and then subtracting the number of walks that image
walkers can take from their starting point to the same ﬁnal point. This subtraction
has the effect of eliminating all of the original walker’s paths that have passed
over the boundary. The way in which the elimination is achieved is illustrated in
Figure 4.1. The number of allowed walks, which we denote by C′(N; ⃗r, ⃗r0), is, then,

4.1 The effects of spatial constraints
71
starting point
ending point
image point
x=0
Fig. 4.1. Illustrating the way in which an image walker eliminates a path by the
original walker that passes over the boundary at x = 0. For every path which
crosses the boundary and returns to the ending point, there is a path starting at the
image point that can reach the same ending point in the same number of steps, N.
This is not the case for walks that do not cross the boundary. Note that the portion
of the image walker’s path that precedes its ﬁrst contact with the boundary is the
mirror image of the corresponding portion of the real walker’s path and that after
that ﬁrst contact the image walker shadows the real one.
given by:
C′(N; ⃗r, ⃗r0) = C(N; ⃗r −⃗r0) −C(N; ⃗r −⃗rI)
(4.1)
On the right hand side of (4.1), the function C(N; ⃗r −⃗r0) is the number of unre-
stricted N-step walks that start at the point of origination, ⃗r0, and that end at the
point ⃗r, while C(N; ⃗r −⃗rI) is the number of N-step walks starting at the image
point ⃗rI that end up at ⃗r. If ⃗r0 = (x0, y, z), then ⃗rI = (−x0, y, z). In terms of the
explicit expression that has been developed for the number of unrestricted walks in
three dimensions, we have for the quantity C′
C′(N; ⃗r, ⃗r0) = µN
2π N
3
−3/2 
e−3|⃗r−⃗r0|2/2N −e−3|⃗r−⃗rI|2/2N
(4.2)
The quantity µ is the connectivity constant. For an unconstrained walk on a simple
cubic (sc) lattice, µ = 6. It is relatively straightforward to verify that the effect of
the constraint that the walk remain in the region x > 0, as enforced by the image
walker, has little effect on the total number of walks unless the number of steps in
the walk, N, is sufﬁciently great that N >∼x2
0. We can make this a little more explicit
by summing over all possible y and z coordinates of the ﬁnal points. Using the fact
that the Gaussian function e−α(x2+y2+z2) can be written as a product of functions
depending on only one of the three coordinates, we ﬁnd that the integrals over y
and z of both contributions to the right hand side of (4.2) are readily evaluated. The
end result of this procedure is the following expression for the number of allowed

72
Boundary conditions, steady state, and the electrostatic analogy
walks starting at ⃗r0 and ending up anywhere in the plane x = xF
µN
2π N
3
−1/2 
e−3(xF−x0)2/2N −e−3(xF+x0)2/2N
(4.3)
Suppose we would like to assess the net effect of the absorbing barrier on the
number of walks. We can sum over all possible locations of the walker after it has
taken N steps. This is an integral over xF, from 0 to ∞. The result is
(N; x0) = µN
2π N
3
−1/2  ∞
0
dxF

e−3(xF−x0)2/2N −e−3(xF+x0)2/2N
(4.4a)
= µN
2π N
3
−1/2  2x0
0
e−3(xF−x0)2/2N dxF +
 ∞
2x0
e−3(xF−x0)2/2N dxF
−
 ∞
0
e−3(xF+x0)2/2N dxF

(4.4b)
= µN
2π N
3
−1/2  x0
−x0
dxe−3x2/2N
(4.4c)
(4.4c) follows from changes of integration variable in the terms in brackets in
(4.4b).
Let’s look at the ﬁnal expression on the right hand side of (4.4c). The integral
is an error function (Abramowitz and Stegun, 1970). If the limits of integration
are sufﬁciently large that the absolute value of the exponent in the integrand is
sizable, then the integral can be replaced by the complete Gaussian integral, and
the end result for the total number of walks is µN, just as if there were no barrier in
place. This limit is achieved under the condition x2
0 ≫2N. In other words, if the
absorbing wall is far enough away from the walker’s starting point that the walker
is unlikely to encounter the wall in the course of its wanderings, then the wall will
exert a relatively mild effect on the number of walks.
On the other hand, suppose that the opposite limit holds, x2
0 ≪2N. Then, the
right hand side of (4.4c) is well-approximated by the expression
µN
2π N
3
−1/2
× 2x0
(4.5)
In the limit that the number of steps, N, is large, the number of allowed walks
becomes a small fraction of the number of unrestricted walks. Dividing (4.5) by
the number of unrestricted walks, we ﬁnd that the reduction in the total number is

6x2
0/π N.
Finally, we can sum over all initial points, to see what effect the absorbing wall
has on the total number of walks that start anywhere. We start by recasting the right

4.1 The effects of spatial constraints
73
hand side of (4.4c) as follows:
µN
2π N
3
−1/2  x0
−x0
dxe−3x2/2N
= µN
2π N
3
−1/2  ∞
−∞
dxe−3x2/2N −2
 ∞
x0
dxe−3x2/2N

= µN −2µN
2π N
3
−1/2  ∞
x0
e−3x2/2N dx
(4.6)
The ﬁrst term on the right hand side of (4.6) is just the total number of unrestricted
walks that start out at x0. The second term represents the reduction in the total
number of walks that results from the presence of the absorbing barrier. We now
sum over all possible starting points, by integrating over x0, to determine the net
effect of the boundary on walks starting anywhere in the half-space x > 0. This
integration is readily performed
 ∞
0
dx0
 ∞
x0
e−3x2/2N dx = x0
 ∞
x0
e−3x2/2N dx
				
x0=∞
x0=0
+
 ∞
0
x0e−3x2
0/2N dx0
(4.7a)
= 1
2
 ∞
0
e−3y/2N dy
(4.7b)
= N
3
(4.7c)
(4.7a) represents the results of an integration by parts. To obtain (4.7b), we imple-
mented a change of integration variables.
Combining (4.7) with (4.6) we ﬁnd that the net reduction in the number of walks
is given by
µN
2N
3π
1/2
(4.8)
The absorbing wall has the effect of substantially eliminating all walks that start
within a distance ∼
√
N of it.
Integration of the ﬁrst term on the right hand of (4.6) yields the total number of
walks that there would be if no absorbing wall were present. We will see that the
reduction in the total number of walkers because of an absorbing wall can have
dramatic effects when the volume to which the walker is conﬁned is ﬁnite.
4.1.2 Conﬁnement to a one-dimensional region
The random walker may face further restrictions. There may be walls on more than
one side, forcing the walk to take place in a smaller portion of space. For example,

74
Boundary conditions, steady state, and the electrostatic analogy
a one-dimensional walker may ﬁnd itself conﬁned by absorbing walls to the region
0 ≤x ≤L. Two questions naturally arise in this case:
(1) What are the consequences of this conﬁnement?
(2) How does one perform calculations leading to the answer to the question above?
Clearly, a successful response to the ﬁrst query depends crucially on how well
one is able to address the second. It is tempting to consider the use of image
walkers to effect a cancellation of unwanted walks. In fact, this stratagem can be
implemented. Unfortunately, it is necessary to introduce an inﬁnite number of image
walkers in order to carry it out. As a result, one is faced with awkward summations.
A much more efﬁcient approach involves the use of the generating function, and
the introduction of an appropriate basis set of functions.
Recall that the generating function obeys the equation (2.9). This equation was
simpliﬁed by the introduction of the spatial Fourier transform of the generating
function. The transformed generating function was deﬁned in terms of plane waves,
ei⃗k·⃗r. These functions express in the simplest form the translational symmetry of
the conﬁguration. However, plane waves are not the most suitable basis set when
the environment is inconsistent with periodic boundary conditions. In the case at
hand, because of the presence of absorbing walls at x = 0 and x = L, the gener-
ating function must equal zero at those two boundaries.1 A set of functions that
incorporate the translational invariance of the recursion relation in the interior of
the allowed volume of space, and that also respect the action of the absorbing walls,
is the following
ψ(x, y, z) = eiky y+ikzz sin qx
(4.9)
The wave-vector component q is quantized according to the prescription
q = nπ
L ;
n = 1, 2, 3 . . .
(4.10)
If (4.10) is satisﬁed, then it is easy to demonstrate that the function ψ(x, y, z) is
zero at both x = 0 and x = L.
Now, let’s simplify the situation, and focus on a one-dimensional walker conﬁned
as above. The recursion relation is as presented in Chapter 1
G(z; x1, x2) = z

G(z; x2 + , x1) + G(z; x2 −, x1)

+ δx1,x2
(4.11)
Here,  is the length of the step taken. For a walker on a lattice,  is the lattice
spacing. Expanding the generating function in terms of the sine waves introduced
1 It is superﬁcially reasonable that the generating function is equal to zero at an absorbing barrier. The detailed
argument leading to this condition is simple but not entirely trivial. See the supplement at the end of this chapter;
for a more detailed discussion, see (Weiss, 1994).

4.1 The effects of spatial constraints
75
above, we ﬁnd that (4.11) is replaced by the following equation for the Fourier
amplitude g(z; q):
g(z; q) = 2z cos qg(z; q) +

2
L sin qx1
(4.12)
The L-dependent factor on the right hand side of (4.12) results from the normal-
ization of the sinusoidal mode. Solving this equation for g(z; q), we obtain
g(z; q) =

2
L
sin qx1
1 −2z cos q
(4.13)
To make life even simpler, we will expand the denominator in (4.13) in powers
of q and retain only those terms in the power-series expansion that are quadratic or
lower. This approximation is tantamount to replacing the difference equation (4.11)
by a partial differential equation; see Chapter 2, Equation (2.37). We then ﬁnd
g(z; q) =

2
L
sin qx2
(1 −2z) + z2q2
(4.14)
Reconstituting the generating function in real space, we ﬁnd
G(z; x1, x2) = 2
L
∞

n=0

sin
nπx1
L

sin
nπx2
L

1 −2z + z2 nπ
L
2
(4.15)
If this expression for G(z; x1, x2) looks familiar to you – it should. We already
demonstrated in Chapter 2 that the generating function satisﬁes, in the continuum
limit, a Green’s function-type equation, see (2.38). And, (4.15) is a solution to that
equation using eigenfunctions appropriate to absorbing-wall boundary conditions.
Exercise 4.1
Derive an expression for the generating function, G(z; x1, x2), of walkers conﬁned
to the half-space x > 0.
Exercise 4.2
Using the generating function obtained above show that (4.3) results.
Let’s use the result (4.15) to calculate the total number of one-dimensional walks
in the region 0 ≤z ≤L, by integrating over both the ﬁnal position of the walk, x1

76
Boundary conditions, steady state, and the electrostatic analogy
and the walk’s initial location x2. Making use of the result
 L
0
sin
nπ
L

dx = L
nπ (1 −cos nπ)
=
0
n = 2m
2L/nπ
n = 2m + 1
(4.16)
we ﬁnd the total number of walks in the conﬁned region is
8L
π2
∞

m=0
1
(2m + 1)2
1
1 −2z + z2((2m + 1) π/L)2
(4.17)
As we are interested in the total number of N-step walks, we extract the coefﬁcient
of zN in (4.17). This procedure generates the factor

2 −2
(2m + 1) π
L
2N
= exp

N ln

2 −2
(2m + 1) π
L
2
→2Nexp

−N2
(2m + 1) π
L
2
2

(4.18)
We then obtain the following result for the total number of one-dimensional random
walks that start anywhere – and that end anywhere – in the region 0 ≤x ≤L
2N 8L
π2
∞

m=0
1
(2m + 1)2 exp

−N2
(2m + 1) π
L
2
2

(4.19)
Exercise 4.3
Show where image walkers are placed in order to reproduce the effect of absorbing
boundaries on walkers conﬁned to the region 0 < x < L.
Exercise 4.4
Verify by explicit calculation that (4.12) follows from the substitution of a sine-wave
expansion of the generating function in (4.11).
4.1.3 Limiting cases
The expression (4.19) does not reduce to a closed form in terms of elementary
functions. However, it is possible to see what happens in interesting limits. For
instance, if the number of steps is sufﬁciently small that 2N/L2 ≪1, we can
replace the exponential in (4.19) by one, and we obtain for the total number of

4.1 The effects of spatial constraints
77
walks conﬁned to the region 0 ≤x ≤L
2N 8L
π2
∞

m=0
1
(2m + 1)2
(4.20)
Thesumin(4.20)isreadilyperformedifwetakeadvantageoftheresult(Gradshteyn
et al., 2000)
∞

n=1
1
n2 = π2
6
(4.21)
Then, we write the sum over odd integers as the difference between the sum over
all integers and the sum over even integers
∞

m=0
1
(2m + 1)2 =
∞

n=1
1
n2 −
∞

l=1
1
(2l)2
=
∞

n=1
1
n2 −1
4
∞

k=0
1
k2
= 3
4
∞

n=0
1
n2
= 3
4
π2
6
= π2
8
(4.22)
Inserting this result into (4.20), we obtain for the total number of N-step one-
dimensional walks in a sufﬁciently large domain
2N L
(4.23)
The total number of N-step walks in the domain is just equal to the total number
of N-step unrestricted walks multiplied by the volume of the domain. This latter
factor simply takes into account the fact that we count walks that start anywhere in
the domain.
On the other hand, in the opposite limit, N2/L2 ≫1, the exponentials play a
decisive role in the evaluation of the sum over m in (4.19). In fact, the exponential
gives rise to a rapid damping out of the terms in the sum. The ﬁrst term dominates,
and we are left with
2N 8L
π2 e−π22N/L2
(4.24)
The effect of the boundary is to introduce an exponential decay into the statistics
of the conﬁned walk. There is an explanation for this factor based on entropic

78
Boundary conditions, steady state, and the electrostatic analogy
considerations, due to de Gennes (1979). One argues that the reduction in the
number of allowed walks ought to be expressible in the form eS, where S is an
entropy-like function. It is known that the entropy of a large system scales linearly
with the size of the system. In the case of the random walk, the “size” is the number
of steps in the walk. Thus, S ∝N. On the other hand, the entropy ought to depend,
in a dimensionless way, on the size of the region to which the walker is conﬁned.
That is, S must be a function of L. There is another length in the system: the natural
extension of the one-dimensional walk. The square of this length goes as 2N. It
is, thus, reasonable to assume that there is an entropy reduction factor going like
e−αN2/L2, where α is a numerical constant that cannot be inferred from general
arguments such as this one.
Another way to understand the factor is in terms of “leakage” of random walkers
out of the conﬁned region. Imagine that a single walker starts out at the point of
origin of the walk. Instead of deciding where to step next, the walker splits up into
a set of descendants, one for each possible step. For instance, the one-dimensional
walker splits into two at each step. One of the descendants moves to the right, and
the other descendant moves to the left. To determine the number of walks after N
steps, one counts the number of descendants. The leakage mentioned above is a
process that competes with the proliferation of walkers. When descendants of the
original walker impinge on the absorbing boundary, they are eliminated from the
population. To calculate how many N-step walks take place in the region, it sufﬁces
to determine the outcome of the combined processes of proliferation and depletion.
The ﬁssion of walkers at each step multiplies the population by two. The net
effect of depletion at the boundary can also be qualitatively assessed. The loss of
walkers at the boundary leads to a reduction of the population in the immediate
vicinity of the edge of the region. The loss is proportional to the “ﬂux” of walkers
across the boundary, where the ﬂux is proportional to the slope of the distribution.
That this is so follows from the fact that the number of walkers arriving from a
neighboring point scales with the number of walkers that were at that point at
the moment immediately preceding the last step. If there are more walkers to the
right of a given location than to the left, then more walkers will be passing by that
location moving right to left than left to right. The excess in left-moving walkers
will increase proportionately to the difference between the population of the site to
the right and the site to the left. Quantitatively, we write
J(x) = −αdn(x)
dx
(4.25)
where J(x) is the ﬂux of walker and α is a constant.2 The minus sign indicates that
2 A more detailed discussion of this relation will be given later in the chapter. See Section 4.2.1 onwards.

4.1 The effects of spatial constraints
79
L
slope = Σ
Fig. 4.2. Density of walkers in a ﬁnite region. Note that the density falls off at the
edge of the allowed region, where the absorbing boundary depletes the population
of walkers. This ﬁgure is an accurate representation of the distribution of a set of
walkers that is continually replenished through the introduction of new walkers at
the center, while depletion takes place as walkers are absorbed at the edges.
the random walkers tend to move from more heavily populated regions to regions
in which there is a relative deﬁcit of walkers. The next step is to relate the slope
of the distribution at the boundary to the total number of walkers contained in
the ﬁnite region. Suppose that the slope of the distribution of walkers is . Then,
the total number of walkers in a region of width L is approximately L times the
magnitude of the distribution. This magnitude goes as the slope times the width
of the region (see Figure 4.2). This means that the total number of walkers, nw, is
equal, to within a constant of proportionality, to L2, or that  = κnw/L2, where
κ is a proportionality constant.
In light of the above discussion, if nw(N) is the number of walkers after N steps,
then the number of walks after N + 1 steps is given by
nw(N) = nw(N −1) × 2 ×

1 −ακ
L2

(4.26)
Iterating this relationship we ﬁnd
nw(N) = 2N 
1 −ακ
L2
N
nw(0)
= 2Nexp

N ln

1 −ακ
L2

≈2Ne−καN/L2
(4.27)
The constants κ and α follow from detailed calculations, which in the case at hand
lead to the more precise formula (4.24).
4.1.4 Conﬁnement in a sphere
As a ﬁnal investigation of the effects of spatial constraints on random walk statistics,
we will consider a three-dimensional walk conﬁned to a sphere of radius R. Here,
the appropriate basis set is the combination, in spherical coordinates (Morse and

80
Boundary conditions, steady state, and the electrostatic analogy
x
y
z
θ
φ
r
Fig. 4.3. The relationship between Cartesian and spherical coordinates.
Feshbach, 1953)
jl(r)Y m
l (θ, φ)
(4.28)
where jl(r) is a spherical Bessel function and Y m
l (θ, φ) is a spherical harmonic
(Abramowitz and Stegun, 1970). The relationship between the Cartesian coordinate
system (x, y, z) and the spherical coordinates (r, θ, φ), is illustrated in Figure 4.3.
The solution to the three-dimensional version of (2.38) is given in terms of the
functions in (4.28) by
∞

l=0

|m|≤l
∞

nl=1
Cnl
jl(knlr1) jl(knlr2)Y m
l (θ1, φ1)Y m
l (θ2, φ2)
1 −6z + z2k2nl
(4.29)
The quantity Cnl represents the normalization of the basis set jlY m
l . The allowed
values of knl are determined by the properties of the spherical Bessel functions. The
quantity knl is adjusted to that jl(knl R) = 0. In this way, the boundary conditions
appropriate to an absorbing wall at the outer bounding surface of the sphere are
reﬂected in the basis set used.
We will focus on the total number of walks inside the sphere, which means that
we will sum (actually integrate) formula (4.29) over all spatial coordinates. The
spherical harmonics have the property that
 2π
0
dφ
 π
0
sin θ dθ Y 0
0 (θ, φ)Y m
l (θ, φ) = δl,0δm,0
(4.30)
Theintegralovertheangularcoordinatesthusleaveuswiththefollowingexpression
to evaluate
8π
R

n0
 R
0
r2
1 dr1
 R
0
r2
2 dr2
2
R
j0(k0r1) j0(k0r2)
1 −2z + z2k2n0
(4.31)

4.1 The effects of spatial constraints
81
The spherical Bessel function j0(x) is given by
j0(x) = sin x
x
(4.32)
This means that the kn0’s are given by
kn0 = n0π
R
(4.33)
where the integers n0 range from 1 to ∞. Integrating over the two radial coordinates,
we are left with the following sum
8R3
π

n
1
n2
1
1 −6z + z2π2n2/R2
(4.34)
We are interested in the coefﬁcient of zN in the above expression. Once again, we
expand with respect to z. We ﬁnd for the number of N-step walks originating and
terminating anywhere in the spherical region for which the walker never leaves that
region
6N 8R3
π
∞

n=1
1
n2 e−2π2N/R2
(4.35)
As in the case of the one-dimensional conﬁned walk, we do not have a formula
that reduces to a simple, closed form expression. Again, as in the one-dimensional
case, we can obtain limiting behavior. When N2/R2 ≪1, we can ignore the
exponential in (4.35). The number of walks in that region is, then, equal to
6N 8R3
π
∞

n=1
1
n2 = 6N
4
3π R3

(4.36)
The right hand side of (4.36) follows from (4.21). Note that we recover in this limit
the result that the total number of random walks in the sphere is equal to the total
number of completely unrestricted random walks multiplied by the volume of the
sphere. This is entirely consistent with the result that was obtained in the equivalent
limit for one-dimensional random walks conﬁned to a ﬁnite domain.
In the limit that N2/R2 ≫1, the sum is once again over a set of rapidly
decreasing terms. The leading contribution is the ﬁrst one:
8R3
π

6e−2π2/R2N
(4.37)
Once again, there is the N-independent prefactor and the exponential decay term.

82
Boundary conditions, steady state, and the electrostatic analogy
This term derives from the mechanisms mentioned in the discussion of the one-
dimensional case.3
Exercise 4.5
Derive an expression for the total number of N-step walks available to a walker
conﬁned to a cube of side length L.
4.2 Random walk in the steady state
Our focus in this section will be somewhat different. We will be interested in time
averages, rather than what takes place at some particular point in the history of
the random walk. We can address this problem by considering an ensemble of
walkers, rather than a single one. We start by reformulating the problem in terms
of what happens to a group of walkers. As discussed previously, we measure time
in discrete intervals. At the Nth interval, the number of walkers at location ⃗r will
be depicted by the function W(N; ⃗r). Let’s assume that the walkers move around
on a cubic lattice, and we’ll also assume that the walk is unbiased. There is a
recursion relation between W(N; ⃗r) and the quantity at the next time step. Here is
the recursion relation:
W(N + 1; ⃗r) = 1
6

⃗ρi
W(N; ⃗ρi) + S(⃗r)
(4.38)
The ⃗ρi’s in (4.38) are the locations of the sites nearest to the site at ⃗r. We assume a
simple cubic lattice, and the factor 1/6 in (4.38) reﬂects the fact that each walker has
six choices for the direction in which it is to step. The ﬁnal term on the right hand
side of (4.38) is the source term, and it represents the constant supply of walkers
that is provided at the various points at which the walkers originate.
It is in this latter point that our reformulation represents the greatest change
from previous discussions. We will allow walkers to appear at different instants
in time. In fact, we will now imagine a steady supply of walkers, materializing at
a continuous rate at one or more points in space. There may be some difﬁculty
in imagining this scenario, unless the walkers appear at the boundary of the re-
gion under consideration, in which case they will have entered from that region’s
surroundings. However, if the walkers are molecules, this kind of production is
possible if chemical reactions are taking place, of which the walkers in question
are the end-products. From a mathematical point of view, we are creating a steady
state that reproduces the results of averaging over a long time interval.
3 For an additional discussion of the above situation, see (Barber and Ninham, 1970).

4.2 Random walk in the steady state
83
Now, in the steady state, the number of walkers at a particular point in space is in-
dependent of time, which means that we can replace W(N; ⃗r) by the N-independent
quantity w(⃗r). The recursion relation is, then, a requirement on this expression:
w(⃗r) = 1
6

⃗ρi
w(⃗ρi) + S(⃗r)
(4.39)
As we’ve done before, we’ll perform a Taylor-series expansion of the terms in the
sum on the right hand side of (4.39). Retaining terms up to second order in the
derivative of w(⃗r), we end up with
w(⃗r) = w(⃗r) + a2
3 ∇2w(⃗r) + S(⃗r)
(4.40)
The quantity a is the distance between the lattice point at ⃗r and any one of its
six nearest neighbors. That there are no linear terms follows from the reﬂection
invariance of the square lattice to which the walkers are restricted. We’ve already
seen that a lattice is not a necessity for the derivation of equations like (4.40).
Cancelling the w(⃗r)’s on the two sides of (4.40), we are left with the following
equation for the steady state distribution of random walkers on the lattice:
−∇2w(⃗r) = 3
a2 S(⃗r)
(4.41)
Except for the constant of proportionality, this is identical to the equation coupling
the electrostatic potential, φ(⃗r), and the charge distribution, ρ(⃗r), that gives rise to it.
Thus, steady state diffusion, or the steady state random walk with sources, maps
onto Poisson’s equation for the electrostatic potential.
In order to fully exploit the relationship between the two problems, we must also
review the effects of boundaries. We’ve already looked at the inﬂuence of absorbing
boundaries. At such bounding surfaces, the distribution of walkers vanishes. This
is equivalent, in the electrostatic analogy, to having the potential φ(⃗r) constant and
equal to zero. In other words, absorbing boundaries play the same mathematical
role in steady state diffusion as conducting boundaries do in electrostatics. There
is another important type of boundary, and that is one that reﬂects the walkers
that impinge on it. The appropriate boundary condition will be taken up after we
rederive (4.41) with the use of another approach that is more intuitive, but identical
in effect, to the recursion-relation-based derivation of that Poisson-like equation
for the distribution of walkers in the steady state.
4.2.1 Fick’s law and the random walk
The second approach to the question of the distribution of random walkers in the
steady state utilizes the notion of a current density of walkers and builds on the

84
Boundary conditions, steady state, and the electrostatic analogy
argument in Section 4.1.3. Let’s imagine that we have a collection of walkers
meandering through a continuum. We’ll assume that there is an overall organized
movement of the walkers in any given point in space, superimposed on the random
motion. As it turns out, this organized motion is driven by spatial imbalances in
the concentration of the walkers, and it is an inevitable consequence of the random
nature of the movement of individual walkers. If we describe this overall motion in
terms of a current density, ⃗j(⃗r, t), then, the density of walkers, which we’ll denote
by c(⃗r, t), satisﬁes the continuity equation
∂c(⃗r, t)
∂t
= −⃗∇· ⃗j(⃗r, t) + S(⃗r, t)
(4.42)
This equation expresses the fact that a change in the number of walkers in a given
region will either result from an excess of current into the region or from any
external source that might be “materializing” walkers into that region.
The current density is the result of walkers that wander in from neighboring
regions. Because the walkers step in random directions, we expect that the current
density that occurs will be the result of walkers “spilling” into regions with a lower
concentration of walkers from regions of higher concentration. We can quantify
this by writing
⃗j(⃗r, t) = −β ⃗∇c(⃗r, t)
(4.43)
This relationship, which actually follows from an analysis that takes a form
similar to our recursion relations, is known as Fick’s ﬁrst law.4 The parameter β
is empirical. It can be obtained if we know more about the details of the random
walk process, or it can be determined from measurements on the system in which
the actual walk is taking place. Combining (4.42) and (4.43), we obtain
∂c(⃗r, t)
∂t
= β∇2c(⃗r, t) + S(⃗r, t)
(4.44)
This is a continuum version of the recursion relation that we’ve used to obtain
the generating function. For the time being, we’ll focus on the steady state, but
it is possible to construct variants of the equations that we’ve used to obtain the
generating functions for random walks from (4.44). Making the assumption that
the source term is time-independent, and removing the time dependence from the
function c(⃗r, t), we are left with the steady state equation
−β∇2c(⃗r) = S(⃗r)
(4.45)
4 It is apparent from (4.43) that the appropriate boundary condition for a wall that perfectly reﬂects walkers instead
of absorbing them is that the net ﬂux of walkers into the wall is zero, or, equivalently, ˆn · ⃗∇c = 0, where ˆn is the
unit vector perpendicular to the wall.

4.2 Random walk in the steady state
85
Again, we have derived an equation that is mathematically identical to Poisson’s
equation for the electrostatic potential.
Worked-out example
For a steady stream of one-dimensional walkers emanating from the point x0,
which lies between two absorbing boundaries located at x = 0 and at x = L, ﬁnd
the concentration of walkers at any point 0 < x < L.
Solution
The one-dimensional version of the equations satistifed by the distribution of
walkers is
∂c(x, t)
∂t
= −∂j(x, t)
∂x
+ s0δ(x −x0)
(4.46a)
j(x, t) = −β ∂c(x, t)
∂x
(4.46b)
where (4.46a) is the one-dimensional version of the continuity equation and (4.46b)
is the one-dimensional version of Fick’s ﬁrst law. In the steady state, the equation
satisﬁed by the concentration, c(x) is
β d2c(x)
dx2
= −s0δ(x −x0)
(4.47)
This equation tells us that the ﬁrst derivative of the steady state concentration is
constant throughout the region 0 < x < L, except for the point x = x0, at which the
slope of c(x) suffers a discontinuity, the concentration itself remaining continuous.
Furthermore, we know that the concentration obeys the boundary conditions c(0) =
c(L) = 0. The discontinuity in the slope is given by
dc(x)
dx
				
x+
0
−dc(x)
dx
				
x−
0
= −s0
β
(4.48)
The solution with all these characteristics is
c(x, x0) =



s0
β x L −x0
L
x < x0
s0
β x0
L −x
L
x > x0
(4.49)

86
Boundary conditions, steady state, and the electrostatic analogy
Exercise 4.6
Show that the probability of a walker being absorbed at x = 0 in the example above
is given by
P(x0) = L −x0
L
Note that as L →∞, P(x0) →1. In other words, the walker never escapes to
inﬁnity.
4.2.2 Solution for c(⃗r ) in the presence of a localized source
We’ll start with the simplest conﬁguration of sources: one that is localized at a
single point in space, which we’ll place at the origin of our coordinate system.
Then,
S(⃗r) = s0δ(⃗r)
(4.50)
We’ll take as our steady state equation, the one we’ve derived from Fick’s ﬁrst law:
−β∇2c(⃗r) = S(⃗r)
= s0δ(⃗r)
(4.51)
By analogy to electrostatics, we know, more or less immediately, that the solution
to this equation is
c(⃗r) =
s0
4πβ
1
|⃗r|
(4.52)
The distribution falls off as a power law in the distance from the source. Note that
this is very different from the Gaussian distribution that we derived, repeatedly,
earlier on. That distribution had an essential time-dependence, expressed in terms
of N, the number of steps that the walker had taken. The situation to which the
former distribution applies is one in which there is a source that supplies a walker,
or a set of walkers, at an instant in time and then “turns off.” By contrast, for (4.52)
walkers materialize at the origin at a constant rate.
Connection between the steady state and Gaussian Distributions
It is instructive to see how the result for the steady state distribution of walkers em-
anating from a point source can be derived from the familiar Gaussian distribution.
In the latter case, the walkers are placed at a particular point in space at an instant

4.2 Random walk in the steady state
87
of time. This corresponds to a source of the form
S(⃗r, t) = S0δ(⃗r)δ(t)
(4.53)
The distribution of walkers will have the form
cG(⃗r, t) ∝t−3/2e−r2/at
(4.54)
for times t greater than zero. In the steady state situation, the source term is con-
stantly “on,” which means that the distribution is obtained by integrating the ex-
pression in (4.54) for over all positive values of t. Thus, we have for the steady state
distribution
cS(⃗r) ∝
 ∞
0
t−3/2e−r2/at dt
(4.55)
As long as r ̸= 0, this integral converges. We can extract the r-dependence of cS(⃗r)
by scaling it out of the integration. Let t = Tr2. Then we have
cS(⃗r) ∝1
r
 ∞
0
T −3/2e−1/aT dT
= K
r
(4.56)
with K an r-independent quantity. The Coulomb’s-law-like form of the steady state
distribution has been recovered.
4.2.3 Steady state distribution of walkers near an absorbing wall
Let’s consider a few interesting cases. Suppose we have a point source of walkers in
the vicinity of an absorbing wall. We know that the concentration of walkers falls to
zero at the wall, and we know that the point source has the same mathematical effect
on the distribution of walkers as a point charge has on an electrostatic potential.
Translating this into a problem in electrostatics, we are looking at the problem of
a point charge in the vicinity of a plane conducting surface. This is solved with the
use of an image charge. Figure 4.4 shows what the distribution of charges looks
like. The image source lies on the other side of the absorbing wall. This tells us
that the steady state distribution of walkers will be given by a concentration, c(⃗r),
going as
c(⃗r) ∝
1
|⃗r −⃗rs| −
1
|⃗r −⃗ri|
(4.57)
where ⃗rs is the position vector of the source and ⃗ri is the position vector of the
image. Suppose that the absorbing wall is in the x, z plane, and the point source

88
Boundary conditions, steady state, and the electrostatic analogy
point source
image source
Fig. 4.4. The image charge conﬁguration.
point source
image source
imaginary
wall
Fig. 4.5. The image charge distribution, with the imaginary wall through which
the current is calculated.
is along the x axis a distance l from the wall. Then, ⃗rs = ˆxl, while ⃗ri = −ˆxl. The
distribution of walkers has the form
c(x, y, z) ∝
1

(x −l)2 + y2 + z2 −
1

(x + l)2 + y2 + z2
→
2lx

x2 + y2 + z23/2
(4.58)
where the last line holds when r ≫l. This “dipole ﬁeld” dependence of the number
of walkers at the location ⃗r represents a serious depletion with respect to the steady
state distribution, which goes as 1/r, when there is no absorbing wall. In fact,
it can be demonstrated that the wall has the effect of eliminating almost all the
walkers that emanate from the source. One way to see this is to calculate the total
current of walkers through an imaginary wall to the right of the source, shown as a
vertical dashed line in Figure 4.5. Recall that the current density is proportional to
the gradient of the concentration of walkers. To calculate the total ﬂux of walkers
through the imaginary wall, we take the partial derivative of c(⃗r) with respect to x.

4.2 Random walk in the steady state
89
We ﬁnd for the current denstiy
jx(x, y, z) ∝−∂
∂x
2lx

x2 + y2 + z23/2
= 2l

3x2

x2 + y2 + z25/2 −
1

x2 + y2 + z23/2

(4.59)
We’ve used the form that works when the imaginary wall is far away from the real
one. Now, if we integrate this component of the current density over the imaginary
wall, we ﬁnd ourselves taking an integral that is proportional to
 ∞
0

3x2

x2 + R25/2 −
1

x2 + R23/2

R dR
= 1
2
 ∞
0

3x2

x2 + w
5/2 −
1

x2 + w
3/2

dw
=

−
x2

x2 + w
3/2 +
1

x2 + w
1/2
					
∞
0
= 0
(4.60)
In the end, no walkers escape from the absorbing wall, no matter how far out they
manage to walk.
4.2.4 Walkers near an absorbing sphere
We can also ask what happens to walkers in the vicinity of an absorbing sphere?
First, we’ll look at the case of a collection of walkers that are supplied by a set of
sources that are inﬁnitely far away from the sphere. The sources are such that the
distribution of walkers is uniform at an inﬁnite distance from the sphere. To ﬁnd
what the concentration of walkers is at an arbitrary distance from the sphere, which
has a radius equal to r0, we search for a solution of Laplace’s equation (because
there are no sources in the region of interest) that is equal to zero at the surface of
the sphere and that is a constant inﬁnitely far away from the sphere. Placing the
origin at the center of the sphere, we can also demand spherical symmetry about
that origin. The solution of interest is then readily intuited. It is
c(⃗r) = c0

1 −r0
r

(4.61)
where r is the distance from the center of the sphere, and the expression on the
right hand side of (4.61) holds when r > r0.
We can use this formula to calculate the rate at which the walkers are absorbed
by the sphere. To do this, we calculate the current density of walkers by taking the

90
Boundary conditions, steady state, and the electrostatic analogy
r0
R
original source
image source
ρ
Fig. 4.6. The source and the image source in the case of an absorbing sphere.
gradient of the right hand side of (4.61), and then by calculating the current ﬂux
into the sphere. From (4.43), we have
⃗j(⃗r) = −βˆr ∂
∂r c0

1 −r0
r

= −ˆr βc0r0
r2
(4.62)
This tells us that the ﬂux of walkers into the sphere is equal to
βc0
r0
r2
0
× 4πr2
0 = 4πβc0r0
(4.63)
Thismeansthatthenumberofwalkersthatareabsorbedbythespherescaleslinearly
with the radius of the sphere. If the sphere represents a cell, and the walkers are
nutrients in the broth in which the cell sits, the rate at which the cell takes in those
nutrients is proportional to its linear size. On the other hand, if the sphere really is a
cell, it has metabolic requirements that scale as its volume – in other words, as r3
0.
Ultimately, those requirements will overwhelm the cell’s ability to absorb nutrients
through diffusion, as the size of the cell increases. In the case of an immobile cell,
these considerations place a limit on the maximum size that it can be. In general,
the fact that metabolic needs will exceed the rate at which nutrients can be gathered
as they diffuse inwards will mandate a different strategy for the acquisition of
biological fuel for any organism that is larger than a certain size (Berg, 1993).
It is also possible to calculate the distribution of walkers when there is a point
source outside of an absorbing sphere (Figure 4.6). In this case, we make use of a
modiﬁcation of the image charge. If the source is a distance R away from the center
of a sphere of radius r0, then the image source is a distance ρ = r2
0/R from the
center of the sphere, and the “strength” of the source is equal to −r0/R times the
strength of the original, external source. It is possible to calculate how many walkers
escape from the sphere by taking the integral of the current ﬂux through a surface
that surrounds both the source and the absorbing sphere. We can short-circuit this
calculation by noting that a version of Gauss’s law holds here, which tells us that
the net ﬂux through a surface surrounding a set of sources is proportional to the
total strength of those sources. In this case, the total strength is equal to the strength

4.2 Random walk in the steady state
91
of the original source plus that of the image source, which is the strength of the
original source multiplied by 1 −r0/R. The fraction of the total number of walkers
that emanate from the source that also escape from the sphere is (R −r0)/R.
What we have also achieved here is a derivation of the probability of escape by
a three-dimensional walker from a spherical region in the vicinity of its point of
origin.
4.2.5 The analogy with capacitance
There is a mathematical connection between an absorbing surface into which matter
diffuses from a distance and the element of a capacitor. Because of the relationship
betweentheconcentrationandtheelectrostaticpotential,wecanimagineananalogy
in which the surface is that of a conductor, surrounded by a spherical shell some
distance away. If the potential difference between the two is φ, and the charge
on the inner surface is Q, then the capacitance of the system is c = Q/φ. The
connection between charge and the electrostatic potential is ρ(⃗r) = −4π∇2φ(⃗r).
The total charge on the inner surface is then given by Gauss’s law:
Q = −1
4π

⃗∇φ(⃗r) · d⃗S
(4.64)
Making use of the relationships we’ve already established between electrostatic
quantities and those in steady state diffusion, we can state that the following holds
−
 ⃗j(⃗r) · d⃗S
c∞
= 4πβC
(4.65)
where c∞is the concentration of walkers far away from the absorbing surface and
C is the capacitance of a capacitor consisting of the absorbing surface surrounded,
at a great distance, by a spherical shell (see Figure 4.7). For reference, recall that
the capacitance of a sphere is given by
Csphere = r0
(4.66)
where r0 is the sphere’s radius. This and (4.65) yields (4.63) for the total current
absorbed by the sphere, where we have replaced c0 by c∞.
4.2.6 A sphere covered with receptors
This scenario allows us to consider what happens if the nutrients are not absorbed
uniformly throughout the sphere. Suppose, instead, that the sphere has a number
of absorbing sites, or receptors, distributed across its surface. Imagine that there
are n of those receptors, and that the radius of a given site is a. We’ll also assume

92
Boundary conditions, steady state, and the electrostatic analogy
inner 
surface
outer
sphere
Fig. 4.7. A capacitor consisting of an inner surface surrounded by a spherical
shell. The radius of the surrounding shell is much greater than the size of the inner
surface.
that na ≪r0, where r0 is the sphere’s radius. This means that the total surface
area accounted for by the receptors is a small fraction of the surface area of the
sphere. Let’s represent this collection of receptors as a network of small conduct-
ing surfaces, arranged in the shape of a spherical shell. This network is utilized
as one of the elements in a capacitor. What is the capacitance of the resulting
capacitor?
First, note that the distance between the receptors is the order of r0/√n. Then,
notice that the potential a distance r away from the sphere, where r ≫r0/√n, is
goingtobethesameastheelectrostaticpotentialgeneratedbyauniformdistribution
of the receptors, smeared out over the sphere. This is because at such a distance, the
difference between a set of discrete charges and a uniformly distributed charge is
negligible, as far as the electrostatic potential goes. If a charge of Q/n is placed on
each receptor, then the electrostatic potential of this array of charges goes as Q/(r +
r0). The electrostatic potential near the sphere is, then, well-approximated by Q/r0.
This means that the capacitance of the spherical arrangement is substantially equal
to the capacitance of a uniform sphere. Making use of the electrostatic analogy, we
ﬁnd that the collection of receptors will absorb nutrients at the same rate as if the
entire surface of the sphere were a receptor.
We can be a little more explicit about the potential immediately above the surface
of the network. The electrostatic potential right next to one of the small components
of the network due to the charge carried by that component will go as Q/na, because
each of the components carries a charge equal to Q/n, and each has an effective
size equal to a. On the other hand, the potential due to all the other components
will be essentially the same as if the charges on them were uniformly distributed
over the surface of the sphere. This potential is equal to Q(n −1)/nr0. If n is large

4.3 Supplement
93
enough that na ≫r0, which is possible for sufﬁciently large n, since all we require
is that a ≪r/√n, the potential is dominated by the second contribution, which, in
the limit of large n, goes to Q/r0. The potential at a point near the surface of the
sphere that is far away from one of the small components compared to its size will
be absolutely dominated by the second term. Thus, to a very good approximation,
the electrostatic potential in the immediate vicinity of the network is the same as
the electrostatic potential right next to a sphere carrying a uniform charge equal
to Q.
For a more extended discussion of the issues addressed in the last two sections
see (Berg, 1993) and (Berg and Purcell, 1977).
4.3 Supplement: boundary conditions at an absorbing boundary
As noted in Section 4.1.2, the distribution of walkers falls to zero at a boundary
that eliminates, or absorbs, walkers that cross over it. This boundary condition may
seem eminently reasonable.5 However, a rigorous derivation is not entirely trivial.
In this supplement, we show how the condition comes about. We focus our attention
on the one-dimensional situation; the argument readily generalizes to walks in a
higher dimensional region.
We start with the equation obeyed by the generating function for the one-
dimensional walk on a lattice:
G(z; x, y) = z (G(z; x, y −) + G(z; x, y + )) + δx,y
(4:S-1)
The quantity G(z; x, y) is the generating function for one-dimensional walks start-
ing at y and ending at y, where y and x are restricted to lie on a one-dimensional
array of points, which means that both x and y take on the values n, where n ≥0.
There is an absorbing boundary immediately to the left of the point n = 0, which
means that all walkers that attempt the step to y = − are eliminated. The restric-
tion on n means that the equation (4:S-1) holds only when y ≥1. When y = 0, the
equation satisﬁed by the generating function is
G(z; 0, y) = zG(z; x, ) + δx,
(4:S-2)
This equation holds because there are no walkers at y = −n.
We can restore the applicability of (4:S-1) to all points in the region occupied by
surviving walkers by requiring that G(z; x, −) is equal to zero. In that case, the
equation satisﬁed by the generating function G(z; x, y) is (4:S-1) at all points n
with n ≥0, supplemented by the boundary condition
G(z; x, −) = 0
(4:S-3)
5 See (Weiss, 1994) and references therein for a fuller discussion of boundary conditions.

94
Boundary conditions, steady state, and the electrostatic analogy
This means that the boundary condition appropriate to an absorbing barrier at the
perimeter of a region occupied by random walkers is that the generating function (or
thedensityofwalkers)isequaltozeroimmediatelyontheothersideoftheboundary.
In the continuum limit, in which we ignore details such as the actual distance
covered by a walker in a single step, this boundary condition is approximated by
the requirement that the walker density – or generating function – vanishes at the
exact location of the barrier.

5
Variations on the random walk
We now discuss the ways in which the statistics of the random walk may be affected
by variations in the rules governing the walk. As we will see, the methods that have
been developed in the previous chapters are still useful. However, the variations
brought about by the nature of the environment and on the rules governing the walk
give rise to interesting new behavior, and to the possibility of analyzing important
physical and probabilistic phenomena.
5.1 The biased random walk
We’ll start by considering a walk in which the walker prefers to take a step in a
particular direction. In other words, the walk is no longer isotropic, but is rather
biased. For simplicity, we’ll look at the case of a one-dimensional walker that
prefers to go either to the right or to the left. In this case, the probability of taking
a step to the right will be p and the probability of taking a step to the left is 1 −p,
where p ̸= 1/2. The probability that the walker has taken n steps to the right and
N −n steps to the left after an N-step walk, is, as before, given in terms of the
binomial distribution. Recall Exercise 1.1, in Chapter 1. We ﬁnd
P(N; n, N −n) = pn(1 −p)N−n
N!
n!(N −n)!
(5.1)
We can also obtain this probability with the use of a recursion relation. We start
by relating P(N + 1; x, y), the probability that the walker that has started out at
x ends up at y after having taken N + 1 steps, to the probabilities that in N steps
the walker ended up immediately to the left or immediately to the right of its ﬁnal
destination. This recursion relation is
P(N + 1; x, y) = pP(N; x, y −l) + (1 −p)P(N; x, y + l)
(5.2)
95

96
Variations on the random walk
We now play the same game as in Chapter 1. We Taylor expand the probabilities
on the right hand side of (5.2):
P(N + 1; x, y) = P(N; x, y) + l(2p −1) ∂
∂y P(N; x, y)
+ l2
2
∂2
∂y2 P(N; x, y) + · · ·
(5.3)
We’ll solve this equation by evaluating the Fourier transform of both sides, taking
advantage of the fact that this probability depends only on the distance from x to y.
Multiplying both sides by eiq(y−x) and integrating, we ﬁnd for the equation satisﬁed
by the Fourier-transformed quantity P(N; q)
P(N + 1; q) = P(N; q)

1 + il(2p −1)q −l2q2
2
+ O(q3)

(5.4)
The solution to this equation is
P(N; q) =

1 + il(2p −1)q −l2q2
2
+ · · ·
N
P(0; q)
(5.5a)
= exp

N ln

1 + il(2p −1)q −l2q2
2
+ · · ·

P(0; q)
(5.5b)
= exp

Nil(2p −1)q −2Nq2l2 p(1 −p) + O(q3)

P(0; q)
(5.5c)
We now neglect terms in the exponent in (5.5c) that are beyond quadratic in q.
This presents us with a Fourier-transformed probability that is a Gaussian in q.
The inverse Fourier transform will also be a probability, this time in the distance
between the initial and end-points. The initial distribution of walkers, P(N; y −x),
is given by
P(0; y −x) = δ(y −x)
(5.6)
a Dirac delta function. The Fourier transform of this function is independent of
q. This means that the inverse Fourier transform of (5.5c), with the terms beyond
O(q2) in the exponent neglected, is given by

1
8π Np(1 −p)l2 exp

−(x −(2p −1)Nl)2/8Nl2 p(1 −p)

(5.7)
This is a Gaussian whose peak moves to the right a distance (2p −1)l at each step
and whose width increases as the square root of N (see Figure 5.1 for an explanation
of this behavior). The statistics of the biased walk are those of a Gaussian with
a moving peak. Note that the width of the peak also goes as the combination
√p(1 −p). The width vanishes as the probability of a step to the right or to the left

5.1 The biased random walk
97
-5
5
10
15
20
25
30
0.05
0.1
0.15
0.2
0.25
P(N;x)
x
Fig. 5.1. The evolution of the probability distribution, P(N; x) of a one-
dimensional walk that is biased to the right. The plots are for various values of N.
The original distribution is a delta function centered at the origin. As N increases
the distributions move to the right, and they also spread, as shown in this ﬁgure.
goes to 1. Of course, this makes perfect sense. In either of those limits there is no
randomness in the walk. The walker proceeds to the left or the right with perfect
certainty. All the members of a collection of such walkers will end up at exactly
the same point after taking N steps from a common origin.
It should be noted that this feature would be lost if we were to go directly
from (5.3) to the diffusion equation by writing P(N + 1; x, y) = P(N; x, y) +
∂P(N; x, y)/∂N. In that case, the probability would still be centered about (2p −
1)l, but the width of the peak would no longer go as √p(1 −p), but, instead, this
factor would be replaced by 1/2. The reason for this becomes clear in a more careful
derivation of the diffusion equation in the continuum limit, when N is large. When
this is done, one ﬁnds that the diffusion equation is only valid in the limit p →1/2,
or for walks that are weakly biased.
Exercise 5.1
Demonstrate the correctness of the statement above, that the diffusion equation is
valid only if p is close to 1/2.
5.1.1 A biased walk in three dimensions
In looking at a biased walk in three dimensions, we will make use of another way
to approach the possibility of biasing in a random walk. The key here lies in the
modiﬁcation of the structure function χ(⃗q). In the case of the unbiased walk, the
structure function is the Fourier transform of a function that is centered about ⃗r = 0,
and that has a simple peak there. We can put a bias in the walk by translating this

98
Variations on the random walk
function. That is, suppose that χ(⃗q) is given by1
χ(⃗q) =

e−i⃗q·⃗r P(⃗r) d3r
≡χ0(⃗q)
(5.8)
Here, P(⃗r) is the probability density for the walker to take a step resulting in a
displacement equal to ⃗r. A way of biasing the walk is to replace P(⃗r) by P(⃗r −
⃗rB). This probability density describes a walk for which the mean value of the
displacement after each step is ⃗rB, instead of zero. Then
χ(⃗q) =

e−i⃗q·⃗r P(⃗r −⃗rB) d3r
= e−i⃗q·⃗rB

e−i⃗q·(⃗r−⃗rB)P(⃗r −⃗rB) d3r
= e−i⃗q·⃗rBχ0(⃗q)
(5.9)
The probability that the walker will end up at the location ⃗r after taking N steps
from its point of origin at ⃗r = 0 is
1
(2π)3

d3q χ(⃗q)Nei⃗q·⃗r =
1
(2π)3

d3q χ0(⃗q)Nei⃗q·(⃗r−N⃗rB)
= P(N; ⃗r −N⃗rB)
(5.10)
where P(N; ⃗r) is the probability that the unbiased walk will end up at the location
⃗r after N steps. Notice that in this case, the width of the distribution is not affected
by the fact that the walker moves preferentially in a given direction. This is because
one way of thinking about the new rule governing the walker is to say that it takes
a purposeful step in a given direction followed by a purely random step. The new
walk is a superposition of a purposeful walk and a random one, in which the random
component obeys the same rules as in the case of the unbiased walk. Such a walk
describes the motion of small particles diffusing in a moving medium that carries
them along at a ﬁxed velocity. For instance, one could imagine a cloud of diffusing
dye molecules in water that ﬂows steadily in one direction.
5.2 The persistent random walk
The “pure” random walker has no recollection of the previous steps that it took
as it decides where to go next. In an important variation of the random walk, the
walker’s choice of a direction at a given step is inﬂuenced by how it moved in the
time leading up to that choice. This kind of walker has a memory. Focusing on a
1 The structure function, χ(⃗q), is deﬁned for lattice walks in (2.11) and for off-lattice walks in (2.23).

5.2 The persistent random walk
99
walker in one dimension, let’s assume that the walker has a tendency to move in
the same direction as it has already gone. That is, we’ll assume that, if the walker
took a step to the right previous to the one it is about to take, then the subsequent
step is more likely to be to the right than to the left, and conversely for a walker that
has arrived at its present position by having taken a step to the left. We’ll quantify
this tendency by assigning a probability Q for taking a step in the same direction as
the immediately preceding one and a probability 1 −Q for the walker’s reversing
its direction at the step it is about to take. Here we encounter a random process
which is still Markovian, but is of second order (Feller, 1968). That is, the process
depends not only on the previous event, but also on the one that precedes it. In order
to analyze this variant of the random walker we’ll deﬁne a new set of probabilities.
Let’s denote by L(N; x, y) the probability that a walker that starts out at x and
takes N steps ends up at y, the last step having been to the left. By the same token,
R(N; x, y) is the same probability, the walker now having taken its last step to the
right.Therecursionrelationisnowtwo-fold.Therecursionrelationfor L(N; x, y)is
L(N + 1; x, y) = QL(N; x, y + l) + (1 −Q)R(N; x, y + l)
(5.11)
The recursion relation for R(N; x, y) is
R(N + 1; x, y) = (1 −Q)L(N; x, y −l) + QR(N; x, y −l)
(5.12)
As the ﬁrst stage in the solution of these two recursion relations, we Fourier
transform both sides of (5.11) and (5.12). The equations for the transformed
functions are then
L(N + 1; q) = e−iql (QL(N; q) + (1 −Q)R(N; q))
(5.13)
R(N + 1; q) = eiql ((1 −Q)L(N; q) + QR(N; q))
(5.14)
We can summarize the two equations above in matrix form. Writing the two
probabilities as the entries in a column vector, we end up with the following single
equation
L(N + 1; q)
R(N + 1; q)

=
	
Qe−iql
(1 −Q)e−iql
(1 −Q)eiql
Qeiql

 L(N; q)
R(N; q)

≡T
L(N; q)
R(N; q)

(5.15)
Here, T is the two-by-two matrix on the right hand side of the ﬁrst line of (5.15).
The solution for the quantities L and R is
L(N; q)
R(N; q)

= TN
L(0; q)
R(0; q)

(5.16)

100
Variations on the random walk
A bit of algebra sufﬁces to determine the eigenvalues of the matrix T. We ﬁnd
λ1 = Q cos ql +

(1 −Q)2 −Q2sin2ql
(5.17)
λ2 = Q cos ql −

(1 −Q)2 −Q2sin2ql
(5.18)
There are various ways of working out the properties of the persistent walk. One
of the easiest utilizes the generating function of the walk. Given the relationship
in (5.15), we are able to evaluate the sum leading to the generating function for
L(N; q) and R(N; q). If we deﬁne
l(z; q) =
∞

N=0
L(N; q)zN
(5.19)
and similarly for r(z; q), then
l(z; q)
r(z; q)

=
∞

N=0
TNzN
L(0; q)
R(0; q)

= (1 −Tz)−1
L(0; q)
R(0; q)

(5.20)
Here
(1 −Tz)−1 =
 1 −zQeiql
z(1 −Q)e−iql
z(1 −Q)eiql
1 −zQe−iql
 
D
(5.21)
where
D = 1 −2zQ cos ql + z2(2Q −1)
= (1 −zλ1) (1 −zλ2)
(5.22)
The problem of determining the probability of a certain type of walk is reduced to
ﬁnding the coefﬁcient of zN in the expansion of the generating function.
Give what we now know about the asymptotic behavior of power series, we are
in a position to extract the long-time statistics of the one-dimensional persistent
random walk. The main thing that helps us is the fact that those statistics are
dominated by the singularity in the generating function that is closest to the origin.
This singularity is at z = 1/λ1, λ1 being the larger of the two eigenvalues of the
matrix T. For small values of the wave-vector q,
λ1 = Q

1 −q2l2
2

+ (1 −Q) −
Q2
1 −Q
q2l2
2
+ O

q4
= 1 −
Q
1 −Q
q2l2
2
+ O

q4
(5.23)

5.2 The persistent random walk
101
One of the key quantities of interest is the total number of walks. The Fourier trans-
form of the total number of N-step walks is equal to the sum L(N; q) + R(N; q).
Given (5.20), we have for the generating function of the total number of walks
l(z; q) + r(z; q). If we are interested in the asymptotic statistics of the per-
sistent walk, we only need look for the leading order coefﬁcient of zN in
the power-series expansion of (1 −zT)−1. This is controlled by the expansion
of the denominator, D, in (5.21), where D is given by (5.22). Now, we can expand
the product 1/(1 −zλ1)(1 −zλ2) in partial fractions as follows:
1
(1 −zλ1)(1 −zλ2) =
1
λ1 −λ2

λ1
1 −zλ1
−
λ2
1 −zλ2

(5.24)
The coefﬁcient of zN in the expansion of this quantity is equal to
1
λ1 −λ2

λN+1
1
−λN+1
2

(5.25)
This is dominated by the Nth power of the larger of the two eigenvalues, λ1. Given
(5.23), we see that the dependence on N and q of the spatial Fourier transform of
the total number of N-step walks is dominated by the quantity

1 −
Q
1 −Q
q2l2
2
N
= exp

N ln

1 −
Q
1 −Q
q2l2
2

→exp

−N
Q
1 −Q
q2l2
2

(5.26)
which tells us that the number of N-step random walks that end up a distance x
from their point of origin goes as
1
2π
 ∞
−∞
exp(−iqx) exp[−N Qq2l2/(2(1 −Q))] dq
=

1 −Q
2π N Ql2 exp

−x2 1 −Q
2N Ql2

(5.27)
Note that in the limit Q = 1 the width of the distribution on the right hand side of
(5.27) goes to inﬁnity, in that the expression loses its dependence on the end-point
x. What this tells us is that the Gaussian limit is never achieved in this instance.
The case Q = 1 corresponds to a walk that is not really random, as it will continue
indeﬁnitely in the direction in which it has set out. What happens when Q = 1 is
that the actual distribution of walkers takes the form of a delta function, centered
on the location of the steadily advancing walker.
On the other hand, when Q = 0, in which limit the walker inevitably changes
its direction, the width of the distribution of walkers, as predicted by the right

102
Variations on the random walk
hand side of (5.27), goes to zero. Of course, this limit is also pathological, in that
all randomness has vanished from the process. The walker, fated to reverse its
direction at every step, never gets anywhere.
In between those two limits, when the number of steps, N, is large enough,
the distribution of walkers takes on an asymptotically Gaussian distribution, with a
width that depends non-trivially on the likelihood that the walk changes its direction.
It is possible to extend the analysis here to a walker in more than one dimension.
The explicit form that the equations take in dimensions greater than or equal to two
is considerably more complicated, but yields to analysis in the continuum limit. We
will take up this case later in the chapter.
It is possible to collapse the dependence of the Gaussian in (5.27) on the proba-
bility Q into a redeﬁnition of the step length that the walker takes. If we make the
following replacements
l →nl
(5.28)
N →N/n
(5.29)
where
n =
Q
1 −Q
(5.30)
Then
exp

−x2 1 −Q
2N Ql2

→exp

−x2/2Nl2
(5.31)
and the Gaussian distribution for the persistent walk looks just like the Gaussian
distribution for an ordinary random walk. What we imagine is a walker that takes
n steps before deciding to take a step that is, with perfect unbiased randomness,
to the left or the right. We call the distance covered in these n steps, lp = nl, the
persistence length. The effect of persistence in this case is to alter the effective
length of each step. The notion of persistence length carries over to other random
walk processes and to many systems that are modeled by random walks.
5.2.1 Another approach to the persistent random walk
While the above development is complete, in principle, it is possible to take another
tack in the calculation of the statistics of the persistent random walk (Weiss, 1994).
We start by recalling the recursion relations for the quantities L(N; q) and R(N; q)
L(N + 1; q) = e−iql (QL(N; q) + (1 −Q)R(N; q))
(5.32)
R(N + 1; q) = eiql ((1 −Q)L(N; q) + QR(N; q))
(5.33)

5.2 The persistent random walk
103
Eliminating L(N; q) in (5.32) and (5.33), and then solving for L(N + 1; q) we ﬁnd
L(N + 1; q) = 1 −2Q
1 −Q e−iql R(N; q) +
Q
1 −Q e−2iql R(N + 1; q)
(5.34)
Now if we replace N by N + 1 in (5.33) and use (5.34) to eliminate L(N + 1; q)
from the resulting equation, we end up with the following recursion relation
R(N + 2; q) = 2Q cos ql R(N + 1; q) + (1 −2Q)R(N; q)
(5.35)
The eigenvalue equation that this recursion relation leads to can be derived from
the matrix form of the recursion relation (5.35):
R(N + 2; q)
R(N + 1; q)

=
2Q cos ql
1 −2Q
1
0
 R(N + 1; q)
R(N; q)

(5.36)
The eigenvalue of the matrix in (5.36) is the solution of the equation
λ2 −2Q cos qlλ + (1 −2Q) = 0
(5.37)
This equation is identical to the equation satisﬁed by the eigenvalues of the matrix
T.
Another way to obtain the eigenvalue equation is to posit for R(N; q) the form
R(N; q) = AλN
(5.38)
Substituting this presumed form into the recursion relation (5.35), we end up with
(5.37) for the appropriate λ.
Exercise 5.2
Using the above ansatz for R(N; q) derive the results found previously for
R(N, q) + L(N, q).
5.2.2 Persistent walks in the diffusion limit: the one-dimensional persistent
walk and the Telegrapher’s equation
Up to now, the discussion of the properties of the persistent walk has assumed that
the walkers in question take discrete steps. The behavior of an individual walker
was utilized to produce results for the distribution of a collection of walkers with
the use of a recursion relation. Here, we will start with the notion of a distribution
walkers. The walkers under consideration will not take steps from one location
to another but will rather travel with a constant speed, changing the direction of
their motion at random points of time as they move along. Our calculation of the

104
Variations on the random walk
statistical properties of a collection of this kind of walker will result in explicit
predictions for their distribution, as a function of space and time.
We start with walkers in one dimension. A walker moves either to the left or the
right, in both cases with a speed equal to v. This means that a full description of
the statistics of a set of such walkers consists of two distributions, one referring
to “left-moving” walkers and the other to “right-moving” walkers. Let’s call the
ﬁrst distribution L(x, t) and the second R(x, t). The recursion relation relevant to
a collection of this type of walker relates the distributions at a given location, x,
and a given time, t, to the distributions at a slightly earlier time. Let the difference
between the two times in question be δt. Then, if there were no interconversion
between left- and right-moving walkers, the recursion relation for the distributions
would be
R(x, t + δt) = R(x −vδt, t)
(5.39)
L(x, t + δt) = L(x + vδt, t)
(5.40)
The ﬁrst equation above tells us that the right-moving walkers that are at the location
x at time t + δt were, at time t at the location x −vδt. The second tells us that
the left-moving walkers that are at x at time t + δt were at x + vδt at time t.
Expanding (5.39) and (5.40) to ﬁrst order in δt, we end up with the following
differential equations for the two distributions:
∂R(x, t)
∂t
+ v∂R(x, t)
∂x
= 0
(5.41)
∂L(x, t)
∂t
−v∂L(x, t)
∂x
= 0
(5.42)
Themostgeneralsolutionto(5.41)is R(x, t) = f (x −vt),with f anyfunction,and
the most general solution to (5.42) is L(x, t) = f (x + vt). There is no randomness;
the distributions simply follow the walkers to which they refer, moving to the right
or left with a speed equal to v.
Thewalksbecomerandomwhenleft-andright-moversinterconvert.Thisprocess
is encoded in the equations for the two distributions as follows. We rewrite the two
equations above as
∂R(x, t)
∂t
+ v∂R(x, t)
∂x
= γ (L(x, t) −R(x, t))
(5.43)
∂L(x, t)
∂t
−v∂L(x, t)
∂x
= γ (R(x, t) −L(x, t))
(5.44)
This system of equations comprises the Telegrapher’s equation.

5.2 The persistent random walk
105
Exercise 5.3
Show that (5.43) and (5.44) follow by taking an appropriate limit of the discrete
version of these equations.
The right hand sides of (5.43) and (5.44) represent the process by which a right-
mover can, with probability γ per unit time, convert into a left-mover and vice versa.
This will eventually cause the distributions to look like those for a random walk.
However, there will be some memory of the ballistic propagation that underlies
the random motion. This memory is eventually expressed in terms of a persistence
length, directly related to the mean distance a walker propagates before suffering a
reversal in its motion.
To process the equations of motion, we deﬁne sum and difference distributions:
(x, t) = R(x, t) + L(x, t)
(5.45)
(x, t) = R(x, t) −L(x, t)
(5.46)
Then, the equations become
∂
∂t + v∂
∂x = 0
(5.47)
∂
∂t + v∂
∂x = −2γ 
(5.48)
The function (x, t) is the quantity of interest, as it is equal to the total number
of walkers reaching the location x at time t.
Exercise 5.4
Find the equation satisﬁed by the function (x, t). This equation is the one-
dimensional version of the Telegrapher’s equation.
Calculation of the distribution (x, t)
As noted above, our ultimate goal is the distribution (x, t), the distribution of all
walkers, both right- and left-movers. The ﬁrst step in the calculation of this distri-
bution is to take the spatial Fourier and temporal Laplace transforms of the above
equations. Let σ(q, λ) be the transformed function for  and δ be the transformed

106
Variations on the random walk
function of . The transformed equations are
λσ(q, λ) + iqvδ(q, λ) =
 ∞
−∞
(x, 0)eiqx dx
(5.49)
(λ + 2γ )δ(q, λ) + iqvσ(q, λ) =
 ∞
−∞
(x, 0)eiqx dx
(5.50)
Assuming that at t = 0, the distribution contains equal amounts of left- and right-
movers, and that all are concentrated at x = 0, the right hand side of (5.49) is equal
to one, and the right hand side of (5.50) is equal to zero. Solving (5.50) for δ(q, λ):
δ(q, λ) = −iqvσ(q, λ)
λ + 2γ
(5.51)
Substituting into (5.50), we end up with

λ2 + 2γ λ

+ (qv)2
σ(q, λ) = (λ + 2γ )
(5.52)
The reconstitution of the distribution is carried out with two integrations. We
start with the integration over q, which is relatively straightforward. Assuming, for
the time being that λ is real and positive, which it is in the standard deﬁnition of
the Laplace transform:
f (λ) =
 ∞
0
F(t)e−λt dt
(5.53)
we have for the Laplace transform of the spatial distribution (x, t)
1
2π
 ∞
−∞
(λ + 2γ )e−iqx
(qv)2 + λ2 + 2γ λ dq = 1
2
λ + 2γ
v
e−√
λ2+2λγ |x|/v

λ2 + 2γ λ
(5.54)
The ﬁnal stage in the derivation of the form of the distribution is the inverse Laplace
transform. This is obtained in the standard way (Jeffreys, 1972). The inverse of
(5.53) is
F(t) =
1
2πi
 ϵ+i∞
ϵ−i∞
f (λ)eλtdλ
(5.55)
Here, ϵ is a positive, real inﬁnitesimal. That is, one integrates with respect to λ
along, and just to the right of, the imaginary axis. The integrand has a branch cut on
the negative λ axis, from λ = −2γ to λ = 0 (See Figure 5.2). The quantity (x, t)
is then given by
(x, t) =
1
4πi
 i∞
−i∞
λ + 2γ
v
e−√
λ2+2λγ |x|/v

λ2 + 2γ λ
eλt dλ
(5.56)
If x > vt, the contour in (5.56) can be closed in the right half-plane, in which

5.2 The persistent random walk
107
−2γ
Re  λ
Im λ
Fig. 5.2. The cut in the integrand, and the contour along the imaginary λ axis.
there are no non-analyticities, and the integration yields the answer zero. On the
other hand, when x < t, the contour is closed in the left half-plane, and it can be
contracted until it wraps around the branch cut. The integral is then reduced with
the use of a change of variables. If one writes
λ = −γ (1 + sin φ)
(5.57)
The integral becomes
1
4π
2γ + ∂/∂t
v

e−γ t
 2π
0
e−γ t sin φ cos (γ x cos (φ) /v) dφ (vt −x)

(5.58)
where the Heaviside step function (vt −x) tells us that the result of the integration
is equal to zero when x > vt, as noted above. According to a standard formula
(Gradshteyn et al., 2000):
 2π
0
ea sin φ cos(b cos φ) dφ = 2π I0

a2 −b2

(5.59)
where I0 is a modiﬁed Bessel function. This tells us that the “sum” distribution,
(x, t) is given by
1
2

2γ + ∂
∂t
 
e−γ t I0

γ

t2 −x2/v2

(vt −x)

(5.60)
The ﬁnal result for the number of walkers in the “sum” distribution is
(x, t) = 1
2

2γ + ∂
∂t

e−γ t I0

γ

t2 −x2/v2

(vt −x)
+ e−γ t (δ(x −vt) + δ(x + vt))

(5.61)
To clarify the meaning of the delta functions in (5.61), we go back to the
case in which there is no interconversion. The equation that is satisﬁed by the

108
Variations on the random walk
Laplace- and Fourier-transformed sum distribution, σ(q, λ), is
σ(q, λ) =
λ
λ2 + q2v2
(5.62)
which is obtained from (5.52) by setting γ = 0 in that equation and solving for
σ(q, λ). Taking the inverse Fourier transform with respect to q, we are left with
e−λ|x|/v
2v
(5.63)
The inverse Laplace transform with respect to λ is, then, the integral along the
imaginary λ axis
1
2πi
 i∞
−i∞
e−λ|x|/veλt dλ
2v =
1
4πv
 ∞
−∞
eiω(t−|x|/v) dω
= 1
2δ(|x| −vt)
= 1
2 (δ(x −vt) + δ(x + vt))
(5.64)
The solution is a pair of delta functions, moving out to the right and left with a
speed equal to v. The delta functions in the distribution in (5.61) contain walkers
that have propagated ballistically without having undergone any interconversion.
Once a walker interconverts, it joins the “trailing” distribution described by the ﬁrst
contribution to the right hand side of that equation. As time goes on, this trailing
distribution settles into a Gaussian form, the width of which increases at a rate that
can be associated with the propagation of a random walker with a persistence length
calculable from the quantities v and γ .
Figure 5.3 displays a set of distributions of ensembles of persistent walkers at
various times. The walkers in the ensembles are assumed to have all started at x = 0
at time t = 0. Furthermore, each ensemble consists initially of equal numbers of
left- and right-moving walkers. The distributions are shown in the half-space x > 0.
The portions of the distributions lying in the half-space x < 0 are the mirror image
of those portions shown in Figure 5.3. We have set v = 1 and γ = 1 in calculating
the graphs.
Exercise 5.5
Show that the distribution of one-dimensional walkers tends to the standard Gaus-
sian form as time, t, becomes sufﬁciently large. You will need the asymptotic
formula
I0(z) →
ez
√
2πz
z is large

5.2 The persistent random walk
109
2
4
6
8
10
0.1
0.2
0.3
x
Σ(x,t)
Fig. 5.3. Sum distribution functions, as given by (5.61), for various times, in the
half-space x > 0. The distributions are symmetric with respect to reﬂection about
the origin. The delta functions at the leading edges of the distribution are indicated
in the ﬁgure. The speed, v, with which the walkers move is set equal to one, and
the times for which the distributions are depicted are t = 1, 3, 5 and 8. The rate of
interconversion, γ , is equal to one.
Exercise 5.6
How does the width of the distribution, as a function of x, depend on time, t and
γ ? Use your result to intuit a persistence length.
5.2.3 Persistent walks in two dimensions
We are now going to look at what happens in two dimensions. Here, things are a
bit more complicated, because, in part, there are more than two distributions. In
fact, there are an inﬁnite number of them, corresponding to the inﬁnite number of
directions in which a walker can be going in two dimensions. We’ll characterize
the direction of the walk by the angle θ. The equation for the distribution function
f (⃗r, t, θ) is
∂f (⃗r, t, θ)
∂t
+ v cos θ ∂f (⃗r, t, θ)
∂x
+ v sin θ ∂f (⃗r, t, θ)
∂y
= −f (⃗r, t, θ)
(5.65)
The terms on the left hand side of (5.65) describe the evolution of a distribution
in which the particles move ballistically. If the right hand side were equal to zero,
then the solution to this equation would have the general form
f (⃗r, t, θ) = f0(⃗r −ˆxvt cos θ −ˆyvt sin θ, θ)
(5.66)
That the right hand side of (5.66) is a solution to (5.65) with  = 0 is readily veriﬁed
by substitution.
The quantity  on the right hand side of (5.65) is an operator, not a number,
and it represents the “mixing” effect of randomizing events. The right hand side of

110
Variations on the random walk
(5.65) is, more explicitly, given by

dθ′ τ(θ, θ′) f (⃗r, t, θ′)
(5.67)
Let’s call the Fourier- and Laplace-transformed distribution F(⃗q, λ, θ). Then, (5.65)
becomes

λ + iv⃗q · ˆn + 

F(⃗q, λ, θ) = f (⃗q, t = 0, θ)
(5.68)
The right hand side of (5.68) is the Fourier-transformed distribution at t = 0, and
the quantity ˆn is the unit vector with x component equal to cos θ and y component
equal to sin θ.
5.2.4 The explicit form of the operator 
It is assumed that, at a rate γ , the distribution relaxes to a uniform one. That is, at
each point at which the random walker decides to change its direction, it does so
completely randomly. The way we write the operator in this case is
 = γ −γ |0⟩⟨0|
(5.69)
Here, |0⟩represents the normalized uniform distribution, which is independent of
θ. As is clear, we are making use of the Dirac bra and ket notation, in order to
take advantage of its compactness and ease of manipulation. The normalization is
⟨0|0⟩= 1. The form (5.69) ensures that |0⟩= 0. In other words, the direction-
independent portion of the distribution of walkers remains unchanged. All depen-
dence on the direction in which the walker moves washes out as scattering takes
place.
Note that if it were not for the second term on the right hand side of (5.69), then
what operates on F(⃗q, λ, θ) would be simply multiplicative. The solution of that
equation would then be
F(⃗q, λ, θ) =
f (⃗q, t = 0, θ)
λ + iv⃗q · ˆn + γ
(5.70)
and the solution for the distribution f (⃗r, t, θ) would then be the inverse Fourier and
Laplace transform of the right hand side of (5.70). Actually, the general solution of
the equation for the distribution in this case is
f (⃗r, t, θ) = f0(⃗r −vˆnt, θ)e−γ t
(5.71)
Again, the unit vector ˆn is given by
ˆn = ˆx cos θ + ˆy sin θ
(5.72)

5.2 The persistent random walk
111
Our general task is to ﬁnd the inverse of the operator in parentheses on the left
hand side of (5.68). This operator can be written in the form
A −γ |0⟩⟨0|
(5.73)
with
A = λ + iv⃗q · ˆn + γ
(5.74)
The inverse of this operator is
A−1 + γ A−1|0⟩⟨0|A−1
1 −γ ⟨0|A−1|0⟩
(5.75)
Exercise 5.7
Show that the operator in (5.75) is the inverse of the operator in (5.73).
Suppose, now that the initial distribution is uniform and is concentrated at the
location ⃗r = 0: f (⃗q, t = 0, θ) = |0⟩. Suppose also that we are interested in the
uniform portion of the distribution, which is directly relevant to the total number of
persistent walkers at a given point in space at a given time. We are then interested
in the amplitude
⟨0|

A−1 + γ A−1|0⟩⟨0|A−1
1 −γ ⟨0|A−1|0⟩

|0⟩=
⟨0|A−10|⟩
1 −γ ⟨0|A−1|0⟩
(5.76)
Equation (5.76) is the symbolic version of the solution in which we are interested.
To construct it explicitly, we need to evaluate the “expectation value” ⟨0|A−1|0⟩.
This is straightforward to set up. In two dimensions, we have
⟨0|A−1|0⟩= 1
2π
 2π
0
dθ
λ + iv(qx cos θ + qy sin θ) + γ
=
1

(λ + γ )2 + q2v2
(5.77)
Substituting this result into (5.76), we obtain for the Fourier- and Laplace-
transformed expression for the uniform distribution of persistent walkers
1

(λ + γ )2 + q2v2 −γ
(5.78)
The full expression for the quantity of interest is the double integral
1
2πi
 ∞
0
dq
 i∞
−i∞
dλ q J0(qr)eλt
1

(λ + γ )2 + q2v2 −γ
(5.79)

112
Variations on the random walk
iqv
Re  λ
Im  λ
Fig. 5.4. The analytic structure in the complex λ-plane of the integrand in (5.79).
Note the poles and the branch cut.
To evaluate the integral over λ in (5.79), we need to investigate the analytical
structure of the function in (5.78). As a function of λ, it has two poles and a branch
cut (Figure 5.4). To ﬁnd the poles, we solve the equation

(λ + γ )2 + q2v2 −γ = 0
(5.80)
The solution to this equation is
λ = −γ ±

γ 2 −q2v2
(5.81)
If we take the contributions from those two poles into account, we are left with the
integration
 ∞
0
2qγ J0(qr)e−γ t sinh

γ 2 −q2v2 t


γ 2 −q2v2
dq
(5.82)
Again, consulting a table of integrals (Gradshteyn et al., 2000), we ﬁnd
 ∞
0
x J0(cx)
sin

a
√
b2 + x2

√
b2 + x2
dx
=
 π
2 b1/2 
a2 −c2−1/4 J−1/2

b
√
a2 −c2

c < a
0
c > a
(5.83)
To match up with the integral that we want to do, we make the correspondences
c →r, a →vt, b →iγ . This leaves us with the result
2γ
π
2 e−γ t cosh

γ
√
v2t2 −r2

v
√
v2t2 −r2


v2t2 −r2
(5.84)
Again, the function  is the Heaviside step function. Note the “causality”

5.2 The persistent random walk
113
requirement it enforces. No walker progresses further than if it were propagating
ballistically.
We now have to look at the branch cut integration. This branch cut runs from
λ = −γ −iqv to λ = −γ + iqv. If we make the change of variables λ = −γ +
iqv sin θ, the integration over λ becomes
1
2π
 ∞
0
dq
 2π
0
q J0(qr)e−γ t eiqvt sin θqv cos θ
qv cos θ −γ
dθ
(5.85)
The integral simpliﬁes if we make use of the identity
1
qv cos θ −γ = −
 ∞
0
ewq cos θ−wγ dw
(5.86)
Inserting this into (5.85), we end up with the double integral
1
2π
 2π
0
dθ
 ∞
0
dwq J0(qr)e−γ te−wγ

−d
dw

exp (iqvt sin θ + wqv cos θ)
=
 ∞
0
dw q J0(qr)e−γ te−wγ

−d
dw

J0

qv

t2 −w2

= q J0(qr)e−γ t J0(qvt) −γ e−γ t
 ∞
0
q J0(qr)e−wγ J0

qv

t2 −w2

dw (5.87)
Using
 ∞
0
q J0(qx)J0(qy) dq = 2π
x δ(x −y)
(5.88)
we ﬁnd for the number of persistent walkers in two dimensions that are a point a
distance r from their common point of origin at a time t:
2π e−γ t
r
δ(r −vt) −2πγ
r
e−γ t
 ∞
0
δ

r −v

t2 −w2

e−γ w dw
= 2π e−γ t
r
δ(r −vt) −2π
γ
v
√
v2t2 −r2 e−γ te−γ
√
v2t2−r2/v(vt −r)
(5.89)
The structure of this distribution is similar to what we found in one dimension.
The analytic procedure utilized above carries over to three dimensions, as we will
see in the next section.

114
Variations on the random walk
5.2.5 The three-dimensional persistent walk
The procedure here is the same as in the case of the two-dimensional persistent
walk. The Laplace- and Fourier-transformed distribution is of the general form
A−1 + γ A−1|0⟩⟨0|A−1
1 −γ ⟨0|A−1|0⟩
(5.90)
and if we start with a distribution that is uniform and we are interested in the
contribution to the total distribution of its uniform component, the quantity of
interest is
⟨0|A−1|0⟩
1 −γ ⟨0|A−1|0⟩
(5.91)
In the case at hand, the expectation value in the above equations is given by
⟨0|A−1|0⟩= 1
4π

d
λ + γ + iv⃗q · ⃗n
(5.92)
The integral in (5.92) is actually over the surface of a three-dimensional sphere. We
can perform the integration by going to spherical coordinates, ˆn = ˆx sin θ cos φ +
ˆy sin θ sin φ + ˆz cos θ. We know that the integral will depend only on the magnitude
of ⃗q, so we can have that vector point entirely in the z direction, without loss of
generality. The integral in (5.92) then becomes
1
2
 π
0
sin θ
λ + γ + ivq cos θ dθ =
1
2vq ln
λ + γ + ivq
λ + γ −ivq

= 1
vq arctan
vq
λ + γ
(5.93)
The spatial distribution of persistent walkers, integrated over direction, as a function
of time is the inverse Fourier and Laplace transform of the function
 1
vq arctan
vq
λ + γ
  
1 −γ
vq arctan
vq
λ + γ

= 1
vq arctan
vq
λ + γ +

γ
(vq)2

arctan
vq
λ + γ
2  
1 −γ
vq arctan
vq
λ + γ

(5.94)

5.2 The persistent random walk
115
Re 2λ
Im 2 λ
2λ
Re 2λ
Im
Fig. 5.5. The contour of integration given the singularity structure of the function
in (5.93).
After performing the angular integration over the wavenumber q, we are left with
the following double integral for the distribution
 ∞
0
q sin qr
q
dq 1
2πi
 i∞
−i∞
dλ eλt

1
vq arctan
vq
λ + γ
+

γ
(vq)2

arctan
vq
λ + γ
2  
1 −γ
vq arctan
vq
λ + γ

(5.95)
Let us look at the leftmost term in curly brackets in (5.95). The integral over λ is
controlled by the singularity structure in the integrand in the complex λ-plane. This
consists of a branch cut running from λ = −γ −ivq to λ = −γ + ivq. Consider
the function on the last line of (5.93). When λ = −γ + ϵ + ixvq, the function is
equal to (1/vq)(π/2 −i ln((1 + x)(1 −x)), this if −1 < x < 1. On the other hand,
if λ is immediately to the left of the branch cut, λ = −γ −ϵ + ixvq, then the
function is equal to (1/vq)(−π/2 −i ln((1 + x)(1 −x)). We integrate up the right
hand sides of the branch cut and down the left hand side (see Figure 5.5). Given
that the real part of the function is the same on both sides of the branch cut, and
that the integrations on either side are in the opposite sense, we end up with the
following result for the branch cut integration
1
2π
1
vq
 vq
−vq
πeixte−γ t dx sin qr
r
(5.96)
Integration over the variable x yields
1
2π
2π
vq e−γ t sin vqt
t
sin qr
r
q
(5.97)
and, ﬁnally, integration over q yields
e−γ t
 ∞
0
1
vtr sin qr sin qvt dq = e−γ t
rvt
π
2 δ(r −vt)
(5.98)

116
Variations on the random walk
This is the exponentially decaying contribution to the distribution consisting of
walkers that have not made the plunge and taken a step in a direction different from
the one in which they originally started out.
The terms in square brackets in (5.95) require a bit more care. First off, there is
a slightly more complicated singularity structure. The function has the branch cut
described above. Also, as a function of λ, there is a pole on the negative λ axis.
This pole is at the location at which 1 = (γ/vq) arctan(vq/λ + γ ). Solving this
equation for λ:
λ = qv cot qv
γ −γ
(5.99)
This pole exists as long as 0 < q < γ π/2v. Closing around this pole, and correctly
evaluating the residue there entails taking the derivative
d
dλ

1 −γ
vq arctan
vq
λ + γ

λ=qv cot qv
γ −γ
=
γ sin2 
qv
γ

(qv)2
(5.100)
Making use of this result, we are left with the following integration over the variable
x = vq/γ
γ
v
2  π/2
0
x3
r
1
sin2 x e−γ teγ xt cot x sin(γ xr/v) dx
(5.101)
The next contribution comes from the branch cut illustrated in Figure 5.5. A careful
consideration of the structure of the integral leads to the following expression for
the integration around this contour
 ∞
−∞
dq
 1
−1
dx

1
2π

γ
(qv)2
π
2 −i
2 ln
1 + x
1 −x
2 

1 −γ
qv
π
2 −i
2 ln
1 + x
1 −x

× q2vsin qr
r
eiqvxte−γ t

(5.102)
Note the change in the range of the integration over q. Now, we perform the in-
tegration over q. The analytic structure of the integrand as a function of q is very
straightforward. There is a simple pole at the zero of the denominator in the in-
tegrand in (5.102). However, there is a convergence issue, having to do with the
behavior of the integrand as a function of q when that variable is large. We take care
of that by replacing q sin qr by −d/dr cos qr. We then accomplish the integration
by closing around that pole. The pole lies above the real q axis when x < 0 and
below when x > 0. Looking carefully at the complex exponentials that enter the in-
tegration, we see that we obtain a non-zero result only when v|x|t < r. Otherwise,
the contour can be closed in such a way that no singularity in q is enclosed. After

5.2 The persistent random walk
117
a bit more work, we ﬁnd that the expression of interest reduces to the following
single integration
e−γ t Re

1
r
 1
0
dx
γ
v
 π
2 −i
2 ln
1 + x
1 −x
2
×

i ∂
∂r

exp

−i(r −vxt)γ
v
π
2 −i
2 ln
1 + x
1 −x

(r −vxt)

= e−γ t Re

1
r
 1
0
dx
γ
v
2 π
2 −i
2 ln
1 + x
1 −x
3
× exp

−i(r −vxt)γ
v
π
2 −i
2 ln
1 + x
1 −x

(r −vxt)

+ e−γ t γ
v
1
rvt
π
2 ln
1 + r/vt
1 −r/vt

(vt −r)
(5.103)
Here, Re means “real part of.” The two terms on the right hand side of (5.103) arise
from the action of the r-derivative on the exponent and on the step function.
It can be shown that when r > vt, the expression in (5.103) cancels (5.101). Here
is how that is done. We note that when r > vt the upper limit on the x-integration
in (5.103) is 1. Then, we replace x in the integration by tanh y, where the range of
integration over y is from 0 to ∞. The integral is now
e−γ t Re
1
r
 ∞
0
dy
cosh2y
γ
v
2 π
2 −iy
3
exp

−i(r −vt tanh y)γ
v
π
2 −iy

(5.104)
Now, we deform the contour of the y-integration. First, we integrate from y = 0 to
−iπ/2 and then from y = −iπ/2 to −iπ/2 + ∞. The integrand along the second
contour is readily shown to be pure imaginary. As for the integral from 0 to −iπ/2,
we replace y in the integral by −iw. The integral is then
e−γ t Re
 π/2
0
γ
v
2 (−idw)
cos2w
π
2 −w
3
exp

−i(r + ivt tan w)γ
v
π
2 −w

(5.105)
We then replace w by π/2 −u, extract the real part – a straightforward procedure
now – and end up with the following expression
−e−γ t
 π/2
0
du
sin2u
γ
v
2
u3 sin(γ ur/v)
r
e−γ ut cot u
(5.106)
which exactly cancels (5.101).

118
Variations on the random walk
2
4
6
8
10
1
2
3
4
5
6
r  (r,t)
r
Σ  
2
Fig. 5.6. Distributions of the walkers, (r, t), multiplied by r2, for various values
of t. The parameters v and γ have both been set equal to one. The functions (r, t)
plotted in this graph are given by the sum of the expressions in (5.98), (5.101), and
(5.103).
There appears to be no way to further reduce the integrals above. However,
numerical integration generates results for the total density of persistent three-
dimensional walks. Figure 5.6 shows a set of curves, without the delta function
contributions for the distribution of such walkers, integrated over angles. Note that
the distribution diverges logarithmically at the leading edge. This is because of
the second term on the right hand side of (5.103). We have veriﬁed by numerical
integration that the total number of walkers is conserved. In fact, if we combine all
three terms, (5.98), (5.101), and (5.103), we end up with a density as a function of
r that, when multiplied by r2, integrates to π/2.
5.3 The continuous time random walk
We’ve looked at walks that take steps in a regular, predictable progression. All of
the randomness appears in the choice of directions. We have also looked at walks
that propagate continuously, making random changes of direction at random times.
As yet another variation, we are going to imagine a walk that sits still between steps,
which it takes after a variable time interval. Furthermore, the likelihood that the walk
takes a step at a given point of time may possess a non-trivial time-dependence. The
analysis again makes use of the Laplace transformation. In our study of persistent
walks, we’ve seen that the Laplace transform is especially useful in the case of a
process involving time delays.
We’ll start by posing questions about the likelihood of a walker taking a certain
number of steps in a given time interval. That is, we ask: what is the likelihood
that at time t the walker has taken N steps? We will deﬁne the likelihood that the
walker takes a step at some time t after it has arrived at a point in terms of the
probability density q(t) as follows. The probability that the walker takes a step in

5.3 The continuous time random walk
119
the time interval between t and t + dt after arriving at the point at which it now sits
is equal to q(t) dt, assuming that it has not taken a step before the time t. Then, the
probability that it has not taken a step by the time τ after it has arrived at the point
at which it now sits is equal to the product of the probability that it has not taken a
step during any of the time intervals leading up to τ. This probability is equal to
m

i=1
(1 −q(ti) dti)
(5.107)
where we have divided the time between t = 0 and t = τ into m intervals. The
product will ultimately be over an inﬁnite set of terms, each corresponding to an
inﬁnitesimal time interval. We can then reformulate the product as follows:
m

i=1
(1 −q(ti) dtt) = exp
 m

i=1
ln (1 −q(ti) dti)

= exp

−
m

i=1

q(ti) dti + O

dt2
i


→exp

−
 τ
0
q(t) dt

≡a(τ)
(5.108)
If the integral
 ∞
0 q(t) dt is inﬁnite, then the probability of not taking a step goes
to zero as τ →∞. On the other hand, if the integral does not diverge, there is a
ﬁnite probability that the walker will never take a step after arriving at its current
position.
Now, let’s calculate the probability that the walker takes no step until t = t1, and
then takes a step between t = t1 and t = t1 + dt. This probability is a(t1)q(t1) dt.
The probability that this will happen at any time is the sum over all times t1, which
is the integral
 ∞
0
a(t1)q(t1) dt1 =
 ∞
0
exp

−
 t1
0
q(t) dt

q(t1) dt1
=
 ∞
0
exp

−
 t1
0
q(t) dt
  d
dt1
 t1
0
q(t) dt

dt1
= −exp

−
 t1
0
q(t) dt

∞
0
= a(0) −a(∞)
= 1 −a(∞)
(5.109)
This result makes sense. The likelihood that the walker takes one step ever is equal
to one minus the probability that it never takes a step.

120
Variations on the random walk
Now, let’s ﬁgure out the probability that the walker takes a single step between
the time t = 0 and the time t = τ. This is equal to the probability that the walker
takes no steps until the time t1, where t1 < τ, then takes a step in the time interval
between t1 and t1 + dt, then takes no more steps until t = τ, integrated over all
possible times t1 between t1 = 0 and t1 = τ. This combination is equal to
 τ
0
a(t1)q(t1)a(τ −t1) dt1
(5.110)
We can clearly go further, and ﬁnd out the likelihood that the walker will take m
steps between a time t = 0 and t = τ. This is the multiple integral
 t1
0
dt0
 t2
0
dt1 · · ·
 τ
0
dtm−1a(t0)q(t0)a(t1 −t0)q(t1 −t0) · · · a(τ −tm−1)
(5.111)
This multiple integral is in the form of a convolution. Its evaluation is not at all
trivial. However, there is a way to simplify the task of determining its value, and
that is to make use of the Laplace trasform. Recall, the Laplace transform, F(λ),
of a function of t, f (t), which is deﬁned for t ≥0, is given by
F(λ) =
 ∞
0
e−λt f (t) dt
(5.112)
For example, the Laplace transform of the probability that the walker takes a step
at exactly the time t after it has arrived at a certain position is given by
 ∞
0
e−λta(t)q(t) dt
(5.113)
If we deﬁne the function Q(t) as follows:
Q(t) =
 t
0
q(t′) dt′
(5.114)
then,
a(t) = e−Q(t)
(5.115)
and the Laplace transform in (5.113) is equal to
 ∞
0
e−Q(t) dQ(t)
dt
e−λt dt
(5.116)

5.3 The continuous time random walk
121
Let’s deﬁne the following Laplace transforms
A(λ) =
 ∞
0
a(t)q(t)e−λt dt
=
 ∞
0
e−Q(t)e−λt dQ(t)
dt
dt
(5.117)
B(λ) =
 ∞
0
a(t)e−λt dt
(5.118)
The relationship between A(λ) and B(λ) is readily established with the use of
(5.117) and integration by parts.
A(λ) =
 ∞
0
e−λte−Q(t) dQ(t)
dt
dt
= −e−Q(t)e−λt∞
0 −λ
 ∞
0
e−Q(t)e−λt dt
= 1 −λB(λ)
(5.119)
The probability displayed in (5.111) has a Laplace transform that is given by
 t1
0
dt0
 t2
0
dt1 · · ·
 τ
0
dtm−1
 ∞
0
dτ a(t0)q(t0)a(t1 −t0)q(t1 −t0) · · · a(τ −tm−1) e−λτ
=
 t1
0
dt0
 t2
0
dt1 · · ·
 τ
0
dtm−1
 ∞
0
dτ a(t0)q(t0) e−λt0a(t1 −t0)q(t1 −t0)
× e−λ(t1−t0) · · · a(τ −tm−1) e−λ(τ−tm−1)
(5.120)
Further analysis of this multiple integral follows from a reordering of the integra-
tions. We note that the function a(t) is equal to zero when t < 0. Then, if the order
is reversed, the integral over τ is from tm−1 to ∞. Then, the integral over tm−1 is
from tm−2 to ∞, and so on. The expression is reduced to
 ∞
0
a(τ1)q(τ1) e−λτ1
 ∞
0
a(τ2)q(τ2) e−λτ2 · · ·
 ∞
0
a(τm+1) e−λτm+1 dτm+1
= A(λ)m B(λ)
(5.121)
What, then, is the Laplace transform of the probability that the walker will have
taken no steps in the time interval τ, or that it will have taken one step in that time
interval, or two, or three, or any number of steps in that time interval? This Laplace

122
Variations on the random walk
transform is given by the geometrical sum
∞

m=0
A(λ)m B(λ) =
B(λ)
1 −A(λ)
(5.122a)
=
B(λ)
1 −(1 −λB(λ))
(5.122b)
= 1
λ
(5.122c)
In (5.122b), we made use of the relationship between A(λ) and B(λ) displayed in
(5.119).
We have already seen how to perform the inverse Laplace transform (See (5.55)
in Section 5.2.2). In the case at hand, we need only note that 1/λ is the Laplace
transform of the constant 1:
 ∞
0
e−λt dt = 1
λ
(5.123)
Thus, we have recovered the pretty-much-obvious result that the probability that
the walker will take any number of steps (including none at all) in a given time
interval is equal to one, regardless of the length of the time interval.
Let’s now take into account the fact that the walker moves some distance at each
step. We’ll do this by making use of the transform of the probability that the step
takes it to a position a displacement vector ⃗r away from where it was standing.
Recall that the probability that the walker has traversed a displacement vector ⃗R in
two steps is given by
P

⃗R

=

ddr1

ddr2 p

⃗r1

p

⃗r2

δ

⃗r1 + ⃗r2 −⃗R

(5.124)
where P( ⃗R) is the probability density that the walker steps a distance ⃗R. If we take
the spatial Fourier transform of this probability, we obtain

e−i⃗q· ⃗R P

⃗R

dd R =

dd R

ddr1

ddr2 p

⃗r1

p

⃗r2

δ

⃗r1 + ⃗r2 −⃗R

e−i⃗q· ⃗R

=

ddr1e−i⃗q·⃗r1 p

⃗r1
 
ddr2e−i⃗q·⃗r2 p

⃗r2

≡χ

⃗q
2
(5.125)
where, as previously,
χ

⃗q

=

e−i⃗q·⃗r p

⃗r

ddr
(5.126)

5.3 The continuous time random walk
123
The spatial Fourier transform of the probability that the walker has traversed a net
displacement vector in n steps is, by an extension of the above, equal to χ(⃗q)n.
This means that the Laplace and spatial Fourier transform of the probability that
the walker has taken n steps in an interval and has traversed a certain displacement
vector as a result of those n steps is given by the formula

A(λ)χ

⃗q
n B(λ)
(5.127)
Summing over n, the Laplace and Fourier transform of the probability that the walk
has arrived at a given location at a particular point in time (taking into account the
possibility that any number of steps may have been taken) is equal to
B(λ)
1 −χ

⃗q

A(λ)
(5.128)
The likelihood that the walker has ended up a displacement vector ⃗r away from
its point of origin at a time t after it started out is given by the inverse Laplace
and spatial Fourier transform of (5.128). Let’s see how the familiar Gaussian limit
asserts itself. We’ll do this by expanding to low order in both λ and ⃗q. We know
that if the walker takes steps that are not too long, we can write
χ(⃗q) = 1 −Cq2
(5.129)
We’ll assume that we can perform a similar expansion of A(λ). Making use of
(5.119):
A(λ) = 1 −λB(λ)
≈1 −λB(0)
(5.130)
Then, the combined Laplace and Fourier transform of the desired probability is,
approximately,
B(0)
1 −

1 −Cq2
(1 −λB(0)) ≈
B(0)
λB(0) + Cq2
(5.131a)
=
1
λ + (c/B(0))q2
(5.131b)
Now, we note that the Laplace transform of the function e−at is given by
 ∞
0
e−ate−λt dt =
1
λ + a
(5.132)
which tells us that (5.131b) is the Laplace transform of e−Ctq2/B(0). The inverse

124
Variations on the random walk
Fourier transform of this quantity is

ddqei⃗q·⃗re−Ctq2/B(0) =
π B(0)
Ct
d/2
e−B(0)r2/4Ct
(5.133)
The probability distribution is again a Gaussian, with a width that is proportional
to the time interval, t.
Exercise 5.8
Suppose that the function q(t) is equal to a constant. Let this constant be p0. Find
the combined Laplace and Fourier transform of the probability that the walk has
gotten to a given location at a speciﬁc time. That is, ﬁnd the explicit form of the
function (5.128).
Exercise 5.9
Another wrinkle on the continuous time random walk problem is to ask what hap-
pens when there is also the ﬁnite probability that a walker will disappear. Suppose
that in a given small time interval t, the walker takes a step with a probability
equal to p0t, and with a probability p1t the walker disappears. We now deﬁne
a(t) as the number of remaining walkers that have not yet taken a step. Then, a(t)
can be shown to be given by
a(t) = e−(p0+p1)t
and
a(t)q(t) = p0e−(p0+p1)t
Show that this tells us that
A(λ) =
p0
p0 + p1 + λ
and that
B(λ) =
1
p0 + p1 + λ
Use these results and (5.128) to ﬁnd the Laplace and Fourier transform of the
probability that a walker ﬁnds itself at a certain location at a particular time. Finally,
makeuseoftheresultyouhavejustobtainedtocalculatetheprobabilityasafunction

5.3 The continuous time random walk
125
of space and time. You can use (5.129). Show that the total number of walks that
remain at a time t decays exponentially. What is the rate of decay?
A discussion of the non-Gaussian case will be delayed until later (see
Chapter 8), where our approach will be slightly different. We leave this chapter
here, as our focus in the remaining chapters will be on other, less traditional, treat-
ments of the random walk problem. However, we commend to the reader further
study of the various “non-ideal” random walks introduced in this chapter; they are
rich in theoretical insights and practical applications (Barber and Ninham, 1970;
Hughes, 1995; Weiss, 1994).


6
The shape of a random walk
6.1 The notion and quantiﬁcation of shape
Shape is an intuitively accessible notion. We organize visual information in terms
of shapes, and the shape of an object represents one of the ﬁrst of its qualities
referred to in an informal descriptive rendering of it. While our language presents
us with a wide repertoire of verbal images for the approximate portrayal of the
shape of a physical entity (“round,” “oblong,” “crescent,” “stellate” . . .) the precise
characterization of a shape, in terms of a number, or set of numbers, has remained
elusive. This is with good reason. It is well-known to mathematicians that the
class consisting of the set of all curves is a higher order of inﬁnity than the set
of all real numbers. This means that there can be no one-to-one correspondence
between curves and real numbers. As shapes, intuitively at least, bear a conceptual
relationship to curves, it is plausible that the set of all shapes dwarfs in magnitude
the set of real numbers, or of ﬁnite sets of real numbers.
On the other hand, if one is willing to content oneself with a general paradigm for
the measurement of shape, there are ways of quantifying it in terms of numbers that
have a certain descriptive and predictive utility. In fact, the numerical speciﬁcation
of shapes has acquired a certain urgency of late, in light of the widespread use of
computer imaging and the concomitant focus on the development of codes for the
creation and manipulation of pictorial quantities.
In this chapter, we will look at different ways of characterizing and measuring
the shape of a random walk. We will focus on one particular method, based on
calculations of the width of the distribution of steps about the “center of mass” of
the walk. The particular quantity studied is the radius of gyration tensor, and the
shape of the polymer is quantiﬁed in terms of the eigenvalues of this tensor, termed
the principal radii of gyration of the walk. We will look at a particular combination
of the principal radii of gyration that provides information with respect to the
deviation from spherical symmetry of the shape of the walk. It will turn out that
127

128
Shape of a random walk
Fig. 6.1. A “cloud” consisting of the paths of 1000 random walkers, each of which
has taken 100 steps from a common point of origin.
shape as a concept is, as one might expect, a bit elusive. For one thing, there is no
generic “shape” for a random walk. However, statistical statements can be made,
with regard to the probability that a walk takes on a particular shape, at least as
characterized by the principal radii of gyration. In addition, there is one limit in
which the shape of the trail left by a walker is ﬁxed and predictable. That is the
limit of a walker in an inﬁnite dimensional space (d = ∞). We will discuss the
construction of an expansion about that limit, the 1/d-expansion. This expansion
yields the shape distribution of a random walker’s trail when the walker wanders in
a high spatial dimension environment. As we will see, this expansion is – at least
for some puroposes – respectably accurate in three dimensions.
6.1.1 Anisotropy of a random walk
When we talk about the distribution of points visited by a random walker, we
generally do so in the context of ensemble averages. That is, we ask on average
how many walks visit a given point. Looking at things this way can obscure the
detailed structure of a given random walk. For example, if we are interested in
how many times a given point at location ⃗r1 is visited by a walker that starts out
at location ⃗r0, we ﬁnd, after suitable averaging, that the answer depends only on
the distance between those points in space, |⃗r1 −⃗r0|. This is true because for every
walker that tends to go off in one direction there will be another walker that ends
up going in the opposite direction. The statistical distribution of places visited is
rotationally symmetric about the point of origin. In other words, the totality of
walkers in the ensemble creates a “cloud” that is spherically symmetric. Figure 6.1
shows just such a cloud, which consists of the paths of 1000 random walkers each
of whom has taken 100 steps from a common point of origin. The near-spherical
symmetry of the cloud is evident from the ﬁgure.

6.1 The notion and quantiﬁcation of shape
129
Fig. 6.2. Stereographic pair of images of a 1000-step three-dimensional random
walk.
Fig. 6.3. Several examples of a 1000-step random walk.
This result of averaging obscures the fact that a given random walk can be quite
anisotropic spatially. Figure 6.2 shows a stereographic pair of images of a single
1000-step random walk. The elongated nature of the walk shown in this ﬁgure is not
a statistical anomaly. Figure 6.3 shows several examples of 1000-step walks. Note
thatnotoneofthosewalksisreminiscentofthecloudofwalkersshowninFigure6.1.
On the other hand, as Figure 6.3 makes abundantly clear, no typical, or deﬁnitive,
shape can be assigned to a random walk. How, then, to quantify the shape of a walk?
6.1.2 Measures of the shape of a walk
The literature presents a number of algorithms for the characterization of the shape
of an object (Bookstein, 1978; Costa and Cesar, 2001). Here, we choose one that is

130
Shape of a random walk
Fig. 6.4. The anisotropic nature of a 1000-step two-dimensional walker. The
shaded lines indicate the directions in which its linear extent is the greatest and
the smallest. The lines also run parallel to the eigenvectors of the matrix deﬁned in
(6.1)–(6.3). The point of intersection of those two lines is the “center of gravity”
of the walk.
particularly well-suited to our needs. We construct a moment-of-inertia-like tensor
(See Supplement 1 at the end of this chapter). By diagonalizing this tensor we
are able to extract numbers that quantify the linear “size” of the walk in various
directions, particularly in the directions in which it has the greatest linear extent
and the direction in which it is most compact. For a visualization of this, see
Figure 6.4, in which the extensions in both directions of a two-dimensional random
walk are illustrated. The thick lines indicate the directions in which random walk
has the greatest and the smallest extension, as determined by the tensor that we are
about to introduce. Note that these two lines provide a quantitative representation
of both the overall orientation of the walk and of its spatial anisotropy –that is, the
degree to which the shape of the walker’s path differs from that of a sphere. As we
will see, the amount of anisotropy exhibited by the walks in Figures 6.2, 6.3, and
6.4 is not at all atypical.1
6.1.3 The radius of gyration tensor
The tensor that we are about to deﬁne is also used to determine the rotational inertia
of a three-dimensional object. Supplement 1 at the end of this chapter reviews its
use in that context. What this means is that the results to be derived here are relevant
to the rotational motion of an object that mimics the form of the path followed by a
random walker, assuming that the constituents of this object have an inertial mass,
that they are uniformly distributed along the path it imitates, and that the object is,
itself, rigid.
1 For a characterization of the anistropy of a random walker using the notion of spans, see (Weiss and Rubin,
1976). Here, our approach will be somewhat different.

6.1 The notion and quantiﬁcation of shape
131
Here is how the tensor is constructed (Solc and Stockmayer, 1971). Given the
location, ⃗ri (1 ≤i ≤N), of each step of a walker in d dimensions that has left N
footprints, we construct a d-dimensional tensor,
↔
T with entries
Tkl = 1
N
N

j=1

r jk −⟨rk⟩
 
r jl −⟨rl⟩

(6.1)
Here, r jk is the kth component of the position vector of the jth step, and ⟨rk⟩is the
average of the kth component of the locations of the steps of the walker:
⟨rk⟩= 1
N
N

j=1
r jk
(6.2)
For example, a walker in two dimensions has radius of gyration tensor
↔
T with the
following form
↔
T =


1
N
N
j=1

x j −⟨x⟩
2
1
N
N
j=1

x j −⟨x⟩
 
y j −⟨y⟩

1
N
N
j=1

x j −⟨x⟩
 
y j −⟨y⟩

1
N
N
j=1

y j −⟨y⟩
2


(6.3)
The eigenvectors and eigenvalues of this tensor quantify the linear dimensions of
the walker – its girth – in various directions. The eigenvectors point in the direction
in which this span is maximized, and the direction in which it is minimized. The
eigenvalues tell us how extended the walk is in those extremal directions. In fact,
the lines in Figure 6.4 lie along the directions in which those two eigenvectors point.
The lengths of those lines are directly proportional to the eigenvalues of the matrix
↔
T appropriate to the walk in that ﬁgure.
For a discussion of the relationship between the eigenvectors and eigenvalues of
↔
T and the maximal and minimal spans of a walk see Section 6.4.1 in Supplement
1 at the end of this chapter.
6.1.4 Eigenvalues of the matrix
↔
T : the asphericity of a random walk
The eigenvalues of the matrix
↔
T are the squares of the principal radii of gyration,
Ri, of the object in question. They are essentially the mean square deviations of
the steps of the walker from the “center of gravity” of the walk. In Figure 6.4 the
walk’s center of gravity lies at the point of intersection of the two thick lines, each
of which lie in the direction of the eigenvectors of the matrix
↔
T for that walk. This

132
Shape of a random walk
means that diagonalized, the matrix
↔
T takes the form
↔
T =


λ1
0
0
0
λ2
0
0
0
λ3


≡


R2
1
0
0
0
R2
2
0
0
0
R2
3


(6.4)
The relative magnitudes of the eigenvalues of the radius of gyration tensor
↔
T then
tell us to what extent the object in question has a shape that differs from that of a
sphere. Clearly if all R2
i ’s in (6.4) are equal, then the linear span of the object will
be the same in all directions, and it is reasonable to attribute a kind of spherical
symmetry to it. However, if, for example, R2
1 ≫R2
2, R2
3, which means that R1 is
signiﬁcantly larger than R2 and R3, then the object can be thought of as greatly
elongated, and not at all spherical.
The eigenvalues of an object’s radius of gyration tensor are invariant with respect
to the overall orientation of the object. That is, a rotation of the object will not change
those eigenvalues. On the other hand, the tensor itself does change as the object is
rotated. If the brackets ⟨· · · ⟩r stand for averaging with respect to overall orientation,
then the average ⟨R2
i ⟩r is just the same as R2
i . On the other hand, performing the
same average over
↔
T produces a matrix altered by the averaging process. In fact,
it is pretty straightforward to argue that ⟨(x −⟨x⟩)(y −⟨y⟩)⟩r will average to zero,
while ⟨(x −⟨x⟩)2⟩r = ⟨(y −⟨y⟩)2⟩r = ⟨(z −⟨z⟩)2⟩r This means that
⟨
↔
T ⟩=


¯T
0
0
0
¯T
0
0
0
¯T


(6.5)
The eigenvalues of this matrix are clearly all equal to ¯T . In averaging the radius
of gyration tensor, we are performing the kind of ensemble average that destroys
information regarding the non-spherical shape of the object in question. This clearly
means an informative characterization of the shape of the random walk is not
contained in the averaged radius of gyration tensor.
We can, nevertheless extract useful shape information by averaging quantities
that are directly derivable from the radius of gyration matrix. What we need to
do is use quantities that are invariant with respect to rotations and reﬂections in
space (the matrix is automatically invariant with respect to translations). All of
these quantities are directly related to the eigenvalues of the matrix. In the case of
a three-dimensional matrix there are three independent invariants. One choice of

6.1 The notion and quantiﬁcation of shape
133
those three is
Tr
↔
T = T11 + T22 + T33
= R2
1 + R2
2 + R2
3
(6.6)
Tr
↔
T
2
= (R2
1)2 + (R2
2)2 + (R2
3)2
(6.7)
Tr
↔
T
3
= (R2
1)3 + (R2
2)3 + (R2
3)3
(6.8)
Another well-known invariant of the tensor, its determinant, is obtained as follows
Det
↔
T = R2
1 R2
2 R2
3
= 1
6
	
R2
1 + R2
2 + R2
3
3 + 2

(R2
1)3 + (R2
2)3 + (R2
3)3
−3

R2
1 + R2
2 + R2
3
 
(R2
1)2 + (R2
2)2 + (R2
3)2
= 1
6

Tr
↔
T
3
+ 2 Tr
↔
T
3
−3 Tr
↔
T Tr
↔
T
2
(6.9)
Now, it is possible to average the three invariants deﬁned in (6.6)–(6.8). These aver-
ages retain important information regarding the devation from spherical symmetry
of the shape of the “average” random walk. Consider, for example, the following
combination of eigenvalues

R2
1 −R2
2
2 +

R2
1 −R2
3
2 +

R2
2 −R2
3
2
= 3

(R2
1)2 + (R2
2)2 + (R2
3)2
−

R2
1 + R2
2 + R2
3
2
= 3 Tr
↔
T
2
−

Tr
↔
T
2
(6.10)
Both sides of this equation can be averaged over all orientations of an object,
and, given the fact that they are invariants with respect to translation, rotation and
reﬂection, they will remain unchanged. In the case of the random walk, this means
that if we average the last line of (6.10) over an ensemble of walkers we are left
with a quantity that tells us something about the differences between the various
principal radii of gyration. That is, we ﬁnd out how different the shape a random
walk is, on the average, from that of a sphere.
To construct a quantity that interpolates between zero when all principal radii of
gyration are equal and unity when one of the Ri’s is much greater than the others
we will divide by
2
 3

i=1
R2
i
2
= 2⟨Tr
↔
T ⟩2
(6.11)
We can, in fact, generalize this quantity and deﬁne the mean asphericity, Ad, of

134
Shape of a random walk
d-dimensional random walks as follows (Aronovitz and Nelson, 1986; Rudnick
and Gaspari, 1986a; Theodorou and Suter, 1985):
Ad =
d
i> j⟨(R2
i −R2
j)2⟩
(d −1)⟨(d
i=1 R2
i )2⟩
(6.12)
The numerator of (6.12) can be rewritten as follows:

R2
1 −R2
2
2 +

R2
1 −R2
3
2 + · · ·

R2
d−1 −R2
d
2
= d Tr⟨
↔
T
2
⟩−

Tr
↔
T
2
= d(d −1)

⟨T 2
11⟩−⟨T11T22⟩

+ d2(d −1)⟨T 2
12⟩
(6.13)
The last line of (6.13) follows from the equations for the trace of a tensor and of
its square. It also follows from the fact that ⟨T 2
11⟩= ⟨T 2
22⟩= · · · ⟨T 2
dd⟩, and similar
equalities for ⟨TiiTj j⟩and ⟨T 2
i j⟩. The denominator of the last line of (6.12) can be
reduced in the same way, leading to the following expression for the asphericity:
Ad = d(d −1)

⟨T 2
11⟩−⟨T11T22⟩

+ d2(d −1)⟨T 2
12⟩
d(d −1)⟨T 2
11⟩+ d(d −1)2⟨T11T22⟩
=

⟨T 2
11⟩−⟨T11T22⟩

+ d⟨T 2
12⟩
⟨T 2
11⟩+ (d −1)⟨T11T22⟩
(6.14)
The calculation of the asphericity reduces to the problem of determining the average
values of powers of the entries in the radius of gyration tensor. The details, which
are a bit involved, are in Supplement 2 at the end of this chapter. The end result
of the calculation is the following general expression for the mean asphericity of a
d-dimensional random walk:
Ad = 4 + 2d
4 + 5d
(6.15)
The three-dimensional walk has a mean asphericity of 10/19, or a little more than
a half, so in this sense the three-dimensional walk is, on the average, somewhere
between an isotropic object and a highly elongated one.
Of course the notion of the mean asphericity of a random walk does not neces-
sarily imply that there is a characteristic shape for three-dimensional walks. Given
the examples depicted in Figure 6.3, it seems much more likely that random walks
come in a wide variety of shapes and that a quantity such as the mean asphericity
provides a very broad-brush characterization of that property of random walks.
Figure 6.5 illustrates this point. It is a histogram of the distribution of the indi-
vidual asphericities of 20000 three-dimensional walks, each comprising 100 steps.

6.2 Walks in d ≫3 dimensions
135
0.2
0.4
0.6
0.8
250
500
750
1000
1250
1500
1750
Fig. 6.5. The distribution of the individual asphericities of 20 000 three-
dimensional 100-step walks.
Note that the distribution spans the range from 0 to 1, and that no narrow region
dominates.
It is also important to note that what is presented in (6.15) is not, strictly speaking,
the average of the individual asphericities of the walks, which is given by
A′
d =

i< j

R2
i −R2
j
2
(d −1) N
k=1 R2
k

(6.16)
This quantity can also be found exactly in the case of the ordinary d-dimensional
walk. The analytical result for this quantity is (Diehl and Eisenriegler, 1989)
A′
d = d
4

3 + 4
d −d
2 Md/2

(6.17)
where
Mp =
 ∞
0
x p+1 sinh−px dx
(6.18)
Inthreedimensions, A′
d = 0.394274 . . . .Theaverageoftheindividualasphericities
is somewhat smaller than the mean asphericity.
Exercise 6.1
Show that (6:S2-2b) in Supplement 2 is correct.
6.2 Walks in d ≫3 dimensions
Often, mathematical models for physical processes are especially amenable to anal-
ysis if the system under consideration is imagined to occupy a ﬁctitious universe
consisting of a large number of spatial dimensions. We will encounter this when

136
Shape of a random walk
we discuss the statistics of self-avoiding walks in later chapters. In that case, if the
dimensionality of the space exceeds four, the statistical properties of a self-avoiding
walk are identical in all important respects to those of an ordinary walk. There are
similar simpliﬁcations in other useful physical models. Given this, a question worth
asking is what shape a random walk takes if the walker is free to explore a large
number of spatial dimensions. Do shape statistics simplify? If so, is there any way
of extrapolating from those statistics to three dimensions?
The mean asphericity, Ad, provides a clue to the behavior of the high-dimensional
random walk. As d →∞, the mean asphericity approaches 2/5. This tells us that
random walks in high dimensions are not spherical, nor are they highly elongated.
They retain, statistically at least, a shape intermediate between those two extremes.
What more can we say about the shape, or range of shapes, of the walk in d ≫1
dimensions?
As it turns out, the shape of a walk in high spatial dimensionality is not only
well-deﬁned, it is, in a sense, universal, in that essentially every random walk has
the same basic shape. We can see this by placing the walker on a d-dimensional
cubic lattice, with d a very large number. In such a lattice each vertex is connected
to 2d nearest neighbors by links that are parallel to the d Cartesian coordinate
axes. A walker will set out from its point of origin along one of those links. Upon
arriving at the nearest neighbor vertex joined to the point of origin by that link, the
walker chooses another link along which to proceed. The choice is purely random,
so it is equally likely to take its next step along any one of the 2d links attached
to that vertex. With overwhelming likelihood, the walker will take its next step in
a direction perpendicular to the one it traveled in its previous step. This is because
there are 2(d −1) choices that cause it to do so and only two that have it moving
parallel, or antiparallel, to its ﬁrst step. At the next vertex, the walker will, with a
probability essentially equal to one, take a step in a direction that is perpendicular
to those of the ﬁrst two. Continuing in this way, the walker executes an N-step walk
that consists of a set of displacements, all of which are mutually perpendicular.2
Exercise 6.2
Consider a walker on a d-dimensional hypercubic lattice. For this walker, show
that the number of N-step walks, in which the walker takes each step in a direction
orthogonal to all previous steps, is approximately equal to
(2d)Ne−N 2/2d

N ≪
√
d

2 This argument, and the results described immediately below, can be found in expanded form in (Rudnick et al.,
1987)

6.2 Walks in d ≫3 dimensions
137
Taking into account the possibility of rotating the walk, and of forming mirror im-
ages, we see that every N-step walk in d ≫1 dimensions will have the same shape.
This, of course, holds true in the strict sense only when N ≪d. The construction of
the radius of gyration tensor
↔
T for such a walk is relatively straightforward (Rud-
nick et al., 1987). The willing reader is led through the essential steps in Exercise
6.3, and the results are obtained using an alternative approach in the next section.
This tensor can be diagonalized, and one ﬁnds for the eigenvalues
R2
n = (N + 1)
π2n2
n ≥1
(6.19)
The principal radii of gyration of the random walk in inﬁnite spatial dimensions
are, with perfect certainty, equal to the above set of values. The asphericity is given
by
A∞=

n

R2
n
2

n R2n
2
(6.20a)
=
∞
n=1 n−4
∞
n=1 n−22
(6.20b)
= π4/90

π2/6
2
(6.20c)
= 2
5
(6.20d)
Equation (6.20c) follows from known results for the inﬁnite sums in the numerator
and denominator in (6.20b) (Gradshteyn et al., 2000).
The simplicity of the shape distribution when d = ∞allows us to hope that we
can extrapolate to ﬁnite dimensions and obtain information about the shapes of
walkers in dimensions of direct interest to us, for instance d = 3.
Exercise 6.3
Here, we are going to go through the derivation of the eigenvalues of the radius of
gyration tensor,
↔
T , in the case of a walker that wanders on a hypercubic lattice in
d dimensions in the limit d = ∞. This exercise asks that you ﬁll in the steps in the
calculation.
(a) The walker is assumed to start out from a site at the origin (xi = 0 for all i). At the ﬁrst
of N steps, the walker moves in the direction of x1, and takes a step with a length of l.
At the second step the walker moves in the positive direction along the x2 axis, at the
third step in the positive direction along the x3 axis, and so on. Note that we can always

138
Shape of a random walk
arrange our notation so that this is so. If the walker takes N steps, so that it visits N + 1
sites, show that ⟨xn⟩= l(N + 1 −n)/(N + 1), where the average is over points visited
by the walker.
(b) Now, show that at the jth step the following is true
x j,n −⟨xn⟩=



−l
 N + 1 −n
N + 1

j < n
l

1 −N + 1 −n
N + 1

j ≥n
(c) Use the above results to show that, for this walk,
N+1

j=1

x j,n −⟨xn⟩
 
x j,m −⟨xm⟩

= l2(N + 1)
n
N + 1

1 −
m
N + 1

n ≤m
If m < n, the result is the same as the above with m and n interchanged.
(d) Deﬁne x ≡n/(N + 1), y ≡m/(N + 1). Note that the radius of gyration tensor becomes
an operator expressed in terms of x and y. The range of this operator is the interval
between 0 and 1. The operator is equal to zero whenever either x or y is on the boundary
of the interval. Show by direct substitution that sin kπy is an eigenvector of this tensor,
with k an integer. What is the eigenvalue?
(e) Use all the above to construct the eigenvalue spectrum of the radius of gyration tensor,
↔
T .
Exercise 6.4
Show that the operator
L(x1, x2) = x<(1 −x>)
where x<(>) is the smaller (larger) of the two xi’s, is the inverse of the operator
−d2/dx2. That is, show that
−d2
dx2 L(x, y) = δ(x −y)
Furthermore, show that this operator, when its range is restricted to the interval
0 < x < 1, maps any bounded function onto a function that goes to zero at the
end-points of the interval.
6.2.1 A key identity
An analysis of walks in high dimensions is greatly facilitated with the use of a
key identity relating entries in the radius of gyration tensor
↔
T to the displacements
corresponding to the steps in a walk. Suppose we denote by ηα,i the displacement in

6.2 Walks in d ≫3 dimensions
139
the direction of the ith cartesian coordinate that occurs in the αth step of the walk.
It is then straightforward to show that the entries in the radius of gyration tensor
can be expressed in terms of the displacement vectors as follows (Kramers, 1946)3
Ti j =
N

α,β=1
aαβηα,iηβ, j
(6.21)
where aαβ are the entries of an N-by-N matrix:
aαβ =



1
(N + 1)2 α (N + 1 −β) , α < β
1
(N + 1)2 β (N + 1 −α) , α > β
(6.22)
These relationships are established in Supplement 3 at the end of this chapter.
The operator
↔a has, in the limit N ≫1, a straightforward set of eigenvectors
and eigenvalues. We can ﬁnd them by inspection. Begin by rewriting the operator
↔a as follows:
ax1,x2 = x<(N + 1 −x>)
(6.23)
where x<(>) is the smaller (greater) of the variables x1 and x2. Then, deﬁning
x = α/(N + 1), y = β/(N + 1), we can rewrite this operator when N ≫1 as
ax,y =
 x(1 −y), x < y
y(1 −x), x > y
(6.24)
The eigenvectors of this operator are
ψn(w) = sin (nπw)
0 < w < 1
(6.25)
where n > 0 is an integer. Direct substitution veriﬁes that
N+1

β=1
aα,βψn(β/(N + 1)) = (N + 1)
 1
0
aα,yψn(y) dy
(6.26a)
= (N + 1)
1
(nπ)2 ψn(α/(N + 1))
(6.26b)
The right hand side of (6.26a) follows from substitution of the variable y for β/(N +
1) and replacement of a sum over β by an integration over y. This replacement is
well-justiﬁed in the limit of very large N. The ﬁnal equality follows from a direct
3 For an expanded version of the development that follows, see Gaspari et al. (1987).

140
Shape of a random walk
evaluation of the resulting integration. This all means that the eigenvalues of the
operator
↔a are given by
λk = N + 1
π2k2
(6.27)
where the k’s are integers.
Our calculations are greatly simpliﬁed – and the results are still valid – if we as-
sume that the displacements ηα,i, are subject to a Gaussian probability distribution:
P

ηα,i

=
 d
2π
1/2
exp

−dη2
α,i/2

(6.28)
Then, the ensemble average of the total length of a single link is unity:
d

i=1
⟨η2
α,i⟩= 1
(6.29)
The displacements corresponding to different steps are statistically independent:
⟨ηα,iηβ, j⟩= 1
d δα,βδi, j
(6.30)
Exercise 6.5
If the random walk is closed, so that the end-to-end distance, ⃗R, is equal to zero, a
correction to (6.30) results. Show that this correction takes the following form:
⟨ηα,iηβ, j⟩= −1
Nd δi, j
(α ̸= β)
(6.31)
Statistical averages of the entries in the radius of gyration tensor,
↔
T , are readily
evaluated. For example
⟨T11T11⟩=

α,β,γ,δ
aα,βaγ,δ⟨ηα,1ηβ,1ηγ,1ηδ,1⟩
(6.32)
As it turns out, the most important terms in the average on the right hand side of
(6.32) have the indices paired off, i.e. α = β, γ = δ or α = γ , β = δ or α = δ,
β = γ . Another possibility yielding a non-zero result on ensemble averaging is
α = β = γ = δ. However, as it turns out, we do not need to pay attention to this
special case, as the correction to the result that we will obtain will be smaller than

6.2 Walks in d ≫3 dimensions
141
that result by the factor 1/N. Given (6.30) and the possibilities for pairing described
above, the result of the averaging yields
⟨T11T11⟩=

α,γ
aα,αaδ,δ
1
d2 + 2

α,δ
aα,δaα,δ
1
d2
(6.33a)
= 1
d2

Tr
↔a
2
+ 2 Tr
↔a
2
(6.33b)
= 1
d2



k
λk
2
+ 2

k
λ2
k


(6.33c)
In (6.33c), the quantities λk are the eigenvalues of the matrix
↔a. Given (6.27), we
know that
λk = N + 1
π2k2
(6.34)
Then,
⟨T11T11⟩= 1
d2
(N + 1)2
π4d2
$
n
1
n2

+ 2

n
1
n4
%
(6.35a)
=
N 2
π4d2
π4
36 + π4
90

(6.35b)
=
N 2
20d2
(6.35c)
In going to (6.35c), we have ignored the difference between (N + 1)2 and N 2,
which is permissible for our purposes in the large-N limit. Furthermore, we have
made use of the results for sums over 1/n2 and 1/n4 that were also utilized in
(6.20d).
A similar set of calculations yields the following results
⟨T11T22⟩= 1
d2

Tr
↔a
2
=
N 2
36d2
(6.36)
⟨T12T12⟩= 1
d2 Tr
↔a
2
=
N 2
90d2
(6.37)
These averages can be inserted into (6.14) to yield the formula (6.15) for Ad, the
asphericity in d dimensions.

142
Shape of a random walk
Exercise 6.6
Verify (6.36) and (6.37).
6.2.2 The eigenvalue structure of
↔
T in high dimensionality
We have already established that the radius of gyration tensor
↔
T has a well-
determined structure and a set of deﬁnite eigenvalues in the limit of very high
dimensionality. We will now see in greater detail how the eigenvalue structure of
this tensor is determined in the limit d →∞, and how one can construct an expan-
sion in 1/d for the eigenvalues, and their distributions. The key to this investigation
is the identity (6.21), and a quantity known as the resolvent of the tensor
↔
T . This
quantity, R(λ), is given by
R(λ) = Tr

1
λ
↔
I −
↔
T

(6.38)
where the quantity λ is assumed to take on complex values. The matrix
↔
I is the
d-dimensional identity tensor:
↔
I =


1
0
0
· · ·
0
1
0
· · ·
0
0
1
· · ·
...
...
...
...


(6.39)
If the eigenvalues of the d × d tensor
↔
T are λ j (1 ≤j ≤d), then
R(λ) =
d

j=1
1
λ −λ j
(6.40)
The function R(λ) thus has poles at the eigenvalues of
↔
T . Given that
↔
T is a real,
symmetric, matrix, which means that its eigenvalues are all real (and are, in fact,
all real and positive), the poles of the resolvent all lie on the real λ axis. We now
make use of the identity
1
x −iϵ = P
1
x

+ iπδ(x)
(6.41)
which holds for real x and real and inﬁnitesimal ϵ > 0, and where P stands for

6.2 Walks in d ≫3 dimensions
143
principal part (Morse and Feshbach, 1953). This allows us to extract the eigenvalue
distribution from the resolvent, via the relationship
Im [R(λ −iϵ)] = π
d

j=1
δ

λ −λ j

(6.42)
The relation assumes that λ is real, and, of course, that ϵ is both real and inﬁnites-
imal. If we perform ensemble averages over both sides of (6.42), we obtain the
distributions of eigenvalues of the radius of gyration tensor
↔
T .
Consider, now, the formal expansion of R(λ) in terms of the tensor
↔
T :
R(λ) = Tr
1
λ
↔
I −
↔
T
= Tr 1
λ

1 +
 ↔
T
λ

+
 ↔
T
λ
2
+ · · ·


= 1
λ
∞

n=0
Tr
 ↔
T
λ
n
(6.43)
This means that
⟨R(λ)⟩= 1
λ
∞

n=0

Tr
 ↔
T
λ
n
(6.44)
We can ﬁnd the ensemble average of the resolvent if we can manage to ﬁnd the
ensemble average of the radius of gyration tensor raised to an arbitrarily high power.
Making use of the identity (6.21), we have
Tr
↔
T
n
=

α1,β1,...αn,βn

i1,...in
⟨ηα1,i1aα1,β1ηβ1,i2ηα2,i2aα2,β2ηβ2,i3 · · · ηαn,inaαn,βnηβn,i1⟩
(6.45)
The matrices aαk,βk are the same for every N-step random walk. The averages are
all over the η’s. Given the fact that the probability density governing the distribution
of the displacements is Gaussian, we can generalize (6.30). For instance, if we have
a product of six η’s, then the average is given by
⟨ηα1,i1ηα2,i2ηα3,i3ηα4,i4⟩= ⟨ηα1,i1ηα2,i2⟩⟨ηα3,i3ηα4,i4⟩
+⟨ηα1,i1ηα3,i3⟩⟨ηα2,i2ηα4,i4⟩+ ⟨ηα1,i1ηα4,i4⟩⟨ηα2,i2ηα3,i3⟩
(6.46)

144
Shape of a random walk
Fig. 6.6. Diagrammatic representation of the right hand side of (6.47). The text
immediately below the equation contains a detailed discussion of the meaning of
the various elements in the diagram.
Fig. 6.7
Diagrammatic representation of (
↔
T/λ)n.
Exercise 6.7
In general, the average of a product of 2m η’s will be equal to a sum of products
of the averages of m pairs of the η’s. This is a classical version of what is known
in ﬁeld theory as Wick’s theorem. Derive an expression for the number of ways of
forming those m pairs.
6.2.3 Diagrammatic expansion
We can use what we now have to formulate a diagrammatic method that keeps
track of the pairings of the η’s. This method forms the basis of an expansion in 1/d
of the eigenvalue distribution of the radius of gyration tensor
↔
T . It has other uses
as well, including the construction of an approximate analytical expression for the
distributions of the individual eigenvalues of
↔
T directly from the average ⟨R(λ)⟩.
We accomplish this by summing a class of terms in the 1/d expansion.
To start, consider the quantity
 ↔
T
λ
2
i, j
=

α1,α2,β1,β2
d

i2=1
ηα1,i
aα1,β1
λ
ηβ1,i2ηα2,i2
aα2,β2
λ
ηβ2, j
(6.47)
The right-hand side of (6.47) is represented diagrammatically in Figure 6.6.
The crosses at the ends of the horizontal lines represent the displacement η, and
the lines themselves stand for the element aα,β. The dot between the two adjacent
crosses in the center of the ﬁgure is for accounting purposes only. The nth order
term (
↔
T/λ)n is represented by a string of lines with crosses at both ends, as shown in
Figure 6.7.
We now introduce a new element into the diagrammatic method: the represen-
tation of the Gaussian pairing of two η’s. This sort of pairing is symbolized by

6.2 Walks in d ≫3 dimensions
145
Fig. 6.8. Representation of the pairing of two adjacent η’s. See the text above
(6.48), for an explanation.
(a)
(b)
Fig. 6.9. (a) Representation of Tr(
↔
T /λ)n. (b) Representation of the lowest order
contribution to ⟨Tr(
↔
T /λ)n⟩. The large dot at the bottom of each of the two ﬁgures
separates the η’s at the beginning and the end of the expansion of
↔
T
n
, and is for
accounting puroposes only.
drawing two lines between them, as shown in Figure 6.8. This diagram represents

α1,β1,α2,β2
d

i2=1
ηα1,i
aα1,β1
λ
⟨ηβ1,i2ηα2,i2⟩aα2,β2
λ
ηβ5, j
=

α1,β1,α2,β2
d

i2=1
ηα1,i
aα1,β1
λ
1
d δβ1,α2δi2,i2
 aα2,β2
λ
ηβ2, j
=

α1,β2,α2
ηα1,i
aα1,α2
λ
aα2,β2
λ
ηβ2, j
=

α1,β2
ηα1,i
 ↔a
λ
2
α1,β2
ηβ2, j
(6.48)
An important point to note is that the delta function, δi2,i2, the delta function for the
components of two η’s, is automatically satisﬁed for two adjacent η’s. This guaran-
teed satisfaction of the delta function for components occurs whenever two adjacent
η’s are paired. There is no such guarantee when the pairing is between two non-
adjacent η’s. This turns out to be the basis of the 1/d expansion. An expansion up to
nth order will result when account is taken of up to 2n pairings of non-adjacent η’s.
We will start with the lowest order term in the expansion – the term of zeroth
order in 1/d. Consider Tr(
↔
T /λ)n, the nth order term in the summation in (6.43).
This term is represented as the ring diagram in Figure 6.9(a). The large dot that

146
Shape of a random walk
separates the crosses representing the η’s at the two ends of the right hand side
of (6.45) is, like the other dots in the diagram, for accounting purposes only. The
zeroth order contribution to ⟨Tr(
↔
T /λ)n⟩is obtained by pairing off adjacent η’s
only. The diagram representing this pairing is shown in Figure 6.9(b). This diagram
represents

α1,β1,...,αn,βn
aα1,β1
λ
δβ1,α2
aα2,β2
λ
· · · aαn,βn
λ
δβn,α1 = Tr
 ↔a
λ
n
(6.49)
Thus, to zeroth order in 1/d,

Tr
 ↔
T
λ
n
= Tr
 ↔a
λ
n
(6.50)
and

Tr
1
λ
↔
I −
↔
T

= Tr
1
λ
↔
I −
↔a
+ O(1/d)
(6.51)
The ensemble average of the eigenvalue distribution of the radius of gyration tensor
↔
T is then given by
Im (R(λ −iϵ)) =

i
δ(λ −λi) + O(1/d)
(6.52)
where λi is the ith eigenvalue of the operator
↔a. As noted earlier (see (6.27)), the
eigenvalues of this operator have the form λk = (N + 1)/π2k2.
To recapitulate, the principal radii of gyration of a walk in very high spatial
dimensionality are well-deﬁned, in that their distributions are essentially delta-
function-like, and they are given by the formula (6.27). It is instructive to compare
the ratio of the three largest eigenvalues in this series with the ratios of the principal
radiiofgyrationofanordinarywalkinthreedimensions,asdeterminedinnumerical
simulations. In asymptotically large dimensionality, (6.27) tells us that
⟨R2
1⟩: ⟨R2
2⟩: ⟨R2
3⟩= 9 : 2.25 : 1
(6.53)
On the other hand, numerical studies yield (Bishop and Michels, 1986)
⟨R2
1⟩: ⟨R2
2⟩: ⟨R2
3⟩= 13.76 : 3.03 : 1
(6.54)
The agreement between the ratios in (6.53) and those in (6.54) is quite good,
indicating that one might be able to learn something about the shapes of three-
dimensional walks by loooking at walks in much higher spatial dimensionality.
Again, as in the case of asphericities, the ratios of principal radii of gyrations are
not perfectly well-deﬁned for an ensemble of walks, but rather occupy a reasonably

6.2 Walks in d ≫3 dimensions
147
20
40
60
80
100
120
250
500
750
1000
1250
1500
5
10
15
20
250
500
750
1000
1250
1500
R1
2/R3
2
R2
2/R3
2
Fig.6.10 Thedistributionsoftheratios R2
1/R2
3 and R2
2/R2
3 foranensembleof5 000
three-dimensional random walks. Note the difference between the horizontal axes
of the two historgams.
broad distribution. Figure 6.10 shows the distributions of the ratios R2
1/R2
3 and
R2
2/R2
3 for an ensemble of 5000 random walks in three dimensions. In other words,
the distribution of ratios of principal radii of gyration is also consistent with the
notion that random walks come in a variety of shapes, rather than a single charac-
teristic one. As we look more closely at random walks in high dimensionality, we
expect to see how a distribution of ﬁnite width emerges from the perfectly localized
distributions in inﬁnite dimensionality.
6.2.4 The 1/d expansion
The next term in the 1/d expansion of the eigenvalue structure of the radius of
gyration tensor follows from a pairing of non-adjacent ηα,i’s. Two sets of non-
adjacent displacements are shown encircled in Figure 6.11(a). The two η’s are
brought together in Figure 6.11(b). The three ways of pairing these η’s are shown
in Figures 6.11(c)–(e). The pairing shown in Figure 6.11(c) is just the pairing of
adjacentη’sthatleadstothezerothorderresult.Theothertwopairingsarenew.Ifthe
remaining pairings are all between adjacent η’s, the ﬁrst-order-in-1/d contribution
to ⟨Tr(
↔
T /λ)n⟩is obtained. Details of the analysis, which is straightforward, but
fairly tedious, can be found in (Gaspari et al., 1987). The end result for the average
of the resolvent, ⟨R(λ)⟩, is
⟨R(λ)⟩=

i


1
λ −λi
+ 1
d

j
j̸=i
λiλ j
(λ −λi)2(λ −λ j) + 2
d
λ2
i
(λ −λi)3

+ O
$1
d
2%
(6.55)

148
Shape of a random walk
(a)
(b)
(c)
(d)
(e)
Fig. 6.11. (a) Two sets of η’s in the diagrammatic representation of (
↔
T /λ)n, shown
encircled. (b) The two sets are pulled together as a preliminary step in the pairing
of those non-adjacent η’s. (c)–(e) The three ways of pairing the four η’s that have
been brought together.
The quantities λi in (6.55) are the eigenvalues of the tensor
↔a. This expression
does not match the zeroth order result for the averaged resolvent of
↔
T , (6.51). If
it did, then the eigenvalues of the moment of intertia tensor would be perfectly
well-deﬁned to ﬁrst order in 1/d. The correction to the zeroth order result for the
resolvent can be interpreted in terms of an O(1/d) shift in each ensemble-averaged
eigenvalue and an O(1/d) contribution to the width of the distribution of each
eigenvalue about its average. That is, the eigenvalue distribution is no longer a set
of delta functions, but rather a set of peaks of ﬁnite width.
To see how this interpretation works, imagine that each eigenvalue, 
i, of a given
realization of the radius of gyration tensor
↔
T can be written as

i = λi(1 + εi)
(6.56)
where λi is the corresponding eigenvalue of the operator
↔a and εi is the small
relative shift in 
i from its limiting d = ∞value. Then

1
λ −
i

=

1
λ −λ(1 + εi)

=

1
λ −λi
+
λiεi
(λ −λi)2 +
λ2
i ε2
i
(λ −λi)3 + . . .

=
1
λ −λi
+
λi
(λ −λi)2 ⟨εi⟩+
λ2
i
(λ −λi)3 ⟨ε2⟩+ . . .
(6.57)
Referring back to (6.55), we are able to assert that this implies a fractional shift
in the eigenvalues of the radius of gyration tensor in for a d-dimensional random

6.2 Walks in d ≫3 dimensions
149
walk given by
⟨εi⟩= 1
d

j̸=i
λ j
λ −λ j
≈1
d

j
j̸=i
λ j
λi −λ j
(6.58)
The last approximate equality is asymptotically accurate as d →∞. Given the
results (6.27) for the eigenvalues λi, we have for the order 1/d fractional shift in
the eigenvalues
⟨ε⟩= 1
d

j
j̸=i
j−2
i−2 −j−2
= 1
d

j
j̸=i
i2
j2 −i2
= 3
4d
(6.59)
The last line of (6.59) follows from the mathematical result (Gradshteyn et al.,
2000)
∞

m=1
m̸=n
n2
m2 −n2 = 3
4
(6.60)
Thus, to order 1/d,
⟨
k⟩= N + 1
π2k2

1 + 3
4d

(6.61)
Making use of a similar line of reasoning, we can also infer, from (6.55) and (6.57),
that
*
(
k −⟨
k⟩)2+
= 2
d
 N + 1
π2k2
2
(6.62)
This tells us that both the shift of the mean principal radii of gyration and the
widths of their distribution about that mean are of the same general magnitude
as their value in the limit d →∞. The shifts and the widths are smaller than the
asymptotic means by a factor that goes as 1/d.
6.2.5 Accuracy of the 1/d expansion
We’ve already seen that the ratios of the mean pricipal radii of gyration are remark-
ably accurate, even at lowest order in 1/d (see (6.53) and (6.54)). We will now look

150
Shape of a random walk
0.05
0.1
0.15
0.2
50
100
150
200
250
Fig. 6.12. Distribution of the three largest eigenvalues of the radius of gyration
tensor for a walk in 100 dimensions. The vertical lines indicate the locations of the
eigenvalues as given by the leading-order prediction of the 1/d expansion, (6.19).
0.05
0.1
0.15
0.2
50
100
150
200
250
Fig. 6.13. Distribution of the three largest eigenvalues of the radius of gyration
tensor for a walk in 20 dimensions. The vertical lines indicate the locations of the
eigenvalues as given by the leading-order prediction of the 1/d expansion, (6.19).
further into the accuracy of the predictions for walks in high dimensions, as they
apply to walks in three dimensions, and to walks taking place in what we would gen-
erally agree are a large number of spatial dimensions. For that purpose, displayed
in Figures 6.12, 6.13, and 6.14 are the distributions of the three largest eigenvalues
of the radius of gyration tensor
↔
T for walks in, respectively, 100 dimensions, 20
dimensions and three dimensions. Three points are worth considering here. The
ﬁrst is qualitative – are the distributions well-separated and at all well-represented
by narrow, nearly delta-function-like, peaks? The second and third are how accu-
rately the means and widths of the distributions are predicted by the 1/d expansion,
carried out to ﬁrst order in the expansion parameter. It is clear from the ﬁgures
that, even in 100 dimensions, the delta-function limit is not achieved. However, the
formula (6.62) tells us that the width of the distribution of an eigenvalue of
↔
T will
not be all that close to zero unless the dimensionality is truly enormous. In fact, if

6.2 Walks in d ≫3 dimensions
151
0.05
0.1
0.15
0.2
50
100
150
200
250
Fig. 6.14. Distribution of the three largest eigenvalues of the radius of gyration
tensor for a walk in three dimensions. The vertical lines indicate the locations of
the eigenvalues as given by the leading-order prediction of the 1/d expansion,
(6.19).
we denote by vi the variance of the distribution of the ith eigenvalue R2
i , the ratios
of the actual variances and the predicted ones are, in 100 dimensions, are
v1
2(N + 1)2/(100 × π4) = 0.97
(6.63)
v2
2(N + 1)2/(100 × (4π2)2) = 0.93
(6.64)
v3
2(N + 1)2/(100 × (9π2)2) = 0.95
(6.65)
As might be expected, the 1/d expansion works reasonably well when d = 100.
Furthermore, even though the distributions are far from delta-function-like, they
are relatively well separated.
When d = 20, the eigenvalue distributions are not quite as well distinguished,
and in three dimensions, the distributions clearly run into each other. Nevertheless,
some (but not all) of the predictions of the 1/d expansion hold up reasonably
well. For example, if we compare the variances of the distributions of individual
eigenvalues with those predicted, we ﬁnd in three dimensions
v1
2(N + 1)2/(3 × π4) = 0.98
(6.66)
v2
2(N + 1)2/(3 × (4π2)2) = 0.48
(6.67)
v3
2(N + 1)2/(3 × (9π2)2) = 0.23
(6.68)
The width of the distribution is accurately given by (6.62) in the case of the largest
eigenvalue, if not for the smaller ones. Comparing to the mean values, ⟨R2
k⟩with

152
Shape of a random walk
⟨
k⟩as given by (6.61), we ﬁnd in three dimensions
⟨R2
1⟩
⟨
1⟩= 1.01
(6.69)
⟨R2
2⟩
⟨
2⟩= 0.91
(6.70)
⟨R2
3⟩
⟨
3⟩= 0.75
(6.71)
Again, the largest eigenvalue is best described by the 1/d expansion. However,
in the worst case, the 1/d expansion is accurate to within 25%. Given that the
comparison is with a formula that is correct to ﬁrst order in 1/d, it is reasonable
to anticipate an error of order 1/d2 ∼0.1 in three dimensions. This expectation is
borne out, or exceeded, in the case of the two largest principal radii of gyration.
The smallest eigenvalue of
↔
T is the most resistant to analysis in the context of the
1/d expansion.
The expansion in inverse dimensionality, with its clear shortcomings, is of some
utility in the analysis of random walk shape.
6.2.6 Prediction of the distribution of the principal radii of gyration
It is possible to peform a partial summation of the 1/d expansion that extends to
all orders in 1/d, yielding an approximation for the distribution of the square of the
ith principal radius of gyration, R2
i . One ﬁnds (Gaspari et al., 1987)
P(R2
i ) = 1
2π
 ∞
−∞
exp

−d
2 ln

1 + 2ixαi
d

+ ix R2
i

dx
(6.72)
Here, αi is the lowest-order-in-1/d prediction for the mean value, ⟨R2
i ⟩, as given
by (6.19). A steepest-descents evaluation of the integral in (6.72) yields
P(R2
i ) ∝(R2
i )d/2−1 exp

−d R2
i /2αi

(6.73)
Figure 6.15 shows how this prediction compares with the actual distribution of the
largest eigenvalue of
↔
T in three dimensions. The quality of the ﬁt, if not perfect, is
encouraging. If we combine the formulas (6.72) as they apply to all the principal
radii of gyration, we end up with the following result for their sum
P(R2) = 1
2π
 ∞
−∞
eisR2 K(s) ds
(6.74)

6.2 Walks in d ≫3 dimensions
153
0.1
0.2
0.3
0.4
0.5
0.6
1
2
3
4
5
6
7
Fig. 6.15. Comparison of the approximate distribution of the largest eigenvalue,
R2
1, of the radius of gyration tensor
↔
T with the actual distribution of that quantity.
Both distributions are normalized.
where
R2 =
d

i=1
R2
i
(6.75)
and
K(R) =
N
,
n=1

1 −2isαn
d
−d/2
(6.76)
and αn = (N + 1)/π2n2. As it turns out, this is an exact result for the distribution of
the sum of the principal radii of gyration, ﬁrst derived by Fixman (Fixman, 1962).
The sum is also known as the mean radius of gyration. It is the trace of the radius of
gyration tensor,
↔
T . Its mean value is calculated in the supplement 2 to this chapter.
(See (6:S2-13)).
Exercise 6.8
Using (6.73), evaluate ⟨

R2
i −⟨R2
i ⟩
2⟩.
Exercise 6.9
Verify that (6.74) follows from (6.72).
6.2.7 Shape of a self-avoiding random walk
In subesequent chapters, we will turn our attention to walks that are self-avoiding,
in that the walker refuses to take a path that intersects itself. This type of walk is
important, in that it provides a model for the conﬁgurational statistics of a long

154
Shape of a random walk
chain polymer. Work on the shape of a self-avoiding walk has been performed
(Aronovitz and Nelson, 1986). The calculation is based on an expansion in the
difference between the dimensionality in which the walk takes place and an “ upper
critical dimensionality,” equal to four. The quantity ϵ = 4 −d is the expansion
parameter. To ﬁrst order in ϵ
Ad = 2d + 4
5d + 4 + 0.008ϵ
(6.77)
The main conclusion to be gleaned from this result is that self-avoidance plays a
non-trivial, but far from decisive, role in the shape of a random walk.4
6.3 Final commentary
We’ve talked a bit on the issue of the shape of a random walk. Given the open-
endedness of the notion of shape, it ought not be surprising that we have not come
close to exhausting the subject in this chapter. The quantiﬁcation of shape on which
we have focused here has the virtue of amenability to analysis, and more than a
little can be done with shape as measured in terms of the eigenvalue structure of
the radius of gyration tensor. In light of the importance of the conformations taken
by biological molecules that are topologically linear (RNA, DNA, and proteins in
particular), it is clear that the subject of shape is a ﬁeld that deserves continued
attention. There will no doubt be important developments in the forseeable future.
We look forward to their advent.
6.4 Supplement 1: principal radii of gyration and rotational motion
Recall Newton’s second law:
⃗F = d ⃗p
dt
(6:S1-1)
Here, ⃗F is the applied force and ⃗p is the linear momentum of the particle or
collection of particles to which that force is applied. When the system undergoing
the change in motion is a single particle, ⃗p = m⃗v, where ⃗v is the particle’s velocity.
Now, assume that we have a system of particles. Newton’s second law, as it refers
to each point particle, is, then
⃗Fl = ml
d⃗vl
dt
(6:S1-2)
4 Notice that the ﬁrst term on the right hand side of (6.77) has not been expanded about d = 4. This is a (minor)
violation of the spirit of the expansion in ϵ = 4 −d, which does not materially affect the conclusion stated
above.

6.4 Supplement 1
155
where l is the subscript that identiﬁes the particle. If we sum up these equations
we end up with (6:S1-1), where ⃗F is the net external force (internal forces cancel
because of Newton’s third law), and ⃗p is the total momentum:
⃗p =

l
ml⃗vl
(6:S1-3)
So far, so good. This is all pretty elementary. Now, let’s focus on rotational
motion. We derive Newton’s second law, as it applies to rotational motion, by
taking the cross product of the position vectors ⃗rl with the corresponding equation
in the set (6:S1-2). Deﬁning the total torque ⃗τ as the sum of the ⃗rl × ⃗Fl’s, we end
up with the equation
⃗τ =

l
ml⃗rl × d⃗vl
dt
= d
dt

l
ml⃗rl × ⃗vl
≡d⃗L
dt
(6:S1-4)
The last two lines of (6:S1-4) constitute a deﬁnition of the angular momentum, ⃗L of
a system of point particles. Note that the precise deﬁnition of angular momentum
depends on the origin with respect to which the position of each particle is deﬁned.
It is often convenient to place the origin at the center of mass of the set of particles.
If the internal force between each pair of particles is along the line joining them,
then the internally generated torques cancel, and the total torque, ⃗τ, is entirely due
to external forces.
Now, suppose that the motion of the system is entirely rotational, about some
point ⃗R. Then,
⃗vl = ⃗ω ×

⃗rl −⃗R

(6:S1-5)
Here, ⃗ω is the angular velocity of the system of particles (see Figure 6.16). Now, we
can choose ⃗R as the center of our system of coordinates, so that ⃗rl −⃗R is replaced
by ⃗rl. In this case, the angular momentum becomes

l
ml⃗rl ×

⃗ω × ⃗rl

(6:S1-6)
We can rewrite the above relationship with the use of the standard identity for the
triple product:
⃗L =

l
ml

⃗ω

⃗rl · ⃗rl

−⃗rl

⃗rl · ⃗ω

(6:S1-7)

156
Shape of a random walk
ω
r – R
v
Fig. 6.16. The angular velocity, ⃗ω, and its relation to the velocity, ⃗v, of a particle.
Suppose, now, we deﬁne the matrix
↔
T as follows:
Ti j =

l
mlrl,irl, j
(6:S1-8)
Then, the relationship between ⃗L and ⃗ω is
⃗L = Tr
↔
T ⃗ω−
↔
T · ⃗ω
(6:S1-9)
Deﬁning
↔
C=
↔
T −
↔
I Tr
↔
T
(6:S1-10)
where
↔
I is the identity operator we ﬁnd that
⃗L = −
↔
C · ⃗ω
(6:S1-11)
Note that the trace of the operator
↔
C has a trace equal to twice the trace of
↔
T . This
is because the trace of the identity operator is three. The matrix
↔
C is the moment
of inertia matrix. The fact that the angular velocity and the angular momentum are
not parallel is just one of the complications of rotational motion.
Now, because the matrix
↔
T is real and symmetric, we know that it has real (in
fact, positive) eigenvalues. Those eigenvalues have a name. They are known as the
principal radii of gyration, R2
i . If ⃗ω points in the same direction as the eigenvector
of one of them, say R2
1, then the angular momentum and the angular velocity point
in the same direction, and the relationship between the two becomes
⃗L =

R2
2 + R2
3

⃗ω
(6:S1-12)
This is because Tr
↔
T = R2
1 + R2
2 + R2
3, while
↔
T · ⃗ω = R2
1 ⃗ω.
The eigenvalues of the tensor
↔
T can also be used as a measure of the extent
to which the system of particles possesses spherical symmetry, at least in terms of

6.4 Supplement 1
157
rotationalmotion.Ifalltheeigenvaluesareequal,thenthesystemhasapproximately
the same weighted extent. In fact, in this case, ⃗L is always parallel to ⃗ω.
Now imagine that all the masses, ml, are equal to one. Then, the matrix measures
the extent to which the particles are in a spherically symmetric distribution. While
equality of the principal radii of gyration is not equivalent to spherical symmetry, it
provides a very useful quantitative measure of that property, and of departure from
it.
We can construct a new tensor,
↔
Q, as follows
↔
Q=
↔
T −1
3
↔
I Tr
↔
T
(6:S1-13)
The trace of this matrix is equal to zero, and, if all the principal radii of gyration
are the same, then the diagonalized form of this matrix has all entries equal to zero.
Position vectors transform under rotations about the center of mass as follows
r′
k =

l
Rklrl
(6:S1-14)
where Rkl are the elements of the rotation matrix. This matrix has the property that
its transpose is also its inverse. That is
↔
R ·
↔
R
T
=
↔
I
(6:S1-15)
A matrix whose transpose is also its inverse is known as an orthogonal matrix.
Then, the matrix
↔
T with elements going as rlrk transforms as follows
T ′
k1k2 =

l1,l2
Rk1l1Tl1l2 Rk2l2
=

l1,l2
Rk1l1Tl1l2 RT
l2k2
=

l1,l2
Rk1l1Tl1l2 R−1
l2k2
(6:S1-16)
or, in shorthand,
↔
T
′
=
↔
R ·
↔
T ·
↔
R
−1
(6:S1-17)
The same relationship clearly holds for the tensor
↔
Q. The demonstration that traces
of powers of this tensor are invariant under rotations follows from this equation for
the way in which rotations give rise to changes in
↔
Q.

158
Shape of a random walk
Fig. 6.17. Looking for the direction in which an object has the greatest spatial
extent.
6.4.1 Just a little bit more on the meaning of the operator
↔
T
Suppose we are interested in ﬁnding the direction in which an object has the greatest
spatial extent. We start by assuming a vector ⃗n, which points along the direction
of interest. We will set the length of the vector at unity (see Figure 6.17). Then the
extent of the object in the direction established by ⃗n is

l

⃗rl · ⃗n
2 =

l

i, j
nirl,irl, jn j
= ⃗n ·
↔
T · ⃗n
(6:S1-18)
Remember that the position vectors are drawn with their tails at the center of mass
of the object. If we wish to extremize the above quantity with respect to ⃗n, subject
to the condition that its length is held constant, we take the derivative with respect
to each component of ⃗n of the expression below
⃗n ·
↔
T · ⃗n −λ⃗n·⃗n
(6:S1-19)
The quantity λ in (6:S1-19) is a Lagrange multiplier. The extremum equation easily
reduces to
↔
T · ⃗n = λ⃗n
(6:S1-20)
That is, in order to ﬁnd the direction of greatest (or least) extent of the object,
we solve the eigenvalue equation of the operator
↔
T . The largest eigenvalue is the
greatest extent, as deﬁned by (6:S1-18), with ⃗n chosen to extremize the quantity.5
The smallest extent, similarly deﬁned, is given by the smallest eigenvalue of
↔
T .
5 In other words, the extremizing choice for the vector ⃗n is the eigenvector of the operator
↔
T with the largest
eigenvalue.

6.5 Supplement 2
159
6.5 Supplement 2: calculations for the mean asphericity
6.5.1 The average ⟨T11⟩
First, we slightly recast the general expression for the mean asphericity
Ad =
⟨T 2
11⟩
⟨T 2
12⟩−⟨T11T22⟩
⟨T 2
12⟩

+ d
 -  ⟨T 2
11⟩
⟨T11T22⟩+ d −1

(6:S2-1)
This means that to calculate the mean aspericity we need to ﬁnd ratios of averages,
rather than the averages themselves. This simpliﬁes our task a bit.
Although the averages that we have to perform in order to arrive at a numerical
result for the asphericity of the random walk involve squares of entries in the radius
of gyration tensor, or products of two entries, it is useful to look at the average
of a single element of that tensor, lying along the diagonal. Eventually, we will
perform this calculation in another way when we develop an expansion in 1/d
for the principal radii of gyration. However, we will start out by showing how the
calculation can be done with the use of the generating functions that have proven so
useful in the study of random walk statistics. As a ﬁrst step, we recast the expression
for the entries Tkl:6
Tkl = 1
N
N

j=1

r jk −⟨rk⟩
 
r jl −⟨rl⟩

(6:S2-2a)
=
1
2N 2
N

i=1
N

j=1

rik −r jk
 
ril −r jl

(6:S2-2b)
Equation 6:S2-2a is a recapitulation of (6.1); 6:S2-2b can be established by inspec-
tion. Consider, now the average
⟨T11⟩=
1
2N 2
N

i=1
N

j=1
⟨(xi −x j)2⟩
(6:S2-3)
We have reverted here to the notation appropriate to a three-dimensional walk, and
replaced r1 by x. The average in the sum on the right hand side of (6:S2-3) is
directly proportional to a function that can be graphically represented as shown in
Figure 6.18.
The dashed curve joins the ith and jth footprints on the walk. The solid lines
stand for the walk that begins at the leftmost end of the three-line segment and
ends at the rightmost point. There will, in general, be N1 steps from the far left
point to the leftmost vertex at which the dashed curve touches the line, N2 steps in
the central segment of the walk, and N3 steps in the far right segment of the walk.
6 Here, we make no distinction between N and N + 1.

160
Shape of a random walk
Fig. 6.18. Graphical representation of ⟨(xi −x j)2⟩.
Subject to the overall constraint that the Ni’s add up to the total number of steps
in the walk, we sum over all values of N1, N2, and N3. The evaluation of the sum
represented by this diagram is most conveniently carried out in the grand canonical
ensemble, with the use of the generating function. We seek the coefﬁcient of zN−1
in the direct product
1
2N 2 × 2

ddr0

ddr2

ddr3G(z; ⃗r1 −⃗r0)G(z; ⃗r2 −⃗r1)G(z; ⃗r3 −⃗r2)(x2 −x1)2
(6:S2-4)
The “missing” integration in (6:S2-4), over ⃗r1, would yield a factor equal to the
volume of the portion of space in which the random walk occurs. The factor of two
multiplying the integral represent the two possible orderings of the indices i and j
(i > j and i < j). As the next step, we rewrite the generating functions in terms
of their spatial Fourier transforms,
G(z; ⃗r) =
1
(2π)d

ddkg(z; ⃗k)e−⃗k·⃗r
(6:S2-5)
Making use of this representation, we ﬁnd that the expression in (6:S2-4) reduces
to
1
N 2 g(z; ⃗k)

−∂2
∂k2x
g(z; k)

g(z; ⃗k)
....⃗k=0
(6:S2-6)
The second derivative follows from the identity
(x1 −x2)2ei⃗k·(⃗r1−⃗r2) = −∂2
∂k2x
ei⃗k·(⃗r1−⃗r2)
(6:S2-7)
and an integration by parts. Once this identity and integration by parts have been
implemented, the integrations over the ⃗ri’s produces Dirac delta functions in the
ki’s, and we are immediately led to (6:S2-6)
Now, in the case of a random walk in d dimensions, we can write
g(z; ⃗k) =
1
1 −zz−1
c
+ k2l2/2d
(6:S2-8)

6.5 Supplement 2
161
where l represents the mean distance covered by the walker in each step. This leaves
us with the following result for the expression (6:S2-6):
1
N 2
l2
d
1
(1 −zz−1
c )4
(6:S2-9)
We now extract the coefﬁcient of zN−1 in the power-series expansion of (6:S2-
9).7 This is straightforward given what we know about the process.8 We ﬁnd for
this coefﬁcient
1
N 2
l2
d z−(N−1)
c
(N + 2)(N + 1)N
6
≈1
N 2
l2
d z−(N−1)
c
N 3
6
(6:S2-10)
This result is the desired value of ⟨T11⟩, multiplied by the total number of random
walks with N −1 steps. To obtain the average, we divide this by the total number
of N −1-step walks, which is equal to the coefﬁcient of zN−1 in

ddrG(z; ⃗r) = g(z; ⃗k = 0)
=
1
1 −zz−1
c
(6:S2-11)
The coefﬁcient in question is z−(N−1)
c
. We thus ﬁnd
⟨T11⟩= N l2
6d
(6:S2-12)
The average of the trace of the tensor
↔
T is, by symmetry, equal to d⟨T11⟩. Given
(6:S2-12) we have
⟨Tr
↔
T ⟩= Nl2/6
(6:S2-13)
The quantity above is also known as the mean radius of gyration.
6.5.2 Calculation of the asphericity
The quantities that contribute to the asphericity are ⟨T 2
11⟩, ⟨T11T22⟩and ⟨T 2
12⟩. Again,
reverting to standard cartesian notation we ﬁnd
⟨T 2
11⟩=
1
4N 4
N

i=1
N

j=1
N

k=1
N

l=1
⟨

xi −x j
2 (xk −xl)2⟩
(6:S2-14)
Now, the graphical representation of the numerator in the expression leading to
the desired average is a bit more complicated, in that there are three different
7 The relevant power is N −1 because there are N −1 steps in a walk that leaves N footprints.
8 See Supplement 3 in Chapter 2.

162
Shape of a random walk
(a)
(b)
(c)
Fig. 6.19. The three different forms of the graph involved in the calculation of
⟨T 2
11⟩.
forms, as shown in Figure 6.19. The dotted curves are a stand-in for the differences
(xm −xn)2, and the three types of graphical representations correspond to the three
“topologically distinct” possibilities for the sequence of the indices i, j, k,l in
(6:S2-14). The calculation proceeds along the same line as the one leading to a
result for ⟨T11⟩. As we can see from (6.14), the only information we need to extract
from our calculation is the ratios ⟨T 2
11⟩/⟨T11T22⟩and ⟨T11⟩/⟨T12⟩. We will go over
the determination of ⟨T 2
11⟩in detail. The other averages are determined in a similar
way.
6.5.3 Determination of ⟨T 2
11⟩
We proceed, following Figure 6.19 diagram-by-diagram.
Diagram (a)
Here, the calculation proceeds as it did in the evaluation of ⟨T11⟩. Taking second
derivatives and performing integrations by parts, we are left with the following
expression
1
4N 4 × 8 g(z; ⃗k)
 ∂2
∂k2x
g(z; ⃗k)

g(z; ⃗k)
 ∂2
∂k2x
g(z; ⃗k)

g(z; ⃗k)
....⃗k=0
(6:S2-15)
The factor of 8 in the above expression counts the number of ways of constructing
diagram (a), exchanging end-points of the two dashed curves, and permuting the
two curves among themselves. The quantity of interest is, of course, the coefﬁcient
of zN−1 in (6:S2-15). We will defer the power-series expansion in the fugacity z.
Making use of (6:S2-8) for the Fourier-transformed generating function, we end up
with the result
2
N 4
l2
d
2
1

1 −zz−1
c
7
(6:S2-16)
Diagram (b)
The evaluation of this diagram is a bit more involved. Utilizing the generating
function in real space, we have the average of interest proportional to the following

6.5 Supplement 2
163
expression:

ddr0

ddr1

ddr2

ddr3

ddr4G

z; ⃗r1 −⃗r0

× G

z; ⃗r2 −⃗r1

G

z; ⃗r3 −⃗r2

G

z; ⃗r4 −⃗r3

G

z; ⃗r5 −⃗r4

× (x4 −x1)2 (x3 −x2)2
(6:S2-17)
The next step is to express the generating functions in terms of their Fourier trans-
forms. We end up with a product containg the factor
(x4 −x1)2 (x3 −x2)2 ei⃗k0·(⃗r1−⃗r0)ei⃗k1·(⃗r2−⃗r1)ei⃗k2·(⃗r3−⃗r2)ei⃗k3·(⃗r4−⃗r3)ei⃗k4·(⃗r5−⃗r4)
=
 ∂
∂k2x
2  ∂
∂k1x
+
∂
∂k2x
+
∂
∂k3x
2
ei⃗k0·(⃗r1−⃗r0)ei⃗k1·(⃗r2−⃗r1)ei⃗k2·(⃗r3−⃗r2)
× ei⃗k3·(⃗r4−⃗r3)ei⃗k4·(⃗r5−⃗r4)
(6:S2-18)
After a series of integrations by parts in the variables ⃗ki the derivatives above act
on the Fourier transforms of the generating functions. The integrations over the ⃗rl’s
produce delta functions, and we are left with the following result
 ∂
∂k2x
2  ∂
∂k1x
+
∂
∂k2x
+
∂
∂k3x
2
g

z; ⃗k0

g

z; ⃗k1

g

z; ⃗k2

g

z; ⃗k3

g

z; ⃗k4
...⃗k0=⃗k1=⃗k2=⃗k3=⃗k4=0
(6:S2-19)
The polynomial expressions in the derivatives are now expanded, discarding in the
process all terms that evaluate to zero when the ⃗ki’s are equal to zero.9 This yields
 ∂2
∂k2
1x
∂2
∂k2
2x
+ ∂2
∂k2
3x
∂2
∂k2
2x
+ ∂4
∂k4
2x

g

z; ⃗k0

g

z; ⃗k1

g

z; ⃗k2

g

z; ⃗k3

g

z; ⃗k4
...⃗k0=⃗k1=⃗k2=⃗k3=⃗k4=0
(6:S2-20)
The remainder of the calculation is fairly straightforward. Inserting combinatorial
factorsnotedaboveandtakingtheappropriatederivativesofthegeneratingfunction,
we end up with the contribution
16
N 4
l2
d
2
1

1 −zz−1
c
7
(6:S2-21)
9 The general operating principle is that any term that contains an odd-order derivative with respect to a kix will
evaluate to zero.

164
Shape of a random walk
Diagram (c)
In this case, the relevant identity is
(x3 −x1)2 (x4 −x2)2 ei⃗k0·(⃗r1−⃗r0)ei⃗k1·(⃗r2−⃗r1)ei⃗k2·(⃗r3−⃗r2)ei⃗k3·(⃗r4−⃗r3)ei⃗k4·(⃗r5−⃗r4)
=
 ∂
∂k1x
+
∂
∂k2x
2  ∂
∂k2x
+
∂
∂k3x
2
ei⃗k0·(⃗r1−⃗r0)ei⃗k1·(⃗r2−⃗r1)ei⃗k2·(⃗r3−⃗r2)
× ei⃗k3·(⃗r4−⃗r3)ei⃗k4·(⃗r5−⃗r4)
(6:S2-22)
The same set of steps as outined immediately above leads to the following non-
vanishing contributions to the diagram, combinatorial factors having been left out,
 ∂2
∂k2
1x
∂2
∂k2
2x
+ ∂2
∂k2
3x
∂2
∂k2
2x
+ ∂2
∂k2
1x
∂2
∂k2
3x
+ ∂4
∂k4
2x

g

z; ⃗k0

g

z; ⃗k1

g

z; ⃗k2

g

z; ⃗k3

g

z; ⃗k4
...⃗k0=⃗k1=⃗k2=⃗k3=⃗k4=0
(6:S2-23)
Taking the derivatives indicated, evaluating the ⃗ki = 0 limits, and inserting the
required combinatorial factors, we end up with
18
N 4
l2
d
2
1

1 −zz−1
c
7
(6:S2-24)
Summing all diagrams
Adding (6:S2-16), (6:S2-21), and (6:S2-23), we end up with the following total
contribution to the generating function yielding ⟨T 2
11⟩
36
N 4
l2
d
2
1

1 −zz−1
c
7
(6:S2-25)
Actually, the generating function yields the numerator in a fraction. The denomina-
tor is the total number of N −1-step walks. However, as we are interested in ratios
of averages, the common denominator is not important for our present purposes.
For the same reason, we are not required to extract the coefﬁcient of zN−1 in the
power-series expansion of (6:S2-25), as that produces a common factor that cancels
out when we evaluate ratios.
6.5.4 The ratios
The calculations of expressions contributing to ⟨T11T22⟩and ⟨T 2
12⟩proceed along
the lines laid out above. For each of these averages, there are three contributions,
corresponding to the three diagrams in Figure 6.19. Carrying out the required

6.6 Supplement 3
165
computations, we ﬁnd
⟨T 2
11⟩
⟨T11T22⟩= 9
5
(6:S2-26)
⟨T 2
11⟩
⟨T 2
12⟩= 9
2
(6:S2-27)
Inserting these results into the right hand side of (6:S2-1), we obtain (6.15) for the
mean asphericity of a d-dimensional random walk.
6.6 Supplement 3: derivation of (6.21) for the radius of gyration tensor,
↔
T ,
and the eigenvalues of the operator
The demonstration of (6.21) is straightforward. We start with (6:S2-2b) for the ele-
ments of
↔
T . Now, we rewrite the difference (r jl −rkl)2 in terms of the displacements
ηil:
(r jl −rkl)2 =

l
m= j+1
ηml
2
=
l
m= j+1
l
n= j+1
ηmlηnl
(6:S3-1)
Now, if we look at the product ηmlηnl, we can ask for the net contribution of this
product to the entry Tll. We do this by counting up the number of (r jl −rkl)2’s that
generate, when expanded as above, in that product. Assume, for the time being,
that m ≤n. Then, investigation leads to the result that there are m(N + 1 −n) such
terms. This is because there are m j’s less than or equal to m and N + 1 −n k’s
greater than or equal to n. We thus arrive at (6.21).


7
Path integrals and self-avoidance
The concept of a ﬁeld dates back to Euler, who introduced the notion to describe
ﬂuid ﬂows in his study of hydrodynamics. Methods and concepts based on ﬁeld
theory now pervade the physical sciences and engineering. Field-theoretical ideas
exert a strong inﬂuence on physical intuition and shape modern nomenclature.
In addition, some of the most powerful analytical tools available to the modern
scientist are those developed to study the behavior of ﬁelds.
In the context of models designed to describe the physical world, a ﬁeld is a
quantity that varies continuously in space and time. Examples are the electric and
magnetic ﬁelds, the velocity and density distributions of a liquid or vapor and the
quantum-mechanical wavefunction of a microscopic particle. In some cases, such
as the velocity and density ﬁelds introduced by Euler, the notion of continuity must
be taken advisedly. Because of the atomic structure of matter, one cannot carry the
notionofasmoothdensitydistributiondowntothelengthscalesonwhichmolecules
can be distinguished. There, the classical description is necessarily in terms of
particles. Quantum mechanically, wavefunctions replace the classical density and
velocity ﬁelds as the appropriate mode of description. This proviso notwithstanding,
in the regimes in which density and velocity ﬁelds accurately describe the state of
a liquid or vapor, they form the basis of an extremely useful theoretical model that
yields important physical properties of these systems.
It turns out that the random walk also lends itself to description in terms of a
ﬁeld. As in the case of a liquid or vapor, the ﬁeld-based description maintains its
validity in a restricted range of length scales. However, those restrictions allow
for the investigation of random walks in most of the interesting regimes, and the
ﬁeld theoretical model of random walks has associated with it a wide range of very
effective techniques.
These techniques will be especially useful when we confront the statistical con-
sequences of self-avoidance, a restriction on the random walk that makes it rele-
vant to the statistical mechanics of long chain polymers. We will discover that the
167

168
Path integrals and self-avoidance
self-avoiding random walk has many of the mathematical features of a self-
interacting ﬁeld, and that the statistics of such walks yield the conformational
properties of long, ﬂexible chain polymers.
A ﬁrst step in the development of a ﬁeld-theoretical description of the random
walk is to express random walk statistics in terms of the various paths a walker
can take as it makes its way from an initial point ⃗x to an ultimate destination ⃗y. In
appropriate limits, the generating function for this process takes the form of a path
integral, of the sort utilized in the study of quantum-mechanical, and especially
quantum ﬁeld-theoretical, processes (des Cloiseaux and Jannink, 1990; Feynman
and Hibbs, 1965; Freed, 1987; Itzykson and Drouffe, 1991; Kleinert, 1995). In
subsequent chapters the path integral formulation will allow us to argue for the
equivalenceofthestatisticsoftheself-avoidingrandomwalker(tobedeﬁnedbelow)
and the statistical mechanics of a ﬁctitious magnetic system. The spin system is an n-
component vector model, in that one imagines the magnetic moments as vector-like
objects, i.e. objects with a head and a tail, that are free to rotate in an n-dimensional
“internal” space. The space is called internal to differentiate it from the actual,
physical space in which the spins reside. This system is commonly referred to as
the O(n) model. The version of the O(n) model that is relevant to the self-avoiding
walker is the n →0 limit. This apparently non-physical (almost non-sensical) limit
admits of the application of powerful ﬁeld-theoretical methods. Among the methods
that prove most useful are those associated with the renormalization group approach
to critical phenomena. The development and exploitation of these matters will be
the subject of the next several chapters.
7.1 The unrestricted random walk as a path integral
The ﬁrst step in the development of a ﬁeld-theoretical description of the random
walk is to reformulate the process as a path integral. In a sense, this is just a matter
of semantics. As we will see almost immediately, the random walk is nothing more
than a discrete version of a path integral. However, the passage to a continuous
process – as mathematically improbable as it may seem at the time we perform
it – will lead us in some very interesting directions.
To begin, note that C(N; ⃗x, ⃗y), the expression for the number of walks starting
at ⃗x and ending at ⃗y is equal to the sum over all N-step paths that a random walker
can take in going from ⃗x to ⃗y. In this sense, we may describe C(N; ⃗x, ⃗y) as a path
sum. Formally, we can write
C(N; ⃗x, ⃗y) =

P
WP(N; ⃗x, ⃗y)
(7.1)
where WP(N; ⃗x, ⃗y) is the number of N-step walks from ⃗x to ⃗y that follow the

7.1 The unrestricted random walk as a path integral
169
path P. In the case of a walker on a lattice, the summand does not reduce to an
expression in terms of other quantities. There is exactly one walk per allowable
path. WP(N; ⃗x, ⃗y) is equal to 1 for every possible path, and the mathematics of the
random walk enters through the restriction on paths contributing to the sum. To
develop an expression of greater use to us, we recast the problem slightly and recall
results from the previous chapter.
Suppose we require that the walker pass through the point ⃗r on the way from ⃗x
to ⃗y. In particular, suppose that we restrict our considerations to walkers who go
from ⃗x to ⃗r in M steps and from ⃗r to ⃗y in N −M steps. The number of such walks
is equal to
C(M; ⃗x, ⃗r)C(N −M; ⃗r, ⃗y)
(7.2)
The total number of N-step paths from ⃗x to ⃗y is equal to the sum over intermediate
points, ⃗r, of the above expression. Thus,
C(N; ⃗x, ⃗y) =

⃗r
C(M; ⃗x, ⃗r)C(N −M; ⃗r, ⃗y)
(7.3)
When M is equal to N −1, the ﬁnal portion of the walk consists of exactly one
step, and (7.3) reduces to (2.7).
Let’s carry the process further, and break the walk into several segments. Suppose
we require that the walker pass through the point ⃗r1 after M1 steps, through the point
⃗r2 after M2 steps, and so on. If there are a total of k segments, then the number of
N-step walks from ⃗x to ⃗y satisfying the above criteria is equal to the product
C(M1; ⃗x, ⃗r1)C(M2; ⃗r1, ⃗r2) · · · C(Mk−1; ⃗rk−2, ⃗rk−1)C(Mk; ⃗rk−1, ⃗y)
(7.4)
Summed over the intermediate points:
C(N; ⃗x, ⃗y) =

⃗r1,..., ⃗rk
C(M1; ⃗x, ⃗r1)C(M2; ⃗r1, ⃗r2)
· · · C(Mk−1; ⃗rk−2, ⃗rk−1)C(Mk; ⃗rk−1, ⃗y)
(7.5)
Now, if the numbers M1, M2, . . ., Mk, where k
l=1 Ml = N, are all reasonably
large, then the individual contributions to the product in the sum can be replaced by
the asymptotic Gaussian forms as given in (2.20). The sums over the intermediate
points are also well-approximated by integrals, and we have
C(N; ⃗x, ⃗y) =

· · ·

d⃗r1 · · · d⃗rk

2π M1a2−3/2 e−3|⃗x−⃗r1|2/2M1a2
×

2π M2a2−3/2 e−3|⃗r1−⃗r2|2/2M2a2 ×
· · · ×

2π Mka2−3/2 e−3|⃗rk−⃗y|2/2Mka2
(7.6)

170
Path integrals and self-avoidance
Of course, we also know that the Gaussian form in (2.20) holds for the left hand
side of (7.6). This means that it must be possible to show that the right hand side
of (7.6) yields the appropriate Gaussian expression for C(N, ⃗x, ⃗y).
Exercise 7.1
Show that the right hand side of (7.6) yields the following:
C(N, ⃗x, ⃗y) =

2π Na2−3/2 exp

−3|⃗x −⃗y|2
2Na2

Now (7.6) tells us nothing new about the number of N-step walks between ⃗x and
⃗y, beyond the somewhat interesting fact that it can be represented as a convolution
of Gaussian integrals. The main utility of the formula lies in its connection to a
form on which one can build the notion of the random walk as a path – or functional
– integral. We arrive at the form as follows. First, we imagine that the differences
|⃗ri −⃗ri+1| ≡|⃗ri| can be considered to be small. If we replace Mi by Ni – so
that N = k
i=1 Ni – the exponents in (7.6) take the form
−3
2a2 Ni
|⃗ri −⃗ri+1|
Ni
2
∼−3
2a2 Ni
|⃗ri|
Ni
2
(7.7)
The term

|⃗ri|/Ni
2 looks like the square of a ﬁrst derivative. As a brake on the
temptation to denote it as such there is the fact that the denominator cannot be even
close to an inﬁnitesimal. Recall that the Gaussian forms for the number of random
walks between the various locations hold only when the Ni’s are large compared
to 1.
Nevertheless, we will, as needed, pretend that the limiting procedure
|⃗ri|
Ni
2
→
|d⃗ri|
dNi
2
is not absurd. In that case, the right hand side of (7.6) has the form of a “true”
functional integral, in that we can write
C(N; ⃗x, ⃗y) ∝

s

d⃗r(s) exp
	
−3
2a2
 N
0




d⃗r(s)
ds




2
ds

.
(7.8)
The above equation contains the newly introduced arc length parameter s = N,
or, passing to the limit of inﬁnitesimals, ds = dN. The path is now speciﬁed in

7.2 Self-avoiding walks
171
terms of the continuous curve ⃗r(s), where the continuous variable s ranges from 0
to N, and the vector function ⃗r(s) satisﬁes the boundary conditions ⃗r(0) = ⃗x and
⃗r(N) = ⃗y. The product in the above equation is clearly over an inﬁnite number of
factors, and the integration variables are the values of the function ⃗r(s) for all values
of the variable s in the range 0 < s < N. If we take the notion of s as a continuous
variable seriously, there is an uncountable inﬁnity of those factors.
It is, therefore, not surprising that the functional integral in (7.8) has mathematical
peculiarities. Not the least of them is the fact that the multiplicative constant that
turns the proportionality into an equality does not have a ﬁnite value. If placed on
the right hand side of the equation it is inﬁnite, and if the proportionality factor
appears on the left hand side it is equal to zero. There are other, more subtle and
vexing, complications that arise from the effectively pathological nature of the
functional integral in (7.8). These complications can be collectively characterized
as “ultraviolet divergences.”
In practice, however, the path integral will always be over a ﬁnite number of
discrete terms, and the ultraviolet problems that attend the functional integral as
deﬁned in (7.8) are not among the challenges facing whoever attempts evaluation
of the path integral. Many of the principal difﬁculties arise when we modify the
rules governing the random walk so as to make it self-avoiding.
7.2 Self-avoiding walks
The term “random walk” conjures up the image of a process that unfolds with
time. Someone, or something, wanders along a path that meanders from a starting
point to an eventual destination. In the cases we have considered up to now, the
trail of the walker may intersect itself at any number of occasions. This particular
kind of random walk is called unrestricted. The walker does not recall where it has
been, and all steps are independent random variables. An interesting, and important,
variation of the random walk endows the walker with just such a memory, which
induces a correlation between steps. The self-avoiding walker remembers its past
itinerary, and tries to avoid treading on the trail it has already left. It is thus clear
the self-avoiding random walk as a sequence of random events is non-Markovian.
In the most extreme case of self-avoidance, the walker stops cold when, by chance,
it lands on a previously visited spot. In a “gentler” version of the model, the walk
ceases with a probability that increases as the walker approaches, or comes into
contact with, the trail it has left.
As another variation, the walker probes its immediate surroundings and utilizes
information about whatever portion of the trail is nearby to bias the random process
that controls the choice of the immediately subsequent steps. The altered process

172
Path integrals and self-avoidance
favors steps that avoid landing on, or crossing over, the walker’s trail. No matter
where the walker steps it keeps on going. This last kind of walker is called the
“true” self-avoiding walker, as opposed to the walker described in the preceding
pragraph, who executes what is known simply as the self-avoiding walk. There are
other ways of altering the rules that govern the random walk, and in time we will
explore their consequences. For the time being, our attention will be focused on the
classical self-avoiding walk described in the ﬁrst paragraph of this section.
7.2.1 Interactions
How does one alter the mathematical prescriptions developed in the last chapter so
that they apply to the case of the self-avoiding walker? Recall (7.1), the formula
for the number of walks starting at ⃗x and ending at ⃗y. It consisted of a sum over all
possible paths. If the walker avoids paths previously taken, then the sum suffers a
modiﬁcation. An appropriate modiﬁcation is effected by the introduction of factors
of the form e−v(| ⃗wi−⃗w j|). In such a factor ⃗wl is the position vector connecting the
origin to the lth step of the walk. In fact, the sum over paths will be weighted by
the factor
N

i> j, i, j=1
e−v(| ⃗wi−⃗w j|)
(7.9)
If the function v(x) becomes positive and inﬁnite for any value of the argument
x, then the weighting factor e−v(x) vanishes, and the path is deleted as a possible
walk from ⃗x to ⃗y. This means that the product in (7.9) can be made consistent with
“strict” self-avoidance. We will assume for the present that v(x) is always positive.
The total number of N-step self-avoiding walks starting a ⃗x and terminating at ⃗y is
then given by
C(N; ⃗x, ⃗y) =

P
WP(N; ⃗x, ⃗y)
	 N

i=1
e−v(| ⃗wi−⃗x|)e−v(| ⃗wi−⃗y|)

N

i> j, i, j=1
e−v(| ⃗wi−⃗w j|)
(7.10)
7.2.2 Path integral revisited: self-avoidance
The functional integral form for the unconstrained walk as prescribed in (7.8) is
now easily modiﬁed to account for the fact that intersecting paths are to be avoided.
When self-avoidance comes into play, extra terms enter into the integrand. These
terms arise as a result of the recasting of the exponential in (7.8).

7.2 Self-avoiding walks
173
Following the development above, we can write for the number of non-
intersecting walks beginning at ⃗x and ending at ⃗y
C(N; ⃗x, ⃗y)
=

· · ·

d⃗r1 · · · d⃗rk

2π M1a2−3/2 e−3|⃗x−⃗r1|2/3M1a2 × · · ·
×

2π Mka2−3/2 e−3|⃗rk−⃗y|2/3Mka2 
l
e−v(|⃗rl−⃗x|)−v(|⃗rl−⃗y|) 
i> j
e−v(|⃗ri−⃗r j|)
(7.11)
The product can be written as a sum:
e−
l v(|⃗rl−⃗x|)−
l v(|⃗rl−⃗y|)e−
i> j v(|⃗ri−⃗r j|)
(7.12)
Under the same limiting procedures as applied in the case of the unrestricted walk,
the factor above takes on the following continuous form
exp

−v0
 N
0
 s1
0
ds1 ds2 v
|⃗r(s1) −⃗r(s2)|
(7.13)
Equation (7.11) then becomes
C(N; ⃗x, ⃗y) ∝

s

d⃗r(s)exp

−

3/2a2  N
0




d⃗r
ds




2
ds −v0
×
 N
0
 s1
0
ds1 ds2 v
|⃗r(s1) −⃗r(s2)|
(7.14)
If the interaction leading to self-avoidance is extremely short-ranged, then the
function v(|⃗r1 −⃗r2|) can be replaced by a Dirac delta function, and the weighting
factor that guarantees self-avoidance takes the form
exp

−v0
 N
0
 s1
0
ds1 ds2 δ
|⃗r(s1) −⃗r(s2)|
(7.15)
and the functional integral in (7.14) is replaced by
C(N; ⃗x, ⃗y) ∝

s

d⃗r(s) exp

−

3/2a2  N
0




d⃗r
ds




2
ds −v0
×
 N
0
 s1
0
ds1 ds2 δ
|⃗r(s1) −⃗r(s2)|
(7.16)
In the next chapter we will return to this functional integral and show that it predicts
interesting and useful scaling properties of the function C(N; ⃗x, ⃗y).

174
Path integrals and self-avoidance
x
f(x)
1
Fig. 7.1. The function f (x) deﬁned in (7.17). The function shown here is appro-
priate to the case of short-ranged, and very strong, self-avoidance of the random
walk.
Expansion in self-avoidance
The product of factors that enforce self-avoidance seriously complicates the eval-
uation of random walk sums. In fact, there is no known exact expression for the
number of self-avoiding walks in three dimensions that satisfy any reasonable set of
self-avoidance requirements. However, approximate methods and some very clever
analytical tricks have yielded non-trivial results of practical value. One such set of
approaches is based on an expansion in the tendency to avoid previously visited
portions of space. Recall an individual term in the product e−v(x). This term can be
rewritten as follows
e−v(x) = 1 +

e−v(x) −1

≡1 −f (x)
(7.17)
The quantity f (x) represents the modiﬁcation of random walk statistics induced by
self-avoidance. If a walker’s tendency to avoid itself as expressed in the function
v(x) is short-ranged, then the function f (x) will be equal to zero except in a small
region in the vicinity of x = 0 (see Figure 7.1). On the other hand, when f (x)
differs from zero it never exceeds 1 in absolute value. Rewriting each term in the
product that appears on the right hand side of (7.10) as above we obtain for the
effect of self-avoidance
C(N; ⃗x, ⃗y)
=

P
WP(N; ⃗x, ⃗y)
	 N

i=1

1 −f
| ⃗wi −⃗x| 
1 −f
| ⃗wi −⃗y|

×
N

i> j i, j=1

1 −f


 ⃗wi −⃗w j



(7.18)
The next step is to expand (7.18) in powers of the factors f . The zeroth order
term in the expansion is just the number of ordinary, unrestricted walks. The ﬁrst

7.2 Self-avoiding walks
175
x
y
u2
u1
Fig. 7.2. The ﬁrst order diagram.
x
x
x
y
y
y
u2
u4
u3
u1
u1
u1
u2
u2
u3
u3
u4
u4
(a)
(c)
(b)
Fig. 7.3. The second order diagrams.
order term is
−

M1,M2,M3

⃗u1,⃗u2 (⃗u1>⃗u2)
C(M1; ⃗x, ⃗u1)C(M2; ⃗u1, ⃗u2)C(M3; ⃗u2, ⃗y) f
|⃗u1 −⃗u2|
(7.19)
where the position vectors ⃗u1 and ⃗u2 range from ⃗x to ⃗y through all the ⃗wi’s. The
quantities Mi satisfy M1 + M2 + M3 = N. Figure 7.2 gives a pictorial representa-
tion of the above ﬁrst order correction to C(N; ⃗x, ⃗y). The second order correction
is a bit long to write out. The pictorial representation provides a useful shorthand.
Figure 7.3 displays the three different expressions that are generated when one
expands the right hand side of (7.18) to second order in the factor f . These pictures
depict all the topologically distinct ways in which a walker can travel from ⃗x to ⃗y
in k + 1 segments that connect the starting and ending points to the intermediate
locations ⃗u1 . . . ⃗uk. The ﬁgures are more than a convenient pictorial representation.
Given the appropriate interpretations, they can be constructed so as to contain all
the information needed to reconstitute the expressions they represent.

176
Path integrals and self-avoidance
The expansion we have developed here is a particular example of a perturbation
theoretical expansion.
Exercise 7.2
Pictorially represent the topologically distinct graphs for the third-order-in- f (x)
correction to the generating function for self-avoiding random walks.
7.2.3 Perturbation theoretical expansion of the generating function
It is considerably more convenient to develop a perturbation series for the generating
function of the number of walks than for the quantity itself. Here’s why.
The series for the number of walks resulted from the expansion of the product in
(7.18), which generated terms like (7.19). This series is readily converted to a series
for the generating function by multiplying each term by zN and summing over N.
The key simpliﬁcation arises from the fact that sums over intermediate numbers of
steps, i.e. M1, M2, . . . Mk, where M1 + M2 + · · · + Mk = N – that is, sums in the
form of convolutions – convert to simple products. To see how this works consider
the following convolution
N

M1=0
N

M2=0
H1(M1)H2(M2)
(7.20)
with M1 + M2 = N.
Now if the sum above is multiplied by zN and N is summed from 0 to ∞, one
obtains
∞

N=0
N

M1=0
zN H1(M1)H2(N −M1)
=
∞

N=0
N

M1=0
zM1 H1(M1)Z N−M1 H2(N −M1)
=
∞

K1=0
zK1 H1(K1)
∞

K2=0
Z K2 H2(K2)
= h1(z)h2(z)
(7.21)
where
h1(z) =
∞

K=0
H1(K)zK
(7.22)
and similarly for h2(z). The reader can readily verify that the change in summation

7.2 Self-avoiding walks
177
variables in (7.21) from N and M1 to K1 and K2 is justiﬁed in that there is a one-
to-one relation between the terms in the sums on the second and third lines of the
equation.
Fourier transforms also have the effect of transforming convolutions into simple
products. Consider, for example the following integral
 ∞
−∞
F1(⃗x −⃗w)F2( ⃗w −⃗y) ddw
(7.23)
Multiplying the expression above by ei⃗q·(⃗x−⃗y) and integrating over ⃗y we obtain
 
ei⃗q·(⃗x−⃗y)F1(⃗x −⃗w)F2( ⃗w −⃗y) ddw dd y
=

ei⃗q·(⃗x−⃗w)F1(⃗x −⃗w) ddw

ei⃗q·( ⃗w−⃗y)F2( ⃗w −⃗y) dd y
=

F1(−⃗r1)e−i⃗q·⃗r1 ddr1

F2(−⃗r2)e−i⃗q·⃗r2 ddr2
≡f1(⃗q) f2(⃗q)
(7.24)
where
f1(⃗q) =

F1(⃗r)ei⃗q·⃗r ddr
(7.25)
and similarly for the function f2(⃗q).
Because of this simpliﬁcation of sums in the form of convolutions, the mathe-
matics of the expansion is greatly simpliﬁed when the quantity under consideration
is the Fourier transform of the generating function.
Diagrammatic rules
The generating function for unrestricted walks is as given by (2.13). The full gener-
ating function follows from the Fourier transform of (7.18), which is then multiplied
by zN and summed over N. The ﬁrst order term in the expansion in powers of the
function f , as illustrated in Figure 7.2, is given by
1
1 −zχ(⃗q)


⃗Q
F(⃗q −⃗Q)
1
1 −zχ( ⃗Q)


1
1 −zχ(⃗q)
≡g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)

g0(z; ⃗q)
(7.26)
The function g0(z; ⃗q) is the “bare,” or unrestricted generating function. Notice that
the transformation from number of steps to fugacity as an independent variable

178
Path integrals and self-avoidance
x
y
u4
u3
u1
u2
u5
u6
Fig. 7.4. The third order ﬁgure.
greatly simpliﬁes the calculation of terms in the expansion, in that it is no longer
necessary to perform convolutions in the number of steps. By the same token, the
spatial Fourier transform allows for a simpliﬁcation of important aspects of the
calculation. Figure 7.3(a), for instance, corresponds to the following expression.
g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)

g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)

g0(z; ⃗q)
= g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)g0(z; ⃗q)


2
(7.27)
Exercise 7.3
Write down the explicit expressions for the contributions to the generating function
corresponding to the second order graphs depicted in Figure 7.3.
This expression, and the ﬁrst order correction in (7.26), look like terms in a
geometric series. If we add them to the generating function of the unrestricted
walk, we have the sum
2

n=0
g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)g0(z; ⃗q)


n
(7.28)
Now, as the order of the expansion increases, additional terms appear that com-
plete the sum above. For example, there is the third order contribution (shown in
Figure 7.4) that corresponds to
g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)g0(z; ⃗q)


3
(7.29)
Assuming, as must now seem to be eminently reasonable, that the expansion yields

7.2 Self-avoiding walks
179
all the terms in the inﬁnite geometric sum, we are left with a new, approximate
result for g(z; ⃗q)
g(z; ⃗q) ≈
∞

n=0
g0(z; ⃗q)


⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)g0(z; ⃗q)


n
=
g0(z; ⃗q)
1 −
⃗Q F(⃗q −⃗Q)g0(z; ⃗Q)g0(z; ⃗q)
=
1

g0(z; ⃗q)
−1 −
⃗Q F(⃗q −⃗Q)g0(z; ⃗Q)
=
1
1 −zχ(⃗q) −
⃗Q F(⃗q −⃗Q)g0(z; ⃗Q)
≡
1
1 −zχ(⃗q) −1(z; ⃗q)
(7.30)
The expression 1(z; ⃗q) is the ﬁrst order term in the expansion in f of the
self-energy. This function is also called the mass operator because of the role it
plays in ﬁeld-theoretical models of elementary particles. The sum we’ve performed
is equivalent to solving the ﬁeld-theoretical recursion relation known as Dyson’s
equation. This is all very nice, and there is much that can be done with a certain
amount of cleverness (and hindsight). However, to make the most of the expansion
to which we’ve just been introduced, it is useful to develop a set of rules that al-
low one in principle to set up the calculation of an arbitrary term in the expansion,
and – as it turns out – to produce systematic groupings of those terms. This account-
ing procedure involves a diagrammatic representation of the terms, the diagrams
being of the sort contained in Figures 7.2–7.4.
This diagrammatic representation of an expansion is standard in the physics of
systems with a large number of degrees of freedom. In the case of quantum ﬁeld
theories the diagrams are known generically as Feynman diagrams. When the focus
is a classical ﬂuid or gas, the names associated with the expansion are Ursell and
Mayer. The advantage of diagrammatic expansions is that a pictorial representation
of various terms elucidates the underlying processes and lends itself quite naturally
to various useful groupings and redeﬁnitions. Explicit demonstrations of the utility
of the diagrammatic expansion will be found in this and subsequent chapters.
The development of the rules to be listed below involves a relatively lengthy and
tedious set of steps. Although there are rules that apply to the expansion as carried
out above, the most straightforward set, and the one giving rise to terms that can
be most easily evaluated, applies to an expansion for g(z; ⃗q), the spatial Fourier
transform of the random-walk-generating function. The rules are as follows.

180
Path integrals and self-avoidance
1. The diagrams consist of lines and vertices. There are two kinds of lines. One is a prop-
agator line, the other is an interaction line. The propagator lines, connected end-to-end
at the vertices form a single segmented representation of the random walk.
2. All lines in the expansion of the Fourier-transformed generating function carry a wave-
vector. Because of the connection between the diagrammatic expansion generated here
and the expansion utilized in quantum ﬁeld theory, the phrase “wave-vector” is replaced
by the term “momentum.” The “direction” of the momentum is indicated by an arrowhead
on the line. The arrowhead on the propagator lines point from the starting point of the
random walk to its ultimate destination.
3. A propagator line carrying a momentum ⃗q stands for the expression g0(z; ⃗q) =
1/ (1 −zχ( ⃗q )). The interaction line carrying a momentum ⃗Q stands for the expres-
sion −F( ⃗Q). The net expression is the product of all the above factors. Lines meet at
vertices. Each vertex enforces “conservation of momentum,” in that the total momentum
carried into the vertex by the lines that meet there is equal to the total momentum carried
out of that vertex. A line with momentum ⃗q whose arrowhead points towards the vertex
carries a momentum ⃗q into the vertex. If the arrowhead points away from the vertex then
the line carries a momentum equal to ⃗q out of the vertex.
In this set of diagrams, the vertex is the meeting point of two propagator lines and one
interaction line. A vertex such as this, at which three lines meet is called a three-point
vertex.
4. The “external” momentum of the diagram is carried by the propagator line that emanates
from the starting point. That momentum is held ﬁxed.
5. With the exception of the external momentum, the ultimate expression is obtained by
integrating over all momenta in the various factors, subject, of course, to the constraints
imposed as the result of momentum conservation at the vertices.
This dizzying collection of rules merits a closer going over. Let’s review them
in a little more detail.
Rule 1
The propagator line is represented as a solid line. The interaction line is
drawn dotted. The vertex is heavy dot. These elements are shown in detail in
Figure 7.5(a).
Rule 2
Figure 7.5(b) shows the lines displayed in Figure 7.5(a) with arrowheads now
attached.
Rules 3 and 4
The expression corresponding to the set of lines and vertices in Figure 7.6 is
g0(z; ⃗q1)g0(z; ⃗q2)(−F(⃗q3))g0(z; ⃗q4). Momentum conservation has not yet been in-
voked, and no integrations are as yet speciﬁed. Figure 7.7 illustrates the meeting of
lines at a vertex. Momentum conservation is made explicit.

7.2 Self-avoiding walks
181
interaction line
interaction line
propagator line
propagator line
vertex
(a)
(b)
Fig. 7.5. Elements of the diagrammatic approach.
q
3
q
4
q
1
q
2
Fig. 7.6. The one loop-correction to the propagator line.
q1
q1
q2
q2
 −
Fig. 7.7. The vertex.

182
Path integrals and self-avoidance
Fig. 7.8. The one-loop self-energy.
Rules 5 and 6
The full expression corresponding to the diagram in Figure 7.6 is
g0(z; ⃗q)

−

⃗Q
F(⃗q −⃗Q)g0(z; ⃗Q)

g0(z; ⃗q)
Sums convert to integrals in inﬁnite systems, as indicated in Supplement 2 of
Chapter 2.
7.2.4 Dyson’s equation
Given the rules above we are now in a position to take a closer look at the expansion
in powers of self-avoidance. The fundamental structure of a term in the expansion
is as follows. There is a central line that carries the external momentum, ⃗q. This line
reappears as segments joining self-energy terms. In the case of the sum of terms
that produced the result in (7.30), the self-energy has the diagrammatic form shown
in Figure 7.8. Note that these have the form of “amputated” diagrams, in that no
propagator lines lead into or out of them.
The mathematical structure of a self-energy is as follows. There will be an even
number of vertices, say 2m. There will be m internal interaction lines and 2m −1
propagator lines. With the exception of two of them, each vertex will have two
propagator lines and one interaction line attached to it. The two exceptional ver-
tices will have an interaction line and only one internal propagator line attached.
The diagram will be “one particle irreducible” (1PI) in that it will not be possible to
split the diagram that represents it into two by eliminating one propagator line. The
last phrase is a residue of the quantum ﬁeld theoretical development of diagram-
matic techniques. Because of momentum conservation there are m integrations.
Figure 7.9 contains an example of a self-energy diagram. The actual expression is

ddq1
(2π)d

ddq2
(2π)d F(⃗q1)g0(z; ⃗q2)g0(z; ⃗q −⃗q1)g0(z; ⃗q2)
× g0(z; ⃗q1 −⃗q2)F(⃗q −⃗q1 −⃗q2)
(7.31)

7.2 Self-avoiding walks
183
q–q1
q1
q1–q2
q–q1+q2
q2
Fig. 7.9. The more general self-energy. The qi’s label the wave-vectors carried by
each line.
+
=
+
+
+
Fig. 7.10. The summation leading to Dyson’s equation.
These contributions enter into the ultimate expression for the generating func-
tion according to the following diagrammatically based argument. Consider the
following set of contributions to the generating function
g0(z; ⃗q) + g0(z; ⃗q)1g0(z; ⃗q) + g0(z; ⃗q)1g0(z; ⃗q)1g0(z; ⃗q) + · · ·
(7.32)
where g0(z; ⃗q) = 1/(1 −zχ(⃗q)) and 1 = 
⃗Q F(⃗q −⃗Q)g0(z; ⃗Q), and the series
was shown to sum to 1/(g−1
0 (z; ⃗q) −1).
This argument can be generalized. Consider the contribution to g(z; ⃗q) displayed
in Figure 7.10 Mathematically, this contribution is
= g0 + g0 [1 + 2] g0 + g0 [1 + 2]2 g0 + · · ·
(7.33)
with 1 as given above and
2 =

⃗Q1, ⃗Q2
F(⃗q −⃗Q1)g0(z; ⃗Q1)F( ⃗Q1 −⃗Q2)g0(z; ⃗Q2)g0(z; ⃗Q1)
(7.34)
Again, the geometrical series is readily summed to yield
g0(z; ⃗q)
1 + (1 + 2) g0(z; ⃗q) =
1
g−1
0
−(1 + 2)
(7.35)

184
Path integrals and self-avoidance
The above generalizes even further. The ultimate perturbation theoretical expres-
sion for the generating function g(z; ⃗q) will have the form
g(z; ⃗q) = g0(z; ⃗q) + g0(z; ⃗q)(z; ⃗q)g0(z; ⃗q)
+ g0(z; ⃗q)(z; ⃗q)g0(z; ⃗q)(z; ⃗q)g0(z; ⃗q) + · · ·
(7.36)
where (z; ⃗q) is the sum of all mass operator insertions. Thus, a given contribution
to g(z; ⃗q) will consist of a string of self-energy diagrams connected by “bare”
propagator lines. The series is formally summed, and the resulting solution has the
form
g(z; ⃗q) =
g0(z; ⃗q)
1 −(z; ⃗q)g0(z; ⃗q)
=
1
g−1
0 (z; ⃗q) −(z; ⃗q)
(7.37)
Notice that the left hand side of (7.37) has exactly the same form as the ﬁnal result
displayed in (7.30). The only difference is that in the latter, complete version, the
self-energy is the sum of an inﬁnite number of terms in the expansion in self-
avoidance. The problem of determining the generating function reduces to the task
of evaluating the members of the series that is summed to obtain the self-energy,
and then of summing the series for that quantity.
At ﬁrst sight the task may seem hopeless. In fact, it is. There is no known method
of exactly performing the indicated sum. Not only is there an inﬁnite number of
terms, but the multiple integrals become increasingly complicated at higher and
higher order. This accumulation of difﬁculties characterizes essentially all expan-
sions of the type discussed here. Fortunately, a number of techniques now exists
for the extraction of the key features of the quantity of interest. Some are relatively
simple to implement, relying on clever, intuitively based, heuristic arguments. Oth-
ers are more technically formidable, and involve substantial calculational effort. In
the sections and chapters to follow, we will make use of some of the methods that
have proven particularly successful when applied to the problem of self-avoiding
random walks.
Before doing so, we will perform a quick but revealing analysis of the dependence
on fugacity of contributions to the self-energy. This analysis tells us something very
important: that no low order truncation of the expansion will sufﬁce to reveal the
asymptotic statistics of a long self-avoiding walk.
7.2.5 Breakdown of perturbation theory
Although the multiple integrals corresponding to a given high order contribution
to the self-energy will, in all probability, defy attempts to produce a closed form

7.2 Self-avoiding walks
185
expression, important aspects of the result of the integration can be inferred with the
use of simple tricks. Here we will use one such stratagem to derive the dependence
on the difference zc −z of a term in the expansion of (z; ⃗q) in powers of the factor
f . Suppose the power is l. There will be 2l vertices in the diagram corresponding
to the term and 2l −1 propagator lines. Each propagator line contributes a factor to
the integrand of the form 1/(1 −zχ(⃗qi)) ∼1/((z −zc) + q2
i ). We will assume that
the momentum dependence of the functions f (⃗q j) is unimportant, or uninteresting.
In fact, it is generally a very good approximation to physical reality to assume
that f (⃗q j) is independent of ⃗q j. Because there are l integrations over the ⃗qi’s, the
integral will, in very general terms, look like

f l

1
(zc −z) + q2
i
2l−1 
ddqi
l
(7.38)
We extract the effect of the combination zc −z on the integral by replacing the qi’s
in the integral by (zc −z)1/2q. This has the effect of replacing the random walk
propagators by 1/(1 + q2
i ) and generating the overall factor
(zc −z)−(2l−1)+dl/2 = (zc −z)1−l(4−d)/2
(7.39)
All dependence on zc −z has been removed from the integrand with one exception.
Because the external momentum ⃗q, is not rescaled or integrated over, the integrand
will depend on the rescaled external momentum ⃗q(zc −z)−1/2 . Aside from this,
the entire effect of the combination (zc −z) can be extracted from the factor in
(7.39). The overall task of performing the multiple integral has not, of course, been
simpliﬁed in any real way. However, the general form of a term in the expansion
for (z; ⃗q) is now apparent. We expect that the mass operator is representable as
the following kind of sum
(z; ⃗q) = (zc −z)

n

(zc −z)−(4−d)/2 f
n Xn

⃗q(zc −z)−1/2
(7.40)
The quantity f in (7.40) is the momentum-independent amplitude of the term
enforcing self-avoidance.
Now, when f is small, the penalty for self-intersection is mild. The probability
that a walker will disappear because it has trespassed on its own path is small.
It is reasonable to expect that the modiﬁcation of the self-energy resulting from
self-avoidance will be correspondingly unimportant. However, high order terms in
the sum in (7.40), as reduced in amplitude as they are because of the factor equal
to a high power of the small amplitude f , also contain the difference zc −z raised
to a large negative power. It is this latter term in the high order correction to the
self-energy that ultimately controls the effect of the correction on the generating
function. In order to see how perturbation theory breaks down, let’s reconstitute

186
Path integrals and self-avoidance
from the generating function the expression for the total number of N-step walks
that start out at ⃗x and end up anywhere. This is just

C(N; ⃗x, ⃗y) dy
(7.41)
The above is equal to the coefﬁcient of zN in the corresponding integral of the
generating function G(z; ⃗x, ⃗y). However, given the deﬁnition of the spatial Fourier
transform, it is clear that the integral gives rise to g(z; ⃗q = 0). Thus, the answer we
seek is the coefﬁcient of zN in
g(z; 0) =
1
(g0(z; 0))−1 −(zc −z; 0)
→
1
(zc −z) 
n Xn(0)

(zc −z)−(4−d)/2 f
n
(7.42)
Now, as demonstrated in the third supplement to Chapter 2, the coefﬁcient of zN
when N is large is determined by the location and order of the singularities in the
complex z-plane. In particular, the leading contribution to the coefﬁcient of zN is
controlled by the singularity of the form (zs −z)−p for which zs lies closest to the
origin. If there is more than one such singularity, then the one with the largest value
of the exponent p contributes the greatest part of the coefﬁcient of interest. The
expression (7.42) exhibits singular behavior at z = zc. This is the dominating point
of non-analyticity in the complex z-plane, in that it is the singularity that is closest
to the origin. The difﬁculty lies in the nature of the singularity. The power p is equal
to 1 −nh(4 −d)/2, where nh is the order beyond which no terms in the perturbation
series are retained.1 The higher the order at which the series is truncated, the lower
the value of the exponent. If all powers are retained, a naive interpretation of the
consequences of the above argument is that the exponent is lower than any given
value, or −∞. However, the sum of the terms may easily lead to a modiﬁcation of
the analytic structure that is entirely different from this guess and from the result
that follows from any truncation of the perturbation series. In all events, it is clear
that there is no reason to trust the results of ﬁnite order perturbation theory.
7.2.6 Mean ﬁeld theory
There are a number of approximate solutions to the problem of summing the per-
turbation theoretical series for the generating function. One of the most venerable
1 This result tells us that d = 4 dimensions is special in some sense. In fact, we will see that the perturbation
expansion derived in this chapter does, indeed, make perfect sense as an expansion in a small quantity in more
than four dimensions. In fewer dimensions we can “rescue” perturbation theory by constructing an expansion
in the difference between four and the dimensionality of interest to us.

7.2 Self-avoiding walks
187
is called the mean ﬁeld solution. This nomenclature derives from the genesis of
the technique in the context of the behavior of magnetic systems and, in particu-
lar, an approximation to the temperature dependence of the magnetism of a set of
interacting moments that is based on the notion of an effective average (or mean)
magnetic ﬁeld. Here, the mean ﬁeld solution follows from an approximation to the
bare generating function based on an expansion of G(z; ⃗x, ⃗y) in a complete set of
states.
Recall that when translational invariance holds, the propagator can be expanded
in plane waves. That is, one can write
G(z; ⃗x, ⃗y) =

⃗q
ei⃗q·⃗x
√
V
g0(z; ⃗q)e−i⃗q·⃗y
√
V
(7.43a)
≡

⃗q
ψ⃗q(⃗x)g0(z; ⃗q)ψ∗
⃗q (⃗y)
(7.43b)
≈

⃗q
ψ⃗q(⃗x)
zc
zc −z + zcza2q2 ψ∗
⃗q (⃗y)
(7.43c)
≡

⃗q
ψ⃗q(⃗x)
zc
z⃗q −z ψ∗
⃗q (⃗y)
(7.43d)
The quantity z⃗q in the last line of (7.43d) is equal to zc + zcza2q2. For our
purposes we can write
z⃗q = zc + z2
ca2q2
(7.44)
(7.43c) holds to the extent that the sum is dominated by the terms for which ⃗q is
small. Now, (7.43d) is a generally proper representation of the unrestricted walk
generating function, even when the environment in which the walk takes place is not
invariant with respect to spatial translations. The more general form of G(z; ⃗x, ⃗y)
is
G(z; ⃗x, ⃗y) =

i
ψi(⃗x)
zc
zi −z ψ∗
i (⃗y).
(7.45)
The functions ψ, ψ∗form a complete orthonormal set, in that

ψi(⃗x)ψ∗
j (⃗x) ddx = δi, j
(7.46)
The expansion above follows from the mathematical equivalence between the
generating function and a Green’s function. In the continuum limit the quantity

188
Path integrals and self-avoidance
G(z; ⃗x, ⃗y) satisﬁes the partial differential equation
za2∇2G(z; ⃗x, ⃗y) −(z/zc −1)G(z; ⃗x, ⃗y) = adδ(⃗x −⃗y)
(7.47)
To derive (7.43d) one expresses the Dirac delta function in terms of the complete
set of eigenfunctions of the operator ∇2. These functions satisfy ∇2ψi = −λiψi.
In particular
ψi(⃗r) = e−⃗q·⃗r
(7.48)
and
λi = |⃗qi|2
(7.49)
In terms of these eigenfunctions
G(z; ⃗x, ⃗y) = zcad 
i
ψi(⃗x)ψi(⃗y)
(zc −z) + zcza2q2
= zcad 
i
ψi(⃗x)ψi(⃗y)
zi −z
(7.50)
where zi = zc + zcza2q2.
The expansion in (7.45) is entirely equivalent to (7.43dd) if one makes the con-
nections zi ↔z⃗q = zc(1 + zca2q2), ψi(⃗x) ↔ψ⃗q(⃗x) = ei⃗q·⃗x/
√
V and similarly for
ψ∗
i . Here V is the volume of the system.
Now, imagine that the sum in (7.45) is dominated by a single term. That is,
imagine that we can approximate G0(z; ⃗x, ⃗y) as follows
G(z; ⃗x, ⃗y) ≈G0(z; ⃗x, ⃗y)
= zc
ψ0(⃗x)ψ∗
0 (⃗y)
z0 −z
(7.51)
If the walk takes place in a large system that has no walls, or for which periodic
boundary conditions hold, then an appropriate choice for the dominant mode ψ0 is
the plane wave with inﬁnite wavelength. In this case ψ0(⃗x) = 1/
√
V , and z0 = zc.
The other plane wave modes, neglected in this approximation, represent, in the
terminology that has developed around this approach, the effects of ﬂuctuations
about the mean ﬁeld solution.
Theresultsthatfollowfromtheapproximationwehavejustmadewillyielduseful
insights into the consequences of self-avoidance. In particular, they will provide
motivation for a powerful set of heuristic arguments ﬁrst put forth by Flory (1953;
1969), leading to some very nice predictions for the behavior of polymers, where
the effects of excluded volume due to the strong repulsive interactions between
monomeric units comprising the polymer are modeled by self-avoiding walks.

7.2 Self-avoiding walks
189
Because of the simple form of the approximate expression for the propagator in
(7.51) the perturbation series is most easily analyzed in real space.
Now, the insertion of a single interaction line, connected to the propagator line
at two vertices is accomplished by acting on the bare propagator line with the
following operator:
O( f ; z) = −
 
ψ( ⃗w1)ψ∗( ⃗w1)
2 ddw1
f
2
d2
dz2
(7.52)
The result is the following ﬁrst order correction to the approximate unrestricted
walk generating function
−

ddw1ψ(⃗x)
zc
z0 −z ψ∗( ⃗w1)ψ( ⃗w1)
zc
z0 −z ψ∗( ⃗w1)ψ( ⃗w1)
zc
z0 −z ψ∗(⃗y) f
(7.53)
This expression has the same general form as the ﬁrst order correction to the un-
restricted random-walk-generating function exhibited in (7.26), with the exception
that here the unrestricted random walk propagator has a speciﬁc, approximate form.
Note that in this case we assume that the factor f is independent of momentum.
Further terms in the expansion in f of the generating function are generated
with additional applications of the operator in (7.52). To eliminate overcounting,
it is necessary to multiply the operator applied n times by the factor 1/n!. All
terms in the expansion of the propagator of a self-avoiding walk in terms of the
self-avoidance factor f arise if one takes the following sum

n
O( f ; z)n
n!
ψ(⃗x)ψ∗(⃗y)
z0 −z
zc = eO( f ;z) ψ(⃗x)ψ∗(⃗y)
z0 −z
zc
(7.54)
A few manipulations sufﬁce to derive a complete expression for the full mean
ﬁeld self-avoiding walk propagator. First we rewrite 1/(z0 −z) as
 ∞
0
dt e−t(z0−z)/zc
(7.55)
Then, we note that d2/dz2etz = t2etz. This means that the right hand side of (7.54)
is equal to
ψ(⃗x)ψ∗(⃗y)
 ∞
0
exp

−f t2/2

ddw

ψ( ⃗w)ψ∗( ⃗w)
2 −t (z0 −z)
zc

dt
(7.56)
The ﬁnal steps leading to the mean ﬁeld generating function follow from the
identiﬁcation of ψ0 with

ad/V = 1/
√
N, N being the number of lattice sites

190
Path integrals and self-avoidance
available to the walker and z0 with zc. This, we can write
G0 = GMF(z; ⃗x, ⃗y)
= 1
N
 ∞
0
exp

−f t2
N −t (1 −z/zc)

dt
=
 ∞
0
exp

N

−f λ2 −λ (1 −z/zc)

dλ
(7.57)
The integral expression for GMF is actually valid for all z, though our derivation
assumes z < zc. In evaluating the integral some care must be taken. The cases
z < zc and z > zc must be treated separately. The relevant case for us is z > zc. For
this case, the expansion in z is obtained in a straightforward way by expanding the
exponential e−Nzλ/zc.
GMF(z; ⃗x, ⃗y) =
∞

m=0
zm N m
m!
 ∞
0
e−N f λ2−Nλλn dλ
=

m
C(m; ⃗x, ⃗y)
 z
zc
m
(7.58)
We immediately ﬁnd that the number of m-step non-intersecting walks that start
out at ⃗x and end at ⃗y is, in the mean ﬁeld approximation, given by
CMF(m; ⃗x, ⃗y) = 1
zmc
N m
m!
 ∞
0
e−N( f λ2+Nλ)+m ln λ dλ
(7.59)
When m is large, the leading contribution to the integral is
CMF(m; ⃗x, ⃗y) = 1
N z−m
c
e−f m2/N
(7.60)
Exercise 7.4
Show that for large N, (7.60) always follows from (7.59).
Exercise 7.5
Using (7.57) for the mean ﬁeld generating function, show that the number of m-step
walks starting at x and ending at y for the case z < zc is just z−m
c
/N, and hence
self-avoidance has no effect. The interpretation of this result is fully explained in
Chapter 10.

7.2 Self-avoiding walks
191
If the linear extent of the region to which the walker is conﬁned is R, so that the
volume is proportional to Rd (i.e. N = αRd), we can write
CMF(N; ⃗x, ⃗y) = z−m
c
N e−f m2/αRd
= (2d)m
N
e−um2/Rd
(7.61)
The last equality in (7.61) follows from the deﬁnition of the quantity u = f/α
and the replacement of zc by its value for the case of a simple cubic lattice.
7.2.7 Flory’s argument
The ﬁnal result above is very revealing. Since N is just the total number of sites
available to the walker, (7.61) tells us how the number of m-step unrestricted walks
with beginning and end-points ﬁxed, (2d)m/N, is modiﬁed as a result of self-
avoidance.
A bit of interpretation is in order at this point. The quantity (2d)m/N is the
total number of paths available to an unrestricted walker divided by the number
of available sites in the volume. Now, if the natural linear dimension of the walk
were small enough so that it ﬁts into the volume V comfortably, then the number of
unrestricted walks between two points in the volume would be given by a Gaussian
formula. In the case at hand, we have a path sum that is independent of the starting
and ending point, so that requiring it to end at a given location simply reduces the
total number of paths by the ratio of the number of sites at the given location (one)
to the total number of sites at which the walk can terminate. This result follows from
the presumption that the walker would naturally ﬁll a much larger volume than the
one available to it. By our choice of ψ0 we assume that the boundary conditions are
periodic, so the walker stays in the volume by reappearing at the far side whenever
it executes an exit. If the walk is a very long one, then the probability that it ends
up somewhere in the volume is independent of both its point of origin and its ﬁnal
resting place. This is because after many exits and re-entries it has lost memory of
where it started. Therefore, our result is, strictly speaking, relevant only to highly
“compressed” walks.
The Boltzmann-like weighting factor incorporates the average effects of the
interactions that enforce self-avoidance. Indeed, such a factor plays a key role in
arguments put forth by Flory for the dependence on the number of units of the
spatial extent of a randomly coiled linear polymer. According to these arguments,
this factor favors an extended polymer, in that it increases as the linear extent of
the polymer coil, R, grows. On the other hand, an “entropic” factor, based on the
expression for C(m; ⃗x, ⃗y) for the unrestricted walk favors a more tightly coiled

192
Path integrals and self-avoidance
polymer. This heuristic expression is
e−R2/m
(7.62)
Combining (7.62) with (7.61) one obtains as the weight for an N-step self-avoiding
walk of net extension R
e−R2/m−um2/Rd
(7.63)
This weight is maximized when the exponent is at a maximum, which occurs when
R ∝m3/(d+2)
(7.64)
As the result of a patching together of mean ﬁeld results for the interaction between
monomers insuring self-avoidance and expressions applying to unrestricted walks,
Flory was able to derive a simple, but extraordinarily accurate, scaling relationship
between the size of a polymer and the number of monomeric units of which it is
comprised (Flory, 1969).

8
Properties of the random walk: introduction to scaling
We now have had an introduction to the random walk, and there has been a dis-
cussion of some of the most useful methods that can be utilized in the analysis
of the process. The unifying theme of this chapter is the introduction of scaling
arguments to provide insights into the behavior of the walker under a variety of
circumstances. First, we will address in more depth the notion of universality of
ordinary random walk statistics. Then, we will discuss the (mathematical) sources
of non-Gaussian statistics. Finally, we will develop a few simple but central scaling
results by looking once more at the path integral formulation of the random walk.
In particular, using simple scaling arguments, we will be able to provide heuristic
arguments leading to Flory’s formula for the inﬂuence of self-avoidance on the
spatial extent of a random walker’s path.
8.1 Universality
Notions of universality play an important role in discussions of the statistics of the
random walk. We have already seen a version of universality in Chapter 2, in which
it was shown that the statistics of a long walk are Gaussian, regardless of details
of the rules governing the walk, as long as the individual steps taken by the walker
do not carry it too far. In the remainder of this section, the idea of universality
will be developed in a way that will allow us to apply it to the random walk when
conditions leading to Gaussian behavior do not apply.
As it turns out, universality in random walk statistics has a close mathematical and
conceptual connection to fundamental properties of a system undergoing a special
type of phase transition. The justiﬁcation of universality in a thermodynamic setting
restsononeofthemostimportanttheoreticaldevelopmentsinlatetwentieth-century
physics: the renormalization group. The recognition of the connection between
random walk statistics and the statistical mechanics of continuous phase transitions
has allowed the very powerful calculational machinery of the renormalization to
193

194
Properties of the random walk: scaling
be applied to random walks, especially self-avoiding walks. In this chapter and in
chapters to follow, we will explore the way in which universality arises, how one
understands its basis, and how it is possible to calculate properties arising from its
inﬂuence on random walk statistics.
8.1.1 Non-Gaussian behavior
Recall the discussion in Chapter 2. In Section 2.1.4, we found that the leading order
contributiontothedistributionofpaths,intermsofend-to-enddistance,isGaussian,
when the paths are left by unrestricted walkers that have taken a sufﬁciently large
number of steps. This follows from the fact that the asymptotic distribution is
independent of higher order moments of the probability that individual steps are
of a given length. The principal determinant of the properties of the end-to-end
distribution is the second moment of the step length probability distribution, p(|⃗r|).
Higher order moments of this probability are not relevant to the asymptotic statistics
of the long random walk.
However, it may not be possible to carry out the above moment expansion to
higher than quadratic order. This is because integrals of the form

|⃗r|n p(|⃗r|) d⃗r
may or may not converge at the value of n in which we are interested. If the integrals
do begin diverging at some value of n, then our analysis of the asymptotic behavior
of C(N; ⃗x −⃗y) can become more complicated. For example, if the integral above
does not have a ﬁnite value when n = 4, the method we used to investigate the
effect on the asymptotic form of C(N; ⃗x −⃗y) of the quartic term in the expansion
of χ(⃗q) may not be useful, simply because there is no quartic term to consider.
Matters can be even more complicated. The moment expansion can fail at
quadratic order. This will occur if the walker takes arbitrarily long steps with a
probability that decays sufﬁciently slowly with the length of the step. Such a case
was discussed as a worked-out example in Chapter 2. Suppose, for example, that
p(|⃗r|) ∝|⃗r|−α
(8.1)
And that the exponent α is less than 5. Then, the integral

p(|⃗r|) |⃗r|2 d⃗r ∝

|⃗r|4−α d|⃗r|
(8.2)
does not converge, and we are no longer able to expand χ(⃗q) as we did. In this case,
the dependence of the function χ(⃗q) on its argument is singular. For example, if
p(|⃗r|) ∝|⃗r|−5+δ
(8.3)

8.1 Universality
195
then, for small |⃗q|,
χ(⃗q) ∝A −B|⃗q|2−δ
(8.4)
It is possible to perform the inverse Fourier transform of χ(⃗q)N, and, thus ﬁnd
C(N; ⃗x, ⃗y). The steps are similar to those taken in (2.17)–(2.19). Now, the integral
needed, C(N; ⃗x −⃗y), is easily demonstrated to behave asymptotically as

ei⃗q·(⃗x−⃗y)ANe−N(B/A)|⃗q|2−δ d⃗q ∝|⃗x −⃗y|−5+δ
(8.5)
The Gaussian form no longer holds. We will have a good deal to say about the
different forms that functions such as C(N; ⃗x −⃗y) take on as the rules governing
the random walker are changed.
The slightly frustrated reader will have noticed that we have not indicated how
the results on the right hand sides of (8.4) and (8.5) were obtained. The general
strategy for performing the sorts of integrals that lead to the results above will be
outlined in the next section. By way of introduction to the overall approach, we
note that one way to ﬁnd the r-dependence of the integral

eikrky dk
(8.6)
is to replace the integration variable k by w = k/r. One is then left with the ex-
pression
r−(y+1)

eiwwy dw
(8.7)
We now replace the integral in (8.7) by a constant. Note that the limits of integration
have been suppressed in both (8.6) and (8.7). The contributions associated with the
limits of integration will undoubtedly have some dependence on r. However, we
assume that this does not alter our conclusions. If this is, indeed, the case, the
dominant dependence of (8.7) on r is contained in the factor r−(y+1).
Exercise 8.1
Using the strategy described above for working out the r-dependence from the
Fourier transform, verify that (8.5) follows from (8.4).
8.1.2 Analytic structure, scaling, and the role of spatial dimensionality
In this section we learn a little more about the dependence of the generating function
on the variable z. As we will soon see, the number of dimensions in which the
random walk takes place has an important effect on the way in which it depends on

196
Properties of the random walk: scaling
z, and, most decisively, on the way it behaves when z approaches a critical value,
zc, which turns out to be the coordination number of the lattice. This, in turn, exerts
a profound effect on the statistics of long random walks.
How does the generating function depend on the dimensionality of the space
explored by the walker? We start with the expression for the function g(z; ⃗q)
g(z; ⃗q) =
1
1 −zχ(⃗q)
where
χ(⃗q) = a −b|⃗q|2 + O

|⃗q|4
If we make use of the above form for χ(⃗q), truncating beyond quadratic order in ⃗q,
we have
g(z; ⃗q) ≈
1
1 −az + zb|⃗q|2
(8.8a)
= 1
a
1
zc −z + zb/a|⃗q|2
(8.8b)
≈1
a
1
zc −z + b/a2|⃗q|2
(8.8c)
(8.8c) holds when z ≈zc = 1/a = 1/2d, if the lattice is cubic. The quantity zc
can be termed the critical fugacity. The approximate equality above is sufﬁciently
accurate for our purposes in the regimes of interest to us – in particular, when the
walks consist of a large number of steps.
The result above tells us that the generating function g(z; ⃗q) has a scaling form.
That is, we can write
g(z; ⃗q) = 1
a
1
zc −z + b/a2|⃗q|2
= 1
a
1
|⃗q|2
1
b/a2 + (zc −z)|⃗q|−2
≡|⃗q|−2 f

⃗q(zc −z)−1/2
(8.9)
A generalization of the above scaling form is
g(z; ⃗q) = |⃗q|−2+η f

⃗q(zc −z)−ν
(8.10)
The introduction of the exponents ν and η, which, in the case at hand, are equal
to 1/2 and 0, respectively is not frivolous. As we will soon see, restrictions on the
random walk, such as a prohibition against intersections in the trail left by the walker
– the constraint that deﬁnes a self-avoiding walk – can have a substantial effect on
the generating function, which is, in part, summarized by the generalization of the

8.1 Universality
197
scaling form displayed in (8.10). The consequences of the scaling form above will
be discussed at length in a subsequent chapter.
For the time being, let’s see what the scaling form for g(z; ⃗q) in (8.10) implies for
the generating function in real space. Instead of undertaking an honest calculation
we’ll make use of some simple tricks to produce the general form. We start with
the relationship between G(z; ⃗r) and g(z; ⃗q):
G(z; ⃗r) =

g(z; ⃗q)ei⃗q·⃗r ddq
(8.11)
We then have
G(z; ⃗r) =

|⃗q|−2+η f

⃗q(zc −z)−ν
ei⃗q·⃗r ddq
(8.12)
Replacing ⃗q by ⃗κ/|⃗r| in the integral, we are left with
G(z; ⃗r) = |⃗r|−d+2−η

|⃗κ|−2+η f

⃗κ/

|⃗r|(zc −z)ν
ei⃗κ·⃗r/|⃗r| ddκ
= |⃗r|−d+2−ηF

|⃗r|(zc −z)ν
(8.13)
where
F(x) ≡

|⃗κ|−2+η f

⃗κ/|⃗x|

ei⃗κ·⃗x/|⃗x| ddκ
(8.14)
As in the previous section, we have neglected the effect limits have on the quantities
deﬁned in terms of them.
The generating function has the same kind of scaling form in real space as it has
as a function of wave number. Because of the short cuts we have taken in getting to
(8.14) we do not yet know much about what the generating function actually looks
like in real space. For the time being, we will make do with assertions concerning
the limiting behavior of F(x). They are:
(1) as its argument goes to zero, F(x) approaches a ﬁnite constant;
(2) the function F(x) decays exponentially to zero as |x| goes to inﬁnity.
We will now make use of the properties of the generating function that we have
just discussed to rederive the result that long, unrestricted random walks obey
Gaussian statistics. This useful exercise allows us to test the consequences of the
various scaling conjectures that have just been advanced, and it also offers us
a chance to practice the application of the steepest-descents approximation as a
method for extracting the coefﬁcient of a high order term in a power series. As an
added beneﬁt, we will see that when the exponents in the scaling form differ from
those that apply to the unrestricted walk – as is the case for self-avoiding random
walks – then the statistics of long random walks are not Gaussian.

198
Properties of the random walk: scaling
Steepest-descents evaluation of an integral
The coefﬁcient of zN in the power-series expansion of
G(z; ⃗r) = |⃗r|−d+2−ηF

|⃗r|(zc −z)ν
is given by the Cauchy formula
1
2πi
 G(z; ⃗r)
zN+1 dz
(8.15)
where the contour encircles the origin. By now the reader should be familiar with
this formula or the method of steepest descents described below. If this is not the
case, the supplemental notes at the end of Chapter 1 may be of some help. For large
N the steepest-descents method derives that coefﬁcient as the function
|⃗r|−d+2−ηF

|⃗r|(zc −z)ν
z−(N+1)
(8.16)
where the fugacity z is adjusted to extremize the above expression. To perform this
mathematical operation, we will ﬁrst exponentiate all z-dependent contributions to
the expression and then ﬁnd the extremum of the exponent. The task at hand is thus
to ﬁnd the maximum, or the minimum, value of
ln

F

|⃗r|(zc −z)ν
−(N + 1) ln z ≡A

|⃗r|(zc −z)ν
−(N + 1) ln z
(8.17)
The limiting behavior of the function F(x) as described above implies that the
function A(x) ≡ln (F(x)) goes to a constant as x →0, and that, as x →∞, A(x)
decreases as a linear function of x, i.e. A(x) = −Bx when x ≫1. Then, the ex-
tremum equation is
ν|⃗r|(zc −z)ν−1A′ 
|⃗r|(zc −z)ν
−(N + 1)/z = 0
(8.18)
To obtain the solution to this equation when N ≫1 we anticipate that the argument
|⃗r|(zc −z)ν will be very large, while, at the same time, z ≈zc. Then the slope of
A

|⃗r|(zc −z)ν
will be constant and negative (≡−B), and we can write z = zc −δ,
where the deviation of z from zc is small. The solution to (8.18) is
δ ≈
 N + 1
νBzc|⃗r|
	1/(ν−1)
(8.19)
Substituting this solution for δ = zc −z into the right hand side of (8.17) we have
C(N; ⃗r) ≈exp

−B|⃗r|δν −(N + 1)/zc

∝exp

−((νzc)ν B)1/(1−ν)(N + 1)−ν/(1−ν)|⃗r|1/(1−ν)
(8.20)

8.1 Universality
199
Exercise 8.2
Complete the steps leading to (8.20).
Exercise 8.3
Use (8.10) and additional assumptions about the structure of the function f in that
equation to show that the generating function for the total number of walks goes
as (zc −z)−γ , where γ = (2 −η)ν. Make clear all the assumptions that go into
the derivation of this result.
Exercise 8.4
Make use of the result of Exercise 8.3 to show that the total number of N-step
walks governed by the generating function in (8.10) is proportional to x N N p.
What are the values of the number, x, and the power, p?
Exercise 8.5
Here, we are going to work backwards from known results for self-avoiding walks.
It turns out that the number of N-step self-avoiding walks that end a distance r
away from their starting point go as z−N
c
N −dν, where ν is the critical exponent in
the equations above. What does this tell us about the way in which the function F
in (8.13) behaves? How does the number of such walks depend on the distance, r?
Since the exponent ν is always positive and less than one, it is relatively easy to
verify that the two assumptions leading to (8.20) indeed hold, as long as |⃗r| ∼N ν.
Setting ν = 1/2, the value this exponent takes for the kind of random walks that
we have studied so far, we regain a Gaussian form for C(N; ⃗r). Note, however, that
when the exponent ν has a value different from 1/2 the Gaussian form implied by
the central limit theorem does not follow.
8.1.3 Scaling and dimensionality: rederivation of Flory’s exponent
In Chapter 7 we demonstrated how the random walk process can be recast as a
sum over all the allowed paths of the walker. If the walker is forbidden to cross
the path it has already left, then an appropriate interaction term in the path integral
formulation can be introduced to eliminate such paths.
Recall the functional integral version of the expression for the total number of
N-step walks from ⃗x to ⃗y
C(N; ⃗x, ⃗y) ∝

s

d⃗r(s)exp

−3
2a2
 N
0

d⃗r
ds

2
ds
−v0
2
 N
0
 N
0
ds1 ds2 δ

⃗r(s1) −⃗r(s2)


(8.21)

200
Properties of the random walk: scaling
We employ the following scaling transformations.1 Suppose we redeﬁne the arc
length variable s and the position vector ⃗r(s) by effecting two rescalings. The arc
length goes to s′ = l2s while ⃗r(s) →⃗r ′(s′) = l1⃗r(l2s). The path integral on the right
hand side of (8.21) can be re-expressed in terms of these new variables. The new
relationship between C(N; ⃗x, ⃗y) and a functional integral is
C(N; ⃗x, ⃗y) ∝

s′

d⃗r ′(s′)exp

−3
2a2
l2
l2
1
 l2N
0

d⃗r ′
ds′

2
ds′
−v0
2
ld
1
l2
2
 l2N
0
 l2N
0
ds′
1 ds′
2 δ

⃗r ′(s′
1) −⃗r ′(s′
2)


(8.22)
The factor ld
1 multiplying the double integral in the exponent on the right hand side
of (8.22) arises because δ(ax) = 1
aδ(x), and because δ(⃗r) = δ(x1)δ(x2) . . . δ(xd),
where x1 . . . xd are the components of the vector ⃗r.
The right hand side of (8.22) looks like the functional integral representation of
a random walk consisting of a different number of steps – and for which the length
is measured with respect to a new scale. There are a few additional differences
between the new and the old random walk. Each of the integrals in the exponent
is now multiplied by an additional factor. The integral over |d⃗r/ds|2 has the new
factor l2/l2
1. In the case of the double integral the factor is ld
1/l2
2.
Exercise 8.6
By deﬁning a new length a′ = al1/√l2, show that the self-avoidance term, rep-
resented by the double integral in (8.22), is rescaled by a factor ld
1/l2. Show
that a redeﬁnition of l1 in terms of the factor l2 that keeps the ﬁrst term in the
double integral constant replaces the factor multiplying the self-avoidance term
by l(d−4)/2
2
After rescaling the length as prescribed in Exercise 8.6, we are left with an
expression for the number of N-step random walks that take the walker a distance
|⃗R| = |⃗y −⃗x| that looks like the expression for the number of steps of an N ′-step
walk that takes the walker a distance |⃗R ′|. The relationship between the primed and
the unprimed quantities is N ′ = l2N and ⃗R ′ = l1/2
2
⃗R. If the rescaling factor l2 is
less than one, we are able to reduce the problem of an N-step walk to the problem
of a walk with fewer than N steps.
1 First suggested by Kosmas and Freed (1978).

8.1 Universality
201
Exercise 8.7
In Exercise 8.6, take the extreme step of setting l2 equal to 1/N. In this case, the
“reduced” walk consists of exactly one step. What is the length of that step? What
does this tell us about the distance away from the origin that a walker travels in N
steps, assuming that self-avoidance plays no role? Now, look at the self-avoidance
contribution to the multiple integral in (8.22). How does the amplitude of this
term change? In particular at what spatial dimensionality does the importance of
self-avoidance remain unchanged as the rescaling described here is carried out?
This dimensionality is known as the upper critical dimensionality. Show that when
the dimensionality, d, is smaller than the upper critical dimensionality the im-
portance of self-avoidance is greatly enhanced as rescaling is implemented, while
above the upper critical dimensionality self-avoidance reduces in signiﬁcance under
this type of rescaling.
Exercise 8.8
Suppose the interaction leading to self-avoidance has a power-law form, so that the
generating function for self-avoiding walks is given by
C(N; ⃗x, ⃗y)
∝

s

d⃗r(s) exp

−3
2a2
 N
0

d⃗r
ds

2
ds −v0
2
 N
0
 N
0
ds1 ds2 |⃗r(s1) −⃗r(s2)|−q

where the power q is positive. Perform the same kind of scaling analysis as was
done in the case of delta-function-like self-avoidance and ﬁnd the upper critical
dimension of this system. How does this dimensionality depend on the power q?
For the moment, we will leave rigor in the dust and construct another heuristic
derivation of Flory’s exponent that relates the average size of a self-avoiding walk
to the number of steps in it (Flory, 1953). Recall that this exponent ν = 3/(d + 2)
enters into the relationship between walk size, R and walk length, N, as follows:
R ∝N ν
(8.23)
Now, if we look once again at the rescaled version of the functional integral, as
displayed in (8.22), we notice that the relative inﬂuence of the two integrals in the
exponent changes under rescaling by the factors l1 and l2. Previously, we adjusted
the rescaling factor l1 so as to maintain the amplitude of the ﬁrst integral. Suppose,
on the other hand, that we ﬁxed l1 so that the relative amplitude of the two integrals
remains unchanged under rescaling. In order for this to be the case, the following

202
Properties of the random walk: scaling
equality must clearly hold:
l2
l2
1
= ld
1
l2
2
(8.24)
or
l1 = l3/(d+2)
2
(8.25)
Given this relationship between l1 and l2 we can duplicate the discussion at the end
of Section 8.1.2 and claim that the mean distance traveled by the N-step walk is
equal to the unit distance traveled by the “reduced” walk multiplied by the factor
l−1
1
= l−3/(d+2)
2
→N 3/(d+2). Flory’s result is thus recovered.
There are, of course, difﬁculties with this set of arguments. Flory’s result is not
exact, so the development leading to it cannot be entirely correct. There’s not all
that much to be gained in addressing the subtleties of scaling at this point. Such a
discussion is more proﬁtably deferred until a ﬁrmer foundation has been laid, and
we are now ready to embark on that project.

9
Scaling of walks and critical phenomena
9.1 Scaling and the random walk
The previous chapters contain a variety of arguments in support of the conclusion
that the mean square end-to-end distance of an N-step random walk obeys the
following power-law relationship
⟨R2⟩∝N p
(9.1)
In the case of unrestricted walks, in which there is no penalty associated with
a trail that crosses itself, the power p is equal to one. When self-avoidance in
introduced, p deviates from this value. Flory used what amounted to an energy–
entropy argument (restated in slightly altered form at the end of Chapter 7) to derive
the dimensionality-dependent result
p =
6
d + 2
(9.2)
This formula turns out to be exact in four, two and one dimensions, and p as given
by the above expression is within a percent or so of the correct exponent when the
spatial dimensionality, d, is three.
Given the remarkable accuracy of Flory’s formula, the conclusion that the theo-
retical arguments mustered to justify (9.2) contain the seeds of a rigorous theoretical
model of the self-avoiding random walk seems inescapable. While Flory’s expres-
sion for the exponent p, (9.2), is clearly not exact, given the small but non-zero
discrepancy in d = 3 dimensions, the general form of the power-law relationship
in (9.1) holds quite generally. In fact, there is a set of relations for the self-avoiding
walk that mirror the power-laws characterizing the thermodynamic properties of
systems in the vicinity of a critical point. Pierre-Giles de Gennes was the ﬁrst to
point out that if one writes p = 2ν, so that ⟨R2⟩∝N 2ν, the quantity ν can be iden-
tiﬁed with one of the critical exponents of a ﬁctitious magnetic system (de Gennes,
1972). In particular, he demonstrated that the statistics of the self-avoiding random
203

204
Scaling of walks and critical phenomena
walk derive from the equilibrium statistical mechanics of an n-component spin
system with ferromagnetic interactions. The magnetic system is known as the O(n)
model, and correspondence with the random walk problem is achieved in the limit
n = 0. This suggestion by de Gennes set the stage for a quantum leap in our ability
to obtain information about self-avoiding random walks. Investigators were pro-
vided access to an arsenal of new and powerful analytical tools, which were brought
to bear on the problem. A variety of quantities can be calculated by appealing to
the connection between the self-avoiding walk and this spin system. A detailed
demonstration of the connection between the statistics of the self-avoiding walk
and the statistical mechanics of the n-component spin system in the limit n = 0 is
contained in Section 9.5.
Before embarking on an exploration of the properties of the self-avoiding walk
as revealed in the behavior of the n = 0 limit of the O(n) model, it is useful to
review the general subject of phase transitions and critical points, so as to establish
the general framework in which the phenomena will be described.
9.2 Critical points, scaling, and broken symmetries
The notion of scaling is intimately connected with the behavior of systems in the
vicinity of critical points. Because of this, many scaling concepts are framed in
the language of critical phenomena. In order to efﬁciently exploit the theoretical
advances that have led to a fuller understanding of the properties of random walks
it is useful to establish a working knowledge of the vocabulary of phase transitions.
This section consists of a review of phase transitions and critical phenomena, with
special (essentially exclusive) reference to a set of magnetic systems in which the
critical point also marks the onset of a symmetry-breaking phase transition.
As an example of the type of transition we will be talking about, consider what
happens to a typical insulating ferromagnetic system. This system consists of a
set of magnetic moments attached to atoms that are ﬁxed in space. Imagine that
the atoms are located at the sites of some lattice and they possess a net magnetic
moment. The spins on neighboring sites interact in such a way that there is an
energetic tendency of those moments to align. At T = 0 K, all moments will align.
In the absence of the disruptive effects of thermal ﬂuctuations, the system is a
permanent magnet.
Suppose the magnetic system is placed in contact with a heat bath at some ﬁnite
temperature T . If T is sufﬁciently high the moments ﬂuctuate randomly, and there is
no net magnetization. This state of affairs persists as the temperature is decreased
until a critical temperature, Tc, is reached.1 As T passes through Tc, the system
1 In the case of ferromagnets, the critical temperature is also called the Curie temperature.

9.2 Critical points, scaling, and broken symmetries
205
Ms(T) 
T
Tc
Fig. 9.1. The spontaneous magnetization of a ferromagnet as a function of
temperature.
reacquires a net magnetization. This magnetization, called the spontaneous mag-
netization because no external ﬁelds are needed to maintain it, obeys the following
power-law at temperatures sufﬁciently close to the critical temperature
Ms ∝(Tc −T )β
(9.3)
with β ≈1/3. A plot of the spontaneous magnetization, Ms, is displayed in
Figure 9.1. The exponent β is known as a critical exponent. There are many others.
The noteworthy feature of the relationship governed by (9.3) is that it describes
the temperature dependence of the spontaneous magnetization for a whole class
of ferromagnets. Each member of this class will have a remanent magnetization
describedby(9.3),andforallthemembersofthisclass,thevalueoftheexponentβ is
the same. This feature is one of the ways in which the transition to the ferromagnetic
state exhibits universal behavior.
9.2.1 The O(n) model
The system of interest here is a version of the insulating ferromagnet. The atoms
carrying the magnetic moments are, as previously noted, ﬁxed in space. The mo-
ments rotate in an n-dimensional “internal” space. Such a system is known as the
O(n) model. One standard realization of this system is the so-called Heisenberg
ferromagnet, in which the moments are carried on atoms that are ﬁxed in place on a
crystalline lattice. In this case, the spins are three-component vectors, so the dimen-
sionality of the internal space is the same as the spatial dimensionality inhabited by
the atoms. The O(3) model applies to this system. The set of moments may have
an “easy plane,” to which the moments are effectively conﬁned. In that case n = 2.
This model is also called the xy model, in reference to the two cartesian coordinates
that span such a plane. There are magnetic realizations of this model, but the most
widely studied system to which the O(2) model applies is the superconductor or the
superﬂuid. In the case of such a system, the two components of the “magnetization”
are the real and imaginary parts of a macroscopic wavefunction.

206
Scaling of walks and critical phenomena
Instead of an easy plane, there may be an “easy axis,” which means the moments
tend to point in one direction or another along a one-dimensional subspace. The
version of the model that applies has n = 1. A number of magnetic materials are
well-described in terms of the O(1) model, but the most famous realization is the
theoretical construct called the Ising model. Systems exist in which the internal
subspace is more than three dimensional, and in that case the O(n > 3) model may
be invoked.
Finally, there are the instances in which the dimensionality of the internal space
is less than one. For example, consider n = 0. While its analysis may seem to be
an exercise in absurdity, the O(n = 0) model will soon emerge as the target of our
investigations.
9.2.2 Symmetries, respected and broken
There is more to the O(n) model than the dimensionality of the internal space. A
complete speciﬁcation of its properties for the purposes of a study of its equilibrium
thermal behavior must also include a description of its symmetries. The O(n) model
is symmetric under a simultaneous rotation of all the moments. This symmetry
restricts the dependence of the energy of the model on the conﬁgurations of the
moments. If we denote by the n-dimensional vector ⃗Si the orientation and amplitude
of the ith moment, the energy of interaction of the moments has the form
Hint

⃗S

= −J

i, j
⃗Si · ⃗S j
(9.4)
The coupling constant J quantiﬁes the strength of the interaction between nearest
neighboring atoms i and j. The negative sign on the right hand side of (9.4) indicates
that the interaction is “ferromagnetic,” in that the energy of a pair of interacting
moments is lowest when they point in the same direction.
Recall that the scalar, or dot product, ⃗A · ⃗B, of the two vectors ⃗A and ⃗B is equal
to the magnitude of one of the vectors multiplied by the projection of the other
one onto it, and that this combination depends only on their individual amplitudes
and the angle between them. In particular, the scalar product does not change if
both vectors rotate simultaneously through the same angle. Thus, the value of the
expression on the right hand side of (9.4) remains the same if all the moments
simultanenously rotate through the same angle. In this sense the set of moments
exhibits a rotational symmetry.
Now,thereisaperfectlyplausibleargumentthatleadsinevitablytotheconclusion
that the set of moments will not, on the average point in any particular direction,
at any non-zero temperature. This argument follows from one of the tenets of
statistical mechanics, the ergodic hypothesis, and the rotational symmetry of the

9.2 Critical points, scaling, and broken symmetries
207
set of moments. The ergodic hypothesis, the (unproven) linchpin of equilibrium
statistical mechanics, asserts that a system will sample, in an absolutely unbiased
fashion, all the conﬁgurations accessible to it. Given the fact that no state of the
set of moments in an O(n) system is distinguishable, in terms of its interaction
energy, from a state related to it by a simultaneous rotation of all of its moments,
there cannot be an overall preference for a state in which the moments point in any
particular direction. More quantitatively, if we deﬁne the net magnetization, ⃗M, of
the set of moments as
⃗M =

i
⃗Si
(9.5)
then, for any state in which net magnetization has a particular value, there are states
having exactly the same energy in which the value of ⃗M is related to the the net mag-
netization of the original state by a simple rotation. These states are sampled with
the same frequency as the one mentioned above. This unbiased sampling of states
with ⃗M’s pointing in all possible directions leads to an average net magnetization
that equals zero.
We thus arrive at the conclusion that, because of the O(n) model’s rotational
symmetry, its average magnetization must be zero.
The above argument is compelling, but, as we’ve already noted, it cannot rep-
resent the last word on the subject. At sufﬁciently low temperatures, systems that
possess the symmetry embodied in the interaction energy (9.4) may well violate the
conclusion above. For example, the average net magnetization of the O(n) model
does not necessarily equal zero. When the temperature T is low enough, an ar-
gument based on the ergodic hypothesis ceases to hold and ⟨⃗M⟩̸= 0. There is, in
fact a symmetry-breaking phase transition as the temperature decreases to a value
lower than the critical temperature, Tc, of the system, from a high-temperature,
symmetry-respecting, “paramagnetic” phase to a low-temperature, broken symme-
try, “ferromagnetic” phase.
It is worthwhile to review in a bit more detail how the magnetic system deﬁes
the ergodic hypothesis in its low-temperature phase. In fact, the ergodic hypothesis
holds at all temperatures. Left to itself the magnet will eventually sample all con-
ﬁgurations. If it is allowed to carry out this sampling long enough, it will produce
a time-averaged magnetization that is equal to zero. The real question is how long
this averaging-out will take. As it turns out, a system in the ferromagnetic phase
ﬂuctuates through the set of states available to it at such a slow rate that no rea-
sonable observation will yield the result ⟨⃗M⟩= 0 that follows from symmetry and
ergodicity. The time scales involved are, literally, astronomical. Furthermore, they
grow in magnitude with the size of the system. In the case of a magnet contain-
ing Avogadro’s number of atoms, the projected lifetime of our universe is not long

208
Scaling of walks and critical phenomena
enough to wait. When the size of the system is inﬁnite (the so-called thermodynamic
limit), one has to wait literally forever to observe a restoration of ergodicity.
The full Hamiltonian
To facilitate a detailed investigation of the properties of the O(n) model it proves
convenient to introduce a term in the energy that explicitly breaks the rotational
symmetry. This term looks like the “Zeeman” energy of interaction between a
magnetic moment and the magnetic ﬁeld in which it ﬁnds itself. Introducing the
symmetry-breaking ﬁeld ⃗h, the new contribution to the energy of the collection of
magnetic moments is, then
−⃗h · ⃗M = −⃗h ·

i
⃗Si
(9.6)
and the total energy of the set of moments is
H

⃗S

, J, ⃗h

= −J

i, j
⃗Si · ⃗S j −⃗h ·

i
⃗Si
(9.7)
The (canonical) partition function of this system, from which all thermodynamic
properties derive, is given by
Z

T, ⃗h

=

Conf.
e
−H

⃗S

,J,⃗h

/kBT
(9.8)
The quantity kB is Boltzmann’s constant and T is the absolute temperature. The
sum 
Conf. in (9.8) is over all conﬁgurations of the moments, Si. The partition
function yields the magnetic Gibbs free energy of the system, F(T, ⃗h), through the
relationship
F(T, ⃗h) = −kBT ln

Z

T, ⃗h

(9.9)
The exponential in (9.8) is instrumental in the evaluation of thermal averages.
Since e−H/kBT /Z is the probability of realizing the conﬁguration {⃗S} for the system
in contact with a heat bath at temperature T , it follows that the mean value of a
quantity X[{⃗S}] depending on the conﬁguration of the moments is given by
	
X

⃗S

= 1
Z

Conf.
X

⃗S

e
−H

⃗S

,J,⃗h

/kBT
(9.10)
Phase diagram and power law behavior of thermodynamic functions
The existence of the ferromagnetic phase is made manifest in the phase diagram de-
picted in Figure 9.2. The thick horizontal line running from the origin (h = T = 0)
to the critical point at h = 0, T = Tc is a line of ﬁrst order phase transitions, in that

9.2 Critical points, scaling, and broken symmetries
209
h
T
Tc
Fig. 9.2. The phase diagram of the ferromagnetic system. The thick line on the
h = 0 axis extending from T = Tc to the origin is a line of ﬁrst order phase
transitions.
the magnetization changes discontinously as the symmetry-breaking ﬁeld traverses
the abscissa when T < Tc. According to the classiﬁcation of phase transitions by
order, a ﬁrst order phase transition is one in which ﬁrst derivatives of the thermody-
namic potential suffer discontinuities, and the average net magnetization is equal
to the derivative of the magnetic Gibbs free energy, F(T, ⃗h) with respect to ⃗h as
follows:
⟨⃗M⟩= −

∂F(T, ⃗h)
∂⃗h

T
(9.11)
The derivative ∂/∂⃗h in (9.11) is shorthand for the gradient with respect to h. Another
way to write it is as ⃗∇h = ⃗i∂/∂hx + ⃗j∂/∂hy + ⃗k∂/∂hz.
The critical point terminates the line of ﬁrst order transitions. It is in the vicinity of
this point that thermodynamic functions exhibit power-law behavior. For example,
the isothermal susceptibility of the system, deﬁned by
χT =

∂⟨⃗M⟩
∂⃗h

T
= −

∂2F(T, ⃗h)
∂⃗h2

T
(9.12)
has the following temperature dependence
χT ∝|Tc −T |−γ

T ≈Tc, ⃗h = 0

(9.13)
The two-point correlation function:
C(T, ⃗h; ⃗ρ) = ⟨⃗S(⃗R)⃗S(⃗R + ⃗ρ)⟩
(9.14)
also exhibits scaling behavior:
C(T, ⃗h; ⃗ρ) ∝
1
|⃗ρ|d−2+η f
|⃗ρ| |T −Tc|ν
≡
1
|⃗ρ|d−2+η f
|⃗ρ| /ξ

(9.15)

210
Scaling of walks and critical phenomena
Table 9.1. Set of thermodynamic exponents.
3D value
Quantity
Behavior
Mean ﬁeld value
O(n = 0)
Speciﬁc heat
ch =
 ∂Q
∂T

h
∝|T −Tc|−α
α = 0
α = 0.12
T > Tc, h = 0
Speciﬁc heat
ch =
 ∂Q
∂T

h
∝|T −Tc|−α′
α′ = 0
α′ = 0.12
T < Tc, h = 0
Magnetization
M(T ≤Tc, h = 0)
∝|Tc −T |β
β = 1/2
β = 0.308
Susceptibility
χT =
 ∂M
∂h

T
∝|T −Tc|−γ
γ = 1
γ = 1.19
h = 0, T > Tc
Susceptibility
undeﬁned
χT =
 ∂M
∂h

T
∝|T −Tc|−γ ′
γ ′ = 1
(χT = ∞)
h = 0, T < Tc
Magnetization
M(T = Tc, h)
∝|h|1/δ
δ = 3
δ = 4.865
Correlation length
ξ 2 =

C(T,⃗h;⃗ρ)ρ2 ddρ

C(T,⃗h;⃗ρ) ddρ
∝|T −Tc|−ν
ν = 1/2
ν = 0.602
T > Tc , h = 0
Correlation length
ξ =

C(T,⃗h;⃗ρ)ρ2 ddρ

C(T,⃗h;⃗ρ) ddρ
∝|T −Tc|−ν′
ν′ = 1/2
ν′ = 0.602
T < Tc , h = 0
Anomalous dimension
C(T = Tc, h = 0, ⃗ρ)
∝ρ−d+2−η
η = 0
η = 0.023
Equation (9.15) contains the deﬁnition of the correlation length, ξ. This important
distance scale plays a fundamental role in the physics of the critical point. In fact, it
is the divergence of this quantity as T →Tc that gives rise to much of the interesting
behavior of the thermodynamic functions at and close to the critical point.
The full set of thermodynamic exponents is displayed in Table 9.1, along with
two key exponents for the behavior of the correlation function.
As noted in Table 9.1, the “3D” exponents are those of a three-dimensional O(n)
system in the limit n = 0.
By and large, when two exponents are utilized to describe the behavior of a
quantity above and below the critical temperature, those two exponents turn out to

9.2 Critical points, scaling, and broken symmetries
211
be equal. However, there are instances, exempliﬁed by the case of the exponents γ
and γ ′, in which the physics of the system rules out equality of the “mirror image”
exponents. The general principle that there is a form of reﬂection symmetry about
the critical point follows directly from the physics underlying critical phenomena.
Equality of γ and γ ′ is ruled out in the case of interest to us by the fact that the
susceptibility of an O(n) system is, in general, inﬁnite when T < Tc and h = 0.
This follows from the existence of gapless spin waves and is a consequence of a
Coulomb-law-like “correlation hole” in that regime. Both of these phenomena will
be discussed at greater length in Chapter 10.
9.2.3 Scaling laws
As it turns out, the thermodynamic exponents satisfy a number of relationships.
These relationships have acquired the generic name of scaling laws. An example
connects the three exponents α, β and γ :
α + 2β + γ = 2
(9.16)
This relation, known as the Rushbrooke scaling law, can be shown to hold for the
mean ﬁeld exponents displayed in Table 9.1, since
α + 2β + γ = 0 + 2 × 1
2 + 1 = 2
Another example of a scaling law is a “reﬂection symmetry” law, such as α = α′,
or ν = ν′, or (when appropriate) γ = γ ′.
There are also scaling laws that connect thermodynamic exponents with expo-
nents for the correlation function. A very important example is
γ = ν(2 −η)
(9.17)
Finally, a set of mathematical connections transcends the standard relations em-
bodied in the scaling laws. These relationships are known as hyperscaling laws.
Two key examples of this class are
α = 2 −dν
(9.18)
δ = d + 2 −η
d −2 + η
(9.19)
A crucial characteristic of a hyperscaling relation is that it connects two, rather than
three exponents, and it contains the spatial dimensionality, d, of the system.

212
Scaling of walks and critical phenomena
Source of the thermodynamic scaling laws
The scaling law (9.16) and other laws relating thermodynamic exponents follow
from the fact that the magnetic Gibbs free energy of the critical system takes on an
asymptotic form, called a generalized homogeneous form. When T ≈Tc and ⃗h ≈0,
the Gibbs free energy acquires the following dependence on the thermodynamic
parameters.
F(T, ⃗h) →|r|2−α f

|r| h−2+α+β, r
|r|

(9.20)
In the above, the quantity r, called the reduced temperature, is given in terms of
the absolute temperature by
r = T −Tc
Tc
.
(9.21)
Note that the last argument in the expression on the right hand side of (9.20) is
just the sign of the reduced temperature. This means that the function f is actually
two functions: one relevant to r > 0 (T > Tc) and the other appropriate to r < 0
(T < Tc).
The speciﬁc heat at constant ordering ﬁeld is given, in terms of the above ther-
modynamic functions, by
ch = T
 ∂S
∂T

⃗h
= −T
∂2F
∂T 2

⃗h
→−∂2
∂r2 |r|2−α f

|r| h−2+α+β, r
|r|

∝|r|−α f

0, r
|r|

(9.22)
In the last line of (9.22), the limit ⃗h = 0 has been taken. Notice that the above
result implies a power-law for the speciﬁc heat. It also predicts equality for the
exponents α and α′. Utilizing relationships (9.11) and (9.12), along with (9.20),
we arrive at the exponent β for the spontaneous magnetization and the exponent
γ = 2 −α −2β for the isothermal susceptibility. Notice that the result for γ leads
directly to the scaling law (9.16).
The asymptotic form (9.20), ﬁrst introduced as a key to the behavior of a system
near its critical point by Widom (1965), gives rise to an extensive set of relations
between exponents. A comprehensive discussion of these relations is beyond the
scope of this book.

9.2 Critical points, scaling, and broken symmetries
213
Exercise 9.1
Given the following scaling form for the Gibbs free energy
G(T, h) = |r|2−α f
 h
|r| , r
|r|

(9.23)
show that
β = 2 −α −
γ =  −β
Exercise 9.2
Using(9.23)fortheGibbsfreeenergyshowthatat T = Tc (t = 0)themagnetization
behaves as
M ∝h1/δ
where β(δ −1) = γ .
Note that the last line of (9.15) implies a similar homogeneous form for the
correlation function in the vicinity of the critical point.
Relationships coupling thermodynamic and correlation function exponents
The second set of scaling laws expresses the fact that many equilibrium properties
reﬂect correlations in the system of interest. To see how this connection plays out
in the case of the O(n) model, we utilize (9.11), (9.9), (9.8), and (9.7) to derive a
result for the mean magnetization. Performing the derivative with respect to ⃗h,
⟨⃗M⟩= −∂
∂⃗h
F(T, ⃗h)
= kBT ∂
∂⃗h
ln

Conf.
exp

J
kBT

i, j
⃗Si · ⃗S j +
⃗h
kBT ·

i
⃗Si

=

Conf.

i ⃗Sie
−H

⃗S

,J,⃗h

/kBT
Z

T, ⃗h

=

i
⟨⃗Si⟩
(9.24)
Note that the last line of (9.24) duplicates (9.5), with the thermal average as set
forth in (9.10).

214
Scaling of walks and critical phenomena
One further differentiation with respect to the ordering ﬁeld ⃗h yields for the
isothermal suseptibility
χT = ∂
∂⃗h

Conf.

i ⃗Sie
−H

⃗S

,J,⃗h

/kBT
Z

T, ⃗h

=

Conf.

i, j ⃗Si ⃗S je
−H

⃗S

,J,⃗h

/kBT
Z

T, ⃗h

−

Conf.

i ⃗Sie
−H

⃗S

,J,⃗h

/kBT
Z

T, ⃗h

×

Conf.

j ⃗S je
−H

⃗S

,J,⃗h

/kBT
Z

T, ⃗h

=
1
kBT

i, j

⟨⃗Si ⃗S j⟩−⟨⃗Si⟩⟨⃗S j⟩

(9.25)
The last line of (9.25) is a sum over a generalized version of the correlation function
deﬁned in (9.14). This more general form deﬁnes a function with a vanishing
amplitude at large separation of the locations i and j under all conditions, while
the version displayed in (9.14) goes to a ﬁnite limit as ⃗r →∞when the average
value of the magnetization is non-zero.
Invoking the scaling form for the correlation function indicated in Table 9.1,
Equation (9.25) implies the following general result for the susceptibility, when the
system under consideration possesses translational symmetry
kBT χT =

i, j
C(T, ⃗h; ⃗Ri −⃗R j)
(9.26a)
= V

i
C(T, ⃗h; ⃗Ri)
(9.26b)
→

ddrC(T, ⃗h; ⃗R)
(9.26c)
∝

dd R
1
⃗R

d−2+η f
⃗R
 |T −Tc|ν
(9.26d)
The quantity V in (9.26b) is the volume of the system; translational invariance
has been assumed. In (9.26c) and (9.26d), the volume has been divided out. The
resulting quantities remain ﬁnite in the thermodynamic (V →∞) limit. (9.26d)
follows from the scaling form of the correlation function set forth in (9.15).

9.3 GLW effective Hamiltonian
215
As a ﬁnal step, we scale the quantity |T −Tc| out of the integral. Writing
⃗R = |T −Tc|−ν ⃗x, one obtains the result
χT ∝|T −Tc|−ν(2−η)

ddx f
|⃗x|
= K |T −Tc|−ν(2−η)
(9.27)
The scaling law (9.17) is thus derived. The constant K in the last line of (9.27) is
just the integral in the ﬁrst line.
Hyperscaling laws
Unfortunately, we are not yet in a position to pay the proper amount of attention
to these very important relations. We will return to them once a more complete
foundation has been laid.
9.3 Ginzburg–Landau–Wilson effective Hamiltonian
There is another, extremely useful way to frame the evaluation of the partition
function of an O(n) system. The new version of the equilibrium statistical mechan-
ics of the system starts from a version of the Hamiltonian originally postulated
by Ginzburg and Landau in the context of a model for superconductivity. In this
approach the Hamiltonian in (9.7) is replaced by the following expression
HGL

⃗S

, T, ⃗h

=
N

i=1
r
2
⃗Si

2
+ 1
2
 ⃗∇⃗Si

2
+ u
4
⃗Si

4
−⃗h · ⃗Si

≡H(0)
GL −⃗h · ⃗Si
(9.28)
The “gradient squared” term on the right hand side of (9.28) is shorthand for
the discrete version of 
k,l(∇kSi,l)2. This term expresses the consequences of
the ferromagnetic coupling between neighboring moments. The quantity r is the
reduced temperature. It is equivalent to the quantity r deﬁned in (9.21).
A very important difference between the system of moments for which the ef-
fective Hamiltonian in (9.28) is appropriate and the set to which the Hamiltonian
in (9.7) applies is that in the latter case, the ⃗S’s have a ﬁxed magnitude, while the
moments in (9.28) take on all possible amplitudes.2 The partition function is as
given in (9.8), except that the sum over conﬁgurations is now a multiple integral
2 Another important characteristic of the Ginzburg–Landau spin variable is that it is classical, as opposed to
quantum mechanical. This allows us to sum over conﬁgurations without regard to the underlying quantum
structure of the spin degree of freedom that carries the magnetic moment.

216
Scaling of walks and critical phenomena
over the individual ⃗Si’s. That is
Z =

i

dnSie
−H

⃗S

,T,⃗h

(9.29)
Notice that the exponent no longer contains the factor kBT . This is because the new
Hamiltonian contains some of the effects of temperature. For this reason, it is more
properly referred to as an effective Hamiltonian, and that is how we will denote it
from now on.
9.3.1 Mean ﬁeld approximation to the effective Hamiltonian
The mean ﬁeld approximation to the critical point of the O(n) model follows
straightforwardly from an approximation to the partition function that replaces the
multiple integral over the ⃗Si’s by an n-fold integration over the components of a
dominant mode. To generate this approximation, and as a prelude to an analysis of
various properties of the O(n) model, we recast the effective Hamiltonian so that it
is expressed in terms of the spatial Fourier transform of the ⃗Si’s. That is, we replace
⃗Si as a variable by
⃗S(⃗q) = 1
N

i
⃗Siei⃗q·⃗ri
(9.30)
The inverse of the above transformation is
⃗Si =
V
(2π)d

BZ
⃗S(⃗q)e−i⃗q·⃗riddq.
(9.31)
As indicated in (9.31), the ⃗q integral is over the ﬁrst Brillouin zone. Details about
the Fourier transform and its inverse are contained in the second supplement to
Chapter 2.
In terms of the new representation, the effective Hamiltonian takes on the form
H

⃗S(⃗q)

, T, ⃗h

= V
2

⃗q∈BZ

r + |⃗q|2
⃗S(⃗q) · ⃗S(−⃗q) ddq
+ V u
4

⃗q1,⃗q2,⃗q3,⃗q4∈BZ

⃗S(⃗q1) · ⃗S(⃗q2)
 
⃗S(⃗q3) · ⃗S(⃗q4)

× δ(⃗q1 + ⃗q2 + ⃗q3 + ⃗q4) ddq1 ddq2 ddq3 ddq4 −V ⃗h · ⃗S(0)
(9.32)
The quantity V in (9.32) is the volume of the system, as previously. To generate the
mean ﬁeld approximation one removes from the effective Hamiltonian all Fourier

9.3 GLW effective Hamiltonian
217
modes except ⃗S(⃗q = 0). The effective Hamiltonian reduces to
HMF = V
r
2
⃗S(0)

2
+ u
4
⃗S(0)

4
−⃗h · ⃗S(0)

≡V
r
2 | ⃗m|2 + u
4 | ⃗m|4 −⃗h · ⃗m

(9.33)
The last line of (9.33) can be taken as a deﬁnition of the magnetization per unit
volume, ⃗m. The mean ﬁeld partition function is then the integral
ZMF =

dnme−HMF( ⃗m)
(9.34)
A full discussion of the mean ﬁeld approximation and its extensions is deferred
to the next chapter.
Exercise 9.3
Derive the expression (9.32) for the effective Hamiltonian of the O(n) model from
(9.28) for that quantity.
Exercise 9.4
Assume that the free energy as a function of reduced temperature, r, magnetization,
m, and magnetic ﬁeld, h, is given by the following expression
F = r
2m2 + m4
12 −hm
(9.35)
This system is in thermal equilibrium when its free energy, as a function of m, is
minimized. Thus, the equation of state of this system, which leads to a dependence
on r and h for the equilibrium value of m is
∂F
∂m = 0
(a) Show that the solution to this equation of state is
m = |r|βM
 h
|r| , r
|r|

What are the values of β and ?
(b) Use the above result to show that the minimized free energy has the following form:
F = r2−αK
 h
|r| , r
|r|

What is the value of the quantity α?

218
Scaling of walks and critical phenomena
9.4 Scaling and the mean end-to-end distance; ⟨R2⟩
A detailed treatment of the O(n) model and its predictions for the self-avoiding
walks is the subject of the next chapter, but before taking up that study, we illustrate
the utility of the spin analogy by deducing the power-law behavior of the mean
end-to-end distance, asserted at the start of this chapter. To accomplish this result,
we only need the invoke the scaling features of the magnetic system as summarized
in Table 9.1.
Our starting point is, not unexpectedly, the formal deﬁnition of the correlation
length
ξ 2 =

C(T, ⃗R)R2 dd R

C(T, ⃗R) dd R
(9.36)
Using the connection between the correlation function of the spin system and the
generating function of self-avoiding walks3 allows us to identify the numerator of
the right hand side of (9.36) as the generator of the mean end-to-end distance, whilst
the denominator is the susceptibility per unit volume
ξ 2 = V
χT
∞

N=0

C(N; ⃗R)R2 dd R
 z
z0
N
= V
χT
∞

N=0
⟨R2⟩N
 z
z0
N
(9.37)
z0 being the mean ﬁeld critical fugacity (2d J)−1 and N the total number of non-
intersecting walks with ﬁxed starting point. Near the critical point, the correlation
length and susceptibility scale according to power-laws, K1/(zc −z)ν and K2/(zc −
z)γ ,respectively.Thecriticalfugacity zc issimplyrelatedto z0 bytheproportionality
relation zc = z0/µ. The constant µ is called the connectivity. Thus, in the critical
region, we can write
K1K2
(zc −z)2ν+γ = V

N
⟨R2⟩N
zN
0
zN
(9.38)
In the supplemental notes of Chapter 1, the coefﬁcient of zN in an expansion of
the function (zc −z)−s was found to be z−(N+s)
c
N δ−1/(s), which permits the
identiﬁcation
V ⟨R2⟩N
zN
0
=
K1K2
zN+γ +2ν
c
(γ + 2ν)
N γ +2ν−1
(9.39)
3 We establish this connection in Section 9.5.

9.5 O(n) model and the self-avoiding walk
219
Nothing can be concluded from (9.39) regarding the dependence of ⟨R2⟩on N
until we know how the total number of walks, N, depends on N. Fortunately, this
quantity can be extracted directly from the susceptibility:
χT = V

dd RC(T, ⃗R)
= V

N

dd RC(N; ⃗R)
 z
z0
N
= V

N
N
 z
z0
N
(9.40)
Again, knowing that χT scales as (zc −z)−γ near zc, the coefﬁcient of zN in
the expansion of the susceptibility in powers of z is, by the above argument,
K2N γ −1/zN+γ
c
(γ ), which must be equal to V N/z0 because of the uniqueness of
power series. From this result, we have
⟨R2⟩= K1
zγ
c
(γ )
(γ + 2ν) N 2ν
(9.41)
and since the prefactor is independent of the number of steps we conclude that
⟨R2⟩∝N 2ν, precisely the observation of de Gennes.
From this simple calculation we only get a glimpse of the powerful analytical
tools made available to us in transforming the statistics of self-avoiding walks to a
statistical analysis of the critical phenomena of interacting magnetic moments. A
modern analysis of the O(n) model not only yields accurate numerical results for
the correlation length exponent, ν, itself, but also provides insights and quantitative
understanding of a host of other features of the self-avoiding walk problem as well,
a task to which we now turn.
9.5 Connection between the O(n) model and the self-avoiding walk
So far we have asserted the linkage between the O(n) model in the limit n =
0 and the self-avoiding random walk. We have talked about the utility of that
connection, in that one can extract the behavior of this kind of random walk from the
thermodynamic properties of the associated magnetic system. In this ﬁnal section,
we will take the reader through the arguments that establish the mathematical
equivalency of the two systems. The original derivation of this equivalency is due to
de Gennes. This derivation is based on a comparison of the perturbation theoretical
expansion for the partition function of the O(n) model with the corresponding
expansion for the generating function of the self-avoiding walk. The expansion
parameter that regulates the convergence of the series is the combination J/kBT ,

220
Scaling of walks and critical phenomena
where J is the exchange energy. This argument requires that there be no signiﬁcant
effects that are non-perturbative in nature. Furthermore, perturbation theory in
the case at hand is known to be an asymptotic expansion, having zero radius of
convergence. Finally, the perturbation theoretical expansion is valid, even in an
asymptotic sense, only at small values of J/kBT . The accessibility of the ordered
phase of the magnetic system is not at all assured. The derivation of the connection
presented here follows an argument due to Emery (1975), who demonstrated the
equivalency between the spin system and the self-avoiding walk without recourse
to perturbation theory.4
WerecalltheGinzburg–Landau–WilsoneffectiveHamiltonian,(9.28).Forpurely
formal reasons, which will become clear shortly, we imagine an ordering ﬁeld ⃗hi
that varies from site to site and replace the Zeeman term in (9.28) by −
i ⃗hi · ⃗Si.
A generalized version of the partition function, Z, is then given by the multiple
integral
Z =
N

i=1

d ⃗Sie
−H(0)
GL

⃗S

−
i ⃗hi · ⃗Si
(9.42)
This partition function is the source of all information, not only regarding the
equilibrium thermodynamics of the system, but also with respect to correlations of
the order parameter. For instance, a quantity of particular relevance is the two-point
correlation function, as given in (9.14). This function is readily obtained from the
partition function in (9.42) by taking derivatives with respect to the ordering ﬁeld
at different sites. To see how this works, take a derivative of the logarithm of the
right hand side of the equation with respect to the αth component of ⃗hi.
∂
∂hi,α
ln (Z) =
N
k=1

d⃗SkSi,αe
−H(0)
GL

⃗S

Z
= ⟨Si,α⟩
(9.43)
Taking one further derivative,
∂2Z
∂hi,α∂h j,α
= ⟨Si,αSj,α⟩−⟨Si,α⟩⟨Sj,α⟩
(9.44)
The right hand side is just the generalized version of the correlation function ap-
pearing in the last line of (9.25). This correlation function plays a key role in the
connection between the spin model and the self-avoiding walk. As ﬁrst pointed out
4 For a somewhat modiﬁed version of the development, see Sch¨afer (1999).

9.5 O(n) model and the self-avoiding walk
221
by de Gennes, if
lim
n→0
1
n
n

α=1
⟨Si,αSj,α⟩= G(z; ⃗Ri, ⃗R j)
≡
∞

N=0
C(N; ⃗Ri, ⃗R j)zN
(9.45)
then C(N; ⃗Ri, ⃗R j) is equal to the number of N-step self-avoiding walks originating
at ⃗Ri and ending at ⃗R j. It is this connection that we will now establish.
The principal difﬁculty in the evaluation of the partition function of the O(n)
model arises as a result of the quartic term u
4|⃗Si|4 in the exponent. If that term were
eliminated, then the integrals in the right hand side of (9.42) would be generalized
versions of a Gaussian integral, and such integrals can be evaluated fairly straight-
forwardly.5 While the quartic term cannot be eliminated (in fact it is essential to
the preservation of a convergent form for the partition function below the critical
temperature), the introduction of an additional set of degrees of freedom allows us
to reduce the integrals over the moments ⃗Si to a Gaussian form. The key to the
transformation of the integrals is the identity
e−u
4 |⃗Si|4 =
1
√πu
 ∞
−∞
dφi exp

−φ2
i
u −iφi|⃗Si|2

(9.46)
Application of this identity at each point i replaces all quartic terms in the exponent
by terms that are quadratic in the moments ⃗Si – at a cost, however. The evaluation
of the partition function now requires an integration over two sets of variable, ⃗Si
and φi. The essential difﬁculties attending the evaluation of the partition function
remain, but, for our present purposes, there are distinct advantages in the new form
of the partition function.
An immediate beneﬁt of the transformation effected by the application of the
relationship in (9.46) is that the limit n →0 is readily achieved. To see how this
occurs, we utilize (9.46) in (9.42). Making use of the form (9.28) for the effective
Hamiltonian, we are left with
Z =

i
  dφi
√πu

e−
i
φ2
i
u 
j

dnSj
× exp

−

j
r
2 + iφ j

|⃗S j|2 −1
2| ⃗∇⃗S j|2

(9.47)
In the above, the ordering ﬁelds have been set equal to zero. Two features of the
right hand side of (9.47) worth noting are as follows.
5 For a discussion of the evaluation of general Gaussian integrals, see the supplement at the end of this chapter.

222
Scaling of walks and critical phenomena
(1) The integral over one spin component at a given site is the same as the integral over any
other spin component at that site.
(2) The N integrals over Si,α can be done explicitly, since they are of a Gaussian form.
We have

dnSi exp

−
r
2 + iφi

|⃗S|2 −1
2| ⃗∇⃗Si|2

=
n
α=1

dSi,αexp

−
r
2 + iφi

S2
i,α −1
2| ⃗∇Si,α|2

(9.48)
which, in light of (1) above, reduces to

dSi exp

−
r
2 + iφi

S2
i −1
2| ⃗∇Si|2
n
(9.49)
In arriving at (9.49) the dummy variable of integration, Siα has been replaced by
Si. The partition function factors nicely:
Z =
N

i=1

dφi
√πu exp

−

i
φ2
i /uZn

(9.50)
where
Zn =
 N

i=1

dSi exp

−

i
r
2 + iφi

S2
i + 1
2| ⃗∇Si|2
n
(9.51)
The integrals appearing in (9.51) are most readily performed by observing the
following.
(1) The 1
2| ⃗∇Si|2 term can be replaced by −1
2 Si∇2Si. This follows from a simple integration
by parts. The integrated part vanishes in the thermodynamic limit.
(2) The exponent in (9.51) can be written

i, j
SiVi j Sj
(9.52)
where
Vi j =
r
2 −1
2∇2

δi j + iφiδi j
(9.53)
which can be written as the matrix element of the operator
↔
V =
r
2 −1
2∇2

I + i
↔
φ
(9.54)
whose matrix elements in the coordinate basis, |⃗Ri⟩are given by
⟨⃗R j|
↔
V |⃗Ri⟩
(9.55)

9.5 O(n) model and the self-avoiding walk
223
The integral over the Si’s now looks like
 N

i=1

dSi exp

−

i, j
SiVi j Sj
n
(9.56)
which readily integrates to
Zn =

(2π)N/2
det
↔
V
n
(9.57)
Of course, the quantity in which we are really interested is the two-point corre-
lation function and not the partition function, Zn. Fortunately, the latter quantity
yields readily to the kind of analysis that we have been applying here. The key
to the extraction of the correlation function is the reintroduction of the Zeeman
term 
i ⃗hi · ⃗Si into the effective Hamiltonian. Taking each of the components of
site-dependent ordering ﬁeld, ⃗hi, to be the same, i.e.
⃗hi = hi(1, 1, . . . , 1)
(9.58)
and proceeding in exactly the same fashion, (9.56) becomes
Zn =
 N

i=1

dSi exp

−

i, j
SiVi j Sj +

i
hi Si
n
(9.59)
This integral is of the form considered in the supplemental notes at the end of this
chapter. Using the results derived there, one ends up with
Zn =

(2π)N/2
det
↔
V
exp

−1
2

i, j
hi
 ↔
V
−1
i j h j
n
(9.60)
In (9.60)
↔
V
−1
is the inverse of the operator
↔
V .
The two-point correlation function is found as indicated in (9.44).
G(⃗R j, ⃗Rk) = lim
n→0
n

α=1
⟨Sj,αSk,α⟩
= lim
n→0
∂2
∂h j∂hk
ln Z
(9.61)
In taking the limit n →0, the trick is to expand ln Z to ﬁrst order in n. All other

224
Scaling of walks and critical phenomena
terms vanish as n →0.
G(⃗R j, ⃗R′
k) =
∂2
∂h j∂hk

i

dφi
√πu e−
i
φ2
i
u 1
2

l,m
hlV −1
lm hm
=

i′

dφi′
√πu V −1
jk e−
i
φ2
i
u
(9.62)
For the components of the inverse matrix
↔
V
−1
,
T −1
i j
= ⟨⃗Ri|
↔
V
−1
|⃗R j⟩
= ⟨⃗Ri|
r
2 −1
2∇2

↔
I + i
↔
φ
−1
|⃗R j⟩
(9.63)
Recall that the quantity
↔
I is the identity and the operator
↔
φ is diagonal in the
coordinate basis, ⟨⃗Ri|
↔
φ |⃗R j⟩= φiδi j. Expressing the inverse operator
↔
V
−1
as
↔
V
−1
=
 ∞
0
e−r
2 te

1
2 ∇2 ↔
I −i
↔
φ

t dt
(9.64)
allows us the recast the correlation function in an extremely compact form
G(⃗Ri, ⃗R j) =

i

dφi
√πu
 ∞
0
dte−r
2 t⟨⃗Ri|e

1
2 ∇2 ↔
I −i
↔
φ

t|⃗R j⟩
(9.65)
Although the above expression is indeed compact, and its appearance is deceptively
simple, it is not straightforward to evaluate. The problem stems from the fact that
the operator
↔
φ depends of the coordinates and therefore does not commute with
∇2. The exponential operator appearing in (9.65) does not factor.
Exercise 9.5
Show that when
↔
V
−1
is given by (9.64),
↔
V
↔
V
−1
=
↔
V
−1 ↔
V =
↔
I .
9.5.1 Path integral reduction of the matrix element
The operator equation appearing in (9.65) closely resembles an imaginary time
quantum-mechanical propagator, and, fortunately, Feynman taught us how to de-
compose operators of this sort into an appropriate path integral (Feynman and
Hibbs, 1965; Kleinert, 1995). As we have mentioned, the two terms in the expo-
nent in (9.65) do not commute, so it is not permissible to factorize the exponent.
However, the following factorization is allowed when t is inﬁnitesimal. Let t = ϵ.

9.5 O(n) model and the self-avoiding walk
225
Then
e

1
2 ∇2 ↔
I −i
↔
φ

ϵ = e−i
↔
φϵe
1
2 ∇2ϵeO(ϵ2)
(9.66)
Exercise 9.6
Find the next order term, in terms of the quantity ϵ, of the expansion (9.66).
Therefore, if the range of t is decomposed into a large number of inﬁnitesimals,
i.e. t = Mϵ, where M →∞as ϵ →0, in such a way as to keep the product Mϵ
ﬁxed, then the propagator can be factorized accordingly. Keeping the above limiting
procedure in mind, we have
⟨⃗Ri|e

1
2 ∇2−i
↔
φ

t|⃗R j⟩= ⟨⃗Ri|

e−i
↔
φϵe
1
2 ∇2ϵ
 
e−i
↔
φϵe
1
2 ∇2ϵ

· · · |⃗R j⟩
(9.67)
Introducing a complete set of intermediate states 
i |⃗Ri⟩⟨⃗Ri| between each term
in the product of (9.67), we are able to represent the matrix element as a sum over
all possible paths:

⃗R0,...,⃗RM
⟨⃗Ri|e−i
↔
φϵ|⃗RM⟩⟨⃗RM|e
1
2 ∇2ϵ|⃗RM−1⟩⟨⃗RM−1|e−i
↔
φϵ|⃗RM−2⟩· · · |⃗R j⟩
(9.68)
The various matrix elements appearing in (9.68) are easily evaluated
⟨⃗RM|e−i
↔
φϵ|⃗RM′⟩= δM,M′e−iϵφ(⃗RM)
(9.69)
⟨⃗RM|e∇2ϵ|⃗RM′⟩= (2π)−d/2 exp

−d
2
⃗RM −⃗RM′

2
ϵ


(9.70)
In the above, d is the spatial dimensionality. Putting everything together, the prop-
agator is seen to take on the form of a path integral.

l

dd Rl exp

−
 t
0
d
2

d⃗R
ds

2
ds −i
 t
0
φ

⃗R (s)

ds


(9.71)
with ﬁxed end-points, ⃗R(0) = ⃗Ri and ⃗R(t) = ⃗R j.
The ﬁnal step in establishing the result that the two-point correlation function is,
indeed, the generating function for self-avoiding walks, is the substitution of (9.71)
for the propagator in (9.65) and carrying out the integrations over the ﬁelds φ(⃗Ri).

226
Scaling of walks and critical phenomena
The N integrations over the ﬁelds φ are all of the form
 ∞
0
dφ(⃗Rl)
√πu exp

−φ(⃗Rl)2
u
−iφ(⃗Rl)

(9.72)
which integrates to e−u/4, unless two of the ⃗R’s are equal, in which case the integral
becomes
 dφ(⃗Rl)
√πu exp

−φ(⃗Rl)2
u
−2iφ(⃗Rl)

(9.73)
which yields the value e−u. This eventually leads to an overall factor of e−u/2 for
each intersection of the path. This factor can be written
exp

−u
4

l,l′
δ⃗R,⃗R′

(9.74)
The factor of 2 is absorbed into the overcounting of the pairs. We see from this
that the Ginzburg–Landau–Wilson version of the O(n) model does not eliminate
intersecting paths, but reduces their contribution to the generating function by the
factor e−u/2. In the continuum limit, the Kronecker delta function goes over to the
Dirac delta function. The ﬁnal expression for the correlation function becomes
G(⃗Ri, ⃗R j) =
 ∞
0
dte−rt/2 
l

dd Rl
× exp

−
 t
0
d
2

d⃗R
ds

2
ds u
4
 t
0
 t
0
δ(⃗R(s′) −⃗R(s)) ds ds′


=
 ∞
0
dte−rt/2C(t; ⃗Ri, ⃗R j)
(9.75)
The quantity C(t; ⃗Ri, ⃗R j) corresponds to the number of self-avoiding walks of
length t with beginning and end-points ⃗Ri and ⃗R j, respectively. Clearly the cor-
relation function and the generating function for self-avoiding walks are related.
Indeed, (9.75) is precisely the form derived by de Gennes in his treatment, but to
make the connection more transparent and to connect with the customary pertur-
bative approach, consider the walk to be composed to discrete steps, N in number.
Then, the integral over t is a sum over N
G(⃗Ri, ⃗R j) =
∞

N=0
C(N; ⃗Ri, ⃗R j)e−r N/2
(9.76)

9.6 Supplement
227
Let’s assume, in addition, that we are above the critical point. Recall
r/2 = (1 −Tc/T ) ≈0, e−r/2 ≈1 −r/2 = Tc/T = J/T = z/zc. Then the result
above takes on a familiar form. Perturbatively
G(⃗Ri, ⃗R j) =
∞

N=0
C(N; ⃗Ri, ⃗R j)
 J
T
N
=
∞

N=0
C(N; ⃗Ri, ⃗R j)
 z
zc
N
(9.77)
Given that the left hand side of (9.77) is the correlation function of the O(n) spin
system and the right hand side is the generating function for self-avoiding walks,
we have, at last, the desired connection.
9.6 Supplement: evaluation of Gaussian integrals
In this supplement, we outline a procedure by which integrals of the kind encoun-
tered in this chapter are evaluated. These integrals are of a Gaussian type with a
canonical form
I =

i
 ∞
−∞
dxi exp

−1
2

i, j
xiTi jx j +

i
hixi

(9:S-1)
The matrix ˆT and the vector ⃗h are independent of the integration variables, and ⃗h
is assumed to have real components, whereas ˆT will not necessarily be real. The
integration variables xi may, likewise, be complex. Here, we assume that they are
real. The integral can be cast into a compact form with the use of Dirac notation.
We deﬁne a column vector |⃗x⟩as follows:
|⃗x⟩=


x1
·
·
·
xn


(9:S-2)
and the adjoint vector ⟨⃗x|
⟨⃗x| = (x1, . . ., xn)
(9:S-3)
Equation (9:S-1) is then, in this notation
I =

i
 ∞
−∞
dxi exp

−1
2 ⟨⃗x| ˆT |⃗x⟩+
	
⃗h|⃗x


(9:S-4)

228
Scaling of walks and critical phenomena
We further assume that ˆT can be diagonalized by a unitary transformation, ˆU
ˆU † ˆT ˆU =


λ1
0
·
·
0
0
λ2
0
·
0
0
0
·
·
0
0
0
·
0
λN


(9:S-5)
The λ’s in (9:S-5) are the eigenvalues of the operator ˆT . In order for (9:S-5) to hold,
the general matrix form of ˆT must be normal, that is, it commutes with its adjoint.
The next step is to transform to a representation that diagonalizes ˆT . Deﬁne a new
set of variables according to the unitary transformation
|⃗x⟩= ˆU |⃗y⟩
(9:S-6)
and a new set of ﬁelds |⃗h′⟩by
⃗h

= ˆU
⃗h′
.
(9:S-7)
The determinant of the unitary operator ˆU is equal to one. Thus, the Jacobian
accompanying this change of variables in (9:S-1) is also equal to one, and the
integral transforms to
I =
 
i
dyi

exp

−1
2 ⟨⃗y| ˆU † ˆT ˆU |⃗y⟩+
	
⃗h′ ˆU † ˆU |⃗y⟩

=
 
i
dyi

exp

i

−λi
y2
i
2 + h′
i yi

(9:S-8)
The integrals are now uncoupled Gaussians, and therefore are easily evaluated.
Of course, we are assuming that λi > 0. There are two cases to discuss. First, the
vectors |⃗y⟩may be real. In that case one simply completes the squares and integrates.
The result is
I =
(2π)N/2
√λ1λ2 . . . λN
exp

i
|h′
i|2
2λi

= (2π)N/2
√
det ˆT
exp
1
2
	
⃗h
 ˆU ˆ−1 ˆU † ⃗h


(9:S-9)
where

−1
i j = 1
λi
· ⃗δi j
(9:S-10)

9.6 Supplement
229
Since ˆ−1 is the inverse of the diagonalized version of ˆT , the operator ˆU ˆ−1 ˆU † is
the inverse of ˆT in the original representation. We thus arrive at our ﬁnal answer
I = (2π)N/2
√
det ˆT
exp

1
2

i, j
hi

T −1
i j h j

(9:S-11)
In the second case of interest, the vectors |⃗y⟩are complex. If this is so, the
integral is carried over both the real and the imaginary parts of the components,
which means dy’s become dyR dyI’s. These integrals are of the same form as before
and are evaluated accordingly.
9.6.1 Gaussian averages and Wick’s theorem
Assuming that the probability density for random variables x1, x2, . . ., xN is gov-
erned by the Gaussian form
ρ (x1, x2, . . ., xN) =
√
det ˆT
(2π)N/2 exp

−1
2

i, j
xiTi jx j

(9:S-12)
then the average of the function F(x1, . . ., xN) is given by
⟨F⟩=

N

i=1
dxi F(x1, x2, . . . )ρ(x1, x2, . . . )
(9:S-13)
An average of particular interest (see, for instance, (6.30) in Chapter 6) is
⟨xrxs⟩=

xrxsρ(x1, x2, . . . )
N

i=1
dxi
(9:S-14)
We now introduce a ﬁctitious “Zeeman-type” term into the exponential in the
probability density. This allows us to take averages of the sort shown in (9:S-14)
without the need to explicitly carry out integrations. Here is how this trick works.
Consider the probability density
ρh(x1, . . ., xN) = ρ(x1, . . ., xN) exp

k
hkxk

(9:S-15)
Then,
⟨xrxk⟩= lim
hi→0
i=1,N
'
∂2
∂hr∂hs

ρ(x1, . . ., xN)
N

i=1
dxi
(
(9:S-16)
From our previous results for the integral of the probability density ρh, (9:S-16)

230
Scaling of walks and critical phenomena
reduces to
⟨xrxs⟩= lim
hi→0
i=1,N
'
∂2
∂hr∂hs
exp

1
2

i, j
hih j
 ˆT
−1
i j
(
(9:S-17)
After taking the two derivatives indicated and setting each of the hi’s equal to zero,
the average ⟨xrxs⟩becomes
⟨xrxs⟩= 1
2
 ˆT
−1
rs +
 ˆT
−1
sr

=
 ˆT
−1
rs
(9:S-18)
The last equality holds for symmetric matrices ˆT . We will assume that this is the
case for the matrices entering into the probability density through (9:S-12).
The general expression (9:S-17) can be generalized:
⟨xrxsxtxu⟩= lim
hi→0
'
∂4
∂hr∂hs∂ht∂hu
exp

1
2

i, j
hih j⟨xix j⟩
(
= lim
hi→0
'
∂3
∂xr∂xs∂xt

j
h j⟨xux j⟩exp

1
2

i,k
hihk⟨xixk⟩
(
= lim
hi→0
'
∂2
∂hr∂hs

⟨xrxs⟩+ h jhk

h jhk
⟨xtx j⟩⟨xuxk⟩

× exp

1
2

l,m
hlhm⟨xlxm⟩
(
(9:S-19)
Taking the remaining two derivatives and setting all hi’s equal to zero, we are left
with the ﬁnal expression for ⟨xrxsxtxu⟩
⟨xrxsxtxu⟩= ⟨xrxs⟩⟨xtxu⟩+ ⟨xrxt⟩⟨xsxu⟩+ ⟨xrxu⟩⟨xtxs⟩
(9:S-20)
This is a special case of Wick’s theorem.
In general, Wick’s theorem states that the Gaussian average of ⟨xi1xi2, . . ., xiN⟩
is given by
⟨xi1xi2, . . ., xiN⟩=

all
distinct
permutations
⟨xi p1xi p2⟩⟨xi p3xi p4⟩· · · ⟨xi p(N−1)xi pN⟩
(9:S-21)
Here, p1 is the ﬁrst element in a given permutation of the integers 1, 2, . . ., N,
and similarly for p2 and so on. The above equation holds only if there are an even

9.6 Supplement
231
number of xin’s, or, in other words, if N is an even number. If N = 2m, then there
are
(2m)!
2mm!
(9:S-22)
permutations leading to distinct contributions to the right hand side of (9:S-21).
The rule, as indicated on the right hand side of that equation, is that one only counts
distinct contributions.


10
Walks and the O(n) model: mean ﬁeld theory
and spin waves
10.1 Mean ﬁeld theory and spin wave contributions
Now that we have established the link between self-avoiding random walks and the
O(n) model of magnetism, it is appropriate to look again at the magnetic system in
the limit n →0. Of special interest to us is its behavior in the immediate vicinity
of the critical point. We will make extensive use of the insights provided by the
study of critical phenomena that have emerged over the past three decades. Our
initial approach to the problem of the statistical mechanics of the O(n) model will
be to utilize the mean ﬁeld ideas developed by Landau and others. Mean ﬁeld
theory will then be enhanced by the introduction of ﬂuctuations which will be
analyzed in a low order spin wave theory. The insights gained by this approach
will lead us to a clearer picture of the phase transition as it pertains to the random
walk problem. Finally, a full renormalization group calculation will be used to
elucidate the scaling properties of this model. This ﬁnal step in the analysis will be
accomplished in Chapter 12.
10.1.1 Outline of the chapter
In this chapter we make use of the connection between the O(n) model and the
self-avoiding walk to discuss aspects of the statistics of such a walk. We begin by
reviewing the relationship between the spin model described by the O(n) energy
function and the self-avoiding walk. Our initial focus will be on self-avoiding
walks that are conﬁned to a ﬁnite volume of space. The appropriate version of the
O(n) system consists of a ﬁnite collection of immobile spins occupying a ﬁxed
volume. When the number of spins is large – and the region containing them is
extended – boundary conditions do not, in general, play a key role in the statistical
mechanics of the system. On the other hand, boundary conditions exert a profound
inﬂuence on the statistics of both self-avoiding and unrestricted walks, as we have
233

234
Walks and the O(n) model
alreadyseen. Inourdiscussion,wewillassumeperiodicboundaryconditions.These
boundary conditions were introduced in Chapter 7, in the discussion of the mean
ﬁeld approximation for self-avoiding walk statistics. Here, we will detail the exact
nature of these conditions. As it turns out, “periodic” boundaries have no inﬂuence
on the total number of paths available to an unrestricted walker, in marked contrast
to the case of absorbing boundaries.1 While periodic boundaries do not, as a rule,
correspond to any recognizable physical situation, they have the same general effect
on key aspects of random walk statistics as reﬂecting boundaries, of which one can
imagine physical realizations. Periodic boundary conditions possess the additional
desirable attribute that calculations are, in general, considerably more tractable in
periodic systems than in systems with real boundaries.
The issue of limits is especially important in the case at hand. The correspondence
between the O(n) model and the self-avoiding walk is ﬁrm only if the limit n →0
is taken before all other limiting procedures are implemented. In particular, it is
important that n be taken to zero before the size of the system is allowed to become
inﬁnite. It is known that interchange of those limits introduces spurious results in
other situations (Rudnick and Gaspari, 1986b).
An additional advantage ﬂows from the tactic of keeping the size of the system
ﬁnite.First,boundaryconditionsareautomaticallyincorporatedintothecalculation.
Second, one is led naturally to a useful interpretation of the “condensed” phase of
the self-avoiding walk. This interpretation has been advanced by the authors of this
book and others.
As a ﬁrst step in our investigation of walks and the O(n) model we work in the
mean ﬁeld approximation. In this approximation, key aspects of the statistics of the
self-avoiding walk conﬁned to a ﬁnite region of space depend on how many steps
the walker has taken. If it has not taken too many, then the total number of walks
available to it cannot be distinguished from the case in which the walker ﬁnds itself
is an environment that extends to inﬁnity in all directions. However, when there
are enough steps in the walk, the self-interaction of the walk leads to a substantial
reduction in the number of paths. This reduction is codiﬁed in terms of a kind of
Boltzmann factor, e−ε. The energy, ε, is given by the expression utilized by Flory in
his argument for the size of a self-avoiding walk. The results of this analysis agree
with those obtained in Chapter 7, where the mean ﬁeld approximation was derived
from a diagrammatic summation.
As the next step, ﬂuctuations are added to the formulation; in the present con-
text, the ﬂuctuations take the form of spin waves. A calculation of the statistical
mechanics of the model is carried out. In this calculation, the system is assumed
to be ﬁnite. That is, we do not invoke the thermodynamic limit at the outset. This
1 See Chapter 4 for a discussion of absorbing boundaries and their effect on unrestricted random walks.

10.1 Mean ﬁeld theory and spin wave contributions
235
polymer
a monomer
Fig. 10.1. A polymer in thermal equilibrium with a bath of monomers.
allows us to take limits in the proper order. We discover a “correlation hole” in
the low-temperature phase, which we interpret in terms of an effective repulsion
between the starting and ending points of the walk, or the two ends of the polymer.
This repulsion plays an important role in the statistics of the self-avoiding polymer
when it is tightly packed into a ﬁnite volume. We close the chapter with a more
detailed discussion of the correlation hole, and, in particular, of the mechanisms
giving rise to it. Before embarking on that project, we review a reinterpretation of
the phase transition proposed by Redner and Reynolds (1981), which provides an
intuitively appealing, if as yet less-than-rigorous, scaling ﬁeld description of the
process.
In this chapter, our primary focus will be on the generating function. That is, we
will not concern ourselves with the N-step walk statistics that can be extracted from
this quantity. To provide the discussions to follow with some intuitive underpin-
nings, we describe a physical system for which the generating function represents
an immediate theoretical model. We have encountered this system, brieﬂy and in
broad outline, in Chapter 2. It consists of a polymer in thermodynamic equilibrium
with a “reservoir” of monomers. We imagine that these monomers can attach to
the ends of the polymer, thus adding to its length, and that, conversely, monomers
can “evaporate” off the ends of the polymer, returning to the reservoir populated by
their cohorts. A picture of such a collection of coexisting molecules is displayed
in Figure 10.1. The statistics of the polymer are controlled by a grand partition
function
G(z; ⃗x, ⃗y) =

N
C(N; ⃗x, ⃗y)zn
(10.1)
in which the quantity z plays the role of the fugacity. Here, one of the end-monomers
of the collection of polymers in question is constrained to sit at the location ⃗x
and the other end-monomer sits at ⃗y. This restricted grand partition function is
directly related to the statistical mechanical properties of the O(n) magnet in the
limit n →0. When we interpret the predictions that follow from calculations on

236
Walks and the O(n) model
the O(n) model, our explications will be directly relevant to the statistics of the
polymer in solution introduced here.
The functions on the left and right hand sides of (10.1) are, of course, also fa-
miliar from our discussion of random walk statistics. The quantity G(z; ⃗x, ⃗y) is the
generating function of walks subjected to the same restrictions that apply to the ran-
domly coiling polymer, while C(N; ⃗x, ⃗y) counts the N-step walks (corresponding
to polymers consisting of N monomers) that start at ⃗x and end up at ⃗y.
The O(n) model describes a system that undergoes a phase transition. In the case
of a magnetic system, this phase transition is between a disordered, paramagnetic
phase at high temperatures and a phase that supports magnetism in the absence of
an external ﬁeld at low temperatures. The low-temperature ferromagnetic, state is
known as the ordered phase of the system. The analogous system in the case of self-
avoiding polymers is a collection of such polymers of various lengths in equilibrium
with a “bath” of momomers. The phase transition in this system separates a phase in
which the polymers are relatively well-separated (the analogy of the paramagnetic
phase) and a phase in which the polymers ﬁll space and intertwine like a plate
of spaghetti (the analogy in this sytem of the ferromagnetic phase)(des Cloiseaux
and Jannink, 1990). The particular case that we discuss in this chapter requires a
slight reinterpretation of the phase transition. We will address this issue when it
arises.
10.2 The mean ﬁeld theory of the O(n) model
The effective Hamiltonian describing an n-component magnetic moment system
with rotationally invariant interactions in an external ﬁeld is given by
H[⃗S] =

i
r
2|⃗Si|2 + 1
2| ⃗∇⃗Si|2 + u
4|⃗Si|4 −⃗h · ⃗Si

(10.2)
The spin ⃗Si resides on the lattice site with label i. The partition function is given
by the following multiple integral
Z =

i

d⃗Sie−H[⃗S]
(10.3)
The partition function, Z, is the source of all information pertaining to the equilib-
rium thermodynamics of this O(n) model.
The quantity of most direct relevance to the random walk is not Z but rather the
spin–spin correlation function, ⟨SiαSjβ⟩. More precisely, the generating function of

10.2 The mean ﬁeld theory of the O(n) model
237
the self-avoiding walk is expressible as the following limit (see Chapter 9)
G(r; ⃗Ri −⃗R j) = lim
n→0
1
n
n

α=1
⟨SiαSjα⟩
=
∞

N=0
C(N; ⃗Ri −⃗R j)e−r N
(10.4)
with C(N; ⃗Ri −⃗R j) and r = (T −Tc)/Tc having the usual meaning.
It proves convenient to work with the generator of walks with a ﬁxed starting
point that terminate at all possible end-points. We denote this quantity by N. The
generating function of this quantity is related to G(r; ⃗Ri −⃗R j) as follows
(r) =

j
G(r; ⃗Ri −⃗R j)
=

N
Ne−r N
(10.5)
From the above relationship we see that (r) is the counterpart for walks of the
susceptibility per spin of the magnetic system:
(r) = kBT
N0
χT
(10.6)
where N0 is the total number of sites in the spin system. Accordingly,
(r) = −(kBT )2
N0
∂2 ln Z
∂h2

h=0,n=0
(10.7)
The above relationships are true in general. The mean ﬁeld results are obtained
by assuming that the average magnetization is uniform and that ﬂuctuations above
the average can be neglected. That is, one writes
⃗Si = ⃗M
(10.8)
for all sites i. With this replacement, the partition function becomes
ZMF =

dnMe
−N0

r
2 | ⃗M|2+ u
4 | ⃗M|4−⃗h· ⃗M

(10.9)
and our formula for (r) reduces to
MF(r) = N0 lim
n→0




dnM M2
1e
−N0

r
2 | ⃗M|2+ u
4 | ⃗M|4

dnMe
−N0

r
2 | ⃗M|2+ u
4 | ⃗M|4




(10.10)
The ordering ﬁeld, ⃗h, has been taken to point in the “1” direction.

238
Walks and the O(n) model
The complications arising from the | ⃗M|4 term can be dealt with in a variety of
ways. One can, for example, note that
MF = N0 lim
n→0

dnM M2
1 f (| ⃗M|2)

dnM f (| ⃗M|2)

= N0 lim
n→0

dλf (λ)

dnM M2
1δ(λ −| ⃗M|2)

dλf (λ)

dnMδ(λ −| ⃗M|2)

(10.11)
The integrals over the components for the magnetization are easily performed with
the use of the Fourier representation of the delta function δ(λ −| ⃗M|2). That is,
replacing the delta function by
1
2π
 ∞
−∞
dSe
iS

λ−| ⃗M|2
(10.12)
in both the numerator and denominator of (10.11), MF becomes
N0 lim
n→0

dλ dSf (λ)eiSλ 
dM1M2
1e−iSM2
1

dMe−iSM2n−1

dλ dSf (λ)eiSλ 
dMe−iSM2n
(10.13)
In the limit n →0, the case of interest here, the denominator tends to 2π f (0) = 2π,
and we are left with
N0
2π
 
dλ dSeiSλ f (λ)

dM1M2
1e−iSM2
1

dM1e−iSM2
1
= N0
2π
 
dλ dSf (λ)eiSλ
2iS
(10.14)
The integral over S is simply iπ, whence our ﬁnal result for the mean ﬁeld partition
function:2
MF = N0
4
 ∞
0
dλf (λ)
= N0
4
 ∞
0
e−N0( r
2 λ+ u
4 λ2) dλ
(10.15)
And we’re done. The result in (10.15) is essentially identical to the result dis-
played in (7.57) in Chapter 7.3 The equivalence between (10.15) and an identical
result obtained directly from the path integral formulation of the random walk
2 An important detail is that the integral over S yields this result only if λ is positive. A proper evaluation of the
integral over M1 in (10.14) requires that we replace −isM2
1 in the exponent on the left hand side of the equation
by (−is −ϵ)M2
1, where ϵ is a real, positive inﬁnitesimal. This induces a pole in the integral on the right hand
side of the equation just above the real axis, and the integral over S is now non-zero only when λ > 0.
3 Full correspondence is achieved if we replace N0 in (10.15) by N, λ by 4λ, 4u by f and 2r by (1 −z/zc). We
forge the ﬁnal link in the chain of connection by noting that GMF(z; ⃗x, ⃗y) does not depend on the starting and
ending points, x and y, which means that the sum over end-points results in a multiplication by the total number
of sites in the ﬁnite system.

10.3 Fluctuations
239
provides additional evidence for the validity of the assertion that the magnetic anal-
ogy is appropriate for the discussion of the statistics of the self-avoiding walk and
buttresses the general result established at the end of Chapter 7.
Recapitulating Chapter 7, it was found that in the mean ﬁeld theory limit and
above the critical temperature, the generating function is that of the unrestricted
random walk. In the condensed phase, self-avoidance severely reduces the number
of walks. This reduction is manifested in the factor eNr2/4u. It was also found that,
upon inverting the generating function to ﬁnd the total number of walks in the
condensed phase, the reduction factor in the mean ﬁeld result is identical to the
effect produced by the repulsive interaction term in Flory’s argument for the mean
radius of a self-avoiding walker. This corresponds to the standard interpretation of
the phase transition in the context of self-avoiding polymer statistics.
10.3 Fluctuations: low order spin wave theory
To improve on mean ﬁeld theory, one incorporates ﬂuctuations into the calculation,
and we do that here. Recall (10.2). In the continuum limit, the effective Hamiltonian
becomes
H[S(⃗x)] =

ddx
r
2|⃗S(⃗x)|2 + 1
2| ⃗∇⃗S(⃗x)|2 + u
4|⃗S(⃗x)|4 −⃗h · ⃗S(⃗x)

(10.16)
Low order spin wave theory amounts to expressing the magnetization ⃗S(⃗x) in terms
of a mean ﬁeld contribution, ⃗M(⃗x) and a ﬂuctuating, “spin wave,” ﬁeld ⃗σ(⃗x). We
then retain terms in the effective Hamiltonian to second order in the ﬁeld σ.
The ﬁnal ingredient in the calculation is to impose appropriate boundary condi-
tions on the spin ﬁeld ⃗S(⃗x). In the present context we have two choices for these
boundary conditions: periodic or free. We choose the former to maintain consis-
tency with the mean ﬁeld result. Free boundary conditions possess the desirable
attribute of direct relevance to the statistics of random walks. The nature of the
boundary conditions. Chapter 4 contains a discussion of the effects of boundary
conditions on unrestricted walks; here, we will touch upon the implications of
boundary conditions for self-avoiding walks.
The ﬁrst consequence of free boundary conditions arises at the mean ﬁeld level.
The mean ﬁeld solution for the magnetization, ⃗M(⃗x), is no longer unifom, as free
boundary conditions require that the magnetization vanishes at the surface. The
mean ﬁeld equation of state takes the form
−∇2 ⃗M(⃗x) + r ⃗M(⃗x) + u ⃗M(⃗x)| ⃗M(⃗x)|2 −⃗h = 0
(10.17)
The fact that ⃗M depends on ⃗x complicates matters. However, deep in the ordered
phase (i.e. at low enough temperatures, or when r ≪0) ⃗M(⃗x) ought to be uni-
form throughout the interior of the system, except in the immediate vicinity of the

240
Walks and the O(n) model
bounding surface. The effects of ﬂuctuations can then be assessed with relative
ease. One might well expect that the same situation holds well above the critical
point (r ≫0) as well. This turns out to be, indeed, the case. It is in the immediate
vicinity of the critical point (r ≈0) that things become complicated. Analytical
solutions to the problem of the O(n) model in this regime require a more advanced
technology than presented in this chapter.
Exercise 10.1
Show that (10.17) follows from the minimization of the effective Hamiltonian in
(10.16) with respect to the ﬁeld S(⃗x).
We now return to the case of periodic boundary conditions and the calculation
at hand. We start, as advertised, by expanding the spin ﬁeld into a mean ﬁeld and a
spin wave contribution
⃗S(⃗x) = ⃗M(⃗x) + ⃗σ(⃗x)
(10.18)
Ournotationimpliesapositiondependenceof ⃗M,whichisabsent,giventheperiodic
boundary conditions that apply here. We maintain the notation in the interest of
generality. The effective Hamiltonian in (10.16) splits into contributions that are
zeroth order, ﬁrst order, second order, and so on, in the ﬂuctuating ﬁeld ⃗σ(⃗x).
All terms higher than second order are ignored. The contributions to the effective
Hamiltonian are as follows.
(1) Zeroth order in ⃗σ
HMF =

ddx
r
2| ⃗M(⃗x)|2 + 1
2| ⃗∇⃗M(⃗x)|2 + u
4| ⃗M(⃗x)|4 −⃗h · ⃗M(⃗x)

(10.19)
(2) First order in ⃗σ
H1 =

ddx

r ⃗M(⃗x) −∇2 ⃗M(⃗x) + u ⃗M(⃗x)| ⃗M(⃗x)|2 −⃗h

· ⃗σ(⃗x)
(10.20)
(3) Second order in ⃗σ
H2 =

ddx
1
2| ⃗∇⃗σ(x)|2 + r
2|⃗σ(⃗x)|2 + u
2| ⃗M(⃗x)|2|⃗σ(⃗x)|2 + u

⃗M(⃗x) · ⃗σ(⃗x)
2
(10.21)
The ﬁrst order term vanishes because M(⃗x) satisﬁes the mean ﬁeld equation of state
(10.17).
Exercise 10.2
Find the third order term in the expansion of the effective Hamiltonian with respect
to σ(⃗x).

10.3 Fluctuations
241
We now set ⃗M(⃗x) equal to a constant. The second order Hamiltonian, H2, then
decouples simply with the introduction of Fourier modes. One writes
⃗σ(⃗x) =

⃗q
⃗σ⃗qψ⃗q(⃗x)
(10.22)
where
ψ⃗q(⃗x) =

1
L1
eiq1x1

1
L2
eiq2x2 · · ·

1
Ld
eiqdxd
(10.23)
The normalized modes in (10.23) are appropriate to a d-dimensional rectangular
solid. The boundary conditions are satisﬁed if the qi’s take on the following discrete
values
qi = 2niπ
Li
(10.24)
the ni’s being positive and negative integers.
When it is expressed in terms of the Fourier amplitudes, ⃗σ⃗q, of the spin wave,
the second order effective Hamiltonian separates into a sum of contributions, each
due to an individual mode. The decoupling is as follows
H2 =

⃗q
r
2 + 1
2q2 + 1
2u| ⃗M|2

⃗σ⃗q · ⃗σ⃗q + u

⃗M · ⃗σ⃗q
2
−⃗h · ⃗σ⃗q⟨φ|ψ⃗q⟩

(10.25)
In (10.25) the external ﬁeld is written in the form ⃗h(⃗x) = ⃗hφ(⃗x), and ⟨φ|ψ⃗q⟩rep-
resents the usual integration of a product of functions. The partition function then
factors nicely
Z =

dnMe−HMF
 
⃗q
dnσ⃗qexp

−

⃗σ⃗q ·
↔
X ⃗q · ⃗σ⃗q + ⃗h · ⃗σ⃗q⟨φ|ψ⃗q⟩

(10.26)
where the tensor operator
↔
X ⃗q is given by
↔
X ⃗q=
r
2 + 1
2q2 + 1
2u| ⃗M|2

↔
I +u| ⃗M⟩⟨⃗M|
(10.27)
The quantity
↔
I in (10.27) is the identity operator.
According to the prescription outlined in the supplement at the end of Chapter 9,
the generalized Gaussian integrals are easily performed once the eigenvalues and
the inverse of the matrix
↔
X ⃗q are known. Both are simply found. The matrix
↔
X ⃗q has

242
Walks and the O(n) model
two distinct eigenvalues:
λ1 = r
2 + 1
2q2 + 1
2u| ⃗M|2
(10.28)
λ2 = r
2 + 1
2q2 + 3
2u| ⃗M|2
(10.29)
with degeneracies n −1 and 1, respectively.
Exercise 10.3
Verify that the eigenvalues of the operator
↔
X ⃗q are as given by (10.28) and (10.29).
Derive the degeneracies of those eigenvalues.
The inverse of this operator is given by
↔
X
−1
⃗q
=
↔
I
λ1
−u| ⃗M⟩⟨⃗M|
λ1λ2
=
↔
I
r/2 + q2/2 + u| ⃗M|2/2
−
u| ⃗M⟩⟨⃗M|

r/2 + q2/2 + u| ⃗M|2/2
 
r/2 + q2/2 + 3u| ⃗M|2/2

(10.30)
Exercise 10.4
Verify that
↔
X
−1
⃗q
↔
X ⃗q=
↔
X ⃗q
↔
X
−1
⃗q =
↔
I
(10.31)
where
↔
X
−1
⃗q
is as deﬁned in (10.30).
Armed with these results and the general formulas for Gaussian integrals, we are
able to perform the indicated integrals. The result of these integrations is

⃗q
πn/2
√λ2

λn−1
1
exp

−1
4
⃗h⟨φ|ψ⃗q⟩·
↔
X
−1
⃗q
· ⃗h⟨φ|ψ⃗q⟩

(10.32)
We are interested in the value of the above expression in the limit n →0. The factor
in (10.32) that depends on the λ’s is then replaced by

λ2
λ1
=

r + q2 + u| ⃗M|2
r + q2 + 3u| ⃗M|2
(10.33)

10.3 Fluctuations
243
The ﬁnal result for the partition function is, then

⃗q

r + q2 + u| ⃗M|2
r + q2 + 3u| ⃗M|2

dnMexp

−N0
r
2| ⃗M|2 + u
2| ⃗M|4
+

⃗M · ⃗h(⃗x) ddx

× exp
 
ddx ddx′ ⃗h(⃗x) · ⃗h(⃗x′)
4
G0(⃗x, ⃗x′)
−1
4
 
ddx ddx′ 
⃗h(⃗x) · ⃗M
 
⃗h(⃗x′) · ⃗M

D(⃗x, ⃗x′)

(10.34)
where
G0(⃗x, ⃗x′) =

⃗q
ψ⃗q(⃗x)ψ⃗q(⃗x′)
r + q2 + 3u| ⃗M|2
(10.35)
and
D(⃗x, ⃗x′) =

⃗q
ψ⃗q(⃗x)ψ⃗q(⃗x′)

r + q2 + u| ⃗M|2
 
r + q2 + 3u| ⃗M|2

(10.36)
We will deal with each of the terms appearing in (10.34) after we have obtained the
ﬁnal result for the generating function. To arrive at that result, we take two steps.
First, the coefﬁcient of the quadratic term in the expansion of the partition function
in powers of ⃗h must be found. Then, it is necessary to evaluate the n →0 limit of
the expression obtained as the result of the ﬁrst step. Both steps are readily taken,
given the equation
G(z; ⃗x, ⃗x′) = lim
n=0
1
n
∂2 ln Z
∂h(⃗x)∂h(⃗x′)
(10.37)
relating the partition function Z to the generating function for self-avoiding random
walks. All n components of the ordering ﬁeld, ⃗h, are equal. For details of the
connection see Chapter 9.
The ﬁrst step involves a straightforward expansion of the exponential. Three
terms are generated, leading to the following result:

dnM exp

−N0
r
2| ⃗M|2 + u
4| ⃗M|4
×
1
2

⃗h(⃗x) · ⃗M ddx + 1
4
 
⃗h(⃗x) · ⃗h(⃗x′)G0(r; ⃗x, ⃗x′) ddx ddx′
−1
4
  
⃗h(⃗x) · ⃗M
 
⃗h(⃗x′) · ⃗M

D(⃗x, ⃗x′) ddx ddx′

(10.38)
The ﬁnal step is also straightforward. We recall that the multidimensional integral
over the components of ⃗M can be reduced to an integral over a single scalar variable,

244
Walks and the O(n) model
λ, in the limit n →0. The two types of integral occurring in (10.38) are of the form

f (| ⃗M|2) dnM
(10.39)
and
 
⃗h(⃗x) · ⃗M
 
⃗h(⃗x′) · ⃗M

f (| ⃗M|2) dnM
(10.40)
Withtheabovesubstitutionfor(| ⃗M|2),andwiththeuseoftheintegralrepresentation
of the delta function δ(λ −| ⃗M|2), the n →0 limit of the above two integrals reduce
to
 ∞
−∞
f (λ)δ(λ) dλ = 1
(10.41)
and
⃗h(⃗x) · ⃗h(⃗x′)
2
 ∞
−∞
f (λ) dλ
(10.42)
respectively, where f (λ) is as given in (10.15). Reassembling the terms in (10.38)
after the integrations over ⃗M have been taken, we are left with
⃗h(x) · ⃗h(x′)
4

G0(⃗x, ⃗x′) −
 ∞
0
f (λ) dλ

D(⃗x, ⃗x′)

(10.43)
We have now reduced the calculation of the correlation function of the O(n)
model, equivalent to the generating function of the self-avoiding walk, to the eval-
uation of second derivative with respect to the amplitude of an n-dimensional
magnetic ﬁeld that has all coefﬁcients equal.4
To summarize, there are now two cases to consider.
Case 1: r ≫0
Here we are high above the critical point. The dominant contribution comes from
the ﬁrst integral, (10.41), and the number of walks starting out at ⃗x and ending at
⃗x′ is essentially given by
G0(⃗x, ⃗x′) =

⃗q
ψ⃗q(⃗x)ψ⃗q(⃗x′)
r + q2
→
1
(2π)d

ddq ei⃗q·(⃗x−⃗x′)
r + q2
(10.44)
4 Again, see Chapter 9.

10.3 Fluctuations
245
As we will see, the condition r ≫0 implies that the linear extent of the walk is
much smaller than the size of the container in which it ﬁnds itself. The conﬁning
volume places no constraints on the walk. The fact that the ψ⃗q(⃗x)’s are exponential
functions instead of plane waves reﬂects the periodic boundary conditions that
apply.
Case 2: r ≪0. The condensed phase and the correlation hole
Now the walks have a linear extent that greatly exceeds the dimensions of the
conﬁning volume. Self-avoidance dominates. The major contribution to the integral
over λ occurs in the region λ ≈|r|/u, where the integral is sharply peaked. Thus,
the integral in (10.41) does not contribute, while the integral in (10.42) is accurately
represented by a steepest-descent calculation, similar to those performed earlier.
The starting point of the integration contour is λ = |r|/u. Combining this result with
the mean ﬁeld term appearing in (10.34) gives for the number of walks beginning
at ⃗x and ending at ⃗x′, in the condensed phase the result
 ∞
0
e−N0(rλ+uλ2) dλ

1 −D(⃗x, ⃗x′)

(10.45)
We recognize the ﬁrst term in (10.45) as the mean ﬁeld contribution, so the effects of
ﬂuctuations, at least within low order spin wave theory, give rise to the second term.
This term – which we call a correlation hole – suggests the following picture of the
condensed phase. The volume occupied by the walk is comparable to the volume of
the box in which it is conﬁned. We therefore expect that the walker will try to avoid
regions in which there is a high concentration of crossings, thereby stretching out
the walk. Recall Flory’s excluded volume argument for polymers. This “stretching”
effect manifests itself through the two-point correlation function D(⃗x, ⃗x′), which
increases with increasing separation. One can think of this as giving rise to an
effective repulsion between endpoints of the walk. Asymptotically, the repulsive
potential has a Coulomb spatial dependence. That is, D(⃗x, ⃗x′) ∝|⃗x −⃗x′|2−d. This
can be seen simply as a change in variables in the expansion for D(⃗x, ⃗x′):
D(⃗x, ⃗x′) =

⃗q
ψ⃗q(⃗x)ψ⃗q(⃗x′)
q2 
2|r| + q2
(10.46)
Replacing the discrete set of ⃗q ’s by a continuum, we ﬁnd that D(⃗x, ⃗x′) is propor-
tional to the integral

ddq
ei⃗q·(⃗x−⃗x′)
q2 
2|r| + q2
(10.47)

246
Walks and the O(n) model
|x – x'|
| x – x'|
–D(| x – x'|)
Fig. 10.2. Showing how the correlation hole affects conﬁgurations in which the
end of the self-avoiding walk – or the excluded-volume polymer – approaches the
point at which it starts. Also shown in this ﬁgure are the monomeric units that
coexist with the longer polymer.
When |⃗x −⃗x′| ≫1/|r|, the integral is dominated by small |⃗q| and the spatial de-
pendence scales out of the integral, yielding the asymptotic form
D(⃗x, ⃗x′) ∝
1
|⃗x −⃗x′|d−2
(10.48)
as stated. This picture of the condensed state emerges from more general arguments
than the speciﬁc calculation presented in this section. Figure 10.2 illustrates how
the correlation hole emerges. The walk, or the polymer, is long compared to the
linear dimensions of the container to whose interior it is restricted. If the ends of
the walk are not too close, the generating function is independent of the start and
end-points. However, if the origin and point of termination of the walk approach
each other, the repulsive interaction, real or effective, that enforces self-avoidance
acts to reduce the weight of this conﬁguration. A correlation hole corresponding to
a three-dimensional walk is shown alongside the ﬁgure.
Exercise 10.5
Show that (10.48) follows from (10.46) and (10.47).
10.3.1 The condensed phase – compact walks
From our discussion of the correlation effects displayed in the results of a spin
wave calculation, the following picture of the condensed phase emerges. Above the
critical point, where r ≫0, the extent of a walk is small in comparison to the di-
mensions of the box, and mean ﬁeld theory adequately describes the gross features.
Below the critical point, where r ≪0, the walks are very compact, occupying a

10.3 Fluctuations
247
substantial fraction of the available volume. Self-avoidance now plays a signiﬁcant
role in the statistics of the walk. One important manifestation of this effect is a
strong statistical repulsion between the end-points of the walk, which takes the
form of a “correlation hole.” The third regime of interest, the immediate vicinity
of the critical point, in which r ≈0, separates the two above. Here, the walks have
a “natural” extent that is the order of the linear dimensions of the box. Modiﬁca-
tions of the mean ﬁeld approximation, due to ﬂuctuations become very important,
and a low order spin wave calculation is inadequate. More advanced calculational
approaches are necessary to tackle this problem.
This picture of the condensed phase is consistent with a description of the phase
transition put forth by Redner and Reynolds (1981). Though their model lacks a
complete theoretical justiﬁcation, it has intuitive appeal and we present it here.
Recall that the generating function for the total number of walks with ﬁxed
starting point has the form
(r) =

N
Ne−r N
(10.49)
Now, , as suggested by the mathematical similarity of the expression in (10.49)
to the grand canonical distribution of statistical mechanics can be viewed as the
partition function of an ensemble of a single random walk with number of steps, N,
the statistics of which are governed by a grand canonical probability distribution.
Here, r plays the role of the chemical potential. As in statistical mechanics, we
deﬁne a grand potential
 = 1
V ln 
(10.50)
from which the “average” properties of the walk can be determined. Of particular
interest will be the average number of steps taken by the walker, ⟨N⟩. Obviously,
⟨N⟩= −∂
∂r ln  = −V ∂
∂r
(10.51)
We further deﬁne the average density of steps by
⟨N⟩
V
= −∂
∂r
(10.52)
It is instructive to evaluate ⟨N⟩with the use of the mean ﬁeld treatment of self-
avoidance. In Chapter 7, our mean ﬁeld result for (r) was found to be
MF = V
ad
1
2r
r > 0
= 1
2
π
u
1/2  V
ad
3/2
e−V
ad
r2
4u
r < 0
(10.53)

248
Walks and the O(n) model
Differentiating ln MF, one ends up with a grand potential of the form
MF = 1
V {ln V −lnr + const.}
r > 0
(10.54)
= 1
V

ln V −V
ad
r2
4u + const.

r < 0
(10.55)
Note the interesting behavior of the density above and below the transition:
⟨N⟩
V
= 1
V
1
r

r > 0
(10.56)
=
|r|
2adu
r < 0
(10.57)
In the thermodynamic limit (N and V going to inﬁnity such that N/V approaches
a constant) the density behaves as one might expect an appropriate order parameter
characterizing the phase transition might behave:
ρMF = 0
r > 0
̸= 0
r < 0
(10.58)
suggesting that in the condensed phase the volume occupied by the walk is of the
order of the volume of the conﬁning space. Above the transition, the walk occupies
a vanishingly small portion of the available volume. This behavior of the quantity
ρ indicates that it might serve the purpose of an order parameter for the “transition”
between the limiting case of a self-avoiding walk whose natural extent is small
compared to the spatial region to which it is conﬁned and the alternate limiting
case of self-avoiding walk that when unconstrained would occupy a signiﬁcantly
larger volume than is contained in the boundaries surrounding the region in which
it propagates. Given this deﬁnition, we will demonstrate that the way in which
the new order parameter behaves can be quantiﬁed in terms of a set of critical
exponents, and that the scaling relations that connect the new critical exponents are
understandable in terms of the scaling form that is taken by the correlation function
from which the density is derived. In the remainder of this section we will establish
the critical exponents of the new order parameter. Then we will explore the source
of these exponents by looking at the behavior of the “thermodynamic” system near
the critical point. In this way we will establish the relationship between the two
sets of critical exponents and, at least in the mean ﬁeld approximation, determine
the value of the new set.
Let’s take up the latter task ﬁrst. From (10.58) we see that ρMF →|r|, when
r < 0. From this we immediately discover that the critical exponent β′ is equal to
1. Note that this value differs from the mean ﬁeld value of the magnetic exponent
β, which has the value 1/2 in the mean ﬁeld approximation. We will establish the

10.3 Fluctuations
249
relationship between the two sets of exponents later on in this section. For the
moment we distinguish between the two by using primes to indicate exponents
of the self-avoiding walk and unprimed exponents for the magnetic system.5 The
other critical exponents of the walk are evaluated in the usual manner: observe that
a second derivative of the grand potential, , leads to the ﬂuctuation formula
∂2
∂r2 = 1
V

⟨N 2⟩−⟨N⟩2
(10.59)
which, with the use of the standard ﬂuctuation response relation,6 allows us to
deﬁne the susceptibility of the walk system. We thus ﬁnd
χ′ = ∂2
∂r2 →|r|−γ ′
(10.60)
which in mean ﬁeld approximation leads to γ ′
MF = 0. However, the expression
∂2/∂r2 can equally well be interpreted, when the variable r is recognized as the
temperature, as the speciﬁc heat, with its own critical exponent α′. Thus,
∂2
∂r2 →|r|−α′
(10.61)
and we see that α′ = γ ′, quite generally. The equality between the speciﬁc heat
and the susceptibility exponents follows directly from the fact that there exists a
single scaling parameter for our system; temperature and magnetic ﬁeld play no
role. . . at least as long as we restrict the system to represent a single self-avoiding
walk. When there are several walks, all of them having an undetermined number of
steps, both the temperature and magnetic ﬁeld are relevant to a description of the
statistical properties of the ensemble.
The ﬁnal critical exponent of our ﬁctitious thermodynamic system describes the
behavior of the correlation length. Here things are simple. The exponent ν′ is the
same for both the thermodynamics of the walk and the magnetic system. In the
mean ﬁeld approximation, ν′
MF = 1/2.
In summary, the new mean ﬁeld exponents are
β′
MF = 1
(10.62)
γ ′
MF = α′
MF = 0
(10.63)
ν′
MF = 1/2
(10.64)
5 We do this in the full realization that that there is the possibility of confusion, in that primes are also used for
thermodynamic exponents in the low-temperature phase.
6 See, e.g. (9.25) in Chapter 9.

250
Walks and the O(n) model
As we are about to show, it is no coincidence that these new exponents satisfy
the usual scaling laws
α′ + 2β′ + γ ′ = 2
(10.65)
2 −α′ = dcν′
(10.66)
which tell us that the upper critical dimensionality, dc is 4, as expected. The scaling
relations (10.65) and (10.66) are quite general and follow in a straightforward way
from the scaling properties of the correlation function. As a by-product, we are able
with a minimum of effort to arrive at the relation between the two sets of critical
exponents – those of the self-avoiding walk and those of the magnetic system.
We have already established that the generating function G(r; ⃗Ri −⃗R j) was
simplythetwo-pointcorrelationfunctionforthespins.Weknowhowthecorrelation
function scales near the critical point, and, therefore, how G scales:
G ∼
1
Rd−2+η F

r L1/ν, R
L

(10.67)
where R = |⃗Ri −⃗R j|, and L is the characteristic linear extent of the system.
From our deﬁnition of , we see that
(r) =

⃗R j
G

r; ⃗Ri −⃗R j

(10.68)
Equation (10.68) implies that  scales as follows
(r) = L2−η f (r L1/ν)
(10.69)
Since log  scales as the function f in (10.69), we can write
(r) ∼L−d H(r L1/ν)
(10.70)
from which we arrive at the following scaling form for the order parameter ρ
ρ = ∂
∂r = L1/ν−d H ′(r L1/ν)
(10.71)
Now, we argue that the order parameter must remain ﬁnite in the “ordered” phase, as
L becomes arbitrarily large. This requires that H ′(r L1/ν) →rνd−1L(νd−1)/ν. This
leads to a cancellation of the L-dependence of ρ in (10.71) and yields
ρ →rνd−1
(10.72)
From this we are able to extract β′ as given by
β′ = dν −1
(10.73)

10.4 The correlation hole
251
Table 10.1. The new critical exponents
and their relationship to previously
deﬁned quantities.
β′ = 1 −α
γ ′ = α
α′ = α
ν′ = ν
dc = 4
The remaining exponents follow by noting the following two points:
(1) νd is also equal to 2 −α
(2) α = α′, since both are derived from the speciﬁc heat. This also leads to γ ′ = α.
These results are summarized in Table 10.1.
From Table 10.1, we see immediately that the new exponents for the walk satisfy
the usual scaling laws
α′ + 2β′ + γ ′ = 2
(10.74)
2 −α′ = dcν′
(10.75)
which hold in general. This establishes the consistency of the grand canonical
ensemble interpretation of the generating function. By deﬁning a thermodynamic
system corresponding to the ensemble of walks in a natural way we are able to
deduce a description of the condensed phase as one in which walks are extremely
compact and ﬁll the allowed volume. Our thermodynamic analysis of the phase
transition, based in part on the scaling properties of the generating function, and
the picture that emerges is in complete accord with what was found with the use of
an independent approach.
10.4 The correlation hole
In order to understand a striking contribution to the random-walk-generating func-
tion engendered by the ﬂuctuation contribution – in particular the “correlation
hole” – it is useful to consider the way in which the self-avoiding walk “sees” itself
as it propagates in the highly condensed phase. We start by noting that the effects
of self-avoidance are concentrated in energetic effects, arising from a Boltzmann-
like factor associated with the energy cost of self-intersection. On the average, one
expects that the probability that a walker will have stepped on a particular location,

252
Walks and the O(n) model
R'
point of origin
R
reference point
end-point
Fig. 10.3. The number of steps a distance R away from the starting point of the
random walk.
⃗r, will be weighted by the factor e−uρ(⃗r), where ρ(⃗r) is the number of steps in the
immediate vicinity of the point in question. If the strength of the interaction is not
great, there will be a modiﬁcation of the walk statistics that goes like ∼−uρ(⃗r). In
order to see just how this modiﬁcation goes, it is necessary to calculate the number
of steps there will be in a random walk at a certain distance from the point of origin
of that walk.
We consider an unrestricted walk that starts out at some point in space. For
convenience, that point will be given the position vector ⃗r = 0. We then ask how
many steps there will be at a distance R away from the starting point. The number
of walks that end a distance R away from where the walker starts can be obtained
directly from the generating function
G(z; ⃗R) =

ei⃗k·⃗R
k2 + zc −z ddk
(10.76)
We are interested in a more general quantity here: the number of walks for which
one or more of the steps takes the walker a distance R away from the point of origin.
The generating function that yields this result is

dd R′

ddk1

ddk2
ei⃗k1·⃗R
k2
1 + zc −z × e
i⃗k2·

⃗R′−⃗R

k2
2 + zc −z
(10.77)
(see Figure 10.3). Integrating over ⃗R′ we obtain a delta function that restricts the
vector ⃗k2 to be equal to zero. We are left with
!
ddk1
ei⃗k1·⃗R
k2
1 + zc −z
"
×
1
zc −z
(10.78)

10.4 The correlation hole
253
The coefﬁcient of zN in the above expression is the coefﬁcient of zN in

dk
1
∞

N1=0
ei⃗k1·⃗R
!
zN1

k2
1 + zc
N1+1
"
×
∞

N2=0
 zN2
zN2+1
c

(10.79)
In order to extract the desired coefﬁcient, we must evaluate the following sum
1
zc
1
zc + k2
1
N

n=0

zc
zc + k2
1
n
= 1
zc
1
k2
1

1 −

1 + k2
1/zc
−(N+1)
(10.80)
In the limit of large N, this expression becomes
1
zck2
1

1 −e−(N+1)k2
1/zc
(10.81)
The number of steps a distance R away from the starting point is, then, given by

ddk1
ei⃗k1·⃗R
zck2
1

1 −e−(N+1)k2
1/zc
(10.82)
Carrying out the integration over ⃗k1, we obtain for the number of steps a distance
R away from the walker’s point of origin
1
4πzcR erfc

zcR2
2(N + 1)

(10.83)
where erfc(x) is the complementary error function, deﬁned by
erfc(x) =
2
√π
 ∞
x
e−t2dt
(10.84)
The function in (10.83) falls off as 1/R when R ≤√2N/zc, when R > √2N/zc,
the fall off is as a Gaussian.
Now, when the walk is densely packed, the region to which it is conﬁned is
small compared to the walk’s natural extent, so we may assume that the inequality
R ≪√N/zc is always satisﬁed. This means that ρ(R) ∼R−1 in three dimensions
(ρ(R) ∼R−(d−2) in general). There will thus be a correction to the number of
walks going as −u/R, which is precisely the result that we ﬁnd for the effect of
ﬂuctuations on the statistics of the random walk.


11
Scaling, fractals, and renormalization
We are almost ready to fully exploit the connection, established in earlier chapters,
betweenthestatisticsofaself-avoidingrandomwalkandthestatisticalmechanicsof
a magnet near the phase transition from its paramagnetic and ferromagnetic states.
Because of the mathematical similarity between the two systems, we will be able to
make use of an array of calculational strategies that, collectively, represent realiza-
tions of the renormalization group. This generic method for the study of systems
with long-range correlations has fundamentally altered the way in which physicists
view the world around them. The method is so powerful and so widespread in its
application, that it seems worthwhile to do a little more than simply explain how
to use it in the present context. This chapter consists of a discussion of the phi-
losophy underlying the renormalization group and of a general description of the
way in which it is applied. We will ﬁnish off by taking the reader through a simple
calculation that is relevant to random walks and the associated magnetic system.
Then, we will generalize the method to encompass a wide class of systems, the
O(n) model being one of them. In the next chaper, the reader will be subjected
to a full-blown introduction to the method, as it applies to the self-avoiding walk.
Those already familiar with the renormalization group may wish to skip directly to
Chapter 12.
11.1 Scale invariance in mathematics and nature
The notion of scale invariance is not exactly new. A famous poem by Jonathan
Swift goes as follows:
So, naturalists observe, a ﬂea
Has smaller ﬂeas that on him prey,
And these have smaller still to bite ’em
And so proceed ad inﬁnitum.1
1 This notion was enlarged upon by the mathematician Augustus De Morgan, who wrote:
Great ﬂeas have little ﬂeas upon their backs to bit ‘em
And little ﬂeas have lesser ﬂeas, and so ad inﬁnitum
And the great ﬂeas themselves, in turn, have greater ﬂeas to go on,
While these again have greater still, and greater still, and so on.
255

256
Scaling, fractals, and renormalization
That the world is replicated on smaller and smaller scales – ad inﬁnitum, as the
poem asserts – is a seductive one. Of course, we now know that life on the scale
of whatever bites a ﬂea is different from life on the scale of a ﬂea, and that life on
the ﬂea’s scale, in fact, differs in important respects from life on the scale of the
animals that are bitten by them. Nevertheless, there are cases in which phenomena
are replicated on smaller and smaller, or larger and larger, length scales. in these
instances, there is said to be scale invariance, and it is on such cases that we will
begin our discussion.
11.1.1 Mathematical scale invariance: fractals
Let’s start with the simplest possible case in which scale invariance manifests
itself. Consider a point. This mathematical ﬁction is a zero-dimensional object
that has no spatial extent in any direction. You cannot acually see a point, as it is
vanishingly small, and is thus beyond the range of the unaided eye or any apparatus
built for magnifying small objects. Nevertheless, we’ll talk about what you “see”
when you look at a point. The important thing about a point is that it is a point
no matter the magniﬁcation with which it is viewed. In this sense, a point exhibits
scale invariance. It is looks the same under all magniﬁcations. This is trivial scale
invariance.
Another example is the case of a straight line. Again, because the ideal line is
inﬁnitesimally thin, you cannot see it, but if you could, the straight line – which
extends out to inﬁnity in one dimension, but which has no width – would look like
a straight line under any magniﬁcation. Here again, there is essentially trivial scale
invariance.
If we curve the line, by making it, for instance, the circumference of a cir-
cle, then what we end up with is something that does not look the same under
all magniﬁcations. The closer you look at a curved line, the more it appears to
straighten. This reﬂects the mathematical fact that a curve possesses a length scale.
In the case at hand, the length scale is established by the radius of the circle of
which the curve is the circumference. The more you magnify the curve, the greater
the radius appears to be. In the limit of inﬁnite magniﬁcation, under which the
circle appears to be inﬁnitely large, the curve has straightened out and becomes
a straight line. On the other hand, as you pull further and further back, so that
the circle becomes smaller and smaller, the curve eventually starts to resemble a
point.
In general, a curved line is an example of a mathematical construction that has
an intrinsic scale. Are there cases of curves that possess scale invariance, and for
which the scale invariance is a little more complicated than in the example of the

11.1 Scale invariance in mathematics and nature
257
Fig. 11.1
The ﬁrst step in the construction of a Koch curve.
Fig. 11.2. The next step in the construction of a Koch curve.
point or the line? The answer is yes. These curves are known as fractals. Just what
is a fractal? Mathematically speaking, a fractal is an entity that has a fractional
dimensionality. We’ll see what that means after we’ve constructed a fractal. We’ll
start simply, with the Koch curve.2
The Koch curve
Recall what happened to the straight line under magniﬁcation. Its apparent curvature
vanished. This means that any “features” that a curve exhibit are magniﬁed out of
visibility as our view of the curve is taken to smaller and smaller length scales.
We can see to it that the curve itself does not straighten out into a line by placing
smaller features on top of features, and even smaller features on top of them, much
in the same way as Jonathan Swift imagined littler and littler ﬂeas living on each
other. The feature in the case of the Koch curve is triangular, and it is placed on the
middle third of a straight line segment. Figure 11.1 illustrates the process. Notice
that one of the consequences of the addition of the triangular peak to the center
of the line segment is that the length of the curve has increased by a factor of
4/3. This is because the new curve consists of four segments, each of which has
a length equal to a third of the original line. The next step in the construction of
the Koch curve is to modify the four line segments that we now have in exactly
the same way as we did the original line segment (see Figure 11.2). Instead of
four segments there are now 4 × 4 = 16 of them, and the length has increased by
another factor of 4/3. At the next step, we decorate each of these 16 segments
in the same way as we have decorated the segments previously, and we continue
in the fashion. . . ad inﬁnitum. The result of this procedure is a curve-like object
that displays interesting structure at any degree of magniﬁcation. The object has
another interesting property. Its length is inﬁnite. This is because at every stage in
its construction you have increased the length of the curve by the factor 4/3, and
2 For a discussion of the Koch curve, and fractals in general, see (Mandelbrot, 1982).

258
Scaling, fractals, and renormalization
Fig. 11.3. Something like a Koch curve.
Fig. 11.4. The steps in the construction of the Cantor set.
you have done this an inﬁnite number of times. Figure 11.3 is an attempt to depict
what a Koch curve looks like. It is impossible to draw, just as it would be impossible
to see, all the structure in the curve. There is the strong intimation of details upon
details that would show up under sufﬁcient magniﬁcation.
The Koch curve is one of the simplest examples of a fractal curve. This particular
fractal is one of a class that goes by the name deterministic fractal. As another
example of this class, consider the Cantor set, constructed in the following manner.
First, remove the middle third sction of a straight line segment. There are now two
segments, each one third the length of the original line. Then, eliminate the middle
third of each of these two segments to generate a total of four segments, each a
ninth of the length of the original line. Continue in this process, again, ad inﬁnitum.
What you are left with constitutes a fractal, similar in properties to the Koch curve,
except that now its total length is zero. The successive steps in the construction of
the Cantor set are illustrated in Figure 11.4.
Fractals need not be geometrically precise – they can also be generated by random
processes. Examples of such processes include the shoreline of a section of coast,
of the trails left by a collection of random walkers. Fractals belonging to this class
are known as random fractals. Other examples of both deterministic and random

11.1 Scale invariance in mathematics and nature
259
L
L
radius = 
radius = 
L
L/3
Fig. 11.5. Finding the fractal dimensionality of the Koch curve.
fractals can be found in books by Mandelbrot, which provide a lucid introduction
to the general topic (e.g. Mandelbrot, 1982).
Now we turn to the etymology of the term fractal, which is short for fractional di-
mensionality. As it turns out, there are quite a few ways to deﬁne the dimensionality
of a mathematical set. We will use one here, corresponding to what is now called
the Hausdorff dimensionality. It corresponds to the “amount” of a curve or other
structure that is contained in a compact volume of some linear extent. In the case
of the Koch curve, imagine drawing a circle of radius r centered at one end of the
curve. We ask how much of the Koch curve is inside that circle. Let the end-to-end
distance of the Koch curve be L. Then if R ≥L, all the Koch curve is contained in
the circle. Start with R = L. Now shrink the radius of the circle to R/3. One quarter
of the Koch curve is now contained in the circle (see Figure 11.5). If the radius of
the circle is shrunk by another factor of three, another three quarters of the Koch
curve is eliminated from the interior of the circle. Now, we deﬁne the amount of
the Koch curve that is contained within a circle of radius R by n(R). The exercise
that we have just performed indicates that n(R) has the property
n(R/3) = n(R)/4
(11.1)
Suppose we make the assumption that n(R) is in the form of a power law, that is
n(R) = K R p
(11.2)
Equation (11.1) then takes the form
K
 R
3
p
= K R p
4
(11.3)
Eliminating the multiplicative constant K and the factor R p from both sides of
(11.3), we ﬁnd
3−p = 1/4
(11.4)

260
Scaling, fractals, and renormalization
Taking the logarithm of both sides of (11.4), we ﬁnd
−p ln 3 = −ln 4 →p = ln 4
ln 3
(11.5)
The power connecting R, the size of the region, and n(R), the amount of the Koch
curve inside the region is ln 4/ ln 3. This exponent p in the power-law dependence
of n on R is the Hausdorff dimensionality of the Koch curve.
The reason that the exponent is called a dimension has to do with what one
ﬁnds if one performs the corresponding calculation for simple curves and surfaces.
Suppose we were interested in the amount of a line segment that would be contained
in a circular region when the circle is centered at the end of the line segment and has
a radius equal to R. If R is less than the length of the line, the amout of line segment
inside the circle is just equal to the length of the portion of the line that lies inside
the circle, and that length is R. Thus n(R) = R, and the power law that one in-
serts in an equation such as (11.2) is one, which is the dimensionality of a line.
Exercise 11.1
Show that if the object of interest is a two-dimensional surface, then the amount of
the object inside a circle of radius R scales as R2.
Seen from the above perspective, the fractal dimensionality is a generalization of
the notion of spatial dimensionality, with the added property that fractal dimension-
alities need not be integral. The fractal dimensionality of a Koch curve lies between
one and two, so in a sense, the Koch curve represents a kind of interpolation between
a line and a plane.
Exercise 11.2
Repeating the argument above in the case of the Cantor set, determine the fractal
dimensionality of this object.
Another quick way to calculate the fractal dimensionality of certain geometric
fractals is to count the replicas, R, of the similar objects, and the magniﬁcation,
M, of one of them that is needed to reproduce the object from which the replicas
were derived. The fractal dimensionality is then given by the formula
p = ln R
ln M
(11.6)
Applying this procedure to the Koch curve and the Cantor set immediately yields
the results just presented for their fractal dimensionalities. This scaling feature of

11.1 Scale invariance in mathematics and nature
261
fractals is made prominent by denoting their Hausdorff dimension as the scaling
dimension.
Exercise 11.3
Show that the method just described yields the same fractal dimensionalities for
the Koch curve and the Cantor set as the method described earlier in this section.
To review the determination of the fractal dimensionality of the Koch curve, we
established a way of determining how much of the curve there is within a given
length of a point. We also saw how that quantity varies as the length is changed.
We found that when the length is changed by one factor, the amount changes by
another. On the basis of this ﬁnding, we are able to infer a power-law relationship
between the length and the amount of the fractal. This all works for an unrestricted
(non-excluded volume) random walk. Here, the amount of the walk inside a region
of linear extent R ought to go as R2. This is because the distance that a walker travels
in N steps goes like N 1/2, and we can interpret the “amount” of a random walker in
a region as the number of steps within that region. The fractal dimensionality of an
unrestricted random walk is, then equal to two. Our goal is the fractal dimensionality
of the self-avoiding walk.
The renormalization group is based on a generalization of the method described
above. One looks at the relationships between the properties of a system as viewed
on one length scale and the same properties as observed on another one. The
relationships are generally expressible in terms of ratios. On the basis of the speciﬁc
ratios discovered, it is possible to construct power-laws for the dependences of
quantities of interest on underlying variables.
11.1.2 Examples of the statistical mechanical renormalization group. The
mean ﬁeld magnet and the Gaussian model
Mean ﬁeld theory, as discussed in Chapter 9, asserts the following power-law de-
pendences of the magnetic susceptibility
χT (T ) ∝(T −Tc)−1
(h = 0, T > Tc)
(11.7)
χT (T ) ∝(Tc −T )−1
(h = 0, T < Tc)
(11.8)
Here, h is the externally applied magnetic ﬁeld, and the quantity χT (T ) =
(∂M/∂h)T

h=0 is the isothermal magnetic susceptibility at zero applied magnetic
ﬁeld. Is there a way in which this power law can be inferred from the way in which
the system looks at different length scales? Recall the Landau theory result for the

262
Scaling, fractals, and renormalization
effective Hamiltonian of the magnetic system:
H(r, h, m) = V

rm2 + um4 −hm

(11.9)
where r is the reduced temperature, ∝(T −Tc), of this magnetic system, and m
is the magnetization per unit volume, which is equal to ML−d. The quantity V in
(11.9) is the volume of the system, equal to Ld, where L is the linear extent of the
system and d is the system’s spatial dimensionality. Now, imagine that we change
our frame of reference by altering the length scale on which we view the system. If
we magnify the system by the factor b, then L →bL. Each term in (11.9) is then
multiplied by bd, because of the change in the common factor V . We can absorb this
change in the effective Hamiltonian by making m →m′ = mbd/4, r →r′ = rbd/2,
and h →h′ = hb3d/4.The equilibrium magnetization is the solution to the magnetic
equation of state: ∂H/∂m = 0, or, equivalently, ∂H/∂m′ = 0. This means that the
magnetization per unit volume of the system has the property
m(r, h)bd/4 = F

rbd/2, hb3d/4
(11.10)
or
m(r, h) = b−d/4F

rbd/2, hb3d/4
(11.11)
Because parameter b is extraneous to the actual physical behavior of the system,
the right hand side of (11.11) ought to be independent of b. Utilizing the fact that
we are able to construct the functional dependence of m on the variables r and h,
(11.11) reduces to
m(r, 0) = b−d/4F

rbd/2, 0

(11.12)
As r →0, if m(r, 0) is to be independent of the scaling parameter b, then this
function must obey the power law
m(r, 0) = κr1/2
(11.13)
as it should. In the other limiting case, when we are at the critical temperature
r = 0, m is consistent with the functional form
m(0, h) = b−d/4F

0, hb3d/4
(11.14)
Again, the b-dependence drops out if m(0, h) has the following power-law behavior
m(0, h) = κ′h1/3
(11.15)
Quite generally, if the magnetization is to be independent of the scale factor b, its
functional dependence on the variables r and h must be of the form
m(r, h) = r1/2 f (h/r3/2)
(11.16)

11.1 Scale invariance in mathematics and nature
263
with f (h/r3/2) →h1/3r as r →0. It is straightforward to establish that the de-
pendence on r and h of m displayed in (11.16) is consistent with the relationship
(11.11). If we take the ﬁrst derivative of m with respect to h, and then set h = 0,
we ﬁnd
∂m
∂h

r

h=0
= r−1 f ′(0)
(11.17)
This is consistent with the results displayed in (11.7) and (11.8). We are thus able
to infer the critical exponent for the susceptibility from the way in which variables
transform under changes of scale.
Let’s look at another example of the statistical mechanics of a magnetic system.
In the so-called Gaussian model the statistical mechanics of the system is controlled
by the effective Hamiltonian
H =

ddx
	r
2 S(⃗x)2 +
 ⃗∇S(⃗x)

2
−hS(⃗x)

(11.18)
Here the quantity S(⃗x) is the spin at the location ⃗x. If the length scale is ad-
justed by the factor b, then the volume element ddx becomes bdddx. On the other
hand, the derivative ⃗∇goes to b−1 ⃗∇. This is because taking a spatial derivative
is, algebraically, the same as dividing by a distance. The changes in the effective
Hamiltonian can be subsumed into the following changes in the spin ﬁeld S(⃗x), the
reduced temperature r and the magnetic ﬁeld h as follows:
S(b⃗x) = b(2−d)/2S′(⃗x)
(11.19)
r = b−2r′
(11.20)
h = b−(d+2)/2h′
(11.21)
This means that the magnetization per unit volume, which is equal to the average
⟨S⟩, has the following dependence on r, h and b
m = b(2−d)/2 f

rb2, hb(d+2)/2
(11.22)
The form displayed in (11.22) is easily arrived at once the scaled variables, (11.19)–
(11.21), are substituted into the expression
m(r, h) =

S(⃗x)e−βHD

S(⃗x)


e−βHD

S(⃗x)

(11.23)
Again,thevariableb isnotrelevanttothephysicsofthesystem,sothemagnetization
must have a form that eliminates that dependence. Repeating the steps leading to

264
Scaling, fractals, and renormalization
(11.16) for the mean ﬁeld case, we arrive at the form
r(d−2)/4F

h
r(d+2)/4

(11.24)
for the Gaussian case. Taking the derivative of m with respect to h, we ﬁnd for the
isothermal susceptibility
χT = r−1F′

h
r(d+2)/4

(11.25)
In the limit h →0, we regain the critical exponent displayed in (11.7) and (11.8).
Although we obtained the same susceptibility exponent in the case of the Gaus-
sian model as we did for the mean ﬁeld approximation, it is important to note that
the combinations of r and h in the “scaling” functions in (11.16) and (11.24) are
different. Note, however, that the combinations coincide when the spatial dimen-
sionality, d, of the system is equal to four. This coincidence is not unexpected, given
the special role that d = 4 was found to play in the self-avoiding walk discussed in
Chapter 8.
Exercise 11.4
For the Gaussian model, calculate the critical exponents β, γ , and δ.
11.2 More on the renormalization group: the real space method
When we investigated the Koch curve to determine its Hausdorff dimensionality,
we utilized a “passive” renormalization group method. We performed no operation
on the curve other than to make measurements on it. This is appropriate to the nature
of a curve. It is an object that, once constructed, does nothing but sit there. In the
case of statistical mechanical systems, we are dealing with variables that ﬂuctuate
as the result of the thermal motion that all physical systems execute. The scrutiny to
which we subject them must take this motion into account, and because of that, the
way in which one determines their properties as viewed on different length scales
is somewhat more complicated.
As a gentle introduction to the renormalization group method for statistical me-
chanical systems, we’ll look at the Gaussian model in one dimension. There, the
partition function is the multiple integral
Z =

· · ·

e−H[Si]i dSi
(11.26)

11.2 More on the renormalization group
265
where the exponent contains the effective Hamiltonian H [Si], given by
H [Si] =

i
r
2 S2
i + a
2 (Si −Si+1)2
(11.27)
The system that we are looking at here is somewhat different from the Gaussian
model discussed in the previous section, in that the spin variables now occupy a
one-dimensional lattice, in contrast to the spin ﬁeld S(⃗x), which is deﬁned on a
continuum of points. The statistical mechanics of the two models do not differ
when it comes to the behavior close to and at the critical point.
The way in which we are going to see how this system behaves as we scrutinize
it on different length scales is with the use of the so-called “decimation” method.3
What we will do is hold ﬁxed the spin variables on every other site, allowing the re-
maining spin variables to ﬂucuate freely. In this way, we will generate a “restricted”
partition function, depending on half of the degrees of freedom of the system. From
this new partition function, we will extract a new effective Hamiltonian, in which
the variables are the degrees of freedom that we have held ﬁxed, which are fewer.
For obvious reasons this process has been identiﬁed as “thinning out” degrees of
freedom. The relationships of interest will be those that couple the parameters in
the new free energy to the parameters in the the original effective Hamiltonian.
The degrees of freedom that will be held ﬁxed are the spins on the even-numbered
sites. Those that are allowed to ﬂuctuate are the spin variables on the odd-numbered
sites. We’ll start by focusing on the spin on site number 3. The contribution to the
partition function that depends on this spin variable is of the form

dS3 exp

−r
2 S2
3 −a
2 (S3 −S2)2 −a
2 (S3 −S4)2
(11.28)
Rearranging terms in the exponential, we are left with the contribution

dS3 exp

−
r
2 + a

S2
3 + aS3 (S2 + S4) −a
2

S2
2 + S2
4

(11.29)
The integral is carried out in the usual manner, by completing squares, yielding

2π
r + 2a exp
	
−
a2
2(r + 2a) (S2 −S4)2 −
ar
2(r + 2a)

S2
2 + S2
4

(11.30)
Combining the results in (11.30) with the remaining terms in the partition func-
tion, we ﬁnd for the new partition function

2π
r + 2a
N/2
exp

i

−r′
2 S2
2i −a′
2

S2i −S2(i+1)
2

(11.31)
3 This method is not, strictly speaking, decimation, which refers to a reduction by 10%. The terminology was
introduced into statistical mechanics with the method, and, for better or worse, it has stuck.

266
Scaling, fractals, and renormalization
where
r′ = r(r + 4a)
r + 2a
(11.32)
a′ =
a2
r + 2a
(11.33)
Exercise 11.5
Fill in the steps leading to (11.31).
The exponential in the restricted partition function looks a lot like the exponen-
tial in the original parition function. We will improve the resemblance a little by
rescaling the spin variables so as to reset the coupling coefﬁcient to its original
value. Replacing S2i by S2i
√(r + 2a)/a, i.e.
S2i →S2i

r + 2a
a
(11.34)
we generate an exponent in the restricted partition function that has the form

i

−r′′
2 S2
2i −a′′
2

S2i −S2(i+1)
2

(11.35)
where
r′′ = r(r + 4a)
a
(11.36)
a′′ = a
(11.37)
We now relabel the site indices (2i →i), so that the effective Hamiltonian of the
new system is of the same appearance as the original effective Hamiltonian, except
for the change in the reduced temperature, r, which has been altered as per (11.36).
What if we had included in the model the ordering ﬁeld term h 
i Si? The effect
of this term is to alter the factor multiplying the exponential. The ordering ﬁeld, after
the decimation procedure, has the following relationship to the original ordering
ﬁeld:
h′′ = h
r + 4a
√a(r + 2a)
(11.38)
Exercise 11.6
Show that if an ordering ﬁeld, h, is included in the Gaussian effective Hamiltonian
in (11.27), through the additional term
−

i
hSi

11.2 More on the renormalization group
267
0
2
4
6
8
10
0
2
4
6
8
10
r/  a
r''/  a
Fig. 11.6. The recursion relation between r and r′′.
then the additional recursion relation (11.38) results. Additionally, show that this
term has no effect on the recursion relation (11.36).
The relationship between the effective Hamiltonians of the original spin system
and the spin system that is the result of the decimation procedure is H(r, h) →
H(r′′, h′′). We infer the statistical mechanics of the system on the basis of this
connection, and in particular on the basis of the relationships between the new
thermodynamic parmeters and the original ones. This is because the decimation
procedure can be applied to the new partition function in exactly the same way as
it was to the original partition function, yielding yet another effective Hamiltonian
of further altered thermodynamic parameters. The relations that connect the triple-
primed parameters to the double-primed ones are identical to (11.36) and (11.37).
That is, we have obtained a set of transformations that yield modiﬁcations of the
reduced temperature and ordering ﬁeld via the recursion relations
r(n) = R1

r(n−1)
(11.39)
h(n) = R2

h(n−1)
(11.40)
The connection between the reduced temperature,r, before the decimation proce-
dure and the reduced temperature, t′′, after decimation is illustrated in Figure 11.6.
The dashed line in the ﬁgure is the 45◦line r′′ = r. To ﬁnd the new reduced tem-
perature associated with an original value of r, one locates r on the x axis, and then
reads value on the y axis corresponding to the solid curve.
How further iterations of the relationship (11.36) between the original re-
duced temperature and the reduced temperature after decimation are graphically

268
Scaling, fractals, and renormalization
0
2
4
6
8
10
0
2
4
6
8
10
r/  a
r''/  a
Fig. 11.7. Graphical iteration of the recursion relation between r and r′′.
accomplished is illustrated in Figure 11.7. To ﬁnd the reduced temperature at the
ﬁrst iteration of the relation, one projects vertically on the solid curve, as indicated
by the leftmost vertical dotted line in the ﬁgure. To ﬁnd the reduced temperature
at the second iteration, one projects horizontally from the solid curve to the 45◦
line, along the lowest horizontal dotted line and then again projects vertically onto
the curve along the next vertical dotted line. As is clear from the ﬁgure, an initial
reduced temperature is increased more and more as the recursion relation is iterated
over and over again.
When the reduced temperature is small enough, the recursion relation between
r and r′′ is well-approximated by
r′′ = 4r
(11.41)
It r′′ is also small, then the approximation in (11.41) can also be used at the next
iteration. Continuing as long as the reduced temperature is small, one ends up with
a reduced temperature, r(n), after n iterations, that is given by
r(n) = 4nr
(11.42)
Again, under the assumption of small r, the value of the new ordering ﬁeld is given
by
h′′ = 23/2h
(11.43)
Then, the result of n iterations is to generate an h(n), that is given by
h(n) = 23n/2h
(11.44)
Again, this result holds as long as r(n) is not too large. By the same reasoning, the

11.2 More on the renormalization group
269
rescaling of the spin ﬁeld in (11.34) yields a total spin rescaling factor equal to
b(n) = 2n/2
(11.45)
We can now apply the same reasoning to the new effective Hamiltonian as we
did in the discussion in Section 11.1.2 on the scaling behavior of the mean ﬁeld
and continuous spin Gaussian models. According to these arguments, the rescaled
expectation value of the magnetization per site is the same function of the rescaled
reduced temperature and ordering ﬁeld as the original magnetization is of the orig-
inal reduced temperature and magnetic ﬁeld. Mathematically,
m2n/2 = f

r22n, h23n/2
(11.46)
or
m = 2−n/2 f

r22n, h23n/2
(11.47)
As the number of iterations of the recursion relations is arbitrary, the net magneti-
zation cannot depend on n, which is consistent with
f = r−1/4F
 h
r3/4

(11.48)
Using this result, we ﬁnd
∂m
∂h

r
= r−1F′
 h
r3/4

(11.49)
and, once again, we recover for the critical exponent γ the mean ﬁeld value of one.
Note also that the general scaling form in (11.49) is consistent with the scaling
function for the magnetic susceptibility in (11.25) with d = 1.
The form of the magnetization when the renormalized reduced temperature is
not small is more complicated than the scaling form displayed in (11.48). This is
because the solution of the general recursion relation does not have the simple form
displayed in (11.41). We’ll pursue the construction of the general partition function
in more detail once we’ve investigated a variant of the above decimation procedure
Worked-out example: the one-dimensional Ising model
Here, we formulate the one-dimensional Ising model and set up the renormalization
group for it. The Ising model Hamiltonian for an array of spins equally spaced along
a line is
H = −J
N

i=1
Si Si+1
(11.50)

270
Scaling, fractals, and renormalization
where the spin variables, Si can take on the values ±1. The coupling constant J
is positive, and we will assume that periodic boundary conditions hold, in that
SN+1 = S1. The partition function for this Ising chain at temperature T is
Z =

S1=±1
· · ·

Si=±1
· · · eK S1S2+K S2S3+···
(11.51)
where K = J/kBT .
For the moment, let’s sum over all conﬁgurations of the spin S3. We isolate the
only terms in the exponential in (11.51) that depend on this degree of freedom, and
obtain
1

S3=−1
eK(S2S3+S3S4) = 2 cosh K(S2 + S4)
(11.52)
Because S2 and S4 take on the values ±1, the function 2 cosh K(S2 + S4) assumes
two values as well. What we have is
cosh K(S2 + S4) =
2 cosh 2K S2 = S4
2
S2 = −S4
(11.53)
We can replace this function by the new one AeK ′S2S4, if the quantities A and K ′
are properly chosen.
Exercise 11.7
Show that the correct choices for A and K ′ are
A = 2
√
cosh 2K
(11.54)
K ′ = 1
2 ln cosh 2K
(11.55)
Exercise 11.8
If an ordering ﬁeld is introduced, so that the Ising model Hamiltonian contains the
additional term
−h

i
Si
repeat the above analysis and show that summing over the conﬁgurations of the
spin S3 results in a function that can be recast into the form
AeK ′
1S2S4+K ′
2(S2+S4)
(11.56)
Determine the values of A, K ′
1 and K ′
2 in terms of K1 = J/kBT and K2 = h/kBT .

11.2 More on the renormalization group
271
Returning to the system with no ordering ﬁeld, we have
1

S3=−1
eK(S2S3+S3S4) = A(K)eK ′S2S4
(11.57)
By summing over the conﬁgurations of all odd-indexed spins, S1, S3, S5, . . . , we
eliminate half of the degrees of freedom in the one-dimensional Ising system. By
this process we thin out the degrees of freedom. The partition function then takes
the form
Z N =

S2=±1

S4=±1
· · · A(K)N/2eK ′(S2S4+S4S6+··· )
(11.58)
Relabeling the sites, S2 →S1, S4 →S2, . . . we have
Z N(K) = A(K)N/2Z N/2(K ′)
(11.59)
The partition function Z N/2(K ′) has the same form as the original partition function,
except that it is for a system with half as many degrees of freedom as the system
with which we started out, and the coupling between adjacent spins is altered. This
tells us that we can write
Z N/2 = A(K ′)N/4Z N/4(K ′′)
(11.60)
where the relationship between K ′′ and K ′ is the same as the relation connecting
K ′ and K. The full partition function is thus given by
Z N(K) = A(K)N/2A(K ′)N/4 
S1=±1
· · ·

Si=±1
· · · eK ′′(S1S2+S2S3... )
(11.61)
The process can clearly be continued until all the spins are eliminated.4 What this
all means is that the evaluation of the partition function is tantamount to solving
the recursion relation
Kn+1 = 1
2 ln cosh 2Kn
(11.62)
where K1 is the original value of J/kBT . The partition function is then given by
Z = A(K1)N/2A(K2)N/4 · · · A(K j)N/2 j+1 · · ·
(11.63)
From this result, an expression for the free energy emerges naturally.5
4 This is, strictly speaking, true only if the total number of spins is a power of two. Otherwise there will be spins
“left over” in the decimation process. We ignore this complication, which turns out not to affect the results
obtained with this approach.
5 The solution of the one-dimensional Ising model via the renormalization group is discussed in (Maris and
Kadanoff, 1978; Nauenberg, 1975; Nelson and Fisher, 1973).

272
Scaling, fractals, and renormalization
Exercise 11.9
Express the renormalization group equation for the one-dimensional Ising model
in terms of the variabel x = e−2K = e−2J/kBT .
(a) Find the ﬁxed points of this recursion relation. Discuss their physical signiﬁcance. The
ﬁxed point of this recursion relation is a K such that that 1
2 ln cosh 2K = K.
(b) Linearize the renormalization group equation about the low-temperature (large K) ﬁxed
point and from this show that the correlation length, ξ(T ), goes as e2J/kBT as T →0.
(c) Calculate the partition function, Z(K), in the limit T →0, and from your result show
that the free energy tends to the limit E = −N J.
11.2.1 The momentum shell renormalization group method
The decimation method works reasonably well for the one-dimensional Gaussian
and Ising models. There are, however, complications when it is applied to these
models in higher dimensions. For one thing, interactions are introduced between
sites that are further and further apart, which means that the coupling terms become
more and more complex. In addition, while the real space is extremely well-suited
for other systems, the self-avoiding walk is not, alas, one of them. We now discuss
a renormalization procedure which is quite general and which we’ll then apply to
the O(n) model.
There is a way to approach the elimination of degrees of freedom that generalizes
quite nicely. One makes use of the fact that one can represent the spins with the
use of spatial Fourier transforms, as discussed in Supplement 2 to Chapter 2. To
recapitulate: in a lattice in which the distance between neighboring sites is c, the
spin degrees of freedom, si, can be represented in terms of Fourier-transformed
variables, s(⃗q), as follows:
Si =
1
√
N

⃗q
S(⃗q)ei⃗q·⃗Ri
(11.64)
where ⃗Ri is the position vector of the ith site, and the wave-vector ⃗q is conﬁned to
a Brillouin zone, the linear extent in q space of which is inversely proportional to
the distance, c between sites in real space. That is, if Q is the width of the Brillouin
zone, then Q ∝1/c. The quantity N in (11.64) is the total number of sites on the
lattice, or the total number of independent wave-vectors in the Brillouin zone.

11.2 More on the renormalization group
273
c
2c
Q
Q/2
Lattice
Brillouin
zone
Fig. 11.8. The connection between the lattice and the associated Brillouin zone.
The elimination of spin degrees of freedom on sites is, in a sense, equivalent to the
elimination of S(⃗q)’s in the region between the larger Brillouin zone appropriate
to the original lattice and the smaller Brillouin zone associated with the lattice
containing the remaining spins. The equivalence is illustrated in Figure 11.8.
The Gaussian model is especially simple when expressed in terms of Fourier-
transformed variables. All that is required is to substitute (11.64) into (11.27), and
to make use of the orthogonality of the plane waves:
N

i=1
ei(⃗q+⃗q′)·⃗Ri = Nδ⃗q,−⃗q′
(11.65)
as deduced in Supplement 2 of Chapter 2. The effective Hamiltonian has the form
H

S(⃗q)

=

⃗q
r
2 + α(⃗q)

S(⃗q)S(−⃗q)

−
√
N S(0)h
(11.66)
where α(q) = −2a(1 −cos qc) in one dimension and for small q the coupling term
takes on the approximate value
α(⃗q) ∼ac2q2 = Jq2
(11.67)
a result that holds in d dimensions.
To simplify matters, we’ll assume that α(⃗q) is as given by the right hand side
of (11.67). The elimination of S(⃗q)’s with wave-vectors lying between the smaller
Brillouin zone associated with the “decimated” lattice and the Brillouin zone ap-
propriate to the full lattice is straightforward, as the coupling in (11.66) is between
S(⃗q)’s for which the magnitude of the wave-vector is the same. The mathematical
elimination of these degrees of freedom is accomplished by performing a set of

274
Scaling, fractals, and renormalization
Gaussian integrations. However, because of the lack of coupling between Fourier-
transformed spin modes for which the wave-vectors have different magnitudes, this
set of integrations has no effect on the dependence of the effective Hamiltonian on
the spin modes in the smaller Brillouin zone. This means that we can effectively ig-
nore the integration over the degrees of freedom that are averaged out and focus on
the degrees of freedom that are held ﬁxed in this realization of the renormalization
group method.
At ﬁrst sight, it appears as if there is no change at all in the effective Hamiltonian
of the reduced set of degrees of freedom. However, it is also necessary to take
into account the fact that the length scale appropriate to the system has changed. In
particular, if a decimation has been carried out, the length scale has doubled. In light
of the reciprocal relationship between scales in real and q space, the scale against
which wave-vectors are measured has been halved. To take this into account, we
replace all wave-vectors, ⃗q in the new effective Hamiltonian by ⃗q/2. Furthermore, it
is necessary to express the number of degrees of freedom, N, in terms of the number
of degrees of freedom in the reduced set of thermodynamic variables. Again, if the
procedure that has been carried out is decimation, then, we ought to replace N
by 2d N, where d is the system’s spatial dimensionality. Having carried out these
rescalings, we are left with an effective Hamiltonian that is of the form

⃗q
	r
2 + Jq2
4

S(⃗q)S(−⃗q)

−
√
N2d/2S(0)h
(11.68)
We now rescale the spin ﬁeld so as to preserve the magnitude of the coupling
coefﬁcient, J. This is accomplished by the scale change
S(⃗q) →2S(⃗q)
(11.69)
The effective Hamiltonian of the reduced set of degrees of freedom is, then

⃗q
	4r
2 + Jq2

S(⃗q)S(−⃗q)

−
√
N2(d+2)/2S(0)h
(11.70)
This is the same as the original effective Hamiltonian, except that the reduced
temperature and the externally imposed magnetic ﬁeld are altered as follows:
r →22r
(11.71)
h →2(d+2)/2h
(11.72)
Now, this procedure can be generalized, in that the change in the size of the
Brillouin zone need not have been by a factor of two. If we replace the 2 by b in
Eqs. (11.69), (11.71) and (11.72), we see, following previous arguments, that the
relationship between the magnetization, the reduced temperature, and the applied

11.2 More on the renormalization group
275
magnetic ﬁeld must obey the following scaling requirement:
m(r, h) = b(2−d)/2m

rb2, hb(d+2)/2
(11.73)
The factor b(2−d)/2 comes about because m, which is essentially ⟨Si⟩, scales as the
spin per site, which, in turn, scales as S(⃗q)/
√
N. As we have seen, this introduces
the factor b/bd/2, or b(2−d)/2. The functional form expressed in (11.73) is precisely
the scaling requirement that was inferred for the Gaussian model earlier with the
use of a “passive” renormalization group argument.
11.2.2 The continuous renormalization group
One particular advantage of the use of the shrinking Brillouin zone to determine
the scaling properties of a statistical mechanical system is that the alterations in
the length scale can be performed continuously. This simpliﬁes portions of the
analysis, and it greatly enhances the power and versatility of the method. As a start,
we’ll reanalyze the Gaussian model by imagining that the Brillouin zone’s width
is equal to e−ℓ, where ℓis a continuous variable. Suppose, now, that in the process
of changing length scales, the parameter ℓchanges by an inﬁnitesimal amount, δℓ.
The new scale on which wave-vectors are measured is now e−ℓ−δℓ= (1 −δℓ)e−ℓ.
Notice that this corresponds to a fractional change in the size of the Brillouin zone.
The rescaling of the wave-vector results in the replacement
⃗q →⃗q (1 −δℓ)e−ℓ
e−ℓ
= ⃗q(1 −δℓ)
(11.74)
Then, the coupling term in the effective Hamiltonian becomes
q2(1 −δℓ)2S(⃗q)S(−⃗q) = q2S(⃗q)S(−⃗q)(1 −2δℓ)
(11.75)
The coupling term is kept constant by a rescaling of the spin ﬁeld:
S(⃗q) →(1 + δℓ)S(⃗q)
(11.76)
The term containing the reduced temperature, r, is then changed as follows:
r S(⃗q)S(−⃗q) →r S(⃗q)S(−⃗q)(1 + 2δℓ)
(11.77)
The number of degrees of freedom, N, is altered as well:
N →N ed(ℓ+δℓ)
edℓ
= N(1 + dδℓ)
(11.78)

276
Scaling, fractals, and renormalization
This means that the term involving the external magnetic ﬁeld alters as follows:
√
N S(0)h →

1 + d + 2
2
δℓ
 √
N S(0)h
(11.79)
The results above indicate that the results of the change in length scales are the
following alterations of the reduced temperature and applied magnetic ﬁeld
r →r(1 + 2δℓ)
(11.80)
h →h

1 + d + 2
2
δℓ

(11.81)
Calculating the inﬁnitesimal changes in r and h, and then dividing by the inﬁnites-
imal fractional increment in the size of the Brillouin zone, δℓ, we end up with the
following differential equations for the changing reduced temperature and applied
ﬁeld:
dr
dℓ= 2r
(11.82)
dh
dℓ= d + 2
2
h
(11.83)
We would like to know the relationship between the magnetization per site on the
original lattice and the variables appropriate to the “thinned out” lattice. To obtain
this relationship, we recall the connection between spin variables on a lattice and
their spatial Fourier transform, given by (11.64). Let N(ℓ) be equal to the number
of degrees of freedom in the reduced lattice, and deﬁne the rescaled spins on that
lattice as S(ℓ)
i , with spatial Fourier transforms S(ℓ)(⃗q). Then, the contribution of the
mode with zero wave-vector to the magnetization per lattice site on the original
lattice is given by
Si = S(0)
√
N
=
S(0)
S(ℓ)(0)
√N(ℓ)
N
S(ℓ)
i
= e((2−d)/2)ℓS(ℓ)
i
(11.84)
The magnetization per site on the thinned out lattice will be some function of the
renormalized parameters appearing in the new effective Hamiltonian, i.e.
m(ℓ) = f (r(ℓ), h(ℓ))
(11.85)
This means that the magnetization per site on the original lattice is given by
m = e((2−d)/2)ℓf (r(ℓ), h(ℓ))
(11.86)

11.3 Recursion relations
277
By the same arguments that we have previously invoked, the left hand side of
(11.86) is independent of ℓ. Taking the derivative with respect to ℓof (11.86) we
end up with the following differential equation for m(r, h)
0 =

−d −2
2
+ dr
dℓ
∂
∂r + dh
dℓ
∂
∂h

m(r, h)
=

−d −2
2
+ 2r ∂
∂r + d + 2
2
h ∂
∂h

m(r, h)
(11.87)
One demonstrates by substitution that the scaling form (11.24) for m(r, h) satis-
ﬁes (11.87). Alternatively, a solution to this partial differential equation can be
constructed with the use of the method of characteristics. The result is the same.
This last development of the scaling form of the magnetization of the Gaussian
model may seem unnecessarily complicated. However, it is a prototypical version
of relationships that are central to the ﬁeld-theoretical approach to scaling in the
vicinity of the critical point.
11.3 Recursion relations: ﬁxed points and critical exponents
In this section, we will describe the general philosophy underlying the renormaliza-
tion group method, along with the assumptions that are central to its application. We
will also introduce the reader to some of the terms that have been coined to describe
operations and entities in the context of the renormalization group method. This
having been done, the reader will be prepared to face the thickets of mathematics
to be confronted in the next chapter.
It should be noted that the discussion to follow could have easily been placed at
the beginning of this chapter, in that nothing that has been previously discussed is
essential to the understanding of what follows. However, the fact that the reader has
already gone through some simple examples of the application of the renormaliza-
tion group method ought to allow for the rapid assimilation of the ideas that will
be outlined in the next few pages. We follow closely the conceptual point of view
laid out in the pioneering work of Kenneth Wilson, the inventor of renormalization
group as applied to critical phenomena (Wilson, 1971a).
11.3.1 Block spins
The key property of a critical point is that there is a diverging length scale, which is
a result of the fact that the correlation length is inﬁnite at the critical point. The kind
of system of interest to us actually possesses additional length scales, because of the
system’s inevitable ﬁnite size, and the fact that it is made up of molecules. However,
the special properties of a critical point are controlled by the correlation length, and

278
Scaling, fractals, and renormalization
in particular by the consequences of that length’s becoming larger and larger as the
critical point is approached. At the critical point the correlation length has become
inﬁnite. Ignoring all other length scales, the system is now scale-invariant, in that
there is no way to determine the scale on which the system is being viewed on the
basis of the (statistical mechanical) properties of the system alone. On the other
hand, when the system is not at its critical point, the connection between the systems
as viewed under different length scales is non-trivial. They key to understanding
how the system behaves both at and away, but not too far away, from the critical
point, lies in the “mappings” that can be established between the properties of the
system under changes of length scale.
Once the nature of a system’s degrees of freedom have been established, the
statistical mechanics of that system are entirely determined by the system’s energy,
or, in the present context, by the system’s effective Hamiltonian. Now, imagine
that we are interested in a system in which the degrees of freedom are magnetic
moments, sitting on the sites of a lattice. The effective Hamiltonian of this system
will be of the general form
H [{Si} , {κi}]
(11.88)
where {κi} represents the complete set of parameters needed to quantify the effective
Hamiltonian, and {Si} stands for the complete set of degrees of freedom in the
system. In the case at hand, the variables are spins on the lattice sites.
Imagine, now that one alters the system in some way so that the appropriate set
of statistical mechanical variables are now “block spins” on a larger lattice. The
arguments in the new effective Hamiltonian will be those variables and a new set of
coupling parameters. It is assumed that one can make a one-to-one correspondence
between the variables in the new system and those in the original one. This is clearly
not possible if the system is formally ﬁnite, because in the process of thinning
out one has necessarily reduced the number of degrees of freedom. That this is so
follows from the fact that the number of sites in a lattice of ﬁnite extent is controlled
by the ratio of the total volume occupied by the lattice to the “primitive” volume
surrounding a site. The greater the spacing between lattice points, the larger this
volume, and the smaller the number of sites that will ﬁt in a given total volume.
We will assume that the system is in the thermodynamic limit, which allows us to
eliminate a ﬁnite fraction of the degrees of freedom and still create a one-to-one
connection between the original set of degrees of freedom and the set we are left
with after the elimination.
Because we have not, by design, changed the set of thermodynamic variables in
the process that we are imagining, the only change in the effective Hamiltonian is
in the values of the parameters. The “block spinning” process that is envisioned

11.3 Recursion relations
279
here thus gives rise to a set of mappings of the form
κi →Ri (κ1, κ2, κ3, . . . )
(11.89)
and so on. The renormalization group procedure consists of the construction and
analysis of the relations in (11.89).
Now, we have already discussed ways in which degrees of freedom can be thinned
out in the context of a scaling analysis of the Gaussian model, and we have generated
relationships that correspond to (11.89). In the next section we will further explore
the construction of those relations, which are, in general, called recursion relations.
We will look at what one ought to expect in the vicinity of a critical point, and how
one utilizes the recursion relations to extract critical exponents.
The recursion relations need not be of the discrete form depicted in (11.89).
We now know from our scrutiny of the Gaussian model that one can, at least in
principle, construct differential recursion relations, of the form
dκi
dℓ= Wi (κ1, κ2, κ3, . . . )
(11.90)
which implies that the scaling properties of the system can be extracted from a set
of ﬁrst order differential equations.
Now, it is well-known that equations of the form (11.90) can give rise to ex-
ceedingly complex behavior. The motion of a turbulent ﬂuid is contained in the
Navier–Stokes equation, which can be expressed as an inﬁnite number of ﬁrst or-
der differential equations. Even when the number of variables and equations is
quite small, chaos may result. There are, in fact, examples in statistical mechanics
in which the recursion relations give rise to chaotic behavior. They are rare, how-
ever, and, in some cases, contrived. We will ignore this possibility, which turns out
to be irrelevant to the random walk. As good fortune has it, the behavior of interest
to us is reasonably simple.
They key to the interpretation of the implications of the recursion relations is
the realization that the properties of the system are controlled by the correlation
length, and by the fact that the correlation length is reduced in magnitude with each
iteration of the recursion relation. We will take the quantity ℓin (11.90) to be the
same as it was in the case of the Gaussian model. This means that the length scale,
s, on which the system is being scrutinized, is given by
s = eℓ
(11.91)
and that we can append to the differential recursion relations (11.90) the following
differential equation for the evolution of the correlation length as viewed on the

280
Scaling, fractals, and renormalization
Fig. 11.9. Flow of parameters to their ﬁxed point values on the critical hypersur-
face.
changing length scale, ξ (ℓ),
dξ (ℓ)
dℓ
= −ξ (ℓ)
(11.92)
Whatshouldoneexpectwhenthesystemisatitscriticalpoint?First,weknowthat
the correlation length is inﬁnite. This means that (11.92) will have no effect on it, in
that it remains inﬁnite, no matter the length scale on which it is measured. If a single
set of thermodynamic parameters describes a system that has this property, then the
right hand sides of the equations in the set (11.90) will all be equal to zero. On the
other hand, the more reasonable expectation is that a system is at its critical point
when the parameters take on a restricted set of values. The particular values of the
parameters are determined by requiring the correlation length, which is a function
of the parameters, to be inﬁnite (ξ(κ1, κ2, . . . ) = ∞). We’ll call this restricted set
the critical hypersurface in the space spanned by the full set of parameters. The
recursion relations will necessarily take a point on the critical hypersurface to
another point on the same subspace.
The next question is: where will a point on the critical hypersurface end up
as it propagates under the inﬂuence of (11.90)? The general theory of ﬁrst order
differential equations allows for a number of possibilities. However, we will assume
that there is only one outcome of the movement of the evolution of the parameter
set under the inﬂuence of the recursion relations, and that is propagation to a ﬁxed
point. In other words, we assume that a point on the critical hypersurface will “ﬂow”
to a point from which it will not depart. Figure 11.9 illustrates this ﬂow to the ﬁxed
point.
Let’s look in a little more detail at this ﬁxed point. Mathematically, the ﬁxed
point, (κ∗
1, κ∗
2, . . . ), corresponds to the set of coupling parameters, κi at which
the right hand side of the continuous recursion relation (11.90) is equal to zero,

11.3 Recursion relations
281
i.e. Wi(κ∗
1, κ∗
2, . . . ) = 0. We’ll make the assumption that the right hand sides of the
differential recursion relations do not have any non-analytic behavior in the vicinity
of this ﬁxed point. This means that we can expand the right hand sides of these
equations as a power series in the difference between the parameters and the values
that they take at the ﬁxed point. Let κ∗
i be the value of the parameter κi at the ﬁxed
point of interest. Then, writing κi(ℓ) = κ∗
i + δκi(ℓ), we have the following set of
equations for the ℓ-dependence of the δκi’s
dδκi(ℓ)
dℓ
=

j
Ai jδκ j(ℓ) +

j,k
Ai jkδκ j(ℓ)δκk(ℓ) + · · ·
(11.93)
The right hand side of (11.93) contains no terms independent of δκi(ℓ), to ensure
consistency with the solution δκi(l) = 0.
When the deviations of the parameters from their ﬁxed point values are small,
the right hand side of (11.93) can be truncated at ﬁrst order in the δκi’s. We are left
with the linear differential equations
dδκi(ℓ)
dℓ
=

j
Ai jδκ j(ℓ)
(11.94)
This set of equations is solved with the use of standard methods, i.e. by linear
transformation to a new set of variables in which the matrix A is diagonal. One
ﬁnds
δκi(ℓ) =

α
Ci,αeαℓ
(11.95)
where the “rates” α are the eigenvalues of the matrix


A11
A12
A13
A21
A22
A23
· · ·
A31
A32
A33
...
...


(11.96)
If the ﬁxed point is stable, in that any deviation from it decays under the action
of the recursion relations, then all the α’s are negative.6 We expect that any
deviation from the ﬁxed point along the critical hypersurface will, indeed, decay.
On the other hand, if one displaces the parameter set off of the critical hypersurface,
then the correlation length is ﬁnite, which reduces under the renormalization group
procedure, and this change must be reﬂected in a change in the set of thermodynamic
parameters in terms of which the correlation length can be expressed. In other
6 The deviations from the ﬁxed point associated with negative eigenvalues are termed “irrelevant,” and the devi-
ations associated with eigenvalues equal to zero are known as “marginal.” By contrast, a deviation associated
with a positive eigenvalue is called “relevant.”

282
Scaling, fractals, and renormalization
words, it is expected that a few of the α’s are positive. How many depends on how
many dimensions are needed to ﬁll out the space of parameters, given the critical
hypersurface. In the case of a spin system, in which there are two ways of displacing
the system from the critical point, corresponding to a change in temperature and the
application of a magnetic ﬁeld, the codimension of the critical hypersurface is two,
which means that two of the eigenvalues of the matrix (11.96) are positive. Let’s
call the two positive eigenvalues r and h, and let’s parameterize the distance off
of the critical hypersurface in terms of the reduced temperature and the magnetic
ﬁeld. In addition, we’ll associate the eigenvalue h with displacements from the
ﬁxed point caused by the imposition of a magnetic ﬁeld.
By the symmetry of the system, we know that the absence of an applied magnetic
ﬁeld at one length scale implies the absence of a symmetry-breaking ﬁeld at any
length scale, or so one hopes. Suppose that the reduced temperature, r, is not equal
to zero, but that otherwise all parameters are set at their ﬁxed point values. If r is
small, then we can write
δκi(0) = r Xi
(11.97)
which means that
δκi(ℓ) = r Xi

Ci,terℓ+

α′
Ci,α′eα′ℓ

(11.98)
The symbol α′ in (11.98) refers to the set of negative eigenvalues of the matrix
(11.96). The important behavior of the parameters is contained in the ﬁrst contri-
bution in parentheses on the right hand side of the equation. For the moment, let’s
discard the others, so that the evolution of the parameter set is controlled by the
contribution from the exponentially growing terms. Their distance from the critical
hypersurface is then proportional to reyℓ.
At the same time as this “distance from criticality” is growing, the correlation
length is shrinking. This quantity is given by
ξ (ℓ)(r) = ξ (0)(r)e−ℓ
(11.99)
as can be readily intuited from (11.92). We can determine the value of ℓ, which
we’ll call ℓ∗, at which the rescaled correlation length is equal to one by setting the
right hand side of (11.99) equal to unity. In this way we ﬁnd
ℓ∗= ln ξ (0)(r)
(11.100)
Choosing ℓin this way imposes no loss in generality, since the thermodynamic
functions are independent of the scale factor. Thus, the value of ℓis arbitrary.
Now, the set of thermodynamic parameters for which the correlation length is

11.3 Recursion relations
283
ξ=∞
ξ=1
ξ=1
D
D
Fig. 11.10. The critical hypersurface, on which the correlation length, ξ, is inﬁnite
and hypersurfaces on which the correlation length is equal to some ﬁxed value.
Here, the value is one. Also shown on this ﬁgure is the ﬂow in parameter space
under the action of the continuous renormalization group. The heavy dot on the
critical hypersurface is the ﬁxed point there.
equal to one ought to constitute another hypersurface in parameter space, separated
from the critical hypersurface by some distance, D. The situation is illustrated in
Figure 11.10. This means that in order for the correlation length to be unity, we
also require
Krerℓ∗= D
(11.101)
where K = XiCi,t, the quantities Xi and Ci,t being those appearing in (11.98).
Equation (11.101) tells us that
eℓ∗=
 Kr
D
−1/r
(11.102)
Inserting this result into the logarithm of (11.100), we ﬁnd
ξ (0)(r) =
 Kr
D
−1/r
(11.103)
Because the right hand side is the correlation length on the original length scale, we
have the result thatthe correlationlength isproportional tor−1/r,or, in otherwords,
that the correlation length exponent is given by ν = 1/r. This is an important
result. It tells us that the critical exponent ν can be calculated directly from the
renormalization group equations – once they are known. We can go further. Again,
for our magnetic system with only two parameters, r and h, the free energy per spin
takes on the scaling form
f (r, h) = e−ℓd f

rerℓ, hehℓ
(11.104)
The factor e−ℓd arises because the degrees of freedom, i.e. number of spins,
is reduced by this factor upon renormalization. All the critical exponents can
be determined according to the prescription discussed earlier in this chapter. A

284
Scaling, fractals, and renormalization
straightforward calculation yields
β = d −h
r
(11.105)
−γ = d −2h
r
(11.106)
2 −α = d
r
(11.107)
δ =
h
d −h
(11.108)
and, of course, ν = 1/r. As promised, all the critical exponents are expressed in
terms of the two relevant eigenvalues, r and h.
As a ﬁnal exercise before bringing this chapter to a close, we apply the general
argument just presented to the Gaussian model. Recall the renormalization group
equations for the reduced temperature, r and the external ﬁeld, h, which are given
in (11.82) and (11.83).
dr(ℓ)
dℓ
= 2r(ℓ)
(11.109)
dh(ℓ)
dℓ
=
d + 2
2

h(ℓ)
(11.110)
We see from these equations that the matrix A is diagonal with eigenvalues r = 2
and h = (d + 2)/2. Placing these values into the general expressions (11.105)–
(11.108), we have for the critical exponents of the Gaussian model
β = d −2
4
(11.111)
γ = 1
(11.112)
2 −α = d
2
(11.113)
δ = d + 2
d −2
(11.114)
and ν = 1/2, completing our analysis of this system.
Arriving at the renormalization group equations and analyzing their solutions
can be a formidable task for more complicated effective Hamiltonians, such as the
one describing the O(n) model. The strategy driving the procedure, however, is the
one we have outlined here.

12
More on the renormalization group
12.1 The momentum-shell method
The method discussed in Section 11.3.1 will now be pursued further, in that it will
be applied to the full effective Hamiltonian of an O(n) spin system, in which the
effective Hamiltonian contains terms that are linear, quadratic, and of fourth or-
der in the spin ﬁeld. It is in the consideration of the higher order terms in the
effective Hamiltonian (higher order than quadratic, that is) that the complica-
tions arise. The calculations that will be outlined here are not especially chal-
lenging in execution, but we will hint at extensions and generalizations that can
become so.
In this chapter, the reader will be introduced to the ﬁeld-theoretical version of the
renormalization group, and to its ﬁrst effective realization, the ϵ expansion for criti-
cal exponents. The approach will be that of the momentum-shell method developed
in the previous chapters. The application of the method to the full O(n) Hamiltonian
will be more complicated due to the coupling terms, which were neglected before. A
straightforward, though somewhat tedious, calculation will lead to a modiﬁed set of
renormalization equations. These differential equations will then be solved to low-
est order in the variable ϵ = 4 −d, where d is the system’s spatial dimensionality
(three in the cases of interest to us). Using scaling arguments, the critical exponents
will be obtained. Their relevance to the self-avoiding random walk will also be
discussed.
The techniques to be discussed here are descendants of the original renor-
malization group method developed by Kenneth Wilson (Wilson, 1971a; Wilson,
1971b; Wilson and Kogut, 1974). They have fallen into disuse, but remain pow-
erful because of their intuitive underpinnings. Indeed, in our opinion, they rep-
resent the fastest route to understanding what the renormalization group is all
about.
285

286
More on the renormalization group
12.2 The effective Hamiltonian when there is fourth order interaction
between the spin degrees of freedom
The form of the O(n) effective Hamiltonian that we have been looking at is
H

⃗S(⃗q)

=

⃗q
1
2q2|⃗S(⃗q)|2 + r
2|⃗S(⃗q)|2

+ u
N

⃗q1...⃗q4

⃗S(⃗q1) · ⃗S(⃗q2)
 
⃗S(⃗q3) · ⃗S(⃗q4)

δ⃗q1+⃗q2+⃗q3+⃗q4
(12.1)
This effective Hamiltonian follows from the application of spatial Fourier trans-
forms to the real-space version of the Ginzburg–Landau–Wilson effective Hamil-
tonian
H

⃗σi
	
=

i
1
2| ⃗∇⃗σi|2 + r
2|⃗σi|2

+ u

i

|⃗σi|22
(12.2)
The derivation of (12.1) from (12.2) is detailed in Chapter 9.
In order to proceed in as direct a way as possible and at the same time retain the
complications caused by the coupled spin interactions, we will carry through the
momentum-shell calculation appropriate to a system in which the spins have one
component only. This is the O(1) model, also known as an Ising-like system. In the
strict version of the Ising model, the spin degrees of freedom are scalars that can
take on the values ±1. As we will see, the calculations carry over, with only slight
modiﬁcations, to the O(n) model with arbitrary n, including n = 0.
12.2.1 Renormalized Ising Hamiltonian
For a one-component system, ⃗S(⃗q) appearing in (12.1) reduces to a scalar, S(⃗q),
and the Ginzburg–Landau–Wilson effective Hamiltonian becomes
H

S(⃗q)
	
=

⃗q
1
2

r + q2
S(⃗q)S(−⃗q) + u
N

⃗q1,... ,⃗q4
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4 (12.3)
Recall that a lattice, such as the square lattice shown in Figure 12.1 has a reciprocal
lattice that is also square, shown in Figure 12.2. If the characteristic spacing in our
real space lattice is a, then the characteristic spacing of the lattice in reciprocal
space is 2π/a. The distances between lattice points in the direct and reciprocal
lattices scale inversely. From this it follows that the spacing between q’s is inversely
proportional to the characteristic linear dimension of the system, L. The length, L,
is proportional to N 1/d, where N is the number of lattice sites, and d is the spatial
dimension of the system. The constant of proportionality depends on the shape of
the system, but we will ignore such factors.

12.2 The effective Hamiltonian
287
a
a
Fig. 12.1. The lattice in real space. The spacing between nearest neighbors is a.
2π/a
2π/a
Fig. 12.2. The reciprocal lattice. Note that the lattice spacing is 2π/a.
(a)
(b)
Fig. 12.3. Thinning out the degrees of freedom. The s(⃗q)’s that are eliminated
reside in the shaded region between the outer boundary of the reduced Brillouin
zone, shown in the right hand side of the ﬁgure, and the outer boundary of the
original Brillouin zone.
12.2.2 Momentum-shell thinning out of degrees of freedom
As we discussed in Chapter 11, thinning out the degrees of freedom is accom-
plished by eliminating S(⃗q)’s in a shell just below the surface of the Brillouin zone
(BZ). Eliminating S(⃗q)’s in the shaded region in Figure 12.3(a) leaves one with a
system in which the degrees of freedom are contained in the reduced zone shown in
Figure 12.3(b). This Brillouin zone corresponds to a system in which the spins sit

288
More on the renormalization group
Λ
Fig. 12.4. The spherical Brillouin zone with radius .
on the points of a lattice with a larger distance between neighboring points. In fact,
if the linear dimension of the BZ is reduced by the factor 1/b, with b > 1, then the
spacing in the corresponding real space lattice is increased by the factor b.
12.2.3 Continuous renormalization
Our effective Hamiltonian is
1
2

⃗q

r + q2
S

⃗q

S

−⃗q

+ u
N

⃗q1,... ,⃗q4
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
(12.4)
TheeliminationprocedurewillbecarriedthroughforaBrillouinzoneintheshapeof
aspheretoexploitthesphericalsymmetryofthequadraticenergycoefﬁcientr + q2.
We take the radius of the sphere to be  = e−ℓ(see Figure 12.4). We eliminate all
S(⃗q)’s within the shell e−ℓ> |⃗q| > e−ℓ−δℓ. We keep δℓﬁnite but small.
12.2.4 How to properly eliminate degrees of freedom
What follows is an outline of the general arguments which allow us to eliminate
S(⃗q)’s as a procedure leading to a renormalization transformation. Suppose the
energy of the system depends on two types of variables and a single coupling
constant. Call them Si, S′
i, and K. The partition function is
Z(K) =

{Si},{S′
i}
e−H(K,Si,S′
i)
(12.5)
where 1/kBT has been absorbed into the constant K. Suppose, further, that for a
given conﬁguration of Si’s we are able to sum over the set of variables {S′
i}. We
could then deﬁne a partial partition function
Z [Si] =

{S′
i}
e−H(K,Si,S′
i)
(12.6)
The above equation can also be written in the following form:
A

K ′
e−H ′(K ′,Si) =

{S′
i}
e−H(K,Si,S′
i)
(12.7)
where H ′(K ′, Si) is an effective Hamiltonian with a reduced number of degrees of

12.2 The effective Hamiltonian
289
S(q) s in the outer shell
S(q) s in the inner zone
Fig. 12.5. The spherical Brillouin zone, the inner portion and the outer shell.
freedom and a changed coupling constant, K ′. If, through a rescaling of amplitudes
of the degrees of freedom and length scales, the Hamiltonian H ′ can be made to
take on the same form as the original Hamiltonian, H, we can write
Z N =

{Si},{S′
i}
e−H(K,Si,S′
i) =

{Si}
A(K ′)e−H ′(K ′,S′
i)
(12.8)
Z N(K) = A(K ′)Z N ′(K ′)
(12.9)
Summing over the variables {S′
i} and rescaling yields the renormalization group
equation
K ′ = R(K)
(12.10)
This is the procedure that we will follow in eliminating the S(⃗q)’s in the outer
shell of the Brillouin zone. Starting with (12.4), we split the contributions to H
accordingly. (see Figure 12.5).
H = 1
2

|⃗q|<e−ℓ−δℓ

r + q2
S

⃗q

S

−⃗q

+ u
N

|⃗q1|<e−ℓ−δℓ
...
|⃗q4|<e−ℓ−δℓ
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
+ 1
2

e−ℓ<|⃗q|<e−ℓ−δℓ

r + q2
S

⃗q

S

−⃗q

+ 4u
N

⃗q1
in shell
⃗q2,... ,q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
+ 6u
N

⃗q1,⃗q2
in shell
⃗q3,⃗q4
in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
+

⃗q1,⃗q2,⃗q3
in shell
⃗q4
in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
+ u
N

⃗q1,... ,⃗q4
in shell
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
(12.11)

290
More on the renormalization group
In the limit δℓ→0, the last two terms in (12.11) are negligibly small, and we
will thus be able to drop them. Thinning out the degrees of freedom in this way, the
partition function becomes
Z =
 
⃗q
dS(⃗q) exp

−1
2

⃗q in zone

r + q2
S(⃗q)S(−⃗q)
−u
N

all ⃗q‘s in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4

× exp

−1
2

⃗q in shell
(r + q2)S(⃗q)S(−⃗q)
−4u
N

⃗q1 in shell
⃗q2,... ,⃗q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4


× exp

−6u
N

⃗q1,⃗q2 in shell
⃗q3,⃗q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4


(12.12)
If the outer shell is thin, and we take e−ℓ= 1, which is consistent with choosing the
coefﬁcient of the q2 term equal to unity, e.g. (r + Jq2) = (r + q2), in the notation of
the previous chapter, then (r + q2)s(⃗q)s(−⃗q) can be replaced by (r + 1)s(⃗q)s(−⃗q)
in the outer shell.
Let’s focus on the spins in the shell, and suppose we expand the exponents with
respect to the quartic terms involving s(⃗q)’s within the shell. We have
exp

−1
2

⃗q in shell
(r + 1)S(⃗q)S(−⃗q)

×

1 −4u
N

⃗q1 in shell
⃗q2,... ,⃗q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4
+ 1
2
4u
N
2



⃗q1 in shell
⃗q2,... ,⃗q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4


2
+ · · ·


×

1 −6u
N

⃗q1,⃗q2 in shell
⃗q3,⃗q4 in zone
S

⃗q1

· · · S

⃗q4

δ⃗q1+···+⃗q4 + · · ·


(12.13)

12.2 The effective Hamiltonian
291
(a)
(b)
Fig. 12.6. Two vertices.
×
×
Fig. 12.7. The graphical version of the expansion.
The number of ways of pairing S(⃗qi)’s with ⃗qi in the shell is a bit complicated
to determine. Graphical methods help. Denote a vertex to which an S(⃗q) with ⃗q in
the shell and with the other S(⃗q)’s in the zone as follows: The vertices depicted in
Figure 12.6 are given by1
(a) = 4u
N

⃗q1 in shell
⃗q2,... ,⃗q4 in zone
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.14)
(b) = 6u
N

⃗q1,⃗q2 in shell
⃗q3,⃗q4 in zone
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.15)
The expansion is then as depicted in Figure 12.7 Multiplying the expansion above
by exp[−1
2

⃗q in shell(r + 1)S(⃗q)S(−⃗q)] and integrating over S(⃗q)’s with ⃗q in the
shell (the integrations required being all of the Gaussian type), we conclude that
it is only the dotted lines that are paired off that give a non-zero value to the
integral. The types of diagram that are generated are shown in Figure 12.8. A factor
of 
⃗q in shell (2π/(r + 1))1/2 multiplies the expressions depicted by the diagrams
in Figure 12.8. These diagrams can be exponentiated, leaving the expression that
is illustrated in Figure 12.9. Note that the terms in the exponential include only
connected diagrams; in particular, terms such as those asterisked in Figure 12.8 are
1 In this chapter the interaction, u, will be represented by a dot,•, while in Chapter 7 interactions were represented
by a dotted line. Here, dotted lines are utilized to represent degrees of freedom whose wave-vectors lie in the
inner Brillouin zone.

292
More on the renormalization group
...
(
(
*
Fig. 12.8. The types of diagram that are generated.
 all connected
diagrams
Fig. 12.9. The form of the exponential, in terms of diagrams. The quantity Z0 in
the ﬁgure stands for exp[−1
2

⃗Q in shell ln

 r + 1
2π

].
...
Fig. 12.10. Diagram sum that becomes a single diagram upon exponentiating.
(a)
(b)
Fig. 12.11. The two diagrams of immediate interest. The ﬁrst is a one-loop diagram
for the renormalized quadratic coefﬁcient and the other is the one-loop diagram
for the alteration of the quartic coefﬁcient.
to be excluded. That exponentiating a diagrammatic expansion removes all terms
that cannot be represented as linked diagrams is a well-known result in a variety of
contexts. In the supplement at the end of this chapter we show in detail how this
comes about for the series of diagrams shown in Figure 12.10.
Exercise 12.1
Show, by extending the arguments in the supplement at the end of this chapter, that
performing Gaussian averages of the sum of terms in Figure 12.7 leads to the linked
cluster result displayed in Figure 12.9.
The two diagrams of immediate interest to us are shown in Figure 12.11.
Figure 12.11(a) is quadratic in the S(⃗q)’s, and Figure 12.11(b) is quartic in the
spin variables. Diagram (a) contributes to a renormalized effective temperature, r,

12.2 The effective Hamiltonian
293
in the term
1
2

all ⃗q′s in zone
(r + q2)S(⃗q)S(−⃗q)
(12.16)
Figure 12.16(b) contributes to a change in the fourth order term
u
N

all ⃗q′s in zone
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.17)
Let’s see exactly what changes are induced by these two terms. For clarity, denote
by ⃗Q the ⃗q’s that are in the outer shell, and by ⃗q the ⃗q’s that are in the inner
zone. Then, performing the speciﬁc Gaussian integrals, we ﬁnd for the value of the
diagrams in Figure 12.11:
(a) = 6u
N
1
r + 1


⃗Q in shell



⃗q in zone
S(⃗q)S(−⃗q)
(12.18)
(b) = 36u2
N 2
1
(r + 1)2



⃗Q, ⃗Q
′ in shell


⃗q1,...,⃗q4
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.19)
The ﬁrst sum, 
⃗Q in shell, is proportional to the volume in q-space of the shell. In
fact, we can write

⃗Q in shell
= N Volume of shell
Volume of BZ
(12.20)
since there are exactly N ⃗q’s in the BZ. Furthermore, we also know that the volume
of the BZ is give by
VBZ = Kdd
(12.21)
where  is the radius of the BZ in q-space, and Kd is a factor that depends on the
BZ’s shape and the system’s dimensionality. This factor cancels when the ratio on
the right hand side of (12.20) is taken. The volume of the shell in q-space between
 and  −δq, where δq is small compared to , can be approximated by
δq ∂
∂

Kdd
+ O

δq2	
= dδqKdd−1
(12.22)
leading to

⃗Q in shell
= N
 d
δq

= N
e−ℓdδq
(12.23)

294
More on the renormalization group
q1
Q
Q'
q2
Fig. 12.12. A vertex in a diagram. The lines ending on the vertex are labeled with
the wave-vectors that they carry.
Recall,  = e−ℓ, which implies
dq = e−ℓ−e−ℓ−δℓ
= e−ℓdℓ
(12.24)
to lowest order in δℓ. We are thus left with
(a) =
6u
N(r + 1) Ndδℓ

⃗q in zone
S(⃗q)S(−⃗q)
(12.25)
Calculation of the term (b)
Because of “momentum conservation,” the sum over ⃗Q and ⃗Q
′ in the expression
represented by the Figure 12.11(b) is just a sum over one of those wave-vectors.
Momentum conservation at a vertex (see Figure 12.12) implies
⃗Q + ⃗Q
′ + ⃗q1 + ⃗q2 = 0
(12.26)
From (12.26), we note that there are restrictions on ⃗q1 and ⃗q2 if ⃗Q and ⃗Q
′ are to be
in the shell. The sum is actually a function of ⃗q1 and ⃗q2, u(⃗q2, ⃗q2). But, since the
variables ⃗q1 and ⃗q2 will eventually be rescaled, the largest contribution turns out
to arise from the limits ⃗q1 + ⃗q2 = 0, with ⃗Q = −⃗Q
′. The argument supporting this
approximation comes from expanding u(⃗q1, ⃗q2) in powers of ⃗q1 and ⃗q2, and then
retaining the leading term u(0, 0). The next term, which is quadratic in the ⃗q’s, is
reduced by a factor of 1/b2, where b is the scale factor, which is, in the case of
interest to us, greater than one. We thus arrive at
(b) = 36u2
N 2
1
(r + 1)2 dδℓN

⃗q1,...,⃗q4 in zone
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.27)
Combining these new terms with the original terms in (12.11) that depend only on
S(⃗q)’s in which the ⃗q’s lie in the inner zone, we obtain an effective Hamiltonian

12.2 The effective Hamiltonian
295
that has the altered form

⃗q in zone
1
2

r + 12u
r + 1dδℓ+ q2

S(⃗q)S(−⃗q)
+
 u
N −
36u2
(r + 1)2
dδℓ
N


⃗q1,...,⃗q4 in zone
S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4
(12.28)
Rescaling N, ⃗q and S(⃗q)
We rescale N ﬁrst. The number of q-lattice sites in the ﬁrst BZ is N, but there are
fewer sites in the inner zone. The number of sites scales with the volume of the
zone, which, in turn, scales as (radius)d. Thus, the number of sites in the inner zone
is reduced by
e−ℓ−δℓ
e−ℓ
d
= e−dδℓ
(12.29)
To compensate for this reduction, we replace N by N ′edδℓ.
Next, we rescale the q’s. To do this, we observe that in the inner zone the
q’s range in magnitude from zero to e−ℓ−δℓ, while for the whole zone the upper
limit on the magnitude of the wave-vectors is e−ℓ. To normalize the magnitude of a
wavelength to the radius of the BZ, we replace ⃗q by ⃗q ′e−δℓ. However, implementing
this rescaling causes the quadratic term proprtional to q2 to become
S′(⃗q ′)S′(−⃗q ′)q′2e−2δℓ
(12.30)
where
S′(⃗q ′) = S(q′e−δℓ)
= S(⃗q)
(12.31)
The rescaling of the S(⃗q)’s is performed to undo the effects of the q-rescaling.
This can be accomplished by replacing S′(⃗q ′) by S′′(⃗q ′)eδℓ. This rescaling is al-
lowed because the S′(⃗q ′)’s are continuous variables. In sum, then, our rescaling is
accomplished by the following substitutions.
N →N ′edδℓ
≈N ′(1 + δℓ)
(12.32)
⃗q →⃗q ′e−dδℓ
≈⃗q ′(1 −dδℓ)
(12.33)
S(⃗q) →S′′(⃗q ′)eδℓ
≈S′′(⃗q ′)(1 + δℓ)
(12.34)

296
More on the renormalization group
After rescaling, the Boltzmann factor looks like
exp
!
−1
2

q′

r + 12udδℓ
r + 1

(1 + 2δℓ) + q′2

S′′(⃗q ′)S′′(−⃗q ′)
"
× exp



 u
N ′ −
36u2
(r + 1)2
dδℓ
N ′

(1 + (4 −d)δℓ)

⃗q ′
1,... ,⃗q ′
4
S′′(⃗q ′
1) · · · S′′(⃗q ′
4)δ⃗q ′
1+···+⃗q ′
4



(12.35)
Because δℓis small, we are allowed to linearize all terms in δℓ. Also, the fact that the
unprimed variables are being summed over or integrated out allow us to eliminate
all primes. At the risk of a bit of confusion, we also eliminate the prime on N, in
the hope that the reader will keep in mind that the symbol N now stands for the
number of remaining degrees of freedom. With all these changes, the free energy
takes the form
1
2

⃗q

r +

2r + 12ud
r + 1

δℓ+ q2

S(⃗q)S(−⃗q)
+ 1
N

⃗q1,...,⃗q4

u + (4 −d) δℓu −
36u2
(1 + r)2 δℓ

S(⃗q1) · · · S(⃗q4)δ⃗q1+···+⃗q4 (12.36)
12.2.5 Renormalization group recursion relations
Clearly, the process outlined above can be repeated to obtain r(ℓ) and u(ℓ). If we
allow δℓto be an inﬁnitesimal, these two parameters satisfy
r(ℓ+ δℓ) = r(ℓ) +

2r(ℓ) + 12u(ℓ)d
r(ℓ) + 1

δℓ
(12.37)
u(ℓ+ δℓ) = u(ℓ) +

(4 −d) u(ℓ) −
36u(ℓ)2
(r(ℓ) + 1)2

δℓ
(12.38)
Here we have a pair of ﬁrst order, non-linear differential equations that describe
the evolution of the effective Hamiltonian under the renormalization group trans-
formations. They are a speciﬁc realization of the general transformation equations
discussed in Chapter 10 (see (10.74) and the discussion that follows). Our interest
here will not be to obtain the most accurate solution to these equations, but rather
to carry out the analysis sufﬁciently far along for a determination of the critical
exponents pertinent to the self-avoiding walk problem. This will be accomplished
to lowest order in ϵ = 4 −d. The basic idea is to solve coupled equations to lowest
order in ϵ by iteration.

12.2 The effective Hamiltonian
297
36d
ε
Fig. 12.13. The ﬂow of the renormalized coupling constant, u(ℓ), under the renor-
malization group transformation, through (12.39).
Let’s look at the equation for u(ℓ) ﬁrst. We start by setting r(ℓ) = 0. Then
du(ℓ)
dℓ
= (4 −d) u(ℓ) −36u(ℓ)2
(12.39)
Let ϵ = 4 −d be greater than zero. Integrating (12.39), we obtain
 u(ℓ)
u0
du
ϵu −36du2 =
 ℓ
0
dℓ′ = ℓ
(12.40)
After some straightforward algebra we ﬁnd for u(ℓ)
u(ℓ) =
ϵu0eϵℓ
ϵ + 36u0d

eϵℓ−1

(12.41)
Exercise 12.2
Show that integration of (12.40) leads to the result (12.41) for u(ℓ).
As ℓ→∞, u∗(ℓ) →ϵ/36d. The coupling constant u(ℓ) approaches a ﬁxed
point.
Exercise 12.3
Findtheotherﬁxedpointinthedifferentialequation(12.39).Showthatitisunstable,
in that if u(ℓ0) is close to that ﬁxed point, but not equal to it, then as ℓincreases
from ℓ0, the difference between u(ℓ) and that ﬁxed point grows. Show that the ﬂow
(i.e. change) in u(ℓ) is as depicted in Figure 12.13.
The ﬂow of the coupling constant under the transformations is shown in
Figure 12.13. The renormalization group equation for the effective reduced tem-
perature, r(ℓ), is
dr(ℓ)
dℓ
= 2r(ℓ) + 12u(ℓ)d
r(ℓ) + 1
= 2r(ℓ) + 12u(ℓ)d −12u(ℓ)dr(ℓ) + 12u(ℓ)dr(ℓ)2
r(ℓ) + 1
(12.42)

298
More on the renormalization group
If we drop the last term on the right hand side of (12.42), we have
dr(ℓ)
dℓ
= 2r(ℓ) + 12u(ℓ)d −12u(ℓ)r(ℓ)d
(12.43)
Writing r(ℓ) = t(ℓ) −6u(ℓ)d, and making use of the fact that in the regime of
interest to us u(ℓ) is small, while we are taking ϵ to be small by assumption, then,
du(ℓ)/dℓ∼O(ϵu, u2), which allows us to write
dr(ℓ)
dℓ
≈dt(ℓ)
dℓ
(12.44)
Dropping terms that are of order u(ℓ)2, we end up with
dt(ℓ)
dℓ
= 2t(ℓ) −12du(ℓ)t(ℓ)
(12.45)
Separating variables and integrating, we ﬁnd
t(ℓ) = t(0)e2ℓ−
) ℓ
0 12du(ℓ′) dℓ′
(12.46)
The integration over ℓ′ can be carried out, yielding our solution for u(ℓ)
 ℓ
0
u(ℓ′) dℓ′ = 12d
 u(ℓ)
u0
u′
 dℓ′
du′

du′
(12.47)
= 12d
 u
u0
du′
ϵu′ −36 du′2
(12.48)
= −1
3 ln

ϵ −36u′d
****
u
u0
(12.49)
which allows us to write for t(ℓ)
t(ℓ) = t(0)e2ℓ
ϵ −36u(ℓ)d
ϵ −36u0d
1/3
(12.50)
Finally, using our previous expression for u(ℓ) yields the following result for t(ℓ)
t(ℓ) = t(0)e2ℓ

ϵ
ϵ + 36 du0

eϵℓ−1

1/3
(12.51)
12.2.6 Correlation length and the critical exponent, ν
If t(0) is small, then ℓmust be large in order that t(ℓ) becomes sizable. In this case
we have
r(ℓ) ≈t(ℓ)
∝r∗(0)e(2−ϵ/3)ℓ
(12.52)
as ℓ→∞.

12.2 The effective Hamiltonian
299
Exercise 12.4 Show that (12.53) follows from (12.51) and that r∗(0) is the reduced
temperature, while the critical temperature has been shifted. What is the value of
this shift, 	Tc?
For large ℓ, the system is driven from its critical point. The variable ℓis large
but arbitrary. We can choose its value such that the correlation length ξ(ℓ) is unity,
i.e. ξ(ℓ∗) = 1. We characterize this property of the system by writing r(ℓ∗) = D,
where D is a constant. Then
e(2−ϵ/3)ℓ∗=
D
r∗(0)
(12.53)
eℓ∗=
 D
r∗(0)

1
2−ϵ/3
=

D
T −T ∗c

1
2−ϵ/3
(12.54)
In other words, to order ϵ in the exponent,
eℓ∗=

T −T ∗
c
−1
2 (1+ϵ/6)
(12.55)
We know that the correlation length decreases as e−ℓ, Thus
1 = ξ(ℓ∗)
= ξ(0)e−ℓ∗
(12.56)
or
ℓ∗= −ln ξ(0)
(12.57)
which implies
ξ(0) ∝

T −T ∗
c
−1
2 (1+ϵ/6)
=

T −T ∗
c
−ν
(12.58)
We ﬁnally arrive at the critical exponent for the correlation length, to ﬁrst order in ϵ,
ν = 1
2 + ϵ
12,
ϵ = 4 −d
(12.59)
If we set ϵ = 4 −3 = 1, then
ν = 7/12 = 0.583 33 . . .
(12.60)
In the case of the three-dimensional Ising model, ν ∼0.61 . . . . Thus, (12.60) is

300
More on the renormalization group
α,q1
α,q2
α,q1
β,Q1
α,Q1
β,Q2
β,Q2
β,q2
Fig. 12.14. The two terms contained in (12.62).
a considerable improvement over the mean ﬁeld value, 1/2, for the correlation
length exponent ν.
As we have argued in previous chapters, the magnetic system pertinent to the ran-
dom walk problem is not the one-component Ising system but one in which the spins
have n components, in the limit n →0. We now take up the task of extending the
above analysis to the full O(n) Ginzburg–Landau–Wilson effective Hamiltonian.
12.3 The O(n) model: diagrammatics
For the O(n) model, the fourth order term in the effective Hamiltonian has the form
u
N

⃗q1,...,⃗q4

⃗S

⃗q1

· ⃗S

⃗q2
 
⃗S

⃗q3

· ⃗S

⃗q4

δ

⃗q1 + · · · + ⃗q4

(12.61)
Recall that the momentum-space renormalization prescription separates the sum
over wave-vectors into those inside and outside a thin shell in q-space. The latter
set of wave-vectors will be denoted as inside the BZ and will be denoted by ⃗q,
while the wave-vectors inside the shell carry the label ⃗Q. Applying this procedure,
(12.61) is written
u
N

⃗q1,⃗q2
in BZ

⃗Q1, ⃗Q2
in shell
n

α,β=1

2Sα 
⃗q1

Sα(⃗q2)Sβ( ⃗Q1)Sβ( ⃗Q2)
+ 4Sα 
⃗q1

Sα( ⃗Q1)Sβ 
⃗q2

Sβ( ⃗Q2)

δ

⃗q1 + ⃗q2 + ⃗Q1 + ⃗Q2

(12.62)
The two terms in (12.62) are represented digrammatically in Figure 12.14. The
solid lines represent Sα(⃗q)’s in the BZ, and dashed lines represent Sβ( ⃗Q)’s in the

12.3 The O(n) model: diagrammatics
301
α,q
α,q
α,q
α,q
β,Q
(a)
(b)
Fig. 12.15. The two contributions to the renormalized reduced temperature, r.
shell. As indicated in the ﬁgure, the wavy line is to be associated with the coupling
constant, u.2
12.3.1 r renormalization
The renormalized reduced temper-ature parameter, r, is determined simply
by averaging (12.62) over the Gaussian distribution of spins in the shell:
exp[−(1/2) 
⃗Q(r + 1)Sα( ⃗Q)Sα(−⃗Q)]. The integrations are straightforward, and
we depict the two contributions to r in Figure 12.15.
Exercise 12.5
Show that
⟨Sα( ⃗Q1)Sβ(S( ⃗Q2)⟩= δα,β
1 + r δ( ⃗Q1 + ⃗Q2)
(12.63)
where the brackets denote averaging over a Gaussian distribution of the spin vari-
ables.
Exercise 12.6
Applying the result of Exercise 12.5, show that the graphical quanitites in Fig-
ure 12.15 sum to
(a) + (b) = 2n + 4
1 + r

⃗Q
in shell



⃗Q
in BZ
⃗S(⃗q) · ⃗S(−⃗q)


(12.64)
2 While dots are a simpler way of depicting interactions, the nature of the interaction in the O(n) model necessitates
the use of some kind of line for the interaction.

302
More on the renormalization group
1/2    2              + 4
{                      }
 
2
Fig. 12.16. The graphs associated with renormalization of the coupling con-
stant, u.
1/2{
{
4
α
β
β
β
β
β
β
α
α
α
α
α
u
u
u
u
u
u
Fig. 12.17. Renormalization of the fourth order coupling constant, u.
12.3.2 u renormalization
As before, the coupling constant u is renormalized by averaging over the graphs
shown in Figure 12.16. After performing the Gaussian integrations, the contribution
which renormalizes u is depicted in Figure 12.17.
Exercise 12.7
Repeat the averaging process asked for in Exercise 12.5 and show that when applied
to the graphs in Figure 12.17 lead, when summed, to
4(n + 8)
(1 + r)2
u2
N 2

⃗Q1, ⃗Q2
in shell



⃗q1,... ,⃗q4
in BZ

⃗S(⃗q1) · ⃗S(⃗q2)
 
⃗S(⃗q3) · ⃗S(⃗q4)

δ

⃗q1 + · · · + ⃗q4



(12.65)
12.4 O(n) recursion relations
The recipe for deriving the recursion equations in the O(n) case is identical,
step-by-step, to the procedure we have already performed for the one-component
spin Hamiltonian. Repeating that calculation, we end up with the O(n) recursion

12.4 O(n) recursion relations
303
relations
dr(ℓ)
dℓ
= 2r(ℓ) + 4(n + 2)u(ℓ)
1 + r(ℓ)
(12.66)
du(ℓ)
dℓ
= (4 −d)u(ℓ) −
4(n + 8)
(1 + r(ℓ))2 u(ℓ)2
(12.67)
The equations are solved, at least to ﬁrst order in ϵ = 4 −d, following the same
analytical steps as before. The correlation length critical exponent, ν, can immedi-
ately be discovered by noting that 12 is replaced by 4(n + 2) in the r-equation, and
36 by 4(n + 8) in the equation for u. With these substitutions, we ﬁnd for ν
ν = 1
2 +
n + 2
4(n + 8)ϵ + O(ϵ2)
(12.68)
The limit n = 0 is pertinent to the case of self-avoiding random walks. We ﬁnd
ν = 1
2 + ϵ
16 + O(ϵ2)
(12.69)
Setting ϵ = 1 (d = 3), we obtain
ν ≈1
2 + 1
16 = 9/16 = 0.563
(12.70)
Exercise 12.8
Perform the steps necessary to arrive at the expression (12.70) for the exponent ν.
It is interesting to compare this result with Flory’s formula for ν:
νF =
3
d + 2
= 3/5
= 0.6
(12.71)
For a “fair” comparison with the n = 0 prediction, we ought to expand Flory’s
expression for the exponent ν in powers of ϵ, and retain the ﬁrst order term in that
expansion. If we do this, we ﬁnd
νF =
3
6 −ϵ
= 1
2 + ϵ
12 + O(ϵ2)
≈7
12
(to ﬁrst order in ϵ at d = 3)
= 0.583
(12.72)

304
More on the renormalization group
Two points are worth making here. First, the ϵ expansion we have just obtained for
ν is numerically close to ϵ-expanded version of Flory’s ν, carried to the same order.
Second, there is a clear difference in the coefﬁcients of the ﬁrst-order-in-ϵ term
of the two expansions. As the ϵ expansion that we have derived here is believed
to be exact, we can see that Flory’s expression cannot be, at least just below four
dimensions. As good as the Flory exponent is, it is not an exact result.
For small ϵ, or, equivalently, for dimensions close to four, we have obtained
results for the ﬂow of the effective Hamiltonian from the critical hypersurface and
have been able to extract the critical exponent, ν. We are now in a position to
determine the scaling properties of the free energy itself. However, a number of
questions justifying our procedure have simply been ignored. For example:
(1) The expansion we produce is in the difference between the dimensionality of the space
in which the random walk takes place and four. Considering that our primary focus is
on three-dimensional walks, the expansion parameter, ϵ, is equal to one. This does not
bode well for a series truncated at low order.
(2) In fact, the kind of expansion we generate is well-known to be asymptotic, in that,
formally, its radius of convergence is equal to zero. That is, at sufﬁciently high
order, coefﬁcients in the series grow faster than exponentially, and standard summation
methods fail.
(3) An issue closely related to the one immediately above is that we do not know that
our power series in the quantity ϵ is complete. In fact, we have ignored the very real
possibility that there are important contributions that are not subsumed in a power series.
Such a contribution would be one going as e−A/ϵ.
(4) In fact, it is not proven that the renormalization group is a formally correct way to
evaluate the kinds of quantities of interest to us. In the case of statistical mechanical
systems, counterexamples have been constructed in which reasonable-looking versions
of the renormalization group manifestly fail to correctly predict the behavior of ther-
modynamic functions.
What this all means is that we cannot blithely assume that the renormalization group
provides us with the complete answer to key questions. For the time being, however,
the evidence before us indicates that the results it generates are trustworthy, and we
will, for the purposes of this book, at least, take that attitude.
12.5 The diagrammatic method
The above discussion of the renormalization group is along the lines of the origi-
nal work by Wilson (Wilson, 1971a; Wilson, 1971b; Wilson and Kogut, 1974). It
is probably the most intuitively appealing version of the method, in that it bears
a direct relationship to standard methods for the description and investigation of
self-similar systems. However, the momentum-shell method has not proven to be
a useful approach to more general ﬁeld-theoretical systems. Furthermore, it does

12.5 The diagrammatic method
305
not lend itself to calculations when the desired results require the evaluation of
terms that are high order in the fourth order coupling constant. For those pur-
poses, professional theorists must avail themselves of approaches that arise from
the adaptation of perturbation theoretical methods. These approaches are based on
differential equations derived from, or related to, the Callan–Symanzik equations
(Callan, 1970; Symanzik, 1970) of the Feynman-diagrammatic renormalization
group. The advantage of these approaches is that they allow the theorist to avail
him or herself of the tricks that have been invented over the course of two thirds of
a century of research in quantum ﬁeld theory. These stratagems have been utilized
in the determination of the critical properties of O(n) systems to quite high order in
the fourth order coupling constant, or equivalently, ϵ = 4 −d in the ϵ expansion.
The recursion relation method developed in the last two chapters has the virtue
of a direct connection with the intuitive underpinnings of the renormalization group
method. The hallmark of systems at a critical point – shared by a long self-avoiding
walk – is self-similarity. Both look the same in fundamental respect when viewed
at a variety of length scales. Indeed, exactly at the critical point of an O(n) model –
and, for the random walk, if it is inﬁnitely long – the system in question will
retain, unchanged, essential characteristics no matter what the scale on which it
is observed, as long as that length scale is sufﬁciently large. Recursion relations
describe the way in which the properties of a system in one length scale relate to
the properties of the same system when viewed at a different length scale. They
provide the most natural way of approaching the behavior of a system with scale
invariance, in that, when properly framed, they yield ﬁxed points, or unchanging
characteristics, for a system whose qualities include self-similarity.
However, the most generally utilized calculational techniques that lead to di-
rect, numerical results for the self-similarity are, in the case of random walks and
the O(n) model, derived from the perturbation theoretical methods developed to
study quantum ﬁeld theories. Over the years, a number of powerful techniques have
emerged, allowing for the relatively rapid and accurate evaluation of the multiple
integrals represented by Feynman diagrams. Indeed, the single indispensable tool
of the (pre-string theory) elementary particle theorist,3 and for just about any the-
oretical physicist concerned with the interplay between a large number of degrees
of freedom, consists of a set of strategems for the analysis of Feynman diagrams.
There is a version of the renormalization group that is formulated for use in con-
junction with diagrammatic methods. In fact, the renormalization group ﬁrst made
its appearance in (1953) papers by Stueckelberg and Peterman and by Gell-Mann
and Low (1954) in which differential equations of the Renormalization Group
type were derived from the diagrammatics of quantum electrodynamics. This
3 Aside, perhaps, from Lie algebra methods.

306
More on the renormalization group
development was effected as a central component of an inquiry into the structure of
quantum electrodynamics at high energy and momentum. Further reﬁnements of the
diagrammatically based renormalization group have led to the Callan–Symanzik
equation, which forms the basis of the most widely utilized renormalization group
method, in its application to critical phenomena and the random walk.
Because of the computational utility of diagrammatic methods, and because the
language of renormalization group is so tied up in these methods, we will in this
chapter rederive the renormalization group equations for the O(n) model making
use of the diagrammatic approach. The key relations will be a set of differential
equations,closelyrelatedtotheequationsintroducedinChapter11.Theseequations
express the fact that physical quantities are independent of certain details in the
method utilized to compute them. The differential equations lead straightforwardly
to scaling forms for key quantities, including free energies and correlation functions.
We will also see how Feynman diagrams generate expressions containing the values
of critical exponents. This approach complements the recursion relation discussion
of the previous two chapters. It will also provide the reader with the background and
vocabulary necessary to assimilate the methods that are most frequently utilized in
the analysis of ﬁeld theories appropriate to critical point behavior.
There will be no attempt in the following sections to properly indoctrinate the
reader in the mysteries of the diagrammatic renormalization group method. The
literature is already replete with books offering exhaustive prescriptions for
the successful calculation of critical exponents and thermodynamic functions in
the vicinity of critical points, and for the analysis of the asymptotic statistical prop-
erties of self-avoiding walks.4 It is, rather, the hope of the authors that the discussion
below will prepare the reader to better understand the conceptual underpinnings of
the methods described in the pedagogical literature.
12.6 Diagrammatic analysis: the two-point correlation function
A fundamental element of our approach is that we partition the multiple integrals
represented by diagrams in terms of the ordering, by magnitude, of wavenumbers
or momenta.5
For example, recall the two-point correlation function, which has the diagram-
matic expansion shown in Figure 12.18 The sum can be written as follows:
1
r + q2 +
1
r + q2 (r, ⃗q)
1
r + q2 + · · · =
1
r + q2 −(r, ⃗q)
(12.73)
4 See, for example, works by Itzykson and Drouffe (1991), Zinn-Justin (2002), Freed (1987), Kleinert and Schulte-
Frohlinde (2001), Sch¨afer (1999), Amit (1984), and Parisi (1998). This collection of references is by no means
exhaustive.
5 We will, from now on, refer to wave numbers as momenta, in keeping with standard usage in physics.

12.6 Diagrammatic analysis
307
S q S
q
( ) (
)
−
...
Fig. 12.18. The diagrammatic sum for the two-point correlation function.
Σ r,q
(
)
=
Fig. 12.19. Diagrammatic sum for the quantity (r, ⃗q). The sum is over all mass
operator insertions.
...
Fig. 12.20. Representation of the renormalized propagator.
where the diagrammatic sum for the quantity (r, ⃗q) is as shown in Figure 12.19
The quantity (r, ⃗q) is called the mass operator. The sum on the right hand side
of the equality in Figure 12.19 is over all mass operator insertions. The quantity
⟨S(⃗q)S(−⃗q)⟩is also known as a propagator. When all mass operator insertions are
taken into account, it is referred to as a renormalized propagator, and is represented
as a double line, as indicated in Figure 12.20.
Figure 12.21 represents an integral with the following form:

ddq1

ddq2

ddq3
1
r + q2
1
1
r + q2
2
1
r + q2
3
δ(⃗q1 + ⃗q2 + ⃗q3 −⃗q)
(12.74)
Here, ⃗q istheexternalmomentum,carriedbytheamputatedexternallines.Thedelta
function in (12.74) effectively reduces the number of integrations from 3d, where d
isthephysicaldimensionalityofthesystem,to2d.Inorderingthemomenta,wesplit
the multiple integral in (12.74) into a number of different contributions, according to
the ordering of the magnitudes of ⃗q, ⃗q1, ⃗q2, and ⃗q3. Of particular interest to us will be
those in which the external momentum has the smallest magnitude. We will focus
on all contributions to the mass operator for which the inequality q < q1, q2, q3

308
More on the renormalization group
Fig. 12.21. The two-loop diagram for the mass operator, aka the “watermelon
diagram.”
(a)
(b)
(c)
Fig. 12.22. (a) A contribution to the mass operator in which the internal propagator
line has the “bare” form, (r + q2)−1. (b) The same contribution, the internal prop-
agator line having been replaced by its fully renormalized version. (c) A diagram
that must be eliminated, as it represents a contribution to the diagram (b).
holds. In fact, we will reinforce this inequality and introduce a quantity q0 that
satisﬁes q < q0 < q1, q2, q3.
The contributions to the two-point correlation function in which this collection
of inequalities applies will be denoted by ′(q0;r, ⃗q), and the sum (r + q2 −′)−1
is denoted as a renormalized propagator G(⃗q). We can go further. The lines in
our diagram for  stand for the bare propagator (r + q2)−1 = G0(⃗q). It can be
veriﬁed that they may be replaced by the fully renormalized lines, provided we
eliminate certain diagrams from the full diagram sum . For example, the diagram
in Figure 12.22(a) can be replaced by the diagram in Figure 12.22(b), provided that
the diagram in Figure 12.22(c) is eliminated, as it represents a contribution to the
renormalization of the doubled propagator line in Figure 12.22(b). This procedure
is referred to as propagator renormalization.
Having done this, the contributions to the mass operator will be denoted by the
symbol ′(q0;rR, ⃗q). The quantity rR appearing in the argument is an adjusted
reduced temperature. This “shifted” version of the bare reduced temperature, r, is
equal to zero at the phase transition.

12.6 Diagrammatic analysis
309
We now assert – without justiﬁcation as yet – that the combination r + q2 + ′
has the following form when rR is equal to zero.
q2 + ′(q0; 0, ⃗q) = q2−η
0
S(q/q0)
(12.75)
We further assert that the function S allows for a power-series expansion in its
argument q/q0, that the series consists only of even-order terms in that argument,
and the series begins with at quadratic order ((q/q0)2). Note that the assertion leads
to a correlation function which goes as q2−η if q0 is set equal to q. Now, let’s
introduce a small but ﬁnite rR. As another assumption, we predict that it is possible
to expand the quantity ′(q0;rR, ⃗q) as follows:
r + q2 + ′(q0;rR, q) = q2 + ′(q0; 0, ⃗q) + rR′′(q0; 0, q) + O

r2
R

= q2−η
0
S(q/q0) + rRq2−η−y
0
S′(q/q0) + O

r2
R

(12.76)
The right hand side of (12.76) is, thus, an expansion in the adjusted reduced tem-
perature, rR, and the last line above displays the scaling form of the coefﬁcients in
this expansion. The exponent y in the last line of (12.76) is equal to 1/ν, ν being
the exponent connecting correlation length with reduced temperature (ξ ∝|r|−ν)
This correspondence follows from the deﬁnition of ξ,
ξ 2 =
)
R2C(⃗R) dd R
)
C(⃗R) dd R
= −
∂2G(⃗q)
∂q2

q+0
+
G(0)
(12.77)
where C(R) is the two-spin correlation function in real space and G(⃗q) is its Fourier
transform. Notice that the expansion in (12.76) is consistent with the following
scaling form for the mass operator contribution ′
q2 + ′(q0;rR, ⃗q) = q2−η
0
σ ′(q/q0,rRq−y
0 )
(12.78)
In fact, we expect that just this form will emerge from further analysis.
Finally, we separate out two of the lowest order terms in the right hand side of
(12.78). The ﬁrst is proportional to q2, and the second goes as rR. That is, we write
′(q0;rR, ⃗q) = q2−η
0
, q
q0
2
+ rRq−y
0
-
+ O

q4,r2
R, q2rR

(12.79)
Now, consider a given propagator line, carrying a momentum equal to ⃗q. Suppose
we collect all mass operator corrections to this line, the internal momenta of which
are larger than q0 > q. Combining those mass operator corrections with the “bare”
terms r + q2, we then separate out the ﬁrst two terms on the right hand side of

310
More on the renormalization group
=
+
+
+
+
...
S( q1)S(q2)S(q3)S(q4)
Fig. 12.23. The diagrammatic expansion of the average ⟨S(⃗q1)S(⃗q2)S(⃗q3)S(⃗q4)⟩.
(12.79). We then incorporate those terms into a (partially) renormalized propagator
line, which takes the form
q−2+η
0
1
rRq−y
0
+ (q/q0)2
(12.80)
This new propagator line will be the one used in all diagrammatic expansions. All
contributions to the renormalized correlation function will be treated as perturba-
tions.6
12.6.1 The renormalized fourth order coupling
The next quantity of interest is the average ⟨S(⃗q1)S(⃗q2)S(⃗q3)S(⃗q4)⟩, referred to as
a four-point correlation function. The perturbation expansion for this quantity in
powers of u is as shown in Figure 12.23. The amputated versions of these diagrams
as shown in Figure 12.24. The quantity 4, equal to the sum of the diagrams in
Figure 12.24, is of interest to us, in that it amounts to a modiﬁcation of the fourth
order coupling constant, previously denoted as the constant u. As Figure 12.24
makes clear, 4 has the external momentum-carrying lines removed.
Again, there are internal and external momenta. In the case of the fourth order
coupling we can identify four external momenta, restricted by the requirement that
they sum to zero. If we denote those external momenta by qext
i , the index i ranging
from one to four, then we can also introduce a magnitude q0 that interposes between
thesmallerexternalmomentaandlargerinternalmomentaofaclassofcontributions
to the modiﬁed fourth order coupling. We then conjecture the following scaling form
6 A note to the ﬁnicky: the result for the propagator in (12.80) is not correct for the fully renormalized propagator.
However, it contains the essence of the corrections leading to exact critical point behavior.

12.6 Diagrammatic analysis
311
...
Γ4
Fig. 12.24. The diagrammatic expansion of the “amputated” modiﬁcation of
the four-point correlation function, ⟨S(⃗q1)S(⃗q2)S(⃗q3)S(⃗q4)⟩. The notation for this
quantity is 4.
for the sum of the bare fourth order coupling and all corrections to that quantity
uR(q0;rR, ⃗q ext
1
, . . . , ⃗q ext
4
) = q4−d+2η
0
g(q0,rRq−y
0 , ⃗q ext
1
/q0, . . . , ⃗q ext
4
/q0)
(12.81)
Note that the quantity g contains an additional explicit q0-dependence.
In this case, we focus on the contribution to the renormalized fourth order cou-
pling that survives the setting of all external momenta equal to zero. According to
(12.81), this term will have the form
q4−d+2η
0
g0(q0;rR)
(12.82)
This is the fourth order term that will be utilized in our diagrammatic expansions.
Relationship of the diagrammatic method to the momentum-shell
renormalization group: recursion relations
The idea of taking into account ﬂuctuations greater than a certain infrared cutoff is
intrinsic to the momentum shell renormalization group. In fact, this is precisely what
occurs as the size of the Brillouin zone shrinks. The modiﬁcations to the effective
Hamiltonian that result from this process of elimination of degrees of freedom
(and rescaling) correspond precisely to the kinds of calculations envisioned in the
above discussion. The diagrammatic method refers to the evaluation of effective
Hamiltonian modiﬁcations according to the rules of perturbation theory, while in
the momentum-shell renormalization group those modiﬁcations are determined
recursively. If both of the above programs are carried out completely, the same
expressions would result.
This clearly implies that recursion relations will also arise from the diagrammatic
approach. In fact, as we will now show, one is led directly to a differential equation
for the quantity g0(q0) appearing in (12.82). To see how this comes about, consider a
diagramcontributingtotherenormalizedfourthordercoupling.Onesuchdiagramis

312
More on the renormalization group
Fig. 12.25. Diagram contributing to the renormalization of the fourth order cou-
pling. The double lines for both propagators and interactions indicate that both
quantities are replaced by appropriately renormalized versions, as given in (12.82)
and (12.80).
shown in Figure 12.25. Setting all the external momenta equal to zero, the momenta
of the internal propagator lines will be ˇQ and −⃗Q. This means that the q0 associated
with those lines must be greater than or equal to Q. Let’s set the two equal. Then,
the integral that this diagram represents goes as

u2
R(Q)G2(Q) dd Q =

g0(Q)2Q8−2d+4η
Q−4+2η
(rRQ−y + 1)2 Qd−1 dQ
(12.83)
In keeping with the philosophy of this approach, we will set the lower limit of the
integration over Q equal to q0, and we take the limit rR = 0. The self-consistent
contribution to the renormalized coupling strength g0(q0) is, then, proportional to

q0
g0(Q)2Q3−d+2η dQ
(12.84)
Now, this integral is a contribution to the renormalized coupling strength uR. The
quantity g is related to uR by
g0(q0) = q−4+d−2η
0
uR(q0;rR = 0, ⃗q1 = 0, . . . , ⃗q4 = 0)
(12.85)
If we take the derivative of this integral with respect to the lower limit, q0, we end up
withacontributiontotherenormalizedcouplingstrengththatgoesas g0(q0)2/q0 and
the contribution (−4 + d −2η)g0(q0)/q0. Both these terms combine to generate the
following differential equation for g0(q0):
q0
dg0(q0)
dq0
= (−4 + d −2η)g0(q0) + Bg0(q0)2
(12.86)
This is precisely the form of the equation that describes the development of the
fourth order coupling in the momentum-shell renormalization group, as embodied
in (12.39) and (12.67). Given the connection between the diagrammatic method
outlined here and that version of the renormalization group, the connection between
the two equations should be no surprise.
A corresponding recursion relation can be derived for the renormalized propa-
gator. In this case, we consider the diagrams contributing to the renormalization

12.6 Diagrammatic analysis
313
Fig. 12.26. The one-loop diagram leading to contributions to the renormalized
propagator shown in (12.87) and (12.88).
of the new reduced temperature, rR. There are two principal ways in which these
contributions arise. The ﬁrst is through an alteration in the temperature at which
rR = 0. This alteration arises from contributions to the mass operator for which
the external momentum is equal to zero, and for which the effective reduced tem-
perature apperaing in internal propagator lines is also zero. At lowest level in per-
turbation theory, the expression for this contribution, depicted diagrammatically in
Figure 12.26, is proportional to

Q>q0
uR(Q)G(Q) dd Q =

Q>q0
(q0(Q))4−d+2η g0(q0(Q))(q0(Q))−2+η
(Q/q0(Q))2 Qd−1 dQ
=

Q>q0
Q1−ηg(Q) dQ
(12.87)
In this expresion, we have taken the appropriate value for the “internal” cutoff,
q0(Q) to be equal to Q, in the case of the effective fourth order coupling and the
mass operator that modiﬁes the propagator line inside the integration. The result
of this integration will be a contribution to the renormalized propagator going
as q2−η
0
, due to the behavior of the integrand in (12.87) near its lower limit of
Q = q0, and another term that is essentially independent of q0, arising from all
other contributions to the integration. These latter contributions are what control
the actual shift in the transition temperature.
The second type of contribution to the renormalized propagator is linear in the
new reduced temperature. It follows from an expansion of the propagator line in
the contribution shown in Figure 12.26 to ﬁrst order in rR. The integral form of this
contribution is

Q>q0
g(q0(Q)) (q0(Q))4−d+2η rR (q0(Q)) (q0(Q))2−η−y Qd−1 dq
=

Q>q0
g(Q)Q−y+1−ηrR dQ
→rRg(q0)q2−η−y
0
y −2 + η
(12.88)

314
More on the renormalization group
To achieve consistency, this implies that the difference between the exponent y and
2 −η is proportional to the asymptotic ﬁxed point value of the modiﬁed coupling
strength g(q0). In fact, if the appropriate combinatorial weights and prefactors
are inserted into the integrals in (12.87), we end up with the correct value for y by
requiring that the last line of that equation be identical to rRq2−η−y
0
. The relationship
(12.88) can also be processed so that it takes the form of a differential equation. In
that case, we have an equation equivalent to (12.66).
Thecalculationofthecriticalexponentη proceedsalongsimilarlines.Inthiscase,
one extracts the term proportional to q2 in the correlation function diagram shown
in Figure 12.21, in which the fourth order couplings are partially renormalized
according to the prescription laid out in the previous pages and the propagators are
modiﬁed by the mass operator ′. The calculations are a bit more involved than
those outlined immediately above, in that the enforcement of the requirement that
internal momenta are greater than the cutoff q0 is a bit more involved. However, one
ﬁnds that this can be done, and a value for the exponent η appears as if by magic.
12.6.2 Construction of a complete, self-consistent diagrammatic method
based on the above results
The calculation of the exponents y and η, and of the behavior of the coupling
strength, g, provides the basis of a self-consistent and convergent calculation of
quantities in the vicinity of the critical point. In particular, their incorporation into a
modiﬁed perturbation theory sufﬁces to “tame” the diagrammatic evaluation of cor-
relation functions and thermodynamic quantities in the vicinity of the critical point
of an O(n) model. This means that we can also utilize a suitably “renormalized”
perturbation theory to obtain the statistical properties of a long self-avoiding walk.
The key to the convergence of the diagrammatic method lies in the scal-
ing properties of the fourth order coupling constant and of propagators. Recall
the assumed scaling form of the modiﬁed fourth order coupling constant, uR,
as embodied in (12.81). Assuming a ﬁxed point for the amplitude g, we have
uR(q0;rR = 0, ⃗qi = 0) ∝q4−d+2η
0
. In fewer than four dimensions, the (partially)
renormalized fourth order coupling constant at the critical temperature, and in the
limit of arbitrarily small external momenta, disappears as the lower limit on internal
momenta in the diagrammatic contributions is reduced to zero. This implies that a
fully renormalized version of the fourth order coupling at the critical point – and
for which all the external momenta have been set equal to zero – has vanishingly
small amplitude. It is this evaporation of the fourth coupling at the critical point
that “rescues” perturbation theory.
To see in broad outline how this comes about, we’ll revisit the power counting
argument in Chapter 7. Naive power counting tells us that the contribution to a

12.6 Diagrammatic analysis
315
Fig. 12.27. Diagram contributing to the renormalization of the mass operator. Both
the propagator lines and the interactions are renormalized.
given quantity that is nth order in the “bare” coupling strength, u, contains a term
going (ur−(4−d)/2)n. This results in a divergent perturbation series as r →0 when
the spatial dimensionality of the system, d, is less than four. However, perturbation
theory is rescued through renormalization. The divergent factor r−(4−d)/2 is can-
celled in all orders of the perturbation expansion by a renormalized fourth order
coupling constant that goes to zero as r(4−d)/2, neglecting the critical exponent η. As
a simpliﬁed illustration consider the second order contribution to the mass operator
shown in Fig. 12.27. This diagram is proportional to the following multiple integral.

u2G(Q1)G(Q2)G(Q3) ddQ1 ddQ2 ddQ3 δ( ⃗Q1 + ⃗Q2 + ⃗Q3 −⃗q)
(12.89)
If the interaction strength, u and the propagator lines, G(Qi), were to take on
their “bare” forms, so that u is constant and G(Qi) = 1/(r + Q2
i ), then, scaling
the reduced temperature, r, out of the integrand we are left with an expression
going as u2r2d−3 = r−1 × (ur−(4−d)/2)2. On the other hand, if u →gq4−d+2η
0
and
G ∝q−2+η
0
f (rq−y
0 , Qi/q0), then, scaling the cutoff q0 out of the integrand, we
end up with a contribution to the mass operator going as q2−ηu2F(rq−y
0 ). This
is consistent with the assumed form of the mass operator contributions to the
correlation function, and, hence, the propagator.
This argument can be made more rigorous, and the results above can be shown
to follow at all orders in perturbation theory.
Counterterms
The introduction of renormalized vertices and propagators into diagrammatic calcu-
lations carries with it a redundancy for which one must compensate. As an example,
consider Figure 12.28 in which (a), (b), and (c) correspond to three different de-
pictions of a single diagram for a mass operator insertion on a propagator line.
There are two fourth order couplings in each diagram. The portions of the diagrams
that are surrounded by the dashed oval can be associated with a modiﬁcation of
the fourth order coupling in diagram (a). This fourth order coupling is depicted
as a doubled wavy line, to indicate that it has been modiﬁed by the incorporation
of corrections. Now, replacing the fourth order coupling by the modiﬁed version
calculated as described in the sections above amounts to the inclusion of diagrams

316
More on the renormalization group
(a)
(b)
(c)
(d)
Fig. 12.28. Illustrating how portions of a diagram ((a), (b), and (c)) can be associ-
ated with a renormalization of the fourth order coupling, which is then incorporated
into a lower order diagram, (d).
such as (a) in the putative low order diagram (d). The separate evaluation of the
higher order diagram then introduces a “double counting” of its inﬂuence on the
properties of the system under consideration. It is necessary to perform subtractions
to remove this double counting. This is done by the introduction of counterterms,
which compensate for the incorporation of higher order diagrams into lower order
ones through the modiﬁcations of fourth order couplings generated by the higher
order diagrams.
This can be done in more than one way. The most straightforward is to order
the multiple integral represented by a diagram according to the magnitudes of the
internal momenta. Then, when the internal momenta in a portion of the diagram that
can be associated with a modiﬁcation of a fourth order coupling are all greater than
the momenta of the propagator lines attached to that portion, one subtracts from
the multiple integral a “diagram” in which that portion is replaced by one in which
all external momenta have been set equal to zero, and for which (in the portion) the
reduced temperature is equal to zero. This approach can be shown to work. However,
as a practical matter, it is extremely unwieldy. The jumble of multiple integrals that
emerge at higher order quickly becomes intractable. A simpler calculational trick
involves the introduction of what are called counterterms in the original effective
Hamiltonian, corresponding to the corrections to the fourth order coupling. Those
counterterms are then inserted strategically into diagrams. If properly exploited,
they account for all corrections and cancel the overcounting alluded to above.
Similar stratagems allow the theorist to avoid redundancies associated with the
modiﬁcations of the mass operator leading to the exponents η and y = 1/ν.

12.7 Supplement
317
12.6.3 Differential equations for correlation functions
Once the renormalized fourth order coupling has been determined with the use
of recursion relations and the exponents y and η have been obtained via calcula-
tions of the properties of the partially modiﬁed mass operator, and after one has
reconstituted correlation or thermodynamic functions by performing diagrammatic
calculations with the new quantities, one is left with expressions for physically
meaningful quantities, such as correlation functions, or thermodynamic potentials,
or susceptibilities, or numbers of random walks. In the course of the calculations,
the quantity q0 has been introduced, and, as a practical matter, it will have been kept
equal to a non-zero value. However, this cutoff is associated with a calculational
method. It is not a physical quantity. In fact, one generally has the freedom to adjust
it at will, within certain limits. This means that the result must be independent of
the precise value of q0. One can formalize this independence by taking a derivative
of the quantity of interest with respect to q0 and checking that this derivative is
equal to zero. In fact, one can turn q0-independence into a requirement on all calcu-
lated quantities. Such an approach leads to a differential equation for key functions,
known generically as a Callan–Symanzik equation (Callan, 1970; Symanzik, 1970).
This equation plays a central role in ﬁeld-theoretical approaches to the renormal-
ization group. The reader is directed to any one of the many pedagogical books that
have been written on the subject, mentioned earlier in a footnote in Section 12.5.
12.7 Supplement: linked cluster expansion
Here, we will demonstrate that averaging an inﬁnite series of diagrams leads
to the exponential of a linked cluster expansion. This equality is depicted in
Figure 12:S-1. The quantity ⟨. . . ⟩signiﬁes a Gaussian average of all S(⃗q)’s in
the shell (see (12.13) in Section 12.2.4).
We start with the basic form of the effective Hamiltonian, as given in (12.3).
Then, we divide the spin degrees of freedom, S(⃗q), into those having argument ⃗q
in the inner Brillouin zone and those whose argument lies in the shell surrounding
that zone. To keep things clear, we replace the argument ⃗q by the argument ⃗Q when
it lies in the shell. The quadratic portion of the Hamiltonian containing S( ⃗Q)’s is

⃗Q
1
2

r + Q2
S( ⃗Q)S(−⃗Q)
(12:S-1)
The quartic term splits up as indicated in (12.11). The quartic term with two S( ⃗Q)’s
and two s(⃗q)’s is shown in Figure 12.6b, or Figure 12.12. The exponential of this
vertex is expressed pictorially as an expansion in Figure 12.10. If we integrate over
the S( ⃗Q)’s we are performing a Gaussian average of this inﬁnite sum of diagrams.

318
More on the renormalization group
(   )
all other
connected 
diagrams
Fig. 12:S-1. The equality that is encapsulated in the linked cluster expansion.
The left hand side of the equality is a sum of all diagrams generated by splitting
the S(⃗q)’s into spin degrees of freedom for which the argument lies in the inner
Brillouin zone and degrees of freedom whose arguments lie in the shell surrounding
that Brillouin zone, as depicted in Figure 12.5.
type 1
type 2
type 3
Fig.12:S-2. Waysinwhichthefour-pointdiagrammaticfragmentsinthetoplineof
Figure12:S-1canbejoinedbytheGaussianpairingoflinescorrespondingto S(⃗q)’s
for which ⃗q is in the spherical shell surrounding the reduced Brillouin zone.
Each term in the sum consists of a product of the quartic vertex, which we will
call a quartic fragment from now on. In averaging over a quartic fragment, we pair
S( ⃗Q)’s up in all possible ways, performing the Gaussian average over each pair.
This Gaussian average leads to the result for each pair
⟨S( ⃗Q1)S( ⃗Q2)⟩= δ

⃗Q1 + ⃗Q2

1
r + Q2
1
(12:S-2)
Some of the expressions that result are illustrated in Figure 12:S-2. The dashed
lines in the ﬁgure now correspond to averages of pairs of S( ⃗Q))’s, and are equal
to 1/(r + Q2). Because the shell surrounding the BZ is thin, the Q in this factor
can be set equal to a constant in all the diagrams. Note that all the diagrams shown
in Figure 12:S-2 are connected, in that they cannot be separated into two different
parts, except by severing one or more dashed lines. In fact, in the case at hand,

12.7 Supplement
319
Fig. 12:S-3. The generation of n type 1 diagrams from a product of n quartic
fragments. The top line is the product of n fragments. The second line indicates
how the S( ⃗Q)’s are paired. The bottom line displays the resulting product of
diagrams.
(    )
Fig. 12:S-4. The sum of diagrams when spins are paired to produce type 1
diagrams.
separating a connected diagram into two component parts requires the severing of
at least two dashed lines.
Let’s start by considering what happens if we count only the “type 1” connected
diagram in Figure 12:S-2. The process by which a set of n diagrams of this type
is generated upon averaging over S( ⃗Q)’s associated with the product of n quartic
fragments is shown in Figure 12:S-3. There is clearly only one way to achieve
the pairings indicated in Figure 12:S-3. This means that the averaged sum shown
in the top line of Figure 12:S-1 generates the sum shown in Figure 12:S-4. If we
denote by T1 the expression corresponding to the type 1 diagram, then the result of
the operations depicted in Figure 12:S-4 is
1 + T1 + 1
2!T 2
1 + 1
3!T 3
1 + · · · = eT1
(12:S-3)

320
More on the renormalization group
Fig. 12:S-5. The pairing of the lines in a product of ﬁve quartic fragments pro-
ducing a product of three type 1 diagrams and 1 type 2 diagram.
Asthenextstepinthisdemonstration,weconsiderwhathappenswhenthe S( ⃗Q)’s
in a product of quartic fragments are paired so as to produce both type 1 and type
2 diagrams. This process is illustrated in Figure 12:S-5. There is a combinatorial
factor associated with the collection of diagrams on the bottom line in Figure 12:S-5.
This factor is equal to the number of ways in which the S( ⃗Q)’s can be paired so
as to form those diagrams. The middle line in the ﬁgure depicts one of the ways in
which this can be done. There are others, however. A bit of investigation, and we
ﬁnd that there are 5!/(3! 2!) ways of doing this.
Exercise 12.9
Perform the investigation leading to the above result for the number of ways of
forming the diagrams at the bottom of Figure 12:S-5.
It is possible to argue for the factor as follows. Start by ordering the ﬁve fragments
in all possible ways. Then, take the ﬁrst three and form type 1 diagrams from them,
and form a type two diagram from the last two. This yields the factor 5!, the number
of ways of ordering ﬁve objects. However, we have overcounted the distinct number
of ways of forming the diagrams using this method. If we were to interchange any
two of the ﬁrst three fragments, we would end up with the same collection of
diagrams. This also holds true if we were to interchange the last two fragments.
In order to compensate for this overcounting, we divide 5! by 3!, the number of
ways of permuting the ﬁrst three fragments, and also by 2!, the number of ways of
permuting the last two. In this way, we end up with the overall combinatorial factor
of 5!/(2! 1!).
To make things a bit more complicated, imagine that we had n = n1 + 2n2
fragments, and that we paired S( ⃗Q)’s so as to construct n1 type 1 diagrams and
n2 type 2 diagrams, in a generalization of the process depicted in Fig. 12:S-6. The

12.7 Supplement
321
×
×
×
× ...
×
× ...
Fig. 12:S-6. An overall diagram consisting of n1 type 1 diagrams and n2 type 2
diagrams.
combinatorial factor that tells us how many ways there are to do this
n!
n1!n2!(2!)n2
(12:S-4)
Exercise 12.10
Show, by a generalization of the argument just given, that the combinatorial factor
(12:S-4) correctly counts the number of ways of combining n = n1 + 2n2 quartic
fragments so as to produce n1 type 1 diagrams and n2 type 2 diagrams.
As there is a factor of 1/n! associated with the original set of fragments in this
example, we see that the product of n1 type 1 diagrams and n2 type 2 diagrams
that have been formed as a result of the pairing of S( ⃗Q)’s will have the overall
combinatorial factor 1/(n1!n2!(2!)n2) associated with it. If we denote by 2!T2 the
expression depicted by the type 2 diagram and sum over all possible values of n1
and n2, we obtain
∞

n1=0
∞

n2=0
T n1
1
n1!
T n2
2
n2! = exp [T1 + T2]
(12:S-5)
We can now generalize. Suppose we have a set of n fragments, which, as a result
of the Gaussian averaging, leads to a product of n1 type 1 diagrams, n2 type 2
diagrams, n3 type 3 diagrams, . . . . The combinatorial factor that tells us how many
ways this can be done will be equal to
n!
n1! (n2!(2!)n2) (n3!( f3)n3) . . .
(12:S-6)
The factor f3 is the number of ways of permuting vertices in a type 3 diagram
without disrupting the pairing of S( ⃗Q)’s that leads to that diagram.

322
More on the renormalization group
Exercise 12.11
Show that f3 = 6 in the case of the type 3 diagram shown in Figure 12:S-2.
If, now, we denote by fnTn the type n diagram, we then discover that summing
over all possible ways of forming connected diagrams yields the following:
∞

n1=1
∞

n2=0
∞

n3=0
· · · T n1
1
n1!
T n3
2
n2!
T n3
3
n3! · · · = exp [T1 + T2 + T3 + . . . ]
(12:S-7)
The right hand side of Figure 12:S-1 is the graphical representation of the right
hand side of (12:S-7). We now have our desired result.

References
Abramowitz, M. and Stegun, I. A. (1970). Handbook of Mathematical Functions: with
Formulas, Graphs, and Mathematical Tables. Dover Publications, New York.
Amit, D. J. (1984). Field Theory, The Renormalization Group, and Critical Phenomena.
World Scientiﬁc, Singapore, revised 2nd edn.
Aronovitz, J. A. and Nelson, D. R. (1986). Universal features of polymer shapes. Journal
de Physique, 47(9):1445–56.
Ashcroft, N. W. and Mermin, N. D. (1976). Solid State Physics. Holt, Rinehart and
Winston, New York.
Barber, M. N. and Ninham, B. W. (1970). Random and Restricted Walks; Theory and
Applications. Gordon and Breach, New York.
Berg, H. C. (1993). Random Walks in Biology. Princeton University Press, Princeton, NJ,
expanded edn.
Berg, H. C. and Purcell, E. M. (1977). Physics of chemoreception. Biophysical Journal,
20(2):193–219.
Bishop, M. and Michels, J. P. J. (1986). Polymer shapes in three dimensions. Journal of
Chemical Physics, 85(10):5961–2.
Boas, M. L. (1983). Mathematical Methods in the Physical Sciences. Wiley, New York,
2nd edn. (A good introduction to combinatorial counting and elementary concepts in
probability theory.)
Bookstein, F. L. (1978). The Measurement of Biological Shape and Shape Change.
Lecture notes in biomathematics; 24. Springer-Verlag, Berlin, New York.
Callan, C. G., J. (1970). Broken scale invariance in scalar ﬁeld theory. Physical Review D,
2(8):1541–7.
Costa, L. d. F. and Cesar, R. M. (2001). Shape Analysis and Classiﬁcation: Theory and
Practice. Image processing series. CRC Press, Boca Raton, FL.
de Gennes, P. G. (1972). Exponents for the excluded volume problem as derived by the
wilson method. Physics Letters A, 38(5):339–340.
—(1979). Scaling Concepts in Polymer Physics. Cornell University Press, Ithaca, NY.
des Closeaux, J. and Jannink, G. (1990). Polymers in Solution: Their Modeling and
Structure. Clarendon Press, Oxford.
Diehl, H. W. and Eisenriegler, E. (1989). Universal shape ratios for open and closed
random walks: exact results for all d. Journal of Physics A (Mathematical and
General), 22(3):L87–91.
Emery, V. J. (1975). Critical properties of many-component systems. Physical Review B
(Condensed Matter), 11(1):239.
323

324
References
Feller, W. (1968). An Introduction to Probability Theory and its Applications. Wiley
series in probability and mathematical statistics. Wiley, New York.
Feynman, R. P. and Hibbs, A. R. (1965). Quantum Mechanics and Path Integrals.
International series in pure and applied physics. McGraw-Hill, New York.
Fixman, M. (1962). Radius of gyration of polymer chains. Journal of Chemical Physics,
36:306–10.
Flory, P. J. (1953). Principles of Polymer Chemistry. Cornell University Press, Ithaca, NY.
—(1969). Statistical Mechanics of Chain Molecules. Interscience, New York.
Freed, K. F. (1987). Renormalization Group Theory of Macromolecules. Wiley, New York.
Gaspari, G., Rudnick, J., and Beldjenna, A. (1987). The shapes of open and closed
random walks: a 1/d expansion. Journal of Physics A (Mathematical and General),
20(11):3393–414.
Gell-Mann, M. and Low, F. E. (1954). Quantum electrodynamics at small distances.
Physical Review, 95:1300–1312.
Gradshteyn, I. S., Ryzhik, I. M., and Jeffrey, A. (2000). Table of Integrals, Series, and
Products. Academic Press, San Diego, 6th edn.
Hughes, B. D. (1995). Random Walks and Random Environments. Clarendon Press,
Oxford; Oxford University Press, New York.
Itzykson, C. and Drouffe, J.-M. (1991). Statistical Field Theory. Cambridge monographs
on mathematical physics. Cambridge University Press, Cambridge, 1st paperback
edn.
Jackson, J. D. (1999). Classical Electrodynamics. Wiley, New York, 3rd edn.
Jeffreys, H. (1972). Methods of Mathematical Physics. Cambridge University Press,
Cambridge, 1st paperback of 3rd edn. (A good introduction to the theory of
asymptotic expansions and the method of steepest descents.)
Kleinert, H. (1995). Path Integrals in Quantum Mechanics, Statistics, and Polymer
Physics. World Scientiﬁc, Singapore.
Kleinert, H. and Schulte-Frohlinde, V. (2001). Critical Properties of φ4-theories. World
Scientiﬁc, River Edge, NJ.
Kosmas, M. K. and Freed, K. F. (1978). On scaling theories of polymer solutions. Journal
of Chemical Physics, 69(8):3647–59.
Kramers, H. A. (1946). The behavior of macromolecules in inhomogeneous ﬂow. Journal
of Chemical Physics, 14:415–24.
Mandelbrot, B. B. (1982). The Fractal Geometry of Nature. Freeman, San Francisco.
Maris, H. J. and Kadanoff, L. P. (1978). Teaching the renormalization group. American
Journal of Physics, 46(6):652–7.
Montroll, E. W. (1956). Random walks on multidimensional spaces, especially on periodic
lattices. Journal of the Society for Applied and Industrial Mathematics, 4:241–60.
Montroll, D. and Shlesinger, M. F. (1983). The wonderful world of random walks. In
Falk, H., ed., CCNY Physics Symposium in Celebration of Melvin Lax’s Sixtieth
Birthday, page 364, City College of New York Physics Dept., New York.
Montroll, E. W. and Weiss, G. H. (1965). Random walks on lattices ii. Journal of
Mathematical Physics, 6:364.
Morse, P. M. and Feshbach, H. (1953). Methods of Theoretical Physics. McGraw-Hill,
New York.
Nauenberg, M. (1975). Renormalization group solution of the one-dimensional Ising
model. Journal of Mathematical Physics, 16(3):703–5.
Nelson, D. R. and Fisher, M. E. (1973). Exact renormalisation groups for one-dimensional
spin systems. 19th Annual Conference on Magnetism and Magnetic Materials,
Boston, MA, USA, 13-16 Nov. 1973. AIP Conf. Proc. (USA), pages 888–90.

References
325
Parisi, G. (1998). Statistical Field Theory. Perseus Books, Reading, MA.
P´olya, G. (1919). Quelques probl`emes de probabilit´e se rapportant `a la ‘promenade au
hasard’ (Some problems of probability associated with the ‘random walk’).
L’Enseignement Math´ematique, 20:444–445.
—(1921). ¨Uber eine aufgabe der wahrscheinlichkeitsrechnung betreffen die irrfahrt im
straßennetz (On a theorem of probability calculus concerning wandering in a
network of streets). Mathematische Annalen, 83:149–160.
Redner, S. and Reynolds, P. J. (1981). Single-scaling-ﬁeld approach for an isolated
polymer chain. Journal of Physics A (Mathematical and General), 14(3):L55–61.
Rudnick, J. and Gaspari, G. (1986a). The aspherity of random walks. Journal of Physics
A (Mathematical and General), 19(4):L191–3.
—(1986b). Bond percolation on a ﬁnite lattice: the one-state Potts model reconsidered.
Journal of Statistical Physics, 42(5–6):833–60.
Rudnick, J., Beldjenna, A., and Gaspari, G. (1987). The shapes of high-dimensional
random walks. Journal of Physics A (Mathematical and General), 20(4):971–84.
Sch¨afer, L. (1999). Excluded Volume Effects in Polyer Solutions, As Explained by the
Renormalization Group. Springer-Verlag, Berlin.
Solc, K. and Stockmayer, W. H. (1971). Shape of a random ﬂight chain. Journal of
Chemical Physics, 54:2756–57.
Stueckelberg, E. C. G. and Peterman, A. (1953). Helvetia Physica Acta, 26:499.
Symanzik, K. (1970). Small distance behaviour in ﬁeld theory and power counting.
Communications in Mathematical Physics, 18(3):227–46.
Theodorou, D. N. and Suter, U. (1985). Shape of unperturbed linear polymers:
polypropylene. Macromolecules, 18:1206–14.
Watson, G. N. (1939). Three triple integrals. Quarterly Journal of Mathematics, Oxford
Series (1), 10:266.
Weiss, G. H. (1994). Aspects and Applications of the Random Walk. Random materials
and processes. North-Holland, Amsterdam.
Weiss, G. H. and Rubin, R. J. (1976). The theory of ordered spans of unrestricted random
walks. Journal of Statistical Physics, 14(4):333–50.
Widom, B. (1965). Surface tension and molecular correlations near the critical point.
Journal of Chemical Physics, 43:3892–905.
Wilf, H. S. (1994). Generatingfunctionology. Academic Press, Boston, 2nd edn. (A very
nice, informal but comprehensive discussion of generating functions and their
applications in a variety of contexts.)
Wilson, K. G. (1971a). Renormalization group and critical phenomena. I.
Renormalization group and the Kadanoff scaling picture. Physical Review B (Solid
State), 4(9):3174–83.
—(1971b). Renormalization group and critical phenomena. II. Phase-space cell analysis
of critical behavior. Physical Review B (Solid State), 4(9):3184–205.
Wilson, K. G. and Kogut, J. (1974). The renormalization group and the epsilon expansion.
Physics Reports. Physics Letters Section C, 12(2):75–200.
Ziman, J. M. (1979). Principles of the Theory of Solids. Cambridge University Press,
Cambridge, 1st paperback edn.
Zinn-Justin, J. (2002). Quantum Field Theory and Critical Phenomena. Clarendon Press,
Oxford; Oxford University Press, New York, 4th edn.


Index
amputated diagram, 182, 310
asphericity of a random walk, 131–5
asymptotic expansion, 18, 21
Bessel function, 27, 80
modiﬁed, 107
biased random walk, 95–8
one-dimensional, 95–7
three-dimensional, 97–8
binomial coefﬁcient, 3, 7
binomial expansion, 15
binomial probability distribution, 5
block spins, 277–84
boundary conditions, 69–93
absorbing, 75, 78, 80
periodic, 46, 74
reﬂecting, 84
branch cut, 106, 112, 113, 115, 116
branch point, 49
Brillouin zone, 33, 34, 43, 272
broken symmetry
Brownian particles, 28
Callan–Symanzik equations, 305, 317
Cantor set, 258
capacitor, 91, 92
Cauchy formula, 17, 19, 57, 60
center of gravity of a random walk, 130
central limit theorem, 12, 38
codimension, 282
coin ﬂipping, 2
compact walks, 246
concentration of walkers, 84, 85, 87–9, 91
condensed phase, 234, 239, 245, 246
connected diagrams, 291, 322
connectivity, 218
continuity equation, 84
continuous renormalization group, 275–7
convolution, 120
coordinate basis, 222
correlation function, 209, 213, 220
correlation hole, 245, 251–3
correlation length, 218, 219, 249, 272, 277, 279, 280,
283, 298
coupled renormalization group equations, 296
coupling parameters, 280
critical exponent
α, 210
β, 210
δ, 210
η, 210, 309
γ , 210
ν, 203, 210, 298
critical fugacity, 196, 218
critical hypersurface, 280, 281
critical point, 19, 34, 40, 204–5, 208, 211, 216, 218,
227, 277
critical temperature, 204
Curie temperature, 204
current density, 83, 84, 88
1/d expansion, 142, 144–52
de Moivre, 2
de Moivre’s theorem, 38
decimation, 265
diffusion
steady state, 83–91
diffusion equation, 14, 39
Dirac bra and ket, 110
displacement vectors, 139
Dyson’s equation, 179, 182–4
effective Hamiltonian, 236
electrostatic potential, 83–93
ensemble, 5
canonical, 27
variety of for random walks, 27
entropy, 78
ϵ expansion, 296–300
equation of state, 240
ergodic hypothesis, 206
error function, 72
excluded volume, 188
exponent, η, 196
exponent, ν, 196
327

328
Index
Feynman diagrams, 179
Fick’s law, 83–6
ﬁrst passage, 64
ﬁxed point, 277–84, 297
Flory, 188, 191–2
ﬂow diagram, 297
ﬂuctuating ﬁeld, 239
ﬂuctuation response relation, 249
ﬂux, 78, 84, 88–90
four-point correlation function, 310
Fourier modes, 241
Fourier transform, 42, 74, 96, 97, 99, 101, 105, 108,
110, 111, 114, 122–4
fourth order coupling constant, 305
renormalized, 302
fractals, 255–61
fractional dimensionality, 259
functional integral, 170, 172
Gauss’s law, 90, 91
Gaussian, 10
Gaussian integral, 41, 227–31
Gaussian limit for random walk statistics, 34
Gaussian model, 263–4
Gaussian probability distribution, 15, 38
generating function, 7, 15, 25–41
analytical structure of, 18
Fourier transform of, 31
scaling form of, 199, 196
Gibbs free energy, 208
Ginzburg–Landau–Wilson effective Hamiltonian, 215,
285
grand partition function, 27
Hausdorff dimensionality, 259
Heaviside function, 107
Heisenberg ferromagnet, 205
hyperscaling law, 211, 215
image source, 87, 91
image walker, 70
inverse operator, 111
irreducible diagram, 182
Ising model, 206, 286
isothermal susceptibility, 209, 214, 261, 269
iteration, 296
Koch curve, 257–8
Kronecker delta function, 226
Laplace transform, 105–8, 110, 111, 114, 118,
120–3
lattice
body-centered cubic, 44, 62
Bravais, 43
face-centered cubic, 44
primitive vectors of, 43
random walker on, 28, 55
reciprocal, 43
simple cubic, 29, 32, 62, 71, 82
structure function of, 32
translational symmetry on, 31
Legendre polynomial, 26
linked-cluster expansion, 317–22
Markovian process, 13
mass operator, 179, 184, 185
mean ﬁeld scaling, 261
mean ﬁeld theory, 186–91
moment of inertia tensor, 154–7
momentum-shell renormalization group, 272–7,
322
non-analyticity, 61
non-Gaussian behavior, 194–9
nutrients, absorption by a cell, 90–3
O(n) model, 204–27, 236–46
O(n) renormalization, 300–22
off-lattice walker, 36
long range, 37
one-dimensional Ising model, 269–72
one-dimensional random walk, 1–23
one-loop diagram, 292
ordering ﬁeld, 237
orthogonal matrix, 157
pairing of spin ﬁelds, 291, 318
partition function, 27, 208, 215, 219, 223
passive renormalization group, 264
path integral, 168
random walk as, 168–92
persistence length, 102
persistent random walk, 98–118
one-dimensional, 98–109
three-dimensional, 114–18
two-dimensional, 109–13
perturbation expansion, 176–91
perturbation theory
breakdown of, 184–6
plane wave expansion, 46
Poisson’s equation, 83, 85
polymers, 167, 188, 191
power counting, 314
power-law, 203, 205, 208–11
power series
asymptotic coefﬁcients of, 47
principal radii of gyration, 131–58
probability distribution of, 152
propagator, 307
renormalized, 308
propagator line, 180, 182, 185, 189, 181
q-space, 59
r-renormalization, 301
radius of gyration
mean, 161
radius of gyration tensor, 127, 130–53, 159, 165
random waiting time random walk, 118–24
random walk anisotropy, 128
random walk shape, 127–65
reciprocal lattice, see lattice, reciprocal
recurrence, 51–63

Index
329
recursion relation, 30, 267, 277–84
reduced temperature, 212, 215
renormalization group, 168
resolvent, 142
rotational symmetry, 128, 206, 208
Rushbrooke scaling law, 211
scale invariance, 255–7
scaling laws, 211–13
scaling transformations, 200–2
self-energy, 179
self-similarity, 305
simple pole, 48
sites
number visited by a walker, 63–7
spatial dimensionality
general role of, 195–7
speciﬁc heat, 212
spherical absorber, 91
spherical Brillouin zone, 288
inner portion, 289, 293
outer shell, 289
spherical harmonics, 80
spin–spin correlation function, 237
spin wave theory, 239–46
spontaneous magnetization, 205
steady state, 69, 83–91
steepest descents, 18, 20–3, 48, 60
structure function, 16
structure function, 32
susceptibility, 237, 249
Telegrapher’s equation, 105
translational symmetry, 31
two-point correlation function, 306
u-renormalization, 302
ultraviolet divergence, 171
universality, 193–5
unrestricted random walk, 51
upper critical dimensionality, 154
Ursell–Mayer expansion, 179
vertex, 180
watermelon diagram, 308
Wick’s theorem, 229–31
Zeeman energy, 208


1 
Neurons as Classifiers and Supervised Learning 
Image Source: Wikimedia Commons 
2 
The Classification Problem 
 
How do we build a classifier to distinguish 
between faces and other objects? 
Image Source: Wikimedia Commons 

3 
The Classification Problem 
   denotes +1 (faces) 
 denotes -1 (other) 
Faces 
Other objects 
Idea: Find a separating hyperplane (line in this case) 
Can neurons do that? 
4 
The Idealized Neuron 
Output Spike 
Images by Eric Chudler, UW 
Output 
Inputs 
(axons 
from 
other 
neurons) 

5 
The “Perceptron” 
Inputs ui 
(+1 or -1) 
Output v 
(+1 or -1) 
Weighted Sum  > Threshold 
(x) = +1 if x > 0 and -1 if x ≤ 0 
)
(





i
i
iu
w
v
w1 
w2 
w3 
 
? 
[Introduced by Rosenblatt (1958) building on McCulloch and Pitts (1943)] 
6 
u1 
u2 
What does a Perceptron do? 
F Weighted sum defines a 
hyperplane (line, plane, …) 
 
 
F All inputs on one side of 
hyperplane have output = +1 
(“class 1”); all inputs on other 
side have output = -1 (“class 2”) 
F Perceptrons can classify! 
Can perform linear classification 
denotes +1 output  
Denotes -1 output 
0




i
i
iu
w
How do we learn the weights and threshold? 
u1 
u2 



i
i
iu
w



i
i
iu
w

7 
Perceptron Learning Rule 
Given input u, output                            , and desired output vd: 
Adjust wi and  according to output error (vd – v):  
 
 
 
)
(
)
(
v
v
u
v
v
w
d
i
d
i










For positive input (ui = +1): 
Increases weight if error is positive 
Decreases weight if error is negative  
(opposite for ui = -1) 
 
Decreases threshold if error is positive 
Increases threshold if error is negative  
 
)
(





i
i
iu
w
v
8 
Can Perceptrons learn any function? 
1 
-1 
1 
-1 
u1 
u2 
-1 
-1 
-1 
1 
-1 
+1 
-1 
1 
+1 
1 
1 
-1 
u1 
u2 XOR 
Perceptrons can only classify linearly separable data 
How do we handle linear inseparability? 
? 
 +1 output 
  -1 output 

9 
Multilayer Perceptrons 
F Can classify linearly inseparable data 
Can solve XOR 
F An example of a two-layer perceptron that computes XOR 
 
 
 
 
1 
2 
 = 1 
 = -1 
1 
-1 
-1 
u1 
u2 
(Inputs and outputs are +1 or -1) 
10 
What if you want continuous outputs rather 
than +1/-1 outputs (i.e., regression)? 
E.g., Teaching a network to drive a truck 
Image Source: Wikimedia Commons 

11 
Input nodes 
a
e
a
g



1
1
)
(
a 
(a) 
1 
 
 
 
Sigmoid output function: 
Parameter  controls 
the slope 
g(a) 
)
(
)
(
i
i
i
T
u
w
g
g
v



u
w
u = (u1     u2     u3)T 
w 
Output 
Continuous Outputs with Sigmoid Networks 
12 
Learning Multilayer Sigmoid Networks 
Learn weights that minimize 
output error:  
Input u = (u1  u2 … uK)T  
Output v = (v1  v2 … vJ)T 
))
(
(
k
k
jk
j
ij
i
u
w
g
W
g
v



Desired output d also given 
2)
(
2
1
)
,
(
i
i
i
v
d
E


w
W
(see Supplementary Materials for details) 
j
j
j
ij
i
i
ij
ij
x
x
W
g
v
d
dW
dE
W
)
(
)
(










Use gradient descent! 
   
jk
jk
dw
dE
w




Backpropagation 
learning rule 
Delta rule 

13 
Example: Backing up a Truck  (courtesy of Keith Grochow) 
Teaching a Network to back a truck into a loading dock 
•
Input: x, y, θ of truck 
•
Output: Steering angle 
 
14 
Next: Predicting Rewards and 
Reinforcement Learning 
Image Source: Wikimedia Commons 


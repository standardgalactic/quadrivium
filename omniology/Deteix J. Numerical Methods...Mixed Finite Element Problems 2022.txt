lecture Notes in Mathematic 2318
Jean Deteix 
Thierno Diop 
Michel Fortin
Numerical 
Methods 
for Mixed Finite 
Element Problems
Applications to Incompressible 
Materials and Contact Problems
Springer

Lecture Notes in Mathematics
Volume 2318
Editors-in-Chief
Jean-Michel Morel, CMLA, ENS, Cachan, France
Bernard Teissier, IMJ-PRG, Paris, France
Series Editors
Karin Baur, University of Leeds, Leeds, UK
Michel Brion, UGA, Grenoble, France
Annette Huber, Albert Ludwig University, Freiburg, Germany
Davar Khoshnevisan, The University of Utah, Salt Lake City, UT, USA
Ioannis Kontoyiannis, University of Cambridge, Cambridge, UK
Angela Kunoth, University of Cologne, Cologne, Germany
Ariane Mezard, IMJ-PRG, Paris, France
Mark Podolskij, University of Luxembourg, Esch-sur-Alzette, Luxembourg
Mark Policott, Mathematics Institute, University of Warwick, Coventry, UK
Sylvia Serfaty, NYU Courant, New York, NY, USA
Laszlo Szekelyhidi ©, Institute of Mathematics, Leipzig University, Leipzig, 
Germany
Gabriele Vezzosi, UniFI, Florence, Italy
Anna Wienhard, Ruprecht Karl University, Heidelberg, Germany

This series reports on new developments in all areas of mathematics and their 
applications - quickly, informally and at a high level. Mathematical texts analysing 
new developments in modelling and numerical simulation are welcome. The type of 
material considered for publication includes:
1. Research monographs
2. Lectures on a new field or presentations of a new angle in a classical field
3. Summer schools and intensive courses on topics of current research.
Texts which are out of print but still in demand may also be considered if they fall 
within these categories. The timeliness of a manuscript is sometimes more important 
than its form, which may be preliminary or tentative.
Titles from this series are indexed by Scopus, Web of Science, Mathematical 
Reviews, and zbMATH.

Jean Deteix • Thierno Diop • Michel Fortin
Numerical Methods 
for Mixed Finite Element 
Problems
Applications to Incompressible Materials 
and Contact Problems
Springer

Jean Deteix
GIREF, Departement de Mathematiques et 
de Statistique
Universite Laval
Quebec, QC, Canada
Thierno Diop
GIREF, Departement de Mathematiques et 
de Statistique
Universite Laval
Quebec, QC, Canada
Michel Fortin
GIREF, Departement de Mathematiques et 
de Statistique
Universite Laval
Quebec, QC, Canada
This work was supported by Agence Nationale de la Recherche (ANR-18-IDEX-0001).
ISSN 0075-8434 
ISSN 1617-9692 (electronic)
Lecture Notes in Mathematics
ISBN 978-3-031-12615-4 
ISBN 978-3-031-12616-1 (eBook)
https://doi.org/10.1007/978-3-031-12616-1
Mathematics Subject Classification: 74S05, 65N22, 65F10, 65F08, 74B20, 74M15
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland 
AG 2022
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Contents
1 Introduction .............................................................................................. 
1
2 Mixed Problems ........................................................................................ 
3
2.1 
Some Reminders About Mixed Problems ........................................  
3
2.1.1 The Saddle Point Formulation .............................................. 
4
2.1.2 Existence of a Solution ........................................................  
4
2.1.3 Dual Problem ........................................................................ 
5
2.1.4 A More General Case: A Regular Perturbation   
6
2.1.5 The Case b(v, q) = (Bv, q)Q   
6
2.2 The Discrete Problem ....................................................................... 
7
2.2.1 Error Estimates .....................................................................  
8
2.2.2 The Matricial Form of the Discrete Problem ....................... 
9
2.2.3 The Discrete Dual Problem: The Schur Complement .......... 
11
2.3 Augmented Lagrangian .................................................................... 
13
2.3.1 Augmented or Regularised Lagrangians ..............................  
13
2.3.2 Discrete Augmented Lagrangian in Matrix Form ................ 
15
2.3.3 Augmented Lagrangian and the Condition Number 
of the Dual Problem .................................................. 
15
2.3.4 Augmented Lagrangian: An Iterated Penalty ....................... 
17
3 Iterative Solvers for Mixed Problems.....................................................  
19
3.1 
Classical Iterative Methods .............................................................. 
19
3.1.1 Some General Points ............................................................  
20
3.1.2 The Preconditioned Conjugate Gradient Method .................  
22
3.1.3 Constrained Problems: Projected Gradient and 
Variants   
24
3.1.4 Hierarchical Basis and Multigrid Preconditioning   
26
3.1.5 Conjugate Residuals, Minres, Gmres and the 
Generalised Conjugate Residual Algorithm ............. 
27
3.2 Preconditioners for the Mixed Problem .......................................... 
32
3.2.1 Factorisation of the System ..................................................  
32
v

vi
Contents
3.2.2 Approximate Solvers for the Schur Complement 
and the Uzawa Algorithm ........................................ 
35
3.2.3 The General Preconditioned Algorithm ................................ 
38
3.2.4 Augmented Lagrangian as a Perturbed Problem .................  
41
4 Numerical Results: Cases Where Q = Q'................................................ 43
4.1 
Mixed Laplacian Problem ................................................................. 
43
4.1.1 Formulation of the Problem ................................................. 
43
4.1.2 Discrete Problem and Classic Numerical Methods ..............  
45
4.1.3 A Numerical Example .......................................................... 
47
4.2 Application to Incompressible Elasticity .......................................... 
48
4.2.1 Nearly Incompressible Linear Elasticity ..............................  
49
4.2.2 Neo-Hookean and Mooney-Rivlin Materials ....................... 
52
4.2.3 Numerical Results for the Linear Elasticity Problem .......... 
55
4.2.4 The Mixed-GMP-GCR Method ...........................................  
56
4.2.5 The Test Case ....................................................................... 
58
4.2.6 Large Deformation Problems   
64
4.3 Navier-Stokes Equations   
68
4.3.1 A Direct Iteration: Regularising the Problem ....................... 
71
4.3.2 A Toy Problem .....................................................................  
72
5 Contact Problems: A Case Where Q = Q'.............................................. 75
5.1 
Imposing Dirichlet’s Condition Through a Multiplier ...................... 
75
5.1.1 H002(Гс) and Its Dual H-1 /2(Гс) ...................................... 77
5.1.2 A Steklov-Poincare operator ................................................ 78
5.1.3 Discrete Problems ................................................................. 79
5.1.4 A Discrete Steklov-Poincare Operator ................................. 80
5.1.5 Computational Issues, Approximate Scalar Product ............ 81
5.1.6 The L2(Гс) Formulation ..................................................... 84
5.1.7 A Toy Model for the Contact Problem ................................. 84
5.2 Sliding Contact ................................................................................. 87
5.2.1 The Discrete Contact Problem .............................................. 89
5.2.2 The Algorithm for Sliding Contact ....................................... 92
5.2.3 A Numerical Example of Contact Problem .......................... 93
6 Solving Problems with More Than One Constraint ................................ 97
6.1 
A Model Problem ............................................................................ 97
6.2 Interlaced Method ............................................................................ 98
6.3 
Preconditioners Based on Factorisation ............................................. 100
6.3.1 The Sequential Method .......................................................... 101
6.4 An Alternating Procedure ................................................................... 102
7 Conclusion................................................................................................... 105
Bibliography ..................................................................................................... 107
Index .................................................................................................................. 113

Chapter 1
Introduction
Mixed Finite Element Methods are often discarded because they lead to indefinite 
systems which are more difficult to solve than the nice positive definite problems of 
standard methods. Indeed, solving indefinite systems is a challenge : direct methods 
[4, 14, 78] might need renumbering (see [28]) of the equations and standard iterative 
methods [22, 81, 86] are likely to stagnate or to diverge as proved in [86]. As an 
example, consider the classical conjugate gradient method. Applied to a symmetric 
indefinite problem it will generate a diverging sequence. As the conjugate gradient 
method is (in exact arithmetic) a direct method, it will yield the exact solution if 
the problem is small enough to avoid loosing orthogonality. Applying a minimum 
residual method to the same problem will in most cases yield stagnation.
These two classical methods are the simplest in a list which grows constantly. 
This monograph does not intend to introduce new iteration methods. We shall rely 
mostly on existing packages, mostly Petsc from Argonne Laboratory [11].
Our concern is to solve algebraic systems associated to mixed discretisation. 
Several approaches (see, for example, [8, 15, 43]) exist in the literature to solve 
this type of problem but convergence is not always guaranteed. They are indefinite 
systems but also structured systems associated with matrices of the form,
(A B) 
(11
where A is often a positive definite matrix.
The key to obtain convergence is preconditioning. For general problem, a 
vast number of preconditioners are available [15]. Our goal is to develop good 
preconditioners for problems arising from (1.1).
We want to show in the present work that efficient iterative methods can be 
developed for this class of problems and that they make possible the solution of 
large problems with both accuracy and efficiency. We shall also insist on the fact that 
these methods should be entirely automatic and free of user dependent parameters.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
1
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_1

2
1 Introduction
We also want to make clear that our numerical results should be taken as 
examples and that we do not claim that they are optimal. Our hope is that they 
could be a starting point for further research.
Here is therefore our plan.
• Chapter
rapidly recalls the classical theory of mixed problems, including 
Augmented Lagrangian methods and their matricial form.
 1 
• Chapter 2 presents some classical iterative methods and describes the precondi­
tioner which will be central to our development. We come back to augmented 
Lagrangian and a mixed form for penalty methods.
• Chapter 3 is devoted to numerical examples. The first one will be the approxima­
tion of a Dirichlet problem with Raviart-Thomas elements [20]. This is a simple 
case which will however permit to consider the fundamental issues. We shall see 
how an Augmented Lagrangian method enables us to circumvent the fact that we 
do not have coercivity on the whole space.
We shall thereafter consider incompressible elasticity in solid mechanics, first 
in the linear case and then for a non linear Mooney-Rivlin model.
In all those problems, the space of multipliers is L2 (Q) and can therefore be 
identified with its dual. We also present some ideas for the solution of the Navier- 
Stokes equations. In those problems, with the discrete spaces that we employ, we 
shall not be able to use a real augmented Lagrangian. However, we shall consider 
a regularised formulation which will accelerate the convergence of our iterations.
• Chapter 4: We consider contact problems. In this case, the space of multipliers 
is not identified to its dual. We shall present some ideas for which we do not 
have numerical results but which we think could be research avenues for the 
future. In particular, we present ideas about discrete Steklov-Poincare operators. 
Numerical results will be presented in the more classical formulation where the 
duality product is approximated by the L2 scalar product.
• Chapter 5: Finally, we shall consider a case, arising from contact mechanics 
between incompressible bodies, in which we have two different types of con­
straints. This will lead us to modify accordingly our preconditioners.

Chapter 2
Mixed Problems
2.1 Some Reminders About Mixed Problems
Basically, mixed problems arise from the simple problem of minimising a quadratic 
functional under linear constraints. Let then V a function space and a bilinear form 
defining an operator A from V into V'.
a(u, v) = {Au, v)v/xv Vu, v e V
This bilinear form should be continuous, that is
a(u,v) <||a || уu || v || v|| V 
(2.1)
where || • ||v is the norm of V and ||a|| is the norm of a(•, •). In the same way, 
consider another function space Q, a bilinear form on V x Q defining an operator 
B from V into Q',
b(v,q) = {Bv,q) V v e V, Vq e Q. 
(2.2)
We also suppose this bilinear form to be continuous and thus
b(v.q) <||b II || v || V ||q || Q 
(2.3)
where ||b|| is the norm of b(•, •). Defining the functional
F(v) := 1 a(v,v) -(f,v) 
(2.4)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
3
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_2

4
2 Mixed Problems
we then want to solve the constrained problem,
inf F(v). 
(2.5)
Bv=g
2.1.1 The Saddle Point Formulation
Problem (2.9) can be classically transformed into the saddle-point problem,
1
inf sup -a(v, v) - b(v, q) - (f, v) + (g, q), 
(2.6)
v ё V q ё Q 2
for which the optimality system is
a(u, v) + b(v, p) = {f, v} 
Vv ё V
(2.7)
b(u, q) = (g, q)Q 
Vq ё Q,
2.1.2 Existence ofa Solution
For problem (2.7) to have a solution, it is clearly necessary that there exists some ug 
satisfying Bug = g. Moreover, the lifting from g to ug should be continuous. This 
is classically equivalent [20] to the inf-sup condition
b(u, q)
inf sup-------------- > k0
qё Q vё V IIu IIV II q IIQ
(2.8)
a condition omnipresent in the following. Solving (2.5) is then equivalent to finding 
u0 ё KerB such that,
a(u0, v0) = (f, v0) - a(ug, v0) Vv0 ё Ker B.
(2.9)
We shall refer to (2.9) as the primal problem. We therefore suppose, as in [20] that 
the bilinear form a(•, •) is coercive on Ker B, that is there exists a constant a0 such 
that,
a(v0, v0) > ao||v0||V Vv0 ё Ker B
(2.10)
Remark 2.1 (Coercivity) Unless there is a simple way to build a basis of Ker B 
or a simple projection operator on Ker B coercivity on Ker B is not suitable for 
numerical computations. In our algorithms, we shall need, in most cases, coercivity

2.1 Some Reminders About Mixed Problems
5
on the whole of V .
a(v, v) > a||v||V Vv e V.
(2.11)
It is always possible to get this by changing a(u, v) (see [43, 46, 79]) into
a(u, v) = a(u, v) + a(Bu, Bv)q'.
Such a change will have different consequences in the development of our algo­
rithms and will lead to augmented and regularised Lagrangians which will be 
considered in detail in Sect. 2.3 
■
Remark 2.2 It should also be noted that (2.11) and (2.1) imply that a(v, v) is a norm 
on v, equivalent to the standard norm. 
■
2.1.3 Dual Problem
Problem (2.6) has the general form,
inf sup L(v, q),
veV qeQ
where L(v, q) is a convex-concave functional on V x Q. If one first eliminates q by 
computing
J(v) = sup L(v, q), 
qeQ
one falls back on the original problem, the primal problem. Reversing the order of 
operations, (this cannot always be done, but no problems arise in the examples we 
present) and eliminating v from L(v, q) by defining
D(q) := inf L(v, q) 
veV
leads to the dual problem
sup D(q).
qeQ
The discrete form of the dual problem and the associated Schur’s complement will 
have an important role in the algorithms which we shall introduce.

6
2 Mixed Problems
2.1.4 A More General Case: A Regular Perturbation
We will also have to consider a more general form of problem (2.7). Let us suppose 
that we have a bilinear form c(p, q) on Q. We also suppose that it is coercive on Q, 
that is
c(q, q) > y |qQ
we thus consider the problem
a(u, v) + b(v, p) = {f, v} 
Vv e V
(2.12)
b(u, q) - c(p, q) = (g, q)Q 
Vq e Q,
It is now elementary to show [20] that we have existence of a solution and
a ||u IIV + YIIP llQ < 11I f IIV, + 11Ig llQ/ 
(2.13)
In practice we shall consider the perturbed problem defined with
c(p, q) = ''/>, q)Q.
The bound (2.13) explodes for y small which can be a problem. If we still have 
the inf-sup condition and the coercivity on the kernel then we can have a bound on 
the solution independent of e . We refer to [20] for a proof and the analysis of some 
more general cases.
Remark 2.3 In this case, the perturbed problem can be seen as a penalty form for the 
unperturbed problem. We shall come back to this in Sect. 2.3.4. It is then easy to see 
that we have an O(e) bound for the difference between the solution of the penalised 
problem and the standard one. The bound depends on the coercivity constant and 
the inf-sup constant. 
■
2.1.5 The Case b(v, q) = (Bv, q)Q
The above presentation is abstract and general. We now consider a special case 
which will be central to the examples that we shall present later. Indeed, in many 
cases, it will more suitable to define the problem through an operator B from V into 
Q. We suppose that on Q, we have a scalar product defining a Ritz operator R from 
Q into Q'
(p,q)Q = (Rp,q)q'xq = (Rp, Rq)QQ

2.2 The Discrete Problem
7
We then define the bilinear form
b(v, q) = (Bv, q)Q = (Bv, q)q>xq
referring to (2.2) we then have,
B = RB 
(2.14)
and
(Bu, q)Q'*q = (Bu, q)Q = (Bu, Rq)qxq' 
(2.15)
The constraint that we want to impose is then Bu = g with g e Q or equivalently 
Bu = g' = Rg. In the case Q = Q' and R = I, the operators B and B coincide. 
This will not be the case for the problems which we shall consider in Chap. 5. But 
even in the case Q = Q', in the discrete formulation the distinction between B and 
B will have to be taken into account.
2.2 The Discrete Problem
Our interest will be in solving discretisations of Problem (2.7). We thus suppose 
that we have subspaces Vh с V and Qh c Q. We shall work in the framework of 
Sect. 2.1.5 and we thus have,
b(vh, qh) = (Bvh, qh)Q = (PQh Bvh, qh)Q 
(2.16)
and we can define
Bh = PQhB
The scalar product (•, • )q defines an operator Rh from Qh onto Qh and we can 
introduce
Bh = RhBh
We want to solve
a(uh, vh) + b(vh, ph) = (f, vh) Vvh e Vh
(2.17) 
b(uh, qh) = (gh, qh)Q, 
Vqh e Q
We denote gh the projection of g onto Qh. The second equation of (2.17) can be 
read as Bhuh = gh or equivalently Bhu = Rhgh. Unless Buh e Qh, this condition 

8
2 Mixed Problems
is weaker than Buh = g which for almost all cases will lead to bad convergence or 
even ‘locking ’, that is a null solution.
For example, we shall meet later the divergence operator div (which acts on a 
space of vector valued functions which we shall then denote by u). When we take 
for Qh a space of piecewise constant functions, divh uh is a local average of div uh.
2.2.1 Error Estimates
We recall here, in its simplest form the theory developed in [20]. This will be 
sufficient for our needs. In the proof of existence of Sect. 2.1.2, we relied on two 
conditions: the coercivity on the kernel (2.10) and the inf-sup condition (2.8). We 
introduce their discrete counterpart. We thus suppose
3 a0 > 0 such that a(vh,vh) > ah ||vh ||v 
V vh e Ker Bh, 
(2.18)
0 
0 
0 
0 0V 
0
or by the even simpler global ellipticity
3 ah > 0 such that a(vh, vh) > ah ||vh ||V 
V vh e Vh.
We also write the discrete inf-sup condition,
b(vh, qh)
3 eh > 0 such that sup 
> eh IIqh IIQ. 
(2.19)
vh e Vh II vh IIV
It will also be convenient, to define the approximation errors
Eu := inf IIu - vh || V 
vheVh
Ep := inf ||p - qh || q 
qheQh
We recall that approximation errors depend on the regularity of the solution. We 
refer to the classical work of [30] for an analysis of this dependence. Let ||a || and 
||b|| be the norms of a(•, •) and of b(•, •) as defined in (2.1) and (2.3). The following 
result is proved in [20].
Proposition 2.1 (The Basic Error Estimate) Assume that Vh and Qh verify (2.18) 
and (2.19). Let f e V' and g e Q'. Assume that the continuous problem (2.17) has 
a solution (u, p) and let (uh, ph) be the unique solution of the discretised problem 
(2.17). If a( •, •) is symmetric and positive semi-definite we have the estimates
IIuh — u || v <
21|a || 
21|a ||1 /2||b || 
||b ||
ah + (a.) 1 /2eh 
Eu + ah Ep

2.2 The Discrete Problem
9
II Ph - P || Q <
2b a 113 /2 + II a Illi b II E , 3|| a ||1 / 2|| b || E 
(Oh\ 1 / 2 в 
в2 
u u (ah) 1 / 2 Bh 
p
\\0 0) Ph 
Ph 
/ 
(a 0 ) Ph
Remark 2.4 (Dependance on h) In the previous result, we allowed, in principle, the 
constants eh and a h (or a 0) to depend on h .It is obvious (but still worth mentioning) 
that if there exist constants в0 and a0 such that ph > в0 and ah > a0 (or ah > 
a0) for all h, then the constants appearing in our estimates will be independent 
of h. Considering constants depending on h will be useful for contact problems 
(Sect. 5.2). 
■
We can also see from this estimate that the approximation of P is more sensitive 
on the value of ph than the approximation of u. One also sees that, paradoxically, 
improving Eu is better to counter this than improving EP.
2.2.2 The Matricial Form of the Discrete Problem
To make explicit the numerical problem associated with (2.17) we need to introduce 
basis vectors ф1н for Vh and -ф-j for Qh and the vectors of coefficients u, q which 
define uh and qh with respect to these bases.
qh = 22 q^j-
12 u^
uh
(2.20)
Remark 2.5 (Notation) To avoid adding cumbersome notation, unless a real ambi­
guity would arise, we shall denote in the following by u and P either the unknowns 
of the continuous problem or the unknowns of the numerical problems arising from 
their discretisation.
We shall also denote by the same symbol the operators A and B of the continuous 
problem and the matrices associated to the discrete problems. As they are used in 
very different contexts, this should not induce confusion. 
■
Denoting {■, ■) the scalar product in Rn, we can now define the matrices associated 
with the bases (2.20)
(A u, v) = a(uh, vh).
We also have a matrix R associated with the scalar product in Qh which represents 
the discrete Ritz operator
(Rp, q) = (ph qh)Qh

10
2 Mixed Problems
The scalar product in Qh will be associated with R 1.
<R-1 p,q) = (ph, qh)Qh.
We then have
(B u, q) = b(uh, qh) = (Bhuh, qh)Qh
and the operator Bhuh can be asociated with the matrix
B = R-1Bu
(2.21)
We summarise this in the following diagram.
R
Remark2.6 (Qh = Qh) It is important to note that even in the case where Q = 
Q'and R = I, this is not the case in the discrete problem. The matrix R defined 
above is not the identity matrix. 
■
Remark 2.7 (Change of Metric on Qh) As we shall see in Sect. 2.2.3, it will 
sometimes be useful in numerical applications to change the scalar product on Qh. 
To distinguish this case we shall denote MQ the matrix defining the scalar product
(Ph,qh)Qh = <MQ p,q).
The choice MQ = R is frequent but not optimal in many cases [48]. In Sect. 2.2.3 
we shall discuss the choice MQ = MS where MS is some approximation of the 
discrete Schur complement. 
■
We now consider the finite dimensional matricial problems associated to our mixed 
formulation, in fact the actual problem for which we want to build efficient solvers.

2.2 The Discrete Problem
11
We first write problem (2.17) in matrix form,
( AB }( u 1 = ( f 1 
(2.22)
B0 
p g
Although this block matrix is a non singular matrix the numerical solution of 
(2.22) is not so simple. The main problem being that this matrix is indefinite. If one 
wanted to employ a direct solver, we might have to introduce a renumbering of the 
equations [28]. Moreover, our examples will come from the application of the mixed 
finite element methods [20]. Indeed, we shall focus on methods suitable for large 
systems arising from the discretisation of three-dimensional problems in mechanics 
which lead to large problems of the form (2.22). Such large systems are not suitable 
for direct solvers and require iterative methods. However, without preconditionning 
the system (acting on its eigenvalues), iterative methods, such as Krylov methods, 
are likely to diverge or stagnate on indefinite problems (see [86]). Therefore the key 
to obtain convergence will be the construction of a good preconditioner.
2.2.3 The Discrete Dual Problem: The Schur Complement
If we eliminate u from (2.22) we obtain the discrete form of the dual problem,
- BA-1Btp = g - BA-1f. 
(2.23)
The matrix S = BA-1Bt is often called the Schur complement. The system (2.23) 
is equivalent to the maximisation problem
sup 1 {(-A-1 Bq, Bq) + (g - BA-1 f, q)} 
(2.24)
q2
When building numerical methods, a crucial point will be the condition number 
of S. Indeed the condition number is an important measure for the behaviour 
(convergence) of iterative methods. Following Remark 2.7, we have on Qh a metric 
defined by the matrix MQ . The standard choice would be to take MQ = R where 
R is associated to the scalar product induced on Qh by the scalar product of Q.In 
some cases, it will be convenient to change this scalar product, using a well chosen 
matrix MS instead of R.
Remark 2.8 We write MS to emphasise that this matrix should be chosen to 
approximate the Schur complement. 
■
We can change (2.23) into
MS-1BA-1Btp = MS-1(g - BA-1f). 
(2.25)

12
2 Mixed Problems
In Sect. 3.1, this will correspond to a preconditioning. The idea is that MS should 
be an approximation of BA-1 Bt improving the condition number of the resulting 
system.
To quantify this, we consider the eigenproblem
BA-1 B фр = Шф
(2.26)
Then the condition number for (2.25) can be defined as
Лтах
Kh = 1----
лт1п
We have written Kh to emphasise that we have a discretised problem. A nice 
property would be to have Kh independent of h. This would be the case if Xmax < C 
and Лт1п > c with C and c independent of h. We then say that MS and S are 
spectrally equivalent [9]. The ideal case would be MS = S. Finding a good MS 
means finding an easily computable approximation of S-1.
Remark 2.9 (MS Depends on A) We shall come back to the choice of MS in 
Sect. 4.2 in particular for linear elasticity (or the Stokes problem). It is worth already 
remarking that MS should be an approximation of BA-Bt and therefore depends 
on A. In the simplest case, if A is changed into cA, one should change MS into 
1 MS. 
■
Although the question of spectral equivalence depends on the problem at hand, we 
have a general case where it holds. We first recall that the minimum and maximum 
eigenvalues in (2.26) are the minimum and maximum values of the Rayleigh 
quotient,
RQ(q) = (A-1 B‘q, Bq)
<MS P, P P
Proposition 2.2 We suppose that the matrix A defines a norm on Vh, equivalent to 
the standard norm.
a ||vh ||2 < (Av, v) < || A || || vh ||2.
We also suppose that we have the inf-sup condition (2.19). Then taking MS = R, 
we have in (2.26)
л. > eL K < IBf 
mmin > .. , .. , mo^ax <
|| A || 
a
(2.27)

2.3 Augmented Lagrangian
13
We recall that the dual norm associated to the norm defined by A is defined by A-1 . 
We can therefore write the Rayleigh quotient, as
{A-1 Bq, Bq)_ 
(v,B}2
(Rp,p) 
SVP (Av,vXRq,q)
The inf-sup condition yields the lower bound and the upper bound is direct.
This clearly also holds for another choice of MS if we have
c о (Rp, q )<{ Msp,q )< c i( Rp,q) 
(2.28)
The bounds of (2.27) show that we have a condition number independent of the 
mesh size if eh > в о > 0 . This is an important property if we want to solve large- 
scale problems. However, if we consider MS as a preconditioner, we shall also want 
to build it as a good approximation of S.
2.3 Augmented Lagrangian
Augmented Lagrangian is a popular method for the solution of mixed problems. We 
shall present it in some details, trying to show its potential but also its shortcomings.
2.3.1 Augmented or Regularised Lagrangians
Ifwe employ the regularised bilinear form a(u, v) defined in (2.11) we can define 
an Augmented Lagrangian method.
a(u, v) + a(Bu, Bv)q' + b(v, p) = {f, v) + a(g', Bv)q' Vv e V
(2.29)
b(u,q) = {g',q}q'xq, 
Vq e Q.
The extra term can be written differently using (2.15). We would then have,
a(u, v) + a(Bu, Bv)q + b(v, p) = {f, v) + a(g, Bv)q Vv e V
b(u, q) = (g, q)Q 
Vq e Q 
with g = R-1 g'. This does not change the solution of the problem. In the discrete 
problems, using an augmented Lagrangian is a classical way of accelerating some 
algorithms and cancelling errors associated with penalty methods. We shall have 
two different ways of implementing this idea.

14
2 Mixed Problems
The direct way to discretise (2.29) would be,
a(uh, vh) + a(Buh, Bvh)Q + b(vh, ph)
= {f, vh) + a(g, Bvh)Q 
Vvh e Vh 
(2.30)
b(uh, qh) = (g, qh)Q,
Vqh e Qh.
This corresponds, for a large, to a penalty method imposing exactly Buh = g, 
in general a bad idea for many discrete problems. This will however be possible 
in some cases, such as the example of Sect. 4.1. In general, for small values of a, 
this formulation will be useful to regularise and improve some numerical solution 
algorithms. This will be the case in Sect. 4.2. We shall call this a regularised 
formulation.
To define a real discrete augmented Lagrangian, we suppose that Qh has a scalar 
product defined by a metric MQ. As in (2.21) we take Bh = PQhB which depends 
on MQ. A real discrete Augmented Lagrangian would then be written as
a(uh, vh) + a(Bhuh, Bhvh)Qh + b(vh, ph)
= <f, vh} + a(gh, BhVh)Qh 
Vvh e Vh
b(uh, qh) = (g, qh)Qh , 
Vqh e Qh,
(2.31)
which we can also write as
a(uh, vh) + a(BhUh, Bhvh)Qh + b(vh, Ph)
= (f, vh } + a(gh, Bhvh)Qh x Q h 
V vh e Vh
b(uh, qh) = (g, qh)Qh , 
Vqh e Qh.
(2.32)
This formulation depends on the choice of the metric MQ on Qh. The simplest 
case being when this scalar product is the scalar product in Q, that is MQ = R. 
We could also consider a scalar product induced by a matrix MS which we suppose 
equivalent to the usual scalar product. As in Sect. 2.2.3 MS would be associated to 
a good approximation of the Schur complement.
The solution is unchanged by the extra terms and everything looks perfect. The 
question will rather be the actual implementation which will depend on the choice 
of finite element spaces and also on the choice of the scalar product on Qh .

2.3 Augmented Lagrangian
15
2.3.2 Discrete Augmented Lagrangian in Matrix Form
We now consider the matricial form of the augmented Lagrangian formulations. In 
the regularised form (2.30) we shall denote
<Cu,v) = (Buh, Bvh)Q
• For the regularised formulation (
) we have
2.30
A + aC
B
Bt u = f + aBtg 
0p 
g
(2.33)
• From (
) we have
2.32
A + aBtMS-1B Bt
B0
pu =
f + aBtMS-1g
(2.34)
Remark 2.10 (The Choice of MS) The standard implementation is evidently to take 
MS = R. If this is not the case, one should remark that in order to have (2.31) and 
(2.32) to be equivalent, we have to write Bh = MS1 B and not R-1 B as in (2.21). ■
Remark 2.11 (MS-1 a Full Matrix?) Another important point is the presence of 
MS-1 which is in general a full matrix. This makes (2.34) hardly usable unless MS 
is diagonal or block diagonal and leads us to employ the regularised formulation. ■
Remark 2.12 One could also write (2.34) writing the penalty term in mixed form, 
that is
ABt 
Bt
B — sMs 0
B00
uf
I p I = I g I 
pg
The solution is of course p = 0. It is not clear if this form can be of some use. One 
might perhaps consider it in the context of Chap. 6. 
■
2.3.3 Augmented Lagrangian and the Condition Number 
of the Dual Problem
Using an augmented Lagrangian can be seen as a form of preconditioning for the 
dual problem. We have already considered in Sect. 2.2.3 the condition number of 
the dual problem (2.26), where Ms defines a scalar product on Qh.

16
2 Mixed Problems
We had considered the eigenvalue problem,
BA-1 В фр = Шф
(2.35)
The eigenvalues of (2.35) are also the non zero eigenvalues of
A -1 BZM-1 Вфи = кфи
with фи = A-1 В‘фр which we can also write
Афи = 1 BtM-1 Вфи
X
If instead of A we use A + aB‘ M- В this becomes
(A + aBM-1 В)фи = 1BM-1 Вфи.
S u ^ S u
Denoting Xa the corresponding eigenvalues, one easily sees that
X
Xa =---------
a 1+Xa
Denoting XM and Xm the largest and the smallest eigenvalues, the condition number 
of the system is thus
Ka
XM( 1 + a Xm)
Xm(1 + aXM)
which converges to 1 when a increases. One sees that this holds for a large even 
if the initial condition number is bad. One also sees that improving K = XM /Xm 
also improves Ka for a given a. Augmented Lagrangian therefore seems to be the 
perfect solver. However, things are not so simple.
• The first problem is that the condition number of A + a В MS- В worsens when 
a increases. As solving systems with this matrix is central to the algorithms that 
we shall introduce, we loose on our right hand what we gain with the left one. In 
practice, this means finding the correct balance between the conflicting effects.
• The other point is the computability of В MS- В. The matrix M-1 could for 
example be a full matrix. Even if we approximate it by a diagonal matrix, the 
structure of the resulting matrix could be less manageable. This will be the case 
in the examples of Sect. 4.2.
• When the real augmented Lagrangian cannot be employed, the regularised for­
mulation (
) might have a positive effect. However, the solution is perturbed 
and only small values of a will be admissible.
2.33

2.3 Augmented Lagrangian
17
2.3.4 Augmented Lagrangian: An Iterated Penalty
Augmented Lagrangian formulations are often used as an iterative process, using a 
penalty method to correct a previous value of p. Given pn , one solves
(A + aB MS' B)un +1 + B pn = f + aB‘M-1 g 
pn+1 = рП + aM-1 BUn
This could also be written as, writing e = 1 /a
/ A Bt W Su \ = / f - Au - Bp \ = /rn \ 
(2 36)
\B -eMs/ \Sp/ 
\ g - Bu'n 
) 
\rp) 
(.
and
pn+1 =pn+Sp, un+1 =un+Su.
System (2.36) is in fact a penalty method.
Remark 2.13 Problem (2.36) is of the perturbed form discussed in Sect. 2.1.4. Its 
solution will rely on the same iterative procedures as the unperturbed problem which 
we discuss in the next chapter. The question is whether there is some advantage in 
doing so.
• It permits to use an augmented Lagrangian without handling the matrix 
Bt Ms-1 B.
• The price is that it is an iterative process.
We must note that the Schur complement now becomes
S = BA-1 B + (MS.
This makes the condition number of the perturbed problem better than that of the 
standard one which makes one hope of a better convergence for the dual variable.
One also sees that if MS is an approximation of S one should rather use now,
Mes = (1 + ()MS
This means that there will be an optimum value of e = 1 /a. Taking a larger 
г for a better convergence enters in conflict with the paradigm of the augmented 
Lagrangian which would need a large and thus e small. We shall present a numerical 
example in Remark 4.11.
■

18
2 Mixed Problems
Remark 2.14 (Correcting the Regularised Form) One could use a similar idea to 
eliminate the perturbation introduced by formulation (2.33), writing
/ A + aCBt Su f - Aun - B pn \ 
rn 
(2 37)
I B 0 ) Pp) \ g - Bun 
) 
\rnp) 
■
again to price of an iteration. 
■

Chapter 3
Iterative Solvers for Mixed Problems
Check for 
updates
We now come to our main issue, the numerical solution of problems (2.22), (2.33) 
and (2.34) which are indefinite problems, although with a well defined structure.
A= 
BA0Bt 
.
(3.1)
Contact problems will also bring us to consider a more general non symmetric 
system
A= 
BA2 0B1t 
.
(3.2)
We intend to solve large problems and iterative methods will be essential. We shall 
thus first recall some classical iterative methods and discuss their adequacy to the 
problems that we consider. From there we introduce a general procedure to obtain 
preconditioners using a factorisation of matrices (3.1) or (3.2).
3.1 Classical Iterative Methods
Iterative methods is a topic in itself and has been the subject of many books 
and research articles. For the basic notions, one may consult [47, 50, 86] but this 
is clearly not exhaustive. Moreover the field is evolving and new ideas appear 
constantly. Our presentation will then be necessarily sketchy and will be restricted 
to the points directly relevant with our needs.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_3
19

20
3 Iterative Solvers for Mixed Problems
3.1.1 Some General Points
Linear Algebra and Optimisation
When considering iterative method, one may view things from at least two different 
points of view:
• linear algebra methods,
• optimisation methods.
These are of course intersecting and each one can bring useful information.
When dealing with systems associated to matrix (3.1), from the linear algebra 
perspective, we have an indefinite problem and from the optimisation perspective, 
we have a saddle point problem.
• We have positive eigenvalues associated with the problem in u and the matrix A 
is often symmetric positive definite defining a minimisation problem.
• On the other hand, we have negative eigenvalues associated to the problem in p, 
that is the dual problem (2.24) which is a maximisation problem.
This induces a challenge for iterative methods which have to deal with conflicting 
goals.
Norms
The linear systems which we consider arise from the discretisation of partial 
differential equations, and are therefore special. Itis also useful to see if the iteration 
considered would make sense in the infinite dimensional case. Ideally, such an 
iterative method would have convergence properties independent of the mesh size.
Considering system (3.1), we have a very special block structure and variables u 
and p which represent functions uh e Vh and ph e Qh that have norms which are 
not the standard norm of Rn .
• This is an important point: we have a problem in two variables which belong to 
spaces with different norms.
In Remark 2.7 we introduced a matrix MQ associated with a norm in Qh, by the 
same reasoning we associate a matrix MV for a norm in Vh . We thus have matrices 
MV and MQ associated with these norms
<Mvu, v) = (uh, vh)vh, 
(Mqp, q) = (ph, qh)Qh.
This also means that residuals must be read in a space with the dual norms MV-1 
and MQ-1 and this will have an incidence on the construction of iterative methods.

3.1 Classical Iterative Methods
21
Krylov Subspace
One should recall that classical iterative methods are based on the Krylov subspace,
Kr(A, b) = span(b, Ab, A2b,...,Ar-1b),
looking for an approximate solution in this space. This is made possible by building 
an orthogonal basis. If the matrix is symmetric one uses the Lanczos method [60] 
to build orthogonal vectors. The important point is that symmetry allows to store a 
small and fixed number of vectors. This is the case in the conjugate gradient method 
and in the Minres [51, 74, 75, 83] algorithm.
• When A is symmetric positive definite (SPD) it defines a norm and one can look 
for a solution in Kr(A, b) minimising ||x — A-1 b ||A = || Ax — b ||A-1. This is the 
conjugate gradient method.
• When A is not positive definite, it does not define a norm. One then must choose 
a norm M and minimise || Ax — b IM-1. This is the Minres algorithm.
When the matrix is not symmetric, the Arnoldi process [7] can be used to build an 
orthogonal basis. This yields the GMRES algorithm [81] and related methods.
Preconditioning
We want to solve the problem Ku = b and we suppose that we have a preconditioner 
P in the sense that P—1 is an approximation of K—1 (see [71, 88] for more details 
on the preconditioner techniques).
• For K = A we shall consider a class of preconditioner in Sect.
.
 3.2
• For the subproblems in u that is (K = A), we shall rely on standard techniques 
such as those available in the Petsc package of Argonne Laboratory [11]. For 
example, we could employ as a preconditioner an Algebraic Multigrid method, 
a SSOR method or an incomplete factorisation. But this is a choice and using 
another technique is clearly permitted, the only criterion would be efficiency.
We shall also describe a special multigrid method in Sect. 3.1.4. The important 
point is that we want to solve large scale three-dimensional problems and that 
preconditioning will be a key issue.
We refer to [45] for a presentation and comparison of iterative methods and 
references.
Remark 3.1 (Preconditioning and Acceleration) A preconditioner should ideally 
be a converging iterative method in itself. We can then consider that the precondi­
tioned iterative method accelerates the preconditioner as well as the preconditioner 
accelerating the iterative method. In our development, we shall indeed try to 
build preconditioners which are convergent by themselves which may be hopefully 
accelerated by some other classical iteration. 
■

22
3 Iterative Solvers for Mixed Problems
Given a preconditioner P and some iterative method IM we shall denote (P -IM) 
the method IM preconditioned by P.
Before considering the indefinite systems (3.1) or (3.2), we shall consider the 
subproblem associated with the matrix A which in many applications will be 
symmetric and positive definite. Here we are on safe ground.
3.1.2 The Preconditioned Conjugate Gradient Method
We first consider the case where A is a positive definite matrix. Solving Au = 
b is equivalent to minimising a functional of the form (2.4). This is a convex 
functional and to minimise it, the standard procedure is a descent method. A simple 
preconditioned gradient method would be
Ui+1 = Ui + ail’-1 (Aui - b)
The residual r = b - Au is then modified by
r+1 = ri — aAP-1 ri.
One then minimises in a
ii ri+i nA—i
which yields, denoting zi = P-1ri
<P-1 ri, ri) 
(Zi,ri')
a = ------- ;-------------- ;----- = -------------- .
{P-1 ri,AP-1 ri) 
(Zi,Azi)
We must remark that we have two different notions.
• A norm on the residual, here defined by A-1.
• A preconditioner P, that is an approximate solver.
This can be improved by the classical preconditioned conjugate gradient method 
(CG) [52, 81]. To use this we must have a symmetric positive definite P = 
P 1/2 P 1/2. In fact, the method is equivalent to applying the standard conjugate 
gradient method to the system
P -1/2AP -1/2y = P -1/2b. 
(3.3)
which is equivalent to the minimisation problem
inf-(Av, v)P-1 — (b, v)P-1 
(3.4)
v2 

3.1 Classical Iterative Methods
23
with u = P -1/2y. The preconditioning can thus be seen as a change of metric on 
the Euclidean space.
Remark 3.2 (Norm and Preconditioner) If we come back to the idea that the 
residual is measured in the norm defined by A-1 one would now have this replaced 
by P 1/2A-1 P 1/2. With respect to the case where the standard Rn norm would 
be employed, we thus have two changes, one induced by A-1 and one by the 
preconditioner. This is special to the symmetric positive definite case and will not 
be directly transposed to the indefinite case where the two notions will have to be 
considered in two different ways. 
■
Remark 3.3 (Convergence) The principle of a conjugate gradient method is to keep 
the residual at every step orthogonal to all the previous directions in the norm 
defined by A-1. This is done implicitly and does not require extra computation.
It can therefore be seen as a direct method as it will provide the solution of a 
problem in Rn in n steps. This would hold in exact arithmetic. Another nice property 
is superlinear convergence [87]. 
■
To be complete, we present explicitly the preconditioned conjugate gradient method
Algorithm 3.1 P-CG algorithm
1: Initialization
• Let u0 the initial value.
• r0 = b - Au0
• z0 = P -1r0
• p0 = z0
• i=0
2: while criterion > tolerance do
- aiApi
= P -1ri+1
zi+1 • ri+1
r • zi 
a; = ----------
Pi • Api 
ui+1 = ui + aipi 
ri+1 = ri 
zi+1 
вг = 
zi • ri
Pi+1 = zi+1 + PiPi
i =i+1
end while
■

24
3 Iterative Solvers for Mixed Problems
3.1.3 Constrained Problems: Projected Gradient and Variants
When considering contact problems in Chap. 5 we shall have to solve a minimisation 
problem with a positivity constraint. The method that we shall use is a special case 
of a more general method.
Equality Constraints: The Projected Gradient Method
The projected gradient method is a very classical method for constrained optimi­
sation [18]. In the simpler case, let us suppose that the solution of a minimisation 
problem must satisfy a linear constraint,
Bu = g 
(3.5)
If we have an initial value ug satisfying (3.5) and if we have a simple way of 
projecting on Ker B, we can solve the problem,
1
V enfrB 2 (A v0 - Aug, v0^ — f, v0^.
This is in fact what we did to prove existence of the mixed problem. We shall 
meet this procedure in Sect. 6.2. We can then apply the conjugate gradient method 
provided the gradient is projected on Ker B. We shall also consider this method in 
Sect. 4.3.
Inequality Constraints
We now consider a set of inequality constraints,
Bi u < gi 1 < i < m
Let u be the solution, we then split the constraints in two sets.
• Active constraints for which we have Biu = gi.
• Inactive constraints for which Biu < gi.
For more details on this strategy, we refer to [16, 17, 56, 57, 72]. If we know 
which constraints are active, the problem reduces to the case of equality constraints. 
In practice, this will be done iteratively.
• Given an initial guess for the active set, use an iterative method to solve the 
equality problem.
• Monitor the possible changes of active constraints. If there is a change, restart 
the iteration with the new set.

3.1 Classical Iterative Methods
25
The monitoring implies a change of active constraints if one has one of two 
conditions.
• The solution is modified such that an inactive constraint is violated, one then 
projects the solution on the constraint and restarts the iteration, now making this 
constraint active.
• On an active constraint, the iteration creates a descent direction that would bring 
the solution to the unconstrained region. This constraint is then made inactive.
This method is specially attractive in the following case.
Positivity Constraints
An important special case is when the solution must satisfy uj > 0. The constraint 
is active if uj = 0 and inactive ifuj > 0. The projection on the active set is readily 
computed by putting inactive values to zero (see [3, 54, 55]). The gradient (or more 
generally the descent direction) is also easily projected. We shall meet this case in 
contact problems (Sect. 5.2.2).
Convex Constraints
To complete this section, we give a hint on how this can be extended to a convex 
constraint [18]. We consider as in Fig. 3.1 a point u0 on the boundary of the convex 
set C. If the constraint is active, the gradient (or some descent direction) z is pointing 
to the exterior of C. We can then project the gradient on the tangent to C (in red in 
Fig. 3.1), search for an optimal point u* on the tangent as we now have a linear 
constraint, and then project the result on C to obtain u1 . This will converge if the 
curvature of the boundary of C is not too large.
Fig. 3.1 Projected gradient 
for convex constraint

26
3 Iterative Solvers for Mixed Problems
3.1.4 Hierarchical Basis and Multigrid Preconditioning
There is a whole literature on preconditioning for standard finite element problems. 
It would be out of scope to present it here. Let us just say, for example, that 
incomplete factorisation is often very efficient. A classical combination is also to 
use a SSOR method to precondition a CG method.
Multigrid methods [27, 70, 81] yield powerful preconditioning. On structured 
meshes, they can be built very efficiently. On a general mesh, algebraic multigrid 
methods (AMG) are also available. In our computational examples, we shall often 
use a multigrid method based on a hierarchical basis. This was described in [1, 38]. 
The principle is simple and we use it in the case of a second degree finite element 
approximation in which we represent these elements by a hierarchical basis.
• The basis function associated to vertices is the standard first degree basis 
function.
• The basis function on edges is associated, not to a nodal value but to a correction 
to the piecewise linear approximation.
When this is done, the matrix A can be subdivided in submatrices
A = A11 A12
A21 A22
The preconditioning consists in solving with a block Gauss-Seidel iteration for the 
piecewise linear part (A11) and the piecewise quadratic correction on the edges 
(A22).
• It must be noted that for a typical three dimensional mesh, matrix A11 is about 
eight times smaller than the global matrix A. This makes it possible to use a 
direct method to solve the associated problem.
• This being a multigrid method, it can also be completed by applying an Algebraic 
Multigrid (AMG) method to the problem in A11.
• The matrix A22 has a small condition number independent of the mesh size [89] 
and a SSOR method is quite appropriate.
In our examples, this preconditioner will be applied to the rigidity matrix of 
elasticity problems. As we shall see, it is quite efficient for large problems.
The idea of hierarchical basis could be employed for other discretisations. In 
some finite element approximations, degrees of freedom are added on the faces to 
better satisfy a constraint.
Internal degrees of freedom are also amenable to this technique in a variant of 
the classical ‘static condensation’.

3.1 Classical Iterative Methods
27
3.1.5 Conjugate Residuals, Minres, Gmres and the Generalised 
Conjugate Residual Algorithm
We now consider the more general case of a system
Kx = b
having in mind K = A as in (3.1) or more generally (3.2).
When the matrix K is symmetric but not positive definite, the idea of keeping a 
residual orthogonal to all the previous ones as in the conjugate gradient method is 
still possible. However we must now choose a norm defining this orthogonality as 
K -1 can no longer be used. This chosen norm will also be the norm in which we 
shall minimise the residual. The simplest thing is to use the euclidean norm of Rn . 
This is the Conjugate Residuals (CR) method for symmetric systems presented in 
[65].
Considering Remark 3.1.1 one should rather introduce some norm which is 
better adapted to our problem. Contrarily to the case of the Conjugate gradient 
method, making the residuals orthogonal in some norm M-1 requires some extra 
computation. Some examples can be found in [67] and also in [64] where a general 
theory is developed. If r = b - Kx, z = M-1r and T = Kr the basic idea is thus 
to minimize
IIr - aT\M-1
which yields
(r, M -1T} 
a = --------- —
{T,M-1T)
This can be written in many equivalent ways. We refer for example to [41, 49] for 
a complete presentation. In the classical Minres algorithm, the Lanczos method is 
completed by Givens rotation to achieve orthogonality of residuals.
Remark 3.4 Although Minres is a well studied and popular method, we did not use 
it for three reasons.
• Preconditioning: the change of metric is often said to be a SPD preconditioning. 
We shall present in Sect. 
 preconditioners which are symmetric but not positive 
definite.
3.2
• Contact problems are constrained problems and one must have access to the 
part of the residual with respect to the multiplier. This was not possible to our 
knowledge with the standard Minres implementation or indeed with GMRES 
(see however [51]).
• Frictional contact leads to non symmetric problems.
■

28
3 Iterative Solvers for Mixed Problems
Our choice was rather to consider a method allowing both a general preconditioning 
as a change of metric and non symmetrical matrix.
The Generalised Conjugate Residual Method
If the conjugate residual method introduced by Luenberger [65] seemed a natural 
starting point, our interest in solving non symmetrical problems arising from 
frictional contact led us to the use of the generalised conjugate residual (GCR) 
method ([36, 39], which can be seen as a variant of the flexible GMRES method 
[80]). Evidently more standard methods such as the Conjugate Gradient (CG) 
method can be preferred when their use is possible.
Since the GCR method does not rely on symmetry, which means that non 
symmetrical preconditioners can be employed. The price to pay is the storage of a 
stack of vectors. When using the GCR method, we have two possibilities to consider 
preconditioning.
• Left preconditioning: the system Kx = b is replaced by the equivalent system 
P-1Kx = P-1b.
• Right preconditioner: the system is replaced by the system KP-1y = b with 
x = P -1y.
Remark 3.5 (Change of Metric) We can introduce a metric M different from the 
standard euclidean metric onRn in the left and right preconditioned GCR algorithm. 
The metric M is applied in the space of solutions while the dual metric M-1 is 
applied to the residuals. 
■
We thus have two possibilities while using the preconditioner P to accelerate a 
GCR method. We give here both left and right generic P -GCR using an arbitrary 
metric M.
The Left Preconditioning
We first present the left preconditioner. This is the standard procedure as presented 
to precondition the GMRES method in [81].
Algorithm 3.2 Left preconditioned P -GCR algorithm with an arbitrary metric M
1: Initialization
• i=0
• Let x0 the initial value.
• r0 = b - Kx0
• r0P = P-1(r0)

3.1 Classical Iterative Methods
29
2: while criterion > tolerance do
zi = riP
Ti = Kzi
zi = P -1Ti
From zi, using the modified Gram-Schmidt (MGS), compute z— orthonor­
mal in the M-norm to [z—, • • • , z—-1]. Using the same transformation on Ti 
compute T— based on [T—, • • • , Ti— 1].
в = (rp,Mz—)
Update
ri+1 = ri - fiT— 
rP+1 = rP - ezi 
xi+1 = Xi + ezi 
i=i+1
endwhile
■
The Right Preconditioning
This is the method which was used in most of our numerical results.
Algorithm 3.3 Right preconditioned P -GCR algorithm with an arbitrary metric 
M-1
1: Initialization
• i=0
• Let x0 the initial value.
• r0 = b - Kx0
2: while criterion > tolerance do
• zi = P-1ri.
• Ti = Kzi
• From Ti, using the MGS, compute T— orthonormal in the M-1 -norm to 
[T0—, • • • , T—-1]. Using the same transformation to zi compute z— based on 
[z0,•• • , z—-1].
• в = { r,M-1 Ti)

30
3 Iterative Solvers for Mixed Problems
• Update
Xi+1 = Xi + 0z- 
r+1 = Г - e'Ti' 
i=i+1
endwhile
■
Remark 3.6 (Choice of Metric) If M-1 is the identity, that is the standard metric of 
Rn, this reduce to the classical algorithm. This raise the question of the choice of 
M . To fix ideas, let us consider two cases frequently met.
When K and the preconditioner P are both SPD. We would then want to obtain 
a method equivalent to the preconditioned conjugate gradient method. As we have 
seen in Remark 3.2 the norm should then be defined using P1/2K-1P1/2. If the 
preconditioner is good, this is close to identity. Thus M = I is not a bad choice 
when we have a good precondioner.
For a symmetric system, if the preconditioner is SPD, we could take M = P. 
One could then read this as a variant of Minres. 
■
The Gram-Schmidt Algorithm
In the GCR methods described above, we want to make a vector Ti orthogonal to 
each vector in a stack in norm M or M-1. We consider for example the case of M- 
orthogonality. If the vectors in the stack are orthonormal in norm M, the modified 
Gram-Schmidt method means computing,
Algorithm 3.4 Modified Gram-Schmidt orthonormalisation
1: Initialisation
• Ti given
2: for j = i - 1, .. . , 0 do
• sj = {MTi, T-,
• Ti = Ti - SjT-
endfor
• T- = Ti/1|Ti ||m
■
For the modified Gram-Schmidt method, one thus needs to compute MTi at every 
step as Ti is changed, which could be expensive. This can be avoided at the cost of 
storing in the stack both T- and MT-.

3.1 Classical Iterative Methods
31
GCR for Mixed Problems
We now come to our central concern: solving mixed problems. We now have an 
indefinite system. In that case, iterative methods will often diverge or stagnate [45]. 
For example, without preconditioning a conjugate gradient method applied to an 
indefinite system will diverge.
Remark 3.7 (We Have a Dream) As we have already stated,we are dealing with a 
problem in two variables. We dream of a method which would really take this fact 
into account. The preconditioner that we shall develop will. The GCR method is 
then not really satisfying even if it provides good results.
Alas, nothing is perfect in our lower world. 
■
To fix ideas, let us consider a direct application of a right preconditioned GCR 
algorithm. Let then A be as in (3.1) and b = (f, g) e Rn x Rm. We are looking 
for x = (u, p) and we use the right preconditioned GCR as in Algorithm 3.3. As 
before we introduce an arbitrary metric. In this case the metric take into account the 
mixed nature of the problem and is characterised by two different matrices Mu and 
Mp giving a (Mu, Mp)-norm for (u, p).
II(u, P)II2ми,мр) = IIuM + IIP\\mp = (Muu, u) + (MpP, P)
Obviously, if both Mu and Mp are identities we have the usual right preconditionned 
GCR algorithm.
Algorithm 3.5 Right preconditioned Mixed-P-GCR algorithm with arbitrary met­
ric (Mu, MP)
1: Initialization
• i=0;
• Let x0 = (u0, p0) the initial value.
• ru = f - Au0 - Bp0, rp = g — Bu0 andr = (ru, rp)
2: while criterion > tolerance do
• From r = (ru, rp) thepreconditionner yields zi = (ziu, zip).
• Tiu = Aziu + Bzip, 
Tip = B ziu
• From Ti = (Tiu,Tip), using the MGS compute Ti- orthonormal in 
(M-1, Mp— )-norm to [ Ti, • • • , Ti-1]. Using the same transformation to 
zi compute zi based on [zi, • • • , zi—1 ].
• в = ( riu,M—1 T^) + (rtp,M- Tii)

32
3 Iterative Solvers for Mixed Problems
Update
Xi+1 = Xi + ezt 
r = r - вЪ 
i=1+1
end while
■
Remark 3.8 In the above GCR algorithm, we could decompose в in two component
в = ви + вр 
ви = { ru,MuTi), 
вр ={ rp,MpTii)
If the system is not properly preconditioned, ви or вр can become negative and 
в = ви + вр can even become null, hence stagnation. This is the symptom of a 
bad preconditioning. Another issue is that giving an equal weight to и and р in the 
scalar product is not a priori the best choice, although the right choice is seldom 
clear. This fact is in reality common to any iterative method applied to a system of 
this form. 
■
The key is therefore in the construction of good preconditioners or in building 
optimisation methods adapted to saddle-point problems. This will be our next point.
Remark 3.9 (The Case of Sect. 2.1.4: Perturbed Problems) Whenever one wants 
to solve a problem of the form (2.12) one should make a few changes.
• In the initialisation phase, one should have rp = g - Bu - cRp
• After the preconditioner, one should have Tp = Bzu - cRzp.
■
3.2 Preconditioners for the Mixed Problem
3.2.1 Factorisation of the System
This section is centered on the development of an efficient preconditioner for the 
system
4 u) = ( f) 
p g
(3.6)
with A an indefinite matrix of the form (3.1) or more generally for the non 
symmetric case (3.2). Our first step toward a general solver for systems (3.6) will be

3.2 Preconditioners for the Mixed Problem
33
to consider a block factorisation LDU of the block matrix A
I0
B1A-1 I
A0
0 -S
I A-1B2t 
0I
= LDU
(3.7)
where the Schur complement which we 
B1A-1B2t is invertible, we then have,
already considered in Sect. 2.2.3 S
(LDU)-1
-A-1B2t
A-1
0 -S-1
I0
-B1A-1 I
(3.8)
I
0
I
0
If A is symmetric and B1 = B2 we have U = LT and the full block decomposition 
F ~ LDLT is symmetric but indefinite. To simplify, we shall focus our presentation 
on the symmetric case as the extension to the non symmetric case is straightforward.
Remark 3.10 (Regular Perturbation) We have considered in Sect. 2.1.4 the case of 
a regular perturbation where Bu = g is replaced by Bu - cMp = g. In that case S 
is replaced in the factorisation (3.7) by S + eM 
■
Solving Using the Factorisation
Let (ru, rp) be the residual of the system (3.6) for (u0, p0)
ru = f - Au0 - Btp0 
rp = g - Bu0.
Using the factorisation (3.8) to obtain (u, p) = (u0 + bu, p0 + bp) leads to three 
subsystems, two with the matrix A and one with the matrix S.
Algorithm 3.6
' bu* = A-1 ru
bp = S-1 (Bbu* - rp) 
(3.9)
bu = A-1 (ru — Bbp) = bu * — A-1 B bp.
■
It must be noted that this is symmetric as it is based on (3.8). Particular attention 
must be given to the solution
Sbp= BA-1Btbp = Bbu* - rp. 
(3.10)
With the exception of very special cases, S is a full matrix and it is not thinkable 
of building it explicitly. Even computing matrix-vector product (needed by any 
iterative solver) would require an exact solver for A which may become impossible 
for very large problems. Therefore, except in rare cases where systems in A can 

34
3 Iterative Solvers for Mixed Problems
be solved extremely efficiently Algorithm 3.6 is practically unusable. Thus in most 
cases efficiency will lead us to iterative solvers for A which can avoid the need for 
an exact solver for S.
Nevertheless (3.9) suggests a way to precondition A. To do so, we shall first 
write an approximate form of the factorisation (3.7).
Assuming that we have an ‘easily invertible’ approximation A ofA and inserting 
it in (3.7) to replace A, we obtain
A=
I0
BA-1 I
I A-1Bt 
0I
= LDU
(3.11)
A
0
0
S
where
S = BA-1Bt
(3.12)
If we now have an approximation S of S, we can introduce
Algorithm 3.7 A general mixed preconditioner (GMP)
z U = A -1 ru
Zp = S--1 (Bzu - 4' 
__ 
(3.8)
zu = A-1(ru - Btzp) = zUu - A-1Btzp.
■
Using the factorisation (3.11), we may associate (3.8) with a matrix A
A= ABt
B BA-1Bt - S
(3.9)
This suggests many variants for a preconditioner of A. Their implementation 
relies on two essential tools.
1. An easily computable and fast approximate solver A-1. For this, we can rely 
on classical iterative methods such as the Conjugate Gradient method when 
the matrix A is symmetric and positive definite, the GMRES method or the 
Generalised Conjugate Residual method. Many other methods are available on 
general packages such as Petsc or Lapack. In all cases, a good preconditioner 
will be essential. We shall often rely on the multigrid method of Sect. 3.1.4.
2. An easily computable and fast solver S to approximate S.
This contains in particular the case A = A and therefore S = S.
Remark 3.11 (A Solver in Itself) Although we shall use Algorithm 3.7 as a 
preconditioner, it should be noted that it is also an iterative method in itself and that 
it can be employed as a solver. Using it as a preconditioner in an iterative method 
can be seen as a way of accelerating it (Remark 3.1).

3.2 Preconditioners for the Mixed Problem
35
3.2.2 Approximate Solvers for the Schur Complement and the 
Uzawa Algorithm
In the following, we consider as in (3.12) approximations of the Schur complement 
S or its approximate form S defined in (3.12).
When the solver in u is exact, we have S = S = BA-1Bt and the preconditioner 
of Algorithm 3.7 reduces to using S instead of S. Therefore, in any cases, S is related 
to a Schur complement matrix. We will not explicitly construct a matrix S. Instead 
the equation Sv = r in (3.8) should be interpreted as an approximate solution of 
Sv = r . We thus describe how we can employ an iterative method to obtain a 
computable S. This iterative method can then be used with a more or less stringent 
desired precision or a limited number of iterations. We also need a preconditioner 
for this iteration, proceeding as follows
• We introduce an easily invertible approximation MS of S, following the discus­
sion Sect. 2.2.3
• We solve Szp = rp by a an iterative method using MS as a preconditioner. In 
practice we restrict this step to a few (most often one) iteration.
Remark 3.12 (The Choice of MS) This is an important point as it can make 
the difference between an efficient method and a poor one. The choice evidently 
depends on the problem. We have considered in Sect. 2.2.3 and in particular in 
Proposition 2.2 how the choice for MS = R or MS satisfying (2.28) would be 
adequate provided the inf-sup condition holds and (Av, v} is a norm on Vh, that are 
the conditions to have a well posed problem.
• For problems of incompressible materials which we shall consider in Sect. 4.2, 
the choice MS = M0 where we denote M0 the ‘mass matrix’ associated to the L2 
scalar product in Q is a good choice. Indeed it was shown in [41] that this choice 
leads to an iteration count independent of the mesh size (See Proposition 2.2.3). 
In this case S is an operator of order 0.
• For contact problems, MS = M0 is less efficient as we do not have the inf-sup 
condition in L2 and S is now of order 1. We shall discuss this in Sect. 5.1. We 
shall also show how a discrete Steklov-Poincare operator could be built.
• One could also think of a BFGS (Broyden, Fletcher, Goldfarb, and Shannon) 
update [63] to improve MS during the iterative process.
• One should also recall (Sect. 2.3.3) that the Augmented Lagrangian Method 
yields a preconditioning for the dual problem.
■
We now make explicit two iterative processes. As we noted above, they will be the 
same for the solution of Sp = rp or Sp = rp . For the GCR method, we have the 
following algorithm.

36
3 Iterative Solvers for Mixed Problems
Algorithm 3.8 A right preconditioned GCR iterative solver for S
1: Initialization
• i=0;
• Let zu and r0p be given values.
• zp = 0
2: while criterion > tolerance or maximum number of iterations do
• zzip = MS rip
• zziu = -A-1Btzzip
• Tip = Bzziu.
• Use the MGS procedure to obtain Tip orthonormal to the previous directions. 
Apply the same transformation to zzip and zziu to get zzpp and zzpu.
• 
в = (Tip ,rip)
• Update
zp zp ezzip 
Zu = Zu - ezziu 
i=i+1
end while
■
One could also use the more general version of Algorithm 3.3 with a change of 
metric.
Remark 3.13 The computation ofzu is optional. It avoids an additional use of A-1 
when this is included in the preconditioner 3.7 to the price of some additional work 
in the Gram-Schmidt process. 
■
If A and MS are symmetric, we can also use the simpler Conjugate Gradient 
method, which takes the following form
Algorithm 3.9 A CG iterative solver for S
1: Initialisation
• i=0
• zu and rp, given values
2: while criterion > tolerance or maximum number of iterations do
• zzp = MS-1rp
• If i> 0 a = (zzp,rp)/(zz°p, rp)
• 
Wp = zzp + a w°p
• zzu = -A-1Btwp
• Tp = Bzzu.

3.2 Preconditioners for the Mixed Problem
37
• в = (Wp,rp)/(Tp,Wp)
• wp0 = wp, zz0p = zzp, rp0 = rp
• Update
Zp = zp + ezzp
Zu = Zu - ezzu 
rp = rp - eTp.
i = i + 1
end while 
■
The Uzawa Algorithm
When A = A we recall that S = S and for sufficiently strict convergence criteria the 
last algorithm corresponds to a solver for S. Then when included in Algorithm 3.7 
it yields solutions of (3.6) that is Algorithm 3.7 coincides with Algorithm 3.6. In 
[43], this was called the Uzawa algorithm, which we can summarise as formed of 
two steps illustrated in Fig. 3.2.
• Solve the unconstrained problem Au = f.
• Project this solution, in the norm defined by A,onthesetBu = g.
In [43] Uzawa’s algorithm was presented in its simplest form depending on an 
arbitrary parameter в . The parameter в must then be chosen properly and depends 
on the spectrum of BA-1Bt. This was studied in detail and convergence follows as 
in the classical analysis of gradient methods (an acceleration by a conjugate gradient 
method was also considered. Using this method implies that one has an efficient 
solver for A.
Many variants of this method are proposed in the literature (see [15, 81]). 
Several authors have studied numerically and in the theoretical framework of the 
variants of the method. Elman and Golub [40] proposed the Uzawa method called
Fig. 3.2 Illustration of the 
Uzawa algorithm 

38
3 Iterative Solvers for Mixed Problems
inexact whose theoretical analysis is made in [23]. Bai et al. [10] presented the 
so-called parametrised method. More recently, Ma and Zang in [66], dealt with 
the so-called corrected method. They have shown that it converges faster than the 
classical method and several of its variants under certain assumptions. However, 
their approach come up against the question of determining the optimal parameter.
3.2.3 The General Preconditioned Algorithm
We now come to the use of Algorithm 3.7: the General Mixed Preconditioner.
• We must first chose A-1. For this we rely on standard and well proven iterative 
methods. Whenever possible we precondition these methods by a multigrid 
procedure.
• We also need an approximate solver for the approximate Schur complement S.
• We also choose a norm N in which we minimise residuals. This will most of 
times be the euclidean norm but better choices are possible
If for S we use Algorithm 3.9 or Algorithm 3.8 (i.e. we solve S)thenweusea 
limited number of iterations. Choosing S = S (i.e. fully converging Algorithm 3.9 
or Algorithm 3.8) is a possibility, but in general it is not a good idea to solve too 
well something that you will throw away at the next iteration. We shall thus develop 
the case where only one iteration is done.
Remark 3.14 One should note that using a better A and more iterations for S is a 
direct way to make things better.
• If A = A we have the Uzawa algorithms or close variants.
• If S is solved exactly, we have a form of the projected gradient method.
■
This being said we shall focus on a simple form of Algorithm 3.7.
Algorithm 3.10 A simple mixed preconditioner
1: Initialization
• ru, rp given
• zu = A-1ru
• rp = Bzu - rp
2: Approximation of S-1
• zp = MS-1rp
• zzu = A-1 Btzp
• Tp = Bzzu
• в = t.Tr,Tr)

3.2 Preconditioners for the Mixed Problem
39
Fig. 3.3 Illustration of the 
preconditioner
3: Final computation
• 
Zp = ezp
• 
Zu = Zu - ezzu
4: End
This essentially amounts to obtaining a scaling by в of M-1 rp• The computation of 
в implies the computation of zzu, which implies a resolution with the approximate 
solver A-1. This computation would anyway be required by the last part of 
Algorithm 3.7 so that there is in fact no extra cost. If we have symmetry of A and 
MS, we can obtain в by one iteration of the gradient method instead of a minimum 
residual.
Remark 3.15 (The Preconditioner Yields a Saddle-Point) It is interesting to see in 
Fig. 3.3 that the preconditioner yields a saddle-point in a two-dimensional space. 
One can also see that ZZu is A-orthogonal (conjugate) to Zu
This makes us think of the Partan method [82] where conjugation is obtained 
after two descent steps (see Fig. 3.4).
■
Remark 3.16 (The Classical Method of Arrow-Hurwicz-Uzawa) We shall later use, 
for the sake of comparison the classical Arrow-Hurwicz-Uzawa method, described 
in [43], as a preconditioner.
Algorithm 3.11 A super-simple mixed preconditioner
1: Input ru, rp, Output Zu, Zp.
2: Compute Zu and Zp
• Zu = A-1ru

40
3 Iterative Solvers for Mixed Problems
Fig. 3.4 Illustration of 
Partan’s method
• rp = rp - Bzu
• Zp = eMS 1 Гр
The problem here is that в has to be determined by the user, while in the previous 
version, everything was automatic. The choice of parameters had been discussed in 
[43]. Moreover, as we said above, the last part of Algorithm 3.7 requires an extra 
resolution. 
■
Remark 3.17 (The Perturbed Problem) We have considered in Sect. 2.1.4, a 
regular perturbation. We would then have to solve a problem of the form 
ABt
B — (Mq
pu = 
fg
where MQ is the matrix defining the metric on Q. We have noted in Remark 2.13 
that S then becomes S + <Mq and that MS is changed into (1 + e)MS if Mq = MS
The preconditioner of Algorithm 3.10 is easily adapted to the perturbed case. If 
one uses MS = MQ one should change,
• rp = Bzu — (Mqp — rp, 
1 
—1
• zp = T+7 M- rp
• 
Tp = Bzzu — eM-zp
When the preconditioner 3.10 is employed for the modified problem of Sect. 2.1.4, 
one should also modify the computation of residuals in the associated CG or GCR 
method, taking into account for example Remark 3.9. 
■

3.2 Preconditioners for the Mixed Problem
41
3.2.4 Augmented Lagrangian as a Perturbed Problem
We have considered in Sect. 2.3.4 a form of the augmented Lagrangian method in 
which one has to solve a sequence of problems of the form (2.36), penalty problems 
written in mixed form.
This could be considered when a classical Augmented Lagrangian as in (2.34) 
would not be possible because R-1 is a full matrix. This could also be an alternative 
to the regularised lagrangian.
Using this implies a two level iteration, which can be compared to a Newton 
method. To fix ideas, we describe an implementation by the GCR algorithm.
• Initialise a GCR method with the residuals of the unperturbed problem.
• Solve for 8u and 8p with the penalty problem in mixed form using the 
preconditioner as Remark 
 and the GCR iteration following Remark
 but 
without changing the initial residuals.
3.17
 3.9
• Update u + 8u and p + 8p.
From there, one has many variants. The internal iteration may not be too exact. 
The update could itself be accelerated by another GCR. When the problem is the 
linearised part of a Newton iteration, one could in fact use this as an inexact Newton.

Chapter 4
Numerical Results: Cases Where Q = QQ
Check for 
updates
We shall now illustrate the behaviour of these algorithms on some examples. In all 
the cases considered here, the space of multipliers Q can be identified with its dual. 
In fact we shall have in all cases Q = L2 (Q).
The first example will be the mixed approximation of a Poisson problem with 
the simplest Raviart-Thomas element. This will allow us to consider a real discrete 
Augmented Lagrangian method and its impact in iterative solvers. We shall then 
consider incompressible problems in elasticity in both the linear and non linear 
cases. Finally we shall introduce an application to the Navier-Stokes equations
4.1 Mixed Laplacian Problem
The first example that we consider is the mixed formulation of a Dirichlet problem 
using Raviart-Thomas element. As an application, we can think of a potential fluid 
flow problem in porous media as in the simulation of Darcy’s law in reservoir 
simulation. This can be seen as the paradigm of mixed methods and the simplest 
case of a whole family of problems. Higher order elements and applications to a 
mixed formulation of elasticity problems are described in [20].
4.1.1 Formulation of the Problem
Let a domain of Rd with d = 2 or 3. Its boundary Г is divide into ГD and I'N on 
which respectively Dirichlet and Neumann conditions will be imposed. We denote 
n the normal to Г. Given f e L2(Q) and g e (L2(ГN)~)d, the problem consists in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
43
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_4

44
4 Numerical Results: Cases Where Q = Q'
finding a function p e H1 (Q) such that
—kp
=f
in
p
=0
on TD 
(4.1)
(grad p) •n
=g
on r,y
Taking u = grad p, Eq. (4.1) is equivalent to
u — grad p
=0
in
— div u
=f
in
(4.2)
p
=0
on TD
u ' n
=g
on r,y
Let
V = H(div, &) = {u e (L2(Q))d, div u e L2(Q)}
and
Q = L2 (Q).
The variational formulation of (4.2) is then to find u e V and p e Q satisfying
f u • v dx + J p div v dx = J g • v ds V v e V, 
d div uqdx = — У f q dx 
V q e Q
(4.3)
In this context, the inf-sup condition [20] is verified and, the operator a(•, •) defined 
on V x V by
a(u , v) = j u • vdx
is coercive on the kernel of the divergence operator but not on the whole space V.
The mixed formulation (4.3) is the optimality condition of the following inf-sup 
problem
inf sup - I | v |2 dx — f g • vds + f q div vdx + f f q dx (4.4)
veVqeQ 2 
Jq 
Q

4.1 Mixed Laplacian Problem
45
In order to obtain a discrete form of the problem, we introduce the Raviart-Thomas 
elements. We suppose to have a partition Th of into tetrahedra. If we denote by 
Pk(K) the polynomials of degree k on a tetrahedron K, we define,
RTk(V) = {uh e (Pk(K))d + xPk(K) VK e Th} 
(4.5)
The properties of these spaces and related ones are well described in [20]. Indeed 
they have been built to be applied to problem (4.3). If we take
Qh ={qh e Pk(K) VK eTh}
the matrix R corresponding to the scalar product on Qh is block diagonal and we 
have the important property that div uh e Qh, the inclusion of kernels property 
which ensures that coercivity on the kernel is also valid for the discrete problem. 
The discrete operator divh corresponding to the definition of (2.16) is indeed the 
restriction of div to Vh .
4.1.2 Discrete Problem and Classic Numerical Methods
The discrete problem is clearly of the form (2.22) and indefinite. In [20, p. 427], this 
was considered ‘a considerable source of trouble’. Let us consider things in some 
detail.
• The matrix A is built from a scalar product in L2 (Q).
• The operator B is the standard divergence
• Bt is a kind of finite volume gradient.
• The Schur complement BA-1 Bt is a (strange) form of the Laplace operator 
acting on piecewise constant.
It was shown in [12] how using a diagonalised matrix for the matrix A, one 
indeed obtains a finite volume method. In the two-dimensional case, it is known 
(see [68]) that the solution can be obtained from the non-conforming discretisation 
of the Laplacian.
Another general approach (see [6] for example) is to impose the interface 
continuity of the normal components in Vh by Lagrange multipliers to generate a 
positive definite form.
We shall rather stick to the indefinite formulation an show how the methods that 
we developed in Chap. 3 can be applied.
The Augmented Lagrangian Formulation
The first point is that we have a lack of coercivity on the whole space. The result of 
Proposition 2.2 does not hold and the convergence of Uzawa’s method, for example 

46
4 Numerical Results: Cases Where Q = Q'
would not be independent of h. To obtain the coercivity on the whole of V we shall 
consider an augmented Lagrangian method, which also means to use
a(u, V) + a(div u, div v).
Solving the equation with the augmented Lagrangian method gives us the inf-sup 
problem
inf sup- I | v |2 dx+— I | div v + f |2 dx
v q 2 Q 
2 Q
(4.6)
— У g • vds + У q div vdx + j f q dx
where a > 0 is a scalar representing the parameter of regularisation. It’s well known 
that the equation (4.4) is equivalent to (4.6) for which optimality conditions are
У u • vdx + ^У divu div vdx + j p divvdx
■ 
= У g • vds — a У f divvdx Vv e V, 
(4.7)
У divuq dx = —j f q dx 
Vq e Q.
Here, for all a greater than zero, the coercivity is satisfied on V since
a(v,v) + ^У div u div vdx > min (1 ,a2) || v ||V, 
V v e V,
with
|| v ||V = УУ |u |2 dx + j | div v|2 dx^
A consequence is that the two forms of the augmented Lagrangian (2.30) and 
(2.32) are the same and that using them will not change the solution of the problem.
Obviously (2.32) corresponds to the linear system of equations associated to the 
discrete version of (4.7). As we said earlier, for the Raviart-Thomas elements MS = 
R is a block diagonal matrix associated to the L2 scalar product on Qh and we are 
in the perfect situation (Remark 2.11) for the augmented Lagrangian.
/ A + aB‘R—1B B \( u \ / g — aBM— f \
=.
B 0p 
f

4.1 Mixed Laplacian Problem
47
4.1.3 A Numerical Example
For our numerical tests, we take k = 0 in (4.5). Qh is thus a space of piecewise 
constants. For the numerical example, we consider the domain as a cube of unit 
edges. We take ГD empty so Г,у = dQ. The functions f and g are given by:
f (x, y, z) = —3n2 sin(nx) sin(ny)sin(nz)
g(x, y, z) = 0
We use a tolerance of 10—12 for the convergence of the algorithms.
The interest of this example is to see the effect of the augmented Lagrangian 
formulation and the role of the parameter a. We would like to emphasise again 
that we are in an ideal case where the augmented Lagrangian does not change the 
solution.
• Small values of a act as regularisation parameter. They provide coercivity on the 
whole of V and not only on the kernel of the divergence operator.
• Large values of a yield a penalty method which is then corrected by an iteration. 
This is the classical augmented Lagrangian method.
To evaluate the effect of the augmented Lagrangian we first present results in which 
we use a relatively crude mesh with 50 688 tetrahedra and 24 576 faces.
We first consider the case of a direct solver for the primal problem in u. This 
is therefore the Uzawa algorithm of Sect. 3.2.2. The number of iterations in p 
depends on the condition number of the dual problem (2.23) which improves when 
a becomes large. This is observed in our experiences. Table 4.1 illustrate that a 
larger a yields a better convergence as should be expected. In fact we see that the 
largest improvement is to have coercivity on the whole space (taking a>0) rather 
then having coercivity only on the kernel (a = 0). Notice that increasing values of a 
does not improve things as the condition number of the problem in u becomes bad.
We can compare this with an incomplete resolution in u. This is done by using 
a simple iterative method: a fixed number of the conjugate gradient method with an 
SSOR preconditioner, which we denote CG(n). The results presented in Table 4.2 
were obtained for n = 5andn = 10.
We observe once again a large difference between a = 0anda>0. We also 
see that there is an optimal a and that for too large values the condition number
Table 4.1 Laplace problem 
with coarse mesh and 
complete (LU) resolution for 
the primal problem: global 
number of iterations and 
computing time in seconds 
according to the value of a
a
# iterations
CPU time (s)
0
133
18.57
10-1
7
3.2995
102
5
2.8836
105
3
2.5867
107
3
2.5305

48
4 Numerical Results: Cases Where Q = Q'
Table 4.2 Laplace problem 
with coarse mesh with 
CG(5)-SOR and 
CG(10)-SOR for the primal 
solver: number global of 
iterations (# it.) and 
computing time in seconds 
according to the value of a
Table 4.3 Laplace problem 
with fine mesh: total number 
of iterations and computing 
time in seconds according to 
the solver of primal problem
a
CG(5)-SOR
CG(10)-SOR
#it.
CPU time(s)
#it.
CPU time(s)
0
133
13.6105
133
14.8259
1.0 X 10-4
80
7.0878
79
8.8246
5.0 x 10-4
45
4. 2140
42
5.1697
1.0 x 10-3
38
3.7885
31
4.2228
2.5 x 10-3
37
3.9470
25
3.7092
5.0 x 10-3
37
3. 7123
24
3.6260
1.0 x 10-2
47
4. 4617
22
3.4336
LU
(a = 105)
CG(100)
(a = 2.5 x 10-3)
CG(50)
(a = 10-2)
#it.
3
15
25
CPU (s)
808.5
586.7
569.1
in u worsens the performance. If we improve the resolution, we have indeed better 
results.
With a better solver, the optimal value ofa increases and the number of iterations 
in p decreases. However, the computing time does not as each iteration becomes 
more expensive.
Since the goal is to solve big problems, we took a rather fine mesh (with 
3,170,304 faces and 1,572,864 tetrahedra) to prove the interest of the algorithm with 
an incomplete resolution. In this context, the direct resolution is very expensive in 
time as can be seen in Table 4.3 whereas in iterative resolution, the computation 
time is less.
We can see that an iterative method for u can be better than a direct solver. 
This result could surely be much improved by a more clever choice of this iterative 
method.
Remark 4.1 (This Is not Optimal) We would like to emphasise that the choice of 
the solver in u is not optimal and that one could improve the results. Our main point 
is that the indefinite form of mixed methods is not such a ‘source of trouble’! ■
4.2 Application to Incompressible Elasticity
We consider a second class of examples, arising from the discretisation of incom­
pressible elasticity problems. We consider two cases : a standard linear elasticity 
model and a Mooney-Rivlin model [21, 53, 84]. This relatively simple case will 
allow us to compare the preconditioners introduced and to discuss strategies to 
improve coercivity.

4.2 Application to Incompressible Elasticity
49
4.2.1 Nearly Incompressible Linear Elasticity
Given a body £2, we want to determine its displacement u under external forces. 
Denoting Г the boundary of £2, we define ГD the part of Г on which Dirichlet 
(displacements) conditions are imposed and r,v the part where we have Neumann 
(forces) conditions. To avoid unnecessary developments, we restrict ourselves to the 
case of null conditions on TD. We thus define,
V = {v |v e (H1 (a))d, v = 0 on VD}
For v e V, we define the linearised strain tensor
^ij(v) = 2 (diUj + djUi)
(4.8)
and its deviator
.=D = £ - 1 tr& I
One then has,
1 ^D(V)I2 = 1Ф.)I2 - 1 tr(g(v)))
(4.9)
To define our problem, we have to define some parameters. Elasticity problems are 
usually described by the Young Modulus E and the Poisson ratio v. We shall rather 
employ the Lame coefficients д, A
and the bulk modulus k
E 
Ev
д = ---------- , A =--------------------
2(1 + v), 
(1 + v)( 1 - 2v)
(4.10)
E 
2д
k = 3 (1 - 2v) = A + 3
(4.11)
We then consider the problem
inf д I |eD(v)|2 dx +— I | div v — g|2 dx — f f • vdx 
veV Ja~ 
2 Jq 
Jq —
(4.12)
or equivalently,
inf д I |s(v)|2 dx +— I | div v — gi2 dx — f f • vdx. 
veV 
Jq ~ 
2
(4.13)

50
4 Numerical Results: Cases Where Q = Q'
Problem (4.13), for example, leads to the variational form,
2№ У V(U) : £(v) dx + У (div U - g) div V dx = j f • vds V v e V.
Q 
Q 
° 
(4.14)
It it is well known that a brute force use of (4.14) or (4.13) could lead to bad 
results for large values of A (or as the Poisson ratio v nearing its maximal value of 
1/2). In extreme cases, one gets a locking phenomenon that is an identically zero 
solution.
The standard way to circumvent this locking phenomenon is to switch to a 
mixed formulation with a suitable choice of elements. Essentially, we introduce the 
variable 
p = A( div v — g)
and we consider the saddle-point problem,
infsup p. I |e(v)|2 dx-----I |q|2 dx
v q Q ~ 
2 A a
+ У q (div v — g)dx — У f • vds
for which optimality conditions are and denoting (u, p) e V x Q the saddle point,
2 ^ J e(u) : e(v) dx + J p div v dx = j f • vds V v e V, 
У div uq dx — — p pq dx = j g qdx 
V q e Q
(4.15)
In the limiting case of A becoming infinite, the case that we want to consider, the 
second equation of (4.15) becomes
У div uq dx = У g qdx V q e Q
Problem (4.15) is well posed. Indeed we have,
• The bilinear form a(u, v) = fQe(u) : e(v) dx is coercive on V. This is Korn’s 
inequality : there exist a constant a suchthat
a(v,v) = ^(IV(v)I2) > a||v||V Vv e V

4.2 Application to Incompressible Elasticity
51
• We have an Inf-sup condition [20]: there exists a constant в such that
b(v, q)
sup 
>в IIq II Vq e Q.
ve V II v II
Remark 4.2 If we use formulation (4.12) instead of (4.13) as the starting point, the 
bilinear form
aD(u, v) = У eD(u) : eD(v) dx
is coercive on the kernel of B. Indeed, by Korn’s inequality and (4.9) : there exist a 
constant a such that
—I^D(v)I2 = — (I£.(v)i2 - 1 tr(g(v)')2) > a||v||2 - -tr(£,(v)')2) Vv e V
This makes it possible for the matrix AD defined by the bilinear form aD(•, •) to be 
singular, a situation which is not be acceptable in our algorithms. 
■
Remark 4.3 In the limiting case of infinite к, that is of incompressible materials, 
problems (4.12) and (4.13) are equivalent as we have div u = tr(e(u)) = 0, the 
solution is unchanged and for (4.13) we have coercvity on the whole of V and 
not only on the kernel of B. However, for the discretised problem, the solution is 
modified as in general one does not have div uh = tr(e(uh)) = 0. 
■
In our numerical tests, we shall use the Augmented Lagrangian but as in (2.30) that 
is a ‘regularised Lagrangian’
2 — У §.(u) : £(v) dx + к У (divu — g) divqiix
• 
+ У p div v dx = У f • vds = 0 V v e V,
У divuq dx = У g qdx 
V q e Q
(4.16)
Where we define к using an artificial Poisson ratio v as in (4.10) which should in 
no way be close to 1/2 unless the choice of elements allows really divergence-free 
solution. The parameter к is arbitrary and is chosen to obtain a good convergence 
without downgrading the condition number of the system. This will be discussed in 
Sect. 4.2.5.
Remark 4.4 (The Stokes Problem) It is clear that all we say about this linear 
elasticity problem is valid for the equivalent Stokes problem for creeping flow

52
4 Numerical Results: Cases Where Q = Q'
problems. The numerical solution of the Stokes problem has been the object of a 
huge number of articles and books. 
■
4.2.2 Neo-Hookean and Mooney-Rivlin Materials
We now consider non linear elasticity models described in a Lagrangian formula­
tion. We thus have a reference domain denoted £2 and a deformed domain ш while 
their respective boundaries are denoted as Г and y . We denote X and x respectively 
their coordinates. We write the deformation of a body as
x = u(X)
The deformation F of this transformation is given by F = — = I + VXu and its 
dX 
~
determinant detF is denoted J . Note that VX stands for the gradient with respect 
to the variable X. The Cauchy-Green tensor is then defined as C = FT F and its 
principal invariants I1, I2 and I3 are given by :
11 = C : 1, 
12 = 1 (I2 - C : C), 
13 = det(C) = J2.
As in the case of linear elasticity, the boundary Г is composed of (at least) two 
parts: ГD where a Dirichlet condition is given and ГN where a Neumann (pressure) 
condition g is imposed.
• The Neo-Hookean model
Although there are many formulations of Neo-Hookean models for compress­
ible materials, they share a unique elastic potential energy function or strain 
energy function W as named in [73]. Following [91], we define a particular 
Neo-Hookean material where the potential energy function W is given by
w = M (i i - з) + 4 (J2 -1) - (2 + m) ln j
= 2 IJ3 3 11 - 0 + 2K(J - 1)2
where the material is characterized by m and к the Lame coefficients as in the 
linear model and к the bulk modulus.
• The Mooney-Rivlin model
For a Mooney-Rivlin hyperelastic material, the energy functional is given by
w = Mio fIз 311 - 3^ + M01 fIз 3I2 - 3^ + кk(J - 1)2
2 

4.2 Application to Incompressible Elasticity
53
where к, the bulk modulus, д 10 and д01 are parameters characterizing the 
material.
The elasticity problem consists in minimizing the potential energy W under appro­
priate boundary conditions. The weak formulation on the reference configuration 12 
can be written as a nonlinear problem,
( (F • S) :VxvdX = 1 f • vdX + ^ g • vdY 
(4.17)
for any v in a proper functional space and where S = 2dW/dC is the second Piola- 
Kirchoff stress tensor. More details on the formulation can be found in [36].
Mixed Formulation for Mooney-Rivlin Materials
We are interested in the simulation of incompressible materials, more particularly 
rubberlike materials. As in the case of linear materials, it is well known that 
displacement-only formulations are inadequate. As the material becomes more 
and more incompressible (corresponding to an increasing bulk modulus), the 
conditioning of the linearized system grows. And the usual locking phenomena may 
occur when the bulk modulus к becomes large. As a consequence, direct solvers and 
iterative solvers are ineffective (even with preconditionning).
It is then convenient to split the second Piola-Kirchoff tensor S into a volumic 
part and an isochoric part and to use a mixed formulation in which p is explicitly an 
unknown. We thus write
S = S' - pjC-1.
Here p is the pressure which is defined by p = — k(J — 1). For a Mooney-Rivlin 
hyperelastic material (of which a Neo-Hookean material is a special case),
S' = 2д 101—1 /3 (I — 311C—1) + 2д011—2/3 (11I — C — 312C— 1)
The mixed formulation can now be written as:
( (F • S') : VXv dX — j pJF—T : VXv dX
= j g • vdS +j f • vdX 
VV £ V 
(4.18)
((J — 1 )q dX + 1 I pq dX = 0 
Vq £ Q

54
4 Numerical Results: Cases Where Q = Q'
We are interested in the really incompressible case when the bulk modulus 
becomes infinite. As we have seen in the linear case, this may lead to an ill 
conditioned or even singular matrix in u. To get good results, we shall again 
introduce a stabilisation parameter K. We then define S using this artificial small 
bulk modulus. and solve
( (F • S) : VXv dX - j pJF-T : VXv dX
g g • vdS + j f • vdX 
Vv e V (4.19)
( (J - 1 )q dX = 0 
Vq e Q
This is a non linear formulation, its resolution will be based on a Newton-like 
method and will imply a linearisation of (4.18). To lighten the presentation (we 
refer to [42] for the derivation of the linearised problem), let us rewrite the non 
linear problem (4.18) as
R1 ((u, p),v) = 0,
R2((u, p), q) = 0.
At each Newton’s iterations, we consider a linearized version of this system with 
respect to u and p. Linearizing around (un, pn), we get the followings bilinears 
operators
an(Su, v)
dR 1 ((un,pn),v) „
-------------------- • Su 
du--- ~
I S'(un) : (VX(Su) • VXVjdX
(C(un) : (FT(un) • Vx(Su))) : (ft(un) • VXv}dX,
bn(v, Sp)
dR 1 ((un, pn),v) 
dp
• Sp
J JSp(F-T(un) : VXH dX
cn(Sp, q)
dR2((un, pn), q) 
dp
• Sp
[ 1 (Sp)qdX. 
Й k

4.2 Application to Incompressible Elasticity
55
The linearised variational formulation is, knowing (un, pn), the previous solution, 
to find (8u, 8p) such that
an(8u,v) + bn(v, 8p) = -RJ (un, pn),vj, V v e V, 
bn(8u, q) - cn(8p, q) = -R2((un, pn),q\, Vq e Q.
(4.20)
Remark 4.5 An important point is that the linearised system (4.20) depends on 
some initial value of the displacement. In general, we do not have the equivalent 
of Korn’s inequality and the matrix can in fact be singular if one has chosen as 
initial guess a bifurcation point.
■
4.2.3 Numerical Results for the Linear Elasticity Problem
To illustrate the behaviour of our algorithms, we first consider the simple case of 
linear elasticity on a three-dimensional problem. The results will also be applicable 
to the Stokes problem.
To obtain an approximation of (4.16), we have to choose a finite element space 
Vh С V and a space Qh c Q .In our numerical experiments, we consider a three­
dimensional problem. We thus had to make a choice of a suitable finite element 
approximation. The catalogue of possibilities has been well studied [20] and we 
made a choice which seemed appropriate with respect to the standard engineering 
applications. We employ tetrahedral elements, a choice motivated by our eventual 
interest in mesh adaptation and we want to respect the inf-sup condition without 
having to use elements of too high degree. The popular Taylor-Hood element was 
retained:
• A piecewise quadratic approximation for the displacement u and a piecewise 
linear approximation of the pressure p.
Remark 4.6 This choice of element is good but there is a restriction for the 
construction of element at the boundary: no element should have all its vertices 
on the boundary. This might happen on an edge if no special care is taken when the 
mesh is built. A bubble can be added at the displacement to avoid this restriction, 
but to the price of more degrees of freedom. 
■
Remark 4.7 (Regularised Lagrangian) We therefore have a continuous approxi­
mation for pressure and this has some consequences on the choice of solvers, in 
particular if one would like to use the augmented Lagrangian methods. It is almost 
impossible to employ (2.32) as for all reasonable choices of MS, MS-1 is a full 
matrix. However, as we shall see later the regularised formulation (2.30) term may 
improve convergence.

56
4 Numerical Results: Cases Where Q = Q'
In order to use the real augmented Lagrangian (2.32) , one would need a 
discontinuous pressure element, which would make MS = R block diagonal. For 
three dimensional problems such elements are of high polynomial degree [20] or 
induce a loss in the order of convergence. 
■
We therefore consider the discrete regularised Lagrangian corresponding to 
(4.16)
2 
£.(uh) : s(vh) dx + A j (div Uh - g) div vhdx
+ У ph divvh dx = У f • vhds = 0 Vvh e Vh, 
(4.21)
У div Uh qh dx = j gqhdx 
V qh e Qh
We shall later discuss in detail the choice of A.
4.2.4 The Mixed-GMP-GCR Method
In the following, we employ Algorithm 3.5 (Mixed-P-GCR) using the general 
mixed precondiioner GMP of Algorithm 3.7. The approximation of S is the one 
iteration version Algorithm 3.10. We shall call this combination Mixed-GMP-GCR. 
To complete the description, we must define a solver in u for which we consider 
different possibilities.
Approximate Solver in u
Our mixed solver relies on an approximate solver for the problem in u. We shall 
explore various possibilities for this choice, corresponding to different choice for 
A-1 in (3.8).
• The direct solver is denoted LU.
• The conjugate gradient method CG, GMRES or GCR methods can also be used.
• We also consider the HP method of [37] already discussed in Sect. 3.1.4. In this 
method, the quadratic approximation P2 is split into a linear P1 part defined on 
the vertices and a complement P2 part defined on edges and the matrix A is split 
into four submatrices
A = ( A11 A12
A21 A22

4.2 Application to Incompressible Elasticity
57
From this splitting, we obtain a two-level algorithm in which one solves in 
sequence the P1 part and the P2 part. From [89], we know that the P2 part is 
well conditioned so that a simple iteration is suitable,
- For the P 1 part, we use an Algebraic Multigrid Method (AMG) or a direct LU 
factorisation.
- For the P2 part, we use a SSOR iteration.
HP-AMG will denote the use of the HP solver with AMG; HP-LU the use of 
the HP solver with LU. We then use these solvers, as a preconditionner in the 
following ways :
- as a preconditioner on its own, limited to one iteration, denoted as PREONLY, 
- as a preconditioner for a GCR method or a GMRES method (see below).
With n the (limited) number of iterations of the GCR or GMRES, we shall denote 
GCR(n)-HP-AMG, GCR(n)-HP-LU, GMRES(n)-HP-AMG or GMRES(n)-HP- 
LU the approximate solver for A using one iteration of the HP-AMG or HP-LU 
solver as a preconditioner.
These approximate solver ofA are used in a general mixed preconditioner (GMP, 
here we use Algorithm 3.10) and the global system is solved using a Mixed-GMP- 
GCR solver (Algorithm 3.5).
Remark 4.8 For the approximation MS of the Schur’s complement in Algo­
rithm 3.10, we take the matrix associated with the L2 scalar product which we 
denote M0. Computing M0-1 can be solved by a direct or iterative solver. For the 
numerical tests, we use the LU factorisation (which can be done once for all). ■
Remark 4.9 (Variable Coefficients) Related problems for non Newtonian flows 
lead to variable coefficients. In [48] one considers the choice of the approximation 
MS (Sect. 2.2.3) to the Schur complement, which also defines a scalar product on 
Qh .They show that if the bilinear form
2№ I £.(u) : б(.Е) dx 
Ja~ ~
is changed into
2 y" ^(x)s_(u_) : e(u) dx 
(4.22)
one should take for MS
Ms = [ 
Phqhdx.
J ^(x)

58
4 Numerical Results: Cases Where Q = Q'
From Sect. 2.3 one should change the regularised form (4.16) into
2j ^(x)e(u) : s(v) dx + XJ ^(x)(divu — g) divvdx
+ У p div v dx = У f • vds = 0 V v e V
■
4.2.5 The Test Case
We consider a simple academic example : a cube [0, 1]3 clamped at the bottom (the 
plane z = 0) is submitted to a vertical displacement imposed on the top (plane 
z = 1) (see Fig. 4.1).
We consider a linear incompressible material with a Young’s modulus 
E = 102 MPa and, depending on the numerical experiment, an artificial 
Poisson coefficient v varying between 0.0 and 0.4. Four meshes of sizes 
h = 0.5, 0.25, 0.125, 0.0625 (respectively 2 187, 14 739, 107 811 and 823 875 
degrees of freedom) will be considered. Exceptionally, for Fig. 4.3, a very coarse 
mesh (h = 0.5) will also be used. Although this can be seen as a simple problem, it 
must be noted that it is a true three-dimensional case.
Remark 4.10 We shall present some examples illustrating the use of our mixed 
solvers. In these experiments, we have imposed a rather strict tolerance of 10-10 
on the l2 norm of the residual in p. 
■
We have introduced in (4.21) a ‘regularised formulation’ parametrised by X 
which we may associate with an artificial Poisson ratio v. We emphasise that this 
is not an Augmented Lagrangian: the penalty term is introduced for the continuous 
divergence-free condition and not for the discrete one so that, for the discretisation 
that we employ, the penalty parameter must be small in order not to perturbate the
Fig. 4.1 Geometry and boundary conditions
u(x, y, 1) := [0, 0, -2]
u(x, y, 0) := [0, 0, 0]

4.2 Application to Incompressible Elasticity
59
20
30
Global iteration number
Global iteration number
Fig. 4.2 Linear elasticity problem with GCR(3)-HP-AMG as primal solver: convergence in l2- 
norm of the primal and dual residuals according to the artificial Poisson ratio v
Fig. 4.3 Linear elasticity problem with GCR(3)-HP-AMG as primal solver: ||u$ — uo||L2 with 
respect h according to the value of v
solution. The global system is solved by Mixed-GMP-GCR as described above . 
The problem in u by three iterations of GCR with a preconditioning by HP-AMG.
Figure 4.2 shows that increasing v yields to a better convergence in p but has 
an adverse effect on the convergence in u when v is larger than 0.3. If we take the 
solution for v = 0 (the non-regularised problem) as a reference, we may quantify 
the effect of the regularising parameter v.
Figure 4.3 illustrate this effect using the difference in 12-norm of u for various 
values of v with respect to the mesh size h (from 0.2 to 0.0125). One sees in Fig. 4.3 
that the regularised formulation changes the solution but still yields convergence to 
the same solution when h goes to zero. It must be noted that the convergence is not 
quadratic as expected for a smooth solution. In fact the solution of our test problem 
is not regular [32] and the slope 1.8 corresponds to this lack of regularity.

60
4 Numerical Results: Cases Where Q = Q'
Fig. 4.4 Linear elasticity 
problem: convergence in 
l2-norm of the dual residuals 
according to the penalty value
Remark 4.11 (Iterated Exact Augmented Lagrangian) In Sect. 2.3.4 we presented 
a way of using an exact augmented Lagrangian method, instead of a regularised one, 
avoiding the matrix Bt MS-1B which cannot be handled as MS-1 is a full matrix with 
our choice of elements. This implies solving the problem
a b A f 8u A 
[ ru A
B - eMS 
8p = rp.
(4.23)
We then consider the same test case and following Sect. 3.17, we solve by the same 
algorithm which we used for the regularised problem.
As we had discussed in Sect. 2.3.4, one sees in Fig. 4.4 that increasing e 
accelerates the convergence in p. In this test the acceleration does not justify the 
extra iteration in 8u, 8p. we conclude that, at least with the solver employed, this 
method would be useful only if the acceleration of the convergence of p is very 
important. 
■
Number of Iterations and Mesh Size
When the solver in u is LU, the algorithm becomes the standard Uzawa method 
(Sect. 3.2.2). For the problem that we consider, the number of iterations in p is then 
independent of the mesh size as is the condition number of the dual problem (see 
[90]). It is interesting that this property holds even with our iterative solver in u as 
can be seen in Table 4.4

4.2 Application to Incompressible Elasticity
61
Table 4.4 Linear elasticity problem with GCR(3)-HP-AMG as primal solver: global number of 
iterations with v = 0.4 according to the size of the mesh
Number of degrees of freedom
2187
14,739
107,811
823,875
Number of iterations
21
22
22
21
Table 4.5 Elasticity problem: Algorithm 3.7 method. Performance with optimal (automated) 
parameter в
Value of n (CG(n))
1
2
3
4
5
# iterations
662
188
134
130
117
CPU time (s)
128
53
44
52
61
Comparison of the Preconditioners of Sect. 3.2
In the previous computations, we employed a combination of methods which were 
indeed quite efficient. One could wonder if this choice is the best. Obviously we 
will not definitively answer to this question, but we can certainly illustrate the merit 
of specific mixed methods built from Algorithm 3.5.
We first try to establish whether
• the use of Algorithm 3.7 as a solver in itself can be considered and if the GCR 
really accelerates.
• the cost of computing the optimal parameter в in Algorithm 3.10 is worth the 
effort of a second resolution in u.
We thus still consider Algorithm 3.7 but we use three variants.
• Algorithm 3.7 as an iterative solver with Algorithm 3.10 for S',
• Algorithm 3.7as an iterative solver with Algorithm 3.11 for S,
• Algorithm 3.7 using Algorithm 3.10 as a preconditioner for a GCR method.
We want to compare these three variants with respect to numerical behaviour and 
performance on a single mesh (here we chose a mesh of size h = 0.0125).
We first present the result for Algorithm 3.7 with Algorithm 3.10 which implies 
computing в, to the price of an additional solution in u.
We shall use the conjugate gradient CG(n) method, preconditioned by SSOR for 
u. For Algorithm 3.10 we use for MS = M0 and we illustrate its dependency to 
the solver in u by using different number of iterations in CG(n). We take the same 
tolerance as above.
In this case CG(3) yields the best performance (Table 4.5).
We now want to know if computing в is worth the cost. To do so, we take CG(3) 
as the solver in u which was best in the previous test and we replace Algorithm 3.10 
by Algorithm 3.11. This corresponds to the classical Arrow-Hurwicz method 
as described in [43]. Since в is arbitrary, we considered various values. Based 
on Table 4.6 the optimal value for a fixed в is between 8 and9 and the corresponding 
number of iterations is about 400.

62
4 Numerical Results: Cases Where Q = Q'
Table 4.6 Elasticity problem: Algorithm 3.7, number of iterations and CPU time according to the 
value of в
в
1
5
7
8
9
# iterations
Max iter
649
471
414
diverged
CPU time (s)
283
188
127
109
—
Table 4.7 Elasticity problem: performance of GCR’s method preconditioned with Algorithm 3.7
Value of n (CG(n))
1
2
3
4
5
# iterations
117
91
82
98
81
CPU time (s)
29
30
33
47
51
One sees that the optimal в is slightly higher that 8 and that the computing time is 
the double than with a computed в . Moreover, the optimal values has to be guessed.
In the last comparison we use a GCR to accelerate the precedent solver, once 
again different number of iteration are taken for the CG(n).
In Table 4.7 the CPU time is increasing with n. However choosing a value of n 
between 1 and 3 achieves a good reduction of CPU time with respect to the optimal 
value of Table 4.5.
Effect of the Solver in u
The next point that we will address is related to the use of HP solver. We are 
interested in the effect of HP-AMG and HP-LU when employed in the solver in 
u.
In the next numerical tests we use a Mixed-GMP-GCR with different variants 
of Algorithm 3.7 with Algorithm 3.10 using solvers in u based on the HP-AMG or 
HP-LU.
We present in Fig. 4.5 the convergence of the residuals on the finer mesh for 
different solvers in u based on the HP-AMG.
One sees that the better the solver in u, the better the convergence. Furthermore, 
the gain becomes negligible if this solver is good enough while using HP in 
PREONLY mode seems to be rather poor. However the picture is quite different 
if one considers computing time. This is what we present in Table 4.8.
For all three meshes, we present the computing time for the different versions of 
the mixed-GMP-GCR solver. Each row corresponds to the use of a specific version 
of the solver in u. For coarse grids, the direct solver is much more efficient than the 
iterative ones but this advantage rapidly disappears when the mesh gets finer. The 
solvers using HP-AMG are clearly the best for large meshes. The good news is that 
there is little difference as soon as a good enough solver is employed.
Remark 4.12 (GCR or GMRES?) In the previous results, we have chosen GCR(n) 
for the solution in u. One might wonder why this choice. Although mathematically

4.2 Application to Incompressible Elasticity
63
0 
20406080
020406080
Number of global iterations
Number of global iterations
Fig. 4.5 Convergence of the residuals in p and u using a PREONLY(HP-AMG) or GCR(n)-HP- 
AMG with n = 1, .., 5
Table 4.8 Linear elasticity
No. of degress of freedom
problem: CPU time in 
seconds according to the 
-----------------
14,739
107,811
823,875
solver used for the primal 
LU
7. 09
70.41
1903.65
problem and the size of the 
PREONLY
HP-AMG
11. 30
49.43
253.11
mesh. On top, as a reference,
HP-LU
10.21
39.37
325.87
the CPU times using a direct 
GCR(1)
HP-AMG
13. 35
58.57
250.38
solver (LU) for u at each
iteration
HP-LU
11. 39
50.92
350.62
GCR(2)
HP-AMG
10.10
46.92
179.11
HP-LU
8. 92
41.70
306.84
GCR(3)
HP-AMG
9. 23
43.41
164.3
HP-LU
7. 92
39.93
313.00
GCR(4)
HP-AMG
8. 44
38.08
159.78
HP-LU
8. 02
40.71
320.47
GCR(5)
HP-AMG
8.86
37.72
158.69
HP-LU
8. 20
42.41
353.74
GCR is equivalent to GMRES, this could indeed be discussed. To evaluate our 
choice, we have made a comparison with the GMRES(n). As we can see in Fig. 4.6, 
and as predicted, the behaviour is essentially the same, however for such saddle 
point system the GCR seems to benefit from orthogonality of the residuals as we 
observe a slight difference in the number of iterations. As to computing time, we 
still have a slight difference in favor of GCR as we see in Table 4.9 which should be 
compared with the last column of Table 4.8. 
■
All this shows that it is possible to solve correctly and rapidly incompressible 
linear elasticity problems, with a method which is totally automatic and does not 
rely on arbitrary parameters. We now consider the non linear case.

64
4 Numerical Results: Cases Where Q = Q'
Fig. 4.6 Iterations number 
for the Mix-GMP-GCR 
preconditioned with a 
GCR(n)-HP or 
GMRES(n)-HP 
preconditioner
Table 4.9 Elasticity problem with a fine mesh (823 875 dof): CPU time according to the 
preconditionner of GMRES(n)-HP of the primal problem
GMRES(n)
1
2
3
4
5
HP-AMG
32,163
22,051
21,581
21,381
19,980
HP-LU
50,314
39,616
48,920
33,563
50,829
4.2.6 Large Deformation Problems
In this section, we consider the solution of non linear elasticity problems for 
incompressible materials. We thus consider a Newton method which brings us to 
solve a sequence of linearised problems. We take as examples the neo-hookean 
and Money-Rivlin models. As we know, for non-linear elasticity, the coercivity of 
the linearised system (4.17) is not guaranteed. It might indeed fail near bifurcation 
points and this would require techniques [61] which are beyond the scope of our 
presentation. We shall focus on two points.
• The algorithms presented for the linear case are directly amenable to the 
linearised problems which we now consider.
• The stabilising terms are important.
In our numerical tests, we apply the Mixed-GMP-GCR method to the linearised 
problem. The solver in u is GCR(3) prconditioned by HP-AMG.
Neo-Hookean Material
The parameter of the considered material is E = 102 with a Poisson ratio v = 0 
which gives д = 50. We regularize the problem with an artificial bulk modulus

4.2 Application to Incompressible Elasticity
65
102
0
500
600
displ. (u)
pressure (p)
100 
200
300 
400
Iteration number
102
10-5
10-12
700
0 
20 
40 
60 
80 
100 
120 
140 
160
Iteration number
Fig. 4.7 Non linear elasticity problem (Neo-Hookean): convergence in l2-norm of the residuals 
for Poisson ratio 0 = 0 (left) and v = 0.4 (right)
k corresponding to an artificial Poisson ratio v. We can see in Fig. 4.7, how the 
stabilizing term accelerates the convergence of the problem.
In this result, GCR(3)-HP-AMG is used as the solver in u. Each decreasing curve 
corresponds to a Newton iteration. Stabilisation does provide a better non linear 
behaviour. All the parameters of the algorithm are computed automatically.
Mooney-Rivlin Material
The material is rubber and the associated parameters are c1 = 0.7348 et c2 = 
0.08164. With this material, the problem in u might not be coercive and as we solve 
at each Newton step a linearised problem the condition number will depend on the 
solution. In the formulation of (4.19), we introduced a stabilisation term K. This 
stabilisation is indeed necessary.
The behaviour of the material for K = 0 is somewhat paradoxical. When we 
solve in displacement only, we obtain the result on the left of Fig. 4.8. This kind of 
behaviour was also observed in the technical report of ADINA [2].
When we solve the incompressible problem with Mixed-GMP-GCR, we find 
a good solution, presented at the right of Fig. 4.8. However, we got a very slow 
convergence of the algorithm as we see in Fig. 4.9. For the first Newton’s iteration 
all seems well but convergence is slow for the second one.

66
4 Numerical Results: Cases Where Q = Q'
Fig. 4.8 Non linear elasticity problem (Mooney-Rivlin): solution of the problem for K = 0. On 
the left using a displacement only formulation and on the right using a mixed formulation
Fig. 4.9 Non linear elasticity problem in mixed formulation: convergence in l2-norm of the 
residuals for K = 0 for the two first Newton steps
When we stabilise with K > 0 we have a better convergence (see Fig. 4.10). In 
this case an optimal value on K is 5 and is independent of the size of the problem. As 
we could expect, taking K too large has an adverse effect on the condition number 
of the problem in u and the algorithm slows down.
Finally we would like to confirm that the computed parameter в of Algo­
rithm 3.10 is still correct for this non linear problem. In order to do so, we consider 
the same Mooney-Rivlin case as above using K = 10 for stabilisation and tested 
with Algorithm 3.11 using some fixed values of в. Figure4.11 allows us to compare 
the convergence for these fixed values of в with the behaviour when using eopt (that 
is using Algorithm 3.10).
Obviously the choice of fixed values of в was not totally random, ‘educated 
guess’ were involved giving reasonable numerical behaviour. We can see again that 
the computed value of в is nearing the optimal value for this parameter. This, once 
again, justifies the use of Algorithm 3.10 and it also shows that the preconditioner 
is independent of the problem and avoids more or less justified guesses.

4.2 Application to Incompressible Elasticity
67
SI 
100
.S
10-5
’аз 
10-10
*
•K =
-
•K =
*
•K =
*к =
AК =
10
20
50
5
7
0 
20 
40 
60 
80 
100 
120 
140 
160 
180 
200 
220
Iteration number
ajc
<xa
Go:
100
10-5
10-10
*
KK =
A
KK =
A
KK =
A к =
A К =
______
5
7
10
20
50
0 
20 
40 
60 
80 
100 
120 
140 
160 
180 
200 
220
Iteration number
Fig. 4.10 Non linear elasticity problem (Mooney-Rivlin): convergence in l 2 -norm of the residuals 
according to different artificial bulk modulus K
101
0
20
40
60
80
100
120
140
Number of iterations
0
20
40
60
80
100
120
140
Number of iterations
Fig. 4.11 Non linear elasticity problem (Mooney-Rivlin): convergence in l 2 -norm of the residuals 
according to the value of в

68
4 Numerical Results: Cases Where Q = Q'
4.3 Navier-Stokes Equations
We now rapidly consider Navier-Stokes equations for incompressible flow prob­
lems. This will evidently be very sketchy as numerical methods for Navier-Stokes 
equations is the subject of an immense literature. We consider the subject only as it 
is related to the previous Sect. 4.2 but also as it shares some points with Sect. 4.1 for 
the importance of the coercivity condition. This will lead us to present the classical 
projection method in a new setting. This should be seen as exploratory. Projection 
methods have been a major development in the solution methods for the Navier- 
Stokes equations, [29, 85]. We refer to [76] for a discussion of its variants and their 
relation to the type of boundary conditions imposed on the system.
We now present a simple form of the problem. Let be a Lipschitzian domain 
in Rd, d = 2 or 3. Let d& = Гр U Гу the boundary of with Гу = 0, n is the 
exterior normal vector and Qt the open set x (0, T), where T > 0 is the final 
time.
We consider an unsteady flow of an incompressible fluid in which the density 
variations are neglected and we restrict ourselves to a Newtonian fluid, that is with 
a constitutive equations of the form
a = — p I + 2 p.e(u),
where e(u) is defined as in (4.8). We consider homogeneous Dirichlet boundary 
conditions on Гр and a homogeneous Neumann condition on Гу,
u(t, x) = 0 on Гр 
a.(u(t, x), p(t, X)) = 0 on Гу.
We thus define,
V = {V e (H 1 (O))d | v = 0 on Гр}, Q = L2(Q), 
a(u, V) = J s(u) : e(v) dx, 
c(u, v, w) = J u • grad v • wdx.
We also denote
(u, V) = j U • vdx 

4.3 Navier-Stokes Equations
69
and we write the Navier-Stokes equations for the fluid velocity u and pressure p as
(du/dt, v) + c(u, u, v) — 2p.a(u, v) — (p, div v) = (f, v) Vv e V
(div u, q) = 0 
Vq e Q.
(4.24)
Here f represents eventual external volumic forces. System (4.24) is completed 
with the following initial data:
u(0, x) = u0(x) e L2(^)rf with divu0 = 0
Here, we choose a uniform time step 8t and a backward (also called implicit) 
Euler time discretisation. For the spatial discretisation we choose a finite element 
approximation (Vh, Qh) for the velocity and pressure. At time th = h8t < T, 
knowing (uh-1, ph-1) we consider the system
(uh, vh) + 8t c(uh,uh, vh) — 2p8t a(uh, vh)
- 
— 8t(ph. div vh) = (uh-1, vh) + 8t(f, vh) Vvh e Vh
(divuh, qh) = 0 
Vqh e Qh.
(4.25)
Remark 4.13 We present this simple implicit time discretisation to fix ideas. Our 
development is in no way restricted to this example. In (4.25) we have a non linear 
problem for uh for which we can consider a Newton linearisation. One could also 
consider a semi implicit formulation with c(uh~x, uh, vh) instead of c(uh, uh, vh).■
Let us denote p = 8t p. We can write (4.25) in the form,
(uh, vh) — (ph. div vh) + 8t c(uh,uh, vh) — 2p 8t a(uh,vh)
= (uh— ,vh) + 8t(f,vh) 
Vvh e Vh
(divuhh,qh) = 0 
Vqh e Qh.
We now apply he classical projection method. In its simplest form, we first 
compute a predictor uh for uh solution of,
(uh, vh) + 8t c(uh,u_h,vh) — 2p8t a(uh,vh)
= (uh-1, vh) + (p)hh—1, div vh) + 8t(f, vh) Vvh e Vh 
(4.26)

70
4 Numerical Results: Cases Where Q = Q'
then projectUh on the divergence-free subspace by solving formally,
— kbp = div 1Zh
dbp „
-----= 0, on Гd 
dn
bp = 0, on r,y
(4.27)
and finally updating the velocity and pressure
Uh = Uh — grad 5 p 
p k = p k -1 + bp
The basic flaw in this approach is that this projection takes place in H(div, Q) and 
not in H1 (Q). Tangential boundary values are lost. Many possibilities have been 
explores to cure this and we shall propose one below.
In a finite element formulation, the exact meaning of Uhh — grad bp must also be 
precised. We consider a non standard way of doing this.
Referring to our mixed formulation (4.3) and defining buh = uh — Uhh
V0h = {vh | vh ■ n = 0 on yd } ■
(4.28)
The Poisson problem (4.27) can be written as finding buh e V0h, solution of
(bUh, Vh) + (bph divVh) = 0 
Vvh e V0h
(divbUh, qh) = (divUh,, qh)
(4.29)
Vqh e Qh
We have an incompressibility condition and we have to choose a discrete 
formulation satisfying the inf-sup condition. To fix ideas, we may assume the same 
choice as we used earlier for incompressible elasticity (cf. Sect. 4.2.3), that is the 
Taylor-Hood element which is also classical for flow problems. It is not, however, 
suitable for the formulation (4.29). The trouble is with the lack of coercivity on the 
kernel. This has been studied in [25] and the cure has already been introduced : 
we add in (4.29) a stabilising term to the first equation and we now have to find 
buh e V0h solution of
(buh, vh) + a(divbuh — divuh,, div vh)
+ (bph div vh) = 0 Vvh e V0h
(div bUh, qh) = (div Uh, qh) 
V qh e Qh^
(4.30)
With the discretisation considered, this will not yield a fully consistent aug­
mented Lagrangian and the stabilising parameter will have to be kept small. We are 

4.3 Navier-Stokes Equations
71
fortunate: our experiments of Sect. 4.1 show that even a small value will be enough 
to get a good convergence.
Remark 4.14 (Why This Strange Mixed Form) Using the mixed form (4.30) makes 
Uhh and grad ph to be in the same finite element space, which is a nice property.
We recall that the normal condition 8uh ■ n = 0 must be imposed explicitly on 
Гр in either (4.29) or (4.30). If no condition is imposed on 8uh ■ n, one then imposes 
8 pih = 0 in a weak form.
■
Since we have no control on the tangential part of grad ph on Гр this method 
potentially leaves us with a tangential boundary conditions which is not satisfied. 
The simplest remedy would be an iteration on the projection. To fix ideas, we shall 
use the semi-implicit problem
(Uh,Vh) + 8tc(Uh 1 ,uh,vh) - 2^8ta(u_h,vh)
- (Phh-divVh) = (uh-,Vh) + 8t(f,Vh) 
VVh e Vh 
(4.31)
(divuhh,qh) = 0 
Vqh e Qh.
One could then, given an initial ph,
1. Solve the first equation of (
)
4.31
2. Project the solution by (
).
4.30
3. Update pk + 8 pih.
4. Repeat until convergence.
From this, one can think of many possibilities using approximate solutions and 
imbedding the procedure, for example, in a GCR method. In all cases, solving (4.30) 
will require a mixed iteration. and this leads us to another approach.
4.3.1 A Direct Iteration: Regularising the Problem
Finally we consider the direct use of the same Algorithms 3.7 and Algorithm 3.10 
that we used for incompressible elasticity. To get a better coercivity we had the 
regularised penalty terms and we change the first equation of (4.25) into
(uh, Vh)+a(div uh, div vh) + 8t c(uh,uh, Vh) — 2p8t a(uh, Vh)
- 8t(ph. div Vh) = (uhT1 ,Vh) + 8t(f,Vh) 
VVh e Vh
(4.32)
This is a non linear problem which should be linearised. We can then apply the 
Mixed-GMP-GCR method to the resulting linearised form.

72
4 Numerical Results: Cases Where Q = Q'
4.3.2 A Toy Problem
In order to show that this technique is feasible, we consider a very simple example. 
We consider =]0, 1[x]0, 1[ and an artificial solution,
u(t, x, y) = (y2(1 + t), x2(1 + t))
with a viscosity’s value equal to 10-2, the source force associated is
f(t, x, y) = (0.98 (1 + t) + y2 + 2 x2 (1 + t)2 y, 
- 1. 02 (1 + t) + x2 + 2 y2 (1 + t)2 x )
Results for the method (4.32) are presented in Table 4.10, which represents the 
global number of iterations and CPU time in seconds to reach T = 1. This is done 
for different values of the time step St and the regularity coefficient a. The table 
shows the interest of the stabilisation parameter a, which can reduce the number 
of global iterations and CPU time. The optimal value of the stabilisation parameter 
is also stable with respect to the time step. We used an iterative solver (precisely 
CG(10)) for solving the problem in u, a choice which could clearly be improved. 
The mesh is composed of 3200 triangles and 6561 degrees of freedom.
The following figures show the convergence for the problem with a = 0.1 and 
St = 10-1. Figure 4.12 presents the convergence of both primal and dual residuals 
for Newton iterations ofa single arbitrary time step (here the fifth one corresponding 
to t = 0.5).
Table 4.10 Navier-Stokes problem: iteration’s number and CPU time in second according to the 
step time (St) and the regularity coefficient (a)
a
St = 0.05
#it.
CPU (s)
St = 0.1
#it.
CPU (s)
St = 0.25
#it.
CPU (s)
0
13,593
945
6701
475
3542
254
1 x 10-2
10,567
739
5167
366
2765
178
1 x 10-1
5709
412
3203
233
2231
161
2.5 x 10-1
5440
396
3756
267
3150
221
1
8315
580
6767
468
6033
441

4.3 Navier-Stokes Equations
73
» primal
□ dual
Residual
10
10 -12
1,250 
1,300 
1,350 
1,400 
1,450 
1,500
Number of cumulative iterations
Fig. 4.12 Navier-Stokes problem: convergence curves for the primal and dual residuals for t = 0.5
The same behaviour repeats at every time step.
This is a very simple case. Nevertheless, we may think that this direct approach 
is feasible. Clearly, a better solver in u is needed and it is thinkable, following [26] 
to include the projection method in the preconditioner.

Chapter 5
Contact Problems: A Case
Check for 
updates
Where Q = Q'
This chapter presents solution methods for the sliding contact. We shall first develop 
some issues related to functional spaces: indeed, in contact problems, we have a case 
where the space of multipliers is not identified to its dual. To address this, we first 
consider the case where a Dirichlet boundary condition is imposed by a Lagrange 
multiplier and present the classical obstacle problem as a simplified model for the 
contact problem. We shall then give a description of the contact problem and its 
discretisation with a numerical example.
5.1 Imposing Dirichlet’s Condition Through a Multiplier
We temporarily put aside contact problem to consider a simple Dirichlet problem 
which will nevertheless enable us to address some issues relative to contact 
problems.
We thus consider a domain of boundary Г. We divide Г into two parts Г о 
where null Dirichlet are imposed and Гс where we have a non homogeneous 
Dirichlet condition which will provide a simplified model of the contact region. 
We then want to solve
- Аи = f 
u =о
u =g
in
on Г0 
on Гс
(5.1)
We define the space V = {v |v e H1 (Q), v|г0 = 0}. We suppose that f e V' and 
1/2 
1/2
g e H00 (ГD). Following Sect. 2.1.5, we take Л = H00 (Гс) and the operator B 
1/2
is the trace of V in Л on Гс. One should note that we take g e H00 (Гс) to make 
our presentation simpler but that this is not essential.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_5
75

76
5 Contact Problems: A Case Where Q = Q'
Remark 5.1 (Change of Notation) In the following, we denote Л instead of Q the 
space of multipliers. 
■
1/2
Remark5.2 (Sobolev Spaces) The space H00 (Гс) was introduced in [62]. The 
elements of this space are in a weak sense null at the boundary of Гс and thus match 
with the zero boundary conditions on Г0. The dual of H002(Гс) is H-122(Гс). The 
1/2
scalar product on H00 (Гс) is usually defined by an interpolation norm which also 
defines a Ritz operator R from H0102(Гс) onto H-122(Гс). We shall consider later 
other possibilities to define the scalar product. 
■
1/2
Let then Bv be the trace of v in H00 (Гс). To write our problem in mixed form, we 
define
a(u, v) = ^ grad u • grad vdx 
(5.2)
b(v, ^) = (Bv, ^)H 122 
= {Bv, R.}H122(r )xH-1 22(Гг)
H00 U C) 
H00 VC)xH ' u о
1/2
To simplify the notation we write (к, ^) 1 22 the scalar product in H00 . We want to 
find u e V, к e Л solution of
a(u, v) + b(v, к) = (f, v) Vv e V, 
b(u, ^) = (g, ^) 1 22 
V^ e Л.
(5.3)
This problem is well posed. Indeed the bilinear form a(u, v) is coercive and we 
have an inf-sup condition. To show this, we use the fact that there exists a continuous 
lifting L from H01022 (Гс) into V. Denoting vk = Lk, we have
| v. | v < C |к | л
sup 
v
b(v,к) 
| v | V
> b(v., к)
|v.|V
- C 1 к । к.
1/2
Remark 5.3 (This May Seem a Non Standard Formulation!) Taking к e H00 (Гс) 
may seem a little strange. Indeed, a more standard formulation would define
b(v, ^ ) = (Bv, ^ )h01022(Гс)xh-1 22(Гс)
In fact the two formulations are equivalent as we have
(Bu, Ц, )h122(pc)xh-1 22(Гс) 
(Bu, R ^ ) 1 22.

5.1 Imposing Dirichlet’s Condition Through a Multiplier
77
where R is the Ritz operator on Л. We therefore have the choice of working with X 
or with Xand this choice will be dictated by numerical considerations. 
■
We may then introduce the operator B = RB from V onto Л' and we have
{Bu, X)л'Xл = b(v, X)
Remark 5.4 We have considered a case where the choice of Л and Л' is simple. In 
more realistic situations Л is a subspace of H1 /2(Гс) corresponding to the trace of 
the elements of a subspace of H1 (Q). The space Л' is then the dual of Л which in 
general will contain boundary terms. 
■
As a simple numerical procedure to solve (5.3) we could use Algorithm 3.7. A 
central point of this algorithm would be to compute,
zx = S-1 rx = s~1 (Bu - g).
It will thus be important to have a good approximation of the operator S-1. To better 
understand the issue, we have to analyse more closely the spaces considered here.
5.1.1 H01/2(Гс) and its dual H-1 /2(Гс)
We rapidly present some results that should help to understand the spaces with 
which we have to work. In the present case, R is the Ritz operator from Л into 
Л', that is from H010/2(Гр) onto H—1 /2(Гр). This corresponds to the Dirichlet- 
Neumann Steklov-Poncare operator which we shall consider in more detail below.
1/2 
2
The classical way to define H00 (Гс) is through an interpolation between L2 
and H01. There has been in the last years many developments to define fractional 
order Sobolev spaces in term of fractional order derivatives. We refer to [31] for a 
general presentation. We show informally how this could bring some insight about 
our problem. If we suppose to simplify that Гс lies in a plane, the elements of 
Hn-1
00 (i с) can be prolonged by zero as elements of i±00 () and we can then 
introduce on Гс a fractional order gradient of order 1 /2 denoted grad 1 /2. It must 
be noted that fractional derivatives are not local operators.
We can define for c Rn and Гс C Rn-1
H010/2(Гс) = {v |v e L2(Гс), grad 1 /2v e (L2(Гс))п-1}
1/2
We can also define on H00 (Гс) a norm
I v 11 /2 = 
IvI dx + 
Igrad 1 /2V|2 dx 
(5.4)
Гс 
Гс
This norm depends on a fractional tangential derivative.

78
5 Contact Problems: A Case Where Q = Q'
Remark 5.5 (Discrete Interpolation Norms) This should be made in relation to 
discrete interpolation norms [5], which involve taking the square root of the matrix 
associated to a discrete Laplace-Beltrami operator on the boundary. This is an 
operation which might become quite costly for large problems. One should also 
recall the work of [44]. 
■
We can then identify the dual of H0/ with L2 (Гс) x (L2(Гс))"-1 and the 
duality product as
(X, и) = I к о vdx + I X 1 • grad 1 /2 v dx 
'сс Jrc
with X0 e L2(Гс) and X 1 e (L2(Гс))"-1. As regular functions are dense in 
H(102(Гс), we could say that H-1 /2(Гс) is the sum
X о + div1 /2 X 1
where div1/2 is a fractional order divergence operator.
5.1.2 A Steklov-Poincare operator
Another important component of our problem is the Dirichlet to Neumann Steklov- 
Poincare operator. Essentially, the idea is to associate to a Dirichlet condition the 
corresponding Neumann condition. This is a wide subject and we present the simple 
case associated with our model problem. We refer to [77] for the discussion of 
domain decomposition methods.
1/2 
1/2
Given r e H0( (Гс), we first use the continuous lifting from H0( (Гс) into V 
and find Ur such that Bur = r on Гс. We then solve for uо e H01 (Q)
a(uо, vо) = -a(ur,vо), 
Vvо e H01 (Q).
and setting Фг = uо + u, we can then define X' e H-1 /2(Гс) by
<X', Bv)л'xл = a(#r,v) Vv e H1 (Q)
Remark5.6 Taking v = u, {X', Bu)л'xл is a norm on HОО2(Гс). 
■

5.1 Imposing Dirichlet’s Condition Through a Multiplier
79
Using This as a Solver
To solve the Dirichlet problem we can now build a Neumann problem using the SP 
operator. Assuming we have some initial guess A0 we shall first solve the Neumann 
problem
a(uN,v) = {f,v) + (v,A0)лxл' Vv e V.
We then set r = g - BuN on Гс, and get A' = A0 + SPr.
Solving the Dirichlet problem (5.1) is then equivalent to solving the Neumann 
problem
a(u,v) + (Bv,A')лxл' = <f, v) Vv e V. 
(5.5)
The condition Bu = g on Гс can be written as (Bu - g, Bv)a = 0 which can also 
be written as
<Bu - g, SP v}лxл' = <Bu — g,A'}лxл'= 0 V^'e Л'. 
(5.6)
Remark 5.7 We have thus written our problem in the form (5.3). One could argue 
that we did nothing as the Steklov-Poincare operator implies the solution of a 
Dirichlet’s problem. This will become useful whenever the A' has an importance 
by itself. This will be the case in a similar formulation of the contact problem where 
A is the physically important contact pressure. 
■
5.1.3 Discrete Problems
The direct way of discretising problem (5.3) would be to consider a space Vh с V 
and to take for Ah a subspace of the space of the traces of Vh on Гс. We then have 
Bhvn = Phh Bvh .We define
b(vh, Ah) = (Bhvh, Ah)1/2,h,
where (Ah, ^h) 1 / 2 ,h is some discrete norm in Ah .We look for uh and Ah solution of
a(uh, vh) + b(vh, Ah) = (f, vh) Vvh e Vh, 
b(uh, Ah) = (gh, Ah) 1 /2,h 
V^h e Ah,
(5.7)
where we may take gh the interpolate of g in Ah. More generally gh might be the 
projection of g on Ah in (•, •) 1 /2,h norm.

80
5 Contact Problems: A Case Where Q = Q'
The Matrix Form and the Discrete Schur Complement
The scalar product defines a discrete Ritz operator Rh from Ah onto Лh. We have,
(Bhvh, Ah) 1 /2,h = {RhBhvh, №h)ЛhxAh = (Bhvh, Ah)ЛhxAh
In the framework of Sect. 2.2.2 and Remark 2.5 we can associate to uh and Ah their 
coordinates u and A on given bases.
R will the matrix associated to Rh that is
(RA, a) = (Ah, Ah) 1 /2,h
We then define the matrix B
{B u, A} = (Bhuh, Ah) 1 /2,h
• We emphasise that these matrices depend on the discrete scalar product.
The Schur complement BA-1B‘ operates from Ah onto Ah and can be read as
1. Solve the Neumann problem a(uh, vh) = (Ah, Bhvh)1/2,h.
2. Take the trace Bhuh and compute Ah = RhBhuh. 
■
This is usable if the bilinear form b(vh, Ah), that is the discrete scalar product in 
H010/2 is easily computable.
Remark 5.8 We can also write the discrete problem using the discrete form of (5.5) 
and (5.6). We then look for uh e Vh and Ah in Лh solution of
a(uh, vh) + (vh, Ah)Ahxлh = (f, vh) vvh e Vh, 
(Bhuh - gh, Ah)Ahxлh = 0 
^Ah e Ah.
As in the continuous case, we can write the problem using Ah e Ah or Ah e Лh. 
The two formulations are equivalent for the equality condition u = g but this will 
not be the case for inequality condition u > g. 
■
We shall first rely, on a discrete Steklov-Poncare operator which will enable us 
to define a ‘perfect’ discrete scalar product in Ah.
5.1.4 A Discrete Steklov-Poincare Operator
We thus take as Ah a subspace of the traces of functions of Vh on Гс and we write 
Bhvh the projection of Bvh on Ah.

5.1 Imposing Dirichlet’s Condition Through a Multiplier
81
We shall also denote the matrix B associated to Bh., the same notation as the 
continuous operator !B as they are used in a different context.
The goal is to associate to an element rh e Ah an element кh in Лh. To do so, 
we first build ФЬг a function of Vh such that Bhфhr = rh and we solve
аф0, vh0) = -a($hr, vh0) Vvh0
where vh0 = 0 on Гс and the bilinear form is as in (5.2). Let Фь = Фь0 + ФЬг. We 
now define SPhrh = кh, an element of Лh by
{Bhvh,kh)Ahxл^ = a($h,vh) Vvh e Vh. 
(5.8)
This would enable us to solve the problem (5.7) just as we had done for the 
continuous case:
• Given к0h solve the Neumann problem
a(uh, vh) = (f, vh) + (vh, к0h ) Vvh e Vh.
• Take rh = Buh - gh and compute кh = к0h + SPhrh
• Solve
a(uh,vh) = (f,vh) + {кh,vh) Vvh e Vh. 
(5.9)
What we have done here is to define the discrete scalar
^h, ^h) 1 / 2 ,h = (SPhkh, ^h ) л h x Ah.
Again, this is somehow tautological: we solve a Dirichlet problem by solving a 
Dirichlet problem. The gain is that we also have кh which has in contact problems a 
physical significance. If we write u and к' the vectors asociated with uh and кh, we 
can introduce a matrix SP and write (5.1.4) as
(к, ^h) 1 /2,h = (5Рк, ^}. 
(5.10)
We shall consider later simplified formulations. It is worth, however to give a 
look at the computation of the discrete Steklov-Poincare operator.
5.1.5 Computational Issues, Approximate Scalar Product
We first note that to compute SPhrh as in (5.8), we need only to compute а(фь, vh 
for all vih associated to a node i on Гс as in Fig. 5.1.

82
5 Contact Problems: A Case Where Q = Q'
Fig. 5.1 Node of the 
boundary
It is also interesting to see what this result looks like. To fix ideas, let us consider 
a piecewise linear approximation for Vh .
Referring to Fig. 5.1, we obtain at node i
а(Ф, vih) = hxhy [ (grad уф (-1 /hy) + grad хФ gradx vih ]. 
(5.11)
The first term can be read as
hx grad у Ф ^ [ дфУ ds. 
y 
Гс dn
But this is not all: the second term depends on the tangential derivative of фг near 
the boundary, and could be thought as a half order derivative of the boundary value 
r . We have
(ri+1 - 2ri + ri-1) 
hy--------- hx-----------
It is interesting to compare this to (5.4). This suggests to define a discrete scalar
1/2
product to approximate the H00 (Гс) scalar product
(kh,^h)h = 
kh^hds + h 
gradkh grad^hds. 
(5.12)
гс 
Гс
Formulas more or less similar to this are often advocated. 
■
Remark 5.9 (A Matrix Representation of SPh)
In Sect. 5.1.4 we have introduced a matrix representing SPh We could also define 
this matrix by computing ф^ for every uih on Гс, computing
SPij = a(&h, <j)
As noted above this reduces to a computation near the boundary.
There is a cost to this: one must solve a Dirichlet problem for every nodal value.
Whether this is worth the effort would depend on each problem and the number of

5.1 Imposing Dirichlet’s Condition Through a Multiplier
83
cases where this matrix form would be used and the cleverness with which we solve 
the Dirichlet problems. 
■
Simplified Forms of the SPh Operator and Preconditioning
We shall ultimately solve our discrete problem (5.7) by Algorithm 3.7 with a suitable 
preconditioner. In Algorithm 3.10, our standard preconditioner, we compute an 
approximation of the Schur complement BA-B and approximate its inverse by 
MS. In this case, the iteration is don in A.
One can see SPh as a representation of the Schur complement. To employ it in 
Algorithm 3.7, one could rely on an approximation SPh
• One could think of building SPh on a subdomain around ГC and not on the 
whole ^.
• The computation of SPh could also be done using a simpler problem, for 
instance using a Laplace operator instead of an elasticity operator.
• The Dirichlet problem defining SPh could be solved only approximately with A.
One should now modify Algorithm 3.10 as we are now iterating in A' and not in 
A .We must also note that we have an approximation SP of the Schur complement 
S.
Algorithm 5.1 A preconditioner using SPh
1: Initialization
• ru, rA given ,
• 
Zu = A-1 Ги
• 
Ги = Ги_++ Bzu
• z A = SP~ru
• zzu = A-1 = B(A')
• Tu = Bzzu
• в '
< Tu, Tu )
2: Final computation
• z a = ezA
• 
Zu = Zu - PzZu
3: End
■
The real efficiency of using a SPh operator is to be tested. We leave this as an 
open point.

84
5 Contact Problems: A Case Where Q = Q'
5.1.6 The L2(Гс) Formulation
A very common choice is to take as a scalar product
(kh,^h) 1 /2,h = 
k^ds 
(5.13)
rC
In this formulation, although kh e L2(Гс), it is in fact a representation of k'h. We 
must expect a weak convergence in H-1 /2(Гс).
Formally, this also means that, using (5.13) we shall have an inf-sup condition 
(2.19) in L2 with eh = O(h1 /2).
The Choice of Л*
We have considered the case where Ah is the trace of Vh. We could also take a 
subspace of the traces. A simple and important example would be to have piecewise 
quadratic elements for Vh and a piecewise linear subspace for Ah. We refer to [24] 
for an analysis of the inf-sup condition and the choice of spaces. Why would we 
do this? Essentially because of в in the inf-sup condition: A larger Ah means a 
smaller eh. This is a standard point in the analysis of mixed methods, augmenting 
the space of multipliers makes the inf-sup condition harder to satisfy. We then have 
two consequences.
• As kh is a representation of k'h which converges in H-1 /2, a richer Л'h will 
produce an oscillatory looking kh .
• A smaller в will mean a slower convergence of the solver.
If we take a reduced Ah it must be noted that the solution is changed as BhUh = 
gh means
P^h(Buh - gh) = 0
which is weaker than Buh = gh .
5.1.7 A Toy Model for the Contact Problem
Introducing a multiplier to manage a Dirichlet condition was a first step in exploring 
some of the technicalities related to contact problems. Another issue will arise when 
considering contact problems. We shall have to introduce positivity constraints. To 
illustrate this, we consider an obstacle problem in which the Dirichlet condition
Bu = g 
on Гс 

5.1 Imposing Dirichlet’s Condition Through a Multiplier
85
is replaced by an inequation Bu > g on Гс . For the continuous problem, we use 
Л = H00/2(Гс) and
b(v, д) = (Bv, д) 1 /2 = (RBv, д)л'xx = (Bv, д)л'xx.
We can then consider in Л, the cone Л + of almost everywhere positive д + and on 
Л'
Л+ = {д+/1 (д+/, д+)л-xx > 0 Vд+ e Л +} 
(5.14)
We thus solve for u e V, X + e Л+ solution of
a(u, v) + b(v, X+) = (f, v) 
Vv e V,
b(u, д+- X+) > (д,д+- X+) 1 /2 Vд +e Л+.
The solution satisfies the Kuhn-Tucker conditions.
X +e Л+, 
(Bu - g, д+) 1 /2 > 0 Vд +e Л+, 
(Bu - g,X+) 1 /2 = 0.
The second condition can be read as
<R(Bu - д),д +)л'x.X > 0 Vд+ 
(5.15)
We have thus imposed the weak condition
(Bu - Rg) > 0 in Л'. 
(5.16)
The Discrete Formulation
To get a discrete version, we have to choose Vh and Ah and we must give a sense to 
Xh > 0.
For the choice of Vh, we take a standard approximation of H1 (Q). For Ah we 
take the traces of Vh or more generally a subspace. For example, if Vh is made of 
quadratic functions, we can take Ah the piecewise quadratic traces or a piecewise 
linear subspace.
To define Л+ the obvious choice is to ask for nodal values to be positive. As 
we can see in Fig. 5.2 this works well for piecewise linear approximation. For 
a piecewise quadratic approximation one sees that positivity at the nodes does 
not yield positivity everywhere. Piecewise linear approximations are thus more 
attractive even if this is not mandatory.

86
5 Contact Problems: A Case Where Q = Q'
Fig. 5.2 Positivity of A
As in the continuous case, the definition of positivity on Kh also defines positivity 
on Лh
Ah > 0 ^ л (Ah , ^lh} д^ x дй > 0 V^h > 0
Let (Ah, ^h) 1 /2,h be a scalar product on Ah. We then define b(vh,^h) = 
(Bhvh, M) 1 /2,h and we solve,
a(uh,vh) + b(vh,Ah) = (f,vh) 
Vvh e Vh,
b(uh, ^h — Ah) > (gh, ^h — Ah) 1 /2,h V^h e Ah.
If we denote ^+ > 0 the elements of Ah positive at the nodes and write rh = 
Bhuh - gh the discrete Kuhn-Tucker conditions are
' A+ e Л+
■ (rh,^+) 1 /2,h > 0 Vp+
. (rh, Ah ) 1 /2,h = 0
Referring to Sect. 5.1.3, this can be written in terms of nodal values,
(Br, ^+) > 0 V^+ 
(Br, A+) = 0.
This means that r is positive in some average sense.
The Active Set Strategy
In Sect. 5.2 we shall us the active set strategy [3, 54, 55] which we already discussed 
in Sect. 3.1.3
This is an iterative procedure, determining the zone where the equality condition 
u = g must be imposed. We define the contact status dividing Гс in two parts.
• Active zone : Ah > 0orAh = 0 and Brh < 0
• Inactive zone : Ah = 0 and Brh > 0

5.2 Sliding Contact
87
Remark 5.10 (A Sequence of Unconstrained Problems) Once the contact status is 
determined, one solves on the active zone equality constraints Brh = 0 using 
algorithms Algorithm 3.7 with Algorithm 3.10. One could also, as in Sect. 5.1.4, 
take an approximate Steklov-Poincare operator on the active zone in Algorithm 5.1. 
We shall discuss this in more detail in Remark 5.13.
The active zone is checked during the iteration and if it changes, the iterative process 
is reinitialised.
We shall illustrate below this procedure for contact problems.
5.2 Sliding Contact
We consider, for example as in Fig. 5.3, an elastic body in contact with a rigid 
surface. We shall restrict ourselves to the case of frictionless contact as the frictional 
case would need a much more complex development. We refer to [34] for a more 
general presentation. We thus look for a displacement u minimising some elasticity 
potential J(v) under suitable boundary conditions. In the case of linear elasticity, 
we would have
J(v) = № У |e(v) |2 dx — У f • V dx.
Here we take v e V c (H1 (^))3, the space of displacements. In the following 
we denote,
• Гс is a part of the boundary where contact is possible.
• B is the trace operator of the elements of V in Л c (H1 /2(Гс))3.
Fig. 5.3 Contact with a horizontal rigid plane. An elastic body Q submitted to a displacement u. 
Illustration of the oriented distance computed on the potential contact surface Г с

88
5 Contact Problems: A Case Where Q = Q'
• R is the Ritz operator from Л onto Л'.
• B = RB is an operator from V into Л'.
The basic quantity for the contact problem is the oriented distance, computed by 
projecting the elastic body on a target surface. The oriented distance to this target 
is defined as negative in case of penetration and this projection implicitly define the 
normal vector n. Since u is a displacement of the body 12, the oriented distance is in 
fact a function of u, we will express this dependency by denoting it d(u).
The distance d(u) is a function on the boundary which belongs to a functional 
space which we assume to be a subspace Л of H1 /2 (Гс). If the boundary is not 
smooth (e.g. Lipschitzian), we have to introduce a space defined on each smooth 
part. We place ourselves in the setting of Sect. 2.1.5. In this context Bv would 
be d(v) but in general d(v) is non linear so that we will need a linearisation to 
apply our algorithms. The first (non linear) condition to be respected is then the non 
penetration, that is
d(u) > 0 on Гс
and we can write the first step of our model as,
inf J(v).
d(v) >0
Contact Pressure
The next step is to introduce a Lagrange multiplier X e Л for the constraint: the 
contact pressure [35, 59]. We thus transform our minimisation problem into the 
saddle-point problem,
inf sup J(v) + (kn, d(v))A.
V Xn >0
The optimality conditions are then
(A(u)u - f,v) + (Xn,v • n)A = 0 
Vv
(d(u), ^n - Xn)A > 0
Vi±n > 0.
where the operator A(u) represent the constitutive law of the material. From this 
system we deduce the Kuhn-Tucker conditions,
Xn > 0,
(d(u), д)А > 0 Vд > 0, 
(d(u), kn)A = 0.

5.2 Sliding Contact
89
Newton’s Method, Sequential Quadratic Programming
It must be noted that even in the case of linear elasticity, the problem is non linear. 
To solve this problem, we apply the Newton method or in this context the sequential 
quadratic programming (SQP) method [58]. Let u0 be some initial value of u and 
дП0 = d(u°) the corresponding initial gap. We recall [33] that the derivative of the 
distance function is given by
d' (u0) • &u = &u • n
The linearised problem is then,
{A'(u0)8u,v) + (kn,v • n)A = {f - A(u0),v) Vv e V
(фп — kn, gn — 8u • n)A > 0
(5.17)
Vфп > 0
Here 8u is the correction of the initial value u0, and we have linearised both the 
constitutive law of the material represented by the operator A and the distance 
function. The Kuhn Tucker conditions then become
kn > 0,
(g0 - SU • n, ^n)A > 0 V^n > 0,
(gn - Su • n, kn)A = 0•
Remark 5.11 (The Choice of Multipliers) In the above formulation, we have 
worked with the bilinear form
b(v, kn) = (v • n, kn)A
that is with the scalar product in Л c H1 /2(Гс). As we have seen in Remark 5.3 
we can also write (5.17) with
(k', v • n) = {Rk, v • n}
We recall that this is the same formulation written in two different ways. ■
5.2.1 The Discrete Contact Problem
In the previous Sect. 5.2 we developed a formulation of sliding contact using a 
Lagrange multiplier. We have presented a simpler problem in Sect. 5.1. We now 
come to the contact problem. We suppose that we have chosen a space Vh to 

90
5 Contact Problems: A Case Where Q = Q'
discretise displacements and a space Ah с Л. The choice of these spaces must 
of course be done to satisfy some requirements.
• The material will often be incompressible : we shall consider this case in 
Chap. 6. This imposes some restrictions on the choice of Vh and of the associated 
pressure space Qh . We refer to [20] for details. In practice, for three-dimensional 
problems, we use as in Sect. 4.2 the Taylor-Hood approximation, that is piecewise 
quadratic elements for Vh and piecewise linear for Qh, but this is one of many 
possibilities.
• The problem (
) is a (constrained) saddle-point problem. This means that the 
choice of Ah requires an inf-sup condition to be satisfied, [13]. In the following, 
we use piecewise linear elements for Ah.
5.17
Remark 5.12 (Scalar Product on Ah) We also have to define a scalar product on 
Ah. Ideally this should be the scalar product in Л .As we have discussed earlier the 
scalar product in H 1/2 is not easily computed and we shall have to employ some 
approximation. In general we consider as in Section 5.1.5 a discrete H1/2 norm 
defined by a discrete scalar product (Xh, Ah) 1 /2,h.
We thus have an operator Rh from Ah onto Лh
<RhXh, Ah)ЛhxAh = (Xh, Ah) 1 /2h
and the associated matrix R. In our computations we shall rely on Algorithm 3.10. 
An important issue is the choice of MS and its inverse MS-1. the normal choice is 
here MS = R. From Proposition 2.2, to obtain a convergence independent of the 
mesh size, R should define on Ah a scalar product coherent with the scalar product 
ofH1/2 in order to have an inf-sup condition independent of h.
We thus have to make a compromise: a better R yields a better convergence but 
may be harder to compute.
In the results presented below, we use the simple approximation by the L2 scalar 
product in Ah and the matrix R becomes M0 defined by,
{MоX,a) = j bhAhds- 
(5.18)
We could also employ a diagonal approximation MD to M0. This can be obtained by 
replacing (5.18) by a Newton-Cotes quadrature using only the nodes of Ah. In the 
case of piecewise linear elements, this is equivalent to using a ‘lumped’(diagonal) 
matrix obtained by summing the rows of M. 
■
The geometrical nodes of the mesh which support degrees of freedom for Xh 
and u are not identical. To emphasise this difference we denote by xi the nodes 
supporting the values of Xh e Ah and yj the nodes for uh. As in Remark 2.5, we 
now denote u and X the vectors of nodal values, whenever no ambiguity arises. We

5.2 Sliding Contact
91
thus have
u = {Uhy), 1 < j < NV}
A = {kh(xi), 1 < i < NA}
Denoting by (•, •} the scalar product in RNV or RNA, we thus have the matrices 
associated with operators Rh and Bh
(Rk, ^) = (kh,^h) 1 /2,h, R e RNAxNA
{Bu, i±) = (Bhuh,^h) 1 /2,h, B e RNAxNV
and then R-1 B is the matrix associated with the projection of Buh on Ah.
• These matrices depend on the choice of the discrete scalar product.
We also suppose that we have a normal ni defined at xi and we define for kh its 
normal component such that at all nodes,
knh(xi) = kh(xi) • ni
which is a scalar. We also use its vectorial version
knh(Xi) = kn(xi)ni 
(5.19)
We denote by A„h the subset of Ah of normal vectors of the form (5.19) and
^+h = {^nh e ^nh | kn(x.i) > 0}
We want to have as in Sect. 5.1.7 a discrete form of the condition u • n > 0. Recall 
that Bu is a vector corresponding to an element of Лh. We consider at node xi its 
normal component
BnU = {(Bu)i • ni}
We then have the condition
(Bnu,£) > 0 ^^+.
which is a weak form of u • n > 0 .
We can now introduce our discrete contact problem. We first consider the 
unconstrained case, Bnu = g where we do not have to consider a positivity 
condition. Indeed our final algorithm will solve a sequence of such unconstrained

92
5 Contact Problems: A Case Where Q = Q'
problems, we thus look for the solution of
infsup 1 (Av, v) + (Rkn, (R 1 Bv - g)) - {f, v) 
v kn 2
(5.20)
that is also
1
infsup- (Av, v) + (kn, (BnV - Rg)) - (f,v). 
(5.21)
v kn 2
This problem can clearly be solved by the algorithms of Chap. 3. We must however 
introduce a way to handle the inequality constraint. To do this we first need the 
notion of contact status.
Contact Status
Let rn = Bnu - g the normal residual. A basic tool in the algorithms which follow, 
will be the contact status. It will be represented point wise by the operator P(k, rn) 
defined by,
ifkn = 0,
(1) ifrn < 0then P(k,rn) = 0
(2) ifrn > 0then P(k,rn) = rn 
ifkn > 0,
(3 )P(k,rn) = rn
The Kuhn-Tucker optimality conditions can then written as P(k, rn) = 0. We say 
that in case (1), the constraints are inactive and that on (2) and (3), they are active.We 
denote ГCA subset of TC where the constraints are active.
5.2.2 The Algorithm for Sliding Contact
We can now present our solution strategy for the sliding contact problem.
A Newton Method
We have a nonlinear problem. As we have seen in Sect. 5.2, we rely on a Newton 
method in which the distance is linearised.

5.2 Sliding Contact
93
The Active Set Strategy
The algorithm proposed here is based on the active set strategy
• Given (u0, k0), compute P(rП, k0), that is the contact status.
• Solve (on the active set) an equality constraint problem.
<A'(u0)8u, v) + (kn,v) = {f, v) Vv 
{8u • n - g0,фп} = 0 Vфп
(5.22)
n
• Check the contact status after a few iterations.
- If the status has changed: project k on the admissible set Ah+; recompute the 
active and inactive sets; restart.
- If the contact status does not change, we continue the iteration.
• Iterate till those sets are stabilised.
The contact status is thus checked during the iterations to avoid continuing to solve 
a problem which is no longer relevant.
Remark 5.13 (Ms and the Active Zone) The active set strategy solves a sequence 
of problems with equality constraints on a subset rcA of Гс where the constraints 
are actually imposed. This will mean in Algorithm 3.10 to restrict MS to rcA.
Let us first consider the case where we would take MS = I. One would then 
have a correction zk on kn given by zk = rn . If we do not want to modify kn on 
the inactive zone we should make rn = 0 on inactive nodes. This is analogue to 
the classical conjugate projected gradient method [69]. The same is true if one has 
MS = MD where MDis the diagonal matrix introduced in Remark 5.12.
In general we can proceed by,
• Make rn = 0 outside the active zone TCA. Solve MSz = rn imposing z = 0 
outside of rcA.
If one uses the diagonal matrix MD, this is automatically taken into account to 
the price of a slower convergence.
A similar procedure should be used if one introduces a discrete Steklov-Poincare 
operator. 
■
5.2.3 A Numerical Example of Contact Problem
To illustrate the behaviour of the algorithm we present the case of an elastic cube 
being forced on a rigid sphere (see Fig. 5.4). In this case a displacement is imposed 
on the upper surface of the cube. We want to illustrate the behaviour of the algorithm 
as the mesh gets refined (see Table 5.1).

94
5 Contact Problems: A Case Where Q = Q'
Fig. 5.4 Contact pressure 
over the potential contact area
Table 5.1 Iteration’s number and CPU time according to the total number of degrees of freedom 
(dof) of u and the solving method for the primal system
14,739 dof
107,811 dof
823,875 dof
#it.
CPU (s)
#it.
CPU (s)
#it.
CPU (s)
LU
32
6.85
39
208.46
41
13099
GCR(10,HP-LU)
40
8.96
42
59.06
53
477.36
We consider a linear elastic material with a Poisson coefficient v = 0.3 and 
an elasticity modulus E = 102. The primal problem will be solved by either a 
direct or an iterative method and we present at Table 5.1 the two usual performance 
indicators i.s.e the total number of iterations and the computational time in seconds. 
Here we take R = M0 (Remark 5.12) for which we do not expect convergence to 
be independent of h. There is room for improvement but even then the results show 
that the method is suitable for large problems.
In Table 5.1 the iterations numbers show that even when using the L2 scalar 
product for b(vh, kh), convergence of the dual problem is only slightly dependent of 
the discretization. The large differences in computation time illustrate the important 
gain in efficiency of the iterative solver for large problems.
It is also interesting to consider convergence curves of the normal residual and 
the importance of the solver in u. In Fig. 5.5 one compares the convergence for three 
mesh sizes. Red dots indicate a change in contact status in the active set strategy. 
The changes occur at the beginning of the process and the iteration is then restarted 
on the new active region.. Most of the computational efforts consist in solving a

5.2 Sliding Contact
95
Number of cumulative iterations
Number of cumulative iterations
Fig. 5.5 Convergence for rn. On top a coarse mesh (14,739 dof for u). Bottom left a mesh with 
107,811 dof, on the right a mesh with 823,875 dof
Number of cumulative iterations
single equality constraint problem at each Newton step. If we ignore computational 
time obviously the direct solver in u perform better since it produces less iterations 
compared to the iterative solver. Finally, for smaller mesh (top of Fig. 5.5) the direct 
solver seems too good: time is lost with excessive tolerance at each Newton step.

Chapter 6
Solving Problems with More Than One
Constraint
In this chapter, we shall consider problems of the form
ABt Ct 
u
B00 
p
C 0 0 
л
(6.1)
A
In theory, this is not fundamentally different from the problem (2.22). From the 
numerical point of view, things are not so simple as the presence of two independent 
constraints brings new difficulties in the building of algorithms and preconditioners. 
We present some ideas which indeed yield more questions than answers.
6.1 A Model Problem
To fix ideas, we consider a case where two simultaneous constraints are to be 
satisfied. We have already considered in Sect. 4.2 problems of incompressible 
elasticity and in Sect. 5 problems of sliding contact. We now want to solve problems 
where both these features are involved.
Let £2 be a domain of R3, Г its boundary and Гс a part of Г where sliding contact 
conditions are imposed by Lagrange multipliers. We suppose that suitable Dirichlet 
conditions are imposed on a part of the boundary to make the problem well posed.
We shall use the same procedure as in Sect. 5.2, the only difference will be 
that we now denote C the operator which was then denoted B, this operator now 
being associated with the discrete divergence: we now want the material to be 
incompressible. Proceeding again to linearise the problem, and using the active set
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
97
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_6 

98
6 Solving Problems with More Than One Constraint
technique, we are led to solve a sequence of saddle-point problem of the form
1
inf sup -(Av, V) + (Xn, (Cv - rx)) + (p, (Bv - rp)) - (r_u,v).
v P,Xn 2
This is indeed a problem similar to (6.1). We shall first consider a naive solution 
technique, the interlaced method and then reconsider the use of approximate 
factorisations.
6.2 Interlaced Method
It would be natural to rewrite the system (6.1) as a problem with a single constraint. 
To do so we introduce block matrices
ACt 
u = ru
C 0 
X r x
where the blocks are defined by
A = (AB) ’ C = (C 0) ’ u = (u) ’ ru = (ru) and rx = (rX) •
B 0 
p 
rp 
0
It is then possible to handle this problem as a contact problem where each 
solution of the elasticity problem is in fact the solution of an incompressible 
elasticity problem which can be solved (with more or less precision) by the methods 
presented in Sect. 4.2. We call this the interlaced method.
Remark 6.1 The choice of the ‘inner’ problem, here the incompressible elasticity 
problem is arbitrary. We could also have chosen the contact problem. Our choice is 
directed by the fact that the incompressible elasticity problem is linear. 
■
Starting from this rewriting of the system, we can use directly the technique of 
Sect. 3.2.1, in particular the preconditioner of Algorithm 3.10 in which we replace 
the approximate solver for A by an approximate solver for A. This is simple but 
carries some difficulties.
• The matrix A is not definite positive. This essentially renders invalid the classical 
proofs of convergence, for example of Uzawa’s method.
• We have to solve an internal problem which will make the method expensive.
The downside of this approach is thus the significant cost of the iterations in x.In 
fact, at each iteration, two solutions to the problem in (u, p) are asked. These latter 
being made iteratively they also require at least two displacement only solutions.

6.2 Interlaced Method
99
Remark 6.2 (Projected Gradient) If we refer ourselves to Sect. 3.1.3 one sees that 
what we are doing is iterating in the divergence-free subspace provided the problems 
in (u, p) are solved precisely. 
■
To illustrate the behaviour of the algorithm, we first consider the case ofan accurate 
solution in (u, p). This will be done as in Sect. 4.2.4. For the solve in u, we take 
either a direct LU solver or some iterations of GCR preconditioned by HP-lU. This 
is clearly an expensive procedure only used for comprehension.
We present the convergence in X for the intermediate mesh of Sect. 5.2.3 with 
107811 degrees of freedom.
As can be expected the results are comparable for the two solver in u as we use 
essentially the same information to update X. Indeed if we have an accurate solution 
in (u, p) it should not be dependent on the way it is computed (Fig. 6.1).
When an incomplete solution in (u, p) is considered by limiting the number of 
iterations permitted in the Mixed-GMP-GCR method, the iterative solution in Xn is 
loosing effectiveness (Fig. 6.2).
Fig. 6. 1 Interlaced method 
using complete solver for A: 
convergence for the normal 
residual according to the 
primal solver with the 
intermediate mesh
Number of cumulative iterations
Fig. 6. 2 Interlaced method 
using incomplete solver for 
A: convergence according to 
the primal solving method 
using intermediate mesh
0 
20406080100
10-2
10-5
10-8
io -11
Number of cumulative iterations

100
6 Solving Problems with More Than One Constraint
Table 6.1 Interlaced method 
using incomplete solver for 
A: number of iterations and 
CPU time in seconds 
according to the solver in u
107 811 dof
# iCPU(s)
LU
93
464.64
GCR(10,HP)
108
4928. 12
Although it works, it is clearly an expensive method (Table 6.1) and we did not 
push the test to finer meshes. We now consider another approach which looked more 
promising.
6.3 Preconditioners Based on Factorisation
In Sect. 3.2.1 we presented a general preconditioner based on the factorisation of the 
matrix associated to our problem. This can be done in the present case but will also 
imply to develop variants. It is easily seen that the matrix of equation (6.1) can be 
factorised in the form,
A=
where
SBB =BA-1Bt,SBC=BA-1Ct,SCB= CA-1Bt,SCC=BA-1Ct.
We shall denote
S=
SBB SBC
SCB SCC
We now see what the Algorithm 3.7 would now become,
Algorithm 6.1 General mixed preconditioner for two constraints Assuming that 
we have A-1 and S-1 approximate solvers for A and S, from the residual ru, rp 
and r^, we compute the vectors zu, zp and z^.
• Initialise
zu = A-1 ru
rp = BzU - rp-
>z - CzU - r^-
(6.1)

6.3 Preconditioners Based on Factorisation
101
• Using S 1, solve approximately for zp and zx the coupled problem,
SBB SBC 
zp = rp
SCB SCC 
zx 
rx
• Compute
Zu = zu - A-1 Bzp - A-1 Czx
(6.2)
(6.3)
■
The key is thus to solve (6.2). There is clearly no general way of doing this. 
Starting from the point of view that we have approximate solvers for SBB and SCC, 
we may use a Gauss-Seidel iteration for zp and zx .
In the simplest case we do one iteration for each problem.
6.3.1 The Sequential Method
In this case we use
• One iteration in p and one in x
• The result is used in a GCR global method.
In the following Table 6.2, we present the number of iterations and the solution 
time, for different ways of solver for the matrix A of the primal problem.
For small meshes, LU factorisation is clearly the best way and this also 
the true in all cases with respect to iteration number. A GCR method without 
preconditioning rapidly deteriorates. The multigrid HP-AMG method becomes the 
best for computing time for the fine mesh.
Looking to convergence curves, we see that even this last method could be 
improved as the convergence seems to slow down when the residual becomes 
small. For a still finer mesh one could need a more accurate solver for A to avoid
Table 6.2 Number of iterations and CPU time (s) according to the size of the mesh and the solving 
method for the primal system
14,739 dof
107,811 dof
823,875 dof
# it.
CPU (s)
# it.
CPU (s)
#it.
CPU (s)
LU
55
8. 36
88
120.94
124
3432.87
GCR(10)
129
92.97
141
724.54
209
7940.04
CG(10,GAMG)
129
62.98
99
332.34
162
4459.59
GCR(5,HP-AMG)
174
36.66
159
191.78
159
1533.71

102
6 Solving Problems with More Than One Constraint
10-3
10-7
10-11
101
50
100 
150
Number of cumulative iterations
Fig. 6.4 Convergence curves for the normal residual (left) and hydrostatic pressure p (right) 
according to the primal problem solving method when the finest mesh is considered
Fig. 6.3 Convergence curves for the normal residual (left) and hydrostatic pressure p (right) 
according to the primal problem solving method when the middle mesh is considered
stagnation, keeping in mind that LU becomes more and more inefficient (Figs. 6.3 
and 6.4).
6.4 An Alternating Procedure
Finally, to end this presentation, we present some results for an ‘Alternating 
Direction’ procedure. The idea is simple: solve alternatingly the contact problem 
for (u, kn) fixing the hydrostatic pressure p and then solve for (u, p) fixing the 
contact pressure kn. Another issue is that for the contact part, a restart procedure 
has to be considered when a change occurs in contact status (Fig. 6.5).

6.4 An Alternating Procedure
103
Fig. 6.5 Illustration of the 
alternating procedure
Table 6.3 Alternating method: iteration’s number/CPU(s) according to the size of the mesh and 
the solving method for the primal system
14,739 dof
107,811 dof
823,875 dof
#it.
CPU (s)
#it.
CPU (s)
#it.
CPU (s)
LU
66
7
110
93
120
2585
GCR(10)
110
35
120
294
150
2966
GCR(5,HP-LU)
120
17
150
139
160
1168
Remark 6.3 (A Projection Method) This could be seen as a Projection method. 
After a few iterations in Л, the solution is projected on the divergence-free 
subspace. 
■
As we did in the previous section with the sequential method, some computations 
are presented with this alternating method for the three different meshes. In the 
Table 6.3, we present the number of global iteration on A and the computing time 
for different solvers for the primal problem. The conclusion is the same as for all 
previous examples: when the mesh size gets large, the iterative method (in particular 
when using the HP-LU method of Sect. 3.1.4) becomes better in computing time.
For the convergence, we can see in Figs. 6.6 and 6.7 respectively the middle and 
finest mesh, the curves for the hydrostatic and contact pressure according to the 
solver of primal problem. We see a good convergence of thoses curves particulary 
the contact one.

104
6 Solving Problems with More Than One Constraint
Fig. 6.6 Alternating method: convergence curves for the normal residual (left) and hydrostatic 
pressure (p) according to the primal problem solving method when the middle mesh is considered
10-15
0 
50 
100 
150
Number of cumulative iterations
oGCR
.GCR-HP
10-3
10-7
10- 11
xLU
0 
20 
40 
60 
80 
100
Number of cumulative iterations
Fig. 6.7 Alternating method: convergence curves for the normal residual (left) and p the 
hydrostatic pressure (right) according to the primal problem solving method when the finest mesh 
is considered
This is a crude first testing and things could clearly be ameliorated. In particular, 
one could think of marrying the alternating idea into the sequential technique of the 
previous Sect. 6.3.1. There is room for new ideas...

Chapter 7
Conclusion
Check for 
updates
We hope to have shown that solving mixed problems can be accomplished effi­
ciently. This work is clearly not exhaustive and we have indeed tried to open the 
way for future research. We have relied as building bricks on rather classical iterative 
methods. However, we think that we have assembled these bricks in some new ways. 
We have also insisted in developing methods as free as possible of user depending 
parameters.
We have also considered Augmented Lagrangians, either in an exact or a 
regularised version. This was done in the mind that direct methods should be 
avoided for large scale computations and that penalty terms destroy the condition 
number of the penalised system.
• For mixed formulations based on elements satisfying the equilibrium conditions, 
we have shown that Augmented Lagrangian is efficient. The problem presented 
was very simple but we think that the results could be extended to more realistic 
situations. In the case of mixed elasticity in which a symmetry condition has to 
be imposed [19] one would have to deal with two constraints as in Chap. 6. The 
situation would be better than in the example presented there as the equilibrium 
constraint is amenable to a real augmented Lagrangian.
• Problems involving incompressible elasticity are of central importance in many 
applications. Unfortunately, they are often solved with poor methods using 
penalty and low order elements. We have shown that continuous pressure 
elements, which are essential for accurate three-dimensional computations at 
reasonable cost, are manageable and can be accelerated by a stabilisation term.
• For contact problems, we have considered some possible alternative avenues 
to the standard approximations where the constraint is treated in L2 instead 
of the correct H 1/2. This is still an open area. We have shown that using the 
more classical formulation, one can obtain results for large meshes with good 
efficiency.
• For problems involving two constraints, we have explored some possibilities and 
many variants are possible.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1_7
105

106
7 Conclusion
The methods that we discussed can in most cases be employed for parallel 
computing. We did not adventure ourselves in this direction which would need a 
research work by itself. We must nevertheless emphasise that the Petsc package 
which we used is intrinsically built for parallel computing.

Bibliography
1. B. Achchab, J.F. Maitre, Estimate of the constant in two strengthened C.B.S. inequalities 
for F.E.M. systems of 2D slasticity: application to multilevel methods and a posteriori error 
estimators. Numer. Linear Algebra Appl. 3(2), 147-159 (1996)
2. ADINA Inc., Instability of two-term mooney-rivlin model.
 
 https://www.adina.com/newsgH48.
shtml
3. P. Alart, A. Curnier, A mixed formulation for frictional contact problems prone to Newton like 
solution methods. Comput. Methods Appl. Mech. Eng. 92(3), 353-375 (1991)
4. G. Allaire, S.M. Kaber, Numerical Linear Algebra. Texts in Applied Mathematics, vol. 55 
(Springer, New York, 2008)
5. M. Arioli, D. Kourounis, D. Loghin, Discrete fractional Sobolev norms for domain decompo­
sition preconditioning. IMA J. Numer. Anal. 33(1), 318-342 (2013)
6. D.N. Arnold, F. Brezzi, Mixed and nonconforming finite element methods: implementation, 
postprocessing and error estimates. ESAIM: Math. Model. Numer. Anal. 19(1), 7-32 (1985)
7. W.E. Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue 
problem. Quart. Appl. Math. 9(1):17-29 (1951)
8. K.J. Arrow, L. Hurwicz, H. Uzawa, Studies in Linear and Non-linear Programming. Stanford 
Mathematical Studies in the Social Sciences, vol. 2 (Stanford University Press, Stanford, 1958)
9. O. Axelsson, G. Lindskog, On the rate of convergence of the preconditioned conjugate gradient 
method. Numer. Math. 48(5), 499-523 (1986)
10. Z.Z. Bai, B.N. Parlett, Z.Q. Wang, On generalized successive overrelaxation methods for 
augmented linear systems. Numer. Math. 102(1), 1-38 (2005)
11. S. Balay, S. Abhyankar, M. Adams, J. Brown, P. Brune, K. Buschelman, L. Dalcin, A. Dener, 
V. Eijkhout, W. Gropp, D. Karpeyev, D. Kaushik, M. Knepley, D. May, L. Curfman McInnes, 
R. Mills, T. Munson, K. Rupp, P. Sanan, B. Smith, S. Zampini, H. Zhang, H. Zhang, PETSc 
Users Manual: Revision 3.10. (Argonne National Lab. (ANL), Argonne, 2018)
12. J. Baranger, J.F. Maitre, F. Oudin, Connection between finite volume and mixed finite element 
methods. ESAIM Math. Model. Numer. Anal. 30(4), 445-465 (1996)
13. K.J. Bathe, F. Brezzi, Stability of finite element mixed interpolations for contact problems. 
Atti della Accademia Nazionale dei Lincei. Classe di Scienze Fisiche, Matematiche e Naturali. 
Rendiconti Lincei. Matematica e Applicazioni 12(3), 167-183 (2001)
14. L. Beilina, E. Karchevskii, M. Karchevskii, Solving systems of linear equations, in Numerical 
Linear Algebra: Theory and Applications (Springer, Cham, 2017), pp. 249-289
15. M. Benzi, G.H. Golub, J. Liesen, Numerical solution of saddle point problems. Acta Numer. 
14, 1-137 (2005)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1
107

108
Bibliography
16. M. Bergounioux, K. Kunisch, Primal-dual strategy for state-constrained optimal control 
problems. Comput. Optim. Appl. 22(2), 193-224 (2002)
17. M. Bergounioux, M. Haddou, M. Hintermuller, K. Kunisch, A comparison of a Moreau-Yosida 
based active set strategy and interior point methods for constrained optimal control problems. 
SIAM J. Optim. 11(2), 495-521 (2000)
18. D.P. Bertsekas, Nonlinear Programming, 2nd edn. (Athena Scientific, Belmont, 1999)
19. D. Boffi, F. Brezzi, M. Fortin, Reduced symmetry elements in linear elasticity. Commun. Pure 
Appl. Anal. 8(1), 95-121 (2009)
20. D. Boffi, F. Brezzi, M. Fortin, Mixed Finite Element Methods and Applications. Springer Series 
in Computational Mathematics, vol. 44 (Springer, Berlin 2013)
21. J. Bonet, R.D. Wood, Nonlinear Continuum Mechanics for Finite Element Analysis, 2nd edn. 
(Cambridge University Press, Cambridge, 2008)
22. D. Braess, Finite elements: theory, fast solvers and applications in solid mechanics. Meas. Sci. 
Technol. 13(9), 365 (2007)
23. J.H. Bramble, J.E. Pasciak, A.T. Vassilev, Analysis of the inexact Uzawa algorithm for saddle 
point problems. SIAM J. Numer. Anal. 34(3), 1072-1092 (1997)
24. F. Brezzi, K.J. Bathe, A discourse on the stability conditions for mixed finite element 
formulations. Comput. Methods Appl. Mech. Eng. 82(1), 27-57 (1990)
25. F. Brezzi, M. Fortin, L.D. Marini, Mixed finite element methods with continuous stresses. 
Math. Models Methods Appl. Sci. 03(02), 275-287 (1993)
26. J. Cahouet, J.P. Chabard, Some fast 3D finite element solvers for the generalized Stokes 
problem. Int. J. Numer. Methods Fluids 8, 869-895 (1988)
27. M. Chanaud, Conception d’un solveur haute performance de systemes lineaires creux couplant 
des methodes multigrilles et directes pour la resolution des equations de Maxwell 3D en regime 
harmonique discretisees par elements finis. Ph.D. Thesis, Universite de Bordeaux 1, France 
(2011). https://www.theses.fr/2011BOR14324/abes
28. S.H. Cheng, Symmetric indefinite matrices: linear system solvers and 
modified inertia problems. Ph.D. Thesis, University of Manchester (1998). 
https://www.maths.manchester.ac.uk/~higham/links/theses/cheng98.pdf
29. A.J. Chorin, Numerical solution of the Navier-Stokes equations. Math. Comput. 22(104), 745­
762 (1968)
30. P.G. Ciarlet, The Finite Element Method for Elliptic Problems. Studies in Mathematics and Its 
Applications (North-Holland, Amsterdam, 1978)
31. G.E. Comi, G. Stefani, A distributional approach to fractional Sobolev spaces and fractional 
variation: existence of blow-up. J. Function. Anal. 277(10), 3373-3435 (2019)
32. M. Dauge, Elliptic Boundary Value Problems on Corner Domains. Lecture Notes in Mathe­
matics, vol. 1341 (Springer, Berlin, 1988)
33. M.C. Delfour, J.P. Zolesio, Shapes and Geometries: Analysis, Differential Calculus and Opti­
mization. Advances in Design and Control (Society for Industrial and Applied Mathematics, 
Philadelphia, 2001)
34. T. Diop, Resolution iterative de problemes de contact frottant de grande taille. Ph.D. Thesis, 
Universite Laval, Canada (2019). https://corpus.ulaval.ca/jspui/handle/20.500.11794/34968
35. G. Duvaut, J.L. Lions, Inequalities in Mechanics and Physics. Grundlehren der mathematis- 
chen Wissenschaften, vol. 219 (Springer, Berlin, 1976)
36. S.C. Eisenstat, H.C. Elman, M.H. Schultz, Variational iterative methods for nonsymmetric 
systems of linear equations. SIAM J. Numer. Anal. 20(2), 345-357 (1983)
37. A. El maliki, R. Guenette, M. Fortin, An efficient hierarchical preconditioner for quadratic 
discretizations of finite element problems. Numer. Linear Algebra Appl. 18(5), 789-803 (2011)
38. A. El maliki, M. Fortin, J. Deteix, A. Fortin, Preconditioned iteration for saddle-point systems 
with bound constraints arising in contact problems. Comput. Methods Appl. Mech. Eng. 254, 
114-125 (2013)
39. H.C. Elman, Iterative methods for large, sparse, nonsymmetric sys­
tems of linear equations. Ph.D. Thesis, Yale University (1982). 
http://www.cvc.yale.edu/publications/techreports/tr229.pdf.

Bibliography
109
40. H.C. Elman, G.H. Golub, Inexact and preconditioned Uzawa algorithms for saddle point 
problems. SIAM J. Numer. Anal. 31(6), 1645-1661 (1994)
41. H.C. Elman, D.J. Silvester, A.J. Wathen, Finite Elements and Fast Iterative Solvers: With 
Applications in Incompressible Fluid Dynamics. Numerical Mathematics and Scientific Com­
putation (Oxford University Press, Oxford, 2005)
42. A. Fortin, A. Garon, Les Elements Finis de La Theorie a La Pratique. GIREF, Universite Laval 
(2018)
43. M. Fortin, R. Glowinski, Augmented Lagrangian Methods: Applications to the Numerical 
Solution of Boundary-Value Problems. Studies in Mathematics and its Applications, vol. 15 
(North-Holland, Amsterdam, 1983)
44. M.J. Gander, Optimized Schwarz methods. SIAM J. Numer. Anal. 44(2), 699-731 (2006)
45. A. Ghai, C. Lu, X. Jiao, A comparison of preconditioned Krylov subspace methods for large- 
scale nonsymmetric linear systems. Numer. Linear Algebra Appl. 26(1), e2215 (2019)
46. G.H. Golub, C. Greif, On solving block-structured indefinite linear systems. SIAM J. Sci. 
Comput. 24(Part 6), 2076-2092 (2003)
47. G.H. Golub, C.F. Van Loan, Matrix Computations, Johns Hopkins Studies in the Mathematical 
Sciences, 3rd edn. (Johns Hopkins University Press, Baltimore, 1996)
48. P.P. Grinevich, M.A. Olshanskii, An iterative method for the stokes-type problem with variable 
viscosity. SIAM J. Sci. Comput. 31(5), 3959-3978 (2010)
49. A. Gunnel, R. Herzog, E. Sachs, A note on preconditioners and scalar products in Krylov 
subspace methods for self-adjoint problems in Hilbert space. Electron. Trans. Numer. Anal. 
41, 13-20 (2014)
50. W. Hackbusch, Iterative Solution of Large Sparse Systems of Equations. Applied Mathematical 
Sciences, vol. 95, 2nd edn. (Springer, Amsterdam, 2016)
51. R. Herzog, K.M. Soodhalter, A modified implementation of Minres to monitor residual 
subvector norms for block systems. SIAM J. Sci. Comput. 39(6), A2645-A2663 (2017)
52. M.R. Hestenes, E. Stiefel, Methods of conjugate gradients for solving linear systems. J. Res. 
Natl. Bureau Standards 49(6), 409-436 (1952)
53. G.A. Holzapfel, Nonlinear Solid Mechanics : A Continuum Approach for Engineering (Wiley, 
New York, 2000)
54. S. Hueber, B.I. Wohlmuth, A primal-dual active set strategy for non-linear multibody contact 
problems. Comput. Methods Appl. Mech. Eng. 194(27), 3147-3166 (2005)
55. S. Hueber, G. Stadler, B.I. Wohlmuth, A primal-dual active set algorithm for three-dimensional 
contact problems with coulomb friction. SIAM J. Sci. Comput. 30(2), 572-596 (2009)
56. K. Ito, K. Kunisch, Augmented Lagrangian methods for nonsmooth, convex optimization in 
Hilbert spaces. Nonlinear Anal. 41(5), 591-616 (2000)
57. K. Ito, K. Kunisch, Optimal control of elliptic variational inequalities. Appl. Math. Optim. Int.
J. Appl. Stoch. 41(3), 343-364 (2000)
58. E.G. Johnson, A.O. Nier, Angular aberrations in sector shaped electromagnetic lenses for 
focusing beams of charged particles. Phys. Rev. 91(1), 10-17 (1953)
59. N. Kikuchi, J.T. Oden, Contact Problems in Elasticity: A Study of Variational Inequalities and 
Finite Element Methods. SIAM Studies in Applied Mathematics, vol. 8 (SIAM, Philadelphia, 
1988)
60. C. Lanczos, An iteration method for the solution of the eigenvalue problem of linear differential 
and integral operators. J. Res. Natl. Bureau Standards 45, 255-282 (1950)
61. S. Leger, Methode lagrangienne actualisee pour des problemes hyperelastiques 
en tres grandes deformations. Ph.D. Thesis, Universite Laval, Canada (2014). 
https://corpus.ulaval.ca/jspui/handle/20.500.11794/25402
62. J.L. Lions, E. Magenes, Non-Homogeneous Boundary Value Problems and Applications. 
Grundlehren der mathematischen wissenschaften; bd. 181, vol. 1 (Springer, Berlin, 1972)
63. D.C. Liu, J. Nocedal, On the limited memory BFGS method for large scale optimization. Math. 
Program. 45(1-3), 503-528 (1989)
64. D. Loghin, A.J. Wathen, Analysis of preconditioners for saddle-point problems. SIAM J. Sci. 
Comput. 25(6), 2029-2049 (2004)

110
Bibliography
65. D.G. Luenberger, The conjugate residual method for constrained minimization problems. 
SIAM J. Numer. Anal. 7(3), 390-398 (1970)
66. C.F. Ma, Q.Q. Zheng, The corrected Uzawa method for solving saddle point problems. Numer. 
Linear Algebra Appl. 22(4), 717-730 (2015)
67. K.-A. Mardal, R. Winther, Preconditioning discretizations of systems of partial differential 
equations. Numer. Linear Algebra Appl. 18(1), 1-40 (2011)
68. L.D. Marini, An inexpensive method for the evaluation of the solution of the lowest order 
Raviart-Thomas mixed method. SIAM J. Numer. Anal. 22(3), 493-496 (1985)
69. H.O. MaY, The conjugate gradient method for unilateral problems. Comput. Struct. 22(4), 
595-598 (1986)
70. S.F. McCormick, Multigrid Methods. Frontiers in applied mathematics, vol. 3 (Society for 
Industrial and Applied Mathematics, Philadelphia, 1987)
71. G.A. Meurant, Computer Solution of Large Linear Systems. Studies in Mathematics and Its 
Applications, vol. 28 (North-Holland, Amsterdam, 1999)
72. J. Nocedal, S. Wright, Numerical Optimization. Springer Series in Operations Research and 
Financial Engineering, 2nd edn. (Springer, New York, 2006)
73. R.W. Ogden, Non-linear Elastic Deformations. Ellis Horwood Series in Mathematics and Its 
Applications (Ellis Horwood, Chichester, 1984)
74. C.C. Paige, M.A. Saunders, Solution of sparse indefinite systems of linear equations. SIAM J. 
Numer. Anal. 12(4), 617-629 (1975)
75. J. Pestana, A.J. Wathen, Natural preconditioning and iterative methods for saddle point 
systems. SIAM Rev. 57(1), 71-91 (2015)
76. L. Plasman, J. Deteix, D. Yakoubi, A projection scheme for Navier-Stokes with variable 
viscosity and natural boundary condition. Int. J. Numer. Methods Fluids 92(12), 1845-1865 
(2020)
77. A. Quarteroni, A. Valli, Theory and application of Steklov-Poincare operators for boundary­
value problems, in Applied and Industrial Mathematics: Venice - 1, 1989, ed. by R. Spigler, 
Mathematics and Its Applications (Springer, Dordrecht, 1991)
78. A. Quarteroni, R. Sacco, F. Saleri, Methodes Numeriques: Algorithmes, Analyse et Applica­
tions (Springer, Milano, 2007)
79. R.T. Rockafellar, The multiplier method of Hestenes and Powell applied to convex program­
ming. J. Optim. Theory Appl. 12(6), 555-562 (1973)
80. Y. Saad, A flexible inner-outer preconditioned GMRES algorithm. SIAM J. Sci. Comput. 14(2), 
461-469 (1993)
81. Y. Saad, Iterative Methods for Sparse Linear Systems, 2nd edn. (Society for Industrial and 
Applied Mathematics, Philadelphia, 2003)
82. B.V. Shah, R.J. Buehler, O. Kempthorne, Some algorithms for minimizing a function of several 
variables. J. Soc. Ind. Appl. Math. 12(1), 74-92 (1964)
83. D.J. Silvester, V. Simoncini, An optimal iterative solver for symmetric indefinite systems 
stemming from mixed approximation. ACM Trans. Math. Softw. 37(4), (2011)
84. J.C. Simo, T.J.R. Hughes, Computational Inelasticity. Interdisciplinary Applied Mathematics, 
vol. 7. (Springer, New York, 1998)
85. R. Temam, Navier-Stokes Equations Theory and Numerical Analysis. Studies in Mathematics 
and Its Applications (North-Holland, Amsterdam, 1977)
86. L.N. Trefethen, D. Bau, Numerical Linear Algebra (Society for Industrial and Applied 
Mathematics, Philadelphia, 1997)
87. A. van der Sluis, H.A. van der Vorst, The rate of convergence of conjugate gradients. Numer. 
Math. 48(5), 543-560 (1986)
88. H.A. van der Vorst, Iterative Krylov Methods for large Linear Systems. Cambridge Monographs 
on Applied and Computational Mathematics, vol. 13 (Cambridge University Press, New York, 
2003)
89. R. Verfurth, A posteriori error estimation and adaptive mesh-refinement techniques. J. Comput. 
Appl. Math. 50(1), 67-83 (1994)

Bibliography
111
90. 
A.J. Wathen, Realistic eigenvalue bounds for the Galerkin mass matrix. IMA J. Numer. Anal. 
7(4), 449-457 (1987)
91. 
P. Wriggers, Computational Contact Mechanics, 2nd edn. (Springer, Berlin, 2006)

Index
MS, 11
Active constraints, 24
Active set stategy, 86
Active set strategy, 93
Arrow-Hurwicz-Uzawa, 39, 61
Augmented Lagrangian, 5, 13, 41, 45
dual problem, 15
iterated penalty, 17
discrete, 15
Choice of MS, 35
variable coefficients, 57
Coercivity, 46
on the kernel, 4, 6, 51
Condition number, 12
Conjugate gradient, 22, 23
contact pressure, 88
Contact problems, 75
Contact status, 92
Convergence
independence of mesh size, 60
Convex constraints, 25
Dirichlet condition, 75
Discrete dual problem, 11
Discrete mixed problem, 7
Discrete norm, 82
Discrete scalar product, 90
Dual problem, 5
Elasticity
choice of elements, 56
Mooney Rivlin model, 52
neo-hookean model, 52
Ellipticity
global, 8
on the kernel, 8
Error estimates, 8
Existence, 4
Factorisation, 33
Fractional order derivatives, 77
GCR, 28
GCR solver for the Schur complement, 36
General mixed preconditioner, 34, 38, 56
GMRES, 62
Hierarchical basis, 26
Incompressible elasticity, 48
inequality, 24
Inequality constraints, 24
inf-sup condition, 4
Kuhn-Tucker conditions, 85, 92
Linear elasticity, 49
Matricial form, 9
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
J. Deteix et al., Numerical Methods for Mixed Finite Element Problems,
Lecture Notes in Mathematics 2318, https://doi.org/10.1007/978-3-031-12616-1
113

114
Index
Matrix form, 80
Metric on Qh, 10
Mixed Laplacian, 43
Mixed problem, 3
Model for discrete contact, 79
Mooney-Rivlin
numerical results, 65
Multigrid, 26, 56, 62
Navier-Stokes, 68
projection, 69
Neo-Hookean
numerical results, 64
Non linear elasticity, 52
Obstacle problem, 84
Operator B,6
Oriented distance, 88
Penalty method, 17
Perturbed problem, 32, 40
Positivity constraints, 84, 91
Preconditioner for the mixed problem, 32
Primal problem, 4
Projected gradient, 24, 99
Raviart-Thomas elements, 45
Rayleigh quotient, 12
Reduced Ah, 84
Regularised Lagrangian, 51, 58
Regular perturbation, 6
Right preconditioned GCR, 29
Saddle-Point, 4
Schur complement, 5, 11
approximate solver, 35
Sequential quadratic programming, 89
Simple mixed preconditioner, 38
Sliding contact, 87
discretisation, 89
incompressible material, 97
numerical example, 93
Sobolev spaces of fractional order, 76
Solver, 33
Spectrally equivalent, 12
Steklov-Pomcare, 78
discrete, 80
computation, 82
matrix, 82
Stokes problem, 51
Two constraints, 97
factorisation, 100
projection method, 102
Uzawa, 37

123
LECTURE NOTES IN MATHEMATICS
Editors in Chief: J.-M. Morel, B. Teissier;
Editorial Policy
1. Lecture Notes aim to report new developments in all areas of mathematics and their 
applications - quickly, informally and at a high level. Mathematical texts analysing new 
developments in modelling and numerical simulation are welcome.
Manuscripts should be reasonably self-contained and rounded off. Thus they may, and 
often will, present not only results of the author but also related work by other people. 
They may be based on specialised lecture courses. Furthermore, the manuscripts should 
provide sufficient motivation, examples and applications. This clearly distinguishes 
Lecture Notes from journal articles or technical reports which normally are very concise. 
Articles intended for a journal but too long to be accepted by most journals, usually do not 
have this “lecture notes” character. For similar reasons it is unusual for doctoral theses to 
be accepted for the Lecture Notes series, though habilitation theses may be appropriate.
2. Besides monographs, multi-author manuscripts resulting from SUMMER SCHOOLS or 
similar INTENSIVE COURSES are welcome, provided their objective was held to present 
an active mathematical topic to an audience at the beginning or intermediate graduate level 
(a list of participants should be provided).
The resulting manuscript should not be just a collection of course notes, but should require 
advance planning and coordination among the main lecturers. The subject matter should 
dictate the structure of the book. This structure should be motivated and explained in 
a scientific introduction, and the notation, references, index and formulation of results 
should be, if possible, unified by the editors. Each contribution should have an abstract 
and an introduction referring to the other contributions. In other words, more preparatory 
work must go into a multi-authored volume than simply assembling a disparate collection 
of papers, communicated at the event.
3. Manuscripts should be submitted either online at 
 to 
Springer’s mathematics editorial in Heidelberg, or electronically to one of the series edi­
tors. Authors should be aware that incomplete or insufficiently close-to-final manuscripts 
almost always result in longer refereeing times and nevertheless unclear referees’ rec­
ommendations, making further refereeing of a final draft necessary. The strict minimum 
amount of material that will be considered should include a detailed outline describing 
the planned contents of each chapter, a bibliography and several sample chapters. Parallel 
submission of a manuscript to another publisher while under consideration for LNM is not 
acceptable and can lead to rejection.
www.editorialmanager.com/lnm
4. In general, monographs will be sent out to at least 2 external referees for evaluation.
A final decision to publish can be made only on the basis of the complete manuscript, 
however a refereeing process leading to a preliminary decision can be based on a pre-final 
or incomplete manuscript.
Volume Editors of multi-author works are expected to arrange for the refereeing, to the 
usual scientific standards, of the individual contributions. If the resulting reports can be 

forwarded to the LNM Editorial Board, this is very helpful. If no reports are forwarded 
or if other questions remain unclear in respect of homogeneity etc, the series editors may 
wish to consult external referees for an overall evaluation of the volume.
5. Manuscripts should in general be submitted in English. Final manuscripts should contain 
at least 100 pages of mathematical text and should always include
- a table of contents;
- an informative introduction, with adequate motivation and perhaps some historical 
remarks: it should be accessible to a reader not intimately familiar with the topic 
treated;
- a subject index: as a rule this is genuinely helpful for the reader.
- For evaluation purposes, manuscripts should be submitted as pdf files.
6. Careful preparation of the manuscripts will help keep production time short besides 
ensuring satisfactory appearance of the finished book in print and online. After 
acceptance of the manuscript authors will be asked to prepare the final LaTeX source 
files (see LaTeX templates online:
 
 plus the corresponding pdf- or zipped ps- 
file. The LaTeX source files are essential for producing the full-text online version of 
the book, see
for the existing online volumes 
of LNM). The technical production of a Lecture Notes volume takes approximately 12 
weeks. Additional instructions, if necessary, are available on request from
 
 https://www.springer.com/gb/authors-editors/book-
authors-editors/manuscriptpreparation/5636)
 http://link.springer.com/bookseries/304 
 lnm@springer.
com.
7. Authors receive a total of 30 free copies of their volume and free access to their book on 
SpringerLink, but no royalties. They are entitled to a discount of 33.3 % on the price of 
Springer books purchased for their personal use, if ordering directly from Springer.
8. Commitment to publish is made by a Publishing Agreement; contributing authors of 
multiauthor books are requested to sign a Consent to Publish form. Springer-Verlag 
registers the copyright for each volume. Authors are free to reuse material contained in 
their LNM volumes in later publications: a brief written (or e-mail) request for formal 
permission is sufficient.
Addresses:
Professor Jean-Michel Morel, CMLA, Ecole Normale Superieure de Cachan, France
E-mail: moreljeanmichel@gmail.com
Professor Bernard Teissier, Equipe Geometrie et Dynamique,
Institut de Mathematiques de Jussieu - Paris Rive Gauche, Paris, France
E-mail: bernard.teissier@imj-prg.fr
Springer: Ute McCrory, Mathematics, Heidelberg, Germany, 
E-mail: lnm@springer.com


Yixin Cao
Jianer Chen (Eds.)
 123
LNCS 10392
23rd International Conference, COCOON 2017
Hong Kong, China, August 3–5, 2017
Proceedings
Computing
and Combinatorics

Lecture Notes in Computer Science
10392
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Yixin Cao
• Jianer Chen (Eds.)
Computing
and Combinatorics
23rd International Conference, COCOON 2017
Hong Kong, China, August 3–5, 2017
Proceedings
123

Editors
Yixin Cao
Department of Computing
Hong Kong Polytechnic University
Hong Kong
China
Jianer Chen
Texas A&M University
College Station, TX
USA
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-62388-7
ISBN 978-3-319-62389-4
(eBook)
DOI 10.1007/978-3-319-62389-4
Library of Congress Control Number: 2017945277
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume contains the papers presented at the 23rd International Computing and
Combinatorics Conference (COCOON 2017), held during August 3–5, 2017, in Hong
Kong, China. COCOON 2017 provided a forum for researchers working in the areas of
algorithms, theory of computation, computational complexity, and combinatorics
related to computing.
The technical program of the conference included 48 contributed papers selected by
the Program Committee from 111 full submissions received in response to the call for
papers. All the papers were peer reviewed by at least three Program Committee
members or external reviewers. The papers cover various topics, including algorithms
and data structures, complexity theory and computability, algorithmic game theory,
computational learning theory, cryptography, computational biology, computational
geometry and number theory, graph theory, and parallel and distributed computing.
Some of the papers will be selected for publication in special issues of Algorithmica,
Theoretical Computer Science (TCS), and Journal of Combinatorial Optimization
(JOCO). It is expected that the journal version of the papers will be in a more complete
form.
The conference also included three invited presentations, delivered by Dániel Marx
(Hungarian Academy of Sciences), Shang-Hua Teng (University of Southern California),
and Virginia Vassilevska Williams (Massachusetts Institute of Technology). Abstracts
of their talks are included in this volume. Also included are eight contributed talks from a
workshop on computational social networks (CSoNet 2017) co-located with COCOON
2017. The papers of CSoNet 2017 were selected by an independent Program Committee,
chaired by Donghyun Kim, Miloš Kudělka, and R.N. Uma. We appreciate the work by
the CSoNet Program Committee that helped enrich the conference topics.
We wish to thank all the authors who submitted extended abstracts for considera-
tion, the members of the Program Committee for their scholarly efforts, and all external
reviewers who assisted the Program Committee in the evaluation process. We thank the
Hong Kong Polytechnic University for hosting COCOON 2017. We are also grateful to
all members of the Organizing Committee and to their supporting staff.
The conference-management system EasyChair was used to handle the submissions,
to conduct the electronic Program Committee meetings, and to assist with the assembly
of the proceedings.
August 2017
Yixin Cao
Jianer Chen

Organization
Conference Chair
Jiannong Cao
Hong Kong Polytechnic University, China
Program Chairs
Yixin Cao
Hong Kong Polytechnic University, China
Jianer Chen
Texas A&M University, USA
Program Committee
Jarek Byrka
University of Wroclaw, Poland
Liming Cai
University of Georgia, USA
Hubert Chan
University of Hong Kong, China
Rajesh Chitnis
Weizmann Institute, Israel
Vida Dujmovic
University of Ottawa, Canada
Funda Ergun
Indiana University Bloomington, USA
Siyao Guo
Chinese University of Hong Kong, China
Magnús Már Halldórsson
Reykjavik University, Iceland
Pinar Heggernes
University of Bergen, Norway
Juraj Hromkovic
ETH Zurich, Switzerland
Kazuo Iwama
Kyoto University, Japan
Iyad Kanj
DePaul University, USA
Bingkai Lin
University of Tokyo, Japan
Daniel Lokshtanov
University of Bergen, Norway
Monaldo Mastrolilli
IDSIA, Switzerland
Ross McConnell
Colorado State University, USA
Vangelis Th. Paschos
University of Paris-Dauphine, France
Youming Qiao
National University of Singapore, Singapore
Piotr Sankowski
University of Warsaw, Poland
Ryuhei Uehara
Japan Advanced Institute of Science and Technology,
Japan
Magnus Wahlström
Royal Holloway, University of London, UK
Gerhard J. Woeginger
RWTH Aachen, Germany
Mary Wootters
Stanford University, USA
Chee Yap
New York University, USA
Grigory Yaroslavtsev
University of Pennsylvania, USA
Neal Young
University of California, Riverside, USA
Huacheng Yu
Stanford University, USA
Shengyu Zhang
Chinese University of Hong Kong, China
Daming Zhu
Shandong University, China

Additional Reviewers
Abboud, Amir
Abdelkader, Ahmed
Agrawal, Akanksha
Akutsu, Tatsuya
Alman, Josh
Applegate, David
Ásgeirsson, Eyjólfur Ingi
Banerjee, Sandip
Barba, Luis
Barcucci, Elena
Bei, Xiaohui
Belmonte, Rémy
Bienkowski, Marcin
Biswas, Arindam
Boeckenhauer, Hans-Joachim
Bose, Prosenjit
Burjons Pujol, Elisabet
Casel, Katrin
Chen, Shiteng
Cheng, Hao-Chung
Cousins, Benjamin
Curticapean, Radu
Dean, Brian
Della Croce, Federico
Eiben, Eduard
Emura, Keita
Epstein, Leah
Erlebach, Thomas
Fan, Chenglin
Fagerberg, Rolf
Fefferman, Bill
Flatland, Robin
Frei, Fabian
Fujiwara, Hiroshi
Ghosal, Pratik
Giannakos, Aristotelis
Giannopoulos, Panos
Golovach, Petr
Golovnev, Alexander
Gordinowicz, Przemysław
Gourves, Laurent
Gurjar, Rohit
Haraguchi, Kazuya
Hatami, Pooya
Hoffmann, Michael
Huang, Xin
Hubáček, Pavel
Im, Sungjin
Izumi, Taisuke
Jahanjou, Hamidreza
Jeż, Łukasz
Jones, Mark
Jowhari, Hossein
Kalaitzis, Christos
Kamiyama, Naoyuki
Khan, Arindam
Kim, Eun Jung
Klein, Kim-Manuel
Kolay, Sudeshna
Komargodski, Ilan
Komm, Dennis
Kozma, Laszlo
Kralovic, Rastislav
Krithika, R.
Kumar, Mrinal
Kurpisz, Adam
Lampis, Michael
van Leeuwen, Erik Jan
Leppänen, Samuli
Levin, Asaf
Li, Wenjun
Li, Yinan
Liedloff, Mathieu
Lin, Chengyu
Marcinkowski, Jan
Markakis, Evangelos
Matsuda, Takahiro
Meulemans, Wouter
Michail, Dimitrios
Misra, Pranabendu
Miyazaki, Shuichi
Monnot, Jerome
Mori, Ryuhei
Morin, Pat
Murat, Cécile
Myrvold, Wendy
VIII
Organization

Nagami Coregliano, Leonardo
Narayanan, Hariharan
Nasre, Meghana
Nilsson, Bengt J.
Nishimura, Harumichi
Nisse, Nicolas
Oum, Sang-Il
Pankratov, Denis
Papadopoulos, Charis
Pergola, Elisa
Pilipczuk, Michał
Rai, Ashutosh
Reyzin, Lev
Rohwedder, Lars
Rote, Günter
Sahu, Abhishek
Saurabh, Saket
Schmidt, Paweł
Seddighin, Saeed
Sheng, Bin
Sikora, Florian
Skopalik, Alexander
Spoerhase, Joachim
Srivastav, Abhinav
Staals, Frank
Stamoulis, Georgios
van Stee, Rob
Suzuki, Akira
Tale, Praffulkumar
Talmon, Nimrod
Tonoyan, Tigran
Tsang, Hing Yin
Ueno, Shuichi
Unger, Walter
Vaze, Vikrant
Vempala, Santosh
Verdugo, Victor
Wang, Guoming
Wang, Joshua
Wehner, David
Wrochna, Marcin
Xia, Ge
Xu, Zhisheng
Yamanaka, Katsuhisa
Yan, Li
Yao, Penghui
Ye, Junjie
Yeo, Anders
Yu, Nengkun
Zhang, Chihao
Zhang, Yihan
Zheng, Chaodong
Organization
IX

Invited Talks

The Optimality Program
for Parameterized Algorithms
Dániel Marx
Institute for Computer Science and Control,
Hungarian Academy of Sciences, Budapest, Hungary
Parameterized complexity analyzes the computational complexity of NP-hard combi-
natorial problems in ﬁner detail than classical complexity: instead of expressing the
running time as a univariate function of the size n of the input, one or more relevant
parameters are deﬁned and the running time is analyzed as a function depending on
both the input size and these parameters. The goal is to obtain algorithms whose
running time depends polynomially on the input size, but may have arbitrary (possibly
exponential) dependence on the parameters. Moreover, we would like the dependence
on the parameters to be as slowly growing as possible, to make it more likely that the
algorithm is efﬁcient in practice for small values of the parameters. In recent years,
advances in parameterized algorithms and complexity have given us a tight under-
standing of how the parameter has to inﬂuence the running time for various problems.
The talk will survey results of this form, showing that seemingly similar NP-hard
problems can behave in very different ways if they are analyzed in the parameterized
setting.

Scalable Algorithms for Data
and Network Analysis
Shang-Hua Teng
Computer Science and Mathematics,
University of Southern California, Los Angeles, USA
In the age of Big Data, efﬁcient algorithms are in higher demand now more than ever
before. While Big Data takes us into the asymptotic world envisioned by our pioneers,
the explosive growth of problem size has also signiﬁcantly challenged the classical
notion of efﬁcient algorithms: Algorithms that used to be considered efﬁcient,
according to polynomial-time characterization, may no longer be adequate for solving
today’s problems. It is not just desirable, but essential, that efﬁcient algorithms should
be scalable. In other words, their complexity should be nearly linear or sub-linear with
respect to the problem size. Thus, scalability, not just polynomial-time computability,
should be elevated as the central complexity notion for characterizing efﬁcient
computation.
In this talk, I will discuss a family of algorithmic techniques for the design of
provably-good scalable algorithms, focusing on the emerging Laplacian Paradigm,
which has led to breakthroughs in scalable algorithms for several fundamental prob-
lems in network analysis, machine learning, and scientiﬁc computing. These techniques
include local network exploration, advanced sampling, sparsiﬁcation, and graph par-
titioning. Network analysis subject include four recent applications: (1) PageRank
Approximation (and identiﬁcation of network nodes with signiﬁcant PageRanks).
(2) Social-Inﬂuence Shapley value. (3) Random-Walk Sparsiﬁcation. (4) Scalable
Newton’s Method for Gaussian Sampling.
Solutions to these problems exemplify the fusion of combinatorial, numerical, and
statistical thinking in network analysis.

Fine-Grained Complexity of Problems in P
Virginia Vassilevska Williams
Massachusetts Institute of Technology, Cambridge, USA
A central goal of algorithmic research is to determine how fast computational problems
can be solved in the worst case. Theorems from complexity theory state that there are
problems that, on inputs of size n, can be solved in tðnÞ time but not in tðnÞ1e time for
e [ 0. The main challenge is to determine where in this hierarchy various natural and
important problems lie. Throughout the years, many ingenious algorithmic techniques
have been developed and applied to obtain blazingly fast algorithms for many prob-
lems. Nevertheless, for many other central problems, the best known running times are
essentially those of their classical algorithms from the 1950s and 1960s.
Unconditional lower bounds seem very difﬁcult to obtain, and so practically all
known time lower bounds are conditional. For years, the main tool for proving hard-
ness of computational problems have been NP-hardness reductions, basing hardness on
P 6¼ NP. However, when we care about the exact running time (as opposed to merely
polynomial vs non-polynomial), NP-hardness is not applicable, especially if the
problem is already solvable in polynomial time. In recent years, a new theory has been
developed, based on “ﬁne-grained reductions” that focus on exact running times.
Mimicking NP-hardness, the approach is to (1) select a key problem X that is con-
jectured to require essentially tðnÞ time for some t, and (2) reduce X in a ﬁne-grained
way to many important problems. This approach has led to the discovery of many
meaningful relationships between problems, and even sometimes to equivalence
classes.
The main key problems used to base hardness on have been: the 3SUM problem,
the CNF-SAT problem (based on the Strong Exponential TIme Hypothesis (SETH))
and the All Pairs Shortest Paths Problem. Research on SETH-based lower bounds has
ﬂourished in particular in recent years showing that the classical algorithms are optimal
for problems such as Approximate Diameter, Edit Distance, Frechet Distance, Longest
Common Subsequence, many dynamic graph problems, etc.
In this talk I will give an overview of the current progress in this area of study, and
will highlight some exciting new developments.

Contents
COCOON 2017
A Time-Space Trade-Off for Triangulations of Points in the Plane . . . . . . . .
3
Hee-Kap Ahn, Nicola Baraldo, Eunjin Oh, and Francesco Silvestri
An FPTAS for the Volume of Some V-polytopes—It is Hard to Compute
the Volume of the Intersection of Two Cross-Polytopes . . . . . . . . . . . . . . . .
13
Ei Ando and Shuji Kijima
Local Search Strikes Again: PTAS for Variants of Geometric
Covering and Packing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
Pradeesha Ashok, Aniket Basu Roy, and Sathish Govindarajan
Depth Distribution in High Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
Jérémy Barbay, Pablo Pérez-Lantero, and Javiel Rojas-Ledesma
An Improved Lower Bound on the Growth Constant of Polyiamonds . . . . . .
50
Gill Barequet, Mira Shalah, and Yufei Zheng
Constrained Routing Between Non-Visible Vertices . . . . . . . . . . . . . . . . . .
62
Prosenjit Bose, Matias Korman, André van Renssen,
and Sander Verdonschot
Deletion Graph Problems Based on Deadlock Resolution . . . . . . . . . . . . . . .
75
Alan Diêgo Aurélio Carneiro, Fábio Protti, and Uéverton S. Souza
Space-Efficient Algorithms for Maximum Cardinality Search, Stack BFS,
Queue BFS and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Sankardeep Chakraborty and Srinivasa Rao Satti
Efficient Enumeration of Non-Equivalent Squares in Partial Words
with Few Holes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Panagiotis Charalampopoulos, Maxime Crochemore,
Costas S. Iliopoulos, Tomasz Kociumaka, Solon P. Pissis,
Jakub Radoszewski, Wojciech Rytter, and Tomasz Waleń
The Approximability of the p-hub Center Problem with Parameterized
Triangle Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Li-Hsuan Chen, Sun-Yuan Hsieh, Ling-Ju Hung, and Ralf Klasing

Approximation Algorithms for the Maximum Weight Internal Spanning
Tree Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
Zhi-Zhong Chen, Guohui Lin, Lusheng Wang, Yong Chen,
and Dan Wang
Incentive Ratios of a Proportional Sharing Mechanism
in Resource Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
Zhou Chen, Yukun Cheng, Qi Qi, and Xiang Yan
Efficient Enumeration of Maximal k-Degenerate Subgraphs
in a Chordal Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
Alessio Conte, Mamadou Moustapha Kanté, Yota Otachi, Takeaki Uno,
and Kunihiro Wasa
Reoptimization of Minimum Latency Problem . . . . . . . . . . . . . . . . . . . . . .
162
Wenkai Dai
Pure Nash Equilibria in Restricted Budget Games . . . . . . . . . . . . . . . . . . . .
175
Maximilian Drees, Matthias Feldotto, Sören Riechers,
and Alexander Skopalik
A New Kernel for Parameterized Max-Bisection Above Tight
Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
Qilong Feng, Senmin Zhu, and Jianxin Wang
Information Complexity of the AND Function in the Two-Party
and Multi-party Settings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
Yuval Filmus, Hamed Hatami, Yaqiao Li, and Suzin You
Optimal Online Two-Way Trading with Bounded Number of Transactions. . .
212
Stanley P.Y. Fung
Parameterized Shifted Combinatorial Optimization . . . . . . . . . . . . . . . . . . .
224
Jakub Gajarský, Petr Hliněný, Martin Koutecký, and Shmuel Onn
Approximate Minimum Diameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Mohammad Ghodsi, Hamid Homapour, and Masoud Seddighin
On Constant Depth Circuits Parameterized by Degree: Identity Testing
and Depth Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
Purnata Ghosal, Om Prakash, and B.V. Raghavendra Rao
A Tighter Relation Between Sensitivity Complexity
and Certificate Complexity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
Kun He, Qian Li, and Xiaoming Sun
Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus . . . . . .
275
Kuan-Yi Ho, Yi-Jun Chang, and Hsu-Chun Yen
XVIII
Contents

Reconfiguration of Maximum-Weight b-Matchings in a Graph . . . . . . . . . . .
287
Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama,
Yusuke Kobayashi, and Yoshio Okamoto
Constant Approximation for Stochastic Orienteering Problem
with ð1 þ Þ-Budget Relaxiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
Yiyao Jiang
Quantum Query Complexity of Unitary Operator Discrimination. . . . . . . . . .
309
Akinori Kawachi, Kenichi Kawano, François Le Gall,
and Suguru Tamaki
Randomized Incremental Construction for the Hausdorff Voronoi
Diagram Revisited and Extended. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
Elena Khramtcova and Evanthia Papadopoulou
NP-completeness Results for Partitioning a Graph
into Total Dominating Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Mikko Koivisto, Petteri Laakkonen, and Juho Lauri
Strong Triadic Closure in Cographs and Graphs
of Low Maximum Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346
Athanasios L. Konstantinidis, Stavros D. Nikolopoulos,
and Charis Papadopoulos
Hardness and Structural Results for Half-Squares of Restricted Tree
Convex Bipartite Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Hoang-Oanh Le and Van Bang Le
Faster Graph Coloring in Polynomial Space . . . . . . . . . . . . . . . . . . . . . . . .
371
Serge Gaspers and Edward J. Lee
On the Modulo Degree Complexity of Boolean Functions . . . . . . . . . . . . . .
384
Qian Li and Xiaoming Sun
Approximating Weighted Duo-Preservation in Comparative Genomics. . . . . .
396
Saeed Mehrabi
An Incentive Compatible, Efficient Market for Air Traffic
Flow Management. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Ruta Mehta and Vijay V. Vazirani
Linear Representation of Transversal Matroids and Gammoids
Parameterized by Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
Pranabendu Misra, Fahad Panolan, M.S. Ramanujan,
and Saket Saurabh
Contents
XIX

Dynamic Rank-Maximal Matchings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
Prajakta Nimbhorkar and Arvind Rameshwar V.
Bend Complexity and Hamiltonian Cycles in Grid Graphs . . . . . . . . . . . . . .
445
Rahnuma Islam Nishat and Sue Whitesides
Optimal Covering and Hitting of Line Segments by Two
Axis-Parallel Squares. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
457
Sanjib Sadhu, Sasanka Roy, Subhas C. Nandy, and Suchismita Roy
Complexity and Algorithms for Finding a Subset of Vectors
with the Longest Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
469
Vladimir Shenmaier
Simple O(n log 2 n) Algorithms for the Planar 2-Center Problem . . . . . . . . .
481
Xuehou Tan and Bo Jiang
Stable Matchings in Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
492
Satoshi Tayu and Shuichi Ueno
Maximum Matching on Trees in the Online Preemptive and the Incremental
Dynamic Graph Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
504
Sumedh Tirodkar and Sundar Vishwanathan
Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops. . .
516
Guangwei Wu and Jianxin Wang
The Existence of Universally Agreed Fairest Semi-matchings
in Any Given Bipartite Graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
Jian Xu, Soumya Banerjee, and Wenjing Rao
Better Inapproximability Bounds and Approximation Algorithms
for Min-Max Tree/Cycle/Path Cover Problems . . . . . . . . . . . . . . . . . . . . . .
542
Wei Yu and Zhaohui Liu
On the Complexity of k-Metric Antidimension Problem
and the Size of k-Antiresolving Sets in Random Graphs. . . . . . . . . . . . . . . .
555
Congsong Zhang and Yong Gao
A Local Search Approximation Algorithm for the k-means Problem
with Penalties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
Dongmei Zhang, Chunlin Hao, Chenchen Wu, Dachuan Xu,
and Zhenning Zhang
Improved Approximation Algorithm for the Maximum Base Pair Stackings
Problem in RNA Secondary Structures Prediction . . . . . . . . . . . . . . . . . . . .
575
Aizhong Zhou, Haitao Jiang, Jiong Guo, Haodi Feng, Nan Liu,
and Binhai Zhu
XX
Contents

CSoNet Papers
Cooperative Game Theory Approaches for Network Partitioning. . . . . . . . . .
591
Konstantin E. Avrachenkov, Aleksei Yu. Kondratev,
and Vladimir V. Mazalov
Chain of Influencers: Multipartite Intra-community Ranking. . . . . . . . . . . . .
603
Pavla Drazdilova, Jan Konecny, and Milos Kudelka
Influence Spread in Social Networks with both Positive
and Negative Influences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
615
Jing (Selena) He, Ying Xie, Tianyu Du, Shouling Ji, and Zhao Li
Guided Genetic Algorithm for the Influence Maximization Problem . . . . . . .
630
Pavel Krömer and Jana Nowaková
Optimal Local Routing Strategies for Community Structured
Time Varying Communication Networks . . . . . . . . . . . . . . . . . . . . . . . . . .
642
Suchi Kumari, Anurag Singh, and Hocine Cherifi
Graph Construction Based on Local Representativeness . . . . . . . . . . . . . . . .
654
Eliska Ochodkova, Sarka Zehnalova, and Milos Kudelka
SHADE Algorithm Dynamic Analyzed Through Complex Network . . . . . . .
666
Adam Viktorin, Roman Senkerik, Michal Pluhacek, and Tomas Kadavy
An Efficient Potential Member Promotion Algorithm
in Social Networks via Skyline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
678
Siman Zhang and Jiping Zheng
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
691
Contents
XXI

COCOON 2017

A Time-Space Trade-Oﬀfor Triangulations
of Points in the Plane
Hee-Kap Ahn1, Nicola Baraldo2, Eunjin Oh1(B), and Francesco Silvestri2
1 Pohang University of Science and Technology, Pohang, Korea
{heekap,jin9082}@postech.ac.kr
2 University of Padova, Padova, Italy
nicola.baraldo@gmail.com, silvestri@dei.unipd.it
Abstract. In this paper, we consider time-space trade-oﬀs for reporting
a triangulation of points in the plane. The goal is to minimize the amount
of working space while keeping the total running time small. We present
the ﬁrst multi-pass algorithm on the problem that returns the edges of a
triangulation with their adjacency information. This even improves the
previously best known random-access algorithm.
1
Introduction
There are two optimization goals in the design of algorithms: the time complexity
and the space complexity. However, one cannot achieve both goals at the same
time in general. This can be seen in a time-space tradeoﬀof algorithmic eﬃciency
that an algorithm has to use more space to improve its running time and it has
to spend more time with less amount of space. With this reason, time-space
trade-oﬀs for a number of problems were considered even as early as in 1980s.
For example, Frederickson presented optimal time-space trade-oﬀs for sorting
and selection problems in 1987 [11]. After this work, a signiﬁcant amount of
research has been done for time-space trade-oﬀs in the design of algorithms.
In this paper, we consider time-space trade-oﬀs for one of fundamental geo-
metric problems, reporting a triangulation of points in the plane. We assume that
the points are given in a read-only memory. This assumption has been considered
in applications where the input is required to be retained in its original state.
Many time-space tradeoﬀs for fundamental problems have been studied under
this read-only assumption. For instance, a few read-only sorting algorithms have
been presented under the assumption [6,14].
There are two typical access models to the read-only input, a random-access
model and a multi-pass model. In the multi-pass model, the only way to access
elements in the input array is to scan the array from the beginning, and algo-
rithms are allowed to make multiple passes over the input. A single pass is a
special case of the multi-pass model. Multi-pass algorithms are more restrictive
than algorithms under the random-access for any element in the input array.
This work was supported by the NRF grant 2011-0030044 (SRC-GAIA) funded by
the government of Korea.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 3–12, 2017.
DOI: 10.1007/978-3-319-62389-4 1

4
H.-K. Ahn et al.
The multi-pass model has applications where large data sets are stored some-
where such as an external memory and it is more eﬃcient in I/O to read them
sequentially in a few passes. Multi-pass algorithms have been studied recently
in areas including geometry [1,15] and graphs [9,10].
The goal of our problem is to minimize the amount of working space while
keeping the total running time small. More precisely, we are allowed to use O(s)
words as working space in addition to the memory for input and output for a
positive integer parameter s which is determined by users. We assume that a
word is large enough to store a number and a pointer. While processing input, we
send the answer to a write-only output stream without repetition. An algorithm
designed in this setting is called an s-workspace algorithm.
1.1
Related Works
A triangulation of a set S of n points in the plane is deﬁned to be a maximal
subdivision of the plane whose vertices are in S and faces are triangles, except
for the unbounded face. The unbounded face of a triangulation of S is the region
outside of the convex hull of S. Thus the sorting problem which asks for sorting
n numbers reduces to this problem. Similarly, the problem of computing the
convex hull of n points in the plane reduces to this problem. In the following,
we simply call these problems the sorting problem and the convex hull problem,
respectively.
The optimal trade-oﬀs for the sorting problem and the convex hull problem
are known for both models (the random-access model and the multi-pass stream-
ing model.) Under the random-access model, both problems can be solved in
O(n2/(s log n)+n log(s log n)) time1 using O(s) words of workspace [8,14]. Under
the multi-pass streaming model, both problems can be solved in O(n2/ log n +
n log s) time [7,13] using O(s) words of workspace and O(n/s) passes of the
input array.
With linear-size working space, a triangulation of S can be computed in
O(n log n) time. For the case that a space is given as a positive integer parame-
ter s at most n, several results are known for the random-access model while no
result is known for the multi-pass streaming model. Korman et al. presented an s-
workspace algorithm for computing a triangulation of S in O(n2/s+n log n log s)
time [12]. In the same paper, they presented an s-workspace algorithm for
computing the Delaunay triangulation of S in O((n2/s) log s + n log s log∗s)
expected time. Recently, it is improved to O(n2 log n/s) deterministic time [4].
Combining [4,12], a triangulation of S can be computed in O(min{n2/s +
n log n log s, n2 log n/s}) time.
The problem of computing a triangulation of a simple polygon has also
been studied under the random-access model. Aronov et al. [2] presented
an s-workspace algorithm for computing a triangulation of a simple n-gon.
Their algorithm returns the edges of a triangulation without repetition in
1 They state that their running time is O(n2/s + n log s) for s bits of workspace, but
we measure workspace in words.

A Time-Space Trade-Oﬀfor Triangulations of Points in the Plane
5
O(n2/s+n log s log5 n/s) expected time. Moreover, their algorithm can be mod-
iﬁed to report the resulting triangles of a triangulation together with their
adjacency information. For a monotone n-gon, Barba et al. [5] presented an
(s logs n)-workspace algorithm for triangulating the polygon in O(n logs n) time
for a parameter s ∈{1, . . . , n}. Later, Asano and Kirkpatrick [3] showed how to
reduce the working space to O(s) words without increasing the running time.
1.2
Our Results
We present an s-workspace O(n2/s + n log s)-time algorithm for computing a
triangulation of a set of n points in the plane. Our algorithm uses O(n/s) passes
over the input array. To our best knowledge, this is the ﬁrst result on the problem
under the multi-pass model. These bounds are asymptotically optimal, which can
be shown by a reduction from the sorting problem [13].
Our multi-pass algorithm also improves the previously best known algorithm
under the random-access model by Korman et al. which takes O(min{n2/s +
n log n log s, n2 log n/s}) time [4,12] although the multi-pass model is more
restrictive than the random-access model. It seems unclear whether the algorithm
by Korman et al. [12] can be extended to a multi-pass streaming algorithm.
Our algorithm has an additional advantage compared to the previously best
one. Our algorithm can be extended to report the triangles together with adja-
cency information as well as the edges of a triangulation without increasing the
running time and space. The edge adjacency is essential information in repre-
senting and reconstructing the triangulation. In contrast, the algorithms in [4,12]
report the edges of a triangulation in an arbitrary order with no adjacency infor-
mation of them. Furthermore, the algorithm by Korman et al. [12] uses the algo-
rithm by Asano and Kirkpatrick [3] as a subprocedure, but it is unclear how to
modify the subprocedure to report a triangulation together with edge adjacency
information [2].
2
Reporting the Edges of a Triangulation
In this section, we present an s-workspace O(n2/s + n log s)-time algorithm to
compute a triangulation of a set S of n points in the plane using O(n/s) passes.
Our algorithm is based on the multi-pass streaming algorithm by Chan and
Chen [7] for computing the convex hull of a set of points in the plane. For a
subset S′ of S, we use ch(S′) to denote the convex hull of S′.
Chan and Chen presented an algorithm to compute the convex hull of a set
S of points in the plane by scanning the points O(n/s) times. They consider
⌈n/s⌉disjoint vertical slabs each of which contains exactly s points of S, except
for the last vertical slab. They use two passes to compute the boundary of ch(S)
contained in each vertical slab. For one pass, they ﬁnd the points of S contained
in the vertical slab using Lemma 1. Then they compute the convex hull of them
in O(s log s) time. For the other pass, they ﬁnd the part of the boundary of the
convex hull which appears on the boundary of ch(S). In total, their algorithm
takes O(n2/s + n log s) time and uses O(s) words of working space.

6
H.-K. Ahn et al.
Lemma 1 ([7]).
Given a point p ∈S, we can compute the leftmost s points
lying to the right of p in O(n) time using O(s) words of working space in a single
pass.
2.1
Our Algorithm
Imagine ⌈n/s⌉disjoint vertical slabs each of which contains exactly s points of
S, except for the last vertical slabs. Let Si be the set of points of S contained
in the ith slab for i ∈{1, 2, . . . , ⌈n/s⌉}. By Lemma 1, we can compute all points
in Si by scanning the points in S once using O(s) words of working space if we
have the leftmost point of Si. The pseudocode of the overall algorithm can be
found in Algorithm 1.
Algorithm 1. Computing a triangulation of S
1: procedure Triangulation(S)
2:
s ←the leftmost point of S
3:
for i ←1 to ⌈n/s⌉do
4:
Compute Si by scanning all points in S once.
5:
Report all edges of a triangulation of Si.
6:
Let Ti be the set of points lying to the left of any point in Si.
7:
a ←the rightmost point of Ti and b ←the leftmost point of Si
8:
LowerTriangulation(a, b, Si)
9:
UpperTriangulation(a, b, Si)
The overall algorithm (Algorithm 1). We consider all vertical slabs from
left to right one by one. After we process a vertical slab, we guarantee that we
report all edges of a triangulation of the points of S contained in the union
of all previous vertical slabs. First, we compute S1 explicitly in O(n) time by
applying Lemma 1, and compute a triangulation of S1 in O(s log s) time. Assume
that we just considered Si−1 for some i ∈{2, . . . , ⌈n/s⌉} and we computed a
triangulation of S1 ∪. . . ∪Si−1. Let Ti be the set of points lying to the left of
any point in Si, that is, Ti = S1 ∪. . . ∪Si−1.
Now we handle Si. We compute Si explicitly in O(n) time by applying
Lemma 1, and compute a triangulation of Si. Let a be the rightmost point
of Ti and b be the leftmost point of Si (See Fig. 1(a).).
For two convex polygons C1 and C2, we say a line segment c1c2 for c1 ∈C1
and c2 ∈C2 a bridge of C1 and C2 if it appears on the boundary of the convex
hull of C1 and C2. If a bridge connects the lower chains of C1 and C2, we call
the bridge the lower bridge. Otherwise, we call it the upper bridge.
We traverse the boundary of ch(Ti) from a in clockwise order and traverse
the boundary of ch(Si) from b in counterclockwise order until we ﬁnd the lower
bridge of ch(Ti) and ch(Si). Let Li be the polygon whose boundary consists of
the chains we visited, ab and the lower bridge. During the traversal, we compute
a triangulation of Li. See Fig. 1(a). The pseudocode of this procedure can be
found in Algorithm 2. We call this procedure LowerTriangulation.

A Time-Space Trade-Oﬀfor Triangulations of Points in the Plane
7
CH(Ti)
CH(Si)
b
a
Ui
Li
CH(Ti)
a
CH(X)
γ
(a)
(b)
Fig. 1. (a) Starting from ab, we report the edges of a triangulation of Li (the gray
region) and the edges of a triangulation of Ui (the dashed region). (b) The edge γ is
the lower bridge of ch(X) and the convex hull of the points lying to the left of any
point of X.
Similarly, we ﬁnd the upper bridge of ch(Ti) and ch(Si) by traversing the
boundaries of ch(Ti) and ch(Si). Let Ui be the polygon whose boundary con-
sists of the chains we visited, ab and the upper bridge. We call this procedure
UpperTriangulation. This can be done a way similar to LowerTriangu-
lation. Note that Li ∪Ui = ch(Ti+1) \ (ch(Ti) ∪ch(Si)). We show how to
compute a triangulation of Li only because a triangulation of Ui can be com-
puted analogously.
Computing a triangulation of Li (Algorithm 2). We can construct and
traverse the boundary of ch(Si) in O(s log s) time since we can store ch(Si)
explicitly. However, this does not hold for ch(Ti) since the size of ch(Ti) might
exceed O(s). To traverse the boundary of ch(Ti) using O(s) words of working
space, we ﬁrst ﬁnd the rightmost s points of Ti using one pass by applying
Lemma 1. Let X denote the set of such s points. We store X explicitly.
Then we compute ch(X) and compute the lower bridge of ch(X) and the
convex hull of points of S lying to the left of any point of X. See Fig. 1(b). We
can compute the lower bridge in O(n) time by considering all points of S lying
to the left of any point of X one by one. Due to this bridge, we can decide which
part of the boundary of ch(X) appears on the boundary of ch(Ti). We traverse
the part of ch(X) appearing on the boundary of ch(Ti) until we ﬁnd the lower
bridge of ch(Ti) and ch(Si).
Once we reach the most clockwise vertex of ch(X) appearing on the boundary
of ch(Ti), we again ﬁnd the rightmost s points lying to the left of the endpoint
of γ not in X, where γ is the lower bridge of ch(X) and the convex hull of
the points lying to the left of any point of X. Then we update X to the set of
these points. Note that X may not be Sj for any 1 ≤j ≤i in this case. By
construction, the rightmost point of X appears on the lower chain of ch(Ti). We
do this until we ﬁnd the lower bridge of ch(Ti) and ch(Si).

8
H.-K. Ahn et al.
Algorithm 2. Computing a triangulation of Li
1: procedure LowerTriangulation(a,b,Si)
2:
⟨x1, . . . , xt⟩←be the lower hull of Si (from right to left).
3:
repeat
4:
X ←the rightmost s points lying to the left of a including a
5:
⟨y1, . . . , yt′⟩←the part of the lower hull of X appearing on ch(Ti)
(from left to right)
6:
repeat
7:
Report the edge xtyt′.
8:
if yt′−s lies to the left of the line containing xtyt′ in direction −−→
xtyt′ then
9:
t′ ←t′ −1
10:
else
11:
t ←t −1
12:
until all points in X ∪Si lie above the line containing xtyt′
13:
γ ←the bridge of ch(X) and the convex hull of points lying to the left of
any point of X
14:
Report γ.
15:
a ←the endpoint of γ not in X
16:
until all points in Ti lie above the line containing γ
2.2
Analyses
In Algorithm 1, Line 2 can be done in O(n) time using a single pass. Line 4 and 6
can be done in O(n) time using O(1) passes due to Lemma 1. Line 5 can be done
in O(s log s) time since we compute Si explicitly. Thus, the total running time
is O(n2/s + n log s + 
i τi), where τi is the running time of LowerTriangu-
lation(·, ·, Si).
Now consider Algorithm 2. Let ti be the number of updates of X for Si. Line 4
takes O(n) time using a single pass due to Lemma 1. Line 5 takes O(s log s + n)
time using a single pass. For Lines 6–12, we compute an edge of a triangulation
in each iteration. And each iteration takes O(1) time. Note that Line 12 can
be also done in O(1) time since it suﬃces to consider the boundaries of ch(Si)
and ch(X) locally. Thus Lines 6–12 can be done in O(n) time in total for all
Si’s. Lines 13–15 take O(n) time using O(1) passes. Therefore, for a ﬁxed i,
the running time of Algorithm 2, except Lines 6–12, is O(s log s + tin). Since
Lines 6–12 can be done in O(n) time for all indices i, the total running time of
Algorithm 1 is O(n2/s + n log s + 
i τi) = O(n2/s + n log s + n 
i ti).
We claim that the sum of ti over all i’s is O(n/s), which implies that
Algorithm 1 takes O(n2/s + n log s) time using O(n/s) passes. Assume that X
is set to A1, A2, . . . , Ak in order when we handle Si. No point in Aℓappears on
the lower chain of ch(Ti) for ℓ= 1, 2, . . . , k −1. Thus, no point in Aℓappears
on the lower chain of ch(Tj) for any j ≥i. Recall that X is set to a point set
whose the rightmost point appears on the lower chain of ch(Tj) when we handle
Sj. Therefore, no point in At is contained in X at any time after we handle Si
for t = 2, 3, . . . , k −1. Therefore, the sum of ti is O(n/s), and the total running
time is O(n2/s + n log s).

A Time-Space Trade-Oﬀfor Triangulations of Points in the Plane
9
Theorem 1. Given a set S of n points in the plane, we can report the edges of
a triangulation of S in O(n2/s+n log s) time using O(s) words of working space
and O(n/s) passes.
3
Reporting the Triangles with Adjacency Information
Let T be the triangulation of S computed by the algorithm in Sect. 2.1. In
this section, we show how to modify the algorithm to report the triangles of T
together with their adjacency information in addition to the edges of T . That is,
we report every pair (τ, τ ′) of the triangles of T such that τ and τ ′ are adjacent
to each other in T .
We say a triangle τ of T is an inner-slab triangle if all three corners of τ are
in the same vertical slab Si for some i = 1, . . . , ⌈n/s⌉. Otherwise, we say τ is a
cross-slab triangle. Note that if two inner-slab triangle are adjacent to each other
in T , they are contained in the same vertical slab. Moreover, for an inner-slab
triangle τ and a cross-slab triangle τ ′, we compute τ ′ after computing τ. In this
case, we report the adjacency between τ and τ ′ when we compute and report τ ′.
Reporting an inner-slab triangle. Consider an inner-slab triangle τ with
corners in Si for some i. Recall that we compute all points in Si explicitly, and
compute a triangulation of them. When we compute a triangulation of them, we
also report the triangles of it with their adjacency information. For a cross-slab
triangle τ ′ of T adjacent to τ, we will report their adjacency information when
we compute and report τ ′.
Reporting a cross-slab triangle. Consider a cross-slab triangle τ ′ whose
rightmost corner lies on Si for some i. This triangle comes from a triangulation
of Li∪Ui. We compute τ ′ while we traverse the boundaries of ch(Ti) and ch(Si).
A cross-slab triangle adjacent to τ ′ is also computed during this traversal, thus
the adjacency information between them can be computed during the traversal.
Each corner of τ ′ lies on the boundary of ch(Si) or the boundary of ch(Ti).
If two corners lie on the boundary of ch(Si), there is an inner-slab triangle
adjacent to τ ′ contained in Si. The adjacency information between them can be
computed without increasing the running time because we compute the convex
hull of Si explicitly.
Now assume that two corners a and b lie on the boundary of ch(Ti). Let
τ ′′ be the triangle of T incident to ab other than τ ′. To report the adjacency
information between τ ′ and τ ′′, we compute τ ′′ together with ab when we traverse
the boundary of ch(Ti). To do this, we specify a way to triangulate Li ∪Ui as
described in Algorithm 2.
For Li, we initially set a′ to the rightmost point of X and b′ to the leftmost
point of Si for each set X. Then we move a′ along the part of the boundary of
ch(X) appearing on ch(Ti) in clockwise direction as much as possible until a′b′
intersects the boundaries of ch(X) and ch(Si). Then we move b′ one step along

10
H.-K. Ahn et al.
the boundary of ch(Si) in counterclockwise direction, and move a′ again. We do
this until we all points in X ∪Si lie above the line containing a′b′. For Ui, we
can compute a triangulation similarly.
Then we have the following lemma.
Lemma 2. Given the convex hull of the set of points in Sj for some j =
1, . . . , ⌈n/s⌉, we can ﬁnd the lowest triangle of T contained in Li in O(n) time
using O(s) words of workspace and O(1) passes.
Proof. The lowest triangle of T contained in Li is incident to the lower bridge
of ch(Tj) and ch(Sj). By scanning the points of S once, we compute the lower
bridge of ch(Tj) and ch(Sj). Let a and b be the endpoints of the lower bridge
such that a ∈ch(Tj) and b ∈ch(Sj). Then, by scanning the points in S once
again, we compute the counterclockwise neighbor a′ of a along the boundary of
ch(Ti). Since we maintain the set Si explicitly, we can compute the clockwise
neighbor b′ of b along the boundary of ch(Si) in constant time without scanning
the points of S.
By construction, the triangle with corners a, b, b′ is the lowest triangle of T
contained in Li if b and b′ lie below the line passing through a and a′. Otherwise,
the triangle with corners a, a′, b is the lowest triangle of T contained in Li. In any
case, we can report the lowest triangle of T in Li using O(s) words of workspace
and O(1) passes.
⊓⊔
We modify Algorithm 2 as follows. For Line 4, we set X to the set of points
in Sj lying to the left of a for a ∈Sj using Lemma 3. Then we can obtain the
triangles of T incident to the part of the lower hull of X appearing on ch(Ti)
by applying the algorithm for triangulating Sj we use in Line 5 of Algorithm 1.
To compute the triangle of T incident to γ and contained in ch(Ti), we apply
Lemma 2.
Lemma 3. Given any point p ∈Sj for some j = 1, . . . , ⌈n/s⌉, we can compute
Si in O(n) time using O(s) words of workspace and O(1) passes.
Proof. For the ﬁrst pass, we compute the number of points in S lying to the left
of p. This determines the value of j with p ∈Sj. Then we ﬁnd the rightmost s
points lying to the left of p in O(n) time using a single pass by applying Lemma 1.
One of them is the leftmost point of Sj, and we can ﬁnd it in O(s) time. We can
compute Si using a single pass by applying Lemma 1 again.
⊓⊔
The following theorem summarizes this section.
Theorem 2. Given a set S of n points in the plane, we can report the triangles
of a triangulation of S with their adjacency information in O(n2/s + n log s)
time using O(s) words of working space and O(n/s) passes.

A Time-Space Trade-Oﬀfor Triangulations of Points in the Plane
11
4
Conclusion
In this paper, we present an s-workspace O(n2/s + s log n)-time algorithm for
computing a triangulation of a set of n points in the plane under the multi-pass
model. Our algorithm uses O(n/s) passes over the input array. It is not only the
ﬁrst algorithm for this problem under the multi-pass model, but it also improves
the previously best known random-access algorithm [12]. Moreover, its running
time is optimal under the multi-pass model.
One interesting open problem remaining from our work is whether our algo-
rithm can be improved under the random-access model. Under this model, the
best known lower bound is Ω(n2/(s log n) + n log(s log n)) for s ≤n/ log n.
References
1. Agarwal, P.K., Krishnan, S., Mustafa, N.H., Venkatasubramanian, S.: Streaming
geometric optimization using graphics hardware. In: Battista, G., Zwick, U. (eds.)
ESA 2003. LNCS, vol. 2832, pp. 544–555. Springer, Heidelberg (2003). doi:10.1007/
978-3-540-39658-1 50
2. Aronov, B., Korman, M., Pratt, S., van Ressen, A., Roeloﬀzen, M.: Time-space
trade-oﬀs for triangulating a simple polygon. In: Proceedings of the 15th Scandi-
navian Symposium and Workshops on Algorithm Theory (SWAT 2016), vol. 53,
pp. 30:1–30:12 (2016)
3. Asano, T., Kirkpatrick, D.: Time-space tradeoﬀs for all-nearest-larger-neighbors
problems. In: Dehne, F., Solis-Oba, R., Sack, J.-R. (eds.) WADS 2013. LNCS, vol.
8037, pp. 61–72. Springer, Heidelberg (2013). doi:10.1007/978-3-642-40104-6 6
4. Banyassady, B., Korman, M., Mulzer, W., van Renssen, A., Roeloﬀzen, M.,
Seiferth, P., Stein, Y.: Improved time-space trade-oﬀs for computing Voronoi dia-
grams. In: Proceedings of the 34th Symposium on Theoretical Aspects of Computer
Science (STACS 2017), vol. 66, pp. 9:1–9:14 (2017)
5. Barba, L., Korman, M., Langerman, S., Sadakane, K., Silveira, R.I.: Space-time
trade-oﬀs for stack-based algorithms. Algorithmica 72(4), 1097–1129 (2015)
6. Borodin, A., Cook, S.: A time-space tradeoﬀfor sorting on a general sequential
model of computation. SIAM J. Comput. 11(2), 287–297 (1982)
7. Chan, T.M., Chen, E.Y.: Multi-pass geometric algorithms. Discret. Comput. Geom.
37(1), 79–102 (2007)
8. Darwish, O., Elmasry, A.: Optimal time-space tradeoﬀfor the 2D convex-hull prob-
lem. In: Schulz, A.S., Wagner, D. (eds.) ESA 2014. LNCS, vol. 8737, pp. 284–295.
Springer, Heidelberg (2014). doi:10.1007/978-3-662-44777-2 24
9. Feigenbaum, J., Kannan, S., McGregor, A., Suri, S., Zhang, J.: On graph problems
in a semi-streaming model. In: D´ıaz, J., Karhum¨aki, J., Lepist¨o, A., Sannella, D.
(eds.) ICALP 2004. LNCS, vol. 3142, pp. 531–543. Springer, Heidelberg (2004).
doi:10.1007/978-3-540-27836-8 46
10. Feigenbaum, J., Kannan, S., McGregor, A., Suri, S., Zhang, J.: Graph distances in
the data-stream model. SIAM J. Comput. 38(5), 1709–1727 (2009)
11. Frederickson, G.N.: Upper bounds for time-space trade-oﬀs in sorting and selection.
J. Comput. Syst. 34(1), 19–26 (1987)
12. Korman, M., Mulzer, W., Renssen, A., Roeloﬀzen, M., Seiferth, P., Stein, Y.: Time-
space trade-oﬀs for triangulations and Voronoi diagrams. In: Dehne, F., Sack, J.-R.,
Stege, U. (eds.) WADS 2015. LNCS, vol. 9214, pp. 482–494. Springer, Cham (2015).
doi:10.1007/978-3-319-21840-3 40

12
H.-K. Ahn et al.
13. Munro, J., Paterson, M.: Selection and sorting with limited storage. Theoret. Com-
put. Sci. 12(3), 315–323 (1980)
14. Pagter, J., Rauhe, T.: Optimal time-space trade-oﬀs for sorting. In: Proceedings of
the 39th Annual Symposium on Foundations of Computer Science (FOCS 1998),
pp. 264–268 (1998)
15. Suri, S., Toth, C.D., Zhou, Y.: Range counting over multidimensional data streams.
Discret. Comput. Geom. 36(4), 633–655 (2006)

An FPTAS for the Volume of Some
V-polytopes—It is Hard to Compute
the Volume of the Intersection
of Two Cross-Polytopes
Ei Ando1(B) and Shuji Kijima2,3
1 Sojo University, 4-22-1, Ikeda, Nishi-Ku, Kumamoto 860-0082, Japan
ando-ei@cis.sojo-u.ac.jp
2 Kyushu University, 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan
kijima@inf.kyushu-u.ac.jp
3 JST PRESTO, 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan
Abstract. Given an n-dimensional convex body by a membership ora-
cle in general, it is known that any polynomial-time deterministic algo-
rithm cannot approximate its volume within ratio (n/ log n)n. There is
a substantial progress on randomized approximation such as Markov
chain Monte Carlo for a high-dimensional volume, and for many #P-
hard problems, while only a few #P-hard problems are known to yield
deterministic approximation. Motivated by the problem of deterministi-
cally approximating the volume of a V-polytope, that is a polytope with
a small number of vertices and (possibly) exponentially many facets, this
paper investigates the problem of computing the volume of a “knapsack
dual polytope,” which is known to be #P-hard due to Khachiyan (1989).
We reduce an approximate volume of a knapsack dual polytope to that
of the intersection of two cross-polytopes, and give FPTASs for those
volume computations. Interestingly, computing the volume of the inter-
section of two cross-polytopes (i.e., L1-balls) is #P-hard, unlike the cases
of L∞-balls or L2-balls.
Keywords: #P-hard · Deterministic approximation · FPTAS ·
V-polytope · Intersection of L1-balls
1
Introduction
1.1
Approximation of a High Dimensional Volume: Randomized
vs. Deterministic
A high dimensional volume is hard to compute, even for approximation.
When an n-dimensional convex body is given by a membership oracle, no
polynomial-time deterministic algorithm can approximate its volume within
ratio (n/ log n)n [3,6,10,21]. The impossibility comes from the fact that the
volume of an n-dimensional L∞-ball (i.e., hypercube) is exponentially large to
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 13–24, 2017.
DOI: 10.1007/978-3-319-62389-4 2

14
E. Ando and S. Kijima
the volume of its inscribed L2-ball or L1-ball, despite that the L2-ball (L1-ball as
well) is convex and touches each facet of the L∞-ball (see e.g., [23]). Lov´asz said
in [21] for a convex body K that “If K is a polytope, then there may be much
better ways to compute Vol(K).” Unfortunately, computing an exact volume is
often #P-hard, even for a relatively simple polytope. For instance, computing
the volume of a knapsack polytope K(b) = {x ∈[0, 1]n| n
i=1 aixi ≤b}, where
a1, . . . , an ∈Z≥0 is the “item sizes” and b ∈Z≥0 is the “knapsack capacity”, is
a well-known #P-hard problem [8].
The diﬃculty caused by the exponential gap between L∞-ball and L1-
ball also does harm a simple Monte Carlo algorithm. Then, the Markov chain
Monte Carlo (MCMC) method achieves a great success for approximating the
high dimensional volume. Dyer, Frieze and Kannan [9] gave the ﬁrst fully
polynomial-time randomized approximation scheme (FPRAS) for the volume
computation of a general convex body1. They employed a grid-walk, which is
eﬃciently implemented with a membership oracle, and showed it is rapidly mix-
ing, then they gave an FPRAS runs in O∗(n23) time where O∗ignores poly(log n)
and 1/ϵ factors. After several improvements, Lov´asz and Vempala [22] improved
the time complexity to O∗(n4) in which they employ hit-and-run walk, and
recently Cousins and Vempala [5] gave an O∗(n3)-time algorithm. Many ran-
domized techniques, including MCMC, also have been developed for designing
FPRAS for #P-hard problems.
In contrast, the development of deterministic approximations for #P-hard
problems is a current challenge, and not many results seem to be known. A
remarkable progress is the correlation decay argument due to Weitz [25]; he
designed a fully polynomial time approximation scheme (FPTAS) for count-
ing independent sets in graphs whose maximum degree is at most 5. A similar
technique is independently presented by Bandyopadhyay and Gamarnik [2], and
there are several recent developments on the technique, e.g., [4,11,17,18,20]. For
counting knapsack solutions2, Gopalan, Klivans and Meka [12], and ˇStefankoviˇc,
Vempala and Vigoda [24] gave deterministic approximation algorithms based on
dynamic programming (see also [13]), in a similar way to the simple random
sampling algorithm by Dyer [7]. (He showed a deterministic dynamic program-
ming and a random sampling algorithm in [7].) Modifying dynamic programming
in [24], Li and Shi [19] gave an FPTAS that can approximate the volume of a
knapsack polytope. Their algorithm runs in O((n3/ϵ2)poly log b) time where b is
the knapsack capacity. Motivated by a diﬀerent approach, Ando and Kijima [1]
gave another FPTAS for the volume of a knapsack polytope.
1 Precisely, they are concerned with a “well-rounded” convex body, after an aﬃne
transformation of a general ﬁnite convex body.
2 Given a ∈Zn
>0 and b ∈Z>0, the problem is to compute |{x ∈{0, 1}n | n
i=1 aixi ≤
b}|. Remark that it is computed in polynomial time when all the inputs ai (i =
1, . . . , n) and b are bounded by poly(n), using a version of the standard dynamic
programming for knapsack problem (see e.g., [7,13]). It should be worth noting that
[12,24] needed special techniques, diﬀerent from ones for optimization problems, to
design FPTASs for the counting problem.

An FPTAS for the Volume of Some V-Polytopes
15
Their scheme is based on a classical approximate convolution and runs in
O(n3/ϵ) time. The running time is independent of the size of items and the
knapsack capacity if we assume that the basic arithmetic operations can be
performed in constant time.
1.2
H-polytope and V-polytope
An H-polyhedron is an intersection of ﬁnitely many closed half-spaces in Rn.
An H-polytope is a bounded H-polyhedron. A V-polytope is a convex hull of
a ﬁnite point set in Rn [23]. From the view point of computational complexity,
a major diﬀerence between an H-polytope and a V-polytope is the measure of
their ‘input size.’ An H-polytope given by linear inequalities deﬁning half-spaces
may have vertices exponentially many to the number of the inequalities, e.g., an
n-dimensional hypercube is given by 2n linear inequalities as an H-polytope,
and has 2n vertices. In contrast, a V-polytope given by a point set may have
facets exponentially many to the number of vertices, e.g., an n-dimensional cross-
polytope (that is an L1-ball, in fact) is given by a set of 2n points as a V-polytope,
and it has 2n facets.
There
are
many
interesting
properties
between
H-polytope
and
V-
polytope [23]. A membership query is polynomial time for both H-polytope
and V-polytope. It is still unknown about the complexity of a query if a given
pair of V-polytope and H-polytope are identical. Linear programming (LP) on a
V-polytope is trivially polynomial time since it is suﬃcient to check the objective
value of all vertices and hence LP is usually concerned with an H-polytope.
1.3
Volume of V-polytope
Motivated by a hardness of the volume computation of a V-polytope,
Khachiyan [15] is concerned with the following V-polytope: Suppose a vector
a = (a1, . . . , an) ∈Zn
≥0 is given. Then let
Pa
def
= conv {±e1, . . . , ±en, a}
(1)
where e1, . . . , en are the standard basis vectors in Rn. This paper calls Pa knap-
sack dual polytope3. Khachiyan [15] showed that computing Vol(Pa) is #P-hard4.
The hardness is given by a Cook reduction from counting set partitions, of which
the decision version is a celebrated weakly NP-hard problem. It is not known if
3 See [23] for the duality of polytopes. In fact, Pa itself is not the dual of a knapsack
polytope in a canonical form, but it is obtained by an aﬃne transformation from
a dual of knapsack polytope under some assumptions. Khachiyan [16] says that
computing Vol(Pa) ‘is “polar” to determining the volume of the intersection of a
cube and a halfspace.’ .
4 If all ai (i = 1, . . . , n) are bounded by poly(n), it is computed in polynomial time,
so did the counting knapsack solutions. See also footnote 1 for counting knapsack
solutions.

16
E. Ando and S. Kijima
we can have an eﬃcient approximation algorithm for computing Vol(Pa) imme-
diately from the approximation algorithm in e.g., [1] by exploiting that Pa is a
dual of a knapsack polytope.
1.4
Contribution
Motivated by a development of techniques for deterministic approximation of
the volumes of V-polytopes, this paper investigates the knapsack dual polytope
Pa given by (1). The main goal of the paper is to establish the following theorem.
Theorem 1. For any ϵ (0 < ϵ < 1), there exists a deterministic algorithm that
outputs a value V satisfying (1 −ϵ)Vol(Pa) ≤V ≤(1 + ϵ)Vol(Pa) in O(n10ϵ−6)
time.
As far as we know, this is the ﬁrst result on designing an FPTAS for computing
the volume of a V-polytope which is known to be #P-hard. We also discuss some
topics related to the volume of V-polytopes appearing in the proof process. Let
us brieﬂy explain the outline of the paper.
Technique/Organization. The ﬁrst step for Theorem 1 is a transformation of the
approximation problem to another one: An approximate volume of Pa is reduced
to the volume of a union of geometric sequence of cross-polytopes (Sect. 3.1),
and then it is reduced to the volume of the intersection of two cross-polytopes
(Sect. 3.2). We remark that the former reduction is just for approximation, and
is useless for proving #P-hardness. A technical point of this step is that the
latter reduction is based on a subtraction—if you are familiar with an approxi-
mation, you may worry that a subtraction may destroy an approximation ratio5.
It requires careful tuning of a parameter (β in Sect. 3) which plays conﬂicting
functions in Sects. 3.1 and 3.2: the larger β, the better approximation in Sect. 3.1,
while the smaller β, the better in Sect. 3.2. Then, Sect. 3.3 claims by giving an
appropriate β that if we have an FPTAS for the volume of an intersection of
two cross-polytopes then we have an FPTAS of Vol(Pa).
Section 4 shows an FPTAS for the volume of the intersection of two cross-
polytopes (i.e., L1-balls). The scheme is based on a modiﬁed version of the
technique developed in [1], which is based on a classical approximate convolution.
At a glance, the volume of the intersection of two-balls may seem easy. It is
true for two L∞-balls (i.e., axis-aligned hypercubes), or L2-balls (i.e., Euclidean
balls). However, we show in Sect. 5 that computing the volume of the intersection
of cross-polytopes is #P-hard. Intuitively, this interesting fact may come from
the fact that the V-polytope, meaning that an n-dimensional cross-polytope, has
2n facets. In Sect. 6, we extend the technique in Sect. 4 to the intersection of any
constant number of cross-polytopes.
5
Suppose you know that x is approximately 49 within 1% error. Then, you know
that x+50 is approximately 99 within 1% error. However, it is diﬃcult to say 50−x
is approximately 1. Even when additionally you know that x does not exceed 50,
50 −x may be 2, 1, 0.1 or smaller than 0.001, meaning that the approximation ratio
is unbounded.

An FPTAS for the Volume of Some V-Polytopes
17
2
Preliminaries
This section presents some notation. Let conv(S) denote the convex hull of S ⊆
Rn, where S is not restricted to a ﬁnite point set. A cross-polytope C(c, r) of
radius r ∈R>0 centered at c ∈Rn is given by
C(c, r)
def
= conv{c ± rei i = 1, . . . , n}
where e1, . . . , en are the standard basis vectors in Rn. Clearly, C(c, r) has 2n
vertices. In fact, C(c, r) is an L1-ball in Rn described by
C(c, r) = {x ∈Rn | ∥x −c∥1 ≤r} = {x ∈Rn | ⟨x −c, σ⟩≤r (∀σ ∈{−1, 1}n)}
where ∥u∥1 = n
i=1 |ui| for u = (u1, . . . , un) ∈Rn and ⟨u, v⟩= n
i=1 uivi for
u, v ∈Rn. Note that C(c, r) has 2n facets. It is not diﬃcult to see that the
volume of a cross-polytope in n-dimension is Vol(C(c, r)) = 2n
n! rn for any r ≥0
and c ∈Rn, where Vol(S) for S ⊆Rn denotes the (n-dimensional) volume of S.
3
FPTAS for Knapsack Dual Polytope
This section reduces an approximation of Vol(Pa) to that of the intersection of
two cross-polytopes. In Sect. 4, we will give an FPTAS for the volume of the
intersection of two cross-polytopes, accordingly we obtain Theorem 1.
3.1
Reduction to a Geometric Series of Cross-Polytopes
Let β be a parameter6 satisfying 0 < β < 1, and let Q0, Q1, Q2, . . . be a sequence
of cross-polytopes deﬁned by
Qk
def
= C((1 −βk)a, βk)
(2)
for k = 0, 1, 2, . . .. Remark that Q0 = C(0, 1), Q1 = C((1 −β)a, β), Q∞=
C(a, 0) = {a}. The goal of Sect. 3.1 is to establish the following. Here 1 ± ϵ is
the ﬁnal relative approximation ratio that we aim to achieve.
Lemma 1. Let ϵ satisfy 0 < ϵ < 1. If 1 −β ≤
c1ϵ
n∥a∥1
where 0 < c1 < 1, then
(1 −c1ϵ)Vol(Pa) ≤Vol (∞
k=0 Qk) ≤Vol(Pa).
Figure 1 illustrates the approximation of Pa by this inﬁnite sequence of cross-
polytopes. The second inequality in Lemma 1 is relatively easy by the following
lemma. 7
Lemma 2. ∞
k=0 Qk ⊆Pa
To prove the ﬁrst inequality in Lemma 1, we need the following lemmas.
Lemma 3. ∞
k=0 conv(Qk ∪Qk+1) ∪{a} ⊇Pa
Lemma 4. If 1 −β ≤
c1ϵ
n∥a∥1
, then Vol (∞
k=0 Qk) ≥(1 −c1ϵ)Vol(Pa).
6 We will set β = 1 −
ϵ
2n∥a∥1 , later.
7 Most of the proofs cannot be included due to the space limit.

18
E. Ando and S. Kijima
Fig. 1. Approximating Pa by an inﬁnite sequence of cross-polytopes.
3.2
Reduction to the Intersection of Two Cross-Polytopes
We here claim the following.
Lemma 5. Vol (∞
k=0 Qk) =
1
1−βn
 2n
n! −Vol(Q1 ∩Q0)

The ﬁrst step of the proof is the following recursive formula.
Lemma 6. m
k=0 Qk =
 ˙m−1
k=0 Qk \ Qk+1

˙∪Qm where A ˙∪B denotes the dis-
joint union of A and B, meaning that A ˙∪B = A ∪B and A ∩B = ∅.
The second step is the following lemma.
Lemma 7. Vol(Qk \ Qk+1) = βnkVol(Q0 \ Q1).
By using Lemmas 6 and 7, we can prove Lemma 5 as follows.
Proof (Proof of Lemma 5).
Vol
	 ∞

k=0
Qk

= Vol
 ˙
∞
k=0Qk \ Qk+1

˙∪Q∞

=
∞

k=0
Vol(Qk \ Qk+1) + Vol(Q∞) =
∞

k=0
βnkVol(Q0 \ Q1)
=
1
1 −βn Vol(Q0 \ Q1) =
1
1 −βn
2n
n! −Vol(Q1 ∩Q0)

⊓⊔
A reader who are familiar with approximation may worry about the subtrac-
tion 2n
n! −Vol(Q0 ∩Q1) in Lemma 5. We claim the following.
Lemma 8. If 1 −β ≥
c2ϵ
n∥a∥1
and 0 < c2ϵ < 1, then Vol(Q0 ∩Q1) ≤
1
1 + c2ϵ
2n
2n
n! .
Intuitively, Lemma 8 implies that 2n
n! −Vol(Q0 ∩Q1) is large enough, and an
approximation of Vol(Q0 ∩Q1) provides a good approximation of Vol(∞
k=0 Qk),
and hence Vol(Pa).

An FPTAS for the Volume of Some V-Polytopes
19
3.3
Approximation Algorithm and Analysis
Based on Lemma 1 in Sect. 3.1 and Lemma 5 in Sect. 3.2, we give an FPTAS for
Vol(Pa) where we assume an algorithm to approximate Vol(Q0 ∩Q1).
Algorithm 1 ( (1 ± ϵ)-approximation (0 < ϵ ≤1/2))
Input: a ∈Zn
+;
1. Set parameter β := 1 −
ϵ
2n∥a∥1
;
2. Approximate I
def
= Vol(C(0, 1) ∩C((1 −β)a, β)) by Z such that
I ≤Z ≤

1 + ϵ2
4n

I;
3. Output V =
1+ϵ
1−βn
 2n
n! −Z

.
Lemma 9. The output V of Algorithm 1 satisﬁes
(1 −ϵ) Vol(Pa) ≤V ≤(1 + ϵ)Vol(Pa).
4
The Volume of the Intersection of Two Cross-Polytopes
This section gives an FPTAS for the volume of the intersection of two cross-
polytopes in the n-dimensional space. Without loss of generality8, we are con-
cerned with Vol(C(0, 1) ∩C(c, r)) for c ≥0 and r (0 < r ≤1). This section
establishes the following.
Theorem 2. For any δ (0 < δ < 1), there exists a deterministic algorithm which
outputs a value Z satisfying Vol(C(0, 1) ∩C(c, r)) ≤Z ≤(1 + δ)Vol(C(0, 1) ∩
C(c, r)) for any input c ≥0 and r (0 < r ≤1) satisfying ∥c∥1 ≤r, and runs in
O(n7δ−3) time.
The assumption that ∥c∥1 ≤r implies both centers 0 and c are contained in
the intersection C(0, 1)∩C(c, r). Note that the assumption does not harm to our
main goal Theorem 1 (recall Algorithm 1 in Sect. 3.3). We show in Sect. 5 that
Computing Vol(C(0, 1) ∩C(c, r)) remains #P-hard even on the assumption.
4.1
Preliminaries: Convolution for the Volume
As a preliminary step, Sect. 4.1 gives a convolution which provides Vol(C(0, 1)∩
C(c, r)). Let Ψ0 : R2 →R be given by Ψ0(u, v) = 1 if u ≥0 and v ≥0, otherwise
Ψ0(u, v) = 0. Inductively, we deﬁne Ψi : R2 →R for i = 1, 2, . . . , n by
Ψi(u, v)
def
=
 1
−1
Ψi−1(u −|s|, v −|s −ci|)ds
(3)
for u, v ∈R. We remark that Ψi(u, v) = 0 holds if u ≤0 or v ≤0, for any
i = 1, 2, . . . , n by the deﬁnition.
8 Remark that Vol(C(c, r) ∩C(c′, r′)) = rnVol

C(0, 1) ∩C

(c−c′)+
r
, r′
r

holds for
any c, c′ ∈Rn and r, r′ ∈R>0, where (c −c′)+ = (|c1 −c′
1|, |c2 −c′
2|, . . . , |cn −c′
n|).

20
E. Ando and S. Kijima
Lemma 10. Ψn(1, r) = Vol(C(0, 1) ∩C(c, r))
To prove Lemma 10, it might be helpful to introduce a probability space.
Let X = (X1, . . . , Xn) be a uniform random variable over [−1, 1]n, i.e., Xi
(i = 1, . . . , n) are mutually independent. Then, Pr [X ∈C(0, 1) ∩C(c, r)] =
Vol(C(0,1)∩C(c,r))
Vol([−1,1]n)
=
1
2n Vol(C(0, 1) ∩C(c, r)) holds.
Lemma 11. For any u, v ∈R and any i = 1, 2, . . . , n,
1
2i Ψi(u, v) = Pr
⎡
⎣
⎛
⎝
i

j=1
|Xj| ≤u
⎞
⎠∧
⎛
⎝
i

j=1
|Xj −cj| ≤v
⎞
⎠
⎤
⎦.
Now, Lemma 10 is easy from Lemma 11.
4.2
Idea for Approximation
Our FPTAS is based on an approximation of Ψi(u, v). Let G0(u, v) = Ψ0(u, v)
for any u, v ∈R, i.e., G0(u, v) = 1 if u ≥0 and v ≥0, otherwise G0(u, v) = 0.
Inductively assuming Gi−1(u, v), we deﬁne
Gi(u, v)
def
=
 1
−1
Gi−1(u −|s|, v −|s −ci|)ds
(4)
for u, v ∈R, for convenience. Then, let Gi(u, v) be a staircase approximation of
Gi(u, v), given by
Gi(u, v)
def
=
⎧
⎪
⎨
⎪
⎩
Gi
 1
M k, r
M ℓ

	
if
1
M (k −1) < u ≤
1
M k (k = 1, 2, . . .), and
r
M (ℓ−1) < v ≤
r
M ℓ(ℓ= 1, 2, . . .).

0
(otherwise)
(5)
for any u, v ∈R. Thus, we remark that
Gi(u, v) = Gi
 1
M ⌈Mu⌉, r
M
 M
r v

(6)
holds for any u, v ∈R, by the deﬁnition. Section 4.3 will show that Gi(u, v)
approximates Ψi(u, v) well.
In the rest of Sect. 4.2, we brieﬂy comment on the computation of Gi. First,
remark that (4) implies that Gi(u, v) is computed only from Gi−1(u′, v′) for
u′ ≤u and v′ ≤v, i.e., we do not need to know Gi−1(u′, v′) for u′ > u or v′ > v.
Second, remark (6) implies that Gi(u, v) for u ≤1 and v ≤r takes (at most)
(M + 1)2 diﬀerent values. Precisely, let
Γ
def
=
 1
M (k, rℓ) | k = 0, 1, 2, . . . , M, ℓ= 0, 1, 2, . . . , M


An FPTAS for the Volume of Some V-Polytopes
21
then Gi(u, v) for (u, v) ∈Γ provides all possible values of Gi(u, v) for u ≤1 and
v ≤r, since (6).
Then, we explain how to compute Gi(u, v) for (u, v) ∈Γ from Gi−1. For an
arbitrary (u, v) ∈Γ, let
S(u)
def
=
 
s ∈[−1, 1] | u −|s| =
1
M k (k = 0, 1, 2, . . . , M)
!
=
 
s ∈[−1, 1] | s = ±(u −
1
M k) (k = 0, 1, 2, . . . , M)
!
,
let
Si(v)
def
=
 
s ∈[−1, 1] | v −|s −ci| =
r
M ℓ(ℓ= 0, 1, 2, . . . , M)
!
=
 
s ∈[−1, 1] | s = ci ± (v −
r
M ℓ) (ℓ= 0, 1, 2, . . . , M)
!
,
and let Ti(u, v)
def
= S(u) ∪Si(v) ∪{−1, 0, ci, 1}. Suppose t0, t1, . . . , tm be an
ordering of all elements of Ti(u, v) such that ti ≤ti+1 for any i = 0, 1, . . . , m,
where m = |Ti(u, v)|. Then, we can compute Gi(u, v) for any (u, v) ∈Γ by
Gi(u, v) = Gi(u, v), which can be transformed into
Gi(u, v) =
 1
−1
Gi−1(u −|s|, v −|s −ci|)ds
=
m−1

j=0
(tj+1 −tj)Gi−1
 1
M ⌈M(u −|tj+1|)⌉, r
M
 M
r (v −|tj+1 −ci|)

(7)
where we remark again that the terms of (7) consist of Gi−1(u, v) for (u, v) ∈Γ.
4.3
Algorithm and Analysis
Based on the arguments in Sect. 4.2, our algorithm is described as follows.
Algorithm 2 (for (1 + δ)-approximation (0 < δ ≤1))
Input: c ∈Qn
≥0, r ∈Q (0 ≤r ≤1);
1. Set M := ⌈4n2δ−1⌉;
2. Set G0(u, v) := 1 for (u, v) ∈Γ, otherwise G0(u, v) := 0;
3. For i := 1, . . . , n,
4.
For (u, v) ∈Γ,
5.
Compute Gi(u, v) from Gi−1 by (7);
6. Output Gn(1, r).
Lemma 12. The running time of Algorithm 2 is O(n7δ−3).
Theorem 2 is immediate from Lemma 12 and the following Lemma 13.
Lemma 13. Ψn(1, r) ≤Gn(1, r) ≤(1 + δ)Ψn(1, r).
The proof sketch of Lemma 13 is the following.

22
E. Ando and S. Kijima
Proof (Proof Sketch of Lemma 13). The ﬁrst inequality is immediate. Then, we
show the latter inequality. We can prove that
Ψn(1, r)
Ψn(1 + n
M , r(1 + n
M )) ≥

M
M + n
2n
=

1
1 + n
M
2n
≥

1 −n
M
2n
≥

1 −δ
4n
2n
≥1 −2n δ
4n = 1 −δ
2.
Then,
Ψn(1+ n
M ,r(1+ n
M ))
Ψn(1,r)
≤
1
1−δ
2 ≤1 + δ for any δ ≤1, and we obtain the
claim.
⊓⊔
5
Hardness of the Volume of the Intersection of Two
Cross-Polytopes
This section establishes the following.
Theorem 3. Given a vector c ∈Zn
>0 and integers r1, r2 ∈Z>0, computing the
volume of C(0, r1)∩C(c, r2) is #P-hard, even when each cross-polytopes contains
the center of the other one, i.e., 0 ∈C(c, r2) and c ∈C(0, r1).
The proof of Theorem 3 is a reduction of counting set partitions, which is a
well-known #P-hard problem. To be precise, we reduce the following problem,
which is a version of counting set partition (for the #P-hardness of counting set
partition, see e.g., [14]).
Problem 1 (#LARGE SET). Given an integer vector a ∈Zn
>0 such that ∥a∥1 is
even, meaning that ∥a∥1/2 is an integer, the problem is to compute
|{σ ∈{−1, 1}n | ⟨σ, a⟩> 0}| .
(8)
Note that |{σ ∈{−1, 1}n | ⟨σ, a⟩= 0}| =
"""
#
S ⊆{1, . . . , n} | 
i∈S ai = ∥a∥1
2
$"""
holds: if σ ∈{−1, 1}n satisﬁes ⟨σ, a⟩= 0, then let S ⊆{1, . . . , n} be the set
of indices of σi = 1 then 
i∈S ai = ∥a∥1/2 holds. Using the following simple
observation, we see that Problem 1 is equivalent to counting set partitions.
Observation 1. For any σ ∈{−1, 1}n, ⟨σ, a⟩> 0 if and only if ⟨−σ, a⟩< 0.
By Observation 1, we see that |{σ ∈{−1, 1}n | ⟨σ, a⟩= 0}| is equal to 2n −
2 |{σ ∈{−1, 1}n | ⟨σ, a⟩> 0}|.
In the following, let a ∈Zn
>0 be an instance of Problem 1. Roughly speaking,
our proof of Theorem 3 claims that the volume of (C(δa, 1)∩C(0, 1+ϵ))\C(0, 1)
is proportional to the answer of #LARGE SET when 0 < ϵ < δ ≪1/∥a∥1. If
we could compute the volume of the intersection of two cross-polytopes exactly,
then we obtain Vol((C(δa, 1) ∩C(0, 1 + ϵ)) \ C(0, 1)) = Vol(C(δa, 1) ∩C(0, 1 +
ϵ)) −Vol(C(δa, 1) ∩C(0, 1)), which would solve #LARGE SET.

An FPTAS for the Volume of Some V-Polytopes
23
6
Intersection of a Constant Number of Cross-Polytopes
Let pi ∈Rn, ri ∈R≥0 and C(pi, ri) for i = 1, . . . , k, where C(p, r) is a cross-
polytope (L1-ball) with center p ∈Rn and radius r ∈R≥0. Then, we are to
compute the following polytope given by S(Π, r) = %k
i=1 C(pi, ri), where Π is
an n × k matrix Π = (p1, . . . , pk) and r = (r1, . . . , rk). For the analysis, we
assume that p1, . . . , pk are internal points of S(Π, r).
Theorem 4. There is an algorithm that outputs an approximation Z of
Vol(S(Π, r)) in O(kk+2n2k+3/δk+1) time satisfying Vol(S(Π, r))
≤
Z
≤
(1 + δ)Vol(S(Π, r)).
7
Conclusion
Motivated by the problem of deterministically approximating the volume of a
V-polytope, this paper gave an FPTAS for the volume of the knapsack dual
polytope Vol(Pa). In the process, we showed that computing the volume of the
intersection of L1-balls is #P-hard, and gave an FPTAS. As we remarked, the
volume of the intersection of two Lq-balls are easy for q = 2, ∞. The complexity
of the volume of the intersection of two Lq-balls for other q > 0 is interesting.
The problem seems diﬃcult even for approximation in the case of q ∈(0, 1),
since Lq-ball is no longer convex. Our FPTAS for the intersection of two cross-
polytopes assumes that each cross-polytope contains the center of the other one.
It is open if an FPTAS exists without the assumption.
Acknowledgments. This work is partly supported by Grant-in-Aid for Scientiﬁc
Research on Innovative Areas MEXT Japan “Exploring the Limits of Computation
(ELC)” (No. 24106008, 24106005) and by JST PRESTO Grant Number JPMJPR16E4,
Japan.
References
1. Ando, E., Kijima, S.: An FPTAS for the volume computation of 0–1 knapsack poly-
topes based on approximate convolution. Algorithmica 76(4), 1245–1263 (2016)
2. Bandyopadhyay, A., Gamarnik, D.: Counting without sampling: asymptotics of
the log-partition function for certain statistical physics models. Random Struct.
Algorithms 33, 452–479 (2008)
3. B´ar´any, I., F¨uredi, Z.: Computing the volume is diﬃcult. Discrete Comput. Geom.
2, 319–326 (1987)
4. Bayati, M., Gamarnik, D., Katz, D., Nair, C., Tetali, P.: Simple deterministic
approximation algorithms for counting matchings. In: Proceedings of STOC 2007,
pp. 122–127 (2007)
5. Cousins, B., Vempala, S., Bypassing, K.L.S.: Gaussian cooling and an O∗(n3) vol-
ume algorithm. In: Proceedings of STOC 2015, pp. 539–548 (2015)
6. Dadush, D., Vempala, S.: Near-optimal deterministic algorithms for volume com-
putation via M-ellipsoids. Proc. Natl. Acad. Sci. USA 110(48), 19237–19245 (2013)

24
E. Ando and S. Kijima
7. Dyer, M.: Approximate counting by dynamic programming. In: Proceedings of
STOC 2003, pp. 693–699 (2003)
8. Dyer, M., Frieze, A.: On the complexity of computing the volume of a polyhedron.
SIAM J. Comput. 17(5), 967–974 (1988)
9. Dyer, M., Frieze, A., Kannan, R.: A random polynomial-time algorithm for approx-
imating the volume of convex bodies. J. Assoc. Comput. Mach. 38(1), 1–17 (1991)
10. Elekes, G.: A geometric inequality and the complexity of computing volume. Dis-
crete Comput. Geom. 1, 289–292 (1986)
11. Gamarnik, D., Katz, D.: Correlation decay and deterministic FPTAS for counting
list-colorings of a graph. In: Proceedings of SODA 2007, pp. 1245–1254 (2007)
12. Gopalan, P., Klivans, A., Meka, R.: Polynomial-time approximation schemes
for
knapsack
and
related
counting
problems
using
branching
programs.
arXiv:1008.3187v1 (2010)
13. Gopalan, P., Klivans, A., Meka, R., ˇStefankoviˇc, D., Vempala, S., Vigoda, E.: An
FPTAS for #knapsack and related counting problems. In: Proceedings of FOCS
2011, pp. 817–826 (2011)
14. Karp, R.: Reducibility among combinatorial problems. In: Miller, R.E., Thatcher,
J.W. (eds.) Complexity of Computer Computations, pp. 85–103. Plenum Press,
New York (1972)
15. Khachiyan, L.: The problem of computing the volume of polytopes is #P-hard.
Uspekhi Mat. Nauk. 44, 199–200 (1989)
16. Khachiyan, L.: Complexity of polytope volume computation. In: Pach, J. (ed.)
New Trends in Discrete and Computational Geometry, pp. 91–101. Springer, Berlin
(1993)
17. Li, L., Lu, P., Yin, Y.: Approximate counting via correlation decay in spin systems.
In: Proceedings of SODA 2012, pp. 922–940 (2012)
18. Li, L., Lu, P., Yin, Y.: Correlation decay up to uniqueness in spin systems. In:
Proceedings of SODA 2013, pp. 67–84 (2013)
19. Li, J., Shi, T.: A fully polynomial-time approximation scheme for approximating
a sum of random variables. Oper. Res. Lett. 42, 197–202 (2014)
20. Lin, C., Liu, J., Lu, P.: A simple FPTAS for counting edge covers. In: Proceedings
of SODA 2014, pp. 341–348 (2014)
21. Lov´asz, L.: An Algorithmic Theory of Numbers, Graphs and Convexity. Applied
Mathematics. SIAM Society for Industrial, Philadelphia (1986)
22. Lov´asz, L., Vempala, S.: Simulated annealing in convex bodies and an O∗(n4)
volume algorithm. J. Comput. Syst. Sci. 72, 392–417 (2006)
23. Matou˘sek, J.: Lectures on Discrete Geometry. Springer, New York (2002)
24. ˇStefankoviˇc, D., Vempala, S., Vigoda, E.: A deterministic polynomial-time approx-
imation scheme for counting knapsack solutions. SIAM J. Comput. 41(2), 356–366
(2012)
25. Weitz, D.: Counting independent sets up to the tree threshold. In: Proceedings of
STOC 2006, pp. 140–149 (2006)

Local Search Strikes Again: PTAS for Variants
of Geometric Covering and Packing
Pradeesha Ashok1(B), Aniket Basu Roy2(B), and Sathish Govindarajan2(B)
1 International Institute of Information Technology, Bangalore, India
pradeesha@iiitb.ac.in
2 Computer Science and Automation, Indian Institute of Science, Bangalore, India
{aniket.basu,gsat}@csa.iisc.ernet.in
Abstract. Geometric Covering and Packing problems have been exten-
sively studied in the last few decades and have applications in diverse
areas. Several variants and generalizations of these problems have been
studied recently. In this paper, we look at the following covering vari-
ants where we require that each point is “uniquely” covered, i.e., it is
covered by exactly one object: Unique Coverage problem, where we want
to maximize the number of uniquely covered points and Exact Cover
problem, where we want to uniquely cover every point and minimize the
number of objects used for covering. We also look at the following gener-
alizations: Multi-Cover problem, a generalization of Set Cover, where we
want to select the minimum subset of objects with the constraint that
each input point is covered by at least k objects in the solution. And
Shallow Packing problem, a generalization of Packing problem, where we
want to select the maximum subset of objects with the constraint that
any point in the plane is contained in at most k objects in the solution.
The above problems are NP-hard even for unit squares in the plane.
Thus, the focus has been on obtaining good approximation algorithms.
Local Search have been quite successful in the recent past in obtain-
ing good approximation algorithms for a wide variety of problems. We
consider the Unique Coverage and Multi-Cover problems on non-piercing
objects, which is a broad class that includes squares, disks, pseudo-disks,
etc. and show that the local search algorithm yields a PTAS approxima-
tion under the assumption that the depth of the input points is at most a
constant. For the Shallow Packing problem, we show that the local search
algorithm yields a PTAS approximation for objects with sub-quadratic
union complexity, which is a very broad class of objects that even includes
non-piercing objects. For the Exact Cover problem, we show that ﬁnding
a feasible solution is NP-hard even for unit squares in the plane, thus
negating the existence of polynomial time approximation algorithms.
Keywords: Packing · Covering · PTAS · Local search · Non-piercing
regions
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 25–37, 2017.
DOI: 10.1007/978-3-319-62389-4 3

26
P. Ashok et al.
1
Introduction
Set Covering and Packing problems form a class of well studied problems in
theoretical computer science. Given a universe U of n elements and a family F
of subsets of U, set cover problem asks for a subset of sets in F such that every
element in U is contained in at least one of the chosen sets and the number
of chosen sets is minimized. An element is said to be covered by a set if it is
contained in that set. Similarly, Set Packing problem asks for subset of sets in
F such that every element is contained in at most one of the chosen subset and
the number of chosen sets is maximized. An important variant of these problems
is when the universe is deﬁned by set of points in Rd and the sets are subsets
of points contained in geometric objects. Apart from the fact that geometric
variants have a lot of real world applications, geometric setting usually leads to
improved algorithms also.
We study variants of geometric covering and packing problems in the context
of approximation algorithms. We look at variants of covering where points are
uniquely covered i.e., a point is contained in exactly one of the chosen sets.
Speciﬁcally, we look at the following variants:
Let U be a set of n points in the plane and F be a family of subsets of U
deﬁned by a class of geometric objects.
1. Unique Coverage: Select a subset F′ ⊆F such that the number of uniquely
covered points in U is maximized.
2. Exact Cover: Select a subset F′ ⊆F such that all points in U are uniquely
covered and the cardinality of F′ is minimized.
We also look into a generalization of the geometric covering problem called
Multi-Cover where each point has an integer demand associated with it.
Multi Cover: Select a subset F′ ⊆F such that each point in U is contained in
at least as many sets in F′ as its demand and the cardinality of F′ is minimized.
Further we also study a generalization of geometric packing called Shallow
Packing:
Shallow Packing: Select a subset F′ ⊆F such that every point in U is
contained in at most k sets in F′ and the cardinality of F′ is maximized.
We study the Unique Coverage, Exact Cover, and Multi Cover prob-
lems for a class of geometric objects called non-piercing regions1 and Shallow
Packing problem for geometric objects with sub-quadratic union complexity2.
Unique Coverage problem was introduced by Demaine et al. [11] moti-
vated by applications in wireless networks. They proved that for any ϵ > 0, it
is hard to approximate Unique Coverage with ratio O(logσ(ϵ) n), assuming
NP ⊈BPTIME(2nϵ), where σ(ϵ) is some constant dependent on ϵ. Geometric
variant of Unique Coverage problem was studied in [13]. They prove that
1 A set of regions is said to be non-piercing if for any pair of regions A and B, A \ B
and B \ A are connected.
2 Union complexity of a set of objects is the description complexity of the boundary
of the union of the objects.

PTAS for Variants of Geometric Covering and Packing
27
Unique Coverage is NP-hard even when the sets are deﬁned by unit-disks
and give an 18-approximation algorithm for Unique Coverage with unit disks,
which was later improved to 4.31-factor by Ito et al. [20]. However, when the
depth3 of the input points is bounded with respect to input disks (bounded
ply), [13] give an asymptotic FPTAS. PTAS for Unique Coverage on unit
squares was given in [21]. The parameterized complexity of Unique Coverage
problem was studied in [3,24].
Exact Cover was one of the twenty one NP-complete problems in [17]. A
related problem is the Exact SAT or 1-in-k SAT problem which is a well-
studied problem [10,28]. The parameterized complexity of the geometric Exact
Cover problem is studied in [3]. Contrary to the FPT tractability results of
Unique Coverage, Exact Cover with unit squares is proved to be W[1]-
hard when parameterized by the number of sets in the solution.
Multi Cover problem is a very natural generalization of the Set Cover
problem. Chekuri et al. [7] gave an O(log OPT)-approximation algorithm when
the objects have bounded VC-dimension with unit weights. Bansal and Pruhs
[5] gave an O(log φ(n))-approximation algorithm when the objects have shallow
cell complexity which means the cell complexity is at most nφ(n)kO(1). This
means that for objects with linear union complexity viz., disks, pseudo-disks,
halfspaces, etc., this algorithm returns solutions that are constant factor away
from the optimum.
Shallow Packing problem is again a natural generalization of the Packing
problem. Aschner et al. [2] gave a PTAS when the objects are fat. Wheras,
Har-Peled [19] gave QPTAS for pseudo-disks which was improved to PTAS by
Govindarajan et al., [18]. Moreover, in [18] the proof also works for the discrete
setting, i.e., there is a given point set and the depth of every given point with
respect to the solution set should be some constant. Ene et al. [12], had given
a O(u(n)/n)-factor approximation algorithm based on LP for the capacitated
packing problem where u(n) is the union complexity of the objects concerned
and the capacities of the points need not be constants. On a related note, Pach
and Walczak [26] proved that k-fold packing can be decomposed into pk many
(1-fold) packings for class of objects with sub-quadratic union complexity, where
pk is a function of k.
In this paper, we study non-piercing regions and objects with sub-quadratic
union complexity in the plane. See Sect. 3 for precise deﬁnitions. Non-piercing
regions are very broad class of objects viz., disks, squares, pseudo-disks, half-
planes, homothets of a ﬁxed convex object, etc. On the other hand, objects
with sub-quadratic union complexity are even more general class of objects.
They include non-piercing regions (having constant description complexity) as
well as piercing objects like fat triangles. Note that axis-parallel rectangles have
quadratic union complexity thus it belongs to neither class of the objects.
We study the approximability of the problems described above. To design
approximation algorithms, more speciﬁcally PTASs, we use a well known design
3 The depth of a point with respect to a set of objects is the number of objects in the
set containing that point.

28
P. Ashok et al.
paradigm called local search. The use of local search algorithms to obtain approx-
imation results has been proved to be quite fruitful in the recent times. Mustafa
and Ray [25] and Chan and Har-Paled [6] used local search algorithm to obtain
PTASs for the geometric hitting set and independent set problems. Local Search
has been successful in obtaining PTAS for various problems like art gallery prob-
lems [4,22], diﬀerent facility location problems [9], k-means [8,16], set cover and
dominating set [18].
2
Our Results
In this paper we study the following problems and prove the corresponding
theorems.
Unique Coverage. Given a ﬁnite set of non-piercing regions R and, a set of
points P in the plane, compute a subset R′ ⊆R such that the number of points
in P that are contained by exactly one region in R′ is maximized.
Theorem 1. The Local Search algorithm yields a PTAS for the Unique Cov-
erage problem for non-piercing regions with the properties: Every object in R
contains at most some constant c number of points from P and every point in
P has depth at most some constant d.
To the best of our knowledge, this is the ﬁrst result based on local search
technique for the Unique Coverage problems where most of the previous
results were based on the grid shifting technique [13,21].
PTAS for the Unique Coverage problem was known only for unit squares
and disks with bounded depth [13,21]. Our result gives a PTAS for a broader
class of objects namely, non-piercing regions with the additional assumptions on
the input as stated in the above theorem. We note that additional assumptions
are needed to obtain a PTAS as the Unique Coverage problem on disks is as
hard as the general Unique Coverage problem, and thus hard to approximate
beyond poly-logarithmic factors [11].
Multi Cover. Given a ﬁnite set of non-piercing regions R, a set of points P
in the plane and a demand dp of every point p ∈P, compute the maximum
cardinality subset R′ ⊆R such that every point p ∈P is contained in at least
dp regions from R′. We assume the demands are bounded by some constant.
Theorem 2. The Local Search algorithm yields a PTAS for the Multi Cover
problem for non-piercing regions where depth of every input point is at most
some constant k.
The current state of the art result for Multi Cover problem for non-piercing
regions is a O(1)-approximation algorithm which also works in the weighted
setting [5]. To our knowledge we do not know of any PTAS for the Multi
Cover problem for geometric objects in the plane even when the demands are
all 2.

PTAS for Variants of Geometric Covering and Packing
29
Shallow Packing. Given a ﬁnite set of regions R in the plane and a constant k,
compute a subset R′ ⊆R such that every point in the plane is contained in at
most k regions from R′. The objective is to maximize the cardinality of R′.
Theorem 3. The Local Search algorithm yields a PTAS for the Shallow
Packing problem for class of regions with sub-quadratic union complexity.
Aschner et al. [2] gave a PTAS for the Shallow Packing problem for fat
objects using local search. Whereas, Govindarajan et al. [18] gave a PTAS again
using local search but for the discrete case, which is a further generalization. In
light of these two results we too give a PTAS using local search (in the continuous
setting) but for a very general class of objects, i.e., objects with sub-quadratic
union complexity that includes both fat objects and non-piercing regions.
Exact Cover. Given a ﬁnite set of regions R and, a set of points P in the
plane, compute a subset R′ ⊆R such that every point is contained by exactly
one region in R′. The objective is to minimize the cardinality of R′.
Theorem 4. Finding any feasible Exact Cover for a set of unit squares is
NP-hard.
This NP-hardness result rules out the possibility of any polynomial time
approximation algorithm, even for objects as special as unit squares. The proof
follows the NP-hardness proof of the Set Cover problem for unit squares by
Fowler et al. [14]. The details of the reduction will be presented in the full
version of this paper.
Organization. Proofs of Theorems 1, 2, and 3 can be found in Sects. 5, 6,
and 7, respectively.
3
Preliminaries
The Unique Coverage, Multi Cover problems are studied for the non-
piercing regions and we formally deﬁne it below.
Deﬁnition 1 (Non-piercing Regions). For a given set R of simply connected
regions in the plane, if for every pair of regions A, B ∈R, A \ B is a connected
region and so is B \ A then we call R to be a set of non-piercing regions.
For technical reasons, we assume that if two regions intersect then their
boundaries intersect at most a constant number of times.
Deﬁnition 2 (Union Complexity). Given a set of objects R in the plane, the
combinatorial complexity of the boundary of the union of objects in R is called
its union complexity.
For objects with constant description complexity the union complexity can
be at most O(n2). For instance, the union complexity of arbitrary family
of axis-parallel rectangles is Θ(n2). Whereas, non-piercing regions (with con-
stant complexity) have linear union complexity [29]. Examples of such class of

30
P. Ashok et al.
objects are pseudo-disks, disks, homothets of a ﬁxed convex object. Somewhere
in between lies the union complexity of objects like α-fat triangles, which is
O((n/α) log log n log(1/α)). For a detailed exposition on the above results on
union complexity we suggest the survey article by Agarwal et al. [1].
Deﬁnition 3 (Discrete Intersection Graph). Given a set of regions R and
a point set P, the discrete intersection graph of R with respect to P is the graph
whose vertex set is R and there exists an edge (A, B) iﬀA ∩B ∩P ̸= ∅for
A, B ∈R.
Lemma 1 ([18]). The discrete intersection graph of a set of non-piercing
regions R with respect to a point set P in the plane, such that the depth of
every point in P is at most some constant k, has a balanced separator of size
sublinear in |R|.
4
Local Search Framework
We use the following local search algorithm as described in the following works
[2,6,18,25]. Fix a parameter k that is polynomial in 1/ϵ. Start with some feasible
solution. Add and remove at most k elements to and from our current solution
retaining its feasibility. If the objective function improves due to this change we
update the current solution to the changed one. We keep making the updates as
long as possible. Once it can no longer be improved we return this solution and
call it the local search solution. The running time of the algorithm is nO(k).
Let A be the local search solution and O be the optimum solution. If there
exists a suitable bipartite graph on the elements of A and O then one can show
that the sizes are within a multiplicative factor of 1 + ϵ (PTAS). The below
theorem explicitly states the properties required by the bipartite graph. For a
detailed exposition see Aschner et al. [2].
Theorem 5 ([2,6,18,25]). Consider a problem Π.
1. Suppose Π is a minimization problem. If there exists a bipartite graph
H = (O ∪A, E), that belongs to a family of graphs having a balanced vertex
separator of sub-linear size, and it satisﬁes the local-exchange property: For
any subset A′ ⊆A, (A \ A′) ∪Γ(A′) is a feasible solution. Then, the Local
Search algorithm is a PTAS for Π. Here, Γ(A′) denotes the set of neighbors
of A′ in H.
2. Suppose Π is a maximization problem. If there exists a bipartite graph H =
(O∪A, E) that belongs to a family of graphs having a balanced vertex separator
of sub-linear size, and it satisﬁes the local-exchange property: For any O′ ⊆
O, (A ∪O′) \ Γ(O′) is a feasible solution. Then, the Local Search algorithm
is a PTAS for Π. Here, as above Γ(O′) denotes the set of neighbors of O′
in H.
The proof of the above theorem relies on the multi-separator theorem by
Frederickson [15] whose generalized version we state as given in [2].

PTAS for Variants of Geometric Covering and Packing
31
Theorem 6 ([2,15]). There are constants c1, c2 such that for any G = (V, E) ∈
G that has a balanced separator of size cn1−δ and r be a parameter, 1 ≤r ≤n,
we can ﬁnd a collection of t = Θ(n/r) pairwise disjoint subsets V1, . . . , Vt such
that the following properties hold.
1. |Vi| ≤c1r.
2. |Γ(Vi)| ≤c2r1−δ, where Γ(Vi) is the neighbourhood of Vi.
3. Γ(Vi) ∩Vj = ∅and (Vi ∪Γ(Vi)) = V .
Also, let X = ∪i=[t]Γ(Vi) be called the set of boundary (or separator) vertices
whose size is at most t · c2r1−δ = Θ(n/rδ).
5
Unique Coverage of Non-piercing Regions
with Bounded Depth and Degree
We are given a point set P and a set of non-piercing regions R in the plane.
A point in P is said to be uniquely covered with respect to R if there exists a
unique region in R containing that point. We need to output a subset R′ ⊆R
such that the number of uniquely covered points in P with respect to R′ is
maximized.
We make the following assumptions on the input.
1. Depth of every point in P with respect to R is at most some constant d.
2. Every region in R has at most some constant c number of points from P. We
call this the degree of the region.
We denote UCR(S) to be the set of uniquely covered points in P covered by
the regions in S with respect to R, assuming S ⊆R. We shall drop the subscript
when S = R, i.e., UC(R).
We run the local search algorithm with parameter k. Let the solution returned
by the algorithm be A and an optimal solution be O. For comparing the quality
of the two solutions we assume without loss of generality, that they are disjoint.
If there is a region common to both the solutions, we make two copies, one for
each solution. Note that the depth of any point may become at most double of
what it was. Thus, the depth of every point still remains a constant as assumed
before. The points that are uniquely covered by both the solutions are removed
as they do not aﬀect the analysis. Also, without loss of generality both A and O
are minimal in size, i.e., every region in them uniquely cover at least one point
in P.
Proof of Theorem 1: Consider the bipartite discrete intersection graph G = (V, E)
over A ⊎O, i.e., we put an edge between a region in A and a region in O if they
both contain a common point from P. From Lemma 1 this graph4 has a balanced
and sub-linear separator since the depth of each point is bounded by a constant
d. Thus we can apply Frederickson’s separator theorem on this graph [15].
4 Actually, Lemma 1 proves for the discrete intersection which is a super graph of the
bipartite version.

32
P. Ashok et al.
Recall that Theorem 6 partitions V into V1, . . . , Vt, X. For every 1 ≤i ≤t,
let Ai = A ∩Vi and Oi = O ∩Vi. Let ˜
Ai be Ai ∪Γ(Oi) and Ai be (A ∪Oi) \ ˜
Ai.
As A is locally optimal, |UC(A)| ≥|UC(Ai)|. Observe that for every uniquely
covered point there is a unique region that is covering that point. This region is
in some part of the partition. Now we write UC(A) in terms of its constituent
parts.
UC(A) =
t
j=1
UC(Aj)

UC((A \ ˜
Ai) ∩X)

UC( ˜
Ai ∩X)
Similarly,
UC(Ai) =

j̸=i
UC(Aj)

UC(Oi)

UC((A \ ˜
Ai) ∩X)
Note that there are some uniquely covered points common to both A and
Ai. As A is locally optimal, the following inequality holds.
t

j=1
|UCA(Aj)| + |UCA((A \ ˜
Ai) ∩X)| + |UCA( ˜
Ai ∩X)| ≥

j̸=i
|UCAi(Aj)| + |UCAi(Oi)| + |UCAi((A \ ˜
Ai) ∩X)|
(1)
⇒|UCA(Ai)| + |UCA( ˜
Ai ∩X)| ≥|UCAi(Oi)|
Lemma 2. UCO(Oi) ⊆UCAi(Oi)
Proof. We need to show that every point in UCO(Oi) is also contained in
UCAi(Oi), i.e., a point that is uniquely covered by some region in Oi with respect
to O remains uniquely covered by some region in O with respect to Ai. As we
are removing all the neighbours of Oi in A, this is true.
⊓⊔
Lemma 2 implies that |UCO(Oi)| ≤|UCAi(Oi)|. Hence,
|UCA(Ai)| + |UCA( ˜
Ai ∩X)| ≥|UCO(Oi)|
Summing over all i, where 1 ≤i ≤t,

i
|UCA(Ai)| +

i
|UCA( ˜
Ai ∩X)| ≥

i
|UCO(Oi)|
⇒|UC(A)| +

i
|UCA( ˜
Ai ∩X)| ≥|UC(O)| −|UCO(O ∩X)|
Theorem 6 implies that every element in Vi ∩X is either in O ∩X or is a
neighbour of some element in Oi. Therefore, 
i |UCA( ˜
Ai∩X)|+|UCO(O∩X)| =

i |(Vi ∩X)|.
|UC(A)| + c

i
|(Vi ∩X)| ≥|UC(O)|

PTAS for Variants of Geometric Covering and Packing
33
where c is the maximum number of points in P that a region can contain
(Assumption 2). Also, from Theorem 6 we know 
i |(Vi ∩X)| ≤Θ(n/kδ), hence
the following.
|UC(A)| + c′(|A| + |O|)/kδ ≥|UC(O)|
where c′ is an appropriate constant. As, the number of uniquely covered points
by a region is at least one,
|UC(A)| + c′(|UC(A)| + |UC(O)|)/kδ ≥|UC(O)|
⇒|UC(A)|(1 + c′
kδ ) ≥|UC(O)|(1 −c′
kδ )
For ϵ = 2c′/(kδ −c′) we get the desired ratio.
|UC(A)|(1 + ϵ) ≥|UC(O)|
⊓⊔
Corollary 1. Given a family of unit disks and a point set in the plane with
constant depth for every input point, the local search algorithm yields a PTAS.
6
Multi-covering of Bounded Depth Points
with Non-piercing Regions
Proof of Theorem 2: Recall we are given a set of regions R and a point set P in
the plane where every p ∈P has a demand dp. Consider the bipartite discrete
intersection graph G over A ∪O. From Lemma 1 we know that G has a small
and balanced separator when every point in P has a constant depth. Next we
claim that G satisﬁes the local-exchange property as mentioned in Theorem 5.
Lemma 3. Given the bipartite discrete intersection graph G, for every subset
A′ ⊆A, (A \ A′) ∪Γ(A′) satisﬁes demands of every point in P.
Proof. For every point p ∈P, let Ap be the set of regions in A containing p.
Similarly, let Op be the set of regions in O containing p. Fix a point p ∈P.
Now there are two possibilities — either A′ ∩Ap = ∅or not. For the ﬁrst case,
p continues to be contained by at least dp regions in (A \ A′) ∪Γ(A′). In case,
if A′ ∩Ap ̸= ∅then let A ∈A′ ∩Ap. As, G contains edges between every region
in Ap to every region in Op, Op ⊆(A \ A′) ∪Γ(A′). Thus, p is contained by at
least dp regions in this case as well. The same argument holds for every point in
P. Thus, (A \ A′) ∪Γ(A′) satisﬁes demands of every point in P.
⊓⊔
Thus, the bipartite discrete intersection G satisﬁes the properties required as
mentioned in Theorem 5. Therefore, the Local Search algorithm yields a PTAS. ⊓⊔

34
P. Ashok et al.
7
Shallow Packing of Regions with Sub-quadratic Union
Complexity
In this section we consider the problem of shallow packing of regions whose union
complexity is sub-quadratic. We are given a set of regions D with the hereditary
property that any of its subset D′ has its union complexity sub-quadratic in |D′|.
The problem is to ﬁnd a subset R ⊆D such that every point in the plane is
contained by at most k regions from R, where k is a constant independent of
the size of D.
We use the local search algorithm and prove that the solution it returns is
within a factor of 1 + ϵ. Our proof is on similar lines to that of the proof of
PTAS for the capacitated region packing problem [18], where they show that
the discrete intersection graph of A ∪O has a balanced separator of sublinear
size using a planar graph on the input points [27]. Note that the planar graph
constructed in [27] works for non-piercing regions only. Since, we deal with a
broader class of objects namely, objects with sub-quadratic union complexity,
we use the arrangement graph on R to show that our intersection graph, which
is not discrete, has a small and balanced separator.
Given a set of regions in the plane, an arrangement graph is a plane multi-
graph on the intersection points of the boundaries of the regions. We assume that
regions are closed, simply connected and no three regions have their boundaries
intersecting at a point. The part of the boundary of a region joining two such
intersection points forms an edge between them.
Proof of Theorem 3: As R is embedded in the plane its arrangement graph H
is planar. We use the separator of H to get a separator of G. Here onwards for
every R ∈R we shall interchangeably use it with the subgraph H[R] induced
over the vertices of H in R. Therefore, G is also the intersection graph over the
subgraphs H[R] for every R ∈R. Also, as R is a simple and connected region
so H[R] is connected too.
We put weights on the vertices of H. For every vertex p of H and R ∈R
we add 1/|R ∩V (H)| whenever p ∈R. Thus, wt(p) = 
R|p∈R 1/|R ∩V (H)|.
We apply the Lipton-Tarjan’s separator theorem on H to get a separator S of
size O(√n) where n is the number of vertices in H. Also the total weight of the
separated parts A and B is at most 2W/3 each, where W = 
p∈V (H) wt(p).
We start with an empty set S. For every R that contains a vertex p from
separator S, we add that to the set S. Note that the union of the vertices of
H[R] in S is a superset of S. We claim that S is a small balanced separator of
G. The fact that S separates G into parts A and B follows because a region R
containing a vertex from A and a vertex from B must contain a vertex from S.
Next we prove that the size of S is sub-linear in the size of R. We refer n
to be the number of vertices in H and m to be the number of regions in R.
We slightly generalize the exposition of the Clarkson-Shor technique as given by
Matouˇsek [23, p. 141].

PTAS for Variants of Geometric Covering and Packing
35
Lemma 4. If the union complexity of R is sub-quadratic in its size, i.e.,
O(m2−δ) where m is the size of R, and every subset of R also has sub-quadratic
union complexity in its size, then the number of ≤k-level vertex points N≤k is
O(kδm2−δ).
Proof. Let the union complexity of R be O(m2−δ), where m is the number of
regions in R. We do a random sampling of regions from R. For every region
R ∈R we pick it with probability p and refer the sample set as R′. Consider the
expected union complexity of R′. We will upper and lower bound this quantity.
The expectation can be upper bounded by O((pm)2−δ).
For every vertex point p whose depth is at most k we compute the probability
of p being a boundary point in R′. Let the depth of p with respect to R be d(p),
where d(p) ≤k. The probability of p being a boundary point is p2(1 −p)d(p)−2
that is at least p2(1 −p)k as p < 1. Therefore, the expected number of vertex
points whose depth is at most k in R that lies in the boundary of the union of
regions in R′ is at least N≤kp2(1 −p)k. Thus, N≤kp2(1 −p)k ≤O((pm)2−δ).
Setting p ←1/k leads N≤k ≤O(kδm2−δ).
⊓⊔
This means in our context, n ≤O(kδm2−δ). Also, we know that |S| ≤k|S|
as depth of every vertex point of H is at most k. Together with the fact |S| ≤
O(√n), it implies that |S| ≤O(k
√
kδm2−δ). As k is a constant independent of
m, size of S is sublinear in m.
Now it only remains to show that the two parts A and B which are separated
by S are balanced, i.e., |A| ≤2m/3 and |B| ≤2m/3. Since the weight of all
regions in A are distributed among vertex points in A, wt(A) ≥|A|. Again from
planar separator theorem, wt(A) ≤2|R|/3. Hence, |A| ≤2m/3. The same holds
for B. Thus S is a balanced separator of sublinear size for the intersection graph
of R.
Lastly, it is easy to see that the local-exchange property is satisﬁed by the
graph G [2,18]. As the properties of the graphs as mentioned in Theorem 5 are
satisﬁed, PTAS follows.
⊓⊔
References
1. Agarwal, P.K., Pach, J., Sharir, M.: State of the union (of geometric objects): a
review (2007)
2. Aschner, R., Katz, M.J., Morgenstern, G., Yuditsky, Y.: Approximation schemes
for covering and packing. In: Ghosh, S.K., Tokuyama, T. (eds.) WALCOM
2013. LNCS, vol. 7748, pp. 89–100. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-36065-7 10
3. Ashok, P., Kolay, S., Misra, N., Saurabh, S.: Unique covering problems with geo-
metric sets. In: Xu, D., Du, D., Du, D. (eds.) COCOON 2015. LNCS, vol. 9198,
pp. 548–558. Springer, Cham (2015). doi:10.1007/978-3-319-21398-9 43
4. Bandyapadhyay, S., Basu Roy, A.: Eﬀectiveness of local search for art gallery prob-
lems. In: WADS (2017)
5. Bansal, N., Pruhs, K.: Weighted geometric set multi-cover via quasi-uniform sam-
pling. JoCG 7(1), 221–236 (2016)

36
P. Ashok et al.
6. Chan, T.M., Har-Peled, S.: Approximation algorithms for maximum independent
set of pseudo-disks. Discrete Comput. Geom. 48(2), 373–392 (2012)
7. Chekuri, C., Clarkson, K.L., Har-Peled, S.: On the set multicover problem in geo-
metric settings. ACM Trans. Algorithms (TALG) 9(1), 9 (2012)
8. Cohen-Addad, V., Klein, P.N., Mathieu, C.: Local search yields approximation
schemes for k-means and k-median in euclidean and minor-free metrics. In: FOCS,
pp. 353–364 (2016)
9. Cohen-Addad, V., Mathieu, C.: Eﬀectiveness of local search for geometric opti-
mization. In: SoCG, pp. 329–343 (2015)
10. Dahll¨of, V., Jonsson, P., Beigel, R.: Algorithms for four variants of the exact sat-
isﬁability problem. Theor. Comput. Sci. 320(2–3), 373–394 (2004)
11. Demaine, E.D., Feige, U., Hajiaghayi, M., Salavatipour, M.R.: Combination can
be hard: approximability of the unique coverage problem. SIAM J. Comput. 38(4),
1464–1483 (2008)
12. Ene, A., Har-Peled, S., Raichel, B.: Geometric packing under non-uniform con-
straints. In: SoCG, pp. 11–20 (2012)
13. Erlebach, T., Van Leeuwen, E.J.: Approximating geometric coverage problems.
In: Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 1267–1276. Society for Industrial and Applied Mathematics (2008)
14. Fowler, R.J., Paterson, M.S., Tanimoto, S.L.: Optimal packing and covering in the
plane are np-complete. Inf. Process. Lett. 12(3), 133–137 (1981)
15. Frederickson, G.N.: Fast algorithms for shortest paths in planar graphs, with appli-
cations. SIAM J. Comput. 16(6), 1004–1022 (1987)
16. Friggstad, Z., Rezapour, M., Salavatipour, M.R.: Local search yields a PTAS for
k-means in doubling metrics. In: FOCS, pp. 365–374 (2016)
17. Garey, M.R., Johnson, D.S.: Computers and intractability (1979)
18. Govindarajan, S., Raman, R., Ray, S., Basu Roy, A.: Packing and covering with
non-piercing regions. In: ESA, pp. 47:1–47:17 (2016)
19. Har-Peled, S.: Quasi-polynomial time approximation scheme for sparse subsets of
polygons. In: SoCG, pp. 120:120–120:129 (2014)
20. Ito, T., Nakano, S., Okamoto, Y., Otachi, Y., Uehara, R., Uno, T., Uno, Y.: A 4.31-
approximation for the geometric unique coverage problem on unit disks. Theor.
Comput. Sci. 544, 14–31 (2014)
21. Ito, T., Nakano, S.-I., Okamoto, Y., Otachi, Y., Uehara, R., Uno, T., Uno, Y.: A
polynomial-time approximation scheme for the geometric unique coverage problem
on unit squares. In: Fomin, F.V., Kaski, P. (eds.) SWAT 2012. LNCS, vol. 7357,
pp. 24–35. Springer, Heidelberg (2012). doi:10.1007/978-3-642-31155-0 3
22. Krohn, E., Gibson, M., Kanade, G., Varadarajan, K.: Guarding terrains via local
search. J. Comput. Geom. 5(1), 168–178 (2014)
23. Matouˇsek, J.: Lectures on Discrete Geometry. Springer, New York (2002)
24. Misra, N., Moser, H., Raman, V., Saurabh, S., Sikdar, S.: The parameterized com-
plexity of unique coverage and its variants. Algorithmica 65(3), 517–544 (2013)
25. Mustafa, N.H., Ray, S.: Improved results on geometric hitting set problems. Dis-
crete Comput. Geom. 44(4), 883–895 (2010)
26. Pach, J., Walczak, B.: Decomposition of multiple packings with subquadratic union
complexity. Comb. Probab. Comput. 25(1), 145–153 (2016)

PTAS for Variants of Geometric Covering and Packing
37
27. Pyrga, E., Ray, S.: New existence proofs for ϵ-nets. In: SoCG, pp. 199–207 (2008)
28. Schaefer, T.J.: The complexity of satisﬁability problems. In: Proceedings of the
Tenth Annual ACM Symposium on Theory of Computing, pp. 216–226. ACM
(1978)
29. Whitesides, S., Zhao, R.: K-admissible collections of jordan curves and oﬀsets of
circular arc ﬁgures. Technical report, McGill University, School of Computer Sci-
ence (1990)

Depth Distribution in High Dimensions
J´er´emy Barbay1, Pablo P´erez-Lantero2, and Javiel Rojas-Ledesma1(B)
1 Departamento de Ciencias de la Computaci´on,
Universidad de Chile, Santiago, Chile
jeremy@barbay.cl, jrojas@dcc.uchile.cl
2 Departamento de Matem´atica y Ciencia de la Computaci´on,
Universidad de Santiago, Santiago, Chile
pablo.perez.l@usach.cl
Abstract. Motivated by the analysis of range queries in databases, we
introduce the computation of the Depth Distribution of a set B of
axis aligned boxes, whose computation generalizes that of the Klee’s
Measure and of the Maximum Depth. In the worst case over instances
of ﬁxed input size n, we describe an algorithm of complexity within
O(n
d+1
2
log n), using space within O(n log n), mixing two techniques pre-
viously used to compute the Klee’s Measure. We reﬁne this result and
previous results on the Klee’s Measure and the Maximum Depth for
various measures of diﬃculty of the input, such as the proﬁle of the input
and the degeneracy of the intersection graph formed by the boxes.
1
Introduction
Problems studied in Computational Geometry have found important applica-
tions in the processing and querying of massive databases [1], such as the com-
putation of the Maxima of a set of points [2,4], or compressed data structures
for Point Location and Rectangle Stabbing [3]. In particular, we consider
cases where the input or queries are composed of axis-aligned boxes in d dimen-
sions: in the context of databases it corresponds for instance to a search for cars
within the intersection of ranges in price, availability and security ratings range.
Consider a set B of n axis-parallel boxes in Rd, for ﬁxed d. The Klee’s
Measure of B is the volume of the union of the boxes in B. Originally suggested
on the line by Klee [18], its computation is well studied in higher dimensions
[7–10,22], and can be done in time within O(nd/2), using an algorithm introduced
by Chan [10] based on a new paradigm called “Simplify, Divide and Conquer”.
The Maximum Depth of B is the maximum number of boxes overlapping at any
point, and its computational complexity is similar to that of Klee’s Measure’s,
converging to the same complexity within O(nd/2) [10].
Hypothesis. The known algorithms to compute these two measures are all strik-
ingly similar. That would suggest a reduction from one to another, except that
those two measures are completely distinct: Klee’s measure is a volume whose
value can be a real number, while Maximum Depth is a cardinality whose value
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 38–49, 2017.
DOI: 10.1007/978-3-319-62389-4 4

Depth Distribution in High Dimensions
39
is an integer in the range [1..n]. Is there any way to formalize the relation-
ship between the computation of these two measures?
Our Results. We describe a ﬁrst step towards such a formalization, in the form of
a new problem, which we show to be intermediary in terms of the techniques
being used, between the Klee’s measure and the Maximum Depth, slightly
more costly in time and space, and with interesting applications and results of
its own.
We introduce the notion of the Depth Distribution of a set B of n axis-
parallel boxes in Rd, formed by the vector of n values (V1, . . . , Vn), where Vi cor-
responds to the volume covered by exactly i boxes from B. The Depth Distrib-
ution of a set B can be interpreted as a probability distribution function (hence
the name): if a point p is selected from the union of the boxes in B uniformly
at random, the probability that p intersects exactly k boxes is (Vk/n
i=1 Vi), for
all k ∈[1..n].
The Depth Distribution reﬁnes both Klee’s measure and Maximum
Depth. It is a measure ﬁner than Klee’s Measure in the sense that the
Klee’s Measure of a set B can be obtained in time linear in the size n of B
by summing the components of the Depth Distribution of B. Similarly, the
Depth Distribution is a measure ﬁner than the Maximum Depth in the
sense that the Maximum Depth of a set B can be obtained in linear time by
ﬁnding the largest i ∈[1..n] such that Vi ̸= 0. In the context of a database,
when receiving multidimensional range queries (e.g. about cars), the Depth
Distribution of the queries yields valuable information to the database owner
(e.g. a car dealer) about the repartition of the queries in the space of the data,
to allow for informed decisions on it (e.g. to orient the future purchase of cars
to resell based on the clients’ desires, as expressed by their queries).
In the classical computational complexity model where one studies the worst
case over instances of ﬁxed size n, we combine techniques previously used
to compute the Klee’s Measure [10,22] to compute the Depth Distribu-
tion in time within O(n
d+1
2 log n), using space within O(n log n) (in Sect. 3.1).
This solution is slower by a factor within O(√n log n) than the best known
algorithms for computing the Klee’s Measure and the Maximum Depth: we
show in Sect. 3.2 that such a gap might be ineluctable, via a reduction from the
computation of Matrix Multiplication.
In the reﬁned computational complexity model where one studies the worst
case complexity taking advantage of additional parameters describing the
diﬃculty of the instance [4,17,21], we introduce (in Sect. 4) new measures of
diﬃculty such as the proﬁle and the degeneracy of the intersection graph of
the boxes, and describe algorithms in these new models to compute the Depth
Distribution, Klee’s Measure and Maximum Depth of a set B.
After a short overview of the known results on the computation of the
Klee’s Measure and Maximum Depth (in Sect. 2), we describe in Sect. 3
the results in the worst case over instances of ﬁxed size. In Sect. 4, we describe
results on reﬁned partitions of the instance universe, both for Depth Distribu-
tion and for the previously known problems, Klee’s Measure and Maximum

40
J. Barbay et al.
Depth. We conclude in Sect. 5 with a discussion on discrete variants and further
reﬁnements of the analysis.
2
Background
The techniques used to compute the Klee’s Measure have evolved over time,
and can all be used to compute the Maximum Depth. We retrace some of the
main results, which will be useful for the deﬁnition of an algorithm computing
the Depth Distribution (in Sect. 3), and for the reﬁnements of the analysis for
Depth Distribution, Klee’s Measure and Maximum Depth (in Sect. 4).
The computation of the Klee’s Measure of a set B of n axis-aligned d-
dimensional boxes was ﬁrst posed by Klee [18] in 1977. After some initial pro-
gresses [6,15,18], Overmars and Yap [22] described a solution running in time
within O(nd/2 log n). This remained the best solution for more than 20 years
until 2013, when Chan [10] presented a simpler and faster algorithm running in
time within O(nd/2).
The algorithms described by Overmars and Yap [22] and by Chan [10], respec-
tively, take both advantage of solutions to the special case of the problem where
all the boxes are slabs. A box b is said to be a slab within another box Γ if
b ∩Γ = {(x1, . . . , xd) ∈Γ | α ≤xi ≤β}, for some integer i ∈[1..d] and some
real values α, β (see Fig. 1 for an illustration). Overmars and Yap [22] showed
that, if all the boxes in B are slabs inside the domain box Γ, then the Klee’s
Measure of B within Γ can be computed in linear time (provided that the
boxes have been pre-sorted in each dimension).
b1
b2
b1
b2
Γ
Γ
(a)
(b)
Fig. 1. An illustration in dimensions 2 (a) and 3 (b) of two boxes b1, b2 equivalent to
slabs when restricted to the box Γ. The Klee’s Measure of {b1, b2} within Γ is the
area (resp. volume) of the shadowed region in (a) (resp. (b)).
Overmars and Yap’s algorithm [22] is based on a technique originally
described by Bentley [6]: solve the static problem in d dimensions by combining
a data structure for the dynamic version of the problem in d−1 dimensions with
a plane sweep over the d-th dimension. The algorithm starts by partitioning the

Depth Distribution in High Dimensions
41
space into O(nd/2) rectangular cells such that the boxes in B are equivalent to
slabs when restricted to each of those cells. Then, the algorithm builds a tree-like
data structure whose leaves are the cells of the partition, supporting insertion
and deletion of boxes while keeping track of the Klee’s Measure of the boxes.
Chan’s algorithm [10] is a simpler divide-and-conquer algorithm, where the
slabs are simpliﬁed and removed from the input before the recursive calls (Chan
[10] named this technique Simplify, Divide and Conquer, SDC for short). To
obtain the recursive subproblems, the algorithm assigns a constant weight of
2
i+j
2
to each (d-2)-face intersecting the domain and orthogonal to the i-th and j-
th dimensions, i, j ∈[1..d]. Then, the domain is partitioned into two sub-domains
by the hyperplane x1 = m, where m is the weighted median of the (d-2)-faces
orthogonal to the ﬁrst dimension. This yields a decrease by a factor of 22/d in
the total weight of the (d-2)-faces intersecting each sub-domain. Chan [10] uses
this, and the fact that slabs have no (d-2)-face intersecting the domain, to prove
that the SDC algorithm runs in time within O(nd/2).
Unfortunately, there are sets of boxes which require partitions of the space
into a number of cells within Ω(nd/2) to ensure that, when restricted to each cell,
all the boxes are equivalent to slabs. Hence, without a radically new technique,
any algorithm based on this approach will require running time within Ω(nd/2).
Chan [10] conjectured that any combinatorial algorithm computing the Klee’s
Measure requires within Ω(nd/2) operations, via a reduction from the parame-
terized k-Clique problem, in the worst case over instances of ﬁxed size n. As a
consequence, recent work have focused on the study of special cases which can be
solved faster than Ω(nd/2), like for instance when all the boxes are orthants [10],
α-fat boxes [7], or cubes [8]. In turn, we show in Sect. 4 that there are measures
which gradually separate easy instances for these problems from the hard ones.
In the next section, we present an algorithm for the computation of the
Depth Distribution inspired by a combination of the approaches described
above, outperforming naive applications of those techniques.
3
Computing the Depth Distribution
We describe in Sect. 3.1 an algorithm to compute the Depth Distribution of
a set of n boxes. The running time of this algorithm in the worst case over d-
dimensional instances of ﬁxed size n is within O(n
d+1
2 log n), using space within
O(n log n). This running time is worse than that of computing only the Klee’s
Measure (or the Maximum Depth) by a factor within O(√n log n): we argue
in Sect. 3.2 that computing the Depth Distribution is computationally harder
than the special cases of computing the Klee’s Measure and the Maximum
Depth, unless computing Matrix Multiplication is much easier than usually
assumed.
3.1
Upper Bound
We introduce an algorithm to compute the Depth Distribution inspired by
a combination of the techniques introduced by Chan [10], and by Overmars and

42
J. Barbay et al.
Yap [22], for the computation of the Klee’s Measure (described in Sect. 2).
As in the approaches mentioned previously, the algorithm partitions the domain
Γ into O(nd/2) cells where the boxes of B are equivalent to slabs, and then
combines the solution within each cell to obtain the ﬁnal answer. Two main
issues must be addressed: how to compute the Depth Distribution when the
boxes are slabs, and how to partition the domain eﬃciently.
We address ﬁrst the special case of slabs. We show in Lemma 1 that comput-
ing the Depth Distribution of a set of n d-dimensional slabs within a domain
Γ can be done via a multiplication of d polynomials of degree at most n.
Lemma 1. Let B be a set of n axis-parallel d-dimensional axis aligned boxes,
with d ≥2, that, when restricted to a domain box Γ, are equivalent to slabs.
The computation of the Depth Distribution (V1, . . . , Vn) of B within Γ can
be performed via a multiplication of d polynomials of degree at most n.
Proof. For all i ∈[1..d], let Bi be the subset of slabs that are orthogonal to the i-
th dimension, and let

V i
1 , . . . , V i
n

be the Depth Distribution of the intervals
resulting from projecting Bi to the i-th dimension within Γ. We associate a
polynomial Pi(x) of degree n with each Bi as follows:
– let Γi be the projection of the domain Γ into the i-th dimension, and
– let V i
0 be the length of the region of Γi not covered by a box in Bi (i.e.,
V i
0 = (|Γi| −n
j=1 V i
j )); then
– Pi(x) = n
j=0 V i
j · xj.
Since any slab entirely covers the domain in all the dimensions but the one
to which it is orthogonal, any point p has depth k in B if and only if it has depth
j1 in B1, j2 in B2, . . . , and jd in Bd, such that j1 + j2 + . . . + jd = k. Thus, for
all k ∈[0..n]:
Vk =

0≤j1,...,jd≤n
j1+...+jd=k
 d

i=1
V i
ji

,
which is precisely the (k + 1)-th coeﬃcient of P1(x) · P2(x) · . . . · Pd(x). Thus,
this product yields the Depth Distribution (V1, . . . , Vn) of B in Γ.
⊓⊔
Using standard Fast Fourier Transform techniques, two polynomials can be
multiplied in time within O(n log n) [12]. Moreover, the Depth Distribution
of a set of intervals (i.e., when d = 1) can be computed in linear time after
sorting, by a simple scan-line algorithm, as for Klee’s Measure [10]. Thus, as
a consequence of Lemma 1, when the boxes in B are slabs when restricted to a
domain box Γ, the Depth Distribution of B within Γ can be computed in
time within O(n log n).
Corollary 2. Let B be a set of n d-dimensional axis aligned boxes, whose inter-
sections with Γ are slabs. The Depth Distribution of B inside Γ can be
computed in time within O(n log n).

Depth Distribution in High Dimensions
43
A naive application of previous techniques [10,22] to the computation of
Depth Distribution yields poor results. Combining the result in Corollary 2
with the partition of the space and the data structure described by Overmars
and Yap [22] yields an algorithm to compute the Depth Distribution running
in time within O(n
d+1
2 log n), and using space within O(nd/2 log n). Similarly, if
the result in Corollary 2 is combined with Chan’s partition of the space [10], one
obtains an algorithm using space linear in the number of boxes, but running in
time within O(n
d
2 +1 log n) (i.e., paying an extra O(n
1
2 )-factor for the reduction
in space usage of Overmars and Yap [22]).
We combine these two approaches into an algorithm which achieves the best
features of both: it runs in time within O(n
d+1
2 log n), and uses O(n log n)-
space. As in Chan’s approach [10] we use a divide and conquer algorithm, but
we show in Theorem 3 that the running time is asymptotically the same as if
using the partition and data structures described by Overmars and Yap [22] (see
Algorithm 1 for a detailed description).
Algorithm 1. SDC-DDistribution(B, Γ, c, (V1, . . . , Vn))
Input: A set B of n boxes in Rd; a d-dimensional domain box Γ; the number c of
boxes not in B but in the original set that completely contain Γ; and a vector
(V1, . . . , Vn) representing the Depth Distribution computed so far.
1: if no box in B has a (d-2)-face intersecting Γ (i.e., all the boxes are slabs) then
2:
Compute the Depth Distribution

V ′
1, . . . , V ′
|B|

of B within Γ using Lemma
1
3:
for i ∈[1..|B|] do
4:
Vi+c ←Vi+c + V ′
i
5: else
6:
Let B0 ⊆B be the subset of boxes completely containing Γ
7:
c ←c + |B0|
8:
Let B′ = B \ B0
9:
Let m be the weighted median of the (d-2)-faces orthogonal to x1
10:
Split Γ into ΓL, ΓR by the hyperplane x1 = m
11:
Rename the dimensions so that x1, . . . , xd becomes x2, . . . , xd, x1
12:
Let BL and BR be the subsets of B′ intersecting ΓL and ΓR respectively
13:
Call SDC-DDistribution(BL, ΓL, c, (V1, . . . , Vn))
14:
Call SDC-DDistribution(BR, ΓR, c, (V1, . . . , Vn))
Theorem 3. Let B be a set of n axis-parallel boxes in Rd. The Depth Distri-
bution of B can be computed in time within O(n
d+1
2 log n), using space within
O(n log n).
Due to lack of space we defer the complete proof to the extended version [5].
The bound for the running time in Theorem 3 is worse than that for the com-
putation of the Klee’s Measure (and Maximum Depth) by a factor within
O(√n log n), which raises the question of the optimality of the bound: we con-
sider this matter in the next section.

44
J. Barbay et al.
3.2
Conditional Lower Bound
As for many problems handling high dimensional inputs, the best lower bound
known for this problem is Ω(n log n), which derives from the fact that the Depth
Distribution is a generalization of the Klee’s Measure problem. This bound,
however, is tight only when the input is a set of intervals (i.e., d = 1). For
higher dimensions, the conjectured lower bound of Ω(nd/2) described by Chan
in 2008 [9] for the computational complexity of computing the Klee’s Measure
can be extended analogously to the computation of the Depth Distribution.
One intriguing question is whether in dimension d = 2, as for Klee’s Mea-
sure, the Depth Distribution can be computed in time within O(n log n).
We argue that doing so would imply breakthrough results in a long standing
problem, Matrix Multiplication. We show that any instance of Matrix
Multiplication can be solved using an algorithm which computes the Depth
Distribution of a set of rectangles in the plane. For this, we make use of the
following simple observation:
Observation 1. Let A,B be two n×n matrices of real numbers, and let Ci denote
the n × n matrix that results from multiplying the n × 1 vector corresponding
to the i-th column of A with the 1 × n vector corresponding to the i-th row of
B. Then, AB = n
i=1 Ci.
We show in Theorem 4 that multiplying two n × n matrices can be done by
transforming the input into a set of O(n2) 2-dimensional boxes, and computing
the Depth Distribution of the resulting box set. Moreover, this transforma-
tion can be done in linear time, thus, the theorem yields a conditional lower
bound for the computation of the Depth Distribution.
Theorem 4. Let A, B be two n×n matrices of non-negative real numbers. There
is a set B of rectangles of size within O(n2), and a domain rectangle Γ, such that
the Depth Distribution of B within Γ can be projected to obtain the value of
the product AB.
Intuitively, we create a gadget to represent each matrix Ci. Within the i-th
gadget, there will be a rectangular region for each component of Ci with the value
of that component as volume. We arrange the boxes so that two distinct regions
have the same depth if and only if they represent the same respective coeﬃcients
of two distinct matrices Ci and Ci′ (formally, they represent coeﬃcients (Ci)j,k
and (Ci′)j′,k′, respectively, such that i ̸= i′, j = j′, and k = k′). Due to lack of
space, we defer the complete proof to the extended version [5].
The optimal time to compute the product of two n×n matrices is still open.
It can naturally be computed in time within O(n3). However, Strassen showed in
1969 that within O(n2.81) arithmetic operations are enough [23]. This gave rise
to a new area of research, where the central question is to determine the value of
the exponent of the computational complexity of square matrix multiplication,
denoted ω, and deﬁned as the minimum value such that two n × n matrices can
be multiplied using within O(nω+ε) arithmetic operations for any ε > 0.

Depth Distribution in High Dimensions
45
The result of Theorem 4 directly yields a conditional lower bound on the
complexity of Depth Distribution: in particular, Depth Distribution in
dimension as low as two, can be solved in time within O(n log n), then Matrix
Multiplication can be computed in time within O(n2), i.e. ω = 2. However,
this would be a great breakthrough in the area, the best known upper bound to
date is approximately ω ≤2.37, when improvements in the last 30 years [11,16]
have been in the range [2.3728, 2.3754].
Corollary 5 (Conditional lower bound). Computing the Depth Distribu-
tion of a set B of n d-dimensional boxes requires time within Ω(n1+c), for some
constant c > 0, unless two n×n matrices can be multiplied in time O(n2+ε), for
any constant ε > 0.
The running time of the algorithm that we described in Theorem 3 can be
improved for large classes of instances (i.e. asymptotically inﬁnite) by considering
measures of the diﬃculty of the input other than its size. We describe two of
these improved solutions in the next section.
4
Multivariate Analysis
Even though the asymptotic complexity of O(n
d+1
2 log n) is the best we know so
far for computing the Depth Distribution of a set of n d-dimensional boxes,
there are many cases which can be solved faster. Some of those “easy” instances
can be mere particular cases, but others can be hints of some hidden measures
of diﬃculty of the Depth Distribution problem. We show that, indeed, there
are at least two such diﬃculty measures, gradually separating instances of the
same size n into various classes of diﬃculty. Informally, the ﬁrst one (the proﬁle
of the input set, Sect. 4.1) measures how separable the boxes are by axis-aligned
hyperplanes, whereas the second one (the degeneracy of the intersection graph,
Sect. 4.2) measures how “complex” the interactions of the boxes are in the set
between them. Those measures inspire similar results for the computation of the
Klee’s Measure and of the Maximum Depth.
4.1
Proﬁle
The i-th proﬁle pi of a set of boxes B is the maximum number of boxes intersected
by any hyperplane orthogonal to the i-th dimension; and the proﬁle p of B is the
minimum p = mini∈[1..d]{pi} of those over all dimensions. D’Amore [13] showed
how to compute it in linear time (after sorting the coordinate of the boxes in
each dimension). The following lemma shows that the Depth Distribution
can be computed in time sensitive to the proﬁle of the input set.
Lemma 6. Let B be a set of boxes with proﬁle p, and Γ be a d-dimensional axis-
aligned domain box. The Depth Distribution of B within Γ can be computed
in time within O(n log n + np
d−1
2 log p) ⊆O(n
d+1
2 log n).

46
J. Barbay et al.
Due to lack of space we defer the complete proof to the extended version [5].
The lemma above automatically yields reﬁned results for the computation of
the Klee’s Measure and the Maximum Depth of a set of boxes B. However,
applying the technique in an ad-hoc way to these problems yields better bounds:
Corollary 7. Let B be a set of boxes with proﬁle p, and Γ be a domain box.
The Klee’s Measure and Maximum Depth of B within Γ can be computed
in time within O(n log n + np
d−2
2 log p) ⊆O(nd/2 log n).
The algorithms from Lemma 6 and Corollary 7 asymptotically outperform
previous ones in the sense that their running time is never worse than previous
algorithms by more than a constant factor, but can perform faster by more than
a constant factor on speciﬁc families of instances.
An orthogonal approach is to consider how complex the interactions between
the boxes are in the input set B, analyzing, for instance, the intersection graph
of B. We study such a technique in the next section.
4.2
Intersections Graph Degeneracy
A k-degenerate graph is an undirected graph in which every subgraph has a
vertex of degree at most k [19]. Every k-degenerate graph accepts an ordering
of the vertices in which every vertex is connected with at most k of the vertices
that precede it (we refer below to such an ordering as a degenerate ordering).
In the following lemma we show that this ordering can be used to compute
the Depth Distribution of a set B of n boxes in running time sensitive to the
degeneracy of the intersection graph of B.
Lemma 8. Let B be a set of boxes and Γ be a domain box, and let k be the
degeneracy of the intersection graph G of the boxes in B. The Depth Distrib-
ution of B within Γ can be computed in time within O(n logd n + e + nk
d+1
2 ),
where e ∈O(n2) is the number of edges of G.
Proof. We describe an algorithm that runs in time within the bound in the
lemma. The algorithm ﬁrst computes the intersection graph G of B in time within
O(n logd n + e) [14], as well as the k-degeneracy of this graph and a degenerate
ordering O of the vertices in time within O(n + e) [20]. The algorithm then
iterates over O maintaining the invariant that, after the i-th step, the Depth
Distribution of the boxes corresponding to the vertices v1, v2, . . . , vi of the
ordering has been correctly computed.
For any subset V of vertices of G, let DDΓ
B(V ) denote the Depth Distrib-
ution within Γ of the boxes in B corresponding to the vertices in V . Also, for
i ∈[1..n] let O[1..i] denote the ﬁrst i vertices of O, and O[i] denote the i-th ver-
tex of O. From DDΓ
B(O[1..i-1]) (which the algorithm “knows” after the (i−1)-th
iteration), DDΓ
B(O[1..i]) can be obtained as follows: (i.) let P be the subset of
O[1..i-1] connected with O[i]; (ii.) compute DDO[i]
B
(P ∪{O[i]}) in time within
O(k
d+1
2 log k) using SDC-DDistribution (note that the domain this time is O[i]

Depth Distribution in High Dimensions
47
itself, instead of Γ); (iii.) add to (DDΓ
B(O[1..i]))1 the value of (DDO[i]
B
(P ∪
O[i]))1; and (iv.) for all j = [2..k+1], substract from (DDΓ
B(O[1..i]))j−1 the
value of (DDO[i]
B
(P ∪O[i]))j and add it to (DDΓ
B(O[1..i-1]))j.
Since the updates to the Depth Distribution in each step take time within
O(k
d+1
2 log k), and there are n such steps, the result of the lemma follows.
⊓⊔
Unlike the algorithm sensitive to the proﬁle, this one can run in time within
O(n1+ d+1
2 ) (e.g. when k = n), which is only better than the O(n
d+1
2 ) complexity
of SDC-DDistribution for values of the degeneracy k within O(n1−2
d ).
Applying the same technique to the computation of Klee’s Measure and
Maximum Depth yields improved solutions as well:
Corollary 9. Let B be a set of boxes and Γ be a domain box, and let
k be the degeneracy of the intersection graph G of the boxes in B. The
Klee’s Measure and Maximum Depth of B within Γ can be computed in
time within O(n logd n+e+nk
d
2 ), where e ∈O(n2) is the number of edges of G.
Such reﬁnements of the worst-case complexity analysis are only examples and
can be applied to many other problems handling high dimensional data inputs.
We discuss a selection in the next section.
5
Discussion
The Depth Distribution captures many of the features in common between
Klee’s measure and Maximum Depth, so that new results on the compu-
tation of the Depth Distribution will yield corresponding results for those
two measures, and has its own applications of interest. Nevertheless, there is no
direct reduction from Klee’s measure or Maximum Depth to Depth Distri-
bution, as the latter is computationally more costly, and clarifying further the
relationship between these problems will require ﬁner models of computation.
We discuss below some further issues to ponder about those measures.
Discrete variants. In practice, multidimensional range queries are applied to a
database of multidimensional points. This yields discrete variants of each of the
problems previously discussed [1,24]. In the Discrete Klee’s measure, the
input is composed of not only a set B of n boxes, but also of a set S of m points.
The problem is now to compute not the volume of the union of the boxes, but the
number (and/or the list) of points which are covered by those boxes. Similarly,
one can deﬁne a discrete version of the Maximum Depth (which points are
covered by the maximum number of boxes) and of the Depth Distribution
(how many and which points are covered by exactly i boxes, for i ∈[1..n]).
Interestingly enough, the computational complexity of these discrete variants is
much less than that of their continuous versions when there are reasonably few
points [24]: the discrete variant becomes hard only when there are many more
points than boxes [1]. Nevertheless, “easy” conﬁgurations of the boxes also yield
“easy” instances in the discrete case: it will be interesting to analyze the discrete

48
J. Barbay et al.
variants of those problems according to the measures of proﬁle and k-degeneracy
introduced on the continuous versions.
Tighter Bounds. Chan [10] conjectured that a complexity of Ω(nd/2) is required
to compute the Klee’s Measure, and hence to compute the Depth Distri-
bution. However, the output of Depth Distribution gives much more infor-
mation than the Klee’s Measure, of which a large part can be ignored during
the computation of the Klee’s Measure (while it is required for the computa-
tion of the Depth Distribution). It is not clear whether even a lower bound
of Ω(nd/2+ϵ) can be proven on the computational complexity of the Depth
Distribution given this fact.
Funding. All authors were supported by the Millennium Nucleus “Information and
Coordination in Networks” ICM/FIC RC130003. J´er´emy Barbay and Pablo P´erez-
Lantero were supported by the projects CONICYT Fondecyt/Regular nos 1170366 and
1160543 (Chile) respectively, while Javiel Rojas-Ledesma was supported by CONICYT-
PCHA/Doctorado Nacional/2013-63130209 (Chile).
References
1. Khamis, M.A., Ngo, H.Q., R´e, C., Rudra, A.: Joins via geometric resolutions: worst-
case and beyond. In: Proceedings of the 34th ACM Symposium on Principles of
Database Systems (PODS), Melbourne, Victoria, Australia, 31 May–4 June, 2015,
pp. 213–228 (2015)
2. Afshani, P.: Fast computation of output-sensitive maxima in a word RAM. In: Pro-
ceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algo-
rithms (SODA), Portland, Oregon, USA, January 5–7, 2014, pp. 1414–1423. SIAM
(2014)
3. Afshani, P., Arge, L., Larsen, K.G.: Higher-dimensional orthogonal range reporting
and rectangle stabbing in the pointer machine model. In: Symposuim on Compu-
tational Geometry (SoCG), Chapel Hill, NC, USA, June 17–20, 2012, pp. 323–332
(2012)
4. Afshani, P., Barbay, J., Chan, T.M.: Instance-optimal geometric algorithms. J.
ACM (JACM) 64(1), 3:1–3:38 (2017)
5. Barbay, J., P´erez-Lantero, P., Rojas-Ledesma, J.: Depth distribution in high dimen-
sion. ArXiv e-prints (2017)
6. Bentley, J.L.: Algorithms for Klee’s rectangle problems. Unpublished notes (1977)
7. Bringmann, K.: An improved algorithm for Klee’s measure problem on fat boxes.
Comput. Geom. Theor. Appl. 45(5–6), 225–233 (2012)
8. Bringmann, K.: Bringing order to special cases of Klee’s measure problem. In:
Chatterjee, K., Sgall, J. (eds.) MFCS 2013. LNCS, vol. 8087, pp. 207–218. Springer,
Heidelberg (2013). doi:10.1007/978-3-642-40313-2 20
9. Chan, T.M.: A (slightly) faster algorithm for Klee’s Measure Problem. In: Proceed-
ings of the 24th ACM Symposium on Computational Geometry (SoCG), College
Park, MD, USA, June 9–11, 2008, pp. 94–100 (2008)
10. Chan, T.M.: Klee’s measure problem made easy. In: 54th Annual IEEE Symposium
on Foundations of Computer Science (FOCS), Berkeley, CA, USA, 26–29 October,
2013, pp. 410–419 (2013)

Depth Distribution in High Dimensions
49
11. Coppersmith, D., Winograd, S.: Matrix multiplication via arithmetic progressions.
In: Proceedings of the 19th Annual ACM Symposium on Theory of Computing
(STOC), New York, USA, pp. 1–6. ACM (1987)
12. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms,
3rd edn. MIT Press, Cambridge (2009)
13. d’Amore, F., Nguyen, V.H., Roos, T., Widmayer, P.: On optimal cuts of hyperrec-
tangles. Computing 55(3), 191–206 (1995)
14. Edelsbrunner, H.: A new approach to rectangle intersections part I. J. Comput.
Math. (JCM) 13(3–4), 209–219 (1983)
15. Fredman, M.L., Weide, B.W.: On the complexity of computing the measure of
∪n
1 [ai; bi]. Commun. ACM (CACM) 21(7), 540–544 (1978)
16. Gall, F.L.: Powers of tensors and fast matrix multiplication. In: International Sym-
posium on Symbolic and Algebraic Computation (ISSAC), Kobe, Japan, July 23–
25, 2014, pp. 296–303. ACM (2014)
17. Kirkpatrick, D.G., Seidel, R.: Output-size sensitive algorithms for ﬁnding maxi-
mal vectors. In: Proceedings of the First Annual Symposium on Computational
Geometry (SoCG), Baltimore, Maryland, USA, June 5–7, 1985, pp. 89–96 (1985)
18. Klee, V.: Can the measure of ∪n
1 [ai; bi] be computed in less than O(n log n) steps?
Am. Math. Mon. (AMM) 84(4), 284–285 (1977)
19. Lick, D.R., White, A.T.: k-Degenerate graphs. Can. J. Math. (CJM) 22, 1082–1096
(1970)
20. Matula, D.W., Beck, L.L.: Smallest-last ordering and clustering and graph coloring
algorithms. J. ACM (JACM) 30(3), 417–427 (1983)
21. Moﬀat, A., Petersson, O.: An overview of adaptive sorting. Aust. Comput. J. (ACJ)
24(2), 70–77 (1992)
22. Overmars, M.H., Yap, C.: New upper bounds in Klee’s measure problem. SIAM J.
Comput. (SICOMP) 20(6), 1034–1045 (1991)
23. Strassen, V.: Gaussian elimination is not optimal. Numer. Math. 13(4), 354–356
(1969)
24. Yildiz, H., Hershberger, J., Suri, S.: A discrete and dynamic version of Klee’s
measure problem. In: Proceedings of the 23rd Annual Canadian Conference on
Computational Geometry (CCCG), Toronto, Ontario, Canada, August 10–12, 2011
(2011)

An Improved Lower Bound on the Growth
Constant of Polyiamonds
Gill Barequet(B), Mira Shalah, and Yufei Zheng
Department of Computer Science, Technion—Israel Institute of Technology,
32000 Haifa, Israel
{barequet,mshalah,yufei}@cs.technion.ac.il
Abstract. A polyiamond is an edge-connected set of cells on the tri-
angular lattice. In this paper we provide an improved lower bound on
the asymptotic growth constant of polyiamonds, proving that it is at
least 2.8424. The proof of the new bound is based on a concatenation argu-
ment and on elementary calculus. We also suggest a nontrivial extension
of this method for improving the bound further. However, the proposed
extension is based on an unproven (yet very reasonable) assumption.
Keywords: Polyiamonds · Lattice animals · Growth constant
1
Introduction
A polyomino of size n is an edge-connected set of n cells on the square lattice Z2.
Similarly, a polyiamond of size n is an edge-connected set of n cells on the
two-dimensional triangular lattice. Fixed polyiamonds are considered distinct if
they have diﬀerent shapes or orientations. In this paper we consider only ﬁxed
polyiamonds, and so we refer to them in the sequel simply as “polyiamonds.”
Fig. 1 shows polyiamonds of size up to 5.
In general, a connected set of cells on a lattice is called a lattice animal. The
fundamental combinatorial problem concerning lattice animals is “How many
animals with n cells are there?” The study of lattice animals began in parallel
more than half a century ago in two diﬀerent communities. In statistical physics,
Temperley [20] investigated the mechanics of macro-molecules, and Broadbent
and Hammersley [6] studied percolation processes. In mathematics, Harary [11]
composed a list of unsolved problems in the enumeration of graphs, and Eden [7]
analyzed cell growth problems. Since then, counting animals has attracted much
attention in the literature. However, despite serious eﬀorts over the last 50 years,
counting polyominoes is still far from being solved, and is considered [2] one of
the long-standing open problems in combinatorial geometry.
The symbol A(n) usually denotes the number of polyominoes of size n; See
sequence A001168 in the On-line Encyclopedia of Integer Sequences (OEIS) [1].
Work on this paper by all authors has been supported in part by ISF Grant 575/15.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 50–61, 2017.
DOI: 10.1007/978-3-319-62389-4 5

An Improved Lower Bound on the Growth Constant of Polyiamonds
51
(a) n = 1: Two moniamonds
(b) n = 2: Three diamonds
(c) n = 3: 6 triamonds
(d) n = 4: 14 tetriamonds
(e) n = 5: 36 pentiamonds
Fig. 1. Polyiamonds of sizes 1 ≤n ≤5
Since no analytic formula for the number of animals is yet known for any nontriv-
ial lattice, a great portion of the research has so far focused on eﬃcient algorithms
for counting animals on lattices, primarily on the square lattice. Elements of the
sequence A(n) are currently known up to n = 56 [12]. The growth constant of
polyominoes was also treated extensively in the literature, and a few asymp-
totic results are known. Klarner [13] showed that the limit λ := limn→∞
n
A(n)
exists, and the main problem so far has been to evaluate this constant. The
convergence of A(n + 1)/A(n) to λ (as n→∞) was proven only three decades

52
G. Barequet et al.
later by Madras [16], using a novel pattern-frequency argument. The best-known
lower and upper bounds on λ are 4.0025 [4] and 4.6496 [14], respectively. It is
widely believed (see, e.g., [8,9]) that λ ≈4.06, and the currently best estimate,
λ = 4.0625696 ± 0.0000005, is due to Jensen [12].
In the same manner, let T(n) denote the number of polyiamonds of size n
(sequence A001420 in the OEIS). Elements of the sequence T(n) were com-
puted up to n = 75 [10, p. 479] using a transfer-matrix algorithm by Jensen
[12, p. 173], adapting his original polyomino-counting algorithm [12]. Earlier
counts were given by Lunnon [15] up to size 16, by Sykes and Glen [19]
up to size 22,1 and by Aleksandrowicz and Barequet [3] (extending Redelmeier’s
polyomino-counting algorithm [18]) up to size 31.
Similarly to polyominoes, the limits limn→∞
n
T(n) and limn→∞T(n +
1)/T(n) exist and are equal. Let, then, λT = limn→∞
n
T(n) denote the growth
constant of polyiamonds. Klarner [13, p. 857] showed that λT ≥2.13 by taking
the square root of 4.54, a lower bound he computed for the growth constant of
animals on the rhomboidal lattice, using the fact that a rhombus is made of two
neighboring equilateral triangles. This bound is also mentioned by Lunnon [15,
p. 98]. Rands and Welsh [17] used renewal sequences in order to show that
λT ≥(T(n)/(2(1 + λT )))1/n
(1)
for any n ∈N. Substituting the easy upper bound2 λT ≤4 in the right-hand
side of this relation, and knowing at that time elements of the sequence T(n)
for 1 ≤n ≤20 only (data provided by Sykes and Glen [19]), they used T(20) =
173, 338, 962 to show that λT ≥(T(20)/10)1/20 ≈2.3011.3 Nowadays, since we
know T(n) up to n = 75,4 we can obtain, using the same method, that λT ≥
(T(75)/10)1/75 ≈2.7714. We can even do slightly better than that. Substituting
in Eq. (1) the upper bound λT ≤3.6050 we obtained elsewhere [5], we see that
λT ≥(T(75)/(2(1+3.6050)))1/75 ≈2.7744. However, we can still improve on this.
1 Note that in this reference the lattice is called “honeycomb” (hexagonal) and the
terms should be doubled. The reason for this is that the authors actually count clus-
ters of vertices on the hexagonal lattice, whose connectivity is the same as that of
cells on the triangular lattice, with no distinction between the two possible orienta-
tions of the latter cells. This is why polyiamonds are often regarded in the literature
as site animals on the hexagonal lattice, and polyhexes (cell animals on the hexag-
onal lattice) are regarded as site animals on the triangular lattice, which sometimes
causes confusion.
2 This easy upper bound, based on an idea of Eden [7] was described by Lunnon
[15, p. 98]. Every polyiamond P can be built according to a set of n−1 “instructions”
taken from a superset of size 2(n−1). Each instruction tells us how to choose a lattice
cell c, neighboring a cell already in P, and add c to P. (Some of these instruction
sets are illegal, and some other sets produce the same polyiamonds, but this only
helps.) Hence, λT ≤limn→∞
2(n−1)
n−1
1/n = 4.
3 We wonder why Rands and Welsh did not use T(22) = 1, 456, 891, 547 (which was
also available in their reference [19]) to show that λT ≥(T(22)/10)1/22 ≈2.3500.
4 T(75) = 15, 936, 363, 137, 225, 733, 301, 433, 441, 827, 683, 823.

An Improved Lower Bound on the Growth Constant of Polyiamonds
53
Based on existing data, it is believed [19] (but has never been proven) that
λT = 3.04±0.02. In this paper we improve the lower bound on λT , showing that
λT ≥2.8424. The new lower bound is obtained by combining a concatenation
argument with elementary calculus.
2
Preliminaries
We orient the triangular lattice as is shown in Fig. 2(a), and deﬁne a lexico-
graphic order on the cells of the lattice as follows: A triangle t1 is smaller than
triangle t2 ̸= t1 if the lattice column of t1 is to the left of the column of t2, or if
t1, t2 are in the same column and t1 lies below t2. Triangles that look like a “left
arrow” (Fig. 2(b)) are of Type 1, and triangles that look like a “right arrow”
(Fig. 2(c)) are of Type 2. In addition, let T1(n) be the number of polyiamonds
of size n whose largest (top-right) triangle is of Type 1, and let T2(n) be the
number of polyiamonds of size n whose largest triangle is of Type 2.
(b) Left arrow
(a) A polyiamond
(c) Right arrow
Fig. 2. Polyiamonds on the triangular lattice
Let x(n) (0 < x(n) < 1) denote the fraction of polyiamonds of Type 1 out of
all polyiamonds of size n, that is, T1(n) = x(n)T(n) and T2(n) = (1−x(n))T(n).
In addition, let y(n) = T2(n)/T1(n) = (1 −x(n))/x(n) denote the ratio between
polyiamonds of Type 2 and polyiamonds of Type 1 (all of size n).
A concatenation of two polyiamonds P1, P2 is the union of P1 and a translated
copy of P2, so that the largest triangle of P1 is attached to the smallest triangle
of P2, and all cells of P1 are smaller than the translates of cells of P2. We use a
concatenation argument in order to improve the lower bound on λT .
3
The Bound
Our proof of a lower bound on λT uses the division of polyiamonds into Type 1
and Type 2, but does not employ the asymptotic proportion between the two
types.

54
G. Barequet et al.
Theorem 1. λT ≥2.8424.
Proof. First note that by rotational symmetry, the number of polyiamonds of
size n, whose smallest (bottom-left) triangle is of Type 2, is T1(n). Similarly, the
number of polyiamonds, whose smallest triangle is of Type 1, is T2(n).
We proceed with a concatenation argument tailored to the speciﬁc case of
the triangular lattice. Interestingly, not all pairs of polyiamonds of size n can
be concatenated. In addition, there exist many polyiamonds of size 2n which
cannot be represented as the concatenation of two polyiamonds of size n. Let us
count carefully the amount of pairs of polyiamonds that can be concatenated.
– Polyiamonds, whose largest triangle is of Type 1, can be concatenated only
to polyiamonds whose smallest triangle is of Type 2, and this can be done in
two diﬀerent ways (see Figs. 3(a–c)). There are 2(T1(n))2 concatenations of
this kind.
– Polyiamonds, whose largest triangle is of Type 2, can be concatenated, in
a single way, only to polyiamonds whose smallest triangle is of Type 1 (see
Figs. 3(d, e)). There are (T2(n))2 concatenations of this kind.
Altogether, we have 2(T1(n))2+(T2(n))2 possible concatenations, and, as argued
above,
2(T1(n))2 + (T2(n))2 ≤T(2n).
(2)
Let us now ﬁnd an eﬃcient lower bound on the number of concatena-
tions. Equation (2) can be rewritten as T(2n) ≥2(x(n)T(n))2 + ((1 −x(n))
(a) Two polyiamonds
(b) Vertical concatenation (c) Horizontal concatenation
Two attachments of Type-1 and Type-2 triangles
(d) Two polyiamonds
(e) Vertical concatenation
A single attachment of Type-2 and Type-1 triangles
Fig. 3. Possible concatenations of polyiamonds

An Improved Lower Bound on the Growth Constant of Polyiamonds
55
T(n))2 = (3x2(n) −2x(n) + 1)T 2(n). Elementary calculus shows that the func-
tion f(x) = 3x2−2x+1 assumes its minimum at x = 1/3 and that f(1/3) = 2/3.
Hence,
2
3T 2(n) ≤T(2n).
By simple manipulations of this relation, we obtain that
2
3T(n)
1/n ≤
2
3T(2n)
1/(2n).
This implies that the sequence
 2
3T(k)
1/k,
 2
3T(2k)
1/(2k),
 2
3T(4k)
1/(4k), . . . is
monotone increasing for any value of k, and, as a subsequence of
 2
3T(n)
1/n
,
it converges to λT too. Therefore, any term of the form
 2
3T(n)
1/n is a lower
bound on λT . In particular, λT ≥( 2
3T(75))1/75 ≈2.8424.
□
4
Convergence, Interval Containment, and Monotonicity
First, we show a simple relation between the number of polyiamonds of Type 2
of size n and the number of polyiamonds of Type 1 of size n+1.
Observation 2. T2(n) = T1(n + 1) (for all n ∈N).
This simple observation follows from the fact that if the largest triangle t of
a polyiamond P is of Type 1, then the only possible neighboring triangle of t
within P is the triangle immediately below it, and so, removing t from P will not
break P into two parts. This implies a bijection between Type-1 polyiamonds of
size n+1 and Type-2 polyiamonds of size n.
An immediate consequence is that y(n) = T1(n + 1)/T1(n), a fact which we
will use later frequently (see, e.g., the proof of Theorem 4(ii) below).
Next, we prove the convergence of the sequences (T1(n + 1)/T1(n)), x(n),
and y(n), and ﬁnd their limits.
Observation 3. The sequence (T1(n + 1)/T1(n))∞
n=1 converges.
Indeed, polyiamonds of Type 1 fulﬁll all the premises of Madras’s Ratio
Limit Theorem [16, Theorem 2.2]. (A more detailed explanation is given in the
full version of the paper.) Hence, the limit limn→∞T1(n+1)/T1(n) exists and is
equal to some constant μ, whose value is speciﬁed (indirectly) in the following
theorem.
Theorem 4. (i) μ = λT ; (ii) lim
n→∞y(n) = λT ; (iii) lim
n→∞x(n) = 1/(λT + 1).

56
G. Barequet et al.
Proof.
(i) On the one hand, by Madras [16] we know that the limit λT
:=
limn→∞T(n + 1)/T(n) exists. On the other hand, we have that
T(n + 1)
T(n)
= T1(n + 1) + T2(n + 1)
T1(n) + T2(n)
= T1(n + 1) + T1(n + 2)
T1(n) + T1(n + 1)
=
1 + T1(n+2)
T1(n+1)
T1(n)
T1(n+1) + 1
−→
n→∞
1 + μ
1
μ + 1 = μ.
(Note that the convergence in the last step relies on the fact that the
sequence (T1(n + 1)/T1(n)) has a limit μ.) Hence, μ = λT .
(ii)
y(n) = T2(n)
T1(n) = T1(n + 1)
T1(n)
−→
n→∞μ = λT .
(iii)
x(n) = T1(n)
T(n) =
T1(n)
T1(n) + y(n)T1(n) =
1
1 + y(n) −→
n→∞
1
λT + 1.
□
Now, we show relations between bounds on the entire sequence (x(n)) and
bounds on the entire sequence (y(n)) ≡(T(n + 1)/T(n)).
Theorem 5. Let d, e be two constants.
(i) x(n) ≥1/d for all n ∈N ←→T(n + 1)/T(n) ≤d−1 for all n ∈N;
(ii) x(n) ≤1/e for all n ∈N ←→T(n + 1)/T(n) ≥e−1 for all n ∈N.
Proof.
(i) First,
x(n) ≥1
d ←→T1(n)
T(n) ≥1
d ←→
T1(n)
T1(n) + T1(n + 1) ≥1
d ←→
T1(n) + T1(n + 1)
T1(n)
≤d ←→1 + T1(n + 1)
T1(n)
≤d ←→T1(n + 1)
T1(n)
≤d −1.
Second,
T(n + 1)
T(n)
= T1(n + 1) + T1(n + 2)
T1(n) + T1(n + 1)
= 1 + T1(n + 2)/T1(n + 1)
T1(n)/T1(n + 1) + 1
≤
1 + (d −1)
1/(d −1) + 1 = d −1.
(ii) The proof is completely analogous to that of item (i).
□

An Improved Lower Bound on the Growth Constant of Polyiamonds
57
Finally, we observe the relation between the directions of monotonicity
(if exist) of the sequences (x(n)) and (y(n)).
Observation 6. If any of the two sequences (x(n)) and (y(n)) is monotone,
then the other sequence is also monotone but in the opposite direction.
Indeed, this follows immediately from the equality y(n) = (1 −x(n))/x(n) =
1/x(n) −1. The available data suggest that (x(n)) be monotone decreasing and
that (y(n)) be monotone increasing. Note that the monotonicity of x(n) (or y(n))
neither implies, nor is implied by, the monotonicity of (T(n + 1)/T(n)).
5
Conditional Lower Bounds
Let L(n) denote the number of animals on a lattice L. It is widely believed (but
has never been proven) that asymptotically
L(n) ∼CLn−θLλL,
(3)
where CL, θL, and λL are constants which depend on L.5 If this were true, then
it would guarantee the existence of the limit λL := limn→∞L(n + 1)/L(n), the
growth constant of animals on the lattice L. As was mentioned in the introduc-
tion, the existence of this limit was proven by Madras [16] without assuming the
relation in Eq. (3). Furthermore, it is widely believed that the “ratio sequence”
(L(n + 1)/L(n)) is monotone increasing.6 Available data (numbers of animals
on various lattices) support this belief, but it was unfortunately never proven.
Such monotonicity of the ratio sequence would imply that the entire sequence
lies below its limit. (In fact, this may be the case even without monotonicity.)
Consequently, every element of the ratio sequence would be a lower bound on
the growth constant of animals on L. In particular, it would imply the lower
bound λT ≥T(75)/(74) ≈2.9959, only 0.05 short of the estimated value.
Another plausible direction for setting a lower bound on λT would be to
prove that the entire sequence (T(n+1)/T(n)) lies below some constant c > λT ,
that is, T(n + 1)/T(n) ≤c for all n ∈N. As we demonstrate at the end of this
section, proving this relation for any arbitrary constant c, even a very large one,
would improve the lower bound on λT , provided in Sect. 3.
In the proof of Theorem 1, we considered the three possible kinds of concate-
nations of the largest and the smallest triangles in two polyiamonds of size n. In
fact, more compositions which preserve the lexicographical order of the polyia-
monds can be obtained by using horizontal attachments of Type-1 and Type-2
5 In fact, it is widely believed (but not proven) that the constant θ is common to all
lattices in the same dimension. In particular, there is evidence that θ = 1 for all
lattices in two dimensions.
6 Madras [16, Proposition 4.2] proved “almost monotonicity” for all lattices, namely,
that L(n + 2)/L(n) ≥(L(n + 1)/L(n))2 −ΓL/n for all suﬃciently large values of n,
where ΓL is a constant which depends on L. Note that if ΓL = 0, then we have
L(n + 2)/L(n + 1) ≥L(n + 1)/L(n), i.e., that the ratio sequence of L is monotone
increasing.

58
G. Barequet et al.
triangles which are not the largest or smallest triangles in the respective polyi-
amonds. The following cases, shown in Fig. 4, are categorized by the type of
extreme triangles (t1, t2) in a pair of composed polyiamonds, where t1 is the
largest triangle of the smaller polyiamond and t2 is the smallest triangle of the
larger polyiamond. Therefore, these cases are distinct. In the ﬁgure, attached
triangles are colored red, the direction of the attachment is marked by a small
arrow, and an “×” sign marks an area free of triangles of the polyiamonds. The
triangles in gray are the largest (or smallest) in polyiamonds of size less than n,
which are then extended to full size-n polyiamonds. (In Case (c), the red and
gray triangles identify.) For the purpose of further computation, let xi denote
the ratio T1(n−i)/T(n −i), for i = 0, 1, 2, 3, and let c be an upper bound on the
sequence (T(n + 1)/T(n)). All compositions below do not appear in the proof of
Theorem 1.
(a.1)
(a.2)
(a.3)
(a.4)
(a.5)
Largest triangle of Type 1
Smallest triangle of Type 1
(a) Case (1,1) (symmetrically, Case (2,2))
a
b
c
d
b′
a′
d′
c′
(b.1)
(b.2)
(b.3)
(b.4)
Largest triangle of Type 1
Smallest triangle of Type 2
(b) Case (1,2)
(c.1)
(c.2)
Largest triangle of Type 2
Smallest triangle of Type 1
(c) Case (2,1)
Fig. 4. Additional compositions of polyiamonds (Color ﬁgure online)
(1,1) In this type of composition, each one of the polyiamond types shown
in Figs. 4(a.1–a.3) is composed with each one of the types shown in
Figs. 4(a.4, a.5). Speciﬁcally, the attached triangle in types (a.1) and (a.2,
a.3) is the largest and third largest, respectively, and the attached trian-
gle in types (a.4, a.5) is the second smallest. The number of polyiamonds
whose largest triangle is of Type 1 (Fig. 4(a.1)) is T1(n), and among these
polyiamonds, the number of polyiamonds shown in Fig. 4(a.2) is T1(n−3).
One way to see it is that, for every polyiamond of of size n−3 whose largest
triangle (shown in gray) is of Type 1, a column-like polyiamond composed
of three triangles whose largest is of Type 1 is attached to the right of it.
With a similar reasoning, there are T2(n −3), T1(n −2), and T2(n −2)

An Improved Lower Bound on the Growth Constant of Polyiamonds
59
polyiamonds of the types shown in Figs. 4(a.3–a.5), respectively. Hence,
the number of compositions in this category is

T1(n) + T1(n −3) + T2(n −3)

T1(n −2) + T2(n −2)

.
By Observation 2, we can substitute T2(n−i) by T1(n−i+1), and by the
deﬁnition of xi, replace T1(n −i) by xiT(n −i). Then, by the deﬁnition
of c, we have T(n −i) ≥
T (n)
ci , and we conclude that in this category
we have

x0T(n) + x3T(n −3) + x2T(n −2)

x2T(n −2) + x1T(n −1)

≥

x0 + x2
c2 + x3
c3
 x1
c + x2
c2

T 2(n).
(4)
Notice that, in principal, the attachment of smaller and larger triangles
(of the two polyiamonds, respectively) can be counted as well. However, in
such cases, higher orders of 1/c would appear in Eq. 4. The contribution of
such compositions is relatively insigniﬁcant, thus, they are not considered.
(1,2) In this case, we count attachments of the largest and the third largest
triangles in the smaller polyiamonds, and the smallest and third smallest
triangles in the larger polyiamonds; see Figs. 4(b.1–b.4). Table 1 lists all
possible attachments and their corresponding numbers of compositions.
To sum up, the total number of compositions of this category is
3T 2
1 (n −3) + 6T1(n −3)T2(n −3) + 3T 2
2 (n −3) ≥3
x2
c2 + x3
c3
2
T 2(n).
(2,1) The numbers of polyiamonds shown in Figs. 4(c.1, c.2) are both T1(n−1).
Therefore, there are
T 2
1 (n −1) ≥x2
1
c2 T 2(n)
compositions in this category.
(2,2) By rotational symmetry, the number of compositions in this category is
the same as that of Case (1,1).
Table 1. Additional compositions in Case (1,2)
Attached triangles Number of compositions
(a ↔a’,c’)
T 2
2 (n −3) + T1(n −3)T2(n −3)
(b ↔a’,b’,c’,d’)
2T 2
2 (n −3) + 2T1(n −3)T2(n −3)
(c ↔a’,c’)
T 2
1 (n −3) + T1(n −3)T2(n −3)
(d ↔a’,b’,c’,d’)
2T 2
1 (n −3) + 2T1(n −3)T2(n −3)

60
G. Barequet et al.
Summing up the four cases and the three cases in the proof of Theorem 1,
we obtain the relation

1 −2x0 + 3x0
2 + 2

x0 + x2
c2 + x3
c3
 x1
c + x2
c2

+ x2
1
c2 + 3
x2
c2 + x3
c3
2
T 2(n)
(5)
≤T(2n)
In order to set a good (high) lower bound on λT , we need good lower bounds
on x0, x1, x2, x3 and a good upper bound on c. Then we obtain a relation of the
form μ · T 2(n) ≤T(2n), where μ is a constant, and proceed in the same way as
in the proof of Theorem 1. If we denote the 4-variable polynomial multiplying
T 2(n) by f(x0, x1, x2, x3), we ﬁnd that f(·, ·, ·, ·) is monotone decreasing when
x1, x2, x3 decrease, so its inﬁmum is obtained when x1 = x2 = x3 = 0, which is
useless.
However, the actual data show that the sequence (T(n+1)/T(n)) is monotone
increasing with the limit λT ≈3.04. Assume, then, without proof that (T(n +
1)/T(n)) ≤4 for all n ∈N. By Theorem 5, this implies that x(n) > 0.2 for
all values of n. Using these bounds, we see that the left-hand side of Eq. (5)
becomes a quadratic function g(x0) = 3x2
0−1.875x0+1.00519. (All computations
were done with a much higher precision.) Elementary calculus shows that g(x0)
assumes its minimum at x0 = 0.3125 and that g(0.3125) = 0.7122. Hence,
0.7122 · T 2(n) ≤T(2n).
With manipulations similar to those in the proof of Theorem 1, it can be shown
that the sequence

0.7122 · T(2ik)
1/(2ik)∞
i=0 is monotone increasing for any
value of k, and, as a subsequence of

0.7122 · T(n)
1/n
, it converges to λT as
well. Therefore, any term of the form

0.7122 · T(n)
1/n is a lower bound on λT .
In particular, λT ≥(0.7122 · T(75))1/75 ≈2.8449, which is a slight improvement
over the lower bound on λT , shown in Sect. 3, and is pending only the correctness
of the assumption that T(n + 1)/T(n) ≤4 for all n ∈N.
References
1. The On-Line Encyclopedia of Integer Sequences. http://oeis.org
2. The Open Problems Project. http://cs.smith.edu/∼orourke/TOPP/
3. Aleksandrowicz, G., Barequet, G.: Counting d-dimensional polycubes and nonrec-
tangular planar polyominoes. Int. J. Comput. Geometry Appl. 19, 215–229 (2009)
4. Barequet, G., Rote, G., Shalah, M.: λ > 4: an improved lower bound on the growth
constant of polyominoes. Comm. ACM 59, 88–95 (2016)
5. Barequet, G., Shalah, M.: Improved bounds on the growth constant of polyia-
monds. In: Proceedings of the 32nd European Workshop on Computational Geom-
etry, Lugano, Switzerland, pp. 67–70, March-April 2016
6. Broadbent, S.R., Hammersley, J.M.: Percolation processes: I. Crystals and mazes.
Proc. Camb. Philosophical Soc. 53, 629–641 (1957)

An Improved Lower Bound on the Growth Constant of Polyiamonds
61
7. Eden, M.: A two-dimensional growth process. In: Proceedings of the 4th Berkeley
Symposium on Mathematical Statistics and Probability, IV, Berkeley, CA, pp.
223–239 (1961)
8. Gaunt, D.S.: The critical dimension for lattice animals. J. Phys. A Math. General
13, L97–L101 (1980)
9. Gaunt, D.S., Sykes, M.F., Ruskin, H.: Percolation processes in d-dimensions. J.
Phys. A Math. General 9, 1899–1911 (1976)
10. Guttmann, A.J.: Polygons, Polyominoes, and Polycubes. Lecture Notes in Physics,
vol. 775. Springer and Canopus Academic Publishing Ltd., Dordrecht (2009)
11. Harary, F.: Unsolved problems in the enumeration of graphs. Pub. Math. Inst.
Hungarian Academy Sci. 5, 1–20 (1960)
12. Jensen, I.: Counting polyominoes: a parallel implementation for cluster computing.
In: Sloot, P.M.A., Abramson, D., Bogdanov, A.V., Gorbachev, Y.E., Dongarra,
J.J., Zomaya, A.Y. (eds.) ICCS 2003. LNCS, vol. 2659, pp. 203–212. Springer,
Heidelberg (2003). doi:10.1007/3-540-44863-2 21
13. Klarner, D.A.: Cell growth problems. Canadian J. Math. 19, 851–863 (1967)
14. Klarner, D.A., Rivest, R.L.: A procedure for improving the upper bound for the
number of n-ominoes. Canadian J. Math. 25, 585–602 (1973)
15. Lunnon, W.F.: Counting hexagonal and triangular polyominoes. In: Read, R.C.
(ed.) Graph Theory and Computing, pp. 87–100. Academic Press, New York (1972)
16. Madras, N.: A pattern theorem for lattice clusters. Ann. Comb. 3, 357–384 (1999)
17. Rands, B.M.I., Welsh, D.J.A.: Animals, trees and renewal sequences. IMA J. Appl.
Math. 27, 1–17 (1981). Corrigendum 28, 107 (1982)
18. Redelmeier, D.H.: Counting polyominoes: yet another attack. Discrete Math. 36,
191–203 (1981)
19. Sykes, M.F., Glen, M.: Percolation processes in two dimensions: I. Low-density
series expansions. J. Phys. A Math. General 9, 87–95 (1976)
20. Temperley, H.N.V.: Combinatorial problems suggested by the statistical mechanics
of domains and of rubber-like molecules. Phys. Rev. 2(103), 1–16 (1956)

Constrained Routing Between
Non-Visible Vertices
Prosenjit Bose1, Matias Korman2, Andr´e van Renssen3,4(B),
and Sander Verdonschot1
1 School of Computer Science, Carleton University, Ottawa, Canada
jit@scs.carleton.ca, sander@cg.scs.carleton.ca
2 Tohoku University, Sendai, Japan
mati@dais.is.tohoku.ac.jp
3 National Institute of Informatics, Tokyo, Japan
andre@nii.ac.jp
4 JST, ERATO, Kawarabayashi Large Graph Project, Tokyo, Japan
Abstract. Routing is an important problem in networks. We look at
routing in the presence of line segment constraints (i.e., obstacles that
our edges are not allowed to cross). Let P be a set of n vertices in the
plane and let S be a set of line segments between the vertices in P,
with no two line segments intersecting properly. We present the ﬁrst 1-
local O(1)-memory routing algorithm on the visibility graph of P with
respect to a set of constraints S (i.e., it never looks beyond the direct
neighbours of the current location and does not need to store more than
O(1)-information to reach the target). We also show that when routing
on any triangulation T of P such that S ⊆T, no o(n)-competitive routing
algorithm exists when only considering the triangles intersected by the
line segment from the source to the target (a technique commonly used
in the unconstrained setting). Finally, we provide an O(n)-competitive
1-local O(1)-memory routing algorithm on any such T, which is optimal
in the worst case, given the lower bound.
1
Introduction
Routing is a fundamental problem in graph theory and networking. What makes
this problem challenging is that often in a network the routing strategy must be
local, i.e. the routing algorithm must decide which vertex to forward a message
to based solely on knowledge of the current vertex, its neighbors and a constant
amount of additional information (such as the source and destination vertex).
Routing algorithms are considered geometric when the graph that is routed on is
embedded in the plane, with edges being straight line segments connecting pairs
P. Bose is supported in part by NSERC. M. Korman was partially supported by
MEXT KAKENHI Nos. 12H00855, 15H02665, and 17K12635. A. van Renssen was
supported by JST ERATO Grant Number JPMJER1305, Japan. S. Verdonschot is
supported in part by NSERC, the Ontario Ministry of Research and Innovation, and
the Carleton-Fields Postdoctoral Award.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 62–74, 2017.
DOI: 10.1007/978-3-319-62389-4 6

Constrained Routing Between Non-Visible Vertices
63
of vertices and weighted by the Euclidean distance between their endpoints.
Geometric routing algorithms are important in wireless sensor networks (see
[11,12] for surveys of the area) since they oﬀer routing strategies that use the
coordinates of the vertices to guide the search, instead of the more traditional
routing tables.
Most of the research on this problem has focused on the situation where the
network is constructed by taking a subgraph of the complete Euclidean graph,
i.e. the graph that contains an edge between every pair of vertices and the
length of this edge is the Euclidean distance between the two vertices. We study
this problem in a more general setting with the introduction of line segment
constraints S. Speciﬁcally, let P be a set of n vertices in the plane and let S
be a set of line segments between the vertices in P, with no two line segments
properly intersecting (i.e., anywhere except at the endpoints). Two vertices u
and v can see each other if and only if either the line segment uv does not
properly intersect any constraint or uv is itself a constraint. If two vertices u
and v can see each other, the line segment uv is a visibility edge. The visibility
graph of P with respect to a set of constraints S, denoted Vis(P, S), has P as
vertex set and all visibility edges as edge set. In other words, it is the complete
graph on P minus all non-constraint edges that properly intersect one or more
constraints in S.
This setting has been studied extensively in the context of motion planning
amid obstacles. Clarkson [8] was one of the ﬁrst to study this problem. He
showed how to construct a (1 + ϵ)-spanner of Vis(P, S) with a linear number
of edges. A subgraph H of G is called a t-spanner of G (for t ≥1) if for each
pair of vertices u and v, the shortest path in H between u and v has length at
most t times the shortest path between u and v in G. The smallest value t for
which H is a t-spanner is the spanning ratio or stretch factor of H. Following
Clarkson’s result, Das [9] showed how to construct a spanner of Vis(P, S) with
constant spanning ratio and constant degree. Bose and Keil [4] showed that the
Constrained Delaunay Triangulation is a 2.42-spanner of Vis(P, S). Recently,
the constrained half-Θ6-graph (which is identical to the constrained Delaunay
graph whose empty visible region is an equilateral triangle) was shown to be a
plane 2-spanner of Vis(P, S) [2] and all constrained Θ-graphs with at least 6
cones were shown to be spanners as well [7].
Spanners of Vis(P, S) are desirable because they are sparse and the bounded
stretch factor certiﬁes that paths do not make large detours. However, it is
not known how to route locally on them. To address this issue, we look at
local routing algorithms in the constrained setting, i.e. routing algorithms that
must decide which vertex to forward a message to based solely on knowledge of
the source and destination vertex, the current vertex, all vertices that can be
seen from the current vertex and a constant amount of memory. We deﬁne this
model formally in the next section. Furthermore, we study competitiveness of
our routing algorithms, i.e. the ratio of the length of the path followed by the
routing algorithm and the length of the shortest path in the graph.

64
P. Bose et al.
In the constrained setting, routing has not been studied much. Bose et al. [3]
showed that it is possible to route locally and 2-competitively between any two
visible vertices in the constrained Θ6-graph. Additionally, an 18-competitive
routing algorithm between any two visible vertices in the constrained half-Θ6-
graph was provided. While it seems like a serious shortcoming that these routing
algorithms only route between pairs of visible vertices, in the same paper the
authors also showed that no deterministic local routing algorithm is o(√n)-
competitive between all pairs of vertices of the constrained Θ6-graph, regardless
of the amount of memory it is allowed to use.
In this paper, we develop routing algorithms that work between any pair
of vertices in the constrained setting. We provide a non-competitive 1-local
routing algorithm on the visibility graph of P with respect to a set of con-
straints S. We also show that when routing on any triangulation T of P such
that S ⊆T, no o(n)-competitive routing algorithm exists when only consider-
ing the triangles intersected by the line segment from the source to the target
(a technique commonly used in the unconstrained setting). Finally, we provide an
O(n)-competitive 1-local routing algorithm on T, which is optimal in the worst
case, given the lower bound. Prior to this work, no local routing algorithms were
known to work between any pair of vertices in the constrained setting.
2
Preliminaries
The Θm-graph plays an important role in our routing strategy. We begin with
their deﬁnitions. Deﬁne a cone C to be the region in the plane between two rays
originating from a vertex referred to as the apex of the cone. When constructing
a (constrained) Θm-graph, for each vertex u consider the rays originating from
u with the angle between consecutive rays being 2π/m. Each pair of consecutive
rays deﬁnes a cone. The cones are oriented such that the bisector of some cone
coincides with the vertical halﬂine through u that lies above u. Let this cone
be C0 of u and number the cones in clockwise order around u (see Fig. 1). The
cones around the other vertices have the same orientation as the ones around u.
We write Cu
i to indicate the i-th cone of a vertex u, or Ci if u is clear from the
context. For ease of exposition, we only consider point sets in general position:
no two points lie on a line parallel to one of the rays that deﬁne the cones, no
two points lie on a line perpendicular to the bisector of a cone, and no three
points are collinear.
Let vertex u be an endpoint of a constraint c and let the other endpoint v
that lies in cone Cu
i (if any). The lines through all such constraints c split Cu
i
into several subcones (see Fig. 2). We use Cu
i,j to denote the j-th subcone of Cu
i
(again, numbered in clockwise order). When a constraint c = (u, v) splits a cone
of u into two subcones, we deﬁne v to lie in both of these subcones. We consider
a cone that is not split to be a single subcone.
We now introduce the constrained Θm-graph: for each subcone Ci,j of each
vertex u, add an edge from u to the closest vertex in that subcone that can see
u, where distance is measured along the bisector of the original cone (not the

Constrained Routing Between Non-Visible Vertices
65
C0
C1
C5
C4
C3
C2
u
Fig. 1. The cones with
apex u in the Θ6-graph.
All
points
of
S
have
exactly six cones.
C0,0
C5,0
C4,0
C3,0
C2,0
u
C0,1C1,0
C1,1
C1,2
C4,1
Fig. 2.
The
subcones
with apex u in the con-
strained Θ6-graph (con-
straints denoted as red
thick segments). (Color
ﬁgure online)
C0,0
C2,0
C1,0
C0,0
C2,0
u
C0,1
C1,0
C1,1
C1,2
C1,1
Fig. 3. If we consider the
half-Θ6-graph instead, we
have
the
same
amount
of
cones,
but
diﬀerent
notation.
subcone). More formally, we add an edge between two vertices u and v if v can
see u, v ∈Cu
i,j, and for all points w ∈Cu
i,j that can see u, |uv′| ≤|uw′|, where v′
and w′ denote the projection of v and w on the bisector of Cu
i and |xy| denotes
the length of the line segment between two points x and y. Note that our general
position assumption implies that each vertex adds at most one edge per subcone.
Next, we deﬁne the constrained half-Θ6-graph. This is a generalized version
of the half-Θ6-graph as described by Bonichon et al. [1]. The constrained half-
Θ6-graph is similar to the constrained Θ6-graph with one major diﬀerence: edges
are only added in every second cone. More formally, its cones are categorized
as positive and negative. Let (C0, C2, C1, C0, C2, C1) be the sequence of cones in
counterclockwise order starting from the positive y-axis. The cones C0, C1, and
C2 are called positive cones and C0, C1, and C2 are called negative cones. Note
that the positive cones coincide with the even cones of the constrained Θ6-graph
and the negative cones coincide with the odd ones. We add edges only in the
positive cones (and their subcones). We use Cu
i and Cu
i to denote cones Ci and
Ci with apex u. Note that, by the way in which cones are labeled, for any two
vertices u and v, it holds that v ∈Cu
i if and only if u ∈Cv
i . Analogous to the
subcones deﬁned for the Θ6-graph, constraints split cones into subcones. We call
a subcone of a positive cone a positive subcone and a subcone of a negative
cone a negative subcone (see Fig. 3). We look at the undirected version of these
graphs, i.e. when an edge is added, both vertices are allowed to use it. This is
consistent with previous work on Θ-graphs.
Finally, we deﬁne the constrained Delaunay triangulation. Given any two
visible vertices p and q, the constrained Delaunay triangulation contains an edge
between p and q if and only if pq is a constraint or there exists a circle O with p
and q on its boundary such that there are no vertices of P in the interior of O
is visible to both p and q. For simplicity, we assume that no four vertices lie on
the boundary of any circle.
There are two notions of competitiveness of a routing algorithm. One is to
look at the Euclidean shortest path between the two vertices, i.e. the shortest
path in Vis(P, S), and the other is to compare the routing path to the shortest

66
P. Bose et al.
path in the subgraph of Vis(P, S). A routing algorithm is c-competitive with
respect to the Euclidean shortest path (resp. shortest path in the graph) provided
that the total distance traveled by the message is not more than c times the
Euclidean shortest path length (resp. shortest path length) between the source
and the destination. The routing ratio of an algorithm is the smallest c for which
it is c-competitive. Since the shortest path in the graph between two vertices is at
least as long as the Euclidean shortest path between them, an algorithm that is
c-competitive with respect to the Euclidean shortest path is also c-competitive
with respect to the shortest path in the graph. We use competitiveness with
respect to the Euclidean shortest path when proving upper bounds and with
respect to the shortest path in the graph when proving lower bounds.
We now deﬁne our routing model. Formally, a routing algorithm A is a deter-
ministic k-local, m-memory routing algorithm, if the vertex to which a message
is forwarded from the current vertex s is a function of s, t, Nk(s), and M, where
t is the destination vertex, Nk(s) is the k-neighborhood of s and M is a memory
of size m, stored with the message. The k-neighborhood of a vertex s is the set of
vertices in the graph that can be reached from s by following at most k edges. For
our purposes, we consider a unit of memory to consist of a log2 n bit integer or
a point in R2. Our model also assumes that the only information stored at each
vertex of the graph is Nk(s). Unless stated otherwise, when we say “local”, we
will assume that k = 1 and that |M| ∈O(1), i.e. our algorithms are 1-local and
use a constant amount of memory. Since our graphs are geometric, we identify
each vertex by its coordinates in the plane.
We say that a region R contains a vertex v if v lies in the interior or on the
boundary of R. We call a region empty if it does not contain any vertex of P.
Lemma 2.1 [2]. Let u, v, and w be three arbitrary points in the plane such
that uw and vw are visibility edges and w is not the endpoint of a constraint
intersecting the interior of triangle uvw. Then there exists a convex chain of
visibility edges from u to v in triangle uvw, such that the polygon deﬁned by uw,
wv and the convex chain is empty and does not contain any constraints.
3
Local Routing on the Visibility Graph
In the unconstrained setting, local routing algorithms have focused on subgraphs
of the complete Euclidean graph such as Θ-graphs. There exists a very simple
local routing algorithm that works on all Θ-graphs with at least 4 cones and is
competitive when the graph uses at least 7 cones. This routing algorithm (often
called the Θ-routing) always follows the edge to the closest vertex in the cone that
contains the destination. In the constrained setting, however, a problem arises
if we try to apply this strategy: even though a cone contains the destination it
need not contain any visible vertices, since a constraint may block its visibility.
Hence, the Θ-routing algorithm will often get stuck since it cannot follow any
edge in that cone. In fact, given a set P of points in the plane and a set S of
disjoint segments, no local routing algorithm is known for routing on Vis(P, S).

Constrained Routing Between Non-Visible Vertices
67
When the destination t is visible to the source s, it is possible to route locally
by essentially “following the segment st”, since no constraint can intersect st.
This approach was used to give a 2-competitive 1-local routing algorithm on the
constrained half-Θ6-graph, provided that t is in a positive cone of s [3]. In the
case where t is in a negative cone of s, the algorithm is much more involved and
the competitive ratio jumps to 18.
The stumbling block of all known approaches is the presence of constraints. In
a nutshell, the problem is to determine how to “go around” a constraint in such
a way as to reach the destination and not cycle. This poses the following natural
question: does there exist a deterministic 1-local routing algorithm that always
reaches the destination when routing on the visibility graph? In this section, we
answer this question in the aﬃrmative and provide such a 1-local algorithm. The
main idea is to route on a planar subgraph of Vis(P, S) that can be computed
locally.
In [10] it was shown how to route locally on a plane geometric graph. Subse-
quently, in [6], a modiﬁed algorithm was presented that seemed to work better in
practice. Both algorithms are described in detail in [6], where the latter algorithm
is called FACE-2 and the former is called FACE-1. Neither of the algorithms is
competitive. FACE-1 reaches the destination after traversing at most Θ(n) edges
in the worst case and FACE-2 traverses Θ(n2) edges in the worst case. Although
FACE-1 performs better in the worst case, FACE-2 performs better on average
in random graphs generated by points uniformly distributed in the unit square.
Coming back to our problem of routing locally from a source s to a destination
t in Vis(P, S), the main diﬃculty for using the above strategies is that the
visibility graph is not plane. Its seems counter-intuitive that having more edges
renders the problem of ﬁnding a path more diﬃcult. Indeed, almost all local
routing algorithms in the literature that guarantee delivery do so by routing on
a plane subgraph that is computed locally. For example, in [6], a local routing
algorithm is presented for routing on a unit disk graph and the algorithm actually
routes on a planar subgraph known as the Gabriel graph. However, none of these
algorithms guarantee delivery in the presence of constraints. In this section, we
adapt the approach from [6] by showing how to locally identify the edges of a
planar spanning subgraph of Vis(P, S), which then allows us to use FACE-1 or
FACE-2 to route locally on Vis(P, S).
The graph in question is the constrained half-Θ6-graph, which was shown to
be a plane 2-spanner of Vis(P, S) [2]. Therefore, if we can show how to locally
identify the edges of this graph, we can apply FACE-1 or FACE-2. If we are at a
vertex v and we know all visibility edges incident to v, then identifying the edges
in v’s positive cones is easy: they connect v to the endpoints in this cone whose
projection on the bisector is closest. Thus, the hard part is deciding which of
the visibility edges in v’s negative cone are added by their endpoints. Next, we
show that v has enough information to ﬁnd these edges locally.
Lemma 3.1. Let u and v be vertices such that u ∈Cv
0. Then uv is an edge of
the constrained half-Θ6-graph if and only if v is the vertex whose projection on

68
P. Bose et al.
the bisector of Cu
0 is closest to u, among all vertices in Cu
0 visible to v and not
blocked from u by constraints incident on v.
Proof. First, suppose that v is not closest to u among the vertices in Cu
0 visible to
v and not blocked by constraints incident on v (see Fig. 4a). Then there are such
vertices whose projection on the bisector is closer to u. Among those vertices,
let x be the one that minimizes the angle between vx and vu. Now v cannot
be the endpoint of a constraint intersecting the interior of triangle uvx, since
the endpoint of that constraint would lie inside the triangle, contradicting our
choice of x. Since both uv and vx are visibility edges, Lemma 2.1 tells us that
there is a convex chain of visibility edges connecting u and x inside triangle uvx.
In particular, the ﬁrst vertex y from u on this chain is visible from both u and v
and is closer to u than v is (in fact, y = x by our choice of x). Moreover, v must
be in the same subcone of u as y, since the region between v and the chain is
completely empty of both vertices and constraints. Thus, uv cannot be an edge
of the half-Θ6-graph.
Second, suppose that v is closest to u among the vertices visible to v and not
blocked by constraints incident on v, but uv is not an edge of the half-Θ6-graph.
Then there is a vertex x ∈Cu
0 and in the same subcone as v, who is visible to
u, but not to v, and whose projection on the bisector is closer to u (see Fig. 4b).
Since x and v are in the same subcone, u is not incident to any constraints that
intersect the interior of triangle uvx, so we again apply Lemma 2.1 to the triangle
formed by visibility edges uv and ux. This gives us that there is a convex chain
of visibility edges connecting v and x, inside triangle uvx. In particular, the ﬁrst
point y from v on this chain must be visible to both u and v. And since y lies in
triangle uvx, it lies in Cu
0 and its projection is closer to u. But this contradicts
our assumption that v was the closest vertex. Thus, if v is the closest vertex, uv
must be an edge of the half-Θ6-graph.
Fig. 4. (a) If v is not closest to u among the vertices visible to v, then uv is not in the
half-Θ6-graph. (b) If v is closest to u among the vertices visible to v, then uv must be
in the half-Θ6-graph.
With Lemma 3.1, we can compute 1-locally which of the edges of Vis(P, S)
incident on v are also edges of the half-Θ6-graph. Therefore, we can apply FACE-
1 or FACE-2 in order to route on Vis(P, S) by routing on the half-Θ6-graph.

Constrained Routing Between Non-Visible Vertices
69
Corollary 3.2. We can 1-locally route on Vis(P, S) by routing on the con-
strained half-Θ6-graph.
Although it was shown in [3] that no deterministic local routing algorithm is
o(√n)-competitive on all pairs of vertices of the constrained Θ6-graph, regardless
of the amount of memory it is allowed to use, the caveat to the above local
routing algorithm is that the competitive ratio is not bounded by any function
of n. In fact, by applying FACE-1, it is possible to visit almost every edge of
the graph four times before reaching the destination. It is worse with FACE-2,
where almost every edge may be visited a linear number of times before reaching
the destination. In the next section, we present a 1-local routing algorithm that
is O(n)-competitive and provide a matching worst-case lower bound.
4
Routing on Constrained Triangulations
Next, we look at routing on a given constrained triangulation: a graph in which
all constraints are edges and all faces are triangles. Hence, we do not have to
check that the graph is a triangulation and we can focus on the routing process.
4.1
Lower Bound
Given a triangulation G and a source vertex s and a destination vertex t, let H be
the subgraph of G that contains all edges of G that are part of a triangle that is
intersected by st. We ﬁrst show that if G is a constrained Delaunay triangulation
or a constrained half-Θ6-graph, the shortest path in H can be a factor of n/4
times longer than that in G. This implies that any local routing algorithm that
considers only the triangles intersected by st cannot be o(n)-competitive with
respect to the shortest path in G on every constrained Delaunay triangulation
or constrained half-Θ6-graph on every pair of points. In the remainder of this
paper, we use πG(u, v) to denote the shortest path from u to v in a graph G.
Lemma 4.1. There exists a constrained Delaunay triangulation G with vertices
s and t such that |πH(s, t)| ≥n
4 · |πG(s, t)|.
Proof. We construct a constrained Delaunay graph with this property. For ease
of description and calculation, we assume that the size of the point set is a
multiple of 4. Note that we can remove this restriction by adding 1, 2, or 3
vertices “far enough away” from the construction so it does not inﬂuence the
shortest path.
We start with two columns of n/2 −1 points each, aligned on a grid. We
add a constraint between every horizontal pair of points. Next, we shift every
other row by slightly less than half a unit to the right (let ε > 0 be the small
amount that we did not shift). We also add a vertex s below the lowest row and
a vertex t above the highest row, centred between the two vertices on said row.
Note that this placement implies that st intersects every constraint. Finally, we
stretch the point set by an arbitrary factor 2x in the horizontal direction, for

70
P. Bose et al.
Fig. 5. Lower bound construction: the constraints are shown in thick red, the shortest
path in G is shown in blue (dotted), and the shortest path in H is shown in orange (dash
dotted). The remaining edges of G are shown in black (solid). (Color ﬁgure online)
some arbitrarily large constant x. When we construct the constrained Delaunay
triangulation on this point set, we get the graph G shown in Fig. 5.
In order to construct the graph H, we note that all edges that are part of H
lie on a face that has a constraint as an edge. In particular, H does not contain
any of the vertical edges on the left and right boundary of G. Hence, all that
remains is to compare the length of the shortest path in H to that in G.
Ignoring the terms that depend on ε, the shortest path in H uses n/2 edges
of length x, hence it has length x · n/2. Graph G on the other hand contains a
path of length 2x + n/2 −1 (again, ignoring small terms that depend on ε), by
following the path to the leftmost column and following the vertical path up.
Hence, the ratio |πH(s, t)|/|πG(s, t)| approaches n/4, since limx→∞
x· n
2
2x+ n
2 −1 = n
4 .
Note that the above construction is also the constrained half-Θ6-graph of the
given points and constraints.
Corollary 4.2. There exist triangulations G such that no local routing algo-
rithm that considers only the triangles intersected by st is o(n)-competitive when
routing from s to t.
4.2
Upper Bound
Next, we provide a simple local routing algorithm that is O(n)-competitive. To
make it easier to bound the length of the routing path, we use an auxiliary graph
H′ deﬁned as follows: let H′ be the graph H, augmented with the edges of the
convex hull of H and all visibility edges between vertices on the same internal
face (after the addition of the convex hull edges). For these visibility edges, we
only consider constraints with both endpoints in H. The diﬀerent graphs G, H,
and H′ are shown in Fig. 6. Note that the gray region in Fig. 6c is one of the
regions where visibility edges are added and note that edge uv is not added,
since visibility is blocked by a constraint that has both endpoints in H. We ﬁrst
show that the length of the shortest path in H′ is not longer than that in G.

Constrained Routing Between Non-Visible Vertices
71
Fig. 6. The three diﬀerent graphs: (a) The original triangulation G, (b) the subgraph
H, (c) graph H′ constructed by adding edges to H.
Lemma 4.3. For any triangulation G, we have |πH′(s, t)| ≤|πG(s, t)|.
Proof. If every vertex along πG(s, t) is part of H′, we claim that every edge of
πG(s, t) is also part of H′. Consider an edge uv of πG(s, t). If uv is part of a
triangle intersected by st, it is already part of H and therefore of H′. If uv is not
part of a triangle intersected by st, then u and v must lie on the same face of
H′ before we add the visibility edges, since otherwise the edge uv would violate
the planarity of G. Furthermore, since uv is an edge of G, u and v can see each
other. Hence, the edge uv is added to H′ when the visibility edges are added.
Therefore, every edge of πG(s, t) is part of H′ and thus |πH′(s, t)| ≤|πG(s, t)|.
If not every vertex along πG(s, t) is part of H′, we create subsequences of the
edges of πG(s, t) such that each subpath satisﬁes either (i) all vertices are in H′,
or (ii) only the ﬁrst and last vertex of the subpath are in H′. Using an argument
analogous to the previous case, it can be shown that subpaths of πG(s, t) that
satisfy (i) only use edges that are in H′.
To complete the proof, it remains to show that given a subpath π′ that
satisﬁes (ii), there exists a diﬀerent path in H′ that connects the two endpoints
of H′ and has length at most |π′|. Let u and v be the ﬁrst and last vertex of this
π′ (see Fig. 7). If u and v lie on the same face of H′ before the visibility edges are
added, H′ contains the geodesic path πH′ between u and v with respect to the
constraints using only vertices in H′. Note that this path can cross constraints
that have at most 1 endpoint in H′. Path π′ uses only edges of G which by
deﬁnition do not cross any constraints. Hence, π′ cannot be shorter than πH′.
Finally, we consider the case in which u and v do not lie on the same face
before the visibility edges are added. We construct πH′ by following the geodesic
paths in the faces of u and v, and using the convex hull edges for the remainder
of the path. Since u and v do not lie on the same face, a convex hull vertex x
lies on this path. Next, we look at π′. Recall that no vertex along π′ (other than
u and v) is in H′. This implies that π′ cannot cross st and it must cross any
ray originating from x that does not intersect the convex hull. Hence, π′ goes
around πH′ and therefore H′ contains a path from u to v of length at most |π′|.

72
P. Bose et al.
Fig. 7. A subpath of πG(s, t) (dotted
blue) that satisﬁes condition (ii): no
vertex other than its endpoints are in
H′. (Color ﬁgure online)
Fig. 8. The path π′ (dotted blue)
simulates uv on the shortest path
in H′ (dot dashed orange) and lies
on the boundary of a pocket of H.
(Color ﬁgure online)
Lemma 4.4. For any triangulation G, we have |πH(s, t)| ≤(n −1) · |πH′(s, t)|.
Proof. To prove the lemma, we show that for every edge uv on the shortest
path in H′, there is a path in H from u to v whose length is at most |πH′(s, t)|.
Consider an edge uv on the shortest path in H′. If uv is also an edge of H we use it
in our path. Since uv is part of the shortest path in H′ we have |uv| ≤|πH′(s, t)|.
It remains to consider the case in which uv is not part of H. Note that
this implies that uv is either an edge of the convex hull of H or a visibility
edge between two vertices of the same internal face. Instead of following uv, we
simulate uv by following the path π′ along the pocket of H from u to v (the part
of the boundary that does not visit both sides of st; see Fig. 8).
The path πH′(s, t) must cross the segment st several times (at least once at
s and once at t). Let x be the last intersection before u in πH′(s, t). Similarly,
let y be the ﬁrst intersection after v. Since π′ lies on the boundary of a pocket,
it cannot cross st and therefore it is contained in the polygon deﬁned by the
segment xy, and the portion of πH′(s, t) that lies between x and y. All edges
of π′ lie inside this polygon. In particular, each such edge has length at most
the length of πH′(s, t) from x to y, which is at most |πH′(s, t)| (since a segment
inside a polygon has length at most half the perimeter of the polygon).
Concatenate all our simulating paths and shortcut the resulting path from s
to t such that every vertex is visited at most once. The result is a path which
consists of at most n −1 edges, each of length at most |πH′(s, t)|.
⊓⊔
Theorem 4.5. For any triangulation G, we have |πH(s, t)| ≤(n−1)·|πG(s, t)|.
In order to route on the graph H, we apply the Find-Short-Path routing
algorithm by Bose and Morin [5]. This routing algorithm is designed precisely
to route on the graph created by the union of the triangles intersected by the
line segment between the source and destination. The algorithm reaches t after
having travelled at most 9 times the length of the shortest path from s to t in

Constrained Routing Between Non-Visible Vertices
73
this union of triangles. Hence, applying Find-Short-Path to graph H yields a
routing path of length at most 9(n −1) · |πG(s, t)|.
Theorem 4.6. For any triangulation, there exists a 1-local O(n)-competitive
routing algorithm that visits only triangles intersected by the line segment between
the source to destination.
5
Conclusions
We presented the ﬁrst local routing algorithm to route on the visibility graph.
We then showed a local O(n)-competitive routing algorithm for any triangula-
tion and showed that this is optimal when restricted to routing on the set of
triangles intersected by the segment from the source to the destination. The
competitiveness of our routing algorithm on Vis(S, P) is not bounded by any
function of n. On the other hand, our local O(n)-competitive routing algorithms
require a triangulated subgraph of Vis(S, P). Unfortunately, it is not known how
to compute such a triangulation locally, which naturally leads to the following
open problem: Can one locally compute a triangulation of Vis(S, P)? It is known
that the constrained Delaunay triangulation cannot be computed locally and the
constrained half-Θ6-graph is not necessarily a triangulation.
Acknowledgements. We thank Luis Barba, Sangsub Kim, and Maria Saumell for
fruitful discussions.
References
1. Bonichon, N., Gavoille, C., Hanusse, N., Ilcinkas, D.: Connections between theta-
graphs, delaunay triangulations, and orthogonal surfaces. In: Thilikos, D.M. (ed.)
WG 2010. LNCS, vol. 6410, pp. 266–278. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-16926-7 25
2. Bose, P., Fagerberg, R., Renssen, A., Verdonschot, S.: On plane constrained
bounded-degree spanners. In: Fern´andez-Baca, D. (ed.) LATIN 2012. LNCS, vol.
7256, pp. 85–96. Springer, Heidelberg (2012). doi:10.1007/978-3-642-29344-3 8
3. Bose, P., Fagerberg, R., van Renssen, A., Verdonschot, S.: Competitive local rout-
ing with constraints. In: Elbassioni, K., Makino, K. (eds.) ISAAC 2015. LNCS, vol.
9472, pp. 23–34. Springer, Heidelberg (2015). doi:10.1007/978-3-662-48971-0 3
4. Bose, P., Keil, J.M.: On the stretch factor of the constrained Delaunay triangula-
tion. In: ISVD, pp. 25–31 (2006)
5. Bose, P., Morin, P.: Competitive online routing in geometric graphs. Theor. Com-
put. Sci. 324(2), 273–288 (2004)
6. Bose, P., Morin, P., Stojmenovic, I., Urrutia, J.: Routing with guaranteed delivery
in ad hoc wireless networks. Wirel. Netw. 7(6), 609–616 (2001)
7. Bose, P., Renssen, A.: Upper bounds on the spanning ratio of constrained theta-
graphs. In: Pardo, A., Viola, A. (eds.) LATIN 2014. LNCS, vol. 8392, pp. 108–119.
Springer, Heidelberg (2014). doi:10.1007/978-3-642-54423-1 10
8. Clarkson, K.: Approximation algorithms for shortest path motion planning. In:
STOC, pp. 56–65 (1987)

74
P. Bose et al.
9. Das, G.: The visibility graph contains a bounded-degree spanner. In: CCCG, pp.
70–75 (1997)
10. Kranakis, E., Singh, H., Urrutia, J.: Compass routing on geometric networks. In:
CCCG, pp. 51–54 (1999)
11. Misra, S., Misra, S.C., Woungang, I.: Guide to Wireless Sensor Networks. Springer,
Heidelberg (2009)
12. R¨acke, H.: Survey on oblivious routing strategies. In: Ambos-Spies, K., L¨owe, B.,
Merkle, W. (eds.) CiE 2009. LNCS, vol. 5635, pp. 419–429. Springer, Heidelberg
(2009). doi:10.1007/978-3-642-03073-4 43

Deletion Graph Problems
Based on Deadlock Resolution
Alan Diˆego Aur´elio Carneiro, F´abio Protti, and U´everton S. Souza(B)
Fluminense Federal University, Niter´oi, RJ, Brazil
{aurelio,fabio,ueverton}@ic.uff.br
Abstract. A deadlock occurs in a distributed computation if a group
of processes wait indeﬁnitely for resources from each other. In this paper
we study actions to be taken after deadlock detection, especially the
action of searching a small deadlock-resolution set. More precisely, given
a “snapshot” graph G representing a deadlocked state of a distributed
computation governed by a certain deadlock model M, we investigate
the complexity of vertex/arc deletion problems that aim at ﬁnding min-
imum vertex/arc subsets whose removal turns G into a deadlock-free
graph (according to model M). Our contributions include polynomial
algorithms and hardness proofs, for general graphs and for special graph
classes. Among other results, we show that the arc deletion problem in
the OR model can be solved in polynomial time, and the vertex deletion
problem in the OR model remains NP-Complete even for graphs with
maximum degree four, but it is solvable in O(m√n) time for graphs with
Δ ≤3.
Keywords: Deadlock
resolution ·
Knot ·
Wait-for
graphs ·
Computational complexity
1
Introduction
A set of processes is in deadlock if each process of the set is blocked, waiting for a
response from another process of the same set. In other words, the processes can-
not proceed with their execution because of necessary events that only processes
in the same set can provide. Deadlock is a stable property, in the sense that
once it occurs in a global state Ψ of a distributed computation, it still holds for
all the states subsequent to Ψ. Deadlock avoidance and deadlock resolution are
fundamental problems in the study of distributed systems [1].
Distributed computations are usually represented by the so-called wait-for
graphs, where the behavior of processes is determined by a set of prescribed rules
(the deadlock model or dependency model). In a wait-for graph G = (V, E), the
vertex set V represents processes in a distributed computation, and the set E
of directed arcs represents wait conditions [4]. An arc exists in E directed away
from vi ∈V towards vj ∈V if vi is blocked, waiting a signal from vj. The graph
G changes dynamically according to the deadlock model, as the computation
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 75–86, 2017.
DOI: 10.1007/978-3-319-62389-4 7

76
A.D.A. Carneiro et al.
progresses. In essence, the deadlock model speciﬁes rules for vertices that are
not sinks in G to become sinks [3]. (A sink is a vertex with out-degree zero.)
The main deadlock models investigated so far in the literature are presented
below.
AND model – In the AND model, a process vi can only become a sink when
it receives a signal from all the processes in Oi, where Oi stands for the set
of outneighbors of vi.
OR model – In this model, it suﬃces for a process vi to become a sink to
receive a signal from at least one of the processes in Oi.
X-Out-Of-Y model – There are two integers, xi and yi, associated with
a process vi. Also, yi = |Oi|, meaning that process vi is in principle waiting
for a signal from every process in Oi. However, in order to be relieved from
its wait state, it suﬃces for vi to receive a signal from any xi of those yi
processes.
AND-OR model – There are ti ≥1 subsets of Oi associated with process
vi. These subsets are denoted by O1
i , . . . , Oti
i and must be such that Oi = O1
i
∪· · ·∪Oti
i . It suﬃces for a process vi to become a sink to receive a signal from
all processes in at least one of O1
i , . . . , Oti
i . For this reason, these ti subsets
of Oi are assumed to be such that no one is contained in another.
The study of deadlocks is fundamental in computer science and can be
divided in four ﬁelds: Deadlock Prevention, Deadlock Avoidance, Deadlock
Detection, and Deadlock Resolution (or Deadlock Recovery). Although preven-
tion, avoidance, and detection of deadlocks have been widely studied in the lit-
erature, only few studies have been dedicated to deadlock recovery [7,10,17,19],
most of them considering only the AND model. One of the reasons for this is
that prevention and avoidance of deadlocks provide rules that are designed to
ensure that a deadlock will never occur in a distributed computation. As pointed
out in [19], deadlock prevention and avoidance strategies are conservative solu-
tions, whereas deadlock detection is optimistic. Whenever the prevention and
avoidance techniques are not applied, and deadlocks are detected, they must be
broken through some intervention such as aborting one or more processes to
break the circular wait condition causing the deadlock, or preempting resources
from one or more processes which are in deadlock. In this paper we consider
such a scenario where deadlock was detected in a system and some minimum
cost deadlock-breaking set must be found and removed from the system.
Although distributed computations are dynamic, deadlock is a stable prop-
erty; thus, whenever we refer to G, we mean the wait-for graph that corresponds
to a snapshot of the distributed computation in the usual sense of a consistent
global state [2,8].
The contribution of this work is to provide an analysis of the computational
complexity of optimization problems (deletion graph problems) related to dead-
lock resolution. Given a deadlocked distributed computation represented by G
graph, that operates according to a deadlock model M ∈{AND, OR, X-Out-
Of-Y, AND-OR}, we investigate vertex deletion and arc deletion problems
whose goal is to obtain the minimum number of removals in order to turn G free

Deletion Graph Problems Based on Deadlock Resolution
77
of graph structures that characterize deadlocks. The complexity of such prob-
lems depends on the deadlock model that governs the computation as well as the
type of structure to be removed. To the best of our knowledge, such a compu-
tational complexity mapping considering the particular combination of deletion
operations and deadlock models is novel. We remark that deadlock detection
can be done in polynomial time for any model M ∈{AND, OR, X-Out-Of-Y,
AND-OR} [16].
The results of this paper are NP-completeness results and polynomial algo-
rithms for general and speciﬁc graph classes. In particular, we focus on the OR
Model. We show that the arc deletion problem on the OR Model can be solved
in polynomial time and the vertex deletion problem on the OR Model remains
NP-Complete even for graphs with Δ(G) = 4, but is solvable in polynomial time
for graphs with Δ(G) ≤3.
Due to space constraints, some proofs are omitted.
Additional concepts and notations. Let G = (V, E) be a directed graph. For
vi ∈V , let Di denote the set of descendants of vi in G (nodes that are reachable
from vi, including itself). Let Oi ⊆Di be the set of immediate descendants
of vi ∈G (descendants that are one arc away from vi). The out-degree (resp.,
in-degree) of a vertex v is denoted by deg+(v) (resp., deg−(v)). In addition,
deg+(G) (resp., deg−(G)) denotes the minimum out-degree (resp., in-degree) of
a vertex in G. Additional graph concepts and notation can be found in [6].
1.1
Deletion Problems
We denote by λ–Deletion(M) a generic optimization problem for deadlock
resolution, where λ indicates the type of deletion operation to be used in order
to break all the deadlocks, and M ∈{AND, OR, X-Out-Of-Y, AND-OR} is
the deadlock model of the input wait-for graph G.
The types of deletion operation considered in this work are given below:
1. Arc: The intervention is given by arc removal. For a given graph G, Arc–
Deletion(M) consists of ﬁnding the minimum number of arcs to be removed
from G in order to make it deadlock-free. The removal of an arc can be viewed
as the preemption of a resource.
2. Vertex: The intervention is given by vertex removal. For a given graph G,
Vertex–Deletion(M) consists of ﬁnding the minimum number of vertices
to be removed from G in order to make it deadlock-free. The removal of a
vertex can be viewed as the abortion of one process.
3. Output: The intervention is given by removing all the out-arcs of a vertex.
For a given graph G, Output–Deletion(M) consists of ﬁnding the minimum
number of vertices to be transformed into sinks in order to make G deadlock-
free. The removal of all the out-arcs of a vertex can be viewed as the immediate
transformation of a blocked process into an executable process by preempting
all its required resources.
The twelve possible combinatorial problems of the form λ–Deletion(M) are
shown in Table 1.

78
A.D.A. Carneiro et al.
Table 1. Computational complexity questions for λ–Deletion(M).
λ–Deletion(M)
λ\M
AND OR AND-OR X-Out-Of-Y
Arc
?
?
?
?
Vertex
?
?
?
?
Output ?
?
?
?
2
Computational Complexities
In this section we present complexity results for all the problems listed in Table 1,
except Vertex–Deletion(OR), which receives a special analysis in the next
section.
2.1
And Model and Generalizations
To determine if there is a deadlock in a graph G in the AND model, it is nec-
essary and suﬃcient to check the existence of cycles. Therefore, it is easy to
see that Vertex–Deletion(AND) coincides with Directed Feedback Vertex
Set and Arc–Deletion(AND) coincides with Directed Feedback Arc Set, well-
known problems proved to be NP-Hard in [14]. In addition, the following lemma
holds:
Lemma 1. Output–Deletion(AND) is NP-Hard.
Proof. Let G be a directed graph and S be a subset of V (G). The transformation
into sinks of all vertices in S can make G cycle-free if and only if G[V \S] is
acyclic.
⊓⊔
The AND-OR model is a generalization of the AND and OR models; there-
fore, every instance of a deadlock resolution problem for either the AND model
or the OR model is also an instance for the AND-OR model. Also, the X-Out-
Of-Y model also generalizes the AND and OR models. From this observation, it
follows that:
Corollary 2. For M ∈{AND-OR, X-Out-Of-Y}, it holds that:
– Vertex–Deletion(M) is NP-hard;
– Arc–Deletion(M) is NP-hard;
– Output–Deletion(M) is NP-hard.
2.2
Or Model
To determine if there is a deadlock in a wait-for graph G on the OR model, it
is suﬃcient and necessary to check the existence of a structure called knot [3].
A knot K is a strongly connected component (SCC) of order at least two where

Deletion Graph Problems Based on Deadlock Resolution
79
no vertex of K has an out-arc pointing to a vertex that is not in K. All the knots
of a digraph can be identiﬁed in linear time as follows: ﬁrst, ﬁnd all the SCCs
in linear time by running a depth-ﬁrst search [9]; next, contract each SCC into
a single vertex, obtaining an acyclic digraph H whose sinks represent the knots
of G.
Lemma 3
(a) Let K be a knot in a wait-for graph G on the OR model. The minimum
number of arcs to be removed in K to make it knot-free is δ+(K).
(b) Let G be a wait-for graph on the OR model, and K = {K1, K2, ..., Kp} the
non-empty set of all the existing knots in G. The minimum number of arcs
to be removed from G to make it knot-free is p
i=1 δ+(Ki).
By Lemma 3 we can obtain in linear time a minimum set of arcs whose
removal turn a given digraph G knot-free.
Corollary 4. Arc–Deletion(OR) can be solved in linear time.
In the preceding subsection we observed that Output–Deletion(AND)
is closely related to Vertex–Deletion(AND). The next result shows that
Output–Deletion(OR) is closely related to Arc–Deletion(OR).
Lemma 5. The minimum number of vertices to be transformed into sinks in
order to make a graph G knot-free is equal to the number of knots of G.
Corollary 6. Output–Deletion(OR) can be solved in linear time.
Table 2 presents the computational complexities of the problems presented
so far. The complexity analysis of Vertex–Deletion(OR) is presented in next
section.
Table 2. Partial scenario of λ–Deletion(M) complexity.
λ–Deletion(M)
λ \ M
AND
OR AND-OR X-Out-Of-Y
Arc
NP-H P
NP-H
NP-H
Vertex
NP-H ?
NP-H
NP-H
Output NP-H P
NP-H
NP-H
3
Vertex–Deletion(OR)
In this section we show that Vertex–Deletion(OR) is NP-hard. In addition,
we analyze the problem for some particular graph classes and present an inter-
esting case solvable in polynomial time.

80
A.D.A. Carneiro et al.
Lemma 7. Vertex–Deletion(OR) is NP-hard.
Proof. Let F be an instance of 3-SAT [12] with n variables and having at most
3 literals per clause. From F we build a graph GF = (V, E) which will contain
a set S ⊆V (G) such that |S| = n and GF [V \S] is knot-free if and only if F is
satisﬁable. The construction of GF is described below:
1. For each variable xi in F, create a directed cycle with two vertices (“variable
cycle”), Txi and Fxi, in GF .
2. For each clause Cj in F create a directed cycle with three vertices (“clause
cycle”), where each literal of Cj has a corresponding vertex in the cycle.
3. for each vertex v that corresponds to a literal of a clause Cj, create an arc
from v to Txi if v represents the positive literal xi, and create an arc from v
to Fxi if v represents the negative literal ¯xi.
Figure 1 shows the graph GF built from an instance F of 3-SAT.
Suppose that F admits a truth assignment A. We can determine a set of
vertices S with cardinality n such that GF [V \S] is knot-free as follows. For
each variable of F, select a vertex of GF according to the assignment A such
that the selected vertex represents the opposite value in A, i.e., if the variable
xi is true in A, Fxi is included in S, otherwise Txi is included in S. Since each
knot corresponds to a variable cycle, it is easy to see that GF [V \S] has exactly
n sinks. Therefore, since A satisﬁes F, at least one vertex corresponding to a
literal in each clause cycle will have an arc towards a sink (vertex that matches
the assignment). Thus GF [V \S] will be knot-free.
Conversely, suppose that GF contains a set S with cardinality n such that
GF [V \S] is knot-free. By construction GF contains n knots, each one associated
with a variable of F. Hence, S has exactly one vertex per knot (one of Txi,
Fxi). As each cycle of GF [V \S] corresponds to a clause of F, and GF [V \S]
is knot-free, each cycle of GF [V \S] has at least one out-arc pointing to a sink.
Thus, we can deﬁne a truth assignment A for F by setting xi = true if and
only if Txi ∈{V \S}. Since at least one vertex corresponding to a literal in each
clause cycle will have an arc towards a sink, we conclude that F is satisﬁable. ⊓⊔
In general, a wait-for graph on the OR model can be viewed as a conglomer-
ate of several strongly connected components. As observed in Subsect. 2.2, the
problems that can be solved in polynomial time have a characteristic in common:
it suﬃces to singly solve every knot in G because no other SCC will become a
knot after these removals.
Corollary 8. Vertex–Deletion(OR) remains NP-Hard even if G is strongly
connected, i.e., G is a single knot.
Now we consider properties of the underlying undirected graph of G.
Since one of the most used architectures in distributed computation follows
the user/server paradigm, a intuitively interesting graph class for distributed
computation purposes are bipartite graphs. Planar graphs can also be interesting
if physical settings must be considered; ﬁnally, bounded-degree graphs are very
common in practice.

Deletion Graph Problems Based on Deadlock Resolution
81
Fig. 1. Graph GF built from F = (x1 ∨¯x2 ∨x3) ∧(x1 ∨x2 ∨¯x3) ∧( ¯x1).
Theorem 9. Vertex–Deletion(OR)
remains
NP-Hard
even
when
the
underlying undirected graph of G is bipartite, planar, and with maximum
degree 4.
Proof. Let F be an instance of Planar 3-SAT where each variable has at most
three occurrences (at least one positive and at least one negative). This problem
is known to be NP-complete [18]. Clearly, GF as built in Lemma 7 will have
maximum degree 4. The bipartition of GF can be obtained by replacing each
clause cycle (with size three) by another directed cycle of size six, such that
no pair of vertices representing positive (resp. negative) literals are adjacent;
moreover, in such cycles, if a vertex represents a positive literal and another
vertex a negative literal then they are at an odd distance. Finally, the planarity
property can be obtained by using the planar embedding of the variable-clause
bipartite graph associated to F to coordinate the ideal position of the vertices
inside the clause cycles as well as the proper position of variable cycles and clause
cycles in order to obtain a planar embedding of GF .
⊓⊔
3.1
Subcubic Graphs
Since Vertex–Deletion(OR) remains NP-hard for graphs with Δ(G) = 4,
and is trivial for graphs with Δ(G) ≤2, an interesting question is to study the
complexity of Vertex–Deletion(OR) when the underlying undirected graph
of G has maximum degree three, i.e., is subcubic.
To answer this question the ﬁrst step is to phase out unnecessary vertices,
i.e., vertices that will never belong to any solution, such as sources and sinks.
Preprocessing. Successively remove all source and sink vertices from G, until
removals can no longer be applied. Using depth-ﬁrst search the preprocessing
can be done in O(n + m) time.

82
A.D.A. Carneiro et al.
After the preprocessing, all vertices of G are in deadlock, and they are clas-
siﬁed in three types: A - with one in-arc and one out-arc; B - with one in-arc
and two out-arcs; C - with two in-arcs and one out-arc.
The next step is to continuously analyse graph aspects in order to establish
rules and procedures that may deﬁne speciﬁc vertices as part of an optimum
solution. Thus, we iteratively build a partial solution contained in some optimum
solution.
To break all the deadlocks in a graph G, it is necessary to destroy each knot
in G. The removal of some vertices can destroy a knot; however, these removals
may produce new knots that also need to be broken. Thus, our goal now is
to identify for each knot a vertex that without loss of generality is part of an
optimum solution.
Let W be a SCC of G. We can classify the vertices of type A in G[W] into
three sub-types. A vertex is sub-type A.1 if it is of type A in G[W], but type C
in the original graph, i.e., has an in-arc from another SCC; a vertex is subtype
A.2 if it is the same type in both G[W] and G; ﬁnally, a vertex is subtype A.3
when it is type A in G[C] but type B in G. Note that in a knot there will never
be vertices of type A.3. It is worth noting that in a subcubic graph every knot
vertex has at most one external neighbor (in-neighbor).
The following lemma presents an interesting relation between vertices of type
B and C.
Lemma 10. Let Q be a strongly connected subcubic graph. The number of ver-
tices type B in Q is equal to the number of vertices type C.
At this point, we can identify some vertices of an optimal solution.
Theorem 11. Let G be a subcubic graph and Q be a knot in G. If Q contains
a vertex type B or C or A.2 then G has an optimal solution S for Vertex–
Deletion(OR) which contains exactly one vertex vi ∈V (Q), and such a vertex
can be found in linear time.
Proof
(a) Since Q is strongly connected, any sink arising from a vertex removal will
break Q. Since a vertex type B has no neighbor outside Q (otherwise Q
would not be a knot) removing it will never create a knot in G\Q. Thus,
given a vertex vi type B in Q, the removal of vi will not turn its in-neighbor
wi into a sink only if wi is also type B. In this case we repeat the same
process for wi. From Lemma 10, eventually we will ﬁnd a vertex vj type B
whose in-neighbor wj is not type B, otherwise, Q would be composed only
by vertices type B.
(b) The proof for the case where Q contains a vertex type C follows direct from
Lemma 10 and (a).
(c) Suppose that vi is a vertex type A.2 in Q. Since such a vertex has no
neighbours outside Q, removing it will never create a knot in G \ Q. In
addition, the removal of vi will not break Q only if its in-neighbor wi is type
B. In this case we can apply (a).
⊓⊔

Deletion Graph Problems Based on Deadlock Resolution
83
Corollary 12. Let G be a subcubic graph. If G has no knot Q having a vertex
vi such that (i) Q−vi is knot-free, (ii) all the knots of G−vi are knots of G, and
(iii) vi has no in-neighbor outside Q, then any knot of G is a cycle composed
only by vertices type A.1.
Proof. By Theorem 11, a knot Q of G contains no vertex of type B, C, or A.2.
Clearly, it cannot have any vertices type A.3. Thus, Q is a cycle of vertices type
A.1.
⊓⊔
Now, we can determine lower and upper bounds for the problem.
Lemma 13. Let S be an optimal solution of a subcubic instance G of Vertex–
Deletion(OR). Then it holds that k ≤|S| ≤2k, where k is the number of
knots of G.
Corollary 14. Vertex–Deletion(OR) on subcubic graphs can be 2-approxi-
mated in linear time.
Proof. Follows from Theorem 11 and Lemma 13.
⊓⊔
A Polynomial Time Algorithm. In order to obtain an optimum solution in
polynomial time, there are some signiﬁcant considerations to be made regarding
the remaining graph.
Lemma 15. Let G be a subcubic instance of Vertex–Deletion(OR). Let C =
{C1, C2, . . . , Cj} be a set of non-knot SCCs of G, where for each Ci ∈C there
is a directed path from Ci to Ci+1 and from Cj to Q. Then:
(a) No vertex in C1, C2, . . . , Cj−1 is part of an optimal solution.
(b) If Cj has two or more out-arcs pointing to a knot Q or there is some i < j
for which Ci is directly connected to Q then there is an optimal solution S
such that V (Q) ∩S = {vi}, and such a vertex can be found in linear time.
At this point, we are able to apply the ﬁrst steps of our algorithm.
First steps of the algorithm. (i) Remove any SCC that is pointing to another
non-knot SCC; (ii) If a non-knot SCC has at least two arcs pointing to a same
knot then remove all such arcs but one; (iii) For each knot Q having vertices
of type B, C, or A.2, ﬁnd a vertex vi that, without loss of generality, is in an
optimal solution and remove it; (iv) Remove all vertices that are no longer in
deadlock.
The correctness of the above steps follows from Theorem 11 and Lemma 15.
Observe also that such routines can be performed in O(n + m) time.
Now, consider the graph G obtained after applying the above steps. The knots
of G are directed cycles composed by vertices of type A.1 (see Corollary 12), and
each non-knot SCC of G is at distance one of a knot. At this point, any vertex
v removed from a knot Q will break it, but potentially creates a new knot from
the SCC W that has an out-arc to v. However, such new knot W will have a

84
A.D.A. Carneiro et al.
vertex of type A.2; therefore, W can be solved by the removal of another vertex
w ∈V (W), called solver vertex.
Our ﬁnal step is to minimize the number of solver vertices that actually need
to be removed in order to make the graph knot-free. To achieve this purpose,
we will consider the bipartite graph B = (K ∪C, E), where K is the set of
vertices representing a contracted knot, and C is the set of vertices representing
contracted non-knot SCCs. Note that each arc from a vertex cj in C to another
vertex ki in K represents a connection from a vertex w of the SCC represented
by cj to a vertex v of the knot represented by ki, and indicates that w becomes
a vertex type A.2 after the removal of v, which guarantees the existence of a
solver vertex, that may or may not be used. Therefore, from B we seek a set M ′
of arcs such that:
1. Each vertex in K is adjacent to at most one arc of M ′. (This arc indicates
the vertex of the knot to be removed.)
2. Each vertex in C has at least one arc that does not belong to M ′. (This arc
indicates a path to a sink, a broken knot in K; thus, the SCC is deadlocked
without removing internal vertices.)
3. M ′ is maximum.
From the set M ′ we can obtain an optimal solution S such that G[V \S]
is knot-free (where G is prior to the contraction). In fact, M ′ indicates the
maximum number of knots that can be broken without generating new knots, as
well as the number of solver vertices needed. Solution S can be built as follows:
for each vertex ki in K, adjacent to M ′, include in S the associated knot vertex
in G; for every vertex ki ∈K such that ki is not adjacent to M ′, choose an arc
e and include in S the knot vertex of G associated with e; then, for the SCC C
of G indicated by the arc e, include in S a solver vertex.
The set M ′ can be obtained by using the concept of (f, g)-semi-matching. The
(f, g)-semi-matching is a generalization of semi-matching presented in [5]. Let
f : K →N and g : C →N be functions. An (f, g)-semi-matching in a bipartite
graph G = (K ∪C, E) is a set of arcs M ⊆E such that each vertex ki ∈K is
incident with at most f(ki) arcs of M, and each vertex cj ∈C is incident with
at most g(cj) arcs of M. In fact, M ′ can be found by an (1, g)-semi-matching
where g(v) = d(v) −1.
Lemma 16 [5,11,15]. Given a bipartite graph G = (K∪C, E) and two functions
f : K →N and g : C →N, ﬁnding a maximum (f, g)-semi-matching of G can
be done in polynomial time, and a maximum (1, g)-semi-matching of G can be
found in O(m√n) time.
These algorithms use similar ideas to those used in the well known Hopcroft-
Karp algorithm [13].
At this point, we have all elements to answer the raised question about the
complexity of Vertex-Deletion(OR).
Theorem 17. Vertex-Deletion(OR) restricted to subcubic graphs is solv-
able in O(m√n) time.

Deletion Graph Problems Based on Deadlock Resolution
85
4
Conclusions
We deﬁne λ–Deletion(M) as a generic optimization problem for deadlock res-
olution, where λ indicates the type of deletion operation to be used in order to
break all the deadlocks, and M ∈{AND, OR, X-Out-Of-Y, AND-OR} is
the deadlock model of the input wait-for graph G. Vertex–Deletion(AND)
and Arc–Deletion(AND) are equivalent to Direct Feedback Vertex Set and
Direct Feedback Arc Set, respectively. We proved that Arc–Deletion(OR)
and Output–Deletion(OR) are solvable in polynomial time. In addition,
Vertex–Deletion(OR) was shown to be NP-complete. Such results are sum-
marized in the following Table 3.
Table 3. Computational complexity of λ–Deletion(M).
λ–Deletion(M)
λ \ M
AND
OR
AND-OR X-Out-Of-Y
Arc
NP-H P
NP-H
NP-H
Vertex
NP-H NP-H NP-H
NP-H
Output NP-H P
NP-H
NP-H
A study of the complexity of Vertex–Deletion(OR) in diﬀerent graph
classes was also done. We proved that the problem remains NP-hard even for
strongly connected graphs and planar bipartite graphs with maximum degree
four. Furthermore, we proved that for graphs with maximum degree three the
problem can be solved in polynomial time. Thus we have the following Table 4:
Table 4. Complexity of Vertex–Deletion(OR) for some graph classes.
Vertex–Deletion(OR)
Instance
Complexity
Weakly connected
NP-Hard
Strongly connected
NP-Hard
Planar, bipartite, Δ(G) ≥4 and Δ(G)+ = 2 NP-Hard
Δ(G) = 3
Polynomial
Acknowledgments. This research was partially supported by the Brazilian National
Council for Scientiﬁc and Technological Development (CNPq), the Brazilian National
Council for the Improvement of Higher Education (CAPES) and FAPERJ.

86
A.D.A. Carneiro et al.
References
1. Atreya, R., Mittal, N., Kshemkalyani, A.D., Garg, V.K., Singhal, M.: Eﬃcient
detection of a locally stable predicate in a distributed system. J. Parallel Distrib.
Comput. 67(4), 369–385 (2007)
2. Barbosa, V.C.: An Introduction to Distributed Algorithms. The MIT Press, Cam-
bridge (1996)
3. Barbosa, V.C.: The combinatorics of resource sharing. In: Corrˆea, R., Dutra, I.,
Fiallos, M., Gomes, F. (eds.) Models for Parallel and Distributed Computation,
pp. 27–52. Kluwer, Dordrecht (2002)
4. Barbosa, V.C., Benevides, M.R.: A graph-theoretic characterization of AND-
OR deadlocks. Technical report COPPE-ES-472/98, Federal University of Rio de
Janeiro, Rio de Janeiro, Brazil (1998)
5. Bokal, D., Breˇsar, B., Jerebic, J.: A generalization of hungarian method and hall’s
theorem with applications in wireless sensor networks. Discret. Appl. Math. 160(4),
460–470 (2012)
6. Bondy, J.A., Murty, U.S.R.: Graph Theory with Applications, vol. 290. Macmilan,
London (1976)
7. Chahar, P., Dalal, S.: Deadlock resolution techniques: an overview. Int. J. Sci. Res.
Publ. 3(7), 1–5 (2013)
8. Chandy, K.M., Lamport, L.: Distributed snapshots: determining global states of
distributed systems. ACM Trans. Comput. Syst. 3, 63–75 (1985)
9. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms.
MIT press, Cambridge (2009)
10. de Mend´ıvil, J.G., Fari˜na, F., Garitagotia, J.R., Alastruey, C.F., Bernabeu-Auban,
J.M.: A distributed deadlock resolution algorithm for the and model. IEEE Trans.
Parallel Distrib. Syst. 10(5), 433–447 (1999)
11. Galˇc´ık, F., Katreniˇc, J., Semaniˇsin, G.: On computing an optimal semi-matching.
In: Kolman, P., Kratochv´ıl, J. (eds.) WG 2011. LNCS, vol. 6986, pp. 250–261.
Springer, Heidelberg (2011). doi:10.1007/978-3-642-25870-1 23
12. Gary, M.R., Johnson, D.S.: Computers and intractability: a guide to the theory of
NP-completeness (1979)
13. Hopcroft, J.E., Karp, R.M.: An n5/2 algorithm for maximum matchings in bipartite
graphs. SIAM J. Comput. 2(4), 225–231 (1973)
14. Karp, R.: Reducibility among combinatorial problems. In: Miller, R., Thatcher, J.,
Bohlinger, J. (eds.) Complexity of Computer Computations. The IBM Research
Symposia Series, pp. 85–103. Springer, New York (1972)
15. Katrenic, J., Semanisin, G.: A generalization of hopcroft-karp algorithm for semi-
matchings and covers in bipartite graphs. arXiv:1103.1091 (2011)
16. Kshemkalyani, A.D., Singhal, M.: Distributed Computing: Principles, Algorithms,
and Systems. Cambridge University Press, Cambridge (2011)
17. Leung, J.Y.-T., Lai, E.K.: On minimum cost recovery from system deadlock. IEEE
Trans. Comput. 9(C–28), 671–677 (1979)
18. Penso, L.D., Protti, F., Rautenbach, D., dos Santos Souza, U.: Complexity analysis
of P3-convexity problems on bounded-degree and planar graphs. Theoret. Comput.
Sci. 607, 83–95 (2015)
19. Terekhov, I., Camp, T.: Time eﬃcient deadlock resolution algorithms. Inf. Process.
Lett. 69(3), 149–154 (1999)

Space-Eﬃcient Algorithms for Maximum
Cardinality Search, Stack BFS, Queue BFS
and Applications
Sankardeep Chakraborty1(B) and Srinivasa Rao Satti2
1 The Institute of Mathematical Sciences,
CIT Campus, Taramani, Chennai 600113, India
sankardeep@imsc.res.in
2 Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, South Korea
ssrao@cse.snu.ac.kr
Abstract. Following the recent trends of designing space eﬃcient algo-
rithms for fundamental algorithmic graph problems, we present sev-
eral time-space tradeoﬀs for performing Maximum Cardinality Search
(MCS), Stack Breadth First Search (Stack BFS), and Queue Breadth
First Search (Queue BFS) on a given input graph. As applications of
these results, we also provide space-eﬃcient implementations for testing
if a given undirected graph is chordal, reporting an independent set, and
a proper coloring of a given chordal graph among others. Finally, we also
show how two other seemingly diﬀerent graph problems and their algo-
rithms have surprising connection with MCS with respect to designing
space eﬃcient algorithms.
1
Introduction
Space eﬃcient algorithms are becoming increasingly important owing to their
applications in the presence of rapid growth of “big data”. Another reason for the
importance of space eﬃcient algorithms is the proliferation of specialized hand-
held devices and embedded systems that have a limited supply of memory. As a
consequence, algorithms that are oblivious to space constraint are not desired in
such scenario. Even if mobile devices and embedded systems are designed with
large supply of memory, it might be useful to restrict the number of write oper-
ations. For example, on ﬂash memory, writing is a costly operation in terms of
speed, and it also reduces the reliability and longevity of the memory. Keeping
all these constraints in mind, it makes sense to consider algorithms that do not
modify the input and use only a limited amount of work space. Such computa-
tional model has been proposed in algorithmic literature to study space eﬃcient
algorithms, and is known as the read-only memory (ROM) model. Following the
recent trend, in this article, we focus on the space requirement for implementing
some fundamental graph algorithms in such settings.
There is already a rich history of designing space eﬃcient algorithms in the
read-only memory model. In computational complexity theory, L is the complex-
ity class [1] containing decision problems that can be solved by a deterministic
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 87–98, 2017.
DOI: 10.1007/978-3-319-62389-4 8

88
S. Chakraborty and S.R. Satti
Turing machine using only logarithmic amount of work space for computation.
There are several important algorithmic results for this class, the most celebrated
being Reingold’s method [20] for checking reachability between two vertices in
an undirected graph. Barnes et al. [6] gave a slightly sublinear space (using
n/2Θ(√lg n) bits1) algorithm for checking s-t connectivity in a directed graph
with polynomial running time. Other than these fundamental graph theoretical
problems, researchers have also focused on designing space-eﬃcient algorithms
for the more classical selection and sorting problems [18], and problems in com-
putational geometry [2,11]. Most of these graph algorithms using small space
i.e., sublinear bits, often take time that is some polynomial of very high degree.
For example, to the best of our knowledge, the exact running time of Reingold’s
algorithm [20] for undirected s-t connectivity is not analysed, yet we know it
admits a large polynomial running time. In fact this phenomenon is not unusual,
as Edmonds et al. [12] have shown in the so-called NNJAG model that only a
slightly sublinear working-space bound is possible for an algorithm that solves
the reachability problem when required to run in polynomial time. Tompa [24]
showed a surprising result that for directed s-t connectivity, if the number of
bits available is o(n) then some natural algorithmic approaches to the prob-
lem require superpolynomial time. Motivated by these impossibility results from
complexity theory and inspired by the practical applications of these fundamen-
tal graph algorithms, recently there has been a surge of interest in improving
the space complexity of the fundamental graph algorithms without paying too
much penalty in the running time i.e., reducing the working space of the clas-
sical graph algorithms to O(n) bits with little or no penalty in running time.
Generally these classical linear time graph algorithms take O(n lg n) bits.
Starting with the paper of Asano et al. [3] who showed how one can implement
DFS using O(n) bits, improving on the na¨ıve O(n lg n)-bit implementation, the
recent series of papers [4,5,9,10] present space-eﬃcient algorithms for few other
basic graph problems: namely BFS, topological sort, (strongly) connected com-
ponents, sparse spanning biconnected subgraph, biconnectivity, st-numbering,
dynamic programming in bounded treewidth graphs among others. We add to
this small yet growing body of space-eﬃcient algorithm design literature by pro-
viding such algorithms for a few more fundamental and classical graph problems,
namely performing maximum cardinality search, using this to recognize chordal
graphs and solve some combinatorial problems like coloring, vertex cover, etc.
in chordal graphs, and computing queue BFS and stack BFS among others.
Model of Computation. We use the same model of computation that is used
in the recent research that focused on space-eﬃcient graph algorithms [3–5,9,10].
More speciﬁcally, we assume that the input graph G with n vertices and m edges
is given in a read-only memory. The algorithm produces the output on a separate
write-only memory which cannot be read or rewritten again. In addition to the
input and the output media, a limited amount of random-access workspace is
available. The data on this workspace is manipulated at word level as in the
standard word RAM model with word size w = Θ(lg n). We count space in terms
1 We use lg to denote logarithm to the base 2.

Space-Eﬃcient Algorithms for Maximum Cardinality Search
89
of the number of bits in the workspace used by the algorithms. We assume that
the input undirected (directed) graph G = (V, E) is represented using (in and
out) adjacency array, i.e., given a vertex v and an integer k, we can access the
k-th (in/out) neighbor of vertex v in constant time. This representation was used
in [4,9,10] recently to design various other space eﬃcient graph algorithms.
Organization of the paper. In Sect. 2 we introduce MCS and provide its
implementation as described in [23]. This is followed by various space eﬃcient
implementations of MCS and the proofs are presented in Sect. 3. Next we show,
in Sect. 4, some applications of our MCS algorithm by providing space eﬃcient
procedures to recognize chordal graphs, ﬁnd Independent set and proper coloring,
etc. in chordal graphs. Section 5 describes space eﬃcient algorithms for two other
popular graph search methods which are known as Stack BFS and Queue BFS
in the literature.
2
Maximum Cardinality Search (MCS)
A widely used graph search method which is a restriction of breadth-ﬁrst search
(BFS) is lexicographic BFS (Lex-BFS), introduced by Rose et al. [21] under the
name Lex-P. They used Lex-BFS to ﬁnd a perfect elimination ordering (PEO)
of the vertices of a graph G if G is chordal. A perfect elimination ordering in G
is an ordering of the vertices such that, for each vertex v, v and the neighbors of
v that occur after v in the order form a clique. Fulkerson et al. [13] showed that
a graph is chordal if and only if it has a perfect elimination ordering. Thus to
recognize a chordal graph, we can run the Lex-BFS algorithm, and test whether
the resulting order is a perfect elimination order. Rose et al. [21] showed both
the tasks of performing Lex-BFS of G and testing if the resulting order is PEO
can be done in O(m + n) time. Even though the Lex-BFS runs in linear time,
its implementation is a bit involved, and it takes O(m + n) words of space [21].
Later Tarjan, in an unpublished note [22], derived another simpler and alternate
graph search method for ﬁnding a PEO of chordal graphs, known as Maximum
Cardinality Search (MCS). Tarjan and Yannakakis [23] presented MCS and its
applications to recognize chordal graphs and test acyclicity of hypergraphs, etc.
We refer the interested readers to the excellent text of Golumbic [14] for thorough
coverage of chordal graph recognition, MCS, Lex-BFS and many other related
topics. In what follows, we provide space eﬃcient algorithms for MCS and its
many applications in chordal graphs.
2.1
The MCS Algorithm and Its Implementation
We start by brieﬂy describing the MCS algorithm and its implementation as
provided in [23]. The output of the MCS algorithm is a numbering of the ver-
tices from 1 to n. During the execution of the algorithm, vertices are chosen by
choosing an unnumbered vertex that is adjacent to the most numbered vertices.
Tarjan et al. [23] gave a O(m+n) time implementation of MCS. Even though
they did not explicitly analyse the space requirement of their algorithm, we

90
S. Chakraborty and S.R. Satti
show that it takes O(n) words of space. This exact space bound is particularly
interesting and worth mentioning since many of the subsequent papers actually
cite this version of MCS result saying, the implementation of Tarjan et al. [23]
takes O(m + n) time and words of space. For example see Theorem 7.1 of [16]
and Theorem 5.2 of [8]. In the rest of this section, we brieﬂy describe the original
algorithm of Tarjan et al. [23] and its time and space complexities.
The MCS algorithm of Tarjan et al. maintains an array of sets set[i] for
0 ≤i ≤n −1 where set[i] stores all unnumbered vertices adjacent to exactly i
numbered vertices. So, at the beginning all the vertices belong to set[0]. They
also maintain the largest index j such that set[j] is non-empty. To implement
an iteration of the MCS algorithm, they remove a vertex v from set[j] and
number it. For each unnumbered vertex w adjacent to v, w is moved from the
set containing it, say set[i], to set[i + 1]. If there is a new entry in (j + 1), we
move to set[j + 1] and repeat the same. Otherwise when set[j] becomes empty,
they repeatedly decrement j till a non-empty set is found and in this set we
repeat the same procedure. In order to delete easily, they implement each set as
a doubly-linked list. In addition, for every vertex, they store the index of the set
containing it. This completes the description of the implementation level details
of MCS as provided by Tarjan et al. [23].
Now, observe that as it is described in above implementation, when the vertex
w needs to be moved from set[i] to set[i + 1], we just know the index of set[i]
that w belongs to, but not w’s location inside set[i]. To get overall linear time,
we cannot aﬀord to perform a linear search for w in set[i] as this might be a
costly operation. A simple way to ﬁx this is to, instead of storing for every vertex
v the set index where it belongs to, store a pair (i, j) if a vertex v belongs to the
list set[i] and j is the pointer to v’s location inside set[i]. Then we can directly
access v and move it to set[i+1] from set[i] in constant time. This concludes the
description of our modiﬁed implementation. It is easy to see that every vertex
appears only once in any of these sets, so array set takes at most 2n words in
worst case. Clearly the running time is O(m + n) and space required is O(n)
words. In the next section we provide space eﬃcient implementations for MCS.
3
Space Eﬃcient Implementations of MCS
Algorithm 1: Using n+O(lg n) bits. Here we show using just n+O(lg n) bits,
albeit with increased time, we can perform MCS. Towards that we maintain a
bit vector B (initialized with all 0 entries) of size n bits where the B[i]-th entry
is 1 if and only if the vertex vi has already been numbered. The algorithm works
as follows: at each step it scans the whole B array to ﬁnd all the zero entries
and for each one of them, it goes over the adjacency array to ﬁnd out how
many of its neighbours are already marked ‘1’. Then the vertex v which has the
maximum number of numbered neighbors (ties are broken arbitrarily) is marked
‘1’ in B. We repeat this step until all the vertices are marked. This procedure
uses n + O(lg n) bits. At each step, the algorithm spends O(m) time to ﬁnd a
vertex to number, and this happens exactly n times. Hence, the running time is
O(mn).

Space-Eﬃcient Algorithms for Maximum Cardinality Search
91
Algorithm 2: Using O(n) bits. By increasing the space bound by a constant
factor, we can design a faster algorithm, by using similar ideas as in Tarjan
et al.’s [23] algorithm with a few changes. We deﬁne the label of an unnumbered
vertex (at any instance of the algorithm) as its number of numbered neighbors.
The main idea is to maintain a doubly linked list, call it L, of size at most n/ lg n
at any point during the execution of the algorithm. Each element in L stores a
label k and a pointer to a sublist. The sublist labeled k stores a set of vertices
with label k, and is itself maintained as a doubly linked list. The sublists are
in L are stored in the increasing order of their labels. Moreover, the sum of the
sizes of all the sublists, at any time, is at most n/ lg n. Also, we maintain all the
vertices that are currently stored in L in a balanced binary search tree, T, and
store pointers from the nodes of T to the corresponding vertices in L.
At the beginning of the algorithm, we add n/ lg n vertices into the tree T and
the same vertices also into the list L in a sublist labeled 0 (with pointers to the
corresponding vertices from T to L). As before, we maintain a bit vector B of
length n to keep track of the numbered vertices. The i-th step of the algorithm,
for 1 ≤i ≤n, proceeds as follows. We select the ﬁrst (arbitrary) element of the
sublist with the largest label in L. Let v be the vertex stored in this element.
Then, we ﬁrst number the vertex v with i and delete v from both L and T. We
then go through each unnumbered neighbor w of v and compute the label k of
w. If k is greater than the label of the sublist with the smallest label, then we
add w to L in the sublist labeled k; delete the ﬁrst (arbitrary) element from the
sublist with the smallest label, if necessary (to maintain the invariant that L has
at most n/ lg n elements); and also add (and delete) the corresponding vertices
to the tree T. (Note that we can also add w to L if k is equal to the label of the
sublist with the smallest label as long as the number of elements in L is at most
n/ lg n; but this does not change the worst-case running time of the algorithm.)
Also, if w is already stored in L (in the sublist labeled k −1), then we move w
from the sublist labeled k −1 to the sublist labeled k.
Note that, after a while, due to deletion of vertices, the list L may become
empty. At that time, we reﬁll the list L with n/ lg n unnumbered vertices with
the highest labels (or reﬁll with all the remaining vertices if there are fewer than
n/ lg n unnumbered vertices). This is done by scanning the bit vector B from left
to right, and for each unnumbered vertex v, computing its label. The ﬁrst n/ lg n
unnumbered vertices are simply inserted into the appropriate sublists in L. For
the remaining unnumbered vertices, if the label is greater than the label of the
smallest labeled sublist currently in L, then we insert the new vertex into the
appropriate sublist, and delete the ﬁrst vertex from the sublist with the smallest
label. The cost of this reﬁlling is dominated by the cost of computing the labels
of the vertices which is O(m). After constructing the list L, we insert each of
the vertices in L into an initially empty binary search tree T and add pointers
from the nodes in T to the corresponding elements in L, which takes O(n) time.
The reﬁlling of the list L happens at most O(lg n) times since at least n/ lg n
vertices will be numbered after each reﬁlling, and hence over the full execution
of the algorithm it takes O(m lg n) time. Computing the label of a vertex v takes

92
S. Chakraborty and S.R. Satti
O(dv) time, where dv is the degree of v. During the execution of the algorithm,
we need to compute the label of a vertex v at most O(dv) times (every time
one of its neighbors is numbered). Thus, running time of computing the labels
of vertices is bounded by, O(
v∈V d2
v) = O(m2/n). Finally, moving an element
from one sublist to another takes O(lg n) time since we need to search for it in
the binary search tree ﬁrst, before moving it. Since a vertex v is moved at most
dv times, this step contributes O(
v∈V dv lg n) = O(m lg n) time to the total
running time. Thus overall the algorithm takes O(m2/n + m lg n) time.
Algorithm 3: Using O(n lg m
n ) bits. We show that using O(n lg m
n ) bits we
can design a signiﬁcantly faster algorithm. Note that for sparse graphs (m =
O(n)), this space is only O(n) bits. We ﬁrst scan the adjacency list of each
vertex and construct a bitvector D as follows: starting with an empty bitvector
D, for 1 ≤i ≤n, if di is the degree of the vertex vi, then we append the
string 0⌈lg di⌉−11 to D. The length of D is n
i=1⌈lg di⌉, which is bounded by
O(n lg(m/n)). We also construct auxiliary structures to support select queries
on D in constant time [17]. Like the previous algorithm, we also maintain the
current top O(n/ lg n) values in the list of doubly linked list L along with all
other auxiliary information. Finally, we maintain in a bitmap B marking all the
already numbered and output vertices. Overall space usage is O(n lg m
n ) bits.
The algorithm is essentially same as earlier. The only diﬀerence is that, using
the structure D, we can compute and update the labels of vertices in O(1) time
(instead of O(dv) time as in the earlier algorithm). Thus the running time of
computing the labels of the vertices is now bounded by O(m), and the rest of
the computations is same as earlier. Hence the overall running time is O(m lg n).
The three algorithms described above can be summarized as follows.
Theorem 1. Given a graph G, we can obtain an MCS ordering of G in (a)
O(mn) time using n + O(lg n) bits, or (b) O(m2/n + m lg n) time using O(n)
bits, or (c) O(m lg n) time using O(n lg(m/n)) bits.
4
Applications of MCS
In this section, we provide several applications of our MCS algorithms presented
in the previous section. We start with some surprising connection of our MCS
algorithm with a few other totally unrelated graph problems and their algorithms
with respect to designing space eﬃcient algorithms. Next we show how to use
MCS to provide space eﬃcient solutions for solving some combinatorial problems
in chordal graphs, and also recognition of chordal graphs. We start by describing
the connection with MCS ﬁrst.
4.1
Connection with Other Problems
In this section, we discuss two other seemingly diﬀerent problems and their algo-
rithms which have surprising connection with MCS with respect to designing

Space-Eﬃcient Algorithms for Maximum Cardinality Search
93
space eﬃcient algorithms. First is the problem of topologically sorting the ver-
tices of a directed acyclic graph, and the second is that of ﬁnding the degeneracy
of an undirected graph. The similarity of these two problems with MCS comes
from the fact that the linear time algorithms for all three of these problems have
a very natural greedy strategy. We start by brieﬂy explaining the linear time
greedy algorithms for these two problems.
One of the algorithms for topological sort works by maintaining the in-degree
count of every vertex, and at each step, it deletes a vertex of in-degree zero and
all of its outgoing edges. The order in which the vertices are deleted gives the
topological sorted order. To eﬃciently implement the algorithm, all the vertices
currently having in-degree zero are stored in a queue. This algorithm can also
test if the given graph is acyclic or not at the same time. If it is acyclic, it
produces a topological sort. Note the similairy of this algorithm with that of
MCS. It is not hard to see that each of the solutions for MCS explained before
could be made to work for topological sort with similar resource bounds.
The degeneracy of a graph G is the smallest value d such that every non-
empty subgraph of G contains a vertex of degree at most d. Such graphs have
a degeneracy ordering, i.e., an ordering in which each vertex has d or fewer
neighbors that come later in the ordering. Degeneracy and degeneracy ordering,
can be computed by a simple greedy strategy of repeatedly removing a vertex
with smallest degree (and its incident edges) from the graph until it is empty.
The degeneracy is the maximum of the degrees of the vertices at the time they
are removed from the graph, and the degeneracy ordering is the order in which
vertices are removed from the graph. The linear time implementation of this
works almost in the same way as the MCS algorithm [7]. One can implement
this algorithm space eﬃciently using similar ideas as that of MCS. We omit the
relatively easy details. It would be intersting to ﬁnd other problems with similar
ﬂavour. We want to conclude this section by remarking that, for any greedy
algorithm of this ﬂavour we can use similar technique to design space eﬃcient
implementation. We summarize our results in the theorem below.
Theorem 2. Given a directed graph G, we can report whether G is acyclic or
not, and if so, we can produce a topological sort ordering in (a) O(mn) time
using n + O(lg n) bits, or (b) O(m2/n + m lg n) time using O(n) bits, or (c)
O(m lg n) time using O(n lg(m/n)) bits. Using the same running time and space
bounds, we can also test if an undirected graph G is d-degenerate, and if so, we
can output a d-degenerate ordering of G.
4.2
Finding Independent Set, Vertex Cover and Proper Coloring
In this section, we show that using Theorem 1 how one can solve some combi-
natorial problems on chordal graphs space eﬃciently. Tarjan et al. [23] showed
that, if G is a chordal graph and σ is an MCS ordering of G, then the reverse of
σ is a PEO of G. More speciﬁcally, given the graph G, and a vertex ordering σ
of G, we deﬁne the following:

94
S. Chakraborty and S.R. Satti
• The edge directions implied by σ is obtained by directing (vi, vj) as vi →vj
if i < j and vj →vi if i > j.
• If vi →vj is an edge implied by the order, then vi is the predecessor of vj and
vj is a successor of vi.
Thus the theorem of Tarjan et al. [23] says that, the predecessor set of every ver-
tex (i.e., set containing all the predecessors) in σ forms a clique, or equivalently
in the reverse of σ, for each vertex v, v and its successor set form a clique if and
only if G is chordal. To solve some combinatorial problems in chordal graphs,
we need the reverse PEO whereas for some applications we need the PEO. For
all applications where we need a PEO, we spend extra time for reversing the list
produced by our MCS algorithm as, due to space restriction, we cannot store the
PEO and reverse it in linear time. This seems to be a fundamental bottleneck
while designing space eﬃcient algorithms. We start with the problem of ﬁnd-
ing a maximum independent set (MIS), and for this, a simple greedy strategy
works [14]. Given a reverse PEO of the input chordal graph G, the algorithm
scans the vertices in order, and for every vertex vi, it adds vi to the solution
set I if none of its predecessors has been added to I already. Note that using a
bitmap S of size n bits, where S[i] is set if the vertex vi belongs to I, on top of
the structures of Theorem 1, we can easily implement this to ﬁnd an MIS with
no extra time. Also the complement of the set S gives us a minimum vertex
cover for G. Thus,
Theorem 3. Given a chordal graph G, we can output a maximum independent
set and/or a minimum vertex cover of G in (a) O(mn) time using 2n + O(lg n)
bits, or (b) O(m2/n + m lg n) time using O(n) bits, or (c) O(m + n lg n) time
using O(n lg(m/n)) bits.
In what follows, we discuss a space eﬃcient implementation of ﬁnding a
proper coloring of a chordal graph G. It is known that the natural greedy algo-
rithm [14] for coloring yields the optimal number of colors iﬀthe vertex order
is a PEO. The algorithm works as follows: given a vertex order, it scans the
vertices in order, and colors each vertex with the smallest color not used among
its predecessors. Note that, we need a PEO here, but MCS produces a reverse
one, thus we ﬁrst need to reverse the list produced by the MCS algorithm.
Also, it is easy to see that this coloring scheme assigns, for any vertex v, the
color at most maxi{indeg(vi) + 1} where indeg(vi) refers to the neighbors of
vi which appeared before in PEO and have already been colored. Suppose we
store the explicit colors for each vertex in an array B, then the length of B is
n
i=1⌈lg di + 1⌉= O(n lg(m/n)). We construct auxiliary structures to support
select queries on B in constant time [17]. Suppose we have O(n lg(m/n)) bits
at our disposal, then we run our MCS algorithm and store the last chunk of
O(n/ lgm/n n) vertices in a queue Q. Now dequeue vertices from Q one by one
and run the greedy coloring algorithm while storing the colors in B array explic-
itly, and continue. Once Q becomes empty, run the MCS algorithm to generate
the previous chunk and store them in Q, and repeat the greedy coloring algo-
rithm afterwards. This process is repeated at most O(lgm/n n) times, and each

Space-Eﬃcient Algorithms for Maximum Cardinality Search
95
time we run the MCS algorithm followed by the greedy coloring scheme, hence
total running time is O((m + n lg n) lgm/n n). We summarize our result below.
Theorem 4. Given a chordal graph G, we can output a proper coloring of G in
O((m + n lg n) lgm/n n) time using O(n lg(m/n)) bits.
Note that with O(n) bits, we cannot store the colors of all the vertices simul-
taneosuly, and this posses a challenge for the greedy algorithm. We leave open
the problem to ﬁnd a proper coloring of chordal graphs using O(n) bits.
4.3
Recognition of Chordal Graphs
In what follows, we present a space eﬃcient implementation for the recognition
of chordal graphs. The idea is to apply MCS on G ﬁrst to generate a vertex
ordering, and then check whether the resulting vertex ordering is indeed a PEO.
Let v1, v2, . . . , vn be the ordering of the vertices reported by the MCS algorithm.
Chordal graph recongnition with O(n lg(m/n)) bits. We ﬁrst observe
that one can compute the predecessor/successor of any vertex vi during the
MCS algorithm. Since we have O(n lg(m/n)) bits, we can store one pointer with
each vertex. We use this space to store a pointer to the last predecessor for each
vertex. To test whether G is chordal, we need to test, for each vertex vi, whether
the neighbors of vi numbered less than i form a clique. To perform this test, it
is enough to test whether the predecessor set of vi is a subset of the predecessor
set of vj where vj is the last predecessor of vi [14].
Now we run the MCS algorithm once again, and whenever we number a
vertex vj, we also generate its predecessor set Pj. For this purpose, we maintain
a bit vector B to mark all the vertices that are numbered during the current
MCS algorithm. Using this (dynamic) bit vector B, one can easily generate the
set Pj by simply scanning the adjacency list of vj and checking whether they are
marked in B. This set Pj is stored as a bit vector Q of length n such that Q[ℓ] = 1
iﬀvℓ∈Pj. This helps us in checking the membership of a vertex in the set Pj in
O(1) time. The bit vector Q is initialized in O(n) time at the beginning of the
algorithm, and is used to store the predecessor set of the currently numbered
vertex vj, for 1 ≤j ≤n. After generating the predecessor set Pj of vj, we
scan the adjacency list of vj to check for any vertex vi, where i > j, if vj is
the last predecessor of vi (given vi and vj, we can check whether vj is the last
predecessor of vi in constant time using the predecessor pointer of vi). If vj is the
last predecessor of vj, then we test whether the predecessor set of vi, excluding
vj, is a subset of Pj. Note that since vj is the last predecessor of vi, all the other
predecessors of vi must have already been numbered when vj is numbered, and
hence are stored in the set represented by the bit vector B. Thus we can test
whether the predecessor set of vi is a subset of Pj in time proportional to the
degree of vi (with the aid of the bit vector Q). This completes the description
of our algorithm for chordal graph recongnition.
The overall runtime of the algorithm is dominated by the runtime of the
MCS algorithm. Thus if we can perform MCS on a graph G in t(n, m) time

96
S. Chakraborty and S.R. Satti
using s(m, n) bits, then we can also check whether G is chardal in O(t(n, m))
time using s(m, n) + O(n lg(m/n)) bits.
Chordal graph recongnition with O(n) bits. Since we cannot store all the n
vertices of G (in the MCS order), we generate these vertices in lg n phases, where
in the ℓ-th phase, we generate the vertices Vℓ= {vℓk+1, vℓk+2, . . . vℓ(k+1)}, for
1 ≤ℓ≤lg and k = n/ lg n. In each phase, for each vertex vi generated, we test
the condition whether predecessor set of vi is a subset of the predecessor set of
vj where vj is the last predecessor vi. To do this, we ﬁrst perform another MCS,
to compute the last predecessor of each vertex in Vℓ(in fact, this step can be
combined with the step where we generate the set Vℓ). We maintain a bit vector
to mark the set of predecessors of elements in Vℓto check their membership in
O(1) time, and another bit vector B to mark the numbered vertices. Now we
start another MCS, and whenever a node vj is numbered, we check to see if it
is the last predecessor of some node vi in Vℓ. If so, we generate the predecessor
sets of vj and vi with the aid of B, and check the required condition.
Thus if we can perform MCS on a graph G in t(n, m) time using s(m, n)
bits, then we can also check whether G is chardal in O(t(n, m) · lg n) time using
s(m, n) + O(n) bits. Thus we obtain the following,
Theorem 5. Given an undirected graph G, if MCS on G can be performed
in t(n, m) time using s(m, n) bits, then chordality of G can be tested in (a)
O(t(n, m)) time using s(m, n) + O(n lg(m/n)) bits, or (b) O(t(n, m) · lg n) time
using s(m, n) + O(n) bits.
5
Queue BFS and Stack BFS
The classical Breadth-ﬁrst search (BFS) algorithm (following the nomenclature
of [15], we call it Queue BFS) is one of the most popular graph search methods
along with Depth-ﬁrst search (DFS). The standard implementation of Queue
BFS uses a queue to store the vertices which are to be explored next. The
algorithm also uses three colors (white for unvisited, grey for partially explored
and black for completely explored) for each vertex v to store the diﬀerent state
v could possible be at anytime during the execution of the algorithm. As there
could be O(n) vertices at anytime in the queue at worst case, the classical imple-
mentation takes O(n lg n) bits and O(m + n) time. Recently Banerjee et al. [4]
showed that using just 2n + o(n) bits, we can still perform a (modiﬁed) BFS
traversal of a graph G in O(m+n) time. To reduce the space from O(n lg n) bits
used in the standard implementation, they crucially observe the following two
properties of BFS: (i) elements in the queue are only from two consecutive levels
of the BFS tree, and (ii) elements belonging to the same level can be processed in
“any” order, but elements of the lower level must be processed before processing
elements of the higher level. Thus they replace the queue by a space eﬃcient
“ﬁndany” data structure, for the two consecutive levels, which quickly (in O(1)
time) returns “any” element from these levels, and the algorithm proceeds with
exploring this returned vertex and so on until all the vertices are exhausted. For

Space-Eﬃcient Algorithms for Maximum Cardinality Search
97
details, the readers are referred to [4]. Although this algorithm actually suﬃces if
we are only interested in the shortest path distance in any arbitrary unweighted
undirected or directed graph by means of BFS, one shortcoming of this algo-
rithm is that it looses the ordered structure of the standard BFS because of the
second property. Greenlaw [15] deﬁned another graph search method which he
calls Stack BFS which is essentially a BFS traversal of the input graph but uses
a stack instead of a queue. In what follows we brieﬂy describe space eﬃcient
implementation for both of these graph search methods. Our main theorem is
the following.
Theorem 6. Given a directed or an undirected graph G, we can perform a
Queue BFS and Stack BFS traversal of G in (a) O(m lg2 n) time using O(n)
bits, or (b) O(m + n lg n) time using O(n lg(m/n)) bits.
Proof sketch: If we have only O(n) bits, we store constant number of chunks of
O(n/ lg n) vertices along with the color array and some other auxilliary informa-
tion using O(n) bits so that we can reconstruct the required level of the partially
explored BFS tree whenever needed. We pay for this repeated reconstruction by
incurring extra time. For O(n lg(m/n)) bit algorithm, we can maintain explicitly
the parent pointers for every vertex, and this along with other stored informa-
tion improves the time bound compared to what we can achieve with O(n) bits.
We omit the detailed proof of this theorem due to lack of space. The proof will
appear in the full version of this paper.
6
Conclusions
We showed several time-space tradeoﬀs for performing three diﬀerent graph
search methods, namely MCS, Stack BFS, and Queue BFS. As applications of
these results, we proposed space-eﬃcient implementations for testing if a given
undirected graph is chordal, reporting an independent set, and a proper coloring
of a given chordal graph. Exploring other graph search methods such as Max-
imum Cardinality BFS, Maximum Cardinality DFS, Local MCS [19] and Lex
DFS [8,16] is an interesting open problem. Also, obtaining a sublinear space
implementation for any of these methods (including MCS) is a challenging open
problem.
References
1. Arora, S., Barak, B.: Computational Complexity - A Modern Approach. Cambridge
University Press, New York (2009)
2. Asano, T., Buchin, K., Buchin, M., Korman, M., Mulzer, W., Rote, G., Schulz, A.:
Reprint of: Memory-constrained algorithms for simple polygons. Comput. Geom.
47(3), 469–479 (2014)
3. Asano, T., et al.: Depth-ﬁrst search using O(n) bits. In: Ahn, H.-K., Shin, C.-S.
(eds.) ISAAC 2014. LNCS, vol. 8889, pp. 553–564. Springer, Cham (2014). doi:10.
1007/978-3-319-13075-0 44

98
S. Chakraborty and S.R. Satti
4. Banerjee, N., Chakraborty, S., Raman, V.: Improved space eﬃcient algorithms
for BFS, DFS and applications. In: Dinh, T.N., Thai, M.T. (eds.) COCOON
2016. LNCS, vol. 9797, pp. 119–130. Springer, Cham (2016). doi:10.1007/
978-3-319-42634-1 10
5. Banerjee, N., Chakraborty, S., Raman, V., Roy, S., Saurabh, S.: Time-space trade-
oﬀs for dynamic programming algorithms in trees and bounded treewidth graphs.
In: Xu, D., Du, D., Du, D. (eds.) COCOON 2015. LNCS, vol. 9198, pp. 349–360.
Springer, Cham (2015). doi:10.1007/978-3-319-21398-9 28
6. Barnes, G., Buss, J., Ruzzo, W., Schieber, B.: A sublinear space, polynomial time
algorithm for directed s-t connectivity. SICOMP 27(5), 1273–1282 (1998)
7. Batagelj, V., Zaversnik, M.: An O(m) algorithm for cores decomposition of net-
works. CoRR cs.DS/0310049 (2003)
8. Berry, A., Krueger, R., Simonet, G.: Maximal label search algorithms to compute
perfect and minimal elimination orderings. SIAM J. Discrete Math. 23(1), 428–446
(2009)
9. Chakraborty, S., Jo, S., Satti, S.R.: Improved space-eﬃcient linear time algorithms
for some classical graph problems. In: 15th CTW (2017)
10. Chakraborty, S., Raman, V., Satti, S.R.: Biconnectivity, chain decomposition and
st-numbering using O(n) bits. In: 27th ISAAC, vol. 64. LIPIcs, pp. 22:1–22:13.
Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik (2016)
11. Darwish, O., Elmasry, A.: Optimal time-space tradeoﬀfor the 2D convex-hull prob-
lem. In: Schulz, A.S., Wagner, D. (eds.) ESA 2014. LNCS, vol. 8737, pp. 284–295.
Springer, Heidelberg (2014). doi:10.1007/978-3-662-44777-2 24
12. Edmonds, J., Poon, C.K., Achlioptas, D.: Tight lower bounds for st-connectivity
on the NNJAG model. SIAM J. Comput. 28(6), 2257–2284 (1999)
13. Fulkerson, D.R., Gross, O.A.: Incidence matrices and interval graphs. Paciﬁc J.
Math 15, 835–855 (1965)
14. Golumbic, M.C.: Algorithmic Graph Theory and Perfect Graphs (2004)
15. Greenlaw, R.: A model classifying algorithms as inherently sequential with appli-
cations to graph searching. Inf. Comput. 97(2), 133–149 (1992)
16. Krueger, R., Simonet, G., Berry, A.: A general label search to investigate classical
graph search algorithms. Discret. Appl. Math. 159(2–3), 128–142 (2011)
17. Munro, J.I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS, vol.
1180, pp. 37–42. Springer, Heidelberg (1996). doi:10.1007/3-540-62034-6 35
18. Munro, J.I., Paterson, M.: Selection and sorting with limited storage. Theor. Com-
put. Sci. 12, 315–323 (1980)
19. Panda, B.S.: New linear time algorithms for generating perfect elimination order-
ings of chordal graphs. Inf. Process. Lett. 58(3), 111–115 (1996)
20. Reingold, O.: Undirected connectivity in log-space. J. ACM 55(4), 1–24 (2008)
21. Rose, D.J., Tarjan, R.E., Lueker, G.S.: Algorithmic aspects of vertex elimination
on graphs. SIAM J. Comput. 5(2), 266–283 (1976)
22. Tarjan, R.E.: Maximum cardinality search and chordal graphs. Unpublished Lec-
ture Notes CS 259
23. Tarjan, R.E., Yannakakis, M.: Simple linear-time algorithms to test chordality of
graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs.
SIAM J. Comput. 13(3), 566–579 (1984)
24. Tompa, M.: Two familiar transitive closure algorithms which admit no polynomial
time, sublinear space implementations. SIAM J. Comput. 11(1), 130–137 (1982)

Eﬃcient Enumeration of Non-Equivalent
Squares in Partial Words with Few Holes
Panagiotis Charalampopoulos1(B), Maxime Crochemore1,2,
Costas S. Iliopoulos1, Tomasz Kociumaka3, Solon P. Pissis1,
Jakub Radoszewski1,3, Wojciech Rytter3, and Tomasz Wale´n3
1 Department of Informatics, King’s College London, London, UK
{panagiotis.charalampopoulos,maxime.crochemore,
costas.iliopoulos,solon.pissis}@kcl.ac.uk
2 Universit´e Paris-Est, Marne-la-Vall´ee , France
3 Faculty of Mathematics, Informatics and Mechanics,
University of Warsaw, Warsaw, Poland
{kociumaka,jrad,rytter,walen}@mimuw.edu.pl
Abstract. A word of the form WW for some word W ∈Σ∗is called
a square, where Σ is an alphabet. A partial word is a word possibly
containing holes (also called don’t cares). The hole is a special symbol
♦/∈Σ which matches (agrees with) any symbol from Σ ∪{♦}. A p-square
is a partial word matching at least one square WW without holes. Two
p-squares are called equivalent if they match the same set of squares. We
denote by psquares(T) the number of non-equivalent p-squares which are
factors of a partial word T. Let PSQUARESk(n) be the maximum value
of psquares(T) over all partial words of length n with at most k holes.
We show asymptotically tight bounds:
c1 · min(nk2, n2) ≤PSQUARESk(n) ≤c2 · min(nk2, n2)
for some constants c1, c2 > 0. We also present an algorithm that com-
putes psquares(T) in O(nk3) time for a partial word T of length n with k
holes. In particular, our algorithm runs in linear time for k = O(1) and its
time complexity near-matches the maximum number of non-equivalent
p-square factors in a partial word.
1
Introduction
A word is a sequence of letters from a given alphabet Σ. By Σ∗we denote the set of
all words over Σ. A word of the form U 2 = UU, for some word U, is called a square.
For a word W, a square factor is a factor of W which is a square. Enumeration of
P. Charalampopoulos—Supported by the Graduate Teaching Scholarship scheme of
the Department of Informatics at King’s College London.
T. Kociumaka—Supported by Polish budget funds for science in 2013–2017 as a
research project under the ’Diamond Grant’ program.
J. Radoszewski, W. Rytter and T. Wale´n—Supported by the Polish National Science
Center, grant no. 2014/13/B/ST6/00770.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 99–111, 2017.
DOI: 10.1007/978-3-319-62389-4 9

100
P. Charalampopoulos et al.
square factors in words is a well-studied topic, both from a combinatorial and from
an algorithmic perspective. Obviously, a word W of length n may contain Θ(n2)
square factors (e.g. W = an), however, it is known that such a word contains
only O(n) distinct square factors [14,17]; currently the best known upper bound
is 11
6 n [12]. Moreover, all distinct square factors of a word can be listed in O(n)
time using the suﬃx tree [15] or the suﬃx array and the structure of runs (maximal
repetitions) in the word [10].
A partial word is a sequence of letters from Σ ∪{♦}, where ♦denotes a hole,
that is, a don’t care symbol. We assume that Σ is non-unary. Two symbols
a, b ∈Σ ∪{♦} are said to match (denoted as a ≈b) if they are equal or one of
them is a hole; note that this relation is not transitive. The relation of matching
is extended in a natural way to partial words of the same length.
A partial word UV is called a p-square if U ≈V . Like in the context of
words, a p-square factor of a partial word T is a factor being a p-square; see
[2,7]. Alongside [2,6,7], we deﬁne a solid square (also called a full square) as a
square of a word, and a square subword of a partial word T as a solid square
that matches a factor of T.
We introduce the notion of equivalence of p-square factors in partial words.
Let sq-val(UV ) denote the set of solid squares that match the partial word UV :
sq-val(UV ) = {WW : W ∈Σ∗, WW ≈UV }.
Example 1. sq-val(a♦b a♦♦) = {(aab)2, (abb)2}, with Σ = {a, b}.
Then p-squares UV
and U ′V ′ are called equivalent if sq-val(UV )
=
sq-val(U ′V ′) (denoted as UV ≡U ′V ′). For example, a♦b a♦♦≡a♦♦♦♦b, but
a♦b a♦♦̸≡a♦♦♦ab.
Note that two p-square factors of a partial word T are equivalent in this
sense if and only if they correspond to exactly the same set of square subwords.
The number of non-equivalent p-square factors in a partial word T is denoted by
psquares(T). Our work is devoted to the enumeration of non-equivalent p-square
factors in a partial word with a given number k of holes.
We say that X2 = XX is the representative (also called general form; see
[6]) of a p-square UV , denoted as repr(UV ), if XX ≈UV and sq-val(XX) =
sq-val(UV ). (In other words, X is the “most general” partial word that matches
both U and V .) It can be noted that the representative of a p-square is unique.
Then UV ≡U ′V ′ if and only if repr(UV ) = repr(U ′V ′).
Example 2. repr(a♦b a♦♦) = (a♦b)2, repr(a♦♦♦ab) = (aab)2.
Previous studies on squares in partial words were mostly focused on combi-
natorics. They started with the case of k = 1 [6], in which case distinct square
subwords correspond to non-equivalent p-square factors. It was shown that a
partial word with one hole contains at most
7
2n distinct square subwords [4]
(3n for binary partial words [16]). Also a generalization of the three squares
lemma (see [11]) was proposed for partial words [5]. As for a larger number of
holes, the existing literature is devoted mainly to counting the number of distinct

Eﬃcient Enumeration of Non-Equivalent Squares
101
square subwords of a partial word [2,6] or all occurrences of p-square factors [2,3].
On the algorithmic side, [21] proved that the problem of counting distinct square
subwords of a partial word is #P-complete and [13,20] and [7] showed quadratic-
and nearly-quadratic-time algorithms for ﬁnding all occurrences of p-square fac-
tors and primitively-rooted p-square factors of a partial word, respectively.
Our combinatorial results. We prove that a partial word of length n with
k holes contains O(nk2) non-equivalent p-square factors. We also construct
a family of partial words that contain Ω(nk2) non-equivalent p-square fac-
tors, for k = O(√n). This proves the aforementioned asymptotic bounds for
PSQUARESk(n). Our work can be viewed as a generalization of the results on
partial words with one hole [4,6,16] to k ≥1 holes.
Our algorithmic results. We present an algorithm that reports all non-
equivalent p-square factors in a partial word of length n with k holes in O(nk3)
time. In particular, our algorithm runs in linear time for k = O(1) and its time
complexity near-matches the maximum number of non-equivalent p-square fac-
tors. We assume integer alphabet Σ ⊆{1, . . . , nO(1)}. The main tool in the
algorithm are two new types of non-standard runs in partial words and rela-
tions between them. We also use recently introduced advanced data structures
from [18].
2
Preliminary Notation for Words and Partial Words
For a word W ∈Σ∗, by |W| = n we denote the length of W, and by W[i], for
i = 1, . . . , n, the ith letter of W. For 1 ≤i ≤j ≤n, W[i..j] denotes the factor of
W equal to W[i] · · · W[j]. A factor of the form W[1..j] is called a preﬁx, a factor
of the form W[i..n] is called a suﬃx, and a factor that is both a preﬁx and a
suﬃx of W is called a border of W. A positive integer q is called a period of W
if W[i] = W[i + q] for all i = 1, . . . , n −q. In this case, W[1..q] is called a string
period of W. W has a period q if and only if it has a border of length n −q;
see [8]. Two equal-length words V and W are called cyclic shifts if there are
words X, Y such that V = XY and W = Y X. A word W is called primitive if
there is no word U and integer k > 1 such that U k = W. Note that the shortest
string period of W is always primitive. Every primitive word W has the following
synchronization property: W is not equal to any of its non-trivial cyclic shifts [8].
For a partial word T we use the same notation as for words: |T| = n for its
length, T[i] for the ith letter, T[i..j] for a factor. If T does not contain holes,
then it is called solid. The relation of matching on Σ ∪{♦} is deﬁned as: a ≈a,
♦≈a, and a ≈♦for all a ∈Σ ∪{♦}. We deﬁne an operation ∧such that
a ∧a = a ∧♦= ♦∧a = a for all a ∈Σ ∪{♦}, and otherwise a ∧b is undeﬁned.
Two equal-length partial words T and S are said to match (denoted as T ≈S) if
T[i] ≈S[i] for all i = 1, . . . , n. In this case, by S ∧T we denote the partial word
S[1] ∧T[1], . . . , S[n] ∧T[n]. If U ≈T[i..i + |U| −1] for a partial word U, then we
say that U occurs in T at position i. Also note that if UV is a p-square, then
repr(UV ) = (U ∧V )2. A quantum period of T is a positive integer q such that

102
P. Charalampopoulos et al.
T[i] ≈T[i + q] for all i = 1, . . . , n −q. A deterministic period of T is an integer
q such that there exists a word W such that W ≈T and W has a period q.
T is called quantum (deterministically) periodic if it has a quantum (determin-
istic) period q such that 2q ≤n.
An integer j is an ambiguous length in the partial word T if there are two
holes in T at distance j/2. A p-square is called ambiguous if its representative
is non-solid. Note that if a p-square factor in T is ambiguous, then the p-square
has an ambiguous length (the converse is not always true). The p-square factors
of T of non-ambiguous length have solid representatives.
Example 3. Let T = ab♦♦ba♦aaba♦b. For T, 4 is a non-ambiguous length. T
contains four non-equivalent classes of p-squares of length 4: a♦aa with repre-
sentative (aa)2, ab♦♦≡♦ba♦≡aba♦with representative (ab)2, ♦♦ba ≡ba♦a with
representative (ba)2, and b♦♦b with representative (bb)2. On the other hand, 6 is
an ambiguous length in T. T contains four non-equivalent classes of p-squares of
length 6: aaba♦b with representative (aab)2, ab♦♦ba ≡a♦aaba with representative
(aba)2, ♦aaba♦with representative (baa)2, and b♦♦ba♦with representative (ba♦)2.
Note that only the last one is an ambiguous p-square. Overall, T contains 14
non-equivalent p-squares.
3
Combinatorial Bounds
3.1
Lower Bound
We say that a set A of positive integers is an (m, t)-cover if the following condi-
tions hold:
(1) For each d ≥m, A contains at most one pair of elements with diﬀerence d;
(2) | { |j −i| ≥m : i, j ∈A } | ≥t.
For a set A ⊆{1, . . . , n} we denote by wA,n the partial word of length n over
the alphabet Σ such that wA,n[i] = ♦⇔i ∈A, and wA,n[i] = a otherwise.
Lemma 4. Assume that A ⊆{1, . . . , n} is an (m, t)-cover such that m = Θ(n),
|A| = k, and t = Ω(k2). Let Σ = {a, b} be the alphabet. Then
psquares(an−2 · wA,n · an−2) = Ω(n · k2).
Proof. Each even-length factor of an−2 · wA,n · an−2 is a p-square. Let Z be
the set of these factors X which contain two positions i, j containing holes with
|j −i| ≥m and |X| = 2|j −i|. As A is an (m, t)-cover, i and j are determined
uniquely by d = |j −i|. Then all elements of Z are pairwise non-equivalent
p-squares. The size of Z is Ω(nt) which is Ω(n·k2). This completes the proof. ⊓⊔
Example 5. Let n = 5, m = 4, and t = 1. aaa♦aaa♦aaa has 4 non-equivalent
p-square factors of length 8 if Σ = {a, b}. If Σ = {a}, all of them are equivalent.

Eﬃcient Enumeration of Non-Equivalent Squares
103
Theorem 6. For every positive integer n and k ≤
√
2n, there is a partial word
of length n with k holes that contains Ω(nk2) non-equivalent p-square factors.
Proof. Due to Lemma 4, it is enough to construct a suitable set A. By monotonic-
ity, we may assume that k and n are even. We take:
A = {1, . . . , k
2} ∪{j · k
2 + n
2 : 1 ≤j ≤k
2}.
We claim that A is an ( n
2 , k2
4 )-cover for t = Ω(k2). Indeed, take any i ∈{1, . . . , k
2}
and j satisfying the above condition. Then j · k
2 + n
2 −i ≥n
2 and all such values
are distinct; hence, t = k2
4 . The thesis follows from the claim.
⊓⊔
3.2
Upper Bound
Let T be a partial word of length n with k holes. The proof of the upper bound
for ambiguous lengths is easy.
Lemma 7. There are at most nk2 p-square factors of ambiguous length in T.
Proof. The number of ambiguous lengths is at most
k
2

, since we have
k
2

possi-
ble distances between k holes. Consequently, the number of p-squares with such
lengths is at most nk2.
⊓⊔
Each of the remaining p-square factors of T has a solid representative. We
say that a solid square W 2 has a solid occurrence in T if T contains a factor
equal to W 2. By the following fact, there are at most 2n non-equivalent p-square
factors of T with solid occurrences.
Fact 8 ([12,14,17]). Every position of a (solid) word contains at most two right-
most occurrences of squares.
We say that a solid square is a u-square in T if it occurs in T, does not have
a solid occurrence in T, and has a non-ambiguous length. We denote by U the
set of u-squares for T.
Observation 9. Each u-square in T corresponds in a one-to-one way to an
equivalence class of p-square factors of T which have non-ambiguous length and
do not have a solid occurrence in T.
Thus it suﬃces to bound |U|. This is the essential part of the proof.
Let α =
1
2k+2 and
U(ℓ) = {W 2 ∈U : 2ℓ≤|W|2 ≤2(ℓ+ ⌊ℓα⌋)}.
Also denote by Ui(ℓ) (and Ulasti(ℓ)) the set of words of U(ℓ) which have an
occurrence (the last occurrence, respectively) at position i in T. The next lemma
follows from the pigeonhole principle and periodicity of (solid) words.

104
P. Charalampopoulos et al.
Lemma 10. Suppose that ℓ≥
1
α and |Ui(ℓ)| ≥2. Let Δ = ⌊ℓα⌋. There exist
positions s, s′ such that:
– s ∈[i, i + ℓ−2Δ],
– s′ ∈[s + ℓ, s + ℓ+ Δ],
– T[s..s + 2Δ −1] = T[s′..s′ + 2Δ −1] is solid and periodic.
Proof. Let T[i..i + 2d −1] be a u-square from Ui(ℓ). Consider positions xj =
i+2jΔ and yj = xj +d for 0 ≤j ≤k. Note that factors Xj = T[xj..xj +2Δ−1]
and Yj = T[yj..yj + 2Δ −1] match; see Fig. 1. Moreover, factors X0, . . . , Xk and
Y0, . . . , Yk are disjoint because 2(k + 1)Δ ≤2(k + 1)
ℓ
2k+2 = ℓ. By the pigeonhole
principle, we can choose j so that Xj and Yj are solid, i.e., Xj = Yj. We set
s = xj and s′ = yj.
It remains to prove that Xj = Yj is periodic. Let T[i..i+2d′−1] (with d′ ̸= d)
be another u-square in Ui(ℓ), and let Y ′
j = T[xj +d′..xj +d′ +2Δ−1]. Note that
Y ′
j ≈Xj = Yj and factors Yj and Y ′
j have an overlap of 2Δ −|d −d′| positions
being a border of Yj. Consequently, |d −d′| ≤⌊ℓα⌋= Δ is a period of Yj.
⊓⊔
Δ
ℓ
Xj
Yj
i
s
s′
W 2
(W ′)2
T
Fig. 1. Situation from the proof of Lemma 10; W 2, (W ′)2 ∈Ui(ℓ). Occurrences of
Xj = Yj are denoted by dark rectangles. Ui(ℓ) is the set of all u-squares having an
occurrence in i with center in the window of size Δ.
We denote by Ii,ℓthe interval [i, i + 2(ℓ+ ⌊ℓα⌋) −1]. Let #♦([a, b]) denote
the number of holes in T[a..b]. Our upper bound for partial words is based on
the following key lemma; it is a property of partial words similar to Fact 8.
Lemma 11. |Ulasti(ℓ)| = O(#♦(Ii,ℓ)).
Proof. Denote k′ = #♦(Ii,ℓ). If k′ = 0, then |Ulasti(ℓ)| = 0. From now we
assume that k′ ≥1. Assume that |Ulasti(ℓ)| ≥2. Let p be the shortest period of
the equal periodic factors X = T[s..s + 2Δ −1] and Y = T[s′..s′ + 2Δ −1] from
the previous lemma. We consider three types of u-squares W 2 ∈Ulasti(ℓ):
Type (a): W 2 has period p;
Type (b): W has period p but W 2 does not have period p;
Type (c): W does not have period p.

Eﬃcient Enumeration of Non-Equivalent Squares
105
At most 1 u-square of type (a). Observe that the length of W is a multiple
of its shortest period p (this is due to the synchronization property for the string
period of W). Consequently, if we have two u-squares of type (a) occurring at
position i and with the same shortest period p, then the shorter u-square also
occurs at position i + p. This contradicts the deﬁnition of Ulasti(ℓ).
At most k′ + 1 u-squares of type (b). Suppose to the contrary that there
are at least k′ + 2 u-squares of type (b), of lengths d1 < . . . < dk′+2. Note that
Y ′
j := T[s + dj..s + dj + 2Δ −1] matches X = Y due to a u-square of length 2dj.
Moreover, the factors Y and Y ′
j have an overlap of at least Δ ≥p positions, so
the string periods of Y ′
j and Y must be synchronized. Consequently, the values
dj mod p are all the same (and non-zero, as these are not squares of type (a)).
Consider the shortest W 2 and the longest (W ′)2 of these u-squares and the
factor Z = T[i+d1..i+dk′+2−1]. It matches a preﬁx P of length dk′+2−d1 of W
and a suﬃx S of the same length of W ′. Both P and S have period p; however,
their string periods of length p are not equal (again, due to synchronization
property), as p does not divide d1. Consequently, in every factor of length p in
Z there must be a hole. This yields ⌊|Z|/p⌋= (dk′+2 −d1)/p ≥k′ + 1 holes
in total, a contradiction.
At most 4k′ + 2 u-squares of type (c). Let d = |W|. Let us extend the
occurrence of X in W at position s −i + 1 to a maximal factor W[j′..j] with
period p. Note that j′ > 1 or j < d as W 2 is not of type (b). Below, we assume
j < d; the other subcase is handled in an analogous way. Consider the positions
j1 = i + j and j2 = i + d + j of T. We will show that there are at most 2k′ + 1
possible pairs (j1, j2) across the u-squares W ∈Ulasti(ℓ), i.e., at most 2k′ + 1
corresponding u-squares, as d = j2 −j1.
Positions T[j1] and T[j2] cannot both contain holes, as 2d is a non-ambiguous
length. If T[j1] is not a hole, then it is determined uniquely as the ﬁrst position
where the deterministic period p breaks, starting from the position s, i.e., j1 is
the smallest index such that T[s..j1] does not have deterministic period p. The
same holds for j2 and s′; this is also due to the fact that Y and the occurrence
of X at position s + d have an overlap of at least Δ ≥p positions, so they
are synchronized. Hence, if neither T[j1] nor T[j2] is a hole, then (j1, j2) is
determined uniquely. Otherwise, if T[j1] or T[j2] is a hole, then the other position
is determined uniquely, so there are at most 2k′ choices. This concludes the
proof.
⊓⊔
Theorem 12. The number of non-equivalent p-square factors in a partial word
T of length n with k holes is O(min(nk2, n2)).
Proof. The O(n2) bound is obvious. Due to Lemma 7 there are at most nk2
p-squares of ambiguous length in T. Let us consider p-squares of non-ambiguous
lengths. By Fact 8, among them there are O(n) non-equivalent p-squares with
a solid occurrence. From now on we count only non-equivalent non-ambiguous
p-squares without a solid occurrence, i.e., diﬀerent u-squares.

106
P. Charalampopoulos et al.
Clearly, there are O(nk) diﬀerent u-squares of length smaller than
2
α. Let
ℓ≥1
α and r = 2(ℓ+ ⌊ℓα⌋). By Lemma 11:
|U(ℓ)| =
n

i=1
|Ulasti(ℓ)| = O
 n

i=1
#♦(Ii,ℓ)

= O(kℓ).
(1)
The last equality is based on the fact that each of the k holes in T is counted in
at most 2r terms #♦(Ii,ℓ).
Let us consider a family of endpoints rj =

n
(1+α)j

for j ≥0 and let t =
max{j : rj > 1}. One can check that U = 	t
j=0 U(rj+1).
By (1), the total number of u-squares of length at least 2
α in T is at most:
t+1

j=1
|U(rj)| = O

t+1

j=1
krj

= O

k
t+1

j=1

1 +
n
(1+α)j

= O

k log1+α n +
∞

j=0
nk
(1+α)j

= O

k log n
α
+
nk
1−
1
1+α

= O(nk2).
⊓⊔
4
Runs Toolbox for Partial Words
A run (also called a maximal repetition) in a word W is a triple (a, b, q) such that
W[a..b] is periodic with period q (2q ≤b −a + 1) and the interval [a, b] cannot
be extended to the left nor to the right without violating the above property,
that is, W[a −1] ̸= W[a + q −1] and W[b −q + 1] ̸= W[b + 1], provided that the
respective positions exist. The exponent of a run is deﬁned as b−a+1
q
. A word of
length n has O(n) runs and they can all be computed in O(n) time [1,19].
From a run (a, b, q) we can produce all triples (a, b, kq) for integer k ≥1 such
that 2kq ≤b −a + 1; we call such triples generalized runs. That is, the period
of a generalized run need not be the shortest period. The number of generalized
runs is also O(n) as the sum of exponents of runs is O(n) [1,19].
For a partial word T, we call a triple (a, b, q) a quantum generalized run (Q-
run, for short) in T if T[a..b] is quantum periodic with period q and none of the
partial words T[a −1..b] and T[a..b + 1] (if it exists) has the quantum period q;
for an example see Fig. 2.
a
b
♦
♦
b
a
♦
a
a
b
a
♦
b
Fig. 2. A partial word together with all its Q-runs.

Eﬃcient Enumeration of Non-Equivalent Squares
107
Generalized runs in words are strongly related to squares: (1) a square of
length 2q belongs to a generalized run of period q and, moreover, (2) all factors of
length 2q of a generalized run with period q are squares being each other’s cyclic
shifts. Unfortunately, Q-runs in partial words have only property (1). However,
we introduce a type of run in partial words that has a property analogous to
(2). A pseudorun is a triple (a, b, q) such that:
(a) T[a..b] is quantum periodic with period q,
(b) T[i −q] ∧T[i] = T[i] ∧T[i + q] for all i such that i −q, i + q ∈[a, b],
(c) none of the partial words T[a −1..b] and T[a..b + 1] (if exists) satisﬁes the
conditions (a) and (b).
We say that a p-square factor T[c..d] is induced by the pseudorun (a, b, q) if
d −c + 1 = 2q and [c, d] ⊆[a, b].
Example 13. The partial word from Fig. 2 contains two Q-runs with period 2:
(1, 9, 2) that corresponds to factor ab♦♦ba♦aa and (9, 12, 2) that corresponds to
factor aba♦. The partial word contains ﬁve pseudoruns with this period: (1, 4, 2):
ab♦♦, (2, 5, 2): b♦♦b, (3, 8, 2): ♦♦ba♦a, (6, 9, 2): a♦aa, and (9, 12, 2): aba♦. All but
one of these pseudoruns induce exactly one p-square; the pseudorun (3, 8, 2)
induces two non-equivalent p-squares: ♦♦ba and ♦ba♦.
Observation 14. (1) Every p-square factor in T is induced by a pseudorun.
(2) All factors of length 2q of a pseudorun with period q are p-squares and their
representatives are each other’s cyclic shifts.
5
The Algorithm
We design an O(nk3)-time algorithm for enumerating non-equivalent p-squares
in a partial word T of length n with k holes. We assume that Σ is an ordered
integer alphabet and that ♦is smaller than all the letters from Σ. Then any two
factors of T can be lexicographically compared using the suﬃx array of T in O(1)
time after O(n)-time preprocessing [8]. The ﬁrst two steps of the algorithm are
computing all Q-runs in T and decomposing Q-runs into pseudoruns. The ﬁnal
phase consists in grouping pseudoruns in T by the representatives of induced
p-squares, which lets us enumerate non-equivalent p-squares.
5.1
Computing Q-Runs
We classify Q-runs into solid Q-runs that do not contain a hole and the remaining
non-solid Q-runs. A solid Q-run is a generalized run in a maximal solid factor of
T that is not adjacent to a hole in T. Thus all solid Q-runs can be computed in
O(n) time using any linear-time algorithm for computing runs in words [1,19].
The length of the longest common compatible preﬁx of two positions i, j,
denoted lccp(i, j), is the largest ℓsuch that T[i..i + ℓ−1] ≈T[j..j + ℓ−1].
Symmetrically, we can deﬁne lccs(i, j) as the length of the longest common

108
P. Charalampopoulos et al.
compatible suﬃx of T[1..i] and T[1..j]. After O(nk)-time preprocessing, queries
for lccp (hence, queries for lccs) can be answered on-line in O(1) time [9].
For every position i containing a hole and integer q ∈{1, . . . , n}, we can use
the lccp- and lccs-queries to check if there is a Q-run with period q containing
the position i. If the Q-run is to contain i anywhere except for its last q positions,
we can compute a = i −lccs(i, i + q) + 1, b = i + q + lccp(i, i + q) −1 and check
if b −a + 1 ≥2q; if so, the sought Q-run is (a, b, q). A symmetric test with i −q
and i can be used to check for a Q-run containing i among its last q positions.
Clearly, this procedure works in O(nk) time. Therefore, the number of Q-
runs is at most O(nk). The same Q-run may be reported several times; therefore,
in the end we remove repeating triples (a, b, q) via radix sort. Together with the
O(n)-time computation of solid Q-runs we arrive at the following lemma.
Lemma 15. A partial word of length n with k holes contains O(nk) Q-runs and
they can all be computed in O(nk) time.
5.2
Computing Pseudoruns
Q-runs correspond to maximal factors of T that satisfy only the condition (a)
of a pseudorun. Hence, every pseudorun is a factor of a Q-run.
A position i inside a Q-run β = (a, b, q) is called a break point if a ≤i −q <
i + q ≤b and T[i −q] ∧T[i] ̸= T[i] ∧T[i + q].
Observation 16. i is a break point for (a, b, q) if and only if a ≤i−q < i+q ≤b,
T[i] = ♦, and T[i −q] ̸= T[i + q].
By Γ(β) we denote the set of all break points of a Q-run β. The Q-run can
be decomposed into |Γ(β)| + 1 pseudoruns: if i is the ﬁrst break point in β,
then we have a pseudorun (a, i + q −1, q) and continue the decomposition for
(i −q + 1, b, q). Consecutive pseudoruns in the decomposition overlap by 2p −1
positions. See Fig. 3 for an abstract illustration.
a
i
j
b
q
q
q
q
q
Fig. 3. A Q-run (a, b, q) with break points at positions i and j is decomposed into three
pseudoruns: (a, i + q −1, q), (i −q + 1, j + q −1, q), and (j −q + 1, b, q).
Lemma 17. 
β∈Q-runs(T ) |Γ(β)| ≤nk.
Proof. Consider all Q-runs β of period q. Every two overlap by at most q −1
positions, so the Γ(β) sets are pairwise disjoint and their sizes sum up to at
most k. Summing up over all q = 1, . . . , n/2, we arrive at the conclusion.
⊓⊔

Eﬃcient Enumeration of Non-Equivalent Squares
109
Lemma 17 shows that there are O(nk) pseudoruns (we use the fact that, by
Lemma 15, there are O(nk) Q-runs). They can be computed in O(nk2) time
by inspecting all the holes inside each Q-run β and checking which of them are
break points in β.
Lemma 18. A partial word of length n with k holes contains O(nk) pseudoruns
and they can all be computed in O(nk2) time.
5.3
Grouping Pseudoruns and Reporting Squares
We deﬁne the representative of a pseudorun β = (a, b, q) as
repr(β) = lex-min{repr(T[i..i + 2q −1]) : a ≤i ≤b −2q + 1}.
First, let us show how to group pseudoruns by equal representatives. This part
of our algorithm builds upon the methods for grouping runs in words from [10].
We use a separate approach for solid and for non-solid pseudoruns. Each solid
pseudorun corresponds to a solid Q-run. Hence, there are O(n) of them and they
can all be grouped using the approach of [10] in O(n) time.
We say that a partial word U is a d-fragment of T if U is a factor of T with
symbols at d positions substituted with other symbols. Obviously, a d-fragment
can be represented in O(d) space. The following lemma is a consequence of
Observation 18 from [18] and Theorem 23 from [18].
Lemma 19 ([18]). For a word of length n, after O(n)-time preprocessing:
(a) Any two d-fragments can be compared lexicographically in O(d) time;
(b) The minimal cyclic shift of a d-fragment can be computed in O(d2) time.
Lemma 20. After O(n)-time preprocessing, for any pseudorun β, repr(β) rep-
resented as a k-fragment can be computed in O(k2) time.
Proof. Let β = (a, b, q). Knowing the positions of holes in T, we can represent
repr(T[a..a + 2q −1]) = U 2 as a k-fragment (the positions with holes of the p-
square are ﬁlled with single symbols). By Lemma 19(b), we can ﬁnd the minimal
cyclic shift of the k-fragment in O(k2) time. The cyclic shift can be represented
as a k-fragment as well. We apply this to ﬁnd (U ′)2, the minimal cyclic shift of
U 2. Then repr(β) = (U ′)2.
⊓⊔
We group non-solid pseudoruns by their periods ﬁrst; let Rq be the set of
non-solid pseudoruns with period q. From what we have already observed, we
see that every pseudorun from Rq can overlap with at most six other pseudoruns
from Rq: two that come from the same Q-run and two that come from each of
the neighbouring Q-runs with period q. Hence, each hole position is contained
in at most seven pseudoruns from Rq, and |Rq| ≤7k. The representatives of
pseudoruns from Rq can be sorted using O(k)-time comparison (Lemma 19(a)).
Thus the time complexity for sorting and grouping all pseudoruns from Rq is
O(k2 log k), which gives O(nk2 log k) in total.

110
P. Charalampopoulos et al.
By Observation 14, the representatives of all p-squares induced by a
pseudorun β are cyclic shifts of repr(β). Thus only pseudoruns from the same
group may induce equivalent p-squares. For each pseudorun β we can specify an
interval I(β) of cyclic shift values of induced p-squares. Then all non-equivalent
p-squares induced by pseudoruns in the same group can be reported by carefully
processing the intervals I(β) as in [10]. This processing takes time linear in the
number of all intervals from all groups and n, i.e., O(nk) time. This concludes
the algorithm.
Theorem 21. All non-equivalent p-squares in a partial word of length n with k
holes can be reported (as factors of the partial word) in O(nk3) time.
Proof. Lemma 18 shows that there are O(nk) pseudoruns in a partial word and
they can all be computed in O(nk2) time. Solid pseudoruns can be handled
separately in O(n) time. Lemma 20 lets us ﬁnd the representatives of non-solid
pseudoruns in O(nk3) time. In the end, we group those pseudoruns by the rep-
resentatives in O(nk2 log k) time and use the approach from [10] to report all
non-equivalent p-squares induced by each group in O(nk) time.
⊓⊔
References
1. Bannai, H., I, T., Inenaga, S., Nakashima, Y., Takeda, M., Tsuruta, K.: A new
characterization of maximal repetitions by Lyndon trees. In: Indyk, P. (ed.) 26th
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pp. 562–571.
SIAM (2015)
2. Blanchet-Sadri, F., Bodnar, M., Nikkel, J., Quigley, J.D., Zhang, X.: Squares and
primitivity in partial words. Discrete Appl. Math. 185, 26–37 (2015)
3. Blanchet-Sadri, F., Jiao, Y., Machacek, J.M., Quigley, J., Zhang, X.: Squares in
partial words. Theor. Comput. Sci. 530, 42–57 (2014)
4. Blanchet-Sadri, F., Merca¸s, R.: A note on the number of squares in a partial word
with one hole. Inform. Theor. Appl. 43(4), 767–774 (2009)
5. Blanchet-Sadri, F., Merca¸s, R.: The three-squares lemma for partial words with
one hole. Theor. Comput. Sci. 428, 1–9 (2012)
6. Blanchet-Sadri, F., Merca¸s, R., Scott, G.: Counting distinct squares in partial
words. Acta Cybern. 19(2), 465–477 (2009)
7. Blanchet-Sadri, F., Nikkel, J., Quigley, J.D., Zhang, X.: Computing primitively-
rooted squares and runs in partial words. In: Kratochv´ıl, J., Miller, M., Froncek, D.
(eds.) IWOCA 2014. LNCS, vol. 8986, pp. 86–97. Springer, Cham (2015). doi:10.
1007/978-3-319-19315-1 8
8. Crochemore, M., Hancart, C., Lecroq, T.: Algorithms on Strings. Cambridge Uni-
versity Press, Cambridge (2007)
9. Crochemore, M., Iliopoulos, C.S., Kociumaka, T., Kubica, M., Langiu, A.,
Radoszewski, J., Rytter, W., Szreder, B., Wale´n, T.: A note on the longest com-
mon compatible preﬁx problem for partial words. J. Discrete Algorithms 34, 49–53
(2015)
10. Crochemore, M., Iliopoulos, C.S., Kubica, M., Radoszewski, J., Rytter, W., Wale´n,
T.: Extracting powers and periods in a word from its runs structure. Theor. Com-
put. Sci. 521, 29–41 (2014)

Eﬃcient Enumeration of Non-Equivalent Squares
111
11. Crochemore, M., Rytter, W.: Squares, cubes, and time-space eﬃcient string search-
ing. Algorithmica 13(5), 405–425 (1995)
12. Deza, A., Franek, F., Thierry, A.: How many double squares can a string contain?
Discrete Appl. Math. 180, 52–69 (2015)
13. Diaconu, A., Manea, F., Tiseanu, C.: Combinatorial queries and updates on
partial words. In: Kutylowski, M., Charatonik, W., G ebala, M. (eds.) FCT
2009. LNCS, vol. 5699, pp. 96–108. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-03409-1 10
14. Fraenkel, A.S., Simpson, J.: How many squares can a string contain? J. Comb.
Theory, Ser. A 82(1), 112–120 (1998)
15. Gusﬁeld, D., Stoye, J.: Linear time algorithms for ﬁnding and representing all the
tandem repeats in a string. J. Comput. Syst. Sci. 69(4), 525–546 (2004)
16. Halava, V., Harju, T., K¨arki, T.: On the number of squares in partial words. RAIRO
Theor. Inform. Appl. 44(1), 125–138 (2010)
17. Ilie, L.: A simple proof that a word of length n has at most 2n distinct squares. J.
Comb. Theory, Ser. A 112(1), 163–164 (2005)
18. Kociumaka, T.: Minimal suﬃx and rotation of a substring in optimal time. In:
Grossi, R., Lewenstein, M. (eds.) Combinatorial Pattern Matching, CPM 2016.
LIPIcs, vol. 54, pp. 28:1–28:12. Schloss Dagstuhl (2016)
19. Kolpakov, R.M., Kucherov, G.: Finding maximal repetitions in a word in linear
time. In: 40th Annual Symposium on Foundations of Computer Science, FOCS
1999, pp. 596–604. IEEE Computer Society (1999)
20. Manea, F., Merca¸s, R., Tiseanu, C.: An algorithmic toolbox for periodic partial
words. Discrete Appl. Math. 179, 174–192 (2014)
21. Manea, F., Tiseanu, C.: Hard counting problems for partial words. In: Dediu, A.-
H., Fernau, H., Mart´ın-Vide, C. (eds.) LATA 2010. LNCS, vol. 6031, pp. 426–438.
Springer, Heidelberg (2010). doi:10.1007/978-3-642-13089-2 36

The Approximability of the p-hub Center
Problem with Parameterized Triangle Inequality
Li-Hsuan Chen1(B), Sun-Yuan Hsieh1, Ling-Ju Hung1, and Ralf Klasing2
1 Department of Computer Science and Information Engineering,
National Cheng Kung University, Tainan 701, Taiwan
{clh100p,hunglc}@cs.ccu.edu.tw, hsiehsy@mail.ncku.edu.tw
2 CNRS, LaBRI, Universit´e de Bordeaux, 351 Cours de la Lib´eration,
33405 Talence Cedex, France
ralf.klasing@labri.fr
Abstract. A complete weighted graph G = (V, E, w) is called Δβ-
metric, for some β ≥1/2, if G satisﬁes the β-triangle inequality, i.e.,
w(u, v) ≤β · (w(u, x) + w(x, v)) for all vertices u, v, x ∈V . Given a
Δβ-metric graph G = (V, E, w) and an integer p, the Δβ-pHub Cen-
ter Problem (Δβ-pHCP) is to ﬁnd a spanning subgraph H∗of G such
that (i) vertices (hubs) in C∗⊂V form a clique of size p in H∗; (ii) ver-
tices (non-hubs) in V \C∗form an independent set in H∗; (iii) each non-
hub v ∈V \C∗is adjacent to exactly one hub in C∗; and (iv) the diam-
eter D(H∗) is minimized. For β = 1, Δβ-pHCP is NP-hard. (Chen et
al., CMCT 2016) proved that for any ε > 0, it is NP-hard to approx-
imate the Δβ-pHCP to within a ratio
4
3 −ε for β = 1. In the same
paper, a 5
3-approximation algorithm was given for Δβ-pHCP for β = 1.
In this paper, we study Δβ-pHCP for all β ≥
1
2. We show that for any
ε > 0, to approximate the Δβ-pHCP to a ratio g(β) −ε is NP-hard and
we give r(β)-approximation algorithms for the same problem where g(β)
and r(β) are functions of β. If β ≤
3−
√
3
2
, we have r(β) = g(β) = 1,
i.e., Δβ-pHCP is polynomial time solvable. If 3−
√
3
2
< β ≤
2
3, we have
r(β) = g(β) =
3β−2β2
3(1−β) . For 2
3 ≤β ≤
5+
√
5
10 , r(β) = g(β) = β + β2.
Moreover, for β ≥1, we have g(β) = β · 4β−1
3β−1 and r(β) = 2β, the approx-
imability of the problem (i.e., upper and lower bound) is linear in β.
Parts of this research were supported by the Ministry of Science and Technology of
Taiwan under grants MOST 105–2221–E–006–164–MY3, and MOST 103–2221–E–
006–135–MY3.
Li-Hsuan Chen is supported by the Ministry of Science and Technology of Taiwan
under grant MOST 106–2811–E–006–008.
Ling-Ju Hung is supported by the Ministry of Science and Technology of Taiwan
under grants MOST 105–2811–E–006–046.
Part of this work was done while Ralf Klasing was visiting the Department of Com-
puter Science and Information Engineering at National Cheng Kung University.
This study has been carried out in the frame of the “Investments for the future”
Programme IdEx Bordeaux - CPU (ANR-10-IDEX-03-02). Research supported by
the LaBRI under the “Projets ´emergents” program.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 112–123, 2017.
DOI: 10.1007/978-3-319-62389-4 10

The Approximability of the p-hub Center Problem
113
1
Introduction
The hub location problems have various applications in transportation and
telecommunication systems. Variants of hub location problems have been deﬁned
and well-studied in the literatures (see the two survey papers [1,15]). Suppose
that we have a set of demand nodes that want to communicate with each other
through some hubs in a network. A single allocation hub location problem requests
each demand node can only be served by exactly one hub. Conversely, if a demand
node can be served by several hubs, then this kind of hub location problem
is called multi-allocation. Classical hub location problems ask to minimize the
total cost of all origin-destination pairs (see e.g., [28]). However, minimizing
the total routing cost would lead to the result that the poorest service quality
is extremely bad. In this paper, we consider a single allocation hub location
problem with min-max criterion, called Δβ-p Hub Center Problem which is
diﬀerent from the classic hub location problems. The min-max criterion is able
to avoid the drawback of minimizing the total cost.
A complete weighted graph G = (V, E, w) is called Δβ-metric, for some
β ≥1/2, if the distance function w(·, ·) satisﬁes w(v, v) = 0, w(u, v) = w(v, u),
and the β-triangle inequality, i.e., w(u, v) ≤β ·(w(u, x)+w(x, v)) for all vertices
u, v, x ∈V . (If β > 1 then we speak about relaxed triangle inequality, and if
β < 1 we speak about sharpened triangle inequality.)
Lemma 1 ([8]). Let G = (V, E) be a Δβ-metric graph for 1
2 ≤β < 1. For any
two edges (u, x), (v, x) with a common endvertex x in G, w(u, x) ≤
β
1−β ·w(v, x).
Let u, v be two vertices in a graph H. Use dH(u, v) to denote the distance
between u, v in H. Deﬁne D(H) = maxu,v∈H dH(u, v) called the diameter of H.
We give the deﬁnition of the Δβ-pHub Center Problem as follows.
Δβ-p Hub Center Problem (Δβ-pHCP)
Input:
A Δβ-metric graph G = (V, E, w) and a positive integer p.
Output: A spanning subgraph H∗of G such that (i) vertices (hubs) in C∗⊂V
form a clique of size p in H∗; (ii) vertices (non-hubs) in V \C∗form
an independent set in H∗; (iii) each non-hub v ∈V \C∗is adjacent
to exactly one hub in C∗; and (iv) the diameter D(H∗) is minimized.
The Δβ-pHCP problem is a general version of the original p-Hub Center
Problem (pHCP) since the original problem assumes the input graph to be a
metric graph, i.e., β = 1. We use pHCP to denote the Δβ-pHCP for β = 1.
The pHCP is NP-hard in metric graphs [23]. Several approaches for PHCP
with linear and quadratic integer programming were proposed in the litera-
ture [14,19,23,25]. Many research eﬀorts for solving the pHCP are focused on
the development of heuristic algorithms, e.g., [13,26,27,30–32]. Chen et al. [16]
proved that for any ε > 0, it is NP-hard to approximate the pHCP to within
a ratio 4/3 −ε. In the same paper, a 5
3-approximation algorithm was given for
pHCP.

114
L.-H. Chen et al.
The Star p-Hub Center Problem (SpHCP) introduced in [29] is closely
related to pHCP and well-studied in [17,18,24]. The diﬀerence between the two
problems is that in SpHCP, the hubs are connected to a center rather than fully
connected.
If β
=
1, Δβ-pHCP is NP-hard and even NP-hard to have a ( 4
3 −ε)-
approximation algorithm for any ε > 0 [16]. In this paper, we investigate the com-
plexity of Δβ-pHCP parameterized by β-triangle inequality. The motivation of this
research for β < 1 is to investigate whether there exists a large subclass of input
instances of Δβ-pHCP that can be solved in polynomial time or admits polynomial-
time approximation algorithms with a reasonable approximation ratio. For β ≥1,
it is an interesting issue to see whether there exists a polynomial-time approxima-
tion algorithm with an approximation ratio linear in β.
Our study uses the well-known concept of stability of approximation for hard
optimization problems [9,11,21,22]. The idea of this concept is similar to that
of the stability of numerical algorithms. But instead of observing the size of
the change in the output value according to a small change of the input value,
one is interested in the size of the change of the approximation ratio accord-
ing to a small change in the speciﬁcation (some parameters, characteristics)
of the set of problem instances considered. If the change of the approximation
ratio is small for every small change in the set of problem instances, then the
algorithm is called stable. The concept of stability of approximation has been
successfully applied to several fundamental hard optimization problems. E.g. in
[2–4,8–10,12] it was shown that one can partition the set of all input instances
of the Traveling Salesman Problem into inﬁnitely many subclasses according to
the degree of violation of the triangle inequality, and for each subclass one can
guarantee upper and lower bounds on the approximation ratio. Similar studies
demonstrated that the β-triangle inequality can serve as a measure of hardness
of the input instances for other problems as well, in particular for the problem of
constructing 2-connected spanning subgraphs of a given complete edge-weighted
graph [5], and for the problem of ﬁnding, for a given positive integer k ≥2 and
an edge-weighted graph G, a minimum k-edge- or k-vertex-connected spanning
subgraph [6,7].
Table 1. The main results where Δβ-pHCP cannot be approximated within g(β) −ε
and has an r(β)-approximation algorithm.
β
Lower bound g(β) Upper bound r(β)
[ 1
2, 3−
√
3
2
]
1
1
( 3−
√
3
2
, 2
3]
3β−2β2
3(1−β)
3β−2β2
3(1−β)
[ 2
3, 5+
√
5
10 ]
β + β2
β + β2
[ 5+
√
5
10 , 3+
√
29
10
]
4β2+3β−1
5β−1
β + β2
[ 3+
√
29
10
, 1]
4β2+3β−1
5β−1
4β2+5β+1
5β+1
[1, ∞)
β · 4β−1
3β−1
2β

The Approximability of the p-hub Center Problem
115
In Table 1, we list the main results of this paper. We prove that for any ε > 0,
to approximate Δβ-pHCP to a ratio g(β) −ε is NP-hard where β >
3−
√
3
2
and
g(β) is a function of β. We give r(β)-approximation algorithms for Δβ-pHCP.
If β ≤
3−
√
3
2
, we have r(β) = g(β) = 1, i.e., Δβ-pHCP is polynomial time
solvable. If
3−
√
3
2
< β ≤
5+
√
5
10 , we have r(β) = g(β). For
5+
√
5
10
≤β ≤1,
r(β) = min{β + β2, 4β2+5β+1
5β+1
} and g(β) = 4β2+3β−1
5β−1
. Moreover, for β ≥1, we
have r(β) = 2β and g(β) = β · 4β−1
3β−1. For β ≥1, the approximability of the
problem (i.e., upper and lower bound) is linear in β.
We use CH to denote the set of hub vertices in solution H. Let H∗be an
optimal solution of Δβ-pHCP in a given β-metric graph G = (V, E, w). For a
non-hub x in H∗, we use f ∗(x) to denote the hub adjacent to x in H∗. We use
˜H to denote the best solution among all solutions in H where H is the collection
of all solutions satisfying that all non-hubs are adjacent to the same hub for
Δβ-pHCP in a given β-metric graph G = (V, E, w).
2
Inapproximability Results
We show that for β > 3−
√
3
2
, to approximate Δβ-pHCP to a factor g(β)−ε is NP-
hard where g(β) = 3β−2β2
3(1−β) for 3−
√
3
2
< β ≤2
3, g(β) = β + β2 for 2
3 ≤β ≤5+
√
5
10 ,
g(β) = 4β2+3β−1
5β−1
for 5+
√
5
10
≤β ≤1, and g(β) = β · 4β−1
3β−1 for β ≥1.
Theorem 1. Let β >
3−
√
3
2
. For any ε > 0, to approximate Δβ-pHCP to a
factor g(β) −ε is NP-hard where
(i) g(β) = 3β−2β2
3(1−β) if 3−
√
3
2
< β ≤2
3;
(ii) g(β) = β + β2 if 2
3 ≤β ≤5+
√
5
10 ;
(iii) g(β) = 4β2+3β−1
5β−1
if 5+
√
5
10
≤β ≤1;
(vi) g(β) = β · 4β−1
3β−1 if β ≥1.
Proof. Due to the limitation of space, we omit the proof of (ii)–(vi). In the
following, we will prove that, if Δβ-pHCP can be approximated to within a
factor 3β−2β2
3(1−β) −ε in polynomial time, for some ε > 0, then Set Cover can be
solved in polynomial time. This will complete the proof, since Set Cover is
well-known to be NP-hard [20].
Let (S, U) be an instance of Set Cover where U is the universal set, |U| = n,
and S = {S1, S2, . . . , Sm} is a collection of subsets of U, |S| = m. The goal is
to decide whether S has a subset S′ of size k such that 
Si∈S′ Si = U. In the
following, we construct a β-metric graph G = (V ∪S ∪{y}, E, w) according to
(S, U). For each element v ∈U, construct a vertex v ∈V , i.e., V = U. For each
set Si ∈S, construct a vertex si ∈S, |S| = |S|. We add a vertex y in G. The
edge cost of G is deﬁned in Table 2.

116
L.-H. Chen et al.
Table 2. The costs of edges (a, b) in G
w(a, b)
b ∈S
b ∈V
b = y
a ∈S
1
1 if b ∈a
β
1−β
2β otherwise
a ∈V
1 if a ∈b
2β
β
1−β
2β otherwise
Clearly, G can be constructed in polynomial time. It is easy to verify that G
is a β-metric graph. Let G be the input of Δβ-pHCP constructed according to
(S, U) where p = k + 1.
Let S′⊂S be a set cover of (S, U) of size k > 1. We then construct a solution
H of Δβ-pHCP according to S′. For each set Si ∈S′, collect the vertex si ∈S′
in G. Let all vertices in S′ ∪{y} be hubs where |S′| = |S′|. For each v ∈V ,
connect v to exactly one vertex si ∈S′ satisfying that v ∈Si. Since each v ∈V
is connected to a vertex si ∈S′ satisfying that v ∈Si, we see that w(v, si) = 1.
Hence D(H) = 3. Let H∗denote an optimal solution of Δβ-pHCP in G. We
have D(H∗) ≤3.
Assume that there exists a polynomial time algorithm that ﬁnds a solution H
of Δβ-pHCP in G with D(H) < 3β−2β2
1−β . W.l.o.g., assume that CH = S′ ∪V ′ ∪Y ′
where S′ ⊆S, V ′ ⊆V , and Y ′ ⊆{y}. For any non-hub v in H, use f(v) to denote
the hub in H adjacent to v. Note that if v is a hub in H, let f(v) = v. For u, v in
H, let dH(u, v) = w(u, f(u))+w(f(u), f(v))+w(v, f(v)) be the distance between
u and v in H.
Claim 1. The vertex y must be a hub.
Proof of Claim. Suppose that y is not a hub in H. There are two cases.
– If f(y) ∈S′, then all vertices v ∈V must adjacent to f(y) and satisfying
w(v, f(y)) = 1; otherwise there exists x ∈V with
dH(x, y) = dH(x, f(y)) + w(f(y), y)
≥2β +
β
1−β
(since 3−
√
3
2
< β ≤2
3)
= 3β−2β2
1−β ,
a contradiction to the assumption that D(H) < 3β−2β2
1−β . Thus the set in S
with respect to the vertex f(y) forms a set cover of (S, U). This contradicts
to the assumption that the optimal solution of Set Cover is of size k > 1.
– If f(y) ∈V ′, then there exists x ∈V \CH with
dH(x, y) = dH(x, f(y)) + w(f(y), y)
≥2β +
β
1−β
(since 3−
√
3
2
< β ≤2
3)
≥3β−2β2
1−β ,
a contradiction to the assumption that D(H) < 3β−2β2
1−β .

The Approximability of the p-hub Center Problem
117
Thus, y must be a hub, i.e., Y ′ = {y}.
■
Claim 2. The hub y is not adjacent to any non-hub in H.
Proof of Claim. Suppose that the hub y is adjacent to a non-hub z ∈(S∪V )\CH,
then there exists x ∈CH with
dH(x, z) = w(x, y) + w(y, z) ≥
β
1 −β +
β
1 −β ≥3β −2β2
1 −β
,
a contradiction to the assumption that D(H) < 3β−2β2
1−β .
Thus, y is not adjacent to any non-hub in H.
■
Claim 3. No v ∈V \V ′ is adjacent to any u ∈V ′.
Proof of Claim. Suppose that there exists v ∈V \V ′ is adjacent to u ∈V ′ in H.
We see that
dH(v, y) = w(v, u) + w(u, y) = 2β +
β
1 −β ≥3β −2β2
1 −β
,
a contradiction to the assumption that D(H) < 3β−2β2
1−β . Thus, no v ∈V \V ′ is
adjacent to any u ∈V ′.
■
According to Claims 1, 2, and 3, in H all vertices V \V ′ must be adjacent to
vertices in S′. If there exists v ∈V \V ′ satisfying that w(v, f(v)) = 2β, then
dH(v, y) = w(v, f(v)) + w(f(v), y) = 2β +
β
1 −β = 3β −2β2
1 −β
,
a contradiction to the assumption that D(H) < 3β−2β2
1−β . Thus, each v ∈V \V ′
satisﬁes that w(v, f(v)) = 1. We see that S′ = S′ forms a set cover of V \V ′. For
each u ∈V ′, pick a set Si ∈S satisfying u ∈Si, call the collection of sets S′′. It
is easy to see that |S′′| = |V ′| and S′ ∪S′′ forms a set cover of V = U of size at
most k. This shows that if Δβ-pHCP has a solution H with D(H) < 3β−2β2
1−β
that
can be found in polynomial time, then Set Cover can be solved in polynomial
time. However, Set Cover is a well-known NP-hard problem [20]. By the fact
that Set Cover is NP-hard and D(H∗) ≤3, this implies that for any ε > 0, to
approximate Δβ-pHCP to a factor 3β−2β2
3(1−β) −ε is NP-hard. This completes the
proof of (i).
⊓⊔
3
Polynomial-Time Algorithms
In this section, we show that for 1
2 ≤β ≤3−
√
3
2
, Δβ-pHCP can be solved in
polynomial time. Besides, we give polynomial-time approximation algorithms for
Δβ-pHCP for β > 3−
√
3
2
. For 3−
√
3
2
< β ≤5+
√
5
10 , our approximation algorithm
achieves the factor that closes the gap between the upper and lower bounds of
approximability for Δβ-pHCP.
Due to the limitation of space, we omit some proofs in this section.

118
L.-H. Chen et al.
Lemma 2. Let 1
2 ≤β < 1. Then the following statements hold.
(i) There exists a solution ˜H satisfying that all non-hubs are adjacent to the
same hub and D( ˜H) ≤max{1, min{ 3β−2β2
3(1−β) , β + β2}} · D(H∗).
(ii) There exists a polynomial-time algorithm to compute a solution H such that
D(H) = D( ˜H).
According to Lemma 2, we obtain the following results.
Lemma 3. Let 1
2 ≤β ≤3+
√
29
10
. Then the following statements hold.
1. If β ≤3−
√
3
2
, then Δβ-pHCP can be solved in polynomial time.
2. If 3−
√
3
2
< β ≤3+
√
29
10
, there is a min{ 3β−2β2
3(1−β) , β + β2}-approximation algo-
rithm for Δβ-pHCP.
Proof. Let H∗denote an optimal solution of the Δβ-pHCP problem. According
to Lemma 2, there is a polynomial-time algorithm for Δβ-pHCP to compute a
solution H such that D(H) ≤max{1, min{ 3β−2β2
3(1−β) , β + β2}} · D(H∗).
If β ≤3−
√
3
2
, then D(H) ≤max{1, min{ 3β−2β2
3(1−β) , β + β2}} · D(H∗) = D(H∗).
If 3−
√
3
2
< β ≤3+
√
29
10
, we see that
D(H) ≤max{1, min{3β −2β2
3(1 −β) , β + β2}} · D(H∗) = min{3β −2β2
3(1 −β) , β + β2} · D(H∗).
This completes the proof.
⊓⊔
Algorithm 1. Approximation algorithm for Δβ-pHCP (G, c).
(i) Run Algorithm APX1.
(ii) Run Algorithm APX2.
(iii) Return the best solution found by Algorithms APX1 and APX2.
Algorithm APX1
Guess the correct edge (y, z) where w(y, z) = ℓis the largest edge cost in an
optimal solution H∗with y as a hub and z as a non-hub. Let U := V and
c1 = y. Let H1 be the graph found by the following steps and C be the hub set
in H1. Initialize C = ∅.
(i) Let C := C ∪{c1}, and let U := U\{c1}.
(ii) For x ∈U, if w(c1, x) ≤ℓ, add an edge (x, c1) in H1 and let U := U\{x}.
(iii) While i = |C| + 1 ≤p and U ̸= ∅,
– choose v ∈U, let ci = v, connect ci to all other vertices in C, let U :=
U\{v}, and let C := C ∪{ci};
– for x ∈U, if w(x, ci) ≤2βℓ, then add edge (x, ci) in H1 and U := U\{x}.
(vi) If |C| < p and U = ∅, we arbitrarily select p −|C| non-hubs to be hubs and
connect all edges between hubs.

The Approximability of the p-hub Center Problem
119
Algorithm APX2
Guess all possible edges (y, z) to be a longest edge in H∗with one end vertex
as a hub and the other end vertex as a non-hub i.e., f ∗(z) = y and w(z, y) ≥
w(v, f ∗(v)) for all non-hubs v. Let H2 be the graph found by the following steps
and C′ be the hub set of H2.
(i) Connect y to all vertices in U.
(ii) Pick (p −1) vertices {v1, v2, . . . , vp−1} with largest distance to y from
U\{y, z}. Let C′ = {y, v1, v2, . . . , vp−1}.
(iii) Connect all pairs of vertices in C′.
It is not hard to see that Algorithm 1 runs in polynomial time. Let ℓbe the
largest edge cost in H∗with one end vertex as a hub and the other end vertex
as a non-hub. Note that both Algorithm APX1 and Algorithm APX2 guess all
possible edges (y, z) to be the longest edge in H∗with y as a hub and z as a
non-hub.
Lemma 4. Let H1 be the best solution returned by Algorithm APX1 and H∗be
the optimal solution. Then D(H1) ≤D(H∗) + 4βℓfor β ≤1.
Proof. Let H∗be an optimal solution of Δβ-pHCP and let f(u) be the hub
adjacent to vertex u in H1 and f(u) = u if u is a hub.
Removing edges with both end vertices in C∗= {s1, s2, . . . , sp} from H∗
obtains p components and each component is a star. Let S1, S2, . . . , Sp be the
p stars and si be the center of star Si for i = 1, 2, . . . , p. W.l.o.g., assume that
c1 = s1. Because for each v ∈V \C∗, w(v, f ∗(v)) ≤ℓ, we see that for u, v ∈Si,
dH∗(u, v) ≤2βℓ. Since the algorithm adds edges (v, c1) in H1 if w(v, c1) ≤ℓ, we
see that S1 ⊂NH[c1]\C. Notice that for each Sj, j ≥2, if there exists v ∈Sj
speciﬁed as ci ∈C, then all the other vertices in Sj are connected to one of
c1, c2, . . . , ci in H1. Moreover, for each ci, 1 < i ≤|C|, there exists Sj, 1 < j ≤p,
such that ci ∈Sj and Sj ∩C = {ci}. Notice that if there exists Sj, 1 < j ≤p,
Sj ∩C = ∅, then all vertices of Sj must be connected to one of vertices in C in
H1 and |C| ≤p. H1 is a feasible solution if |C| = p. Suppose that |C| < p and
the algorithm selects p−|C| vertices non-hubs to be hubs in Step (iv). Thus, H1
returned by Algorithm APX1 is a feasible solution.
We then show that D(H1) ≤D(H∗) + 4βℓ. For any two hubs u, v in H1,
dH1(u, v) = w(u, v) ≤D(H∗) if β ≤1. Notice that each non-hub v in H1 is
adjacent to a hub f(v) in H1 if w(v, f(v)) ≤2βℓ. Thus, for u, v in H1, dH1(u, v) ≤
D(H∗) + 4βℓand D(H1) ≤D(H∗) + 4βℓ. Since ℓis the largest edge cost in H∗
with y as a hub and z as a non-hub, the pairwise distances between non-hubs
which are connected to the same hub in H∗are at most 4βℓ. This completes the
proof.
⊓⊔
Lemma 5. Let H2 be the best solution returned by Algorithm APX2 and H∗
be the optimal solution. Then D(H2) ≤max{D(H∗), (1 + β) · (D(H∗) −
ℓ), 2β(D(H∗) −ℓ)}.

120
L.-H. Chen et al.
Proof. Let H∗be an optimal solution. For a non-hub v, use f ∗(v) to denote
the hub adjacent to v in H∗. For a hub v in H∗, let f ∗(v) = v. Notice that
Algorithm APX2 guesses all possible edges (y, z) to be a longest edge in H∗with
one end vertex as a hub and the other end vertex as a non-hub. In the following
we assume that w(y, z) = ℓis the largest edge cost in H∗with y as a hub and z
as a non-hub. We see that for any hub v in H2, dH2(v, y) = w(v, y) ≤D(H∗)−ℓ.
For two non-hubs u, v in H2, we have the following three cases.
– f ∗(u) = f ∗(v) = y, we see that dH2(u, v) = dH∗(u, v) ≤D(H∗).
– f ∗(u) = y and f ∗(v) ̸= y, we see that
dH2(u, v) = w(u, y)+w(v, y) ≤ℓ+β·dH∗(v, y) ≤ℓ+β·(D(H∗)−ℓ) ≤D(H∗).
– f ∗(u) ̸= y and f ∗(v) ̸= y, we see that
dH2(u, v) = w(u, y) + w(v, y) ≤β · dH∗(u, y) + β · dH∗(v, y) ≤2β(D(H∗) −ℓ).
For a non-hub u and a hub v in H2, there are two cases.
– If f ∗(u) = y, we see that
dH2(u, v) = w(u, y) + dH2(v, y) ≤ℓ+ D(H∗) −ℓ= D(H∗).
– If f ∗(u) ̸= y, we see that
dH2(u, v) = w(u, y) + dH2(v, y) ≤β · (D(H∗) −ℓ) + (D(H∗) −ℓ).
For two hubs u, v in H2, u ̸= y and v ̸= y, we see that dH2(u, v) = w(u, v) ≤
D(H∗).
Thus, D(H2) ≤max{D(H∗), (1 + β) · (D(H∗) −ℓ), 2β(D(H∗) −ℓ)}. This
completes the proof.
⊓⊔
Lemma 6. Let
3+
√
29
10
≤β ≤1. Then, there is a ( 4β2+5β+1
5β+1
)-approximation
algorithm for Δβ-pHCP.
Proof. Let H∗be an optimal solution of Δβ-pHCP. In this lemma, we show
that for 3+
√
29
10
≤β ≤1, Algorithm 1 returns a solution H such that D(H) ≤
( 4β2+5β+1
5β+1
) · D(H∗).
By Lemmas 4 and 5, we see that the approximation ratio of Algorithm 1 is
r(β) = min{ D(H1)
D(H∗), D(H2)
D(H∗)}.
Note that if
ℓ
D(H∗) ≥
β
1+β , then D(H2) = D(H∗). Assume that
ℓ
D(H∗) <
β
1+β ,
we see that D(H2) ≤D(H∗) −ℓ+ β(D(H∗) −ℓ).
The worst case approximation ratio of Algorithm 1 happens when D(H1) =
D(H2), i.e.,
D(H∗) + 4βℓ= max{(1 + β) · (D(H∗) −ℓ), 2β · (D(H∗) −ℓ)}.
Since β ≤1, we obtain that
ℓ
D(H∗) =
β
5β+1. Thus,
r(β) = min{ D(H1)
D(H∗), D(H2)
D(H∗)} ≤1 +
4β2
5β+1.
This completes the proof.
⊓⊔

The Approximability of the p-hub Center Problem
121
In Lemma 7, we prove that if β ≥1, Algorithm 1 is a 2β-approximation
algorithm for Δβ-pHCP.
Lemma 7. Let β ≥1. Then, there is a 2β-approximation algorithm for Δβ-
pHCP.
Proof. Since ℓ≥0, by Lemma 5, we have
D(H2) ≤max{D(H∗), (1 + β) · (D(H∗) −ℓ), 2β(D(H∗) −ℓ)} ≤2β · D(H∗).
This completes the proof.
⊓⊔
We close this section with the following theorem.
Theorem 2. Let β ≥
1
2. There exists a polynomial-time r(β)-approximation
algorithm for Δβ-pHCP where
(i) r(β) = 1 if β ≤3−
√
3
2
;
(ii) r(β) = 3β−2β2
3(1−β) if 3−
√
3
2
< β ≤5+
√
5
10 ;
(iii) r(β) = β + β2 if 5+
√
5
10
≤β ≤3+
√
29
10
;
(vi) r(β) = 4β2+5β+1
5β+1
if 3+
√
29
10
≤β ≤1;
(v) r(β) = 2β if β ≥1.
4
Conclusion
In this paper, we have studied Δβ-pHCP for all β ≥1
2. We showed that for any
ε > 0, to approximate Δβ-pHCP to a ratio g(β) −ε is NP-hard where g(β) =
3β−2β2
3(1−β) if 3−
√
3
2
< β ≤2
3; g(β) = β + β2 if 2
3 < β ≤5+
√
5
10 ; g(β) = 4β2+3β−1
5β−1
if
5+
√
5
10
< β ≤1; g(β) = β · 4β−1
3β−1 if β ≥1. Moreover, we gave r(β)-approximation
algorithms for the same problem. If β ≤3−
√
3
2
, we have r(β) = g(β) = 1, i.e.,
Δβ-pHCP is polynomial-time solvable for β ≤3−
√
3
2
. If 3−
√
3
2
< β ≤5+
√
5
10 , we
have r(β) = g(β). For 2
3 ≤β ≤1, r(β) = min{β + β2, 4β2+5β+1
5β+1
}. For β ≥1,
we have r(β) = 2β. In the future work, it is of interest to extend the range
of β for Δβ-pHCP such that the gap between the upper and lower bounds of
approximability can be reduced.
References
1. Alumur, S.A., Kara, B.Y.: Network hub location problems: the state of the art.
Eur. J. Oper. Res. 190, 1–21 (2008)
2. Andreae, T.: On the traveling salesman problem restricted to inputs satisfying a
relaxed triangle inequality. Networks 38, 59–67 (2001)
3. Andreae, T., Bandelt, H.-J.: Performance guarantees for approximation algorithms
depending on parameterized triangle inequalities. SIAM J. Discr. Math. 8, 1–16
(1995)

122
L.-H. Chen et al.
4. Bender, M.A., Chekuri, C.: Performance guarantees for the TSP with a parame-
terized triangle inequality. Inf. Process. Lett. 73, 17–21 (2000)
5. B¨ockenhauer, H.-J., Bongartz, D., Hromkoviˇc, J., Klasing, R., Proietti, G.,
Seibert, S., Unger, W.: On the Hardness of constructing minimal 2-connected
spanning subgraphs in complete graphs with sharpened triangle inequality. In:
Agrawal, M., Seth, A. (eds.) FSTTCS 2002. LNCS, vol. 2556, pp. 59–70. Springer,
Heidelberg (2002). doi:10.1007/3-540-36206-1 7
6. B¨ockenhauer, H.-J., Bongartz, D., Hromkoviˇc, J., Klasing, R., Proietti, G., Seibert,
S., Unger, W.: On k-edge-connectivity problems with sharpened triangle inequality.
In: Petreschi, R., Persiano, G., Silvestri, R. (eds.) CIAC 2003. LNCS, vol. 2653,
pp. 189–200. Springer, Heidelberg (2003). doi:10.1007/3-540-44849-7 24
7. B¨ockenhauer, H.-J., Bongartz, D., Hromkoviˇc, J., Klasing, R., Proietti, G., Seibert,
S., Unger, W.: On k-connectivity problems with sharpened triangle inequality. J.
Discr. Algorithms 6(4), 605–617 (2008)
8. B¨ockenhauer, H.-J., Hromkoviˇc, J., Klasing, R., Seibert, S., Unger, W.: Approx-
imation algorithms for the TSP with sharpened triangle inequality. Inf. Process.
Lett. 75, 133–138 (2000)
9. B¨ockenhauer, H.-J., Hromkoviˇc, J., Klasing, R., Seibert, S., Unger, W.: Towards
the notion of stability of approximation for hard optimization tasks and the trav-
eling salesman problem. In: Bongiovanni, G., Petreschi, R., Gambosi, G. (eds.)
CIAC 2000. LNCS, vol. 1767, pp. 72–86. Springer, Heidelberg (2000). doi:10.1007/
3-540-46521-9 7
10. B¨ockenhauer, H.-J., Hromkoviˇc, J., Klasing, R., Seibert, S., Unger, W.: An
improved lower bound on the approximability of metric TSP and approximation
algorithms for the TSP with sharpened triangle inequality. In: Reichel, H., Tison,
S. (eds.) STACS 2000. LNCS, vol. 1770, pp. 382–394. Springer, Heidelberg (2000).
doi:10.1007/3-540-46541-3 32
11. B¨ockenhauer, H.-J., Hromkoviˇc, J., Seibert, S.: Stability of approximation. In:
Gonzalez, T.F. (ed.) Handbook of Approximation Algorithms and Metaheuristics,
Chapman & Hall/CRC, Chap. 31 (2007)
12. B¨ockenhauer, H.-J., Seibert, S.: Improved lower bounds on the approximability of
the traveling salesman problem. RAIRO Theor. Inf. Appl. 34, 213–255 (2000)
13. Brimberg, J., Mladenovi´c, N., Todosijevi´c, R., Uroˇsevi´c, D.: General variable neigh-
borhood search for the uncapacitated single allocation p-hub center problem, Opti-
mization Letters. doi:10.1007/s11590-016-1004-x
14. Campbell, J.F.: Integer programming formulations of discrete hub location prob-
lems. Eur. J. Oper. Res. 72, 387–405 (1994)
15. Campbell, J.F., Ernst, A.T.: Hub location problems. In: Drezner, Z., Hamacher,
H.W. (eds.) Facility Location: Applications and Theory, pp. 373–407. Springer,
Berlin (2002)
16. Chen, L.-H., Cheng, D.-W., Hsieh, S.-Y., Hung, L.-J., Lee, C.-W., Wu, B.Y.:
Approximation algorithms for single allocation k-hub center problem. In: Pro-
ceedings of the 33rd Workshop on Combinatorial Mathematics and Computation
Theory (CMCT 2016), pp. 13–18 (2016)
17. Chen, L.-H., Hsieh, S.-Y., Hung, L.-J., Klasing, R., Lee, C.-W., Wu, B.Y.: On the
complexity of the star p-hub center problem with parameterized triangle inequality.
In: Fotakis, D., Pagourtzis, A., Paschos, V.T. (eds.) CIAC 2017. LNCS, vol. 10236,
pp. 152–163. Springer, Cham (2017). doi:10.1007/978-3-319-57586-5 14

The Approximability of the p-hub Center Problem
123
18. Chen, L.-H., Cheng, D.-W., Hsieh, S.-Y., Hung, L.-J., Lee, C.-W., Wu, B.Y.:
Approximation algorithms for the star k-hub center problem in metric graphs.
In: Dinh, T.N., Thai, M.T. (eds.) COCOON 2016. LNCS, vol. 9797, pp. 222–234.
Springer, Cham (2016). doi:10.1007/978-3-319-42634-1 18
19. Ernst, A.T., Hamacher, H., Jiang, H., Krishnamoorthy, M., Woeginger, G.: Unca-
pacitated single and multiple allocation p-hub center problem. Comput. Oper. Res.
36, 2230–2241 (2009)
20. Garey, M.R., Johnson, D.S.: Computers and Intractability: a guide to the theory
of NP-completeness. W. H. Freeman and Company, San Francisco (1979)
21. Hromkoviˇc, J.: Stability of approximation algorithms and the knapsack problem.
In: Karhum¨aki, J., Maurer, H., Paun, G., Rozenberg, G. (eds.) Jewels are Forever,
pp. 238–249. Springer, Heidelberg (1999)
22. Hromkoviˇc, J.: Algorithmics for Hard Problems - Introduction to Combinatorial
Optimization, Randomization, Approximation, and Heuristics, 2nd edn. Springer,
Heidelberg (2003)
23. Kara, B.Y., Tansel, B.C¸.: On the single-assignment p-hub center problem. Eur. J.
Oper. Res. 125, 648–655 (2000)
24. Liang, H.: The hardness and approximation of the star p-hub center problem. Oper.
Res. Lett. 41, 138–141 (2013)
25. Meyer, T., Ernst, A., Krishnamoorthy, M.: A 2-phase algorithm for solving the
single allocation p-hub center problem. Comput. Oper. Res. 36, 3143–3151 (2009)
26. Pamuk, F.S., Sepil, C.: A solution to the hub center problem via a single-relocation
algorithm with tabu search. IIE Trans. 33, 399–411 (2001)
27. Rabbani, M., Kazemi, S.M.: Solving uncapacitated multiple allocation p-hub center
problem by Dijkstra’s algorithm-based genetic algorithm and simulated annealing.
Int. J. Ind. Eng. Comput. 6, 405–418 (2015)
28. Todosijevi´c, R., Uroˇsevi´c, D., Mladenovi´c, N., Hanaﬁ, S.: A general variable neigh-
borhood search for solving the uncapacitated r-allocation p-hub median problem.
Optim. Lett. doi:10.1007/s11590-015-0867-6
29. Yaman, H., Elloumi, S.: Star p-hub center problem and star p-hub median problem
with bounded path lengths. Comput. Oper. Res. 39, 2725–2732 (2012)
30. Yang, K., Liu, Y., Yang, G.: An improved hybrid particle swarm optimization
algorithm for fuzzy p-hub center problem. Comput. Ind. Eng. 64, 133–142 (2013)
31. Yang, K., Liu, Y., Yang, G.: Solving fuzzy p-hub center problem by genetic algo-
rithm incorporating local search. Appl. Soft Comput. 13, 2624–2632 (2013)
32. Yang, K., Liu, Y., Yang, G.: Optimizing fuzzy p-hub center problem with general-
ized value-at-risk criterion. Appl. Math. Model. 38, 3987–4005 (2014)

Approximation Algorithms for the Maximum
Weight Internal Spanning Tree Problem
Zhi-Zhong Chen1(B), Guohui Lin2(B), Lusheng Wang3, Yong Chen2,
and Dan Wang3
1 Division of Information System Design, Tokyo Denki University,
Hatoyama, Saitama 350-0394, Japan
zzchen@mail.dendai.ac.jp
2 Department of Computing Science, University of Alberta,
Edmonton, AB T6G 2E8, Canada
{guohui,yong5}@ualberta.ca
3 Department of Computer Science, City University of Hong Kong,
Tat Chee Avenue, Kowloon, Hong Kong, China
cswangl@cityu.edu.hk
Abstract. Given a vertex-weighted connected graph G = (V, E), the
maximum weight internal spanning tree (MwIST for short) problem asks
for a spanning tree T of G such that the total weight of the internal
vertices in T is maximized. The unweighted variant, denoted as MIST, is
NP-hard and APX-hard, and the currently best approximation algorithm
has a proven performance ratio 13/17. The currently best approximation
algorithm for MwIST only has a performance ratio 1/3−ϵ, for any ϵ > 0.
In this paper, we present a simple algorithm based on a novel relationship
between MwIST and the maximum weight matching, and show that it
achieves a better approximation ratio of 1/2. When restricted to claw-
free graphs, a special case been previously studied, we design a 7/12-
approximation algorithm.
Keywords: Maximum weight internal spanning tree · Maximum weight
matching · Approximation algorithm · Performance analysis
1
Introduction
In the maximum weight internal spanning tree (MwIST for short) problem, we
are given a vertex-weighted connected graph G = (V, E), where each vertex v of
V has a nonnegative weight w(v), with the objective to compute a spanning tree
T of G such that the total weight of the internal vertices in T, denoted as w(T),
is maximized. MwIST has applications in the network design for cost-eﬃcient
communication [19] and water supply [1].
This work was supported by KAKENHI Japan Grant No. 24500023 (ZZC), NSERC
Canada (GL), GRF Hong Kong Grants CityU 114012 and CityU 123013 (LW),
NSFC Grant No. 61672323 (GL), CSC Grant No. 201508330054 (YC).
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 124–136, 2017.
DOI: 10.1007/978-3-319-62389-4 11

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
125
When the vertex weights are uniform, or simply vertex-unweighted, the
problem is referred to as the maximum internal spanning tree (MIST for short)
problem. MIST is clearly NP-hard because it includes the NP-hard Hamiltonian-
path [7] problem as a special case. Furthermore, MIST has been proven
APX-hard [13], suggesting that it does not admit a polynomial-time approxi-
mation scheme (PTAS). In the literature, much research is done on designing
(polynomial-time, if not speciﬁed) approximation algorithms for MIST to achieve
the worst-case performance ratio as close to 1 as possible.
The probably ﬁrst approximation for MIST is a local search algorithm,
which achieves a ratio of 1/2 and is due to Prieto and Sliper [15]. Salamon
and Wiener [19] later modiﬁed slightly Prieto and Sliper’s algorithm to make
it run faster (in linear-time) while achieving the same ratio of 1/2. Besides,
two special cases of MIST were considered by Salamon and Wiener [19]: when
restricted to claw-free graphs, they designed a 2/3-approximation algorithm;
when restricted to cubic graphs, they designed a 5/6-approximation algorithm.
Later, Salamon [18] proved that the 1/2-approximation algorithm in [19] actu-
ally achieves a performance ratio of 3/(r +1) for the MIST problem on r-regular
graphs (r ≥3). Based on local optimization, Salamon [17] presented an O(n4)-
time 4/7-approximation algorithm for MIST restricted to graphs without leaves.
The algorithm was subsequently simpliﬁed and re-analyzed by Knauer and
Spoerhase [8] to run faster (in cubic time), and it becomes the ﬁrst improved
3/5-approximation for the general MIST. Via a deeper local search strategy than
those in [8,17], Li et al. [9] presented a further improved approximation algo-
rithm for MIST with ratio 2/3. At the same time, Li and Zhu [13] presented
another 2/3-approximation algorithm for MIST.
Unlike the other previously known approximation algorithms for MIST, the
2/3-approximation by Li and Zhu [13] is based on a simple but crucial observa-
tion that the maximum number of internal vertices in a spanning tree of a graph
G can be upper bounded by the maximum number of edges in a triangle-free
2-matching (a.k.a. path-cycle cover) of G. The time complexity of this approx-
imation algorithm is dominated by computing the maximum triangle-free 2-
matching, O(nm1.5 log n), where n and m are the numbers of vertices and edges
in G, respectively. Li and Zhu [12] claimed that they are able to further improve
their design to achieve a 3/4-approximation algorithm for MIST, of the same
time complexity. Recently, Chen et al. [2] gave another 3/4-approximation algo-
rithm for MIST, which is simpler than the one in [12]; and they showed that
by applying three more new ideas, the algorithm can be reﬁned into a 13/17-
approximation algorithm for MIST of the same time complexity. This is currently
the best approximation algorithm for MIST.
The parameterized MIST by the number of internal vertices k, and its special
cases and variants, have also been extensively studied in the literature [1,4–
6,10,11,14–16]. The best known kernel for the general problem has a size 2k,
which leads to the fastest known algorithm with running time O(4knO(1)) [10].
For the vertex-weighted version, MwIST, Salamon [17] designed the ﬁrst
O(n4)-time 1/(2Δ −3)-approximation algorithm, based on local search, where
Δ is the maximum degree of a vertex in the input graph. For MwIST on

126
Z.-Z. Chen et al.
claw-free graphs without leaves, Salamon [17] also designed an O(n4)-time 1/2-
approximation algorithm. Subsequently, Knauer and Spoerhase [8] proposed the
ﬁrst constant-ratio 1/(3 + ϵ)-approximation algorithm for the general MwIST,
for any constant ϵ > 0. The algorithm is based on a new pseudo-polynomial time
local search algorithm, that starts with a depth-ﬁrst-search tree and applies six
rules to reach a local optimum. It yields a 1/3-approximation for MwIST and
then is extended to a polynomial time 1/(3 + ϵ)-approximation scheme. The
authors also showed that the ratio of 1/3 is asymptotically tight.
In this paper, we deal with the MwIST problem. We ﬁrst prove a novel
relationship between the total weight of the internal vertices in a spanning tree
of the given vertex-weighted graph and the maximum weight matching of an
edge-weighted graph, that is constructed out of the given vertex-weighted graph.
Based on this relationship, we present a simple 1/2-approximation algorithm for
MwIST; this ratio 1/2 signiﬁcantly improves upon the previous known ratio of
1/3. When restricted to claw-free graphs, a special case previously studied in
[17,19], we design a 7/12-approximation algorithm, improving the previous best
ratio of 1/2.
2
The 1/2-approximation Algorithm
Recall that in the MwIST problem, we are given a connected graph G = (V, E),
where each vertex v of V has a nonnegative weight w(v), with the objective
to compute a spanning tree T of G such that the total weight of the internal
vertices in T, denoted as w(T), is maximized. We note that for such an objective
function, we may assume without loss of generality that every leaf in the given
graph G has weight 0.
We construct an edge-weighted graph based on G = (V, E). In fact, the
structure of the new graph is identical to that of G: the vertex set is still V , but
instead the vertices have no weights; the edge set is still E, where the weight
of each edge e = {u, v} is w(e) = w(u) + w(v), i.e., the weight of an edge is
the total weight of its two ending vertices in the original graph. Since there is
no ambiguity when we discuss the edge weights or the vertex weights, the new
edge-weighted graph is still referred to as G. The weight of an edge subset refers
to the total weight of the edges therein; while the weight of an acyclic subgraph
refers to the total weight of the internal (and those surely will become internal)
vertices therein.
Let M ∗denote the maximum weight matching of (the edge-weighted graph)
G, which can be computed in O(n min{m log n, n2})-time, where n = |V | and
m = |E|.
Lemma 1. Given a spanning tree T of G, we can construct a matching M
of G such that w(T) ≤w(M).
Proof. We construct M iteratively. Firstly, we root the tree T at an internal
vertex r, and all the edges of T are unmarked; then in every iteration we include
into M an unmarked edge e = {u, v} of T such that (1) both u and v are internal

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
127
and (2) e is the closest to the root r measured by the number of edges on the
path from r to e, followed by marking all the edges incident at u or v. This way,
the total weight of the two internal vertices u and v in the tree T is transferred
to M by adding the edge e to M. At the time this iterative procedure stops,
there is no unmarked edge of T connecting two internal vertices, and thus every
internal vertex whose weight has not been transferred to M must be adjacent to
at least a leaf each via an unmarked edge.
Next, we iteratively include into M a remaining unmarked edge e = {u, v}
of T, followed by marking all the edges incident at u or v. This way, the total
weight of u and v, which is greater than or equal to the weight of the internal
vertex between u and v, is transferred to M by adding the edge e to M. At
the end of this procedure, T contains no more unmarked edges. Since leaves in
the tree T count nothing towards w(T), we conclude that w(T) ≤w(M). This
proves the lemma.
⊓⊔
The following corollary directly follows from Lemma 1, stating an upper
bound on the total weight of an optimal solution to the MwIST problem.
Corollary 1. Let T ∗denote an optimal (maximum weight internal) spanning
tree of G. Then, w(T ∗) ≤w(M ∗).
We next start with M ∗to construct a spanning tree T. Let the edges of M ∗
be e1, e2, . . . , ek; let ej = {aj, bj}, such that w(aj) ≥w(bj), for all j = 1, 2, . . . , k.
Note that there could be vertices of degree 0 in the spanning subgraph G[V, M ∗]
with the edge set M ∗, and there could be edges of weight 0 in M ∗; let X denote
the set of such degree-0 vertices and the end-vertices of such weight-0 edges.
Essentially we do not worry about the degree of any vertex of X in our ﬁnal
tree T, since their weights (if any) are not counted towards w(M ∗). This way,
we assume without loss of generality that w(aj) > 0 for each edge ej of M ∗, and
consequently the degree of aj is dG(aj) ≥2, that is, aj is adjacent to at least
one other vertex than bj in the graph G. Let A = {aj | j = 1, 2, . . . , k}, and
B = {bj | j = 1, 2, . . . , k}; note that V = A ∪B ∪X.
Let Eaa = E(A, A ∪X), i.e., the set of edges each connecting a vertex of A
and a vertex of A∪X, and Eab = E(A, B), i.e., the set of edges each connecting
a vertex of A and a vertex of B. Our construction algorithm ﬁrst computes a
maximal acyclic subgraph of G, denoted as H0, by adding a subset of edges of
Eaa to M ∗. This subset of edges is a maximum weight spanning forest on A∪X,
and it can be computed in O(|Eaa| log n)-time via a linear scan. In the achieved
subgraph H0, if one connected component C contains more than one edge, then
the vertex aj of each edge ej = {aj, bj} in C has degree at least 2, i.e. is internal.
Therefore, the total weight of the internal vertices in the component C is at least
half of w(C ∩M ∗), and C is called settled and left alone by the algorithm.
Our algorithm next considers an arbitrary edge of M ∗that is not yet in any
settled component, say ej = {aj, bj}. In other words, the edge ej is an isolated
component in the subgraph H0. This implies that the vertex aj is not incident to
any edge of Eaa, and thus it has to be adjacent to some vertex in B−{bj}. If aj is
adjacent to some vertex bi in a settled component, then this edge (aj, bi) is added

128
Z.-Z. Chen et al.
to the subgraph H0 (the edge ej is said merged into a settled component) and
the iteration ends. The updated component remains settled, as w(aj) ≥w(ej)/2
is saved towards the weight of the ﬁnal tree T.
In the other case, the vertex aj is adjacent to a vertex bi, such that the
edge ei = {ai, bi} is also an isolated component in the current subgraph. After
adding the edge (aj, bi) to the subgraph, the algorithm works with the vertex ai
exactly the same as with aj at the beginning. That is, if ai is adjacent to some
vertex bℓin a settled component, then this edge (ai, bℓ) is added to the subgraph
(the component that ai belongs to is merged into a settled component) and the
iteration ends; if ai is adjacent to a vertex bℓ, such that the edge eℓ= {aℓ, bℓ}
is also an isolated component in the current subgraph, then the edge (ai, bℓ) is
added to the subgraph, the algorithm works with the vertex aℓexactly the same
as with aj at the beginning; in the last case, ai is adjacent to a vertex bℓinside
the current component that ai belongs to, then the edge (ai, bℓ) is added to the
current component to create a cycle, subsequently the lightest edge of M ∗in
the cycle is removed, the iteration ends, and the current component becomes
settled. We note that in the above last case, the formed cycle in the current
component contains at least 2 edges of M ∗; breaking the cycle by removing the
lightest edge ensures that at least half of the total weight of the edges of M ∗in
this cycle (and thus in this component) is saved towards the weight of the ﬁnal
tree T. Therefore, when the iteration ends, the resulting component is settled.
When the second step of the algorithm terminates, there is no isolated edge
of M ∗in the current subgraph, denoted as H1, and each component is acyclic
and settled. In the last step, the algorithm connects the components of H1 into
a spanning tree using any possible edges of E. We denote the entire algorithm
as Approx.
Lemma 2. At the end of the second step of the algorithm Approx, every com-
ponent C of the achieved subgraph H1 is acyclic and settled (i.e., w(C) ≥
w(C ∩M ∗)/2).
Proof. Let C denote a component; C ∩M ∗is the subset of M ∗, each edge of
which has both end-vertices in C.
If C is obtained at the end of the ﬁrst step, then C is acyclic and for every edge
ej ∈C ∩M ∗, the vertex aj has degree at least 2, and thus w(C) ≥w(C ∩M ∗)/2.
If a subgraph of C is obtained at the end of the ﬁrst step but C is ﬁnalized
in the second step, then C is also acyclic and for every edge ej ∈C ∩M ∗, the
vertex aj has degree at least 2, and thus w(C) ≥w(C ∩M ∗)/2.
If C is newly formed and ﬁnalized in the second step, then at the time C was
formed, there was a cycle containing at least 2 edges of M ∗of which the lightest
one is removed to ensure the acyclicity, and thus the total weight of the internal
vertices on this path is at least half of the total weight of the edges of M ∗on
this cycle. Also, the vertex aj of every edge ej not on the cycle has degree at
least 2. Thus, w(C) ≥w(C ∩M ∗)/2.
⊓⊔
Theorem 1. Approx is a 1/2-approximation for the MwIST problem.

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
129
Proof. One clearly sees that Approx runs in polynomial time, and in fact the
running time is dominated by computing the maximum weight matching M ∗.
From Lemma 2, at the end of the second step of the algorithm Approx,
every component C of the achieved subgraph H1 is acyclic and satisﬁes w(C) ≥
w(C ∩M ∗)/2. Since there is no edge of M ∗connecting diﬀerent components
of the subgraph H1, the total weight of the internal vertices in H1 is already
at least w(M ∗)/2, i.e. w(H1) ≥w(M ∗)/2. The last step of the algorithm may
only increase the total weight. This proves that the total weight of the internal
vertices of the tree T produced by Approx is
w(T) ≥w(H1) ≥w(M ∗)/2 ≥w(T ∗)/2,
where the last inequality is by Corollary 1, which states that w(M ∗) is an upper
bound on the optimum. Thus, Approx is a 1/2-approximation for the MwIST
problem.
⊓⊔
3
A 7/12-approximation Algorithm for Claw-Free Graphs
We present a better approximation algorithm for the MwIST problem on claw-
free graphs. A graph G = (V, E) is called claw-free if, for every vertex, at least
two of its arbitrary three neighbors are adjacent. We again assume without loss
of generality that every leaf in the graph G has weight 0. Besides, we also assume
that |V | ≥5.
We ﬁrst present a reduction rule, which is a subcase of Operation 4 in [2],
that excludes certain induced subgraphs of the given graph G from consideration.
Each of these subgraphs is hanging at a cut-vertex of G (full details in [3]) and
can be dealt with separately from the other part of G.
Operation 1. If G has a cut-vertex v such that one connected component C of
G −v has two, three or four vertices, then obtain G1 from G −V (C) by adding
a new leaf u of weight 0 and a new edge {v, u}.
Let tw(C) denote the maximum total weight of the internal vertices in a
spanning tree of the subgraph induced on V (C) ∪v, in which w(v) is revised to
0. Then there is an optimal spanning tree T1 of G1 of weight w(T1) if and only
if there is an optimal spanning tree T of G of weight w(T) = w(T1) + tw(C).
Proof. Let Gc denote the subgraph induced on V (C)∪v, that is, Gc = G[V (C)∪
v]; and let Tc denote the spanning tree of Gc achieving the maximum total weight
of the internal vertices, that is, w(Tc) = tw(C) (Tc can be computed in O(1)-
time).
Note that in T1, the leaf u must be adjacent to v and thus w(v) is counted
towards w(T1). We can remove the edge {v, u} and u from T1 while attach the
tree Tc to T1 by collapsing the two copies of v. This way, we obtain a spanning
tree T of G, of weight w(T) = w(T1) + w(Tc) since w(v) is not counted towards
w(Tc).
Conversely, for any spanning tree T of G, the vertex v is internal due to
the existence of C. We may duplicate v and separate out a subtree Tc on the

130
Z.-Z. Chen et al.
set of vertices V (C) ∪v, in which the weight of v is revised to 0. This subtree
Tc is thus a spanning tree of Gc, and every vertex of V (C) is internal in T if
and only if it is internal in Tc. We attach the 0-weight vertex u to the vertex v
in the remainder tree via the edge {v, u}, which is denoted as T1 and becomes
a spanning tree of G1; note that the vertex v is internal in T1. It follows that
w(T) = w(Tc) + w(T1).
⊓⊔
Let M ∗denote the maximum weight matching of G. Let the edges of M ∗be
e1, e2, . . . , ek; let ej = {aj, bj}, such that w(aj) ≥w(bj), for all j = 1, 2, . . . , k.
For convenience, aj and bj are referred to as the head and the tail vertices of
the edge ej, respectively. The same as in the last section, we assume without
loss of generality that w(aj) > 0 for each j, and consequently the degree of aj
is dG(aj) ≥2, that is, aj is adjacent to at least one vertex other than bj in
the graph G. Let A = {aj | j = 1, 2, . . . , k}, B = {bj | j = 1, 2, . . . , k}, and
X = V −(A ∪B).
Let Eaa = E(A, A), i.e., the set of edges each connecting two vertices of A,
Eax = E(A, X), i.e., the set of edges each connecting a vertex of A and a vertex
of X, and Eab = E(A, B), i.e., the set of edges each connecting a vertex of A
and a vertex of B, respectively.
Let M aa ⊆Eaa be a maximum cardinality matching within the edge set Eaa.
We next prove a structure property of the spanning subgraph G[V, M ∗∪M aa],
which has the edge set M ∗∪M aa. For an edge ej = {aj, bj} of M ∗, if aj is not
incident to any edge of M aa, then ej is called isolated in G[V, M ∗∪M aa].
Lemma 3. Assume that two edges ej1 = {aj1, bj1} and ej2 = {aj2, bj2} of M ∗
are connected by the edge {aj1, aj2} ∈M aa in G[V, M ∗∪M aa]. Then there is at
most one isolated edge ej3 = {aj3, bj3} whose head aj3 can be adjacent to aj1 or
aj2.
Proof. By contradiction, assume that there are two isolated edges ej3 = {aj3, bj3}
and ej4 = {aj4, bj4} such that both the vertices aj3 and aj4 are adjacent to aj1
or aj2. Then from the maximum cardinality of M aa, aj3 and aj4 must be both
adjacent to aj1 or both adjacent to aj2. Suppose they are both adjacent to aj1;
from the claw-free property, at least two of aj2, aj3 and aj4 are adjacent, which
contradicts the maximum cardinality of M aa. This proves the lemma.
⊓⊔
For an isolated edge ej3 = {aj3, bj3} whose head is adjacent to an edge
{aj1, aj2} ∈M aa (i.e., satisfying Lemma 3), and assuming that {aj2, aj3} ∈Eaa,
we add the edge {aj2, aj3} to G[V, M ∗∪M aa]; consequently the edge ej3 is no
longer isolated. Let N aa denote the set of such added edges associated with M aa.
At the end, the achieved subgraph is denoted as H0 = G[V, M ∗∪M aa ∪N aa].
Lemma 4. In the subgraph H0 = G[V, M ∗∪M aa ∪N aa],
– every connected component containing more than one edge has either two
or three edges from M ∗, with their head vertices connected (by the edges of
M aa ∪N aa) into a path; it is called a type-I component (see Fig. 1a) and a
type-II component (see Fig. 1b), respectively;

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
131
Fig. 1. The conﬁgurations of a type-I component and a type-II component.
– for every isolated edge ej = {aj, bj}, the head vertex is incident with at least
one edge of Eax ∪Eab, but with no edge of Eaa.
Proof. The proof directly follows the deﬁnition of H0 and Lemma 3.
⊓⊔
The following lemma is analogous to Lemma 3.
Lemma 5. Any vertex of X can be adjacent to the head vertices of at most two
isolated edges in the subgraph H0 = G[V, M ∗∪M aa ∪N aa].
Proof. By contradiction, assume that x ∈X and there are three isolated edges
ejk = {ajk, bjk}, k = 1, 2, 3, in the subgraph H0 = G[V, M ∗∪M aa ∪N aa], such
that the edge {ajk, x} ∈Eax. From the claw-free property, at least two of aj1,
aj2 and aj3 are adjacent, which contradicts Lemma 4. This proves the lemma. ⊓⊔
For an isolated edge ej = {aj, bj} in the subgraph H0 = G[V, M ∗∪M aa∪N aa]
whose head is adjacent to a vertex x ∈X (i.e., satisfying Lemma 5), we add
the edge {aj, x} to H0; consequently the edge ej is no longer isolated. Let N ax
denote the set of such added edges associated with X. At the end, the achieved
subgraph is denoted as H1 = G[V, M ∗∪M aa ∪N aa ∪N ax].
Lemma 6. In the subgraph H1 = G[V, M ∗∪M aa ∪N aa ∪N ax],
– every connected component of H0 containing more than one edge remains
unchanged in H1;
– every connected component containing a vertex x of X and some other vertex
has either one or two edges from M ∗, with their head vertices connected (by
the edges of N ax) to the vertex x; it is called a type-III component (see Fig. 2a)
and a type-IV component (see Fig. 2b), respectively;
– for every isolated edge ej = {aj, bj}, the head vertex is incident with at least
one edge of Eab, but with no edge of Eaa ∪Eax.
Proof. The proof directly follows the deﬁnition of H1 and Lemmas 4 and 5.
⊓⊔
Let Eab
0
denote the subset of Eab, to include all the edges {aj, bℓ} where
both the edges ej = {aj, bj} and eℓ= {aℓ, bℓ} are isolated in the subgraph
H1 = G[V, M ∗∪M aa ∪N aa ∪N ax]. Let M ab ⊆Eab
0
be a maximum cardinality
matching within the edge set Eab
0 . Let H2 = G[V, M ∗∪M aa ∪N aa ∪N ax ∪M ab]
be the subgraph obtained from H1 by adding all the edges of M ab. One clearly

132
Z.-Z. Chen et al.
Fig. 2. The conﬁgurations of a type-III component and a type-IV component.
sees that all the isolated edges in the subgraph H1 are connected by the edges
of M ab into disjoint paths and cycles; while a path may contain any number
of isolated edges, a cycle contains at least two isolated edges. Such a path and
a cycle component are called a type-V component (see Fig. 3a) and a type-VI
component (see Fig. 3b), respectively.
Fig. 3. The conﬁgurations of a type-V component and a type-VI component.
Note that in a type-V component, there is exactly one head vertex of degree 1
and there is exactly one tail vertex of degree 1. We assume that for the tail vertex
in a type-V component, it is not adjacent to the head of any other edge (via an
edge of Eab) in the same component; otherwise, through an edge exchange, the
component is decomposed into a smaller type-V component and a new type-VI
component.
Lemma 7. In the subgraph H2 = G[V, M ∗∪M aa ∪N aa ∪N ax ∪M ab], for every
type-V component, the degree-1 head vertex is adjacent (via an edge of Eab) to
the tail vertex of an edge in a type-I, -II, -III, or -IV component; on the other
hand, the tail vertex of every edge in a type-I, -II, -III, or -IV component is
adjacent to at most one such head vertex.
Proof. We ﬁrst show that the degree-1 head vertex in a type-V component C,
denoted as aj, cannot be adjacent to the tail of any edge in another type-V or a
type-VI component C′. By contradiction, assume {aj, bℓ} ∈Eab and eℓis in C′.
If the tail bℓis already incident to some edge of M ab, say {ai, bℓ}, then by the
claw-free property at least two of ai, aj, aℓmust be adjacent, contradicting the
fact that they are all isolated in the subgraph H1. In the other case, the tail bℓ
is the tail vertex of C′ (which is a type-V component too), then it violates the
maximum cardinality of M ab since {aj, bℓ} ∈Eab can be added to increase the
size of M ab. This proves the ﬁrst half of the lemma.

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
133
The second half can be proven by a simple contradiction using the claw-free
property of the graph.
⊓⊔
Subsequently, every type-V component C is connected to a type-I, -II, -III,
or -IV component C′, via the edge between the degree-1 head vertex of C and
the tail vertex of an edge in C′ ∩M ∗. This way, the degree-1 tail vertex of C
takes up the role of “the tail vertex” of the edge in C′ ∩M ∗, to become a tail
vertex in the newly formed bigger component. For simplicity, the type of the
component C′ is passed to the newly formed bigger component. Denote this set
of newly added edges as N ab, which is a subset of Eab −M ab. The achieved
subgraph is denoted as H3 = G[V, M ∗∪M aa ∪N aa ∪N ax ∪M ab ∪N ab].
Lemma 8. In the subgraph H3 = G[V, M ∗∪M aa ∪N aa ∪N ax ∪M ab ∪N ab],
– there is no isolated type-V component;
– the head vertex of every edge of M ∗has degree at least 2.
Proof. The ﬁrst half of the lemma follows from Lemma 7; the second half holds
since there is no more isolated type-V component, which is the only type of
component containing a degree-1 head vertex.
⊓⊔
We next create a set F of edges that are used to interconnect the components
in the subgraph H3. F is initialized to be empty. By Lemma 8, for every type-I,
-II, -III, or -IV component C in the subgraph H3, of weight w(C ∩M ∗), it is a
tree and the total weight of the internal vertices therein is at least 1
2w(C ∩M ∗);
for every type-VI component C, which is a cycle, by deleting the lightest edge
of C ∩M ∗from C we obtain a path and the total weight of the internal vertices
in this path is also at least 1
2w(C ∩M ∗). In the next theorem, we show that
every component C in the subgraph H3 can be converted into a tree on the
same set of vertices, possibly with one edge speciﬁed for connecting a leaf of
this tree outwards, such that the total weight of the internal vertices (and the
leaf, if speciﬁed, which will become internal) in the tree is at least 2
3w(C ∩M ∗).
The speciﬁed edge for the interconnection purpose, is added to F. At the end of
the process, the component C is called settled. A settled component C can be
expressed in multiple equivalent ways, for example, that the total weight of the
internal vertices (and the leaf, if speciﬁed, which will become internal) in the
resulting tree is at least 2
3w(C ∩M ∗), or that the total weight of the internal
(and the leaf, if speciﬁed, which will become internal) vertices in the resulting
tree is at least twice the total weight of the leaves (excluding the speciﬁed leaf,
if any). We may use any of these ways for convenience of presentation.
In the sequel, we abuse the vertex notation to also denote its weight in math
formulae; this simpliﬁes the presentation and the meaning of the notation is
easily distinguishable. For estimating the total weight of the internal vertices in
a tree in the sequel, we frequently use the following inequality:
∀w1, w2, w3 ∈R, w1 + w2 + w3 −min{w1, w2, w3} ≥2 min{w1, w2, w3}.
The following theorem summarizes the ﬁve lemmas and their technical proofs
in [3] that every component in the subgraph H3 can be settled.

134
Z.-Z. Chen et al.
Theorem 2. Every component in the subgraph H3 can be settled.
Theorem 3. The
MwIST
problem
on
claw-free
graphs
admits
a
7/12-
approximation algorithm.
Proof. The above Theorem 2 states that every component of the subgraph H3 =
G[V, M ∗∪M aa ∪N aa ∪N ax ∪M ab ∪N ab] can be settled, without aﬀecting any
other components. Also, such a settling process for a component takes only linear
time, by scanning once the edges in the subgraph induced on the set of vertices
of the component. By settling, essentially the component is converted into a tree,
possibly with one edge of F speciﬁed for connecting a leaf of the tree outwards.
In the next step of the algorithm, it iteratively processes the heaviest compo-
nent C, i.e. with the largest w(C ∩M ∗). If the component C has been associated
with an edge e of F, and using the edge e to connect a leaf of the resulting tree
for C outwards does not create a cycle, then the algorithm does this and C is
processed. This guarantees that the total weight of the internal vertices in V (C)
is at least 2w(C ∩M ∗)/3. If using the edge e to connect a leaf of the resulting
tree for C outwards would create a cycle, the algorithm processes C by replacing
C with another tree that guarantees that the total weight of the internal vertices
in V (C) is at least 1
2w(C∩M ∗). Notice that the latter case happens only because
of (at least) one edge of F in an earlier iteration where a distinct component C′
was processed, which connects a vertex of C′ into a vertex of C. Therefore, every
such C is associated with a distinct component C′ processed by the algorithm
in an earlier iteration, and thus w(C′) ≥w(C). On the other hand, every such
component C′ is associated to one C only, due to its edge in F connecting a leaf
outwards into a vertex of C. It follows that for this pair of components C and
C′, the total weight of the internal vertices in V (C) ∪V (C′) is at least
w(C)/2 + 2w(C′)/3 ≥7(w(C) + w(C′))/12.
After all components of H3 are processed, we obtain a forest for which the total
weight of the internal vertices therein is at least 7w(M ∗)/12. The algorithm
lastly uses any other available edges of E to interconnect the forest into a ﬁnal
tree, denoted as T; clearly w(T) ≥7w(M ∗)/12.
The time for the interconnecting purpose is at most O(m log n). Therefore,
by Corollary 1 we have a 7/12-approximation algorithm for the MwIST problem
on claw-free graphs.
⊓⊔
4
Concluding Remarks
We presented a 1/2-approximation algorithm for the vertex weighted MIST prob-
lem, improving the previous best ratio of 1/(3 + ϵ). The key ingredient in the
design and analysis of our algorithm is a novel relationship between MwIST
and the maximum weight matching. When restricted to claw-free graphs, we
presented a 7/12-approximation algorithm, improving the previous best ratio of
1/2. It would be interesting to see whether this newly uncovered relationship,
possibly combined with other new ideas, can be explored further.

Approximation Algorithms for the Maximum Weight Internal Spanning Tree
135
The authors are grateful to two reviewers for their insightful comments and
changes that improve the presentation greatly.
References
1. Binkele-Raible, D., Fernau, H., Gaspers, S., Liedloﬀ, M.: Exact and parameterized
algorithms for max internal spanning tree. Algorithmica 65, 95–128 (2013)
2. Chen, Z.-Z., Harada, Y., Wang, L.: An approximation algorithm for maximum
internal spanning tree. CoRR, abs/1608.00196 (2016)
3. Chen, Z.-Z., Lin, G., Wang, L., Chen, Y., Wang, D.: Approximation algorithms for
the maximum weight internal spanning tree problem. arXiv, 1608.03299 (2016)
4. Coben, N., Fomin, F.V., Gutin, G., Kim, E.J., Saurabh, S., Yeo, A.: Algorithm for
ﬁnding k-vertex out-trees and its application to k-internal out-branching problem.
J. Comput. Syst. Sci. 76, 650–662 (2010)
5. Fomin, F.V., Gaspers, S., Saurabh, S., Thomasse, S.: A linear vertex kernel for
maximum internal spanning tree. J. Comput. Syst. Sci. 79, 1–6 (2013)
6. Fomin, F.V., Lokshtanov, D., Grandoni, F., Saurabh, S.: Sharp separation and
applications to exact and parameterized algorithms. Algorithmica 63, 692–706
(2012)
7. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-completeness. W. H. Freeman and Company, San Francisco (1979)
8. Knauer, M., Spoerhase, J.: Better approximation algorithms for the maximum
internal spanning tree problem. In: Dehne, F., Gavrilova, M., Sack, J.-R., T´oth,
C.D. (eds.) WADS 2009. LNCS, vol. 5664, pp. 459–470. Springer, Heidelberg
(2009). doi:10.1007/978-3-642-03367-4 40
9. Li, W., Chen, J., Wang, J.: Deeper local search for better approximation on
maximum internal spanning trees. In: Schulz, A.S., Wagner, D. (eds.) ESA
2014. LNCS, vol. 8737, pp. 642–653. Springer, Heidelberg (2014). doi:10.1007/
978-3-662-44777-2 53
10. Li, W., Wang, J., Chen, J., Cao, Y.: A 2k-vertex kernel for maximum internal
spanning tree. In: Dehne, F., Sack, J.-R., Stege, U. (eds.) WADS 2015. LNCS, vol.
9214, pp. 495–505. Springer, Cham (2015). doi:10.1007/978-3-319-21840-3 41
11. Li, X., Jiang, H., Feng, H.: Polynomial time for ﬁnding a spanning tree with max-
imum number of internal vertices on interval graphs. In: Zhu, D., Bereg, S. (eds.)
FAW 2016. LNCS, vol. 9711, pp. 92–101. Springer, Cham (2016). doi:10.1007/
978-3-319-39817-4 10
12. Li, X., Zhu, D.: A 4/3-approximation algorithm for the maximum internal spanning
tree problem. CoRR, abs/1409.3700 (2014)
13. Li, X., Zhu, D.: Approximating the maximum internal spanning tree problem via a
maximum path-cycle cover. In: Ahn, H.-K., Shin, C.-S. (eds.) ISAAC 2014. LNCS,
vol. 8889, pp. 467–478. Springer, Cham (2014). doi:10.1007/978-3-319-13075-0 37
14. Prieto, E.: Systematic kernelization in FPT algorithm design. Ph.D. thesis. The
University of Newcastle, Australia (2005)
15. Prieto, E., Sloper, C.: Either/or: using Vertex Cover structure in designing
FPT-algorithms — the case of k-Internal Spanning Tree. In: Dehne, F., Sack,
J.-R., Smid, M. (eds.) WADS 2003. LNCS, vol. 2748, pp. 474–483. Springer,
Heidelberg (2003). doi:10.1007/978-3-540-45078-8 41
16. Prieto, E., Sloper, C.: Reducing to independent set structure - the case of k-internal
spanning tree. Nordic J. Comput. 12, 308–318 (2005)

136
Z.-Z. Chen et al.
17. Salamon, G.: Approximating the maximum internal spanning tree problem. The-
oret. Comput. Sci. 410, 5273–5284 (2009)
18. Salamon, G.: Degree-based spanning tree optimization. Ph.D. thesis. Budapest
University of Technology and Economics, Hungary (2009)
19. Salamon, G., Wiener, G.: On ﬁnding spanning trees with few leaves. Inf. Process.
Lett. 105, 164–169 (2008)

Incentive Ratios of a Proportional Sharing
Mechanism in Resource Sharing
Zhou Chen1(B), Yukun Cheng2, Qi Qi1, and Xiang Yan3
1 The Hong Kong University of Science and Technology, Sai Kung, Hong Kong
zchenaq@connect.ust.hk, kaylaqi@ust.hk
2 Suzhou University of Science and Technology, Suzhou, China
ykcheng@amss.ac.cn
3 Shanghai Jiaotong University, Shanghai, China
xyansjtu@163.com
Abstract. In a resource sharing system, resources are shared among
multiple interconnected peers. Peers act as both suppliers and customers
of resources by making a certain amount of their resource directly avail-
able to others. In this paper we focus on a proportional sharing mecha-
nism, which is fair, eﬃcient and guarantees a market equilibrium in the
resource sharing system. We study the incentives an agent may manip-
ulate such a mechanism, by the vertex splitting strategy, for personal
gains and adopt a concept called incentive ratio to quantify the amount
of gains. For the resource sharing system where the underlying network
ia a cycle, we prove that the incentive ratio on this kind of network is
bounded by 2 ≤ζ ≤4. Furthermore, the incentive ratio on an even cycle,
a cycle with even number of vertices, is proved to be exactly 2.
Keywords: Mechanism design · Market equilibrium · Combinatorial
optimization · Incentive ratio · Resource sharing
1
Introduction
With the rapid growth of wireless and mobile Internet, the resource exchange or
sharing over networks becomes widely applied, which goes beyond the peer-to-
peer (P2P) bandwidth sharing idea [14]. Peers in networks act as both suppliers
and customers of resources (such as processing power, disk storage or network
bandwidth), and make their resources directly available to others according to
preset rules [12]. The problem we considered here can be modeled as an undi-
rected graph G, where each vertex v represents an agent with wv units of divisible
resources (or weight) to be distributed among its neighbors. The utility Uv is
determined by the total amount of resources obtained from its neighbors. The
resource exchange system can be viewed as a pure exchange economy, in which
each agent owns some divisible resource and trades its resource to its neigh-
bors in the network. An eﬃcient allocation in a pure exchange economy can
be characterized by the market equilibrium. However, the existence of a market
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 137–149, 2017.
DOI: 10.1007/978-3-319-62389-4 12

138
Z. Chen et al.
equilibrium does not imply that it can be achieved easily. Wu and Zhang [14]
have proposed the proportional response protocol (also called proportional shar-
ing mechanism) for this model, under which each peer contributes its resource
to each of its neighbors in proportion to the amount received from its neigh-
bors. They showed that this distributed proportional dynamics converged to a
proportional sharing equilibrium and surprisingly this equilibrium is equivalent
to a market equilibrium. Their results were obtained by a combinatorial proof,
based on the bottleneck decomposition method.
Wu and Zhang’s results implied that the proportional sharing mechanism
is fair and eﬃcient. But agents are strategic in reality. The market equilibrium
is determined by agents’ reported information, which may be misreported by
the agents for gaining more utility. It has been known that in the linear Fisher
market an agent may lie to improve its utility [1,4]. An immediate question is:
is it possible for an agent to improve its utility via strategic behaviors in our
setting? It is known that an agent can not beneﬁt from misreporting its weight or
neighborhood information in the proportional sharing mechanism [6,7]. In this
paper, we consider another commonly used strategic behavior where an agent v
splits itself into a set of nodes, such as taking new IP addresses in the network
environment. Each copied node is connected to some of its original neighbors
and is assigned a certain amount of resource (summing up to its original amount
of resource) to manipulate the proportional sharing mechanism. The new utility
of this agent is the sum of the utility from all its copied nodes. We call this
strategic behavior as vertex splitting.
Example 1. Consider a cycle G containing three vertices {x, y, v} as shown in
Fig. 1-(a). In the proportional sharing mechanism, agent v gets utility 10 from y.
In Fig. 1-(b), v splits itself into two copied nodes vl and vr, and assigns 5 units
of resource to each node respectively. Then the utilities of vl and vr are 2 and
12, respectively, which means the total utility of v increases to 14 after strategic
manipulation.
Fig. 1. An example that agent v can improve its utility by vertex splitting strategy. The
numbers in the circles represent weights and dashed arrows indicate resource exchange.
Example 1 shows that the proportional sharing mechanism is not incentive
compatible to the vertex splitting strategy. Thus there is a challenge that char-
acterize how much one can improve its utility. In this paper, we will quantify the
amount of improvement by a concept called incentive ratio [5]. Incentive ratio
is deﬁned as the ratio of the largest possible utility by manipulation over its
original utility. We prefer a smaller incentive ratio because it implies that the
mechanism is more robust to strategic behaviors.

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
139
Related Works. The voluntary cooperation of participating agents have been a
key to the successes of automated Internet application through information and
communication technology. [8] pioneered the study of incentive techniques for
peer-to-peer resource sharing systems in terms of mechanism design and perfor-
mance analysis. After that, [14] showed that a proportional response protocol is
equivalent to the market equilibrium in the resource exchange system and thus
considered to be a fair and eﬃcient solution. On the other hand, market equilib-
rium may not be truthful. Agents’ strategic behaviors are studied in the Fisher
market equilibrium for linear markets [1] and for constant elasticity of substitu-
tion markets [3]. Then the concept of incentive ratio [5] is introduced to measure
such strategic behaviors. The incentive ratio is similar to the ratio of price of
anarchy [10,13], the former one measures individual gains one may acquire in
deviation from truthful behavior, while the latter characterizes the loss of social
eﬃciency in a selﬁsh Nash equilibrium. [4,5] proved that the incentive ratio is
exactly 2 in a Fisher market with linear utilities and Leontief utilities, and 1.445
with Cobb-Douglas utilities. No constant incentive ratio is known except Fisher
market. The resource exchange model considered in this paper is a special case
of the Arrow-Debreu market. It is proved that the proportional sharing mech-
anism is truthful against two kinds of strategic behaviors: misreporting on its
connectivity or its own resource weight [6,7]. The vertex splitting behavior, stud-
ied in this paper, is motivated by the strategic behavior of false-name bidding,
introduced by [16] in the auction design. Agents can play this strategy by cre-
ating multiple ﬁctitious identities. It has been proved that the well known VCG
mechanism is not robust to the false-name bidding strategy, which is followed by
the study on the false-name-proof mechanism design [9,15], and on the eﬃciency
guarantee of the VCG mechanism [2].
Main Results. In this paper, we consider resource sharing on a network with
cycles only. A cycle network is the simplest graph that is still connected after an
agent playing vertex splitting strategy. Such connectivity poses a big challenge
for the analysis of incentive ratio, because each copied node’s utility depends on
the initial assigned weights of all copied nodes as well as the weights of all other
nodes. The particular interesting techniques we adopt here are to identify the
worst case of the splitting strategy (Proposition 6) and design auxiliary graphs in
the proof of general cycles (Lemma 2). Furthermore, edge classiﬁcation analysis
is introduced for cycles with even number of vertices. Compared with a recent
work [11] proving that the incentive ratio for the linear exchange economy is
unbounded, we show a bounded incentive ratio of 2 ≤ζ ≤4 on a general cycle
and tight bound of 2 on a cycle with even number of vertices, for a resource
sharing system. Our results ﬁll in the blank of bounded incentive ratios in Arrow-
Debreu markets.
2
Preliminary
The resource exchange system studied in this paper is modeled as an undirected
and weighted network G = (V, E; w), where each vertex v ∈V represents an

140
Z. Chen et al.
agent with an amount of resource (weight) wv > 0 exchanging with its neigh-
bors. W.l.o.g, we assume G is connected. Γ(v) = {u : (v, u) ∈E} is the set
of neighbors of v, and xvu denotes the amount of resource v allocates to its
neighbor u (0 ≤xvu ≤wv). We call a vector X = (xvu) a feasible allocation
if ∀v ∈V, 
u∈Γ (v) xvu = wv, i.e. v allocates all its resource out. The utility of
agent v is deﬁned as Uv(X) = 
u∈Γ (v) xuv, which is the total amount of resource
received from all its neighbors. Given a set S ⊆V , let w(S) = 
v∈S wv and
Γ(S) = ∪v∈SΓ(v). It is possible that S ∩Γ(S) ̸= ∅. Deﬁne α(S) = w(Γ (S))
w(S)
as an
inclusive expansion ratio of S, named by α-ratio. A set B is called a bottleneck
of G if α(B) = minS⊆V α(S), and a bottleneck with maximal size is called the
maximal bottleneck. The bottleneck decomposition of a graph G is deﬁned as
follows.
Bottleneck Decomposition. Given network G = (V, E; w), start with V1 = V ,
G1 = G and i = 1. Find the maximal bottleneck Bi of Gi and let Gi+1 be the
induced subgraph on the vertex set Vi+1 = Vi−(Bi ∪Ci), where Ci = Γ(Bi)∩Vi
is the neighbor set of Bi in Gi. Repeat until Gi+1 = ∅and set k = i where
Gi+1 = ∅. Finally, B = {(B1, C1), · · · , (Bk, Ck)} is the bottleneck decomposition
of G. (Bi, Ci) is the i-th bottleneck pair and αi = w(Ci)
w(Bi) is the α-ratio of (Bi, Ci).
Proposition 1 ([14]). Given a graph G, the bottleneck decomposition of G is
unique and
(1) 0 < α1 < α2 < · · · < αk ≤1;
(2) if αi = 1, then i = k and Bi = Ci; otherwise Bi is an independent set and
Bi ∩Ci = ∅;
(3) there is no edge between Bi and Bj, i ̸= j ∈{1, · · · , k};
(4) if there is an edge between Bi and Cj, then j ≤i.
B-class and C-class. Given B = {(B1, C1), · · · , (Bk, Ck)} of G, each vertex in
Bi (or Ci) with αi < 1 is called a B-class (or C-class) vertex. For the special
case Bk = Ck, i.e. αk = 1, vertices in Bk are both in B-class and C-class.
BD Mechanism. Given the bottleneck decomposition B of G, an allocation is
determined by the following algorithm [14], which is named as BD Mechanism.
• For (Bi, Ci) with αi < 1, consider the bipartite graph G = (Bi, Ci; Ei) with
Ei
= (Bi × Ci) ∩E. Construct a network N
= (VN, EN) with VN
=
{s, t} ∪Bi ∪Ci and directed edges (s, u) with capacity wu for u ∈Bi, (v, t)
with capacity wv
αi for v ∈Ci and (u, v) with capacity +∞for (u, v) ∈Ei.
The max-ﬂow min-cut theorem ensures a maximal ﬂow {fuv}, u ∈Bi and
v ∈Ci, such that 
v∈Γ (u)∩Ci fuv = wu and 
u∈Γ (v)∩Bi fuv = wv
αi . Let the
allocation be xuv = fuv and xvu = αifuv. Then we have 
u∈Γ (v)∩Bi xvu =

u∈Γ (v)∩Bi αi · fvu = wv.
• For αk = 1 (i.e., Bk = Ck), construct a bipartite graph G = (Bk, B′
k; E′
k)
where B′
k is a copy of Bk. There is an edge (u, v′) ∈E′
k if and only if (u, v) ∈

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
141
E[Bk]. Construct a network by the above method. For any edge (u, v′) ∈E′
k,
there exists ﬂow fuv′ such that 
v′∈Γ (u)∩B′
k fuv′ = wu. Let the allocation be
xuv = fuv′.
• For any other edge (u, v) ̸∈(Bi × Ci) ∩E, i = 1, 2, · · · , k, deﬁne xuv = 0.
BD Mechanism assigns all resource of each agent to its neighbors in the same
pair. Therefore, all available resources exchange along edges in (Bi ×Ci)∩E, i =
1, · · · , k. Next we show that BD mechanism is a proportional sharing mechanism.
Proportional Sharing. A mechanism is called proportional sharing if the allo-
cation from this mechanism satisﬁes xvu =
xuv

k∈Γ (v) xkv wv, i.e., each agent v’s
resource is allocated proportionally to what it receives from its neighbors.
Proposition 2 ([14]). BD Mechanism is a proportional sharing mechanism.
As stated before, the resource sharing system can be modeled as an exchange
economy where each agent u sells its own divisible resource and uses the money
earned through trading to buy its neighbors’ resource. A proportional sharing
equilibrium is a market equilibrium, which is considered as an eﬃcient allocation
in the exchange economy. Given a bottleneck decomposition, if a price vector P
is well deﬁned as: pu = αiwu if u ∈Bi; and pu = wu otherwise, then such a
price vector together with the allocation X obtained from BD Mechanism is a
market equilibirum.
Proposition 3 ([14]). (p, X) is a market equilibrium and each agent u’s utility
is Uu = wu · αi if u ∈Bi; and Uu = wu
αi if u ∈Ci.
Note that Uu ≥wu if u is in Ci and Uu ≤wu if u is in Bi because αi ≤1 by
Proposition 1-(1). Let αu be the α-ratio of u, i.e. αu = αi, if u ∈Bi ∪Ci.
Resource exchange game. Although BD Mechanism ensures a fair and eﬃ-
cient allocation as network mechanism, an agent may not follow BD Mechanism
at the execution level. Can agents play strategically to increase their utilities?
We call such a problem with incentive consideration as the resource exchange
game.
Recently, [6,7] studied two kinds of manipulative moves: cheating on its
resource amount and its connectivity. They proved that BD Mechanism is truth-
ful against these two strategies.
Proposition 4 ([7]). For any agent u in a resource exchange game, the utility
of u is continuous and monotonically nondecreasing on its reported value x. As
x ≤wu, the dominant strategy of agent u is to report its true weight.
Proposition 5 ([6]). For any agent u in a resource exchange game, the utility
of u cannot be increased by cutting any incident edges connecting to it.

142
Z. Chen et al.
In this paper, we consider another strategic move, called vertex splitting strat-
egy, where an agent splits itself into several copied nodes, and assigns a weight
to each copied node. Formally, in a resource sharing network G, the collection
w = (w1, w2, · · · , wn) ∈Rn is referred as the weight proﬁle, and w−v is the
weight proﬁle without v for any vertex v. Then the utility of agent v can be
written as Uv(G; w). After v splits itself into m nodes, 1 ≤m ≤dv (dv is the
degree of v), and assigns an amount of resource to each node respectively, its
new utility is denoted by U ′
v(G′; wv1, · · · , wvm, w−v). Here G′ is the resulting
graph and wvi ∈[0, wv] is the amount of resource assigned to node vi with the
constraint m
i=1 wvi = wv.
As shown in Example 1, BD mechanism is not robust against vertex splitting
strategy. To quantify the incentives an agent may manipulate the BD mechanism,
we will redeﬁne the incentive ratio, originally deﬁned by [5], for the resource
exchange game.
Deﬁnition 1 (Incentive Ratio). The incentive ratio of agent v under BD
Mechanism for the vertex splitting strategy is deﬁned as
ζv =
max
1≤m≤dv
max
wvi ∈[0,wv],m
i=1 wvi =wv;w−v;G′
U ′
v(G′; wv1, · · · , wvm, w−v)
Uv(G; wv)
.
The incentive ratio of BD mechanism in the resource exchange game is deﬁned
to be ζ = maxv∈V ζv.
Note that we only need to consider a special case for vertex splitting strategy
where agent v splits itself into dv nodes and each node is connected to one of
its neighbors. Proposition 6 below shows this special case will not reduce the
generality of the result. The proof will be provided in our full version.
Proposition 6. In a resource exchange game, the incentive ratio of BD mecha-
nism with respect to vertex splitting strategy can be achieved by splitting dv nodes
and each node is connected to one neighbor, where dv is the degree of agent v.
3
Incentive Ratio of BD Mechanism on Cycles
In this section, we study the incentive ratio of BD Mechanism with respect to
the vertex splitting strategy on cycles. Suppose the strategic vertex u splits itself
into two copied nodes ul and ur with weight assignment (wul, wur). Since the
underlying network is a cycle, the resulting graph G′ shall be a path after the
manipulation, which is still connected. Therefore, the utilities of copied nodes
depend on both of their weights, written as U ′
ul(wul, wur) and U ′
ur(wul, wur).
Let B′ = {(B′
1, C′
1), · · · , (B′
k′, C′
k′)} be the bottleneck decomposition of G′. The
α-ratio of pair (B′
i, C′
i) is denoted by α′
i, i = 1, · · · , k′.

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
143
Fig. 2. An example that shows the lower bound of incentive ratio on cycles, where the
gray or white vertices represent the B-class or C-class vertices respectively.
3.1
Incentive Ratio Bounds on Cycles
We ﬁrst use the following example (Example 2) with 8 vertices to show that the
lower bound of incentive ratio on cycles is 2.
Example 2. G is a cycle with eight vertices shown in Fig. 2-(a). Vertices’ weights
are wu = 1
α, wv1 = wv4 = M, wv2 = wv3 = wv5 = wv7 = αM/2 and wv6 = 1,
where α ∈(0, 1) is a preset value and M is a large enough number satisfying M >
1
α −α. The bottleneck decomposition of G is {(B1, C1)}, where B1 = {u, v1, v4}
and C1 = {v2, v3, v5, v6, v7} with α1 = α. The utility of u is Uu = wu · α1 = 1.
Now vertex u splits itself into two copied nodes with weights α and
1
α −α
respectively in Fig. 2-(b). The condition of M > 1
α −α promises that (B′
1, C′
1) =
{{u1, v1}, {v2, v7}}, and (B′
2, C′
2) = {{v4, v6}, {v3, v5, u2}}. The α-ratios are
α′
1 =
αM
M+( 1
α −α) and α′
2 = α. So limM→∞U ′
u1 = limM→∞(α −1
α) · α′
1 = 1 −α2,
U ′
u2 = α ·
1
α′
2 = 1 and limM→∞ζu = limM→∞U ′
u
Uu
=
limM→∞U ′
u1+U ′
u2
Uu
= 2 −α2. So
if α is fairly small, the incentive ratio is close to 2 as much as desired.
In the following, we discuss the upper bound of incentive ratio. As stated
before, the utilities of copied nodes depend on weights wul and wur simultane-
ously. This brings us a big challenge to compute U ′
ul and U ′
ur with two variables
of wul and wur, and it’s also hard to characterize the connections between U ′
ul
(or U ′
ur) and Uu. The vertex splitting strategy can be decomposed into two steps:
(1) Fix vertex u’s weight wu and add an additional node x with weight wur ∈
[0, wu], which is adjacent to u’s right neighbor v;
(2) Remove edge (v, u) from the network obtained in step 1, and decrease u’s
weight to wul = wu −wur.
Then the utilities of node x and vertex u are U ′
ur(wur, wul) and U ′
ul(wur, wul)
respectively. Based on the construction of the network in the ﬁrst step, we deﬁne
an auxiliary graph G as follows: given a vertex u and its neighbor v on a cycle
G, add a node x with weight x ∈[0, wu] adjacent to v. For easy of presentation,
we use x to represent the new node and its weight. So V = V (G) ∪{x} and
E = E(G) ∪(x, v). As utility Uu of u in G shall change with the weight of the
new node x, given the weights of all vertices, it can be described as a function
of x : Uu(x). Thus our proof is constructed based on three networks as shown in
Fig. 3: the original cycle G, the split network G′ and the auxiliary network G.

144
Z. Chen et al.
Fig. 3. The original cycle G, the split cycle G′ and the auxiliary cycle G.
Lemma 1. U ′
ul(wul, wur) ≤2Uu and U ′
ur(wul, wur) ≤2Uu.
Proof. Here we only give the proof for the result U ′
ul(wul, wur) ≤2Uu, the upper
bound of U ′
ur(wul, wur) can be deduced similarly. We focus on G′ and G ﬁrst.
If other vertices’ weights are ﬁxed and only the weight wul of ul increases up to
wu in G′, then the monotonicity of utility on weight by Proposition 4 promises
U ′
ul(wul, wur) ≤U ′
ul(wu, wur). From the structures of G′ and G, we observe that
G′ can be obtained by deleting the edge (u, v) from G and replacing u and x
with ul and ur respectively. So the incentive compatibility of BD Mechanism
for deleting edge strategy by Proposition 5 guarantees U ′
ul(wu, wur) ≤Uu(wur),
where x = wur. We can further prove ˆUu(x) ≤2Uu, ∀x ∈[0, wu] in Lemma 2
below (The proof is in Subsect. 3.2).
Lemma 2. In the auxiliary network G, the utility Uu(x) ≤2Uu, ∀x ∈[0, wu].
Combining the above two inequalities and Lemma 2, we have U ′
ul(wul, wur) ≤
U ′
ul(wu, wur) ≤Uu(wur) ≤2Uu.
□
Therefore, we have
Theorem 1. For the resource exchange system, the incentive ratio of BD Mech-
anism for the vertex splitting strategy on cycles is 2 ≤ζ ≤4.
Proof. From Lemma 1, we have U ′
ul(wul, wur) ≤2Uu and U ′
ur(wul, wur) ≤2Uu.
Thus it is easy to compute U ′
u = U ′
ur(wul, wur) + U ′
ul(wul, wur) ≤4Uu, which
means ζ ≤4.
□
3.2
Proof of Lemma 2
Lemma 2 characterizes the utility change of u caused by introducing another
node x with weight x ∈[0, wu]. To obtain Uu(x) ≤2Uu, ∀x ∈[0, wu], the
structure of G and the new bottleneck decomposition B are the keys.
Deﬁnition 2 (Bottleneck Decomposition).
Let the auxiliary graph be G
by adding one node x with weight x ∈[0, wu] adjacent to one of u’s neighbors
v. Denote its bottleneck decomposition as B = {( B1, C1), · · · , ( Bk, Ck)} with
α-ratio αj, j = 1, · · · , k. Similarly, V1 = V ∪{x}, Vj+1 = Vj −( Bj ∪Cj) for
j = 1, · · · , k−1. We call vertices in Bj (or Cj), 1 ≤j ≤k, B-class (or C-class).

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
145
Recall vertex v is the neighbor of u adjacent to x in G. Let ( Bs, Cs) be the
ﬁrst pair in B which diﬀers from the one in B. As the diﬀerence is due to the
appearance of x, x and v are in Bs ∪Cs. Before we give the formal proof for
Lemma 2, we propose a useful characterization of B below, its proof will be
provided in our full version.
Proposition 7. If v ∈Cj, 1 ≤j ≤k, then the bottleneck decomposition B
satisﬁes: there exist two indexes s, t ∈{1, 2, · · · , k} with s ≤t, such that
(1) ( B1, C1) = (B1, C1), · · · , ( Bs−1, Cs−1) = (Bs−1, Cs−1);
(2) ( Bt+1, Ct+1) = (Bj+1, Cj+1), · · · , ( Bk, Ck) = (Bk, Ck);
(3) t
h=s Bh =
j
h=s Bh

∪{x} and t
h=s Ch =
j
h=s Ch

;
(4) for each vertex y ∈Bh ∪Ch, h = s, · · · , t, αy ≤αy.
Proof of Lemma 2
• Case 1. v ∈Cj, 1 ≤j ≤k. Based on Proposition 7, we conclude:
(a) if u ∈Bh ∪Ch, h = 1, · · · , s−1 or h = j +1, · · · , k, then Proposition 7-(1)
and (2) guarantee that the pairs keep the same which leads to Uu(x) = Uu.
(b) if u ∈Bh, h = s, · · · , j, then u is still a B-class vertex by Proposition
7-(3). From Proposition 7-(4), we know that Uu(x) = wu · αu ≤wu ·αu = Uu.
(c) if u ∈Ch, h = s, · · · , j, then Uu ≥wu and u is still a C-class vertex by
Proposition 7-(3). Uu(x) = wu/αu ≥wu/αu = Uu. This result holds for each
vertex y ∈Ch, h = s, · · · , j. W.l.o.g, let Uy = Uy + δy, where δy ≥0. By the
allocation rule from BD Mechanism, we know all resources owned by vertices
in j
h=s Bh ∪{x} = t
h=s Bh are assigned to j
h=s Ch = t
h=s Ch, and vice
versa.
j

h=s
w(Bh) + x =
t

h=s
w( Bh) =
t

h=s

y∈
Ch
Uy =
j

h=s

y∈Ch
(Uy + δy) =
j

h=s
w(Bh) + δ
Then j
h=s

y∈Ch δy = δ = x and δy ≤x for each y ∈Ch, s ≤h ≤j.
Especially for vertex u ∈Ch, we have Uu(x) = Uu +δu ≤Uu +x ≤Uu +wu ≤
2Uu, where wu ≤Uu because u is a C-class vertex.
• Case 2. v ∈Bj, 1 ≤j ≤k. Since v is a B-class vertex, u must be in Ci with
index i ≤j by Proposition 1. Therefore, αu ≤αj and Uu ≥wu. So if u is a
B-class vertex in G, its utility Uu(x) ≤wu ≤Uu. In the following we focus on
the case that both u and x are C-class vertices. The proof of Uu(x) ≤2Uu when
x is a B-class vertex is left in the full version.
x is in C-class implies that v is in B-class because v is the unique neighbor
of x in G. Then ( Bh, Ch) = (Bh, Ch), h = 1, · · · , j −1 and Vj = Vj ∪{x}.
Since ( Bj, Cj −{x}) is a candidate pair in Vj (i.e., Cj −{x} = Γ( Bj) ∩Vj),
αj ≤w( 
Cj−{x})
w( 
Bj)
≤w( 
Cj)
w( 
Bj) = αj by the deﬁnition of bottleneck. So

146
Z. Chen et al.
(a) if u ∈Ci, 1 ≤i ≤j −1, then u must be in Ci with αu = αu and
Uu(x) = Uu.
(b) if u ∈Cj and u ∈Ch ̸= Cj (Note here we assume u is in C-class), then
h ≥j. Therefore, αu = αh ≥αj ≥αj = αu and Uu(x) = wu/αu ≤wu/αu =
Uu.
□
4
Incentive Ratio on Even Cycles
In this section, we focus on the incentive ratio of BD Mechanism on a cycle with
an even number of vertices, called even cycle for short.
Theorem 2. For the resource sharing system, the incentive ratio of BD Mech-
anism for the vertex splitting strategy on even cycles is exactly 2.
Example 2 tells us the lower bound of the incentive ratio on even cycles is 2.
To obtain the tight bound of 2, we shall prove that the upper bound is also 2.
Lemma 3. If the network G of resource exchange system is an even cycle and
G is the auxiliary graph of G, then Uu(x) ≤Uu, for any x ∈[0, wu].
To simplify our discussion, let the vertex set of even cycle G be {v1, · · · , v2n}.
Given the bottleneck decomposition B = {(B1, C1) · · · , (Bk, Ck)}, all vertices are
categorized as B-class or C-class. For the pair (Bi, Ci) with α1 < 1, we know the
Bi is independent by Proposition 1. But for (Bk, Ck) with αk = 1 (i.e. Bk = Ck),
we redeﬁne Bk and Ck as follows to obtain two disjoint vertex subsets: for vertex
vi in Bk = Ck with an odd index i, let vi be in Bk; otherwise let vi be in Ck.
So after such preprocessing, there is no edge between any two B-class vertices.
According to the classes that the endpoints u and v belong to, we categorize
each edge (u, v) as B-C edge or C-C edge. There is no B-B edge in G and
Proposition 8. On an even cycle G, the number of C-C edges is even.
Deﬁnition 3. Two vertices u and v are called B-C connected in G, if there
exists a path in G connecting them and all edges on this path are B-C edges.
Such a path is called a B-C connected path.
W.l.o.g. we assume each ( Bh, Ch) is connected, that means the edges from
( Bh × Ch)∩E construct a path and any two vertices in Bh ∪Ch are connected by
it. If not, ( Bh, Ch) is composed of several connected subpairs and each subpair’s
α-ratio is equal to αh. Therefore, we only need to consider connected subparis.
Proof of Lemma 3 (Sketch). Recall the proof in Lemma 2. We know Uu(x) ≤
Uu does not hold for two cases: (1) v ∈Cj and u ∈Ci ∩Cl, i = s, · · · , j; (2)
v ∈Bj, x is in B-class and u ∈Ci ∩Cl, i ≤j. So it is enough for us to show
ˆUu(x) ≤Uu for the above two cases on an even cycle.
As noted before, ( Bs, Cs) is the ﬁrst bottleneck pair in B which diﬀers from
B. We must have x, v ∈Bs ∪Cs. Suppose on the contrary that Uu(x) > Uu,

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
147
meaning ∃x ∈[0, wu], αl(x) = αu(x) < αu. Then we next show that G must
have an odd number of C-C edges, which is a contradiction to Proposition 8.
(1) v ∈Cj and u ∈Ci ∩Cl with assumption αl < αi. So (u, v) is a C-C
edge in G. Firstly, we start from the initial condition that u ∈Ci ∩Cl with
αl < αi, and prove that there exists another vertex u′ ∈Ci′ ∩Cl′ with αl′ < αi′,
l′ < l, and there is a B-C connected path P[u, u′] between u and u′. Secondly,
we continue to use the same analysis for u′ ∈Ci′ ∩Cl′ with αl′ < αi′. Because
in each step of analysis, we could always ﬁnd a pair ( Bl′, Cl′) whose index is
smaller than the previous one. The ﬁniteness of network makes us reach at pair
( Bs, Cs). For pair ( Bs, Cs), there exists a vertex u′′ ∈Ci′′ ∩Cs and there is a
B-C connected path P[u, u′′] by the previous analysis. On the other hand, we
know there is a path P[u′′, v] along the edges in ( Bs × Cs) ∩E connecting v
and u′′ based on our assumption. Proposition 7 shows that Bs or Cs only have
B-class or C-class vertices respectively. So path P[u′′, v], on which the vertices
in Bs and Cs alternate, is B-C connected. By merging P[u, u′′] and P[u′′, v], we
ﬁnd a B-C connected path between u and v. Since there are two paths between
any two vertices on a cycle, for u and v, there is a B-C connected path between
them and another one is a C-C edge (u, v). So the whole cycle G contains only
one C-C edge (u, v), contradicts to Proposition 8.
(2) v ∈Bj, x is in B-class and u ∈Ci ∩Cl, i ≤j, with αl < αi. So (u, v) is
a B-C edge. This case is more complicated since Proposition 7 no longer holds
and a B set may have some C-class vertices. Our objective is also to derive
a contradiction that even cycle G contains an odd number of C-C edges. The
following claim is essential to our analysis, its proof is provided in the full version.
Proposition 9. If Bh, s ≤h ≤l, has a C-class vertex a, then there is a path
P[v, a] from v to a which contains an even number of C-C edges.
Firstly we deal with pair ( Bl, Cl) where u is in. We discuss two cases that
whether Bl contains C-class vertices or not. If Bl has some C-class vertices (Case
A), we care for the special one, say a, such that the path P[u, a] along the edges
in ( Bl× Cl)∩E does not contain any other vertices both in C-class and Bl, except
for a. On one hand, we prove path P[u, a] only has one C-C edge. On the other
hand, Proposition 9 ensures there is a path P[a, v] containing an even number
of C-C edges. So by merging them, we get a path P[u, v] having an odd number
of C-C edges from u to v. In addition, cycle G is the union of path P[u, v] and
edge (u, v). Thus G has an odd number of C-C edges as (u, v) is a B-C edge. It
is a contradiction. If Bl dose not have C-class vertices (Case B), then Bl and Cl
only contain B-class and C-class vertices respectively. The analysis for (1) can
be utilized here. Thus there is a vertex u′ ∈Ci′ ∩Cl′ with αl′ < αi′, l′ < l and
u and u′ are B-C connected. Secondly, we continue to use the same analysis for
u′ and Case A or Case B may arise. Once Case A happens, a contradiction is
derived. Otherwise, we continue to study Case B until we reach pair ( Bs, Cs).
For pair ( Bs, Cs), there is a vertex u′′ ∈Ci′′ ∩Cs which is B-C connected to
u, as proved before. We also know there must be a path P[v, u′′] along edges in

148
Z. Chen et al.
( Bs × Cs) ∩E to connect v and u′′. The vertex b on P[v, u′′] adjacent v must be
in Bs and is a C-class vertex since v is in Cs and Bj. Thus Bs at least has one
C-class vertex and Case A appears, which derives a contradiction.
□
5
Conclusion
Our paper investigates the possible strategic behaviors of agents with respect
to a proportional sharing mechanism, i.e. BD Mechanism, in a resource sharing
system. We discuss the incentives of agents to play the vertex splitting strategy,
and characterize how much utility can be improved by incentive ratio. Our main
results are to prove that the incentive ratios are bounded by 2 ≤ζ ≤4 on general
cycles and are tight of 2 on even cycles. There is a space left that how to narrow
the gap of upper and lower bounds of incentive ratio on general cycles and further
how to explore the incentive ratio of BD Mechanism on any other graphs. We
have done a lot of numerical experiments on cycles containing diﬀerent number of
vertices, and on random graphs in which each edge is generated with probability
p ∈{0.1, 0.2, · · · , 0.9}. All simulation results demonstrate that the incentive
ratio is no more than 2 as shown in Fig. 4. Thus, two questions are raised from
our work: 1. Is it possible to prove the incentive ratio is 2 for any cycle? 2. Is it
possible to prove the incentive ratio is no more than 2 for any graph? These will
be interesting open problems for future work.
Fig. 4. The numerical experiment results.
Acknowledgments. This research was partially supported by the National Nature
Science Foundation of China (No. 11301475, 11426026, 61632017, 61173011), by a
Project 985 grant of Shanghai Jiao Tong University, and by the Research Grant Coun-
cil of Hong Kong (ECS Project No. 26200314, GRF Project No. 16213115 and GRF
Project No.16243516).

Incentive Ratios of a Proportional Sharing Mechanism in Resource Sharing
149
References
1. Adsul, B., Babu, C.S., Garg, J., Mehta, R., Sohoni, M.: Nash equilibria in
ﬁsher market. In: Kontogiannis, S., Koutsoupias, E., Spirakis, P.G. (eds.) SAGT
2010. LNCS, vol. 6386, pp. 30–41. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-16170-4 4
2. Alkalay-Houlihan, C., Vetta, A.: False-name bidding and economic eﬃciency in
combinatorial auctions. In: AAAI, pp. 538–544 (2014)
3. Brˆanzei, S., Chen, Y., Deng, X., Aris, F., Frederiksen, S., Zhang, J.: The ﬁsher
market game: equilibrium and welfare. In: AAAI (2014)
4. Chen, N., Deng, X., Zhang, H., Zhang, J.: Incentive ratios of ﬁsher mar-
kets. In: Czumaj, A., Mehlhorn, K., Pitts, A., Wattenhofer, R. (eds.) ICALP
2012. LNCS, vol. 7392, pp. 464–475. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-31585-5 42
5. Chen, N., Deng, X., Zhang, J.: How proﬁtable are strategic behaviors in a market?
In: Demetrescu, C., Halld´orsson, M.M. (eds.) ESA 2011. LNCS, vol. 6942, pp.
106–118. Springer, Heidelberg (2011). doi:10.1007/978-3-642-23719-5 10
6. Cheng, Y., Deng, X., Pi, Y., Yan, X.: Can bandwidth sharing be truthful? In:
Hoefer, M. (ed.) SAGT 2015. LNCS, vol. 9347, pp. 190–202. Springer, Heidelberg
(2015). doi:10.1007/978-3-662-48433-3 15
7. Cheng, Y., Deng, X., Qi, Q., Yan, X.: Truthfulness of a proportional sharing mech-
anism in resource exchange. In: IJCAI, pp. 187–193 (2016)
8. Feldman, M., Lai, K., Stoica, I., Chuang, J.: Robust incentive techniques for peer-
to-peer networks. In: EC (2004)
9. Iwasaki, A., Conitzer, V., Omori, Y., Sakurai, Y., Todo, T., Guo, M., Yokoo, M.:
Worst-case eﬃciency ratio in false-name-proof combinatorial auction mechanisms.
In: AAMAS, pp. 633–640 (2010)
10. Koutsoupias, E., Papadimitriou, C.: Worst-case equilibria. In: STACS, pp. 404–413
(1999)
11. Polak, I.: The incentive ratio in exchange economies. In: Chan, T.-H.H., Li, M.,
Wang, L. (eds.) COCOA 2016. LNCS, vol. 10043, pp. 685–692. Springer, Cham
(2016). doi:10.1007/978-3-319-48749-6 49
12. Schollmeier, R.: A deﬁnition of peer-to-peer networking for the classiﬁcation of
peer-to-peer architectures and applications. In: P2P, pp. 101–102 (2001)
13. Roughgarden, T., Tardos, ´E.: How bad is selﬁsh routing? J. ACM 49(2), 236–259
(2002)
14. Wu, F., Zhang, L.: Proportional response dynamics leads to market equilibrium.
In: STOC, pp. 354–363 (2007)
15. Yokoo, M.: The characterization of strategy/false-name proof combinatorial auc-
tion protocols: Price-oriented, rationing-free protocol. In: IJCAI, pp. 733–739
(2003)
16. Yokoo, M., Sakurai, Y., Matsubara, S.: Robust combinatorial auction protocol
against false-name bids. Artif. Intell. 130, 167–181 (2001)

Eﬃcient Enumeration of Maximal k-Degenerate
Subgraphs in a Chordal Graph
Alessio Conte1(B), Mamadou Moustapha Kant´e2, Yota Otachi3, Takeaki Uno4,
and Kunihiro Wasa4
1 Universit`a di Pisa, Pisa, Italy
conte@di.unipi.it
2 Universit´e Clermont Auvergne, LIMOS, CNRS, Aubi`ere, France
mamadou.kante@uca.fr
3 Kumamoto University, Kumamoto, Japan
otachi@cs.kumamoto-u.ac.jp
4 National Institute of Informatics, Tokyo, Japan
{uno,wasa}@nii.ac.jp
Abstract. In this paper, we consider the problem of listing the max-
imal k-degenerate induced subgraphs of a chordal graph, and propose
an output-sensitive algorithm using delay O(m · ω(G)) for any n-vertex
chordal graph with m edges, where ω(G) ≤n is the maximum size of a
clique in G. The problem generalizes that of enumerating maximal inde-
pendent sets and maximal induced forests, which correspond to respec-
tively 0-degenerate and 1-degenerate subgraphs.
1
Introduction
One of the fundamental problems in network analysis is ﬁnding subgraphs with
some desired properties. A great body of literature has been devoted to develop
eﬃcient algorithms for many diﬀerent types of subgraphs, such as frequent sub-
graphs [12], dense subgraphs [13] or complete subgraphs [5,9]. A more compre-
hensive list can be found in [20].
Dense subgraphs are object of extensive research, especially due to their close
relationship to community detection; however, one may be interested in ﬁnding
sparse graphs as many networks are sparse even if locally dense. For instance,
[21] addresses the enumeration of induced trees in k-degenerate graphs.
The degeneracy of a graph is the smallest value k for which every subgraph of
the graph has a vertex of degree at most k. A graph is said to be k-degenerate if
its degeneracy is k or less. Degeneracy is also referred to as the coloring number
or k-core number, as a k-degenerate graph may contain a k-core but not a k +1-
core, and is a widely used sparsity measure [5,9,15,19,21]. Several studies tend
to take into account the degeneracy of graphs, as it tends to be very small
M.M. Kant´e is supported by French Agency for Research under the GraphEN project
(ANR-15-CE-0009)
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 150–161, 2017.
DOI: 10.1007/978-3-319-62389-4 13

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
151
in real-world networks [19], many important graph classes in structural graph
theory are degenerate [15]. Furthermore, it is straightforward to see that k-
degenrate subgraphs generalize well known structures, as 0-degenerate subgraphs
correspond to independent sets, while 1-degenerate subgraphs correspond to
induced forests.
Alon et al. [1] investigated the size of the largest k-degenerate induced sub-
graph in a graph, giving tight lower bounds in relation to the degree sequence
of the graph. Whilst Pilipczuk et al. [16] showed that a maximum k-degenerate
induced subgraph can be found in randomized time O((2 −ϵk)nnO(1)), for some
ϵk > 0 depending only on k, and moreover showed that there are at most (2−ϵk)n
such subgraphs. See [2,14] for other recent studies on degeneracy.
In this paper we address the enumeration of maximal k-degenerate induced
subgraphs, and provide an eﬃcient polynomial delay algorithm for chordal input
graphs. An enumeration algorithm is of polynomial delay if the maximum com-
putation time between two outputs is bounded by a polynomial in the size of the
input. Enumeration algorithms are of high importance in several areas such as
data-mining, biology, artiﬁcial intelligence, or databases. (see for instance [7,20]).
Chordal graphs (also known as triangulated graphs) have been a topic of
intensive study in computer science due to the applications in phylogenetic net-
works and also many NP-complete problems become tractable when the inputs
are chordal graphs [3,8,11,17,18]. A graph is chordal if and only if every cycle
of length 4 or more has a chord, i.e. an edge joining two non-consecutive ver-
tices. Chordal graphs have been equivalently characterized in diﬀerent ways:
they are the graphs that allow a perfect elimination ordering, that is an elimina-
tion ordering in which every eliminated vertex is simplicial (its neighbors form
a clique) [17,18]; the graphs that allow a clique tree [3] (see Sect. 2.1); the inter-
section graphs of subtree families in trees [11]. In our case, we will consider the
characterization by clique-trees. It is well-known that n-vertex chordal graphs
have at most n maximal cliques. A clique-tree of a chordal graph G is a tree T
whose nodes are in bijection with the set of maximal cliques, and such that for
each vertex x the set of maximal cliques containing x form a subtree of T.
Our algorithm is based on the well-known Extension Problem (also known
as backtracking or ﬂashlight or binary partition) and uses the clique-tree. The
enumeration can be reduced to the following question: Given two sets of vertices
S and X, decide whether there is a maximal k-degenerate graph which contains S
and does not intersect X. Indeed, if we can answer this question in polynomial
time, the algorithm can be summarized as follows: start from the empty set,
and in each iteration with given sets (S, X) pick a vertex v and partition the
problem into those containing v (a call to the iteration (S ∪{v}, X)) or those
not containing v (a call to the iteration (S, X ∪{v}), both calls depending on the
answer given by the Extension problem. The delay of such algorithms is usually
O(n · poly(n)) with poly(n) being the time to decide the Extension problem.
This problem, however, can be shown to be NP-Complete for generic graphs (the
proof is omitted for space reasons), and even on chordal graphs its complexity
is not clear. We thus need some additional techniques: for our algorithm we do

152
A. Conte et al.
not consider all possible sets (S, X) for the Extension problem, but only some
special cases driven by the clique-tree. Our special case of the Extension problem
is the following (we consider the clique-tree T to be rooted):
Input. A node C of T, a partition (S, X) of the set of vertices in all the cliques
preceding C in a pre-order traversal of T and a partition (S′, X′) of C\(S∪X).
Output. Decide whether there is a maximal solution containing S ∪S′ and
avoiding X ∪X′.
We propose a notion of greedy solution and show that this special case of the
Extension problem is a Yes-instance if and only if a greedy solution exists; we
also propose an O(m)-time algorithm to compute the greedy solution.
2
Preliminaries
An algorithm is said to be output-polynomial if the running time is bounded
by a polynomial in the input and the output sizes. The delay is the maximum
computation time between two outputs, pre-processing, and post-processing. If
the delay is polynomial in the input size, the algorithm is called polynomial delay.
For two sets A and B we denote by A\B the set {x ∈A | x /∈B}. Our graph
terminology is standard, we refer to the book [6]. In this paper, we assume that
graphs are simple, ﬁnite, loopless, and each graph is given with a linear ordering
of its vertices. We can further assume graphs to be connected as the solutions
of a non-connected graph are obtained by combining those of its connected
components. We use n and m to denote respectively the numbers of vertices
and edges in any graph. The vertex set of a graph G is denoted by V (G) and its
edge set by E(G). The subgraph of G induced by X ⊆V (G), denoted by G[X],
is the graph (X, (X × X) ∩E(G)); and we write G \ X to denote G[V (G) \ X].
For a vertex x of G we denote by NG(x) the set of neighbors of x, i.e., the set
{y ∈V (G) | xy ∈E(G)}, and we let NG[x], the closed neighborhood of x, be
NG(x) ∪{x}; the degree of a vertex x, dG(x), is deﬁned as the size of NG(x).
A tree is an acyclic connected graph. A clique of a graph G is a subset C of
G that induces a complete graph, and a maximal clique is a clique C of G such
that C ∪{x} is not a clique for all x ∈V (G) \ C. Depending on the context, C
may refer to its set of vertices, the subgraph induced by C or the corresponding
node in the clique tree. We denote by Q(G) the set of maximal cliques of G, and
by ω(G) the maximum number of vertices in a clique in Q(G). For a vertex x,
we denote by Q(G, v) the set of maximal cliques containing x.
For a rooted tree T and two nodes u and v of T, we call v an ancestor of u,
and u a descendant of v, if v is on the unique path from the root to u; u and v
are incomparable if v is neither an ancestor or descendant of u. In what follows,
we omit the subscript G and ﬁx a graph G = (V (G), E(G)) if it is clear from
the context.

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
153
2.1
Chordal Graphs and Clique Trees
A graph G is a chordal graph if it does not contain an induced cycle of length
more than three. It is well-known that a chordal graph G has at most n maximal
cliques, and they can be enumerated in linear time [4]. With every chordal graph
G, one can associate a tree that we denote by QT (G), called clique tree, whose
nodes are the maximal cliques of G and such that for every vertex x ∈V (G) the
set Q(G, x) is a subtree of QT (G) [11]. Moreover, for every chordal graph G,
one can compute a clique tree in linear time (see for instance [10]). In the rest
of the paper all clique trees are considered rooted.
2.2
K-degenerate Graphs
A graph G is a k-degenerate graph if for any induced subgraph H in G, H has
a vertex whose degree is at most k. The degeneracy of a graph is the mini-
mum value k for which the graph is k-degenerate, and is a well known sparsity
measure [5,9,15,19,21]. We consider the following question.
Problem 1. Given a chordal graph G and a positive integer k, enumerate all
maximal k-degenerate induced subgraphs in G, with polynomial delay.
Note that a complete graph Kn is an (n −1)-degenerate graph, as all its
vertices have degree n −1. Therefore, for any clique C of a graph G, any k-
degenerate induced subgraph of G may have no more than k+1 vertices belonging
to C. Chordal graphs have the following property.
Theorem 1. The degeneracy of a chordal graph is exactly ω(G) −1.
Proof. Since the degeneracy is a hereditary property (i.e., any subgraph of a
k-degenrate graph is k-degenerate), and the complete graph Kn has degeneracy
n −1, ω(G) −1 is a lower bound for the degeneracy of any graph. The fact
that ω(G) −1 is an upper bound on chordal graphs relies on the fact that every
chordal graph has at least a vertex whose neighbour is a clique [17]. Therefore,
in any chordal graph we can ﬁnd a vertex of degree at most ω(G) −1.
⊓⊔
3
Enumeration Algorithm
This section describes our algorithm for enumerating all maximal k-degenerate
induced subgraphs of a given chordal graph G = (V, E). In the following, we
sometimes refer to maximal k-degenerate induced subgraphs as solutions, and
we denote them by their vertex set as for cliques, to ease the reading.
Our proposed algorithm is based on the binary partition method. The outline
of our algorithm is as follows: We start with an empty induced subgraph S.
Then we pick a vertex v from G and add v to S. If S + v is a maximal k-
degenerate subgraph, then we output S + v, otherwise we choose another vertex
and add it to S + v. After that we backtrack and add v to an excluded set X, to
generate all solutions that contain S and not v. By recursively applying the above
operation to G we can enumerate all solutions. However, certain pairs (S, X)

154
A. Conte et al.
may not generate a solution, as there may be no maximal k-degenerate induced
subgraph containing S but no vertex in X (e.g., if S = ∅, X = V ). If we test all
the possibilities that will not lead to a solution, the cost of this process is not
output sensitive, i.e. not bounded by a polynomial in the number of solutions. To
develop eﬃcient enumeration algorithm, we have to limit such redundant testing
as much as possible. To achieve this we focus on the rooted clique tree QT (G)
and introduce the concepts of greedy ﬁlling and partial solution. In what follows
we let G be a ﬁxed chordal graph.
3.1
Greedy Filling Strategy
Let R be a ﬁxed maximal clique, called the root of QT (G), and let us root QT (G)
at R. For a maximal clique C of G, whose parent in QT (G) is the clique P, we
call private vertices of C be the set of vertices in C \ P. Because all cliques in
QT (G) are diﬀerent and inclusion-maximal, and by the properties of the clique
tree, one can deduce the following.
Lemma 1. Given a clique tree QT (G), every clique in QT (G) contains at least
one private vertex, and every vertex v is private in exactly one clique in QT (G).
Let C be a maximal clique of G. For X ⊆V (G), let A(C, X) = 1 if |C \
X| ≥k + 1, and A(C, X) = 0 otherwise. For any vertex v ∈X, A(v, X) =

C∈Q(G,v) A(C, X), i.e. the number of maximal cliques containing v for which
|C \ X| ≥k + 1. As adding more than k + 1 vertices from the same clique to
any solution S would cause S to not be k-degenerate anymore, we say that C is
saturated in S if |C ∩S| = k + 1.
The function A allows us to check the maximality of a k-degenerate subgraph,
thanks to the following lemma.
Lemma 2. Let G = (V, E) be a chordal graph and M ⊆V be a k-degenerate
subgraph of G, with X = V \ M. M is maximal if and only if A(x, X) ≥1 for
each x ∈X.
Proof. Assume A(x, X) ≥1 for each x ∈X and there exists a k-degenerate
subgraph M ′ ⊃M, with v ∈M ′ \M. As v ∈X we have A(v, X) ≥1, thus there
exists a clique C containing v s.t. |C \ X| ≥k + 1. As M = V \ X, we have
|C \ X| = |C ∩M| ≥k + 1. As M ∪{v} ⊆M ′ we have |C ∩M ′| ≥k + 2, thus
M ′ contains a complete subgraph with k + 2 vertices and is not k-degenerate,
which contradicts the hypothesis.
On the other hand, if for a vertex x ∈X we have A(x, X) = 0, then for
any clique C containing x we have |(M ∪{x}) ∩C| ≤k + 1, since |C \ X| =
|C ∩M| < k + 1. Thus the largest clique in M ∪{x} has size at most k + 1, and
as M ∪{x} is a chordal graph (it is an induced subgraph of G) it is k-degenerate
by Theorem 1. Thus M is not maximal, which contradicts the hypothesis.
⊓⊔
We now deﬁne the notion of partial solution as a pair of disjoint vertex
subsets (S, X), where S contains vertices (to include) in the k-degenerate induced

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
155
subgraph, and X is a set of vertices that must be excluded from the solution,
with some additional properties:
Deﬁnition 1 (partial solution). A pair (S, X) of subsets of V (G) with S ∩
X = ∅is a partial solution if
1. |S ∩C| ≤k + 1 for any maximal clique C,
2. ∀x ∈X, A(x, X) ≥1,
3. for each maximal clique C, if Pv(C) ∩(S ∪X) ̸= ∅, then C′ ⊆S ∪X for all
ancestors C′ of C.
Given a pair (S, X) of disjoint subsets of V (G), it is not trivial to decide
whether there exists a solution M ⊇S with M ∩X = ∅. However, as we will later
demonstrate, this is always true if (S, X) is a partial solution. Next, we introduce
the strategy that will be used by our algorithm to guarantee the existence of
solutions. Let π : {1, . . . , |Q(G)|} →Q(G) be a ﬁxed linear ordering of Q(G)
obtained from a pre-order traversal of QT (G), and let us call π−1(C) the rank
of C ∈Q(G). We use the rank of the cliques to deﬁne the order in which they
are considered by the following procedure.
Deﬁnition 2 (Greedy ﬁlling). The greedy ﬁlling of a partial solution (S, X)
consists in the following. Let C be the maximal clique with the smallest rank for
which C \(S ∪X) ̸= ∅. Add vertices one by one from C to S until C is saturated
for S or C \ (S ∪X) = ∅. Then add the remaining vertices in C \ (S ∪X) to X,
if any, and repeat the process until no such clique C exists.
Finally, we can now show that a partial solution can always be extended into
a maximal one by means of a greedy ﬁlling.
Lemma 3. For any partial solution (S, X), the greedy ﬁlling yields a maximal
k-degenerate subgraph M of G such that S ⊆M and M ∩X = ∅.
Proof. Let M be the greedy ﬁlling of (S, X). By deﬁnition, S ⊆M and X ∩M = ∅.
Let XM = V \ M.
We prove the statement by showing that at all times during the greedy ﬁlling
(S, X) maintains the property of being a partial solution (see Deﬁnition 1), so
in the end we have A(x, XM) ≥1 for each x ∈XM, making M a maximal k-
degenerate subgraph by Lemma 2. Let Q be the maximal clique of the smallest
rank for which Q \ (S ∪X) ̸= ∅. Let (S′, X′) be the new pair constructed from
Q by the greedy ﬁlling, and let (SQ, XQ) be the partition of Q \ (S ∪X) such
that S′ = S ∪SQ and X′ = X ∪XQ. First notice that for all the ancestors Q′ of
Q we have Q′ \ (S ∪X) = ∅as their rank is smaller than the one of Q.
By deﬁnition of greedy ﬁlling, |Q∩S′| = |(Q∩S)∪SQ| ≤k+1. If XQ = ∅, then
X′ = X and A(x, X′) = A(x, X) ≥1 for each x ∈X′. Otherwise, by deﬁnition
of greedy ﬁlling, Q is saturated in S′ (|Q ∩S′| = k + 1). Hence A(Q, X′) = 1,
and for each x ∈Q A(x, X′) ≥1, while for each x ∈X′ \ Q = X \ Q A(x, X′) =
A(x, X) ≥1. Thus, (S′, X′) is a partial solution, which completes the proof. ⊓⊔

156
A. Conte et al.
3.2
Binary Partition Method
We are now ready to describe our algorithm kMIG(G, k), whose pseudo-code is
given in Algorithm 1.
The principle is to start from the partial solution S = ∅, X = ∅, where S
represent the vertices that will be in the solution, and X the vertices that are
excluded from the solution, and proceed with binary partition: in each recursive
call we consider a vertex v ∈Q, initially from the clique Q with the smallest
rank, i.e. the root of QT (G); we will ﬁrst add v to S and ﬁnd all the solutions
containing S ∪{v} and nothing in X; then add v to X and ﬁnd all the solutions
containing S and nothing in X ∪{v}, if any exists. At any step, we keep the
invariant that (S, X) is a partial solution: If we add v to S (Line 12), this is
equivalent to performing a step of the greedy ﬁlling, thus we know that (S ∪
{v}, X) is still a partial solution (see proof of Lemma 3). When, on the other
hand, we try to add v to X (Line 14), we only explore this road if there exists
a solution that contains all the vertices in S and no vertex in X ∪{v}. Thanks
to (S, X) being a partial solution we will be able to discover this eﬃciently,
and we will demonstrate (Lemma 4 in Sect. 3.3) that this is true if and only if
(S, X ∪{v}) is still a partial solution. Only once Q \ (S ∪X) is empty, we then
proceed to the clique Q′ next in the ranking (Lines 16–17). This guarantees that
Q is always the clique of smallest rank such that Q\(S∪X) ̸= ∅, thus condition 1
of Deﬁnition 1 still holds, and so (S, X) is still a partial solution. It is important
Algorithm 1. kMIG: Enumerating all maximal k-degenerate induced sub-
graphs in a chordal graph G = (V, E)
1 Procedure kMIG(G, k)
2
Compute QT (G) of G;
3
R ←the root clique of QT (G);
4
π : {1, . . . |Q(G)|} →Q(G) ←the pre-order traversal of QT (G);
5
Call SubkMIG(G, R, ∅, ∅, k);
6 Procedure SubkMIG(G, Q, S, X, k)
7
if V = S ∪X then
8
Output S
9
if Q \ (S ∪X) ̸= ∅then
10
v ←the smallest vertex in Q \ (S ∪X);
11
if |Q ∩S| < k + 1 then
12
SubkMIG(G, Q, S ∪{v} , X, k)
13
if there exists a solution S∗s.t. S ⊆S∗∧S∗∩(X ∪{v}) = ∅then
14
SubkMIG(G, Q, S, X ∪{v} , k)
15
else
16
Q′ ←π(π−1(Q) + 1);
17
SubkMIG(G, Q′, S, X, k)

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
157
to remark that, as all ancestors of Q are fully contained in S ∪X, and v ̸∈S ∪X,
then v is always a private vertex of Q, not contained in the ancestors of Q.
Finally, if S ∪X = V we can output S as a solution: by keeping the invariant
that (S, X) is a partial solution, we know by Lemma 2 that S is a maximal
k-degenerate induced subgraph of G.
3.3
Correctness
In this section we show the following theorem, that is the correctness of our
algorithm.
Theorem 2. Let G be a chordal graph and k be a non-negative integer. Then
kMIG(G, k) outputs all and only maximal k-degenerate induced subgraphs of G
without duplicates.
As mentioned in the description, kMIG(G, k) uses binary partition, thus every
recursive call has either a single child (Line 17) which will simply extend the
current solution, or will produce two recursive calls (Lines 12 and 14) that will
lead to diﬀerent solutions, as the ﬁrst one considers only solutions for which
v ∈S, and the second only solutions for which v ̸∈S (if any). Thus the same
solution cannot be found more than once.
Furthermore, as we keep the invariant that (S, X) is a partial solution, by
Lemma 2 we know that when V = S ∪X then S is a maximal k-degenerate
induced subgraph, thus kMIG(G, k) outputs only solutions.
Finally, any solution, i.e. maximal k-degenerate induced subgraph M is found
by the algorithm, and we can prove this by induction: consider the set of cliques
Q1, Q2, . . . in QT (G), ordered by ranking. As base condition assume that (S, X)
is a partial solution such that S ⊆M, X ∩M = ∅; this is always true in the
beginning, when (S = ∅, X = ∅). Let Qi be the clique that we are considering,
i.e. the one of smallest rank such that Qi \ (S ∪X) ̸= ∅, and v be the smallest
vertex in Qi \(S ∪X). If v ∈M, then the recursive call in Line 12 will consider a
partial solution which has one more vertex in common with M, i.e. (S ∪{v} , X).
Otherwise, v ̸∈M, that is, there exists a solution S∗such that S ⊆S∗and
S∗∩(X ∪{v}) = ∅, thus the recursive call in Line 14 is executed; this recursive
call will consider a partial solution that has one more vertex in common with
V \ M, i.e. (S, X ∪{v}). In both cases the base condition is still true, thus by
induction kMIG(G, k) will ﬁnd M. In order to prove Theorem 2, it only remains
to show how to decide whether, given (S, X), there is a solution containing S but
nothing in X ∪{v}, i.e., how to compute Line 13. This is shown in the following
lemma.
Lemma 4. Let (S, X) be any partial solution of G, Q be a clique such that its
ancestor cliques are fully contained in S ∪X, and v ̸∈S ∪X be a private vertex
of Q. Then, there exists a solution S∗such that S ⊆S∗and S∗∩(X ∪{v}) = ∅,
if and only if A(x, X ∪{v}) ≥1 for each vertex x ∈N[v] ∩(X ∪{v}).

158
A. Conte et al.
Proof. Let X′ = X ∪{v}. If for each vertex x ∈N[v] ∩X′, A(x, X′) ≥1, then
(S, X′) still satisﬁes all the properties in Deﬁnition 1, as A(w, X) is unchanged
for any vertex w ∈X \ N(v). Thus (S, X′) is a partial solution, and a solution
S∗is given by Lemma 3.
Otherwise, there is a vertex x ∈X′ such that A(x, X′) = 0, i.e., there is
no clique Q containing x such that |Q \ X′| ≥k + 1. As X′ ⊆V \ S∗for
any solution S∗disjoint from X′, there is no clique Q containing x such that
|Q \ (V \ S∗)| ≥k + 1, thus A(x, V \ S∗) = 0, and there is no maximal solution
S∗by Lemma 2.
⊓⊔
Thus Theorem 2 is true, and kMIG(G, k) ﬁnds all and only maximal k-
degenerate induced subgraphs of the chordal graph G exactly once.
4
Complexity Analysis
In this section we analyze the cost of our algorithm, and prove that it can
enumerate all maximal k-degenerate subgraphs of G in O(m · ω(G)) time per
solution. First, we recall some important properties of cliques in chordal graphs.
Remark 1 (From [3] and [10]). Let G be a connected chordal graph with n > 1
vertices and m edges. Then the number of maximal cliques in G is at most n−1,
and the sum of their sizes is 
C∈Q(G) |C| = O(m).
And regarding the cliques in G containing a speciﬁc node, we can state the
following.
Lemma 5. In a chordal graph G, the number of cliques containing a vertex v
is at most |N(v)|.
Proof. Consider G[N[v]], the subgraph of G induced by vertices of N[v]. G[N[v]]
is chordal as it is an induced subgraph of a chordal graph, it has |N[v]| vertices,
and at most |N[v]| −1 = |N(v)| maximal cliques, which exactly correspond to
the maximal cliques in G containing v.
⊓⊔
Now, consider the cost of executing Line 13, which dominates the cost of
each iteration of the algorithm. We show in the next lemma that it can be done
eﬃciently by exploiting Lemma 4. We recall that ω(G) denotes the maximum
size of a clique in G.
Lemma 6. Line 13 can be executed in time O(ω(G) · |N(v)|).
Proof. By Lemma 4 it is suﬃcient to check, for every vertex in x ∈N[v], whether
there must be a clique Q′ containing x such that |Q′ \ (X ∪{v})| ≥k + 1. As
(S, X) is a partial solution, if a vertex x is not contained in any clique such that
|Q′ \ (X ∪{v})| ≥k + 1, there exists a clique Q′ such that |Q′ \ X| ≥k + 1 >
|Q′ \ (X ∪{v})| = k, thus x is contained in one of the cliques containing v.

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
159
Assume we have a table that keeps track of the value B(Q) = |Q\(X ∪{v})|
for every clique Q, and one that keeps the value A(x) = |{Q | x ∈Q and B(Q) ≥
k +1}|. When adding v to X, we can update the B table by decrementing B(Q)
by 1 for every clique containing v. The number of such cliques in a chordal graph
is at most |N(v)| by Lemma 5. Every time the value of B(Q) is decremented
to less than k + 1, we can update the A table by decrementing A(x) by 1 for
each vertex x in Q. During this process, the check fails if and only if A(x) is
decremented to 0 for any x. The time required is |Q| ≤ω(G) for each considered
clique, for a total cost of O(ω(G) · |N(v)|).
⊓⊔
Finally, we are ready to prove the complexity bound for kMIG(G, k).
Theorem 3. kMIG(G, k) runs with delay O(m · ω(G)).
Proof. First, we need to compute QT (G), which takes O(n+m) time [10]. Note
that O(m + n) = O(m) as G is connected. Computing a pre-order traversal of
QT (G) takes O(n) time as QT (G) has at most n nodes.
In each recursive call we add a vertex either in S or in X or consider a next
maximal clique. Hence, the depth of the tree of recursive calls is bounded by 2n.
To bound the delay between two solutions M and M ′, it is enough to bound the
sum of the cost of all recursive calls in the path from the recursive call outputting
M to the one that outputs M ′. For clarity, let us use the term recursive node to
refer a node in the tree of the recursive calls. Note that the recursive nodes that
output a solution are exactly the leaves of this tree, thus the path between M
and M ′ is bounded by the cost of a root-to-leaf and a leaf-to-root path.
As to execute Line 13 we use tables A and B (see Lemma 6), let us explain
how to initialise them (we already explain in Lemma 6 how to update them). For
each vertex x, we set A(x) = |{Q ∈Q(G, x) | |Q| ≥k + 1}|, and set B(Q) = |Q|
for each Q ∈Q(G). In order to set these values we can simply iterate over all
maximal cliques in QT (G): initialising B(Q) takes O(1) time, and if |Q| ≥k + 1
we increment A(x) by 1 for each x ∈Q, which takes O(|Q|) time. The total
running time for initialising the tables A and B take thus O(n + m) = O(m)
time (see Remark 1).
Let v1, . . . vt be the recursive nodes in the path from the root to the node that
outputs M ′. First, t ≤2n as in each step either we add v to S or to X or we take
another Q. The delay now is the sum of the cost of each vi. Lines 9–14 can be
done in time O(|N(x)|·ω(G)) by Lemma 6. The cost for Lines 16–17 is O(1). By
summing, we have the upper bound 
Q∈Q(G) O(1)+
x∈V (G) O(|N(x)|·ω(G)) =
O(m · ω(G)). The O(m) preprocessing cost is negligible as there always exists at
least one solution.
⊓⊔
Note that this holds for any value of k: indeed, by Theorem 1 we know that
chordal graphs are ω(G) −1-degenerate, thus for any k ≥ω(G), the problem is
trivial as the only maximal solution is G itself.

160
A. Conte et al.
5
Conclusion
We presented the ﬁrst output-polynomial algorithm for enumerating maximal k-
degenerate induced subgraphs in a chordal graph. The algorithm runs in O(m ·
ω(G)) time per solution for any given k. It would be interesting for future work to
investigate the feasibility of an output-polynomial algorithm for general graphs.
It is worth noticing that the enumeration of maximal independent sets in graphs
is a special case as X is an independent set in G if and only if G[X] is 0-
degenerate.
References
1. Alon, N., Kahn, J., Seymour, P.D.: Large induced degenerate subgraphs. Graph.
Combin. 3(1), 203–211 (1987)
2. Bauer, R., Krug, M., Wagner, D.: Enumerating and generating labeled k-
degenerate graphs. In: 2010 Proceedings of the Seventh Workshop on Analytic
Algorithmics and Combinatorics, pp. 90–98. SIAM, Philadelphia (2010)
3. Blair, J.R., Peyton, B.: An introduction to chordal graphs and clique trees. In:
George, A., Gilbert, J.R., Liu, J.W.H. (eds.) Graph Theory and Sparse Matrix
Computation, pp. 1–29. Springer, New York (1993)
4. Chandran, L.S.: A linear time algorithm for enumerating all the minimum and
minimal separators of a chordal graph. In: Wang, J. (ed.) COCOON 2001. LNCS,
vol. 2108, pp. 308–317. Springer, Heidelberg (2001). doi:10.1007/3-540-44679-6 34
5. Conte, A., Grossi, R., Marino, A., Versari, L.: Sublinear-space bounded-delay enu-
meration for massive network analytics: Maximal cliques. In: ICALP (2016)
6. Diestel, R.: Graph Theory (Graduate Texts in Mathematics). Springer, New York
(2005)
7. Eiter, T., Makino, K., Gottlob, G.: Computational aspects of monotone dualiza-
tion: a brief survey. Discrete Appl. Math. 156(11), 2035–2049 (2008)
8. Enright, J., Kondrak, G.: The application of chordal graphs to inferring phylo-
genetic trees of languages. In: Fifth International Joint Conference on Natural
Language Processing, IJCNLP, pp. 545–552 (2011)
9. Eppstein, D., L¨oﬄer, M., Strash, D.: Listing all maximal cliques in sparse graphs
in near-optimal time. In: Cheong, O., Chwa, K.-Y., Park, K. (eds.) ISAAC
2010. LNCS, vol. 6506, pp. 403–414. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-17517-6 36
10. Galinier, P., Habib, M., Paul, C.: Chordal graphs and their clique graphs. In: Nagl,
M. (ed.) WG 1995. LNCS, vol. 1017, pp. 358–371. Springer, Heidelberg (1995).
doi:10.1007/3-540-60618-1 88
11. Gavril, F.: The intersection graphs of subtrees in trees are exactly the chordal
graphs. J. Comb. Theor. Ser. B 16(1), 47–56 (1974)
12. Kuramochi, M., Karypis, G.: Frequent subgraph discovery. In: Proceedings IEEE
International Conference on Data Mining, pp. 313–320. IEEE (2001)
13. Lee, V.E., Ruan, N., Jin, R., Aggarwal, C.: A survey of algorithms for dense sub-
graph discovery. In: Aggarwal, C.C., Wang, H. (eds.) Managing and Mining Graph
Data, pp. 303–336. Springer, Heidelberg (2010)
14. Lukot’ka, R., Maz´ak, J., Zhu, X.: Maximum 4-degenerate subgraph of a planar
graph. Electron. J. Comb. 22(1), P1–11 (2015)

Eﬃcient Enumeration of Maximal k-Degenerate Subgraphs
161
15. Neˇsetˇril, J., Ossona de Mendez, P.: Sparsity: Graphs, Structures, and Algorithms.
Algorithms and Combinatorics, vol. 28. Springer, Heidelberg (2012)
16. Pilipczuk, M., Pilipczuk, M.: Finding a maximum induced degenerate subgraph
faster than 2n. In: Thilikos, D.M., Woeginger, G.J. (eds.) IPEC 2012. LNCS, vol.
7535, pp. 3–12. Springer, Heidelberg (2012). doi:10.1007/978-3-642-33293-7 3
17. Rose, D.J., Tarjan, R.E., Lueker, G.S.: Algorithmic aspects of vertex elimination
on graphs. SIAM J. Comput. 5(2), 266–283 (1976)
18. Tarjan, R.E., Yannakakis, M.: Simple linear-time algorithms to test chordality of
graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs.
SIAM J. Comput. 13(3), 566–579 (1984)
19. Ugander, J., Karrer, B., Backstrom, L., Marlow, C.: The anatomy of the facebook
social graph. arXiv preprint arXiv: 1111.4503 (2011)
20. Wasa, K.: Enumeration of enumeration algorithms. arXiv preprint arXiv:1605.05102
(2016)
21. Wasa, K., Arimura, H., Uno, T.: Eﬃcient enumeration of induced subtrees in a
K-degenerate graph. In: Ahn, H.-K., Shin, C.-S. (eds.) ISAAC 2014. LNCS, vol.
8889, pp. 94–102. Springer, Cham (2014). doi:10.1007/978-3-319-13075-0 8

Reoptimization of Minimum Latency Problem
Wenkai Dai(B)
Saarland University, Saarbr¨ucken, Germany
wenkai.dai@gmail.com
Abstract. The minimum latency problem (MLP) is a prominent variant
of the traveling salesman problem, which is APX-hard and currently has
the best approximation ratio about 3.59 on metric graphs [14]. In this
paper, we consider several reoptimization variants of the metric MLP. In
a reoptimization problem, an optimal solution to an instance is given,
the goal is to obtain a good solution for a locally modiﬁed instance. Here,
we consider four common local modiﬁcations: adding (resp., removing)
a vertex and increasing (resp., decreasing) the cost of an edge. First, we
prove that these four reoptimization problems are NP-hard. Then, we
provide 7/3-approximation and 3-approximation algorithms for adding
and removing a vertex respectively. As for changing the cost of an edge
e∗, we study them by parameterizing the position of e∗. For increasing
the cost, our approximation ratios range from 2.1286 to 4/3 during e∗
moving from the ﬁrst edge of the given optimal tour to the last edge.
About decreasing the cost, we show that if the given optimal tour visits
e∗as the i-th edge then the problem is NP-hard and 2-approximable for
i ≥3, while it has a PTAS but not FPTAS for a constant i. However, if
e∗is not in the given optimal solution, the problem for decreasing the
edge cost is approximable by at least 2.1286 + O(1/n), where n is the
number of vertices in the given optimal solution. Moreover, we show that
relaxing the optimality of the given solution causes the approximability
of the problem to remove a vertex to be as hard as the metric MLP itself.
1
Introduction
The minimum latency problem (MLP) is a classic variant of the traveling sales-
man problem (TSP). Similar to TSP, it is not eﬃciently approximable in general
weighted graphs. Thus, we are only interested in its metric version. MLP seeks
for a tour starting from a ﬁxed vertex which minimizes the sum of distances
from each vertex to this starting vertex.
This problem is NP-hard even when the metric space is induced by a weighted
tree [23], or when the edge costs are restricted into 1 and 2 [10,21]. Although it
is also APX-hard, the former studies reveal it is much harder to be approximated
than TSP. At present, the best approximation ratio of MLP is about 3.59 by
Chaudhuri et al. [14], while the metric TSP is 1.5-approximable [15].
In this paper, we study reoptimization aspects of MLP. Reoptimization is a
framework in the context of NP-hard optimization problems: given an optimal
solution to an initial instance I′, then a local modiﬁcation turns I′ into a similar
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 162–174, 2017.
DOI: 10.1007/978-3-319-62389-4 14

Reoptimization of Minimum Latency Problem
163
instance I, the goal is to compute a solution to the modiﬁed instance I. Actually,
the scenario of reoptimization is motivated by real-world applications, e.g., a TSP
example in practice: the salesman has known an optimal tour on some cities after
working for some years, but on one day he is required to visit an extra city by
his company; instead of computing a new tour from scratch, he may prefer to
extend the former tour locally to integrate this new city.
The concept of reoptimization was ﬁrst mentioned in [22]. Since then, the
concept of reoptimization has been applied to various problems, including the
traveling salesman problem [1,3–5,20], the Steiner tree problem [6,9,17], the
knapsack problem [2], maximum weight induced heredity problems [12], covering
problems [8], scheduling problems [11] and the shortest common superstring
problem [7]. There are some surveys on reoptimization [13,16,18].
In this paper, we study the reoptimization of MLP for four typical local mod-
iﬁcations: adding (resp., removing) a vertex denoted by MLPV+ (resp., MLPV−)
and increasing (resp., decreasing) the cost of an edge denoted by MLPE+ (resp.,
MLPE−). Surprisingly, these four problems have not received much attention in
previous researches. To the best of our knowledge, only MLPV+ was given a 3-
approximation algorithm by Ausiello et al. [16]. Firstly, we show that these four
problems are NP-hard. By a general method in [18], the hardness of MLPV+,
MLPE+ and MLPE−can be easily established by polynomial-time Turing reduc-
tions; but for MLPV−, we give a Karp reduction instead since a trivial instance
extensible to a general instance of MLPV−is hard to be detected. For MLPV+,
we provide a 7/3-approximation algorithm to improve the 3-approximation result
by Ausiello et al. [16]. We also show that MLPV−is at least 3-approximable,
which outperforms computing from scratch for MLP. Furthermore, we provide a
new idea to study the reoptimization of altering the cost of an edge. By parame-
terizing the position of the modiﬁed edge e∗, we can exhibit a more ﬁne-grained
analysis of how the approximability of MLPE+ and MLPE−varies in Table 1.
In particular, for the MLPE−where the given optimal tour OPT ′ contains e∗as
the i-th edge, unlike reoptimizing TSP, OPT ′ cannot remain optimal anymore in
the modiﬁed instance, while this problem is NP-hard and has a PTAS, but no
FPTAS unless P=NP, for every constant i ≥3.
Goyal and M¨omke [17] proposed a diﬀerent reoptimization model which slacks
the optimality of the given solution. They were concerned with how the approx-
imability of reoptimization varies when the quality of the given solution declines.
Following their way, we prove that MLPV−cannot be approximated better than
MLP itself if the given solution of the initial instance is not optimal anymore.
2
Preliminaries
For a graph G, let V (G) and E(G) denote the set of vertices and the set of edges
of G respectively. A Hamiltonian path (cycle also called tour) of a graph is a path
(cycle) that visits each vertex exactly once. A complete, edge-weighted graph
G = (V, E, c), where c : E →R≥0, is called metric if the triangle inequality:
c({a, b}) ≤c({a, c}) + c({b, c}) is satisﬁed for all vertices a, b, c ∈V .

164
W. Dai
Table 1. Approximation results about MLPE+ and MLPE−, where E(OPT ′) denotes
the set of edges in the given optimal solution OPT ′, the parameter i indicates that e∗
is the i-th edge in OPT ′ for 1 ≤i ≤n −1, and n is the number of vertices of OPT ′.
Problem
Approximation results
Ref.
MLPE+
i = 1
i ̸= 1, n −1
i = n −1
e∗∈E(OPT ′) 2.1286
2
4/3
Theorem 5
e∗/∈E(OPT ′)
OPT ′ remains optimal after increasing
MLPE−
i = 1, 2 constant i
i ≥3
e∗∈E(OPT ′) P
PTAS, no FPTAS 2
Theorems 6–8
e∗/∈E(OPT ′) 2.1286 + O(1/n)
Theorem 9
The minimum latency problem (MLP) has an input instance (G, s), where
G = (V, E, c) is a metric graph with V = {v1, . . . , vn} and a ﬁxed vertex s ∈V
which is called source. Let a tour P = (v1, . . . , vn) starting at v1 be a subgraph
(path) in G consisting of n vertices and a sequence of n −1 edges, then P is a
feasible tour for the MLP instance (G, v1). Please note that the feedback edge
(vn, v1) is not included in P. The cost of P is deﬁned as c(P) = 
e∈E(P ) c(e). A
subtour (subpath) of P from vi to vj is denoted by viPvj. The latency l(vi, P, G)
of a vertex vi ∈V along the tour P is the cost of the subtour v1Pvi, i.e.,
l(vi, P, G) = c(v1Pvi). The starting vertex (source) v1 ∈V has l(v1, P, G) = 0.
The (total) latency L(P, G) of P is the sum of latencies for all vertices in P.
Alternatively, Deﬁnition 1 expresses the latency of P in the form of edges.
Deﬁnition 1. Let P = (e1, . . . , en−1) be a feasible tour for the MLP instance
(G, v1). The (total) latency L(P, G) of P can be expressed by (1).
L(P, G) =
n−1

i=1
(n −i) · c(ei)
(1)
Deﬁnition 2 (Metric Minimum Latency Problem (MLP)). Given an
MLP instance (G, s), the problem is to ﬁnd a feasible tour P such that P has
the minimum total latency for all feasible tours starting at s.
For a metric graph G = (V, E, c) and a vertex t ∈V , we deﬁne G −{t} to be
a metric graph G′ = (V ′, E′, c′) s.t., V = V ′ ∪{t} and ∀e ∈E′ : c′(e) = c(e).
Deﬁnition 3. MLPV+ (resp., MLPV−) is the following optimization problem:
Input: A vertex t (also called the modiﬁed vertex), two MLP instances (G, s)
and (G′, s) on metric graphs G = (V, E, c) and G′ = (V ′, E′, c′) respectively,
where G′ = G −{t} (resp., G = G′ −{t}), and an optimal tour OPT ′ for (G′, s);
Question: Find an optimal tour OPT for the MLP instance (G, s).
Deﬁnition 4. MLPE+ (resp., MLPE−) is the following optimization problem:
Input: A ﬁxed edge e∗(also called the modiﬁed edge), two MLP instances (G, s)

Reoptimization of Minimum Latency Problem
165
and (G′, s) on metric graphs G = (V, E, c) and G′ = (V, E, c′) respectively, where
the edge e∗∈E satisﬁes c′(e∗) < c(e∗) (resp., c′(e∗) > c(e∗)) and ∀e ∈E \{e∗} :
c(e) = c′(e), and an optimal tour OPT ′ for (G′, s).
Question: Find an optimal tour OPT for the MLP instance (G, s).
For reoptimization problems deﬁned by Deﬁnitions 3–4, from now on, we
implicitly use G′ and OPT ′ (resp., G and OPT ) to denote the input metric graph
and the given optimal tour (resp., an arbitrary optimal tour) of the initial (resp.,
modiﬁed) MLP instance. We will use opt′ (resp., opt) to denote the objective
value of an optimal solution for the initial (resp., modiﬁed) instance throughout
this paper. Since OPT ′ is also a graph, V (OPT ′) and E(OPT ′) denote vertices and
edges of OPT ′ respectively, while these notations are applied to OPT similarly.
Particularly, we will implicitly assume that V (G′) = {v1, . . . , vn} of n vertices
and OPT ′ = (v1, . . . , vn) in our discussions, unless otherwise explicitly stated.
Let MLPV−
ϵ
denote a variant of MLPV−such that the given solution is not
optimal but a (1+ϵ)-approximation solution for the initial instance, where ϵ > 0.
In Deﬁnition 5, we introduce a useful NP-complete problem for later proofs.
Deﬁnition 5 (Restricted Hamiltonian Cycle Problem (RHC) [19]).
Given a general (unweighted) graph G, and a Hamiltonian path P in G where
two endpoints of P are not adjacent in G, the problem is to decide whether G
contains a Hamiltonian cycle.
3
Hardness Results
In this section, we show that for the reoptimization problems deﬁned in Sect. 2
there is no hope for polynomial-time exact algorithms.
Theorem 1. MLPV+, MLPE+ and MLPE−are NP-hard.
The proof of Theorem 1 can be easily established via a general framework intro-
duced in [18]. The basic idea is to employ a polynomial-time Turing reduction,
which tries to ﬁnd a trivial MLP instance I0 such that a general MLP instance
I can be obtained by modifying I0 polynomially many times, then I is solved by
using an oracle of the MLP reoptimization problem polynomially many times.
However, this framework cannot be applied to MLPV−directly. Instead, for
Theorem 2, we take the Karp reduction to show the hardness of MLPV−.
Theorem 2. MLPV−is NP-hard.
Proof. The reduction is from RHC (Deﬁnition 5). Given an instance of RHC
(G⋆, P ⋆), where G⋆= (V ⋆, E⋆), V ⋆= {v1, . . . , vn} and a Hamiltonian path
(HP) P ⋆= (v1, . . . , vn), we construct an instance of MLPV−as follows.
The initial instance (G′, s) has the source s and the metric graph G′ =
(V ′, E′, c′), where V ′ = V ⋆∪{s, t, d} and c′ : E →{1, 2}. The modiﬁed instance
(G, s) has the metric graph G = (V, E, c), where V = V ′ \ {t} and ∀e ∈E :
c(e) = c′(e). The cost function c′ assigns the cost one to edges {s, t}, {t, d} and

166
W. Dai
{d, v1}. For each v ∈V ⋆\ {v1}, if {v1, v} ∈E⋆then c′ ({s, v}) = 1. Moreover,
for any two vertices vi, vj ∈V ⋆, if {vi, vj} ∈E⋆then c′ ({vi, vj}) = 1. For all
unmentioned edges in G′, c′ gives each edge the cost two. The construction is
completed by setting the given optimal tour OPT ′ = (s, t, d, v1, v2, . . . , vn).
It is easy to verify that G and G′ are metric. Clearly, OPT ′ is optimal to (G′, s)
since each edge of OPT ′ has the cost one in G′. Hence, we have constructed a
valid instance of MLPV−in polynomial time. Now, we claim that G⋆has a
Hamiltonian cycle (HC) if and only if opt ≤opt′ −(n + 2).
If there is a HC T ⋆in G⋆, then there must be a vertex v⋆∈V ⋆such that
{v1, v⋆} ∈E(T ⋆). Now, we can obtain one tour P = (s, v⋆T ⋆v1, d) in G, where
v⋆T ⋆v1 is a HP in G⋆from v⋆to v1. Clearly P has L(P, G) = opt′ −(n + 2).
Conversely, if a tour P for (G, s) has the latency opt′ −(n + 2) then each
edge of P has the cost one. This also implies that {v1, d} is the last edge of P
and d is the last vertex of P. Let {s, v⋆} be the ﬁrst edge of P, then {s, v⋆} has
the cost one and the vertex v⋆must be adjacent to v1 in G⋆. If {vi, vj} ∈E(P),
where vi, vj ∈V ⋆, then {vi, vj} ∈E⋆. Therefore, removing vertices d and s from
P and rejoining v⋆and v1 in P must produce a HC for the graph G⋆.
Finally, since RHC is NP-hard [19], this implies that MLPV−is NP-hard. ⊓⊔
Algorithm 1.1. The simple approximation algorithm for MLPV+
Input
: An instance (G, G′, OPT ′, s, t) of MLPV+;
Output: A feasible tour Tout of (G, s) starting at the source s;
1 Let the given optimal tour OPT ′ = (v1, . . . , vn) and the source s = v1;
2 The ﬁrst feasible tour of (G, s): T0 := (v1, . . . , vn, t);
3 The second feasible tour of (G, s): T1 := (v1, . . . , vn−1, t, vn);
4 return a tour Tout in {T0, T1} with the least total latency;
4
Approximation Results of MLP Reoptimization
4.1
Locally Modifying a Single Vertex
Theorem 3. Algorithm 1.1 is a 7/3-approximation algorithm for MLPV+.
Proof. The input instance is (G, G′, OPT ′, v1, t), where G′ = (V ′, E′, c′), G =
(V, E, c), the modiﬁed vertex t ∈V , G′ = G −{t}, v1 is the source. Recall that
∀e ∈E′ : c′(e) = c(e) and c(OPT ′) = l(vn, OPT ′, G′) is the cost of OPT ′ in G′.
In G, let cmin(t) denote the minimum edge cost for all edges incident on t,
then there must be v∗∈V ′ s.t. c({v∗, t}) = cmin(t). For MLPV+, it is obvious
that opt′ < opt. Due to l (t, OPT , G) ≥cmin(t), we know that opt′+cmin(t) ≤opt.
Since G is metric, the triangle inequality implies that
c ({vn, t}) ≤c({v∗, t}) + c ({v∗, vn}) ≤cmin(t) + c (OPT ′) .
(2)

Reoptimization of Minimum Latency Problem
167
Recalling Deﬁnition 1, by the factors given in (1), we know that
2 · c (OPT ′) ≤opt′ + c ({vn−1, vn}) .
(3)
According to Algorithm 1.1, we can bound the total latency of T0 in G by (4).
L(T0, G) ≤opt′ + c (OPT ′) + c ({vn, t})
≤opt′ + 2 · c (OPT ′) + cmin(t)
≤2 · opt + c ({vn−1, vn})
(4)
Particularly, if c({vn, t}) = cmin(t) then T0 directly leads to 2-approximation.
Hence, we will simply assume v∗̸= vn in the remainder of this proof.
Now, we discuss the tour T1, which visits t right after vn−1 but before vn.
Firstly, for each vi ∈{v1, . . . , vn−1}, it has l(vi, OPT ′, G′) = l(vi, T1, G). The
latency of vn in T1 is increased compared to its latency in OPT ′. By the triangle
inequality, we can bound this increased value as follows:
l(vn, T1, G) −l(vn, OPT ′, G′) ≤2 · c ({vn−1, t}) .
(5)
The latency of t in T1, i.e., l(t, T1, G), is c (v1OPT ′vn−1) + c ({vn−1, t}). The
cost c({vn−1, t}) is also bounded by c (v1OPT ′vn−1)+cmin(t) if v∗̸= vn. Accord-
ing to the above inequalities and Deﬁnition 1, the latency of T1 in G is bounded
by the following equality.
L(T1, G) ≤opt′ + c (v1OPT ′vn−1) + 3 · c ({vn−1, t})
≤opt′ + 4 · c (v1OPT ′vn−1) + 3 · cmin(t)
≤3 · opt′ −2 · c ({vn−1, vn}) + 3 · cmin(t)
≤3 · opt −2 · c ({vn−1, vn})
(6)
Since Algorithm 1.1 outputs Tout as the better one in {T0, T1}, then we can
combine (4) and (6) to ﬁnally show that L (Tout, G) ≤(7/3) · opt.
⊓⊔
Due to the limited space, the proof of Theorem 4 is not provided.
Theorem 4. MLPV−is 3-approximable by short-cutting the vertex t of OPT ′.
4.2
Altering the Cost of an Edge
Deﬁnition 4 specially requires the graph after changing the edge cost to be metric.
Then this constraint leads to a useful observation, which is given in Lemma 1.
Lemma 1 ([4]). Let G = (V, E, c) and G = (V, E, c′) be two metric graphs such
that the cost functions c and c′ coincide except for one edge e∗∈E. Then every
edge adjacent to e∗has the cost at least |c(e∗) −c′(e∗)| /2.
For MLPE+, if e∗/∈E(OPT ′) then OPT ′ remains optimal after increasing
the cost of e∗. Thus, we only need to consider the restricted MLPE+ where
e∗∈E(OPT ′).

168
W. Dai
Theorem 5. The restricted MLPE+ where e∗is the i-th edge of the given opti-
mal tour OPT ′ for 1 ≤i ≤n−1, can gain an approximation ratio α as follows: if
i ̸= 1, n−1 then α = 2; if i = n−1 then α = 4/3; and if i = 1 then α = 2.1286.
Proof. Let an instance of this restricted MLPE+ be (G, G′, OPT ′, v1, e∗), where
G = (V, E, c), G′ = (V, E, c′), OPT ′ = (v1, . . . , vn) and v1 is the source. We note
that |E (OPT )| = n −1, ∀e ∈E \ {e∗} : c′(e) = c(e) and e∗is the i-th edge of
OPT ′. Deﬁne δ = c(e∗) −c′(e∗) for δ > 0. Clearly, MLPE+ has opt′ ≤opt.
For each i ∈{1, . . . , n −1}, the latency of OPT ′ after increasing is given as
L (OPT ′, G) = opt′ + (n −i) · δ.
(7)
Firstly, we show that OPT ′ is still a 2-approximation tour for the modiﬁed
instance (G, v1) when i /∈{1, n −1}. If i /∈{1, n −1}, the i-th edge e∗of
OPT ′ implies that OPT ′ must contain the (i −1)-th edge e∗
l and the (i + 1)-th
edge e∗
r. Since these two edges are adjacent to e∗, Lemma 1 directly shows that
δ/2 ≤min{c′ (e∗
l ) , c′ (e∗
r)} .
(8)
Recall Deﬁnition 1, we can use (8) to further derive that
(n −i) · δ ≤(n −(i −1)) · c′ (e∗
l ) + (n −(i + 1)) · c′ (e∗
r) ≤opt′ .
(9)
Bounding the term (n −i) · δ in (7) with (9), it directly implies that
L (OPT ′, G) ≤2 · opt′ ≤2 · opt.
(10)
Secondly, we prove that OPT ′ is a 4/3-approximation tour for the (G, v1) when
i = n −1. If i = n −1, e∗= {vn−1, vn} is the last edge of OPT ′. Thus, e∗cannot
be in any optimal tour of (G, v1), otherwise OPT ′ remains optimal for (G, v1).
Let OPT be an arbitrary optimal tour of (G, v1). Firstly, we assume vn−1 is
visited before vn in OPT . Now, we consider a special case where vn is the n-th
vertex and vn−1 is the (n −2)-th vertex visited in OPT . In such a case, let en
k
denote the (n −k)-th edge traversed in OPT for k ∈{1, 2, 3} respectively. Then,
for each k = 1, 2, 3, the edge en
k is adjacent to e∗, which implies c(en
k) ≥δ/2.
opt ≥3 · c(en
3) + 2 · c (en
2) + c (en
1)
≥(3 + 2 + 1) · (δ/2) ≥3 · δ
(11)
By Deﬁnition 1, we can further infer (11). By using δ ≤opt/3 in (7), we show
the conclusion L(OPT ′, G) ≤4
3 · opt if i = n −1. For other diﬀerent cases, the
sum of factors in (11) should be larger than 6, then approximation ratios cannot
be worse than 4/3. On the other hand, a similar discussion can be given if vn−1
is visited after vn in OPT .
Finally, if i = 1, we will show that OPT ′ cannot directly provide a good
approximation for (G, v1). Thus, we will ﬁnd the other tour to complement OPT ′
to achieve a 2.1286-approximation. If i = 1 then e∗= {v1, v2} is the ﬁrst edge
of OPT ′. Let OPT be an arbitrary optimal tour for (G, v1). Note that e∗cannot

Reoptimization of Minimum Latency Problem
169
be the ﬁrst edge of OPT , otherwise OPT ′ remains optimal after increasing. The
ﬁrst edge of OPT , denoted by ef = {v1, v∗
f}, must be adjacent to e∗, which also
implies δ ≤2 · c (ef).
L (OPT ′, G) ≤(1 + 2 · ρ) · opt
(12)
By Deﬁnition 1, we can set ρ · opt := (n −1) · c(ef), where 0 < ρ < 1. Then we
can rewrite (7) to (12). When ρ →1, the approximation ratio (1 + 2 · ρ) given
by OPT ′ gets close to three, which is not good enough. Hence, we will obtain an
alternative tour Talt for ρ →1.
Clearly, the edge ef = {v1, v∗
f} can be found by the exhaustive search. After
removing v1 from OPT , the remaining part of OPT is still an optimal tour for
the MLP instance (G −{v1}, v∗
f). By the algorithm of Chaudhuri et al. [14], we
can compute a 3.59-approximation tour T for the instance (G −{v1}, v∗
f). Let
the tour Talt traverse ef at ﬁrst and then T. The latency of Talt is bounded by
L (Talt, G) ≤ρ · opt + 3.59 · (1 −ρ) · opt
≤3.59 · opt −2.59 · ρ · opt .
(13)
The better tour of OPT ′ and Talt will be the ﬁnal output Tout. Thus, by
combining (12) and (13), we obtain L(Tout, G) ≤2.1286 · opt.
⊓⊔
For MLPE−, we ﬁrstly study the restricted version that has e∗∈E(OPT ′).
By Theorems 6–7, we show the complexity dichotomy for this restricted MLPE−.
Theorem 6. For MLPE−, if the modiﬁed edge e∗is the ﬁrst or second edge of
the given optimal tour OPT ′, then OPT ′ remains optimal in the modiﬁed instance.
Proof. For MLPE−, if e∗is the ﬁrst edge of OPT ′, then no tour can lose more
latency than OPT ′ after decreasing the cost of e∗by Deﬁnition 1. Moreover, if e∗
is the second edge of OPT ′ then e∗cannot contain the source. It means no feasible
tour can visit e∗as the ﬁrst edge. Thus, no feasible tour can lose more latency
than OPT ′ after decreasing. Hence, OPT ′ remains optimal for above cases.
⊓⊔
Theorem 7. The restricted MLPE−where e∗is the i-th edge of the given opti-
mal tour OPT ′ of the initial instance is NP-hard for a constant i ≥3 and has no
FPTAS unless P=NP.
Proof. To show NP-hardness, we reduce from RHC (Deﬁnition 5). Given an
instance (G⋆, P ⋆) of RHC, where G⋆= (V ⋆, E⋆), V ⋆= {v1, . . . , vn}, and
Hamiltonian path (HP) P ⋆= (v1, . . . , vn), we will construct an instance of
MLPE−where the modiﬁed edge e∗is visited as the third edge of OPT ′.
We construct the initial instance (G′, s), where G′ = (V, E, c′), V = V ⋆∪
{s, a, b}, c′ : E →{2, 2.5, 3} and s is the source. The cost function c′ gives the
cost two to each of edges {s, a}, {s, b}, {b, v1} and {a, b}. For any vi, vj ∈V ⋆, if
{vi, vj} ∈E⋆then c′ ({vi, vj}) = 2. For each v ∈V ⋆\ {v1}, if {v1, v} ∈E⋆then
c′ ({a, v}) = 2.5. Each unmentioned edge of E has cost three in c′. The modiﬁed
instance (G, s) has G = (V, E, c), where ∀e ∈E \ {e∗} : c(e) = c′(e). The given

170
W. Dai
optimal tour OPT ′ of (G′, s) is (s, a, b, v1, v2 . . . , vn). The modiﬁed edge e∗is
{b, v1}, which has the cost c (e∗) = 1 after decreasing the cost.
It is easy to see that G′ is metric due to c′ : E′ →{2, 2.5, 3}. The graph
G is also metric since c : E′ →{1, 2, 2.5, 3} and only one edge e∗has the cost
one. The constructed OPT ′ is optimal to (G′, s) since ∀e ∈E(OPT ′) : c′(e) = 2.
Clearly, we constructed a valid instance of MLPE−in polynomial time. Now, we
claim that G⋆has a Hamiltonian cycle (HC) if and only if opt ≤opt′ −n −1/2.
Given a HC T ⋆in G⋆, there is a vertex v⋆∈V ⋆such that {v1, v⋆} ∈E(T ⋆). A
HP from v1 to v⋆in G⋆, denoted by v1P ⋆v⋆, is generated by removing {v1, v⋆}
from T ⋆. Then a tour P = (s, b, v1P ⋆v⋆, a) is a feasible tour for (G, s). The
latency of P in G is L (P, G) = opt′ −n −1/2.
Conversely, let P be a feasible tour to (G, s) such that L (P, G) = opt′−n−1
2.
By this latency, we can infer as follows: e∗is the second edge of P (v1 is visited
after b); a is the last vertex of P; and the last edge of P, denoted by {v⋆, a}, has
c ({v⋆, a}) = 2.5. Then c ({v⋆, a}) = 2.5 implies {v⋆, v1} ∈E⋆. Thus, removing
vertices a, b and s from P and rejoining v⋆and v1 must form one HC for G⋆.
Due to NP-hardness of RHC [19], our constructed instance is also NP-hard.
Now, we suppose there is an FPTAS A that returns a tour TA with
L(TA, G) ≤(1 + θ) · opt in the polynomial time poly(1/θ, |G|). The construc-
tion is the same as above. For the modiﬁed instance (G, v1), every non-optimal
tour T has
L(T, G) ≥opt + 0.5 ≥

1 +
1
2 · opt

· opt .
By setting θ < 1/(2·opt), A must return an optimal tour to (G, v1) in polynomial
time, which decides whether G⋆has a HC. Hence, no FPTAS unless P = NP. ⊓⊔
Theorem 8. For MLPE−, if the modiﬁed edge e∗is the i-th edge of the given
optimal tour OPT ′ for 3 ≤i ≤n −1, then OPT ′ is 2-approximation tour after
decreasing the cost of e∗, and a PTAS is admitted if i is a constant.
Proof. Let (G, G′, OPT ′, v1, e∗) be an instance of MLPE−, where G = (V, E, c),
G′ = (V, E, c′), V = {v1, . . . , vn}, OPT ′ = (v1, . . . , vn) and v1 is the source. We
note that |E (OPT )| = n −1 and ∀e ∈E \ {e∗} : c′(e) = c(e). Let OPT denote
an arbitrary optimal tour to (G, v1) s.t. OPT ̸= OPT ′. Since the cost of e∗is
decreased, it holds that opt′ ≥opt. We also deﬁne δ = c′(e∗) −c(e∗) > 0.
Clearly, OPT must take e∗as the j-th edge for 1 < j < i, otherwise OPT ′
remains optimal after decreasing the cost of e∗. By Deﬁnition 1, we can express
the latency of OPT ′ in G as follows:
L (OPT ′, G) = opt′ −(n −i) · δ
≤opt + (n −j) · δ −(n −i) · δ
≤opt + (i −j) · δ .
(14)
Now, we try to bound (i −j) · δ. We note that e∗cannot be the last edge for
OPT , otherwise j < i is violated and OPT ′ remains optimal in (G, v1). Thus, there
must be two edges ej−1, ej+1 ∈E(OPT ) such that ej−1 and ej+1 are traversed

Reoptimization of Minimum Latency Problem
171
in OPT exactly before and after the edge e∗respectively. Moreover, Lemma 1
indicates that c(ej−1) ≥δ/2 and c(ej+1) ≥δ/2. Hence, by Deﬁnition 1, we can
bound δ via (15).
(n −(j −1)) · c (ej−1) + (n −(j + 1)) · c (ej+1) ≤opt
=⇒δ ≤
opt
(n −j)
(15)
By bounding δ in (14) with (15), and due to n > i, we can conclude via (16).
L (OPT ′, G) ≤

1 + (i −j)
(n −j)

· opt ≤2 · opt
(16)
For a constant i, i−j is also constant. Then, given a constant i, the approximation
ratio (i −j)/(n −j) shown in (16) can be arbitrarily close to one if n →∞.
By this observation, a PTAS can be designed if i is constant. Given an error
parameter θ, the PTAS A will output the given optimal tour OPT ′ directly if
(i −j)/(n −j) ≤θ, otherwise an optimal tour OPT of (G, v1) will be searched
exhaustively. The run-time of the exhaustive search is bounded within O

n1/θ
.
Thus, MLPE−has a PTAS if e∗is the i-th edge of OPT ′ for a constant i.
⊓⊔
For MLPE−, if e∗/∈E(OPT ′) then each optimal tour OPT of the modiﬁed
instance must contain e∗. Thus, similar to Theorem 5, we introduce Theorem 9.
Unfortunately, we currently have no idea about how to determine the position of
e∗in OPT except that e∗contains the source. If e∗includes the source, this case
results in the worst ratio 2.1286 + O(1/n). Since Theorem 9 can be proved in a
similar way of proving Theorem 5, the detailed proof is omitted to save space.
Theorem 9. For the restricted MLPE−where e∗/∈E(OPT ′) and e∗is the j-th
edge in an optimal tour OPT of the modiﬁed instance for 1 ≤j ≤n −1, it can
be approximated by a ratio α as follows: if j = 1 then α = 2.1286 + O(1/n); if
j = n −1 then α = 1.5; otherwise α = 2.
5
Relaxing the Optimality of the Given Solution
In this section, we study MLPV−
ϵ
, which is a variant of MLPV−by relaxing the
optimality of the given solution. We will understand the optimality of the given
solution is critical for MLPV−to outperform MLP since Theorem 10 shows that
MLPV−
ϵ
cannot be approximated better than MLP anymore for a small ϵ > 0.
Theorem 10. If MLPV−
ϵ
is α-approximable then MLP is also α-approximable.
Proof (Sketch). We give an approximation factor preserving reduction from
MLP. Given an instance (G⋆, s) of MLP, where G⋆= (V ⋆, E⋆, c⋆) and s ∈V ⋆is
the source, we construct a MLPV−
ϵ
instance (G, G′, Tϵ, s, t) with ϵ > 0 as follows.
Let G = (V, E, c) be the metric graph generated by removing a vertex t from
a metric graph G′ = (V ′E′, c′). Firstly, we set G = G⋆. Then the graph G′ has

172
W. Dai
the following settings: V ′ = V ∪{t}, E′ = E ∪Et and ∀e ∈E : c′(e) = c(e),
where Et is the set of edges incident with t. Since ∀e ∈E : c(e) = c⋆(e), we only
need to consider the costs for the edges in Et.
We assume there is a γ-approximation algorithm A for the MLP, e.g., the
3.59-approximation algorithm by Chaudhuri et al. [14]. By applying A to (G, s),
we obtain a γ-approximation tour T . For each vertex v ∈V , we set
c′ ({v, t}) = L(T , G) · 2 · γ −1
ϵ
.
Finally, let the given feasible tour Tϵ be a tour of T followed by the vertex t.
At ﬁrst, we show that this construction is a valid instance of MLPV−
ϵ
. It is
easy to verify that G′ is also metric if G is metric. Then we show that Tϵ is a
valid given solution. Clearly, our settings imply that ∀e ∈Et : c′(e) > L(T , G).
Thus, it is better to visit t at the end for any optimal tour of (G′, s). Now, we
can prove that Tϵ is a (1 + ϵ)-approximation tour for the initial instance (G′, s).
L (Tϵ, G′)
opt′
≤γ · opt + c (T ) + L(T , G) · 2γ−1
ϵ
opt + L(T , G) · 2γ−1
ϵ
≤1 +
(2γ −1) · opt
opt + L(T , G) · 2γ−1
ϵ
≤1 + ϵ
(17)
After removing t, the modiﬁed instance (G, s) is the same as the given MLP
instance (G⋆, s). Thus, MLPV−
ϵ
cannot be approximated better than MLP.
⊓⊔
Acknowledgement. This is part of the author’s master thesis, written under the
supervision of Tobias M¨omke at Saarland University.
References
1. Archetti, C., Bertazzi, L., Speranza, M.G.: Reoptimizing the traveling salesman
problem. Networks 42(3), 154–159 (2003)
2. Archetti, C., Bertazzi, L., Speranza, M.G.: Reoptimizing the 0–1 knapsack prob-
lem. Discret. Appl. Math. 158(17), 1879–1887 (2010)
3. Ausiello, G., Escoﬃer, B., Monnot, J., Paschos, V.: Reoptimization of minimum
and maximum traveling salesman’s tours. J. Discret. Algorithms 7(4), 453–463
(2009). http://www.sciencedirect.com/science/article/pii/S1570866708001019
4. B¨ockenhauer, H.J., Forlizzi, L., Hromkovic, J., Kneis, J., Kupke, J., Proietti, G.,
Widmayer, P.: On the approximability of TSP on local modiﬁcations of opti-
mally solved instances. Algorithmic Oper. Res. 2(2), 83 (2007). https://journals.
lib.unb.ca/index.php/AOR/article/view/2803
5. Berg, T., Hempel, H.: Reoptimization of traveling salesperson problems: changing
single edge-weights. In: Dediu, A.H., Ionescu, A.M., Mart´ın-Vide, C. (eds.) LATA
2009. LNCS, vol. 5457, pp. 141–151. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-00982-2 12

Reoptimization of Minimum Latency Problem
173
6. Bil`o, D., B¨ockenhauer, H.-J., Hromkoviˇc, J., Kr´aloviˇc, R., M¨omke, T., Widmayer,
P., Zych, A.: Reoptimization of steiner trees. In: Gudmundsson, J. (ed.) SWAT
2008. LNCS, vol. 5124, pp. 258–269. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-69903-3 24
7. Bil`o, D., B¨ockenhauer, H.J., Komm, D., Kr´aloviˇc, R., M¨omke, T., Seibert, S., Zych,
A.: Reoptimization of the shortest common superstring problem. Algorithmica
61(2), 227–251 (2011). http://dx.doi.org/10.1007/s00453-010-9419-8
8. Bil`o, D., Widmayer, P., Zych, A.: Reoptimization of weighted graph and covering
problems. In: Bampis, E., Skutella, M. (eds.) WAOA 2008. LNCS, vol. 5426, pp.
201–213. Springer, Heidelberg (2009). doi:10.1007/978-3-540-93980-1 16
9. Bil`o, D., Zych, A.: New advances in reoptimizing the minimum steiner tree prob-
lem. In: Rovan, B., Sassone, V., Widmayer, P. (eds.) MFCS 2012. LNCS, vol. 7464,
pp. 184–197. Springer, Heidelberg (2012). doi:10.1007/978-3-642-32589-2 19
10. Blum, A., Chalasani, P., Coppersmith, D., Pulleyblank, B., Raghavan, P., Sudan,
M.: The minimum latency problem. In: Proceedings of the Twenty-sixth Annual
ACM Symposium on Theory of Computing, STOC 1994, NY, USA, pp. 163–171
(1994). http://doi.acm.org/10.1145/195058.195125
11. Boria, N., Croce, F.D.: Reoptimization in machine scheduling. Theor. Comput.
Sci. 540, 13–26 (2014). http://dx.doi.org/10.1016/j.tcs.2014.04.004
12. Boria, N., Monnot, J., Paschos, V.T.: Reoptimization of maximum weight
induced hereditary subgraph problems. Theor. Comput. Sci. 514, 61–74 (2013).
http://dx.doi.org/10.1016/j.tcs.2012.10.037
13. Boria, N., Paschos, V.T.: A survey on combinatorial optimization in dynamic
environments. RAIRO - Oper. Res. 45(3), 241–294 (2011). http://dx.doi.org/10.
1051/ro/2011114
14. Chaudhuri, K., Godfrey, B., Rao, S., Talwar, K.: Paths, trees, and minimum latency
tours. In: 44th Annual IEEE Symposium on Foundations of Computer Science,
2003, Proceedings, pp. 36–45, October 2003
15. Christoﬁdes, N.: Worst-case analysis of a new heuristic for the travelling sales-
man problem. Technical report 388, Graduate School of Industrial Administration,
Carnegie Mellon University (1976)
16. Giorgio Ausiello, V.B., Escoﬃer, B.: Complexity and approximation in reoptimiza-
tion. In: Cooper, S.B., Sorbi, A. (eds.) Computability in Context: Computation and
Logic in the Real World, pp. 101–129. World Scientiﬁc, Singapore (2011)
17. Goyal, K., M¨omke, T.: Robust reoptimization of Steiner trees. In: 35th IARCS
Annual Conference on Foundation of Software Technology and Theoretical Com-
puter Science, FSTTCS 16–18, 2015, Bangalore, India, pp. 10–24 (2015). http://
dx.doi.org/10.4230/LIPIcs.FSTTCS.2015.10
18. B¨ockenhauer, H.-J., Hromkoviˇc, J., M¨omke, T., Widmayer, P.: On the hardness of
reoptimization. In: Geﬀert, V., Karhum¨aki, J., Bertoni, A., Preneel, B., N´avrat,
P., Bielikov´a, M. (eds.) SOFSEM 2008. LNCS, vol. 4910, pp. 50–65. Springer,
Heidelberg (2008). doi:10.1007/978-3-540-77566-9 5
19. Hromkoviˇc, J.: Algorithmics for Hard Problems: Introduction to Combinator-
ial Optimization, Randomization, Approximation, and Heuristics. Springer-Verlag
New York Inc., New York (2001)
20. Monnot, J.: A note on the traveling salesman reoptimization problem under
vertex insertion. Inf. Process. Lett. 115(3), 435–438 (2015). http://dx.doi.org/
10.1016/j.ipl.2014.11.003
21. Papadimitriou, C.H., Yannakakis, M.: The traveling salesman problem with dis-
tances one and two. Math. Oper. Res. 18(1), 1–11 (1993). http://dx.doi.org/10.
1287/moor.18.1.1

174
W. Dai
22. Sch¨aﬀter, M.W.: Scheduling with forbidden sets. Discrete Appl. Math. 72(1–2),
155–166 (1997). http://dx.doi.org/10.1016/S0166-218X(96)00042-X
23. Sitters, R.: The minimum latency problem is NP-hard for weighted trees. In: Pro-
ceedings of the 9th Integer Programming and Combinatorial Optimization Con-
ference, pp. 230–239 (2002)

Pure Nash Equilibria in Restricted
Budget Games
Maximilian Drees1(B), Matthias Feldotto2, S¨oren Riechers2,
and Alexander Skopalik2
1 Departement of Applied Mathematics, University of Twente,
Enschede, The Netherlands
m.w.drees@utwente.nl
2 Department of Computer Science and Heinz Nixdorf Institute,
Paderborn University, Paderborn, Germany
{feldi,soerenri,skopalik}@mail.upb.de
Abstract. In budget games, players compete over resources with ﬁnite
budgets. For every resource, a player has a speciﬁc demand and as a strat-
egy, he chooses a subset of resources. If the total demand on a resource
does not exceed its budget, the utility of each player who chose that
resource equals his demand. Otherwise, the budget is shared propor-
tionally. In the general case, pure Nash equilibria (NE) do not exist for
such games. In this paper, we consider the natural classes of singleton
and matroid budget games with additional constraints and show that
for each, pure NE can be guaranteed. In addition, we introduce a lexi-
cographical potential function to prove that every matroid budget game
has an approximate pure NE which depends on the largest ratio between
the diﬀerent demands of each individual player.
1
Introduction
Resource allocation problems are widely considered in theory and practice. In
computing centers, for example, resources such as processing power and available
data rate have to be divided such that the overall performance is optimized. In
our paper, we consider the problem that service providers often cannot satisfy
the needs of all clients. Here, the total payoﬀobtainable from a system is often
independent of the number of its participants. For example, the computational
capacity of a server is usually ﬁxed and does not grow with the number of
requests. In a diﬀerent use case, the overall size of connections between a service
provider and all clients may be limited by the amount of data the provider can
process. In our model, this is reﬂected by a limited budget for each resource. Now,
diﬀerent clients may have diﬀerent agreed target uses with a provider, which we
model by diﬀerent weights, also called demands throughout the paper. In case
a provider cannot fulﬁll the requirements of all clients, the available resource
This work was partially supported by the German Research Foundation (DFG)
within the Collaborative Research Centre “On-The-Fly Computing” (SFB 901).
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 175–187, 2017.
DOI: 10.1007/978-3-319-62389-4 15

176
M. Drees et al.
needs to be split, resulting in clients not being supplied with their full demand.
In video streaming, for example, this may lead to a lower quality stream for
certain clients. Additionally, we allow part of a resource to be reserved by some
external party, which we model as oﬀsets in our setting.
We consider this model in a game theoretic setting called budget games. Here,
we are interested in the eﬀects of rational decision making by individuals. In our
context, the clients act as the players, who compete over resources with a ﬁnite
budget. We assume that clients can choose freely among diﬀerent strategies, with
each available strategy being a subset of resources. A player has a speciﬁc demand
on every resource. For example, in cloud computing, we view each strategy as a
distribution of the necessary computing power on diﬀerent computing centers.
Now, each player strives to maximize the overall amount of resource capacities
that is supplied to him. Our main interest lies in states in which no client wants to
deviate from his current strategy, as this would yield no or only a marginal beneﬁt
for him in the given situation. These states are called pure Nash equilibria,
or approximate pure Nash equilibria, respectively. Instead of a global instance
enforcing such stable states, they occur as the result of player-induced dynamics.
At every point in time, exactly one player changes his strategy such that the
amount of received demand is maximized, assuming the strategies of the other
players are ﬁxed. It is known that in general, pure Nash equilibria do not exist
in budget games. In our earlier research, we considered pure Nash equilibria in
ordered budget games [7], where the order of the players arriving at a resource
inﬂuences the distribution of its budget. In [6], we further discussed approximate
pure Nash equilibria in standard budget games, where the resource is distributed
proportionally between the players on the basis of their demands. However, the
question whether there are pure Nash equilibria for certain restricted instances
of standard budget games remained open. In this paper, we focus on budget
games with restrictions on the strategies of the players and show that there are
indeed certain properties under which pure Nash equilibria always exist. Matroid
budget games capture the natural assumption that for any player, the value of a
resource is independent of the other chosen resources. A special case are singleton
budget games in which each player can only choose one resource at a time.
Our contribution. For matroid budget games, we show that under the restric-
tion of ﬁxed demands per player, they possess the ﬁnite improvement property.
This implies that the player-induced dynamic mentioned above always leads to
a pure Nash equilibrium. On the other hand, we also show that even under
this restriction, the matroid property is still required for the existence of pure
Nash equilibria. Without any extra conditions on the demands, we can guarantee
approximate pure Nash equilibria with a small approximation ratio depending
on the maximum ratio between the demands of a single player. By further limit-
ing the structure of the strategies to singleton, we can loosen the restriction on
the demands and still obtain positive results regarding equilibria. In some cases,
singleton budget games are weakly acyclic, i.e., there is an improving path from
each initial state to a pure Nash equilibrium. For the additional class of oﬀset
budget games we can guarantee the existence of pure Nash equilibria under some
additional restrictions.

Pure Nash Equilibria in Restricted Budget Games
177
Related Work. Budget games share many properties with congestion games.
Although the speciﬁc structure of the utility functions makes budget games a
special case, the fact that the demand of a player can vary between resources also
qualiﬁes them as a more general model for representing diﬀerent impacts of play-
ers on resources. In congestion games, players choose among subsets of resources
while trying to minimize personal costs. In the initial (unweighted) version [17],
the cost of each resource depends only on the number of players choosing it
and it is the same for each player using that resource. They are exact potential
games [16] and therefore always possess pure Nash equilibria. In the weighted
version [15], each player has a ﬁxed weight and the cost of a resource depends on
the sum of weights. For this larger class of games, pure Nash equilibria can no
longer be guaranteed. Ackermann et al. [1] determined that the structure of the
strategy spaces is a crucial property in this matter. While a matroid congestion
game always has a pure Nash equilibrium, every non-matroid set system induces
a game without it. Harks and Klimm [10] gave a complete characterization of the
class of cost functions for which every weighted congestion game possesses a pure
Nash equilibrium. The cost functions have to be aﬃne transformations of each
other as well as be aﬃne or exponential. Another extension considers player-
speciﬁc payoﬀfunctions for the resources, which depend only on the number
of players using a resource, but are diﬀerent for each player [15]. For singleton
strategy spaces, these games maintain pure Nash equilibria. Ackerman et al. [1]
showed that, again, every player-speciﬁc matroid congestion game has a pure
Nash equilibrium, while this is also a maximal property.
In a model similar to ours, each player does not choose only his resources,
but also his demand on them [11]. In contrast, the players in our model can-
not inﬂuence their demands. These games have pure Nash equilibria if the cost
functions are either exponential or aﬃne. Mavronicolas et al. [14] combined the
concepts of weighted and player-speciﬁc congestion games and gave a detailed
overview of the existence of pure Nash equilibria. In these games, the cost func-
tion ci,r of player i for resource r consists of a base function cr, which depends
on the weights of all players using r, as well as a constant ki,r, both connected by
abelian group operations. Later, Gairing and Klimm [8] characterized the con-
ditions for pure Nash equilibria in general player-speciﬁc congestion games with
weighted players. Pure Nash equilibria exist, if, and only if, the cost functions of
the resources are aﬃne transformations of each other as well as aﬃne or expo-
nential. Another generalization of congestion games is given by Byde et al. [3]
and Voice et al. [19]. They introduce the model of games with congestion-averse
utility functions. They show under which properties pure Nash equilibria exist
and identify the matroid as a required property for the existence in most cases.
Although they consider more general utility functions than standard congestion
games, their model does not consider players’ weights or demands.
Instead of assigning the whole cost of a resource to each player using it, the
cost can also be shared between those players, so that everyone only pays a
part of it. Such games are known as cost sharing games [12]. One method to
determine the share of each player is proportional cost sharing, in which the

178
M. Drees et al.
share increases with the weight of a player. This is exactly what we are doing
with budget games, but with utilities instead of costs. Under proportional cost
sharing, which corresponds to our utility functions, pure Nash equilibria again do
not exist in general [2]. Kollias and Roughgarden [13] took a diﬀerent approach
by considering weighted games in which the share of each player is identical to his
Shapley value [18]. Using this method, every weighted congestion game yields a
weighted potential function. However, we do not approach this from a mechanism
design angle. Instead, we consider this system and especially the structure of
the utility functions as given by the scenarios we introduced. Negative results
regarding both existence and complexity of pure Nash equilibria lead to the study
of approximate pure Nash equilibria [5]. Caragiannis et al. [4] and Hansknecht
et al. [9] showed the existence of approximate pure Nash equilibria for weighted
congestion games.
Model. A budget game B is a tuple (N, R, (br)r∈R, (Si)i∈N , (Di)i∈N ) where
the set of players is denoted by N = {1, . . . , n}, the set of resources by R =
{r1, . . . , rm}, the budget of resource r by br ∈R>0, the strategy space of player
i by Si and the demands of player i by Di = (di(r1), . . . , di(rm)). Each strategy
si ⊆2R is a subset of resources. We call di(rj) > 0 the demand of i on rj and
say that a strategy si uses a resource rj if rj ∈si. The set of strategy proﬁles is
denoted by S := S1 × . . . × Sn. Each player i has a private utility function ui :
S →R≥0, which he strives to maximize. For a strategy proﬁle s = (s1, . . . , sn),
let Tr(s) := 
i∈N:r∈si di(r) be the total demand on resource r. The utility of
player i from resource r is denoted by ui,r(s) ∈R≥0 and deﬁned as ui,r(s) = 0 if
r /∈si and ui,r(s) := di(r) · cr(s) if r ∈si, where cr(s) := min (1, br/Tr(s)) denotes
the utility per unit demand. The total utility of i is ui(s) := 
r∈R ui,r(s). When
increasing the demand on a resource by some value d, we write cr(s) ⊕d :=
min (1, br/(Tr(s)+d)). If Mi = (R, Ii) is a matroid with Ii = {x ⊆s | s ∈Si}
for every player i, we call B a matroid budget game. A matroid budget game
is called a singleton budget game if every strategy uses exactly one resource.
Let s ∈S and i ∈N. We denote with s−i := (s1, . . . , si−1, si+1, . . . , sn) the
strategy proﬁle excluding i. For any s′
i ∈Si, we can extend this to (s−i, si) :=
(s1, . . . , si−1, s′
i, si+1, . . . , sn) ∈S. The best response of i to s−i is denoted by
sb
i ∈Si: i.e., ui(s−i, sb
i) ≥ui(s−i, si) for all si ∈Si. We call the switch from si
to s′
i with ui(s−i, si) < ui(s−i, s′
i) an improving move for player i. Sequential
execution of best response improving moves creates a best response dynamic. A
strategy proﬁle s is called an α-approximate pure Nash equilibrium if α · ui(s) ≥
ui(s−i, s′
i) for every i ∈N and s′
i ∈Si. For α = 1, s is simply called a pure Nash
equilibrium. For the rest of this paper, we mostly omit the preﬁx pure. If from
any initial strategy proﬁle there is a path of improving moves which reaches an
(α-approximate) Nash equilibrium, then the game is said to be weakly acyclic.
If from any initial strategy proﬁle each path of improving moves reaches an (α-
approximate) Nash equilibrium, then the game possesses the ﬁnite improvement
property. For a strategy proﬁle s, the lexicographical potential function φ : S →
Rm
>0 is deﬁned as φ(s) := (cr1(s), . . . , crm(s)) with the entries crk(s) being sorted
in ascending order. The augmented lexicographical potential function φ∗: S →

Pure Nash Equilibria in Restricted Budget Games
179
Rm+1
>0
extends this deﬁnition with φ∗(s) := (T(s), cr1(s), . . . , crm(s)), whereas
T(s) := 
i∈N

r∈si di(r) is the total demand by all players under s.
2
Matroid Budget Games
By deﬁnition, all strategies of player i in a matroid budget game have the same
size mi. In addition, for any strategy proﬁle s, a strategy change from si to s′
i
can be decomposed into a sequence si = s0
i , s1
i , . . . , smi
i
= s′
i of lazy moves which
satisfy sk
i ∈Si, |sk
i \ sk+1
i
| ≤1 and ui(s−i, sk
i ) < ui(s−i, sk+1
i
) for 0 ≤k ≤mi
(see [1]). A matroid budget game has ﬁxed demands if there exists a constant
di ∈R>0 for every player i such that di(r) = di for all r ∈R.
Theorem 1. A matroid budget game with ﬁxed demands reaches a pure Nash
equilibrium after a ﬁnite number of improving moves.
Proof. We show that a single lazy move already increases the lexicographical
potential function φ. Let player i perform a lazy move in strategy proﬁle s,
switching resource r1 for r2. Let s′ be the resulting strategy proﬁle. We get
ui,r1(s) = di · cr1(s) < di · cr2(s′) = ui,r2(s′) or simply cr1(s) < cr2(s′). Since
cr1(s) < cr1(s′) also holds due to Tr1(s′) = Tr1(s) −di, we get φ(s) <lex φ(s′)
and see that φ is strictly increasing regarding the lexicographical order for every
improving lazy move. Since the number of diﬀerent values of φ is ﬁnite, the best
response eventually reaches a strategy proﬁle without any further improving
move. By deﬁnition, this is a pure Nash equilibrium.
⊓⊔
For this result, the structure of the strategy spaces is a crucial property.
Consider the budget game B0 shown in Fig. 1, which is deﬁned as follows: N =
{1, 2, 3}, R = {r1, r2, r3, r4}, br = 2 for all r, S1 = {s1
1 = {r1, r2}, s2
1 = {r3, r4}},
S2 = {s1
2 = {r1, r3}, s2
2 = {r2, r4}}, S3 = {s3 = {r1, r4}} and d1 = 2, d2 =
d3 = 1.
Theorem 2. There is a budget game with ﬁxed demands which is not a matroid
budget game and does not have a pure Nash equilibrium.
Fig. 1. The budget game B0 with ﬁxed demands and no pure Nash equilibrium.

180
M. Drees et al.
Proof. We analyze the game B0. Player 3 has only one strategy, so we focus on
the four diﬀerent strategy proﬁles resulting from the choices of player 1 and 2
(see Table 1). In each strategy proﬁle, one player is able to increase his utility
through a unilateral strategy change, so no pure Nash equilibrium exists.
⊓⊔
Table 1. Overview of the diﬀerent strategy proﬁles (restricted to players 1 and 2) and
the corresponding utilities of the budget game B0.
players strategy proﬁles
(s1
1, s1
2)
(s1
1, s2
2)
(s2
1, s1
2)
(s2
1, s2
2)
1
2 + 1 = 3
4
3 + 2
3 = 2
4
3 + 2
3 = 2 2 + 1 = 3
2
1 + 1
2 = 3
2
2
3 + 1 = 5
3
2
3 + 1 = 5
3
1 + 1
2 = 3
2
When considering singleton budget games with ﬁxed demands, a Nash equi-
librium can also be computed eﬃciently. Before proving this, we introduce a
technical result.
Lemma 1. Let d1, d2 ∈R>0 with d1 ≤d2 and br, Tr(s) ∈R≥0 with Tr(s)+d1 ≥
br. Then d1 · min

1,
br
Tr(s)+d1

≤d2 · min

1,
br
Tr(s)+d2

.
We omit the proof of this lemma. In the context of budget games, it implies
that a player with higher demand always receives a higher utility from the same
resource than another player with lower demand.
Theorem 3. For a singleton budget game with ﬁxed demands, pure Nash equi-
libria can be computed in time O(n).
Proof. We start with an empty strategy proﬁle where si = ∅for every player
i. The players then choose their actual strategy sequentially in ascending order
of their demands. We show that a strategy choice made by player j does not
change the best response of any player i with di ≤dj. Let s be the strategy
proﬁle the moment before j chooses his strategy. If j picks the same resource r
as i, then dj · (cr(s) ⊕dj) ≥dj · (cr′(s) ⊕dj) ≥di · (cr′(s) ⊕di) for all r′ ∈R due
to Lemma 1, meaning that r is still the best response for i.
⊓⊔
The potential function φ can also be used to give an upper bound on approx-
imate Nash equilibria in any matroid budget game, even if the demands are not
ﬁxed. Starting with an arbitrary strategy proﬁle s0, we only allow improving
moves that also strictly increase φ. For player i, let dmax
i
:= max{di(r) | r ∈R}
and dmin
i
:= min{di(r) | r ∈R}.
Theorem 4. A matroid budget game has an α-approximate pure Nash equilib-
rium for α = max {dmax
i /dmin
i
| i ∈N}.
Proof. Let s be a strategy proﬁle of a matroid budget game B in which player
i can switch resource r1 for r2 to increase his utility. We restrict the best
response dynamic such that we only allow this lazy move if it also satisﬁes

Pure Nash Equilibria in Restricted Budget Games
181
di(r1) · cr1(s) < di(r1) · (cr2(s) ⊕di(r1)). If this condition holds, player i would
still proﬁt from the lazy move if his demands on both r1 and r2 were the same.
Such a lazy move would also increase φ as shown in the proof of Theorem 1.
Therefore, the number of such improving moves is ﬁnite and this restricted best
response dynamic arrives at a strategy proﬁle sα. Let s be a strategy proﬁle
which originates from sα through a unilateral improving move by player i to si
and let Δα = |si \ sα
i |. We assign an index k to every rα
k ∈sα
i and every rk ∈si.
If a resource r is used by both sα
i and si, then it has the same index ℓfor both
strategies, where ℓ≥Δα. The improving move from sα
i to si consists only of lazy
moves with di(rα
k ) < di(rk) and crα
k (sα) ≥(crk(sα)⊕di(rk)). Since di(rk)
di(rα
k ) ≤dmax
i
dmin
i
holds for all resources, we get
ui(s) =

r∈si
ui,r(s) =
Δα

k=1
di(rk) · (crk(sα) ⊕di(rk)) +
mi

k=Δα+1
di(rα
k ) · crα
k (sα)
≤
Δα

k=1
dmax
i
dmin
i
· di(rα
k ) · crα
k (sα) +
mi

k=Δα+1
dmax
i
dmin
i
· di(rα
k ) · crα
k (sα)
= dmax
i
dmin
i
·
mi

k=1
di(rα
k ) · crα
k (sα) = dmax
i
dmin
i
· ui(sα)
⊓⊔
3
Singleton Budget Games with Two Demands
We now consider singleton budget games with two demands: i.e., di(r) ∈
{d−, d+} for every demand of a player i on a resource r. We assume d−< d+.
Also, all budgets are uniform: i.e., br = br′ for all resources r, r′. Finally, every
resource r is available to every player i: i.e., there is a strategy si ∈Si using r.
This models situations in which each player partitions the resources into two sets
such that he prefers the resources from the ﬁrst set over those in the second and
he regards all resources from the same set as equally good. In our model, a more
prefered resource is identiﬁed by a higher demand. Note that the preferences of
two diﬀerent players do not have to be the same. We show that Algorithm 1
always computes a Nash equilibrium by using the best response dynamic, which
proves Theorem 5. The algorithm utilizes the best response dynamic and only
controls the order of the improving moves. For the following discussion, we sepa-
rate the improving moves into diﬀerent types. The type depends on the demand
of the corresponding player before and after his strategy change. Since we con-
sider only two demands, there are only four diﬀerent types: d+ →d+, d+ →d−,
d−→d+ and d−→d−. We immediately see that in the intermediate strategy
proﬁle right after Phase 1 of the algorithm, no improving move of type d+ →d−
exists. In addition, we now introduce the concepts of pushing and pulling strat-
egy changes.
Let B be a singleton budget game with players i, j, resources r1, r2 and strat-
egy proﬁle s. In s, let si = {r1} and sj = {r2} with ui(s) < ui(s−i, r2) and
uj(s) ≥uj(s−j, r) for all r ∈R. Denote s′ = (s−i, r2). If uj(s′) < uj(s′
−j, r3) for

182
M. Drees et al.
Algorithm 1. ComputeNE
s ←arbitrary initial strategy proﬁle
Phase 1:
while there is a player in s with best resp. improving move of type d+ →d−do
perform best response improving move of type d+ →d−
s ←resulting strategy proﬁle
Phase 2:
while current strategy proﬁle s is not a pure Nash equilibrium do
if there is a player with best resp. improving move of type d+ →d−then
perform best response improving move of type d+ →d−
else if there is a player i with b.-r. improving move of type d+ →d+ then
N ′ ←{j ∈N | j has best response improving move of type d+ →d+}
choose i ∈N ′ such that Tsi(s) ≥Tsj(s) for all j ∈N ′
perform best response improving move of i
else
perform any best response improving move
▷d−→d−or d−→d+
s ←resulting strategy proﬁle
return s
▷s is pure Nash equilibrium
some r3 ∈R, then the strategy change by i from r1 to r2 is called a pushing
strategy change for j. In the same scenario, let ui(s) ≥ui(s−i, r) for all r ∈R and
uj(s) < uj(s−j, r3) for some r3 ∈R. Denote s∗= (s−j, r3). If ui(s∗) < ui(s∗
−i, r2),
then the strategy change by j from r2 to r3 is called a pulling strategy change
for i (see Fig. 2). If a strategy change is both pushing and pulling for the same
player, we always regard it as the former. On the basis of these characterizations,
we analyze the eﬀects of strategy changes between the players. The proofs of the
following three lemmata are done by case distinction and omitted here.
Lemma 2. Let s be a strategy proﬁle during Phase 2 of Algorithm 1. In s, no
best response improving move of type d+ →d−is created by a pushing strategy
change.
Lemma 3. Let s be a strategy proﬁle during Phase 2 of Algorithm 1. In s, no
best response improving move of type d+ →d−is created by a pulling strategy
change of type d−→d−.
Fig. 2. Examples for pushing (left) and pulling (right) strategy changes.

Pure Nash Equilibria in Restricted Budget Games
183
Lemma 4. Let s be a strategy proﬁle during Phase 2 of Algorithm 1. In s, no
best response improving move of type d+ →d−is created by a pulling strategy
change of type d+ →d+.
We use the augmented lexicographical potential function φ∗to show that our
algorithm actually terminates. With the three lemmata above, we see that during
Phase 2, any improving move of type d+ →d−has to be created by a pulling
strategy change of type d−→d+. By executing both strategy changes right
after another, we can combine them into a so-called macro strategy change. In a
macro strategy change, two players i, j change their resources, with r being both
the old resource of i and the new resource of j and di(r) = dj(r). As a result, the
total demand on r does not change during a macro strategy change. An example
can be seen in Fig. 3. Although not associated with an actual player, we say
that a macro strategy change is performed by a virtual player. The following
lemma shows that in our case, this virtual player would actually beneﬁt from
his strategy change. Again, we omit a proof.
Lemma 5. Let s be a strategy proﬁle in which a macro strategy change of type
d+ →d+ from r1 to r3 is executed. Then d+ · cr1(s) < d+ · (cr3(s) ⊕d+).
Using this lemma, we conclude that a macro strategy change strictly increases
φ∗. Its type is d+ →d+ and from the results in the previous section, we know
that such a strategy change strictly increases φ. Since the total demand of all
players does not change, this holds for φ∗as well.
Fig. 3. Example for a macro strategy change.This sequence of strategy changes is
equivalent to the strategy change of a virtual player k from r1 to r3.
Theorem 5. A singleton budget game with two demands and uniform budgets
is weakly acyclic.
Proof. By construction, the output of Algorithm 1 is a Nash equilibrium. It
remains to show that it actually terminates at some point. The number of
improving moves in the ﬁrst phase is at most n, as every player changes his
strategy at most once. For the second phase, we use the augmented lexicograph-
ical potential function φ∗. This function is strictly increasing regarding <lex for
all strategy changes of type d−→d−and d+ →d+, since φ is strictly increasing
for these types and the total demand of all players does not change. For strat-
egy changes of type d−→d+, φ∗is also strictly increasing because the total
demand is always the ﬁrst entry in φ∗(s) and it increases. φ∗can only decrease
for improving moves of type d+ →d−.

184
M. Drees et al.
Let s1 be the strategy proﬁle right after Phase 1 has terminated. Then s1
contains no best response improving move of type d+ →d−. According to Lem-
mata 2, 3 and 4, such moves can appear only as the result of a pulling strategy
change of type d−→d+. In this case, both can be regarded as a single macro
strategy change of type d+ →d+. Because of Lemma 5 and the fact that such a
macro strategy change does not change the total demand, φ∗strictly increases
for such a macro strategy change, too. If a pulling strategy change creates mul-
tiple best response improving moves of type d+ →d−to a resource r, then
the algorithm executes one of them, chosen by some arbitrary rule. Afterwards,
the total demand on r is the same as it was before the pulling strategy change.
Hence, the other best response moves of type d+ →d−cease to exist.
φ∗strictly increases after at least every second strategy change, so our algo-
rithm has to terminate at some point. The resulting strategy proﬁle is a Nash
equilibrium. During its execution, the algorithm performs only best response
improving moves and decides only the order in which they are completed. There-
fore, the game is weakly acyclic.
⊓⊔
We do not know if this result carries over to matroid budget games under
the same restrictions. Regarding budget games with two demands and uniform
budgets, but arbitrary strategy space structures, we already know that Nash
equilibria do not exist in general [6].
4
Singleton Oﬀset Budget Games
In this section, we introduce a new variant of budget games that allows a ﬁxed
oﬀset to the total demand on a resource. As already mentioned in our introduc-
tion, this allows us to include reserved instances for speciﬁc users in our games.
An oﬀset σr ∈R≥0 for resource r ∈R changes the utility of player i from r under
strategy proﬁle s to ui,r(s) = di(r) · min (1, br/(Tr(s)+σr)). It is easy to see that
by setting σr = 0 for every r ∈R, every oﬀset budget game becomes a regular
budget game. We now consider budget games with two additional restrictions:
a total order on the players (based on their demands) and increasing demand
ratios. Let i, j be players with di(r) ≤dj(r) for some resource r. The ﬁrst restric-
tion states that although the demands of the players are no longer ﬁxed, their
order is the same for every resource, so di(r′) ≤dj(r′) for all resources r′. This
is a quite natural assumption, as bigger players (like global companies) normally
have high demands on all resources. The second restriction requires larger play-
ers to have bigger deviations between their demands: i.e., di(r′)/di(r) ≤dj(r′)/dj(r)
(assuming di(r) ≤di(r′)). Again, this assumption is only natural. Larger players
(e.g., jobs on servers) oﬀer more room for optimization and are more inﬂuenced
by their choice of resource (e.g., servers which better support certain kinds of
operations) than smaller ones, which are already quite compact. For this class
we can guarantee the existence of pure Nash equilibria.
Theorem 6. Singleton oﬀset budget games with ordered players and increasing
demand ratios always have a pure Nash equilibrium.

Pure Nash Equilibria in Restricted Budget Games
185
Proof. Proof by induction over the number of players. For a game with n players,
we denote the oﬀset of resource r by σn(r). For n = 2, the statement becomes
trivial. For n > 2, we assume without loss of generality that dn(r1) ≥di(r1) for
all i ∈N and dn(r1) ≥dn(r) for all r ∈R. Fix the strategy of n to {r1}. The
resulting game is identical to one with n −1 players and σn−1
r1
= σn
r1 + dn(r1)
and σn−1
r
= σn
r for all r ∈R \ {r1}. By induction hypothesis, this game has a
Nash equilibrium s′. Let s = (s′, r1) and assume that s is not a Nash equilibrium
for n. We get un(s) = dn(r1) · (cr1(s) ⊕σn
r1) < dn(r2) · (cr2(s) ⊕(σn
r2 + dn(r2)) =
un(s−n, r2) for some r2 ∈R. Let i be another player on r1, i.e. si = {r1}. Since r1
is the best response of i, the following has to hold: ui(s) = di(r1)·(cr1(s)⊕σn
r1) ≥
di(r2) · (cr2(s) ⊕(σn
r2 + di(r2)) = ui(s−i, r2). By combining these two, we get
di(r2)
di(r1) · (cr2(s) ⊕(σn
r2 + di(r2)) < dn(r2)
dn(r1) · (cr2(s) ⊕(σn
r2 + dn(r2)).
(1)
Since the players are ordered, we know that dn(r2) ≥di(r2) and therefore
(cr2(s) ⊕(σn
r2 + di(r2)) ≥(cr2(s) ⊕(σn
r2 + dn(r2)). Equation 1 can thus be simpli-
ﬁed to di(r1)
di(r2) > dn(r1)
dn(r2). This contradicts our restriction that the demand ratios
are increasing and implies that i cannot exist. n is the only player on resource r1
and since this is his preferred resource, it is also his best response. We conclude
that s has to be a Nash equilibrium for all n players.
⊓⊔
This result holds for regular budget games, in particular.
Corollary 1. Singleton budget games with ordered players and increasing
demand ratios always have a pure Nash equilibrium.
For matroid (oﬀset) budget games with only two resources, this result can be
improved even more. In this case, we can drop both restrictions regarding ordered
players and increasing demand ratios. In addition, besides the pure existence of
equilibria, such games are also weakly acyclic. The proof of the following result
is similar to the one of Theorem 6, so we omit it here.
Theorem 7. Every matroid (oﬀset) budget game with two resources is weakly
acyclic.
5
Conclusion
The model of budget games enables us to analyze diﬀerent eﬀects which appear
speciﬁcally in, but are not limited to, cloud computing. In emerging markets
with shared resources, the question of resource allocation becomes more and
more important. We focus on a speciﬁc method of distributing resources among
the market participants. On the one hand, this model can be extended with
additional properties of the cloud computing scenario (e.g., prices). On the other
hand, other allocation mechanisms can be investigated.

186
M. Drees et al.
References
1. Ackermann, H., R¨oglin, H., V¨ocking, B.: Pure Nash equilibria in player-speciﬁc
and weighted congestion games. Theor. Comput. Sci. 410(17), 1552–1563 (2009)
2. Anshelevich, E., Dasgupta, A., Kleinberg, J., Tardos, E., Wexler, T., Roughgarden,
T.: The price of stability for network design with fair cost allocation. SIAM J.
Comput. 38(4), 1602–1623 (2008)
3. Byde, A., Polukarov, M., Jennings, N.R.: Games with congestion-averse utilities.
In: Mavronicolas, M., Papadopoulou, V.G. (eds.) SAGT 2009. LNCS, vol. 5814,
pp. 220–232. Springer, Heidelberg (2009). doi:10.1007/978-3-642-04645-2 20
4. Caragiannis, I., Fanelli, A., Gravin, N., Skopalik, A.: Approximate pure Nash equi-
libria in weighted congestion games: existence, eﬃcient computation, and structure.
ACM Trans. Econ. Comput. 3(1), 2 (2015)
5. Chien, S., Sinclair, A.: Convergence to approximate Nash equilibria in congestion
games. Games Econ. Behav. 71(2), 315–327 (2011)
6. Drees, M., Feldotto, M., Riechers, S., Skopalik, A.: On existence and properties of
approximate pure Nash equilibria in bandwidth allocation games. In: Proceedings
of the 8th International Symposium on Algorithmic Game Theory, pp. 178–189
(2015)
7. Drees, M., Riechers, S., Skopalik, A.: Budget-restricted utility games with ordered
strategic decisions. In: Proceedings of the 7th International Symposium on Algo-
rithmic Game Theory, pp. 110–121 (2014)
8. Gairing, M., Klimm, M.: Congestion games with player-speciﬁc costs revisited. In:
V¨ocking, B. (ed.) SAGT 2013. LNCS, vol. 8146, pp. 98–109. Springer, Heidelberg
(2013). doi:10.1007/978-3-642-41392-6 9
9. Hansknecht, C., Klimm, M., Skopalik, A.: Approximate pure Nash equilibria in
weighted congestion games. In: Proceedings of the 17th International Workshop
on Approximation Algorithms for Combinatorial Optimization Problems, pp. 242–
257. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik (2014)
10. Harks, T., Klimm, M.: On the existence of pure Nash equilibria in weighted
congestion games. In: Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf der
Heide, F., Spirakis, P.G. (eds.) ICALP 2010. LNCS, vol. 6198, pp. 79–89. Springer,
Heidelberg (2010). doi:10.1007/978-3-642-14165-2 8
11. Harks, T., Klimm, M.: Congestion games with variable demands. Mathemat. Oper.
Res. 41(1), 255–277 (2015)
12. Jain, K., Mahdian, M.: Cost sharing. In: Algorithmic Game Theory, pp. 385–410
(2007)
13. Kollias, K., Roughgarden, T.: Restoring pure equilibria to weighted congestion
games. ACM Trans. Econ. Comput. 3(4), 21 (2015)
14. Mavronicolas, M., Milchtaich, I., Monien, B., Tiemann, K.: Congestion games with
player-speciﬁc constants. In: Kuˇcera, L., Kuˇcera, A. (eds.) MFCS 2007. LNCS, vol.
4708, pp. 633–644. Springer, Heidelberg (2007). doi:10.1007/978-3-540-74456-6 56
15. Milchtaich, I.: Congestion games with player-speciﬁc payoﬀfunctions. Games Econ.
Behav. 13(1), 111–124 (1996)
16. Monderer, D., Shapley, L.S.: Potential games. Games Econ. Behav. 14(1), 124–143
(1996)
17. Rosenthal, R.W.: A class of games possessing pure-strategy Nash equilibria. Int.
J. Game Theory 2(1), 65–67 (1973)

Pure Nash Equilibria in Restricted Budget Games
187
18. Shapley, L.S.: A Value for n-Person Games. Technical report, DTIC Document
(1952)
19. Voice, T., Polukarov, M., Byde, A., Jennings, N.R.: On the impact of strategy
and utility structures on congestion-averse games. In: Leonardi, S. (ed.) WINE
2009. LNCS, vol. 5929, pp. 600–607. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-10841-9 61

A New Kernel for Parameterized Max-Bisection
Above Tight Lower Bound
Qilong Feng, Senmin Zhu, and Jianxin Wang(B)
School of Information Science and Engineering,
Central South University, Changsha 410083, People’s Republic of China
jxwang@csu.edu.cn
Abstract. In this paper, we study kernelization of Parameterized Max-
Bisection above Tight Lower Bound problem, which is to ﬁnd a bisection
(V1, V2) of G with at least ⌈|E|/2⌉+ k crossing edges for a given graph
G = (V, E). The current best vertex kernel result for the problem is of
size 16k. Based on analysis of the relation between maximum matching
and vertices in Gallai-Edmonds decomposition of G, we divide graph G
into a set of blocks, and each block in G is closely related to the number
of crossing edges of bisection of G. By analyzing the number of crossing
edges in all blocks, an improved vertex kernel of size 8k is presented.
1
Introduction
Given a graph G = (V, E), for two subsets V1, V2 of V , if V1∪V2 = V , V1∩V2 = ∅,
and ||V1| −|V2|| ≤1, then (V1, V2) is called a bisection of G. An edge of G
with one endpoint in V1 and the other endpoint in V2 is called a crossing edge
of (V1, V2). The Maximum Bisection problem is to ﬁnd a bisection (V1, V2) of
G with maximum number of crossing edges. Jansen et al. [8] proved that the
Maximum Bisection problem is NP-hard on planar graph. D´ıaz and Kami´nski
[1] proved that the Maximum Bisection is NP-hard on unit disk graphs.
Frieze and Jerrum [4] gave an approximation algorithm for the Maximum
Bisection problem with ratio 0.651. Ye [13] presented an improved approxima-
tion algorithm with ratio 0.699. Halperin and Zwick [7] gave an approximation
algorithm with ratio 0.701. Feige et al. [3] studied the Maximum Bisection prob-
lem on regular graphs, and presented an approximation algorithm with ratio
0.795. Karpi´nski et al. [9] studied approximation algorithms for Maximum Bisec-
tion problem on low degree regular graphs and planar graphs. For three regular
graphs, an approximation algorithm of ratio 0.847 was presented in [9]. For four
and ﬁve regular graphs, two approximation algorithms with ratios 0.805, 0.812
were presented in [9], respectively. For planar graph of a sublinear degree, a poly-
nomial time approximation scheme was presented in [9]. Jansen et al. [8] studied
Maximum Bisection problem on planar graphs, and gave the ﬁrst polynomial
time approximation scheme for the problem.
This work is supported by the National Natural Science Foundation of China under
Grants (61420106009, 61232001, 61472449, 61672536, 61572414).
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 188–199, 2017.
DOI: 10.1007/978-3-319-62389-4 16

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
189
For a given graph G, it is easy to ﬁnd a bisection with ⌈|E|/2⌉crossing edges
by probabilistic method. In this paper, we study the following problem.
Parameterized Max-Bisection above Tight Lower Bound (PMBTLB):
Given a graph G = (V, E) and non-negative integer k, ﬁnd a bisection
of G with at least ⌈|E|/2⌉+ k crossing edges, or report that no such bisec-
tion exists.
Gutin and Yeo [5] gave a vertex kernel of size O(k2) for the PMBTLB prob-
lem. Based on the relation between edges in maximum matching and crossing
edges, a parameterized algorithm of running time O∗(16k) was presented in [5].
Mnich and Zenklusen [11] presented a vertex kernel of size 16k for PMBTLB
problem based on Gallai-Edmonds decomposition of the given graph.
In this paper, we further analyze the relation between maximum matching
and vertices in Gallai-Edmonds decomposition for a given graph G. The vertices
in Gallai-Edmonds decomposition are divided into several categories, which play
important role in getting improved kernel. Based on the categories of vertices,
we divide graph G into a set of blocks, where each block is closely related to the
number of crossing edges of bisection of G. By analyzing the number of crossing
edges in all blocks, a vertex kernel of size 8k is presented.
2
Preliminaries
For a given graph G = (V, E), we use n, m to denote the number of vertices in V
and the number of edges in E, respectively. Assume that all the graphs discussed
in the paper are loopless undirected graph with possible parallel edges. For a
graph G = (V, E), if |V | is a odd number, we can add an isolated vertex into G
such that the number of crossing edges in each bisection of G is not changed.
For simplicity, we assume that all the graphs in the paper have even number of
vertices.
For two subsets A, B ⊆V , let E(A) be the set of edges in G[A], and let
E(A, B) be the set of edges with one endpoint in A and the other endpoint in
B. For two vertices u and v in G, for simplicity, let uv denote an edge between
u and v, and let E(u, v) denote the set of edges between u and v. For a vertex
v in G, let d(v) denote the degree of v in G. For a subset X ⊆V and a vertex
v in X, let dX(v) denote the degree of v in induced subgraph G[X], and let
δ(G[X]) be the number of connected components in G[X]. For a subgraph H of
G, let V (H) be the set of vertices contained in H. Let N[V (H)] denote the set
of neighbors of vertices in V (H), where V (H) is contained in N[V (H)], and let
N(V (H)) = N[V (H)] −V (H).
Given a matching M in G, let V (M) denote the set of vertices in M. If a
vertex u in G is not contained in V (M), then u is called an unmatched ver-
tex. Matching M is called a near-perfect matching of G if there is exactly one
unmatched vertex in G. For a connected graph G, and any vertex u in G, if the
size of maximum matching in G\{u} is equal to the size of maximum match-
ing in G, then G is called a factor-critical graph. A Gallai-Edmonds decom-
position of graph G is a tuple (X, Y, Z), where X is the set of vertices in G

190
Q. Feng et al.
which are not covered by at least one maximum matching of G, Y is N(X), and
Z = V (G)\(X ∪Y ). The Gallai-Edmonds decomposition of G can be obtained
in polynomial time [10].
Lemma 1 ([2,10]).
For a given graph G, a Gallai-Edmonds decomposition
(X, Y, Z) of G has the following properties:
1. the components of the subgraph induced by X are factor-critical,
2. the subgraph induced by Z has a perfect matching,
3. if M is any maximum matching of G, it contains a near-perfect matching of
each component of G[X], a perfect matching of each component of G[Z], and
matches all vertices of Y with vertices in distinct components of G[X],
4. the size of the maximum matching is 1
2(|V | −δ(G[X]) + |Y |).
For two subsets A, B of V , if A ∩B = ∅and |A| = |B|, then (A, B) is called
a basic block of graph G. Let C = {C1, . . . , Ch} be the set of basic blocks of
G, where Ci = (Ai, Bi). For a basic block Ci = (Ai, Bi), let V (Ci) denote the
set of vertices in Ai ∪Bi. Given two basic blocks Ci, Cj ∈C, for simplicity, let
E(Ci, Cj) = E(V (Ci), V (Cj)). For all basic blocks in C, if V (Ci) ∩V (Cj) = ∅
(i ̸= j) and h
i=1 V (Ci) = V , then C is called a block cluster of G. For a basic
block C ∈C, we use C −C to denote C\{C}.
Based on the block cluster C and V , a bisection (V1, V2) of G can be con-
structed in the following way: for each basic block Ci = (Ai, Bi) in C, put all
vertices in Ai into V1 and V2 with probability 1/2, 1/2, respectively; if Ai is put
into V1, then Bi will be put into V2, and if Ai is put into V2, then Bi will be put
into V1.
Let r1 = h
i=1 |E(Ai, Bi)|, r2 = h−1
i=1
h
j=i+1 |E(Ci, Cj)|, and r3 = h
i=1
(|E(Ai)|+|E(Bi)|). For a basic block Ci = (Ai, Bi) in C, let r(Ci) = |E(Ai, Bi)|−
|E(Ai)| −|E(Bi)|. Let r(C) = h
i=1 r(Ci).
Lemma 2. For any block cluster C of graph G, there exists a bisection (V ′
1, V ′
2)
of G obtained from C such that |E(V ′
1, V ′
2)| is at least ⌈m/2⌉+ r(C)/2.
Proof. For any two basic blocks Ci, Cj (i ̸= j) in C, we now analyze the expected
number of crossing edges from E(Ci, Cj) for bisection (V1, V2). Assume that Ai
is in V1, and Bi is in V2. In the process of constructing (V1, V2), Aj is put into V1
and V2 with probability 1/2, 1/2, respectively. Therefore, the expected number
of crossing edges from E(Ci, Cj) is |E(Ci, Cj)|/2. Moreover, for a basic block Ci
in C, if E(Ai, Bi) ̸= ∅, then the edges in E(Ai, Bi) are all crossing edges, and
edges in E(Ai) ∪E(Bi) are not crossing edges. Therefore, the expected number
of crossing edges in (V1, V2) is r1 + r2/2 = (2r1 + r2)/2 = (r(C) + r3 + r1 + r2)/2.
Since r1 + r2 + r3 = m, (r(C) + r3 + r1 + r2)/2 = m/2 + r(C)/2. Therefore, there
must exist a bisection (V ′
1, V ′
2) of G with |E(V ′
1, V ′
2)| ≥⌈m/2⌉+ r(C)/2.
⊓⊔
Lemma 3. For a given instance (G, k) of PMBTLB problem and any block
cluster C of G, if r(C) ≥2k, then G has a a bisection of size at least ⌈m/2⌉+ k
based a standard derandomization as given by Ries and Zenklusen [12].

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
191
3
Kernelization for PMBTLB Problem
For a given instance (G, k) of PMBTLB, assume that (X, Y, Z) is a Gallai-
Edmonds decomposition of G. Let MM be a maximum matching of G. Based
on the degree of vertices in X and the maximum matching MM, we divide X
into following subsets:
X0 = {v|v ∈X, d(v) = 0},
X1 = {v|v ∈X, dX(v) = 0, v ∈V (MM)},
X2 = {v|v ∈X, dX(v) = 0, v /∈V (MM)},
X3 = {v|v ∈X, dX(v) ≥1, ∃u ∈Y, uv ∈MM},
X4 = {v|v ∈X, ∃u ∈X, uv ∈MM},
X5 = {v|v ∈X, dX(v) ≥1, v /∈V (MM)}.
We now give the process to construct a block cluster C of graph G, as given
in Fig. 1. Assume that C = {C1, . . . , Ch} is the block cluster of G obtained by
algorithm BBDA1 in Fig. 1.
Fig. 1. Algorithm for constructing block cluster C
Lemma 4 ([6]). If M is a matching in a graph G, then G has a bisection of
size at least ⌈m/2⌉+ ⌊|M|/2⌋, which can be found in O(m + n) time.
By Lemma 4, we can get that the size of matching M is less than 2k, otherwise
a bisection with at least ⌈m/2⌉+ k crossing edges can be found in polynomial
time. For a maximum matching MM of G, if V ′ = V \V (MM) is empty, then
G is a graph with perfect matching. Since the size of matching MM is less than
2k, the number of vertices in G is bounded by 4k.
In the following, assume that V ′ is not empty. Since V ′ = V \V (MM), V ′
is an independent set. Assume that Ch is the basic block constructed by step
4 of algorithm BBDA1. According to the construction process of C, for each
basic block Ci in C −Ch, r(Ci) ≥1. Especially, r(Ch) = 0. We now construct a

192
Q. Feng et al.
new block cluster based on C. The general idea is to move vertices of V (Ch) to
the basic blocks in C −Ch to get new basic blocks. In the construction process,
if no vertex in added into a basic block C = (A, B), then the value r(C) is
not changed. Since vertices in V (Ch) form an independent set, after removing
some vertices of Ch, the vertices in the remaining basic block Ch still form an
independent set, and r(Ch) is still zero. In the following, we give the process to
get a new block cluster C′ = {C′
1, . . . , C′
h} based on C, which is given in Fig. 2.
Fig. 2. Algorithm for constructing block cluster C′
Let C′ be the block cluster returned by algorithm BBDA2. We now analyze
the diﬀerence between r(C) and r(C′). For two vertices v, w in X2 that are
added into Cl in step 3 of algorithm BBDA2, r(Cl) is increased by at least one.
Assume that S is returned by algorithm BBDA2. For any two vertices w, v in
S, it is easy to see that for each vertex u in G, |E(w, u)| = |E(v, u)|. Since all
vertices in |X2\S| are moved to C′
Y in algorithm BBDA2, r(C′
Y ) −r(CY ) is at
least |X2\S|/2, and r(C′)−r(C) is at least |X2\S|/2. In algorithm BBDA1, each
edge in MM is chosen to construct a basic block. Therefore, the value r(C) is at
least |MM|. Since C′ is constructed based on C, we have
r(C′) ≥|MM| + |X2\S|/2.
(1)
Since the vertices in X0, X2 and X5 are not in V (MM), in algorithm BBDA1,
V (Ch) = X0 ∪X2 ∪X5. In algorithm BBDA2, the vertices in X2\S are moved
from Ch to C′
Y . Therefore, V (C′
h) = X0 ∪S ∪X5.
Lemma 5. For any basic block C′
l = (A′
l, B′
l) in C′, where V (C′
l) contains one
vertex u of Y and u ∈N(S), assume that u ∈A′
l. Then, S cannot be connected
to any vertex in B′
l, and the number of basic blocks in C′ containing one vertex
in N(S) is |N(S)|.

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
193
For any connected component H in G[X] with at least three vertices, there
is exactly one vertex v in V (H) such that v is in either X3 or X5, and other
vertices in V (H)\{v} are in X4.
Lemma 6. |X5| < 2k.
Proof. If X5 is empty, then this lemma is correct. Let H = {H1, . . . , Hl} be the
set of connected components in G[X], each of which has size at least three and
contains one vertex in X5. For any Hi (1 ≤i ≤l) in H, there exists a perfect
matching in G[V (Hi)\{v}], and the number of edges from E(Hi) in MM is
(|V (Hi)| −1)/2. By above discussion, the number of edges in MM is less than
2k. Therefore, l
i=1(|V (Hi)| −1)/2 < 2k. Thus, |X5| < 2k.
⊓⊔
In the following, we will construct two new block clusters C′′ and C′′′ based
on C′ by adding vertices in X0, X5 and S into basic blocks of C′ −C′
h.
Case 1. |X0| ≥|S|. Under this case, the general idea to construct C′′ is to use
the vertices in S and X0 ﬁrstly. When all vertices in S are added into basic
blocks in C′ −C′
h to get new basic blocks in C′′, we consider the vertices X5 and
the remaining vertices in X0 to construct basic blocks in C′′′ based on C′′.
By Lemma 5, ﬁnd a basic block C′
i = (A′
i, B′
i) in C′ such that V (C′
i)∩N(S) =
{u}, and assume that u is contained in A′
i. A new basic block C′′
i = (A′′
i , B′′
i ) of
C′′ can be constructed from C′
i by the following steps: B′′
i = B′
i ∪S; arbitrarily
choose |S| vertices from X0, denoted by P; A′′
i = A′
i ∪P. Let C′′
h = C′
h\(S ∪P),
C′′ = (C′ −C′
i −C′
h) ∪C′′
i ∪C′′
h. Since no vertex in S is connected to any vertex
in B′
i and no vertex in P is connected to any vertex in V (C′
i), r(C′′
i ) −r(C′
i) is
at least |S|. Thus,
r(C′′) −r(C′) ≥|S|.
(2)
Let H be the set of connected components in G[X] such that for each con-
nected component H in H, V (H) contains one vertex of X5, and N(V (H))∩Y ̸=
∅. For each connected component H in H, assume that E′ is the set of edges
in H that are contained in MM. Let CH be a subset of C′′, where CH can be
constructed by the edges in E′. Since H is factor-critical, there exists a vertex v
in V (H) such that v is an unmatched vertex. If v is not connected to any vertex
in Y , then ﬁnd a vertex u in V (H) that is connected to some vertices in Y , and
ﬁnd a perfect matching M ′ in G[V (H)\{u}]. Construct a set F of basic blocks
by the edges in M ′, and let C′′ = (C′′ −CH)∪F. After dealing with all connected
components in H by the above process, a new maximum matching MM ′ can be
obtained, and for each connected component H in G[X] satisfying that V (H)
has at least three vertices and N(H) ∩Y ̸= ∅, if v is an unmatched vertex in H,
then v is connected to at least one vertex in Y .
The vertices in X5 are divided into the following two types. Let Xy
5 be a
subset of X5 such that each vertex v in Xy
5 is connected to at least one vertex in
Y , and let Xi
5 be a subset of X5 such that each vertex u in Xi
5 is not connected
to any vertex in Y .

194
Q. Feng et al.
Lemma 7. Given a vertex v ∈Xy
5 , for any basic block C′′
l = (A′′
l , B′′
l ) in C′′,
where V (C′′
l ) contains one vertex u of Y and u ∈N(v), assume that u ∈A′′
l .
Then, v cannot be connected to any vertex in B′′
l .
Let X′
0 = X0\P. We now construct basic blocks of C′′′ by vertices in X′
0
and Xy
5 . Assume that |X′
0| ≥|Xy
5 |. For a vertex v in Xy
5 , by Lemma 7, there
exists a basic block C′′
i = (A′′
i , B′′
i ) in C′′ containing u such that u ∈N(v).
Without loss of generality, assume that u is contained in A′′
i . A new basic block
C′′′
i
= (A′′′
i , B′′′
i ) of C′′′ can be constructed from C′′
i by the following process:
B′′′
i
= B′′
i ∪{v}; arbitrarily choose a vertex w in X′
0, let A′′′
i
= A′′
i ∪{w}. Let
C′′′
h = C′′
h\{v, w}, C′′′ = (C′′ −C′′
i −C′′
h)∪{C′′′
i }∪{C′′′
h }. Since v is not connected
to any vertex in B′′
i and w is not connected to any vertex in V (C′′
i ), it is easy
to see that r(C′′′
i ) −r(C′′
i ) ≥1. By considering all vertices in Xy
5 , we have
r(C′′′) −r(C′′) ≥|Xy
5 |.
(3)
For the case when |X′
0| < |Xy
5 |, a similar construction process can be obtained
as above. By considering all vertices in X′
0, we can get that
r(C′′′) −r(C′′) ≥|X′
0|.
(4)
Lemma 8. Based on MM ′, |Xi
5| ≤δ(G\X0).
Proof. We prove this lemma by discussing the types of connected components
in G\X0. For a connected component H in G\X0, if V (H) contains no vertex of
X, then no vertex in V (H) is contained in Xi
5. If all vertices in V (H) are from
X, then H is a factor-critical connected component, and no vertex in V (H)
is connected to Y . Under this case, one vertex from V (H) is contained in Xi
5.
Assume that V (H)\X is not empty. By the construction process of MM ′, if a
vertex v in N(Y ) ∩V (H) is contained in X5, then v is in Xy
5 , and no vertex in
V (H) is in Xi
5. Therefore, |Xi
5| ≤δ(G\X0).
⊓⊔
Rule 1. For a given instance (G, k) of PMBTLB, if |X0| + δ(G\X0) > n
2 , then
arbitrarily delete |X0| + δ(G\X0) −n
2 vertices from X0.
Lemma 9. Rule 1 is safe.
Proof. Assume that |X0|+δ(G\X0) > n
2 , and assume that (V1, V2) is a maximum
bisection of G. If X0 = ∅, then the number of connected components in G\X0
is at most n/2, because each connected component in G\X0 has at least two
vertices. Assume that the number of connected components in G\X0 is at least
one, otherwise, the PMBTLB problem can be trivially solved. Assume that H =
{H1, . . . , Hl} is the set of connected components in G\X0. We now prove that all
vertices in X0 cannot be contained totally in V1 or V2. Assume that all vertices of
X0 are contained in V1. Since |X0|+δ(G\X0) > n
2 , there must exist a connected
component Hi in H such that all vertices in V (Hi) are contained in V2. Choose a
vertex v in V (Hi), and ﬁnd a vertex u of X0 in V1. Let V ′
1 = (V1\{u})∪{v}, V ′
2 =
(V2\{v}) ∪{u}. Then, |E(V ′
1, V ′
2)| −|E(V1, V2)| ≥1, contradicting the fact that

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
195
(V1, V2) is a maximum bisection of G. Therefore, V1 ∩X0 ̸= ∅and V2 ∩X0 ̸= ∅.
Assume that X′
0 and X′′
0 are two subsets of X0 such that X′
0 is contained in V1,
and X′′
0 is contained in V2. Without loss of generality, assume that |X′
0| ≤|X′′
0 |.
Let R be any subset of X′′
0 of size |X′
0|. Let V ′′
1 = V1 −X′
0, V ′′
2 = V2 −R. Then,
|V ′′
1 | = |V ′′
2 |. Denote the new graph with bisection (V ′′
1 , V ′′
2 ) by G′. It is easy
to see that |E(V1, V2)| is bounded by ⌈m/2⌉+ k if and only if |E(V ′′
1 , V ′′
2 )| is
bounded by ⌈m/2⌉+ k. Repeat the above process until |X0| + δ(G\X0) ≤n
2 . ⊓⊔
For a given instance (G, k) of PMBTLB, by applying Rule 1 on G exhaus-
tively, we can get the following result.
Lemma 10. For a given instance (G, k) of PMBTLB, if |X0| ≥|S|, then the
number of vertices in G is bounded by 8k.
Proof. Based on block cluster C′′′, we prove this lemma by analyzing the sizes
of X′
0 and Xy
5 .
(1) |X′
0| ≥|Xy
5 |. By Lemma 3 and inequalities (1), (2) and (3), we can get
that r(C′′′) = |MM| + |X2\S|
2
+ |S| + |Xy
5 | < 2k. Based on Gallai-Edmonds
decomposition (X, Y, Z) and MM, we know that |MM| = (|Z|+|Y |+|X\(X0 ∪
X2 ∪X5)|)/2. Then, |Z| + |Y | + |X\(X0 ∪S ∪X5)| + 2|S| + 2|Xy
5 | < 4k. Since
X5 = Xy
5 ∪Xi
5, and S, Xi
5, Xy
5 , X0 are disjoint, we can get that
|Z| + |Y | + |X| −|X0| −|Xi
5| + |S| + |Xy
5 | < 4k.
(5)
By Lemmas 8 and 9, |X0| + |Xi
5| ≤n
2 . Then, we can get that
|V | = |Z| + |Y | + |X| = |Z| + |Y | + |X\(X0 ∪X5i)| + |X0| + |Xi
5|
= |Z| + |Y | + |X| −|X0| −|Xi
5|



<4k by (5)
+|X0| + |Xi
5|
< 4k + |V |/2.
Therefore, |V | < 8k.
(2) |X′
0| < |Xy
5 |. By Lemma 3 and inequalities (1), (2) and (4), we can get
that r(C′′′) = |MM| + |X2\S|
2
+ |S| + |X′
0| < 2k. Since P ⊆X0, X′
0 = X0\P
and |S| = |P|, we can get that r(C′′′) = |MM| + |X2\S|
2
+ |X0| < 2k. Similarly,
|MM| = (|Z| + |Y | + |X\(X0 ∪X2 ∪X5)|)/2. Then, |Z| + |Y | + |X\(X0 ∪S ∪
X5)| + 2|X0| < 4k. Since S, X5, X0 are disjoint and |X0| ≥|S|, we can get that
|Z| + |Y | + |X| −|X5| < 4k.
(6)
By Lemma 6, |X5| < 2k. Then, we can get that
|V | = |Z| + |Y | + |X| = |Z| + |Y | + |X\X5| + |X5|
= |Z| + |Y | + |X| −|X5|



<4k by (6)
+|X5|
< 4k + 2k = 6k.
Therefore, if |X0| ≥|S|, the number of vertices in G is bounded by 8k.
⊓⊔

196
Q. Feng et al.
Case 2. |X0| < |S|. Under this case, the general idea to construct C′′ is to use
the vertices in S and X0 ﬁrstly. When all vertices in X0 are added into basic
blocks in C′ −C′
h to get new basic blocks in C′′, we consider vertices in X5 and
the remaining vertices in S to construct basic blocks in C′′′ based on C′′.
By Lemma 5, ﬁnd a basic block C′
i = (A′
i, B′
i) in C′ such that V (C′
i)∩N(S) =
{u}, and assume that u is contained in A′
i. A new basic block C′′
i = (A′′
i , B′′
i ) of
C′′ can be constructed from C′
i by the following steps: A′′
i = A′
i ∪X0; arbitrarily
choose |X0| vertices from S, denoted by Q; B′′
i = B′
i∪Q. Let C′′
h = C′
h\(X0 ∪Q),
C′′ = (C′−C′
i−C′
h)∪{C′′
i }∪{C′′
h}. Since no vertex in Q is connected to any vertex
in B′
i and no vertex in X0 is connected to any vertex in V (C′
i), r(C′′
i ) −r(C′
i) is
at least |X0|. Thus,
r(C′′) −r(C′) ≥|X0|.
(7)
Since |X0| < |S|, S is not empty. For a connected component H in G[X], and
for any three vertices w, v and u, where w ∈S, v ∈V (H) and u ∈N(S), if
N(V (H)) = N(S) and |E(w, u)| = |E(v, u)|, then H is called a special component
in G[X].
Let H be the set of connected components in G[X] such that for each con-
nected component H in H, V (H) contains one vertex of X5 and H is not a special
component. For each connected component H in H, assume that E′ is the set
of edges in H that are contained in MM. Let CH be a subset of C′′, where CH
can be constructed by the edges in E′. Since H is factor-critical, there exists a
vertex v in V (H) such that v is an unmatched vertex. Based on N(S) and N(v),
we give following ﬁve conditions: (1) N(v) ∩N(S) = ∅; (2) N(v)\N(S) ̸= ∅; (3)
N(v) ⊂N(S); (4) N(v) = N(S), and for any two vertices w ∈S and u ∈N(S),
|E(v, u)| > |E(w, u)|; (5) N(v) = N(S), and for any two vertices w ∈S and
u ∈N(S), |E(w, u)| < |E(v, v)|.
We now introduce how to get a new maximum matching based on the above
ﬁve conditions. Since V (H) is not a special component, if v does not satisfy
any condition from conditions (1)–(5), then a vertex u in H can be found such
that u satisﬁes one of the above conditions. Then, ﬁnd a perfect matching M ′
in G[V (H)\{u}], and construct a set F of basic blocks by the edges in M ′. Let
C′′ = (C′′ −CH)∪F. After dealing with all connected components in H by above
process, a new maximum matching MM ′ can be obtained.
The vertices in X5 are divided into the following three types. Let X1
5 be a
subset of X5 such that for each vertex v in X1
5, the connected component in
G[X] containing v is a special component. Let X2
5 be a subset of X5 such that
for each vertex v in X2
5, v satisﬁes one of conditions (1), (2), and (4). Let X3
5
be a subset of X5 such that for each vertex v in X3
5, v satisﬁes one of conditions
(3) and (5).
Lemma 11. For any vertex v in X2
5 and any vertex w in S, there exists a vertex
u in N(v) with |E(v, u)| > |E(w, u)| such that a basic block C′′
i = (A′′
i , B′′
i ) in
C′′ can be found with V (C′′
i ) ∩Y = {u}. Assume that A′′
i ∩Y = {u}. Then, no
vertex in B′′
i is connected to {v, w}.

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
197
Lemma 12. For any vertex v in X3
5 and any vertex w in S, there exists a vertex
u in N(w) with |E(w, u)| > |E(v, u)| such that a basic block C′′
i = (A′′
i , B′′
i ) in
C′′ can be found with V (C′′
i ) ∩Y = {u}. Assume that A′′
i ∩Y = {u}. Then, no
vertex in B′′
i is connected to {v, w}.
Let S′ = S\Q. We now construct basic blocks of C′′′ by vertices in S′, X2
5 and
X3
5. Assume that |S′| ≥|X2
5|+|X3
5|. For a vertex v in X2
5 and any vertex w in S′,
by Lemma 11, there exists a vertex u in N(v) with |E(v, u)| > |E(w, u)| such that
a basic block C′′
i = (A′′
i , B′′
i ) in C′′ can be found with V (C′′
i )∩Y = {u}. Without
loss of generality, assume that u is contained in A′′
i . A new basic block C′′′
i
=
(A′′′
i , B′′′
i ) can be constructed from C′′
i by the following process: B′′′
i = B′′ ∪{v}
and A′′′
i = A′′∪{w}. Let C′′′
h = C′′
h\{v, w}, C′′′ = (C′′−C′′
i −C′′
h)∪{C′′′
i }∪{C′′′
h }.
By Lemma 11, no vertex in B′′
i is connected to {v, w}. It is easy to see that
r(C′′′
i ) −r(C′′
i ) ≥1.
For a vertex v in X3
5 and any vertex w in S′, by Lemma 12, there exists
a vertex u in N(w) with |E(w, u)| > |E(v, u)| such that a basic block C′′
j =
(A′′
j , B′′
j ) in C′′ can be found with V (C′′
j ) ∩Y = {u}. Without loss of generality,
assume that u is contained in A′′
j . A new basic block C′′′
j
can be constructed
from C′′
j by the following process: B′′′
j
= B′′ ∪{w} and A′′′
j
= A′′ ∪{v}. Let
C′′′
h = C′′
h\{v, w}, C′′′ = (C′′−C′′
j −C′′
h)∪{C′′′
j }∪{C′′′
h }. By Lemma 12, no vertex
in B′′
j is connected to {v, w}. It is easy to see that r(C′′′
j ) −r(C′′
j ) ≥1.
By considering all vertices in X2
5 ∪X3
5, we can get that
r(C′′′) −r(C′′) ≥|X2
5| + |X3
5|.
(8)
For the case when |S′| < |X2
5| + |X3
5|, a similar construction process can be
obtained as above. By considering all vertices in S′, we can get that
r(C′′′) −r(C′′) ≥|S′|.
(9)
Rule 2.
For a given instance (G, k) of PMBTLB, if |S| + |X1
5| >
n
2 , then
arbitrarily delete |S| + |X1
5| −n
2 vertices from S.
Lemma 13. Rule 2 is safe.
Proof. Assume that |S| + |X1
5| > n
2 , and assume that (V1, V2) is a maximum
bisection of G. Since |S| > |X0|, S is not empty. We prove this lemma by
discussing whether X1
5 is empty or not.
(a) X1
5 ̸= ∅. Assume that H = {H1, . . . , Hl} is the set of connected compo-
nents in G[X] containing one vertex of X1
5, and assume that S = {v1, . . . , vj}. We
now prove that all vertices in S cannot be contained totally in V1 or V2. Assume
that all vertices of S are contained in V1. Since |S|+|X1
5| > n
2 , there must exist a
connected component Hi in H such that all vertices in V (H) are contained in V2.
Choose a vertex v in Hi, and ﬁnd a vertex u of S in V1. Let V ′
1 = (V1−{u})∪{v},
V ′
2 = (V2 −{v})∪{u}. Then, |E(V ′
1, V ′
2)|−|E(V1, V2)| ≥1, contradicting the fact
that (V1, V2) is a maximum bisection of G. Therefore, V1 ∩S ̸= ∅and V2 ∩S ̸= ∅.
(b) X1
5 = ∅. Since |S| > n
2 , V1 ∩S ̸= ∅and V2 ∩S ̸= ∅.

198
Q. Feng et al.
Assume that S′ and S′′ are two subsets of S such that S′ is contained in V1,
and S′′ is contained in V2. Without loss of generality, assume that |S′| ≤|S′′|.
Let R be any subset of S′′ of size |S′|. Let V ′′
1 = V1 −S′, V ′′
2 = V2 −R. Then,
|V ′′
1 | = |V ′′
2 |. Denote the new graph with bisection (V ′′
1 , V ′′
2 ) by G′. It is easy
to see that |E(V1, V2)| is bounded by ⌈m/2⌉+ k if and only if |E(V ′′
1 , V ′′
2 )| is
bounded by ⌈m′/2⌉+ k, where m′ is the number of edges in G′. Repeat the
above process until |S| + |X1
5| ≤n
2 .
⊓⊔
For a given instance (G, k) of PMBTLB, by applying Rule 2 on G exhaus-
tively, we can get following result.
Lemma 14. For a given instance (G, k) of PMBTLB, if |S| > |X0|, then the
number of vertices in G is bounded by 8k.
Proof. Based on block cluster C′′′, we prove this lemma by analyzing the sizes
of S′, X2
5 and X3
5.
(1) |S′| ≥|X2
5| + |X3
5|. By Lemma 3 and inequalities (1), (7) and (8), we can
get that r(C′′′) = |MM| + |X2\S|
2
+ |X0| + |X2
5| + |X3
5| < 2k. Since |MM| =
(|Z| + |Y | + |X\(X0 ∪X2 ∪X5)|)/2, we can get that |Z| + |Y | + |X\(X0 ∪S ∪
X5)| + 2(|X0| + |X2
5| + |X3
5|) < 4k. Since X5 = X1
5 ∪X2
5 ∪X3
5, and S, X1
5, X2
5,
X3
5, X0 are disjoint, we have
|Z| + |Y | + |X| −|S| −|X1
5| + |X0| + |X2
5| + |X3
5| < 4k.
(10)
By Lemma 13, |S| + |X1
5| < n/2. Then, we can get that
|V | = |Z| + |Y | + |X| = |Z| + |Y | + |X\(S ∪X1
5)| + |S| + |X1
5|
= |Z| + |Y | + |X| −|S| −|X1
5|



<4k by (10)
+|S| + |X1
5|
< 4k + |V |/2.
Therefore, |V | < 8k.
(2) |S| < |X2
5| + |X3
5|. By Lemma 3 and inequalities (1), (7) and (9), we
have r(C′′′) = |MM| + |X2\S|
2
+ |X0| + |S′| < 2k. Since Q ⊆S, S′ = S\Q and
|X0| = |Q|, we can get that r(C′′′) = |MM| + |X2\S|
2
+ |S| < 2k. Similarly,
|MM| = (|Z| + |Y | + |X\(X0 ∪X2 ∪X5)|)/2. Then, |Z| + |Y | + |X\(X0 ∪S ∪
X5)| + 2|S| < 4k. Since S, X5, X0 are disjoint and |X0| < |S|, we can get that
|Z| + |Y | + |X| −|X5| < 4k.
(11)
By Lemma 6, |X5| < 2k. Then, we can get that
|V | = |Z| + |Y | + |X| = |Z| + |Y | + |X\X5| + |X5|
= |Z| + |Y | + |X| −|X5|



<4k by (11)
+|X5|
< 4k + 2k = 6k.
Therefore, if |X0| < |S|, the number of vertices in G is bounded by 8k.
⊓⊔

A New Kernel for Parameterized Max-Bisection Above Tight Lower Bound
199
For a given instance (G, k) of PMBTLB, our kernelization algorithm is to
apply Rule 1 and Rule 2 on G exhaustively. By Lemmas 10 and 14, we can get
the following result.
Theorem 1. The PMBTLB problem admits a vertex kernel of size 8k.
References
1. D´ıaz, J., Kami´nski, M.: MAX-CUT and MAX-BISECTION are NP-hard on unit
disk graphs. Theor. Comput. Sci. 377(1–3), 271–276 (2007)
2. Edmonds, J.: Paths, trees, and ﬂowers. Can. J. Math. 17(3), 449–467 (1965)
3. Feige, U., Karpi´nski, M., Langberg, M.: A note on approximating Max-Bisection
on regular graphs. Inf. Process. Lett. 79(4), 181–188 (2001)
4. Frieze, A., Jerrum, M.: Improved approximation algorithms for max k-cut and max
bisection. Algorithmica 18(1), 67–81 (1997)
5. Gutin, G., Yeo, A.: Note on maximal bisection above tight lower bound. Inf.
Process. Lett. 110(21), 966–969 (2010)
6. Haglin, D.J., Venkatesan, S.M.: Approximation and intractability results for the
maximum cut problem and its variants. IEEE Trans. Comput. 40(1), 110–113
(1991)
7. Halperin, E., Zwick, U.: A uniﬁed framework for obtaining improved approximation
algorithms for maximum graph bisection problems. Random Struct. Algorithms
20(3), 382–402 (2002)
8. Jansen, K., Karpi´nski, M., Lingas, A., Seidel, E.: Polynomial time approximation
schemes for max-bisection on planar and geometric graph. SIAM J. Comput. 35(1),
163–178 (2000)
9. Karpi´nski, M., Kowaluk, M., Lingas, A.: Approximation algorithms for max bisec-
tion on low degree regular graphs and planar graphs. Electron. Colloq. Comput.
Complex. 7(7), 369–375 (2000)
10. Lov´asz, L., Plummer, M.D.: Matching Theory. NorthHolland, Amsterdam (1986)
11. Mnich, M., Zenklusen, R.: Bisections above tight lower bounds. In: Golumbic,
M.C., Stern, M., Levy, A., Morgenstern, G. (eds.) WG 2012. LNCS, vol. 7551, pp.
184–193. Springer, Heidelberg (2012). doi:10.1007/978-3-642-34611-8 20
12. Ries, B., Zenklusen, R.: A 2-approximation for the maximum satisfying bisection
problem. Eur. J. Oper. Res. 210(2), 169–175 (2011)
13. Ye, Y.: A 0.699-approximation algorithm for max-bisection. Math. Program. 90(1),
101–111 (2001)

Information Complexity of the AND Function
in the Two-Party and Multi-party Settings
Yuval Filmus1, Hamed Hatami2, Yaqiao Li2(B), and Suzin You2
1 Technion — Israel Institute of Technology, Haifa, Israel
yuvalfi@cs.technion.ac.il
2 McGill University, Montreal, Canada
hatami@cs.mcgill.ca, yaqiao.li@mail.mcgill.ca, suzinyou.sy@gmail.com
Abstract. In a recent breakthrough paper [M. Braverman, A. Garg,
D. Pankratov, and O. Weinstein, From information to exact communi-
cation, STOC’13] Braverman et al. developed a local characterization for
the zero-error information complexity in the two-party model, and used
it to compute the exact internal and external information complexity of
the 2-bit AND function. In this article, we extend their results on AND
function to the multi-party number-in-hand model by proving that the
generalization of their protocol has optimal internal and external infor-
mation cost for certain distributions. Our proof has new components,
and in particular it ﬁxes a minor gap in the proof of Braverman et al.
1
Introduction
Although communication complexity has since its birth been witnessing steady
and rapid progress, it was not until recently that a focus on an information-
theoretic approach resulted in new and deeper understanding of some of the
classical problems in the area. This gave birth to a new area of complexity
theory called information complexity. Recall that communication complexity is
concerned with minimizing the amount of communication required for players
who wish to evaluate a function that depends on their private inputs. Information
complexity, on the other hand, is concerned with the amount of information that
the communicated bits reveal about the inputs of the players to each other, or
to an external observer.
One of the important achievements of information complexity is the recent
breakthrough of [BGPW13] that determines the exact asymptotics of the ran-
domized communication complexity of one of the oldest and most studied prob-
lems in communication complexity, set disjointness:
lim
ε→0 lim
n→∞
Rε(DISJn)
n
≈0.4827.
(1)
Here Rε(·) denotes the randomized communication complexity with an error
of at most ε on every input, and DISJn denotes the set disjointness problem.
H. Hatami—Supported by an NSERC grant.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 200–211, 2017.
DOI: 10.1007/978-3-319-62389-4 17

Information Complexity of the AND Function in the Two-Party
201
Prior to the discovery of these information-theoretic techniques, proving the
lower bound Rε(DISJn) = Ω(n) had already been a challenging problem, and
even Razborov’s [Raz92] short proof of that fact is intricate and sophisticated.
Note that the set disjointness function is nothing but an OR of AND func-
tions. More precisely, for i = 1, . . . , n, if xi is the Boolean variable which rep-
resents whether i belongs to Alice’s set or not, and yi is the corresponding
variable for Bob, then n
i=1(xi ∧yi) is true if and only if Alice’s input inter-
sects Bob’s input. Braverman et al. [BGPW13] exploited this fact to prove (1).
Roughly speaking, they ﬁrst determined the exact information complexity of
the 2-bit AND function for any underlying distribution μ on the set of inputs
{0, 1} × {0, 1}, and then used the fact that amortized communication equals
information complexity [BR14] to relate this to the communication complexity
of the set disjointness problem. The constant 0.4827 in (1) is indeed the maxi-
mum of the information complexity of the 2-bit AND function over all measures
μ that assign a zero mass to (1, 1) ∈{0, 1} × {0, 1}. That is
max
μ:μ(11)=0 ICμ(AND) ≈0.4827,
where ICμ(AND) denotes the information complexity of the 2-bit AND function
with respect to the distribution μ with no error (see Deﬁnition 1 below). These
results show the importance of knowing the exact information complexity of
simple functions such as the AND function.
Although obtaining the asymptotics of Rε(DISJn) from the information
complexity of the AND function is not straightforward and a formal proof
requires overcoming some technical diﬃculties, the bulk of [BGPW13] is ded-
icated to computing the exact information complexity of the 2-bit AND func-
tion. This rather simple-looking problem had been studied previously by Ma and
Ishwar [MI11,MI13], and some of the key ideas of [BGPW13] originate from their
work. In [BGPW13] Braverman et al. introduced a protocol to solve the AND
function, and proved that it has optimal internal and external information cost.
Interestingly this protocol is not a conventional communication protocol as it has
access to a continuous clock, and the players are allowed to “buzz” at randomly
chosen times. Indeed, it is known [BGPW13] that no protocol with a bounded
number of rounds can have optimal information cost for the AND function, and
hence the inﬁnite number of rounds, implicit in the continuous clock, is essential.
We shall refer to this protocol as the buzzers protocol.
1.1
Our Contributions
Fixing the Argument of [BGPW13]: In order to show that the buzzers
protocol has optimal information cost, inspired by the work of Ma and
Ishwar [MI11,MI13], Braverman et al. came up with a local concavity condi-
tion, and showed that if a protocol satisﬁes this condition, then it has optimal
information cost. This condition, roughly speaking, says that it suﬃces to verify
that one does not gain any advantage over the conjectured optimal protocol if
one of the players starts by sending a bit B. In the original paper [BGPW13], it

202
Y. Filmus et al.
is claimed that it suﬃces to verify this condition only for signals B that reveal
arbitrarily small information about the inputs. As we shall see, however, this is
not true, and one can construct counter-examples to this statement.
In Theorem 3 we prove a variant of the local concavity condition that allows
one to consider only signals B with small information leakage, and then apply
it to ﬁx the argument in [BGPW13]. We have been informed through private
communication that Braverman et al. have also independently ﬁxed this error.
Extension of [BGPW13] to the Multi-party Setting: We then apply
Theorem 3 to extend the result of [BGPW13] to the multi-party number-in-
hand model by deﬁning a generalization of the buzzers protocol, and then prove
in Theorem 4 that it has optimal internal and external information cost when
the underlying distribution satisﬁes the following assumption:
Assumption 1. The support of μ is a subset of {0, 1, e1, . . . , ek}, where ei is
the usual ith basis vector (0, . . . , 0, 1, 0, . . . , 0), and k is the number of parties.
Note that in the two-party setting, every distribution satisﬁes this assumption
and thus our results are complete generalizations of the results of [BGPW13] of
the two-party setting. The distributions in Assumption 1 arise naturally in the
study of the set disjointness problem, and as a result they have been considered
previously in [Gro09].
This extension is not straightforward since in [BGPW13], a large part of
the calculations for verifying the local concavity conditions are carried out by
the software Mathematica, however, in the number-in-hand model, the number
of players k can be arbitrary, one cannot simply rely on a computer for those
calculations. Indeed, we had to look into the protocol and show that it suﬃces
to analyze the behaviour of the information cost in a small time interval that is
determined by the distribution of the inputs, and furthermore one can reduce all
distributions into a class of essentially two-parameter distributions. Hence the
problem can be reduced to one that has only a constant number of variables, thus
allowing us to use Mathematica to verify the concavity condition. We believe our
analysis provides new insights even for the two-party setting.
2
Preliminaries
2.1
Notation
We typically denote the random variables by capital letters (e.g. A, B, C,
X, Y, Π), and write A1 . . . An to denote the random variable (A1, . . . , An). Let
[k] := {1, . . . , k}, and supp(μ) denote the support of a measure μ. We denote
the statistical distance (a.k.a. total variation distance) of two measures μ and ν
on a sample space Ω by |μ −ν| := 1
2

a∈Ω |μ(a) −ν(a)|. For every ε ∈[0, 1],
H(ε) = −ε log ε −(1 −ε) log(1 −ε) denotes the binary entropy, where here and
throughout the paper log(·) is in base 2, and 0 log 0 = 0. We also use H(X) to
denote the entropy of a random variable X.

Information Complexity of the AND Function in the Two-Party
203
Recall D(μ||ν) := 
a∈Ω μ(a) log μ(a)
ν(a) denotes the Kullback-Leibler diver-
gence (a.k.a. relative entropy) between two distributions μ and ν. Let X and
Y be two random variables, the standard notation I(X, Y ) := H(X) −H(X|Y )
means the mutual information between X and Y , we also use D(X||Y ) to denote
the divergence between the distributions of X and Y . For more on divergence
and mutual information, see [CT12].
2.2
Communication Complexity and Information Complexity
We brieﬂy review the notion of two-party communication complexity which was
introduced by Yao [Yao79], see [KN97] for detailed deﬁnitions. In this model
there are two players (with unlimited computational power), often called Alice
and Bob, who wish to collaboratively compute a given function f : X × Y →Z.
Alice receives an input x ∈X and Bob receives y ∈Y. Neither of them knows
the other player’s input, and they wish to communicate in accordance with an
agreed-upon protocol π to compute f(x, y). The protocol π speciﬁes as a function
of (only) the transmitted bits whether the communication is over, and if not,
who sends the next bit. Furthermore π speciﬁes what the next bit must be as a
function of the transmitted bits and the input of the player who sends the bit.
The transcript Π of a protocol π is the list of all the transmitted bits during the
execution of the protocol. In the randomized communication model, the players
have access to private random strings. These random strings are independent,
and they can have any desired distributions individually.
While communication complexity cares about the number of transmitted
bits, the information complexity cares about the information revealed by the
communication, see [BBCR10] for a detailed discussion. To be able to measure
information, we also need to assume that there is a prior distribution μ on X ×Y.
Deﬁnition 1. The external information cost and the internal information cost
of a protocol π with respect to a distribution μ on inputs from X ×Y are deﬁned as
ICext
μ (π) = I(Π; XY ) and ICμ(π) = I(Π; X|Y ) + I(Π; Y |X), respectively, where
Π = ΠXY is the transcript of the protocol when it is executed on the input XY .
Given a function f and ε ⩾0, we deﬁne
ICμ(f, ε) = inf
π ICμ(π),
(2)
where π computes the value of f(x, y) with error at most ε for every input (x, y),
i.e., Pr[π(x, y) ̸= f(x, y)] ⩽ε, for all (x, y) ∈X ×Y. Given another distribution
ν on X × Y, deﬁne
ICμ(f, ν, ε) = inf
π ICμ(π),
(3)
where π computes the value of f(x, y) with error at most ε if the input (x, y) is
sampled according to ν, i.e., Pr(x,y)∼ν[π(x, y) ̸= f(x, y)] ⩽ε.
In particular, ICμ(f, 0) is the minimal information cost of all protocols that
compute f correctly on every input, while ICμ(f, μ, 0) is the minimal informa-
tion cost of all protocols that compute f correctly on the support of μ. Hence
ICμ(f, μ, 0) can be strictly smaller than ICμ(f, 0). Denote ICμ(f) = ICμ(f, 0).

204
Y. Filmus et al.
Remark 1 (A warning regarding our notation). In the literature of information
complexity it is common to use “ICμ(f, ε)” to denote the distributional error
case, i.e. what we denote by ICμ(f, μ, ε). Unfortunately this has become the
source of some confusions in the past, as sometimes “ICμ(f, ε)” is used to denote
both of the distributional error [f, μ, ε] and the point-wise error [f, ε]. To avoid
ambiguity we distinguish these two cases by using diﬀerent notations ICμ(f, μ, ε)
and ICμ(f, ε).
It turns out that ICμ(f, ε) and ICμ(f, ν, ε) are both continuous with respect
to ε. For our purpose we need the uniform continuity.
Theorem 1 ([BGPW16, Lemma 4.4]). ICμ(f) is uniformly continuous with
respect to μ, under the statistical distance of distributions.
2.3
The Multi-party Number-In-Hand Model
The number-in-hand model is the most straightforward generalization of Yao’s
two-party model to the settings where more than two players are present. In
this model there are k players who wish to collaboratively compute a function
f : X1 × . . . × Xk →Z. The communication is in the shared blackboard model,
which means that all the communicated bits are visible to all the players. Let μ be
a probability distribution on X1×. . .×Xk, and let X = (X1, . . . , Xk) be sampled
from X1 × . . . × Xk according to μ. Deﬁnition 1 generalizes in a straightforward
manner to ICext
μ (π) = I(Π; X), and ICμ(π) = k
i=1 I(Π; X−i|Xi) where X−i :=
(X1, . . . , Xi−1, Xi+1, . . . , Xk). Note also that I(Π; X|Xi) = I(Π; X−i|Xi), and
thus we have ICμ(π) = k
i=1 I(Π; X|Xi). The notations ICμ(f), ICμ(f, ε), and
ICμ(f, ν, ε), and the continuity results also generalize in a straightforward man-
ner to this setting.
3
The Local Characterization of the Optimal Information
Cost
Let B be a random bit sent by one of the players, and let μ0 = μ|B=0 and
μ1 = μ|B=1, or in other words μb(xy) := Pr[XY = xy|B = b] for b = 0, 1.
Denote Pr[·|xy] := Pr[·|XY = xy].
Deﬁnition 2. Let μ be a distribution and B be a signal sent by one of the
players.
– B is called unbiased with respect to μ if Pr[B = 0] = Pr[B = 1] = 1
2.
– B is called non-crossing if μ(xy) < μ(x′y′) implies that μb(xy) ⩽μb(x′y′) for
b = 0, 1.
– B is called ε-weak if |Pr[B = 0|xy] −Pr[B = 1|xy]| ⩽ε for every input xy.
A protocol is said to be in normal form with respect to μ if all its signals are
unbiased and non-crossing with respect to μ.
Let Δ(X × Y) denote the set of distributions on X × Y. A measure μ ∈
Δ(X ×Y) is said to be internal-trivial (resp. external-trivial) for f if ICμ(f) = 0
(resp. ICext
μ (f) = 0). These measures are characterized in [DFHL16].

Information Complexity of the AND Function in the Two-Party
205
3.1
The Local Characterization
Suppose that after a random bit B is sent, if B = 0, the players continue by
running a protocol that is (almost) optimal for μ0, and if B = 1, they run a
protocol that is (almost) optimal for μ1. Note that the amount of information
that B reveals about the inputs to an external observer is I(B; XY ). This shows
ICext
μ (f) ⩽I(B; XY ) + E
B[ICext
μB (f)],
(4)
and similarly
ICμ(f) ⩽I(B; X|Y ) + I(B; Y |X) + E
B[ICμB(f)].
(5)
In [BGPW13] it is shown that (4) and (5) essentially characterize
ICext
μ (f, μ, 0) and ICμ(f, μ, 0). Denote Iext
B
:= I(B; XY ), and IB := I(B; X|Y ) +
I(B; Y |X).
Theorem 2 ([BGPW13]).
Suppose that C : Δ(X × Y) →[0, log(|X × Y|)]
satisﬁes
(i) C(μ) = 0 for every measure μ such that ICμ(f, μ, 0) = 0, and
(ii) for every signal B that can be sent by one of the players
C(μ) ⩽IB + E
B[C(μB)].
Then C(μ) ⩽ICμ(f, μ, 0). Similarly if ICμ(f, μ, 0) is replaced by ICext
μ (f, μ, 0),
and IB is replaced by Iext
B , then C(μ) ⩽ICext
μ (f, μ, 0). Furthermore, in both of
the external and the internal cases, it suﬃces to verify (ii) only for non-crossing
unbiased signals B.
In light of Theorem 2, in order to determine the values of ICμ(f, μ, 0), one
has to ﬁrst prove an upper bound by constructing a protocol (or a sequence of
protocols) for every measure μ. Then it suﬃces to verify that the bound satisﬁes
the conditions of Theorem 2.
A
Counterexample:
In
[BGPW13],
ICμ(AND2)
is
determined
using
Theorem 2. However, the proof presented in [BGPW13] contains a small gap. It
is claimed that it suﬃces to verify (2) for suﬃciently weak signals. While it is not
diﬃcult to see that indeed it suﬃces to verify (2) for signals B which are ε-weak
for an absolute constant ε > 0, in [BGPW13] the condition (ii) is only veriﬁed
for ε that is smaller than a function of μ. This is not suﬃcient, and one can
easily construct a counter-example by allowing the signal B to become increas-
ingly weaker as μ moves closer to the boundary (by boundary we mean the set
of measures μ that satisfy Theorem 2 (i)). Indeed, for example, set C(μ) = K
for a very large constant K if μ does not satisfy Theorem 2 (i), and otherwise
set C(μ) = 0. Obviously (2) holds if μ is on the boundary. On the other hand, if
μ is not on the boundary, then by taking B to be suﬃciently weak as a function
of μ, we can guarantee that μ0 and μ1 are not on the boundary either, and thus

206
Y. Filmus et al.
(2) holds in this case as well. However, taking K to be suﬃciently large violates
the desired conclusion that C(μ) ⩽ICμ(f, μ, 0).
To ﬁx this error, we start by noting that one can obtain a similar character-
ization of ICμ(f) (see the full version paper). Using the uniform continuity of
ICμ(f) with respect to μ, we prove that it suﬃces to verify Condition (ii) for
signals B that are weaker than quantities that can depend on μ. This as we shall
see suﬃces to ﬁx the proof of [BGPW13]. The proof of Theorem 3 is presented
in Sect. 3.3.
Theorem 3 (Main Theorem 1). Let w: (0, 1] →(0, 1] be a non-decreasing
function, Ω ⊆Δ(X × Y) be a subset of measures containing the internal trivial
distributions for function f. Let δ(μ) denote the distance of μ from Ω. Suppose
that C : Δ(X × Y) →[0, log(|X × Y|)] satisﬁes
(i) C(μ) is uniformly continuous with respect to μ;
(ii) C(μ) = ICμ(f) if δ(μ) = 0, and
(iii) for every non-crossing unbiased w(δ(μ))-weak signal B that can be sent by
one of the players,
C(μ) ⩽IB + E
B[C(μB)].
(6)
Then C(μ) ⩽ICμ(f). Similarly, if we replace Ω by a subset containing the
external trivial distributions for function f, in Condition (ii) replace ICμ(f) by
ICext
μ (f), and in Condition (iii) replace IB by Iext
B , then C(μ) ⩽ICext
μ (f).
3.2
Communication Protocols as Random Walks on Δ(X × Y)
Consider a protocol π and a prior distribution μ on X × Y. Suppose that in the
ﬁrst round Alice sends a random signal B to Bob, we can interpret this as a
random update of the prior distribution μ to a new distribution μ0 := μ|B=0 or
μ1 := μ|B=1 depending on the value of B. Similarly if Bob is sending a signal.
Therefore, we can think of a protocol as a random walk on Δ(X ×Y) that starts
at μ, and every time that a player sends a signal, it moves to a new distribu-
tion. The random walk terminates at μΠ := μ|Π. Since Π is random variable,
μΠ is also a random variable taking value in Δ(X × Y). Recall I(X; Π|Y ) =
Eπ∼Π,y∼Y D(X|Π=π,Y =y∥X|Y =y) and I(XY ; Π) = Eπ∼ΠD(XY |Π=π∥XY ), this
shows, interestingly, the distribution of μΠ (which is a distribution on the set
Δ(X × Y)) completely determines the information cost of the protocol π.
Proposition 1 [BS15]. Let π and τ be two communication protocols with the
same input set X ×Y endowed with a probability measure μ. Let Π and T denote
the transcripts of π and τ, respectively. If μΠ has the same distribution as μT ,
then ICμ(π) = ICμ(τ) and ICext
μ (π) = ICext
μ (τ).
Let CT
μ (Δ(X ×Y)) denote the set of all probability distributions on Δ(X ×Y)
that can be obtained, starting from the distribution μ, through communication
protocols that perform a given communication task T. The information cost of
performing the task T is the inﬁmum of the information costs of the distributions

Information Complexity of the AND Function in the Two-Party
207
in CT
μ (Δ(X ×Y)). Although this inﬁmum is not always attained (see [BGPW13]),
if one takes the closure of CT
μ (Δ(X ×Y)) (under weak convergence) then one can
replace the inﬁmum with minimum. For the 2-bit AND function, the buzzers
protocol of [BGPW13] yields the distribution in the closure of CT
μ (Δ(X × Y))
that achieves the minimum information cost. We believe that the following is an
important open problem.
Problem 1 Deﬁne a paradigm such that for every communication task T and
every measure μ on an input set X × Y, the set of distributions on Δ(X × Y)
resulting from the protocols performing the task T in this paradigm is exactly
equal to the closure of CT
μ (Δ(X × Y)).
Partial progress on this problem has been made in [DF16,DFHL16].
3.3
Proof of Theorem 3
We present the proof for the internal case only, as the external case is similar.
We need two technical lemmas (see proofs in the full version paper). The signal
simulation lemma says every signal can be perfectly simulated by a non-crossing
unbiased ε-weak signal sequence, it generalizes [BGPW13, Lemma 5.2].
Lemma 1 (Signal Simulation).
Let ε > 0, and consider μ ∈Δ(X × Y)
and a signal B sent by one of the players. There exists a sequence of non-
crossing unbiased ε-weak random signals B = (B1B2 . . .) that with probability 1
terminates, and furthermore μ|B has the same distribution as μ|B.
Lemma 2. Let w, δ(μ) and C be as in Theorem 3, and suppose C satisﬁes
Conditions (i), (ii) and (iii). Let τ be a protocol that terminates with proba-
bility 1, and further assume τ is in normal form and every signal sent in τ is
ε-weak. Given a probability distribution μ ∈Δ(X × Y), for every node u in the
protocol tree of τ, let μu be the probability distribution conditioned on the event
that the protocol reaches u. If μ satisﬁes w(δ(μu)) ⩾ε for every internal node
u, then
C(μ) ⩽ICμ(τ) + E
ℓ[C(μℓ)],
where the expected value is over all leaves ℓof τ chosen according to the distri-
bution (on the leaves) when the inputs are sampled according to μ.
Proof of Theorem 3. Firstly by (ii), δ(μ) = 0 implies C(μ) = ICμ(f) ⩽ICμ(f).
Hence assume δ(μ) > 0. Consider an arbitrary signal B sent by Alice. As we
discussed before, one can interpret B as a one step random walk that starts
at μ and jumps either to μ0 or to μ1 with corresponding probabilities Pr[B =
0|X = x] and Pr[B = 1|X = x]. The idea behind the proof is to use Lemma 1 to
simulate this random jump with a random walk that has smaller steps so that
we can apply the concavity assumption of the theorem to those steps.
Let π be a protocol that computes [f, 0]. For 0 < η < δ(μ), applying Lemma 1
one gets a new protocol ˜π by replacing every signal sent in π with a random walk

208
Y. Filmus et al.
consisting of w(η)-weak non-crossing unbiased signals. Note ˜π terminates with
probability 1. Moreover, since ˜π is a perfect simulation of π, by Proposition 1
we have ICμ(π) = ICμ(˜π).
For every node v in the protocol tree of ˜π, let μv be the measure μ condi-
tioned on the event that the protocol reaches the node v. Obtain τ from ˜π by
terminating at every node v that satisﬁes δ(μv) ⩽η. Note that by the construc-
tion, Condition (iii) is satisﬁed on every internal node v of τ, as every such node
satisﬁes η < δ(μv), thus w(η) ⩽w(δ(μv)) implying the signal sent on node v is
w(δ(μv))-weak. Hence by Lemma 2,
C(μ) ⩽ICμ(τ) + E
ℓ[C(μℓ)],
where the expected value is over all leaves of τ. For every μℓ, let μ′
ℓ∈Ω be
a distribution such that δ(μℓ) = |μℓ−μ′
ℓ|. By Conditions (i) and (ii), and the
uniform continuity of ICμ(f), we have that for every ε > 0 there exists η > 0,
such that for all μℓ, as long as δ(μℓ) = |μℓ−μ′
ℓ| ⩽η, then
C(μℓ) ⩽C(μ′
ℓ) + ε = ICμ′
ℓ(f) + ε ⩽ICμℓ(f) + ε + ε = ICμℓ(f) + 2ε.
As a result, C(μ) ⩽ICμ(τ)+Eℓ[ICμℓ(f)+2ε] = ICμ(τ)+Eℓ[ICμℓ(f)]+2ε. Since μℓ
is generated by truncating ˜π, we have ICμ(τ) + Eℓ[ICμℓ(f)] ⩽ICμ(˜π) = ICμ(π).
Therefore C(μ) ⩽ICμ(π) + 2ε. As this holds for arbitrary ε, we must have
C(μ) ⩽ICμ(π).
4
The Multi-party AND Function in the Number-In-Hand
Model
We now show that the buzzers protocol proposed in [BGPW13] can be general-
ized to the multi-party number-in-hand setting. Denote μx := μ({x}) for every
x ∈{0, 1}k, and assume that μe1 ⩽. . . ⩽μek.
Fig. 1. The protocol π∧
µ for solving the multi-party AND function on a distribution μ.
For every i ∈[k], if xi = 0 then the i-th player activates a buzzer at time ti
and becomes “active”. The protocol terminates with k
i=1 xi = 0 once the ﬁrst
buzz happens, otherwise the time reaches ∞without anyone buzzing, and they
decide k
i=1 xi = 1.

Information Complexity of the AND Function in the Two-Party
209
Theorem 4 (Main Theorem 2).
For every μ satisfying Assumption 1, the
protocol π∧
μ has the smallest external and internal information cost.
Observe that a direct calculation as in [BGPW13] is now simply impossible
due to the nature of multi-party setting where the number of players k can be
arbitrary. We have to reduce the problem to one that has only a constant number
of variables before a computation can be performed.
4.1
Setting up
Let μ be a measure satisfying Assumption 1, and X = (X1, . . . , Xk) be the
random k-bit input. Let Π be the transcript of protocol π∧
μ, and Πx = Π|X=x.
To verify the concavity condition, consider an unbiased signal B with para-
meter ε sent by the player s. That is Pr[B = 0|Xs = 0] =
1+εPr[Xs=1]
2
and
Pr[B = 1|Xs = 1] = 1+εPr[Xs=0]
2
. Let μ0 and μ1 denote the distributions of
X0 := X|B=0 and X1 := X|B=1, then μ = μ0+μ1
2
. Let Π0 and Π1 denote the
random variables corresponding to the transcripts of π∧
μ0 and π∧
μ1, respectively.
Note Π contains the termination time t, and if t < ∞, also the name of
the player who ﬁrst buzzed. We denote by π∞the transcript corresponding to
termination time t = ∞, and by πm
t
the termination time t < ∞with the m-th
player buzzing. For t ∈[0, ∞), let Φx(t) denote the total amount of active time
spent by all players before time t if the input is x. For tr ⩽t < tr+1, we have
Φx(t) = 
i:xi=0 max(t −ti, 0) = 
i∈[1,r],xi=0(t −ti). The probability density
function fx of Πx satisﬁes fx(πm
t ) = 0 if tm > t or xm = 1, and equals to e−Φx(t)
otherwise. Also Pr(Π1 = π∞) = 1. The distribution of the transcript Π is then
f(πm
t ) = 
x μxfx(πm
t ). Deﬁne f 0, f 0
x and f 1, f 1
x analogously for π∧
μ0 and π∧
μ1.
Denote βs := Pr[Xs = 1], and ζs := Pr[Xs = 0]. For B = 0, the new starting
times are t0
i = ti for i ̸= s, and t0
s = ts −γ0 where γ0 = ln

1+εβs
1−εζs

. On the
other hand, for B = 1, we have the new starting times are t1
i = ti for i ̸= s, and
t1
s = ts + γ1 where γ1 = ln

1+εζs
1−εβs

.
Let φ(x) := x ln(x). The concavity conditions of Theorem 3 reduce to
 ∞
−∞

m
	
φ(f(πm
t )) −φ(f 0(πm
t )) + φ(f 1(πm
t ))
2

−

m

x
	
φ(μxfx(πm
t )) −φ(μ0
xf 0
x(πm
t )) + φ(μ1
xf 1
x(πm
t ))
2

dt ⩾0,
(7)
for the external case, and
k

j=1
 ∞
−∞

m
1

b=0

φ(fxj=b(πm
t )) −
φ(f 0
xj=b(πm
t )) + φ(f 1
xj=b(πm
t ))
2

−

m

x
	
φ(μxfx(πm
t )) −φ(μ0
xf 0
x(πm
t )) + φ(μ1
xf 1
x(πm
t ))
2

dt ⩾0, (8)

210
Y. Filmus et al.
for the internal case, where fxj=b(πm
t ) := 
X:Xj=b μXfX(πm
t ), and f 0
xj=b(π
m
t ) := 
X:Xj=b μ0
Xf 0
X(πm
t ), f 1
xj=b(πm
t ) := 
X:Xj=b μ1
Xf 1
X(πm
t ).
Denote the function inside the integral of (8) by concavμ(t, j), and the func-
tion inside the integral of (7) by concavext
μ (t). Hence to prove Theorem 4 it
suﬃces to verify
 ∞
−∞concavext
μ (t)dt ⩾0 and k
j=1
 ∞
−∞concavμ(t, j)dt ⩾0.
4.2
Reductions
Our ﬁrst reduction is to deal with the uniform distribution on e1, . . . , ek. In this
measure all players are active at t = 0 and the protocol stops if one play buzzes
or at inﬁnity. For technical reasons, we need to analyze this particular measure
separately. Later we will put this μ into Ω as in Theorem 3.
Statement 1. Let μ be the measure μe1 = . . . = μek = 1/k. The internal and
external information cost of the protocol π∧is optimal with respect to μ.
Observe that all the information is revealed if the input 1, this degeneracy
can be eliminated.
Statement 2. It suﬃces to assume μ satisfying μ(1) = 0.
Using the memoryless property of exponential distribution, one can
shift the activation time of all the players by −ln(μes/μe1), and assume that
t1 = −ln(μes/μe1), . . . , ts = 0, . . . , tk = ln(μek/μes). Our third major reduc-
tion says that it suﬃces to focus on the time interval t ∈[−γ0, γ1] instead
of (−∞, ∞). This is achieved by showing that
 −γ0
−∞concavext
μ (t)dt ⩾0 and
 ∞
γ1 concavext
μ (t)dt = 0. Similarly for the internal case.
Statement 3. It suﬃces to assume μ satisﬁes μ(1)
=
0, and verify
 γ1
−γ0 concavext
μ (t)dt ⩾0 and k
j=1
 γ1
−γ0 concavμ(t, j)dt ⩾0.
Lastly we reduce all the distributions into a class of essentially two-parameter
distributions. Firstly we observe that conditioned on the buzz time t ∈[−γ0, γ1],
we have μe1|t⩾ts−γ0 = · · · = μes−1|t⩾ts−γ0. Secondly, we show that one can
transfer the mass on those ej such that μej > μes to μ0, without harming the
concavity condition we aimed to verify.
Statement 4. It suﬃces to assume μ satisﬁes μe1 = · · · = μes−1 = β, μes =
· · · = μek = eγ0β, and μ0 = 1 −(s −1)β −(k −s + 1)eγ0β, where 0 < β < 1.
4.3
Proof of Theorem 4
Set Ω to be the set of all external (resp. internal) trivial measures together with
the measure in Statement 1, obviously Condition (i) and (ii) are satisﬁed.
External Information Cost. Set w(x) = ck−20x4 for some ﬁxed constant
c > 0. Using Wolfram Mathematica, we obtain
(7) ⩾(k + 5s −6)(1 −2β)β
12(1 −β) ln 2
ε3 + O(ε4).
(9)

Information Complexity of the AND Function in the Two-Party
211
Using the bound of the error term given in the full version paper, one can verify
the concavity condition (7) is satisﬁed for all w(δ(μ))-weak signals as long as
ε < ck−20 min{β, 1 −kβ}3. Hence by Theorem 3, the protocol is optimal for
external information cost. More details are presented in the full version paper.
Internal Information Cost. Similarly, using Wolfram Mathematica, we obtain
(8) ⩾
 (k+5s−6)(1−2β)β
12(1−β) ln 2
ε3 + O(ε4),
k = 2,
(k+5s−6)((3k−2)β2−4(k−1)β+k−1)β
12(1−β)(1−2β) ln 2
ε3 + O(ε4),
k ⩾3.
(10)
Similarly one can pick an appropriate function w to verify that the concavity
condition (8) is satisﬁed for all w(δ(μ))-weak signals when ε is suﬃciently small.
Hence by Theorem 3, the protocol is optimal for internal information cost.
More detailed analysis can be found in the full version of our paper.
References
[BBCR10] Barak, B., Braverman, M., Chen, X., Rao, A.: How to compress interac-
tive communication [extended abstract]. In: STOC 2010, pp. 67–76. ACM,
New York (2010)
[BGPW13] Braverman, M., Garg, A., Pankratov, D., Weinstein, O.: From information
to exact communication (extended abstract). In: STOC 2013, pp. 151–160.
ACM, New York (2013)
[BGPW16] Braverman, M., Garg, A., Pankratov, D., Weinstein, O.: Information lower
bounds via self-reducibility. Theory Comput. Syst. 59(2), 377–396 (2016)
[BR14] Braverman, M., Rao, A.: Information equals amortized communication.
IEEE Trans. Inform. Theory 60(10), 6058–6069 (2014)
[BS15] Braverman, M., Schneider, J.: Information complexity is computable,
arXiv preprint arXiv:1502.02971 (2015)
[CT12] Cover, T.M., Thomas, J.A.: Elements of Information Theory. Wiley,
New York (2012)
[DF16] Dagan, Y., Filmus, Y.: Grid protocols (2016, in preparation)
[DFHL16] Dagan, Y., Filmus, Y., Hatami, H., Li, Y.: Trading information complexity
for error, arXiv preprint arXiv:1611.06650 (2016)
[Gro09] Gronemeier, A.: Asymptotically optimal lower bounds on the nih-multi-
party information, arXiv preprint arXiv:0902.1609 (2009)
[KN97] Kushilevitz,
E.,
Nisan,
N.:
Communication
Complexity.
Cambridge
University Press, Cambridge (1997)
[MI11] Ma, N., Ishwar, P.: Some results on distributed source coding for interac-
tive function computation. IEEE Trans. Inform. Theory 57(9), 6180–6195
(2011)
[MI13] Ma, N., Ishwar, P.: The inﬁnite-message limit of two-terminal interactive
source coding. IEEE Trans. Inform. Theory 59(7), 4071–4094 (2013)
[Raz92] Razborov, A.A.: On the distributional complexity of disjointness. Theoret.
Comput. Sci. 106(2), 385–390 (1992)
[Yao79] Yao, A.C.-C.: Some complexity questions related to distributive computing
(preliminary report). In: STOC 1979, pp. 209–213. ACM (1979)

Optimal Online Two-Way Trading
with Bounded Number of Transactions
Stanley P.Y. Fung(B)
Department of Informatics, University of Leicester, Leicester LE1 7RH, UK
pyf1@leicester.ac.uk
Abstract. We consider a two-way trading problem, where investors buy
and sell a stock whose price moves within a certain range. Naturally they
want to maximize their proﬁt. Investors can perform up to k trades,
where each trade must involve the full amount. We give optimal algo-
rithms for three diﬀerent models which diﬀer in the knowledge of how
the price ﬂuctuates. In the ﬁrst model, there are global minimum and
maximum bounds m and M. We ﬁrst show an optimal lower bound of
ϕ (where ϕ = M/m) on the competitive ratio for one trade, which is
the bound achieved by trivial algorithms. Perhaps surprisingly, when we
consider more than one trade, we can give a better algorithm that loses
only a factor of ϕ2/3 (rather than ϕ) per additional trade. Speciﬁcally,
for k trades the algorithm has competitive ratio ϕ(2k+1)/3. Furthermore
we show that this ratio is the best possible by giving a matching lower
bound. In the second model, m and M are not known in advance, and
just ϕ is known. We show that this only costs us an extra factor of ϕ1/3,
i.e., both upper and lower bounds become ϕ(2k+2)/3. Finally, we con-
sider the bounded daily return model where instead of a global limit, the
ﬂuctuation from one day to the next is bounded, and again we give opti-
mal algorithms, and interestingly one of them resembles common trading
algorithms that involve stop loss limits.
1
Introduction
The model. We consider a scenario commonly faced by investors. The price of a
stock varies over time. In this paper we use a ‘day’ as the smallest unit of time,
so there is one new price each day. Let p(i) be the price at day i. The investor
has some initial amount of money. Over a time horizon of ﬁnite duration, the
investor wants to make a bounded number of trades of this one stock. Each trade
(b, s) consists of a buy transaction at day b, followed by a sell transaction at day
s where s > b. (Thus one trade consists of two transactions.) Both transactions
are ‘all-in’: when buying, the investor uses all the money available, and when
selling all stock they currently own is sold. A sale must be made before the next
purchase can take place. Also, no short selling is allowed, i.e., there can be no
selling if the investor is not currently holding stock. When the end of the time
horizon is reached, i.e., on the last day, no buying is allowed and the investor
must sell oﬀall the stocks that they still hold back to cash at the price of the day.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 212–223, 2017.
DOI: 10.1007/978-3-319-62389-4 18

Optimal Online Two-Way Trading with Bounded Number of Transactions
213
There are a number of rationales for considering a bounded number of trades
and/or that trades must involve all the money available. Individual, amateur
investors typically do not want to make frequent transactions due to high trans-
action fees. Often transaction fees have a ﬁxed component (i.e., a ﬁxed amount
or a minimum tariﬀper transaction, irrespective of the trading amount) which
makes transaction fees disproportionally high for small trades. Frequent trad-
ing also requires constant monitoring of the markets which amateur investors
may not have the time or resources for; often they only want to change their
investment portfolios every now and then. Also, for investors with little money
available, it is not feasible or sensible to divide them into smaller pots of money,
in arbitrary fractions as required by some algorithms. The ﬁniteness of the time
horizon (and that its length is possibly unknown as well) corresponds to sit-
uations where an investor may be forced to sell and leave the market due to
unexpected need for money elsewhere, for example.
Each trade with a buying price of p(b) and a selling price of p(s) gives a gain
of p(s)/p(b). This represents how much the investor has after the trade if they
invested 1 dollar in the beginning. Note that this is a ratio, and can be less than
1, meaning there is a loss, but we will still refer to it as a ‘gain’. If a series of
trades are made, the overall gain or the return of the algorithm is the product
of the gains of each of the individual trades. This correctly reﬂects the fact that
all the money after each trade is re-invested in the next.
Since investors make decisions without knowing future stock prices, the prob-
lem is online in nature. We measure the performance of online algorithms with
competitive analysis, i.e., by comparing it with the optimal oﬄine algorithm
OPT that knows the price sequence in advance and can therefore make optimal
decisions. The competitive ratio of an online algorithm ONL is the worst possible
ratio of the return of OPT to the return of ONL, over all possible input (price)
sequences. The multiplicative nature of the deﬁnition of the return (instead of
specifying a negative value for a loss) means that the competitive ratio can be
computed in the normal way in the case of a loss: for example, if OPT makes a
gain of 2 and ONL makes a ‘gain’ of 1/3, then the competitive ratio is 6.
Three models on the knowledge of the online algorithm. We consider three dif-
ferent models on how the price changes, or equivalently, what knowledge the
online algorithm has in advance. In the ﬁrst model, the stock prices are always
within a range [m..M], i.e., m is the minimum possible price and M the maxi-
mum possible price. Both m and M are known to the online algorithm up front.
In the second model, the prices still ﬂuctuate within this range, but m and M
are not (initially) known; instead only their ratio ϕ = M/m, called the ﬂuc-
tuation ratio, is known. In both these models the length of the time horizon
(number of days) is unknown (until the ﬁnal day arrives). In the third model,
called the bounded daily return model, there is no global minimum or maximum
price. Instead, the maximum ﬂuctuation from day to day is bounded: namely,
the price p(i + 1) of the next day is bounded by the price p(i) of the current
day by p(i)/β ≤p(i + 1) ≤αp(i) for some α, β > 1. This means the prices can-
not suddenly change a lot. Many stock markets implement the so-called ‘circuit

214
S.P.Y. Fung
breakers’ where trading is stopped when such limits are reached. Here α, β and
the trade duration T are known to the online algorithm. All three models are
well-established in the algorithms literature; see e.g. [1,5].
Previous results and related work. Financial trading and related problems are
obviously important topics and have been much studied from the online algo-
rithms perspective. A comprehensive survey is given in [8]. Here we only sample
some of the more important results and those closer to the problems we study
here. In the one-way search problem, the online player chooses one moment of
time to make a single transaction from one currency to another currency. Its
return is simply the price at which the transaction takes place. A reservation
price (RP) based policy is to buy as soon as the price reaches or goes above a
pre-set reservation price. It is well-known that, if m and M are known, the RP
policy with a reservation price of
√
Mm is optimal and achieves a competitive
ratio of √ϕ. If only ϕ is known, then no deterministic algorithm can achieve a
ratio better than ϕ. With the help of randomization, however, a random mix of
diﬀerent RPs gives a competitive ratio of O(log ϕ) if ϕ is known. Even if ϕ is
not known, a competitive ratio of O(log ϕ · log1+ϵ(log ϕ)) can be achieved. See
[5] for all the above results and more discussions.
In the one-way trading problem, the objective is again to maximize the return
in the other currency, but there can be multiple transactions, i.e., not all the
money has to be traded in one go. (This distinction of terminology between
search and trading is used in [5], but is called non-preemptive vs. preemptive in
[8]. We prefer calling them unsplittable vs. splittable here.) The relation between
one-way trading and randomized algorithms for one-way search is described in
[5]. Many variations of one-way search or one-way trading problems have since
been studied; some examples include the bounded daily return model [1,11],
searching for k minima/maxima instead of one [7], time-varying bounds [4],
unbounded prices [2], search with advice complexity [3], etc.
What we study here, however, is a two-way version of the unsplittable trading
problem1, which is far less studied. Here the online player has to ﬁrst convert
from one currency (say cash) to another (a stock), hopefully at a low price, and
then convert back from the stock to cash at some later point, hopefully at a
high price. All the money must be converted back to the ﬁrst currency when or
before the game ends. This model is relevant where investors are only interested
in short term, speculative gains. For the models with known m, M or known ϕ
and with one trade, Schmidt et al. [9] gave a ϕ-competitive algorithm; it uses
the same RP for buying and selling. But consider the Do-Nothing algorithm
that makes no trades at all. Clearly it is also ϕ-competitive as ONL’s gain is
1 and OPT’s gain is at most ϕ (if the price goes from m to M). A number of
common algorithms, such as those based on moving averages, were also studied
in [8]. It was shown that they are ϕ2-competitive (and not better), which are
therefore even worse. It is easy to show that these algorithms have competitive
1 In the terminology of [5] this should be called ‘two-way search’, but we feel that the
term does not convey its application in stock market trading.

Optimal Online Two-Way Trading with Bounded Number of Transactions
215
ratios ϕk and ϕ2k respectively when extended to k trades. Schroeder et al. [10]
gave some algorithms for the bounded daily return model, without limits on the
number of trades. However, most of these algorithms tend to make decisions that
are clearly bad, have the worst possible performance (like losing by the largest
possible factor every day throughout), and have competitive ratios no better
than what is given by Do-Nothing.
Our results. In this paper we consider the two-way unsplittable trading prob-
lem where a bounded number k of trades are permitted, and derive optimal
algorithms. First we consider the model with known m and M. We begin by
considering the case of k = 1. Although some naive algorithms are known to
be ϕ-competitive and seemingly nothing better is possible, we are not aware of
any matching general lower bound. We give a general lower bound of ϕ, show-
ing that the naive algorithms cannot be improved. The result is also needed in
subsequent lower bound proofs.
It may be tempting to believe that nothing can beat the naive algorithm
also for more trades. Interestingly, we prove that for k ≥2 this is not true.
While naive algorithms like Do-Nothing are no better than ϕk-competitive,
we show that a reservation price-based algorithm is ϕ(2k+1)/3-competitive. For
example, when k = 2, it is ϕ5/3-competitive instead of trivially ϕ2-competitive.
Furthermore, we prove a matching lower bound, showing that the algorithm is
optimal.
Next, we consider the model where only ϕ is known, and give an algorithm
with a competitive ratio of ϕ(2k+2)/3, i.e., only a factor ϕ1/3 worse than that of
the preceding model. Again we show that this bound is optimal.
Finally we consider the bounded daily return model, and give two optimal
algorithms where the competitive ratio depends on α, β and T. For example,
with one trade and in the symmetric case α = β, the competitive ratio is α2T/3.
While this is exponential in T (which is unavoidable), naive algorithms could
lose up to a factor of max(α, β) every day, and Do-Nothing has a competitive
ratio of αT . One of the algorithms uses the ‘stop loss / lock proﬁt’ strategy
commonly used in real trading; as far as we are aware, this is the ﬁrst time
where competitive analysis justiﬁes this common stock market trading strategy,
and in fact suggests what the stop loss limit should be.
Some proofs are omitted due to space constraints.
2
Known m and M
In this section, where we consider the model with known m and M, we can with-
out loss of generality assume that m = 1. This is what we will do to simplify
notations. It also means M and ϕ are equal and are sometimes used interchange-
ably.
Theorem 1. For k = 1, no deterministic algorithm has a competitive ratio
better than ϕ1−ϵ, for any ϵ > 0.

216
S.P.Y. Fung
Proof. Choose n = ⌈1/ϵ⌉and deﬁne vi = M i/n for i = 0, 1, . . . , n. The following
price sequence is released until ONL buys: vn−1, M, vn−2, M, . . . , vi, M, . . . , v1,
M, v0. If ONL does not buy at any point, or buys at price M, then its return
is at most 1. Then OPT buys at v1 and sells at M to get a return of M 1−1/n.
So suppose ONL buys at vi for some 1 ≤i ≤n −1. (It cannot buy at v0 as it
is the last time step.) As soon as ONL bought, the rest of the sequence is not
released; instead the price drops to m and the game ends. ONL’s return is m/vi.
If i = n −1, then OPT makes no trade and its return is 1, so competitive ratio
= vn−1/m = M 1−1/n. Otherwise, if i < n −1, OPT buys at vi+1 (two days
before ONL’s purchase) and sells at the next day at price M, giving a return of
M/vi+1. The competitive ratio is therefore Mvi/(mvi+1) = M 1−1/n.
Thus in all cases the competitive ratio is at least M 1−1/n ≥ϕ1−ϵ.
⊓⊔
Note that the proof does not require ONL to use only one trade: it cannot
beneﬁt even if it is allowed to use more trades. This fact will be used later in
Theorems 3 and 5.
For k > 1, we analyze the following algorithm:
Algorithm 1. The reservation price algorithm, with known price range [m..M]
Upon release of the i-th price p(i):
if i = n then
if currently holding stock then
sell at price p(n) and the game ends
else
if p(i) ≤M 1/3 and currently not holding stock and not used up all trades then
buy at price p(i)
else if p(i) ≥M 2/3 and currently holding stock then
sell at price p(i)
Theorem 2. Algorithm 1 has competitive ratio ϕ(2k+1)/3, for k ≥1.
Proof. First we make a few observations. We call a trade winning if its gain is
higher than 1, and losing otherwise. Any winning trade made by ONL has a gain
of at least M 1/3. If the algorithm makes a losing trade, it must be a forced sale
at the end and the gain is not worse than M −1/3. Moreover, it follows that the
algorithm cannot trade anymore after it.
We consider a number of cases separately based on the sequence of win/loss
trades. If we use W and L to denote a winning and a losing trade respectively,
then following the above discussion, the possible cases are: nil (no trade), L,
W jL for 1 ≤j ≤k −1, and W j for 1 ≤j ≤k. (Here W j denotes a sequence of
j consecutive W’s).
Case nil: Since ONL has never bought, the prices were never at or below M 1/3
(except possibly the last one, but neither OPT nor ONL can buy there) and
hence OPT’s return cannot be better than (M/M 1/3)k = M 2k/3. ONL’s
return is 1. So the competitive ratio is at most M 2k/3.

Optimal Online Two-Way Trading with Bounded Number of Transactions
217
M2/3
M1/3
M1/3
M2/3
s1
b1
b1
b1
b2
b1
b2
s1
s1
s2
M
M
M
m(=1)
m(=1)
M
m(=1)
Fig. 1. Four cases illustrated, for k = 2. Horizontal axis is time, vertical axis is price.
Shaded regions are the regions where the prices cannot fall into. Green solid arrows
depict possible buying and selling actions of ONL, red dashed arrows for OPT. Top
left: case L, Top right: case W, Bottom left: case WL, Bottom right: case WW. (Color
ﬁgure online)
Case L: Suppose ONL buys at time b1 and is forced to sell at the end. The prices
before b1 cannot be lower than M 1/3 (or else it would have bought) and the
prices after b1 cannot be higher than M 2/3 (or else it would have sold). Thus,
it is easy to see (Fig. 1) that OPT cannot make any trade with gain higher
than M 2/3. So the competitive ratio is at most (M 2/3)k/M −1/3 = M (2k+1)/3.
Case W: Suppose ONL buys at time b1 and sells at time s1. Then before b1,
the prices cannot be below M 1/3; between b1 and s1, the prices cannot be
higher than M 2/3; and after s1, the prices cannot be lower than M 1/3. It
can be seen from Fig. 1 that OPT can make at most one trade with gain M
(crossing time s1); any other trade it makes must be of gain at most M 2/3.
So the competitive ratio is at most M(M 2/3)k−1/M 1/3 = M 2k/3.
Case W jL, 1 ≤j ≤k −1: Similarly, we can partition the timeline into regions
(Fig. 1), from which we can see that OPT can make at most j trades of
gain M and the rest have gain at most M 2/3. Thus competitive ratio =
(M j(M 2/3)k−j)/((M 1/3)jM −1/3) = M (2k+1)/3.
Case W j, 1 < j ≤k −1: This can only be better than the previous case, as
OPT again can make at most j trades of gain M and the rest have gain at
most M 2/3, but ONL’s return is better than the previous case.
Case W k: In this case the competitive ratio is simply M k/(M 1/3)k = M 2k/3. ⊓⊔
Theorem 3. No deterministic algorithm has a competitive ratio better than
ϕ(2k+1)/3−ϵ, for any ϵ > 0 and k ≥1.
Proof. The prices are released in up to k rounds. The ﬁnal round k is a special
round. For all other rounds, we maintain the following invariants. For each 1 ≤

218
S.P.Y. Fung
i ≤k −1, just before the i-th round starts, OPT completed exactly i −1 trades,
is holding no stock, and accumulated a return of exactly M i−1, while ONL
completed at most i−1 trades, is holding no stock, and accumulated a return of
at most M (i−1)/3. So the competitive ratio up to this point is at least M 2(i−1)/3.
For any i < k, round i begins with the price sequence M 1/3, M, M 1/3, M, . . .
until either ONL buys or k −i such pairs of oscillating prices have been released.
If ONL does not buy at any point, then the round ends. Clearly, ONL maintains
its variants. OPT makes k −i trades giving a total gain of (M 2/3)k−i in this
round, and thus the accumulated competitive ratio is M 2(k−1)/3. It also used
(i −1) + (k −i) = k −1 trades. Any remaining intermediate rounds are then
skipped and we jump directly to the special last round k.
Otherwise, assume ONL buys at one of the M 1/3 prices (M is clearly even
worse). The rest of that sequence will not be released. Instead, the price sequence
that follows is m, M 2/3, m, M 2/3, . . . until either ONL sells or k−i+1 such pairs
of oscillating prices were released. If ONL does not sell at any of these, then
the price drops to m and the game ends (with no further rounds, not even the
special round). ONL’s gain in this round is M −1/3. OPT uses all its remaining
k −i + 1 trades and gains (M 2/3)k−i+1. Combining with the previous rounds,
the competitive ratio is at most M 2(i−1)/3M 2(k−i+1)/3/M −1/3 = M (2k+1)/3.
Otherwise ONL sells at one of the M 2/3 prices (m is even worse). The rest
of that sequence will not be released; instead the price goes up to M and this
round ends. OPT’s gain in this round is M by making one trade from m to M;
ONL gains M 1/3. Thus the invariants are maintained and we move on to the
next round. (Regarding the invariant that ONL is not holding stock at the end
of the round, we can assume w.l.o.g. that ONL does not buy at the last price
M, since clearly it cannot make a proﬁt doing so. In any case, even if it does
buy, it can be treated as if it were buying at the beginning of the next round).
Finally, if we arrive at round k, then the same price sequence as in Theorem 1
is used to give an additional factor of M 1−ϵ to the competitive ratio. Note that
at the start of this round, OPT has one trade left, and ONL has one or more
trades left, but that will not help. Thus the competitive ratio is not better than
M 2(k−1)/3+1−ϵ = M (2k+1)/3−ϵ.
⊓⊔
3
Known ϕ only
For k = 1 Do-Nothing is clearly still ϕ-competitive, and Theorem 1 still applies
here, so we focus on k > 1. We adapt Algorithm 1 by buying only when it is
certainly ‘safe’, i.e., when it is certain that the price is within the lowest ϕ1/3
of the actual price range, and sells when it gains ϕ1/3. The formal description is
given in Algorithm 2. Let Mt be the maximum price observed up to and including
day t. Note that Mt is a stepwise increasing function of t.
Theorem 4. Algorithm 2 has competitive ratio ϕ(2k+2)/3, for any k ≥2.
Proof. Clearly ONL gets the same as in Theorem 2: each winning trade has gain
at least ϕ1/3 and a losing trade, which can only appear as the last trade, has
gain at least ϕ−1/3. The diﬀerence is in how we bound OPT’s gain.

Optimal Online Two-Way Trading with Bounded Number of Transactions
219
Algorithm 2. The reservation price algorithm, with known ϕ
Upon release of the i-th price p(i):
if i = n then
if currently holding stock then
sell at price p(n) and the game ends.
else
if i = 1 then
M1 := p(1)
else
Mi := max(Mi−1, p(i))
if p(i) ≤Mi/ϕ2/3 and currently not holding stock and not used up all trades then
buy at price p(i)
else if currently holding stock bought at price p(b) and p(i) ≥ϕ1/3p(b) then
sell at price p(i)
In the case of W k (ONL makes k winning trades) then the same argument
as Theorem 2 applies, so in the following we only consider the case where ONL
did not use up all its trade, i.e., it is always able to buy if it is not holding.
A sell event happens at a day when ONL sells and makes a proﬁt (i.e.,
excludes the forced sale at the end). An M-change event happens at day t when
Mt changes. Each OPT trade (b∗, s∗) can be classiﬁed into one of the following
types:
(1) There is at least one sell event during [b∗, s∗]. Clearly the number of such
OPT trades is limited by the number of sell events. Each such trade can
gain up to ϕ.
(2) There is no sell event during [b∗, s∗], and at b∗ONL is holding or buy-
ing. Suppose ONL’s most recent purchase is at time b ≤b∗. Then p(b) ≤
Mb/ϕ2/3 ≤Mb∗/ϕ2/3. It is holding stock throughout and still did not sell
at s∗(or is forced to sell if s∗is the last day), hence p(s∗) < p(b)ϕ1/3 ≤
Mb∗/ϕ1/3. But clearly p(b∗) ≥Mb∗/ϕ, hence the gain of OPT is at most
ϕ2/3.
(3) There is no sell event during [b∗, s∗], at b∗ONL is neither holding nor buying,
and there is no M-change event in (b∗, s∗]. We have p(b∗) > Mb∗/ϕ2/3 as
otherwise ONL would have bought at b∗. Clearly p(s∗) ≤Ms∗= Mb∗. Hence
the gain of OPT is p(s∗)/p(b∗) < ϕ2/3.
(4) There is no sell event during [b∗, s∗], at b∗ONL is neither holding nor buying,
and there is/are M-change event(s) in (b∗, s∗]. Suppose there are a total of x
such OPT trades, (b∗
1, s∗
1), (b∗
2, s∗
2), . . . , (b∗
x, s∗
x), in chronological order. Note
that p(b∗
i ) > Mb∗
i /ϕ2/3 or else ONL would have bought at b∗
i . So for all
i, p(b∗
i+1) > Mb∗
i+1/ϕ2/3 ≥Ms∗
i /ϕ2/3 ≥p(s∗
i )/ϕ2/3. Thus the total gain of
these x trades is
x

i=1
p(s∗
i )
p(b∗
i ) =
1
p(b∗
1)
p(s∗
1)
p(b∗
2) · · · p(s∗
x−1)
p(b∗x)
p(s∗
x)
1
< p(s∗
x)
p(b∗
1) (ϕ2/3)x−1 ≤ϕ(2x+1)/3.

220
S.P.Y. Fung
Suppose ONL makes y winning trades and one losing trade. Then OPT makes
at most y trades of type (1), gaining at most ϕy from those. Then, if x of OPT’s
trades are of type (4), they in total gives another gain of at most ϕ(2x+1)/3. The
remaining trades are of types (2) and (3), gaining ϕ2/3 each. The competitive
ratio is therefore at most
ϕyϕ(2x+1)/3ϕ2(k−x−y)/3
ϕy/3ϕ−1/3
= ϕ(2k+2)/3.
If ONL makes y < k winning trades and no losing trade, the competitive ratio
can only be better, as OPT’s return is as above but ONL’s is ϕ1/3 better.
⊓⊔
Theorem 5. No deterministic algorithm is better than ϕ(2k+2)/3−ϵ-competitive,
for any ϵ > 0 and k ≥2.
Proof. Again there will be a number of rounds. Round 1 is special, in that OPT
will get a factor of ϕ better than ONL but will afterwards reveal knowledge of
m and M. Rounds 2 to k are then similar to Theorem 3.
Round 1: The ﬁrst price is 1. If ONL does not buy, then the price goes up to
ϕ. OPT makes one trade and gains ϕ. Now we know the range is [1..ϕ], and we
can assume w.l.o.g. that ONL does not buy at ϕ. Then the round ends. At the
end of this round, both OPT and ONL are not holding stock, OPT made one
trade and ONL none, but ONL is a factor of ϕ behind in the return.
Otherwise, if ONL buys at 1, then the subsequent price sequence is 1/ϕ, 1,
1/ϕ, 1, . . . for up to k such pairs, until ONL sells. Now we know the range is
[1/ϕ..1]. Without loss of generality we can assume ONL does not sell at 1/ϕ
since it is clearly the lowest possible price. If ONL does not sell at any point,
then the game ends with no further rounds. OPT makes k trades gaining ϕk,
and ONL’s gain is 1. The competitive ratio is ϕk, which is at least ϕ(2k+2)/3. If
ONL sells at some point with price 1, then the sequence stops and this round
ends. OPT buys at 1/ϕ and sells at 1, getting a gain of ϕ. ONL’s gain is 1. Both
OPT and ONL used one trade, and OPT is a factor of ϕ ahead of ONL.
Each of rounds 2 to k −1 are the same as the intermediate rounds in Theo-
rem 3, with OPT gaining a factor of ϕ2/3 ahead of ONL in each round.
Finally, in round k we use the same price sequence in Theorem 1, which gives
an extra factor of ϕ1−ϵ. Note that ONL may have more trades left then OPT (in
addition to the same reason as in Theorem 3, in round 1 ONL may have done
no trade), but again it is not useful for ONL.
⊓⊔
4
Bounded Daily Return, Known Duration
Recall that in this model, the prices are bounded by p(i)/β ≤p(i+1) ≤αp(i) for
some α, β > 1. Trades can take place at days 0, 1, . . . , T.
Theorem 6. No deterministic algorithm has a competitive ratio better than
αT (2k log β)/((k+1) log β+k log α).

Optimal Online Two-Way Trading with Bounded Number of Transactions
221
Proof. The adversary strategy is very simple and natural: whenever ONL is not
holding stock, the price goes up by a factor of α every day, and while it is holding
stock it goes down by β every day. Let the ONL trades be (bi, si), i = 1, . . . , k. (If
there are fewer than k trades, auxillary ones with bi = si can be added.) Deﬁne
t2i−1 = bi−si−1 for 1 ≤i ≤k+1, and t2i = si−bi for 1 ≤i ≤k. (For convenience
deﬁne s0 = 0 and bk+1 = T.) ONL’s return is 1/(βt2βt4 · · · βt2k). OPT’s optimal
actions, if allowed k + 1 trades, is to hold during the exact opposite intervals as
ONL, i.e., buy at si and sell at bi+1 for 0 ≤i ≤k. But since it can make at
most k trades, its possible course of actions include skipping one of those trades,
or making one of the trades ‘span across two intervals’, e.g., buying at si and
selling at bi+2. Thus OPT’s return is one of
αt3αt5 · · · αt2k+1, αt1αt5 · · · αt2k+1, . . . , αt1αt3 · · · αt2k−1,
αt1αt3 · · · αt2k+1/βt2, αt1αt3 · · · αt2k+1/βt4, . . . , αt1αt3 · · · αt2k+1/βt2k.
To attain the worst competitive ratio, these ratios should be equal, which
means t1 = t3 = · · · = t2k+1 and t2 = t4 = · · · = t2k. This further implies
αkt1 = α(k+1)t1/βt2, which gives αt1 = βt2. Together with t1 + · · · + t2k+1 =
(k + 1)t1 + kt2 = T, this gives
t1 =
log β
(k + 1) log β + k log αT,
t2 =
log α
(k + 1) log β + k log αT
The
competitive
ratio
is
then
αkt1/(1/βkt2)
=
α2kt1
=
αT (2k log β)/((k+1) log β+k log α).
⊓⊔
Theorem 7. Algorithm 3 has competitive ratio αT (2k log β)/((k+1) log β+k log α).
Algorithm 3 may feel unnatural since it does not depend on the price sequence
at all (this is called ‘static’ in [1]). But we prove that the following variation
of the algorithm has the same competitive ratio: it sells only when the current
price falls below h/βt2 where h is the highest price seen since the last purchase.
This coincides with the ‘stop loss’ strategy very common in real trading (more
precisely ‘trailing stop’ [6] where the stop loss limit is not ﬁxed but tracks the
highest price seen thus far, to potentially capture the most proﬁt).
Algorithm 3. Static algorithm for known α, β and T.
Set t1 =
log β
(k+1) log β+k log αT and t2 =
log α
(k+1) log β+k log αT, rounding to nearest integers.
Upon release of the i-th price p(i):
if i = T then
if currently holding stock then
sell at price p(T) and the game ends
else
if have not been holding stock for t1 days and not used up all trades then
buy at price p(i)
else if have been holding stock for t2 days then
sell at price p(i)

222
S.P.Y. Fung
Algorithm 4. Stop loss based algorithm for known α, β and T.
Set t1 and t2 as in Algorithm 3.
Upon release of the i-th price p(i):
if i = T then
if currently holding stock then
sell at price p(T) and the game ends
else
if have not been holding stock for t1 days and not used up all trades then
buy at price p(i)
set h = p(i)
else if currently holding stock then
set h = max(h, p(i))
sell at price p(i) if p(i) < h/βt2
Theorem 8. Algorithm 4 has competitive ratio αT (2k log β)/((k+1) log β+k log α).
Proof. (Sketch) Recall that αt1 = βt2. Let r denote this common value, and the
competitive ratio we want to prove is then r2k. Our approach is to partition the
time horizon so that in each partition x trades in OPT are associated to y trades
in ONL such that the ratio between their gains is at most rx+y. Since in total
they make at most 2k trades, this proves the theorem.
Denote by (bi, si) ONL’s i-th trade. Let Hi = [bi, si] (a closed time interval)
and Ni = (si−1, bi) (an open interval). Let hi be the highest price during Hi.
We can show the following properties:
(1) For any two days x and y in the same Hi, where x < y, we have p(y) ≥
p(x)/r. As a direct consequence, p(si) ≥hi/r.
(2) Without loss of generality we can assume p(bi+1) = p(si)r.
(3) OPT would not buy or sell strictly within any Ni.
Consider an OPT trade (b∗, s∗). Suppose b∗falls within Hu and s∗falls within
Hv, where v ≥u. Its gain is
p(s∗)
p(b∗) ≤
hv
p(bu)/r ≤p(sv)r
p(bu)/r = p(sv)
p(bu)r2
where the inequalities are due to (1). Then
p(s∗)
p(b∗) ≤p(sv)
p(bv)
p(bv)
p(sv−1)
p(sv−1)
p(bv−1)
p(bv−1)
p(sv−2) . . . p(su+1)
p(bu+1)
p(bu+1)
p(su)
p(su)
p(bu)r2
= p(sv)
p(bv)rp(sv−1)
p(bv−1)r . . . rp(su)
p(bu)r2 = p(sv)
p(bv)
p(sv−1)
p(bv−1) . . . p(su)
p(bu)rv−u+2
due to (2). If no other OPT trade sells during Hu or buys during Hv, we
can associate this x = 1 OPT trade with these y = v −u + 1 ONL trades
(bu, su), . . . , (bv, sv). The ratio between their gains is rv−u+2 = r1+y. Now sup-
pose there are two OPT trades (b∗, s∗) and (b∗∗, s∗∗) such that s∗and b∗∗are

Optimal Online Two-Way Trading with Bounded Number of Transactions
223
in the same Hi. Then (p(s∗∗)/p(b∗∗))(p(s∗)/p(b∗)) ≤(p(s∗∗)/p(b∗))r due to (1),
and by losing a factor of r we can replace the two OPT trades with one (b∗, s∗∗).
By repeatedly applying the argument, we can partition the time horizon into
disjoint parts, where in each part x OPT trades have been merged into one,
and this merged trade buys during some Hu and sells during some Hv, and no
other OPT trade sells at Hu or buys at Hv. The ratio between the gains, after
considering the loss due to merging the OPT trades, is r1+yrx−1 = rx+y.
⊓⊔
References
1. Chen, G.-H., Kao, M.-Y., Lyuu, Y.-D., Wong, H.-K.: Optimal buy-and-hold strate-
gies for ﬁnancial markets with bounded daily returns. SIAM J. Comput. 31(2),
447–459 (2001)
2. Chin, F.Y.L., Fu, B., Guo, J., Han, S., Hu, J., Jiang, M., Lin, G., Ting, H.F.,
Zhang, L., Zhang, Y., Zhou, D.: Competitive algorithms for unbounded one-way
trading. Theoret. Comput. Sci. 607(1), 35–48 (2015)
3. Clemente, J., Hromkoviˇc, J., Komm, D., Kudahl, C.: Advice complexity of
the online search problem. In: M¨akinen, V., Puglisi, S.J., Salmela, L. (eds.)
IWOCA 2016. LNCS, vol. 9843, pp. 203–212. Springer, Cham (2016). doi:10.1007/
978-3-319-44543-4 16
4. Damaschke, P., Ha, P.H., Tsigas, P.: Online search with time-varying price bounds.
Algorithmica 55, 619–642 (2009)
5. El-Yaniv, R., Fiat, A., Karp, R.M., Turpin, G.: Optimal search and one-way trading
online algorithms. Algorithmica 30(1), 101–139 (2001)
6. Glynn, P.W., Iglehart, D.L.: Trading securities using trailing stops. Manage. Sci.
41(6), 1096–1106 (1995)
7. Lorenz, J., Panagiotou, K., Steger, A.: Optimal algorithms for k-search with appli-
cation in option pricing. Algorithmica 55(2), 311–328 (2009)
8. Mohr, E., Ahmed, I., Schmidt, G.: Online algorithms for conversion problems: a
survey. Surv. Oper. Res. Manage. Sci. 19, 87–104 (2014)
9. Schmidt, G., Mohr, E., Kersch, M.: Experimental analysis of an online trading
algorithm. Electron. Notes Discrete Math. 36, 519–526 (2010)
10. Schroeder, P., Schmidt, G., Kacem, I.: Optimal on-line algorithms for bi-directional
non-preemptive conversion with interrelated conversion rates. In: Proceedings of
the 4th IEEE Conference on Control, Decision and Information Technology, pp.
28–33 (2016)
11. Zhang, W., Xu, Y., Zheng, F., Dong, Y.: Optimal algorithms for online time series
search and one-way trading with interrelated prices. J. Comb. Optim. 23, 159–166
(2012)

Parameterized Shifted Combinatorial
Optimization
Jakub Gajarsk´y1,4, Petr Hlinˇen´y1(B), Martin Kouteck´y2, and Shmuel Onn3
1 Masaryk University, Brno, Czech Republic
{gajarsky,hlineny}@fi.muni.cz
2 Charles University, Prague, Czech Republic
koutecky@kam.mff.cuni.cz
3 Technion - Israel Institute of Technology, Haifa, Israel
onn@ie.technion.ac.il
4 Technical University Berlin, Berlin, Germany
jakub.gajarsky@tu-berlin.de
Abstract. Shifted combinatorial optimization is a new nonlinear opti-
mization framework which is a broad extension of standard combinato-
rial optimization, involving the choice of several feasible solutions at a
time. This framework captures well studied and diverse problems ranging
from so-called vulnerability problems to sharing and partitioning prob-
lems. In particular, every standard combinatorial optimization problem
has its shifted counterpart, which is typically much harder. Already with
explicitly given input set the shifted problem may be NP-hard. In this
article we initiate a study of the parameterized complexity of this frame-
work. First we show that shifting over an explicitly given set with its
cardinality as the parameter may be in XP, FPT or P, depending on
the objective function. Second, we study the shifted problem over sets
deﬁnable in MSO logic (which includes, e.g., the well known MSO parti-
tioning problems). Our main results here are that shifted combinatorial
optimization over MSO deﬁnable sets is in XP with respect to the MSO
formula and the treewidth (or more generally clique-width) of the input
graph, and is W[1]-hard even under further severe restrictions.
Keywords: Combinatorial optimization · Shifted problem · Treewidth ·
MSO logic · MSO partitioning
J. Gajarsk´y’s research was partially supported by the European Research Council
under the European Union’s Horizon 2020 research and innovation programme (ERC
Consolidator Grant DISTRUCT, grant agreement No 648527).
P. Hlinˇen´y, and partially J. Gajarsk´y, were supported by the research centre Institute
for Theoretical Computer Science (CE-ITI), project P202/12/G061 of the Czech
Science Foundation.
M. Kouteck´y was partially supported by the project 17-09142S of the Czech Science
Foundation.
Shmuel Onn was partially supported by the Dresner Chair at the Technion.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 224–236, 2017.
DOI: 10.1007/978-3-319-62389-4 19

Parameterized Shifted Combinatorial Optimization
225
1
Introduction
The following optimization problem has been studied extensively in the litera-
ture.
(Standard) Combinatorial Optimization. Given S ⊆{0, 1}n and w ∈Zn,
solve
max{ws | s ∈S} .
(1)
The complexity of the problem depends on w and the type and presentation
of S. Often, S is the set of indicating (characteristic) vectors of members of a
family of subsets over a ground set [n] := {1, . . . , n}, such as the family of s −t
dipaths in a digraph with n arcs, the set of perfect matchings in a bipartite or
arbitrary graph with n edges, or the set of bases in a matroid over [n] given by
an independence oracle.
Partly motivated by vulnerability problems studied recently in the literature
(see a brief discussion below), in this article we study a broad nonlinear extension
of CO, in which the optimization is over r choices of elements of S and which
is deﬁned as follows. For a set S ⊆Rn, let Sr denote the set of n × r matrices
having each column in S,
Sr := {x ∈Rn×r | xk ∈S , k = 1, . . . , r} .
Call x, y ∈Rn×r equivalent and write x ∼y if each row of x is a permutation of
the corresponding row of y. The shift of x ∈Rn×r is the unique matrix x ∈Rn×r
satisfying x ∼x and x1 ≥· · · ≥xr, that is, the unique matrix equivalent to x
with each row nonincreasing. Our nonlinear optimization problem follows:
Shifted Combinatorial Optimization (SCO). Given S ⊆{0, 1}n and c ∈
Zn×r, solve
max{cx | x ∈Sr} .
(2)
(Here cx is used to denote the ordinary scalar product of the vectors c and x.)
This problem easily captures many classical fundamental problems. For
example, given a graph G = (V, E) with n vertices, let S := {N[v] | v ∈V } ⊆
{0, 1}n, where N[v] is the characteristic vector of the closed neighborhood of
v. Choose an integer parameter r and let c1
i := 1 for all i and cj
i := 0 for all
i and all j ≥2. Then the optimal objective function value of (2) is n if and
only if we can select a set D of r vertices in G such that every vertex belongs
to the closed neighborhood of at least one of the selected vertices, that is, when
D is a dominating set of G. Likewise, one can formulate the vertex cover and
independent set problems in a similar way.
One speciﬁc motivation for the SCO problem is as follows. Suppose S is the
set of indicators of members of a family over [n]. A feasible solution x ∈Sr
then represents a choice of r members of the given family such that the k-th
column xk is the indicator of the k-th member. Call element i in the ground set
k-vulnerable in x if it is used by at least k of the members represented by x,

226
J. Gajarsk´y et al.
that is, if the i-th row xi of x has at least k ones. It is easy to see that the k-th
column xk of the shift of x is precisely the indicator of the set of k-vulnerable
elements in x. So the shifted optimization problem is to maximize
cx =

{ck
i | i is k-vulnerable in x, i = 1, . . . , n , k = 1, . . . , r} .
Minimizing the numbers of k-vulnerable elements in x may be beneﬁcial for
survival of some family members under various attacks to vulnerable elements
by an adversary, see e.g. [1,20] for more details. For example, to minimize the
number of k-vulnerable elements for some k, we set ck
i := −1 for all i and cj
i := 0
for all i and all j ̸= k. To lexicographically minimize the numbers of r-vulnerable
elements, then of (r −1)-vulnerable elements, and so on, till that of 1-vulnerable
elements, we can set ck
i := −(n + 1)k−1 for all i, k.
As another natural example, consider c with c1
i := 1 and cj
i := −1 for 1 <
j ≤r. Then cx = n if and only if the columns of x indicate a partition of S.
This formulation hence allows us to optimize over partitions of the ground set
(see Sect. 3). Or, consider c with ci = (1, . . . , 1, −1, . . . , −1) of length a > 0 with
b ≤a ones, and let S be the family of independent sets of a graph G. Then
max cx relates to fractional coloring of G; it holds max cx = bn if and only if
G has a coloring by a colors in total such that every vertex receives b distinct
colors – this is the so-called (a : b)-coloring problem.
The complexity of the shifted combinatorial optimization (SCO) problem
depends on c and on the presentation of S, and is often harder than the corre-
sponding standard combinatorial optimization problem. Say, when S is the set
of perfect matchings in a graph, the standard problem is polynomial time solv-
able, but the shifted problem is NP-hard even for r = 2 and cubic graphs, as the
optimal value of the above 2-vulnerability problem is 0 if and only if the graph
is 3-edge-colorable [17]. The minimization of 2-vulnerable arcs with S the set of
s–t dipaths in a digraph, also called the Minimum shared edges problem, was
recently shown to be NP-hard for r variable in [20], polynomial time solvable for
ﬁxed r in [1], and ﬁxed-parameter tractable with r as a parameter in [5].
In the rest of this article we always assume that the number r of choices
is variable. Call a matrix c ∈Zn×r shifted if c = c, that is, if its rows are
nonincreasing. In [13] it was shown that when S = {s ∈{0, 1}n | As = b} where
A is a totally unimodular matrix and b is an integer vector, the shifted problem
with shifted c, and hence in particular the above lexicographic vulnerability
problem, can be solved in polynomial time. In particular this applies to the
cases of S the set of s–t dipaths in a digraph and S the set of perfect matchings
in a bipartite graph. In [17] it was shown that the shifted problem with shifted c
is also solvable in polynomial time for S the set of bases of a matroid presented
by an independence oracle (in particular, spanning trees in a graph), and even
for the intersection of matroids of certain type.
Main Results and Paper Organization. In this article we continue on sys-
tematic study of shifted combinatorial optimization.

Parameterized Shifted Combinatorial Optimization
227
First, in Sect. 2, we consider the case when the set S is given explicitely.
While the standard problem is always trivial in such case, the SCO problem can
be NP-hard for explicit set S (Proposition 2.1). Our main results on this case
can be brieﬂy summarized as follows:
– (Theorem 2.2) The shifted combinatorial optimization problem, parameter-
ized by |S| = m, is; (a) for general c in the complexity class XP and W[1]-hard
w.r.t. m, (b) for shifted c in FPT, and (c) for shifted −c in P.
– (Theorem 2.4) The latter case (c) of shifted −c is in P even for sets S
presented by a linear optimization oracle.
In Sect. 3, we study a more general framework of SCO for the set S deﬁnable in
Monadic Second Order (MSO) logic. This rich framework includes, for instance,
the well-studied case of so called MSO partitioning problems on graphs. We prove
the following statement which generalizes known results about MSO partitioning:
– (Theorem 3.5, Corollary 3.9) The shifted combinatorial optimization prob-
lem, for (a) graphs of bounded treewidth and S deﬁned in MSO2 logic, or
(b) graphs of bounded clique-width and S deﬁned in MSO1 logic, is in XP
(parameterized by the width and the formula deﬁning S).
In the course of proving this statement we also provide a connection of shifted
optimization to separable optimization when the corresponding polyhedron is
decomposable and 0/1 (Lemma 3.3).
To complement the previous tractability result, in Sect. 4 we prove the fol-
lowing negative result under much more restrictive parametrization.
– (Theorem 4.2). There exists a ﬁxed First Order formula φ such that the
associated MSO1 partitioning problem, and hence also the SCO problem with
S deﬁned by φ, are W[1]-hard on graphs of bounded treedepth.
We use standard terminology of graph theory, integer programming and para-
meterized complexity. Due to space restrictions, the missing proofs of our state-
ments are left for the full version [arXiv:1702.06844]. The statements with proofs
presented only in the full version are marked with (*).
2
Sets Given Explicitly
In this section we consider the shifted problem (2) over an explicitly given set
S = {s1, . . . , sm}. We demonstrate that already this seemingly simple case is in
fact nontrivial and interesting, and that the brute-force algorithm which tries all
possible r-subsets of S is likely close to optimal:
Proposition 2.1 (*). The SCO problem (2) is NP-hard for 0/1 shifted matrices
c = c ∈{0, 1}n×r and explicitly given 0/1 sets S = {s1, . . . , sm} ⊆{0, 1}n.
Moreover, unless the Exponential Time Hypothesis (ETH) fails, it cannot be
solved in time O(no(r)).
Note that the next results in this section concerning Shifted IP apply to the
more general situation in which S may consist of arbitrary integer vectors, not
necessarily 0/1. This is formulated as follows.

228
J. Gajarsk´y et al.
Shifted Integer Programming. Given S ⊆Zn and c ∈Zn×r, similarly to
(2), solve
max{cx | x ∈Sr} .
(3)
For S = {s1, . . . , sm} and nonnegative integers r1, . . . , rm with m
i=1 ri = r,
let x(r1, . . . , rm) be the matrix in Sr with ﬁrst r1 columns equal to s1, next r2
columns equal to s2, and so on, with last rm columns equal to sm, and deﬁne
f(r1, . . . , rm) := cx(r1, . . . , rm).
We have got the following eﬀective theorem in contrast with Proposition 2.1.
Theorem 2.2 (*). The shifted integer programming problem (3) over an explic-
itly given set S = {s1, . . . , sm} ⊆Zn reduces to the following nonlinear integer
programming problem over a simplex,
max

f(r1, . . . , rm)
 r1, . . . , rm ∈Z+ ,
m

k=1
rk = r

.
(4)
If c = c is shifted then f is concave, and if −c is shifted then f is convex.
Furthermore, the following cases hold:
1. With m parameter and c arbitrary, problem (3) is in XP. Moreover, the prob-
lem is W[1]-hard with parameter m even for 0/1 sets S.
2. With m parameter and c shifted, problem (3) is in FPT.
3. With m variable and −c shifted, problem (3) is in P.
Let us ﬁrst give an exemplary application of part 2 of Theorem 2.2 now.
Bredereck et al. [3] study the Weighted Set Multicover (WSM) prob-
lem, which is as follows. Given a universe U = {u1, . . . , uk}, integer demands
d1, . . . , dk ∈N and a multiset F = {F1, . . . , Fn} ⊆2U with weights w1, . . . , wn ∈
N, ﬁnd a multiset F′ ⊆F of smallest weight which satisﬁes all the demands –
that is, for all i = 1, . . . , k, |{F ∈F′ | ui ∈F}| ≥di. It is shown [3] that this
problem is FPT when the size of the universe is a parameter, and then several
applications in computational social choice are given there.
Notice that F can be represented in a succinct way by viewing F as a set
Fs = {F1, . . . , FK} and representing the diﬀerent copies of F ∈Fs in F by
deﬁning K weight functions w1, . . . , wK such that, for each i = 1, . . . , K, wi(j)
returns the total weight of the ﬁrst j lightest copies of Fi, or ∞if there are less
than j copies. We call this the succinct variant.
Bredereck et al. [3] use Lenstra’s algorithm for their result, which only works
when F is given explicitly. We note in passing that our approach allows us to
extend their result to the succinct case.
Proposition 2.3 (*). Weighted Set Multicover is in FPT with respect to uni-
verse size k, even in the succinct variant.
Theorem 2.2, part 3, can be applied also to sets S presented implicitly by an
oracle. A linear optimization oracle for S ⊆Zn is one that, queried on w ∈Zn,
solves the linear optimization problem max{ws | s ∈S}. Namely, the oracle

Parameterized Shifted Combinatorial Optimization
229
either asserts that the problem is infeasible, or unbounded, or provides an opti-
mal solution. As mentioned before, even for r = 2, the shifted problem for perfect
matchings is NP-hard, and hence for general c the shifted problem over S pre-
sented by a linear optimization oracle is also hard even for r = 2. In contrast,
we have the following strengthening.
Theorem 2.4 (*). The shifted problem (3) with c nondecreasing, over any set
S ⊂Zn which is presented by a linear optimization oracle, can be solved in
polynomial time.
3
MSO-deﬁnable Sets: XP for Bounded Treewidth
In this section we study another tractable and rich case of shifted combinatorial
optimization, namely that of the set S deﬁned in the MSO logic of graphs. This
case, in particular, includes well studied MSO partitioning framework of graphs
(see below) which is tractable on graphs of bounded treewidth and clique-width.
In the course of proving our results, it is useful to study a geometric connection
of 0/1 SCO problems to separable optimization over decomposable polyhedra.
3.1
Relating SCO to Decomposable Polyhedra
The purpose of this subsection is to demonstrate how shifted optimization over
0/1 polytopes closely relates to an established concept of decomposable polyhe-
dra. We refer to Ziegler [25] for deﬁnitions and terminology regarding polytopes.
Deﬁnition 3.1 (Decomposable polyhedron and Decomposition oracle).
A polyhedron P ⊆Rn is decomposable if for every k ∈N and every x ∈kP ∩Zn,
there are x1, . . . , xk ∈P ∩Zn with x = x1 + · · · + xk, where kP = {ky | y ∈P}.
A decomposition oracle for a decomposable P is one that, queried on k ∈N given
in unary and on x ∈kP ∩Zn, returns x1, . . . , xk ∈P ∩Zn with x = x1+· · ·+xk.
This property is also called integer decomposition property or being integrally
closed in the literature. The best known example are polyhedra given by totally
unimodular matrices [2]. Furthermore, we will use the following notion.
Deﬁnition 3.2 (Integer separable (convex) minimization oracle).
Let
P ⊆Rn and let f(x) = n
i=1 fi(xi) be a separable function on Rn. An integer
separable minimization oracle for P is one that, queried on this f, either reports
that P ∩Zn is empty, or that it is unbounded, or returns a point x ∈P ∩Zn
which minimizes f(x). An integer separable convex minimization oracle for P
is an integer separable minimization oracle for P which can only be queried on
functions f as above with all fi convex.

230
J. Gajarsk´y et al.
We now formulate how these notions naturally connect with SCO.
Lemma 3.3 (*). Let (S, c, r) be an instance of shifted combinatorial optimiza-
tion, with S ⊆{0, 1}n, r ∈N and c ∈Zn×r. Let P ⊆[0, 1]n be a polytope such
that S = P ∩{0, 1}n and let Q ⊆[0, 1]n+n′ be some extension of P, that is,
P = {x | (x, y) ∈Q}.
Then, provided a decomposition oracle for Q and an integer separable mini-
mization oracle for rQ, the shifted problem given by (S, c, r) can be solved with
one call to the optimization oracle and one call to the decomposition oracle. Fur-
thermore, if c is shifted, an integer separable convex minimization oracle suﬃces.
To demonstrate Lemma 3.3 we use it to give an alternative proof of the
result of Kaibel et al. [13] that the shifted problem is polynomial when S =
{x | Ax = b, x ∈{0, 1}n} and A is totally unimodular. It is known that
P = {x | Ax = b, 0 ≤x ≤1} is decomposable and a decomposition oracle is
realizable in polynomial time [23]. Moreover, it is known that an integer sep-
arable convex minimization oracle for rP is realizable in polynomial time [12].
Lemma 3.3 implies that the shifted problem is polynomial for this S when c is
shifted.
The reason we have formulated Lemma 3.3 for S given by an extension Q of
the polytope P corresponding to S, is the following: while P itself might not be
decomposable, there always exists an extension of it which is decomposable. See
the full version for more details.
Other potential candidates where Lemma 3.3 could be applied are classes
of polytopes that are either decomposable, or allow eﬃcient integer sepa-
rable (convex) minimization. Some known decomposable polyhedra are sta-
ble set polytopes of perfect graphs, polyhedra deﬁned by k-balanced matri-
ces [24], polyhedra deﬁned by nearly totally unimodular matrices [10], etc. Some
known cases where integer separable convex minimization is polynomial are for
P = {x | Ax = b, x ∈{0, 1, . . . , r}n} where the Graver basis of A has small size
or when A is highly structured, namely when A is either an n-fold product, a
transpose of it, or a 4-block n-fold product; see the books of Onn [21] and De
Loera, Hemmecke and K¨oppe [18].
3.2
XP Algorithm for MSO-deﬁnable Set
We start with deﬁning the necessary specialized terms. We assume that the
reader is familiar with the standard treewidth of a graph (see also the full paper).
Given a matrix A ∈Zn×m, we deﬁne the corresponding Gaifman graph
G = G(A) as follows. Let V (G) = [m]. We let {i, j} ∈E(G) if and only if there
is an r ∈[n] with A[r, i] ̸= 0 and A[r, j] ̸= 0. Intuitively, two vertices of G are
adjacent if the corresponding variables xi, xj occur together in some constraint
of Ax ≤b. The (Gaifman) treewidth of a matrix A is then the treewidth of its
Gaifman graph, i.e., tw(A) := tw(G(A)).

Parameterized Shifted Combinatorial Optimization
231
The aforementioned MSO partitioning framework of graphs comes as follows.
MSO Partitioning Problem. Given a graph G, an MSO2 formula ϕ with one
free vertex-set variable and an integer r, the task is as follows;
– to ﬁnd a partition U1 ˙∪U2 . . . ˙∪Ur = V (G) of the vertices of G such that
G |= ϕ(Ui) for all i = 1, . . . , r, or
– to conﬁrm that no such partition of V (G) exists.
For example, if ϕ(X) expresses that X is an independent set, then the ϕ-
MSO partitioning problem decides if G has an r-coloring, and thus, ﬁnding
minimum feasible r (simply by trying r = 1, 2, . . . ) solves the Chromatic num-
ber problem. Similarly, if G |= ϕ(X) when X is a dominating set, minimizing r
solves the Domatic number problem, and so on.
Rao [22] showed an algorithm for MSO partitioning, for any MSO2 formula
ϕ, on a graph G with treewidth tw(G) = τ running in time rf(ϕ,τ)|V (G)| (XP)
for some computable function f. Our next result widely generalizes this to SCO
over MSO-deﬁnable sets.
Deﬁnition 3.4 (MSO-deﬁnable sets). For a graph G on |V (G)| = n vertices,
we interpret a 0/1 vector x ∈{0, 1}n as the set X ⊆V where v ∈X iﬀxv = 1.
We then say that x satisﬁes a formula ϕ if G |= ϕ(X). Let
Sϕ(G) = {x | x satisﬁes ϕ in G} .
Let c be deﬁned as c1
i := 1 for 1 ≤i ≤n and cj
i := −1 for 2 ≤j ≤r and
1 ≤i ≤n. Observe then the following: deciding whether the shifted problem
with S = Sϕ(G), c and r, has an optimum of value n is equivalent to solving the
MSO partitioning problem for ϕ.
Theorem 3.5 (*). Let G be a graph of treewidth tw(G) = τ, let ϕ be an MSO2
formula and Sϕ(G) = {x | x satisﬁes ϕ}. There is an algorithm solving the
shifted problem with S = Sϕ(G) and any given c and r in time rf(ϕ,τ) · |V (G)|
for some computable function f. In other words, for parameters ϕ and τ, the
problem is in the complexity class XP.
We will prove Theorem 3.5 using Lemma 3.3 on separable optimization over
decomposable polyhedra. To that end we need to show the following two steps:
1. There is a 0/1 extension Q of the polytope P = conv(Sϕ(G)) which is decom-
posable and endowed with a decomposition oracle (Deﬁnition 3.1),
2. there is an integer separable minimization oracle (Deﬁnition 3.2) for the poly-
tope rQ.

232
J. Gajarsk´y et al.
The ﬁrst point is implied by a recent result of Kolman, Kouteck´y and Tiwary:
Proposition 3.6 ([15]1). Let G be a graph on n vertices of treewidth tw(G) = τ,
and ϕ be an MSO2 formula with one free vertex-set variable. Then, for some
computable functions f1, f2, f3, there are matrices A, B and a vector b, com-
putable in time f1(ϕ, τ) · n, such that
1. the polytope Q = {(x, y) | Ax + By = b, y ≥0} is a 0/1 polytope which is
an extension of the polytope P = conv(Sϕ(G)), and Q ⊆[0, 1]f2(ϕ,τ)n,
2. Q is decomposable and endowed with a decomposition oracle, and
3. the (Gaifman) treewidth of the matrix (A B) is at most f3(ϕ, τ).
The second requirement of Lemma 3.3 follows from eﬃcient solvability of the
constraint satisfaction problem (CSP) of bounded treewidth, originally proven
by Freuder [7]. We will use a natural weighted version of this folklore result.
For a CSP instance I = (V, D, H, C) one can deﬁne the constraint graph of
I as G = (V, E) where E = {{u, v} | (∃CU ∈H) ∨(∃wU ∈C) s.t. {u, v} ⊆U}.
The treewidth of a CSP instance I is deﬁned as the treewidth of the constraint
graph of I.
Proposition 3.7 ([7]). Given a CSP instance I of treewidth τ and maximum
domain size D = maxu∈V |Du|, a minimum weight solution can be found in time
O(Dτ(n + |H| + |C|)).
Proposition 3.7 can be used to realize an integer separable minimization ora-
cle for integer programs of bounded treewidth, as follows.
Lemma 3.8 (*). Let A ∈Zn×m, b ∈Zm, ℓ, u ∈Zn be given s.t. tw(A) = τ,
and let D = ∥u −ℓ∥∞. Then an integer separable minimization oracle over
P = {x | Ax = b, ℓ≤x ≤u} is realizable in time Dτ(n + m).
The proof of Lemma 3.8 proceeds by constructing a CSP instance I based
on the ILP Ax = b, ℓ≤x ≤u, such that solving I corresponds to integer
separable minimization over P. Since the treewidth of I is τ and the maximum
domain size is D, Proposition 3.7 does the job.
Now, in our case of rQ where Q is a 0/1 polytope, we have got D ≤r.
Consequently, we can ﬁnish the proof of Theorem 3.5 by using Lemma 3.3.
Besides treewidth, another useful width measure of graphs is the clique-width
of a graph G which, brieﬂy, means the minimum number k of labels needed to
construct G along a so called k-expression. Rao’s result [22] applies also to the
MSO partitioning problem for MSO1 formulas and graphs of bounded clique-
width. We show the analogous extension of Theorem 3.5 next.
Corollary 3.9 (*). Let G be a graph of clique-width cw(G) = γ given along
with a γ-expression, let ψ be an MSO1 formula and Sψ(G) = {x | x satisﬁes ψ}.
There is an algorithm solving the shifted problem with S = Sψ(G) and any given
c and r in time rf(ψ,γ) · |V (G)| for some computable f.
1 We remark that while [15, Theorem 4] explicitly speaks about generally weaker
MSO1 logic, it is folklore that in the realm of graphs of bounded treewidth the
same can be equivalently stated with MSO2 logic (as noted also in [15]).

Parameterized Shifted Combinatorial Optimization
233
While it is possible to prove Corollary 3.9 along the same lines as used above,
we avoid repeating the previous arguments and, instead, apply the following
technical tool which extends the known fact that a class of graphs is of bounded
clique-width iﬀit has an MSO1 interpretation in the class of rooted trees.
Lemma 3.10 (*). Let G be a graph of clique-width cw(G) = γ given along with
a γ-expression Γ constructing G, and let ψ be an MSO1 formula. One can, in
time O(|V (G)| + |Γ| + |ψ|), compute a tree T and an MSO1 formula ϕ such that
V (G) ⊆V (T) and
for every X it is T |= ϕ(X), iff X ⊆V (G) and G |= ψ (X).
With Lemma 3.10 at hand, it is easy to derive Corollary 3.9 from previous
Theorem 3.5 applied to the tree T.
Finally, we add a small remark regarding the input G in Corollary 3.9; we
are for simplicity assuming that G comes along with its γ-expression since it
is currently not known how to eﬃciently construct a γ-expression for an input
graph of ﬁxed clique-width γ. Though, one may instead use the result of [11]
which constructs in FPT a so-called rank-decomposition of G which can be used
as an approximation of a γ-expression for G (with up to an exponential jump,
but this does not matter for a ﬁxed parameter γ in theory).
4
MSO-deﬁnable Sets: W[1]-Hardness
Recall that natural hard graph problems such as Chromatic number are
instances of MSO partitioning and so also instances of shifted combinato-
rial optimization. While we have shown an XP algorithm for SCO with MSO-
deﬁnable sets on graphs of bounded treewidth and clique-width in Theorem 3.5
and Corollary 3.9, it is a natural question whether an FPT algorithm could exist
for this problem, perhaps under a more restrictive width measure.
Here we give a strong negative answer to this question. First, we note the
result of Fomin et al. [6] proving W[1]-hardness of Chromatic number parame-
terized by the clique-width of the input graph. This immediately implies that an
FPT algorithm in Corollary 3.9 would be very unlikely. Although, Chromatic
number is special in the sense that it is solvable in FPT when parameterized
by the treewith of the input. Here we prove that it is not the case of MSO
partitioning problems and SCO in general, even when considering restricted
MSO1 formulas and shifted c, and parameterizing by a much more restrictive
treedepth parameter.
Deﬁnition 4.1 (Treedepth). Let the height of a rooted tree or forest be the
maximum root-to-leaf distance in it. The closure cl(F) of a rooted forest F is the
graph obtained from F by making every vertex adjacent to all of its ancestors.
The treedepth td(G) of a graph G is one more than the minimum height of a
forest F such that G ⊆cl(F).

234
J. Gajarsk´y et al.
Note that always td(G) ≥tw(G) + 1 since we can use the vertex sets of the
root-to-leaf paths of the forest F (from Deﬁnition 4.1) in a proper order as the
bags of a tree-decomposition of G.
Theorem 4.2 (*). There exists a graph FO formula ϕ(X) with a free set variable
X, such that the instance of the MSO partitioning problem given by ϕ, is
W[1]-hard when parameterized by the treedepth of an input simple graph G.
Consequently, the shifted problem with Sϕ(G) is also W[1]-hard (for
suitable c) when parameterized by the treedepth of G.
The way we approach Theorem 4.2 is by a reduction from W[1]-hardness of
Chromatic number with respect to clique-width [6], in which we exploit some
special hidden properties of that reduction, as extracted in [8]. In a nutshell, the
“diﬃcult cases” of [6] can be interpreted in a special way into labeled rooted
trees of small height, and here we further trade the labels (the number of which
is the parameter) for increased height of a tree and certain additional edges
belonging to the tree closure.
5
Conclusions and Open Problems
We close with several open problems we consider interesting and promising.
Parameterizing by r. It is interesting to consider taking r as a parameter. For
example, Fluschnik et al. [5] prove that the Minimum Shared Edges problem
is FPT parameterized by the number of paths. Omran et al. [20] prove that the
Minimum Vulnerability problem is in XP with the same parameter. Since
both problems are particular cases of the shifted problem, we ask whether the
shifted problem with S being the set of s −t paths of a (di)graph lies in XP or
is NP-hard already for some constant r.
Further uses of Lemma 3.3.
For example, which interesting combinatorial
sets S can be represented as n-fold integer programs [18,21] such that the cor-
responding polyhedra are decomposable?
Approximation. The Minimum Vulnerability problem has also been stud-
ied from the perspective of approximation algorithms [20]. What can be said
about the approximation of the shifted problem?
Going beyond 0/1. The results in Sect. 2 are the only known ones in which S
does not have to be 0/1. What can be said about the shifted problem with such
sets S that are not given explicitly, e.g., when S is given by a totally unimodular
system?

Parameterized Shifted Combinatorial Optimization
235
References
1. Assadi,
S.,
Emamjomeh-Zadeh,
E.,
Norouzi-Fard,
A.,
Yazdanbod,
S.,
Zarrabi-Zadeh, H.: The minimum vulnerability problem. Algorithmica 70(4),
718–731 (2014)
2. Baum, S., Trotter Jr., L.E.: Integer rounding and polyhedral decomposition for
totally unimodular systems. In: Henn, R., Korte, B., Oettli, W. (eds.) Optimization
and Operations Research, pp. 15–23. Springer, Heidelberg (1978)
3. Bredereck, R., Faliszewski, P., Niedermeier, R., Skowron, P., Talmon, N.: Elections
with few candidates: prices, weights, and covering problems. In: Walsh, T. (ed.)
ADT 2015. LNCS, vol. 9346, pp. 414–431. Springer, Cham (2015). doi:10.1007/
978-3-319-23114-3 25
4. Chen, J., Huang, X., Kanj, I.A., Xia, G.: Strong computational lower bounds via
parameterized complexity. J. Comput. Syst. Sci. 72(8), 1346–1367 (2006)
5. Fluschnik, T., Kratsch, S., Niedermeier, R., Sorge, M.: The parameterized complex-
ity of the minimum shared edges problem. In: FSTTCS 2015, vol. 45, pp. 448–462.
LIPIcs, Schloss Dagstuhl (2015)
6. Fomin, F., Golovach, P., Lokshtanov, D., Saurab, S.: Clique-width: on the price of
generality. In: SODA 2009, pp. 825–834. SIAM (2009)
7. Freuder, E.C.: Complexity of K-tree structured constraint satisfaction problems.
In: Proceedings of the 8th National Conference on Artiﬁcial Intelligence, pp. 4–9
(1990)
8. Gajarsk´y, J., Lampis, M., Ordyniak, S.: Parameterized algorithms for modular-
width. In: Gutin, G., Szeider, S. (eds.) IPEC 2013. LNCS, vol. 8246, pp. 163–176.
Springer, Cham (2013). doi:10.1007/978-3-319-03898-8 15
9. Ganian, R., Hlinˇen´y, P., Neˇsetˇril, J., Obdrˇz´alek, J., Ossona de Mendez, P.,
Ramadurai, R.: When trees grow low: shrubs and fast MSO1. In: Rovan, B.,
Sassone, V., Widmayer, P. (eds.) MFCS 2012. LNCS, vol. 7464, pp. 419–430.
Springer, Heidelberg (2012). doi:10.1007/978-3-642-32589-2 38
10. Gijswijt, D.: Integer decomposition for polyhedra deﬁned by nearly totally uni-
modular matrices. SIAM J. Discrete Math. 19(3), 798–806 (2005)
11. Hlinˇen´y, P., Oum, S.: Finding branch-decompositions and rank-decompositions.
SIAM J. Comput. 38(3), 1012–1032 (2008)
12. Hochbaum, D.S., Shanthikumar, J.G.: Convex separable optimization is not much
harder than linear optimization. J. ACM 37(4), 843–862 (1990)
13. Kaibel, V., Onn, S., Sarrabezolles, P.: The unimodular intersection problem. Oper.
Res. Lett. 43(6), 592–594 (2015)
14. Knop, D., Kouteck´y, M., Masaˇr´ık, T., Toufar, T.: Simpliﬁed algorithmic metathe-
orems beyond MSO: treewidth and neighborhood diversity, 1 March 2017.
arXiv:1703.00544
15. Kolman, P., Kouteck´y, M., Tiwary, H.R.: Extension complexity, MSO logic, and
treewidth, 28 February 2017. arXiv:1507.04907
16. Kreutzer, S.: Algorithmic meta-theorems. In: Electronic Colloquium on Computa-
tional Complexity (ECCC), vol. 16, p. 147 (2009)
17. Levin, A., Onn, S.: Shifted matroid optimization. Oper. Res. Lett. 44, 535–539
(2016)
18. De Loera, J.A., Hemmecke, R., K¨oppe, M.: Algebraic and Geometric Ideas in
the Theory of Discrete Optimization. MOS-SIAM Series on Optimization, vol. 14.
SIAM, Philadelphia (2013)

236
J. Gajarsk´y et al.
19. Oertel, T., Wagner, C., Weismantel, R.: Integer convex minimization by mixed
integer linear optimization. Oper. Res. Lett. 42(6–7), 424–428 (2014)
20. Omran, M.T., Sack, J.-R., Zarrabi-Zadeh, H.: Finding paths with minimum shared
edges. J. Comb. Optim. 26(4), 709–722 (2013)
21. Onn, S.: Nonlinear discrete optimization. Zurich Lectures in Advanced Mathemat-
ics. European Mathematical Society. http://ie.technion.ac.il/∼onn/Book/NDO.
pdf
22. Rao, M.: MSOL partitioning problems on graphs of bounded treewidth and clique-
width. Theor. Comput. Sci. 377(1–3), 260–267 (2007)
23. Schrijver, A.: Combinatorial Optimization: Polyhedra and Eﬀciency. Algorithms
and Combinatorics, vol. 24. Springer, Heidelberg (2003)
24. Zambelli, G.: Colorings of k-balanced matrices and integer decomposition property
of related polyhedra. Oper. Res. Lett. 35(3), 353–356 (2007)
25. Ziegler, G.M.: Lectures on Polytopes. Graduate Texts in Mathematics, vol. 152.
Springer, New York (1995)

Approximate Minimum Diameter
Mohammad Ghodsi1,2, Hamid Homapour1(B), and Masoud Seddighin1(B)
1 Sharif University of Technology, Tehran, Iran
ghodsi@sharif.edu, {homapour,mseddighin}@ce.sharif.edu
2 IPM - Institute for Research in Fundamental Sciences, Tehran, Iran
Abstract. We study the minimum diameter problem for a set of inexact
points. By inexact, we mean that the precise location of the points is not
known. Instead, the location of each point is restricted to a continuous
region (Imprecise model) or a ﬁnite set of points (Indecisive model).
Given a set of inexact points in one of Imprecise or Indecisive models,
we wish to provide a lower-bound on the diameter of the real points.
In the ﬁrst part of the paper, we focus on Indecisive model. We
present an O(2
1
ϵd · ϵ−2d · n3) time approximation algorithm of factor
(1 + ϵ) for ﬁnding minimum diameter of a set of points in d dimensions.
This improves the previously proposed algorithms for this problem sub-
stantially.
Next, we consider the problem in Imprecise model. In d-dimensional
space, we propose a polynomial time
√
d-approximation algorithm. In
addition, for d = 2, we deﬁne the notion of α-separability and use our
algorithm for Indecisive model to obtain (1+ϵ)-approximation algorithm
for a set of α-separable regions in time O(2
1
ϵ2 .
n3
ϵ10. sin(α/2)3 ).
Keywords: Indecisive · Imprecise · Computational Geometry · Approx-
imation algorithms · Core-set
1
Introduction
Rapid growth in the computational technologies and the vast deployment of
sensing and measurement tools turned Big Data to an interesting and interdis-
ciplinary topic that attracted lots of attentions in the recent years.
One of the critical issues in Big Data is dealing with uncertainty. Computa-
tional Geometry (CG) is one of the ﬁelds, that is deeply concerned with large
scale data and the issue of imprecision. The real data for a CG problem is often
gathered by measurement and coordination instrument (GPS, Digital compass,
etc.) which are inexact. In addition, the movement is an unavoidable source of
inaccuracy. Hence, the data collected by these tools are often noisy and suﬀer
from inaccuracies. In spite of this inaccuracy, most of the algorithms are based
on the fact the input data are precise. These algorithms become useless in the
presence of inaccurate data. Thus, it is reasonable to design algorithms that take
this inaccuracy into account.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 237–249, 2017.
DOI: 10.1007/978-3-319-62389-4 20

238
M. Ghodsi et al.
Point is the primary object in a geometric problem. Roughly speaking, the
input for most of the CG problems is a set of points in diﬀerent locations of a
metric space. The exact location of these points is one of the parameters that
might be inaccurate. To capture this inaccuracy in geometric problems, several
models have been proposed, e.g. Uncertain, Indecisive, Imprecise and Stochastic
models [9]. Suppose that P = {ρ1, ρ2, . . . , ρn} is the set of exact points. Due to
the measurement inaccuracy, P is unknown to us. In the following, we give a
short description of the way that each model tries to represent P:
– Uncertain (or locational uncertain) model: in the uncertain model, the
location of each point is determined by a probability distribution function.
Formally, for every point ρi ∈P a probability distribution Di is given in
input, that represent the probability that ρi appears in every location.
– Indecisive model: in the indecisive model, each input point can take one of
k distinct possible locations. In other words, for each real point ρi ∈P, a set
of k points is given, where the location of ρi equals to one of these points.
– Imprecise model: in the imprecise model, possible locations of a point is
restricted to a region, i.e., for each point ρ ∈P, a ﬁnite region Ri is given in
input. All we know is ρi ∈Ri.
– Stochastic model: every point p in input has a deterministic location, but
there is a probability that p /∈P. Stochastic model is used for the case that
false reading is possible, usually in database scenarios [2,4].
Let C be an input related to one of these models, that represents the exact
point set P. The question is, what information can we retrieve from C? For
brevity, suppose that the objective is to compute a statistical value D(P) (e.g.,
D(P) can be the diameter or width of P). Trivially, ﬁnding the exact value of
D(P) is not possible, since we don’t have access to the real points P. Instead,
there may be several approaches, based on the input model. For example, one
can calculate the distribution of D(P) (for uncertain model), or can provide
upper bound or lower bound on the value of D(P) (for Indecisive and Imprecise
models) [9,11].
In this paper, we investigate on Indecisive and Imprecise models and con-
sider the diameter as the objective function. Furthermore, our approach is to
compute bounds on the output. Thus, the general form of the problem we
wish to observe is as follows: Given an input C corresponding to an Indecisive
(Imprecise) data model of a set P of exact data. The goal is to lower-bound the
value of the D(P), i.e., ﬁnding a lower-bound on the minimum possible value for
D(P). In Sect. 1.1 you can ﬁnd a formal deﬁnition of the problem. It is worth
mentioning that this problem has many applications in computer networking
and databases [6,7,14].
1.1
Model Deﬁnition and Problem Statement
Indecisive model. As mentioned, in Indecisive model, each actual point ρi ∈P
is represented via a ﬁnite set of points with diﬀerent locations. For simplicity,

Approximate Minimum Diameter
239
suppose that we color the representatives of each point with a unique color. Thus,
the input for a problem in Indecisive model is a set P = {P1, P2, . . . , Pm},
where each Pi is a set of points representing alternatives of ρi, i.e., ρi is one of
the elements in Pi. All the points in Pi are colored with color Ci. For Indecisive
model, we assume that total number of the points is n, i.e. 
i |Pi| = n.
In addition to uncertainty, this model can represent various situations. For
example, consider an instance of a resource allocation problem where we have a
set of resources and each resource has a number of copies (or alternatives) and
the solutions are constrained to use exactly one of the alternatives. Indecisive
model can be used to represent this kind of issues. To do this, each resource can
be represented by a point in d dimensional Euclidean space. To represent the
type of resources, each point is associated with a color indicating its type.
Imprecise model. In Imprecise model, the possible locations of a point is
restricted by a ﬁnite and continuous area. The input for an imprecise problem
instance is a set R = {R1, R2, . . . , Rn}, where for every region Ri, we know
that ρi ∈Ri. Therefore, each point of Ri is a possible location for the actual
point ρi.
This model can be applied in many situations. For example, we know that all
measurements have a degree of uncertainty regardless of precision and accuracy.
To represent an actual point, we can show the amount of inaccuracy with a circle
(i.e. as a region) which is centered at the measured point with a radius equals
to the size of tolerance interval in the measurement tool, which means that the
actual point lies somewhere in this circle.
Problem Statement. For brevity, we use MinDCS and MinDiam to refer to
the problem in models Indecisive and Imprecise, respectively. In the following,
we formally deﬁne MinDCS and MinDiam problems.
Problem 1 (MinDCS). Given a set P = {P1, P2, . . . , Pm}, with each Pi being
a ﬁnite set of points in d-dimensional euclidean space with the same color Ci
and 
i |Pi| = n. A color selection of P is a set of m points, one from each Pi.
Find a color selection S of P so that the diameter of these points is the smallest
among all options.
Denote by Dmin, the diameter of the desired selection in MinDCS. Formally:
Dmin =
min
S=Sel(P)( max
∀pi,pj∈S ||pi −pj||),
where Sel(P) is the set of all possible color selections of P. Furthermore, we
deﬁne OPT as the color selection that the diameter of its points is smallest
among all possible color selections:
OPT = arg
min
S=Sel(P)( max
∀pi,pj∈S ||pi −pj||).
We also denote by D(Y), the diameter of the point set Y.
Problem 2 (MinDiam). In this problem, a set R = {R1, R2, . . . , Rn} is given,
where each Ri is a bounded convex region in d-dimensional euclidean space.

240
M. Ghodsi et al.
We want to select one point from each region, such that the diameter of
the selected points is minimized. Formally, we want to select a set S =
{p1, p2, . . . , pn} of points, where pi ∈Ri such that D(S) is the smallest possible.
1.2
Related Works
For Indecisive model, Fan et al. [10] suggested a randomized algorithm with the
time complexity O(n1+ϵ) for (1 + ϵ)-approximation of the maximum diameter,
where ϵ could be an arbitrarily small positive constant.
Zhang et al. [14] suggested an O(nk) time brute force algorithm to ﬁnd mini-
mum possible diameter in Indecisive model. Furthermore, Fleischer and Xu [6,7]
showed that this problem can be solved in polynomial time for the L1 and L∞
metrics, while it is NP-hard for all other Lp metrics, even in two dimensions.
They also gave an approximation algorithm with a constant approximation fac-
tor. By extending the deﬁnition of ϵ-kernels from Agarwal et al. [1] to avatar
ϵ-kernels (the notion of avatar is the same as Indecisive model) in d dimensional
Euclidean space, Consuegra et al. [3] proposed an (1 + ϵ)-approximation algo-
rithm with running time O((n)(2d+3) · m
δd · (2d)2 · 2
1
δd (
2
δd−1 )⌊d
2 ⌋+ (
2
δd−1 )a) (where
δ is the side length of the grid cells for constructing core-set, d is the dimension
of the points, k is the maximum frequency of the alternatives of a points, and a
is a small constant). Furthermore, for Uncertain model, Fan et al. [5] proposed
an approximation algorithm to compute the expected diameter of a point set.
In Imprecise model, in the Euclidean plane, L¨oﬄer and van Kreveld [12]
proposed an O(n log n) time algorithm for the minimum diameter of a set of
imprecise points modeled as squares and a (1+ϵ)-approximation algorithm with
running time O(ncϵ−1
2 ) for the points modeled as discs, where c = 6.66.
1.3
Our Results and Techniques
The ﬁrst part of our work (Sect. 2) is devoted to the diameter problem in
Indecisive model. In this case, we present an O(2
1
ϵd ·
1
ϵ2d · n3) time approxi-
mation algorithm of factor (1 + ϵ). This improves the previous O((n)(2d+3) · m
δd ·
(2d)2 ·2
1
δd (
2
δd−1 )⌊d
2 ⌋+(
2
δd−1 )a) time algorithm substantially. The idea is to build
several grids with diﬀerent side-lengths and round the points in P to the grid
points.
In the second part of our work (Sect. 3) we study the problem in Imprecise
model. In d-dimensional space, we propose a polynomial time
√
d-approximation
algorithm. We obtain this result by solving a linear program that formulates a
relaxed version of the problem. Next, for d = 2, we deﬁne the notion of α-
separability and use a combination of our previous methods to obtain a poly-
nomial time (1 + ϵ)-approximation algorithm for α-separable regions. For this
purpose, we ﬁrst determine a lower-bound on the minimum diameter by the
√
d
approximation algorithm, and convert the problem into an Indecisive instance.
Next, we use the results for Indecisive model to obtain a (1+ϵ)-approximation.

Approximate Minimum Diameter
241
2
Approximation Algorithm for MinDCS
We use a powerful technique that is widely used to design approximation
algorithms for geometric problems. The technique is based on the following
steps [1,8,13]:
1. Extract a small subset of data that inherited the properties of original data
(i.e., coreset)
2. Run lazy algorithm on the coreset.
For a set P of points in Rd, and an optimization problem X (e.g., X(P) would
be the diameter or width of P), a subset Q of the points of P is an ϵ-coreset, if
(1 −ϵ)X(P) ≤X(Q) ≤(1 + ϵ)X(P).
We state this fact, by saying that Q is an ϵ-coreset of P for X(.). Considering
the input data in indecisive model, we have the following deﬁnition of coreset:
given a set P of n points colored with m colors. We say Q is an ϵ-coreset of P
for MinDCS iﬀ: (i) Q contains at least one point of each color, (ii) D(Q) ≤
(1 + ϵ)Dmin.
1
0
1
1
0
0
0
0
0
1
0
1
0
1
1
1
0
1
0
0
0
1
1
0
0
ϵΔ
ϵΔ
(a)
(b)
p
q
Fig. 1. (a) The possible area of two points p and q. (b) A binary assignment for a grid.
2.1
Approximate Minimum Diameter
Deﬁnition 1. Given a set of colored points P and a set C of colors. P is C-legal
iﬀfor each c ∈C there exists a point p ∈P with color c.
Deﬁnition 2 (Possible area). Consider two points p and q. Draw two balls
of radius |pq|, one of them centered at p and the other centered at q. Name the
intersection area of these two balls as possible area (Cpq), see Fig. 1(a).
Observation 1. If p and q be the points that determine the diameter of a point
set, all the points in the set must lay in Cpq.
Regarding Observation 1, we compute an ϵ-approximation of MinDCS by
the process described in Algorithm 1. The algorithm operates as follows: let
S = {S1, S2, . . .} be the set of all pairs of points in P. For each Si = {pi, qi},

242
M. Ghodsi et al.
let Pi be the set of points in Cpiqi. For each Pi which is C-legal, we compute an
approximation of minimum diameter of Pi by Algorithm 2, as will be described
further. Next, among all computations, we choose the one with the minimum
value (Dalg).
Note that, since we consider all pairs of the points, for some C-legal pair Si
we have Dmin = |piqi| and hence Dalg is an ϵ-approximation of Dmin.
Algorithm 1. Minimum Diameter Approximation
1: function MinDiameterAPX(P)
2:
Dalg = null
3:
for each Si ∈S do
4:
if Pi is C-legal then
5:
Dapx(Pi) = DiameterAPX(Pi, {pi, qi})
6:
Dalg = min{Dalg, Dapx(Pi)}
7:
end if
8:
end for
9:
return Dalg
10: end function
In Algorithm 2, you can ﬁnd the procedure for ﬁnding an approximation of
minimum diameter for each Pi.
The description of Algorithm 2 is as follows: let Δ = |piqi|. First, we compute
the smallest axis parallel bounding box of Pi (B(Pi)). Next, we split B(Pi) into
the cells with side lengths ϵΔ and name the produced uniform grid as G. A binary
assignment of G is to assign 0 or 1 to each cell of G, see Fig. 1(b). Consider all
binary assignments of G. In the jth assignment, let Qj be the set of the cells
with value ‘1’. We call Qj legal, if the set of points in Qj’s cells is C-legal.
Number of the cells in G is O( 1
ϵd ) and hence, there are at most O(2
1
ϵd ) legal
assignments. For each cell of a legal Qj, choose an arbitrary point in that cell as
a representative. Next, we compute the diameter of the representatives in time
O(( 1
ϵd )2). Regarding all the computations, we return the minimum of them as
an approximation of D(Pi).
Note that if Si would be the optimum color selection of Pi, then obviously
there exists an assignment j, such that Qj is legal and includes the cells corre-
sponding to the points in Si.
In order to determine whether or not Qj is legal, we can check in O(n) time
the existence of each color in at least one cell of Qj.
Finally, for all C-legal Pi, we select the smallest among all approximations
of Pi as an approximation of Dmin.
Theorem 2. The MinDCS problem can be approximated in time O(2
1
ϵd .ϵ−2d.n3)
of factor (1 + ϵ) for ﬁxed dimensions.
Proof. Let Dalg be the value returned by our algorithm, and p, q ∈OPT be
the points with maximum distance in the optimal solution. Obviously, the set

Approximate Minimum Diameter
243
Algorithm 2. Approximate DPi Respect to p, q ∈Si
1: function DiameterAPX(Pi, Si)
2:
Let Si = {pi, qi}
3:
Δ = |piqi|
4:
D(Pi) = null
5:
Let G be a uniform grid on B(Pi) in d dimensional space with cells of size ϵΔ
6:
for each binary assignment of cells of G do
7:
Let Qj be the cells that is assigned a value of 1
8:
if Qj is legal then
9:
Let Q′
j be the set of representative points of cells
10:
D(Pi) = min{D(Pi), D(Q′
j)}
11:
end if
12:
end for
13:
return D(Pi)
14: end function
of points in Cpq is C-legal, and all the points in OPT are in Cpq. Consider the
binary assignment corresponding to OPT, i.e., a cell is 1, iﬀit contains at least
one point of OPT. Since this assignment is C-legal, it would be considered by
our algorithm. Thus, Dalg ≤(1+ϵ)D(OPT). On the other hand, the assignment
related to Dalg is also C-legal. Hence, Dalg ≥(1 −ϵ)D(OPT).
There are n2 diﬀerent possible areas. For each of them we have 2
1
ϵd diﬀerent
assignments. Checking the legality of each assignment takes O(n) time. For a
C-legal assignment we can ﬁnd the minimum diameter in O(ϵ−2d) time since
each assignment contains at most ϵ−d cells. Thus, total running time would be
O(2
1
ϵd .ϵ−2d.n3).
It is worth mentioning that we can improve the running time to O(2
1
ϵd .(ϵ−2d+
n3)) by a preprocessing phase that computes the diameter for every 2
1
ϵd diﬀerent
binary assignments and uses these preprocessed values for every pair of S.
3
Approximation Algorithm for MinDiam
In this section, we consider the problem of ﬁnding minimum diameter in
Imprecise model. As mentioned, in this model an imprecise point is represented
by a continuous region. Given a set R = {R1, R2, . . . , Rn} where each Ri is a
polygonal region. A selection S = {s1, s2, ..., sn} of R is a set of points where
si ∈Ri. In the minimum diameter problem, the goal is to ﬁnd a selection S
such that D(S) is minimized among all options. We consider the problem for
the case where every Ri is a polygonal convex region with constant complexity
(i.e., number of edges).
In Sect. 3.1, we present a polynomial time
√
d-approximation algorithm for
MinDiam.

244
M. Ghodsi et al.
3.1
√
d-approximation
Our
√
d-approximation construction is based on LP-programming which uses
the rectilinear distances (dl1) of the points. For now, suppose that d is constant
and consider LP 1. In this LP, we want to select the points s1, s2, . . . , sn such
that si ∈Ri and the rectilinear diameter of these points is minimized.
LP 1 :
minimize
ℓ
subject to
dl1(si, sj) ≤ℓ
∀i, j
si ∈Ri
∀i, j
(1)
We show that dl1(si, sj) can be expressed by 2d linear constraints (note that
dl1(si, sj) = |si,1−sj,1|+. . .+|si,d−sj,d|). Based on the relative value of si,k, sj,k,
there are 2d diﬀerent possible expressions for dl1(si, sj). For example, for d = 2,
we have:
dl1(si, sj) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
si,1 −sj,1 + si,2 −sj,2
si,1 ≥sj,1
si,2 ≥sj,2
sj,1 −si,1 + si,2 −sj,2
si,1 < sj,1
si,2 ≥sj,2
si,1 −sj,1 + sj,2 −si,2
si,1 ≥sj,1
si,2 < sj,2
sj,1 −si,1 + sj,2 −si,2
si,1 < sj,1
si,2 < sj,2
Call these expressions as e1, e2, . . . , e2d. Note that if dl1(si, sj) ≤ℓ, then
for all 1 ≤k ≤2n, ek ≤ℓ. With this in mind, we can replace dl1(si, sj) ≤ℓ
with these 2d linear expressions. It is worth mentioning that we can reduce the
number of expressions of this type in LP 1 using some additional variables, see
Sect. 3.2 for more details.
Since the regions are convex, the constraint si ∈Ri can be described by |Ri|
linear inequalities where |Ri| is the complexity of region Ri. Thus, the number
of the constraints of this type is |R| = 
i |Ri|. Therefore, total number of the
constraints would be 2d · n2 + |R| which is poly(n) for constant d.
Theorem 3. Dmin ≤ℓ≤
√
dDmin.
Proof. Let Smin be the optimal selection of R so that Dmin = D(Smin). Obvi-
ously, ℓ≥Dmin. On the other hand, Smin is a feasible solution for LP 1 with
rectilinear diameter at most
√
dDmin.
Theorem 3 shows that LP 1 results in a
√
d approximation algorithm for
MinDiam. We use this result to bound the optimal solution in Sect. 3.3.
3.2
Reducing the Number of Constraints
In this section, we describe how to reduce the number of constraints in LP 1.
For this, we use Observation 4.

Approximate Minimum Diameter
245
Observation 4. |x| is the smallest value z such that z ≥x and z ≥−x.
Thus, for the following LP, we have z = |x|:
LP 2 :
minimize
z
subject to
x ≤z
−x ≤z
(2)
(b)
A
B
R
R
sin (α/2)
α
X
Y
T
Z
(a)
A
B
R
R
sin (α/2)
α
X
Y
Z
T
Fig. 2. (a) Regions are separated by two intersecting lines. The optimal solution Smin,
must be entirely within the rectangle XY ZT. (b) The points inside the green area are
at least R distance away from the region A (similar fact is intact between the points
inside the blue area and the region B). (Color ﬁgure online)
We use Observation 4 to modify LP 1 into LP 3. In LP 3, Q is a large
enough integer, ensuring that minimizing ℓis more important than  di,j,k.
LP 3 :
minimize
Qℓ+

di,j,k
subject to

k
di,j,k ≤ℓ
∀i, j
si ∈Ri
∀i, j
di,j,k ≥si,k −sj,k
∀i, j, k
di,j,k ≥sj,k −si,k
∀i, j, k
(3)
Number of the constraints in LP 3 is O(n2d + |R|) which is poly(n).
3.3
(1 + ϵ)-approximation for d = 2
In this section, we propose an ϵ-approximation algorithm for MinDiam when
d = 2 and the regions in R admit a special notion of separability, called α-
separability.

246
M. Ghodsi et al.
α-separability
Deﬁnition 3. Two intersecting lines, divide the plane into four areas. Two
regions A and B are separated by these lines, if they completely belong to opposite
areas, See Fig. 2(a). In Fig. 2(a), we name α as the degree of separation.
Deﬁnition 4. A set S of regions is α-separable, if there exists two regions X
and Y in S, which are α-separable.
Deﬁnition 5. For two regions X and Y , maximum degree of separability is
deﬁned as the maximum α, such that X and Y are α-separable. Subsequently,
maximum degree of separability for a set R of regions, is the maximum possible
α, such that R is α-separable.
It’s easy to observe that the maximum degree of separability for two regions
can be computed in polynomial time by computing the common tangent of the
regions. Similarly, maximum degree of separability for a set R of regions, can be
computed in polynomial time.
Note that the maximum degree of separability is not well deﬁned for the case,
where every pair of regions have a nonempty intersection. We postpone resolving
this issue to Sect. 3.4. For now, we suppose that R contains at least two regions
that are completely disjoint.
(1 + ϵ)-approximation Algorithm
Theorem 5. Let R be an α-separable set of regions. then there exists an
ϵ-approximation algorithm for computing MinDiam of R with running time
O(2
1
ϵ2 .
n3
ϵ10. sin(α/2)3 ).
Proof. Assume that A, B are the regions in R, that are α-separable. Figure 2(a)
shows the regions and the separating lines. Let R be the value, with the property
that Dmin ≤R ≤
√
2Dmin. Such R can be computed by the
√
d-approximation
algorithm described in Sect. 3.1.
Argue that the points in the optimal solution Smin must be entirely within
the rectangle XY ZT (See Fig. 2(a)), while every point out of the rectangle has
distance more than R to at least one of the selected points from regions A or B
(see Fig. 2(b)). The area of XY ZT is
4R2
sin(α/2), that is, O(
R2
sin(α/2)).
Color each region Ri with color ci. Now, construct a grid G on the rectangle
XY ZT with cells of size ϵR. For each grid point p and region Ri containing
p create a point with the same location as p and color ci, see Fig. 3(a). Total
number of the points is O(
n
ϵ2 sin(α/2)).
Let P be the set of generated points. We give P as an input instance for
MinDCS problem. Next, using Algorithm 1 we ﬁnd a color selection Q with
D(Q) ≤(1+ϵ)Dmin where Dmin is the diameter of the optimal selection. While
the diameter of each cell in G equals
√
2ϵ = O(ϵ), total error of selection Q
for MinDiam is O(ϵ). Considering the time complexity obtained for MinDCS,
total running time would be O(2
1
ϵ2 .
n3
ϵ10·sin (α/2)3 ).

Approximate Minimum Diameter
247
A
B
(a)
(b)
α
A
B
ϵR
ϵR
Fig. 3. (a) Constructing a grid on XY ZT. (b) Special cases.
Note that 1/ sin(α) is exponentially descending for 0 ≤α ≤π/2. As an
example, for α > 20◦, 1/ sin(α) < 3 and for α > 45◦, 1/ sin(α) < 1.5. Thus, for
large enough α (e.g. α > 10), we can consider this value as a constant.
3.4
A Discussion on α-separability
As previously mentioned, α-separability is not well deﬁned in the case that all
pairs of the regions have a nonempty intersection. In this section, we wish to
address this issue.
Note that by Helly’s theorem, if every triple regions of the set R have a
nonempty intersection, then the intersection of all the regions in R is nonempty,
i.e., 
i Ri ̸= ∅. In this case, the optimal selection in MinDiam is trivial: select
the same point for all the regions. Thus, we can assume that there exists a set
of three regions, namely, A, B, and C in R such that any pair of them has a
nonempty intersection, but A ∩B ∩C is empty.
Denote by pA, pB, and pC the selected points from the regions A, B, and C,
respectively. To solve the problem of MinDCS on R we divide the problem into
three sub problems: (i) {pA, pB} ∈A ∩B, (ii) pA ∈(A \ B) and pB ∈(B \ A),
and (iii) pA ∈A ∩B and pB ∈(A \ B).
For the ﬁrst case, we can assume w.l.o.g that pA and pB are in the same
location in A ∩B. Thus, we can remove AΔB and only keep A ∩B. Since the
intersection of (A ∩B) and C is empty, they are separable.
For the second case, we partition AΔB into the constant number of convex
sub-regions (for example, by triangulating AΔB as shown in Fig. 3(b)). Suppose
that A \ B is divided into r sub-regions and B \ A is divided into s sub-regions.
Then we solve r×s sub-problems, according to the sub-regions pA and pB belong.
In all of these sub-problems, α-separability is well deﬁned.

248
M. Ghodsi et al.
Finally, note that considering the third case is not necessary, since if pA ∈
A ∩B and pB /∈A ∩B then we can move pB to the same location as pA without
any decrease in the diameter of the selected points.
4
Conclusions and Future Works
In this paper, we tried to address the diameter problem in two models of
uncertainty. In Sect. 2, we investigate on the problem of minimum diameter in
Indecisive model. For this problem, we present an approximation algorithm
with factor (1 + O(ϵ)) and running time O(2
1
ϵd .ϵ−2d.n3). We follow the idea
introduced by Consuegra et al. in [3].
In Sect. 3, we studied the MinDiam problem, where each imprecise point is
represented with a convex regions. For this problem, we presented a polynomial
time
√
d-approximation algorithm for constant d. next, we used this result to give
a (1+ϵ)-approximation for the case, where d = 2 and the regions are α-separate.
A future direction would be generalizing the (1+ϵ)-approximation algorithm
proposed for MinDiam to higher dimensions. In addition, one can think of
removing the condition of α-separability for this problem.
References
1. Agarwal, P.K., Har-Peled, S., Varadarajan, K.R.: Approximating extent measures
of points. J. ACM (JACM) 51(4), 606–635 (2004)
2. Aggarwal, C.C.: Trio a system for data uncertainty and lineage. In: Aggarwal, C.C.
(ed.) Managing and Mining Uncertain Data, pp. 1–35. Springer, Heidelberg (2009)
3. Consuegra, M.E., Narasimhan, G., Tanigawa, S.I.: Geometric avatar problems. In:
IARCS Annual Conference on Foundations of Software Technology and Theoretical
Computer Science (FSTTCS 2013), vol. 24, pp. 389–400. Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik (2013)
4. Cormode, G., Li, F., Yi, K.: Semantics of ranking queries for probabilistic data
and expected ranks. In: IEEE 25th International Conference on Data Engineering,
ICDE 2009, pp. 305–316. IEEE (2009)
5. Fan, C., Luo, J., Zhong, F., Zhu, B.: Expected computations on color spanning
sets. In: Fellows, M., Tan, X., Zhu, B. (eds.) AAIM/FAW -2013. LNCS, vol. 7924,
pp. 130–141. Springer, Heidelberg (2013). doi:10.1007/978-3-642-38756-2 15
6. Fleischer, R., Xu, X.: Computing minimum diameter color-spanning sets. In: Lee,
D.-T., Chen, D.Z., Ying, S. (eds.) FAW 2010. LNCS, vol. 6213, pp. 285–292.
Springer, Heidelberg (2010). doi:10.1007/978-3-642-14553-7 27
7. Fleischer, R., Xu, X.: Computing minimum diameter color-spanning sets is hard.
Inf. Process. Lett. 111(21), 1054–1056 (2011)
8. Har-Peled, S.: Geometric Approximation Algorithms, vol. 173. American Mathe-
matical Society, Boston (2011)
9. Jørgensen, A., L¨oﬄer, M., Phillips, J.M.: Geometric computations on indecisive
points. In: Dehne, F., Iacono, J., Sack, J.-R. (eds.) WADS 2011. LNCS, vol. 6844,
pp. 536–547. Springer, Heidelberg (2011). doi:10.1007/978-3-642-22300-6 45
10. Ju, W., Fan, C., Luo, J., Zhu, B., Daescu, O.: On some geometric problems of
color-spanning sets. J. Comb. Optim. 26(2), 266–283 (2013)

Approximate Minimum Diameter
249
11. L¨oﬄer, M.: Data imprecision in computational geometry (2009)
12. L¨oﬄer, M., van Kreveld, M.: Largest bounding box, smallest diameter, and related
problems on imprecise points. Computat. Geom. 43(4), 419–433 (2010)
13. Zarrabi-Zadeh, H.: Geometric approximation algorithms in the online and data
stream models. Ph.D. thesis, University of Waterloo (2008)
14. Zhang, D., Chee, Y.M., Mondal, A., Tung, A., Kitsuregawa, M.: Keyword search
in spatial databases: towards searching by document. In: IEEE 25th International
Conference on Data Engineering, ICDE 2009, pp. 688–699. IEEE (2009)

On Constant Depth Circuits Parameterized
by Degree: Identity Testing
and Depth Reduction
Purnata Ghosal, Om Prakash, and B.V. Raghavendra Rao(B)
Department of Computer Science and Engineering, IIT Madras, Chennai, India
purnatag@gmail.com, op708543@gmail.com, bvrr@cse.iitm.ac.in
Abstract. In this article we initiate the study of polynomials parame-
terized by degree by arithmetic circuits of small syntactic degree. We
deﬁne the notion of ﬁxed parameter tractability and show that there are
families of polynomials of degree k that cannot be computed by homo-
geneous depth four ΣΠ
√
kΣΠ
√
k circuits. Our result implies that there
is no parameterized depth reduction for circuits of size f(k)nO(1) such
that the resulting depth four circuit is homogeneous.
We show that testing identity of depth three circuits with syntactic
degree k is ﬁxed parameter tractable with k as the parameter. Our algo-
rithm involves an application of the hitting set generator given by Shpilka
and Volkovich [APPROX-RANDOM 2009]. Further, we show that our
techniques do not generalize to higher depth circuits by proving certain
rank-preserving properties of the generator by Shpilka and Volkovich.
1
Introduction
Parameterized Complexity is the discipline where an additional parameter along
with the input is used for measuring the complexity of computational prob-
lems. This leads to more ﬁne-grained complexity classiﬁcation of computational
problems and a relaxed notion of tractability. Downey and Fellows [9] were the
ﬁrst to study complexity of problems with a parameter, and develop the area
of parameterized complexity theory. Over the last two decades, parameterized
complexity has played a pivotal role in algorithmic research [9].
Fixed Parameter Tractability (FPT) is the notion of tractability in Parameter-
ized Complexity Theory. A decision problem with parameter k that is decidable
in deterministic time f(k)poly(n) is said to be ﬁxed parameter tractable (FPT for
short). The whole area of parameterized complexity theory is centered around
this deﬁnition of tractability. The parameterized intractable problems are based
on the hierarchy of classes known as the W-hierarchy. The smallest member of
W-hierarchy, W[1] consists of problems that are FPT equivalent to the clique
problem with the size of the clique as the parameter.
Motivation: Parameterized complexity of problems based on graphs and other
combinatorial structures played pivotal role in the development of Parameter-
ized Algorithms and Complexity Theory. Many of the parameterized algorithms
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 250–261, 2017.
DOI: 10.1007/978-3-319-62389-4 21

On Constant Depth Circuits Parameterized by Degree
251
involve evaluation of polynomials of degree bounded by the parameter. For exam-
ple, in [5], Bj¨orklund et. al., deﬁned and used a degree k polynomial which is
identically zero if an only if the given graph has no cycle of length k or smaller.
Polynomials of degree bounded by the parameter has been extensively used to
develop eﬃcient randomized parameterized algorithms, see e,g., [2,4,13]. Fur-
ther, obtaining a deterministic f(k)nO(1) time algorithm checking if a n variate
polynomial of degree at most k is zero or not for some function f of k would
lead to fast deterministic FPT algorithms for a wide variety of problems. The
construction of representative sets in [12] can also be viewed a parameterized
de-randomization of the polynomial identity testing problem for a special class
of polynomials where the monomials have a matroidal structure.
Wide application of polynomials whose degree is bounded by the parameter
in Parameterized Complexity Theory merits a study of such polynomials in
algebraic models of computation. We initiate the study of polynomials with the
degree as the parameter.
Our Model: Motivated by the applications of polynomials parameterized by
degree, we study the parameterized complexity of polynomials parameterized
by degree. More speciﬁcally, given a parameter k = k(n), classify the families of
polynomials of degree k based on the minimum size of the arithmetic circuit com-
puting it. Here, the notion of eﬃciency is based on the ﬁxed parameter tractabil-
ity. A natural model of computation for polynomials with degree bounded by k
would be arithmetic circuits where every gate computes a polynomial of degree
at most k. However, such a circuit of size f(k)nO(1) for some function f of k
can compute polynomials with coeﬃcients as large as 22n where n is the num-
ber of variables, and hence evaluation of such polynomials unlikely to be Fixed
Parameter Tractable. Thus we need models were the coeﬃcients computed can
be represented in f(k)nO(1) many bits. Towards this, we consider the arithmetic
circuits where the syntactic degree (see Sect. 2 for a deﬁnition) is bounded by
the degree of the polynomial being computed as the computational model.
Further, we study the parameterized version of Arithmetic Circuit Identity
Testing (ACIT) problem: testing if the given arithmetic circuit computes the
zero polynomial or not. ACIT is one of the fundamental computational questions
on polynomials. Schwartz [22] and Zippel [26] independently showed that there
is a randomized polynomial time algorithm for ACIT. Their algorithm worked
for a more general setting, where the polynomials are given in the black-box
form. However, obtaining deterministic polynomial time algorithm for ACIT has
been one of the prominent open questions for decades, playing a pivotal role in
Algebraic Complexity Theory. Motivated by the application of ACIT in several
parameterized algorithms [2,4,5,13] with degree as the parameter, we study the
complexity of ACIT with degree as the parameter.
Our Results: We deﬁne the notion of ﬁxed parameter tractability (FPT) of a
family of polynomials parameterized by the degree. We show that there are
polynomials that have FPT circuits but cannot be computed by depth four
homogeneous ΣΠ
√
kΣΠ
√
k circuits of FPT size (Theorem 2). Also, we show a
parameterized separation between depth three and four circuits (Theorem 4).

252
P. Ghosal et al.
Then we study the parameterized complexity of the arithmetic circuit identity
testing problem and show that non-black box identity testing of depth three
circuits of syntactic degree at most k is ﬁxed parameter tractable (Theorem 5).
This result is obtained by the application of a hitting set generator deﬁned
by Shpilka and Volkovich [23]. Finally we show that the techniques used in
Theorem 5 cannot be used to obtain eﬃcient parameterized identity tests for
depth four circuits by proving that the generator given by [23] preserves rank of
certain matrix associated with polynomials (Theorem 6).
Related work: Though algebraic techniques have been well utilized in obtain-
ing eﬃcient parameterized algorithms, the focus on parameterized complexity
of algebraic problems is very limited. Chen et. al. [8] studied the parameterized
complexity of detecting and testing monomials in polynomials given as arith-
metic circuits. Arvind et. al [3] obtained parameterized algorithms for solving
systems of linear equations parameterized by the hamming weight. Engels [10]
developed Parameterized Algebraic Complexity theory in analogy to Valiant’s
notion of Algebraic Complexity. Apart from these results, there have not been
much attention on algebraic problems in the parameterized world.
M¨uller [16] studied several parameterized variants of ACIT and obtained ran-
domized parameterized algorithms for those variants that use O(k log n) random
bits where k is the parameter. Further, Chauhan and Rao [7] studied ACIT with
the syntactic degree of the circuit as a parameter, and showed that the problem
has a randomized algorithm that uses only O(k log n) random bits, where k is the
syntactic degree. Finally, it can be seen easily from the observations in [7] that
ACIT with syntactic degree as a parameter is equivalent to the same problem
with the number of variables as a parameter.
2
Preliminaries
In this section we will introduce necessary notions on arithmetic circuits and
parameterized complexity. For more details the reader is referred to [9].
An arithmetic circuit C over a ﬁeld F and variables X = {x1, . . . , xn} is a
labeled directed acyclic graph. The nodes in C are called gates. Gates of zero
in-degree are called input gates and are labeled by either variables in X, or
constants in F. Other gates in C are labeled by either × or +. Gates with zero
out-degree are called output gates. In our applications, an arithmetic circuit will
have a unique output gate. Every gate in C naturally represents a polynomial
in F[x1, . . . , xn]. The polynomial computed by C is the polynomial represented
at the output gate.
Depth of an arithmetic circuit is the length of the longest path from an input
gate to the output gate. In this paper, our focus is on constant depth arithmetic
circuits. It should be noted that, constant depth circuits are interesting only
when the fan-in of gates are allowed to be unbounded. ΣΠΣ denotes the class
of depth three circuits of the form r
i=1
t
j=1 ℓi,j for some r, t ≥0 and ℓi,js
are linear functions of the input variables. Similarly, depth four ΣΠΣΠ circuits
are deﬁned. The fan-in restriction on the gates at a speciﬁc layer are denoted

On Constant Depth Circuits Parameterized by Degree
253
by superscripts, e.g., ΣΠkΣ denotes the sub class of ΣΠΣ circuits where the
middle layer of product gates have fan-in bounded by k.
Saxena [21] introduced the notion of dual representation of polynomials. Let
f ∈F[x1, . . . , xn]. Then f is said to have a dual representation of size t, if there
are univariate polynomials gij(xj) such that f = t
i=1 gi1(x1) · · · gin(xn).
The syntactic degree denoted by syntdeg for every gate v of an arithmetic
circuit is deﬁned as follows:
syntdeg(v) =
⎧
⎪
⎨
⎪
⎩
1
if v is an input gate
max{syntdeg(v1), syntdeg(v2)}
if v = v1 + v2
syntdeg(v1) + syntdeg(v2)
if v = v1 × v2
We need the notion of hitting sets for arithmetic circuits. Let Cn be a class
of polynomials in n variables. A set Hn ⊆Fn is called a hitting set the class C
with n inputs, such that for all polynomials f ∈C, f ̸≡0, ∃a ∈H, f(a) ̸= 0.
We also require the notion of hitting set generators. Consider a polynomial
mapping G = (G1, . . . , Gn) : Ft →Fn where G1, . . . , Gn are t variate polynomi-
als. G is a generator for the circuit class Cn if for every polynomial f ∈C, f ̸≡0,
it holds that G(f) ̸≡0 where G(f) = f(G1, . . . , Gn).
It is known that the image of a generator for polynomials in C contains a
hitting-set for all non-zero polynomials in C [23]. In this paper we will require a
hitting set generator deﬁned by Shpilka and Volkovich [23].
Deﬁnition 1 (S-V Generator [23]). Let a1, a2 . . . an be distinct elements in
the given ﬁeld F. Let Gi
k ∈F[y1, y2 . . . yk, z1, z2 . . . zk] be the polynomial deﬁned
as follows:
Gi
k(y1, y2 . . . yk, z1, z2 . . . zk) =
k

j=1
Li(yj)zj, where Li(x) =

j̸=i(x −aj)

j̸=i(ai −aj).
The generator Gk is deﬁned as Gk
Δ= (G1
k, . . . Gn
k).
For a polynomial f, let Gk(f) ≜f(G1
k, . . . , Gn
k). In [23], Shpilka and Volkovich
showed that Gk is a hitting set generator for sum of k read-once polynomials.
Further, in [7] Chauhan and Rao showed that the generator Gk is also a hitting
set generator for degree k polynomials:
Lemma 1. For any n-variate polynomial f of degree k, f ≡0 ⇐⇒Gk(f) ≡0.
A parameterized problem is a set P ⊆Σ∗×N, where Σ is a ﬁnite set of alphabets.
If (x, k) ∈Σ∗× N is an instance of a parameterized problem, we refer to x as
the input and k as the parameter.
Deﬁnition 2 (Fixed Parameter Tractability). A parameterized problem
P ⊆Σ∗×N is ﬁxed-parameter tractable (FPT) if there is a computable function
f : N →N, a constant c ∈N and an algorithm that, given a pair (x, k) ∈Σ∗×N,
decides if (x, k) ∈P in at most f(k)poly(n) steps, where n is the length of input.

254
P. Ghosal et al.
ACIT is the problem of testing whether a given arithmetic circuit C computes
a polynomial p that is identically zero. There have been several parameterized
variants of ACIT studied in the literature [16]. Following [7] we consider the
syntactic degree of the arithmetic circuit as a parameter.
Problem para-ACIT
Input A polynomial p given as an arithmetic circuit C of syntactic degree k.
Parameter : k
Output YES iﬀp ≡0.
Chauhan and Rao [7] show that para −ACIT ∈W[P] −RFPT, a parameter-
ized class analogous to RP. In this paper we consider the problem when restricted
to depth three circuits.
We consider the notion of partial derivative matrix of a polynomial deﬁned
by Nisan [17]. Raz [18] used a variant of partial derivative matrix, which later
were generalized by Kumar et al [15]. In this paper we consider yet another
variant of partial derivative matrices:
Deﬁnition 3. Let f ∈F[x1, . . . , xn] be a polynomial of degree d, ϕ : X →Y ∪Z
be a partition of the input variables of f. Then the coeﬃcient matrix Mf ϕ has its
rows indexed by monomials μ of degree at most d in variables in Y , and columns
indexed by monomials ν of degree at most d in variables in Z. For monomials
μ and ν respectively in variables Y and Z, the entry Mf ϕ(μ, ν) is the coeﬃcient
of the monomial μν in f.
The deﬁnition above is an adaptation of [17] using a partition of variables and
diﬀerent from the notion of polynomial coeﬃcient matrices in [15]. The coeﬃcient
matrix of a polynomial is well studied in the literature, however, the speciﬁc form
used in the above deﬁnition has not been mentioned explicitly in the literature.
The following fundamental properties of the rank of the coeﬃcient matrix follow
directly from [18].
Lemma 2. If f, g, h ∈F[X] such that f = g + h, then ∀ϕ : X →Y ∪Z,
rank(Mf ϕ) ≤rank(Mgϕ) + rank(Mhϕ).
Recall that an arithmetic circuit is a formula if the out-degree of every gate is
either one or zero. Finally, we need the following result on testing identity of non-
commutative formulas. Let F{x1, x2 . . . , xn} be the non-commutative polynomial
ring over the ﬁeld F. The following theorem was proved by Raz and Shpilka [19].
Theorem 1 [19].
Let C ∈F{X} be a non-commutative arithmetic formula,
then there is a white-box identity testing algorithm for C having time complexity
O(size(C)).
3
Parameterization by Degree
3.1
Fixed Parameter Tractability in Arithmetic Computation
In this section we deﬁne the notion of parameterized tractability of polynomials
over Z parameterized by degree.

On Constant Depth Circuits Parameterized by Degree
255
Deﬁnition 4. Let k = k(n). A family (pn)n≥0 of polynomials over Z is said to
be degree k parameterized if
– There is a c > 0 such that pn is an nc variate polynomial for every n ≥0;
– Degree of pn is bounded by k = k(n) for every n ≥0; and
– The absolute value of the coeﬃcients of pn is bounded by 2g(k)nc, for some
function g that depends only on k.
Since any arithmetic circuit can be homogenized eﬃciently, we have:
Proposition 1. [6] For any parameterized polynomial family (p, k) if there is
a family of arithmetic circuits C = (Cn)n≥0 of size f(k)nc computing p, where
f(k) is a function of k and c is a constant, then there is a family of arithmetic
circuits C′ = (C′
n) of size f ′(k)nc′ for p such that every gate in C′
n computes a
polynomial of degree at most k.
It can be seen that a circuit Cn of size f(k)nO(1) where every gate computes
a polynomial of degree at most k can compute polynomials where the absolute
value of the coeﬃcient can be as large as 22nO(1)
even when the only constants
allowed in the circuit are {−1, 0, 1}. This makes the evaluation of such polyno-
mial in FPT time unfeasible. A natural restriction solution would be to bound
the syntactic degree of the circuit. An arithmetic circuit Cn is said to be of
syntactic degree d if every gate in Cn has syntactic degree bounded by d.
A degree parameterized polynomial family (p = (pn)n≥0, k) with k as the
parameter is said to be ﬁxed parameter tractable (FPT) if for every n ≥0, there
is an arithmetic circuit Cn of syntactic degree at most k and of size f(k)nc
computing pn where f is a function of k and c is a constant.
3.2
Parameterized Depth Reduction
Depth reduction is one of the most fundamental structural aspects in algebraic
complexity theory: Given a polynomial family p = (pn)n≥0 and a size bound
s = s(n), what is the minimum depth of an arithmetic circuit of size s computing
p? The parameterized depth reduction problem can be stated as:
Given a parameterized polynomial family (p, k) in FPT what is the minimum
depth of a size f(k)nc circuit computing p where f(k) is an arbitrary function
of k and c is some constant.
By applying the well known depth reduction technique in [25], we have:
Proposition 2. [25] Any parameterized polynomial family (p, k) in FPT can be
computed by circuits of depth f(k) log n and size f ′(k)nO(1), for some functions
f and f ′ that depend only on the parameter.
In a surprising result, Agrawal and Vinay [1] showed that any homogeneous
polynomial p computed by polynomial size arithmetic circuits can be computed
by depth four ΣΠ
√nΣΠ
√n homogeneous circuits of size 2O(√n log2 n). Further,
Tavenas [24] improved this bound to 2
√n log n. Over inﬁnite ﬁelds, there is a
depth three ΣΠΣ circuit of size 2
√n log n for p [14].

256
P. Ghosal et al.
A parameterized counterpart of the depth reduction in [1] would be to trans-
form a circuit Cn of size f(k)nO(1) and syntactic degree k to a depth four ΣΠΣΠ
circuit of syntactic degree k and size f ′(k)nO(1) where f and f ′ are functions of
k alone. Note that a ΣΠΣΠ circuit C of syntactic degree k will have Π fan-in
bounded by k at both of the Π layers and hence can assumed to be of the form
ΣΠkΣΠk. Further, if C is homogeneous with the bottom Π layer having syn-
tactic degree t then C can be assumed to be a homogeneous ΣΠk/tΣΠt circuit.
We ﬁrst observe that we can replace Π gates with ∧(powering) gates in any
depth four circuit with syntactic degree bounded by the parameter k. The proof
is a direct application of Fischer’s identity [11] twice and is omitted.
Lemma 3. Let C be a ΣΠk/tΣΠt circuit of size s computing a polynomial p
over Z. Then there is a Σ∧k/tΣ∧tΣ circuit C′ of size max{2k/t, 2t}·s computing
p. Moreover, if C is homogeneous, so does C′.
Thus a parameterized version of depth reduction in [1] would imply that
every parameterized polynomial family (p, k) in FPT can be computed by a
homogeneous Σ ∧O(
√
k) Σ ∧O(
√
k) Σ circuit of size f(k)nO(n) for some function
f of k. However, in the next section, we observe that this is not possible.
3.3
Parameterized Lower Bound for Σ∧O(
√
k)Σ ∧O(
√
k) Σ circuits
In this section we prove lower bounds against homogeneous Σ∧O(
√
k)Σ∧O(
√
k)Σ
circuits using the well known method of partial derivatives. To begin with, we
obtain an upper bound on the dimension of partial derivatives of small powers
of sum of powers of homogeneous linear forms:
Lemma 4. Let f = ℓd
1 + ℓd
2 + · · · + ℓd
t where ℓ1, . . . , ℓt are linear forms in
{x1, . . . , xn}. Then, for any r ≤k, dim(⟨∂=rf α⟩) ≤g(k)tα for α = o(k), d = o(k)
and some computable function g that depends only on k.
Now, we obtain a polynomial with large dimension of partial derivatives:
Lemma 5. There is a polynomial p ∈F[x1, . . . , xn] of degree k that can be
computed by polynomial size ΠΣ∧2 circuits with dim(⟨∂≤k/2p⟩) = nΩ(k).
Combining Lemmas 4 and 5 (whose proofs we omit) along with the sub-
additivity property of the dimension of partial derivatives, we get:
Theorem 2. There is a polynomial p ∈ΣΠk/2Σ∧2 such that any homogeneous
Σ ∧o(k) Σ ∧Σ circuit computing it will have top fan-in at least nΩ(k).
Proof. Consider f ∈Σs ∧o(k) Σt ∧Σ such that f = f1 + · · · + fs where fi ∈
∧o(k)Σt ∧Σ and set r = k/3. By Lemma 4 we have dim(⟨∂=rfi⟩) ≤g(k)tα for
some g. Then dim(⟨∂=rf⟩) ≤
i dim(⟨∂=rfi⟩) ≤sg(k)tα. Since t = f(k)nO(1)
for some function f of k, α = o(k) and dim(⟨∂=rp⟩) = nΩ(k), we have s = nΩ(k).
It can be noted that the lower bound in Theorem 2 also holds for the degree
k elementary symmetric polynomial Symn,k(X) = 
S⊆[n],|S|=k

i∈S xi:

On Constant Depth Circuits Parameterized by Degree
257
Corollary 1. Any homogeneous Σ ∧o(k) Σ ∧Σ circuit computing Symn,k will
have top fan-in at least nΩ(k).
However, the homogeneity condition in Corollary 1 is necessary due to the fol-
lowing result in [20]:
Proposition 3. Symn,k can be computed by 2O(
√
k)nO(1) size Σ ∧
√
k Σ ∧O(k) Σ
circuit.
3.4
Parameterized Lower Bounds for Depth Three Circuits
In this section, we prove parameterized lower bound against depth three ΣΠΣ
circuits of syntactic degree bounded by the parameter. We use the method of
partial derivative matrix used in [18]. We begin with a lower bound on the
measure for a polynomial computed by depth four circuits.
Lemma 6. Let X = {y1, . . . , ym, z1, . . . , zm}. Let f = k
i=1 Qi where Qi = 1 +
y (i−1)m
k
+1z (i−1)m
k
+1+y (i−1)m
k
+2z (i−1)m
k
+2+. . .+y im
k z im
k are multivariate quadratic
polynomials in F[X], then ∃ϕ : X →Y ∪Z such that rank(Mf ϕ) = Ω(( m+k
k )k).
Now, we need the folklore fact that for any partition of its variables, a poly-
nomial with a ‘small’ dual representation will have rank of the coeﬃcient matrix
small.
Lemma 7. (folklore) Suppose f = t
i=1 gi,1(x1)gi,2(x2) . . . gi,n(xn). Then, for
all partitions ϕ : X →Y ∪Z, rank(Mf ϕ) ≤t where gi,1, gi,2, . . . , gi,n are
univariate polynomials in x1, x2, . . . , xn respectively.
We omit the proofs of the Lemmas 7 and 6. As an immediate consequence of
these lemmas we have:
Theorem 3. There exists a polynomial f ∈F[x1, . . . , xn], f = s
i=1
di
j=1 Qi,j,
where s > 0, Qi,j is a multivariate quadratic polynomial with maxi{di} ≤k, such
that f = Σt
i=1gi,1(x1)gi,2(x2) . . . gi,n(xn)
=⇒
t = nΩ(k) where gi,1gi,2 . . . gi,n
are univariate polynomials of syntactic degree k = O(log n).
Further, we observe that depth three ΣΠΣ circuits of syntactic degree k,
compute polynomials of ‘small’ degree under every partition:
Lemma 8. Let f ∈F[x1, . . . , xn], f ∈ΣΠkΣ and Mf ϕ is the coeﬃcient matrix
corresponding to the partition of variables of f, ϕ : X →Y ∪Z. Then
rank(Mf ϕ) ≤s · 2O(k), where s is the smallest size of a ΣΠkΣ circuit for f.
Proof. Let f ∈F[x1, . . . , xn], f ∈ΣΠkΣ. Hence f =  k n
i aixi, where
ai ∈F. Then, ∀ϕ, rank(Mf ϕ) ≤2O(k)s since rank(Mℓϕ) ≤2, for all linear
functions ℓ= n
i=1 aixi; and rank of the coeﬃcient matrix is sub-multiplicative
and sub-additive.
Combining Lemmas 6 and 8 we get the following immediately:
Theorem 4. ΣΠkΣ ⊊ΣΠk/2ΣΠ2.
In the next section, we study the parameterized complexity of the identity testing
problem for the models considered in this section.

258
P. Ghosal et al.
4
Para-ACIT for constant depth circuits
This section is devoted to the study of the ACIT problem for depth three and
four circuits. We obtain a non-black box identity test for the class of ΣΠkΣ
circuits and argue the limitations of the technique used in the test.
4.1
Depth Three Circuits Parameterized by Degree
In this section we show that the non-black box para-ACIT for depth three ΣΠkΣ
circuits is FPT. The proof follows by the application of S-V generator and
Theorem 1.
Theorem 5. Let f be a polynomial of the form m
i=1
di
j=1 ℓi,j where ℓi,js are
linear forms. There is a white-box identity testing algorithm for C that runs
in time g(k).size(C)O(1), where g is a computable function, i.e., the white-box
ACIT for ΣΠΣ circuits with syntactic degree as the parameter is in FPT.
Theorem 5 can also be obtained by applying the Fischer’s identity on ΣΠkΣ
circuits to obtain equivalent Σ ∧k Σ circuits and then applying the techniques
in [21]. However, this works only over inﬁnite ﬁelds, whereas our proof above
works over any ﬁeld, though the identity test needs a suitable extension of the
underlying ﬁeld.
A natural next step would be to obtain dual representations for parameterized
families computed by ΣΠkΣΠk circuits. However, from Theorems 3 and 4 it
follows that there are depth four ΣΠk/tΣΠt arithmetic circuits that cannot
have dual representation of FPT size. This does not rule out the possibility of
getting a dual representation via the application of S-V generator. However, in
the following section, we show that every image of a polynomial f under the
S-V generator has many partitions that preserve the rank of the polynomial
coeﬃcient matrix of f.
4.2
Rank Preserving Property of the S-V Generator
In this section, we show that the rank of the coeﬃcient matrix of a polynomial
is invariant under the S-V generator map Gk.
Theorem 6. Let f ∈F[x1, . . . , xn] be a polynomial of degree ≤k. Let g =
G2k(f). Then, ∃ϕ, rank(Mf ϕ) ≥r
=⇒
Prϕ′[rank(Mgϕ′ ) = Ω(r)] ≥Ω(1/k2)
where the probability is over the uniform distribution over the set of all partitions
ϕ′ of a set of 4k variables into two equal parts.
Proof. Fix a1, . . . , an ∈F be distinct. Then,
Gk(xi) =
k

p=1
zpLi(yp) =
k

p=1
zp

j̸=i(yp −aj)

j̸=i(ai −aj)

On Constant Depth Circuits Parameterized by Degree
259
=
k

p=1
zp
(yp −a1) . . . (yp −ai−1)(yp −ai+1) . . . (yp −an)
(ai −a1) . . . (ai −ai−1)(ai −ai+1) . . . (ai −an)
=
k

p=1
zp(yn−1
p
−

j̸=i
ajyn−2
p
+ . . . + (−1)n−1 	
j̸=i
aj).
Therefore, Gk(xi) =
k

p=1
n

q=1
zpyn−q
p
(−1)q 	
∀tjt̸=i
aj1 . . . ajq
=

p∈[k]
q∈[n]
zpyn−q
p
cpqi
(where cpqi = (−1)q 	
∀tjt̸=i
aj1 . . . ajq).
Using the above for a monomial m = xi1xi2 . . . xik, we get:
Gk(m) =

p1,...,pk∈[k]
q1...qk∈[n−1]
zp1 . . . zpkyn−q1
p1
. . . yn−qk
pk
k
	
j=1
cpjqjij
Let Mk be the set of all degree k monomials in the variables {x1, . . . , xn}.
Let Snk be the set of all monomials of the form 
i∈I ziyn−qi
i
, for all multi-sets
I ⊆{1, . . . , k} of size k and 1 ≤qi ≤n −1 for every i.
Let V = Span(Mk), and W = Span(Snk) be the vector spaces spanned
by the the sets. The vector space V contains all polynomials in F of degree k,
and hence the dimension of V is

n+k
k

. Also, dimension of W is bounded by

2k
k

nk. Note that Gk is indeed a linear map from V to W. Let C be the be the

n+k
k

×

2k
k

nk matrix representing Gk. Then, ∀w ∈W, Gk(w) = CwT ∈V .
Now, we argue that C has full row-rank.
Claim. C has full row-rank.
Proof (of the Claim). Suppose C is not of full row rank. Then ∃αi1, . . . , αir ∈R,
such that r
j=1 αijC[ij] = 0 with αij ̸= 0 for some j, where C[i] represents the
ith row of C, and r ≤dim(V ). As Gk is linear, there must be some vi1, . . . , vir
such that Gk(vij) = C[ij]. Then we have:
r

j=1
αijGk(vij) =
r

j=1
Gk(αijvij) = 0 =⇒Gk(αi1vi1 + . . . + αirvir) = 0.
We can see that P ≡αi1vi1 + . . . + αirvir is a polynomial of degree at most
k in F[x1, . . . , xn], such that Gk(P) ≡0, whereas P ̸≡0 since ∃αij ̸= 0. This
contradicts Lemma 1. Hence, the Claim is proved.
Consider a partition ϕ : X →A∪B and suppose rank(Mf ϕ) ≥r. Let m1, . . . , mr
be r linearly independent rows of Mf (chosen arbitrarily). Let p1, . . . , pr be

260
P. Ghosal et al.
the polynomials representing these rows, i.e., pi = 
S⊆B Mf[mi, mS]mS. Then
p1, . . . , pr are linearly independent, i.e., ∀α1 . . . αr ∈F, r
i=1 αipi = 0
=⇒
∀i, αi = 0. Let qi = Gk(pi), 1 ≤i ≤r then clearly, r
i=1 αiqi = 0 =⇒∀i, αi =
0. This however, is not suﬃcient, since the partition ϕ does not imply a partition
on Y ∪Z. To overcome this diﬃculty we consider the generator G2k rather than
Gk. Note that for any degree k polynomial f, G2k(f) ≡0
⇐⇒
Gk(f) ≡0.
Suppose G2k : F[x1, . . . , xn] →F[Y ′ ∪Z′] where Y ′ = {y1, . . . , y2k} and Z′ =
{z!, . . . , z2k}. Consider arbitrary partitions:Y ′ = Y1 ∪Y2, |Y1| = |Y2| = k, and
Z′ = Z1 ∪Z2, |Z1| = |Z2| = k. Deﬁne the map 
G2k = ( 
G(1)
2k , . . . , 
G(n)
2k ), where

G(i)
2k = 
G2k(xi) =

G(i)
2k|{w=0|w∈Y2∪Z2}
if i ∈A
G(i)
2k|{w=0|w∈Y1∪Z1}
if i ∈B
Note that the polynomial G(i)
2k|{x=0|x∈Y2∪Z2} is indeed a copy of Gi
k for every
i, is deﬁned over Y1 ∪Z1 for i ∈A, and over Y2 ∪Z2 for i ∈B. Now, the
partition ϕ naturally induces a partition ϕ′ of Y ′ ∪Z′. Let q′
i = 
G2k(pi), then
from the above observations, we have that the polynomials q′
1, . . . , q′
i are linearly
independent. Since these polynomials correspond to rows in the matrix Mgϕ′, we
have rank(Mgϕ′) ≥r. Now, to prove the required probability bound, note that
the choice of the partitions Y ′ = Y1 ∪Y2 and Z′ = Z1 ∪Z2 was arbitrary, and the
rank bound holds for every such partition. There are

2k
k
2 such partitions. Thus,
over a random choice of partition, Pr[rank(Mgϕ′) ≥r] ≥

2k
k
2/

4k
2k

= Ω(1/k2).
Finally, we show that polynomials with high dimension of partial derivatives
cannot have small rank polynomial coeﬃcient matrices. Let ⟨∂≤k(f)⟩be the
space spanned by kth order partial derivatives of a polynomial f.
Then, we show that:
Lemma 9. Let f ∈F[X], where X = {x1, . . . , xn} be a polynomial of degree d.
Then ∀R ∈R, 0 ≤k ≤d, dim(⟨∂≤k(f)⟩) ≥R =⇒∃ϕ : X →Y ∪Z ∪F, such
that |Y | = |Z| and rank(M ϕ
f ) = Ω(R/2k).
References
1. Agrawal, M., Vinay, V.: Arithmetic circuits: a chasm at depth four. In: FOCS, pp.
67–75 (2008)
2. Amini, O., Fomin, F.V., Saurabh, S.: Counting subgraphs via homomorphisms.
SIAM J. Discrete Math. 26(2), 695–717 (2012)
3. Arvind, V., K¨obler, J., Kuhnert, S., Tor´an, J.: Solving linear equations parameter-
ized by hamming weight. Algorithmica 75(2), 322–338 (2016)
4. Bj¨orklund, A.: Exact covers via determinants. In: STACS, pp. 95–106 (2010)
5. Bj¨orklund, A., Husfeldt, T., Taslaman, N.: Shortest cycle through speciﬁed ele-
ments. In: SODA, pp. 1747–1753 (2012)
6. B¨urgisser, P.: Completeness and Reduction in Algebraic Complexity Theory, vol.
7. Springer Science & Business Media, Heidelberg (2013)
7. Chauhan, A., Rao, B.V.R.: Parameterized analogues of probabilistic computation.
In: CALDAM, pp. 181–192 (2015)

On Constant Depth Circuits Parameterized by Degree
261
8. Chen, Z., Fu, B., Liu, Y., Schweller, R.T.: On testing monomials in multivariate
polynomials. Theor. Comput. Sci. 497, 39–54 (2013)
9. Downey,
R.G.,
Fellows,
M.R.:
Fundamentals
of
Parameterized
Com-
plexity.
Texts
in
Computer
Science.
Springer,
London
(2013).
http://dx.doi.org/10.1007/978-1-4471-5559-1
10. Engels, C.: Why are certain polynomials hard?: a look at non-commutative, para-
meterized and homomorphism polynomials. Ph.D. thesis, Saarland University
(2016)
11. Fischer, I.: Sums of like powers of multivariate linear forms. Mathemat. Mag. 67(1),
59–61 (1994)
12. Fomin, F.V., Lokshtanov, D., Panolan, F., Saurabh, S.: Eﬃcient computation of
representative families with applications in parameterized and exact algorithms. J.
ACM 63(4), 29:1–29:60 (2016)
13. Fomin, F.V., Lokshtanov, D., Raman, V., Saurabh, S., Rao, B.V.R.: Faster algo-
rithms for ﬁnding and counting subgraphs. J. Comput. Syst. Sci. 78(3), 698–706
(2012). http://dx.doi.org/10.1016/j.jcss.2011.10.001
14. Gupta, A., Kamath, P., Kayal, N., Saptharishi, R.: Arithmetic circuits: a chasm
at depth three. In: FOCS 2013, pp. 578–587. IEEE (2013)
15. Kumar, M., Maheshwari, G., Sarma M.N., J.: Arithmetic circuit lower bounds
via maxrank. In: Fomin, F.V., Freivalds, R., Kwiatkowska, M., Peleg, D. (eds.)
ICALP 2013. LNCS, vol. 7965, pp. 661–672. Springer, Heidelberg (2013). doi:10.
1007/978-3-642-39206-1 56
16. M¨uller,
M.:
Parameterized
randomization.
Ph.D.
thesis,
Albert-Ludwigs-
Universit¨at Freiburg im Breisgau (2008)
17. Nisan, N.: Lower bounds for non-commutative computation. In: STOC, pp. 410–
418. ACM (1991)
18. Raz, R.: Multi-linear formulas for permanent and determinant are of super-
polynomial size. J. ACM 56(2) (2009)
19. Raz, R., Shpilka, A.: Deterministic polynomial identity testing in non-commutative
models. Comput. Complex. 14(1), 1–19 (2005)
20. Saptharishi, R., Chillara, S., Kumar, M.: A survey of lower bounds in arith-
metic circuit complexity. Technical report (2016). https://github.com/dasarpmar/
lowerbounds-survey/releases
21. Saxena, N.: Diagonal circuit identity testing and lower bounds. In: Aceto, L.,
Damg˚ard, I., Goldberg, L.A., Halld´orsson, M.M., Ing´olfsd´ottir, A., Walukiewicz,
I. (eds.) ICALP 2008. LNCS, vol. 5125, pp. 60–71. Springer, Heidelberg (2008).
doi:10.1007/978-3-540-70575-8 6
22. Schwartz, J.T.: Fast probabilistic algorithms for veriﬁcation of polynomial identi-
ties. J. ACM (JACM) 27(4), 701–717 (1980)
23. Shpilka, A., Volkovich, I.: Improved polynomial identity testing for read-once for-
mulas. In: Dinur, I., Jansen, K., Naor, J., Rolim, J. (eds.) APPROX/RANDOM
-2009. LNCS, vol. 5687, pp. 700–713. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-03685-9 52
24. Tavenas, S.: Improved bounds for reduction to depth 4 and depth 3. Inf. Comput.
240, 2–11 (2015)
25. Valiant, L.G., Skyum, S., Berkowitz, S., Rackoﬀ, C.: Fast parallel computation of
polynomials using few processors. SIAM J. Comput. 12(4), 641–644 (1983)
26. Zippel, R.: Probabilistic algorithms for sparse polynomials. In: Ng, E.W. (ed.)
Symbolic and Algebraic Computation. LNCS, vol. 72, pp. 216–226. Springer,
Heidelberg (1979). doi:10.1007/3-540-09519-5 73

A Tighter Relation Between Sensitivity
Complexity and Certiﬁcate Complexity
Kun He1,2(B), Qian Li1,2, and Xiaoming Sun1,2
1 CAS Key Lab of Network Data Science and Technology, Institute of Computing
Technology, Chinese Academy of Sciences, Beijing 100190, China
{hekun,liqian,sunxiaoming}@ict.ac.cn
2 University of Chinese Academy of Sciences, Beijing 100049, China
Abstract. The sensitivity conjecture, proposed by Nisan and Szegedy
in 1994 [20] which asserts that for any Boolean function, its sensitivity
complexity is polynomially related to the block sensitivity complexity,
is one of the most important and challenging problems in the study
of decision tree complexity. Despite of a lot of eﬀorts, the best known
upper bounds of block sensitivity, as well as the certiﬁcate complex-
ity, is still exponential in terms of sensitivity [1,5]. In this paper, we
give a better upper bound for certiﬁcate complexity and block sensitiv-
ity, bs(f) ≤C(f) ≤( 8
9 + o(1))s(f)2s(f)−1, where bs(f), C(f) and s(f)
are the block sensitivity, certiﬁcate complexity and sensitivity, respec-
tively. The proof is based on a deep investigation on the structure of
the sensitivity graph. We also provide a tighter relationship between the
0-certiﬁcate complexity C0(f) and 0-sensitivity s0(f) for functions with
small 1-sensitivity s1(f).
Keywords: Sensitivity conjecture · Sensitivity · Block sensitivity · Cer-
tiﬁcate complexity · Boolean functions
1
Introduction
The relation between sensitivity complexity and other decision tree complex-
ity measures is one of the most important topics in Boolean function com-
plexity theory. Sensitivity complexity is ﬁrst introduced by Cook, Dwork and
Reischuk [11,12] to study the time complexity of CREW-PRAMs. The sensitiv-
ity of a Boolean function, s(f), is the maximum number of variables xi in an
input assignment x = (x1, · · · , xn) with the property that changing xi changes
the value of f(x). Nisan [19] then introduced the concept of block sensitivity, and
demonstrated the remarkable fact that block sensitivity can fully characterize the
time complexity of CREW-PRAMs. Block sensitivity, bs(f), is a generalization
This work was supported in part by the National Natural Science Foundation of
China Grant 61433014, 61502449, 61602440, the 973 Program of China Grants No.
2016YFB1000201 and the China National Program for support of Top-notch Young
Professionals.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 262–274, 2017.
DOI: 10.1007/978-3-319-62389-4 22

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
263
of sensitivity to the case when we are allowed to change disjoint blocks of vari-
ables. Block sensitivity turns out to be polynomially related to a number of other
complexity measures for Boolean functions [9], such as decision tree complexity,
certiﬁcate complexity, polynomial degree and quantum query complexity, etc.
One exception is sensitivity. So far it is still not clear whether sensitivity com-
plexity could be exponentially smaller than block sensitivity and other measures.
The famous sensitivity conjecture, proposed by Nisan and Szegedy in 1994 [20],
asserts that block sensitivity and sensitivity complexity are also polynomially
related. According to the deﬁnition of sensitivity and block sensitivity, it is easy
to see that s(f) ≤bs(f) for any total Boolean function f. But in the other direc-
tion, it is much harder to prove an upper bound of block sensitivity in terms of
sensitivity complexity.
We brieﬂy recall some of the remarkable improvements about the upper
bound of block sensitivity here. Kenyon and Kutin [17] proved that bs(f) ≤
(
e
√
2π)es(f)
s(f). Then Ambainis et al. [1] proved that bs(f) ≤C(f) ≤
2s(f)−1s(f) −(s(f) −1), where C(f) is the certiﬁcate complexity, a com-
plexity measure with clear combinational interpretation. Certiﬁcate complex-
ity being at least c simply means that there is an input x = (x1, · · · , xn)
that is not contained in an (n −(c −1))-dimensional subcube of the Boolean
hypercube on which f is constant. The last improvement is bs(f) ≤C(f) ≤
max{2s(f)−1(s(f) −1
3), s(f)} [3], only minus a constant 1/3 from the term s(f).
Despite of a lot of eﬀorts, the best known upper bound of block sensitivity is
still exponential in terms of sensitivity. The best known separation between sen-
sitivity and block sensitivity complexity is quadratic [4]: there exists a sequence
of Boolean functions f with bs(f) = 2
3s(f)2 −1
3s(f).
Recently, Tal [24] showed that any upper bound of the form bsl(f) ≤s(f)l−ε
for ε > 0 implies a subexponential upper bound on bs(f) in terms of s(f). Here
bsl(f), the l-block sensitivity, deﬁned by Kenyon and Kutin [17], is the block sen-
sitivity with the size of each block at most l. There are also many works on the
equivalent propositions of sensitivity conjecture, i.e., whether sensitivity com-
plexity is polynomially related to Fourier degree, certiﬁcate complexity or quan-
tum query complexity. Gopalan et al. [15] proved a 2-approximate version of the
degree vs. sensitivity conjecture (the original one needs an ∞-approximation):
every Boolean function with sensitivity s can be ϵ-approximated (in ℓ2) by a
polynomial whose degree is O(s log(1/ϵ)). Recently, Lovett et al. [22] proved a
robust analog of the sensitivity conjecture, due to Gopalan et al., which relates
the decay of the Fourier mass of a boolean function to moments of its sensitivity.
Ben-David [7] provided a cubic separation between quantum query complexity
and sensitivity, as well as a power of 2.1 separation between certiﬁcate complexity
and sensitivity. While to solve the sensitivity conjecture seems very challenging
for general Boolean functions, special classes of functions have been investigated,
such as functions with graph properties [25], cyclically invariant functions [10],
functions with small alternating number [18], disjunctive normal forms [21], con-
stant depth regular read-k formulas [6], etc. We recommend readers [16] for an

264
K. He et al.
excellent survey about the sensitivity conjecture. For other recent progresses, see
[1,2,5,8,13,14,23].
Our Results. In this paper, we give a better upper bound of block sensitivity
in terms of sensitivity.
Theorem 1. For any total Boolean function f : {0, 1}n →{0, 1},
bs(f) ≤C(f) ≤(8
9 + o(1))s(f)2s(f)−1.
Here o(1) denotes a term that vanishes as s(f) →+∞.
Similar to the Ambainis et al. [3], our proof is also based on analysis of the
structure of the sensitivity graph. However, their result, max{2s(f)−1(s(f) −
1
3), s(f)}, is about (1−o(1))s(f)2s(f)−1. The improvement is multiplying by the
factor (1 −o(1)) comparing to the previous result s(f)2s(f)−1 −(s(f) −1) [1],
while our improvement is multiplying by factor ( 8
9 +o(1)). Ambainis et al. [3] also
investigated the Boolean functions with s1(f) = 2, and showed that C0(f) ≤
9
5s0(f) for this class of Boolean functions. In this paper, we also improve this
bound.
Theorem 2. Let f be a Boolean function with s1(f) = 2, then
C0(f) ≤37 +
√
5
22
s0(f) ≈1.7835s0(f).
Organization. We present preliminaries in Sect. 2. We give the overall structure
of our proof for Theorem 1 in Sect. 3 and the detailed proofs for lemmas in Sect. 4.
Finally, we conclude this paper in Sect. 5. The proof of Theorem 2 is given in
Appendix.
2
Preliminaries
Let f : {0, 1}n →{0, 1} be a Boolean function. For an input x ∈{0, 1}n and a
subset B ⊆[n], xB denotes the input obtained by ﬂipping all the bits xj such
that j ∈B.
Deﬁnition 1. The sensitivity of f on input x is deﬁned as s(f, x) := |{i :
f(x) ̸= f(xi)}|. The sensitivity s(f) of f is deﬁned as s(f) := maxx s(f, x).
The b-sensitivity sb(f) of f, where b
∈
{0, 1}, is deﬁned as sb(f)
:=
maxx∈f −1(b) s(f, x).
Deﬁnition 2. The block sensitivity bs(f, x) of f on input x is the maximum
number of disjoint subsets B1, B2, · · · , Br of [n] such that for all j ∈[r],
f(x) ̸= f(xBj). The block sensitivity of f is deﬁned as bs(f) := maxx bs(f, x).
The b-block sensitivity bsb(f) of f, where b ∈{0, 1}, is deﬁned as bsb(f) :=
maxx∈f −1(b) bs(f, x).

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
265
Deﬁnition 3. A partial assignment is a function p : [n] →{0, 1, ∗}. We call S =
{i|p(i) ̸= ∗} the support of this partial assignment. We deﬁne the co-dimension
of p, denoted by co-dim(p), to be |S|. We say x is consistent with p if xi = pi
for every i ∈S. p is called a b-certiﬁcate if f(x) = b for any x consistent with
p, where b ∈{0, 1}. For B ⊆S, pB denotes the partial assignment obtained by
ﬂipping all the bits pj such that j ∈B. For i ∈[n]/S and b ∈{0, 1}, pi=b denotes
the partial assignment obtained by setting pi = b.1
Deﬁnition 4. The certiﬁcate complexity C(f, x) of f on x is the minimum co-
dimension of f(x)-certiﬁcate that x is consistent with. The certiﬁcate complexity
C(f) of f is deﬁned as C(f) := maxx C(f, x). The b-certiﬁcate complexity Cb(f)
of f is deﬁned as Cb(f) := maxx∈f −1(b) C(f, x).
Deﬁnition 5. For any set S ⊆{0, 1}n, let s(f, S) to be 
x∈S s(f, x), the aver-
age sensitivity of S is deﬁned by s(f, S)/|S|.
In this work we regard {0, 1}n as a set of vertices for an n-dimensional hyper-
cube Qn, where two nodes x and y have an edge if and only if the Hamming
distance between them is 1. A Boolean function f : {0, 1}n →{0, 1} can be
regarded as a 2-coloring of the vertices of Qn, where x is black if f(x) = 1 and x
is white if f(x) = 0. Let f −1(1) = {x|f(x) = 1} be the set of all black vertices.
If f(x) ̸= f(y), we call the edge (x, y) a sensitive edge and x is sensitive to y
(y is also sensitive to x). We regard a subset S ⊆{0, 1}n as the subgraph G
induced by the vertices in S. Deﬁne the size of G, |G|, as the size of S. It is easy
to see that s(f, x) is the number of neighbors of x with a diﬀerent color than
x. A certiﬁcate is a monochromatic subcube, and C(f, x) is the co-dimension of
the largest monochromatic subcube which contains x.
There is a natural bijection between the partial assignments and the sub-
cubes, where a partial assignment p corresponds to a subcube induced by the
vertices consistent with p. Without ambiguity, we sometimes abuse these two
concepts.
Deﬁnition 6. Let G and H be two induced subgraphs of Qn. Let G ∩H denote
the graph induced on V (G)∩V (H). For any two subcubes G and H, we call H a
neighbor cube of G if their corresponding partial assignments pG and pH satisfy
pG = pi
H for some i.
Our proofs rely on the following result proved by Ambainis and Vihrovs [5].
Lemma 1.
[5] Let G be a non-empty induced subgraph of Qk induced by Qk ∩
f −1(b) where b ∈{0, 1} and satisfying that the sensitivity of every vertice in G is
at most s, then either |G| ≥3
2 · 2k−s, or G is a subcube of Qk with co-dim(G)=s
and |G| = 2k−s.
The following formulation of Turan’s theorem [26] is also used in our proofs.
Lemma 2 (Turan’s theorem [26]). Every graph G with n vertices and average
degree Δ has an independent set of size at least
n
Δ+1.
1 The function p can be viewed as a vector, and we sometimes use pi to represent p(i).

266
K. He et al.
3
The Sketch of Proof for Theorem 1
In this section, we give the sketch of the proof for Theorem 1. We ﬁrst present
some notations used in the proof. Let f be an n-input Boolean function. Let
z be an input with f(z) = 0 and C(f, z) = C0(f) = m. Then there exists
a 0-certiﬁcate of co-dimension C0(f) consistent with z, and let H be the one
with maximum average sensitivity if there are many such 0-certiﬁcates. W.l.o.g.,
we assume z = 0n and H = 0m∗n−m. Among the m neighbor cubes of H, from
Lemma 1 we have for any i ∈[m], either |Hi∩f −1(1)| ≥3
2 ·
|H|
2s1(f)−1 or Hi∩f −1(1)
is a subcube of Hi of size
|H|
2s1(f)−1 , which are called heavy cube and light cube,
respectively. W.l.o.g., assume H1, H2 · · · , Hl are light cubes and Hl+1, · · · , Hm
are heavy cubes, where l ≤m is the number of light cubes. For k > m, let
N 0
k = {i ∈[l]|(Hi ∩f −1(1))k = 0}. Intuitively, N 0
k consists of the indexes of
light cubes in which the kth bit of black subcube, Hi ∩f −1(1), is 0. Similarly,
let N 1
k = {i ∈[l]|(Hi ∩f −1(1))k = 1} and Nk = N 0
k ∪N 1
k. In the following, we
will assume i ∈[l] and k > m while i ∈Nk is used.
For any subcube H′ ⊆H, we use sl(f, H′) (sh(f, H′) respectively) to denote
the number of sensitive edges of H′ adjacent to the light cubes (heavy cubes
respectively). Similarly, for subcube H′ ⊆Hi where i ≤l and H′ ⊆f −1(0), we
use sl(f, H′) (sh(f, H′) respectively) to denote the number of sensitive edges of
H′ adjacent to H1,i, · · · , Hi−1,i, Hi, Hi+1,i, · · · , Hl,i (Hl+1,i, · · · , Hm,i respec-
tively). It is easy to see sl(f, H′) + sh(f, H′) = s(f, H′).
The overall structure of our proof is as follows.
Lemma 1
Lemma 6
⇒Lemma 3
Lemma 7
Lemma 8

⇒Lemma 4
Lemma 5
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
⇒Theorem 1.
Lemmas 3, 4, 6, 7 and 8 are about the structures of light cubes while Lemma 5
gives a lower bound of the number of 1-inputs in heavy cubes. The main idea
of our proof is to show that there are many 1-inputs in the heavy cubes. To
see why it works, consider the extremal case where there are no light cubes
(i.e. l = 0), then the average sensitivity of H is at least m ·
3
2s1(f) . Because the
average sensitivity of H can not exceed s0(f), we have m ·
3
2s1(f) ≤s0(f) and
m ≤2
3s0(f)2s1(f)−1.
More generally, the average sensitivity of H is at least
l
2s1(f)−1 + 3(m−l)
2s1(f) .
Let L = s0(f)2s1(f)−1/l. If L ≥2, we have l ≤s0(f)2s1(f)−2 and m ≤
5
6s0(f)2s1(f)−1. If s1(f) = 1, it has already been shown that C0(f) ≤s0(f)
[3]. In the following proof, we assume l > 0, L < 2 and s1(f) ≥2. Note that if
i ∈N 1
k, then Hk=0 together with Hi
k=0 is another certiﬁcate of z of the same
co-dimension with H. Thus according to the assumption that H is the one with
maximum average sensitivity, we have
s(f, H) −
	
s(f, Hk=0) + s(f, Hi
k=0)

= s(f, Hk=1) −s(f, Hi
k=0) ≥0.

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
267
By summing over diﬀerent cubes and diﬀerent bits, we get

k:|N1
k|≥s1(f)−1

i∈S1
k

sh(f, Hk=1) −sh(f, Hi
k=0)

=

k:|N1
k|≥s1(f)−1

i∈S1
k

sl(f, Hi
k=0) −sl(f, Hk=1)

−

s(f, Hi
k=0) −s(f, Hk=1)

≥

k:|N1
k|≥s1(f)−1

i∈S1
k

sl(f, Hi
k=0) −sl(f, Hk=1)

≥(s1(f) −1)|H|
2s1(f)−1

k:|N1
k|≥s1(f)−1
|N 0
k|
≥(1
2 −o(1))(s1(f) −1)2|H|l
2s1(f)−1
.
(1)
Here o(1) denotes a term that vanishes as s1(f) →+∞, and S1
k is a subset of
N 1
k of size s1(f) −1. The last but one inequality is due to the following lemma.
Lemma 3. For any i ∈N 1
k, sl(f, Hi
k=0) −sl(f, Hk=1) ≥|N0
k|·|H|
2s1(f)−1 .
The last inequality is due to
Lemma 4. If L < 2, then 
k:|N1
k|≥s1(f)−1 |N 0
k| ≥( 1
2 −o(1))l(s1(f) −1).
On the other side, we can show that
Lemma 5.

k:|N1
k|≥s1(f)−1

i∈S1
k
	
sh(f, Hk=1)−sh(f, Hi
k=0)

≤(s1(f)−1)2 
l<t≤m
|Ht∩f −1(1)|.
The proofs of these three lemmas are postponed to the next section. We
ﬁrst ﬁnish the proof of Theorem 1 here. Equality 1 together with Lemma 5
demonstrates that there are many 1-inputs in the heavy cubes, i.e.

l<t≤m
|Ht ∩f −1(1)| ≥( 1
2 −o(1))l|H|
2s1(f)−1
.
This inequality gives a new lower bound of the number of 1-inputs in the
heavy cubes neighboring H by relating it to the number of light cubes. This
lower bound provides new structural insights of the sensitivity graph. Combining
it with
l
2s1(f)−1 +

l<t≤m
|Ht ∩f −1(1)|
|H|
≤s0(f),
we get
l ≤(2
3 + o(1))2s1(f)−1s0(f).

268
K. He et al.
Moreover, recall that |Ht ∩f −1(1)|/|H| ≥
3
2s1(f) , thus
l
2s1(f)−1 + 3
2 ·
m −l
2s1(f)−1 ≤s0(f).
Therefore,
C0(f) = m ≤(8
9 + o(1))s0(f)2s1(f)−1.
Here, o(1) denotes a term that vanishes as s1(f) →+∞. Similarly, we can also
obtain
C1(f) ≤(8
9 + o(1))s1(f)2s0(f)−1.
Therefore,
C(f) ≤(8
9 + o(1))s(f)2s(f)−1.
where o(1) denotes a term that vanishes as s(f) →+∞.
4
Proofs of the Lemmas
4.1
Proof of Lemma 3
Before giving the proof of Lemma 3, we ﬁrst prove the following lemma which
will be used.
Lemma 6. If i, j ∈Nk, then |Hi,j
k=1 ∩f −1(1)| ≥
|H|
2s1(f)−1 and |Hi,j
k=0 ∩f −1(1)| ≥
|H|
2s1(f)−1 .
Proof. W.l.o.g., assume i ∈N 1
k. For any x ∈Hi ∩f −1(1), there are (s1(f) −1)
vertices in Hi as well as xi ∈H sensitive to x, thus xj ∈Hi,j is in f −1(1), since
otherwise x would be sensitive to s1(f)+1 vertices. Therefore, |Hi,j
k=1∩f −1(1)| ≥
|Hi∩f −1(1)| =
|H|
2s1(f)−1 . Similarly, if j ∈N 0
k, we have |Hi,j
k=0∩f −1(1)| ≥
|H|
2s1(f)−1 .
If j ∈N 1
k, note that Hi,j
k=0 ∩f −1(1) ̸= ∅, since otherwise Hi,j
k=0, Hi
k=0, Hj
k=0
and Hk=0 would become a larger monochromatic subcube containg z, which
is contradictory with the assumption of H, i.e., the maximum monochromatic
subcube containg z. For any y ∈Hi,j
k=0 ∩f −1(1), y is sensitive to yi ∈Hi
and yj ∈Hj, thus y has at most s1(f) −2 sensitive edges in Hi,j
k=0. Therefore,
|Hi,j
k=0 ∩f −1(1)| ≥
|Hi,j
k=0|
2s1(f)−2 =
|H|
2s1(f)−1 according to Lemma 1.
Proof. (Proof of Lemma 3) Since Hk=1 ∩f −1(1) = ∅and Hi
k=0 ∩f −1(1) = ∅,
it is easy to see
sl(f, Hk=1) =
l

j=1
|Hj
k=1 ∩f−1(1)| = |N1
k| · |H|
2s1(f)−1 + (l −|Nk|)|H|
2s1(f)
= (l + |N1
k| −|N0
k|)|H|
2s1(f)
.

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
269
Similarly,
sl(f, Hi
k=0) =
l

j=1,j̸=i
|Hi,j
k=0 ∩f −1(1)| + |Hi ∩f −1(1)|.
If j /∈Nk, then for any x ∈Hj ∩f −1(1), we have xi ∈f −1(1) since otherwise x
would have s1(f) + 1 sensitive edges, thus |Hi,j
k=0 ∩f −1(1)| ≥|Hj
k=0 ∩f −1(1)| =
|Hj∩f −1(1)|
2
=
|H|
2s1(f) . If j ∈Nk, |Hi,j
k=0∩f −1(1)| ≥
|H|
2s1(f)−1 according to Lemma 6.
Therefore, sl(f, Hi
k=0) ≥(l+|N1
k|+|N0
k|)|H|
2s1(f)
= sl(f, Hk=1) + |N0
k|·|H|
2s1(f)−1 .
4.2
Proof of Lemma 4
We ﬁrst prove two lemmas. With these lemmas Lemma 4 follows. In our proofs,
the logarithm uses base 2.
Lemma 7. For any integer c > 2,

|Nk|≥c
|Nk| ≥l

logl −log
	
s0(f)(s1(f) −1)(c −2) + s0(f)


.
Proof. First note that for i ≤l, Hi ∩f −1(1) is a subcube and co-dim(Hi ∩
f −1(1)) = n −m −s1(f) + 1, which means
{k > m|(Hi ∩f −1(1))k ̸= ∗}
 =
s1(f) −1. Let w =
{k > m| |Nk| ≥c}
. W.l.o.g., we assume that |Nk| ≥c if
and only if k ∈[m + 1, m + w]. For any y ∈{0, 1}w, let Iy = {i ∈[l]|∀j ∈[w] :
(Hi ∩f −1(1))j+m ̸= yj}.
We claim that for any y, |Iy| can not be “too large”. Think about the graph
G = (V, E) where V = Iy and (i, j) ∈E if i, j ∈Nk and (Hi ∩f −1(1))k ̸=
(Hj ∩f −1(1))k for some k ≥m + w + 1. Note that |Nk| ≤c −1 holds for
any k ≥m + w + 1. For any i ∈Nk where k ≥m + w + 1, there are at most
|Nk| −1 ≤c −2 diﬀerent j, where j ∈Nk, having an edge with i. Therefore for
any i ∈Iy,
deg(i) ≤

k:i∈Nk,k≥m+w+1
(|Nk|−1) ≤

k:i∈Nk,k≥m+w+1
(c−2) ≤(s1(f)−1)(c−2).
Thus according to Turan’s theorem, there exists an independent set S of size
|S| =
|V |

i deg(i)
|V |
+ 1
≥
|V |
maxi deg(i) + 1 ≥
|Iy|
(s1(f) −1)(c −2) + 1,
which means that there exists an input x ∈H such that xi ∈f −1(1) for any
i ∈S, therefore |S| ≤s0(f), implying
|Iy| ≤((s1(f) −1)(c −2) + 1)s0(f).

270
K. He et al.
Therefore, we have

y∈{0,1}w
|Iy| ≤2w((s1(f) −1)(c −2) + 1)s0(f).
(2)
On the other side, let wi = |{k ∈[m + 1, m + w]|i ∈Nk}|, then there are exact
2w−wi diﬀerent ys such that Iy contains i, thus

y∈{0,1}w
|Iy| =

i≤l
2w−wi ≥l · 2w−
i≤l wi/l = l · 2w−m+w
k=m+1 |Nk|/l = l · 2w−
|Nk|≥c |Nk|/l.
(3)
The inequality is due to the AM-GM inequality. Combining Inequality (2) and
(3), we can ﬁnish the proof of the lemma.
Lemma 8. 
k>m
|N 0
k| −|N 1
k|
 ≤l

2 ln L(s1(f) −1).
Proof. We sample an input x ∈H as Pr(xk = 0) = p independently for each
k > m. Here p := 
k>m |N 0
k|/ 
k>m |Nk|. Recall that for i ∈[l], |{k > m :
(Hi ∩f −1(1))k ̸= ∗}| = s1(f) −1, then Pr(xi ∈f −1(1)) = pdi(1 −p)s1(f)−1−di,
where di := |{k > m : (Hi ∩f −1(1))k = 0}|. Therefore
s0(f) ≥E(s(f, x))
≥

i∈[l]
Pr(xi ∈f −1(1))
=

i∈[l]
pdi(1 −p)s1(f)−1−di
≥lp

i∈[l] di
l
(1 −p)

i∈[l]s1(f)−1−di
l
= lp

k>m |N0
k|
l
(1 −p)

k>m |N1
k|
l
= lp
p 
k>m |Nk|
l
(1 −p)
(1−p) 
k>m |Nk|
l
= lpp(s1(f)−1)(1 −p)(1−p)(s1(f)−1).
(4)
Step four is due to the AM-GM inequality. Step ﬁve is due to the fact 
i∈[l] di =

k>m |N 0
k| and 
i∈[l] s1(f) −1 −di = 
k>m |N 1
k|, which are immediate from
the deﬁnition of di, N 0
k and N 1
k. The last step is due to the fact that 
k>m |Nk| =
l(s1(f) −1). By calculus, it is not hard to obtain e2(p−1/2)2 ≤2pp(1 −p)1−p for
0 ≤p ≤1. Together with Inequality (4) and recall that L = s0(f)2s1(f)−1/l, it
implies
p −1
2
 ≤

ln L
2(s1(f)−1). Therefore

k>m
|N 1
k| −|N 0
k|
 = |1 −2p|

k>m
|Nk| ≤l

2 ln L(s1(f) −1).

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
271
Now, we can prove Lemma 4. For any c2 > 2c1, ﬁrst note that

|N1
k|<c1,|Nk|≥c2

|N0
k|−|N1
k|

=

|N1
k|<c1,|Nk|≥c2
|Nk|

1−2|N1
k|
|Nk|

≥c2 −2c1
c2

|N1
k|<c1,|Nk|≥c2
|Nk|.
Then we have

|N1
k|≥c1
2|N0
k| ≥

|N1
k|≥c1,|Nk|≥c2
2|N0
k|
=

|Nk|≥c2
|Nk| −

|N1
k|<c1,|Nk|≥c2
|Nk| −

|N1
k|≥c1,|Nk|≥c2
(|N1
k| −|N0
k|)
≥

|Nk|≥c2
|Nk| −
c2
c2 −2c1

|N1
k|<c1,|Nk|≥c2
(|N0
k| −|N1
k|) −

|N1
k|≥c1,|Nk|≥c2
|N1
k| −|N0
k|

≥

|Nk|≥c2
|Nk| −
c2
c2 −2c1

|Nk|≥c2
|N0
k| −|N1
k|
 .
According to Lemmas 7 and 8, we have

|N1
k|≥c1
|N0
k| ≥l(logl −log(s0(f)(s1(f) −1)(c2 −1) + s0(f)))
2
−lc2

2 ln L(s1(f) −1)
2(c2 −2c1)
= l(s1(f) −1 −log L −log((s1(f) −1)(c2 −2) + 1))
2
−lc2

2 ln L(s1(f) −1)
2(c2 −2c1)
.
Recall L ≤2, and let c1 = s1(f) −1 and c2 = 3c1, thus

|N1
k|≥s1(f)−1
|N 0
k| ≥l(s1(f) −1)(1
2 −o(1)).
4.3
Proof of Lemma 5
Proof. Note that Hk=1 ∩f −1(1) = ∅and Hi
k=0 ∩f −1(1) = ∅for i ∈N 1
k. Thus it
is easy to see that
sh(f, Hk=1) −sh(f, Hi
k=0) =

l<t≤m

x∈Ht
k=1
(f(x) −f(xi,k)).
Therefore,

272
K. He et al.

k:|N1
k|≥s1(f)−1

i∈S1
k

sh(f, Hk=1) −sh(f, Hi
k=0)

=

k:|N1
k|≥s1(f)−1

i∈S1
k

l<t≤m

x∈Ht
k=1

f(x) −f(xi,k)

≤

k:|N1
k|≥s1(f)−1

i∈S1
k

l<t≤m

x∈Ht,
f(x)=1,f(xi,k)=0
1
=

k:|N1
k|≥s1(f)−1

i∈S1
k

l<t≤m


x∈Ht,f(x)=1,
f(xk)=0,f(xi,k)=0
1 +

x∈Ht,f(x)=1,
f(xk)=1,f(xi,k)=0
1
	
≤

k:|N1
k|≥s1(f)−1

i∈S1
k

l<t≤m


x∈Ht,f(x)=1,
f(xk)=0,f(xi,k)=0
1 +

x∈Ht,
f(x)=1,f(xi)=0
1
	
≤

l<t≤m


x∈Ht,f(x)=1

k:|N1
k|≥s1(f)−1,
f(xk)=0

i∈S1
k
1 +

x∈Ht,f(x)=1

i≤l:f(xi)=0

k:i∈S1
k
1
	
≤

l<t≤m


x∈Ht,f(x)=1

k:|N1
k|≥s1(f)−1,
f(xk)=0
(s1(f) −1) +

x∈Ht,f(x)=1

i≤l:f(xi)=0
(s1(f) −1)
	
≤

l<t≤m

x∈Ht,f(x)=1


k>m,f(xk)=0
1 +

i≤l,f(xi)=0
1
	
(s1(f) −1)
≤

l<t≤m

x∈Ht,f(x)=1
(s1(f) −1)2 = (s1(f) −1)2

l<t≤m
|Ht ∩f−1(1)|.
In Step four, we substitute xk with x for the second part. In Step six, for the
ﬁrst part, 
i∈S1
k 1 ≤s1(f) −1 because S1
k is a subset of N 1
k of size s1(f) −1.
For the second part, |{k > m|i ∈S1
k}| ≤|{k > m|i ∈N 1
k}| = s1(f) −1.
5
Conclusion
In this work, we give a better upper bound of block sensitivity in terms of
sensitivity. Our results are based on carefully exploiting the structure of the light
cubes. However, our approach has an obvious limitation. In the extremal case,
if there are no light cubes, then we can only get bs(f) ≤C(f) ≤2
3s(f)2s(f)−1.
Better understanding about the structure of heavy cubes is needed in order to
conquer this limitation.

A Tighter Relation Between Sensitivity and Certiﬁcate Complexity
273
References
1. Ambainis, A., Bavarian, M., Gao, Y., Mao, J., Sun, X., Zuo, S.: Tighter relations
between sensitivity and other complexity measures. In: Esparza, J., Fraigniaud, P.,
Husfeldt, T., Koutsoupias, E. (eds.) ICALP 2014. LNCS, vol. 8572, pp. 101–113.
Springer, Heidelberg (2014). doi:10.1007/978-3-662-43948-7 9
2. Ambainis, A., Pr¯usis, K.: A tight lower bound on certiﬁcate complexity in terms
of block sensitivity and sensitivity. In: Csuhaj-Varj´u, E., Dietzfelbinger, M., ´Esik,
Z. (eds.) MFCS 2014. LNCS, vol. 8635, pp. 33–44. Springer, Heidelberg (2014).
doi:10.1007/978-3-662-44465-8 4
3. Ambainis, A., Pr¯usis, K., Vihrovs, J.: Sensitivity versus certiﬁcate complexity of
boolean functions. In: Kulikov, A.S., Woeginger, G.J. (eds.) CSR 2016. LNCS, vol.
9691, pp. 16–28. Springer, Cham (2016). doi:10.1007/978-3-319-34171-2 2
4. Ambainis,
A.,
Sun,
X.:
New
separation
between
s(f)
and
bs(f).
CoRR,
abs/1108.3494 (2011)
5. Ambainis, A., Vihrovs, J.: Size of sets with small sensitivity: a generalization of
simon’s lemma. In: Jain, R., Jain, S., Stephan, F. (eds.) TAMC 2015. LNCS, vol.
9076, pp. 122–133. Springer, Cham (2015). doi:10.1007/978-3-319-17142-5 12
6. Bafna, M., Lokam, S.V., Tavenas, S., Velingker, A.: On the sensitivity conjecture for
read-k formulas. In: 41st International Symposium on Mathematical Foundations
of Computer Science, MFCS, pp. 16:1–16:14 (2016)
7. Ben-David, S., Hatami, P., Tal, A.: Low-sensitivity functions from unambiguous
certiﬁcates. In: 8th Innovations in Theoretical Computer Science, ITCS 9–11, 2017,
Berkeley, USA, January 2017
8. Boppana, M.: Lattice variant of the sensitivity conjecture. In: Electronic Collo-
quium on Computational Complexity (ECCC), vol. 19, p. 89 (2012)
9. Buhrman, H., De Wolf, R.: Complexity measures and decision tree complexity: a
survey. Theor. Comput. Sci. 288(1), 21–43 (2002)
10. Chakraborty, S.: On the sensitivity of cyclically-invariant boolean functions. In:
Proceedings of the 20th Annual IEEE Conference on Computational Complexity,
CCC 2005, pp. 163–167 (2005)
11. Cook, S., Dwork, C.: Bounds on the time for parallel ram’s to compute simple
functions. In: Proceedings of the Fourteenth Annual ACM Symposium on Theory
of Computing, STOC 1982, pp. 231–233 (1982)
12. Cook, S.A., Dwork, C., Reischuk, R.: Upper and lower time bounds for parallel
random access machines without simultaneous writes. SIAM J. Comput. 15(1),
87–97 (1986)
13. Gilmer, J., Kouck´y, M., Saks, M.E.: A new approach to the sensitivity conjecture.
In: Proceedings of the 2015 Conference on Innovations in Theoretical Computer
Science, ITCS 2015, Rehovot, Israel, January 11–13, 2015, pp. 247–254 (2015)
14. Gopalan, P., Nisan, N., Servedio, R.A., Talwar, K., Wigderson, A.: Smooth boolean
functions are easy: eﬃcient algorithms for low-sensitivity functions. In: Proceedings
of the 2016 ACM Conference on Innovations in Theoretical Computer Science,
Cambridge, MA, USA, January 14–16, 2016, pp. 59–70 (2016)
15. Gopalan, P., Servedio, R.A., Tal, A., Wigderson, A.: Degree and sensitivity: tails
of two distributions. In: Electronic Colloquium on Computational Complexity
(ECCC), vol. 23, p. 69 (2016)
16. Hatami, P., Kulkarni, R., Pankratov, D.: Variations on the sensitivity conjecture.
Theor. Comput. Grad. Surv. 4, 1–27 (2011)

274
K. He et al.
17. Kenyon, C., Kutin, S.: Sensitivity, block sensitivity, and l-block sensitivity of
boolean functions. Inf. Comput. 189(1), 43–53 (2004)
18. Lin, C., Zhang, S.: Sensitivity conjecture and log-rank conjecture for functions with
small alternating numbers. CoRR, abs/1602.06627 (2016)
19. Nisan, N.: Crew prams and decision trees. SIAM J. Comput. 20(6), 999–1007
(1991)
20. Nisan, N., Szegedy, M.: On the degree of Boolean functions as real polynomials.
Comput. Complex. 4, 301–313 (1994)
21. Karthik, C.S., Tavenas, S.: On the sensitivity conjecture for disjunctive normal
forms. CoRR, abs/1607.05189 (2016)
22. Zhan, J., Lovett, S., Tal, A.: Robust sensitivity. In: Electronic Colloquium on
Computational Complexity (ECCC), vol. 23, p. 161 (2016)
23. Szegedy, M.: An o(n0.4732) upper bound on the complexity of the GKS communi-
cation game. CoRR, abs/1506.06456 (2015)
24. Tal, A.: On the sensitivity conjecture. In: 43rd International Colloquium on
Automata, Languages, Programming, ICALP 2016, July 11–15, 2016, Rome, Italy,
pp. 38:1–38:13 (2016)
25. Tur´an, G.: The critical complexity of graph properties. Inf. Process. Lett. 18(3),
151–153 (1984)
26. Tur´an, P., Paul, E.: On an extremal problem in graph theory. Matematikai ´es
Fizikai Lapok (in Hungarian) 48, 436–452 (1941)

Unfolding Some Classes of Orthogonal
Polyhedra of Arbitrary Genus
Kuan-Yi Ho1, Yi-Jun Chang2, and Hsu-Chun Yen1(B)
1 Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan
hcyen@ntu.edu.tw
2 Department of EECS, University of Michigan, Ann Arbor, MI, USA
Abstract. Unfolding polyhedra beyond genus zero (i.e., with holes) is
challenging, yet it has not been investigated until very recently. We
show two types of orthogonal polyhedra of arbitrary genus, namely, well-
separated orthographs and regular orthogonal polyhedra with x- and z-
holes, to enjoy (2 × 1)-grid-unfoldings, generalizing some prior work in
the literature by allowing holes (or more complicated holes) to exist. In
addition to the development of new unfolding techniques, for the ﬁrst
time we identify classes of nontrivial orthogonal polyhedra of arbitrary
genus to admit grid-unfoldings subject to a small amount of reﬁnements.
1
Introduction
Unfolding a polyhedron refers to the process of cutting and ﬂattening the surface
of a polyhedron to yield a single connected planar piece. As documented in [9],
constructing a polyhedron by folding from its unfolding can be traced back to
several hundred years ago.
For the sake of simplicity, it is desirable to only cut along a subset of edges of
a polyhedron when producing an unfolding. It is known, however, that even for
orthogonal polyhedra (i.e., polyhedra with their faces orthogonal to x-, y- and z-
axes), cutting along edges is not suﬃcient to guarantee an unfolding in general.
Grid-unfolding, introduced in [11], provides additional ﬂexibility by allowing
new edges resulting from intersecting the polyhedron with all coordinate planes
passing through vertices of the polyhedron to be cut. Even with such a relaxation,
it remains a major open problem as to whether every orthogonal polyhedron
admits a grid-unfolding [8,11].
To oﬀer an even higher degree of ﬂexibility, the notion of a reﬁnement comes
into play, by allowing each rectangular face of a polyhedron to be further reﬁned
to an a × b grid with all grid lines cuttable [3,7]. Such an unfolding style
is called an (a × b)-grid-unfolding (or grid-unfolding with (a × b)-reﬁnement).
With (O(n) × O(n))-reﬁnement, general orthogonal polyhedra could always be
grid-unfolded [2]. If we only allow constant reﬁnements, orthostacks [11] and
Manhattan towers [6] are unfoldable. On the other hand, only several special
H.-C Yen—Research supported in part by Ministry of Science and Technology,
Taiwan, under grant MOST 103-2221-E-002-154-MY3.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 275–286, 2017.
DOI: 10.1007/978-3-319-62389-4 23

276
K.-Y. Ho et al.
classes of orthogonal polyhedra can be grid-unfolded without reﬁnement, includ-
ing well-separated orthotrees [5].
All the aforementioned results assume the underlying polyhedra to be of
genus zero, i.e., without holes. As far as we know, only a scarcity of results are
available for unfolding polyhedra of genus greater than zero. In [10], it was shown
that several special classes of one-layer lattice polyhedra with cubic holes admit
edge-unfoldings. For general orthogonal polyhedra, the only study [4] was for
the case of genus at most 2, showing such polyhedra to have (O(n)×O(n))-grid-
unfoldings. It remains a challenging open problem whether orthogonal polyhedra
of any genus could be grid-unfolded with reﬁnements.
In this paper, we investigate the issue of unfolding the following two types
of orthogonal polyhedra of arbitrary genus, namely, well-separated orthographs
and regular orthogonal polyhedra with x- and z-holes.
An orthograph is a polyhedron made from gluing congruent faces (of the same
shape and size) of rectangular boxes together. Figure 1(a) shows an orthograph
with the so-called junctions colored in grey. An orthograph is well-separated if
no two junctions are next to each other. A regular orthogonal polyhedron can
be thought of as the outcome of pulling up an orthogonal polygon on the x −z
plane along the y direction to form a polyhedron. As illustrated in Fig. 1(b),
holes are only allowed in the x and z directions. It turns out that both types of
orthogonal polyhedra can be grid-unfolded with (2 × 1)-reﬁnement.
x
y
z
(a)
(b)
Fig. 1. (a) A well-separated orthograph in which junctions are colored in grey. (b) A
regular orthogonal polyhedra with x- and z-holes.
Orthographs can be thought of as extensions of the so-called orthotrees inves-
tigated in [1,5], by allowing ends of an orthotree to be glued to the rest of the
structure to form cycles (i.e., of genus greater than zero). In [5], well-separated
orthotrees admit edge-unfoldings (assuming box edges are cuttable). Our result
shows that allowing (2 × 1)-reﬁnement is suﬃcient to grid-unfold well-separated
orthographs - a much larger class in comparison with orthotrees.
Regular orthogonal polyhedra with x- and z-holes represent a natural exten-
sion/generalization of one-layer lattice polyhedra with cubic holes studied in [10],
which were shown to enjoy edge-unfoldings. In particular, one-layer polyhedra
with cubic holes with rectangular boundary (see Sect. 3.1 of [10]) is a sub-
class of regular orthogonal polyhedra with only x-holes. Again, allowing (2 × 1)-
reﬁnement enables orthogonal polyhedra with a more complicated structure of
holes to be unfolded.

Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus
277
The signiﬁcance of our results is two-fold. First, for the ﬁrst time we identify
classes of nontrivial orthogonal polyhedra of arbitrary genus that can be grid-
unfolded with reﬁnements. Second, we feel that the new techniques devised in
this paper are applicable to unfolding generalized versions (by allowing holes to
exist) of certain existing types of orthogonal polyhedra.
2
Preliminaries
An orthograph O is a polyhedron made from gluing congruent faces (of the same
shape and size) of rectangular boxes together. With respect to an orthograph O,
its dual graph GO = (V, E) is a graph in which each vertex of GO corresponds to
a box of O and an edge between two vertices of GO exists if the corresponding two
boxes are glued to each other. As there is a one-to-one correspondence between
boxes of O and vertices of GO, throughout this paper we use the name of a box
to denote that of the corresponding vertex of GO as well. A face of a box is said
to be open if it is not glued to any other box; otherwise, it is called closed. A
box bi ∈O is called a
– junction: if there exist two boxes that are glued to bi on the neighboring faces
(i.e., faces sharing a common boundary edge),
– connector: if deg(bi) = 2 and the two boxes that are glued to bi are on the
opposite faces,
– leaf: if deg(bi) = 1.
Note that each box in O belongs to exactly one of the above three cases.
An orthograph is well-separated if there is no junction that is the neighbor of
another junction. As opposed to the orthotrees discussed in [5], the dual graph
of an orthograph can have cycles.
Figure 1(a) displays an example of a well-separated orthograph with junctions
colored in grey.
The boundary of an unfolding of a polyhedron is a polygon in the plane. We
call a polygon Q y-convex if the intersection of any straight line parallel to the
y-axis and the interior of Q is a single connected line segment.
For convenience of illustration, we adopt the notations used in [5] for labeling
faces and edges of a box. The two faces perpendicular to the x-axis (y-axis and
z-axis, respectively) are annotated by x+ and x−(y+ and y−, and z+ and z−,
respectively). The edges of x+ and x−are labeled as u (up), d (down), f (front),
and b (back). Similarly, edges of y+ and y−are labeled as f (front), b (back), l
(left), and r (right), and z+ and z−are labeled as u, d, l, and r. See Fig. 2 for
details. Note that an edge may carry diﬀerent labels depending on the face it
is referred to. For notational convenience, the w edge of a face F is denoted by
F(w). For instance, the up edge of x+ is denoted by x+
(u).

278
K.-Y. Ho et al.
z+
z
z
-
x+
y+
x
x
-
y
y
-
u
r
l
d
u
b
d
f
Fig. 2. Annotations of a single box.
3
Unfolding Paths on Modiﬁed Dual Graphs
A key ingredient in our analysis is the concept of an unfolding path on a modiﬁed
version of the dual graph of an orthograph.
The intuition is that if there is an Euler path (meeting some mild require-
ments) on the dual graph, we can unfold a well-separated orthograph along this
path. For any well-separated orthograph, however, there may not be an Euler
path on its dual graph G. To overcome this diﬃculty, we allow our unfolding
path to pass through each edge twice by simply doubling each edge in the dual
graph of G.
To come up with an unfolding path which is simple enough to facilitate
unfolding, we further modify the dual graph of a well-separated orthograph in
the way explained below. In our modiﬁcation, we will make use of the property
that junctions could not touch each other in a well-separated orthograph.
Fig. 3. Modifying the dual graph structure of a junction. Indirect augmented edges are
drawn as thick edges.

Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus
279
Deﬁnition 1. Given a well-separated orthograph O, a modiﬁed dual graph is a
multigraph graph G′
O = (V, E) obtained by applying the following operations to
the dual graph GO:
(1) For each edge e = (a, b) such that both a and b are not junctions, replace
the edge (a, b) by two parallel edges {(a, b), (a, b)}.
(2) For each junction c, connect the neighborhood of c by a cycle as speciﬁed in
Fig. 3, and then remove c from the multigraph.
Consider Fig. 3(b) for example. In the original dual graph, the junction (of
degree 3) is glued to the three boxes in the x−, y+ and z+ directions. Instead
of having the edges linking the junction directly to its three neighbors, we link
the three neighbors together in a cycle.
The remaining cases are explained in Fig. 3. Associated with a junction, each
of the solid edges is called an augmented edge of the junction. At this point, the
reader may ignore the meaning of edge colors in Fig. 3. It should be noted that
just forming a cycle is not suﬃcient to guarantee an unfolding. For instance, in
Fig. 3(g) if a cycle contains an edge connecting two opposite boxes, such a cycle
does not yield any valid unfolding.
As we shall see later, augmented edges map to fragments of the “backbone”
of the unfolding. Augmented edges are divided into two types, namely, direct
and indirect. For an augmented edge e between two boxes B1 and B2, if e is
direct, then e is realized by two connected squares Q1 Q2 (where Q1 and Q2
are faces of B1 and B2, respectively) in the x+ direction along the backbone. If e
is indirect, it is realized by a sequence of squares connected in a line Q1 · · · Q2
in the x+ direction such that Q1 and Q2 are faces of B1 and B2, respectively,
and the intermediate squares belong to faces of the junction.
Deﬁnition 2. Given a well-separated orthograph O with its modiﬁed dual graph
G′
O, a path P of G′
O is an unfolding path if it has the following properties.
(1) P passes through all the edges of G′
O except one edge which links the two
end vertices of P.
(2) P does not pass through two augmented edges consecutively.
(3) If there are vertices u and v such that uvu is a subpath of P, then v is a
leaf.
The following result is easy to show:
Lemma 1. For every well-separated orthograph, there is an unfolding path in
the modiﬁed dual graph of the orthograph.
4
Unfolding Well-Separated Orthographs
In this section, we show how to unfold a well-separated orthograph taking advan-
tage of the unfolding path guaranteed by Lemma 1.

280
K.-Y. Ho et al.
The notion of a backbone will serve for the purpose of a y-convex partial
unfolding to which the remaining faces of a polyhedron could be attached. The
reader should note the diﬀerence between an unfolding path and a backbone. An
unfolding path only gives the sequence of boxes to be visited in the unfolding
process. It does not tell the exact faces of the unfolding along the backbone
of the unfolding. For ease of explanation, in our subsequent discussion we use
(colored) line segments to represent fragments of the backbone in such a way
that a line segment cuts through a face if the face is on the backbone.
Before proceeding further, we ﬁrst give the intuitive idea behind our unfolding
strategy. For every box B, each of its closed faces is associated with two edges
called ports, and a set of links is to identify which pair of ports are connected
together. A link between ports a and b is written as a ∼b. The idea is to associate
each link (pairing diﬀerent ports) with a y-convex partial unfolding in the way
that any two such partial unfoldings are disjoint, and the union of all such partial
unfoldings covers all the open faces of box B. Finally, the unfolding of the entire
polyhedron is obtained by connecting all the y-convex partial unfoldings together
piece by piece based on the unfolding path constructed in the beginning of the
procedure. Consider Fig. 4 in which three boxes Bl (a leaf), Bc (a connector) and
BJ (a junction) are given. Box Bl has link {k ∼m}, Bc has links {i ∼h, j ∼g},
and BJ has links {a ∼b, c ∼d, e ∼f}. The y-convex partial unfoldings realizing
the links of a box are displayed at the top of the ﬁgure in Fig. 4. Note that for
BJ, since c = d and a = b (i.e., c and d are identical edges, so are a and b), there
need not be any partial unfolding associated with these two links. Such a ∼b
and c ∼d are called null links. A null link could only appear on a junction box,
serving as a turning point connecting two faces belonging to two neighbors of
the junction. It is easy to see that if the unfolding starts at edge k, by joining
m and i, and later h and a, a longer fragment of the backbone (highlighted by
the sequence of line segments in yellow) is thus obtained, and the corresponding
partial unfolding remains y-convex.
Fig. 4. Ports, links, and y-convex partial unfoldings of boxes. (Color ﬁgure online)

Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus
281
Suppose B is a box of degree d (1 ≤d ≤6), and let F1, · · · , Fd be the closed
faces of B. Let Ei be the set of four edges of Fi. A conﬁguration of B is a set
of d links L = d
i=1{xi ∼yi}, where | d
i=1{xi, yi} ∩Ej |= 2, ∀1 ≤j ≤d, i.e.,
each face Fi contributes exactly two edges (ports) to L, and unless d = 1 (i.e.,
the case of a leaf), xi and yi do not belong to the same face.
A conﬁguration L of a box B is realizable if the following hold:
(1) a y-convex partial unfolding Ui (with possible reﬁnements on faces) can be
assigned to each non-null link xi ∼yi, and in the unfolding xi and yi are
located at the left and right borders of Ui, and
(2) Ui and Uj are disjoint for i ̸= j, and the union of all such Ui covers all the
open faces of B.
Note that there are multiple conﬁgurations associated with a box of a polyhe-
dron. We shall show in our subsequent discussion that regardless of the type of a
junction, there always exists one conﬁguration which is realizable. Furthermore,
the conﬁguration respects the structure of the modiﬁed dual graph described in
Fig. 3, in the sense that
– if B is a junction, then for every link u ∼v of B and suppose Bu (Bv, resp.)
is the box glued to the face of B on which port u (v, resp.) resides, then in
the modiﬁed dual graph (Bu, Bv) is a direct augmented edge if u = v (i.e., a
null link); otherwise, (Bu, Bv) is indirect.
For leaves and connectors, we are also able to show that all of their conﬁgurations
are realizable. Since the polyhedron under consideration is well-separated, for
two consecutive junctions B1 and B2 along the unfolding path, there must be a
connector Bc in between, which has the ability to “rearrange” ports arbitrarily
so that ports of B1 and B2 can match with each other in forming the backbone.
Fig. 5. Unfolding a single box.
Lemma 2. Given a leaf B with closed face x+ and for arbitrary ports p and
q such that p, q ∈{x−
(u), x−
(d), x−
(f), x−
(b)} and p ̸= q, then it is always possible
to unfold B (excluding the x+ face) into a y-convex polygon such that p and
q are the left and right borders of the polygon, respectively. Hence, all possible
conﬁgurations of a leaf are realizable.

282
K.-Y. Ho et al.
Proof sketch. Figure 5(a) displays two cases of p and q (i.e., neighbored to each
other, or on opposite sides). The remaining cases of p and q are symmetric. The
corresponding two y-convex unfoldings are shown in Fig. 5(b) and (c).
⊓⊔
Recall that in a well-separated orthograph, the unfolding path from one junc-
tion to another must pass through some connector, and each connector is visited
twice along the unfolding path. Consider a connector Bc shown in Fig. 6(a).
Without loss of generality we assume that the unfolding path ﬁrst enters the
connector from the left and at a later time reenters from the right. W.r.t. such
an unfolding path, the backbone enters from c and exits at d, visits a portion
of the orthograph attached to x+ of Bc (i.e., the right face of Bc), and reenters
Bc from b and leaves at a. That is, c ∼d and b ∼a serve as the two links of the
connector Bc, while a, b, c, and d are ports.
Fig. 6. (a) c ∼d is unfolded in a clockwise fashion in the x+ direction. (b) c ∼d is
unfolded in a counter-clockwise fashion in the x+ direction. (c) Separated y-convex
unfoldings of c ∼d and a ∼b in (a).
Lemma 3. Let Bc be a connector and {c ∼d, a ∼b} be the set of links of Bc,
where c and a (b and d resp.) are ports of face x−(x+, resp.). For all possible
combinations of c, a ∈{x−
(u), x−
(d), x−
(f), x−
(b)}, and b, d ∈{x+
(u), x+
(d), x+
(f), x+
(b)}
such that a ̸= c and b ̸= d, two separated y-convex (2 × 1)-unfoldings of Bc
(containing the y+, y−, z+, and z−faces) are always feasible. Hence, all possible
conﬁgurations of a connector are realizable.
Proof sketch. The basic idea of the proof is illustrated in Fig. 6.
⊓⊔
Lemma 4. Given a junction BJ with degree d, 2 ≤d ≤6, there exists a realiz-
able conﬁguration L that respects the structure of the modiﬁed dual graph asso-
ciated with BJ.
Proof sketch. The proof is done by case analysis, as illustrated in Fig. 8. The
reader should also consult Fig. 3. Note that there is a 1–1 correspondence between
the seven structures in Figs. 3 and 8 (including the matching edge colors between
corresponding structures).
To understand what the lemma says, consider Fig. 7 again. By letting {a, c}
({b, f} and {d, e}, resp.) be the two ports associated with face x−(y+ and z+,
resp.) and the set of links be L = {a ∼b, c ∼d, e ∼f} (where c = d and a = b),

Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus
283
c ∼d admits a y-convex partial unfolding as Fig. 7(b) shows. Hence, such a
conﬁguration is realizable. In view of the colored line segments in Fig. 7(a), it is
also easy to see that the conﬁguration respects the structure of Fig. 3(b). From
the colored lines in each of the cases of Fig. 8, it is not diﬃcult to see that in
each case a realizable conﬁguration is available which also respects the respective
structure displayed in Fig. 3. Note that direct and indirect augmented edges in
each of the cases of Fig. 8 are easy to be identiﬁed.
⊓⊔
Figure 7 illustrates how to piece y-convex partial unfoldings in the x−, z+
and y+ directions to the y-convex partial unfolding of BJ.
We are in a position to prove the main result of this section.
Theorem 1. Every well-separated orthograph can be (2 × 1)-grid-unfolded.
Proof sketch. Given a well-separated orthograph, we ﬁrst obtain the unfolding
path P guaranteed by Lemma 1. Based on the unfolding path, we carry out the
following:
Fig. 7. Unfolding a junction of degree 3.
Fig. 8. Segments along the backbone associated with various cases in Fig. 3.
(Color ﬁgure online)

284
K.-Y. Ho et al.
(1) select ports/links for junctions in accordance to the modiﬁed dual graph
G′
O,
(2) select conﬁgurations for connectors and leaves that match the conﬁgurations
of junctions selected in Step (1), and
(3) for all boxes, compute unfoldings that realize their conﬁgurations (guaran-
teed by Lemmas 2, 3, and 4, see also Figs. 5, 6 and 7).
The ﬁnal unfolding is formed by gluing these partial unfoldings together along
the unfolding path.
⊓⊔
Figure 9 displays the backbone of an unfolding of a well-separated orthograph.
Fig. 9. An example of a (2 × 1)-grid-unfolding - 3D view.
5
Unfolding Regular Orthogonal Polyhedra with x- and
z-holes
In this section, we turn our attention to another class of orthogonal polyhedra
with genus greater than zero, which are called regular orthogonal polyhedra with
holes in the x and z directions.
Deﬁnition 3. A regular orthogonal polyhedron Q with x- and z-holes is an
orthogonal polyhedron which can be divided by y-planes (i.e., planes perpendicular
to the y-axis and intersecting at least one vertex in Q) into several layers in such
a way that
(1) the holes do not intersect with each other,
(2) the outside bands (bands obtained by ignoring holes) of any two layers
enclose the same shape,
(3) each hole belongs to a single layer, and
(4) holes of the same layer have the same height, which is also the height of the
layer.

Unfolding Some Classes of Orthogonal Polyhedra of Arbitrary Genus
285
b
d
d
b
(a)
(b)
Fig. 10. An example of one-layer of a regular orthogonal polyhedron with holes and
its unfolding cycle.
The reader may refer to Fig. 10(a) as an example of one layer of a regular orthog-
onal polyhedron with holes.
Note that in each layer, holes partition the layer into several modules; the
band of each module may again be split into segments. See Fig. 10(a) for an
example, in which b and d are two segments split by the two z-holes. In the
way displayed in Fig. 10(b), it is straightforward to connect all segments split
by holes into a cycle (called an unfolding cycle). Note that a segment always
takes the neighboring hole to cross over to the next segment on the other side
along an unfolding cycle. By doing so, the four faces of a hole can be unfolded
in a y-convex fashion, as shown in Fig. 11. Analogous to the unfolding path in
the previous section, the unfolding cycle plays a key role in unfolding regular
orthogonal polyhedra with holes, as the cycle induces an unfolding backbone.
We use similar notations as in Fig. 2 for a hole. We have the following result:
a
b
d
c
c
b
a
d
)
b
(
)a(
Fig. 11. The unfolding near a hole.
Theorem 2. Every regular orthogonal polyhedron with x- and z-holes can be
(2 × 1)-grid-unfolded.
Proof sketch. The unfolding is carried out layer-by-layer in a top-down fashion.
What follows are key steps in the unfolding. The reader is referred to Fig. 10.

286
K.-Y. Ho et al.
(1) There is a face F extendable to all the layers of the polyhedron;
(2) For layer i, construct its unfolding cycle Ci, and start the unfolding from a
node on face F to traverse along Ci either clockwise or counter-clockwise;
(3) The y-convexity is guaranteed, because when passing through a hole the
backbone ﬁrst turns right (left, resp.) then turns left (right, resp.). See
Fig. 11;
(4) To enclose the current layer, use a (2×1)-reﬁnement to separate the starting
and the end paths.
⊓⊔
References
1. Aloupis, G., et al.: Common unfoldings of polyominoes and polycubes. In:
Akiyama, J., Bo, J., Kano, M., Tan, X. (eds.) CGGA 2010. LNCS, vol. 7033,
pp. 44–54. Springer, Heidelberg (2011). doi:10.1007/978-3-642-24983-9 5
2. Chang, Y.-J., Yen, H.-C.: Unfolding orthogonal polyhedra with linear reﬁnement.
In: Elbassioni, K., Makino, K. (eds.) ISAAC 2015. LNCS, vol. 9472, pp. 415–425.
Springer, Heidelberg (2015). doi:10.1007/978-3-662-48971-0 36
3. Damian, M., Demaine, E.D., Flatland, R.: Unfolding orthogonal polyhedra with
quadratic reﬁnement: the delta-unfolding algorithm. Graphs Comb. 30(1), 125–140
(2014)
4. Damian, M., Demaine, E.D., Flatland, R., O’Rourke, J.: Unfolding genus-2 orthog-
onal polyhedra with linear reﬁnement arXiv:1611.00106v1 [cs.CG] (2016)
5. Damian, M., Flatland, R., Meijer, H., O’Rourke, J.: Unfolding well-separated
orthotrees. In: Proceedings of 15th Annual Fall Workshop on Computational
Geometry, pp. 23–25 (2005)
6. Damian, M., Flatland, R., O’Rourke, J.: Unfolding Manhattan towers. Comput.
Geom. 40(2), 102–114 (2008)
7. Damian, M., Flatland, R., O’Rourke, J.: Epsilon-unfolding orthogonal polyhedra.
Graphs Comb. 23(1), 179–194 (2007)
8. Demaine, E.D., O’Rourke, J.: Geometric Folding Algorithms. Cambridge Univer-
sity Press, Cambridge (2007)
9. D¨urer, A.: Underweysung der Messung, mit dem Zirckel und Richtscheyt, in Linien,
Ebenen unnd gantzen corporen. N¨urnberg (1525)
10. Liou, M.-H., Poon, S.-H., Wei, Y.-J.: On edge-unfolding one-layer lattice polyhe-
dra with cubic holes. In: Cai, Z., Zelikovsky, A., Bourgeois, A. (eds.) COCOON
2014. LNCS, vol. 8591, pp. 251–262. Springer, Cham (2014). doi:10.1007/
978-3-319-08783-2 22
11. O’Rourke, J.: Unfolding orthogonal polyhedra. In: Surveys on Discrete and Com-
putational Geometry: Twenty Years Later, pp. 231–255. American Mathematical
Society (2008)

Reconﬁguration of Maximum-Weight
b-Matchings in a Graph
Takehiro Ito1(B), Naonori Kakimura2, Naoyuki Kamiyama3,
Yusuke Kobayashi4, and Yoshio Okamoto5
1 Graduate School of Information Sciences, Tohoku University, Sendai, Japan
takehiro@ecei.tohoku.ac.jp
2 Department of Mathematics, Keio University, Yokohama, Japan
kakimura@math.keio.ac.jp
3 Institute of Mathematics for Industry, Kyushu University, Fukuoka, Japan
kamiyama@imi.kyushu-u.ac.jp
4 Faculty of Engineering, Information and Systems,
University of Tsukuba, Tsukuba, Japan
kobayashi@sk.tsukuba.ac.jp
5 Department of Communication Engineering and Informatics,
University of Electro-Communications, Chofu, Japan
okamotoy@uec.ac.jp
Abstract. Consider a graph such that each vertex has a nonnegative
integer capacity and each edge has a positive integer weight. Then, a b-
matching in the graph is a multi-set of edges (represented by an integer
vector on edges) such that the total number of edges incident to each
vertex is at most the capacity of the vertex. In this paper, we study a
reconﬁguration variant for maximum-weight b-matchings: For two given
maximum-weight b-matchings in a graph, we are asked to determine
whether there exists a sequence of maximum-weight b-matchings in the
graph between them, with subsequent b-matchings obtained by removing
one edge and adding another. We show that this reconﬁguration problem
is solvable in polynomial time for instances with no integrality gap. Such
instances include bipartite graphs with any capacity function on vertices,
and 2-matchings in general graphs. Thus, our result implies that the
reconﬁguration problem for maximum-weight matchings can be solved
in polynomial time for bipartite graphs.
T. Ito – Supported by JST CREST Grant Number JPMJCR1402, Japan, and JSPS
KAKENHI Grant Number JP16K00004.
N. Kakimura – Supported by JST ERATO Grant Number JPMJER1305, Japan,
and by JSPS KAKENHI Grant Number JP17K00028.
N. Kamiyama – Supported by JST PRESTO Grant Number JPMJPR14E1, Japan.
Y. Kobayashi – Supported by JST ERATO Grant Number JPMJER1305, Japan,
and by JSPS KAKENHI Grant Numbers JP16K16010 and JP16H03118.
Y. Okamoto – Supported by Kayamori Foundation of Informational Science
Advancement, JST CREST Grant Number JPMJCR1402, Japan, and JSPS KAK-
ENHI Grant Numbers JP24106005, JP24700008, JP24220003, JP15K00009.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 287–296, 2017.
DOI: 10.1007/978-3-319-62389-4 24

288
T. Ito et al.
1
Introduction
Recently, reconﬁguration problems [11] have attracted much attention in the
ﬁeld of theoretical computer science. These problems arise when we wish to
ﬁnd a step-by-step transformation between two feasible solutions of a combi-
natorial problem such that all intermediate results are also feasible and each
step conforms to a ﬁxed reconﬁguration rule (i.e., an adjacency relation deﬁned
on feasible solutions of the original combinatorial problem). For example, in
the (cardinality) matching reconfiguration problem, feasible solutions are
matchings in a graph having the same cardinality and one of the studied reconﬁg-
uration rules is to exchange an edge in the current matching with an edge which
is not contained in the matching. This kind of reconﬁguration problems has been
studied extensively for several well-known combinatorial problems, including
satisfiability [6,16,18], independent set [4,5,14], vertex cover [12,19],
clique [13], dominating set [7,8], vertex-coloring [1,3,9], and so on.
(See also a survey [10]).
(a) G
2
3
3
5
5
3
2
2
2
1
e’
(b) xs=x0
2
2
2
2
1
(c) x1
2
2
2
2
1
(d) x2
2
2
2
2
1
(e) x3
2
2
2
2
1
(f) x4
2
2
2
2
1
(g) xt=x5 
2
2
2
2
1
Fig. 1. (a) Graph G with vertex-capacities and edge-weights, and (b)–(g) a sequence
⟨xs = x0, x1, . . . , x5 = xt⟩of maximum-weight b-matchings in G, where each xi(e),
0 ≤i ≤5, is represented as the number of parallel edges between the endpoints of the
edge e.
1.1
Our Problem
In this paper, we generalize (cardinality) matching reconfiguration, and
study a reconﬁguration problem for maximum-weight b-matching deﬁned as
follows.
For a graph G, we denote by V (G) and E(G) the vertex set and edge set of
G, respectively. Let b: V (G) →Z≥0 be a capacity function on vertices, where
Z≥0 is the set of all nonnegative integers. Then, a vector x ∈ZE(G)
≥0
is called a
b-matching in G if 
e∈δ(v) x(e) ≤b(v) holds for each vertex v ∈V (G), where
δ(v) denotes the set of all edges incident to the vertex v. For example, Fig. 1(b)–
(g) illustrate six b-matchings in the graph G of Fig. 1(a). Note that an ordinary

Reconﬁguration of Maximum-Weight b-Matchings in a Graph
289
matching in a graph G is a b-matching in G such that b: V (G) →{1}. The
cardinality of a b-matching x in G is deﬁned as 
e∈E(G) x(e). Let w: E(G) →Z+
be a weight function on edges, where Z+ is the set of all positive integers. Then,
the weight of a b-matching x in G is deﬁned as 
e∈E(G) w(e)x(e).
For two b-matchings x and x′ in a graph G, we write x ↔x′ if there exists
a pair of edges e and f in G such that x(e) −x′(e) = x′(f) −x(f) = 1 and
x(g) = x(g) for all edges g ∈E(G) \ {e, f}. Thus, both x and x′ have the same
cardinality. (See any two consecutive b-matchings in Fig. 1(b)–(g) as examples.)
For two maximum-weight b-matchings x and x′ in G, we write x
w
↭x′ if there
exists a sequence ⟨x0, x1, . . . , xℓ⟩of b-matchings in G such that
(i) x0 = x and xℓ= x′;
(ii) all b-matchings x0, x1, . . . , xℓhave the maximum weight in G; and
(iii) xi−1 ↔xi holds for each i ∈{1, 2, . . . , ℓ}.
Then, the maximum-weight b-matching reconfiguration problem is
deﬁned as follows:
Input: A graph G, a capacity function b: V (G) →Z≥0 on
vertices, a weight function w: E(G) →Z+ on edges,
and two maximum-weight b-matchings xs and xt in G
Question: Determine whether xs
w
↭xt or not.
We denote by a 5-tuple (G, b, w, xs, xt) an instance of maximum-weight b-
matching reconfiguration. Note that this is a decision problem and hence
it does not ask for an actual sequence of maximum-weight b-matchings. For the
particular instance of Fig. 1, it has a desired sequence ⟨xs = x0, x1, . . . , x5 = xt⟩
as illustrated in the ﬁgure, and hence the answer is yes.
1.2
Known and Related Results
Ito et al. [11, Proposition 2] studied (cardinality) matching reconfigura-
tion, and gave a polynomial-time algorithm to solve the problem for any graph.
M¨uhlenthaler [20] generalized matching reconfiguration to the reconﬁgu-
ration problem for degree-constrained subgraphs in a graph G, where a degree-
constrained subgraph is a subgraph of G such that the degree of each vertex
satisﬁes both lower and upper bounds of the vertex. This generalized reconﬁgu-
ration problem is also solvable in polynomial time for any graph [20].1
In the reconﬁguration problem of M¨uhlenthaler [20], each edge can be cho-
sen at most once in a degree-constrained subgraph. However, the algorithm
of [20] can be easily extended so that it works correctly and runs in polyno-
mial time even if multiplicities on edges are allowed. By setting the lower bound
equal to zero and the upper bound equal to b(v) for each vertex v in a graph
1 Properly speaking, both Ito et al. [11] and M¨uhlenthaler [20] studied their recon-
ﬁguration problems under a more generalized reconﬁguration rule, called the TAR
(Token Addition and Removal) rule. Their results hold also under the reconﬁguration
rule of this paper, which is called the TJ (Token Jumping) rule.

290
T. Ito et al.
G, b-matchings (and hence ordinary matchings) in G can be seen as degree-
constrained subgraphs of G. Consider maximum-weight b-matching recon-
figuration when restricted to identical edge-weight. Then, each maximum-
weight b-matching in a graph G is simply a maximum-cardinality b-matching in
G. Thus, the result by M¨uhlenthaler [20] implies the following proposition.
Proposition 1 [20]. Maximum-weight b-matching reconfiguration is
solvable in polynomial time when restricted to identical edge-weight.
As far as we know, reconﬁguration problems have been studied mostly for
unweighted instances. Note that shortest path reconfiguration [2] and
Steiner tree reconfiguration [17] are deﬁned on unweighted graphs, and
hence they are cardinality variants. Matroid reconfiguration [11, Proposi-
tion 1] is only the example in reconﬁguration which admits a polynomial-time
algorithm for weighted instances. However, matchings do not form matroid bases.
1.3
Our Contribution
In this paper, we show that maximum-weight b-matching reconfigura-
tion is solvable in polynomial time for instances with no integrality gap. Such
instances include bipartite graphs with any capacity function b on vertices, and
general graphs G with the capacity function b: V (G) →{2}. Thus, our result
yields that the reconﬁguration problem for maximum-weight matchings can be
solved in polynomial time for bipartite graphs.
Our idea is to use the structure of maximum-weight b-matchings in a graph
with no integrality gap. As an intuitive example, the edge e′ in Fig. 1 would be
“useless” if w(e′) ≤2 because edges in two given maximum-weight b-matchings
have weights at least three; indeed, it is a no-instance if w(e′) ≤2. In Sect. 2, we
formulate the problem of ﬁnding a maximum-weight b-matching in a graph as
an integer program, and show that the complementary slackness condition gives
a characterization of b-matchings that have the maximum weight (Lemma 1).
Then, in Sect. 3, we will make use of Lemma 1, and reduce the problem of ask-
ing the existence of a desired sequence of maximum-weight b-matchings to the
problem of asking that of maximum-cardinality b-matchings; recall that the car-
dinality variant is solvable in polynomial time (Proposition 1).
2
Maximum-Weight b-Matchings
In this section, we give a characterization of maximum-weight b-matchings which
will play an important role in our algorithm in Sect. 3.

Reconﬁguration of Maximum-Weight b-Matchings in a Graph
291
Let G be a graph, and let b: V (G) →Z≥0 and w: E(G) →Z+ be capacity
and weight functions, respectively. We can formulate the problem of ﬁnding a
maximum-weight b-matching in G as the following integer program IP:
max.

e∈E(G)
w(e)x(e)
s.t.

e∈δ(v)
x(e) ≤b(v)
(∀v ∈V (G))
x(e) ∈Z≥0
(∀e ∈E(G)).
We denote by a triple (G, b, w) an input to IP. Let LP be the following linear
programming relaxation of IP:
max.

e∈E(G)
w(e)x(e)
s.t.

e∈δ(v)
x(e) ≤b(v)
(∀v ∈V (G))
x(e) ≥0
(∀e ∈E(G)).
The dual program DP of LP can be described as follows:
min.

v∈V (G)
b(v)y(v)
s.t.
y(u) + y(v) ≥w(e)
(∀e = {u, v} ∈E(G))
y(v) ≥0
(∀v ∈V (G)).
The complementary slackness condition (see, e.g., [15, Corollary 3.23]) implies
the following theorem.
Theorem 1. Suppose that x and y are feasible solutions of LP and DP, respec-
tively. Then, the following two statements (1) and (2) are equivalent.
(1) x and y are optimal solutions of LP and DP, respectively.
(2) x and y satisfy the following (i) and (ii):
(i) y(u) + y(v) = w(e) for every edge e = {u, v} ∈E(G) with x(e) > 0; and
(ii)

e∈δ(v)
x(e) = b(v) for every vertex v ∈V (G) with y(v) > 0.
For each feasible solution y of DP, let Vy = {v ∈V (G) | y(v) > 0} and
Ey = {e = {u, v} ∈E(G) | y(u) + y(v) = w(e)}. Then, Theorem 1 implies the
following corollary.
Corollary 1. Assume that the optimal value of IP for (G, b, w) is equal to that
of LP. Let y be an optimal solution of DP. Then, a b-matching x ∈ZE(G)
≥0
in
G has the maximum weight if and only if {e ∈E(G) | x(e) > 0} ⊆Ey and

e∈δ(v) x(e) = b(v) for every vertex v ∈Vy.

292
T. Ito et al.
Proof. We ﬁrst prove the only-if direction. Suppose that x is a maximum-weight
b-matching in G. Then, because the optimal value of LP is assumed to be equal
to that of IP, x is an optimal solution of LP. Since y is an optimal solution
of DP, x and y satisfy Theorem 1(2). Therefore, {e ∈E(G) | x(e) > 0} ⊆Ey
holds. For every vertex v ∈Vy, we have y(v) > 0 and hence Theorem 1(2)-(ii)
yields 
e∈δ(v) x(e) = b(v).
We then prove the if direction. Suppose that a b-matching x in G satisﬁes
{e ∈E(G) | x(e) > 0} ⊆Ey and 
e∈δ(v) x(e) = b(v) for every vertex v ∈Vy.
Since each edge e = {u, v} ∈E(G) with x(e) > 0 is contained in Ey, we have
y(u)+y(v) = w(e). Therefore, Theorem 1(2)-(i) holds. We then claim that x and
y satisfy Theorem 1(2)-(ii). Consider any vertex v ∈V (G) such that y(v) > 0.
Then, we have v ∈Vy, and hence 
e∈δ(v) x(e) = b(v) holds; thus, Theorem 1(2)-
(ii) holds. In this way, x and y satisfy Theorem 1(2). Then, Theorem 1(1) yields
that x is an optimal solution of LP. Since the optimal value of IP is assumed
to be equal to that of LP, x is a maximum-weight b-matching in G.
⊓⊔
We now rephrase Corollary 1 so that it can be easily applied to our algorithm
in the next section. For a graph G and its edge subset E′ ⊆E(G), we denote
by G[E′] the subgraph of G induced by E′, that is, the vertex set of G[E′]
is {u, v ∈V (G) | {u, v} ∈E′} and the edge set of G[E′] is E′. For a vertex
subset C ⊆V (G), we say that a b-matching x ∈ZE(G)
≥0
in G is C-saturated
if 
e∈δ(v) x(e) = b(v) holds for every vertex v ∈C. Then, Corollary 1 can be
rephrased as the following lemma; recall that a vertex cover of a graph G is a
vertex subset of G which contains at least one of the endpoints of every edge
in G.
Lemma 1. Assume that the optimal value of IP for (G, b, w) is equal to that of
LP. Then, there exist a vertex subset C ⊆V (G) and an edge subset E′ ⊆E(G)
such that
(a) C is a vertex cover of G[E′]; and
(b) a b-matching x ∈ZE(G)
≥0
in G has the maximum weight if and only if {e ∈
E(G) | x(e) > 0} ⊆E′ and x is C-saturated.
Furthermore, such a pair of C and E′ can be found in polynomial time.
Proof. Because an optimal solution y of DP can be computed in polynomial
time [21], we can obtain Vy and Ey in polynomial time. Let C = Vy and E′ = Ey.
Then, Condition (b) follows immediately from Corollary 1.
We now verify Condition (a). Consider any edge e = {u, v} ∈E′ = Ey.
Then, y(u) + y(v) = w(e) holds. Since w(e) > 0, we have y(u) > 0 or y(v) > 0.
Therefore, u ∈Vy or v ∈Vy, that is, at least one of the endpoints of e is contained
in Vy. In this way, C = Vy forms a vertex cover of G[E′].
⊓⊔
Note that we use the assumption of (nonzero) positive edge-weights only in
the proof of Lemma 1(a). Theorem 1 and Corollary 1 hold even for nonnegative
edge-weights, that is, w(e) ≥0 for all edges e ∈E(G).

Reconﬁguration of Maximum-Weight b-Matchings in a Graph
293
3
Algorithm
In this section, we give the main result of the paper as the following theorem.
Theorem 2. Maximum-weight
b-matching
reconfiguration
can
be
solved in polynomial time for any instance (G, b, w, xs, xt) such that the opti-
mal value of IP for (G, b, w) is equal to that of LP.
It is known that the optimal value of IP for (G, b, w) is equal to that of LP if
G is bipartite [22, Theorem 21.2], or b: V (G) →{2} [22, Corollary 30.2a]. Then,
we have the following corollary.
Corollary 2. Maximum-weight
b-matching
reconfiguration
can
be
solved in polynomial time for bipartite graphs, or b: V (G) →{2}.
In the remainder of this section, we prove Theorem 2 by giving such an algo-
rithm. As we mentioned in Introduction, we will reduce the problem of asking the
existence of a desired sequence of maximum-weight b-matchings to the problem
of asking that of maximum-cardinality b-matchings, by using the characterization
of maximum-weight b-matchings (Lemma 1).
Let (G, b, w, xs, xt) be an instance of maximum-weight b-matching
reconfiguration such that the optimal value of IP for (G, b, w) is equal to
that of LP. Let C ⊆V (G) and E′ ⊆E(G) be the pair obtained by Lemma 1. By
Lemma 1(b), any maximum-weight b-matching x ∈ZE(G)
≥0
in G satisﬁes x(e) = 0
for all edges e ∈E(G) \ E′. Therefore, it suﬃces to consider only C-saturated
b-matchings in the induced subgraph G[E′]. Note that both xs and xt are C-
saturated b-matchings in G[E′], because they are maximum-weight b-matchings
in G.
For two C-saturated b-matchings x, x′ ∈ZE′
≥0 in G[E′], we write x
C,E′
↭x′ if
there exists a sequence ⟨x0, x1, . . . , xℓ⟩of b-matchings in G[E′] such that
(i) x0 = x and xℓ= x′;
(ii) all b-matchings x0, x1, . . . , xℓare C-saturated; and
(iii) xi−1 ↔xi holds for each i ∈{1, 2, . . . , ℓ}.
By Lemma 1(b) we then have the following proposition.
Proposition 2. xs
C,E′
↭xt if and only if xs
w
↭xt.
Therefore, for proving Theorem 2, it suﬃces to determine whether xs
C,E′
↭xt
or not, in polynomial time. Our algorithm can be outlined as Algorithm 1. By
Lemma 1 and Proposition 1, Algorithm 1 runs in polynomial time. Thus, we will
prove its correctness in the remainder of this section.
We ﬁrst note that no edge in EC can be touched by any transformation of
C-saturated b-matchings, as in the following lemma.
Lemma 2. Let x and x′ be C-saturated b-matchings in G[E′] such that x
C,E′
↭x′.
Then, x(e) = x′(e) holds for each edge e ∈EC.

294
T. Ito et al.
Algorithm 1. Polynomial-time algorithm for maximum-weight b-matching
reconfiguration
Input: An instance (G, b, w, xs, xt) of maximum-weight b-matching reconfig-
uration such that the optimal value of IP for (G, b, w) is equal to that of
LP
Output: yes/no
Step 1. Obtain a vertex subset C ⊆V (G) and an edge subset E′ ⊆E(G) satisfying
Conditions (a) and (b) of Lemma 1. Let EC be the set of all edges e in
G[E′] such that both endpoints of e belong to C.
Step 2. If there exists an edge e ∈EC such that xs(e) ̸= xt(e), then return no.
Step 3. Let b′(v) = b(v) −
e∈δ(v)∩EC xs(e) for each vertex v in G[E′]. Delete all
edges in EC from G[E′]; let G′ be the resulting graph. Let x′
s and x′
t be two
b′-matchings in G′ such that x′
s(e) = xs(e) and x′
t(e) = xt(e), respectively,
for all edges e in G′.
Step 4. Apply Proposition 1 to the instance (G′, b′, wid, x′
s, x′
t) where wid : E(G′) →
{1}, and return its answer.
This lemma ensures the correctness of Step 2 of Algorithm 1.
We then show that the graph G′ obtained by Step 3 of Algorithm 1 satisﬁes
the following lemma.
Lemma 3. The graph G′ obtained by Algorithm 1 is a bipartite graph whose
bipartition is C and V (G′) \ C.
Proof. Since all edges in EC have been deleted, there is no edge joining two
vertices in C. On the other hand, by Lemma 1(a) at least one of the endpoints of
each edge in G[E′] is contained in C. Thus, there is no edge joining two vertices
in V (G′) \ C = V (G[E′]) \ C. Therefore, G′ is a bipartite graph with bipartition
(C, V (G′) \ C).
⊓⊔
Finally, the correctness of Step 4 of Algorithm 1 can be veriﬁed by combining
the following lemma with Lemma 2.
Lemma 4. A b′-matching x in G′ is C-saturated if and only if x has the maxi-
mum cardinality in G′.
Proof. We ﬁrst prove the only-if direction. Suppose that a b′-matching x in G′
is C-saturated. Then, 
e∈E(G′) x(e) ≥
v∈C b′(v) holds. Since G′ is a bipartite
graph whose one side of the bipartition is C, any b′-matching in G′ is of cardi-
nality at most 
v∈C b′(v). Therefore, x is a maximum-cardinality b′-matching
in G′.
We then prove the if direction. Suppose that a b′-matching x in G′ has the
maximum cardinality in G′. It suﬃces to prove

e∈E(G′)
x(e) ≥

e∈E(G′)
x′
s(e) ≥

v∈C
b′(v);

Reconﬁguration of Maximum-Weight b-Matchings in a Graph
295
then, all vertices in C must be saturated by x, because G′ is a bipartite
graph with bipartition (C, V (G′) \ C). The ﬁrst inequality holds because x is
a maximum-cardinality b′-matching in G′. We thus prove the second inequality,
as follows. Since xs is a maximum-weight b-matching in G, by Lemma 1(b) it
satisﬁes {e ∈E(G) | xs(e) > 0} ⊆E′ and is C-saturated. Therefore, we have

e∈E(G′)
x′
s(e) =

e∈E′\EC
xs(e) =

e∈E(G)\EC
xs(e)
≥

v∈C
b(v) −

e∈EC
xs(e) ≥

v∈C
b′(v),
as claimed.
⊓⊔
In this way, Algorithm 1 correctly solves maximum-weight b-matching
reconfiguration in polynomial time. This completes our proof of Theorem 2.
4
Conclusion
In this paper, we have shown that maximum-weight b-matching reconfigu-
ration is solvable in polynomial time for instances with no integrality gap. We
emphasize again that such instances include b-matchings (and hence ordinary
matchings) in bipartite graphs and 2-matchings in general graphs.
As we have mentioned in Sect. 1.2, both Ito et al. [11] and M¨uhlenthaler [20]
studied their reconﬁguration problems under a more generalized reconﬁguration
rule, called the TAR rule. In the weighted b-matching reconfiguration
problem under the TAR rule, we are given two b-matchings (which do not nec-
essarily have the maximum weight) together with an integer threshold k ∈Z≥0,
and asked the existence of a sequence of b-matchings between them, obtained
by either adding or deleting one edge at a time, with keeping weights at least
k. It remains open to clarify the complexity status for weighted b-matching
reconfiguration under the TAR rule; this open question was originally posed
by Ito et al. [11] for weighted matching reconfiguration.
References
1. Bonamy, M., Johnson, M., Lignos, I., Patel, V., Paulusma, D.: Reconﬁguration
graphs for vertex colourings of chordal and chordal bipartite graphs. J. Comb.
Optim. 27, 132–143 (2014)
2. Bonsma, P.: Rerouting shortest paths in planar graphs. In: Proceedings of FSTTCS
2012, LIPIcs, vol. 18, pp. 337–349 (2012)
3. Bonsma, P., Cereceda, L.: Finding paths between graph colourings: PSPACE-
completeness and superpolynomial distances. Theoret. Comput. Sci. 410, 5215–
5226 (2009)
4. Bonsma, P., Kami´nski, M., Wrochna, M.: Reconﬁguring independent sets in claw-
free graphs. In: Ravi, R., Gørtz, I.L. (eds.) SWAT 2014. LNCS, vol. 8503, pp.
86–97. Springer, Cham (2014). doi:10.1007/978-3-319-08404-6 8

296
T. Ito et al.
5. Demaine, E.D., Demaine, M.L., Fox-Epstein, E., Hoang, D.A., Ito, T., Ono, H.,
Otachi, Y., Uehara, R., Yamada, T.: Linear-time algorithm for sliding tokens on
trees. Theoret. Comput. Sci. 600, 132–142 (2015)
6. Gopalan, P., Kolaitis, P.G., Maneva, E.N., Papadimitriou, C.H.: The connectiv-
ity of Boolean satisﬁability: computational and structural dichotomies. SIAM J.
Comput. 38, 2330–2355 (2009)
7. Haas, R., Seyﬀarth, K.: The k-dominating graph. Graphs Comb. 30, 609–617
(2014)
8. Haddadan, A., Ito, T., Mouawad, A.E., Nishimura, N., Ono, H., Suzuki, A., Tebbal,
Y.: The complexity of dominating set reconﬁguration. Theoret. Comput. Sci. 651,
37–49 (2016)
9. Hatanaka, T., Ito, T., Zhou, X.: The list coloring reconﬁguration problem for
bounded pathwidth graphs. IEICE Trans. Fundam. Electron. Commun. Comput.
Sci. E98-A, 1168–1178 (2015)
10. van den Heuvel, J.: The complexity of change. In: Surveys in Combinatorics 2013.
London Mathematical Society Lecture Notes Series, vol. 409 (2013)
11. Ito, T., Demaine, E.D., Harvey, N.J.A., Papadimitriou, C.H., Sideri, M., Uehara,
R., Uno, Y.: On the complexity of reconﬁguration problems. Theoret. Comput.
Sci. 412, 1054–1065 (2011)
12. Ito, T., Nooka, H., Zhou, X.: Reconﬁguration of vertex covers in a graph. IEICE
Trans. Inf. Syst. E99-D, 598–606 (2016)
13. Ito, T., Ono, H., Otachi, Y.: Reconﬁguration of cliques in a graph. In: Jain, R.,
Jain, S., Stephan, F. (eds.) TAMC 2015. LNCS, vol. 9076, pp. 212–223. Springer,
Cham (2015). doi:10.1007/978-3-319-17142-5 19
14. Kami´nski, M., Medvedev, P., Milaniˇc, M.: Complexity of independent set recon-
ﬁgurability problems. Theoret. Comput. Sci. 439, 9–15 (2012)
15. Korte, B., Vygen, J.: Combinatorial Optimization: Theory and Algorithms, 5th
edn. Springer, Heidelberg (2012)
16. Makino, K., Tamaki, S., Yamamoto, M.: An exact algorithm for the Boolean con-
nectivity problem for k-CNF. Theoret. Comput. Sci. 412, 4613–4618 (2011)
17. Mizuta, H., Ito, T., Zhou, X.: Reconﬁguration of Steiner trees in an unweighted
graph. In: M¨akinen, V., Puglisi, S.J., Salmela, L. (eds.) IWOCA 2016. LNCS, vol.
9843, pp. 163–175. Springer, Cham (2016). doi:10.1007/978-3-319-44543-4 13
18. Mouawad, A.E., Nishimura, N., Pathak, V., Raman, V.: Shortest reconﬁguration
paths in the solution space of Boolean formulas. In: Halld´orsson, M.M., Iwama, K.,
Kobayashi, N., Speckmann, B. (eds.) ICALP 2015. LNCS, vol. 9134, pp. 985–996.
Springer, Heidelberg (2015). doi:10.1007/978-3-662-47672-7 80
19. Mouawad, A.E., Nishimura, N., Raman, V.: Vertex cover reconﬁguration and
beyond. In: Ahn, H.-K., Shin, C.-S. (eds.) ISAAC 2014. LNCS, vol. 8889, pp.
452–463. Springer, Cham (2014). doi:10.1007/978-3-319-13075-0 36
20. M¨uhlenthaler, M.: Degree-constrained subgraph reconﬁguration is in P. In: Italiano,
G.F., Pighizzini, G., Sannella, D.T. (eds.) MFCS 2015. LNCS, vol. 9235, pp. 505–
516. Springer, Heidelberg (2015). doi:10.1007/978-3-662-48054-0 42
21. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, Chichester (1986)
22. Schrijver, A.: Combinatorial Optimization: Polyhedra and Eﬃciency. Springer,
Berlin (2003)

Constant Approximation for Stochastic
Orienteering Problem with (1 + ϵ)-Budget
Relaxiation
Yiyao Jiang(B)
Institute for Interdisciplinary Information Sciences,
Tsinghua University, Beijing, China
jiangyy13@mails.tsinghua.edu.cn
Abstract. In the Stochastic Orienteering Problem (SOP), we are given
ﬁnite metric space (V, d) and a starting point ρ ∈V . Each v ∈V has
an associated reward rv ≥0 and random completion time Sv, where the
distribution of each Sv is know (once a reward has been earned, it cannot
be earned again); the time cost of traveling from u ∈V to v ∈V is d(u, v).
The goal is to sequentially visit vertices and complete tasks in order to
maximize the total rewards within 1 unit of time (after normalization).
In this paper, we present a nonadaptive O(ϵ−14)-approximation for (the
original, adaptive) SOP when we relax the unit time budget to (1 + ϵ),
0 < ϵ < 1.
1
Introduction
In the competitive practice of orienteering, participants are given a map with
marked locations. Starting from a given point, they must try to visit as many
locations as possible within a given time limit. Each point has a given reward,
and the goal for competitors is to maximize the total reward within the time
limit. This problem was ﬁrst studied in [8] from an algorithmic perspective; see
[13] for a more recent survey.
A similar problem to Orienteering Problem (OP) is the Prize Collecting Trav-
eling Salesman Problem (PCTSP) [5]. In the latter, the salesman’s costs are the
time spent traveling between points, plus a point-dependent penalty that is
incurred for each point that he fails to visit. PCTSP’s goal is to minimize the
traveling time plus the penalties of the points. Comparing with OP, PCTSP is
not limited in time, whereas OP is strictly bounded with time limit.
1.1
The (Simple) Orienteering Problem
An instance of the Orienteering Problem (OP) consists of a metric space (V, d)
with |V | = n points and distances d(u, v) ∈R+ for each (u, v) ∈V × V ; more-
over, the instance speciﬁes a starting point ρ ∈V , a total time budget B, and
a reward rv ∈R+ for each v ∈V . The object is to chart a path that maximizes
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 297–308, 2017.
DOI: 10.1007/978-3-319-62389-4 25

298
Y. Jiang
the sum of rewards rv while staying within the time budget B, where the dis-
tances d(u, v) are interpreted as time costs. In this setting, and as all quantities
are real numbers, we can assume without loss of generality that B = 1 after
normalization.
In 1998 Arkin et al. [1] obtained a 2-approximation for OP with planar Euclid-
ean distances. Furthermore, Chen and Har-Peled [7] obtained a polynomial-time
approximation scheme (PTAS) for OP in ﬁxed-dimensional Euclidean space.
For general metric spaces (V, d), Blum et al. [2] obtained the ﬁrst constant-
factor approximation (with factor 4) in 2007. The best result until now is a
(2 + ϵ)-approximation algorithm by Chekuri et al. [6].
The Orienteering Problem ﬁnds many natural applications. For example, a
traveling salesman with limited time [12] (who must maximize his total proﬁt in
one day).
1.2
Stochastic Orienteering Problem
In the Stochastic Orienteering Problem (SOP) [9] a job is associated to each
vertex v ∈V , which must be completed at v in order to obtain the reward rv.
The time necessary to complete the job, moreover, is randomly distributed. For
example, a salesman may need to wait for his customer to come downstairs, or
a tourist need time to enjoy the scenery, and so on.
Generally, one is not allowed to give up a job until it is ﬁnished. In certain
models one is allowed to give up a job and its reward after starting the job,
while being forbidden from re-trying to complete the job later, once it has been
given up.
The time required by job v is a random variable Sv, where Sv has a known
discrete distribution πv for each v ∈V , where πv modeled as a function from R+
into [0, 1]; the distributions {πv : v ∈V } are thus part of an instance of SOP.
We note that SOP can be considered as a kind of combination of OP and of
the Stochastic Knapsack Problem (SKP) [11]. The Stochastic Knapsack Prob-
lem, indeed, corresponds to an instance of SOP in which d(u, v) = 0 for all
(u, v) ∈V × V . For SKP, moreover, a (2 + ϵ)-approximation algorithm is
known [4].
For a non-adaptive variant of SOP in which the path is chosen in advance
and in which the distances d(u, v) are restricted to integers, Gupta et al. [9] have
shown a constant factor approximation algorithm. In that setting (and due to
the integer restriction on distances) the time budget B is not normalized to 1,
and remains a parameter of the problem. In fact, Gupta et al. show that their
algorithm is an O(log log B)-approximation algorithm for SOP with arbitrary
adaptive policies. In the same formal setting, moreover, Bansal and Nagarajan
[3] have shown that a Ω((log log B)1/2) gap between adaptive and non-adaptive
policies is unavoidable. This contrasts with the case of SKP, for which non-
adaptive policies can approximate adaptive policies to within a constant factor
[4] (In OP the distinction between adaptive and non-adaptive policies is moot.).
In this paper we present an O(ϵ−14)-approximation for the (original, adap-
tive) SOP with time budget (1+ϵ)B = 1+ϵ, 0 < ϵ < 1. More precisely, we show

Constant Approximation for Stochastic Orienteering Problem
299
a non-adaptive policy of time budget 1 + ϵ such that the expected reward of the
optimal adaptive policy at time budget 1 is at most O(ϵ−14) times the expected
reward of our [non-adaptive, time 1+ϵ] policy. Our algorithm therefore achieves
an O(1)-approximation for any constant 0 < ϵ < 1.
Theorem 1. There is a polynomial time, non-adaptive O(ϵ−14)-approximation
for SOP with respect to a relaxed time budget of (1 + ϵ)B, 0 < ϵ < 1.
2
Deﬁnitions and Notations
We write R+ for the nonnegative real numbers [0, ∞). In this paper all quantities
are real-valued.
2.1
Orienteering
As introduced in Sect. 1, an instance of the Orienteering Problem (OP) is deﬁned
on an underlying metric space (V, d) with |V | = n points and distances d(u, v) ∈
R+ for each (u, v) ∈V × V . We are given a starting “root” point ρ ∈V and
a total time budget B. An instance of OP thus corresponds to a tuple IO =
(V, d, {rv}v∈V , B, ρ).
2.2
Stochastic Orienteering
In the Stochastic Orienteering Problem (SOP) each point v ∈V is related to a
unique stochastic job. The job associated to v has a ﬁxed reward rv ≥0, and a
random variable of processing time size Sv with a known but arbitrary discrete
probability distribution πv : U →[0, 1], where U ⊆R+ is a countable set, such
that 
t∈U πv(t) = 1 for all v ∈V . (Time size is the time that we have to wait at
the point before receiving the reward for job v. Note that we can take the same
underlying time set U for all v ∈V , wlog.) An instance of SOP thus corresponds
to a tuple ISO = (π, V, d, {rv}v∈V , {Sv}v∈V , B, ρ).
At each step we travel to a point v from a current point u (at time cost
d(u, v)) and then process the job v, at time cost Sv, and once a job v is selected,
one must wait for v to complete. If we are still within the time budget B when
the job is completed we receive the reward rv, and choose a new point as the
next destination.
For generality, ∀t ∈U we assume t ∈[0, B]. If one time size t > B, we can just
truncated it to B, since we will always stop at time budget B before completing
the job and never get the reward. Also, we can assume wlog that d(u, ρ) ≤B
for all u ∈V , which also implies that d(u, v) ∈[0, 2B] for all u, v ∈V .
2.3
Task Orienteering Problem
Our analysis refers to a simpliﬁed version of the Stochastic Orienteering Problem
known as the “Task Orienteering Problem” (TOP); TOP corresponds to an
instance of SOP in which each time size Sv is deterministic. An instance of TOP
thus corresponds to a tuple ITO = (V, d, {rv}v∈V , {sv}v∈V , B, ρ) with sv ∈R+
for each v ∈V .

300
Y. Jiang
3
Algorithm
We begin with an analysis of the Task Orienteering Problem described in
Sect. 2.3, to be used as a component in our main analysis.
3.1
An Algorithm for TOP
We ﬁrst provide a polynomial time algorithm AlgTO (Algorithm 1) and then
prove that AlgTO gives an O(1)-approximation to TOP.
Deﬁnition 1 (Valid OP Instance). Given an instance ITO = (V, d, {rv}v∈V ,
{sv}v∈V , B, ρ) of TOP, we deﬁne the following valid OP instance IO(ITO) :=
(V ′, d′, {rv}v∈V ′, B, ρ), where
1. V ′ := {v ∈V : d(ρ, v) + sv ≤B};
2. d′(u, v) := d(u, v) + sv/2 + su/2 for all u, v ∈V ′, u ̸= v;
3. d′(u, u) := 0 for all u ∈V ;
4. r′
v := rv for all v ∈V ′.
Lemma 1. The function d′ : V ′ × V ′ →R+ constructed in Deﬁnition 1 is a
metric.
The proof is easy.
Algorithm 1. AlgTO for Task Orienteering on input ITO = (V, d, {rv}v∈V ,
{sv}v∈V , B, ρ)
1. let IO(ITO) := (V ′, d′, {r′
v}v∈V ′, B, ρ);
2. run AlgOrient on the (Simple) Orienteering Problem IO,
3.
get the path P, the reward OPT′
O = 
v∈P r′
v, and the ending point ρ′;
4. compare 
v∈P \{ρ′} r′
v and r′
ρ′;
5. output the path
P ′ :=

ρ −→ρ′,
if r′
ρ′ ≥
v∈P \{ρ′} r′
v,
P\{ρ′},
otherwise;
6.
and the reward OPT′
TO := 
v∈P ′ r′
v.
Theorem 2. Algorithm AlgTO (Algorithm 1) is a polynomial time O(1)-
approximation for the Task Orienteering Problem.
Proof. Assume the optimal result for Task Orienteering problem ITO is OPTTO.
Now we prove that OPT′
TO returning by the polynomial time algorithm AlgTO
(Algorithm 1) is an Ω(1)-approximation to OPTTO.
In Algorithm 1 line 1, as in Deﬁnition 1 item 1, we delete all points v ∈V
which are obviously not in the optimal path of ITO, because even we only reach

Constant Approximation for Stochastic Orienteering Problem
301
to one point v from the starting point ρ, we still do not have enough time to
reach the point v and complete the job with time d(ρ, v) + sv > B.
In Deﬁnition 1 item 2, for each point v ∈V ′, we divide the ﬁxed job time
sv by 2, and add sv/2 to the lengths of all edges adjacent to point v. (Note
that, although we add sv/2 to the lengths of all edges adjacent to point v, in
real algorithm path, we only count twice of sv/2 as doing the job in time sv:
the edge into point v, and the edge out of point v). Deﬁne the new distance as
d′(u, v).
In Lemma 1, we prove that (V ′, d′) is a metric, thus we create an Orienteering
Problem instance IO = (V ′, d′, {rv}v∈V , B, ρ) successfully in Algorithm 1 step 1.
In Algorithm 1 line 2, we use the polynomial time algorithm AlgOrient (by
Blum et al. [2] mentioned in Sect. 2.1) on OP instance IO. And in line 3, the
path P of reward OPT′
O is a constant approximation of the optimal solution for
IO by algorithm AlgOrient. Note that any path in ITO including the optimal
path of ITO is surely a feasible path in IO (since the only change is the distances
in ITO are longer), so OPT′
O = Ω(OPTTO).
Note that in path P, we always get in and out of a point v except for the
starting point ρ and ending point ρ′. So the sv/2 are always counted twice, and
in ITO we do complete all the jobs on the path except for ρ and ρ′. Thus the only
incidence we cannot complete path P in ITO is the jobs on ρ and ρ′ in path P.
At the starting point ρ in ITO, we can always create a fake “starting point”
ρ⋆which is the same d′(, ) as ρ but d′(ρ, ρ⋆) = d′(ρ⋆, ρ) = d′(ρ⋆, ρ⋆) = 0, sρ⋆= 0
and rρ⋆= 0, and we set ρ as a normal point. Run AlgTO (Algorithm 1) on ITO
with ρ⋆. If ρ is still in path, we just start from ρ and use sρ time to get the
reward rρ then go through path P; else, ρ is not in path, we also start from ρ,
but give up the job and reward on ρ, then go through path P. All these changes
do not change the OPT′
O, and do not eﬀect on OPTTO. Thus, we now only need
to concern about ρ′.
In Algorithm 1 line 4, we compare the reward r′
ρ′ at the ending point ρ′ (note
that d(ρ, ρ′) + sρ′ ≤B in Deﬁnition 1 indicates that path ρ −→ρ′ is feasible
in ITO), and the rewards 
v∈P \{ρ′} r′
v of all the points in path P except for
ρ′. Since OPT′
O = 
v∈P r′
v = r′
ρ′ + 
v∈P \{ρ′} r′
v, max{r′
ρ′, 
v∈P \{ρ′} r′
v} ≥
OPT′
O/2 = Ω(OPTTO).
In Algorithm 1 line 5 and 6, we thus choose the larger one between r′
ρ′ and

v∈P \{ρ′} r′
v as OPT′
TO. If rρ′ is larger, then we just go to a single point ρ′ and
complete the job; if the other is larger, we just ignore ρ′, go and do the jobs
through the path P except for ρ′.
Thus, we have a polynomial time O(1)-approximation algorithm AlgTO for
Task Orienteering problem.
3.2
The Algorithm for Stochastic Orienteering
Deﬁnition 2 (Valid TOP Instance). Given an instance ISO = (π, V, d, {rv}v∈V ,
{Sv}v∈V , B, ρ) of SOP, and a value ϵ > 0, we deﬁne the following valid TOP

302
Y. Jiang
instance with parameter ϵ to be ITO(ϵ, ISO) := (V , d, {ru}∀u∈V , {su}∀u∈V , B, ρ),
where we
1. deﬁne V : let V := V ,
for all u ∈V , if d(ρ, u) > B, V := V \{u},
create a virtual point ρ, V := V ∪{ρ};
2. for all u, v ∈V , deﬁne distances d(u, v): if u, v ∈V , d(u, v) := d(u, v),
for all u ∈V , set d(u, ρ) = d(ρ, u) = B,
set d(ρ, ρ) = 0;
3. for all u ∈V , deﬁne rewards ru: if u ∈V , ru = ru, set rρ = 0;
4. for all u ∈V , deﬁne deterministic job times su: if u ∈V , su = E[S′
u],
where S′
u = min{Su, B},
set sρ = 0;
5. deﬁne time bound B: let B := (1 + ϵ13)B;
6. deﬁne the starting point: set ρ to be the new starting point.
Lemma 2. The (V , d) of the TOP instance ITO in Deﬁnition 2 is a metric
space.
The proof is easy.
Algorithm 2. Algorithm AlgSO for SOP on input ISO = (π, V, d, {rv}v∈V ,
{Sv}v∈V , B, ρ) and parameter 0 < ϵ < 1
1. for all v ∈V do
2.
let Rv := rv · PrSv∼πv[Sv ≤(B −d(ρ, v))] be the expected reward of the single-
node tour from ρ to v;
3. w.p. 1/2, just visit the point v with the highest Rv and exit.
4. let ITO(ϵ, ISO) := (V , d, {ru}∀u∈
V , {su}∀u∈
V , B, ρ);
5. run AlgTO (Algorithm 1) on the valid TOP instance ITO(ϵ, ISO),
6.
get the path P, the reward

OPTTO;
7. replace the starting point ρ in path P of ITO(ϵ, ISO) by ρ and output path P;
8. go through path P in ISO with time budget (1 + ϵ)B and output the total reward

OPTSO.
We want to prove that Algorithm 2 is a polynomial time O(ϵ−14)-
approximation for Stochastic Orienteering problem with respect to a relaxed
time budget of (1 + ϵ)B, 0 < ϵ < 1.
Deﬁne the expected reward on optimal adaptive policy on ISO as OPT, and
deﬁne the expected reward on optimal policy on ITO(ϵ, ISO) as OPTTO.
We design to have either a single-point tour ρ −→v of expected reward Rv =
Ω(OPT) with 50 % chance, or the path P on ISO of reward

OPTSO = Ω(OPT)
with 50 % chance. For this purpose, we want the following theorem.

Constant Approximation for Stochastic Orienteering Problem
303
Theorem 3. Given an instance ISO for which an optimal adaptive strategy has
an expected reward of OPT, either there is a single-point tour with expected
reward Ω(OPT), or the valid Task Orienteering instance ITO(ϵ, ISO) has reward
OPTTO = Ω(OPT), with constant approximation parameter ϵ14.
We will prove Theorem 3 later in Sect. 4. For now, let us assume that
Theorem 3 is correct, and use it to complete the proof of Theorem 1. The follow-
ing proof is for Theorem 1.
3.3
Proof for Theorem 1
Suppose we enter Algorithm 2 line 4, and in line 6, AlgTO (Algorithm 1) ﬁnds
a path P = (ρ, v1, v2, . . . , vk) with reward

OPTTO. And then we get the path
P = (ρ, v1, v2, . . . , vk).
Lemma 3. In ISO = (π, V, d, {rv}v∈V , {Sv}v∈V , B, ρ), for any point vi ∈P, the
probability of successfully reaching to v and ﬁnishing the job at v before violating
the budget (1 + ϵ)B is at least 1 −ϵ13.
Proof. Note that now we relax the time budget on ISO to (1 + ϵ)B, but we still
have time budget (1 + ϵ13)B on ITO(ϵ, ISO) as in Deﬁnition 2.
In Deﬁnition 2, for ITO(ϵ, ISO), k
i=1(svi + d(vi−1, vi)) ≤(1 + ϵ13)B, S′
vi =
min{Svi, B}, and svi = E[S′
vi]. Since d(ρ, v1) = B, E[S′
v1] + k
i=2(E[S′
vi] +
d(vi−1, vi)) ≤ϵ13B.
Deﬁne S⋆
vi =

S′
v1
if i = 1,
S′
vi + d(vi−1, vi)
if 2 ≤i ≤k.
Then k
i=1 E[S⋆
vi] ≤ϵ13B. By using Markov’s inequality, for each j ≤k, we
have
Pr[
j

i=1
S⋆
vi ≤ϵB] ≥1 −ϵ13
The change from Su to S′
u does not eﬀect on optimal adaptive policy on
ISO, the distance d(ρ, v1) ≤B, and d(vi−1, vi) = d(vi−1, vi). So in path P of
ISO with time budget (1 + ϵ)B, for each point vj (1 ≤j ≤k), the probability
we successfully reach to point vj, and complete the job on point vj is not less
than Pr[j
i=1 S⋆
vi ≤ϵB]. Thus the probability of successfully reaching to v and
ﬁnishing the job at v before violating the budget (1+ϵ)B is at least 1−ϵ13. And
this complete the proof for Lemma 3.
Proof (The proof of Theorem 1). We prove that AlgSO (Algorithm 2) is a poly-
nomial time O(ϵ−14)-approximation for Stochastic Orienteering problem with
respect to a relaxed time budget of (1 + ϵ)B, 0 < ϵ < 1.
We assume that Theorem 3 is correct. Then either there is a single-point
tour with expected reward Ω(OPT), or the valid Task Orienteering instance
ITO(ϵ, ISO) has reward OPTTO = Ω(OPT), with a constant approximation para-
meter ϵ14.

304
Y. Jiang
1. There is a single-point tour ρ −→v satisﬁes Rv = Ω(ϵ14OPT).
We have 50% chance to obtain the highest Rv. And Rv ≥Rv = Ω(ϵ14OPT).
2. The valid Task Orienteering instance ITO(ϵ, ISO) has reward OPTTO =
Ω(OPT).
We have 50% chance to get

OPTSO.
In Algorithm 2 line 6, by applying Theorem 2, AlgTO (Algorithm 1) will
ﬁnd a path P
= (ρ, v1, v2, . . . , vk) with reward

OPTTO = Ω(OPTTO).
By assuming Theorem 3 is correct, we have OPTTO = Ω(ϵ14OPT). Then

OPTTO = Ω(OPTTO) = Ω(ϵ14OPT).
Now applying Lemma 3 on path P that we get in Algorithm 2 line 7, and
that gives us an expected reward of at least

OPTSO =

OPTTO · (1 −ϵ13) =
Ω(ϵ14OPT).
So we always have at least 50% probability to get Ω(ϵ14OPT) by Algorithm 2,
and thus we complete the proof of Theorem 1.
4
Optimal Policy for Stochastic Orienteering
The main method to deal with the huge optimal decision tree deﬁned in Sect. 4.1
is Discretization and Block Policy [10]. Since all quantities are all real-valued, by
scaling, we can assume B = 1 and the size of each item is distributed between 0
and B. The relaxed time budget B + O(ε) should be less than 2B.
4.1
Policy and Decision Tree
A policy σ for an SOP instance (π, V, d, B) can be represented as a decision tree
Tσ(π, V, d, B).
Fig. 1. Decision tree Tσ
In Fig. 1, each node of Tσ is labeled by a vertex v ∈V , with the root of Tσ
being labeled by ρ. The vertices on a path from the root to a node are distinct,
hence Tσ has depth at most n = |V |. A node of Tσ has countably many children,

Constant Approximation for Stochastic Orienteering Problem
305
or one for each element of U, where U = {t ∈R+ : πv(t) > 0 for some v ∈V } is
the (countable) support set of the time size distributions. Let z ∈Tσ be a node
of label v ∈V , and let e be the t-th edge emanating from z, t ∈U; then e has
probability πe := πv(t) = Pr[Sv = t]; moreover if z’s parent node is node y ∈Tσ
of label u ∈V , then e has weight we := d(u, v) + t.
We will sometimes refer to nodes in Tσ by their label. (With little chance of
confusion.) For a node v ∈Tσ, we deﬁne P(v) to be the path of edges of leading
from the root to v. We deﬁne W(v) := 
e∈P (v) we and Φ(v) := 
e∈P (v) πe.
Thus W(v) is the total time elapsed before reaching v and Φ(v) is the probability
of reaching v.
We note that a given policy σ will be “compatible” with any tuple (π′, V ′,
d′, B′) such that V ′ = V and such that π′ has the same support U as π. Namely,
the modiﬁed π′ and d′ will induce modiﬁed edge weights w′
e and modiﬁed edge
probabilities p′
e. We emphasize this by including (π, V, d, B) as arguments to Tσ
(though we will always use the same V for a given policy σ).
We let
R(σ, π, V, d, B) =

v∈Tσ,e=(v,u):W (v)+we≤B
πe · rvΦ(v)
denote the expected reward that policy σ achieves with respect to the metric
d, time size distributions π, and time budget B. We use OPT to denote the
expected reward of the optimal adaptive policy.
For the following lemma, we also consider a modiﬁed node label Xv := Sv +
d(u, v), where u is the parent and v is the child. Thus, while we = t+d(u, v) is a
number, Xv is a random variable (Moreover, Xv “ignores” the fact that edge e is
the t-th edge of v, i.e., that edge e is associated to a particular outcome of Sv.).
Lemma 4 (part of Lemma 2.4 in [4]). For any policy σ on instance (π, V, d, B),
there exists a policy σ′ such that
R(σ′, π, V, d, B) = (1 −O(ϵ))R(σ, π, V, d, B),
and for any realization path P in Tσ′(π, V, d, B), 
v∈P E[Xv] = O(B/ϵ).
Here we mention that policy σ′ is just cutting some branches on policy σ to
ensure 
v∈P E[Xv] = O(B/ϵ). Note that policy σ is all designed by us, we surely
may have a node v that 
v∈P E[Xv] > O(B/ϵ) though v has no contribution
to reward.
4.2
Discretization
We present how to discretize the decision tree in Sect. 4.1 with given ϵ. We use
the discretization method similar in paper [10] Sect. 2.1. We split all jobs into two
parts: small size vertices and big size vertices, and discretize them separately.
Denote Sv for value of vertex v after discretization, and the new distribu-
tion πv. And for each node v in the tree, deﬁne 
Xv := Sv + d(u, v) as a random
variable which is similar in the deﬁnition of Xv in Sect. 4.1.

306
Y. Jiang
1. Small size region if Sv ≤ϵ4.
There exists a value 0 ≤h ≤ϵ4, such that Pr[Sv ≥h|Sv ≤ϵ4] · ϵ4 =
E[Sv|Sv ≤ϵ4]. Then set: Sv =
⎧
⎪
⎨
⎪
⎩
0,
0 ≤Sv < h;
ϵ4,
h ≤Sv ≤ϵ4;
Sv,
Sv > ϵ4.
2. Large size region if Sv > ϵ4.
We simply discretize it as Sv = ⌊Sv
ϵ5 ⌋ϵ5.
We denote the set of the discretized size by S = {s0, s1, · · · , sz−1} where
s0 = 0, s1 = ϵ5, s2 = 2ϵ5, . . . , sz−1. Note that s1 = ϵ5, s2 = 2ϵ5, . . . , s1/ϵ−1 =
ϵ4 −ϵ5 are also in S though their probability is 0. The total number of values of
time size is |S| = z = O(B/ϵ5) which is a constant.
4.3
Canonical Policies
We need to use canonical policies introduced in [4]. A policy σ is a canonical
policy if it makes decisions based on the discretized sized of vertices, but not their
actual sizes. Under the canonical policy, we will keep trying new vertices if the
total discretized size budget does not exceed, even the actual budget overﬂows.
But no reward will get from these vertices which make actual budget exceeding.
Lemma 5 (Lemma 4.2 in [10]). Let π be the distribution of vertice size and π
be the discretized version of π. Then, the following statements hold:
1. For any policy σ, there exists a canonical policy σ such that
R(σ, π, V, d, (1 + 4ϵ)B) = (1 −O(ϵ))R(σ, π, V, d, B);
2. For any canonical policy σ,
R(σ, π, V, d, (1 + 4ϵ)B) = (1 −O(ϵ))R(σ, π, V, d, B).
Proof sketch: for the ﬁrst result, we prove that there is randomized canonical
policy σr such that R(σr, π, V, d, (1 + 4ϵ)B) = (1 −O(ϵ))R(σ, π, V, d, B). Thus
such a deterministic policy σ exists. The randomized policy σr is derived from
σ as follows. Tσr keeps the same tree structure as Tσ. If σr visits a vertex v
and observes a discretized size s ∈S, it chooses a random branch in σ among
those sizes that are mapped to s. Let t1, t2, . . . , tk are possible size realization of
s as per size distribution π and let πv(t1), πv(t2), . . . , πv(tk) be the correspond-
ing probabilities. Hence we choose one of the branches corresponding to size
t1, t2, . . . , tk w.p. πv(t1), πv(t2), . . . , πv(tk) (normalized) respectively. For more
detail, see Lemma 4.2 in [10].
From Lemma 5, we conclude that the reward will not loss too much if we
using canonical policies to applying the policies with Tσ described in Sect. 4.2,
but not T.

Constant Approximation for Stochastic Orienteering Problem
307
4.4
Block Tree
Now we partition the decision tree Tσ created in Sect. 4.3 into blocks.
For any node v in the decision tree Tσ, we deﬁne the leftmost path of v to be
the realization path which starts at v,ends at a leaf, and consists of only edges
corresponding to size zero. We deﬁne the block starting with node v (denote
as seg(v)) in Tσ as the maximal preﬁx of the leftmost path of v such that: If
E[ 
X(v)] > ϵ13, seg(v) is the singleton node {v}. Otherwise, E[ 
X(seg(v))] ≤ϵ13.
We partition Tσ into blocks as follows. We say a node v is a starting node
if v is a root or v corresponds to a non-zero size realization of its parent. For
each starting node v, we greedily partition the leftmost path of v into blocks,
i.e., delete seg(v) and recurse on the remaining part.
Lemma 6. A canonical policy σ with expected reward (1 −O(ϵ))OPT can be
partitioned into several blocks that satisfy the following properties:
1. There are at most O(ϵ−14) blocks on any root-leaf path in the decision tree.
2. There are |S| = O(B/ϵ5) children for each block.
3. Each block M with more than one node satisﬁes that 
b∈M E[
Xb] ≤ϵ13.
Proof. 1. Fix a particular root-to-leaf path R. Let us bound the number of blocks
on R.
We have 
v∈R E[ 
X(v)] = O(B/ϵ) = O(1/ϵ) = O(ϵ−1) by Lemma 4. By deﬁ-
nition of Block Policies, any single-point block v in Tσ satisﬁes E[ 
X(v)] > ϵ13.
Then there are at most O(ϵ−1)/ϵ13 = O(ϵ−14) single-point in path R.
By deﬁnition of Discretization, any 
X(v) with non-zero size after discretiza-
tion, is no less than ϵ4. Then there are at most O(1/ϵ4) = O(ϵ−4) nodes w
corresponding to a non-zero size realization.
This gives a bound O(ϵ−14 + ϵ−4) = O(ϵ−14) blocks on any root-leaf path in
the decision tree.
2. After Discretization in Sect. 4.2, we have only O(B/ϵ5) number of values
which means there are |S| = O(B/ϵ5) children for each block.
3. This is exactly what we deﬁne our blocks.
And then we prove Theorem 3.
Proof (The proof of Theorem 3). By Properties 1,2 in Lemma 6 above, there are
only constant number of blocks in the decision tree Tσ with block policies. Since
there are at most O(ϵ−14) blocks in the optimal policy path in decision tree Tσ
of reward (1 −O(ϵ))OPT, there exists a block M with Ω(ϵ14OPT) reward.
We consider two situations in block M with diﬀerent number of nodes.
Block M is a single-node block of node v.
And the node v has a huge reward. Then the single-node tour from the
starting point ρ to v has an expected reward Ω(ϵ14OPT).
Otherwise, block M has at least two nodes.
And block M has an expected reward Ω(ϵ14OPT). By Property 3 in Lemma 6,
we have 
b∈M E[
Xb] ≤ϵ13. Since the distance from the starting point ρ to the

308
Y. Jiang
ﬁrst node in M is at most 1, the path P ⋆which connects ρ and the block M in
order has length at most 1 + ϵ13. Thus P ⋆is a feasible path for ITO(ϵ, ISO) as
deﬁned in Deﬁnition 2, and it has an expected reward Ω(ϵ14OPT) ≤OPTTO.
Thus, either there is a single-point tour with expected reward Ω(OPT), or
the valid Task Orienteering instance ITO(ϵ, ISO) has reward OPTTO = Ω(OPT),
with constant approximation parameter ϵ14. And this completes the proof for
Theorem 3.
5
Conclusion
We describe an O(1)-approximation algorithm with time budget (1+ϵ)B. As the
expected reward of the optimal policy at time budget B is at most O(ϵ−14) times
the expected reward of our [time (1+ϵ)B] policy, we get an O(1)-approximation
for any constant 0 < ϵ < 1.
References
1. Arkin, E.M., Mitchell, J.S.B., Narasimhan, G.: Resource-constrained geometric
network optimization. In: Proceedings of the Fourteenth Annual Symposium on
Computational Geometry, pp. 307–316 (1998)
2. Avrim, B., Shuchi, C., Karger David, R., Terran, L., Adam, M., Maria, M.: Approx-
imation algorithms for orienteering and discounted-reward TSP. SIAM J. Comput.
37(2), 653 (2007)
3. Bansal, N., Nagarajan, V.: On the Adaptivity Gap of Stochastic Orienteering.
Springer International Publishing, Cham (2014)
4. Bhalgat, A., Goel, A., Khanna, S.: Improved approximation results for stochastic
knapsack problems. In: Proceedings of the Twenty Second Annual ACM SIAM
Symposium on Discrete Algorithms, vol. 27(2), pp. 1647–1665 (2011)
5. Bienstock, D., Goemans, M.X., Simchi-Levi, D., Williamson, D.: A note on the
prize collecting traveling salesman problem. Math. Program. 59(3), 413–420 (1993)
6. Chekuri, C., Korula, N., Pal, M.: Improved algorithms for orienteering and related
problems. ACM Trans. Algorithms 8(3), 661–670 (2008)
7. Chen, K., Har-Peled, S.: The orienteering problem in the plane revisited. In: Pro-
ceedings of Symposium on Computational Geometry, pp. 247–254 (2006)
8. Golden, B.L., Levy, L., Vohra, R.: The orienteering problem. Nav. Res. Logist.
34(34), 307–318 (1987)
9. Gupta, A., Krishnaswamy, R., Nagarajan, V., Ravi, R.: Approximation algorithms
for stochastic orienteering. In: Proceedings of the Twenty-Third Annual ACM-
SIAM Symposium on Discrete Algorithms, pp. 1522–1538 (2012)
10. Li, J., Yuan, W.: Stochastic combinatorial optimization via poisson approximation.
In: Proceedings of the Annual ACM Symposium on Theory of Computing, pp. 971–
980 (2012)
11. Ross, K.W., Tsang, D.H.K.: Stochastic knapsack problem. IEEE Trans. Commun.
37(7), 740–747 (1989)
12. Tsiligirides, T.: Heuristic methods applied to orienteering. J. Oper. Res. Soc. 35(9),
797–809 (1984)
13. Vansteenwegen, P., Souﬀriau, W., Van Oudheusden, D.: The orienteering problem:
a survey. Mitteilungen Klosterneuburg Rebe Und Wein Obstbau Und Fruchtever-
wertung 209(209), 1–10 (2011)

Quantum Query Complexity of Unitary
Operator Discrimination
Akinori Kawachi1, Kenichi Kawano1(B), Fran¸cois Le Gall2,
and Suguru Tamaki2
1 Tokushima University, 2-1 Minamijyousanjima, Tokushima 770-8506, Japan
c501637039@tokushima-u.ac.jp
2 Kyoto University, Yoshida Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
Abstract. Unitary operator discrimination is a fundamental problem in
quantum information theory. The basic version of this problem can be
described as follows: given a black box implementing a quantum oper-
ator U, and the promise that the black box implements either the uni-
tary operator U1 or the unitary operator U2, the goal is to decide whether
U = U1 or U = U2. In this paper, we consider the query complexity of
this problem. We show that there exists a quantum algorithm that solves
this problem with bounded-error probability using

π
3θcover

queries to the
black-box, where θcover represents the “closeness” between U1 and U2 (this
parameter is determined by the eigenvalues of the matrix U †
1U2). We also
show that this upper bound is essentially tight: we prove that there exist
operators U1 and U2 such that any quantum algorithm solving this prob-
lem with bounded-error probability requires at least

2
3θcover

queries.
Keywords: Quantum algorithms · Quantum information theory ·
Query complexity
1
Introduction
Background. Quantum state discrimination is one of the most fundamental prob-
lems in quantum information theory [3,4,10,14]. In a typical setting, the goal of
this problem is to discriminate two quantum states. The success probability of
this problem is known to be characterized by the orthogonality of the two states.
In particular, non-orthogonal states cannot be discriminated with probability 1,
no matter how many independent copies of the input state are given.
A problem closely related to quantum state discrimination is the quantum
operator discrimination problem [1,5–9,12,15,16]. Similarly to quantum state
discrimination, in a typical setting the goal of quantum operator discrimination
is to discriminate two operators: given a black box implementing a quantum
operator O, and the promise that the black box implements either operator O1
or operator O2, the goal is to decide whether O = O1 or O = O2. A speci-
ﬁcity of the quantum operator discrimination problem, which is not present in
the quantum state discrimination problem, is that we can choose an arbitrary
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 309–320, 2017.
DOI: 10.1007/978-3-319-62389-4 26

310
A. Kawachi et al.
input state on which the operator O is applied. Additionally, the operator O
can be applied more than once in various combinations (parallel, sequential,
or in any other scheme physically allowed). In contrast to quantum state dis-
crimination, it is known that for several classes of operators discrimination is
possible without error [1,6–9,12]. For example, any two unitary operators can
be discriminated without error by applying the operator in parallel one some
well-chosen entangled quantum state [1,7], or by applying the operator sequen-
tially on a non-entangled state [8]. Besides unitary operators, it is also known
that projective measurements can be discriminated without error [12]. Necessary
and suﬃcient conditions for discriminating trace preserving completely positive
(TPCP) maps without errors are given in [9] as well.
Many computational problems solved by quantum algorithms can be recasted
as discrimination problems. Here the quantum operator given as input typically
implements a classical operation on the basis of the corresponding Hilbert space
and the goal is to (possibly partially) identify which classical operation it imple-
ments. Such problems generalize Grover’s original quantum search problem [11]
and have been studied under the name of the oracle identiﬁcation problem [2,13].
For instance, Grover’s algorithm for search solves the following problem: given
an oracle Ux corresponding to an unknown string x ∈{0, 1}n that maps any
quantum basis state |i⟩|b⟩, with i ∈{1, . . . , n} and b ∈{0, 1}, to the quantum
state |i⟩|b ⊕xi⟩, determine if x contains at least one non-zero coordinate. These
problems have been mainly considered in the query complexity setting, where
the complexity is deﬁned as the number of calls of the operator Ux used by the
algorithm. Upper and lower bounds on the query complexity of several oracle
identiﬁcation problems have been obtained [2,13].
Quantum operator discrimination problems relevant to quantum information
theory, on the other hand, have not yet been the subject of much study in the
framework of quantum query complexity.
Our Results. In this paper we investigate the query complexity of quantum oper-
ator discrimination problem for general quantum unitary operators (i.e., quan-
tum unitary operators not necessarily corresponding to classical operations) in
bounded-error settings. More precisely, we consider the following problem: given
an unknown unitary operator U ∈{U1, U2}, where U is given as a (quantum)
black-box and both U1 and U2 are known unitary operators, determine whether
U = U1 or U = U2 correctly with bounded-error probability.
Our main contribution is a characterization of the query complexity of
this problem (i.e., the number of times the black-box U has to be applied to
solve the problem) in terms of a parameter θcover, which is deﬁned formally
later (Deﬁnition 8 in Sect. 3), representing the “closeness” between U1 and U2
by showing a tradeoﬀbetween the number of queries and success probability.
We show the following upper and lower bounds:
– there exists a quantum algorithm that makes

π
3θcover

queries and can cor-
rectly discriminates U1 from U2, for any unitary operators U1 and U2, with
probability 2/3 (Theorem 4);

Quantum Query Complexity of Unitary Operator Discrimination
311
– there exist unitary operators U1 and U2 such that any quantum algorithm
requires at least

2
3θcover

queries to discriminate U1 from U2 with probability
2/3 (Theorem 5).
We thus obtain a tight (up to possible constant factors) characterization of the
query complexity of unitary operator discrimination. Our upper bound is actu-
ally achieved by a quantum algorithm that makes only non-adaptive queries,
i.e., a quantum algorithm in which all queries to the black-box can be made in
parallel. On the other hand, our lower bound holds even for adaptive quantum
algorithms. Our results thus show that for the quantum unitary operator discrim-
ination problem making adaptive query cannot (signiﬁcantly) reduce the query
complexity of the algorithm. This contrasts with oracle identiﬁcation problems
such as quantum search, where adaptive queries are necessary for achieving the
speed-up exhibited by Grover’s algorithm.
Relation with Other Works. Note that Ac´ın actually showed the same upper
bound on the query complexity of the problem with non-adaptive queries [1].
Ac´ın provided a geometric interpretation for the statistical distinguishability of
unitary operators based on sophisticated metrics of Fubini-Study and Bures.
D’Ariano et al. brieﬂy noted a simpler geometric interpretation based on the
Euclidean metric on the complex plane for the same result [7]. Duan et al. also
showed the same upper bound with adaptive queries and a non-entangled input
state [8]. Our algorithm of the upper bound uses an entangled input state with
non-adaptive queries to analyze a tradeoﬀbetween the number of queries and
success probability for the upper bound from a geometric interpretation similar
to D’Ariano et al.’s. Duan et al. also showed the optimality of their lower bounds
to perfectly discrimate two unitary operators [8]. Our proof of the lower bound
additionally analyzes the necessary number of queries for imperfect discrimina-
tion in details by showing a tradeoﬀbetween the number of queries and success
probability.
Organization of the Paper. The organization of this paper is as follows. In Sect. 2,
we deﬁne formally the quantum state discrimination problem, the unitary oper-
ator discrimination problem, the error probability of a discriminator, and the
query complexity. In Sect. 3, we construct a discriminator for arbitrary unitary
operators U1 and U2, and give our upper bound on the query complexity by esti-
mating the error probability of this discriminator. Finally, in Sect. 4, we show
the lower bound on the query complexity of any quantum algorithm solving the
quantum unitary operator discrimination problem.
2
Preliminaries
First, we deﬁne the quantum state discrimination problem since the unitary oper-
ator discrimination problem can be reduced to the quantum state discrimination
problem.

312
A. Kawachi et al.
Deﬁnition 1 (Quantum state discrimination problem). The quantum
state discrimination problem is deﬁned as the problem of determining whether
an unknown state |φ⟩is |φ1⟩or |φ2⟩, where |φ⟩is given from a candidate set
S = {|φ1⟩, |φ2⟩} of arbitrary two quantum states. Below, we denote the quan-
tum state discrimination problem of S = {|φ1⟩, |φ2⟩} by QSDP ({|φ1⟩, |φ2⟩}).
Unless otherwise noted, |φ1⟩and |φ2⟩are given with probability 1/2.
Deﬁnition 2 (Error probability of QSDP ({|φ1⟩, |φ2⟩})).
For a quantum
state discriminator A, when |φ1⟩and |φ2⟩are given with probability 1/2, the
error probability perror of A is deﬁned as follows:
perror = 1
2 Pr [ A (|φ1⟩) = “ 2” ] + 1
2 Pr [ A (|φ2⟩) = “ 1” ] .
Namely, the Error probability perror is the expected value of the probability that
A mistakes |φ1⟩for |φ2⟩and |φ2⟩for |φ1⟩.
The error probability is characterized by the closeness between two quantum
states such as the trace distance and the ﬁdelity.
Deﬁnition 3 (Trace distance). Let ρ and σ be pure quantum states. The trace
distance d (ρ, σ) is deﬁned as d (ρ, σ) := 1
2 ∥ρ −σ∥tr , where ∥X∥tr := Tr |X| =
Tr
√
X†X.
Deﬁnition 4 (Fidelity). Let ρ = |ψ⟩⟨ψ| and σ = |φ⟩⟨φ| be pure quantum
states. The ﬁdelity F (ρ, σ) is deﬁned as F (ρ, σ) = |⟨ψ|φ⟩| . Since it is an
absolute value of inner product, it holds that 0 ≤F (ρ, σ) ≤1, F (ρ, σ) = 1 ⇔
|ψ⟩= |φ⟩, and F (ρ, σ) = 0 ⇔|ψ⟩⊥|φ⟩.
Also, the trace distance and the ﬁdelity satisfy the following relation.
Lemma 1. Let ρ and σ be pure quantum states. We then have d (ρ, σ) =

1 −F (ρ, σ)2.
Using Deﬁnitions 3, 4 and Lemma 1, the error probability perror can be char-
acterized as follows.
Theorem 1 ([16]). Suppose the quantum states ρ = |ψ⟩⟨ψ| and σ = |φ⟩⟨φ| are
given from the candidate set S = {ρ, σ} of pure quantum states with probability
1/2. Then there exists a discriminator A such that its error probability perror is
given as follows:
perror = 1
2

1 −

1
2ρ −1
2σ

tr

= 1
2 (1 −d (ρ, σ)) = 1
2

1 −

1 −F (ρ, σ)2

.
The error probability is zero if and only if two quantum states are orthogonal.
Next, we deﬁne the unitary operator discrimination problem.

Quantum Query Complexity of Unitary Operator Discrimination
313
Deﬁnition 5 (Unitary operator discrimination problem). The unitary
operator discrimination problem is deﬁned as the problem of determining whether
an unknown operator U is U1 or U2, where U is given from a candidate set
S = {U1, U2} of arbitrary two unitary operators. Below, we denote the unitary
operator discrimination problem of S = {U1, U2} by UODP ({U1, U2}). Unless
otherwise noted, U1 and U2 are given with probability 1/2.
Deﬁnition 6 (Error probability of UODP ({U1, U2})). For a unitary oper-
ator discrimination A, when U1 and U2 are given with probability 1/2, the error
probability pU
error of A is deﬁned as follows:
pU
error = 1
2 Pr

AU1 = “ 2”
	
+ 1
2 Pr

AU2 = “ 1”
	
.
Namely, the error probability pU
error is the expected value of the probability that
A mistakes U1 for U2 and U2 for U1.
Generally, the unitary operator discrimination problem can be reduced to the
quantum state discrimination problem by applying an unknown unitary operator
U to an arbitrary |φ⟩. An application of U to |φ⟩is called a query. Then, we
denote by


φU
q

the quantum state generated by q queries to U and unitary
operators {Vi}q
i=1, which are independent of U (however, possibly depend on
{U1, U2}), speciﬁed by A. It is given as follows:


φU
q

= Vq (U ⊗I) Vq−1 (U ⊗I) · · · V1 (U ⊗I) |φ⟩.
Here, |φ⟩and


φU
q

are quantum states of m + n qubits, Vi is a unitary operator
over m + n qubits for each i, and U and I are unitary operators over n qubits
and m qubits, respectively. Let ˆU is a sequence of unitary operators of A. Then
we have


φU
q

= ˆU |φ⟩. Also, we deﬁne ˆU1 and ˆU2 as unitary operators satisfying


φU1
q

= ˆU1 |φ⟩and


φU2
q

= ˆU2 |φ⟩, respectively.
The unitary operator discriminator that makes all the queries at once (regard-
less of the answers to other queries) is called a non-adaptive unitary operator
discriminator. Otherwise it is called an adaptive unitary operator discriminator.
The error probability of a unitary operator discriminator is given as follows.
Theorem 2. For unitary operators ˆU1 and ˆU2, let U ′ = V † ˆU †
1 ˆU2V , where U ′ is
a diagonal matrix, and let |φ′⟩= V † |φ⟩. When the probability given U1 and U2
from the candidate set S = {U1, U2} is each probability 1/2, the error probability
of U1 and U2 is given as follows:
pU
error = 1
2

1 −

1 −min
|φ⟩
⟨φ| ˆU †
1 ˆU2 |φ⟩

2

= 1
2

1 −

1 −min
|φ′⟩|⟨φ′| U ′ |φ′⟩|2

.
This is shown immediately from Theorem 1.
Deﬁnition 7. We say a discriminator A makes q queries if A applies a uni-
tary operator U q times to the initial state |φ⟩in total. We say A solves

314
A. Kawachi et al.
UODP ({U1, U2}) with q queries if A correctly discriminates unitary opera-
tors U1 and U2 with probability at least 2/3 with q queries. The query com-
plexity of UODP ({U1, U2}) is the minimum number of queries where A solves
UODP ({U1, U2}).
3
Upper Bounds on Query Complexity
Let U1 and U2 be unitary operators acting on n qubits. Below we give a construc-
tion of a non-adaptive q-query discriminator for UODP ({U1, U2}); see Fig. 1 for
its schematic description.
Construction 3 (Non-adaptive q-query
discriminator A for UODP
({U1, U2}))
1. Generate an initial nq-bit quantum state |φ⟩that is determined by U1, U2
and q as in the proof of Lemma 2.
2. Apply U ⊗q to |φ⟩, where U ∈{U1, U2} is the unknown unitary operator.
3. Apply the quantum state discriminator for QSDP

U ⊗q
1
|φ⟩, U ⊗q
2
|φ⟩

, due
to Theorem 1, to the quantum state U ⊗q |φ⟩.
4. Output the result of the quantum state discriminator.
Fig. 1. Non-adaptive unitary operator discriminator A
To analyze the error probability of the non-adaptive q-query discriminator A,
let us introduce the notion of covering angle.
Deﬁnition 8 (Covering angle θcover). For real numbers α and 0 ≤β < 2π,
let Dα,β := {z ∈C : α ≤arg z ≤α + β}. Let eiθ1, . . . , eiθn be complex numbers
of absolute value 1, where 0 ≤θi < 2π, i = 1, . . . n. Then, the covering angle of
the set

eiθ1, . . . , eiθn
, denoted by θcover, is deﬁned as
θcover := min{β : ∃α s.t. eiθ1, . . . , eiθn ∈Dα,β}.

Quantum Query Complexity of Unitary Operator Discrimination
315
Fig. 2. Covering angle θcover
The covering angle θcover is formed by eiθk and eiθl if θl = θk + θcover and
eiθ1, . . . , eiθn ∈Dθk,θcover hold. See Fig. 2 for an illustrated example of a covering
angle.
In the rest of this section, we assume that the set of eigenvalues of the uni-
tary operator U †
1U2 is

eiθ1, . . . , eiθn
and the covering angle of it is θcover. The
following lemma is the main technical result of this section.
Lemma 2. The error probability pU
error of the non-adaptive q-query discrimina-
tor A is given as follows:
pU
error =
⎧
⎨
⎩
1
2

1 −sin qθcover
2

(0 ≤qθcover < π) ,
0
(π ≤qθcover) .
The lemma immediately implies the following:
Theorem 4. The non-adaptive q-query discriminator A solves UODP ({U1, U2})
if q ≥

π
3θcover

. Furthermore, the error probability of A is zero if q ≥

π
θcover

.
Proof (of Theorem 4). If q ≥

π
θcover

, then π ≤qθcover holds and the error
probability of A is zero by Lemma 2. For 0 ≤qθcover < π, again by Lemma 2, it
suﬃces to ﬁnd q such that
pU
error = 1
2

1 −sin qθcover
2

≤1
3
(1)
holds according to Deﬁnition 7. By calculation, we have that the inequality in
(1) holds if q ≥

π
3θcover

.
⊓⊔
It remains to prove Lemma 2. It is instructive to ﬁrst analyze a special case of
q = 1 as it captures the essence of general cases.
Lemma 3. The error probability of the non-adaptive 1-query discriminator A
is given as follows:
pU
error =
⎧
⎨
⎩
1
2

1 −sin θcover
2

(0 ≤θcover < π) ,
0
(π ≤θcover ≤2π) .

316
A. Kawachi et al.
Proof (of Lemma 3). We consider two cases according to the value of θcover.
Case (i) 0 ≤θcover < π.
Let U ′ = diag

eiθ1, . . . , eiθn
and |φ′⟩= (α1, . . . , αn). By Theorem 2, the min-
imum value of the square of the ﬁdelity of U1 |φ⟩and U2 |φ⟩is represented as
follows:
min
|φ⟩



⟨φ| U †
1U2 |φ⟩



2
= min
|φ′⟩|⟨φ′| U ′ |φ′⟩|2 =
min
n
j=1|αj|2=1






n

j=1
|αj|2 eiθj






2
.
The last term above is equal to the square of the shortest distance from the origin
of the complex plane to the convex set C :=
n
j=1 |αj|2 eiθj : n
j=1 |αj|2 = 1

.
The shortest distance from the origin of the complex plane to C is equal to
the shortest distance from the origin of the complex plane to the line segment
C′ :=

|αk|2 eiθk + |αl|2 eiθl : |αk|2 + |αl|2 = 1

, where eiθk and eiθl form the
covering angle θcover, see Fig. 3 for illustration. Thus, we have
min
n
j=1|αj|2=1






n

j=1
|αj|2 eiθj






2
=
min
|αk|2+|αl|2=1



|αk|2 eiθk + |αl|2 eiθl



2
.
The minimum of the right hand side above is achieved by setting |αk|2 = 1
2,
|αl|2 = 1
2. Hence
min
|αk|2+|αl|2=1



|αk|2 eiθk + |αl|2 eiθl



2
=
1
2


eiθk + eiθl

2
= cos2 θcover
2
.
Thus, pU
error of A is represented as follows:
pU
error = 1
2

1 −

1 −cos2 θcover
2

= 1
2

1 −sin θcover
2

.
Case (ii) π ≤θcover ≤2π.
The error probability can be calculated in the same way as Case (i). In this
case, the convex set C contains the origin of the complex plane, i.e., the shortest
distance from the origin to C is zero, hence we have pU
error = 0.
⊓⊔
We are prepared to prove Lemma 2.
Proof (of Lemma 2)
Case (i) 0 ≤qθcover ≤π.
Let U ′ = diag

eiθ1, . . . , eiθn
. Then the set of eigenvalues of U ′⊗q is
Λ =
⎧
⎨
⎩
q

j=1
eiθkj : 1 ≤k1, . . . , kq ≤n
⎫
⎬
⎭.

Quantum Query Complexity of Unitary Operator Discrimination
317
Note that if the covering angle θcover of {eiθ1, . . . , eiθn} is formed by eiθk and
eiθl, then the covering angle of Λ is qθcover and formed by eiqθk and eiqθl, as long
as q is not too large.
As the proof of Lemma 3, the minimum value of the square of the ﬁdelity of
U ⊗q
1
|φ⟩and U ⊗q
2
|φ⟩is
min
|φ⟩



⟨φ| U ⊗q
1
†U ⊗q
2
|φ⟩



2
= min
|φ′⟩


⟨φ′| U ′⊗q |φ′⟩


2
=
min
qn
j=1|αj|2=1



|α1|2 eiqθ1 + |α2|2 ei((q−1)θ1+θ2) + · · · + |αqn|2 eiqθn



2
.
This is the square of the shortest distance from the origin of the complex plane
to the convex set
Cq :=
⎧
⎨
⎩|α1|2 eiqθ1 + |α2|2 ei((q−1)θ1+θ2) + · · · + |αqn|2 eiqθn :
qn

j=1
|αj| = 1
⎫
⎬
⎭.
The shortest distance from the origin of the complex plane to Cq is equal to the
line segment
Cq
′ :=

|αx|2 eiqθk + |αy|2 eiqθl : |αx|2 + |αy|2 = 1

,
see Fig. 4 for illustration. Hence, in the same way as the proof of Lemma 3, we
have
pU
error = 1
2

1 −sin qθcover
2

(0 ≤qθcover ≤π) .
Case (ii) π ≤qθcover.
Since the convex set Cq contains the origin of the complex plane, the error
probability of A can be made zero as the case of q = 1.
⊓⊔
Fig. 3. The shortest distance to the con-
vex set C
Fig. 4. The shortest distance to the
convex set Cq

318
A. Kawachi et al.
4
Lower Bounds of Query Complexity
Every unitary operator discriminator can be represented as an adaptive discrim-
inator given in Fig. 5. We now evaluate the necessary number q of queries to
solve UODP ({U1, U2}) for some U1, U2, where ˆU in Fig. 5 is constructed as
ˆU := Vq (U ⊗I) Vq−1 (U ⊗I) · · · V1 (U ⊗I)
from a given U ∈{U1, U2} and ﬁxed unitary operators Vi (i = 1, 2, . . . , q).
The following theorem shows the necessary number of queries for some spe-
ciﬁc U1, U2.
Fig. 5. The arbitrary adaptive discriminator
Theorem 5. For every θ (0 < θ < π), we deﬁne the unitary operators U1 and
U2 as follows:
U1 =
⎛
⎜
⎜
⎜
⎝
1
0
1
...
0
1
⎞
⎟
⎟
⎟
⎠,
U2 =
⎛
⎜
⎜
⎜
⎝
eiθ
0
1
...
0
1
⎞
⎟
⎟
⎟
⎠.
If A solves UODP ({U1, U2}) with adaptive q queries, q ≥
$ 2
3θ
%
holds.
Proof. Let |φ⟩be any initial state. Applying ˆU to the initial state, we obtain the
following states for U1 and U2 respectively:


φU1
q

= Vq (U1 ⊗I) Vq−1 (U1 ⊗I) · · · V1 (U1 ⊗I) |φ⟩


φU2
q

= Vq (U2 ⊗I) Vq−1 (U2 ⊗I) · · · V1 (U2 ⊗I) |φ⟩.
If the number of queries is small,


φU1
q

and


φU2
q

are close to each other. We
estimate the necessary number q of queries of A to discriminate them by analyz-
ing the distance between the two states. The trace distance of the two states can
be given as ∥ρq −σq∥tr, where ρq :=


φU2
q
&
φU2
q


, and σq :=


φU1
q
&
φU1
q


. Then,
we have
∥ρq −σq∥tr =
Vq (U2 ⊗I) ρq−1 (U2 ⊗I)† V †
q −Vq (U1 ⊗I) σq−1 (U1 ⊗I)† V †
q

tr
=
Vq (U2 ⊗I) ρq−1 (U2 ⊗I)† V †
q −Vqσq−1V †
q

tr .

Quantum Query Complexity of Unitary Operator Discrimination
319
Since unitary operators do not change the trace distance, we have
			Vq (U2 ⊗I) ρq−1 (U2 ⊗I)† V †
q −Vqσq−1V †
q
			
tr =
			(U2 ⊗I) ρq−1 (U2 ⊗I)† −σq−1
			
tr .
By the triangle inequality,
(U2 ⊗I) ρq−1 (U2 ⊗I)† −σq−1

tr
=
(U2 ⊗I) ρq−1 (U2 ⊗I)† −(U2 ⊗I) σq−1 (U2 ⊗I)† + (U2 ⊗I) σq−1 (U2 ⊗I)† −σq−1

tr
≤
(U2 ⊗I) ρq−1 (U2 ⊗I)† −(U2 ⊗I) σq−1 (U2 ⊗I)†
tr
+
(U2 ⊗I) σq−1 (U2 ⊗I)† −σq−1

tr
= ∥ρq−1 −σq−1∥tr +
(U2 ⊗I) σq−1 (U2 ⊗I)† −σq−1

tr .
By Deﬁnitions 3, 4 and Lemma 1,
(U2 ⊗I) σq−1 (U2 ⊗I)† −σq−1

tr is:
(U2 ⊗I) σq−1 (U2 ⊗I)† −σq−1

tr = 2d
'
(U2 ⊗I) σq−1 (U2 ⊗I)† , σq−1
(
= 2

1 −F
'
(U2 ⊗I) σq−1 (U2 ⊗I)† , σq−1
(2
= 2

1 −



)
φU1
q−1



 (U2 ⊗I)



φU1
q−1
*


2
≤2
+
,
,
-1 −min
φU1
q−1




)
φU1
q−1



 (U2 ⊗I)



φU1
q−1
*


2
= 2

1 −cos2 θ
2 = 2 sin θ
2.
Since sin θ ≤θ, we have 2 sin θ
2 ≤θ from the above equations. Therefore,
∥ρq −σq∥tr ≤∥ρq−1 −σq−1∥tr + θ.
If the number of queries is zero, we have ∥ρ0 −σ0∥tr = 0. After q queries, the
distance between the two states is at most qθ. If the number of queries is q,
the error probability of A is at least 1
2
'
1 −qθ
2
(
from Theorem 1. Thus, from
1
2
'
1 −qθ
2
(
<
1
3, we have q >
2
3θ. Since the number of queries is an integer,
q ≥
$ 2
3θ
%
holds.
⊓⊔
Acknowledgments. AK was partially supported by MEXT KAKENHI (24106009)
and JSPS KAKENHI (16H01705, 17K12640). ST was supported in part by MEXT
KAKENHI (24106003) and JSPS KAKENHI (26330011, 16H02782). FLG was par-
tially supported by MEXT KAKENHI (24106009) and JSPS KAKENHI (16H01705,
16H05853). The authors are grateful to Akihito Soeda for helpful discussions.

320
A. Kawachi et al.
References
1. Ac´ın, A.: Statistical distinguishability between unitary operations. Phys. Rev. Lett.
87(17), 177901 (2001)
2. Ambainis, A., Iwama, K., Kawachi, A., Raymond, R., Yamashita, S.: Improved
algorithms for quantum identiﬁcation of boolean oracles. Theoret. Comput. Sci.
378(1), 41–53 (2007)
3. Audenaert, K.M.R., Calsamiglia, J., Masanes, L., Mu˜noz-Tapia, R., Ac´ın, A.,
Bagan, E., Verstraete, F.: The quantum Chernoﬀbound. Phys. Rev. Lett. 98(16),
160501 (2007)
4. Cheﬂes, A.: Unambiguous discrimination between linearly independent quantum
states. Phys. Lett. A 239(6), 339–347 (1998)
5. Cheﬂes, A., Kitagawa, A., Takeoka, M., Sasaki, M., Twamley, J.: Unambiguous
discrimination among oracle operators. J. Phys. A: Math. Theor. 40(33), 10183
(2007)
6. Childs, A.M., Preskill, J., Renes, J.: Quantum information and precision measure-
ment. J. Mod. Opt. 47, 155–176 (2000)
7. D’Ariano, G.M., Lo Presti, P., Paris, M.G.A.: Using entanglement improves the
precision of quantum measurements. Phys. Rev. Lett. 87(27), 270404 (2001)
8. Duan, R., Feng, Y., Ying, M.: Entanglement is not necessary for perfect discrimi-
nation between unitary operations. Phys. Rev. Lett. 98, 100503 (2007)
9. Duan, R., Feng, Y., Ying, M.: The perfect distinguishability of quantum operations.
Phys. Rev. Lett. 103, 210501 (2009)
10. Feng, Y., Duan, R., Ying, M.: Unambiguous discrimination between quantum
mixed states. Phys. Rev. A 70, 012308 (2004)
11. Grover, L.K.: Quantum mechanics helps in searching for a needle in a haystack.
Phys. Rev. Lett. 79, 325 (1997)
12. Ji, Z., Feng, Y., Duan, R., Ying, M.: Identiﬁcation and distance measures of mea-
surement apparatus. Phys. Rev. Lett. 96, 200401 (2006)
13. Kothari, R.: An optimal quantum algorithm for the oracle identiﬁcation problem.
Proceedings of the 31st International Symposium on Theoretical Aspects of Com-
puter Science (STACS 2014), Leibniz International Proceedings in Informatics, vol.
25, pp. 482–493 (2014)
14. Mochon, C.: Family of generalized “pretty good” measurements and the minimal-
error pure-state discrimination problems for which they are optimal. Phys. Rev. A
73, 012308 (2006)
15. Sacchi, M.F.: Optimal discrimination of quantum operations. Phys. Rev. A 71,
062340 (2005)
16. Ziman, M., Sedl´ak, M.: Single-shot discrimination of quantum unitary processes.
J. Mod. Opt. 57(3), 253–259 (2010)

Randomized Incremental Construction
for the HausdorﬀVoronoi Diagram Revisited
and Extended
Elena Khramtcova1 and Evanthia Papadopoulou2(B)
1 Computer Science Department,
Universit´e Libre de Bruxelles (ULB), Brussels, Belgium
2 Faculty of Informatics,
Universit`a Della Svizzera Italiana (USI), Lugano, Switzerland
evanthia.papadopoulou@usi.ch
Abstract. The HausdorﬀVoronoi diagram of clusters of points in the
plane is a generalization of Voronoi diagrams based on the Hausdorﬀ
distance function. Its combinatorial complexity is O(n + m), where n is
the total number of points and m is the number of crossings between
the input clusters (m = O(n2)); the number of clusters is k. We
present eﬃcient algorithms to construct this diagram via the random-
ized incremental construction (RIC) framework [Clarkson et al. 89,93].
For non-crossing clusters (m = 0), our algorithm runs in expected
O(n log n+k log n log k) time and deterministic O(n) space. For arbitrary
clusters the algorithm runs in expected O((m + n log k) log n) time and
O(m+n log k) space. The two algorithms can be combined in a crossing-
oblivious scheme within the same bounds. We show how to apply the RIC
framework eﬃciently to handle non-standard characteristics of general-
ized Voronoi diagrams, including sites (and bisectors) of non-constant
complexity, sites that are not enclosed in their Voronoi regions, empty
Voronoi regions, and ﬁnally, disconnected bisectors and Voronoi regions.
The diagram ﬁnds direct applications in VLSI CAD.
1
Introduction
The Voronoi diagram is a powerful geometric partitioning structure [2] with
diverse applications in science and engineering. We consider the Hausdorﬀ
Voronoi diagram of clusters of points in the plane, a generalization of Voronoi
diagrams based on the Hausdorﬀdistance function, which ﬁnds applications in
predicting (and evaluating) faults in VLSI design and other geometric networks.
Research supported in part by the Swiss National Science Foundation, projects SNF
20GG21-134355 (ESF EUROCORES EuroGIGA/VORONOI) and SNF 200021E-
154387. E. K. was also supported partially by F.R.S.-FNRS and the SNF P2TIP2-
168563 under the Early PostDoc Mobility program.
Research performed mainly while E. K. was at the Universit`a della Svizzera italiana
(USI).
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 321–332, 2017.
DOI: 10.1007/978-3-319-62389-4 27

322
E. Khramtcova and E. Papadopoulou
Given a family F of clusters of points in the plane, the HausdorﬀVoronoi dia-
gram of F is a subdivision of the plane into maximal regions such that all points
within one region have the same nearest cluster (see Fig. 1a). Distance between
a point t ∈R2 and a cluster P ∈F is measured as their Hausdorﬀdistance,
which equals the farthest distance between t and P, df(t, P) = maxp∈P d(t, p),
where d(·, ·) denotes the Euclidean distance1 between two points in the plane.
The total number of points is n and the number of clusters is k. No two clusters
in F share a common point.
Informally, the HausdorﬀVoronoi diagram is a min-max type of diagram.
The opposite max-min type, where distance is minimum and the diagram is
farthest, is also of interest, see e.g., [1,7,13]. Recently both types of diagrams
were combined to determine stabbing circles for sets of segments in the plane [10].
We remark that the Hausdorﬀdiagram is diﬀerent in nature from the clustering
induced Voronoi diagram by Chen et al. [6], where sites can be all subsets of
points and the inﬂuence function reﬂects a collective eﬀect of all points in a site.
The HausdorﬀVoronoi diagram ﬁnds direct applications in Very Large Scale
Integration (VLSI) circuit design. It can be used to model the location of defects
falling over parts of a network that have been embedded in the plane, destroying
its connectivity. It has been extensively used by the semiconductor industry to
estimate the critical area of a VLSI layout for various types of open faults, see
e.g., [4,16]. Critical area is a measure reﬂecting the sensitivity of a design to
random manufacturing defects. The diagram can ﬁnd applications in geometric
networks embedded in the plane, such as transportation networks, where critical
area may need to be extracted for the purpose of ﬂow control and disaster
avoidance.
Previous work. The HausdorﬀVoronoi diagram was ﬁrst considered by Edels-
brunner et al. [12] under the name cluster Voronoi diagram. The authors showed
that its combinatorial complexity is O(n2α(n)), and gave a divide and con-
quer construction algorithm of the same time complexity, where α(n) is the
inverse Ackermann function. These bounds were later improved to O(n2) by
Papadopoulou and Lee [18]. When the convex hulls of the clusters are disjoint [12]
or non-crossing [18] (see Deﬁnition 1), the combinatorial complexity of the dia-
gram is O(n). The O(n2)-time algorithm of Edelsbrunner et al. exploits the
equivalence of the Hausdorﬀdiagram of k clusters to the upper envelope of a
family of k lower envelopes in an arrangement of planes in R3, and it is opti-
mal in the worst case. However, it remains quadratic even if the diagram has
complexity O(n).
Deﬁnition 1. Two clusters P and Q are called non-crossing, if the convex hull
of P ∪Q admits at most two supporting segments with one endpoint in P and one
endpoint in Q. In the convex hull of P ∪Q admits more than two such supporting
segments, then P and Q are called crossing (see Fig. 1b).
1 Other metrics, such as the Lp metric, are also possible.

RIC for HVD Revisited and Extended
323
The combinatorial complexity of the HausdorﬀVoronoi diagram is O(n+m),
where m is the number of crossings between pairs of crossing clusters (see Def-
inition 2), and this is tight [17]. The number of crossings m is upper-bounded
by the number of supporting segments between pairs of crossing clusters. In
the worst case, m is O(n2). Computing the HausdorﬀVoronoi diagram in sub-
quadratic time when m is O(n) (even if m = 0) has not been an easy task. For
non-crossing clusters (m = 0), the HausdorﬀVoronoi diagram is an instance
of abstract Voronoi diagrams [14]. But a bisector can have complexity Θ(n),
thus, if we directly apply the randomized incremental construction for abstract
Voronoi diagrams [15] we get an O(n2 log n)-time algorithm, and this is not easy
to overcome (see [5]). When clusters are crossing, their bisectors are disconnected
curves [18], and thus, they do not satisfy the basic axioms of abstract diagrams.
For non-crossing clusters, Dehne et al. [11] gave the ﬁrst subquadratic-time
algorithm to compute the Hausdorﬀdiagram, in time O(n log4 n) and space
O(n log2 n). Recently, we gave a randomized incremental algorithm as based on
point location in a hierarchical dynamic data structure [5]. The expected running
time of this algorithm is O(n log n log k) and the expected space complexity is
O(n) [5]. However, it does not easily generalize to crossing clusters.
Fig. 1. (a) The HausdorﬀVoronoi diagram of a family of four clusters, where each
cluster has three points. (b) A pair of clusters. Above: clusters are non-crossing; below:
clusters are crossing. (c) Two crossing clusters P and Q (ﬁlled disks and squares, resp.),
and their HausdorﬀVoronoi diagram (black lines). The region of P is disconnected into
three faces. A crossing mixed vertex v, the circle passing through the three points that
induce v (dotted lines), and two diagonals of P and Q related to v (bold, grey).
Our Contribution. In this paper we revisit the randomized incremental construc-
tion for the Hausdorﬀdiagram obtaining three new results that complete our
investigation on eﬃcient construction algorithms for this diagram, especially in
the setting driven by our application, where crossings may be present but their
number is expected to be small (typically, m = O(n)). We follow the randomized
incremental construction (RIC) framework introduced by Clarkson et al. [8,9]. We
show how to eﬃciently apply this framework to construct a generalized Voronoi
diagram in the presence of several non-standard features: (1) bisectors can each
have complexity Θ(n); (2) sites need not be enclosed in their Voronoi regions;
(3) Voronoi regions can be empty; and (4) bisector curves may be disconnected.
Note that a direct application of the framework would yield an O(n2 log n) (or
O(kn log n)) algorithm, even for a diagram of complexity O(n).

324
E. Khramtcova and E. Papadopoulou
First, we consider non-crossing clusters, when the complexity of the dia-
gram is O(n). Our algorithm runs in expected O(n log n+k log n log k) time and
deterministic O(n) space. In comparison to [5], the construction is considerably
simpler and it slightly improves its time complexity; more importantly, it extends
to arbitrary clusters, unlike [5]. We give the construction for both a conﬂict and
a history graph, where the latter is an on-line variation of the algorithm.
Then, we consider arbitrary clusters of points. Allowing clusters to cross adds
an entire new challenge to the construction algorithm: a bisector between two
clusters consists of multiple components, and one Voronoi region may disconnect
in several faces. We show how to overcome this challenge on a conﬂict graph and
derive an algorithm whose expected time and space requirements are respectively
O(m log n + n log k log n) and O(m + n log k). To the best of our knowledge, this
is the ﬁrst time the RIC framework is applied to the construction of a Voronoi
diagram with disconnected bisectors and disconnected regions.
Finally, we address the question of deciding which algorithm to use on a
given input, without a prior knowledge on the existence of crossings. Deciding
whether the input clusters are crossing or not, may require quadratic time by
itself, because the convex hulls of the input clusters may have quadratic inter-
sections, even if the clusters are non-crossing. In Sect. 5, we show how to only
detect crossings which are relevant to the construction of the HausdorﬀVoronoi
diagram, and thus, provide a crossing-oblivious algorithm that combines our two
previous algorithms, while keeping intact the time complexity bounds.
2
Preliminaries
Let F be a family of k clusters of points in the plane, and let n be the total
number of points in F. No two clusters share a point. We assume that each cluster
equals the vertices on its convex hull, as only points on a convex hull may have
non-empty regions in the Hausdorﬀdiagram. For simplicity of presentation, we
follow a general position assumption that no four points lie on the same circle.
The farthest Voronoi diagram of a cluster C, for brevity FVD(C), is a par-
titioning of the plane into regions, where the farthest Voronoi region of a point
c ∈C is fregC(c) = {t | ∀c′ ∈C \ {c}: d(t, c) > d(t, c′)}. Let T (C) denote the
graph structure of FVD(C), T (C) = R2 \ 
c∈C fregC(c). T (C) is well known to
be a tree. If C = {c}, let T (C) = c. If |C| > 1, we assume that T (C) is rooted
at a point at inﬁnity on an unbounded Voronoi edge.
The HausdorﬀVoronoi diagram, for brevity HVD(F), is a partitioning of the
plane into regions, where the HausdorﬀVoronoi region of a cluster C ∈F is
hregF (C) = {p | ∀C′ ∈F \ {C}: df(p, C) < df(p, C′)}. Region hregF (C) is fur-
ther subdivided into subregions by FVD(C). In particular, the HausdorﬀVoronoi
region of a point c ∈C is hregF (c) = hregF (C) ∩fregC(c). Fig. 1a illustrates the
HausdorﬀVoronoi diagram of a family of four clusters. The convex hulls of the
clusters are shown in grey lines. Solid black lines indicate the HausdorﬀVoronoi
edges bounding the Voronoi regions of individual clusters; the dashed lines indi-
cate the ﬁner subdivision by the farthest Voronoi diagram of each cluster.

RIC for HVD Revisited and Extended
325
The HausdorﬀVoronoi edges are portions of Hausdorﬀbisectors between
pairs of clusters; the bisector of two clusters P, Q ∈F is bh(P, Q) = {y |
df(y, P) = df(y, Q)}. See the solid black lines in Fig. 1c. The Hausdorﬀbisector
is a subgraph of T (P ∪Q); it consists of one (if P, Q are non-crossing) or more
(if P, Q are crossing) unbounded polygonal chains [18]. In Fig. 1c the Hausdorﬀ
bisector has three such chains. Each vertex of bh(P, Q) is the center of a circle
passing through two points of one cluster and one point of another that entirely
encloses P and Q, see e.g. the dotted circle around vertex v in Fig. 1c.
Deﬁnition 2. A vertex on the bisector bh(C, P), induced by two points ci, cj ∈C
and a point pl ∈P, is called crossing, if there is a diagonal plpr of P that crosses
the diagonal cicj of C, and all points ci, cj, pl, pr are on the convex hull of C ∪P.
(See vertex v in Fig. 1c.) The total number of crossing vertices along the bisectors
of all pairs of clusters is the number of crossings and is denoted by m.
The HausdorﬀVoronoi diagram contains three types of vertices [17] (see
Figs. 1a and c): (1) pure Voronoi vertices, equidistant to three clusters; (2) mixed
Voronoi vertices, equidistant to three points of two clusters; and (3) farthest
Voronoi vertices, equidistant to three points of one cluster. The mixed vertices,
which are induced by two points of cluster C (and one point of another cluster),
are called C-mixed vertices; they are incident to edges of FVD(C). The Hausdorﬀ
Voronoi edges are polygonal lines (portions of Hausdorﬀbisectors) that connect
pure Voronoi vertices. Mixed Voronoi vertices are vertices of Hausdorﬀbisectors.
They are characterized as crossing or non-crossing according to Deﬁnition 2. The
following property is crucial for our algorithms.
Lemma 1 [17].
Each face of a (non-empty) region hregF (C) intersects T (C)
in one non-empty connected component. The intersection points delimiting this
component are C-mixed vertices.
Unless stated otherwise, we use a reﬁnement of the HausdorﬀVoronoi dia-
gram as derived by the visibility decomposition [18] of each region hregF (p) (see
Fig. 2a): for each vertex v on the boundary of hregF (p) draw the line segment pv,
as restricted within hregF (p). In Fig. 2a, the edges of the visibility decomposition
in hregF (p) are shown in bold. Each face f within hregF (p) is convex; point p is
called the owner of f.
Observation 1. A face f of hregF (p), p ∈P, borders the regions of O(1) (at
most 3) other clusters in F \ {P}.
Overview of the RIC framework [8,9]. The randomized incremental construction
framework to compute a Voronoi diagram, inserts sites (or objects) one by one
in random order, each time recomputing the target diagram. The diagram is
viewed as a collection of ranges, deﬁned and without conﬂicts with respect to
the set of sites inserted so far. To update the diagram eﬃciently, a conﬂict or
history graph is maintained. An important prerequisite for using the framework
is: each range must be deﬁned by a constant number of objects.

326
E. Khramtcova and E. Papadopoulou
The conﬂict graph is a bipartite graph where one group of nodes correspond
to the ranges of the diagram of sites inserted so far, and the other group corre-
spond to sites that are not yet inserted. In the conﬂict graph, a range and a site
are connected by an arc if and only if they are in conﬂict. A RIC algorithm that
uses a conﬂict graph is eﬃcient if the following update condition is satisﬁed at
each incremental step: (1) Updating the set of ranges deﬁned and without con-
ﬂicts over the current subset of objects requires time proportional to the number
of ranges killed or created during this step; and (2) Updating the conﬂict graph
requires time proportional to the number of arcs of the conﬂict graph added or
removed during this step.
The expected time and space complexity for a RIC algorithm is as stated
in [3, Theorem 5.2.3]: Let f0(r) be the expected number of ranges in the target
diagram of a random sample of r objects, and let k be the number of insertion
steps of the algorithm. Then (1) Expected number of ranges created during the
algorithm is O
 k
r=1
(f0(r)/r)

. (2) If the update condition holds, the expected
time and space of the algorithm is O

k
k
r=1
(f0(r)/r2)

.
3
Constructing HVD(F ) for Non-crossing Clusters
Let the clusters in F be pairwise non-crossing. Then the Voronoi regions are
connected and the combinatorial complexity of the Hausdorﬀdiagram is O(n).
Fig. 2. Left: (a) Visibility decomposition of the diagram. Right: Insertion of a cluster
C (unﬁlled disks): (b) T (C) (dashed) rooted at r, its active subtree (bold) rooted at
x. (c) After the insertion: x is a C-mixed vertex of the HVD.
Let S ⊂F and C ∈F \S. Suppose that HVD(S) has been computed; our goal
is to insert C and obtain HVD(S ∪{C}). We introduce the following deﬁnition.
Consider hregS∪{C}(C). As we traverse T (C) starting at its root, let x be the
ﬁrst point we encounter in the closure of hregS∪{C}(C). We refer to the subtree
of T (C) rooted at x as the active subtree of T (C), and denote it by Ta(C, S).
In Fig. 2b, Ta(C, S) is shown in bold dashed lines superimposed on HVD(S).
Figure 2c illustrates HVD(S ∪{C}).

RIC for HVD Revisited and Extended
327
By Lemma 1, since Voronoi regions of non-crossing clusters are connected,
hregS∪{C}(C) ̸= ∅if and only if Ta(C, S) ̸= ∅. Further, the root of Ta(C, S) is a
C-mixed vertex of HVD(S ∪{C}), unless Ta(C, S) = T (C).
3.1
Objects, Ranges and Conﬂicts
We formulate the problem of computing HVD(F) in terms of objects, ranges and
conﬂicts. Objects are the clusters in F. Ranges are the faces of HVD(S) as reﬁned
by the visibility decomposition (see Sect. 2 and Fig. 1c). A range corresponding
to face f, f ⊂hregS(p), where p ∈P, is said to be deﬁned by the cluster P and by
the remaining clusters in S whose Voronoi regions border f. By Observation 1,
there are at most three such clusters, and thus, we can apply the RIC framework.
Point p is called the owner of range f, f ⊂hregS(p).
Deﬁnition 3 (Conﬂict for non-crossing clusters). A range f is in conﬂict
with a cluster C ∈F \ S if Ta(C, S) is not empty and its root x lies in f. A
conﬂict is a triple (f, x, C). The list of conﬂicts of range f is denoted as L(f).
The following is implied by Lemma 1 and it is the basis of our algorithm.
Lemma 2. Each cluster C ∈F \ S has at most one conﬂict. If a cluster C has
no conﬂicts, then hregS∪{C}(C) = ∅, thus, hregF (C) = ∅.
Due to space constraints, we only present here the variant of the algorithm
that uses the conﬂict graph. It is not hard to adapt it to use a history graph
using the same deﬁnitions of ranges and conﬂicts.
3.2
Insertion of a Cluster
Suppose that HVD(S), S ⊂F, and the conﬂict graph have been constructed.
Let C ∈F \ S. Using the conﬂict of C, we can easily compute hregS∪{C}(C).
Starting at the root of Ta(C, S) (stored with the conﬂict) trace the boundary of
the region as in [18]. The remaining problem is to identify conﬂicts for the new
ranges of hregS∪{C}(C) and to update the conﬂict graph. The algorithm is given
in Fig. 3 as pseudocode.
To identify new conﬂicts we use the information of conﬂicts stored with the
ranges that get deleted. Let f be a deleted range of owner p (f ⊂hregS(p)). For
each conﬂict (f, y, Q), we compute the new root of Ta(Q, S ∪{C}), if diﬀerent
from Ta(Q, S), and identify the new range that contains it. To compute the root
of Ta(Q, S ∪{C}), it is enough to traverse Ta(Q, S), searching for an edge uv
that contains a point equidistant from Q and C (see Line 12). If such an edge
uv exists, we compute the point equidistant from C and Q on uv (a Q-mixed
vertex) by performing a segment query [5] for uv in FVD(C). If no such edge
exists then Ta(Q, S ∪{C}) = ∅, and no conﬂicts for Q should be created.
Given a segment uv of T (C), such that u is closer to C then to Q, and v is
closer to Q than to C, the segment query ﬁnds the unique point on uv, which
is equidistant to Q and C. This query can be answered in O(log |P|) time using
the centroid decomposition of FVD(P), as detailed in [5].
The remaining algorithm is straightforward (see Fig. 3).

328
E. Khramtcova and E. Papadopoulou
Algorithm Insert-NonCrossing
(∗Inserts a cluster C in HVD(S); updates the conﬂict graph ∗)
1.
Let (g, x, C) be the conﬂict of C.
2.
Compute hregS∪{C}(C) by tracing its boundary starting at x.
3.
Update HVD(S) to HVD(S ∪{C}).
4.
for f ∈HVD(S) \ HVD(S ∪{C}) do
5.
Let p be the owner of f (f ⊂hregS(p)).
6.
for each conﬂict (f, y, Q) ∈L(f) do
7.
Discard the conﬂict (f, y, Q).
8.
if d(y, p) < df(y, C) then
9.
Locate the range
f
⊂
hregS∪{C}(p) that contains
y by binary search in the visibility decomposition of
hregS∪{C}(p).
10.
Create conﬂict (f , y, Q).
11.
else
12.
Search
for
an
edge
uv
in
Ta(Q, S)
such
that
df(u, Q) > df(u, C) and df(v, Q) < df(v, C).
13.
if uv is found then
14.
Find root z of Ta(Q, S ∪{C}) by a segment query for
uv in FVD(C).
15.
Find point c ∈C such that z ∈hregS∪{C}(c) by a
point location in FVD(C).
16.
Locate the range h ⊂hregS∪{C}(c) that contains z
by a binary seacrh in the visibility decomposition of
hregS∪{C}(c).
17.
Create conﬂict (h, z, Q).
Fig. 3. Algorithm to insert cluster C; case of non-crossing clusters.
Theorem 1. The HausdorﬀVoronoi diagram of k non-crossing clusters of total
complexity n can be computed in expected O(n log n+k log n log k) time and O(n)
(deterministic) space.
Proof (Sketch). Let {C1, . . . , Ck} be a random permutation of F, {F0, . . . , Fk}
be such that F0 = ∅, Fi = Fi−1 ∪{Ci} for 1 ≤i ≤k −1, and Fk = F. At step i
we insert cluster Ci by the algorithm in Fig. 3.
By Lemma 2, at any step i the conﬂict graph has O(k) arcs; the complexity
of HVD(Fi) is O(n). Thus the space required by the algorithm is O(n).
The expected total number of ranges created and deleted during the algo-
rithm is O(n) [5]. Updating the diagram can be done in time proportional to
this number times O(log n) [18]. Updating the conﬂict graph at step i requires
O(log n(Ri+Ni)) time, where Ri is the number of conﬂicts deleted at step i, and
Ni is the number of edges dropped out of the active subtrees at step i. Since an
edge is dropped out at most once, and there is O(n) edges in total, the claimed
time bound follows by applying the Clarkson-Shor analysis (see Sect. 2). ⊓⊔
4
Computing HVD(F ) for Arbitrary Clusters of Points
In this section we drop the assumption that clusters are pairwise non-crossing.
This raises a major diﬃculty that Voronoi regions may be disconnected and

RIC for HVD Revisited and Extended
329
Hausdorﬀbisectors may consist of more than one polygonal curve. The deﬁnition
of conﬂict from Sect. 3.1 no longer guarantees a correct diagram. For a region
r ⊂R2, its boundary and its closure are denoted, respectively, ∂r and r.
Let C1, . . . , Ck be a random permutation of clusters in F. We incrementally
compute HVD(Fi), i = 1, . . . , k, where Fi = {C1, C2, . . . , Ci}. At each step i,
cluster Ci is inserted in HVD(Fi−1). We maintain the conﬂict graph (see Sect. 2)
between the ranges of HVD(Fi) and the clusters in F \Fi. Like in Sect. 3, ranges
correspond to faces of HVD(Fi) as partitioned by the visibility decomposition.
To correctly insert cluster Ci in HVD(Fi−1), one must know at least one
point in each face of the (new) region of Ci in HVD(Fi) = HVD(Fi−1 ∪{Ci}).
By Lemma 1, it is suﬃcient to maintain information on the Q-mixed vertices of
HVD(Fi∪{Q}), for every Q ∈F \Fi. But to apply the Clarkson-Shor analysis we
need the ability to determine the new conﬂicts from the conﬂicts of ranges that
get deleted. Due to this requirement, it is essential that we maintain not only
conﬂicts between Q and the ranges of HVD(Fi) that contain Q-mixed vertices
in HVD(Fi ∪{Q}), as Lemma 1 suggests, but also conﬂicts between Q and every
range that intersects the boundary of hregFi∪{Q}(Q).
Let f be a range of HVD(Fi), f ⊂hregFi(p), where p ∈P and P ∈Fi. Let
Q be a cluster in F \ Fi−1, We deﬁne a conﬂict between f and Q:
Deﬁnition 4 (Conﬂict for arbitrary clusters). Range f is in conﬂict with
cluster Q, if f intersects the boundary of hregFi∪{Q}(Q). The vertex list of a
conﬂict V (f, Q) is the list of all vertices and all endpoints of bh(P, Q)∩f, ordered
in clockwise angular order around p.
Observation 2 bh(P, Q) ∩f = bh(p, Q) ∩f. The order of vertices in V (f, Q)
coincides with the natural order of vertices along bh(p, Q).
Note bh(p, Q) is a single convex chain [18], unlike bh(P, Q). Figure 4 shows
bisector bh(P, Q) and a range f intersected by it. ∂f consists of four parts: the
top side is a portion of a pure edge of HVD(Fi); the bottom chain, shown in bold,
is a portion of T (P); the two sides are edges of the visibility decomposition.
p
bh(P, Q)
∂f
Fig. 4. A range f
⊂
hregFi(p)
(shaded); Bisector bh(P, Q), Q ∈
F \Fi; vertices of bh(P, Q) (unﬁlled
circle marks).
Fig. 5. An old range f
⊂
hregFi−1(p);
New ranges of hregFi(p) derived from f
(bounded by light solid lines). bh(P, Q) ∩f
where Q is a cluster in conﬂict with f.

330
E. Khramtcova and E. Papadopoulou
4.1
Inserting a Cluster
Insert Ci into HVD(Fi−1). We compute all faces of hregFi(Ci) by tracing their
boundary, starting at the vertices in the vertex lists of the conﬂicts of Ci. The
insertion of hregFi(Ci) results in deleting some old ranges and inserting some
new ones. We have two types of new ranges: type (1): ranges in hregFi(Ci); and
type (2): updated ranges of clusters in Fi−1.
Update the conﬂict graph. For each cluster Q ∈F \ Fi in conﬂict with a deleted
range, compute the conﬂicts of Q with the new ranges of type (1) and (2).
Ranges of type (1). Consider the ranges in hregFi(Ci). We follow the bisector
bh(Q, Ci) within hregFi(Ci), while computing it on the ﬂy. For each face fi ⊂
hregFi(Ci) that we encounter as we walk on bh(Q, Ci), we update the vertex list
V (fi, Q). To update V (fi, Q) we insert the vertices on the branch of bh(Q, Ci)∩fi
that was just encountered, including its endpoints on ∂fi. The insertion point
in V (fi, Q) can be determined by binary search. Note that bh(Q, Ci) ∩fi may
consist of several components, thus, fi can be encountered a number of times;
each time, we augment V (fi, Q) independently.
Ranges of type (2). Let f ⊂hregFi−1(p), p ∈P, be a deleted range that was
in conﬂict with Q. The vertex list V (f, Q) reveals bh(P, Q) ∩f. For each new
range f ′ of type (2) such that f ′ ⊂f and f ′ is in conﬂict with Q, we need to
compute V (f ′, Q). Observe, that the union of bh(P, Q) ∩f ′ for all such ranges
f ′ is (bh(P, Q) ∩f) \ hregFi(Ci). We call the maximal contiguous portions of
bh(P, Q)∩f, outside hregFi(Ci), the active parts of bh(P, Q)∩f. The non-active
parts of bh(P, Q) ∩f are its maximal contiguous portions inside hregFi(Ci).
Figure 5 shows the active and the non-active parts of bh(P, Q) ∩f by (red) bold
and dotted lines respectively. Note that one active (resp., non-active) part may
consist of multiple polygonal curves. A point incident to one active and one
non-active part is called a transition point, see e.g. point z in Fig. 5. Transition
points lie in bh(Q, Ci)∩∂hregFi(Ci); they are used as starting points to compute
the conﬂicts of ranges of type (1). Our task is to determine all active parts of
bh(P, Q) ∩f, their incident transition points, and to create conﬂicts induced by
these active parts.
We process active and non-active parts of bh(P, Q) ∩f sequentially.
– For a non-active part, we trace it in hregFi(Ci), simply to determine the tran-
sition point where the next active part begins.
– For an active part, we process sequentially the new ranges of HVD(Fi) inter-
sected by it. For each such range fj ⊂f, we compute V (fj, Q), given the
point x where the active part enters fj. Once we have the point z where it
exits fj, list V (fj, Q) is easily derived from the portion of bh(P, Q) between
x and z. To ﬁnd z, consider the rightmost ray r originating at p and passing
through ∂fj. If bh(P, Q) ∩f intersects r (see Fig. 5), let y be the point of this
intersection. Otherwise, we let t be the rightmost endpoint of bh(P, Q) ∩f to
the left of r. If t ∈T (P), set z = t, otherwise set y = t.

RIC for HVD Revisited and Extended
331
• If y ∈∂fj, then we set z = y. In this case the active part of bh(P, Q) ∩f
enters the next new range fk at point y. In Fig. 5, this case is illustrated
by point x that plays the role of z = y.
• If y ̸∈fj, we determine point z as the unique point on bh(P, Q), such that
z is between x and y, and z ∈∂fj. See Fig. 5. Point z is the endpoint of
the active part that we were processing, thus z is a transition point.
The following theorem states the complexity of our algorithm. The key ingre-
dient of the proof is charging the time required to update the conﬂict graph at
step i to the number of conﬂicts created and deleted at that step, the total size
of vertex lists of conﬂicts of the new ranges of type (1), and the vertices of the
vertex lists of deleted conﬂicts that do not appear in vertex lists of new conﬂicts.
Theorem 2. The HausdorﬀVoronoi diagram of a family F of k clusters of
total complexity n can be computed in O((m + n log k) log n) expected time and
O(m + n log k) expected space.
5
A Crossing-Oblivious Algorithm
Below we discuss how to compute HVD(F) if it is not known whether clusters
in F have crossings. Deciding fast whether F has crossings is not easy, as the
convex hulls of the clusters may have a quadratic total number of intersections,
even if the clusters are actually non-crossing.
Our algorithm combines the light algorithm for non-crossing clusters (Sect. 3)
with the heavy algorithm for arbitrary clusters (Sect. 4), and overcomes the
above issue, staying within best complexity bound (see Theorem 3). We start
with the algorithm of Sect. 3 and run it until we realize that the diagram cannot
be updated correctly. If this happens, we terminate the algorithm of Sect. 3,
and run the one of Sect. 4. In particular, after each insertion of a cluster Ci
we perform a check. A positive answer to this check guarantees that the region
of Ci in HVD(Fi) is connected, and thus, it is computed correctly. A negative
answer indicates that Ci has a crossing with some cluster which has already
been inserted in the diagram and its region may be disconnected. At the ﬁrst
negative check, we restart the computation of the diagram from scratch using
the algorithm of Sect. 4. We can aﬀord running the latter algorithm, since it is
certain that the input set of clusters has crossings.
The procedure of the check is based on properties established in [17], which
guarantee that if hregFi(Ci) is disconnected then each of its connected com-
ponents is incident to a crossing Ci-mixed vertex of HVD(Fi). It is possible in
O(|Ci| log n) to check whether all Ci-mixed vertices adjacent to a connected
component of hregFi(Ci) are non-crossing. We thus obtain the following.
Theorem 3. Let F be a family of k clusters of total complexity n. There is
an algorithm that computes HVD(F). If the clusters in F are non-crossing, the
algorithm requires O(n) space, and expected O(n log n+k log n log k) time. If the
clusters in F are crossing, the algorithm requires O((m + n log k) log n) expected
time and O(m+n log k) expected space, where m is the total number of crossings
between pairs of clusters in F.

332
E. Khramtcova and E. Papadopoulou
References
1. Abellanas, M., Hurtado, F., Icking, C., Klein, R., Langetepe, E., Ma, L., Palop,
B., Sacrist´an, V.: The farthest color Voronoi diagram and related problems. In:
17th European Workshop on Computational Geometry (EWCG), pp. 113–116,
full version: Technical Report 002 2006, Universit¨at Bonn (2006)
2. Aurenhammer, F., Klein, R., Lee, D.T.: Voronoi Diagrams and Delaunay Triangu-
lations. World Scientiﬁc, Singapore (2013)
3. Boissonnat, J.D., Yvinec, M.: Algorithmic Geometry. Cambridge University Press,
New York (1998)
4. Voronoi,
C.A.A.:
Voronoi
Critical
Area
Analysis.
IBM
VLSI
CAD
Tool,
IBM
Microelectronics
Division,
Burlington,
VT,
distributed
by
Cadence.
Patents: US6178539, US6317859, US7240306, US7752589, US7752580, US7143371,
US20090125852
5. Cheilaris, P., Khramtcova, E., Langerman, S., Papadopoulou, E.: A randomized
incremental algorithm for the HausdorﬀVoronoi diagram of non-crossing clusters.
Algorithmica 76(4), 935–960 (2016)
6. Chen, D.Z., Huang, Z., Liu, Y., Xu, J.: On clustering induced Voronoi diagrams. In:
2013 IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),
pp. 390–399. IEEE (2013)
7. Cheong, O., Everett, H., Glisse, M., Gudmundsson, J., Hornus, S., Lazard, S., Lee,
M., Na, H.S.: Farthest-polygon Voronoi diagrams. Comput. Geom. 44(4), 234–247
(2011)
8. Clarkson, K., Shor, P.: Applications of random sampling in computational geom-
etry, II. Discrete Comput. Geom. 4, 387–421 (1989)
9. Clarkson, K.L., Mehlhorn, K., Seidel, R.: Four results on randomized incremental
constructions. Comput. Geom. Theory Appl. 3(4), 185–212 (1993)
10. Claverol, M., Khramtcova, E., Papadopoulou, E., Saumell, M., Seara, C.: Stab-
bing circles for sets of segments in the plane. Algorithmica (2017). doi:10.1007/
s00453-017-0299-z
11. Dehne, F., Maheshwari, A., Taylor, R.: A coarse grained parallel algorithm for
HausdorﬀVoronoi diagrams. In 35th ICPP, pp. 497–504 (2006)
12. Edelsbrunner, H., Guibas, L., Sharir, M.: The upper envelope of piecewise linear
functions: algorithms and applications. Discrete Comput. Geom. 4, 311–336 (1989)
13. Huttenlocher, D.P., Kedem, K., Sharir, M.: The upper envelope of Voronoi surfaces
and its applications. Discrete Comput. Geom. 9, 267–291 (1993)
14. Klein, R.: Concrete and Abstract Voronoi diagrams. LNCS, vol. 400. Springer,
Heidelberg (1989)
15. Klein, R., Mehlhorn, K., Meiser, S.: Randomized incremental construction of
abstract Voronoi diagrams. Comput. Geom. 3(3), 157–184 (1993)
16. Papadopoulou, E.: Net-aware critical area extraction for opens in VLSI circuits via
higher-order Voronoi diagrams. IEEE Trans. CAD Integrated Circuits and Systems
30(5), 704–717 (2011)
17. Papadopoulou, E.: The HausdorﬀVoronoi diagram of point clusters in the plane.
Algorithmica 40(2), 63–82 (2004)
18. Papadopoulou, E., Lee, D.T.: The HausdorﬀVoronoi diagram of polygonal objects:
a divide and conquer approach. Int. J. Comput. Geom. Ap. 14(6), 421–452 (2004)

NP-completeness Results for Partitioning
a Graph into Total Dominating Sets
Mikko Koivisto1, Petteri Laakkonen2, and Juho Lauri2,3P(B)
1 University of Helsinki, Helsinki, Finland
mikko.koivisto@helsinki.fi
2 Tampere University of Technology, Tampere, Finland
{petteri.laakkonen,juho.lauri}@tut.fi
3 Bell Labs, Dublin, Ireland
juho.lauri@nokia.com
Abstract. A total domatic k-partition of a graph is a partition of its
vertex set into k subsets such that each intersects the open neighborhood
of each vertex. The maximum k for which a total domatic k-partition
exists is known as the total domatic number of a graph G, denoted by
dt(G). We extend considerably the known hardness results by showing
it is NP-complete to decide whether dt(G) ≥3 where G is a bipartite
planar graph of bounded maximum degree. Similarly, for every k ≥3, it
is NP-complete to decide whether dt(G) ≥k, where G is a split graph or
k-regular. In particular, these results complement recent combinatorial
results regarding dt(G) on some of these graph classes by showing that
the known results are, in a sense, best possible. Finally, for general n-
vertex graphs, we show the problem is solvable in 2nnO(1) time, and
derive even faster algorithms for special graph classes.
1
Introduction
Domination is undoubtedly one of the most intensively studied concepts in graph
theory. Besides being a fundamental graph property, domination also routinely
appears in real-world applications related to data transfer (see e.g., [9,16,31]).
Let G = (V, E) be a graph. A dominating set of G is a set of vertices S ⊆V
such that every vertex in V either is in S or is adjacent to a vertex in S. The
domination number of a graph G, denoted by γ(G), is the size of a minimum
dominating set in G. A classical variant of domination is the concept of total
domination, introduced by Cockayne et al. [7] in 1980. Here, a set of vertices
D ⊆V is a total dominating set of G if every vertex in V is adjacent to some
vertex in D. The total domination number of G, denoted by γt(G), is the size of a
minimum total dominating set in G. Note that every total dominating set is also
a dominating set, but the converse is not true in general. Given the centrality
of the topic, much is known about total domination in graphs. For instance,
Bollob´as and Cockayne [5] proved that γt(G) ≤2γ(G). For other combinatorial
results, we refer the reader to the books [17,18] and the recent monograph [22].
From the viewpoint of complexity, the problem of deciding whether a graph G
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 333–345, 2017.
DOI: 10.1007/978-3-319-62389-4 28

334
M. Koivisto et al.
Fig. 1. The graph G = C6 has dt(G) = 3.
has a total dominating set of size at most k is well-known to be NP-complete,
even for bipartite graphs [27]. The reader interested in computational results is
referred to the survey [20].
In this work, we focus on the problem of partitioning the vertex set of a graph
into a maximum number of total dominating sets, a concept also introduced by
Cockayne et al. [7]. For ease of presentation, we follow Zelinka [32] and use a
deﬁnition rooted in graph colorings. A total domatic k-coloring is a partition
of V into k color classes such that each vertex in V is adjacent to a member
from each class. The maximum k for which a graph G has a total domatic k-
coloring is known as the total domatic number of G, and is denoted by dt(G).
This is equivalent to the maximum number of total dominating sets the vertex
set of G can be partitioned into. For convenience, we can say a graph G is
total domatic k-colorable if dt(G) ≥k. It is immediate from the deﬁnition that
for any nontrivial graph G, we have that 1 ≤dt(G) ≤δ(G) ≤Δ(G), where
δ(G) and Δ(G) denote the minimum and maximum degree, respectively. For an
illustration of the concept, consider the graph G = C6, i.e., the complement of
a 6-cycle (see Fig. 1). As G is 3-regular, we have that dt(G) ≤3. On the other
hand, a total domatic 3-coloring of G is straightforward to construct proving
that dt(G) = 3.
Zelinka [33] proved that no minimum degree condition suﬃces to guarantee
dt(G) = 2. Later on, Heggernes and Telle [19] showed that deciding whether
dt(G) ≥2 is NP-complete for bipartite graphs. The study of total domatic
number has regained attention lately. Recently, motivated by communication
problems in large multi-robot networks, Chen et al. [6] reintroduced the concept
under the name coupon coloring. In particular, they called the total domatic
number the coupon coloring number. Subsequently, and seemingly unaware of
the earlier work by Zelinka [32–34], the coupon coloring number was studied by
Shi et al. [30]. In particular, Zelinka [32] determined the total domatic number
of cactus graphs, majorizing Shi’s et al. [30] result for cycles.
As further motivation, we mention a slight variation of an application in
network science described by Abbas et al. [1]. Imagine a network of agents
where each agent holds an instrument (e.g., thermometer, hygrometer, or so
on). Moreover, interaction between agents is limited such that an agent can only
communicate with its neighbors. As an agent requires access to each instrument,

NP-completeness Results for Partitioning a Graph
335
it needs to be copied across the agents to form a particular kind of dominating
set. Indeed, to accommodate e.g., power limitations or system failures, we do
not rely on the instrument an agent itself holds. Now, each instrument forms
a total dominating set, and the maximum number of instruments that can be
made available is precisely the total domatic number of the network. Abbas et
al. [1] also stress that “... the underlying network topology of multirobot net-
works plays a crucial role in achieving the system level objectives within the
network in a distributed manner.” This motivates the study of the complexity
of computing the total domatic number on restricted topologies. Furthermore,
we also remark that Henning [20, Problem 12] calls for a deeper investigation of
total domination on planar graphs.
Our results. We considerably extend the known hardness results for computing
the total domatic number of a graph.
– For Sect. 3, our main result is that it is NP-complete to decide whether a
bipartite planar graph G of bounded maximum degree has dt(G) ≥3. This
complements the recent combinatorial results of Goddard and Henning [14]
who showed that no planar graph G′ has dt(G′) = 5. In other words, our result
shows that it is unlikely one could have a polynomial-time characterization
of planar graphs with described total domatic numbers.
– In Sect. 4, we prove that for every k ≥3, it is NP-complete to decide whether
a k-regular graph G has dt(G) ≥3. In contrast, Akbari et al. [2] charac-
terized the 3-regular graphs H with dt(H) ≥2. This is best possible in the
sense that our hardness result gives strong evidence for the non-existence of
a polynomial-time characterization of k-regular graphs with dt(G) ≥k.
– In Sect. 5 we focus on chordal graphs. We begin by proving that it is NP-
complete to decide whether dt(G) ≥2 where G is a split graph. On a positive
side, we show that the total domatic number can be computed in polynomial-
time for threshold graphs.
– Finally, in Sect. 6, we give fast exact exponential-time algorithms for the
problem. In particular, we show how the problem can be solved in 2nnO(1)
time for general graphs, and derive even faster algorithms for special graph
classes.
Statements whose proofs are located in the appendix are marked with ⋆.
2
Preliminaries
For a positive integer n, we write [n] = {1, 2, . . . , n}.
In what follows, we deﬁne the graph-theoretic concepts most central to our
work. For graph-theoretic notation not deﬁned here, we refer the reader to [10].
We also brieﬂy introduce decision problems our hardness results depend on.

336
M. Koivisto et al.
Graph parameters and classes. All graphs we consider are undirected and
simple. For a graph G, we denote by V (G) and E(G) its vertex set and edge set,
respectively. To reduce clutter, an edge {u, v} is often denoted as uv. Two vertices
x and y are adjacent (or neighbors) if xy is an edge of G. The neighborhood of a
vertex v, denoted by N(v), is the set of all vertices adjacent to v. Let G = (V, E).
When we identify two distinct vertices v, w ∈V , we obtain the graph with vertex
set V \ {v, w} ∪{u} and edge set E \ {{v′, v′′} | v′ ∈V, v′′ ∈{v, w}} ∪{{v′, u} |
v /∈{v, w} and {v′, v} ∈E or {v′, w} ∈E}.
A vertex-coloring is a function c : V →[k] assigning a color from [k] to each
vertex of a graph G = (V, E). The coloring is said to be proper if c(u) ̸= c(v) for
every uv ∈E. A graph G is said to be k-colorable if there exists a proper vertex-
coloring using k colors for it. The minimum k for which a graph G is k-colorable
is known as its chromatic number, denoted by χ(G). In particular, a 2-colorable
graph is bipartite. Similarly, an edge-coloring is a function f : E →[k] assigning
a color from [k] to each edge. We say that f is proper if two adjacent edges
receive a distinct color under f. A graph G is said to be k-edge-colorable if there
exists a proper edge-coloring using k colors for it. The minimum k for which G
is k-edge-colorable is known as its chromatic index, denoted by χ′(G).
A chord is an edge joining two non-consecutive vertices in a cycle. A graph
is chordal if every cycle of length 4 or more has a chord. Equivalently, a graph
is chordal if it contains no induced cycle of length 4 or more. A graph is a split
graph if its vertex set can be partitioned into a clique and an independent set. It
is known that all split graphs are chordal. A well-known subclass of split graphs
is formed by threshold graphs, which are the graphs that can be formed from
the empty graph by repeatedly adding either an isolated vertex or a dominating
vertex (see [25, Theorem 1.2.4]).
Finally, we mention the following well-known structural measure for “tree-
likeness” of graphs. A tree decomposition of G is a pair (T, {Xi : i ∈I}) where
Xi ⊆V , i ∈I, and T is a tree with elements of I as nodes such that:
1. for each edge uv ∈E, there is an i ∈I such that {u, v} ⊆Xi, and
2. for each vertex v ∈V , T[{ i ∈I | v ∈Xi }] is a tree with at least one node.
The width of a tree decomposition is maxi∈I |Xi|−1. The treewidth of G, denoted
by tw(G), is the minimum width taken over all tree decompositions of G.
Decision problems. Our main focus is the computational complexity of the
following problem.
Total Domatic k-Partition
Instance: A graph G = (V, E).
Question: Can V be partitioned into k total dominating sets, i.e., is
dt(G) ≥k?
Our NP-completeness results are established by polynomial-time reductions
from well-known coloring problems. We introduce them here for completeness.

NP-completeness Results for Partitioning a Graph
337
In both k-Coloring and Edge k-Coloring, the input is a graph G = (V, E).
In the former, we must decide whether χ(G) ≤k, while in the latter, we are
asked whether χ′(G) ≤k. Both problems are NP-complete for every k ≥3 (see
e.g., [12,24]). Finally, in the Set Splitting problem, we are given a universe
U = [n] and a set system F ⊆2U. The task is to decide whether the elements of
U can be colored in 2 colors such that each member of F contains both colors.
This problem is well-known to be NP-complete as well.
3
Total Domatic Partitioning of Planar Graphs
It is well-known that every planar graph G has a vertex with degree at most
ﬁve, so we have that 1 ≤dt(G) ≤5. However, very recently, Goddard and
Henning [14] proved that no planar graph G has dt(G) = 5 by establishing the
following tight bounds.
Theorem 1 (Goddard and Henning [14]). Any planar graph G has 1 ≤
dt(G) ≤4. Moreover, these bounds are tight.
In the following, we prove that there is likely no straightforward (i.e., polynomial-
time) characterization of planar graphs with speciﬁed total domatic numbers.
More precisely, we show that it is NP-complete to decide whether a planar graph
G of bounded maximum degree has dt(G) ≥3. We ﬁrst make the idea of the
reduction clear, and then introduce slightly more complex gadgets that establish,
using the same correctness argument, the same result for bipartite planar graphs
of bounded maximum degree.
Lemma 2. 3-Coloring reduces in polynomial time to Total Domatic 3-
Partition.
Proof. Let G be an instance of 3-Coloring. In polynomial time, we will create
the following instance G′ of Total Domatic 3-Partition, such that G is
3-colorable if and only if G′ is total domatic 3-colorable.
The graph G′ = (V ′, E′) is obtained from G by replacing each edge with a
diamond (see Fig. 2), and by attaching to each vertex of G a copy of C6 (see
Fig. 1). Formally, we let V ′ = V ∪{xij, x′
ij | ij ∈E} ∪{wvi | v ∈V, i ∈[5]}. Set
E′ = D ∪C, where
D = {ixij, ix′
ij, xijx′
ij, jxij, jx′
ij | ij ∈E},
C = {wviwvi+1, wv1wv5, wv2wv4, vwv1, vwv3, vwv5 | 1 ≤i < 4, v ∈V }.
This ﬁnishes the construction of G′.
Let c : V →[3] be a proper vertex-coloring of G. We will construct a total
domatic 3-coloring c′ : V ′ →[3] as follows. We retain the coloring of the vertices
in V , that is, c′(v) = c(v) for every v ∈V . Then, as the degree of xij is 3, it
holds that in any valid total domatic 3-coloring of G′, the colors from [3] must
be bijectively mapped to the neighborhood of xij; by symmetry, the same holds

338
M. Koivisto et al.
i
j
xij
x′
ij
Fig. 2. The edge ij replaced by a diamond in the construction of Lemma 2.
for x′
ij. Then consider two adjacent vertices i and j in G. We set c′(xij) =
c′(x′
ij) = f, where f is the unique color in [3] neither c(i) nor c(j). Clearly, the
neighborhood of both xij and x′
ij contains every color from [3]. Finally, consider
an arbitrary vertex v ∈V . Without loss of generality, suppose c(v) = 1. We will
then ﬁnish the vertex-coloring c′ as follows (see also Fig. 1):
c′(wv5) = 1 , c′(wv1) = 2 , c′(wv3) = 2 , c′(wv2) = 3 , c′(wv4) = 3 .
It is straightforward to verify that c′ is indeed a total domatic 3-coloring of G′.
For the other direction, suppose that there is a total domatic 3-coloring c′
of G′. Again, it holds that every color from [3] is bijectively mapped to the
neighborhood of xij, for every i and j. Moreover, it holds that c′(xij) = c′(x′
ij),
implying that c′(i) ̸= c′(j). Thus, c′ restricted to G gives a proper 3-coloring for
G. This concludes the proof.
⊓⊔
It is known that deciding whether a planar graph of maximum degree 4 can
be properly 3-colored is NP-complete [12]. This combined with the previous
lemma establishes the following.
Theorem 3. It is NP-complete to decide whether a planar graph G of maximum
degree 11 has dt(G) ≥3.
Corollary 4. It is NP-complete to decide whether a bipartite planar graph G
of maximum degree 19 has dt(G) ≥3.
Proof. To build G′, we proceed with a construction similar to Lemma 2. However,
for each edge ij ∈E(G), instead of a diamond, we construct the gadget shown
in Fig. 3 (a). For each vertex v ∈V , instead of C6, we identify v with the gadget
shown in Fig. 3 (b).
It is straightforward to verify that both gadgets are planar and bipartite.
Clearly, G′ is planar. Moreover, G′ is bipartite as an odd cycle of G has even
length in G. Correctness follows by the same argument as in Lemma 2.
⊓⊔
We remark that Lemma 2 has consequences for the complexity of total
domatic 3-coloring graphs of bounded treewidth, and further consequences for
planar graphs as well. Before proceeding, we brieﬂy recall that a parameterized
problem I is a pair (x, k), where x is drawn from a ﬁxed, ﬁnite alphabet and k

NP-completeness Results for Partitioning a Graph
339
i
2
5
j
xij
x′
ij
10
(a)
3
4
6
v
11
(b)
Fig. 3. (a) A replacment gadget for edge ij, and (b) a gadget each vertex v is identiﬁed
with. Both gadgets are planar and bipartite.
is an integer called the parameter. Then, a kernel for (x, k) is a polynomial-time
algorithm that returns an instance (x′, k′) of I such that (x, k) is a YES-instance
if and only if (x′, k′) is a YES-instance, and |x′| ≤g(k), for some computable
function g : N →N. If g(k) is a polynomial (exponential) function of k, we say
that I admits a polynomial (exponential) kernel (for more, see Cygan et al. [8]).
We then recall the following earlier result.
Theorem 5 (van Rooij et al. [29]). For every k ≥1, Total Domatic
k-Partition parameterized by treewidth admits an exponential kernel.
In the following, we show that this is the best possible, i.e., there is no
polynomial kernel under reasonable complexity-theoretic assumptions. We make
the following observations regarding the gadget construction in Lemma 2.
Observation 6. It holds that tw(C6) = 3.
By identifying a C6 with a vertex of a bounded treewidth graph G, we do
not considerably increase the treewidth of G.
Observation 7. Let G be a graph of treewidth k, let G′ be a graph of treewidth
k′, and let H be the graph obtained through the identiﬁcation of two vertices
v ∈V (G) and v′ ∈V (G′). Then tw(H) ≤max{k, k′}.
Finally, Bodlaender et al. [4] proved that 3-Coloring does not admit a
polynomial kernel parameterized by treewidth unless NP ⊆coNP/poly. As the
proof of Lemma 2 gives a parameter-preserving transformation guaranteeing
tw(G′) ≤tw(G) + 3, we have the following.
Theorem 8. Total Domatic 3-Partition
parameterized by treewidth does
not admit a polynomial kernel unless NP ⊆coNP/poly.
Another consequence of Lemma 2 is captured by the following observation.
For its statement, we recall the well-known exponential time hypothesis (ETH),
which is a conjecture stating that there is a constant c > 0 such that 3-SAT
cannot be solved in time O(2cn), where n is the number of variables.

340
M. Koivisto et al.
Corollary 9 (⋆).
Total Domatic 3-Partition
for planar graphs cannot
be solved in time 2o(√n) unless ETH fails, where n is the number of vertices.
However, the problem admits an algorithm running in time 2O(√n) for planar
graphs.
4
Total Domatic Partitioning of Regular Graphs
Recently, Akbari et al. [2] characterized the 3-regular graphs with total domatic
number at least two. In particular, they showed that these are precisely the 3-
regular graphs that do not contain a particular tree of maximum degree 3 as an
induced subgraph. Moreover, it follows from the work of Henning and Yeo [21]
that all k-regular graphs for k ≥4 have dt(G) ≥2. We remark that the result
of Akbari et al. [2] is the best possible in the sense that it is NP-complete to
decide whether a k-regular graph G has dt(G) ≥k, for every k ≥3. We establish
this by reducing from Edge k-Coloring on k-regular graphs, where k ≥3, a
problem shown to be NP-complete by Leven and Galil [24].
Essentially, our construction to follow is used already by Heggernes and
Telle [19, Theorem 5], but we describe it in the appendix for completeness.
Theorem 10 (⋆).
For every k ≥3, it is NP-complete to decide whether a
k-regular graph G has dt(G) ≥k.
5
Total Domatic Partitioning of Chordal Graphs
In the spirit of the previous sections, we begin by proving that total domatic
coloring of chordal graphs is computationally diﬃcult. More precisely, we show
that for every k ≥2, deciding whether dt(G) ≥k is NP-complete already for split
graphs. The result is obtained by showing polynomial-time equivalence between
this problem and Hypergraph Rainbow k-Coloring, studied by Guruswami
and Lee [15] among others. In the latter, we are given a universe U = [n] and
a set family F ⊆2U. The goal is to decide whether the elements of U can be
colored in k colors such that each member of F contains each of the k colors.
The problem is equivalent to Set Splitting for k = 2.
Theorem 11 (⋆). For every k ≥1, Total Domatic k-Partition for split
graphs is equivalent to Hypergraph Rainbow k-Coloring.
Theorem 12 (⋆). For every k ≥2, it is NP-complete to decide whether a split
graph G has dt(G) ≥k.
Given this negative result, it is interesting to consider split graphs with fur-
ther restrictions on their structure. For instance, every complete graph is a split
graph, and the total domatic number of complete graphs is known.
Proposition 13 (Shi et al. [30]). Let G be the complete graph with n vertices.
Then dt(G) = ⌊n/2⌋.

NP-completeness Results for Partitioning a Graph
341
However, complete graphs have a very special structure: can we impose a weaker
structural requirement on split graphs to obtain a graph class G for which dt(G)
can be computed in polynomial time? Before giving a positive answer to this
question, we show the following.
Lemma 14. Let G = (V, E) be a graph, and let S be a subset of V such that
every s ∈S is a dominating vertex. Then dt(G) ≥min{⌊n/2⌋, |S|}.
Proof. Denote ℓ= |S| and h = |V \ S|. Suppose ﬁrst that h ≥ℓ. We will prove
that dt(G) ≥ℓ. Construct a total domatic ℓ-coloring c : V →[ℓ] as follows. Map
[ℓ] bijectively to S and surjectively to V \ S. Then, each v /∈S is dominated
by each s ∈S, and thus each color from [ℓ] appears in the neighborhood of v.
Similarly, by construction, [ℓ] is mapped surjectively to V \S and s ∈S dominates
each vertex not in S. Therefore, when h ≥ℓ, we conclude that dt(G) ≥ℓ.
Finally, suppose that h < ℓ. Let q = ⌊(ℓ+ h)/2⌋= ⌊n/2⌋, and choose A ⊆S
such that |A| = q. Map [ℓ] bijectively to the vertices in A and surjectively to
V \A. By the above argument, this is a valid total domatic q-coloring. Therefore,
we have that dt(G) ≥q, concluding the proof.
⊓⊔
Theorem 15. Let G = (V, E) be a connected threshold graph with n vertices,
and let S be a subset of V such that every s ∈S is a dominating vertex. Then
dt(G) = min{⌊n/2⌋, |S|}.
Proof. It is well-known that every threshold graph G can be represented as a
string s(G) of characters u and j, where u denotes the addition of an isolated ver-
tex, and j the addition of a dominating vertex (see [25, Theorem 1.2.4]). Observe
that if the last symbol of s(G) is u, then G is not connected and consequently
dt(G) = 0. Thus, it holds that the last symbol of s(G) is j.
Denote ℓ= |S| and h = |V \ S|. Suppose h ≥ℓ, and consider the ﬁrst
occurrence of symbol u in s(G). The corresponding vertex has degree ℓ, and no
other vertex has degree less than ℓ. Thus, dt(G) ≤ℓ. By Lemma 14, it follows
that in this case, dt(G) = ℓ. Finally, suppose h < ℓ. In the extremal case, ℓ= n,
i.e., each vertex of G is a dominating vertex. By Proposition 13, we have that
dt(G) ≤(⌊ℓ+ h⌋)/2 = ⌊n/2⌋. On the other hand, Lemma 14 gives us a matching
lower bound. This concludes the proof.
⊓⊔
By the previous theorem, the total domatic number can be computed eﬃ-
ciently for threshold graphs.
Finally, despite Theorem 12, we observe that for some graphs G we can
always guarantee dt(G) ≥2.
Proposition 16. Let G be an n-vertex Hamiltonian graph where n is a multiple
of four. Then dt(G) ≥2.
Proof. Let v0v1 · · · vn be a Hamiltonian cycle in G such that n mod 4 = 0.
Consider a vertex-coloring c : V →[2] such that c(vi) = c(vi+1) = 1 and
c(vi+2) = c(vi+3) = 2 for i = 0, 4, . . . , n−4. By construction, each neighborhood
contains both colors 1 and 2, so we are done.
⊓⊔

342
M. Koivisto et al.
6
On Exact Algorithms for Total Domatic Partitioning
When a problem of interest is shown to be NP-complete, it motivates the con-
sideration of alternative algorithmic approaches and easier special cases. Our
results show that Total Domatic k-Partition remains hard for several spe-
cial cases. In addition, parameterization (see, e.g., [8]) by the number of colors k
seems uninteresting since the problem remains NP-complete for constant values
of k. These observations further motivate the study of exact (exponential-time)
algorithms. A brute-force algorithm tries every possible k-coloring and outputs
YES if and only if one of the k-colorings is a total domatic k-coloring. Such an
algorithm runs in time knnO(1), where n is the number of vertices. Can we do
considerably better? In what follows, we show this to be the case, and give even
faster algorithms for special graph classes.
To obtain a moderately exponential algorithm, we apply a result of
Bj¨orklund et al. [3] for the Set Partition problem. In this problem we are
given a universe U = [n], a set family F ⊆2U, and an integer k. The task is
to decide whether U admits a partition into k members of F. Using algebraic
methods, Bj¨orklund et al. showed the following.
Theorem 17 (Bj¨orklund et al. [3, Theorems 2 and 5]). Set Partition can
be solved in 2nnO(1) time. If membership in F can be decided in nO(1) time, then
Set Partition can be solved in 3nnO(1) time and nO(1) space.
We apply this result to the set family consisting of all total dominating sets
of a given graph. Since we can decide in polynomial time whether a given set of
vertices is a total dominating set, we get the following.
Corollary 18. Total Domatic k-Partition can be solved in 3nnO(1) time
and polynomial space. In exponential space, the time can be improved to 2nnO(1).
We note that Bj¨orklund et al. give a similar application to domatic number.
Relying on sophisticated algorithms for enumerating minimal dominating sets
due to Fomin et al. [11], they further improve the polynomial-space results by
lowering the constant of the exponential from 3 to 2.8718. Currently, the lowest
constant is 2.7139, due to Nederlof et al. [26].
For total domatic number we discover another way to improve the
polynomial-space algorithm, however, restricting ourselves to regular graphs.
We get the following result by a simple reduction to graph coloring, for which
the constant was recently improved to 2.2355 by Gaspers and Lee [13].
Theorem 19. One can decide in O(2.2355n) time and polynomial space whether
a given k-regular graph G with n vertices has dt(G) ≥k.
Proof. Deﬁne a graph G′ = (V, E′), where we put an edge between two vertices
u, v ∈V exactly when they occur in the same neighborhood in G. It holds that
dt(G) = k if and only if χ(G′) = k.
⊓⊔
As noted by Chen et al. [6], Total Domatic k-Partition for k = 2 cor-
responds to the well-known problem of hypergraph 2-coloring, also known as

NP-completeness Results for Partitioning a Graph
343
Set Splitting. To see this, we construct an instance of Set Splitting with
the universe corresponding to the vertex set of the graph, and the set family to
the neighborhoods. We obtain the following bound exploiting the algorithms of
Nederlof et al. [26] for Set Splitting.
Theorem 20. One can decide in O(1.8213n) time and polynomial space whether
a given graph G with n vertices has dt(G) ≥2. In exponential-space, the time
can be improved to O(1.7171n).
7
Concluding Remarks
Our hardness results mirror those known for domatic number. Indeed, the com-
putation of the domatic number was shown to be NP-complete for split graphs by
Kaplan and Shamir [23], while hardness for bipartite planar graphs was proved
by Poon et al. [28]. For positive results on special graph classes, results appear
more scattered. For instance, it seems unknown whether domatic number can
be solved in polynomial time for threshold graphs.
Concerning exact algorithms for total domatic number, an intriguing question
is whether one can beat the 3n time bound in polynomial space in general graphs.
For domatic number the known algorithms achieve that via inclusion–exclusion
and a branching algorithm that either lists minimal dominating sets or counts
dominating sets. Currently we do not know whether these branching algorithms
can be eﬀectively adapted to total dominating sets.
Acknowledgments. This work was supported in part by the Academy of Finland,
under Grant 276864 (M.K.), and by the Emil Aaltonen Foundation, under Grant
160138 N (J.L.).
References
1. Abbas, W., Egerstedt, M., Liu, C.H., Thomas, R., Whalen, P.: Deploying robots
with two sensors in K1,6-free graphs. J. Graph Theor. 82(3), 236–252 (2016)
2. Akbari, S., Motiei, M., Mozaﬀari, S., Yazdanbod, S.: Cubic graphs with total
domatic number at least two. arXiv preprint arXiv:1512.04748 (2015)
3. Bj¨orklund, A., Husfeldt, T., Koivisto, M.: Set partitioning via inclusion-exclusion.
SIAM J. Comput. 39(2), 546–563 (2009)
4. Bodlaender, H.L., Downey, R.G., Fellows, M.R., Hermelin, D.: On problems with-
out polynomial kernels. J. Comput. Syst. Sci. 75(8), 423–434 (2009)
5. Bollob´as, B., Cockayne, E.J.: Graph-theoretic parameters concerning domination,
independence, and irredundance. J. Graph Theor. 3(3), 241–249 (1979)
6. Chen, B., Kim, J.H., Tait, M., Verstraete, J.: On coupon colorings of graphs. Discr.
Appl. Math. 193, 94–101 (2015)
7. Cockayne, E.J., Dawes, R.M., Hedetniemi, S.T.: Total domination in graphs. Net-
works 10(3), 211–219 (1980)
8. Cygan, M., Fomin, F.V., Kowalik, L., Lokshtanov, D., Marx, D., Pilipczuk, M.,
Pilipczuk, M., Saurabh, S.: Parameter. Algorithms. Springer, Heidelberg (2015)

344
M. Koivisto et al.
9. Dai, F., Wu, J.: An extended localized algorithm for connected dominating set
formation in ad hoc wireless networks. IEEE Trans. Parallel Distrib. Syst. 15(10),
908–920 (2004)
10. Diestel, R.: Graph Theory. Springer, Heidelberg (2010)
11. Fomin, F.V., Grandoni, F., Pyatkin, A.V., Stepanov, A.A.: Combinatorial bounds
via measure and conquer: bounding minimal dominating sets and applications.
ACM Trans. Algorithms 5(1), 9:1–9:17 (2008)
12. Garey, M., Johnson, D., Stockmeyer, L.: Some simpliﬁed NP-complete graph prob-
lems. Theor. Comput. Sci. 1(3), 237–267 (1976)
13. Gaspers, S., Lee, E.: Faster graph coloring in polynomial space. ArXiv e-prints
arXiv:1607.06201 (2016)
14. Goddard, W., Henning, M.A.: Thoroughly distributed colorings. arXiv preprint
arXiv:1609.09684 (2016)
15. Guruswami, V., Lee, E.: Strong inapproximability results on balanced rainbow-
colorable hypergraphs. In: Proceedings of the 26th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pp. 822–836. SIAM (2015)
16. Han, B., Jia, W.: Clustering wireless ad hoc networks with weakly connected dom-
inating set. J. Parallel Distrib. Comput. 67(6), 727–737 (2007)
17. Haynes, T.W., Hedetniemi, S.T., Slater, P.J.: Domination in Graphs: Advanced
Topics. Marcel Dekker Inc., New York (1998)
18. Haynes, T.W., Hedetniemi, S.T., Slater, P.J.: Fundamentals of Domination in
Graphs. CRC Press, Boca Raton (1998)
19. Heggernes, P., Telle, J.A.: Partitioning graphs into generalized dominating sets.
Nordic J. Comput. 5(2), 128–142 (1998)
20. Henning, M.A.: A survey of selected recent results on total domination in graphs.
Discr. Math. 309(1), 32–63 (2009)
21. Henning, M.A., Yeo, A.: 2-colorings in k-regular k-uniform hypergraphs. Eur. J.
Comb. 34(7), 1192–1202 (2013)
22. Henning, M.A., Yeo, A.: Total Domination in Graphs. Springer, New York (2013)
23. Kaplan, H., Shamir, R.: The domatic number problem on some perfect graph
families. Inf. Process. Lett. 49(1), 51–56 (1994)
24. Leven, D., Galil, Z.: NP-completeness of ﬁnding the chromatic index of regular
graphs. J. Algorithms 4(1), 35–44 (1983)
25. Mahadev, N.V.R., Peled, U.N.: Threshold Graphs and Related Topics, vol. 56.
Elsevier, Amsterdam (1995)
26. Nederlof, J., van Rooij, J.M.M., van Dijk, T.C.: Inclusion/exclusion meets measure
and conquer. Algorithmica 69(3), 685–740 (2014)
27. Pfaﬀ, J., Laskar, R., Hedetniemi, S.T.: NP-completeness of total and connected
domination and irredundance for bipartite graphs. Technical report, Clemson Uni-
versity, Department of Mathematical Sciences 428 (1983)
28. Poon, S.-H., Yen, W.C.-K., Ung, C.-T.: Domatic partition on several classes of
graphs. In: Lin, G. (ed.) COCOA 2012. LNCS, vol. 7402, pp. 245–256. Springer,
Heidelberg (2012). doi:10.1007/978-3-642-31770-5 22
29. Rooij, J.M.M., Bodlaender, H.L., Rossmanith, P.: Dynamic programming on tree
decompositions using generalised fast subset convolution. In: Fiat, A., Sanders,
P. (eds.) ESA 2009. LNCS, vol. 5757, pp. 566–577. Springer, Heidelberg (2009).
doi:10.1007/978-3-642-04128-0 51
30. Shi, Y., Wei, M., Yue, J., Zhao, Y.: Coupon coloring of some special graphs. J.
Comb. Optim. 33(1), 156–164 (2017)

NP-completeness Results for Partitioning a Graph
345
31. Stojmenovic, I., Seddigh, M., Zunic, J.: Dominating sets and neighbor elimination-
based broadcasting algorithms in wireless networks. IEEE Trans. Parallel Distrib.
Syst. 13(1), 14–25 (2002)
32. Zelinka, B.: Total domatic number of cacti. Math. Slovaca 38(3), 207–214 (1988)
33. Zelinka, B.: Total domatic number and degrees of vertices of a graph. Math. Slovaca
39(1), 7–11 (1989)
34. Zelinka, B.: Domination in generalized Petersen graphs. Czech. Math. J. 52(1),
11–16 (2002)

Strong Triadic Closure in Cographs and Graphs
of Low Maximum Degree
Athanasios L. Konstantinidis1, Stavros D. Nikolopoulos2,
and Charis Papadopoulos1(B)
1 Department of Mathematics, University of Ioannina, Ioannina, Greece
konsakis@yahoo.com, charis@cs.uoi.gr
2 Department of Computer Science and Engineering,
University of Ioannina, Ioannina, Greece
stavros@cs.uoi.gr
Abstract. The MaxSTC problem is an assignment of the edges with
strong or weak labels having the maximum number of strong edges such
that any two vertices that have a common neighbor with a strong edge
are adjacent. The Cluster Deletion problem seeks for the minimum
number of edge removals of a given graph such that the remaining graph
is a disjoint union of cliques. Both problems are known to be NP-hard and
an optimal solution for the Cluster Deletion problem provides a solu-
tion for the MaxSTC problem, however not necessarily an optimal one.
In this work we give the ﬁrst systematic study that reveals graph families
for which the optimal solutions for MaxSTC and Cluster Deletion
coincide. We ﬁrst show that MaxSTC coincides with Cluster Dele-
tion on cographs and, thus, MaxSTC is solvable in quadratic time on
cographs. As a side result, we give an interesting computational charac-
terization of the maximum independent set on the cartesian product of
two cographs. Furthermore we study how low degree bounds inﬂuence
the complexity of the MaxSTC problem. We show that this problem is
polynomial-time solvable on graphs of maximum degree three, whereas
MaxSTC becomes NP-complete on graphs of maximum degree four.
The latter implies that there is no subexponential-time algorithm for
MaxSTC unless the Exponential-Time Hypothesis fails.
1
Introduction
The principle of strong triadic closure is an important concept in social networks
[8]. It states that it is not possible for two individuals to have a strong relation-
ship with a common friend and not know each other [11]. Towards the prediction
of the behavior of a network, such a principle has been recently proposed as a
maximization problem, called MaxSTC, in which the goal is to maximize the
number of strong edges of the underlying graph that satisfy the strong triadic
closure [20]. Closely related to the MaxSTC problem is the Cluster Dele-
tion problem which ﬁnds important applications in areas involving information
clustering [1]. In the second problem the goal is to remove the minimum number
of edges such that the resulting graph consists of vertex-disjoint union of cliques.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 346–358, 2017.
DOI: 10.1007/978-3-319-62389-4 29

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
347
The connection between MaxSTC and Cluster Deletion arises from the
fact that the edges inside the cliques in the resulting graph for Cluster Dele-
tion can be seen as strong edges for MaxSTC which satisfy the strong triadic
closure. Thus the number of edges in an optimal solution for Cluster Deletion
consists a lower bound for the number of strong edges in an optimal solution for
MaxSTC. However there are examples of graphs showing that an optimal solu-
tion for MaxSTC contains larger number of edges than an optimal solution for
Cluster Deletion [16]. Interestingly there are also families of graphs in which
their optimal value for MaxSTC matches such a lower bound. For instance any
solution on graphs that do not contain triangles coincide for both problems. Here
we initiate a systematic study on other non-trivial classes of graphs for which
the optimal solutions for both problems have exactly the same value.
Our main motivation is to further explore the complexity of the MaxSTC
problem when restricted to graph classes. As MaxSTC has been recently intro-
duced, there are few results concerning its complexity. The problem has been
shown to be NP-complete for general graphs [20] and split graphs [16] whereas
it becomes polynomial-time tractable on proper interval graphs and trivially
perfect graphs [16]. The NP-completeness on split graphs shows an interest-
ing algorithmic diﬀerence between the two problems, since Cluster Deletion
on such graphs can be solved in polynomial time [2]. Cluster Deletion is
known to be NP-complete on general graphs [19]. It remains NP-complete on
chordal graphs and also on graphs of maximum degree four [2,15]. On the pos-
itive side Cluster Deletion admits polynomial-time algorithms on proper
interval graphs [2], graphs of maximum degree three [15], and cographs [10]. In
fact for cographs a greedy algorithm that ﬁnds iteratively maximum cliques gives
an optimal solution, although no running time was explicitly given in [10].
Such a greedily approach is also proposed for computing a maximal inde-
pendent set of the cartesian product of general graphs. Summing the partial
products between iteratively maximum independent sets consists a lower bound
for the cardinality of the maximum independent set of the cartesian product
[13,14]. Here we prove that a maximum independent set of the cartesian prod-
uct of two cographs matches such a lower bound. We would like to note that
a polynomial-time algorithm for computing such a maximum independent set
is already claimed [12]. However no characterization is given, nor an explicit
running time of the algorithm is reported.
Our results. In this work we further explore the complexity of the MaxSTC
problem. We consider two unrelated families of graphs, cographs and graphs of
bounded degree. Cographs are characterized by the absence of a chordless path
on four vertices. For such graphs we prove that the optimal value for MaxSTC
matches the optimal value for Cluster Deletion. For doing so, we reveal an
interesting vertex partitioning with respect to their maximum clique and maxi-
mum independent set. This result enables us to give an O(n2)-time algorithm for
MaxSTC on cographs. As a byproduct we characterize a maximum independent
set of the cartesian product of two cographs which implies a polynomial-time
algorithm for computing such a maximum independent set. Moreover we study

348
A.L. Konstantinidis et al.
the inﬂuence of low maximum degree for the MaxSTC problem. We show an
interesting complexity dichotomy result: for graphs of maximum degree four
MaxSTC remains NP-complete, whereas for graphs of maximum degree three
the problem is solved in polynomial time. Our reduction for the NP-completeness
on graphs of maximum degree four implies that, under the Exponential-Time
Hypothesis, there is no subexponential time algorithm for MaxSTC.
2
Preliminaries
We refer to [3] for our standard graph terminology. For R ⊆E(G), G\R denotes
the graph (V (G), E(G) \ R), that is a subgraph of G and for S ⊆V (G), G −S
denotes the graph G[V (G)−S], that is an induced subgraph of G. Given a graph
G = (V, E), a strong-weak labeling on the edges of G is a bijection λ : E(G) →
{strong, weak}; i.e., λ assigns to each edge of E(G) one of the labels strong or
weak. The strong triadic closure of a graph G is a strong-weak labeling λ such
that for any two strong edges {u, v} and {v, w} there is a (weak or strong)
edge {u, w}. The problem of computing the maximum strong triadic closure,
denoted by MaxSTC, is to ﬁnd a strong-weak labeling on the edges of E(G)
that satisﬁes the strong triadic closure and has the maximum number of strong
edges. We denote by (ES, EW ) the partition of E(G) into strong edges ES and
weak edges EW . The graph spanned by ES is the graph G \ EW . For a strong
edge {u, v}, we say that u (resp., v) is a strong neighbor of v (resp., u). We
denote by NS(v) ⊆N(v) the strong neighbors of v. Given an optimal solution
for MaxSTC that consists of the strong edges ES, the graph spanned by the
edges of ES is denoted by ES(G). Whenever we write |ES(G)| we refer to its
number of edges, that is |ES(G)| = |ES|.
In the Cluster Deletion problem the goal is to partition the vertices of
a given graph G into vertex-disjoint cliques with the minimum number of edges
outside the cliques, or, equivalently, with the maximum number of edges inside
the cliques. A cluster graph is a graph in which every connected component is
a clique. Cluster graphs are characterized as exactly the graphs that do not
contain a P3 as an induced subgraph. Given an optimal solution for Cluster
Deletion, the cluster graph spanned by the edges that are inside the cliques
is denoted by EC(G). We write |EC(G)| to denote the number of edges in the
cluster graph. Notice that labeling strong all the edges of a cluster graph satisﬁes
the strong triadic closure, so that |EC(G)| ≤|ES(G)| holds for any graph G.
3
Computing MaxSTC on Cographs
Let G = (V, E) and H = (W, F) be two undirected graphs with V ∩W = ∅.
The disjoint union of G and H is the graph obtained from the union of
G and H, denoted by G ⊕H = (V ∪W, E ∪F). The complete join of G
and H is the graph obtained from the union of G and H and adding edges
between every pair of vertices that belong to diﬀerent graphs, denoted by
G⊗H = (V ∪W, E ∪F ∪{vw | v ∈V, w ∈W}). A graph is a cograph if it can be

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
349
generated from single-vertex graphs and recursively applying the disjoint union
and complete join operations. The complement of a cograph is also a cograph.
Cographs are exactly the graphs that do not contain any chordless path on four
vertices [5], and they can be recognized in linear time [6].
Let G be the given cograph. Our main goal is to show that there is an
optimal solution for MaxSTC on G that coincides with an optimal solution
for Cluster Deletion on G. The strong edges that belong to an optimal
solution for MaxSTC span the graph ES(G). An optimal solution for Cluster
Deletion consists of a cluster graph EC(G) by removing a minimum number
of edges of G. Labeling all edges of a cluster graph as strong, results in a strong-
weak labeled graph that satisfy the strong triadic closure. Thus our goal is to
show that there is an optimal solution ES(G) for MaxSTC that is a cluster
graph.
A clique (resp. independent set) of G having the maximum number of vertices
is denoted by Cmax(G) (resp., Imax(G)). A greedy clique partition of G, denoted
by C, is the ordering of cliques (C1, C2, . . . , Cp) in G such that C1 = Cmax(G) and
Ci = Cmax

G −i−1
j=1 Cj

for i = 2, 3, . . . , p. Similarly, a greedy independent set
partition of G, denoted by I, is the ordering of independent sets (I1, I2, . . . , Iq)
in G such that I1 = Imax(G) and Ii = Imax

G −i−1
j=1 Ij

for i = 2, 3, . . . , q.
Observe that the subgraph spanned by the edges of C does not contain any P3
and, thus, forms a solution for Cluster Deletion. Although in general a greedy
clique partition does not necessarily imply an optimal solution for Cluster
Deletion, when restricted to cographs the optimal solution is characterized by
the greedy clique partition.
Lemma 1 ([10]). Let G be a cograph with a greedy clique partition C. Then the
edges of C span an optimal solution EC(G) for Cluster Deletion.
We will use such a characterization of Cluster Deletion in order to give
its equivalence with the MaxSTC problem. Notice, however, that due to the
freedom of the adjacencies between the cliques of a greedy clique partition, it is
not suﬃcient to consider such a partition of the vertices. For doing so, we will
further decompose the cliques of a greedy clique partition. It is known that a
graph G is a cograph if and only if for any maximal clique C and any maximal
independent set I of every induced subgraph of G, |C ∩I| = 1 holds (also known
as the clique-kernel intersection property) [5]. Thus we state the following lemma.
Lemma 2 ([5]). Let G be a cograph. Then Cmax(G) ∩Imax(G) = {v} for some
vertex v.
We recursively apply Lemma 2, to obtain the following result.
Lemma 3. Let G be a cograph with a greedy clique partition C = (C1, . . . , Cp)
and a greedy independent set partition I = (I1, . . . , Iq). For every i, j with 1 ≤
i ≤p and 1 ≤j ≤q, if |Ci| ≥j or |Ij| ≥i then Ci ∩Ij ̸= ∅.

350
A.L. Konstantinidis et al.
a
b
c
d
e
f
u
x
a
c
x
b
d
f
u
e
C1
C2
C3
C4
I1
I2
I3
Fig. 1. A cograph and its greedy canonical partition (C, I) where C = (C1, C2, C3, C4)
and I = (I1, I2, I3).
Proof. We prove the statement by induction on p and q. If p = 1 (resp., q = 1)
then |C1| ≥1 (resp., |I1| ≥1) and C1 ∩I1 ̸= ∅trivially holds by Lemma 2.
Consider the sets Ci and Ij. Let Gi,j be the graph obtained from G by removing
the sets of vertices C1, . . . , Ci−1 and I1, . . . , Ij−1. By the induction hypothe-
sis we know that Ci ∩I1 = {u1}, . . . , Ci ∩Ij−1 = {uj−1} and C1 ∩Ij =
{v1}, . . . , Ci−1∩Ij = {vi−1}. We will prove that Cmax(Gi,j) = Ci\{u1, . . . , uj−1}
and Imax(Gi,j) = Ij \ {v1, . . . , vi−1}.
Observe that |Ci \ {u1, . . . , uj−1}| = |Ci| −j + 1. Assume for contradiction
that there is clique C′ in Gi,j such that |C′| > |Ci|−j+1. Then C′∩Ij−1 = ∅. We
consider the subgraph G′ of G induced by the vertices of Ij−1 and the vertices of
Gi,j. Since Ij−1 is an independent set, at most one vertex of Ij−1 is adjacent to
all the vertices of a maximum clique of Gi,j. Thus a maximum clique of G′ has
size at most |Ci|−j +2, so that C′ is a maximum clique of G′. Now observe that
Ij−1 is a maximum independent set of G′ by the greedy choice of Ij−1. Then,
however, we reach a contradiction, since by Lemma 2 we know that C′∩Ij−1 ̸= ∅.
Therefore Ci \ {u1, . . . , uj−1} is a maximum clique of Gi,j.
Symmetric arguments show that Imax(Gi,j) = Ij \ {v1, . . . , vi−1}. By the
choices of i and j notice that {u1, . . . , uj−1}∩{v1, . . . , vi−1} = ∅. Then Lemma 2
applies for Gi,j which shows that (Ci \ {u1, . . . , uj−1})∩(Ij \ {v1, . . . , vi−1}) ̸= ∅.
Therefore Ci ∩Ij ̸= ∅, showing the desired statement.
⊓⊔
Lemma 3 suggests a partition of the vertices of G with respect to C and I
as follows. We call greedy canonical partition a pair (C, I) with elements ⟨vi,j⟩
where 1 ≤i ≤p and 1 ≤j ≤|Ci|, such that V (G) =

v1,1, . . . , vp,|Cp|

and
vi,j ∈Ci∩Ij. Figure 1 shows such a greedy canonical partition of a given cograph.
Observe that such a partition corresponds to a 2-dimensional representation of
G. By Lemma 3 it follows that a cograph admits a greedy canonical partition.
Let us turn our attention back to the initial MaxSTC problem. We ﬁrst
consider the disjoint union of cographs.

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
351
Lemma 4. Let G and H be vertex-disjoint cographs. Then ES(G ⊕H) =
ES(G) ⊕ES(H) and EC(G ⊕H) = EC(G) ⊕EC(H).
Proof. There are no edges between G and H so that a strong edge of G and a
strong edge of H have no common endpoint. Thus the union of the solutions for
G and H satisfy the strong triadic closure. By Lemma 1, EC(G ⊕H) contains
the edges of a greedy clique partition which is obtained from the corresponding
cliques of G and H.
⊓⊔
We next consider the complete join of cographs. Given two vertex-disjoint
cographs G and H with greedy clique partitions C = (C1, . . . , Cp) and C′ =
(C′
1, . . . , C′
p′), respectively, we denote by Ci(G, H) the edges that have one end-
point in Ci and the other endpoint in C′
i, for every 1 ≤i ≤min{p, p′}.
Lemma 5. Let G and H be vertex-disjoint cographs with greedy clique partitions
C = (C1, . . . , Cp) and C′ = (C′
1, . . . , C′
p′), respectively. Then,
– ES(G ⊗H) = (ES(G) ⊕ES(H)) ∪E(G, H) and
– EC(G ⊗H) = (EC(G) ⊕EC(H)) ∪E(G, H),
where E(G, H) = C1(G, H) ∪· · · ∪Ck(G, H) and k = min{p, p′}.
Proof. For the edges of EC(G ⊗H) we know that a greedy clique partition of
G ⊗H forms an optimal solution by Lemma 1. A greedy clique partition of
G ⊗H is obtained from the cliques Ci ∪C′
i, for every 1 ≤i ≤k, since all the
vertices of G are adjacent to all the vertices of H. The edges of Ci ∪C′
i can be
partitioned into the sets E(Ci), E(C′
i), Ci(G, H) giving the desired formulation
for EC(G ⊗H).
We consider the optimal solution for MaxSTC described by the edges of
ES(G ⊗H). Let us show that any solution on the edges of G satisfy the strong
triadic closure in the graph G ⊗H. Consider a strong edge {x, y} of G. If the
resulting labeling does not satisfy the strong triadic closure then there is a strong
edge {x, w} such that y and w are non-adjacent. As G and H are vertex-disjoint
graphs, w ∈V (G) or w ∈V (H). If w ∈V (G) then we already know that
the labeling of ES(G) satisﬁes the strong triadic closure so that y and w are
adjacent. If w ∈V (H) then by the complete join operation w is adjacent to y.
Thus maximizing the number of strong edges that belong in G and H results in
an optimal solution for G ⊗H.
We next consider the edges that have one endpoint in G and the other in H,
denoted by E(G, H). Our goal is to show that edges of C1(G, H)∪· · ·∪Ck(G, H)
belong to an optimal solution. Let (C, I) and (C′, I′) be the greedy canonical
partitions of G and H, respectively, where C = (C1, . . . , Cp), I = (I1, . . . , Iq),
and C′ = (C′
1, . . . , C′
p′), I′ = (I′
1, . . . , I′
q′). In the forthcoming arguments we
prove that |E(G, H)| ≤ |Ci||C′
i|.
We consider the vertices of Ij, 1 ≤j ≤q, and count the number of strong
edges that have one endpoint in Ij and the other endpoint on a vertex of H.
Without loss of generality assume that |I1| ≤|I′
1|. Then notice that k = |I1|,

352
A.L. Konstantinidis et al.
since p = |I1| and p′ = |I′
1| by Lemma 3. For a subset W of vertices of G, we
denote by s(W) the number of strong edges of E(G, H) that are incident to
the vertices of W. By the strong triadic closure, any vertex of H has at most
one strong neighbor in Ij and any vertex of G has at most one strong neighbor
in I′
j′, 1 ≤j′ ≤q′. Thus for any I′
j′ of H there are at most min{|Ij|, |I′
j′|}
strong edges between the vertices of Ij and I′
j′. Let rj be the largest index of
{1, . . . , q′} for which |I′
rj| ≥|Ij|; notice that rj exists since |Ij| ≤|I1| ≤|I′
1|.
Then, since |I′
1| ≥· · · ≥|I′
q′|, it is clear that |Ij| is smaller or equal than any
of |I′
1|, . . . , |I′
rj| and greater than any of |I′
rj+1|, . . . , |I′
q′|. Moreover by Lemma 3
we know that for every 1 ≤i ≤|Ij|, C′
i contains exactly one vertex from each of
I′
1, . . . , I′
rj, I′
rj+1, . . . , I′
|C′
i|. Thus we get the following inequality.
s(Ij) ≤
q′

j′=1
min{|Ij|, |I′
j′|} =
rj

j′=1
|Ij| +
q′

j′=rj+1
|I′
j′| =
|Ij|

i=1
|C′
i|.
Summing up each of s(Ij) for every Ij, 1 ≤j ≤q, we obtain:
|E(G, H)| =
q

j=1
s(Ij) ≤
q

j=1
|Ij|

i=1
|C′
i|
Observe that in the described sum each |C′
i| is counted for all 1 ≤j ≤q such
that |Ij| ≥i. Thus by Lemma 3 the number that |C′
i| appears in the formula is
exactly |Ci|, since for such |Ij| and i, we have Ci ∩Ij ̸= ∅. Therefore we get the
desired upper bound for the number of strong edges in E(G, H):
|E(G, H)| ≤
q

j=1
|Ij|

i=1
|C′
i| =
|I1|

i=1
|Ci||C′
i|.
Notice that the edges of C1(G, H) ∪· · · ∪Ck(G, H) satisfy the strong triadic
closure, since every two strong edges incident to a vertex of G belong to Ci(G, H)
which implies that the endpoints of H belong to a clique C′
i and, thus, are
adjacent in G ⊗H. Also observe that the number of edges of C1(G, H) ∪· · · ∪
Ck(G, H) matches the given upper bound. Therefore the claimed formula holds
for the strong edges of ES(G ⊗H) and this concludes the proof.
⊓⊔
Now we are ready to state our claimed result, namely that the solutions for
MaxSTC and Cluster Deletion coincide for the class of cographs.
Theorem 1. Let G be a cograph. There is an optimal solution for MaxSTC
on G that is a cluster graph. Moreover MaxSTC can be solved in O(n2) time.
Proof. An optimal solution for MaxSTC coincides with an optimal solution
for Cluster Deletion trivially for graphs that consist of a single vertex. If
G is a non-trivial cograph then it is constructed by the disjoint union or the
complete join operation. In the former case Lemma 4 applies, whereas in the
later Lemma 5 applies showing that in all cases ES(G) = EC(G).

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
353
Regarding the running time notice that given a suitable data structure, called
a cotree, a maximum clique C1 of G can be found in O(n) time [5]. At the
beginning we construct the cotree of G which takes time O(n+m) [6]. Removing
a vertex v from a cograph G and updating the cotree takes O(d(v)) time, where
d(v) is the degree of v in G [18]. Thus after removing all vertices from G we can
maintain the cotree in an overall O(n + m) time. In every intermediate step we
ﬁrst remove the set of vertices Ci in O(d(Ci)) time where d(Ci) is the sum of the
degree of the vertices of Ci, and then spend O(n) time to compute a maximum
clique by using the resulting cotree. Therefore a greedy clique partition of G can
be found in total O(n2) time, since there are at most n such cliques in C.
⊓⊔
3.1
Maximum Independent Set of the Cartesian Product of
Cographs
In this section we apply the characterization of Theorem 1 in order to show an
interesting computational characterization of the cartesian product of cographs.
Towards such a characterization we take advantage of an equivalent transforma-
tion of an optimal solution for MaxSTC in terms of a maximum independent
set of an auxiliary graph that is called the line-incompatibility graph. The line-
incompatibility graph (also known under the term Gallai graph [4,17]), denoted
by Γ(G), has a node uv in Γ(G) for every edge {u, v} of G, and two nodes uv,
vw of Γ(G) are adjacent if and only if the vertices u, v, w induce a P3 in G. The
connection between Γ(G) and MaxSTC is given in the following result.
Proposition 1 ([16]). For any graph G, a subset ES of edges span ES(G) if
and only if the nodes corresponding to ES form Imax (Γ (G)).
Let G and H be two vertex-disjoint graphs. The cartesian product of G and
H, denoted by G × H, is the graph with the vertex set V (G) × V (H) and any
two vertices (u, u′) and (v, v′) are adjacent in G × H if and only if either u = v
and u′ is adjacent to v′ in H, or u′ = v′ and u is adjacent to v in G. We are
interested in computing a maximum independent set of G × H whenever G and
H are cographs. We ﬁrst characterize the graph Γ(G ⊗H) in terms of G × H.
Lemma 6. Let G and H be two vertex-disjoint cographs. Then, Γ(G ⊗H) =
Γ(G) ⊕Γ(H) ⊕
	
G × H

.
Proof. Notice that G⊗H is a connected cograph, as every vertex of G is adjacent
to every vertex of H. The edges of G⊗H can be partitioned into the following sets
of edges: E(G), E(H), and E(G, H) where E(G, H) is the set of edges between
G and H in G ⊗H. By deﬁnition the nodes of Γ(G) and Γ(H) correspond to
the sets E(G) and E(H). Moreover since G and H are vertex-disjoint graphs,
Γ(G) and Γ(H) are also node-disjoint. This means that there are no common
endpoints in the edges inside G and H. Hence every node of Γ(G) is non-adjacent
to all nodes of Γ(H).
Next we show that every node of Γ(G ⊗H) that corresponds to an edge of
E(G, H) is non-adjacent to the nodes of Γ(G) and Γ(H). If a node xy of Γ(G)

354
A.L. Konstantinidis et al.
is adjacent to a node xa of E(G, H) then a is a vertex of H and {y, a} is not
an edge of G ⊗H contradicting the adjacency between the vertices of G and H.
Symmetric arguments show that any node of Γ(H) is non-adjacent to any node
of E(G, H). Thus no node that corresponds to an edge of E(G, H) is adjacent
to any node of Γ(G) ⊕Γ(H).
To complete the proof we need to show that graph of Γ(G ⊗H) induced by
the nodes of E(G, H) is exactly the graph G × H. Let x, y be two vertices of G
and let w, z be two vertices of H. By deﬁnition of Γ(G ⊗H) two nodes xw, yz
are adjacent if and only if either x = y and w is non-adjacent to z in H (so that
w is adjacent to z in H), or w = z and x is non-adjacent to y in G (so that x
is adjacent to y in G). Such an adjacency corresponds exactly to the deﬁnition
of the cartesian product of G and H. Therefore the graph of Γ(G ⊗H) induced
by the nodes of E(G, H) is exactly the graph G × H.
⊓⊔
Now we are ready to give the characterization of a maximum independent set
of the cartesian product of cographs, in terms of their greedy independent set
partition. Although a polynomial-time algorithm for computing such a maximum
independent set has already been claimed earlier [12], no characterization is
proposed nor an explicit bound on the running time is reported.
Theorem 2. Let G and H be two vertex-disjoint cographs with greedy indepen-
dent set partitions I = (I1, . . . , Iq) and I′ = (I′
1, . . . , I′
q′), respectively. Then the
vertices of (I1 × I′
1)⊕· · ·⊕(Iℓ× I′
ℓ) form a maximum independent set of G×H,
where ℓ= min {q, q′}. Moreover Imax(G × H) can be computed in O(n2) time,
where n = max{|V (G)|, |V (H)|}.
Proof. Let (C1, . . . , Cp) and (C′
1, . . . , C′
p′) be greedy clique partitions of G and
H, respectively. By Lemma 5 we know that ES(G ⊗H) = ES(G) ⊕ES(H) ∪
E(G, H), where E(G, H) = C1(G, H) ∪· · · ∪Ck(G, H) and k = min{p, p′}.
Notice that if (C1, . . . , Cp) is a greedy clique partition for G then (C1, . . . , Cp)
is a greedy independent set partition for G. Moreover by Proposition 1 we know
that the edges of ES (G ⊗H) correspond to the nodes of Imax (Γ (G ⊗H)). Since
Γ (G ⊗H) = Γ(G)⊕Γ(H)⊕
	
G × H

by Lemma 6, we get E(G, H) = Imax(G×
H). Therefore the vertices of (I1 × I′
1)⊕· · ·⊕(Iℓ× I′
ℓ) consist a Imax (Γ (G ⊗H)).
For the running time we need to compute two greedy independent set parti-
tions (I1, . . . , Iq) and (I′
1, . . . , I′
q′) for G and H, respectively, and then combine
each of Ij with I′
j, for 1 ≤j ≤ℓ. Computing a greedy independent set partition
for a cograph G can be done in O(n2) time by applying the algorithm on G
given in the proof of Theorem 1. Therefore the total running time is bounded
by O(|V (G)|2 + |V (H)|2).
⊓⊔
4
Graphs of Low Maximum Degree
Here we study the inﬂuence of the bounded degree in a graph for the MaxSTC
problem. We show an interesting complexity dichotomy result: for graphs of

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
355
maximum degree four MaxSTC remains NP-complete, whereas for graphs of
maximum degree three the problem is solved in polynomial time.
We prove the hardness result even on a proper subclass of graphs with max-
imum degree four. A graph G is a 4-regular K4-free graph, if every vertex of
G has degree four and there is no K4 in G. The decision version of MaxSTC
takes as input a graph G and an integer k and asks whether there is a strong-
weak labeling of G that satisﬁes the strong triadic closure with at least k strong
edges. Similarly the decision version of Cluster Deletion takes as input a
graph G and an integer k and asks whether G has a spanning cluster subgraph
by removing at most k edges. It is known that the decision version of Cluster
Deletion on connected 4-regular K4-free graphs is NP-complete [15].
Theorem 3. The decision version of MaxSTC is NP-complete on connected
4-regular K4-free graphs.
Proof. We give a polynomial-time reduction to MaxSTC from the Cluster
Deletion problem on connected 4-regular K4-free graphs which is already
known to be NP-complete [15]. Let G = (V, E) be a connected 4-regular K4-free
graph with n = 3q and 2n edges. Let EC(G) be a solution for the Cluster
Deletion with k = n edges. It is not diﬃcult to see that every connected com-
ponent of EC(G) is a triangle, since the graph is 4-regular and K4 is a forbidden
graph [15]. Then EC(G) is a solution for MaxSTC with at least n strong edges.
For the opposite direction, assume that ES(G) is a solution for MaxSTC
with at least n strong edges. We show that the graph spanned by the strong
edges of ES(G) is a two-regular graph. That is, every vertex of G has exactly
two strong neighbors. Assume that there is a vertex v that has at least three
strong neighbors. By the strong triadic closure all its strong neighbors must
induce a clique in G. Then N[v] induces a K4 which is a forbidden subgraph.
Thus every vertex has at most two strong neighbors. Furthermore if there is
a vertex having only one strong neighbor then |ES(G)| < n which contradicts
the assumption of n strong edges. Hence every vertex has exactly two strong
neighbors in ES(G).
Since ES(G) is a 2-regular graph we know that the graph spanned by the
strong edges is the disjoint union of triangles or chordless cycles Cp, with 4 ≤
p ≤n. Let us also rule out that a connected component of ES(G) is a chordless
cycle on four vertices C4. To see this, observe that if there is a C4 in ES(G)
then the four vertices of the C4 induce a K4 in G. Now assume that there is
a connected component of ES(G) that is a chordless cycle Cp with 4 < p < n.
In such a connected component, every vertex belongs to two distinct P3’s as an
endpoint. More precisely, let v1, . . . , vp be the vertices of Cp such that {vi, vi+1}
and {vp, v1} are strong edges with 1 ≤i < p. Then for every vertex vi of Cp there
two P3’s vi−2, vi−1, vi and vi, vi+1, vi+2 such that vi−2 ̸= vi+2. By the strong
triadic closure we know that vi is adjacent to both vi−2 and vi+2 in G. Since G
is a 4-regular graph, there are no more edges incident to any vertex of Cp. Thus
every vertex of Cp is non-adjacent to any other vertex of G−Cp which contradicts
the original connectivity of G. Therefore either every connected component of
ES(G) is a triangle, or ES(G) is connected and ES(G) = Cn.

356
A.L. Konstantinidis et al.
If every connected component of ES(G) is a triangle then clearly ES(G) spans
a cluster graph. Suppose that ES(G) = Cn. Since n = 3q, we can partition the
vertices of Cn into q triangles with the same number of strong edges as follows.
For every triplet of vertices vi, vi+1, vi+2, 1 ≤i ≤n −2, we further label the
edge {vi, vi+2} strong and the edges {vi+2, vi+3} and {vn, v1} are labeled weak.
Observe that {vi, vi+2} is an edge of G, since both {vi, vi+1}, {vi+1, vi+2} are
strong edges. Such a labeling satisﬁes the strong triadic closure property and
maintain the same number of strong edges. Therefore in every case a solution
for MaxSTC with n edges can be equivalently transformed into a solution for
Cluster Deletion with n edges.
⊓⊔
We can also obtain lower bounds for the running time of MaxSTC with
respect to the integer k (size of the solution) or the number of vertices n. A
subexponential-time algorithm for MaxSTC would imply an algorithm for solv-
ing Cluster Deletion that has running time subexponential in the size of the
solution k or the number of vertices n. However Cluster Deletion does not
admit such subexponential-time algorithms even if we restrict to graphs of max-
imum degree four [15]. Since we can reduce Cluster Deletion to MaxSTC
instances on the same graph with k = n, we arrive at the following.
Corollary 1. MaxSTC cannot be solved in 2o(k) · poly(n) time or in O(2o(n))
time unless the exponential-time hypothesis fails.
We stress that due to Proposition 1, MaxSTC reduces to ﬁnding a minimum
vertex cover of Γ(G) corresponding to the weak edges in an optimal solution.
Thus MaxSTC admits algorithms with running times 2Ω(k) poly(n) or O∗(cn)1
where k is the minimum number of weak edges and c < 2 is a constant [7,9].
Now let us show that if we restrict to graphs of maximum degree three then
MaxSTC becomes polynomial-time solvable. Our goal is to show that there is an
optimal solution for MaxSTC that is a cluster graph, since Cluster Deletion
is solved in polynomial time on such graphs [15].
Theorem 4. Let G be a graph with maximum degree three. Then there is an
optimal solution for MaxSTC on G that is a cluster graph.
Since Cluster Deletion can be solved in O(n1.5 · log2 n) on graphs with
maximum degree three [15], we get the following result.
Corollary 2. MaxSTC can be solved in O(n1.5 · log2 n) time when the input
graph has maximum degree three.
5
Concluding Remarks
We have performed a systematic study on families of graphs for which the opti-
mal solutions for MaxSTC and Cluster Deletion problems coincide. As an
1 The O∗notation suppresses polynomial factors of n.

Strong Triadic Closure in Cographs and Graphs of Low Maximum Degree
357
important outcome, we have complemented previous results regarding the com-
plexity of MaxSTC when restricted to cographs or graphs of bounded degree.
Some open questions arise from our work. It is interesting to completely charac-
terize graphs by forbidden subgraphs for which MaxSTC and Cluster Dele-
tion solutions coincide. Towards such an approach, Proposition 1 seems a useful
tool. Moreover, despite the fact that the optimal solutions for MaxSTC and
Cluster Deletion do not coincide in a superclass of cographs, namely to that
of permutation graphs, both problems restricted to such graphs have unresolved
complexity status which is interesting to settle.
A more general and realistic scenario for both problems is to restrict the
choice of the considered edges. Assume that a subset F of edges is required to
be included in the same clusters for Cluster Deletion or those edges are
required to be strong for MaxSTC. Then it is natural to ask for a suitable
set of edges E′ ⊆E \ F with |E′| as large as possible such that the edges
of E′ ∪F span a cluster graph or satisfy the strong triadic closure. Clarifying
the complexity of such generalized problems is interesting on graphs for which
Cluster Deletion or MaxSTC are solved in polynomial time.
References
1. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. Mach. Learn. 56, 89–113
(2004)
2. Bonomo, F., Dur´an, G., Valencia-Pabon, M.: Complexity of the cluster deletion
problem on subclasses of chordal graphs. Theoret. Comput. Sci. 600, 59–69 (2015)
3. Brandst¨adt, A., Le, V.B., Spinrad, J.: Graph Classes: A Survey. Society for Indus-
trial and Applied Mathematics (1999)
4. Cochefert, M., Couturier, J.-F., Golovach, P.A., Kratsch, D., Paulusma, D.: Para-
meterized algorithms for ﬁnding square roots. Algorithmica 74, 602–629 (2016)
5. Corneil, D.G., Lerchs, H., Stewart, L.K.: Complement reducible graphs. Discret.
Appl. Math. 3, 163–174 (1981)
6. Corneil, D.G., Perl, Y., Stewart, L.K.: A linear recognition algorithm for cographs.
SIAM J. Comput. 14, 926–934 (1985)
7. Cygan, M., Fomin, F.V., Kowalik, L., Lokshtanov, D., Marx, D., Pilipczuk, M.,
Pilipczuk, M., Saurabh, S.: Parameterized Algorithms. Springer, Cham (2015)
8. Easley, D., Kleinberg, J.: Networks, Crowds, and Markets: Reasoning About a
Highly Connected World. Cambridge University Press, New York (2010)
9. Fomin, F.V., Kratsch, D.: Exact Exponential Algorithms. Springer, Heidelberg
(2010)
10. Gao, Y., Hare, D.R., Nastos, J.: The cluster deletion problem for cographs. Discret.
Math. 313, 2763–2771 (2013)
11. Granovetter, M.: The strength of weak ties. Am. J. Sociol. 78, 1360–1380 (1973)
12. Hon, W.-K., Kloks, T., Liu, H.-H., Poon, S.-H., Wang, Y.-L.: On independence
domination. In: G asieniec, L., Wolter, F. (eds.) FCT 2013. LNCS, vol. 8070, pp.
183–194. Springer, Heidelberg (2013). doi:10.1007/978-3-642-40164-0 19
13. Imrich, W., Klavzar, S., Rall, D.F.: Topics in Graph Theory: Graphs and Their
Cartesian Product. AK Peters Ltd., Wellesley (2008)
14. Jha, P.K., Slutzki, G.: Independence numbers of product graphs. Appl. Math. Lett.
7, 91–94 (1994)

358
A.L. Konstantinidis et al.
15. Komusiewicz, C., Uhlmann, J.: Cluster editing with locally bounded modiﬁcations.
Discret. Appl. Math. 160, 2259–2270 (2012)
16. Konstantinids, A., Papadopoulos, C.: Maximizing the strong triadic closure in split
graphs and proper interval graphs. CoRR, abs/1609.09433 (2016)
17. Le, V.B.: Gallai graphs and anti-gallai graphs. SIAM J. Discret. Math. 159, 179–
189 (1996)
18. Shamir, R., Sharan, R.: A fully dynamic algorithm for modular decomposition and
recognition of cographs. Discret. Appl. Math. 136, 329–340 (2004)
19. Shamir, R., Sharan, R., Tsur, D.: Cluster graph modiﬁcation problems. Discret.
Appl. Math. 144, 173–182 (2004)
20. Sintos, S., Tsaparas, P.: Using strong triadic closure to characterize ties in social
networks. In: Proceedings of KDD 2014, pp. 1466–1475 (2014)

Hardness and Structural Results
for Half-Squares of Restricted Tree Convex
Bipartite Graphs
Hoang-Oanh Le1 and Van Bang Le2(B)
1 Berlin, Germany
lehoangoanh@web.de
2 Institut F¨ur Informatik, Universit¨at Rostock, Rostock, Germany
van-bang.le@uni-rostock.de
Abstract. Let B = (X, Y, E) be a bipartite graph. A half-square of B
has one color class of B as vertex set, say X; two vertices are adjacent
whenever they have a common neighbor in Y . Every planar graph is
half-square of a planar bipartite graph, namely of its subdivision. Until
recently, only half-squares of planar bipartite graphs (the map graphs)
have been investigated, and the most discussed problem is whether it
is possible to recognize these graphs faster and simpler than Thorup’s
O(n120) time algorithm.
In this paper, we identify the ﬁrst hardness case, namely that deciding
if a graph is a half-square of a balanced bisplit graph is NP-complete.
(Balanced bisplit graphs form a proper subclass of star convex bipar-
tite graphs.) For classical subclasses of tree convex bipartite graphs such
as biconvex, convex, and chordal bipartite graphs, we give good struc-
tural characterizations of their half-squares that imply eﬃcient recog-
nition algorithms. As a by-product, we obtain new characterizations of
unit interval graphs, interval graphs, and of strongly chordal graphs in
terms of half-squares of biconvex bipartite, convex bipartite, and chordal
bipartite graphs, respectively.
1
Introduction
The square of a graph H, denoted H2, is obtained from H by adding new edges
between two distinct vertices whenever their distance is two. Then, H is called
a square root of G = H2. Given a graph G, it is NP-complete to decide if G is
the square of some graph H [23], even for a split graph H [16].
Given a bipartite graph B = (X, Y, EB), the subgraphs of the square B2
induced by the color classes X and Y , B2[X] and B2[Y ], are called the two
half-squares of B [4].
While not every graph is the square of a graph and deciding if a graph is the
square of a graph is hard, every graph G = (V, EG) is half-square of a bipartite
graph: if B = (V, EG, EB) is the bipartite graph with EB = {ve | v ∈V, e ∈
EG, v ∈e}, then clearly G = B2[V ]. So one is interested in half-squares of special
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 359–370, 2017.
DOI: 10.1007/978-3-319-62389-4 30

360
H.-O. Le and V.B. Le
bipartite graphs. Note that B is the subdivision of G, hence every planar graph
is half-square of a planar bipartite graph.
Let B be a class of bipartite graphs. A graph G = (V, EG) is called half-square
of B if there exists a bipartite graph B = (V, W, EB) in B such that G = B2[V ].
Then, B is called a B-half root of G. With this notion, the following decision
problem arises.
Half-Square Of B
Instance: A graph G = (V, EG)
Question: Is G half-square of a bipartite graph in B, i.e., does there exist
a bipartite graph B = (V, W, EB) in B such that G = B2[V ]?
In this paper, we discuss Half-Square Of B for several restricted bipartite
graph classes B.
Previous results and related work. Half-squares of bipartite graphs have
been introduced in [4] in order to give a graph-theoretical characterization of the
so-called map graphs. It turns out that map graphs are exactly half-squares of
planar bipartite graphs. As we have seen at the beginning, every planar graph
is a map graph. The main problem concerning map graphs is to recognize if
a given graph is a map graph. In [27], Thorup shows that Half-Square Of
Planar, that is, deciding if a graph is a half-square of a planar bipartite graph,
can be solved in polynomial time1. Very recently, in [22], it is shown that Half-
Squares Of Outerplanar and Half-Square Of Tree are solvable in linear
time. Other papers deal with solving hard problems in map graphs include [3,5,
6,8]. Some applications of map graphs have been addressed in [1].
Our results. We identify the ﬁrst class B of bipartite graphs for which Half-
Square Of B is NP-hard. Our class B is a subclass of the class of the bisplit
bipartite graphs and of star convex bipartite graphs (all terms are given later).
For some other subclasses of bipartite graphs, such as biconvex, convex, and
chordal bipartite graphs, we give structural descriptions for their half-squares,
that imply polynomial-time recognition algorithms:
– Recognizing half-squares of balanced bisplit graphs (a proper subclass of star
convex bipartite graphs) is hard, even when restricted to co-bipartite graphs;
– Half-squares of biconvex bipartite graphs are precisely the unit interval
graphs;
– Half-squares of convex bipartite graphs are precisely the interval graphs;
– Half-squares of chordal bipartite graphs are precisely the strongly chordal
graphs.
2
Preliminaries
Let G = (V, EG) be a graph with vertex set V (G) = V and edge set E(G) = EG.
A stable set (a clique) in G is a set of pairwise non-adjacent (adjacent) vertices.
1 Thorup did not give the running time explicitly, but it is estimated to be roughly
O(n120) with n being the vertex number of the input graph.

Hardness and Structural Results for Half-Squares
361
The complete graph on n vertices, the complete bipartite graph with s vertices in
one color class and t vertices in the other color class, the cycle with n vertices are
denoted Kn, Ks,t, and Cn, respectively. A K3 is also called a triangle, a complete
bipartite graph is also called a biclique, a complete bipartite graph K1,n is also
called a star.
The neighborhood of a vertex v in G, denoted by NG(v), is the set of all
vertices in G adjacent to v; if the context is clear, we simply write N(v). A
universal vertex v in G is one with N(v) = V \{v}, i.e., v is adjacent to all other
vertices in G.
For a subset W ⊆V , G[W] is the subgraph of G induced by W, and G −W
stands for G[V \ W]. We write B = (X, Y, EB) for bipartite graphs with a
bipartition into stable sets X and Y . For subsets S ⊆X, T ⊆Y we denote
B[S, T] for the bipartite subgraph of B induced by S ∪T.
We will consider half-squares of the following well-known subclasses of bipar-
tite graphs: Let B = (X, Y, EB) be a bipartite graph.
– B is X-convex if there is a linear ordering on X such that, for each y ∈Y ,
N(y) is an interval in X. Being Y -convex is deﬁned similarly. B is convex if it
is X-convex or Y -convex. B is biconvex if it is both X-convex and Y -convex.
We write Convex and Biconvex to denote the class of convex bipartite
graphs, respectively, the class of biconvex bipartite graphs.
– B is chordal bipartite if B has no induced cycle of length at least six.
Chordal Bipartite stands for the class of chordal bipartite graphs.
– B is tree X-convex if there exists a tree T = (X, ET ) such that, for each
y ∈Y , N(y) induces a subtree in T. Being tree Y -convex is deﬁned similarly.
B is tree convex if it is tree X-convex or tree Y -convex. B is tree biconvex if
it is both tree X-convex and tree Y -convex. When T is a star, we also speak
of star convex and star biconvex bipartite graphs.
Tree Convex and Tree Biconvex are the class of all tree convex and all
tree biconvex bipartite graphs, respectively, and Star Convex and Star
Biconvex are the class of all star convex and all star biconvex bipartite,
respectively.
It is known that Biconvex ⊂Convex ⊂Chordal Bipartite ⊂Tree
Biconvex ⊂Tree Convex. All inclusions are proper; see [2,19,26] for more
information on these graph classes.
Given a graph G, we often use the following two kinds of bipartite graphs
associated to G:
Deﬁnition 1. Let G = (V, EG) be an arbitrary graph.
– The bipartite graph B = (V, EG, EB) with EB = {ve | v ∈V, e ∈EG, v ∈e}
is the subdivision of G.
– Let C(G) denote the set of all maximal cliques of G. The bipartite graph
B = (V, C(G), EB) with EB = {vQ | v ∈V, Q ∈C(G), v ∈Q} is the vertex-
clique incidence bipartite graph of G.

362
H.-O. Le and V.B. Le
Note that the subdivision of a planar graph is planar, and subdivisions and
vertex-clique incidence graphs of triangle-free graphs coincide.
Proposition 1. Every graph is half-square of its vertex-clique incidence bipar-
tite graph. More precisely, if B = (V, C(G), EB) is the vertex-clique incidence
bipartite graph of G = (V, EG), then G = B2[V ]. Similar statement holds for
subdivisions.
Proof. For distinct vertices u, v ∈V , uv ∈EG if and only if u, v ∈Q for some
Q ∈C(G), if and only if u and v are adjacent in B2[V ]. That is, G = B2[V ]. ⊓⊔
3
Recognizing Half-Squares of Balanced Bisplit Graphs
Is Hard
Recall that a biclique is a complete bipartite graph. Following the concept of split
graphs, we call a bipartite graph bisplit if it can be partitioned into a biclique
and a stable set. In this section, we show that Half-Square Of Balanced
Bisplit is NP-hard. Balanced bisplit graphs form a proper subclass of bisplit
graphs, and are deﬁned as follows.
Deﬁnition 2. A bipartite graph B = (X, Y, EB) is called balanced bisplit if it
satisﬁes the following properties:
(i) |X| = |Y |;
(ii) there is partition X = X1 ˙∪X2 such that B[X1, Y ] is a biclique;
(iii) there is partition Y = Y1 ˙∪Y2 such that the edge set of B[X2, Y2] is a perfect
matching.
Note that by (i) and (iii), |X1| = |Y1|, and by (ii) and (iii), every vertex in X1
is universal in B2[X].
In order to prove the NP-hardness of Half-Square Of Balanced Bisplit,
we will reduce the following well-known NP-complete problem Edge Clique
Cover to it.
Edge Clique Cover
Instance: A graph G = (V, EG) and a positive integer k.
Question: Do there exist k cliques in G such that each edge of G is
contained in some of these cliques?
Edge Clique Cover is NP-complete [12,14,24], even when restricted to co-
bipartite graphs [17]. (A co-bipartite graph is the complement of a bipartite
graph.)
Theorem 1. Half-Square Of Balanced Bisplit is NP-complete, even
when restricted to co-bipartite graphs.

Hardness and Structural Results for Half-Squares
363
Proof. It is clear that Half-Square Of Balanced Bisplit is in NP, since
guessing a bipartite-half root B = (V, W, EB) with |W| = |V |, verifying that B
is balanced bisplit, and G = B2[V ] can obviously be done in polynomial time.
Thus, by reducing Edge Clique Cover to Half-Square Of Balanced
Bisplit, we will conclude that Half-Square Of Balanced Bisplit is NP-
complete.
Let (G = (V, EG), k) be an instance of Edge Clique Cover. Note that
we may assume that k ≤|EG|, and that G is connected and has no universal
vertices. We construct an instance G′ = (V ′, EG′) as follows: G′ is obtained from
G by adding a set U of k new vertices, U = {u1, . . . , uk}, and all edges between
vertices in U and all edges uv with u ∈U, v ∈V . Thus, V ′ = V ∪U, G′[V ] = G
and the k new vertices in U are exactly the universal vertices of G′. Clearly, G′
can be constructed in polynomial time O(k|V |) = O(|EG|·|V |), and in addition,
if G is co-bipartite, then G′ is co-bipartite, too. We now show that (G, k) ∈Edge
Clique Cover if and only if G′ ∈Half-Square Of Balanced Bisplit.
First, suppose that the edges of G = (V, EG) can be covered by k cliques
Q1, . . . , Qk in G. We are going to show that G′ is half-square of some balanced
bisplit bipartite graph. Consider the bipartite graph B = (V ′, W, EB) (see also
Fig. 1) with
W = W1 ∪W2, where W1 = {w1, . . . , wk}, and W2 = {wv | v ∈V }.
In particular, |V ′| = |W| = k + |V |. The edge set EB is as follows:
– B has all edges between U and W, i.e., B[U, W] is a biclique,
– B has edges vwv, v ∈V . Thus, the edge set of B[V, W2] forms a perfect
matching, and
– B has edges vwi, v ∈V , 1 ≤i ≤k, whenever v ∈V is contained in clique Qi,
1 ≤i ≤k.
U
ui
V
v
V ′ = U ∪V
W1
wi
W2
wv
W = W1 ∪W2
Fig. 1. The balanced bisplit graph B = (V ′, W, EB) proving G′ = B2[V ′]; v ∈V is
adjacent to wi ∈W1 if and only if v ∈Qi.

364
H.-O. Le and V.B. Le
Thus, B is a balanced bisplit graph. Moreover, by the construction of B, we
have in B2[V ′]:
– U = {u1, . . . , uk} is a clique (as B[U, W] is a biclique);
– every vertex u ∈U is adjacent to all vertices v ∈V (recall that G is connected,
so every v ∈V is in some Qi, and wi ∈W1 is a common neighbor of u and
v), and
– no two distinct vertices v, z ∈V have common neighbor in W2. So u and z
are adjacent in B2[V ′] if and only if v and z have a common neighbor wi in
W1, if and only if v and z belong to clique Qi in G, if and only if u and z are
adjacent in G.
That is, G′ = B2[V ′].
Conversely, suppose G′ = H2[V ′] for some balanced bisplit graph H =
(V ′, Y, EH) with |V ′| = |Y | and partitions V ′ = X1 ˙∪X2 and Y = Y1 ˙∪Y2
as in Deﬁnition 2. We are going to show that the edges of G can be covered by k
cliques. As H[X1, Y ] is a biclique, all vertices in X1 are universal in H2[V ′] = G′.
Hence
X1 = U
because no vertex in V = V ′ \U is universal in G′ (recall that G has no universal
vertex). Therefore
X2 = V and G = H2[V ].
Note that, as H is a balanced bisplit graph, |Y1| = |U| = k. Write Y1 =
{q1, . . . , qk} and observe that no two vertices in V have a common neighbor
in Y2. Thus, for each edge vz in G = H2[V ], v and z have a common neighbor qi
in Y1. Therefore, the k cliques Qi in H2[V ], 1 ≤i ≤k, induced by the neighbors
of qi in V , cover the edges of G.
⊓⊔
Theorem 1 indicates that recognizing half-squares of restricted bipartite
graphs is algorithmically much more complex than recognizing squares of bipar-
tite graphs; the latter can be done in polynomial time [15].
Observe that balanced bisplit graphs are star convex: Let B = (X, Y, EB) be
a bipartite graph with the properties in Deﬁnition 2. Fix a vertex u ∈X1 and
consider the star T = (X, {uv | v ∈X −u}). Since every vertex y ∈Y is adjacent
to u, N(y) induces a substar in T. Note, however, that the hardness of Half-
Square Of Balanced Bisplit does not imply the hardness of Half-Square
Of Star Convex. This is because the proof of Theorem 1 strongly relies on
the properties of balanced bisplit graphs. Indeed, in the meantime, we are able
to show that half-squares of star-convex bipartite graphs can be recognized in
polynomial time. This result will be included in the full version of this conference
paper.
4
Half-Squares of Biconvex and Convex Bipartite Graphs
In this section, we show that half-squares of convex bipartite graphs are precisely
the interval graphs and half-squares of biconvex bipartite graphs are precisely
the unit interval graphs.

Hardness and Structural Results for Half-Squares
365
Recall that G = (V, EG) is an interval graph if it admits an interval rep-
resentation I(v), v ∈V , such that two vertices in G are adjacent if and only
if the corresponding intervals intersect. Let G be an interval graph. It is well-
known [9,10] that there is a linear ordering of the maximal cliques of G, say
C(G) = (Q1, . . . , Qq), such that every vertex of G belongs to maximal cliques
that are consecutive in that ordering, that is, for every vertex u of G, there are
indices ℓ(u) and r(u) with
{i | 1 ≤i ≤q and u ∈Qi} = {i | ℓ(u) ≤i ≤r(u)}.
If C and D are distinct maximal cliques of G, then C \D and D \C are both
not empty, that is, for every j ∈{1, . . . , q}, there are vertices u and v such that
r(u) = ℓ(v) = j.
Recall that unit interval graphs are those interval graphs admitting an inter-
val representation in which all intervals have the same length. It is well known
[25] that a graph is a unit interval graphs if and only if it has an interval repre-
sentation in which no interval is properly contained in another interval (a proper
interval graph), if and only if it is a K1,3-free interval graph.
Lemma 1. The half-squares of a biconvex bipartite graph are K1,3-free.
Proof. Let B = (X, Y, EB) be a biconvex bipartite graph. By symmetry we
need only to show that B2[X] is K1,3-free. Suppose, by contradiction, that
x1, x2, x3, x4 induce a K1,3 in B2[X] with edges x1x2, x1x3 and x1x4. Let yi
be a common neighbor of x1 and x2, yj be a common neighbor of x1 and x3,
and yk be a common neighbor of x1 and x4. Then, yi, yj, yk are pairwise distinct
and induce, in B, a subdivision of K1,3. This is a contradiction because biconvex
bipartite graphs do not contain an induced subdivision of the K1,3. Thus, the
half-squares of a biconvex bipartite graph are K1,3-free.
⊓⊔
Lemma 2
(i) Every interval graph is half-square of a convex bipartite graph. More precisely,
if G = (V, EG) is an interval graph and B = (V, C(G), EB) is the vertex-clique
incidence bipartite graph of G, then G = B2[V ] and B is C(G)-convex.
(ii) If B = (X, Y, EB) is X-convex, then B2[Y ] is an interval graph.
Proof. (i) Let G = (V, EG) be an interval graph, and let B = (V, C(G), EB) be
the vertex-clique incidence bipartite graph of G. Since each v ∈V appears in
the interval {Qi | ℓ(v) ≤i ≤r(v)} in C(G) = (Q1, . . . , Qq), B is C(G)-convex.
Moreover, by Proposition 1, G = B2[V ].
(ii) This is because X admits a linear ordering such that, for each y ∈Y , N(y)
is an interval in X. This collection is an interval representation of B2[Y ] because
y and y′ are adjacent in B2[Y ] if and only if N(y) ∩N(y′) ̸= ∅.
⊓⊔
Theorem 2. A graph is half-square of a biconvex bipartite graph if and only if
it is a unit interval graph.

366
H.-O. Le and V.B. Le
Proof. First, by Lemma 2 (ii), half-squares of biconvex bipartite graphs are inter-
val graphs, and then by Lemma 1, half-squares of biconvex bipartite graphs are
unit interval graphs.
Next we show that every unit interval graph is half-square of some biconvex
bipartite graph. Let G = (V, EG) be a unit interval graph. Let B = (V, C(G), EB)
be the vertex-clique incidence bipartite graph of G. By Lemma 2 (i), G = B2[V ]
and B is C(G)-convex. We now are going to show that B is V -convex, too.
Consider a linear order in C(G), C(G) = (Q1, . . . , Qq), such that each v ∈V
is contained in exactly the cliques Qi, ℓ(v) ≤i ≤r(v). Let v ∈V be lexicograph-
ically sorted according (ℓ(v), r(v)). We claim that B is V -convex with respect to
this ordering. Assume, by a contradiction, that some Qi has neighbors v, u and
non-neighbor x with v < x < u in the sorted list, say. In particular, v, u belong
to Qi, but x not; see also Fig. 2.
Qℓ(v)
. . .
Qℓ(x)−1
Qℓ(x)
. . .
Qr(x)
Qr(x)+1
. . .
Qi
v
⋆
...
v
y
...
v
x
...
v
x
...
v
z
...
v
u
...
Fig. 2. Assuming v < x < u, and v, u ∈Qi, but x  ∈Qi.
Since x < u and ℓ(u) ≤i, we have ℓ(x) < i. Since u is not in Qi, we therefore
have
ℓ(x) ≤r(x) < i.
In particular, r(x) + 1 ≤i. Since v < x and r(v) ≥i, we have
ℓ(v) < ℓ(x).
In particular, ℓ(x) −1 ≥1. Now, by the maximality of the cliques, there exists
y ∈Qℓ(x)−1 with r(y) = ℓ(x)−1 (hence y is non-adjacent to x), and there exists
z ∈Qr(x)+1 with ℓ(z) = r(x) + 1 (hence z is non-adjacent to x and y; note that
r(x) + 1 = i and z = u are possible). But then v, x, y, and z induce a K1,3 in G,
a contradiction.
Thus, we have seen that every unit interval graph is half-square of a biconvex
bipartite graph.
⊓⊔
We next characterize half-squares of convex bipartite graphs as interval
graphs. This is somehow surprising because the deﬁnition of being convex bipar-
tite is asymmetric with respect to the two half-squares.
Theorem 3. A graph is a half-square of a convex bipartite graph if and only if
it is an interval graph.

Hardness and Structural Results for Half-Squares
367
Proof. By Lemma 2 (i), interval graphs are half-squares of convex bipartite
graphs. It remains to show that half-squares of convex bipartite graphs are inter-
val graphs. Let B = (X, Y, EB) be an X-convex bipartite graph. By Lemma 2
(ii), B2[Y ] is an interval graph. We now are going to show that B2[X] is an
interval graph, too.
Let B′ = (X, Y ′, EB′) be obtained from B by removing all vertices y ∈Y such
that NB(y) is properly contained in NB(y′) for some y′ ∈Y . Clearly, B2[X] =
B′2[X]. We show that B′ is Y ′-convex, hence, by Lemma 2 (ii), B2[X] = B′2[X]
is an interval graph, as claimed. To this end, let X = {x1, . . . , xn} such that, for
every y ∈Y ′, NB′(y) is an interval in X. (Recall that B, hence B′ is X-convex.)
For y ∈Y ′ let left(y) = min{i | xi ∈NB′(y)}, and sort y ∈Y ′ increasing
according left(y). Then, for each x ∈X, NB′(x) is an interval in Y ′: Assume, by
contradiction, that there is some x ∈X such that NB′(x) is not an interval in Y ′.
Let y be a leftmost and y′ be a rightmost vertex in NB′(x). By the assumption,
there is some y′′ ∈Y ′ \ NB′(x) with left(y) < left(y′′) < left(y′). Then, as
NB′(y), NB′(y′′) and NB′(y′) are intervals in X, NB′(y′′) must be a subset of
NB′(y); see also Fig. 3. Since x ∈NB′(y) but x ̸∈NB′(y′′), NB′(y′′) must be a
proper subset of NB′(y), contradicting to the fact that in B′, no such pair of
vertices exists in Y ′. Thus, B′ is Y ′-convex.
x
y
y′′
y′
N(y)
N(y′′)
N(y′)
Fig. 3. Assuming left(y) < left(y′′) < left(y′), and x is adjacent to y and y′, but
non-adjacent to y′′.
Note that B′ is indeed biconvex, hence, by Theorem 2, B2[X] = B′2[X] is
even a unit interval graph.
⊓⊔
Since (unit) interval graph with n vertices and m edges can be recognized in
linear time O(n + m) and all maximal cliques of an (unit) interval graph can be
listed in the same time complexity (cf. [11]), Theorems 3 and 2 imply:
Corollary 1. Half-Square Of Convex and Half-Square Of Biconvex
can be solved in linear time. A (bi)convex bipartite half-root, if any, can be con-
structed in linear time.

368
H.-O. Le and V.B. Le
5
Half-Squares of Chordal Bipartite Graphs
In this section, we show that half-squares of chordal bipartite graphs are precisely
the strongly chordal graphs. Recall that a graph is chordal if it has no induced
cycle of length at least four. It is well-known (see, e.g., [11,21,26]) that a graph
G = (V, EG) is chordal if and only if it admits a tree representation, that is,
there exists a tree T such that, for each vertex v ∈V , Tv is a subtree of T and
two vertices in G are adjacent if and only if the corresponding subtrees in T
intersect. Moreover, the vertices of T can be taken as the maximal cliques of the
chordal graph (a clique tree). Recall also that a graph is strongly chordal if it is
chordal and has no induced k-sun, k ≥3. Here a k-sun consists of a stable set
{s1, . . . , sk} and a clique {t1, . . . , tk} and edges siti, siti+1, 1 ≤i ≤k. (Indices
are taken modulo k.)
We ﬁrst begin with the following fact.
Lemma 3. Let B = (V, W, EB) be bipartite graph without induced C6 and let
k ≥3. If B2[V ] contains an induced k-sun, then B contains an induced cycle of
length 2k.
The proof of Lemma 3 will be given in the full version of this paper.
Theorem 4. A graph is half-square of a chordal bipartite graph if and only if it
is a strongly chordal graph.
Proof. We ﬁrst show that half-squares of chordal bipartite graphs are chordal.
Let B = (X, Y, EB) be a chordal bipartite graph. It is known that B is tree
convex [13,18]. Thus, there is a tree T = (X, ET ) such that, for each y ∈Y ,
N(y) induces a subtree in T. Then, for distinct vertices y, y′ ∈Y , y and y′ are
adjacent in B2[Y ] if and only if N(y) ∩N(y′) ̸= ∅, and thus, B2[Y ] has a tree
representation, hence chordal. Now, by Lemma 3, B2[Y ] cannot contain any sun
k-sun, k ≥3, showing that it is a strongly chordal graph. By symmetry, B2[X] is
also strongly chordal. We have seen that half-squares of chordal bipartite graphs
are strongly chordal graphs.
Next, let G = (V, EG) be a strongly chordal graph, and let B = (V, C(G), EB)
be the vertex-clique incidence bipartite graph of G. By Proposition 1, G = B2[V ].
Moreover, it is well-known [7] that B is chordal bipartite. Thus, every strongly
chordal graph is a half-square of some chordal bipartite graph, namely of its
vertex-clique incidence bipartite graph.
⊓⊔
Testing if G is strongly chordal can be done in O(min{n2, m log n}) time
[7,20,26]. Assuming G is strongly chordal, all maximal cliques Q1, . . . , Qq of G
can be listed in linear time (cf. [11,26]); note that q ≤n. So, Theorem 4 implies:
Corollary 2. Half-Square Of Chordal Bipartite can be solved in time
O(min{n2, m log n}), where n and m are the vertex and edge number of the input
graph, respectively. A chordal bipartite half-root, if any, can be constructed in the
same time complexity.

Hardness and Structural Results for Half-Squares
369
Theorem 4 (and its proof) gives another proof for a characterization of half-
squares of trees found in [22]. A block graph is one in which every maximal
2-connected subgraph (a block) is a complete graph; equivalently, a block graph
is a chordal graph without induced K4 −e, a K4 minus an edge.
Theorem 5 ([22]). Half-squares of trees are exactly the block graphs.
6
Conclusions
Until recently, only half-squares of planar bipartite graphs (the map graphs)
have been investigated, and the most considered problem is if it is possible to
recognize these graphs faster and simpler than Thorup’s O(n120) time algorithm.
In this paper, we have shown the ﬁrst NP-hardness result, namely that recog-
nizing if a graph is half-square of a balanced bisplit graph is NP-complete. For
classical subclasses of tree convex bipartite graphs such as biconvex, convex, and
chordal bipartite graphs, we have given good structure characterizations for their
half-squares. These structural results imply that half-squares of these restricted
classes of bipartite graphs can be recognized eﬃciently.
Recall that chordal bipartite graphs form a subclass of tree biconvex bipartite
graphs [13,18], and that half-squares of chordal bipartite graphs can be recog-
nized in polynomial time, while the complexity of recognizing half-squares of
tree (bi)convex bipartite graphs is unknown. So, an obvious question is: what is
the computational complexity of Half-Square Of Tree (Bi)convex?
Acknowledgment. We thank Hannes Steﬀenhagen for his careful reading and very
helpful remarks.
References
1. Brandenburg, F.J.: On 4-map graphs and 1-planar graphs and their recognition
problem. ArXiv (2015). http://arxiv.org/abs/1509.03447
2. Brandst¨adt, A., Le, V.B., Spinrad, J.P.: Graph Classes: A Survey. Society for
Industrial and Applied Mathematics, Philadelphia (1999)
3. Chen, Z.-Z.: Approximation algorithms for independent sets in map graphs. J.
Algorithms 41, 20–40 (2001). doi:10.1006/jagm.2001.1178
4. Chen, Z.-Z., Grigni, M., Papadimitriou, C.H.: Map graphs. J. ACM 49(2), 127–138
(2002). doi:10.1145/506147.506148
5. Demaine, E.D., Fomin, F.V., Hajiaghayi, M.T., Thilikos, D.M.: Fixed-parameter
algorithms for (k, r)-center in planar graphs and map graphs. ACM Trans.
Algorithms 1(1), 33–47 (2005). doi:10.1145/1077464.1077468
6. Demaine, E.D., Hajiaghayi, M.T.: The bidimensionality theory and its algorithmic
applications. Comput. J. 51(3), 292–302 (2007). doi:10.1093/comjnl/bxm033
7. Farber, M.: Characterizations of strongly chordal graphs. Discrete Math. 43,
173–189 (1983). doi:10.1016/0012-365X(83)90154-1
8. Fomin, F.V., Lokshtanov, D., Saurabh, S.: Bidimensionality and geometric graphs.
In: Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms 2012 (SODA 2012), pp. 1563–1575 (2012)

370
H.-O. Le and V.B. Le
9. Fulkerson, D.R., Gross, O.A.: Incidence matrices and interval graphs. Paciﬁc J.
Math. 15, 835–855 (1965)
10. Gilmore, P.C., Hoﬀman, A.J.: A characterization of comparability graphs and of
interval graphs. Canad. J. Math. 16, 539–548 (1964). doi:10.4153/CJM-1964-055-5
11. Golumbic, M.C.: Algorithmic Graph Theory and Perfect Graphs. Academic Press,
New York (1980)
12. Holyer, I.: The NP-completeness of some edge-partition problems. SIAM J. Com-
put. 4, 713–717 (1981). doi:10.1137/0210054
13. Jiang, W., Liu, T., Wang, C., Ke, X.: Feedback vertex sets on restricted bipartite
graphs. Theoret. Comput. Sci. 507, 41–51 (2013). doi:10.1016/j.tcs.2012.12.021
14. Kou, L.T., Stockmeyer, L.J., Wong, C.-K.: Covering edges by cliques with regard
to keyword conﬂicts and intersection graphs. Comm. ACM 21, 135–139 (1978).
doi:10.1145/359340.359346
15. Lau, L.C.: Bipartite roots of graphs. ACM Trans. Algorithms 2(2), 178–208 (2006).
doi:10.1145/1150334.1150337. Proceedings of the 15th Annual ACM-SIAM Sym-
posium on Discrete Algorithms (SODA 2004), pp. 952–961
16. Lau, L.C., Corneil, D.G.: Recognizing powers of proper interval, split, and
chordal graphs. SIAM J. Discrete Math. 18(1), 83–102 (2004). doi:10.1137/
S0895480103425930
17. Le, V.B., Peng, S.-L.: On the complete width and edge clique cover problems. In:
Xu, D., Du, D., Du, D. (eds.) COCOON 2015. LNCS, vol. 9198, pp. 537–547.
Springer, Cham (2015). doi:10.1007/978-3-319-21398-9 42
18. Lehel, J.: A characterization of totally balanced hypergraphs. Discrete Math. 57,
59–65 (1985). doi:10.1016/0012-365X(85)90156-6
19. Liu, T.: Restricted bipartite graphs: comparison and hardness results. In: Gu, Q.,
Hell, P., Yang, B. (eds.) AAIM 2014. LNCS, vol. 8546, pp. 241–252. Springer,
Cham (2014). doi:10.1007/978-3-319-07956-1 22
20. Lubiw, A.: Doubly lexical orderings of matrices. SIAM J. Comput. 16, 854–879
(1987). doi:10.1137/0216057
21. McKee, T.A., McMorris, F.R.: Topics in Intersection Graph Theory. Society for
Industrial and Applied Mathematics, Philadelphia (1999)
22. Mnich, M., Rutter, I., Schmidt, J.M.: Linear-time recognition of map graphs with
outerplanar witness. In: Proceedings of the 15th Scandinavian Symposium and
Workshops on Algorithm Theory 2016 (SWAT 2016), article no. 5, pp. 5:1–5:14
(2016). http://www.dagstuhl.de/dagpub/978-3-95977-011-8
23. Motwani, R., Sudan, M.: Computing roots of graphs is hard. Discrete Appl. Math.
54(1), 81–88 (1994). doi:10.1016/0166-218X(94)00023-9
24. Orlin, J.: Contentment in graph theory: covering graphs with cliques. Indagationes
Math. 80(5), 406–424 (1977). doi:10.1016/1385-7258(77)90055-5
25. Roberts, F.S.: Indiﬀerence graphs. In: Harary, F. (ed.) Proof Techniques in Graph
Theory, pp. 139–146. Academic Press, New York (1969)
26. Spinrad, J.P.: Eﬃcient Graph Representations. Fields Institute Monographs,
Toronto (2003)
27. Thorup, M.: Map graphs in polynomial time. In: Proceedings of the 39th IEEE
Symposium on Foundations of Computer Science 1998 (FOCS 1998), pp. 396–405
(1998). doi:10.1109/SFCS.1998.743490

Faster Graph Coloring in Polynomial Space
Serge Gaspers1,2 and Edward J. Lee1,2(B)
1 UNSW, Sydney, Australia
sergeg@cse.unsw.edu.au, e.lee@unsw.edu.au
2 Data61, CSIRO, Sydney, Australia
Abstract. We present a polynomial-space algorithm that computes the
number of independent sets of any input graph in time O(1.1389n) for
graphs with maximum degree 3 and in time O(1.2356n) for general
graphs, where n is the number of vertices. Together with the inclusion-
exclusion approach of Bj¨orklund, Husfeldt, and Koivisto [SIAM J. Com-
put. 2009], this leads to a faster polynomial-space algorithm for the graph
coloring problem with running time O(2.2356n). As a byproduct, we
also obtain an exponential-space O(1.2330n) time algorithm for count-
ing independent sets.
Our main algorithm counts independent sets in graphs with maxi-
mum degree 3 and no vertex with three neighbors of degree 3. This
polynomial-space algorithm is analyzed using the recently introduced
Separate, Measure and Conquer approach [Gaspers & Sorkin, ICALP
2015]. Using Wahlstr¨om’s compound measure approach, this improve-
ment in running time for small degree graphs is then bootstrapped to
larger degrees, giving the improvement for general graphs. Combining
both approaches leads to some inﬂexibility in choosing vertices to branch
on for the small-degree cases, which we counter by structural graph prop-
erties. The main complication is to upper bound the number of times the
algorithm has to branch on vertices all of whose neighbors have degree
2, while still decreasing the size of the separator each time the algorithm
branches.
1
Introduction
Graph coloring is a central problem in discrete mathematics and computer sci-
ence. In exponential time algorithmics [16], graph coloring is among the most well
studied problems, and it is an archetypical partitioning problem. Given a graph
G and an integer k, the problem is to determine whether the vertex set of G can
be partitioned into k independent sets. Already in 1976, Lawler [26] designed a
dynamic programming algorithm for graph coloring and upper bounded its run-
ning time by O(2.4423n), where n is the number of vertices of the input graph.
This was the best running time for graph coloring for 25 years, when Eppstein
[10,11] improved the running time to O(2.4150n) by using better bounds on
the number of small maximal independent sets in a graph. Based on bounds
on the number of maximal induced bipartite subgraphs and reﬁned bounds on
the number of size-constrained maximal independent sets, Byskov [7] improved
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 371–383, 2017.
DOI: 10.1007/978-3-319-62389-4 31

372
S. Gaspers and E.J. Lee
the running time to O(2.4023n). An algorithm based on fast matrix multiplica-
tion by Bj¨orklund and Husfeldt [3] improved the running time to O(2.3236n).
The current fastest algorithm for graph coloring, by Bj¨orklund et al. [2,4,25], is
based on the principle of inclusion–exclusion and Yates’ algorithm for the fast
zeta transform. This breakthrough algorithm solves graph coloring in O∗(2n)
time, where the O∗-notation is similar to the O-notation but ignores polynomial
factors.
A signiﬁcant drawback of the aforementioned algorithms is that they use
exponential space. Often, the space bound is the same as the time bound,
up to polynomial factors. This is undesirable [31], certainly for modern comput-
ing devices. Polynomial-space algorithms for graph coloring have been studied
extensively as well with successive running times O∗(n!) [8], O((k/e)n) (random-
ized) [12], O((2 + log k)n) [1], O(5.283n) [6], O(2.4423n) [3], and O(2.2461n) [4].
The latter algorithm is an inclusion-exclusion algorithm relying on a O(1.2461n)
time algorithm [17] for computing the number of independent sets in a graph.
Their method transforms any polynomial-space O(cn) time algorithm for count-
ing independent sets into a polynomial-space O((1 + c)n) time algorithm for
graph coloring. The running time bound for counting independent sets was sub-
sequently improved by Fomin et al. [13] to O(1.2431n) and by Wahlstr¨om [30] to
O(1.2377n). Wahlstr¨om’s algorithm is the current fastest published algorithm for
counting independent sets of a graph, it uses polynomial space, and it works for
the more general problem of computing the number of maximum-weight satisfy-
ing assignments of a 2-CNF formula. For a reduction from counting independent
sets to counting maximum-weight satisfying assignments of a 2-CNF formula
where the number of variables equals the number of vertices, see [9].
We note that Junosza-Szaniawski and Tuczynski [24] present an algorithm for
counting independent sets with running time O(1.2369n) in a technical report
that also strives to disconnect low-degree graphs. For graphs with maximum
degree 3 that have no degree-3 vertex with all neighbors of degree 3, they present
a new algorithm with running time 2n3/5+o(n), where n3 is the number of degree-
3 vertices, and the overall running time improvement comes from plugging this
result into Wahlstr¨om’s [30] previously fastest algorithm for the problem. How-
ever, we note that the 2n3/5+o(n) running time for counting independent sets
can easily be obtained from previous results. Namely, the problem of counting
independent sets is a polynomial PCSP with domain size 2, as shown in [28], and
the algorithm of [21] for polynomial PCSPs preprocesses all degree-2 vertices,
leaving a cubic graph on n3 vertices that is solved in 2n3/5+o(n) time. Improving
on this bound is challenging, and degree-3 vertices with all neighbors of degree
2 need special attention since branching on them aﬀects the degree-3 vertices
of the graph exactly the same way as for the much more general polynomial
PCSP problem, whereas for other degree-3 vertices one can take advantage of
the asymmetric nature of the typical independent set branching (i.e., we can
delete the neighbors when counting the independent sets containing the vertex
we branch on).

Faster Graph Coloring in Polynomial Space
373
Our Results. We present a polynomial-space algorithm computing the number of
independent sets of any input graph G in time O(1.2356n), where n is the number
of vertices of G. Our algorithm is a branching algorithm that works initially
similarly as Wahlstr¨om’s algorithm, where we slightly improve the analysis using
potentials (as, e.g., in [20,23,29]) to amortize some of the worst branching cases
with better ones. This algorithm uses a branching strategy that basically ensures
that both the maximum degree and the average degree of the graph do not
increase. This makes it possible to divide the analysis of the algorithm into
sections depending on what local structures can still occur in the graph, use a
separate measure for the analysis of each section, and combine these measures
giving a compound (piecewise linear) measure for the analysis of the overall
algorithm.
For instances where the maximum degree is 3 and no vertex has three neigh-
bors with degree 3, we substitute a subroutine that is designed and analyzed
using the recently introduced Separate, Measure and Conquer technique [21].
It computes a small balanced separator of the graph and prefers to branch on
vertices in the separator, adjusting the separator as needed by the analysis, and
reaping a huge beneﬁt when the separator is exhausted and the resulting con-
nected components can be handled independently. The Separate, Measure and
Conquer technique helps to amortize this sudden gain with the analysis of the
previous branchings, for an overall running time improvement.
Since using a separator restricts our choice in the vertices to branch on, we
use the structure of the graph and its separation to upper bound the number of
unfavorable branching situations and adapt our measure accordingly. Namely,
the algorithm avoids branching on degree-3 vertices in the separator with all
neighbors of degree 2 as long as possible, often rearranging the separator to
avoid this case. In our analysis we can then upper bound the number of unfavor-
able branchings and give the central vertex involved in such a branching a special
weight and role in the analysis. We call these vertices spider vertices. Our meticu-
lous analysis of this subroutine upper bounds its running time by O(1.0963n). For
graphs with maximum degree at most 3, we obtain a running time of O(1.1389n).
This improvement for small degree graphs is bootstrapped, using Wahlstr¨om’s
compound measure analysis, to larger degrees, and gives a running time improve-
ment to O(1.2356n) for counting independent sets of arbitrary graphs and to
O(2.2356n) for graph coloring. Bootstrapping an exponential-space pathwidth-
based O(1.1225n) time algorithm [15] for cubic graphs instead, we obtain an
exponential-space algorithm for counting independent sets with running time
O(1.2330n). We refer to [19] for a full version of the paper with all details.
2
Methods
Measure and Conquer. The analysis of our algorithm is based on the Measure
and Conquer method [14]. A measure for a problem (or its instances) is a function
from the set of all instances of the problem to the set of non-negative reals.
Modern branching analyses often use a potential function as measure that gives

374
S. Gaspers and E.J. Lee
a more ﬁne-grained way of tracking the progress of a branching algorithm than
a measure that is merely the number of vertices or edges of the graph. The
following lemma is at the heart of our analysis. It generalizes a similar lemma
from [20] to the treatment of subroutines.
Lemma 1 ([18]). Let A be an algorithm for a problem P, B be an algorithm
for a class C of instances of P, c ≥0 and r > 1 be constants, and μ(·), μB(·), η(·)
be measures for P, such that for any input instance I from C, μB(I) ≤μ(I),
and for any input instance I, A either solves P on I ∈C by invoking B with
running time O(η(I)c+1rμB(I)), or reduces I to k instances I1, . . . , Ik, solves
these recursively, and combines their solutions to solve I, using time O(η(I)c)
for the reduction and combination steps (but not the recursive solves),
(∀i)
η(Ii) ≤η(I) −1,
and
k

i=1
rμ(Ii) ≤rμ(I).
(1)
Then A solves any instance I in time O(η(I)c+1rμ(I)).
When Algorithm A does not invoke Algorithm B, we have the usual Measure
and Conquer analysis. Here, μ is used to upper bound the number of leaves of
the search tree and deserves the most attention, while η is usually a polynomial
measure to upper bound the depth of the search tree. For handling subroutines,
it is crucial that the measure does not increase when Algorithm A hands over
the instance to Algorithm B and we constrain that μB(I) ≤μ(I).
Compound Analysis. We can view Wahlstr¨om’s compound analysis [30] as a
repeated application of Lemma 1. For example, there is a subroutine A3 for when
the maximum degree of the graph is 3. The algorithm prefers then to branch on
a degree-3 vertex with all neighbors of degree 3. After all such vertices have been
exhausted, the algorithm calls a new subroutine A8/3 that takes as input a graph
with maximum degree 3 where no degree-3 vertex has only degree 3 neighbors.
In this case the average degree of the graph is at most 8/3, and the algorithm
prefers to branch on vertices of degree 3 that have 2 neighbors of degree 3, etc.
The analysis constrains that the measure for the analysis of A8/3 is at most the
measure for A3 for the instance that is handed to A8/3. In an optimal analysis,
we expect the measure for such an instance to be equal in the analysis of A3 and
A8/3, and Wahlstr¨om actually imposes equality at the pivot point 8/3.
Separate, Measure and Conquer. In our case, the A8/3 algorithm is based on
Separate, Measure and Conquer. For small-degree graphs, we can compute small
balanced separators in polynomial time. The algorithm then prefers to branch
on vertices in the separator. The Separate, Measure and Conquer technique
allows to distribute the large gain obtained by disconnecting the instance onto
the previous branching vectors. While, often, the measure is made up of weights
that are assigned to each vertex, this method assigns these weights only to the
larger part of the graph that is separated from the rest by the separator, and

Faster Graph Coloring in Polynomial Space
375
somewhat larger weights to the vertices in the separator. See (3) on page 8. Thus,
after exhausting the separator, the measure accurately reﬂects the “amount of
work” left to do. We artiﬁcially increase the measure of very balanced instances
by small penalty weights – this is so because in this case it is more diﬃcult
for the branching strategy to make most of its progress on what ends uo being
the large side. Since we may exhaust the separators a logarithmic number of
times, and computing a new separator might introduce a penalty term each time,
the measure also includes a logarithmic term that counteracts these artiﬁcial
increases in measure, and will in the end only contribute a polynomial factor
to the running time. For an in-depth treatment of the method we refer to [21].
Since we use the Separate, Measure and Conquer method when the average
degree drops to at most 8/3, we slightly generalize the separation computation
from [21], where the bound on the separator size depended only on the maximum
degree. A separation (L, S, R) of a graph G is a partition of the vertex set into
(possibly empty) sets L, S, R such that every path from a vertex in L to a vertex
in R contains a vertex from S.
Lemma 2. Let B ∈R. Let μ be a measure for graph problems such that for
every graph G = (V, E), every R ⊆V , and every v ∈V , we have that |μ(R ∪
{v})−μ(R)| ≤B. Assume that μ(R), the restriction of μ to R, can be computed
in polynomial time. If there is an algorithm computing a path decomposition of
width at most k of a graph G in polynomial time, then there is a polynomial
time algorithm computing a separation (L, S, R) of G with |S| ≤k and |μ(L) −
μ(R)| ≤B.
Proof. The proof is basically the same as for the separation computation from
[21], but we repeat it here for completeness. First, compute a path decomposition
of width k in polynomial time. We view a path decomposition as a sequence of
bags (B1, . . . , Bb) which are subsets of vertices such that for each edge of G, there
is a bag containing both endpoints, and for each vertex of G, the bags containing
this vertex form a non-empty consecutive subsequence. The width of a path
decomposition is the maximum bag size minus one. We may assume that every
two consecutive bags Bi, Bi+1 diﬀer by exactly one vertex, otherwise we insert
between Bi and Bi+1 a sequence of bags where the vertices from Bi \ Bi+1 are
removed one by one followed by a sequence of bags where the vertices of Bi+1\Bi
are added one by one; this is the standard way to transform a path decomposition
into a nice path decomposition of the same width where the number of bags is
polynomial in the number of vertices [5]. Note that each bag is a separator
and a bag Bi deﬁnes the separation (Li, Bi, Ri) with Li = (i−1
j=1 Bj) \ Bi and
Ri = V \ (Li ∪Bi). Since the ﬁrst of these separations has L1 = ∅and the last
one has Rb = ∅, at least one of these separations has |μr(Li) −μr(Ri)| ≤B.
Finding such a bag can clearly be done in polynomial time.
⊓⊔
We will use the lemma for graphs with maximum degree 3 and graphs with max-
imum degree 3 and average degree at most 8/3, for which path decompositions
of width at most n/6 + o(n) and n/9 + o(n) can be computed in polynomial time,
[13,15].

376
S. Gaspers and E.J. Lee
One disadvantage of using the Separate, Measure and Conquer method for
A8/3 is that the algorithm needs to choose vertices for branching so that the
size of the separator decreases in each branch. However, Wahlstr¨om’s algorithm
defers to branch on degree-3 vertices with all neighbors of degree 2 until this is
no longer possible, since this case leads to the largest branching factor for degree
3. For our approach, we instead rearrange the separator in some cases until we
are only left with spider vertices, a structure where our algorithm cannot avoid
branching on a degree-3 vertex with all neighbors of degree 2, we give a special
weight to these spider vertices and upper bound their number.
Potentials. To optimize the running time further, we use potentials; see [20,23,
29]. These constant weights are added to the measure if certain global properties
of the instance hold. We may use them to slightly increase the measure when
an unfavorable branching strategy needs to be used. The constraint (1) for this
unfavorable case then becomes less constraining, while all branchings that can
lead to this unfavorable case get tighter constraints. This allows to amortize
unfavorable cases with favorable ones.
3
Algorithm
We ﬁrst introduce notation necessary to present the algorithm. Let V (G) and
E(G) denote the vertex set and the edge set of the input graph G. For a vertex
v ∈V (G), its neighborhood, NG(v), is the set of vertices adjacent to v. The
closed neighborhood of a vertex v is NG[v] = NG(v) ∪{v}. If G is clear from
context, we use N(v) and N[v].
The degree of v is denoted d(v) = |NG(v)|. A (d1, d2, d3)-vertex is a degree 3
vertex that has neighbors of degree d1, d2, d3. An edge uv ∈E(G) is adjacent to
vertex u ∈V (G) and v ∈V (G). For two vertices u and v connected by a path,
let P ⊂V (G) with u, v ̸∈P be the intermediate vertices between u and v on the
path. If P consists only of degree-2 vertices then we call P a 2-path of u and v.
The maximum degree of G is denoted Δ(G) and d(G) = 2|E(G)|/|V (G)| is
its average degree. A cubic graph consists only of degree-3 vertices. A subcubic
graph has maximum degree at most 3. A (k1, k2, ..., kd) vertex is a degree-d
vertex with neighbors of degree k1, k2, ..., kd. A separation (L, S, R) of G is a
partition of its vertex set into the three sets L, S, R such that no vertex in L
is adjacent to any vertex in R. The sets L, S, R are also known as the left set,
separator, and right set. Using a similar notion to [21], a separation (L, S, R)
of G is balanced with respect to some measure μ, and a branching constant B
if |μ(R) −μ(L)| ≤2B and imbalanced if |μ(R) −μ(L)| > 2B. By convention,
μ(R) ≥μ(L) otherwise, we swap L and R. We use the measure μr deﬁned
on page 8 to compute the separation in our algorithm. We will now describe
the algorithm #IS which takes as input a graph G, a separation (L, S, R), and
a cardinality function c : {0, 1} × V (G) →N, and computes the number of
independent sets of G weighted by the cardinality function c. For clarity, let
cout(v) = c(0, v) and cin(v) = c(1, v). More precisely, it computes
ind(G, c) =

X⊆V (G)
1(X is an independent set in G) ·

v∈X
cin(v) ·

v∈V \X
cout(v)

Faster Graph Coloring in Polynomial Space
377
where 1(·) is an indicator function which returns 1 if its argument is true and 0
otherwise. Note that for a cardinality function c initialized to c(0, v) = c(1, v) =
1 for every vertex v ∈V (G), we have that ind(G, c) is the number of independent
sets of G. Cardinality functions are used for bookkeeping during the branching
process and have been used in this line of work before. The separation (L, S, R)
is initialized to (∅, ∅, V (G)) and will only come into play when G is subcubic
and has no (3,3,3)-vertex. In this case, the algorithm calls a subroutine #3IS,
which constitutes the main contribution of this paper. #3IS computes a balanced
separation of G, preferring to branch on vertices in the separator, readjusting the
separator as needed, and is analyzed using the Separate, Measure and Conquer
method. The full description of the algorithms #IS, #3IS and associated helper
algorithms can be found in [19].
Skeleton Graph. The skeleton graph Γ(G), or just Γ, of a subcubic graph G is
a graph where the degree-3 vertices of G are in bijection with the vertices in
Γ. Two vertices in Γ are adjacent if the corresponding vertices are adjacent in
G, or there exists a 2-path between the corresponding vertices in G. If G has a
separation (L, S, R) then denote (LΓ , SΓ , RΓ ) to be the same separation of G in
Γ consisting of only degree-3 vertices. Dragging refers to moving vertices or a
set of vertices of G from one component of (L, S, R) to another, creating a new
separation (L′, S′, R′) such that S′ is still a separator of G.
Spider Vertices. As Wahlstr¨om’s [30] analysis showed, an unfavorable branching
case occurs on (2, 2, 2) vertices. Due to our algorithm’s handling of these vertices
we narrowed down the undesirable vertices called spider vertices to a speciﬁc list
of properties. If s is a spider vertex then:
– s ∈S
– s has neighbors of degree (2,2,2)
– Either:
• |NΓ (s) ∩L| = 2 and NΓ (s) ∩R = {r} with r having neighbors of degree
(2,2,2). In this case we call s a left spider vertex
• |NΓ (s) ∩R| = 2 and NΓ (s) ∩L = {l} with l having neighbors of degree
(2,2,2). In this case we call s a right spider vertex
• |NΓ (s) ∩L| = 1, |NΓ (s) ∩R| = 1, NΓ (s) ∩S = {s′} and s′ has neighbors
of degree (2,2,2). In this case we call both s and s′ a center spider vertex,
which occur in pairs.
A left spider vertex s ∈S can be dragged to the left along with the 2-path from
s to r. If this were ever to occur, then r would be a right spider vertex, and
vice versa (Fig. 1).
Multiplier Reduction. We use a reduction called multiplier reduction to simplify
graphs that have a cut vertex eﬃciently. Suppose G has a separation (V1, {x}, V2)
and G1 = G[V1∪{x}] has measure at most a constant B. The multiplier reduction
can be applied to compute #IS(G, (L, S, R), c) as follows.

378
S. Gaspers and E.J. Lee
r
s
Fig. 1. A left spider vertex s.
1. Let:
– Gout = G1 \ {x}
– Gin = G1 \ NG1[x]
– cout =#IS(Gout,(L[Gout], S[Gout], R[Gout]),c)
– cin = #IS(Gin, (L[Gin], S[Gin], R[Gin]), c)
2. Modify c such that cin(x) = cin(x) · cin and cout(x) = cout(x) · cout
3. Return #IS(G[V2 ∪{x}], (L, S, R), c)
Since G1 has a measure of constant size, both steps 1 and 2 take polynomial
time.
Lazy 2-separator. Suppose there is a vertex x initially chosen to branch on as
well as two vertices {y, z} ⊂V (G) with d(y) ≥3 and d(z) ≥3 such that {y, z} is
a separator which separates x from G in a constant measure subgraph. We call
such vertices lazy 2-separators, for a vertex x. Similar to Wahlstr¨om’s elimination
of separators of size 2 in [29], in line 15 of #IS ([19]) instead of branching on x, if
there exists a lazy 2-separator {y, z} for x we branch on y. A multiplier reduction
will be performed on z in the recursive calls. Prioritizing lazy 2-separators allows
to exclude some unfavorable cases when branching on x.
Associated Average Degree. Similar to [30], we deﬁne the associated average
degree of a vertex x ∈V (G) as α(x)/β(x), in G with average degree d(G) = k
where
α(x) = d(x) + |{y ∈N(x) : d(y) < k}|, and β(x) = 1 +

{y∈N(x)|d(y)<k}
1/d(y) .
(2)
By selecting vertices with high associated average degree, our algorithm priori-
tizes branching on vertices with larger decreases in measure.
Branching. We now outline the branching routine used to recursively solve
smaller instances of the problem. Suppose we have a graph G, a separation
(L, S, R), and a cardinality function c. For a vertex x we denote the following
steps as branching on x.
1. Let:
– Gout = G \ {x}
– Gin = G \ (N(x) ∪{x})
– cout = #IS(Gout, (L[Gout], S[Gout], R[Gout]), c)
– cin = #IS(Gin, (L[Gin], S[Gin], R[Gin]), c)
– c′
out = cout(x)
– c′
in = cin(x) · 
v∈N(x) cout(v)
2. Return c′
out · cout + c′
in · cin

Faster Graph Coloring in Polynomial Space
379
4
Running Time Analysis
This section describes the running time analysis for #IS and #3IS, conducted
via compound measures. Constraints are presented as branching vectors (δ1, δ2)
which equates to the constraints 2−δ1 + 2−δ2 ≤1. We ﬁrst describe some special
vertex weights.
4.1
Measures
Measure with no (3,3,3) vertex. When using the Separate, Measure and Conquer
technique from [21] the measure of a cubic graph instance G with no (3,3,3)
vertices consists of additive components μs and μr, the measure of vertices in
the separator, and those in either L or R, respectively. Let S′ ⊆S be the set of
all spider vertices, si and ri refer to the weight attributed to a separator vertex
and a right vertex, in R or L, respectively, of degree i. Left and right spider
vertices have weight s′
3. In a center spider vertex pair s and s′, one of them has
weight s′
3 while the other takes on an ordinary weight of s3. These structurally
applied weights allow amortization of the spider vertex cases against non-spider
vertices. Deﬁne the measure μ8/3 as
μ8/3 = μs(S) + μr(R) + μo(L, S, R),
(3)
where μs(S) = |S′| · s′
3 + 
v∈S\S′ sd(v), μr(R) = 
v∈R rd(v), B = 6s3 and
μo(L, S, R) = max

0, B −μr(R) −μr(L)
2

+ (1 + B) · log1+ϵ(μr(R) + μs(S)).
We also require that si ≥si−1 and ri ≥ri−1 for i ∈{1, 2, 3}. The constant
B is larger than the maximum change in imbalance in each transformation in
the analysis, except the separation transformation.
Lemma 3. For a balanced separation (L, S, R) of a graph G with average degree
d = d(G), an upper bound for the measure μ8/3 is:
μ8/3(d) ≤
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
n
6 (d −2)s′
3
+ 1
2
 5n
6 (d −2)r3 + n(3 −d)r2

+μo(L, S, R) + o(n)
if 2 ≤d ≤28
11
n
4 (8 −3d)s′
3 + n
12(11d −28)s3
+ 1
2
 5n
6 (d −2)r3 + n(3 −d)r2

+μo(L, S, R) + o(n)
if 28
11 < d ≤8
3
which is maximised when d = 8
3 with the value
μ8/3 ≤n
9 s3 + 1
2
5n
9 r3 + n
3 r2

+ μo(L, S, R) + o(n)
if constraints r2
2 ≤s′
3
11 + 5r3
22 + 5r2
22 ≤s3
9 + 5r3
18 + r2
3 are satisﬁed.

380
S. Gaspers and E.J. Lee
General Measure. In order to analyze higher degree cases, we use a measure of
the form
μi(G) =

v∈G
rd(v) + μo(L, S, R)
where Δ(G) = i
for each part of the compound measure. The term μo(L, S, R) is the same sub-
linear term from the Separate, Measure and Conquer analysis on cubic graphs
which needs to be propagated into the higher degree analyses.
4.2
Degree 3 Analysis
#IS can be solved in polynomial time when Δ(G) ≤2 [27]. However, stepping
up to cubic graphs is a much harder problem. Greenhill [22] proves that counting
independent sets is actually a #P-hard problem even for graphs with maximum
degree 3.
Lemma 4. Algorithm #IS applied to a graph G with Δ(G) ≤3 and no (3, 3, 3)
vertex has running time O(1.0963n).
Proof (sketch). We present a sketch of the proof, emphasizing the tight con-
straints generated from algorithms #3IS, simplify and spider described in
detail in [19], where full proofs can be found. As suggested in [21], each case will
provide constraints that the weights described above will need to satisfy.
Some trivial constraints we must satisfy are r0 = r1 = s0 = s1 = 0 since these
vertices can easily be eliminated and require no branching rules. Our algorithm
considers skeleton graph vertices, and several rules drag entire 2-paths from one
separation to another, requiring r2 = 0. In simplify, line 8 implies the constraint
s2 +s′
3 + 1
2(r2 −r3) ≤0, enabling us to move a degree-3 vertex into the separator
by dragging out a degree-2 vertex.
From #3IS, line 2 imposes the constraint 1
6s′
3 + 5
12r3 ≤r3. If a (2,2,3) vertex
s is chosen to branch on in line 19 as shown in Fig. 2(a) ([19]), then we get the
constraint

s3 + Δs3 + 1
2(2Δr3) −3δ, s3 + 2Δs3 + 1
2(r3 + 2Δr3) −4δ

. The last
tight constraint is from spider line 17, displayed in Fig. 3(a) ([19]), giving the
constraint

s′
3 + 3
2Δr3, s′
3 + 3
2Δr3

.
s
S
R
L
(a) Balanced branching
r
S
R
L
(b) Imbalanced branching on r
s
S
R
L
(c) Imbalanced branching on s
Fig. 2. Worst case conﬁgurations for non-spider vertex branching in #3IS

Faster Graph Coloring in Polynomial Space
381
s
l
S
R
L
(a) Balanced branching on l
s
S
R
L
(b) Balanced branching on s
Fig. 3. Worst case conﬁgurations for spider vertex branching in spider
While the cases in Figs. 2(b) and 3(b) ([19]) are not tight, they are of inter-
est since these cases branch on vertices located outside the separator and it is
guaranteed that s is removed from the separator after branching.
⊓⊔
Lemma 5. Algorithm #IS applied to a graph G with d(G) ≤3 has running time
O(1.1389n) and uses polynomial space.
The algorithm #IS uses subroutine #3IS, which we analyze the measure and
the weights for. We equate the Separate, Measure and Conquer weights with
weights of the measure μ3, based on the compound analysis from Wahlstr¨om [30].
As Wahlstr¨om’s analysis only contains weights w′
3 and w′
2, for vertices of degree
3 and degree 2 respectively, the measure is μ3(G) = ((d −2)w′
3 + (3 −d)w′
2) n+
μo(L, S, R) where d = d(G) is the average degree of a cubic graph, and
μo(L, S, R) is the sub-linear term left over from the average degree
8/3
analysis.
In the case of a graph G with no (3,3,3) vertex, in order to use Lemma 1, the
values of w1 and w2 must satisfy inequalities r2
2 ≤w2, s′
3
11 + 5r3
22 + 5r2
22 ≤6w3
11 + 5w2
11 ,
s3
9 + 5r3
18 + r2
3 ≤2w3
3
+ w2
3 , induced when d = 2, 28
11, and 8
3 for μ8/3 respectively.
This results in the weights w3 = 0.1973 and w2 = 0.0033 when G has no (3,3,3)
vertex. We also let w′
3 ≥0 and w′
2 ≥0 be the weights associated with vertices of
degree 3 and degree 2 respectively, for a subcubic graph G. Using the analysis
by compound measures with μ3(G) = 
i∈{2,3} w′
i · ni, the following constraint
μ8/3(G) = μ3(G) when d(G) = 8/3 is required for a valid compound measure.
This can be rewritten as 2w3 + w2 = 2w′
3 + w′
2. Branching on a (3,3,3) vertex,
the only type of degree-3 that #IS branches on, gives a branching vector of
(4w′
3 −3w′
2, 8w′
3 −4w′
2). Setting the weights w′
3 = 0.1876 and w′
2 = 0.0228
satisﬁes the system of constraints described above and by using the measure
μ3(G), results in a running time of O∗(1.1389n).
Lemma 6. #IS can be solved in time O∗(1.2070n) for graphs with maximum
degree 4.
Theorem 1. #IS can be solved in time O∗(1.2356n) and polynomial space.

382
S. Gaspers and E.J. Lee
If we plug in a simple pathwidth-based subroutine [15] for graphs of maximum
degree 3, we obtain the following exponential-space result.
Theorem 2. #IS can be solved in time O∗(1.2330n).
Acknowledgements. We thank Magnus Wahlstr¨om for clarifying an issue of the
case analysis in [30] and an anonymous reviewer for useful comments on an earlier
version of the paper. Serge Gaspers is the recipient of an Australian Research Council
(ARC) Future Fellowship (FT140100048) and acknowledges support under the ARC’s
Discovery Projects funding scheme (DP150101134).
References
1. Angelsmark, O., Thapper, J.: Partitioning based algorithms for some colouring
problems. In: Hnich, B., Carlsson, M., Fages, F., Rossi, F. (eds.) CSCLP 2005.
LNCS (LNAI), vol. 3978, pp. 44–58. Springer, Heidelberg (2006). doi:10.1007/
11754602 4
2. Bj¨orklund, A., Husfeldt, T.: Inclusion-exclusion algorithms for counting set par-
titions. In: Proceedings of the 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS 2006), pp. 575–582. IEEE Computer Society (2006)
3. Bj¨orklund, A., Husfeldt, T.: Exact algorithms for exact satisﬁability and number
of perfect matchings. Algorithmica 52(2), 226–249 (2008)
4. Bj¨orklund, A., Husfeldt, T., Koivisto, M.: Set partitioning via inclusion-exclusion.
SIAM J. Comput. 39(2), 546–563 (2009)
5. Bodlaender, H.L., Kloks, T.: Eﬃcient and constructive algorithms for the path-
width and treewidth of graphs. J. Algorithm. 21(2), 358–402 (1996)
6. Bodlaender, H.L., Kratsch, D.: An exact algorithm for graph coloring with polyno-
mial memory. Technical report UU-CS-2006-015, Department of Information and
Computing Sciences, Utrecht University (2006)
7. Byskov, J.M.: Enumerating maximal independent sets with applications to graph
colouring. Oper. Res. Lett. 32(6), 547–556 (2004)
8. Christoﬁdes, N.: An algorithm for the chromatic number of a graph. Comput. J.
14(1), 38–39 (1971)
9. Dahll¨of, V., Jonsson, P., Wahlstr¨om, M.: Counting models for 2SAT and 3SAT
formulae. Theor. Comput. Sci. 332(1–3), 265–291 (2005)
10. Eppstein, D.: Small maximal independent sets and faster exact graph coloring.
In: Dehne, F., Sack, J.-R., Tamassia, R. (eds.) WADS 2001. LNCS, vol. 2125, pp.
462–470. Springer, Heidelberg (2001). doi:10.1007/3-540-44634-6 42
11. Eppstein, D.: Small maximal independent sets and faster exact graph coloring. J.
Graph Algorithms Appl. 7(2), 131–140 (2003)
12. Feder, T., Motwani, R.: Worst-case time bounds for coloring and satisﬁability
problems. J. Algorithm. 45(2), 192–201 (2002)
13. Fomin, F.V., Gaspers, S., Saurabh, S., Stepanov, A.A.: On two techniques of com-
bining branching and treewidth. Algorithmica 54(2), 181–207 (2009)
14. Fomin, F.V., Grandoni, F., Kratsch, D.: A measure & conquer approach for the
analysis of exact algorithms. J. ACM 56(5), 25 (2009)
15. Fomin, F.V., Høie, K.: Pathwidth of cubic graphs and exact algorithms. Inform.
Process. Lett. 97(5), 191–196 (2006)

Faster Graph Coloring in Polynomial Space
383
16. Fomin, F.V., Kratsch, D.: Exact Exponential Algorithms. Springer Science & Busi-
ness Media, New York (2010)
17. F¨urer, M., Kasiviswanathan, S.P.: Algorithms for counting 2-Sat solutions and
colorings with applications. In: Kao, M.-Y., Li, X.-Y. (eds.) AAIM 2007. LNCS,
vol. 4508, pp. 47–57. Springer, Heidelberg (2007). doi:10.1007/978-3-540-72870-2 5
18. Gaspers, S.: Exponential Time Algorithms. VDM Verlag, Saarbr¨ucken (2010)
19. Gaspers,
S.,
Lee,
E.:
Faster
graph
coloring
in
polynomial
space.
CoRR,
abs/1607.06201 (2016)
20. Gaspers, S., Sorkin, G.B.: A universally fastest algorithm for Max 2-Sat, Max
2-CSP, and everything in between. J. Comput. Syst. Sci. 78(1), 305–335 (2012)
21. Gaspers, S., Sorkin, G.B.: Separate, measure and conquer: faster polynomial-space
algorithms for Max 2-CSP and counting dominating sets. In: Halld´orsson, M.M.,
Iwama, K., Kobayashi, N., Speckmann, B. (eds.) ICALP 2015. LNCS, vol. 9134,
pp. 567–579. Springer, Heidelberg (2015). doi:10.1007/978-3-662-47672-7 46
22. Greenhill, C.: The complexity of counting colourings and independent sets in sparse
graphs and hypergraphs. Comput. Complex. 9(1), 52–72 (2000)
23. Iwata, Y.: A faster algorithm for dominating set analyzed by the potential method.
In: Marx, D., Rossmanith, P. (eds.) IPEC 2011. LNCS, vol. 7112, pp. 41–54.
Springer, Heidelberg (2012). doi:10.1007/978-3-642-28050-4 4
24. Junosza-Szaniawski, K., Tuczynski, M.: Counting independent sets via divide mea-
sure and conquer method. Technical report abs/1503.08323, arXiv CoRR (2015)
25. Koivisto, M.: An O∗(2n) algorithm for graph coloring and other partitioning prob-
lems via inclusion-exclusion. In: Proceedings of the 47th Annual IEEE Symposium
on Foundations of Computer Science (FOCS 2006), pp. 583–590. IEEE Computer
Society (2006)
26. Lawler, E.L.: A note on the complexity of the chromatic number problem. Inform.
Process. Lett. 5(3), 66–67 (1976)
27. Roth, D.: On the hardness of approximate reasoning. Artif. Intell. 82(1), 273–302
(1996)
28. Scott, A.D., Sorkin, G.B.: Polynomial constraint satisfaction problems, graph bisec-
tion, and the ising partition function. ACM T. Algorithms 5(4), 45 (2009)
29. Wahlstr¨om, M.: Exact algorithms for ﬁnding minimum transversals in rank-3
hypergraphs. J. Algorithm 51(2), 107–121 (2004)
30. Wahlstr¨om, M.: A tighter bound for counting max-weight solutions to 2SAT
instances. In: Grohe, M., Niedermeier, R. (eds.) IWPEC 2008. LNCS, vol. 5018,
pp. 202–213. Springer, Heidelberg (2008). doi:10.1007/978-3-540-79723-4 19
31. Woeginger, G.J.: Open problems around exact algorithms. Discret. Appl. Math.
156(3), 397–405 (2008)

On the Modulo Degree Complexity
of Boolean Functions
Qian Li1,2(B) and Xiaoming Sun1,2
1 CAS Key Lab of Network Data Science and Technology, Institute of Computing
Technology, Chinese Academy of Sciences, Beijing 100190, China
{liqian,sunxiaoming}@ict.ac.cn
2 University of Chinese Academy of Sciences, Beijing 100049, China
Abstract. For each integer m ≥2, every Boolean function f can be
expressed as a unique multilinear polynomial modulo m, and the degree
of this multilinear polynomial is called its modulo m degree. In this paper
we investigate the modulo degree complexity of total Boolean functions
initiated by Parikshit Gopalan et al. [8], in which they asked the fol-
lowing question: whether the degree complexity of a Boolean function
is polynomially related with its modulo m degree. For m be a power of
primes, it is already known that the module m degree can be arbitrarily
smaller compare to the degree complexity (see Sect. 2 for details). When
m has at least two distinct prime factors, the question remains open.
Towards this question, our results include: (1) we obtain some nontrivial
equivalent forms of this question; (2) we aﬃrm this question for some
special classes of functions; (3) we prove a no-go theorem, explaining
why this problem is diﬃcult to attack from the computational complex-
ity point of view; (4) we show a super-linear separation between the
degree complexity and the modulo m degree.
1
Introduction
The polynomial representation of Boolean functions in diﬀerent characteristics is
a powerful tool in extensive areas of computer science, such as machine learning
[12,13,16,18], computational complexity [1–3,19–21,23,25], explicit combinato-
rial constructions [5,7,9,10]. In this paper, we investigate the polynomial degree
of a function.
The modulo m degree of a Boolean function f, denoted by degm(f), is the
degree of the unique multilinear polynomial representing f over Z/mZ. In addi-
tion, we denote deg0(f) (where the underlie ring is Z) simply by deg(f). A central
topic here is to investigate the relationship between module m degrees for dif-
ferent m. From the deﬁnition it is clear that for any f, degm(f) ≥degm′(f)
if m′ is a factor of m, particularly, deg(f) ≥degm(f). This is because the
This work was supported in part by the National Natural Science Foundation of
China Grant 61433014, 61502449, 61602440, the 973 Program of China Grants No.
2016YFB1000201 and the China National Program for support of Top-notch Young
Professionals.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 384–395, 2017.
DOI: 10.1007/978-3-319-62389-4 32

On the Modulo Degree Complexity of Boolean Functions
385
polynomial representing f over Z/mZ can be obtained from the representa-
tion over Z by taking each coeﬃcient modulo m. The gap between deg(f)
and degm(f) can be arbitrarily large when m is a prime: consider the func-
tion f(x) = (x1 + · · · + xn)m−1 mod m, it is easy to see f is Boolean due to
Fermat’s little theorem, degm(f) ≤m −1, and deg(f) = Ω(n). Actually, the
gap can be arbitrarily large even when m is a prime power [6].
In the seminal paper, Gopalan et al. [8] showed a general principle: low degree
polynomials modulo p are hard to compute by polynomials in other characteris-
tics. More precisely, let f be a Boolean function which depends on n variables,
p and q be distinct primes, then
degq(f) ≥
n
⌈log2 p⌉degp(f)p2 degp(f) .
Moreover, they also showed that it’s still hard even to approximate, which implies
most known lower bounds for AC0[q] circuits.
In this work, we focus on the relation between deg(f) and degm(f). As men-
tioned above, deg(f) ≥degm(f), and the equality can be achieved by AND
function. For the other direction, the gap can be arbitrarily large for prime
powers [6]. The situation becomes diﬀerent when m has at least two distinct
prime factors p and q: according to the result in [8] as mentioned above, we have
degm(f) ≥max{degp(f), degq(f)} = Ω(log n) = Ω(log deg(f)). Gopalan et al.
[8] asked what is the largest possible separation between deg(f) and degm(f).
Here we conjecture these quantities are polynomially related:
Conjecture 1. Let f be a boolean function and m be an integer which has at
least two distinct prime factors, then
deg(f) ≤poly(degm(f)).
Our Results. Towards Conjecture 1, we ﬁrst give some equivalent conjectures
that might easier to solve. More precisely, we can replace the degree complexity
on the left side by some other complexity measures that could be exponentially
smaller than deg(f), such as the minimum certiﬁcate complexity etc.
We also conﬁrm the conjecture for some special classes of functions, such as
k-uniform hypergraph properties and functions with small alternating numbers.
Theorem 1. For any non-trivial k-uniform hypergraph property f on n vertices
and any integer m with at least two distinct prime factors, we have
deg(f) = O(degm(f)k).
Theorem 2. Let f be a boolean function, then for any m ≥2,
deg(f) = O(alt(f) · degm(f)2),
where alt(f) is the alternating number of f.

386
Q. Li and X. Sun
Note that deg6(f) = max{deg2(f), deg3(f)} according to the Chinese
Remainder Theorem, thus Conjecture 1 for the case m = 6 is equivalent to
conjecture that the module 3 degree of any polynomial P2 over F2 with low
degree must be large if the degree of the function represented by P2 is large. The
following no-go theorem somehow explains why this problem is hard to solve
even for this simplest case from the computational complexity point of view.
Theorem 3. Given a polynomial P2(x1, x2, . . . , xn) over F2 with poly(n) mono-
mials, it’s impossible to decide whether deg3(P2) = n or not in polynomial time,
unless NP = RP.
Finally, in the direction to disprove this conjecture, we provide a quadratic
separation. As we will see in Sect. 2, Conjecture 1 doesn’t lose generality only
focusing on the case m = p1p2, where p1 and p2 are two distinct primes.
Theorem 4. For any two distinct primes p1 and p2, there exists a sequence of
boolean functions f, s.t:
degp1p2(f) = O(deg(f)1/2).
We wonder whether this is the largest separation between degp1p2(f) and deg(f).
Organization. We present some preliminaries in Sect. 2, and give other equiva-
lent conjectures in Sect. 3. We conﬁrm this conjecture for k-uniform hypergraph
properties and functions with small alternating number in Sect. 4 and present a
no-go theorem and a super-linear separation in Sect. 5. Finally, we conclude this
paper in Sect. 6.
2
Preliminaries
Let f : {0, 1}n →{0, 1} be a Boolean function, and R be a commutative
ring containing {0, 1} with characteristic m, we say a multilinear polynomial
P(x1, · · · , xn) ∈R[x1, · · · , xn] represents f if P(x) = f(x) for any x ∈{0, 1}n.
From the Mobius inversion formula, such a polynomial always exists and is
unique. Moreover, the degree of P only depends on the characteristic of R[6],
thus we can denote the degree of P by degm(f). In the paper we will only
consider the case where R = Z/mZ and denote such polynomials by Pm(x).
We list some basic facts in the following, proofs of which can be found in [6].
Fact 1. Suppose the polynomial representation of f is 
S⊆[n] CS

i∈S xi, then
the representation over Z/mZ should be 
S⊆[n](CS mod m) 
i∈s xi.
For example, let f be the parity function, i.e., x1 ⊕· · · ⊕xn. The polynomial
representing f over Z is 
∅̸=S⊆[n](−2)|S| 
i∈S xi with deg(f) = n from the
Mobius inverse formula, the representation over F2 is 
i xi with deg2(f) = 1
by taking each coeﬃcient modulo 2, and similarly the representation over F3 is

∅̸=S⊆[n]

i∈S xi with deg3(f) = n. Indeed, it is not hard to see that degp(f) =
n for every prime p ̸= 2.

On the Modulo Degree Complexity of Boolean Functions
387
Fact 2. For any Boolean function f, we have deg(f) ≥degm(f) for all m.
Similarly degm(f) ≥degm′(f) if m′|m.
The above fact implies degm(f) ≤degmk(f). The following fact shows that
they are always within a factor 2k −1 of each other.
Fact 3. For any Boolean function f, and any integers m ≥2, k ≥1, we have
degm(f) ≤degmk(f) ≤(2k −1) degm(f).
Now recall the function f(x) = (x1 + · · · + xn)m−1 mod m with degm(f) ≤
m−1 and deg(f) = Ω(n) for prime m, as mentioned in the introduction. Indeed,
such functions also exist for power of primes.
Fact 4. For any prime power m, there exists a sequence of functions f such
that degm(f) = O(1) and deg(f) = Ω(n).
The following fact is a consequence of the Chinese Remainder Theorem,
Fact 5. For any Boolean function f and any m and m′ with gcd(m, m′) = 1,
we have
degm′m(f) = max{degm′(f), degm(f)}.
Due to Facts 2 and 5, we get an equivalent form of Conjecture 1 straightfor-
wardly:
Conjecture 2. Let f be a boolean function, p and q be two distinct primes, then
deg(f) ≤poly(degp(f), degq(f)).
Next, we give the deﬁnitions of some other complexity measures which will
be used in this paper. For an input x ∈{0, 1}n and a subset B, xB denotes the
input obtained by ﬂipping all the bit xj such that j ∈B.
Deﬁnition 1. The sensitivity complexity of f on input x is deﬁned as s(f, x) :=
|{i : f(x) ̸= f(xi)}|. The sensitivity complexity of the function f is deﬁned as
s(f) := maxxs(f, x).
It has been shown that s(f) = O(deg(f)2)[19], but whether deg(f) can be
polynomially bounded in terms of s(f) is still open today, actually it is what the
famous sensitivity conjecture asks [11].
Deﬁnition 2. The block sensitivity bs(f, x) of f on input x is the maximum
number of disjoint subsets B1, B2, · · · , Br of [n] such that for all j, f(x) ̸=
f(xBj). The block sensitivity of f is deﬁned as bs(f) = maxxbs(f, x), and the
minimum block sensitivity of f is deﬁned as bsmin(f) = minxbs(f).
Deﬁnition 3. Let C be an assignment C : S →{0, 1} of values to some subsets
S ⊆[n]. We say C is consistent with x ∈{0, 1}n if xi = C(i) for all i ∈S.
For b ∈{0, 1}, a b−certiﬁcate for f is an assignment C such that f(x) = b
whenever x is consistent with C. The size of C is |S|.
The certiﬁcate complexity C(f, x) of f on input x is the size of a small-
est f(x)-certiﬁcate that is consistent with x. The certiﬁcate complexity of f is
C(f) = maxx C(f, x). The minimum certiﬁcate complexity of f is Cmin(f) =
minx C(f, x).

388
Q. Li and X. Sun
Deﬁnition 4. Let m ≥2 be an integer, the mod-m rank of a boolean function
f, denoted by rankm(f), is the minimum integer r s.t. f can be expressed as
f = xi1f1 + · · · + xirfr + f0
(mod m),
where degm(fi) < degm(f) for all 0 ≤i ≤r. Equivalently, rankm(f) is the
minimum number of variables to hit all largest monomials in Pm(x). Here we
say a monomial is largest if it has maximal degree.
Since we have to ﬁx at least rankm(f) variables to make all the largest
monomials in Pm(x) vanish, thus rankm(f) ≤Cmin(f) for any m. Cmin(f),
bsmin(f) and rankm(f) are all polynomially bounded by deg(f), since
{bsmin(f), rankm(f)} ≤Cmin(f) ≤C(f) = O(deg(f)3)[17], and sometimes they
can be very small: rankm(ANDn) = bsmin(ANDn) = Cmin(ANDn) = 1 ≪n =
deg(ANDn).
Deﬁnition 5. For a Boolean function f : {0, 1}n →{0, 1}, we deﬁne the
alternating number alt(f) of f to be the largest k such that there exist a list
{x(1), x(2), · · · , x(k+1)} with x(i) ⪯x(i+1) and f(x(i)) ̸= f(x(i+1)) for any i ∈[k].
Here we say x ⪯y if xi ≤yi for all i.
Deﬁnition 6. A Boolean function f is symmetric if for every input x =
x1, · · · , xn and every permutation σ ∈Sn,
f(x1, · · · , xn) = f(xσ(1), · · · , xσ(n)).
A Boolean string can represent a graph in the following manner: x(i,j) = 1
means there is an edge connecting vertex i and vertex j, and x(i,j) = 0 means
there is no such edge. Graph properties are functions which are independent
with the labeling of vertices, i.e. two isomorphic graphs have the same function
value.
Deﬁnition 7. A Boolean function f : {0, 1}(
n
2) →{0, 1} is called a graph prop-
erty if for every input x = (x(1,2), · · · , x(n−1,n)) and every permutation σ ∈Sn,
f(x(1,2), · · · , x(n−1,n)) = f(x(σ(1),σ(2)), · · · , x(σ(n−1),σ(n))).
Similarly, we deﬁne k-uniform hypergraph properties.
Deﬁnition 8. A Boolean function f : {0, 1}(
n
k) →{0, 1} is called a k-uniform
hypergraph property if for every input x = (x(1,2,...,k), · · · , x(n−k+1,...,n−1,n)) and
every permutation σ ∈Sn,
f(x(1,2,...,k), · · · , x(n−k+1,...,n−1,n)) = f(x(σ(1),σ(2),...,σ(k)), · · · , x(σ(n−k+1),...,σ(n−1),σ(n))).
It is easy to see graph property is 2-uniform hypergraph property.

On the Modulo Degree Complexity of Boolean Functions
389
3
Equivalent Conjectures
Observe that the deg(f) on the left side in Conjectures 1 and 2 can be replaced by
any other complexity measures which are polynomially related with deg(f), such
as D(f), bs(f) etc. [4], to get equivalent conjectures. Surprisingly, we ﬁnd that
we can also replace it with some smaller complexity measures, such as rankp(f),
Cmin(f), bsmin(f) and s(f). In the following, we prove them one by one.
Conjecture 3. Let f be a boolean function, p and q be two distinct primes, then
rankp(f) ≤poly(degp(f), degq(f)).
Theorem 5. Conjecture 3 ⇐⇒Conjecture 2.
Proof. ⇐=: Trivial, since rankp(f) = O(deg(f)3), as mentioned above.
=⇒: We design an algorithm to query f, which contains at most degp(f)
rounds and each round reduces degp by at least one. Denote the function at
round t by f (t). Note that f (t) is a subfunction of f, hence degp(f (t)) ≤degp(f)
and degq(f (t)) ≤degq(f). For each round, we can query rankp(f (t)) variables
to make the largest monomials in Pp(x) vanish, which means degp(f (t)) is
reduced by at least one. Therefore assuming Conjecture 3, we have rankp(f (t)) ≤
poly(degp(f (t)), degq(f (t))) ≤poly(degp(f), degq(f)), which implies deg(f) ≤
D(f) ≤poly(degp(f), degq(f)).
Recall that rankp(f) ≤Cmin(f) = O(deg(f)3), we get another equivalent
conjecture.
Conjecture 4. Let f be a boolean function, p and q be two distinct primes, then
Cmin(f) ≤poly(degp(f), degq(f)).
Now, we show deg(f) in Conjecture 2 can be replaced with bsmin(f):
Conjecture 5. Let f be a boolean function, p and q be two distinct primes, then
bsmin(f) ≤poly(degp(f), degq(f)).
Theorem 6. Conjecture 5 ⇐⇒Conjecture 2.
Proof. ⇐=: Directly follows from bsmin(f) ≤bs(f) = O(deg(f)2) [19].
=⇒: We call monomial M maximal in Pp(x) if no other monomials contains
it. Observe that for any input x and any maximal monomial M, there exist
a block B ⊆supp(M) such that f(x) ̸= f(xB), because for any restriction S:
[n]\M →{0, 1} monomial M can’t be cancelled, which implies f|S is a non-
constant function. In addition, according to the deﬁnition of rankp(f), there
exists at least rankp(f)/ degp(f) disjoint largest monomials in Pp(x). There-
fore we get bsmin(f) ≥rankp(f)/ degp(f), which implies Conjecture 3 assuming
Conjecture 5.

390
Q. Li and X. Sun
Finally, we show deg(f) in Conjecture 2 can be replaced with s(f). The key
technique is called ”replacing”: just replace the occurrences of xi with xj, i.e., the
new function is f(· · · , xi, · · · , xi, · · · ). Note that xi in the corresponding Pm(x)
are also replaced with xj, thus degm(f) cannot increase, and the new function
is still boolean.
For example, let P2(x) = x1x2 + x1x3 + x2x3 and the corresponding P3(x)
is x1x2 + x1x3 + x2x3 + x1x2x3. If we replace x2 with x1, the new P2(x) is
x1x1 +x1x3 +x1x3 = x1 and the new P3(x) is x1x1 +x1x3 +x1x3 +x1x1x3 = x1.
Conjecture 6. Let f be a boolean function, p and q be two distinct primes, then
s(f) ≤poly(degp(f), degq(f)).
Theorem 7. Conjecture 2 ⇐⇒Conjecture 6.
The following simpliﬁed proof is observed by Shachar Lovett.
Proof. =⇒: Recall s(f) = O(deg(f)2) [19], thus Conjecture 3 ⇒Conjecture 2
⇒Conjecture 6.
⇐=: W.L.O.G, assume bs(f, 0) = bs(f) = r, thus there exist r disjoint blocks
B1, · · · , Br ⊆[n] such that for all i, f(0) ̸= f(0Bi). Further, we assume that
i ∈Bi. Now, we “replace” all variables in Bi with xi to get a new function f ′.
It is easy to see that f ′(0) = f(0) ̸= f(0Bi) = f ′(0i), thus
bs(f) = s(f ′) ≤poly(degp(f ′), degq(f ′)) ≤poly(degp(f), degq(f)),
Now we get the conclusion immediately by noting that bs(f) and deg(f) are
polynomially related [4].
4
Special Classes of Functions
In this section, we conﬁrm Conjecture 1 for some special classes of functions.
4.1
Symmetric Functions
Chia-Jung Lee et al. [14] already conﬁrmed the case of symmetric functions by
showing that 2 degp1(f) degp2(f) > n. Here, we give another proof with better
parameters.
Theorem 8. Let f : {0, 1}n →{0, 1} be symmetric and nonconstant, and p1,
p2 are two distinct primes. Then
deg(f) ≤n < p1 degp1(f) + p2 degp2(f).

On the Modulo Degree Complexity of Boolean Functions
391
Proof. For the sake of the presentation, let di = degpi(f) and Li = p
1+⌊logpi di⌋
i
.
Since f is symmetric, each Ppi(x) can be written as di
k=0 ci,k
|x|
k

. Then accord-
ing to Lucas formula, for any nonnegative integers s, j and k ≤di, we have
sLi + j
k

≡pi
j
k

.
Deﬁne g(|x|) = f(x), the above equality says g(k +Li) = g(k). Next, we want to
show n < L1 + L2, which implies n < p1d1 + p2d2. Note that L1 ̸= L2, w.l.o.g.,
assume L1 < L2.
Suppose n ≥L1 + L2, we claim that ∀k ≤L2, g(k) = g(k + L1 mod L2),
this is because if k + L1 ≤L2, it’s trivial, otherwise, g(k) = g(k + L1) = g(k +
L1 −L2) = g(k + L1 mod L2). Moreover, gcd(L1, L2) = 1, hence ∀l ≤L2, there
exists a integer t such that l −k ≡L2 tL1, i.e. g(k) = g(k + tL1 mod L2) = g(l),
which means f is constant, a contradiction.
Corollary 1. Let f : {0, 1}n →{0, 1} be symmetric and nonconstant, p1
< p2 < · · · pr be distinct primes, and r and ei’s be positive integers. Let
m = Πr
i=1pei
i . Then
deg(f) ≤n < degm(f)(p1 + p2).
Proof. First we have degm(f) ≥degp1p2(f) = max{degp1(f), degp2(f)}. Then
according to the above theorem, (p1+p2) degp1p2(f) ≥p1degp1(f)+p2degp2(f) >
n, as expected.
4.2
Uniform Hypergraph Properties
Using Theorem 8, we can conﬁrm Conjecture 2 for all k-uniform hypergraph
properties, where k is a constant. For the reader’s convenience, we restate The-
orem 1 here.
Theorem 1. For any non-trivial k-uniform hypergraph property f on n vertices
and any integer m with distinct prime factors p1 and p2, we have
1
p1 + p2 + k n ≤degm(f),
which implies
deg(f) ≤
n
k

= O(degm(f)k).
Proof. (The proof is similar with Lemma 8 in [22].) W.l.o.g., we assume that for
the empty graph Kn, f(Kn) = 0. Since f is non-trivial, there must exist a graph
G such that f(G) = 1. Let’s consider graphs in f −1(1) = {G : f(G) = 1} with
the minimum number of edges. Deﬁne m = min{|E(G)| : f(G) = 1}.
We claim that if m ≥
1
p1+p2+kn, then degm(f) ≥
1
p1+p2+kn. Let G be a graph
in f −1(1) and |E(G)| = m. Consider the subfunction f ′ where ∀e /∈E(G), xe

392
Q. Li and X. Sun
is restricted to 0, since G has the the minimum number of edges, deleting any
edges from G will change the values of f(G), therefore, f ′ is a AND function.
Thus, degm(f) ≥degm(f ′) = m ≥
1
p1+p2+kn.
In the following we assume m <
1
p1+p2+kn. Again let G be a graph in f −1(1)
with |E(G)| = m. Let us consider the isolated vertices set I, as

v∈V
deg(v) = k|E(G)| <
k
p1 + p2 + k n.
We have
|I| ≥n −

v∈V
deg(v) >
p1 + p2
p1 + p2 + k n.
Suppose degm(f) <
1
p1+p2+kn, we will deduce that there exists another graph
with fewer edges and the same value, against the assumption that G has the
minimum number of edges in f −1(1), which ends the whole proof.
Pick a vertex u with deg(u) = d > 0. Suppose in the graph G vertex u is
adjacent to (k −1)-edges {e(k−1)
1
, e(k−1)
2
, · · · , e(k−1)
d
} and I = {u1, u2, · · · , ut},
where t = |I|.
Consider the t-variable Boolean function g1: {0, 1}t →{0, 1}, where
g1(x1, · · · , xt) = f(G + x1(e(k−1)
1
, u1) + · · · + xt(e(k−1)
1
, ut)).
It is easy to see that g1 is a symmetric function. We claim that g1 is a con-
stant function: if not, we have degm(g1) ≥
1
p1+p2 t according to Corollary 1,
which implies degm(f) ≥
1
p1+p2+kn since g1 is a restriction of f. In particular,
g1(1, · · · , 1) = g1(0, · · · , 0), i.e. f(G1) = f(G), where G1 = G+t
i=1(e(k−1)
1
, ui).
Deﬁne Gi = Gi−1 + t
j=1(e(k−1)
i
, uj) (i = 2, · · · , d). Similarly, we can show
that
f(G) = f(G1) = · · · = f(Gd).
Next
we
will
delete
all the edges between {u, u1, · · · , ut} and {e(k−1)
1
, e(k−1)
2
, · · · , e(k−1)
d
} from Gd
by reversing the adding edge procedure of G →G1 →· · · →Gd. More precisely,
deﬁne H1 = Gd; for i = 2, · · · , d, deﬁne
Hi = Hi−1 −(e(k−1)
i
, u) −(e(k−1)
i
, u1) −· · · −(e(k−1)
i
, ut),
and
hi(y0, y1, · · · , yt) = f(Hi + y0(e(k−1)
i
, u) + y1(e(k−1)
i
, u1) + · · · + yt(e(k−1)
i
, ut)).
Similarly, by the fact degm(f) <
1
p1+p2+kn we can show that all the functions
h2, · · · , hd are constant, which implies f(H1) = f(H2) = · · · = f(Hd). So we
ﬁnd another graph Hd with fewer edges than G and f(Hd) = 1.

On the Modulo Degree Complexity of Boolean Functions
393
4.3
Functions with Small Alternating Numbers
We can also conﬁrm the functions with small alternating numbers.
Theorem 2. Let f be a boolean function, then for any m ≥2,
D(f) = O(alt(f) · degm(f)2),
which implies
deg(f) = O(alt(f) · degm(f)2).
Recall that degm(f) = Ω(log n) when m has two distinct prime factors [8],
thus the above theorem conﬁrms Conjecture 1 for non-degenerate functions with
alt(f) = poly log(n).
Lin and Zhang [15] have shown the case m = 2, and their argument applies
to general m as well. We omit the proof here.
5
A No-Go Theorem and a Super-Linear Separation
The following theorem somehow explains why it’s hard to solve Conjecture 2,
even for the simplest case where p = 2 and q = 3.
Theorem 3. Given a polynomial P2(x1, x2, . . . , xn) over F2 with poly(n) mono-
mials, it’s impossible to decide whether deg3(P2) = n or not in polynomial time,
unless NP = RP.
Proof. It’s suﬃcient to give a reduction to Unique-3CNF, since Unique-3CNF
can’t be solved in polynomial time unless NP = RP [24].
Given a Unique-3CNF formula φ(x1, · · · , xn) with m clauses, we ﬁrst remove
negated literals to make the formula monotone: for any variable xi replace the
occurrences of its negation by a new variable x⋆
i . Also introduce new variables x′
i
and x′′
i and conjoin φ with the clauses (xi ∨x⋆
i )∧(xi ∨x′
i)∧(x⋆
i ∨x′′
i )∧(x′
i ∨x′′
i ).
Denote the new formula by φ′ with n′ variables and m′ clauses. It is easy to see
that ♯φ ≡−♯φ′ mod 3. Here ♯φ is the number of solutions of φ, i.e., ♯φ = ♯{x :
φ(x) = 1}.
Then we construct a polynomial P2 over F2 from φ′: There are m′ variables
y1, y2, · · · , ym′ and n′ monomials t1, t2, · · · , tn′ in P2, and ti contains yj if the
jth clause contains xi in φ′.
Note that the corresponding polynomial over F3 is
P3 = 1
2[1 −
m′
	
i=1
(1 −2ti)] =
m′
	
i=1
(1 + ti) −1,
According to the fact that yl
i = yi for any integer l ≥1 and any yi ∈{0, 1}, it is
not hard to see the coeﬃcient of m′
i=1 yi is ♯φ′ mod 3. Note that φ has at most
one solution, then φ is satisﬁable if and only if ♯φ ≡−♯φ′ ≡1 mod 3, which
means deg3(P2) = n.

394
Q. Li and X. Sun
In the direction to disprove Conjecture 1, we give a quadratic separation.
Theorem 4. For any two distinct prime p1 and p2, there exists a sequence of
boolean functions f, s.t:
degp1p2(f) = O(deg(f)1/2).
Proof. Let
f=Mod3(Mod2(x1, · · · , x√n), · · · , Mod2(xn−√n+1, · · · , xn)).
Here,
Modpi(·) = 0, if the sum of inputs can be divided by pi, otherwise Modpi(·) = 1.
On one hand, since Modpi(·) is symmetric, it is easy to see deg(f) = Ω(n). On the
other hand, it is also not hard to see that degpi(f) = O(√n) for each i, which
implies degp1p2(f) = O(√n).
6
Conclusion
In this work, we investigate the relationship between deg(f) and degm(f), more
speciﬁcally, we focus on an open problem proposed by Gopalan et al. in [8],
which asks whether deg(f) and degm(f) are polynomially related, when m has
at least two distinct prime factors. First we present some nontrivial equivalent
forms of this problem, then we aﬃrm it for some special classes of functions.
Finally we show a no-go theorem by which try to explain why this problem is
hard, as well as a super-linear separation. Most of the problems remain open,
here we list some of them:
1. Can we prove Conjecture 1 for cyclically invariant functions ﬁrst?
2. Given a polynomial P2 over F2 with poly(n) size, we have shown that there’s
no eﬃcient algorithms to compute its modulo 3 degree exactly unless NP =
RP. Is it still hard to approximate that?
Acknowledgments. We thank the anonymous reviewer for pointing out the better
construction in Theorem 4, and Shachar Lovett for providing us the simple proof of
Theorem 7.
References
1. Aspnes, J., Beigel, R., Furst, M.L., Rudich, S.: The expressive power of voting
polynomials. Combinatorica 14(2), 135–148 (1994)
2. Beigel, R., Reingold, N., Spielman, D.A.: The perceptron strikes back. In: Proceed-
ings of the Sixth Annual Structure in Complexity Theory Conference, pp. 286–291
(1991)
3. Bhatnagar, N., Gopalan, P., Lipton, R.J.: Symmetric polynomials over zm and
simultaneous communication protocols. J. Comput. Syst. Sci. 72(2), 252–285
(2006)
4. Buhrman, H., De Wolf, R.: Complexity measures and decision tree complexity: a
survey. Theor. Comput. Sci. 288(1), 21–43 (2002)
5. Efremenko, K.: 3-query locally decodable codes of subexponential length. SIAM J.
Comput. 41(6), 1694–1703 (2012)

On the Modulo Degree Complexity of Boolean Functions
395
6. Gopalan, P.: Computing with Polynomials over Composites. Ph.D. thesis (2006)
7. Gopalan, P.: Constructing ramsey graphs from boolean function representations.
Combinatorica 34(2), 173–206 (2014)
8. Gopalan, P., Shpilka, A., Lovett, S.: The complexity of boolean functions in diﬀer-
ent characteristics. Comput. Complex. 19(2), 235–263 (2010)
9. Grolmusz, V.: Superpolynomial size set-systems with restricted intersections mod
6 and explicit ramsey graphs. Combinatorica 20(1), 71–86 (2000)
10. Grolmusz, V.: Constructing set systems with prescribed intersection sizes. J. Algo-
rithms 44(2), 321–337 (2002)
11. Hatami, P., Kulkarni, R., Pankratov, D.: Variations on the sensitivity conjecture.
Theor. Comput. Grad. Surv. 4, 1–27 (2011)
12. Klivans, A.R., Servedio, R.A.: Learning DNF in time 2˜o(n1/3). J. Comput. Syst.
Sci. 68(2), 303–318 (2004)
13. Kushilevitz, E., Mansour, Y.: Learning decision trees using the fourier spectrum.
SIAM J. Comput. 22(6), 1331–1348 (1993)
14. Lee, C.J., Lokam, S.V., Tsai, S.C., Yang, M.C.: Restrictions of nondegenerate
boolean functions and degree lower bounds over diﬀerent rings. In: Proceedings of
IEEE International Symposium on Information Theory, pp. 501–505 (2015)
15. Lin, C., Zhang, S.: Sensitivity conjecture and log-rank conjecture for functions with
small alternating numbers. CoRR, abs/1602.06627 (2016)
16. Linial, N., Mansour, Y., Nisan, N.: Constant depth circuits, fourier transform, and
learnability. J. ACM 40(3), 607–620 (1993)
17. Midrijanis, G.: Exact quantum query complexity for total boolean functions. arXiv
preprint quant-ph/0403168 (2004)
18. Mossel, E., O’Donnell, R., Servedio, R.A.: Learning juntas. In: Proceedings of the
35th Annual ACM Symposium on Theory of Computing, pp. 206–212 (2003)
19. Nisan, N., Szegedy, M.: On the degree of boolean functions as real polynomials.
Comput. Complex. 4, 301–313 (1994)
20. Razborov, A.A.: Lower bounds for the size of circuits of bounded depth with basis
{∧, ⊕}. Math. Notes Acad. Sci. USSR 41, 333–338 (1987)
21. Smolensky, R.: Algebraic methods in the theory of lower bounds for boolean circuit
complexity. In: Proceedings of the 19th Annual ACM Symposium on Theory of
Computing, pp. 77–82 (1987)
22. Sun, X.: An improved lower bound on the sensitivity complexity of graph proper-
ties. Theor. Comput. Sci. 412(29), 3524–3529 (2011)
23. Tsang, H.Y., Wong, C.H., Xie, N., Zhang, S.: Fourier sparsity, spectral norm,
and the log-rank conjecture. In: Proceedings of 54th Annual IEEE Symposium on
Foundations of Computer Science, pp. 658–667 (2013)
24. Valiant, L.G., Vazirani, V.V.: NP is as easy as detecting unique solutions. Theor.
Comput. Sci. 47, 85–93 (1986)
25. Viola, E.: The sum of D small-bias generators fools polynomials of degree D. Com-
put. Complex. 18(2), 209–217 (2009)

Approximating Weighted Duo-Preservation
in Comparative Genomics
Saeed Mehrabi(B)
Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada
smehrabi@uwaterloo.ca
Abstract. Motivated by comparative genomics, Chen et al. [9] intro-
duced the Maximum Duo-preservation String Mapping (MDSM) problem
in which we are given two strings s1 and s2 from the same alphabet and
the goal is to ﬁnd a mapping π between them so as to maximize the num-
ber of duos preserved. A duo is any two consecutive characters in a string
and it is preserved in the mapping if its two consecutive characters in s1
are mapped to same two consecutive characters in s2. The MDSM problem
is known to be NP-hard and there are approximation algorithms for this
problem [3,5], all of which consider only the “unweighted” version of the
problem in the sense that a duo from s1 is preserved by mapping to
any same duo in s2 regardless of their positions in the respective strings.
However, it is well-desired in comparative genomics to ﬁnd mappings
that consider preserving duos that are “closer” to each other under some
distance measure [18].
In this paper, we introduce a generalized version of the problem, called
the Maximum-Weight Duo-preservation String Mapping (MWDSM) prob-
lem, capturing both duos-preservation and duos-distance measures in
the sense that mapping a duo from s1 to each preserved duo in s2 has a
weight, indicating the “closeness” of the two duos. The objective of the
MWDSM problem is to ﬁnd a mapping so as to maximize the total weight of
preserved duos. We give a polynomial-time 6-approximation algorithm
for this problem.
1
Introduction
Strings comparison is one of the central problems in the ﬁeld of stringology with
many applications such as in Data Compression and Bioinformatics. One of the
most common goals of strings comparison is to measure the similarity between
them, and one of the many ways in doing so is to compute the edit distance
between them. The edit distance between two strings is deﬁned as the minimum
number of edit operations to transform one string into the other. In biology,
during the process of DNA sequencing for instance, computing the edit distance
between the DNA molecules of diﬀerent species can provide insight about the
level of “synteny” between them; here, each edit operation is considered as a
single mutation.
In the simplest form, the only edit operation that is allowed in computing
the edit distance is to shift a block of characters; that is, to change the order of
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 396–406, 2017.
DOI: 10.1007/978-3-319-62389-4 33

Approximating Weighted Duo-Preservation
397
s1:
b
a
c
b
d
c
d
d
b
a
a
c
s2:
a
c
d
b
b
c
a
d
d
a
c
b
Fig. 1. An instance of the MDSM problem in which the mapping π preserves three duos.
the characters in the string. Computing the edit distance under this operation
reduces to the Minimum Common String Partition (MCSP) problem, which was
introduced by Goldstein et al. [14] (see also [20]) and is deﬁned as follows. For
a string s, let P(s) denote a partition of s. Given two strings s1 and s2 each of
length n, where s2 is a permutation of s1, the objective of the MCSP problem
is to ﬁnd a partition P(s1) of s1 and P(s2) of s2 of minimum cardinality such
that P(s2) is a permutation of P(s1). The problem is known to be NP-hard and
even APX-hard [14].
Recently, Chen et al. [9] introduced a maximization version of the MCSP
problem, called the Maximum Duo-preservation String Mapping (MDSM) problem.
A duo in a string s is a pair of consecutive characters in s. For two strings s1
and s2, where s2 is a permutation of s1 under a mapping π, we say that a duo
is preserved in the mapping π, if its two consecutive characters are mapped to
same two consecutive characters in s2. Notice that if partitions P(s1) and P(s2)
are a solution of size r for an instance of the MCSP problem, then this solution
can be used to obtain a mapping π from s1 to s2 that preserve n −r duos.
As such, given two strings s1 and s2, the objective of the MDSM problem is to
compute a mapping π from s1 to s2 that preserves a maximum number of duos.
See Fig. 1 for an example.
Related Work. Since the MCSP problem is NP-hard [14], there has been many
works on designing polynomial-time approximation algorithms for this prob-
lem [10–12,14,17]. The best approximation results thus far are an O(log n log∗n)-
approximation algorithm for the general version of the problem [12], and an
O(k)-approximation for the k-MCSP problem [12] (the k-MCSP is a variant of
the problem in which each character can appear at most k times in each string).
In terms of the parameterized complexity, the problem is known to be ﬁxed-
parameter tractable with respect to k and the size of an optimal partition [6,16],
as well as the size of an optimal solution only [7]. For the MDSM problem, we
observe that since the MCSP problem is NP-hard [14], the MDSM problem (i.e.,
its maximization version) is also NP-hard, and in fact even APX-hard [4]. More-
over, the problem is also shown to be ﬁxed-parameter tractable with respect to
the number of duos preserved [2]. Boria et al. [4] gave a 4-approximation algo-
rithm for the MDSM problem, which was subsequently improved to algorithms
with approximation factors of 7/2 [3], 3.25 [5] and (recently) (2 + ϵ) for any
ϵ > 0 [13].

398
S. Mehrabi
Motivation and Problem Statement. Observe that in the MDSM problem
mapping a duo from s1 to a preserved duo in s2 does not consider the position
of the two duos in s1 and s2. In Fig. 1, for instance, the ﬁrst ac in s1 is mapped
to the second ac in s2 and the second ac in s1 is mapped to the ﬁrst ac in
s2. But, another (perhaps more realistic) mapping would be the one that maps
the ﬁrst ac in s1 to the ﬁrst ac in s2 and the second one in s1 to the second
one in s2. The latter mapping would become more desirable when solving the
problem on strings of extremely long length. In fact, considering the applications
of the MDSM problem in comparative genomics, it is much more desirable to ﬁnd
mappings that take into account the position of the preserved features in the
two sequences [15,18]. One reason behind this is the fact that focusing on giving
priority to preserving features that are “closer” to each other (distance-wise
under some distance measure) provides better information about the “synteny”
of the corresponding species [15,18].
In this paper, we introduce a more general variant of the MDSM problem,
called the Maximum-Weight Duo-preservation String Mapping (MWDSM) problem.
In this problem, in addition to s1 and s2, we are also given a weight function
deﬁned on pairs of duos that considers the position of the duos in s1 and s2, and
so better captures the concept of “synteny” in comparative genomics. Now, the
objective becomes maximizing the total weight of the preserved duo (instead of
maximizing the number of them). Let us deﬁne the problem more formally. For
a string s, we denote by D(s) the set of all duos in s ordered from left to right.
For example, if s = acbbda, then D(s) = {ac, cb, bb, bd, da}.
Deﬁnition 1 (The MWDSM Problem). Let s1 and s2 be two strings of length
n. Moreover, let w : D(s1) × D(s2) →R+ denote a weight function. Then, the
MWDSM problem asks for a mapping π from s1 to s2 that preserve a set S of duos
such that

d∈S
w(d, π(d))
is maximized over all such sets S, where π(d) denotes the duo in s2 to which
d ∈s1 is mapped.
We note that the weight function is very ﬂexible in the sense that it can
capture any combination of duos-preservation and duos-distance measures. To
our knowledge, this is the ﬁrst formal study of a “weighted version” of the
MDSM problem.
Our Result. Notice that the MWDSM problem is NP-hard as its unweighted
variant (i.e., the MDSM problem) is known to be NP-hard [14]. We note that
the previous approximation algorithms for the MDSM problem do not apply to
the MWDSM problem. In particular, both 7/2-approximation algorithm of Boria
et al. [3] and (2 + ϵ)-approximation algorithm of Dudek et al. [13] are based on
the local search technique, which is known to fail for weighted problems [8,19].
Moreover, the 3.25-approximation algorithm of Brubach [5] relies on a triplet
matching approach, which involves ﬁnding a weighted matching (with specialized
weights) on a particular graph, but it is not clear if the approach could handle
the MWDSM problem with any arbitrary weight function w. Finally, while the linear

Approximating Weighted Duo-Preservation
399
programming algorithm of Chen et al. [9] might apply to the MWDSM problem,
the approximation factor will likely stay the same, which is k2, where k is the
maximum number of times each character appears in s1 and s2.
In this paper, we give a polynomial-time 6-approximation algorithm for the
MWDSM problem for any arbitrary weight function w. To this end, we construct
a vertex-weighted graph corresponding to the MWDSM problem and show that
the problem reduces to the Maximum-Weight Independent Set (MWIS) problem
on this graph. Then, we apply the local ratio technique to approximate the
MWIS problem on this graph. The local ratio technique was introduced by Bar-
Yehuda and Even [1], and is used for designing approximation algorithms for
mainly weighted optimization problems (see Sect. 2 for a formal description of
this technique). While the approximation factor of our algorithm is slightly large
in compare to that of algorithms for the unweighted version of the problem [3,
5], as we now have weights, our algorithm is much simpler as it beneﬁts from
the simplicity of the local ratio technique. To our knowledge, this is the ﬁrst
application of the local ratio technique to problems in stringology.
Organization. The paper is organized as follows. We ﬁrst give some deﬁnitions
and preliminary results in Sect. 2. Then, we present our 6-approximation algo-
rithm in Sect. 3, and will conclude the paper with a discussion on future work
in Sect. 4.
2
Preliminaries
In this section, we give some deﬁnitions and preliminaries. For a graph G, we
denote the set of vertices and edges of G by V (G) and E(G), respectively. For
a vertex u ∈V (G), we denote the set of neighbours of u by N[u]; note that
u ∈N[u].
Let w ∈Rn be a weight vector, and let F be a set of feasibility constraints
on vectors x ∈Rn. A vector x ∈Rn is a feasible solution to a given problem
(F, p) if it satisﬁes all of the constraints in F. The value of a feasible solution
x is the inner product w · x. A feasible solution is optimal for a maximization
(resp., minimization) problem if its value is maximal (resp., minimal) among
all feasible solutions. A feasible solution x is an α-approximation solution, or
simply an α-approximation, for a maximization problem if w · x ≥α · w · x∗,
where x∗is an optimal solution. An algorithm is said to have an approximation
factor of α (or, it is called an α-approximation algorithm), if it always computes
α-approximation solutions.
Local Ratio. Our approximation algorithm uses the local ratio technique. This
technique was ﬁrst developed by Bar-Yehuda and Even [1]. Let us formally state
the local ratio theorem.
Theorem 1.
[1] Let F be a set of constraints, and let w, w1 and w2 be weight
vectors where w = w1 + w2. If x is an α-approximation solution with respect to
(F, w1) and with respect to (F, w2), then x is an α-approximation solution with
respect to (F, w).

400
S. Mehrabi
We now describe how the local ratio technique is usually used for solving a
problem. First, the solution set is empty. The idea is to ﬁnd a decomposition of
the weight vector w into w1 and w2 such that w1 is an “easy” weight function in
some sense (we will discuss this in more details later). The local ratio algorithm
continues recursively on the instance (F, w2). We assume inductively that the
solution returned recursively for the instance (F, w2) is a good approximation
and need to prove that it is also a good approximation for (F, w). This requires
proving that the solution returned recursively for the instance (F, w2) is also a
good approximation for the instance (F, w1). This step is usually the main part
of the proof of the approximation factor.
Graph GI. Given an instance of the MWDSM problem, we ﬁrst construct a bipartite
graph GI = (A∪B, E) as follows. The vertices in the left-side set A are the duos
in D(s1) in order from top to bottom and the vertices in the right-side set
B are the duos in D(s2) in order from top to bottom. There exists an edge
between two vertices if and only if they represent the same duo. See Fig. 2 for
an example. Boria et al. [4] showed that the MDSM problem on s1 and s2 reduces
to the Maximum Constrained Matching (MCM) problem on GI, which is deﬁned
as follows. Let A = a1, . . . , an and B = b1, . . . , bn. Then, we are interested in
computing a maximum-cardinality matching M such that if (ai, bj) ∈M, then
ai+1 can be only matched to bj+1 and bj+1 can be only matched to ai+1. In the
following, we ﬁrst assign weights to the edges of GI and will then show that a
similar reduction holds between the MWDSM problem on s1 and s2, and a weighted
version of the MCM problem on GI.
To weigh the edges of GI, we simply assign w(al, br) as the weight of e, for all
e ∈E(GI), where al ∈A and br ∈B are the endpoints of e and w(al, br) is given
by Deﬁnition 1. Now, we deﬁne the Maximum-Weight Constrained Matching
(MWCM) problem as the problem of computing a maximum-weight matching M
in GI such that if (ai, bj) ∈M, then ai+1 can be only matched to bj+1 and bj+1
can be only matched to ai+1. To see the equivalence between the MWDSM problem
on s1 and s2 and the MWCM problem on GI, let S be a feasible solution to the
MWDSM problem with total weight w(S) determined by a mapping π. Then, we
can obtain a feasible solution S′ for the MWCM problem on GI by selecting the
edges in GI that correspond to the preserved duos in S determined by π such
that w(S′) = w(S). Moreover, it is not too hard to see that any feasible solution
for the MWCM problem on GI gives a feasible solution for the MWDSM problem with
the same weight. This gives us the following lemma.
Lemma 1. The MWDSM problem on s1 and s2 reduces to the MWCM problem on GI.
By Lemma 1, any feasible solution M with total weight w(M) for the MWCM
problem on GI gives a mapping π between the strings s1 and s2 that preserves
a set of duos with total weight w(M). As such, for the rest of this paper, we
focus on the MWCM problem on GI and give a polynomial-time 6-approximation
algorithm for this problem on GI, which by Lemma 1, results in an approxima-
tion algorithm with the same approximation factor for the MWDSM problem on s1
and s2.

Approximating Weighted Duo-Preservation
401
Fig. 2. The graph GI corresponding to s1 = abacddd and s2 = acddbad.
3
Approximation Algorithm
In this section, we give a 6-approximation algorithm for the MWCM problem. We
were unable to apply the local ratio directly to the MWCM problem on GI due
to the constraint involved in the deﬁnition of the problem. Instead, we con-
struct a vertex-weighted graph GC, called the conﬂict graph, and show that the
MWCM problem on GI reduces to the Maximum-Weight Independent Set (MWIS)
problem on GC. We then apply the local ratio to approximate the MWIS problem
on GC, which results in an approximation algorithm for the MWCM problem on GI.
Consequently, this gives us an approximation algorithm for the MWDSM problem
on s1 and s2 by Lemma 1.
Graph GC. We now describe the concept of conﬂict. We say that two edges
in E(GI) are conﬂicting if they both cannot be in a feasible solution for the
MWCM problem at the same time, either because they share an endpoint or their
endpoints are consecutive on one side of the graph, but not on the other side.
The following observation is immediate.
Observation 1. Let e1 = (ai, bj) and e2 = (ak, bl) be two conﬂicting edges in
E(GI). Then, either k ∈{i −1, i, i + 1} or l ∈{j −1, j, j + 1}.
1
2
3
4
We deﬁne the conﬂict graph GC as follows. Let V (GC)
be E(GI); that is, V (GC) is the set of all edges in GI.
For a vertex i ∈V (GC), we denote the edge in E(GI)
corresponding to i by ei. Two vertices i and j are adjacent
in GC if and only if ei and ej are conﬂicting in GI. The
conﬂict graph GC corresponding to the graph GI in Fig. 2
is shown on the right.
To assign weights to the vertices of GC, let i be a vertex of GC. Notice that
i corresponds to the edge ei = (al, br) in E(GI), where al ∈A and br ∈B

402
S. Mehrabi
are preserved duos. Then, the weight of vertex i is deﬁned as w(i) := w(al, br)
in which recall that w(al, br) is the weight assigned to these preserved duos
by Deﬁnition 1. Although not precisely deﬁned, we again note that the weight
function is very ﬂexible and it can capture any combination of duos-preservation
and duos-distance measures.
Lemma 2. The MWCM problem on GI reduces to the MWIS problem on GC.
Proof. Suppose that S is a feasible solution to the MWCM problem on GI with total
weight w(S). For each edge ei ∈S, add the vertex i ∈V (GC) to S′. Clearly, S′
is an independent set because two vertices i and j in S′ being adjacent would
imply that ei and ej are conﬂicting in GI, contradicting the feasibility of S. To
see w(S′), notice that
w(S′) =

i∈S′
w(i) =

ei=(al,br)∈S
w(al, br) =

ei∈S
w(ei) = w(S).
Now, suppose that S′ is an independent set in GC with total weight w(S′).
For each u ∈S′, add eu ∈E(GI) to S. First, S is a feasible solution for the
MWCM problem on GI because the vertices of GC corresponding to any two con-
ﬂicting edges in S would be adjacent in GC, contradicting the fact that S′ is an
independent set. Moreover,
w(S) =

ei∈S
w(ei) =

ei=(al,br)∈S
w(al, br) =

i∈S′
w(i) = w(S′).
This completes the proof of the lemma.
⊓⊔
By Lemma 2, any approximation algorithm for the MWIS problem on GC
results in an approximation algorithm with the same factor for the MWCM problem
on GI. As such, for the rest of this section, we focus on the MWIS problem on GC
and show how to apply the local ratio technique to compute a 6-approximation
algorithm for this problem on GC.
Approximating the MWIS Problem on GC. We ﬁrst formulate the MWIS prob-
lem on GC as a linear program. We deﬁne a variable x(u) for each vertex
u ∈V (GC); if x(u) = 1, then vertex u belongs to the independent set. The
integer program assigns the binary values to the vertices with the constraint
that for each clique Q, the sum of the values assigned to all vertices in Q is at
most 1.
maximize

u∈V (GC)
w(u) · x(u)
(1)
subject to

v∈Q
x(v) ≤1
∀cliques Q ∈GC,
x(u) ∈{0, 1}
∀u ∈V (GC)

Approximating Weighted Duo-Preservation
403
Note that the number of constraints in (1) can be exponential in general, as
the number of cliques in GC could be exponential. However, for the MWIS problem
on GC, we can consider only a polynomial number of cliques in GC. To this end,
let u = (ai, bj) be a vertex in GC. By Observation 1, if v = (ak, bl) is in conﬂict
with u = (ai, bj), then either k ∈{i −1, i, i + 1} or l ∈{j −1, j, j + 1}. Let
Si−1
u
denote the set of all neighbours v of u in GC such that v = (ai−1, bs)
for some bs ∈B (recall the bipartite graph GI = (A ∪B, E)). Deﬁne Si
u and
Si+1
u
analogously. Similarly, let Sj−1
u
be the set of all neighbours v of u such
that v = (as, bj−1) for some as ∈A, and deﬁne Sj
u and Sj+1
u
analogously. Let
M := {i −1, i, i + 1, j −1, j, j + 1}. Then, by relaxing the integer constraint of
the above integer program, we can formulate the MWIS problem on GC as the
following linear program.
maximize

u∈V (GC)
w(u) · x(u)
(2)
subject to

v∈Sr
u
x(v) ≤1
∀u ∈V (GC), ∀r ∈M,
x(u) ≥0
∀u ∈V (GC)
Notice that the linear program (2) has a polynomial number of constraints.
These constraints suﬃce for the MWIS problem on GC because, by Observation 1,
the vertices u = (ai, bj) and v of GC corresponding to the two conﬂicting edges
eu and ev in GI belong to Sr
u, for some r ∈M. Moreover, we observe that any
independent set in GC gives a feasible integral solution to the linear program.
Therefore, the value of an optimal (not necessarily integer) solution to the linear
program is an upper bound on the value of an optimal integral solution.
We are now ready to describe the algorithm. We ﬁrst compute an optimal
solution x for the above linear program. Then, the rounding algorithm applies
a local ratio decomposition of the weight vector w with respect to x. See Algo-
rithm 1. The key to our rounding algorithm is the following lemma.
Lemma 3. Let x be a feasible solution to (2). Then, there exists a vertex u ∈
V (GC) such that

v∈N[u]
x(v) ≤6.
Proof. Let u ∈V (GC). Notice that u corresponds to an edge in GI; that is,
u = (ai, bj), where ai and bj are a pair of preserved duos in the mapping π from
s1 to s2. Observe that v ∈N[u] for some v = (ak, bl) ∈V (GC) if and only if
(ak, bl) conﬂicts with (i, j) in GI. Let M := {i −1, i, i + 1, j −1, j, j + 1} and
deﬁne the set Sr
u as above, for all r ∈M. Note that the vertices in Sr
u form
a clique in GC, for all r ∈M, because the set of edges corresponding to the
vertices of Sr
u in GI all share one endpoint (in particular, this endpoint is in
A if r ∈{i −1, i, i + 1} or it is in B if r ∈{j −1, j, j + 1}). See Fig. 3 for an
illustration. This means by the ﬁrst constraint of the linear program (2) that

v∈Sr
u
x(v) ≤1,

404
S. Mehrabi
Algorithm 1. ApproximateMWIS(GC)
1: Delete all vertices with non-positive weight. If no vertex remains, then return the
empty set;
2: Let u ∈V (GC) be a vertex satisfying

v∈N[u]
x(v) ≤6.
Then, decompose w into w := w1 + w2 as follows:
w1(v) :=

w(u)
if v ∈N[u],
0
otherwise.
3: Solve the problem recursively using w2 as the weight vector. Let S′ be the inde-
pendent set returned;
4: If u is not adjacent with some vertex in S′, then return S := S′ ∪{u}; otherwise,
return S := S′.
for all r ∈M. Therefore, we have

v∈N[u]
x(v) ≤

r∈M

v∈Sr
u
x(v) = |M| = 6.
This completes the proof of the lemma.
⊓⊔
We now analyze the algorithm. First, the set S returned by the algorithm is
clearly an independent set. The following lemma establishes the approximation
factor of the algorithm.
Lemma 4. Let x be a feasible solution to (2). Then, w(S) ≥1
6(w · x).
Proof. We prove the lemma by induction on the number of recursive calls. In
the base case, the set returned by the algorithm satisﬁes the lemma because no
vertices have remained. Moreover, the ﬁrst step that removes all vertices with
non-positive weight cannot decrease the right-hand side of the above inequality.
We now prove the induction step. Suppose that z and z′ correspond to the
indicator vectors for S and S′, respectively. By induction, w2 · z′ ≥1
6(w2 · x).
Since w2(u) = 0, we have w2 ·z ≥1
6(w2 ·x). From the last step of the algorithm,
we know that at least one vertex from N[u] is in S and so we have
w1 · z = w(u)

v∈N[u]
z(v) ≥w(u).
Moreover, by Lemma 3,
w1 · x = w(u)

v∈N[u]
x(v) ≤6w(u).
Hence, w1 · x ≤6w(u) ≤6(w1 · z), which gives w1 · z ≥1
6(w1 · x). Therefore,
we conclude that (w1 + w2) · z ≥1
6(w1 + w2) · x and so w(S) ≥1
6w · x. This
completes the proof of the lemma.
⊓⊔

Approximating Weighted Duo-Preservation
405
bj
ai−1
bj−1
ai+1
bj+1
ai
Fig. 3. Graph GI with edge u = (ai, bj). The edge corresponding to any vertex v ∈N[u]
in GC is incident to at least one of the six vertices in {i −1, i, i + 1, j −1, j, j + 1.
Since there exists at least one vertex u for which w2(u) = 0 in each recursive
step, Algorithm 1 terminates in polynomial time. Therefore, by Lemmas 1, 2
and 4, we have the main result of this paper.
Theorem 2. There exists a polynomial-time 6-approximation algorithm for the
MWDSM problem on s1 and s2.
4
Conclusion
In this paper, we studied a weighted version of the MDSM problem [9] that con-
siders the position of the preserved duos in the respective input strings (i.e., the
MWDSM problem). This is a natural variant of the problem, as considering the posi-
tion of the preserved features in the strings provides solutions with better qual-
ity in many applications, such as in comparative genomics in which more weight
could indicate more synteny between the corresponding preserved features. We
gave a polynomial-time 6-approximation algorithm for the MWDSM problem using
the local ratio technique. Although the approximation factor of our algorithm
is a bit large in compare to that of algorithms for the unweighted version of the
problem, our algorithm is much simpler as it beneﬁts from the simplicity of the
local ratio technique. Giving approximation algorithms with better approxima-
tion factors for the MWDSM problem remains open for future work.
References
1. Bar-Yehuda, R., Even, S.: A local-ratio theorem for approximating the weighted
vertex cover problem. In: Ausiello, G., Lucertini, M. (eds.) Analysis and Design of
Algorithms for Combinatorial Problems, vol. 109, pp. 27–45. North-Holland (1985)
2. Beretta, S., Castelli, M., Dondi, R.: Parameterized tractability of the maximum-
duo preservation string mapping problem. Theor. Comput. Sci. 646, 16–25 (2016)
3. Boria, N., Cabodi, G., Camurati, P., Palena, M., Pasini, P., Quer, S.: A 7/2-
approximation algorithm for the maximum duo-preservation string mapping prob-
lem. In: Proceedings of the 27th Annual Symposium on Combinatorial Pattern
Matching (CPM 2016), Tel Aviv, Israel, pp. 11:1–11:8 (2016)

406
S. Mehrabi
4. Boria, N., Kurpisz, A., Lepp¨anen, S., Mastrolilli, M.: Improved approxima-
tion for the maximum duo-preservation string mapping problem. In: Brown,
D., Morgenstern, B. (eds.) WABI 2014. LNCS, vol. 8701, pp. 14–25. Springer,
Heidelberg (2014). doi:10.1007/978-3-662-44753-6 2
5. Brubach,
B.:
Further
improvement
in
approximating
the
maximum
duo-
preservation string mapping problem. In: Frith, M., Storm Pedersen, C.N. (eds.)
WABI 2016. LNCS, vol. 9838, pp. 52–64. Springer, Cham (2016). doi:10.1007/
978-3-319-43681-4 5
6. Bulteau, L., Fertin, G., Komusiewicz, C., Rusu, I.: A ﬁxed-parameter algorithm for
minimum common string partition with few duplications. In: Darling, A., Stoye,
J. (eds.) WABI 2013. LNCS, vol. 8126, pp. 244–258. Springer, Heidelberg (2013).
doi:10.1007/978-3-642-40453-5 19
7. Bulteau, L., Komusiewicz, C.: Minimum common string partition parameterized
by partition size is ﬁxed-parameter tractable. In: Proceedings of the Twenty-Fifth
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2014), Portland,
Oregon, USA, pp. 102–121 (2014)
8. Chan, T.M., Har-Peled, S.: Approximation algorithms for maximum independent
set of pseudo-disks. Discrete Comput. Geometry 48(2), 373–392 (2012)
9. Chen, W., Chen, Z., Samatova, N.F., Peng, L., Wang, J., Tang, M.: Solving the
maximum duo-preservation string mapping problem with linear programming.
Theor. Comput. Sci. 530, 1–11 (2014)
10. Chen, X., Zheng, J., Zheng, F., Nan, P., Zhong, Y., Lonardi, S., Jiang, T.: Assign-
ment of orthologous genes via genome rearrangement. IEEE/ACM Trans. Comput.
Biology Bioinform. 2(4), 302–315 (2005)
11. Chrobak, M., Kolman, P., Sgall, J.: The greedy algorithm for the minimum
common string partition problem. In: Jansen, K., Khanna, S., Rolim, J.D.P.,
Ron, D. (eds.) APPROX/RANDOM -2004. LNCS, vol. 3122, pp. 84–95. Springer,
Heidelberg (2004). doi:10.1007/978-3-540-27821-4 8
12. Cormode, G., Muthukrishnan, S.: The string edit distance matching problem with
moves. ACM Trans. Algorithms 3(1), 2:1–2:19 (2007)
13. Dudek, B., Gawrychowski, P., Ostropolski-Nalewaja, P.: A family of approximation
algorithms for the maximum duo-preservation string mapping problem. CoRR,
abs/1702.02405 (2017)
14. Goldstein, A., Kolman, P., Zheng, J.: Minimum common string partition problem:
hardness and approximations. Electr. J. Comb. 12 (2005)
15. Hardison, R.C.: Comparative genomics. PLoS Biol. 1(2), e58 (2003)
16. Jiang, H., Zhu, B., Zhu, D., Zhu, H.: Minimum common string partition revisited.
J. Comb. Optim. 23(4), 519–527 (2012)
17. Kolman, P., Walen, T.: Reversal distance for strings with duplicates: linear time
approximation using hitting set. Electr. J. Comb. 14(1) (2007)
18. Mushegian, A.R.: Foundations of Comparative Genomics. Academic Press (AP),
Cambridge (2007)
19. Mustafa, N.H., Ray, S.: Improved results on geometric hitting set problems. Dis-
crete Comput. Geometry 44(4), 883–895 (2010)
20. Swenson, K.M., Marron, M., Earnest-DeYoung, J.V., Moret, B.M.E.: Approximat-
ing the true evolutionary distance between two genomes. ACM J. Experimental
Algorithmics 12, 3.5:1–3.5:17 (2008)

An Incentive Compatible, Eﬃcient Market
for Air Traﬃc Flow Management
Ruta Mehta1(B) and Vijay V. Vazirani2
1 Department of Computer Science, University of Illinois at Urbana-Champaign,
Champaign, USA
rutamehta@cs.illinois.edu
2 College of Computing, Georgia Tech, Atlanta, Georgia
Abstract. We present a market-based approach to the Air Traﬃc Flow
Management (ATFM) problem. The goods in our market are delays and
buyers are airline companies; the latter pay money to the Federal Avi-
ation Administration (FAA) to buy away the desired amount of delay
on a per ﬂight basis. We give a notion of equilibrium for this market
and an LP whose every optimal solution gives an equilibrium allocation
of ﬂights to landing slots as well as equilibrium prices for the landing
slots. Via a reduction to matching, we show that this equilibrium can be
computed combinatorially in strongly polynomial time. Moreover, there
is a special set of equilibrium prices, which can be computed easily, that
is identical to the VCG solution, and therefore the market is incentive
compatible in dominant strategy.
1
Introduction
Air Traﬃc Flow Management (ATFM) is a challenging operations research prob-
lem whose importance keeps escalating with the growth of the airline industry. In
the presence of inclement weather, the problem becomes particularly serious and
leads to substantial monetary losses and delays1, Yet, despite massive eﬀorts on
the part of the U.S. Federal Aviation Administration (FAA), airline companies,
and even the academia, the problem remains largely unsolved.
In a nutshell, the reason for this is that any viable solution needs to satisfy sev-
eral conﬂicting requirements, e.g., in addition to ensuring eﬃciency the solution
also needs to be viewed as “fair” by all parties involved. Indeed, [8] state that “ ...
While this work points at the possibility of dramatically reducing delay costs to
the airline industry vis-a-vis current practice, the vast majority of these propos-
als remain unimplemented. The ostensible reason for this is fairness ... .” It also
needs to be computationally eﬃcient – even moderate sized airports today handle
hundreds of ﬂights per day, with the 30 busiest ones handling anywhere from 1000
1 According to [7], the U.S. Congress Joint Economic Committee estimated that in
2007, the loss to the U.S. economy was $25.7 billion, due to 2.75 million hours of
ﬂight delays. In contrast, the total proﬁt of U.S. airlines in that year was $5 billion.
Also, see [4] for another perspective.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 407–419, 2017.
DOI: 10.1007/978-3-319-62389-4 34

408
R. Mehta and V.V. Vazirani
to 3000 ﬂights per day. The full problem involves scheduling ﬂight-landings simul-
taneously for multiple airports over a large period of time, taking into considera-
tion inter-airport constraints. Yet, according to [7], current research has mostly
remained at the level of a single airport because of computational tractability
reasons.
Building on a sequence of recent ideas that were steeped in sound economic
theory, and drawing on ideas from game theory and the theory of algorithms,
we present a solution that has a number of desirable properties. Our solution
for allocating ﬂights to landing slots at a single airport is based on the principle
of a free market, which is known to be fair and a remarkably eﬃcient method
for allocating scarce resources among alternative uses (sometimes stated in the
colorful language of the “invisible hand of the market” [27]). We deﬁne a market
in which goods are delays and buyers are airline companies; the latter pay money
to the FAA to buy away the desired amount of delay on a per ﬂight basis and
we give a notion of equilibrium for this market. W.r.t. equilibrium prices, the
total cost (price paid and cost of delay) of each agent, i.e., ﬂight, is minimized.
This involves a multi-objective optimization, one for each agent, just like
all market equilibrium problems. Yet, for some markets an equilibrium can be
found by optimizing only one function. As an example, consider the linear case of
Fisher’s market [10] for which an optimal solution to the remarkable Eisenberg-
Gale [16] convex formulation gives equilibrium prices and allocations. For our
market, we give a special LP whose optimal solution gives an equilibrium.
Using results from matching theory, we show how to ﬁnd equilibrium alloca-
tions and prices in strongly polynomial time. Moreover, using [21] it turns out
that our solution is incentive compatible in dominant strategy, i.e., the players
will not be able to game the ﬁnal allocation to their advantage by misreporting
their private information.
We note that the ATFM problem involves several issues that are not of a
game-theoretic or algorithmic nature, e.g., the relationship between long term
access rights (slot ownership or leasing) and short term access rights on a given
day of operations, e.g., see [6]. Our intention in this paper is not to address the
myriad of such issues. Instead, we have attempted to identify a mathematically
clean, core problem that is amenable to the powerful tools developed in the
theories stated above, and whose solution could form the core around which a
practical scheme can be built.
Within academia, research on this problem started with the pioneering work
of Odoni [24] and it ﬂourished with the extensive work of Bertsimas et. al.;
we refer the reader to [7,9] for thorough literature overviews and references to
important papers. These were centralized solutions in which the FAA decides a
schedule that is eﬃcient, e.g., it decides which ﬂights most critically need to be
served ﬁrst in order to minimize cascading delays in the entire system.

An Incentive Compatible, Eﬃcient Market for ATFM
409
A conceptual breakthrough came with the realization that the airlines them-
selves are the best judge of how to achieve eﬃciency2, thus moving away from
centralized solutions. This observation led to a solutions based on collaborative
decision making (CDM) which is used in practice [5,28,29].
More recently, a market based approach was proposed by Castelli, Pesenti and
Ranieri [11]. Although their formulation is somewhat complicated, the strength
of their approach lies in that it not only leads to eﬃciency but at the same time,
it ﬁnesses away the sticky issue of fairness – whoever pays gets smaller delays,
much the same way as whoever pays gets to ﬂy comfortably in Business Class!
Paper [11] also gave a tatonnement-based implementation of their market. Each
iteration starts with FAA announcing prices for landing slots. Then, airlines
pick their most preferred slots followed by FAA adjusting prices, to bring parity
between supply and demand, for the next iteration. However, they are unable to
show convergence of this process and instead propose running it a pre-speciﬁed
number of times, and in case of failure, resorting to FAA’s usual solution. They
also give an example for which incentive compatibility does not hold.
Our market formulation is quite diﬀerent and achieves both eﬃcient running
time and incentive compatibility. We believe that the simplicity of our solution
for this important problem, and the fact that it draws on fundamental ideas from
combinatorial optimization and game theory, should be viewed as a strength
rather than a weakness.
1.1
Salient Features of Our Solution
In Sect. 2 we give details of our basic market model for allocating a set of ﬂights
to landing slots for one airport. This set of ﬂights is picked in such a way that
their actual arrival times lie in a window of a couple of hours; the reason for
the latter will be clariﬁed in Sect. 4. The goods in our market are delays and
buyers are airline companies; the latter pay money to the FAA to buy away the
desired amount of delay on a per ﬂight basis. Typically ﬂights have a myriad
interdependencies with other ﬂights – because of the use of the same aircraft
for subsequent ﬂights, passengers connecting with other ﬂights, crew connecting
with other ﬂights, etc. The airline companies, and not FAA, are keenly aware of
these and are therefore in a better position to decide which ﬂights to avoid delay
for. The information provided by airline companies for each ﬂight is the dollar
value of delay as perceived by them.
For ﬁnding equilibrium allocations and prices in our market, we give a special
LP in which parameters can be set according to the prevailing conditions at the
airport and the delay costs declared by airline companies. We arrived at this LP
as follows. Consider a traﬃc network in which users selﬁshly choose paths from
2 e.g., they know best if a certain ﬂight needs to be served ﬁrst because it is carrying
CEOs of important companies who have paid a premium in order to reach their
destination on time or if delaying a certain ﬂight by 30 min will not have dire con-
sequences, however delaying it longer would propagate delays through their entire
system and result in a huge loss.

410
R. Mehta and V.V. Vazirani
their source to destination. One way of avoiding congestion is to impose tolls on
roads. [13] showed the existence of such tolls for minimizing the total delay for
the very special case of one source and one destination, using Kakutani’s ﬁxed
point theorem. Clearly, their result was highly non-constructive. In a followup
work, [18] gave a remarkable LP whose optimal solution yields such tolls for the
problem of arbitrary sources and destinations and moreover, this results in a
polynomial time algorithm. Their LP, which was meant for a multi-commodity
ﬂow setting, was the starting point of our work. One essential diﬀerence between
the two settings is that whereas they sought a Nash equilibrium, we seek a market
equilibrium; in particular, the latter requires the condition of market clearing.
We observe that the underlying matrix of our LP is totally unimodular and
hence it admits an integral optimal solution. Such a solution yields an equilibrium
schedule for the set of ﬂights under consideration and the dual of this LP yields
equilibrium price for each landing slot. Equilibrium entails that each ﬂight is
scheduled in such a way that the sum of the delay price and landing price is
minimum possible. We further show that an equilibrium can be found via an
algorithm for the minimum weight perfect b-matching problem and hence can
be computed combinatorially in strongly polynomial time. In hindsight, our LP
resembles the b-matching LP, but there are some diﬀerences.
Since the b-matching problem reduces to the maximum matching problem,
our market is essentially a matching market. Leonard [21] showed that the set
of equilibrium prices of a matching market with minimum sum corresponds pre-
cisely to VCG payments [23], thereby showing that the market is incentive com-
patible in dominant strategy. Since equilibrium prices form a lattice [1,15,26],
the one minimizing sum has to be simultaneously minimum for all goods. For
our market, we give a simple, linear-time procedure that converts arbitrary equi-
librium prices to ones that are simultaneously minimum for all slots. Incentive
compatibility with these prices follows. An issue worth mentioning is that the
total revenue, or the total cost, of VCG-based incentive compatible mechanisms
has been studied extensively, mostly with negative results [2,14,17,19,20]. In
contrast, since the prices in our natural market model happened to be VCG
prices, we have no overhead for making our mechanism incentive compatible.
The next question is how to address the scheduling of landing slots over longer
periods at multiple airports, taking into consideration inter-airport constraints.
Airlines can and do anticipate future congestion and delay issues and take these
into consideration to make complex decisions. However, sometimes unexpected
events happening even at a few places are likely to have profound cascading
eﬀects at geographically distant airports, making it necessary to make changes
dynamically. For such situations, in Sect. 4, we propose a dynamic solution by
decomposing this entire problem into many small problems, each of which will
be solved by the method proposed above. The key to this decomposition is the
robustness of our solution for a single set of ﬂights at one airport: we have not
imposed any constraints on delay costs, not even monotonicity. Therefore, airline
companies can utilize this ﬂexibility to encode a wide variety of inter-airport
constraints.

An Incentive Compatible, Eﬃcient Market for ATFM
411
We note that this approach opens up the possibility of making diverse types
of travelers happy through the following mechanism: the additional revenues
generated by FAA via our market gives it the ability to subsidize landing fees
for low budget airlines. As a result, both types of travelers can achieve an end
that is most desirable to them, business travelers and casual/vacation travelers.
The former, in inclement weather, will not be made to suﬀer delays that ruin
their important meetings and latter will get to ﬂy for a lower price (and perhaps
sip coﬀee for an additional hour on the tarmac, in inclement weather, while
thinking about their upcoming vacation).
To the best of our knowledge, ours is the ﬁrst work to give a simple LP-
based eﬃcient solution for the ATFM problem. We note that an LP similar
to ours is also given in [1]. This paper considers two-sided matching markets
with payments and non-quasilinear utilities. They show that the lowest priced
competitive equilibria are group strategy proof, which induces VCG payments for
the case of quasilinear utilities. Another related paper is [12], which considers
a Shapley-Shubik assignment model for unit-demand buyers and sellers with
one indivisible item each. Buyers have budget constraint for every item. This
sometimes prevents a competitive equilibrium from existing. They give a strongly
polynomial-time algorithm to check if an equilibrium exists or not, and if it does
exist, then it computes the one with lowest prices. However, they do not ensure
incentive compatibility.
2
The Market Model
In this section we will consider the problem of scheduling landings at one airport
only. Let A be the set of all ﬂights, operated by various airlines, that land in this
airport in a given period of time. We assume that the given period of time is
partitioned into a set of landing time slots, in a manner that is most convenient
for this airport; let S denote this set. Each slot s has a capacity cap(s) ∈Z+
specifying the number of ﬂights that can land in this time slot. As mentioned
in [6] the arrival of each aircraft consumes approximately the same amount of
airport capacity, therefore justifying the slot capacities as the number of ﬂights
while ignoring their types. We will assume that cap(s) is adjusted according to
the prevailing weather condition.
For i ∈A, the airline of this ﬂight decides the landing window for ﬂight i,
denoted by W(i). This gives the set of time slots in which this ﬂight should land
as per prevailing conditions, e.g., if there are no delays, the earliest time slot
in W(i) will be the scheduled arrival time3 of ﬂight i. For each slot s ∈W(i),
the airline also decides its delay cost, denoted by cis ≥0. Thus, if slot s is the
scheduled arrival time of ﬂight i, then cis = 04 and in general cis is the dollar
value of the cost perceived by the airline, for delay due to landing in slot s.
3 We will assume that if the ﬂight arrives before this time, it will have to wait on the
tarmac for some time. This appears to be a standard practice for the majority of
times, in case gates are not available.
4 All the results of this paper hold even if cis ̸= 0.

412
R. Mehta and V.V. Vazirani
A landing schedule is an assignment of ﬂights to time slots, respecting capac-
ity constraints. Each time slot will be assigned a landing price which is the
amount charged by FAA from the airline company if its ﬂight lands in this time
slot. We will deﬁne the total cost incurred by a ﬂight to be the sum of the price
paid for landing and the cost of the delay.
We say that a given schedule and prices are an equilibrium landing schedule
and prices if:
1. W.r.t. these prices, each ﬂight incurs a minimum total cost.
2. The landing price of any time slot that is not ﬁlled to capacity is zero. This
condition is justiﬁed by observing that if at equilibrium, a slot with zero price
is not ﬁlled to capacity, then clearly its price cannot be made positive. This
is a standard condition in equilibrium economics.
2.1
LP Formulation
In this section, we will give an LP that yields an equilibrium schedule; its dual
will yield equilibrium landing prices. Section 3 shows how they can be computed
in strongly polynomial time.
For s ∈S, xis will be the indicator variable that indicates whether ﬂight i
is scheduled in time slot s; naturally, in the LP formulation, this variable will
be allowed to take fractional values. The LP given below obtains a scheduling
where a ﬂight may be assigned partially to a slot (fractional scheduling), that
minimizes the total dollar value of the delays incurred by all ﬂights, subject
to capacity constraints of the time slots. (Note that the inequality in the ﬁrst
constraint will be satisﬁed with equality since the objective is being minimized;
the formulation below was chosen for reasons of convenience).
minimize 
i∈A,s∈S cisxis
subject to ∀i ∈A : 
s∈W (i) xis ≥1
∀s ∈S : 
i∈A,s∈W (i) xis ≤cap(s)
∀i ∈A, s ∈W(i) : xis ≥0
(1)
Let ps denote the dual variable corresponding to the second set of inequalities.
We will interpret ps as the price of landing in time slot s. Thus if ﬂight i lands
in time slot s, the total cost incurred by it is ps + cis. Let ti denote the dual
variable corresponding to the ﬁrst set of inequalities. In Lemma 1 we will prove
that ti is the total cost incurred by ﬂight i w.r.t. the prices found by the dual;
moreover, each ﬂight incurs minimum total cost.
The dual LP is the following.
maximize 
i∈A ti −
s∈S cap(s) · ps
subject to ∀i ∈A, ∀s ∈W(i) : ti ≤ps + cis
∀i ∈A : ti ≥0
∀s ∈S : ps ≥0
(2)
Lemma 1. W.r.t. the prices found by the dual LP (2), each ﬂight i incurs min-
imum total cost and it is given by ti.

An Incentive Compatible, Eﬃcient Market for ATFM
413
Proof. Apply complementary slackness conditions to the primal variables we get
∀i ∈A, ∀s ∈W(i) :
xis > 0
⇒
ti = ps + cis.
Moreover, for time slots s ∈S which are not used by ﬂight i, i.e., for which
xis = 0, by the dual constraint, the total cost of using this slot can only be
higher than ti. The lemma follows.
The second condition required for equilibrium is satisﬁed because of comple-
mentarity applied to the variables ps: If 
i∈A,s∈W (i) xis < cap(s), then ps = 0.
At this point, we can provide an intuitive understanding of how the actual
slot assigned to ﬂight i by LP (1) is inﬂuenced by the delay costs declared for
ﬂight i and how LP (2) sets prices of slots. Assume that time slot s is the
scheduled arrival time of ﬂight i, i.e., cis = 0 and s′ is a later slot. Then by
Lemma 1, slot s will be preferred to slot s′ only if ps −ps′ ≤cis′. Thus cis′ places
an upper bound on the extra money that can be charged for buying away the
delay incurred by landing in s instead of s′. Clearly, ﬂight i will incur a smaller
delay, at the cost of paying more, if its airline declares large delay costs for late
landing. Furthermore, by standard LP theory, the dual variables, ps, will adjust
according to the demand of each time slot, i.e., a time slot s that is demanded
by a large number of ﬂights that have declared large delay costs will have a high
price. In particular, if a slot is not allocated to capacity, its price will be zero as
shown above.
It is easy to see that the matrix underlying LP (1) is totally unimodular,
since it corresponds to the incidence matrix of a bipartite graph [3]. Therefore,
it has an integral optimal solution. Further, minimization ensures that for every
ﬂight i at most one of the xiss is one and the rest are zero. Hence we get:
Theorem 1. Solution of LP (1) and its dual (2) give an (optimal) equilibrium
schedule and equilibrium prices.
3
Strongly Polynomial Implementation
As discussed in the previous section, LP (1) has an integral optimal solution
as its underlying matrix is totally unimodular. In this section, we show that
the problem of obtaining such a solution can be reduced to a minimum weight
perfect b-matching problem5, and hence can be found in strongly polynomial
time; see [25] Volume A. The equilibrium prices, i.e., solution of (2), can be
obtained from the dual variables of the matching. Furthermore, we show that
there exist equilibrium prices that induce VCG payments, and hence is incentive
compatible in dominant strategy. Finally, we give a strongly polynomial time
procedure to compute such prices.
Consider the edge-weighted bipartite graph (A′, S, E), with bipartition A′ =
A∪{v}, where A is the set of ﬂights and v is a special vertex, and S is the set of
5 The instance we construct can also be reduced to a minimum weight perfect matching
problem with quadratic increase in number of nodes.

414
R. Mehta and V.V. Vazirani
time slots. The set of edges E and weights are as follows: for i ∈A, s ∈W(i),
(i, s) is an edge with weight cis, and for each s ∈S, there are cap(s) many (v, s)
edges6, each with unit weight (a multi-graph).
The matching requirements are: bi = 1 for each i ∈A, bs = cap(s) for
each s ∈S, and bv= 
s∈S cap(s) −|A| for v. Clearly, the last quantity is
non-negative, or else LP (1) is infeasible. The following lemmas show that the
equilibrium landing schedule and prices can be computed using minimum weight
perfect b-matching of graph (A′, S, E).
Lemma 2. Let F ∗⊂E be a perfect b-matching in (A′, S, E) and x∗be a sched-
ule where x∗
is = 1 if (i, s) ∈F ∗. F ∗is a minimum weight perfect b-matching if
and only if x∗is an optimal solution of LP (1).
Proof. To the contrary suppose x′ and not x∗is the optimal solution of LP (1).
Let F ′ = {(i, s) ∈E | x′
is = 1}∪{(cap(s)−
i;s∈W (i) x′
is) many (v, s) | s ∈S} be
the set of edges corresponding to schedule x′. Clearly, F ′ is a perfect b-matching.
Note that the matching edges incident on v contribute cost bv in any perfect b-
matching. Since, x′ and not x∗is an optimal solution of LP (1), we have,

i∈A,s∈W (i)
cisx′
is + bv <

i∈A,s∈W (i)
cisx∗
is + bv ⇒

(i,j)∈F ′
cij <

(i,j)∈F ∗
cij
Contradicting F ∗being the minimum weight perfect matching. The reverse
implication follows by similar argument in the reverse order.
Using Lemma 2, next we show that the dual variables of the b-matching LP
give an equilibrium price vector. In the b-matching LP there is an equality for
each node to ensure its matching requirement. Let uv, ui and qs be the dual
variables corresponding to the equalities of nodes v, i ∈A and s ∈S. Then the
dual LP for minimum weight perfect b-matching in graph (A′, S, E) is as follows.
max :

i∈A
ui +

s∈S
cap(s)qs + uvbv
s.t.
∀i ∈A, s ∈W(i) : ui ≤−qs + cis
∀s ∈S :
uv ≤−qs + 1
(3)
There are no non-negativity constraints on the dual variables since the cor-
responding primal constraints are equality.
Lemma 3. There exists a dual solution (u∗, q∗) of (3) with u∗
v = 1, and given
that, −q∗yields a solution of LP (2).
Proof. If (u∗, q∗) is a dual solution then so is v = (u∗+ δ, q∗−δ) for any δ ∈R.
This is because, clearly v is feasible. Further, since |A| + bv = 
s cap(s) the
value of objective function at v is same as that at (u∗, q∗).
Therefore given any solution of the dual, we can obtain one with u∗
v = 1 by an
additive scaling. Replacing uv with 1 and qs with −ps in (3) gives max{
i ui −

s cap(s)ps + bv | ui ≤ps + cis, ps ≥0}, which is exactly (2).
6 This is not going to aﬀect strong polynomiality, because we can assume that cap(s) ≤
|A|, ∀s without loss of generality.

An Incentive Compatible, Eﬃcient Market for ATFM
415
Since a primal and a dual solution of a minimum weight perfect b-matching
can be computed in strongly polynomial time [25], the next theorem follows
using Lemmas 2 and 3, and Theorem 1.
Theorem 2. There is a combinatorial, strongly polynomial algorithm for com-
puting an equilibrium landing schedule and equilibrium prices.
3.1
Incentive Compatible in Dominant Strategy
Since equilibrium price vectors of the market is in one-to-one correspondence
with the solutions of the dual matching LP with uv = 1 (Lemma 3), they need
not be unique, and in fact form a convex set. In this section we show that
one of them induces VCG payments, and therefore is incentive compatible in
dominant strategy. Further, we will design a method to compute such VCG
prices in strongly polynomial time.
An instance of the perfect b-matching problem can be reduced to the perfect
matching problem by duplicating node n, bn times. Therefore, if we convert the
costs cis on edge (i, s) to payoﬀs H −cis for a big enough constant H, the market
becomes an equivalent matching market (also known as assignment game) [26]
where the costs of producing goods, the slots in our case, are zero. It is not
diﬃcult to check that equilibrium allocations and prices of our original market
and the transformed matching market exactly match.
For such a market, Leonard [21] showed that the set of equilibrium prices of
a matching market with minimum sum correspond precisely to VCG payments
[23], thereby showing that the market is incentive compatible in dominant strat-
egy at such a price vector. Since the proof in [21] is not formal, we have provided
a complete proof in the full version [22]. Since equilibrium prices form a lattice
[1,15,26], the one minimizing sum has to be simultaneously minimum for all
goods.7 Clearly, such a price vector has to be unique. Next we give a procedure
to compute the minimum equilibrium price vector, starting from any equilibrium
price vector p∗and corresponding equilibrium schedule x∗.
The procedure is based on the following observation: Given equilibrium prices
p∗and corresponding schedule x∗, construct graph G(x∗, p∗) where slots form
the node set. Put a directed edge from slot s to slot s′ if there exists a ﬂight, say
i, scheduled in s at x∗, and it is indiﬀerent between s and s′ in terms of total
cost, i.e. x∗
is = 1 and p∗
s + cis = p∗
s′ + cis′. An edge in graph G(x∗, p∗) indicates
that if the price of slot s′ is decreased then i would prefer s′ over s. Therefore,
in order to maintain x∗as an equilibrium schedule the price of s also has to be
decreased by the same amount.
Lemma 4. Prices p∗m give the minimum equilibrium prices if and only if every
node in G(x∗, p∗m) has a directed path from a zero priced node, where x∗is the
corresponding equilibrium schedule.
7 Equilibrium prices p are minimum if for any other equilibrium prices p′ we have
ps ≤p′
s, ∀s ∈S.

416
R. Mehta and V.V. Vazirani
Proof. Suppose slot s does not have a path from a zero priced node. Consider
the set D of nodes which can reach s in G∗= G(x∗, p∗m); these have positive
prices. ∃ϵ > 0 s.t. the prices of all the slots in D can be lowered by ϵ without
violating the equilibrium condition (1), contradicting minimality of p∗m.
For the other direction, the intuition is that if every node is connected to a
zero priced node in G(x∗, p∗m), then price of any slot can not be reduced without
enforcing price of some other slot go negative, in order to get the corresponding
equilibrium schedule. The formal proof is as follows:
To the contrary suppose every node is connected to a zero priced node in G∗
and there are equilibrium prices p′ ≤p∗m such that for some s ∈S, p∗m
s
> p′
s ≥0.
Consider, one such s with the smallest path from a zero-priced node in G∗. Since,
p′
s ≥0, we have p∗m
s
> 0, and therefore s is ﬁlled to its capacity at prices p∗m
(using equilibrium condition (2) of Sect. 2). Let x′ be the equilibrium schedule
corresponding to prices p′.
Let there be a directed edge from s′ to s in G∗. By choice of s we have that
p′
s′ = p∗m
s′ . In that case, a ﬂight, say i′, allocated to s′ at p∗will move to s at p′.
This implies that 
i x′
is′ < 
i x∗
is′ ≤cap(s′). Hence p′
s′ = 0 ⇒p∗m
s′
= 0 (using
equilibrium condition (2)). Let Z = {s | p∗m
s
= 0}. There are two cases:
Case I - Flights in slot s at x∗remain in s at x′, i.e., {i |x∗
is = 1} ⊆{i |x′
is = 1}:
Since, x′
i′s = 1 and x∗
i′s = 0, implying 
i x′
is > 
i x∗
is = cap(s), a contradiction.
Case II - ∃i, x∗
is = 1, x′
is = 0:
Wlog let x′ be an optimal allocation at p′ that is at minimum distance from x∗
in l1-norm. In other words, a ﬂight that is allocated to slot u at prices p∗m moves
to slot v at prices p′ only if price of slot v has decreased.
Construct a graph H, where slots are nodes, and there is an edge from u to v
if ∃i, x∗
iu = 1, x′
iv = 1, i.e., ﬂight i moved from u to v when prices change from
p∗m to p′, with weight being number of edges moved. Note that price of every
node with an incoming edge should have decreased while going from p∗m to p′.
Therefore, nodes of Z have no incoming edges. Further, nodes with incoming
edges are ﬁlled to capacity at p∗m since their prices are non-zero. Hence, total
out going weight of such a node should be at least total incoming weight in H.
If there is a cycle in H, then subtract weight of one from all its edges, and
remove zero-weight edges. Repeat this until there are no cycles. Since, s′ ∈Z,
it had no incoming edge, but had an edge to s. Therefore, there is a path in
remaining H starting at s′. Consider the other end of this path. Clearly, it has
to be ﬁlled beyond its capacity at x′, a contradiction.
Using the fact established by Lemma 4 next we design a procedure to compute
the minimum equilibrium prices in Table 1, given any equilibrium (p∗, x∗).
Lemma 5. Given an equilibrium (x∗, p∗), MinimumPrices(x∗, p∗) outputs min-
imum prices in time O(|A||S|2).

An Incentive Compatible, Eﬃcient Market for ATFM
417
Table 1. Procedure for computing minimum optimal prices
MinimumPrices(x∗, p∗)
1. Z ←Nodes reachable from zero-priced nodes in G(x∗, p∗).
2. Pick a d ∈S \ Z
3. D ←{Nodes that can reach d in G(x∗, p∗)}, δ ←0,
and p∗
s ←p∗
s −δ, ∀s ∈D
4. Increase δ until one of the following happen
- If price of a slot in D becomes zero, then go to 1.
- If a new edge appears in G(x∗, p∗), then recompute Z.
If d ∈Z then go to 2 else go to 3.
5. Output p∗as the minimum prices.
Proof. Note that the size of Z and edges in G(x∗, p∗) are increasing. There-
fore, Step 3 is executed O(|S|) many times in total. Step 4 may need O(|A||S|)
time to compute the threshold δ. Therefore the running time of the procedure
MinimumPrices is O(|A||S|2). Let the output price vector be p∗m. The lemma
follows from the fact that (x∗, p∗m) still satisfy both the equilibrium conditions,
and every slot is reachable from a zero priced node in G(x∗, p∗m) (Lemma 4).
Theorems 1 and 2, Lemma 5, together with [21] give:
Theorem 3. There exists an incentive compatible (in dominant strategy) mar-
ket mechanism for scheduling a set of ﬂight landings at a single airport; moreover,
it is computable combinatorially in strongly polynomial time.
4
Dealing with Multiple Airports
In this section, we suggest how to use the above-stated solution to deal with
unexpected events that result in global, cascading delays. Our proposal is to
decompose the problem of scheduling landing slots over a period of a day at
multiple airports into many small problems, each dealing with a set of ﬂights
whose arrival times lie in a window of a couple of hours – the window being chosen
in such a way that all ﬂights would already be in the air and their actual arrival
times, assuming no further delays, would be known to the airline companies and
to FAA. At this point, an airline company has much crucial information about
all the other ﬂights associated with its current ﬂight due to connections, crew
availability, etc. It is therefore in a good position to determine how much delay it
needs to buy away for its ﬂight and how much it is willing to pay, by setting ciss
accordingly. This information is used by FAA to arrive at a landing schedule.
The process is repeated every couple of hours at each airport.

418
R. Mehta and V.V. Vazirani
References
1. Alaei, S., Jain, K., Malekian, A.: Competitive equilibria in two sided matching
markets with non-transferable utilities (2012). arxiv:1006.4696
2. Archer, A., Tardos, E.: Frugal path mechanisms. In: ACM-SIAM Annual Sympo-
sium on Discrete Algorithms, pp. 991–999 (2002)
3. Bapat, B.R.: Incidence matrix. In: Bapat, B.R. (ed.) Graphs and Matrices.
Springer, London (2010). doi:10.1007/978-1-84882-981-7
4. Ball, M., Barnhart, C., Dresner, M., Hansen, M., Neels, K., Odoni, A., Peterson, E.,
Sherry, L., Trani, A., Zou, B., Britto, R.: Total delay impact study. In: NEXTOR
Research Symposium, Washington DC (2010)
5. Ball, M., Barnhart, C., Nemhauser, G., Odoni, A.: Air transportation: irregular
operations and control. In: Barnhart, C., Laporte, G. (eds.) Handbook of Opera-
tions Research and Management Science: Transportation (2006)
6. Ball, M.O., Donohue, G., Hoﬀman, K.: Auctions for the safe, eﬃcient and equitable
allocation of airspace system resources. In: Cramton, P., Shoham, Y., Steinberg,
R. (eds.) Combinatorial Auctions, pp. 507–538. MIT Press, Cambridge (2005)
7. Barnhart, C., Bertsimas, D., Caramanis, C., Fearing, D.: Equitable and eﬃcient
coordination in traﬃc ﬂow management. Transp. Sci. 42(2), 262–280 (2012)
8. Bertsimas, D., Farias, V., Trichakis, N.: The price of fairness. Oper. Res. 59(1),
17–31 (2011)
9. Bertsimas, D., Gupta, S.: A proposal for network air traﬃc ﬂow management
incorporating fairness and airline collaboration. Oper. Res. (2011)
10. Brainard, W.C., Scarf, H.E.: How to compute equilibrium prices in 1891. Cowles
Foundation Discussion Paper 1270 (2000)
11. Castelli, E., Pesenti, R., Ranieri, A.: The design of a market mechanism to allocate
air traﬃc ﬂow management slots. Trans. Res. Part C 19, 931–943 (2011)
12. Chen, N., Deng, X., Ghosh, A.: Competitive equilibria in matching markets with
budgets. SIGecom Exch. 9(1), 5:1–5:5 (2010)
13. Cole, R., Dodis, Y., Roughgarden, T.: Pricing network edges for heterogeneous
selﬁsh users. In: STOC, pp. 521–530 (2003)
14. Conitzer, V., Sandholm, T.: Failures of the VCG mechanism in combinatorial auc-
tions and exchanges. In: AAMAS, pp. 521–528 (2006)
15. Demange, G., Gale, D.: The strategy structure of two-sided matching markets.
Econometrica 53(4), 873–888 (1985)
16. Eisenberg, E., Gale, D.: Consensus of subjective probabilities: the Pari-Mutuel
method. Ann. Math. Stat. 30, 165–168 (1959)
17. Elkind, E., Sahai, A., Steiglitz, K.: Frugality in path auctions. In: ACM-SIAM
Annual Symposium on Discrete Algorithms, pp. 701–709 (2004)
18. Fleischer, L., Jain, K., Mahdian, M.: Tolls for heterogeneous selﬁsh users in mul-
ticommodity networks and generalized congestion games. In: STOC, pp. 277–285
(2004)
19. Hartline, J.D., Roughgarden, T.: Simple versus optimal mechanisms. In: EC (2009)
20. Karlin, A.R., Kempe, D., Tamir, T.: Beyond VCG: frugality of truthful mecha-
nisms. In: FOCS, pp. 615–624 (2005)
21. Leonard, H.B.: Elicitation of honest preferences for the assignment of individuals
to positions. J. Polit. Econ. 91(3), 461–479 (1983)
22. Mehta, R., Vazirani, V.V.: An incentive compatible, eﬃcient market for air traﬃc
ﬂow management (2017). arxiv:1305.3241

An Incentive Compatible, Eﬃcient Market for ATFM
419
23. Nisan, N.: Introduction to mechanism design (for computer scientists). In: Nisan,
N., Roughgarden, T., Tardos, E., Vazirani, V.V. (eds.) Algorithmic Game Theory,
pp. 209–241. Cambridge University Press (2007)
24. Odoni, A.: The ﬂow management problem in air traﬃc control. In: Odoni, A.,
Szego, G. (eds.) Flow Control of Congested Networks. Springer, Berlin (1987)
25. Schrijver, A.: Combinatorial Optimization. Springer, Heidelberg (2003)
26. Shapley, L.S., Shubik, M.: The assignment game I: The core. Int. Game Theor.
1(2), 111–130 (1972)
27. Smith, A.: The Wealth of Nations. Forgotten Books, London (1776)
28. Vossen, T., Ball, M.: Slot trading opportunities in collaborative ground delay pro-
grams. Transp. Sci. 40, 29–43 (2006)
29. Wambsganss, M.: Collaborative decision making through dynamic information
transfer. Air Traﬃc Control Q. 4, 107–123 (1996)

Linear Representation of Transversal Matroids
and Gammoids Parameterized by Rank
Pranabendu Misra1(B), Fahad Panolan1,
M.S. Ramanujan2, and Saket Saurabh1,3
1 Department of Informatics, University of Bergen, Bergen, Norway
{pranabendu.misra,fahad.panolan}@ii.uib.no
2 TU Wien, Vienna, Austria
ramanujan@ac.tuwien.ac.at
3 The Institute of Mathematical Sciences, HBNI, Chennai, India
saket@imsc.res.in
Abstract. Given a bipartite graph G = (U ⊎V, E), a linear representa-
tion of the transversal matroid associated with G on the ground set U,
can be constructed in randomized polynomial time. In fact one can get
a linear representation deterministically in time 2O(m2n), where m = |U|
and n = |V |, by looping through all the choices made in the randomized
algorithm. Other important matroids for which one can obtain linear
representation deterministically in time similar to the one for transver-
sal matroids include gammoids and strict gammoids. Strict gammoids
are duals of transversal matroids and gammoids are restrictions of strict
gammoids. We give faster deterministic algorithms to construct linear
representations of transversal matroids, gammoids and strict gammoids.
All our algorithms run in time
m
r

mO(1), where m is the cardinality
of the ground set and r is the rank of the matroid. In the language of
parameterized complexity, we give an XP algorithm for ﬁnding linear
representations of transversal matroids, gammoids and strict gammoids
parameterized by the rank of the given matroid.
1
Introduction
Matroids are important mathematical objects in the theory of algorithms and
combinatorial optimization. Often an algorithm for a class of matroids gives us
an algorithmic meta theorem, which gives a uniﬁed solution to several problems.
For example, it is known that any problem which admits a greedy algorithm
can be embedded into a matroid and ﬁnding minimum (maximum) weighted
independent set in this matroid corresponds to ﬁnding a solution to the prob-
lem. Other important examples are the Matroid Intersection and Matroid
Parity problems, which encompass several combinatorial optimization prob-
lems such as Bipartite Matching, 2-Edge Disjoint Spanning Trees and
Arborescence. A matroid M is deﬁned as a pair (E, I), where E is called
the ground set and I is a family of subsets of E, called independent sets, with
following three properties: (i) ∅∈I, (ii) if A ∈I and A′ ⊆A, then A′ ∈I and
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 420–432, 2017.
DOI: 10.1007/978-3-319-62389-4 35

Linear Representation of Transversal Matroids
421
(iii) If A, B ∈I and |A| < |B|, then there is a e ∈B \ A such that A ∪{e} ∈I.
As the cardinality of I could be exponential in |E|, as it is in many applications,
explicitly listing I for algorithms is highly ineﬃcient both in terms of time com-
plexity as well as space complexity. Thus, during the early works of algorithms
using matroids, algorithms were designed using the oracle model. An indepen-
dence oracle for a matroid is a black box algorithm which takes as input a subset
of the ground set and returns Yes if the set is independent in the matroid and
No otherwise. Many algorithms are designed using these oracles. These oracle
based algorithms lead to eﬃcient algorithms for problems where we have good
algorithms that can act as an oracles. A few matroids for which we have eﬃcient
oracles include, but are not limited to, graphic matroids, co-graphic matroids,
transversal matroids and linear matroids.
Another way of representing a matroid succinctly is by encoding the infor-
mation about the family of independent sets in a matrix. A matrix A over a ﬁeld
F is called a linear representation of a matroid M = (E, I), if there is a bijection
between the columns of A and E and a subset S ⊆E is independent in M if and
only if the corresponding columns in A are linearly independent over the ﬁeld
F. Note that while not all matroids admit a linear representation, a number of
important classes of matroids do. Recently, several algorithmic results have been
obtained in the ﬁelds of Parameterized Complexity and Exact Algorithms, which
require a linear representations of certain classes of matroids [2–6,10,11,13,15].
This naturally motivates the question of constructing linear representations for
various classes of matroids eﬃciently. Deterministic polynomial time algorithms
were known for linear representations of many important classes of matroids
such as uniform matroids, partition matroids, graphic matroids and co-graphic
matroids. In all these algorithms the running time is polynomial in the size of the
ground set. However, for transversal matroids and gammoids, only randomized
polynomial time algorithms are known for constructing its linear representations.
These matroids feature in many of the results mentioned above, and determin-
istic algorithms to compute linear representations for them will derandomize
several algorithms in literature. In this paper we give a modest improvement
over the na¨ıve algorithm for constructing deterministic linear representations for
transversal matroids and gammoids.
Let G = (U ⊎V, E) be a bipartite graph, the transversal matroid MG on the
ground set U has the following family of independent sets: U ′ ⊆U is independent
in MG if and only if there is a matching in G saturating U ′. Furthermore assume
that |U| = |V | = m and G has a perfect matching. A natural question in this
direction is as follows.
Question 1: Could we exploit the fact that G has a perfect matching to
design a deterministic polynomial time algorithm to ﬁnd a linear repre-
sentation for MG, where we can test whether a subset is independent or
not in deterministic polynomial time?
The answer to this question is of course, Yes! An m × m identity matrix is a
linear representation of MG. This naturally leads to the following question.

422
P. Misra et al.
Question 2: Suppose G has a matching of size m−ℓ, where ℓis a constant.
Can we design a deterministic polynomial time algorithm to ﬁnd a linear
representation for MG, where we can test whether a subset is independent
or not in deterministic polynomial time?
This question is the starting point of the present work. As mentioned earlier,
there is a randomized polynomial time algorithm to obtain a linear represen-
tation of a transversal matroid for any bipartite graph. Let G = (U ⊎V, E)
be a bipartite graph, where U = {u1, . . . , um} and V
= {v1, . . . , vn}. Let
X = {xi,j | i ∈[n], j ∈[m]}. Deﬁne an n × m matrix A as follows: for each
i ∈[n], j ∈[m], A[i, j] = 0 if viuj /∈E and xi,j otherwise. Then for any
R ⊆[n], C ⊆[m], |R| = |C|, det(A[R, C]) ̸≡0 if and only if there is a per-
fect matching in G[{vi | i ∈R} ∪{uj | j ∈C}]. This implies that A is in fact
a linear representation of the transversal matroid MG on the ground set U over
the ﬁeld of fractions F(X), where F is any ﬁeld of size at least 2. Notice that the
above construction can be done in deterministic polynomial time.
However, in the representation above, to check whether a set is linearly inde-
pendent we need to test if the corresponding determinant polynomial, which
is a multivariate polynomial, is identically non-zero. This is a case of the well
known polynomial identity testing (PIT) problem, and we do not know of a
deterministic polynomial time algorithm for this problem. Hence, this represen-
tation is diﬃcult to use in deterministic algorithms for many applications [10,13].
Furthermore, it rather diﬃcult to carry out ﬁeld operations over the ﬁeld of frac-
tions F(X) in polynomial time. As a result, even though we get the above linear
representation in deterministic polynomial time, we are not able to use it for
deterministic algorithms eﬃciently. We can obtain another representation, by
substituting random values for each xi,j ∈X from a ﬁeld of size at least 2pm2m,
where p ∈N, and succeed with probability at least

1 −1
2p

. This leads to a ran-
domized polynomial time algorithm [13], to obtain a representation over a ﬁnite
ﬁeld or a ﬁeld such as R, where ﬁeld operations can be carried out eﬃciently. It
appears that derandomizing the above approach has some obstacles, as this will
have some important consequences on lower bounds in complexity theory [9].
Observe that the above approach implies a deterministic algorithm of running
time 2O(m2n), that tests all possible assignments of at most mn variables from
a ﬁeld of size m2m (setting p = O(1)), since one of them will certainly be a lin-
ear representation of MG. Although we have not been able to obtain polynomial
time deterministic algorithms for computing linear representations of transversal
matroids and gammoids, our results do imply an aﬃrmative answer to Question
2. Our main theorem is the following.
Theorem 1. There is a deterministic algorithm that, given a bipartite graph
G = (U ⊎V, E) with a maximum matching of size r, outputs a linear representa-
tion of the transversal matroid MG = (U, I) over a ﬁeld F of size >
|U|
r

, in time
O(
|U|
r

·|E|

|V |+N), where N is the time required to perform O(
|U|
r

|V |·|U|)
operations over F.

Linear Representation of Transversal Matroids
423
An XP algorithm for a parameterized problem Π ⊆Σ∗× N, takes as input
(I, k) ∈Σ∗× N and decides whether (I, k) ∈Π in time |I|g(k), where g(k)
is a computable function in k alone. Thus, in the language of parameterized
complexity, Theorem 1 gives an XP algorithm for ﬁnding a linear representation
of transversal matroids parameterized by the rank of the given matroid. Observe
that if r is the rank of the matroid then the maximum matching of the graph
is r. That is, r = |U| −ℓand hence ℓ= |U| −r. This together with the fact
that
m
a

=
 m
m−a

implies that Theorem 1 gives a polynomial time algorithm for
Question 2, whenever ℓis a constant.
Transversal matroids are closely related to the class of gammoids. A gammoid
is deﬁned by a digraph D and two vertex subsets S and T of V (D). Here, T is
the ground set and a subset X of T is independent if and only if X is reachable
from S by a collection of |X| vertex disjoint paths. It was shown by Ingleton and
Piﬀ[8], that a subclass of gammoids, called strict gammoids where T = V (D),
are the duals of transversal matroids. Thus, one can also view gammoids as
matroids obtained from strict gammoids by deleting some of the elements from
the ground set. Therefore the task of designing an algorithm to construct a
linear representation for gammoids is at least as hard as constructing one for
transversal matroids. In this work we prove the following theorem.
Theorem 2. There is a deterministic algorithm that, given an n-vertex digraph
D and S, T ⊆V (D) such that |S| = r and |T| = n′, outputs a linear represen-
tation of the gammoid deﬁned in D with ground set T, over a ﬁeld F of size
strictly greater than
n′
r

in time O(
n′
r

n3 + N), where N is the time required
to perform O(
n′
r

n3) operations over F.
2
Preliminaries
For n ∈N, we use [n] to denote the set {1, 2, . . . , n}. Let U be a set. We use
|U| to denote the cardinality of U and 2U to denote the set of subsets of U. For
i ∈[|U|],
U
i

denotes the set {S ⊆U | |S| = i}.
Graphs. We use G to denote a graph and D to denote a digraph. The vertex
set and edge (arc) set of a graph (digraph) are denoted as V (G) (V (D)) and
E(G) (A(D)) respectively. We also use G = (V, E) (or D = (V, A)) to denote a
graph (digraph) with vertex set V and edge set E (arc set A). We use standard
notations from graph theory [1]. For a graph G, and u, v ∈V (G) we use uv
to denote the edge with end vertices u and v. An induced subgraph of G on
the vertex set V ′ ⊆V (G) is written as G[V ′]. A matching in a graph G is a
collection of edges such that no two edges share a common end vertices. We say
a matching M in a graph G saturates V ′ ⊆V (G), if all the vertices in V ′ are
end vertices of edges in M. For a digraph D and u, v ∈V (D), a path from u
to v is a sequence of vertices u1, u2, . . . , uk such that u1 = u, uk = v and for all
i ∈[k −1], uiui+1 ∈A(D).
Matrices and Linear algebra. For a ﬁnite ﬁeld F, F[X] denotes the ring of
polynomials in X over F and F(X) denotes the ﬁeld of fractions of F[X]. A vector

424
P. Misra et al.
v over a ﬁeld F is an array of elements from F. A set of vectors {v1, v2, . . . , vk}
are said to be linearly dependent if there exist elements a1, a2, . . . , ak ∈F, not
all zero, such that k
i=1 aivi = 0. Otherwise these vectors are called linearly
independent. We say a matrix A has dimension n × m (or n × m matrix A) if
A has n rows and m columns. For a matrix A (or a vector v) we use AT (or
vT ) to denote its transpose. The determinant of an n × n matrix A is denoted
by det(A). For a matrix A, with rows indexed by elements from the set R and
columns indexed by elements from the set C, R′ ⊆R and C′ ⊆C, we use
A[R′, C′] to denote the matrix A restricted to rows R′ and columns C′. The
rank of a matrix is the cardinality of a maximum sized set of columns which
are linearly independent. Equivalently, the rank of a matrix A is the maximum
number r such that there is a r × r submatrix whose determinant is non-zero.
Matroids. We recall a few deﬁnitions related to matroids. For a broader
overview on matroids, we refer to [14].
Deﬁnition 1. A matroid M is a pair (E, I), where E is a set, called ground
set and I is a family of subsets of E, called independent sets, with the following
three properties: (i) ∅∈I, (ii) if I ∈I and I′ ⊆I, then I′ ∈I, and (iii) if
I, J ∈I and |I| < |J|, then there is e ∈J \ I such that I ∪{e} ∈I.
Any set F ⊆E, F /∈I, is called a dependent set and an inclusion wise maximal
set B such that B ∈I is called a basis. The cardinality of a basis in a matroid
M is called the rank of M and is denoted by rank(M). The rank function of M,
denoted by rM, is a function from 2E to N ∪{0} and is deﬁned as, rM(S) =
maxS′⊆S,S′∈I |S′| for any S ⊆E.
Proposition 1. Let M = (E, I) be a matroid. If A ∈I, B ⊆E and rM
(A ∪B) = ℓthen there is a subset B′ ⊆B, |B′| = ℓ−|A| such that A ∪B′ ∈I.
Deﬁnition 2. Let A be a matrix over a ﬁeld F and E be the set of columns
of A. The pair (E, I), where I deﬁned as follows, is a matroid, called linear
matroid and is denoted by M[A]. For any X ⊆E, X ∈I if and only if the
columns of A corresponding to X are linearly independent over F. If a matroid
M can be deﬁned by a matrix A over a ﬁeld F, then we say that the matroid is
representable over F and A is a linear representation of M.
A matroid M = (E, I) is called representable or linear if it is representable
over some ﬁeld F.
Deﬁnition 3. The dual of a matroid M = (E, I), is a matroid, denoted by M ∗,
on the same ground set E, in which a set S is independent if and only if there
is basis of M fully contained in E \ S.
Deﬁnition 4. Let M = (E, I) be a matroid and F ⊆E. Then the deletion of F
in M, is a matroid, denoted by M\F, whose independent sets are the independent
sets of M that are contained in E \ F.

Linear Representation of Transversal Matroids
425
Deﬁnition 5. Let M = (E, I) be a matroid and F ⊆E. Then the contraction
of M by F, is a matroid, denoted by M/F, on the ground set E \ F, whose rank
function is deﬁned as, rM/F (A) = rM(A ∪F) −rM(F) for any A ⊆E \ F.
Proposition 2 ([14]). Let M = (E, I) be a matroid and F ⊆E. Then M ∗\F =
(M/F)∗.
Proposition 3 ([14]). Given a matroid M = (E, I) with an n × m matrix A
over a ﬁeld F as its linear representation and F ⊆E. Then linear representations
of M ∗, M \ F and M/F over the same ﬁeld F can be computed by using only
polynomially (in n and m) many operations over F.
Deﬁnition 6 (Truncation of a matroid). Let M = (E, I) be a matroid and
k ∈N. Then the k-truncation of M is a matroid M ′ = (E′, I′) such that for any
F ⊆E, F ∈I′ if and only if |F| ≤k and F ∈I.
The following theorem proved in [12] implies that there is an algorithm which
given a linear representation of a matroid M over a ﬁeld F, outputs a linear
representation of a truncation of M over the ﬁeld F(Y ) using only polynomially
many operations over F.
Theorem 3 ([12]). There is an algorithm that, given an n × m matrix M of
rank n over a ﬁeld F and a number k ≤n, uses O(mnk) ﬁeld operations over F
and computes a matrix Mk over the ﬁeld F(Y ), which represents the k-truncation
of M. Furthermore, given Mk and a set of columns in Mk, we can test whether
they are linearly independent using O(n2k3) ﬁeld operations over F.
Note that, in the above theorem Y is a single variable, rather than a set of
variables. Hence testing if a determinant is non-zero over this ﬁeld is simply
a matter of checking if a univariate polynomial in the variable Y is non-zero.
Furthermore, in many cases the output of the above theorem lies in a ﬁnite
degree extension of F. The two important matroids we are interested in this
work are transversal matroids and gammoids.
Deﬁnition 7. Let G = (U ⊎V, E) be a bipartite graph. The transversal matroid
associated with G, denoted by MG, has ground set U and a set U ′ ⊆U is
independent in MG if and only if there is a matching in G saturating U ′.
Deﬁnition 8. Let D be a digraph and S, T ⊆V (D). Then a gammoid with
respect to D and S on ground set T is a matroid (T, I), where I is deﬁned as
follows. For any T ′ ⊆T, T ′ ∈I, if and only if there are |T ′| vertex disjoint
paths whose start vertices are in S and end vertices are in T ′. When T = V (D),
the gammoid is called a strict gammoid. In other words, for any T ⊆V (D) a
gammoid on the ground set T is obtained from the strict gammoid by deleting
V (D) \ T.
As mentioned earlier, there is a randomized polynomial time algorithm to con-
struct a linear representation of transversal matroids [13]. The following useful
result is proved by Ingleton and Piﬀin 1972 [8] (see also [13,14]), shows that
strict gammoids and transversal matroids are duals.

426
P. Misra et al.
Lemma 1. Let D be a digraph and S ⊆V (D). Let T = V (D) and V ′ = V (D)\S
be two copies of vertex subsets. Then there is bipartite graph G = (T ⊎V ′, E) such
that the strict gammoid with respect to D and S is the dual of the transversal
matroid MG on the ground set T. Moreover, there is a polynomial time algorithm
that, given D and S, outputs the graph G.
3
The Algorithm for Representing Transversal Matroids
In this section, we ﬁrst give a deterministic algorithm to compute a linear rep-
resentation of transversal matroids. That is, we give a deterministic algorithm
to construct a linear representation of transversal matroids, and moreover, one
can test whether a subset of the ground set is independent or not, in polyno-
mial time. In the next section, we show that this algorithm may be modiﬁed to
obtain more eﬃcient algorithms for other classes of matroids that are related to
transversal matroids.
Let G = (U ⊎V, E) be a bipartite graph such that U and V contain m
and n vertices, respectively. Let U = {u1, . . . , um} and V = {v1, . . . , vn}. Let
MG = (U, I) be the transversal matroid associated with G where the ground set
is U. That is, S ⊆U is independent in MG if and only if there is a matching
in G, saturating S. Let A′ be the bipartite adjacency matrix of G. That is,
the rows of A′ are indexed with elements from V , columns of A′ are indexed
with elements from U, and for any v ∈V, u ∈U, A′[v, u] = 1 if and only if
vu ∈E. Let A be an n × m matrix deﬁned as follows. For any vi ∈V, uj ∈U,
A[vi, uj] = A′[vi, uj]·xi,j, where X = {xi,j | i ∈[n], j ∈[m]} is a set of variables.
Notice that for any v ∈V, u ∈U, A[v, u] = 0 if and only if vu /∈E. Also, note
that each variable xi,j appears at most once in the matrix A.
Now we describe our algorithm to ﬁnd a linear representation of transver-
sal matroids. We call this algorithm, Algorithm A. For each j ∈[m], we
deﬁne a set Xj = {xi,j | i ∈[n]}. Our algorithm is an iterative algorithm
that produces values for X1, X2, . . . , Xm in order, by solving a system of lin-
ear inequations in the variables in Xi in the i-th iteration. Let r be the size
of a maximum matching in G. Now we deﬁne a family of subsets of U as
B = {S ∈
U
r

| there is a matching in G saturating S}. That is B is the set
of bases in MG and let B = {S1, . . . , St}. Notice that t = |B|, is the number of
bases in MG. Now, for any S ∈B, we ﬁx a matching M(S) saturating S. For
any S ∈B, let R(S) be the set of vertices from V , saturated by M(S). Note that
R(S) ∈
V
r

, and G[S ∪R(S)] has a perfect matching (a matching of size r). Our
goal is to assign values to all the variables in X from a ﬁeld such that for any
S ∈B, det(A[R(S), S]) ̸= 0. We will then show that this is enough to produce a
linear representation of MG (the details may be found in the correctness proof
presented later in this section).
Let us now describe the steps of our algorithm. Recall that for each j ∈[m],
Xj = {xi,j | i ∈[n]}. Let X<j = j−1
j′=1 Xj′ for any j ∈[m] \ {1} and X<1 = ∅.
For S ⊆U, and j ∈[m], we deﬁne S(j) = {uj′ ∈S | j′ ∈[j]}. Let F be a ﬁeld
of size strictly more than
m
r

. Our algorithm assigns values for the variables in

Linear Representation of Transversal Matroids
427
Xj in the increasing order of j. In the j-th iteration, for j ≥1, the algorithm
will assign values for variables in Xj as follows. Note that at this stage, all the
variables in X<j have been assigned values already. We denote by Aj−1 the
matrix A instantiated with the values for X<j.
For any Si ∈B, recall that M(Si) is a ﬁxed matching. Let Mij be the set of
edges in M(Si) which saturate the vertices in Si(j). Let Rij be the subset of V
saturated by Mij. Notice that the matching Mij saturates the vertices Si(j)∪Rij.
Now the algorithm obtains values for Xj by solving the following inequalities.
det(Aj−1[Rij, Si(j)]) ̸= 0
(1)
Observe that for each i ∈[t], det(Aj−1[Rij, Si(j)]) is a linear function of the
variables in Xj, since all the entries in Aj−1[Rij, Si(j)] are elements from F
except the entries in the last column which are either 0 or variables from Xj.
Therefore, (1) can be stated as a system of linear inequations in the variables in
Xj.
DXj ̸= 0
(2)
where D is a t′ × n matrix for some t′ ≤t and 0 is a zero column vector of
length t′. The entries in DXj are linear functions, and note that the constraints
require that every one of these linear functions is non-zero. We also know that
|F| > t′. Now our algorithm will execute the following algorithm (Lemma 2) to
ﬁnd a solution to the system (2).
Lemma 2. Let F be a ﬁeld of size > t and let D be a t × n matrix over F such
that no row vector in D contains only zeros. Then there is an n-length column
vector Y ∗such that DY ∗̸= 0. Moreover such a vector can be computed using
O(t · n) operations over the ﬁeld F.
Proof. Let Y = [y1, . . . , yn]T be a column vector containing n variables. We will
directly give an n step iterative process to compute Y ∗, an instantiation of Y
satisfying the system of linear inequations DY ̸= 0. Initially, set all the variables
in Y to be 0 and we use Y (0) to denote this assignment. In other words, Y (0) is
a n length zero column vector. At each step we ﬁnd out new assignment for Y . In
step i we ﬁnd out an assignment Y (i) for Y and prove that indeed D · Y (i) ̸= 0.
Now for any i ∈[n], at step i, Y (i) is computed as follows. Note that at this step
we have the assignment Y (i −1) for Y . In other words, Y (i −1) is an n-length
column vector such that the jth entry is same as the value assigned for yj in
step i −1. Let Zi be an n-length column vector where all entries are same as
the entries in Y (i −1), except the ith entry which is the variable yi. That is,
in Zi, we did not assign any value to yi, but all other variables are assigned
the same value as in the assignment Y (i −1). Now consider the entries in the
column vector DZi. Some entries are elements in the ﬁeld F and some are linear
functions on variable yi. Let P1(yi) = p1yi + q1, . . . , Pt′(yi) = pt′yi + qt′ be the
entries in DZj which are linear functions. Notice that t′ ≤t. Since the size of
the ﬁeld F is strictly larger than t ≥t′, there is an element a ∈F such that for

428
P. Misra et al.
all j ∈[t′], Pj(yi) ̸= 0, because for any linear function Pj(yi), Pj(yi) = 0 only
when yi = −qj/pj. Now we set Y (i) as follows. Each entry in Y (i) is same as
each entry in Y (i −1), except the ith entry which is set to a. Our algorithm will
output Y (n) as the required column vector. The correctness of the algorithm
follows from the claim below.
Claim (⋆1). For all i ∈{0, 1, . . . , n}, Y (i) satisﬁes the set of inequalities in
DY ̸= 0, containing variables only from {y1, . . . , yi}.
At step i, the algorithm computes DZi and chooses a value for yi by looking
at a linear function of yi in the t-length column vector DZi. Since Zi−1 and Zi
diﬀer only in two entries, DZi can be computed from DZi−1, and the (i−1)st and
ith columns of D, using O(t) operations over F. Since algorithm has n iterations,
the number of ﬁeld operations performed is O(t · n).
⊓⊔
Our algorithm iterates over all values of j from 1, 2, . . . m, and produces an
assignment of values from the ﬁeld F for the variables in Xj in the j-th iteration.
After m iterations an assignment for all variables will have been computed,
and we let Am be the instantiation of the matrix A with this assignment. Our
algorithm outputs Am as the representation of the transversal matroid MG.
This completes the description of Algorithm A. The following two lemmata are
required for proving the correctness of Algorithm A.
Lemma 3 (⋆).
For any j ∈[m] and Si ∈B, let Mij be the set of edges in
M(Si) which saturates Si(j). Let Rij ⊆V be the set of vertices in V saturated
by Mij. Then det(Aj[Rij, Si(j)]) ̸= 0.
Lemma 4 (⋆). Let S′ ⊆U such that there is no matching saturating S′ (or in
other words S′ /∈I). Then the columns corresponding to S′ in Am are linearly
dependent.
Lemma 5. The matrix Am is a linear representation of the matroid MG =
(U, I).
Proof. We need to show that for any U ′ ⊆U, U ′ ∈I if and only if the columns
in Am indexed with elements from U ′ are linearly independent. If U ′ /∈I, then
by Lemma 4, the columns in Am indexed with elements from U ′ are linearly
dependent. Now we need to show that if U ′ ∈I, then the corresponding columns
in Am are linearly independent. Since (U, I) is a matroid, there is a set S ∈B such
that U ′ ⊆S. Note that S(m) = S and M(S) is a ﬁxed matching. Let R be the
subset of V , saturated by the matching M(S). By Lemma 3, det(Am[R, S]) ̸= 0.
This implies that the columns of Am corresponding to S are linearly independent
and since U ′ ⊆S, the columns of Am corresponding to U ′ are also linearly
independent. This completes the proof of the lemma.
⊓⊔
Lemma 6. Algorithm A runs in time O(
m
r

·|E|√n+N), where N is the time
required to perform O(
m
r

n · m) operations over F.
1 Proof of results marked (⋆) have been omitted due to space constraints.

Linear Representation of Transversal Matroids
429
Proof. Algorithm A ﬁrst computes B and for each S ∈B a matching M(S). This
can be done by executing the bipartite maximum matching algorithm for each
r-vertex subset of U. Since there are
m
r

such subsets and each execution of
the bipartite maximum matching algorithm is on a bipartite graph having at
most r + n ≤2n vertices, this step takes time O(
m
r

· |E|√n) [7]. Following
this, Algorithm A executes the algorithm of Lemma 2 once for each Xi, where
i ∈[m]. Since each execution of this algorithm takes O(
m
r

n) ﬁeld operations,
the total number of ﬁeld operations required for the second phase of Algorithm
A is O(
m
r

mn). This completes the proof of the lemma.
⊓⊔
The correctness and running time bounds we have obtained for Algorithm A
imply Theorem 1.
4
Representing Matroids Related to Transversal
Matroids
In this section, we give deterministic algorithms for constructing linear represen-
tations of gammoids and strict gammoids. These algorithms utilize the algorithm
for constructing linear representation of transversal matroids.
Truncations of transversal matroids. Several algorithmic applications
require a linear representation of the k-truncation of matroids [2,5,12]. While,
we can obtain a representation of the k-truncation of a transversal matroid
MG, by applying Theorem 3 to a representation of MG, it very ineﬃcient
when k ≪n which is usually the case in many applications. We can get
a faster algorithm for this problem by slightly modifying Algorithm A and
using Theorem 3. In the new algorithm, call it Algorithm A′, we deﬁne B as
B = {S ∈
U
k

| there is a matching in G saturating S}. That is, B is directly
deﬁned to be the set of bases in the k-truncation of MG. Then we follow the
steps of algorithm A. This algorithm will output an n × m matrix ˆA over a ﬁeld
of size strictly more than
|U|
k

. Lemmata 3 and 4 are clearly true for Algorithm
A′ as well. That is, all the columns corresponds to a basis in k-truncation of MG
form a set of linearly independent vectors. But there may be a set of columns
of size strictly greater than k which are also linearly independent. To get rid
of this, we apply Theorem 3 to obtain the k-truncation of ˆA, which gives us a
linear representation of the k-truncation of MG.
Theorem 4. There is a deterministic algorithm that, given a bipartite graph
G = (U ⊎V, E) and k ∈N, outputs a linear representation of the k-truncation of
the transversal matroid MG = (U, I) over a ﬁeld F(Y ) where F has size strictly
greater than
|U|
k

, in time O(
|U|
k

|V | · |E| + N), where N is the time required to
perform O(
|U|
k

|V | · |U|) operations over F.
Contractions of transversal matroids. Here, we give a faster algorithm to
compute a linear representation for a contracted transversal matroid by mod-
ifying Algorithm A. We will use this linear representation to obtain a linear

430
P. Misra et al.
representation of gammoids. Let MG = (U, I) be the transversal matroid asso-
ciated with the graph G = (U ⊎V, E) on ground set U. Let F ⊆U. One way of
getting a representation of MG/F is to ﬁnd a linear representation of MG and
then ﬁnd a linear representation of MG/F by applying Proposition 3, but we can
do better. Let r be the size of a maximum matching in G, that is rMG(U) = r.
Let rMG(F) = ℓand k = rMG(U) −rMG(F). Note that the rank of MG/F is
k. Now we explain how to modify Algorithm A to get an algorithm, A′′, for
computing a linear representation of MG/F. Towards that we ﬁrst deﬁne B as
B = {S ∈
U
r

| there is a matching in G saturating S and |S \ F| = k}. Now,
Algorithm A′′ follows the steps of Algorithm A and it constructs an n×m matrix
Am. Lemmata 3 and 4 are true in this case as well. Let M[Am] = (E, I′′) be the
matroid represented by the matrix Am. Now Algorithm A′′ run the algorithm
mentioned in Proposition 3 to compute a linear representation C of M[Am]/F.
Lemma 7 (⋆). The matrix C is a linear representation of MG/F.
Now consider the running time of Algorithm A′′. Let M be a maximum matching
in G[F ∪V ] and the F ′ be the set of vertices from F saturated by M. By
Proposition 1, for any S′ ⊆U \ F of cardinality k, there is a matching of size
r in G[F ∪S′] if and only if there is a matching of size r in G[F ′ ∪S′]. This
implies that algorithm A′′ can construct B, by running the bipartite maximum
matching algorithm at most
n−|F |
k

times. Thus, we get the following theorem.
Theorem 5. There is a deterministic algorithm that, given a bipartite graph
G = (U ⊎V, E) and a vertex set F ⊆U, outputs a linear representation of MG/F
over a ﬁeld F of size strictly greater than
|U|−|F |
k

, in time O(
|U|
k

|V |·|E|+N),
where k = rank(MG/F) and N is the time required to perform O(
|U|−|F |
k

|V | ·
|U|) operations over F.
Gammoids. Now we give an algorithm to compute a linear representation of
a gammoid eﬃciently. By Lemma 1, we know that there is a polynomial time
algorithm which given a digraph D and S ⊆V (D), outputs a bipartite graph
G = (T ⊎V ′, E), where T = V (D) and V ′ = V (D) \ S, such that the strict
gammoid with respect to D and S is the dual of the transversal matroid MG on
the ground set T. Thus by Lemma 1, Theorem 1 and Proposition 3, we get the
following theorem.
Theorem 6. Let D be an n-vertex digraph, S ⊆V (D), |S| = r and F be a
ﬁeld of size strictly greater than
 n
n−r

. Then there is a deterministic algorithm
which outputs a linear representation of the strict gammoid with respect to D
and S, over F in time O(
 n
n−r

n3 + N), where N is the time required to perform
O(
 n
n−r

n2) operations over F.
One way to get a representation of a gammoid is to ﬁrst construct a represen-
tation of the strict gammoid in the graph and then delete some elements from
the strict gammoid. However, observe that if we compute the representation of
the strict gammoid via the algorithm of Theorem 6, the running time depends

Linear Representation of Transversal Matroids
431
on the total number of vertices in the graph. We can obtain a much faster algo-
rithm as follows. Let D be a digraph and let S and W be subsets of V (D). Let
M be the gammoid in D with ground set W ⊆V (D), with respect to S ⊆V (D)
of rank r. We may assume that r = |S|. Otherwise, we construct the graph D′
obtained from D by adding S′, a set of r new vertices, and all possible edges from
S′ to S. Now consider the gammoid in D′ with ground set W with respect to
S′. It is easy to see that, for any subset of X of W, there are |X| vertex disjoint
paths from S′ to X in D′ if and only if there are |X| vertex disjoint paths from
S to X in D. So these two gammoids are the same and our assumption holds.
Now let MS be the strict gammoid in D with respect to S and note that the
rank of MS and M are same. Let M ∗
S be the transversal matroid which is the
dual of MS and it is deﬁned on the bipartite graph G = (V (D) ⊎V ′, E), where
V ′ = V (D) \ S. Now let N be the matroid obtained from M ∗
S by contracting
V (D) \ W. It is easy to see the following lemma.
Lemma 8. M = N ∗.
Proof. Since N is a matroid obtained by contracting V (D) \ W in M ∗
S, Proposi-
tion 2 implies that N ∗is the matroid MS \ (V (D) \ W). That is N ∗= M.
⊓⊔
Combining Lemma 8, Theorem 5 and Proposition 3 we obtain Theorem 2.
References
1. Diestel, R.: Graph Theory. Graduate Texts in Mathematics, vol. 173, 3rd edn.
Springer, Berlin (2005)
2. Fomin, F.V., Golovach, P.A., Panolan, F., Saurabh, S.: Editing to connected f-
degree graph. In: 33rd Symposium on Theoretical Aspects of Computer Science,
STACS 2016, 17–20 February 2016, Orl´eans, France, pp. 36:1–36:14 (2016)
3. Fomin, F.V., Lokshtanov, D., Panolan, F., Saurabh, S.: Representative sets of
product families. In: Schulz, A.S., Wagner, D. (eds.) ESA 2014. LNCS, vol. 8737,
pp. 443–454. Springer, Heidelberg (2014). doi:10.1007/978-3-662-44777-2 37
4. Fomin, F.V., Lokshtanov, D., Panolan, F., Saurabh, S.: Eﬃcient computation of
representative families with applications in parameterized and exact algorithms. J.
ACM 63(4), 29:1–29:60 (2016)
5. Goyal, P., Misra, P., Panolan, F., Philip, G., Saurabh, S.: Finding even subgraphs
even faster. In: 35th IARCS Annual Conference on Foundation of Software Tech-
nology and Theoretical Computer Science, FSTTCS 2015, 16–18 December 2015,
Bangalore, India, pp. 434–447 (2015)
6. Hols, E.C., Kratsch, S.: A randomized polynomial kernel for subset feedback vertex
set. In: 33rd Symposium on Theoretical Aspects of Computer Science, STACS 2016,
17–20 February 2016, Orl´eans, France, pp. 43:1–43:14 (2016)
7. Hopcroft, J.E., Karp, R.M.: An n5/2 algorithm for maximum matchings in bipartite
graphs. SIAM J. Comput. 2, 225–231 (1973)
8. Ingleton, A., Piﬀ, M.: Gammoids and transversal matroids. J. Comb. Theory Ser.
B 15(1), 51–68 (1973)
9. Kabanets, V., Impagliazzo, R.: Derandomizing polynomial identity tests means
proving circuit lower bounds. Comput. Complex. 13(1–2), 1–46 (2004)

432
P. Misra et al.
10. Kratsch, S., Wahlstr¨om, M.: Representative sets and irrelevant vertices: new tools
for kernelization. In: Proceedings of the 53rd Annual Symposium on Foundations
of Computer Science (FOCS), pp. 450–459 (2012)
11. Kratsch, S., Wahlstr¨om, M.: Compression via matroids: a randomized polynomial
kernel for odd cycle transversal. ACM Trans. Algorithms 10(4), 20:1–20:15 (2014)
12. Lokshtanov, D., Misra, P., Panolan, F., Saurabh, S.: Deterministic truncation of
linear matroids. In: Halld´orsson, M.M., Iwama, K., Kobayashi, N., Speckmann, B.
(eds.) ICALP 2015. LNCS, vol. 9134, pp. 922–934. Springer, Heidelberg (2015).
doi:10.1007/978-3-662-47672-7 75
13. Marx, D.: A parameterized view on matroid optimization problems. Theor. Com-
put. Sci. 410(44), 4471–4479 (2009)
14. Oxley, J.G.: Matroid Theory. Oxford Graduate Texts in Mathematics, vol. 21, 2nd
edn. Oxford University Press, Cambridge (2010)
15. Shachnai, H., Zehavi, M.: Representative families: a uniﬁed tradeoﬀ-based app-
roach. In: Schulz, A.S., Wagner, D. (eds.) ESA 2014. LNCS, vol. 8737, pp. 786–797.
Springer, Heidelberg (2014). doi:10.1007/978-3-662-44777-2 65

Dynamic Rank-Maximal Matchings
Prajakta Nimbhorkar1(B) and Arvind Rameshwar V.2
1 Chennai Mathematical Institute, Chennai, India
prajakta@cmi.ac.in
2 Birla Institute of Technology and Science Pilani, Hyderabad Campus,
Hyderabad, India
arvind.rameshwar@gmail.com
Abstract. We consider the problem of matching applicants to posts
where applicants have preferences over posts. Thus the input to our
problem is a bipartite graph G = (A ∪P, E), where A denotes a set of
applicants, P is a set of posts, and there are ranks on edges which denote
the preferences of applicants over posts. A matching M in G is called
rank-maximal if it matches the maximum number of applicants to their
rank 1 posts, subject to this the maximum number of applicants to their
rank 2 posts, and so on.
We consider this problem in a dynamic setting, where vertices and
edges can be added and deleted at any point. Let n and m be the number
of vertices and edges in an instance G, and r be the maximum rank used
by any rank-maximal matching in G. We give a simple O(r(m+n))-time
algorithm to update an existing rank-maximal matching under each of
these changes. When r = o(n), this is faster than recomputing a rank-
maximal matching completely using a known algorithm like that of Irving
et al. [13], which takes time O(min((r + n, r√n)m).
1
Introduction
We consider matchings under one-sided preferences. The problem can be modeled
as that of matching applicants to posts where applicants have preferences over
posts. This problem has several important practical applications like allocation
of graduates to training positions [11] and families to government housing [19].
The input to the problem consists of a bipartite graph G = (A ∪P, E), where
A is a set of applicants, P is a set of posts. Each applicant has a subset of posts
ranked in an order of preference. This is referred to as the preference list of the
applicant. An edge (a, p) has rank i if p is an ith choice of a. An applicant can
have any number of posts at rank i, including zero. Thus the edge-set E can be
partitioned as E = E1 ˙∪. . . ˙∪Er, where Ei contains the edges of rank i.
This problem has received lot of attention and there exist several notions of
optimality like pareto-optimality [1], rank-maximality [13], popularity [2], and
fairness. The notion of rank-maximality has been ﬁrst studied by Irving [12], who
R.V. Arvind—Part of the work was done when the author was a summer intern at
Chennai Mathematical Institute.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 433–444, 2017.
DOI: 10.1007/978-3-319-62389-4 36

434
P. Nimbhorkar and A. Rameshwar V.
called it greedy matchings and also gave an algorithm for computing such match-
ings in case of strict lists. A rank-maximal matching matches maximum number
of applicants to their rank 1 posts, subject to that, maximum number of appli-
cants to their rank 2 posts and so on. Irving et al. [13] gave an O(min(n +
r, r√n)m)-time algorithm to compute a rank-maximal matching. Here n =
|A| + |P|, m = |E|, and r denotes the maximum rank on any edge in a rank-
maximal matching. The weighted and capacitated versions of this problem have
been studied in [14] and [17] respectively.
We consider the rank-maximal matching problem in a dynamic setting where
vertices and edges are added and deleted over time. The requirement of dynamic
updates in matchings has been well-studied in literature, with the motivation
of updating an existing optimal matching without recomputing it completely.
Dynamic updates are important in real-world applications as applicants matched
to posts can leave their jobs, or new applicants can apply for a job, or an applicant
can acquire new skills and hence becomes eligible for more posts.
Related work: Bipartite matchings as well as popular matchings have been
extensively studied in a dynamic setting [3–7,10,16]. The algorithms for main-
taining maximum matchings in dynamic bipartite graphs maintain a matching
under addition and deletion of edges that closely approximates the maximum
cardinality matching, and the update time is small i.e. sub-linear or even poly-
logarithmic in the size of the graph. The algorithm of [7] maintains a matching
that has an unpopularity factor of (Δ+k) with O(Δ+Δ2/k) amortized changes
per round for addition or deletion of an edge, and O(Δ2 + Δ3/k) changes per
round for addition and deletion of a vertex for any k > 0. In contrast to this, our
algorithm maintains rank-maximal matchings exactly but needs O(r(m + n))
time for each update. We describe our contribution below.
Recently, independent of our work, [8] give an O(m) algorithm for updating
rank-maximal matchings under addition and deletion of vertices using techniques
similar to ours.
1.1
Our Contribution
We consider the problem of updating an existing rank-maximal matching when
a vertex or edge is added or deleted. We show the following in this paper:
Theorem 1. Given an instance of the rank-maximal matching problem with n
vertices and m edges, there is an O(r(m + n))-time algorithm for updating a
rank-maximal matching when a vertex or edge is added to or deleted from the
instance. Here r is the maximum rank used in any rank-maximal matching in
the instance.
When r = o(n), this is faster than recomputing a rank-maximal matching using
the fastest known algorithm by Irving et al. [13].
Our algorithm crucially uses Irving et al.’s algorithm and the graphs it creates
for each stage. In Irving et al.’s algorithm, at stage i, edges of rank i are added
to the instance and some edges which can not belong to any rank-maximal
matching are deleted. We show that addition or deletion of a vertex or edge can

Dynamic Rank-Maximal Matchings
435
lead to addition and deletion of several edges at each stage, however, at most
one augmenting path is created at each stage. This helps us update each stage
in time O(m+n), thus total time taken is O(r(m+n)) where r is the maximum
rank on any edge in a rank-maximal matching.
It is important to note that addition or deletion of even one edge can change an
existing rank-maximal matching by as much as Ω(n) edges. Also, addition or dele-
tion of a vertex can potentially lead to addition or deletion of Ω(n) edges. In light
of this, it is an interesting aspect of our algorithm that it avoids a complete recom-
putation of a rank-maximal matching. Also, in the instances that arise in practice,
where there is a large number of applicants and posts, typically each applicant
ranks only a small subset of posts. Therefore our algorithm is useful for updating
a rank-maximal matching in such instances substantially faster than recomputing
it completely. We refer the reader to the full version [15] for omitted details.
1.2
Organization of the Paper
In Sect. 2, we give some deﬁnitions and recall the algorithm of Irving et al. [13]
for computing a rank-maximal matching along with some of its properties. The
preprocessing and an overview of the algorithm appear in Sect. 3. The descrip-
tion and analysis of the algorithm is given in Sect. 4. We discuss some related
questions in Sect. 5.
2
Preliminaries
We recall some well-known deﬁnitions and terminology (see e.g. [9]). A matching
M in a graph G is a subset of edges, such that no two of them share a vertex.
For a matched vertex u, we denote by M(u) its partner in M.
Properties of maximum matchings in bipartite graphs: Let G = (A ∪P, E) be
a bipartite graph and let M be a maximum matching in G. The matching M
deﬁnes a partition of the vertex set A∪P into three disjoint sets, deﬁned below:
Deﬁnition 1 (Even, odd, unreachable vertices). A vertex v ∈A ∪P is
even (resp. odd) if there is an even (resp. odd) length alternating path with
respect to M from an unmatched vertex to v. A vertex v is unreachable if there
is no alternating path from an unmatched vertex to v.
The following lemma is well-known in matching theory; see [18] or [13] for a
proof.
Lemma 1 ([18]).
Let E, O, and U be the sets of even, odd, and unreachable
vertices deﬁned by a maximum matching M in G. Then,
(a) E, O, and U are disjoint, and are the same for all the maximum matchings
in G.
(b) In any maximum matching of G, every vertex in O is matched with a vertex
in E, and every vertex in U is matched with another vertex in U. The size
of a maximum matching is |O| + |U|/2.
(c) No maximum matching of G contains an edge with one end-point in O and
the other in O ∪U. Also, G contains no edge with one end-point in E and
the other in E ∪U.

436
P. Nimbhorkar and A. Rameshwar V.
Rank-maximal matchings: An instance of the rank-maximal matchings problem
consists of a bipartite graph G = (A ∪P, E), where A is a set of applicants,
P is a set of posts, and applicants rank posts in order of their preference. That
is the input is a bipartite graph G = (A ∪P, E) where the edges in E can be
partitioned as E1 ∪E2 ∪. . . ∪Er. Here Ei denotes the edges of rank i, and r
denotes the maximum rank any applicant assigns to a post. An edge (a, p) has
rank i if p is an ith choice of a.
Deﬁnition 2 (Signature). The signature of a matching M is deﬁned as an
r-tuple ρ(M) = (x1, . . . , xr) where, for each 1 ≤i ≤r, xi is the number of
applicants who are matched to their ith rank post in M.
Let M, M ′ be two matchings in G, with signatures ρ(M) = (x1, . . . , xr) and
ρ(M ′) = (y1, . . . , yr). Deﬁne M ≻M ′ if xi = yi for 1 ≤i < k ≤r and xk > yk.
Deﬁnition 3 (Rank-maximal matching). A matching M in G is rank-
maximal if M has the maximum signature under the above ordering ≻.
Observe that all the rank-maximal matchings in an instance have the same
cardinality and the same signature.
Construction of Rank-Maximal Matchings: Now we recall Irving et al.’s
algorithm [13] for computing a rank-maximal matching in a given instance
G = (A ∪P, E1 ∪. . . ∪Er). The pseudocode of the algorithm is given in
Algorithm 1, we refer the reader to [13] for details. Recall that Ei is the set
of edges of rank i.
Algorithm 1. An algorithm to compute a rank-maximal matching from [13].
Input: G = (A ∪P, E1 ∪E2 ∪· · · ∪Er).
Output: A rank maximal matching M in G.
1: Let Gi = (A ∪P, E1 ∪E2 ∪· · · ∪Ei)
2: Construct G′
1 = G1. Let M1 be a maximum matching in G′
1.
3: for i = 1 . . . r do
4:
Partition A ∪P as Oi, Ei, Ui with respect to Mi in G′
i.
5:
Delete all edges of rank j > i incident on vertices in Oi ∪Ui.
6:
Delete all edges from G′
i between a node in Oi and a node in Oi ∪Ui.
7:
Add edges in Ei+1 to G′
i; denote the resulting graph G′
i+1.
8:
Compute a maximum matching Mi+1 in G′
i+1 by augmenting Mi.
9: end for
10: Delete all edges from G′
r+1 between a node in Or+1 and a node in Ur+1.
11: Denote the graph G′
r+1 as G′.
12: Return a rank-maximal matching M = Mr+1.
We note the following properties of Irving et al.’s algorithm:
(I1) For every 1 ≤i ≤r, every rank-maximal matching in Gi is contained in
G′
i.
(I2) The matching Mi is rank-maximal in Gi, and is a maximum matching
in G′
i.

Dynamic Rank-Maximal Matchings
437
(I3) If a rank-maximal matching in G has signature (s1, . . . , si, . . . sr) then Mi
has signature (s1, . . . , si).
(I4) The graphs G′
i, 1 ≤i ≤r constructed at the end of iteration i of Irving et al.’s
algorithm, and G′ are independent of the rank-maximal matching computed
by the algorithm. This follows from Lemma 1 and the above invariants.
3
Preprocessing and Overview
In the preprocessing stage, we store the information necessary to perform an
update in O(r(m + n)) time. The preprocessing time is asymptotically same as
that of computing a rank-maximal matching in a given instance by Irving et al.’s
algorithm viz. O(min((r + n, r√n)m)) and uses O((m + n) log n) storage.
3.1
Preprocessing
Given an instance of the rank-maximal matching problem, G = (A ∪P, E) and
ranks on edges, we execute Irving et al.’s algorithm on G. (Algorithm 1 from
Sect. 2.) Recall that n is the number of vertices and m is the number of edges.
We use the reduced graphs G′
i for 1 ≤i ≤r, where G′
i = (A ∪P, E′
i), com-
puted by Algorithm 1 for updating a rank-maximal matching in G on addition
or deletion of an edge or a vertex. If M is a rank-maximal matching in G, then in
each G′
i, we consider the matching Mi = M ∩E′
i. By Invariant (I2) from Sect. 2,
Mi is rank-maximal in Gi. When a vertex or an edge is added to or deleted from
G, the goal is to emulate Algorithm 1 on the new instance H using the reduced
graphs G′
i for each i.
We prove in Lemma 2 below that we do not need to store the reduced graphs
explicitly. The storage can be achieved by storing the original graph G along with
some extra information for each stage. If a vertex becomes odd (respectively
unreachable) at stage i of Algorithm 1, we store the number i and one bit 0
(respectively 1) indicating that, at stage i, the vertex became odd (respectively
unreachable). For each edge, we store the stage at which it gets deleted, if at all.
This takes O((m + n) log n) bits of extra storage.
Lemma 2. A reduced graph G′
i of any stage i of Algorithm 1 can be completely
reconstructed from the stored information as described above. Moreover, this
reconstruction can be done in O(m + n) time.
Proof. Edge-set E′
i of G′
i is a subset of E1 ∪. . . ∪Ei. We go over all the edges
in E1 ∪. . . ∪Ei and keep those edges in E′
i which have not been deleted up to
stage i. This is precisely the information we have stored for each edge. As we go
over each edge exactly once, we need O(m + n) time.
3.2
An Overview of the Algorithm
Let G be a given instance and let H be the updated instance obtained by addition
or deletion of an edge or a vertex. As stated earlier, the goal of our algorithm

438
P. Nimbhorkar and A. Rameshwar V.
is to emulate Algorithm 1 on H using stored in the preprocessing step described
above. Thus our algorithm constructs the reduced graphs H′
i for H by updating
the reduced graphs G′
i, and also a rank-maximal matching M ′ in H by updating
a rank-maximal matching M in G. We prove that the graphs H′
i are same as the
reduced graphs that would be obtained by executing Algorithm 1 on H.
The reduced graph H′
i can be signiﬁcantly diﬀerent from the reduced graph
G′
i for a stage i. However, we show that there is at most one augmenting path
in H′
i for any stage i. Thus each H′
i+1 and M ′
i can be obtained from H′
i, G′
i+1,
and Mi in linear time i.e. O(m + n) time. Also, we need to recompute the labels
E, O, U. This can be done in O(m + n) time by connecting all the unmatched
vertices to a new vertex s and performing BFS from s.
We note that, in Irving et al.’s algorithm, an applicant is allowed to have
any number of posts of a rank i, including zero. Also, because of the one-sided
preferences model, each edge has a unique rank associated with it. Thus addition
of an applicant is analogous to addition of a post. In both the cases, a new vertex
is added to the instance, along with the edges incident on it, and along with the
ranks on these edges. The ranks can be viewed from either applicants’ side or
posts’ side. Therefore, we describe our algorithm for addition of an applicant,
but the same can be used for addition of a post. The same is true for deletion
of a vertex. Deletion of an applicant or post involves deleting a vertex, along
with its incident edges. Hence the same algorithm applies to deletion of both
applicants and posts.
4
The Algorithm
We describe the update algorithm here. Throughout this discussion, we assume
that G is an instance of the rank-maximal matching problem and H is an updated
instance, where an update could be addition or deletion of an edge or a vertex.
We discuss each of these updates separately.
As described in Sect. 3, we ﬁrst run Algorithm 1 on G and compute a rank-
maximal matching M in G. We also store the information regarding each vertex
and edge as described in Sect. 3. In the subsequent discussion, we assume that
we have the reduced graphs G′
i for each rank 1 ≤i ≤r, which can be obtained
in linear-time from the stored information as proved in Lemma 2.
4.1
Addition of a Vertex:
We describe the procedure for addition of a vertex in terms of addition of an
applicant. Addition of a post is analogous as explained in Sect. 3. A description
of the vertex-addition algorithm is given below and then we prove its correctness.
Description of vertex-addition algorithm: Let a be a new applicant to be
added to the instance G. Let Ea be the set of edges along with their ranks, that
correspond to the preference list of a. Thus the new instance is H = ((A∪{a})∪
P, E ∪Ea). The update algorithm starts from G′
1, adds edges of rank 1 from Ea
to G′
1 to get H′
1 and then updates M and H′
1 as follows:

Dynamic Rank-Maximal Matchings
439
Initialization: S, T = ∅. These sets are used later as described below.
The following cases arise while updating H′
1:
Case 1: Each rank 1 post p of a is odd in G′
1: Then H′
1 is same as G′
1, along
with a and its rank 1 edges added.
Case 2: No rank 1 post of a is even but some post is unreachable in
G′
1: Update the labels E, O, U.1 Add those applicants whose label changes
from U to E to the set S, as they need to get higher rank edges in subsequent
stages. Note that their higher rank edges are deleted by Algorithm 1 as they
become unreachable in G′
1. Thus S always stores the vertices which need to
get higher rank edges in subsequent iterations.
Case 3: A rank 1 post p of a is even in G′
1: Then there is an augmenting path
starting with (a, p) in H′
1. Find it and augment M1 to get a rank-maximal
matching M ′
1 in H′
1. Recompute the E, O, U labels. Delete higher rank edges
on those vertices whose labels change from E in G′
1 to U in H′
1.
Delete OO and OU edges if present. Add those vertices to T which are odd
or unreachable in H′
1. These are precisely those vertices that will not get higher
rank edges in any subsequent iteration even if they become even in one such
iteration.
For each subsequent stage i > 1, the algorithm proceeds as follows:
1. Start with H′
i = G′
i. Add a and its undeleted edges up to rank i to H′
i.
2. If there are applicants in the set S as described in Case 2 above, add edges
of rank i incident on them to H′
i.
3. Start with a matching M ′
i in H′
i such that M ′
i has all the edges of M ′
i−1 and
those rank i edges of Mi which are not incident on any vertex matched in
M ′
i−1.
4. Check if there is an augmenting path in H′
i with respect to M ′
i. If so, augment
M ′
i.
5. Recompute the labels E, O, U.
6. Delete higher rank edges on those vertices whose labels change from E to U
or O. Remove such vertices from S if they are present in S.
7. Delete OO or OU edges, if present. Now we have the ﬁnal updated reduced
graph H′
i.
8. Add those vertices from V \ T to S whose labels change from U or O to E.
Add those vertices to T which are odd or unreachable in H′
i.
The algorithm stops when there are no more edges left in H. Figure 1 shows an
example of the various cases considered above.
Analysis of the Vertex-Addition Algorithm: Recall the notation that G
is the given instance and H is the instance obtained by adding an applicant a
along with its incident edges. Moreover, Gi and Hi are subgraphs of G and H
1 In Irving et al.’s algorithm, these labels are called E1, O1, U1. We omit the subscripts
for the sake of bravity. The subscripts are clear from the stage under consideration.

440
P. Nimbhorkar and A. Rameshwar V.
a1
a2
a3
a4
p1
p2
p3
p4
(i)
a1 : p1
a2 : p1, (p2, p3)
a3 : p1, (p2, p4)
a4 : p1
a1
a2
a3
a4
p1
p2
p3
p4
(ii)
a1
:
p1
a2
:
p1
a3
:
p1, (p2, p3, p4)
a4
:
p1
a1
a2
a3
a4
p1
p2
p3
p4
(iii)
a1
:
p1
a2
:
p1, p2
a3
:
p1, (p3, p4)
a4
:
p1, p3
Fig. 1. Example of status change of nodes after addition of applicant a4. Dashed lines
indicate a rank-maximal matching before addition of a4. In (i), a1, p1 are unreachable
before adding a4. After adding a4, p1 becomes odd while a1 becomes even. In (ii), there
is no status change after adding a4. In (iii), there is an augmenting path a4, p3, a3, p4
after adding a4. Augmentation makes all the nodes unreachable. Preference list for
each ﬁgure is shown below the ﬁgure. Note that some edges on p1 are deleted because
they are OO or OU edges.
respectively, consisting of edges up to rank i respectively from G and H. Also G′
i
is the reduced graph corresponding to stage i of an execution of Algorithm 1 on
G whereas H′
i is the graph of stage i for H constructed by the vertex-addition
algorithm. In Theorem 2, we prove that H′
i is indeed the reduced graph that
would be constructed by an execution of Algorithm 1 on H.
The following Lemma is useful in analyzing the running time of the algorithm.
It proves that there can be at most one new augmenting path at any stage i with
respect to Mi in H′
i. Recall that M is a rank-maximal matching in G and Mi
is the subset of M consisting of edges of rank only up to i. Also, M ′
i is a rank-
maximal matching in Hi.
Lemma 3. At each stage i, |Mi| ≤|M ′
i| ≤|Mi|+1. Thus, for any stage i, there
can be at most one augmenting path with respect to Mi in Hi.
Proof. Recall from invariant (I3) of Algorithm 1 mentioned in Sect. 2 that Mi
and M ′
i are the rank-maximal matchings in Gi and Hi respectively. Here Gi and
Hi are the instances G and H with only the edges of ranks 1 to i present.
Consider Mi ⊕M ′
i, which is the set of edges present in exactly one of the two
matchings. This is a collection of vertex-disjoint paths and cycles. Each path
on which the new applicant a does not appear, and each cycle must have the
same number of edges of each rank from Mi and M ′
i. Otherwise we can obtain a
matching which has a better signature than either Mi or M ′
i, which contradicts
the rank-maximality of both the matchings in Gi and Hi respectively. At most
one path can have the new applicant a as one end-point. This path can contain
at most one more edge of M ′
i than that of Mi. This proves the ﬁrst part.

Dynamic Rank-Maximal Matchings
441
To see that there can be an augmenting path at multiple stages, consider
the case where a post p gets matched to the new applicant a at stage i. If p
is matched to an applicant b in M and the edge (b, p) has rank j such that
j > i, then b is matched in G′
j but not in H′
j. Hence there can possibly be a new
augmenting path in H′
j starting at b.
⊓⊔
Correctness of the algorithm is given by the following theorem.
Theorem 2. The vertex-addition amunl43098165928lgorithm correctly updates
the rank-maximal matching and the reduced graphs. Moreover, it runs in time
O(r(m + n)).
Proof. We prove this by induction on ranks. Thus we prove that, if the stage-
wise reduced graphs are updated correctly up to stage i −1, then the algorithm
correctly constructs H′
i, and gives a rank-maximal matching M ′
i in Hi. The base
case is straightforward, we give the induction step here:
Assume that the algorithm has correctly computed H′
j for 1 ≤j < i. We
show that the algorithm then correctly computes H′
i and M ′
i.
Initialization: The algorithm starts from H′
i = G′
i and the matching M ′
i in Hi
is initialized to M ′
i−1 ∪set of those edges in Mithat are vertex-disjoint from
edges in M ′
i−1. Note that there could be a vertex that is matched in M ′
i−1
but not in Mi−1, and possibly matched in Mi. Thus the initial matching M ′
i
is same as Mi except for the updates performed in earlier stages.
Recall that S is the set of vertices which do not have rank i edges in G′
i but
need to get rank i edges in H′
i. The algorithm adds rank i edges on such
vertices. It also adds applicant a and its undeleted edges to H′
i.
Checking for augmenting path: If a is still unmatched, then there could be
an augmenting path in H′
i starting at a. Even if a is matched in M ′
i−1, there
could still be an augmenting path in H′
i with respect to M ′
i, as explained
below:
If a is matched in M ′
i−1, say by a rank j ≤i−1 edge, then there is a post that
is matched in M ′
j but not in Mj, say q. This is because augmentation along
an augmenting path always matches an additional applicant (in this case, a)
and an additional post (in this case q). However, in M, i.e. prior to addition
of a, q may have been matched to some applicant b at a rank k > j. Now q
is matched to a, so b loses its matched edge at stage k. This needs updating
labels O, U, E at subsequent stages. Also, we need to ﬁnd an augmenting path
from b, if any, at a later stage.
Note that there can be at most one augmenting path according to Lemma 3.
Recomputation of labels: Thus, at each stage, the algorithm looks for an
augmenting path and augments M ′
i, if such a path is found. The augmentation
can lead to change of labels, and deletion of edges on those vertices whose
labels change from E to U due to the augmentation. Also, even if there is
no augmentation, there could still be a change of labels due to addition of
edges on vertices in S and also due to addition of edges incident on a. Thus
the labels need to be recomputed anyway. The sets S and T are updated as
mentioned in the base case above.

442
P. Nimbhorkar and A. Rameshwar V.
As all the possible diﬀerences between G′
i to H′
i are considered above, H′
i is
the correct reduced graph of stage i. Further, by Lemma 3, M ′
i is a maximum
matching in Hi obtained by augmenting a rank-maximal matching M ′
i−1 from
H′
i−1. Thus M ′
i is rank-maximal in Hi by correctness of Algorithm 1.
Each of the three operations described above need O(m+n) time. Whenever
the label on a vertex changes, or an edge is deleted, the stored information can
be updated in O(1) time. Thus each stage can be updated in time O(m + n), so
total update time is O(r(m + n)).
⊓⊔
4.2
Deletion of a Vertex
Let an applicant a be deleted from the instance. The case of deletion of a post p
is analogous, as explained in Sect. 3. Let G be the given instance and H be the
updated instance. Thus H = (A \ {a} ∪P, E \ Ea) where Ea is the set of edges
incident on a. Let M be a rank-maximal matching in G. Also assume that the
preprocessing step is executed on G and the information as mentioned in Sect. 3
is stored.
Description of the Vertex-Deletion Algorithm. If a is not matched in
M, then M clearly remains rank-maximal in H, although the reduced graphs
H′
i could diﬀer a lot from the corresponding reduced graphs G′
i for each i. We
describe the algorithm below.
Initialization: S, T = ∅. These sets will be used later, as given in the following
description.
Case (I): a is matched in M: Let j be the rank of the matched edge in M
incident on a and Let M(a) = p. Thus a remains even in the execution of
Algorithm 1 on G at least for j iterations. The algorithm now works as follows:
For each rank i from 1 to j −1, initialize H′
i = G′
i and M ′
i = Mi. Delete edges
of rank up to i incident on a from H′
i. Recompute the labels E, O, U. Delete
from H the edges of rank > i on those applicants whose label changes from E
to U. This is the ﬁnal reduced graph H′
i. Add odd and unreachable vertices
from H′
i to T. The set T contains those vertices that will not get higher rank
edges at later stages even if their label changes to E.
Now we come to the rank j at which a is matched in M. Initialize H′
j = G′
j
and M ′
j = Mj \ {(a, p)}. Delete edges incident on a from H′
j. The following
cases arise:
Case 1: p is odd in G′
j: Find an augmenting path in H′
j with respect to M ′
j
starting at p. Augment M ′
j along this path. Recompute the labels. Delete
from H the edges of rank > j incident on those applicants whose labels
change from E in G′
j to U in H′
j.
Case 2: p is unreachable in G′
j: Recompute the labels E, O, U. Include
those posts to S whose label changes from U to E. These posts need to
get edges of rank > j in subsequent iterations.
Case 3: p is even in G′
j: Recompute the labels E, O, U in H′
j. Remove higher
rank edges on those posts whose labels change from E to U.

Dynamic Rank-Maximal Matchings
443
Add the odd and unreachable vertices from H′
j to T. Remove such vertices
from S, if they are present in S. These are the vertices that will not get higher
rank edges even if they get the label E at a later stage.
For each rank i from j + 1 to r, initialize H′
i = G′
i, except for a and
its incident edges. Add edges of rank i on posts in S. Initialize M ′
i =
M ′
i−1 ∪set of those edges in Mi which are disjoint from the edges in M ′
i−1.
Look for an augmenting path, and augment M ′
i if an augmenting path is
found. Recompute the labels E, O, U. Update S and T as mentioned above.
Case (II): a is unmatched in M: The algorithm involves iterating over i = 1
to r and computing the reduced graphs H′
i as follows: Start with H′
i = G′
i,
deleting a and its incident edges from H′
i, add rank i edges on vertices in
S, recompute the labels E, O, U, include those vertices from V \ T into set
S whose labels change from U to E. Add vertices with O or U labels to T.
Delete higher rank edges on the vertices whose labels are O or U.
The correctness of the vertex-deletion algorithm is given by the theorem
below. The proof and an example appear in the full version [15].
Theorem 3. The vertex-deletion algorithm correctly updates the rank-maximal
matching M on deletion of an applicant. Moreover, it takes time O(r(m + n)).
4.3
Addition and Deletion of an Edge
Modules similar to those for vertex-addition and vertex-deletion can be writ-
ten for addition and deletion of an edge, which would have time complexity
O(r(m + n)) each. However, both edge-addition and edge-deletion can be per-
formed as a vertex-deletion followed by vertex-addition, achieving the same run-
ning time O(r(m + n)). We explain this here. To add an edge (a, p), one can
ﬁrst delete applicant a using the vertex-deletion algorithm thereby deleting all
the edges Ea incident on a, and then the applicant a is added back along with
the edge-set Ea ∪{(a, p)}. The case of edge-deletion is analogous. It is clear that
both edge-addition and edge-deletion can thus be carried out in O(r(m + n))
time.
5
Discussion
In this paper, we give an O(r(m + n)) algorithm to update a rank-maximal
matching when vertices or edges are added and deleted over time. Independent
of our work, [8] give an algorithm for vertex addition and deletion that runs in
O(m) time using similar techniques.
In [9], a switching graph characterization of rank-maximal matchings has
been developed, which has found several applications. It is an interesting question
to explore whether this characterization can be used for dynamic updates.
Acknowledgement. We thank anonymous reviewers for their comments on an earlier
version of this paper. We thank Meghana Nasre for helpful discussions.

444
P. Nimbhorkar and A. Rameshwar V.
References
1. Abraham, D.J., Cechl´arov´a, K., Manlove, D.F., Mehlhorn, K.: Pareto-optimality
in house allocation problems. In: Proceedings of 15th ISAAC, pp. 3–15 (2004)
2. Abraham, D.J., Irving, R.W., Kavitha, T., Mehlhorn, K.: Popular matchings.
SIAM J. Comput. 37(4), 1030–1045 (2007)
3. Abraham, D.J., Kavitha, T.: Dynamic matching markets and voting paths. In:
Arge, L., Freivalds, R. (eds.) SWAT 2006. LNCS, vol. 4059, pp. 65–76. Springer,
Heidelberg (2006). doi:10.1007/11785293 9
4. Baswana, S., Gupta, M., Sen, S.: Fully dynamic maximal matching in o(log n)
update time. SIAM J. Comput. 44(1), 88–113 (2015)
5. Bhattacharya, S., Henzinger, M., Italiano, G.F.: Deterministic fully dynamic data
structures for vertex cover and matching. In: Proceedings of the Twenty-Sixth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, pp. 785–
804 (2015)
6. Bhattacharya, S., Henzinger, M., Nanongkai, D.: New deterministic approximation
algorithms for fully dynamic matching. In: Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2016, pp. 398–411 (2016)
7. Bhattacharya, S., Hoefer, M., Huang, C.-C., Kavitha, T., Wagner, L.: Maintaining
near-popular matchings. In: Halld´orsson, M.M., Iwama, K., Kobayashi, N., Speck-
mann, B. (eds.) ICALP 2015. LNCS, vol. 9135, pp. 504–515. Springer, Heidelberg
(2015). doi:10.1007/978-3-662-47666-6 40
8. Ghosal, P., Kunysz, A., Paluch, K.: The dynamics of rank-maximal and popular
matchings. CoRR, abs/1703.10594 (2017)
9. Ghosal, P., Nasre, M., Nimbhorkar, P.: Rank-maximal matchings – structure and
algorithms. In: Ahn, H.-K., Shin, C.-S. (eds.) ISAAC 2014. LNCS, vol. 8889, pp.
593–605. Springer, Cham (2014). doi:10.1007/978-3-319-13075-0 47
10. Gupta, M., Peng, R.: Fully dynamic (1+ e)-approximate matchings. In: Proceed-
ings of the 54th Annual IEEE Symposium on Foundations of Computer Science,
FOCS 2013, pp. 548–557 (2013)
11. Hylland, A., Zeckhauser, R.: The eﬃcient allocation of individuals to positions. J.
Polit. Econ. 87(2), 293–314 (1979)
12. Irving, R.W.: Greedy matchings. Technical report, University of Glasgow, TR-
2003-136 (2003)
13. Irving, R.W., Kavitha, T., Mehlhorn, K., Michail, D., Paluch, K.E.: Rank-maximal
matchings. ACM Trans. Algorithms 2(4), 602–610 (2006)
14. Kavitha, T., Shah, C.D.: Eﬃcient algorithms for weighted rank-maximal matchings
and related problems. In: Asano, T. (ed.) ISAAC 2006. LNCS, vol. 4288, pp. 153–
162. Springer, Heidelberg (2006). doi:10.1007/11940128 17
15. Nimbhorkar,
P.,
Arvind,
R.V.:
Dynamic
rank-maximal
matchings.
CoRR,
abs/1704.00899 (2017)
16. Onak, K., Rubinfeld, R.: Maintaining a large matching and a small vertex cover.
In: Proceedings of the Forty-Second ACM Symposium on Theory of Computing,
STOC 2010, pp. 457–464 (2010)
17. Paluch, K.: Capacitated rank-maximal matchings. In: Spirakis, P.G., Serna, M.
(eds.) CIAC 2013. LNCS, vol. 7878, pp. 324–335. Springer, Heidelberg (2013).
doi:10.1007/978-3-642-38233-8 27
18. Pulleyblank, W.R.: Matchings and extensions. In: Handbook of Combinatorics,
vol. 1, pp. 179–232. MIT Press, Cambridge (1995)
19. Yuan, Y.: Residence exchange wanted: a stable residence exchange problem. Eur.
J. Oper. Res. 90(3), 536–546 (1996)

Bend Complexity and Hamiltonian Cycles
in Grid Graphs
Rahnuma Islam Nishat(B) and Sue Whitesides
Department of Computer Science, University of Victoria, Victoria, BC, Canada
{rnishat,sue}@uvic.ca
Abstract. Let G be an m × n rectangular grid graph. We study the
problem of transforming Hamiltonian cycles on G under two basic opera-
tions we call ﬂip and transpose. We introduce a new complexity measure,
the bend complexity, for Hamiltonian cycles. Given any two Hamiltonian
cycles C1 and C2 of bend complexity 1, we show that C1 can be trans-
formed to C2 using only a linear number of ﬂips and transposes.
1
Introduction
A grid graph is a ﬁnite, embedded, vertex-induced subgraph of the inﬁnite two
dimensional integer grid. Its vertex set is ﬁnite and its vertices have integer
coordinates. Two vertices are adjacent if and only if they are distance one apart.
A solid grid graph is a grid graph without holes, i.e., each bounded face of the
graph is a unit square. An m × n rectangular grid graph is a solid grid graph of
area (m −1) × (n −1) such that the boundary of the outer face is rectangular.
This graph has m rows and n columns.
The Hamiltonian path and cycle problems have been studied in detail in the
context of grid graphs as initiated by Itai et al. in 1982 [8]. They showed that
it is NP-complete to decide whether a grid graph has a Hamiltonian path or
a Hamiltonian cycle. They also gave necessary and suﬃcient conditions for a
rectangular grid graph to have a Hamiltonian cycle. They left the problem of
deciding whether a Hamiltonian cycle exists in a solid grid graph open. Later
Umans and Lenhart [9] gave a polynomial time algorithm to ﬁnd a Hamiltonian
cycle (if it exists) in a solid grid graph. Afrati [1] gave a linear time algorithm
for ﬁnding Hamiltonian cycles in restricted grids. Cho and Zelikovsky [4] studied
spanning closed trails containing all the vertices of a grid graph. They showed
that the problem of ﬁnding such a trail is NP-complete for grid graphs, but
solvable for a subclass of grid graphs that they called polyminos. Researchers
have explored other grids as well. In 2009, Arkin et al. [2] studied the existence
of Hamiltonian cycles in grid graphs and proved several complexity results for a
variety of types of grids.
In this paper, we deﬁne two simple “local” operations, ﬂip and transpose,
and study transformation of one Hamiltonian cycle of a rectangular grid graph
into another Hamiltonian cycle under those two operations. The idea of applying
local operations to transform one cycle into another was inspired by the study of
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 445–456, 2017.
DOI: 10.1007/978-3-319-62389-4 37

446
R.I. Nishat and S. Whitesides
transforming triangulations by “ﬂipping” one edge at a time. In 1936, Wagner
showed that any triangulation can be transformed into a canonical form, and
hence can be transformed into any other triangulation [10]. Researchers since
have studied diﬀerent aspects of this problem (see [3] for a survey).
Our work is motivated also by applications of Hamiltonian paths and cycles
in grid graphs. Grid graphs are used in path planning problems [7]. Suppose
a vacuum cleaning robot is cleaning an indoor environment mapped as a grid
graph, and to prevent damage to the ﬂoor, the robot must visit each vertex
exactly once. The problem gives rise to a Hamiltonian path or cycle problem [6].
Another application is exploring an area with a given map. By overlaying the
map with a grid graph, exploration is reduced to a Hamiltonian cycle problem.
Enumeration of Hamiltonian paths and cycles in grid graphs has found applica-
tion in polymer science [5].
From now on, unless speciﬁed otherwise, the term m×n grid graph, or simply
grid graph, refers to a rectangular grid graph.
The rest of this paper is organized as follows. In Sect. 2, we introduce two
operations, ﬂip and transpose, and a complexity measure for Hamiltonian cycles
in rectangular grid graphs that we call bend complexity. Section 3 gives an algo-
rithm that, given any two Hamiltonian cycles C1 and C2 with bend complexity
1, transforms C1 into C2 using only a linear number of ﬂips and transposes.
Section 4 concludes the paper with some open problems.
2
Preliminaries
In this section, we introduce two local operations on Hamiltonian cycles on grid
graphs that we call ﬂip and transpose, and a complexity measure for such cycles
that we call the bend complexity.
Let G be an m×n grid graph. We assume that G is embedded on the integer
grid such that the top left corner vertex is at (0, 0). We also assume that the
positive y direction is downward. A vertex of G with integer coordinates (x, y)
is denoted as vx,y. Vertices with the same x-coordinates or y-coordinates form a
column or a row, respectively. All the vertices of column a have x-coordinate a,
and all the vertices of row b have y-coordinate b. We call a subgraph G′ of G a
subgrid of G if G′ is a grid graph, i.e., G′ is an m′ × n′ rectangular grid graph.
We now deﬁne, by way of concrete examples, two “local” operations on any
Hamiltonian cycle on G, where by local, we mean that the changes to the cycle
are made within a small subgrid G′ of G.
Flip. Let G′ be a 3 × 2 or a 2 × 3 subgrid of a grid graph G, where the vertices
a, b, d, f, e, c appear on the boundary of the outer face of G′ in clockwise order,
and the corner vertices of G′ are a, b, f, e. Let C be a Hamiltonian cycle of G
such that only the edges (a, c), (c, d), (d, b) and (e, f) of G′ are included in C.
A ﬂip operation on C in G′ is performed in two steps. First, we remove the
edges (a, c), (c, d) and (d, b) and connect a to b, which gives a Hamiltonian cycle
on G −{c, d}. We then remove the edge (e, f) and add the edges (e, c), (c, d)

Bend Complexity and Hamiltonian Cycles in Grid Graphs
447
and (d, f), which gives a Hamiltonian cycle C′ on G. See Fig. 1(a). Note that
applying ﬂip to C′ in G′ gives back the original Hamiltonian cycle C.
Fig. 1. Two local operations on a Hamiltonian cycle on a grid graph.
Transpose. Let G′′ be a 3 × 3 subgrid of a grid graph G, where the vertices
a, b, g, h, i, f, e, c appear on the boundary of the outer face of G′′ in clockwise
order, and the corner vertices of G′′ are a, g, i, e. The vertex d of G′′ is the only
vertex that does not appear on the boundary of the outer face of G′′. Let C be a
Hamiltonian cycle on G such that the edges (a, c), (f, d), (d, b), (b, g), (g, h), (h, i)
are included in C. Either or both or none of the edges (e, c) and (e, f) might
be on C. A transpose operation on C in G′′ is performed in two steps. First,
we remove the edges (f, d), (d, b), (b, g), (g, h), (h, i) and add the edge (f, i). This
gives a Hamiltonian cycle in G −{d, b, g, h}. We then remove the edge (a, c) and
add the edges (a, b), (b, g), (g, h), (h, d), (d, c), which gives a Hamiltonian cycle
C′ in G. See Fig. 1(b). The edges incident to e are not aﬀected by the transpose
operation. We call e the pin vertex of G′′. Note that C′ can be transformed back
to C by applying transpose on C′ in G′′.
We introduce a complexity measure for Hamiltonian cycles in G.
Bend Complexity. Let C be a Hamiltonian cycle of G. Let v be a vertex of G
and let e1 and e2 be the two edges incident to v that are also included in C. We
say that v is a bend on C if one of e1, e2 is horizontal and the other is vertical.
We deﬁne the level of a bend on C as follows. If the bend is on the boundary
of the outer face of G, then its level is 0. Otherwise, if the bend is connected
to a bend of level k by a straight line of edges of C then the level of this bend
is k + 1. If the bend is connected to two bends, its level is one more than the
minimum of the levels of the neighboring bends.
The bend complexity B(C) of Hamiltonian cycle C is the highest level of any
bend on C. We call a Hamiltonian cycle with bend complexity k a k-complex
Hamiltonian cycle. The 1-complex Hamiltonian cycles have simple structures
that look like “combs”; cycles with higher bend complexity are more complex.
3
1-Complex Hamiltonian Cycles
In this section, we study the structure of Hamiltonian cycles with bend com-
plexity one, i.e., the 1-complex Hamiltonian cycles. We show that any 1-complex
Hamiltonian cycle can be transformed to any other 1-complex Hamiltonian cycle
using a linear number of ﬂips and transposes.

448
R.I. Nishat and S. Whitesides
We ﬁrst deﬁne some terminology. Let G be an m × n rectangular grid graph.
Throughout this section, we assume that m, n ≥3 as otherwise G has at most one
Hamiltonian cycle. The vertices of G with x-coordinate 0 form the west boundary,
the vertices with x-coordinate n −1 form the east boundary, the vertices with
y-coordinate 0 form the north boundary, and the vertices with y-coordinate m−1
form the south boundary.
Let C be a 1-complex Hamiltonian cycle of G. A cookie c is a path in C
that contains more than two vertices and that starts and ends at two adjacent
vertices on the same boundary (north, south, west or east) of G but does not
contain any boundary edges; thus the path travels along a row or column, bends
before reaching the opposite side, then immediately bends again to return to the
same boundary, ending at a vertex adjacent to the start vertex. Thus, C consists
of a set of cookies, and a set of sequences of vertices on C that form straight
line segments on the boundary connecting two cookies or a cookie to a corner
vertex. We call the length of the parallel sides of c the height of c. Observe that
the height of a cookie is always greater than zero. Based on which one of the
four boundaries contains the endpoints of a cookie, we have four types of cookies:
north, south, west and east.
A cookie c lies in a pair of adjacent rows (e.g., rows y and y + 1, for 0 < y <
m−2, when c is a west or east cookie) or adjacent columns (e.g., columns x and
x + 1, for 0 < x < n −2, when c is a north or a south cookie). We call a pair
of adjacent rows of G a horizontal track and denote it by tx, and we call a pair
of adjacent columns a vertical track and denote it by ty, where x and y are the
smaller indices of the pairs.
If C has only one type of cookie, then we call C a canonical Hamiltonian
cycle. Note that if G has a Hamiltonian cycle, at least one of m and n must be
even [8]. If only one of them is even, then G has two canonical Hamiltonian cycles;
otherwise G has four canonical Hamiltonian cycles. Let C1 and C2 be any two 1-
complex Hamiltonian cycles of G. Let C1 and C2 be two canonical Hamiltonian
cycles of G. We show that C1 can be transformed to C2 using only ﬂip and
transpose operations in four steps as shown in Fig. 2: (a) we ﬁrst transform C1
to C1, (b) we transform C2 to C2, (c) if C1 ̸= C2, we then transform C1 to C2,
and (d) ﬁnally, we reverse the steps of (b) to transform C2 to C2. Eﬀectively, we
transform C1 to C1 to C2 to C2 as shown in Fig. 2.
Fig. 2. (a) C1, (b) C1, (c) C2, and (d) C2. Steps (a) and (b): Transforming 1-complex
Hamiltonian cycles C1 and C2 to canonical Hamiltonian cycles C1 and C2, respectively.
Step (c): Transforming C1 to C2. Step (d): Reversing Step (b).

Bend Complexity and Hamiltonian Cycles in Grid Graphs
449
We ﬁrst assume that C1 has only three types of cookies and give an algorithm
to transform C1 to a canonical Hamiltonian cycle in Sect. 3.1. In Sect. 3.2, we use
the algorithm in Sect. 3.1 to design an algorithm for the general case, where C1
can have four types of cookies (Steps (a), (b) and (d)). In Sect. 3.3, we give an
algorithm for transforming any canonical Hamiltonian cycle to another canonical
Hamiltonian cycle (Step (c)). Finally, in Sect. 3.4, we analyze the time complexity
of the algorithms.
3.1
1-Complex Hamiltonian Cycles with Three Types of Cookies
In this section, we give an algorithm, called Algorithm 1, to transform a 1-
complex Hamiltonian cycle with at most three types of cookies to a canonical
Hamiltonian cycle. We ﬁrst give an overview of Algorithm 1.
Let C be a 1-complex Hamiltonian cycle on an m × n grid graph G such
that C has at most three types of cookies. Since we can rotate G by multiples
of 90◦, we can assume that C has no east cookies. Thus all the edges on the
east boundary, which is also column n −1, must be on C. The only bends are
at the corner vertices vn−1,0 and vn−1,m−1. We therefore start from column
n −2 and scan the vertices from top (y-coordinate 0) to bottom (y-coordinate
m−1) in that column. Scanning down column n−2 from the ﬁrst internal vertex
vn−2,1 toward the last one vn−2,m−2 we note that these internal vertices must be
covered by a north cookie until the ﬁrst encounter with a west or south cookie.
After a west cookie is met, subsequent internal vertices must be covered by west
cookies until either a south cookie is met or the south boundary is reached. If
a south cookie is met, it contains all remaining internal vertices. Note that any
north and south cookies covering the internal vertices of column n −2 must
be on the vertical track tn−3. Let the height of the north and south cookies in
vertical track tn−3 be yN and yS, respectively, where yN = 0 if there is no north
cookie in this track and yS = 0 if there is no south cookie in this track. Then,
the west cookies we encounter in column n −2 must be in horizontal tracks
tyN+1, tyN+3, . . . , tm−3−yS.
We apply transpose operations on the west cookies until there are no west
cookies in the vertical track tn−3 and the height of the north cookie is m−2−yS.
Then we apply ﬂip on the south cookie until the height of the north cookie is
maximum (i.e., m−2). An example is shown in Fig. 3. We then move two columns
to the left and continue the same process until we reach column 1 or column
0 (the west boundary). Observe that at any time, the internal vertices in the
vertical tracks that have already been processed are covered by north cookies.
If n is even, we reach column 0 eventually. Then the algorithm ends as
the Hamiltonian cycle now has only north cookies, which means that we have
obtained a canonical Hamiltonian cycle. Otherwise, we reach column 1. In this
case the internal vertices in column 1 must be covered by only west cookies,
since a north or south cookie requires two columns. We then apply (n −3)/2
transpose operations on each west cookie, starting from the bottommost (in the
horizontal track tm−3) west cookie, and get a canonical Hamiltonian cycle with
only west cookies. We now prove the correctness of Algorithm 1.

450
R.I. Nishat and S. Whitesides
Fig. 3. (a) Column n −2 intersects two west cookies and a south cookie. Apply trans-
pose in the 3 × 3 subgrid inside the red box, where the top left corner of the subgrid is
the pin vertex, (b) apply another transpose, (c) then a ﬂip in the 3 × 2 subgrid inside
the red box, (d) apply another ﬂip, and reach (e) ﬁnal state. (Color ﬁgure online)
Theorem 1. Let C be a 1-complex Hamiltonian cycle on an m×n grid graph G
with no east cookies. Then Algorithm 1 transforms C to a canonical Hamiltonian
cycle C of G, where C has either north cookies or west cookies.
Proof. Since C is a Hamiltonian cycle, at least one of m and n must be even [8]. If
n is even, we eventually reach column 0, the west boundary, and all the internal
vertices in the columns between the east and west boundaries are covered by
north cookies only. We now assume that n is odd, and hence m must be even.
When we reach column 1, all the internal vertices in columns 2 through n −2
are covered by only north cookies, so the internal vertices on column 1 must be
covered by only west cookies. We then apply (n −3)/2 transposes on each west
cookie. Since m is even and we are moving two rows up after each step, we will
reach row 0, at which stage we will have a canonical Hamiltonian cycle with only
west cookies.
⊓⊔
3.2
1-Complex Hamiltonian Cycles with Four Types of Cookies
In this section, we give an algorithm to transform any 1-complex Hamiltonian
cycle to a canonical Hamiltonian cycle.
Let C be a 1-complex Hamiltonian cycle on a grid graph G with as many as
four types of cookies. A subpath P ′ of C is a sequence of consecutive vertices on
C such that P ′ is a Hamiltonian path in an m′ × n′ subgrid G′ of G, where the
two endpoints of P ′ are corner vertices of G′ on the same row or same column.
Without loss of generality, assume that the endpoints of P ′ are on row 0 of G′.
We create a subproblem C′ for C by adding another row above row 0 of G′ and
connecting the row’s endpoints to the endpoints of P ′ to form a Hamiltonian
cycle in that augmentation of G′. An example is shown in Fig. 4.
We ﬁrst observe some properties of C.
Properties. Assume there exists a column x that intersects an east and a west
cookie. Then there cannot exist any row that intersects both a north cookie

Bend Complexity and Hamiltonian Cycles in Grid Graphs
451
Fig. 4. (a) A 1-complex Hamiltonian cycle C. Two subpaths of C are shown in red and
gray. (b) A subproblem for C created from the red subpath, and (c) a subproblem for
C created from the gray subpath. The dashed lines show the edges that were added to
the subpaths to create the subproblems. (Color ﬁgure online)
and a south cookie. Moreover, there must exist a pair of east and west cookies
intersecting column x such that if the east cookie is on horizontal track ty, then
the west cookie must be on horizontal track ty+2 or ty−2.
Let C1, C2, . . . , Cp be suproblems for C and let P1, P2, . . . , Pp be the corre-
sponding subpaths. We say that C is partitioned into subproblems C1, C2, . . . , Cp
if each vertex of C appears on exactly one Pi, 1 ≤i ≤p. We now use the above
properties to show that C can be partitioned into at most two subproblems, each
with at most three types of cookies.
Lemma 1. Let C be a 1-complex Hamiltonian cycle. Then C can be partitioned
into at most two disjoint subproblems such that each subproblem has at most
three types of cookies.
Proof. We start scanning from the east boundary. If all the edges on that bound-
ary are on C, then there are no east cookies. Therefore, C is a subproblem in
itself with at most three types of cookies. Otherwise, there is at least one east
cookie. We move one column to the left and continue scanning until one of the
two following events occurs.
1. A column intersects both east and west cookies. Then by the properties
of Hamiltonian cycles, there is no row that intersects both a north and a south
cookie, and there exists a pair of east west cookies such that if the east cookie is
in horizontal track ty then the west cookie must be in horizontal track ty+2 or
horizontal track ty−2. Without loss of generality, we assume that the west cookie
is in horizontal track ty−2 as shown in Fig. 5(a). Then there is no north cookie
that goes below row y −1 and no south cookie that goes above row y. Let P ′
be the subpath of C from vertex v0,y to vn−1,y that includes the vertex v0,m−1,
and let P ′′ be the subpath of C from vertex v0,y−1 to vn−1,y−1 that includes the
vertex v0,0 as shown in Fig. 5(b). We then create two subproblems C′ and C′′ as
shown in Fig. 5(c).

452
R.I. Nishat and S. Whitesides
Fig. 5. (a) Column x intersecting both east and west cookies. (b) Subpaths P ′ and
P ′′, (c) the two subproblems C′ and C′′ created from P ′ and P ′′, respectively.
2. A column has no east cookies. Let that column be x. There can be at
most one south cookie and at most one north cookie intersecting column x. We
now have the following cases to consider.
(a) Any north and south cookies that x intersects are in vertical track tx−1 as
shown in Fig. 6(a). Let P ′ be the subpath of C from vertex vx,0 to vx,m−1
that includes the vertex v0,0, and let P ′′ be the subpath of C from vertex
vx+1,0 to vx+1,m−1 that includes the vertex vn−1,0 (Fig. 6(b)). Then the
subproblem created from P ′ will have no east cookies and the subproblem
created from P ′′ will have no west cookies.
(b) Any north and south cookies are in vertical track tx as shown in Fig. 6(c).
Since this is the ﬁrst column that intersects no east cookies when scanning to
the left from the east boundary, there is at least one east cookie intersecting
column x + 1. Let the ﬁrst east cookie from the top be in horizontal track
ty, where 0 < y ≤m −3. Then there must be a west cookie in the same
track, since the vertex vx,y cannot be covered by any north or south cookies.
Then no north cookie goes below row y−1 and no south cookie comes above
row y + 2. Let P ′ be the subpath of C from vertex v0,y−1 to vn−1,y−1 that
Fig. 6. (a) Vertical track tx−1 contains north or south cookies intersected by column
x, (b) example of this case showing subpaths P ′ and P ′′. (c) Vertical track tx contains
north or south cookies intersected by column x, (d) example of this case.

Bend Complexity and Hamiltonian Cycles in Grid Graphs
453
includes the vertex v0,0, and let P ′′ be the subpath of C from vertex v0,y
to vn−1,y that includes the vertex v0,m−1 (Fig. 6(d)). Then the subproblem
created from P ′ will have no south cookies and the subproblem created from
P ′′ will have no north cookies.
(c) Column x intersects north and south cookies on diﬀerent vertical tracks. Let
the height of the cookie in vertical track tx be y. We assume that the north
cookie is in vertical track tx as shown in Fig. 7(a), where possibly column x
intersects some west cookies. The case when the south cookie is in vertical
track tx is similar. Column x + 1 must intersect an east cookie but no west
cookies, so vertex vx+1,y+1 must be covered by an east cookie. From x we
move to the left column by column until we ﬁnd a west cookie which must
occur at some column x′ > 0. There may be three cases as described below;
see Figs. 7(b)–(d). If there is a west cookie in column x (Fig. 7(b)), then
there must be one in horizontal track ty+1, as the height of the north cookie
is y. If there is no west cookie in column x, then since x does not intersect
any east cookie, vertices vx,y+1 to vx,m−1 must lie in a south cookie or on
the south boundary. Thus no column left of x intersects an east cookie, and
thus the vertex vx−1,y is covered by a north cookie or a west cookie. Vertex
vx−1,y+1 must be covered by a south cookie.
Fig. 7. (a) The north cookie is in vertical track tx and the south cookie is in vertical
track tx−1. (b) Column x intersects a west cookie in horizontal track ty+1. (c) The ﬁrst
west cookie to the left of column x is in horizontal track ty−1. (d) The ﬁrst west cookie
to the left of column x is in horizontal track ty+1.
We keep moving to the left until we arrive at a column x′ such that there is
a west cookie in column x′ −1. Observe that all the north cookies between (and
including) columns x′ and x have height y and all the south cookies between
(and including) these columns have height m−2−y. If there is a north cookie in
vertical track tx′, then the west cookie will be in horizontal track ty−1 as shown
in Fig. 7(c). Otherwise, there is a north cookie in vertical track tx′−1, and the

454
R.I. Nishat and S. Whitesides
west cookie will be in horizontal track ty+1 as shown in Fig. 7(d). Since we have
a west cookie in either horizontal track ty−1 or ty+1 with height x′ −1 and an
east cookie in horizontal track ty+1 with height n −2 −x, and all the north
cookies in columns x′ to x have height y, there is no north cookie in C with
height greater than y. Similarly, there is no south cookie in C that has height
greater than m −2 −y.
We then create two subproblems in a similar way to that shown in
Figs. 6(c)–(d). Let P ′ be the subpath of C from vertex v0,y to vn−1,y that includes
the vertex v0,0, and let P ′′ be the subpath of C from vertex v0,y+1 to vn−1,y+1
that include the vertex v0,m−1. Then the subproblem C′ created from P ′ will
have no south cookies and the subproblem C′′ created from P ′′ will have no
north cookies.
Therefore, C can always be partitioned into at most two subproblems, each
with at most three types of cookies. Observe that in each case above, the sub-
paths are connected on C by two edges on the boundary.
⊓⊔
We now give an algorithm to transform any 1-complex Hamiltonian cycle C
to a canonical Hamiltonian cycle. We call it Algorithm 2.
We ﬁrst partition C into at most two subproblems C′ and C′′ using Lemma 1.
Let C′ and C′′ correspond to the subpaths P ′ and P ′′ of C, respectively. By the
proof of Lemma 1, P ′ and P ′′ must be connected by two boundary edges e1 and
e2. Then we use Algorithm 1 to transform C′ and C′′ to canonical Hamiltonian
cycles. We then remove the extra rows or columns we added to P ′ and P ′′ to
create C′ and C′′, respectively, and join the two subpaths using e1 and e2. Let C1
be the Hamiltonian cycle obtained in this way. Then C1 has at most two types of
cookies. We apply Algorithm 1 on C1 to obtain a canonical Hamiltonian cycle.
We now prove correctness of Algorithm 2.
Theorem 2. Let C be a 1-complex Hamiltonian cycle on an m × n grid graph
G. Then Algorithm 2 transforms C to a canonical Hamiltonian cycle of G.
Proof. By Lemma 1, C can be partitioned into at most two subproblems C′ and
C′′, corresponding to subpaths P ′ and P ′′ of C, respectively. We then apply
Algorithm 1 on C′ and C′′ to obtain canonical Hamiltonian cycles. Recall that
Algorithm 1 takes a 1-complex Hamiltonian cycle with no east cookies and trans-
forms it to a Hamiltonian cycle with only north cookies or only west cookies.
Thus the canonical Hamiltonian cycles obtained from C′ and C′′ cannot have
cookies from the boundaries that were added to P ′ and P ′′; removing those
boundaries gives two subpaths that can be joined to obtain another 1-complex
Hamiltonian cycle C1 on G. Since C was partitioned into at most two subprob-
lems, C1 can have at most two types of cookies. Now we can apply Algorithm 1
on C1 again to obtain a canonical Hamiltonian cycle of G.
⊓⊔
3.3
Transformation Between Canonical Hamiltonian Cycles
In this section we give an algorithm to transform a canonical Hamiltonian cycle
C1 of an m × n grid graph G to another canonical Hamiltonian cycle C2 of G.
We call it Algorithm 3.

Bend Complexity and Hamiltonian Cycles in Grid Graphs
455
Without loss of generality, assume that C1 has west cookies. If C2 has east
cookies, we apply n −2 ﬂips on the west cookie in horizontal track ty, 1 ≤y ≤
m−3, starting from horizontal track t1 until we have only an east cookie in that
track. If C2 has north (or south) cookies, we apply (m −2)/2 transposes in the
vertical tracks tx, 1 ≤x ≤n −3, such that the pin vertex is in column x −1,
starting from vertical track tn−3 until that track is covered by only one north
(south) cookie.
3.4
Complexity Analysis
In this section we analyze the time complexity of Algorithms 1, 2 and 3.
Let C be a 1-complex Hamiltonian cycle in an m × n grid graph G. In
Algorithm 1, we scan every column of G from column n −2 to column 1 or
column 0. The scanning takes linear time O(mn). We apply at most m −2
transposes and ﬂips in each vertical track of C. Since there are (n−2)/2 vertical
tracks, at most (m−2)(n−2)/4 operations are performed on the vertical tracks.
In case n is odd, (n −3)/2 transposes are applied on each horizontal track.
Since there are (m −2)/2 horizontal tracks, exactly (m −2)(n −3)/4 additional
transpose operations are performed in this case. Therefore, the algorithm takes
O(mn) time in total, since each ﬂip or transpose operation takes O(1) time.
In Algorithm 2, partitioning C into two subproblems and merging the solu-
tions to the subproblems take O(mn) time. We apply Algorithm 1 on the two
subproblems and ﬁnally on the cycle obtained from merging the solutions to the
subproblems. Therefore, the total running time of Algorithm 2 is O(mn).
In Algorithm 3, we apply O(mn) ﬂips or transposes, which takes O(mn) time.
Since Algorithm 2 (Steps (a) and (c)) takes O(mn) time and Algorithm 3 (Step
(b)) takes O(mn) time, we can transform any 1-complex Hamiltonian cycle on
G to any other 1-complex Hamiltonian cycle on G in O(mn) time with O(mn)
ﬂips and transpose operations, in other words, in linear time (and space).
4
Conclusions
In this paper, we have studied Hamiltonian cycles on grid graphs, by which
we mean solid rectangular grid graphs. We deﬁned a complexity measure for
Hamiltonian cycles on rectangular grid graphs, and two local operations, ﬂip and
transpose. We showed that any 1-complex Hamiltonian cycle can be transformed
to any other 1-complex Hamiltonian cycle using only a linear number of ﬂips and
transposes, thereby initiating a study of k-complex Hamiltonian cycles in grid
graphs as we next describe.
We deﬁne Hamiltonian cycle graph Gm,n to be the graph whose vertices are
Hamiltonian cycles on an m × n grid graph G. Two vertices u, v of Gm,n have
an edge between them if one can obtain u from v and vice versa by applying a
single ﬂip or transpose operation. The subgraph Gm,n,k of Gm,n contains exactly
the Hamiltonian cycles with bend complexity k. Our result shows that Gm,n,1
is a connected graph and that the diameter of Gm,n,1 is at most O(mn). We

456
R.I. Nishat and S. Whitesides
pose the question whether Gm,n,k is a connected graph, where k > 1. We also
ask whether similar results can be achieved for grid graphs with non-rectangular
outer face boundary or grid graphs with holes. It would also be interesting to
ﬁnd out whether our results can be extended to Hamiltonian paths.
References
1. Afrati, F.: The hamilton circuit problem on grids. RAIRO Theor. Inform. Appl.
28(6), 567–582 (1994)
2. Arkin, E.M., Fekete, S.P., Islam, K., Meijer, H., Mitchell, J.S., Rodr´ıguez, Y.N.,
Polishchuk, V., Rappaport, D., Xiao, H.: Not being (super)thin or solid is hard: a
study of grid hamiltonicity. Comput. Geom. 42(6–7), 582–605 (2009)
3. Bose, P.: Flips. In: Didimo, W., Patrignani, M. (eds.) GD 2012. LNCS, vol. 7704,
pp. 1–1. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36763-2 1
4. Cho, H.-G., Zelikovsky, A.: Spanning closed trail and hamiltonian cycle in grid
graphs. In: Staples, J., Eades, P., Katoh, N., Moﬀat, A. (eds.) ISAAC 1995. LNCS,
vol. 1004, pp. 342–351. Springer, Heidelberg (1995). doi:10.1007/BFb0015440
5. des Cloizeaux, J., Jannik, G.: Polymers in Solution: Their Modelling and Structure.
Clarendon Press, Oxford (1987)
6. Gorbenko, A., Popov, V.: On hamilton paths in grid graphs. Adv. Stud. Theor.
Phys. 7(3), 127–130 (2013)
7. Gorbenko, A., Popov, V., Sheka, A.: Localization on discrete grid graphs. In:
He, X., Hua, E., Lin, Y., Liu, X. (eds.) CICA 2011. Lecture Notes in Electri-
cal Engineering, vol. 107, pp. 971–978. Springer, Dordrecht (2012). doi:10.1007/
978-94-007-1839-5 105
8. Itai, A., Papadimitriou, C.H., Szwarcﬁter, J.L.: Hamilton paths in grid graphs.
SIAM J. Comput. 11(4), 676–686 (1982)
9. Umans, C., Lenhart, W.: Hamiltonian cycles in solid grid graphs. In: Proceedings
of the 38th Annual Symposium on Foundations of Computer Science, FOCS 1997,
pp. 496–505. IEEE Computer Society (1997)
10. Wagner, K.: Bemerkungen zum vierfarbenproblem. Jahresbericht der Deutschen
Mathematiker-Vereinigung 46, 26–32 (1936)

Optimal Covering and Hitting of Line Segments
by Two Axis-Parallel Squares
Sanjib Sadhu1(B), Sasanka Roy2, Subhas C. Nandy2, and Suchismita Roy1
1 Department of CSE, National Institute of Technology Durgapur, Durgapur, India
sanjibsadhu411@gmail.com
2 Indian Statistical Institute, Kolkata, India
Abstract. This paper discusses the problem of covering and hitting a
set of line segments L in R2 by a pair of axis-parallel squares such that
the side length of the larger of the two squares is minimized. We also
discuss the restricted version of covering, where each line segment in L is
to be covered completely by at least one square. The proposed algorithm
for the covering problem reports the optimum result by executing only
two passes of reading the input data sequentially. The algorithm proposed
for the hitting and restricted covering problems produces optimum result
in O(n log n) time. All the proposed algorithms are in-place, and they
use only O(1) extra space. The solution of these problems also give a
√
2 approximation for covering and hitting those line segments L by two
congruent disks of minimum radius with same computational complexity.
Keywords: Two-center problem · Covering line segments by squares ·
Two pass algorithm · Computational geometry
1
Introduction
Covering a point set by squares/disks has drawn interest to the researchers due
to its applications in sensor network. Covering a given point set by k congruent
disks of minimum radius, known as k-center problem, is NP-Hard [12]. For k = 2,
this problem is referred to as the two center problem [3,5,6,8,9,13].
A line segment ℓi is said to be covered (resp. hit) by two squares if every
point (resp. at least one point) of ℓi lies inside one or both of the squares. For a
given set L of line segments, the objective is to ﬁnd two axis-parallel congruent
squares such that each line segment in L is covered (resp. hit) by the union of
these two squares, and the size of the squares is as small as possible. There are
mainly two variations of the covering problem: standard version and discrete
version. In discrete version, the center of the squares must be on some speciﬁed
points, whereas there are no such restriction in standard version. In this paper,
we focus our study on the standard version of covering and hitting a set L of
line segments in R2 by two axis-parallel congruent squares of minimum size.
As an application, consider a sensor network, where each mobile sensor is
moving to and fro along diﬀerent line segment. The objective is to place two
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 457–468, 2017.
DOI: 10.1007/978-3-319-62389-4 38

458
S. Sadhu et al.
base stations of minimum transmission range so that each of mobile sensors are
always (resp. intermittently) connected to any of the base stations. This problem
is exactly same as to cover (resp. hit) the line segments by two congruent disks
(in our case axis-parallel congruent squares) of minimum radius.
Most of the works on the two center problem deal with covering a given point
set. Kim and Shin [11] provided an optimal solution for the two center problem of
a convex polygon where the covering objects are two disks. As mentioned in [11],
the major diﬀerences between the two-center problem for a convex polygon P and
the two-center problem for a point set S are (i) points covered by the two disks
in the former problem are in convex positions (instead of arbitrary positions),
and (ii) the union of two disks should also cover the edges of the polygon P. The
feature (i) indicates the problem may be easier than the standard two-center
problem for points, but feature (ii) says that it might be more diﬃcult. To the
best of our knowledge, there are no works on covering or hitting a set of line
segments by two congruent squares of minimum size.
Related Work: Drenzer [4] covered a given point set S by two axis-parallel
squares of minimum size in O(n) time, where where n = |S|. Ahn and Bae [10]
proposed an O(n2 log n) time algorithm for covering a given point set S by two
disjoint rectangles where one of the rectangles is axis parallel and other one is
of arbitrary orientation, and the area of the larger rectangle is minimized. Two
congruent squares of minimum size covering all the points in S, where each one is
of arbitrary orientation, can be computed in O(n4 log n) time [1]. The best known
deterministic algorithm for the standard version of two-center problem for a point
set S is given by Sharir [13] that runs in O(n log9 n) time. Eppstein [5] proposed
a randomized algorithm for the same problem with expected time complexity
O(n log2 n). The standard and discrete versions of the two-center problem for a
convex polygon P was ﬁrst solved by Kim and Shin [11] in O(n log3 n log log n)
and O(n log2 n) time respectively. Hoﬀmann [7] solved the rectilinear 3-center
problem for a point set in O(n) time. However none of the algorithms in [1,4,7]
can handle the line segments.
Our Work: We propose in-place algorithms for covering and hitting n line
segments in R2 by two axis-parallel congruent squares of minimum size. We also
study the restricted version of the covering problem where each object needs to be
completely covered by at least one of the reported squares. The time complexities
of our proposed algorithms for these three problems are O(n), O(n log n) and
O(n log n) respectively, and these work using O(1) extra work-space. The same
algorithms work for covering/hitting a polygon, or a set of polygons by two
axis-parallel congruent squares of minimum size. We show that the result of
this algorithm can produce a solution for the problem of covering (resp. hitting)
these line segments by two congruent disks of minimum radius in O(n) (resp.
O(n log n)) time with an approximation factor
√
2.

Optimal Covering and Hitting of Line Segments
459
1.1
Notations and Terminologies Used
Throughout this paper, unless otherwise stated a square is used to imply an
axis-parallel square. We will use the following notations and deﬁnition.
Symbols used
Meaning
pq and |pq|
the line segment joining two points p and q, and its length
x(p) (resp. y(p))
x- (resp. y-) coordinates of the point p
|x(p) −x(q)|
horizontal distance between a pair of points p and q
|y(p) −y(q)|
vertical distance between a pair of points p and q
s ∈pq
the point s lies on the line segment pq
□efgh
an axis-parallel rectangle with vertices at e, f, g and h
size(S)
size of square S; it is the length of its one side
LS(S), RS(S)
Left-side of square S and right-side of square S
TS(S), BS(S)
Top-side of square S and bottom-side of square S
Deﬁnition 1. A square is said to be anchored with a vertex of a rectangle
R = □efgh, if one of the corners of the square coincides with that vertex of R.
2
Covering Line Segments by Two Congruent Squares
LCOVER problem: Given a set L = {ℓ1, ℓ2, . . . , ℓn} of n line segments (pos-
sibly intersecting) in R2, the objective is to compute two congruent squares
S1 and S2 of minimum size whose union covers all the members in L.
In the ﬁrst pass, a linear scan is performed among the objects in L, and four
points a, b, c and d are identiﬁed with minimum x-, maximum y-, maximum x-
and minimum y-coordinate respectively among the end-points of L. This deﬁnes
an axis-parallel rectangle R = □efgh of minimum size that covers L, where
a ∈he, b ∈ef, c ∈fg and d ∈gh. We use L = |x(c)−x(a)| and W = |y(b)−y(d)|
as the length and width respectively of the rectangle R = □efgh, and we assume
that L ≥W. We assume that S1 lies to the left of S2. S1 and S2 may or may
not overlap (see Fig. 1). We use σ = size(S1) = size(S2).
Lemma 1
(a) There exists an optimal solution of the problem where LS(S1) and RS(S2)
pass through the points a and c respectively.
(b) The top side of at least one of S1 and S2 pass through the point b, and the
bottom side of at least one of S1 and S2 pass through the point d.

460
S. Sadhu et al.
e
f
g
h
L
2
v2
a
b
c
d
W
L
(a)
(b)
S1
S2
h
f
L
S1
S2
e
g
a
b
c
d
Fig. 1. Squares S1 and S2 are (a) over-
lapping, (b) disjoint
S1
S2
S2
S1
(a)
(b)
TS(S1)
TS(S1)
BS(S1)
RS(S1)
LS(S1)
TS(S2)
RS(S2)
BS(S2)
LS(S1)
LS(S2)
TS(S2)
BS(S2)
RS(S2)
a
b
c
d
a
b
c
d
i1
i2
i1
i2
e
f
g
e
g
f
h
h
RS(S1)
LS(S2)
Fig. 2. (a) Conﬁguration 1 and
(b) Conﬁguration 2 of squares S1
and S2
Thus in an optimal solution of the LCOVER problem, a ∈LS(S1) and
c ∈RS(S2). We need to consider two possible conﬁgurations of an optimum
solution (i) b ∈TS(S2) and d ∈BS(S1), and (ii) b ∈TS(S1) and d ∈BS(S2).
These are named as Conﬁguration 1 and Conﬁguration 2 respectively (see
Fig. 2).
Observation 1
(a) If the optimal solution of LCOVER problem satisﬁes Conﬁguration 1, then
the bottom-left corner of S1 will be anchored at the point h, and the top-right
corner of S2 will be anchored at the point f.
(b) If the optimal solution of LCOVER problem satisﬁes Conﬁguration 2, then
the top-left corner of S1 will be anchored at the point e, and the bottom-right
corner of S2 will be anchored at the point g.
We consider each of the conﬁgurations separately, and compute the two axis-
parallel congruent squares S1 and S2 of minimum size whose union covers the
given set of line segments L. If σ1 and σ2 are the sizes obtained for Conﬁgura-
tion 1 and Conﬁguration 2 respectively, then we report min(σ1, σ2).
Consider the rectangle R = □efgh covering L, and take six points k1, k2, k3,
k4, v1 and v2 on the boundary of R satisfying |k1f| = |ek3| = |hk4| = |k2g| = W
and |ev1| = |hv2| = L
2 (see Fig. 3). Throughout the paper we assume h as the
origin in the co-ordinate system, i.e. h = (0, 0).
Observation 2
(i) The Voronoi partitioning line λ1 of the corners f and h of R = □efgh with
respect to the L∞norm1 is the polyline k1z1z2k4, where the coordinates of its
deﬁning points are k1 = (L −W, W), z1 = (L/2, L/2), z2 = (L/2, W −L/2)
and k4 = (W, 0) (see Fig. 3(a)).
(ii) The Voronoi partitioning line λ2 of e and g of R = □efgh in L∞norm is
the polyline k3z1z2k2 where k3 = (W, W) and k2 = (L −W, 0) (see Fig. 3(b)).
1 L∞distance between two points a and b is given by max(|x(a)−x(b)|, |y(a)−y(b)|).

Optimal Covering and Hitting of Line Segments
461
e
f
g
h
L
L
2
L
2
W
W
L
2
z1
z2
v1
v2
k2
k1
k3
k4
W
(a)
i2
i1
S1
S2
λ1
e
f
g
h
L
L
2
L
2
W
W
L
2
z1
z2
v1
v2
k2
k1
k3
k4
W
(b)
i1
i2
S1
S2
λ2
Fig. 3. Voronoi partitioning line (a) λ1 = k1z1z2k4 of f and h in Conﬁguration 1
(b) λ2 = k3z1z2k2 of e and g in Conﬁguration 2
Note that, if W ≤L
2 , then the voronoi partitioning lines λ1 and λ2 for both the
pairs (f, h) and (e, g) will be same, i.e., λ1 = λ2 = v1v2, where v1 = ( L
2 , 0)
and v2 = ( L
2 , W).
Lemma 2
(a) For Conﬁguration
1, All the points p inside the polygonal region
ek1z1z2k4h satisfy d∞(p, h) < d∞(p, f), and all points p inside the polyg-
onal region k1fgk4z2z1 satisfy d∞(p, f) < d∞(p, h) (see Fig. 3(a)).
(b) Similarly for Conﬁguration
2, all points p inside polygonal region
ek3z1z2k2h, satisfy d∞(p, e) < d∞(p, g), and all points p that lie inside the
polygonal region k3fgk2z2z1, satisfy d∞(p, g) < d∞(p, e) (see Fig. 3(b)).
Lemma 3. If S1 and S2 intersect, then the points of intersection i1 and i2
will always lie on voronoi partitioning line λ1 = k1z1z2k4 (resp. λ2 = k3z1z2k2)
depending on whether S1 and S2 satisfy Conﬁguration 1 or Conﬁguration 2.
Our algorithm consists of two passes. In each pass we sequentially read each
element of the input array L exactly once. We consider W > L
2 only. The other
case i.e. W ≤L
2 can be handled in the similar way.
Pass-1: We compute the rectangle R = □efgh, and the voronoi partitioning
lines λ1 and λ2 (see Fig. 3) for handling Conﬁguration 1 and Conﬁguration 2.
We now discuss Pass 2 for Conﬁguration 1. The same method works for Con-
ﬁguration 2, and for both the conﬁgurations, the execution run simultaneously
keeping a O(1) working storage.
Pass-2: λ1 splits R into two disjoint parts, namely R1 = region ek1z1z2k4h and
R2 = region fk1z1z2k4g. We initialize σ1 = 0. Next, we read elements in the
input array L in sequential manner. For each element ℓi = [pi, qi], we identify

462
S. Sadhu et al.
its portion lying in one/both the parts R1 and R2. Now, considering Lemma 2
and Observation 1, we execute the following:
ℓi lies inside R1: Compute δ = max(d∞(pi, h), d∞(qi, h)).
ℓi lies inside R2: Compute δ = max(d∞(pi, f), d∞(qi, f)).
ℓi is intersected by λ1: Let θ be the point of intersection of ℓi and λ1, pi ∈R1
and qi ∈R2. Here, we compute δ = max(d∞(pi, h), d∞(θ, h), d∞(qi, f)).
If δ > σ1, we update σ1 with δ. Similarly, σ2 is also computed in this pass
considering the pair(e, g) and their partitioning line λ2. Finally, min(σ1, σ2) is
returned as the optimal size along with the centers of the squares S1 and S2.
Theorem 1. Given a set of line segments L in R2 in an array, one can compute
two axis-parallel congruent squares of minimum size whose union covers L by
reading the input array only twice in sequential manner, and maintaining O(1)
extra work-space.
3
Hitting Line Segments by Two Congruent Squares
Deﬁnition 2. A geometric object Q is said to be hit by a square S if at least
one point of Q lies inside (or on the boundary of) S.
Line segment hitting (LHIT) problem: Given a set L = {ℓ1, ℓ2, . . . , ℓn} of
n line segments in R2, compute two axis-parallel congruent squares S1 and
S2 of minimum size whose union hits all the line segments in L.
a
a
d
d
c
c
b
b
e
f
g
h
S1
S2
a
b
c
d
a
a
d
d
c
c
b
b
e
f
g
h
S1
S2
a
b
c
d
(a) S1
S2 = φ
(b) S1
S2 = φ
Fig. 4. Two axis-parallel congruent squares S1 and S2 hit line segments in L
The squares S1 and S2 may or may not be disjoint (see Fig. 4). We now
describe the algorithm for this LHIT problem.
For each line segment ℓi, we use LP(ℓi), RP(ℓi), TP(ℓi) and BP(ℓi) to denote
its left end-point, right end-point, top end-point and bottom end-point using the
relations x(LP(ℓi)) ≤x(RP(ℓi)) and y(BP(ℓi)) ≤y(TP(ℓi)). Now we compute
four line segments ℓa, ℓb, ℓc, and ℓd ∈L such that one of their end-points a, b,
c and d, respectively satisfy the following
a = min
∀ℓi∈L x(RP(ℓi)), b = max
∀ℓi∈L y(BP(ℓi)), c = max
∀ℓi∈L x(LP(ℓi)), d = min
∀ℓi∈L y(TP(ℓi))

Optimal Covering and Hitting of Line Segments
463
e
f
g
h
a
d
a
d
p
q
D1
e
f
g
h
a
d
a
d
p
q
D1
(a)
(b)
Fig. 5. D1 for y(LP(ℓa)) ≥y(RP(ℓa))
and x(TP(ℓd)) < x(BP(ℓd))
e
f
g
h
a
d
a
d
p
q
D1
e
f
g
h
a
d
a
d
p
q
D1
(a)
(b)
r
Fig. 6. D1 for y(LP(ℓa)) ≥y(RP(ℓa))
and x(TP(ℓd)) ≥x(BP(ℓd))
We denote the other end point of ℓa, ℓb, ℓc and ℓd by a′, b′, c′ and d′,
respectively. The four points a, b, c, d deﬁne an axis-parallel rectangle R =
□efgh of minimum size that hits all the members of L (as per Deﬁnition 2),
where a ∈he, b ∈ef, c ∈fg and d ∈gh (see Fig. 4). We use L = |x(c) −x(a)|
and W = |y(b) −y(d)| as the length and width of the rectangle R, and assume
L ≥W. Let S1 and S2 be the two axis-parallel congruent squares that hit the
given line segments L optimally, where S1 lies to the left of S2.
Observation 3. (a) The left side of S1 (resp. right side of S2) must not lie to
the right of (resp. left of) the point a (resp. c), and (b) the top side (resp. bottom
side) of both S1 and S2 cannot lie below (resp. above) the point b (resp. d).
For the LHIT problem, we say S1 and S2 are in Conﬁguration 1, if S1 hits
both ℓa and ℓd, and S2 hits both ℓb and ℓc. Similarly, S1 and S2 are said to be
in Conﬁguration 2, if S1 hits both ℓa and ℓb, and S2 hits both ℓc and ℓd.
Without loss of generality, we assume that S1 and S2 are in Conﬁguration 1.
We compute the reference (poly) line D1 (resp. D2) on which the top-right corner
of S1 (resp. bottom-left corner of S2) will lie. Let T1 (resp. T2) be the line passing
through h (resp. f) with slope 1. Our algorithm consists of the following phases:
1. Computation of the reference lines D1 and D2.
2. Computation of event points for the top-right (resp. bottom-left) corner of
S.1 (resp. S2) on D1 (resp. D2).
3. Searching for pair (S1, S2) that hit all the line segments in L and
max(size(S1),
size(S2)) is minimized.
Computation of the reference lines D1 and D2: The reference line D1 is
computed based on the following four possible orientations of ℓa and ℓd
(i) y(LP(ℓa)) ≥y(RP(ℓa)) and x(TP(ℓd)) < x(BP(ℓd)): Here D1 is the
segment pq on T1 where p is determined (i) by its x-coordinate i.e. x(p) =
x(d), if |ha| < |hd| (see Fig. 5(a)), (ii) by its y-coordinate i.e. y(p) = y(a),
if |ha| ≥|hd| (see Fig. 5(b)). The point q on T1 satisfy x(q) = x(f).

464
S. Sadhu et al.
e
f
g
h
a
d
a
d
p
q
D1
(a)
r
e
f
g
h
a
d
a
d
p
D1
(b)
r
q
e
f
g
h
a
d
a
d
p
q
D1
(c)
r
s
a
d
u
e
f
g
h
a
d
a
d
p
D1
(d)
r
s
a
d
u
q
Fig. 7. D1 for y(LP(ℓa)) < y(RP(ℓa)) and x(TP(ℓd)) > x(BP(ℓd))
(ii) y(LP(ℓa)) ≥y(RP(ℓa)) and x(TP(ℓd)) ≥x(BP(ℓd)): Here,
if |ha| < |hd| (see Fig. 6(a)), then the reference line D1 is a polyline pqr,
where (i) y(p) = y(a) and x(p) satisﬁes |x(p) −x(a)| = vertical distance of
p from the line segment ℓd, (ii) the point q lies on T1 satisfying x(q) = x(d)
and (iii) the point r lies on T1 satisfying x(r) = x(f).
If |ha| ≥|hd| (see Fig. 6(b)), then the reference line D1 is a line segment pq,
where p, q lies on T1, and p satisﬁes y(p) = y(a) and q satisﬁes x(q) = x(f).
(iii) y(LP(ℓa)) < y(RP(ℓa)) and x(TP(ℓd)) ≤x(BP(ℓd)): This case is similar
to case (ii), and we can compute the respective reference lines.
(iv) y(LP(ℓa)) < y(RP(ℓa)) and x(TP(ℓd)) > x(BP(ℓd)): There are two
possible subcases:
(A) If ℓa and ℓd are parallel or intersect (after extension) at a point to the
right of he (Fig. 7(a,b)), then the reference line D1 is a polyline pqr, where
(a) if |ha| < |hd| (Fig. 7(a)), then (1) y(p) = y(a) and |x(p) −x(a)| = the
vertical distance of p from ℓd, (2) the points q and r lie on T1 satisfying
x(q) = x(d) and x(r) = x(f), (b) if |ha| > |hd| (Fig. 7(b)), then (1) x(p) =
x(d) and |y(p) −y(d)| = the horizontal distance of p from ℓa, (2) the points
q and r lie on T1 satisfying y(q) = y(a) and x(r) = x(f).
(B) If extended ℓa and ℓd intersect at a point to the left of he (Fig. 7(c,d)),
then D1 is a polyline pqrs, where
(i) the line segment pq is such that for every point θ ∈pq, the horizontal
distance of θ from ℓa and the vertical distance of θ from ℓd are same.
(ii) the line segment qr is such that for every point θ ∈qr, we have
if |ha| < |hd| then |x(θ) −x(a)| = vertical distance of θ from ℓd (Fig. 7(c)),
else |y(θ) −x(d)| = horizontal distance of θ from ℓa, (Fig. 7(d))
(iii) the point s lies on T1 satisfying x(s) = x(f).
In the same way, we can compute the reference line D2 based on the four
possible orientations of ℓb and ℓc. The break points/end points of D2 will be
referred to as p′, q′, r′, s′ depending on the appropriate cases. From now onwards,
we state the position of square S1 (resp. S2) in terms of the position of its top-
right corner (resp. bottom-left corner).

Optimal Covering and Hitting of Line Segments
465
Observation 4. The point p ∈D1 (resp. p′ ∈D2) gives the position of minimum
sized axis-parallel square S1 (resp. S2) that hit ℓa and ℓd (resp. ℓb and ℓc).
Computation of discrete event points on D1and D2: Observe that the line
segments in L that hits the vertical half-line below the point p ∈D1 (resp. above
the point p′ ∈D2), or the horizontal half-line to the left of the point p ∈D1
(resp. to the right of the point p′ ∈D2) will be hit by any square that hits ℓa
and ℓd (resp. ℓb and ℓc), and these line segments need not contribute any event
point on D1 (resp. D2). For each of the other segments ℓi ∈L, we create an event
point e1
i (resp. e2
i ) on D1 (resp. D2) as follows:
(i) p is an event point on D1 and p′ is an event point on D2 (see Observation 4)
(ii) If ℓi lies completely above D1 (resp. D2), then we compute an event point
e1
i = (xi1, yi1) on D1 (resp. e2
i = (xi2, yi2) on D2) where yi1 = y(BP(ℓi))
(resp. xi2 = x(RP(ℓi))). (e.g. e1
1 for ℓ1 and e2
4 for ℓ4 in Fig. 8).
(iii) If ℓi lies completely below D1 (resp. D2), we compute an event point e1
i =
(xi1, yi1) on D1 (resp. e2
i = (xi2, yi2) on D2) where xi1 = x(LP(ℓi)) (resp.
yi2 = y(TP(ℓi))). (e.g. e1
3 for ℓ3 and e2
6 for ℓ6 in Fig. 8).
(iv) If ℓi intersects with D1 (resp. D2) at point p1 (resp. q1), then we create the
event point e1
i on D1 (resp. e2
i on D2) according to the following rule:
(a) If the x(BP(ℓi)) > x(p1) (resp. x(TP(ℓi)) < x(q1)), then we take p1
(resp. q1) as the event point ei
1 (resp. ei
2). (e.g. e1
4 for ℓ4 in Fig. 8).
(b) If x(BP(ℓi)) < x(p1) then if BP(ℓi) lies below D1 then we consider the
point of intersection by D1 with the vertical line passing through the
BP(ℓi) as the event point e1
i (see e1
2 for ℓ2 in Fig. 8), and if BP(ℓi) lies
above D1 then we consider the point of intersection D1 with the horizontal
line passing through BP(ℓi) as the event point e1
i (see e1
5 for ℓ5 in Fig. 8).
(c) If x(TP(ℓi)) > x(q1) then if TP(ℓi) lies above D2 then we consider the
point of intersection by D2 with the vertical line passing through TP(ℓi)
as the event point e2
i , and if TP(ℓi) lies below D2 then we consider the
point of intersection D2 with the horizontal line passing through TP(ℓi)
as the event point e2
i .
e
f
g
h
a
a
b
b
c
c
d
d
D1
D2
1
2
3
4
5
e1
2
e1
1
e1
3
e1
4
a
d
c
b
6
e2
4
e2
5
e1
5
e2
3
e2
6
p
p
Fig. 8. Event points for LHIT prob-
lem under Conﬁguration 1
Fig. 9. Covering L by two disks D1
& D2

466
S. Sadhu et al.
Observation 5
(i) An event e1
i on D1 shows the position of the top-right corner of the minimum
sized square S1 that hits ℓa, ℓd and ℓi, and an event e2
i on D2 shows the
position of the bottom-left corner of the minimum sized square S2 that hits
ℓb, ℓc and ℓi.
(ii) The square S1 whose top-right corner is at e1
i on D1 hits all those line seg-
ments ℓj whose corresponding event points e1
j on D1 satisﬁes x(h) ≤x(e1
j) ≤
x(e1
i ). Similarly, the square S2 whose bottom-left corner is at ei
2 on D2 hits
all those line segments ℓj whose corresponding event point e2
j on D2 satisﬁes
x(e1
i ) ≤x(e1
j) ≤x(f).
Let us consider an event point e1
i , and the corresponding square S1. We can
identify the size and position of the other square S2 that hit the line segments ℓj
which were not hit by S1 in linear time. Observe that, as the size of S1 increases,
the size of S2 either decreases or remains same. Thus max(size(S1), size(S2))
is a convex function, and we can compute the minimum value of this function
in O(log n) iterations.
We initially set α = 1 and β = n. In each iteration of our in-place algo-
rithm, we compute μ = ⌊α+β
2 ⌋, and compute the μ-th smallest element e∗
among {e1
i , i = 1, 2, . . . , n}, and deﬁne S1 in O(n) time [2]. Next, in a linear
pass, we compute S2 for hitting the line segments ℓj that are not hit by S1. If
size(S1) < size(S2) then we set α = μ, otherwise we set β = μ to execute the
next iteration. If in two consecutive iterations we get the same μ, the process
terminates.
Similarly, we can determine the optimal size of the congruent squares S1 and
S2 in Conﬁguration 2. Finally we consider that conﬁguration for which the
size of the congruent squares is minimized. Thus we get the following result:
Theorem 2. The LHIT problem can be solved optimally in O(n log n) time
using O(1) extra work-space.
4
Restricted Version of LCOVER Problem
In restricted version of the LCOVER problem, each line segment in L is to
be covered completely by atleast one of the two congruent axis-parallel squares
S1 and S2. We compute the axis-parallel rectangle R = □efgh passing through
the four points a, b, c and d as in our algorithm for LCOVER problem. As
in the LCOVER problem, here also we have two possible conﬁgurations for
optimal solution. Without loss of generality, we assume that S1 and S2 satisfy
Conﬁguration 1. We consider two reference lines D1 and D2, each with unit
slope that passes through h and f, respectively. These reference lines D1 and
D2 are the locus of the top-right corner of S1 and bottom-left corner of S2,
respectively. For each line segment ℓi, we create an event point e1
i = (xi1, yi1)
on D1 (resp. e2
i = (xi2, yi2) on D2) as follows:

Optimal Covering and Hitting of Line Segments
467
(i) If ℓi lies completely above D1 (resp. D2), then the event point e1
i on D1
(resp. e2
i on D2) will satisfy yi1 = y(TP(ℓi)) (resp. xi2 = x(LP(ℓi))).
(ii) If ℓi lies completely below D1 (resp. D2) then the event point e1
i on D1 (resp.
e2
i on D2) will satisfy xi1 = x(RP(ℓi)) (resp. yi2 = y(BP(ℓi))).
(iii) If ℓi intersects with D1 then we create the event point e1
i on D1 as follows:
Let the horizontal line through TP(ℓi) intersect with D1 at point p, and the
vertical line through BP(ℓi) intersect with D1 at point q. If x(p) > x(q),
then we take p (else q) as the event point on D1.
(iv) If ℓi intersects with D2, then we create the event point e2
i on D2 as follows:
Let the vertical line through BP(ℓi) intersect with D2 at point p, and the
horizontal line through TP(ℓi) intersect with D2 at point q. If x(p) > x(q),
then we take q (else p) as the event point on D2.
Observation similar to Observation 5 in LHIT problem also holds for this
problem where S1 and S2 cover L with restriction. Thus, here we can follow the
same technique as in LHIT problem to obtain the following result:
Theorem 3. The restricted version of LCOVER problem can be solved opti-
mally in O(n log n) time using O(1) extra work-space.
5
Covering/Hitting Line Segments by Two Congruent
Disks
In this section, we consider problems related to LCOVER, LHIT and
restricted LCOVER problem, called two center problem, where the objective
is to cover, hit or restricted-cover the given line segments in L by two congruent
disks so that their (common) radius is minimized. Figure 9 demonstrates a cover-
ing instance of this two center problem. Here, we ﬁrst compute two axis-parallel
squares S1 and S2 whose union covers/ hits all the members of L optimally as
described in the previous section. Then we report the circum-circles D1 and D2
of S1 and S2 respectively as an approximate solution of the two center problem.
Lemma 4. A lower bound for the optimal radius of two center problem for L
is the radius r′ of in-circle of the two congruent squares S1 and S2 of minimum
size that cover/ hit/ restricted-cover L; i.e. r′ ≤r∗.
The radius r of the circum-circle D1 and D2 of the squares S1 and S2 is
√
2
times of the radius r′ of their in-circles. Lemma 4 says that r′ ≤r∗. Thus, we
have
Theorem 4. Algorithm Two center generates a
√
2 approximation result for
LCOVER, LHIT and restricted LCOVER problems for the line segments in L.

468
S. Sadhu et al.
References
1. Bhattacharya, B., Das, S., Kameda, T., Sinha Mahapatra, P.R., Song, Z.: Opti-
mizing squares covering a set of points. In: Zhang, Z., Wu, L., Xu, W., Du, D.-Z.
(eds.) COCOA 2014. LNCS, vol. 8881, pp. 37–52. Springer, Cham (2014). doi:10.
1007/978-3-319-12691-3 4
2. Carlsson, S., Sundstr¨om, M.: Linear-time in-place selection in less than 3n compar-
isons. In: Staples, J., Eades, P., Katoh, N., Moﬀat, A. (eds.) ISAAC 1995. LNCS,
vol. 1004, pp. 244–253. Springer, Heidelberg (1995). doi:10.1007/BFb0015429
3. Chan, T.M.: More planar two-center algorithms. Comput. Geom. Theory Appl.
13(3), 189–198 (1999)
4. Drezner, Z.: On the rectangular p-center problem. Naval Res. Log. 34(2), 229–234
(1987)
5. Eppstein, D.: Faster construction of planar two-centers. In: 8th ACM-SIAM Sym-
posium On Discrete Algorithms (SODA), pp. 131–138 (1997)
6. Hershberger, J.: A fast algorithm for the two-Center decision Problem. Inf. Process.
Lett. (Elsevier) 47(1), 23–29 (1993)
7. Hoﬀmann, M.: A simple linear algorithm for computing rectilinear 3-centers. Com-
put. Geom. 31(3), 150–165 (2005)
8. Jaromczyk, J.W., Kowaluk, M.: An eﬃcient algorithm for the euclidean two-center
problem. In: Mehlhorn, K. (ed.) Symposium on Computational Geometry, pp. 303–
311. ACM (1994)
9. Katz, M.J., Kedem, K., Segal, M.: Discrete rectilinear 2-center problems. Comput.
Geom. 15(4), 203–214 (2000)
10. Kim, S.S., Bae, S.W., Ahn, H.K.: Covering a point set by two disjoint rectangles.
Int. J. Comput. Geometry Appl. 21(3), 313–330 (2011)
11. Kim, S.K., Shin, C.-S.: Eﬃcient algorithms for two-center problems for a convex
polygon. In: Du, D.-Z.-Z., Eades, P., Estivill-Castro, V., Lin, X., Sharma, A. (eds.)
COCOON 2000. LNCS, vol. 1858, pp. 299–309. Springer, Heidelberg (2000). doi:10.
1007/3-540-44968-X 30
12. Marchetti-Spaccamela, A.: The p center problem in the plane is NP complete.
In: Proceedings of the 19th Allerton Conference on Communication, Control and
Computing, pp. 31–40 (1981)
13. Sharir, M.: A near-linear algorithm for the planar 2-center problem. Discrete Com-
put. Geom. 18(2), 125–134 (1997)

Complexity and Algorithms for Finding
a Subset of Vectors with the Longest Sum
Vladimir Shenmaier(B)
Sobolev Institute of Mathematics, 4 Koptyug Avenue,
630090 Novosibirsk, Russia
shenmaier@mail.ru
Abstract. The problem is, given a set of n vectors in a d-dimensional
normed space, ﬁnd a subset with the largest length of the sum vector.
We prove that the problem is APX-hard for any ℓp norm, p ∈[1, ∞).
For the general problem, we propose an algorithm with running time
O(nd−1(d + log n)), improving previously known algorithms. In particu-
lar, the two-dimensional problem can be solved in a nearly linear time.
We also present an improved algorithm for the cardinality-constrained
version of the problem.
Keywords: Computational geometry · Vector sum · Normed space ·
APX-hardness · Optimal solution
1
Introduction
We consider the following problem:
Longest vector sum (LVS). Given a set X of n vectors in a normed space
(Rd, ∥.∥), ﬁnd a subset S ⊆X with the maximum value of
f(S) =
 
x∈S
x
.
The variation where the subset S is required to have a given cardinality k ∈[1, n]
will be called the longest k-vector sum problem (Lk-VS).
These problems have applications in such ﬁelds as geophysics, radiolocation,
signal ﬁltering. As an example, suppose that we are given a set of measurements
of the direction to some signal source (an acoustic or radio wave source, the mag-
netic ﬁeld), each measurement is a unit vector in R3. The measurements contain
an error and some of them may be received from outliers (other random sources,
reﬂected waves). How to determine the true direction? A reasonable answer is to
ﬁnd a subset of measurements with the maximum aggregate correlation, which
equals to the Euclidean length of the sum vector.
The cardinality-constrained version also arises in the context of searching a
quasiperiodically repeating fragment in a noisy number sequence [5] and detect-
ing the active and passive states of an observed object [3].
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 469–480, 2017.
DOI: 10.1007/978-3-319-62389-4 39

470
V. Shenmaier
Related Work. The LVS and Lk-VS problems are strongly NP-hard if the
Euclidean norm is used [1,13] and polynomially solvable in the case of the ℓ∞
norm and other polyhedral norms with a polynomial number of facets [2]. For
an arbitrary norm, both problems become polynomial if the dimension of space
is ﬁxed [8]. The best known algorithm for the LVS ﬁnds an optimal solution in
time O(nd) [11]1. The Lk-VS problem can be solved in time O(n4d) [8], but the
algorithm from [14] solves the Euclidean problem in time O(nd+1).
Our Contributions. First, we prove a stronger hardness result: for any ℓp
norm, p ∈[1, ∞), the LVS and Lk-VS problems are NP-hard to approximate
within a factor better than α1/p, where α is the inapproximability bound for
Max-Cut, α = 16/17. This holds even if coordinates of the input vectors belong
to {0, ±1}.
On the other hand, for an arbitrary norm, we propose an algorithm solving
the LVS problem in time O(nd−1(d + log n)) if d ≥2. The algorithm is based
on counting regions in hyperplane arrangements in Rd. In particular, we have a
simple nearly linear-time algorithm for the two-dimensional case. Finally, for the
general Lk-VS problem, we give a Voronoi-based algorithm with running time
O(dnd+1), generalizing the result of [14].
2
The Hardness Result
In this section, we prove that the longest vector sum problem is APX-hard. The
proof is done by a simple PTAS-reduction from the following problem:
Max-Cut. Given an undirected graph G = (V, E), ﬁnd a subset of vertices
S ⊆V for which the value of cut(S) is maximized, where cut(S) is the number
of edges in E with one endpoint in S and the other in V \ S.
The complexity of this problem can be described by the following facts:
Fact 1. [6] Max-Cut is NP-hard to approximate within a factor better than 16/17.
Fact 2. [9] Assuming the Unique Games Conjecture, there is no polynomial-
time algorithm which approximates Max-Cut within a factor of 0.879.
Consider the LVS problem for the ℓp norm, p ∈[1, ∞). In this case,
∥x∥=

d

i=1
|x(i)|p1/p
,
where x(i) is the ith coordinate of x. Denote this special case by LVSp.
Theorem 1. For any p ∈[1, ∞) and α ∈(0, 1], if there exists an α-approxima-
tion polynomial-time algorithm for the LVSp, then there exists an αp-approxi-
mation polynomial-time algorithm for Max-Cut.
1 According to [12], the running time of this algorithm is as pointed above and not
O(nd−1) as is asserted in [11].

Finding a Subset of Vectors with the Longest Sum
471
Proof. Given a graph G = (V, E), we construct a set X, an instance of the LVS,
in the following way. Let A be the incidence matrix of G, i.e., the |V |×|E|-matrix
whose (v, e)-entry is 1 if vertex v is incident to edge e and 0 otherwise. Then, for
each e ∈E, the eth column of A contains exactly two 1s, at the rows indexed by
the endpoints of e. Denote one of these endpoints by a(e), the other by b(e), and
replace 1 at the row b(e) with −1. Let xv, v ∈V , be the vth row of the matrix
obtained in this way and X = {xv| v ∈V }.
Given a subset of vertices S ⊆V , compute the vector ξ = 
v∈S xv. For each
e ∈E, we have
ξ(e) =
⎧
⎨
⎩
1 if a(e) ∈S and b(e) /∈S,
−1 if a(e) /∈S and b(e) ∈S,
0 otherwise.
So
∥ξ∥p =

e∈E
|ξ(e)|p = cut(S).
But ∥ξ∥= f(XS), where XS = {xv| v ∈S}. It follows that XS is an α-
approximate solution of the LVS problem on the input set X if and only if
S is an αp-approximate solution of the Max-Cut problem on the graph G. The
theorem is proved.
⊓⊔
Corollary 1. For any p ∈[1, ∞), the LVSp problem is NP-hard to approximate
within a factor better than (16/17)1/p. Assuming the Unique Games Conjecture,
there is no polynomial-time algorithm which approximates this problem within a
factor of 0.8791/p. Both statements hold even if X ⊆{0, ±1}d.
Clearly, all inapproximability factors for the LVS problem are also valid for
its cardinality-constrained version. Moreover, by using the above idea, we can
get a similar reduction to the Lk-VS from the Maximum Bisection problem
which yields an inapproximability factor of (15/16)1/p under the assumption
NP ̸⊆∩ϵ>0DTIME(2nϵ) [7].
3
Algorithms
For a ﬁxed space dimension, both LVS and Lk-VS problems are polynomially
solvable. However, the computational time of O(nd) for the ﬁrst of them and
that of O(n4d) for the second seem to be excessive. In this section, we show how
to improve these time bounds to O(nd−1 log n) and O(nd+1) respectively.
3.1
Finding an Optimal Solution for the LVS Problem
Make some notations. Given a set S ⊆X, put ΣS = 
x∈S x. For any vectors
x, y ∈Rd, denote by xy their dot product. For any v ∈Rd, deﬁne the set
S(v, X) = {x ∈X| xv > 0} and let D(X) = {v ∈Rd| xv ̸= 0 for all x ∈X \{0}},
where 0 is the origin.

472
V. Shenmaier
Lemma 1. (Folklore) Let P be a convex polytope in Rd and u ∈P. Then u is a
vertex of P if and only if there exists a vector v ∈Rd for which u is the unique
maximum point of xv over x ∈P.
Proof. By a deﬁnition, u is a vertex of P if there exists a supporting hyperplane
h such that u is the unique intersection of h and P (e.g., see [10]). Clearly, the
outer normal of h is the required vector v.
⊓⊔
Lemma 2. Let F(X) = {ΣS | S ⊆X}, Z(X) be the convex hull of F(X), and
u ∈Rd. Then u is a vertex of the polytope Z(X) if and only if u = ΣS(v, X)
for some v ∈D(X).
Proof. Suppose that u is a vertex of Z(X). Then u ∈F(X), i.e., u = ΣU for
some U ⊆X. By Lemma 1, there exists a vector v ∈Rd for which u is the unique
maximum point of zv over z ∈Z(X). Then v ∈D(X) since otherwise xv = 0
for some x ∈X \ {0} that gives another maximum point, either u −x if x ∈U
or u + x if x /∈U. On the other hand, by the construction of Z(X), we have
(ΣU) v > (ΣS) v for all S ⊆X such that S ̸= U. Since xv ̸= 0 for all x ∈X,
it follows that U consists exactly of those x ∈X for which xv > 0. This means
that U = S(v, X).
Suppose that u = ΣS(v, X) for some v ∈D(X). By the construction of the
sets S(v, X) and D(X), we have (ΣS(v, X)) v > (ΣS) v for all S ⊆X such that
S ̸= S(v, X). Therefore, u is the unique maximum point of zv over z ∈Z(X).
By Lemma 1, it follows that u is a vertex of Z(X). Lemma 2 is proved.
⊓⊔
Theorem 2. There exists an algorithm which computes all the vectors from the
set S(X) = {ΣS(v, X) | v ∈D(X)} in time O(nd−1(d+log n)) if d ≥2. For any
output vector u ∈S(X), the algorithm allows to restore a set S ⊆X such that
ΣS = u in time O(dn).
Theorem 2 will be proved in Sect. 4.
Theorem 3. There is an algorithm which ﬁnds an optimal solution of the LVS
problem in time O(nd−1(d + log n)) if d ≥2.
Proof. Since any norm is a convex function, the maximum of ∥.∥over the poly-
tope Z(X) is reached at its vertices. On the other hand, all these vertices belong
to the set F(X) and F(X) ⊂Z(X). Therefore, we can ﬁnd an optimal solution
of the LVS problem among all the sets S ⊆X for which ΣS is a vertex of Z(X).
By Lemma 2, it is equivalent to ﬁnding a vector
u∗= arg max
u∈S(X) ∥u∥
and a set S ⊆X such that ΣS = u∗. By Theorem 2, we can do it in time
O(nd−1(d + log n)). Theorem 3 is proved.
⊓⊔
Remark 1. It is easy to see that Theorem 3 is also true for the more general
problem of maximizing any convex function on the set F(X).

Finding a Subset of Vectors with the Longest Sum
473
3.2
Finding Optimal Solutions for the Lk-VS Problem
Given a point v ∈Rd, deﬁne a set Sk(v, X) of k vectors from X with the
maximum dot products with v and let Nk(v, X) be a set of k vectors from X
nearest to v (if such sets are not unique, break the ties arbitrarily). Denote by
Dk(X) the set of vectors v ∈Rd for which Sk(v, X) is uniquely deﬁned and let
Ek(X) be the similar set for Nk( . , X).
Lemma 3. Let Fk(X) = {ΣS | S ⊆X, |S| = k}, Zk(X) be the convex hull of
Fk(X), and u ∈Rd. Then the following holds:
(a) u is a vertex of the polytope Zk(X) if and only if u = ΣSk(v, X) for some
v ∈Dk(X);
(b) if u is a vertex of Zk(X), then u = ΣNk(v, X) for some v ∈Ek(X).
Proof. (a) Suppose that u is a vertex of Zk(X). Then u ∈Fk(X), i.e., u = ΣU
for some k-element set U ⊆X. By Lemma 1, there exists a vector v ∈Rd for
which u is the unique maximum point of zv over z ∈Zk(X). By the construction
of Zk(X), we have (ΣU) v > (ΣS) v for all the k-element sets S ⊆X such that
S ̸= U. It follows that U is the only set of k vectors from X with the maximum
dot products with v. This means that v ∈Dk(X) and U = Sk(v, X).
Suppose that u = ΣSk(v, X) for some v ∈Dk(X). By the construction of the
sets Sk(v, X) and Dk(X), we have (ΣSk(v, X)) v > (ΣS) v for all the k-element
sets S ⊆X such that S ̸= Sk(v, X). Therefore, u is the unique maximum point
of zv over z ∈Zk(X). By Lemma 1, it follows that u is a vertex of Zk(X).
(b) According to (a), if u is a vertex of Zk(X), then u = ΣSk(v, X) for some
v ∈Dk(X). By Lemma 2 of [14], there exists a vector v′ ∈Rd such that, for all
x, y ∈X, if xv > yv, then ∥x −v′∥< ∥y −v′∥. It follows that u = ΣNk(v′, X)
and v′ ∈Ek(X). Lemma 3 is proved.
⊓⊔
Theorem 4. There exists an algorithm which computes all the sets Nk(X) =
{ΣNk(v, X) | v ∈Ek(X)}, k = 1, . . . , n, in time O(dnd+1). For any output vector
u ∈Nk(X), the algorithm allows to restore a k-element set S ⊆X such that
ΣS = u in time O(dn).
Theorem 4 will be proved in Sect. 5.
Theorem 5. There is an algorithm which ﬁnds optimal solutions of the Lk-VS
problem for all k = 1, . . . , n in time O(dnd+1).
Proof. Since any norm is a convex function, the maximum of ∥.∥over the poly-
tope Zk(X) is reached at its vertices. On the other hand, all these vertices belong
to the set Fk(X) and Fk(X) ⊂Zk(X). Therefore, we can ﬁnd an optimal solu-
tion of the Lk-VS problem among all the k-element sets S ⊆X for which ΣS is
a vertex of Zk(X). By Lemma 3(b), it is suﬃcient to ﬁnd a vector
u∗= arg
max
u∈Nk(X) ∥u∥
and a k-element set S ⊆X such that ΣS = u∗. By Theorem 4, we can do it in
time O(dnd+1). Theorem 5 is proved.
⊓⊔

474
V. Shenmaier
Remark 2. It is easy to see that Theorem 5 is also true for the more general
problem of maximizing any convex function on the set Fk(X).
4
Proof of Theorem 2
In this section, we present an algorithm which computes all the vectors from the
set S(X) and, for any output vector u, allows to restore a set S ⊆X such that
ΣS = u. The idea of this algorithm is based on the following concept.
Deﬁnition 1. The hyperplane arrangement for the set X in space Rd is the
collection A(X) of all the open d-dimensional regions in the partition of this
space by the hyperplanes H(x) = {v ∈Rd| xv = 0}, x ∈X \ {0}.
It is easy to see that, for all vectors v from the same region C ∈A(X), the
sets S(v, X) coincide. We will denote these sets by S(C, X). On the other hand,
the set D(X) from the deﬁnition of S(X) is exactly the union of all the regions
in the arrangement A(X). Thus, we have
S(X) = {ΣS(C, X) | C ∈A(X)} = {ΣS(v, X) | v ∈M},
where M is any collection of points in Rd such that M ∩C ̸= ∅for each region
C ∈A(X). Such a collection will be referred to as a member collection for the
arrangement A(X).
4.1
Algorithm for the Plane
First, describe a quick way to construct the set S(X) when the vectors of X lie in
some two-dimensional subspace G of space Rd. This case is one of the recursion
base cases for the future high-dimensional algorithm.
Let E be an orthonormal basis of the linear span of X, which can be con-
structed by using the rules of linear algebra. If |E| = 1, the vectors of X are
collinear and S(X) = {ΣS(±e1, X)}, where e1 is the only vector of E. Hence-
forth, we assume that E consists of two vectors, say, e1 and e2. We will measure
the angles between vectors in G in the direction from e1 to e2. In particular, the
angle from e1 to e2 is π/2, the angle from e2 to e1 is −π/2.
Clearly, the regions of the two-dimensional arrangement A(X) in the plane
G are bounded by the rays starting at the origin and orthogonal to the non-
zero vectors from X. For each x ∈X \ {0}, we have two such rays. These
pass through the vectors l(x) and r(x) obtained by rotating x through angles
of π/2 and −π/2 respectively. Denote these rays by [0, l(x)) and [0, r(x)). Put
L(λ) = {x ∈X| x ̸= 0, l(x) ∈λ} and R(λ) = {x ∈X| x ̸= 0, r(x) ∈λ}.
Lemma 4. Suppose that C1, C2 are adjacent regions in A(X), λ is a ray between
them, and C2 lies in the direction of negative angles from λ. Then
S(C2, X) = (S(C1, X) ∪L(λ)) \ R(λ).
(1)

Finding a Subset of Vectors with the Longest Sum
475
Proof. Let v1 ∈C1, v2 ∈C2, and x ∈X \ {0}. Then we have three following
cases. If l(x) ∈λ, then v2x > 0 and v1x < 0. If r(x) ∈λ, then v2x < 0 and
v1x > 0. Finally, if l(x) /∈λ and r(x) /∈λ, then the sign of v2x equals to that of
v1x. It follows that S(v2, X) = (S(v1, X) ∪L(λ)) \ R(λ) which is equivalent to
(1). The lemma is proved.
⊓⊔
Lemma 4 allows to enumerate all the sets S(C, X), C ∈A(X), by sequential
turning around the origin from ray to ray and updating the set S(C, X) according
to Eq. (1). Therefore, all the vectors from S(X) can be computed with the
following algorithm:
Algorithm 1
Step 1. Construct an orthonormal basis E of the linear span of X.
Step 2. If |E| = 1, return the set {ΣS(±e1, X)}.
Step 3. For each x ∈X \ {0}, construct the vectors l(x) and r(x).
Step 4. Sort the rays [0, l(x)), [0, r(x)), x ∈X \ {0}, by decreasing the angle
from e1; remove coincided; denote the resulting sequence of rays by λ1, . . . , λt;
determine the sets L(λi), R(λi), i = 1, . . . , t.
Step 5. Calculate the vector u1 = ΣS(v, X), where v is any non-zero vector
on the bisector of the rays λ1 and λ2; for each i = 2, . . . , t, compute the vector
ui = ui−1 + ΣL(λi) −ΣR(λi).
Lemma 5. Algorithm 1 computes all the vectors from the set S(X) in time
O(n(d + log n)). For each output vector u, the algorithm allows to restore a set
S ⊆X such that ΣS = u in time O(dn).
Proof. Lemma 4 yields that S(X) = {u1, . . . , ut}. Estimate the running time of
the algorithm. Sorting the rays [0, p), where p = l(x), r(x), x ∈X \ {0}, takes
time O(dn + n log n) since it is reduced to computing and sorting the angles
from e1 to the corresponding vectors p. The other operations can be realized in
time O(dn). For each ui, the set S for which ΣS = ui equals to the set S(v, X),
where v is any non-zero vector on the bisector of λi and λi+1 (here λt+1 = λ1).
Therefore, this set can be computed in a linear time. The lemma is proved.
⊓⊔
4.2
Algorithm for Higher Dimensions
Suppose that a space G is either Rd or the intersection of some hyperplanes H(x),
x ∈X \ {0}. Let x1, . . . , xn be the vectors of the input set X and g1, . . . , gn be
their orthogonal projections into the space G. Given an integer m ∈[1, n], put
X(G, m) = {g1, . . . , gm} and denote by A(G, m) the arrangement for the set
X(G, m) in the space G.
Deﬁnition 2. A member collection M for the arrangement A(G, m) is called
normal if M ⊂D(X(G, n)).
Let P(G, m) be the problem of computing a set of vectors which can be
represented as {ΣS(v, X) | v ∈M}, where M is any normal member collection
for the arrangement A(G, m).

476
V. Shenmaier
Lemma 6. Suppose that the space G is k-dimensional, k, d ≥2, and the set
X(G, m) is given explicitly. Then the problem P(G, m) can be solved in time
Time(k, m) ≤cn(d + log n) min{k, 3} mk−2,
where c is a universal constant independent of k, d, m, n.
Proof. We will use induction on k and m.
Base case k = 2: Construct an orthonormal basis E of the linear span of
X(G, n). If E consists of a single element e1, the set {ΣS(±e1, X)} is a solution
of the problem P(G, m). Otherwise, perform Steps 3 and 4 of Algorithm 1 to the
set X(G, n) and obtain a sequence λ1, . . . , λt of the rays bounding the regions
of the two-dimensional arrangement A(G, n). For each i = 1, . . . , t, choose any
non-zero vector vi from the bisector of the rays λi and λi+1 (where λt+1 = λ1).
Then the vectors v1, . . . , vt form a normal member collection for A(G, n).
It remains to compute the vectors ΣS(vi, X). Note that, for each i = 1, . . . , t
and j = 1, . . . , n, we have vixj = vigj since vi ∈G. Then, by the observations
similar to Lemma 4, the vectors ΣS(vi, X) can be computed as follows:
Step 5’. Calculate the vector ΣS(v1, X); for each i = 2, . . . , t, compute the
vector ΣS(vi, X) as ΣS(vi−1, X) + Σ{xj| gj ∈L(λi)} −Σ{xj| gj ∈R(λi)}.
Thus, we obtain a solution of the problem P(G, n). Show that, for an arbi-
trary m ≤n, we can obtain a solution of P(G, m) which contains at most 2m
vectors. Deﬁne the set Im of indices i such that λi is one of the rays [0, l(x)),
[0, r(x)), x ∈X(G, m) \ {0}. Then |Im| ≤2m and the vectors vi, i ∈Im, form
a normal member collection for the arrangement A(G, m). Therefore, the set
{ΣS(vi, X) | i ∈Im} is a desired solution of P(G, m).
Constructing the basis E takes time O(dn). Steps 3 and 4 of Algorithm 1
and also Step 5’ can be performed in time O(n(d + log n)). The set Im can be
determined in a linear time. Thus, the problem P(G, m) can be solved in time
at most cn(d + log n), where c is a universal constant.
Base case m = 1: Given a vector v = (v(1), . . . , v(d)), let α(v) be the value of
the ﬁrst non-zero coordinate of v and β(v) = maxi |v(i)|/α(v) (put α(0) = 0,
β(0) = 1). Deﬁne the vector p = (bd−1, bd−2, . . . , 1), where b = maxj |β(gj)| + 1.
Then, for each j = 1, . . . , n, the sign of pgj equals to that of α(gj). On the other
hand, pgj = qgj, where q is the projection of p into the space G. Therefore,
q ∈D(X(G, n)), so the vectors ±q form a normal member collection for the
arrangement A(G, 1). At the same time, we have qgj = qxj, which follows that
S(q, X) = {xj | α(gj) > 0} and S(−q, X) = {xj | α(gj) < 0}. Thus, the problem
P(G, 1) can be solved in time O(dn).
Induction step, k ≥3, m ≥2: If gm = 0 or gm is collinear with one of the
non-zero vectors from X(G, m −1), then the arrangement A(G, m) equals to
A(G, m −1), so the solution of the problem P(G, m −1) is also a solution for
P(G, m). Suppose that gm ̸= 0 and gm is non-collinear with the non-zero vectors
from X(G, m −1).

Finding a Subset of Vectors with the Longest Sum
477
Denote by H the (k −1)-dimensional subspace G ∩H(gm). Note that the
arrangement A(G, m) can be obtained from A(G, m −1) by dividing an each
region C ∈A(G, m −1) crossed by H into two parts, C+ and C−. In the ﬁrst of
them, the dot products with gm are positive; in the second, these are negative.
At the same time, the intersection of the region C with H is some region CH in
the arrangement A(H, m −1).
Consider any solution of the problem P(H, m −1) and let ΣS(vH, X) be a
vector from this solution for which vH ∈CH. Put v+ = vH + εgm for some
suﬃciently small ε > 0. Then v+ ∈C+ and, since vH ∈D(X(H, n)), we
have v+ ∈D(X(G, n)) and S(v+, X) = S(vH, X) ∪Dir(gm), where Dir(v) =
{xj| gj ̸= 0, gj is codirected with v, j = 1, . . . , n}. On the other hand, the sets
S(vH, X) and Dir(gm) are disjoint since gm is orthogonal to vH. Therefore,
ΣS(v+, X) = ΣS(vH, X) + ΣDir(gm).
(2)
Similarly, for some v−∈C−∩D(X(G, n)), we have
ΣS(v−, X) = ΣS(vH, X) + ΣDir(−gm).
(3)
Then, after we append the vectors of the type ΣS(v±, X) to a solution of
the problem P(G, m −1), we get that of P(G, m). Thus, we reduce the problem
P(G, m) to the problems P(G, m −1) and P(H, m −1). To apply induction, it
remains to determine the vectors of the set X(H, n). We can calculate them by
using the equation
hj = gj −zm(zmgj),
(4)
where hj is the projection of xj into the space H, j = 1, . . . , n, zm = gm/∥gm∥.
Estimate the running time of the resulting algorithm.
Lemma 7. Let t(k, m) be the number of output vectors in the obtained solution
of the problem P(G, m). Then t(k, m) ≤2mk−1.
Proof. We use induction on k and m. If k = 2 or m = 1, the obtained solutions
are at most 2m-element. Suppose that k ≥3 and m ≥2. Then t(k, m) ≤
t(k, m −1) + 2t(k −1, m −1) which is at most 2(m −1)k−1 + 4(m −1)k−2
by induction hypothesis. Next, for any positive integers a and b, a binomial
inequality (a + 1)b ≥ab + bab−1 holds. Replacing a by a −1 yields
(a −1)b ≤ab −b(a −1)b−1.
(5)
Therefore, t(k, m) ≤2

mk−1 −(k −1)(m −1)k−2
+ 4(m −1)k−2 ≤2mk−1. The
lemma is proved.
⊓⊔
By the construction of the obtained solution, Time(k, m) is at most
Time(k, m −1) + Time(k −1, m −1) + c1dn + c2d t(k −1, m −1),
(6)
where c1dn is a time bound for checking the non-collinearity of gm, computing
two sets Dir(±gm) and their sums, and constructing the input of the problem

478
V. Shenmaier
P(H, m−1) by using Eq. (4); c2d is a time bound for calculating a pair of vectors
ΣS(v±, X) by using Eqs. (2, 3).
By induction hypothesis and Lemma 7, expression (6) does not exceed
cn(d + log n)

min{k, 3}(m −1)k−2 + min{k −1, 3}(m −1)k−3
+c1dn + 2c2d(m −1)k−2.
By (5), the ﬁrst term in the square brackets can be estimated as
min{k, 3}

mk−2 −(k −2)(m −1)k−3
.
Then, since −min{k, 3}(k −2) + min{k −1, 3} ≤−1 for k ≥3, we obtain
Time(k, m) ≤cn(d + log n)

min{k, 3} mk−2 −(m −1)k−3
+c1dn + 2c2d(m −1)k−2
which is at most cn(d + log n) min{k, 3} mk−2 whenever c ≥c1 + 2c2. Lemma 6
is proved.
⊓⊔
To compute all the vectors from the set S(X), it is suﬃcient to solve the prob-
lem P(Rd, n), which can be performed in time O(nd−1(d + log n)) by Lemma 6.
To enable restoring the set S of summands of any output vector u, we equip
each vector ΣS(v±, X) obtained by using Eqs. (2, 3) in the above algorithm
with pointers into the vector ΣS(vH, X) and the corresponding set Dir(±gm).
This allows to represent the set S as Dir(y1) ∪· · · ∪Dir(ys) ∪S(v, X), where
each yi is a vector of the type ±gm, s ≤min{d −2, n −1}, and S(v, X) is a set
whose sum is an output vector for one of the base cases, k = 2 or m = 1. To
restore the set S(v, X) in the case k = 2, we equip each output vector ΣS(vi, X),
i = 1, . . . , t, with the corresponding vector vi, which is computed explicitly. In
the case m = 1, we equip each output vector ΣS(±q, X) with a pointer to the
corresponding set S(±q, X). Thus, since s ≤n, the required set S can be restored
in time O(dn). Theorem 2 is proved.
5
Proof of Theorem 4
In this section, we present an algorithm which computes all the sets Nk(X),
k = 1, . . . , n, and, for any output vector u ∈Nk(X), allows to restore a k-
element set S ⊆X such that ΣS = u. The idea of this algorithm is based on
using higher-order Voronoi diagrams.
Deﬁnition 3. The Voronoi cell for a subset S ⊆X is the set
V (S, X) = {z ∈Rd| ∥z −x∥< ∥z −y∥for all x ∈S, y ∈X \ S}.
Given an integer k ∈[1, n], the k-order Voronoi diagram for the set X is the
collection Vk(X) of all the non-empty Voronoi cells V (S, X), S ⊆X, |S| = k.

Finding a Subset of Vectors with the Longest Sum
479
A Voronoi cell V (S, X) consists of the points of Rd for which the elements of
S are closer than the other elements of X. Clearly, for each cell C ∈Vk(X), the
k-element subset S ⊆X such that C = V (S, X) is exactly the set Nk(z, X) for
any z ∈C. We denote this subset by Nk(C, X).
Fact 3. [4] The total number of cells in all the Voronoi diagrams of order 1 to
n is O(nd+1).
Edelsbrunner et al. [4] proposed an algorithm for computing higher-order
Voronoi diagrams. We will denote this algorithm by V. Its output is a data
structure which describes the construction (faces and cells) of all Vk(X), k =
1, . . . , n. Particularly, each cell C ∈Vk(X), k > 1, is equipped with a pointer
into a cell C′ ∈Vk−1(X) and a vector s(C) ∈Nk(C, X) \ Nk−1(C′, X) for which
Nk(C, X) = Nk−1(C′, X)∪{s(C)}; each cell of V1(X) is equipped with the vector
s(C) for which N1(C, X) = {s(C)}. It follows that
Nk(C, X) = {s(C), s(C′), s(C′′), . . . }.
(7)
Fact 4. [4] The running time of the algorithm V is O(nd+1).
On the other hand, for each k = 1, . . . , n, the set Ek(X) from the deﬁnition
of Nk(X) is exactly the union of all the cells from Vk(X). Therefore, we can
compute the vectors from all the sets Nk(X) with the following algorithm:
Algorithm 2
Step 1. By using algorithm V, construct all the Voronoi diagrams Vk(X), k =
1, . . . , n.
Step 2. For each k = 1, . . . , n and each cell C ∈Vk(X), calculate the vector
ΣNk(C, X) that equals to either ΣNk−1(C′, X)+s(C) if k > 1 or s(C) if k = 1.
By Facts 3 and 4, the running time of Algorithm 2 is O(dnd+1). For any
output vector u, we may restore a k-element set S ⊆X such that ΣS = u by
using Eq. (7) in time O(dn). Theorem 4 is proved.
6
Conclusion
We present improved hardness and algorithmic results for the problem of ﬁnding
subset of vectors with the longest sum. We prove its APX-hardness in the case
of any ℓp norm, p ∈[1, ∞), and reduce the computational time for an arbitrary
norm. A similar result for the cardinality-constrained version is given.
An interesting open question is the existence of a polynomial-time constant-
factor approximation algorithm for the LVS and Lk-VS problems. In other words,
are these problems APX-complete? Another open question is their parameterized
complexity with respect to the parameter d in the Euclidean case.
Acknowledgments. This work is supported by the Russian Science Foundation under
grant 16-11-10041.

480
V. Shenmaier
References
1. Baburin, A.E., Gimadi, E.K., Glebov, N.I., Pyatkin, A.V.: The problem of ﬁnding
a subset of vectors with the maximum total weight. J. Appl. Industr. Math. 2(1),
32–38 (2008)
2. Baburin, A.E., Pyatkin, A.V.: Polynomial algorithms for solving the vector sum
problem. J. Appl. Industr. Math. 1(3), 268–272 (2007)
3. Dolgushev, A.V., Kel’manov, A.V., Shenmaier, V.V.: Polynomial-time approxima-
tion scheme for a problem of partitioning a ﬁnite set into two clusters. Proc. Steklov
Inst. Math. 295(Suppl 1), 47–56 (2016)
4. Edelsbrunner, H., O’Rourke, J., Seidel, R.: Constructing arrangements of lines and
hyperplanes with applications. SIAM J. Comput. 15(2), 341–363 (1986)
5. Gimadi, E.K., Kel’manov, A.V., Kel’manova, M.A., Khamidullin, S.A.: A poste-
riori detecting a quasiperiodic fragment in a numerical sequence. Pattern Recogn.
Image Anal. 18(1), 30–42 (2008)
6. H˚astad, J.: Some optimal inapproximability results. J. ACM 48(4), 798–859 (2001)
7. Holmerin, J., Khot, S.: A new PCP outer veriﬁer with applications to homogeneous
linear equations and max-bisection. In: 36th Annual ACM Symposium on Theory
of Computing, pp. 11–20. ACM, New York (2004)
8. Hwang, F.K., Onn, S., Rothblum, U.G.: A polynomial time algorithm for shaped
partition problems. SIAM J. Optim. 10(1), 70–81 (1999)
9. Khot, S., Kindler, G., Mossel, E., O’Donnell, R.: Optimal inapproximability results
for MAX-CUT and other 2-variable CSPs? SIAM J. Comput. 37(1), 319–357
(2007)
10. Matouˇsek, J.: Lectures on Discrete Geometry. Springer, New York (2002)
11. Onn, S., Schulman, L.J.: The vector partition problem for convex objective func-
tions. Math. Oper. Res. 26(3), 583–590 (2001)
12. Onn, S.: Personal communication, November 2016
13. Pyatkin, A.V.: On the complexity of the maximum sum length vectors subset
choice problem. J. Appl. Industr. Math. 4(4), 549–552 (2010)
14. Shenmaier, V.V.: Solving some vector subset problems by Voronoi diagrams. J.
Appl. Industr. Math. 10(4), 560–566 (2016)

Simple O(n log2 n) Algorithms
for the Planar 2-Center Problem
Xuehou Tan1,2(B) and Bo Jiang1
1 Dalian Maritime University, Linghai Road 1, Dalian, China
2 Tokai University, 4-1-1 Kitakaname, Hiratsuka 259-1292, Japan
tan@wing.ncc.u-tokai.ac.jp
Abstract. The planar 2-center problem for a set S of points in the plane
asks for two congruent circular disks of the minimum radius, whose union
covers all points of S. In this paper, we present a simple O(n log2 n) time
algorithm for the planar 2-center problem, improving upon the previously
known bound by a factor of (log log n)2. We ﬁrst describe an O(n log2 n)
time solution to a restricted 2-center problem in which the given points
are in convex position (i.e., they are the vertices of a convex polygon), and
then extend it to the case of arbitrarily given points. The novelty of our
algorithms is their simplicity: our algorithms use only binary search and
the known algorithms for computing the smallest enclosing disk of a point
set, avoiding the use of relatively complicated parametric search, which
is the base for most planar 2-center algorithms. Our work sheds more
light on the (open) problem of developing an O(n log n) time algorithm
for the planar 2-center problem.
Keywords: Computational geometry · Planar 2-center problem ·
The smallest enclosing circle problem · Convex polygons
1
Introduction
Let S denote a set of n points in the plane. The planar p-center problem asks
for p congruent closed disks of the minimum radius, whose union covers S.
The problem is NP-complete if p is part of input [10], and can be solved in
O(n2p−1 log n) time for any ﬁxed p [6]. On the other hand, eﬃcient algorithms
are known for small values of p. The 1-center problem, known as the smallest
enclosing disk problem, is widely studied and can be solved in O(n) time [3,4,9].
The planar 2-center problem has also been investigated extensively. After
several near-quadratic time algorithms were given, Sharir [11] was the ﬁrst to
present a near-linear algorithm with running time O(n log9 n), using the pow-
erful (but complicated) parametric search paradigm. Later, Eppstein [5] pre-
sented a randomized algorithm with O(n log2 n) expected time, and Chan [2]
The work by Tan was partially supported by JSPS KAKENHI Grant Number
15K00023, and the work by Jiang was partially supported by National Natural Sci-
ence Foundation of China under grant 61173034.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 481–491, 2017.
DOI: 10.1007/978-3-319-62389-4 40

482
X. Tan and B. Jiang
proposed an O(n log2 n) time randomized algorithm with high probability, and
an O(n(log n log log n)2) time deterministic algorithm. Both Eppstein and Chan
made some further reﬁnements of Sharir’s work. Whether an o(n log2 n) time
algorithm for the planar 2-center problem can be developed is left as an open
problem [2].
Kim and Shin [8] have also studied the 2-center problem for a set of points
in convex position. A set of points is said to be in convex position if it is the
set of vertices of a convex polygon. Let P be a convex polygon of n vertices.
We want to ﬁnd two congruent closed disks of the minimum radius, whose
union covers the whole polygon P, or only the vertices of P. Kim and Shin
[8] announced an O(n log3 n log log n) algorithm for covering the whole polygon
P and an O(n log2 n) time algorithm for covering all vertices of P. But, there is
an error in their analysis on the time complexity of their algorithm for covering
all vertices of P. A detailed discussion on it is provided in the appendix.
Suppose that r∗is the solution (optimum radius) for the planar 2-center
problem for a given point set S. The idea of parametric search is to use a data
structure for the decision problem (i.e., deciding whether S can be covered by two
disks of a given radius r) to test whether r∗is larger than or smaller than some
given value r. By making sure that the test for r = r∗actually happens, one can
eventually obtain the solution to the 2-center problem. In the known near-linear
algorithms for the planar 2-center problem, O(n) tests are usually used and each
test takes O(polylog n) time. Since parallel algorithms are usually used, the
parametric search paradigm is complicated and not easy to be implemented [5].
Note that the p-center problem has an important application in modern wire-
less communication systems. It is a natural requirement that some transmissions
should be set up so that mobile phones in a city (or region) can be serviced. Usu-
ally, the city can be represented by a simple or roughly convex polygon, and the
region covered by a transmission is represented by a closed disk with radius
r. The solution to the p-center problems helps give an optimal placement of
transmissions.
Our work. The general approach employed in most previous work for the pla-
nar 2-center problem is ﬁrst to solve the decision problem, and then apply an
optimization scheme, such as the matrix search, parametric search, or expander-
based technique [2,5,7,11]. Hence, the resulting algorithms are relatively com-
plicated.
In this paper, we present a simple approach to the following two restricted
2-center problems: One is to cover the set of vertices of a given convex polygon,
and the other is that the asked two congruent disks of the minimum radius are
assumed (or previously known) to have a non-empty overlap and a point in the
overlap is also given. Our new idea is to partition (or organize) the given point
set S into two subsets such that a sequence of locally optimal two disks for
either subset can be found; the union of two optimal disks covers S and the radii
of locally optimal disks in the sequence are monotonically decreasing and then
increasing. So, a binary search on either sequence of locally optimal solutions
can be used to ﬁnd the overall optimum solution.

Simple O(n log2 n) Algorithms for the Planar 2-Center Problem
483
We will develop two O(n log2 n) time algorithms for the restricted 2-center
problems described above. Our second result, together with a known result of
Eppstein [5], directly leads to an O(n log2 n) time solution to the planar 2-center
problem. This improves upon the previously known bound O(n(log n log log n)2)
[2]. The novelty of our algorithms is their simplicity: our algorithms use only
binary search and the known algorithms for computing the smallest enclosing
disk of a point set, avoiding the use of complicated parametric search, which is the
base for most planar 2-center algorithms. Although our improvement upon the
previous results is small, it sheds more light on the (open) problem of developing
an optimal O(n log n) time algorithm for the planar 2-center problem.
2
Covering a Set of Points in Convex Position
Let P be a convex polygon in the plane. In order to solve the 2-center problem for
the set of vertices of P, we make a substantial use of the smallest disk enclosing
P as well as the convexity of P. Let S denote the set of vertices of P. It is
well known that the smallest disk covering the point set S, which is determined
either by the diameter of S or by three points of S on its boundary, can be
found in O(n) time [9]. Denote by s1, s2 and s3 (if it exists), in clockwise order,
the points on the boundary of the smallest disk enclosing S. Without loss of
generality, assume that the x-coordinate of s1 is smaller than those of s2 and s3.
Denote by P1 the polygonal chain of P from s1 to s2 clockwise, and P2 the
chain of P from s3 (or s2 if point s3 does not exist) to s1 clockwise. For two
vertices x and y of P (P1, or P2), denote by P[x, y] (P1[x, y], or P2[x, y]) the
polygonal chain of P (P1, or P2) from x to y clockwise. Also, denote by P(x, y)
the open polygonal chain of P[x, y], in which x and y are excluded.
Suppose that D1 and D2 are two optimum disks for the 2-center problem for
the point set S. Clearly, three points s1, s2 and s3 cannot be contained in either
optimum disk. Following from the pigeon-hole principle, two of s1, s2 and s3 are
contained in one of the optimum disks.
Assume that D1 contains s1, and D2 contains s2 and s3 (if it exists). Since
P is convex, all the vertices of P[s2, s3] have to be contained in D2; otherwise,
either s2 or s3 is contained in D1, contradicting our assumption. (If point s3 does
not exist, then P[s2, s3] is a single point s2.) Suppose that the number of the
vertices of P1 is larger than or equal to that of P2. For ease of presentation, we
assume below that both P1 and P2 consist of exactly m vertices. (To this end,
one can simply add some “pseudo-vertices” to an edge of P2.) Note that the
vertices of the polygonal chain P(s2, s3) are excluded from those 2m vertices.
Let us number the points on P1 clockwise by 0, 1, . . ., m −1, and the
points on P2 by m, m + 1, . . . , 2m −1. (So, s1 and s2 are numbered as 0
and m −1 in P1, respectively. And, s1 and s3 are numbered as 2m −1
and m in P2, respectively.) For two points i and j, 1 ≤i ≤m −1 and
m+1 ≤j ≤2m−1, denote by D(j, i−1) the smallest disk enclosing the polygonal
chain P[j, i−1], and D(i, j−1) the smallest disk enclosing P[i, j−1]. Also, denote
by R(j, i −1) and R(i, j −1) the radii of D(j, i −1) and D(i, j −1), respectively.

484
X. Tan and B. Jiang
Let R∗(i, j) = max{R(j, i −1), R(i, j −1)}. We can consider R∗(i, j) as a (not
an optimum) solution to the 2-center problem for the vertices of P.
For a point i (1
≤
i
≤
m −1), let r∗
i
=
min{R∗(i, m + 1),
R∗(i, m + 2), . . . , R∗(i, 2m −1)}. We will refer r∗
i to as a locally optimal solution
to the 2-center problem for the vertices of P, because it is required in the solu-
tion r∗
i that two points i −1 and i be in diﬀerent disks. See Fig. 1. Two adjacent
solutions r∗
p and r∗
p+1 can be identical; in this case, two disks determining the
solution r∗
p, say, R(q, p −1) and R(p, q −1) (m + 1 ≤q ≤2m −1), are identical
to those determining r∗
p+1.
Fig. 1. The locally optimal radius r∗
i .
From the deﬁnition of radius R∗(i, j), the following observation can be made.
Observation 1. Suppose that r∗
i = R∗(i, q) holds, for some pair of points i and
q, 1 ≤i ≤m −1 and m + 1 ≤q ≤2m −1. Then, R∗(i, m + 1) ≥R∗(i, m + 2)
≥. . . ≥R∗(i, q −1) ≥R∗(i, q), and R∗(i, q) ≤R∗(i, q +1) ≤. . . ≤R∗(i, 2m−1).
Analogously, let r∗
j = min{R∗(1, j), R∗(2, j), . . . , R∗(m −1, j)}, m + 1 ≤j ≤
2m −1. Also, rj is a locally optimal solution to the 2-center problem for the
vertices of P.
Lemma 1. If R(j, i−1) > R(i, j −1) and R(j, i−2) < R(i−1, j −1) hold, then
r∗
j = min{R(j, i−1), R(i−1, j−1)}, where 2 ≤i ≤m−1 and m+1 ≤j ≤2m−1.
Proof. Since R(j, i −1) > R(i, j −1) holds, we have R(j, i) > R(i + 1,
j −1), R(j, i + 1) > R(i + 2, j −1), . . . , R(j, m −2) > R(m −1, j −1). This
is because R(j, i −1) ≤R(j, i) ≤. . . ≤R(j, m −2) and R(i, j −1) ≥
R(i+1, j −1) ≥. . . ≥R(m−1, j −1). Hence, R∗(i, j) = R(j, i−1), R∗(i+1, j) =
R(j, i) . . . , R∗(m −1, j) = R(j, m −2).

Simple O(n log2 n) Algorithms for the Planar 2-Center Problem
485
Also, since R(j, i −2) < R(i −1, j −1) holds, we have R(j, i −3) < R(i −
2, j −1), . . . , R(j, 0) < R(1, j −1). This is because R(j, i −2) ≥R(j, i −3)
≥. . . ≥R(j, 0) and R(i −1, j −1) ≤R(i −2, j −1) ≤. . . ≤R(1, j −1). Hence,
R∗(i−1, j) = R(i−1, j−1), R∗(i−2, j) = R(i−2, j−1) . . . , R∗(1, j) = R(1, j−1).
Finally, since R(j, i −1) ≤R(j, i) ≤. . . ≤R(j, m −2) and R(i −1, j −1) ≤
R(i−2, j−1) ≤. . . ≤R(1, j−1), we obtain r∗
j = min{R(j, i−1), R(i−1, j−1)}. ⊓⊔
Denote by r∗the optimum solution to the 2-center problem for the vertices
of P. Following from the convexity of P and our assumption that point s1 is
contained in D1 and two points s2 and s3 are in D2,
r∗=
min
1≤i≤m−1,m+1≤j≤2m−1 max{R(j −1, i), R(i −1, j)}.
Equivalently, we have r∗= min{r∗
1, r∗
2, . . . , r∗
m−1} = min{r∗
m+1, r∗
m+2, . . . ,
r∗
2m−1}. So, there are two points p and q, 1 ≤p ≤m−1 and m+1 ≤q ≤2m−1,
such that r∗= r∗
p = r∗
q.
Lemma 2. Suppose that r∗
i = R∗(i, j) holds for some pair of points i and j,
1 ≤i ≤m −1 and m + 1 ≤j ≤2m −1. If r∗
j = R∗(i, j) also holds, then r∗= r∗
i
(= r∗
j ).
Proof. Suppose that (i, j) is such a pair of points that r∗
i = R∗(i, j) = r∗
j .
Assume by contradiction that r∗< r∗
i . If R∗(i−, j) < R∗(i, j) or R∗(i+, j) <
R∗(i, j) holds for any 1 ≤i−≤i−1 or i+1 ≤i+ ≤m−1, then r∗
j is not locally
optimal, a contradiction. Also, if R∗(i, j−) < R∗(i, j) or R∗(i, j+) < R∗(i, j)
holds for any m + 1 ≤j−≤j −1 or j + 1 ≤j+ ≤2m −1, then r∗
i is not
locally optimal, a contradiction. In the case that none of R∗(i−, j) < R∗(i, j),
R∗(i+, j) < R∗(i, j), R∗(i, j−) < R∗(i, j) and R∗(i, j+) < R∗(i, j) holds, r∗
i (or
r∗
j ) is overall optimum, contradicting the assumption that r∗< r∗
i . The proof is
complete.
⊓⊔
Let us now introduce an important concept to locally optimal solutions. For
a point i (1 ≤i ≤m −1), the solution r∗
i is said to be adjustable on P1 if there
exists another solution r∗
i′, 1 ≤i′ ≤m −1, such that r∗
i′ < r∗
i .1
Lemma 3. Suppose that the solution r∗
i is adjustable on P1, and r∗
i = R∗(i, j)
holds for some point j ∈P2. If R(j, i −1) > R(i, j −1) then r∗
i−1 ≤r∗
i , 2 ≤i ≤
m −1. Also, if R(j, i −1) < R(i, j −1) then r∗
i+1 ≤r∗
i , 1 ≤i ≤m −2.
Proof. We ﬁrst claim that R(j, i −1) ̸= R(i, j −1). Assume that R(j, i −1) =
R(i, j −1). Then, r∗
j = R∗(i, j), because any solution R∗(i′, j), i′ ̸= i, has to
have a disk that contains the chain P[j, i −1] or P[i, j −1] and is thus of radius
at least R(j, i −1). It follows from Lemma 1 that r∗
i = r∗, a contradiction. Our
claim is proved.
1 The idea of deﬁning the adjustablity of locally optimal solutions originates from the
previous work on the well known watchman route problem in computational geometry
[12].

486
X. Tan and B. Jiang
By symmetry, we prove only that if R(j, i −1) > R(i, j −1) then r∗
i−1 ≤r∗
i ,
2 ≤i ≤m −1. First, we have R∗(i, j) = R(j, i −1) by the deﬁnition of R∗(i, j).
If R(i −1, j −1) > R(j, i −2) and R(i −1, j −1) > R(j, i −1) hold, then
from our assumption that R(j, i −1) > R(i, j −1), we have r∗
j = min{R(j, i −
1), R(i −1, j −1)} (Lemma 1) = R(j, i −1) = R∗(i, j). From the assumption
of the lemma that r∗
i = R∗(i, j), we have r∗= r∗
i (Lemma 2), a contradiction.
Hence, if R(i −1, j −1) > R(j, i −2) holds, then R(i −1, j −1) ≤R(j, i −1);
in this case, R∗(i −1, j) = R(i −1, j −1) ≤R(j, i −1) = R∗(i, j). Therefore,
r∗
i−1 ≤R∗(i −1, j) ≤R∗(i, j) = r∗
i . If R(i −1, j −1) < R(j, i −2) holds,
then R∗(i −1, j) = R(j, i −2) ≤R(j, i −1) = R∗(i, j). Again, we have r∗
i−1 ≤
R∗(i −1, j) ≤R∗(i, j) = r∗
i .
⊓⊔
From Lemma 3, we say the adjustable direction of r∗
i is leftwards on P1, if
R(j, i−1) > R(i, j −1). Also, if R(j, i−1) < R(i, j −1), the adjustable direction
of r∗
i is rightwards on P1. Thus, the adjustable direction of r∗
i on P1 is unique.
Lemma 4. A locally optimal solution r∗
i , 1 ≤i ≤m −1, gives r∗if and only if
it is not adjustable on P1.
Proof. Assume ﬁrst that r∗= r∗
i holds for some point i. From the optimality
of r∗, the solution r∗
i is not adjustable.
On the other hand, if r∗
i is not adjustable, then there are no other solutions,
which are strictly smaller than r∗
i . Thus, r∗
i = r∗holds.
⊓⊔
Theorem 1. Suppose that r∗= r∗
i holds for some point i, 1 ≤i ≤m−1. Then,
r∗
1 ≥r∗
2 ≥. . . ≥r∗
i and r∗
i ≤r∗
i+1 ≤. . . ≤r∗
m−1.
Proof. Assume ﬁrst that r∗= r∗
i holds, for some point i ≥2. Then, r∗
1 is
adjustable on P1 (Lemma 4). Since r∗
1 is the very ﬁrst locally optimal solution
on P1, the adjustable direction of r∗
1 (by the deﬁnition of adjustable solutions)
is rightwards. Thus, r∗
1 ≥r∗
2 (Lemma 3). So, if r∗
2 = r∗
i (probably, i > 2), we
obtain r∗
1 > r∗
2 = · · · = r∗
i .
Consider now the situation in which r∗
2 > r∗
i . Again, it follows from Lemma 4
that r∗
2 is adjustable. If the adjustable direction of r∗
2 is leftwards, then as the
adjustable direction of r∗
1 is rightwards, r∗
1 = r∗
2 (Lemma 3). This implies that
two disks determining r∗
1 are identical to those determining r∗
2, contradicting
the claim that their adjustable directions are diﬀerent. Hence, the adjustable
direction of r∗
2 has to be rightwards, and thus r∗
2 ≥r∗
3. In this way, we can
eventually obtain r∗
1 ≥r∗
2 ≥. . . ≥r∗
i .
Analogously, if i ≤m −2, we have r∗
i ≤r∗
i+1 ≤. . . ≤r∗
m−1.
⊓⊔
By a similar argument, we can also show that there exists a point j in P2, m+1 ≤
j ≤2m −1, such that r∗
m+1 ≥r∗
m+2 ≥. . . ≥r∗
j and r∗
j ≤r∗
j+1 ≤. . . ≤r∗
2m−1.
Theorem 2. Given a set S of n points in convex position, one can compute
in O(n log2 n) time the minimum radius r∗such that the union of two disks of
radius r∗covers S.

Simple O(n log2 n) Algorithms for the Planar 2-Center Problem
487
Proof. Let P be the convex polygon whose vertices coincide with the points of
S. First, we compute the smallest enclosing disk of S. Denote by s1, s2 and s3
the points of S on the boundary of the enclosing disk. (The special situation in
which only two points are on the boundary of the enclosing disk can be dealt
with analogously.)
Suppose that D1 and D2 are two optimum disks for the 2-center problem for
the set of the vertices of P, and r∗is the radius of them. Then, two of s1, s2
and s3 are contained in one optimum disk, and the third is in the other disk.
There are three such combinations of s1, s2 and s3. The algorithm described
below is performed for each of these combinations, and the smallest among their
outputted radii then gives the overall optimum radius r∗.
Suppose below that D1 contains s1 and D2 contains s2 and s3. Assume also
that the x-coordinate of s1 is smaller than those of s2 and s3. If the locally
optimal solution giving r∗is unique, it can simply be found by a binary search
on the sequence r∗
1, r∗
2, . . . r∗
m−1 (Theorem 1). In the case that there are multiple
optimal solutions r∗
k of the same value r∗, these points k are consecutive on the
polygonal chain P1 (Theorem 1). With a little more caution, r∗can also be found
by a binary search on the sequence r∗
1, r∗
2, . . . , r∗
m−1.
Finally, a locally optimal solution r∗
i (1 ≤i ≤m −1) can be found by
a binary search on the sequence R∗(i, m + 1), R∗(i, m + 2), . . . , R∗(i, 2m −1)
(Observation 1). Since the radius of the smallest disk enclosing a given point set
can be computed in linear time, r∗
i can be found in O(n log n) time. Therefore,
the optimum radius r∗can be computed in O(n log2 n) time.
⊓⊔
Remark. It is the ﬁrst time to make use of three points s1, s2 and s3 on the
smallest enclosing disk of a point set in a solution to the planar 2-center problem.
This helps to reduce the 2-center problem into at most three subproblems, which
have an important property as described by Theorem 1.
3
Extension to the set of arbitrarily given points
Let us brieﬂy review the known results on the 2-center problem for a set S of
points in the plane. Sharir [11] was the ﬁrst to present a near-linear algorithm
with running time O(n log9 n), using the powerful (but complicated) parametric
search paradigm. Subsequently, Eppstein [5] presented a randomized algorithm
with O(n log2 n) expected time. The best known deterministic algorithm with
O(n(log n log log n)2) running time was due to Chan [2], who reﬁned the deter-
ministic and randomized algorithms of Sharir and Eppstein.
We ﬁrst outline the basic strategy employed in these near-linear time algo-
rithms. Depending on the ratio of the distance between two centers of optimum
disks to r∗, two diﬀerent algorithms are designed: one is for the case that two
optimum disks are well separated, and the other for the case that they are close
to each other, or to be exact, they have a non-empty common area. To avoid the
work of determining which case the input data falls into, one can perform the
algorithms for both cases and then return or report the smaller of the outputs.
(The smaller output tells us which case actually happens.)

488
X. Tan and B. Jiang
Two optimum disks D1 and D2 are said to be well separated if the distance
between two centers of D1 and D2 is at least ϵr∗, for some constant ϵ > 0. In
this case, Eppstein [5] has given an O(n log2 n) time algorithm to compute r∗,
using a parametric search scheme.
Two disks D1 and D2 are said to be close to each other if the distance between
two centers of D1 and D2 is strictly smaller than (2 −ϵ)r∗, for some constant
ϵ > 0. In this case, one can generate a constant number of points such that at
least one of them belongs to D1 ∩D2. We don’t know which of these points is
the one in D1 ∩D2. Again, we can try all of them, as the number of these points
is a constant. The 2-center problem is then reduced to a constant number of the
restricted problems, in which a known point is assumed to be in D1 ∩D2.
From the discussion made above, what we need to do is to solve the following
restricted 2-center problem: for a given point set S and a given point t, ﬁnd two
congruent disks D1 and D2 of the minimum radius such that S ⊆D1 ∪D2 and
t ∈D1∩D2 [2,5]. (It is not necessary for t to be a point of S.) Again, assume that
three points s1, s2 and s3 in clockwise order are on the smallest enclosing disk
of S, and the x-coordinate of s1 is smaller than those of s2 and s3. Assume also
that D1 contains s1 and D2 contains s2 and s3 (if it exists). We ﬁrst compute (in
O(n log n) time) the convex hull of S. Still, denote by P[s2, s3] the convex chain
between s2 and s3, which does not contain s1, and P(s2, s3) the open chain of
P[s2, s3].
Denote by E1 the smallest disk that encloses s1 and t, and E2 the smallest
disk that encloses the vertices of P[s2, s3] and t. Denote by S(E1) (S(E2)) the
set of the points contained in E1 (E2). All the points of S(E1) (S(E2)), by its
deﬁnition, are contained in D1 (D2), and can thus be omitted in the following
discussion.
Let γ1 and γ2 be the rays that emanate from t and pass through s1 and s2,
respectively. Two rays γ1 and γ2 together partition the points of S−S(E1)∪S(E2)
into two families. Let P1 (P2) denote the family of the points of S−S(E1)∪S(E2),
which are vertically above (below) either γ1 or γ2. Notice that S −S(E1)∪S(E2)
does not contain any point that is vertically below γ2 and above the ray, which
emanates from t and passes through s3.
For ease of presentation, assume that s1 and s2 (s3) also belong to P1 (P2),
and both P1 and P2 have k points. Let us sort the points of P1 (P2) radially
around point t in clockwise order. That is, the points of P1 (P2) are encountered
in order if one rotates γ1 (γ2) around t clockwise. We number the sorted points
of P1 by 0, 1, . . . , k −1, and the sorted points of P2 by k, k + 1, . . . , 2k −1. For
the example given in Fig. 2, the points of P1 and P2 are numbered as 0, 1, . . .,
13 (point 13 is identical to point 0). Again, s1 and s2 are numbered as 0 and
k −1 in P1 respectively, and s1 and s3 are numbered as 2k −1 and k in P2
respectively.
Since we have assumed that both optimum disks D1 and D2 (of the same
radius r∗) contain t, the boundaries of D1 and D2 have two intersection points;
one is vertically above γ1 or γ2, and the other is below γ1 or γ2. Denote by γ3
and γ4 two rays, which emanate from t and pass through these points, see Fig. 2.

Simple O(n log2 n) Algorithms for the Planar 2-Center Problem
489
γ2
γ3
γ4
γ1
s1
s2
s3
E1
E2
D1
D2
1
2
3
4
0
6
t
7
8
9
10
11
5
12
Fig. 2. Illustration for the sorted points of P1 and P2.
The following observation directly follows from the deﬁnitions of γ3, γ4, and the
sorted points of P1 and P2.
Observation 2. γ3 (γ4) partitions the sorted points of P1 (P2) into two subsets:
one has all its points in D1 and the other has its points in D2.
We use the same notation as described in Sect. 2. From Observation 2,
r∗=
min
1≤i≤k−1,k+1≤j≤2k−1 max{R(j −1, i), R(i −1, j)}.
Thus, the argument given in Sect. 2 can directly be applied to the point sequences
P1 and P2. Let r∗
i = min{R∗(i, k+1), R∗(i, k+2), . . . , R∗(i, 2k−1)}, 1 ≤i ≤k−1.
Again, r∗
i is the locally optimal solution to the 2-center problem for the points
of S, under the requirement that two points i −1 and i be in diﬀerent disks.
Hence, we have the following result.
Theorem 3. Suppose that r∗= r∗
i holds for some point i of P1, 1 ≤i ≤k −1.
Then, r∗
1 ≥r∗
2 ≥. . . ≥r∗
i and r∗
i ≤r∗
i+1 ≤. . . ≤r∗
k−1.
The method described in Sect. 2 can then be used to compute r∗. What is
new in computing a locally optimal solution is that the points in P1 (P2) are now
not in convex position. Since the smallest enclosing disk of a point set can be
computed in linear time, it does not aﬀect the time complexity of our algorithm
at all. Still, the optimum radius r∗can be computed in O(n log2 n) time.
Combining with Eppstein’s result for the well separated case [5], we obtain
the main result of this paper.
Theorem 4. For a set S of n given points, we can compute in O(n log2 n) time
the minimum radius r∗such that the union of two disks of radius r∗covers S.

490
X. Tan and B. Jiang
4
Conclusions
In this paper, we have presented a simple O(n log2 n) time algorithm for the
planar 2-center problem, improving upon the previously known bound by a factor
of (log log n)2. The novelty of our presented algorithms is their simplicity: our
algorithms use only binary search and the known algorithms for computing the
smallest enclosing disk of a point set, avoiding the use of relatively complicated
parametric search, which is the base for most planar 2-center algorithms.
We pose several open problems for further research. First, our algorithm
for the planar 2-center problem makes use of the algorithm of Eppstein for the
ﬁrst case, in which two optimum disks are well separated [5]. We would like
to ask for an O(n log2 n) time solution to the 2-center problem without using
parametric search, even for the well separated case. It has been known that the
lower bound on the planar 2-center problem is Ω(n log n) [5]. Whether an optimal
O(n log n) time algorithm can be developed is a challenging open problem. From
the simplicity of our presented algorithms, we believe that the method employed
in this work can be used to develop an O(n log n) time algorithm for the planar
2-center problem.
Finally, Agarwal et al. have also considered the discrete 2-center problem,
where the centers of two disks are restricted to be at points of S, and given an
O(n4/3 log5 n) time algorithm [1]. Whether or not a near-linear time algorithm
can be developed is an interesting open problem.
Appendix: Note on the Kim and Shin algorithm for
covering a set of points in convex position
In this appendix, we point out a mistake that occurs in the previously announced
O(n log2 n) time algorithm, by Kim and Shin, for covering a set A of points in
convex position [8].
We use the same notation as given in [8]. For any i, j ∈A, let r1
i,j (r2
i,j) be the
radius of the smallest disk containing ⟨i, j −1⟩(⟨j, i −1⟩). Then, the optimum
solution r∗= mini,j∈A max{r1
i,j, r2
i,j}.
Let r∗
0 = minj∈A max{r1
0,j, r2
0,j}, and let k be the point such that r∗
0 =
max{r1
0,k, r2
0,k}. Then, it was stated in Lemma 7 of [8] that “For any i, j such
that i, j ∈⟨0, k −1⟩or i, j ∈⟨k, n −1⟩, max{r1
i,j, r2
i,j} > r∗
0”.
By Lemma 7 of [8], all the pairs of i, j ∈⟨0, k −1⟩or i, j ∈⟨k, n −1⟩needn’t
be considered. Kim and Shin then described a divide-and-conquer procedure to
compute all the pairs of i and j such that i ∈⟨0, k −1⟩and j ∈⟨k, n −1⟩, in
the paragraph immediately following Lemma 7 of [8]. Note that the number of
the pairs of i and j, which are found by their divide-and-conquer procedure, is
O(n) (other than O(log n)). To compute r∗, these O(n) pairs of i and j have
further to be handled (e.g., using parametric search to compute the smallest
enclosing disks for them). However, it is NOT considered in [8]. Therefore, the
O(n log2 n) time solution to the planar 2-center problem for a set of points in
convex position was not obtained in [8].

Simple O(n log2 n) Algorithms for the Planar 2-Center Problem
491
References
1. Agarwal, P.K., Sharir, M., Welzl, E.: The discrete 2-center problem. Discret. Com-
put. Geom. 20, 287–305 (1998)
2. Chan, T.M.: More planar two-center algorithms. Comput. Geom. Theor. Appl. 13,
189–198 (1999)
3. Chazelle, B., Matouˇsek, J.: On linear-time deterministic algorithms for optimiza-
tion problems in ﬁxed dimension. J. Algorithms 21, 579–597 (1996)
4. Dyer, M.E.: On a multidimensional search technique and its application to the
Euclidean one-center problem. SIAM J. Comput. 15, 725–738 (1986)
5. Eppstein, D.: Faster construction of planar two-centers. In: Proceedings of 8th
ACM-SIAM Symposium on Discrete Algorithms, pp. 131–138 (1997)
6. Hwang, R.Z., Lee, R.C.T., Chang, R.C.: The slab dividing approach to the euclid-
ean P-center problem. Algorithmica 9, 1–22 (1993)
7. Katz, M.J., Sharir, M.: An expander-based approach to geometric optimization.
SIAM J. Comput. 26, 1384–1408 (1997)
8. Kim, S.K., Shin, C.-S.: Eﬃcient algorithms for two-center problems for a convex
polygon. In: Du, D.-Z.-Z., Eades, P., Estivill-Castro, V., Lin, X., Sharma, A. (eds.)
COCOON 2000. LNCS, vol. 1858, pp. 299–309. Springer, Heidelberg (2000). doi:10.
1007/3-540-44968-X 30
9. Megiddo, N.: Linear time algorithms for linear programming in R3 and related
problems. SIAM J. Comput. 12, 759–776 (1983)
10. Megiddo, N., Supowit, K.: On the complexity of some common geometric location
problems. SIAM J. Comput. 13, 1182–1196 (1984)
11. Sharir, M.: A near-linear time algorithm for the planar 2-center problem. Discret.
Comput. Geom. 18, 125–134 (1997)
12. Tan, X.: Fast computation of shortest watchman routes in simple polygons. Inform.
Process. Lett. 87, 27–33 (2001)

Stable Matchings in Trees
Satoshi Tayu(B) and Shuichi Ueno
Department of Information and Communications Engineering,
Tokyo Institute of Technology, S3-57, Tokyo 152-8550, Japan
tayu@eda.ict.e.titech.ac.jp
Abstract. The maximum stable matching problem (Max-SMP) and the
minimum stable matching problem (Min-SMP) have been known to be
NP-hard for subcubic bipartite graphs, while Max-SMP can be solved in
polynomal time for a bipartite graph G with a bipartition (X, Y ) such
that degG(v) ≤2 for any v ∈X. This paper shows that both Max-SMP
and Min-SMP can be solved in linear time for trees. This is the ﬁrst
polynomially solvable case for Min-SMP, as far as the authors know. We
also consider some extensions to the case when G is a general/bipartite
graph with edge weights.
1
Introduction
Let G be a simple bipartite graph (bigraph) with vertex set V (G) and edge
set E(G). For each vertex v ∈V (G), let IG(v) be the set of all edges incident
with v, and degG(v) = |IG(v)|. For each v ∈V (G), ⪯v is a total preorder (a
binary relation with transitivity, totality, and hence reﬂexivity) on I(v), and
⪯G= {⪯v| v ∈V (G)}. A total preorder ⪯v is said to be strict if e ⪯v f and
e ̸= f imply f ̸⪯v e. We say that ⪯G is strict if ⪯v is strict for every v ∈V (G). It
should be noted that a strict total preorder is just a linear order. A pair (G, ⪯G)
is called a preference system. A preference system (G, ⪯G) is said to be strict if
⪯G is strict. We say that an edge e dominates f at vertex v if e ⪯v f. A matching
M of G is said to be stable if each edge of G is dominated by some edge in M.
The stable matching problem (SMP) is to ﬁnd a stable matching of a preference
system (G, ⪯G). It is well-known that any preference system (G, ⪯G) has a sta-
ble matching, and SMP can be solved in linear time by using the Gale/Shapley
algorithm [3]. It is also well-known that every stable matching for a strict pref-
erence system has the same size and spans the same set of vertices, while a
general preference system can have stable matchings of diﬀerent sizes [3]. This
leads us to the following two problems. The maximum stable matching problem
(Max-SMP) is to ﬁnd a stable matching with the maximum cardinality, and the
minimum stable matching problem (Min-SMP) is to ﬁnd a stable matching with
the minimum cardinality. Manlove, Irving, Iwama, Miyazaki, and Morita showed
that Max-SMP and Min-SMP are both NP-hard [9].
Let (X, Y ) be a bipartition of a bigraph G. A bigraph G is called a (p, q)-graph
if degx(≤)p for every x ∈X, and degy(≤)q for every y ∈Y . Irving, Manlove,
and O’Malley showed that Max-SMP is NP-hard even for (3, 3)-graphs, while
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 492–503, 2017.
DOI: 10.1007/978-3-319-62389-4 41

Stable Matchings in Trees
493
Max-SMP can be solved in polynomial time for (2, ∞)-graphs [7]. Some indepth
consideration on the approximation for both problems can be found in [5].
The purpose of the paper is to show that Max-SMP and Min-SMP can be
solved in linear time if G is a tree. This is the ﬁrst polynomially solvable case
for Min-SMP, as far as the authors know. We also consider some extensions to
the case when G is a general/bipartite graph with edge weights.
The rest of the paper is organized as follows. Section 2 gives a foundation for
our algorithms. Section 3.1 shows a linear time algorithm based on a dynamic
programming to compute the size of a maximum stable matching in a tree.
Section 3.2 shows a linear time algorithm, a modiﬁcation of the algorithm in
Sect. 3.1, to compute a maximum stable matching in a tree. Section 3.3 mentions
an extension of the algorithm in Sect. 3.2 to compute a maximum-weight stable
matching in linear time for trees with edge weights. Section 4.1 mentions that
minimum stable matchings can be computed in linear time for trees (with edge
weights) by modifying algorithms in Sect. 3. Section 4.2 mentions some exten-
sions of our results to the case when G is a general graph with edge weights.
2
Stable Matchings in Trees
We need preliminaries to describe our algorithms.
Let T be a tree, and (T, ⪯T ) be a preference system, which is called a tree
preference system. A stable matching of (T, ⪯T ) is called a stable matching of
T, for simplicity. We use the following notations:
– we write u ⪯v w (or w ⪰v u) if (v, u) ⪯v (v, w),
– we write u ≡v w if (v, u) ⪯v (v, w) and (v, w) ⪯v (v, u),
– we write u ≺v w (or w ≻v u) if u ⪯v w and u ̸≡v w.
It should be noted that if ⪯v is strict, then u ≡v w if and only if u = w. It
should be also noted that ≡v is an equivalence relation on IG(v).
We consider T as a rooted tree with the root r, which is a leaf (a vertex
of degree one) of T. For each vertex v ∈V (T) −{r}, p(v) is the parent of v,
and D(v) is the set of descendants of v. For any v ∈V (T) −{r}, we denote by
T(v) the subtree induced by D(v) ∪{p(v)}. A matching M of T(v) is said to be
v-stable if every edge of E(T(v)) −{(v, p(v))} is dominated by some edge in M.
A vertex v is said to be matched with u in M if (u, v) ∈M.
We deﬁne ﬁve sets of v-stable matchings of T(v) as follows.
– MP
v is the set of v-stable matchings of T(v) in which v is matched with p(v).
– MH
v is the set of v-stable matchings of T(v) in which v is matched with a
child c such that c ⪯v p(v).
– ML
v is the set of v-stable matchings of T(v) in which v is matched with a
child c such that c ≻v p(v).
– MF
v is the set of v-stable matchings of T(v) in which v is matched with no
other vertices of T(v).
– MP
v is the set of v-stable matchings of T(v) in which v is not matched with
p(v).

494
S. Tayu and S. Ueno
If v(̸= r) is a leaf, T(v) is a tree with E(T(v)) = {(v, p(v))}, and we have
E(T(v)) −{(v, p(v))} = ∅. Thus, we have the following.
Lemma 1. If v(̸= r) is a leaf, then MP
v = {{(v, p(v))}}, MH
v = ML
v = ∅, and
MP
v = MF
v = ∅.
⊓⊔
It should be noted that for any v ∈V (T) −{r}, MP
v = MH
v ∪ML
v ∪MF
v ,
MP
v ∩MP
v = ∅, and every v-stable matching of T(v) is in MP
v ∪MP
v .
Let r′ be the child of r. Since r′ is the only child of r, we obtain the following.
Lemma 2. A set M ⊆E(T) is a stable matching of T if and only if M ∈
MP
r′ ∪MH
r′.
Proof. Suppose M is a stable matching of T = T(r′). Since M is an r′-stable
matching, M ∈MS
r′ for some S ∈{P, H, L, F}. If (r′, r) ∈M then M ∈MP
r′.
If (r′, r) ̸∈M then (r′, r) must be dominated by an edge in M, and thus, there
is a child c of r′ such that (r′, c) ∈M and c ⪯r′ r, which means that M ∈MH
r′.
Therefore, we conclude that M ∈MP
r′ ∪MH
r′.
Conversely, suppose M ∈MP
r′ ∪MH
r′. Since M is an r′-stable matching of
T = T(r′), every edge in E(T(v)) −{(r′, r)} is dominated by an edge in M.
If M ∈MP
r′ then (r′, r) ∈M. If M ∈MH
r′, then there exists a child c of r′
such that (r′, c) ∈M and c ⪯r′ r, which means that (r′, r) is dominated by
(r′, c) ∈M. Thus, we conclude that M is a stable matching of T, and we have
the lemma.
⊓⊔
For a vertex v ∈V (T), let C(v) be the set of children of v. For a set M ⊆E(T)
and v ∈V (T) −{r}, we deﬁne M(v) = E(T(v)) ∩M.
Lemma 3. If M is a v-stable matching of T(v) then M(c) is a c-stable matching
of T(c) for any c ∈C(v).
Proof. Since M is a matching of T(v), M(c) is a matching of T(c). Since M is a
v-stable matching of T(v), every edge in E(T(c)) −{(c, v)} is dominated by an
edge in M(c). Thus, M(c) is a c-stable matching of T(c).
⊓⊔
Lemma 4. For any v ∈V (T) −{r} and set M ⊆E(T(v)), M ∈MP
v if and
only if the following conditions are satisﬁed:
(i) (v, p(v)) ∈M,
(ii) M(c) ∈MP
c for every c ∈C(v), and
(iii) M(c) ∈MH
c for every c ∈C(v) with c ≺v p(v).
Proof. Suppose M ∈MP
v . Then, (i) follows from the deﬁnition of MP
v . Since
M is a v-stable matching of T(v), M(c) is a c-stable matching of T(c) for every
c ∈C(v) by Lemma 3. Since v is matched with p(v), (v, c) ̸∈M for any c ∈C(v),
that is, M(c) ∈MP
c . Thus, we have (ii). For any c ∈C(v) with c ≺v p(v),
(v, c) is not dominated by (v, p(v)). Thus, there exists g ∈C(c) such that (c, g)
dominates (c, v) = (v, c). Since g ⪯c v, M(c) ∈MH
c , and we have (iii).

Stable Matchings in Trees
495
Conversely, suppose a set M ⊆E(T(v)) satisﬁes (i), (ii), and (iii). For any
c ∈C(v), M(c) is a matching such that v is matched with no other vertex by (ii).
Thus, M is a matching of T(v). For any c ∈C(v), each edge of E(T(c))−{(v, c)}
is dominated by an edge in M, since M(c) is a c-stable matching by (i), (ii), and
(iii). For any c ∈C(v), if c ⪰v p(v) then (v, c) is dominated by (v, p(v)), which is
in M by (i). If c ≺v p(v) then (v, c) is dominated by an edge in M by (iii). Thus,
M is a v-stable matching of T(v), and we conclude that M ∈MP
v by (i).
⊓⊔
Lemma 5. For any v ∈V (T) −{r} and set M ⊆E(T(v)), M ∈MH
v if and
only if the following conditions are satisﬁed:
(i) (v, p(v)) ̸∈M, and
(ii) there exists c′ ∈C(v) such that the following conditions are satisied:
(ii-1) c′ ⪯v p(v) ,
(ii-2) M(c′) ∈MP
c′,
(ii-3) M(c) ∈MP
c for every c ∈C(v) −{c′}, and
(ii-4) M(c) ∈MH
c for every c ∈C(v) with c ≺v c′.
Proof. Suppose M ∈MH
v . Then, (i) and (ii-1) follow from the deﬁnition of MH
v .
Since M is a v-stable matching of T(v), M(c) is a c-stable matching of T(c) for
every c ∈C(v) by Lemma 3. Since M ∈MH
v , there exists c′ ∈C(v) such that
(v, c′) ∈M and c′ ⪯v p(v), that is, M(c′) ∈MP
c′. Thus, we have (ii-2). Since
M is a matching and (v, c′) ∈M, (v, c) ̸∈M(c) for every c ∈C(v) −{c′}. Thus,
M(c) ∈MP
c for every c ∈C(v) −{c′}, and we have (ii-3). For any c ∈C(v) with
c ≺v c′, (v, c) is not dominated by (v, c′). Therefore, there exists g ∈C(c) such
that (c, g) dominates (c, v). Since g ⪯c v, M(c) ∈MH
c , and we have (ii-4).
Conversely, suppose a set M ⊆E(T(v)) satisﬁes (i) and (ii). For any c ∈
C(v) −{c′}, M(c) is a matching such that v is matched with no other vertex by
(ii-3). Also, M(c′) is a matching by (ii-2). Thus M is a matching of T(v). For
any c ∈C(v) −{c′}, each edge in E(T(c)) −{(v, c)} is dominated by an edge
in M, since M(c) is a c-stable matching by (ii-3). For any c ∈C(v), if c ⪰v c′
then (v, c) is dominated by (v, c′), which is in M by (ii-2). If c ≺v c′ then (v, c)
is dominated by an edge in M by (ii-4). Thus, M is a v-stable matching of T(v)
by (i) and (ii), and we conclude that M ∈MH
v by (ii-1) and (ii-2).
⊓⊔
Lemma 6. For any v ∈V (T) −{r} and set M ⊆E(T(v)), M ∈ML
v if and
only if the following conditions are satisﬁed:
(i) (v, p(v)) ̸∈M, and
(ii) there exists c′ ∈C(v) such that the following conditions are satisﬁed:
(ii-1) c′ ≻v p(v),
(ii-2) M(c′) ∈ML
c′,
(ii-3) M(c) ∈MP
c for every c ∈C(v) −{c′}, and
(ii-4) M(c) ∈MH
c for every c ∈C(v) with c ≺v c′.

496
S. Tayu and S. Ueno
Proof. Suppose M ∈ML
v . Then, (i) and (ii-1) follow from the deﬁnition of ML
v .
Since M is a v-stable matching of T(v), M(c) is a c-stable matching of T(c) for
every c ∈C(v) by Lemma 3. Since M ∈ML
v , there exists c′ ∈C(v) such that
(v, c′) ∈M and c′ ≻v p(v), that is, M(c′) ∈ML
c′. Thus, we have (ii-2). Since
M is a matching and (v, c′) ∈M, (v, c) ̸∈M(c) for every c ∈C(v) −{c′}. Thus,
M(c) ∈MP
c for every c ∈C(v) −{c′}, and we have (ii-3). For any c ∈C(v) with
c ≺v c′, (v, c) is not dominated by (v, c′). Therefore, there exists g ∈C(c) such
that (c, g) dominates (c, v). Since g ⪯c v, M(c) ∈MH
c , and we have (ii-4).
Conversely, suppose a set M ⊆E(T(v)) satisﬁes (i) and (ii). For any c ∈
C(v) −{c′}, M(c) is a matching such that v is matched with no other vertex by
(ii-3). Also, M(c′) is a matching by (ii-2). Thus M is a matching of T(v). For
any c ∈C(v) −{c′}, each edge in E(T(c)) −{(v, c)} is dominated by an edge
in M, since M(c) is a c-stable matching by (ii-3). For any c ∈C(v), if c ⪰v c′
then (v, c) is dominated by (v, c′), which is in M by (ii-2). If c ≺v c′ then (v, c)
is dominated by an edge in M by (ii-4). Thus, M is a v-stable matching of T(v)
by (i) and (ii), and we conclude that M ∈ML
v by (ii-1) and (ii-2).
⊓⊔
Lemma 7. For any v ∈V (T) −{r} and set M ⊆E(T(v)), M ∈MF
v if and
only if the following conditions are satisﬁed:
(i) (v, p(v)) ̸∈M , and
(ii) M(c) ∈MH
c for any c ∈C(v).
Proof. Suppose M ∈MF
v . Then, (i) follows from the deﬁnition of MF
v . Since
M is a v-stable matching of T(v), M(c) is a c-stable matching of T(c) for every
c ∈C(v) by Lemma 3. Since M ∈MF
v , for any c ∈C(v), there exists g ∈C(c)
such that (c, g) dominates (c, v). Since g ⪯c v, M(c) ∈MH
c for any c ∈C(v),
and we have (ii).
Conversely, suppose a set M ⊆E(T(v)) satisﬁes (i) and (ii). For any c ∈C(v),
M(c) is a matching such that v is matched with no other vertex by (ii). Thus,
M is a matching of T(v). For any c ∈C(v), each edge in E(T(c)) −{(c, v)} is
dominated by an edge in M, since M(c) is a c-stable matching by (ii). For any
c ∈C(v), (c, v) is dominated by an edge in M by (ii). Thus, M is a v-stable
matching of T(v), and we conclude that M ∈MF
v by (i).
⊓⊔
3
Linear Time Algorithms for Trees
3.1
Computing the Size of Maximum Stable Matchings
Now, we are ready to show a linear time algorithm to compute the size of a
maximum stable matching for a tree preference system. Our algorithm applies
a dynamic programming scheme based on the results in the previous section.
Let (T, ⪯T ) be a tree preference system. We consider T as a rooted tree with
root r, which is a leaf of T. For any v ∈V (T) −{r} and S ∈

P, H, L, F, P

, we
deﬁne that
μS
v = max
M∈MS
v
|M|.
(1)

Stable Matchings in Trees
497
That is, μS
v is the maximum number of edges of a v-stable matching in MS
v . We
deﬁne that μS
v = −∞if MS
v = ∅.
From Lemma 1, we have the following.
Lemma 8. If v(̸= r) is a leaf of T,
μH
v = μL
v = −∞,
(2)
μP
v = 1, and
(3)
μF
v = 0.
⊓⊔
Deﬁne that for any v ∈V (T) −{r} and c ∈C(v),
μv,c = μP
c +

c∈C(v), c′≺vc
μH
c′ +

c∈C(v), c′⪰vc, c′̸=c
μP
c′.
(4)
From Lemmas 4 –7, we have the following.
Lemma 9. For any v ∈V (T) −{r},
μP
v =

c∈C(v), c≺vp(v)
μH
c +

c∈C(v), c⪰vp(v)
μP
c + 1,
(5)
μH
v = max{μv,c | c ∈C(v), c ⪯v p(v)},
(6)
μL
v = max{μv,c | c ∈C(v), c ≻v p(v)},
(7)
μF
v =

c∈C(v)
μH
c , and
(8)
μP
v = max{μH
v , μL
v , μF
v }.
(9)
⊓⊔
From Lemmas 8 and 9, we have the following.
Lemma 10. Procedure Comp μ(v) shown in Fig. 1 computes μS
v for all v ∈
V (T) −{r} and S ∈{P, H, L, F, P}.
⊓⊔
Now, we are ready to show the following.
Theorem 1. Algorithm Max-SIZE(T, ⪯T , r) shown in Fig. 2 solves Max-SMP
for a tree preference system (T, ⪯T ) in linear time.
Proof. The validity of the algorithm follows from Lemmas 2 and 10. We shall
show that the time complexity of the algorithm is O(n), where n = |V (T)|.
Lemma 11. Given μS
c for every c ∈C(v) and S ∈

P, H, L, F, P

, {μv,c | c ∈
C(v)} can be computed in O(|C(v)|) time,

498
S. Tayu and S. Ueno
Fig. 1. Procedure Comp μ(v).
Proof of Lemma 11. If v is a leaf, {μv,c | c ∈C(v)} can be computed in O(1)
time, by deﬁnition. Let v ∈V (T) be a vertex of degree at least 2, δ = |C(v)|,
and c1, c2, . . . , cδ be a sequence of the children of v such that ci ⪯v ci+1 for any
i ∈[δ −1], where c1, c2, . . . , cδ are sorted in the problem instance. Deﬁne that
for any v ∈V (T) −{r} and c ∈C(v),
[c]v = {c′ | c′ ≡v c},
which is an equivalence class for an equivalence relation ≡v on IG(v). Then, C(v)
is partitioned into equivalence classes for ≡v. Let x ≥2. If cx ≡v cx−1 then
μv,cx = μv,cx−1 −(μP
cx−1 + μP
cx) + (μP
cx−1 + μP
cx).
Thus, by (4), μv,cx can be computed in O(1) time by using μv,cx−1. If cx−1 ≺v cx,
let y be the integer satisfying
[cx−1]v = {cy, cy+1, . . . , cx−1}.
Since cx−1 ̸≡v cx, cx−1 and cx are contained in diﬀerent equivalnce classes
for ≡v. Thus from (4), we have
μv,cx = μv,cx−1 −
⎛
⎝
x−2

i=y
μP
ci + μP
cx−1 + μP
cx
⎞
⎠+
⎛
⎝
x−1

i=y
μH
ci + μP
cx
⎞
⎠.
(10)

Stable Matchings in Trees
499
Therefore, μv,cx can be computed in O(|[cx−1]v|) time by using μv,cx−1.
Thus, {μv,c | c ∈C(v)} can be computed in O(|C(v)|) time, since the compu-
tation shown in (10) is executed once for each x with cx ≻v cx−1.
Once μv,c are obtained for every c ∈C(v),

μS
v | S ∈{H, L, P, F, P}

can be computed in O(|C(v)|)
=
O(deg Tv) time, by Eqs. (5)–(9). From
Lemma 10, except for the recursive calls, Comp μ(v) for each vertex v can be
done in O(deg Tv) time. Moreover, in the execution of Max-SIZE(T, ⪯T , r)
shown in Fig. 2, Comp μ(v) is called once for every v ∈V (T), and thus,
Max-SIZE(T, ⪯T , r) can be executed in 	
v∈V (T ) deg Tv = O(|V (T)|) time.
Since Max-SIZE(T, ⪯T , r) computes max{μH
r′, μP
r′}, we have the theorem, by
Lemma 2.
⊓⊔
Fig. 2. Algorithm Max-SIZE(T, ⪯T , r).
3.2
Computing Maximum Stable Matchings
Before describing an algorithm for computing a maximum stable matching, we
modify Comp μ(v) to store an edge (u, v) in a matching with μS
v edges for any
S ∈{H, P} when μS
v is computed. We use two variables γ(H, v) and γ(P, v) to
store a child c of v. γ(H, v) stores a child c with (c, v) ∈M for any M ∈MH
v
corresponding to μH
v . γ(P, v) stores a child c with (c, v) ∈M for any M ∈MH
v
corresponding to μP
v .
Figure 3 shows a recursive procedure Comp γ(v), which is obtained from
Comp μ(v) shown in Fig. 1 by adding some instructions for γ(S, v).
Lemma 12. For any S ∈{H, P} and v ∈V (T) −{r}, γ(S, v) stores the edge
of M incident with v such that M ∈MS
v with |M| = μS
v , if any.
⊓⊔
We show an algorithm for computing a maximum stable matching of a tree
preference system (T, ⪯T ) in Fig. 4, where Procedure Add Edges(v, S, M) is
shown in Fig. 5.
Procedure Add Edges(v, S, M) traverses vertices of T in DFS order, and
we have the following.
Lemma 13. For any S ∈{P, H, P} and M ⊆E(T) with M ∩E(T(v)) = ∅,
Add Edges(v, S, M) adds edges in M ′ to M for some M ′ ∈MS
v satisfying
|M ′| = μS
v .
⊓⊔

500
S. Tayu and S. Ueno
Fig. 3. Procedure Comp γ(S, v).
From Lemmas 12 and 13, we have the following.
Theorem 2. Algorithm Max-SMP(T, ⪯T , r) solves Max-SMP in linear time
for a tree preference system (T, ⪯T ).
⊓⊔
3.3
Computing Maximum-Weight Stable Matchings
A weighted preference system (G, ⪯G, w) is a preference system (G, ⪯G) with a
weight function w : E(G) →Z+. For a matching M of G, w(M) = 	
e∈M w(e) is
a weight of M. The maximum-weight stable matching problem (Max-WSMP) is
to ﬁnd a stable matching with maximum weight for a weighted preference system.
It is easy to see that we can solve Max-WSMP for weighted tree preference
systems by the algorithm in Sect. 3.2 with a slight modiﬁcation of μS
v . For any

Stable Matchings in Trees
501
Fig. 4. Algorithm Max-SMP(T, ⪯T , r).
S ∈{H, L, P, F, P}, we deﬁne that
μS
v = max
M∈MS
v
w(M)
(11)
instead of (1). We also replace (3) and (5) with
μP
v = w(v, p(v)) and
(12)
μP
v =

c∈C(v), c≺vp(v)
μH
c +

c∈C(v), c⪰vp(v)
μP
c + w(v, p(v)),
(13)
respectively, and let Max-WSMP(T, ⪯T , r, w) be the algorithm obtained by the
modiﬁcations above. Thus, we have the following.
Theorem 3. Algorithm Max-WSMP(T, ⪯T , r, w) solves Max-WSMP in linear
time for a weighted tree preference system (T, ⪯T , w).
⊓⊔
4
Concluding Remarks
4.1
Min-SMP and Min-WSMP
The minimum-weight stable matching problem (Min-WSMP) is to ﬁnd a stable
matching with minimum weight for a weighted preference system. We can com-
pute minimum stable matchings in a similar way. We deﬁne μS
v = +∞if MS
v = ∅.
Let Min-SMP(T, ⪯T , r) be the algorithm obtained from Max-SMP(T, ⪯T , r)
by replacing (2), (6), (7), and (9) with
μH
v = μL
v = +∞
μH
v = min{μv,c | c ∈C(v), c ⪯v p(v), c′ ̸= p(v)},
μL
v = min{c ∈C(v), μv,c | c ≻v p(v)}, and
μP
v = min{μH
v , μL
v , μF
v },

502
S. Tayu and S. Ueno
Fig. 5. Procedure Add Edges(v, S, M).
respectively. Then, we have the following.
Theorem 4. Algorithm Min-SMP(T, ⪯T , r) solves Min-WSMP in linear time
for a tree preference system (T, ⪯T ).
⊓⊔
Moreover, we can also compute the size of a minimum weighted stable matching
for a weighted preference system by replacing (3) and (5) with (12) and (13),
respectively. Let Min-WSMP(T, ⪯T , r, w) be the algorithm obtained from Min-
SMP(T, ⪯T , r) by the modiﬁcations. Then, we have the following.
Theorem 5. Algorithm Min-WSMP(T, ⪯T , r, w) solves Min-WSMP in linear
time for a weighted tree preference system (T, ⪯T , w).
⊓⊔
4.2
Extensions
The (weighted) preference system can be deﬁned for general graphs without any
modiﬁcation. The general stable matching problem (GSMP) is to ﬁnd a stable
matching of a general preference system (G, ⪯G), where G is a general graph.
It has been known that there exists a general preference system which has no
stable matching, and GSMP is NP-hard [6,10]. If G is a bipartite graph, (G, ⪯G)
is referred to as a bipartite preference system in this section.
The maximum-weight general stable matching problem (Max-WGSMP) is to
ﬁnd a stable matching with maximum weight for a weighted general preference
system. The minimum-weight general stable matching problem (Min-WGSMP)

Stable Matchings in Trees
503
is to ﬁnd a stable matching with minimum weight for a weighted general pref-
erence system.
It has been known that both Max-WGSMP and Min-WGSMP are NP-hard
even for weighted strict general preference systems [2]. It is also known that
both problems are sovable in O(m2 log m) time for a weighted strict bipartite
preference system (G, ⪯G, w), where m = |E(G)| [1,4,8]. An extension can be
found in [1].
It is interesting to note that both Max-WGSMP and Min-WGSMP can be
solved in plynomial time if the treewidth of G is bounded by a constant. We
can prove the following by extending our resutlts on trees, although the proof is
complicated.
Theorem 6. Both Max-WGSMP and Min-WGSMP can be solved in O(nΔk+1)
time if the treewidth of G is bounded by k, where n = |V (G)|, and Δ is the
maximum degree of a vertex of G.
⊓⊔
It should be noted that if k = 1, both problems can be solved in linear time
as shown in Theorems 3 and 5.
Acknowledgements. The research was partially supported by JSPS KAKENHI
Grant Number 26330007.
References
1. Chen, X., Ding, G., Hu, X., Zang, W.: The maximum-weight stable matching
problem: duality and eﬃciency. SIAM J. Discret. Math. 26, 1346–1360 (2012)
2. Feder, T.: A new ﬁxed point approach for stable networks and stable marriages.
J. Comput. Syst. Sci. 45, 233–284 (1992)
3. Gale, D., Shapley, L.S.: College admissions and the stability of marriage. Am.
Math. Mon. 69, 9–15 (1962)
4. Gusﬁeld, D., Irving, R.W.: The Stable Marriage Problem - Structure and Algo-
rithms. Foundations of computing series. MIT Press, Cambridge (1989)
5. Halld´orsson, M., Irving, R., Iwama, K., Manlove, D., Miyazaki, S., Morita, Y.,
Scott, S.: Approximability results for stable marriage problems with ties. Theor.
Comput. Sci. 306, 431–447 (2003)
6. Irving, R.W., Manlove, D.: The stable roommates problem with ties. J. Algorithms
43, 85–105 (2002)
7. Irving, R.W., Manlove, D., O’Malley, G.: Stable marriage with ties and bounded
length preference lists. J. Discret. Algorithms 1, 213–219 (2009)
8. Kir´aly, T., Pap, J.: Total dual integrality of rothblum’s description of the stable-
marriage polyhedron. Math. Oper. Res. 33, 283–290 (2008)
9. Manlove, D., Irving, R., Iwama, K., Miyazaki, S., Morita, Y.: Hard variants of
stable marriage. Theor. Comput. Sci. 276, 261–279 (2002)
10. Ronn, E.: NP-complete stable matching problems. J. Algorithms 11, 285–304
(1990)

Maximum Matching on Trees
in the Online Preemptive
and the Incremental Dynamic Graph Models
Sumedh Tirodkar1(B) and Sundar Vishwanathan2
1 School of Technology an Computer Science, TIFR, Mumbai, India
sumedh.tirodkar@tifr.res.in
2 Department of Computer Science and Engineering, IIT Bombay, Mumbai, India
sundar@cse.iitb.ac.in
Abstract. We study the Maximum Cardinality Matching (MCM) and
the Maximum Weight Matching (MWM) problems, on trees and on some
special classes of graphs, in the Online Preemptive and the Incremental
Dynamic Graph models. In the Online Preemptive model, the edges of
a graph are revealed one by one and the algorithm is required to always
maintain a valid matching. On seeing an edge, the algorithm has to either
accept or reject the edge. If accepted, then the adjacent edges are dis-
carded, and all rejections are permanent. In this model, the complexity
of the problems is settled for deterministic algorithms [11,15]. Epstein
et al. [5] gave a 5.356-competitive randomized algorithm for MWM, and
also proved a lower bound of 1.693 for MCM. The same lower bound
applies for MWM.
In this paper we show that some of the results can be improved in the
case of trees and some special classes of graphs. In the online preemp-
tive model, we present a 64/33-competitive (in expectation) randomized
algorithm (which uses only two bits of randomness) for MCM on trees.
Inspired by the above mentioned algorithm for MCM, we present the
main result of the paper, a randomized algorithm for MCM with a “worst
case” update time of O(1), in the incremental dynamic graph model,
which is 3/2-approximate (in expectation) on trees, and 1.8-approximate
(in expectation) on general graphs with maximum degree 3.
We also present a minor result for MWM in the online preemptive
model, a 3-competitive (in expectation) randomized algorithm (that uses
only O(1) bits of randomness) on growing trees (where the input revealed
upto any stage is always a tree, i.e. a new edge never connects two dis-
connected trees).
1
Introduction
The Maximum (Cardinality/Weight) Matching problem is one of the most exten-
sively studied problems in Combinatorial Optimization. See Schrijver’s book [12]
S. Tirodkar—A part of this work was done when the author was a student in the
Department of Computer Science and Engineering at IIT Bombay.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 504–515, 2017.
DOI: 10.1007/978-3-319-62389-4 42

Maximum Matching on Trees in the Online Preemptive
505
and references therein for a comprehensive overview of classic work. A matching
M ⊆E is a set of edges such that at most one edge is incident on any vertex. Tra-
ditionally the problem was studied in the oﬄine setting where the entire input is
available to the algorithm beforehand. But over the last few decades it has been
extensively studied in various other models where the input is revealed in pieces,
like the vertex arrival model (adversarial and random), the edge arrival model
(adversarial and random), streaming and semi-streaming models, the online pre-
emptive model, etc. [4–6,9–11]. In this paper, we study the Maximum Cardinality
Matching (MCM) and the Maximum Weight Matching (MWM) problems, on
trees and on some special classes of graphs, in the Online Preemptive model,
and in the Incremental Dynamic Graph model. (Refer Appendix D in [14] for a
comparison between the two models.)
In the online preemptive model, the edges arrive in an online manner, and
the algorithm is supposed to accept or reject an edge on arrival. If accepted, the
algorithm can reject it later, and all rejections are permanent. The algorithm
is supposed to always maintain a valid matching. There is a 5.828-competitive
deterministic algorithm due to McGregor [11] for MWM, and a tight lower bound
on deterministic algorithms due to Varadaraja [15]. Epstein et al. [5] gave a
5.356-competitive randomized algorithm for MWM, and also proved a 1.693
lower bound on the competitive ratio achievable by any randomized algorithm
for MCM. No better lower bound is known for MWM.
In [3], the authors gave the ﬁrst randomized algorithm with competitive ratio
(28/15 in expectation) less than 2 for MCM in the online preemptive model, on
growing trees (deﬁned in Sect. 1.1). In Sect. 2, we extend their algorithm to give
a 64/33-competitive (in expectation) randomized (which uses only two bits of
randomness) algorithm for trees. Although the algorithm is an extension of the
one for growing trees in [3], it serves as a basis of the algorithm (described in
Sect. 3) for MCM in the incremental dynamic graph model.
Note that the adversary presenting the edges in the online preemptive model
is oblivious, and does not have access to the random choices made by the
algorithm.
In recent years, algorithms for approximate MCM in dynamic graphs have
been the focus of many studies due to their wide range of applications.
Here [1,2,8,13] is a non-exhaustive list some of the studies. The objective of
these dynamic graph algorithms is to eﬃciently process an online sequence of
update operations, such as edge insertions and deletions. It has to quickly main-
tain an approximate maximum matching despite an adversarial order of edge
insertions and deletions. Dynamic graph problems are usually classiﬁed accord-
ing to the types of updates allowed: incremental models allow only insertions,
decremental models allow only deletions, and fully dynamic models allow both.
We study MCM in the incremental model. Gupta [7] proved that for any ϵ ≤1/2,
there exists an algorithm that maintains a (1+ϵ)-approximate MCM on bipartite
graphs in the incremental model in an “amortized” update time of O

log2 n
ϵ4

.
We present a randomized algorithm for MCM in the incremental model with a
“worst case” update time of O(1), which is 3/2-approximate (in expectation) on

506
S. Tirodkar and S. Vishwanathan
trees, and 1.8-approximate (in expectation) on general graphs with maximum
degree 3. Note that the algorithm of Gupta [7] is based on multiplicative weights
update, and it therefore seems unlikely that a better running time analysis for
special classes of graphs is possible.
We present a minor result in Sect. 4, a 3-competitive (in expectation) ran-
domized algorithm (which uses only O(1) bits of randomness) for MWM on
growing trees in the online preemptive model. Although, growing trees is a very
restricted class of graphs, there are a couple of reasons to study the performance
of the algorithm on this class of input. Firstly, almost all lower bounds, includ-
ing the one due to Varadaraja [15] for MWM are on growing trees. Secondly,
even for this restricted class, the analysis is involved. We use the primal-dual
technique for analyzing the performance of this algorithm, and show that this
analysis is indeed tight by giving an example, for which the algorithm achieves
the competitive ratio 3. We describe the algorithm for general graphs, but were
only able to analyze it for growing trees, and new ideas are needed to prove a
better bound for general graphs.
1.1
Preliminaries
We use primal-dual techniques to analyze the performance of all the randomized
algorithms described in this paper. Here are the well known Primal and Dual
formulations of the matching problem.
Primal LP
Dual LP
max 
e wexe
min 
v yv
∀v : 
v∈e xe ≤1 ∀e : yu + yv ≥we
xe ≥0
yv ≥0
For MCM, we = 1 for any edge. Any matching M implicitly deﬁnes a feasible
primal solution. If an edge e ∈M, then xe = 1, otherwise xe = 0.
Suppose an algorithm outputs a matching M, then let P be the corresponding
primal feasible solution. Let D denote some feasible dual solution. The following
claim can be easily proved using weak duality.
Claim 1. If D ≤α · P, then the algorithm is α-competitive.
If M is any matching, then for an edge e, X(M, e) denotes edges in M which
share a vertex with the edge e. We will say that a vertex(/an edge) is covered by
a matching M if there is an edge in M which is incident on(/adjacent to) the
vertex(/edge). We also say that an edge is covered by a matching M if it belongs
to M.
In the online preemptive model, growing trees are trees, such that a new edge
has exactly one vertex common with already revealed edges.

Maximum Matching on Trees in the Online Preemptive
507
2
MCM in the Online Preemptive Model
In this section, we present a randomized algorithm (that uses only 2 bits of
randomness) for MCM on trees in the online preemptive model.
The algorithm maintains four matchings M1, M2, M3, M4, and it tries to
ensure that a large number of input edges are covered by some or other match-
ings. (Here, the term “large number” is used vaguely. Suppose more than four
edges are incident on a vertex, then at most four of them will belong to match-
ings, one to each.) One of the four matchings is output uniformly at random. A
more formal description of the algorithm follows.
Algorithm 1. Randomized Algorithm for MCM on Trees
1. Pick l ∈u.a.r. {1, 2, 3, 4}.
2. The algorithm maintains four matchings: M1, M2, M3, and M4.
3. On arrival of an edge e, the processing happens in two phases.
(a) The augment phase. The new edge e is added to each Mi in which there
are no edges adjacent to e.
(b) The switching phase. For i = 2, 3, 4, in order, Mi ←Mi \ X(Mi, e) ∪{e},
provided it decreases the quantity 
j∈[4],i̸=j,|X(Mi∩Mj,e)|=|X(Mi,e)| |Mi ∩Mj|.
4. Output Ml.
Although, l is picked randomly at the beginning of the algorithm, this value
is not known to the adversary.
Note that in the switching phase, the expected size of the matching stored
by the algorithm might decrease. For example, consider two disjoint edges e1
and e2 that have been revealed. Each of them will belong to all four matchings.
So the expected size of the matching stored by the algorithm is 2. Now, if an
edge e is revealed between e1 and e2, then e will be added to M2 and M3. The
expected size of the matching is now 1.5. The important thing to notice here is
that the decrease is not too much, and we are able to prove that the competitive
ratio of the algorithm still remains below 2.
We begin with the following observations.
– After an edge is revealed, its end points are covered by all four matchings.
– An edge e that does not belong to any matching has four edges incident on
its end points such that each of these edges belongs to a distinct matching.
This holds when the edge is revealed, and does not change subsequently.
– Every edge is covered by at least three matchings.
An edge is called internal if there are edges incident on both its end points which
belong to some matching. An edge is called a leaf edge either if one of its end
point is a leaf or if all the edges incident on one of its end points do not belong
to any matching. An edge is called bad if its end points are covered by only three
matchings.

508
S. Tirodkar and S. Vishwanathan
We begin by proving some properties about the algorithm. The key structural
lemma that keeps “inﬂuences” of bad edges local is given below.
Lemma 1. At most ﬁve consecutive vertices on a path can have bad edges inci-
dent on them.
According to Lemma 1, there can be at most four consecutive internal bad edges
or at most ﬁve bad leaf edges incident on ﬁve consecutive vertices of a path.
Lemma 1 is proved in Appendix A of [14].
Once all edges have been seen, we distribute the primal charge among the
dual variables, and use the primal-dual framework to infer the competitive ratio.
If end points of every edge are covered with four matchings, then the distribution
of dual charge is easy. However we do have bad edges, and would like the edges in
matchings to contribute more to the end-points of these edges. Then, the charge
on the other end-point would be less and we need to balance this through other
edges. Details follow.
Lemma 2. There exists an assignment of the primal charge to the dual variables
such that the dual constraint for each edge e ≡(u, v) is satisﬁed at least 33
64 in
expectation, i.e. E[yu + yv] ≥33
64.
Proof. Root the tree at an arbitrary vertex. For any edge e ≡(u, v), let v be the
parent vertex, and u be the child vertex. The dual variable assignment is done
after the entire input is seen, as follows.
Dual Variable Management: An edge e will distribute its primal weight
between its end-points. The exact values are discussed below. In general, we
look to transfer all of the primal charge to the parent vertex. But this does not
work and we need a ﬁner strategy. This is detailed below.
– If e does not belong to any matching, then it does not contribute to the value
of dual variables.
– If e belongs to a single matching then, depending on the situation, one of 0,
ϵ, 2ϵ, 3ϵ, 4ϵ, or 5ϵ of its primal charge will be assigned to u and the rest will
be assigned to v.
– If e belongs to two matchings, then at most 6ϵ of its primal charge will be
assigned to u as required. The rest is assigned to v.
– If e belongs to three or four matchings, then its entire primal charge is assigned
to v.
We will show that yu + yv ≥2 + ϵ for such an edge, when summed over all
four matchings. The value of ϵ is chosen later.
For the sake of analysis, if there are bad leaf edges incident on both end
points of an internal edge, then we analyze it as a bad internal edge. We need to
do this because a bad leaf edge might need to transfer its entire primal charge
to the vertex on which there are edges which do not belong to any matching.
Note that the end points of the internal edge would still be covered by three
matchings, even if we consider that the bad leaf edges do not exist on its end
points. The analysis breaks up into eight cases.

Maximum Matching on Trees in the Online Preemptive
509
Case 1. Suppose e does not belong to any matching. There must be a total
of at least 4 edges incident on u and v besides e, each belonging to a distinct
matching. Of these 4, at least a total of 3, say e1, e2, and e3, must be between
some children of u and v, to u and v respectively. The edges e1, e2, and e3,
each assign a charge of at least 1 −5ϵ to yu and yv, respectively. Therefore,
yu + yv ≥3 −15ϵ ≥2 + ϵ.
Case 2. Suppose e is a bad leaf edge that belongs to a single matching, and
internal edges are incident on v. This implies that there is an edge e1 from a
child vertex of v to v, which belongs to single matching, and another edge e2,
also belonging to single matching from v to its parent vertex. The edge e assigns
a charge of 1 to yv. If e1 assigns a charge of 1 or 1−ϵ or 1−2ϵ or 1−3ϵ or 1−4ϵ
to yv, then e2 assigns ϵ or 2ϵ or 3ϵ or 4ϵ or 5ϵ respectively to yv. In either case,
yu + yv = 2 + ϵ. The key fact is that e1 could not have assigned 5ϵ to its child
vertex. Since, then, by Lemma 1, e cannot be a bad edge.
Case 3. Suppose e is a bad leaf edge that belongs to a single matching, and
internal edges are incident on u. This implies that there are two edges e1 and e2
from children of u to u, each belonging to a single distinct matching. The edge
e assigns a charge of 1 to yv. Both e1 and e2 assign a charge of at least 1 −4ϵ
to yu. In either case, yu + yv ≥3 −8ϵ ≥2 + ϵ. The key fact is that neither e1
nor e2 could have assigned more than 4ϵ to their corresponding child vertices.
Since, then, by Lemma 1, e cannot be a bad edge.
Case 4. Suppose e is an internal bad edge. This implies (by Lemma 1) that there
is an edge e1 from a child vertex of u to u, which belongs to a single matching.
Also, there is an edge e2, from v to its parent vertex (or from a child vertex v
to v), which also belongs to a single matching. The edge e assigns its remaining
charge (1 or 1 −ϵ or 1 −2ϵ or 1 −3ϵ or 1 −4ϵ) to yv. If e1 assigns a charge of 1
or 1−ϵ or 1−2ϵ or 1−3ϵ or 1−4ϵ to yu, then e2 assigns ϵ or 2ϵ or 3ϵ or 4ϵ or 5ϵ
respectively to yv. In either case, yu + yv = 2 + ϵ. The key fact is that e1 could
not have assigned 5ϵ to its child vertex. Since, then, by Lemma 1, e cannot be a
bad edge.
Case 5. Suppose e is not a bad edge, and it belongs to a single matching. Then
either there are at least two edges e1 and e2 from child vertices of u or v to u or v
respectively, or e1 on u and e2 on v, each belonging to a single matching, or one
edge e3 from a child vertex of u or v to u or v, respectively, which belongs to two
matchings, or one edge e4 from a child vertex of u or v to u or v, respectively,
which belongs to single matching, and one edge e5 from v to its parent vertex
which belongs to two matchings. In either case, yu + yv ≥3 −10ϵ ≥2 + ϵ.
Case 6. Suppose e is a bad edge that belongs to two matchings, and internal
edge is incident on u or v. This implies that there is an edge e1, from a child
vertex of u to u or from v to its parent vertex which belongs to a single matching.
The edge e assigns a charge of 2 to yv, and the edge e1 assigns a charge of ϵ to
yu or yv respectively. Thus, yu + yv = 2 + ϵ.

510
S. Tirodkar and S. Vishwanathan
Case 7. Suppose e is not a bad edge and it belongs to two matchings. This means
that either there is an edge e1 from a child vertex of u to u, which belongs to at
least one matching, or there is an edge from child vertex of v to v that belongs
to at least one matching, or there is an edge from v to its parent vertex which
belongs to two matchings. The edge e assigns a charge of 2 among yu and yv.
The neighboring edges assign a charge of ϵ to yu or yv (depending on which
vertex it is incident to), yielding yu + yv ≥2 + ϵ.
Case 8. Suppose, e belongs to 3 or 4 matchings, then trivially yu + yv ≥2 + ϵ.
From the above cases, yv + yv ≥3 −15ϵ and yu + yv ≥2 + ϵ. The best value
for the competitive ratio is obtained when ϵ =
1
16, yielding E[yu + yv] ≥33
64.
⊓⊔
Lemma 2 immediately implies Theorem 2 using Claim 1.
Theorem 2. Algorithm 1 is a 64
33-competitive randomized algorithm for ﬁnding
MCM on trees.
3
MCM in the Incremental Dynamic Graph Model
In this section, we present our main result, a randomized algorithm (that uses
only O(1) bits of randomness) for MCM in the incremental dynamic graph model,
which is 3/2-approximate (in expectation) on trees, and is 1.8-approximate (in
expectation) on general graphs with maximum degree 3. It is inspired by the
randomized algorithm for MCM on trees described in Sect. 2. In the online pre-
emptive model, we cannot add edges in the matching which were discarded ear-
lier, which results in the existence of bad edges. But in the incremental dynamic
graph model, there is no such restriction. For some i ∈[3], let e ≡(u, v) ∈Mi
be switched out by some edge e′ ≡(u, u′), i.e. Mi ←Mi \ {e} ∪{e′}. If there is
an edge e′′ ≡(v, v′) ∈Mj for i ̸= j, then we can add e′′ to Mi if possible. Using
this simple trick, we get a better approximation ratio in this model, and also,
the analysis becomes signiﬁcantly simpler. Details follow.
Algorithm 2. Randomized Algorithm for MCM
1. Pick l ∈u.a.r. {1, 2, 3}.
2. The algorithm maintains three matchings: M1, M2, and M3.
3. When an edge e is inserted, the processing happens in two phases.
(a) The augment phase. The new edge e is added to each Mi in which there
are no edges adjacent to e.
(b) The switching phase. For i = 2, 3, in order, Mi ←Mi \ X(Mi, e) ∪{e},
provided it decreases the quantity 
j∈[3],i̸=j,|X(Mi∩Mj,e)|=|X(Mi,e)| |Mi ∩Mj|.
For every edge e′ discarded from Mi, add edges on the other end point of e′
in Mj (∀j ̸= i) to Mi if possible.
4. Output the matching Ml.

Maximum Matching on Trees in the Online Preemptive
511
Note that the end points of every edge will be covered by all three matchings.
We again use the primal-dual technique to analyze the performance of this
algorithm on trees.
Lemma 3. There exists an assignment of the primal charge amongst the dual
variables such that the dual constraint for each edge e ≡(u, v) is satisﬁed at least
2
3rd in expectation.
Proof. Root the tree at an arbitrary vertex. For any edge e ≡(u, v), let v be the
parent vertex, and u be the child vertex. The dual variable assignment is done
at the end of input, as follows.
– If e does not belong to any matching, then it does not contribute to the value
of dual variables.
– If e belongs to a single matching, then its entire primal charge is assigned to
v as yv = 1.
– If e belongs to two matchings, then its entire primal charge is assigned equally
amongst u and v, as yu = 1 and yv = 1.
– If e belongs to three matchings, then its entire primal charge is assigned to v
as yv = 3.
The analysis breaks up into three cases.
Case 1. Suppose e does not belong to any matching. There must be a total of
at least 2 edges incident amongst u and v besides e, each belonging to a distinct
matchings, from their respective children. Therefore, yu + yv ≥2.
Case 2. Suppose e belongs to a single matching. Then either there is an edge
e′ incident on u or v which belongs to a single matching, from their respective
children, or there is an edge e′′ incident on u or v which belongs to two matchings.
In either case, yu + yv ≥2.
Case 3. Suppose e belongs to two or three matchings, then yu + yv ≥2
trivially.
⊓⊔
Lemma 3 immediately implies Theorem 3 using Claim 1.
Theorem 3. Algorithm 2 is a 3
2-approximate (in expectation) randomized algo-
rithm for MCM on trees, with a worst case update time of O(1).
We also analyze Algorithm 2 for general graphs with maximum degree 3, and
prove the following Theorem.
Theorem 4. Algorithm 2 is a 1.8-approximate (in expectation) randomized
algorithm for MCM on general graphs with maximum degree 3, with a worst
case update time of O(1).
The proof of Theorem 4 is presented in Appendix E of [14].

512
S. Tirodkar and S. Vishwanathan
4
MWM in the Online Preemptive Model
In this section, we present a randomized algorithm (that uses only O(1) bits of
randomness) for MWM in the online preemptive model, and analyze its perfor-
mance for growing trees. We describe the algorithm next. (An intuition for the
algorithm is presented in Appendix B of [14].)
Algorithm 3. Randomized Algorithm for MWM
1. Maintain two matchings M1 and M2. Let j = 1 with probability p, and j = 2
otherwise.
2. On receipt of an edge e:
For i = 1, 2, if w(e) > (1 + γi)w(X(Mi, e)), then Mi = Mi \ X(Mi, e) ∪{e}.
3. Output Mj.
Note that we cannot just output the best of two matchings because that
could violate the constraints of the online preemptive model.
4.1
Analysis
We use the primal-dual technique to analyze the performance of this algorithm.
The primal-dual technique used to analyze McGregor’s deterministic algorithm
for MWM described in [3] is fairly straightforward. However the management
becomes complicated with the introduction of randomness, and we are only able
to analyze the algorithm in a very restricted class of graphs, which are growing
trees.
Theorem 5. The expected competitive ratio of Algorithm 3 on growing trees is
max
1 + γ1
p
, 1 + γ2
1 −p , (1 + γ1)(1 + γ2)(1 + 2γ1)
p · γ1 + (1 −p)γ2 + γ1γ2

,
where p is the probability to output M1.
We maintain both primal and dual variables along with the run of the algo-
rithm. Consider a round in which an edge e ≡(u, v) is revealed, where v is the
new vertex. Before e is revealed, let e1 and e2 be the edges incident on u which
belong to M1 and M2 respectively. If such an ei does not exist, then we may
assume w(ei) = 0. The primal and dual variables are updated as follows.
– e is rejected by both matchings, we set the primal variable xe = 0, and the
dual variable yv = 0.
– e is added to M1 only, then we set the primal variable xe = p, and the dual
variable yu = max(yu, min((1 + γ1)w(e), (1 + γ2)w(e2))), and yv = 0;.
– e is added to M2 only, then we set the primal variable xe = 1 −p, and the
dual variable yu = max(yu, min((1 + γ1)w(e1), (1 + γ2)w(e))), and yv = 0.

Maximum Matching on Trees in the Online Preemptive
513
– e is added to both the matchings, then we set the primal variable xe = 1, and
the dual variables yu = max(yu, (1 + γ1)w(e)) and yv = (1 + γ1)w(e).
– When an edge e′ is evicted from M1 (or M2), we decrease its primal variable
xe′ by p (or (1 −p) respectively), and the corresponding dual variables are
unchanged.
We begin with three simple observations.
1. The cost of the primal solution is equal to the expected weight of the matching
maintained by the algorithm.
2. The dual variables never decrease. Hence, if a dual constraint is feasible once,
it remains so.
3. yu ≥min((1 + γ1)w(e1), (1 + γ2)w(e2)).
The idea behind the analysis is to prove a bound on the ratio of the dual cost
and the primal cost while maintaining dual feasibility. By Observation 2, to
ensure dual feasibility, it is suﬃcient to ensure feasibility of the dual constraint
of the new edge. If the new edge e is not accepted in any Mi, then w(e) ≤
min((1 + γ1)w(e1), (1 + γ2)w(e2)). Hence, the dual constraint is satisﬁed by
Observation 3. Else, it can be seen that the dual constraint is satisﬁed by the
updates performed on the dual variables.
The following lemma implies Theorem 5 using Claim 1.
Lemma 4.
ΔDual
ΔPrimal ≤max

1+γ1
p
, 1+γ2
1−p , (1+γ1)(1+γ2)(1+2γ1)
p·γ1+(1−p)γ2+γ1γ2

after every round.
We will use the following simple technical lemma to prove Lemma 4.
Lemma 5.
ax+b
cx+dincreases with x iﬀad −bc ≥0.
Proof (of Lemma 4). There are four cases to be considered.
1. If edge e is accepted in M1, but not in M2. Then (1 + γ1)w(e1) < w(e) ≤
(1 + γ2)w(e2). By Observation 3, before e was revealed, yu ≥(1 + γ1)w(e1).
After e is accepted in M1, ΔPrimal = p(w(e) −w(e1)), and ΔDual ≤(1 +
γ1)(w(e) −w(e1)). Hence,
ΔDual
ΔPrimal ≤(1 + γ1)
p
.
2. If edge e is accepted in M2, but not in M1. Then (1 + γ2)w(e2) < w(e) ≤
(1 + γ1)w(e1). By Observation 3, before e was revealed, yu ≥(1 + γ2)w(e2).
After e is accepted in M2, ΔPrimal = (1 −p)(w(e) −w(e2)), and ΔDual ≤
(1 + γ2)(w(e) −w(e2)). Hence,
ΔDual
ΔPrimal ≤(1 + γ2)
1 −p .

514
S. Tirodkar and S. Vishwanathan
3. If edge e is accepted in both the matchings, and (1+γ1)w(e1) ≤(1+γ2)w(e2)
< w(e). By Observation 3, before e was revealed, yu ≥(1 + γ1)w(e1). After
e is accepted in both the matchings, ΔDual ≤(1 + γ1)(2w(e) −w(e1)). The
change in primal cost is
ΔPrimal ≥w(e) −p · w(e1) −(1 −p) · w(e2)
≥w(e) −p · w(e1) −(1 −p) · w(e)
1 + γ2
= p + γ2
1 + γ2
w(e) −p · w(e1).
ΔDual
ΔPrimal ≤(1 + γ1)
2w(e) −w(e1)
p+γ2
1+γ2 w(e) −p · w(e1).
By Lemma 5, this value increases, for a ﬁxed w(e), with w(e1) if γ2 ≤
p
1−2p,
and its worst case value is achieved when (1 + γ1)w(e1) = w(e). Thus,
ΔDual
ΔPrimal ≤(1 + γ1)2(1 + γ1)(1 + γ2) −(1 + γ2)
(p + γ2)(1 + γ1) −p(1 + γ2)
= (1 + γ1)(1 + γ2)
1 + 2γ1
p · γ1 + (1 −p)γ2 + γ1γ2
.
4. If e is accepted in both the matchings, and (1 + γ2)w(e2) ≤(1 + γ1)w(e1) <
w(e). By Observation 3, before e was revealed, yu ≥(1 + γ2)w(e2). The
following bound can be proved similarly.
ΔDual
ΔPrimal ≤(1 + γ1)(1 + γ2)
1 + 2γ1
p · γ1 + (1 −p)γ2 + γ1γ2
.
⊓⊔
The following theorem is an immediate consequence of Theorem 5.
Theorem 6. Algorithm 3 is a 3-competitive (in expectation) randomized algo-
rithm for MWM on growing trees, when p = 1/3, γ1 = 0, and γ2 = 1; and the
analysis is tight.
In Appendix C of [14], an input is presented for which both M1 and M2 are
simultaneously of 1/3rd the weight of the optimum.
Note. In the analysis of Algorithm 3 for growing trees, we crucially use the
following fact in the dual variable assignment. If an edge e /∈Mi for some i,
then a new edge incident on its leaf vertex will deﬁnitely be added to Mi, and
it suﬃces to assign a zero charge to the corresponding dual variable. This is not
necessarily true for more general classes of graphs, and new ideas are needed to
analyze the performance for those classes.
Acknowledgements. The ﬁrst author would like to thank Ashish Chiplunkar for
helpful suggestions to improve the competitive ratio of Algorithm 3, and also to improve
the presentation of Sect. 4.

Maximum Matching on Trees in the Online Preemptive
515
References
1. Bernstein, A., Stein, C.: Faster fully dynamic matchings with small approximation
ratios. In: Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA 2016, Arlington, VA, USA, 10–12 January 2016, pp.
692–711 (2016)
2. Bhattacharya, S., Henzinger, M., Nanongkai, D.: Fully dynamic approximate max-
imum matching and minimum vertex cover in O(log3n) worst case update time.
In: Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, 16–19 January, pp.
470–489 (2017)
3. Chiplunkar, A., Tirodkar, S., Vishwanathan, S.: On randomized algorithms for
matching in the online preemptive model. In: Bansal, N., Finocchi, I. (eds.) ESA
2015. LNCS, vol. 9294, pp. 325–336. Springer, Heidelberg (2015). doi:10.1007/
978-3-662-48350-3 28
4. Epstein, L., Levin, A., Mestre, J., Segev, D.: Improved approximation guarantees
for weighted matching in the semi-streaming model. SIAM J. Discrete Math. 25(3),
1251–1265 (2011)
5. Epstein, L., Levin, A., Segev, D., Weimann, O.: Improved bounds for online preemp-
tive matching. In: Proceedings of the 30th International Symposium on Theoretical
Aspects of Computer Science, STACS 2013, Kiel, Germany, pp. 389–399 (2013)
6. Feigenbaum, J., Kannan, S., McGregor, A., Suri, S., Zhang, J.: On graph problems
in a semi-streaming model. Theor. Comput. Sci. 348(2), 207–216 (2005)
7. Gupta, M.: Maintaining approximate maximum matching in an incremental bipar-
tite graph in polylogarithmic update time. In: Proceedings of the 34th International
Conference on Foundation of Software Technology and Theoretical Computer Sci-
ence, FSTTCS 2014, New Delhi, India, 15–17 December 2014, pp. 227–239 (2014)
(2014)
8. Gupta, M., Peng, R.: Fully dynamic (1+ϵ)-approximate matchings. In: Proceedings
of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science,
FOCS 2013, pp. 548–557. IEEE Computer Society, Washington, DC (2013)
9. Kalyanasundaram, B., Pruhs, K.: Online weighted matching. J. Algorithms 14(3),
478–488 (1993)
10. Karp, R.M., Vazirani, U.V., Vazirani, V.V.: An optimal algorithm for on-line bipar-
tite matching. In: Proceedings of the Twenty-Second Annual ACM Symposium on
Theory of Computing, STOC 1990, pp. 352–358. ACM, New York (1990)
11. McGregor, A.: Finding graph matchings in data streams. In: Chekuri, C., Jansen,
K., Rolim, J.D.P., Trevisan, L. (eds.) APPROX/RANDOM -2005. LNCS, vol. 3624,
pp. 170–181. Springer, Heidelberg (2005). doi:10.1007/11538462 15
12. Schrijver, A.: Combinatorial Optimization - Polyhedra and Eﬃciency. Springer,
Heidelberg (2003)
13. Solomon, S.: Fully dynamic maximal matching in constant update time. In: IEEE
57th Annual Symposium on Foundations of Computer Science, FOCS 2016, Hyatt
Regency, New Brunswick, New Jersey, USA, 9–11 October 2016, pp. 325–334 (2016)
14. Tirodkar, S., Vishwanathan, S.: Maximum matching on trees in the online preemp-
tive and the incremental dynamic graph models. CoRR abs/1612.05419 (2016).
http://arxiv.org/abs/1612.05419
15. Badanidiyuru Varadaraja, A.: Buyback problem - approximate matroid intersec-
tion with cancellation costs. In: Aceto, L., Henzinger, M., Sgall, J. (eds.) ICALP
2011. LNCS, vol. 6755, pp. 379–390. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-22006-7 32

Approximation Algorithms for Scheduling
Multiple Two-Stage Flowshops
Guangwei Wu1,2 and Jianxin Wang1(B)
1 School of Information Science and Engineering, Central South University,
Changsha, People’s Republic of China
jxwang@csu.edu.cn
2 College of Computer and Information Engineering,
Central South University of Forestry and Technology,
Changsha, People’s Republic of China
Abstract. This paper studies the problem that schedules n two-stage
jobs on m multiple two-stage ﬂowshops, with the objective of minimiz-
ing the makespan. The problem is NP-hard even when m is a ﬁxed con-
stant, and becomes strongly NP-hard when m is a part of input. A 17/6-
approximation algorithm along with its analysis is presented for arbitrary
m ≥2. This is the ﬁrst approximation algorithm for multiple ﬂowshops
when the number m of ﬂowshops is a part of input. The arbitrary m
and the time complexity O(n log n + mn) of the algorithm demonstrate
that the problem, which plays an important role in the current research
in cloud computing and data centers, can be solved eﬃciently with a
reasonable level of satisfaction.
Keywords: Scheduling · Multiple two-stage ﬂowshops · Approximation
algorithm · Cloud computing
1
Introduction
Motivated by the current research in data centers and cloud computing, this
paper studies the scheduling problem for two-stage jobs on multiple two-stage
ﬂowshops. A job is a two-stage job if it consists of an R-operation and a T-
operation. A ﬂowshop is a two-stage ﬂowshop if it contains an R-processor and
a T-processor. When a job is assigned to a ﬂowshop, its R-operation and T-
operation are executed by R-processor and T-processor of the same ﬂowshop,
respectively, and the T-operation cannot start until the R-operation is ﬁnished.
Therefore in this problem, a schedule is to assign the jobs to ﬂowshops, and then
for each ﬂowshop, to determine the executing order of the R- and T-operations
of the jobs assigned to that ﬂowshop. The problem is formally stated as follows:
This work is supported by the National Natural Science Foundation of China under
grants 61420106009, 61672536, 61232001, and 61472449, Scientiﬁc Research Fund of
Hunan Provincial Education Department under grant 16C1660.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 516–528, 2017.
DOI: 10.1007/978-3-319-62389-4 43

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
517
Construct a schedule for n given two-stage jobs on m identical two-stage
ﬂowshops that minimizes the makespan, i.e., the completion time of the
last job.
The drive of our paper on the multiple two-stage ﬂowshops problem came
from the recent research on data centers and cloud computing. Cloud comput-
ing refers to both the applications delivered as services over the Internet and
the hardware and system software in the data centers that provide these ser-
vices [3]. Today’s data center contains tens of thousands or more servers con-
sisting of CPUs, memory, network interface, and local high-speed I/O (disk or
ﬂash), where applications and data are stored as resources [1,9]. In this kind of
context, when clients request services in cloud computing, for a corresponding
resource, the server needs ﬁrst to read the resource from local I/O to memory –
this is the R-operation, and then to send it through network interface over Inter-
net to the clients – this is the T-operation. Therefore, given a set of resources
requested from clients, scheduling them on multiple servers in data centers to
make the completion time of the last resource minimum is meaningful in practi-
cal applications, which is exactly the scheduling problem studied in this paper.
The throughput of server is limited by the bandwidth of disk (or ﬂash) and of
network [13]. Typical settings of server are shown in Table 1 [2], which shows
that the R-operation and the T-operation in a typical server are comparable in
general. The two operations need not to have a linear relation due to the impact
of cache system, therefore, neither of them can be simply ignored when consid-
ering this scheduling problem. There are other characteristics to be considered.
For example, the number of servers in data centers is usually very large and may
vary frequently due to the issues such as economic factors [9]. Thus, it is natural
to consider the number of servers as a part of input rather than a ﬁxed integer.
Table 1. Typical server settings in data centers
Media
Capacity (TB) Bandwidth (GB/sec)
Disk(x18) 12–144
0.2–2
Flash(x4) 1–4
1–4
Network
N/A
1.25–5
When the number m of ﬂowshops is 1, this scheduling problem becomes the
classic two-stage ﬂowshop problem, which can be solved in O(n log n) time by the
classical Johnson’s algorithm where n is the number of jobs [12]. For a general
case where m ≥2, unfortunately, even in the special case when the R-operation
of each job costs 0, the problem becomes the classic Makespan problem, which
is NP-hard when m ≥2 is a ﬁxed number and becomes strongly NP-hard when
m is an arbitrary integer [5]. As a consequence, the scheduling problem studied
in the current paper, where m is a part of input, is strongly NP-hard.

518
G. Wu and J. Wang
The current paper focuses on approximation algorithms for scheduling on
multiple two-stage ﬂowshops, with the objective of minimizing the scheduling
makespan. An α-approximation algorithm, that achieves the approximation ratio
α, is a polynomial-time algorithm that for all instances of the problem produces
a schedule whose makespan is within α times the minimum makespan [16].
We review some important results related to the problem studied in this
paper. For the classic Makespan problem, which can be viewed as schedul-
ing one-stage jobs on one-stage ﬂowshops, the well-known ListRanking algo-
rithm is an important technique when the number m of ﬂowshops is arbitrary.
The ListRanking algorithm was due to Graham who also proved that the algo-
rithm achieves an approximation ratio (2 −1/m) for arbitrary m [6]. Graham
further improved the approximation ratio to (4 −1/m)/3 by sorting the jobs
in non-increasing order before ListRanking [7]. Hochbaum and Shmoys gave a
polynomial-time approximation scheme using dual approximation algorithm [11].
When m is a ﬁxed integer, Sahni proposed a fully polynomial-time approxima-
tion scheme [14].
Scheduling multiple two-stage ﬂowshops had not been studied thoroughly
until very recently, and most studies focused on the problem when the number
m of ﬂowshops is a ﬁxed constant. He et al. [10] was the ﬁrst group who stud-
ied this problem, motivated by applications in glass manufacturing. A mixed-
integer programming formulation was proposed and a heuristic algorithm was
developed. Vairaktarakis et al. [15] studied the problem in order to cope with
the hybrid ﬂowshop problem. In particular, when the number m of ﬂowshops
equals 2, they proposed a formulation that leads to a pseudo-polynomial time
exact algorithm. Zhang et al. [19] developed a 3/2-approximation algorithm for
m = 2 and a 12/7-approximation algorithm for m = 3 for the problem. Both
algorithms running in time O(n log n) ﬁrst sort the jobs by Johnson’s algorithm,
then split the jobs into two (or three) subsequences for two (or three) ﬂowshops.
Using a similar formulation to that for m = 2 in [15], Dong et al. [4] developed
a pseudo-polynomial time exact algorithm, and proposed a fully polynomial-
time approximation scheme based on the exact algorithm for a ﬁxed constant
m ≥2. Very recently, Wu et al. [17] proposed a new formulation for the problem
that leads directly to improvements to the complexity of scheduling algorithms.
They further proposed a new approach that signiﬁcantly improves scheduling
algorithms when the costs of the two stages are diﬀerent signiﬁcantly. Further
improvements for the approximation algorithms were also studied.
When the number of ﬂowshops is unbounded, the multiple two-stage ﬂow-
shops problem is strongly NP-hard and had almost been untouched. Wu et al. [18]
considered two restricted versions of the problem: one restricts the R-operation
to cost no less time than the T-operation for each job, while the other assumes
that the T-operation is more time-consuming than the R-operation for each
job. For the ﬁrst case, an online 2-competitive algorithm and an oﬄine 11/6-
approximation algorithm were developed. For the second case, an online 5/2-
competitive algorithm and an oﬄine 11/6-approximation algorithm were given.

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
519
To the best of our knowledge, no approximation results for the problem of
scheduling multiple ﬂowshops have been known when m is not a ﬁxed constant.
On the other hand, as explained earlier in this section, for applications in cloud
computing and data centers, scheduling two-stage jobs on unbounded number
of two-stage ﬂowshops seems a common practice. This motivated the research of
the current paper. In this paper, we present a 17/6-approximation algorithm for
the multiple two-stage ﬂowshop scheduling problem for arbitrary m ≥2, which
runs in time O(n log n + nm). This is the ﬁrst approximation algorithm for the
problem when the number of ﬂowshops is not a ﬁxed constant. The arbitrary m
and the time complexity of the algorithm demonstrate that the problem, which
plays an important role in the current research in cloud computing and data
centers, can be solved eﬃciently with a reasonable level of satisfaction. Moreover,
practically, for the case where the number of ﬂowshops is a ﬁxed constant, our
algorithm is more eﬃcient when compared with those approximation algorithms
constructed from pseudo-polynomial time exact algorithms [4,17].
2
Preliminary
For n two-stage jobs G = {J1, . . . , Jn} to be scheduled on m identical two-stage
ﬂowshops M = {M1, . . . , Mm}, we make the following assumptions:
1. each job consists of an R-operation and a T-operation;
2. each ﬂowshop has an R-processor and a T-processor that can run in parallel
and can process the R-operations and the T-operations, respectively, of the
assigned jobs;
3. the R-operation and T-operation of a job must be executed in the R-processor
and T-processor, respectively, of the same ﬂowshop, in such a way that the
T-operation cannot start unless the R-operation is completed;
4. there is no precedence constraints among the jobs; and
5. preemption is not allowed.
Under this model, each job Ji can be given as a pair (ri, ti) of integers,
where ri, the R-time, is the time for processing the R-operation of Ji by an R-
processor, and ti, the T-time, is the time for processing the T-operation of Ji by
a T-processor. A schedule S of a set of jobs {J1, . . . , Jn} on m ﬂowshops M1, . . .,
Mm consists of an assignment that assigns each job to a ﬂowshop, and, for each
ﬂowshop, the execution orders of the R- and T-operations of the jobs assigned
to that ﬂowshop. The completion time of a ﬂowshop Mj under the schedule
S is the time when Mj ﬁnishes the last T-operation of the assigned jobs. The
makespan Cmax of S is the largest ﬂowshop completion time under the schedule S
over all ﬂowshops. Following the three-ﬁeld notation α|β|γ suggested by Graham
et al. [8], we refer to the scheduling model studied in this paper as P|2FL|Cmax,
or as Pm|2FL|Cmax if the number m of ﬂowshops is a ﬁxed constant.
We will use an ordered sequence ⟨J1, J2, . . . , Jt⟩of two-stage jobs to denote
the schedule S of the jobs on a single two-stage ﬂowshop, in which both the
executions of the R- and T-operations of the jobs, by the R- and T-processor of

520
G. Wu and J. Wang
the ﬂowshop, respectively, strictly follow the given order. If our objective is to
minimize the makespan of schedules, then we can make the following assumption,
which was proved in [17].
Lemma 1. Let S = ⟨J1, J2, . . . , Jt⟩be a two-stage job schedule on a single two-
stage ﬂowshop, where Ji = (ri, ti), for 1 ≤i ≤t. Let ¯ρi and ¯τi, respectively,
be the times at which the R-operation and the T-operation of job Ji are started.
Then for all i, 1 ≤i ≤t, we can assume:
(1)
¯ρi = i−1
k=1 rk; and
(2)
¯τi = max{¯ρi + ri, ¯τi−1 + ti−1}.
We give a brief description of this lemma. For an ordered job sequence on a
two-stage ﬂowshop, the R- and T-operation of each job on the ﬂowshop always
start once they can, without further wait if the objective is to minimize the
completion time of the ﬂowshop. Therefore, the execution of the R-processor is
continuous: the R-operation of a job starts immediately when the R-operations of
the previous jobs in the sequence are ﬁnished. On the other hand, the execution
of the T-operation of a job waits for the time not only when the T-operations
of the previous jobs are ﬁnished but also when its corresponding R-operation is
ﬁnished. Each “gap” in the execution of the T-processor implies that there is a
T-operation of a job waiting for its R-operation to ﬁnish.
Therefore, the status of a ﬂowshop Mq at any moment can be represented
by a conﬁguration (ρq, τq) for a corresponding schedule, where ρq and τq are the
completion times of the R-processor and the T-processor, respectively, of the
ﬂowshop Mq. The status (ρq, τq) of the ﬂowshop Mq can be easily updated when
a new job Ji = (ri, ti) is added to it. We give the procedure of assigning a next
two-stage job Ji to a two-stage ﬂowshop Mq as Fig. 1.
Algorithm AssignJob
input: a job Ji = (ri, ti), a ﬂowshop Mq with conﬁguration (ρq, τq).
output: assign the job Ji to the ﬂowshop Mq and update the conﬁguration
of the ﬂowshop Mq.
1.
ρq = ρq + ri;
2.
if ρq ≤τq then τq = τq + ti else τq = ρq + ti;
Fig. 1. Assigning a new job Ji to a ﬂowshop Mq
3
An Approximation Algorithm for P |2FL|Cmax
In this section, we consider approximation algorithms for the multiple two-stage
ﬂowshops problem, that is: schedule n two-stage jobs G = {J1, ..., Jn} on m
two-stage ﬂowshops M = {M1, ..., Mm} with the objective of minimizing the

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
521
makespan, where m is a part of input. An approximation algorithm along with
its analysis is given in this section.
We start this section with the description of the approximation algorithm.
Before scheduling a job set G on m ﬂowshops, without loss of generality, we
ﬁrst sort G into a job sequence ⟨J1, ..., Jd−1, Jd, ..., Jn⟩, where the subsequence
⟨J1, ..., Jd−1⟩, denoted by G1, contains the jobs whose R-times are no larger
than their corresponding T-times and which are sorted in non-increasing order
by their T-times, and ⟨Jd, ..., Jn⟩, denoted by G2, contains the rest of the jobs
sorted in an arbitrary order. We introduce a new array ψ = {ψ1, ..., ψm} where
ψq, for all 1 ≤q ≤m, records the sum of the T-times of the jobs assigned to the
ﬂowshop Mq for now. Now we are ready for our algorithm given in Fig. 2.
Algorithm Approx
input: a job sequence G = J1, ..., Jd−1, Jd, ..., Jn of n two-stage jobs, which
has been sorted into G1 and G2. A integer m which represents the
number of two-stage ﬂowshops.
output: a makespan of scheduling the two-stage job set G on m two-stage
ﬂowshops M = {M1, ..., Mm}.
1.
for q = 1 to m do
1.1
ρq = 0, τq = 0, ψq = 0;
2.
for i = 1 to d −1 do
2.1
ﬁnd the ﬂowshop Mq whose ψq is minimum, for all 1 ≤q ≤m;
2.2
call the algorithm AssignJob to assign Ji to Mq;
2.3
ψq = ψq + ti;
3.
for i = d to n do
3.1
ﬁnd the ﬂowshop Mq whose ρq is minimum, for all 1 ≤q ≤m;
3.2
call the algorithm AssignJob to assign Ji to Mq;
4.
return the value max1≤q≤m{τq};
Fig. 2. An approximation algorithm for P|2FL|Cmax
The algorithm Approx contains two main steps: ﬁrst to schedule the jobs
in G1, which follows this ordered job sequence G1, and then to schedule the
rest of the jobs following the job sequence G2. When scheduling a job in G1,
this algorithm always picks the ﬂowshop Mq, whose ψq is minimum for all 1 ≤
q ≤m, to assign the job to, i.e., the ﬂowshop which has the minimum sum of
the T-times of the jobs on it over all ﬂowshops. Then, at the second step when
scheduling a job in G2, the algorithm picks Mq with the minimum ρq instead of
the minimum ψq, that is the ﬂowshop which has the minimum completion time
of the R-processor over all ﬂowshops at that time. Notice that there may be more
than one ﬂowshops, which have the same minimum cost of the T-operations of
the assigned jobs (or the same minimum completion time of the R-processor)
when scheduling a job in G1 (or G2). We mention that this algorithm always
selects the ﬂowshop with the minimum ﬂowshop index from those ﬂowshops.
The time complexity of the algorithm is given in the following theorem.

522
G. Wu and J. Wang
Theorem 1. The algorithm Approx runs in time O(n log n + nm).
We deﬁne some notations which will be used in the rest of this paper. Given
a job set G of n two-stage jobs and a integer m, let Opt(G) be the minimum
makespan of scheduling G on m two-stage ﬂowshops. It is supposed that Mh
is the ﬂowshop which achieves the makespan after scheduling the job set G by
the algorithm Approx. The completion time of Mh is denoted by τ ∗instead of
Cmax for convenience. Without loss of generality, denote by ⟨J1, ..., Jc⟩the job
sequence scheduled on the ﬂowshop Mh. Let k be the minimum job index from
which the T-operations of the following jobs on Mh are executed continuously
by Mh. In this paper, the analysis of the approximation ratio for the algorithm
Approx is divided into three cases, based on which job set Jc and Jk belong to:
Case 1. Jc belongs to the job set G1;
Case 2. Jc and Jk belong to the job set G2;
Case 3. Jc belongs to the job set G2, and Jk belongs to the job set G1.
According to Fig. 2, the algorithm Approx does not start scheduling the
jobs in G2 unless the jobs in G1 are ﬁnished, thus the case, where the job Jk
belongs to G2 while the last job Jc on Mh belongs to G1, cannot exist.
We start the analysis with Case 1. The condition in this case, that Jc belongs
to the job set G1, means that all the jobs in {J1, ..., Jc} belong to G1, i.e., for each
job Ji scheduled on Mh, we have ri ≤ti. Furthermore, we can discard the jobs
following Jc in the job sequence G to get a new instance. Such action does not
aﬀect our analysis for the approximation ratio: ﬁrst, the completion time of the
ﬂowshop Mh dose not change, because Jc is the last job on Mh thus the following
jobs discarded are scheduled on other ﬂowshops; second, Opt(G) cannot increase
by discarding jobs in G. Thus in Case 1, we can make an assumption that all the
jobs in G belong to G1, and have been sorted in non-increasing order by their
T-times before scheduling by the algorithm Approx.
For a job Ji = (ri, ti), the T-partial job of Ji is JT
i = (0, ti), i.e., the T-partial
job JT
i is constructed from the original job Ji by setting its R-time to 0. Given a
job set G = {J1, ..., Jn}, the T-partial job set GT of G is {JT
1 , ..., JT
n }, where JT
i
is the T-partial job of Ji, for 1 ≤i ≤n. It is obvious that Opt(G) ≥Opt(GT ).
We also have further observation as the following lemma.
Lemma 2. Given a job set G of n two-stage jobs, construct its T-partial job
set GT . On m two-stage ﬂowshops, if all jobs in G belong to G1, then a job Ji
is assigned to a ﬂowshop Mq when scheduling the job set G by the algorithm
Approx if and only if its T-partial job JT
i is assigned to the same ﬂowshop Mq
when scheduling the corresponding T-partial job set GT by the same algorithm.
The algorithm Approx on the job set GT , where each job with its R-time
equaling 0 can be regarded as a one-stage job, is actually the well-known Lis-
tRanking algorithm based on ψ, that sets all ψq in ψ to 0 at the initial step,
ﬁnds the ﬂowshop Mq whose ψq is minimum in ψ over all ﬂowshops for the next
job JT
i , increases ψq by ti after placing JT
i
to Mq, and returns the maximum

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
523
value in ψ over all ﬂowshops as the makespan after ﬁnishing the scheduling of all
jobs. As supposed, GT has been sorted in non-increasing order by T-time before
scheduling. Therefore the makespan of scheduling the job set GT on m ﬂowshops
by the algorithm Approx is bounded by 4
3Opt(GT ) for arbitrary m ≥2 [7], thus
further by 4
3Opt(G). Based on Lemma 2, the following theorem holds.
Theorem 2. When scheduling a job set G of n two-stage jobs on m two-stage
ﬂowshops by the algorithm Approx, if all the jobs in G belong to G1, then ψq,
for all 1 ≤q ≤m, is bounded by 4
3Opt(G) for all m ≥2.
Notice that in Case 1, we make an assumption that all jobs in the job set G
belong to G1, which does not aﬀect the analysis of the approximation ratio for
the algorithm Approx. Thus the above theorem can also be written as:
Theorem 3. In Case 1, ψq of scheduling a job set G of n two-stage jobs on m
two-stage ﬂowshops by the algorithm Approx, for all 1 ≤q ≤m, is bounded by
4
3Opt(G) for all m ≥2.
As supposed before, Mh is the ﬂowshop achieving the makespan τ ∗of schedul-
ing a job set G of n two-stage jobs on m two-stage ﬂowshops by the algo-
rithm Approx, and the job sequence ⟨J1, ..., Jc⟩is the schedule on Mh. Let
a0 = k−1
i=1 ri represent the sum of the R-times of the assigned jobs on Mh from
J1 to Jk−1, and b0 = k−1
i=1 ti denote the sum of the T-times of these jobs corre-
spondingly. Similarly, a1 = c−1
i=k+1 ri and b1 = c−1
i=k+1 ti. It is straightforward
that b0 ≥a0 and b1 ≥a1, because we have ri ≤ti for all jobs Ji where 1 ≤i ≤n
in Case 1. Figure 3 illustrates the state of Mh. Note that for convenience, we
denote the sum of the T-times of the jobs on Mh before Jk by b0, though the
execution of the T-operations of these jobs may not be continuous. Now we begin
our analysis for the algorithm Approx in this case.
Fig. 3. The state of the ﬂowshop Mh in Case 1
Theorem 4. In Case 1, the algorithm Approx is
11
6 -approximation for all
m ≥2.
Proof. The makespan of scheduling n two-stage jobs on m two-stage ﬂowshops
by the algorithm Approx, that is the completion time τ ∗of the ﬂowshop Mh,
is shown as:
τ ∗= a0 + rk + tk + b1 + tc ≤b0 + rk + tk + b1 + tc
(1)
≤4
3Opt(G) + 1
2Opt(G) = 11
6 Opt(G).
(2)

524
G. Wu and J. Wang
We explain the derivations in (1) and (2). The execution of the R-processor
of any ﬂowshop is continuous by Lemma 1. From the deﬁnition of k, the
T-operations of the jobs on Mh from the job Jk to Jc are executed continuously.
Moreover, note that the T-operation of Jk starts right after its R-operation is
ﬁnished: otherwise there would be no execution gap between the T-operations
of the jobs Jk−1 and Jk, contradicting the assumption of the minimality of the
index k. These explain the equation in (1). As discussed above, a0 is no larger
than b0, that is why the inequation in (1) holds. The inequation in (2) holds
for two reasons: ﬁrst, by Theorem 3, all ψq where 1 ≤q ≤m are bounded
by
4
3Opt(G), which implies b0 + tk + b1 + tc ≤
4
3Opt(G); second, it is clear
that Opt(G) must be no smaller than the time of completing any job, that is
ri + ti ≤Opt(G) for 1 ≤i ≤n. Combining the assumption that ri is no larger
than ti for all jobs in this case, we have rk ≤1
2Opt(G). The equation in (2) gives
the conclusion that τ ∗is no larger than 11
6 Opt(G).
⊓⊔
Now consider Case 2 where the jobs Jc and Jk belong to the job set G2, in
which each job has its R-time larger than its T-time. We use the same notations
deﬁned in the previous case: a0 = k−1
i=1 ri, b0 = k−1
i=1 ti, a1 = c−1
i=k+1 ri and
b1 = c−1
i=k+1 ti. The state of the ﬂowshop Mh, which achieves the makespan
τ ∗, is shown in Fig. 4. According to Fig. 2, the job set G has been sorted before
scheduling and the algorithm Approx does not start scheduling the jobs in G2
unless it ﬁnishes the scheduling of all the jobs in G1. Therefore the assumption
in this case, that Jk belongs to G2, implies that the jobs on Mh following Jk are
also in G2 and hence b1 < a1. We also have the following theorem.
a0
rk
a1
rc
b0
tk
b1
tc
T
R
τ
Fig. 4. The state of the ﬂowshop Mh in Case 2
Theorem 5. In Case 2, a0 + rk + a1 is bounded by Opt(G).
Proof. This case assumes that the job Jc belongs to the job set G2. According to
the algorithm Approx, the jobs in G2 are scheduled by step 3 of this algorithm.
Therefore the ﬂowshop Mh must have the minimum ρh when Jc arrives, which
means that the completion times of the R-processors of all ﬂowshops are no
smaller than ρh before Jc is assigned, which is expressed as a0 +rk +a1 here. By
Lemma 1, the execution of the R-processor of any ﬂowshop is continuous, thus
the makespan on any schedule must be no smaller than the minimum completion
time of the R-processor over all ﬂowshops, that is Opt(G) ≥a0 + rk + a1.
⊓⊔

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
525
The approximation ratio of the algorithm in Case 2 is shown as follows.
Theorem 6. In Case 2, the algorithm Approx is 2-approximation for all
m ≥2.
Proof. The makespan τ ∗achieved on the ﬂowshop Mh is expressed as:
τ ∗= a0 + rk + tk + b1 + tc < a0 + rk + tk + a1 + tc
(3)
< Opt(G) + 1
2Opt(G) + 1
2Opt(G) = 2Opt(G).
(4)
The reason for the equation in (3) is the same as that for the equation in (1).
The inequation in (3) holds obviously: b1 < a1 in this case. By Theorem 5,
a0+rk+a1 is no larger than Opt(G). This case assumes that Jk and Jc belong to
the job set G2, thus both jobs have their R-times larger than their corresponding
T-times. Due to a similar reason to that explained in the inequation in (2), the
fact that Opt(G) is no smaller than the time of completing any job Ji which
equals ri+ti, implies that tk and tc are smaller than 1
2Opt(G). These explain the
inequation in (4). The equation in (4) completes the proof of the approximation
ratio for the algorithm in Case 2.
⊓⊔
Finally we consider Case 3 where the job Jc belongs to the job set G2, while Jk
belongs to G1. As supposed in the previous cases, the job sequence ⟨J1, ..., Jc⟩
is scheduled on the ﬂowshop Mh achieving the makespan τ ∗. It is obvious that
there must be a job index d (k < d ≤c) such that the assigned jobs on Mh whose
job index are smaller than d are in the job set G1, and the rest of these jobs on
Mh belong to G2. We deﬁne the notations slightly diﬀerent from those in the
previous cases: a0 = k−1
i=1 ri, b0 = k−1
i=1 ti, a1 = d−1
i=k+1 ri, b1 = d−1
i=k+1 ti,
a2 = c−1
i=d ri, and b2 = c−1
i=d ti. From the deﬁnition of d, it is clear that
b2 < a2. The state of the ﬂowshop Mh after scheduling is shown in Fig. 5. Just
as in Case 2, Mh has the minimum ρh over all ﬂowshops when scheduling Jc in
this case, thus the following theorem similar to Theorem 5 holds.
a0
rk
a1
a2
rc
b0
tk
b1
b2
tc
T
R
τ
Fig. 5. The state of the ﬂowshop Mh in Case 3
Theorem 7. In Case 3, a0 + rk + a1 + a2 is bounded by Opt(G).
The following theorem also holds.
Theorem 8. In Case 3, b0 + tk + b1 is bounded by 4
3Opt(G).

526
G. Wu and J. Wang
Proof. The algorithm Approx can be considered as two main steps: the ﬁrst step
is scheduling the jobs in the job set G1, where the R-operation takes no more time
than the T-operation for each job and which is sorted in non-increasing order by
T-time; the second step is scheduling the jobs in G2, where the R-operation costs
more time than the T-operation for each job. As discussed in Theorem 2, when
ﬁnishing the ﬁrst step of scheduling all jobs in G1, ψq, which records the sum
of the T-times of the jobs on the ﬂowshop Mq at that time for all 1 ≤q ≤m,
is no larger than 4
3Opt(G1). It is clear that Opt(G) ≥Opt(G1), because G1 is a
subsequence of the job sequence G. Thus the ψh, which equals b0 + tk + b1 after
ﬁnishing the scheduling of all jobs in G1, is bounded by 4
3Opt(G).
⊓⊔
We mention that there is a subcase where the T-operation of the job Jd may
start after the time when the R-operation of the job Jc−1 is ﬁnished, which is
not shown in Fig. 5 and obviously is not against Theorems 7 and 8. Actually, it
is easy to see that the other subcase shown in Fig. 5, where the T-operation of
Jd starts before the time when the R-operation of Jc−1 is ﬁnished, means that
b0 + tk + b1 is no larger than a0 + rk + a1 + a2 thus further Opt(G), which is
tighter than Theorem 8. As a consequence, the following analysis is still based
on Theorem 8, which holds in both subcases.
Theorem 9. In Case 3, the algorithm Approx is
17
6 -approximation for all
m ≥2.
Proof. The makespan τ ∗is expressed as:
τ ∗= a0 + rk + tk + b1 + b2 + tc < a0 + rk + tk + b1 + a2 + tc
(5)
< Opt(G) + 4
3Opt(G) + 1
2Opt(G) = 17
6 Opt(G).
(6)
The equation in (5) is straightforward from Fig. 5, whose reason is similar to
that for the equation in (1). The fact b2 < a2 in this case proves the inequation
in (5). The inequation in (6) holds for three reasons: ﬁrst, Theorem 7 implies
a0 + rk + a2 ≤Opt(G) because a1 is obviously no smaller than 0; second, by
Theorem 8, we have tk + b1 ≤4
3Opt(G) due to that b0 ≥0; third, as discussed
in the inequation in (4), the assumption that tc is smaller than rc in this case
means that tc < 1
2Opt(G). The equation in (6) completes the proof.
⊓⊔
Combining Theorems 4, 6 and 9 about three cases, the conclusion follows.
Theorem 10. The algorithm Approx is 17
6 -approximation for all m ≥2.
Let S = {S1, ..., Sm} be the schedule for the job set G of n two-stage jobs
on m two-stage ﬂowshops by the algorithm Approx, where especially the job
sequence Sq is the schedule on the ﬂowshop Mq, for 1 ≤q ≤m. Notice that
Sq must be a subsequence of the job sequence G, which has been sorted into
two parts before scheduling and does not follow the order by Johnson’s algo-
rithm, thus may not be the optimal schedule for Mq when aiming at minimizing
the makespan. We can sort all the Sq separately by Johnson’s algorithm after

Approximation Algorithms for Scheduling Multiple Two-Stage Flowshops
527
scheduling by the algorithm Approx. It is easy to see that the additional sorting
step can get a new schedule whose makespan is no larger than that of the original
one, and can be done in time O(n log n). Therefore such step will not change the
approximation ratio and the time complexity of the algorithm Approx.
4
Conclusion
This paper studied the multiple two-stage ﬂowshops problem for scheduling n
two-stage jobs on m identical two-stage ﬂowshops. In particular, to meet the
practical demands in our research on data centers and cloud computing, we
addressed this problem in the situation where the number m of ﬂowshops is a part
of input, which makes the problem become strongly NP-hard. We proposed an
eﬃcient algorithm which runs in time O(n log n + mn). The subsequent analysis
showed that this algorithm achieves an approximation ratio 17/6. To the best of
our knowledge, it is the ﬁrst approximation algorithm for the multiple two-stage
ﬂowshops problem when the number m of ﬂowshops is a part of input.
The approximation algorithms given in [4,17], which are based on diﬀer-
ent pseudo-polynomial time exact algorithms for a ﬁxed constant m ≥2, pro-
duce schedules with makespan bounded by Opt(G)(1 + ϵ), but run in time
O(22m−1n2mm2m+1/ϵ2m−1) and O(n2m−1m2m/ϵ2m−2) respectively. Compared
to these time complexities, which become unacceptable high when considering
the enormous number m of servers in data centers even when ϵ is bigger than 1,
the time complexity O(n log n + mn) for arbitrary m makes our algorithm much
more eﬃcient in practical applications with a reasonable level of satisfaction.
References
1. Abts, D., Felderman, B.: A guided tour through data-center networking. Queue
10(5), 10 (2012)
2. Dell. http://www.dell.com/us/business/p/servers
3. Armbrust, M., Fox, A., Griﬃth, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G.,
Patterson, D., Rabkin, A., Stoica, I., et al.: A view of cloud computing. Commun.
ACM 53(4), 50–58 (2010)
4. Dong, J., Tong, W., Luo, T., Wang, X., Hu, J., Xu, Y., Lin, G.: An FPTAS for
the parallel two-stage ﬂowshop problem. Theoret. Comput. Sci. 657, 64–72 (2017)
5. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-completeness. Freeman, San Francisco (1979)
6. Graham, R.L.: Bounds for certain multiprocessing anomalies. Bell Labs Tech. J.
45(9), 1563–1581 (1966)
7. Graham, R.L.: Bounds on multiprocessing timing anomalies. SIAM J. Appl. Math.
17(2), 416–429 (1969)
8. Graham, R.L., Lawler, E.L., Lenstra, J.K., Kan, A.R.: Optimization and approxi-
mation in deterministic sequencing and scheduling: a survey. Ann. Discret. Math.
5, 287–326 (1979)
9. Greenberg, A., Hamilton, J., Maltz, D.A., Patel, P.: The cost of a cloud: research
problems in data center networks. ACM SIGCOMM Comput. Commun. Rev.
39(1), 68–73 (2008)

528
G. Wu and J. Wang
10. He, D.W., Kusiak, A., Artiba, A.: A scheduling problem in glass manufacturing.
IIE Trans. 28(2), 129–139 (1996)
11. Hochbaum, D.S., Shmoys, D.B.: Using dual approximation algorithms for schedul-
ing problems theoretical and practical results. J. ACM (JACM) 34(1), 144–162
(1987)
12. Johnson, S.M.: Optimal two-and three-stage production schedules with setup times
included. Nav. Res. Logist. (NRL) 1(1), 61–68 (1954)
13. Li, H., Ghodsi, A., Zaharia, M., Shenker, S., Stoica, I.: Tachyon: reliable, mem-
ory speed storage for cluster computing frameworks. In: Proceedings of the ACM
Symposium on Cloud Computing, pp. 1–15. ACM (2014)
14. Sahni, S.K.: Algorithms for scheduling independent tasks. J. ACM (JACM) 23(1),
116–127 (1976)
15. Vairaktarakis, G., Elhafsi, M.: The use of ﬂowlines to simplify routing complexity
in two-stage ﬂowshops. IIE Trans. 32(8), 687–699 (2000)
16. Vazirani, V.V.: Approximation Algorithms. Springer Science & Business Media,
New york (2013)
17. Wu, G., Chen, J., Wang, J.: On scheduling two-stage jobs on multiple two-stage
ﬂowshops. Technical report, School of Information Science and Engineering Central
South University (2016)
18. Wu, G., Chen, J., Wang, J.: On approximation algorithms for two-stage scheduling
problems. In: Xiao, M., Rosamond, F. (eds.) Frontiers in Algorithmics, FAW 2017.
LNCS, vol. 10336, pp. 241–253. Springer, Cham (2017)
19. Zhang, X., van de Velde, S.: Approximation algorithms for the parallel ﬂow shop
problem. Eur. J. Oper. Res. 216(3), 544–552 (2012)

The Existence of Universally Agreed Fairest
Semi-matchings in Any Given Bipartite Graph
Jian Xu(B), Soumya Banerjee, and Wenjing Rao(B)
Department of Electrical and Computer Engineering,
University of Illinois at Chicago, Chicago, IL 60607, USA
{jxu39,sbaner8,wenjing}@uic.edu
Abstract. In a bipartite graph G = (U ∪V, E) where E ⊆U × V ,
a semi-matching is deﬁned as a set of edges M ⊆E, such that each
vertex in U is incident with exactly one edge in M. Many previous
works focus on the problem of fairest semi-matchings: ones that assign
U-vertices with V -vertices as fairly as possible. In these works, fairness
is usually measured according to a speciﬁc index. In fact, there exist
many diﬀerent fairness measures, and they often disagree on the fair-
ness comparison of some semi-matching pairs. In this paper, we prove
that for any given bipartite graph, there always exists a (set of equally)
fairest semi-matching(s) universally agreed by all the fairness measures.
In other words, given that fairness measures disagree on many compar-
isons between semi-matchings, they nonetheless are all in agreement on
the (set of) fairest semi-matching(s), for any given bipartite graph. To
prove this, we propose a partial order relationship (Transfer-based Com-
parison) among the semi-matchings, showing that the greatest elements
always exist in such a partially ordered set. We then show that such
greatest elements can guarantee to be the fairest ones under the criteria
of Majorization [10]. We further show that all widely used fairness mea-
sures are in agreement on such a (set of equally) fairest semi-matching(s).
Keywords: Bipartite graph · Semi-matching · Fairness measure · Par-
tial order · Majorization · Load balancing · Resource allocation
1
Introduction
This paper focuses on the problem of a Fairest Semi-matching. A semi-matching
in a bipartite graph G = (U ∪V, E) where E ⊆U × V , is deﬁned as a set of
edges M ⊆E such that each vertex in U is incident with exactly one edge in
M. A vertex in V might be incident with more than one edge in M. Note that
in general, valid semi-matchings can be easily obtained by matching each vertex
u ∈U with an arbitrary vertex v ∈V for which (u, v) ∈E. The problem of
ﬁnding a fairest semi-matching is related to the load-balancing problem in a
system, where discrete resources (set of U) need to be assigned to the agents
This work is supported by NSF Grant CNS-1149661.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 529–541, 2017.
DOI: 10.1007/978-3-319-62389-4 44

530
J. Xu et al.
(set of V ) in the fairest way. Particularly, the given bipartite graph corresponds
to a system under some assignment constraints, where the edge set E indicate
the possible assignment options. This is representative for a large-scale system,
where resource assignment is constrained to a limited set of agents in the local
area. In such cases, the optimal solution of a load-balance problem is given by
a fairest semi-matching indicating how the resources (of U-vertices) should be
assigned to the agents (of V -vertices), under assignment constraints (of edge
set E).
Given a semi-matching M, the quota of vertex v ∈V , is deﬁned as the
number of U-vertices matched with v. The corresponding quotas vector of M,
denoted as QM, is the vector of which each element QM(v) represents the quota
of a vertex v ∈V . The fairness of a semi-matching is usually deﬁned based on its
quotas vector, with regard to some fairness measure. An example of the fairest
semi-matching according to Jain’s index [6] is shown in Fig. 1.
f(QM4) = 0.86
v3
v2
u6
v1
u2
u3
u4
u5
u1
Bipartite Graph
v3
v2
v3
v2
v3
v2
v3
v2
u6
QM4 = (3, 2, 1)
v1
u2
u3
u4
u5
u1
u6
v1
u2
u3
u4
u5
u1
M3
M4: the fairest one
QM3 = (4, 2, 0)
u6
v1
u2
u3
u4
u5
u1
u6
v1
u2
u3
u4
u5
u1
M1
M2
QM1 = (3, 3, 0)
QM2 = (4, 1, 1)
f(QM1) = 0.67
f(QM2) = 0.67
f(QM3) = 0.6
Fig. 1. An example of the fairest semi-matching, where f(QM) =
(m
i=1 QM (i))2
m m
i=1 QM (i)2 (where
m = |QM|) represents the score of QM from Jain’s index [6].
Many fairness measures have been proposed to compare quotas vectors, such
as Jain’s index [6], Entropy [11], Atkinson’s index [1], α-fairness [9], Max-min
index [2], convex cost functions [5], Lp-norm [5], lexicographically minimum fair-
ness [3]. Lan et al. [8] constructed a family of fairness measures which satis-
ﬁes several axioms. Note that, for all the fairness measures, any pair of semi-
matchings with the same sorted quotas vector are considered equal in fairness.
Nonetheless, the existence of numerous fairness measures indicates that they do
not agree on the fairness comparison between many semi-matchings (or the quo-
tas vectors of them). For this reason, most approaches choose a speciﬁc fairness
measure to work with.
Harvey et al. [5] proposed a number of algorithms to achieve a semi-matching
which is fairest with regard to Max-min index and L2-norm, and showed that

The Existence of Universally Agreed Fairest Semi-matchings
531
the obtained semi-matching is also the fairest with regard to any convex cost
functions and any Lp-norm where p ∈R and p > 1. Bokal et al. [3] showed a
semi-matching is the fairest with regard to L2-norm if and only if the sorted
quotas vector of this semi-matching is lexicographically minimum.
In this paper, we prove that for any given bipartite graph, there always exists
a set of equally fair semi-matchings, which are uniformly considered the fairest by
all the fairness measures. This paper is organized as follows. Section 2 discusses
the consensus among fairness measures with regard to a partially ordered mea-
sure of Majorization. Section 3 deﬁnes a partial order (namely Transfer-based
Comparison) between any pair of semi-matchings, which is shown to be more
rigorous than Majorization. Section 4 proves there exists a set of the greatest
semi-matchings in the proposed partially ordered set based on the Transfer-
based Comparison. Moreover, those greatest semi-matchings are shown to be
agreed by all existing fairness measures. Section 5 provides a discussion on var-
ious issues related to the Transfer-based Comparison, and some algorithms for
achieving the fairest semi-matchings. This paper is concluded in Sect. 6.
2
Consensus Among Fairness Measures
Most of the existing and widely used fairness measures are index-based. An
index-based fairness measure is a function f which maps a sorted quotas vector
into a real number, so that it can be compared against any other ones. For
instance, Jain’s index [6] was deﬁned as the modiﬁed “variance” among elements
in a quotas vector. Using an index-based fairness measure, any arbitrary pair of
quotas vectors are comparable (either one is considered fairer, or both considered
equally fair). However, diﬀerent index-based fairness measures often disagree on
the comparison of some pairs of quotas vectors. For example, for the comparison
of Q1 = (0, 3, 3) and Q2 = (4, 1, 1), the following fairness measures disagree,
and their comparison results are shown in Table 1. Suppose quotas vector Q =
(q1, · · · , qi, · · · , qm), Q1 is considered fairer than Q2, if f(Q1) > f(Q2), where
f(Q) represent the score of Q from a fairness measure f.
– Jain’s index [6]: J(Q) = (m
i=1 qi)2
m m
i=1 qi2 .
– Max-Min index [2]: M(Q) = mini=1,··· ,m{qi}.
– An index proposed in [8]: G(Q) = {m
i=1 ( qi
s )1−β}
1
β , where β ∈(−1, −∞) and
s = m
i=1 qi.
Majorization [10] is a partial order over quotas vectors, which by allowing
some pairs to be “incomparable”, oﬀers a “stricter” way of fairness comparison.
Deﬁnition 1 [10] (Majorization).
For x, y ∈Rn, x is majorized by y
(denoted as x ⪯Maj y), if n
i=1 xi = n
i=1 yi, and d
i=1 x↑
i ⩽d
i=1 y↑
i for
d = 1, . . . , n, where x↑
i and y↑
i are the ith elements of x↑and y↑, which are
sorted in ascending order.

532
J. Xu et al.
Table 1. Comparison among Jain’s index, Max-min index, and an index in [8] (β = −2)
Jain’s index [6] Max-min index [2] An index in
[8] (β = −2)
Q1 = (0, 3, 3)
0.67
0
2
Q2 = (4, 1, 1)
0.67
1
1.809
Fairness Comparison Q1 = Q2
Q1 < Q2
Q1 > Q2
For example, x = (0, 3, 3) is majorized by y = (3, 1, 2). The sorted vectors are
x↑= (0, 3, 3) and y↑= (1, 2, 3) respectively. Let Sd
x = d
i=1 x↑
i for d = 1, 2, 3.
Then we have Sx = (S1
x, S2
x, S3
x) = (0, 0 + 3, 0 + 3 + 3) = (0, 3, 6) and Sy =
(S1
y, S2
y, S3
y) = (1, 1 + 2, 1 + 2 + 3) = (1, 3, 6). Therefore, it meets x ⪯Maj y.
An example of incomparable pair is x = (0, 3, 3) and y = (4, 1, 1), such that
Sx = (S1
x, S2
x, S3
x) = (0, 3, 6) and Sy = (S1
y, S2
y, S3
y) = (1, 2, 6). Because S1
x < S1
y
and S2
x > S2
y, x and y are incomparable with regard to Majorization.
A fairness measure f is called “compatible” with Majorization,1 if it satisﬁes
that f(x) < f(y) when x ⪯Maj y. Lan et al. [8] studied various fairness measures
in networking ﬁeld, and constructed a unique family of fairness measures satisfy-
ing ﬁve axioms (continuity, homogeneity, saturation, partition, and starvation),
which includes all the popularly used fairness measures such as Atkinson’s index,
α-fairness, Jain’s index, Entropy function. This family of fairness measures has
been proved to be compatible with Majorization. To our best knowledge, all
existing fairness measures in the literature are compatible with Majorization.
3
Transfer-Based Comparison for Semi-matching
For a given semi-matching M in a bipartite graph G = (U ∪V, E), we deﬁne
a Transfer t on M to be a sequence of alternating edges ({v1, u1}, {u1, v2}, {v2,
u3}, . . . , {uk−1, vk}) with vi ∈V, ui ∈U, and {vi, ui} ∈M for each 1 ≤i ≤
k −1. Note that according to the deﬁnition of semi-matching, when {vi, ui} ∈M,
then {ui, vi+1} /∈M. Essentially, a Transfer t on M is a path beginning and
ending in V -vertices in M, consisting of alternating edges in and out of M.
The application of Transfer t to semi-matching M is deﬁned as switching the
matching and non-matching edges in M along Transfer t. The result of the
application of t to M will change M to a diﬀerent semi-matching M ′, which
includes all the {ui, vi+1} edges in t, but excludes all the {vi, ui} edges in t. For
example, in Fig. 1, a sequence of edges ({v1, u4}, {u4, v2}, {v2, u6}, {u6, v3}) is
one Transfer t on semi-matching M3, and the application of t to M3 yields M4
which is also shown in Fig. 1.
For a Transfer t on semi-matching M, t = ({v1, u1}, {u1, v2}, {v2, u3}, . . . ,
{uk−1, vk}), we call v1 and vk to be the source and destination vertex of t,
denoted as vs(t) and vd(t) respectively. The application of t to M decreases
1 A function compatible with Majorization is also known as Schur-convex function.

The Existence of Universally Agreed Fairest Semi-matchings
533
the quota of the source vertex vs(t) by 1, and increases the quota of the sink
vertex vd(t) by 1, without changing the quotas of any other vertices. Depending
on the original quota diﬀerence between the source and destination vertices, we
can deﬁne the Transfer t as one of the three types shown below. Let QM(vs(t))
and QM(vd(t)) represent the quota of vertex vs(t) and vd(t) in M respectively.
Suppose vs(t) ̸= vd(t) (source and destination are not the same vertex).
– If QM(vs(t)) > QM(vd(t)) + 1, then t is called an Improving Transfer (IT)
– If QM(vs(t)) = QM(vd(t)) + 1, then t is called a Neutral Transfer (NT)
– If QM(vs(t)) < QM(vd(t)) + 1, then t is called a Deteriorating Transfer (DT)
The special case of vs(t) = vd(t) is denoted as a Cyclic Neutral Transfer,
and can be classiﬁed as Neutral Transfer, as it does not change the quota of any
V -vertex. Overall, the application of any Transfer t to a semi-matching M will
at most change the quotas of two vertices vs(t) and vd(t). If t is an Improving
Transfer, it will reduce the absolute quotas diﬀerence between vs(t) and vd(t).
Thus, an Improving Transfer always changes a semi-matching to a fairer one.
Similarly, a Deteriorating Transfer always changes a semi-matching to a less fair
one; a Neutral Transfer changes a semi-matching to an equally fair one.
A comparison relationship between semi-matchings is proposed based on the
properties of a sequence of Transfers that can be used to change one semi-
matching into the other one, shown as in Deﬁnition 2.
Deﬁnition 2 (Transfer-based Comparison). For any two semi-matchings
Mx and My, if there exists a Transfer Sequence (t1, t2, · · · , ti, · · · , tn) such that
Mx
t1
−→M1
t2
−→M2 · · · −→Mi−1
ti
−→Mi −→· · ·
tn
−→My, and for each 1 ≤i ≤n, ti
is IT or NT, then My is deﬁned as not less fair than Mx in terms of a Transfer-
based Comparison, denoted as Mx ⪯T My. Speciﬁcally, if all ti are NT, then
Mx is deﬁned as equally fair with My, denoted as Mx ≈T My; if all ti are either
IT or NT, with at least one ti being IT, then My is deﬁned as fairer than Mx,
denoted as Mx ≺T My.
If for every possible sequences (t1, t2, · · · , ti, · · · , tn) such that Mx
t1
−→M1
t2
−→
M2 · · · −→Mi−1
ti
−→Mi −→· · ·
tn
−→My, there always exist some ti, tj with
1 ≤i, j ≤n such that ti is IT and tj is DT, then Mx and My are incomparable,
denoted as Mx ≺≻T My.
Such a Transfer-based Comparison aims at reserving the “strictly compara-
ble” relationships between semi-matchings via identiﬁable Improving or Neutral
Transfer Sequence, while leaving out the “incomparable” ones with mixtures of
both Improving and Deteriorating Transfers. It is easy to prove this deﬁned com-
parison meets the properties of Reﬂexivity, Antisymmetry, Transitivity. Thus it
is a partial order on the set of all semi-matchings in a given bipartite graph.
Lemma 1 (Partially Ordered Set (poset)). The set of all semi-matchings
of a bipartite graph is a partially ordered set with regard to the Transfer-based
Comparison deﬁned above.

534
J. Xu et al.
Diﬀerent from the partial order of Majorization, which is deﬁned on the quo-
tas vectors, the proposed Transfer-based Comparison is a partial order deﬁned
over the set of semi-matchings. Moreover, we prove that the Transfer-based Com-
parison implies the Majorization relationship between the corresponding quotas
vectors. In other words, the proposed Transfer-based Comparison is a “stricter”
fairness measure than Majorization, by leaving out some pairs that can be com-
pared under Majorization as incomparable.
Lemma 2 (Improving Transfer ⇒Majorization).
If Mx ⪯T My, then
QMx ⪯Maj QMy.
Proof. Let Q↑
Mx = (qx
1, qx
2, · · · , qx
i , · · · , qx
n) be the sorted quotas vectors of Mx in
ascending order and the partial sums Sd
x = d
i=1 qx
i for d = 1, · · · , n. Assume the
application of an Improving Transfer t to Mx yields another semi-matching M ′
x,
which decreases the quota of source vertex of t (suppose qx
j ) by 1, and increases
the quota of destination vertex of t (suppose qx
i ) by 1. Note that Transfer t is IT,
therefore the quota of its source vertex must be larger than the quota of its source
vertex in Mx, thus i < j. Let Q↑
Mx′ represents the quotas vector of Mx′ sorted in
ascending order, and Sd
x′ represents the partial sums of Q↑
Mx′ for d = 1, · · · , n. It
can be easily derived that Sd
x′ = Sd
x when d < i, that Sd
x′ > Sd
x when i ≤d < j,
and that Sd
x′ = Sd
x when j ≤d ≤n. Likewise, if a Neutral Transfer is applied,
then Sd
x′ = Sd
x when 1 ≤d ≤n. Thus, it meets QMx ⪯Maj QMx′ if M ′
x can be
derived by applying an Improving or Neutral Transfer to Mx.
If Mx ⪯T My, then My can be derived by applying a sequence of Improving or
Neutral Transfers to Mx. Thus, it meets QMx ⪯Maj QMy due to the Transitivity
of the partial order “⪯Maj”.
⊓⊔
Corollary 1. Any fairest semi-matching with regard to “⪯T ” is the fairest with
regard to “⪯Maj”.
Corollary 2. Any fairest semi-matching with regard to “⪯T ” is the fairest with
regard to any fairness measure compatible with Majorization.
4
The Existence of the Fairest Semi-matchings Under
Transfer-Based Comparison
In this section, we prove the poset of semi-matchings under Transfer-based Com-
parison has the greatest elements, i.e., the fairest semi-matchings. This is done
by showing there always exists a fairer semi-matching for any two incomparable
ones. The proof is done in two steps. First, we show that if one semi-matching Mx
can be changed to another one My via a Bitonic Transfer Sequence (essentially
a Transfer Sequence where all IT ′s are before all DT ′s), then a fairer semi-
matching can be (straightforwardly) found. Then, we show how to construct
a Bitonic Transfer Sequence between any two incomparable semi-matchings by
modifying a Transfer Sequence between them.

The Existence of Universally Agreed Fairest Semi-matchings
535
Deﬁnition 3 (Bitonic
Transfer
Sequence).
For a Transfer Sequence
(t1, t2, · · · , ti, · · · , tn) such that Mx
t1
−→M1
t2
−→M2 · · · −→Mi−1
ti
−→Mi −→· · ·
tn
−→
My, if there exists 1 ≤k < n, such that for all ti with i ≤k are either IT
or NT, with at least one ti being IT, and for all tj with k < j ≤n are either
DT or NT, with at least one tj being DT, then (t1, t2, · · · , ti, · · · , tn) is called
a Bitonic Transfer Sequence.
Lemma 3. For a pair of semi-matchings Mx and My, if there exists a Bitonic
Transfer Sequence (t1, t2, · · · , ti, · · · , tn) such that Mx
t1
−→M1
t2
−→M2 · · · −→
Mi−1
ti
−→Mi −→· · ·
tn
−→My, then there exists a semi-matching Mk satisfying
that Mx ⪯T Mk and My ⪯T Mk.
Proof. A Bitonic Transfer Sequence (t1, t2, · · · , ti, · · · , tn) can be divided into
two Sub-Sequences Tfront = (t1, t2, · · · , ti, · · · , tk) and Tback = (tk+1, tk+2, · · · ,
tj, · · · , tn), where each ti ∈Tfront is either IT or NT, and each tj ∈Tback is
either DT or NT.
Let Mk be the semi-matching obtained by applying Tfront to Mx. Then Mk
is not less fair than Mx (Mx ⪯T Mk). Besides, it is obvious that My can be
derived by applying Tback to Mk, that is Mk
tk+1
−−−→Mk+1
tk+2
−−−→Mk+2 · · · −→
Mj−1
tj
−→Mj −→· · ·
tn
−→My. Then it meets My ⪯T Mk.
⊓⊔
Lemma 4. For a pair of incomparable semi-matchings Mx and My (Mx ≺≻T
My), there always exists a Bitonic Transfer Sequence which changes Mx to My.
Proof. Assume there exists a “Simplest” Transfer Sequence changing Mx to My
(Mx ≺≻T My), of which the source vertex of one Transfer cannot be the sink
vertex of another Transfer (otherwise, the Sequence can be further “Simpliﬁed”
by merging such two Transfers into one larger Transfer). We will prove any Sim-
plest Transfer Sequence can be re-organized to form a Bitonic Transfer Sequence
which also changes Mx to My.
For two adjacent Transfers ta and tb in a Simplest Transfer Sequence (assume
ta precedes tb) such that Mi
ta
−→Ma
tb
−→Mj, swapping the order of ta and
tb (such that Mi
tb
−→Mb
ta
−→Mj) will not change the beginning and ending
semi-matchings Mi and Mj, but will yield a diﬀerent intermediate one Mb. The
following proves that if tb is Improving Transfer before the swapping (i.e., tb is
IT in Ma
tb
−→Mj), then tb remains to be Improving Transfer after the swapping
(i.e., tb is IT in Mi
tb
−→Mb).
Suppose the source and sink vertex of ta are a and a′ respectively, and the
source and sink vertex of tb are b and b′ respectively. If tb is an Improving Transfer
before the swapping, then QMa(b) −QMa(b′) > 1. The following shows it is also
true that QMi(b) −QMi(b′) > 1, by examining how the application of ta to Mi
changes the quotas of b and b′. In a Simplest Transfer Sequence, it guarantees
that a ̸= b′ and a′ ̸= b. Thus, one of the following four cases should apply:
– Case 1: ta and tb have no common source vertex and no common sink vertex
(a ̸= b and a′ ̸= b′). The application of ta to Mi will not change the quotas

536
J. Xu et al.
of b and b′, thus QMi(b) = QMa(b), QMi(b′) = QMa(b′). It indicates QMi(b) −
QMi(b′) = QMa(b) −QMa(b′) > 1.
– Case 2: ta and tb have common source vertex but no common sink vertex
(a = b and a′ ̸= b′). The application of ta to Mi will not change the quota
of b′, but will decrease the quota of b by 1, thus QMi(b) = QMa(b) + 1,
QMi(b′) = QMa(b′). It indicates QMi(b)−QMi(b′) = QMa(b)−QMa(b′)+1 > 1.
– Case 3: ta and tb have no common source vertex but common sink vertex
(a ̸= b and a′ = b′). The application of ta to Mi will not change the quota of
b, but will increase the quota of b′ by 1, thus QMi(b) = QMa(b), QMi(b′) =
QMa(b′) −1. It indicates QMi(b) −QMi(b′) = QMa(b) −QMa(b′) + 1 > 1.
– Case 4: ta and tb have common source vertex and common sink vertex (a = b
and a′ = b′). The application of ta to Mi will decrease the quota of b by 1 and
increase the quota of b′ by 1, thus QMi(b) = QMa(b)+1, QMi(b′) = QMa(b′)−1.
It indicates QMi(b) −QMi(b′) = QMa(b) −QMa(b′) + 2 > 1.
The above-proven property of “IT conservation when swapped ahead”
ensures that it is possible to re-organize any Simplest Transfer Sequence with a
mixture of IT ′s and DT ′s into a Bitonic Transfer Sequence, by swapping all the
IT ′s to be ahead of all the DT ′s. As the swapping operations do not change the
individual Transfers, the original Simplest Transfer Sequence and the resultant
Bitonic Transfer Sequence have the same set of Transfers, thus perform the same
operation of changing Mx to My.
In fact, for any pair of semi-matchings, one can always ﬁnd a Simplest Trans-
fer Sequence and a sequence of Cyclic Neutral Transfers, which change one semi-
matching to another. The construction process is shown in the Appendix. For any
pair of incomparable semi-matchings Mx and My, let SSimp and Scyclic repre-
sent the Simplest Transfer Sequence and the sequence of Cyclic Neutral Transfers
respectively, the concatenation of which changes Mx to My. Obviously, SSimp
contains both IT ′s and DT ′s, and can be re-organized into a Bitonic Transfer
Sequence. Then, adding Scyclic into the front or back of this Bitonic Trans-
fer Sequence, results in yet another Bitonic Transfer Sequence changing Mx
to My.
⊓⊔
Corollary 3. Let posetG represents a partially ordered set deﬁned by “⪯T ”
which contains all semi-matchings of a bipartite Graph G. For any pair of semi-
matchings Mx, My ∈posetG, if Mx ≺≻T My, then there exists at least one
semi-matching Mz ∈posetG satisfying that Mx ⪯T Mz and My ⪯T Mz.
From Corollary 3 and the properties of a poset, Theorem 1 can be derived.
Theorem 1 (Existence of a set of fairest semi-matchings with regard
to Transfer-based Comparison for any bipartite graph). For any bipar-
tite graph G, let posetG represents the partially ordered set deﬁned by “⪯T ”
which contains all semi-matchings of G. There always exists a set of equally fair
semi-matchings that constitute the greatest elements in posetG, and all semi-
matchings in this set have the same sorted quotas vector.
From Theorem 1 and Corollary 2, we can furthermore derive Theorem 2.

The Existence of Universally Agreed Fairest Semi-matchings
537
Theorem 2. For any bipartite graph, there always exists a set of equally fair
semi-matchings (with the same sorted quotas vector), which are uniformly
considered the fairest by all the fairness measures which are compatible with
Majorization.
5
Discussions
A. The poset from Transfer-based Comparison might be a semi-lattice
For the poset derived from Transfer-based comparison, we did not prove it to
be a join-semilattice deﬁned as the poset which has a least upper bound for any
nonempty ﬁnite subset [4]. In other words, we proved that the poset itself has
a greatest element which is the least upper bound, but this might not be true
for all of its subsets. There might exist a counter example, such as a pair of
semi-matchings M1, M2 having three common upper bounds M3, M4, M5 which
meet M3 ≺≻T M4 and M3 ≺T M5, M4 ≺T M5. That being said, we also did not
ﬁnd such counter examples for any given bipartite graph setting. Consequently,
whether the poset is a semi-lattice remains a hypothesis.
B. The existence of the least elements with regard to Transfer-based
Comparison is not always true
This paper has proved the existence of the greatest elements in the partially
ordered set with regard to Transfer-based Comparison. However, the least ele-
ments in this partially ordered set do not always exist. This means there does
not exist a uniformly agreed upon “most unfair” semi-matching set among all
the fairness measures. For example, in Fig. 2, the two worst semi-matchings M1
and M2 are incomparable with regard to Transfer-based Comparison. Fairness
measures disagree on the comparison of their quotas vectors QM1 = (4, 1, 1) and
QM2 = (0, 3, 3). The details of conﬂicting comparisons between QM1 and QM2
have been shown in Table 1.
M1
v3
v2
Bipartite Graph
u6
v1
u2
u3
u4
u5
u1
v3
v2
u6
v1
u2
u3
u4
u5
u1
QM2 = (0, 3, 3)
v3
v2
u6
v1
u2
u3
u4
u5
u1
QM1 = (4, 1, 1)
M1
T M2
M2
Fig. 2. An example of two incomparable semi-matchings both as the most unfair ones
regarding “⪯T ”.

538
J. Xu et al.
The insight behind the existence of the greatest elements but not the least
elements in this partially ordered set, is in the asymmetry of “swapping IT”
ahead: we have proved that an IT can be swapped ahead without losing its
attribute of being an Improving Transfer, but this is not true for a DT. A
Deteriorating Transfer, when swapped ahead, does not always maintain to be
a DT. As a result, for any two incomparable semi-matchings Mx and My, it is
guaranteed that there always exists a fairer one Mz that Mx ⪯T Mz, Mx ⪯T Mz,
but not necessarily an “unfairer” one Mz′ that Mz′ ⪯T Mx, Mz′ ⪯T My.
C. Algorithms to achieve the fairest semi-matching in applications
The problem addressed in this paper has many applications such as load balanc-
ing, fair allocation, especially for their online scenario. Kleinberg et al. [7] studied
on the load balancing problem which is concerned with assigning uniform jobs
J to machines M. For each job Ji, there is a set Si ⊆M on which job Ji can
run. The aim is to assign each job Ji to a machine M ∈Si in a way that the
assignment has the fairest loads among all machines. An iterative network ﬂow
algorithm was proposed by Kleinberg et al. [7] to achieve an assignment of jobs
to machines, which is the fairest with regard to lexicographically minimum fair-
ness. This algorithm builds the assignment step by step starting with an empty
assignment. In each step, it applies a max-ﬂow algorithm to ﬁnd a “partial”
assignment, until eventually a legitimate assignment is reached which will turn
out to be the fairest under lexicographical measure. However, this algorithm
cannot be applied in the scenario of having to start from an existing assignment,
and iteratively improving to achieve a fairest one. This limits its applicability
in the online scenario, where a new assignment is always based on modifying an
existing one.
Based on the conclusions achieved from this paper, two algorithms can be
derived to achieve the fairest assignment, for the above job assignment system
described in [7]. One algorithm is to iteratively improve an arbitrary assign-
ment, according to the Transfer-based Comparison, to eventually achieve one
of the fairest assignments. Starting from an arbitrary assignment, by iteratively
applying an Improving Transfer until no more found, one fairest assignment will
be achieved. The result is guaranteed by choosing any Improving Transfer, not
necessarily the “best” one. This algorithm is similar to the one proposed by
Harvey et al. [5] to achieve the assignment with minimal makespan and min-
imal ﬂow time. The upper bound of the runtime complexity is shown to be
O(min{|U|3/2, |U||V |} · |E|) for a bipartite graph G = (U ∪V, E).
Alternatively, an online algorithm can be derived to maintain an online job
assignment system to be fairest. Assume initially a job assignment system is the
fairest, and new jobs are coming to this system sequentially, which needs to be
assigned to some machines. For each incoming job Ji, it will ﬁrst be assigned
to the least loaded machine M ∗which can run job Ji. Subsequently, a search is
conducted for an Improving Transfer t, starting from M ∗as a source vertex. We
claim that, if t is not found, the system is then already the fairest; if t is found,
then by applying t which is a chain of jobs re-assignments to the system, the

The Existence of Universally Agreed Fairest Semi-matchings
539
system can be successfully updated to be a fairest one with the new job. This
claim can be easily proved based on the conclusions achieved from this paper.
6
Conclusion
This paper proves there always exist the universally agreed fairest semi-
matchings in any given bipartite graph. To prove this, we deﬁne a poset of
semi-matchings based on the way to transfer one into another. This poset
(Transfer-based Comparison) is shown to always have the greatest elements, as
the fairest semi-matchings. Subsequently, we show that the proposed Transfer-
based Comparison can strictly imply the Majorization order. In other words,
the fairest semi-matchings under the proposed Transfer-based Comparison are
always regarded as fairest under Majorization. To our best knowledge, all exist-
ing fairness measures in the literature are compatible with Majorization. In con-
clusion, for any bipartite graph, there always exists a set of equally fair semi-
matchings, which are universally regarded as the fairest ones, by all existing
fairness measures, even though they may disagree on the comparisons among
the ones that are not the fairest.
Acknowledgments. We are grateful to Kai Da Zhao, Professor Miloˇs ˇZefran, and
Professor Bhaskar DasGupta for their helpful insights and discussion.
Appendix
Claim. For any pair of semi-matchings Mx and My, there always exists a
sequence containing of a Simplest Transfer Sequence and a sequence of Cyclic
Neutral Transfer, which changes Mx to My.
Proof. The notion Mx ⊕My denotes the symmetric diﬀerence of edges set Mx
and My, that is, Mx ⊕My = (Mx \ My) ∪(My \ Mx). Let S represents the
set of all Cyclic Neutral Transfers in Mx ⊕My. Suppose Mx′ can be derived by
applying all Cyclic Neutral Transfers of S to Mx.
We assume all the edges of Mx′ \ My are colored by green, and all the edges
of My \ Mx′ are colored by red. An observation on Mx′ ⊕My is that there exist
one or more V -vertices which are endpoints of only green edges, but not red
edges. We call those V -vertices as Starting Vertices. We build a Transfer which
is an alternating green-red sequence of edges, as follows. (1) Find a green edge of
which the V -endpoint is one arbitrary Starting Vertex, and set its U-endpoint as
Current Vertex. Then repeat the following two steps. (2) Find a red edge of which
the U-endpoint is Current Vertex. Set its V -endpoint as Current Vertex. (3) Find
a green edge of which the V -endpoint is Current Vertex. Set its U-endpoint as
Current Vertex. Continue until we cannot ﬁnd any green edges. Delete all chosen
edges from Mx′ ⊕My, and then repeat above procedures to build more Transfers
until Mx′ ⊕My becomes empty.

540
J. Xu et al.
Throughout this process, we maintain that among all the obtained Transfers,
the source vertex of one Transfer cannot be the sink vertex of another Transfer.
Then, an arbitrarily ordered sequence of all the obtained Transfers constructs
a Simplest Transfer Sequence from Mx′ to My. An illustration of the Transfers
Construction is shown in Fig. 3.
⊓⊔
Fig. 3. An illustration of Transfers Construction.
References
1. Atkinson, A.B.: On the measurement of inequality. J. Econ. Theory 2(3), 244–263
(1970)
2. Bansal, N., Sviridenko, M.: The santa claus problem. In: Proceedings of the Thirty-
Eighth Annual ACM Symposium on Theory of Computing, pp. 31–40. ACM (2006)
3. Bokal, D., Breˇsar, B., Jerebic, J.: A generalization of hungarian method and hall’s
theorem with applications in wireless sensor networks. Discret. Appl. Math. 160(4),
460–470 (2012)
4. Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order. Cambridge
University Press, Cambridge (2002)
5. Harvey, N.J., Ladner, R.E., Lov´asz, L., Tamir, T.: Semi-matchings for bipartite
graphs and load balancing. J. Algorithms 59(1), 53–78 (2006)
6. Jain, R., Chiu, D.M., Hawe, W.R.: A Quantitative Measure of Fairness and Dis-
crimination for Resource Allocation in Shared Computer System, vol. 38. Eastern
Research Laboratory, Digital Equipment Corporation Hudson, MA (1984)
7. Kleinberg, J., Rabani, Y., Tardos, ´E.: Fairness in routing and load balancing. In:
40th Annual Symposium on FOCS, 1999, pp. 568–578. IEEE (1999)

The Existence of Universally Agreed Fairest Semi-matchings
541
8. Lan, T., Kao, D., Chiang, M., Sabharwal, A.: An axiomatic theory of fairness in
network resource allocation. IEEE (2010)
9. Mo, J., Walrand, J.: Fair end-to-end window-based congestion control. IEEE/ACM
Trans. Netw. (ToN) 8(5), 556–567 (2000)
10. Olkin, I., Marshall, A.W.: Inequalities: Theory of Majorization and Its Applica-
tions, vol. 143. Academic Press, New York (2016)
11. Tse, D., Viswanath, P.: Fundamentals of Wireless Communication. Cambridge
University Press, New York (2005)

Better Inapproximability Bounds
and Approximation Algorithms for Min-Max
Tree/Cycle/Path Cover Problems
Wei Yu and Zhaohui Liu(B)
Department of Mathematics, East China University of Science and Technology,
Shanghai 200237, China
{yuwei,zhliu}@ecust.edu.cn
Abstract. We study the problem of covering the vertices of an undi-
rected weighted graph with a given number of trees (cycles, paths) to
minimize the weight of the maximum weight tree (cycle, path). Improved
inapproximability lower bounds are proved and better approximation
algorithms are designed for several variants of this problem.
Keywords: Approximation hardness · Approximation algorithm · Tree
cover · Cycle cover · Path cover · Traveling salesman problem
1
Introduction
Given an undirected graph with nonnegative edge weights and a positive integer
k, the objective is to ﬁnd k trees (cycles, paths) to cover all the vertices such
that the weight of the maximum weight tree (cycle, path) is minimized. This
fundamental optimization problem, known as Min-Max Tree/Cycle/Path Cover
Problem, and its variants have attracted considerable research attention in recent
decades. This is mainly due to its wide range of application in both operations
research and computer science, such as mail and newspaper delivery [10], nurse
station location [8], disaster relief eﬀorts routing [5], data gathering and wireless
recharging in wireless sensor networks [22], multi-vehicle scheduling problem
[4,15], political districting [14], and so on. For more practical examples that can
be modeled by min-max tree/cycle/path cover problems, we refer to [1,21,22,
24,25].
Given the NP-hardness of the min-max tree/cycle/path cover problem [8],
most researchers have concentrated on designing approximation algorithms that
generate near-optimal solutions in polynomial time as well as on deducing inap-
proximability lower bounds.
1.1
Previous Results
For the Min-Max Tree Cover Problem (MMTCP), Even et al. [8] and Arkin
et al. [1] devised independently 4-approximation algorithms, which was improved
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 542–554, 2017.
DOI: 10.1007/978-3-319-62389-4 45

Better Inapproximability Bounds and Approximation Algorithms
543
to a 3-approximation algorithm by Khani and Salavatipour [16]. The above-
mentioned 4-approximation algorithm by Arkin et al. [1] is actually developed
for the Min-Max Path Cover Problem (MMPCP), which is still the best available
algorithm. For the Min-Max Cycle Cover Problem (MMCCP), Xu et al. [25] gave
a 6-approximation algorithm, which can also be achieved by a combination of the
3-approximation algorithm for MMTCP with standard edge-doubling strategy.
Xu et al. [22] and Jorati [13] proposed independently 16/3-approximation algo-
rithms. Recently, Yu, Liu [26] developed a min{4ρ, 5}-approximation algorithm
provided an algorithm for TSP with approximation ratio ρ is available.
In Rooted MMTCP/MMCCP/MMPCP, which is an important variant
treated in the literature, a depot set D is prescribed and the aim is to cover all
the vertices not in D with at most k trees/cycles/paths each of which contains
at least one vertex from D. If each vertex in D has a unit-capacity, i.e., it can be
included in at most one tree/cycle/path, we obtain a capacitated rooted problem.
Otherwise, if each vertex in D is allowed to be present in an arbitrary number
of trees/cycles/paths, we have an uncapacitated rooted problem. As shown in
[22,24,26], an (α + 1)-approximation algorithm for the uncapacitated rooted
problem can be derived from an α-approximation algorithm for the correspond-
ing unrooted problem. For Single-Depot Rooted MMTCP/MMCCP/MMPCP,
i.e., |D| = 1, better algorithms with approximation ratios 3, ρ+1, 3 were obtained
by Nagamochi [18], Frederickson et al. [10], Xu et al. [24], respectively (ρ deﬁned
as before).
For Capacitated Rooted MMCCP, Xu et al. [25] developed the ﬁrst approx-
imation algorithm with ratio 13 and obtained a 7-approximation algorithm for
the case |D| = k. The latter result was shown independently by Jorati [13].
Subsequently, Xu et al. [22] improved the ratio to 7 for the general problem.
For Capacitated Rooted MMTCP with |D| = k, Even et al. [8] provided a 4-
approximation algorithm.
In addition, the above results can be further improved if either the graph has
a special structure, such as a tree [19], is embedded into a Euclidean space [14]
or if k is ﬁxed [9].
As for the approximation hardness, Xu, Wen [23] proved an inapproxima-
bility lower bound of 3/2 for both MMTCP and MMPCP, which applies to
the capacitated rooted problems even with the restriction |D| = k. Xu et al. [24]
obtained the same result for Uncapacitated Rooted MMTCP/MMPCP. Xu et al.
[25] gave a lower bound of 4/3 for both MMCCP and its rooted version. Xu,
Wen [23] also showed a lower bound of 10/9 (20/17) for Single-Depot Rooted
MMTCP (MMCCP), complemented by a lower bound of 4/3 for Single-Depot
Rooted MMPCP in [24].
1.2
Our Results
We obtain both improved approximation algorithms and better inapproxima-
bility lower bounds for several variants of the min-max tree/cycle/path cover
problem. Firstly, we devise a 6-approximation algorithm for Capacitated Rooted
MMCCP, improving on the algorithm in [22] with an approximation ratio of 7.

544
W. Yu and Z. Liu
Second, we propose the ﬁrst approximation algorithms for Capacitated Rooted
MMTCP and Capacitated Rooted MMPCP, both of which have a constant ratio
of 7. Third, we provide further improvement on the approximation algorithms
for some interesting special cases, i.e., planar problem, graphic problem (see
Sect. 2 for deﬁnitions). Last, we prove better inapproximability lower bounds
for (Rooted) MMCCP, Single-Depot Rooted MMCCP, Single-Depot Rooted
MMTCP, as illustrated in Table 1, and show that the reductions used to prove
the lower bounds are essentially best possible.
Table 1. Previous and new lower bounds on min-max tree/cycle/path cover problems
Unrooted
Uncapacitated rooted
Capacitated rooted
|D| = 1
|D| = k
|D| = k
MMTCP 4/3
10/9 4/3 (Theorem 2) 3/2
3/2
MMCCP 4/3
20/17
4/3
4/3
3/2 (Theorem 1) 5/4 (Theorem 3)
3/2 (Theorem 1)
3/2 (Theorem 1)
MMPCP
3/2
4/3
3/2
3/2
The remainder of the paper is organized as follows. We explain some nota-
tions and formally describe the problems in Sect. 2. The inapproximablility lower
bounds are proved in Sect. 3. This is followed by approximation algorithms for
the capacitated rooted min-max tree/cycle/path cover problem in Sect. 4.
2
Preliminaries
We denote by G = (V, E) an undirected weighted complete graph with the
vertex set V and the edge set E. Let w(e) denote the weight or length of edge
e. If e = (u, v), we also use w(u, v) to denote the weight of e. The weights
are metric, i.e., nonnegative, symmetric and satisfy the triangle inequality. For
B > 0, G[B] denotes the subgraph of G obtained by removing all the edges in E
with weights greater than B. For a subgraph H (e.g. tree, cycle, path) of G, let
V (H), E(H) denote the vertex set and edge set of H, respectively. The weight of
H is deﬁned as w(H) = 
e∈E(H) w(e). For u ∈V , d(u, H) = minv∈V (H) w(u, v)
denotes the distance from u to H. The weight of a path or cycle is also called its
length. A tree (cycle, path) containing only one vertex and no edges is a trivial
tree (cycle, path) and its weight is deﬁned as zero.
For a subset V ′ of V , a set {T1, T2, . . . , Tk} of trees (some of them may be
trivial trees) is called a tree cover of V ′ if V ′ ⊆∪k
i=1V (Ti). The cost of this
tree cover is deﬁned as max1≤i≤k w(Ti), i.e., the maximum weight of the trees.
Particularly, a tree cover of V is simply called a tree cover. By replacing trees
with cycles (paths) we can deﬁne cycle cover (path cover) and its cost similarly.
Note that a cycle containing exactly two vertices u, v consists of two copies of
edge (u, v) and has a length of 2w(u, v).

Better Inapproximability Bounds and Approximation Algorithms
545
Now we can formally state the following problems.
Deﬁnition 1. In the Min-Max Tree Cover Problem (MMTCP), we are given
an undirected complete graph G = (V, E) with a metric weight function w on E
and a positive integer k, the goal is to ﬁnd a minimum cost tree cover with at
most k trees.
Deﬁnition 2. In the Rooted Min-Max Tree Cover Problem (RMMTCP), we are
given an undirected complete graph G = (V, E) with a metric weight function w
on E, a depot set D ⊆V , and a positive integer k, the objective is to ﬁnd
a minimum cost tree cover of V \ D with at most k trees such that each tree
contains exactly one vertex in D.
Deﬁnition 3. In
the
Capacitated
Rooted
Min-Max
Tree
Cover
Problem
(CRMMTCP), we are given an undirected complete graph G = (V, E) with a
metric weight function w on E, a depot set D ⊆V , and a positive integer k, the
objective is to ﬁnd a minimum cost tree cover of V \D with at most k trees such
that each tree contains exactly one vertex in D and each vertex in D is contained
in at most one tree.
The ﬁrst problem is called unrooted problem as before, since no depot set is
given. The second problem is referred to as uncapacitated rooted problem while
the last problem is called capacitated rooted problem.
By replacing trees with cycles or paths in the above deﬁnitions we have
another six problems, which are denoted by similar acronyms. For example,
CRMMPCP stands for the Capacitated Rooted Min-Max Path Cover Problem
while RMMCCP refers to the Rooted Min-Max Cycle Cover Problem. Note that
in a rooted path cover, not only should each path contain one vertex in D but
also this vertex has to be an end vertex of the path. This is natural if one
interprets paths as the routing of a ﬂeet of k vehicles that service the customers
located at the vertices from given depots.
In our problem deﬁnitions we assume that the graphs are complete. This
involves no loss of generality since we can take the metric closure of a con-
nected graph if it is not complete but connected. For a disconnected graph, we
ﬁrst add some edges of suﬃciently large length to obtain a connected graph,
and then consider the problems deﬁned on the metric closure of this connected
graph. In MMTCP, by restricting the graph to be the metric closure of a
connected (weighted) planar graph we obtain (Weighted) Planar MMTCP. In
Graphic MMTCP, the graph is required to be the metric closure of a connected
unweighted graph. The same special case of the min-max cycle/path cover prob-
lems is deﬁned similarly. We also suppose w.l.o.g. that the weights of the edges
are integers.
Given an instance of some min-max tree/cycle/path cover problem, OPT
indicates its optimal value as well as the corresponding optimal solution. Each
tree (cycle, path) in OPT is called an optimum tree (cycle, path). By the triangle

546
W. Yu and Z. Liu
inequality, except for the uncapacitated rooted problem, we can assume w.l.o.g
that any two optimum cycles (paths) are vertex-disjoint, since one can eliminate
the repeated vertices without increasing the cost. However, the optimum trees
are not necessarily vertex-disjoint since there may exist vertices of degree greater
than 2 and eliminating these vertices may increase the cost.
3
Inapproximability Lower Bounds
In this section we show that even Planar Min-Max Tree/Cycle/Path Cover Prob-
lems are hard to approximate and give improved lower bounds for some single-
depot, i.e., |D| = 1, rooted problems. However, Weighted Planar TSP, the special
case of Weighted Planar MMCCP with k = 1, admits PTAS (see [3,17]).
Our results are based on reductions from a restriction of the following well-
known NP-Complete problem (see [11]).
3-Dimensional Matching Problem (3DM). Given disjoint sets X, Y, Z that
have the same number q of elements and a set M ⊆X × Y × Z of triples with
|M| = m, is there an exact matching M′ ⊆M such that |M′| = q and any two
triples in M′ do not have any common coordinate?
Associated with an instance of 3DM there is a bipartite graph B = (W ∪
(X ∪Y ∪Z), E), where each element of W represents a triple of M. There is
an edge connecting w ∈W to v ∈X ∪Y ∪Z if and only if v is a coordinate of
the triple represented by w. By restricting the associated bipartite graph to be
planar we obtain Planar 3-Dimensional Matching Problem (Planar 3DM), which
is still NP-Complete (see [7]).
Xu and Wen [23], Xu et al. [24], Xu et al. [25] derived the lower bounds
for min-max tree/cycle/path cover problems summarized in Table 1. All these
bounds are deduced by gap-reductions from 3DM. In what follows, we show that
the a lower bound of 3/2 holds even for the planar version of all the problems
in Table 1 except for the single-depot rooted problems. First, we observe that
restricting the planar bipartite graph to be connected in Planar 3DM preserves
NP-Completeness. This special case is called Connected Planar 3DM.
Lemma 1. Connected Planar 3DM is NP-Complete.
Now we are ready to show the inapproximablity lower bounds.
Theorem 1. It is NP-hard to approximate the planar version of the min-max
tree/cycle/path cover problem and its rooted version within a performance ratio
less than 3/2.
Remark 1. Replacing 3DM with Planar 3DM in the reductions of Xu, Wen [23]
and redeﬁning the edge length as the shortest path length are not suﬃcient
to obtain our results. In this case we may derive a disconnected planar graph.
To make it connected we have to add some long edges. As a result, we have
a connected weighted planar graph, instead of a connected unweighted planar
graph.

Better Inapproximability Bounds and Approximation Algorithms
547
In Single-Depot RMMTCP/RMMCCP/RMMPCP, the depot set D contains
a single vertex. Xu, Xu, Li [24] derived a lower bound of 4/3 for Graphic Single-
Depot RMMPCP using the following reduction. Given an instance I of 3DM,
each triple w = (x, y, z) ∈M corresponds to a subgraph of G as illustrated in
Fig. 1. They proved that I has an exact matching if and only if the metric closure
of G has a path cover with k = q + 2m paths of cost at most 3. We note that
the same reduction also implies a lower bound of 4/3 for Graphic Single-Depot
RMMTCP. A tree cover of the metric closure of G with k = q + 2m trees of cost
at most 3 has to be a path cover with q + 2m paths of cost at most 3.
Theorem 2. It is NP-hard to approximate Graphic Single-Depot RMMTCP
within a performance ratio less than 4/3.
-
@
@
@
@
s
s
s
s
@
@
@
@
@
@
@
@

s
s
s
s
s
s
s
s
s
s
e
x
y
z
w
x
y
z
Fig. 1. The constructions in proof of Theorem 2 (The circled vertex is the only depot.)
For the single-depot cycle cover problem we have the following result.
Theorem 3. It is NP-hard to approximate Graphic Single-Depot RMMCCP
within a performance ratio less than 5/4.
We end this section by showing that the results in Theorems 1, 2 and 3 are
the best possible if we are conﬁned to give a gap-reduction from an NP-Complete
problem to derive inapproximability lower bounds for graphic tree/cycle/path
cover problems.
Lemma 2. There is a polynomial time algorithm to decide if (i) the optimal
value of an instance of Graphic (Rooted) MMTCP/MMPCP equals 1; (ii) the
optimum value of an instance of Graphic (Rooted) MMCCP equals 2.
Lemma 3. There is a polynomial time algorithm to decide if (i) the opti-
mal value of an instance of Graphic Single-Depot RMMTCP/RMMPCP is not
greater than 2; (ii) the optimal value of an instance of Graphic Single-Depot
RMMCCP is not greater than 3.

548
W. Yu and Z. Liu
4
Approximation Algorithms
In this section we show how to obtain approximation algorithms for Capacitated
Rooted Min-Max Tree/Cycle/Path Cover Problem by using the approximation
algorithm for the unrooted problem.
To design an approximation algorithm for CRMMTCP, we adopt a two-stage
process. The ﬁrst is to obtain an α-subroutine deﬁned as below.
Deﬁnition 4. A polynomial time algorithm is called an α-subroutine for
CRMMTCP, if for any instance consisting of G = (V, E) and k > 0, and any
nonnegative integer λ ≤
e∈E w(e), the algorithm always returns a feasible tree
cover of cost at most αλ, as long as OPT ≤λ.
Given an instance of CRMMTCP consisting of G = (V, E) and k > 0, clearly
0 ≤OPT ≤
e∈E w(e). We guess an objective value λ ∈[0, 
e∈E w(e)] and
run the α-subroutine. If no feasible tree cover is returned, we have OPT > λ
by the above deﬁnition. So we increase the value of λ and run the subroutine
again. If the subroutine does return a feasible tree cover of cost at most αλ,
we decrease the value of λ and run the subroutine again. Finally, we ﬁnd the
minimum value λ∗such that a feasible tree cover of cost no more than αλ∗is
returned by the subroutine. It follows that OPT ≥λ∗since the subroutine does
not return a feasible tree cover for λ∗−1. Therefore, the tree cover returned
by the α-subroutine for λ∗is an α-approximate solution. By a binary search for
λ∗, this procedure uses the α-subroutine at most log 
e∈E w(e) times and hence
runs in polynomial time.
One
can
see
that
the
above
procedure
applies
to
other
min-max
tree/cycle/path cover problems.
Lemma 4. A T(n) time α-subroutine for MMTCP (MMCCP, MMPCP) or its
rooted version implies an α-approximation algorithm for the same problem that
runs in O(T(n) log 
e∈E w(e)) time.
4.1
Cycle Cover
Suppose there is a ρc-subroutine for MMCCP. We give the following algorithm
for CRMMCCP. The input is an instance of CRMMCCP consisting of G = (V, E)
and k > 0 and a value λ ∈[0, 
e∈E w(e)].
Algorithm CRMMCCP(ρc)
Step 1. Delete all the edges with weight greater than λ
2 in G. If G[ λ
2 ] has some
connected component containing no vertices in D, stop and return failure.
Otherwise, remove the connected components containing no vertices in V \D
and let the remaining p connected components be F1, F2, . . . , Fp.
Step 2. For each i = 1, 2, . . . , p,

Better Inapproximability Bounds and Approximation Algorithms
549
Step 2.1. Let Di = D ∩V (Fi) = {r1, r2, . . . , r|Di|} and ﬁnd the minimum
integer ki with 1 ≤ki ≤|Di| such that the ρc-subroutine for MMCCP
outputs a cycle cover Ci = {Ci,1, Ci,2, . . . , Ci,ki} of V (Fi) \ Di of cost
at most ρcλ, while using λ, the complete subgraph Gi of G induced by
V (Fi) \ Di and ki as inputs.
Step 2.2. Construct a bipartite graph Hi = (U ∪Di, E′
i), where U =
{t1, . . . , tki} represents the ki cycles in Ci (tj corresponds to Ci,j) and
(tj, rl) ∈E′
i if and only if d(rl, Ci,j) ≤λ
2 . Find a maximum matching Mi
on Hi.
Step 2.3. If Mi matches all the vertices in U, i.e., |Mi| = ki, then we can
obtain a feasible cycle cover of V (Fi) \ Di as follows. For (tj, rl) ∈Mi we
add two copies of the edge in the original graph G that corresponds to
d(rl, Ci,j) and make a shortcut to derive a cycle C′
i,j on V (Ci,j) ∪{rl}.
Step 2.4. If Mi does not match all the vertices in U, i.e., |Mi| < ki, choose
any unmatched vertex u ∈U and determine the set S of vertices that can
be reached by an alternating path starting from u. Let U ′ = S ∩U and
D′ = S∩Di. Run the ρc-subroutine for λ and the instance consisting of the
complete subgraph induced by ∪tj∈U ′V (Ci,j) and |D′| to obtain a cycle
cover of ∪tj∈U ′V (Ci,j) of cost at most ρcλ. Update Ci by replacing the
cycles corresponding to the vertices in U ′ by the newly generated cycles.
For simplicity the cycles in Ci are still denoted by Ci,1, Ci,2, . . . , Ci,ki−1.
Set ki := ki −1 and go to Step 2.2.
Step 3. If p
i=1 ki
≤
k, return the cycle cover C′
1,1, . . . , C′
1,k1, . . . . . . ,
C′
p,1, . . . , C′
p,kp of V \ D; otherwise, return failure.
Let C∗
1, C∗
2, . . . , C∗
k be all the vertex-disjoint optimum cycles. By the triangle
inequality, it is easy to verify that
Observation 1. If OPT ≤λ, then w(e) ≤OP T
2
≤λ
2 for each e ∈∪k
i=1E(C∗
i ).
By this observation the vertex set of each optimum cycle is contained entirely
in exactly one of V (F1), V (F2), . . . , V (Fp). As a consequence, the optimum cycles
whose vertex sets are contained in V (Fi) constitute a cycle cover of V (Fi) \ Di.
Moreover, the cost of this cycle cover of V (Fi) \ Di is at most OPT since the
length of each optimum cycle is no more than OPT. Therefore, we have
Observation 2. If OPT ≤λ, the optimum cycles can be partitioned into p
groups such that the ith (i = 1, 2, . . . , p) group consisting of k∗
i ≥1 optimum
cycles is a cycle cover of V (Fi) \ Di with cost at most λ.
The performance of Algorithm CRMMCCP(ρc) can be derived by the fol-
lowing lemma.

550
W. Yu and Z. Liu
Lemma 5. If OPT ≤λ, then
(i) Any connected component of G[ λ
2 ] contains at least one vertex in D;
(ii) For each i = 1, 2, . . . , p, Step 2.1. returns a cycle cover of V (Fi)\Di of cost
at most ρcλ and ki ≤k∗
i ;
(iii) For each i = 1, 2, . . . , p, whenever Step 2.4. is executed for |Mi| < ki it
holds that |D′| = |U ′| −1 ≥1 and the ρc-subroutine returns a cycle cover
of ∪tj∈U ′V (Ci,j) of cost at most ρcλ;
(iv) Algorithm CRMMCCP(ρc) returns a feasible cycle cover of cost at most
(ρc + 1)λ.
This lemma shows that Algorithm CRMMCCP(ρc) is a (ρc +1)-subroutine
for CRMMCCP. Combining this with Lemma 4, we have the following result.
Theorem 4. Given a ρc-subroutine for MMCCP, there is a (ρc + 1)-
approximation algorithm for CRMMCCP.
In [26], we have obtained a 5-subroutine for MMCCP running in O(n3) time
for instances with n-vertex graphs. Plugging in this subroutine and using the
O(n2.5) time matching algorithm of Hopcroft, Karp [12] for n-vertex bipartite
graphs we deduce that the Algorithm CRMMCCP(ρc) has a time complexity
of O(n3k) since the possible running of k subroutines dominates the complexity.
Therefore we conclude that
Theorem 5. There is an O(n3k log 
e∈E w(e)) time 6-approximation algo-
rithm for CRMMCCP.
Given a ρ-approximation algorithm for TSP, we also devised a 4ρ-subroutine
for MMCCP in [26] that runs in polynomial time. Since Weighted Planar TSP
admits a PTAS [3,17], we have a (4+ϵ)-subroutine for Weighted Planar MMCCP.
For Graphic MMCCP, using the fact that a set of k connected subgraphs can be
turned into a single connected subgraphs by adding k −1 unit-length edge one
can show that the same subroutine in [26] always returns a cycle cover of cost at
most 2ρc(λ + 2) whenever OPT ≤λ. By Lemma 2, we can assume λ ≥3. This
implies 2ρ(λ + 2) ≤10
3 ρλ and we have a 10
3 ρc-subroutine for Graphic MMCCP.
Using Theorem 4, we have improved the results for weighted planar or graphic
cycle cover problem.
Corollary 1. For any ϵ > 0, there is a (5 + ϵ)-approximation algorithm for
Weighted Planar CRMMCCP and a ( 13
3 +ϵ)-approximation algorithm for Planar
CRMMCCP.
4.2
Path Cover
The framework to design approximation algorithms for CRMMPCP is similar to
the cycle cover problem. However, in Step 1., we delete edges of length greater
than λ instead of λ
2 . And in other steps, we construct a path (cover) instead of
a cycle (cover). Given a ρp-subroutine for MMPCP, the algorithm accepts an
instance of CRMMPCP consisting of the graph G = (V, E) and k > 0 and a
value λ ∈[0, 
e∈E w(e)] as inputs.

Better Inapproximability Bounds and Approximation Algorithms
551
Algorithm CRMMPCP(ρp)
Step 1. Delete all the edges with weight greater than λ in G. If G[λ] has some
connected component containing no vertices in D, stop and return failure.
Otherwise, remove the connected components containing no vertices in V \D
and the remaining p connected components are F1, F2, . . . , Fp.
Step 2. For each i = 1, 2, . . . , p,
Step 2.1. Let Di = V ∩V (Fi) = {r1, r2, . . . , r|Di|} and ﬁnd the minimum
integer ki with 1 ≤ki ≤|Di| such that the ρp-subroutine for MMPCP
outputs a path cover Pi = {Pi,1, Pi,2, . . . , Pi,ki} of V (Fi) \ Di of cost at
most ρpλ using λ, the complete subgraph Gi of G induced by V (Fi) \ Di
and ki as inputs.
Step 2.2. Construct a bipartite graph Hi = (U ∪Di, E′
i), where U =
{t1, . . . , tki} represents the ki paths in Pi (tj corresponds to Pi,j) and
(tj, rl) ∈E′
i if and only if d(rl, Pi,j) ≤λ. Find a maximum matching Mi
on Hi.
Step 2.3. If Mi matches all the vertices in U, i.e., |Mi| = ki, then we can
obtain a feasible path cover of V (Fi)\Di as follows. For each (tj, rl) ∈Mi,
let (uj, rl) be the corresponding edge in the original graph. Denote by
vj, wj the two end vertices of Pi,j such that w(uj, vj) ≤w(uj, wj). After
adding edge (uj, rl) to connect rl with Pi,j and edge (uj, vj), we can
derive an Eulerian path from rl to wj that goes through {rl} ∪V (Pi,j).
By shortcutting this path, we obtain a path with one end vertex rl ∈D
covering all the vertices in V (Pi,j).
Step 2.4. If Mi does not match all the vertices in U, i.e., |Mi| < ki, choose
any unmatched vertex u ∈U and determine the set S of vertices in Hi that
can be reached by an alternating path starting from u. Let U ′ = S∩U and
D′ = S ∩Di. Run the ρp-subroutine for λ and the instance consisting of
the complete subgraph induced by ∪tj∈U ′V (Pi,j) and |D′| to obtain a path
cover of ∪tj∈U ′V (Pi,j) of cost at most ρpλ. Update Pi by replacing the
paths corresponding to the vertices in U ′ by the newly generated paths.
For simplicity the paths in Pi are still denoted by Pi,1, Pi,2, . . . , Pi,ki−1.
Set ki := ki −1 and go to Step 2.2.
Step 3. If p
i=1 ki ≤k, return the path cover P1,1, . . . , P1,k1, . . . . . . , Pp,1 . . . Pp,kp
of V \ D; otherwise, return failure.
Let P ∗
1 , P ∗
2 , . . . , P ∗
k be all the vertex-disjoint optimum paths. We have two
observations similar to the cycle cover problem.
Observation 3. If OPT ≤λ, then w(e) ≤OPT ≤λ for each e ∈∪k
i=1E(P ∗
i ).
Observation 4. If OPT ≤λ, the optimum paths can be partitioned into p
groups such that the ith (i = 1, 2, . . . , p) group consisting of k∗
i ≥1 optimum
paths is a path cover of V (Fi) \ Di with cost at most λ.

552
W. Yu and Z. Liu
Based on these facts and an analogous proof to Lemma 5 we have
Lemma 6. If OPT ≤λ, then Algorithm CRMMCCP(ρp) returns a feasible
path cover of cost at most ( 3
2ρp + 1)λ.
By the results of Akin, Hassin, Levin [1], there is an O(n2) time ρp-subroutine
with ρp = 4. This implies
Theorem 6. There is an O(n2.5k log 
e∈E w(e)) time 7-approximation algo-
rithm for CRMMPCP.
Again one can show that for graphic instances, the subroutine in [1] returns
a path cover of cost at most 2(λ + 1) whenever OPT ≤λ. By Lemma 2, we
may assume that λ ≥2 and hence 2(λ + 1) ≤3λ. So we have a 3-subroutine for
Graphic MMPCP, which implies by Theorem 6 that
Corollary 2. There is an 4-approximation algorithm for Graphic CRMMCCP.
4.3
Tree Cover
For any V ′ ⊆V , a cycle (path) cover of cost at most λ can be transformed
into a cycle (path) cover of V ′ of cost at most λ by shortcuting the vertices of
V \ V ′, since the vertices in a cycle (path) are of degree no more than 2. This
transformation cannot be conducted directly for a tree cover since the degree of
the vertices of a tree can be greater than 2. However, by doubling the edges of a
tree cover of cost at most λ we ﬁrst obtain a cycle cover of cost at most 2λ. After
that we can shortcut the vertices of V \ V ′ and delete one edge from each cycle
to obtain a path cover of V ′ (and hence a tree cover of V ′) of cost no greater
than 2λ.
Due to this subtle diﬀerence, we obtain an algorithm for CRMMTCP
proceeding
as
follows.
First,
we
replace
paths
by
trees
in
Algorithm
CRMMPCP(ρp) and change ρp-subroutine for MMPCP to ρt-subroutine for
MMTCP. In Step 2.1. and Step 2.4. whenever running the subroutine for
MMTCP, we also replace the value λ with 2λ. As a consequence, we obtain
a tree cover of cost at most 2ρtλ. In addition, in Step 2.3. the construction
of a tree cover is simpliﬁed to connect each matched depot vertex to the cor-
responding tree by the edge in Mi. One can verify that this procedure is a
(2ρt + 1)-subroutine for CRMMTCP. Since Khani, Salavatipour [16] derived an
O(n5) time 3-subroutine for MMTCP, we have the following result.
Theorem 7. There is an O(n5k log 
e∈E w(e)) time 7-approximation algo-
rithm for CRMMTCP.
Acknowledgements. This research is supported in part by the National Natural
Science Foundation of China under grants number 11671135, 11301184.

Better Inapproximability Bounds and Approximation Algorithms
553
References
1. Arkin, E.M., Hassin, R., Levin, A.: Approximations for minimum and min-max
vehicle routing problems. J. Algorithms 59, 1–18 (2006)
2. Arora, S.: Polynomial time approximation schemes for euclidean traveling salesman
and other geometric problems. J. ACM 45, 753–782 (1998)
3. Arora, S., Grigni, M., Karger, D.R., Klein, P., Woloszyn, A.: A polynomial-time
approximation scheme for weighted planar graph TSP. In: the Proceedings of the
9th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 33–41 (1998)
4. Bhattacharya, B., Hu, Y.: Approximation algorithms for the multi-vehicle schedul-
ing problem. In: Cheong, O., Chwa, K.-Y., Park, K. (eds.) ISAAC 2010. LNCS, vol.
6507, pp. 192–205. Springer, Heidelberg (2010). doi:10.1007/978-3-642-17514-5 17
5. Campbell, A.M., Vandenbussche, D., Hermann, W.: Routing for relief eﬀorts.
Transportation Sci. 42, 127–145 (2008)
6. Christoﬁdes, N.: Worst-case analysis of a new heuristic for the traveling sales-
man problem. Technical report, Graduate School of Industrial Administration,
Carnegie-Mellon University, Pittsburgh, PA (1976)
7. Dyer, M., Frieze, A.: Planar 3DM is NP-complete. J. Algroithms 7, 174–184 (1986)
8. Even, G., Garg, N., Koemann, J., Ravi, R., Sinha, A.: Min-max tree covers of
graphs. Operations Res. Letters 32, 309–315 (2004)
9. Farbstein, B., Levin, A.: Min-max cover of a graph with a small number of parts.
Discrete Optimiz. 16, 51–61 (2015)
10. Frederickson, G.N., Hecht, M.S., Kim, C.E.: Approximation algorithms for some
routing problems. SIAM J. Comput. 7(2), 178–193 (1978)
11. Garey, M., Johnson, D.: Computers and Intractability. Freeman, San Francisco
(1979)
12. Hopcroft, J., Karp, R.: An n2.5 algorithm for maximum matchings in bipartite
graphs. SIAM J. Comput. 2, 225–231 (1973)
13. Jorati, A.: Approximation algorithms for some min-max vehicle routing problems.
Master thesis, University of Alberta (2013)
14. Karakawa, S., Morsy, E., Nagamochi, H.: Minmax tree cover in the euclidean space.
J. Graph Algorithms Appl. 15, 345–371 (2011)
15. Karuno, Y., Nagamochi, H.: 2-Approximation algorithms for the multi-vehicle
scheduling problem on a path with release and handling times. Discrete Appl.
Mathe. 129, 433–447 (2003)
16. Khani, M.R., Salavatipour, M.R.: Approximation algorithms for min-max tree
cover and bounded tree cover problems. Algorithmica 69, 443–460 (2014)
17. Klein, P.: A linear-time approximation scheme for TSP in undirected planar graphs
with edge-weights. SIAM J. Comput. 37, 1926–1952 (2008)
18. Nagamochi, H.: Approximating the minmax rooted-subtree cover problem. IEICE
Trans. Fund. Electron. E88–A, 1335–1338 (2005)
19. Nagamochi, H., Okada, K.: Polynomial time 2-approximation algorithms for the
minmax subtree cover problem. In: Ibaraki, T., Katoh, N., Ono, H. (eds.) ISAAC
2003. LNCS, vol. 2906, pp. 138–147. Springer, Heidelberg (2003). doi:10.1007/
978-3-540-24587-2 16
20. Nagamochi, H., Okada, K.: Approximating the minmax rooted-tree cover in a tree.
Inf. Process. Lett. 104, 173–178 (2007)
21. Nagarajan, V., Ravi, R.: Approximation algorithms for distance constrained vehicle
routing problems. Networks 59(2), 209–214 (2012)

554
W. Yu and Z. Liu
22. Xu, W., Liang, W., Lin, X.: Approximation algorithms for min-max cycle cover
problems. IEEE Trans. Comput. 64, 600–613 (2015)
23. Xu, Z., Wen, Q.: Approximation hardness of min-max tree covers. Oper. Res. Lett.
38, 408–416 (2010)
24. Xu, Z., Xu, L., Li, C.-L.: Approximation results for min-max path cover problems
in vehicle routing. Nav. Res. Log. 57, 728–748 (2010)
25. Xu, Z., Xu, L., Zhu, W.: Approximation results for a min-max location-routing
problem. Discrete Appl. Mathe. 160, 306–320 (2012)
26. Yu, W., Liu, Z.: Improved approximation algorithms for some min-max and mini-
mum cycle cover problems. Theor. Comput. Sci. 654, 45–58 (2016)

On the Complexity of k-Metric Antidimension
Problem and the Size of k-Antiresolving Sets
in Random Graphs
Congsong Zhang(B) and Yong Gao
Department of Computer Science, University of British Columbia Okanagan,
Kelowna, BC V1V 1V7, Canada
congsong.zhang@alumni.ubc.ca, yong.gao@ubc.ca
Abstract. Network analysis has beneﬁted greatly from published data
of social networks. However, the privacy of users may be compromised
even if the data are released after applying anonymization techniques.
To measure the resistance against privacy attacks in an anonymous net-
work, Trujillo-Rasua R. et al. introduce the concepts of k-antiresolving
set and k-metric antidimension [1]. In this paper, we prove that the
problem of k-metric antidimension is NP-hard. We also study the size
of k-antiresolving sets in random graphs. Speciﬁcally, we establish three
bounds on the size of k-antiresolving sets in Erd˝os-R´enyi random graphs.
1
Introduction
The study of privacy in social networks has attracted much recent interest [2].
A common approach to data privacy is to use anonymization techniques when
publishing social network data [3–5]. However, an adversary may compromise
the privacy of users in anonymous social networks by active and passive attacks
[6–9]. For example, Peng, W. et. al introduce a two-stage deanonymization attack
in [8]. Firstly, an adversary can register new users with connections to the tar-
geted users in a social network, and then creates edges between the newly reg-
istered users to construct a special subgraph. After that, the adversary identi-
ﬁes the subgraph in the anonymized social network that is released. Then the
adversary identiﬁes the targeted users. To measure the resistance against such
attacks, Trujillo-Rasua R. et al. introduce the following concepts: metric repre-
sentation, k-antiresolving set, k-metric antidimension, k-antiresolving basis, and
(k, l)-anonymity [1].
Let G = (V, E) be a simple connected graph, S be a subset of V , and v
be a vertex in V \ S. The metric representation of v with respect to S is a
tuple formed by the shortest-path distances from v to vertices in S. The set
S is a k-antiresolving set if k is the greatest integer such that for any vertex
v ∈V \ S there are at least k −1 diﬀerent vertices in V \ S having the same
metric representation with respect to S as v. A k-antiresolving basis is deﬁned to
be a k-antiresolving set of minimum cardinality. The k-metric antidimension is
the cardinality of a k-antiresolving basis. We denote the k-metric antidimension
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 555–567, 2017.
DOI: 10.1007/978-3-319-62389-4 46

556
C. Zhang and Y. Gao
by adimk(G). The graph G meets (k, l)-anonymity if k is the smallest positive
integer such that adimk(G) is less than or equal to l.
It has been shown that the probability of identifying an anonymous vertex not
in a k-antiresolving set S by exploiting the variation of metric representations
with respect to S is at most
1
k [1]. The adimk(G) is the lower bound on the
number of vertices controlled by an adversary to approach this probability.
In this paper, we prove that the problem of computing adimk(G) is NP-
hard1. We also establish three bounds on the size of k-antiresolving sets in the
Erd˝os-R´enyi random graphs G(n, p) with constant p.
2
Preliminary and Main Results
The following deﬁnitions of metric representation, k-antiresolving set, k-
antiresolving basis, k-metric antidimension, and (k, l)-anonymity are from [1].
Deﬁnition 1 (Metric representation). Let G = (V,E) be a simple connected
graph and dG(u, v) be the length of a shortest path between the vertices u and
v in G. For a set S = {u1, ..., ut} of vertices in V and a vertex v, we call the
t-tuple r(v|S) := (dG(v, u1), ..., dG(v, ut)) the metric representation of v with
respect to S.
Deﬁnition 2 (k-antiresolving set). Let G = (V,E) be a simple connected graph
and let S = {u1, ..., ut} be a subset of vertices of G. The set S is called a k-
antiresolving set if k is the greatest positive integer such that for every vertex
v ∈V \ S there exist at least k - 1 diﬀerent vertices v1, ..., vk−1 ∈V \ S with
r(v|S) = r(v1|S) = ... = r(vk−1|S), i.e., v and v1 , ..., vk−1 have the same
metric representation with respect to S.
Deﬁnition 3 (k-metric antidimension and k-antiresolving basis). The k-metric
antidimension of a simple connected graph G = (V,E) is the minimum car-
dinality amongst the k-antiresolving sets in G and is denoted by adimk(G). A
k-antiresolving set of cardinality adimk(G) is called a k-antiresolving basis for G.
Deﬁnition 4 ((k, l)-anonymity). A graph G meets (k,l)-anonymity with respect
to active attacks if k is the smallest positive integer such that the k-metric antidi-
mension of G is lower than or equal to l.
Below are the notations used in this paper.
1. Let G = (V, E) be a simple connected graph, S be a subset of V , and v be
a vertex in V \ S. We deﬁne NS(v) = {u|u ∈S, u and v are neighbors}. For
two vertices u, v in V \ S, we use u =S v to mean r(u|S) = r(v|S). Moreover,
we denote {v|v ∈V \ S, r(v|S) = r′} by MS[r′] for a metric representation r′
with respect to S.
1 After ﬁnishing our proof, we found that Chatterjee T. et al. had proved that the prob-
lem of computing adimk(G) is NP-hard by a diﬀerent reduction independently [10].

On the Complexity of k-Metric Antidimension Problem
557
2. Let G(n, p) be an Erd˝os-R´enyi random graph and m be an integer. We deﬁne
Γm as the family of all m-element subsets of vertices in G(n, p). Let S be a
vertex set in G(n, p). We deﬁne RS as the family of metric representations of
vertices not in S with respect to S. Moreover, we use whp as the abbreviation
of with high probability to mean that the occurrence probability of an event
tends to 1 when n tends to inﬁnity.
3. We deﬁne βu(S) = |{e|e ∈S, e = u}| where S is a tuple of integers and u is
an integer. We denote an n-tuple where all elements are valued x by (x)n.
Our main results are: (1) a reduction from the exact cover by 3-sets problem
(X3C)2 to prove that the problem of computing adimk(G) is NP-hard; (2) three
bounds on the size of k-antiresolving sets in G(n, p) with constant p. The ﬁrst
bound is an upper bound such that whp there is no k-antiresolving set where
k is constant. The second one is a lower bound such that whp there is no k-
antiresolving set where k is not constant. The last one is a lower bound such
that whp there is at least one k-antiresolving set where k is constant.
3
Computational Complexity of Computing Adimk(G)
Before proving the computational complexity of computing adimk(G), we show
two properties of k-antiresolving sets in simple connected graphs.
Proposition 1. Let S be a subset of a k-antiresolving set Sk in a simple con-
nected graph G. If there is a metric representation r with respect to S such that
0 < |MS[r]| < k, then MS[r] ⊂Sk.
By the deﬁnition of k-antiresolving set, we know that this proposition is true.
Proposition 2. Let G be a simple connected graph, S be a k-antiresolving set
in G with k ≥3, and Pn = {vp1, ..., vpn} be a path in G satisfying: (1) the degree
of the vertex vpn is equal to 1; (2) the degree of the vertex vpi is equal to 2 where
i ∈[2, n −1]. Then Pn ⊆S if any vpi ∈S where i ∈[2, n].
Proof. By the deﬁnition of k-antiresolving set, we know that for a vertex v in S
|NV \S(v)| = 0 or |NV \S(v)| ≥k. Therefore, if a vertex vpi is in S where i ∈[2, n],
its neighborhood {vpi−1, vpi+1} is also in S. By applying this observation again,
we have that the neighborhood of vpi−1 is in S if i−1 > 1, and the neighborhood
of vpi+1 is in S if i + 1 < n. By repeating this observation, we have Pn ⊆S.
⊓⊔
Theorem 1. The problem of computing adimk(G) is NP-hard.
Proof. We prove that the following decision version of the problem of computing
adimk(G) is NP-complete: given two integers k and m, is there a k-antiresolving
set of the cardinality is less than or equal to m in a simple connected graph
G = (V, E)? Given a set of vertices in V , in polynomial time, we can verify that
whether the cardinality of the set is less than or equal to m and whether the set
is a k-antiresolving set. Therefore, this decision problem belongs to the class NP.
2 X3C is a well known NP-complete problem [13].

558
C. Zhang and Y. Gao
We prove the NP-completeness by a reduction from the X3C problem : given
a set B = {e1, ..., e3q} and a family S = {S1, ..., Sp} of 3-element subsets of
B, does S contain a subfamily such that every element in B occurs in exactly
one member of the subfamily. The X3C problem is NP-complete [13]. We may
suppose that p −q ≥12. If not, we create a set B′ = {e3q+1, ..., e3q+36} and a
family S′ = {Sp+1, ..., Sp+24} of 3-element subsets of B′ such that S′ contains
an exact cover for B′. Then we get a new instance of X3C with B ∪B′ and
S ∪S′ that satisﬁes our assumption. Apparently, S ∪S′ contains an exact cover
for B ∪B′ iﬀS contains an exact cover for B. Let n be an integer such that
⌊p−q
3 ⌋< n < ⌊p−q
2 ⌋. We construct a simple connected graph G = (V, E) with
|V | = 3qn(q + 2) + p from an instance of X3C with 3q elements and p subsets as
follows. Note that n < ⌊p−q
2 ⌋. Then we can get G in polynomial time. Figure 1
shows the gadget corresponding to a subset Si = {ea, eb, ec}.
1. Vertices
(a) For each Si, we create a vertex vSi.
(b) For each ei, we create n vertices vei,1, ..., vei,n.
(c) For each vei,j, we create q + 1 vertices vpi,j,1, ..., vpi,j,q+1.
2. Edges
(a) We create edges {vSi, vSj} where i ̸= j.
(b) We create edges {vei,j, vei′,j′} where i ̸= i′ or j ̸= j′.
(c) For each vpi,j,1, ..., vpi,j,q+1, we create a path = {vpi,j,1, ..., vpi,j,q+1}.
(d) For each vei,j, we create edges {vei,j, vpi′,j′,1} where i ̸= i′ or j ̸= j′.
(e) If a subset Si = {ea, eb, ec}, we create edges {vSi, vea,j}, {vSi, veb,j}, and
{vSi, vec,j} for all j ∈[1, n].
vea,1
......
vea,n
veb,1
....
...
veb,n
vec,1......
vec,n
vSi
vpa,1,1
..
vpa,1,q+1
....
.
. vpa,n,1
..
vpa,n,q+1
vpb,1,1
..
vpb,1,q+1
....
.
.
vpb,n,1
..
vpb,n,q+1
vpc,1,1
..
vpc,1,q+1
....
.
.
vpc,n,1
..
vpc,n,q+1
Fig. 1. A gadget for a subset Si = {ea, eb, ec}

On the Complexity of k-Metric Antidimension Problem
559
Lemma 1. If X3C has an exact cover Sc, there is a (p −q)-antiresolving set S
in G such that |S| ≤q.
Proof. Let VSc and VS be the sets of vertices corresponding to the subsets in Sc
and S. As Sc is an exact cover, we know |VSc| = q and |VS \VSc| = p−q. Let v be
a vertex in VS \ VSc. Then we have r(v|VSc) = (1)q. Moreover, for a vertex vei,j,
β1(r(vei,j|VSc)) = 1 and β2(r(vei,j|VSc)) = q −1. Futhermore, there are 3n −1
diﬀerent vertices vei′,j′ such that vei′,j′ =VSc vei,j. Similarly, for a vertex vpi,j,l
we can see that r(vpi,j,l|VSc) = (l + 1)q, and there are 3qn −1 diﬀerent vertices
vpi′,j′,l where vpi′,j′,l =VSc vpi,j,l. Because 3qn > 3n > p −q, we have that VSc is
a (p −q)-antiresolving set.
⊓⊔
Lemma 2. If there is a (p −q)-antiresolving set S in G such that |S| ≤q, X3C
has an exact cover.
Proof. We show that there are no vertices vei,j or vpi,j,l in S. As the length of a
path {vpi,j,1, ..., vpi,j,q+1} is q+1, we know that the vertex vpi,j,l where l ∈[2, q+1]
is not in S by Proposition 2. Moreover, as M{vei,j }[(q + 2)] = {vpi,j,q+1}, we get
that the vertex vei,j is not in S by Proposition 1. For vertices vpi,j,1, we consider
two cases: (1) more than two vertices vpi,j,1, vpi′,j′,1 are in S; (2) only one vertex
vpi,j,1 is in S. In Case 1, we get a contradiction that the vertex vpi,j,2 should be
in S by Proposition 1 as M{vpi,j,1,vpi′,j′,1}[(1, 3)] = {vpi,j,2}. In Case 2, because
{vpi,j,1} is not a (p−q)-antiresolving set, there is at least one vertex vSi in S. Then
we get the same contradiction by Proposition 1 as M{vpi,j,1 ,vSi }[(1, 3)] = {vpi,j,2}.
We now prove |S| ≥q. If not, more than p −q vertices vSi not in S
have r(vSi|S) = (1)|S|. Moreover, 3qn vertices vpi,j,l where l ∈[1, q + 1] have
r(vpi,j,l|S) = (l + 1)|S|. Therefore, only vertices vei,j may have metric represen-
tations with respect to S diﬀerent from (l)|S| where l ∈[1, q + 2]. For a metric
representation r with respect to S diﬀerent from (l)|S| where l ∈[1, q + 2], we
have |MS[r]| is n, 2n, or 3n. Note that ⌊p−q
3 ⌋< n < ⌊p−q
2 ⌋. Hence we get a
contradiction that S is not a (p −q)-antiresolving set.
In the following, we show β1(r(vei,j|S)) = 1 for any vertex vei,j. As
the discussion in the previous paragraph, we know that |MS[(1)q]| should be
p −q which means r(vei,j|S) ̸= (1)q. Moreover, if r(vei,j|S) ̸= (2)q, we have
β1(r(vei,j|S)) = 1. If not, there are three vertices vSa, vSb, and vSc in S such
that vei,j ∈M{vSa ,vSb ,vSc }[(1, 1, 2)]. Because S is a (p−q)-antiresolving set, there
should be 3n −1 diﬀerent vertices vei′,j′ such that vei′,j′ =S vei,j. Let Sa and Sb
be the subsets in S corresponding to vSa and vSb. By the construction of G, we
have Sa = Sb which contradicts that no subsets in S are equal. Next, we prove
r(vei,j|S) ̸= (2)q. If not, let Ve be the set of vertices vei,j which are adjacent to
at least one vertex in S. Then, for a v ∈Ve we have β1(r(v|S)) = 1, and there
are 3n −1 diﬀerent vertices v′ in Ve such that v′ =S v. As |Ve| < 3qn, we know
that at least one vertex in S is not adjacent to any vertex vei,j. Let S′ be the
3-element subset in S corresponding to the vertex. We have a contradiction that
S′ = ∅.

560
C. Zhang and Y. Gao
We have proved that for any vertex vei,j β1(r(vei,j|S)) = 1, and there are
3n −1 diﬀerent vertices vei′,j′ where vei′,j′ =S vei,j. Let S′ be the subfamily of
3-element subsets in S corresponding to the vertices in S. Then we know that
S′ is an exact cover for B.
⊓⊔
Lemmas 1 and 2 complete the reduction from the X3C problem to the deci-
sion version of the problem of computing adimk(G). Therefore the problem of
computing adimk(G) is NP-hard.
⊓⊔
4
Antiresolving Sets in Random Graphs
Since the problem of computing adimk(G) is NP-hard, the (k, l)-anonymity prob-
lem is also NP-hard. To understand the relationship between the two parameters
k and l, we study the size of k-antiresolving sets in random graphs which, we
hope, will help characterize the trade-oﬀbetween the level of anonymity and
the cost of achieving such level of anonymity. For constant k, we establish three
bounds on the size of k-antiresolving sets in G(n, p) where the edge probability
p is constant.
As shortest-path distances between diﬀerent pairs of vertices are not mutu-
ally independent in G(n, p), the analysis of the size of k-antiresolving sets is
much more diﬃcult than the analysis of the size of cliques and independent sets
studied in the literature [12]. To overcome this diﬃculty due to the correlation
among distances, we introduce the concept of a relaxed metric representation
and prove bounds on the size of k-antiresolving sets under the relaxed metric
representation. These bounds are then converted to the bounds on the size of
k-antiresolving sets under the standard metric representation by taking into the
consideration of an observation on the diameter of the random graph G(n, p).
In the relaxed metric representation, we use the relaxed shortest-path dis-
tance instead of the shortest-path distance. Given two vertices vi, vj in G(n, p),
we deﬁne the relaxed shortest-path distance dij as
dij =

1 : vi and vj are adjacent,
∗: otherwise.
For a set S of vertices and a vertex v /∈S, we denote the relaxed metric
representation of v with respect to S by r∗(v|S), and denote the family of relaxed
metric representations with respect to S by R∗
S. Let
Cα = min
α2
2 , (1 + α) ln(1 + α) −α

and ϵ = ln
 (2 + β)
Cα ln( 1
pm ) ln2(n)

· [ln(n)]−1
where α, β are arbitrary small constants, and pm = min(p, 1 −p). We have
the following theorem.
Theorem 2. Given a random graph G(n, p) where p is constant, whp there is
no k-antiresolving set S such that k is constant and |S| ≤(1−ϵ) log
1
pm (n) where
pm = min(p, 1 −p).

On the Complexity of k-Metric Antidimension Problem
561
Proof. Let S be a subset in Γi with i ≤(1 −ϵ) log
1
pm (n), v be a vertex not in
S, and r′
∗be a relaxed metric representation in R∗
S. We deﬁne Iv(r′
∗, S) as the
indicator function of the event r∗(v|S) = r′
∗and Xr′
∗(S) = 
v /∈S Iv(r′
∗, S). As
the relaxed shortest-path distances between v and vertices in S are mutually
independent, we have
E(Iv(r′
∗, S)) = pβ1(r′
∗) · (1 −p)|S|−β1(r′
∗) and E(Xr′
∗(S)) ≥(n −|S|) · p|S|
m .
By the assumptions of α and ϵ, we get
(n −|S|) · p|S|
m ≥nϵ −|S| · nϵ−1 where nϵ =
(2 + β)
Cα ln( 1
pm ) ln2(n)
which means min(E(Xr′
∗(S))) ∈Ω(ln2(n)). Let B(r′
∗, S) be the event
|Xr′∗(S) −E(Xr′∗(S))| ≤αE(Xr′∗(S)).
By ChernoﬀBound, see Corollary A.1.14 in [11], we know
Pr{
∼B(r′
∗, S)} = Pr{|Xr′∗(S) −E(Xr′∗(S))| ≥αE(Xr′∗(S))} ≤2e−CαE(Xr′∗(S)).
Let r′ be a metric representation with respect to S corresponding to r′
∗. We
deﬁne Iv(r′, S) as the indicator function of the event r(v|S) = r′ and Xr′(S) =

v /∈S Iv(r′, S). Let B(r′, S) be the event
|Xr′(S) −E(Xr′
∗(S))| ≤αE(Xr′
∗(S)).
Let D be the event that the diameter of G(n, p) is less than or equal to 2.
Then
∼B(r′, S) = (
∼B(r′, S) ∩D) ∪(
∼B(r′, S) ∩∼D). Note that the event
∼B(r′, S) ∩D is the event
∼B(r′
∗, S) ∩D. Therefore we have
Pr{
∼B(r′, S)} = Pr{
∼B(r′
∗, S)∩D}+Pr{
∼B(r′, S)∩∼D} ≤Pr{
∼B(r′
∗, S)}+Pr{∼D}.
Note that two vertices have the shortest-path distance greater than 2 iﬀthey
have no common neighbors. Let X be the number of pairs of vertices in G(n, p)
that they have no common neighbors. By Markov’s inequality, we have
Pr{∼D} = Pr{X > 0} ≤E(X) =
n
2

(1 −p2)n−2.
Let B(S) be the event

r′∈RS
B(r′, S). By Boole’s inequality, also well known
by the union bound, we have
Pr{B(S)} ≥1 −n|S| ·
	
2e−Cα(n−|S|)·p|S|
m +
n
2

(1 −p2)n−2

.

562
C. Zhang and Y. Gao
Let A = |{S|S ∈Γi,∼B(S)}|. By the inequality
n
k

≤( en
k )k, we have
lim
n→∞E(A) ≤lim
n→∞2 ·
en2
|S| ·
	eCαnϵ−1
n2+β
+
n
2

(1 −p2)n−2

1
|S| 
|S|
= 0.
By Markov’s inequality, we get lim
n→∞Pr{A = 0} = 1.
⊓⊔
Theorem 3. Given a random graph G(n, p) with constant p, whp there is no
k-antiresolving set S such that k is not constant and |S| ≥log
1
2p2−2p+1 (n).
Proof. Let S be a subset in Γi with i ≥log
1
2p2−2p+1 (n). We prove Theorem 3 by
considering two cases: (1) i ∈Θ(n); (2) i ∈o(n). In Case 1, we use Iv(r′, S) as
the deﬁnition in Theorem 2. Let pm = min(p, 1 −p). Then we have
Pr{Iv(r′, S) = 1} ≤(1 −pm)|S|.
Let B(r′, S) be the event 
v /∈S Iv(r′, S) ∈ω(1).
Thus we know
Pr{B(r′, S)} ≤
 n
ω(1)

(1 −pm)|S|ω(1).
Let B(S) be the event that S is a k-antiresolving set where k ∈Θ(1). By
Fr´ehet inequalities, also known as Boole-Fr´ehet inequalities, we get
Pr{B(S)} = 1 −Pr{

r′∈RS
B(r′, S)} ≥1 −
 n
ω(1)

(1 −pm)|S|ω(1).
Let A = |{S|S ∈Γi,∼B(S)}|. Hence we have
lim
n→∞E(A) ≤lim
n→∞
	
ne
ω(1)(
1
1−pm )
|S|
2

ω(1)
·
	
ne
|S|(
1
1−pm )
ω(1)
2

|S|
≤lim
n→∞
	
1
c1 e
(
1
1−pm )
ω(1)
2

|S|
= 0.
By the notation of Θ, we know that there are a n0 and a constant c1, such
that n > n0, c1n ≤|S|. Then we have lim
n→∞Pr{A = 0} = 1.
In Case 2, we ﬁrstly prove that if |RS| ∈Θ(n −|S|), S is a k-antiresolving
set of constant k. If not, we have that for any r′ ∈RS

v /∈S Iv(r′, S) ∈ω(1).
Therefore, we know that the number of vertices not in S is in ω(1)·Θ(n−|S|) ∈
ω(n−|S|). This contradicts that the number of vertices not in S is exactly n−|S|.
In the following, we prove that E(|R∗
S|) ∈Θ(n −|S|)) in Case 2. Given a
r′
∗∈R∗
S, let IS(r′
∗) be the indicator function as
IS(r′
∗) =

1 : ∃v /∈S such that r∗(v|S) = r′
∗,
0 : otherwise.

On the Complexity of k-Metric Antidimension Problem
563
Then we have
Pr{IS(r′
∗) = 1} = 1 −

1 −pβ1(r′
∗)(1 −p)|S|−β1(r′
∗)n−|S|.
Let Φ = pβ1(r′
∗)(1 −p)|S|−β1(r′
∗). By the following inequality 1 −(1 −x)n ≥
nx −(nx)2
2
for x, n > 0, we get
Pr{IS(r′
∗) = 1} ≥[Φ · (n −|S|)] −[Φ · (n −|S|)]2
2
which means
E(|R∗
S|) ≥
|S|

β1(r′
∗)=0
 |S|
β1(r′∗)

[Φ · (n −|S|)] −[Φ · (n −|S|)]2
2

≥(n −|S|) −1
2n2(2p2 −2p + 1)|S| −1
2|S|2(2p2 −2p + 1)|S|.
As |S| ≥log
1
2p2−2p+1 (n), we know E(|R∗
S|) ∈Ω(n −|S|). Clearly, |R∗
S| ≤
n −|S|. Then we have E(|R∗
S|) ∈Θ(n −|S|)).
For a vertex v /∈S, we know that r∗(v|S) takes values from the Cartesian
product Λv = {1, ∗}|S|. For v1, ..., vn−|S| /∈S, we deﬁne a function f such that
f(r∗(v1|S), ..., r∗(vn−|S||S)) = |R∗
S|. If two vectors x, x′ ∈n−|S|
j=1
Λvj are diﬀer-
ent with only one coordinate, we have |f(x) −f(x′)| ≤1. Let α be an arbitary
small constant. By the Azuma-Hoeﬀding inequality, see Corollary 2.27 in [12],
we get
Pr{
|R∗
S| −E(|R∗
S|)
 ≥αE(|R∗
S|)} ≤2e−
(αE(|R∗
S|))2
2(n−|S|) .
As E(|R∗
S) ∈Θ(n −|S|), there are a n0 and a constant c1, such that n > n0,
c1(n −|S|) ≤E(|R∗
S|). Let B(S) be the event |RS| ∈Θ(n −|S|) and B∗(S) be
the event |R∗
S| ∈Θ(n −|S|). As |RS| ≥|R∗
S|, we have
Pr{∼B(S)} ≤Pr{∼B∗(S)}
which means
lim
n→∞Pr{∼B(S)} ≤2e−Cα(n−|S|) where Cα =
(αc1)2
2
. Let A =
|{S|S ∈Γi,∼B(S)}|. By |S| ∈o(n) and the inequality
n
k

≤( en
k )k, we have
lim
n→∞E(A) ≤lim
n→∞2 ·
 ne
|S|
|S|e−Cα(n−|S|) = lim
n→∞2 ·
	
e ·
n
|S|
eCα( n
|S| −1)

|S|
= 0.
By Markov’s inequality, we know lim
n→∞Pr{A = 0} = 1.
⊓⊔
Theorem 4. Given a random graph G(n, p) with constant p, whp there is at
least one k-antiresolving set S such that k is constant and |S| ≥log
1
pm (n) where
pm = min(p, 1 −p).

564
C. Zhang and Y. Gao
Proof. Because the conclusion of Theorem 3, we only need to consider the case
i ∈Θ(log
1
pm (n)) in here. We use Iv(r′
∗, S) as the deﬁnition in Theorem 3. Let
S be a subset in Γi and v be a vertex not in S. We deﬁne rS
∗as the relaxed
metric representation with respect to S such that Pr{Iv(rS
∗, S) = 1} = p|S|
m . Let
B(S) be the event that S is a k-antiresolving set of constant k and C(S) be the
event that 
v /∈S Iv(rS
∗, S) is a constant c. Clearly, Pr{B(S)} ≥Pr{C(S)}. Let
A = |{S|S ∈Γi, C(S)}|. Then we have
E(A) =
 n
|S|

Pr{C(S)} =

n
|S| + c

p|S|c
m (1 −p|S|
m )n−|S|−c.
By the inequality
n
k

≥( n
k )k and
lim
n→∞(1 −p|S|
m )n−|S|−c ≥e−1 for |S| ≥
log
1
pm (n), we know
lim
n→∞E(A) = ∞.
By Chap. 4.1, Corollaries 4.3.3, and 4.3.4 in [11], we have the following
inequality
Var(A) ≤E(A) +

S,S′∈Γi,S̸=S′
Cov(C(S), C(S′))
where

S,S′∈Γi,S̸=S′
Cov(C(S), C(S′)) ≤E(A) ·

S,S′∈Γi,S̸=S′
Pr{C(S′)|C(S)}.
In the following, we show that Var(A) ∈o(E(A)2) by considering two cases:
(1) S ∩S′ ̸= ∅; (2) S ∩S′ = ∅. Let dm = 1 if p = pm; otherwise let dm = ∗. In
Case 1, let y = |S ∩S′|. Then, given a vertex vu /∈S′ \S and a vertex vt ∈S′ \S,
we have
Pr{dut = dm|C(S)} =Pr{dut = dm, vu ∈S|C(S)} + Pr{dut = dm, vu /∈S|C(S)}
≤
|S|
n −|S| + y + pm
which means
Pr{r∗(v|S′) = rS′
∗|C(S)} ≤Pr{r∗(v|S′ \ S) = rS′\S
∗
|C(S)}
≤(
|S|
n −|S| + y + pm)|S′|−y.
Let
pm(y) =
|S|
n −|S| + y + pm
(1)

On the Complexity of k-Metric Antidimension Problem
565
and z be the number of vertices v /∈S∪S′ such that r∗(v|S) = rS
∗and r∗(v|S′) =
rS′
∗. Then we have
Pr{C(S′)|C(S)} ≤
 n
c −z

[pm(y)]c(|S′|−y)
which means

S,S′∈Γi,S̸=S′
Pr{C(S′)|C(S)} ≤
c

z=0
|S|−1

y=1
 n
c −z

n
|S| −y

[pm(y)]c(|S|−y).
By changing variables z to c −z, y to |S| −y, we have
pm(y) =
|S|
n −y + pm

S,S′∈Γi,S̸=S′
Pr{C(S′)|C(S)} ≤
c

z=0
|S|−1

y=1
n
z
n
y

[pm(y)]cy.
By y ∈O(log
1
pm (n)), we know
lim
n→∞

n
y + 1

[pm(y + 1)]c(y+1) ≥lim
n→∞
n
y

[pm(y)]cy
which means
lim
n→∞
max
y∈[1,|S|−1]
	n
y

[pm(y)]cy

= lim
n→∞

n
|S| −1

[pm(|S| −1)]c(|S|−1).
(2)
By the inequality
n
k

≥
nk
4k! for n
1
2 ≥k, lim
n→∞(1 −p|S|
m )n−|S|−c ≥e−1, and
(2), we have
lim
n→∞
Var(A)
E(A)2 ≤lim
n→∞
c · nc · |S| ·

n
|S|−1

· [pm(|S| −1)]c(|S|−1)
n|S|+c
4(|S|+c)! · e−1 · pc|S|
m
≤lim
n→∞
4e · c · |S| · (|S| + c)1+c · (1 +
|S|
n−|S|+1 ·
1
pm )c(|S|−1)
pcm · n
= 0.
Now we consider Case 2. Let V ′ be the set of vertices v /∈S′ where r∗(v|S′) =
rS′
∗. If V ′ ∩S = ∅, C(S) and C(S′) are independent. Otherwise, let z = |V ′ ∩S|.
Then we have
Pr{C(S′)|C(S)} ≤
c

z=1
|S|
z
n −2|S|
c −z

[pm(0)]c|S|

566
C. Zhang and Y. Gao
where pm(0) is the formula by plugging y = 0 into (1). Then we have
lim
n→∞
Var(A)
E(A)2 ≤lim
n→∞
c · |S|c ·
 n
|S|

· nc−1 · [pm(0)]c|S|
n|S|+c
4(|S|+c)! · e−1 · pc|S|
m
≤lim
n→∞
4ec · |S|c · (|S| + c)c · [1 +
|S|
n−|S| ·
1
pm ]c|S|
n
= 0.
Now we have proved that Var(A) ∈o(E(A)2). By the inequality
Pr{A = 0} ≤Var(A)
E(A)2 ,
see Theorem 4.3.1 in [11], we have
lim
n→∞Pr{A = 0} = 0. As Pr{B(S)} ≥
Pr{C(S)}, by the coupling method, see Example 7.1 in [14], we know
lim
n→∞Pr{

S∈Γi
B(S)} ≥lim
n→∞Pr{

S∈Γi
C(S)} = lim
n→∞Pr{A > 0} = 1.
⊓⊔
5
Future Discussion
The time complexity of brute force algorithm to ﬁnd a k-antiresolving basis in
a simple conntected graph G = (V, E) is in O(2|V | · |V |2). We are looking for
an exact algorithm to ﬁnd a k-antiresolving basis which performs better than
the brutal one. For G(n, p) with constant p, there still is a gap of Θ(ln(ln(n)))
between the ﬁrst bound and the third bound. We conjecture there is a greater
value for the ﬁrst bound, or log
1
pm (n) is the sharp threshold for the appear-
ance of k-antiresolving sets with constant k. We also conjecture there is a lower
value for the second bound. Moreover, we are interest in how to add noises into
anonymized social networks to increase the resistance against privacy attacks.
References
1. Trujillo-Rasua, R., Yero, I.G.: k-Metric antidimension: a privacy measure for social
graphs. Inf. Sci. 328, 403–417 (2016)
2. Netter, M., Herbst, S., Pernul, G.: Analyzing privacy in social networks - an inter-
disciplinary approach. In: IEEE 3rd International Conference on Social Computing,
pp. 1327–1334 (2011)
3. Zhou, B., Pei, J., Luk, W.: A brief survey on anonymization techniques for privacy
preserving publishing of social network data. ACM SIGKDD Explor. 10(2), 1222
(2008)
4. Meyerson, A., Williams, R.: On the complexity of optimal K-anonymity. In: ACM
Symposium on the Principles of Database Systems, pp. 223–228 (2004)

On the Complexity of k-Metric Antidimension Problem
567
5. Wang, S.-L., Tsai, Z.-Z., Hong, T.-P., Ting, I.-H.: Anonymizing shortest paths on
social network graphs. In: Nguyen, N.T., Kim, C.-G., Janiak, A. (eds.) ACIIDS
2011. LNCS, vol. 6591, pp. 129–136. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-20039-7 13
6. Liu, K., Terzi, E.: Towards identity anonymization on graphs. In: ACM SIGMOD
International Conference on Management of Data, pp. 93–106 (2008)
7. Backstrom, L., Dwork, C., Kleinberg, J.: Wherefore art thou R3579X? anonymized
social networks, hidden patterns, and structural steganography. In: 16th Interna-
tional Conference on World Wide Web, pp. 181–190 (2007)
8. Peng, W., Li, F., Zou, X., Wu, J.: A two-stage deanonymization attack against
anonymized social networks. IEEE Trans. Comput. 63(2), 290–303 (2014)
9. Narayanan, A., Shmatikov, V.: De-anonymizing social networks. In: 30th IEEE
Symposium on Security and Privacy, pp. 173–187 (2009)
10. Chatterjee, T., DasGupta, B., Mobasheri, N., Srinivasan, V., Yero, I.G.: On the
computational complexities of three privacy measures for large networks under
active attackde-anonymizing social networks (2016). arxiv:1607.01438
11. Alon, N., Spencer, J.H.: The Probabilistic Method, 2nd edn. A Wiley-Interscience
Publication, New York (2000)
12. Janson, S., Luczak, T., Ruci´nski, A.: Random Graphs. A Wiley-Interscience
Publication, New York (2000)
13. Garey, M., Johnson, D.: Computers and Intractability: A Guide to the Theory of
NP-Completeness. W.H. Freeman and Company, New York (1979)
14. Dubhashi, D., Panconesi, A.: Concentration of Measure for the Analysis of Ran-
domised Algorithms, 1st edn. Cambridge University Press, New York (2009). ISBN
978-0-521-88427-3

A Local Search Approximation Algorithm
for the k-means Problem with Penalties
Dongmei Zhang1, Chunlin Hao2, Chenchen Wu3, Dachuan Xu2(B),
and Zhenning Zhang2
1 School of Computer Science and Technology, Shandong Jianzhu University,
Jinan 250101, People’s Republic of China
2 Department of Information and Operations Research,
College of Applied Sciences, Beijing University of Technology, Beijing 100124,
People’s Republic of China
xudc@bjut.edu.cn
3 College of Science, Tianjin University of Technology,
Tianjin 300384, People’s Republic of China
Abstract. In this paper, we study the k-means problem with (nonuni-
form) penalties (k-MPWP) which is a natural generalization of the classic
k-means problem. In the k-MPWP, we are given an n-client set D ⊂Rd,
a penalty cost pj > 0 for each j ∈D, and an integer k ≤n. The goal is to
open a center subset F ⊂Rd with |F| ≤k and to choose a client subset
P ⊆D as the penalized client set such that the total cost (including
the sum of squares of distance for each client in D \ P to the nearest
open center and the sum of penalty cost for each client in P) is mini-
mized. We oﬀer a local search (81 + ε)-approximation algorithm for the
k-MPWP by using single-swap operation. We further improve the above
approximation ratio to (25 + ε) by using multi-swap operation.
Keywords: Approximation algorithm · k-means · Penalty · Local
search
1
Introduction
The k-means problem is a classic problem in the Machine Learning area. The
problem is deﬁned formally as follows. Given an n-client set D in Rd and an
integer k, the goal is to partition D into k clusters X1, ..., Xk and assign each
cluster a center such that the sum of squares of distances from each client to its
center is minimized.
Since the k-means problem is NP-hard [1,8,15], one natural way is to design
eﬃcient heuristic algorithm. Among all the heuristic algorithms, the Lloyd’s
algorithm is the most popular one [13,14]. Another important way is to design
approximation algorithm. Most approximation algorithms for the k-means prob-
lem are based on the important observation of Matouˇsek [18] which points out
the connection between k-means and k-median problems via constructing an
approximate centroid set.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 568–574, 2017.
DOI: 10.1007/978-3-319-62389-4 47

Approximation Algorithm for the k-means Problem with Penalties
569
For the metric k-median problem, Charikar et al. [6] give the ﬁrst constant
approximation algorithm. This ratio is further improved by [2,4,12]. Zhang [20]
gives a local search (2+
√
3+ε)-approximation algorithm for the general k-facility
location problem.
For the k-means problem, Kanungo et al. [10] give the ﬁrst constant local
search (9 + ε)-approximation algorithm. Makarychev et al. [16] present a bi-
criteria approximation algorithm. There are many practical and interesting vari-
ants for the k-means problem (cf. [3,9,17]).
In order to handle outliers in clustering applications, we introduce a k-means
problem with (nonuniform) penalties (k-MPWP) which is a natural generaliza-
tion of the classic k-means problem. In the k-MPWP, we are given a n-client
set D ⊂Rd, a penalty cost pj > 0 for each j ∈D, and an integer k ≤n. The
goal is to open a center subset F ⊂Rd with |F| ≤k and to choose a client
subset P ⊆D as the penalized client set such that the total cost (including the
sum of squares of distance for each client in D \ P to the nearest open center
and the sum of penalty cost for each client in P) is minimized. Following the
approaches of Kanungo et al. [10] for the k-means problem and Arya et al. [2]
for the k-median problem, we present two local search constant approximation
algorithms for the k-MPWP.
We remark that Tseng [17] introduces the penalized and weighted k-means
problem which is similar to the k-MPWP but with uniform penalties. The
penalty version for the classic facility location problem and k-median problem is
proposed by Charikar et al. [7]. The currently best approximation ratios for the
facility location problems with linear/submodular penalties are given by [11].
The contributions of our paper are summarized as follows.
– Introduce ﬁrstly the k-means problem with (nonuniform) penalties which
generalizes the k-means problem;
– Oﬀer (81 + ε)- and (25 + ε)- approximation algorithms for the k-MPWP by
using single-swap and multi-swap local search techniques respectively. To the
best of our knowledge, no constant approximation algorithm is known for this
problem even for the uniform penalties version.
The organization of this paper is as follows. In Sect. 2, we present a single-
swap local search (81+ε)-approximation algorithm for the k-MPWP. We further
improve this approximation ratio to 25 + ε by using multi-swap in Sect. 3. We
give some discussions in Sect. 4.
All the proofs are deferred to the journal version.
2
A Local Search (81 + ε)-approximation Algorithm
2.1
Notations
The square of distance between any two points a, b ∈Rd, is deﬁned as follows,
Δ(a, b) := ||a −b||2.

570
D. Zhang et al.
Given a client subset U ⊆D, a point c ∈Rd, we deﬁne the total sum of
squares distances of U with respect to c and the centroid of U as follows,
Δ(c, U) :=

j∈U
Δ(c, j) =

j∈U
||c −j||2,
cen(U) :=
1
|U|

j∈U
j.
The following lemma gives the important property of centroidal solution.
Lemma 1 [10]. For any subset U ⊆D and a point c ∈Rd, we have
Δ(c, U) = Δ(cen(U), U) + |U|Δ(cen(U), c).
(1)
Let S and O be a feasible solution and a global optimal solution to the k-
MPWP with penalized sets P and P ∗, respectively. For each client j ∈D\P, we
denote Sj := min
s∈S Δ(j, s) and sj := arg min
s∈S Δ(j, s). For each client j ∈D \ P ∗,
we denote Oj := min
o∈O Δ(j, o) and oj := arg min
o∈O Δ(j, o). For each center o ∈O, we
denote so := arg min
s∈S Δ(o, s) and say that so captures o. Furthermore, we denote
– NS(s) := {j ∈D \ P|sj = s}, ∀s ∈S;
– Cs :=

j∈D\P
Sj = 
s∈S
Δ(s, NS(s)), Cp := 
j∈P
pj, cost(S) := Cs + Cp;
– NO(o) := {j ∈D \ P ∗|oj = o}, ∀o ∈O;
– C∗
s :=

j∈D\P ∗Oj = 
o∈O
Δ(o, NO(o)), C∗
p := 
j∈P ∗pj, cost(O) := C∗
s + C∗
p.
We remark that o = cen(NO(o)) for each o ∈O.
Matouˇsek [18] introduces the following concept of approximate centroid set.
Deﬁnition 2. A set of C ⊂Rd is an ˆε-approximate centroid set for D ⊂Rd if
for any set U ⊆D, we have
min
c∈C

j∈U
Δ(c, j) ≤(1 + ˆε) min
c∈Rd

j∈U
Δ(c, j).
One can compute a ˆε-approximate centroid set C for D of polynomial size in
polynomial time [16,18]. Equipped with this result, we assume that all candidate
centers are chosen from C.
2.2
Local Search Algorithm
For any feasible solution S, we deﬁne the single-swap operation swap(a, b) as
follows. Given a ∈S and b ∈C \ S, the center a is deleted from S and the center
b is added to S.
We deﬁne the neighborhood of S associated with the above operation as
follows,
Ngh1(S) := {S \ {a} ∪{b}|a ∈S, b ∈C \ S, } .
For any given ε > 0, we present our single-swap local search algorithm as
follows.

Approximation Algorithm for the k-means Problem with Penalties
571
Algorithm 1 (ALG1(ε))
Step 0. (Initialization) Setting
ˆε := 72 + ε −

722 + 64ε,
construct ˆε-approximate centroid set C. Arbitrarily choose a feasible solu-
tion S.
Step 1. (Local search) Compute
Smin := arg
min
S′∈Ngh1(S) cost(S′).
Step 2. (Stop criterion) If cost(Smin) ≥cost(S), output S. Otherwise, set
S := Smin and go to Step 1.
2.3
Analysis
To proceed the analysis, we need the following technical lemma.
Lemma 3. Let S and O be a local optimal solution and a global optimal solution
to the k-MPWP, respectively. We have,

j∈D\(P ∪P ∗)

SjOj ≤


j∈D\(P ∪P ∗)
Sj ·


j∈D\(P ∪P ∗)
Oj,
(2)

Δ(soj, j) ≤

Sj + 2

Oj,
∀j ∈D \ (P ∪P ∗).
(3)
In order to proceed the analysis, we need to introduce a center ˆo ∈C associ-
ated with each o ∈O. For each o ∈O, let us deﬁne
ˆo := arg min
c∈C Δ(c, NO(o)).
(4)
From Deﬁnitions 2 and 4, we have

j∈NO(o)
Δ(ˆo, j) = Δ(ˆo, NO(o))
= min
c∈C Δ(c, NO(o))
≤(1 + ˆε) min
c∈Rd Δ(c, NO(o))
= (1 + ˆε)Δ(o, NO(o))
= (1 + ˆε)

j∈NO(o)
Δ(o, j).
(5)
We remark that the idea of choosing ˆo satisfying inequality 5 is essentially
due to Ward [19].

572
D. Zhang et al.
Then, we estimate the cost of S in the following theorem.
Theorem 4. For any given ε > 0, ALG1(ε) produces a local optimal solution
S satisfying
Cs + Cp ≤(81 + ε)C∗
s + 18C∗
p.
Using standard technique of [2,5], we can obtain a polynomial time local
search algorithm which sacriﬁces any given ε′ > 0 in the approximation ratio.
3
Improved Local Search (25 + ε)-approximation
Algorithm
For any feasible solution S, we deﬁne the so-called multi-swap operation
swap(A, B) as follows. In the operation, we are given two subsets A ⊆S and
B ⊆C \ S with |A| = |B| ≤p, where p is a ﬁxed integer. All centers in A are
deleted from S. Meanwhile, all centers in B are added to S.
We deﬁne the neighborhood of S with respect to the above multi-swap oper-
ation as follows,
Nghp(S) := {S \ A ∪B|A ⊆S, B ⊆C \ S, |A| = |B| ≤p} .
For any given ε > 0, we present our multi-swap local search algorithm as
follows.
Algorithm 2 (ALG2(ε))
Step 0. (Initialization) Setting
p :=

5
√25 + ε −5

,
ˆε := 6
p + 5
p2 ,
construct ˆε-approximate centroid set C. Arbitrarily choose a feasible solu-
tion S.
Step 1. (Local search) Compute
Smin := arg
min
S′∈Nghp(S) cost(S′).
Step 2. (Stop criterion) If cost(Smin) ≥cost(S), output S. Otherwise, set
S := Smin and go to Step 1.
Theorem 5. For any given ε > 0, ALG2(ε) produces a local optimal solution
S satisfying
Cs + Cp ≤(25 + ε) C∗
s +
	
5 + ε
5

C∗
p.

Approximation Algorithm for the k-means Problem with Penalties
573
4
Discussions
In this paper, we study the k-MPWP and present (81+ε)- and (25+ε)- approx-
imation algorithms using single-swap and multi-swap respectively. Since there
is a local search (9 + ε)-approximation algorithm for the classic k-means prob-
lem [10], it is natural to ask whether our (25 + ε)-approximation can be further
improved. Another research direction is to design approximation algorithm for
this problem using LP rounding technique.
Acknowledgements. The research of the ﬁrst author is supported by Higher Edu-
cational Science and Technology Program of Shandong Province (No. J15LN23). The
second author is supported by Ri-Xin Talents Project of Beijing University of Tech-
nology. The third author is supported by Natural Science Foundation of China (No.
11501412). The fourth author is supported by Natural Science Foundation of China
(No. 11531014). The ﬁfth author is supported by Beijing Excellent Talents Funding
(No. 2014000020124G046).
References
1. Aloise, D., Deshpande, A., Hansen, P., Popat, P.: NP-hardness of Euclidean sum-
of-squares clustering. Mach. Learn. 75, 245–249 (2009)
2. Arya, V., Garg, N., Khandekar, R., Meyerson, A., Munagala, K., Pandit, V.: Local
search heuristics for k-median and facility location problems. SIAM J. Comput.
33, 544–562 (2004)
3. Bandyapadhyay, S., Varadarajan, K.: On variants of k-means clustering. In: Pro-
ceedings of SoCG, Article No. 14, pp. 14:1–14:15 (2016)
4. Byrka, J., Pensyl, T., Rybicki, B., Srinivasan, A., Trinh, K.: An improved approx-
imation for k-median, and positive correlation in budgeted optimization. In: Pro-
ceedings of SODA, pp. 737–756 (2014)
5. Charikar, M., Guha, S.: Improved combinatorial algorithms for the facility location
and k-median problems. In: Proceedings of FOCS, pp. 378–388 (1999)
6. Charikar, M., Guha, S., Tardos, ´E., Shmoys, D.B. A constant-factor approximation
algorithm for the k-median problem. In: Proceedings of STOC, pp. 1–10 (1999)
7. Charikar, M., Khuller, S., Mount, D.M., Narasimhan, G.: Algorithms for facility
location problems with outliers. In: Proceedings of SODA, pp. 642–651 (2001)
8. Dasgupta, S. The hardness of k-means clustering. Technical Report CS2007-0890,
University of California, San Diego (2007)
9. Georgogiannis, A.: Robust k-means: a theoretical revisit. In: Proceedings of NIPS,
pp. 2883–2891 (2016)
10. Kanungo, T., Mount, D.M., Netanyahu, N.S., Piatko, C.D., Silverman, R., Wu,
A.Y.: A local search approximation algorithm for k-means clustering. Comput.
Geom. Theory Appl. 28, 89–112 (2004)
11. Li, Y., Du, D., Xiu, N., Xu, D.: Improved approximation algorithms for the facility
location problems with linear/submodular penalties. Algorithmica 73, 460–482
(2015)
12. Li, S., Svensson, O.: Approximating k-median via pseudo-approximation. In: Pro-
ceedings of STOC, pp. 901–910 (2013)
13. Lloyd, S.: Least squares quantization in PCM. Technical report, Bell Laboratories
(1957)

574
D. Zhang et al.
14. Lloyd, S.: Least squares quantization in PCM. IEEE Trans. Inf. Theory 28, 129–
137 (1982)
15. Mahajan, M., Nimbhorkar, P., Varadarajan, K.: The planar k-means problem is
NP-hard. In: Proceedings of WALCOM, pp. 274–285 (2009)
16. Makarychev, K., Makarychev, Y., Sviridenko, M., Ward, J.: A bi-criteria approx-
imation algorithm for k-means. In: Proceedings of APPROX/RONDOM, Article
No. 14, pp. 14:1–14:20 (2016)
17. Tseng, G.C.: Penalized and weighted k-means for clustering with scattered objects
and prior information in high-throughput biological data. Bioinformatics 23, 2247–
2255 (2007)
18. Matouˇsek, J.: On approximate geometric k-clustering. Discrete Comput. Geom.
24, 61–84 (2000)
19. Ward, J. Private Communication (2017)
20. Zhang, P.: A new approximation algorithm for the k-facility location problem.
Theoret. Comput. Sci. 384, 126–135 (2007)

Improved Approximation Algorithm
for the Maximum Base Pair Stackings Problem
in RNA Secondary Structures Prediction
Aizhong Zhou1, Haitao Jiang1(B), Jiong Guo1, Haodi Feng1,
Nan Liu2, and Binhai Zhu3
1 School of Computer Science and Technology, Shandong University,
Jinan, Shandong, China
398239146@qq.com, {htjiang,jguo,fenghaodi}@sdu.edu.cn
2 School of Computer Science and Technology, Shandong Jianzhu University,
Jinan, Shandong, China
belovedmilk@126.com
3 Gianforte School of Computing, Montana State University,
Bozeman, MT 59717-3880, USA
bhz@montana.edu
Abstract. We investigate the maximum base pair stackings problem
from RNA Secondary Structures prediction in this paper. Previously,
Ieong et al. deﬁned a basic version of this maximum base pair stackings
problem as: given an RNA sequence, ﬁnding a set of base pairs to con-
stitute a maximum number of stackings, and proved it to be NP-hard,
where the base pairs are default under some biology principle and are
given implicitly. Jiang proposed a generalized version of this problem,
where the candidate base pairs are given explicitly as input and pre-
sented an approximation algorithm with a factor 8/3. In this paper, we
present a new approximation algorithm for the generalized maximum
base pair stackings problem by a two-stage local search method, improv-
ing the approximation factor from 8/3+ε to 5/2. Since we adopt only
two basic local operations, 1-substitutions and 2-substitutions, during
the local improvement stage, the time complexity can be bounded by
O(n7), much faster than the previous approximation algorithms.
1
Introduction
According to the central dogma of biology, Ribonucleic acids (RNAs) play an
important role in regulating genetic and metabolic activities. Moreover, as new
RNA sequences are constantly being discovered, in order to understand the bio-
logical functions of RNAs elaborately, we need to ﬁrst know their structures.
An RNA is single-stranded chain and can be viewed as a sequence of
nucleotides (also known as bases, denoted by A, C, G and U). The order of A, C,
G, U’s on the sequence form the primary structure of an RNA strand. An RNA
folds into a three-dimensional structure by forming hydrogen bonds between
nonconsecutive bases that are complementary, such as the Watson-Crick pairs
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 575–587, 2017.
DOI: 10.1007/978-3-319-62389-4 48

576
A. Zhou et al.
A-U and C-G and the wobble pair G-U. The three-dimensional arrangement of
the atoms in the folded RNA molecule forms the tertiary structure; the collec-
tion of base pairs in the tertiary structure forms the secondary structure. The
secondary structure can in fact tell us where there are additional connections
between the bases, and where the RNA molecule could be folded. In [13], the
authors claimed that “the folding of RNA is hierarchical, since secondary struc-
ture is much more stable than tertiary folding”, which implies that the tertiary
folding would mostly obey the secondary structure. Since the three-dimensional
structure determines the function of the RNA to some extent, predicting the
secondary structure of RNA becomes a key problem to study RNA in a larger
and deeper scope.
In 1978, Nussinov et al. [9] initiated the computational study of RNA sec-
ondary structures prediction, but this problem is still not well-solved yet. The
biggest impediment is the existing of pseudoknots, which is composed of two
interleaving base pairs provided that we arrange the RNA sequence in a linear
order.
In the case where there is no pseudoknot, there have been a lot of positive
results. Almost all of them use a dynamic programming method [7–9,11,15,
16]. As a consequence, the optimal RNA secondary structure can be computed
roughly in O(n3) time and O(n2) space.
When pseudoknots do exist in some RNAs, the secondary structures predic-
tion problem is harder. Lyngsø and Pedersen [6] proved that determining the
optimal secondary structure possibly with pseudoknots is NP-hard under some
special energy functions. Akutsu [1] showed that it remains NP-hard, even if
the secondary structure requires to be planar. For limited types of pseudoknots,
polynomial-time algorithms have been presented [1,10,14].
According to Tinoco’s energy model [12], an RNA structure can be decom-
posed recursively into loops with independent free energy, the stacking loops
formed by two adjacent base pairs have negative energy, which stabilizes the
RNA structure. Hence Ieong et al. [3] initiated the study for the maximum
base pair stackings problem with arbitrary pseudoknots. They proved that it is
NP-hard to compute the planar secondary structure with the largest number
of stackings, and proposed a 2-approximation for the planar version and a 3-
approximation for the general version of this problem. Later, Lyngsø [5] proved
that the maximum base pair stacking loops problem without the planar restric-
tion remains NP-hard, even for binary sequences with 0–1 base pairs. He also
devised a polynomial-time approximation scheme (PTAS) for this problem, with
bases over a ﬁxed-size alphabet Σ and the base pairs being a subset of Σ × Σ,
which runs in O(n|Σ|
1
ε ) time. Unfortunately, this PTAS is impractical even for
|Σ| = 4 (e.g., Σ = {A, C, G, U}), and ε = 1/2.
Among all the above results, the base pairs are given implicitly, that is,
under some ﬁx biology principle, e.g., Watson-Crick base pairs: A-U and C-G,
where any two such bases can form a base pair. As an alternative, the set of
candidate base pairs may be given explicitly as input, because there could be
additional conditions from comparative analysis which prevents two bases from

Maximum Base Pair Stackings Problem
577
forming a pair. This generalizes the maximum base pair stacking problem with
implicit base pairs, hence the problem remains NP-hard. Jiang [4] improved
the approximation factor for the maximum base pair stackings problem with
explicit base pairs to 8/3+ε. Jiang’s algorithm combines the greedy strategy of
Ieong’s approximation algorithm and Berman’s [2] approximation algorithm for
computing a Maximum Weight Independent Set in (d + 1)-claw-free graphs; to
be more precise, its approximation factor is 8/3+ε, and the time complexity is
O(nlogd 1
ε ).
In this paper, we devise a new approximation algorithm for the maximum
base pair stacking problems with explicit base pairs. Our method is based on
local search. The new approximation factor is 5/2, and the time complexity is
O(n7).
2
Preliminaries
Let S = s1s2 · · · sn be an RNA sequence of n bases on {A, C, G, U}. We say
that two bases si and si+1 (1≤i ≤n −1) are continuous on S. A secondary
structure of S is a set of base pairs (si1, sj1), (si2, sj2), . . . , (sir, sjr), where
ik + 2 ≤jk for all k = 1, . . . , r and no two base pairs share a base. Two
base pairs, such as (si, sj) and (si+1, sj−1) with i + 4 ≤j, are said to be
adjacent. A stacking is a loop formed by two adjacent base pairs (si, sj) and
(si+1, sj−1), denoted by (si, si+1; sj−1, sj). A helix H of length q is composed of
q + 1 consecutive base pairs (si, sj), (si+1, sj−1), . . . , (si+q, sj−q), denoted by
(si, si+1, . . . , si+q; sj−q, sj−q+1, . . . , sj). (si, sj) and (si+q, sj−q) are called end-
ing base pairs of the helix H. We refer the segment of bases si, si+1, . . . , si+q
as the α-side of the helix, and sj−q, sj−q+1, . . . , sj as the β-side of the helix,
denoted by Hα and Hβ respectively. Note that there are exactly q stackings in
a helix of length q. A helix contains at least two stackings is called a long helix,
and a stacking is also called a short helix.
Now we present the formal deﬁnition of the problem to be studied in this
paper. An example is shown in Fig. 1.
Problem Description: Maximum Base Pair Stackings
Input: An RNA sequence S, and a set of candidate base pairs BP.
Output: A set of chosen base pairs to constitute a maximum number of stack-
ings.
Fig. 1. The optimal base pairs found: (s1,s5), (s2,s4); and (s6,s12), (s7,s11), (s8,s10)
and the maximum base pair stackings is 3. We cannot choose the base pair (s5,s13), as
the base s5 has been chosen in (s1,s5).

578
A. Zhou et al.
3
Our Algorithm
In this section, we depict our algorithm in detail, where the main idea of our
algorithm is a two-stage local search. We ﬁrstly search for long helices by 1-
substitutions and some special 2-substitutions, without considering the short
helices. Then, we search for the remaining short helices, with the long helices
previously found unchanged. A base is free if it is not involved in any stacking,
otherwise, it is occupied. The stackings in set T are the output of our algorithm,
where T is initialized as the empty set. In the algorithm, we will perform the
following 3 operations of local search to obtain more stackings.
– Operation 1⃝(1-substitution local improvement ⟨q ⟩): For a helix H of length
q (q ≥2) in T, replace it by other long helices with a total length of q′(q′ > q);
for a short helix in T, replace it by other short helices with a total length of
q′(q′ > 1).
– Operation 2⃝(2-substitution local improvement ⟨2, p ⟩(p ≥3)): For a helix
H of length 2 and another helix H′ of length p (p ≥3) in T, replace H and an
ending base pair of H′ by other long helices with a total length of p′(p′ > 3).
– Operation 3⃝(2-substitution local improvement ⟨2, 2 ⟩): For two helices H
and H′ both of length 2 in T, Replace H and H′ by other long helices with a
total length of p′(p′ > 4).
Algorithm 1. Long helices
1: for (q from 8 to 2) do
2:
while ( there exists a helix of length q, with all its bases being free ) do
3:
Put it into T; mark its bases occupied.
4:
end while
5: end for
6: Perform the operation 1⃝to the long helices in T until no other operation 1⃝can
not be performed.
7: Perform the operation 2⃝, 3⃝until no other operation 2⃝and 3⃝can not be per-
formed.
The following Algorithm 2 shows how to search short helices and locally
improve them by 1-substitutions.
Algorithm 2. Short helices
1: while ( there exists a short helix, with all its bases being free ) do
2:
Put it into T; mark its bases occupied.
3: end while
4: Perform the operation 1⃝to the short helices in T, until no other operation 1⃝can
not be performed.

Maximum Base Pair Stackings Problem
579
Theorem 1. The time complexity of Algorithms 1 and 2 is O(n7).
Proof. To generate an initial feasible solution, we search for long helices of length
at most 8, it takes O(n) time to ﬁnd such a helix, and there are at most O(n)
such helices. In total, it takes O(n2) time to obtain an initial feasible solution.
During the 1-substitution local improvement process in Algorithm 1, each
long helix of length q (2 ≤q ≤7) occupies 2q bases, while we possibly make use
of the 4 bases adjacent to them, so we search for long helices from these (at most)
2q + 4 bases, which means we can obtain at most 2q + 4 base pairs. These base
pairs can constitute at most (2q + 4)/3 long helices. To ﬁx a helix, it suﬃces to
ﬁx its starting base pair. Hence we have to search for at most (2q +4)/3 starting
base pairs, while ﬁxing each starting base pair takes O(n) time. Consequently,
for each iteration of the 1-substitution local improvement process, it takes O(n6)
time.
During the 2-substitution local improvement process in Algorithm 1, we
choose a helix of length 2 and a single base pair from a helix of length greater
than 2, or two helices of length 2. In total they occupy 8 bases, while there are
at most 8 bases adjacent to them, hence we search for long helices from these (at
most) 16 bases. Similar to the above argument, it takes O(n5) time. By a similar
analysis, the time complexity of the 1-substitution local improvement process in
Algorithm 2 is O(n4).
Since the value of our solution is at most n, and the value of our solution
would increase by at least 1 during each local improvement step, the algorithm
executes at most n local improvements.
Finally, to check whether our solution cannot be further improved, we have
to check all the O(n2) pairs of long helices (at least one of which must be of
length 2), and check all the long helices of length less than 8 individually. In
summary, the time complexity of Algorithm 1 and Algorithm 2 is O(n7).
⊓⊔
4
Approximation Factor Analysis
To analyze the performance of our algorithm, we should compare our solution
with the optimal solution. At the termination of our algorithm, there would not
be any stacking with its four bases being free, then, all the stacking in the optimal
solution would either be found by our algorithm or at least one of its bases be
occupied by stackings in our solution. For a stacking T ∗= (si, si+1; sj−1, sj) in
the optimal solution, we say that it is destroyed by these helices in our solution
using si, si+1, sj−1 or sj (even if T ∗is also in a stacking in our solution);
moreover, it can be destroyed by Hα (or Hβ), where H is helix and some bases
of si, si+1, sj−1 or sj belong to Hα (or Hβ). The following lemma shows an upper
bound of the number of stackings in the optimal solution which is destroyed by
some helices in our solution.
Lemma 1. A helix of length q in our solution can destroyed at most 2q + 4
stackings in the optimal solution.

580
A. Zhou et al.
Proof. Let H = (si, si+1, . . . , si+q; sj−q, sj−q+1, . . . , sj) be a helix of length q.
It contains q + 1 base pairs, as well as 2(q + 1) bases. Each stacking in the
optimal solution is composed of two adjacent bases in one segment. The seg-
ment of bases si, si+1, . . . , si+q can constitute at most q + 2 adjacent bases
together with another two bases: si−1, si+q+1. Similarly, the segment of bases
sj−q, sj−q+1, . . . , sj can constitute at most q + 2 adjacent bases together with
another two bases: sj−q−1, sj+1. In total, at most 2q+4 stackings in the optimal
solution, using these bases, could be destroyed.
⊓⊔
A stacking in the optimal solution is singly destroyed, if only one helix in our
solution using its bases, otherwise, it is multiply destroyed.
Lemma 2. At the termination of Algorithm 1, the number of stackings, which
are singly destroyed by a long helix H of length q (2 ≤q ≤7) in our solution
and all of which can constitute long helices without H, is at most q.
Proof. Since otherwise, by replacing H, we would obtain some long helices with
more stackings, which means the Algorithm 1 would not terminate.
⊓⊔
To analyze the performance of our algorithm, we divide the stackings in the
optimal solution into two parts: (1) stackings that are singly destroyed by some
helices in our solution; (2) stackings that are multiply destroyed by at least two
long helices in our solution. Then we assign the stackings in the optimal solution
to destroyed-sets of helices by our solution. For a helix of length 8 or more in
our solution, its destroyed-set DSH contains the stackings in the optimal solution
that are destroyed by it, and these stackings could appear in any other destroyed-
sets. For a long helix H of length q (2 ≤q ≤7) in our solution, its destroyed-set
DSH contains the stackings in the optimal solution that are destroyed by it. For
a short helix H′ in our solution, its destroyed-set DSH′ contains the stackings
in the optimal solution that are destroyed by it but not by any long helix. As
each stacking contributes a weight of 1 to the value in the optimal solution, we
assign the weight for destroyed-sets as follows:
– a singly destroyed stacking contributes a weight of 1 to the destroyed-set
containing it;
– a multiply destroyed stacking contributes a weight of 1/2 to each destroyed-set
containing it (see Fig. 2 for an example).
Obviously, the total weight of all the destroyed-sets would be greater than
or equal to the optimal solution value, since all the stackings in the optimal
solution are destroyed. To guarantee a 2.5 approximation ratio, it is suﬃcient to
show that, given each helix H of length q, with the weight of its destroyed-set
being W(DSH), it satisﬁes W(DSH)/q ≤2.5. Henceforth, we say a helix is safe
if the above condition is fulﬁlled.
Let H = (si, si+1, . . . , si+q; sj−q, sj−q+1, . . . , sj) be a helix of length q in our
solution, two consecutive bases sk, sk+1 (i ≤k ≤i + q −1 or j −q ≤k ≤j −1)
form a gap, if they are not in a common stacking in the optimal solution. Deﬁne
l(Hα) and l(Hβ) to be the total length of long helices in the optimal solution,

Maximum Base Pair Stackings Problem
581
Fig. 2. The stackings in the Hα side are singly destroyed and the stackings in the other
side are multiply destroyed by H and H′.
that are singly destroyed by H, and use the bases from the α-side and the β-side
of H respectively. Deﬁne s(Hα) and s(Hβ) to be the number of short helices in
the optimal solution, that are singly destroyed by H, and also use the bases from
the α-side and the β-side of H respectively. Deﬁne g(Hα) and g(Hβ) to be the
number of gaps, in the α-side and the β-side of H respectively.
Lemma 3. At the termination of Algorithm 1, let H be a helix of length q
in our solution, s(Hα) ≤g(Hα) + 1 + ⌊q+2−max{1,l(Hα)+g(Hα)}
5
⌋, and s(Hβ) ≤
g(Hβ) + 1 + ⌊q+2−max{1,l(Hβ)+g(Hβ)}
5
⌋.
Proof. We just prove the former inequality, since the other is exactly the same.
From the deﬁnition of s(Hα), the short helices cannot share common base pairs,
otherwise, they would form long helices. Therefore, the short helices and long
helices must be spaced by gaps or multiply destroyed stackings. At the termina-
tion of Algorithm 1, our solution only contains long helices, and each long helix
of length p (p ≥2) can occupy p + 1 bases, and destroy p + 2 stackings of a
helix in the optimal solution. Hence, between two short helices which are singly
destroyed by Hα, if there is no gap, there should be at least four stackings which
are multiply destroyed. In other words, without gaps, every 5 consecutive stack-
ing can contain a singly destroyed short helix. In case there is no gap and no
long helix, the singly destroyed short helices are spaced by segments of multiply
destroyed stackings, there would be one more singly destroyed short helix. So
we have an extra one in the inequality, meanwhile this short helix occupies two
consecutive bases.
⊓⊔
In fact, it always holds that l(Hα) + s(Hα) + g(Hα) ≤q + 2. Hence, when q = 3
and l(Hα) = 3, we have s(Hα) ≤g(Hα) ≤1.
Now we show that most helices are safe, except a speciﬁc case, which we
would analyze separately.
Lemma 4. A helix H of length q, where q ≥8, is safe.
Proof. From Lemma 1, H can destroy at most 2q + 4 stackings in the optimal
solution, all of which are in the destroyed-set of H. Then, we have (2q + 4)/q ≤
2.5, provided that q ≥8.
⊓⊔
Lemma 5. At the termination of Algorithm 1, a helix H of length q, where
3 ≤q ≤7, is safe.

582
A. Zhou et al.
Proof. From Lemma 1, H can destroy at most 2q + 4 stackings in the optimal
solution. At the termination of Algorithm 1, from Lemma 2, l(Hα) + l(Hβ) ≤q.
Each singly destroyed stacking contributes a weight of 1 to the destroyed-set
of H, and each multiply destroyed stacking contributes a weight of 1/2 to the
destroyed-set of H, totally, we can show,
W(DSH)
q
≤3q + 4 + ρ
2q
(1)
From Lemma 3, we conclude that ρ ≤1, when q = 3; ρ ≤2, when q = 4; and
ρ ≤3, when 5 ≤q ≤7. Therefore,
W(DSH)
q
≤5/2
(2)
In fact, even if we add a weight 1/2 to the numerator, the inequality still holds. ⊓⊔
It remains to deal with the helices of length 2.
Lemma 6. Let H = (si, si+1, si+2; sj−2, sj−1, sj) be a helix of length 2 in our
solution, if l(Hα)=0, then the total weight of stackings in the optimal solution
assigned to H by Hα is at most 2.5.
Proof. From Lemma 3, we have, s(Hα) ≤g(Hα) + 1 + ⌊2+2−max{1,g(Hα)}
5
⌋. That
means s(Hα) −g(Hα) ≤1. As there are at most 4 possible stackings in the
optimal solution using bases of Hα, besides the gaps and short helices, all the
other possible stackings can contribute a weight of 1/2 to the destroyed-set of H,
so we have s(Hα) + 4−s(Hα)−g(Hα)
2
≤2.5. (An example is shown in Fig. 3(a)). ⊓⊔
Fig. 3. (a) Case 1: the Hα side is a singly destroyed stacking with a gap, and the Hβ
side is a singly destroyed stacking together with three multiply destroyed stackings,
and the total weight is at most be 5/2.(b) Case 2: the Hα and Hβ sides decide that H
gets a weight 3 when l(Hα) = 2 or l(Hβ) = 2.
Lemma 7. Let H = (si, si+1, si+2; sj−2, sj−1, sj) be a helix of length 2 in our
solution, if l(Hα) = 2, then the total weight of stackings in the optimal solution
assigned to H by Hα is at most 3.

Maximum Base Pair Stackings Problem
583
Proof. From Lemma 3, we have, s(Hα) ≤g(Hα)+1+⌊2+2−max{1,l(Hα)+g(Hα)}
5
⌋.
As there are at most 4 possible stackings in the optimal solution using bases of
Hα, if s(Hα) = 1, to split this short helix with the long helix, then g(Hα)=1, and
there would be no other possible stackings left. If s(Hα) = 0, besides the gaps
and short helices, all the other possible stackings can contribute a weight of 1/2
to the destroyed-set of H, so we have l(Hα) + s(Hα) + 4−l(Hα)−s(Hα)−g(Hα)
2
≤3.
(An example is shown in Fig. 3(b).)
⊓⊔
The following lemma can be obtained directly from Lemma 6.
Lemma 8. Let H = (si, si+1, si+2; sj−2, sj−1, sj) be a helix of length 2 in our
solution, if l(Hα) = l(Hβ) = 0, then H is safe.
Obviously, there could also be helices of length 2 which is not safe. Suppose
H = (si, si+1, si+2; sj−2, sj−1, sj) is such an unsafe helix, in this case, one of
l(Hα) and l(Hβ) could not be zero. Without loss of generality, we assume that
l(Hα) = 2 and l(Hβ) = 0. From Lemma 2, we have l(Hα) + l(Hβ) ≤2. Then
the total weight of stackings in the optimal solution assigned to H by Hα is at
most 3, and the total weight of stackings in the optimal solution assigned to H
by Hβ is at most 2.5. There are four possible stackings in the optimal solution
using the bases of Hβ, we deﬁne a weight-vector to record the weight of these
four stackings contributing to H,
V (Hβ) = ⟨W(si−1si), W(sisi+1), W(si+1si+2), W(si+2si+3)⟩,
where W(sk−1sk) ∈{1, 0.5, 0}, i ≤k ≤i + 3. To make W(si−1si) + W(sisi+1) +
W(si+1si+2) + W(si+2si+3) ≤2.5, since there could not be two continuous 1’s,
V (Hβ) has two choices: either V (Hβ) contains exactly one 1 and three 0.5’s; or
V (Hβ) contains exactly two 1’s, one 0.5, and one zero. More speciﬁcally, V (Hβ)
has the following conﬁgurations: (a) ⟨1, 0.5, 0.5, 0.5⟩or symmetrically ⟨0.5, 0.5,
0.5, 1⟩, (b) ⟨0.5, 1, 0.5, 0.5⟩or symmetrically ⟨0.5, 0.5, 1, 0.5⟩, (c) ⟨1, 0, 1, 0.5⟩
or symmetrically ⟨0.5, 1, 0, 1⟩. (See Fig. 4 for examples.) We have the following
lemmas.
Fig. 4. (a),(b) and (c) are the examples of conﬁguration (a),conﬁguration (b) and
conﬁguration (c). They are all unsafe.

584
A. Zhou et al.
Lemma 9. At the termination of Algorithm 1, conﬁguration (a) could not exist.
Proof. Omitted due to space constraint.
⊓⊔
We can observe that either conﬁguration (b) or conﬁguration (c) contains two
continuous weight ⟨1, 0.5⟩with the weight of 1/2 being from a stacking using
sj and sj+1, and the weight of 1 being from a stacking using sj−1 and sj.
Without loss of generality, let the stacking S1
H using sj−1 and sj be the stacking
singly destroyed by Hβ, and the stacking S
1
2
H = (sj, sj+1; sr, sr+1) be a multiply
destroyed stacking, which is destroyed by both Hβ and H′ = (H′
α, H′
β) where
H′
α = (sx−p, sx−p+1, . . . , sx) and H′
β = (sy−p, sy−p+1, . . . , sy) and where x = r.
But sj+1 could be possible occupied by another long helix H′′ in our solution,
which means the stacking S
1
2
H is commonly destroyed by H, H′ and H′′. Then
we reassign the weight of S
1
2
H as follows:
– If S
1
2
H is destroyed by only H and H′, we assign the total weight 1 of S
1
2
H to
the destroyed-set of H′.
– If S
1
2
H is destroyed by H, H′ and H′′, we assign a weight of 1/2 to each of the
destroyed-set of H′ and H′′.
It is obviously that, under this weight assignment, H is always safe, no matter
whether V (Hβ) is in conﬁguration (b) or conﬁguration (c). We still have to prove
that under this new weight assignment, H′ and H′′ are safe.
Lemma 10. At the termination of Algorithm 1, if S
1
2
H is destroyed by only Hβ
and H′
α, then p = 2 and l(H′
α) = l(H′
β) = 0; moreover, there cannot be a stacking
in the optimal solution using sx and sx−1, which is singly destroyed by H′
α.
Proof. We show that if any of the three consequences in the lemma does not hold,
there would exist feasible local improvement for our solution, which contradicts
the assumption that Algorithm 1 has terminated.
If p ≥3, by removing H and the base pair (sx, sy), we could obtain a helix
of length 2 which is singly destroyed by H, as well as a helix of length 2, which
is composed of the stacking singly destroyed by Hβ and an adjacent stacking
multiply destroyed by both Hβ and H′
α.
If l(H′
α) + l(H′
β) = 2, by removing H and H′, we could obtain a helix of
length 2 which is singly destroyed by H, a helix of length 2 which is singly
destroyed by H′, as well as a helix of length 2, which is composed of the stacking
singly destroyed by Hβ and an adjacent stacking multiply destroyed by both Hβ
and H′
α.
Now consider that case when there is a stacking in the optimal solution using
sx and sx−1, which is singly destroyed by H′
α. By removing H and H′, we could
obtain the helix of length 2 which is singly destroyed by H, as well as a helix of
length 3, which is composed of the stacking singly destroyed by Hβ, an adjacent
stacking multiply destroyed by both Hβ and H′
α, and the stacking using sx and
sx−1 singly destroyed by H′
α. In all these cases, a local improvement is possible. ⊓⊔

Maximum Base Pair Stackings Problem
585
Lemma 11. The weight reassignment cannot generate two new continuous
stackings, both of which contributing a weight of one to the destroyed-set of H′.
Proof. The weight assignment only involves the weight of S
1
2
H. If S
1
2
H is destroyed
by only H and H′, from Lemma 10, S
1
2
H cannot be adjacent to a stacking which
contributes a weight of one to H′. If S
1
2
H is destroyed by H, H′ and H′′, the
weight assignment cannot generate a new stacking which contributes a weight
of 1 to H′ or H′′.
⊓⊔
Lemma 11 indicates that, if H′ cannot be replaced by 1-substitution after the
termination of Algorithm 1, then it still cannot be replaced by any 1-substitution
local improvement, even if S
1
2
H is singly destroyed by H′.
Now, we show that, after the weight reassignment, H′ remains safe. It can be
shown similarly that H′′ would be safe as well, since viewing from H, the role
of H′ and H′′ is equivalent.
Lemma 12. If p ≥3, then H′ is safe.
Proof. From Lemma 11, no matter how much weight H′ contributes from the
weight reassignment, there would not be two new continuous stackings, both
of which contributes a weight of one to the destroyed-set of H′. Since H′ can-
not be replaced by any 1-substitution local improvement, Lemma 1 still holds.
Lemma 3 holds since we only keep long helix in our solution at the termination
of Algorithm 1. By the same argument as in Lemma 5, we can conclude that the
lemma holds.
⊓⊔
Lemma 13. If p = 2 and S
1
2
H is destroyed by only H and H′, then H′ is safe.
Proof. From Lemma 11, no matter how much weight H′ contributes from the
weight reassignment, there would not be two continuous stackings, both of which
contributes a weight of one to the destroyed-set of H′. Then, following Lemma 8,
H′ is safe.
⊓⊔
Lemma 14. If p = 2 and if S
1
2
H is destroyed by H, H′ and H′′, then H′ is safe.
Proof. Omitted due to space constraint.
⊓⊔
Lemma 15. At the termination of Algorithm 2, every short helix H is safe.
Proof. There would not be any long helix left after Algorithm 1, hence a short
helix can only destroy some other short helices. A short helix occupies four bases,
then it can destroy at most 4 short helices in the optimal solution. By the 1-
substitution local improvement process, among the destroyed short helices, at
most one of them could be singly destroyed. All the other short helices must be
multiply destroyed, each of which contributing a weight of 1/2 to the destroy set
of H. Hence, we have,
W(DSH) ≤1 + 3/2 = 2.5,
and we are done.
⊓⊔

586
A. Zhou et al.
Theorem 2. Our algorithm approximates the maximum base pair stackings
within a factor 5/2.
Proof. At the termination of Algorithm 2, all the stackings in the optimal solu-
tion would be destroyed. (Recall that if some stackings are also found by our
algorithm, we consider them to be destroyed by themselves.) If a stacking in the
optimal solution is singly destroyed by some helix H, it contributes a weight of
1 to W(DSH); if a stacking in the optimal solution is multiply destroyed, it con-
tributes a weight of 1/2 to two of the helices destroying it; if the weight of some
stacking in the optimal solution is reassigned, its total contribution remains 1.
Consequently, the total weight assigned to the destroyed-set of all the helices
in our solution is exactly the value of the optimal solution. From Lemmas 12,
13, 14, 15, the weight contributed by each helix is at most 2.5 times of its own
length. Since the total length of helices is exactly the value in our solution, then
we are done.
⊓⊔
5
Concluding Remarks
In this paper, we investigate the maximum base pair stackings problem, which is
a well-deﬁned combinatorial problem from RNA secondary structures prediction.
We obtain a 5/2-approximation by a two-stage local search method together with
an amortization analysis. A direction for future research is to further improve
the approximation factor. In our algorithm, we use only 1-substitutions and 2-
substitutions. In fact, as we discussed in this paper, using 1-substitutions alone
cannot reach this ratio.
Acknowledgments. This research is partially supported by NSF of China under
grant 61472222 and 61202014. Haitao Jiang is supported by Young Scholars Program
of Shandong University. Nan Liu is supported by he Foundation for Outstanding Young
Scientists in Shandong Province (project no. BS2014DX017), by the Doctoral Founda-
tion of Shandong Jianzhu University (project no. 0000601512). Haodi Feng is supported
by NSF of China under grant 61672325. Binhai Zhu is supported by NSF of China under
grant 61628207.
References
1. Akutsu, T.: Dynamic programming algorithms for RNA secondary structure pre-
diction with pseudoknots. Discrete Appl. Mathe. 104(1–3), 45–62 (2000)
2. Berman, P.: A d/2 approximation for maximum weight independent set in d-claw
free graphs. Nordic J. Comput. 7, 178–184 (2000)
3. Ieong, S., Kao, M.-Y., Lam, T.-W., Sung, W.-K., Yiu, S.-M.: Predicting RNA sec-
ondary structure with arbitrary pseudoknots by maximizing the number of stacking
pairs. J. Comput. Biol. 10, 981–995 (2003)
4. Jiang, M.: Approximation algorithms for predicting RNA secondary structures
with arbitrary pseudoknots. IEEE/ACM Trans. Comput. Biol. Bioinform. 7(2),
323–332 (2010)

Maximum Base Pair Stackings Problem
587
5. Lyngsø, R.B.: Complexity of pseudoknot prediction in simple models. In: D´ıaz, J.,
Karhum¨aki, J., Lepist¨o, A., Sannella, D. (eds.) ICALP 2004. LNCS, vol. 3142, pp.
919–931. Springer, Heidelberg (2004). doi:10.1007/978-3-540-27836-8 77
6. Lyngsø, R.B., Pedersen, C.N.S.: RNA pseudoknot prediction in energy based mod-
els. J. Comput. Biol. 7(3/4), 409–428 (2000)
7. Lyngsø, R.B., Zuker, M., Pedersen, C.N.S.: Fast evaluation of interval loops in rna
secondary structure prediction. Bioinformatics 15, 440–445 (1999)
8. Nussinov, R., Jacobson, A.B.: Fast algorithm for predicting the secondary structure
of single-stranded RNA. Proc. Nat. Acad. Sci. USA 77, 6309–6313 (1980)
9. Nussinov, R., Pieczenik, G., Griggs, J.R., Kleitman, D.J.: Algorithms for loop
matchings. SIAM J. Appl. Mathe. 35(1), 68–82 (1978)
10. Rivas, E., Eddy, S.R.: A dynamic programming algorithm for RNA structure pre-
diction including pseudoknots. J. Mol. Biol. 285(5), 2053–2068 (1999)
11. Sankoﬀ, D.: Simultaneous solution of the RNA folding, alignment and protose-
quence problems. SIAM J. Appl. Mathmatics 45, 810–825 (1985)
12. Tinoco, I., Borer, P.N., Dengler, B., Levine, M.D., Uhlenbeck, O.C., Crothers,
D.M., Gralla, J.: Improved estimation of secondary structure in ribonucleic acids.
Nat. New Biol. 246, 40–42 (1973)
13. Tinoco, I., Bustamante, C.: How RNA folds. J. Mol. Biol. 293, 271–281 (1999)
14. Uemura, Y., Hasegawa, A., Kobayashi, S., Yokomori, T.: Tree adjoining grammars
for RNA structure prediction. Theore. Comput. Sci. 210(2), 277–303 (1999)
15. Zuker, M., Sankoﬀ, D.: RNA secondary structures and their prediction. Bull. Math.
Biol. 46, 591–621 (1984)
16. Zuker, M., Stiegler, P.: Optimal computer folding of large RNA sequences using
thermodynamics and auxiliary information. Nucleic Acids Res. 9, 133–148 (1981)

CSoNet Papers

Cooperative Game Theory Approaches
for Network Partitioning
Konstantin E. Avrachenkov1(B), Aleksei Yu. Kondratev2,
and Vladimir V. Mazalov2
1 INRIA, 2004 Route des Lucioles, Sophia-Antipolis, France
k.avrachenkov@sophia.inria.fr
2 Karelian Research Center, Institute of Applied Mathematical Research,
Russian Academy of Sciences, 11, Pushkinskaya Street,
Petrozavodsk 185910, Russia
vmazalov@krc.karelia.ru
Abstract. The paper is devoted to game-theoretic methods for commu-
nity detection in networks. The traditional methods for detecting com-
munity structure are based on selecting denser subgraphs inside the net-
work. Here we propose to use the methods of cooperative game theory
that highlight not only the link density but also the mechanisms of clus-
ter formation. Speciﬁcally, we suggest two approaches from cooperative
game theory: the ﬁrst approach is based on the Myerson value, whereas
the second approach is based on hedonic games. Both approaches allow to
detect clusters with various resolution. However, the tuning of the reso-
lution parameter in the hedonic games approach is particularly intuitive.
Furthermore, the modularity based approach and its generalizations can
be viewed as particular cases of the hedonic games.
Keywords: Network partitioning · Community detection · Cooperative
games · Myerson value · Hedonic games
1
Introduction
Community detection in networks or network partitioning is a very important
topic which attracted the eﬀort of many researchers. Let us just mention several
main classes of methods for network partitioning. The ﬁrst very large class is
based on spectral elements of the network matrices such as adjacency matrix
and Laplacian (see e.g., the survey [23] and references therein). The second class
of methods, which is somehow related to the ﬁrst class, is based on the use of
random walks (see e.g., [1,2,6,16,18,20] for the most representative works in
this research direction.) The third class of approaches to network partitioning is
based on methods from statistical physics [3,21,22]. The fourth class, which is
probably most related to our approach, is based on the concept of modularity
and its various generalizations [4,9,19,24]. For a very thorough overview of the
community detection methods we recommend the survey [7].
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 591–602, 2017.
DOI: 10.1007/978-3-319-62389-4 49

592
K.E. Avrachenkov et al.
In essence, all the above methods (may be with some exception of the statis-
tical physics methods), try to detect denser subgraphs inside the network and do
not address the question: what are the natural forces and dynamics behind the
formation of network clusters. We feel that the game theory, and in particular,
cooperative game theory is the right tool to explain the formation of network
clusters.
In the present work, we explore two cooperative game theory approaches
to explain possible mechanisms behind cluster formation. Our ﬁrst approach
is based on the Myerson value in cooperative game theory, which particularly
emphasizes the value allocation in the context of games with interactions between
players constrained by a network. The advantage of the Myerson value is in
taking into account the impact of all coalitions. We use the eﬃcient method
developed in [14,15] based on characteristic functions to calculate quickly the
Myerson value in the network. We would like to mention that in [14] a network
centrality measure based on the Myerson value was proposed. It might be inter-
esting to combine node ranking and clustering based on the same approach such
as the Myerson value to analyze the network structure.
The second approach is based on hedonic games, which are games explaining
the mechanism behind the formation of coalitions. Both our approaches allow
to detect clusters with varying resolution and thus avoiding the problem of the
resolution limit [8,12]. The hedonic game approach is especially well suited to
adjust the level of resolution as the limiting cases are given by the grand coalition
and maximal clique decomposition, two very natural extreme cases of network
partitioning. Furthermore, the modularity based approaches can be cast in the
setting of hedonic games. We ﬁnd that this gives one more, very interesting,
interpretation of the modularity based methods.
Some hierarchical network partitioning methods based on tree hierarchy, such
as [9], cannot produce a clustering on one resolution level with the number of
clusters diﬀerent from the predeﬁned tree shape. Furthermore, the majority of
clustering methods require the number of clusters as an input parameter. In
contrast, in our approaches we specify the value of the resolution parameter
and the method gives a natural number of clusters corresponding to the given
resolution parameter.
In addition, our approach easily works with multi-graphs, where several edges
(links) are possible between two nodes. A multi-edge has several natural interpre-
tations in the context of social networks. A multi-edge can represent: a number of
telephone calls; a number of exchanged messages; a number of common friends;
or a number of co-occurrences in some social event.
The paper is structured as follows: in the following section we formally deﬁne
network partitioning as a cooperative game. Then, in Sect. 3 we present our ﬁrst
approach based on the Myerson value. The second approach based on the hedo-
nic games is presented in Sect. 4. In both Sects. 3 and 4 we provide illustrative
examples which explain the essence of the methods. Finally, Sect. 5 concludes
the paper with directions for future research.

Network Partitioning as Cooperative Game
593
2
Network Partitioning as a Cooperative Game
Let g = (N, E) denote an undirected multi-graph consisting of the set of nodes
N and the set of edges E. We denote a link between node i and node j as ij.
The interpretation is that if ij ∈E, then the nodes i ∈N and j ∈N have
a connection in network g, while ij /∈E, then nodes i and j are not directly
connected. Since we generally consider a multi-graph, there could be several
edges between a pair of nodes. Multiple edges can be interpreted for instance as
a number of telephone calls or as a number of message exchanges in the context
of social networks.
We view the nodes of the network as players in a cooperative game. Let
N(g) = {i : ∃j such that ij ∈g}. For a graph g, a sequence of diﬀerent nodes
{i1, i2, . . . , ik}, k ≥2, is a path connecting i1 and ik if for all h = 1, . . . , k −1,
ihih+1 ∈g. The length of the path l is the number of links in the path, i.e.
l = k −1. The length of the shortest path connecting i and j is distance between
i and j. Graph g on the set N is connected graph if for any two nodes i and j
there exists a path in g connecting i and j.
We refer to a subset of nodes S ⊂N as a coalition. The coalition S is
connected if any two nodes in S are connected by a path which consists of nodes
from S. The graph g′ is a component of g, if for all i ∈N(g′) and j ∈N(g′),
there exists a path in g′ connecting i and j, and for any i ∈N(g′) and j ∈N(g),
ij ∈g implies that ij ∈g′. Let N|g is the set of all components in g and let g|S
is the subgraph with the nodes in S.
Let g −ij denote the graph obtained by deleting link ij from the graph g
and g + ij denote the graph obtained by adding link ij to the graph g.
The result of community detection is a partition of the network (N, E) into
subsets (coalitions) {S1, ..., SK} such that Sk∩Sl = ∅, ∀k, l and S1∪...∪SK = N.
This partition is internally stable or Nash stable if for any player from coalition
Sk it is not proﬁtable to join another (possibly empty) coalition Sl. We also
say that the partition is externally stable if for any player i ∈Sl for whom it is
beneﬁtial to join a coalition Sk there exists a player j ∈Sk for whom it is not
proﬁtable to include there player i. The payoﬀdeﬁnition and distribution will
be discussed in the following two sections.
3
Myerson Cooperative Game Approach
In general, cooperative game of n players is a pair < N, v > where N =
{1, 2, . . . , n} is the set of players and v: 2N →R is a map prescribing for a
coalition S ∈2N some value v(S) such that v(∅) = 0. This function v(S) is the
total utility that members of S can jointly attain. Such a function is called the
characteristic function of cooperative game [13].
Characteristic function (payoﬀof coalition S) can be determined in diﬀerent
ways. Here we use the approach of [10,11,14,15], which is based on discounting
directed paths. The payoﬀto an individual player is called an imputation. The
imputation in this cooperative game will be Myerson value [14,15,17].

594
K.E. Avrachenkov et al.
Let < N, v > be a cooperative game with partial cooperation presented by
graph g and characteristic function v. An allocation rule Y describes how the
value associated with the network is distributed to the individual players. Denote
by Yi(v, g) the value allocated to player i from graph g under the characteristic
function v.
Myerson proposed in [17] the allocation rule
Y (v, g) = (Y1(v, g), . . . , Yn(v, g)),
which is uniquely determined by the following two axioms:
A1. If S is a component of g then the members of the coalition S ought to
allocate to themselves the total value v(S) available to them, i.e. ∀S ∈N|g

i∈S
Yi(v, g) = v (S) .
(1)
A2. ∀g, ∀ij ∈g both players i and j obtain equal payoﬀs after adding or deleting
a link ij,
Yi (v, g) −Yi (v, g −ij) = Yj (v, g) −Yj (v, g −ij) .
(2)
Let us determine the characteristic function by the following way
vg (S) =

K∈S|g
v (K) .
Then the Myerson value can be calculated by the formula
Yi (v, g) =

S⊂N\{i}
(vg (S ∪i) −vg (S)) s! (n −s −1)!
n!
,
(3)
where s = |S| and n = |N| .
Let us determine the characteristic function which is determined by the
scheme proposed by Jackson [11]: every direct connection gives to coalition S
the impact r, where 0 ≤r ≤1. Moreover, players obtain an impact from indirect
connections. Each path of length 2 gives to coalition S the impact r2, a path of
length 3 gives to coalition the impact r3, etc. So, for any coalition S we obtain
v (S) = a1r + a2r2 + · · · + akrk + · · · + aLrL =
L

k=1
akrk,
(4)
where L is a maximal distance between two nodes in the coalition; ak is the
number of paths of length k in this coalition. Set
v(i) = 0, ∀i ∈N.

Network Partitioning as Cooperative Game
595
In [15] it was proven that the Myerson value can be found by the following
simple procedure of allocation the general gain v(N) to each player i ∈N:
Stage 1. Two direct connected players together obtain r. Individually, they
would receive nothing. So, each of them receives at least r/2. If player i has
some direct connections then she receives the value r/2 times the number of
paths of length 1 which contain the node i.
Stage 2. Three connected players obtain r2, so each of them must receive r2/3,
and so on.
Arguing this way, we obtain the allocation rule of the following form:
Yi (v, g) = ai
1
2 r + ai
2
3 r2 + · · · +
ai
L
L + 1rL =
L

k=1
ai
k
k + 1rk,
(5)
where ai
k is the number of all paths of length k which contain the node i.
Example 1. Consider network of six nodes presented in Fig. 1 Below we show
how to calculate characteristic function for diﬀerent coalitions.
Fig. 1. Network of six nodes.
For the network N = {A, B, C, D, E, F} we ﬁnd L = 3, a1 = 9, a2 = 4, a3 = 4.
Consequently, the value of grand-coalition is
v (N) = 9r + 4r2 + 4r3.
For coalition S = {A, B, C, D} we have L = 2, a1 = 5, a2 = 2 and we obtain
v (S) = 5r + 2r2.
This way we can calculate the values of characteristic function for all coalitions
S ⊂N. After that we can ﬁnd the Myerson vector.
Example 1 (ctnd). Let us calculate the Myerson value for player A in Exam-
ple 1 using the allocation rule (5). Mark all paths which contain node A. The
paths of length 1 are: {A,B}, {A,C}, {A,D}, hence aA
1 = 3. The paths of length
2 are: {B,A,C}, {B,A,D}, {C,A,D}, {A,D,E}, {A,D,F}, so aA
2 = 5. The paths

596
K.E. Avrachenkov et al.
of length 3: {B,A,D,E}, {B,A,D,F}, {C,A,D,E}, {C,A,D,F}, so aA
3 = 4. Conse-
quently,
YA = 3
2r + 5
3r2 + r3.
Thus, we can propose the following algorithm for network partitioning based
on the Myerson value: Start with any partition of the network N = {S1, . . . , SK}.
Consider a coalition Sl and a player i ∈Sl. In cooperative game with partial
cooperation presented by the graph g|Sl we ﬁnd the Myerson value for player i,
Yi(g|Sl). That is reward of player i in coalition Sl. Suppose that player i decides
to join the coalition Sk. In the new cooperative game with partial cooperation
presented by the graph g|Sk ∪i we ﬁnd the Myerson value Yi(g|Sk ∪i). So, if
for the player i ∈Sl : Yi(g|Sl) ≥Yi(g|Sk ∪i) then player i has no incentive to
join to new coalition Sk, otherwise the player changes the coalition. The partition
N = {S1, . . . , SK} is the Nash stable if for any player there is no incentive to
move from her coalition. Notice that for unweighted graphs the deﬁnition of the
Myerson value implies that for any coalition it is always beneﬁcial to accept a
new player (of course, for the player herself it might not be proﬁtable to join
that coalition), the Nash stability (internal stability) in this game coincides with
the external stability.
Example 1 (ctnd). Let us clarify this approach on the network
N = {A, B, C, D, E, F}
presented in Fig. 1 Natural way of partition here is {S1 = (A, B, C), S2 =
(D, E, F)}. Let us determine under which condition this structure will present
the stable partition.
Suppose that characteristic function is determined by (4). For coalition S1
the payoﬀv(S1) = 4r. The payoﬀof player A is YA(g|S1) = r. Imagine that
player A decides to join the coalition S2.
Coalition S2 ∪A has payoﬀv(S2 ∪A) = 5r + 2r2. The imputation in this
coalition is YA(g|S2 ∪A) = r/2 + 2r2/3, YD(g|S2 ∪A) = 3r/2 + 2r2/3, YE(g|S2 ∪
A) = YF (g|S2 ∪A) = 3r/2 + r2/3. We see that for player A it is proﬁtable to
join this new coalition if r/2 + 2r2/3 > r, or r > 3/4. Otherwise, the coalitional
structure is stable.
Thus, for the network in Fig. 1 the Myerson value approach will give the
partition {S1 = (A, B, C), S2 = (D, E, F)} if r < 3/4 and, otherwise, it leads to
the grand coalition. This example already gives a feeling that the parameter r
can be used to tune the resolution of network partitioning. Such tuning will be
even more natural in the ensuing approach.
4
Hedonic Coalition Game Approach
There is another game-theoretic approach for the partitioning of a society into
coalitions based on the ground-breaking work [5]. We apply the framework of

Network Partitioning as Cooperative Game
597
Hedonic games [5] to network partitioning problem, particularly, specifying the
preference function.
Assume that the set of players N = {1, . . . , n} is divided into K coalitions:
Π = {S1, . . . , SK}. Let SΠ(i) denote the coalition Sk ∈Π such that i ∈Sk.
A player i preferences are represented by a complete, reﬂexive and transitive
binary relation ⪰i over the set {S ⊂N : i ∈S}. The preferences are additively
separable [5] if there exists a value function vi : N →R such that vi(i) = 0 and
S1 ⪰i S2 ⇔

j∈S1
vi(j) ≥

j∈S2
vi(j).
The preferences {vi, i ∈N} are symmetric, if vi(j) = vj(i) = vij = vji for
all i, j ∈N. The symmetry property deﬁnes a very important class of Hedonic
games.
As in the previous section, the network partition Π is Nash stable, if SΠ(i) ⪰i
Sk ∪{i} for all i ∈N, Sk ∈Π ∪{∅}. In the Nash-stable partition, there is no
player who wants to leave her coalition.
A potential of a coalition partition Π = {S1, . . . , SK} (see [5]) is
P(Π) =
K

k=1
P(Sk) =
K

k=1

i,j∈Sk
vij.
(6)
Our method for detecting a stable community structure is based on the fol-
lowing better response type dynamics:
Start with any partition of the network N = {S1, . . . , SK}. Choose any player
i and any coalition Sk diﬀerent from SΠ(i). If Sk ∪{i} ⪰i SΠ(i), assign node i
to the coalition Sk; otherwise, keep the partition unchanged and choose another
pair of node-coalition, etc.
Since the game has the potential (6), the above algorithm is guaranteed to
converge in a ﬁnite number of steps.
Proposition 1. If players’ preferences are additively separable and symmetric
(vii = 0, vij = vji for all i, j ∈N), then the coalition partition Π giving a local
maximum of the potential P(Π) is the Nash-stable partition.
One natural way to deﬁne a symmetric value function v with a parameter
α ∈[0, 1] is as follows:
vij =
⎧
⎨
⎩
1 −α, (i, j) ∈E,
−α, (i, j) /∈E,
0,
i = j.
(7)
For any subgraph (S, E|S), S ⊆N, denote n(S) as the number of nodes in
S, and m(S) as the number of edges in S. Then, for the value function (7), the
potential (6) takes the form
P(Π) =
K

k=1

m(Sk) −n(Sk)(n(Sk) −1)α
2

.
(8)
We can completely characterize the limiting cases α →0 and α →1.

598
K.E. Avrachenkov et al.
Proposition 2. If α = 0, the grand coalition partition ΠN = {N} gives the
maximum of the potential (8). Whereas if α →1, the maximum of (8) corre-
sponds to a network decomposition into maximal cliques. (A maximal clique is
a clique which is not contained in another clique.)
Proof. It is immediate to check that for α = 0 the grand coalition partition
ΠN = {N} gives the maximum of the potential (8), and P(ΠN) = m(N).
For values of α close to 1, the partition into maximal cliques Π
=
{S1, . . . , SK} gives the maximum of (8). Indeed, assume that a player i from
the clique SΠ(i) of the size m1 moves to a clique Sj of the size m2 < m1. The
player i ∈SΠ(i) and Sj are connected by at most m2 links. The impact on P(Π)
of this movement is not higher than
m2(1 −α) −(m1 −1)(1 −α) ≤0.
Now, suppose that player i from the clique SΠ(i) moves to a clique Sj of the
size m2 ≥m1. The player i ∈SΠ(i) is connected with the clique Sj by at most
m2 −1 links. Otherwise, it contradicts the fact that Π is maximal clique cover
and the clique Sj can be increased by adding of i. If i has an incentive to move
from SΠ(i) to the clique Sj, then for new partition the sum (8) would be not
higher than for partition Π by
m2 −1 −m2α −(m1 −1)(1 −α) = m2 −m1 −α(m2 −m1 + 1).
For α close to 1, this impact is negative, so there is no incentive to join the
coalition Sj.
The grand coalition and the maximal clique decomposition are two extreme
partitions into communities. By varying the parameter α we can easily tune the
resolution of the community detection algorithm.
Example 2. Consider graph G = G1 ∪G2 ∪G3 ∪G4, which consists of n = 26
nodes and m = 78 edges (see Fig. 2) This graph includes 4 fully connected sub-
graphes: (G1, 8, 28) with 8 vertices connected by 28 links, (G2, 5, 10), (G3, 6, 15)
and (G4, 7, 21). Subgraph G1 is connected with G2 by 1 edge, G2 with G3 by 2
edges, and G3 with G4 by 1 edge.
Firstly, ﬁnd the potentials (8) for large-scale decompositions of G for any
parameter α ∈[0, 1]. It is easy to check, that P(G) = 78 −325α, P({G1, G2 ∪
G3∪G4}) = 77−181α, P({G1, G2∪G3, G4}) = 76−104α, P({G1, G2, G3, G4}) =
74 −74α.
Other coalition partitions give smaller potentials: P({G1 ∪G2, G3 ∪G4}) =
76 −156α < 76 −104α, P({G1 ∪G2 ∪G3, G4}) = 77 −192α < 77 −181α,
P({G1, G2, G3 ∪G4}) = 75 −116α < 76 −104α, P({G1 ∪G2, G3, G4}) = 75 −
114α < 76 −104α.
We solve a sequence of linear inequalities in order to ﬁnd maximum of the
potential for all α ∈[0, 1]. The result is presented in the table below.

Network Partitioning as Cooperative Game
599
Fig. 2. Graph with four fully connected subgraphs.
Nash-stable coalition partitions in Example 2.
α
Coalition partition
potential
[0, 1/144]
G1 ∪G2 ∪G3 ∪G4 78 −325α
[1/144, 1/77] G1, G2 ∪G3 ∪G4
77 −181α
[1/77, 1/15]
G1, G2 ∪G3, G4
76 −104α
[1/15, 1]
G1, G2, G3, G4
74 −74α
Example 1 (ctnd). Note that for the unweighted version of the network exam-
ple presented in Fig. 1, there are only two stable partitions: Π = N for small
values of α ≤1/9 and Π = {{A, B, C}, {D, E, F}} for α > 1/9.
Example 3. Consider the popular example of the social network from Zachary
karate club (see Fig. 3). In his study [25], Zachary observed 34 members of a
karate club over a period of two years. Due to a disagreement developed between
the administrator of the club and the club’s instructor there appeared two new
clubs associated with the instructor (node 1) and administrator (node 34) of
sizes 16 and 18, respectively.
Fig. 3. Zachary karate club network.

600
K.E. Avrachenkov et al.
The authors of [9] divide the network into two groups of roughly equal size
using the hierarchical clustering tree. They show that this split corresponds
almost perfectly with the actual division of the club members following the
break-up. Only one node, node 3, is classiﬁed incorrectly.
Let us now apply the hedonic game approach to the karate club network.
We start from the ﬁnal partition N = {S15, S19}, which was obtained in [9]. We
calculate the potential for grand-coalition P(N) = 78 −561α and for partition
P(S15, S19) = 68 −276α. From the equation P(N) = P(S15, S19) we obtain the
cutoﬀpoint α = 2/57. So, if α < 2/57, P(N) is larger than P(S15, S19), so
partition {S15, S19} is not Nash-stable. For α = 2/57 the potential increases if
the node 3 moves from S19 to S15. For the new partition P(S16, S18) = 68−273α.
Comparing with potential of the grand coalition we obtain α = 5/144. For α =
5/144 the potential increases if the node 10 moves to S16. Now P(S17, N \S17) =
68 −272α and the new cutoﬀpoint is α = 10/289. Finally, in order to ﬁnd the
upper bound of the resolution parameter, we have to check that for any player
there is no incentive to move from her coalition to the empty coalition.
Thus, for 1/16 ≥α ≥10/289 the Nash-stable partition is
S17 = {1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 17, 18, 20, 22} ∪{N \ S17}.
Notice that in this new partition the node 3 belongs to the “right” coaltion.
Another natural approach to deﬁne a symmetric value function is, roughly
speaking, to compare the network under investigation with the conﬁguration
random graph model. The conﬁguration random graph model can be viewed as
a null model for a network with no community structure. Namely, the following
value function can be considered:
vij = βij

Aij −γ didj
2m

,
(9)
where Aij is a number of links between nodes i and j, di and dj are the degrees
of the nodes i and j, respectively, m = 1
2

l∈N dl is the total number of links in
the network, and βij = βji and γ are some parameters.
Note that if βij = β, ∀i, j ∈N and γ = 1, the potential (8) coincides with the
network modularity [9,19]. If βij = β, ∀i, j ∈N and γ ̸= 1, we obtain the general-
ized modularity presented ﬁrst in [22]. The introduction of the non-homogeneous
weights was proposed in [24] with the following particularly interesting choice:
βij = 2m
didj
.
The introduction of the resolution parameter γ allows to obtain clustering with
varying granularity and in particular this helps to overcome the resolution
limit [8].

Network Partitioning as Cooperative Game
601
Thus, we have now a game-theoretic interpretation of the modularity func-
tion. Namely, the coalition partition Π = {S1, . . . , SK} which maximises the
modularity
P(Π) =
K

k=1

i,j∈Sk,i̸=j

Aij −didj
2m

(10)
gives the Nash-stable partition of the network in the Hedonic game with the
value function deﬁned by (9), where γ = 1 and βij = β.
Example 1 (ctnd). For the network example presented in Fig. 1 we calculate
P(N) = 3/2, P({B, C}∪{A, D}∪{E, F}) = P({A, B, C, D}∪{E, F}) = 7/2 and
P({A, B, C} ∪{D, E, F}) = 5. Thus, according to the value function (9) with
γ = 1 and βij = β (modularity value function), Π = {{A, B, C}, {D, E, F}} is
the unique Nash-stable coalition.
Example 3 (ctnd). Numerical calculations show that the partition S17 ∪{N \
S17} gives the maximum of potential function (10). It means that this partition
is Nash stable.
5
Conclusion and Future Research
We have presented two cooperative game theory based approaches for network
partitioning. The ﬁrst approach is based on the Myerson value for graph con-
strained cooperative game, whereas the second approach is based on hedonic
games which explain coalition formation. We ﬁnd the second approach espe-
cially interesting as it gives a very natural way to tune the clustering resolution
and generalizes the modularity based approaches. Our near term research plans
are to test our methods on more social networks and to develop eﬃcient com-
putational Monte Carlo type methods.
Acknowledgements. This research is supported by Russian Humanitarian Science
Foundation (project 15-02-00352), Russian Fund for Basic Research (projects 16-51-
55006 and 17-11-01079), EU Project Congas FP7-ICT-2011-8-317672 and Campus
France.
References
1. Avrachenkov, K., Dobrynin, V., Nemirovsky, D., Pham, S.K., Smirnova, E.: Pager-
ank based clustering of hypertext document collections. In: Proceedings of ACM
SIGIR 2008, pp. 873–874 (2008)
2. Avrachenkov, K., El Chamie, M., Neglia, G.: Graph clustering based on mixing
time of random walks. In: Proceedings of IEEE ICC 2014, pp. 4089–4094 (2014)
3. Blatt, M., Wiseman, S., Domany, E.: Clustering data through an analogy to the
Potts model. In: Proceedings of NIPS 1996, pp. 416–422 (1996)
4. Blondel, V.D., Guillaume, J.L., Lambiotte, R., Lefebvre, E.: Fast unfolding of
communities in large networks. J. Stat. Mech. Theory Exp. 10, P10008 (2008)

602
K.E. Avrachenkov et al.
5. Bogomolnaia, A., Jackson, M.O.: The stability of hedonic coalition structures.
Games Econ. Behav. 38(2), 201–230 (2002)
6. Dongen, S.: Performance criteria for graph clustering and Markov cluster experi-
ments. CWI Technical report (2000)
7. Fortunato, S.: Community detection in graphs. Phys. Rep. 486(3), 75–174 (2010)
8. Fortunato, S., Barthelemy, M.: Resolution limit in community detection. Proc.
Nat. Acad. Sci. 104(1), 36–41 (2007)
9. Girvan, M., Newman, M.E.J.: Community structure in social and biological net-
works. Proc. Nat. Acad. Sci. USA 99(12), 7821–7826 (2002)
10. Jackson, M.O.: Allocation rules for network games. Games Econ. Behav. 51(1),
128–154 (2005)
11. Jackson, M.O.: Social and Economic Networks. Princeton University Press,
Princeton (2008)
12. Leskovec, J., Lang, K.J., Dasgupta, A., Mahoney, M.W.: Community structure in
large networks: Natural cluster sizes and the absence of large well-deﬁned clusters.
Internet Math. 6(1), 29–123 (2009)
13. Mazalov, V.: Mathematical Game Theory and Applications. Wiley, Hoboken (2014)
14. Mazalov, V., Avrachenkov, K., Trukhina, I.: Game-theoretic centrality measures
for weighted graphs. Fundamenta Informaticae 145(3), 341–358 (2016)
15. Mazalov, V.V., Trukhina, L.I.: Generating functions and the Myerson vector in
communication networks. Disc. Math. Appl. 24(5), 295–303 (2014)
16. Meila, M., Shi, J.: A random walks view of spectral segmentation. In: Proceedings
of AISTATS 2001
17. Myerson, R.B.: Graphs and cooperation in games. Math. Oper. Res. 2, 225–229
(1977)
18. Newman, M.E.J.: A measure of betweenness centrality based on random walks.
Proc. Nat. Acad. Sci. USA 27, 39–54 (2005)
19. Newman, M.E.J.: Modularity and community structure in networks. Soc. Netw.
103(23), 8577–8582 (2006)
20. Pons, P., Latapy, M.: Computing communities in large networks using random
walks. J. Graph Algorithms Appl. 10(2), 191–218 (2006)
21. Raghavan, U.N., Albert, R., Kumara, S.: Near linear time algorithm to detect
community structures in large-scale networks. Phys. Rev. E 76(3), 036106 (2007)
22. Reichardt, J., Bornholdt, S.: Statistical mechanics of community detection. Phys.
Rev. E 74(1), 016110 (2006)
23. von Luxburg, U.: A tutorial on spectral clustering. Stat. Comput. 17(4), 395–416
(2007)
24. Waltman, L., van Eck, N.J., Noyons, E.C.: A uniﬁed approach to mapping and
clustering of bibliometric networks. J. Inform. 4(4), 629–635 (2010)
25. Zachary, W.W.: An information ﬂow model for conﬂict and ﬁssion in small groups.
J. Anthropol. Res. 33(4), 452–473 (1977)

Chain of Inﬂuencers: Multipartite
Intra-community Ranking
Pavla Drazdilova(B), Jan Konecny, and Milos Kudelka
Department of Computer Science, Faculty of Electrical Engineering
and Computer Science, VˇSB - Technical University of Ostrava,
17. listopadu 15/2172, 708 33 Ostrava, Czech Republic
{pavla.drazdilova,jan.konecny.st,milos.kudelka}@vsb.cz
Abstract. Ranking of vertices is an important part of social network
analysis. However, thanks to the enormous growth of real-world net-
works, the global ranking of vertices on a large scale does not provide
easily comparable results. On the other hand, the ranking can provide
clear results on a local scale and also in heterogeneous networks where
we need to work with vertices of diﬀerent types. In this paper, we present
a method of ranking objects in a community which is closely related to
the analysis of heterogeneous information networks. Our method assumes
that the community is a set of several groups of objects of diﬀerent types
where each group, so-called object pool, contains objects of the same
type. These community object pools can be connected and ordered to the
chain of inﬂuencers, and ranking can be applied to this structure. Based
on the chain of inﬂuencers, the heterogeneous network can be converted
to a multipartite graph. In our approach, we show how to rank vertices
of the community using the mutual inﬂuence of community object pools.
In our experiments, we worked with a computer science research com-
munity. Objects of this domain contain authors, papers (articles), topics
(keywords), and years of publications.
Keywords: Ranking · Multipartite graph · Heteregonous network ·
Community
1
Introduction
Ranking of vertices is a task historically associated with the social network analy-
sis (SNA). The ﬁrst methods of measurement of vertex importance were based
on so-called centrality. Freeman [2] formalized three diﬀerent measures of vertex
importance: degree, closeness and betweenness centralities. With the enormous
growth of networks and, in particular, thanks to the Internet, the original cen-
tralities did not provide reliable results on a global scale, and new approaches
were investigated. As a key method, Page et al. published the so-called PageR-
ank [11]. This method is based on a simple recursive principle that deﬁnes the
important vertex as a vertex which is a neighbor of many other important ver-
tices. The idea of Pagerank was followed by other similar approaches focusing
on speciﬁc properties of vertices.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 603–614, 2017.
DOI: 10.1007/978-3-319-62389-4 50

604
P. Drazdilova et al.
With the growth of social and information networks, the ranking of vertices
loses, in part, its importance for the analysis of network structure. In addition
to the other, the original purpose of the ranking was to sort vertices by their
importance; it is impractical in networks with billions of vertices. This impor-
tance is, in particular, dependent on the perspective from which the network
vertices are assessed.
Networks are inherently heterogeneous. We can have a network of authors
and their papers, topics, and aﬃliation. We can have customers who buy various
products from diﬀerent categories at a diﬀerent time. Similarly, we can have
users who watch movies of various genres, with diﬀerent actors and which are
created by diﬀerent directors. In each of these examples, a group of vertices of
diﬀerent types can be found (authors, products, genres, etc.). The well-known
approach of analysis of heterogeneous information networks (HIN) was presented
by Han et al. [3].
The aim of our approach is to evaluate a community in a speciﬁc domain
(e.g., publication activities, e-shops, the Internet movie database). As the com-
munity, we understand several groups of vertices of diﬀerent types. Moreover,
these individual groups of vertices (pools containing objects of the same type)
are connected in pairs (see Fig. 1). By the evaluation the community we under-
stand the answer to a simple question: “What value has a selected vertex from a
selected group in the community?” As we show in this paper, this question leads
to a more precise formulation of the problem. Its solution is based on organizing
pools to Chain of Inﬂuencers and the subsequent analysis of the multipartite
graph. In our experiment with a community from the publication domain, we
demonstrate how the diﬀerent pools in the chain of inﬂuencers aﬀect the evalu-
ation of the vertices.
Fig. 1. Selected domain pools based on DBLP data organized, as a community, to
complete graph of pools. A base pool is authors from one computer science department.
Vertices represent domain pools and the edges between vertices represent relations
between pools. Closed chain of infuencers is represented by a sequence APYA.

Chain of Inﬂuencers: Multipartite Intra-community Ranking
605
2
Related Work
Ranking of vertices is an important data mining task in network analysis. Many
ranking methods have been proposed for homogeneous networks. For example,
PageRank [11] evaluates the importance of vertices through a random walk
process in the graph of the Web. HITS (Hyperlink-Induced Topic Search) [6]
ranks objects in the directed network using authority and hub scores. Authority
score estimates the value of the content of the object, whereas hub scores mea-
sure the value of its links to other objects. SimRank [5] measures the similarity of
the structural context in which objects occur, based on their relationships with
other objects. Authors compute a measure that says “two objects are similar if
they are related to similar objects.”
These approaches consider the same type of objects in homogeneous net-
works, and they cannot be applied in heterogeneous networks with diﬀerent types
of objects. To rank tweets eﬀectively by capturing the semantics and importance
of diﬀerent linkages, Huang et al. [4] propose the Tri-HITS model. This model
makes use of heterogeneous networks composed of tweets, users, and web docu-
ments to rank tweets.
Some researchers focused on the co-ranking of multiple types of objects. Zhou
et al. [20] co-rank authors and their publications by coupling two random walk
processes. Sun et al. [17] address the problem of generating clusters for a speciﬁed
type of objects, as well as ranking information for all types of objects based on
these clusters in a heterogeneous information network. They performed experi-
ments on the bi-type information network extracted from DBLP. Soulier et al.
[15] propose a bi-type entity ranking algorithm to rank documents and authors in
a bibliographic network jointly by combining content-based and network-based
features. These methods can rank diﬀerent types of objects existing in heteroge-
neous information networks; however, it is restricted to bipartite graphs. Multi-
Rank [7] determines the importance of both objects and relations simultaneously
for multi-relational data.
In recent years many articles with analysis of HIN have been published
[1,9,18,19]. As a feature of HIN, the links connect diﬀerent types of objects con-
taining semantics. The meta path [16], connecting object types via a sequence
of relations, has been widely used to capture relation semantics. Liu et al. [8]
propose a ranking method with pseudo-relevance feedback by investigating some
hypothesis-driven meta-paths on the scholarly heterogeneous graph. HeteSim is
proposed by Shi et al. [12] to measure the relevance scores of heterogeneous
objects in HIN. HRank [13] is based on a path-constrained random walk process
and it can simultaneously determine the importance of objects and constrained
meta-paths by applying the tensor analysis. A survey of models and methods
for analysis of heterogeneous information networks is in [14].
A new network data model is proposed in [10] - a Network of Networks
(NoN), where each vertex of the main network itself can be further represented
as another (domain-speciﬁc) network. Authors formulate ranking tasks in this
model as a regularized optimization problem.

606
P. Drazdilova et al.
2.1
Terminology
We have designed our terminology diﬀerently from the terminology used by Han
et al. in [3]. Our terminology is concentrated on the heterogeneous networks
where the relation between groups of objects is symmetric. The following deﬁn-
itions make it easier to navigate through the rest of the text.
Deﬁnition 1. Domain is a set of objects of diﬀerent types.
Deﬁnition 2. Domain Pool is a set of objects of the same type which are inde-
pendent of each other.
Deﬁnition 3. Community Pool (or only Pool) is a non-empty subset of the
domain pool.
Deﬁnition 4. Community is a set of community pools where:
– One pool is selected as a Base Pool.
– Each of the other community pools contains all objects from the corresponding
domain pool which are adjacent to at least one object of the base pool.
The community can be described as a complete graph where vertices are the
pools and edges are symmetrical relations between them.
Deﬁnition 5. Object Rank is an evaluation of an object regarding its impor-
tance in the community.
Deﬁnition 6. Ranked Pool is a pool in which each object has its object rank.
Deﬁnition 7. Two diﬀerent pools are in Inﬂuencer-Inﬂuencee relation when
object ranks of objects in the pool Inﬂuencee depend on object ranks of objects in
the pool Inﬂuencer.
Deﬁnition 8. Chain of Inﬂuencers is a sequence of at least three pools such
that:
1. Consecutive pools are diﬀerent.
2. Two consecutive pools are in the relation Inﬂuencer-Inﬂuencee.
3. The ﬁrst and the last pools are the same.
Example 1. Let the above-deﬁned domain is a research area. As a base pool,
for instance, we can select people of an academic or research department. Other
pools can be papers, topics, and years; all objects of all of the pools are adjacent
to at least one person from the department. The community contains all authors
of the department, all papers/articles in which these authors participated in, all
topics which they deal with, and all years when they published results. We can
set a chain of inﬂuencers as this sequence of pools: authors →papers →topics
→years →authors. The inﬂuencer-inﬂuencee relations are between authors and
papers, papers and topics, topics and years, and years and authors. Now, we
are able to calculate individual rankings for all of these pools (authors, papers,
topics, years).

Chain of Inﬂuencers: Multipartite Intra-community Ranking
607
2.2
Multipartite Graph
We use a heterogeneous network to describe a domain. A heterogeneous network
contains diﬀerent types of objects and diﬀerent types of links. We focus on
heterogeneous networks where objects in diﬀerent pools can be connected via
diﬀerent types of links, and objects in the same pool are not connected.
Let a heterogeneous network is represented by multipartite undirected
graph G = ({V1, . . . , Vn}, {E12 . . . , E(n−1)n}) where Vi are pools, and edges
Eij between objects of two pools (Vi, Vj) have a diﬀerent type than edges
Ekl between objects belonging to another pair of pools (Vk, Vl). A biadja-
cency matrix of a subgraph (Vi, Vj, Eij) corresponds to heterogeneous relation
Rij ⊆Vi · Vj.
An example of a heterogeneous network based on DBLP data1 with selected
domain pools is shown in Fig. 1. Figure 2 contains a network representing a
community extracted from this data and the APYA chain of inﬂuencers.
Fig. 2.
On the left is an example of a heterogeneous network with heterogeneous
vertices and edges. A multipartite graph on the right represents the APYA chain of
inﬂuencers. A vertex a1 is inﬂuencer of vertices p1, p2 and one vertex p3 is inﬂuencee
of vertices a2, a3.
3
Ranking Algorithm
The proposed ranking method is based on composing relations between domain
pools. This composition of relations corresponds to a certain chain of inﬂuencers
on the complete graph representing a heterogeneous network. Each speciﬁc chain
of inﬂuencers over the heterogeneous network is projected into a speciﬁc way
of ranking of all the community pools belonging to the chain. Each pool is
represented by a vector of the dimension corresponding to the number of objects
1 This data is freely available at http://dblp.org/xml/release/

608
P. Drazdilova et al.
in the pool. In the beginning, the rank of each object is set to any initial value;
this value is 1 in our case.
Ranked pools are a result of the iteration process described in Algorithm 1.
This process recalculates pool (vector) ranking in each step of the algorithm.
3.1
Mathematical Formulation
Let us have a selected chain of inﬂuencers V1
R1,2
−−−→V2
R2,3
−−−→. . .
Rk−1,k
−−−−→Vk where
V1, . . . , Vk are the community pools and R1,2, . . . , Rk−1,k are biadjacency matri-
ces corresponding to relations between adjacent pools. We can deﬁne ranking
vector ri for each community pool Vi in the chain of inﬂuencers where |Vi| = |ri|.
Now we can count for vector ri the corresponding vector r
′
i+1:
r
′
i+1 = ri · Ri,i+1
(1)
Next, we need to normalize the resulting vector r′
i+1, and set it as the new vector
ri+1:
r
′
i+1 =
r′
i+1
max(r
′
i+1)
(2)
Vector ri+1 is now a ranking vector of the pool Vi+1. This recounting is repeated
until we reach the end of the chain of inﬂuencers. Then we use the last ranking
vector for recounting the ranking vector of the ﬁrst pool in the chain of inﬂu-
encers. The process of recounting ranks for each vector in the chain is considered
as one iteration of Algorithm 1.
input : Set of biadjacency matrices Ri,i+1 representing relations between each
consecutive community pools, set of vectors ri = (1, . . . , 1) where
|ri| = |Vi| for i = 1, . . . , k and k equals to number of community pools
and θ is a accuracy.
output: Ranked pools Vi
while Consecutive iteration of ri diﬀer more than θ do
for i = 1, 2, . . . k do
ri+1 = ri · Ri,i+1
Normalize(ri+1)
if (i+1) mod k = 1 then
r1 = ri+1
endif
end
end
Algorithm 1. Pseudo-code of the ranking algorithm

Chain of Inﬂuencers: Multipartite Intra-community Ranking
609
4
Experiments
We extracted the experimental dataset from the DBLP data. The dataset con-
tains authors, their publications, topics and, years; the base community pool is
53 authors from the Department of Computer Science and Electrical Engineer-
ing at Stanford University, Stanford, California. Other community pools - 792
topics, 4231 papers/articles, and 42 years -were chosen regarding objects rela-
tionship with at least one author. If the paper/article had no topic, the special
topic was used for this case (“no topic” keyword). Most popular keywords repre-
senting topics in this paper can be found at FacetedDBLP2; topics were detected
in titles of papers/articles from the experimental dataset. We extracted relations
from the dataset for each pair of the community pools.
Next, we had to select the chains of inﬂuencers for ranking. There were many
possibilities. We could e.g. deﬁne ranking methods like APA (authors →papers
→authors) that rank authors only by their publications and publications by
their authors. We could also create more, e.g. APYTA (authors →papers →
years →topics →authors) which have more aspects inﬂuencing the ranking of
authors, publications, years, and topics. In the end, we ordered authors by the
number of publications and could compare this simple ordering with the results
of our methods.
A list of further used chains of inﬂuencers: AYA (authors →years →authors),
ATA (authors →topics →authors), ATPA (authors →topics →papers →
authors), APTYA (authors →papers →topics →years →authors), and the
longest chain of inﬂuencers AYAPATA (authors →years →authors →papers
→authors →topics →authors).
4.1
Experiment: AYA Ranking
We can see results for AYA ranking in Table 1. There are top 5 authors, top
5 years, rA is the ranking of authors, and rY is the ranking of years. The AYA
ranking adds more importance to authors publishing in many highly ranked
years. Since each year is ranked, an author with a smaller number of years can
have a higher ranking than an author who was active in numerous years with a
lower ranking. It is important to say that there is no notion of how many times
an author did publish in the speciﬁc year since this method uses a binary matrix
of heterogeneous relation. However, in more complex chains with more pools, the
composition of relations results in a more complex multipartite graph in which
this quantitative information is also indirectly taken into account.
4.2
Experiment: ATPA Ranking
In Table 2, there are results for a more complex method ATPA. This method
considers authors as more important if they use more popular topics in their
2 http://dblp.l3s.de/browse.php?browse=mostPopularKeywords.

610
P. Drazdilova et al.
Table 1. Ranking of AYA chain of inﬂuencers – Top 5
Authors
rA
Year rY
H. Garcia-Molina 1.00 2011 1.00
M. Horowitz
0.98 2015 0.99
L. J. Guibas
0.97 2013 0.96
P. Hanrahan
0.97 2010 0.95
J. Widom
0.97 2008 0.94
publications. The ﬁnding is that top 5 authors were using almost all top 300 best-
ranked topics. It is interesting that, except for prof. J. Guibas who is the author
of the second best-ranked publication, top 5 publications were not written by top
5 authors. This is because an author with a smaller number of topics can have
top-ranked publications if she/he uses interesting topics in these publications
but can still be considered less inﬂuential in comparison with other authors who
used much more topics in their publications.
Table 2. Ranking of ATPA chain of inﬂuencers – Top 5
Author
rA
Topics
rT
Publications
rP
L. J. Guibas
1.00
no topic
1.00
Eﬃcient randomized web-cache . . .
1.00
H. Garcia-Molina
0.91
optimization
0.86
Learning Large-Scale Dynamic . . .
0.98
J. Widom
0.56
applications
0.83
Time and Cost-Eﬃcient Modeling . . .
0.92
O. Khatib
0.49
eﬃciency
0.8
Compressed Sensing and . . .
0.91
D. Boneh
0.48
algorithms
0.79
A scalable parallel framework . . .
0.83
Table 3 contains a summary of top 5 authors, topics, and papers. In the left
part, there are numbers for the authors in top 5. #AT is the number of topics
which the author used, #AP is the number of papers which the author published,
and #AY is the number of years in which the author published. In the middle
part, there are numbers for the topic in the top 5. #TA means how many authors
used the topic and #TP means how many papers deal with the topic. In the right
part, there are the numbers for the papers in the top 5. #PA means how many
co-authors created the paper and #PT means how many topics were used in the
title of the paper.
4.3
Experiment: APTYA Ranking
Results for method APTYA are shown in Table 4. In this method, authors are
ranked by the importance of their publications, topics and, years. We can say
that an author is more important if she/he published in important years where
many important topics were published in many important publications.

Chain of Inﬂuencers: Multipartite Intra-community Ranking
611
Table 3. Summary of quantitative indicators of ATPA – Top 5
#AT
#AP
# AY
#TA # TP
#PA #PT
198
382
40
44
349
1
7
206
360
35
34
116
1
5
135
178
31
34
100
1
5
89
154
28
32
130
1
6
127
221
23
32
108
1
5
Table 4. Ranking for APTYA chain of inﬂuencers – Top 5
Author
rA
Publication
rP
Topic
rT
Year
rY
H. Garcia-Molina
1.00
The case for RAMClouds. . .
1.00
no topic
1.00
2013
1.00
L. J. Guibas
0.98
The case for RAMCloud
1.00
systems
0.41
2011
0.98
M. Horowitz
0.97
CSEL: Securing a Mote for 20. . .
0.7
network
0.38
2007
0.95
J. Widom
0.96
The Performance Impact. . .
0.59
eﬃciency
0.36
2012
0.95
P. Hanrahan
0.96
The Stanford FLASH Multi. . .
0.59
algorithms
0.3
2008
0.93
4.4
Experiment: AYAPATA Ranking of Authors
An interesting aspect of the chain of inﬂuencers is the possibility of repeating
pools. If we choose chain AYAPATA, then we have three diﬀerent author’s rat-
ings. Each of these rankings is most aﬀected by one of the Y-P-T pools. Moreover,
each of the pools Y-P-T is inﬂuenced by another ranking of the authors. When
we use this chain of inﬂuencers, we will get more varied intra-community rank-
ing. Rankings of authors for the AYAPATA chain is in Table 5. Columns A1.TA
and rT A contain the names of authors and their ﬁrst ranking by the chain of
inﬂuencers AYAPATA, columns A2.YA and rY A contain the names of authors
and their second ranking by the chain of inﬂuencers AYAPATA and columns
A3.PA and rP A contain the names of authors and their third ranking in the
same chain of inﬂuencers.
Table 5. Rankings of authors by AYAPATA chain of inﬂuencers – Top 5
A1.TA
rT A
A2.YA
rY A
A3.PA
rP A
H. Garcia-Molina 1.00 H. Garcia-Molina 1.00 H. Garcia-Molina 1.00
L. J. Guibas
0.96 L.J. Guibas
0.98 L. Guibas
0.98
J. Widom
0.73 M. Horowitz
0.97 J. Widom
0.51
M. Horowitz
0.67 P. Hanrahan
0.96 D. Boneh
0.50
P. Hanrahan
0.66 J. Widom
0.95 M. Horowitz
0.38

612
P. Drazdilova et al.
4.5
Correlations Between Rankings of Authors
In this section, we show information about the similarity between the pre-
sented methods. This similarity is expressed by Spearman’s correlations coeﬃ-
cient which was evaluated on an author’s ranking created by diﬀerent methods.
Spearman’s rank correlation coeﬃcient ρ (rho) is a nonparametric measure of
rank correlation (statistical dependence between the ranking of two variables -
for example between the ranking of authors by the method AYA and the method
ATPA).
Table 6. Spearmans correlation coeﬃcient between presented methods for authors
ranking.
C. CoeﬀATA A1.TA APTYA AYA A2.YA APA ATPA A3.PA P.Count
ATA
1.00
0.99
0.84
0.83
0.83
0.68
0.94
0.92
0.9
A1.TA
0.99
1.00
0.86
0.85
0.85
0.67
0.95
0.94
0.92
APTYA 0.84
0.86
1.00
0.99
0.99
0.57
0.86
0.94
0.88
AYA
0.83
0.85
0.99
1.00
0.99
0.57
0.86
0.94
0.87
A2.YA
0.83
0.85
0.99
0.99
1.00
0.57
0.86
0.94
0.88
APA
0.68
0.67
0.57
0.57
0.57
1.00
0.64
0.65
0.63
ATPA
0.94
0.95
0.86
0.86
0.86
0.64
1.00
0.96
0.96
A3.PA
0.92
0.94
0.94
0.94
0.94
0.65
0.96
1.00
0.97
P.Count 0.9
0.92
0.88
0.87
0.88
0.63
0.96
0.97
1.00
We compared methods presented in the experiments section in Table 6.
P.Count represents authors sorted by the number of their publications. A1.TA
represents the ﬁrst ranking of authors in the chain of inﬂuencers AYAPATA,
A2.YA represents the second ranking of authors and A3.PA represents the third
ranking of authors in the same chain of inﬂuencers. The blue highlighted sub-
matrices correspond to the strongly correlated rankings of authors by diﬀerent
methods. We can see that AYA and APTYA and A2.YA have a high correlation.
This can be explained because, in all of these chain of inﬂuencers, the pool of
years is the inﬂuencer of the pool of authors. We can also see that the ordering
by P.Count best correlates with A1.TA and ATPA. Both these methods depend
on the number of publications. The ranking by APA least correlates with all
other methods. The reason is that some authors do not have co-authors, and the
graph corresponding to composite relation RAP ◦RP A is not connected. More-
over, the proposed method ranks, simultaneously, sets of authors who do not
have common publications. A better solution is to use more pools in the chain
of inﬂuencers to ensure a connected graph and obtain a more complex ranking
for all the pools in the chain.

Chain of Inﬂuencers: Multipartite Intra-community Ranking
613
5
Conclusion
In the paper, we introduced ranking of vertices of a multipartite graph represent-
ing a heterogeneous network. We understand this multipartite graph as a repre-
sentation of the community which is a composition of objects of diﬀerent types.
Groups of objects of the same type, pools, are organized to the complete graph.
Edges of this graph represent symmetrical relationships between pools. For the
purpose of ranking of vertices, we deﬁned “chain of inﬂuencers” represented by
the sequence (graph walk) of pools. Ranking of vertices in one pool in the chain
is most inﬂuenced by the ranking of vertices of the previous pool. We described
the theoretical background of our approach and performed experiments with the
community from the publication domain. The experimental community we used
contained authors, topics, papers/articles, and a temporal aspect. Our method
ranks all the community pools in the chain of inﬂuencers. We compared the
ranking authors resulting from various chains of inﬂuencers with a simple quan-
titative evaluation based on numbers of published papers/articles. As a major
contribution, we showed that the chain of inﬂuencers provides a straightforward
and understandable alternative to this quantitative evaluation. This alternative
takes into account other aspects of the intra-community evaluation.
Further research will be focused on the application of the presented approach
in other domains and on the analysis of the impact of ranking on the relationships
inside the community.
Acknowledgement. This work was supported by the Czech Science Foundation
under the grant no. GA15-06700S, and by the projects SP2017/100 and SP2017/85
of the Student Grant System, VˇSB-Technical University of Ostrava.
References
1. Chen, J., Dai, W., Sun, Y., Dy, J.: Clustering and ranking in heterogeneous infor-
mation networks via gamma-poisson model. In: Proceedings of the 2015 SIAM
International Conference on Data Mining, pp. 424–432. SIAM (2015)
2. Freeman, L.C.: Centrality in social networks conceptual clariﬁcation. Soc. Netw.
1(3), 215–239 (1978)
3. Han, J., Sun, Y., Yan, X., Yu, P.S.: Mining heterogeneous information networks.
In: Tutorial at the 2010 ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD 2010), Washington, DC (2010)
4. Huang, H., Zubiaga, A., Ji, H., Deng, H., Wang, D., Le, H.K., Abdelzaher, T.F.,
Han, J., Leung, A., Hancock, J.P., et al.: Tweet ranking based on heterogeneous
networks. In: COLING, pp. 1239–1256 (2012)
5. Jeh, G., Widom, J.: Simrank: a measure of structural-context similarity. In: Pro-
ceedings of the Eighth ACM SIGKDD International Conference on Knowledge
Discovery And Data Mining, pp. 538–543. ACM (2002)
6. Kleinberg, J.M.: Authoritative sources in a hyperlinked environment. J. ACM
(JACM) 46(5), 604–632 (1999)
7. Li, X., Ng, M., Ye, Y.: Multirank: co-ranking for objects and relations in multi-
relational data. In: 17th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD-2011), pp. 1217–1225 (2011)

614
P. Drazdilova et al.
8. Liu, X., Yu, Y., Guo, C., Sun, Y.: Meta-path-based ranking with pseudo relevance
feedback on heterogeneous graph for citation recommendation. In: Proceedings of
the 23rd ACM International Conference on Conference on Information and Knowl-
edge Management, pp. 121–130. ACM (2014)
9. Liu, X., Yu, Y., Guo, C., Sun, Y., Gao, L.: Full-text based context-rich heteroge-
neous network mining approach for citation recommendation. In: Proceedings of
the 14th ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 361–370. IEEE
Press (2014)
10. Ni, J., Tong, H., Fan, W., Zhang, X.: Inside the atoms: ranking on a network of
networks. In: Proceedings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 1356–1365. ACM (2014)
11. Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking:
bringing order to the web. Technical report, Stanford InfoLab (1999)
12. Shi, C., Kong, X., Huang, Y., Philip, S.Y., Wu, B.: Hetesim: a general framework
for relevance measure in heterogeneous networks. IEEE Trans. Knowl. Data Eng.
26(10), 2479–2492 (2014)
13. Shi, C., Li, Y., Philip, S.Y., Wu, B.: Constrained-meta-path-based ranking in het-
erogeneous information network. Knowl. Inf. Syst. 49(2), 719–747 (2016)
14. Shi, C., Li, Y., Zhang, J., Sun, Y., Philip, S.Y.: A survey of heterogeneous infor-
mation network analysis. IEEE Trans. Knowl. Data Eng. 29(1), 17–37 (2017)
15. Soulier, L., Jabeur, L.B., Tamine, L., Bahsoun, W.: On ranking relevant entities
in heterogeneous networks using a language-based model. J. Am. Soc. Inf. Sci.
Technol. 64(3), 500–515 (2013)
16. Sun, Y., Han, J., Yan, X., Yu, P.S., Wu, T.: Pathsim: Meta path-based top-k sim-
ilarity search in heterogeneous information networks. Proc. VLDB Endow. 4(11),
992–1003 (2011)
17. Sun, Y., Han, J., Zhao, P., Yin, Z., Cheng, H., Rankclus, T.: Integrating clustering
with ranking for heterogeneous information network analysis. In: Proceedings of
the 12th International Conference on Extending Database Technology: Advances
in Database Technology, pp. 565–576. ACM (2009)
18. Tsai, M.-H., Aggarwal, C., Huang, T.: Ranking in heterogeneous social media. In:
Proceedings of the 7th ACM International Conference on Web Search and Data
Mining, pp. 613–622. ACM (2014)
19. Yu, X., Ren, X., Sun, Y., Gu, Q., Sturt, B., Khandelwal, U., Norick, B., Han, J.:
Personalized entity recommendation: a heterogeneous information network app-
roach. In: Proceedings of the 7th ACM International Conference on Web Search
and Data Mining, pp. 283–292. ACM (2014)
20. Zhou, D., Orshanskiy, S.A., Zha, H., Giles, C.L.: Co-ranking authors and docu-
ments in a heterogeneous network. In: Seventh IEEE International Conference on
Data Mining, ICDM 2007, pp. 739–744. IEEE (2007)

Inﬂuence Spread in Social Networks
with both Positive and Negative Inﬂuences
Jing (Selena) He1, Ying Xie1, Tianyu Du3, Shouling Ji2(B), and Zhao Li3
1 Department of Computer Science, Kennesaw State University,
Marietta, GA 30060, USA
{jhe4,yxie2}@kennesaw.edu
2 College of Computer Science and Technology, Zhejiang University,
Hangzhou, China
sji@zju.edu.cn
3 Alibaba Group, Hangzhou, China
{tianyu.dty,lizhao.lz}@alibaba-inc.com
Abstract. Social networks are important mediums for spreading infor-
mation, ideas, and inﬂuences among individuals. Most of existing
research works of social networks focus on understanding the character-
istics of social networks and spreading information through the “word of
mouth” eﬀect. However, most of them ignore negative inﬂuences among
individuals and groups. Motivated by alleviating social problems, such
as drinking, smoking, gambling, and inﬂuence spreading problems such
as promoting new products, we take both positive and negative inﬂu-
ences into consideration and propose a new optimization problem, named
the Minimum-sized Positive Inﬂuential Node Set (MPINS) selection, to
identify the minimum set of inﬂuential nodes, such that every node in
the network can be positively inﬂuenced by these selected nodes no less
than a threshold θ. Our contributions are threefold. First, we prove
that, under the independent cascade model considering both positive
and negative inﬂuences, MPINS is APX-hard. Subsequently, we present
a greedy approximation algorithm to address the MPINS selection prob-
lem. Finally, to validate the proposed greedy algorithm, extensive sim-
ulations and experiments are conducted on random Graphs and seven
diﬀerent real-world data sets representing small, medium, and large scale
networks.
Keywords: Inﬂuence spread · Social networks · Positive inﬂuential
node set · Greedy algorithm · Positive and negative inﬂuences
1
Introduction
A social network (e.g., Facebook, Google+, and MySpace) is composed of a
set of nodes that share the similar interest or purpose. The network provides a
powerful medium of communication for sharing, exchanging, and disseminating
information. With the emergence of social applications (such as Flickr, Wikis,
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 615–629, 2017.
DOI: 10.1007/978-3-319-62389-4 51

616
J. (Selena) He et al.
Netﬂix, and Twitter, etc.), there has been tremendous interests in how to eﬀec-
tively utilize social networks to spread ideas or information within a community
[1–8]. In a social network, individuals may have both positive and negative inﬂu-
ence on each other. For example, within the context of gambling, a gambling
insulator has positive inﬂuence on his friends/neighbors. Moreover, if many of
an individual’s friends are gambling insulators, the aggregated positive inﬂuence
is exacerbated. However, an individual might turn into a gambler, who brings
negative impact on his friends/neighbors.
One application of MPINS is described as follows. A community wants to
implement a smoking intervention program. To be cost eﬀective and get the
maximum eﬀect, the community wishes to select a small number of inﬂuential
individuals in the community to attend a quit-smoking campaign. The goal is
that all other individuals in the community will be positively inﬂuenced by the
selected users. Constructing an MPINS is helpful to alleviate the aforementioned
social problem, and it is also helpful to promote new products in the social
network. Consider the following scenario as another motivation example. A small
company wants to market a new product in a community. To be cost eﬀective
and get maximum proﬁt, the company would like to distribute sample products
to a small number of initially chosen inﬂuential users in the community. The
company wishes that these initial users would like the product and positively
inﬂuence their friends in the community. The goal is to have other users in
the community be positively inﬂuenced by the selected users no less than θ
eventually. To sum up, the speciﬁc problem we investigate in this work is the
following: given a social network and a threshold θ, identify a minimum-sized
subset of individuals in the network such that the subset can result in a positive
inﬂuence on every individual in the network no less than θ.
Hence, we explore the MPINS selection problem under the independent cas-
cade model considering both positive and negative inﬂuences, where individuals
can positively or negatively inﬂuence their neighbors with certain probabilities.
In this paper, ﬁrst we formally deﬁne the MPINS problem and then propose a
greedy approximation algorithm to solve it. Particularly, the main contributions
of this work are summarized as follows:
– Taking both positive and negative inﬂuences into consideration, we introduce
a new optimization problem, named the Minimum-sized Positive Inﬂuential
Node Set (MPINS) selection problem, for social networks, which is to identify
the minimum-sized set of inﬂuential nodes, that could positively inﬂuence
every node in the network no less than a pre-deﬁned threshold θ. We prove
that it is an APX-hard problem under the independent cascade model.
– We deﬁne a contribution function, which suggests us a greedy approximation
algorithm called MPINS-GREEDY to address the MPINS selection problem.
The correctness of the proposed algorithm is analyzed in the paper as well.
– We also conduct extensive simulations and experiments to validate our pro-
posed algorithm. The simulation and experiment results show that the pro-
posed greedy algorithm works well to solve the MPINS selection problem.

Inﬂuence Spread in Social Networks
617
More importantly, the solutions obtained by the greedy algorithm is very
close to the optimal solution of MPINS in small scale networks.
The rest of this paper is organized as follows: in Sect. 2, we review some related
literatures with remarking the diﬀerence. In Sect. 3, we ﬁrst introduce the net-
work model and then we formally deﬁne the MPINS selection problem and prove
its APX-hardness. The greedy algorithm and theoretical analysis on the correct-
ness of the algorithm are presented in Sect. 4. The simulation and experimental
results are presented in Sect. 5 to validate our proposed algorithm. Finally, the
paper is concluded in Sect. 6.
2
Related Work
In this section, we ﬁrst brieﬂy review the related works of social inﬂuence analy-
sis. Subsequently, we summarize some related literatures of the PIDS problem
and the inﬂuence maximization problem.
Inﬂuence maximization, initially proposed by Kempe et al. [1], is targeting at
selecting a set of users in a social network to maximize the expected number of
inﬂuenced users through several steps of information propagation [9]. A series of
empirical studies have been performed on inﬂuence learning [10,11], algorithm
optimizing [12,13], scalability promoting [14,15], and inﬂuence of group con-
formity [4,16]. Saito et al. predicted the information diﬀusion probabilities in
social networks under the independent cascade model in [17]. Tang et al. argued
that the eﬀect of the social inﬂuence from diﬀerent angles (topics) may be dif-
ferent. Hence, they introduced Topical Aﬃnity Propagation (TAP) to model
topic-related social inﬂuence on large social networks in [18,19]. Later, Tang et
al. [20] proposed a Dynamic Factor Graph (DFG) model to incorporate the time
information to analyze dynamic social inﬂuences.
Wang et al. ﬁrst proposed the PIDS problem under the deterministic lin-
ear threshold model in [21], which is to ﬁnd a set of nodes D such that every
node in the network has at least half of its neighbor nodes in D. Subsequently,
Zhu et al. proved that PIDS is APX-hard and proposed two greedy algorithms
with approximation ratio analysis in [22,23]. He et al. [24,25] proposed a new
optimization problem named the Minimum-sized Inﬂuential Node Set (MINS)
selection problem, which is to identify the minimum-sized set of inﬂuential nodes.
But they neglected the existence of negative inﬂuences.
To address the scalability problem of the algorithms in [1,26], Leskovec et al.
[27] presented a “lazy-forward” optimization scheme on selecting initial nodes,
which greatly reduces the number of inﬂuence spread evaluations. Laterly, Chen
et al. [28] showed that the problem of computing exact inﬂuence in social net-
works under both models are #P-Hard. They also proposed scalable algorithms
under both models, which are much faster than the greedy algorithms in [1,26].
Most recently, consider the data from both cyber-physical world and online social
network, [29,30] proposed methods to solve the problem of inﬂuence maximiza-
tion comprehensively.

618
J. (Selena) He et al.
However, all the aforementioned works did not consider negative inﬂuence
when they model the social networks. Besides taking both positive and nega-
tive inﬂuences into consideration, our work try to ﬁnd a minimum-sized set of
individuals that guarantees the positive inﬂuences on every node in the network
no less than a threshold θ, while the inﬂuence maximization problem focuses on
choosing a subset of a pre-deﬁned size k that maximizes the expected number of
inﬂuenced individuals. Since we study the MPINS selection problem under the
independent cascade model and take both positive and negative inﬂuences into
consideration, our problem is more practical. In addition, PIDS is investigated
under the deterministic linear threshold model.
3
Problem Deﬁnition and Hardness Analysis
In this section, we ﬁrst introduce the network model. Subsequently, we formally
deﬁne the MPINS selection problem and make some remarks on the proposed
problem. Finally, we analyze the hardness of the MPINS selection problem.
3.1
Network Model
We model a social network by an undirected graph G(V, E, P(E)), where V is the
set of n nodes, denoted by ui, and 0 ≤i < n. i is called the node ID of ui. An
undirected edge (ui, uj) ∈E represents a social tie between the pair of nodes.
P(E) = {pij | if (ui, uj) ∈E, 0 < pij ≤1, else pij = 0}, where pij indicates
the social inﬂuence between nodes ui and uj1. It is worth to mention that the
social inﬂuence can be categorized into two groups: positive inﬂuence and nega-
tive inﬂuence. For simplicity, we assume the links are undirected (bidirectional),
which means two linked nodes have the same social inﬂuence (i.e., pij value) on
each other.
3.2
Problem Deﬁnition
The objective of the MPINS selection problem is to identify a subset of inﬂuential
nodes as the initialized nodes. Such that, all the other nodes in a social network
can be positively inﬂuenced by these nodes no less than a threshold θ. For
convenient, we call the initial nodes been selected as active nodes, otherwise,
inactive nodes. Therefore, how to deﬁne positive inﬂuence is critical to solve
the MPINS selection problem. In the following, we ﬁrst formally deﬁne some
terminologies, and then give the deﬁnition of the MPINS selection problem.
Deﬁnition 1. Positive Inﬂuential Node Set (I). For social network G(V, E,
P(E)), the positive inﬂuential node set is a subset I ⊆V, such that all the
nodes in I are initially selected to be the active nodes.
1 This model is reasonable since many empirical studies have analyzed the social inﬂu-
ence probabilities between nodes [10,17,20].

Inﬂuence Spread in Social Networks
619
Deﬁnition 2. Neighboring Set (B(ui)). For social network G(V, E, P(E)), ∀ui ∈
V, the neighboring set of ui is deﬁned as: B(ui) = {uj | (ui, uj) ∈E, pij > 0}.
Deﬁnition 3. Active Neighboring Set (AI(ui)). For social network G(V, E,
P(E)), ∀ui ∈V, the active neighboring set of ui is deﬁned as: AI(ui) = {uj | uj ∈
B(ui), uj ∈I}.
Deﬁnition 4. Non-active
Neighboring
Set
(N I(ui)).
For
social
network
G(V, E, P(E)), ∀ui ∈V, the non-active neighboring set of ui is deﬁned as:
N I(ui) = {uj | uj ∈B(ui), uj /∈I}.
Deﬁnition 5. Positive Inﬂuence (pui(AI(ui))). For social network G(V, E,
P(E)), a node ui ∈V, and a positive inﬂuential node set I, we deﬁne a joint
inﬂuence probability of AI(ui) on ui, denoted by pui(AI(ui)) as pui(AI(ui)) =
1 −

uj∈AI(ui)
(1 −pij).
Deﬁnition 6. Negative Inﬂuence (pui(N I(ui))). For social network G(V, E,
P(E)), a node ui ∈V, and a positive inﬂuential node set I, we deﬁne a joint
inﬂuence probability of N I(ui) on ui, denoted by pui(N I(ui)) as pui(N I(ui)) =
1 −

uj∈N I(ui)
(1 −pij).
Deﬁnition 7. Ultimate Inﬂuence (ϱI(ui)). For social network G(V, E, P(E)), a
node ui ∈V, and a positive inﬂuential node set I, we deﬁne an ultimate inﬂu-
ence of B(ui) on ui, denoted by ϱI(ui) as ϱI(ui) = pui(AI(ui)) −pui(N I(ui)).
Moreover, if ϱI(ui) < 0, we set ϱI(ui) = 0. If ϱI(ui) ≥θ, where 0 < θ < 1 is a
pre-deﬁned threshold, then ui is said been positively inﬂuenced. Otherwise, ui
is not been positively inﬂuenced.
Deﬁnition 8. Minimum-sized Positive Inﬂuential Node Set (MPINS). For
social network G(V, E, P(E)), the MPINS selection problem is to ﬁnd a minimum-
sized positive inﬂuential node set I ⊆V, such that ∀ui ∈V \ I, ui is positively
inﬂuenced, i.e., ϱI(ui) = pui(AI(ui)) −pui(N I(ui)) ≥θ, where 0 < θ < 1.
3.3
Problem Hardness Analysis
In general, given an arbitrary threshold θ, the MPINS selection problem is APX-
hard. We prove the APX-hardness of MPINS by constructing a L-reduction
from Vertex Cover problem in Cubic Graph (denoted by VCCG) to the MPINS
selection problem. The decision problem of VCCG is APX-hard which is proven
in [31]. A cubic graph is a graph with every vertex’s degree of exactly three.
Given a cubic graph, VCCG is to ﬁnd a minimum-sized vertex cover2.
First, consider a cubic graph G(V, E, P(E)), where P(E) = {1 | (ui, uj) ∈
E; ui, uj ∈V}, as an instance of VCCG. We construct a new graph G as follows:
2 A vertex cover is deﬁned as a subset of nodes in a graph G such that each edge of
the graph is incident to at least one vertex of the set.

620
J. (Selena) He et al.
Fig. 1. Illustration of the construction from G to G.
(1) We create |V| + |E| nodes with |V| nodes vui = {vu1, vu2, · · · , vu|V|} rep-
resenting the nodes in G and |E| nodes vei = {ve1, ve2, · · · , ve|E|} representing
the edges in G. (2) We add an edge with inﬂuence weight p between nodes vui
and vej if and only if node ui is an endpoint of edge ej. (3) We attach addi-
tional ⌈log1−p((1 −p)|V| −θ)⌉active nodes to each node vui, denoted by set
vA
ui = {vj
ui | 1 ≤j ≤⌈log1−p((1 −p)|V| −θ)⌉}. Obviously, |vA
ui| = ⌈log1−p((1 −
p)|V| −θ)⌉. (4) We attach additional ⌈log1−p(1 −p −θ)⌉−1 active nodes to
each node vej, denoted by set vA
ej = {vj
ej | 1 ≤j ≤⌈log1−p(1 −p −θ)⌉−1}.
Obviously, |vA
ej| = ⌈log1−p(1 −p −θ)⌉−1. (5) G = {V, E}, where V =
{vu1, · · · , vu|V|} ∪{ve1, · · · , ve|E|} ∪|V|
i=1 vA
ui ∪|E|
i=1 vA
ei, E is the set of all
the edges associated with the nodes in V, and P(E) = {p | for every edge in E}.
Taking the cubic graph shown in Fig. 1(a) as an example to illustrate the
construction procedure from G to G. There are 4 nodes and 6 edges in G. There-
fore, we ﬁrst create {vui}4
i=1 and {vej}6
j=1 nodes in G. Then we add edges with
inﬂuence weight p between nodes vui and vej based on the topology shown in G.
Subsequently, we add additional vA
ui = {vj
ui | 1 ≤j ≤⌈log1−p((1 −p)|V| −θ)⌉}
active nodes to each node vui (marked by upper shaded nodes in Fig. 1(b)).
Similarly, we add additional vA
ej = {vj
ej | 1 ≤j ≤⌈log1−p(1 −p −θ)⌉−1}
active nodes to each node vej (marked by bottom shaded nodes in Fig. 1(b)).
The inﬂuence weights on all the additional edges are p. Finally, the new graph
G is constructed as shown in Fig. 1(b).
Lemma 1. G has a VCCG D of size at most d if and only if G has a positive
inﬂuential node set I of size at most k by setting k = |V|⌈log1−p((1 −p)|V| −
θ)⌉+ |E|(⌈log1−p(1 −p −θ)⌉−1) + d.
Due to limited space in this paper, for a comprehensive proof of Lemma 1 we
refer the reader to our technical report in [32].
Theorem 1. The MPINS selection problem is APX-hard.

Inﬂuence Spread in Social Networks
621
Proof. An immediate conclusion of Lemma 1 is that G has a minimum-sized
vertex cover of size OPTV CCG(G) if and only if G has a minimum-sized positive
inﬂuential node set of size
OPTMP INS( G)
= |V|⌈log1−p((1 −p)|V| −θ)⌉+ |E|(⌈log1−p(1 −p −θ)⌉−1) + OPTV CCG(G).
(1)
Note that in a cubic graph G, |E| = 3|V|
2 . Hence, we have
|V|
2 = |E|
3 ≤OPTV CCG(G).
(2)
Based on Lemma 1, plugging
|V| =
OP TMP INS( 
G)−OP TV CCG(G)
⌈log1−p((1−p)|V|−θ)⌉+ 3
2 (⌈log1−p(1−p−θ)⌉−1)
(3)
into the inequality 2, we have
OPTMP INS( G)
≤[2⌈log1−p((1 −p)|V| −θ)⌉+ 3⌈log1−p(1 −p −θ)⌉−1
2]OPTV CCG(G).
(4)
This means that VCCG is L-reducible to MPINS. In conclusion, we proved that
a speciﬁc case of the MPINS selection problem is APX-hard, since the VCCG
problem is APX-hard. Consequently, the general MPINS selection problem is
also at least APX-hard.
Based on Theorem 1, we conclude that MPINS cannot be solved in polynomial
time. Therefore, we propose a greedy algorithm to solve the problem in the next
section.
4
Greedy Algorithm and Performance Analysis
Since MPINS is APX-hard, we propose a greedy algorithm to solve it named
MPINS-GREEDY. Before introducing MPINS-GREEDY, we ﬁrst deﬁne a useful
contribution function as follows:
Deﬁnition 9. Contribution function (f(I)). For a social network represented
by graph G(V, E, P(E)), and a positive inﬂuential node set I, the contribution
function of I to G is deﬁned as: f(I) =
|V|

i=1
max{min(ϱI(ui), θ), 0}.
Based on the deﬁned contribution function, we propose a heuristic algorithm,
which has two phases. First, we ﬁnd the node ui with the maximum f(I), where
I = {ui}; and after that, we select a Maximal Independent Set (MIS)3 induced
3 MIS can be deﬁned formally as follows: given a graph G = (V, E), an Independent
Set (IS) is a subset I ⊂V such that for any two vertex v1, v2 ∈I, they are not
adjacent, i.e., (v1, v2) /∈E. An IS is called an MIS if we add one more arbitrary node
to this subset, the new subset will not be an IS any more.

622
J. (Selena) He et al.
Fig. 2. Illustration of MPINS-Greedy algorithm.
by a breadth-ﬁrst-search (BFS) ordering starting from ui. Second, employ the
pre-selected MIS denoted by M as the initial active node set to perform the
greedy algorithm called MPINS-GREEDY as shown in Algorithm 1. MPINS-
GREEDY starts from I = M. Each time, it adds the node having the maximum
f(·) value into I. The algorithm terminates when f(I) = |V|θ.
Algorithm 1. MPINS-GREEDY Algorithm
Require: A social network represented by graph G(V, E, P(E)); a pre-deﬁned threshold
θ.
1: Initialize I = M
2: while f(I) < |V|θ do
3:
choose u ∈V \ I to maximize f(I  {u})
4:
I = I  {u}
5: end while
6: return I
To better understand the proposed algorithm, we use the social network rep-
resented by the graph shown in Fig. 2(a) to illustrate the selection procedure as
follows. In the example, θ = 0.8. Since u1 has the maximum f({ui}) value, we
construct a BFS tree rooted at u1, as shown in Fig. 2(b) with the help of the
BFS ordering, we ﬁnd the MIS set which is M = {u1, u6}. Next, we go to the
second phase to perform Algorithm 1. (1) First round: I = M = {u1, u6}.
(2) Second round: we ﬁrst compute f(I
= {u1, u2, u6}) = 4.45, f(I
=
{u1, u3, u6}) = 3.018, f(I = {u1, u4, u6}) = 3.65, f(I = {u1, u5, u6}) = 3.65,
and f(I = {u1, u6, u7}) = 3.778. Therefore, we have I = {u1, u2, u6}, which has
the maximum f(I) value. However, f(I = {u1, u2, u6}) = 4.45 < 7 ∗0.8 = 5.6.
Consequently, the selection procedure continues. (3) Third round: we ﬁrst com-
puter f(I = {u1, u2, u3, u6}) = 4.45, f(I = {u1, u2, u4, u6}) = 5.6, f(I =
{u1, u2, u5, u6}) = 5.6, and f(I = {u1, u2, u6, u7}) = 4.45. Therefore, we have
I = {u1, u2, u4, u6}4. Since f(I = {u1, u2, u4, u6}) = 7 ∗0.8 = 5.6, algorithm
4 If there is a tie on the f(I) value, we use the node ID to break the tie.

Inﬂuence Spread in Social Networks
623
terminates and outputs set I = {u1, u2, u4, u6} as shown in Fig. 2(c), where
black nodes represent the selected inﬂuential nodes.
Based on Algorithm 1, in each iteration, only one node is selected to be added
into the output set I. In the worst case, all nodes are added into I in the |V|-th
iteration. Then, f(I) = f(V) = |V|θ and Algorithm 1 terminates and outputs
I = V. Therefore, Algorithm 1 terminates for sure. Also, if f(I) = |V|θ, then
∀ui ∈V, ϱI(ui) ≥θ followed by Deﬁnition 9. Therefore, all nodes in the network
are positively inﬂuenced. In another side, if ∀ui ∈V, ϱI(ui) ≥θ, then we obtain
∀ui ∈V, min(ϱI(ui), θ) = θ. Therefore, Algorithm 1 must produce a feasible
solution of the MPINS selection problem.
5
Performance Evaluation
Since there is no existing work studying the MPINS problem under the indepen-
dent cascade model currently, in the real data experiments, the results of MPINS-
GREEDY (MPINS) are compared with the most related work [22] (PIDS), and
the optimal solution of MPINS (OPTIMAL) which is obtained by exhausting
searching. To ensure the fairness of comparison, the condition of termination
to the algorithm proposed in [22] is changed to ﬁnd a PIDS, such that every
node in the network is positively inﬂuenced no less than the same threshold θ in
MPINS. All experiments were performed on a desktop computer equipped with
Inter(R) Core(TM) 2 Quad CPU 2.83 GHz and 6 GB RAM.
5.1
Experimental Setting
We also implement experiments run on diﬀerent kinds of real-world data sets.
The ﬁrst group of data sets are shown in Table 1 come from SNAP5. The network
statistics are summarized by the number of nodes and edges, and the diameter
(i.e., longest shortest path). The data collected in Table 1 is based on the Cus-
tomers Who Bought This Item Also Bought feature of the Amazon website. Four
diﬀerent networks are composed of the data collected from March to May in 2003
in Amazon. In each network, for a pair of nodes (products) i and j, there is an
edge between them if and only if a product i is frequently co-purchased with
product j [33]. Besides the Amazon product co-purchasing data sets shown in
Table 1, we also evaluate our algorithm in the additional real data sets listed as
follows:
1. WikiVote: a data set obtained from [34], which contains the vote history data
of Wikipedia. The data set includes 7115 vertices and 103689 edges which
contains the voting data of Wikipedia from the inception till January 2008.
If user i voted on user j for the administrator election, there will be an edge
between i to j.
5 http://snap.stanford.edu/data/.

624
J. (Selena) He et al.
Table 1. Data Set 1 in Our Experiment
Data Nodes
Edges
Diameter
A1
262111 1234877 29
A2
400727 3200440 18
A3
410236 3356824 21
A4
403394 3387388 21
2. Coauthor: a data set obtained from [35], which hold the coauthors information
maintained by ArnetMiner.
We chosen the subset which include 53442 vertices and 127968 edges.
When the author i has a relationship with author j, there will be one edge
between i to j.
3. Twitter: a data set obtained from [36,37], which stores the information col-
lected from Twitter. We picked the subset with 92180 vertices and 188971
edges, which represent the user account and their relationships.
Moreover, the social inﬂuence on each edge (i, j) is calculated by
1
deg(j) [38],
where deg(j) is the degree of node j. Similarly, if one node is selected as the
active node, it has positively inﬂuence on all its neighbors. Otherwise, it only
has negative inﬂuence on its neighbors.
Fig. 3. Size of inﬂuential nodes in (a) Amazon, (b) WikiVote, Coauthor, and Twitter.
Experimental Results. The impacts of θ on the size of MIS, the solutions
of MPINS, and the solution of PIDS on Amazon co-purchase data sets, when
θ change from 0.005 to 0.02, are shown in Fig. 3(a). As shown in Fig. 3(a), the
solution sizes of PIDS and MINS increase when θ increases. This is because,
when the pre-set threshold becomes large, more inﬂuential nodes are required
to be chosen to inﬂuence the whole network. On average, the diﬀerence between
the size of PIDS and MPINS solutions is 37.23%. This is because that MPINS

Inﬂuence Spread in Social Networks
625
chooses the most inﬂuential node ﬁrst instead of the node with the largest degree
ﬁrst. Moreover, the growth rate of the solution size of PIDS is higher than that
of MPINS.
Similarly, the impacts of θ on the size of MIS, the solutions of MPINS, and the
solution of PIDS on WikiVote, Coauthor, and Twitter, when θ change from 0.02
to 0.08, are shown in Fig. 3(b). The solution sizes of PIDS and MINS increase
when θ increases as well. For the Twitter data set, MPINS outperforms PIDS
signiﬁcantly, i.e., MPINS selects 45.45% less inﬂuential nodes than that of PIDS.
On average, the diﬀerence between the sizes of PIDS and MPINS solutions is
36.37%.
Fig. 4. % of inﬂuential nodes in (a) Amazon (b) WikiVote, Coauthor, and Twitter.
Figure 4 shows how many nodes are selected as the inﬂuential nodes repre-
sented by the ratio over the total number of nodes in the network. Figure 4 (a)
shows the impacts of θ on the ratio of MIS, MPINS, and PIDS on Amazon co-
purchase data sets. While, Fig. 4 (b) shows the the impacts of θ on the ratio
of MIS, MPINS, and PIDS on WikiVote, Coauthor, and Twitter data sets. One
interesting observation here is that much less nodes are selected as the inﬂuential
nodes for Amazon co-purchase data sets compared to the WikiVote, Coauthor,
and Twitter data sets.
Finally, we compare the performance of our proposed method MPINS with
PIDS and the method denoted by “Random”, which randomly chooses a node
as the inﬂuential node. The impacts of θ on the sizes of the solutions of MPINS,
PIDS, and Random, when θ change from 0.02 to 0.08, are shown in Fig. 5 for
the WikiVote, Coauthor data set, and Twitter data sets. As shown in Fig. 5,
the solution sizes of Random, PIDS and MPINS increase when n increases.
Moreover, for a speciﬁc θ, MPINS produces a smaller inﬂuential node set than
PIDS. This is consistent with the simulation results and previous experimental
results. Furthermore, both PIDS and MPINS produce much smaller inﬂuential
node sets than Random for a speciﬁc θ. This is because Random picks node
randomly without any selection criterion. However, PIDS’s selection process is
based on degree and our MPINS greedy criterion is based on social inﬂuence.

626
J. (Selena) He et al.
Fig. 5. MPINS VS. PIDS VS. Random in (a) WikiVote (b) Coauthor (c) Twitter
From the results of experiments on real-world data sets, we can conclude that
the size of the constructed initial active node set of MPINS is smaller than that
of PIDS. Moreover, the solution of MPINS is very close to the optimal solutions
in small scale networks.
6
Conclusion
In this paper, we study the Minimum-sized Positive Inﬂuential Node Set
(MPINS) selection problem in social networks. We show by reduction that
MPINS is APX-hard under the Independent Cascade Model. Subsequently, a
greedy algorithm is proposed to solve the problem. Furthermore, we validate
our proposed algorithm through simulations on random graphs and experiments
on seven diﬀerent real-world data sets. The simulation and experimental results
indicate that MPINS-GREEDY can construct smaller sized satisﬁed initial active
node sets than the latest related work PIDS. Moreover, for small scale net-
work, MPINS-GREEDY has very similar performance as the optimal solution
of MPINS. Furthermore, the simulation and experimental results indicate that
MPINS-GREEDY considerably outperforms PIDS in medium and large scale
networks, sparse networks, and for high threshold θ.
Acknowledgment. This research is funded in part by the Kennesaw State University
College of Science and Mathematics Interdisciplinary Research Opportunities (IDROP)
Program, the Provincial Key Research and Development Program of Zhejiang, China
under No. 2016C01G2010916, the Fundamental Research Funds for the Central Uni-
versities, the Alibaba-Zhejiang University Joint Research Institute for Frontier Tech-
nologies (A.Z.F.T.) under Program No. XT622017000118, and the CCF-Tencent Open
Research Fund under No. AGR20160109.
References
1. Kempe, D., Kleinberg, J., Tardos, ´E.: Maximizing the spread of inﬂuence through
a social network. In: Proceedings of the Ninth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, pp. 137–146. ACM (2003)

Inﬂuence Spread in Social Networks
627
2. Saito, K., Kimura, M., Motoda, H.: Discovering inﬂuential nodes for sis models
in social networks. In: Gama, J., Costa, V.S., Jorge, A.M., Brazdil, P.B. (eds.)
DS 2009. LNCS, vol. 5808, pp. 302–316. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-04747-3 24
3. Li, Y., Chen, W., Wang, Y., Zhang, Z.-L.: Inﬂuence diﬀusion dynamics and inﬂu-
ence maximization in social networks with friend and foe relationships. In: Proceed-
ings of the sixth ACM International Conference on Web Search and Data Mining,
pp. 657–666. ACM (2013)
4. Han, M., Yan, M., Cai, Z., Li, Y., Cai, X., Yu, J.: Inﬂuence maximization by
probing partial communities in dynamic online social networks. Trans. Emerging
Telecommun. Technol
5. He, X., Song, G., Chen, W., Jiang, Q.: Inﬂuence blocking maximization in social
networks under the competitive linear threshold model. In: Proceedings of the 2012
SIAM International Conference on Data Mining, pp. 463–474. SIAM (2012)
6. Lu, W., Bonchi, F., Goyal, A., Lakshmanan, L.V.: The bang for the buck: fair
competitive viral marketing from the host perspective. In: Proceedings of the 19th
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
928–936. ACM (2013)
7. Selcuk Uluagac, A., Beyah, R., Ji, S., He, J. (Selena), Li, Y.: Cell-based snapshot
and continuous data collection in wireless sensor networks. ACM Trans. Sensor
Networks (TOSN) 9(4)
8. He, J. (Selena), Ji, S., Li, Y.: Genetic-algorithm-based construction of load-
balanced cdss in wireless sensor networks. MILCOM 9(4)
9. Albinali, H., Han, M., Wang, J., Gao, H., Li, Y.: The roles of social network mavens.
In: The 12th International Conference on Mobile Ad-hoc and Sensor Networks
10. Goyal, A., Bonchi, F., Lakshmanan, L.V.: Learning inﬂuence probabilities in social
networks. In: Proceedings of the third ACM International Conference on Web
Search and Data Mining, pp. 241–250. ACM (2010)
11. Han, M., Yan, M., Li, J., Ji, S., Li, Y.: Generating uncertain networks based
on historical network snapshots. In: Du, D.-Z., Zhang, G. (eds.) COCOON
2013. LNCS, vol. 7936, pp. 747–758. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-38768-5 68
12. Goyal, A., Lu, W., Lakshmanan, L.V.: Celf++: optimizing the greedy algorithm for
inﬂuence maximization in social networks. In: Proceedings of the 20th International
Conference Companion on World Wide Web, pp. 47–48. ACM (2011)
13. Han, M., Yan, M., Cai, Z., Li, Y.: An exploration of broader inﬂuence maximization
in timeliness networks with opportunistic selection. J. Netw. Comput. Appl. 63,
39–49 (2016)
14. Wang, C., Chen, W., Wang, Y.: Scalable inﬂuence maximization for independent
cascade model in large-scale social networks. Data Mining Know. Discovery 25(3),
545 (2012)
15. Han, M., Duan, Z., Ai, C., Lybarger, F.W., Li, Y., Bourgeois, A.G.: Time constraint
inﬂuence maximization algorithm in the age of big data. Int. J. Comput. Sci. Eng
16. Tang, J., Wu, S., Sun, J.: Conﬂuence: conformity inﬂuence in large social networks.
In: Proceedings of the 19th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 347–355. ACM (2013)
17. Saito, K., Nakano, R., Kimura, M.: Prediction of information diﬀusion probabilities
for independent cascade model. In: Lovrek, I., Howlett, R.J., Jain, L.C. (eds.)
KES 2008. LNCS, vol. 5179, pp. 67–75. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-85567-5 9

628
J. (Selena) He et al.
18. Tang, J., Sun, J., Wang, C., Yang, Z.: Social inﬂuence analysis in large-scale net-
works. In: Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 807–816. ACM (2009)
19. Han, M., Yan, M., Li, J., Ji, S., Li, Y.: Neighborhood-based uncertainty generation
in social networks. J. Comb. Optim. 28(3), 561–576 (2014)
20. Wang, C., Tang, J., Sun, J., Han, J.: Dynamic social inﬂuence analysis through
time-dependent factor graphs. In: 2011 International Conference on Advances in
Social Networks Analysis and Mining (ASONAM), pp. 239–246. IEEE (2011)
21. Wang, F., Camacho, E., Xu, K.: Positive inﬂuence dominating set in online social
networks. In: Du, D.-Z., Hu, X., Pardalos, P.M. (eds.) COCOA 2009. LNCS, vol.
5573, pp. 313–321. Springer, Heidelberg (2009). doi:10.1007/978-3-642-02026-1 29
22. Wang, F., Du, H., Camacho, E., Xu, K., Lee, W., Shi, Y., Shan, S.: On positive
inﬂuence dominating sets in social networks. Theoret. Comput. Sci. 412(3), 265–
269 (2011)
23. Zhu, X., Yu, J., Lee, W., Kim, D., Shan, S., Du, D.-Z.: New dominating sets in
social networks. J. Global Optim. 48(4), 633–642 (2010)
24. He, J.S., Ji, S., Beyah, R., Cai, Z.: Minimum-sized inﬂuential node set selection for
social networks under the independent cascade model. In: Proceedings of the 15th
ACM International Symposium on Mobile ad hoc Networking and Computing, pp.
93–102. ACM (2014)
25. Kaur, H., He, J.S.: Blocking negative inﬂuential node set in social networks: from
host perspective. Trans. Emerg. Telecommun. Technol. (ETT) 28(4)
26. Kempe, D., Kleinberg, J., Tardos, ´E.: Inﬂuential nodes in a diﬀusion model for
social networks. In: Caires, L., Italiano, G.F., Monteiro, L., Palamidessi, C., Yung,
M. (eds.) ICALP 2005. LNCS, vol. 3580, pp. 1127–1138. Springer, Heidelberg
(2005). doi:10.1007/11523468 91
27. Leskovec, J., Krause, A., Guestrin, C., Faloutsos, C., VanBriesen, J., Glance, N.:
Cost-eﬀective outbreak detection in networks. In: Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
420–429. ACM (2007)
28. Chen, W., Yuan, Y., Zhang, L.: Scalable inﬂuence maximization in social networks
under the linear threshold model. In: 2010 IEEE 10th International Conference on
Data Mining (ICDM), pp. 88–97. IEEE (2010)
29. Han, M., Li, J., Cai, Z., Qilong, H.: Privacy reserved inﬂuence maximization in
gps-enabled cyber-physical and online social networks. SocialCom 2016, 284–292
(2016)
30. Han, M., Han, Q., Li, L., Li, J., Li, Y.: Maximizing inﬂuence in sensed heterogenous
social network with privacy preservation. Int. J. Sens. Netw
31. Du, D.-Z., Ko, K.-I.: Theory of Computational Complexity, vol. 58. Wiley, Hoboken
(2011)
32. Technical report. http://ksuweb.kennesaw.edu/∼jhe4/Research/MPINS
33. Leskovec, J., Adamic, L.A., Huberman, B.A.: The dynamics of viral marketing.
ACM Trans. Web (TWEB) 1(1) (2007). 5
34. Leskovec, J., Huttenlocher, D., Kleinberg, J.: Predicting positive and negative links
in online social networks. In: Proceedings of the 19th International Conference on
World Wide Web, pp. 641–650. ACM (2010)
35. Tang, J., Zhang, J., Yao, L., Li, J., Zhang, L., Su, Z.: Arnetminer: extraction and
mining of academic social networks. In: Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 990–998.
ACM (2008)

Inﬂuence Spread in Social Networks
629
36. Hopcroft, J., Lou, T., Tang, J.: Who will follow you back?: reciprocal relationship
prediction. In: Proceedings of the 20th ACM International Conference on Informa-
tion and Knowledge Management, pp. 1137–1146. ACM (2011)
37. Lou, T., Tang, J., Hopcroft, J., Fang, Z., Ding, X.: Learning to predict reciprocity
and triadic closure in social networks. ACM Trans. Know. Discov. from Data
(TKDD) 7(2) (2013). 5
38. Wang, C., Chen, W., Wang, Y.: Scalable inﬂuence maximization for independent
cascade model in large-scale social networks. Data Min. Know. Disc. 25(3)

Guided Genetic Algorithm for the Inﬂuence
Maximization Problem
Pavel Kr¨omer(B) and Jana Nowakov´a
Department of Computer Science, VˇSB Technical University of Ostrava,
Ostrava, Czech Republic
{pavel.kromer,jana.nowakova}@vsb.cz
Abstract. Inﬂuence maximization is a hard combinatorial optimization
problem. It requires the identiﬁcation of an optimum set of k network ver-
tices that triggers the activation of a maximum total number of remaining
network nodes with respect to a chosen propagation model. The problem
is appealing because it is provably hard and has a number of practical
applications in domains such as data mining and social network analysis.
Although there are many exact and heuristic algorithms for inﬂuence
maximization, it has been tackled by metaheuristic and evolutionary
methods as well. This paper presents and evaluates a new evolutionary
method for inﬂuence maximization that employs a recent genetic algo-
rithm for ﬁxed–length subset selection. The algorithm is extended by the
concept of guiding that prevents selection of infeasible vertices, reduces
the search space, and eﬀectively improves the evolutionary procedure.
Keywords: Inﬂuence maximization · Information diﬀusion · Social net-
works · Genetic algorithms
1
Introduction
The propagation of various phenomena through networks and linked environ-
ments is nowadays subject of intense research. An increasing number of real–
world domains can be perceived and modelled as networks and analyzed by
network and graph methods. Contemporary technological and social networks
are complex distributed environments where phenomena can be carried from one
node to another and spread along diﬀerent routes and pathways. The knowledge
of information propagation patterns and network structures that are associated
with information diﬀusion can be used for a number of purposes including opin-
ion making, viral marketing, expert (authority) identiﬁcation, and e.g. robust-
ness and stability analysis.
Inﬂuence maximization is a common term for a family of network problems
that consist in the search for a set of seed nodes from which information spreads
through a network most eﬃciently. The seed nodes need to be selected so that
the information they release to the network aﬀects the maximum of the remain-
ing network nodes under an assumed propagation model. Inﬂuence maximiza-
tion has been formulated as a combinatorial optimization problem and shown
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 630–641, 2017.
DOI: 10.1007/978-3-319-62389-4 52

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
631
NP–hard [9]. It has been addressed by a number of exact and heuristic algo-
rithms and, due to its hardness and the limitations of traditional methods, also
by various stochastic and metaheuristic approaches.
Evolutionary methods have been successfully employed to solve a wide range
of combinatorial optimization problems. They are often used in place of tradi-
tional exact and heuristic methods because the search and optimization strate-
gies they implement are ﬂexible and adapt very well to a wide range of conditions
associated with diﬀerent instances of solved problems. Evolutionary metaheuris-
tics facilitate an eﬃcient search process in a search space composed of encoded
candidate problem solutions. An appropriate encoding that would translate can-
didate problem solutions to a form required by the employed metaheuristic algo-
rithm eﬃciently is a crucial part of every metaheuristic problem solving strategy.
A good encoding is compact and does not allow construction of invalid problem
solutions through the artiﬁcial evolution.
In this work, a recent genetic algorithm for ﬁxed–length subset selection is
employed to address the inﬂuence maximization problem. The algorithm used
previously for sampling from scale–free networks [11] and feature subset selec-
tion
[12,13], is in this work extended by the concept of guiding to increase
its performance for the inﬂuence maximization problem. The rest of this paper
is organized in the following way. Section 2 introduces the inﬂuence maximiza-
tion problem and outlines several heuristic and metaheuristic algorithms that
were proposed to solve it. Section 3 presents the proposed evolutionary app-
roach to inﬂuence maximization. It brieﬂy outlines the basic principles of genetic
algorithms, summarizes genetic algorithm for ﬁxed–length subset selection, and
suggests its new variant suitable for network problems such as inﬂuence maxi-
mization. The proposed approach is experimentally evaluated in Sect. 4 and the
work is concluded in Sect. 5.
2
Inﬂuence Maximization Problem
Inﬂuence maximization problem (IMP) is a high–level problem that involves
analysis of inﬂuence spreading through a (social) network and identiﬁcation
of the most important network nodes taking part in this process [9]. An IMP
instance is deﬁned by three parameters: a graph, G = (V, E, W), with a set of
vertices V , a set of edges E, a set of edge weights W, a discrete–time information
propagation model, M, and the number of seed nodes, k [1]. The goal of the
IMP is to select a set of seed nodes from which information spreads through the
graph (network) most eﬃciently and propagates to the maximum of remaining
network nodes.
Linear threshold and independent cascade are two most used progressive infor-
mation propagation (diﬀusion) models [2,9]. Under the linear threshold model,
each node, i, chooses a random threshold, θi ∈[0, 1], that corresponds to the
weighted fraction of its neighbors that must be active for i to activate. The inde-
pendent cascade model, on the other hand, takes a diﬀerent approach. When a

632
P. Kr¨omer and J. Nowakov´a
node, i, becomes active, it becomes a single chance to activate its currently inac-
tive neighbours. The activation of an inactive neighbor, j, happens with probabil-
ity pij and is often associated with the weight of the edge between i and j.
The IMP has many applications in a number of areas and especially in social
network analysis [1,2,4–7,9,14,18,19,21]. It is known that an optimum IMP
solution under linear threshold and independent cascade propagation models is
NP–hard and a number of heuristic and metaheuristic approximate algorithms
has been proposed to tackle it.
2.1
Recent Algorithms for the Inﬂuence Maximization Problem
An IMP algorithm designed for modular social networks was proposed in [2].
The problem was cast as an optimal resource allocation problem and a quasi-
optimal dynamic programming method called OASNET (Optimal Allocation in
Social NETwork) was designed. A scalable algorithm named SIMPATH, based
on a ”lazy–forward” (CELF) optimization, was introduced in [7]. In this app-
roach, seeds are chosen iteratively in a lazy–forward manner and the IMP under
the linear threshold model is solved by an alternative algorithm that evaluates
information spreading by exploring simple paths in node neighborhood.
Two algorithms for inﬂuence estimation in continuous–time diﬀusion net-
works were presented in [4]. The ﬁrst one, based on continuous–time Markov
chains, was designed to estimate inﬂuence spreading and to identify a set of
nodes with a guaranteed inﬂuence. It was, however, not scalable to large net-
works and another randomized method, able to ﬁnd in networks with a general
transition function sets of nodes with inﬂuence reduced by a constant factor
only, was proposed.
Another work introduced a distributed inﬂuence propagation model with
multiagent architecture [14]. It was used to discover inﬂuence propagation pat-
terns in simulated evolving networks with seed sets detected by an evolution–
based backward algorithm. A game–theoretic approach to the IMP was proposed
in [10]. The IMP is in this approach cast as a game in which agents maximize
the diﬀusion of their information. The Nash equilibrium of the game is sought in
order to ﬁnd an optimum seed set. An analysis of this approach showed that it
is most aﬀected by the social skills (i.e. connectedness) of the agent. Because of
the computational complexity, an approximate algorithm was used for scenarios
with larger seed sets.
A recent study [17] showed that the popular independent cascade model
does not correspond to information diﬀusion patterns observed in real–world
networks. An alternative diﬀusion model called three steps cascade was proposed
and shown more reliable and robust than the independent cascade. It was found
especially suitable for modelling information diﬀusion in online social networks.
An adaptive method for seed set selection was proposed in [20]. It is based
on another dynamic modiﬁcation of the independent cascade model suitable
for analysis of diﬀusion in dynamic networks with uncertainty. Finally, a novel
probabilistic multi–hop information diﬀusion strategy was used in context of
IMP as well [16].

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
633
Several metaheuristic and hybrid methods have been proposed for the IMP in
the past. A high–performance algorithm based on a combination of genetic algo-
rithms and a greedy method has been introduced in [21]. The greedy approach
was used to reﬁne the results obtained by the genetic search. An experimen-
tal evaluation showed that the introduction of local search improved the results
obtained by the evolutionary method by approximately 10%.
A straightforward genetic algorithm for the IMP was introduced in [1]. It used
only simple genetic operators and showed that such approach is well–comparable
with traditional heuristics for the IMP. The work also concluded that the this
approach is widely applicable and not limited to networks with special proper-
ties only. Swarm [6] and memetic [5] approaches have been proposed for the IMP
recently as well. Clearly, nature–inspired methods and evolutionary metaheuris-
tics in particular are at the present time intensively investigated in context of
the IMP.
3
Genetic Algorithm for the Inﬂuence Maximization
Problem
Genetic algorithms (GA) form a family of stochastic, population-based, meta-
heuristic optimization methods [8,15]. They can be used to tackle hard optimiza-
tion problems by an iterative evolution of a population of encoded candidate
solutions. The candidate solutions are in each iteration of the algorithm (gener-
ation) evaluated by a problem speciﬁc objective (ﬁtness) function and compete
for survival in the population. This high–level procedure, implemented by the
application of so–called genetic operators, yields an iterative problem–solving
strategy inspired by the survival–of–the–ﬁttest model of genetic evolution.
Problem encoding is an important part of the genetic search. It translates
candidate solutions from the problem domain (phenotype) to the encoded search
space (genotype) of the algorithm. It deﬁnes the internal representation of the
problem instances used during the optimization process. The representation
speciﬁes the chromosome data structure and the decoding function [3]. The data
structure, in turn, deﬁnes the actual size and shape of the search space.
Crossover (recombination), mutation, and selection are the most important
genetic operators used by GAs. Crossover leads to creation of new (oﬀspring)
problem solutions from selected existing (parent) solutions and mimicks sex-
ual reproduction. It is the main operator that distinguishes GAs from other
population–based stochastic search methods [8,15]. Its role in GAs has been
thoroughly investigated and it has been identiﬁed as the primary creative force
in the evolutionary search process. It propagates so called building blocks (solu-
tion patterns with above average ﬁtness) from one generation to another and
creates new, better performing, building blocks through their recombination. It
can introduce large changes in the population with small disruption of these
building blocks [22]. In contrast, mutation is expected to insert new material
into the population by random perturbation of the chromosome structure and is
essential for the introduction of new genetic material into the pool of candidate
problem solutions.

634
P. Kr¨omer and J. Nowakov´a
3.1
Genetic Algorithms for Fixed–length Subset Selection
The application of GA to a particular problem involves deﬁnition of encoding,
ﬁtness function, and operator implementation. The IMP is a special case of ﬁxed–
length subset selection problem. Fixed–length subset selection is a combinatorial
optimization problem that involves search for a subset of k distinct elements from
a (larger) ﬁnite set of n elements. The subset is formed with respect to a speciﬁc
criterion (property) and the process can be cast as an optimization problem.
Although many GAs for subset selection exist, a compact and computation-
ally inexpensive representation of a ﬁxed–length subsets is not straightforward.
Nevertheless, an eﬃcient GA for the class of ﬁxed–length subset selection prob-
lems has been introduced recently [11,12]. It involved a suitable chromosome
encoding and customized genetic operators. The encoding and genetic opera-
tors were designed with the aim to use compact chromosomes, to exploit both
crossover and mutation, and to avoid the creation of invalid individuals during
the artiﬁcial evolution [11,12]. Later, it was extended by a new, more universal,
crossover operator and successfully applied to image classiﬁcation [13].
A ﬁxed-length subset of k elements can be deﬁned by the indices of the ele-
ments selected from the full set of n elements. No element can appear more than
once in order to achieve the required subset size k. A chromosome representing
k elements, c, can be deﬁned by:
c = (c1, c2, . . . ck),
∀(i, j) ∈{0, . . . n} : ci ̸= cj
(1)
Apparently, such encoding can spawn invalid individuals in case the traditional
genetic operators such as the 1–point crossover or uniform mutation are applied.
Equation (2) shows an example of two valid chromosomes forming an invalid
oﬀspring (o1) through 1–point mutation.
c = ( 9 | 2 7 )
×
d = ( 5 | 8 9 )
=⇒
o1 = ( 9 | 8 9 )
o2 = ( 5 | 2 7 )
(2)
To avoid this undesired behaviour, the encoding can be modiﬁed by sorting the
indices within each chromosome [11]
c = (c1, c2, . . . ck), ∀(i, j) ∈{0, . . . n} : i < j =⇒ci < cj.
(3)
Then, the generation of random ﬁxed–length subset individuals requires two
steps. First, k unique elements out of all n possible elements are selected. Then,
the indices in each individual are sorted in increasing order. On top of that,
special mutation and crossover operators that respect the ordering of the indices
within the chromosome are used. The operators are based on the traditional
mutation and crossover operators that were modiﬁed so that they do not disrupt
the ordering of the chromosomes and do not create invalid oﬀspring [12,13].

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
635
The order–preserving mutation [11] replaces ith gene ci in chromosome c with
a value from the interval deﬁned by its left and right neighbour as deﬁned in:
mut(ci) =
⎧
⎪
⎨
⎪
⎩
urand∗(0, ci+1),
if i = 0
urand(ci−1, ci+1),
if i ∈(0, n −1)
urand(ci−1, N),
if i = N −1
(4)
where i ∈{0, . . . , n} and urand(a, b) selects a uniform pseudo–random integer
from the interval (a, b) (whereas urand∗(a, b) selects a uniform pseudo–random
integer from the interval [a, b)). The operator guarantees that the ordering of
the indices within the chromosome remains valid. However, it has no eﬀect for
the ith gene in chromosomes for which it holds that (ci−1 + 1) = ci = (ci+1 −1).
The order–preserving crossover [11] is based on the traditional one–point
crossover operator [15]. It selects a random position, i, in parent chromosomes
c and d and checks if the position can be used for crossover, i.e. whether (5)
ci < di+1 ∧di < ci+1
(5)
holds. If Eq. (5) does not hold for i, the remaining positions in the chromosomes
are sequentially scanned in the search for a suitable crossover point for which
(5) holds. The order–preserving crossover closely imitates the traditional one–
point recombination strategies known from many variants of genetic algorithms.
However, it cannot be applied to all possible pairs of parent chromosomes and
an alternative crossover strategy, employing the Mergesort algorithm, was pro-
posed in [13]. A Mergesort–based recombination of two ordered chromosomes
representing ﬁxed–length subsets of k elements is performed in two steps. First,
the Mergesort algorithm is employed to create from the parent chromosomes
of length k, c, d, one temporary sorted array, t, of length 2k. Then, oﬀspring
chromosomes, o1 and o2, are formed from t according to
o1 = (t0, t2, . . . , t2k−1),
(6)
o2 = (t1, t3, . . . , t2k).
(7)
This approach guarantees that valid oﬀspring chromosomes are created from
every possible combination of two valid parent chromosomes. Oﬀspring chro-
mosomes remain sorted and cannot contain duplicate column indices. The
Mergesort–based crossover is illustrated in Eq. (8).
c = ( 1 2 4 5 8 )
×
d = ( 0 1 3 8 9 )
merge
=⇒( 0 1 1 2 3 4 5 8 8 9 )
split
=⇒
o1 = ( 0 1 3 5 8 )
o2 = ( 1 2 4 8 9 )
(8)
3.2
Guided Genetic Algorithm for the Inﬂuence Maximization
Problem
As a ﬁxed–length subset selection problem, the IMP can be solved directly by
the GA described in Sect. 3.1. This approach uses artiﬁcial evolution to search

636
P. Kr¨omer and J. Nowakov´a
for a set of k seed nodes that would activate the maximum of remaining nodes
in a network with ﬁxed structure under a ﬁxed activation (diﬀusion) model.
However, the IMP is a network problem. The network structure, associated with
each IMP instance, provides an additional information that can be used to guide
the GA and improve its ability to ﬁnd good IMP solutions. Various metrices can
be computed for each network node and used to guide the GA towards nodes
with higher information diﬀusion potential. For example, it is easy to see that
a sink (i.e. a node no outgoing edges) plays in the information diﬀusion process
a diﬀerent role than a hub (i.e. a node with a large number of outgoing edges)
and the knowledge of node outdegree can be therefore useful for the algorithm.
The guided GA for IMP (G2A) takes advantage of the auxiliary informa-
tion hidden in network structure and uses it whenever feasible. In G2A, node
outdegree it is used to generate initial population and as part of the mutation
operator. In particular, the probability of a node, i ∈V , being included in a
randomly generated initial solution and being inserted to a chromosome by a
mutation operator is proportional to its outdegree. Because the network struc-
ture is constant, the probabilities associated with each node can be computed
beforehand and the computational complexity of the algorithm increases only
marginally.
The ﬁtness function used by both GAs for IMP is based on the indepen-
dent cascade diﬀusion model. Because the diﬀusion process is stochastic, it was
executed as a series of independent Monte Carlo iterations (information propa-
gations) and the average number of activated nodes was used as a ﬁtness value.
In the remainder of this work, the ability of GA and G2A to ﬁnd IMP solutions
is evaluated experimentally.
4
Experiments
In order to assess the ability of GA and G2A to ﬁnd IMP solutions, a set of
weighted test networks with 50, 100, 150, and 200 nodes was generated randomly.
They are labeled G50, G100, G150, and G200, respectively. Examples of the test
networks are shown in Fig. 1.
GA and G2A were used to ﬁnd in each test graph seed sets of 10, 20, 30,
and 40 seed nodes, respectively. Both algorithms were executed with the same
ﬁxed parameters, based on past experience and initial trial–and–error runs. They
implemented a steady–state genetic algorithm with population gap 2 [15] and
evolved a population of 100 candidate solutions. The crossover probability was
set to 0.8 and mutation probability to 0.02. The maximum number of genera-
tions was set to 10,000 and the information propagation model was independent
cascade with 1,000 Monte Carlo iterations. The average number of nodes acti-
vated by selected seed sets was used as a ﬁtness function. However, because of
the stochastic nature of genetic algorithms, each experiment was executed 31
times independently and average results are reported in Table 1.
The table displays for each experiment (i.e. a combination of data set and seed
set size) minimum, mean, and maximum average number of nodes activated by

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
637
Fig. 1. Examples of test networks. Node size and color is proportional to its outdegree,
edge thickness and color is proportional to its weight. The lighter the lower. Note that
the sizes and colors are relative to other nodes within the same network (i.e. subﬁgure).
Table 1. Average number of activated nodes (ﬁnal coverage).
dataset k
Original GA
G2A
min
mean (σ)
max
min
mean (σ)
max
G50
10 24.782 25.189 (0.407)
25.5
25.590 25.683 (0.092) 25.7
20 35.843 36.193 (0.350)
36.5
36.284 36.445 (0.161) 36.6
30 43.380 43.435 (0.055) 43.4 42.462 42.521 (0.059)
42.5
40 47.224 47.421 (0.198) 47.6 45.336 45.342 (0.007)
45.3
G100
10 30.981 31.444 (0.463)
31.9 31.677 31.783 (0.106) 31.8
20 47.953 48.296 (0.343)
48.6
48.324 49.155 (0.831) 49.9
30 61.063 61.115 (0.052)
61.1
61.495 61.640 (0.145) 61.7
40 69.910 70.154 (0.243)
70.3
69.862 70.330 (0.468) 70.7
G150
10 39.897 40.038 (0.141)
40.1
39.683 40.169 (0.486) 40.6
20 60.762 60.970 (0.208)
61.1
62.005 62.054 (0.049) 62.1
30 75.362 76.012 (0.650)
76.6
76.629 76.940 (0.310) 77.2
40 86.346 87.381 (1.035)
88.4
86.865 87.989 (1.124) 89.1
G200
10 38.304 40.516 (2.212)
42.7
42.402 42.988 (0.586) 43.5
20 65.104 65.665 (0.561)
66.2
64.271 66.162 (1.891) 68.0
30 82.861 84.508 (1.647)
86.1 85.547 85.809 (0.262) 86.0
40 95.369 96.127 (0.757)
96.8
96.285 97.432 (1.147) 98.5
seed sets obtained by GA and G2A (the higher the better). The better mean and
maximum results are typed in bold face. The results show that the proposed app-
roach delivers average IMP solutions better than plain GA for ﬁxed–length subset
selection in 10 out of 12 test cases and better best (maximum) IMP solutions in 8

638
P. Kr¨omer and J. Nowakov´a
out of 12 test cases. Although the improvements are in some cases only negligible
(for example, an average of 0.5 more activated nodes for G50 and seed set size 10),
G2Adiscovered in some other cases seed sets that activated on average more than
2 extra nodes (as in the experiment with G200 and seed set 10).
The course of the artiﬁcial evolution in these two cases is displayed in Fig. 2.
The ﬁgures illustrate that G2A is superior to the original GA for ﬁxed–length
subset selection during the whole evolution procedure. It also clearly shows that
the initialization strategy used by G2A provides the algorithm with initial popu-
lation of signiﬁcantly better quality than that used by the original GA. A similar
behaviour was observed in all test cases where G2A outperformed GA.
The evolution in cases when the original GA has found better mean result
than the proposed algorithm is illustrated in Fig. 3. One can immediately note
the lack of improvement in Fig. 3b. It suggests that the algorithm might be prone
to premature convergence.
Fig. 2. Examples of IMP solution evolution (cases when G2A outperforms GA). The
displayed range corresponds to a 95% conﬁdence interval for average ﬁtness values.
Fig. 3. Examples of IMP solution evolution (cases when GA outperforms G2A). The
displayed range corresponds to a 95% conﬁdence interval for average ﬁtness values.

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
639
Fig. 4. Results of experiments in relation to search space size. Note the logarithmic
scale of the x–axes.
Finally, the relation between the search space size (i.e. problem complexity)
and obtained results is illustrated in Fig. 4. The ﬁgures clearly show that the
proposed G2A algorithm outperforms the original GA especially for larger search
spaces. It also illustrates that the fact that the original GA has found better best
solution for G200 with sees set size 30 is an exception rather than a trend (note
that the G2A has fund better mean result also for this case).
5
Conclusions
A new genetic algorithm for the inﬂuence maximization problem was proposed
and experimentally evaluated in this work. The proposed approach is an exten-
sion of a previous genetic algorithm for ﬁxed–length subset selection that can
use information from the underlying network structure, associated with an IMP
problem instance, to guide the metaheuristic algorithm towards potentially bet-
ter problem solutions.
The algorithm was experimentally evaluated on a series of randomly gener-
ated test networks and obtained results conﬁrm that it outperforms the original
genetic method in the majority of cases. The analysis of experimental results
shows that it is especially useful when the search space is large. However, it also
shows that it might be prone to premature convergence. Future work on this
topic will include developement of measures to mitigate this problem, evalua-
tion of the usefulness of diﬀerent network metrices as auxiliary information, and
an evaluation of the proposed approach on real–world IMP instances from the
areas of social networks and others.
Acknowledgement. This work was supported by the Czech Science Foundation
under the grant no. GA15-06700S, and by the projects SP2017/100 and SP2017/85
of the Student Grant System, VˇSB-Technical University of Ostrava.

640
P. Kr¨omer and J. Nowakov´a
References
1. Bucur, D., Iacca, G.: Inﬂuence Maximization in Social Networks with Genetic
Algorithms, pp. 379–392. Springer, Cham (2016)
2. Cao, T., Wu, X., Wang, S., Hu, X.: Maximizing inﬂuence spread in modular social
networks by optimal resource allocation. Expert Syst. Appl. 38(10), 13128–13135
(2011)
3. Czarn, A., MacNish, C., Vijayan, K., Turlach, B.A.: Statistical exploratory analysis
of genetic algorithms: the inﬂuence of gray codes upon the diﬃculty of a problem.
In: Australian Conference on Artiﬁcial Intelligence, pp. 1246–1252 (2004)
4. Gomez-Rodriguez, M., Song, L., Du, N., Zha, H., Sch¨olkopf, B.: Inﬂuence esti-
mation and maximization in continuous-time diﬀusion networks. ACM Trans. Inf.
Syst. 34(2), 1–33 (2016)
5. Gong, M., Song, C., Duan, C., Ma, L., Shen, B.: An eﬃcient memetic algorithm
for inﬂuence maximization in social networks. IEEE Comput. Intell. Mag. 11(3),
22–33 (2016)
6. Gong, M., Yan, J., Shen, B., Ma, L., Cai, Q.: Inﬂuence maximization in social
networks based on discrete particle swarm optimization. Inf. Sci. 367–368, 600–
614 (2016)
7. Goyal, A., Lu, W., Lakshmanan, L.V.S.: Simpath: an eﬃcient algorithm for inﬂu-
ence maximization under the linear threshold model. In: 2011 IEEE 11th Interna-
tional Conference on Data Mining, pp. 211–220, December 2011
8. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems. University of
Michigan Press, Ann Arbor (1975)
9. Kempe, D., Kleinberg, J., Tardos, E.: Maximizing the spread of inﬂuence through
a social network. In: Proceedings of the Ninth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, pp. 137–146, KDD 2003. ACM,
New York (2003)
10. Kermani, M.A.M.A., Ardestani, S.F.F., Aliahmadi, A., Barzinpour, F.: A novel
game theoretic approach for modeling competitive information diﬀusion in social
networks with heterogeneous nodes. Physica A: statistical mechanics and its appli-
cations 466, 570–582 (2017)
11. Kr¨omer, P., Platos, J.: Genetic algorithm for sampling from scale-free data and
networks. In: Genetic and Evolutionary Computation Conference, GECCO 2014,
Vancouver, BC, Canada, 12–16 July 2014, pp. 793–800 (2014)
12. Kr¨omer, P., Platos, J.: Evolutionary feature subset selection with compression-
based entropy estimation. In: Proceedings of the 2016 on Genetic and Evolution-
ary Computation Conference, Denver, CO, USA, 20–24 July 2016, pp. 933–940
(2016)
13. Kr¨omer, P., Platoˇs, J., Nowakov´a, J., Sn´aˇsel, V.: Optimal column subset selection
for image classiﬁcation by genetic algorithms. Ann. Oper. Res. 1–18 (2016)
14. Li, W., Bai, Q., Zhang, M.: Agent-based inﬂuence propagation in social networks.
In: 2016 IEEE International Conference on Agents (ICA), pp. 51–56, September
2016
15. Mitchell, M.: An Introduction to Genetic Algorithms. MIT Press, Cambridge
(1996)
16. Nguyen, D.L., Nguyen, T.H., Do, T.H., Yoo, M.: Probability-based multi-hop diﬀu-
sion method for inﬂuence maximization in social networks. Wirel. Pers. Commun.
93(4), 1–14 (2017)

Guided Genetic Algorithm for the Inﬂuence Maximization Problem
641
17. Qin, Y., Ma, J., Gao, S.: Eﬃcient inﬂuence maximization under tscm: a suitable
diﬀusion model in online social networks. Soft Comput. 21(4), 827–838 (2017)
18. Samadi, M., Nikolaev, A., Nagi, R.: The temporal aspects of the evidence-based
inﬂuence maximization on social networks. Optim. Methods Softw. 32(2), 290–311
(2017)
19. Shi, Q., Wang, H., Li, D., Shi, X., Ye, C., Gao, H.: Maximal Inﬂuence Spread for
Social Network Based on MapReduce. In: Wang, H., et al. (eds.) ICYCSEE 2015.
Communications in Computer and Information Science, vol. 503, pp. 128–136.
Springer, Berlin, Heidelberg (2015)
20. Tong, G., Wu, W., Tang, S., Du, D.Z.: Adaptive inﬂuence maximization in dynamic
social networks. IEEE/ACM Trans. Netw. 25(1), 112–125 (2017)
21. Tsai, C.W., Yang, Y.C., Chiang, M.C.: A genetic newgreedy algorithm for inﬂu-
ence maximization in social network. In: 2015 IEEE International Conference on
Systems, Man, and Cybernetics, pp. 2549–2554, October 2015
22. Wu, A.S., Lindsay, R.K., Riolo, R.: Empirical observations on the roles of crossover
and mutation. In: B¨ack, T. (ed.) Proceedings of the Seventh International Con-
ference on Genetic Algorithms, pp. 362–369. Morgan Kaufmann, San Francisco
(1997)

Optimal Local Routing Strategies
for Community Structured Time Varying
Communication Networks
Suchi Kumari1, Anurag Singh1(B), and Hocine Cheriﬁ2
1 Department of Computer Science and Engineering,
National Institute of Technology Delhi, New Delhi 110040, India
{suchisingh,anuragsg}@nitdelhi.ac.in
2 University of Burgundy, LE21 UMR CNRS 6306, Dijon, France
hocine.cherifi@gmail.com
Abstract. In time varying data communication networks (TVCN), traf-
ﬁc congestion, system utility maximization and network performance
enhancement are the prominent issues. All these issues can be resolved
either by optimizing the network structure or by selecting eﬃcient rout-
ing approaches. In this paper, we focus on the design of a time varying
network model and propose an algorithm to ﬁnd eﬃcient user route in
this network. Centrality plays a very important role in ﬁnding conges-
tion free routes. Indeed, the more a node is central, the more it can be
congested by the ﬂow coming from or going to its neighborhood. For
that reason, classically, routes are chosen such that the sum of centrality
of the nodes coming in user’s route is minimum. In this paper, we show
that closeness centrality outperforms betweenness centrality in the case
of community structured time varying networks. Furthermore, Kelly’s
optimization formulation for a rate allocation problem is used in order
to compute optimal rates of distinct users at diﬀerent time instants.
Keywords: Data communication networks model · System utility ·
Community structure · Closeness and betweenness centrality
1
Introduction
Well structured network topology and eﬃcient routing can lead to a congestion
free system together with an improved network capacity. The aim of this paper
is to develop a network model by considering all the aspects of growth (network
size) and alteration (rewiring & removal of link) of the network. In the designed
network, nodes establish dedicated connections to the nodes they want to com-
municate with. These connections are formed such that they are able to manage
traﬃc eﬃciently and minimize congestion in the network. In the communication
networks, communities are formed and these structures can be exploited in order
to send data more eﬃciently. As the network topology is evolving trough time, it is
important to study the strategies used for routing data between user’s source and
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 642–653, 2017.
DOI: 10.1007/978-3-319-62389-4 53

Optimal Local Routing Strategies
643
destination pair. In this work we consider approaches based on the importance of
the nodes. The importance of the node is related to various centrality measures:
(a) calculate the betweenness centrality of nodes using shortest path. (b) calcu-
late the closeness centrality of the nodes. It is of prime interest to compare both
routing strategies. Indeed, if the network is least congested then users can send a
maximum amount of data to the destination node through the established route
and can maximally utilize the system within their decided budget.
Real-world networks, such as social networks, communication networks, cita-
tion networks, traﬃc networks etc. [15] are growing with time, hence there is a
need to include time with the nodes and links of the basic network structure.
G(N, E, t). Many models have been proposed for time-varying networks (TVNs)
[3,18]. Wehmuth et al. [18] have proposed a unifying model for representing
TVNs. TVN nodes are considered as independent entities at each time and it is
analyzed that the proposed TVN is isomorphic to a static graph. A framework to
represent mobile networks dynamically in a spatio-temporal fashion is designed
and algebraic structural representation is also provided [10]. Most of the real
world networks are scale free and follow a power law degree distribution with
power law exponent α in the range of (2, 3]. In the Barabasi Albert (BA) model
[1], new links are added when new nodes are added into the system and this
links preferentially attach the new nodes to high degree nodes. Many models
have been developed as a variation of the BA model. Tadic [17] has proposed a
model considering both outﬂowing and inﬂowing links and has shown correlation
between them. She has considered network expansion as well as update within
the network. Apart from addition and update of links, removal of infrequently
used links is considered in [9]. Addition of new links is based on preferential
attachment, while preference for update is given to most active nodes at time t.
Some researchers have tackled the issue of improving the capacity of the
network by restructuring the network according to traﬃc and congestion in the
network. Zhao et al. [19] have redistributed the load of heavily loaded nodes to
the other nodes in the network. Once a network is designed, then a connection
is established between source and destination. In diﬀerent networks connections
are established for diﬀerent purposes. In communication networks, data packets
are sent between source and destination, goods are supplied from vendor to
customer in transportation networks, signals are passing in electric network. In
all these networks, objects are sent such that routing cost as well as congestion
is minimized. In communication networks, two rates are associated with each
node: packet generation rate λ and packet forwarding rate μ. There is a critical
rate of packet generation (λc), below which network is free from congestion. Two
models are proposed by Zhao et al. [19] for ﬁnding packet delivery rate of a node,
(i) based on node’s links and (ii) number of the shortest path passing through
the node. Du et al. [4] have developed a model in which packets propagate
to minimize the load and are processed by the Shortest Remaining Packet First
(SRPF) strategy. A heuristic routing strategy is used by Jiang et al. [7], in which
routing process is divided into N (size of network) steps and routes are based
on the betweenness centrality and node degree information. All the proposed
strategies are used to enhance the capacity of the system.

644
S. Kumari et al.
In all the previously discussed approaches, global information is needed for
addition, update and removal of links. It is not easy to get this global information
about the network as it is evolving and its size is usually very large. Therefore
local information can play an important role. Various local routing strategies
have been proposed by Lin et al. [13]. It includes node duplication avoidance,
searching next nearest neighbors and restricted queue length. In Internet net-
work, a router searches for the connection of shortest distance when establishing
a new connection. Thus, the routers within the same region will have more con-
nections than the others. Hence, to capture the localization property of these
kind of networks, a local world model is proposed [20]. This local inhomogeneity
leads to the formation of communities in the networks. Community structure is
an important characteristic of networks structure; it is observed in many real
world network such as social, biological, computer network, Internet etc. A group
of nodes which have common properties or play similar role within the network
form communities. Even if there is no clear deﬁnition of communities, it is gen-
erally admitted that a network is community structured, if group of nodes are
densely connected within the community, and sparsely connected between the
communities [5]. To understand the strength and weakness of all the available
community detection methods, Fortunato et al. [6] have surveyed the most inﬂu-
ential detection algorithms. In community structured network, dedicated routing
approaches may be applied for sending user’s data. As the size of the commu-
nication networks are increasing day by day, so one of the challenging task for
service providers is to allocate the resource bandwidth among the users fairly
and acquiring global information about the network topology. As Internet is a
public domain, so pricing mechanism is also introduced to control the action of
users and to provide quality of service among users.
Kelly [8] has considered rate allocation problem as an assignment of optimal
rate to each user for maximizing individual user’s utility as well as system utility.
Users are assumed to generate elastic traﬃc and they control their rates based on
the feedback provided by the network. For large network, it is quite diﬃcult to
solve the system utility, hence he divided the whole problem in two subproblem:
network and user’s optimization problem. The network tends to optimize its cost
whereas the user wants to maximize its rate based on system utility. La et al. [11]
have introduced a suitable pricing scheme for each user where users update their
target queue size when certain constraints are fulﬁlled. Mo et al. have proposed
an algorithm in which computation of the optimal target queue sizes are not
needed, but the users update their window size implicitly. The concept of self
updating of window size is taken by the paper based on ﬂuid model of network
and is developed by Mo et al. [14].
This paper addresses the problem to discover suitable route in time vary-
ing networks with or without congestion. For routing, centrality of nodes is
considered as its plays an important role in network congestion. In community
structured network, we rely on closeness and betweenness centrality of each node
individually within each community. Closeness centrality of a node u is the recip-
rocal of sum of the shortest path distances from all other nodes NΩ(t) −1 to u

Optimal Local Routing Strategies
645
within the community Ω and it is normalized by the sum of all possible short
distance i.e.,
C(u) =
NΩ(t) −1

v̸=u∈Ω d(v, u)
where d(v, u) is the shortest path distance between v and u and NΩ(t) is the
total number of nodes within the community at time t.
Similarly, Betweenness centrality of a node v is equal to the number of short-
est paths from all node pairs of the community pass through that node v and is
given by
g(v) =

s̸=v̸=d∈Ω
σs→d(v)
σs→d
where s, d and v lies within the community Ω. In the proposed approach, the
route of the user is chosen such that the sum of closeness centrality of the nodes
coming in user’s shortest route should be minimum to avoid congestion. If a
node is closer to more number of nodes then the probability of appearance of
the node in the user’s routes increases and hence chances of congestion at that
node increases accordingly. Our aim is to check availability of alternate routes to
minimize congestion in the system. User’s optimal data rate of the chosen route
is calculated by using Kelly’s rate control algorithm [8].
Section 2 describes the mathematical model used in the analysis of rate con-
trol behavior of the user’s route. Section 3 introduces model for growth dynamics
of TVCN. Section 4 presents the proposed routing strategies for communica-
tion network. Section 5 reports simulation results, and in Sect. 6, conclusion and
future directions of the work are discussed.
2
Mathematical Model for Rate Allocation Problem
Network structure is changing with time as nodes and links are added and
rewired into the system. Let the network consists of N nodes and E links and
life expectancy of the network is considered as T. A set of R users are willing
to access and send data to the desired location through the network. A link eab
establishes a connection between node a and node b and can send maximum
Ceab units of data through it, where, Ceab is the capacity of the link eab and
eab ∈E. Each user r is assigned a route r ∈R in the duration of time t(i−1) ∈T
to ti ∈T. At the end of tth
i
time, a zero-one matrix A of the size (N × N)ti is
deﬁned where, Ani,nj(ti) = 1, if node ni and nj are connected in the duration
of time t(i−1) to ti, otherwise Ani,nj(ti) = 0. System utility for each user r ∈R
is dependent on the data rate xr(ti) of the user r and is denoted by Ur(xr(ti)).
It is a increasing, strictly concave function of xr(ti) over the range xr(ti) ≥0.
Each user calculate its utility by using a suitable utility function. Once utility of
each user r is calculated then aggregate utility is calculated by summing all the

646
S. Kumari et al.
utilities of users and is denoted as 
r∈R Ur(xr(ti)). The rate allocation problem
can be formulated as the following optimization problem.
SY STEM(U(ti), A(ti))
maximize

r∈R,ti
Ur(ti)(xr(ti))
(1)
AT (ti)x(ti) ≤C(ti) and x(ti) ≥0
A(ti) is a matrix at time interval ti−1 to ti. The given constraint states that
a link can not send data more than its capacity and the value of the ﬂow, x(ti),
should be non-negative [8]. As the system does not know the utility function of
each user so it is quite diﬃcult and unmanageable for the system to ﬁnd optimal
rate of distinct users in the network. Hence, Kelly [8] has divided this prob-
lem into two simpler problems named as user’s optimal problem and network’s
optimal problem [8].
At each time instant if a user wants to access a link eab ∈E then, the cost
will depend on the total data ﬂow through the link at that time. It is given
by ψeab(ti) = ςeab(
r:eab∈E xr(ti)) where, ςeab(•) is an increasing function of
the data rate. The optimal data rate (x∗) can be obtained by considering the
following system of diﬀerential equation
dxr(ti)
dti
= ϑr(Pr(ti) −xr(ti)

eab∈r
ψeab(ti))
(2)
Here, ϑr is a proportionality constant. Equation (2) consists of two compo-
nents: a steady increase in the rate proportional to Pr(ti) and a steady decrease
in the rate proportional to the response ψeab(ti) provided by the network.
3
Proposed Time Varying Network Model
Time varying data communication networks are designed by maintaining speciﬁc
set of rules where nodes are connected via directed links. Directed links may
belong to two categories: links which come out from a node (out-ﬂowing) and
links that are incidental to a node (in-ﬂowing). As the network is scale free
hence, the degree distributions of both type of links follow a power law with
distinct power law exponents. In the proposed model, a node prefer to attach
with the node which is endorsed by the maximum number of existing nodes in
the network and hence, the preferences are assigned to in-ﬂowing links [1,17].
A communication network model is proposed based on the BA model [1]; the
concept of rewiring of links is taken from the model proposed in [17]. Apart
from the concept of rearrangement of links, we have considered the removal of
ineﬃcient links from the network. At each time instant ti ∈T, a new node n(ti) is
added to the network (expansion) and a number X(≤n0) is selected for network
growth and link rearrangement and removal, where n0 is the initial number of
nodes present in the seed network. Links are divided into three categories: newly

Optimal Local Routing Strategies
647
added, rewired and removed links. Distribution of links is done using the given
set of rules [9].
Notations:
(a) β = fraction of links out-ﬂowing from new incoming node n(ti) at time ti,
0 < β < 1.
(b) γ = fraction of links, those are rearranged in the existing network, 0.5 <
γ ≤1.
Using the above notations, the following set of rules may be deﬁned:
(i) Total number of newly appearing out-ﬂowing links from the new appearing
node, n(ti)
fadd(ti) = βX
(ii) Total rewired links within existing network,
frewire(ti) = γ(X −fadd(ti)) = γ(1 −β)X
(ii) Remaining part of X is used for deleting the most infrequently used links.
fdelete(ti) = X −fadd(ti) −frewire(ti) = (1 −γ)(1 −β)X
While studying the behavior of communication networks, the concept of pre-
ferred linking to a node is driven by the demand of the node to outﬂow the data
into the network. Only few inﬂuential nodes have the right to rewire their links.
Nodes rewire their infrequently used links and connect it to preferred nodes for
data communication. The preference is given to the infrequently used links for
the removal of links. Hence, the links attached to a node having lower degree
will be preferred for removal. Once the network is formed, we use the community
detection algorithm proposed in [2] in order to uncover its community structure.
This community detection method is based on modularity optimization and it
performs well in terms of computation time and quality. The method is divided
in two parts: in ﬁrst part, a diﬀerent community is assigned to each node and
in second part, new network is built based on the results obtained during the
ﬁrst part and both parts are called recursively. In the following we refer to it as
the community structured network. Further routing strategies for each user is
formed such that network congestion is minimized.
4
Routing Strategies Using Closeness Centrality
Here, the capacity of a node is calculated by the in-degree of the node and over-
all capacity (C) is calculated by summing up the capacity of all the individual
nodes in the communication network. The load, ln at any node n, is calculated
by summing up the packet generation rate of the in-ﬂowing link at that node.
Aggregate values of the packet generation rate of all the nodes are termed as

648
S. Kumari et al.
the load of the network. When, load (L) exceeds the capacity (C) of the net-
work then, it becomes congested with the overloaded packets. In large networks,
multiple path exists between the pair of nodes. We want to ﬁnd eﬃcient short
path among them. Multiple users try to access the network and want to send
data from source s to destination d. As the network is community structured,
user’s source and destination may lie in diﬀerent communities. The betweenness
centrality and closeness centrality of each node are calculated within each com-
munity. If a node serves as the only way to reach some other nodes then that
node will have the highest betweenness centrality. It becomes an articulation
point and removing it would disconnect the network. If most of the shortest
paths pass through a node, then its betweenness centrality is high and sending
data through it will lead to the congestion in the network. When a node has the
shortest paths to all other nodes then closeness centrality of the node has the
highest value and it becomes a key node to send data eﬃciently across the net-
work; indeed, it will also be congested. Therefore, it is important to investigate
a shortest path σ(s →d) between the user pairs such that the overall value of
closeness centralities of the nodes appear in the path.
θC[σz(s →d)] =

v:v∈σz(s→d)
Cv
here, Cv = closeness of node v
should be minimum. It is deﬁned by,
min {∀z : θC(σz(s →d))}
A similar formulation can be given for ﬁnding minimum sum of betweenness
centrality θg of user paths. Capacity of a link Ceab is dependent on the degrees
ka and kb of nodes a and b. It can be approximated by a power-law dependence
[16].
Ceab = b(kakb)α
here, b = 1
K and K =

n∈N deg−(n)
| N |
, average degree of the network
α = 1
(3)
α is the degree inﬂuence exponent which depends on the type of networks and b is
a positive quantity. Three possible relations between capacity Ceab and exponent
α: (i) if α > 0 the data is transmitted through high degree nodes (ii) if α < 0
the data will be sent through low degree nodes and (iii) α = 0 shows degree
independent transmission. In the proposed model, we have taken a positive value
of α. Algorithmic steps for expansion, rewiring and removal of links in the TVCN
is given in [9]. In the time varying network, various users want to communicate
data from a source s to destination d. User gives information about the source
and destination, and accordingly s →d pairs are generated. Increments in the

Optimal Local Routing Strategies
649
number of users lead to the network being into a congested state. Therefore, our
aim is to ﬁnd eﬃcient routing paths such that a maximum number of users are
getting beneﬁted with a unique stable value of the optimal data sending rate x∗
r
and corresponding convergence vectors will be x∗= x∗
r, r ∈R using Eq. (2).
After ﬁnding paths of all the users, optimal data sending rate of each user
is calculated by using rate allocation Eq. (2). Detailed description for selecting
shortest path with lowest closeness centrality along with optimal rate is given in
Algorithm 1.
Algorithm 1. Finding shortest path having lowest (highest) closeness centrality
and optimal rate for each user
1: Input: All user’s s −d pairs Nsd, a and b such that a > 0 & 0 < b < 1.
2: for i := 1 to length(Nsd) do
3:
Evaluate all shortest paths χi of user i.
4:
Select shortest paths χi(s →d) having maximum and minimum value of
θC[σm(χi(s →d))].
5: end for
6: for i := 1 to length(Nsd) do
7:
for k := 1 to length(Nsd(i)) do
8:
Update network feedback ψk for each element k.
9:
end for
10:
xr = min(xNsd(i));
11:
A(r) = rand(1, 10);
12:
Pr = xr ∗(
a
xr+b);
13:
ψr = ψd{∀d : d ∈Nsd(i)};
14: end for
15: Use the value of xr, A(r), Pr and ψr to ﬁnd the rate of convergence of each user.
5
Simulation and Results
The simulation is started by establishing the structure of the network accord-
ing to the TVCN model proposed in the Sect. 3. It is aimed for representing
real-world networks, hence, the degree distribution is found power law. In the
simulation, the parameters are set to be seed node n0 = 5, number X = 5,
fraction of newly added links β range in (0, 1), fraction of rewired links γ is in
the range of (0.5, 1), with network size ranging from N = 103 to 104. Any pair of
nodes in the network may be considered as user’s s−d sets or may participate in
routing also. Data forwarding capacity of a node n, Cn is deﬁned by in-ﬂowing
degree deg−(n) of the node n. Capacity of a link Cemn is obtained by Eq. (3). At
each step, the degree of the nodes will be diﬀerent, hence capacity of the nodes
as well as links change accordingly.
In the communication networks, multiple paths are available among desired
source destination pairs for sending packets for each user. All routes are assumed
as equally weighted hence, users can select any of the available routes for sending

650
S. Kumari et al.
the packets. Each user can send data along one of the shortest paths to the
destination with a maximum ﬂow rate of individual links. The data sending rate
is reduced as multiple users want to share the common resources. Furthermore,
user may not send more data with maximum rate. User’s rate depends on two
parameters; it’s own willingness to pay Pr(ti) and network’s feedback ψr(ti) =

e∈r ψe(ti). Using rate control given in Eq. (2), an optimal data sending rate of
each user is obtained and ﬁnally the system utility is calculated by using Eq. (1).
User rates depend on the demand of particular resources coming in the shortest
route. If demand is high then, data sending rate will be low.
In Table 1, the average value of optimal user’s rate for highest and lowest
value of θg and θC are shown for both community and non-community structured
network. Betweenness centrality and closeness centrality are considered. Commu-
nity structured network, eﬃciently analyze the traﬃc condition in the network
and helps user to get more value of optimal rates for both highly congested and
least congested shortest paths. Average optimal rate obtained through the path
by considering betweenness centrality is lower for both type of networks. The
betweenness centrality approach in non community structured network gives the
lowest value.
Table 1. Average value of user’s highest and lowest rate in community and non com-
munity structured networks using betweenness and closeness centrality measure when
N = 103.
Avg. user rate lowest Avg. user rate highest
Betweenness with community
2.4537
3.2563
Betweenness without community 2.3438
2.8541
Closeness with community
2.5043
3.3241
Closeness without community
2.3992
3.2861
Total
1
10 fraction of total nodes are considered as total number of user pairs.
In the community and non community structured networks, ﬁrstly we get user’s
source destination (s −d) information and then all the shortest paths between
(s −d) pair is calculated. Among all the available shortest routes, highest and
lowest value of θC[σ(s →d)] and θg[σz(s →d)] are calculated and then optimal
data rate (highest and lowest) of each user is calculated. Simulation is per-
formed in python 2 with canopy editor and conﬁguration of the system is given
as Intel® Core T M i5-5200U CPU @ 2.20 GHz * 4 with memory size 7.7 GiB.
Overall computation time for getting optimal rate of all users with varying net-
work size is shown in Table 2. Betweenness and closeness centrality measures
in non-community structured network takes 3.81 and 0.4734 times more time
than the network with community structure respectively. While comparing the
computation time through betweenness centrality and closeness centrality mea-
sures in both kind of networks, betweenness centrality takes 0.0245 and 2.1760
times more time than closeness centrality in community and non-community

Optimal Local Routing Strategies
651
structured networks respectively. In community structured network, we study
betweenness and closeness centrality of nodes within community. Size of the
community will always far less than the size of the network N and we know
that time complexity of both the centralities depend on size of the number of
nodes in the network. From the results of both Tables 1 and 2, it is shown that
performance of betweenness centrality measure in non-community structured
networks, is not eﬃcient.
Table 2. Time taken by betweenness and closeness centrality measures in community
and non community structured networks for network size ranging from 103 to 104.
Network size Betweenness centrality
Closeness centrality
Within
community
Without
community
Within
community
Without
community
Time (in Seconds)
1000
3.240
11.412
3.092
4.462
2000
13.822
44.712
13.097
19.365
3000
33.845
101.519
32.275
49.035
4000
58.997
192.232
55.242
85.452
5000
95.512
411.987
95.216
137.906
6000
163.573
702.687
164.549
215.222
7000
178.294
992.269
170.104
316.451
8000
233.572
1055.539
235.651
364.511
9000
306.327
1490.951
290.613
443.249
10000
400.853
1917.571
392.571
543.462
Size of the network N is 103 and total
1
10 of N i.e., 102 users want to access
the networks. Here, 49 users’ source destination pairs are taken and optimal rates
of each user in community and non-community structured networks are shown
in Fig. 1. Optimal data sending rates of the 49 users with highest and lowest
closeness centrality are calculated and the result is shown in Fig. 1(a). User’s
optimal data rate is sorted according to the highest closeness value within the
community and the value obtained through the path with lowest closeness within
community and without community structured networks are plotted accordingly.
Apart from closeness centrality, there is one more measure named as betweenness
centrality, eﬀects optimal data rate x∗of each user. Results of optimal data
rates (x∗) of 49 users with aggregate sum of most and least betweenness central
nodes in community and non-community structured networks are displayed in
Fig. 1(b). User’s optimal data rate is sorted according to the highest betweenness
centrality value within the community and it is shown that the deviation of other
optimal date rate values of that user is more. Data sending rate of the user path
having lowest or highest value for the aggregate sum of betweenness and lowest
or highest total sum of closeness, is always maximum or minimum.

652
S. Kumari et al.
Fig. 1. User’s optimal rates of user paths with (a) highest & lowest closeness and (b)
betweenness (highest & lowest) in community and non-community structured networks
6
Conclusion and Future Directions
A time varying communication networks (TVCN) model is designed by applying
variation on BA model [1]. We consider the case where nodes are added continu-
ously and links may appear, disappear and rewired randomly with time by using
some predeﬁned probability condition. Using a community detection algorithm,
we partition the proposed TVCN into communities and then ﬁnd betweenness
and closeness centrality of each node within community. For dynamic networks,
closeness centrality measure performs better in terms of time complexity [12].
Time complexity for ﬁnding user optimal path on the basis of betweenness cen-
trality measure for the case of non community structured networks is much
higher than the closeness centrality approach in all kind of networks (community
and non community structured). Computation time for both kind of measures
are studied at diﬀerent time instants with increased network size N. Interval
of the two time instants (Δt) is taken as Δt = 103. For each user’s path opti-
mal data sending rate is obtained by using rate control equation proposed by
Kelly [8]. Shortest path with lowest and highest sum of closeness centrality are
chosen for routing and observed that the value of optimal rate will always be
maximum for the path containing least closeness central nodes.
In this paper, system utility may be calculated with consideration of all kind
of delays in the network. Utility function, itself can be replaced by using some
other Lyapunov function. Some window size update method can also be studied.

Optimal Local Routing Strategies
653
References
1. Barab´asi, A.L., Albert, R., Jeong, H.: Mean-ﬁeld theory for scale-free random
networks. Phys. A 272(1), 173–187 (1999)
2. Blondel, V.D., Guillaume, J.L., Lambiotte, R., Lefebvre, E.: Fast unfolding of
communities in large networks. J. Stat. Mech. Theory Exp. 2008(10), P10008
(2008)
3. Casteigts, A., Flocchini, P., Quattrociocchi, W., Santoro, N.: Time-varying graphs
and dynamic networks. Int. J. Parallel Emergent Distrib. Syst. 27(5), 387–408
(2012)
4. Du, W.B., Wu, Z.X., Cai, K.Q.: Eﬀective usage of shortest paths promotes trans-
portation eﬃciency on scale-free networks. Phys. A 392(17), 3505–3512 (2013)
5. Fortunato, S.: Community detection in graphs. Phys. Rep. 486(3), 75–174 (2010)
6. Fortunato, S., Hric, D.: Community detection in networks: a user guide. Phys. Rep.
659, 1–44 (2016)
7. Jiang, Z.Y., Liang, M.G.: Incremental routing strategy on scale-free networks.
Phys. A 392(8), 1894–1901 (2013)
8. Kelly, F.P., et al.: Mathematical modelling of the internet. In: Engquist, B., Schmid,
W. (eds.) Mathematics Unlimited-2001 and Beyond 1, pp. 685–702. Springer,
Heidelberg (2001)
9. Kumari, S., Singh, A.: Modeling of data communication networks using dynamic
complex networks and its performance studies. In: Cheriﬁ, H., Gaito, S., Quattro-
ciocchi, W., Sala, A. (eds.) Complex Networks & Their Applications V. SCI, vol.
693, pp. 29–40. Springer, Cham (2017). doi:10.1007/978-3-319-50901-3 3
10. Kumari, S., Singh, A., Ranjan, P.: Towards a framework for rate control on dynamic
communication networks. In: Proceedings of the International Conference on Inter-
net of Things and Cloud Computing, p. 12. ACM (2016)
11. La, R.J., Anantharam, V.: Utility-based rate control in the internet for elastic
traﬃc. IEEE/ACM Trans. Netw. 10(2), 272–286 (2002)
12. Lee, M.J., Lee, J., Park, J.Y., Choi, R.H., Chung, C.W.: Qube: a quick algorithm
for updating betweenness centrality. In: Proceedings of the 21st International Con-
ference on World Wide Web, pp. 351–360. ACM (2012)
13. Lin, B., Chen, B., Gao, Y., Chi, K.T., Dong, C., Miao, L., Wang, B.: Advanced algo-
rithms for local routing strategy on complex networks. PLoS ONE 11(7), e0156756
(2016)
14. Mo, J., Walrand, J.: Fair end-to-end window-based congestion control. IEEE/ACM
Trans. Netw. (ToN) 8(5), 556–567 (2000)
15. Newman, M.: Complex systems: a survey. arXiv preprint arXiv:1112.1440 (2011)
16. Onnela, J.P., Saram¨aki, J., Hyv¨onen, J., Szab´o, G., Lazer, D., Kaski, K., Kert´esz,
J., Barab´asi, A.L.: Structure and tie strengths in mobile communication networks.
Proc. Natl. Acad. Sci. 104(18), 7332–7336 (2007)
17. Tadi´c, B.: Dynamics of directed graphs: the world-wide web. Phys. A 293(1), 273–
284 (2001)
18. Wehmuth, K., Ziviani, A., Fleury, E.: A unifying model for representing time-
varying graphs. In: IEEE International Conference on Data Science and Advanced
Analytics (DSAA), 36678 2015, pp. 1–10. IEEE (2015)
19. Zhao, L., Lai, Y.C., Park, K., Ye, N.: Onset of traﬃc congestion in complex net-
works. Phys. Rev. E 71(2), 026125 (2005)
20. Zhou, M., Yang, J.H., Liu, H.B., Wu, J.P.: Modeling the complex internet topology.
J. Softw. 20(1), 109–123 (2009)

Graph Construction Based on Local
Representativeness
Eliska Ochodkova, Sarka Zehnalova, and Milos Kudelka(B)
FEI, VSB, Technical University of Ostrava,
17. Listopadu 15, 708 33 Ostrava, Czech Republic
{eliska.ochodkova,sarka.zehnalova,milos.kudelka}@vsb.cz
Abstract. Graph construction is a known method of transferring the
problem of classic vector data mining to network analysis. The advan-
tage of networks is that the data are extended by links between certain
(similar) pairs of data objects, so relationships in the data can then be
visualized in a natural way. In this area, there are many algorithms, often
with signiﬁcantly diﬀerent results. A common problem for all algorithms
is to ﬁnd relationships in data so as to preserve the characteristics related
to the internal structure of the data. We present a method of graph con-
struction based on a network reduction algorithm, which is found on
analysis of the representativeness of the nodes of the network. It was
veriﬁed experimentally that this algorithm preserves structural charac-
teristics of the network during the reduction. This approach serves as
the basis for our method which does not require any default parameters.
In our experiments, we show the comparison of our graph construction
method with one well-known method based on the most commonly used
approach.
Keywords: Graph construction · Machine learning · Nearest neighbor ·
Local representativeness
1
Introduction
Simply put, the methods of data mining analyze data arranged in a table
whose rows are data objects and columns are attributes (features). Typical
tasks include, e.g., clustering and classiﬁcation. These two areas have diﬀer-
ent objectives, but both are based on the assumption that there are internal
data structures which can be utilized. These structures are then meant for solv-
ing clustering, classiﬁcation, and other problems. While examining structures,
the distance (similarity) of individual data objects, in particular, is examined.
Assuming that we are working with numeric attributes, using e.g. the Euclidean
distance or Gaussian function, the distance or similarity, respectively, can be
precisely measured. Due to these measurable properties, we can apply many dif-
ferent algorithms that reveal information about the structures mentioned above.
However, it is not easy to ﬁnd the way to visualize such data and structures
because usually we work with more than three dimensions. Visualization is one
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 654–665, 2017.
DOI: 10.1007/978-3-319-62389-4 54

Graph Construction Based on Local Representativeness
655
of the essential tools allowing for an easier understanding of the data. Graphs
(networks) have the property that, thanks to various layout algorithms, pro-
vide a clear visualization depicting the natural structure of data, particularly
for small networks (for large-scale networks with higher computational costs).
Besides visualization, networks also provide space for the application of analyt-
ical methods which can describe other characteristics (node ranking, local and
global clustering coeﬃcient, community structure, etc.).
As a graph (or network), we understand an ordered pair G = (V, E) (undi-
rected, unweighted graph) of a set V of nodes and a set E of edges which are
unordered pairs of nodes from G.
The basic problem of graph construction is to ﬁnd the parameters for conver-
sion, which retain the essential properties of transferred data. The nodes in the
constructed graph represent individual objects where interconnection between
nodes can be done in various ways. The result of conversion should be a graph
in which the clusters, the nearest neighbors (in the sense of distance or similarity,
respectively), outliers, etc., are preserved. One of the known approaches based
on the nearest neighbor analysis published Huttenhower et al. [9]. In this app-
roach, in addition to graph construction, the main objective is to ﬁnd strongly
interconnected clusters in the data. However, the method assumes that the user
must specify the number of nearest neighbors with which the algorithm works.
Methods using the principle based on the use of k nearest neighbors are referred
to as the k-NN networks and assume the k parameter to be a previously known
value.
In our method, we use the nearest neighbors in another way. We assume
that data objects (and consequently future graph nodes) have a diﬀerent repre-
sentativeness. The representativeness is a local property based on the number
of objects that are the nearest neighbors of a selected node. Zehnalova et al.
[17,18]) showed in their experiments that local representativeness could be used
for data reduction in which the structure and properties of the original data are
very well preserved.
In our approach we use representativeness in the construction of the graph so
that (1) we create edges between all pairs of nearest neighbors, and (2) we create
additional edges between the individual data objects in the number proportional
to the representativeness of these objects. Representativeness of nodes in the
constructed graph then corresponds, approximately, to the representativeness of
objects in the data. This makes a natural graph representation of the original
data, which preserves its local properties.
The paper is organized as follows: In Sect. 2, we discuss the related work. The
proposed method is presented in Sect. 3. In Sect. 4, we focus on the experiment
and on its results. Section 5 concludes the paper.
2
Related Work
Graphs are powerful tools for data analysis, and graph-based algorithms are
currently widely used in various ﬁelds. Construction of a graph from vector data

656
E. Ochodkova et al.
represents a precursor of many tasks in machine learning, computer vision, and
signal processing. Examples include semi-supervised learning (SSL) or spectral
clustering. For graph-based SSL algorithms, a graph is a central object on which
the subsequent learning takes place. The nodes of the graph represent data
objects, and weighted edges express the relationships between the nodes.
For some application domains, the relationships between data are natural
(e.g. social networks, collaboration networks, citation networks, the web). In this
case, graph-based methods, such as methods of social network analysis (SNA)
which use a graph as the basic representation of data, can be applied straightfor-
wardly. However, most data mining tasks consider data to be independent and
identically distributed (i.i.d.) variables so we do not have access to any explicit
graph structure. Therefore, a graph of independent objects is often generated
ﬁrst, and then graph-based machine learning methods are used. Even though
graph-based machine learning methods are currently very popular, little atten-
tion is paid to the impact of graph construction methods and their results [7] and
there is not a general way to establish high-quality graphs [11]. As an example
study of the impact of graph construction see Maier et al. [12], where the authors
discuss the inﬂuence of graph construction on graph-based clustering measures
such as the normalized cut. However, little attention is paid to analyzing the
properties of the resulting graph using SNA methods.
From a wider perspective, graph construction techniques can be divided into
the following two groups [14]:
1. Task-dependent graph construction: algorithms in this group use both
labeled and unlabeled data to construct the graph. Labeled data can be used
for adapting the graph to the task at hand. This group of algorithms receives
less attention. An example is the semi-supervised metric learning algorithm
IDML [6] which shows how the available label information can be used for
graph construction in graph-based SSL. In particular, authors focus on learn-
ing a distance metric using available label information, which can then be
used to set the edge weights on the constructed graph.
2. Task-independent graph construction methods: algorithms in this
group do not use labeled data, so these methods can be considered unsu-
pervised. Their advantage is that they are applicable to any graph-based
learning tasks. The disadvantage is that they do not necessarily help subse-
quent learning tasks.
Graph construction, in general, consists of two consecutive steps. Once a
neighborhood graph is constructed, the edge weight assignment step follows.
The basic edge weighting methods include Gaussian function (Heat kernel) [1],
inverse Euclidean distance, local reconstructive relationship [15] or e.g. sparse
representation (SR) [16].
Construction of a graph in which both the selection of neighbors and edge
weighting take place in one step, minimizing the weighted sum of the squared
distance from each node to the weighted average of its neighbors, was proposed
by [4] as a concept of hard and α−soft graphs.

Graph Construction Based on Local Representativeness
657
A graph-based algorithm to generate clusters of genes with similar expression
proﬁles was proposed in [9]. The k-NN approach is used to construct the graph
together with the detection of overlapping cliques of a given size (similarly to [5]).
3
Theoretical Background
In this section, we deﬁne the basic ideas behind the proposed graph construction
algorithm. Zehnalova et al. [18] present an algorithm for network reduction with
linear (quadratic in the worst case) complexity, which is based on the so-called
local representativeness. The algorithm is based on the idea that objects which
are the nearest neighbors of other objects are the important ones in a dataset.
This algorithm is used in [17] for the reduction of vector data. In both cases, the
reduction retains the structural properties of the original data very well. The
following is a description of that approach.
In the following text we assume that D = (O, s) is a dataset, where O is a
set of objects and s : O × O →R≥0 is a similarity function that quantiﬁes the
similarity between two objects. A similarity function is dependent on diﬀerent
kinds of data, e.g. weighted and unweighted network data and vector data. The
similarity measure is often based on the Gaussian function for the vector data.
Objects Oj for which s(Oi, Oj) > 0 are neighbors. The neighborhood of the
object Oi is a set of all its neighbors. The nearest neighbor of object Oi is an
object Oj which is most similar to the object Oi. When more such object exists,
object Oi has more nearest neighbors.
Deﬁnition 1 (Local degree). Let d(Oi) be the number of objects for which
object Oi is a neighbor. Then d(Oi) is a local degree of object Oi. Objects for
which d(Oi) = 0 are outliers.
Deﬁnition 2 (Local signiﬁcance). Let g(Oi) be the number of objects for which
object Oi is the nearest neighbor. Then g(Oi) is local signiﬁcance of object Oi.
Object (Oi) is locally signiﬁcant if g(Oi) > 0, otherwise it is locally insigniﬁcant.
Deﬁnition 3 (x-representativeness). Let x ∈R>1 and d(Oi) > 0. Then rx(Oi)
is x-representativeness of object Oi deﬁned as follows:
rx(Oi) =
g(Oi)
logx(1 + d(Oi))
(1)
An important parameter of the x-representativeness is the base of the logarithm
x. Informally speaking, x aﬀects the degree of representativeness of the object
in relationship to its surroundings.
Deﬁnition 4 (x-representativeness base). Let x ∈Rx>1 and rx(Oi) = 1. Then
x = b(Oi) is x-representativeness base of object Oi deﬁned as follows:
b(Oi) = (1 + d(Oi))
1
g(Oi)
(2)

658
E. Ochodkova et al.
The x-representativeness base represents the threshold of x-representativeness
of the object. The smaller the x-representativeness base of the object is, the
higher its local importance is.
Locally insigniﬁcant objects and outliers do not have any x-representativeness
base, i.e. there is no x, for which locally insigniﬁcant objects would be x-
representatives of dataset D.
3.1
Local Representativeness
To construct a graph, we can use local information about individual objects
in the dataset. Based on the deﬁnitions above, we further assume that local
importance of the object is related to its x-representative base. For the graph
construction algorithm let’s deﬁne representativeness of the object and its neigh-
bor, respectively, as follows:
Deﬁnition 5 (local representativeness). Let b(Oi) be x-representativeness base
of object Oi. Then lr(Oi) is local representativeness of object Oi deﬁned as
follows:
lr(Oi) =
1
b(Oi)for locally signiﬁcantOi,
(3)
lr(Oi) = 0otherwise
(4)
Deﬁnition 6 (representative neighbor). Let
k = ROUND (lr(Oi) · d(Oi))
(5)
and K(Oi) be a set of k neighbors Oj of object Oi with highest similarity
s(Oi, Oj). Then representative neighbors of object Oi are objects Oj ∈K(Oi).
3.2
Proposed LRNet algorithm
The LRNet algorithm for the construction of the weighted graph utilizing local
representativeness is composed of four steps:
1. Create a similarity matrix S of dataset D.
2. Calculate the representativeness of all objects Oi.
3. Create the set V of nodes of graph G so that node vi of graph G represents
object Oi of dataset D.
4. Create the set of edges E of graph G so that E contains an edge eij between
nodes vi and vj (i ̸= j) if Oj is the nearest neighbor of Oi or Oj is the
representative neighbor of Oi.
The time complexity of the algorithm is O(n2), where n = |D|. This is based
on following facts:
– The calculation of the similarity matrix (the ﬁrst step of the algorithm) has
O(n2) complexity (if we neglect the complexity of calculating the similarity of
two objects).

Graph Construction Based on Local Representativeness
659
– The calculation of representativeness, as mentioned above, has in the worst
case also O(n2) complexity (the second step of the algorithm). The worst case
is a situation where the similarity matrix is dense; this can be assumed for
vector data. For data represented by a sparse similarity matrix, the complexity
is O(n); for details see [17].
– The complexity of selecting representative neighbors of objects from dataset
D (the fourth step of the algorithm) depends on the average number of rep-
resentative neighbors k that we need to select from (in general) n neighbors.
In the graph construction task, we can assume that the constructed graph is
sparse; then it holds that k ≪n and complexity is O(n2).
In the proposed LRNet algorithm, each object Oi of dataset D contributes to
the graph with the number of edges corresponding to its local representativeness.
Regarding local properties and structures in the original data, we understand
this principle as crucial to maintaining these properties.
4
Experiment
It is not possible to clearly state what properties the constructed graph should
have. It depends on the task in which the graph would be utilized. In our app-
roach, we assume that the constructed graph should be designed to best match
the converted data. Moreover, because the data are an image of reality, the con-
structed graph should reﬂect this reality. Therefore, to assess graph properties,
we use methods of social network analysis. The results should then lead to the
properties observed in real-world networks.
For comparison, we chose the often used k-NN network method [9]. This
method requires a pre-selected number of nearest neighbors k. For all of the
analyzed datasets, we chose k = Sqrt(N) where N is the number of objects
in the dataset (SqrtNN). For evaluation, we selected three datasets meant for
classiﬁcation. The reason for the analysis of labeled data is to assess how the
compared methods reﬂect known information about classes in the data. The
chosen datasets are ‘Cortex Nuclear’ [8], ‘Ecoli’ [10] and ‘Seeds’ [3]. For all of the
datasets, rescaling the range of features to scale the range in [0, 1] and Gaussian
function as a similarity measure were used.
The key properties of real-world networks are so-called small-world (small
average shortest path length and diameter of the graph), scale-free structure
(power-law degree distribution), high clustering coeﬃcient and community struc-
ture. Another property is assortativity, which has positive values for social
networks but negative values for e.g. biological or technological networks (see
Newman [13]). Table 1 shows the properties of graphs constructed from analyzed
datasets. The measured properties include the number of nodes n and edges m,
average degree <k>, average shortest path length <l>, diameter Lmax, average
clustering coeﬃcient CC, assortativity r, number of communities comL detected
by Louvain [2] algorithm, and the corresponding modularity QL.
It is evident from Table 1 that the pairs of graphs constructed by diﬀerent
methods have a similar number of edges, and therefore also an average degree

660
E. Ochodkova et al.
Table 1. Global properties of constructed graphs
Graph
n
m
<k>
<l>
lmax CC
r
comL QL
Cortex: LRNet
1080 10648 19.719 3.212
7
0.519 −0.212
8
0.607
Cortex: SqrtNN 1080 11934 22.162 4.407 10
0.528
0.476 10
0.741
Ecoli: LRNet
336
2214 13.179 3.519
9
0.599 −0.352
5
0.624
Ecoli: SqrtNN
336
1986 11.857 4.834 12
0.497
0.521
9
0.703
Seeds: LRNet
210
997
9.495 3.472
7
0.662 −0.397
5
0.635
Seeds: SqrtNN
210
1040
9.905 5.207 13
0.509
0.484
7
0.707
and density. However, graphs constructed by the k-NN algorithm, unlike graphs
constructed by LRNet, do not have some properties known for real-world net-
works. This is, in particular, longer average shortest path, large diameter, and
unnaturally high assortativity. Another important property observed in real-
world networks, that k-NN graphs do not have, is scale-free structure. The con-
sequence of scale-freeness is so-called hubs, which are nodes with a very high
degree, and also the fact that most of the nodes in real-world networks have a
low degree. Degree distribution for each of the graphs on a log-log scale is for
the LRNet method in Fig. 1 and for the SqrtNN method in Fig. 2. In graphs con-
structed by the SqrtNN method, most nodes have a higher degree and hubs are
missing. Also, only a few nodes have a lower degree. This characteristic is also
clearly documented by constructed graphs in Figs. 3 and 4. The size of the nodes
in those graphs corresponds to their degree; colors represent classes (labels).
Fig. 1. Degree distribution, LRNet
4.1
Analysis Using Classes
All three pairs of analyzed graphs are constructed from labeled data. This allows
us to measure how precisely the constructed graphs connect nodes belonging to
the same class. In Table 2, three diﬀerent values are calculated. The ﬁrst is the
average weighted precision which is the average value of
w = wpos
wall
(6)

Graph Construction Based on Local Representativeness
661
Fig. 2. Degree distribution, SqrtNN
Table 2. Classiﬁcation accuracy of constructed graphs
Graph
Weighted precission Precission Modularity by classes
Cortex: LRNet
0.607
0.870
0.376
Cortex: SqrtNN 0.643
0.887
0.457
Ecoli: LRNet
0.764
0.842
0.370
Ecoli: SqrtNN
0.777
0.869
0.343
Seeds: LRNet
0.872
0.914
0.341
Seeds: SqrtNN
0.891
0.929
0.312
for all nodes of the graph, where wpos is the sum of edge weights with neighbors
of the same class and wall is the sum of edge weights to all neighbors. The second
value is the average precision, which is the average value of p. This value p = 1
for the selected node if the sum of the of edge weights with neighbors in the
same class is higher than the sum of edge weights with neighbors in each of the
other classes. Otherwise, p = 0. The third value is modularity Q (Blondel et al.
[2]) by classes deﬁned as follows:
Q =
1
2m

ij

wij −kikj
2m

δ(ci, cj),
(7)
where wij is the edge weight between nodes vi and vj, ki and kj are the sums
of the weights of the edges between nodes vi and vj, m is the sum of all of the
edge weights in the graph, ci and cj are classes of the nodes vi and vj, and δ is
Kronecker delta.
Furthermore, Figs. 5 and 6 show ROC (receiver operating characteristic)
curves with calculated values of AUC (area under the curve) describing the
accuracy of assigning objects to each class. ROC curves are calculated from the
values of weighted precision w of individual nodes of constructed graphs.

662
E. Ochodkova et al.
Fig. 3. Constructed graphs: Ecoli dataset
The results show that the accuracy of connecting the nodes to the nodes of
the same class for the two methods diﬀer a little. The advantage of the LRNet
method is, however, that for the construction of the graph, no parameter has to
be selected, which is a required step for the k-NN method.

Graph Construction Based on Local Representativeness
663
Fig. 4. Constructed graphs: Seeds dataset
Fig. 5. Receiver operating characteristic (ROC) curves, LRNet

664
E. Ochodkova et al.
Fig. 6. Receiver operating characteristic (ROC) curves, SqrtNN
5
Conclusions
The aim of this paper was to introduce the method of graph construction, LRNet,
that does not use any parameters for the construction. The method is based
on the use of local representativeness, which, as previous research has shown,
ensures a very good preservation of the internal data structure. In our experi-
ments we used methods of social network analysis and showed that the result
of the application of the LRNet method on real biological data is a graph with
properties observed in real-world networks. Comparing with the most commonly
used k-NN method, which is based on estimating the number of nearest neigh-
bors needed to construct the graph, we showed on labeled data that the resulting
graphs interconnect nodes belonging to the same class with comparable accuracy.
One of the interesting features of the LRNet method is that due to the
properties of the constructed graph, we are able to detect objects with high
representativeness. These objects are in the network represented by high-degree
nodes and can be of two types. The ﬁrst type is the hubs, the nodes connecting
the clusters. The second type is the centers of these clusters. A more detailed
analysis of the role of these objects in the data will be the subject of further
research.
Acknowledgments. This work was supported by grant of Ministry of Health of Czech
Republic (MZ CR VES16-31852A) and by SGS, VSB-Technical University of Ostrava,
under the grant no. SP2017/85.
References
1. Belkin, M., Niyogi, P., Sindhwani, V.: Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. J. Mach. Learn. Res. 7,
2399–2434 (2006)
2. Blondel, V.D., Guillaume, J.L., Lambiotte, R., Lefebvre, E.: Fast unfolding of
communities in large networks. J. Stat. Mech. Theory Exp. 2008(10), P10008
(2008)

Graph Construction Based on Local Representativeness
665
3. Charytanowicz, M., Niewczas, J., Kulczycki, P., Kowalski, P.A., Lukasik, S., ˙Zak,
S.: Complete gradient clustering algorithm for features analysis of X-ray images.
In: Pi¸etka, E., Kawa, J. (eds.) Information Technologies in Biomedicine. Advances
in Intelligent and Soft Computing, vol. 69, pp. 15–24. Springer, Heidelberg (2010)
4. Daitch, S.I., Kelner, J.A., Spielman, D.A.: Fitting a graph to vector data. In:
Proceedings of the 26th Annual International Conference on Machine Learning,
pp. 201–208. ACM (2009)
5. Der´enyi, I., Palla, G., Vicsek, T.: Clique percolation in random networks. Phys.
Rev. Lett. 94(16), 160202 (2005)
6. Dhillon, P.S., Talukdar, P.P., Crammer, K.: Inference-driven metric learning
for graph construction. In: 4th North East Student Colloquium on Artiﬁcial
Intelligence (2010)
7. Dornaika, F., Bosaghzadeh, A.: Adaptive graph construction using data self-
representativeness for pattern classiﬁcation. Inf. Sci. 325, 118–139 (2015)
8. Higuera, C., Gardiner, K.J., Cios, K.J.: Self-organizing feature maps identify pro-
teins critical to learning in a mouse model of down syndrome. PLoS one 10(6),
e0129126 (2015)
9. Huttenhower, C., Flamholz, A.I., Landis, J.N., Sahi, S., Myers, C.L., Olszewski,
K.L., Hibbs, M.A., Siemers, N.O., Troyanskaya, O.G., Coller, H.A.: Nearest neigh-
bor networks: clustering expression data based on gene neighborhoods. BMC Bioin-
form. 8(1), 250 (2007)
10. Lichman, M.: UCI machine learning repository (2013). http://archive.ics.uci.
edu/ml
11. Liu, W., Chang, S.F.: Robust multi-class transductive learning with graphs. In:
2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009,
pp. 381–388. IEEE (2009)
12. Maier, M., Von Luxburg, U., Hein, M.: Inﬂuence of graph construction on graph-
based clustering measures. In: NIPS, pp. 1025–1032 (2008)
13. Newman, M.E.: Assortative mixing in networks. Phys. Rev. Lett. 89(20), 208701
(2002)
14. Subramanya, A., Talukdar, P.P.: Graph-based semi-supervised learning. In: Syn-
thesis Lectures on Artiﬁcial Intelligence and Machine Learning, vol. 8(4), pp. 1–125
(2014)
15. Wang, F., Zhang, C.: Label propagation through linear neighborhoods. IEEE
Trans. Knowl. Data Eng. 20(1), 55–67 (2008)
16. Yan, S., Wang, H.: Semi-supervised learning by sparse representation. In: Pro-
ceedings of the 2009 SIAM International Conference on Data Mining, SIAM, pp.
792–801 (2009)
17. Zehnalova, S., Kudelka, M., Platos, J.: Local representativeness in vector data. In:
2014 IEEE International Conference on Systems, Man and Cybernetics (SMC),
pp. 894–899. IEEE (2014)
18. Zehnalova, S., Kudelka, M., Platos, J., Horak, Z.: Local representatives in weighted
networks. In: 2014 IEEE/ACM International Conference on Advances in Social
Networks Analysis and Mining (ASONAM), pp. 870–875. IEEE (2014)

SHADE Algorithm Dynamic Analyzed Through
Complex Network
Adam Viktorin(B), Roman Senkerik, Michal Pluhacek, and Tomas Kadavy
Faculty of Applied Informatics, Tomas Bata University in Zlin,
T. G. Masaryka 5555, 760 01 Zlin, Czech Republic
{aviktorin,senkerik,pluhacek,kadavy}@fai.utb.cz
Abstract. In this preliminary study, the dynamic of continuous opti-
mization algorithm Success-History based Adaptive Diﬀerential Evolu-
tion (SHADE) is translated into a Complex Network (CN) and the basic
network feature, node degree centrality, is analyzed in order to provide
helpful insight into the inner workings of this state-of-the-art Diﬀerential
Evolution (DE) variant. The analysis is aimed at the correlation between
objective function value of an individual and its participation in produc-
tion of better oﬀspring for the future generation. In order to test the
robustness of this method, it is evaluated on the CEC2015 benchmark
in 10 and 30 dimensions.
Keywords: Diﬀerential evolution · SHADE · Complex network ·
Centrality
1
Introduction
Heuristic algorithms based on the original Diﬀerential Evolution (DE) [1] are
constantly ranked high in competitions on continuous optimization during the
last years [2–7]. The common aspect of successful variants is the ability to adapt
the optimization process to the given problem. The original DE from 1995 had
three static control parameters, which were required to be set by the user. Those
parameters are population size NP, scaling factor F and crossover rate CR.
Adaptive algorithms overcome the issue of ﬁne-tuning of those parameters by
initializing them to predeﬁned values and changing them during the optimization
process in accordance with successful values of these parameters from previous
generations. The adaptivity of these algorithms is trying to overcome the famous
No Free Lunch (NFL) theorem [8].
A. Viktorin—This work was supported by Grant Agency of the Czech Republic –
GACR P103/15/06700S, further by the Ministry of Education, Youth and Sports
of the Czech Republic within the National Sustainability Programme Project no.
LO1303 (MSMT-7778/2014). Also by the European Regional Development Fund
under the Project CEBIA-Tech no. CZ.1.05/2.1.00/03.0089 and by Internal Grant
Agency of Tomas Bata University under the Projects no. IGA/CebiaTech/2017/004.
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 666–677, 2017.
DOI: 10.1007/978-3-319-62389-4 55

SHADE Algorithm Dynamic Analyzed Through Complex Network
667
An overview of DE based algorithms can be found in a recent survey [9], but
some of the most interesting variants are SDE [10], JADE [11], SHADE [12] and
L-SHADE [13]. The last two named were successful in recent competitions in
continuous numerical optimization [14–17] and therefore, the SHADE algorithm
was selected as a candidate for the analysis in this paper.
The main motivation is to understand the inner dynamic of the algorithm
and use this information for the future changes in its design because there is still
a lot of room for improvement, as suggested in [18].
In this work, the dynamic of the algorithm is transformed into the Complex
Network (CN) and node degree values are used for the sake of analysis of the
algorithm performance and in an eﬀort to detect its possible weak spots in the
design. Similar CN creation was done in recently published papers [19,20]. Since
the analysis is based on the activity of an individual in evolution, such simple
network design is suﬃcient.
In order to provide a robust overview of the method, CEC2015 benchmark
set of 15 test functions [21] was used for the analysis in two dimensional settings
– 10 and 30.
The remainder of the paper is structured as follows: Sects. 2 and 3 depict
the DE and SHADE algorithms, Sect. 4 describes the network creation, Sect. 5
deﬁnes the experimental settings, Sect. 6 provides results and discussion. Finally,
concluding remarks can be found in the last section.
2
Diﬀerential Evolution
The DE algorithm is initialized with a random population of individuals P, that
represent solutions of the optimization problem. The population size NP is set
by the user along with other control parameters – scaling factor F and crossover
rate CR.
In continuous optimization, each individual is composed of a vector x of
length D, which is a dimensionality (number of optimized attributes) of the
problem, and each vector component represents a value of the corresponding
attribute, and of objective function value f (x).
For each individual in a population, three mutually diﬀerent individuals are
selected for mutation of vectors and resulting mutated vector vis combined with
the original vector x in crossover step. The objective function value f (u) of the
resulting trial vector uis evaluated and compared to that of the original individ-
ual. When the quality (objective function value) of the trial individual is better,
it is placed into the next generation, otherwise, the original individual is placed
there. This step is called selection. The process is repeated until the stopping
criterion is met (e.g. the maximum number of objective function evaluations, the
maximum number of generations, the low bound for diversity between objective
function values in population).
The following sections describe four steps of DE: Initialization, mutation,
crossover and selection.

668
A. Viktorin et al.
2.1
Initialization
As aforementioned, the initial population P, of size NP, of individuals is ran-
domly generated. For this purpose, the individual vector x i components are
generated by Random Number Generator (RNG) with uniform distribution from
the range which is speciﬁed for the problem by lower and upper bound (1).
x j,i = U

lowerj, upperj

for j = 1, . . . , D
(1)
where i is the index of a current individual, j is the index of current attribute
and D is the dimensionality of the problem.
In the initialization phase, a scaling factor value F and crossover value CR
has to be assigned as well. The typical range for F value is [0, 2] and for CR, it
is [0, 1].
2.2
Mutation
In the mutation step, three mutually diﬀerent individuals x r1, x r2, x r3 from a
population are randomly selected and combined in mutation according to the
mutation strategy. The original mutation strategy of canonical DE is “rand/1”
and is depicted in (2).
v i = x r1 + F (x r2 −x r3)
(2)
where r1 ̸= r2 ̸= r3 ̸= i, F is the scaling factor value and v i is the resulting
mutated vector.
2.3
Crossover
In the crossover step, mutated vector v i is combined with the original vector x i
and produces trial vector ui. The binary crossover (3) is used in canonical DE.
uj,i =

vj,i if U [0, 1] ≤CR or j = jrand
xj,i
otherwise
(3)
where CR is the used crossover rate value and jrand is an index of an attribute
that has to be from the mutated vector v i (ensures generation of a vector with
at least one new component).
2.4
Selection
The selection step ensures, that the optimization progress will lead to better
solutions because it allows only individuals of better or at least equal objective
function value to proceed into next generation G+1 (4).
x i,G+1 =
ui,G if f (ui,G) ≤f (x i,G)
x i,G
otherwise
(4)
where G is the index of current generation.

SHADE Algorithm Dynamic Analyzed Through Complex Network
669
The whole DE algorithm is depicted in pseudo-code below.
Algorithm 1. DE
1: Set NP, CR, F and stopping criterion;
2: G = 0, xbest = {};
3: Randomly initialize (1) population P = (x 1,G,. . . ,x NP,G);
4: Pnew = {}, x best = best from population P;
5: while stopping criterion not met do
6:
for i = 1 to NP do
7:
x i,G = P[i];
8:
v i,G by mutation (2);
9:
ui,G by crossover (3);
10:
if f (ui,G) < f (x i,G) then
11:
x i,G+1 = ui,G;
12:
else
13:
x i,G+1 = x i,G;
14:
end if
15:
x i,G+1 →Pnew;
16:
end for
17:
P = Pnew, Pnew = {}, x best = best from population P;
18: end while
19: return x best as the best found solution
3
Success-History Based Adaptive Diﬀerential Evolution
and Enhanced Archive
In SHADE, the only control parameter that can be set by the user is popula-
tion size NP, other two (F, CR) are adapted to the given optimization task, a
new parameter H is introduced, which determines the size of F and CR value
memories. The initialization step of the SHADE is, therefore, similar to DE.
Mutation, however, is completely diﬀerent because of the used strategy “current-
to-pbest/1” and the fact, that it uses diﬀerent scaling factor value Fi for each
individual. Mutation strategy also works with a new feature – external archive of
inferior solutions. This archive holds individuals from previous generations, that
were outperformed in the selection step. The size of the archive retains the same
size as the size of the population by randomly discarding its contents whenever
the size overﬂows NP.
Crossover is still binary, but similarly to the mutation and scaling factor
values, crossover rate value CRi is also diﬀerent for each individual.
The selection step is the same and therefore following sections describe only
diﬀerent aspects of initialization, mutation and crossover.
3.1
Initialization
As aforementioned, initial population P is randomly generated as in DE, but
additional memories for F and CR values are initialized as well. Both memories

670
A. Viktorin et al.
have the same size H and are equally initialized, the memory for CR values is
titled MCR and the memory for F is titled MF . Their initialization is depicted
in (5).
MCR,i = MF,i = 0.5 for i = 1, . . . , H
(5)
Also, the external archive of inferior solutions A is initialized. Since there are
no solutions so far, it is initialized empty A = Ø and its maximum size is set
to NP.
3.2
Mutation
Mutation strategy “current-to-pbest/1” was introduced in [11] and unlike
“rand/1”, it combines four mutually diﬀerent vectors, where pbest ̸=r1 ̸=
r2 ̸= i (6).
v i = x i + Fi (x pbest −x i) + Fi (x r1−x r2)
(6)
where x pbest is randomly selected from the best NP × p best individuals in the
current population. The p value is randomly generated for each mutation by
RNG with uniform distribution from the range [pmin, 0.2]. where pmin = 2/NP.
Vector x r1 is randomly selected from the current population and vector x r2 is
randomly selected from the union of current population P and archive A. The
scaling factor value Fi is given by (7).
Fi = C [MF,r, 0.1]
(7)
where MF,r is a randomly selected value (by index r) from MF memory and
C stands for Cauchy distribution, therefore the Fi value is generated from the
Cauchy distribution with location parameter value MF,r and scale parameter
value 0.1. If the generated value Fi > 1, it is truncated to 1 and if it is Fi ≤0,
it is generated again by (7).
3.3
Crossover
Crossover is the same as in (3), but the CR value is changed to CRi, which is
generated separately for each individual (8). The value is generated from the
Gaussian distribution with mean parameter value of MCR.r, which is randomly
selected (by the same index r as in mutation) from MCR memory and standard
deviation value of 0.1.
CRi = N [MCR,r, 0.1]
(8)
3.4
Historical Memory Updates
Historical memories MF and MCR are initialized according to (5), but its com-
ponents change during the evolution. These memories serve to hold successful
values of F and CR used in mutation and crossover steps. Successful in terms of
producing trial individual better than the original individual. During one gen-
eration, these successful values are stored in corresponding arrays SF and SCR.

SHADE Algorithm Dynamic Analyzed Through Complex Network
671
After each generation, one cell of MF and MCR memories is updated. This cell
is given by the index k, which starts at 1 and increases by 1 after each generation.
When it overﬂows the size limit of memories H, it is again set to 1. The new
value of k-th cell for MF is calculated by (9) and for MCR by (10).
MF,k =

meanW L (S F ) if S F ̸= ∅
MF,k
otherwise
(9)
MCR,k =

meanW A (SCR) if SCR ̸= ∅
MCR,k
otherwise
(10)
where meanW L() and meanW A() are weighted Lehmer (11) and weighted arith-
metic (12) means correspondingly.
meanW L (S F ) =
|S F |
k=1 wk • S2
F,k
|S F |
k=1 wk • SF,k
(11)
meanW A (SCR) =
|SCR|

k=1
wk • SCR,k
(12)
where the weight vector w is given by (13) and is based on the improvement in
objective function value between trial and original individuals.
w k =
abs (f (uk,G) −f (x k,G))
|SCR|
m=1 abs (f (um,G) −f (x m,G))
(13)
And since both arrays SF and SCR have the same size, it is arbitrary which size
will be used for the upper boundary for m in (13). Complete SHADE algorithm
is depicted in pseudo-code below.
4
Network Design and Metrics
The network creation is designed to be as simple as possible, therefore edges in
the network are undirected and unweighted.
The network is represented by a multigraph G = (V, E), where V is a set
of vertices and E is a set of edges. Vertices mirror individuals in the population,
therefore the size of V is the same as population size NP. Undirected edges are
created after each successful evolution (produced trial individual ui has better
objective function value than the active individual x i) between active individual
x i and three individuals form the mutation step – x pbest, x r1 and x r2. And the
edges are denoted as ei,pbest, ei,r1 and ei,r2. The scheme is depicted below in
Fig. 1.
The maximum number of edges created in one generation is equal to 3×NP
and this occurs only if all individuals in the population were improved in this
generation.
The optimization process is captured in snapshots of each generation. There-
fore, for each generation a new multigraph Gn is created, where n stands for
generation number. There is no transfer of information between generations.
For the purpose of the analysis of CN two metrics were introduced.

672
A. Viktorin et al.
Algorithm 2. SHADE
1: Set NP, H and stopping criterion;
2: G = 0, xbest = {}, k = 1, pmin = 2/NP, A = Ø;
3: Randomly initialize (1) population P = (x 1,G,. . . ,x NP,G);
4: Set M F and M CR according to (5);
5: Pnew = {}, x best = best from population P;
6: while stopping criterion not met do
7:
S F = Ø, S CR = Ø;
8:
for i = 1 to NP do
9:
x i,G = P[i];
10:
r = U [1, H ], pi = U [pmin, 0.2];
11:
Set Fi by (7) and CRi by (8);
12:
v i,G by mutation (6);
13:
ui,G by crossover (3);
14:
if f (ui,G) < f (x i,G) then
15:
x i,G+1 = ui,G;
16:
x i,G →A;
17:
Fi →S F , CRi →S CR;
18:
else
19:
x i,G+1 = x i,G;
20:
end if
21:
if |A|>NP then
22:
Randomly delete an individual from A;
23:
end if
24:
x i,G+1 →Pnew;
25:
end for
26:
if S F ̸= Ø and S CR ̸= Ø then
27:
Update M F,k (9) and M CR,k (10), k++;
28:
if k > H then
29:
k = 1;
30:
end if
31:
end if
32:
P = Pnew, Pnew = {}, x best = best from population P;
33: end while
34: return x best as the best found solution
4.1
Objective Function Value Rank
This metric is abbreviated to ofvRank and is evaluated for each individual in
the population and corresponds to the number of individuals, that have worse
objective function value 14.
ofvRanki,G = |S|, S ∈∀x j,G : f(x i,G) < f(x j,G), j = 1, . . . , NP
(14)
where S is a set of individuals with worse objective function value than that of
the evaluated individual.

SHADE Algorithm Dynamic Analyzed Through Complex Network
673
ei,pbest
ei,r1
ei,r2
xi
ui
xpbest
xr1
xr2
Fig. 1. Schema of the network creation for one successful evolution.
4.2
Centrality Rank
This metric is abbreviated to cenRank and mirrors the individual’s position in
the node degree centrality in the population 15.
cenRanki,G = |S|, S ∈∀x j,G : deg(x i,G) < deg(x j,G), j = 1, . . . , NP
(15)
where deg stands for node degree centrality and S is a set of individuals with
lower node degree centrality than that of the evaluated individual.
5
Experimental Setting
The SHADE algorithm was run 51 times on each of the test functions of CEC2015
benchmark set and this was done for two diﬀerent dimensional settings – D =
10 and D = 30. In each run, the stopping criterion was set according to the
benchmark requirements to 10,000 × D. The population size NP was set to 100
and the size of historical memories H was set to 10.
For the purpose of the analysis of CN, node degree centrality values in each
generation were recorded and two above mentioned metrics were evaluated.
The greedy approach is quite common for various optimization techniques
and SHADE algorithm is no exception. There is the x pbest individual in the
mutation step, which is selected from the best subset of the population and
therefore provides the greediness. This greediness is balanced by the use of the
external archive of inferior solutions A. However, the basic assumption in this
approach is that individuals with better objective function values are the ones
which lead the optimization process and their search direction should be followed.
In order to test that assumption, the Pearson correlation coeﬃcient between the
two metrics ofvRank and cenRank was calculated and the results are presented
in the next section.

674
A. Viktorin et al.
6
Results and Discussion
According to the assumption mentioned in the previous section, the correlation
between ofvRank and cenRank should be positive and quite high. In this section,
selected correlation history is provided along with the objective function value
history, in order to conﬁrm or contradict this assumption.
Figure 2 depicts the convergence and correlation comparison between the
average optimization of function 1 in 10 and 30 dimensions. It can be clearly
seen, that the correlation between ofvRank and centrRank is quite high during
the convergence phase and logically drops to 0 after the optimum was found
(only in 10D). The situation is similar for function 2 from the benchmark set,
which suggests that unimodal functions conﬁrm the assumption about correla-
tion. However, the correlation value is stagnating around 0.5 during the conver-
gence, which is on the edge between medium and high correlation.
200
400
600
800
1000
gen
100
1000
104
105
106
107
108
OFV
500
1000
1500
2000
2500
3000
gen
1000
105
107
109
OFV
200
400
600
800
1000
gen
-1.0
-0.5
0.5
1.0
cor
500
1000
1500
2000
2500
3000
gen
-1.0
-0.5
0.5
1.0
cor
Fig. 2. Average correlation and convergence history (with 95% conﬁdence interval -
lighter color) for test function 1 in two diﬀerent dimensional settings. Top left – con-
vergence of f1 in 10D, top right – convergence of f1 in 30D, bottom left – correlation
of f1 in 10D, bottom right – correlation of f1 in 30D.
In most cases of the simple multimodal functions (functions 3 to 9 in the
benchmark set), the convergence and correlation graphs are quite similar in both
dimensional settings (see Fig. 3, left part), except for the function 6. The corre-
lation and convergence history of the optimization of this function are depicted
in Fig. 3. In the case of 30 D, the correlation history is similar to the one from

SHADE Algorithm Dynamic Analyzed Through Complex Network
675
200
400
600
800
1000
gen
1000
104
105
106
107
OFV
500
1000
1500
2000
2500
3000
gen
1000
104
105
106
107
108
109
OFV
200
400
600
800
1000
gen
-1.0
-0.5
0.5
1.0
cor
500
1000
1500
2000
2500
3000
gen
-1.0
-0.5
0.5
1.0
cor
Fig. 3. Average correlation and convergence history (with 95% conﬁdence interval -
lighter color) for test function 6 in two diﬀerent dimensional settings. Top left – con-
vergence of f6 in 10D, top right – convergence of f6 in 30D, bottom left – correlation
of f6 in 10D, bottom right – correlation of f6 in 30D.
simple unimodal functions, whereas in 10D it is similar to the correlation his-
tory on simple multimodal functions. This suggests, that the algorithm in 30 D
converges fast to the local optima and the divergence in the population is low.
As for other simple multimodal functions, the correlation is around 0 almost
from the beginning of the optimization and therefore it contradicts the assump-
tion about positive correlation. This means, that the individuals that are most
active in the population and lead the evolution might not be the one with the
best objective function values and therefore the greedy approach might not be
suitable.
Because of the limited length of this contribution, the rest of the results is
omitted, but all correlation and convergence graphs along with the statistical
results of the optimization can be found here [22].
The correlation history for hybrid (functions 10 to 12) and composition func-
tions (functions 13 to 15) is similar to the correlation history of simple multi-
modal functions with the exception of functions 10, 11 and 15 in 30D, where the
correlation history is of the same type as on simple unimodal functions.
Overall, the assumption about correlation stands only for simple unimodal
functions and some exceptions from other types of functions in the benchmark
set. Therefore, the greedy approach might be labeled as insuﬃcient and a diﬀer-
ent type of approach to pbest set might be more beneﬁcial.

676
A. Viktorin et al.
7
Conclusion
In this preliminary study, the dynamic of the SHADE algorithm was analyzed
with the help of a simple CN created from the optimization process of the algo-
rithm.
Findings from this paper suggest, that the approach of translating a behavior
of evolutionary algorithm dynamic into the CN might be useful for the succeeding
study of the algorithm design and that possible improvements might be derived
from the CN analysis. This paper should be understood as an introduction into
this hybrid ﬁeld and tries to promote the idea of exploiting the knowledge about
the inner workings of DE based algorithms rather than just proposing algorithms
with minor improvements on selected set of test functions.
The future work will be aimed at thorough analysis of created complex net-
work and its features in order to provide beneﬁcial search directions into the
algorithm design. Results suggest, that the greedy approach for pbest selection
could be replaced by more intelligent approach using the information from com-
plex network. Also the combination of individuals for mutation could be selected
from ranked population based on the ranking proposed in this paper.
References
1. Storn, R., Price, K.: Diﬀerential evolution-a simple and eﬃcient adaptive scheme
for global optimization over continuous spaces, vol. 3. ICSI, Berkeley (1995)
2. Brest, J., Greiner, S., Boskovic, B., Mernik, M., Zumer, V.: Self-adapting control
parameters in diﬀerential evolution: a comparative study on numerical benchmark
problems. IEEE Trans. Evol. Comput. 10(6), 646–657 (2006)
3. Qin, A.K., Huang, V.L., Suganthan, P.N.: Diﬀerential evolution algorithm with
strategy adaptation for global numerical optimization. IEEE Trans. Evol. Comput.
13(2), 398–417 (2009)
4. Das, S., Abraham, A., Chakraborty, U.K., Konar, A.: Diﬀerential evolution using
a neighborhood-based mutation operator. IEEE Trans. Evol. Comput. 13(3), 526–
553 (2009)
5. Mininno, E., Neri, F., Cupertino, F., Naso, D.: Compact diﬀerential evolution.
IEEE Trans. Evol. Comput. 15(1), 32–54 (2011)
6. Mallipeddi, R., Suganthan, P.N., Pan, Q.K., Tasgetiren, M.F.: Diﬀerential evolu-
tion algorithm with ensemble of parameters and mutation strategies. Appl. Soft
Comput. 11(2), 1679–1696 (2011)
7. Brest, J., Koroˇsec, P., ˇSilc, J., Zamuda, A., Boˇskovi´c, B., Mauˇcec, M.S.: Diﬀerential
evolution and diﬀerential ant-stigmergy on dynamic optimisation problems. Int. J.
Syst. Sci. 44(4), 663–679 (2013)
8. Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization. IEEE
Trans. Evol. Comput. 1(1), 67–82 (1997)
9. Das, S., Mullick, S.S., Suganthan, P.N.: Recent advances in diﬀerential evolution-an
updated survey. Swarm Evol. Comput. 27, 1–30 (2016)
10. Omran, M.G.H., Salman, A., Engelbrecht, A.P.: Self-adaptive diﬀerential evolu-
tion. In: Hao, Y., Liu, J., Wang, Y., Cheung, Y., Yin, H., Jiao, L., Ma, J., Jiao,
Y.-C. (eds.) CIS 2005. LNCS, vol. 3801, pp. 192–199. Springer, Heidelberg (2005).
doi:10.1007/11596448 28

SHADE Algorithm Dynamic Analyzed Through Complex Network
677
11. Zhang, J., Sanderson, A.C.: JADE: adaptive diﬀerential evolution with optional
external archive. IEEE Trans. Evol. Comput. 13(5), 945–958 (2009)
12. Tanabe, R., Fukunaga, A.: Success-history based parameter adaptation for diﬀer-
ential evolution. In: 2013 IEEE Congress on Evolutionary Computation (CEC),
pp. 71–78. IEEE, June 2013
13. Tanabe, R., Fukunaga, A.S.: Improving the search performance of SHADE using
linear population size reduction. In: 2014 IEEE Congress on Evolutionary Compu-
tation (CEC), pp. 1658–1665. IEEE, July 2014
14. Brest, J., Mauˇcec, M.S., Boˇskovi´c, B.: iL-SHADE: improved L-SHADE algorithm
for single objective real-parameter optimization. In: 2016 IEEE Congress on Evo-
lutionary Computation (CEC), pp. 1188–1195. IEEE, July 2016
15. Viktorin, A., Pluhacek, M., Senkerik, R.: Success-history based adaptive diﬀerential
evolution algorithm with multi-chaotic framework for parent selection performance
on CEC2014 benchmark set. In: 2016 IEEE Congress on Evolutionary Computation
(CEC), pp. 4797–4803. IEEE, July 2016
16. Pol´akov´a, R., Tvrd´ık, J., Bujok, P.: L-SHADE with competing strategies applied
to CEC 2015 Learning-based Test Suite. In: 2016 IEEE Congress on Evolutionary
Computation (CEC), pp. 4790–4796. IEEE, July 2016
17. Awad, N.H., Ali, M.Z., Suganthan, P.N., Reynolds, R.G.: An ensemble sinusoidal
parameter adaptation incorporated with L-SHADE for solving CEC 2014 bench-
mark problems. In: 2016 IEEE Congress on Evolutionary Computation (CEC), pp.
2958–2965. IEEE, July 2016
18. Tanabe, R., Fukunaga, A.: How far are we from an optimal, adaptive DE? In:
Handl, J., Hart, E., Lewis, P.R., L´opez-Ib´a˜nez, M., Ochoa, G., Paechter, B. (eds.)
PPSN 2016. LNCS, vol. 9921, pp. 145–155. Springer, Cham (2016). doi:10.1007/
978-3-319-45823-6 14
19. Viktorin, A., Pluhacek, M., Senkerik, R.: Network based linear population size
reduction in SHADE. In: 2016 International Conference on Intelligent Networking
and Collaborative Systems (INCoS), pp. 86–93. IEEE, September 2016
20. Skanderova, L., Fabian, T.: Diﬀerential evolution dynamics analysis by complex
networks. Soft Comput. 1–15 (2015)
21. Chen, Q., Liu, B., Zhang, Q., Liang, J.J., Suganthan, P.N., Qu, B.Y.: Problem
deﬁnition and evaluation criteria for CEC 2015 special session and competition on
bound constrained single-objective computationally expensive numerical optimiza-
tion. Computational Intelligence Laboratory, Zhengzhou University, China and
Nanyang Technological University, Singapore, Technical Report (2014)
22. Viktorin, A., Senkerik, R., Pluhacek, M., Kadavy, T.: CSoNet 2017 data (2017).
https://owncloud.cesnet.cz/index.php/s/GLw9XggT0cBky1N

An Eﬃcient Potential Member Promotion
Algorithm in Social Networks via Skyline
Siman Zhang1(B) and Jiping Zheng1,2(B)
1 College of Computer Science and Technology, Nanjing University
of Aeronautics and Astronautics, Nanjing, People’s Republic of China
{zhangsiman,jzh}@nuaa.edu.cn
2 Collaborative Innovation Center of Novel Software Technology
and Industrialization, Nanjing, People’s Republic of China
Abstract. With the development of skyline queries and social networks,
the idea of combining member promotion with skyline queries attracts
people’s attention. Some algorithms have been proposed to deal with this
problem so far, such as skyline boundary algorithms in unequal-weighted
social networks. In this paper, we propose an improved member promo-
tion algorithm by presenting reputation level based on eigenvectors as
well as in/out-degrees and providing skyline distance and sort-projection
operations. The added reputation level helps a lot to describe the impor-
tance of a member, the skyline distance and sort-projection operations
help us to obtain the necessary situations for not being dominated so
that some meaningless plans can be pruned. Then we verify some pro-
motion plans in minimum time cost based on dominance. Experiments on
the DBLP dataset verify the eﬀectiveness and eﬃciency of our proposed
algorithm.
Keywords: Social networks · Member promotion · Reputation level ·
Skyline distance
1
Introduction
Diﬀerent members play diﬀerent roles in Social Networks (SN for short). Some
members play an important role, and others who seems ordinary for the moment,
but it may be outstanding in the future. Our goal is to ﬁnd those members who
have the potential to be a “star” in the future.
When skyline was ﬁrst used to promoting in SNs [9], the author proposed the
deﬁnition of promotion and the brute-force algorithm to realize it. However, this
algorithm was inadvisable for a waste of time and space. After that, the author
took the in-degree and out-degree as the measurements and in the form of adding
directed edges to get the potential members via skyline queries. Nevertheless,
the ﬁnal result was unsatisfactory on account of the simple measurement of
importance.
In this paper, we mainly study directed social graph with the knowledge of
graph theory [10], taking the in-degree, out-degree and reputation level as the
c
⃝Springer International Publishing AG 2017
Y. Cao and J. Chen (Eds.): COCOON 2017, LNCS 10392, pp. 678–690, 2017.
DOI: 10.1007/978-3-319-62389-4 56

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
679
measurement of member’s importance. What’s more, we intuitively employ edge
addition as the promotion manner to change the walk structure. Generally, it will
take some costs to add new edges into the SNs. Therefore, the problem of member
promotion in SNs is deﬁned as to excavate the most appropriate non-skyline
member(s) which can be promoted to be skyline member(s) by adding new edges
with the minimum cost. However, the calculation of reputation level involves
series of mathematical operations, it may need enormous computational cost.
To tackle the above challenges, we only consider the changes of in-degree and
out-degree due to the great number of members in large SNs in the process of edge
addition. When calculating a point’s reputation level, we need to take the total
number of the members as denominator. Apparently, for the great changes of
the denominator (we assume the SN is dynamic), the weak changes of numerator
can be ignored. If we combine the in-degree, out-degree and reputation level to
get a skyline set before promotion, we can get the non-skyline set and take it as
the candidate set [1], which helps to reduce the number of candidates greatly.
Therefore, the contributions of this paper are summarized as follows.
– We add the reputation level as a measure attribute, it helps to improve the
accuracy of the prediction.
– The skyline distance and the sort-projection operation are utilized to obtain
the necessary situations for not being dominated and prune the search space.
– Experiments on DBLP dataset are conducted to show the eﬀectiveness and
eﬃciency of our approach.
The rest of this paper is organized as follows. Section 2 reviews related work.
In Sect. 3, we introduce several preliminary concepts. Then we bring forward
the problem and propose the algorithm with analysis in Sect. 4. The results of
the experiments are presented to show the eﬀectiveness and eﬃciency of our
algorithms in Sect. 5. Finally, we conclude our work in Sect. 6.
2
Related Work
2.1
Skyline Queries
Skyline query processing has been studied extensively in recent years. The skyline
operator was ﬁrst introduced by Borzsony et al. [1] in 2001. Then some represen-
tative algorithms for skyline computation including Block Nested Loops (BNL)
and Divide-and-Conquer (D&C) [1], Bitmap and Index [12], Nearest Neighbor
(NN) [4], and the Branch and Bound Skyline (BBS) algorithm [7] are proposed.
Both BNL and D&C need to process the entire object set before reporting sky-
line data. The bitmap-based method transforms each object to bit vectors. In
each dimension, the value is represented by the same number of ‘1’. However, it
cannot guarantee a good initial response time and the bitmaps would be very
large for large values. Therefore another method which transforms high dimen-
sion into a single dimension space where objects are clustered and indexed using
a B+ tree is raised. It helps a lot to save the time because skyline points can be

680
S. Zhang and J. Zheng
determined without examining the rest of the object set not accessed yet. The
NN algorithm proposed by Kossmann et al. [4] can process an online skyline
query which can progressively report the skyline points in an order according to
user’s preferences. However, one object may be processed many times until being
dominated. To remedy this problem, Papadias et al. [7] proposed an R-tree based
algorithm, BBS, which retrieves skyline points by browsing the R-tree based on
the best-ﬁrst strategy. There are also numerous studies on skyline variations for
diﬀerent applications, such as subspace skylines [8], probabilistic skyline compu-
tation on uncertain data [5], weighted attributes skylines [6], skyline computation
in partially ordered domains [11] and using skylines to mine user preferences,
making recommendations [2].
2.2
Member Promotion
Promotion, as an important concept in marketing, has been introduced into data
management applications recently. Wu et al. [13] proposed a method to ﬁnd such
appropriate subspaces that the target can be prominent in these subspaces. The
authors bring forward an eﬀective framework, namely “PromoRank”, for promo-
tion query and propose a “Promotion Cube” based on the concept of “Cube” in
OLAP for further speeding up the query. Furthermore, they have extended the
algorithm by developing a cell clustering approach that further achieves better
tradeoﬀbetween oﬄine materialization and online query processing, and thus
greatly speed up the query processing.
Member promotion in SNs was ﬁrst proposed and studied in [9]. The for-
mal deﬁnition was provided and a brute-force algorithm was proposed at ﬁrst.
Based on the characteristics of the skyline and the promotion process, several
optimization strategies were proposed to improve the eﬃciency and led to the
IDP (Indexed Dynamic Program) algorithm [10]. Later, in the unequal-weighted
SNs, the author proposed an eﬀective promotion boundary-based pruning strat-
egy to prune the search space and a cost-based pruning strategy based on the
permutation and combination theories to verify the plans in ascending order
of cost. So as to the equal-weighted SNs, the author optimized the cost model
and put forward a new concept named “Infra-Skyline” to remarkably prune the
candidate space. However, those algorithms were limited for only metrics like in-
degree and out-degree which couldn’t describe the member’s importance entirely,
so the prediction of promoting was not very satisfying.
A major distinction between our model and existing methods is that we add
the reputation level as a measure attribute, which works a lot to describe a mem-
ber’s characteristic. With an upgrade of the measurement, our work shows more
eﬃcient.
3
Preliminaries
In this paper, an SN, is modeled as a weighted directed graph G(V, E, W). The
nodes in V represent the members in the SN. Those elements of E are the

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
681
existing directed edges between the members. Each w ∈W : V × V →R+
denotes the cost for establishing the directed edge between any two diﬀerent
members. Assume din(v), dout(v) and P(v) represent the in-degree, out-degree
and reputation level of node v in V respectively. We consider the larger the
din(v), dout(v) and P(v) (P(v) is deﬁned in Deﬁnition 3), the better the v is.
Deﬁnition 1 (Social relationship matrix). Given an SN G(V, E, W), the
social relationship matrix is an adjacency matrix which expresses the links
between the members in the SN, denoted as M.
Deﬁnition 2 (Normalization social matrix). If a social relationship matrix
is M, then its normalization social matrix is a matrix which has sum of the
elements for any column is 1, we denote the normalization matrix as M ′.
Deﬁnition 3 (Reputation level). Given a node v in a SN G(V, E, W), the
reputation level of v, marked as P(v), is the value of the corresponding com-
ponent in the eigenvector of the normalized social relationship matrix whose
eigenvalue is 1.
Example 1. Suppose that there are three nodes in the SN, let the nodes be
v1, v2, v3, the eigenvector of the normalized social relationship matrix whose
eigenvalue is 1 is p = (p1, p2, p3), then the v1, v2, v3’s reputation level is p1, p2
and p3 respectively.
Deﬁnition 4 (Dominator set). Given an SN G(V, E, W), if v1 dominates v2,
we call v1 a dominator of v2. Correspondingly, all dominators of a member v,
marked as δ(v), is denoted as the dominator set of v.
Deﬁnition 5 (Infra-Skyline). Given an SN G(V, E, W), the Infra-Skyline of
G is the skyline of the set of all non-skyline members of G, namely, where is a
new SN generated from G by eliminating its skyline members.
Example 2. Given an SN consists of seven members, namely {A, B, C, D,
E, F, G}, suppose that the skyline set is {A, B, D}, then the Infra-Skyline in
the SN is {C, E, F, G}.
3.1
Reputation Level
In SNs, if a member’s moments are composed of prestigious people, then we
think she/he is reputable. Otherwise, her/his reputation should be low.
From the point of mathematics, members’ reputation level depends on the
reputation of those members who follow them. The reputation level of those
members who are following depends on those who follow them, and the subse-
quent process can be implemented in the same manner. So, for solving this kind
of “inﬁnite regress”, we deﬁne P(vi) as the reputation level of member i, and
we noticed that the ith column of the social relationship matrix shows those
members who follow it. Therefore, we can get vi’s reputation level by adding

682
S. Zhang and J. Zheng
these products between the relation state and the reputation level of all other
members, namely
P(vi) = x1iP(v1) + x2iP(v2) + ... + xgiP(vg)
(1)
where the coeﬃcient xij denotes the relation state between the member i and j
and g denotes the number of the members.
Example 3. If there are 7 members in an SN, the member v2 is followed by
v5 and v7, then the rest entries of the second column in the social relationship
matrix are all be 0s. Thus, we consider v2’s reputation level is p(v5) + p(v7).
From Example 3, we know that if the member v5 and v7 have a high reputa-
tion level, so does v2. It means that if a member is followed by some prestigious,
the member may be reputable as well.
Therefore, we have g formulas like Eq. (1), and we have a system of g linear
equations. If we compute the social relationship matrix M and put the value of
the reputation level into the vector, the whole formula system is expressed as
P = M T P
(2)
where P represents the vector consisting of the corresponding reputation level
of each member in the limited state.
By organizing these formulas, we obtain the formula (I −M T )P = 0, where
I represents a g dimensional unit matrix, P and 0 both represent vectors with
the length of g. In order to solve this formula, Katz [3] supposed that the social
relationship matrix should be normalized. If the matrix has been normalized [5],
the eigenvector P ′ whose eigenvalue is 1 represents the reputation level of the
members [6].
3.2
The Property of Reputation Level
It should be noticed that a point’s reputation level is partially consistent with its
in-degree. However, by this property alone cannot show the diﬀerence between
the top and the next. When some members have the same in-degrees, the out-
degree also aﬀects the reputation level.
Example 4. Given 7 members in the SN, as shown in Fig. 1, its corresponding
social relationship matrix M and its form of normalization M ′ are as follows.
M =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 1 1 1 1 0 1
1 0 0 0 0 0 0
1 1 0 0 0 0 0
1 1 1 0 1 0 0
1 0 1 1 0 1 0
1 0 0 0 1 0 0
0 0 0 0 1 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
M ′ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 1
3
1
3
1
2
1
4 0 1
1
5 0 0 0 0 0 0
1
5
1
3 0 0 0 0 0
1
5
1
3
1
3 0 1
4 0 0
1
5 0 1
3
1
2 0 1 0
1
5 0 0 0 1
4 0 0
0 0 0 0 1
4 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
(3)

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
683
Then we obtain the eigenvector α = (0.304, 0.166, 0141, 0.105, 0.179, 0.045,
0.061)T of M ′ when the eigenvalue is 1, we think the top one whose ID is 1 has the
highest reputation level because she/he gains all the reputation from ID 2. What’s
more, ID 1 has the highest in-degree and out-degree, thus we consider she/he is the
most popular one in the SN. On the other hand, ID 2 and ID 3 have the same in-
degree but ID 2’s out-degree is smaller than ID 3, we can see that ID 2’s reputation
level is larger than ID 3.
Fig. 1. A social network example
Fig. 2. A skyline distance example
4
Prediction of Promoting Members in the SNs
4.1
Problem Statement
The problem we study in this paper is to locate the most “appropriate” mem-
ber(s) for promotion by means of elevating it (them) into skyline. Suppose we
have two datasets D1 and D2. The D1 represents some data of previous years
and the D2 represents that of the following several years. If S1 = SKY (D1),
S′
1 = SKY (D1 −S1), S2 = SKY (D2), where the SKY () represents the skyline
set of the dataset, then S′
1 is the candidate set in our algorithm. After promoting
towards each point in S′
1, if there exist some of points in S′
1 appearing in S2, the
prediction is successful, otherwise, it fails.
4.2
The Sort-Projection Operation
We consider projecting all the members into a 2-dimenional Cartesian coordinate
system, where the axe x represents the in-degree and the axe y represents the
out-degree. Taking the candidate c as an example, suppose that c is dominated
by t skyline points, and we simply sort the skyline points in ascending order on
axe x.
Deﬁnition 6 (Strictly dominate). If the point p1 is not smaller than p2 on
each dimension and larger than p2 on at least one dimension, we say p1 dominate
p2, denoted by p1 ≺p2. Furthermore, if p1 is larger than p2 on each dimension,
we say p1 strictly dominate p2, denoted by p1 ≺≺p2.

684
S. Zhang and J. Zheng
Deﬁnition 7 (Skyline distance). Given a set DS of points in a 2-dimenional
space D, a candidate c, and a path function Path(., .), the skyline distance of c
is the minimum Path(c, c′), where c′ is a position in D such that c′.Di ≥c.Di,
i ∈[1, 2], and c′ is not strictly dominated by any point in DS. We denote the
skyline distance as SkyDist().
Suppose that c is strictly dominated by t skyline points in SKY (DS). For
any position c′ which is not strictly dominated by any point in DS satisﬁes
c′.Di ≥c.Di, i ∈[1, 2], the upgrade from c to c′ can be viewed as a path from
c to c′, which always goes up along axes. Since we use linear cost functions
cost(c, c′) as the sum of the weighted length of the segments on the path. We
aim to ﬁnd a path with the minimum value so that the end point c′ is not strictly
dominated by any skyline point, and c′.Di ≥c.Di, i ∈[1, 2].
Given a path described above, we deﬁne t turning positions. The kth(1 ≤
k ≤t) turning position of a path, denoted by ck, is the ﬁrst position in the path
such that ck, is strictly dominated by at most t −k skyline points. Apparently,
ct is not strictly dominated by any skyline point. We always set c′ to be ct to
minimize the length of the path.
Deﬁnition 8 (Skyline boundary). Given a set SKY of skyline points in DS,
we say a point p is on the skyline boundary if there exists a point u such that
u ≺p and there does not exist a point u′ ∈SKY , such that u′ ≺≺p.
From the deﬁnition of skyline boundary, we know that each point on the
skyline boundary has a skyline distance 0. In the 2-dimensional space D, if there
is an intersection point in the skyline boundary, then we call this intersection
“boundary intersection point”.
Property 1 (Dominance in boundary intersection points). Let p is a
point on the skyline boundary w.r.t. a set DS of points and p is not a boundary
intersection point. Then, there exists a boundary intersection point p′ such that
p ≺p′.
We call a boundary intersection point a local optimal point if it does not
dominate any other boundary intersection point.
Consider a candidate c dominated by t skyline points s1, s2,..., st. Let p1,...,
pr be the r local optimal points determined by c and s1, s2,..., st, then the skyline
distance of c is the minimum path from c to pi.
In the 2-dimensional space D, for the candidate c and the t skyline points
s1, s2,...,st, we have s1.D1 < s2.D1 <, ..., < st.D1. Without loss of generality, we
know s1.D2 > s2.D2 >, ..., > st.D2. Clearly, there are t + 1 local optimal points
and the ith one pi is given by the following formula,
Pi =
⎧
⎪
⎨
⎪
⎩
(c.D1, s1.D2),
i = 1;
(si−1.D1, si.D2),
2 ≤i ≤t;
(st.D1, c.D2),
i = t + 1.
(4)

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
685
Example 5. There is a candidate c and s1, s2, s3 are skyline points which dom-
inate c, as shown in Fig. 2, we can obtained the 4 local optimal points p1, p2,
p3 and p4 by Eq. (4), by comparing the path between c and pi, we can get the
skyline distance of c.
Algorithm 1 gives the pseudo-codes of the sort-projection operation.
Algorithm 1. The sort-projection algorithm SP(c, SKY )
Input: SKY ()
Output: SkyDist(c)
1 sort points in SKY () in ascending order on dimension D1;
2 P = {pi | l} where pi is given by Eq. (4);
3 return min{P ath(c, P ) | P };
4.3
Pruning Based on Cost and Dominance
After obtaining the skyline distance of a candidate, we know the necessary situ-
ations for not being dominated by skyline points. Taking the candidate c as an
example, if c′ is the end point in the skyline distance of c, we both plus 1 on the
axe x and axe y of the c′ (we denote the increased c′ as c′′). It is obvious that c′′
could not be dominated by any point at all. If we call the position where a can-
didate will not be dominated as GoodPosition(), we say c′′ ∈GoodPosition().
Besides c′′, all points in skyline will not be dominated either. Thus the dominator
set of c belongs to GoodPosition(c).
In view of unequal costs for establishing diﬀerent edges, it probably takes
diﬀerent costs to promote c by diﬀerent plans. If we respectively organize all the
edges which can be added in the plans against each candidate c, denoted as Ec,
sorted in ascending order of weights. Then we can easily locate the plans from
Ec with minimum costs. Once the plan is veriﬁed to be successful to promote
the candidates, the process of promotion could be ended.
Observation 1. The successive plans are introduced by the following rules:
– If the edge e0 with minimum cost in Ec is not included in the current plan,
add e0.
– If any successive edge of ei, namely ei+1, does not belong to the current plan,
replace ei with ei+1.
Observation 2. The prunable plans are introduced by the following rules:
– If adding an edge e connecting node vi and the candidate node c still cannot
promote c into the skyline set, all the attempts of adding an edge e’ connecting
the node vj and c with the same direction as e is not able to successfully
promote c, where vj ∈δ(vi).
– If a plan p(e1, ..., ew) cannot get its target candidate c promoted, all the plans
with w edges which belong to can be skipped in the subsequent veriﬁcation
process against c, where for each ei connecting vi and c, l is a list containing
all the non-existing edges each of which links one member of δ(vi) and c with
the same direction as ei (i = 1, 2, ..., w), is the Cartesian product of l.

686
S. Zhang and J. Zheng
4.4
Veriﬁcation of the Result
We notice that those points which don’t dominate the candidate point before
promoting wouldn’t dominate it after promotion. Thus we can ignore it in the
veriﬁcation process. Therefore, after pruning, we should just consider the follow-
ing situations when verifying,
1. The points which dominate the candidate before promoting.
2. The points which related to the promotion plans.
4.5
The PromSky Algorithm
The whole process of member promotion in an SN is presented in Algorithm 2.
Line 4–5 represents a preprocessing phase by generating the sorted available
edge. Then GoodPosition() is generated in line 5–9. Line 10–11 shows that the
corresponding promotion plans are generated and put into the priority queue Q.
Once the queue is not empty, we fetch the plan with minimum cost for further
veriﬁcation. Line 12 shows that before verifying the plan, we ﬁrst generate its
children plans by Observation 1 so that we can verify all the possible plans in
ascending order of cost. Line 14–15 represents that after checking based on the
result veriﬁcation strategy the result will be output if the promotion succeeds.
If not, some prunable plans will be generated. The generation of prunable plans
are showed in Line 17. Line 16 represents that if the plan is in the prunable
list, there is no need of further veriﬁcation. Line 13 shows that after a successful
promotion, the process will halt once we encounter a plan with the higher cost.
Algorithm 2. The promotion algorithm PromSky(G)
Input: social network G(V, E, W ).
Output: optimal members for promotion and corresponding plans.
1 initialize a priority queue Q;
2 C = V −SKY (V );
3 for each c ∈C do
4
E=genCandidateEdgeSet(c);
5
sortByWeight(E);
6
SkyDist(c) = P ath(c, SKY );
7
good.x = DistSky(c.x + 1);
8
good.y = DistSky(c.y + 1);
9
GetDominator(c);
10
P =getMinCostPlan(good, GetDominator(c), E) and add P into Q;
11
while p=ExtractMin(Q) do
12
pchild = GenerateChildren(p, E) and add it into Q;
13
if psucc is NULL and cost(c) > cost(psucc) then
14
break;
15
if c is promoted by p then
16
return c and p;
17
if psucc=∅then
18
psucc = p;
19
if p ∈prunableList then
20
continue;
21
else
22
genP runableP lans(p, c);

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
687
5
Experimental Analysis
5.1
Experimental Setup
We take the evaluation of the experiments on the dataset of DBLP1 each record
of which consists of authors’ name, paper title and published year. We collect all
the records from 1992 to 2016, in order to properly show the value of the authors
in their own eras, for each year’s data, we consequently combine the current
yearly data with its previous 4 years’ data to generate a 5-year sub-network.
Then we run our promotion algorithm on the 5-year sub-networks (from 1996 to
2016 with 4 sub-networks), so as to make comparisons between the corresponding
yearly potential stars and those skyline authors in the following couple of years.
The experiments are implemented using C++ with Visual Studio 2010 and
conducted on an Intel Core CPU i75500U@2.4GHZ machine with 8G RAM and
1Tbytes Hard disk running on Windows 7.
5.2
Results
Successful rate comparison. In this section, we record the predicted potential
stars and the skyline authors detected by our algorithm from 1996 to 2016. We
can get the successful rate by using the number of potential stars promoted into
skyline in the next few years divided by the size of the whole potential star set,
namely
r = PN/CS
(5)
where “r” denotes the successful rate, “PN” and “CS” are the number of suc-
cessfully promoted members and the number of all the candidates respectively.
From Table 1, we know that the number of the potential candidates is 20 (by
merging the duplicated potential stars and removing the potential stars of the
year 2016 because it is unable to be veriﬁed), and the number of the potential
candidates who appear in the next skyline authors is 13 (some successfully pro-
moted candidates are marked), which means the successful rate is 65%. However,
in the previous research [10], when conducting the experiments on the dataset
from 1971 to 2012, we just ﬁnd the successful rate is only 48%. It shows that our
algorithm is more accurate than the previous. The skyline authors and potential
stars for each year are illustrated in Table 1.
Successful promotion rate comparison based on network scales. At the
same time, we conduct our PromSky and SkyBoundary algorithms in various
network scales. From Fig. 3, we know that the successful rate of our promotion
algorithm is apparently higher than the previous algorithm in various network
scales. This is because we add more attributes in our PromSky algorithm for
a skyline member that it should increase the number of skyline set. Thus our
successful promotion rate is higher in various network scales.
Eﬀect of the PromSky algorithm. Figure 4 shows the average processing
time under diﬀerent network scales (varying from 1 K to 1000K). We can see that
1 http://dblp.org/.

688
S. Zhang and J. Zheng
 0
 20
 40
 60
 80
 100
 120
50
100
200
500 1000 2000 5000 10000
Time Cost(ms)
Network Scale
SkyBoundary
PromSky
Fig. 3. Successful rate comparison on
various network scales
 0
 1
 2
 3
 4
 5
1
10
100
1000
Time Cost (×102 ms)
Network Scale (×103)
PromSky
SkyBoundary
Fig. 4. Eﬀect comparison with the
SkyBoundary algorithm
Table 1. Skyline authors and potential stars from 1996 to 2016
Year
Skyline
Potential skyline
1996
Robert L. Glass, David Wilczynski
Robert W. Floyd
1997
Noga Alon, Jean P, Caxton Foster
Peter Kron
1998
Noga Alon, Robert L. Glass, V. Kevin M
Carl Hewitt, Bill Hancock
1999
Robert W. Floyd, Noga Alon, Honien Liu
Paul A.D., Alan G. Merten
2000
Bill Hancock, Peter Kron
Paul A.D.
2001
Bill Hancock, Nan C. Shu
Pankaj K. Agarwal
2002
Bill Hancock, Charles W. Bachman, Daniel L.
Weller
Pankaj K. Agarwal
2003
Bill Hancock, Daniel L. Weller
Elisa Bertino, Alan G. Merten
2004
Pankaj K. Agarwal, Morton M. Astrahan, David
R. Warn
Elisa Bertino, Mary Zosel
2005
Gary A. Kildall, Diane Crawford, Hans-Peter
Seidel, Erik D.Demaine
Carl Hewitt
2006
Noga Alon, Diane Crawford, Pankaj K. Agarwal
Ingo H. Karlowsky, Louis Nolin
2007
Elisa Bertino, G. RuggiuW, J. Waghorn, M.H.
Kay, Erik D. Demaine
T. William Olle
2008
Diane Crawford, Paul A.D.
B.M. Fossum
2009
Wen Gao, Xin Li, Jun Wang, P.A. Dearnley,
Giampio Bracchi, Paolo Paolini, Ajith Abraham
H. Schenk, Gordon E. Sayre
2010
Xin Li, B.M. Fossum, J.K. Iliﬀe, Wen Gao, Mary
Zosel, Wei Wang
Paul Mies, Ingo H. Karlowsky
2011
Xin Li, Gordon E. Sayre, T. William Olle
Peter Sandner
2012
H. Vincent Poor, Peter Sandner, Ulrich Licht
Yan Zhang
2013
Ingo H. Karlowsky, Heidi Anlauﬀ, G¨unther
Zeisel
Guy G. Boulaye
2014
Yan Zhang, Yu Zhang, Gordon E. Sayre, Witold
Pedrycz
Carl Hewitt
2015
Harold Joseph Highland, Bernard Chazelle
Won Kim
2016
Won Kim, Dale E. Jordan, B.M. Fossum
Nan C. Shu

An Eﬃcient Potential Member Promotion Algorithm in SNs via Skyline
689
as the network scale increases, the processing time of our PromSky algorithm
increases slightly, which performs much better than the SkyBoundary algorithm.
This is because the candidates in SkyBoundary algorithm are all the non-skyline
set but we conduct the skyline query over the non-skyline set and take the infra-
skyline as the candidates thus remarkably reducing the size of the candidates and
controlling the result in a reliable range to a great extent. Besides, by bringing
forward the skyline distance, we can get the minimum cost from the candidates
to the position where not being dominated.
6
Conclusions
In this paper, we propose an improved member promotion algorithm in SNs,
which aims at discovering the most potential stars which can be promoted into
the skyline with minimum cost. By adding the attribute of reputation level, we
describe members’ importance more precisely. Then we introduce the skyline
distance and the sort-projection operations to obtain the necessary situations
for not being dominated. At the same time it also helps a lot to reduce the
number of promotion plans. What’s more, we prune some extra plans based
on the dominance while verifying. Finally, experiments on the DBLP dataset
illustrate the eﬀectiveness and eﬃciency of our approach.
Acknowledgment. This work is partially supported by Natural Science Foundation
of Jiangsu Province of China under grant No. BK20140826, the Fundamental Research
Funds for the Central Universities under grant No. NS2015095, Funding of Graduate
Innovation Center in NUAA under grant No. KFJJ 20161606.
References
1. B¨orzs¨onyi, S., Kossmann, D., Stocker, K.: The skyline operator. In: ICDE, pp.
421–430 (2001)
2. Jiang, B., Pei, J., Lin, X., Cheung, D.W., Han, J.: Mining preferences from superior
and inferior examples. In: SIGKDD, pp. 390–398 (2008)
3. Katz, L.: A new status index derived from sociometric analysis. Psychometrika
18(1), 39–43 (1953)
4. Kossmann, D., Ramsak, F., Rost, S.: Shooting stars in the sky: an online algorithm
for skyline queries. In: VLDB, pp. 275–286 (2002)
5. Lian, X., Chen, L.: Monochromatic and bichromatic reverse skyline search over
uncertain databases. In: SIGMOD, pp. 213–226 (2008)
6. Mindolin, D., Chomicki, J.: Discovering relative importance of skyline attributes.
PVLDB 2(1), 610–621 (2009)
7. Papadias, D., Tao, Y., Fu, G., Seeger, B.: Progressive skyline computation in data-
base systems. ACM Trans. Database Syst. 30(1), 41–82 (2005)
8. Pei, J., Jiang, B., Lin, X., Yuan, Y.: Probabilistic skylines on uncertain data. In:
VLDB, pp. 15–26 (2007)
9. Peng, Z., Wang, C.: Discovering the most potential stars in social networks with
skyline queries. In: Proceedings of the 3rd International Conference on Emerging
Databases (2011)

690
S. Zhang and J. Zheng
10. Peng, Z., Wang, C.: Member promotion in social networks via skyline. World Wide
Web 17(4), 457–492 (2014)
11. Sacharidis, D., Papadopoulos, S., Papadias, D.: Topologically sorted skylines for
partially ordered domains. In: ICDE, pp. 1072–1083 (2009)
12. Tan, K., Eng, P., Ooi, B.C.: Eﬃcient progressive skyline computation. In: VLDB,
pp. 301–310 (2001)
13. Wu, T., Xin, D., Mei, Q., Han, J.: Promotion analysis in multi-dimensional space.
PVLDB 2(1), 109–120 (2009)

Author Index
Ahn, Hee-Kap
3
Ando, Ei
13
Ashok, Pradeesha
25
Avrachenkov, Konstantin E.
591
Banerjee, Soumya
529
Baraldo, Nicola
3
Barbay, Jérémy
38
Barequet, Gill
50
Basu Roy, Aniket
25
Bose, Prosenjit
62
Carneiro, Alan Diêgo Aurélio
75
Chakraborty, Sankardeep
87
Chang, Yi-Jun
275
Charalampopoulos, Panagiotis
99
Chen, Li-Hsuan
112
Chen, Yong
124
Chen, Zhi-Zhong
124
Chen, Zhou
137
Cheng, Yukun
137
Cheriﬁ, Hocine
642
Conte, Alessio
150
Crochemore, Maxime
99
Dai, Wenkai
162
Drazdilova, Pavla
603
Drees, Maximilian
175
Du, Tianyu
615
Feldotto, Matthias
175
Feng, Haodi
575
Feng, Qilong
188
Filmus, Yuval
200
Fung, Stanley P.Y.
212
Gajarský, Jakub
224
Gao, Yong
555
Gaspers, Serge
371
Ghodsi, Mohammad
237
Ghosal, Purnata
250
Govindarajan, Sathish
25
Guo, Jiong
575
Hao, Chunlin
568
Hatami, Hamed
200
He, Jing (Selena)
615
He, Kun
262
Hliněný, Petr
224
Ho, Kuan-Yi
275
Homapour, Hamid
237
Hsieh, Sun-Yuan
112
Hung, Ling-Ju
112
Iliopoulos, Costas S.
99
Ito, Takehiro
287
Ji, Shouling
615
Jiang, Bo
481
Jiang, Haitao
575
Jiang, Yiyao
297
Kadavy, Tomas
666
Kakimura, Naonori
287
Kamiyama, Naoyuki
287
Kanté, Mamadou Moustapha
150
Kawachi, Akinori
309
Kawano, Kenichi
309
Khramtcova, Elena
321
Kijima, Shuji
13
Klasing, Ralf
112
Kobayashi, Yusuke
287
Kociumaka, Tomasz
99
Koivisto, Mikko
333
Kondratev, Aleksei Yu.
591
Konecny, Jan
603
Konstantinidis, Athanasios L.
346
Korman, Matias
62
Koutecký, Martin
224
Krömer, Pavel
630
Kudelka, Milos
603, 654
Kumari, Suchi
642
Laakkonen, Petteri
333
Lauri, Juho
333
Le Gall, François
309
Le, Hoang-Oanh
359

Le, Van Bang
359
Lee, Edward J.
371
Li, Qian
262, 384
Li, Yaqiao
200
Li, Zhao
615
Lin, Guohui
124
Liu, Nan
575
Liu, Zhaohui
542
Mazalov, Vladimir V.
591
Mehrabi, Saeed
396
Mehta, Ruta
407
Misra, Pranabendu
420
Nandy, Subhas C.
457
Nikolopoulos, Stavros D.
346
Nimbhorkar, Prajakta
433
Nishat, Rahnuma Islam
445
Nowaková, Jana
630
Ochodkova, Eliska
654
Oh, Eunjin
3
Okamoto, Yoshio
287
Onn, Shmuel
224
Otachi, Yota
150
Panolan, Fahad
420
Papadopoulos, Charis
346
Papadopoulou, Evanthia
321
Pérez-Lantero, Pablo
38
Pissis, Solon P.
99
Pluhacek, Michal
666
Prakash, Om
250
Protti, Fábio
75
Qi, Qi
137
Radoszewski, Jakub
99
Ramanujan, M.S.
420
Rameshwar V., Arvind
433
Rao, B.V. Raghavendra
250
Rao, Wenjing
529
Riechers, Sören
175
Rojas-Ledesma, Javiel
38
Roy, Sasanka
457
Roy, Suchismita
457
Rytter, Wojciech
99
Sadhu, Sanjib
457
Satti, Srinivasa Rao
87
Saurabh, Saket
420
Seddighin, Masoud
237
Senkerik, Roman
666
Shalah, Mira
50
Shenmaier, Vladimir
469
Silvestri, Francesco
3
Singh, Anurag
642
Skopalik, Alexander
175
Souza, Uéverton S.
75
Sun, Xiaoming
262, 384
Tamaki, Suguru
309
Tan, Xuehou
481
Tayu, Satoshi
492
Tirodkar, Sumedh
504
Ueno, Shuichi
492
Uno, Takeaki
150
van Renssen, André
62
Vazirani, Vijay V.
407
Verdonschot, Sander
62
Viktorin, Adam
666
Vishwanathan, Sundar
504
Waleń, Tomasz
99
Wang, Dan
124
Wang, Jianxin
188, 516
Wang, Lusheng
124
Wasa, Kunihiro
150
Whitesides, Sue
445
Wu, Chenchen
568
Wu, Guangwei
516
Xie, Ying
615
Xu, Dachuan
568
Xu, Jian
529
Yan, Xiang
137
Yen, Hsu-Chun
275
You, Suzin
200
Yu, Wei
542
Zehnalova, Sarka
654
Zhang, Congsong
555
692
Author Index

Zhang, Dongmei
568
Zhang, Siman
678
Zhang, Zhenning
568
Zheng, Jiping
678
Zheng, Yufei
50
Zhou, Aizhong
575
Zhu, Binhai
575
Zhu, Senmin
188
Author Index
693


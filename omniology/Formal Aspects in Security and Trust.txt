Lecture Notes in Computer Science
5983
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Alfred Kobsa
University of California, Irvine, CA, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Germany
Madhu Sudan
Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max-Planck Institute of Computer Science, Saarbruecken, Germany

Pierpaolo Degano Joshua D. Guttman (Eds.)
Formal Aspects
in Security
and Trust
6th International Workshop, FAST 2009
Eindhoven, The Netherlands, November 5-6, 2009
Revised Selected Papers
1 3

Volume Editors
Pierpaolo Degano
Università di Pisa, Dipartimento di Informatica
Largo Bruno Pontecorvo, 3, 56127 Pisa, Italy
E-mail: degano@di.unipi.it
Joshua D. Guttman
Worcester Polytechnic Institute
100 Institute Rd, Worcester, MA 01609, USA
E-mail: guttman@wpi.edu
Library of Congress Control Number: 2010924066
CR Subject Classiﬁcation (1998): C.2.0, K.6.5, D.4.6, E.3, K.4.4, H.3-4
LNCS Sublibrary: SL 4 – Security and Cryptology
ISSN
0302-9743
ISBN-10
3-642-12458-5 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-12458-7 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2010
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
06/3180

Preface
The present volume contains the proceedings of the 6th International workshop
on Formal Aspects of Security and Trust (fast 2009), held in Eindhoven, The
Netherlands, 5–6 November 2009, as part of Formal Methods Week 2009. fast
is sponsored by IFIP WG 1.7 on Foundations of Security Analysis and Design.
The previous ﬁve fast workshop editions have fostered cooperation among
researchers in the areas of security and trust, and we aimed to continue this
tradition. As computing and network infrastructures become increasingly perva-
sive, and as they carry increasing economic activity, society needs well-matched
security and trust mechanisms. These interactions increasingly span several en-
terprises and involve loosely structured communities of individuals. Participants
in these activities must control interactions with their partners based on trust
policies and business logic. Trust-based decisions eﬀectively determine the secu-
rity goals for shared information and for access to sensitive or valuable resources.
fast sought original papers focusing of formal aspects of: security and trust
policy models; security protocol design and analysis; formal models of trust
and reputation; logics for security and trust; distributed trust management sys-
tems; trust-based reasoning; digital assets protection; data protection; privacy
and id issues; information ﬂow analysis; language-based security; security and
trust aspects in ubiquitous computing; validation/analysis tools; Web service
security/trust/privacy; grid security; security risk assessment; and case studies.
The fast proceedings contain—in addition to an abstract of the invited talk
by Anindya Banerjee—revisions of full papers accepted for presentation at fast.
The 18 papers appearing here were selected out of 50 submissions. Each paper
was reviewed by at least three members of the Program Committee, whom we
wish to thank for their eﬀort. Many thanks also to the organizers of the For-
mal Methods Week 2009 for accepting fast 2009 as an aﬃliated event and for
providing a perfect environment for running the workshop.
We are also grateful to the Easychair organization, which created a help-
ful framework for refereeing and PC discussion, and helped to construct these
proceedings.
November 2009
Pierpaolo Degano
Joshua Guttman

Organization
Program Committee
Gilles Barthe
IMDEA Software, Spain
Fr´ed´eric Cuppens
Telecom Bretagne, France
Pierpaolo Degano
University of Pisa, Italy (Program Co-chair)
Theo Dimitrakos
BT, UK
Sandro Etalle
TU Eindhoven, The Netherlands
Roberto Gorrieri
University of Bologna, Italy
Joshua Guttman
Worcester Polytechnic Institute, USA
(Program Co-chair)
Masami Hagiya
University of Tokyo, Japan
Chris Hankin
Imperial College (London), UK
Bart Jacobs
Radboud University Nijmegen,
The Netherlands
Christian Jensen
DTU, Denmark
Yuecel Karabulut
SAP Research, USA
Igor Kotenko
SPIIRAS, Russia
Fabio Martinelli
IIT-CNR, Italy
Catherine Meadows
Naval Research Lab, USA
Ron van der Meyden
University of New South Wales, Australia
Mogens Nielsen
University of Aarhus, Denmark
Dusko Pavlovic
Kestrel Institute, USA and Oxford, UK
Riccardo Pucella
Northeastern University, USA
Peter Ryan
University of Luxembourg
Steve Schneider
University of Surrey, UK
Jean-Marc Seigneur
University of Geneva, Switzerland
Ketil Stølen
SINTEF, Norway
Organizing Committee
Sandro Etalle
Eindhoven, The Netherlands (Chair)
Jolande Matthijsse
Eindhoven, The Netherlands
Additional Reviewers
Alessandro Aldini
Miguel Andrs
Damiano Bolzoni
Gabriele Costa
Jason Crampton
Rafael Deitos
Juergen Doser
Ichiro Hasuo
Daniel Hedin
Cesar Kunz
Aliaksandr Lazouski
Stephane Lo-Presti
Mass Lund
Pratyusa Manadhata
Isabella Mastroeni

VIII
Organization
Ilaria Matteucci
Wojciech Mostowski
Marinella Petrocchi
B. Pontes Soares Rocha
Peter van Rossum
Alejandro Russo
Hideki Sakurada
Bjornar Solhaug
Daniel Trivellato
Yasuyuki Tsukada

Table of Contents
Invited Lecture
Semantics and Enforcement of Expressive Information Flow Policies . . . .
1
Anindya Banerjee
Session 1. Trust
An Algebra for Trust Dilution and Trust Fusion . . . . . . . . . . . . . . . . . . . . .
4
Baptiste Alcalde and Sjouke Mauw
HMM-Based Trust Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Ehab ElSalamouny, Vladimiro Sassone, and Mogens Nielsen
Deriving Trust from Experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
Florian Eilers and Uwe Nestmann
Reﬂections on Trust: Trust Assurance by Dynamic Discovery of Static
Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
Andrew Cirillo and James Riely
Session 2. Workﬂow and Orchestration
Model Checking of Security-Sensitive Business Processes . . . . . . . . . . . . . .
66
Alessandro Armando and Serena Elisa Ponta
Session 3. Secure Flow
Analysing the Information Flow Properties of Object-Capability
Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Toby Murray and Gavin Lowe
Applied Quantitative Information Flow and Statistical Databases . . . . . .
96
Jonathan Heusser and Pasquale Malacaria
Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation . . . . . . . . .
111
Josef Svenningsson and David Sands
Secure Information Flow for Distributed Systems . . . . . . . . . . . . . . . . . . . .
126
Rafael Alp´ızar and Geoﬀrey Smith

X
Table of Contents
Session 4. Mobility and Deniability
Probable Innocence in the Presence of Independent Knowledge . . . . . . . .
141
Sardaouna Hamadou, Catuscia Palamidessi,
Vladimiro Sassone, and Ehab ElSalamouny
A Calculus of Trustworthy Ad Hoc Networks . . . . . . . . . . . . . . . . . . . . . . . .
157
Massimo Merro and Eleonora Sibilio
Session 5. Protocols 1
Comparison of Cryptographic Veriﬁcation Tools Dealing with Algebraic
Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Pascal Lafourcade, Vanessa Terrade, and Sylvain Vigier
Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols . . . .
186
Ying Zhang, Chenyi Zhang, Jun Pang, and Sjouke Mauw
Attack, Solution and Veriﬁcation for Shared Authorisation Data in
TCG TPM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Liqun Chen and Mark Ryan
Session 6. Protocols 2
Trusted Multiplexing of Cryptographic Protocols. . . . . . . . . . . . . . . . . . . . .
217
Jay McCarthy and Shriram Krishnamurthi
Specifying and Modelling Secure Channels in Strand Spaces . . . . . . . . . . .
233
Allaa Kamil and Gavin Lowe
Session 7. Protocols 3
Integrating Automated and Interactive Protocol Veriﬁcation . . . . . . . . . . .
248
Achim D. Brucker and Sebastian A. M¨odersheim
A User Interface for a Game-Based Protocol Veriﬁcation Tool . . . . . . . . .
263
Peeter Laud and Ilja Tˇsahhirov
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279

Semantics and Enforcement of
Expressive Information Flow Policies
Anindya Banerjee⋆
IMDEA Software, Madrid, Spain
anindya.banerjee@imdea.org
The following is intended as an overview of my invited talk at the 2009 FAST work-
shop. The primary reference for this work remains the earlier paper [4] that contains the
necessary technical details, motivating examples and commentary on particular design
choices.
The talk focuses on conﬁdentiality policies of sequential, heap-manipulating pro-
grams (typically formalized as noninterference) and shows how to exploit techniques
from type systems, program logics and veriﬁcation for
– Speciﬁcation of expressive conﬁdentiality policies based on declassiﬁcation of in-
formation.
– Modular enforcement of conﬁdentiality policies mixing security type-based analy-
sis and veriﬁcation.
Policy speciﬁcations often can be incomplete and restrictive in that they do not cap-
ture requirements adequately. For example, one might desire a policy that captures the
requirement “secret until Tuesday” rather than “secret forever”. In a medical setting, one
might want to capture the requirement that a patient’s medical status can be revealed
to a specialist only under consent from the patient and her primary care physician and
only after a log entry has been written to the effect. As part of a disaster relief plan, one
might want to capture the requirement that the medical histories of all patients — but
not their doctors’ notes — be revealed.
The above requirements are intended as examples for the need to specify expressive
declassiﬁcation policies that include conditions under which downgrading of conﬁden-
tial information is permitted. In the terminology of Sabelfeld and Sands [12], a policy
may need to encompass when declassiﬁcation may happen, what information can be
declassiﬁed, where in the code declassiﬁcation is allowed, etc. What is the end-to-end
semantics of such declassiﬁcation policies? How can such policies be speciﬁed and
enforced?
The semantics of policies draws inspiration from Askarov and Sabelfeld’s knowledge-
based formulation [3] of noninterference. The knowledge-based formulation permits an
end-to-end semantic property based on a model that allows observations of intermediate
public states as well as termination. An attacker’s knowledge only increases at explicit
declassiﬁcation steps, and within limits set by policy.
⋆Partially supported by US NSF awards CNS-0627748, ITR-0326577 and by a sabbatical visit
to Microsoft Research, Redmond.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 1–3, 2010.
c⃝Springer-Verlag Berlin Heidelberg 2010

2
A. Banerjee
Static enforcement is provided by combining type-checking with program veriﬁca-
tion techniques applied to the small subprograms (or sessions) that carry out declassi-
ﬁcations. The enforcement has been proven sound for the simple imperative language.
The veriﬁcation techniques are based on a relational Hoare logic [6] that combines
reasoning about ordinary assertions as well as “two-state” assertions called agreement
assertions that express what is released. The use of two-state assertions stems from
the observation that noninterference can be speciﬁed using Hoare triples that assert the
equality of observable variables over two runs of a program [1,2]. The talk shows how
the logic can take care of the what, when and where aspects of declassiﬁcation poli-
cies. In the case of object-oriented programs the ordinary assertions belong to region
logic [5] which facilitates reasoning about the heap. An illustrative example of static
enforcement involves the veriﬁcation of a heap-based data structure with declassiﬁca-
tion. In summary, there are three steps in the enforcement process: type checking for
baseline policies, assertion checking using e.g., region logic, and relational veriﬁcation
of two-state assertions.
One beneﬁt of using assertion checking in concert with relational veriﬁcation is
that the approach ﬁts well with access control. For example, it is possible to track a
program’s currently enabled permissions in a ghost variable (say SecurityCtx). The
speciﬁcation can express what is released given various permissions. The policy “re-
lease h provided permission p is enabled” has two speciﬁcations with preconditions
p ∈SecurityCtx ∧A(h) and p ̸∈SecurityCtx. The “A(h)” in the ﬁrst precondition is a
relational (agreement) assertion: it says that h is released, that is, in two runs of the
program the values of h are equal.
The bibliography below provides the main inspirations for the work. A complete
bibliography appears in the original paper [4].
Acknowledgements. I am very grateful to the organizers of FAST 2009 for their invita-
tion and to Joshua Guttman, in particular, for encouragement.
This work is in collaboration with David Naumann and Stan Rosenberg. I would
like to thank them for the many hours (years!) of stimulating discussions, and for their
patience and camaraderie.
References
1. Amtoft, T., Banerjee, A.: Information ﬂow analysis in logical form. In: Giacobazzi, R. (ed.)
SAS 2004. LNCS, vol. 3148, pp. 100–115. Springer, Heidelberg (2004)
2. Amtoft, T., Bandhakavi, S., Banerjee, A.: A logic for information ﬂow in object-oriented
programs. In: ACM Symposium on Principles of Programming Languages (POPL), pp. 91–
102 (2006)
3. Askarov, A., Sabelfeld, A.: Gradual release: Unifying declassiﬁcation, encryption and key
release policies. In: IEEE Symposium on Security and Privacy, pp. 207–221 (2007)
4. Banerjee, A., Naumann, D., Rosenberg, S.: Expressive declassiﬁcation policies and their
modular static enforcement. In: IEEE Symposium on Security and Privacy, pp. 339–353
(2008)
5. A. Banerjee, D. Naumann, and S. Rosenberg. Regional logic for local reasoning about global
invariants. In ECOOP. pages 387–411, 2008.

Semantics and Enforcement of Expressive Information Flow Policies
3
6. Benton, N.: Simple relational correctness proofs for static analyses and program transforma-
tions. In: POPL, pp. 14–25 (2004)
7. Broberg, N., Sands, D.: Flow locks. In: ESOP, pp. 180–196 (2006)
8. Chong, S., Myers, A.C.: Security policies for downgrading. In: ACM CCS, pp. 198–209
(2004)
9. Myers, A.C.: JFlow: Practical mostly-static information ﬂow control. In: POPL, pp. 228–241
(1999)
10. Rushby, J.: Noninterference, transitivity, and channel-control security policies. Technical re-
port, SRI (December 1992)
11. Sabelfeld, A., Myers, A.C.: A model for delimited information release. In: Futatsugi, K.,
Mizoguchi, F., Yonezaki, N. (eds.) ISSS 2003. LNCS, vol. 3233, pp. 174–191. Springer,
Heidelberg (2004)
12. Sabelfeld, A., Sands, D.: Dimensions and principles of declassiﬁcation. Journal of Computer
Security (2007)
13. Zdancewic, S.: Challenges for information-ﬂow security. In: Proceedings of the 1st Interna-
tional Workshop on the Programming Language Interference and Dependence, PLID 2004
(2004)

An Algebra for Trust Dilution and Trust Fusion
Baptiste Alcalde and Sjouke Mauw
University of Luxembourg
baptiste.alcalde@uni.lu, sjouke.mauw@uni.lu
Abstract. Trust dilution and trust fusion are two operators that are
used to calculate transitive trust in a trust network. Various implemen-
tations of these operators already exist but are not fully motivated. In
this paper we deﬁne the basic properties of these two operators by de-
veloping a trust algebra. We evaluate several new and existing models
against the axioms of this algebra, amongst which a number of variations
of the Subjective Logic. The algebra enables the comparison of models
and gives more insight in the available recommendation models and their
properties.
1
Introduction
Trust transitivity is deﬁned as the possibility to use trust information from other
entities in order to infer a trust evaluation to a given entity. Trust transitivity
is a key concept of recommendation systems and it attracts an ever increasing
interest in the very recent years [4,6,12,13]. To date, we can identify two main
recommendation model families. The ﬁrst is qualitative and uses, for instance,
modal logic [4]. The second, which is the focus of this paper, is quantitative and
deﬁnes special trust operators, named fusion and dilution operators, in order to
compute the resulting trust of a trust network [9,14,15].
Dilution is used to calculate the trust along trust chains. This operator com-
bines agent A’s trust in agent B with agent B’s trust in agent C, to derive
A’s trust in C. Fusion is used to compute the overall trust if there are diﬀerent
sources of information. If agent A has two independent sources of information,
say B and D, on the trustworthiness of agent C, then B and D’s information
can be combined using the fusion operator.
In literature, several diﬀerent deﬁnitions of fusion and dilution operators have
been proposed. These deﬁnitions are often mainly motivated by technical obser-
vations, rather than by strong and defendable intuition. Hence these deﬁnitions
can be hard to understand or to interpret from the point of view of an outsider.
Therefore, one of the main motivations for the current research is the lack of
determination of the intrinsic properties of these operators.
In order to judge whether dilution and fusion deﬁnitions provide a suitable
modeling of the phenomena, a description of these phenomena at a higher level
of abstraction is required. In addition, such abstraction will enable to compare
the relative merits of alternative deﬁnitions and extensions.
Hence, the ﬁrst contribution of this paper is to develop a higher level of
abstraction in the form of a trust algebra. This algebra consists of a number of
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 4–20, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

An Algebra for Trust Dilution and Trust Fusion
5
deﬁning properties of the fusion and dilution operators. One of the merits of
this approach is that these properties, stated in the form of equational axioms,
can be motivated from the domain of trust and recommendation. Therefore,
rather than proposing a new recommendation model, this paper aims at giving
formal guidelines to possible implementation of recommendation models as well
as a means for their comparison. To our knowledge, this approach was never
explored until now.
Such an algebraic approach has shown very beneﬁcial e.g. in the realm of
parallel systems, in which the development and analysis of process algebras has
added to the understanding of the many diﬀerent process models. Similarly, the
development of a trust algebra will help to understand the diﬀerent trust models.
Using this approach, Subjective Logic (SL), as proposed in [9], can then be
seen as one of a number of possible (and plausible) models for the abstract alge-
bra. We consider the investigation of existing recommendation models, amongst
which SL, and the development of new models as one of the contributions of
this paper. Our study also reveals weaknesses in some of the models, and can
provide a valuable feedback for the establishment of future models.
The paper is structured as follows. In Sect. 2 we clarify some general deﬁni-
tions and assumptions needed as a background for this research. In Sect. 3 we
present the trust algebra and some extensions as well as the motivation for the
rules composing this algebra. In Sect. 4 we show the applicability of the algebra
through the comparison of the canonical models (with three and four elements),
and Subjective Logic variations. As a result from the evaluation, we can prove
impossibility results in some models, show the limitations of others, and prove
the correctness of a newly crafted model. A summary of the results is provided
in a table at the end of Sect. 4. In the conclusion we interpret our results and
propose a number of interesting venues for future work.
2
Trust Relations
Trust has been deﬁned in several diﬀerent ways. The deﬁnition of trust adopted
here, ﬁrst formulated by Gambetta [5], is often referred to as “reliability trust”.
Thus, we deﬁne trust as the belief or subjective probability of the trustor that
the trustee will adequately perform a certain action on which the trustor’s wel-
fare depends. We also refer to the trustor and trustee as agents, which may be
humans or computer programs acting on the behalf of humans. Trust is hence a
quantiﬁable relation between two agents.
In literature, many factors have been identiﬁed that can be taken into account
when calculating the trust relation between two agents (see [1] for an overview).
These factors comprise e.g. the trustor’s personality, the trustee’s competence,
contextual information such as local norms and customs, and the opinions of
other agents. The introduction of opinions allows a trustor to take other agents’
opinions into account when determining the trustworthiness of a trustee, thus
yielding a trust network. We start oﬀwith the observation from [11] that there
are diﬀerent notions of trust involved. First, we make a distinction between two

6
B. Alcalde and S. Mauw
variants of trust, viz. functional trust and referral trust. Functional trust is the
belief in an entity’s ability (and willingness) to carry out or support a speciﬁc
function on which the relying party depends. Referral trust is the belief in an
entity’s ability to recommend another entity w.r.t functional trust. The other
distinction is between two types of trust, viz. direct trust and indirect trust. A
direct trust relation occurs when the trustor trusts the trustee directly (without
intermediaries), e.g. based on past experiences between them. An indirect trust
occurs when the trustor trusts a trustee based on one or more opinions from
third parties.
By combining a trust variant with a trust type we can obtain functional direct
trust, functional indirect trust, referral direct trust, and referral indirect trust.
For example, Alice wants to know where to ﬁnd a good car mechanic to ﬁx
her car. She asks Bob’s opinion because he is knowledgeable about cars (direct
referral trust). Bob happens to know a good car mechanic (direct functional
trust). Bob then suggests Alice the name of this car mechanic (recommendation).
Alice can then bring her car to this car mechanic (indirect functional trust). We
can note that after this transaction, Alice will transform her indirect functional
trust into direct functional trust (since she will then have a direct experience
with the car mechanic).
In addition to these deﬁnitions we set a number of assumptions. First, we
assume that the trustor knows all trust relations between agents that are relevant
for her own trust calculations. In literature, this rather strong assumption is often
called perfect forward, as to indicate that all agents are willing to forward other
agents’ trust values without modiﬁcation.
Further, we assume that each agent keeps track of his own functional and
referral trust in other agents. We will use the same domain for expressing trust
values of all variants and types of trust. In a given trust graph all trust relations
concern referral trust, except for the arrows directly ending at the trustee, which
concern functional trust. In the remainder of the paper, we will therefore not
explicitly mention the type of a given trust relation if it can be derived from the
context.
In literature, a distinction is made between two ways of composing trust val-
ues, viz. fusion and dilution. Trust fusion occurs if there are multiple trust paths
from a trustor to a trustee, meaning that the trustee is recommended by several
agents. In order to calculate his trust in the trustee, the trustor then has to
fuse the trust values of these other agents. The fusion of trust values does not
necessarily lead to a higher level of trust. The dilution of trust occurs if there is
a trust chain from the trustor to the trustee. Every link in the chain reduces (or
dilutes) the overall trust of the trustor in the trustee implied by the trust chain.
3
A Trust Algebra
In this section we develop the algebra of trust expressions, which is the ﬁrst
contribution of this paper. This abstract algebra is partly based on the more
concrete operators found in literature (see e.g. [14,15,8]).

An Algebra for Trust Dilution and Trust Fusion
7
We develop our algebra in four layers. The ﬁrst layer introduces the basic
constants and operators and their basic properties. The second layer provides
an extension of this algebra which allows us to compare trust expressions. In
the third layer we express the duality of belief and disbelief, while in the fourth
layer we treat the special case of full direct functional belief and disbelief.
Basic Fusion and Dilution Algebra. A trust expression is obtained by (re-
cursively) applying some trust operators to a number of trust atoms (or trust
values). See the upper frame of Fig. 1 for the signature of basic trust expres-
sions. The set of trust expressions is denoted by T and the set of trust atoms
by A. We consider two basic trust operators: trust fusion (denoted by
+ ) and
trust dilution (denoted by
· ). The set of trust atoms is not speciﬁed in detail.
We require that it contains at least the three constants υ, β, and δ. Constant
υ denotes full uncertainty, i.e. the absence of any information that can help to
assess the trustworthiness of the trustee. Constant β denotes full trust of the
trustor in the trustee, without any uncertainty. Constant δ is the dual of β. It
denotes full distrust of the trustor in the trustee, without any uncertainty. We
use parentheses to disambiguate trust expressions.
An example of a trust expression is (β · β) + (υ · δ). This expresses that,
although the trustor has no direct trust relation with the trustee, he knows two
independent sources that have a direct functional trust relation with the trustee.
The trustor has full direct referral trust in the ﬁrst source, who has full direct
functional trust in the trustee (β·β). Further, the trustor is completely uncertain
whether to trust his second source or not. His second source does not trust the
trustee at all; he has full distrust in the trustee (υ · δ).
In order to simplify trust expressions we assume that the dilution operator ·
binds stronger than the fusion operator + . We will often omit the · operator
from expressions if no confusion can arise. In this way, the above example can
be simpliﬁed to ββ + υδ.
T
set of trust expressions
·
: T × T →T dilution (operator)
A ⊆T
set of trust atoms
υ ∈A
full uncertainty (constant)
x, y ∈T
variables
β ∈A
full belief (constant)
+
: T × T →T fusion (operator)
δ ∈A
full disbelief (constant)
(B1) x + y = y + x
(C1) x + υ = x
(C4) β + β = β
(C6) δ + δ = δ
(B2) x + (y + z) = (x + y) + z
(C2) x · υ = υ
(C5) β · x = x
(C7) δ · x = υ
(B3) x(yz) = (xy)z
(C3) υ · x = υ
Fig. 1. Basic Fusion and Dilution algebra (BFD)
The properties of these constants and operators are expressed by a set of
axioms, which we call the Basic Fusion and Dilution (BFD) algebra (Fig. 1). The
ﬁrst three axioms express properties of the basic operators. The fusion operator
is commutative (B1) and associative (B2), since the order in which the trustor
receives independent recommendations is irrelevant. Calculating transitive trust
along a trust chain is also associative, so dilution is an associative operator as
well (B3). However, dilution is not commutative. This can be seen by a simple

8
B. Alcalde and S. Mauw
example. Assume that agent A fully trusts agent B’s opinion on agent C and
assume that B fully distrusts C. Then A should also fully distrust C. However,
if we swap the values, i.e. A has full distrust in B, who fully trusts C, then A
should not necessarily (dis)trust C, so βδ ̸= δβ.
Axioms C1–C7 deﬁne the properties of the three constants. The uncertainty
constant υ behaves like a zero element. Adding a fully uncertain opinion to an
opinion x does not give any extra information, so x + υ = x (axiom C1). By
combining this with axiom B1 we obtain the symmetric case υ + x = x. Axioms
C2 and C3 express that full uncertainty in a trust chain annihilates any other
information in this chain, so x · υ = υ · x = υ. If we fuse full belief with itself, it
remains full belief (axiom C4). Axiom C5 expresses that the full belief constant
β behaves as a left-unit for dilution. This follows from the fact that if we fully
belief another agent, we adopt his opinion without any hesitation. Clearly, β is
not a right-unit, so we don’t have x·β = x. If A distrusts B and B trusts C, then
this does not mean that A should distrust C, δβ ̸= δ. The disbelief constant δ
behaves similar to β in a fusion context: if we get our full disbelief conﬁrmed by
another source, the fusion is still full disbelief (axiom C6). Finally, if we consider
the opinion of somebody whom we disbelief, it will give us no information at all,
so δ · x = υ (axiom C7). Obviously, the converse, x · δ = υ, does not hold, since
e.g. β · δ = δ. Later we will come back to expressions of the form x · β and x · δ.
Jøsang [11] also mentions interpretations which are diﬀerent from the intuition
sketched above. If we assume that “the friend of my enemy is my enemy”, then
the interpretation of β as a right-unit would make sense. This interpretation
would also have consequences for axiom C7, since then we would have δβ = δ.
However, following Jøsang, we consider these interpretations as rather exotic
and we will leave them for future study.
It is important to notice that there exist expressions that are not equal (after
applying the axioms) to a constant. An example is β+δ which expresses that via
one route we obtain the information that the trustee can be trusted without any
uncertainty, while via another independent route we learn that the trustee must
be distrusted without any uncertainty. There are diﬀerent ways to interpret the
fusion of such dogmatic opinions, some of which are discussed in [10]. In order
to allow for such diﬀerent interpretations, we decided to not settle for a ﬁxed
interpretation in the algebra. Alternative interpretations can then be expressed
by deﬁning diﬀerent models of the algebra.
As discussed by Jøsang [8] the fusion and dilution operators do not distribute.
For instance, xz + yz = (x + y)z is not a desired property, because in the left-
hand side of this equation the two occurrences of z represent two independent
opinions, which must both be taken into consideration in the fusion. Hence,
they will reinforce each other. However, in the right-hand side of the equation,
opinion z is only considered once. For the same reason, idempotence of the fusion
operator (x + x = x), is not a required property either.
Comparing Trust Expressions. In the following, we impose some additional
structure on trust expressions by introducing a number of auxiliary operators.
The ﬁrst extension of the basic algebra allows us to compare trust expressions.

An Algebra for Trust Dilution and Trust Fusion
9
In order to evaluate the results of a trust calculation, one must be able to
compare trust values. This will, for instance, allow one to select an agent that
he considers most trusted for a speciﬁc task. Given the three-valued basis (υ,
β, δ) of our algebra, a one-dimensional measure on trust values will be insuﬃ-
cient. Therefore, we will introduce three diﬀerent measures, one for each of the
components. These measures will be formally modeled as total orders on trust
expressions: ≤u, ≤b, and ≤d (see Fig. 2).
The inequality x ≤u y expresses that the uncertainty component in expression
x is at most as high as the uncertainty component in y. The inequality x ≤b y
expresses that the belief component in expression x is at most as high as the
belief component in y. Likewise, the inequality x ≤d y expresses that the disbelief
component in expression x is at most as high as the disbelief component in y.
≤u: T × T
compare uncertainty(total order)
≤b: T × T
compare belief (total order)
≤d: T × T
compare disbelief (total order)
(T1) x ≤u υ
(T4) υ ≤b x
(T7) υ ≤d x
(T10) x + y ≤u x
(T2) β ≤u x
(T5) x ≤b β
(T8) β ≤d x
(T11) x ≤u x · y
(T3) δ ≤u x
(T6) δ ≤b x
(T9) x ≤d δ
(T12) y ≤u x · y
Fig. 2. Axioms for the total orders (TO)
Axiom T1 states that full uncertainty υ is the top element in the uncertainty
order ≤u, since it dominates all other elements. Axioms T2 and T3 state that full
belief β and full disbelief δ do not express any uncertainty, and hence they are
bottom elements. Axioms T4–T9 specify similar properties for the belief order (in
which β is the top element) and the disbelief order (in which δ is the top element).
Axiom T10 expresses a basic property of trust fusion, namely that uncertainty
does not increase if we receive more information on the trustworthiness of a
trustee. In presence of the symmetry axiom B1, this axiom is equivalent to
x+y ≤u y. Axioms T11 and T12 state a similar basic property for trust dilution:
along a trust chain, uncertainty can only grow. The set of axioms T1–T12 forms
the TO (for Total Order) extension of the BFD algebra.
The Duality of Belief and Disbelief. The second extension of the algebra
serves to express the duality of belief and disbelief. In order to express this
duality, we introduce the inverse operator x (see Fig. 3). This operator swaps
the belief and disbelief components of a trust expression. Axiom I1 expresses the
basic inversion property. Distributivity of inversion over fusion is expressed in
axiom I2. This means that belief and disbelief are treated similarly when fusing
trust opinions. Distributivity of inversion over dilution, x · y = x · y, does not
hold, because in a trust chain belief and disbelief are not each other’s duals.
Axiom I3 states that if we have full uncertainty (so no belief nor disbelief), the
inverse operator has no eﬀect. This also stresses that υ is a zero element.
Axiom I4 expresses the duality of belief and disbelief. In presence of axiom
I1 this axiom is equivalent to β = δ. Axioms I5 and I6 state that uncertainty

10
B. Alcalde and S. Mauw
: T →T
invert belief/disbelief (operator)
(I1) x = x
(I3) υ = υ
(I5) x ≤u x
(I7) x ≤b y = x ≤d y
(I2) x + y = x + y
(I4) δ = β
(I6) x ≤u x
Fig. 3. Axioms for the inverse operator (INV)
is orthogonal to belief and disbelief. Finally, axiom I7 states that the inverse
function swaps the belief and disbelief values of a trust expression. The set of
axioms I1–I7 forms the INV (for INVerse) extension of the BFD algebra. Using
the dilution and inverse operators we now have that one constant suﬃces to
deﬁne the other two. For instance, υ = δ · δ and β = δ (or υ = β · β and δ = β).
Further, by using I2 we achieve equivalence of axioms C4 and C6.
Full Direct Functional Belief and Disbelief. The third, and ﬁnal, extension
of our algebra concerns the meaning of full direct functional belief and disbelief.
This means that we consider trust chains ending in β or δ, as in x · δ and x · β,
which capture the situation that the last agent in a chain has full belief or
disbelief in the trustee. Although it is tempting to set e.g. x · β = x, we consider
this as a too strong axiom. The belief component of x · β is clearly identical to
the belief component of x, but their disbelief components are not. Therefore, we
weaken this axiom to x · β =b x (see axiom R1 in Fig. 4). Here we use x =b y
as a shorthand notationfor x ≤b y ∧y ≤b x. Thus, x =b y means that trust
expressions x and y express equal belief. Likewise, we deﬁne =d and =u.
Intuitively, axiom R1 states that if the last element in a chain has full belief,
then belief of the whole chain is determined by the remainder of the chain.
Axiom R2 states that in this case there is no disbelief. If we consider a trust
chain ending in full disbelief, then the whole chain does not express any belief
(axiom R3) and the disbelief expressed in the whole chain is exactly the belief
that we have in the last agent before the trustee (axiom R4). We consider these
axioms as a separate module since they are of a less basic nature. The set of
axioms R1–R4 forms the RM (for Right Multiplication) extension of the BFD
algebra.
(R1) x · β =b x
(R2) x · β =d β
(R3) x · δ =b δ
(R4) x · δ =d x
Fig. 4. Axioms for right-multiplication (RM)
In the remaining of the paper we will refer to combinations of the rules
of the basic algebra BFD and one or more of the extensions. For instance,
BFD+TO+INV denotes the algebra consisting of the basic rules of BFD, and
the extensions of TO and INV.

An Algebra for Trust Dilution and Trust Fusion
11
4
Models
In this section we investigate possible models of the algebra. First we look at
small models containing three and four elements, and next we consider some
models derived from the Subjective Logic. A distinguishing factor is the inter-
pretation of the term β + δ. The main purpose of this chapter is to show how
trust evaluation algorithms can be validated. In particular, several published
variants of the Subjective Logic will be studied. We propose two new variants
of the Subjective Logic which satisfy a larger set of axioms than the existing
variants.
Given an algebra (Σ, E), consisting of a signature Σ and a set of equations E,
a model is a mathematical structure which interprets the sorts and functions of
Σ as sets and (total) functions on these sets. This interpretation must be such
that all equations of E are valid in the model. An equation t = t′ (where t and t′
are terms over the signature, possibly containing variables) is valid in a model if
for every instantiation of the variables the interpretation of t is the same element
as the interpretation of t′.
4.1
Three-Element Models
First, we investigate the canonical model of three distinct elements {b, d, u} with
interpretation β →b, δ →d, υ →u. This interpretation does not uniquely deﬁne
the model, since there is still freedom in choosing a suitable interpretation of the
fusion and dilution operators. Nevertheless, in order to satisfy the axioms, there
is only little freedom left, namely in deﬁning the outcome of b + d, which we
consider a parameter σ of the model. Thus, we introduce the class of possible
models M3(σ) in which σ ∈{b, d, u} represents the fusion of b and d. The
interpretation of the operators is given in Fig. 5. The tables in this ﬁgure must
be read in the order “row-operator-column”. For instance, in the second table
we can look up the value of b · d by crossing the row labeled b (ﬁrst row) with
the column labeled d (second column). This yields b · d = d. The fusion table
is determined, up to σ, by axioms B1, C1, C4, and C6. The dilution table is
fully determined by axioms C3, C5, and C7. The inversion table is determined
by axioms I1, I3, and I4.
+ b d u
b b σ b
d σ d d
u b d u
· b d u
b b d u
d u u u
u u u u
x x
b d
d b
u u
Fig. 5. The three-element models M3(σ)
Next, we investigate possible choices for σ. M3(u) is not a model of BFD,
because associativity of fusion yields the following derivation b = b + u = b +
(b + d) = (b + b) + d = b + d = u. Thus M3(u) does not satisfy axiom B2.

12
B. Alcalde and S. Mauw
On the other hand, a simple case distinction suﬃces to verify that M3(b)
and M3(d) are indeed models of BFD. Surprisingly, these are not models of
the extended algebra BFD+TO+INV (irrespective of the deﬁnition of the total
orders). Using axiom I2 we can, e.g., derive the following equality for M3(d):
b = d = b + d = b + d = d + b = d.
The origin of the problem is in the requirement that belief and disbelief are
treated equally by the fusion operator (axiom I2), which cannot be realized with
three elements. By generalizing this reasoning we obtain the following impossi-
bility result.
Theorem 1. There does not exist a (non-trivial) three-element model of BFD+
TO+ INV.
4.2
Four-Element Models
As a consequence of the previous observations, we investigate somewhat richer
models consisting of four elements {u, b, d, i}, where i denotes inconsistency or
contradiction. The element i is used to give a meaning to β + δ. The underlying
idea is that if we receive fully certain, but contradictory information, we cannot
combine this in a consistent way.
We interpret the constants as before (β →b, δ →d, υ →u) and the operators
as in Fig. 6. The table for the fusion operator follows from the fusion axioms in
BFD. Observe that inconsistencies in a fusion are persistent (yielding e.g. i+u =
i). The table for the dilution operator has three parameters, π, ρ, and σ. The
other values are determined by the dilution axioms in BFD. The inequalities are
straightforward; they express the inconsistent nature of i by assigning it minimal
uncertainty, minimal belief and minimal trust.
We shall denote these models by M4(π, ρ, σ). They show some resemblance
with Belnap’s four-valued logic [2], but since the operators
+
and
·
are
diﬀerent from logical conjunction and disjunction, M4(π, ρ, σ) is not isomorphic
to Belnap’s logic.
+ b d u i
b b i b i
d i d d i
u b d u i
i i i i i
· b d u i
b b d u i
d u u u u
u u u u u
i π ρ u σ
x x
b d
d b
u u
i i
u =b d =b i ≤b b
u =d b =d i ≤d d
b =u d =u i ≤u u
Fig. 6. The four-element models M4(π, ρ, σ)
Using tool support to exhaustively verify all possible instantiations of π, ρ and
σ, we found six diﬀerent models satisfying all axioms: M4(u, u, u), M4(i, u, u),
M4(i, u, i), M4(b, d, i), M4(i, d, b), and M4(i, d, i). The last three models, how-
ever, require a simpliﬁed deﬁnition of the total order, viz. one in which all ele-
ments are equivalent (e.g. u =b d =b i =b b). The model M4(b, d, i) is isomorphic

An Algebra for Trust Dilution and Trust Fusion
13
to a model deﬁned by Gutscher [7]. The variety of models implies that there
are several diﬀerent ways to interpret the proliferation of inconsistencies in a
dilution context.
Theorem 2. M4(u, u, u), M4(i, u, u), and M4(i, u, i) are models of BFD+ TO+
INV+ RM.
Proof. We will sketch the proof for M4(i, u, i). Axioms B1, C1–C7, T1–T9, I1,
and I3–I6 follow easily by inspecting the tables. For instance, B1 follows from the
symmetry of the table for +. Axiom B2 x + (y + z) = (x + y) + z follows from a
simple case distinction. If any of x, y or z equals i or u, then associativity clearly
holds. Next if x, y and z are all b or all d, then associativity is also simple. Finally,
if there is at least one b and at least one d and no u, then the associativity holds
because the outcome is always i. The veriﬁcation of axiom B3 (associativity of
dilution) follows in a similar way, but needs some more case distinctions. Axiom
T10 x + y ≤u x is true because every element of a row in the + table is ≤u-
dominated by the left-hand argument. A similar check of the · table suﬃces to
verify axioms T11 and T12. Axiom I2 follows from a straightforward veriﬁcation
of all cases (ten cases, using symmetry of +). Axiom I7 clearly holds if x = u or
x = i because they are minimal w.r.t. ≤b and ≤d. If x = b or x = d it follows
from the duality of b and d. Axioms R1-R4 follow by simple inspection of the ·
table.
⊓⊔
4.3
Subjective Logic
Subjective Logic [9] is a framework to compute the trust between two agents in
a trust network. In its simplest form, a trust value is represented by a triplet
(b, d, u), representing belief, disbelief, and uncertainty, respectively. These values
satisfy b, d, u ∈[0, 1] and b + d + u = 1. Each such triplet can be represented as
a point in a triangle.
Coordinate b of point p = (b, d, u) (see
Disbelief
Belief
p
B
1
U
Uncertainty
0
D
1
1
0
0
b
d
u
Fig. 7. The SL triangle
Fig. 7) determines the (perpendicular) dis-
tance between p and side DU. Likewise, d
determines the distance between p and side
BU, and u the distance between p and BD.
Some examples: point B has coordinates (1,
0, 0) and represents full belief, the middle
point between B and D is ( 1
2, 1
2, 0) and rep-
resents the fully certain opinion that there is
as much belief as disbelief in the trustee. In
this framework, the fusion and dilution operators are called consensus (notation
⊕) and conjunction (notation ⊗). Reformulated in our notation, the operators
are deﬁned as follows.
(b, d, u) ⊕(b′, d′, u′) =

bu′ + b′u
u + u′ −uu′ ,
du′ + d′u
u + u′ −uu′ ,
uu′
u + u′ −uu′

(b, d, u) ⊗(b′, d′, u′) = (bb′, bd′, d + u + bu′)

14
B. Alcalde and S. Mauw
The fusion operator is undeﬁned if and only if u = u′ = 0 (assuming u, u′ ∈
[0, 1]). This indicates that the fusion of two dogmatic opinions (e.g. (1, 0, 0) ⊕
(0, 1, 0)) is not straightforward. In order for the Subjective Logic to serve as a
model of our algebra, the deﬁnition of the fusion operator must be extended. We
investigate several extensions in the following sections.
The Model SLγ. Recent versions of the Subjective Logic [13,10] use a limit
construction to deﬁne fusion for u = u′ = 0.
(b, d, 0) ⊕(b′, d′, 0) =
 γb + b′
γ + 1 , γd + d′
γ + 1 , 0

According to [13], γ is deﬁned by γ =
lim
u,u′→0

u′
u

. It expresses the relative
dogmatism between the expressions (b, d, 0) and (b′, d′, 0) (or rather, between
the agents expressing these dogmatic opinions). The higher the value of γ, the
higher the relative weight of opinion (b, d, 0) in a fusion with (b′, d′, 0). The
“default value” of γ is 1, meaning that in general dogmatic values are averaged.
We shall denote this model by SLγ.
In [10] it is stated that “in case of dogmatic opinions the associativity of the
consensus operator does not emerge directly”. Indeed, taking γ = 1, we can
use associativity to derive ( 1
2, 1
2, 0) = (1, 0, 0) ⊕(0, 1, 0) = (1, 0, 0) ⊕((0, 1, 0) ⊕
(0, 1, 0)) = ((1, 0, 0) ⊕(0, 1, 0)) ⊕(0, 1, 0) = ( 1
2, 1
2, 0) ⊕(0, 1, 0) = ( 1
4, 3
4, 0). This
gives the following negative result.
Theorem 3. SLγ is not a model of the algebra BFD.
As a possible solution, Jøsang et al. [10] introduce an algorithm which can be
used to calculate the fusion of three or more dogmatic beliefs. Expressions like
((1, 0, 0)⊕(0, 1, 0))⊕(0, 1, 0) are interpreted by applying a ternary fusion operator
to these three arguments. This approach consists in fact of the introduction of
a collection of n-ary fusion operators, each of which is still not associative. An
additional problem, overlooked by the authors, is the fact that expressions must
ﬁrst undergo some kind of normalization procedure before their algorithm can
be applied correctly. For instance, the expression (0, 1, 0)⊕((1, 0, 0)⊗(( 1
4, 3
4, 0)⊕
( 3
4, 1
4, 0))) must ﬁrst be reduced to (0, 1, 0) ⊕

( 1
4, 3
4, 0) ⊕( 3
4, 1
4, 0)

and not to
(0, 1, 0) ⊕

(1, 0, 0) ⊗( 1
2, 1
2, 0)

. The former expression equals ( 4
12, 8
12, 0), while
the latter equals ( 1
4, 3
4, 0). This normalization is not a simple innermost-ﬁrst
rewriting, but must make use of the property (1, 0, 0) ⊕x = x, which coincides
with axiom C5 of our algebra.
We conclude by stating that, even though the authors claim associativity of
their logic, this is not the case for any of the interpretations they give. Hence,
SLγ is not a model of BFD.
The Model SLc. Based on the idea of taking the average of conﬂicting dog-
matic beliefs, we deﬁne the extension SLc. Elements of this model are four-
tuples (b, d, u, c), where b, d, and u play their usual role and c counts the

An Algebra for Trust Dilution and Trust Fusion
15
(T13) β + β =bdu β
(T14) δ + δ =bdu δ
Fig. 8. Weakening axioms C4 and C6
weight of a dogmatic opinion. Thus, we have the following set of trust atoms
A = {(b, d, u, c) | b, d, u ∈[0, 1] ∧c ∈N+ ∧b + d + u = 1 ∧(u = 0 ∨c = 1)}. The
last condition means that only opinions with uncertainty equal to zero can have
a counter diﬀerent from 1. Fusion and dilution are deﬁned as follows.
(b, d, u, c) ⊕(b′, d′, u′, c′) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩

bc+b′c′
c+c′
, dc+d′c′
c+c′
, 0, c + c′
if u = u′ = 0
(b, d, u, c)
if u = 0, u′ ̸= 0
(b′, d′, u′, c′)
if u ̸= 0, u′ = 0

bu′+b′u
u+u′−uu′ ,
du′+d′u
u+u′−uu′ ,
uu′
u+u′−uu′ , 1

else
(b, d, u, c) ⊗(b′, d′, u′, c′) =

(b′, d′, u′, c′)
if (b, d, u, c) = (1, 0, 0, c)
( bb′, bd′, d + u + bu′ , 1)
else
The ﬁrst case of the fusion deﬁnition clariﬁes the role of the counter. If two dog-
matic views are combined, then the resulting belief is calculated as the weighted
average of the individual beliefs. The counter of the resulting trust value is the
sum of the counters of the individual trust values. The other three cases are
straightforward extensions of SL. For the dilution operator we treat the case
where the left operand equals (1, 0, 0, c) diﬀerently. The reason is that in this
case the resulting value must inherit the counter value of the right operand. This
is motivated by the fact that (1, 0, 0, c) acts as a left-unit for dilution (cf. axiom
C5 and the discussion on this axiom in the previous section).
We interpret the constants as follows: β →(1, 0, 0, 1), δ →(0, 1, 0, 1), υ →
(0, 0, 1, 1). The inverse operator is deﬁned by (b, d, u, c) = (d, b, u, c), and the total
orders by: (b, d, u, c) ≤b (b′, d′, u′, c′) ⇔b ≤b′, (b, d, u, c) ≤d (b′, d′, u′, c′) ⇔d ≤
d′, and (b, d, u, c) ≤u (b′, d′, u′, c′) ⇔u ≤u′.
This model, which we call SLc, satisﬁes all axioms except (C4) β +β = β and
(C6) δ + δ = δ. This is because the weight of β + β is higher than the weight of
β: (1, 0, 0, 1) ⊕(1, 0, 0, 1) = (1, 0, 0, 2).
However, SLc satisﬁes two weaker axioms T13 and T14 (see Fig. 8). We use
x=bdu y as a shorthand notation for x =b y ∧x =d y ∧x =u y. It easily follows
that axiom C4 implies T13 and that C6 implies T14. If we denote by BFD−the
axiom system BFD minus axioms C4 and C6, we can formulate the following
theorem.
Theorem 4. SLc is a model of the algebra BFD−+TO+INV+RM+T13+T14.
Proof. Axiom B1 follows by observing the symmetry in the deﬁnition of the
fusion operator (e.g. bc+b′c′
c+c′
= b′c′+bc
c′+c ). The proof of axiom B2 consists of a case
distinction and a number of straightforward calculations. As an illustration, we
show the calculation for the ﬁrst component b+
1 of ((b, d, u, c) ⊕(b′, d′, u′, c′)) ⊕
(b′′, d′′, u′′, c′′) if u ̸= 0, u′ ̸= 0.

16
B. Alcalde and S. Mauw
b+
1 =
bu′+b′u
u+u′−uu′ u′′ + b′′
uu′
u+u′−uu′
uu′
u+u′−uu′ + u′′ −
uu′
u+u′−uu′ u′′ =
(bu′ + b′u)u′′ + b′′uu′
uu′ + u′′(u + u′ −uu′) −uu′u′′
The ﬁrst component b+
2 of (b, d, u) ⊕((b′, d′, u′) ⊕(b′′, d′′, u′′)) is
b+
2 =
b
u′u′′
u′+u′′−u′u′′ +
b′u′′+b′′u′
u′+u′′−u′u′′ u
u +
u′u′′
u′+u′′−u′u′′ −u
u′u′′
u′+u′′−u′u′′
=
bu′u′′ + (b′u′′ + b′′u′)u
u(u′ + u′′ −u′u′′) + u′u′′ −uu′u′′
It is easy to check that the resulting expressions are equal.
For axiom B3 (associativity of dilution) the most complex case is equality of
the third component. For instance, if (b, d, u) ̸= (1, 0, 0), (b′, d′, u′) ̸= (1, 0, 0),
and (b′′, d′′, u′′) ̸= (1, 0, 0), the third component of (b, d, u, c) ⊗((b′, d′, u′, c′) ⊗
(b′′, d′′, u′′, c′′)) is d + u + b(d′ + u′ + b′u′′). This is equal to the third component
of ((b, d, u, c)⊗(b′, d′, u′, c′))⊗(b′′, d′′, u′′, c′′), which is bd′ +(d+u+bu′)+bb′u′′.
Axioms C1, C2, C3, C5, C7 and T1–T9 can be veriﬁed easily. For axiom
T10 we have to consider four cases, three of which are trivial. The fourth case
(u ̸= 0, u′ ̸= 0) is treated as follows:
uu′
u+u′−uu′ ≤u ⇔
u′
u+u′−uu′ ≤1 ⇔u′ ≤
u + u′ −uu′ ⇔0 ≤u −uu′, which is true for u, u′ ∈[0, 1]. The most interesting
case for axiom T11 is (b, d, u) ̸= (1, 0, 0). We then have u ≤d + u + bu′, which
holds for u, b, d, u′ ∈[0, 1]. Likewise, axiom T12 follows from u′ = (d+u+b)u′ =
du′ +uu′ +bu′ ≤d+u+bu′. The remaining axioms T13, T14, I1–I7, and R1–R4
are trivial.
⊓⊔
The Model SLi. Finally, we will construct a model by extending SL with
a constant for inconsistency (as for the M4-models in Section 4.2). We deﬁne
A = {(b, d, u) | b, d, u ∈[0, 1] ∧b + d + u = 1} ∪{i}. The fusion and dilution
operators are a merger of their deﬁnitions in SL and M4(i, u, i).
i ⊕(b, d, u) = (b, d, u) ⊕i = i ⊕i = i
(b, d, 0) ⊕(b, d, 0) = (b, d, 0)
(b, d, 0) ⊕(b′, d′, 0) = i
if b ̸= b′
(b, d, u) ⊕(b′, d′, u′) = (
bu′+b′u
u+u′−uu′ ,
du′+d′u
u+u′−uu′ ,
uu′
u+u′−uu′ )
if u ̸= 0 ∨u′ ̸= 0
i ⊗(1, 0, 0) = (1, 0, 0) ⊗i = i ⊗i = i
(b, d, u) ⊗i = i ⊗(b, d, u) = (0, 0, 1)
if (b, d, u)̸=(1, 0, 0)
(b, d, u) ⊗(b′, d′, u′) = ( bb′, bd′, d + u + bu′)
We interpret the constants as follows: β →(1, 0, 0), δ →(0, 1, 0), υ →(0, 0, 1).
The inverse operator is deﬁned by (b, d, u) = (d, b, u) and i = i. The total orders
are given by:
(b, d, u) ≤b (b′, d′, u′) ⇔b ≤b′
i ≤b (b, d, u)
(b, d, u) ≤d (b′, d′, u′) ⇔d ≤d′
i ≤d (b, d, u)
(b, d, u) ≤u (b′, d′, u′) ⇔u ≤u′
i ≤u (b, d, u)
This model, which we call SLi, satisﬁes all axioms. The proof follows the same
line of reasoning as the proof of Theorem 4.

An Algebra for Trust Dilution and Trust Fusion
17
Theorem 5. SLi is a model of the algebra BFD+TO+INV+RM.
All results of the current section are gathered in the table displayed in Fig. 9.
The result for M4 holds only for certain values of the parameters and the result
for SLc only for BFD−, T13, and T14.
BFD TO INV RM
M3(σ)
✓
×
×
×
M4(π, ρ, σ) and [7]
✓
✓
✓
✓
BFD TO INV RM
SL
(incomplete model)
SLγ
×
×
×
×
SLc BFD−✓
✓
✓
SLi
✓
✓
✓
✓
Fig. 9. Results of the evaluation of the models (✓means satisﬁed)
5
Related Work
There are many existing models that propose ways to combine trust values or
more widely to combine opinions. In the simplest models, the trust values are
discrete or continuous values on a given interval (implying at least two elements
in the model, i.e. a bottom and a top element). This is the case for instance in
PGP [17].
Other models are taking the uncertainty into account, such as Subjective Logic
[9], Dempster-Shafer [14], or Yager [15] to name only a few. The uncertainty
level adds another dimension to the trust metrics and can proﬁtably be used
in order to compute more accurate trust values. Subjective Logic was extended
several times, e.g. with a limit construction enabling the fusion of two fully
certain opinions [10], with an algorithm enabling the commutativity of the fusion
operator [10], with diﬀerent operators deﬁnitions depending on the (partial)
dependence of the trust values [13].
Nevertheless, the combination of trust values in 3-elements models can also
lead to further questions. For instance, this raises the question on the dogmatic
belief composition [16]. The extension to a 4-element model such as Gutscher’s
[7] (or based on Belnap’s [2] or Bergstra’s [3] theories) seems to answer this issue
partially.
We noticed that the available models were developed in a bottom-up fashion,
i.e. starting from the model and showing which properties it satisﬁes or not. To
our knowledge, in the domain of trust, the development of a top-down approach
such as the algebra proposed in the current paper, is novel. This algebra takes
the fusion and dilution operators as a starting point since these are the common
point of all the available models with only diﬀerences in their naming (fusion
and dilution can respectively be referred to as consensus and recommendation in
some models). The developed algebra focuses speciﬁcally on the trust application
domain and all axioms of the algebra are motivated in this context. For this
reason, the algebra may or may not make sense for other domains.

18
B. Alcalde and S. Mauw
6
Conclusion
Taking the Subjective Logic as a starting point, we developed an abstract al-
gebra expressing the basic properties of trust fusion and trust dilution. To the
core of this algebra belong the three absolute trust values β, δ, and υ. In a
modular way, we extended this core algebra with some auxiliary operators to
capture more properties of the operators involved. Since there are diﬀerent ways
to fuse dogmatic beliefs (such as considering β + δ as an inconsistency), we de-
cided to not enforce one particular choice in the algebra. As a consequence, the
algebra is not complete for any of the models studied. This is also reﬂected in
the fact that the initial algebra (which we did not study in this paper) is not
particularly interesting. An interesting next step would be to extend the algebra
with additional properties (and possibly operators) that more precisely capture
certain interpretations of the fusion of dogmatic beliefs, as to develop complete
axiomatizations.
We studied two types of models of this algebra: canonical models with only a
few elements, and models based on SL with an inﬁnite number of elements. Partly
to our surprise, there is no three-element model of the full algebra, indicating
that the expression β + δ necessarily must be interpreted by a special fourth
element.
As expected, SL with a partially deﬁned fusion operator cannot be considered
a model. This also applies to the SL model extended with a limit construction
presented in [13], in contradiction with its (unproven) claim of associativity.
More surprising is that the extension of SL with a limit construction is not a
model because it lacks associativity of fusion. This contradicts the (unproven)
claim of associativity in [13]. The algorithmic approach to associativity of fusion
proposed in [10] does not imply associativity of the (binary) fusion operator
either. In fact, while verifying the axioms of our algebra, it turned out that the
reduction of terms according to axiom C5 is an essential, but omitted, step for
the algorithm to work correctly. If this reduction step is not performed before
evaluating a trust expression, then the algorithm does not take all +-related
terms into account and gives the wrong result.
In order to overcome these problems, we experimented with two extensions
of SL. The ﬁrst extension tries to achieve the same results as SL with a limit
construction by introducing a weight for dogmatic opinions. Due to this weight,
which can be any positive natural number, the collection of possible interpre-
tations of β becomes inﬁnite and has no maximum element. As a consequence,
axiom C4 which states that β is maximum, becomes invalid. This extension of
SL satisﬁes a slightly weaker algebra. The second extension of SL concerns the
introduction of a special element expressing inconsistency. This is a model of the
full algebra.
In addition to this, the validation of the axioms of our algebra for SL also
gives more insight in the properties that SL satisﬁes. Whereas e.g. associativity
and commutativity have been discussed in detail by Jøsang et al., properties as
expressed in e.g. T10–T12 have not been mentioned explicitly.

An Algebra for Trust Dilution and Trust Fusion
19
The proofs presented here mostly consist of a number of straightforward case
distinctions. Rather than in the advanced level of the proofs, the complexity of
our work lies in the design. A slight modiﬁcation of the deﬁnition of e.g. M4 or
SLi will already invalidate essential properties like associativity.
An important next step is to validate other extensions of SL that were pro-
posed in literature and to model other ways to deal with dogmatic beliefs. It is
also interesting to look at more practical models, such as the model underlying
PGP.
A particularly interesting model to investigate is the model of trust graphs
(or transitive trust networks [11]). An open question is the reduction of such
networks. Because not every trust graph can be represented as a trust expression,
our theory has to be extended (e.g. with the notion of recursive equations) to
deal with trust graphs.
Finally, we mention that our model does not consider dynamic aspects, such
as the possible decay of trust or the occurrence of events that inﬂuence opinions.
Extending our algebra in this direction would also be an interesting topic for
future research.
Acknowledgment. This work as been partially funded by the Fonds National de
la Recherche (Luxembourg), grant number TR-PDR BFR08-038.
References
1. Alcalde, B., Dubois, E., Mauw, S., Mayer, N., Radomirovi´c, S.: Towards a deci-
sion model based on trust and security risk management. In: AISC 2009, vol. 98,
Australian Computer Society (2009)
2. Belnap, N.D.: A useful four-valued logic. In: Epstein, G., Dunn, J. (eds.) Modern
uses of multiple valued logics, pp. 8–37. Reidel, Dordrecht (1977)
3. Bergstra, J.A., Bethke, I., Rodenburg, P.: A propositional logic with 4 values: true,
false, divergent and meaningless. Journal of Applied Non-Classical Logics 5(2)
(1995)
4. Dong, C., Russello, G., Dulay, N.: Trust transfer in distributed systems. In: Trust
Management, number 238/2007 in IFIP, pp. 17–30. Springer, Heidelberg (2007)
5. Gambetta, D. (ed.): Trust: Making and breaking cooperative relations. Department
of Sociology. University of Oxford, Oxford (1988)
6. Gray, E., Seigneur, J.-M., Chen, Y., Jensen, C.: Trust propagation in small worlds.
In: Nixon, P., Terzis, S. (eds.) iTrust 2003. LNCS, vol. 2692, pp. 239–254. Springer,
Heidelberg (2003)
7. Gutscher, A.: Reasoning with uncertain and conﬂicting opinions in open reputation
systems. In: STM 2008, Trondheim, Norway (2008)
8. Jøsang, A.: An algebra for assessing trust in certiﬁcation chains. In: Proceedings
of the Network and Distributed Systems Security, NDSS (1999)
9. Jøsang, A.: A logic for uncertain probabilities. Int. J. Uncertain. Fuzziness Knowl.-
Based Syst. 9(3), 279–311 (2001)
10. Jøsang, A., Daniel, M., Vannoorenberghe, P.: Strategies for combining conﬂicting
dogmatic beliefs. In: Proceedings of the 6th International Conference on Informa-
tion Fusion, pp. 1133–1140 (2003)

20
B. Alcalde and S. Mauw
11. Jøsang, A., Gray, E., Kinateder, M.: Simpliﬁcation and analysis of transitive trust
networks. Web Intelli. and Agent Sys. 4(2), 139–161 (2006)
12. Jøsang, A., Kinateder, M.: Analysing topologies of transitive trust. In: Workshop
of Formal Aspects of Security and Trust (FAST), pp. 9–22 (2003)
13. Jøsang, A., Marsh, S., Pope, S.: Exploring diﬀerent types of trust propagation.
In: Stølen, K., Winsborough, W.H., Martinelli, F., Massacci, F. (eds.) iTrust 2006.
LNCS, vol. 3986, pp. 179–192. Springer, Heidelberg (2006)
14. Shafer, G.: A Mathematical Theory of Evidence. Princeton Univ. Press, Princeton
(1976)
15. Yager, R.R.: On the Dempster-Shafer framework and new combination rules. In-
formation Sciences 4, 93–137 (1987)
16. Zadeh, L.A.: Review of mathematical theory of evidence by Glenn Shafer. AI Mag-
azine 5(3), 81–83 (1984)
17. Zimmermann, P.R.: The Oﬃcial PGP User’s Guide. MIT Press, Cambridge (1995)

HMM-Based Trust Model
Ehab ElSalamouny1, Vladimiro Sassone1, and Mogens Nielsen2
1 ECS, University of Southampton, UK
2 University of Aarhus, Denmark
Abstract. Probabilistic trust has been adopted as an approach to taking security
sensitive decisions in modern global computing environments. Existing proba-
bilistic trust frameworks either assume ﬁxed behaviour for the principals or in-
corporate the notion of ‘decay’ as an ad hoc approach to cope with their dynamic
behaviour. Using Hidden Markov Models (HMMs) for both modelling and ap-
proximating the behaviours of principals, we introduce the HMM-based trust
model as a new approach to evaluating trust in systems exhibiting dynamic be-
haviour. This model avoids the ﬁxed behaviour assumption which is considered
the major limitation of existing Beta trust model. We show the consistency of the
HMM-based trust model and contrast it against the well known Beta trust model
with the decay principle in terms of the estimation precision.
1
Introduction
In modern open network systems where principals can autonomously enter and leave
the environment at any time, and generally in a global computing environment, any
particular principal has incomplete information about other principals currently in the
same environment. In such an environment, interactions of a principal A with other
principals are not assumed to be at the same level of satisfaction, or even safety, to A.
One approach of taking security sensitive decisions in a global computing environment
regarding interactions with principals is to adopt the notion of probabilistic trust, which
can broadly be characterised as aiming to build probabilistic models upon which to
base predictions about principals’ future behaviours. Using these models, the trust of a
principal A in another principal B is the probability distribution, estimated by A, over
outcomes of the next interaction with B. Here the estimation process is based on the
history of interactions h with the principal B. This notion of trust ensembles the trusting
relationship between humans as seen by Gambetta [8].
In many existing frameworks the so-called Beta model [12] is adopted. This is a
static model in the precise sense that the behaviour of any principal is assumed to be
representable by a ﬁxed probability distribution over outcomes, invariantly in time. That
is each principal p is associated with a ﬁxed real number 0 ≤Θp ≤1 indicating the
assumption that an interaction involving p yields success with probability Θp. Using
this assumption, the Beta model for trust is based on applying Bayesian data analysis
(see e.g. [20]) to the history of interactions h with a given principal p to estimate the
probability Θp that an interaction with p yields success. In this framework the family of
beta probability density functions (pdfs) is used, as a conjugate prior, together with the
data h to derive a posterior beta probability density function for Θp. Full explanation
can be found in [12,19].
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 21–35, 2010.
c⃝Springer-Verlag Berlin Heidelberg 2010

22
E. ElSalamouny, V. Sassone, and M. Nielsen
There are several examples in the literature where the Beta model is used, either
implicitly or explicitly, including Jøsang and Ismail’s Beta reputation system [12], the
systems of Mui et al. [15] and of Buchegger [4], the Dirichlet reputation systems [11],
TRAVOS [21], and the SECURE trust model [5]. Recently, the Beta model and its
extension to interactions with multiple outcomes (the Dirichlet model) have been used
to provide a ﬁrst formal framework for the analysis and comparison of computational
trust algorithms [19,16,13]. In practice, these systems have found space in diﬀerent
applications of trust, e.g., online auctioning, peer-to-peer ﬁlesharing, mobile ad-hoc
routing and online multiplayer gaming.
One limitation of current Beta based probabilistic systems is that they assume a ﬁxed
probabilistic behaviour for each principal; that is for each principal, there exists a ﬁxed
probability distribution over possible outcomes of its interactions. This assumption of
ﬁxed behaviour may not be realistic in many situations, where a principal possibly
changes its behaviour over time. Just consider, e.g., the example of an agent which can
autonomously switch between two internal states, a normal ‘on-service’ mode and a
‘do-not-disturb’ mode. This limitation of the Beta model systems has been recognised
by many researchers [12,4,22]. This is why several papers have used a ‘decay’ principle
to favour recent events over information about older ones [12]. The decay principle can
be implemented in many diﬀerent ways, e.g., by a using a ﬁnite ‘buﬀer’ to remember
only the most recent n events, or linear and exponential decay functions, where each
outcome in the given history is weighted according to the occurrence time (old out-
comes are given lower weights than newer ones). Whilst decay-based techniques have
proved useful in some applications, we have shown in [7] that the decay principal is
useful (for the purpose of estimating the predictive probability) only when the system
behaviour is highly stable, that is when it is very unlikely to change its behaviour.
Given the above limitations of existing probabilistic trust systems, we try to develop
a more general probabilistic trust framework which is able to cover cases where a prin-
cipal’s behaviour is dynamic. Following the probabilistic view of the behaviour, one
can represent the behaviour of a principal p at any time t by a particular state qt which
is characterised by a particular probability distribution over possible outcomes of an in-
teraction with p. If p exhibits a dynamic behaviour, it indeed transits between diﬀerent
states of behaviour. This suggests using a multiple state transition system to represent
the whole dynamic behaviour of a principal, where each state is deﬁned by a probability
distribution over observables. Since the deﬁnition of hidden Markov models (HMMs)
coincides with this description, we elect to use HMMs for modelling and approximating
the dynamic behaviour of principals.
Aiming at avoiding the assumption of ﬁxed behaviour in Beta systems, we introduce
the HMM-based trust as a more sophisticated trust model which is capable of capturing
the natural dynamism of real computing systems. Instead of modelling the behaviour
of a principal by a ﬁxed probability distribution representing one state of behaviour,
the behaviour of a principal p is approximated by a ﬁnite state HMM η, called the
approximate behaviour model. Then, given any sequence of outcomes of interactions
with p, the approximate model η is used to estimate the probability distribution over
the potential outcomes of the next interaction with p. We call the resulting probability
distribution the estimated predictive probability distribution of p under the approximate

HMM-Based Trust Model
23
model η. Following the existing notion of probabilistic trust, the estimated predictive
probability distribution represents the trust in the principal p.
In order to evaluate the quality of the HMM-based trust, we contrast its estimated
predictive probability distribution against the real predictive probability distribution
which depends on the real behaviour of the concerned principal. For this purpose, we
adopt the relative entropy measure [14,6]. Relying on this measure we evaluate the ex-
pected estimation error as a measure for the quality of the trust evaluation. Note that
this notion of estimation error has been used for comparison between trust algorithms
in other works. See for example [16,19].
Original contribution of the paper. In this paper we describe the basics of the HMM-
based trust model. Namely, the methods of obtaining the approximate behaviour model
η for a principal p, and also estimating the probability distribution over possible out-
comes of the next interaction with p using η. We show that maximising the probability
of the observations, using the Baum-Welch algorithm detailed in [2,18], minimises the
expected estimation error and therefore is a consistent method for obtaining the approx-
imate behaviour HMM η. For the sake of comparison to the traditional Beta trust model
with a decay factor, we use Monte-Carlo methods to evaluate the expected estimation
error in both cases.
Structure of the paper. The next section brieﬂy describes the Beta trust model and the
decay principle. Section 3 provides a basic and precise description for hidden markov
models. Subsequently, the basic model of HMM-based trust is described in Section 4.
Then it is formally shown in Section 5 that the maximum likelihood estimation, as the
basis of the HMM-based trust model, is adequate in the sense that it minimises the
expected relative entropy between the real and estimated predictive probability distri-
butions. Section 6 provides an experimental comparison between the HMM-based trust
model and the well known Beta trust model with decay factor. Finally we conclude our
results in section 7.
2
Beta Model with a Decay Factor
In the Beta trust model introduced by [12] an interaction with any principal yields either
success s or failure f. It is also based on the assumption that any interaction with a given
principal p yields success with a ﬁxed probability θp. Under this assumption a sequence
of ℓoutcomes hℓ= o0 · · · oℓ−1 is a sequence of Bernoulli trials, and the number of
successful outcomes in hℓis probabilistically distributed by a binomial distribution. The
objective of the Beta trust model is then to estimate the parameter θp given a historical
sequence of outcomes hℓ.
Using the Bayesian data analysis (see e.g. [20]), θp is seen as a random variable
whose prior (initial) probability density function (pdf) is updated to a posterior pdf
using given observations. With the fact that the beta pdf is a conjugate prior to the
binomial distribution, the posterior pdf of θ given the sequence h is also a beta pdf. The
Beta trust model then gives an estimate for θp as the expected value of its posterior beta
pdf. This estimate, denoted by B (s | h), is related to the sequence hℓas follows.

24
E. ElSalamouny, V. Sassone, and M. Nielsen
B (s | hℓ) =
#s (hℓ) + 1
#s (hℓ) + #f (hℓ) + 2
(1)
where #s (hℓ) and #f (hℓ) are the numbers of successful and unsuccessful interactions in
hℓrespectively.
In order to cope with the cases where the behaviour of a principal is dynamic, the
notion of exponential decay (or forgetting) has been incorporated in the Beta trust
model [12]. The intuitive idea is to capture the most recent behaviour of the principal
by favouring the recent outcomes over old ones. This is performed by associating each
outcome oi in hℓwith an exponential weight rℓ−i−1, where 0 ≤r ≤1 is called the de-
cay (forgetting) factor. Observe that recent outcomes are associated with higher weights
than older outcomes. With the decay factor r, the Beta estimate for the distribution over
{s, f} is denoted by Br (. | hℓ), and given by the following equations.
Br(s | hℓ) =
mr(hℓ) + 1
mr(hℓ) + nr(hℓ) + 2
,
Br(f | hℓ) =
nr(hℓ) + 1
mr(hℓ) + nr(hℓ) + 2
(2)
and
mr(hℓ) =
ℓ−1

i=0
rℓ−i−1δi(s)
nr(hℓ) =
ℓ−1

i=0
rℓ−i−1δi(f)
(3)
for
δi(X) =
1
if oi = X
0
otherwise
(4)
Note that incorporating the decay principle in the Beta trust model is implemented by
replacing the counts #s (hℓ) and #f (hℓ) in Equation (1) by the sum of weights associated
with the past outcomes. Although this approach has been used in many works, we have
shown in [7] that it is not eﬀective when the principal’s behaviour is highly dynamic;
that is when the system tends to change its state of behaviour, characterised by the
exhibited probability distribution over possible outcomes. Another major limitation of
this approach is that it appears hard to formally determine the optimal value for the
decay factor from only observations.
3
Hidden Markov Models (HMMs)
A Hidden Markov Model (HMM) [1] is a well-established probabilistic model essen-
tially based on a notion of system state. Underlying any HMM there is a Markov chain
modelling (probabilistically) the system’s transitions between a set of internal states.
Each state in this chain is associated with a particular probability distribution over the
set of possible outcomes (observations). The output of an HMM is a sequence of out-
comes where each outcome is sampled according to the probability distribution of the
underlying state. In the following, we denote the state of the HMM and the observation
at time t by qt and ot respectively.
Deﬁnition 1 (hidden Markov model). A (discrete) hidden Markov model (HMM) is
a tuple λ = (Q, π, A, O, B) where Q is a ﬁnite set of states; π is a distribution on Q,

HMM-Based Trust Model
25
the initial distribution; A : Q × Q →[0, 1] is the state transition matrix, with Ai j =
P (qt+1 = j | qt = i) and 
j∈Q Ai j = 1; O is a ﬁnite set of possible observations; and
B : Q × O →[0, 1] is the observation probability matrix, with Bik = P (ot = k | qt = i),

k∈O Bik = 1.
HMMs provide the computational trust community with several obvious advantages:
they are widely used in scientiﬁc applications, and come equipped with eﬃcient algo-
rithms for computing the probabilities of events and for parameter estimation (cf. [18]),
the chief problem for probabilistic trust management. It is worth noticing that an HMM
is a generalisation of the Beta model. Indeed, in the context of computational trust, rep-
resenting the behaviour of a principal p by a HMM λp provides a diﬀerent distribution
B j over O for each possible state j of p. In particular, the states of λp can be seen as
a collection of independent Beta models, the transitions between which are governed
by the Markov chain formed by π and A, as principal p switches its internal state. Ac-
cording to the above deﬁnition of HMM, the probability of a sequence of outcomes
h = o1 o2 · · · on given a HMM λ is given by the following equation.
P(h | λ) =

q1,...,qn∈Q
π(q1) · Bq1o1 · Aq1q2 · Bq2o2 · · · Aqn−1qn · Bqnon
The above probability is evaluated eﬃciently by an algorithm called the forward-
backward algorithm. One instance of this algorithm, called the forwardinstance, is based
on inductively (on time t) evaluating the forward variable αt(j)= P(o1 o2 · · · ot, qt = j |
λ), that is the joint probability that the partial sequence o1 o2 · · ·ot is observed and the
state at time t is j. The required probability P(h | λ) is then obtained by
P(h | λ) =

j∈Q
αn(j)
Alternatively, P(h | λ) can be obtained using the backward instance of the algorithm,
where the backward variable βt(j) = P(ot+1 ot+2 · · · on, | qt = j, λ) is inductively (on
time t) evaluated. More details on these instances of the forward-backward algorithm
can be found in [18].
Another major problem of HMMs is to ﬁnd the model λ which maximises the above
probability of a sequence h. This problem has been addressed by Baum and his col-
leagues whose eﬀorts resulted in the Baum-Welch algorithm [2,18]. This algorithm
1
0.1
 2
0.12

π1 = 0.5
B(1, s) = 0.95
B(1,f) = 0.05
O = {s, f}
π2 = 0.5
B(2, s) = 0.05
B(2, f) = 0.95
Fig. 1. Example Hidden Markov Model

26
E. ElSalamouny, V. Sassone, and M. Nielsen
iteratively estimates the parameters of an HMM λ which maximises the probability of
a given sequence of outcomes h. One limitation of this algorithm is that it ﬁnds a local
maxima in the model space rather than the global one.
Example 1. Figure 1 shows a two-state HMM over the observation set {s, f}. Both states
are relatively stable. That is the probability of making both transitions 1 →2 and 2 →1
are relatively small (0.1,0.12 respectively). Also at state 1, it is very likely to observe s
(with probability 0.95), whereas at state 2 it is very likely to observe f (with probability
0.95). This HMM describes the behaviour of a stable principal whose internal state is
unlikely to change.
In the area of trust, we remark that Markovian models have also been used in [10] to
model the evolution of trust in the users of collaborative information systems. However,
in our work, HMMs model the principal’s behaviour upon which trust is computed.
4
HMM-Based Trust Model
As described in the introduction, the HMM-based trust relies on approximating the
behaviour of any given principal by a ﬁnite-state HMM η called the approximate be-
haviour model. The approximate behaviour model is then used to estimate the predictive
probability distribution. In order to precisely deﬁne this model, it is required to deﬁne a
method for computing the approximate behaviour model η, and also for estimating the
predictive probability distribution using η. As a general notation which will be used in
these deﬁnitions we will write the probability of any random variable ζ, under a given
probabilistic model R, as P (ζ | R).
For computing η, the maximum likelihood criterion is adopted as follows. Let y =
y0 y2 · · · yℓ−1 be an observed sequence of outcomes of interactions with a given princi-
pal, where ℓis an arbitrary length. Let also Rn denote any n-state HMM. Then, using
the sequence y, the n-state approximate behaviour model η is obtained by the following
equation.
η = argmax
Rn
P (hℓ= y | Rn)
(5)
That is η is the n-state HMM under which the probability of the given history y is max-
imised. The HMM model η can be therefore obtained by the Baum-Welch algorithm
which is described brieﬂy in Section 3 and detailed in [2,18].
Now we address the problem of estimating the predictive probability distribution
given a particular sequence of outcomes. Let hℓ= o0 o1 · · · oℓ−1 be a random variable
representing any sequence of observed outcomes of interaction with the principal p,
where o0 and oℓ−1 represent respectively the least and the most recent outcomes, and
ℓis an arbitrary length. Extending this notation to future outcomes, the outcome of
the next interaction with p is denoted by oℓ. Note that each outcome oi is therefore a
random variable representing the outcome at time i. Let also O = {1, 2, . . ., κ} be the
alphabet of each single outcome. Using the n-state approximate behaviour HMM η de-
ﬁned by Equation (5), the estimated predictive probability distribution given a particular
sequence of outcomes w is denoted by Hη(. | w) and deﬁned by the following equation.

HMM-Based Trust Model
27
Hη (z | w) = P (oℓ= z | hℓ= w, η) = P (hℓ= w, oℓ= z | η)
P (hℓ= w | η)
(6)
where z ∈O. The above probabilities are eﬃciently evaluated by the forward-backward
algorithm brieﬂy described in Section 3, and detailed in [18].
5
Consistency of Maximum Likelihood Estimation
Like other existing probabilistic trust models, the objective of the HMM-based trust
model is to estimate the predictive probability distribution for a given principal p, that
is the probability of each possible outcome in the next interaction with p. Therefore it
is a fundamental requirement that the approximate behaviour model η computed for p
is chosen such that the error of such an estimation is minimised.
To analyse this error, we need to model the real behaviour of the principal p. This
allows expressing the real predictive probability distribution of p. The estimation error
can be therefore evaluated as the diﬀerence between the real and estimated predictive
probability distributions. In this section it is shown that the maximum likelihood crite-
rion, deﬁned by Equation (5) for choosing the approximate behaviour model provides
a consistent method to minimise the estimation error.
5.1
Modelling the Real System
In this work we are interested in studying systems which exhibit a dynamic behaviour,
that is changing their behaviour over time. We mathematically model the behaviour of
the system at any time by a particular probability distribution over possible outcomes.
A system p with a dynamic behaviour can be therefore modelled by a multiple state
transition system where each state exhibits a particular behaviour (probability distribu-
tion). This naturally leads to choosing a generic Hidden Markov Model (HMM) λ as
the real model of p’s behaviour.
Here the state of a system real model λ at the time of observing oi is denoted by the
random variable qi. Thus, given that the current underlying state is x, i.e. qℓ−1 = x, we
can compute the real predictive probability distribution, denoted by P (. | x, λ), that is
the probability of each possible next observation, z ∈O, using the following equation.
P (z | x, λ) = P (oℓ= z | qℓ−1 = x, λ)
=

y∈Qλ
P (qℓ= y | qℓ−1 = x, λ) P (oℓ= z | qℓ= y, λ)
=

y∈Qλ
(Aλ)xy (Bλ)yz
(7)
where Qλ, Aλ, and Bλ are respectively the set of states, the state transition matrix, and
the observation probability matrix of λ. We shall also work under the hypothesis that
λ is ergodic. This corresponds to demanding that the Markov chain underlying λ is
irreducible and aperiodic (more details on these properties can be found in [9,17,3]).

28
E. ElSalamouny, V. Sassone, and M. Nielsen
5.2
The Estimation Error
In this paper the relative entropy measure [6] is used for evaluating the diﬀerence be-
tween the real and estimated predictive probability distributions, given by Equations (7)
and (6) respectively. Namely, given a sequence of outcomes hℓ= w and the current state
qℓ−1 = x, this diﬀerence measure is written as follows.
D

P (. | x, λ) || Hη (. | w)

=

z∈O
P (z | x, λ) log
P (z | x, λ)
Hη (z | w)

(8)
The above diﬀerence can be seen as the estimation error given a particular current
state qℓ−1 of λ, and the sequence of outcomes hℓ. Hence we deﬁne the expected esti-
mation error as the expected relative entropy between the real and estimated predictive
probability distributions, where the expectation is evaluated on the underlying random
variables qℓ−1 and hℓ. This error is denoted by Errorℓ

λ, Hη

. Thus,
Errorℓ

λ, Hη

= E
	
D

P (. | qℓ−1, λ) || Hη (. | hℓ)

(9)
Now we formally show that choosing the approximate behaviour model η by maximis-
ing the likelihood of a given suﬃciently long sequence y (by Equation (5)) minimises
the expected estimation error.
Equation (9) can be written as follows.
Errorℓ

λ, Hη

=

w∈Oℓ

x∈Qλ
P (hℓ= w, qℓ−1 = x | λ) ·
· D

P (. | x, λ) || Hη (. | w)

(10)
Using Equation (8) we rewrite the above equation.
Errorℓ

λ, Hη

=

w∈Oℓ

x∈Qλ
P (hℓ= w, qℓ−1 = x | λ) ·
·

z∈O
P (z | x, λ) log
P (z | x, λ)
Hη (z | w)

(11)
Substituting P (z | x, λ) and Hη (z | w) using Equations (7) and (6) respectively, we write
the above equation as follows.
Errorℓ

λ, Hη

=

w∈Oℓ

x∈Qλ
P (hℓ= w, qℓ−1 = x | λ) ·
·

z∈O
P (oℓ= z | qℓ−1 = x, λ) log
P (oℓ= z | qℓ−1 = x, λ)
P (oℓ= z | hℓ= w, η)

=

w∈Oℓ

x∈Qλ

z∈O
P (oℓ= z | qℓ−1 = x, λ) ·
· P (hℓ= w, qℓ−1 = x | λ) log
P (oℓ= z | qℓ−1 = x, λ)
P (oℓ= z | hℓ= w, η)

(12)

HMM-Based Trust Model
29
Since the next outcome oℓdepends only on the current state qℓ−1 regardless of the
history sequence hℓ, we have
P (oℓ= z | qℓ−1 = x, λ) = P (oℓ= z | hℓ= w, qℓ−1 = x, λ)
(13)
Thus Equation (12) becomes
Errorℓ

λ, Hη

=

w∈Oℓ

x∈Qλ

z∈O
P (oℓ= z | hℓ= w, qℓ−1 = x, λ) ·
· P (hℓ= w, qℓ−1 = x | λ) log
P (oℓ= z | qℓ−1 = x, λ)
P (oℓ= z | hℓ= w, η)

=

w∈Oℓ

x∈Qλ

z∈O
P (oℓ=z, hℓ=w, qℓ−1 = x | λ) log
P (oℓ=z | qℓ−1 = x, λ)
P (oℓ=z | hℓ=w, η)

(14)
The above equation can be simpliﬁed to the following equation.
Errorℓ

λ, Hη

= E log P (oℓ| qℓ−1, λ) −E log P (oℓ| hℓ, η)
(15)
Observe that the ﬁrst term in the above equation depends only on the real behaviour
model λ, while the second term depends on both the real and approximate behaviour
models λ and η. Denoting the ﬁrst and second terms respectively by Cℓ(λ) and Hℓ(λ, η),
we rewrite the above equation as following.
Errorℓ

λ, Hη

= Cℓ(λ) −Hℓ(λ, η)
(16)
Assuming that (Aη)i j > 0, that is the state transition probabilities of η are strictly posi-
tive, it has been proved by Baum and Petrie in [1] that the following limit exists.
lim
ℓ→∞Hℓ(λ, η) = H (λ, η)
(17)
Observe also that the limit limℓ→∞Cℓ(λ) = C (λ) exists. This is because the ergodicity
of λ implies that the distribution of the random variable qℓ−1 converges to a stationary
(ﬁxed) distribution according to which the expectation E log P (oℓ| qℓ−1, λ) is evalu-
ated. The convergence of both Cℓ(λ) and Hℓ(λ, η) implies the convergence of the esti-
mation error (as ℓ→∞) to an asymptotic estimation error denoted by Error

λ, Hη

,
and expressed as follows.
Error

λ, Hη

= C (λ) −H (λ, η)
(18)
Also, (by Theorem 3.2 in [1]) the log-probability of any observation sequence hℓis
related to H (λ, η) as follows.
1
ℓlog P (hℓ| η)
a.s.
→H (λ, η)
(19)

30
E. ElSalamouny, V. Sassone, and M. Nielsen
The above equation means that the log-probability of a random sequence hℓunder the
approximate model η, divided by its length converges almost surely to H (λ, η). Here
‘almost surely’ (also known as ‘almost everywhere’ and ‘with probability 1’) conver-
gence means that the probability that the function 1
ℓlog P (hℓ| η) converges to the above
limit is 1. That is
P

lim
ℓ→∞
1
ℓlog P (hℓ| η) = H (λ, η)

= 1
Equation (19) implies that choosing an approximate model η which maximises the prob-
ability of a suﬃciently long sequence hℓalmost surely maximises H(λ, η), and therefore
reduces the asymptotic estimation error given by Equation (18). Thus, the maximum
data likelihood criterion, expressed by Equation (5) is a consistent method to obtain
the approximate behaviour model, which is used to estimate the predictive probability
distribution.
6
Comparison with Beta-Based Trust with Decay Principle
In this section we contrast the HMM-based trust model described above against the ex-
isting Beta trust model with exponential decay, described in [12] and Section 2 in terms
of the expected estimation error. Here the estimation error is deﬁned as the relative en-
tropy between the real and estimated predictive probability distributions. In Section 5.2
above, we used the results obtained by Baum and Petrie in [1] to derive an expression
for the expected estimation error (see Equation (16)). It appears diﬃcult to evaluate this
error analytically, or even numerically. So we use a simulation framework for HMMs
to simulate the real model and adopt Monte Carlo methods to evaluate the estimation
error using both HMM-based and Beta-based trust models, and therefore perform the
comparison.
6.1
Evaluation of Estimation Error Using Monte Carlo Simulation
In general, any probabilistic trust model is described by an estimating algorithm Aσ,
with a parameter σ. The estimating algorithm is fed with any observation sequence h
generated by the real system λ and computes an estimated predictive probability distri-
bution denoted by Aσ(. | h). In the case of Beta trust model, the estimating algorithm is
denoted by Br, where the parameter r is the decay factor, and the estimated predictive
probability distribution Br(. | h) is evaluated by Equations (2). In the case of HMM-
based trust model, on the other hand, the estimating algorithm is denoted by Hη, where
the parameter η is an approximate behaviour HMM. Note that the parameter η is ob-
tained by maximising the probability of any suﬃciently long sequence y generated by
λ as shown in Section 4. The estimated predictive probability distribution Hη(. | h) is
evaluated by Equation (6).
Given a real HMM model λ, let the random variables hℓdenote any generated se-
quence of observations of length ℓ. Let also the random variable qℓdenote the under-
lying hidden state sequence. Given an estimating algorithm Aσ (e.g. Br or Hη), the
expected estimation error using Aσ is given by the following equation.
Errorℓ(λ, Aσ) = E D (P (. | qℓ, λ) || Aσ (. | hℓ))
(20)

HMM-Based Trust Model
31
The above expected error can be approximated by the following Monte-Carlo proce-
dure.
1. Simulate the real model λ to generate a large sample S m of size m:
S m = {(w1, u1), (w2, u2), . . ., (wm, um)}
where wj and u j are respectively the observation sequence, and the underlying state
sequence generated in the jth simulation run.
2. For each pair

wj, u j

,
(a) compute both P

. | u j, λ

and Aσ (. | hℓ), that is the real and estimated predictive
probability distributions, respectively.
(b) Evaluate the estimation error, denoted by e j, as
e j = D

P

. | u j, λ

|| Aσ

. | wj

(21)
3. Approximate the required expected estimation error by evaluating the sample
average.
Errorℓ(λ, Aσ) ≈1
m
m

j=1
e j
(22)
The above approximation of the expected estimation error by the sample average is
based on the law of large numbers. Note that the approximation error can be made
arbitrarily small by making the sample size m suﬃciently large.
6.2
Experiments
Throughout our comparison we will a 4-state real model λ with the observation alphabet
O = {1, 2}, the observation probability matrix is
Bλ =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1.0
0.0
0.7
0.3
0.3
0.7
0.0
1.0
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(23)
and the state transition matrix is
Aλ =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
s
1 −s
3
1 −s
3
1 −s
3
1 −s
3
s
1 −s
3
1 −s
3
1 −s
3
1 −s
3
s
1 −s
3
1 −s
3
1 −s
3
1 −s
3
s
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(24)
where the parameter s is called the system stability, which indicates the tendency of the
system to staying in the same state rather than transiting to a diﬀerent one.

32
E. ElSalamouny, V. Sassone, and M. Nielsen
Fig. 2. Beta and HMM estimation errors versus decay factor given stability < 0.5
In the following experiments, we study the eﬀect of the system stability on both Beta
estimation with a decay factor and HMM based estimation. For simplicity we conﬁne
our HMM-based trust model to use only 2-state approximate behaviour models. We
also base our trust estimation on sequences of length 300. For diﬀerent stability values
0 ≤s < 1 and decay values 0 ≤r ≤1, we apply the Monte-Carlo procedure described
above to evaluate the expected estimation error using both Beta (Br) and HMM (Hη)
trust algorithms. Each generated sample is of size 10000.
Figure 2 shows Beta and HMM estimation errors when the system λ is unstable
(s < 0.5). It is obvious that the minimum error value for Beta error is obtained when the
decay tends to 1. The reason for this is that an unstable system is relatively unlikely to
stay in the same state, and therefore unlikely to preserve the previous distribution over
observations. If the estimation uses low values for the decay, then the resulting estimate
for the predictive probability distribution is close to the previous distribution; this is
unlikely to be the same as in the next time instant, due to instability. On the other hand,
using a decay r tending to 1 favours equally all previous observations, and the resulting

HMM-Based Trust Model
33
probability distribution is expected to be the average of the distributions exhibited by the
model states. Such an average provides a better estimate for the predictive probability
distribution than approximating the distribution of the most recent set of states using
low decay values.
It is also obvious that the HMM estimation error is lower than Beta estimation error.
The reason is that the 2-state HMM η is a more ﬂexible model to approximate the real
HMM λ than the Beta model which is, with decay 1, equivalent to 1-state HMM model.
It is worth noting that when stability is 0.25, the minimum expected beta error is 0,
when the decay is 1. The HMM-estimation error is also approximately 0. In this case
all elements of the transition matrix Aλ are equal and therefore, the whole behaviour
can eﬀectively be modelled by a single probability distribution over observations. This
single probability distribution is perfectly approximated by taking the whole history
into account using Beta model with decay 1, and also with 2-state HMM where both
states are equivalent.
Figure 3 shows Beta and HMM estimations errors when the system λ is stable (sta-
bility > 0.5). Observe that both Beta with decay 1 and HMM estimation errors are
Fig. 3. Beta and HMM estimation errors versus decay factor given stabilities 0.6, 0.7, 0.8, and 0.9

34
E. ElSalamouny, V. Sassone, and M. Nielsen
increasing as the stability is higher. The reason is that, at relatively high stability, old
observations become irrelevant to the current behaviour which determines the real pre-
dictive probability distribution. Hence, the estimation based on the whole history using
HMM or Beta with decay 1 is worse than the estimation with the same parameters when
the system is unstable, where both old and recent outcomes are relevant to the current
behaviour.
Observe also in the cases of high stability that HMM based estimation is better than
Beta estimation for most values of decay. However, for a particular range of decay,
Beta estimation is slightly better than HMM estimation. Using any decay value in this
range for Beta estimation has the eﬀect of considering only relatively recent outcomes
which characterize the current system behaviour and therefore give a better estimation
for the predictive distribution. Although using any value from this speciﬁc range of
decay makes Beta estimation better than HMM estimation, it appears hard to formally
determine this range given only observations. When the stability is 1, the assumption of
irreducibility is violated (see Section 5.1). In this case any sequence y of observations
characterises only one single state and therefore the approximate behaviour model η
trained on y fails to approximate the whole behaviour of the real system.
7
Conclusion
In this paper we introduced the foundations for the HMM-based trust model. This model
is based on approximating the behaviour of the principal by the n-states HMM η which
maximises the likelihood of the available history of observations. The approximate be-
haviour model η is then used to evaluate the estimated predictive probability distribution
given any sequence of observations. Modelling the real dynamic behaviour of princi-
pals by hidden Markov models, and using the results obtained by Baum and Petrie in
[1], we justiﬁed the consistency of the HMM-based trust model. This justiﬁcation relies
on showing that maximising the likelihood of a given observation sequence minimises
the relative entropy between the real and estimated predictive probability distributions.
To assess the estimation quality of a particular trust algorithm, we use the notion of
expected estimation error that is the expected diﬀerence between the real and predictive
probability distribution. Since we have no means yet to evaluate the expected estima-
tion error expressed by Equation (18) for the HMM-based trust model using analytical
or numerical methods, we use a Monte-Carlo algorithm, described in Section 6.1, for
evaluating the expected estimation error.
Using an implementation of this algorithm, and adopting the relative entropy as a
measure for the estimation error, we performed an experimental comparison between
HMM-based trust algorithm and the Beta-based trust algorithm with an exponential de-
cay scheme. The results of this comparison are given in Section 6.2. These results shows
that HMM-based trust algorithm gives a better estimation for the predictive probability
distribution when the principal behaviour is highly dynamic. When the real behaviour
is more stable (less dynamic), the Beta-based algorithm with the optimal value of decay
gives slightly better estimation than the HMM-based algorithm.

HMM-Based Trust Model
35
References
1. Baum, L.E., Petrie, T.: Statistical inference for probabilistic functions of ﬁnite-state Markov
chains. Annals of Mathematical Statistics 37(6), 1554–1563 (1966)
2. Baum, L.E., Petrie, T., Soules, G., Weiss, N.: A maximization technique occurring in the
statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical
Statistics 41(1), 164–171 (1970)
3. Br´emaud, P.: Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues. Springer,
Heidelberg (1998)
4. Buchegger, S., Le Boudec, J.-Y.: A Robust Reputation System for Peer-to-Peer and Mobile
Ad-hoc Networks. In: P2PEcon 2004 (2004)
5. Cahill, V., Gray, E., Seigneur, J.-M., Jensen, C.D., Chen, Y., Shand, B., Dimmock, N., Twigg,
A., Bacon, J., English, C., Wagealla, W., Terzis, S., Nixon, P., di Marzo Serugendo, G., Bryce,
C., Carbone, M., Krukow, K., Nielsen, M.: Using trust for secure collaboration in uncertain
environments. IEEE Pervasive Computing 2(3), 52–61 (2003)
6. Cover, T.M., Thomas, J.A.: Elements of Information Theory, 2nd edn. Wiley Series in
Telecommunications and Signal Processing. Wiley Interscience, Hoboken (2006)
7. ElSalamouny, E., Krukow, K., Sassone, V.: An analysis of the exponential decay principle in
probabilistic trust models. Theoretical Computer Science 410(41), 4067–4084 (2009)
8. Gambetta, D.: Can We Trust Trust? Basil Blackwell (1988)
9. Grimmet, G., Stirzaker, D.: Probability and Random Processes, 3rd edn. Oxford University
Press, Oxford (2001)
10. Javanmardi, S., Lopes, C.V.: Modeling trust in collaborative information systems. In: Interna-
tional Conference on Collaborative Computing: Networking, Applications and Worksharing,
pp. 299–302 (2007)
11. Jøsang, A., Haller, J.: Dirichlet reputation systems. In: The Second International Conference
on Availability, Reliability and Security, 2007. ARES 2007, pp. 112–119 (2007)
12. Jøsang, A., Ismail, R.: The beta reputation system. In: Proceedings from the 15th Bled Con-
ference on Electronic Commerce, Bled (2002)
13. Krukow, K., Nielsen, M., Sassone, V.: Trust models in Ubiquitous Computing. Philosophical
Transactions of the Royal Society A 366(1881), 3781–3793 (2008)
14. Kullback, S., Leibler, R.A.: On information and suﬃciency. Annals of Mathematical Statis-
tics 22(1), 79–86 (1951)
15. Mui, L., Mohtashemi, M., Halberstadt, A.: A computational model of trust and reputation (for
ebusinesses). In: Proceedings from 5th Annual Hawaii International Conference on System
Sciences (HICSS 2002), p. 188. IEEE, Los Alamitos (2002)
16. Nielsen, M., Krukow, K., Sassone, V.: A bayesian model for event-based trust. In: Festschrift
in hounour of Gordon Plotkin. Electronic Notes in Theoretical Computer Science (2007)
17. Norris, J.R.: Markov chains. Cambridge University Press, Cambridge (1997)
18. Rabiner, L.R.: A tutorial on hidden markov models and selected applications in speech recog-
nition. Proceedings of the IEEE 77(2), 257–286 (1989)
19. Sassone, V., Krukow, K., Nielsen, M.: Towards a formal framework for computational trust.
In: de Boer, F.S., Bonsangue, M.M., Graf, S., de Roever, W.-P. (eds.) FMCO 2006. LNCS,
vol. 4709, pp. 175–184. Springer, Heidelberg (2007)
20. Sivia, D.S.: Data Analysis: A Bayesian Tutorial (Oxford Science Publications). Oxford Uni-
versity Press, Oxford (1996)
21. Teacy, W., Patel, J., Jennings, N., Luck, M.: Travos: Trust and reputation in the context of
inaccurate information sources. Autonomous Agents and Multi-Agent Systems 12(2), 183–
198 (2006)
22. Xiong, L., Liu, L.: PeerTrust: Supporting reputation-based trust for peer-to-peer electronic
communities. IEEE Transactions on knowledge and data engineering 16(7), 843–857 (2004)

Deriving Trust from Experience
Florian Eilers and Uwe Nestmann
Technische Universit¨at Berlin, Germany
f.eilers@tu-berlin.de, uwe.nestmann@tu-berlin.de
Abstract. In everyday life, trust is largely built from experience. Repu-
tation-based trust models have been developed to formalize this concept.
The application to networks like the Internet where a very large num-
ber of predominantly unknown principal identities engage in interactions
is appealing considering that the evaluation of trusted experience may
result in a more successful choice of trusted parties to interact with.
In this paper we pick the SECURE framework, as developed within
the equally named EU project on Global Computing, which builds upon
event structures to model possible outcomes of interactions. We extend it
by three concepts: (i) a ﬂexible way to determine a degree of trust from
given past behavior, (ii) a basic notion of context, exemplarily in the
form of roles the interacting parties may occupy, and (iii) we explicitly
equip observed events with a time component to reﬁne the granularity
of observations.
We extend deﬁnitions of concepts used in SECURE in order to in-
corporate our notion of context information, we provide the syntax and
semantics of an LTL-like logic, in its basics similar to the one proposed
by Krukow, Nielsen and Sassone, that allows for layered reasoning about
context information. We then show how this new language relates to the
one used in SECURE and we determine under which conditions our con-
cept of deriving trust from experience may be used within SECURE’s
computational model to obtain a global state of trust.
1
Introduction
A model for trust and experience. The SECURE framework as described in
[Kru06] introduces so called trust structures to represent diﬀerent degrees of
trust and relations between them. A trust structure is a triple T = (D, ⊑, ⪯)
consisting of a set D of trust values ordered by two partial orderings: the trust
ordering (⪯) and the information ordering (⊑). The trust ordering simply sorts
trust values by the level of trust they assert (a ⪯b means that b denotes at
last as much trust as a), while the information ordering models a reﬁnement
w.r.t. the amount of available information (a ⊑b means that a can be reﬁned
into b should more information become available). In SECURE, the trust of one
individual in another is always represented by a single trust value.
There are a few side conditions such as ⪯being a lattice. While they are all
important for the calculation of a global trust state (intuitively a matrix that
holds all the information of “who trusts whom to what degree” for a given set of
principal identities) it is only the lattice property that is important to us here.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 36–50, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Deriving Trust from Experience
37
Gathering experience is understood as the observation of events from a given
mathematical structure. A prime event structure is a triple (E, ≤, #) consisting
of a set E of events that are partially ordered by ≤, the necessity relation (or
causality relation), and # is a binary, symmetric, irreﬂexive relation # ⊂E ×E,
called the conﬂict relation. The relations satisfy for all e, e′, e′′ ∈E:
[e]
def
= {e′ ∈E | e′ ≤e} is ﬁnite
and
if e#e′ and e′ ≤e′′ then e#e′′
Local interaction histories as deﬁned in [Kru06] hold all the information gath-
ered during interactions with another principal identity. Consequently there is a
local interaction history for each principal identity. They are sequences of con-
ﬁgurations (i.e. sets of events that are free of conﬂict and consistent with respect
to the necessity relation), each of which models information about a “session”,
that is (partial) knowledge about the outcome of an interaction.
Calculating a trust value. The SECURE trust model oﬀers a powerful tool to
reason about these local interaction histories. In [KNS05] Krukow, Nielsen and
Sassone describe a Linear Temporal Logic that contains events from an un-
derlying event structure as atoms, conjoined with the classical connectives and
pure-past temporal modalities. They use this logic to make binary decisions, i.e.
whether to interact with an entity or not. They also show a way to calculate a
trust value from a local interaction history, which requires the trust structure to
consist of triples that count good, bad and neutral outcomes of interactions. In
the following we propose a way to use this logic to obtain a trust value, without
the constraint of a ﬁxed trust structure, that consists purely of ordered triples.
Seeing how our whole approach is largely motivated by real life situations, it
seems reasonable to ask: “How does one solve the problem of deciding upon a
degree of trust given some previous experience in real life?” The most obvious
way is probably to ask oneself “Am I justiﬁed in trusting them this much?” for
every conceivable degree of trust and then picking the one that appears most
appropriate. Taking this concept over to SECURE would mean to deﬁne for each
trust value a condition under which we are justiﬁed to trust this much and then,
from the set of all justiﬁed trust values, pick the one that is “most appropriate”.
In order to explore how “most appropriate” could be formalized in this context,
we consider a few distinct cases. If only one trust value is justiﬁed then this is
trivial. Consider a set of justiﬁed trust values {a, b}. Obviously if we trust enough
to justify a and b and in our trust structure a ⪯b1 is true, then we should pick
b. In those cases the answer which one to pick is rather easy and the method
can be extended to ⪯-ordered sets of ﬁnite size by picking the supremum of the
set. But what should happen if a and b from our example are unrelated by ⪯?
As an example, consider a ﬁle system with trust values restricted, read, write
and random, where random also includes deletion of ﬁles which is not included
in either read or write. Both trust- and information-wise read and write are
greater than restricted and less than random, while being unrelated to each
1 ⪯being the trust ordering of the trust structure we are considering.

38
F. Eilers and U. Nestmann
other. Now assume we are justiﬁed to trust a principal with reading and writing,
i.e. J = {restricted, read, write} but not with deletion. If we had to name a single
trust value that deﬁnes the trust we have in them, we would not know what to
do: “restricted” would obviously be wrong, “read” or “write” would both be
inaccurate and we would not know which of the two to choose, while “random”
would grant the principal too many rights.
How can such a situation occur in general? It would mean that in our trust
structure we had two distinct unrelated trust values for which there is no justiﬁed
trust value, that incorporates the trust of both lower trust values. In this case
we can rightfully say that our trust structure is “wrong”: Under the assumption
that it should always be possible to model the trust in a principal with a single
trust value (which is one of the main principles in SECURE), this trust structure
is not adequate for our scenario. We would either have to change our reasoning
about justiﬁed trust values (i.e. restrict the user to either reading or writing) or
change the trust structure itself, because it obviously lacks a trust value for our
asserted degree of trust (i.e. allow reading and writing, but not deletion).
It therefore appears reasonable to demand that if we are justiﬁed in trusting
with two trust values, then there has to be a justiﬁed trust value, which incor-
porates the trust of both. Given this side condition it makes sense to pick the
“trust-supremum” (using the supremum from the lattice (D, ⪯) of the justiﬁed
trust values) to determine the representative trust value. The fact that this trust
value does not necessarily lie within the set of justiﬁed trust values is not dis-
concerting, because it would mean that it was our reasoning about this set that
was wrong in the ﬁrst place.
Criteria for justiﬁed trust values. We now know how to pick a representative
trust value given a set of justiﬁed trust values. But when is a trust value justiﬁed?
When we are given a condition we should be able to check whether it is met and
thereby determine whether a trust value is justiﬁed or not. A standard solution
is to use a logical formula and then to evaluate it within a concrete structure.
Contributions. The main contributions of this paper are the following: (i) We
introduce a ﬂexible, yet formal, way to calculate a trust value from a local
interaction history by equipping trust values with user-deﬁned conditions; (ii) We
equip events from local interaction histories with additional context information;
(iii) We reﬁne local interaction histories by explicitly storing the time at which an
event has been observed; (iv) We determine under which conditions our method
can be applied in the SECURE framework.
Related work. There are a number of experience based trust models, such as the
EigenTrust model (see [KSGm03]) or PathTrust (see [KHKR06]). The SECURE
trust model we base our work on can be found in an extensive compilation by
Krukow in [Kru06]. Closely related is the concept of attestation, i.e. ﬁnding
evidence to support the prediction of a certain behavior (see [CGL+08]).
The ﬁeld of trust and security is a prominent example for the application
of modal logics. Diﬀerent approaches include deontic and doxastic logics (see

Deriving Trust from Experience
39
f.ex. [CD97]) as well as temporal logics (see [KNS08]) which we follow in this
paper. Other approaches for reasoning about trust include probabilistic logic
(see [HP03] or [NKS07]).
2
Calculating Trust from Experience
Preliminaries. We brieﬂy recall the logic by Krukow et al. to reason about
local interaction histories as deﬁned in [KNS05], with a few minor syntactic
changes. The semantics of this logic are based on interpreting a local interaction
history as a Kripke-Structure, which is done as follows: We start oﬀwith a local
interaction history h with respect to an event structure ES, from which we deﬁne
a corresponding Kripke structure K = (W, R) together with an assignment β for
our atomic formulas and call the pair KES,h = (K, β) a history structure.
Deﬁnition 1 (History Structure). Let ES = (E, ≤, #) be an event structure
and h = c0 . . . cn, n ∈N a local interaction history. Then KES,h = ((W, R), β)
with W = {0, . . ., n}, R = {(i, i + 1) | i ∈{0, . . . n −1}}) and β : E →P(W)
deﬁned as β = {(e, w) | e ∈E ∧w = {c ∈W | e ∈c}} is called the history
structure with respect to ES and h.
We are now ready to deﬁne a logic to reason about local interaction histories.
This is done in the standard way to deﬁne a temporal logic. Events become
the atoms of the logic, conjoined with the classical connectives and temporal
modalities X (“in the next step”) and U (“until”). In SECURE the formulas
from this logic are called policies, a term that is also used in its computational
model to calculate of a global trust state. In order to distinguish these two kinds
of policies and to emphasize what we use this language for, we pick the new term
justiﬁcation for such a formula together with an associated trust value.
Deﬁnition 2 (Justiﬁcation Language). Let ES = (E, ≤, #) be an event
structure. The justiﬁcation language L(ES) is deﬁned as follows:
Syntax:
– If e ∈E, then e ∈L(ES)
– If ϕ and ψ ∈L(ES), then {¬(ϕ), (ϕ →ψ), X(ϕ), (ϕUψ)} ⊆L(ES)
Semantics: Let KES,h = ((W, R), β) be a history structure and w ∈W arbi-
trary. The truth of a L(ES)-formula is inductively deﬁned as follows:
– (KES,h, w) |= e ⇔w ∈β(e), for any e ∈E
– (KES,h, w) |= ¬(ϕ) ⇔(KES,h, w) ̸|= ϕ, for any ϕ ∈L(ES)
– (KES,h, w) |= (ϕ →ψ) ⇔(KES,h, w) ̸|= ϕ or (KES,h, w) |= ψ, for any
ϕ, ψ ∈L(ES)
– (KES,h, w) |= X(ϕ) ⇔∃(w, w′) ∈R ∧(KES,h, w′) |= ϕ, for any ϕ ∈
L(ES)
– (KES,h, w) |= (ϕUψ) ⇔∃w′ ∈W : (w, w′) ∈r(t(R)) ∧(KES,h, w′) |=
ψ ∧∀w′′ ∈W : ((w, w′′) ∈r(t(R)) ∧(w′′, w′) ∈t(R)) →(KES,h, w′′) |= ϕ
where t(R) denotes the transitive closure and r(t(R)) the reﬂexive and transitive
closure of the relation R.

40
F. Eilers and U. Nestmann
Notation 1. For reasons of legibility, we are going to deﬁne the following stan-
dard abbreviations for formulas ϕ and ψ:
⊥≡(ϕ ∧¬(ϕ))
⊤≡¬(⊥)
(ϕ ∨ψ) ≡¬(¬(ϕ) ∧¬(ψ))
(ϕ →ψ) ≡(¬ϕ ∨ψ)
(ϕ ↔ψ) ≡((ϕ →ψ) ∧(ψ →ϕ)) F(ϕ) ≡(⊤Uϕ)
G(ϕ) ≡¬(F(¬(ϕ)))
Evaluating Experience. As mentioned before, in order to specify conditions for
trust values, we bind formulas from this logic to trust values.
Deﬁnition 3 (Justiﬁcation). Let ES be an event structure, TS = (D, ⪯, ⊑) a
trust structure, d ∈D a trust value and ϕ a formula in L(ES), then a pair J =
(d, ϕ) is called a justiﬁcation (with respect to ES and TS). A set of justiﬁcations
with respect to ES and TS τ is called complete if it is a function τ : D →L(ES).
We have now deﬁned a formal way to specify a condition under which a trust
value is justiﬁed. The aforementioned “picking” of a single trust value is then
done by choosing the supremum of the set of all justﬁed trust values.
Deﬁnition 4 (History Evaluation). Let ES be an event structure, TS =
(D, ⪯, ⊑) a trust structure and τ a complete set of justiﬁcations and h a
local interaction history with respect to ES and TS respectively. We deﬁne the
history evaluation τ ∗by:
τ ∗(h)
def
=

⊥⊑,
if h = λ
({d ∈D |(KES,h, 0) |= τ(d)}),
otherwise
where  denotes the supremum with respect to ⪯.
Compatibility with the SECURE framework. SECURE oﬀers a way to calculate
a global trust state (an exact deﬁnition can be found in [Kru06], for the purpose
of this section an intuitive idea of a matrix that contains information about
“who trusts whom to what degree” is suﬃcient), given a set of policies for each
principal identity. These policies may contain user deﬁned functions to calculate
trust values. In order for them to preserve the soundness of the calculation they
have to be continuous with respect to the information ordering of the underlying
trust structure.
Allowing our history evaluation to be used in these policies may lead to distur-
bances in the calculation process if the result of the history evaluation ﬂuctuates
too much as more experience is gathered. One obvious solution would be to
simply restart the calculation of the global trust state, should the history evalu-
ation change to an information-wise lower trust value. While for small systems,
where this calulation does not take a considerable amount of time, this might
be acceptable, for bigger systems one of the main advatages of SECURE is lost.
We therefore instead extend the deﬁnition of information continuity and give a
condition under which the calculation of the global trust state is not endangered.

Deriving Trust from Experience
41
Notation 2. Let h = a1·a2·. . .·an and h′ = b1·b2·. . .·bm be two local interaction
histories with respect to the same event structure ES. We write h ⊆h′ if n ≤m
and for all i with 1 ≤i ≤n : ai ⊆bi
Deﬁnition 5 (Information
continuity).
A L(ES)-formula ϕ is called
information-continuous if for any two local interaction histories h and h′ with
h ⊆h′:
KES,h |= ϕ →KES,h′ |= ϕ
We can ensure the soundness of the calculation of the global trust state by keep-
ing the fomula of every justiﬁcation information-continuous. While this seems
a severe restriction, we claim that in a lot of cases it is still favorable over the
counting of outcomes of interactions. A certain kind of behaviour may be con-
sidered “unforgivable” while others are merely considered “bad”, so a ﬁner and
more ﬂexible concept is often in order. We leave for future work to analyze in
how far the technique used in [KNS08] can be simulated with our method.
An example. Given these deﬁnitions we can take a look at how our concept
works out in an example. Consider trust and event structures in the context of
an online shop that sells books and CDs: The observable events are the following:
{good quality, bad quality, fast delivery, slow delivery, no delivery}
As for the causality relation, naturally an observation of the quality of an ordered
article can only be observed if the article has in fact been delivered. The conﬂict
relation is straightforward too, diﬀerent degrees of quality are in conﬂict with
each other and so are outcomes of the delivery of an item.
The trust structure for our example knows the following degrees of trust:
{no trust, no info, buy books, buy CDs, buy anything}
Information-wise no info is the least value, followed by no trust, buy books and
buy CDs both greater than no info and ﬁnally by buy anything greater than
both buy books and buy CDs. Trust-wise no trust is the least value with no info
being greater, buy books and buy CDs both greater than no info but unrelated
to each other followed by buy anything as the “top” value.
A principal’s (complete) set of justiﬁcations τ could now look like this:
trust value
condition
no trust
⊤
no info
G(⊥)
buy books
G(good quality) ∧F(fast delivery)
buy CDs
G(fast delivery ∨good quality) ∧F(good quality)
buy anything G(fast delivery ∧good quality) ∧F(fast delivery)
Now, consider the three following local interaction histories. (In general, there
is always only one local interaction history for each principal identity, but as we
look at ways to interpret them it makes sense to consider more than just one.)

42
F. Eilers and U. Nestmann
h1 = {fast delivery, good quality}·{good quality, slow delivery}·{fast delivery}·
{fast delivery, bad quality}
h2 = λ
h3 = {fast delivery, good quality} · {fast delivery, good quality}
Given the above deﬁnitions we can now calculate the principal’s history evalua-
tion for each local interaction history:
τ ∗(h1) = ({no trust, buy CDs}) = buy CDs
τ ∗(h2) = ⊥⊑= no info
τ ∗(h3) = ({no trust, buy books, buy CDs, buy anything}) = buy anything.
3
Adding Context Information
Until now, we have been working quite naturally with the deﬁnition of a local
interaction history. The question arises whether this deﬁnition implies some un-
wanted limitations. Indeed it does: While we do not abstract completely from
the order in which events have occurred, some information is lost. It is possible
to order sessions in some way and assuming that there is always only one active
session (i.e. at any one point in time there is at most one conﬁguration in the
local interaction history that is not complete) this does not pose a problem. If we
allow concurrent sessions then we have to make abstractions that may go too far:
we only have means to order sessions, but neither events within conﬁgurations
(which holds true even in the case of only one active session at a time) nor events
from diﬀerent conﬁgurations (except for the order given by the conﬁgurations
that contain the events).
If we are looking at independent events (i.e. events unrelated by conﬂict or
causality relations) then this may become a problem. Assume a and b are events
we can observe. It may be possible that a trustworthy behavior requires to
observe only a or only b or ﬁrst a and then b but not the other way around.
We can not extract information about the order in which these events have
occurred with the given deﬁnition of a local interaction history2.
As this information seems quite valuable, it stands to reason to start looking
for an alternative deﬁnition of “history”. What we would like is a concept that
allows for reasoning about events in the context of their session, about events
independent from the session in which they have occurred and about a mix of
those two ideas. In other words, we would like to know the order in which events
have been observed, while still being aware of their respective session.
Furthermore we introduce the concept of roles. While we only consider ob-
servable evidence from a single principal identity at a time, this principal may
have acted in diﬀerent roles. As a simple example, consider a chat system in
2 Note that in SECURE this is not considered a limitation, for example in [KNS08]
it is noted that “in a scenario where this order of events is relevant, one can always
use a ’serialized’ event structure in which this order of occurrences is recorded”.
Hence the idea of recording the time of events explicitly is not new, but to our best
knowledge it has not been formally done before.

Deriving Trust from Experience
43
which there are moderated channels. Users can act in their role as a chatter or a
moderator. Certain behavior may be acceptable from moderators but not from
chatters. It is therefore important to know the role that a principal has played
within a given interaction.
3.1
A Language of Reasonable Complexity
The logic we deﬁne allows for references to roles and sessions. Since quantiﬁcation
is a nice tool, but at the same time increases the complexity of checking formulas,
we limit it in such a way that quantiﬁcations can not be nested to unlimited
depth. Arguing how much complexity in formulas is needed to cover all relevant
cases can only be a matter of experience. While it is easy to construct examples
that the logic can not handle, we claim that most sensible ways to reason about
experience are still covered.
Deﬁnition 6 (Local Interaction History). Let ES = (E, ≤, #) be an event
structure and S and R be sets (the set of sessions and the set of roles), then a
function h : N →P(R × S × E) is called a local interaction history (with respect
to ES, S and R). A local interaction history h is called consistent if
– S and R are ﬁnite
– ∀n ∈N : h(n) is ﬁnite
– ∃n ∈N : ∀n′ > n : h(n′) = ∅
– ∀s ∈S : ∀r ∈R : ∀k ∈N : k
n=0{e|(r, s, e) ∈h(n)} is a conﬁguration
– ∀s ∈S : ∀r ∈R : ∀e ∈E : ∀n ∈N :
(r, s, e) ∈h(n) →∀n′ ∈N : (r, s, e) ∈h(n′) →n = n′
Our local interaction histories are now functions that map a point in time (rep-
resented by a natural number) to a set of events within a context that have been
observed at that time. In order to distinguish and compare this new concept to
the one originally deﬁned in the SECURE framework, we refer to these local
interaction histories as “old” and “new”. We omit this attribute if it is clear
from the context which one is meant.
Deﬁnition 7 (Conversion old to new local interaction history). Let h =
c0 . . . cn, n ∈N be an old local interaction history with respect to ES = (E, ≤, #),
then the conversion h : N →P({0} × {0 . . . n} × E) of h is deﬁned as
h(i)
def
=

{(0, i, e) |e ∈ci},
if 0 ≤i ≤n
∅,
otherwise
Lemma 1. Let h = c0 . . . cn, n ∈N be an old local interaction history with
respect to ES = (E, ≤, #), then the conversion h is a consistent local interaction
history with respect to ES, S = {0, . . ., n} and R = {0}
Proof. Simple inspection.

44
F. Eilers and U. Nestmann
This new deﬁnition voids our way to reason about observed events. However the
general principle of using Kripke-semantics still seems to oﬀer a powerful tool
to do so. Hence we are going to introduce a way to obtain a Kripke-structure
together with an assignment from a local interaction history:
Deﬁnition 8 (History Structure). Let h be a local interaction history. The
history structure Kh is deﬁned by: Kh = ((W, R), h) with W = N and R =
{(n, n + 1)|n ∈N}
The next step is to deﬁne a language to reason about this history structure,
the basic idea is that we use a three-layered Kripke-semantics: The lowest layer
only contains information about events within a given role and a given session.
The middle layer adds information about sessions and the topmost layer allows
reasoning about roles.
Deﬁnition 9 (Justiﬁcation Language)
Syntax: Let ES = (E, ≤, #) be an event structure and R and S sets (of roles
and sessions). The justiﬁcation language L(R, S, ES) is deﬁned as follows:
– If e ∈E then {e, ♦(e)} ⊆L(ES)
– If ϕ ∈L(ES) then {¬(ϕ), X(ϕ)} ⊆L(ES)
– If {ϕ, ψ} ⊆L(ES) then {(ϕ ∧ψ), (ϕUψ)} ⊆L(ES)
– If (s, e) ∈(S × E) then (s, e) ∈L(S, ES)
– If ϕ ∈L(ES) then ∗S(ϕ) ∈L(S, ES)
– If ϕ ∈L(ES) and s ∈S then s : (ϕ) ∈L(S, ES)
– If ϕ ∈L(S, ES) then {¬(ϕ), X(ϕ)} ⊆L(S, ES)
– If {ϕ, ψ} ⊆L(S, ES) then {(ϕ ∧ψ), (ϕUψ)} ⊆L(S, ES)
– If (r, s, e) ∈(R × S × E) then (r, s, e) ∈L(R, S, ES)
– If ϕ ∈L(S, ES) then ∗R(ϕ) ∈L(R, S, ES)
– If ϕ ∈L(S, ES) and r ∈R then r : (ϕ) ∈L(R, S, ES)
– If ϕ ∈L(R, S, ES) then {¬(ϕ), X(ϕ)} ⊆L(R, S, ES)
– If {ϕ, ψ} ⊆L(R, S, ES) then {(ϕ ∧ψ), (ϕUψ)} ⊆L(R, S, ES)
Semantics: The deﬁnition is as expected. As it is rather lengthy, we only give
a few examples here. The full Deﬁnition can be found in [EN09].
– (Kh(r, s), w) |=R,S e ⇔(r, s, e) ∈h(w) for any e ∈E
– (Kh(r), w) |=R ∗S(ϕ) ⇔∀s ∈S : (Kh(r, s), w) |=R,S ϕ for any ϕ ∈
L(ES)
– (Kh, w) |= r : (ϕ) ⇔(Kh(r), w) |=R ϕ for any ϕ ∈L(S, ES)
Notation 3. Again, we use symbols like ⊥, →and G to improve legibility. Fur-
thermore for ϕ ∈L(ES): +S(ϕ) ≡¬(∗S(¬(ϕ))) and for ϕ ∈L(S, ES): +R(ϕ) ≡
¬(∗R(¬(ϕ)))
Again, we extend the notion of information continuity to ensure the soundness
of the calculation of a global trust state.

Deriving Trust from Experience
45
Notation 4. Let h and h′ be two local interaction histories (with respect to the
same ES, S and R), then we write h ⊆h′ if ∀n ∈N : h(n) ⊆h′(n).
Deﬁnition 10 (Information continuity). A L(R, S, ES)-formula ϕ is called
information-continuous if for all local interaction histories h′, for all h ⊆h′ and
for all n ∈N : (Kh, n) |= ϕ →(Kh′, n) |= ϕ.
The set of formulas we are able to deﬁne obviously depends on the sets of sessions
and roles (as well as the underlying event structure). When, while interacting
with a principal identity, new information becomes available and the sets of
sessions and roles change (i.e. grow) we would still like to use our previously
deﬁned formulas. This however is not a problem as can be seen in the following
lemma, the language only becomes richer.
Lemma 2 (Language extension via bigger context). Let ES = (E, ≤, #)
be an event structure and S ⊆S′ and R ⊆R′ sets, then L(R, S, ES) ⊆
L(R′, S′, ES)
Proof. Simple inspection.
Obviously truth can not necessarily be preserved when the context becomes
bigger. As an example consider a formula that says “in all roles and all sessions
ϕ holds.” and is true in some context. If we now add more sessions or roles
and in some of which ϕ does not hold, then the truth of our formula is lost.
Consequently formulas have to be designed accordingly. This does not necessarily
mean avoiding universal quantiﬁcation, but rather choosing ϕ wisely or S and
R respectively.
As we have seen earlier we can convert an old local interaction history to a
new one. We would like to be able to do the same with formulas.
Deﬁnition 11 (Conversion old to new formula). Let ES = (E, ≤, #) be an
event structure and ϕ ∈L(ES), then the conversion ϕ is recursively deﬁned as:
– If ϕ ∈E then ϕ
def
= +R : (+S : (ϕ))
– If ϕ = ¬(ψ) for some ψ ∈L(ES), then ϕ
def
= ¬ψ
– If ϕ = X(ψ) for some ψ ∈L(ES), then ϕ
def
= Xψ
– If ϕ = (ψ →χ) for some ψ, χ ∈L(ES), then ϕ
def
= (ψ →χ)
– If ϕ = (ψUχ) for some ψ, χ ∈L(ES), then ϕ
def
= (ψUχ)
Theorem 1 (Conservative language extension). Let ES = (E, ≤, #) an
event structure and h be an old local interaction history with respect to ES, then
for any ϕ ∈L(ES) and w ∈N:
(KES,h, w) |= ϕ ⇔(Kh, w) |= ϕ
Proof. The idea here is to show this property by structural induction over the
structure of formulas ϕ. See [EN09] for the complete proof.

46
F. Eilers and U. Nestmann
3.2
Working with Local Interaction Histories
We now have a means to reason about local interaction histories, therefore we
are going to elaborate a bit, how we can use it. The most obvious way to work
with a local interaction history, besides reasoning about the experience it holds,
is to add new observations. Some of which may not be possible, e.g. within a
role and a session an event that is in conﬂict with an event previously observed
does not make any sense and should make us reconsider our event structure.
Deﬁnition 12 (Valid Observation,Updated LocalInteraction History).
Let h : N →P(R × S × E) be a local interaction history with respect to ES, S
and R. A tuple (r, s, e, i) is called a valid observation with respect to h, iﬀ
– e ∈E
– i ∈N
– e /∈{e′ | ∃n ∈N : (r, s, e′) ∈h(n)}
– ∀n ∈N : {e′ | ∃n′ ∈{0..n} : (r, s, e′) ∈h(n′)} ∪{e} ∈C0
ES
Let h : N →P(R × S × E) be a local interaction history with respect to ES, S
and R and u = (r, s, e, i) a valid observation, then (h · u) : N →P((R ∪{r}) ×
(S ∪{s}) × E) with
(h · u)(n) =

h(n) ∪{(r, s, e)},
if n = i
h(n),
otherwise
is called the local interaction history h updated with u.
The validity of an observation ensures that a local interaction history remains
consistent when the observation is added.
Lemma 3. Let h be a consistent local interaction history with respect to ES, S
and R and u = (r, s, e, i) a valid observation, then (h · u) is consistent.
Proof. To show the consistency of (h · u) the following ﬁve points must hold:
– S ∪{s} and R ∪{r} are ﬁnite: Since h is consistent and therefore S and R
are ﬁnite this is obvious.
– ∀n ∈N : (h·u)(n) is ﬁnite: Since this holds for h and only a single observation
is added this also holds for (h · u).
– ∃n ∈N : ∀n′ > n : (h · u)(n′) = ∅: Since h is consistent there exists
m ∈N for which this holds for h. h is only changed at i, so we can choose
n = max(m, i + 1).
– ∀s′ ∈S ∪{s} : ∀r′ ∈R ∪{r} : ∀k ∈N : k
n=0{e′|(r, s, e′) ∈(h · u)(n)} is a
conﬁguration: By consistency of h this already holds true for all s′ ∈S \ {s}
and r′ ∈R \ {r}. For r and s this follows directly from the validity of
(r, s, e, i).
– ∀s′ ∈S ∪{s} : ∀r′ ∈R ∪{r} : ∀e′ ∈E : ∃n ∈N : (r′, s′, e′) ∈(h · u)(n) →
∀n′ ∈N : (r′, s′, e′) ∈(h · u)(n′) →n = n′: By consistency of h this already
holds true for all s′ ∈S and r′ ∈R. The validity of (r, s, e, i) ensures that
e /∈{e′ | ∃n ∈N : (r, s, e′) ∈h(n)} and therefore there exists at most one n
for which (h · u)(n) = (r, s, e).

Deriving Trust from Experience
47
As mentioned before, there should be some way to reason about experience that
is not actually our own. In real life situations we often trust another indiviual
enough to assume that any experience they share with us is genuine. We therefore
sometimes handle this experience as if it was actually our own. The equivalent
in our trust model would be to combine two local interaction histories.
Before we do that, we have to make sure, that these two local interaction
histories are not conﬂicting in some way:
Deﬁnition 13 (Compatibility)
– Two event structures ES = (E, ≤, #) and ES′ = (E′, ≤′, #′) are compatible
iﬀ(E ∪E′, ≤∪≤′, # ∪#′) is an event structure.
– Two local interaction histories h (with respect to ES = (E, ≤, #), S and R)
and h′ (with respect to ES′ = (E′, ≤′, #′), S′ and R′) are compatible with
respect to a function insert : N →N iﬀ
• insert is continuous with respect to ≤(on N),
• ES and ES′ are compatible,
• ∀s ∈S ∪S′ : ∀r ∈R ∪R′ : ∀n ∈N : ∀(r, s, e) ∈h′(insert(n)) : ∄n′ ∈N :
∃(r, s, e′) ∈h(n′) : (e#e′ ∨(e = e′ ∧n′ ̸= insert(n))).
Deﬁnition 14 (Combined Local Interaction History). Let h and h′ be
two local interaction histories with respect to ES = (E, ≤, #), S and R and
ES′ = (E′, ≤′, #′), S′ and R′ respectively, compatible with respect to a function
insert : N →N then
(h +insert h′) : N →P((R ∪R′) × (S ∪S′) × (E ∪E′))
with (h +insert h′)(n)
def
= h(n) ∪h′(insert(n))
is called the combined local interaction history of h and h′ with respect to insert.
Note that this method of using simple unions is merely one way to combine two
local interaction histories. Disjoint unions may be used if identiﬁcation of obser-
vations is unwanted. A generic solution would be to adapt the concept used in
graph grammars (see f.ex. [EEPT06]) that builds on a so-called double-pushout
construction to deﬁne interfaces (that is deﬁning which items are supposed to
be identiﬁed and which are not).
Of course we would not want to lose consistency when combining two local
interaction histories, but compatibility ensures that:
Lemma 4. Let h and h′ be two consistent local interaction histories with respect
to ES = (E, ≤, #), S and R and ES′ = (E′, ≤′, #′), S′ and R′ respectively,
compatible with respect to a function insert : N →N then (h +insert h′) is
consistent.
Proof. To show the consistency of (h +insert h′) the following points must hold:
– S ∪S′ and R ∪R′ are ﬁnite: h and h′ are consistent, therefore R, R′, S and
S′ are ﬁnite and so are their unions.

48
F. Eilers and U. Nestmann
– ∀n ∈N : (h +insert h′)(n) is ﬁnite: Since h and h′ are consistent they both
only yield ﬁnite sets of observations so their be ﬁnite too.
– ∃n ∈N : ∀n′ > n : (h +insert h′)(n′) = ∅: h and h′ are consistent, so there
exist m and m′ such that this holds for h and h′ respectively. Since insert
is continuous with respect to ≤we can choose n = max(m, insert(m′)).
– ∀s ∈S ∪S′ : ∀r ∈R ∪R′ : ∀k ∈N : k
n=0{e|(r, s, e) ∈h(n)} is a conﬁg-
uration: A conﬁguration has to be necessity-closed and conﬂict free. h and
h′ are consistent, therefor necessity-closed. insert is continuous with respect
to ≤, so the projection of h′ to the combined local interaction history is
necessity-closed, too. Since h and h′ are compatible with respect to insert,
∀s ∈S ∪S′ : ∀r ∈R ∪R′ : ∀n ∈N : ∀(r, s, e) ∈h′(insert(n)) : ∄n′ ∈N :
∃(r, s, e′) ∈h(n′) : e#e′, so all the sets of observed events within the same
role and sesson are conﬂict free and therefore conﬁgurations.
– ∀s ∈S ∪S′ : ∀r ∈R∪R′ : ∀e ∈E ∪E′ : ∃n ∈N : (r, s, e) ∈h(n) →∀n′ ∈N :
(r, s, e) ∈h(n′) →n = n′: In other words, no event has been observed more
than once. That is already the case for h and h′ on their own, so we only
need to ensure that h and h′ do not contain the same event within the same
role and session at a diﬀerent time. However the compatibility of h and h′
ensures that: ∀s ∈S ∪S′ : ∀r ∈R ∪R′ : ∀n ∈N : ∀(r, s, e) ∈h′(insert(n)) :
∄n′ ∈N : ∃(r, s, e′) ∈h(n′) : (e = e′ ∧n′ ̸= insert(n)).
3.3
An Example
Let us look at an example to show how this concept could be put into practice.
Consider an online service that performs a certain task after a user has ﬁled a
request. Assume the task is time-critical in some cases so the service oﬀers a
“premium membership” that guarantees responses to queries within a certain
time frame. Responses can turn out to be correct or incorrect later on.
Roles in our example will be “user” (u) and “premium user” (pu). Observable
events are E = {query, response, correct, incorrect} with the obvious dependen-
cies. The trust structure conists of four elements “unreliable”, “fast”, “correct”
and “reliable”. Its deﬁnition is as expected and can be taken from the complete
set of justiﬁcations:
trust value
condition
unreliable
⊤
fast
pu : (∗S : (G(query →X(response))))
correct
∗R : (∗S : ¬F(incorrect))
reliable
⊥
Note that with these justiﬁcations we consider the server to be reliable until it
has turned out not to be. Also note that we can annotate the highest degree of
trust “reliable” with ⊥because our history will evaluate to “reliable” if “fast”
and “correct” are justiﬁed, which is exactly what we consider reliable behavior
in our example.

Deriving Trust from Experience
49
Given the set of roles R = {u, pu} and the set of sessions S = N a local
interaction history h could look like this:
n
h(n)
0
{(u, 1, query)}
1
{(pu, 2, query), (u, 3, query)}
2
{(u, 1, response), (pu, 2, response)}
3
{(u, 1, correct), (pu, 2, correct)}
4
{(u, 3, response)}
5 . . .
∅
A trust value derived from this local interaction history can now be obtained
using the complete set of justiﬁcations. Again we ﬁrst calculate the set of all
justiﬁed trust values J = {unreliable, fast, correct} and then take the supremum
of this set, “reliable” as our derived trust value.
Assume we make the (valid) observation (u, 3, incorrect, 6). We can add it
to our local interaction history and after evaluation, we obtain “fast” as the
most approriate trust value since the observation of “incorrect” invalidated the
condition for “correct”.
Assume that another principal gathered experience with the same service:
n
h(n)
4
{(pu, 4, query)}
5
{(u, 5, query), (u, 5, response)}
6
{(pu, 4, response)}
7 . . .
∅
Since we fully trust the principal, we simply integrate their interaction history
into our own. Consistency conditions hold, so the combined (updated) local
interaction history is as follows:
n
h(n)
0
{(u, 1, query)}
1
{(pu, 2, query), (u, 3, query)}
2
{(u, 1, response), (pu, 2, response)}
3
{(u, 1, correct), (pu, 2, correct)}
4
{(u, 3, response), (pu, 4, query)}
5
{(u, 5, query), (u, 5, response)}
6
{(pu, 4, response), (u, 3, incorrect)}
7 . . .
∅
Notice that the response in session 4 takes too long and thus violates the
condition for “fast”. The set of justiﬁed trust values becomes J = {unreliable},
thus attesting the service to sometimes provide incorrect and/or late responses.

50
F. Eilers and U. Nestmann
4
Conclusion and Future Work
We presented a formal concept to derive trust from experience within the SE-
CURE framework by adding conditions in the form of modal-logic formulas to
trust values. We introduced context information as well as a ﬁner granularity of
time in observations and showed the relation to the original SECURE model.
As for future work, it looks promising to extend the concept of context to
a more generic level, where not only roles, but information like location or re-
sources may be considered. Another strand would be the extension of the concept
of roles to include capabilities or obligations, whose actual status may in turn de-
pend on other current contexts. Furthermore complexity analysis of the resulting
logic would grant new insights into the practical applicability of the presented
method.
References
[CD97]
Cuppens, F., Demolombe, R.: A modal logical framework for security
policies. In: Foundations of Intelligent Systems (1997)
[CGL+08]
Coker, G., Guttman, J.D., Loscocco, P., Sheehy, J., Sniﬀen, B.T.: Attes-
tation: Evidence and trust. In: Proc. Information and Communications
Security, 10th International Conference (2008)
[EEPT06]
Ehrig, H., Ehrig, K., Prange, U., Taentzer, G.: Fundamentals of Algebraic
Graph Transformation. Springer, Heidelberg (2006)
[EN09]
Eilers, F., Nestmann, U.: Deriving trust from experience (2009),
http://www.mtv.tu-berlin.de/menue/forschung/publikationen
[HP03]
Halpern, J.Y., Pucella, R.: A logic for reasoning about evidence. In: Proc.
19th Conference on Uncertainty in Artiﬁcial Intelligence (2003)
[KHKR06]
Kerschbaum, F., Haller, J., Karabulut, Y., Robinson, P.: PathTrust:
A trust-based reputation service for virtual organization formation. In:
Stølen, K., Winsborough, W.H., Martinelli, F., Massacci, F. (eds.) iTrust
2006. LNCS, vol. 3986, pp. 193–205. Springer, Heidelberg (2006)
[KNS05]
Krukow, K., Nielsen, M., Sassone, V.: A framework for concrete
reputation-systems with applications to history-based access control. In:
Proc. of the 12th CCS, pp. 7–11 (2005)
[KNS08]
Krukow, K., Nielsen, M., Sassone, V.: A logical framework for history-
based access control and reputation systems. Journal of Computer Secu-
rity 16(1), 63–101 (2008)
[Kru06]
Krukow, K.: Towards a Theory of Trust for the Global Ubiquitous Com-
puter. PhD thesis, University of Aarhus (2006)
[KSGm03]
Kamvar, S.D., Schlosser, M.T., Garcia-molina, H.: The eigentrust algo-
rithm for reputation management in p2p networks. In: Proceedings of
the 12th International World Wide Web Conference (2003)
[NKS07]
Nielsen, M., Krukow, K., Sassone, V.: A bayesian model for event-based
trust. Electronic Notes on Theoretical Computer Science, pp. 172 (2007)

Reﬂections on Trust: Trust Assurance by Dynamic
Discovery of Static Properties
Andrew Cirillo and James Riely⋆
DePaul University, School of Computing
 	



Abstract. Static analyses allow dangerous code to be rejected before it runs. The
distinct security concerns of code providers and end users necessitate that analysis
be performed, or at least conﬁrmed, during deployment rather than development;
examples of this approach include bytecode veriﬁcation and proof-carrying code.
The situation is more complex in multi-party distributed systems, in which the
multiple web services deploying code may have their own competing interests.
Applying static analysis techniques to such systems requires the ability to iden-
tify the codebase running at a remote location and to dynamically determine the
static properties of a codebase associated with an identity. In this paper, we pro-
vide formal foundations for these requirements. Rather than craft special-purpose
combinators to address these speciﬁc concerns, we deﬁne a reﬂective, higher-
order applied pi calculus and apply it. We treat process abstractions as serialized
program ﬁles, and thus permit the direct observation of process syntax. This leads
to a semantics quite different from that of higher-order pi or applied pi.
1
Security in Distributed Open Systems
In an open system, program code is under the control of mutually distrusting parties
prior to deployment. Local software security may be maintained in such a system by
using dynamic veriﬁcation at load time, rejecting code that fails analysis. For example,
a client browser may validate embedded scripts before execution; a server application
may validate SQL queries derived from client input. It is common for virtual machines
to perform bytecode veriﬁcation on class ﬁles loaded from remote sources [1]. Similar
approaches are taken in [2,3].
Such analysis can establish local properties, for example, that unauthorized code
does not gain access to sensitive system resources. It is more difﬁcult to obtain global
security guarantees, since no single observer has access to all of the required code.
Consider the simplest possible such system, consisting of a client and server. The client
may wish to ensure that sensitive data does not escape the server. Note that the client’s
trust in the organization running the server is not sufﬁcient—the client must also trust
the software running on the server. If the software is buggy, the client may need to trust
all other clients as well. The client may not require a full proof of correctness on the
part of the server, but may be satisﬁed to know that the server’s runtime system has all
current security patches applied or that it performs some simple integrity checks on data
⋆This work was supported by the National Science Foundation under Grant No. 0347542.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 51–65, 2010.
c⃝Springer-Verlag Berlin Heidelberg 2010

52
A. Cirillo and J. Riely
supplied by other users. The server has symmetric concerns, for example, restricting
client software in order to establish non-repudiation of a commercial transaction.
In current practice, attempts to establish such global properties are ad hoc and in-
formal: “If I only give my credit card number to pay-pal, everything will be ﬁne.” The
biggest ﬂaw of such policies is not that they lack formality, but that they are overly
restrictive. Lesser known vendors are high risk simply because they are lesser known.
Trusted computing [4] has the potential to enable less restrictive policies. Systems
that use trusted computing and remote attestation [5] conditionalize their interactions
with currently running, but physically distant, processes based on the identity of the
program code the remote party is running. Secure messages identify the code of their
senders; recipients trust the contents based on static properties of the senders’ code.
In prior work [6], we made a ﬁrst step toward formalizing such systems, developing a
higher-order π calculus with ad hoc primitives for remote attestation and a type system
that enforced memory safety in the face of arbitrary attackers. Here we improve this
work by generalizing both the primitives of the language and the policies to which it
applies; we also provide a more powerful and realistic attacker model.
In practice, there are several operations available on an executable: (a) one can com-
municate it as data, (b) one can execute it, (c) one can identify it by comparing it syn-
tactically to another value, (d) one can extract data from it, or (e) one can disassemble it
and operate on its components. Operations (a) and (b) are features of what is commonly
referred to as higher order programming. Operations (c) through (e), which expose the
syntax of mobile code, are features of what might be called introspective, or reﬂective
programming.
Formalizing many aspects of open systems requires a reﬂective approach; trusted
computing requires at least syntactic identiﬁcation of code, dynamic veriﬁcation also
requires disassembly. While identiﬁcation and data extraction are reasonably straight-
forward operations (see e.g., [6]), modeling the disassembly of an executable can be
complicated. For example, if primitive destructors for process syntax are used one must
take special precautions to keep names and variables from escaping their scopes, and
also to ensure that syntax is preserved by substitution.
Our key observation is that all three can be represented by extending higher order π
with pattern matching on abstractions. Our interest is to internalize static analysis at the
level of speciﬁcation, rather than implementation. We are thus able to restrict pattern
variables to match subvalues, rather than subvalues and subprocesses. The language can
encode arbitrary calculations on the syntax of an abstraction by “guessing” the structure
of the program and substituting pattern variables for values that are not known a priori.
To make up for the loss of induction over process syntax, we allow an inﬁnite number
of such processes in parallel. The language can thus model abstract speciﬁcations of
veriﬁers for static properties.
The paper is structured as follows. In Section 2, we present the syntax and opera-
tional semantics of the language. In Section 2, we then develop a systematic method
for describing processes that perform dynamic veriﬁcation. In Section 3 we apply the
theory to a simple type system that guarantees memory safety. In Section 4 we apply it
to a trusted computing platform.

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
53
2
A Reﬂective Pattern-Matching π-Calculus
Higher-Order π. The higher-order π calculus (HOπ) [8,9,10] is a natural model for
systems that communicate program code. In the face of attackers, however, HOπ raises
subtle issues. Consider the following example, where pub represents a public channel,
accessible to all parties including attackers, and passwd a channel accessible to only
  and
.
  ≜νsecret.pub!((x,y)if x = passwd then y!secret)
 ≜pub?(prog)νb.(prog ·(passwd,b)|b?(z)Q)
	
 ≜νpub.(

|νpasswd.( |
))
Alice creates a secret name (secret) and embeds it in an abstraction, which is then
written on the public channel. If the ﬁrst argument x of the abstraction matches passwd
then the secret is written on the second argument y. Bob reads the code from the public
channel and instantiates it with passwd and a callback channel. After unlocking the
secret Bob continues as Q with z bound to secret.
Consider an arbitrary attacker, Mallory, who knows pub but not passwd. Because
Mallory has access to pub, he can intercept Alice’s program before it is received by
Bob. Once in possession of the program, Mallory need only inspect its contents to ex-
tract the embedded secret without executing the code, thus circumventing the password
check.
HOπ does not model this sort of attack, since HOπ abstractions may only be com-
municated as data or run. By analogy to object [11] and class [12, Ch. 5] serialization,
HOπ allows process abstractions to be serialized, but does not allow for inspection of
the serialized form. Nonetheless, such attacks are of direct relevance to practical sys-
tems [13], therefore in this section we extend HOπ with reﬂection features.
Reﬂective π. We deﬁne a local, value-passing, asynchronous higher-order π parame-
terized over a signature that speciﬁes value constructors. A general-purpose pattern-
matching destructor works for any kind of value, including abstractions. As in pattern
matching spi [14], we equip pattern matching with a notion of Dolev-Yao derivability
that gives a semantics to cryptographic primitives by restricting patterns to those that
represent implementable operations. The resulting language is simple, yet powerful.
Syntax and Operational Semantics. A value signature (Σ) comprises three components:
a set of value constructors (f), a sorting that assigns each constructor an arity, and a
Dolev-Yao derivability judgment (⊩) that constrains value patterns. Fix a value signa-
ture. Values include names, variables, process abstractions and constructor applications;
processes include stop, local input, asynchronous output, application, parallel compo-
sition, restriction, replication, value construction and a pattern matching destructor. We
also allow some processes to contain inﬁnite parallel components.

54
A. Cirillo and J. Riely
REFLECTIVE π
Syntax:
L,M,N,S,T ::= a
 x
 (x)P
 f ( 
M)
O,P,Q,R ::= 0
 a?N
 M!N
 M ·N
 ΠiPi
 νa.P
 ∗P
 let x = f ⟨
M⟩in P
 case M of ∃x.N in P
where f n(N),(f v(N)−x),N ⊩x
Reduction Axioms:
(COMM) a?M |a!N −→M ·N
(APP) ((x)P)·N −→P{x := N}
(CONST) let x = f ⟨
M⟩in P −→P{x := f ( 
M)}
(CASE) case M{x := N} of ∃x.M in P −→P{x := N}
We distinguish variables from names, allowing input only on names; therefore only
output capabilities may be communicated. This restriction makes examples simpler but
is not essential to the theory. We require that process abstractions have ﬁnite syntax ex-
cept when they are used as the right-hand side of an input process. The name a is bound
in “νa.P” with scope P. The variable x is bound in “(x)P” and in “let x = f⟨
M⟩in P”
with scope P. The variables x are bound in “case M of ∃x.N in P” with scope N and
P. Let fn and fv return free names and variables, respectively. Identify syntax up to
renaming of bound names and variables. Write “P{x := M}” and “N{a := M}” for the
capture-avoiding substitution of M for x in P and M for a in N. A constructor applica-
tion, f( 
M), is well-sorted if | 
M| matches the arity of f. Constructor applications in both
the value and process languages are assumed to be well sorted, as in applied π [15].
The variables x are pattern bound in ∃x.N with scope N. We say that ∃x.N is a
well-formed pattern if x ⊆fn(N). A term (or process) is well-formed if every pattern it
contains is well-formed and if any variable x that occurs under a constructor application
is pattern bound by an enclosing pattern. For example, “case M of ∃x.f(x) in 0” is well-
formed, but “case M of ∃x.f(x) in a!f(x)” and “(x)a!f(x)” are not well-formed. In the
sequel, we assume that all terms are well-formed.
Note that while ﬁrst order value passing languages, such as applied π [15], are of-
ten abstract with respect to the time at which a value is constructed, mixing reﬂection
and cryptography requires that we distinguish the code that creates a value from the
value itself. As an example, suppose “enc(M,N)” represents encryption of M with key
N and consider the abstraction “(x)a!enc(x,b)”; the missing payload implies that the
encryption has not yet taken place, in which case an observer should be able to extract
b. Similarly in “(x)a!enc(b,x)” we expect b to be visible. The case of “(x)a!enc(b,b′)”
is, however, ambiguous; if it represents a program that does an encryption then both
b and b′ should be visible, but if it represents a program embedded with an already-
encrypted message then neither should be visible. We resolve this ambiguity by provid-
ing an explicit construction call in the process language and requiring that constructor
applications in the value language contain no free (non-pattern) variables.
The pattern-matching destructor “case M of ∃x.N in P” allows nested matching into
constructed values and abstractions. We require that all bound pattern variables (x) oc-
cur at least once in N, and they may occur more than once. To match, all occurrences of

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
55
a pattern variable must match identical values. When matching abstractions we assume
that pattern variables are always chosen so as not to conﬂict with variables bound by
the abstraction.
Patterns are also constrained by the Dolev-Yao derivability judgment. The judgment
“ 
M ⊩N” expresses that the values N can be constructed by agents with knowledge
of the values 
M. We then require that pattern variables be derivable from the terms
mentioned explicitly in the pattern. For example, a sensible derivability judgment might
include “enc(x,M) ⊩x,” which would allow decryption when the key is speciﬁed, but
not “enc(x,y) ⊩x,y,” which would allow extracting both the contents and the key of an
encrypted message without specifying the key.
For clarity, we make use of a more concise syntax in written examples by observing
the following notational conventions. We omit binders from patterns clauses when they
are clear from context (as in case M of (x,y) in P). We omit unused bound variables,
writing ()P for (x)P when x ̸∈fn(P). We omit explicit let binders when the meaning
is clear, for example writing “a!f⟨x⟩” for “let y = f⟨x⟩in a!y.” We also assume that a
value constructor for pairs is available and use the obvious derived forms for tuples.
As usual, operational semantics are described in terms of separate structural equiva-
lence and reduction relations. We elide the deﬁnition of structural equivalence and the
context rules for reduction, which are entirely standard for π calculi, and present only
the reduction axioms. COMM brings an abstraction and an argument together over a
named channel; APP applies an argument to an abstraction, substituting the argument
for the parameter variable; and CONST constructs a new value from a constructor sym-
bol and a series of arguments. CASE allows a pattern match to proceed only if the value
is syntactically identical (up to α-equivalence) to the pattern modulo a substitution for
the bound variables of the pattern. For example, the pattern ∃x.(y)a!x does not match
(y)a!(y,b) because the substitution of (y,b) for x would capture y, however the pattern
∃x.(z)a!(z,x) does match because the bound z can be renamed to y.
Equivalences. Behavioral equivalences are not the focus of this paper (see [10] for a
thorough introduction), however we very brieﬂy note that adding reﬂection to HOπ in
almost any capacity will have a dramatic effect on its equivalences. In particular, any
equivalence closed under arbitrary contexts, which may have holes under abstraction
binders, collapses immediately to syntactic identity.
An interesting equivalence would therefore only consider contexts without holes in
abstractions (these could be called non-value contexts). Since they are transparent, pass-
ing process abstractions in this context is no different than for any ordinary values such
as pairs or integers, hence the standard deﬁnitions for value-passing π-calculi [10, Sec
6.2] can be used. While complications do arise in the presence of non-transparent (i.e.,
cryptographic) values, these issues are orthogonal to higher-orderness and reﬂection
and have already been addressed in the literature [16,15].
Embedded Password Attack Revisited. We now reconsider the example above and see
that, as desired, it is not secure. Consider an attacker, Mallory, deﬁned as follows.


△= pub?(prog)case prog of ∃(z1,z2).((x,y)if x = z1 then y!z2) in (...)

56
A. Cirillo and J. Riely
As was the case with only a higher-order features, Mallory is able to intercept the
program ﬁle with an input on the public channel, pub. By using reﬂection, however,
Mallory is now also able to extract both the password and secret without running the
program. The continuation (...) has z1 bound to password and z2 bound to secret.
Dynamic Veriﬁcation. Inspection of mobile code is not useful only for attackers, how-
ever. It can also be used to dynamically establish trust in code that was received, for
example over a public channel, via dynamic veriﬁcation.
A veriﬁable property is a property of abstractions that can be decided by a static
analysis tool that is invoked at runtime. We call such tools veriﬁers. The proper use of
a veriﬁer can ensure the safety of a process even when it executes code obtained from
an untrusted source.
Formally, a veriﬁable property is a predicate on ﬁnite abstractions subject to the
following constraints. First, it must be at least semi-decidable. Second, we require that
it depend on a speciﬁc usage of only a ﬁnite set of names. Given a property, P, and
a set of names, S, we say that S supports P if for every M ∈P and every a,b ̸∈S
where a ̸∈fn(M), M{b := a} ∈P. We write supp(P) for the smallest set of names
that supports P and restrict our attention to properties that have supp(P) ﬁnite. As
an example of a predicate on values without ﬁnite support, impose an total ordering on
inﬁnite subset of names ni such that ni < ni+1 and consider the predicate that insists that
only ni+1 may be output on ni. Such a predicate is not interesting to us, since names
have no inductive structure and therefore one cannot deﬁne an algorithm to decide it.
It is relatively easy to describe processes that implement veriﬁers using inﬁnite syn-
tax. Treating properties as sets of values, we quantify clauses over elements of the
set. Note, however that it is not quite as simple as specifying one pattern that exactly
matches each element of the set. For example, the naive veriﬁer,
a?((z))ΠM∈P

case z of M in ((x)P·z)

|ΠN̸∈P

case z of N in Q

inputs a value to be checked and then pattern matches all values that satisfy the property
continuing as P with the value bound to x, and all values that do not satisfy the property
continuing as Q. Quantiﬁcation over all values, however, means that such a process
would reference not just an inﬁnite subset of names but the whole universe of names,
thus violating important assumptions about bound names. With a little more effort,
though, we can build a veriﬁer that has a ﬁnite set of free names provided that the
underlying property has ﬁnite name support, as in the following derived form.
DERIVED FORM: VERIFY
verify M as P(x) in P else Q
△= νb.

ΠN∈P (case M of ∃z.N{a := z} in b?()((x)P·N{a := z}))
| ΠL̸∈P (case M of ∃z.L{a := z} in b?()Q) | b!b

where a = f n(M)−supp(P) , z∩(f v(P)∪f v(Q)∪{x}) = /0 and |z| = |a|
Now fn(verify M as P(x) in P else Q) = supp(P) ∪fn(M) ∪fn(P)fn(Q), hence it is
ﬁnite if and only if supp(P) is ﬁnite and M,P,Q have ﬁnite free names. The elimination
of uninformative names from patterns allows ﬁnite name dependency, but also causes
some of the pattern clauses under the quantiﬁcation to overlap. We can be assured that

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
57
this overlap is safe because names outside of supp(P) by deﬁnition cannot affect the
satisfaction of the property, hence true patterns may only overlap with other true pat-
terns and false with false. The use of b as a signal channel prevents more than one clause
from executing so the behavior resembles that of a naive implementation.
Note that this representation is general enough to allow one to express veriﬁers for
a wide range of analyses and properties, including even those that may not be ﬁnitely
realizable. In particular, when properties are only semi-decidable this representation
will be unrealistically powerful, however for the purpose of establishing safety theorems
the approach is adequate.
3
Typability as a Veriﬁable Property
The framework for dynamic veriﬁcation presented above may be applied to any veri-
ﬁable property. A veriﬁable property is not necessarily a useful security property. For
example, it is veriﬁable that an executable is signed, but this does not impart any secu-
rity in itself. To establish a security theorem of some sort, we must choose a property
with provable security guarantees.
In this section we consider an example of such a property, formalizing a common
approach to typing that guarantees the absence of certain runtime errors in the presence
of arbitrary attackers [17,18,19,20,21]. Typability in this type system represents a veri-
ﬁable property subject to implementation as an analysis procedure that can be invoked
at runtime. To provide support for interesting examples, we use a signature that includes
some basic constructs that are useful in open distributed systems, including dynamically
typed messages [22] and cryptographic hashes and symmetric-key encryption.
The novelty is not in the type system itself, which is mostly standard, so much as
how it serves as an example for dynamic veriﬁcation. For this reason we simplify the
typed language by supporting nested pattern matching only when extracted values can
be treated at type Top, and type-safe pattern matching only for top-level patterns. Many
of these restrictions can be eased using, for example, techniques developed in [14].
SIGNATURE (Σ)
Value Constructors (where f k is a constructor f of arity k):
Σ = unit0,
 2,
2, #1,
	2, →2, Unit0, ×2, Dyn2, Hash1, Un0, Top0, Ch1, Key1
Derivability Rules:

M,N ⊩N

M ⊩N1 ... 
M ⊩Nk

M ⊩N1,...,Nk

M ⊩N

M ⊩f (N)

M, N ⊩L
f ̸∈{#,
	}

M, f (N) ⊩L

M ⊩N′,L

M,enc(N,N′) ⊩N,L
Language. Assume a signature with the following values: unit and
, which work
as usual; dyn(M,T), for dynamically-typed message that asserts that M is a value of
type T; #(M) for the cryptographic hash of M; enc(M,N) for the message M encrypted
with key N; and type constructors Unit, Un, Top, Ch(T), T →Proc, Hash(T), Key(T),
and Dyn(M,T). We write “(M,N)” as shorthand for “(M,N),” we write abstraction
types postﬁx, as in “T →Proc,” and we write “Dyn” for “Dyn(unit,Unit).”

58
A. Cirillo and J. Riely
Derivability rules exclude only patterns that would allow one to derive the original
value of a cryptographic hash or the contents of an encrypted message without the key.
We elide the derivability rules for processes since process syntax is always transparent.
Since they appear in dynamically typed messages, types are nominally ﬁrst-class
values. Informally, we use T,S for values that represent types, however note that there
is no dedicated syntactic category for type values. Our treatment of dynamic typing is
standard except for our use of the type Dyn(M,T), which is explained later.
We avoid annotating processes with types primarily so we do not have to commit
to whether annotations should be visible to inspection or not (in comparison to un-
typed machine code vs. typed bytecode). Annotations can instead be coded up using dy-
namically typed messages. We write “ν(a : T)P” for “νa.let x =

⟨a,T⟩in P” where
x ̸∈fn(P) when we wish to force the typechecker to commit to a speciﬁc type or simply
add clarity.
Safety and Robust Safety. Our objective is simply to prevent the misuse of a ﬁxed set
of typed initial channels. Let the metavariable T range over a language of types that
includes type values, plus the non-ﬁrst class type TYPE. A type environment (Γ) binds
names and variables to types in the usual fashion; we write “Γ ∋a : T ” to mean that
Γ = Γ′,a : T ,Γ′′ and a ̸∈dom(Γ′′). An initial typing is a type environment taking a set
of initial channels to channel types. An error occurs if a process violates the contract
of an initial channel by writing a non-abstraction value on a channel with a type of the
form Ch(T →Proc). Our focus on shape errors involving abstractions is arbitrary; other
errors are also possible.
Let Δ be an initial typing with domain a1,...,an. We say that a process P is Δ-safe
if whenever P =⇒νb.(ai?M | ai!N | Q) and Δ(ai) = Ch(T →Proc), N is of the form
(x)R. We say that a process O is an initial Δ-opponent if for all a ∈(fn(O)∩dom(Δ)),
Δ(a) = Ch(Un). We say that P is robustly Δ-safe if (O|P) is safe for an arbitrary initial
Δ-opponent O.
Type System. We now present a type system that enforces robust safety. The system
includes type judgments for well-formed values and well-formed processes.
The rules for well-formed values are mostly standard: hashes of values of type T type
at Hash(T); names that are used as signing keys for values of type T type at Key(T);
encrypted messages type at Un and require that the content type be compatible with the
key type. The one novelty is in the rules for dynamically typed messages, which allow a
forwarder to delegate part of the task of judging the trustworthiness of a message to the
recipient. A message dyn(M,T) types at Dyn(N,S) if either M can be typed at T, or N
cannot be typed at S. Opponent values are constructed from names that type at Ch(Un),
cryptographic hashes and encrypted messages.
The rules for well-formed processes are similarly standard, except for the rules for
pattern matching. Speciﬁc rules are deﬁned for top-level (non-nested) pair splitting,
typecase and decryption operations. A separate general-purpose rule permits pattern
matching with arbitrarily nested patterns but restricts pattern variables to Top.
The type rules support the use of dynamic types to authenticate data based on the
trust placed in the program that created it. For example, the type Dyn(#(N),Hash(S →
Proc)) can be given to messages that are known to have been received from a residual of

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
59
WELL-FORMED VALUES (Γ ⊢M : T )
Trusted Values:
Γ ∋a,x : T
Γ ⊢a,x : T
Γ,x : T ⊢P
Γ ⊢(x)P : T →Proc
Γ ⊢unit : Unit
Γ ⊢M : T
Γ ⊢N : S
Γ ⊢(M,N) : T ×S
Γ ⊢T : TYPE
Γ ⊢Ch(T) : TYPE
Γ ⊢T : TYPE
Γ ⊢T →Proc : TYPE
Γ ⊢Top,Un,Unit,Dyn : TYPE
Γ ⊢T : TYPE
Γ ⊢Hash(T) : TYPE
Γ ⊢T : TYPE
Γ ⊢Key(T) : TYPE
Γ ⊢M : T
Γ ⊢S : TYPE
Γ ⊢Dyn(M,S) : TYPE
Γ ⊢M : T
Γ ⊢M : Top
Γ ⊢T : TYPE
Γ ⊢M : T
Γ ⊢N : S
Γ ⊢dyn(M,T) : Dyn(N,S)
Γ ⊢M : T
Γ ⊢#(M) : Hash(T )
Γ ⊢M : T
Γ ⊢N : Key(T )
Γ ⊢enc(M,N) : Un
Opponent Values:
Γ ⊢a : Ch(T )
T ∈{Un,Top}
Γ ⊢a : Un
Γ,x : Un ⊢P
(∀a ∈f n(P)) Γ ⊢a : Un
Γ ⊢(x)P : Un
Γ ⊢
M : Un
Γ ⊢f ( 
M) : Un
Γ ⊢M : Un
Γ ⊢N : T
Γ ⊬N : S
Γ ⊢M : Dyn(N,S)
Γ ⊢M : T
Γ ⊢#(M) : Un
the abstraction N applied to an argument of type S. If the identity but not typability of the
sender is known, a forwarder can thus record the (code) identity of the sender without
judging whether the sender is actually well-typed. If a later recipient can establish that
N does type at S →Proc they can use the contents of the value safely.
Results. The main result of the type system is the following theorem of robust safety,
which states that well-formed processes are robustly safe. We elide the proof, which is
fairly standard and follows from lemmas for subject reduction (if Γ ⊢P and P −→Q
then Γ ⊢Q) and opponent typability (if O is an initial Δ-opponent then Δ ⊢O).
THEOREM (ROBUST SAFETY). If Δ ⊢P then P is robustly Δ-safe.
Robust safety can be ensured, for example, by limiting interactions with opponents
to untyped data communicated over untyped initial channels, however using dynamic
veriﬁcation one should also be able to safely accept and conditionally execute an ab-
straction from an opponent if the abstraction can be proved to be well typed. To this
aim we internalize the type system into the language by describing it as a veriﬁer.
Let T be a type and Δ type environment. Then P(M) = Δ, fn(M) : 
Top ⊢M :
T denotes a veriﬁable property supported by dom(Δ). A veriﬁer then has the form:
“verify M as P(x) in P else Q.” (Note that the addition of fn(M) : 
Top to the type en-
vironment allows accepted abstractions to contain arbitrary extra free names as long as
they do not affect typability.)
We are helped by the fact that the veriﬁer is itself well-typed more or less by deﬁni-
tion because the relevant clauses in the encoding are drawn from the set of well-typed

60
A. Cirillo and J. Riely
WELL-FORMED PROCESSES (Γ ⊢P)
Trusted Processes:
Γ ⊢0
Γ ⊢a : Ch(T )
Γ ⊢M : T →Proc
Γ ⊢a?M
Γ ⊢M : Ch(T )
Γ ⊢N : T
Γ ⊢M!N
Γ ⊢M : T →Proc
Γ ⊢N : T
Γ ⊢M ·N
Γ ⊢P
Γ ⊢Q
Γ ⊢P|Q
Γ,a : T ⊢P
Γ ⊢νa.P
Γ ⊢P
Γ ⊢∗P
Γ ⊢f ( 
M) : T
Γ,x : T ⊢P
Γ ⊢let x = f ⟨
M⟩in P
Γ ⊢M : T ×S
Γ,x : T ,y : S ⊢P
Γ ⊢case M of ∃(x,y).(x,y) in P
Γ ⊢M : Dyn(N,S)
Γ ⊢N : S
Γ ⊢T : TYPE
Γ,x : T ⊢P
Γ ⊢case M of ∃x.dyn(x,T) in P
Γ ⊢M : Key(T )
Γ,x : T ⊢P
Γ ⊢case M of ∃x.enc(x,M) in P
Γ ⊢M : T
Γ,y : 
Top ⊢N : T
Γ,y : 
Top ⊢P
Γ ⊢case M of ∃y.N in P
Opponent Processes:
Γ ⊢a : Un
Γ ⊢M : Un
Γ ⊢a?M
Γ ⊢M : Un
Γ ⊢N : Un
Γ ⊢M!N
Γ ⊢M : Un
Γ ⊢N : Un
Γ ⊢M ·N
Γ ⊢M : Un
Γ,x : 
Un ⊢N : Un
Γ,x : 
Un ⊢P
Γ ⊢case M of ∃x.N in P
terms, which allows us to type x at T →Proc. If the veriﬁcation succeeds x gets bound
to N in P. Since N types only at Un, the verify construct implements what amounts to
a dynamic cast, allowing one to take arbitrary data from an untyped opponent and cast
it to a well-typed abstraction.
For example, suppose Δ
△= anet : Ch(Un),b1 : T1,...,bn : Tn where T1−n ̸= Ch(Un)
and deﬁne P(M) as Δ, fn(M) : 
Top ⊢M : (T1 ×...×Tn) →Proc. Then the following
process is robustly Δ-safe.
∗anet?(x : Un)verify x as P(y) in (y·b)
The process repeatedly reads arbitrary values from an open network channel (anet) and
tests them dynamically to see if they are well-typed at (T1 × ... × Tn) →Proc before
applying them to a series of protected channels. If b represent, for example, a series
of protected system calls this process could represent a virtual machine that performs
bytecode veriﬁcation, as well as many other applications of dynamic veriﬁcation.
4
Example: Dynamic Veriﬁcation and Trusted Computing
On its own, dynamic veriﬁcation can be used to conditionalize the application of an
abstraction on the results of static analysis of the program code. In this section we
expand the use of dynamic veriﬁcation to also conditionalize interactions with running

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
61
processes using remote attestation. This solution utilizes a notion of code identity,
whereby an active process is identiﬁed by the process abstraction it started as.
Background. Trusted computing is an architecture for secure distributed computing
where trust is rooted in a small piece of hardware with limited resources known as the
trusted platform module (TPM). The TPM is positioned in the boot sequence in such
a way that it is able to observe the BIOS code as it is loaded. It takes and stores the
hash of the BIOS as the system boots, thus establishing itself as the root of a chain-
of-trust; a secure BIOS records the hash of the operating system kernel with the TPM
before it loads, and a secure operating system records the hash of an application before
it is executed. If the BIOS and operating system are known to be trustworthy, then
the sequence of hashes will securely identify the currently running program. Remote
attestation is a protocol by which an attesting party demonstrates to a remote party
what code it is currently running by having the TPM sign a message with a private key
and the contents of its hash register. If the recipient trusts the TPM to identify the BIOS
correctly, and knows of the programs that hash to each identity in the chain, then they
can use static analysis of the program code to establish trust in the message.
Representing a Trusted Computing Platform. We represent a trusted computing frame-
work as follows. The TPM is represented by a process parameterized on a boot chan-
nel (aboot) and an attestation identity key (aaik). The TPM listens on the boot channel
for an operating system abstraction to load; upon receiving the OS (xos) it reserves
fresh attestation (bat) and check (bchk) channels and instantiates the OS with the new
channels. This calling convention is expressed as an abstraction type for “certiﬁable”
programs, which we abbreviateCert. The TPM accepts requests on the attestation chan-
nel in the form of a message and callback channel. An attestation takes the form of a
message signed by the TPMs attestation identity key where the contents are a dynam-
ically typed message where the type is bounded by a provenance tag; that is, of the
form dyn⟨ymsg,Dyn(#(xos),Hash(Cert))⟩. This message is then encrypted with aaik and
returned on the callback. The check channel is provided to clients so that they can ver-
ify TPM signatures; the TPM simply tests the signature and, if successful, returns the
payload typed at Dyn.
An example of a trustworthy operating system, OS, is initialized with an attestation
and a check channel. It repeatedly accepts outside requests to run abstractions; a fresh
attestation channel is created for each request that binds a message to the identity of the
abstraction before passing it on to the TPM. For user programs, such as virtual machines
or Internet browsers, that themselves host outside code, this protocol can be extended
arbitrarily. Each layer provides the next layer up with an attestation service that appends
the clients identity to a message before passing the request down.
Because attestation channels are general-purpose, dynamic types are needed to type
the payload of an attestation. The form of an attestation is therefore a nested series of
dynamically typed messages, with the innermost carrying the payload and actual type
and each successive layer being of the form dyn(M,Dyn(#(N))) where #(N) identiﬁes
the layer that generated M. The outermost message is then signed by the TPM.

62
A. Cirillo and J. Riely
DEFINITIONS
Cert
△= (Ch(Dyn×Ch(Un))×Ch(Un×Ch(Dyn))×Ch(Dyn)) →Proc
TPM(aboot,aaik)
△= ∗aboot?((xos,xarg))νbat.νbchk.

xos ·⟨bchk,bat,xarg⟩
| ∗bat?((ymsg,yrtn))yrtn!enc⟨dyn⟨ymsg,Dyn⟨#⟨xos⟩,Hash(Cert)⟩⟩,aaik⟩
| ∗bchk?((zmsg,zrtn))case zmsg of enc(x,aaik) in zrtn!x

OS
△= ((xat1,xchk,xarg))νbrun.

xarg!brun
| ∗brun?((yapp,yarg))νbat2.

yapp ·⟨xchk,bat2,yarg⟩
| ∗bat2?((ymsg,yrtn))xat1!(dyn⟨ymsg,Dyn⟨#⟨yapp⟩,Hash(Cert)⟩⟩,yrtn)

Initial Processes. We assume that initial processes have the following conﬁguration.
Execution occurs in the context of an initial environment (Δ) consisting of a ﬁxed num-
ber of Ch(Top)-typed channels (a1,...,a j), an arbitrary number of Ch(Un)-typed chan-
nels (a j+1,...,ak) and some number of additional channels (b) at various types.
The trusted world consists of k copies of TPM which share a single aik key name but
listen on individual boot channels, and j subjects (P1,...,Pj) with fn(Pi) ⊆{ai} ∪bi
where b1,...,b j are disjoint subsets of b, and a Δ-opponent (O) with fn(O) ⊆{ai | i >
j}. The opponent may control any number of TPM channels, but none that are in use
by another subject. No two subjects initially share a name that is not also known to the
opponent, therefore any secure communications between subjects has to be brokered by
the TPM.

νaaik.Πi≤k

TPM(aaik,ai)
	
|P1 | ... | Pj |O
A typical subject “boots up” by sending the TPM an OS ﬁle and a fresh channel. After
receiving an OS callback, the subject loads some number of concurrent applications.
Each application receives its own identifying attestation channel from the operating
system.
Pi
△= νb.(ai!(OS,b)|b?(x)x!(APP1,bi) | ... | x!(APPk,bi))
Therobustsafety ofan initialprocessfollowsfromthetypability ofTPM(aaik,ai)foralli,
which we establish informally by noting that (1) when theTPM receives a well-typed OS,
the new attestation channelwill type at Ch(Dyn×Ch(Un)), and attestations will have the
form dyn(M,Dyn(#(OS),Hash(Cert))) which will be well typed because Γ ⊢M : Dyn;
and (2) when the TPM loads an untyped OS, the new attestation channel will type at
Ch(Top) and attestations will have the form dyn(M,Dyn(#(OS),Hash(Cert))), which
will be well typed because Γ ⊬OS : Cert.
Using Attestations. Even a signed attestation cannot be automatically trusted. Because
the opponent controls some number of TPMs, the signature provides assurance only
that the message was created by a TPM that was initially running the particular abstrac-
tion that hashes to the attested identity. To trust the contents one must also trust that
the attesting abstraction (1) protects its attestation channel, and (2) only generates ac-
curate dynamic types, which in the case of nesting implies that a host program correctly
identiﬁes a hosted application when attestations are created.
Destructing an attestation is a three-step process. First the signature is validated using
the bchk channel provided by the TPM, which returns dyn(M,Dyn(#(OS),Hash(Cert))).

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
63
Second, the identity #(OS) is checked to ensure that it corresponds to an abstraction that
types at Cert. Dynamic veriﬁcation cannot be used here because the original program
code is not recoverable from the hash, so checking the identity amounts to testing equal-
ity with something with which there is a priori trust. Attested messages will generally
be nested so this process is repeated once per layer, eventually exposing a value of the
form dyn(L,T). This is matched against an expected type and the payload L is recov-
ered, typed at T. The processes of creating and destructing attestations are summarized
in the following derived forms:
DERIVED FORMS: ATTEST AND CHECK
let x = attest(Mat,N,T) in P
△= νb.Mat!(dyn⟨N,T⟩,b)|b?(x)P
let x = check(Mchk,N,(L1...n),T) in P
△= νb.Mchk!(N,b)|

b?(x)
case x of dyn(y1,Dyn(L1,Hash(Cert))) in
... case yn−1 of dyn(yn,Dyn(Ln,Hash(Cert))) in
case yn of dyn(z,T) in P

The robust safety theorem, combined with the typability of the derived forms for
attest and check, implies that any well-typed program written to use this infrastructure
is robustly safe.
Bidirectional Authentication with a Trusted Veriﬁer. We now turn to a speciﬁc exam-
ple that uses trusted computing to allow two mutually distrusting parties to authenticate.
The parties initially share no secure channels, have no knowledge of the other’s program
code and are unwilling to share their source code with the other. (Swapping source code
may be unacceptable in practice due to proprietary interests, or simply performance rea-
sons.) The parties do however initially trust the same veriﬁer which together with the
TPM is sufﬁcient to establish bidirectional trust. This very general example is broad
enough to suggest a wide range of applications, particularly in the context of commu-
nication over the public Internet where parties are frequently anonymous.
The example comprises three software components: TV deﬁnes a trusted third-party
veriﬁer, CLIENT deﬁnes the initiator of the communication, and SERVER deﬁnes the
other party to the communication. The trusted veriﬁer inputs an abstraction on a pub-
lic channel (aver) and uses dynamic veriﬁcation to test it for typability. If successful,
the hash of the abstraction is taken and packed into an attestation typed at Hash(cert),
which is returned to the requester to be used as a certiﬁcate. CLIENT and SERVER
are each passed their own abstractions when they are initialized, which they send to the
veriﬁer to obtain certiﬁcates. CLIENT initiates the communication by sending ﬁrst its
certiﬁcate and second an attested response channel on the public channel areq. SERVER
reads the certiﬁcate and uses it to trust the second message and recover the typed re-
sponse channel, on which it writes its own certiﬁcate and another attestation containing
the secret data.
TV
△= ((xat,xchk,
))aver?((yval,yrtn))
verify yval as {M | Γ, fn(M) : 
Top ⊢M : Cert}(z1) in
let z2 = attest(xat,#(z1),Hash(Cert)) in yrtn!z2

64
A. Cirillo and J. Riely
CLIENT
△= ((xat,xchk,xarg))νb.xarg!b |b?((xsel f ))(νa.aver!(xsel f ,a)|a?(ycert)areq!ycert)
| (νbrsp.let yreq = attest(xat,brsp,Ch(Top)) in (apub!yreq)
| brsp?(y)let zsid = check(xchk,y,(#(OS),#(TV)),Hash(Cert)) in
b?(y)let zdat = check(xchk,y,(#(OS),zsid),Ch(T)) in P)
SERVER
△= ((xat,xchk,xarg))νb.xarg!b |b?((xsel f ,xdat))νa.aver!(xsel f ,a)|a?(ycert)
apub?(y)let ycid = check(xchk,y,(#(OS),#(TV)),Hash(Cert)) in
apub?(y)let yreq = check(xchk,y,(#(OS),ycid),Ch(Top)) in
(yreq!ycert)|let yresp = attest(xchk,xdat,T) in (yreq!yresp |Q)
We assume that all three components will be run on trusted platforms with CLIENT and
SERVER on distinct TPMs. Trust in the veriﬁer is based on the identity of the program
code, not the party running it, therefore it can be run on its own TPM, or on the same
TPM as either party, or even as separate processes on both. The TPM therefore allows
parties to reliably certify their own code.
5
Conclusions
We have presented a new reﬂective variant of the higher-order π calculus that allows
for the dynamic inspection of process syntax and is useful for modeling open sys-
tems, which often rely on such operations. Reﬂection has also been considered for the
λ-calculus [23,24], and dynamic veriﬁcation using type-checking primitives has been
considered in a π-calculus [25].
A language which allows explicit decomposition of processes has recently been pro-
posed by Sato and Sumii [7]; the language considered here represents a middle-ground,
giving a simpler syntax and semantics but with a slight cost in terms of expressiveness.
In particular, while we can model arbitrary veriﬁers, we do not permit the veriﬁers them-
selves to be treated as programs, which would then be subject to veriﬁcation.
We considered two speciﬁc applications that use reﬂection: dynamic veriﬁcation,
which relies on an ability to dynamically typecheck mobile code prior to execution, and
trusted computing, which relies on an ability to associate a running process with the
identity of the process abstraction it started as.
The genesis of this work was our previous work with trusted computing in higher-
order pi [6]. Many issues, such as code identity and allowing attackers to extract names
from mobile code, were considered in the previous paper but handled in an ad-hoc fash-
ion. This paper fulﬁlls two additional objectives. First, it comprises a more foundational
and expressive approach to understanding such systems. Second, it has allowed us to
internalize static analysis. The approach to trusted computing in this paper lacks rich
access control features which were the focus of the prior paper, however adding them
would not be difﬁcult.
References
1. Yellin, F.: Low-level security in Java. In: WWW4 Conference (1995)
2. Necula, G.C.: Proof-carrying code. In: Principles of Programming Languages, POPL 1997
(1997)

Reﬂections on Trust: Trust Assurance by Dynamic Discovery
65
3. Riely, J., Hennessy, M.: Trust and partial typing in open systems of mobile agents. In: Prin-
ciples of Programming Languages, POPL 1999 (1999)
4. Trusted Computing Group: TCG TPM Speciﬁcation Version 1.2 (March 2006),

 	  

5. Brickell, E., Camenisch, J., Chen, L.: Direct anonymous attestation. In: Computer and Com-
munications Security (CCS), pp. 132–145. ACM Press, New York (2004)
6. Cirillo, A., Riely, J.: Access control based on code identity for open distributed systems. In:
Barthe, G., Fournet, C. (eds.) TGC 2007 and FODO 2008. LNCS, vol. 4912, pp. 169–185.
Springer, Heidelberg (2008)
7. Sato, N., Sumii, E.: A higher-order, call-by-value applied pi-calculus. In: Hu, Z. (ed.) APLAS
2009. LNCS, vol. 5904, pp. 311–326. Springer, Heidelberg (2009)
8. Sangiorgi, D.: Expressing Mobility in Process Algebras: First-Order and Higher-Order
Paradigms. PhD thesis, University of Edinburgh (1993)
9. Sangiorgi, D.: Asynchronous process calculi: the ﬁrst-order and higher-order paradigms (tu-
torial). Theoretical Computer Science 253, 311–350 (2001)
10. Sangiorgi, D., Walker, D.: The π-calculus: a Theory of Mobile Processes. Cambridge Uni-
versity Press, Cambridge (2001)
11. Sun Microsystems: Java Object Serialization Speciﬁcation (2005),

 	
	 
 	

12. Lindholm, T., Yellin, F.: The Java Virtual Machine Speciﬁcation Second Edition. Sun Mi-
crosystems (1999)
13. Anderson, N.: Hacking Digital Rights Management. ArsTechnica.com (July 2006),

 
	
		
		
	

14. Haack, C., Jeffrey, A.S.A.: Pattern-matching spi-calculus. In: Proc. IFIP WG 1.7 Workshop
on Formal Aspects in Security and Trust (2004)
15. Abadi, M., Fournet, C.: Mobile values, new names, and secure communication. In: Principles
of Programming Languages, POPL 2001 (2001)
16. Abadi, M., Gordon, A.: A calculus for cryptographic protocols: The spi calculus. Information
and Computation 148, 1–70 (1999)
17. Abadi, M.: Secrecy by typing in security protocols. J. ACM 46(5) (1999)
18. Gordon, A.D., Jeffrey, A.S.A.: Authenticity by typing for security protocols. J. Computer
Security 11(4) (2003)
19. Fournet, C., Gordon, A., Maffeis, S.: A type discipline for authorization policies. In: Sagiv,
M. (ed.) ESOP 2005. LNCS, vol. 3444, pp. 141–156. Springer, Heidelberg (2005)
20. Gordon, A.D., Jeffrey, A.S.A.: Secrecy despite compromise: Types, cryptography, and the pi-
calculus. In: Abadi, M., de Alfaro, L. (eds.) CONCUR 2005. LNCS, vol. 3653, pp. 186–201.
Springer, Heidelberg (2005)
21. Fournet, C., Gordon, A., Maffeis, S.: A type discipline for authorization in distributed sys-
tems. CSF 00, 31–48 (2007)
22. Abadi, M., Cardelli, L., Pierce, B., Plotkin, G.: Dynamic typing in a statically typed language.
ACM Transactions on Programming Languages and Systems 13(2), 237–268 (1991)
23. Alt, J., Artemov, S.: Reﬂective lambda-calculus. Proof Theory in Computer Science, 22–37
(2001)
24. Artemov, S., Bonelli, E.: The intensional lambda calculus. Logical Foundations of Computer
Science, 12–25 (2007)
25. Maffeis, S., Abadi, M., Fournet, C., Gordon, A.D.: Code-carrying authorization. In: Jajodia,
S., Lopez, J. (eds.) ESORICS 2008. LNCS, vol. 5283, pp. 563–579. Springer, Heidelberg
(2008)

Model Checking of Security-Sensitive Business
Processes⋆
Alessandro Armando and Serena Elisa Ponta
DIST, Universit`a di Genova, Italy
{armando,serena.ponta}@dist.unige.it
www.avantssar.eu
Abstract. Security-sensitive business processes are business processes
that must comply with security requirements (e.g. authorization con-
straints). In previous works it has been shown that model checking can
be proﬁtably used for the automatic analysis of security-sensitive busi-
ness processes. But building a formal model that simultaneously accounts
for both the workﬂow and the access control policy is a time consum-
ing and error-prone activity. In this paper we present a new approach
to model checking security-sensitive business processes that allows for
the separate speciﬁcation of the workﬂow and of the associated security
policy while retaining the ability to carry out a fully automatic analysis
of the process. To illustrate the eﬀectiveness of the approach we describe
its application to a version of the Loan Origination Process featuring an
RBAC access control policy extended with delegation.
1
Introduction
A business process is a set of coordinated activities carried out concurrently by
diﬀerent entities and using a set of resources with the aim to achieve a goal
or to deliver a service. The design and development of business processes is a
non trivial activity as they must meet several contrasting requirements, e.g. the
compliance with mandatory regulations and the ability to support a wide range
of execution scenarios.
Security-sensitive business processes are business processes in which security
requirements play a signiﬁcant role. In this paper we focus on security require-
ments on authorization. Failure to meet authorization constraints may lead to
economic losses and even to legal implications. The evolution from static, well
established processes to dynamic ones—a current trend in the development of
business processes—may seriously aﬀect their security and often this occurs in
subtle and unexpected ways. As an example consider a business process in which
agents can be dynamically delegated to perform tasks they were not initially au-
thorized to execute. This is desirable as delegation provides additional ﬂexibility
to the process, but it also oﬀers new ways to circumvent security. Since these
⋆This work was partially supported by the FP7-ICT-2007-1 Project no. 216471,
“AVANTSSAR: Automated Validation of Trust and Security of Service-oriented
Architectures”.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 66–80, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Model Checking of Security-Sensitive Business Processes
67
kinds of vulnerabilities are very diﬃcult to spot by simple inspection of the work-
ﬂow and of the associated security policy, security-sensitive business processes
are a new, promising application domain for formal methods.
Model checking is a technique for the automatic analysis of concurrent sys-
tems. Given a formal model of the system formally speciﬁed by a ﬁnite transition
system and the expected property speciﬁed by a formula in some temporal logic,
e.g. LTL, a model checker either establishes that the system enjoys the property
or returns an execution trace witnessing its violation.
Model checking has been remarkably successful in many key application areas,
such as hardware and protocol veriﬁcation and, more recently, software and se-
curity protocol veriﬁcation. A natural question is whether model checking can be
proﬁtably used for the automatic analysis of security-sensitive business processes.
Previous works [1,2,3] provide a positive answer to this question by showing that
business processes under authorization constraints can be formally speciﬁed as
transition systems and automatically analyzed by model checkers taken oﬀthe
shelf. However the manual deﬁnition of the transition system starting from the
workﬂow and the associated security policy is a complex and error-prone activ-
ity, which—if not carried out correctly—may undermine the signiﬁcance of the
whole method.
In this paper we present a new approach to the speciﬁcation and model check-
ing of security-sensitive business processes that comprises the following steps:
1. formal
modeling
of
the
security-sensitive
business
process
as
an
access-controlled workﬂow system;
2. formal modeling of the expected security property as an LTL formula φ;
3. automatic translation of the access-controlled workﬂow system into a plan-
ning system, a formal framework amenable to automatic analysis; and
4. model checking of the planning system to determine whether it enjoys φ.
An access-controlled workﬂow system is a formal, yet natural framework for spec-
ifying security-sensitive business processes and results from the combination of
a workﬂow system and of an access control system. A workﬂow system supports
the speciﬁcation of the control ﬂow of the business process by extending Petri
Nets [4] through a richer notion of state that accounts not only for the concur-
rent execution of the tasks but also for the eﬀects that their execution has on
the global state of the system. An access control system provides a declarative,
rule-based language for specifying a wide variety of security policies and oper-
ations for updating them. Our approach thus facilitates the modeling activity,
while supporting a fully automatic analysis of the process.
To illustrate the eﬀectiveness of the approach we have applied it against a
version of the Loan Origination Process (LOP) that features an RBAC access
control policy extended with delegation. By using SATMC [5,6], a model checker
for planning systems, we have detected serious ﬂaws in our original speciﬁcation
of the LOP and this has led us to the deﬁnition of a new, improved version of
the business process.
Our approach improves upon existing works on model checking of business
processes by simultaneously supporting:

68
A. Armando and S.E. Ponta
<process
name="LOP".../ >
<sequence >
<invoke > inputCustData </invoke >
<flow >
<invoke > prepareContract </invoke >
<sequence >
<invoke > intRating </invoke >
<if>
<condition > ¬ lowRisk </condition >
<invoke > extRating </invoke >
</if>
</sequence >
</flow >
<invoke > approve </invoke >
<if>
<condition > productOK </condition >
<invoke > sign </invoke >
</if>
</sequence >
</process >
(a) BPEL Program
p1
inputCustData
p2
beginFlow
p3
prepareContract
p5
p4
intRating
p6
extRating
¬ lowRisk
nop
lowRisk
p7
endFlow
p8
approve
p9
sign
productOK
nop
¬ productOK
p10
(b) Workﬂow System
Fig. 1. Loan Origination Process
– the separate speciﬁcation of the workﬂow and of the associated security
policy;
– the formal and declarative speciﬁcation of a wide range of security policies;
– the speciﬁcation of tasks with non-deterministic eﬀects;
– LTL to specify complex security properties at the level of access-controlled
workﬂow system, a higher level than that provided by transition systems;
– full automation of the analysis.
To the best of our knowledge no other model checking platform encompassing
all the above features exists.
2
Security-Sensitive Business Processes
Let us consider the BPEL [7] speciﬁcation of the Loan Origination Process
(LOP) given in Fig. 1a The process starts with the input of the customer’s
data (inputCustData). Afterwards a contract for the current customer is pre-
pared (prepareContract) while the customer’s rating evaluation takes place
concurrently. The rating enables the bank to determine whether the customer
can be granted the requested loan. To this end, the execution may follow diﬀerent
paths: if the risk associated with the loan is low (lowRisk), then an internal rat-
ing suﬃces (intRating); otherwise the internal rating is followed by an external
evaluation (extRating) carried out by a Credit Bureau, a third-party ﬁnancial
institution. The lowRisk condition indicates a situation in which the internal

Model Checking of Security-Sensitive Business Processes
69
Table 1. Permission assignment for the LOP
Task
Role
inputCustData preprocessor
prepareContract postprocessor
intRating if (isIndustrial) then supervisor else postprocessor
extRating supervisor
approve if (isIndustrial) then manager else supervisor
sign if (intRatingOK) then manager else director
rating is positive and the amount of the loan is not high. The loan request must
then be approved (approve) by the bank. Subsequently, if the customer and the
bank have reached an agreement, the contract is signed (sign). Notice that the
execution of a task may aﬀect the state of the process. For example, the task
approve modiﬁes the state of the execution by issuing a statement asserting if
the proposed product is suitable or not for the customer.
An agent can execute a task only if she has the required permissions. As it
is common in the business domain, the LOP relies on an access control model
based on RBAC [8] extended with delegation. According to the RBAC model, to
perform a task an agent must be assigned a role that is enabled to execute the
task and the agents must be also active in that role. The roles used in our case
study are given in Table 1 together with the tasks they are enabled to execute.
Roles can be organized hierarchically. In our case study, a director is more
senior than a manager and a supervisor is more senior than a postprocessor.
Senior roles inherit the permission to perform tasks assigned to more junior
roles. As a consequence, an agent can execute a task if her role (i) is directly
assigned the required permissions or (ii) is more senior than a role owning such
permissions. The permission assignment relation in Table 1 associates each task
of the LOP with the most junior role entitled to execute it.
Following the idea of conditional delegation presented in [9], we consider del-
egation rules of the form: ⟨PreConds, ARole, DRole, T ask⟩, where ARole and
DRole are roles, T ask is a task, and PreConds is a set of conditions that must
hold for the delegation to be applicable. A delegation rule states that if PreConds
holds and ARole is authorized to perform Task according to the permission as-
signment relation, then ARole can delegate DRole to execute Task. Notice that
this is a task delegation rather than a role delegation. In fact, the delegated agent
does not acquire a new role but she only obtains the permission to perform Task
by means of ARole. Examples of delegation rules considered in our case study
are:
- D1:
⟨intRatingOK, manager, supervisor, approve⟩,
- D2:
⟨intRatingOK, manager, supervisor, sign⟩,
As far as the security requirements are concerned, here we focus on Separation of
Duty (SoD) properties which are used for internal control and are probably the
most common application-level properties that business processes must comply

70
A. Armando and S.E. Ponta
with. SoD amounts to requiring that some critical tasks are executed by diﬀerent
agents. This can be achieved by constraining the assignment of roles (Static
SoD), their activation (Dynamic SoD) or even the execution of tasks [1]. In this
paper we focus on Object-based SoD (ObjSoD) and Operational SoD (OpSoD).
The former requires that no agent performs all the tasks accessing the same
object, while the latter requires that no agent performs all the tasks of the
workﬂow.
Object-based SoD for the LOP. Since intRating, extRating, and approve ac-
cess and deal with the rating of the customer they form a set of critical tasks and
the LOP is thus expected to meet the following ObjSoD property: “If the process
terminates successfully, then no single agent has performed all the critical tasks
(namely intRating, extRating, and approve).”
Operational SoD for the LOP. The OpSoD for the LOP can be expressed as
follows: “An agent cannot perform all the tasks of a successful process execution.”
Notice that in this case the set of critical tasks depends on lowRisk whose
value cannot be predicted in advance. If lowRisk is false then the set of critical
tasks comprises all the tasks of the process, otherwise it contains all tasks but
extRating.
3
Access-Controlled Workﬂow Systems
At the core of our approach lies the notion of access-controlled workﬂow system,
a formal framework supporting the separate speciﬁcation of the workﬂow of the
business process and of the associated security policy in terms of a workﬂow
system and of an access control system respectively. A workﬂow system is a
Petri Net extended with a richer notion of state that accounts not only for the
concurrent execution of tasks but also for the eﬀects that their execution has
on the global state of the system. An access control system supports the formal
and declarative speciﬁcation of the access control policy (stating which agent
can perform which task) including advanced but often used features such as
delegation. In this section we provide a detailed account of our speciﬁcation
framework, a trace-based semantics, and a temporal logic that allows for the
formal statement of properties of business processes.
3.1
Workﬂow Systems
A fact is an atomic proposition. Let F be a set of facts. A literal over F is either
a fact in F or the negation of a fact in F. The set of literals over F is denoted
by L(F). A set of literals L is consistent if and only if L does not contain a fact
and its negation. A formula over F is a propositional combination of facts and
the propositional constant true using the usual propositional connectives (i.e.
¬, ∧, ∨, and ⇒).

Model Checking of Security-Sensitive Business Processes
71
Table 2. Mapping BPEL programs into workﬂow systems
<invoke >
t
</invoke >
t
<sequence >
p1
p2
</sequence >
p1
p2
<if>
<condition >
c
</condition >
p
</if>
p
c
nop
¬c
<flow >
p1
p2
</flow >
nop
p2
p1
nop
Legenda:
t task
place
c literal
p, p1, p2 BPEL programs
transition
p
workﬂow associated with BPEL program p
nop any operation s.t. π(nop) = μ(nop) = ν(nop) = ∅
Let F be a set of facts and T be a set of transitions. A workﬂow system
over F and T is a tuple WS = ⟨P, F, IWS, T , In, Out, γ, α⟩, where P is a set
of places, In ⊆(P × T ), Out ⊆(T × P), γ is a function that associates the
elements of In with formulae over F expressing applicability conditions, α :
T →2L(F) × 2L(F) × 2L(F) is a partial function mapping transitions into sets of
literals corresponding to their preconditions π(t), their deterministic eﬀects η(t),
and their non-deterministic eﬀects ν(t) respectively, i.e. α(t) = ⟨π(t), η(t), ν(t)⟩,
where π(t) and η(t) are consistent. We call tasks the transitions for which α is
deﬁned. If T ⊆T is a set of transitions, then Tα denotes the set of tasks in T .
A marking is a function M : P →N. A state of WS is a pair (M, L), where
M is a marking and L ⊆L(F) is a maximally consistent set of literals, (i.e. a
truth-value assignment to the facts in F). IWS is the set of initial states of WS.
The preset of a transition t, in symbols •t, is the set {p ∈P : (p, t) ∈In}. The
set of preconditions of a transition t, in symbols ∗t, is the set {γ(p, t) : (p, t) ∈
In}. The postset of a transition t, in symbols t•, is the set {p ∈P : (t, p) ∈Out}.
Let T ⊆T be a set of transitions. We deﬁne π(T ) = 
t∈Tα π(t), η(T ) =

t∈Tα η(t), and ν(T ) = 
t∈Tα ν(t). A step is a set of transitions T ⊆T such
that π(T ) and η(T ) are consistent. A step T is enabled in a state (M, L) iﬀ
- L |= γ(p, t) for all p ∈P and t ∈T s.t. (p, t) ∈In, where |= is the consequence
relation in classical propositional logic.
- π(T ) ⊆L, and
- M(p) ≥
t∈T In(p, t) for all p ∈P.
Notice that here and in the sequel we use In(p, t) and Out(t, p) to denote also
the characteristic function of In and Out relations respectively.

72
A. Armando and S.E. Ponta
It is worth pointing out that workﬂow systems extend Petri nets by providing
a richer notion of state given by the pair (M, L), that accounts not only for
the concurrent execution of the component activities (by means of the marking
M) but also for the eﬀects that these activities have on the state of the system
(represented by the set of literals L). For this reason workﬂow systems are a
natural formal model for business processes. The mapping in Table 2 shows how
the constructs occurring in the BPEL program of Fig. 1a can be mapped to
workﬂow system templates. The mapping can be readily extended to support
other basic activities (e.g. <receive>, <reply>) as well as more complex BPEL
structured activities (e.g. <while> and the <link> construct which is used to
join activities occurring in diﬀerent branches of a <flow>).
3.2
Access Control Systems
An access control system over F and T is a tuple ACS = ⟨F, IACS, A, T , U, α, H⟩
where F, A, T , U are sets of facts, agents, tasks, and policy updates respectively,
α : U →2L(F) × 2L(F) is a function mapping policy updates into consistent sets
of literals corresponding to their preconditions π(u) and their eﬀects η(u) respec-
tively, i.e. α(u) = ⟨π(u), η(u)⟩, and H is a set of rules of the form ℓ0 ←ℓ1, . . . , ℓn
with ℓi ∈L(F) for i = 0, . . . , n and n ≥0. A state of ACS is a maximally con-
sistent set of literals S ⊆L(F) such that H(S) ⊆S where H(S) = {ℓ0 : (ℓ0 ←
ℓ1, . . . , ℓn) ∈H and ℓi ∈S for all i = 1, . . . , n}. We assume that the set F of an
ACS always contains a fact granted(a, t) for all a ∈A and t ∈T expressing the
authorization of an agent a to execute a task t. IACS ⊆L(F) are the initial states
of ACS.
Let U be a set of policy updates. We deﬁne π(U) = 
u∈U π(u) and η(U) =

u∈U η(u). A step of ACS is a set of policy updates U ⊆U such that π(U) and
η(U) are consistent. A step U is enabled in a state L iﬀπ(U) ⊆L.
3.3
Access-Controlled Workﬂow Systems
An access-controlled workﬂow system over F and T is a pair AWS = ⟨WS, ACS⟩
where WS is a workﬂow system over F and T and ACS is an access control
system over F and Tα. A state of AWS is a pair (M, L), where M is a marking of
the workﬂow system WS and L is a maximally consistent set of literals in L(F).
The initial states of AWS = ⟨WS, ACS⟩are the initial states (M, L) of WS such
that L is also an initial state of ACS. We use IAWS to denote the set of initial
states of AWS. A task allocation for Tα is a total function λ : Tα →A. A step
W of AWS = ⟨WS, ACS⟩is a triple (T, U, λ) where T ⊆T and U ⊆U such that
both π(T ) ∪π(U) and η(T ) ∪η(U) are consistent and λ is a task allocation for
Tα. A step W = (T, U, λ) is enabled in a state (M, L) iﬀgranted(λ(t), t) ∈L for
all t ∈Tα, T is enabled in (M, L), and U is enabled in L. If a step W = (T, U, λ)
is enabled in S = (M, L), then the occurrence of W in S leads to a new state
S′ = (M ′, L′), in symbols S[W⟩S′. A literal l is caused by a step transition
S[W⟩S′ iﬀat least one of the following conditions holds:

Model Checking of Security-Sensitive Business Processes
73
– l ∈η(T ) ∪η(U), i.e. l is a deterministic eﬀect of some transition or update,
– l ∈ν(T ) and l ∈L′, i.e. l is a non-deterministic eﬀect of some transition,
– there exists l ←l1, . . . , ln ∈H with li ∈L′ for i = 1, . . . , n, i.e. l is the head
of a rule in H whose body holds in L′,
– l ∈L and l ∈L′, i.e. the truth-value of l is left untouched by the occurrence
of W.
A step transition S[W⟩S′ is causally explained according to AWS iﬀS′ coincides
with the set of literals caused by the occurrence of W in S. (The notion of causal
explanation is adapted from [10].) An execution path χ of AWS is an alternating
sequence of states and steps S0W0S1W1 · · · such that Si[Wi⟩Si+1 are causally
explained step transitions for i ≥0. We use χs(i) and χw(i) to denote Si and
Wi respectively. An execution path χ is initialized iﬀχs(0) ∈IAWS. A state S
is reachable in AWS iﬀthere exists an initialized execution path χ such that
χs(i) = S for some i ≥0. A state (M, L) is n-safe iﬀM(p) ≤n for all p ∈P.
An access-controlled workﬂow system AWS is n-safe iﬀall its reachable states
are n-safe.
Notice that the mapping of Table 2 yields 1-safe access-controlled workﬂow
systems, provided that the initial states are 1-safe. This trivially follows from
the observation that every place of the workﬂow system has at most one incom-
ing edge. In the sequel we will restrict our attention to 1-safe access-controlled
workﬂow systems.
3.4
A Logic for Access-Controlled Workﬂow System
Properties of access-controlled workﬂow system can be expressed by means of
LTL formulae. Let AWS be an access-controlled workﬂow system. The set of
LTL formulae associated with AWS is the smallest set containing F, an atomic
proposition for each place in P, an atomic proposition exec(a, t) for each a ∈A
and t ∈Tα, an atomic proposition exec(t) for each transition t ∈T \ Tα, an
atomic proposition exec(u) for each policy update u ∈U, and such that if φ
and ψ are LTL formulae, then also ¬φ, (φ ∨ψ), (φ ∧ψ), (φ ⇒ψ), X φ, F φ, and
G φ are LTL formulae. Let χ be an initialized path of AWS. An LTL formula φ
is valid on χ, in symbols χ |= φ, if and only if (χ, 0) |= φ, where (χ, i) |= φ, for
i ≥0, is inductively deﬁned as follows:
- if φ is a fact then (χ, i) |= φ iﬀχs(i) = (M, L) with φ ∈L,
- if φ is an atomic proposition corresponding to a place p ∈P, then (χ, i) |= φ
iﬀχs(i) = (M, L) with M(p) ≥1,
- if φ is an atomic proposition of the form exec(a, t), then (χ, i) |= φ iﬀχw(i) =
(T, U, λ) with t ∈Tα and λ(t) = a,
- if φ is an atomic proposition of the form exec(o), then (χ, i) |= φ iﬀχw(i) =
(T, U, λ) with o ∈(T \ Tα) ∪U,
- (χ, i) |= ¬φ iﬀ(χ, i) ̸|= φ,
- (χ, i) |= (φ ∨ψ) iﬀ(χ, i) |= φ or (χ, i) |= ψ,
- (χ, i) |= X φ iﬀ(χ, i + 1) |= φ, and
- (χ, i) |= F φ iﬀthere exists j ≥i s.t. (χ, j) |= φ.

74
A. Armando and S.E. Ponta
The semantics of the remaining connectives readily follows from the following
equivalences: (φ ∧ψ) ≡¬(¬φ ∨¬ψ), (φ ⇒ψ) ≡(¬φ ∨ψ), and G(φ) ≡¬ F ¬φ.
A formula φ is valid in AWS, in symbols |=AWS φ, iﬀχ |= φ for all initialized
execution paths χ of AWS.
By using LTL we can now give a formal deﬁnition of the SoD properties
presented in Sect. 2.
ObjSoD for the LOP. If the process terminates successfully, then for all agents
a ∈A there exists a task t ∈T , where T = {intRating, extRating, approve},
such that a does not perform t:
F(p10 ∧productOK) ⇒

a∈A

t∈T
G ¬ exec(a, t).
(1)
OpSoD for the LOP. For all agents a ∈A there exists at least a task in the
workﬂow that is never executed by a:

a∈A
⎛
⎝
G ¬ exec(a, inputCustData) ∨G ¬ exec(a, prepareContract)∨
G ¬ exec(a, intRating) ∨G(lowRisk∨¬ exec(a, extRating))∨
G ¬ exec(a, approve) ∨G ¬ exec(a, sign)
⎞
⎠. (2)
4
Model Checking Access-Controlled Workﬂow Systems
A planning system is a formal framework for the speciﬁcation of concurrent
systems inspired by the model used by the AI community to specify planning
domains: states are represented by sets of literals and state transitions by actions,
where an action is speciﬁed by preconditions (i.e. literals that must hold for the
action to be executable) and eﬀects (i.e. which literals are possibly aﬀected by
the execution of the action); the possible behaviors of the system can also be
constrained by means of rules. It must be noted that the ability to specify rules
and actions with non-deterministic eﬀects goes beyond the expressiveness of
traditional planning languages, e.g. STRIPS, but are supported by more recent
developments [10,11]. In this section we formally deﬁne a planning system and
provide a trace-based semantics, a temporal logic for specifying properties, and a
formal translation from access-controlled workﬂow systems to planning systems.
4.1
Planning Systems
A planning system is a tuple PS = ⟨FP , IP S, OP , ω, HP ⟩, where FP is a set of
facts, OP is a set of planning operators, ω : OP →2L(FP ) × 2L(FP ) × 2L(FP ) is a
function mapping planning operators into sets of literals corresponding to their
preconditions π(o), their deterministic eﬀects η(o), and their non-deterministic
eﬀects ν(o) respectively, i.e. ω(o) = ⟨π(o), η(o), ν(o)⟩, where π(o) and η(o) are
consistent, and HP is a set of rules over FP . A state of PS is a maximally
consistent set of literals over FP . IP S is the set of initial states of PS. If O is
a set of planning operators, then π(O) = 
o∈O π(o), η(O) = 
o∈O η(o), and

Model Checking of Security-Sensitive Business Processes
75
ν(O) = 
o∈O ν(o). A step O of PS is a set of planning operators such that π(O)
and η(O) are consistent. A step O is enabled in state S iﬀπ(O) ⊆S. If a step
O is enabled in a state S, then the occurrence of O in S leads to a new state S′,
in symbols S[O⟩S′. A literal l is caused by a step transition S[O⟩S′ iﬀat least
one of the following conditions holds:
– l ∈η(O), i.e. l is a deterministic eﬀect of some planning operator,
– l ∈ν(O) and l ∈S′, i.e. l is a non-deterministic eﬀect of some planning
operator,
– there exists l ←l1, . . . , ln ∈HP with li ∈S′ for i = 1, . . . , n, i.e. l is the
head of a rule in HP whose body holds in S′,
– l ∈S and l ∈S′, i.e. the truth-value of l is left untouched by the occurrence
of O.
A step transition S[O⟩S′ is causally explained according to PS iﬀS′ coincides
with the set of literals caused by the occurrence of O in S. An execution path
χ of PS is an alternating sequence of states and steps S0O0S1O1 · · · such that
Si[Oi⟩Si+1 are causally explained step transitions for i ≥0. We use χs(i) and
χo(i) to denote Si and Oi respectively. An execution path χ is initialized iﬀ
χs(0) ∈IP S. A state S is reachable iﬀthere exists an initialized execution path
χ such that χs(i) = S for some i ≥0.
Let PS be a planning system. The set of LTL formulae associated with PS
can be deﬁned analogously to the set of LTL formulae for an access-controlled
workﬂow system (cf. Sect. 3.3) by using facts and planning operators as atomic
propositions. The validity of an LTL formula φ of PS on an execution trace
also closely follows the one given for access-controlled workﬂow systems. Finally,
we say that φ is valid in PS, in symbols |=P S φ, iﬀχ |= φ for all initialized
execution paths χ of PS.
4.2
From Access-Controlled Workﬂow Systems to Planning Systems
Let AWS = ⟨WS, ACS⟩be an access-controlled workﬂow system over F and T
with WS = ⟨P, F, IWS, T , In, Out, γ, α⟩and ACS = ⟨F, IACS, A, Tα, U, α, H⟩.
The planning system associated with AWS is PS = ⟨FP , IP S, OP , ω, HP ⟩, where
FP is obtained from F by adding a new fact for each place in P,1 IP S contains
a state L ∪{p : M(p) = 1} ∪{¬p : M(p) = 0} for each initial state (M, L) of
AWS, HP = H, OP contains
- a planning operator exec(a, t) for each a ∈A and t ∈Tα,
- a planning operator exec(t) for each transition t ∈T \ Tα,
- a planning operator exec(u) for each policy update u ∈U,
and ω is such that:
- for all a ∈A and t ∈Tα, π(exec(a, t)) = π(t) ∪•t ∪∗t ∪{granted(a, t)},
η(exec(a, t)) = η(t) ∪t• ∪¬•t, ν(exec(a, t)) = ν(t), where ¬F = {¬f : f ∈F}
for F ⊆F;
1 We will not bother distinguishing between a place and the corresponding fact.

76
A. Armando and S.E. Ponta
- for all t ∈T \Tα, π(exec(t)) = •t∪∗t, η(exec(t)) = t•∪¬•t, and ν(exec(t)) = ∅;
- for all u ∈U, π(exec(u)) = π(u), η(exec(u)) = η(u), and ν(exec(u)) = ∅.
Notice that the set of LTL formulae used to specify the properties of PS coincides
with the set of LTL formulae used to specify the properties of AWS.
Theorem 1. Let φ be an LTL formula, AWS be an access-controlled workﬂow
system and PS be the planning system associated with AWS, then |=AWS φ iﬀ
|=P S φ.
This allows us to reduce the problem of checking whether AWS enjoys a given
property φ to the problem of checking whether PS enjoys φ. The proof of the
theorem (available in [12]) amounts to showing that AWS and PS are bisimu-
lation equivalent.
We have developed a prototype implementation of the above translation from
access-controlled workﬂow systems to planning systems within SATMC, a SAT-
based bounded model checker for planning systems. SATMC [5,6] is one of the
back-ends of the AVISPA Tool [13] and has been key to the discovery of seri-
ous ﬂaws in security protocols [14,15]. Given a planning system PS, an LTL
formula φ, and a positive integer k as input, SATMC builds a propositional for-
mula whose models (if any) correspond to initialized execution paths χ of PS
of length at most k such that χ |= φ. (We have recently extended SATMC so to
handle planning systems as deﬁned in Sect. 4.1, i.e. planning systems featuring
rules as well as operators with non-deterministic eﬀects.) The propositional for-
mula is then fed to a state-of-the-art SAT solver and any model found by the
solver is translated back into a counterexample. It can be shown (see, e.g., [16])
that the encoding time (i.e. time required to build the propositional formula) is
polynomial in the size of the planning system and of the goal formula for any
given value of k > 0.
Given an LTL formula φ and an access-controlled workﬂow system AWS, the
translator automatically reduces the problem of checking whether |=AWS φ to
that of checking whether |=P S φ, where PS is the planning system obtained by
applying the translation to AWS. The resulting planning system PS is given as
input to SATMC. Therefore by the addition of the above translator, SATMC is
now capable to model check access-controlled workﬂow systems.
5
Experiments
We have formalized the LOP of Sect. 2 as an access-controlled workﬂow system
AWS in a scenario characterized by the following agents: davide, the director,
maria and marco, managers, pierPaolo, who can act both as preprocessing clerk
and as postprocessing clerk, pierSilvio, who can act both as preprocessing
clerk and as supervisor, pietro, postprocessing clerk, and stefano, supervisor.
(See [12] for more details.) We have then automatically translated AWS into
the corresponding planning system and by using SATMC we have analyzed the
planning system of the LOP w.r.t. the SoD properties (1) and (2).

Model Checking of Security-Sensitive Business Processes
77
ObjSoD for the LOP. Let AWS0 be the access-controlled workﬂow system for the
LOP we presented in Sect. 2. Our ﬁrst experiment was to check whether |=AWS0
(1). SATMC found a counterexample where pierSilvio can execute all the
tasks intRating, extRating and approve through his role supervisor, thereby
violating the property. By inspecting the intermediate states of the trace it is easy
to conclude that the violation occurs if the customer is not industrial, the internal
rating is not ok, and the loan is neither highValue nor lowRisk. Indeed, in this
scenario the permission assignment relation, together with the seniority relation
between supervisor and postprocessor, allows a supervisor to perform all
the critical tasks.
It is easy to see that this violation can be prevented by restricting the per-
mission assignment relation. However this solution has the negative eﬀect of
reducing the ﬂexibility of the business process. We therefore considered an al-
ternative solution based on the idea of implementing in the LOP a mechanism
that prevents an agent to activate a role if she had already executed intRating
and extRating in the same role. Notice that the mechanism has the eﬀect of
restricting the execution paths to those satisfying the following LTL formula:

a∈A

r∈R
G(executed(a, r, intRating) ⇒
G(executed(a, r, extRating) ⇒G ¬ activated(a, r)))
(3)
where R is the set of roles involved in the process and executed(a, r, t) abbre-
viates the formula (granted(a, r, t) ∧X exec(a, t)). Thus instead of changing
AWS 0 into a new access-controlled workﬂow system AWS 1 implementing the
above mechanism and checking whether |=AWS 1 (1), we asked SATMC to check
whether |=AWS 0 ((3) ⇒(1)). SATMC did not ﬁnd the previous counterexample
any more, but found a new one. In the new counterexample the agent respon-
sible for the violation is stefano, who executes intRating and extRating as
supervisor, but can nevertheless execute approve because a manager, maria,
delegates him to approve the document by means of the delegation rule D1 and
this leads to the violation. A further inspection of the intermediate states of the
trace shows that the violation occurs if the customer is industrial, the internal
rating is ok, and the loan is highValue and not lowRisk.
To avoid this new violation we constrained the applicability of rule D1 by
conjoining its applicability condition with the literal ¬ highValue. By changing
AWS 0 in this way we obtained a new access-controlled workﬂow system, say
AWS 2, and asked SATMC to check whether |=AWS 2 ((3) ⇒(1)). SATMC did
not ﬁnd any counterexamples to this formula.
OpSoD for the LOP. We then checked |=AWS2 (2) and SATMC found a new vi-
olation. The violation occurs if the customer is not industrial, the internal rating
is ok, and the loan is lowRisk and not highValue. Notice that in this situation
the loan is lowRisk and extRating is not performed and therefore the ObjSoD
is ensured. However pierSilvio, who is assigned to roles preprocessor and
supervisor, successfully completes the LOP and thus violates the OpSoD. No-
tice that the user and permission assignment relations do not enable pierSilvio
to sign the contract. In fact sign must be executed by an agent who is at least

78
A. Armando and S.E. Ponta
manager. However a manager, maria in the trace, delegates pierSilvio to sign
the document by executing the delegation rule D2.
By inspecting the counterexample leading to the violation, it appears that
pierSilvio inherits the permission to execute prepareContractand intRating
by the more junior role postprocessor. Thus, a possible solution is to modify
the role hierarchy by breaking the seniority relation between supervisor and
postprocessor. An alternative solution is to restrict the applicability condition
of D2 by conjoining it with the fact highValue. In fact, when the loan is not
highValue, the access control policy is less restrictive and the application of D2
must be prevented. SATMC does not ﬁnd any violation in the access-controlled
workﬂow systems obtained by modifying AWS2 in both ways.
The experiments were performed on a notebook with an Intel Core 2 Duo
processor with 1.50GHz clock and 2GB of RAM memory. All the vulnerabilities
were found by SATMC in little times: the encoding time ranges from 2.08 sec
for |=AWS0 (1) to 6.43 sec for |=AWS2 (2) while the solving time is less than 0.5
sec for all the experiments considered.
6
Related Work
An approach to the automatic analysis of security-sensitive business processes
is put forward in [1]. The paper shows that business processes with RBAC poli-
cies and delegation can be formally speciﬁed as transition systems and that SoD
properties can be formally expressed as LTL formulae specifying the allowed be-
haviors of the transition systems. The viability of the approach is shown through
its application to the LOP and the NuSMV model checker is used to carry out
the veriﬁcation. Our approach provides the user with a level of abstraction which
is much closer to the process being modeled and provides a number of important
advantages including (i) the separate speciﬁcation of the workﬂow and of the ac-
cess control policy and (ii) the formal speciﬁcation of the security properties as
LTL formulae that specify the allowed behavior of the access-controlled workﬂow
system. This is not the case in the approach presented in [1], where even small
changes in the workﬂow or in the access control policy may aﬀect the speciﬁca-
tion of the whole transition system and the speciﬁcation of the security property
is relative to the (low level) transition system. In our approach the compilation
of the access controlled workﬂow system and of the expected security properties
into the corresponding planning system and properties (resp.) can be done au-
tomatically and proved correct once and for all as we have done in this paper.
Our approach therefore greatly simpliﬁes the speciﬁcation process, reduces the
semantic gap, and considerably reduces the probability of introducing bugs in
the speciﬁcation.
A formal framework that integrates RBAC into the semantics of BPEL and
uses the SAL model checker to analyze SoD properties as well as to synthesize
a resource allocation plan has been presented in [2]. However the approach sup-
ports the RBAC model with tasks rigidly associated with speciﬁc roles, while our
approach supports the speciﬁcation of a wide variety of access control models

Model Checking of Security-Sensitive Business Processes
79
and policy updates. Moreover the semantics of BPEL adopted in [2] does not
take into account the global state of the process and assumes an interleaving
semantics. This is not the case in our approach as it accounts for a global state
that can be aﬀected by the execution of the tasks as well as for the simultaneous
execution of actions.
An approach to the combined modeling of business workﬂows with RBAC
models is presented in [3]. The paper proposes an extended ﬁnite state machine
model that allows for the model checking of SoD properties by using the model
checker SPIN. It considers a simple RBAC model only based on previous acti-
vation (or non-activation) of roles and it does not take into account delegation.
Another approach to the automated analysis of business processes is presented
in [17]. The paper proposes to model workﬂows and security policies in a security
enhanced BPMN notation, a formal semantics based on Coloured Petri nets,
an automatic translation from the process model into the Promela speciﬁcation
language and the usage of SPIN to verify SoD properties. However no provision is
made for the assignment of an agent to multiple roles, role hierarchy, delegation,
and the global state of the process.
An approach based on model checking for the analysis and synthesis of ﬁne-
grained security policies is presented in [18]. The framework supports the spec-
iﬁcation of complex policies (including administrative policies), but mutual ex-
clusion in the user assignment relation is not supported, nor it is possible to
express role inheritance. Moreover, the modeling of the workﬂow is not in the
scope of [18], whereas modeling and analyzing the interplay of the workﬂow and
the access control policy is one of the main objectives of our work.
7
Conclusions and Future Work
We have presented a new approach to the formal modeling and automatic analy-
sis of security-sensitive business processes that greatly simpliﬁes the speciﬁcation
activity while retaining full automation. Our approach improves upon the state-
of-the-art by supporting the separate speciﬁcation of the workﬂow and of the
security policy as well as a translation into a speciﬁcation framework amenable
to automatic analysis. Our experiments conﬁrm that model checking can be very
eﬀective not only to detect security ﬂaws but also to identify possible solutions.
The analysis of business processes via reduction to planning is currently being
implemented and integrated by SAP within SAP NetWeaver BPM [19] by using
SATMC as back-end. The version of SATMC currently integrated supports a
simpler deﬁnition of planning systems than that given in Sect. 4.1 (i.e. it does
not feature rules nor operators with non-deterministic eﬀects). The integration
of the latest version of SATMC will enable the SAP platform to support the
advanced features that have been presented in this work.
References
1. Schaad, A., Lotz, V., Sohr, K.: A model-checking approach to analysing organi-
sational controls in a loan origination process. In: SACMAT, pp. 139–149. ACM,
New York (2006)

80
A. Armando and S.E. Ponta
2. Cerone, A., Xiangpeng, Z., Krishnan, P.: Modelling and resource allocation plan-
ning of BPEL workﬂows under security constraints. TR 336, UNU-IIST (2006),
http://www.iist.unu.edu/
3. Dury, A., Boroday, S., Petrenko, A., Lotz, V.: Formal veriﬁcation of business work-
ﬂows and role based access control systems. In: SECURWARE 2007, pp. 201–210
(2007)
4. Peterson, J.L.: Petri Net Theory and the Modeling of Systems. Prentice Hall,
Englewood Cliﬀs (1981)
5. Armando, A., Compagna, L.: SATMC: a SAT-based model checker for security
protocols. In: Alferes, J.J., Leite, J. (eds.) JELIA 2004. LNCS (LNAI), vol. 3229,
pp. 730–733. Springer, Heidelberg (2004)
6. Armando, A., Compagna, L.: Sat-based model-checking for security protocols anal-
ysis. In: IJIS. Springer, Heidelberg (2007)
7. OASIS: Web Services Business Process Execution Language Version 2.0 (2007),
http://docs.oasis-open.org/wsbpel/2.0/OS/wsbpel-v2.0-OS.html
8. Sandhu, R.S., Coyne, E.J., Feinstein, H.L., Youman, C.E.: Role-based access con-
trol models. Computer 29(2), 38–47 (1996)
9. Atluri, V., Warner, J.: Supporting conditional delegation in secure workﬂow man-
agement systems. In: SACMAT, pp. 49–58. ACM Press, New York (2005)
10. Giunchiglia, E., Lifschitz, V.: An action language based on causal explanation:
Preliminary report. In: AAAI 1998, pp. 623–630. AAAI Press, Menlo Park (1998)
11. Ferraris, P., Giunchiglia, E.: Planning as satisﬁability in nondeterministic domains.
In: AAAI 2000 and IAAI 2000, pp. 748–753. AAAI Press / The MIT Press (2000)
12. Armando, A., Ponta, S.E.: Model checking of security-sensitive business processes
(2009), http://www.ai-lab.it/serena/tr090724.pdf
13. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Drielsma, H.P., He´am, P.C., Kouchnarenko, O., Mantovani, J., M¨odersheim, S., von
Oheimb, D., Rusinowitch, M., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.:
The AVISPA Tool for the Automated Validation of Internet Security Protocols and
Applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576,
pp. 281–285. Springer, Heidelberg (2005)
14. Armando, A., Carbone, R., Compagna, L.: LTL model checking for security pro-
tocols. In: CSF-20, pp. 385–396. IEEE Computer Society, Los Alamitos (2007)
15. Armando, A., Carbone, R., Compagna, L., Cu´ellar, J., Tobarra, M.L.: Formal
analysis of SAML 2.0 web browser single sign-on: breaking the SAML-based single
sign-on for google apps. In: FMSE, pp. 1–10. ACM, New York (2008)
16. Kautz, H., McAllester, H., Selman, B.: Encoding Plans in Propositional Logic. In:
KR, pp. 374–384 (1996)
17. Wolter, C., Miseldine, P., Meinel, C.: Veriﬁcation of business process entailment
constraints using SPIN. In: Massacci, F., Redwine Jr., S.T., Zannone, N. (eds.)
ESSoS 2009. LNCS, vol. 5429. Springer, Heidelberg (2009)
18. Guelev, D.P., Ryan, M., Schobbens, P.Y.: Model-checking access control policies.
In: Zhang, K., Zheng, Y. (eds.) ISC 2004. LNCS, vol. 3225, pp. 219–230. Springer,
Heidelberg (2004)
19. SAP NetWeaver Business Process Management,
http://www.sap.com/platform/netweaver/components/sapnetweaverbpm/
index.epx

Analysing the Information Flow Properties of
Object-Capability Patterns
Toby Murray and Gavin Lowe
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford, OX1 3QD, United Kingdom
{toby.murray,gavin.lowe}@comlab.ox.ac.uk
Abstract. We consider the problem of detecting covert channels within
security-enforcing object-capability patterns. Traditional formalisms for
reasoning about the security properties of object-capability patterns re-
quire one to be aware, a priori, of all possible mechanisms for covert
information ﬂow that might be present within a pattern, in order to de-
tect covert channels within it. We show how the CSP process algebra,
and its model-checker FDR, can be applied to overcome this limitation.
1
Introduction
The object-capability model [9] is a security architecture for the construction of
software systems that naturally adhere to the principle of least authority [9],
a reﬁnement of Saltzer and Schroeder’s principle of least privilege [19]. Several
current research projects, including secure programming languages like E [9],
Joe-E [8] and Google’s Caja [10], and microkernel operating systems like the
Annex Capability Kernel [5] and seL4 [1], implement the object-capability model
to provide platforms for cooperation in the presence of mutual suspicion.
Security properties are enforced in object-capability systems by deploying
security-enforcing abstractions, called patterns, much in the same way that a
program’s ordinary functional properties are implemented by ordinary program-
ming abstractions and design patterns. It is therefore very important to be able to
understand precisely the security properties that an individual object-capability
pattern does, and does not, enforce.
For systems in which conﬁdentiality is a primary concern, we are most often
interested in those security properties that capture the ways in which informa-
tion may ﬂow within them. In object-capability applications that involve conﬁ-
dentiality, the information ﬂow properties of security-enforcing object-capability
patterns are of vital importance. In particular, it is necessary to be able to detect
the existence of covert channels within object-capability patterns.
Whilst the formal analysis of object-capability patterns has received some
attention [20], the previous formalisms that were employed require all eﬀects that
are to be reasoned about to be explicitly included in any model of an object-
capability pattern that is being analysed. Thus, in order for covert channels
to be detected within an object-capability pattern, the mechanisms for covert
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 81–95, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

82
T. Murray and G. Lowe
information propagation must be explicitly modelled. This requires one who
wishes to detect covert channels in a pattern to be aware, a priori, of the possible
mechanisms for covert information ﬂow within it.
In this paper, we show how the CSP process algebra [14], and its model
checker FDR [4], can be applied to model object-capability patterns and detect
covert channels within them, without forcing the programmer to enumerate the
mechanisms by which information may covertly propagate. We adopt CSP for
modelling object-capability systems, as opposed to what others might consider to
be a more natural formalism such as the π-calculus, because we can use FDR to
automatically check our properties via CSP’s formal theory of reﬁnement which,
as will become evident, is integral to our understanding of both object-capability
systems and information ﬂow within them.
We conclude this section by brieﬂy explaining the object-capability model and
the fragment of CSP used in this paper. Further details about CSP can be found
in [14]. In Section 2, we explain how object-capability systems can be modelled
in CSP. In doing so, we present an example model of a Data-Diode pattern,
from [9], that is designed to allow data to ﬂow from low-sensitivity objects to
high-sensitivity ones, whilst preventing data propagating in the reverse direction.
In Section 3, we give a general deﬁnition for information ﬂow security for object-
capability systems modelled in CSP and argue that the information ﬂow property
Weakened RCFNDC for Compositions [12], which can be automatically tested
in FDR, is an appropriate test to apply to such systems. Applying this test to
our model from Section 2, we ﬁnd that it does indeed contain covert channels,
before showing how to reﬁne the model to an implementation that passes the
test. The analysis here considers only a small instance of the Data-Diode pattern
composed with a handful of other objects. Therefore, in Section 4, we show how
to generalise our results to systems of arbitrary size in which objects may create
arbitrary numbers of other objects, applying the theory of data-independence [6].
Finally, we conclude and consider related work in Section 5.
Some proofs are omitted but appear in [11]. Thanks to Bill Roscoe for useful
discussions about data-independence, and to the anonymous reviewers.
The Object-Capability Model. The object-capability model [9] is a model
of computation and security that aims to capture the semantics of many actual
object-based programming languages and capability-based systems, including all
of those mentioned in Section 1. An object-capability system is an instance of the
model and comprises just a collection of objects, connected to each other by capa-
bilities. An object is a protected entity comprising state and code that together
deﬁne its behaviour. An object’s state includes both data and the capabilities
it possesses. A capability, c, is an unforgeable object reference that allows its
holder to send messages to the object it references by invoking c.
In an object-capability system, the only overt means for objects to interact is
by sending messages to each other. Capabilities may be passed between objects
only within messages. In practice, object o can pass one of its capabilities, c,
directly to object p only by invoking a capability it possesses that refers to p,

Analysing the Information Flow Properties
83
including c in the invocation. This implies that capabilities can be passed only
between objects that are connected, perhaps via intermediate objects.
Each object may expose a number of interfaces, known as facets. A capability
that refers to an object, o, also identiﬁes a particular facet of o. This allows the
object to expose diﬀerent functionality to diﬀerent clients by handing each of
them a capability that identiﬁes a separate facet, for example.
An object may also create others. In doing so, it must supply any resources
required by the newly created object, including its code and any data and capa-
bilities it is to possess initially. Hence, a newly created object receives its ﬁrst
capabilities solely from its parent. When creating an object, the parent exclu-
sively receives a capability to the child. Thus, an object’s parent has complete
control over those objects the child may come to interact with in its lifetime.
This is the basis upon which mandatory security policies can be enforced [9].
In object-capability operating systems like seL4, each process may be thought
of as a separate object. In object-capability languages like Caja, objects are akin
to those from object-oriented languages; capabilities are simply object references.
CSP. A system modelled in CSP comprises a set of concurrently executing
processes that execute by performing events. Processes communicate by syn-
chronising on common events, drawn from the set Σ of all visible events.
The process ST OP represents deadlock and cannot perform any events. The
process ?a : A →Pa is initially willing to perform all events from the set A and
oﬀers its environment the choice of which should be performed. Once a particular
event, a ∈A, has been performed, it behaves like the process Pa.
CSP allows multi-part events to be deﬁned, where a dot is used to separate
each part of an event. Suppose we deﬁne the set of events {plot.x.y | x, y ∈N}.
Then the process plot?x?y →ST OP oﬀers all plot events whilst the process
plot?x : {1, . . . , 5}!3 →ST OP oﬀers all events from {plot.x.3 | x ∈{1, . . ., 5}}.
{|c1. . . . .ck|} denotes the set of events whose ﬁrst k components are c1, . . . , ck.
The process P □Q can behave like either the process P or the process Q and
oﬀers its environment the initial events of both processes, giving the environment
the choice as to which process it behaves like. The process P ⊓Q can also behave
like either P or Q but doesn’t allow the environment to choose which; instead,
it makes this choice internally. P \ A denotes the process obtained when P is
run but all occurrences of the events in A are hidden from its environment.
The process P ∥
A
Q runs the processes P and Q in parallel forcing them to
synchronise on all events from the set A. The process S = ∥1≤i≤n(Pi, Ai) is the
alphabetised parallel composition of the n processes P1, . . . , Pn on their corre-
sponding alphabets A1, . . . , An. Each process Pi may perform events only from
its alphabet Ai, and each event must be synchronised on by all processes in
whose alphabet it appears. P1 A1∥A2 P2 is equivalent to ∥1≤i≤2(Pi, Ai).
A process diverges when it performs an inﬁnite number of internal τ events.
A process terminates by performing the special termination event ✓. In this
paper, we restrict our attention to processes that never diverge nor terminate.

84
T. Murray and G. Lowe
Given a CSP process P, traces(P) denotes the set that contains all ﬁnite
sequences of visible events (including all preﬁxes thereof) that it can perform. A
stable-failure is a pair (s, X) and denotes a process performing the ﬁnite sequence
of events s and then reaching a stable state in which no internal τ events can
occur, at which point all events from X are unavailable, i.e. X can be refused.
We write failures(P) for the set that contains all stable-failures of the process
P. We write divergences(P) for the set of traces of P after which it can diverge.
For all of the processes P that we consider in this paper, divergences(P) = {}.
For any divergence-free process P, traces(P) = {s | (s, X) ∈failures(P)}.
CSP’s standard denotational semantic model is the failures-divergences
model [14]. Here a process P is represented by the two sets: failures(P)
and divergences(P). One CSP process P is said to failures-divergences re-
ﬁne another Q, precisely when failures(P) ⊆failures(Q) ∧divergences(P) ⊆
divergences(Q). In this case, we write Q ⊑P.
Sequences are written between angle brackets; ⟨⟩denotes the empty sequence.
sˆt denotes the concatenation of sequences s and t. s \ H denotes the sequence
obtained by removing all occurrence of events in the set H from the sequence s.
s `| H denotes the sequence obtained by removing all non-H events from s.
2
Modelling Object-Capability Systems in CSP
In this section, we describe our approach to modelling object-capability systems
in CSP. Note that we ignore the issue of object creation for now. This will be
handled later on in Section 4.
We model an object-capability system System that comprises a set Object
of
objects
as
the
alphabetised
parallel
composition
of
a
set
of
pro-
cesses {behaviour(o) | o ∈Object} on their corresponding alphabets {α(o) | o ∈
Object}. So System = ∥o∈Object(behaviour(o), α(o)).
The facets of each object o ∈Object are denoted facets(o). We restrict our
attention to those well-formed systems in which facets(o) ∩facets(p) ̸= {} ⇒
o = p. Recall that an individual capability refers to a particular facet of a
particular object. Hence, we deﬁne the set Cap = {facets(o) | o ∈Object} that
contains all entities to which capabilities may refer.
The events that each process behaviour(o) can perform represent it sending
and receiving messages to and from other objects in the system. We deﬁne events
of the form f1.f2.op.arg to denote the sending of a message from the object with
facet f1 to facet f2 of the object with this facet, requesting it to perform operation
op, passing the argument arg and a reply capability f1, which can be used later
to send back a response. Here f1, f2 ∈Cap. Arguments are either capabilities,
data or the special value null, so arg ∈Cap ∪Data ∪{null}, for some set Data of
data. An operation op comes from the set {Call, Return}. These operations model
a call/response remote procedure call sequence in an object-capability operating
system or a method call/return in an object-capability language.
The alphabet of each object o ∈Object contains just those events involving o.
Hence, α(o) = {|f1.f2 | f1, f2 ∈Cap ∧(f1 ∈facets(o) ∨f2 ∈facets(o))|}.

Analysing the Information Flow Properties
85
We require that the process behaviour(o) representing the behaviour of each
object o ∈Object adheres to the basic rules of the object-capability model, such
as not being able to use a capability it has not legitimately acquired. We codify
this by deﬁning the most general and nondeterministic process that includes all
permitted behaviours (and no more) that can be exhibited by an object o. Letting
facets = facets(o) denote the set that comprises o’s facets, and caps ⊆Cap and
data ⊆Data denote the sets of capabilities and data that o initially possesses,
the most general process that includes all behaviours permitted by the object-
capability model that o may perform is denoted Untrusted(facets, caps, data).
Untrusted(facets, caps, data) =
⎛
⎜
⎝
?me : facets?c : caps ∪facets?op?arg : caps ∪data ∪{null} →
Untrusted(facets, caps, data) □
?from : Cap −facets?me : facets?op?arg →
Untrusted(facets, caps ∪({arg, from} ∩Cap), data ∪({arg} ∩Data))
⎞
⎟
⎠
⊓ST OP.
This object can invoke only those capabilities c ∈caps ∪facets that it pos-
sesses. In doing so it requests an operation op, and may include only those argu-
ments arg ∈caps ∪data ∪{null} it has, along with a reply capability me ∈facets
to one of its facets. Having done so, it returns to its previous state.
This object can also receive any invocation from any other, where the reply
capability included in the invocation is from ∈Cap −facets, to one of its facets
me ∈facets, requesting an arbitrary operation op, and containing an arbitrary
argument arg. If such an invocation occurs, the object may acquire the reply
capability from, as well as any capability or datum arg in the argument. This
process may also deadlock at any time, making it maximally nondeterministic.
The behaviour behaviour(o) of an object o ∈Object, whose initial capabilities
and data are caps(o) and data(o) respectively, is then valid if and only if all
behaviours it contains are present in Untrusted(facets(o), caps(o), data(o)). This
leads to the following deﬁnition of a valid object-capability system.
Deﬁnition 1 (Object-Capability System). An object-capability system is a
tuple (Object, behaviour, facets, Data), where Object, behaviour, facets and Data
are as discussed above and, letting Cap = {facets(o) | o ∈Object}, there exist
functions caps : Object →P Cap and data : Object →P Data that assign
minimal initial capabilities and data to each object so that, for each o ∈Object,
Untrusted(facets(o), caps(o), data(o)) ⊑behaviour(o).
2.1
An Example Pattern
We illustrate these concepts by modelling the Data-Diode pattern [9, Fig-
ure 11.2], which is designed to allow low-sensitivity objects to send data to
to high-sensitivity ones whilst preventing information ﬂowing the other way.

86
T. Murray and G. Lowe
Fig. 1. A system in which to analyse the Data-Diode pattern. Bold circles indicate
objects with arbitrary behaviour.
A data-diode is an object that has two facets, a read-facet and a write-
facet1. It stores a single datum and begins life holding some initial value.
Invoking its read-facet causes it to return its current contents. Invoking its
write-facet with an argument causes it to replace its current contents with the ar-
gument. We model a data-diode with read-facet readme and write-facet writeme
that initially contains the datum val from the set Data as the CSP pro-
cess ADataDiode(readme, writeme, val), deﬁned as follows.
ADataDiode(readme, writeme, val) =
?from : Cap −{readme, writeme}!readme!Call!null →
readme!from!Return!val →ADataDiode(readme, writeme, val) □
?from : Cap −{readme, writeme}!writeme!Call?newVal : Data →
writeme!from!Return!null →ADataDiode(readme, writeme, newVal).
Observe that this process passes Data items only, refusing all Cap arguments.
To analyse this pattern, we instantiate it in the context of the object-capability
system System depicted in Figure 1. Here, we see a data-diode, DataDiode, with
read- and write-facets DDReader and DDWriter respectively. An arbitrary high-
sensitivity object High has a capability to the data-diode’s read-facet, allowing it
to read data written by an arbitrary low-sensitivity object Low, which has a capa-
bility to the data-diode’s write-facet. High and Low possess the data HighDatum
and LowDatum respectively.
Let
Object
=
{High, DataDiode, Low},
HighData
=
{HighDatum},
LowData = {LowDatum}, Data = HighData ∪LowData, facets(DataDiode) =
{DDReader, DDWriter} and facets(other) = {other} for other ̸= DataDiode. The
process System is then deﬁned as explained earlier using the behaviours:
behaviour(DataDiode) = ADataDiode(DDReader, DDWriter, null),
behaviour(High) =
Untrusted(facets(High), facets(High) ∪{DDReader}, HighData),
behaviour(Low) = Untrusted(facets(Low), facets(Low) ∪{DDWriter}, LowData).
3
Information Flow in Object-Capability Patterns
Performing some basic reﬁnement checks in FDR, which test whether certain
events cannot occur in System, reveals that Low cannot obtain any HighData
1 It is unclear whether the read and write interfaces should be implemented as facets
of a single object or as forwarding objects of a composite object. We choose the
former option at this point and will explore the latter in Section 3.1.

Analysing the Information Flow Properties
87
but that High can obtain LowDatum in this system. We now consider how to test
whether, despite preventing this overt ﬂow of data, DataDiode might provide a
covert channel from High to Low. We will argue that the correct property to
apply to System is Weakened RCFNDC for Compositions, introduced in [12].
Information ﬂow properties have been well-studied in the context of process
algebras, including CSP (e.g. [3,2,17,7]). The obvious approach would take one
of these properties and apply it to the process behaviour(DataDiode) to see
whether it allows information to ﬂow from its high interface DDReader to its
low interface DDWriter.
However, this approach doesn’t take into account the constraints imposed
by the object-capability model on the objects like High and Low that may
interact with DDReader and DDWriter. This is because these constraints
are not reﬂected in behaviour(DataDiode) but are instead imposed upon
behaviour(High) and behaviour(Low). For example, observe that initially the pro-
cess behaviour(DataDiode) can perform the event High.DDWriter.Call.null; how-
ever, this event cannot be performed in System because it can occur there only
when both High and DataDiode are willing to perform it, and behaviour(High)
cannot perform it initially because High does not initially possess a capability to
DDWriter. In order to get accurate results, therefore, one needs to analyse the
entire system System, using an appropriate information ﬂow property.
Recall that the processes behaviour(High) and behaviour(Low), which both
instantiate the process Untrusted, are purposefully highly nondeterministic, in
order to ensure that each is as general as possible. This makes the entire sys-
tem System very nondeterministic. It has long been recognised that many stan-
dard information ﬂow properties suﬀer from the so-called “reﬁnement paradox”
in which a property holds for a system but can be violated by one of the system’s
reﬁnements. The reﬁnements of a system capture the ways in which nondeter-
minism can be resolved in it. The reﬁnement paradox is dangerous because it
allows a nondeterministic system to be deemed secure when, under some reso-
lution of the system’s nondeterminism, it may actually be insecure [7].
A fail-safe way to avoid the reﬁnement paradox is to apply an information ﬂow
property that is reﬁnement-closed [7]. A property is reﬁnement-closed when, for
every process P, it holds for P only if it holds for all P’s reﬁnements.
While we want to avoid the reﬁnement paradox, reﬁnement-closed proper-
ties are too strong for our purposes. This is because the reﬁnements of a parallel
composition include those in which the resolution of nondeterminism in one com-
ponent can depend on activity that occurs within the system that the component
cannot overtly observe.
For
example,
System
is
reﬁned
by
a
process
that
has
the
trace
⟨High.High.Call.High, Low.Low.Call.null⟩
but
also
has
the
stable-failure
(⟨⟩, {Low.Low.Call.null}). This reﬁnement means that System fails a num-
ber of reﬁnement-closed information ﬂow properties, e.g. Roscoe’s Lazy
Independence [14, Section 12.4] and Lowe’s Reﬁnement-Closed Failures Non-
Deducibility on Compositions [7]. These two behaviours arise because of the
nondeterminism in Low: initially Low may either perform Low.Low.Call.null or

88
T. Murray and G. Lowe
may refuse it, depending on how this nondeterminism is resolved. In the trace
above, where High performs the event High.High.Call.High, Low’s nondetermin-
ism is resolved such that Low.Low.Call.null occurs; while in the stable-failure,
where High doesn’t perform its event, this nondeterminism in Low is resolved
the other way. The resolution of the nondeterminism in Low here thus depends
on whether High has performed its event, in which it interacts with just itself.
A system that exhibits both of these behaviours therefore allows High’s inter-
actions with just itself to somehow inﬂuence Low. In such systems it is impossible
to talk sensibly about the information ﬂow properties of the Data-Diode pattern.
We see that in general, one cannot talk sensibly about the information ﬂow
properties of object-capability patterns without assuming that the only way for
one object to directly inﬂuence another is by sending it a message or receiving
one from it, since it is only overt message passing that any pattern can hope to
control. Thus, in any system, we assume that the resolution of nondeterminism
in any object can be inﬂuenced only by the message exchanges in which it has
partaken before the nondeterminism is resolved.
Without specifying how the nondeterminism in any object may be resolved
after it has engaged in some sequence s of message exchanges, this therefore im-
plies that whenever it performs s, the nondeterminism should be resolved con-
sistently [12]. Two resolutions of the nondeterminism in a process after it has
performed s are inconsistent when it can perform some event e in one but refuse e
in the other. Under this deﬁnition, the two diﬀerent resolutions of the nonde-
terminism in Low above, depending on whether High has performed its event
that doesn’t involve Low, are inconsistent: in each case, Low performs/refuses
the event Low.Low.Call.null after performing no others.
We therefore conﬁne ourselves to the ways of resolving the nondeterminism
in each object in which this kind of inconsistency does not arise. Note that these
are precisely the deterministic reﬁnements of each object, under the standard
deﬁnition of determinism for CSP processes.
Deﬁnition 2 (Determinism). A divergence-free process P is said to be deter-
ministic, written det(P), iﬀ̸∃s, e • sˆ⟨e⟩∈traces(P) ∧(s, {e}) ∈failures(P).
With this in mind, we seek an information ﬂow property that holds for a sys-
tem System = ∥o∈Object(behaviour(o), α(o)) just when those reﬁnements of
System, in which the nondeterminism in each object is resolved to produce
a deterministic process, are deemed secure. Any such deterministic compo-
nentwise reﬁnement may be written as System′ = ∥o∈Object(bo, α(o)) where
∀o ∈Object • behaviour(o) ⊑bo ∧det(bo). Let DCRef (System) denote the
set of all deterministic componentwise reﬁnements of System. Any such reﬁne-
ment System′ ∈DCRef (System) will itself be deterministic [14]. Many infor-
mation ﬂow properties, which might otherwise disagree, agree when applied to
deterministic processes. Hence, given any such property Prop, we arrive at the
following deﬁnition of information ﬂow security for object-capability systems.

Analysing the Information Flow Properties
89
Deﬁnition 3. An
object-capability
system
captured
by
the
CSP
pro-
cess System
= ∥o∈Object(behaviour(o), α(o))
is secure under componen-
twise reﬁnement with respect to the information ﬂow property Prop
iﬀ
∀System′ ∈DCRef (System) • Prop(System′).
3.1
Testing Information Flow
The information ﬂow property Weakened RCFNDC for Compositions [12] is
equivalent to Deﬁnition 3 when Prop is Lowe’s Reﬁnement-Closed Failures Non-
Deducibility on Compositions [7] (RCFNDC). RCFNDC is equivalent when
applied to deterministic processes to a number of standard information ﬂow prop-
erties, including [16] at least all those that are no stronger than Roscoe’s Lazy In-
dependence [14, Section 12.4] and no weaker than Ryan’s traces formulation of
noninterference [18, Equation 1]. Therefore, we adopt Weakened RCFNDC and
its associated automatic reﬁnement check [12] to test for information ﬂow here.
Like similar information ﬂow properties, given two sets H and L that partition
the alphabet of a system, Weakened RCFNDC tests whether the occurrence of
events from H can inﬂuence the occurrence of events from L. In [12], it is shown
that any divergence-free alphabetised parallel composition S = ∥1≤i≤n(Pi, Ai)
satisﬁes Weakened RCFNDC, written WRCFNDC(S), iﬀ:
̸∃s, l • s `| H ̸= ⟨⟩∧l ∈L ∧
	
sˆ⟨l⟩∈traces(S) ∧s \ H ∈traces(S) ∧
∃i • l ∈Ai ∧s `| Ai ̸= s \ H `| Ai ∧(s \ H `| Ai, {l}) ∈failures(Pi)

∨
	
s \ Hˆ⟨l⟩∈traces(S) ∧s ∈traces(S) ∧
∃i • l ∈Ai ∧s `| Ai ̸= s \ H `| Ai ∧(s `| Ai, {l}) ∈failures(Pi)

.
(1)
Let H = {|h.DDReader, DDReader.h, h.h′ | h, h′ ∈facets(High)|} denote the set
of events that represent High interacting with DDReader and itself. Similarly let
L = {|l.DDWriter.Call.arg, DDWriter.l.Return.null, l.l′ | l, l′ ∈facets(Low)|}. Then
a reﬁnement check in FDR reveals that System from Section 2.1 can perform no
events outside of H ∪L. This implies, for example, that neither High nor Low
can obtain a capability to the other. Therefore, H and L partition the eﬀective
alphabet of System.
Applying the reﬁnement check for Weakened RCFNDC to System with these
deﬁnitions of H and L, using FDR, reveals that Weakened RCFNDC doesn’t
hold. Interpreting the counter-example returned from FDR, we see that System
can perform the trace ⟨Low.DDWriter.Call.LowDatum⟩but also has the failure
(⟨High.DDReader.Call.null⟩, {Low.DDWriter.Call.LowDatum}). This indicates that
initially Low can invoke DDWriter but that if High invokes DDReader, it can cause
Low’s invocation to be refused. This occurs because DataDiode cannot service
requests from High and Low at the same time. This constitutes a clear covert
channel, since High can signal to Low by invoking DDReader which alters whether
Low’s invocation is accepted.
Low may be unable to observe this covert channel in some object-capability
systems, e.g. those in which a sender of a message is undetectably blocked until

90
T. Murray and G. Lowe
the receiver is ready to receive it. For this kind of system, one might wish to re-
place Prop with another property, such as Focardi and Gorrieri’s Traces NDC [3],
that detects only when high events can cause low events to occur, rather than
also detecting when they can prevent them from occurring as happens in the
counter-example above. Modifying Weakened RCFNDC to do so simply involves
removing the second disjunct from Equation 1. However, we choose to make the
conservative assumption that this counter-example represents a valid fault.
Correcting the fault here involves modifying the data-diode implementation
so that its interfaces for writing and reading, DDWriter and DDReader, can be
used simultaneously. We do so by promoting these interfaces from being facets
of a single process to existing as individual processes in their own right. These
processes simply act now as proxies that forward invocations to the facets of an
underlying ADataDiode process, as depicted in Figure 2.
The behaviour of a proxy me that forwards invocations it receives using the
capability target is given by the process AProxy(me, target) deﬁned as follows.
AProxy(me, target) =?from : Cap −{me}!me!Call?arg : Data ∪{null} →
me!target!Call!arg →target!me!Return?res : Data ∪{null} →
me!from!Return!res →AProxy(me, target).
The data-diode is now a composite of three entities, DDReader, DDWriter
and DataDiode, and as such is referred to as DDComposite. We model
the
system
depicted
in
Figure
2
as
an
object-capability system
com-
prising
the
objects
from
Object
=
{High, DDComposite, Low},
where
facets(DDComposite)
=
{DDReader, DDWriter, DDR, DDW}
and,
letting
R = {|DDReader.x, x.DDReader | x ∈facets(DDComposite) −{DDReader}|},
W = {|DDWriter.x, x.DDWriter | x ∈facets(DDComposite) −{DDWriter}|},
DD = ADataDiode(DDR, DDW, null) and the other deﬁnitions be as before,
behaviour(DDComposite) =
	
(AProxy(DDReader, DDR) ∥
R
DD) ∥
W
AProxy(DDWriter, DDW)

\ (R ∪W).
DDComposite is formed by taking the two proxies, DDReader and DDWriter, and
composing them in parallel with DataDiode, whose read- and write-interfaces
are now DDR and DDW respectively. Notice that we then hide the internal
communications within DDComposite since these are not visible to its outside
environment and it is unclear how to divide these events between the sets H
and L. FDR can be used to check that this system, System, satisﬁes Deﬁnition 1.
Fig. 2. An improved Data-Diode implementation

Analysing the Information Flow Properties
91
Performing the appropriate reﬁnement checks in FDR reveal that High can
acquire LowDatum but Low cannot acquire any HighData, and that System can
perform no events outside of H ∪L, as before. FDR reveals that Weakened
RCFNDC holds for System. Hence, we are unable to detect any covert channels
in this model of the improved Data-Diode implementation.
4
Generalising the Results
We have veriﬁed this improved Data-Diode implementation in the context of
only a handful of other objects and in the absence of object creation. In this
section, we show how to generalise our analysis to all systems that have the
form of Figure 3, and have arbitrary HighData and LowData. Here, the objects
within each cloud can be interconnected in any way whatsoever; however, the
only capability to an object outside of the high object cloud that each high object
may possess is DDReader. The same is true for the low objects and DDWriter.
This ﬁgure captures all systems containing an arbitrary number of high- and low-
sensitivity objects and, thus, all those in which each object may create arbitrary
numbers of others that share its level of sensitivity.
Roughly, the approach we take is to show that the improved system analysed
in the previous section is a safe abstraction of all systems captured by Figure 3,
such that if the safe abstraction is deemed secure then so will all of the systems
it abstracts. For one system System′ = ∥o∈Object′(behaviour ′(o), α′(o)) to be a
safe abstraction of another System = ∥o∈Object(behaviour(o), α(o)), we require
that if System′ is deemed secure, then so must System.
Recall that, by Deﬁnition 3, System is secure iﬀProp(SystemD) holds for
all SystemD ∈DCRef (System) for some information ﬂow property Prop. We
therefore insist that in order for System′ to be a safe abstraction of System, that
each SystemD is also present in DCRef (System′).
Deﬁnition 4 (Safe Abstraction). Given any System′ and System as above,
System′ is a safe abstraction of System iﬀDCRef (System) ⊆DCRef (System′).
We now show that each system System captured by Figure 3 can be safely
abstracted by a system System′ of the form of Figure 2. We form System′ by
taking each cloud of objects in System and aggregating all of the objects in the
Fig. 3. Generalising the results

92
T. Murray and G. Lowe
cloud into a single object in System′. In order to be a proper aggregation, each
object from System′ must have all facets, capabilities, data and behaviours of all
the objects from System that it aggregates. We formally capture that System′
is an aggregation of System via a surjection Abs : Object →Object′ that maps
each object of System to the object that aggregates it in System′.
Deﬁnition 5 (Aggregation).
Let
(Object, behaviour, facets, Data)
and
(Object′, behaviour ′, facets′, Data) be two object-capability systems with iden-
tical sets of data, captured by System = ∥o∈Object(behaviour(o), α(o)) and
System′ = ∥o∈Object′(behaviour ′(o), α′(o)) respectively. Then the second is an
aggregation of the ﬁrst when there exists some surjection Abs : Object →Object′
such that for all o′ ∈Object′, facets′(o′) = {facets(o) | o ∈Abs−1(o′)} and
∀s ∈traces(System) • ∀X ∈P Σ •
(s `| α′(o′), X) ∈failures(∥o∈Abs−1(o′)(behaviour(o), α(o))) ⇒
(s `| α′(o′), X) ∈failures(behaviour ′(o′)),
where Abs−1(o′) = {o | o ∈Object ∧Abs(o) = o′}.
The proof of the following theorem requires some technical results beyond the
scope of this paper; given limitations on space, it can be found in [11].
Theorem 1. Let System and System′ capture two object-capability systems as
stated in Deﬁnition 5. Then if System′ is an aggregation of System, it is also a
safe abstraction of System.
We
claim
that
any
ﬁnite
collection
K
⊆
Object
of
objects
∥o∈K(behaviour(o), α(o)),
can
be
aggregated
by
a
single
object
with
behaviour
Untrusted(
o∈K facets(o), 
o∈K caps(o), 
o∈K data(o))
that
has all of their capabilities, data and facets. Brieﬂy, by Deﬁnition 1,
behaviour(o)
⊑
Untrusted(facets(o), caps(o), data(o)) for each o
∈
K for
some sets caps(o) and data(o) of capabilities and data that it possesses initially.
Further Untrusted(facets(o)∪facets(o′), caps(o)∪caps(o′), data(o)∪data(o′)) ⊑
Untrusted(facets(o), caps(o), data(o))
α(o)∥α(o′)
Untrusted(facets(o′), caps(o′),
data(o′)). The claim then follows by induction on the size of K.
So consider any system that has the form of Figure 3 and let T denote
the facets of the high objects, U the facets of the low objects and V
=
HighData ∪LowData, i.e. V = Data. Then this system can be safety abstracted
by a system SystemT,U,V of the form of Figure 2 in which facets(High) = T ,
facets(Low) = U, HighData = V and LowData = V , so that Data = V . Notice
that we allow High and Low to both possess all data in the safe abstraction in
order to obtain maximum generality. If we can show that SystemT,U,V is secure
for all non-empty choices of T , U and V , by Theorem 1, we can conclude that
the improved Data-Diode implementation is secure in all systems captured by
Figure 3 with arbitrary HighData and LowData.
The theory of data-independence [6] can be applied to show that a property
Prop holds of a process PT , parameterised by some set T , for all non-empty

Analysing the Information Flow Properties
93
choices of T , if Prop(PT ) can be shown for all non-empty T of size N or less,
for some N. N is called the data-independence threshold for T for Prop(PT ).
The theory requires that PT be data-independent in T , meaning roughly that PT
handles members of the type T uniformly, not distinguishing one particular value
of T from another. We apply data-independence theory to show that thresholds
of size 1, 2 and 2 for T , U and V respectively are suﬃcient to demonstrate the
security of SystemT,U,V for all non-empty choices of each set.
We will use the following standard result. Let PT be a process that is data-
independent in some set T and satisﬁes NoEqT for T , meaning that it never
needs to test two values of T for equality. Let φ be a surjection whose domain is T ,
where we write φ(T ) for {φ(t)|t ∈T } and φ−1(X) for {y | y ∈T ∧φ(y) ∈X}.
Then [6, Theorem 4.2.2], lifting φ to events and traces,
{(φ(s), X) | (s, φ−1(X)) ∈failures(PT )} ⊆failures(Pφ(T )).
(2)
Theorem 2. Let ST = ∥1≤i≤n(PT,i, AT,i) be an alphabetised parallel composi-
tion, whose components and alphabets are polymorphically parameterised by some
set T , such that ST and each PT,i are data-independent in T and satisfy NoEqT
for T . Also let HT and LT be two sets polymorphically parameterised by T that
partition the alphabet of ST for all non-empty T . Let W denote the maximum
number of distinct elements of T that appear in any single event from LT . Then
W + 1 is a suﬃcient data-independence threshold for T for WRCFNDC(ST ).
Proof. Assume the conditions of the theorem. Suppose for some T with size
greater than W, ST fails Weakened RCFNDC for HT and LT . Then let
˜T = {˜t0, . . . , ˜tW } for fresh elements ˜t0, . . . , ˜tW . We show that S ˜T fails Weak-
ened RCFNDC for H ˜T and L ˜T .
Let φ : T →˜T be a surjection; we ﬁx the choice of φ below. Lift φ to
events by applying φ to all components of type T . Then φ maps an event in
the alphabet of ST to an event in the alphabet of S ˜T . Also, lifting φ to sets of
events, φ(AT,i) = A ˜T ,i for 1 ≤i ≤n, φ(HT ) = H ˜T and φ(LT ) = L ˜T .
Observe that Sφ(T ) = S ˜T . So, by Equation 2, the presence of certain be-
haviours in ST implies the presence of related behaviours in S ˜T . Recall the
characterisation of Weakened RCFNDC from Equation 1. Suppose ST fails the
ﬁrst disjunct of Equation 1 for HT and LT . We show that S ˜T fails this disjunct
for H ˜T and L ˜T . The second disjunct is handled similarly. Then there exists some
s, l and i ∈{1, . . . , n} such that
s `| HT ̸= ⟨⟩∧l ∈LT ∧sˆ⟨l⟩∈traces(ST ) ∧s \ HT ∈traces(ST ) ∧
l ∈AT,i ∧s `| AT,i ̸= s \ HT `| AT,i ∧(s \ HT `| AT,i, {l}) ∈failures(PT,i).
Let t0, . . . , tk−1 be the distinct members of T that appear in l. Then k ≤W.
Choose φ(ti) = ˜ti for 0 ≤i ≤k −1 and let φ(t) = ˜tk for all other t ∈T −
{t0, . . . , tk−1}. Let ˜s = φ(s) and ˜l = φ(l). Then ˜s `|
˜H ̸= ⟨⟩∧˜l ∈˜L ∧˜l ∈
A ˜T ,i ∧˜s `| A ˜T ,i ̸= ˜s \ ˜H `| A ˜T ,i. Applying Equation 2 to ST , we have ˜sˆ⟨˜l⟩∈
traces(S ˜T ) ∧˜s \ ˜H ∈traces(S ˜T ). Further, {l} = φ−1({˜l}) by construction. So,
applying Equation 2 to PT,i, we obtain (˜s \ ˜H `| A ˜T ,i, {˜l}) ∈failures(P ˜T ,i).
⊓⊔

94
T. Murray and G. Lowe
Set
HT,U,V
=
{|t.DDReader, DDReader.t, t.t′|t, t′
∈
T |} and
LT,U,V
=
{|u.DDWriter.Call.d, DDWriter.u.Return.null, u.u′ | u, u′ ∈U, d ∈Data ∪{null}|}.
Then SystemT,U,V and all of its components are data-independent in T , U and
V and satisfy NoEqT for each.
Applying Equation 2, it is easily shown that HT,U,V and LT,U,V partition the
alphabet of SystemT,U,V for all non-empty choices of T , U and V , if they do so
when each of these sets has size 1. FDR conﬁrms the latter to be true.
To verify Weakened RCFNDC, Theorem 2 suggests thresholds for T , U and
V of 1, 4 and 2 respectively. This threshold for U arises from events in LT,U,V
of the form u.u′.op.u′′ for u, u′, u′′ ∈U.
In fact, in the proof of Theorem 2, l is necessarily an event in the alphabet of
a process that can perform both HT and LT events. Hence, we can strengthen
this theorem to take W to be the maximum number of distinct values of type T
in all such events in LT . For SystemT,U,V , this means that all events from {|u.u′ |
u, u′ ∈U|} can be excluded when calculating the threshold for U, reducing it to 2.
The most expensive of the 4 tests implied by these thresholds examines about
6 million state-pairs, taking around 4 minutes to compile and complete on a
desktop PC; the others are far cheaper. All tests pass, generalising our results.
5
Conclusion and Related Work
We have shown how to apply CSP and FDR to automatically detect covert chan-
nels in security-enforcing object-capability patterns without forcing the program-
mer to specify the mechanisms by which information may propagate covertly.
Our approach couples the objects that implement a pattern with arbitrary,
Untrusted, high- and low-sensitivity objects that exhibit all behaviours permit-
ted by the object-capability model. This has the added advantage that we can
compare how a pattern functions in diﬀerent kinds of object-capability system,
such as single-threaded versus concurrent systems, by simply reﬁning the deﬁni-
tion of the Untrusted process. Investigating how the information ﬂow properties
of patterns are aﬀected by changing the context in which they are deployed is
an obvious avenue for future work.
The assumption that objects aﬀect each other only by passing messages means
that our analysis cannot be applied to timed systems in which objects have access
to a global clock, for instance. Extending this work to cover such systems may
allow us to detect possible timing channels that may exist in them.
Spiessens’ [20] is the only prior work of which we are aware that examines
the security properties of object-capability patterns. The ideas of safe abstrac-
tion and aggregation deﬁned in Section 4 were heavily inspired by similar ideas
in [20]. Spiessens’ formalism has the advantage of not requiring the use of data-
independence arguments to generalise analyses of small systems to large systems.
On the other hand, our approach, unlike Spiessens’, can detect covert channels in
a pattern without forcing the programmer to specify the means by which infor-
mation can propagate covertly. Instead, these means are captured by information
ﬂow properties that can be applied to any pattern being analysed.

Analysing the Information Flow Properties
95
The notion of aggregation is also similar to (the inverse of) van der Meyden’s
architectural reﬁnement [21]. Finally, data-independence theory has been applied
before to generalise analyses of small systems to larger systems, including to the
analysis of cryptographic protocols [15] and intrusion detection systems [13].
References
1. Elkaduwe, D., Klein, G., Elphinstone, K.: Veriﬁed protection model of the seL4
microkernel. In: Shankar, N., Woodcock, J. (eds.) VSTTE 2008. LNCS, vol. 5295,
pp. 99–114. Springer, Heidelberg (2008)
2. Focardi, R.: Comparing two information ﬂow security properties. In: Proceedings
of CSFW 1996, pp. 116–122. IEEE Computer Society, Los Alamitos (1996)
3. Focardi, R., Gorrieri, R.: A classiﬁcation of security properties for process algebras.
Journal of Computer Security 3(1), 5–33 (1995)
4. Formal Systems (Europe), Limited. FDR2 User Manual (2005)
5. Grove, D., Murray, T., Owen, C., North, C., Jones, J., Beaumont, M.R., Hopkins,
B.D.: An overview of the Annex system. In: Proceedings of ACSAC 2007 (2007)
6. Lazi´c, R.S.: A Semantic Study of Data Independence with Applications to Model
Checking. D.Phil. thesis. Oxford University Computing Laboratory (1999)
7. Lowe, G.: On information ﬂow and reﬁnement-closure. In: Proceedings of the Work-
shop on Issues in the Theory of Security, WITS 2007 (2007)
8. Mettler, A.M., Wagner, D.: The Joe-E language speciﬁcation, version 1.0. Technical
Report EECS-2008-91, University of California, Berkeley (August 2008)
9. Miller, M.S.: Robust Composition: Towards a Uniﬁed Approach to Access Control
and Concurrency Control. PhD thesis. Johns Hopkins University (2006)
10. Miller, M.S., Samuel, M., Laurie, B., Awad, I., Stay, M.: Caja: Safe active content
in sanitized JavaScript, draft (2008)
11. Murray, T.: Analysing the Security Properties of Object-Capability Patterns.
D.Phil. thesis. University of Oxford (2010) (Forthcoming)
12. Murray, T., Lowe, G.: On reﬁnement-closed security properties and nondetermin-
istic compositions. In: Proceedings of AVoCS 2008, pp. 49–68 (2009)
13. Rohrmair, G.T., Lowe, G.: Using data-independence in the analysis of intrusion
detection systems. Theoretical Computer Science 340(1), 82–101 (2005)
14. Roscoe, A.W.: The Theory and Practice of Concurrency. Prentice-Hall, Englewood
Cliﬀs (1997)
15. Roscoe, A.W., Broadfoot, P.J.: Proving security protocols with model checkers by
data independence techniques. J. Comput. Secur. 7(2-3), 147–190 (1999)
16. Roscoe, A.W., Goldsmith, M.H.: What is intransitive noninterference? In: Proceed-
ings of CSFW 1999, p. 228. IEEE Computer Society, Los Alamitos (1999)
17. Ryan, P., Schneider, S.: Process algebra and non-interference. Journal of Computer
Security 9(1/2), 75–103 (2001)
18. Ryan, P.Y.A.: A CSP formulation of non-interference and unwinding. IEEE Cipher,
19–30 (Winter 1991)
19. Saltzer, J.H., Schroeder, M.D.: The protection of information in computer systems.
Proceedings of the IEEE 63(9), 1208–1308 (1975)
20. Spiessens, A.: Patterns of Safe Collaboration. PhD thesis, Universit´e catholique de
Louvain, Louvain-la-Neuve, Belgium (February 2007)
21. van der Meyden, R.: Architectural reﬁnement and notions of intransitive nonin-
terference. In: Massacci, F., Redwine Jr., S.T., Zannone, N. (eds.) ESSoS 2009.
LNCS, vol. 5429, pp. 60–74. Springer, Heidelberg (2009)

Applied Quantitative Information Flow and
Statistical Databases
Jonathan Heusser and Pasquale Malacaria
School of Electronic Engineering and Computer Science
Queen Mary University of London
{jonathanh,pm}@dcs.qmul.ac.uk
Abstract. We ﬁrstly describe an algebraic structure which serves as
solid basis to quantitatively reason about information ﬂows. We demon-
strate how programs in form of partition of states ﬁt into that theoretical
framework.
The paper presents a new method and implementation to automati-
cally calculate such partitions, and compares it to existing approaches.
As a novel application, we describe a way to transform database queries
into a suitable program form which then can be statically analysed to
measure its leakage and to spot database inference threats.
1
Introduction
Quantitative Information Flow (QIF) [5,6] provides a general setting for mea-
suring information leaks in programs and protocols. In QIF programs are in-
terpreted as equivalence relations on input states: two inputs are equivalent if
they generate the same observations, e.g. if the program-run on those two inputs
terminates with the same output. These equivalence relations form a complete
lattice, the Lattice of Information [13] that satisﬁes nice algebraic properties.
Also, once input states are equipped with a probability distribution the equiva-
lence relations correspond to random variables. Information theoretical notions
like entropy can be used on these relations to quantify information leaks.
Applied Quantitative Information Flow, i.e. the automatic interpretation of
programs in the lattice of information and the related information theoretical
computations, is steadily coming of age. Based on a growing number of impres-
sive progress in the ﬁeld of model checking, SAT solvers, theorem provers and
program analysis it is now possible to test quantitative information ﬂow ideas
on real code [2,17].
Of course there are still severe limitations of this kind of automatic analysis
and it is well possible that most complex code will be out of reach for the foresee-
able future. As a comparison a quantitative analysis is intrinsically more complex
than a qualitative one and hence we should accordingly moderate our expecta-
tions in quantitative tools achieving the same results as qualitative ones anytime
soon. There are however important families of programs where automatic analy-
sis is within reach, for example side channel analysis for cryptographic protocols
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 96–110, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Applied Quantitative Information Flow and Statistical Databases
97
[12]. Also the integration of quantitative analysis with heuristics and software
engineering tools has been successfully demonstrated [17].
In this paper we will introduce an original technique to automatically compute
QIF. This technique uses state of the art technology to compute the lattice
interpretation of a program. While stressing the general purpose nature of our
applied QIF, we investigate in Section 5 the possible application of this tool to a
particular ﬁeld where we believe these techniques have great potential: statistical
databases.
1.1
Statistical Databases
Database queries are a major source of information. While data mining tries
to maximise the amount of information that can be extracted by a database,
security experts work in the opposite direction, i.e. to minimise the conﬁdential
information that can be extracted. This paper addresses security issues of statis-
tical databases, i.e. databases where users are allowed to query statistics about
conﬁdential data; a typical example would be the average salary in a company:
the security threat is that information about an individual salary may be leaked
by one or more queries. As a trivial example, knowing that all employees are
paid the same amount in conjunction with knowing the average salary will re-
veal the individual salary of all employees. Ideally, a statistical database security
oﬃcer should prevent or detect attacks that gain individual information. How-
ever, this has been shown as unachievable, for example because of “trackers” [9].
In Section 5 we sketch how applied QIF can be used to measure the amount of
conﬁdential information leaked by a set of queries and hence to improve security
risk assessment for statistical databases.
In relating QIF to statistical databases, the idea is to interpret a statistical
query as a simple program in a programming language and use the interpretation
of programs in the Lattice of Information to apply known tools and techniques
to measure the amount of conﬁdential information leaked by a set of queries.
1.2
Contributions
The tool presented in Section 4 is original; its relation to DisQuant is discussed
in the same section. Proposition 2 is also original and allows for an automatic and
elegant interpretation of databases queries in LoI . Also the ideas in Section 5
about the use of applied QIF in the statistical databases context are original.
2
Lattice of Information
It has been shown by Landauer and Redmond [13] that observations about the
behaviour of a deterministic system can be elegantly modelled in terms of a
lattice. Let Σ be the set of all states in a system. An observation can be seen
as an equivalence class of states deﬁned by σ ∼σ′ if and only if σ and σ′ are

98
J. Heusser and P. Malacaria
indistinguishable given that observation. The join ⊔and meet ⊓lattice opera-
tions stand for the intersection of relations and the transitive closure union of
relations respectively. Thus, higher elements in the lattice can distinguish more
while lower elements in the lattice can distinguish less states. It is easy to show
that this is a complete lattice of equivalence relations.
The bottom of this lattice is the least informative observation (any two states
are equivalent, i.e. all states are equivalent) and the top of the lattice is the
most informative observation (each state is only equivalent to itself). Aptly they
named this lattice Lattice of Information ( LoI ). The ordering of LoI is deﬁned
as
≈⊑∼↔∀σ1, σ2 (σ1 ∼σ2 ⇒σ1 ≈σ2)
(1)
where σ1, σ2 ∈Σ. An equivalent presentation for the same lattice is in terms
of partitions. In fact any equivalence relation can be seen as a partition whose
blocks are its equivalence classes. Seen as a lattice of partition we have σ ⊔σ′ =
{a ∩b|a ∈σ, b ∈σ′}.
In this paper we will assume this lattice to be ﬁnite; this is motivated by
considering information storable in programs variables: such information is ≤2k
where k is the number of bits of the secret variable.
We give a typical example of how these equivalence relations can be used in
an information ﬂow setting. Let us assume the set of states Σ consists of a tuple
⟨l, h⟩where l is a low variable and h is a conﬁdential variable. One possible
observer can be described by the equivalence relation
⟨l1, h1⟩≈⟨l2, h2⟩↔l1 = l2
That is the observer can only distinguish two states whenever they agree on
the low variable part. Clearly, a more powerful attacker is the one who can
distinguish any two states from one another, or
⟨l1, h1⟩∼⟨l2, h2⟩↔l1 = l2 ∧h1 = h2
The ∼-observer gains more information than the ≈-observer by comparing states,
therefore ≈⊑∼.
A random variable on a ﬁnite space can be seen as map X : D →R(X),
where D is a ﬁnite set with a probability distribution and R(X), a measurable
set, is the range of X. For each element d ∈D, the probability of it is denoted
p(d). For x ∈R(X) p(x) means the probability that X takes on the value x, i.e.
p(x)
def
= 
d∈X−1(x) p(d). From that perspective, X partitions the space D into
sets which are indistinguishable to an observer who sees the value that X takes
on. This can be seen as the equivalence relation ker(X):
d ker(X) d′ iﬀX(d) = X(d′)
(2)
The Shannon entropy of a random variable X is denoted H(X), deﬁned as follows
H(X) = −

x
p(x) log p(x)

Applied Quantitative Information Flow and Statistical Databases
99
As seen from the deﬁnition of p(x), the entropy of X only depends on its set of
inverse images X−1(x). Thus, if two random variables X and Y have the same
inverse images they will necessarily have the same entropy. More formally, we
write X ≃Y whenever the following holds
X ≃Y iﬀ{X−1(x) : x ∈R(X)} = {Y −1(y) : y ∈R(Y )}
and thus if X ≃Y then H(X) = H(Y ).
This shows that each element of the lattice
LoI can be seen as a random
variable. We can hence identify LoI with a lattice of random variables ordered
by (1).
Notice that the ⊔of two random variables is the classic notion of joint random
variable, i.e. X ⊔Y = (X, Y ). In general, LoI is not distributive.
2.1
Measures
We can attempt to quantify the amount of information provided by a point in
LoI by using lattice theoretic notions, such as semivaluations.
A join semivaluation on LoI is a real valued map ν : LoI →R, that satisﬁes
the following properties:
ν(X ⊓Y ) + ν(X ⊔Y ) ≤ν(X) + ν(Y )
(3)
X ⊑Y
implies ν(X) ≤ν(Y )
(4)
for every element X and Y in a lattice [16]. The property (4) is order-preserving:
a higher element in the lattice has a larger valuation than elements below itself.
The ﬁrst property (3) is a weakened inclusion-exclusion principle.
Proposition 1. The map
ν(X ⊔Y ) = H(X, Y )
(5)
is a join semivaluation.
Equation 5 is an important result, ﬁrstly described by Nakamura [16]. He proved
that the only probability-based join semivaluation on the lattice of information
is Shannon’s entropy. It is easy to show that a valuation itself is not deﬁnable on
this lattice, thus Shannon’s entropy is the best approximation to a probability-
based valuation on this lattice.
Other measures can be used, which are however less mathematically appeal-
ing. We will also consider Min-Entropy which seems like a good complementing
measure. While Shannon entropy intuitively results in an “averaging” measure
over a probability distribution, the Min-Entropy H∞takes on a “worst-case”
view: only the maximal value p(x) of a random variable X is considered
H∞(X) = −log max
x∈X p(x)
where it is always the case that H∞(X) ≤H(X).
We write M(X) to indicate Shannon’s entropy or a more general Renyi’s
entropy.

100
J. Heusser and P. Malacaria
3
Measuring Program Leakage
In previous works, we developed theories to quantify the information leakage
of programs [5,14]. The main idea for deterministic programs is to interpret
observations on a program as equivalence relations on states [14,15] and therefore
as random variables in the lattice of information. The random variable associated
to a program P is the equivalence relation on any states σ, σ′ from the universe
of states Σ deﬁned by
σ ≃σ′ ⇐⇒P(σ) =obs P(σ′)
(6)
in this paper =obs represents the relation “to have the same observable out-
put”. We denote the interpretation of a program P in LoI as deﬁned by the
equivalence relation (6 ) by Π(P).
Consider the example if h=0 then access else deny where the variable h
ranges over {0, . . . , 3}. The output random variable O associated to the program
represents the information available to an observer The equivalence relation (i.e.
partition) associated to the above program is hence
O = { {0}

access
{1, 2, 3}
  
deny
}
O eﬀectively partitions the domain of the variable h, where each disjoint subset
represents an output. The partition reﬂects the idea of what a passive attacker
can learn of secret inputs by backwards analysis of the program, from the outputs
to the inputs.
The quantitative evaluation of the partition O is measuring such knowledge
gains of an attacker, solely depending on the partition of states and the proba-
bility distribution of the input.
The next proposition says that we can represent algebraic operations in LoI
using programs:
Proposition 2. Given programs P1, P2 there exists a program P1⊔2 such that
Π(P1⊔2) = Π(P1) ⊔Π(P2)
Given programs P1, P2, we deﬁne P1⊔2 = P ′
1; P ′
2 where the primed programs
P ′
1, P ′
2 are P1, P2 with variables renamed so to have disjoint variable sets. If the
two programs are syntactically equivalent, then this results in self-composition
[3]. For example, consider the two programs
P1 ≡if (h == 0) x = 0 else x = 1,
P2 ≡if (h == 1) x = 0 else x = 1
with their partitions Π(P1) = {{0}{h ̸= 0}} and Π(P2) = {{1}{h ̸= 1}}.
The program P1⊔2 is the concatentation of the previous programs with variable
renaming
P1⊔2 ≡h′ = h; if (h′ == 0) x′ = 0 else x′ = 1;
h′′ = h; if (h′′ == 1) x′′ = 0 else x′′ = 1

Applied Quantitative Information Flow and Statistical Databases
101
The corresponding lattice element is the join, i.e. intersection of blocks, of the
individual programs P1,2
Π(P1⊔2) = {{0}{1}{h ̸= 0, 1} = {{0}{h ̸= 0}} ⊔{{1}{h ̸= 1}}
The above result can be extended to expressions of the language: we can asso-
ciate to an expression e the program consisting of the assignment x = e and use
Proposition 2 to compute the lub in
LoI of a set of expressions. This is the
basic technique we will later use for computing leakage of database queries.
Notice that Π(P) is a general representation that can be used as the basis
for several quantitative measures likes Shannon’s entropy, Renyi entropies or
guessability measures, as described in Section 2.
The overarching idea for quantifying the leakage of a partition Π(P) is to
compute the diﬀerence between uncertainty about the secret before and after
observing the output of the program. For a Shannon-based measure, leakage is
deﬁned in [5,14] as I(Π(P); h|l), i.e. the conditional mutual information between
the program and the secret given the low input.
I(Π(P); h|l) =
H(Π(P)|l) −H(Π(P)|l, h)
=A H(Π(P)|l) −0 = H(Π(P)|l)
=B H(Π(P))
where equality A holds because the program is deterministic and B holds when
the program only depends on the high inputs, i.e. all low variables are initialised
in the code of the program. Thus, for such programs, the Shannon-based leakage
measure is reduced to simply the Shannon entropy of the partition Π(P).
We can relate the order in LoI and the amount of leakage by the following
result
Proposition 3. Let P1, P2 be two programs depending only on the high in-
puts. Then Π(P1) ⊑Π(P2) iﬀfor all probability distributions on states in LoI,
H(Π(P1)) ⊑H(Π(P2)).
4
Automatically Calculating Π(P )
The computationally intensive task in quantifying information leaks is calcu-
lating the partition of input states Π(P). Applying a measure M(Π(P)) is in
comparison cheap and easy to do (if the probability distribution is known). We
developed a tool, AQuA (Automated Quantitative Analysis) which calculates
Π(P) given a program P in the programming language C without user interac-
tion or code annotations.
The idea is best explained using a similar example from before with 4 bit
variable width, and the secret input variable pwd:
P ≡if(pwd == 4) { return 1; } else { return 0; }

102
J. Heusser and P. Malacaria
The ﬁrst step of the method is to ﬁnd a representative input for each possi-
ble output. In our case, AQuA could ﬁnd the set {4, 5}, for outputs 1 and 0,
respectively. This is accomplished using SAT-based ﬁxed point computation.
The next step runs on that set of representative inputs. For each input in
that set, it counts the number of possible inputs which lead to the same implicit,
distinct output. This step is accomplished using model counting.
The next section will look at these two steps in more detail.
4.1
Method
The method consists of two reachability analyses, which can be run either one
after another or interleaved.
The ﬁrst analysis ﬁnds a set of inputs to which the original program produces
distinct outputs for. That set has cardinality of the number of possible outputs
for the program. The second analysis counts the set of all inputs which lead
to the same output. This analysis is run on all members of the set of the ﬁrst
analysis. Together, these two analyses allow to discover the partition of the input
space according to a program’s outputs.
To a program P we associate two modiﬁed programs P̸= and P=, representing
the two reachability questions. The two programs are deﬁned as follows:
P̸=(i) ≡h = i; P; P ′; assert(l! = l′)
P=(i) ≡h = i; P; P ′; assert(l = l′)
The program P is self-composed [3,18] and is either asserting low-equality or low-
inequality on the output variable and its copy. Their argument is the initialisation
value for the input variable. This method works on any number of input variables,
but we simplify it to a single variable.
The programs P̸= and P= are unwound into propositional formula and then
translated in Conjunctive Normal Form (CNF) in a standard fashion.
P̸= is solved using a number of SAT solver calls using a standard reachability
algorithm (SAT-based ﬁxed point calculation) [11].
Algorithm 1 describes this input discovery. In each iteration it discovers a new
input h′ which does not lead to the same output as previous the input h. The
new input h′ is added to the set Sinput. The observable output l is added to the
formula as blocking clause, to avoid ﬁnding the same solution again in a diﬀerent
iteration. This process is repeated until P̸= is unsatisﬁable which signiﬁes that
the search for Sinput elements is exhausted.
Given Sinput (or a subset of it) as result of Algorithm 1, we can use P= to count
the sizes of the equivalence classes represented by Sinput using model counting.
This process is displayed in Algorithm 2 and is straightforward to understand.
The algorithm calculates the size of the equivalence class [h]P= for every h in
Sinput by counting the satisfying models of P=(h). The output M of Algorithm
2 is the partition Π(P) of the original program P.
Proposition 4 (Correctness). The set Sinput of Algorithm 1 contains a rep-
resentative element for each possible equivalence class of Π(P). Algorithm 2
calculates {[s1]P=, . . . , [sn]P=} which, according to (6), is Π(P).

Applied Quantitative Information Flow and Statistical Databases
103
Input: P̸=
Output: Sinput
Sinput ←∅
h ←random
Sinput ←Sinput ∪{h}
while P̸=(h) not unsat do
(l, h′) ←Run SAT solver on P̸=(h)
Sinput ←Sinput ∪{h′}
h ←h′
P̸= ←P̸= ∧l′ ̸= l
end
Algorithm 1. Calculation of Sinput using P̸=
Input: P=, Sinput
Output: M
M = ∅
while Sinput ̸= ∅do
h ←s ∈Sinput
#models ←Run allSAT solver on P=(h)
M = M ∪{#models}
Sinput ←Sinput \ {s}
end
Algorithm 2. Model counting of equivalence classes in Sinput
4.2
Implementation
The implementation builds up on a toolchain of existing tools, together with
some interfacing, language translations, and optimisations. See Figure 1 for an
overview.
AQuA has the following main features:
– runs on a subset of ANSI C without memory allocation and with integer
secret variables
– no user interaction or code annotations needed except command line options
– supports non-linear arithmetic and integer overﬂows
AQuA works on the equational intermediate representation of the CBMC
bounded model checker [7]. C code is translated by CBMC into a program
of constraints which in turn gets optimised through standard program analysis
techniques into cleaned up constraints1. This program then gets self-composed
and user-provided source and sink variables get automatically annotated.
In a next step, the program gets translated into the bit-vector arithmetic
Spear format of the Spear theorem prover [1]. At this point, AQuA will spawn
the two instances, P= and P̸=, from the input program P.
1 CBMC adds some constraints which distorts the model counting.

104
J. Heusser and P. Malacaria
Constr
aints
Self-
Comp
Spear 
Format
C
SAT
S_input
#SAT
Partition
CBMC
Optimisations
Language 
translation
P̸=
P=
Fig. 1. Translation steps
Algorithms 1 and 2 get executed sequentially on those two program versions.
However, depending on the application and cost of the SAT queries, once could
also choose to execute them interleaved, by ﬁrst calculating one input to the
program P= and then model counting that equivalence class.
For Algorithm 1, Spear will SAT solve P̸= directly and report the satisfying
model to the tool. The newly found inputs are stored until P̸= is reported to be
unsat.
For Algorithm 2, Spear will bit-blast P= down to CNF which in turn gets
model counted by either RelSat [4] or C2D. C2D is only used in case the user
speciﬁes fast model counting through command line options. While the counting
is much faster on diﬃcult problems than RelSat, the CNF instances have to
be transformed into a d-DNNF tree [8] which is very costly in memory. This is
a trade-oﬀbetween time and space. In most instances, RelSat is fast enough,
except in cases with multiple constraints on more than two secret input variables.
Once the partition Π(P) is calculated, the user can choose which measure to
apply.
Loops.
The ﬁrst step of the program transformations is treating loops in an
unsound way, i.e. a user needs to deﬁne a ﬁxed number of loop unwindings. This
is a inherent property of the choice of tools used, as CBMC is a bounded model
checker, which limit the number of iterations down to what counterexamples can
be found. While this is a real restriction in program veriﬁcation – as bugs can
be missed in that way – it is not as crucial for our quantiﬁcation purposes.
Algorithm 1 detects at one point an input which contains all inputs beyond
the iteration bound. Using the principle of maximum entropy, this “sink state”
can be used to always safely over-approximate entropy.
Let us assume we analyse a binary search examples with 15 unwindings of the
loop and 8 bit variables. AQuA reports the partition

Applied Quantitative Information Flow and Statistical Databases
105
Table 1. Performance examples. * 30 loop unrollings; † from [2]; ⋆counted with C2D
Machine: Linux, Intel Core 2 Duo 2GHz.
Program
#h range
Σh bits
P̸= Time P̸= + P= Time Spear LOC
CRC8 1h.c
1
8 bit
8
17.36s
32.68s
370
CRC8 2h.c
2
8 bit
16
34.93s
1m18.74s
763
sum3.c†
3
0 . . . 9 9.96 (103)
0.19s
0.95s
16
sum10.c⋆
10 0 . . . 5 25.84 (610)
1.59s
3m30.76s
51
nonlinear.c
1
16 bit
16
0.04s
13.46s
20
search30.c*
1
8 bit
8
0.84s
2.56s
186
auction.c†⋆
3
20 bit
60
0.06s
16.90s
42
Partition:
{241}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}{1}: 256
where the number in the brackets are the model counts. We have 15 singleton
blocks and one sink block with a model count of the remaining 241 unprocessed
inputs. When applying a measure, the 241 inputs could be distributed as well in
singleton blocks which would over-approximate (and in this case actually exactly
ﬁnd) the leakage of the input program.
Proposition 5 (Sound loop leakage). Let us assume partition Π(P)n is the
result of n unwindings of P, and Π(P)m is m unwindings of P, where m ≥n.
If every element of the “sink state” block b ∈Π(P)n is distributed in individual
blocks, the partition denoted as ˆΠ(P)n, then Π(P)m ⊑ˆΠ(P)n. From Proposition
3 follows that H(Π(P)m) ⊑H( ˆΠ(P)n).
Experiences.
Table 1 provides a small benchmark to give an idea on what
programs AQuA has been tested on. The running times have been split between
Algorithm 1 to calculate P̸= and the total run time; also it provides the lines of
code (LOC) the program has in Spear format.
The biggest example is a full CRC8 checksum implementation where the input
is two char variables (16 bit) which has over 700 LOC.
The run time depends on the number of secrets and their ranges and as a
result on the cardinality of the partition. The programs are available from the
ﬁrst author’s website.
4.3
Comparison to DisQuant
Recently, Backes, K¨opf, and Rybalchenko published an elegant, and inspiring
method to calculate and quantify an equivalence relation given a C-like
program [2].
Their tool DisQuant turns information ﬂow checking into a reachability
problem by self-composing a program and then applying a model checker to
ﬁnd pairs of inputs which violate the secure information ﬂow safety property.
This will result in a logical formula of secret relations. Out of the relation, equiv-
alence classes and their sizes can be calculated. The relation is built by a guided

106
J. Heusser and P. Malacaria
abstraction-reﬁnement technique. This means that the tool starts with a blank
canvas where every high input is related to each other. Then it successively re-
ﬁnes an equivalence relation R by learning that some input pairs (hi, hj) lead to
diﬀerent outputs, in which case a new equivalence class is added to the relation.
However, before DisQuant can say anything about the (size of the) partition
it has to complete the reﬁnement process; intermediate results in the CEGAR
reﬁnement process might not be useable for quantiﬁcation.
In comparison, our method is in a way the opposite: it calculates a whole
equivalence class for one input, independent of the remaining equivalence classes.
This has multiple advantages: it is easy to distribute the computation for diﬀer-
ent inputs and model counting over multiple computers; for some problems, not
all equivalence classes need to be calculate and the computation can stop after
ﬁnding certain properties, e.g. ﬁnding a too large/small equivalence class for a
given policy; we can calculate a partition for a subset of inputs; we can provide
incremental lower bounds on the leakage.
Two additional diﬀerences between the two approaches are worth mentioning
separately: Due to the bit precise modelling of arithmetic operators and overﬂows
our tool can handle non-linear constraints on secret inputs, while DisQuant is
limited by its underlying techniques to linear arithmetic.
Also, Algorithm 2 in our tool is not only able to count the number of elements
in an equivalence class but also enumerate the models. While it is prohibitive
to completely enumerate large equivalence classes, it is still possible to extract
example models which could be used for some purposes.
5
Database Queries as Programs
We will now describe how we can model statistical database queries as programs.
Once a database query has been modelled as a program, we can apply our
program analysis tools to calculate the partition of states and in turn quantify the
leakage of the queries. This section is not about showcasing AQuA’s performance
but to illustrate the width of applications of applied QIF.
We will use concepts used by Dobkin et al. [10] to describe databases
Deﬁnition 1. A database D is a function from 1, . . . , n to N. The number of
elements in the database is denoted by n; N is the set of possible attributes.
A database D can also be directly described by its elements {d1, . . . , dn}, with
D(i) = di for 1 ≤i ≤n. For a database with n number of objects, a query is
an n-ary function. Given D, q(D) = q(d1, . . . , dn) is the result of the query q on
the database D.
We assume that a database user can choose the function q and restrict its
application to some of the elements of {d1, . . . , dn}, depending on the query
structure. However, the user can not see any values the function q runs on.
An arbitrary query is translated by the following transformation
Q1 = q(di, . . . , dj)
⇒
l1 = e(hi, . . . hj)

Applied Quantitative Information Flow and Statistical Databases
107
where the function q applied to (di, . . . , dj) is rewritten to some C expression e2
on the secret variables hi, . . . , hj, where hn is equal to dn for all i ≤n ≤j; the
output is stored in the observable variable l1. A sequence of queries Q1, . . . , Qn
results in tuples of observable variables (l1, . . . , ln). We denote the partition of
states for a query Qi, after the transformation above, as Π(Qi).
5.1
Database Inference by Examples
To measure the degree of database inferences possible by a sequence of queries
we deﬁne the following ratio, comparing leakage with the respective secret space
Deﬁnition 2 (SDB Leakage Ratio). Given an SDB, let Q1, . . . , Qn be queries,
and h1, . . . , hm be the involved secret elements in the database. The percentage of
leakage revealed by the sequence of queries is given by
M(
1≤i≤n Π(Qi))
M(h1, . . . , hm)
(7)
In the deﬁnition we can use Proposition 2 to compute 
1≤i≤n Π(Qi)
Max/Sum Example. Two or more queries can lead to an inference problem
when there is an overlap on the query ﬁelds. Assume two series of queries:
Q1 = max(h1, h2)
Q2 = sum(h3, h4)
The ﬁrst series of queries ask for the max and sum of two disjoint set of ﬁelds.
The two queries don’t share any common secret ﬁelds, so Q1 does not contribute
to the leakage of Q2.
Q′
1 = max(h1, h2)
Q′
2 = sum(h1, h2)
It is a diﬀerent picture if the two queries run on the same set of ﬁelds, as shown
in Q′
1, Q′
2. Intuitively, we learn the biggest element of the two and we learn the
sum of the two. The queries combined reveal the values of both secret ﬁelds, i.e.
sum −max = min.
Assuming 2 bit variables, we get the following calculations:
H(Π(Q1)) = 1.7490
H(Π(Q2)) = 2.6556
H(Π(Q1) ⊔Π(Q2)) = 4.4046
H(Π(Q′
1)) = 1.7490
H(Π(Q′
2)) = 2.6556
H(Π(Q′
1) ⊔Π(Q′
2)) = 3.25
The measure of how much of the secret the two series of queries revealed is the
ratio between the join of the queries to the whole secret space:
2 Expressions
usually
used
in
statistical
database
are
sum, count,average,
mean,median etc.. our context is however general so any C expression can be used.

108
J. Heusser and P. Malacaria
Table 2. Contributors
Contributor Industry Geograph. Area
C1
Steel
Northeast
C2
Steel
West
C3
Steel
South
C4
Sugar
Northeast
C5
Sugar
Northeast
C6
Sugar
West
Table
3.
Summary
Table
for
Contributors
Contributing Group
Amount
Steel
h1 + h2 + h3
Sugar
h4 + h5 + h6
. . .
. . .
Northeast
h1 + h4 + h5
. . .
. . .
H(Π(Q1) ⊔Π(Q2))
H(h1, h2, h3, h4)
= 4.4046
8.0
≈55%
H(Π(Q′
1) ⊔Π(Q′
2))
H(h1, h2)
= 3.25
4.0 ≈81%
where we have used H, the Shannon entropy as the leakage measure3. The 3.25
bits, or 81% of the secret, is the maximal possible leakage for the query, as we
still don’t know which of the two secrets secret was the bigger one of the two,
however “everything” is leaked in a sense, while the ﬁrst query only reveals 55%
of the secret space.
For the enforcement, we could think of a simple monitor which keeps adding
up the information released so far for individual users and which would refuse
certain queries in order to not reveal more than a policy allows. A policy can be
as simple as a percentage of the secret space to be released.
Sum Queries Inference. Consider a database storing donations of contributors
to a political party from the steel and sugar industry, contributors coming from
several geographical areas. Given Tables 2 and 3, a user is allowed to make
sum queries on all contributors which share a common attribute (Industry or
Geographic Area)4. Table 3 summarises all possible queries, where the amount
donated by each contributor Ci is represented by the value hi.
In this scenario, the owner of the databases wants to make sure that no
user can learn more than 50% of the combined secret knowledge of what each
contributor donated.
We will look at two users querying the database; the queries of the ﬁrst user
fulﬁll the requirements of the database owner, the second user (who happens
to be contributor C1) is clearly compromising the database information release
requirements.
User 1 is making two queries
Q1 = sum(h1, h2, h3)
Q2 = sum(h4, h5, h6)
In other words, User 1 is asking for the sum of the contributors from the steel and
sugar industry. For simplicity, we assume only 2 bit variables for each contributor
hi. AQuA calculates a partition with 100 equivalence classes, and a Shannon
entropy of 5.9685 of total 12 bits.
3 Taking a diﬀerent measure like min entropy we would get 40% and 75% respectively.
4 Example adapted from [10].

Applied Quantitative Information Flow and Statistical Databases
109
This results in a ratio of
H(Π(Q1) ⊔Π(Q2))
H(h1, . . . , h6)
= 5.9685
12
≈49.73%
which is just within the requirements of 50% information leakage.
User 2, who is contributor C1, is inquiring the following two queries:
Q3 = sum(h4, h5, h6)
Q4 = sum(h1, h4, h5)
Here, Q3 and Q4 have an overlap in the ﬁelds h4 and h5. Since User 2 is C1,
the ﬁeld h1 is known, so with these two queries, User 2 is able to learn h6, i.e.
h6 = Q3 −Q4 + h1. The substantial knowledge gain of User 2 is revealed in the
leakage ratio
H(Π(Q3) ⊔Π(Q4))
H(h1, h4, h5, h6)
= H(Π(Q3) ⊔Π(Q′
4))
H(h4, h5, h6)
= 4.6556
6
≈77.6%
where in the second equation term h1 in the denominator disappear because
contributor C1 knows h1 (similarly Q′
4 = sum(h4, h5))5. If our tool was evaluat-
ing the information leakage of these queries before the result was reported back
to the user, then Q4 could be denied for User 2.
We can see the previous database as an (easily computable) abstraction of
a real database with a large number of entries. In this case C1 could represent
the set of contributors form the Steel industry in the Northeast. In this case the
leakage ratio would tell us the amount of information the queries leak about
the group of individual (or set of secret data). We can hence extract valuable
information about the threat of a set of queries by automatically computing the
leakage on an abstraction of a database. This measure can be combined with
more classical query restriction techniques like set size and overlap restriction
within a threat monitor. While a precise theory of this monitor is beyond the
scope of this work we believe the ideas are sound and workable.
6
Related Work
The closest work to ours is the one reviewed in Section 4.3. Another impres-
sive method to quantify information ﬂows in large programs is described by
McCamant in multiple works [17]. The lattice of information has been de-
scribed by Landauger and Redmond [13]. k-Anonymity [19] is a notion related
to our database inference work. In our framework, a partition which satisﬁes
k-Anonymity has no equivalence class smaller than k. However, we consider the
whole probability distribution and thus measure more than what k-Anonymity
does. Further research is needed to clarify the connection of the two works.
5 To understand the numbers 4.6556 comes by the fact that the queries reveal h6 i.e.
2 bits, plus sum(h4, h5) which is 2.6556 bits.

110
J. Heusser and P. Malacaria
References
1. Babi´c, D., Hutter, F.: Spear Theorem Prover. In: Proc. of the SAT 2008 Race
(2008)
2. Backes, M., K¨opf, B., Rybalchenko, A.: Automatic Discovery and Quantiﬁcation
of Information Leaks. In: Proc. 30th IEEE Symposium on Security and Privacy,
S& P 2009 (2009) (to appear)
3. Barthe, G., D’Argenio, P.R., Rezk, T.: Secure Information Flow by Self-
Composition. In: Proceedings of the 17th IEEE workshop on Computer Security
Foundations CSFW (2004)
4. Bayardo, R., Schrag, R.: Using CSP look-back techniques to solve real-world SAT
instances. In: Proc. of AAAI 1997, pp. 203–208. AAAI Press/The MIT Press (1997)
5. Clark, D., Hunt, S., Malacaria, P.: A static analysis for quantifying information
ﬂow in a simple imperative language. Journal of Computer Security 15(3) (2007)
6. Clark, D., Hunt, S., Malacaria, P.: Quantitative information ﬂow, relations and
polymorphic types. Journal of Logic and Computation, Special Issue on Lambda-
calculus, type theory and natural language 18(2), 181–199 (2005)
7. Clarke, E., Kroening, D., Lerda, F.: A Tool for Checking ANSI-C Programs.
In: Jensen, K., Podelski, A. (eds.) TACAS 2004. LNCS, vol. 2988, pp. 168–176.
Springer, Heidelberg (2004)
8. Darwiche, A., Marquis, P.: A Knowledge Compilation Map. Journal of Artiﬁcial
Intelligence Research 17, 229–264 (2002)
9. Denning, D.E., Schlˇsrer, J.: A fast procedure for ﬁnding a tracker in a statistical
database. ACM Transactions on Database Systems 5(1), 88–102 (1980)
10. Dobkin, D., Jones, A.K., Lipton, R.J.: Secure databases: Protection against user
inﬂuence. ACM Transactions on Database Systems 4, 97–106 (1979)
11. Chauhan, P., Clarke, E.M., Kroening, D.: Using SAT based Image Computation
for Reachability. Carnegie Mellon University, Technical Report CMU-CS-03-151
(2003)
12. K¨opf, B., Basin, D.: An information-theoretic model for adaptive side-channel at-
tacks. In: Proceedings of the 14th ACM conference on Computer and communica-
tions security CCS 2007, pp. 286–296 (2007)
13. Landauer, J., Redmond, T.: A Lattice of Information. In: Proc. of the IEEE Com-
puter Security Foundations Workshop. IEEE Computer Society Press, Los Alami-
tos (1993)
14. Malacaria, P.: Assessing security threats of looping constructs. In: Proc. ACM
Symposium on Principles of Programming Language (2007)
15. Malacaria, P.: Risk Assessment of Security Threats for Looping Constructs. To
appear in the Journal Of Computer Security (2009)
16. Nakamura, Y.: Entropy and Semivaluations on Semilattices. Kodai Math. Sem.
Rep. 22, 443–468 (1970)
17. McCamant, S.A.: Quantitative Information-Flow Tracking for Real Systems. MIT
Department of Electrical Engineering and Computer Science, Ph.D., Cambridge,
MA (2008)
18. Terauchi, T., Aiken, A.: Secure information ﬂow as a safety problem. In: Hankin,
C., Siveroni, I. (eds.) SAS 2005. LNCS, vol. 3672, pp. 352–367. Springer, Heidelberg
(2005)
19. Sweeney, L.: k-anonymity: a model for protecting privacy. International Journal on
Uncertainty, Fuzziness and Knowledge-based Systems 10(5), 557–570 (2002)

Speciﬁcation and Veriﬁcation of Side Channel
Declassiﬁcation
Josef Svenningsson and David Sands
Department of Computer Science and Engineering,
Chalmers University of Technology
G¨oteborg, Sweden
{josefs,dave}@chalmers.se
Abstract. Side channel attacks have emerged as a serious threat to
the security of both networked and embedded systems – in particular
through the implementations of cryptographic operations. Side channels
can be diﬃcult to model formally, but with careful coding and program
transformation techniques it may be possible to verify security in the
presence of speciﬁc side-channel attacks. But what if a program inten-
tionally makes a tradeoﬀbetween security and eﬃciency and leaks some
information through a side channel? In this paper we study such trade-
oﬀs using ideas from recent research on declassiﬁcation. We present a
semantic model of security for programs which allow for declassiﬁcation
through side channels, and show how side-channel declassiﬁcation can
be veriﬁed using oﬀ-the-shelf software model checking tools. Finally, to
make it simpler for veriﬁers to check that a program conforms to a partic-
ular side-channel declassiﬁcation policy we introduce a further tradeoﬀ
between eﬃciency and veriﬁability: by writing programs in a particular
“manifest form” security becomes considerably easier to verify.
1
Introduction
One of the pillars of computer security is conﬁdentiality – keeping secrets secret.
Much recent research in language based security has focused on how to ensure
that information ﬂows within programs do not violate the intended conﬁden-
tiality properties [SM03]. One of the diﬃculties of tracking information ﬂows is
that information may ﬂow in various indirect ways. Over 30 years ago, Lamp-
son [Lam73] coined the phrase covert channel to describe channels which were
not intended for information transmission at all. At that time the concern was
unintended transmission of information between users on timeshared mainframe
computers. In much security research that followed, it was not considered worth
the eﬀort to consider covert channels. But with the increased exposure of sensi-
tive information to potential attackers, and the ubiquitous use of cryptographic
mechanisms, covert channels have emerged as a serious threat to the security
of modern systems – both networked and embedded. The following key papers
provide a view of the modern side-channel threat landscape:
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 111–125, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

112
J. Svenningsson and D. Sands
• Kocher [Koc96] showed that by taking timing measurements of RSA cryp-
tographic operations one could discover secret keys. Later [KJJ99] it was
shown that one could do the same by measuring power consumption.
• Based on Kocher’s ideas numerous smart card implementations of crypto-
graphic operations have shown to be breakable. See e.g. [MDS99].
• Brumley and Boneh [BB05] showed that timing attacks were not just relevant
to smart cards and other physical cryptographic tokens, but could be eﬀective
across a network; they developed a remote timing attack on an SSL library
commonly used in web servers.
What is striking about these methods is that the attacks are on the implementa-
tions and not features of the basic intended functionality. Mathematically, cryp-
tographic methods are adequately secure, but useless if the functionally correct
implementation has timing or other side channels.
1.1
Simple Timing Channels
Timing leaks often arise from the fact that computation involves branching on
the value of a secret. Diﬀerent instructions are executed in each branch, and
these give rise to a timing leak or a power leak (whereby a simple power analysis
[MS00] can reveal information about e.g. control ﬂow paths).
One approach is to ensure that both branches take the same time [Aga00],
or to eliminate branches altogether [MPSW05] – an approach that is also well
known from real-time systems where it is used to make worst case execution
time easy to determine [PB02].
1
r = 1;
2
i = m - 1;
3
while
(i >= 0) {
4
r = r * r;
5
i f
(d[i] == 1)
{
6
r = r * x;
7
}
8
i = i - 1;
9
}
10
return r;
Fig. 1.
Modular
exponentia-
tion
Consider the pseudocode in Figure 1 represent-
ing a na¨ıve implementation of modular exponen-
tiation, which we will use as our running example
throughout the paper.
The data that goes in to this function is typi-
cally secret. A common scenario is that the vari-
able x is part of a secret which is to be encrypted
or decrypted and variable d is the key (viewed
here as an array of bits). It is important that
these remain secret. (On the other hand, m, the
length of the key, is usually considered public
knowledge.)
However, as this function is currently written
it is possible to derive some or all of the informa-
tion about the key using either a timing or power
attack. The length of the loop will always reveal
the size of the key – and this is accepted. In the
body of the loop there is a conditional statement which is executed depending on
whether the current bit in the key is set or not. This means that each iteration
of the loop will take diﬀerent amount of time depending on the value of the key.
A timing attack measuring the time it takes to compute the whole result can

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
113
be used to learn the hamming weight of the key, i.e. the number of 1’s. With
control over the key and repeated runs this is suﬃcient to leak the key [Koc96].
A power analysis could in principle even leak the key in a single run.
1.2
Timing and Declassiﬁcation
Often the run-time cost of securing an algorithm against timing attacks using a
general purpose method is higher than what we are prepared to pay.
1
z[0] = r*r mod N
2
z[1] = r*r*M mod N
3
r = z[d[i]]
For example, using a table-lookup instead of a
branch [Cor99] the conditional can be replaced
by the code to the left. This ﬁxes the timing leak,
but the algorithm becomes considerably slower –
even after eliminating the common subexpres-
sion. Another even more costly approach is Agat’s cross copying idea, whereby
(roughly speaking) every branch on a secret value if h then A else B is trans-
formed into if h then A;[B] else [A];B where [A] is a ghost copy of A which
takes the same time to compute but otherwise has no eﬀect. There are opti-
misations of this approach using uniﬁcation [KM06], or by making the padding
probabilistic [DHW08], but eﬃciency wise the improvements oﬀered by those
techniques are probably not suﬃcient in this context.
1
r = 1;
2
i = m - 1;
3
k = 0;
4
while (i >= 0) {
5
r = r * (k ? x : r);
6
k = k ^ d[i];
7
i = i - (k ? 0 : 1);
8
}
Fig. 2. Protected exponentiation
A potential solution to this tension be-
tween security and eﬃciency is to make
a tradeoﬀbetween the two. For this rea-
son it is not uncommon for algorithms
to have some side channel leakage. An
example of this is the following varia-
tion on modular exponentiation, adapted
from [CMCJ04], which is intended to pro-
vide some (unspeciﬁed) degree of protec-
tion against simple power analysis attacks
(but still leaks the hamming weight of the
key).
Research Goals and Approach. Our research goal is to determine how to
express this tradeoﬀ. There are three key issues to explore:
• Security Policies: how should we specify side-channel declassiﬁcation?
• Security Mechanisms: how to derive programs which achieve the tradeoﬀ?
• Security Assurance: how can we show that programs satisfy a given policy,
with a rigorous speciﬁcation and formal veriﬁcation?
This paper deals primarily with the ﬁrst and the third point.
The ﬁrst step – a prerequisite to a rigorous speciﬁcation – is to specify our
attacker model. A model sets the boundaries of our investigation (and as always
with covert channels, there are certainly attacks which fall outside). We choose

114
J. Svenningsson and D. Sands
(as discussed in Section 3) the program counter security model [MPSW05]. This
model captures attackers performing simple power and timing analysis.
To specify a security policy we turn to work on declassiﬁcation. The concept
of declassiﬁcation has been developed speciﬁcally to allow the programmer to
specify what, where, or when a piece of information is allowed to leak (and by
whom). A simple example is a program which requires a password based login.
For this program to work it must declassify (intentionally leak) the value of the
comparison between the actual and the user supplied password strings.
Declassiﬁcation has been a recent hot topic in information ﬂow security (see
[SS05] for an overview). The standard techniques for declassiﬁcation seem largely
applicable to our problem, but there are some diﬀerences. The reason being that
(in the context of cryptographic algorithms in particular) we may be interested
in the distinction between declassifying some data directly (something which
has potentially zero cost to the attacker), and declassifying the data but only
through a side channel – the latter is what we call side channel declassiﬁcation.
We will adapt existing declassiﬁcation concepts to specify what information
we are willing to leak through timing channels (Section 4) . More speciﬁcally, we
use small programs as a speciﬁcation of what information is leaked. This follows
the style of delimited release [SM04]. As an example, we might want to specify
that a program does not leak more that the hamming weight of the key. This
can be achieved by using the program fragment in Figure 3 as a speciﬁcation: it
explicitly computes the hamming weight of the key.
1
h = 0;
2
i = m - 1;
3
while
(i >= 0) {
4
i f
(d[i] == 1) {
5
h = h + 1;
6
}
7
i = i - 1;
8
}
Fig. 3. Hamming weight computation
The formal deﬁnition of side-channel de-
classiﬁcation (Section 4) is that if the at-
tacker knows the information leaked by the
declassiﬁer then nothing more is learned by
running the program.
We then turn to the question of veriﬁca-
tion. We investigate the use of oﬀ-shelf au-
tomatic program veriﬁcation tools to verify
side-channel declassiﬁcation policies. The
ﬁrst step is to reify the side channel by
transforming the program to represent the
side-channel as part of the program state
(Section 5). This reduces the speciﬁcation of side-channel declassiﬁcation to an
extensional program property.
The next step is to observe that in many common cases we can simplify the
side-channel instrumentation. This simpliﬁcation (described in Section 5.1) does
not need to be semantics preserving – it simply needs to preserve the side-channel
declassiﬁcation condition.
As we aim to use automatic oﬀ-the-shelf model checkers we need one ﬁnal
transformation to make our programs amenable to veriﬁcation. We use self com-
position to reduce the veriﬁcation problem to a safety property of a transformed
program. Section 6 describes the approach and experiments with software model
checkers.

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
115
For various reasons the side-channel declassiﬁcation property of algorithms
can still be hard to verify. The last part of this work (Section 7) introduces a
tradeoﬀwhich makes veriﬁcation much simpler. The idea is to write programs
in what we call manifest form. In manifest form the program is written in two
parts: a declassiﬁer ﬁrst computes what is to be released, and then using this
information a side-channel secure program computes the rest.
The veriﬁcation problem amounts to showing that the second part of the
program is indeed side-channel secure (this can be rather straightforward due
to the strength of the side-channel security condition), and that the declassiﬁer
satisﬁes the property that it does not leak more through its side channel than it
leaks directly. We call these manifest declassiﬁers. Since declassiﬁers are much
simpler (and quite likely useful in many diﬀerent algorithmic contexts) veriﬁca-
tion of manifest declassiﬁers is relatively simple. We show how this technique
can overcome the veriﬁcation limitations of certain veriﬁcation tools.
An extended version of this article containing material left out for reasons of
space constraints is available as technical report [SS09].
2
Preliminaries
In this section we present the language we are going to use and set up the basic
machinery in order to deﬁne our notion of security.
Since we target cryptographic algorithms we will be using a small while lan-
guage with arrays. It’s syntax is deﬁned below:
C ∈Command ::= x=e | x[y]=e | C1; C2| if e thenC1 elseC2 |while e C | skip
e ∈Expression ::= x | x[e] | n | e1 op e2 | x ? y : z
op ∈Operators := + | ∗| −| ˆ | mod | . . .
The commands of the program should not require much explanation as they
are standard for a small while language.
One particular form of expression that we have chosen to include that may not
look very standard (for a toy language) is the ternary operator borrowed from
the language C. It’s a conditional expression that can choose between the value
of two diﬀerent register based on the value of a third register. We have restricted
it to only operate on registers since allowing it to choose between evaluating two
general expressions may give rise to side channels. This kind of operation can
typically be implemented to take a constant amount of time so that it doesn’t
exhibit a side channel by using conditional assignment that is available in e.g.
x86 machine code.
The semantics of programs is completely standard. We defer the deﬁnition
until the next section where an operational semantics is given together with
some additional instrumentation.
3
Baseline Security Model
In this section we present the semantic security model which we use to model the
attacker and to deﬁne the baseline notion of declassiﬁcation-free security. For a

116
J. Svenningsson and D. Sands
good balance between simplicity and strength we adopt an existing approach:
the program counter security model [MPSW05]. This attacker model is strong
enough to analyze simple power analysis attacks [KJJ99] – where the attacker
is assumed to be able to make detailed correlations between the power proﬁle of
a single run with the instructions executed during that run.
The idea of the program counter security model is to assume the attacker can
observe a transcript consisting of the sequence of program counter positions. This
is slightly stronger than an attacker who could perfectly deduce the sequence
of instructions executed from a (known) program given a power consumption
proﬁle of an execution. It does, however, assume that the power consumption of
a particular operation does not depend on the data it manipulates. In particular
it does not model diﬀerential power analysis.
Suppose a program operates on a state which can be partitioned into a low
(public) part, and a high (secret) part. A program is said to be Transcript-secure
if given any two states whose low parts are equal, running the program on these
respective states yields equal transcripts and ﬁnal states which also agree on
their low parts.1
To specialise this deﬁnition to our language we note that it is suﬃcient for the
attacker to observe the sequence of branch decisions in a given run in order to be
able to deduce the sequence of instructions that were executed. To this end, in
Figure 4 we give an instrumented semantics for our language which makes this
model of side channels concrete. Apart from the instrumentation (in the form
of labels on the transitions) this is a completely standard small-step operational
semantics. The transition labels, o, are either a silent step (τ), a 0 or a 1. A zero
or one is used to record which branch was taken in an if or while statement.
Deﬁnition 1 (Transcript). Let d1, d2, . . . range over {0, 1}. We say that a con-
ﬁguration ⟨C, S⟩has a transcript d1, . . . , dn if there exist conﬁgurations ⟨Ci, Si⟩,
i ∈[1, n] such that
⟨C, S⟩
τ→∗d1
→⟨C1, S1⟩
τ→∗d2
→· · ·
τ→∗dn
→⟨Cn, Sn⟩
τ→∗⟨skip, S′⟩
for some S′.
In the above case we will write [[C]]S = S′ (when we only care about the ﬁnal
state) and [[C]]T S = (S′, t) where t = d1, . . . , dn (when we are interested in the
state and the transcript).
For the purpose of this paper (and the kinds of algorithms in which we are inter-
ested in this context) we will implicitly treat [[C]] and [[C]]T as functions rather
than partial functions, thus ignoring programs which do not always terminate.
Now we can formally deﬁne the baseline security deﬁnition, which following
[MPSW05] we call Transcript-security:
1 It would be natural to assume that attackers have only polynomially bounded com-
puting power in the size of the high part of the state. For the purposes of this paper
our stronger deﬁnition will suﬃce.

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
117
⟨n, S⟩⇓n
⟨x, S⟩⇓S(x)
⟨e, S⟩⇓v
⟨x[e], S⟩⇓S(x)(v)
⟨e1, S⟩⇓v1
⟨e2, S⟩⇓v2
⟨e1 op e2, S⟩⇓v1 op v2
S(x) ̸= 0
⟨x?y : z, S⟩⇓S(y)
S(x) = 0
⟨x?y : z, S⟩⇓S(z)
⟨e, S⟩⇓v
⟨x = e, S⟩
τ→⟨skip, S[x →v]⟩
⟨e, S⟩⇓v
⟨x[y] = e, S⟩
τ→⟨skip, S[x →x[S(y) →v]]⟩
⟨skip; C, S⟩
τ→⟨C, S⟩
⟨C1, S⟩
o→⟨C′
1, S′⟩
⟨C1; C2, S⟩
o→⟨C′
1; C2, S′⟩
⟨e, S⟩⇓v
v ̸= 0
⟨if e C1 C2, S⟩
1→⟨C1, S⟩
⟨e, S⟩⇓0
⟨if e C1 C2, S⟩
0→⟨C2, S⟩
⟨e, S⟩⇓v
v ̸= 0
⟨while e C, S⟩
1→⟨C; while e C, S⟩
⟨e, S⟩⇓0
⟨while e C, S⟩
0→⟨skip, S⟩
Fig. 4. Instrumented Semantics
Deﬁnition 2 (Transcript-Security). Assume a partition of program variables
into low and high. We write R =L S if program states R and S diﬀer on at
most their high variables. We extend this to state-transcript pairs by (R, t1) =L
(S, t2) ⇐⇒R =L S & t1 = t2 reﬂecting the fact that a transcript is considered
attacker observable (low).
A program C is Transcript-secure if for all R, S, if R =L S then [[C]]T R =L
[[C]]T S.
Note that Transcript-security, as we have deﬁned it, is a very strong condition
and also very simple to check. A suﬃcient condition for Transcript-security is
that the program in question (i) does not assign values computed using high
variables to low variables, and (ii) does not contain any loops or branches on
expressions containing high variables. The main contribution of [MPSW05] is a
suite of methods for transforming programs into this form. Unfortunately the
transformation can be too costly in general, but that method is nicely comple-
mented by use of declassiﬁcation.
4
Side Channel Declassiﬁcation
To weaken the baseline deﬁnition of security we adopt one of the simplest mecha-
nisms to specify what information may be leaked about a secret: delimited release
[SM04]. The original deﬁnition of delimited release speciﬁed declassiﬁcation by
placing declassify labels on various expressions occurring in a program. The idea
is that the attacker is permitted to learn about (at most) the values of those
expressions in the initial state, but nothing more about the high part of the
state.

118
J. Svenningsson and D. Sands
We will reinterpret delimited release using a simple program rather than a
set of expressions. The idea will be to specify a (hopefully small and simple)
program D which leaks information from high variables to low ones. A program
is Transcript-secure modulo declassiﬁer D if it leaks no more than D, and this
leak occurs through the side channel.
Deﬁnition 3 (Side Channel Declassiﬁcation). Let D be a program which
writes to variables distinct from all variables occurring in C. We deﬁne C to be
Transcript-secure modulo D if for all R and S such that R =L S we have
[[C]]R =L [[C]]S & ([[D]]R = [[D]]S ⇒[[C]]T R =L [[C]]T S).
The condition on the variables written by D is purely for convenience, but is
without loss of generality. The ﬁrst clause of the deﬁnition says that the only
information leak can be through the side channel. The second clause says that
the leak is no more than what is directly leaked by D. It is perhaps helpful to
consider this clause in contrapositive form: [[C]]T R ̸=L [[C]]T S ⇒[[D]]R ̸= [[D]]S.
This means that if there is an observable diﬀerence in the transcripts of two runs
then that diﬀerence is manifest in the corresponding runs of the declassiﬁer. Note
that if we had omitted the condition [[C]]R =L [[C]]S then we would have the
weaker property that C would be allowed to leak either through the store or
through the side channel – but we wouldn’t know which. From an attackers
point of view it might take quite a bit more eﬀort to attack a program if it only
leaks though the side channel so it seems useful to make this distinction. Clearly
there are other variations possible involving multiple declassiﬁers each leaking
through a particular subset of observation channels.
5
Reifying the Side Channel
In the previous sections we have a deﬁnition of security that enables us to for-
mally establish the security of programs with respect to side channel declassiﬁ-
cation. We now turn to the problem of verifying that particular programs fulﬁl
the security condition. In order to avoid having to develop our own veriﬁcation
method we have chosen to use oﬀ-the-shelf software veriﬁcation tools.
Software veriﬁcation tools work with the standard semantics of programs. But
recall that our security condition uses an instrumented semantics which involves
a simple abstraction of side channels. In order to make it possible to use oﬀ-
the-shelf tools for our security condition we must reify the transcript so that it
becomes an explicit value in the program which the tools can reason about. It is
easy to see how to do this: we add a list-valued variable t to the program, and
transform, inductively, each conditional
if e then C else C’ into if e then
t = t++"1"; C else t = t++"0"; C’ and each while loop while e do C into
(while e do t = t++"1"; C); t= t++"0"
and inductively transform the subexpressions C and C’.

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
119
5.1
Simplifying the Instrumentation
Reifying the transcript from the instrumented semantics in this way will create
a dynamic data structure (a list) which is not bounded in size in general. Such
data structures make programs more diﬃcult to reason about, especially if we
want some form of automation in the veriﬁcation process. Luckily, there are
several circumstances which help us side step this problem. Concretely we use
two facts to simplify the reiﬁcation of the side channel.
The ﬁrst simpliﬁcation we use depends on the fact that we do not have to pre-
serve the transcript itself – it is suﬃcient that it yields the same low-equivalence
on programs. Suppose that P T is the reiﬁed variant of the program P and that
the reiﬁcation is through the addition of some low variables. In order to use P T
for veriﬁcation of side-channel security properties it is suﬃcient for it to satisfy
the following property:
∀R, S.[[P]]T R =L [[P]]T S ⇐⇒[[P T ]]R =L [[P T ]]S
We call such a P T an adequate reiﬁcation of P.
1
r = 1;
2
i = m - 1;
3
k = 0; t = 0;
4
while (i >= 0) {
5
t = t + 1;
6
r = r * (k ? x : r);
7
k = k xor d[i];
8
i = i - (k ? 0 : 1);
9
}
Fig. 5.
Instrumented
modular
exponentiation
The second simpliﬁcation that we can
perform in the construction of a reiﬁed
program is that we are speciﬁcally tar-
geting cryptographic algorithms. A com-
mon structure among the ones we have
tried to verify is that the while loops con-
tain straight line code (but potentially
conditional expressions). If it is the case
that while loops don’t contain any nested
branching or looping constructs then we
can avoid introducing a dynamic data
structure to model the transcript. Let us
refer to such programs as unnested. For
unnested programs it is simply enough to
use one fresh low variable for each occur-
rence of a branch or loop. Thus the reiﬁ-
cation transformation for unnested programs is deﬁned by applying the two
transformation rules below to each of the loops and branches respectively:
while e C ; v = 0; while e (v = v + 1; C)
(v fresh)
if e then C else C’ ; if e then v = 1; C else v = 0; C’
(v fresh)
The program in Figure 5 is an instrumented version of the program in Figure 2.
The only change is the new (low) variable t which keeps track of the number of
iterations in the while loop.
6
Self Composition
Standard automatic software model checking tools cannot reason about multiple
runs of a program. They deal exclusively with safety properties which involves

120
J. Svenningsson and D. Sands
reasoning about a single run. As is well-known, noninterference properties (like
side-channel declassiﬁcation) are not safety properties – they are deﬁned as prop-
erties of pairs of computations rather than individual ones. However, a recent
technique has emerged to reduce noninterference properties to safety properties
for the purpose of veriﬁcation. The idea appeared in [DHS03], and was explored
extensively in [BDR04] where the idea was dubbed self composition. Suppose C
is the program for which we want to verify noninterference. Let θ be a bijective
renaming function to a disjoint set of variables from those used in C. Let Cθ de-
note a variable renamed copy of C. Then the standard noninterference property
of C can be expressed as a safety property of C; Cθ viz. the Hoare triple
{∀v ∈Low.v = θ(v)}C; Cθ{∀v ∈Low.v = θv}
To extend this to deal with side-channel declassiﬁcation, let us suppose that CT
is an adequate reiﬁcation of C. Then we can verify Transcript-security modulo
D by the Hoare triple above (non side-channel security) in conjunction with:
{∀v ∈Low.v = θ(v)}D; Dθ; CT ; CT
θ {(∀x ∈W.x = θ(x)) ⇒∀y ∈Low.y = θ(y)}
where W denotes the variables written by D. Here we take advantage of the
assumption that the variables written by D are disjoint from those used in CT .
This enables us to get away with a single renaming. Note that since D is a
program and not an expression we cannot simply use it in the precondition of
the Hoare triple (c.f. [BDR04, TA05]).
6.1
Experiments Using Self Composition
As Terauchi and Aiken discovered when they used self composition, it often
resulted in veriﬁcation problems that were too hard for the model checkers to
handle [TA05]. As a result of this they developed a series of techniques for
making the result of self composition easier to verify. The main technique is the
observation that the low part of the two initial states must be equal and hence
any computation that depends only on the low part can safely be shared between
the two copies of the program. This was reported to help verifying a number of
programs. We employ the same technique in our experiments.
We have used the model checkers Blast[HJMS03] and Dagger[GR06] and ap-
plied them to self composed version of the cryptographic algorithms. In partic-
ular we have tried to verify the instrumented modular exponentiation algorithm
in Figure 5 secure modulo the hamming weight of the key (Figure 3). We have
also tried all the algorithms proposed in [CMCJ04] since they all exhibit some
form of side-channel leak and therefore have to be shown to be secure relative
that leak. None of the model checkers were powerful enough to automatically
verify the programs secure.
The main reason these tools fail seems to be that they do not reason about the
contents of arrays. Being able to reason about arrays is crucial for our running
example, as it involves computing the hamming weight of an array.

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
121
Another problem comes from the fact that the programs we wish to prove
secure may be very diﬀerent from its declassiﬁer. Relating two diﬀerent programs
with each other is a very diﬃcult task and not something that current software
model checkers are designed to do.
By helping the model checkers with some manual intervention it is possible to
verify the programs secure. Blast has a feature which allows the user to supply
their own invariants. Given the correct invariants it will succeed with the veriﬁ-
cation. However, these predicates are not checked for correctness and coming up
with them can be a highly non-trivial task. We have therefore developed another
method for veriﬁcation which we will explore in the next section.
7
Manifest Form
In this section we introduce a new way to structure programs to make veriﬁca-
tion considerably easier: Manifest Form. In manifest form the program is written
in two parts: a declassiﬁer ﬁrst computes what is to be released, and then using
this information a Transcript-secure program computes the rest. Manifest form
represents a tradeoﬀ: writing a program in manifest form may make it less ef-
ﬁcient. The idea is that the program makes the declassiﬁcation explicit in its
structure (this is similar to the speciﬁcation of relaxed noninterference [LZ05]).
But for this to be truly explicit declassiﬁcation the declassiﬁer itself should not
leak through its side channel – or more precisely, the declassiﬁer should not leak
more through its side channel than it does directly through the store.
Deﬁnition 4 (Manifest Declassiﬁer). A program D is said to be a Manifest
Declassiﬁer if for all R and S
[[D]]S =L [[D]]R ⇒[[D]]T S =L [[D]]T R
As an example of a non manifest declassiﬁer, consider the program to the left
below which declassiﬁes whether an array of length m contains all zeros. Here
the array length m, and i and the declassiﬁed value allz, are low. This is not
manifest because the transcript leaks more than the store: it reveals the position
of the ﬁrst nonzero element. A manifest version of this declassiﬁer is shown on
the right:
1
i = m - 1; allz = 1;
2
while (allz and i >= 0) {
3
allz = (d[i]? 0 : 1);
4
i = i - 1;
5
}
6
i = 0
1
i = m - 1; allz = 1;
2
while (i >= 0) {
3
allz *= (d[i]? 0 : 1);
4
i = i - 1;
5
}
Deﬁnition 5 (Manifest Form). A program P is in Manifest Form if P =
D; Q where D is a manifest declassiﬁer and Q is transcript secure.

122
J. Svenningsson and D. Sands
The program in Figure 6 is written in manifest form but otherwise it represents
the same algorithm as the program in Figure 2. The ﬁrst part of the program
(lines 1–6) computes the hamming weight of the key, d, and this (using low
variable hamming) is then used in the second part of the program to determine
the number of loop iterations.
1
hamming = 0;
2
i = m - 1;
3
while (i >= 0) {
4
hamming += (d[i] ? 1 : 0);
5
i = i + 1;
6
}
7
r = 1; k = 0;
8
i = m - 1;
9
j = m - 1 + hamming;
10
while (j >= 0) {
11
r = r * (k ? x : r);
12
k = k xor d[i];
13
i = i - (k ? 0 : 1);
14
j = j - 1;
15
}
Fig. 6. Modular Exponentiation in Manifest Form
7.1
Manifest Security Theorem
Armed with the deﬁnitions of sound manifest declassiﬁers we can now state the
theorem which is the key to the way we verify side-channel declassiﬁcation.
Theorem 1. Given a program P = D; Q with D being a sound manifest declas-
siﬁer and Q is transcript secure then P is transcript secure modulo D
This theorem helps us decompose and simplify the work of verifying that a
program in manifest form is secure. First, showing that Q is transcript secure is
straightforward as explained in section 3. Verifying that D is a sound manifest
declassiﬁer, which might seem like a daunting task given the deﬁnition, is actually
something that is within the reach of current automatic tools for model checking.
We apply the same techniques of reifying the side channel and self composition
to the problem of verifying sound manifest declassiﬁers. When doing so we have
been able to verify that our implementation of the hamming weight computation
in Figure 3 is indeed a sound manifest declassiﬁer and thereby establishing the
security of the modular exponentiation algorithm in Figure 6. We have had the
same success2 with all the algorithms presented in [CMCJ04].
8
Related Work
The literature on programming language techniques for information ﬂow secu-
rity is extensive. Sabelfeld and Myers survey [SM03] although some seven years
old remains the standard reference in the ﬁeld. It is notable that almost all of
2 Using Blast version 2.5.

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
123
the work in the area has ignored timing channels. However any automated se-
curity checking that does not model timing will accept a program which leaks
information through timing, no matter how blatant the leak is.
Agat [Aga00] showed how a type system for secure information ﬂow could be
extended to also transform out certain timing leaks by padding the branches of
appropriate conditionals. K¨opf and Mantel give some improvements to Agat’s
approach based on code uniﬁcation [KM06]. In a related line, Sabelfeld and
Sands considered timing channels arising from concurrency, and made use of
Agat’s approach [SS00]. Approximate and probabilistic variants of these ideas
have also emerged [PHSW07, DHW08]. The problem with padding techniques
in general is that they do not change the fundamental structure of a leaky
algorithm, but use the “worst-case principle” [AS01] to make all computation
paths equally slow. For cryptographic algorithms this approach is probably not
acceptable from a performance perspective.
Hedin and Sands [HS05, Hed08] consider applying Agat’s approach in the
context of Java bytecode. One notable contribution is the use of a family of time
models which can abstract timing behaviour at various levels of accuracy, for
example to model simple cache behaviour or instructions whose time depends
on runtime values (e.g. array allocation). The deﬁnitions and analysis are param-
eterised over the time models. The program counter security model [MPSW05]
can be seen as an instance of this parameterised model.
More speciﬁc to the question of declassiﬁcation and side channels, as we men-
tioned above, [DHW08] estimates the capacity of a side channel – something
which can be used to determine whether the leak is acceptable – and propose
an approximate version of Agat’s padding technique. Giacobazzi and Mastroeni
[GM05] recently extended the abstract noninterference approach to character-
ising what information is leaked to include simple timing channels. Their theo-
retical framework could be used to extend the present work. In particular they
conclude with a theoretical condition which, in principle, could be used to verify
manifest declassiﬁers. K¨opf and Basin’s study of timing channels in synchronous
systems[KB06] is the most closely related to the current paper. They study a
Per model for expressing declassiﬁcation properties in a timed setting – an ab-
stract counterpart to the more programmer-oriented delimited release approach
used here. They also study veriﬁcation for deterministic systems by the use of
reachability in a product automaton – somewhat analogous to our use of self
composition. Finally their examples include leaks of hamming weight in a ﬁnite-
ﬁeld exponentiation circuit.
9
Conclusions and Further Work
Reusing theoretical concepts and practical veriﬁcation tools we have introduced
a notion of side channel declassiﬁcation and shown how such properties can be
veriﬁed by a combination of simple transformations and application of oﬀ-the-
shelf software model checking tools. We have also introduced a new method to
specify side-channel declassiﬁcation, manifest form, a form which makes the se-
curity property explicit in the program structure, and makes veriﬁcation simpler.

124
J. Svenningsson and D. Sands
We have applied these techniques to verify the relative security of a number of
cryptographic algorithms. It remains to investigate how to convert a given pro-
gram into manifest form. Ideas from [MPSW05, LZ05] may be adaptable to
obtain the best of both worlds: a program without the overhead of manifest
form, but satisfying the same side-channel declassiﬁcation property.
References
[Aga00]
Agat, J.: Transforming out timing leaks. In: Proc. ACM Symp. on Prin-
ciples of Programming Languages, January 2000, pp. 40–53 (2000)
[AS01]
Agat, J., Sands, D.: On conﬁdentiality and algorithms. In: Proc. IEEE
Symp. on Security and Privacy, May 2001, pp. 64–77 (2001)
[BB05]
Brumley, D., Boneh, D.: Remote timing attacks are practical. Journal of
Computer and Telecommunications Networking 48, 701–716 (2005)
[BDR04]
Barthe, G., D’Argenio, P., Rezk, T.: Secure information ﬂow by self-
composition. In: Proceedings of CSFW 2004, June 2004. LNCS, pp. 100–
114. IEEE Press, Los Alamitos (2004)
[CMCJ04]
Chevallier-Mames, B., Ciet, M., Joye, M.: Low-cost solutions for prevent-
ing simple sidechannel analysis: Side-channel atomicity. IEEE Transac-
tions on Computers 53(6), 760–768 (2004)
[Cor99]
Coron, J.-S.: Resistance against diﬀerential power analysis for elliptic
curve cryptosystems. In: Koc, C.K., Paar, C. (eds.) Cryptographic Hard-
ware and Embedded Systems, pp. 292–302 (1999)
[DHS03]
Darvas, A., H¨ahnle, R., Sands, D.: A theorem proving approach to analy-
sis of secure information ﬂow. In: Proc. Workshop on Issues in the Theory
of Security (April 2003)
[DHW08]
Di Pierro, A., Hankin, C., Wiklicky, H.: Quantifying timing leaks and
cost optimisation. In: Chen, L., Ryan, M.D., Wang, G. (eds.) ICICS 2008.
LNCS, vol. 5308, pp. 81–96. Springer, Heidelberg (2008)
[GM05]
Giacobazzi, R., Mastroeni, I.: Timed abstract non-interference. In: Pet-
tersson, P., Yi, W. (eds.) FORMATS 2005. LNCS, vol. 3829, pp. 289–303.
Springer, Heidelberg (2005)
[GR06]
Gulavani, B.S., Rajamani, S.K.: Counterexample driven reﬁnement for
abstract interpretation. In: Hermanns, H., Palsberg, J. (eds.) TACAS
2006. LNCS, vol. 3920, pp. 474–488. Springer, Heidelberg (2006)
[Hed08]
Hedin, D.: Program analysis issues in language based security. PhD thesis,
Department of Computer Science and Engineering, Chalmers University
of Technology (2008)
[HJMS03]
Henzinger, T.A., Jhala, R., Majumdar, R., Sutre, G.: Software veriﬁca-
tion with blast. In: Ball, T., Rajamani, S.K. (eds.) SPIN 2003. LNCS,
vol. 2648, pp. 235–239. Springer, Heidelberg (2003)
[HS05]
Hedin, D., Sands, D.: Timing aware information ﬂow security for a
JavaCard-like bytecode. In: First Workshop on Bytecode Semantics, Ver-
iﬁcation, Analysis and Transformation (BYTECODE 2005). Electronic
Notes in Theoretical Computer Science (2005) (to appear)
[KB06]
K¨opf, B., Basin, D.A.: Timing-sensitive information ﬂow analysis for syn-
chronous systems. In: Gollmann, D., Meier, J., Sabelfeld, A. (eds.) ES-
ORICS 2006. LNCS, vol. 4189, pp. 243–262. Springer, Heidelberg (2006)

Speciﬁcation and Veriﬁcation of Side Channel Declassiﬁcation
125
[KJJ99]
Kocher, P., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. In: Wiener, M.
(ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 388–397. Springer, Heidelberg
(1999)
[KM06]
K¨opf, B., Mantel, H.: Eliminating implicit information leaks by transfor-
mational typing and uniﬁcation. In: Dimitrakos, T., Martinelli, F., Ryan,
P.Y.A., Schneider, S. (eds.) FAST 2005. LNCS, vol. 3866, pp. 47–62.
Springer, Heidelberg (2006)
[Koc96]
Kocher, P.C.: Timing attacks on implementations of Diﬃe-Hellman, RSA,
DSS, and other systems. In: Koblitz, N. (ed.) CRYPTO 1996. LNCS,
vol. 1109, pp. 104–113. Springer, Heidelberg (1996)
[Lam73]
Lampson, B.W.: A note on the conﬁnement problem. Comm. of the
ACM 16(10), 613–615 (1973)
[LZ05]
Li, P., Zdancewic, S.: Downgrading policies and relaxed noninterference.
In: Proc. ACM Symp. on Principles of Programming Languages, January
2005, pp. 158–170 (2005)
[MDS99]
Messergers, T.S., Dabbish, E.A., Sloan, R.H.: Power analysis attacks on
modular exponentiation in smartcards, in cryptographic hardware and
embedded systems. In: Ko¸c, C¸.K., Paar, C. (eds.) CHES 1999. LNCS,
vol. 1717, pp. 144–157. Springer, Heidelberg (1999)
[MPSW05]
Molnar, D., Piotrowski, M., Schultz, D., Wagner, D.: The program counter
security model: Automatic detection and removal of control-ﬂow side
channel attacks. In: Won, D.H., Kim, S. (eds.) ICISC 2005. LNCS,
vol. 3935, pp. 156–168. Springer, Heidelberg (2006)
[MS00]
Mayer-Sommer, R.: Smartly analyzing the simplicity and the power of
simple power analysis on smartcards. In: Paar, C., Ko¸c, C¸.K. (eds.) CHES
2000. LNCS, vol. 1965, pp. 78–92. Springer, Heidelberg (2000)
[PB02]
Puschner, P., Burns, A.: Writing temporally predictable code. In: 7th
IEEE International Workshop on Object-Oriented Real-Time Dependable
Systems (2002)
[PHSW07]
Di Pierro, A., Hankin, C., Siveroni, I., Wiklicky, H.: Tempus fugit: How
to plug it. J. Log. Algebr. Program. 72(2), 173–190 (2007)
[SM03]
Sabelfeld, A., Myers, A.C.: Language-based information-ﬂow security.
IEEE J. Selected Areas in Communications 21(1), 5–19 (2003)
[SM04]
Sabelfeld, A., Myers, A.C.: A model for delimited information release.
In: Futatsugi, K., Mizoguchi, F., Yonezaki, N. (eds.) ISSS 2003. LNCS,
vol. 3233, pp. 174–191. Springer, Heidelberg (2004)
[SS00]
Sabelfeld, A., Sands, D.: Probabilistic noninterference for multi-threaded
programs. In: Proc. IEEE Computer Security Foundations Workshop,
July 2000, pp. 200–214 (2000)
[SS05]
Sabelfeld, A., Sands, D.: Dimensions and principles of declassiﬁcation. In:
Proceedings of the 18th IEEE Computer Security Foundations Workshop,
Cambridge, England, pp. 255–269. IEEE Computer Society Press, Los
Alamitos (2005)
[SS09]
Svenningsson, J., Sands, D.: Speciﬁcation and veriﬁcation of side channel
declassiﬁcation. Technical Report 2009:13, Department of Computer Sci-
ence and Engineering, Chalmers University of Technology and University
of Gothenburg (December 2009) arXiv:0912.2952 (cs.CR)
[TA05]
Terauchi, T., Aiken, A.: Secure information ﬂow as a safety problem. In:
Proceedings of the 12th International Static Analysis Symposium, pp.
352–367 (2005)

Secure Information Flow for Distributed Systems
Rafael Alp´ızar and Geoffrey Smith
School of Computing and Information Sciences, Florida International University,
Miami, FL 33199, USA
{ralpi001,smithg}@cis.fiu.edu
Abstract. We present an abstract language for distributed systems of processes
with local memory and private communication channels. Communication be-
tween processes is done via messaging. The language has high and low data and
is limited only by the Denning restrictions; this is a signiﬁcant relaxation as com-
pared to previous languages for concurrency. We argue that distributed systems
in the abstract language are observationally deterministic, and use this result to
show that well-typed systems satisfy termination-insensitive noninterference; our
proof is based on concepts of stripping and fast simulation, which are a valuable
alternative to bisimulation. We then informally explore approaches to implement
this language concretely, in the context of a wireless network where there is a risk
of eavesdropping of network messages. We consider how asymmetric cryptogra-
phy could be used to realize the conﬁdentiality of the abstract language.
1
Introduction
In this paper we craft a high-level imperative language for distributed systems. Our goal
is to provide the programmer with a simple and safe abstract language, with a built-in
API to handle communications between processes. The abstract language should hide
all messy communication protocols that control the data exchange between processes
and all the cryptographic operations that ensure the conﬁdentiality of the data trans-
mitted. We also want to classify variables into different security levels, and we want a
secure information ﬂow property that says that distributed system cannot leak informa-
tion from higher to lower levels. We would like our language to have a clean familiar
syntax and to have the maximum power of expression that we can give it.
A distributed system thus, is a group of programs executing in a group of nodes such
that there is at least one program per node. An executing program with its local data is
a process. As our processes may reside in separate nodes, they should have their own
private memories and be able to send and receive messages from other processes.
We would like to classify data according to a security lattice, which in our case will
be limited to H and L, and we would like to maintain the ability to transmit and receive
H and L values. To do this we will need separate channels for each classiﬁcation, oth-
erwise we would only be able to receive messages using H variables as demonstrated
in adversary Δ1 of Figure 1. In this attack, Process 1 sends a H variable on channel
a, but Process 2 receives it into a L variable. Because of subsumption, H channels can
transmit H or L data but the receiving variable must be typed H while L channels
can only transmit L data. Therefore our communication channels must have a security
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 126–140, 2010.
c⃝Springer-Verlag Berlin Heidelberg 2010

Secure Information Flow for Distributed Systems
127
-Δ1-
Process 1
send(a, h1)
Process 2
receive(a, l2)
-Δ2-
Process 1
if (h1 is even) then
run a long time;
send(a, 1)
Process 2
if (h2 is odd) then
run a long time;
send(a, 0)
Process 3
run a short time;
receive(a, l3)
-Δ3-
Process 1
if (h1 is even) then
run a long time;
send(a, 1)
Process 2
run a short time;
if (channel a has data)
then l2 := 0
else l2 := 1
Fig. 1. Attacks (ﬁrst wave)
classiﬁcation. What else do they need? We shall see that channels also need a speciﬁc
source process and a speciﬁc destination process. The reason is exempliﬁed by dis-
tributed system Δ2 of Figure 1. In this attack, Process 1 and Process 2 both send on
channel a. If we assume that h1 and h2 are initialized to the same secret value, then the
last bit of this value is leaked into l3 (assuming sufﬁciently “fair” scheduling). Our type
system prevents attacks Δ1 and Δ2 by giving each channel a type of the form τ chi,j
that speciﬁes the security level (τ) and the sending (i) and receiving (j) processes.
Also, processes cannot be allowed to test if a channel has data because this ability
would also render the language unsound by allowing timing channels. This is illustrated
by distributed system Δ3 of Figure 1. This attack leaks the last bit of h1 to l2. When h1
is even Process 1 takes a long time to send its message so when Process 2 checks, the
channel will be empty. Therefore we do not allow processes to make such tests.
Because a process trying to receive from a channel must block until a message is
available, the programmer has to be careful to ensure that for each receive, there is a
corresponding send. The converse, however, is not required since a process may send a
message that is never received. Indeed, processes should not be required to wait on send
and should be able to send multiple times on the same channel. To handle this, we will
need an unbounded buffer for each channel, where sent messages wait to be received.
Once we have some idea of what the language should be like, we would like to know
that it is safe and argue a noninterference (NI) property on it. But we would like to
restrict processes as little as possible. To this end, we explore the possibility of typing
processes using only the classic Denning restrictions [1], which disallows an assign-
ment l := e to a L variable if either e contains H variables or if the assignment is
within an if or while command whose guard contains H variables. This would be in
sharp contrast to prior works in secure information ﬂow for concurrent programs (such
as [2,3,4,5]) which have required severe restrictions to prevent H variables from af-
fecting the ordering of assignments to shared memory. Our language, being based on
message passing rather than shared memory, is much less dependent on timing and the
behavior of the scheduler. Indeed, it turns out that our distributed systems are observa-
tionally deterministic which means that, despite our use of a purely nondeterministic

128
R. Alp´ızar and G. Smith
process scheduler, the ﬁnal result of programs is uniquely determined by the initial
memories.
Next we would like to explore what it would take to implement our language in a
concrete setting. We wish our operational model to be as “close to the ground” as we
can. We would like something like a wireless LAN where eavesdroppers can see all
communications; what would it take to implement a safe language there? Obviously
secret data cannot be transmitted in a wireless LAN with any expectation of conﬁden-
tiality, hence, we need cryptography. What kind? Asymmetric cryptography seems to
be the appropriate style for our setting.
The paper is organized as follows: Section 2 formally deﬁnes the abstract language
for distributed systems and argues the key noninterference theorem; our proof is not
based on bisimulation, however, but instead on concepts of stripping and fast simu-
lation as in [6]. In Section 3 we informally explore what it would take to implement
the abstract language in a concrete setting over a public network, and we work out our
adversarial model. Section 4 presents related work and Section 5 concludes the paper.
2
An Abstract Language for Distributed Systems
This section deﬁnes an abstract language for distributed systems. The language syntax
(Figure 2) is that of the simple imperative language except that processes are added to
the language and they may send or receive messages from other processes. Note that
pseudocommand done is used to denote a terminated command. It may not be used
as a subcommand of another command, except that for technical reasons we do allow
the branches of an if command to be done. Allowing this also has the minor practical
beneﬁt of letting us code “if e then c” by “if e then c else done”.
(phrases)
p ::= e | c
(variables)
x, y, z, . . .
(channel ids) a, b, . . .
(process ids) i, j, . . .
(expressions) e ::= x | n | e1 + e2 | . . .
(commands) c ::= done | skip | x := e |
send(a, e) | receive(a, x) |
if e then c1 else c2 |
while e do c | c1; c2
Fig. 2. Abstract Language Syntax
Semantics of processes (−→): Each process can refer to its local memory μ, which
maps variables to integers, and to the global network memory Φ, which maps chan-
nel identiﬁers to lists of messages currently waiting to be received; we start execution
with an empty network memory Φ0 such that Φ0(a) = [ ], for all a. Thus we specify
the semantics of a process via judgments of the form (c, μ, Φ)−→(c′, μ′, Φ′). We use a
standard small-step semantics with the addition of rules for the send and receive com-
mands; the rules are shown in Figure 3. In the rules, we write μ(e) to denote the value
of expression e in memory μ. The rule for send(a, e) updates the network memory by
adding the value of e to the end of the list of messages waiting on channel a. The rule
for receive(a, x) requires that there be at least one message waiting to be received on
channel a; it removes the ﬁrst such message and assigns it to x.

Secure Information Flow for Distributed Systems
129
Semantics of distributed systems (=⇒): We model a distributed system as a function
Δ that maps process identiﬁers to pairs (c, μ) consisting of a command and a local
memory. A global conﬁguration then has the form (Δ, Φ), and rule globals deﬁnes the
purely nondeterministic behavior of the process scheduler, which at each step can select
any process that is able to make a transition.
updates x ∈dom(μ)
(x := e, μ, Φ)−→
(done, μ[x := μ(e)], Φ)
if s
μ(e) ̸= 0
(if e then c1 else c2, μ, Φ)−→
(c1, μ, Φ)
μ(e) = 0
(if e then c1 else c2, μ, Φ)−→
(c2, μ, Φ)
whiles
μ(e) = 0
(while e do c, μ, Φ)−→
(done, μ, Φ)
μ(e) ̸= 0
(while e do c, μ, Φ)−→
(c; while e do c, μ, Φ)
skips
(skip, μ, Φ)−→(done, μ, Φ)
composes (c1, μ, Φ)−→(done, μ′, Φ′)
(c1; c2, μ, Φ)−→(c2, μ′, Φ′)
(c1, μ, Φ)−→(c′
1, μ′, Φ′) c′
1 ̸= done
(c1; c2, μ, Φ)−→(c′
1; c2, μ′, Φ′)
sends
Φ(a) = [m1, · · · , mk] k ≥0
(send(a, e), μ, Φ)−→
(done, μ, Φ[a := [m1, . . . , mk, μ(e)]])
receives
Φ(a) = [m1, . . . , mk] k ≥1
(receive(a, x), μ)−→
(done, μ[x := m1],
Φ[a = [m2, . . . , mk]])
globals
Δ(i) = (c, μ)
(c, μ, Φ)−→(c′, μ′, Φ′)
(Δ, Φ)=⇒(Δ[i := (c′, μ′)], Φ′)
Fig. 3. Abstract Language Semantics
The Type System: Figure 4 shows the type system of the abstract language; its rules
use an identiﬁer typing Γ that maps identiﬁers to types. The typing rules enforce only
the Denning restrictions [1]; in particular notice that we allow the guards of while loops
to be H. Channels are restricted to carrying messages of one security classiﬁcation from
a speciﬁc process i to a speciﬁc process j and accordingly are typed Γ(a) = τ chi,j
where τ is the security classiﬁcation of the data that can travel in the channel, i is
the source process and j is the destination process. So to enable full communications
between processes i and j we need four channels with types H chi,j, H chj,i, L chi,j,
and L chj,i. In a typing judgment
Γ, i ⊢c : τ cmd
the process identiﬁer i speciﬁes which process command c belongs to; this is used to
enforce the rule that only process i can send on a channel with type τ chi,j or receive on
a channel with type τ chj,i. We therefore say that a distributed system Δ is well typed
if Δ(i) = (c, μ) implies that Γ, i ⊢c : τ cmd, for some τ.
Language Soundness: We now argue soundness properties for our language and type
system, starting with some standard properties, whose proofs are straightforward.
Lemma 1 (Simple Security). If Γ, i ⊢e : τ, then e contains only variables of level τ
or lower.

130
R. Alp´ızar and G. Smith
sec
τ ::= H | L
phrase ρ ::= τ | τ var |
τ cmd | τ chi,j
base
L ⊆H
cmd
τ ⊆τ ′
τ ′ cmd ⊆τ cmd
reﬂex
ρ ⊆ρ
trans
ρ1 ⊆ρ2
ρ2 ⊆ρ3
ρ1 ⊆ρ3
subsump
Γ, i ⊢p : ρ1
ρ1 ⊆ρ2
Γ, i ⊢p : ρ2
terminalt
Γ, i ⊢done : H cmd
skipt
Γ, i ⊢skip : H cmd
intt
Γ, i ⊢n : L
rvalt
Γ(x) = τ var
Γ, i ⊢x : τ
updatet
Γ(x) = τ var Γ, i ⊢e : τ
Γ, i ⊢x := e : τ cmd
plust
Γ, i ⊢e1 : τ Γ, i ⊢e2 : τ
Γ, i ⊢e1 + e2 : τ
if t
Γ, i ⊢e : τ
Γ, i ⊢c1 : τ cmd Γ, i ⊢c2 : τ cmd
Γ, i ⊢if e then c1 else c2 : τ cmd
whilet
Γ, i ⊢e : τ Γ, i ⊢c1 : τ cmd
Γ, i ⊢while e do c1 : τ cmd
composet
Γ, i ⊢c1 : τ cmd Γ, i ⊢c2 : τ cmd
Γ, i ⊢c1; c2 : τ cmd
sendt
Γ(a) = τ chi,j Γ, i ⊢e : τ
Γ, i ⊢send(a, e) : τ cmd
receivet
Γ(a) = τ chj,i Γ(x) = τ var
Γ, i ⊢receive(a, x) : τ cmd
Fig. 4. Abstract Language Type System
Lemma 2 (Conﬁnement). If Γ, i ⊢c : τ cmd, then c assigns only to variables of level
τ or higher, and sends or receives only on channels of level τ or higher.
Lemma 3 (Subject Reduction). If Γ, i ⊢c : τ cmd and (c, μ, Φ) −→(c′, μ′, Φ′), then
Γ, i ⊢c′ : τ cmd.
We now turn to more interesting properties. We begin by deﬁning terminal global con-
ﬁgurations; these are simply conﬁgurations in which all processes have terminated:
Deﬁnition 1. A global conﬁguration (Δ, Φ) is terminal if for all i, Δ(i) = (done, μi)
for some μi.
Notice that we do not require that Φ be an empty network memory—we allow it to
contain unread messages.
Now we argue that, in spite of the nondeterminism of rule globals, our distributed
programs are observationally deterministic [7], in the sense that each program can reach
at most one terminal conﬁguration.
Theorem 1 (Observational Determinism). Suppose that Δ is well typed and that
(Δ, Φ)=⇒∗(Δ1, Φ1) and (Δ, Φ)=⇒∗(Δ2, Φ2), where (Δ1, Φ1) and (Δ2, Φ2) are ter-
minal conﬁgurations. Then (Δ1, Φ1) = (Δ2, Φ2).

Secure Information Flow for Distributed Systems
131
Proof. We begin by observing that the behavior of each process i is completely in-
dependent of the rest of the distributed system, with the sole exception of its receive
commands. Thus if we specify the sequence of messages [m1, m2, . . . , mn] that pro-
cess i receives during its execution, then process i’s behavior is completely determined.
(Notice that the sequence [m1, m2, . . . , mn] merges together all of the messages that
process i receives on any of its input channels.)
We now argue by contradiction. Suppose that we can run from (Δ, Φ) to two different
terminal conﬁgurations, (Δ1, Φ1) and (Δ2, Φ2). By the discussion above, it must be that
some process receives a different sequence of messages in the two runs. So consider the
ﬁrst place in the second run (Δ, Φ)=⇒∗(Δ2, Φ2) where a process i receives a different
message than it does in the ﬁrst run (Δ, Φ)=⇒∗(Δ1, Φ1). But for this to happen, there
must be another process j that earlier sent a different message to i than it does in the
ﬁrst run. (Note that this depends on the fact that, in a well-typed distributed system,
any channel can be sent to by just one process and received from by just one process.)
But for j to send a different message than in the ﬁrst run, it must itself have received
a different message earlier. This contradicts the fact that we chose the ﬁrst place in the
second run where a different message was received.
⊓⊔
We now wish to argue that well-typed distributed systems satisfy a termination-
insensitive noninterference property. (We certainly need a termination-insensitive prop-
erty since, under the Denning restrictions, H variables can affect termination.)
Deﬁnition 2. Two memories μ and ν are L-equivalent, written μ ∼L ν, if they agree
on the values of all L variables. Similarly, two network memories Φ and Φ′ are L-
equivalent, also written Φ ∼L Φ′, if they agree on the values of all L channels.
Now we wish to argue that if we run a distributed system twice, using L-equivalent
initial memories for each process, then, assuming that both runs terminate successfully,
we must reach L-equivalent ﬁnal memories for each process. A standard way to prove
such a result is by establishing some sort of low bisimulation between the two runs.
However this does not seem to be possible for our abstract language, because changing
the values of H variables can affect when receive commands are able to be executed.
Figure 5 shows an example that illustrates the difﬁculty. Suppose we run this program
twice, using two L-equivalent memories for Process 1, namely [h1 = 1, l1 = 0] and
[h1 = 0, l1 = 0], and the same memory for Process 2, [h2 = 0, l2 = 0]. Under the ﬁrst
memory, Process 1 immediately sends on channel aH,1,2, which then allows Process
2 to do its receive and then to assign to l2 before Process 1 assigns to l1. But under
the second memory, Process 1 does not send on channel aH,1,2 until after assigning to
l1, which means that the assignment to l2 must come after the assignment to l1. Thus
Process 1
if h1 then send(aH,1,2, 1) else done;
l1 := 2;
send(aH,1,2, 2)
Process 2
receive(aH,1,2, h2);
l2 := 3
Fig. 5. A difﬁcult example for low bisimulation

132
R. Alp´ızar and G. Smith
the two runs are not low bisimilar. (Notice that the two runs are ﬁne with respect to
noninterference, however—in both cases we end up with l1 = 2 and l2 = 3.)
Because of this difﬁculty, we develop a different approach to noninterference, via the
concepts of stripping and fast simulation, which were ﬁrst used in [6]. Intuitively, the
processes in Figure 5 contain H commands that are irrelevant to the L variables, except
that they can cause delays. If we strip them out, we are left with
Process 1
Process 2
l1 := 2
l2 := 3
This shows what will happen to the L variables if the system terminates. We therefore
introduce a stripping operation that eliminates all subcommands of type H cmd, so that
the delays that such subcommands might have caused are eliminated. More precisely,
we have the following deﬁnition:
Deﬁnition 3. Let c be a well-typed command. We deﬁne ⌊c⌋= done if c has type
H cmd; otherwise, deﬁne ⌊c⌋by
– ⌊x := e⌋= x := e
– ⌊ife then c1 else c2⌋= if e then⌊c1 ⌋else ⌊c2⌋
– ⌊while e do c1⌋= while e do ⌊c1⌋
– ⌊send(a, e)⌋= send(a, e)
– ⌊receive(a, x)⌋= receive(a, x)
– ⌊c1; c2⌋=
⎧
⎨
⎩
⌊c2⌋
if c1 : H cmd
⌊c1⌋
if c2 : H cmd
⌊c1⌋; ⌊c2⌋otherwise
Also, we deﬁne ⌊μ⌋to be the result of deleting all H variables from μ, and ⌊Φ⌋to be
the result of deleting all H channels from Φ. We extend ⌊·⌋to well-typed global conﬁg-
urations by ⌊(Δ, Φ)⌋= (⌊Δ⌋, ⌊Φ⌋), where if Δ(i) = (c, μ), then ⌊Δ⌋(i) = (⌊c⌋, ⌊μ⌋).
We remark that stripping as deﬁned in [6] replaces subcommands of type H cmd with
skip; in contrast our new deﬁnition here aggressively eliminates such subcommands.
Note also that ⌊μ⌋∼L μ and ⌊Φ⌋∼L Φ. Now we have a simple lemma:
Lemma 4. For any c, ⌊c⌋contains only L variables and channels.
Proof. By induction on the structure of c. If c has type H cmd, then ⌊c⌋= done, which
(vacuously) contains only L variables and channels.
If c does not have type H cmd, then consider the form of c. If c is x := e, then
⌊c⌋= x := e. Since c does not have type H cmd, then by rule updatet we must have
that x is a L variable and e : L, which implies by Simple Security that e contains only
L variables. The cases of send(a, e) and receive(a, x) are similar. If c is while e do c1,
then ⌊c⌋= while e do ⌊c1⌋. By rule whilet e : L, which implies by Simple Security
that e contains only L variables and channels. And, by induction, ⌊c1⌋contains only L
variables and channels. The cases of if e then c1 else c2 and c1; c2 are similar.
⊓⊔
Now the key result that we wish to establish is that ⌊(Δ, Φ)⌋can simulate (Δ, Φ), up to
the ﬁnal values of L variables. To this end we ﬁrst adapt fast simulation from [6] (which
in turn was based on strong and weak simulation in Baier et al [8]) to a nondeterministic
(rather than probabilistic) setting.

Secure Information Flow for Distributed Systems
133
Deﬁnition 4. A binary relation R on global conﬁgurations is a fast low simulation with
respect to =⇒if whenever (Δ1, Φ1)R(Δ2, Φ2) we have
1. (Δ1, Φ1) and (Δ2, Φ2) agree on the values of L variables and channels, and
2. if (Δ1, Φ1)=⇒(Δ′
1, Φ′
1), then either (Δ′
1, Φ′
1)R(Δ2, Φ2) or there exists (Δ′
2, Φ′
2)
such that (Δ2, Φ2)=⇒(Δ′
2, Φ′
2) and (Δ′
1, Φ′
1)R(Δ′
2, Φ′
2). That is, (Δ2, Φ2) can
match, in zero or one steps, any move from (Δ1, Φ1). In pictures:
(Δ′
2, Φ′
2)
R
(Δ2, Φ2)
(Δ′
1, Φ′
1)
(Δ1, Φ1)
R
or
⇓
⇓
R
Viewing our stripping function ⌊·⌋as a relation, we write (Δ1, Φ1) ⌊·⌋(Δ2, Φ2) if
⌊(Δ1, Φ1)⌋= (Δ2, Φ2). Here is the key theorem about the stripping relation ⌊·⌋:
Theorem 2. ⌊·⌋is a fast low simulation with respect to =⇒.
Proof. First, it is immediate from the deﬁnition of ⌊·⌋that (Δ, Φ) and ⌊(Δ, Φ)⌋agree
on the values of L variables and channels.
Next we must show that any move from (Δ, Φ) can be matched by ⌊(Δ, Φ)⌋in zero
or one steps. Suppose that the move from (Δ, Φ) involves a step on process i. Then
we must have Δ(i) = (c, μ), (c, μ, Φ)−→(c′, μ′, Φ′), and Δ′ = Δ[i := (c′, μ′)]. To
show that ⌊(Δ, Φ)⌋can match this move in zero or one steps, note that ⌊(Δ, Φ)⌋=
(⌊Δ⌋, ⌊Φ⌋) and that ⌊Δ⌋(i) = (⌊c⌋, ⌊μ⌋). Hence it sufﬁces to show that either
(⌊c⌋, ⌊μ⌋, ⌊Φ⌋) = (⌊c′⌋, ⌊μ′⌋, ⌊Φ′⌋)
or else
(⌊c⌋, ⌊μ⌋, ⌊Φ⌋)−→(⌊c′⌋, ⌊μ′⌋, ⌊Φ′⌋).
We argue this by induction on the structure of c.
If c has type H cmd, then ⌊c⌋= done. Also, by Conﬁnement we have μ ∼L μ′ and
Φ ∼L Φ′, which implies that ⌊μ′⌋= ⌊μ⌋, and ⌊Φ′⌋= ⌊Φ⌋. And by Subject Reduction we
have c′ : H cmd, which implies that ⌊c′⌋= done. So the move (c, μ, Φ)−→(c′, μ′, Φ′)
is matched in zero steps by (done, ⌊μ⌋, ⌊Φ⌋).
If c does not have type H cmd, then consider the possible forms of c:
1. c = x := e. Here ⌊c⌋= c. By updatet, x : L var and e : L. So by Simple Se-
curity ⌊μ⌋(e) = μ(e), which implies that ⌊μ⌋[x := ⌊μ⌋(e)] = ⌊μ[x := μ(e)]⌋.
Hence the move (c, μ, Φ)−→(done, μ[x := μ(e)], Φ) is matched by the move
(c, ⌊μ⌋, ⌊Φ⌋)−→(done, ⌊μ⌋[x := ⌊μ⌋(e)], ⌊Φ⌋).
2. c = send(a, e). Here ⌊c⌋= c. By sendt, a is a low channel and e : L. Hence
⌊Φ⌋(a) = Φ(a) = [m1, . . . , mk]. Also, by Simple Security, ⌊μ⌋(e) = μ(e).
Hence the move (c, μ, Φ)−→(done, μ, Φ[a := [m1, . . . , mk, μ(e)]) is matched by
the move (c, ⌊μ⌋, ⌊Φ⌋)−→(done, ⌊μ⌋, ⌊Φ⌋[a := [m1, . . . , mk, ⌊μ⌋(e)]).
3. c = receive(a, x). Here ⌊c⌋= c. By receivet, a is a low channel and x : L var.
Hence ⌊Φ⌋(a) = Φ(a) = [m1, . . . , mk], where k ≥1. Therefore, the move
(c, μ, Φ)−→(done, μ[x := m1], Φ[a := [m2, . . . , mk]]) is matched by the move
(c, ⌊μ⌋, ⌊Φ⌋)−→(done, ⌊μ⌋[x := m1], ⌊Φ⌋[a := [m2, . . . , mk]]).

134
R. Alp´ızar and G. Smith
4. c = if e then c1 else c2. Here ⌊c⌋= if e then ⌊c1⌋else ⌊c2⌋. By if t, e : L and
by Simple Security μ(e) = ⌊μ⌋(e). So if μ(e) ̸= 0, then (c, μ, Φ)−→(c1, μ, Φ)
is matched by (⌊c⌋, ⌊μ⌋, ⌊Φ⌋)−→(⌊c1⌋, ⌊μ⌋, ⌊Φ⌋). The case when μ(e) = 0 is
similar.
5. c = while e do c1. Here ⌊c⌋= while e do ⌊c1⌋. By whilet, e : L and c1 does
not have type H cmd. By Simple Security, we have ⌊μ⌋(e) = μ(e). So in case
μ(e) ̸= 0, then the move (c, μ, Φ)−→(c1; while e do c1, μ, Φ) is matched by the
move (⌊c⌋, ⌊μ⌋, ⌊Φ⌋)−→(⌊c1⌋; while e do ⌊c1⌋, ⌊μ⌋, ⌊Φ⌋). (This uses the fact that
⌊c1; while e do c1⌋= ⌊c1⌋; while e do ⌊c1⌋.) The case when μ(e) = 0 is similar.
6. c = c1; c2. Here ⌊c⌋= ⌊c1; c2⌋has three possible forms: ⌊c1⌋; ⌊c2⌋, if neither c1
nor c2 has type H cmd (ﬁrst subcase); ⌊c2⌋, if c1 : H cmd (second subcase); or
⌊c1⌋, if c2 : H cmd (third subcase).
In the ﬁrst subcase, neither c1 nor c2 has type H cmd. If the move from c
is by the ﬁrst rule composes, then (c1, μ, Φ)−→(done, μ′, Φ′). By induction, this
move can be matched by (⌊c1⌋, ⌊μ⌋, ⌊Φ⌋) in zero or one steps. In fact it cannot
be matched in zero steps—because c1 does not have type H cmd, it is easy to see
that ⌊c1⌋̸= done. Hence we must have (⌊c1⌋, ⌊μ⌋, ⌊Φ⌋)−→(done, ⌊μ′⌋, ⌊Φ′⌋).
It follows that (c1; c2, μ, Φ)−→(c2, μ′, Φ′) is matched by (⌊c1⌋; ⌊c2⌋, ⌊μ⌋, ⌊Φ⌋)
−→(⌊c2⌋, ⌊μ′⌋, ⌊Φ′⌋). If instead the move from c is by the second rule composes,
then (c1, μ, Φ)−→(c′
1, μ′, Φ′), where c′
1 ̸= done. By induction, (⌊c1⌋, ⌊μ⌋, ⌊Φ⌋)
can match this move, going in zero or one steps to (⌊c′
1⌋, ⌊μ′⌋, ⌊Φ′⌋). Hence the
move (c1; c2, μ, Φ)−→(c′
1; c2, μ′, Φ′) can be matched by (⌊c1⌋; ⌊c2⌋, ⌊μ⌋, ⌊Φ⌋), go-
ing in zero or one steps to (⌊c′
1⌋; ⌊c2⌋, ⌊μ′⌋, ⌊Φ′⌋). A subtle point, however, is that
even though c1 does not have type H cmd, it is still possible that c′
1 : H cmd. In this
case we cannot match by moving to (⌊c′
1⌋; ⌊c2⌋, ⌊μ′⌋, ⌊Φ′⌋), since here ⌊c′
1; c2⌋=
⌊c2⌋̸= ⌊c′
1⌋; ⌊c2⌋= done; ⌊c2⌋. But here the match of the move from c1 must
actually be to (done, ⌊μ′⌋, ⌊Φ′⌋), and it must be in one step (rather than zero) since
⌊c1⌋̸= done. Hence in this case the move (c1; c2, μ, Φ)−→(c′
1; c2, μ′, Φ′) is instead
matched by the ﬁrst rule composes: (⌊c1⌋; ⌊c2⌋, ⌊μ⌋, ⌊Φ⌋)−→(⌊c2⌋, ⌊μ′⌋, ⌊Φ′⌋). 1
In the second subcase we have c1 : H cmd so ⌊c1; c2⌋= ⌊c2⌋. If the move from c is
by the ﬁrst rule composes, then we must have (c1, μ, Φ)−→(done, μ′, Φ′), where by
Conﬁnement μ′ ∼L ⌊μ⌋and Φ′ ∼L ⌊Φ⌋. So the move (c1; c2, μ, Φ)−→(c2, μ′, Φ′)
is matched in zero steps by (⌊c2⌋, ⌊μ⌋, ⌊Φ⌋). If instead the move from c is by
the second rule composes, then we must have (c1, μ, Φ)−→(c′
1, μ′, Φ′), where by
Conﬁnement μ′ ∼L ⌊μ⌋and Φ′ ∼L ⌊Φ⌋, and by Subject Reduction c′
1 : H cmd.
Hence the move (c1; c2, μ, Φ)−→(c′
1; c2, μ′, Φ′) is again matched in zero steps by
(⌊c2⌋, ⌊μ⌋, ⌊Φ⌋), since ⌊c′
1; c2⌋= ⌊c2⌋.
Finally, the third subcase is similar to the ﬁrst.
⊓⊔
Now we are ready to use these results in establishing our termination-insensitive non-
interference result:
Theorem 3. Let Δ1 be a well-typed distributed program and let Δ2 be formed by re-
placing each of the initial memories in Δ1 with a L-equivalent memory. Let Φ1 and
1 An example illustrating this situation is when c is (if 0 then l := 1 else h := 2); l :=
3. This goes in one step to h := 2; l := 3, which strips to l := 3. In this case, ⌊c⌋=
(if 0 then l := 1 else done); l := 3, which goes in one step to l := 3.

Secure Information Flow for Distributed Systems
135
Φ2 be L-equivalent channel memories. Suppose that (Δ1, Φ1) and (Δ2, Φ2) can both
execute successfully, reaching terminal conﬁgurations (Δ′
1, Φ′
1) and (Δ′
2, Φ′
2) respec-
tively. Then the corresponding local memories of Δ′
1 and Δ′
2 are L-equivalent, and
Φ′
1 ∼L Φ′
2.
Proof. By deﬁnition, (Δ1, Φ1) ⌊·⌋⌊(Δ1, Φ1)⌋and (Δ2, Φ2) ⌊·⌋⌊(Δ2, Φ2)⌋. Hence,
since ⌊·⌋is a fast low simulation, we know that ⌊(Δ1, Φ1)⌋and ⌊(Δ2, Φ2)⌋can also
execute successfully, and can reach terminal conﬁgurations whose local memories are
L-equivalent to the corresponding memories of Δ′
1 and Δ′
2. Moreover, by Theorem 1
we know that those terminal conﬁgurations are unique.
But ⌊(Δ1, Φ1)⌋is identical to ⌊(Δ2, Φ2)⌋, since neither contains H variables or
channels. Hence they must reach the same terminal conﬁguration. It follows that the
corresponding local memories of Δ′
1 and Δ′
2 are L-equivalent and that Φ′
1 ∼L Φ′
2.
⊓⊔
3
Towards a Concrete Implementation
In this section we explore the implementation of the abstract language in a concrete
setting over a public network, including the abilities of the external adversary (Eve),
the exploration of the appropriate network environment, and the characteristics of the
security tools used to protect the data while it is being transmitted.
The abstract language’s requirement of private channels limits its applicability to
secure settings but we would like to implement our language in a more practical set-
ting where communications between programs happen via a public network. We would
like a setting like a wireless LAN but then we are faced with signiﬁcant challenges to
ensure conﬁdentiality. Eavesdroppers can easily see all communications; what would
it take to implement our language for distributed systems in a wireless environment?
Clearly, secret data cannot be transmitted in a wireless LAN with any expectation of
conﬁdentiality as any computer with a receiver can get all the data that has been trans-
mitted. Hence our ﬁrst requirement, we need cryptography to hide the information being
transmitted. Asymmetric cryptography seems to be the appropriate style for our setting.
Having decided on cryptography to hide the information that is being transmitted,
we really want to encrypt only what is necessary to maintain the soundness of the dis-
tributed system, since encryption and decryption are expensive operations. In a wireless
LAN, communications happen via electromagnetic signals which contain not only the
message (payload) but also other information like source, destination, and data clas-
siﬁcation (header). Clearly we have to encrypt the payload but do we have to encrypt
the header? In fact we do, for otherwise the language conﬁdentiality would be lost as
exempliﬁed in Δ4 of Figure 6. The channel used in both processes is a high channel
(encrypted payload) yet an eavesdropper can still discern the value of the least bit of the
secret h1 by looking in the header of each packet for which process sends ﬁrst; if Pro-
cess 1 sends ﬁrst then h1 is odd and the least bit is 1. Therefore our second requirement:
we have to encrypt the header and the payload of packets on high channels to prevent
the leakage of secret information. Yet we are not done because surprisingly, this attack
works even if the message sent is public and it is being sent on a public channel. Con-
sider Δ4 again and let’s assume that we are encrypting all headers (secret and public)

136
R. Alp´ızar and G. Smith
-Δ4-
Process 1
if (h1 is even) then run a long time
send(aH,1,7, 1)
Process 2
if (h2 is odd) then run a long time
send(aH,2,8, 0)
-Δ5-
Process 1
if (h1 is even) then
C
?←Epk(someMsg);
send(aL,1,3, C)
send(aL,1,3, C)
Fig. 6. Attacks (second wave)
and the secret payloads, but we are allowing the public data to be transmitted in the
clear. This seems reasonable enough since the adversary will get all public data at the
end of the execution. Nevertheless, if we observe a public value of 1 being transmitted
ﬁrst we will know with high probability that the least bit of h1 is 1. Hence our third
requirement: we have to encrypt all transmitted data.
We remark that if we had an active adversary which was able to drop packets, modify
them and resend them, in addition to the attacks that we have seen she could modify
packets to leak information and to affect the integrity of the distributed system. For
example if the packet header was not encrypted she could change the packet classiﬁ-
cation from H to L thereby declassifying the payload which could cause the packet to
be received by a low channel buffer in the receiving process. Then she can wait until
the end of the execution to pick up the leaked secret from the process’s public memory.
This distinction may play a role in deciding what kind of security property will be nec-
essary in our encryption scheme. Speciﬁcally a passive adversary might only require
IND-CPA security while the active adversary will deﬁnitely require IND-CCA security.
As an illustration of this distinction consider the following variation of the Warinschi
attack on the Needham-Schroeder-(Lowe) protocol [9] where an IND-CPA scheme has
the ﬂaw where there is a function C′ := f(C) that takes an encrypted plaintext (like
a packet header) and returns the ciphertext of an identical plaintext but with a certain
location within it changed to L. This would not affect the security of the encryption
scheme in any other way, i.e., an adversary would not be able to know anything about
the content of the ciphertext but by simply substituting the header of any packet with C′
and re-sending it, the adversary would be able to declassify the payload of the message.
But continuing with our analysis, are we done? We have decided to encrypt all data
that is transmitted in the network, yet it is not enough to ensure conﬁdentiality. Consider
adversary Δ5 of Figure 6, it encrypts a message and sends it two times if a secret is even.
Meanwhile Eve scans every transmission waiting for two identical ciphertexts; if they
are found she knows with high probability that the least bit of h1 is 1, if all ciphers are
distinct, it is 0. Therefore our fourth requirement: all transmitted data must be composed
of freshly generated ciphertexts to ensure the conﬁdentiality of our distributed systems.
Finally, there are two more attacks that we need to consider. The ﬁrst one is the clas-
sical timing attack. If we know how long a typical execution step takes and can measure
time intervals then we can leak information. Δ6 of Figure 7 exempliﬁes this attack. If
Eve is able to measure the time interval between the two transmissions she will have

Secure Information Flow for Distributed Systems
137
-Δ6-
Process 1
send(aL,1,2, 0)
if (h1 is even) then delay 1 sec;
send(aL,1,2, 0)
-Δ7-
Process 1
if (h1 is even) then
send(aH,1,2, 0)
Fig. 7. Attacks (third wave)
the value of the least bit of h1. A related timing attack is to count the number of mes-
sage transmitted as in Δ7 of Figure 7. In this attack, the last bit of h1 is leaked by the
transmission of one or zero packets. These attacks can be generalized to changing the
statistics of packet transmission; for example, one can conceive an attack where the time
distribution for packet transmission is uniform to leak a 0 and χ-square or normal for a
1. However, this area of study seems inappropriate for us to handle. A solution might be
to impose a super-density of packet transmission at regular intervals, where only some
of the packets are real messages. This would eliminate the problem but signiﬁcantly
increase the network bandwidth utilization. Another solution to this problem may the
NRL-Pump [10,11] which is currently available at the local (government-owned) elec-
tronics shop. The pump obfuscates Eve’s ability to measure the time between messages.
It does this by inserting random delays based on an adaptive mechanism that adjusts the
delays based on network trafﬁc statistics. However, the pump cannot prevent timing
channels based on the order of distinguishable messages.
To summarize, we not only have to somehow hide the meaning of messages, but also
hide anything in a message that makes it distinguishable to Eve. Obviously, this includes
payload, but also the message header, since the source or destination of a message can
be used to distinguish it from another. Although L values are public, they cannot be seen
within the network trafﬁc. This is not a problem when computation happens within a
processor (like in some multithreaded environments) because it is not reasonable that
Eve would have access to the public memory in real time.
Soundness: Next we sketch a possible way to argue a computational noninterference
property for our concrete language.
Random Transfer Language Deﬁnition and Soundness: First, we should be able to
move our language, at the most basic level, from a nondeterministic to a probabilis-
tic setting by constructing a subset language which we will call Random Transfer
Language, and prove PNI on it. The PNI property on this language shall establish
that if we allow only fresh-random trafﬁc, the language is safe and sound.
Message Transfer Language Deﬁnition and Soundness: Then, we should be able to
construct a Message Transfer Language and prove CNI on it. This language simply
has the regular send command encrypt its payload before transmission on a public
channel but keeps private channels for transmission of header information.
Header Transfer Language Deﬁnition and Soundness: Next, we should be able to
construct Header Transfer Language and prove CNI on it. In this language the
send command encrypt its header before transmission, but keeps private channels
for transmission of the payload.

138
R. Alp´ızar and G. Smith
Hybrid Cryptographic Argument: Finally, we should be able to argue that since the
Message and Header Transfer languages both satisfy CNI, the combination also
should via a hybrid cryptographic argument.
4
Related Work
Peeter Laud [12,13] pioneers computationally secure information ﬂow analysis with
cryptography. Later, with Varmo Vene [14], they develop the ﬁrst language and type
system with cryptography and a computational security property. Recently, in [15], he
proves a computational noninterference property on a type system derived from the
work of Askarov et al [16].
In previous work on multithreaded languages [17,2,18,19,5], the type systems have
strongly curtailed the language’s expressive power in order to attain soundness. An
expansion of these languages, with a rich set of cryptographic primitives, a treatment
of integrity as well as conﬁdentiality, and a subject close to ours is the work of Fournet
and Rezk [20]. The primary differences between our papers are that their system does
not handle concurrency and is subject to timing channels; on the other hand, their active
adversary is more powerful having the ability to modify public data.
Another effort toward enhancing the usability of languages with security properties
is the extensive functional imperative language Aura [21]. The language maintains
conﬁdentiality and integrity properties of its constructs as speciﬁed by its label [22] by
“packing” it using asymmetric encryption before declassiﬁcation. The cryptographic
layer is hidden to the programmer making it easier to use. This system uses static and
runtime checking to enforce security. Using a different approach, Zheng and Myers
[23] use a purely static type system to achieve conﬁdentiality by splitting secrets under
the assumption of non-collusion of repositories (e.g. key and data repositories). Under
this model ciphertexts do not need to be public which allows relaxation of the type
system while maintaining security. Further towards the practical end of the spectrum
are efforts to provide assurance levels to software (as in EAL standard). In this line of
work (Shaffer, Auguston, Irvine, Levin [24]) a security domain model is established
and “real” programs are veriﬁed against it to detect ﬂow violations.
Another paper close to ours is by Focardi and Centenaro [5]. It treats a multipro-
grammed language and type system over asymmetric encryption and proves a noninter-
ference property on it. The main differences are that their type system is more restrictive,
requiring low guards on loops, and they use a formal methods approach rather than com-
putational complexity.
5
Conclusion and Future Work
In this paper we have crafted an abstract language for distributed systems while main-
taining a relaxed computational environment with private data, and we have argued that
it has the noninterference property. We have explored the feasibility of implementing
this language in a concrete setting where all communications happen via a public net-
work with cryptography to protect conﬁdentiality.

Secure Information Flow for Distributed Systems
139
The obvious course for future work is to formalize these explorations and to prove
computational noninterference on the concrete system. Another interesting area is to
identify the environments where encryption schemes with weaker security (like IND-
CPA) is sufﬁcient to ensure soundness.
As the complexity of these languages increases, our reduction proofs may become
unmanageable. One solution may be to use an automatic proving mechanism as in [25].
This work applies to security protocols in the computational model rather than lan-
guages. The tool works as a sequence of reductions towards a base that is easily proved
to be secure, hence the original protocol is secure.
Acknowledgments
This work was partially supported by the National Science Foundation under grant
CNS-0831114. We are grateful to Joshua Guttman, Pierpaolo Degano, and the FAST09
referees for helpful comments and suggestions.
References
1. Denning, D., Denning, P.: Certiﬁcation of programs for secure information ﬂow. Communi-
cations of the ACM 20(7), 504–513 (1977)
2. Smith, G., Volpano, D.: Secure information ﬂow in a multi-threaded imperative language.
In: Proceedings 25th Symposium on Principles of Programming Languages, San Diego, CA,
January 1998, pp. 355–364 (1998)
3. Sabelfeld, A., Sands, D.: Probabilistic noninterference for multi-threaded programs. In: Pro-
ceedings 13th IEEE Computer Security Foundations Workshop, Cambridge, UK, July 2000,
pp. 200–214 (2000)
4. Smith, G.: Improved typings for probabilistic noninterference in a multi-threaded language.
Journal of Computer Security 14(6), 591–623 (2006)
5. Focardi, R., Centenaro, M.: Information ﬂow security of multi-threaded distributed pro-
grams. In: PLAS 2008: Proceedings of the Third ACM SIGPLAN Workshop on Program-
ming Languages and Analysis for Security, pp. 113–124. ACM, New York (2008)
6. Smith, G., Alp´ızar, R.: Fast probabilistic simulation, nontermination, and secure information
ﬂow. In: Proc. 2007 ACM SIGPLAN Workshop on Programming Languages and Analysis
for Security, San Diego, California, June 2007, pp. 67–71 (2007)
7. Zdancewic, S., Myers, A.C.: Observational determinism for concurrent program security. In:
Proceedings 16th IEEE Computer Security Foundations Workshop, Paciﬁc Grove, Califor-
nia, June 2003, pp. 29–43 (2003)
8. Baier, C., Katoen, J.P., Hermanns, H., Wolf, V.: Comparative branching-time semantics for
Markov chains. Information and Computation 200(2), 149–214 (2005)
9. Warinschi, B.: A computational analysis of the Needham-Schroeder-(Lowe) protocol. In:
Proceedings 16th IEEE Computer Security Foundations Workshop, Paciﬁc Grove, Califor-
nia, June 2003, pp. 248–262 (2003)
10. Kang, M.H., Moskowitz, I.S.: A pump for rapid, reliable, secure communication. In: CCS
1993: Proceedings of the 1st ACM Conference on Computer and Communications Security,
pp. 119–129. ACM, New York (1993)
11. Kang, M.H., Moskowitz, I.S., Chincheck, S.: The pump: A decade of covert fun. In: 21st
Annual Computer Security Applications Conference (ACSAC 2005), pp. 352–360. IEEE
Computer Society, Los Alamitos (2005)

140
R. Alp´ızar and G. Smith
12. Laud, P.: Semantics and program analysis of computationally secure information ﬂow. In:
Sands, D. (ed.) ESOP 2001. LNCS, vol. 2028, pp. 77–91. Springer, Heidelberg (2001)
13. Laud, P.: Handling encryption in an analysis for secure information ﬂow. In: Degano, P. (ed.)
ESOP 2003. LNCS, vol. 2618, pp. 159–173. Springer, Heidelberg (2003)
14. Laud, P., Vene, V.: A type system for computationally secure information ﬂow. In:
Li´skiewicz, M., Reischuk, R. (eds.) FCT 2005. LNCS, vol. 3623, pp. 365–377. Springer,
Heidelberg (2005)
15. Laud, P.: On the computational soundness of cryptographically masked ﬂows. In: Proceed-
ings 35th Symposium on Principles of Programming Languages, San Francisco, California
(January 2008)
16. Askarov, A., Hedin, D., Sabelfeld, A.: Cryptographically-masked ﬂows. In: Proceedings of
the 13th International Static Analysis Symposium, Seoul, Korea, pp. 353–369 (2006)
17. Abadi, M., Fournet, C., Gonthier, G.: Secure implementation of channel abstractions. In:
LICS 1998: Proceedings of the 13th Annual IEEE Symposium on Logic in Computer Sci-
ence, Washington, DC, USA, p. 105. IEEE Computer Society, Los Alamitos (1998)
18. Smith, G.: Probabilistic noninterference through weak probabilistic bisimulation. In: Pro-
ceedings 16th IEEE Computer Security Foundations Workshop, Paciﬁc Grove, California,
June 2003, pp. 3–13 (2003)
19. Abadi, M., Corin, R., Fournet, C.: Computational secrecy by typing for the pi calculus. In:
Kobayashi, N. (ed.) APLAS 2006. LNCS, vol. 4279, pp. 253–269. Springer, Heidelberg
(2006)
20. Fournet, C., Rezk, T.: Cryptographically sound implementations for typed information-ﬂow
security. In: Proceedings 35th Symposium on Principles of Programming Languages, San
Francisco, California (January 2008)
21. Jia, L., Vaughan, J.A., Mazurak, K., Zhao, J., Zarko, L., Schorr, J., Zdancewic, S.: Aura: a
programming language for authorization and audit. In: Hook, J., Thiemann, P. (eds.) ICFP,
pp. 27–38. ACM, New York (2008)
22. Vaughan, J., Zdancewic, S.: A cryptographic decentralized label model. In: IEEE Symposium
on Security and Privacy, Oakland, California, pp. 192–206 (2007)
23. Zheng, L., Myers, A.C.: Securing nonintrusive web encryption through information ﬂow.
In: PLAS 2008: Proceedings of the third ACM SIGPLAN Workshop on Programming Lan-
guages and Analysis for Security, pp. 125–134. ACM, New York (2008)
24. Shaffer, A.B., Auguston, M., Irvine, C.E., Levin, T.E.: A security domain model to assess
software for exploitable covert channels. In: Erlingsson, ´U., Pistoia, M. (eds.) PLAS, pp.
45–56. ACM, New York (2008)
25. Blanchet, B.: A computationally sound mechanized prover for security protocols. In: SP
2006: Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P 2006), Wash-
ington, DC, USA, pp. 140–154. IEEE Computer Society Press, Los Alamitos (2006)

Probable Innocence in the Presence
of Independent Knowledge
Sardaouna Hamadou1, Catuscia Palamidessi2, Vladimiro Sassone1,
and Ehab ElSalamouny1
1 ECS, University of Southampton
2 INRIA and LIX, Ecole Polytechnique
Abstract. We analyse the Crowds anonymity protocol under the novel assump-
tion that the attacker has independent knowledge on behavioural patterns of in-
dividual users. Under such conditions we study, reformulate and extend Reiter
and Rubin’s notion of probable innocence, and provide a new formalisation for
it based on the concept of protocol vulnerability. Accordingly, we establish new
formal relationships between protocol parameters and attackers’ knowledge ex-
pressing necessary and suﬃcient conditions to ensure probable innocence.
1
Introduction
Anonymity protocols often use random mechanisms. It is therefore natural to think of
anonymity in probabilistic terms. Various notions of such probabilistic anonymity have
been proposed and a recent trend of work in formalizing these notions is directed at ex-
ploring the application of information-theoretic concepts (e.g. [18,4,5,6,1,15]). In our
opinion, however, except a recent paper by Franz, Meyer, and Pashalidis [11] which ad-
dresses the advantage that an adversary could take of hints from the context in which the
protocol operates, such approaches fail to account for the fact that in the real world, the
adversary often have some extra information about the correlation between anonymous
users and observables. Consider for example the following simple anonymous voting
process. In a parliament composed by Labourists and Conservatives, one member voted
against a proposal banning minimum wages. Without any additional knowledge it is
reasonable to assume that the person is more likely to be in the most liberal political
group. If however we know in addition that one Conservative voted against, then it is
more reasonable to suspect the liberally-inclined Conservatives. Similarly, suppose that
in a classroom of n students the teacher asks to tick one of two boxes on a piece of paper
to indicate whether or not they are satisﬁed by her teaching. If n is small and the teacher
noticed that the pupils use pens of diﬀerent colours, then she can use these colours to
partition the class so as to make the vote of some students more easily identiﬁable. Extra
knowledge of this kind, independent of the logic of the protocol used, can aﬀect dra-
matically its security. The extra knowledge can either arise from an independent source,
as in the ﬁrst example, or simply from the context in which the anonymity protocol is
run, as in the second example.
A relevant point in case is Reiter and Rubin’s Crowds protocol [16], which allows
Internet users to perform anonymous web transactions. The idea is to send the message
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 141–156, 2010.
c⃝Springer-Verlag Berlin Heidelberg 2010

142
S. Hamadou et al.
through a chain of users participating in the protocol. Each user in the ‘crowd’ must
establish a path between her and a set of servers by selecting randomly some users to
act as routers. The random selection process is performed in such a way that when a
user in the path relays a message, she does not know whether or not the sender is the
initiator of the message, or simply a forwarder like herself. Each user only has access
to messages routed through her, and some participants may be corrupted, i.e., they may
work together in order to uncover the identity of the initiator. It is well known that
Crowds cannot ensure strong anonymity [16,3] in presence of corrupted participants,
but when the number of corrupted users is suﬃciently small, it provides a weaker notion
of anonymity known as probable innocence. Informally, a sender is probably innocent
if to an attacker she is no more likely to be the originator than not to be.
Although Crowds has been widely analysed in the literature (e.g. [3,15]), the fact
that independent information may be available to the attacker has been so far ignored.
We maintain that this is ultimately incompatible with achieving a comprehensive and
reliable analysis of the protocol, as attackers’ extra knowledge is inherent to Crowds.
In particular, as any request routed through an attacker reveals the identity of the target
server, a team of attackers will soon build up a host of observations suitable to classify
the behaviour of honest participants.
This paper is to the best of our knowledge the ﬁrst to investigate the impact of the
attacker’s independent knowledge on the anonymity in the Crowds protocol.
Related work. Quantitative approach to the foundations of information-hiding has be-
came a very active and mature research ﬁeld. Several various formal deﬁnitions and
frameworks have been proposed for reasoning about secure information ﬂow analy-
sis (e.g. [19,7,8,9]), side-channel analysis (e.g [14]) and anonymity. Our work follows
a recent trend in the analysis of anonymity protocols directed to the application of
information-theoretic notions (e.g. [17,18,4,5,6,1,15,10,2]).
The most related work to ours is the one of Reiter and Ruben [16], the one of Halpen
and O’Neill [12], and the recent paper of Chatzikokolakis, and Palamidessi [3]. In [16]
the authors propose a formal deﬁnition of probable innocence that considers the prob-
ability of observable events induced by actions of an anonymous user participating in
the protocol. They require that the probability of an anonymous user producing any ob-
servable to be less than one half. In [12] the authors formalize probable innocence in
terms of the adversary’s conﬁdence that a particular anonymous event happened, after
performing an observation. Their deﬁnition requires that the probability of an anony-
mous events should be at most one half, under any observation. In [3] the authors argue
that the deﬁnition of [16] makes sense only for systems satisfying certain properties
while the deﬁnition of [12] depends on the probabilities of anonymous events which
are not part of the protocol. They propose a deﬁnition of probable innocence that tries
to combine the two previous ones by considering both the probability of producing
some observable and the adversary’s conﬁdence after the observation.
Another recent work closely related to ours is the one of Smith’s [19] which proposes
a new metric for quantitative information ﬂow based on the concept of vulnerability
as an alternative to previous metrics based on Shannon entropy and mutual informa-
tion. Informally, the idea is that the adversary knows the a priori distributions of the
hidden (anonymous) events and always ‘bets’ on the most likely culprit. The a priori

Probable Innocence in the Presence of Independent Knowledge
143
vulnerability then is the probability that the adversary guesses the true culprit based
only on the a priori distribution. The a posteriori vulnerability is the average proba-
bility that the adversary guesses the true culprit based on the a posteriori probability
distribution on the agents after the observation.
The main diﬀerence between these approaches and ours is that they do not take into
account the very likely additional knowledge of the adversary about the correlation be-
tween the anonymous events and some observables independent from the behaviour of
the protocol. In this paper we ﬁrst generalize the concepts of probable innocence and
vulnerability. Instead than just comparing the probability of being innocent with the
probability of being guilty, we consider the degree of the probability of being innocent.
Informally a protocol is α-probable innocent if for any anonymous user the probabil-
ity of being innocent is less than or equal to α. Similarly a protocol is α-vulnerable
if the a posteriori vulnerability of the anonymous users is less than or equal to α. We
prove that these two notions are related. In particular (α-)probable innocence implies
(α-)vulnerability and in the speciﬁc case when the a priori distribution of the anony-
mous events is uniform, they are equivalent. We furthermore extend these deﬁnitions in
order to cope with the extra independent knowledge of the adversary by computing the a
posteriori probability and the a posteriori vulnerability w.r.t to both the protocol observ-
ables and the independent observables. We show that the presence of extra knowledge
makes probable innocence (resp. vulnerability) more diﬃcult to be achieved.
Finally, it should be acknowledged that our observations about the importance of
additional knowledge of the adversary are not entirely new. Indeed, as already noticed
above, Franz, Meyer, and Pashalidis [11] considered the fact that an adversary could
take advantage of hints from the context in which a protocol operates. However, though
that their approach is closely related to ours in spirit, it is not general in the sense that it
assumes a deterministic correlation between the anonymous events and the observable
hints and a uniform distribution on the anonymous events. Moreover, their metric is
associated to Shannon entropy which is recently proven by Smith [19] of being less
accurate than vulnerability-based metric.
Structure of the paper. The paper is organised as follows: in §2 we ﬁx some basic
notations and recall the fundamental ideas of the Crowds protocol and its properties,
including the notion of probable innocence. In §3 we reformulate and extend probable
innocence using the idea of protocol vulnerability; §4 and §5 deliver our core technical
contribution by respectively extending probable innocence and vulnerability to the case
of attacker’s independent knowledge.
2
Preliminaries
This section describes our conceptual framework and brieﬂy revises the Crowds proto-
col and the notion of probable innocence. We use capital letters A, B to denote discrete
random variables and the corresponding small letters a, b and calligraphic letters A,
B for their values and set of values respectively. We denote by p(a), p(b) the proba-
bilities of a and b respectively and by p(a ∧b) their joint probability. The conditional
probability of a given b is deﬁned as

144
S. Hamadou et al.
p(a | b) = p(a ∧b)
p(b)
The Bayes theorem relates the conditional probabilities p(a | b) and p(b | a) as follows
p(a | b) = p(b | a) p(a)
p(b)
.
2.1
The Framework
In this paper we consider a framework similar to the probabilistic approaches to anony-
mity and information ﬂow used in (for instance) [13], [5], [15], and [19]. We restrict
ourselves to total protocols and programs with one high level (or anonymous) input
A, a random variable over a ﬁnite set A, and one low level output (observable) O, a
random variable over a ﬁnite set O. We represent a protocol/programby the matrix of the
conditional probabilities p(o j | ai), where p(o j | ai) is the probability that the low output
is o j given that the high input is ai. We assume that the high input is generated according
to an a priori publicly-known probabilistic distribution. An adversary or eavesdropper
can see the output of a protocol, but not the input, and he is interested in deriving the
value of the input from the observed output in one single try.
In this paper we will also assume that the attacker has access to the value of a random
variable S distributed over S that summarizes his additional knowledge (information)
about A independent from the behavior of the protocol, as explained in the introduction.
The matrix of the conditional probabilities p(sk | ai) expresses the correlation between
the anonymous events and the additional knowledge of the adversary.
When | S | = 1 the adversary’s additional information about A is a trivial one and
cannot help his eﬀort in determining the value of A. For example knowing the length
of a password in a ﬁxed-length password system is a trivial information since all pass-
words have the same length. Trivial information allows us to model the absence of
additional information. The standard framework can therefore be seen as an instance of
our framework.
2.2
The Crowds Protocol and the Deﬁnition of Probable Innocence
The protocol. Crowds is a protocol proposed by Reiter and Rubin in [16] to allow Inter-
net users performing anonymous web transactions by protecting their identity as origi-
nators of messages. The central idea to ensure anonymity is that the originator forwards
the message to another, randomly-selected user, which in turn forwards the message to
another user, and so on until the message reaches its destination (the end server). This
routing process ensures that, even when a user is detected sending a message, there is a
substantial probability that she is simply forwarding it on behalf of somebody else.
More speciﬁcally, a crowd is a ﬁxed number of users participating in the protocol.
Some members (users) in the crowd may be corrupted (the attackers), and they can col-
laborate in order to discover the originator’s identity. The purpose of the protocol is to
protect the identity of the message originator from the attackers. When an originator –
also known as initiator– wants to communicate with a server, she creates a random path
between herself and the server through the crowd by the following process.

Probable Innocence in the Presence of Independent Knowledge
145
– Initial step: the initiator selects uniformly at random a member of the crowd (pos-
sibly herself) and forwards the request to her. We refer to the latter user as the
forwarder.
– Forwarding steps: a forwarder, upon receiving a request, ﬂips a biased coin. With
probability 1 −p f she delivers the request to the end server. With probability p f
she selects uniformly at random a new forwarder (possibly herself) and forwards
the request to her. The new forwarder repeats the same forwarding process.
The response from the server to the originator follows the same path in the opposite
direction. Each user (including corrupted users) is assumed to have only access to mes-
sages routed through her, so that she only knows the identities of her immediate prede-
cessor and successor in a path, and the end server.
Informal deﬁnition of Probable Innocence. In [16] Reiter and Rubin have proposed a hi-
erarchy of anonymity notions in the context of Crowds. These range from ‘absolute pri-
vacy,’ where the attacker cannot perceive the presence of communication, to ‘provably
exposed,’ where the attacker can prove the sender and receiver relationship. Clearly,
as most protocols used in practice, Crowds cannot ensure absolute privacy in presence
of attackers or corrupted users, but can only provide weaker notions of anonymity. In
particular, in [16] the authors propose an anonymity notion called probable innocence
and prove that, under some conditions on the protocol parameters, Crowds ensures the
probable innocence property to the originator. Informally, they deﬁne it as follows:
A sender is probably innocent if, from the attacker’s point of view, the sender
appears no more likely to be the originator than to not be the originator.
(1)
In other words, the attacker may have reason to suspect the sender of being more likely
than any other potential sender to be the originator, but it still appears at least as likely
that she is not.
The formal property proved by Reiter and Rubin. Let m be the number of users par-
ticipating in the protocol and let c and n be the number of the corrupted and honest
users, respectively, with m = n + c. Since anonymity makes only sense for honest
users, we deﬁne the set of anonymous events as A = {a1, a2, . . . , an}, where ai indi-
cates that user i is the initiator of the message. We deﬁne the set of observable events as
O = {o1, o2, . . ., on}, where oi indicates that user i forwarded a message to a corrupted
user. We also say that user i is detected by the attacker.
As it is usually the case in the analysis of Crowds, we assume that attackers will
always deliver a request to forward immediately to the end server, since forwarding it
any further cannot help them learn anything more about the identity of the originator.
In [16] Reiter and Rubin formalise their notion of probable innocence via the condi-
tional probability p(I | H) that the initiator is detected given that any user is detected at
all. Here I denotes the event that it is precisely the initiator to forward the message to
the attacker on the path, and H that there is an attacker in the path. Precisely, probable
innocence holds if p(I | H) ≤1
2.
In our setting the probability that user j is detected given that user i is the initiator,
can be written simply as p(o j | ai). As we are only interested in the case in which a

146
S. Hamadou et al.
user is detected, for simplicity we do not write such condition explicitly. Therefore, the
property proved in [16] (i.e. p(I | H) ≤1
2) translates in our setting as:
∀i. p(oi | ai) ≤1/2
(2)
Reiter and Rubin proved in [16] that, in Crowds, the following holds:
p(o j | ai) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
1 −n−1
m p f
i = j
1
m p f
i  j
Therefore, probable innocence (2) holds if and only if
m ≥c + 1
p f −1
2
p f
3
Probable Innocence Revisited and Extended
In our opinion there is a mismatch between the idea of probable innocence expressed
informally in (1) and the property actually proved by Reiter and Rubin, cf. (2). The
former, indeed, seems to correspond to the following:
∀i. p(ai | oi) ≤1/2
(3)
It is worth noting that this is also the interpretation given by Halpern and O’Neill [13].
The properties (2) and (3) however coincide under the assumption that the a priori
distribution is uniform, i.e. that each honest user has equal probability of being the
initiator. This is a standard asumption in Crowds.
Proposition 1. If the a priori distribution is uniform, then ∀i, j. p(ai | o j) = p(o j | ai).
Proof. If the a priori distribution is uniform, then for every i we have p(ai) = 1/n where
n is the number of honest users. The probability of user j being detected is also uniform,
and hence equal to 1/n. In fact, every initiator forwards the message to each other user
with the same probability, and each forwarder does the same, hence each user has the
same probability of being detected when she is the initiator, and the same probability
of being detected when she is not the initiator. Therefore we have: p(o j | a j) = p(ok | ak)
and p(o j | ai) = p(ok | ai) for every j, k and i  j, k, and hence:
p(o j) = p(o j ∧a j) + 
i j p(o j ∧ai)
= p(o j | a j)p(a j) + 
i j p(o j | ai)p(ai)
= p(ok | ak)p(ak) + 
ik p(ok | ai)p(ai)
by symmetry
= p(ok)
Finally, by using the Bayes theorem, we have:
p(ai | o j) =
p(o j | ai) p(ai)
p(o j)
=
p(o j | ai) · 1/n
1/n
= p(o j | ai)
⊓⊔

Probable Innocence in the Presence of Independent Knowledge
147
Corollary 1. If the a priori distribution is uniform, then (2) and (3) are equivalent.
The following proposition points out that in presence of uniform a priori distribu-
tion, the matrix associated to the protocol, i.e. the array of the conditional probabilities
p(o j|ai), has equal elements everywhere except on the diagonal:
Proposition 2. If the a priori distribution is uniform, then there exists a p such that
p(o j | ai) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
p
i = j
1−p
n−1 i  j
Proof. As already noted in the proof of Proposition 1, for symmetry reasons we have
p(o j | a j) = p(ok | ak) and p(o j | ai) = p(ok | ai) for every j, k and i  j, k.
⊓⊔
It is generally the case, in Crowds, that p is (much) greater than (1 −p)/(n −1), which
means that the user which is detected is also the most likely culprit. This allows us to re-
formulate the property of probable innocence in terms of the (a posteriori) vulnerability
[19] of a protocol, which coincides with the converse of the Bayes risk [6].
Let us brieﬂy recall the deﬁnition of vulnerability. The idea is that the adversary
knows the a priori distributions and always ‘bets’ on the most likely culprit. The a pri-
ori vulnerability then is the probability that the adversary guesses the true culprit based
only on the a priori distribution p(a). The a posteriori vulnerability is the average prob-
ability that the adversary guesses the true culprit based on the a posteriori probability
distribution on the agents after the observation, i.e., p(a | o). Formally:
Deﬁnition 1 ([19])
– The a priori vulnerability is V(A) = maxi p(ai)
– The a posteriori vulnerability is V(A | O) = 
j p(o j) maxi(p(ai |, o j))
Using the Bayes theorem, we can reformulate V(A | O) as follows:
V(A | O) =

j
max
i (p(o j|ai) p(ai))
(4)
It is easy to see that probable innocence implies that the a posteriori vulnerability is
smaller than 1/2. The converse also holds, if the a priori distribution is uniform.
Proposition 3
– If either (2) or (3) holds, then V(A | O) ≤1/2.
– If V(A | O) ≤1/2 and the a priori distribution is uniform, then (2) and (3) hold.
We now generalize the concept of probable innocence. Instead than just comparing
the probability of being innocent with the probability of being guilty, we consider the
degree of the probability of being innocent. Similarly for the vulnerability.
Deﬁnition 2. Given a real number α ∈[0, 1], we say that a protocol satisﬁes

148
S. Hamadou et al.
– α-probable innocence if and only if ∀i. p(ai | oi) ≤α
– α-vulnerability if and only if V(A | O) ≤α.
Clearly α-probable innocence coincides with the standard probable innocence for α =
1/2. It is also to be remarked that the minimum possible value of α is 1/n, i.e., it is not
possible for a protocol to satisfy α-probable innocence or α-vulnerability if α is smaller
than this value.
4
Probable Innocence in Presence of Extra Information
We now consider the notion of probable innocence when we assume that the adversary
has some extra information about the correlation between the culprit and the observable.
We express this extra information in terms of a random variable S , whose values
s1 . . . sℓwe assume to be observable, and the conditional probabilities p(sk | ai). We as-
sume that, the original observables O and the additional observables S are independent,
for every originator.
Example 1. Consider an instance of the Crowds protocol in which there are two servers,
and assume that the users are divided in two parts, A1 and A2. Assume that each user
in A1, when he is the initiator, has probability p1 to address his message to the ﬁrst
server (as the ﬁnal destination of the message). Conversely, assume that each user in
A2 has probability p2 to address the second server. The address of the server appears
in the message, and it is therefore observed by the adversary when he intercepts the
message. It is clear that (because of the way Crowds works) the event that the message
is intercepted is independent from the server to which the message is addressed.
If we indicate by s1 the fact that the message is addressed to the ﬁrst server, and by s2
the fact that the message is addressed to the second server, the matrix of the conditional
probabilities corresponding to this example is as follows:
p(s | a) =
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
p1
a ∈A1, s = s1
1 −p1
a ∈A1, s = s2
1 −p2
a ∈A2, s = s1
p2
a ∈A2, s = s2
We are interested in exploring how the extra information provided by S and the condi-
tional probabilities of S given A aﬀects the notion of probable innocence.
We take the point of view that the invariant property should be the one expressed
by (3), generalized by Deﬁnition 2. We reformulate this deﬁnition to accommodate the
presence of extra information in the observables.
Deﬁnition 3 (α-probable innocence in presence of extra information). Given a real
number α ∈[0, 1], we say that a protocol satisﬁes α-probable innocence if and only if
∀i, k. p(ai | oi ∧sk) ≤α

Probable Innocence in the Presence of Independent Knowledge
149
The following lemma expresses the relation between the conditional probabilities with
respect to the new observables and the original ones
Lemma 1. ∀i, j, k. p(ai | o j ∧sk) = p(ai | o j) p(sk | ai)
p(sk | oj)
Proof. By Bayes theorem we have, for every i, j, k
p(ai | o j ∧sk) = p(o j ∧sk | ai) p(ai)
p(o j ∧sk)
Since we are assuming that, given any originator ai, O and S are independent, we have
p(o j ∧sk | ai) = p(o j | ai) p(sk | ai), and therefore
p(ai | o j ∧sk) = p(o j | ai) p(sk | ai) p(ai)
p(o j ∧sk)
We can rewrite p(o j ∧sk) as p(sk | o j) p(o j). Hence:
p(ai | o j ∧sk) = p(o j | ai) p(sk | ai) p(ai)
p(sk|o j) p(o j)
Finally, using Bayes theorem again, we conclude.
⊓⊔
We can now prove the presence of extra information reduces the degree α of probable
innocence by a factor q = mini,k
p(sk | oi)/p(sk | ai)	:
Proposition 4
– In presence of extra information, a protocol satisﬁes α-probable innocence if
∀i. p(ai | oi) ≤q α
– If ∀i, j. p(ai | oi) = p(a j | o j), then the above condition is also necessary, i.e. the
protocol satisﬁes α-probable innocence only if
∀i. p(ai | oi) ≤q α
Proof. Immediate from previous lemma, with j = i.
⊓⊔
In general the factor q in the above proposition is strictly greater than 0 and strictly
smaller than 1. Note also that, in the case of Crowds, the protocol satisﬁes the required
symmetry, i.e. the elements in the principal diagonal of the matrix of the conditional
probabilities are all the same (cf. Prop. 2) and therefore the above factor q is strict.
Example 2. Consider an instance of the Crowds protocol where there are 6 members
(m = 6). One of these members is an attacker (c = 1), and the others are honest (n = 5).
Assume that p f = 3/4 then we have
p(oi | ai) = 1 −n −1
m
p f = 1 −4
6 · 3
4 = 1
2

150
S. Hamadou et al.
and, for i  j,
p(o j | ai) = 1
m p f = 1
6 · 3
4 = 1
8
Now suppose that, as in Example 1, there are two servers and the honest members are
divided into two groups A1 and A2, where A1 = {1, 2} (resp. A2 = {3, 4, 5}) are the users
which prefer the server 1 (resp. the server 2). Assume that the preference probabilities
p1 = p2 = 3/4, i.e. that the conditional probabilities p(s | a) are given by
p(sk | ai) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
3
4 ai ∈Ak
1
4 ai  Ak
Because of the independence assumption, the conditional probabilities p(o ∧s | a) can
be computed as the product p(o | a) p(s | a) (see Fig. 1). From these we can compute the
joint probabilities p(o ∧s) by using the formula
p(o j ∧sk) =

i
p(o j ∧sk | ai) p(ai)
Assuming that the a priori distribution is uniform (p(ai) = 1
5), we obtain the probabili-
ties shown in Fig. 1. From these we can then calculate p(s | o) using the deﬁnition
p(sk | o j) = p(sk ∧o j)
p(o j)
and the fact that if A is uniformly distributed then also O is uniformly distributed
(p(o j) = 1
5). Finally, using Bayes theorem, we can calculate the probabilities p(a | o ∧s)
from p(o ∧s | a), p(o ∧s), and p(a).
Using the values of p(sk | oi) and p(sk | ai), the factor q = mini,k
p(sk | oi)/p(sk | ai)	
in Proposition 4 is evaluated to 3/4. It is easy to see that Proposition 4 holds for this
instance of Crowds, i.e. ∀i, k. p(ai | oi ∧sk) ≤α if and only if ∀i. p(ai | oi) ≤q α. In fact
p(ai | oi) = 1/2 and maxi,k p(ai | oi ∧sk) = 2/3.
We note that in some cases the extra information may contradict the original observable.
For instance it could be the case that user 1, when she is the originator, has a strong
preference for the server 1. So if the attacker receives a message from user 1 addressed
to the server 2, it may be better for him to assume that the originator is one (arbitrary)
user from the group that favors the server 2, rather than user 1.
We argue, therefore, that the presence of extra information makes the property of
probable innocence more diﬃcult to satisfy, because the attacker can use the extra in-
formation to improve his guess about the culprit, and he may guess a user which is
not necessarily the one who sent the message to him. Therefore it seems reasonable to
consider the following deﬁnition:
Deﬁnition 4 (α-probable innocence in presence of extra information, safe version).
Given a real number α ∈[0, 1], a protocol satisﬁes α-probable innocence if and only if
∀i, j, k. p(ai | o j ∧sk) ≤α

Probable Innocence in the Presence of Independent Knowledge
151
p(o | a) o1
o2
o3
o4
o5
a1
1
2
1
8
1
8
1
8
1
8
a2
1
8
1
2
1
8
1
8
1
8
a3
1
8
1
8
1
2
1
8
1
8
a4
1
8
1
8
1
8
1
2
1
8
a5
1
8
1
8
1
8
1
8
1
2
p(s | a)
s1
s2
a1
3
4
1
4
a2
3
4
1
4
a3
1
4
3
4
a4
1
4
3
4
a5
1
4
3
4
p(s | o)
s1
s2
o1
9
16
7
16
o2
9
16
7
16
o3
3
8
5
8
o4
3
8
5
8
o5
3
8
5
8
p(o, s | a) o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
a1
3
8
3
32
3
32
3
32
3
32
1
8
1
32
1
32
1
32
1
32
a2
3
32
3
8
3
32
3
32
3
32
1
32
1
8
1
32
1
32
1
32
a3
1
32
1
32
1
8
1
32
1
32
3
32
3
32
3
8
3
32
3
32
a4
1
32
1
32
1
32
1
8
1
32
3
32
3
32
3
32
3
8
3
32
a5
1
32
1
32
1
32
1
32
1
8
3
32
3
32
3
32
3
32
3
8
p(o, s)
o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
9
80
9
80
6
80
6
80
6
80
7
80
7
80
10
80
10
80
10
80
p(a | o, s) o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
a1
2
3
1
6
1
4
1
4
1
4
2
7
1
14
1
20
1
20
1
20
a2
1
6
2
3
1
4
1
4
1
4
1
14
2
7
1
20
1
20
1
20
a3
1
18
1
18
1
3
1
12
1
12
3
14
3
14
3
5
3
20
3
20
a4
1
18
1
18
1
12
1
3
1
12
3
14
3
14
3
20
3
5
3
20
a5
1
18
1
18
1
12
1
12
1
3
3
14
3
14
3
20
3
20
3
5
Fig. 1. The matrices of the conditional probabilities of Example 2. We use here the notation o, s
to represent o ∧s.
However, it turns out that the relation with the original notion of probable innocence
remains the same, and Proposition 4 still provides the appropriate bound:
Proposition 5
– In presence of extra information, a protocol satisﬁes the safe version of α-probable
innocence if
∀i, j. p(ai | o j) ≤q α

152
S. Hamadou et al.
– If ∀i, j. p(ai | oi) = p(a j | o j), then the above condition is also necessary, i.e. the
protocol satisﬁes the safe version of α-probable innocence only if
∀i. p(ai | oi) ≤q α
where q = mini, j,k
p(sk|o j)/p(sk|ai)	.
Example 3. Consider again the instance of Crowds like in Example 2, but assume now
that the preference probabilities are much higher than before, namely
p(sk | ai) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
9
10 ai ∈Ak
1
10 ai  Ak
We can compute the probabilities p(o∧s | a), p(o∧s), p(s | o) and p(a | o∧s) like before.
The results are shown in Fig. 2.
We note that in certain cases the extra knowledge dominates over the original ob-
servables. For instance, if the adversary receives a message from user 3 addressed to
server 1, it is better for him to bet that a sender of group 1 is the originator, rather than
user 3. In fact the a posteriori probability of the latter is p(a3 | o3 ∧s1) = 1/6 while the
a posteriori probability of (say) user 1 is p(a1 | o3 ∧s1) = 3/8.
Using the values of p(sk | oi) and p(sk | ai), the factor q = mini,k
p(sk | oi)/p(sk | ai)	
in Proposition 4 is evaluated to 2/3, and we can see that Proposition 5 holds for this
instance of Crowds, i.e. ∀i, k. p(ai | oi ∧sk) ≤α if and only if ∀i. p(ai | oi) ≤q α. In fact
p(ai | oi) = 1/2 and maxi, j,k p(ai | o j ∧sk) = 3/4.
5
Vulnerability in Presence of Extra Information
In this section we explore how the deﬁnition of α-vulnerability is aﬀected by the pres-
ence of extra information. Let us start with the deﬁnition of α-vulnerability in presence
of the new observables. It is natural to extend the notion of α-vulnerability by consid-
ering the (a posteriori) vulnerability when the observables are constituted by the joint
random variables O, S , which is given by
V(A | O, S ) =

j,k
p(o j ∧sk) max
i
p(ai | o j ∧sk)
Hence we extend α-vulnerability as follows:
Deﬁnition 5 (α-vulnerability in presence of extra information). Given a real num-
ber α ∈[0, 1], a protocol satisﬁes α-vulnerability if and only if V(A | O, S ) ≤α.
For the next proposition, we consider the speciﬁc case in which the protocol satisﬁes
the symmetry of Crowds.
Proposition 6. Let ℓ= |S| denote the cardinality of the extra observables. Assume that,
for each i, p(oi | ai) = p = maxi, j p(o j | ai) and let q = maxi,k p(sk | ai). We have:

Probable Innocence in the Presence of Independent Knowledge
153
p(o | a) o1
o2
o3
o4
o5
a1
1
2
1
8
1
8
1
8
1
8
a2
1
8
1
2
1
8
1
8
1
8
a3
1
8
1
8
1
2
1
8
1
8
a4
1
8
1
8
1
8
1
2
1
8
a5
1
8
1
8
1
8
1
8
1
2
p(s | a)
s1
s2
a1
9
10
1
10
a2
9
10
1
10
a3
1
10
9
10
a4
1
10
9
10
a5
1
10
9
10
p(s | o)
s1
s2
o1
6
10
4
10
o2
6
10
4
10
o3
3
10
7
10
o4
3
10
7
10
o5
3
10
7
10
p(o, s | a) o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
a1
9
20
9
80
9
80
9
80
9
80
1
20
1
80
1
80
1
80
1
80
a2
9
80
9
20
9
80
9
80
9
80
1
80
1
20
1
80
1
80
1
80
a3
1
80
1
80
1
20
1
80
1
80
9
80
9
80
9
20
9
80
9
80
a4
1
80
1
80
1
80
1
20
1
80
9
80
9
80
9
80
9
20
9
80
a5
1
80
1
80
1
80
1
80
1
20
9
80
9
80
9
80
9
80
9
20
p(o, s)
o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
6
50
6
50
3
50
3
50
3
50
4
50
4
50
7
50
7
50
7
50
p(a | o, s) o1, s1 o2, s1 o3, s1 o4, s1 o5, s1 o1, s2 o2, s2 o3, s2 o4, s2 o5, s2
a1
3
4
3
16
3
8
3
8
3
8
1
8
1
32
1
56
1
56
1
56
a2
3
16
3
4
3
8
3
8
3
8
1
32
1
8
1
56
1
56
1
56
a3
1
48
1
48
1
6
1
24
1
24
9
32
9
32
9
14
9
56
9
56
a4
1
48
1
48
1
24
1
6
1
24
9
32
9
32
9
56
9
14
9
56
a5
1
48
1
48
1
24
1
24
1
6
9
32
9
32
9
56
9
56
9
14
Fig. 2. The matrices of the conditional probabilities of Example 3. We use here the notation o, s
to represent o ∧s.
1. V(A | O, S ) ≤α if V(A | O) ≤
α
ℓq.
2. If the a priori distribution is uniform and (1−p)
n−1 q ≤p (1−q)
ℓ−1 , then V(A | O, S ) ≤α if
and only if V(A | O) ≤α.
Proof. By deﬁnition we have:
V(A | O, S ) =

j,k
p(o j ∧sk) max
i
p(ai | o j ∧sk)

154
S. Hamadou et al.
Using Bayes theorem we derive:
V(A | O, S ) =

j,k
max
i (p(o j ∧sk | ai) p(ai))
Because of the independence of O and S for any given originator, we deduce:
V(A | O, S ) =

j,k
max
i (p(o j | ai) p(sk | ai) p(ai))
(5)
1. Since q = maxi,k p(sk | ai), from (5) we derive:
V(A | O, S ) ≤

j,k
max
i (p(oj | ai) q p(ai)) = ℓq

j
max
i (p(oj | ai) p(ai)) = ℓq V(A | O)
2. Since the input distribution is uniform:
V(A | O, S ) = 1
n

j,k
max
i (p(o j | ai) p(sk | ai))
If (1−p)
n−1 q ≤p (1−q)
ℓ−1 then maxi(p(o j | ai) p(sk | ai)) = p(o j | a j) p(sk | a j) = p p(sk | a j).
Hence
V(A | O, S ) = 1
n

j,k
p p(sk | a j) = 1
n

j
p

k
p(sk | a j) = 1
n

j
p = V(A|O)
⊓⊔
It is interesting to note that, in the part (2) of Proposition 6, the extra knowledge does not
make the protocol more vulnerable. This is because the additional knowledge is some-
times in accordance with the best guess based on the original observable, and sometimes
in conﬂict, but the original observable always dominates, and therefore the additional
knowledge is either redundant or disregarded. In any case, it is not used to make the
guess. In the general case (represented by the ﬁrst part of the proposition), however,
the additional knowledge may dominate the original observable, and induce the adver-
sary to change his bet, thus increasing his chances. For this reason, the vulnerability
increases in general of a factor ℓq.
6
Conclusion
In this paper we focussed on the Crowds anonymity protocol and asked the question of
how its existing analyses are aﬀected by taking into account that attackers may have in-
dependent knowledge about users’ behaviours. This amounts to providing the attackers
with information about the correlation between a set of observables s1, . . . , sℓand the
event that user i is the originator of a message, as formalised by the conditional prob-
ability p(sk | ai). We formalised the idea of probable innocence for such systems, both
in standard terms and via the notion of protocol vulnerability, and identiﬁed a simple
and neat measure of the impact of independent knowledge. Namely, it makes probable

Probable Innocence in the Presence of Independent Knowledge
155
innocence (resp. vulnerability) more diﬃcult to achieve by a factor q (resp. ℓq) which
depends on the ratio between the probability of the observables conditional to the orig-
inator and conditional to the user detected (and, in the case of vulnerability, also from
the cardinality of the random variable that represents the extra knowledge).
In conclusion, we remark that although the scenario in which attackers possess or can
acquire extra knowledge is highly likely, it has so far been ignored. In the near future,
we plan to work on the even more interesting scenario in which the attackers use their
‘beliefs’ about users behaviour to raise the vulnerability of anonymity protocols such
as Crowds.
References
1. Bhargava, M., Palamidessi, C.: Probabilistic anonymity. In: Abadi, M., de Alfaro, L. (eds.)
CONCUR 2005. LNCS, vol. 3653, pp. 171–185. Springer, Heidelberg (2005)
2. Braun, C., Chatzikokolakis, K., Palamidessi, C.: Compositional methods for information-
hiding. In: Amadio, R.M. (ed.) FOSSACS 2008. LNCS, vol. 4962, pp. 443–457. Springer,
Heidelberg (2008)
3. Chatzikokolakis, K., Palamidessi, C.: Probable innocence revisited. Theor. Comput.
Sci. 367(1-2), 123–138 (2006)
4. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: Probability of error in information-
hiding protocols. In: CSF, pp. 341–354. IEEE Computer Society, Los Alamitos (2007)
5. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: Anonymity protocols as noisy chan-
nels. Inf. Comput. 206(2-4), 378–401 (2008)
6. Chatzikokolakis, K., Palamidessi, C., Panangaden, P.: On the Bayes risk in information-
hiding protocols. Journal of Computer Security 16(5), 531–571 (2008)
7. Chen, H., Malacaria, P.: Quantitative analysis of leakage for multi-threaded programs. In:
PLAS 2007: Proceedings of the 2007 workshop on Programming languages and analysis for
security, pp. 31–40. ACM, New York (2007)
8. Clark, D., Hunt, S., Malacaria, P.: A static analysis for quantifying information ﬂow in a
simple imperative language. Journal of Computer Security 15(3), 321–371 (2007)
9. Clarkson, M.R., Myers, A.C., Schneider, F.B.: Belief in information ﬂow. In: CSFW, pp.
31–45. IEEE Computer Society, Los Alamitos (2005)
10. Deng, Y., Pang, J., Wu, P.: Measuring anonymity with relative entropy. In: Dimitrakos, T.,
Martinelli, F., Ryan, P.Y.A., Schneider, S. (eds.) FAST 2006. LNCS, vol. 4691, pp. 65–79.
Springer, Heidelberg (2007)
11. Franz, M., Meyer, B., Pashalidis, A.: Attacking unlinkability: The importance of context. In:
Borisov, N., Golle, P. (eds.) PET 2007. LNCS, vol. 4776, pp. 1–16. Springer, Heidelberg
(2007)
12. Halpern, J.Y., O’Neill, K.R.: Anonymity and information hiding in multiagent systems. Jour-
nal of Computer Security 13(3), 483–512 (2005)
13. Halpern, J.Y., O’Neill, K.R.: Anonymity and information hiding in multiagent systems. Jour-
nal of Computer Security 13(3), 483–512 (2005)
14. K¨opf, B., Basin, D.A.: An information-theoretic model for adaptive side-channel attacks.
In: Ning, P., di Vimercati, S.D.C., Syverson, P.F. (eds.) ACM Conference on Computer and
Communications Security, pp. 286–296. ACM, New York (2007)
15. Malacaria, P., Chen, H.: Lagrange multipliers and maximum information leakage in diﬀerent
observational models. In: Erlingsson, ´U., Pistoia, M. (eds.) PLAS, pp. 135–146. ACM, New
York (2008)

156
S. Hamadou et al.
16. Reiter, M.K., Rubin, A.D.: Crowds: Anonymity for web transactions. ACM Transactions on
Information and Systems Security 1(1), 66–92 (1998)
17. Serjantov, A., Danezis, G.: Towards an information theoretic metric for anonymity. In: Din-
gledine, R., Syverson, P.F. (eds.) PET 2002. LNCS, vol. 2482, pp. 41–53. Springer, Heidel-
berg (2003)
18. Shmatikov, V., Wang, M.-H.: Measuring relationship anonymity in mix networks. In: Juels,
A., Winslett, M. (eds.) WPES, pp. 59–62. ACM, New York (2006)
19. Smith, G.: On the foundations of quantitative information ﬂow. In: de Alfaro, L. (ed.) FOS-
SACS 2009. LNCS, vol. 5504, pp. 288–302. Springer, Heidelberg (2009)

A Calculus of Trustworthy Ad Hoc Networks⋆
Massimo Merro and Eleonora Sibilio
Dipartimento di Informatica, Universit`a degli Studi di Verona, Italy
Abstract. We propose a process calculus for mobile ad hoc networks
which embodies a behaviour-based multilevel decentralised trust model.
Our trust model supports both direct trust, by monitoring nodes be-
haviour, and indirect trust, by collecting recommendations and spread-
ing reputations. The operational semantics of the calculus is given in
terms of a labelled transition system, where actions are executed at a
certain security level. We deﬁne a labelled bisimilarity parameterised on
security levels. Our bisimilarity is a congruence and an eﬃcient proof
method for an appropriate variant of barbed congruence, a standard
contextually-deﬁned program equivalence. Communications are proved
safe with respect to the security levels of the involved parties. In partic-
ular, we ensure safety despite compromise: compromised nodes cannot
aﬀect the rest of the network. A non interference result expressed in
terms of information ﬂow is also proved.
1
Introduction
Wireless technology spans from user applications such as personal area networks,
ambient intelligence, and wireless local area networks, to real-time applications,
such as cellular and ad hoc networks. A Mobile ad hoc network (MANET) is
a self-conﬁguring network of mobile devices (also called nodes) communicating
with each other via radio transceivers without relying on any base station. Lack
of a ﬁxed networking infrastructure, high mobility of the devices, shared wire-
less medium, cooperative behaviour, and physical vulnerability are some of the
features that make challenging the design of a security scheme for mobile ad hoc
networks.
Access control is a well-established technique for limiting access to the re-
sources of a system to authorised programs, processes, users or other systems.
Access control systems typically authenticate principles and then solicit access
to resources. They rely on the deﬁnition of speciﬁc permissions, called access
policies, which are recorded in some data structure such as Access Control Lists
(ACLs). ACLs work well when access policies are set in a centralised manners.
However, they are less suited to ubiquitous systems where the number of users
may be very large (think of sensor networks) and/or continuously changing. In
these scenarios users may be potentially unknown and, therefore, untrusted. In
order to overcome these limitations, Blaze et at. [1] have introduced the notion of
⋆This work has been partially supported by the national MIUR Project SOFT.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 157–172, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

158
M. Merro and E. Sibilio
Decentralised Trust Management as an attempt to deﬁne a coherent framework
in which safety critical decisions are based on trust policies relying on partial
knowledge.
Trust formalisation is the subject of several academic works. According to
[2], trust is the quantiﬁed belief by a trustor, the trusting party, with respect
to the competence, honesty, security and dependability of a trustee, the trusted
party, within a speciﬁed context. Trust information is usually represented as a
collection of assertions on the reliability of the parties. The trust establishment
process includes speciﬁcation of valid assertions, their generation, distribution,
collection and evaluation. Trust assertions may be uncertain, incomplete, sta-
ble and long term. Trust evaluation is performed by applying speciﬁc policies
to assertions; the result is a trust relation between the trustor and the trustee.
According to their scope and kind of trust evidence, trust frameworks can be
divided in two categories: certiﬁcate-based and behaviour-based. In the ﬁrst ones,
trust relations are usually based on certiﬁcates, to be spread, maintained and
managed either independently or cooperatively. In behaviour-based frameworks,
each node performs trust evaluation based on continuous monitoring of misbe-
haviours of neighbours (direct trust). Misbehaviours typically include dropping,
modiﬁcation, and misrouting of packets at network layer. However, trust evalu-
ation may also depend on node reputation. Node reputation usually comes from
other nodes (indirect trust) and does not reﬂect direct experience of the inter-
ested node. In history-based trust models, node reputation may also depend on
past behaviours.
The characteristics of mobile ad hoc networks pose a number of challenges
when designing an appropriate trust model for them. Due to the lack of a ﬁxed
network infrastructure, trust models for MANETs must be decentralised and
should support cooperative evaluation, according to the diversity in roles and
capabilities of nodes. There are various threats to ad hoc networks, of which the
most interesting and important is node subversion. In this kind of attack, a node
may be reverse-engineered, and replaced by a malicious node. A bad node can
communicate with any other node, good or bad. Bad nodes may have access to
the keys of all other bad nodes, whom they can impersonate if they wish. They do
not execute the authorised software and thus do not necessarily follow protocols
to identify misbehaviour, revoke other bad nodes, vote honestly or delete keys
shared with revoked nodes. So, trust frameworks for ad hoc networks should
support node revocation to isolate malicious nodes.
Another key feature of MANETs is their support for node mobility: devices
move while remaining connected to the network, breaking links with old neigh-
bours and establishing fresh links with new devices. This makes security even
more challenging as the compromise of a legitimate node or the insertion of a
malicious node may go unnoticed in such a dynamic environment. Thus, a mo-
bile node should acquire trust information on new neighbours, and remove trust
information on old neighbours that cannot be monitored anymore.
In this paper, we propose a process calculus for mobile ad hoc networks which
embodies a behaviour-based multilevel trust model. Our trust model supports

A Calculus of Trustworthy Ad Hoc Networks
159
both direct trust, by monitoring nodes behaviour, and indirect trust, by collecting
recommendations and spreading reputations. No information on past behaviours
of nodes is recorded. We model our networks as multilevel systems where each
device is associated to a security level depending on its role [3]. Thus, trust rela-
tions associate security levels to nodes. Process calculi have been recently used
to model diﬀerent aspects of wireless systems [4,5,6,7,8,9,10]. However, none of
these papers address the notion of trust. In our calculus, each node is equipped
with a local trust store containing a set of assertions. These assertions supply
trust information about the other nodes, according to a local security policy. Our
calculus is not directly concerned with cryptographic underpinnings. However,
we assume the presence of a hierarchical key generation and distribution proto-
col [11]. Thus, messages are transmitted at a certain security level relying on an
appropriate set of cryptographic keys. We provide the operational semantics of
our calculus in terms of a labelled transition system. Our transitions are of the
form
M
λ
−−→ρ N
indicating that the network M can perform the action λ, at security level ρ,
evolving into the network N. For simplicity, our operational semantics does not
directly express mobility. However, we can easily adapt the approach proposed
in [9] to annotate our labelled transitions with the necessary information to
represent node mobility.
Our calculus enjoys two desirable security properties: safety up to a security
level and safety despite compromise. Intuitively, the ﬁrst property means that
only trusted nodes, i.e. with an appropriate security level, may synchronise with
other nodes. The second property says that bad (compromised) nodes, once
detected, may not interact with good nodes.
A central concern in process calculi is to establish when two terms have the
same observable behaviour. Behavioural equivalences are fundamental for justi-
fying program transformations. Our program equivalence is a security variant of
(weak) reduction barbed congruence, a branching-time contextually-deﬁned pro-
gram equivalence. Barbed equivalences [12] are simple and intuitive but diﬃcult
to use due to the quantiﬁcation on all contexts. Simpler proof techniques are
based on labelled bisimilarities [13], which are co-inductive relations that char-
acterise the behaviour of processes using a labelled transition system. We deﬁne
a labelled bisimilarity parameterised on security levels proving that it represents
an eﬃcient proof method for our reduction barbed congruence.
We apply our notion of bisimilarity to prove a non-interference property for
our networks. Intuitively, a network is interference free if its low security level
behaviour is not aﬀected by any activity at high security level.
2
A Behaviour-Based Multilevel Decentralised Trust
Model
In our framework each node comes together with an extra component called
trust manager. A trust manager consists of two main modules: the monitoring

160
M. Merro and E. Sibilio
module and the reputation handling module. The ﬁrst one monitors the be-
haviour of neighbours, while the second one collects/spreads recommendations
and evaluates trust information about other nodes using a local security pol-
icy. The continuous work of the trust manager results in a local trust store T
containing the up-to-date trust relations.
Trust information may change over time due to mobility, temporary discon-
nections, recommendations, etc. As a consequence, trust knowledge may be un-
certain and incomplete. The main objective of the model is to isolate bad nodes,
i.e. nodes which do not behave as expected. For this reason, we support node
revocation. This may happens when a node detects a misbehaviour of another
node, and spreads this information to its neighbours. Repudiable evidences en-
able bad nodes to falsely accuse good nodes. Hence, it would be foolish to design
a simple decision mechanism that revokes any node accused of misbehaviour.
Thus, recommendations are always evaluated using a local security policy im-
plementing an appropriate metric.
The basic elements of our model are nodes (or principals), security levels,
assertions, policies and trust stores. We use k, l, m, n, . . . to range over the set
Nodes of node names. We assume a complete lattice ⟨S, <⟩of security levels:
bad < trust < low < high. We use the Greek letter ρ for security levels
belonging to S. The set of assertions is deﬁned as Assertions = Nodes ×Nodes ×
S. Thus, an assertion ⟨m, n, ρ⟩says that a node m trusts a node n at security level
ρ. A local trust store T contains a set of assertions, formally T ⊆℘(Assertions).
A node can receive new assertions from its neighbours. These assertions will be
opportunely stored in the local trust store by the trust manager, according to
a local security policy P. A security policy P is a function that evaluates the
current information collected by a node and returns a set of consistent assertions,
formally P : ℘(Assertions) →℘(Assertions). For simplicity, we assume that all
nodes have the same security policy P. Notice that the outcome of the policy
function could diﬀer from one node to another as the computation depends
on the local knowledge of nodes. Thus, when a node m (the trustor) wants to
know the security level of a node n (the trustee), it has to check its own trust
store T . For convenience, we often use T as a partial function of type Nodes →
Nodes →S, writing T (m, n) = ρ if m considers n as a node of security level ρ.
If ρ = bad then m considers n a bad (unreliable) node and stops any interaction
with it.
Messages exchanged among nodes are assumed to be encrypted using a hier-
archical key generation and distribution protocol [14]. The trust manager may
determine a key redistribution when a security level is compromised. More
generally, re-keying [15] allows to refresh a subset of keys when one or more
nodes join or leave the network; in this manner nodes are enable to decrypt
past traﬃc, while evicted nodes are unable to decrypt future traﬃc. As showed
in [14] re-keying may be relatively unexpensive if based on “low-cost” hashing
operators.

A Calculus of Trustworthy Ad Hoc Networks
161
3
The Calculus
In Table 1, we deﬁne the syntax of our calculus in a two-level structure, a lower
one for processes and a upper one for networks. We use letters k, l, m, n, . . . for
node names. The Greek symbol σ ranges over the security levels low and high,
the only ones which are directly used by programmers. We use letters x, y, z for
variables, u for values, and v and w for closed values, i.e. values that do not
contain free variables. We write ˜u to denote a tuple u1, . . . , uk of values.
Networks are collections of nodes (which represent devices) running in par-
allel and using channels at diﬀerent security levels to communicate with each
other. We use the symbol 0 to denote an empty network. We write M | N for
the parallel composition of two sub-networks M and N. We write n[P]T for a
node named n (denoting its network address) executing the sequential process
P, with a local trust store T . Processes are sequential and live within the nodes.
We write nil to denote the skip process. The sender process σ!⟨˜v⟩.P can broad-
cast the value ˜v at security level σ, continuing as P. A message transmitted at
security level ρ can be decrypted only by nodes at security level ρ or greater, ac-
cording to the trust store of both sender and receiver. Moreover, we assume that
messages are always signed by transmitters. The receiver process σ?(˜x).P listens
on the channel for incoming communications at security level σ. Upon reception,
the receiver process evolves into P, where the variables of ˜x are replaced with
the message ˜v. We write {˜v/˜x}P for the substitution of variables ˜x with values
˜v in P. Process [˜v = ˜w]P, Q is the standard “if then else” construct: it behaves
as P if ˜v = ˜w, and as Q otherwise. We write H⟨˜v⟩to denote a process deﬁned
via a deﬁnition H(˜x)
def
= P, with | ˜x |=| ˜v |, where ˜x contains all variables that
appear free in P. Deﬁning equations provide guarded recursion, since P may con-
tain only guarded occurrences of process identiﬁers. In process σ?(˜x).P variables
˜x are bound in P. This gives rise to the standard notion of α-conversion and
Table 1. The Syntax
Values
u ::= v
closed value

x
variable
Networks:
M, N ::= 0
empty network

M | N
parallel composition

n[P]T
node
Processes:
P, Q ::= nil
termination

σ!⟨˜u⟩.P
broadcast

σ?(˜x).P
receiver

[˜u = ˜u′]P, Q
matching

H⟨˜u⟩
recursion

162
M. Merro and E. Sibilio
free and bound variables. We assume there are no free variables in our networks.
The absence of free variables in networks is trivially maintained as the network
evolves. Given a network M, nds(M) returns the set of the names of those nodes
which constitute the network M. Notice that, as networks addresses are unique,
we assume that there cannot be two nodes with the same name in the same net-
work. We write 
i Mi to denote the parallel composition of all sub-networks Mi.
Finally, we deﬁne structural congruence, written ≡, as the smallest congruence
which is a commutative monoid with respect to the parallel operator.
3.1
The Operational Semantics
We give the operational semantics of our calculus in terms of a Labelled Tran-
sition System (LTS). We have divided our LTS in two sets of rules. Table 2
contains the rules to model the synchronisation between sender and receivers.
Table 3 contains the rules to model trust management, i.e. the actions of the
trust manager components.
Our transitions are of the form M
λ
−−→ρ M ′, indicating that the network M
can perform the action λ, at security level ρ, evolving into the network M ′. By
construction, in such a transition, ρ will be always diﬀerent from bad. More
precisely, ρ will be equal to low for low-level-security transmissions, and equal
to high for high-level-security transmissions. If ρ = trust then the transition
models some aspects of trust management and involves all trusted nodes. The
label λ ranges over the actions m!˜v▷D, m?˜v▷D, and τ. The action m!˜v▷D models
the transmission of message ˜v, originating from node m, and addressed to the
set of nodes in D. The action m?˜v ▷D represents the reception of a message
˜v, sent by m, and received by the nodes in D. We sometimes write m?˜v ▷n
as an abbreviation for m?˜v ▷{n}. The action τ models silent actions, as usual.
Table 2. LTS - Synchronisation
(Snd)
D := {n : T(m, n) ≥σ}
m[σ!⟨˜v⟩.P]T
m!˜v▷D
−−−−−−→σ m[P]T
(Rcv)
T(n, m) ≥σ
| ˜x |=| ˜v |
n[σ?(˜x).P]T
m?˜v▷n
−−−−−−→σ n[{˜v/˜x}P]T
(RcvPar) M
m?˜v▷D
−−−−−−→ρ M ′
N
m?˜v▷D′
−−−−−−−→ρ N ′
D := D ∪D′
M | N
m?˜v▷
D
−−−−−−→ρ M ′ | N ′
(Sync) M
m!˜v▷D
−−−−−−→ρ M ′
N
m?˜v▷D′
−−−−−−−→ρ N ′
D′ ⊆D
M | N
m!˜v▷D
−−−−−−→ρ M ′ | N ′
(Par) M
λ
−−→ρ M ′
sender(λ) /∈nds(N)
M | N
λ
−−→ρ M ′ | N

A Calculus of Trustworthy Ad Hoc Networks
163
The function sender(·) applied to an action returns the name of the sender, thus
sender(m!˜v▷D) = sender(m?˜v ▷D) = m, whereas sender(τ) =⊥.
Let us comment on the rules of Table 2. Rule (Snd) models a node m which
broadcasts a message ˜v at security level σ; the set D contains the nodes at
security level at least σ, according to the trust store of m. Rule (Rcv) models a
node n receiving a message ˜v, sent by node m, at security level σ. Node n receives
the message from m only if it trusts m at security level σ. Rule (RcvPar) serves
to put together parallel nodes receiving from the same sender. If sender and
receiver(s) trust each other there will be a synchronisation.1 Rule (Sync) serves
to synchronise the components of a network with a broadcast communication;
the condition D′ ⊆D ensures that only authorised recipients can receive the
transmitted value. Rule (Par) is standard in process calculi. Notice that using
rule (Par) we can model situations where potential receivers do not necessarily
receive the message, either because they are not in the transmission range of the
transmitter or simply because they loose the message. Rules (Sync), (RcvPar)
and (Par) have their symmetric counterparts.
Example 1. Let us consider the network:
M
def
= k[σ?(˜x).Pk]Tk | l[σ?(˜x).Pl]Tl | m[σ!⟨˜v⟩.Pm]Tm | n[σ?(˜x).Pn]Tn
where Tk(k, m) ≥σ, Tl(l, m) < σ, Tm(m, n) = Tm(m, l) ≥σ, Tm(m, k) < σ and
Tn(n, m) ≥σ. In this conﬁguration, node m broadcasts message ˜v at security
level σ, knowing that the nodes allowed to receive the message at that security
level are n and l. However, node l does not trust m at security level σ. Thus, n
is the only node that may receive the message. By an application of rules (Snd),
(Rcv), (Par), and (Sync) we have:
M
m!˜v▷D
−−−−−−→σ k[σ?(˜x).Pk]Tk | l[σ?(˜x).Pl]Tl | m[Pm]Tm | n[{˜v/˜x}Pn]Tn .
Now, let us comment on the rules of Table 3 modelling trust management. Rule
(Susp) models direct trust. This happens when the monitoring module of a node
m, while monitoring the activity of a trusted node n, detects a misbehaviour of
n. In this case, node m executes two operations: (i) it implements node revoca-
tion updating its trust store, according to its local policy; (ii) it broadcasts the
corresponding information to inform all trusted nodes about the misbehaviour
of n. Notice that this transmission is not under the control of the code of m
but it rather depends on the reputation handling module. Notice also that the
transmission is addressed to all trusted nodes, that’s why the transmission ﬁres
at security level trust. Rule (SndRcm) models indirect trust by sending a rec-
ommendation. This may happen, for example, when a node moves and asks for
recommendations on new neighbours. Again, recommendations are addressed to
all trusted nodes, according to the trust knowledge of the recommender. Rule
(RcvRcm) models the reception of a recommendation from a trusted node: a
1 Here, we abstract on the actual behaviour of receivers as they verify the identity of
the sender and discard unauthorised messages.

164
M. Merro and E. Sibilio
Table 3. LTS - Trust Management
(Susp)
T(m, n) > bad
˜v := n, bad
T ′ := P(T ∪⟨m, ˜v⟩)
D := {n : T(m, n) > bad}
m[P]T
m!˜v▷D
−−−−−−→trust m[P]T ′
(SndRcm)
T(m, n) = ρ
˜v := n, ρ
D := {n : T(m, n) > bad}
m[P]T
m!˜v▷D
−−−−−−→trust m[P]T
(RcvRcm)
T(n, m) > bad
˜v := l, ρ
T ′ := P(T ∪⟨m, ˜v⟩)
n[P]T
m?˜v▷n
−−−−−−→trust n[P]T ′
(Loss) T ′ ⊆T
T ′′ := P(T ′)
n[P]T
τ
−−→trust n[P]T ′′
new trust table T ′ is calculated, applying the local policy to T ∪⟨m, ˜v⟩. Rule
(Loss) models loss of trust information. This happens, for instance, when a node
moves, changing its neighbourhood. In this case, assertions concerning old neigh-
bours must be deleted as they cannot be directly veriﬁed. The consistency of the
remaining assertions must be maintained by applying the security policy.
Example 2. Let us show how direct and indirect trust work. Let us consider the
network:
M
def
= k[Pk]Tk | l[Pl]Tl | m[Pm]Tm | n[Pn]Tn
where Tk(k, m) ≥trust, Tl(l, m) = bad, Tm(m, n) = Tm(m, l) = Tm(m, k) ≥
trust, and Tn(n, m) ≥trust. Now, if node m observes that node k is misbe-
having, then (i) it adds an assertion ⟨m, k, bad⟩to its local knowledge; (ii) it
broadcasts the information to its neighbours. Thus, by an application of rules
(Susp), (RcvRcm), (Par), and (Sync) we have
M
m!˜v▷D
−−−−−−→trust k[Pk]T ′
k | l[Pl]Tl | m[Pm]T ′
m | n[Pn]T ′
n .
Notice that since l does not trust m, only node n (but also the bad node k)
will receive m’s recommendation. Moreover the local knowledge of m and n will
Table 4. LTS - Matching and recursion
(Then)
n[P]T
λ
−−→ρ n[P ′]T ′
n[[˜v = ˜v]P, Q]T
λ
−−→ρ n[P ′]T ′
(Else)
n[Q]T
λ
−−→ρ n[Q′]T ′
˜v1 ̸
= ˜v2
n[[ ˜v1 = ˜v2]P, Q]T
λ
−−→ρ n[Q′]T ′
(Rec) n[{˜v/˜x}P]T
λ
−−→ρ n[P ′]T ′
H(˜x)
def
= P
n[H⟨˜v⟩]T
λ
−−→ρ n[P ′]T ′

A Calculus of Trustworthy Ad Hoc Networks
165
change, accordingly to the local policy. This is a case of direct trust for m, and
indirect trust for n. The security level that n will assign to k will actually depend
the local policy of n.
Finally, Table 4 contains the standard rules for matching and recursion.
4
Node Mobility
In wireless networks node mobility is associated with the ability of a node to
access telecommunication services at diﬀerent locations from diﬀerent nodes.
Node mobility in ad hoc networks introduces new security issues related to user
credential management, indirect trust establishment and mutual authentication
between previously unknown and hence untrusted nodes. Thus, mobile ad hoc
networks has turned to be a challenge for automated veriﬁcation and analysis
techniques. After the ﬁrst works on model checking of (stationary) ad hoc net-
works [16], Nanz and Hankin [5] have proposed a process calculus where topology
changes are abstracted into a ﬁxed representation. This representation, called
network topology, is essentially a set of connectivity graphs denoting the possible
connectivities within the nodes of the network.
Table 5. LTS - Synchronisation with network restrictions
(SndR)
D := {n : T(m, n) ≥σ}
m[σ!⟨˜v⟩.P]T
m!˜v▷D
−−−−−−→σ,∅m[P]T
(RcvR)
T(n, m)≥σ
|˜x|=|˜v|
P ′:={˜v/˜x}P
n[σ?(˜x).P]T
m?˜v▷n
−−−−−−→σ,(n,m) n[P ′]T
(RcvParR) M
m?˜v▷D
−−−−−−→ρ,C1 M ′
N
m?˜v▷D′
−−−−−−−→ρ,C2 N ′
D := D ∪D′
M | N
m?˜v▷
D
−−−−−−→ρ,C1∪C2 M ′ | N ′
(SyncR) M
m!˜v▷D
−−−−−−→ρ,C1 M ′
N
m?˜v▷D′
−−−−−−−→ρ,C2 N ′
D′ ⊆D
M | N
m!˜v▷D
−−−−−−→ρ,C1∪C2 M ′ | N ′
(ParR) M
λ
−−→ρ,C M ′
sender(λ) /∈nds(N)
M | N
λ
−−→ρ,C M ′ | N
As the reader may have noticed, our calculus does not directly model the
network topology neither in the syntax nor in the semantics. However, it is very
easy to add topology changes at semantics level, so that each state represents
a set of valid topologies, and a network can be at any of those topologies at
any time [9]. In Table 5 we rewrite the rules of Table 2 in the style of [9]. Rules
are of the form M
λ
−−→ρ,C M ′, indicating that the network M can perform the
action λ, at security level ρ, under the network restriction C, evolving into the

166
M. Merro and E. Sibilio
network M ′. Thus, a network restriction C keeps track of the connections which
are necessary for the transition to ﬁre. The rules in Table 3 can be rewritten in a
similar manner, except for rule (Loss) in which the network restriction is empty
i.e. C = ∅.
Example 3. Consider the same network given in the Example 1. Then by apply-
ing rules (SndR), (RcvR), (ParR), and (SyncR) we have
M
m!˜v▷D
−−−−−−→σ,{(n,m)} k[σ?(˜x).Pk]Tk | l[σ?(˜x).Pl]Tl | m[Pm]Tm | n[{˜v/˜x}Pn]Tn.
The transition is tagged with the network restriction {(n, m)}, as only node n
has synchronised with node m.
Notice that the rule (Loss) in Table 3 may indirectly aﬀect future communica-
tions. In fact, if a trust information is lost then certain nodes may not be able
of communicating anymore.
The reader may have noticed that the rules of Table 5 do not use network
restrictions in the premises. As a consequence, there is a straightforward opera-
tional correspondence between a transition
λ
−−→ρ and one of the form
λ
−−→ρ,C.
Proposition 1
1. M
λ
−−→ρ M ′ with λ ∈{m!˜v▷D, m?˜v ▷D} iﬀthere exists a restriction C such
that M
λ
−−→ρ,C M ′ and C ⊆{(m, n) for all n ∈D}.
2. M
τ
−−→ρ M ′ iﬀM
τ
−−→ρ,∅M ′.
Proof. By transition induction.
□
5
Safety Properties
In this section, we show how to guarantee in our setting that only authorised
nodes receive sensible information. We deﬁne a notion of safety up to a security
level to describe when a communication is safe up to a certain security level.
Deﬁnition 1 (Safety up to a security level). A node m transmitting at level
ρ may only synchronise with a node n receiving at level ρ or above, according to
the local knowledge of m and n, respectively.
Intuitively, Deﬁnition 1 says that a synchronisation at a certain security level ρ
is safe if the involved parties trust each other at that security level.
The safety property is then preserved at run time.
Theorem 1 (Safety preservation). Let M
m!˜v▷D
−−−−−−→ρ M ′ with
M ≡m[P]T | 
i ni[Pi]Ti and M ′ ≡m[P ′]T ′ | 
i ni[P ′
i]T ′
i .
1. If P ′
i ̸= Pi, for some i, then T (m, ni) ≥ρ and Ti(ni, m) ≥ρ.
2. If T ′
i ̸= Ti, for some i, then T (m, ni) ≥ρ and Ti(ni, m) ≥ρ.
Proof. By induction on the transition M
m!˜v▷D
−−−−−−→ρ M ′.
□

A Calculus of Trustworthy Ad Hoc Networks
167
A consequence of Theorem 1, is that (trusted) nodes never synchronise with
untrusted nodes. In this manner, bad nodes (recognised as such) are isolated
from the rest of the network.
Corollary 1 (Safety despite compromise). Let M
m!˜v▷D
−−−−−−→ρ M ′ such that
M ≡m[P]T |

i
ni[Pi]Ti and M ′ ≡m[P ′]T ′ |

i
ni[P ′
i ]T ′
i .
If T (m, ni)=bad or Ti(ni, m)=bad, for some i, then P ′
i=Pi and T ′
i=Ti.
6
Behavioural Semantics
Our main behavioural equivalence is σ-reduction barbed congruence, a variant of
Milner and Sangiorgi’s (weak) barbed congruence [12] which takes into account
security levels. Basically, two terms are barbed congruent if they have the same
observables (called barbs) in all possible contexts, under all possible evolutions.
For the deﬁnition of barbed congruence we need two crucial concepts: a reduction
semantics to describe how a system evolves, and a notion of observable which
says what the environment can observe in a system.
From the LTS given in Section 3.1 it is easy to see that a network may evolves
either because there is a transmission at a certain security level or because a
node looses some trust information. Thus, we can deﬁne the reduction relation
 between networks using the following inference rules:
(Red1) M
m!˜v▷D
−−−−−−→ρ M ′
M  M ′
(Red2) M
τ
−−→trust M ′
M  M ′
We write ∗to denote the reﬂexive and transitive closure of .
In our calculus, we have both transmission and reception of messages al-
though only transmissions may be observed. In fact, in a broadcasting calculus
an observer cannot see whether a given process actually receives a broadcast
synchronisation. In particular, if the node m[σ!⟨˜v⟩.P]T evolves into m[P]T we
do not know whether some potential recipient has synchronised with m. On the
other hand, if a node n[σ?(˜x).P]T evolves into n[{˜v/˜x}P]T , then we can be sure
that some trusted node has transmitted a message ˜v to n at security level σ.
Deﬁnition 2 (σ-Barb). We write M ↓σ
n if M ≡m[σ!⟨˜v⟩.P]T | N, for some
m, N, ˜v, P, T such that n /∈nds(M), and T (m, n) ≥σ. We write M ⇓σ
n if M ∗
M ′ ↓σ
n for some network M ′.
The barb M ⇓σ
n says that there is a potential transmission at security level σ,
originating from M, and that may reach the node n in the environment. In the
sequel, we write R to denote binary relations over networks.
Deﬁnition 3 (σ-Barb preserving). A relation R is said to be σ-barb pre-
serving if whenever M R N it holds that M ↓σ
n implies N ⇓σ
n.

168
M. Merro and E. Sibilio
Deﬁnition 4 (Reduction closure). A relation R is said to be reduction closed
if M R N and M  M ′ imply there is N ′ such that N ∗N ′ and M ′ R N ′.
As we are interested in weak behavioural equivalences, the deﬁnition of reduction
closure is given in terms of weak reductions.
Deﬁnition 5 (Contextuality). A relation R is said to be contextual if M R
N implies that M | O R N | O, for all networks O.
Finally, everything is in place to deﬁne our σ-reduction barbed congruence.
Deﬁnition 6 (σ-Reduction barbed congruence). The σ-reduction barbed
congruence, written ∼=σ, is the largest symmetric relation over networks which
is σ-barb preserving, reduction closed and contextual.
7
Bisimulation Proof Ethod
The deﬁnition of σ-reduction barbed congruence is simple and intuitive. How-
ever, due to the universal quantiﬁcation on parallel contexts, it may be quite
diﬃcult to prove that two terms are barbed congruent. Simpler proof techniques
are based on labelled bisimilarities. In the sequel we deﬁne an appropriate no-
tion of bisimulation. As a main result, we prove that our labelled bisimilarity is
a proof-technique for our σ-reduction barbed congruence.
In general, a bisimulation describes how two terms (in our case networks) can
mimic each other actions. First of all we have to distinguish between transmis-
sions which may be observed and transmissions which may not be observed by
the environment.
(Shh) M
m!˜v▷D
−−−−−−→ρ M ′ D⊆nds(M) ρ′̸=bad
M
τ
−−→ρ′ M ′
(Obs) M
m!˜v▷D
−−−−−−→ρ M ′
D:=D\nds(M)̸=∅
M
m!˜v▶D
−−−−−−→ρ M ′
Rule (Shh) models transmissions that cannot be observed because none of the
potential receivers are in the environment. Notice that security levels of τ-action
are not related to the transmissions they originate from. Rule (Obs) models a
transmission, at security level ρ, of a message ˜v, from a sender m, that may be
received by the nodes of the environment contained in D. Notice that the rule
(Obs) can only be applied at top-level of a derivation tree. In fact, we cannot use
this rule together with rule (Par) of Table 2, because λ does not range on the
new action.
In the sequel, we use the metavariable α to range over the following actions: τ,
m?˜v ▷D, and m!˜v▶D. Since we are interested in weak behavioural equivalences,
that abstract over τ-actions, we introduce a standard notion of weak action: we
write =⇒ρ to denote the reﬂexive and transitive closure of
τ
−−→ρ; we also write
α
==⇒ρ to denote =⇒ρ
α
−−→ρ =⇒ρ;
ˆα
==⇒ρ denotes =⇒ρ if α = τ and
α
==⇒ρ otherwise.

A Calculus of Trustworthy Ad Hoc Networks
169
Deﬁnition 7 (δ-Bisimilarity). The δ-bisimilarity, written ≈δ, is the largest
symmetric relation over networks such that whenever M ≈δ N if M
α
−−→ρ M ′,
with ρ ≤δ, then there exists a network N ′ such that N
ˆα
==⇒ρ N ′ and M ′ ≈δ N ′.
This deﬁnition is inspired by that proposed in [17]. Intuitively, two networks are
δ-bisimilar if they cannot be distinguished by any observers that cannot perform
actions at security level greater than δ.
Theorem 2 (≈δ is contextual). Let M and N be two networks such that
M ≈δ N. Then M | O ≈δ N | O for all networks O.
Proof. We prove that the relation
S
def
= {

M | O , N | O

for all O such that M ≈δ N}
is a δ-bisimulation.
□
Theorem 3 (Soundness). Let M and N be two networks such that M ≈δ N.
Then M ∼=σ N, for σ ≤δ.
Proof. It is easy to verify that δ-bisimilarity is σ-barb preserving and reduction
closed, by deﬁnition. Contextuality follows by Theorem 2.
□
Remark 1. For the sake of analysis, we can deﬁne the δ-bisimilarity using the
labelled transition system with network restrictions of Table 5. However, by
Proposition 1 the resulting bisimilarity would not change.
8
Non-interference
The seminal idea of non interference [18] aims at assuring that “variety in a
secret input should not be conveyed to public output”. In a multilevel computer
system [3] this property says that information can only ﬂow from low levels
to higher ones. The ﬁrst taxonomy of non-interference-like properties has been
uniformly deﬁned in a CCS-like process calculus with high-level and low-level
processes, according to the level of actions that can be performed [19]. To de-
tect whether an incorrect information ﬂow (i.e. from high-level to low-level) has
occurred, a particular non-interference-like property has been deﬁned, the so-
called Non Deducibility on Composition (NDC). This property basically says
that a process is secure with respect to wrong information ﬂows if its low-level
behaviour is independent of changes to its high-level behaviour.
Here, we prove a non-interference result using as process equivalence the no-
tion of δ-bisimilarity previously deﬁned. Formally, high-level behaviours can be
arbitrarily changed without aﬀecting low-level equivalences.
Deﬁnition 8 describes what high-level behaviour means in our setting. We
recall that we assumed the presence of a trust manager component for each
node to manage trust information. As a consequence, actions at security level
trust do not depend on the syntax of the processes as they depend on the trust
manager. These actions can ﬁre at any step of the computation and cannot be
predicted in advance.

170
M. Merro and E. Sibilio
Deﬁnition 8 (δ-high level network). A network H is a δ-high level network,
written H ∈Hδ, if whenever H
λ
−−→δ′ H′ then either δ′ = trust or δ′ > δ.
Moreover, H′ ∈Hδ.
The non-interference result can be stated as follows.
Theorem 4 (Non-interference). Let M and N be two networks such that
M ≈δ N. Let H and K be two networks such that: (i) H, K ∈Hδ, (ii) H ≈trust
K, and (iii) nds(H) = nds(K). Then, M | H ≈δ N | K.
Proof. We prove that the relation
{

M | H , N | K

: H, K ∈Hδ, M ≈δ N, H ≈trust K and nds(H)=nds(K)}
is a δ-bisimulation.
□
9
Related Work
Formal methods have been successfully applied for the analysis of network secu-
rity (see, for instance, [20,21,22,23,24]).
Komarova and Riguidel [25] have proposed a centralised trust-based access
control mechanism for ubiquitous environments. The goal is to allow a service
provider for the evaluation of the trustworthiness of each potential client. Crafa
and Rossi [17] have introduced a notion of controlled information release for a
typed version of the π-calculus extended with declassiﬁed actions. The controlled
information release property scales to non interference when downgrading is
not allowed. They provide various characterisations of controlled release, based
on typed behavioural equivalence, parameterised on security levels, to model
observers at a certain security level. Hennessy [26] has proposed a typed version
of the asynchronous π-calculus in which I-O types are associated to security
levels. Typed equivalences are then used to prove a non interference result.
As regards process calculi for wireless systems, Mezzetti and Sangiorgi [4]
have proposed calculus to describe interferences in wireless systems. Nanz and
Hankin [5] have introduced a calculus for mobile wireless networks for speciﬁca-
tion and security analysis of communication protocols. Merro [7] has proposed
a behavioural theory for MANETs. Godskesen [8] has proposed a calculus for
mobile ad hoc networks with a formalisation of an attack on the cryptographic
routing protocol ARAN. Singh et al. [6] have proposed the ω-calculus for mod-
elling the AODV routing protocol. Ghassemi et al. [9] have proposed a process
algebra where topology changes are implicitly modelled in the semantics. Merro
and Sibilio [27] have proposed a timed calculus for wireless systems focusing on
the notion of communication collision. In trust models for ad hoc networks, the
timing factor is important because more recent trust informations should have
more inﬂuence on the trust establishment process. More generally, a notion of
time would allow to record past behaviours. Finally, Godskesen and Nanz [10]
have proposed a simple timed calculus for wireless systems to express a wide
range of mobility models.

A Calculus of Trustworthy Ad Hoc Networks
171
None of the calculi mentioned above deal with trust. Carbone et al. [28] have
introduced ctm, a process calculus which embodies the notion of trust for ubiqui-
tous systems. In ctm each principal is equipped with a policy, which determines
its legal behaviour, formalised using a Datalog-like logic, and with a protocol, in
the process algebra style, which allows interactions between principals and the
ﬂow of information from principals to policies. In [29] Martinelli uses a crypto-
graphic variant of CCS to describe and analyse diﬀerent access control policies.
References
1. Blaze, M., Feigenbaum, J., Lacy, J.: Decentralized Trust Management. In: Sympo-
sium on Security and Privacy, pp. 164–173. IEEE Computer Society, Los Alamitos
(1996)
2. Grandison, T.W.A.: Trust Management for Internet Applications. PhD thesis, De-
partment of Computing, University of London (2003)
3. Bell, D.E., LaPadula, L.J.: Secure Computer System: Uniﬁed Exposition and Mul-
tics Interpretation. Technical Report MTR-2997, MITRE Corporation (1975)
4. Mezzetti, N., Sangiorgi, D.: Towards a Calculus For Wireless Systems. Electronic
Notes in Theoretical Computer Science 158, 331–353 (2006)
5. Nanz, S., Hankin, C.: A Framework for Security Analysis of Mobile Wireless Net-
works. Theoretical Computer Science 367(1-2), 203–227 (2006)
6. Singh, A., Ramakrishnan, C.R., Smolka, S.A.: A Process Calculus for Mobile Ad
Hoc Networks. In: Lea, D., Zavattaro, G. (eds.) COORDINATION 2008. LNCS,
vol. 5052, pp. 296–314. Springer, Heidelberg (2008)
7. Merro, M.: An Observational Theory for Mobile Ad Hoc Networks (full paper).
Information and Computation 207(2), 194–208 (2009)
8. Godskesen, J.: A Calculus for Mobile Ad Hoc Networks. In: Murphy, A.L., Vitek, J.
(eds.) COORDINATION 2007. LNCS, vol. 4467, pp. 132–150. Springer, Heidelberg
(2007)
9. Ghassemi, F., Fokkink, W., Movaghar, A.: Equational Reasoning on Ad Hoc Net-
works. In: Sirjani, M. (ed.) FSEN 2009. LNCS, vol. 5961, pp. 113–128. Springer,
Heidelberg (2010)
10. Godskesen, J.C., Nanz, S.: Mobility Models and Behavioural Equivalence for Wire-
less Networks. In: Field, J., Vasconcelos, V.T. (eds.) COORDINATION 2009.
LNCS, vol. 5521, pp. 106–122. Springer, Heidelberg (2009)
11. Huang, D., Medhi, D.: A Secure Group Key Management Scheme for Hierarchical
Mobile Ad Hoc Networks. Ad Hoc Networks 6(4), 560–577 (2008)
12. Milner, R., Sangiorgi, D.: Barbed Bisimulation. In: Kuich, W. (ed.) ICALP 1992.
LNCS, vol. 623, pp. 685–695. Springer, Heidelberg (1992)
13. Milner, R.: Communication and Concurrency. Prentice-Hall, Englewood Cliﬀs
(1989)
14. Shehab, M., Bertino, E., Ghafoor, A.: Eﬃcient Hierarchical Key Generation and
Key Diﬀusion for Sensor Networks. In: SECON, pp. 76–84. IEEE Communications
Society, Los Alamitos (2005)
15. Di Pietro, R., Mancini, L.V., Law, Y.W., Etalle, S., Havinga, P.J.M.: LKHW: A
Directed Diﬀusion-Based Secure Multicast Scheme for Wireless Sensor Networks.
In: ICPP Workshops 2003, pp. 397–413. IEEE Computer Society, Los Alamitos
(2003)

172
M. Merro and E. Sibilio
16. Bhargavan, K., Obradovic, D., Gunter, C.A.: Formal Veriﬁcation of Standards for
Distance Vector Routing Protocols. Journal of the ACM 49(4), 538–576 (2002)
17. Crafa, S., Rossi, S.: Controlling Information Release in the π-calculus. Information
and Computation 205(8), 1235–1273 (2007)
18. Goguen, J.A., Meseguer, J.: Security Policies and Security Models. In: IEEE Sym-
posium on Security and Privacy, pp. 11–20 (1982)
19. Focardi, R., Gorrieri, R.: A Classiﬁcation of Security Properties for Process Alge-
bras. Journal of Computer Security 3(1), 5–33 (1995)
20. Reitman, R., Andrews, G.: An Axiomatic Approach to Information Flow in Pro-
grams. ACM Transactions on Programming Languages and Systems 2(1), 56–76
(1980)
21. Smith, G., Volpano, D.: Secure Information Flow in a Multi-threaded Imperative
Language. In: Proc. 25th POPL, pp. 355–364. ACM Press, New York (1998)
22. Heintz, N., Riecke, J.G.: The SLam Calculus: Programming with Secrecy and In-
tegrity. In: Proc. 25th POPL, pp. 365–377. ACM Press, New York (1998)
23. Bodei, C., Degano, P., Nielson, F., Nielson, H.R.: Static Analysis for the pi-Calculus
with Applications to Security. Information and Computation 168(1), 68–92 (2001)
24. Boudol, G., Castellani, I.: Noninterference for Concurrent Programs and Thread
Systems. Theoretical Computer Science 281(1-2), 109–130 (2002)
25. Komarova, M., Riguidel, M.: Adjustable Trust Model for Access Control. In: Rong,
C., Jaatun, M.G., Sandnes, F.E., Yang, L.T., Ma, J. (eds.) ATC 2008. LNCS,
vol. 5060, pp. 429–443. Springer, Heidelberg (2008)
26. Hennessy, M.: The Security pi-calculus and Non-Interference. Journal of Logic and
Algebraic Programming 63(1), 3–34 (2005)
27. Merro, M., Sibilio, E.: A Timed Calculus for Wireless Systems. In: Arbab, F.,
Sirjani, M. (eds.) FSEN 2009. LNCS, vol. 5961, pp. 228–243. Springer, Heidelberg
(2010)
28. Carbone, M., Nielsen, M., Sassone, V.: A Calculus for Trust Management. In:
Lodaya, K., Mahajan, M. (eds.) FSTTCS 2004. LNCS, vol. 3328, pp. 161–173.
Springer, Heidelberg (2004)
29. Martinelli, F.: Towards an Integrated Formal Analysis for Security and Trust. In:
Steﬀen, M., Zavattaro, G. (eds.) FMOODS 2005. LNCS, vol. 3535, pp. 115–130.
Springer, Heidelberg (2005)

Comparison of Cryptographic Veriﬁcation Tools
Dealing with Algebraic Properties
Pascal Lafourcade, Vanessa Terrade, and Sylvain Vigier⋆
Universit´e Grenoble 1, CNRS, Verimag, France
{firstname.lastname}@imag.fr
Abstract. Recently Kuesters et al proposed two new methods using
ProVerif for analyzing cryptographic protocols with Exclusive-Or and
Diﬃe-Hellman properties. Some tools, for instance CL-Atse and OFMC,
are able to deal with Exclusive-Or and Diﬃe-Hellman. In this article we
compare time eﬃciency of these tools verifying some protocols of the
litterature that are designed with such algebraic properties.
1
Introduction
Use of cryptographic primitives for encoding secret data is not suﬃcient to ensure
security. For example even with encrypted data, there exist security ﬂaws: an
intruder is able to discover information that should remain secret between two
participants. As a consequence, an important activity of research on this topic
leads to the development of diﬀerent tools for verifying cryptographic protocols.
One of the main properties required by cryptographic protocols is the secrecy,
which means that secret data generated by an honest agent should not be learnt
by an intruder. Another important property is the authentication, which means
that every party can authenticate the party with whom they are executing the
protocol.
Over the last decades many automatic tools based on formal analysis tech-
niques have been presented for verifying cryptographic protocols [2,8,11,16,19,
31, 33, 35, 41, 22]. All these tools use the so-called Dolev-Yao intruder model.
This modelling of the adversary oﬀers a good level of abstraction and allows
such tools to perform a formal analysis. All these works use the perfect encryp-
tion hypothesis, which means that the only way to decrypt a cipher text is to
know the inverse key. This hypothesis abstracts the cryptography in order to
detect a “logical ﬂaw” due to all possible interleaving of diﬀerent executions of
the protocol. For relaxing this assumption, many works have been done for veri-
fying protocols under some equational theories, like Exclusive-Or. In [18] we list
protocols using algebraic properties by conception and protocols that are safe
without considering any algebraic property but that are ﬂawed if we consider
an algebraic property. All these examples are evidences that relaxing the perfect
encryption hypothesis by considering equational theories is an important issue
in security. In order to achieve this goal some tools have been developed for
considering some algebraic properties [7,8,22,27,28,43].
⋆This work was supported by ANR SeSur SCALP, SFINCS, AVOTE.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 173–185, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

174
P. Lafourcade, V. Terrade, and S. Vigier
Contribution:
We compare tools which are able to deal with algebraic prop-
erties for the veriﬁcation of cryptographic protocols. More speciﬁcally we look
at the Exclusive-Or property and the so-called Diﬃe-Hellman property, since
these are the most frequently used. In order to verify cryptographic protocols
using such properties we use CL-Atse and OFMC two tools of the platform
Avispa. Both CL-Atse and OFMC analyze protocols with such algebraic prop-
erties for a bounded number of sessions. On the other hand ProVerif can verify
cryptographic protocols for an unbounded number of session, but is not able to
deal with such properties. But, recently Kuesters et al proposed in [27,28] two
methods for transforming a protocol description using Exclusive-Or and Diﬃe-
Hellman property into a ProVerif input ﬁle. Hence we use these two “translators”
in order to compare the following three tools: CL-Atse, OFMC and ProVerif. Our
main contribution is to compare some protocols presented in [17] which are using
Exclusive-Or and Diﬃe-Hellman. We also compare the tools using a more com-
plex e-auction protocol [24]. We have chosen this protocol because it is longer
than protocols in [17] and uses Exclusive-Or.
We check secrecy and authentication properties for most of the protocols
under study. Moreover for the e-auction protocol, we analyze the non-repudiation
property, meaning that a participant cannot claim that he never did something.
The non-repudiation is a property that is often involved in e-commerce protocols,
because the seller or the bank usually do not want a customer to be allowed to
deny a transaction. For modelling the property of non-repudiation we follow the
approach of L. Vigneron et al [26] which expresses non-repudiation in term of
authentication. Using this method we are able to check this property with the
three tools in presence of algebraic properties.
State of the art: Some work exists on comparing the performance of diﬀerent
security protocol analysis tools. In [34] C. Meadows compares the approach used
by NRL [33] and the one used by G. Lowe in FDR [37] on the Needham-Schroeder
example [36]. The two tools are shown to be complementary even though NRL
is considerably slower. In [4], a large set of protocols is veriﬁed using the AVISS
tool and timing results are given. In [44], a similar test is performed using the
successor of AVISS, called the Avispa tool suite [2]. As the AVISS/Avispa tools
consist of respectively three and four back-end tools, these tests eﬀectively work
as a comparison between the back end tools. No conclusions about the relative
results are drawn in these articles. A qualitative comparison between Avispa and
Hermes [11] is presented in [25]. This test leads to some generic advice for users
of these two tools. It is not based on actual testing, but rather on conceptual
diﬀerences between the modeling approaches of the tools. In [13], a number
of protocol analysis tools are compared with respect to their ability to ﬁnd a
particular set of attacks.
Recently in [20], we proposed a fair comparison of the following tools:
Casper/FDR, ProVerif, Scyther and Avispa. We obtain for the ﬁrst time an
“eﬃciency ranking” of such tools. In this work we only looked at some proto-
cols without any algebraic property. Here we continue our investigations with
algebraic properties.

Comparison of Cryptographic Veriﬁcation Tools
175
Outline: In the next Section we describe brieﬂy the diﬀerent tools used. In
Section 3, for each analyzed protocol we present a short description and eventual
attacks found by the tools. Finally in the last Section, we conclude by discussing
our results obtained in Section 4.
2
Tools
In this section, we present the three tools used for our comparison. We have
chosen these tools because we have already compared them in [20] on a set
of protocols without algebraic properties and because they are dealing with the
two main algebraic properties used in cryptographic protocols: Exclusive-Or and
Diﬃe-Hellman.
Avispa
[2] (V: 1.1) Automated Validation of Iternet Security Protocols and
Applications consists of the following four tools: CL-Atse [43] developed by
M. Turuani, Sat-MC [3] created by A. Armando et al, OFMC [6] designed by
S. M¨odersheim, and TA4SP [10] proposed by Y. Boichut. All these tools take
the same input language called HLPSL (High Level Protocol Speciﬁcation Lan-
guage). We describe a little bit more the two tools OFMC and CL-Atse which
deal with selected algebraic properties. CL-Atse: (V: 2.2-5) Constraint-Logic-
based Attack Searcher applies constraint solving with simpliﬁcation heuristics
and redundancy elimination techniques [43]. OFMC (V: 2006/02/13) On-the-
Fly Model-Checker employs symbolic techniques to perform protocol falsiﬁca-
tion as well as bounded analysis, by exploring the state space in a demand-driven
way [6].
ProVerif [8,9] (V: 1.16) developed by B. Blanchet analyzes an unbounded num-
ber of sessions by using over-approximation and represents protocols by Horn
clauses. ProVerif accepts two kinds of input ﬁles: Horn clauses and a subset of
the Pi-calculus. The tool uses an abstraction of fresh nonce generation, enabling
it performs unbounded veriﬁcation for a class of protocols. It can handle many
diﬀerent cryptographic primitives (shared and public-key cryptographic, hash
functions...) and an unbounded number of sessions of the protocol. In ProVerif
it is possible to model any equational theory, but the tool might not termi-
nate. It is the case with Exclusive-Or or Diﬃe-Hellman exponentiation, however
commutativity of the exponentiation alone is supported by ProVerif. It allows
B. Blanchet’s tool to verify protocols that only use this particular algebraic
property of the exponentiation.
Recently
R.
K¨uster
and
T.
Truderung
proposed
two
new
“tools”
XOR-ProVerif [29] and DH-ProVerif [27]. These small programs transform a
protocol using Exclusive-Or and Diﬃe-Hellman mechanism into a protocol in
Horn clauses compatible with ProVerif. The input ﬁles for XOR-ProVerif and
DH-ProVerif are Prolog ﬁles. This allows us to compare ProVerif with CL-Atse
and OFMC for protocols using algebraic properties of Exclusive-Or and Diﬃe-
Hellman.

176
P. Lafourcade, V. Terrade, and S. Vigier
3
Results
In this section, we present the results obtained by implementing the selected
protocols in OFMC, CL-Atse and ProVerif. For our experiments we used a PC
DELL E4500 Intel dual Core 2.2 Ghz with 2 GB of RAM. Due to obvious reasons
all complete descriptions of protocols are not given here, but for each protocol
we try to present the minimum in order to clearly explain the results given by
the tools. In [30], we propose a short description and an implementation of each
analyzed protocols.
We ﬁrst present protocols dealing with Exclusive-Or and after the ones us-
ing Diﬃe-Hellman. All time measurements are given in Figure 1 and 2. In the
ProVerif column of these tables, we mention two numbers, the ﬁrst one corre-
sponds to the time of the transformation done by Kuesters et al’s tool and the
second one is the veriﬁcation time given by ProVerif. All our experiments are
accessible at this address http://www-verimag.imag.fr/~plafourc/FAST09.
tar.gz
Notations: We denote principals by A, B, S..., messages by Mi, nonces gener-
ated by A by NA, public keys of A by PKA, symmetric keys between A and B
by KAB, fresh symmetric keys by KA, a prime number by P, a primitive root
by G. The Exclusive-Or is denoted by A ⊕B. The exponentiation of G by the
nonce NA modulo P is denoted by GNA mod P.
3.1
Bull’s Authentication Protocol
This protocol [12, 38] aims at establishing fresh session keys between a ﬁxed
number of participants and a server. The protocol is recursive and uses one key
for each pair of agents adjacent in the chain. In our modelling, the protocol is
initiated by A and then goes through B and C before reaching S. At the end,
new session keys KAB and KBC are established. Each key KXY should be known
exactly by X and Y (and also S), even if other participants are malicious.
Results: The checked property is the secrecy of KAB between A and B and
the secrecy of KBC between B and C. We ﬁrst notice that OFMC is slower than
CL-Atse. The analysis using XOR-ProVerif crashes after more that one hour
and the size of partial output produced is more that 400 MB. This corresponds
to the fact that the algorithm proposed by Kuesters et al is exponential in the
number of variables used in Exclusive-Or and the number of constants used in
the protocol. This point demonstrates clearly the limit of the approach using
XOR-ProVerif.
So we restrict the cases considered in the XOR-ProVerif ﬁle in order that
the analysis end. It means that guided by the already known attack we ﬁx
some variables by the name of the principal according to the attack. This allows
XOR-ProVerif to generate in 5 seconds the input ﬁle for ProVerif. In this setting
ProVerif found the same attack as Avispa tools in 5 + 12 = 17 seconds.

Comparison of Cryptographic Veriﬁcation Tools
177
The attack found is the same as the one presented in the survey [18,38]. I is
part of the protocol since he plays the third role, and the diﬀerent steps of the
protocol take place normally. Yet, at the end of the protocol, I is able to retrieve
the key shared by A and B. Indeed, he had with step 4 KAB ⊕h(NB, KBS)
and KBI ⊕h(NB, KBS) with which he can compute KAB ⊕KBI. Moreover, he
had also obtained KBI, as it is the aim of the protocol, and with KAB ⊕KBI,
he can obviously retrieve KAB, that should be shared only by A and B. We
can see in this attack that the intruder uses the property of the Exclusive-Or:
X ⊕X = 0. Indeed, it permits him to eliminate the term h(NB, KBS) in order
to ﬁnd KAB ⊕KBI. We propose a new version of the protocol, to prevent the
third participant from using this property.
New Protocol: In this new version of the protocol, the idea was to introduce
a second nonce created by B, which permits to avoid the dual use of a unique
nonce on which lays on the attack. Here, S uses the ﬁrst nonce NB1 to encrypt
the key KAB and the second nonce NB2 to encrypt KBC. As a result, the attack
consisting in computing two parts intended for B to ﬁnd KAB ⊕KBI is no more
valid.
The analysis of this second version with the back end OFMC did not end
after more that 20 hours of computation, while the analysis with CL-Atse gave
no attack after 1h23. This result shows that small modiﬁcation can drastically
change the time of veriﬁcation. We can also notice that in this case and as we
have observed in the case without algebraic considerations, OFMC is slower than
CL-Atse. Finally XOR-ProVerif also crashes with this new protocol, but if we
ﬁx again some variables in XOR-ProVerif the transformation ends in 17 seconds
and the total analysis takes 2 minutes and 15 seconds.
3.2
Wired Equivalent Privacy Protocol
The Wired Equivalent Privacy protocol (WEP) [1], is used to protect data during
wireless transmission. To encrypt a message M for X, A applies the operator ⊕to
RC4(v, KAX) and [M, C(M)], where RC4 is a public algorithm using v an initial
vector and a symmetric key KAX, and C is an integrity checksum function. For
decrypting the received message, X computes RC4(v, KAX) and after applying
Exclusive-Or, he obtains [M, C(M)] and can verify that the checksum is correct.
Results: The property veriﬁed was the secrecy of M2 between A and B. All
the tools found quickly the same following attack:
0.1. A −→B :
v, ([M1, C(M1)] ⊕RC4(v, KAB))
0.2. A −→I :
v, ([M1, C(M1)] ⊕RC4(v, KAI))
1. A −→I :
v, ([M2, C(M2)] ⊕RC4(v, KAB))
First of all, A sends the same message M1 to B and I in steps 0.1 and 0.2. I is able
to determine RC4(v, KAI). Then, by computing ([M1, C(M1)] ⊕RC4(v, KAB)) ⊕
([M1, C(M1)] ⊕RC4(v, KAI)) ⊕RC4(v, KAI), I can reach RC4(v, KAB) and con-
sequently, he can access to every following messages intended for B. Indeed, in

178
P. Lafourcade, V. Terrade, and S. Vigier
step 1, the intruder intercepts ([M2, C(M2)]⊕RC4(v, KAB)) and he can compute
([M2, C(M2)] ⊕RC4(v, KAB)) ⊕RC4(v, KAB)), which is equal to [M2, C(M2)].
We have implemented a new version of this protocol, based on the changing
of the initial vector at every message sent. Like for the “Bull’s Authentication
Protocol”, it has permitted to prevent the intruder from retrieving RC4(v, KAB)
with the Exclusive-Or property, and to ensure secrecy for the messages reserved
to B. Avispa tools and ProVerif have indeed considered the new protocol safe in
less than 1 second.
3.3
Gong’s Mutual Authentication Protocol
The protocol [23] aims at providing mutual authentication and distributing a
fresh secret key K. It makes use of a trusted server S with which each of the
two agents A and B shares a secret password respectively PA and PB. As an
alternative to encryption algorithms, this protocol uses the one-way functions
f1,f2,f3 and g. The principal B can obtain, using the properties of Exclusive-Or,
the triple (K, HA, HB) from the message that he receives at step 3, and check
it by computing g(K, HA, HB, PB). Knowing PA and after receiving NS, A uses
the functions f1,f2 and f3 to get K, HA and HB. Hence, she can verify the
message HB sent by B at step 4 and sending the message HA to B in order to
prove her identity.
Results: Here, we checked the secrecy of the key created. It was declared safe by
CL-Atse and OFMC. This time OFMC with 19 seconds is faster than CL-Atse
with one minute and 34 seconds. For ProVerif, we have no result since converting
the XOR-ProVerif ﬁle to horn clauses returns “out of global stack”. Here also the
number of variables used in Exclusive-Or is important comparing to the other
protocols, which could explain the crash of the tool.
3.4
TMN
This is a symmetric key distribution protocol [32,42]. Indeed, here, the key KB
is given to A. For each session, the server veriﬁes that the keys KA and KB have
not been used in previous sessions.
Results: The same attack which is described below was found with ProVerif
and Avispa in less than one second. There exists another attack presented in
the survey (or see [40]) based on a property of encryption. It used the fact that
{X}P KS ∗{Y }P KS = {X ∗Y }P KS. Yet, the tools used does not seem to be able
to ﬁnd this attack, since they can not take into account such property.
1.
A −→S :
B, {KA}P KS
2.
S −→I :
A
3. I(B) −→S :
A, {KI}P KS
4.
S −→I :
B, KI ⊕KA

Comparison of Cryptographic Veriﬁcation Tools
179
In the ﬁrst step, A starts a normal session with B. In the second step, I intercepts
the message sent by S and then, in step 3, he impersonates B and sends his own
symmetric key to the server. Finally, the intruder intercepts B and KI ⊕KA
and as he knows KI, he can ﬁnd KA by computing (KI ⊕KA) ⊕KI. Finally, I
can transmit B, KI ⊕KA to A.
3.5
Salary Sum
This protocol [39] allows a group of people to compute the sum of their salaries
without anyone declaring his own salary to the others. For the sake of simplicity,
the protocol is only considered for four principals A, B, C and D. This protocol
uses addition, but OFMC can analyze this property with similar time results
as for the Exclusive-Or. Hence, in order to make the comparison we replace
addition by Exclusive-Or in the analyzed version.
Results: We veriﬁed the secrecy of all salaries, and we found diﬀerent attacks,
depending on the tool used. The attack found with Avispa tools is based on the
fact that I plays both the role of C and D:
1.
A −→B :
A, {NA ⊕SA}P KB
2.
B −→I :
B, {NA ⊕SA ⊕SB}P KI
3. I(B) −→C :
B, {NA ⊕SA ⊕SB}P KC
4.
C −→I :
C, {NA ⊕SA ⊕SB ⊕SC}P KI
In this attack, B believes he is doing the protocol with I in the third position
while C believes he is doing it with I in the fourth position. Indeed, in step 2, B
sends NA⊕SA⊕SB encrypted with PKI and in step 4, C sends NA⊕SA⊕SB⊕SC
encrypted with PKI too. Consequently, by subtracting these two numbers, I can
obviously reach SC, which should have remain secret.
Note that the ﬁrst implementation ends with XOR-ProVerif but this times
ProVerif does not terminate after more than six hours. Here we can see the
limitations of ProVerif. It is well known that for some protocols ProVerif does
not terminates. On the other side for the ﬁrst time on this example we can clearly
observe that OFMC is much more better than CL-Atse.
We also change a little bit the modeling by ﬁxing some agent in the XOR-
ProVerif input ﬁle. With this new version ProVerif terminates and ﬁnds an attack
in less than 1 + 11 = 12 seconds. The attack described below is very similar to
the attack of the survey. SA, SB, SC, SD are numbers representing salaries.
1.
A −→I :
A, {NA ⊕SA}P KI
2. I(D) −→A :
D, {NA ⊕SA}P KA
3.
A −→I :
SA
Here, I is the second participant of the protocol. Indeed, A sends him NA⊕SA as
expected. Then, contrary to the normal protocol, I impersonates D and sends to
A directly what he has received: NA ⊕SA. A, believing it comes from D, applies
again the Exclusive-Or with NA, and consequently sends SA to I, instead of the
sum of all the salaries. In the attack presented in the survey, I adds SI ⊕SI ⊕SI

180
P. Lafourcade, V. Terrade, and S. Vigier
to NA ⊕SA before sending it to A. Then, A sends him SA ⊕SI ⊕SI ⊕SI and
since he knows SI, he can retrieve SA. The principles of the two attacks are the
same, but the second version permits to prevent A from realizing it is his own
salary he received at step 3.
3.6
E-Auction
The H-T Liaw, W-S Juang and C-K Lin’s protocol [24] is an improvement of the
Subramanian’s protocol. It satisﬁes the requirements for electronic auction like
anonymity, privacy... but also adds the properties of non-repudiation, untrace-
ability, auditability, one time registration and unlinkability.
Results: We check the secrecy, the authentication and the non repudiation
with the three tools and all ﬁnd it secure in less than 1 second. This protocol
is composed of 13 exchanges of messages, but contains only two Exclusive-Or
operations. This conﬁrms the theory, meaning that the complexity for verifying
protocol with Exclusive-Or increases exponentially with the number of Exclusive-
Or operations.
3.7
Others
We compare the tools on the Exclusive-Or Needham-Schroeder protocol pro-
posed by Mathieu Turuani on the web site of Avispa. We also code the Three
Pass protocol proposed by R. Shamir et al. with the One Time Pad encryption
scheme (protocol described in [15]). In this situation there exists a simple attack
which consists of xoring all exchanged messages for recovering the secret. The
results are the same for the both protocols, all tools ﬁnd the attack in less than
one second.
3.8
Diﬃe-Hellman Key Exchange Protocol
In the protocol presented in [21], the initiator A ﬁrst chooses a prime number P
and a primitive root G of the group Z/PZ. He sends them with the exponenti-
ation of G by a fresh number NA and the responder does the same with a fresh
number NB. At the end, they share a common key which is the exponentiation
of G by NA and NB.This protocol has to guarantee the secrecy of the fresh key.
Results: The implementation of the protocol realized is the simpliﬁed version
of the one presented above. Indeed, in the ﬁrst step of the protocol, A sends to
B only GNA, and we consider that P and G were known by everybody. Here,
we can see that B (but also A) has no means to check authentication on the
three numbers received. ProVerif and Avispa give us in less than one second the
following well-known authentication attack:
1.
A −→I :
P, G, (GNA) mod P
2. I(A) −→B :
P, X1, X2
3.
B −→I(A) :
(XNB
1
) mod P

Comparison of Cryptographic Veriﬁcation Tools
181
3.9
IKA
The Initial Key Agreement [5] or also called the GDH.2 protocol, uses the same
idea as the Diﬃe-Hellman protocol but it is extended to an unlimited number of
agents. It aims at establishing a group key between a ﬁxed number of participants
X1,...,Xn. Each agent has a secret nonce Ni and at the end of the protocol, the
key shared between all principals is the exponentiation of G (primitive root of
the group Z/PZ where P is a prime number) by all the participant’s nonces.
Results: We found an attack with CL-Atse and OFMC in less than 2s. This
attack is similar to the attack on the Diﬃe-Hellman protocol. For DH-ProVerif
this protocol was already studied by Kuesters et al in [27]. As it is mentioned
in [27] a naive modelling of this protocol produces an input ﬁle for ProVerif,
which does not terminate. The situation is similar as the situation we found in
the Salary Sum with Exclusive-Or. The authors “used a technique inspired by the
one sometimes used in the process calculus mode for ProVerif when encoding
phases”. Hence they proposed two versions: one safe version for one session
Analyzed
Avispa
ProVerif
Protocols
OFMC
CL-Atse
XOR-ProVerif
UNSAFE
UNSAFE
No result
Bull [12]
secrecy attack
secrecy attack XOR-ProVerif
0.08 s
0.08 s
does not end
The analysis
SAFE
No result
Bull v2
does not end
XOR-ProVerif
time search: 20 h
1 h 10 min
does not end
UNSAFE
UNSAFE
UNSAFE
WEP [1]
secrecy attack
secrecy attack
secrecy attack
0.01 s
less than 0.01 s
less than 1 s
WEP v2
SAFE
SAFE
SAFE
0.01 s
less than 0.01 s
less than 1 s
Gong [23]
SAFE
SAFE
No result
19 s
1 min 34 s
does not end
UNSAFE
UNSAFE
UNSAFE
Salary Sum [39]
secrecy attack
secrecy attack
secrecy attack
0.45 s
11 min 16 s
does not end
UNSAFE
UNSAFE
UNSAFE
TMN [32,42]
secrecy attack
secrecy attack
secrecy attack
0.04 s
less than 0.01 s
less than 1 s
E-Auction [24]
SAFE
SAFE
SAFE
less than 1s
0.59 s
less than 1 s
3-Pass Shamir [15]
UNSAFE
UNSAFE
UNSAFE
less than 1s
less than 1s
less than 1 s
⊕Needham-Schroeder
UNSAFE
UNSAFE
UNSAFE
less than 1s
less than 1s
less than 1 s
Fig. 1. Results for protocols using XOR

182
P. Lafourcade, V. Terrade, and S. Vigier
Analyzed
Avispa
ProVerif
Protocols
OFMC
CL-Atse
DH-ProVerif
UNSAFE
UNSAFE
UNSAFE
D.H [21]
authentication
Survey authentication authentication
attack
attack
attack
0.01 s
less than 0.01 s
less than 1 s
UNSAFE
UNSAFE
UNSAFE
IKA [5]
authentication
authentication
1s+2min 33s
and secrecy attack
and secrecy attack
SAFE
less than 0.01 s
less than 0.01 s
3s + 1s
Fig. 2. Results for protocols using DH
(second line in the table of Figure 2) and one unsafe version for two sessions in
order to ﬁnd the well-known attack (ﬁrst line in the table of Figure 2). We adapt
the two versions given by the authors for 4 principals to 3 principals, in order to
analyze the same conﬁguration as the one used in the Avispa tools. We observe
that ProVerif is a little bit slower that the other tools, due to the translation
performed by XOR-ProVerif.
4
Conclusion and Discussion
In this paper we have compared time eﬃciency of cryptographic veriﬁcation
tools using Exclusive-Or and Diﬃe-Hellman properties. In Figure 1 and 2 we
sum up the results obtained with the diﬀerent tools for the studied protocols.
Globally, we found the same attacks with OFMC, CL-Atse, and XOR-ProVerif
or DH-ProVerif. Most of the time these attacks were identical to those of the
survey [18] except for Salary Sum and TMN. These exceptions are normal since
for the ﬁrst one we change the addition into Exclusive-Or and for the second one
any tool can deal with the homomorphism property used in the attack presented
in the survey.
For the Exclusive-Or property, it seems that when OFMC terminates it is
globally faster that CL-Atse. But for protocols using a large number of Exclusive-
Or operations, e.g. for instance in the Bull’s protocol, OFMC does not termi-
nates whereas CL-Atse does. The diﬀerence between the Bull’s protocol and the
E-auction’s protocol shows clearly that the number of Exclusive-Or used in a
protocol is the parameter which increases veriﬁcation time. This conﬁrms that
complexity is exponential in the number of Exclusive-Or. This also explains the
failure of XOR-ProVerif in this situation. On the other hand, if the number of
variables and constants is not too large ProVerif is very eﬃcient and faster that
Avispa tools. Finally, for some protocols, such as modiﬁed version of the Salary
Sum for ProVerif or the improved version of Bull’s protocol for OFMC the tools
were not able to end the analysis in a limited period of time.
For Diﬃe-Hellman property, all protocols were analyzed quickly by all the
tools. This conﬁrms the polynomial complexity of DH-ProVerif and the fact
that this equational theory is less complex than Exclusive-Or.

Comparison of Cryptographic Veriﬁcation Tools
183
Indeed, the use of variables with Exclusive-Or or “exponentiation” seems to
increase rapidly the search time of the tools, especially for XOR-ProVerif, but
also for OFMC and CL-Atse.
Future work: Recently a new version of OFMC has been proposed in the project
AVANTSSAR. A new version of TA4SP is also announced on the website of
the author, this new version deals with some algebraic properties including
Exclusive-Or and Diﬃe-Hellman. In the future we plan to check the same pro-
tocols with this new version of OFMC and also to include the new version of
TA4SP in our study. Moreover we would like to see if the new OFMC is more
eﬃcient than its older version. We also would like to include the tool Maude
NPA [22] in our analysis. This tool uses rewriting techniques for proving secu-
rity of cryptographic protocols in presence of equational theories. Our ﬁsrt test
shows on DH protocol that Maude takes around 6 minutes instead of less than
one second for all the other tools. Moreover in [14] the authors propose an im-
proved version of XOR-ProVerif, we would like to test this new version. Finally
we project to continue this preliminary analysis in a fair way as we did in [20].
References
1. IEEE 802.11 Local and Metropolitan Area Networks: Wireless LAN Medium Acess
Control (MAC) and Physical (PHY) Speciﬁcations (1999)
2. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Drielsma, P.H., He´am, P.-C., Kouchnarenko, O., Mantovani, J., M¨odersheim, S.,
von Oheimb, D., Michael, R., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.:
The AVISPA tool for the automated validation of internet security protocols and
applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576,
pp. 281–285. Springer, Heidelberg (2005)
3. Armando, A., Compagna, L.: An optimized intruder model for SAT-based model-
checking of security protocols. In: Armando, A., Vigan`o, L. (eds.) ENTCS, March
2005, vol. 125, pp. 91–108. Elsevier Science Publishers, Amsterdam (2005)
4. Armando, A., Basin, D.A., Bouallagui,
M., Chevalier, Y., Compagna, L.,
M¨odersheim, S., Rusinowitch, M., Turuani, M., Vigan`o, L., Vigneron, L.: The aviss
security protocol analysis tool. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002.
LNCS, vol. 2404, pp. 349–353. Springer, Heidelberg (2002)
5. Ateniese, G., Steiner, M., Tsudik, G.: New multiparty authentication services and
key agreement protocols. IEEE Journal of Selected Areas in Communications 18(4),
628–639 (2000)
6. Basin, D., M¨odersheim, S., Vigan`o, L.: An On-The-Fly Model-Checker for Security
Protocol Analysis. In: Snekkenes, E., Gollmann, D. (eds.) ESORICS 2003. LNCS,
vol. 2808, pp. 253–270. Springer, Heidelberg (2003)
7. Basin, D.A., M¨odersheim, S., Vigan`o, L.: Ofmc: A symbolic model checker for
security protocols. Int. J. Inf. Sec. 4(3), 181–208 (2005)
8. Blanchet, B.: An eﬃcient cryptographic protocol veriﬁer based on prolog rules. In:
Proc. CSFW 2001, pp. 82–96. IEEE Comp. Soc. Press, Los Alamitos (2001)
9. Blanchet, B.: Cryptographic Protocol Veriﬁer User Manual (2004)
10. Boichut, Y., H´eam, P.-C., Kouchnarenko, O., Oehl, F.: Improvements on the Genet
and Klay technique to automatically verify security protocols. In: Proc. AVIS 2004
(April 2004)

184
P. Lafourcade, V. Terrade, and S. Vigier
11. Bozga, L., Lakhnech, Y., Perin, M.: HERMES: An Automatic Tool for Veriﬁcation
of Secrecy in Security Protocols. In: Computer Aided Veriﬁcation (2003)
12. Bull,
J.,
Otway,
D.J.:
The
authentication
protocol.
Technical
Report
DRA/CIS3/PROJ/CORBA/SC/1/CSM/436-04/03,
Defence
Research
Agency
(1997)
13. Cheminod, M., Cibrario Bertolotti, I., Durante, L., Sisto, R., Valenzano, A.: Ex-
perimental comparison of automatic tools for the formal analysis of cryptographic
protocols. In: DepCoS-RELCOMEX 2007, Szklarska Poreba, Poland, June 14-16,
pp. 153–160. IEEE Computer Society Press, Los Alamitos (2007)
14. Chen, X., van Deursen, T., Pang, J.: Improving automatic veriﬁcation of security
protocols with xor. In: Cavalcanti, A. (ed.) ICFEM 2009. LNCS, vol. 5885, pp.
107–126. Springer, Heidelberg (2009)
15. Clark, J., Jacob, J.: A survey of authentication protocol literature (1997),
http://www.cs.york.ac.uk/~jac/papers/drareviewps.ps
16. Corin, R., Etalle, S.: An improved constraint-based system for the veriﬁcation of
security protocols. In: Hermenegildo, M.V., Puebla, G. (eds.) SAS 2002. LNCS,
vol. 2477, pp. 326–341. Springer, Heidelberg (2002)
17. Cortier, V., Delaune, S., Lafourcade, P.: A survey of algebraic properties used in
cryptographic protocols. Journal of Computer Security 14(1), 1–43 (2006)
18. Cortier, V., Delaune, S., Lafourcade, P.: A survey of algebraic properties used in
cryptographic protocols. Journal of Computer Security 14(1), 1–43 (2006)
19. Cremers, C.J.F.: The Scyther Tool: Veriﬁcation, falsiﬁcation, and analysis of se-
curity protocols. In: Gupta, A., Malik, S. (eds.) CAV 2008. LNCS, vol. 5123, pp.
414–418. Springer, Heidelberg (2008)
20. Cremers, C.J.F., Lafourcade, P., Nadeau, P.: Comparing state spaces in automatic
protocol analysis. In: Cortier, V., Kirchner, C., Okada, M., Sakurada, H. (eds.)
Formal to Practical Security. LNCS, vol. 5458, pp. 70–94. Springer, Heidelberg
(2009)
21. Diﬃe, W., Hellman, M.: New directions in cryptography. IEEE Transactions on
Information Society 22(6), 644–654 (1976)
22. Escobar, S., Meadows, C., Meseguer, J.: Maude-npa: Cryptographic protocol anal-
ysis modulo equational properties. In: Aldini, A., Barthe, G., Gorrieri, R. (eds.)
FOSAD. LNCS, vol. 5705, pp. 1–50. Springer, Heidelberg (2007)
23. Gong, L.: Using one-way functions for authentication. SIGCOMM Computer Com-
munication 19(5), 8–11 (1989)
24. Liaw, H.-T., Juang, W.-S., Lin, C.-K.: An electronic online bidding auction protocol
with both security and eﬃciency. Applied mathematics and computation 174, 1487–
1497 (2008)
25. Hussain, M., Seret, D.: A comparative study of security protocols validation tools:
HERMES vs. AVISPA. In: Proc. ICACT 2006, vol. 1, pp. 303–308 (2006)
26. Klay, F., Vigneron, L.: Automatic methods for analyzing non-repudiation protocols
with an active intruder. In: Degano, P., Guttman, J., Martinelli, F. (eds.) FAST
2008. LNCS, vol. 5491, pp. 192–209. Springer, Heidelberg (2009)
27. K¨usters, R., Truderung, T.: Using ProVerif to Analyze Protocols with Diﬃe-
Hellman Exponentiation. In: Proceedings of the 22nd Computer Security Foun-
dations Symposium (CSF), pp. 157–171. IEEE Computer Society, Los Alamitos
(2009)
28. K¨usters, R., Truderung, T.: Reducing protocol analysis with xor to the xor-free
case in the horn theory based approach. In: Ning, P., Syverson, P.F., Jha, S. (eds.)
ACM Conference on Computer and Communications Security, pp. 129–138. ACM,
New York (2008)

Comparison of Cryptographic Veriﬁcation Tools
185
29. K¨usters, R., Truderung, T.: Reducing protocol analysis with xor to the xor-free
case in the horn theory based approach. In: ACM Conference on Computer and
Communications Security, pp. 129–138 (2008)
30. Lafourcade, P., Terrade, V., Vigier, S.: Comparison of cryptographic veriﬁcation
tools dealing with algebraic properties. Technical Report TR-2009-16, Verimag
(October 2009)
31. Lowe, G.: Casper: a compiler for the analysis of security protocols. J. Comput.
Secur. 6(1-2), 53–84 (1998)
32. Lowe, G., Roscoe, A.W.: Using CSP to detect errors in the TMN protocol. IEEE
Transactions on Software Engineering 23(10), 659–669 (1997)
33. Meadows, C.: Language generation and veriﬁcation in the NRL protocol analyzer.
In: Proc. CSFW 1996, pp. 48–62. IEEE Comp. Soc. Press, Los Alamitos (1996)
34. Meadows, C.: Analyzing the needham-schroeder public-key protocol: A comparison
of two approaches. In: Martella, G., Kurth, H., Montolivo, E., Bertino, E. (eds.)
ESORICS 1996. LNCS, vol. 1146, pp. 351–364. Springer, Heidelberg (1996)
35. Mitchell, J.C., Mitchell, M., Stern, U.: Automated analysis of cryptographic pro-
tocols using Murphi. In: IEEE Symposium on Security and Privacy (May 1997)
36. Needham, R., Schroeder, M.: Using encryption for authentication in large networks
of computers. Communication of the ACM 21(12), 993–999 (1978)
37. Roscoe, A.W.: Model-checking CSP. Prentice-Hall, Englewood Cliﬀs (1994)
38. Ryan, P.Y.A., Schneider, S.A.: An attack on a recursive authentication protocol.
a cautionary tale. Inf. Process. Lett. 65(1), 7–10 (1998)
39. Schneier, B.: Applied Cryptography, 2nd edn. Wiley, Chichester (1996)
40. Simmons, G.J.: Cryptoanalysis and protocol failures. Communications of the
ACM 37(11), 56–65 (1994)
41. Song, D., Berezin, S., Perrig, A.: Athena: A novel approach to eﬃcient automatic
security protocol analysis. Journal of Computer Security 9(1/2), 47–74 (2001)
42. Tatebayashi, M., Matsuzaki, N., Newman, D.B.: Key distribution protocol for dig-
ital mobile communication systems. In: Brassard, G. (ed.) CRYPTO 1989. LNCS,
vol. 435, pp. 324–334. Springer, Heidelberg (1990)
43. Turuani, M.: The CL-Atse Protocol Analyser. In: Pfenning, F. (ed.) RTA 2006.
LNCS, vol. 4098, pp. 277–286. Springer, Heidelberg (2006)
44. Vigan`o,
L.: Automated security protocol analysis
with the AVISPA tool.
ENTCS 155, 61–86 (2006)

Game-Based Veriﬁcation of Multi-Party
Contract Signing Protocols
Ying Zhang1,2, Chenyi Zhang1, Jun Pang1, and Sjouke Mauw1
1 University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, L-1359 Luxembourg
2 Shandong University, Jinan, 250101 China
Abstract. A multi-party contract signing (MPCS) protocol is used for a
group of signers to sign a digital contract over a network. We analyse the
protocols of Mukhamedov and Ryan (MR), and of Mauw, Radomirovi´c
and Torabi Dashti (MRT), using the ﬁnite-state model checker Mocha.
Mocha allows for the speciﬁcation of properties in alternating-time tem-
poral logic (ATL) with game semantics, and the model checking problem
for ATL requires the computation of winning strategies. This gives us an
intuitive interpretation of the veriﬁcation problem of crucial properties of
MPCS protocols. We analyse the MR protocol with up to 5 signers and
our analysis does not reveal any ﬂaws. MRT protocols can be generated
from minimal message sequences, depending on the number of signers.
We discover an attack in a published MRT protocol with 3 signers, and
present a solution for it. We also design a number of MRT protocols
using minimal message sequences for 3 and 4 signers, all of which have
been model checked in Mocha.
1
Introduction
The goal of a multi-party contract signing (MPCS) protocol is to allow a number
of parties to sign a digital contract over a network. Such a protocol is designed
as to ensure that no party is able to withhold his signature after having received
another party’s signature. A simple way to achieve this is to involve a trusted
third party (T ). This trusted third party simply collects the signatures of all
signers and then distributes them to all parties. A major drawback of this ap-
proach is that the trusted third party easily becomes a bottleneck, since it will
be involved in all communications for all contracts. This problem is addressed
by the introduction of, so-called, optimistic multi-party contract signing proto-
cols [1]. The idea is that involvement of the trusted third party is only required
if something goes wrong, e.g. if one of the parties tries to cheat or if a non-
recoverable network error occurs. If all parties and the communication network
behave correctly, which is considered the optimistic case, the protocol terminates
successfully without intervention of the trusted third party.
MPCS protocols are supposed to satisfy three properties: fairness, abuse-
freeness and timeliness. Fairness means that each signer who sends out his sig-
nature has a means to receive all the other signers’ signatures. Abuse-freeness
guarantees that no signer can prove to an outside observer that he is able to
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 186–200, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
187
determine the result of the protocol. Timeliness ensures that each signer has the
capability to end inﬁnite waiting.
Several optimistic contract signing protocols have been proposed, most of
which only focus on the special case of two parties [2,3]. In 1999, Garay and
Mackenzie proposed the ﬁrst optimistic contract signing protocol [4] with multi-
ple parties, which we call the GM protocol. Chadha, Kremer and Scedrov found
a ﬂaw in the GM protocol for n ≥4, where n is the number of signers. They
revised the GM protocol by modifying one of its sub-protocols and proposed a
ﬁxed protocol [5] in 2004 (which we call the CKS protocol).
Mukhamedov and Ryan later showed that the CKS protocol fails to satisfy
the fairness property for n ≥5 by giving a so-called abort-chaining attack. They
proposed a ﬁxed protocol [6] in 2008 based on the CKS protocol (which we call
the MR protocol). Mukhamedov and Ryan proved that their protocol satisﬁes
fairness and claimed that it satisﬁes abuse-freeness and timeliness as well. They
also gave a formal analysis of fairness in the NuSMV model checker for 5 signers.
Using the notion of abort-chaining attacks, Mauw, Radomirovi´c and Torabi
Dashti analysed the message complexity of MPCS protocols [7]. Their results
made it feasible to construct MPCS protocols excluding abort-chaining attacks
but with minimal messages, which we call the MRT protocols, based on so-called
signing sequences. They also gave an example protocol with 3 signers. However,
they only provided a veriﬁcation of the protocol at a conceptual level.
In this paper, we follow the approach of Chadha, Kremer and Scedrov [5] to
model check the two recently proposed protocols, the MR and MRT protocols,
in Mocha [8]. Mocha can be used to model check properties speciﬁed in ATL [9].
This allows us to have a precise and natural formulation of desired properties
of contract signing, as the model checking problem for ATL requires the com-
putation of winning strategies. We model the MR protocol with up to 5 signers
and verify both fairness and timeliness properties, while Mukhamedov and Ryan
only analysed fairness of their protocol with 5 signers.
We clarify how to construct an MRT protocol from a minimal signing se-
quence. According to this methodology, we design a number of MRT protocols
for 3 and 4 signers, all of which have been model checked in Mocha. In particular,
we discover a fairness attack on the published MRT protocol with 3 signers [7]
and we present a solution to it. The ﬁxed protocol is shown to satisfy fairness.
2
Preliminaries
This section describes the basic structure of an optimistic contract signing pro-
tocol with its underlying assumptions. A few cryptographic primitives are em-
ployed in such protocols which we only brieﬂy introduce. We also explain the
security requirements associated with MPCS protocols.
2.1
Basic Notions
An optimistic MPCS protocol generally involves a group of signers P1, . . . , Pn,
who want to sign a contract monitored by a trusted third party T . A signer

188
Y. Zhang et al.
may be honest and thus strictly follow the protocol, or he may be dishonest
and deviate from the protocol in order to collude with other dishonest signers
to get undesirable advantages over the remaining signers. The structure of a
protocol consists of a main protocol and one or several sub-protocols. The main
protocol is executed by signers to exchange their promises at diﬀerent levels and
signatures without the intervention from the trusted third party T . The sub-
protocols, which usually include an abort protocol and a resolve protocol, are
launched by a user Pi on contacting T to deal with awry situations.
Once having contacted T by initiating a sub-protocol, the signers would never
be allowed to proceed the main protocol any more. T makes a decision on basis
of the information contained in a request provided by a signer as well as all
previous requests that have been sent by other participants. A request consists
of the promises that the requesting signer has received so far, serving as a clue for
T to judge the signer’s position in the current protocol execution. On making a
decision, T presumes that all the signers are honest, unless the received requests
contradict, showing that someone has lied. A reply from T can be either an abort
conﬁrmation or a contract signed by all the participants. After T has sent an
abort reply, she may later overturn that abort and reply with a signed contract
to subsequent requests if T detects that all the signers who have previously
contacted T are dishonest.1 However, once T has sent a signed contract, she will
have to stick to that decision for all subsequent requests. Without launching a
sub-protocol, a signer Pi quits a protocol if he simply follows the main protocol
till the end. Otherwise, Pi quits the protocol once a reply from T is received.
An important assumption of optimistic contract signing protocols is that all
communication channels between the signers and the trusted third party are
resilient, which means that messages sent over the channels are guaranteed to
be delivered eventually.
2.2
Cryptographic Primitives
An optimistic MPCS protocol usually employs zero-knowledge cryptographic
primitives, private contract signatures (PCS) [4]. We write PCSPi((c, τ), Pj, T )
for a promise made by Pi to Pj (i ̸
= j) on contract c at level τ, where τ indicates
the current level of a protocol execution where Pi makes the promise. A promise
is assumed to have the following properties.
– PCSPi((c, τ), Pj, T ) can only be generated by Pi and Pj.
– Only Pi, Pj and T can verify PCSPi((c, τ), Pj, T ).
– PCSPi((c, τ), Pj, T ) can be transformed into Pi’s signature only by Pi and T .
Intuitively, PCSPi((c, τ), Pj, T ) acts as a promise by Pi to Pj to sign the con-
tract c at level τ. However, the properties guarantee that Pj cannot use it to
prove to anyone except T that he has this promise. This is essential to achieve
abuse-freeness for MPCS protocols. Since these properties suﬃciently describe
the purpose and use of this primitive, we will not discuss its implementation.
1 Otherwise T’s overturn decision may impair fairness of an honest signer who has
previously received an abort reply.

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
189
2.3
Desirable Properties
All contract signing protocols are expected to satisfy three security properties [6],
viz. fairness, abuse-freeness and timeliness.
Fairness. At the end of the protocol, either each honest signer gets all the others’
signatures, or no signer gets any signature. Fairness ensures that no signer can
get any valuable information without sending out his signature, and once a
signer sends out his signature, he will eventually get all the others’ signatures.
An abort chaining [6] is a sequence of abort and resolve messages to T in a
particular order, such that it enforces T to return an abort reply to an honest
user who has already sent out his signature. Abort-chaining attacks are a major
challenge to fairness, and were instrumental to deriving the resolve-impossibility
result for a trusted third party for a certain class of MPCS protocols [6].
Abuse-freeness. At any stage of the protocol, any set of signers are unable to
prove to an outside observer that they have the power to choose between aborting
the protocol and getting the signature from another signer who is honest and
optimistically participating in the protocol. Intuitively, a protocol not being
abuse-free implies that some of the signers have an unexpected advantage over
other signers, and therefore they may enforce others to compromise on a contract.
Timeliness. Each signer has a solution to prevent endless waiting at any time.
That means no signer is able to force anyone else to wait forever.
3
Formal Model
In this section, we discuss how to model protocols in Mocha using a concurrent
game structure, and how to express speciﬁcations for the desired properties in
alternating-time temporal logic (ATL) with game semantics. We start with the
introduction of concurrent game structures and ATL [9].
3.1
Concurrent Game Structures and ATL
A (concurrent) game structure is a tuple S = ⟨k, Q, Π, π, d, δ⟩with components:
– k ∈N+ is the number of players, identiﬁed with the numbers 1, . . . , k.
– Q is a ﬁnite set of states.
– Π is a ﬁnite set of propositions.
– π : Q →2Π is a labeling function. For each state q ∈Q, a set π(q) ⊆Π of
propositions are true.
– d : {1, . . . , k} × Q →N+. da(q) represents the number of available moves for
player a ∈{1, . . . , k} at state q ∈Q. We identify the moves of player a at
state q with the numbers 1, . . . , da(q).
– δ is a transition function. For each q ∈Q and each move vector ⟨j1, . . . , jk⟩,
δ(q, j1, . . . , jk) is the state that results from q if every player a ∈{1, . . . , k}
chooses move ja ≤da(q).

190
Y. Zhang et al.
The temporal logic ATL (Alternating-time Temporal Logic) is deﬁned with re-
spect to a ﬁnite set Π of propositions and a ﬁnite set Σ = {1, . . . , k} of players.
An ATL formula is one of the following:
– p for propositions p ∈Π.
– ¬φ or φ1 ∨φ2, where φ, φ1, and φ2 are ATL formulas.
– ⟨⟨A⟩⟩ φ, ⟨⟨A⟩⟩φ, or ⟨⟨A⟩⟩φ1Uφ2, where A⊆Σ is a set of players, and φ, φ1
and φ2 are ATL formulas.
We interpret ATL formulas over the states of a concurrent game structure S
that has the same propositions and players. The labeling of the states of S
with propositions is used to evaluate the atomic formulas of ATL. The logical
connectives ¬ and ∨have the standard meaning.
In order to give the deﬁnition of the semantics of ATL, we ﬁrst give the
notion of strategies. Consider a game structure S = ⟨k, Q, Π, π, d, δ⟩. A strategy
for player a ∈Σ is a mapping fa : Q+ →N such that λ is a non-empty
ﬁnite state sequence and fa(λ) ≤da(q) if λ’s last state is q. In other words, a
strategy fa represents a set of computations that player a can enforce. Hence,
FA = {fa | a ∈A} induces a set of computations that all the players in A can
cooperate to enforce. Given a state q ∈Q, out(q, FA) is the set of computations
enforced by the set of players A applying strategies in FA. Write λ[i] for the i-th
state in the sequence λ starting from 0.
We are now ready to give the semantics of ATL. We write S, q |= φ to indicate
that the state q satisﬁes the formula φ in the structure S. And if S is clear from
the context we can omit S and write q |= φ. The satisfaction relation |= is deﬁned
for all states q of S inductively as follows:
– q |= p, for propositions p ∈Π, iﬀp ∈π(q).
– q |= ¬φ iﬀq ̸
|= φ.
– q |= φ1 ∨φ2 iﬀq |= φ1 or q |= φ2.
– q |= ⟨⟨A⟩⟩ φ iﬀthere exists a set FA of strategies, one for each player in A,
such that for all computations λ ∈out(q, FA), we have λ[1] |= φ.
– q |= ⟨⟨A⟩⟩φ iﬀthere exists a set FA of strategies, one for each player in A,
such that for all computations λ ∈out(q, FA) and for all positions i ≥0, we
have λ[i] |= φ.
– q |= ⟨⟨A⟩⟩φ1Uφ2 iﬀthere exists a set FA of strategies, one for each player
in A, such that for all computations λ ∈out(q, FA), there exists a position
i ≥0 such that λ[i] |= φ2 and for all positions 0 ≤j < i, we have λ[j] |= φ1.
Note that φ can be deﬁned as true Uφ. The logic ATL generalises Computation
Tree Logic (CTL) [10] on game structures, in that the path quantiﬁers of ATL
are more general: the existential path quantiﬁer ∃of CTL corresponds to ⟨⟨Σ⟩⟩,
and the universal path quantiﬁer ∀of CTL corresponds to ⟨⟨∅⟩⟩.
3.2
Modelling MPCS Protocols in Mocha
Mocha [8] is an interactive veriﬁcation environment for the modular and hierar-
chical veriﬁcation of heterogeneous systems. Its model framework is in the form

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
191
of reactive modules [11]. The states of a reactive module are determined by vari-
ables and are changed in a sequence of rounds. Mocha can check ATL formulas,
which express properties naturally as winning strategies with game semantics.
This is the main reason we choose Mocha as our model checker in this work.
Mocha provides a guarded command language to model the protocols, which
uses the concurrent game structures as its formal semantics. The syntax and
semantics of this language can be found in [8]. Intuitively, each player a ∈Σ
conducts a set of guarded commands in the form of guardξ →updateξ. The
update step is executed by each player choosing one of its commands whose
boolean guard evaluates to true. The next state combines the outcomes of the
guarded commands chosen by the players.
We now describe how to model MPCS protocols in detail, following [5]. Each
participant is modelled as a player using the above introduced guarded command
language. In order to model that a player could be either honest or malicious,
for each player Pi we build a process PiH, which honestly follows the steps of
his role in the protocol, and another process Pi, which is allowed to cheat. An
honest signer only sends out a message when the required messages according
to the protocol are received, i.e., he faithfully follows the protocol all the time.
A dishonest signer may send out a message if he gets enough information for
generating the message. He can even send messages after he is supposed to stop.
The trusted third party T is modelled to be honest throughout the time. We
express the communicational messages as shared boolean variables. The variables
are false by default and set to true when they are sent out by the signers.
For signers Pi and Pj, a variable Pi Sj represents that Pi has got Pj’s sig-
nature. Since Pi continues to hold Pj’s signature once Pi gets it, we model that
once Pi Sj is set to true its value would never be changed thereafter. For each
Pi, a variable Pi stop models whether signer Pi has quitted the protocol. Since
¬Pi stop is one of conditions within each Pi’s guarded command, Pi would never
change any of its variables once Pi stop is set to true. The integer Pr i j L = τ
represents that Pi has sent out his τ-th level promise to Pj. In particular for
MRT protocols, the integer Pr i k j L = τ represents that Pi has forwarded
Pk’s τ-th level promise to Pj. All Mocha models can be found at [12].
3.3
Expressing Properties of MPCS Protocols in ATL
We formalise both fairness and timeliness as in [5].
Fairness. A protocol is fair for signer Pi can be expressed as: if any signer obtains
Pi’s signature, then Pi has a strategy to get all the others’ signatures. In ATL,
it can be formalised as follows:
fairnessPi ≡∀ ((

1≤j̸=i≤n
Pj Si) ⇒⟨⟨PiH⟩⟩ (

1≤j̸=i≤n
Pi Sj)).
Timeliness. At any time, every signer has a strategy to prevent endless waiting.
Signer Pi’s timeliness is expressed as:
timelinessPi ≡∀ (⟨⟨PiH⟩⟩ Pi stop).

192
Y. Zhang et al.
Chadha, Kremer and Scedrov also gave an invariant formulation of fairness for
Pi as follows:
invfairnessPi ≡∀ (Pi stop ⇒((

1≤j̸=i≤n
Pj Si) ⇒(

1≤j̸=i≤n
Pi Sj)))
They proved that if a contract signing protocol interpreted as a concurrent game
structure satisﬁes timelinessPi for Pi then the protocol satisﬁes fairnessPi iﬀit
satisﬁes invfairnessPi [5, Thm. 3].2
4
Model Checking the MR Protocol
In this section, we give the description of the MR protocol [6] proposed by
Mukhamedov and Ryan. We build models using a program for any number n of
signers, and model check both fairness and timeliness for the models with up to
5 signers in Mocha.
4.1
Description of the MR Protocol
The MR protocol is based on PCSs and consists of one main protocol, one abort
sub-protocol and one resolve sub-protocol.
Main protocol. The main protocol consists of ⌈n/2⌉+ 1 rounds for n signers,
and requires n(n −1)(⌈n/2⌉+ 1) messages for the optimistic execution. In each
round τ (τ ≤⌈n/2⌉), a signer Pi starts with waiting for the τ-level promises from
lower signers Pj (j < i). After receiving all the lower signers’ promises, he sends
out his τ-level promise to all the higher signers Pk (k > i) and then waits for the
promises from higher signers. On receipt of all higher signers’ promises, Pi then
sends out his own τ-level promise to lower signers and ﬁnishes his current round.
If Pi has received the (⌈n/2⌉+ 1)-th level promises and signatures from all the
lower signers, he broadcasts his (⌈n/2⌉+ 1)-th level promise and signature to all
the other signers.
If Pi does not receive all the expected messages, he may quit the protocol, or
send an abort or a resolve request to the trusted third party T , according to his
current position in the main protocol. The abort request has the following form:
SPi((c, Pi, (P1, . . . , Pn), abort))
The resolve request is as follows:
SPi({PCSPj((m, τj), Pi, T )}j∈{1,...,n}\{i}, SPi(m, 0))
where for j > i, τj is the maximal level promise received from all the signers Pj′
such that j′ > j; for j < i, τ is the maximal level promise received from all the
signers Pj′ such that j′ < j.
2 For MRT protocols with 4 signers, we verify invfairnessPi instead of fairnessPi on
their Mocha models after we have successfully checked timeliness for Pi.

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
193
τj=

max{τ | ∀j′ > i, Pi has received PCSPj′ ((m,τ),Pi,T )}
if j > i
max{τ | ∀j′ < i, Pi has received PCSPj′ ((m,τ),Pi,T )}
if j < i
Sub-protocols. T maintains a boolean variable validated to indicate whether
T has ever replied with a full signed contract. T uses a set S(c) to record all
the signers which have contacted T and received an abort reply. T also controls
two variables hi(c) and ℓi(c) for each Pi to record Pi’s executing position at the
moment Pi contacts T . The variable hi(c) indicates the highest level promise Pi
has sent to all the signers Pj where j > i, and ℓi(c) indicates the highest level
promise Pi has sent to all the signers Pj where j < i.
Abort sub-protocol. When receiving an abort request from Pi, T ﬁrst checks if
she has ever sent a signed contract. If not, i.e., validated is false, T adds i into
S(c), sends Pi an abort reply, and stores the reply. Besides, T sets hi(c) = 1 and
ℓi(c) = 0. Otherwise, T sends a signed contract to Pi.
Resolve sub-protocol. When receiving a resolve request form Pi, T checks if it is
the ﬁrst request she has ever received. If it is, T simply replies Pi with a signed
contract and sets validated to true. T judges Pi’s current execution position and
updates hi(c) and ℓi(c) according to that position. If it is not the ﬁrst request,
T checks if she has ever sent a signed contract by checking if validated is true.
(1) If yes, T sticks to the decision and replies Pi with a signed contract and
updates hi(c) and ℓi(c). (2) If not, that means T has ever replied an abort to
some signer. In order to make a decision on whether to stick to the abort or to
overturn it, T checks if it is the case that all signers in S(c) are cheating. That
is, for each j ∈S(c), T compares τj from Pi’s request and hj(c) and ℓj(c) from
T ’s record to check if Pj continues the main protocol after receiving an abort
reply. If all j ∈S(c) are dishonest, T overturns her abort decision, and replies
Pi with a signed contract, at the same time updating hi(c) and ℓi(c). Otherwise,
T sticks to her abort reply to Pi and updates hi(c) and ℓi(c).
4.2
Automatic Analysis
We now give the analysis results of the MR protocol. We have veriﬁed fairness
and timeliness of the MR protocol with 2, 3 and 4 signers, and our analysis did
not reveal any ﬂaw.
In our analysis of the MR protocol with 5 signers, timeliness can be checked
in Mocha. While for fairness, it seems infeasible for Mocha to verify. So instead
of building one entire model covering all possible behaviours, we built a number
of speciﬁc models, each of which focuses on one certain possible abort-chaining
attack scenario. For instance, in order to check if there is an abort-chaining
attack in which P1 aborts ﬁrst and cooperates with P2, P3 and P4 to get honest
P5’s signature, we built a model where only P5 is honest, P1 will abort ﬁrstly
and the other dishonest signers only resolve rather than abort. In this way, we
reduce the size of the state space signiﬁcantly when checking fairness.3
3 In the future, we want to apply this technique to even larger protocol instances.

194
Y. Zhang et al.
We explain that the above checks can cover all possible abort-chaining scenar-
ios (in case of 5 signers). As mentioned in Sect. 2.3, an abort-chaining attack is
achieved by collaborative malefactors to enforce an aborting outcome after they
obtain the honest signer’s signature. We use Mi (i ∈N) to indicate a set consist-
ing of dishonest signers. Intuitively, if M1 ⊆M2, and M2 is not able to achieve an
abort-chaining attack, then neither is M1. So for the MR protocol with 5 signers,
we only need to check scenarios where one of the signers is the victim, and all
the other 4 signers are dishonest and collude to cheat. If there does not exist
an abort-chaining attack for such scenario, then there does not exist an abort-
chaining attack for a scenario with fewer dishonest signers. Since each abort-
chaining attack starts with some dishonest signer contacting T with an abort
reply, and ends up with the victim signer sending out his signature, we choose
one signer to abort in our model, and choose another signer to be the victim
from the last 4 signers. For the MR protocol with 5 signers, only signers P1, P2,
P3 and P4 have the possibility to abort. We use iAjH(i ∈[1, 4], j ∈[1, 5], i ̸
= j)
to indicate a model in which Pi aborts and Pj is the victim. So, in total we get
16 possible attack scenarios to check. Ultimately, the analysis result shows that
no abort-chaining attack is detected for the MR protocol with 5 signers.
If no one aborts, an honest signer can always get other signers’ signature by
simply sending a resolve request to T . This means our analysis of fairness in
Mocha is exhaustive. A formal correctness argument of our reasoning is post-
poned for future research.
Mukhamedov and Ryan have shown that the CKS protocol [5] fails to satisfy
the property of fairness with n ≥5. They propose a new protocol [6] and give
its formal analysis in the model checker NuSMV [13] for 5 signers. They split
fairness into two sub-properties in order to cover all possible scenarios, for which
it is necessary to go through a number of cases. ATL has the advantage to express
fairness in terms of strategies, so that our fairness speciﬁcations turn out more
natural than are deﬁnable in CTL [10]. Comparing to Mukhamedov and Ryan’s
work, we reduce the veriﬁcation problem of fairness on the system model level
instead of decomposing speciﬁcations.
5
Model Checking MRT Protocols
The main result of Mauw, Radomirovi´c and Torabi Dashti [7] makes it feasible
to construct MPCS protocols excluding abort-chaining attacks with minimal
communication messages. We ﬁrst describe a methodology for designing an MRT
protocol in Sect. 5.1. The description of the message order in the derived protocol
is fully determined as in [7]. However, since Mauw, Radomirovi´c and Torabi
Dashti only gave a high-level description of the message contents, we make the
underlying assumptions precise. In Sect. 5.2, we design a family of MRT protocols
and give their analysis in Mocha. Our model checking results reveal an abort-
chaining attack on an example protocol with 3 signers described in [7, Sect. 7],
for which we propose a ﬁx based on an important assumption that was not made
explicit in the original paper.

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
195
Fig. 1. MRT protocols with 3 signers (the left one is based on the signing sequence
12 | 3121 | 3212, the right one describes the protocol in [7])
5.1
Design Methodology of MRT Protocols
An MRT protocol deﬁnes a sequence of messages m1, m2, . . . , mℓto be exchanged
between a group of n signers in the main protocol, where every mi is supposed
to be received before mj is sent out if i < j. The principles of MRT protocols are
exactly those of MR, except that we have the following additional assumptions.
1. In each step a signer sends out message mi (1 ≤i ≤ℓ) to another signer.
2. The receiver of mi is the sender of mi+1, where i < ℓ.
3. The receiver of each message is allowed to have the most recent promises
(signatures) of all the other signers, provided that they have ever sent out
promises (signatures). That is, a sender may need to forward up to n −2
promises of other signers besides his own promise.
Based on the assumptions, an MRT protocol can be regarded as a list of the
indices of the signers in which order they send their messages. Such a list is
called a signing sequence.
A signing sequence α for an MRT protocol with signers P1, . . . , Pn can be
divided into three phases. In the initial phase, the ﬁrst n −1 signers send out
their promises according to the ﬁrst n −1 distinct elements of α. The middle
phase is initiated by a (ﬁrst level) promise of the signer who was missed out in
the initial phase, followed by a sequence of numbers indicating the particular
order of further promise exchanges. In the end phase the signers exchange their
signatures. A typical signing sequence for n = 5 is of the following form.
1234 | 543212345432 | 12345123
From the example one may easily observe that the end phase needs to be at
least of length 2n −2, in that the ﬁrst n numbers (as a permutation) are for all
the signers to send out their signatures, and the remaining n −2 messages are
necessary to further distribute the signatures. The last receiver is implicit in a
sequence but can be uniquely determined, e.g., signer P4 in the above example.

196
Y. Zhang et al.
An MRT protocol does not explicitly distinguish abort and resolve, i.e., every
request to the trusted third party T is a resolve. It is obvious that if a signer in
the initial phase sends a request to T , an abort will always be replied. However in
the middle phase and end phase, T will have to make a decision based on whether
all the previously requested signers have been dishonest. A major contribution
of [7] is showing that a protocol generated by a signing sequence α is free of abort
chaining attacks iﬀα’s middle phase together with the ﬁrst n elements from its
end phase contains all permutations of the set {1, . . . , n}. Therefore, ﬁnding the
shortest sequence containing all permutations yields a solution to minimize the
number of message exchanges in this particular class of protocols.
To design an MRT protocol for n signers, we ﬁrst ﬁnd a shortest sequence α
containing all permutations of the set {1, . . . , n}, using Adleman’s algorithm [14].
This sequence serves as the middle phase and partial end phase of a signing
sequence. To complete the end phase, we append more indices of the signers at
the end of α such that the end phase is able to distribute all the signatures to all
signers. The initial phase can be obtained simply by pre-pending a sequence of
length n −1 to α to construct a full permutation at the beginning. There exist
7 (isomorphically) distinct shortest sequences which contain all permutations in
{1, 2, 3} and they are presented below.4
➊3123 | 123
➋3121 | 321
➌3123 | 132
➍31323 | 13
➎31321 | 31
➏3123 | 213
➐3121 | 312
The symbol ‘|’ is used to separate diﬀerent phases in the ﬁnal signing sequence.
Taking sequence ➋as an example. First we complete the end phase by appending
a 2 in the end. After adding the initial phase 12 at the beginning, we get a
complete signing sequence 12 | 3121 | 3212. The main protocol derived from this
signing sequence is depicted in the left-hand side of Fig. 1.5 Note that a shortest
sequence containing all permutations does not necessarily give rise to a protocol
with minimal messages: sequence ➍requires appending two numbers in the end
phase for completing the ﬁnal signature distribution. For 4 signers, there are 9
distinct sequences modulo isomorphism:
➀42314234 | 1243
➁42314234 | 1234
➂42314234 | 1324
➃42314324 | 1234
➄42314324 | 1342
➅42314324 | 1324
➆42312432 | 1423
➇42312432 | 1432
➈42312432 | 1342
Fig. 2 shows a protocol designed from sequence ➁.
5.2
Design and Veriﬁcation of MRT Protocols
In this section, we design a number of MRT protocols based on the methodol-
ogy in Sect. 5.1. Each MRT protocol consists of a main protocol and a resolve
sub-protocol. Similar to the MR protocol, the MRT protocols assume resilient
4 Sequence ➊determines the example protocol in [7, Sect. 7].
5 We circle the positions where a signer is allowed to send a request to T. prτ(c, i) and
s(c, i) denote Pi’s τ-level promise and Pi’s signature on c, respectively.

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
197
Fig. 2. An MRT protocol with 4 signers based on the sequence 123 | 42314234 | 123432
communication channels and private contract signatures (PCS). We have mod-
elled and veriﬁed fairness and timeliness properties of the MRT protocols gen-
erated from all 7 shortest sequences for 3 signers. As for 4 signers, we veriﬁed
the protocols generated from sequence ➁, sequence ➃and sequence ➆as afore-
mentioned. We brieﬂy present our modelling of MRT protocols as follows.
Main protocol. The signers send and receive messages in the order speciﬁed
by a signing sequence which is generated from a shortest sequence containing
all permutation as introduced before. Upon receipt of a message containing all
required information, a signer Pi generates a message consisting of all the up-
to-date promises and signatures and sends it to the next designated receiver. If
Pi does not receive the expected message, he may quit the protocol if he has
not sent out any messages yet, or he may start the resolve protocol by sending
a resolve request to T . The request is in the form of {dispute, i, Hi, c}i, where
dispute is a reserved keyword indicating Pi is contacting T for intervention, and
Hi is Pi’s history including all the messages he has sent or received so far, which
gives T suﬃcient information to judge Pi’s current position in an execution.
The identiﬁer c is meant to uniquely identify this contract signing session that
includes the contract, the signing partners and the contract text. Pi’s request
does not indicate whether Pi asks T to abort or resolve. It is T ’s responsibility
to make a decision and to reply with an abort or a signed contract.
Resolve Sub-protocol. T maintains a tuple ⟨c, status⟩in her database indicating
a list of signers who have requested so far. Together with the history Hi of each
received request, T is able to make a decision on whether to reply with an abort
or a signed contract. The reasoning patterns of T in the sub-protocols of MRT
are very similar to that of the MR protocol: a signer is considered dishonest if

198
Y. Zhang et al.
he is shown by another signer’s request to have continued in the main protocol
after having sent a request to T . However in the MRT protocols, diﬀerent signers
may have diﬀerent promise levels at a particular position, which are induced by
the signing sequences of the main protocols. As a consequence, a sub-protocol
of MRT has to be slightly adjusted from that of MR, and the sub-protocols may
diﬀer from each other.
5.3
An Attack on the Example Protocol
Our analysis in Mocha reveals an abort chaining attack in the example MRT pro-
tocol with 3 signers in [7]. This is due to that the protocol does not strictly follow
the methodology as described in Sect. 5.1. Here we also present a simple ﬁx.
The protocol with its attack scenario is depicted in Fig. 1 (right). The abort-
chaining attack is highlighted as shadowed circles. In this scenario, P1 and P3
are dishonest and collude to obtain P2’s signature. The attack is achieved as
follows, where promτ(c, i) denotes the τ-level promise of Pi on contract c:
– P1 sends his ﬁrst message out, and then contacts T with H1 = {prom1(c, 1)},
by which T presumes P1 is in the initial phase, and replies with an abort at
the same time storing ⟨c, (1 : {prom1(c, 1)})⟩into her database. After having
contacted T , P1 continues in the main protocol till the end.
– P3 contacts T at the position of the ﬁrst highlighted R circle with H3 =
{prom1(c, 1), prom1(c, 2), prom1(c, 3)}. This message does not reveal that
P1 is continuing the main protocol, thus T also replies with an abort and
stores ⟨c, (3 : {prom1(c, 1), prom1(c, 2), prom1(c, 3)})⟩into her database. Af-
ter having contacted T , P3 continues in the main protocol up to the receipt
of P2’s signature.
– P2 faithfully follows the main protocol till the end. After sending out his
signature, P2 will never receive P3’s signature. Then P2 contacts T with H2 =
{prom1(c, 1), prom1(c, 2), prom1(c, 3), prom2(c, 1), prom2(c, 2), sig(c, 1),
sig(c, 2)}. On receipt of such a request, T is able to deduce that P1 has been
dishonest. However, T is unable to conclude that P3 is cheating, because
P3’s second level promise was not forwarded by P1 according to the protocol
design as shown in [7, Sect. 7].
The ﬂaw of this protocol is due to a violation of assumption 3 in Sect. 5.1.
In order to ﬁx the problem, we change P1’s last message from {sig(c, 1)} into
{sig(c, 1), prom2(c, 3)}, i.e., P1 is required to forward all the up-to-date promises
and signatures in his hand to P2. With P3’s second level promise in H2, T is able
to ﬁnd out that P3 is dishonest. Therefore, T can overturn her abort decision
and guarantee fairness for P2.
6
Discussion and Conclusion
In this paper, we have used the model checker Mocha to analyse two types of
MPCS protocols – the MR protocol [6] and a number of MRT protocols [7].6
6 All Mocha models and ATL properties can be found at [12].

Game-Based Veriﬁcation of Multi-Party Contract Signing Protocols
199
Mocha allows one to specify properties in ATL which is a branching-time tempo-
ral logic with game semantics, and the model checking problem for ATL requires
the computation of winning strategies. Thus the use of Mocha allows us to have
a precise and natural formulation of desired properties of contract signing.
Mukhamedov and Ryan showed that the CKS protocol is not fair for n ≥5
by giving an abort-chaining attack. The fairness of their ﬁxed protocol [6] has
been analysed in NuSMV for 5 signers. Instead, we modelled the MR protocol
in Mocha with up to 5 signers and both fairness and timeliness properties have
been checked. The formulation of fairness in ATL as winning strategies is model
independent, while Mukhamedov and Ryan have to split fairness into two CTL
sub-properties in order to cover all possible scenarios, for which it is necessary
to go through a number of cases (see [6], Sect. 7).
The main result of Mauw, Radomirovi´c and Torabi Dashti [7] made it feasible
to construct fair MPCS protocols with a minimal number of messages. Their
main theorem [7] states that there is a fair signing sequence of length n2 −n +
3, where n is the number of signers in an MPCS protocol. This fair sequence
must contain all permutations of {1, . . . , n} as sub-sequences, and it can be
transformed back into an MPCS protocol of length n2+1. However, the resulting
MPCS protocol is only free of abort-chaining attacks, and it is merely conjectured
that this implies fairness. We described how to derive an MR protocol from a
minimal signing sequence explicitly. Following this methodology, we designed a
number of MRT protocols for 3 and 4 signers, all of which have been checked in
Mocha. In particular, we discovered an abort-chaining attack in the published
MRT protocol with 3 signers [7]. The ﬂaw is due to a mistake in the protocol
design. We also presented a solution to it, and the ﬁxed protocol is shown to
satisfy fairness in Mocha.
Chadha, Kremer and Scedrov used Mocha to check abuse-freeness in the GM
protocol and the CKS protocol, and found a vulnerability in the ﬁrst protocol [5].
The vulnerability is due to the fact that T ’s reply to a signer’s abort or resolve
request contains additional information, which can be used by the signer as a
proof for an outside challenger. Their ﬁx is to exclude the additional information
from T ’s replies. The MR protocol uses similar abort and resolve sub-protocols.
Mukhamedov and Ryan claimed that their protocol is abuse-free because of the
use of PCS. However, the situation with MRT protocols is diﬀerent: a single
signer not only sends out his own promise to the intended receiver, but forwards
the other signers’ promises. It might give a coalition of signers an advantage
to the remaining signers. However, the advantage has to be provable. How to
formalise abuse-freeness in a precise and correct way is a challenging research
topic [15,16,17]. Our immediate future work is to analyse abuse-freeness in the
MRT protocols; either we prove the designed MRT protocols abuse-free or we
can use the built models to identify a point that a coalition of signers have a
provable advantage against an honest signer. In this paper, we have veriﬁed pro-
tocols with a quite limited number of signers (up to ﬁve), and the veriﬁcation of
timeliness properties in Mocha usually took minutes while for fairness properties

200
Y. Zhang et al.
it might need a number of days. Another future direction is to study abstract
interpretation [18] in order to analyse the models in Mocha with more signers.
Acknowledgement. We thank Saˇsa Radomirovi´c for many helpful discussions.
References
1. Asokan, N., Waidner, M., Schunter, M.: Optimistic protocols for fair exchange. In:
Proc. CCS, pp. 7–17. ACM, New York (1997)
2. Asokan, N., Shoup, V., Waidner, M.: Optmistic fair exchange of digital signatures.
Selected Areas in Communications 18(4), 591–606 (2000)
3. Kremer, S., Markowitch, O., Zhou, J.: An intensive survey of fair non-repudiation
protocols. Computer Communications 25(17), 1606–1621 (2002)
4. Garay, J.A., MacKenzie, P.D.: Abuse-free multi-party contract signing. In: Jayanti,
P. (ed.) DISC 1999. LNCS, vol. 1693, pp. 151–166. Springer, Heidelberg (1999)
5. Chadha, R., Kremer, S., Scedrov, A.: Formal analysis of multi-party contract sign-
ing. J. Autom. Reasoning 36(1-2), 39–83 (2006)
6. Mukhamedov, A., Ryan, M.D.: Fair multi-party contract signing using private con-
tract signatures. Inf. Comput. 206(2-4), 272–290 (2008)
7. Mauw, S., Radomirovi´c, S., Torabi Dashti, M.: Minimal message complexity of
asynchronous multi-party contract signing. In: Proc. CSF, pp. 13–25. IEEE CS,
Los Alamitos (2009)
8. Alur, R., Henzinger, T.A., Mang, F.Y.C., Qadeer, S., Rajamani, S.K., Tasiran, S.:
Mocha: Modularity in model checking. In: Y. Vardi, M. (ed.) CAV 1998. LNCS,
vol. 1427, pp. 521–525. Springer, Heidelberg (1998)
9. Alur, R., Henzinger, T.A., Kupferman, O.: Alternating-time temporal logic. J.
ACM 49(5), 672–713 (2002)
10. Emerson, E.A.: Temporal and modal logic. In: Handbook of Theoretical Computer
Science (B), pp. 955–1072. MIT Press, Cambridge (1990)
11. Alur, R., Henzinger, T.A.: Reactive modules. Formal Methods in System De-
sign 15(1), 7–48 (1999)
12. Zhang, Y., Zhang, C., Pang, J., Mauw, S.: Game-based veriﬁcation of multi-
party contract signing protocols – Mocha models and ATL properties (2009),
http://satoss.uni.lu/members/jun/mpcs/
13. Cimatti, A., Clarke, E.M., Giunchiglia, E., Giunchiglia, F., Pistore, M., Roveri, M.,
Sebastiani, R., Tacchella, A.: NuSMV 2: An open source tool for symbolic model
checking. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS, vol. 2404, pp.
359–364. Springer, Heidelberg (2002)
14. Adleman, L.: Short permutation strings. Discrete Mathematics 10, 197–200 (1974)
15. Chadha, R., Mitchell, J.C., Scedrov, A., Shmatikov, V.: Contract signing, opti-
mism, and advantage. J. Log. Algebr. Program. 64(2), 189–218 (2005)
16. K¨ahler, D., K¨usters, R., Wilke, T.: A Dolev-Yao-based deﬁnition of abuse-free
protocols. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP
2006. LNCS, vol. 4052, pp. 95–106. Springer, Heidelberg (2006)
17. Cortier, V., K¨usters, R., Warinschi, B.: A cryptographic model for branching time
security properties - the case of contract signing protocols. In: Biskup, J., L´opez, J.
(eds.) ESORICS 2007. LNCS, vol. 4734, pp. 422–437. Springer, Heidelberg (2007)
18. Henzinger, T.A., Majumdar, R., Mang, F.Y.C., Raskin, J.F.: Abstract interpre-
tation of game properties. In: Palsberg, J. (ed.) SAS 2000. LNCS, vol. 1824, pp.
220–239. Springer, Heidelberg (2000)

Attack, Solution and Veriﬁcation for Shared
Authorisation Data in TCG TPM
Liqun Chen and Mark Ryan
HP Labs, UK,
and University of Birmingham, UK
Abstract. The Trusted Platform Module (TPM) is a hardware chip
designed to enable computers to achieve greater security. Proof of pos-
session of authorisation values known as authdata is required by user
processes in order to use TPM keys. If a group of users are to be autho-
rised to use a key, then the authdata for the key may be shared among
them. We show that sharing authdata between users allows a TPM im-
personation attack, which enables an attacker to completely usurp the
secure storage of the TPM. The TPM has a notion of encrypted transport
session, but it does not fully solve the problem we identify.
We propose a new authorisation protocol for the TPM, which we call
Session Key Authorisation Protocol (SKAP). It generalises and replaces
the existing authorisation protocols (OIAP and OSAP). It allows auth-
data to be shared without the possibility of the impersonation attack,
and it solves some other problems associated with OIAP and OSAP. We
analyse the old and the new protocols using ProVerif. Authentication
and secrecy properties (which fail for the old protocols) are proved to
hold of SKAP.
1
Introduction
The Trusted Platform Module (TPM) speciﬁcation is an industry standard [14]
and an ISO/IEC standard [6] coordinated by the Trusted Computing Group
(TCG), for providing trusted computing concepts in commodity hardware. TPMs
are chips that aim to enable computers to achieve greater levels of security than
is possible in software alone. There are 100 million TPMs currently in exis-
tence, mostly in high-end laptops. Application software such as Microsoft’s Bit-
Locker and HP’s HP ProtectTools use the TPM in order to guarantee security
properties.
The TPM stores cryptographic keys and other sensitive information in shielded
locations. Keys are organised in a tree hierarchy, with the Storage Root Key
(SRK) at its root. Each key has associated with it some authorisation data,
known as authdata. It may be thought of as a password to use the key. Processes
running on the host platform or on other computers can use the TPM keys in
certain controlled ways. To use a key, a user process has to prove knowledge
of the relevant authdata. This is done by accompanying the command with an
HMAC (a hash-function-based message authentication code, as speciﬁed in [5]),
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 201–216, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

202
L. Chen and M. Ryan
keyed on the authdata or on a shared secret derived from the authdata. When
a new key is created in the tree hierarchy, its authdata is chosen by the user
process, and sent encrypted to the TPM. The encryption is done with a key
that is derived from the parent key authdata. The TPM stores the new key’s
authdata along with the new key. Creating a new key involves using the parent
key, and therefore an HMAC proving knowledge of the parent key’s authdata
has to be sent.
If a group of users are to be authorised to use a key, then the authdata for the
key may be shared among them. In particular, the authdata for SRK (written
srkAuth) is often assumed to be a widely known value, in order to permit anyone
to create child keys of SRK. This is analogous to allowing several people to share
a password to use a resource, such as a database.
We show that sharing authdata between users has some signiﬁcant undesir-
able consequences. For example, an attacker that knows srkAuth can fake all the
storage capabilities of the TPM, including key creation, sealing, unsealing and
unbinding. Shared authdata completely breaks the security of the TPM stor-
age functions. Some commentators to whom we have explained our attack have
suggested the TPM’s encrypted transport sessions as a way of mitigating the
attack. We show that they are not able to do that satisfactorily (section 2.4).
We solve this problem by proposing a new authorisation protocol for the
TPM, which we call Session Key Authorisation Protocol (SKAP). It generalises
and replaces the existing authorisation protocols (OIAP and OSAP). In contrast
with them, it does not allow an attacker that knows authdata to fake a response
by the TPM. SKAP also ﬁxes some other problems associated with OIAP and
OSAP. To demonstrate its security, we analyse the old and the new protocols
using the protocol analyser ProVerif [8,9], and prove authentication and secrecy
properties of SKAP.
Related work. Other attacks of a less signiﬁcant nature have been found
against the TPM. The TPM protocols expose weak authdata secrets to oﬄine
dictionary attacks [11]. To ﬁx this, we proposed to modify the TPM protocols
by using SPEKE (Simple Password Exponential Key Exchange [1]). However,
the modiﬁcations proposed in [11] do not solve the problem of shared authdata.
An attacker can in some circumstances illegitimately obtain a certiﬁcate on
a TPM key of his choice [12]. Also, an attacker can intercept a message, aiming
to cause the legitimate user to issue another one, and then cause both to be
received, resulting in the message being processed twice [10]. Some veriﬁcation
of certain aspects of the TPM is done in [13]. Also in [13], an attack on the
delegation model of the TPM is described; however, experiments with real TPMs
have shown that the attack is not possible [7].
Paper overview. Section 2 describes the current authorisation protocols for
the TPM, and in Sections 2.2 and 2.3 we demonstrate our attack. In Section
2.4 we explain why the TPM’s encrypted transport sessions don’t solve the
problems. Section 3 describes our proposed protocol, SKAP, that replaces OIAP
and OSAP. In section 4, we use ProVerif to demonstrate the security of SKAP
compared with OIAP and OSAP. Conclusions are in Section 5.

Attack, Solution and Veriﬁcation for Shared Authorisation Data
203
2
TPM Authorisation
A TPM command that makes use of TPM keys requires the process issuing the
command to be authorised. A process demonstrates its authorisation by proving
knowledge of the relevant authdata. Since the TPM is a low-power device, its
design minimises the use of heavy-weight cryptography, preferring light-weight
solutions (such as hashes and HMACs) where possible. Demonstration of autho-
risation is done by accompanying a TPM command with such an HMAC of the
command parameters, keyed on the authdata or on a shared secret derived from
the authdata. We note the result of the HMAC by hmacad(msg), where ad is
the authdata, and msg is a concatenation of selected message parameters.
The response from the TPM to an authorised command is also accompanied
by an HMAC of the response parameters, again keyed on the authdata or the
shared secret. This is intended to authenticate the response to the calling process.
The TPM provides two kinds of authorisation sessions, called object inde-
pendent authorisation protocol (OIAP) and object speciﬁc authorisation protocol
(OSAP). OIAP allows multiple keys to be used within the same session, but
it doesn’t allow commands that introduce new authdata, and it doesn’t allow
authdata for an object to be cached for use over several commands. An OSAP
session is restricted to a single object, but it does allow new authdata to be
introduced and it creates a session secret to securely cache authorisation over
several commands. If a command within an OSAP session introduces new au-
thdata, then the OSAP session is terminated by the TPM (because the shared
secret is contaminated by its use in XOR encryption).
In order to prevent replay attacks, each HMAC includes two nonces, respec-
tively from the user process and TPM, as part of msg. The nonces created by
the calling process are called “odd”, denoted no, and the nonces created by the
TPM are called “even”, denoted by ne. The nonces are sent in the clear, and
also included in the msg part of the HMAC. Both the process and TPM use a
fresh nonce in each HMAC computation, and they verify the incoming HMACs
to check integrity and authorisation. For example, the process sends the ﬁrst
nonce odd no1 to the TPM and receives the ﬁrst nonce even ne1 along with
mac1 = hmacad(no1, ne1, ...), and then sends mac2 = hmacad(ne1, no2, ...) with
no2 and receives ne2 along with mac3 = hmacad(no2, ne2, ...), and so on. This is
sometimes called a rolling nonce protocol.
2.1
Authorisation Example
In this subsection, we will take a look at an authorisation example, in which
a user process ﬁrst asks the TPM create a new key as part of the storage key
tree, then loads this key into the TPM internal memory, and ﬁnally uses this
key to encrypt some arbitrary data. These three functions are demonstrated in
Figure 1 in three separated sessions.
Session 1 shows the exchange of messages between the user process and the
TPM when a child key of another loaded key (called the parent key) is created

204
L. Chen and M. Ryan
using the TPM command TPM CreateWrapKey. The TPM returns a blob, con-
sisting of the newly created key and some other data, encrypted with the parent
key. The user and TPM achieve this function by performing the following steps:
1. First, the user process sets up a OSAP session based on the currently loaded
parent key. The parent key handle is pkh, and its authdata is ad(pkh). The
TPM OSAP command includes pkh and the nonce nosap
o
.
2. Upon receipt of the TPM OSAP command, the TPM assigns a new session
authorisation handle ah, generates two nonces ne and nosap
e
, and sends these
items back as the response.
3. The user process and the TPM each calculate the shared secret S derived
from ad(pkh), and the two nonces for OSAP by using the HMAC algorithm.
4. Then, the user process calls TPM CreateWrapKey, providing arguments in-
cluding authdata newauth for the key being created, some other parameters
about the key, and the HMAC keyed on S demonstrating knowledge of SRK
authdata. To protect the new authdata, it is XOR-encrypted with a key
derived from ad(pkh) and ne using the hash-function SHA1.
5. After receiving this command, the TPM checks the HMAC and creates the
new key. The TPM returns a blob, keyblob, consisting of the public key and
an encrypted package containing the private key and the new authdata. The
returned message is authenticated by accompanying it with an HMAC with
the two nonces keyed on S.
6. Because the shared secret S has been used as a basis for an authdata en-
cryption key, the OSAP session is terminated by the TPM. Later commands
will have to start a new session.
In order to be used, the newly created key must be loaded into the TPM.
For this, an OIAP session may be used. Session 2 shows the messages exchanged
between the user process and the TPM during the creation of the OIAP session
and the TPM LoadKey2 command. The following steps are performed:
1. The user process sends the TPM OIAP command to the TPM.
2. The TPM assigns the session authorisation handle ah′ and sends it back
along with the newly created nonce n′′
e. ,
3. The process calls TPM LoadKey2, providing arguments including the parent
key handle pkh and keyblob. The authorisation of this command is achieved
using the authdata of the parent key, ad(pkh).
4. The TPM checks the HMAC, and if the check passes, decrypts keyblob and
loads the key into its internal memory. The TPM ﬁnally creates a key handle
for the loaded key kh and a nonce n′′′
e and sends them back together with
an HMAC keyed on the authdata of the parent key ad(pkh).
After the key is loaded, it can be used to encrypt data using TPM Seal. As
well as encrypting the data, TPM Seal binds the encrypted package to particular
Platform Conﬁguration Registers (PCRs) speciﬁed in the TPM Seal command.
The TPM will later unseal the data only if the platform is in a conﬁguration
matching those PCRs. TPM Seal requires a new OSAP session based on the
newly created key. The details are shown in Session 3, where the user process
and TPM perform the following steps:

Attack, Solution and Veriﬁcation for Shared Authorisation Data
205
User
TPM
Session 1:
TPM OSAP( pkh,
nosap
o
)
ah, ne, nosap
e
S = hmacad(pkh)(nosap
e
, nosap
o
)
S = hmacad(pkh)(nosap
e
, nosap
o
)
TPM CreateWrapKey( ah, pkh, no, . . . ,
SHA1(S, ne) ⊕newauth ),
hmacS(ne, no, . . .)
keyblob,
n′
e,
hmacS(n′
e, no, . . .)
Session 2:
TPM OIAP( )
ah′, n′′
e
TPM LoadKey2( ah′, pkh, n′
o, keyblob, . . . ),
hmacad(pkh)(n′′
e , n′
o, . . .)
kh,
n′′′
e ,
hmacad(pkh)(n′′′
e , n′
o, . . .)
Session 3:
TPM OSAP( kh,
nosap
o
′ )
ah′′, n′′′′
e , nosap
e
′
S′ = hmacad(kh)(nosap
e
′, nosap
o
′)
S′ = hmacad(kh)(nosap
e
′, nosap
o
′)
TPM Seal( ah′′, kh, n′′
o, info of sealed data, PCR, . . . ,
SHA1(S′, n′′′′
e ) ⊕newauth′ ),
hmacS′(n′′′′
e , n′′
o, . . .)
sealedblob, n′′′′′
e
, hmacS′(n′′′′′
e
, n′′
o, . . .)
Fig. 1. Session 1: Creating a key on the TPM. TPM OSAP creates an OSAP session
and the shared secret S by both parties. TPM CreateWrapKey requests the TPM to
create a key. The command and the response are authenticated by the shared secret
S. Session 2: Loading a key on the TPM. TPM OIAP creates an OIAP session for the
TPM LoadKey2 command. Session 3: Using the key to seal data. TPM OSAP creates
an OSAP session and its corresponding shared secret S′ for the TPM Seal command.
The seal command and the response are authenticated by S′.
1. The ﬁrst three steps are identical to Session 1, except they use the key handle
kh and authdata ad(kh) belonging to the newly loaded key, instead of pkh
and ad(pkh).
2. After setting up the OSAP session, the user process calls TPM Seal, pro-
viding arguments including the information of the sealed data, a PCR and
the new authdata for the corresponding unseal process. The new authdata is
again XOR-encrypted with a key derived from the encryption key authdata.

206
L. Chen and M. Ryan
The message is authenticated by accompanying it with an HMAC keyed on
the secret S′.
3. The TPM responds the command with a sealed blob sealedblob, which con-
sists of an encrypted package containing the sealed data, the PCR value and
the new authdata. Again the returned message is authenticated by accom-
panying it with an HMAC keyed on the secret S′.
2.2
The Problem of Shared Authdata
If authdata is a secret shared only between the calling process and the TPM,
then the HMACs serve to authorise the command and to authenticate the TPM
response. However, as mentioned earlier, authdata may be shared between sev-
eral users, in order to allow each of them to use the resource that the authdata
protects. In particular, the authdata of SRK is often assumed to be a well-known
value. E.g., in Design Principles of the TPM speciﬁcation [6,14], sections 14.5,
14.6 refer to the possibility that SRK authdata is a well-known value, and sec-
tions 30.2, 30.8 refer to other authorisation data being well-known values.
The usage model is that a platform has a single TPM, and the TPM has a
single SRK, which plays the role of the root of a trusted key hierarchy tree.
If the platform has multiple users, each of them can build their own branches
of the tree on the top of the same root. In order to let multiple users access
SRK, the authdata of SRK is made available to all of them. The goal is that
although these users share the same SRK and its authdata, they are only able to
access their own key branches but not anyone else’s. We will show how the idea
of sharing SRK authdata fails to achieve the design principle of the protected
storage functionality of the TPM.
Suppose one of these users who knows an authdata value is malicious and he
can intercept a command from another user to the TPM (the TPM protocols
involving encryption and HMACs are clearly designed on the assumption that
such interception is possible). He can use knowledge of the authdata to decrypt
any new authdata that the command is introducing; and he can fake the TPM
response that is authenticated using the shared authdata.
It follows that an attacker that knows the authdata for SRK can fake the
creation of child keys of SRK. Those keys are then keys made by the attacker
in software, and completely under his control. He can intercept requests to use
those keys, and fake the response. Therefore, all keys intended to be descendants
of SRK can be faked by the TPM. An attacker with knowledge of SRK authdata
can completely usurp the storage functionality of the TPM, by creating all the
keys in software under his own control, and faking all the responses by the TPM.
2.3
The Attack in Practice
We suppose that Alice is in possession of a laptop owned by her employer,
that has an IT department which we call ITadmin. The TPM TakeOwnership
command has been performed by ITadmin when the laptop was ﬁrst procured;
thus, the TPM has created SRK and given its authdata to ITadmin. When Alice

Attack, Solution and Veriﬁcation for Shared Authorisation Data
207
receives the laptop, she is also provided with SRK authdata so that she can use
the storage functions of the TPM.
Alice now decides to create a key on the TPM with authdata of her own
choosing, and wants to encrypt her data using that key. She invokes the com-
mands of Figure 1 of Section 2.1. Unknown to her, ITadmin has conﬁgured the
laptop so that commands intended to go to the TPM go instead to software
under ITadmin’s control. This software responds to all the commands that Alice
sends. ITadmin’s software creates the necessary nonces and fakes the response
to TPM OSAP. Next, it fakes the creation of the key and fakes all the responses
to the user (again creating all the necessary nonces). In particular, in the case
of TPM CreateWrapKey, ITadmin’s software
– is able to calculate the session secret S, since it is based on SRK authdata
and other public values (namely, the OSAP nonces that are sent in the clear);
– is able to decrypt the new authdata, since it is XOR encrypted with a key
based on SRK authdata and other public values (namely, the command
nonces that are sent in the clear);
– is able to create an RSA key in software, according to the parameters spec-
iﬁed in the command;
– is able to create the message returned to the user process. This involves
encrypting the “secret” package with SRK, and creating the HMAC that
“authenticates” the TPM.
Next, ITadmin’s software fakes the response to TPM LoadKey2 (using its knowl-
edge of SRK authdata to create the necessary HMAC). Finally, it fakes the re-
sponse to TPM Seal (using its knowledge of the new key’s authdata to create
the necessary HMAC). Therefore, the ITadmin can successfully impersonate the
TPM just because it knows the authdata of SRK.
The attack scenario given in this example, in which ITadmin is the attacker,
is similar to that illustrating the TPM CertifyKey attack in [12]. Many other
scenarios are possible. For example, TPMs are now common in servers, and
many interesting use cases involve remote clients accessing TPM functionality
on a server (for instance, to achieve guarantees about the server behaviour). In
that scenario, our attack means that the server is able to spoof all the responses
from the TPM. Another class of scenarios which illustrate this attack revolve
around virtualisation; there too, independent virtual environments share a TPM
and share knowledge of SRK authdata, allowing one such environment to spoof
TPM replies to another.
2.4
Encrypted Transport Sessions
The OIAP and OSAP sessions are intended to provide message integrity, but not
message conﬁdentiality. The TPM has a notion of encrypted transport session
[14,6] which is intended to provide message conﬁdentiality. Encrypted transport
sessions are initiated with the TPM EstablishTransport command, which allows
a session key to be established, using a public storage key of the TPM. Since

208
L. Chen and M. Ryan
the security of the session is anchored in a public key, and that public key can
be certiﬁed, this does indeed defeat the TPM spooﬁng attack we have described
above.
However, encrypted transport sessions are not an ideal solution to be used
as an alternative of the OIAP and OSAP sessions for the purpose of providing
robust TPM authorisation, because the encrypted transport sessions do not solve
the problem of weak authdata, reported in [11]. In that paper, it is shown that
the TPM protocols expose authdata to the possibility of oﬄine guessing attacks.
If authdata is based on a weak secret, then an attacker that tries to guess the
value of the authdata is able to conﬁrm his guess oﬄine. Encrypted transport
sessions do not resist against this attack because they do not encrypt the high-
entropy values (the rolling nonces) that are used in the authorisation HMACs.
Therefore, changes to OIAP and OSAP are necessary, to avoid the attack of
[11]. Unfortunately, the changes proposed in [11] do not solve the attack we have
identiﬁed in this paper. The solution proposed in [11] is based on the SPEKE
protocol, which relies on a secret being shared between the two participants,
whereas shared authdata precisely invalidates that assumption.
Thus, there is no alternative to a thorough re-design of the authorisation
protocols of the TPM.
3
A New TPM Authorisation Protocol
Our aim is to design an authorisation protocol that solves both the weak auth-
data problem of [11] and the shared authdata problem reported in this paper.
Moreover, we aim to avoid the complexity and cost of the encrypted transport
session. (We showed above that the encrypted transport session doesn’t solve
both attacks anyway.) Our solution relies on public-key cryptography; the TPM
designers wanted to avoid that, since it is expensive, but it seems impossible
to achieve proper authentication with shared authdata without it. We design
our protocol to minimise the frequency with which public key operations are
required.
We propose Session Key Authorisation Protocol (SKAP), which has the fol-
lowing advantages over the existing OIAP and OSAP protocols:
– It generalises OIAP and OSAP, providing a session type that oﬀers the
advantages of both. In particular, it can cache a session secret to avoid
repeatedly requesting the same authdata from a user (like OSAP), and it
allows diﬀerent objects within the same session (like OIAP).
– It is a long-lived session. In contrast with OSAP, it is not necessary to
terminate the session when a command introduces new authdata.
– It allows authdata to be shared among users, without allowing users that
know authdata to impersonate the TPM.
– In contrast with existing TPM authorisation, it does not expose low-entropy
authdata to oﬄine dictionary attacks [11].

Attack, Solution and Veriﬁcation for Shared Authorisation Data
209
User
TPM
TPM SKAP( kh,
{S}pk(kh) )
ah,
ne
K1 = hmacS(ad(kh), ne, 1)
K2 = hmacS(ad(kh), ne, 2)
K1 = hmacS(ad(kh), ne, 1)
K2 = hmacS(ad(kh), ne, 2)
TPM Command1( ah, kh, no, . . . ),
encK2(newauth),
hmacK1(null, ne, no, . . .)
response,
n′
e,
hmacK1(null, n′
e, no, . . .)
TPM Command2( ah, kh′, n′
o, . . . ),
encK2(newauth),
hmacK1(ad(kh′), n′
e, n′
o, . . .)
response,
n′′
e ,
hmacK1(ad(kh′), n′′
e, n′
o, . . .)
Fig. 2. Establishing a session using Session Key Authorisation Protocol, and executing
two commands in the session. The session is established relative to a loaded key with
handle kh. Command1 uses that key, and therefore does not need to cite authdata.
Command2 uses a diﬀerent key, and cites authdata in the body of the authorisation
HMAC.
The message exchanges between a user process and the TPM in the SKAP
protocol is illustrated in Figure 2. Similarly to OSAP, an SKAP session is es-
tablished relative to a loaded key with handle (say) kh. The secret part of this
key sk(kh) is known to the TPM and the public part pk(kh) is known to all user
processes which want to use the key. At the time the session is established, the
user process generates a high-entropy session secret S, which could be created as
a session random number, and sends the encryption {S}pk(kh) of S with pk(kh)
to the TPM. Theoretically any secure asymmetric encryption algorithm can be
used for this purpose; in the TPM Speciﬁcation uses RSA-OAEP [2] throughout,
so we propose to use that too. The TPM responds with an authorisation handle
ah and the ﬁrst of the rolling nonces, ne, as usual. Then each side computes two
keys K1, K2 from S by using a MAC function keyed on S. The authdata ad(kh)
for the key and the nonce ne are cited in the body of the MAC. Any secure MAC
function is suitable for our solution, but the TPM speciﬁcation uses HMAC [5]
for other purposes so we use that too.
Command1 in the illustrated session uses the key (sk(kh), pk(kh)) for which
the session was established. The authorisation HMAC it sends is keyed on K1, a
secret known only to the user process and the TPM. In contrast with OSAP, this
secret is not available to other users or processes that know the authdata for the
key. Moreover, K1 is high-entropy even if the underlying authdata is low entropy

210
L. Chen and M. Ryan
(thanks to the high-entropy session secret S). New authdata (written newauth)
that Command1 introduces to the TPM is encrypted using K2. In the ﬁgure,
encK2(newauth) denotes the result of encrypting newauth with a symmetric
encryption algorithm using the secret key K2. In general, any secure symmetric
encryption scheme can be used in this solution. More speciﬁcally, in order to
guarantee against not only eavesdropping but also unauthorised modiﬁcation,
we suggest using authenticated encryption as speciﬁed in [4]. One example is
AES Key Wrap with AES block cipher [3].
In contrast with OSAP, SKAP sessions may use keys other than the one
relative to which the session was established. Command2 in Figure 2 uses a
diﬀerent key, whose handle is kh′. Authdata for that key is cited in the body of
the HMAC that is keyed on S.
3.1
The Example Revisited
We revisit the authorisation example described in Section 2.1, where the user
wants to perform three commands, TPM CreateWrapKey, TPM LoadKey2 and
TPM Seal in a short period. We brieﬂy demonstrate how these commands can be
run in a single session (Figure 3). Suppose that the user starts from a parent key
User
TPM
TPM SKAP( pkh,
{S}pk(pkh) )
ah,
ne
K1 = hmacS(ad(pkh), ne, 1)
K2 = hmacS(ad(pkh), ne, 2)
K1 = hmacS(ad(pkh), ne, 1)
K2 = hmacS(ad(pkh), ne, 2)
TPM CreateWrapKey( ah, pkh, no, . . . ,
encK2(newauth)),
hmacK1(null, ne, no, . . .)
keyblob,
n′
e,
hmacK1(null, n′
e, no, . . .)
TPM LoadKey2( ah, pkh, n′
o, . . . ),
hmacK1(null, n′
e, n′
o, . . .)
kh,
n′′
e ,
hmacK1(null, n′′
e , n′
o, . . .)
TPM Seal( ah, kh, n′′
o, . . . ,
encK2(newauth′) ),
hmacK1(ad(kh), n′′
e , n′′
o, . . .)
sealedblob,
n′′′
e ,
hmacK1(ad(kh′), n′′′
e , n′′
o, . . .)
Fig. 3. An example of SKAP, showing creating a key, loading the key, and sealing with
the key in a single SKAP session. Compare Figure 1.

Attack, Solution and Veriﬁcation for Shared Authorisation Data
211
whose handle is pkh, and whose authdata ad(pkh) is well-known. (This parent
key might be SRK, for example.) By following the SKAP protocol, the user ﬁrst
establishes a session for the parent key. To do this, he chooses a 160-bit random
number as the session secret S, then encrypts S with the public part of the
parent key and sends {S}pk(pkh) to the TPM.
After that both sides compute two keys K1 and K2 based on the values S
and ad(pkh). Then the user sends TPM CreateWrapKey as TPM Command1
in Figure 2 along with an encrypted new authorisation data for the requested
key and HMAC for integrity check. The TPM responds the command with a
key blob for the newly created key. When receiving any message which shows
either of these two keys K1 and K2 has been used, the user is convinced that
he must be talking to the TPM and the TPM knows that its communication
partner knows ad(pkh).
When the user wants to use this key (for example, for the sealing function),
he sends the TPM the second command TPM LoadKey2 in the same session.
Since this also uses the parent key, it is again an example of Command1. The
user and the TPM carry on using K1 for authentication. Since TPM LoadKey2
does not introduce new authdata, K2 is not used. After the loading key process
succeeds, the user sends the last command TPM Seal. This command uses the
newly created and loaded key, which is not the key for which the session is cre-
ated. Therefore it is an example of Command2 in the ﬁgure, and the authdata
for the key is required. The command uses the session keys K1 and K2 for au-
thentication and protection of the sealed blob authdata, as before. So as we have
seen that a single session of the SKAP protocol can handle multiple commands
comfortably. The commands are shown in Figure 3. Comparison with Figure 1
shows a reduction from 12 to 8 messages, showing that that our protocol is more
eﬃcient as well as more secure.
4
Veriﬁcation
We have modelled the current OSAP authorisation protocol using ProVerif [8,9].
ProVerif is a popular and widely-used tool that checks security properties of
protocols. It uses the Dolev-Yao model; that is, it assumes the cryptography is
perfect, and checks protocol errors against an active adversary that can cap-
ture and insert messages, and can perform cryptographic operations if it has the
relevant keys. ProVerif is particularly good for secrecy and authentication prop-
erties, and is therefore ideal for our purpose. ProVerif is easily able to ﬁnd the
shared authdata attack of section 2.3. It shows both failure of secrecy and failure
of authentication. We have also modelled the new proposed protocol SKAP, and
ProVerif conﬁrms the secrecy and authentication properties.
Our ProVerif code scripts for OSAP and SKAP are shown in Appendixes 1
and 2 respectively1. In both models, there are two processes, representing the
user process and the TPM. The user process requests to start a new session
1 If not present in this version, those appendices can be found on the version on Mark
Ryan’s web page.

212
L. Chen and M. Ryan
(respectively OSAP or SKAP) and then requests the execution of a command,
such as TPM CreateWrapKey to create a new key. The user process then checks
the response from the TPM, and (in our ﬁrst version) declares the event successU.
The TPM process provides the new session, executes the requested command
(after checking correct authorisation), and provides the response to the calling
user process. It declares the event successT.
The properties we verify are
– query attacker:newauth
– query ev:successU(˜x) ==> ev:successT(˜x)
The ﬁrst one checks if newauth is available to the attacker. The second one stip-
ulates that if the user declares success (i.e. the user considers that the command
has executed correctly) for parameters ˜x, then the TPM also declares success
(i.e. it has executed the command) with the same parameters. (Here, the param-
eters include the agreed session key.) If this property is violated, then potentially
an attacker has found a means to impersonate the TPM.
We expect the secrecy property (ﬁrst query) to fail for OSAP and succeed for
SKAP, and this is indeed the case. The correspondence property (second query)
is also expected to expected to fail OSAP and succeed for SKAP. Unfortunately
the second query fails for both models, for the trivial reason that the TPM can
complete the actions in its trace and then stop just before it declares success. To
avoid this trivial reason, we extend the user process so it asks the TPM to prove
knowledge of the new authdata introduced by the command, before it declares
success. Now if the user declares success, the TPM should have passed the point
at which it declares success too. If it has not, then an attacker has found a means
to impersonate the responses of the TPM.
With this modiﬁcation, we ﬁnd an attack for each of the properties for OSAP,
demonstrating the attack of section 2.3. ProVerif proves that SKAP satisﬁes both
properties, demonstrating its security.
5
Conclusion
Sharing authorisation data between several users of a TPM key is a practice
endorsed by the Trusted Computing Group [6,14, Design principles, §14.5, §14.6,
§30.2, §30.8], but it makes the TPM vulnerable to impersonation attacks. An
attacker in possession of the authorisation data for the storage root key (which
is the authdata most likely to be shared among users) can completely usurp the
secure storage functionality of the TPM.
The encrypted transport sessions of the TPM solve this problem, but they
do not solve the related problem of guessing attacks (also known as dictionary
attacks) on weak authdata, reported in [11]. The solution proposed for guessing
attacks does not solve the problem of shared authdata. Therefore, a re-design of
the TPM authorisation sessions is necessary.
We propose SKAP, a new authorisation session, to replace the existing au-
thorisation sessions OIAP and OSAP. It generalises both of them and improves

Attack, Solution and Veriﬁcation for Shared Authorisation Data
213
them in several ways, in particular by avoiding the TPM impersonation attack
and the weak authdata attack.
We have analysed the old authorisation sessions and the new proposed one
in ProVerif, the protocol analyser. The results show the vulnerability of the old
sessions, and the security of the new one.
References
1. ISO/IEC 11770-4: Information technology – Security techniques – Key manage-
ment – Part 4: Mechanisms based on weak secrets
2. ISO/IEC 18033-2: Information technology – Security techniques – Encryption al-
gorithms – Part 2: Asymmetric ciphers
3. ISO/IEC 18033-3: Information technology – Security techniques – Encryption al-
gorithms – Part 3: Block ciphers
4. ISO/IEC 19772: Information technology – Security techniques – Authenticated
encryption
5. ISO/IEC 9797-2: Information technology – Security techniques – Message authen-
tication codes (MACs) – Part 2: Mechanisms using a dedicated hash-function
6. ISO/IEC, P.D.: 11889: Information technology – Security techniques – Trusted
platform module
7. Ables, K.: An attack on key delegation in the trusted platform module (ﬁrst
semester mini-project in computer security). Master’s thesis, School of Computer
Science, University of Birmingham (2009)
8. Blanchet, B.: An eﬃcient cryptographic protocol veriﬁer based on prolog rules. In:
Schneider, S. (ed.) 14th IEEE Computer Security Foundations Workshop, Cape
Breton, Nova Scotia, Canada, June 2001, pp. 82–96. IEEE Computer Society Press,
Los Alamitos (2001)
9. Blanchet, B.: ProVerif: Automatic Cryptographic Protocol Veriﬁer User Manual
(2008)
10. Bruschi, D., Cavallaro, L., Lanzi, A., Monga, M.: Replay attack in TCG speciﬁca-
tion and solution. In: ACSAC 2005: Proceedings of the 21st Annual Computer Secu-
rity Applications Conference, pp. 127–137. IEEE Computer Society, Los Alamitos
(2005)
11. Chen, L., Ryan, M.D.: Oﬄine dictionary attack on TCG TPM weak authorisation
data, and solution. In: Grawrock, D., Reimer, H., Sadeghi, A., Vishik, C. (eds.)
Future of Trust in Computing. Vieweg & Teubner (2008)
12. G¨urgens, S., Rudolph, C., Scheuermann, D., Atts, M., Plaga, R.: Security evalua-
tion of scenarios based on the TCG’s TPM speciﬁcation. In: Biskup, J., L´opez, J.
(eds.) ESORICS 2007. LNCS, vol. 4734, pp. 438–453. Springer, Heidelberg (2007)
13. Lin, A.H.: Automated Analysis of Security APIs. Master’s thesis, MIT (2005),
http://sdg.csail.mit.edu/pubs/theses/amerson-masters.pdf
14. Trusted Computing Group. TPM Speciﬁcation version 1.2. Parts 1–3 (2007),
http://www.trustedcomputinggroup.org/specs/TPM/

214
L. Chen and M. Ryan
Appendix 1: ProVerif Script for OSAP
free null, c, one, two.
fun enc/2. fun dec/2. fun senc/2. fun sdec/2.
fun hmac/2. fun pk/1. fun handle/1.
equation dec(sk, enc(pk(sk), m)) = m.
equation sdec(k, senc(k, m)) = m.
query attacker:newauth.
(* ATTACK FOUND *)
query ev:successU(x,y,z) ==> ev:successT(x,y,z). (* ATTACK FOUND *)
let User =
(* request an OSAP session *)
new no;
new noOSAP;
out(c, (kh, noOSAP));
in(c, (ah, ne, neOSAP) );
let K = hmac(authdata, (neOSAP, noOSAP)) in
(* request execution of a command, e.g. TPM_CreateWrapKey *)
new newauth;
out(c,
no);
out(c,
senc(K,newauth) );
out(c,
hmac(K,(ne,no)) );
(* receive the response from the TPM, and check it *)
in(c, (r, hm) );
if hm = hmac( K , r) then
(* check that the TPM has newauth *)
new n;
out(c, n);
in(c, hm2);
if hm2=hmac(newauth,n) then
event
successU(K, r, newauth).
let TPM =
(* handle the request for an OSAP session *)
new ne;
new neOSAP;
in(c, noOSAP );
out(c, (ne, neOSAP) );
let K = hmac(authdata, (neOSAP, noOSAP)) in
(* execute a command from the user, e.g. TPM_CreateWrapKey *)
in(c, (no, encNewAuth, hm));
if hm = hmac(K, (ne,no)) then
let newauth = sdec(K, encNewAuth) in

Attack, Solution and Veriﬁcation for Shared Authorisation Data
215
(* return a response to the user *)
new response;
out(c, ( response, hmac( K , response) ));
event successT(K, response, newauth);
(* if asked, prove knowledge of newauth *)
in(c, n);
out(c, hmac(newauth,n)).
process
new skTPM; (* secret part of a TPM key *)
let pkTPM = pk(skTPM) in
(* public part of a TPM key *)
new authdata;
(* the shared authdata *)
let kh = handle(pkTPM) in
out(c, (pkTPM, authdata, kh) );
( !User | !TPM )
Appendix 2: ProVerif Script for SKAP
free null, c, one, two.
fun enc/2. fun dec/2. fun senc/2. fun sdec/2.
fun hmac/2. fun pk/1. fun kdf/2. fun handle/1.
equation dec(sk, enc(pk(sk), m)) = m.
equation sdec(k, senc(k, m)) = m.
query attacker:newauth.
(* SECRECY HOLDS *)
query ev:successU(w,x,y,z) ==> ev:successT(w,z,y,z). (* CORRESPONDENCE HOLDS *)
let User =
(* request an OSAP session *)
new K;
new no;
out(c, (kh, enc(pkTPM, K)) );
in(c, (ah, ne));
let K1 = hmac(K, (authdata, ne, one)) in
let K2 = hmac(K, (authdata, ne, two)) in
(* request execution of a command, e.g. TPM_CreateWrapKey *)
new newauth;
out(c, ( no, senc(K2,(ne,no,newauth)), hmac(K1,(null,ne,no)) ) );
(* receive the response from the TPM, and check it *)
in(c, (response, hm) );
if hm = hmac( kdf(K1,newauth), response) then

216
L. Chen and M. Ryan
(* check that the TPM has newauth *)
new n;
out(c, n);
in(c, hm2);
if hm2=hmac(newauth,n) then
event
successU(K1, K2, response, newauth).
let TPM =
(* handle the request for an OSAP session *)
new ne;
in(c, encSessKey );
let K = dec(skTPM, encSessKey) in
out(c, ne);
let K1 = hmac(K, (authdata, ne, one)) in
let K2 = hmac(K, (authdata, ne, two)) in
(* execute a command from the user, e.g. TPM_CreateWrapKey *)
in(c, (no, encNewAuth, hm));
if hm = hmac(K1, (null,ne,no)) then
let (ne’,no’,newauth) = sdec(K2, encNewAuth) in
if ne’=ne then
if no’=no then
(* return a response to the user *)
new reponse;
out(c, ( response, hmac( kdf(K1,newauth), response) ));
event successT(K1, K2, response, newauth);
(* if asked, prove knowledge of newauth *)
in(c, n);
out(c, hmac(newauth,n)).
process
new skTPM; (* secret part of a TPM key *)
let pkTPM = pk(skTPM) in
(* public part of a TPM key *)
new authdata;
(* the shared authdata *)
let kh = handle(pkTPM) in
out(c, (pkTPM, authdata, kh) );
( !User | !TPM )

Trusted Multiplexing of Cryptographic Protocols
Jay McCarthy1 and Shriram Krishnamurthi2
1 Brigham Young University
2 Brown University
Abstract. We present an analysis that determines when it is possible to
multiplex a pair of cryptographic protocols. We present a transformation
that improves the coverage of this analysis on common protocol formula-
tions. We discuss the gap between the merely possible and the pragmatic
through an optimization that informs a multiplexer. We also address the
security ramiﬁcations of trusting external parties for this task and eval-
uate our work on a large repository of cryptographic protocols. We have
veriﬁed this work using the Coq proof assistant.
1
Problem and Motivation
A fundamental aspect of a cryptographic protocol is the set of messages that it
accepts. Protocol speciﬁcations contain patterns that specify the messages they
accept. These patterns describe an inﬁnite set of messages, because the variables
that appear in them may be bound to innumerable values. We call this set a
protocol’s message space.
There is a history of attacks on protocols based on the use of (parts of) mes-
sages of one protocol as (parts of) messages of another protocol [2,11,15]. These
attacks, called type-ﬂaw (or type-confusion) attacks, depend fundamentally on
the protocol relation of message space overlap. If the message spaces of two pro-
tocols overlap, then there is at least one session of each protocol where at least
one message could be accepted by both protocols. This property, however, is
more general than a “presence of type-ﬂaw attack” property, because not all
overlaps are indications of successful attacks. (In fact, it is common for new
versions of a protocol to contain many similar messages.)
The message space overlap property not only gives us insight into the protocol
and its relation to other protocols it also provides a test for a fundamental
deployment property: dispatchability. We deﬁne dispatchability as the ability
for a multiplexer to unambiguously deliver incoming protocol messages to the
proper protocol session. (We can compare a protocol session’s message space
with another session’s message space to determine if it is possible to dispatch
to the correct session. This basic property is necessary for servers to provide
concurrency and support for many protocol clients.)
Servers typically rely on tcp for this property. They assign a diﬀerent tcp
port for each protocol and trust the operating system’s tcp implementation to
do the dispatching. However, when cryptographic protocols are embedded in
other contexts, such as existing Web service protocols (e.g., soap), more ex-
plicit methods of distinguishing protocol messages must be used. Furthermore,
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 217–232, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

218
J. McCarthy and S. Krishnamurthi
by leaving this essential step implicit, it is not included in the formally veriﬁed
portion of the protocol speciﬁcation. This means that the protocol that is actu-
ally used is not the one that is veriﬁed. Finally, the delegated notion of a session
(e.g., tcp’s or ssl’s) may not match the protocol’s notion. This is particularly
problematic in protocols with more than two participants that are not simply
compositions of two-party protocols.
Notice that message space overlap implies that dispatchability is not achiev-
able. If there is a message M that could be accepted by session p and session q
of some protocols, then what would a dispatcher do when delivered M? A faulty
identiﬁcation might cause the actual (though unintended) recipient to go into an
inconsistent state or even leak information while the intended recipient starves.
It cannot unambiguously deliver the message and therefore is not a correct dis-
patcher. We present a dispatching algorithm that correctly delivers messages if
there is no message space overlap. This algorithm provides proof that the lack
of message space overlap implies dispatchability.
We present an analysis that determines whether the message spaces of two
protocols (sessions of a protocol) overlap. We also present an analysis, phrased
as an optimized dispatcher, that determines why there is no overlap between
two spaces by ﬁnding the largest abstractions of two protocols for which there
is no overlap. We present our analysis of protocols from the spore protocol
repository [16] and show they provide insights to improve our analyses.
We present our work in the context of an adaptation of cppl, the Crypto-
graphic Protocol Programming Language [8]. We have built an actual tool and
applied it to concrete representations of protocols. All of our work is formalized
using the Coq proof assistant [19], and we make our formalization freely avail-
able.1 Coq provides numerous advantages over paper-and-pencil formalizations.
First, we use Coq to mechanically check our proofs, thereby bestowing much
greater conﬁdence on our formalization and on the correctness of our theorems.
Second, because all proofs in Coq are constructive, our tool is actually a cer-
tiﬁed implementation that is extracted automatically in a standard way from
our formalization, thereby giving us conﬁdence in the tool also. Finally, being a
mechanized representation means others can much more easily adapt this work
to related projects and obtain high conﬁdence in the results.
Outline. In Sec. 2, we explain the technical background of our theory. Next, in
Sec. 3, we develop the decision procedure for message space overlap. In Sec. 4, we
show how message space overlap provides a suﬃcient foundation for a dispatching
algorithm. This algorithm is ineﬃcient, so we present an analysis in Sec. 5 that
optimizes it. Finally, we discuss related work and conclusions.
2
Introduction to CPPL
cppl [8] is a language for expressing cryptographic protocols with trust an-
notations. cppl allows the programmer to control protocol actions with trust
1 Sources are available at: http://faculty.cs.byu.edu/~jay/tmp/dispatch09/

Trusted Multiplexing of Cryptographic Protocols
219
a knows a:name b:name kab:symkey
learns kabn:symkey
b knows a:name b:name kab:symkey
learns kabn:symkey
1 a -> b : a, {|na:nonce|} kab
2 b -> a : {|na, nb:nonce|} kab
3 a -> b : {|nb|} kab]
4 b -> a : {|kabn:symkey, nbn:nonce|} kab
Fig. 1. Andrew Secure RPC Protocol
1 proc b (a:name b:name kab:symkey) _
2
let chana = accept in
3
recv chana (a, {| na:nonce |} kab) -> _ then
4
let nb = new nonce in
5
send _ -> chana {| na, nb |} kab then
6
recv chana {| nb |} kab -> _ then
7
let nbn = new nonce in
8
let kabn = new symkey in
9
send _ -> chana {| kabn, nbn |} kab then
10
return _ (kabn)
Fig. 2. Andrew Secure RPC Role B in cppl
constraints so that an action such as transmitting a message will occur only
when the indicated constraint is satisﬁed. The cppl semantics identiﬁes a set of
strands [18], annotated with trust formulas and the values assumed to be unique,
as the meaning of a role in a protocol. We will explain the relevant aspects of
cppl by using the Andrew Secure RPC protocol (Figure 1) and the encoding of
its b role in cppl (Figure 2) as our example.
Message Syntax. The various kinds of messages that may be sent and received
are paramount to our investigation. We give their syntax in Figure 3. Messages
(m) may be constructed by concatenation (,), hashing (hash(m)), variable bind-
ing and pattern matching (< v = m >), asymmetric signing ([m]v), symmet-
ric signing ([|m|]v), asymmetric encryption ({m}v), and symmetric encryption
({|m|}v). In these last four cases, v is said to be in the key-position. For exam-
ple, in the Andrew role kab is in key-position on line 3. Concatenation is right
associative. Parentheses are control precedence.
Well-formedness. As a cppl program executes, it builds an environment of lo-
cally known values associated with identiﬁers. This environment is consulted to
determine the values of pattern identiﬁers in message syntax and is extended dur-
ing matching when those identiﬁers are free. Not all syntactically valid messages
are well-formed in a cppl program, because they may refer to free identiﬁers in
positions that cannot be free. These patterns are not well-formed.

220
J. McCarthy and S. Krishnamurthi
m := nil
|
v
|
k
|
(m, m′)
|
hash(m)
|
< v = m >
|
[m]v
|
[|m|]v
|
{m}v
|
{|m|}v
v := x : t
t := text
|
msg
|
nonce
|
name
|
symkey
|
pubkey
|
channel
Fig. 3. cppl Message Syntax
Intuitively, to send a message we must be able to construct it, and to do that,
every identiﬁer must be bound. Therefore, a pattern m is well-formed for sending
in an environment σ (written σ ⊢s m herein) if all identiﬁers that appear in it
are bound. For example, the message on line 5 of the Andrew role is well-formed,
but if we removed line 4, it would not be because nb would not be bound.
Surprisingly, the well-formedness condition is diﬀerent for message patterns
used for receiving rather than sending: only some identiﬁers must be bound due
to meaning of the cryptographic primitives.
However, a similar intuition holds for using a message pattern to receive mes-
sages. When a message matches a pattern, the identiﬁers that conﬁrm its shape—
those that are used as keys or under a hash—must be known to the principal.
Thus, a pattern m is well-formed for receiving in σ (written σ ⊢r m) if all identi-
ﬁers that appear in key-positions or hashes are bound. For example, the pattern
on line 3 of the Andrew role is well-formed because kab is bound, but if it were
not, then the pattern would not be well-formed.
A cppl program (p) is well-formed (⊢p), when each message is well-formed
in the appropriate context and runtime environment.
Semantics and Adversary. The semantics of a cppl program is given by a
set of strands where each strand describes one possible local run. A strand, s, is
simply a list of messages that are sent (+m) or received (−m):
s := .
|
+m →s
|
−m →s
The adversary in the strand semantics is essentially the Dolev-Yao adver-
sary [5]. Since a strand merely speciﬁes what messages are sent and received
rather than how they are constructed, where they are sent, or from whence they
come, the adversary has maximal power to manipulate the protocol by modi-
fying, redirecting, and generating messages ex nihilo. This ensures that proofs
built on the semantics are secure in the face of a powerful adversary.
The basic abilities of adversary behavior that make up the Dolev-Yao model
include: transmitting a known value such as a name, a key, or whole message;
transmitting an encrypted message after learning its plain text and key; and
transmitting a plain text after learning a ciphertext and its decryption key. The
adversary can also manipulate the plain-text structure of messages: concate-
nating and separating message components, adding and removing constants,
etc. Since an adversary that encrypts or decrypts must learn the key from the

Trusted Multiplexing of Cryptographic Protocols
221
network, any key used by the adversary—compromised keys—have always been
transmitted by some participant.
A useful concept when discussing the adversary is a uniquely originating value.
This is a value that enters the network at a unique location. Locally produced
nonces2 are uniquely originating values. By deﬁnition, the adversary cannot
know these values until they have been sent in an unprotected context.
3
Analysis
In this section we present our analysis that determines when there is a message
that could be accepted by two sessions of two protocols. Two sessions of one
protocol can be analyzed by comparing a protocol with itself.
The strand space model of protocols is aptly suited for this problem. From the
strand, we can read oﬀeach message pattern the protocol accepts. For example,
the strand +m1 →−m2 →−m3 →. accepts messages with patterns m2 and
m3. We denote this set of message patterns as M(s) for strand s.
Each message pattern m describes an inﬁnite set of messages (one for each
instantiation of the variables in m) that would be accepted at that point of the
protocol. If we could compare the sets of two patterns, then we could easily lift
this analysis to two protocols s and s′ by checking each pattern in M(s) against
each pattern in M(s′). The essence of our problem is therefore determining
when a message pattern m “overlaps” with another message pattern m′, i.e.,
when there is an actual message M that could be matched by both m and m′.
We call this analysis match.
3.1
Deﬁning match
We have many options when deﬁning match. We could assume that the structure
of message patterns are ambiguous. That is, we could assume that (m1, m2) could
possibly overlap with hash(m3) or {m4}k. We will not do this. We assume that
messages are encoded unambiguously. Concrete protocol implementations that
do not conform to this assumption may have type-ﬂaw attacks [2,11,15].
This initial consideration shrinks the design space of match: message patterns
must have identical structure for them to possibly overlap. There are two im-
portant caveats: variables with type msg and bind patterns (< v = m >). In
the ﬁrst, we treat such variables as “wildcards” because they will accept any
message when used in a pattern. In the second, we ignore the variable binding
and use the sub-pattern m in the comparison.
With this structural means of determining when two message patterns poten-
tially overlap, all that remains is to specify when to consider two variables as
potentially overlapping. The simplest strategy is to assume that if the types of
two variables are the same, then it is possible that each could refer to the same
value. We call this strategy type-based and write it matchτ.
2 Numbers used once.

222
J. McCarthy and S. Krishnamurthi
Correctness. matchτ is correct if it soundly approximates message space over-
lap, i.e., if ¬ matchτ m m′ then there is no overlap between the possible messages
accepted by pattern m and pattern m′. This implies that matchτ m m′ should
not be read as “every message accepted by m is accepted by m′” (or vice versa),
because there are some environments (and therefore protocol sessions) where
there can be no overlap between messages. For example, the pattern x does not
overlap with y if x is bound to 2 and y is bound to 3. But there is at least one
environment pair that contains at least one message that is accepted by both:
when x and y are bound to 2 and the message is 2.
Evaluation. The theorem prover can tell us if matchτ is correct, but it cannot
tell us if the analysis is useful. We address the utility of the analysis by running
it on a large number of protocol role pairs.
We have encoded 121 protocol roles from 43 protocol deﬁnitions found in
the Security Protocols Open Repository (spore) [16] in cppl. For each role, our
analysis generates every possible strand interpretation of the role, then compares
each message pattern with those of another role. Analyzing all possible pairs only
takes a few seconds and we ﬁnd that when using matchτ, 15.7% of protocol role
pairs are non-overlapping (i.e., for 84.3% of the pairs there is a message that is
accepted by both roles in a run.) This is an extravagantly high number.
If we actually look at the source of many protocols in cppl, we learn why there
are such poor results with matchτ. Many protocols have the following form:
1
recv chan (m_1, m:msg) -> _ then
...
n
match m m_2 then
where m1 and m2 are particular patterns, such as (price, p) or {m1}k.
Consider how matchτ would compare this message with another: Because it
contains a wildcard message (with type msg), it is possible for any message to be
accepted. This tells us that the speciﬁcity of the protocol role impacts the eﬃcacy
of our analysis. In the next section, we develop a transformation on protocol roles
that increases their speciﬁcity. This greatly improves the performance of matchτ.
3.2
Message Speciﬁcity
Suppose we have a protocol with the following protocol role:
1
recv ch (m1, a) -> _
2
then let nc = new nonce in
3
match m1 {|b, k’|} k -> _
If this role were slightly diﬀerent, then we could execute it with more partners:
1’ recv ch (<m1={|b, k’|} k>, a) -> _
2
then let nc = new nonce in
3
match m1 {|b, k’|} k -> _

Trusted Multiplexing of Cryptographic Protocols
223
In this modiﬁed protocol, the wildcard message m1 on line 1 is replaced by
a more speciﬁc pattern on line 1′. We say that message pattern m1 is more
speciﬁc than message pattern m2 if for all messages m, matchτ m1 m implies
matchτ m2 m (i.e., every message that is accepted by m1 is accepted by m2.)
Our transformation, called foldm, increases the speciﬁcity of message patterns.
It works as follows: for each message reception point where message m is received,
foldm records the environment before reception as σm, inspects the rest of the
role for pattern points where identiﬁer i is compared with pattern p such that
σm ⊢r p, and replaces each occurrence of i in m with < i = p >, thereby
increasing the speciﬁcity of m.
We prove the following theorems about this transformation:
Theorem 1. If ⊢p then ⊢foldm p.
Theorem 2. Every pattern in p has a more speciﬁc pattern in foldm p.
Preservation. We must also ensure that this transformation preserves the se-
mantics of the protocol meaningfully. However, since we are clearly changing the
set of messages accepted by the protocol (requiring them to be more speciﬁc),
the transformed protocol does not have the same meaning.
The fundamental issue is whether the protocol meaning is diﬀerent. Recall that
the meaning of a protocol is a set of strands that represent potential runs. This
is smaller after the transformation. However, if we consider only the runs that
end in success—those runs in which a message matching pattern p is provided
when expected—then there is no diﬀerence in protocol behavior.
Why? Consider the example from above. Suppose that a message M matching
the pattern (m1, a) is provided at step 1 in the original protocol and that the
rest of protocol executes successfully. Then m1 must match the pattern {|b,
k’|} k, and, the message M must match the pattern (<m1={|b, k’|} k>, a).
Therefore, if the same message was sent to the transformed protocol, the protocol
would execute successfully. This holds in every case because the transformation
always results in more speciﬁc patterns that have exactly this property.
What happens to runs that fail in the original protocol? They continue to fail
in the transformed protocol, but may fail diﬀerently. Suppose that a message M
is delivered to the example protocol at step 1 and the protocol fails. It either
fails at step 1 or step 3. If it fails at step 1, then it does not match the pattern
(m1,a) or the pattern (<m1={|b, k’|} k>, a). Therefore it fails at step 1 in
the transformed protocol as well. If it fails at step 3, then the left component of
the message M does not match the pattern {|b,k’|} k, and, the transformed
protocol will fail at step 1 for the very same reason.
In general, then, the transformed protocol’s behavior is identical modulo fail-
ure. If the same sequence of external messages is delivered to a transformed role,
then it will either (a) succeed like the untransformed counterpart or (b) fail ear-
lier because some failing pattern matching was moved earlier in the protocol.
Semantically, this means that the set of strand bundles that a protocol can be a
part of is smaller. This could be thought of as a fault preserving transformation
in the style of Lowe [12].

224
J. McCarthy and S. Krishnamurthi
The transformed protocol must actually be used in deployment for the analysis
to be sound. If not, a message may be delivered to the wrong recipient. Worse,
this mis-delivery will only be apparent later when the principal attempts a deeper
pattern match. Since the more speciﬁc pattern was not matched initially, this
deep match will fail and signal an error.
Adversary. This transformation either decreases the amount of harm the ad-
versary can do or does not change it. Since the only diﬀerence in behavior is
that faulty messages are noticed sooner, whatever action the principal would
have taken before performing the lifted pattern matching is not done. Therefore,
the principal does less before failing, and therefore the “hooks” for the adversary
are decreased. Of course, for any particular protocol, these hooks may or may
not be useful, but in general there are fewer hooks.
Evaluation. When we apply foldm to our test suite of 121 protocol roles and
then run the matchτ analysis, we ﬁnd within seconds that the percentage of non-
overlapping role pairs increases from 15.7% to 61%. This means that for 61% of
protocol role pairs from our repository, it is always possible to unambiguously
deliver a message to a single protocol handler. However, when we look just at the
special case of comparing a role with itself (i.e., determining if it is possible to
dispatch to sessions correctly) we ﬁnd that none of the roles have this property
according to matchτ.
This is an unsurprising result. Every message pattern p is exactly the same
as itself. Therefore, matchτ will resolve that p has the same shape as p and
could potentially accept the same messages. The problem is that matchτ looks
only at the two patterns. It does not consider the context in which they appear:
a cryptographic protocol that may make special assumptions about the values
bound to certain variables. In particular, some values are assumed to be unique.
For example, in many protocols, nonces are generated randomly and used to
prevent replay attacks and conduct authentication tests [7]. In the next section,
we incorporate uniqueness into our analysis.
3.3
Relying on Uniqueness
In the Andrew Secure RPC role (Fig. 2), the message received on line 6 must
match the pattern {|nb|}kab, where nb is a nonce that was freshly generated
on line 4. This means that no two sessions of this role could accept the same
message at line 6, because each is waiting for a diﬀerent value for nb.
We call the version of our analysis that incorporates information about unique-
ness matchδ. Whenever the analysis compares a variable u from protocol α and
a variable v from protocol β, if u is in the set of unique values generated by α
or v is in the set of unique values generated by β, then the two are assumed not
to match, regardless of anything else about the variables. In all other cases, two
variables are assumed to be potentially overlapping. In particular, the types are
ignored, unlike matchτ.

Trusted Multiplexing of Cryptographic Protocols
225
Table 1. Analysis Results
matchτ matchδ matchτ+δ
initial 15.7% 10.0%
15.8%
foldm 61.0% 55.2%
62.1%
matchτ matchδ matchτ+δ
initial
0.0%
00.8%
00.8%
foldm
0.0%
14.8%
14.8%
ι + foldm 31.4% 62.8%
62.8%
(a) Non-overlapping Protocol Role Pairs
(b) Non-overlapping Protocol Role Sessions
Evaluation. When we apply matchδ to our test suite, we ﬁnd that the percent-
age of non-overlapping sessions is 0.8%. After applying the foldm transformation,
this increases to 14.8%. There is no degradation to the performance of the anal-
ysis either: the entire test suite results are available almost instantaneously.
If we look at the other 85.2% of the protocols, is there anything more that can
be incorporated into the analysis? There is. The ﬁrst action of many protocol
roles is to receive a particular initiation message. Since this is the ﬁrst thing
the role does, it cannot possibly contain a unique value generated by the role.
Therefore, the matchδ analysis will not be able to ﬁnd a unique value that dis-
tinguishes the session that the message is meant for. In the next section, we will
discuss how to get around this diﬃculty.
3.4
Handling Initial Messages
The ﬁrst thing the Andrew Secure RPC role (Fig. 2) does (shown on line 3)
is receive a certain message: (a, {|na|}kab). Since this message does not contain
any value uniquely generated for the active session role, it seems that the initial
messages of two sessions can be confused. A little reﬂection reveals that initial
messages create sessions, thus they may not be confused across sessions.
Therefore, we can ignore the ﬁrst message of a protocol role, if it is not
preceded by any other action, for the purposes of determining the dispatchability
of a protocol role’s sessions. We must, of course, compare the initial message with
all other messages to ensure that the initial message cannot be confused with
them, but we do not need to compare the initial message with itself. When we
use this insight with the matchδ analysis, we write it as matchι(δ).
Evaluation. Table 1a presents the results when analyzing each pair of protocol
roles. Interestingly, unique values are not very useful when comparing roles,
although they do increase the coverage slightly. We have inspected the protocols
not handled by matchτ+δ to determine why the protocol pairs may potentially
accept the same message.
1. Protocols with similar goals and similar techniques for achieving those goals
typically have the same initial message. Examples include the Neumann
Stubblebine, Kao Chow, and Yahalom protocol families.
2. Diﬀerent versions of the same protocol will often have very similar messages,
typically in the initial message, though not always. Often these protocols are

226
J. McCarthy and S. Krishnamurthi
modiﬁed by making tiny changes so that the other messages remain identical.
A good example is the Yahalom family of protocols.
3. Some protocols have messages that cannot be reﬁned by foldm because the
key necessary to decrypt certain message components must be received from
another message or from a computation. This leaves a message component
that will match any other message, so such protocols cannot be paired with
a large number of other protocols. One example is the S role of Yahalom.
4. For many protocols, there is dependence among the pattern-matching in the
continuation of message reception. (One example is the P role of the Woo Lam
Mutual protocol.) As a result, only the independent pattern is substituted
into the original message reception pattern. This leaves a variable in the
pattern that matches all messages.
Table 1b presents the results when analyzing the sessions of each protocol role.
It may seem odd that the matchι(τ) analysis is able to verify any sessions, given
our argument against matchτ. Why should removing the initial message make
any diﬀerence? In 31.4% of the protocols, the protocol receives only a single,
initial message. We have also inspected the protocols that the most permissive
session-based analysis rules out.
1. Some messages simply do not contain a unique value. A prominent example
is the A role of many variants of the Andrew Secure RPC protocol.
2. Some roles have the same problems listed above as (3) and (4), except that
in these instances the lack of further reﬁnement hides a unique value. One
example is the C role of the Splice/AS protocol.
Performance. Computing these tables takes about two minutes.
4
Dispatching
Our analysis determines when there is no message that could be confused during
any run of two protocols. We can use this property to build a dispatching algo-
rithm. The algorithm is very simple: forward every incoming message to every
protocol handler. (For sessions, we must recognize the initial message and create
a new session; otherwise, forward the message to each session.)
This algorithm is correct because every message that is accepted by some
protocol (session) is only accepted by one protocol (session), according to the
overlap property. This (absurd) algorithm makes no attempt to determine which
protocol an incoming message is actually intended for. This is clearly ineﬃcient.
Yet, it shows that distinct message spaces are suﬃcient for dispatching.
In a network load-balancing setting, where “forwarding a message” actually
corresponds to using network bandwidth, this algorithm betrays the intent of
load-balancing. On a single machine, where “forwarding a message” corresponds
to invoking a handling routine, there are two major costs: (1) a linear search
through the various protocol/session handlers; and, (2) the cpu cost associated
with each of these handlers. In some scenarios, cost 2 is negligible because most

Trusted Multiplexing of Cryptographic Protocols
227
Nil
nil ↓σ= nil
Var
v ↓σ= v
Const
k ↓σ= k
Join
(m, m′) ↓σ= (m ↓σ, m′ ↓σ)
Hash
σ ⊢s hash(m)
hash(m) ↓σ= hash(m)
Hash (Wild)
σ ⊬s hash(m)
hash(m) ↓σ= ∗
SymEnc
k ∈σ
{|m|}k ↓σ= {|m ↓σ |}k
SymEnc (Wild)
k /∈σ
{|m|}k ↓σ= ∗
. . .
Bind
< v = m >↓σ=< v = m ↓σ>
Fig. 4. Message Redaction
network servers are not cpu-bound. However, since we are dealing with cryp-
tographic protocols, the cost of performing decryption only to ﬁnd an incorrect
nonce, etc., is likely to be prohibitive.
A better algorithm would use a mapping from input patterns to underlying
sessions and eﬃciently compare new messages with patterns in the mapping prior
to delivery. The main problem with this mapping algorithm is that it requires
trust in the dispatcher: the dispatcher must look inside encrypted messages to
determine which protocol (session) they belong to. In the next section we discuss
how to (a) minimize and (b) characterize the amount of trust that must be given
to a dispatcher of this sort to perform correct dispatching.
5
Optimization
Our task in this section is to determine how much trust, in the form of secret
data (e.g., keys), must be given to a dispatcher to inspect incoming messages
to the point that they can be distinguished. First, we will formalize how deep
a dispatcher can inspect any particular message with a certain amount of infor-
mation. Second, we will describe the process that determines the optimal trust
for any pair of protocols (or any pair of sessions of one protocol.) Finally, we
formalize the security repercussions of this trust. The end result of this section
is a metric of how eﬃcient dispatching can be for a protocol; all protocols should
aspire to require no trust in the dispatcher.
Message Redaction. Suppose that a message is described by the pattern
(a, {|b|}k). If the inspector of this message does not know key k, then in general3
this message is not distinguishable from (a, ∗). We call this the redaction of pat-
tern (a, {|b|}k) under an environment that does not contain k. We write m ↓σ
to denote the redaction of message m under σ. This is deﬁned in Figure 4.
3 There are kinds of encryption that allow parties without knowledge of a key to know
that some message is encrypted by that key but still not know the contents of the
message.

228
J. McCarthy and S. Krishnamurthi
0%
25%
50%
75%
100%
Percentage of Total Trust Necessary
0%
10%
20%
30%
40%
50%
60%
Percentage of Protocol Role Pairs
0%
25%
50%
75%
100%
Percentage of Total Trust Necessary
0%
10%
20%
30%
40%
50%
60%
Percentage of Protocol Roles
(a)
(b)
Fig. 5. Trust Optimization Graphs
Theorem 3. An environment σ can interpret m ↓σ: for all σ and m, σ ⊢r m ↓σ.
Theorem 4. Every message that is matched by m is matched by m ↓σ. (Sec. 3.2)
Theorem 5. σ ⊢r m implies m ↓σ= m.
These theorems establish that m ↓σ captures the view that a dispatcher, that is
trusted with σ only, has of a message m. The next task is to minimize σ while
ensuring that match can rule out potential message confusion.
Minimizing
σ.
Suppose
we
compare
m
=
({|b|}k, {|c|}j)
with
m′ = ({|b′|}k′, {|c′|}j′), where b and b′ are unique values of their respective
protocols, with matchτ+δ. Because b and b′ are unique, the analysis, and there-
fore the dispatcher, needs to look at b and b′ only to ensure that these message
patterns cannot describe the same messages. This means that even though the
patterns mention the keys k and j (k′ and j′), only k (k′) is necessary to distin-
guish the messages. Another way of putting this is that m ↓{k}= ({|b|}k, ∗) does
not overlap with m′ ↓{k′}= ({|b′|}k′, ∗), according to matchτ+δ.
We prove that if m and m′ cannot be confused according to match, then there
is a smallest set σ, such that m ↓σ also cannot be confused with m′ ↓σ according
to match. We prove this by showing that for all m, there is a set Vm, such that
for all σ, m ↓Vm∪σ= m ↓Vm. In other words, there is a “strongest” set for ↓
that cannot be improved. This set is the set σ such that σ ⊢r m. Our brute-force
search construction algorithm then considers each subset of Vm (Vm′) and selects
the smallest subset such that the two messages are distinct after ↓.
We have run this optimization on our test-suite of 121 protocol roles; it takes
about one minute total to complete. Figure 5a breaks down protocol pairs ac-
cording to the percentage of their keys required to establish trust. This graph
shows that 43% of protocol pairs do not require any trust to properly dispatch.
The other end of the graph shows that only 18% of all protocol pairs require
complete trust in the dispatcher. Figure 5b shows the same statistics for protocol
sessions. In this situation, 54% of the protocol roles do not require any trust for

Trusted Multiplexing of Cryptographic Protocols
229
the dispatcher to distinguish sessions, while 37% require complete trust. These
results were calculated in 7.6 minutes and 1.6 seconds respectively.
These experiments indicate that it is very fruitful to pursue optimizing the
amount of trust given to a dispatcher. However, we have not yet characterized
the security considerations of this trust. We do so in the next section.
Managing Trust. In previous sections, we have discussed how much trust to
give to a load-balancer so it can dispatch messages correctly. In this section, we
provide a mechanism for determining the security impact of that trust.
Recall that a strand is a list of messages to send and receive. We have formal-
ized “trust” as a set of keys (and other data) to be shared with a load-balancer.
We deﬁne a strand transformation ↑k that transforms a strand s such that it
shares k by sending a particular message containing k as soon as possible. (It is
trivial to lift ↑k to share multiple values.) We deﬁne s ↑k as:
(sd →s) ↑k = sd →+(LB, v) →s if k ∈bound(sd)
(sd →s) ↑k = sd →(s ↑k) if k /∈bound(sd)
. ↑k = .
(This deﬁnition clearly preserves well-formedness and performs its task.) In this
deﬁnition the tag LB indicates that this value is shared with the load-balancer by
some means. Depending on the constraints of the environment, this means can
be assumed to be perfectly secure or have some speciﬁc implementation (e.g.,
by using a long-term shared key or public-key encryption.)
Since s ↑k is a strand, it can be analyzed using existing tools and tech-
niques [4,6,10,17] to determine the impact of an adversary load-balancer.
6
Insights
The development of the message space overlap analysis and the trust optimiza-
tion give us insight into why and how message spaces do not overlap.
The eﬀectiveness of matchτ for pairs of protocols demonstrates that it is pri-
marily shape that prevents overlap between diﬀerent protocols. This corresponds
with our intuitions, because protocols typically use dissimilar formats.
The disparity between matchτ and matchδ demonstrates that for pairs of
protocol sessions, it is uniquely originating values that prevent overlap. This
matches our intuition, because nonces are consciously designed to prevent replay
attacks and ensure freshness, which correspond to the goal of identifying sessions.
The statistical diﬀerences between these two analyses in diﬀerent settings
allow us to make these conclusions in a coarse way. But the trust optimization
process answers the real question: “Why do two message spaces not overlap?”
When the trust optimization redacts a message, it is removing the parts of
the message that are not useful for distinguishing that protocol (session). This
means that what remains is useful, and thus the fully redacted message is only
what is necessary to ensure that there is no message space overlap. Thus, for any
two protocols (sessions), trust optimization explains why there is no overlap.

230
J. McCarthy and S. Krishnamurthi
7
Related Work
Previous Work. In prior work with Guttman and Ramsdell [14], we only ad-
dressed the question of when a protocol role supports the use of multiple sessions.
In addition, that approach was signiﬁcantly diﬀerent from the one presented here.
Though we presented a program transformation similar to foldm, we did not for-
malize the correctness of the transformation. Second, we used only the na¨ıve
dispatching algorithm and did not investigate a more useful algorithm. Third,
we did not consider pairs of protocols. Therefore, the current presentation is
more rigorous, practical, and general.
Our previous problem was only to inspect protocol role message patterns for
the presence of distinguishing (unique) values. This is clearly incorrect in the
case of protocol role pairs. Consider the role A, which accepts the message Ma,
then (Na, ∗), and role B, which accepts the message Mb, then (∗, Nb), where Nx
is a local nonce for x. Each message pattern of each role contains a distinguishing
value, so it passes the analysis. But it is not deployable with the other protocol
because it is not possible to unambiguously deliver the message (Na, Nb) after
the messages Ma and Mb have been delivered.
It is actually worse than this. We can encode these two protocols as one
protocol: accept either Ma or Mb, then depending on the ﬁrst message, accept
(Na, ∗) or (∗, Nb). Our earlier analysis would ignore the initial messages (which
is problematic in itself if Ma and Mb overlap), then check all the patterns in
each branch, and report success. This is clearly erroneous because it is possible
to confuse an A session with a B session.
This work avoids these problems by directly phrasing the problem in terms of
deciding message overlap—the real property of interest rather than a proxy to it
as distinguishing values were. It is useful to point out, however, that the earlier
work was sound for protocol roles that did not contain branching, which is an
very large segment of our test suite. Our use of Coq ensures that our analysis is
correct for all protocols.
Dispatching. The Guttman and Thayer [9] notion of protocol independence
through disjoint encryption and a related work by Cortier et al. [3] study the
conditions under which security properties of cryptographic protocols are pre-
served under composition with one or more other protocols. This is an important
problem, since it ensures that it is safe to compose protocols. A fundamental
result of the Guttman study shows that diﬀerent protocols must not encrypt
similar patterns by the same keys—a similar conclusion to some of our work.
However, our work complements theirs by studying whether it is possible to
compose protocols and, in particular, how eﬃcient such a multiplexer can be.
Ideally both of these problems must be addressed before deployment.
Detecting type-ﬂaw attacks [2,11,15] is a similar problem to ours. These at-
tacks are based on the inability of a protocol message receiver to unambiguously
determine the shape of a message. For example, a nonce may be sent where
the receiver expects a key, a composite message may be given in place of a key,
etc. These attacks are often eﬀective when they force a regular participant into

Trusted Multiplexing of Cryptographic Protocols
231
using known values as if they were keys. Detecting when a particular attack is
a type-ﬂaw attack, or when components of a regular protocol execution may be
used as such, is similar to our problem. These analyses try to determine when
sent message components can be confused with what a regular participant ex-
pects. However, in these circumstances a peculiar notion of message matching
captures the ambiguity in bit patterns. Some analyses use size-based matching
where any message of n-bits can be accepted by a pattern expecting n-bits; for
example, an n-bit nonce can be considered an n-bit key. Others assume that mes-
sage structure is discernible but the leaf-types are not, so a nonce paired with
a nonce cannot be interpreted as single nonce, but it may be interpreted as a
nonce paired with a key. Our analysis is similar in spirit but diﬀers in the notion
of message overlap: we assume that message shapes can be encoded reliably.
Optimization. The problem of optimizing the amount of trust given to a dis-
patch is very similar in spirit to ordering of pattern-matching clauses [13] and
ordering rules in a ﬁrewall or router [1], which are both similar to the decision
tree reduction problem. But, our domain is much simpler than the general do-
main of these problems and the constants are much smaller (|Vm| is rarely greater
than 3 for most protocols), so we are not aﬄicted with many of the motivating
concerns in those areas. Even so, these problems serve only as guidelines for the
actual optimization process, not the formulation of the solution.
8
Conclusion
We have presented an analysis (match) that determines if there is an overlap in
the message space of diﬀerent protocols (or sessions of the same protocol.) We
have shown how it is important to look at real protocols in the development of
this analysis (in our case, the spore repository [16].) By looking at real protocols,
we learned that it was necessary to (1) reﬁne protocol speciﬁcations (foldm), (2)
incorporate cryptographic assumptions about unique values (matchδ), and (3)
take special consideration of the initial messages of a protocol (matchι(δ)).
We have shown how this analysis and the message space overlap property can
be used to provide the correctness proof of a dispatching algorithm. We have
discussed the performance implications of this algorithm and pointed toward
the essential features of a better algorithm. We have developed a formalization
(↓σ) of the “view” that a partially trusted dispatcher has of messages. We have
presented an optimization routine that minimizes the amount of trust necessary
for match to succeed on a protocol pair. We have presented the results of this
analysis for the spore repository. We have also formalized the modiﬁcations
(↑k) that must be made to a protocol in order to enable trust of a load-balancer.
Lastly, we have discussed how this optimization explains why there is no overlap
between two message spaces. The entire work was formally veriﬁed in the Coq
theorem proof assistant to increase conﬁdence in our results.

232
J. McCarthy and S. Krishnamurthi
Acknowledgments. This work is partially supported by the NSF (CCF-0447509,
CNS-0627310, and a Graduate Research Fellowship), Cisco, and Google. We are
grateful for the advice of Joshua Guttman and John Ramsdell.
References
1. Begel, A., McCanne, S., Graham, S.L.: BPF+: exploiting global data-ﬂow optimiza-
tion in a generalized packet ﬁlter architecture. In: Symposium on Communications,
Architectures and Protocols (1999)
2. Bodei, C., Degano, P., Gao, H., Brodo, L.: Detecting and preventing type ﬂaws:
a control ﬂow analysis with tags. Electronic Notes in Theoretical Computer Sci-
ence 194(1), 3–22 (2007)
3. Cortier, V., Delaitre, J., Delaune, S.: Safely Composing Security Protocols. In: Con-
ference on Foundations of Software Technology and Theoretical Computer Science
(2007)
4. Doghmi, S.F., Guttman, J.D., Thayer, F.J.: Skeletons, homomorphisms, and
shapes: Characterizing protocol executions. Electronic Notes in Theoretical Com-
puter Science, vol. 173, pp. 85–102 (2007)
5. Dolev, D., Yao, A.: On the security of public-key protocols. IEEE Transactions on
Information Theory 29, 198–208 (1983)
6. F´abrega, F.J.T., Herzog, J.C., Guttman, J.D.: Strand spaces: Why is a security
protocol correct? In: IEEE Symposium on Security and Privacy (1998)
7. Guttman, J.D.: Authentication tests and disjoint encryption: a design method for
security protocols. Journal of Computer Security 12(3/4), 409–433 (2004)
8. Guttman, J.D., Herzog, J.C., Ramsdell, J.D., Sniﬀen, B.T.: Programming crypto-
graphic protocols. In: Trust in Global Computing (2005)
9. Guttman, J.D., Thayer, F.J.: Protocol independence through disjoint encryption.
In: Computer Security Foundations Workshop (2000)
10. Guttman, J.D., Thayer, F.J.: Authentication tests and the structure of bundles.
Theoretical Computer Science 283(2), 333–380 (2002)
11. Heather, J., Lowe, G., Schneider, S.: How to prevent type ﬂaw attacks on security
protocols. In: Computer Security Foundations Workshop (2000)
12. Hui, M.L., Lowe, G.: Fault-perserving simplifying transformations for security pro-
tocols. Journal of Computer Security 9(1-2), 3–46 (2001)
13. Lee, P., Leone, M.: Optimizing ML with run-time code generation. Programming
Language Design and Implementation (1996)
14. McCarthy, J., Guttman, J.D., Ramsdell, J.D., Krishnamurthi, S.: Compiling cryp-
tographic protocols for deployment on the Web. In: World Wide Web, pp. 687–696
(2007)
15. Meadows, C.: Identifying potential type confusion in authenticated messages. In:
Computer Security Foundations Workshop (2002)
16. Project EVA. Security protocols open repository (2007),
http://www.lsv.ens-cachan.fr/spore/
17. Song, D.X.: Athena: a new eﬃcient automated checker for security protocol anal-
ysis. In: Computer Security Foundations Workshop (1999)
18. Thayer, F.J., Herzog, J.C., Guttman, J.D.: Strand spaces: Proving security proto-
cols correct. Journal of Computer Security 7(2/3), 191–230 (1999)
19. The Coq development team. The Coq proof assistant reference manual, 8.1 edn.
(2007)

Specifying and Modelling Secure Channels in
Strand Spaces
Allaa Kamil and Gavin Lowe
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford, OX1 3QD, UK
{allaa.kamil,gavin.lowe}@comlab.ox.ac.uk
Abstract. We adapt the Strand Spaces model to reason abstractly
about layered security protocols, where an Application Layer protocol
is layered on top of a secure transport protocol. The model abstracts
away from the implementation of the secure transport protocol and just
captures the properties that it provides to the Application Layer. We
illustrate the usefulness of the model by using it to verify a small single
sign-on protocol.
1
Introduction
Many security architectures make use of layering of protocols: a special-purpose
Application Layer protocol is layered on top of a general-purpose Secure Trans-
port Layer protocol, such as SSL/TLS [Tho00]. The secure transport protocol
provides a secure channel to the Application Layer, i.e., it provides a commu-
nication channel with some extra security services such as authentication and
conﬁdentiality. The Application Layer protocol builds on this to provide extra
functionality and security guarantees.
As an example, one common use of such layered architectures is in Single-
Sign-On (SSO) protocols.
In such protocols, a User seeks to access services
provided by a Service Provider; the User is authenticated by a trusted Identity
Provider.
Typically, the User can open a unilateral TLS connection to the
Service Provider, which authenticates the Service Provider but not the User.
Further, the User can open a unilateral TLS connection to the Identity Provider,
which authenticates the Identity Provider; the User then provides a password
to authenticate herself. The SSO protocol builds upon these secure channels to
allow the User to authenticate herself to the Service Provider. The SAML SSO
Protocol [OAS05] is one such protocol.
However, the use of secure channels is not enough to ensure the security of
the application protocol. For example, Google adapted the SAML SSO for use
with Google Apps [Goo08]. Unfortunately, this adaptation introduced a ﬂaw,
reported in [ACC+08].
The aim of our research programme is to investigate how to analyse such
layered protocols. In this paper, we extend the Strand Spaces model [THG98]
in order to specify and model layered protocols.
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 233–247, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

234
A. Kamil and G. Lowe
One way to analyse such layered protocols would be to explicitly model both
layers; this is the approach taken in [HSN05]. We take the view that it is better
to abstract away from the implementation of the Secure Transport Layer and
simply to model the services it provides to the Application Layer. This greatly
simpliﬁes the analysis of the architecture. Further, such an analysis produces
more general results: it allows us to deduce the security of the Application
Layer protocol when layered on top of an arbitrary secure transport protocol
that provides (at least) the assumed services.
Of course, this approach introduces a proof obligation that the secure trans-
port protocol does indeed provide the services assumed of it. However, such
a proof only needs to be done once per transport protocol. An example such
proof, for bilateral TLS, appears in [Kam09, KL09]. Such proofs tend to assume
that the two layers are independent, so that no message can be replayed from
one layer to the other.
Diﬀerent secure transport protocols will allow or prevent diﬀerent actions
by a dishonest penetrator.
Any reasonable transport protocol will allow the
penetrator to take part in sessions, to send messages using his own identity,
and receive messages intended for him.
Some transport protocols will keep
Application Layer messages conﬁdential, such as the transport protocol that
encodes Application Layer message m from A to B as
TL1A,B(m) = A, B, {m}PK(B)
(where PK(B) is B’s public key). But others will allow the penetrator to learn
them, such as the transport protocol that encodes m from A to B as
TL2A,B(m) = A, B, {m}SK(A)
[CGRZ03] (where SK(A) is A’s secret key). Some transport protocols will allow
the penetrator to fake messages, causing a regular (i.e. honest) agent to receive
an arbitrary Application Layer message known to the penetrator, apparently
from some third party; this is the case with encoding TL1 but not TL2. Finally,
some transport protocols will allow the penetrator to hijack messages, changing
either the intended recipient or the apparent sender of the message; for example,
with encoding TL2, the penetrator may transform the Transport Layer message
into A, C, {m}SK(A) and redirect it to C; alternatively, with encoding TL1, the
penetrator may transform the Transport Layer message into C, B, {m}PK(B)
and re-ascribe it to C.
Our approach is to build an abstract model that describes each of these poten-
tial penetrator actions —sending, receiving, learning, faking and hijacking— as
a high-level penetrator strand. Of course, the penetrator may also build Appli-
cation Layer messages himself, pick them apart, or otherwise transform them; we
capture these abilities using (slightly adapted versions of) standard penetrator
strands.
We assume that diﬀerent transport protocols deployed in the same system
are independent, in the sense that the penetrator cannot directly transform a
message sent over one transport protocol into a message over another protocol,
other than by performing a receive or learn followed by a send or fake.

Specifying and Modelling Secure Channels in Strand Spaces
235
In the next section we present the foundations of the model, describing the
way we abstractly represent Transport Layer messages and the penetrator’s pos-
sible actions in high-level bundles. In Section 3 we describe how to specify the
properties of secure channels by disallowing appropriate high-level penetrator
strands. In Section 4 we prove a normal form lemma that subsequently allows
us to restrict our attention to bundles in a particular form. We illustrate our
model in Section 5 by using it to analyse a small single sign-on protocol. We
sum up and discuss forthcoming work in Section 6.
Related work. The work closest to the current paper is [DL08, Dil08]. That
paper uses a CSP-style formalism [Ros98], with a view towards analysing pro-
tocols using model checking. That paper, like this, deﬁnes potential capabilities
of the penetrator, and then speciﬁes secure channels by limiting those capabili-
ties. We see the two approaches as complementary: model checking is good for
ﬁnding attacks; the Strand Spaces approach is good for building the theoretical
foundations, and producing proofs of protocols that reveal why the protocol is
correct.
Armando et al. [ACC07] use LTL to specify security properties of channels,
and then use SATMC, a model checker for security protocols, to analyse a fair
exchange protocol. In [ACC+08] they analyse SAML SSO and the Google Apps
variant using the same techniques.
Bella et al. [BLP03] adapt the inductive
approach to model authenticated and conﬁdential channels, and use these ideas
to verify a certiﬁed e-mail delivery protocol. Bugliesi and Focardi [BF08] model
secure channels within a variant of the asynchronous pi-calculus. Each of these
works captures the properties of secure channels by limiting the penetrator’s
abilities regarding messages on such channels, although each considers fewer
variants of authenticated channels than the current paper.
As noted above, our work is mainly targeted at layered protocol architectures.
However, it can also be used to model empirical channels, where some messages
of a protocol are implemented by human mediation, and so satisfy extra security
properties. Creese et al. [CGRZ03] capture such empirical channels, within the
context of CSP model checking, again by restricting the penetrator’s abilities.
2
The Abstract Model: High-Level Bundles
In this section we present high-level bundles, which abstractly model secure
transport protocols. We present high-level terms, which capture Transport Layer
messages. We then adapt the notion of a strand space [THG98] to such high-level
terms. We then describe how we model the penetrator’s ability to manipulate
both Transport Layer and Application Layer messages. Finally, we deﬁne high-
level bundles.
2.1
High-Level Terms and Nodes
Let A be the set of possible messages that can be exchanged between principals
in a protocol. The elements of A are usually referred to as terms. As in the

236
A. Kamil and G. Lowe
original Strand Spaces model [THG98], A is freely generated from two disjoint
sets, T (representing tags, texts, nonces, and principals) and K (representing
keys) by means of concatenation and encryption.
Deﬁnition 1. Compound terms are built by two constructors:
– encr : K × A →A representing encryption;
– join : A × A →A representing concatenation.
Conventionally, {t}k is used to indicate that a term t is encrypted with a key k
and t0ˆt1 to denote the concatenation of t0 and t1.
The set K of keys is equipped with a unary injective symmetric operator
inv : K →K; inv(k) is usually denoted k −1. Let Tname ⊆T be the set of agent
names, ranged over by X , Y ; let Tpname ⊆Tname, ranged over by P, be the
set of names the penetrator uses when actively participating in a protocol as
himself; we let A, B range over names of regular agents.
As in the original Strand Spaces model, a strand is a sequence of message
transmissions and receptions. A node is the basic element of a strand. Each
node n is associated with a message, or high-level term, denoted msg(n). A
positive node is used to represent a transmission while a negative node is used to
denote reception. Each node communicates over a channel, which may provide
some security services to the message; a channel that does not provide any
security services is called the bottom channel, denoted ⊥. In our abstract model,
messages are modelled as follows.
Deﬁnition 2. Every node n in strand st is associated with a high-level term of
the form (X , Y , m, c) where:
– m ∈A is the Application Layer message.
– X ∈Tname: If n is positive and st is a regular strand, then X is the name
of the regular agent who is running st. Otherwise, X refers to the agent that
is claimed to have sent m.
– Y ∈Tname: If n is negative and st is a regular strand, then Y is the name
of the regular agent who is running st. Otherwise, Y refers to the agent that
is intended to receive m.
– c is the identiﬁer of the secure channel over which n communicates.
We write ˆ
A for the set of high-level terms.
We may use an underscore ( ) in the ﬁrst or second position of the tuple to
indicate that the term is not associated with a particular sender or receiver
respectively.
If n is a regular node then we assume that its term must be
associated with a speciﬁed sender and receiver.
The following deﬁnition is a straightforward adaptation from [THG98]. The
relation n →n′ represents inter-strand communication, while n ⇒n′ represents
ﬂow of control within a strand.
Deﬁnition 3. A directed term is a pair ⟨σ, a⟩with σ ∈{+, −} and a ∈ˆ
A; we
write it as as +t or −t. (± ˆ
A)∗is the set of ﬁnite sequences of directed terms.
A typical element of (± ˆ
A)∗is denoted by ⟨⟨σ1 , a1⟩, ..., ⟨σn, an⟩⟩. A strand space
over ˆ
A is a set Σ with a trace mapping tr : Σ →(± ˆ
A)∗. Fix a strand space Σ.

Specifying and Modelling Secure Channels in Strand Spaces
237
1. A node n is a pair ⟨st, i⟩, with st ∈Σ and i an integer satisfying 1 ≤i ≤
length(tr(st)).
The set of nodes is denoted by N.
We deﬁne msg(n) =
tr(st)(i). We will say the node n belongs to the strand st.
2. There is an edge n1 →n2 if and only if msg(n1) = +a and msg(n2) = −a
for some a ∈ˆ
A. The edge means that node n1 sends the message a, which
is received by n2, recording a potential causal link between those strands.
3. When n1 = ⟨st, i⟩, and n2 = ⟨st, i + 1⟩are members of N, there is an edge
n1 ⇒n2. The edge expresses that n1 is an immediate causal predecessor
of n2 on the strand st. n′ ⇒+ n is used to denote that n′ precedes n (not
necessarily immediately) on the same strand.
We now deﬁne the notions of origination and unique origination in the context
of a high-level strand space.
Deﬁnition 4. Let Σ be a high-level strand space.
1. Let I be a set of undirected terms. The node n ∈Σ is an entry point for I
iﬀn is positive and associated with a high-level term (A, B, m, c) for some
m ∈I, and whenever n′ ⇒+ n and n′ is associated with a high-level term
(A′, B′, m′, c′), m′ /∈I.
2. An undirected term t originates on a node n iﬀn is an entry point for the
set of messages that contain t as a subterm.
3. An undirected term t is uniquely originating in a set of nodes S ⊂N iﬀ
there is a unique n ∈S such that t originates on n.
2.2
The Penetrator
We can classify the activities of the penetrator, according to their eﬀects on
Application Layer messages:
– Actions that are used to construct or pick apart Application Layer messages;
– Actions that are used to handle high-level terms, aﬀecting the Transport
Layer “packaging” without modifying the corresponding Application Layer
messages.
The ﬁrst type of actions is used to transform and create messages of the form
( , , m, ⊥), i.e. messages that are sent on the bottom channel without being
associated with a particular sender or receiver. To model them, we adapt the
standard penetrator strands from [THG98] to handle high-level terms.
Deﬁnition 5. A standard penetrator strand in a high level bundle is one of the
following:
M. Text message: ⟨+( , , r, ⊥)⟩where r ∈TP;
K. Key: ⟨+( , , k, ⊥)⟩where k ∈KP;
C. Concatenation: ⟨−( , , t0, ⊥), −( , , t1, ⊥), +( , , t0ˆt1, ⊥)⟩;
S. Separation into components: ⟨−( , , t0ˆt1, ⊥), +( , , t0, ⊥), +( , , t1, ⊥)⟩;
E. Encryption: ⟨−( , , k, ⊥), −( , , t, ⊥), +( , , {t}k, ⊥)⟩where k ∈K;
D. Decryption: ⟨−( , , k −1, ⊥), −( , , {t}k, ⊥), +( , , t, ⊥)⟩where k ∈K.

238
A. Kamil and G. Lowe
The second type of penetrator actions only aﬀects the “packaging” of the
Application Layer message, i.e. it only aﬀects the ﬁrst, second, and fourth com-
ponents of a high-level term. These paths are used to perform the following
activities:
1. Send: the penetrator may send an Application Layer message m by creating
a Transport Layer message with payload m, and inserting it in the network
using a penetrator’s identity.
2. Receive: the penetrator may receive an Application Layer message m as
a payload of a Transport Layer message that was sent for him by a regular
agent.
3. Learn: the penetrator may intercept and learn an Application Layer mes-
sage m from a Transport Layer message with a payload m that was ex-
changed between regular agents.
4. Fake: the penetrator may fake an Application Layer message by creating a
Transport Layer message with payload m, and inserting it in the network
dishonestly (i.e. using another agent’s identity).
5. Hijack: the penetrator may change the sender and/or receiver ﬁeld in a
previously sent Transport Layer message without changing the Application
Layer message; the penetrator can perform hijacking in three ways [DL08]:
(a) Re-ascribe: the penetrator may re-ascribe a previously sent message
by intercepting and sending it using another agent’s identity.
(b) Redirect: the penetrator may redirect a previously sent message by
intercepting it and sending it to a diﬀerent agent.
(c) Re-ascribe/redirect: the penetrator may re-ascribe and redirect a
previously sent message at the same time.
We abstractly model each of the penetrator paths deﬁned above as a high-
level penetrator strand that sends and receives terms in the form (A, B, m, c).
Deﬁnition 6. A penetrator strand in a high-level bundle is either a standard
penetrator strand or a high-level penetrator strand of one of the following forms:
SD. Sending: ⟨−( , , m, ⊥), +(P, B, m, c)⟩where P ∈Tpname and B /∈Tpname;
RV. Receiving: ⟨−(A, P, m, c), +( , , m, ⊥)⟩where P ∈Tpname and A /∈Tpname;
LN. Learning: ⟨−(A, B, m, c), +( , , m, ⊥)⟩where A, B /∈Tpname;
FK. Faking: ⟨−( , , m, ⊥), +(A, B, m, c)⟩where A, B /∈Tpname;
HJ. Hijacking: ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩such that X ̸= X ′ or Y ̸= Y ′.
As an example, Figure 1 illustrates part of a bundle, where the penetrator uses
several diﬀerent strands to transform the high level message (S, P, AˆN , c) into
(S, B, PˆN , c).
2.3
High-Level Bundles
A high-level bundle is a ﬁnite subgraph of ⟨N, (→∪⇒)⟩for which the edges
express the causal dependencies of the nodes.

Specifying and Modelling Secure Channels in Strand Spaces
239
RV
•
(S,P,AˆN,c)  •

S
C
M
•
( , ,AˆN,⊥)  •

•

•
( , ,P,⊥)

•

( , ,N,⊥)  •

FK
•
( , ,A,⊥)

•
( , ,PˆN,⊥)  •
•
(S,B,PˆN,c) 
Fig. 1. Transforming a high-level term
Deﬁnition 7. [THG98] Suppose →B ⊂→, ⇒B ⊂⇒, and B = ⟨NB, →B ∪⇒B⟩
is a subgraph of ⟨N, →∪⇒⟩. B is a bundle if (1) NB and →B ∪⇒B are ﬁnite;
(2) If n2 ∈NB and msg(n2) is negative, then there is a unique n1 such that
n1 →B n2; (3) If n2 ∈NB and n1 ⇒n2 then n1 ⇒B n2; and (4) B is acyclic.
We write ⪯B for (→B ∪⇒B)∗.
The relation ⪯B expresses the causal relationship in the high-level bundle B.
Proposition 8. Let B be a high-level bundle. Then ⪯B is a partial order, i.e.
a reﬂexive, antisymmetric, transitive relation. Every non-empty subset of the
nodes in B has a ⪯B-minimal member.
Deﬁnition 9. Bundles B and B′ in a strand space Σ are equivalent iﬀthey
have the same regular strands.
3
Modelling Secure Channels
So far we allow high-level bundles with arbitrary penetrator strands. In this
section we restrict these strands to capture properties of secure channels. Our
approach follows [DL08]. We begin with conﬁdential channels, and then provide
the building blocks of authenticated channels. Each channel is associated with
a speciﬁcation that states which of these properties it satisﬁes.
Conﬁdential channels protect the conﬁdentiality of the messages sent on them.
If message (A, B, m, c) is sent over a conﬁdential channel c, the penetrator can-
not deduce m if the message was not intended for him. However, he can still
see the high-level message.
We can deﬁne a secure channel c to satisfy the
conﬁdentiality property C in terms of the penetrator’s activity as follows.
Deﬁnition 10 (Conﬁdential Channels). Let channel c satisfy C. Then there
is no LN strand of the form ⟨−(A, B, m, c), +( , , m, ⊥)⟩where A, B /∈Tpname
in any high-level bundle.
For example, if the Transport Layer protocol encodes the high-level term
(A, B, m, c) as Aˆ{m}PK(B), where PK(B) is B’s public key, then it provides a
conﬁdential channel. We write C(c) to indicate that c satisﬁes C, and similarly
for the properties we deﬁne below.

240
A. Kamil and G. Lowe
If a channel is non-fakable, then the penetrator cannot create and send an
application message using another agent’s identity.
Deﬁnition 11 (No faking). Let channel c satisfy NF. Then there is no FK
strand of the form ⟨−( , , m, ⊥), +(A, B, m, c)⟩where A, B /∈Tpname in any
high-level bundle.
For example, if the Transport Layer protocol encodes the high-level term
(A, B, m, c) as Bˆ{m}SK(A), where SK(A) is A’s secret key, then it provides
a non-fakable channel.
We now consider various restrictions on hijacking. In each case we do not
want to prevent HJ strands of the form ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩where
(i) if C(c) then Y ∈Tpname, and (ii) if NF(c) then X ′ ∈Tpname: in such
cases the penetrator could learn m (via a RV or LN strand) and then produce
+(X ′, Y ′, m, c) (via a SD or FK strand) to produce the same eﬀect.
If a channel is non-re-ascribable, then the penetrator cannot intercept a pre-
viously sent message and send it using a diﬀerent sender’s identity.
Follow-
ing [DL08], we distinguish between two notions of no re-ascribing:
– No re-ascribing where the penetrator cannot re-ascribe messages using any
identity;
– No honest re-ascribing where the penetrator cannot re-ascribe messages us-
ing an honest identity, but can still re-ascribe messages using a penetrator’s
identity.
For example, if the Transport Layer protocol encodes the high-level term
(A, B, m, c) as {{m}PK(B)}SK(A), then the penetrator P may replace the sig-
nature using SK(A) with his own signature using SK(P), so as to re-ascribe
the message to himself; however, he cannot re-ascribe the message to an honest
agent. On the other hand, if (A, B, m, c) is encoded as {{m, A}PK(B)}SK(A),
then he can no longer re-ascribe the message to himself.
We deﬁne non re-ascribable channels as follows.1
Deﬁnition 12 (No honest re-ascribing). Let channel c satisfy NRA−. Then
for every HJ strand of the form ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩in a high-level
bundle, one of the following holds: (a) X = X ′, i.e. no re-ascribing takes place;
(b) X ′ ∈Tpname, i.e. the message is re-ascribed with a penetrator’s identity; or
(c) if C(c) then Y ∈Tpname, and if NF(c) then X ′ ∈Tpname, i.e., as discussed
above, the penetrator can learn the underlying Application Layer message and
then send or fake the message.
Deﬁnition 13 (No re-ascribing). Let channel c satisfy NRA. Then for ev-
ery HJ strand of the form ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩in a high-level bun-
dle, one of the following holds: (a) X = X ′, i.e. no re-ascribing takes place;
(b) X , X ′ ∈Tpname, i.e. the message is re-ascribed from one penetrator identity
to another; or (c) if C(c) then Y ∈Tpname, and if NF(c) then X ′ ∈Tpname.
1 Some of the details of these deﬁnitions are a little delicate, and are necessary for
some of the future work discussed in Section 6.

Specifying and Modelling Secure Channels in Strand Spaces
241
If a channel is non-redirectable, the penetrator cannot intercept a previously sent
message and send it for a diﬀerent receiver. As with re-ascribing, we distinguish
between two notions of no redirecting:
– No-redirecting where the penetrator cannot redirect any message;
– No-honest redirecting where the penetrator cannot redirect messages sent to
honest participants, but can redirect messages sent to himself.
For example, if the Transport Layer protocol encodes the high-level term
(A, Y , m, c) as {{m}SK(A)}PK(Y ), then the penetrator P can transform a mes-
sage for himself, i.e. {{m}SK(A)}PK(P), into one for B, i.e. {{m}SK(A)}PK(B),
and so redirect it to B; however he cannot redirect a message sent to an honest
agent. On the other hand, if (A, Y , m, c) is encoded as {{m, Y }SK(A))}PK(Y ),
then he can no longer redirect a message sent to himself.
We deﬁne non-redirectable channels as follows.
Deﬁnition 14 (No honest redirecting). Let channel c satisfy NRD−. Then
for every HJ strand of the form ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩in a high-level
bundle, one of the following holds: (a) Y = Y ′, i.e. no redirecting takes place;
(b) Y ∈Tpname, i.e. the original message was sent to the penetrator; or (c) if
C(c) then Y ∈Tpname, and if NF(c) then X ′ ∈Tpname.
Deﬁnition 15 (No-redirecting). Let channel c satisfy NRD. Then for ev-
ery HJ strand of the form ⟨−(X , Y , m, c), +(X ′, Y ′, m, c)⟩in a high-level bun-
dle, one of the following holds: (a) Y = Y ′, i.e. no redirecting takes place;
(2) Y , Y ′ ∈Tpname, i.e. the message is redirected from one penetrator identity
to another; or (c) if C(c) then Y ∈Tpname, and if NF(c) then X ′ ∈Tpname.
4
Normal and Abstractly Eﬃcient Bundles
Bundles can contain various types of redundancy. For example, an encryption
edge immediately followed by a decryption edge just reproduces the original
term: this redundancy can be removed to produce an equivalent bundle. It is
clearly simpler if we can restrict our attention to bundles without such redun-
dancies. This is the question we consider in this section.
Deﬁnition 16. In a high-level bundle, a ⇒+ edge is constructive if it is part
of an E, C, SD or FK strand. It is destructive if it is part of a D, S, LN or RV
strand. An edge is non-destructive if it is constructive or part of an HJ strand.
Similarly, an edge is non-constructive if it is destructive or part of an HJ strand.
Deﬁnition 17. A high-level bundle B is normal iﬀfor any penetrator path of
B, no non-destructive edge precedes a non-constructive edge.
Proposition 18. For every high-level bundle B, there exists an equivalent high-
level normal bundle B′. Moreover, the penetrator nodes of B′ form a subset of the
penetrator nodes of B and the ordering ⪯B′ is a restriction of the ordering ⪯B.

242
A. Kamil and G. Lowe
Proof (Sketch.). The proof proceeds by showing that whenever a non-destructive
edge precedes a non-constructive edge, an equivalent bundle can be found with-
out this redundancy. For standard penetrator strands, the proof is as in [GT01].
Figure 2 (a)–(d) gives some examples of how to replace redundancies arising
from high-level penetrator strands. A simple case analysis shows no redundancy
involves a standard penetrator strand and a high-level penetrator strand.
Normal bundles may still contain redundancies.
For example, an LN strand
followed by an SD strand may be replaced by an HJ strand (or a transmission
edge if the identities match).
Deﬁnition 19. A high-level bundle B is abstractly eﬃcient if every penetrator
path p that starts at n1 such that msg(n1) = +(X , Y , m, c), and ends at n2
such that msg(n2) = −(X ′, Y ′, m, c), consists of a single HJ strand or else there
is a transmission edge between n1 and n2.
Proposition 20. For every high-level bundle, there exists an equivalent high-
level eﬃcient bundle that is also normal.
Proof (Sketch.). Figure 2 (e)–(f) gives examples of how some of the remaining
redundancies can be removed.
5
Example: A Single Sign-On Protocol
In this example we illustrate our deﬁnitions of high-level bundles and secure
channels via a small example. We consider a single sign-on protocol that au-
thenticates a User U to a Service Provider SP, with the help of an Identity
Provider IdP.
We will use a secure channel c that satisﬁes C ∧NRD−. It is reasonable to
suppose that the Service Provider and Identity Provider each has a public key
certiﬁcate, so unilateral TLS can be used to establish an authenticated channel
to them; we believe that this channel satisﬁes C ∧NRD−. For messages sent
from the Identity Provider to the User, the channel could be implemented using
unilateral TLS to authenticate the Identity Provider, combined with the User
sending a password to authenticate herself; we believe that this channel satisﬁes
C ∧NF ∧NRD ∧NRA, which is stronger than is required.
We will consider the following protocol, where →c indicates messages sent
using channel c, and →indicate messages sent on the bottom channel.
0.
U →SP
: UˆIdP
1.
SP →c IdP : 1ˆSPˆUˆN
2. IdP →c U
: 2ˆIdPˆSPˆN
3.
U →c SP
: 3ˆUˆIdPˆN
Here N is a fresh unpredictable value; “1”, “2” and “3” are distinct tags used to
ensure unique readability of the messages. Message 0 is sent across the bottom
channel to initiate the protocol. SP then creates a fresh nonce which is passed
via IdP to U , and then back to U in order to authenticate U to SP.

Specifying and Modelling Secure Channels in Strand Spaces
243
FK
LN
◦
( , ,m,⊥)  •
•
(A,B,m,c)  •
•
( , ,m,⊥)  ◦
◦
( , ,m,⊥) 















•
•
(A,B,m,c)♣
◦
(a) A path containing an FK-LN redun-
dancy, replaced by a transmission edge.
FK
HJ
◦
( , ,m,⊥)  •
•
(A,Y ,m,c)  •
•
(A′,Y ′,m,c) ◦
FK
◦
( , ,m,⊥)  •













•
(A,Y ,m,c)♣ •
•
(A′,Y ′,m,c) ◦
(b) A path containing an FK-HJ redun-
dancy, replaced by an FK strand.
HJ
LN
◦
(X ,B,m,c)  •
•
(X ′,B′,m,c)  •
•
( , ,m,⊥)  ◦
LN
◦
(X ,B,m,c)  •













•
(X ′,Y ,m,c)♣
•
( , ,m,⊥)  ◦
(c) A path containing an HJ-LN redun-
dancy, replaced by an LN strand.
HJ
HJ
◦
(X ,Y ,m,c)  •
•
(X1 ,Y1 ,m,c) •
•
(X2 ,Y2 ,m,c) ◦
HJ
◦
(X ,Y ,m,c)  •













•
(X1 ,Y1 ,m,c)♣ •
•
(X2 ,Y2 ,m,c) ◦
(d) A path containing an HJ-HJ redun-
dancy, replaced by an HJ strand.
LN
SD
◦
(A,B,m,c)  •
•
( , ,m,⊥)  •
•
(P,B′,m,c)  ◦
HJ
◦
(A,B,m,c)  •























•
(P,B′,m,c)  ◦
(e) An ineﬃcient LN-SD path and the
corresponding eﬃcient path.
RV
FK
◦
(A,P,m,c)  •
•
( , ,m,⊥)  •
•
(A′,B,m,c)  ◦
HJ
◦
(A,P,m,c)  •























•
(A′,B,m,c)  ◦
(f) An ineﬃcient RV-FK path and the cor-
responding eﬃcient path.
Fig. 2. Redundancies and how to eliminate them. ♣indicates a discarded message.

244
A. Kamil and G. Lowe
In order to model this protocol, we start by deﬁning regular strands for each
of the three roles.
– Strands of the form User(U , SP, IdP, N ) have trace
⟨+ (U , SP,
UˆIdP,
⊥),
−(IdP, U , 2ˆIdPˆSPˆN , c),
+ (U , SP, 3ˆUˆIdPˆN , c) ⟩.
– Strands of the form ServProv(SP, U , IdP, N ) have trace
⟨−(U , SP,
UˆIdP,
⊥),
+ (SP, IdP, 1ˆSPˆUˆN , c),
−(U , SP, 3ˆUˆIdPˆN , c) ⟩.
– Strands of the form IdProv(IdP, U , SP, N ) have trace
⟨−(SP, IdP, 1ˆSPˆUˆN ,
c),
+ (IdP, U , 2ˆIdPˆSPˆN , c) ⟩.
We will therefore consider bundles B containing strands of the above form (and
no other regular strands). Further, since each nonce N is freshly generated, we
assume that for every ServProv(SP, U , IdP, N ) strand st containing at least two
nodes in the bundle, N originates uniquely at (st, 2).
Consider a bundle B containing all three nodes of a Service Provider strand
st = ServProv(SP, U , IdP, N ), and such that SP, U , IdP /∈Tpname. We aim
to show that there is a corresponding User strand in B, i.e., the User is au-
thenticated to the Service Provider. By Proposition 20, we may, without loss of
generality, assume that B is normal and abstractly eﬃcient.
The reason the protocol works is that only SP, U
and IdP can ob-
tain N .
The lemma below captures this.
Let X be the set containing
the terms (SP, IdP, 1ˆSPˆUˆN , c), (IdP, U , 2ˆIdPˆSPˆN , c), and (U , SP,
3ˆUˆIdPˆN , c).
Lemma 21. Every occurrence of N on a regular node is within a high-level term
from X ; N does not occur within any high-level term of the form ( , , m, ⊥).
Proof. Suppose for a contradiction that the result does not hold. Let n be a
⪯B-minimal node where this occurs (there must be a minimal such node by
Proposition 8). Clearly n ̸= (st, 2), since msg(st, 2) is in X . Since N originates
uniquely at (st, 2), N does not originate at n. Hence one of the following holds.
– n is a positive regular node.
Then there must be some node n′ such
that n′ ⇒+ n and N occurs within msg(n′). Then, by the assumed ⪯B-
minimality of n, msg(n′) ∈X ; hence the strand containing n and n′ trans-
forms a message from X into a message not in X . But no regular strand can
do this; for example:
• If an Identity Provider strand receives a message from X , it is neces-
sarily of the form (SP, IdP, 1ˆSPˆUˆN , c); it will then send (IdP, U ,
2ˆIdPˆSPˆN , c), which is also in X : the presence of the SP and U ﬁelds
within message 1 is important here.

Specifying and Modelling Secure Channels in Strand Spaces
245
• If a User strand receives a message from X , it is necessarily of the form
(IdP, U , 2ˆIdPˆSPˆN , c); it will then send (U , SP, 3ˆUˆIdPˆN , c),
which is also in X : the presence of the SP ﬁeld within message 2 is
important here.
– n is either a negative regular node containing N outside X , or a penetrator
node containing N in a term of the form ( , , m, ⊥).
In each case, the
term is produced by a penetrator path that starts at a regular node n′
containing a term from X . Since B is normal and abstractly eﬃcient, every
such penetrator path must start with an RV or LN strand, or comprise a
single HJ strand. Clearly no RV strand can operate on terms from X . Since
c is a conﬁdential channel, no LN strand can operate on messages from X .
Since c satisﬁes C ∧NRD−, every HJ strand either: (case (a) of Deﬁnition 14)
changes only the ﬁrst ﬁeld of high-level messages, but no honest strand will
accept the result of transforming a term from X in this way; or (cases (b)
and (c) of Deﬁnition 14) operates on high-level messages whose second ﬁeld
is an element of Tpname, so cannot operate on messages from X .
⊓⊔
Now consider the term (U , SP, 3ˆUˆIdPˆN , c) received at (st, 3). From the
above lemma, the term could not have been produced by an FK strand. Using
this and the fact that the bundle is normal and abstractly eﬃcient, the term must
result from either a transmission edge, or an HJ strand, from a term from X .
The latter case cannot occur (since each HJ strand changes either the sender or
receiver ﬁeld). An analysis of the honest strands then shows that the message
is transmitted from the ﬁnal node of a User(U , SP, IdP, N ) strand.
A similar analysis could be used to show the presence of a corresponding
Identity Provider strand; we omit the details.
It is possible to simplify the protocol slightly, by removing some ﬁelds. How-
ever, we do need the ﬁelds U and SP in message 1, and SP in message 2 to
ensure that Lemma 21 is satisﬁed, in particular by honest Identity Provider and
User strands. Further (but arguably less importantly), the IdP ﬁeld in message 3
is needed to ensure the User and Service Provider agree upon the identity of the
Identity Provider. Finally, the presence of the U ﬁeld in message 3 simpliﬁes
the proof slightly.
6
Conclusion
In this paper we have described how to model secure channels within the Strand
Spaces formalism. We represent messages sent over the network using high-level
terms, which abstract away from the implementation of the secure transport
protocol. We then abstractly modelled ways in which the penetrator can operate
upon such high level terms: to obtain the underlying Application Layer message
(either honestly or dishonestly); to have an honest agent receive the Application
Layer message (either apparently from the penetrator or some third party); or
to hijack the message, to change either the recipient or the apparent sender. We
speciﬁed properties of secure channels by restricting the capabilities available to
the penetrator. Finally, we illustrated the model by using it to verify a property

246
A. Kamil and G. Lowe
of a simple single sign-on protocol: we believe that the proof helps to explain
why the protocol is correct.
This is the ﬁrst of a planned series of papers reporting work from [Kam09].
We brieﬂy discuss some of the results here.
Many secure transport protocols group messages together into sessions, so
that the recipient of messages receives an assurance that the sender sent those
messages as part of the same session. For example, a single sign-on protocol is
normally used as a prelude to some session: the Service Provider wants to be
sure that all the messages in that session came from the same User who was
authenticated by the single sign-on. Further, some transport protocols give the
recipient a guarantee that the messages were received in the same order in which
they were sent. These properties are captured in [Kam09, Chapter 5].
In this paper we have presented high-level bundles, which abstract away from
the implementation of the secure transport protocol. As mentioned in the In-
troduction, one could also model layered architectures by explicitly modelling
the transport protocol, in low-level bundles. In [Kam09, Chapter 6] the rela-
tionship between these models is described, and it is shown that —subject to
certain independence assumptions— for every low-level bundle, there is a high-
level bundle that abstracts it. Hence the abstraction is sound: by verifying a
protocol in a high-level Strand Space, one can deduce that the implementation
of the protocol, as modelled in the low-level Strand Space, is also correct.
In [DL08] it is shown that not all combinations of the properties from Section 3
are distinct, and a hierarchy of diﬀerent properties is —informally— derived.
In [Kam09, Chapter 7] the same result is —more formally— obtained for our
Strand Spaces deﬁnitions.
In Section 5, we performed a direct veriﬁcation of the example protocol.
In [Kam09, Chapter 7], a number of veriﬁcation-oriented rules are presented.
Some rules concern when an honest agent receives a message over a particular
secure channel, and allow one to deduce facts about how that message was pro-
duced. Further rules allow one to verify the secrecy of certain terms, while oth-
ers adapt the Authentication Tests of [GT00] to high-level bundles. In [Kam09,
Chapter 8], these rules are used in a number of examples, concerning both lay-
ered protocol architectures and empirical channels.
References
[ACC07]
Armando, A., Carbone, R., Compagna, L.: LTL model checking for se-
curity protocols. In: 20th IEEE Computer Security Foundations Sympo-
sium (2007)
[ACC+08]
Armando, A., Carbone, R., Compagna, L., Cuellar, J., Tobarra, L.: For-
mal analysis of SAML 2.0 web browser single sign-on:
Breaking the
SAML-based single sign-on for Google Apps. In: The 6th ACM Workshop
on Formal Methods in Security Engineering, FMSE 2008 (2008)
[BF08]
Bugliesi, M., Focardi, R.: Language based secure communication. In:
Proceedings of the 21st IEEE Computer Security Foundations Sympo-
sium (2008)

Specifying and Modelling Secure Channels in Strand Spaces
247
[BLP03]
Bella, G., Longo, C., Paulson, L.: Verifying second-level security proto-
cols. In: Basin, D., Wolﬀ, B. (eds.) TPHOLs 2003. LNCS, vol. 2758, pp.
352–366. Springer, Heidelberg (2003)
[CGRZ03]
Creese, S.J., Goldsmith, M.H., Roscoe, A.W., Zakiuddin, I.: The attacker
in ubiquitous computing environments: formalising the threat model.
In: Proceedings of the 1st International Workshop on Formal Aspects in
Security and Trust, FAST (2003)
[Dil08]
Dilloway, C.: On the Speciﬁcation and Analysis of Secure Transport Lay-
ers. DPhil thesis, Oxford University (2008)
[DL08]
Dilloway, C., Lowe, G.: Specifying secure transport layers. In: 21st IEEE
Computer Security Foundations Symposium, CSF 21 (2008)
[Goo08]
Google. Web-based reference implementation of SAML-based SSO for
Google Apps (2008),
http://code.google.com/apis/apps/sso/
saml reference implementation web.html
[GT00]
Guttman, J.D., Thayer, F.J.: Authentication tests. In: IEEE Symposium
on Security and Privacy, pp. 96–109 (2000)
[GT01]
Guttman, J.D., Thayer, F.J.: Authentication tests and the structure of
bundles. Theoretical Computer Science (2001)
[HSN05]
Hansen, S.M., Skriver, J., Nielson, H.R.: Using static analysis to validate
the SAML single sign-on protocol. In: Proceedings of the 2005 Workshop
on Issues in the Theory of Security, WITS 2005 (2005)
[Kam09]
Kamil, A.: The Modelling and Analysis of Layered Security Architectures
in Strand Spaces. DPhil thesis, Oxford University, Forthcoming (2009)
[KL09]
Kamil, A., Lowe, G.: Analysing TLS in the Strand Spaces model (2009)
(Submitted for publication)
[OAS05]
OASIS Security Services Technical Committee. Security assertion markup
language (SAML) v2.0 technical overview (2005),
http://www.oasis-open.org/committees/security/
[Ros98]
Roscoe, A.W.: The Theory and Practice of Concurrency. Prentice Hall,
Englewood Cliﬀs (1998)
[THG98]
Javier Thayer, F., Herzog, J.C., Guttman, J.D.: Strand spaces: Why is a
security protocol correct? In: IEEE Symposium on Research in Security
and Privacy, pp. 160–171. IEEE Computer Society Press, Los Alamitos
(1998)
[Tho00]
Thomas, S.: SSL and TLS: Securing the Web. Wiley, Chichester (2000)

Integrating Automated and Interactive Protocol
Veriﬁcation
Achim D. Brucker1 and Sebastian A. M¨odersheim2
1 SAP Research, Vincenz-Priessnitz-Str. 1, 76131 Karlsruhe, Germany
achim.brucker@sap.com
2 IBM Research, S¨aumerstrasse 4, 8803 R¨uschlikon, Switzerland
smo@zurich.ibm.com
Abstract. A number of current automated protocol veriﬁcation tools
are based on abstract interpretation techniques and other over-approx-
imations of the set of reachable states or traces. The protocol models that
these tools employ are shaped by the needs of automated veriﬁcation
and require subtle assumptions. Also, a complex veriﬁcation tool may
suﬀer from implementation bugs so that in the worst case the tool could
accept some incorrect protocols as being correct. These risks of errors are
also present, but considerably smaller, when using an LCF-style theorem
prover like Isabelle. The interactive security proof, however, requires a
lot of expertise and time.
We combine the advantages of both worlds by using the representation
of the over-approximated search space computed by the automated tools
as a “proof idea” in Isabelle. Thus, we devise proof tactics for Isabelle
that generate the correctness proof of the protocol from the output of
the automated tools. In the worst case, these tactics fail to construct a
proof, namely when the representation of the search space is for some
reason incorrect. However, when they succeed, the correctness only relies
on the basic model and the Isabelle core.
1
Introduction
Over the last decade, a number of automated tools for security protocol veri-
ﬁcation have been developed such as AVISPA [1] and ProVerif [4]. They allow
engineers to ﬁnd problems in their security protocols before deployment. Indeed,
several attacks to security protocols have been detected using automated tools.
The focus of this work is the positive case—when no attack is found: to obtain
a proof of security.
Many automated tools employ over-approximation and abstraction techniques
to cope with the inﬁnite search spaces that are caused, e.g., by an unbounded
number of protocol sessions. This means to check the protocol in a ﬁnite abstract
model that subsumes the original model. Thus, if the protocol is correct in the
abstract model, then so it is in the original model. However, the soundness of
such abstractions depends on subtle assumptions, and it is often hard to keep
track of them, even for experts. Moreover, it is often hard to formalize protocols
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 248–262, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Integrating Automated and Interactive Protocol Veriﬁcation
249
correctly in such over-approximated models. Finally, tools may also have bugs.
For all these reasons, it is not unlikely that insecure protocols are accidentally
veriﬁed by automated veriﬁcation tools.
There are semi-automated methods such as the Isabelle theorem prover which
oﬀer a high reliability: if we trust in a small core (the proof checking and some
basic logical axioms), we can rely on the correctness of proved statements. How-
ever, conducting proofs in Isabelle requires considerable experience in both for-
mal logic and proof tactics, as well as a proof idea for the statement to show.
In this work, we combine the best of both worlds: reliability and full au-
tomation. The idea is that abstraction-based tools supposedly compute a ﬁnite
representation of an over-approximated search space, i.e., of what can happen
in a given protocol, and that this representation can be used as the basis to
automatically generate a security proof in Isabelle. This proof is w.r.t. a clean
standard protocol model without over-approximation. If anything goes wrong,
e.g., the abstraction is not sound, this proof generation fails. However, if we
succeed in generating the proof, we only need to trust in the standard protocol
model and the Isabelle core. Our vision is that such automatically generated
veriﬁable proofs can be the basis for reaching the highest assurance level EAL7
of a common criteria certiﬁcation at a low cost.
Proof
Abstract Model
Abstraction/
Reﬁnement
Proof
Generator
OFMC
Isabelle/OFMC
Reference Model
Veriﬁed
Attack
OR
Trace
Fixedpoint
FP Module
Isabelle Core
Fig. 1. The workﬂow of a protocol ver-
iﬁcation approach combing automated
(e.g., OFMC) and interactive (e.g., Is-
abelle) techniques
We have realized the integration of au-
tomated and interactive protocol veriﬁca-
tion in a prototype tool that is summa-
rized in Fig. 1. The protocol and the proof
goals (e.g., secrecy, authentication) are
speciﬁed in the reference model (Sect. 2
and Sect. 3). This reference model is not
driven by technical needs of the auto-
mated veriﬁcation and is close to other
high-level protocol models. The descrip-
tion is fed into the automated tool, where
we consider (a novel module of) the
Open-source Fixed-point Model-Checker
OFMC [24], formerly called On-the-Fly
Model-Checker. OFMC ﬁrst chooses an
initial abstraction and produces an ab-
stracted version of the protocol descrip-
tion as a set of Horn clauses. From this, the ﬁxed-point (FP) module computes
a least ﬁxed-point of derivable events. If this ﬁxed-point contains an attack, this
can either be a real attack (that similarly works in the reference model) or a
false attack that was caused by the abstraction. By default, OFMC will assume
that the attack is false, reﬁne the abstraction based on the attack details and
restart the veriﬁcation. If the computed ﬁxed-point does not contain an attack,
it is handed to the proof generator of the Isabelle/OFMC, our extension of
the interactive theorem prover Isabelle [25]. The proof generator translates the
ﬁxed-point into the terms of the reference model (using annotations about the

250
A.D. Brucker and S.A. M¨odersheim
abstraction OFMC considered) and generates an Isabelle proof with respect to
the protocol and goal description in the reference model. This proof is fed into
the Isabelle core to check the proof.
We emphasize two points. First, the entire approach is completely automatic:
after the speciﬁcation of the protocol and goals in the reference model, no further
user interaction is required. Second, we need to trust in only two points—marked
with a dark-gray background in Fig. 1: the reference model and the Isabelle core.
Bugs in any other part, namely OFMC or the proof generation, can in the worst
case result in a failure to verify a (possibly correct) protocol, but they cannot
make us falsely accept a ﬂawed protocol as being correct.
Our approach currently has two limitations: we consider a typed model and,
based on this, we limit the intruder composition to well-typed messages of
the protocol. The ﬁrst limitation can be justiﬁed by implementation discipline
(see [19]). The second limitation is not a restriction as we show in Theorem 1.
Contributions. An increasing number of works considers the combination of au-
tomated methods with interactive theorem proving to obtain both highly reliable
and fully automated veriﬁcation. In this paper, we contribute to this line of work
with a novel approach for security protocols. The ﬁrst novel aspect is that our
approach automatically generates a proof from the representation of an over-
approximation of the search space computed by an automated protocol veriﬁer.
Techniques based on over-approximation, similar to the ones we consider, have
turned out to be very successful in protocol veriﬁcation [4,6,7,10], and our ap-
proach is thus the ﬁrst step towards employing a whole class of established tools
for automated proof generation. The second novel aspect is that the proof is
entirely based on a standard protocol model without over-approximation close
to the model employed for instance in [26]. Our approach thus relates over-
approximated representations with standard protocol models.
Practically, we have implemented the integration between Isabelle on the in-
teractive side and the novel FP-module of OFMC on the automated side. The
result is a completely automated protocol veriﬁer for an unbounded number of
sessions and agents that produces Isabelle-veriﬁable proofs with respect to a
standard protocol model.
2
The Reference Protocol Model
We begin with a reference protocol model, which is used in the Isabelle theorem
prover and is thus the basis of this work. The model is inspired by the formal-
ization of several security protocols in Isabelle by Paulson and others in [26,3]
and is close to the persistent IF model in [23].
Messages. We follow the common black-box cryptography model where messages
are modeled as symbolic terms. We deﬁne the set of all messages in style of
an inductive datatype in a functional programming language. Here, all sans-
serif symbols and the symbols of F are constructors. The inductively deﬁned
datatype is interpreted like a term in the free term algebra, i.e. syntactically
diﬀerent terms are interpreted as being diﬀerent.

Integrating Automated and Interactive Protocol Veriﬁcation
251
Deﬁnition 1. Let F, LA, LN , LS, LP, VA, VN , VS, VP, and VU be pairwise
disjoint sets of symbols where F, LA, LN , LS, and LP are ﬁnite, and VA, VN ,
VS, VP, and VU are countable. We deﬁne the sets of messages, agents, nonces,
symmetric keys, and public keys, respectively, to be the following sets of terms:
M = agent A | nonce N | symkey S | pubkey P | VU
| crypt M M | inv M | scrypt M M | cat [M] | F M
A = LA × N | VA
N = LN × N | VN
S = LS × N | VS
P = LP × N | VP
The set A contains both the concrete agent names (LA × N) and the variables
for agent names (VA). The concrete agent names consist of a label and a natural
number. The labels are for the interplay of Isabelle and the automated methods,
for now it is suﬃcient to think just of an inﬁnite set of agents, indexed by natural
numbers. Similarly, N, S, and P deﬁne inﬁnite reservoirs of concrete constants
and variables for nonces, symmetric keys, and public keys.
For convenience, we write in the examples of this paper simply a, b, or i for
concrete agent names, A, B for agent variables, n, n1, etc. for concrete nonces,
NA, NB etc. nonce variables. In general, we use lower-case letters for constants
and function symbols and upper-case letters for variables.
We distinguish atomic messages and composed messages (ﬁrst and second
line in the deﬁnition of M). Except for the untyped variables of VU, all atomic
messages are of a particular type, namely from one of the sets A, N, S, or P. The
constructors like agent ensure that the respective subsets of the message space
are disjoint, for instance, no agent name can be a nonce. We discuss the details of
typing below. In examples, we will omit the constructors for convenience, when
the type is clear from the context.
Messages can be composed with one of the following operations: crypt and scrypt
represent asymmetric and symmetric encryption, respectively. We also simply
write {m}k for crypt k m, and {|m|}k for scrypt k m. inv(M) represents the private
key belonging to a public key. cat denotes concatenation. For readability, we omit
the cat and the list brackets. For instance the term {na3, b}pk(a) is convenient no-
tation for the following more technical message (for given labeling and number-
ing): crypt (pk (agent (honest, 1))) (cat [nonce (na, 3), agent (dishonest, 2)]). Here
we have used two labels, honest and dishonest, for agents. This represents the
default abstraction for agents in the abstract model. We require that the ab-
straction for the agents is a reﬁnement of the default abstraction. We use this
labeling to distinguish honest and dishonest agents also in the reference model.
We use the standard notion of matching messages. The constructors like agent
here enforce a typing regime: typed variables can only be matched with atomic
messages of the same type. Only untyped variables can be matched with com-
posed messages. In rules for honest agents, we only use typed variables. Such a
typed model which is standard in protocol veriﬁcation, even in interactive ver-
iﬁcation with Isabelle [26,3], considerably simpliﬁes the veriﬁcation task. The
typing can be justiﬁed by tagging as in [19].
Events and Traces. We deﬁne the set of events also as an inductive datatype,
based on messages:

252
A.D. Brucker and S.A. M¨odersheim
t ∈T
iknows m ∈[t]
iknows k ∈[t]
iknows {m}k # t ∈T
t ∈T
iknows m ∈[t]
iknows k ∈[t]
iknows {|m|}k # t ∈T
t ∈T
iknows m1 ∈[t]
. . .
iknows mn ∈[t]
iknows [m1, . . . , mn] # t ∈T
t ∈T
iknows m ∈[t]
iknows f(m) # t ∈T
f ∈Fpub
t ∈T
iknows {m}k ∈[t]
iknows inv(k) ∈[t]
iknows m # t ∈T
t ∈T
iknows {m}inv(k) ∈[t]
iknows k ∈[t]
iknows m # t ∈T
t ∈T
iknows {|m|}k ∈[t]
iknows k ∈[t]
iknows m # t ∈T
t ∈T
iknows m1 ∈[t]
. . .
iknows mn ∈[t]
iknows m1 #, . . . # iknows mm # t ∈T
t ∈T
secret A M ∈[t]
iknows M ∈[t]
honest A
attack M # t ∈T
t ∈T
request B A id M ∈[t]
witness A B id M /∈[t]
honest A
attack M#t ∈T
Fig. 2. The protocol independent rules of the reference model: the ﬁrst four are compo-
sition rules (C), the next four are decomposition rules (D) and the last two are attack
rules (A)
Deﬁnition 2. The set of events is deﬁned as follows:
E ::=iknows M | state R [M] |
secret A M | witness A A I M | request A A I M | attack M
where I is a ﬁnite set of identiﬁers disjoint from all other symbols so far and
R ⊂I. A trace is a ﬁnite sequence ⟨e1 # · · · # en ⟩of events ei.
The identiﬁer set I contains constant symbols for the protocol variables, allowing
us to describe as which protocol variable an agent interprets a particular message.
We use Gothic fonts for identiﬁers, e.g. A and B.
The event iknows m means that the intruder just learned the message m. The
event state R msgs means that an honest agent playing role R has reached a
state of its protocol execution that is characterized by the list msgs of messages.
We need the other four events for expressing the goals in a protocol independent
form when we introduce attack rules below.
Rules and Protocols. Based on these deﬁnitions, we formalize protocols by a set
of inductive rules on traces that have the following form:
t ∈T
φ(t, e1, . . . , en)
e1 # . . . # en # t ∈T

Integrating Automated and Interactive Protocol Veriﬁcation
253
i.e. whenever t is a valid trace of the set T of traces and e1, . . . , en are events
that fulﬁll a certain condition φ with t, then also the extension of t with these
events is also part of T. Also, we have the rule that the empty trace is part of
T. Note that we require that all transition rules of honest agents contain only
typed variables, i.e. no variables of VU.
Fig. 2 shows the protocol independent rules for the intruder (C) and (D),
following the standard Dolev-Yao style intruder deduction, as well as the attack
rules (A), which follow the standard deﬁnitions of attacks in AVISPA [1]; here [t]
denotes the set of events in the trace t and Fpub ⊆F is the set of functions that
is accessible to the intruder. Before we explain the attack rules, we describe the
transition rules of role Alice of the standard example protocol, NSL [20] (more
interesting examples are found in Sect. 6):
Example 1
t ∈T
NA /∈used (t)
iknows {NA, A}pk(B) # state A [A, B, NA] #
witness A B NA NA # secret B NA # t ∈T
t ∈T
state A [A, B, NA] ∈[t]
iknows {NA, NB, B}pk(A) ∈[t]
iknows {NB}pk(B) # request A B NB NB # t ∈T
Here, used (t) is the set of all atomic messages that occur in t to allow for the
fresh generation of nonces.
⊓⊔
The goals of a protocol are described negatively by what counts as an attack.
This is done by attack rules that have the event attack on the right-hand side.
We now explain the events that we use in attack rules. First, secret A M means
that some honest agent (not speciﬁed) requires that the message M is a secret
with agent A. Thus, it counts as an attack, if a trace contains both iknows M
and secret A M for an honest agent A (ﬁrst attack rule in Fig. 2).
For authentication, the event witness A B id m means that for a particular
purpose id, the honest agent A wants to transmit the message M to agent B.
Correspondingly, when B believes to have received message M for purpose id
from agent A, the event request B A id M occurs. It thus counts as an attack, if
request B A id M occurs in a trace for an honest agent A and the trace does not
contain the corresponding event witness A B id M (second attack rule in Fig. 2).
We call a trace an attack trace if it contains the attack event.
Deﬁnition 3. Let a protocol be described by an inductive set R of rules. The
protocol is said to be safe, if the least set T of traces that is closed under R
contains no attack trace.
Despite some diﬀerences, our model is similar to the one of Paulson [26], as
discussed in detail in the extended version of this paper [8].
3
Limiting Intruder Composition
The closure of the intruder knowledge under the composition rules of the intruder
is generally inﬁnite, e.g. the intruder can concatenate known messages arbitrarily.

254
A.D. Brucker and S.A. M¨odersheim
However, many of these messages are useless to the intruder since, due to typing,
no honest agent accepts them. It is therefore intuitive that we do not loose any
attacks if we limit intruder composition to terms, and subterms thereof, that
some honest agent can actually receive. (A similar idea has been considered e.g.
in [28].) This limitation on intruder composition makes our approach signiﬁcantly
simpler, and we have therefore chosen to integrate this simpliﬁcation into our
reference model for this ﬁrst version, and leave the generalization to an unlimited
intruder for future versions. We formally deﬁne the transformation that limits
intruder composition as follows:
Deﬁnition 4. For a set of rules R that contain no untyped variables VU, let MR
be the set of all messages that occur in an iknows or secret event of any rule of
R along with as their sub-messages. We say that an intruder composition rule
r can compose terms for R, if the resulting term of r can be uniﬁed with a term
in MR. In this case we call rσ an r-instance for compositions of MR if σ is the
most general uniﬁer between the resulting term of r and a message in MR. We
say that R is saturated if it contains all r-instances for composition in MR.
Since we excluded untyped variables, atomic messages in MR are typed, i.e. of
the form type(·). Also, due to typing, every ﬁnite R has a ﬁnite saturation.
Theorem 1. Given an attack against a protocol described by a set of rules R ∪
C ∪D ∪A where C and D are the intruder composition and decomposition rules
and A are the attack rules. Let R′ be a saturated superset of R. Then there is an
attack against R′ ∪D ∪A.
The proof is found in the extended version of this paper [8].
4
The Abstract Protocol Model
We now summarize two kinds of over-approximations of our model that are used
in our automated analysis tool to cope with the inﬁnite set of traces induced by
the reference model. These techniques are quite common in protocol veriﬁcation
and a more detailed description can be found in [23]; we discuss them here only as
far as they are relevant for our generation of Isabelle proofs. The ﬁrst technique
is a data abstraction that maps the inﬁnite set of ground atomic messages (that
can be created in an unbounded number of sessions) to a ﬁnite set of equivalence
classes in the style of abstract interpretation approaches. The second is a control
abstraction: we forget about the structure of traces and just consider reachable
events in the sense that they are contained in some trace of the reference model.
Neither of these abstractions is safe: each may introduce false attacks (that are
not possible in the reference model). Also, it is not guaranteed in general that
the model allows only for a ﬁnite number of reachable events, i.e. the approach
may still run into non-termination.
Data Abstraction. In the style of abstract interpretation, we ﬁrst partition the
set of all ground atomic messages into ﬁnitely many equivalence classes and

Integrating Automated and Interactive Protocol Veriﬁcation
255
then work on the basis of these equivalence classes. Recall that atomic ground
messages are deﬁned as a pair (l, n) where l is a label and n is a natural number.
We use the label to denote the equivalence class of the message in the abstraction.
The abstract model thus identiﬁes diﬀerent atoms with the same label, and hence
we just omit the second component in all messages of the abstract model.
There is a large variety of such data abstractions. For the proof generation the
concrete way of abstraction is actually irrelevant, and we just give one example
for illustration:
Example 2. The initial abstraction that OFMC uses for freshly created data is
the following. If agent a creates a nonce n for agent b, we characterize the equiv-
alence class for that nonce in the abstract model by the triple (n, a, b). This
abstraction can be rephrased as “in all sessions where a wants to talk to b, a
uses the same constant for n (instead of a really fresh one)”. For a large number
of cases, this simple abstraction is suﬃcient; in the experiments of Sect. 6, only
two examples (NSL and Non-reversible Functions) require a more ﬁne-grained
abstraction. Note that OFMC automatically reﬁnes abstractions when the ver-
iﬁcation fails, but this mechanism is irrelevant for the proof generation. The
protocol from Example 1, rules for A, then look as follows:
t ∈T
iknows {(NA, A, B), A}pk(B) # state A [A, B, (NA, A, B)] #
witness A B NA (NA, A, B) # secret B (NA, A, B), # t ∈T
t ∈T
state A [A, B, NA] ∈[t]
iknows {NA, NB, B}pk(A) ∈[t]
iknows {NB}pk(B) # request A B NB NB # t ∈T
Here, we only abstract NA in the ﬁrst rule where it is created by the A. It is
crucial that the nonce NB is not abstracted in the second rule: since NB is not
generated by A, A cannot be sure a priori that was indeed generated by B. In
fact, if we also abstract NB here, the proof generation fails, because the resulting
ﬁxed-point does no longer over-approximate the traces of the reference model.
More generally, fresh data are abstracted only in a rule where they are created.
Finally, observe that the condition is gone that tells that the freshly created NA
never occurred in the trace before, because now agents may actually use the
same value several times in place of the fresh nonce.
The key idea to relate our reference model with the abstract one is the use
of labels in the deﬁnition of concrete data. Recall that each concrete atomic
message in the reference model is a pair of a label and a natural number. The
ﬁnite set of labels is determined by the abstraction we use in the abstracted
model; in the above example, we use LN = {NA, NB} × LA × LA where LA is
the abstraction of the agents (for instance LA = {(honest, dishonest)}). As the
atomic messages consist of both such a label and a natural number, the reference
model is thus endowed with an inﬁnite supply of constants for each equivalence
class of the abstract model. The relationship between data in the reference and
abstract models is straightforward: the abstraction of the concrete constant (l, n)

256
A.D. Brucker and S.A. M¨odersheim
is simply l. Vice-versa, each equivalence class l in the abstract model represents
the set of data {(l, n) | n ∈N} in the reference model.
It is crucial that in the reference model, the labels are merely annotations
to the data and the rules do not care about these annotations, except for the
distinction of honest and dishonest agents as discussed before. The labels however
later allow us to form a security proof in the reference model based on the
reachable events in the concrete model.
We need to take the abstraction into account in the reference model when
creating fresh data. In particular, we need to enforce the labeling that reﬂects ex-
actly the abstraction. We extend the assumptions of the ﬁrst rule from Example 1
by a condition on the label of the freshly created NA:
t ∈T
NA /∈used (t)
label(NA) = (NA, A, B)
iknows {NA, A}pk(B) # state A [A, B, NA] #
witness A B NA NA # secret B NA # t ∈T
where label(l, n) = l. (Recall that every concrete value is a pair of label and a
natural number.)
Control Abstraction. We now come to the second part of the abstraction. Even
with the ﬁrst abstraction on data, the model gives us an inﬁnite number of traces
(that are of ﬁnite but of unbounded length). The idea for simpliﬁcation is that
under the data-abstraction, the trace structure is usually not relevant anymore.
In the reference model, we need the trace structure for the creation of fresh
data and for distinguishing potentially diﬀerent handling of the same constant
in diﬀerent traces. Under the data-abstraction, however, all these occurrences
fall together. In the abstract model we thus abandon the notion of traces and
consider only the set E of events that can ever occur.
Example 3. Our running example has now the following form:
iknows {(NA, A, B), A}pk(B) , state A [A, B, (NA, A, B)] ,
witness A B NA (NA, A, B) , secret B (NA, A, B) ∈E
state A [A, B, NA] ∈E
iknows {NA, NB, B}pk(A) ∈E
iknows {NB}pk(B) , request A B NB NB ∈E
5
Turning Fixed-Points into Proofs
We now turn to the proof generator itself (see Fig. 1), putting the pieces together
to obtain the security proof with respect to the reference model.
Let RG denote in the following the given set of the reference model that
describes the protocol, its goals, and the intruder behavior, as described in Sect. 2
and 3. Recall that OFMC chooses an abstraction for the data that honest agents
freshly create, and reﬁnes these abstractions if the veriﬁcation fails. As described

Integrating Automated and Interactive Protocol Veriﬁcation
257
in Sect. 4, the connection between the data abstraction and the reference model
is made by annotating each freshly created message with a label expressing the
abstraction. Since this annotation is never referred to in the conditions of any
rule, the set of traces remains the same modulo the annotation. We denote by
RM the variant of the rules with the annotation of the freshly created data.
The next step is the inductive deﬁnition of the set of traces T in Isabelle,
representing the least ﬁxed-point of RM. For such inductive deﬁnitions, Isabelle
proves automatically various properties (e.g., monotonicity) and derives an in-
duction scheme, i.e. if a property holds for the empty trace and is preserved
by every rule of RM, then it holds for all traces of T. This induction scheme is
fundamental for the security proof.
We deﬁne in Isabelle a set of traces T′ that represents the over-approximated
ﬁxed-point FP computed by OFMC, expanding all abstractions. We deﬁne this
via a concretization function ·:
l = {(l, n) | n ∈N}
f t1 . . . tn = {f s1 . . . sn | si ∈ti}
F = ∪f∈F f
T′ = {e1# . . . #en | ei ∈FP}
This replaces each occurrence of an abstract datum with an element of the
equivalence class it represents, and then builds all traces composed of events
from the ﬁxed-point. Note that while FP is ﬁnite, T′ is inﬁnite.
As the next step, the proof generation module proves several auxiliary theo-
rems using a set of specialized tactics we designed for this purpose. Each proved
auxiliary theorem can be used as a proof rule in subsequent proofs. We thus use
Isabelle as a framework for constructing a formal tool in a logically sound way
(see also [31]).
The ﬁrst auxiliary theorem is that T′ does not contain any attacks. The the-
orem is proved by unfolding the deﬁnition of the ﬁxed-point and applying Is-
abelle’s simpliﬁer.
The main part of the proof generation is an auxiliary theorem for each rule
r ∈RM that T′ is closed under r. In a nutshell, these theorems are also shown
by unfolding the deﬁnition of T and applying the simpliﬁer. In this case, how-
ever, on a more technical level, we need to convert the set comprehensions of
the deﬁnition into a predicate notation so that the simpliﬁer can recognize the
necessary proof steps. This is actually also the point where the labels that anno-
tate the abstraction silently fulﬁll their purpose: the rules are closed under any
concretization of the abstract data with the elements from the equivalence class
they represent. By our construction, the proof generation does not need to take
care of the abstraction at all.
Using the theorems that all rules are closed under T′, we can now show the
last auxiliary theorem, namely that T ⊆T′, i.e., that OFMC indeed computed
an over-approximation of what can happen according to the reference model.
This theorem is proved by induction, using the induction scheme we have auto-
matically obtained from the deﬁnition of T above, i.e. we show that the subset
relation is preserved for each rule of RM, using the set of the auxiliary theorems.

258
A.D. Brucker and S.A. M¨odersheim
Table 1. Analyzing security protocols using Isabelle/OFMC
Protocol
FP time [s]
Protocol
FP time [s]
ISO 1-pass (sk)
40
21
ISO 2-pass mutual (sk)
130
266
ISO 2-pass (sk)
56
60
NSCK
137
6756
NSL
75
64
TLS (simpliﬁed)
166
14202
DenningSacco
76
3984
ISO 2-pass (pk)
168
2965
ISO 1-pass (pk)
82
177
ISO 3-pass mutual (sk)
229
6992
Bilateral Key Exchange
87
182
ISO 2-pass mutual (ccf) 300
1808
Andrew Secure RPC
104
617
ISO 2-pass mutual (pk) 322
5295
DenningSacco
117
1472
ISO 1-pass (ccf)
418
1731
ISO 2-pass (ccf)
124
269
ISO 3-pass mutual (ccf) 664
14434
NSL (w. key server)
127
242
Finally, we derive our main theorem that T contains no attack, which imme-
diately follows from T ⊆T′ and T′ containing no attack. We have thus automat-
ically derived the proof that the protocol is safe from the given reference model
description and OFMC’s output.
6
Experimental Results
For ﬁrst experiments, we have considered several protocols from the Clark-Jacob
library [11] and a simpliﬁed version of TLS. Tab. 1 shows the results in detail,
namely the size of the ﬁxed-point and the time to generate and check the Isabelle
proof. Here, we have considered for each protocol all those secrecy and authen-
tication goals that do actually hold (we do not report on the well-known attacks
on some of these protocols that can be detected with OFMC). The runtime
for generating the ﬁxed-point in OFMC is negligible (< 10 s for each example),
while the runtime for Isabelle/OFMC is signiﬁcantly larger. We hope to improve
on the proof generator performance by ﬁne-tuning and specializing the low-level
proof tactics where we currently use generic ones of Isabelle.
We suggest, however, that the proof generation time does not aﬀect the ex-
perimentation with OFMC such as testing diﬀerent designs and variants of a
newly designed protocol, because the proof generation is meant only as a ﬁnal
step when the protocol design has been ﬁxed and veriﬁed with OFMC.
7
Related and Future Work
There is a large number of automated tools for protocol veriﬁcation. [4,6,7,10]
in particular are close to the method that is implemented in the new ﬁxed-point
module of OFMC: they are all based on an over-approximation of the search
space as described in Sect. 4: the ﬁrst over-approximation concerns the fresh
data, following the abstract interpretation approach of [14] and the second con-
cerns the control structure, i.e., considering a set of reachable events rather than

Integrating Automated and Interactive Protocol Veriﬁcation
259
traces. We propose that the other over-approximation-based tools can similarly
be connected to Isabelle as we did it for OFMC.
The work most closely related to ours is a recent paper by Goubault-Larrecq
who similarly considers generating proofs for an interactive theorem prover from
the output of automated tools [18]. He considers a setting where the protocol
and goal are given as a set S of Horn clauses; the tool output is a set S∞of Horn
clauses that are in some sense saturated and such that the protocol has an attack
iﬀa contradiction is derivable. He brieﬂy discusses two principal approaches
to the task of generating a proof from S∞. First, showing that the notion of
saturation implies consistency of the formula and that the formula is indeed
saturated. Second, showing the consistency by ﬁnding a ﬁnite model. He suggests
that the ﬁrst approach is unlikely to give a practically feasible procedure, and
rather follows the second approach using tools for ﬁnding models of a formula.
In contrast, our work, which is closer to the ﬁrst kind of approach, shows that
this proof generation procedure does indeed work in practice for many protocols,
comparable to the results of [18]. We see the main beneﬁt of our approach in the
fact that we can indeed use the output of established veriﬁcation tools dedicated to
the domain of security protocols. However, note that the work of [18] and ours have
some major diﬀerences which makes results hard to compare. First, we consider
a reference model where the protocol is modeled as a set of traces; the generated
proofs are with respect to this reference model and all abstractions are merely part
of the automatic tools. In contrast, [18] considers only one protocol model based
on Horn clauses, close to the abstract model in our paper. Taking the soundness
of all these abstractions for granted is a weakness of [18]. However, also our ap-
proach takes some things for granted, namely a strictly typed model and, based
on this, specialized composition rules for the intruder. The typing is common in
protocol veriﬁcation and can be justiﬁed by a reasonable protocol implementation
discipline [19]. The second assumption is justiﬁed by Theorem 1. Our next steps
are concerned with lifting these two assumptions from our reference model, i.e.
allowing for an untyped reference model with unbounded intruder composition.
First experiments suggest that at least the unbounded intruder composition can
be feasibly integrated into the proof generation procedure.
Similarly, [32] performs static analysis of security protocols using the tool
Rewrite which can generate proofs for the theorem prover Coq. Like in the case
of [18], the resulting proof is with respect to an over-approximated model only
and takes the soundness of all abstractions for granted.
A completely diﬀerent approach to achieve the same goal is currently followed
by Meier. Based on [21], he considers an embedding of the Scyther tool [15] into
Isabelle in order to generate proofs automatically when they fall into the scope
of the Scyther method. The advantage is here that one does not rely on a typed
model, while at the stage of this writing the proof generation is not in all cases
completely automated.
Several automated veriﬁcation tools are based on, or related to, automated
theorem provers, e.g. SATMC [2] generates Boolean formulae that are fed into
a SAT-solver, and ProVerif [4] can generate formulae for the ﬁrst-order theorem

260
A.D. Brucker and S.A. M¨odersheim
prover SPASS [30]. While there is some similarity with our approach, namely
connecting to other tools including the subtle modeling issues, this goes into
a diﬀerent direction. In fact, these approaches additionally rely on both the
correctness of the translation to formulae, and the correctness of the automated
theorem prover that proves them. In contrast, we generate the proof ourselves
and let Isabelle check that proof.
Several papers such as [9,5,23,13]have studied the relationships between proto-
col models and the soundness of certain abstractions and simpliﬁcations in par-
ticular. For instance, [13] shows that for a large class of protocols, two agents (an
honest and a dishonest one) is suﬃcient. Recall that we have used this as a stan-
dard abstraction of honest agents. While such arguments have thus played an im-
portant role in the design of the automated tool and its connection to the reference
model, the correctness of our approach does not rely on such arguments and the
question whether a given protocol indeed satisﬁes the assumptions. Rather, it is
part of the Isabelle proof we automatically construct that the abstract model in-
deed covers everything that can happen in the reference model. The automated
veriﬁer may thus try out whatever abstraction it wants, even if it is not sound.
Once again, in the worst case, the proof in Isabelle simply fails, if the abstrac-
tion is indeed unsound for the given protocol, e.g., when some separation of duty
constraints invalidate the assumptions of the two-agents abstraction.
This was indeed one of the main motivations of our work: our system does
not rely on the subtle assumptions and tricks of automated veriﬁcation. This
also allows for some heuristic technique that extends the classical abstraction
reﬁnement approaches such as [12]. There, the idea is to start with a simple most
abstraction, and when the automatic veriﬁcation fails, to reﬁne the abstraction
based on the counter-example obtained. This accounts for the eﬀect that the
abstraction may lead to incompleteness (i.e., failure to verify a correct system),
but it is essential that one uses only sound abstractions (i.e., if the abstract
model is ﬂawless then so is the concrete model). With our approach, we are now
even able to try out potentially unsound abstractions in a heuristic way, i.e. start
with abstractions that usually work (like the two-agent abstraction). If they are
unsound, i.e. the Isabelle proof generation fails, then we repeat the veriﬁcation
with a more reﬁned abstraction.
Isabelle has been successfully used for the interactive veriﬁcation in various
areas, including protocol veriﬁcation [26,3]. These works are based on a protocol
model that is quite close the our reference model (see Sect. 2). There are several
works on increasing the degree of automation in interactive theorem provers
by integrating external automated tools [17,29,16,27,22]. These works have in
common that they integrate generic tools like SAT or SMT tools. In contrast,
we integrate a domain-speciﬁc tool, OFMC, into Isabelle.
As further future work, we plan the development of additional Isabelle tac-
tics improving the performance of the veriﬁcation in Isabelle. Currently, the
main bottleneck is a proof step in which a large number of existential quantiﬁed
variables need to be instantiated with witnesses. While, in general, such satis-
fying candidates cannot be found eﬃciently, we plan to provide domain-speciﬁc

Integrating Automated and Interactive Protocol Veriﬁcation
261
tactics that should be able infer witnesses based on domain-speciﬁc knowledge
about the protocol model. Finally, we plan to eliminate the limitations on the
intruder inductions from the model explained in Sect. 3. While this limitation
can be reasonably justiﬁed in many cases, the fact that our approach relies on
it is a drawback, both in terms of eﬃciency and also theoretically. In fact, tools
like ProVerif instead employ a more advanced approach of rule saturation that
allow to work without the limitation. We plan to extend our approach to such
representations of the ﬁxed-point.
Acknowledgments. The work presented in this paper was partially supported by
the FP7-ICT-2007-1 Project no. 216471, “AVANTSSAR: Automated Validation
of Trust and Security of Service-oriented Architectures” (www.avantssar.eu).
We thank Luca Vigan`o for helpful comments.
References
1. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Hankes Drielsma, P., H´eam, P.C., Mantovani, J., M¨odersheim, S., von Oheimb, D.,
Rusinowitch, M., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.: The AVISPA
Tool for the Automated Validation of Internet Security Protocols and Applications.
In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576, pp. 281–285.
Springer, Heidelberg (2005), http://www.avispa-project.org
2. Armando, A., Compagna, L.: SAT-based Model-Checking for Security Protocols
Analysis. Int. J. of Information Security 6(1), 3–32 (2007)
3. Bella, G.: Formal Correctness of Security Protocols. Springer, Heidelberg (2007)
4. Blanchet, B.: An eﬃcient cryptographic protocol veriﬁer based on prolog rules. In:
CSFW 2001, pp. 82–96. IEEE Computer Society Press, Los Alamitos (2001)
5. Blanchet, B.: Security protocols: from linear to classical logic by abstract interpre-
tation. Information Processing Letters 95(5), 473–479 (2005)
6. Boichut, Y., H´eam, P.C., Kouchnarenko, O., Oehl, F.: Improvements on the Genet
and Klay technique to automatically verify security protocols. In: AVIS 2004, pp.
1–11 (2004)
7. Bozga, L., Lakhnech, Y., Perin, M.: Pattern-based abstraction for verifying secrecy
in protocols. Int. J. on Software Tools for Technology Transfer 8(1), 57–76 (2006)
8. Brucker, A., M¨odersheim, S.: Integrating Automated and Interactive Protocol Ver-
iﬁcation (extended version). Tech. Rep. RZ3750, IBM Zurich Research Lab (2009),
http://domino.research.ibm.com/library/cyberdig.nsf
9. Cervesato, I., Durgin, N., Lincoln, P.D., Mitchell, J.C., Scedrov, A.: A Comparison
between Strand Spaces and Multiset Rewriting for Security Protocol Analysis. In:
Okada, M., Pierce, B.C., Scedrov, A., Tokuda, H., Yonezawa, A. (eds.) ISSS 2002.
LNCS, vol. 2609, pp. 356–383. Springer, Heidelberg (2003)
10. Chevalier, Y., Vigneron, L.: Automated Unbounded Veriﬁcation of Security Pro-
tocols. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS, vol. 2404, pp.
324–337. Springer, Heidelberg (2002)
11. Clark, J., Jacob, J.: A survey of authentication protocol: Literature: Version 1.0
(1997), http://www.cs.york.ac.uk/~jac/papers/drareview.ps.gz
12. Clarke, E., Fehnker, A., Han, Z., Krogh, B., Ouaknine, J., Stursberg, O., Theobald,
M.: Abstraction and counterexample-guided reﬁnement in model checking of hybrid
systems. Int. J. of Foundations of Computer Science 14(4), 583–604 (2003)

262
A.D. Brucker and S.A. M¨odersheim
13. Comon-Lundh, H., Cortier, V.: Security properties: two agents are suﬃcient. In:
Degano, P. (ed.) ESOP 2003. LNCS, vol. 2618, pp. 99–113. Springer, Heidelberg
(2003)
14. Cousot, P.: Abstract interpretation. Symposium on Models of Programming Lan-
guages and Computation, ACM Computing Surveys 28(2), 324–328 (1996)
15. Cremers, C.: Scyther. Semantics and Veriﬁcation of Security Protocols. Phd-thesis,
University Eindhoven (2006)
16. Erk¨ok, L., Matthews, J.: Using Yices as an automated solver in Isabelle/HOL. In:
AFM 2008 (2008)
17. Fontaine, P., Marion, J.Y., Merz, S., Nieto, L.P., Tiu, A.F.: Expressiveness + au-
tomation + soundness: Towards combining SMT solvers and interactive proof as-
sistants. In: Hermanns, H., Palsberg, J. (eds.) TACAS 2006. LNCS, vol. 3920, pp.
167–181. Springer, Heidelberg (2006)
18. Goubault-Larrecq, J.: Towards producing formally checkable security proofs, au-
tomatically. In: CSF 2008, pp. 224–238. IEEE Computer Society, Los Alamitos
(2008)
19. Heather, J., Lowe, G., Schneider, S.: How to prevent type ﬂaw attacks on security
protocols. In: CSFW 2000. IEEE Computer Society Press, Los Alamitos (2000)
20. Lowe, G.: Breaking and ﬁxing the Needham-Schroeder public-key protocol using
FDR. In: Margaria, T., Steﬀen, B. (eds.) TACAS 1996. LNCS, vol. 1055, pp. 147–
166. Springer, Heidelberg (1996)
21. Meier, S.: A formalization of an operational semantics of security protocols.
Diploma thesis, ETH Zurich (2007), http://people.inf.ethz.ch/meiersi/fossp
22. Meng, J., Quigley, C., Paulson, L.C.: Automation for interactive proof: First pro-
totype. Information and Computation 204(10), 1575–1596 (2006)
23. M¨odersheim, S.: On the Relationships between Models in Protocol Veriﬁcation. J.
of Information and Computation 206(2–4), 291–311 (2008)
24. M¨odersheim, S., Vigan`o, L.: The open-source ﬁxed-point model checker for sym-
bolic analysis of security protocols. In: Aldini, A., Barthe, G., Gorrieri, R. (eds.)
FOSAD 2007. LNCS, vol. 4677, pp. 166–194. Springer, Heidelberg (2007)
25. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL: A Proof Assistant for
Higher-Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002)
26. Paulson, L.C.: The inductive approach to verifying cryptographic protocols. J. of
Computer Security 6(1-2), 85–128 (1998)
27. Paulson, L.C., Susanto, K.W.: Source-level proof reconstruction for interactive the-
orem proving. In: Schneider, K., Brandt, J. (eds.) TPHOLs 2007. LNCS, vol. 4732,
pp. 232–245. Springer, Heidelberg (2007)
28. Roscoe, A.W., Goldsmith, M.: The perfect spy for model-checking crypto-protocols.
In: DIMACS (1997)
29. Weber, T., Amjad, H.: Eﬃciently checking propositional refutations in HOL theo-
rem provers. J. of Applied Logic 7(1), 26–40 (2009)
30. Weidenbach, C., Schmidt, R.A., Hillenbrand, T., Rusev, R., Topic, D.: System
description: Spass version 3.0. In: Pfenning, F. (ed.) CADE 2007. LNCS (LNAI),
vol. 4603, pp. 514–520. Springer, Heidelberg (2007)
31. Wenzel, M., Wolﬀ, B.: Building formal method tools in the Isabelle/Isar framework.
In: Schneider, K., Brandt, J. (eds.) TPHOLs 2007. LNCS, vol. 4732, pp. 352–367.
Springer, Heidelberg (2007)
32. Zunino, R., Degano, P.: Handling exp, × (and Timestamps) in Protocol Analysis.
In: Aceto, L., Ing´olfsd´ottir, A. (eds.) FOSSACS 2006. LNCS, vol. 3921, pp. 413–
427. Springer, Heidelberg (2006)

A User Interface for a Game-Based Protocol
Veriﬁcation Tool⋆
Peeter Laud1 and Ilja Tˇsahhirov2
1 Cybernetica AS and Tartu University
peeter@cyber.ee
2 Institute of Cybernetics at Tallinn University of Technology
itshahhirov@gmail.com
Abstract. We present a platform that allows a protocol researcher to
specify the sequence of games from an initial protocol to a protocol where
the security property under consideration can be shown to hold using
“conventional” means. Our tool represents the protocol in the form of
a program dependency graph. A step in the sequence corresponds to
replacing a local fragment in the current graph. The researcher interacts
with the tool by pointing out the location of this fragment and choosing
the applied transformation from a list. The tool guarantees the error-
freeness of the sequence. By our knowledge, this is the ﬁrst time where the
aspects of user interaction have been seriously considered for a sequence-
of-games-based protocol analyzer.
1
Introduction
The sequence-of-games-based approach is a method for giving security proofs
for cryptographic protocols that is at the same time computationally sound and
suﬃciently organized for keeping track of all the details about the probabilities
and conditional probabilities of various events. It is based on the fact that most
cryptographic primitives have their security deﬁnitions stated as two experiments
(or cryptographic games) that an adversary can interact with. A primitive is
secure if the adversary cannot tell those two experiments apart. In this approach,
the security proof of a cryptographic protocol (or a primitive) consists of two
steps (which may take place simultaneously). The ﬁrst step is the construction of
a sequence of cryptographic games, the ﬁrst of which is the original protocol and
the last is a game that obviously fulﬁlls the security property we want to prove
(e.g. if the goal is the conﬁdentiality of some value, the ﬁnal game should contain
no references to that value). The second step is the veriﬁcation that to a resource-
bounded adversary, each protocol in that sequence is indistinguishable from the
one that immediately precedes it. To make such veriﬁcation easy, the neighboring
⋆This research has been supported by Estonian Science Foundation, grant #6944, by
the European Regional Development Fund through the Estonian Center of Excel-
lence in Computer Science, EXCS, and by EU Integrated Project AEOLUS (contract
no. IST-15964).
P. Degano and J. Guttman (Eds.): FAST 2009, LNCS 5983, pp. 263–278, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

264
P. Laud and I. Tˇsahhirov
protocols in that sequence should syntactically diﬀer only a little. For example,
a protocol in that sequence may have been obtained from the previous one by
locating one of the experiments from the deﬁnition of a cryptographic primitive
in the code of this protocol, and replacing that part of the code with the code
of the other experiment. Alternatively, the change from one protocol to the next
could be a simple program transformation/optimization (e.g. copy propagation),
done in order to make locating one of the aforementioned experiments easier.
A protocol researcher needs tool support for both steps of the proof done in
the style of sequences of games. As a protocol in the sequence is constructed
by applying a rather small change to the previous protocol, it makes sense to
constrain the researcher in constructing the next protocol, thereby avoiding tran-
scription errors. The veriﬁcation of the proof (the second step) is also better left
to an automated theorem prover.
The most recent results in this area mostly tackle the second problem — veri-
fying the given sequence of games. Languages for cryptographic games have been
proposed and certain program transformations have been proven (using proof
assistants, such as Coq or Isabelle/HOL) to keep the games indistinguishable to
an adversary [5,9]. In contrast, we consider the ﬁrst problem in this paper. We
present a tool that helps a protocol researcher to interactively construct that se-
quence. So far, similar tools (Blanchet’s CryptoVerif [12,13] and the analyzer of
Tˇsahhirov and Laud [37]) have worked almost fully automatically. An automatic
generation of a game sequence is convenient, but not necessarily scalable. There
are no guarantees that the set of transformations that the analyzer applies is
convergent. Hence the analyzer may get stuck in a game that is not yet obvi-
ously secure but also cannot be transformed any longer, while a diﬀerent order
of transformations could have lead to a complete sequence. One may try to come
up with heuristics for choosing the order of transformations, but this approach
is certainly not complete and may not be worth the eﬀort. Instead, one should
rely on the knowledge of the protocol designer — he/she should have some idea
why the protocol is secure, and be able to guide the analyzer.
Our tool is an extension of our protocol analyzer [37,36]. The protocol is pre-
sented to the protocol researcher in a form (a dependency graph) that we believe
is relatively easy comprehend and where, importantly, the location where one
wishes to apply a certain transformation can be easily indicated. The researcher
starts with the initial protocol representation (translated from a language simi-
lar to applied π-calculus) and applies one transformation after another until the
protocol is easy to analyze. The tool makes sure that the researcher will not make
invalid transformations. The form in which the protocols are represented has a
well-deﬁned semantics, hence it should not be too diﬃcult to combine our tool
with some of the veriﬁers of game sequences we have mentioned above to create
a complete tool-chain for producing computationally sound proofs of protocols.
2
Related Work
The task of tool-supported computationally sound proving of security proper-
ties of cryptographic properties has received closer attention for almost a decade

A User Interface for a Game-Based Protocol Veriﬁcation Tool
265
now. Starting from Abadi and Rogaway [4], a line of work [3,29,16,22,15] has
attempted to show that the security of a protocol in formal model implies its
security in the computational model, thereby leveraging the body of work on
protocol analysis in the formal model. In parallel to that, program analyses have
been devised that are correct with respect to the computational security deﬁni-
tions of cryptographic primitives [39,23,24,27,33,20]. A somewhat similar line of
work tries to axiomatize the computational semantics of protocols [28,17,18].
A somewhat diﬀerent “formal model” with full computational justiﬁcation was
oﬀered by Backes et al. [7] in the form of a universally composable cryptographic
library. Various methods of protocol analysis (in the formal model) have been
successfully carried over to this model, including type systems [26,1], abstract
interpretation [6] and theorem-proving [34,35].
The consideration of the sequence of code transformations as a universally
and automatically applicable method for protocol analysis ﬁrst appeared in [25].
The method was generally popularized by Bellare and Rogaway [11] as the game-
based method. It was quickly recognized as allowing automated or computer as-
sisted analysis of protocols [32]. By now, the underlying principles of the method
have been formalized, also in proof assistants [14,30,5,9] and automatic analyzers
have appeared [12,37]. Active research is going on in this area.
3
Game-Based Protocol Analysis
A cryptographic game is the interaction between the adversary and its envi-
ronment containing the protocol we want to analyze. A game is speciﬁed by
describing the operations that the environment performs and values it makes
available to, or receives from the adversary. The adversary’s goal is to bring the
game to a state that is considered as winning for it. For example, the adversary
may win a game if it correctly guesses a bit generated by the environment.
To formally argue about a game, and to locate a game (or an experiment) in
a larger game, it has to be expressed in a programming language with formal
semantics. In the sequence-of-games-based protocol analysis, the initial game is
transformed to a ﬁnal game that is obviously secure. Each transformation step
changes the game in a way that makes the adversary’s winning probability larger
or only negligibly smaller. In the latter case we have assumed that the adversary’s
running time is constrained to be polynomial; in the following we only consider
probabilistic polynomial-time (PPT) adversaries. The obviousness of the security
of the ﬁnal game just means that it is easy to analyze and bound the adversary’s
probability of winning by using some conventional means. For example, if the
adversary’s goal is to guess a randomly generated bit, and the ﬁnal game makes
no references to that bit, then the adversary’s winning probability is deﬁnitely
no more than 1/2.
4
Protocol Representation
We use dependency graphs as our intermediate representation of protocols [37,36].
It has advantages with respect to abstract syntax trees / control ﬂow graphs

266
P. Laud and I. Tˇsahhirov
(used by CryptoVerif) in naturally allowing certain transformations one would
like to invoke after applying a cryptographic transformation. Also, the depen-
dency graph emphasizes the producers and consumers of diﬀerent data items
and henceforth appears to be a natural way to specify cryptographic games (de-
spite the tendency to use imperative languages for that purpose in cryptographic
literature).
The dependency graph is a directed graph, where each node corresponds to
a computation, producing a value (either a bit-string or a Boolean). The edges
of the graph indicate which nodes use values produced at another nodes. A
computation happening at a node could be the execution of a cryptographic
algorithm, an arithmetic or a boolean operation. The values produced are either
bit strings or boolean values. The values produced outside of the graph (for
example, random coin tosses, incoming messages, secret payloads) are brought
into it via special nodes, having no incoming edges. Additionally, certain nodes
(modeling the sending of messages) explicitly make their input values available
to the adversary.
Program dependency graphs have originated as a program analysis and op-
timization tool [19], systematically recording the computational relationships
between diﬀerent parts of a program. Since then, several ﬂavors of dependency
graphs have been proposed, some of them admitting a formal semantics [8,31],
thus being suitable as intermediate program representations in a compiler. Pro-
grams represented as dependency graphs are amenable to aggressive optimiza-
tions as all program transformations we may want to apply are incremental on
dependency graphs. The translation from an optimized dependency graph back
to a sequence of instructions executable on an actual processor may be tricky as
the optimizations may have introduced patterns that are not easily serializable.
This is not al issue for us because we do not have to translate the optimized /
simpliﬁed / analyzed protocol back to a more conventional form.
The formal deﬁnition and semantics of dependency graphs (DGs) can be found
in [36]. Informally, DG is a directed, possibly inﬁnite graph where each node v
contains an operation λ(v) and edges carry the values produced by their source
node to be used in the computation at the target node. The nodes have input
ports to distinguish the roles of incoming values. For each port of each node,
there is exactly one incoming edge. The “normal” nodes of a DG are functional
— same inputs cause it to produce the same output. Special nodes are used for
inputs from and outputs to the outside world.
To represent scheduling information, most computational nodes of a DG have
a special boolean input — the control dependency. A node can execute only if
the value of its control dependency is true (initially, the value of all nodes is
either ⊥(for nodes producing bit-strings) or false). During the execution of the
dependency graph, the adversary can set the values of certain Boolean-valued
input-nodes labeled Req to true and thereby initiate the execution of (certain
parts of) the DG.
The execution of a DG proceeds in alteration with the adversary. First the
adversary sets some Req-nodes and/or the values of some Receive-nodes (these

A User Interface for a Game-Based Protocol Veriﬁcation Tool
267
nodes bring bit-string inputs to the DG). The setting of these nodes causes cer-
tain nodes of the DG to compute their values. If a value reaches some Send-node
then such value is reported back to the adversary. The adversary can then again
set some Req- and Receive-nodes and the process repeats, until the adversary
decides to stop. The adversary then tries to output something related to secret
values in the environment, made available to the DG through Secret-nodes.
Two dependency graphs G1 and G2 with the same set of input/output nodes
(labeled Req, Receive or Send) are indistinguishable if for all PPT adversaries A,
the output of A running in parallel with G1 is indistinguishable from its output
if it runs in parallel with G2. A dependency graph is polynomial if at any time
the number of its nodes with values diﬀerent from ⊥or false is polynomial in
the number of its Receive- and Req-nodes that the adversary has set.
A game transformation is given by two dependency graph fragments (DGF).
A DGF is basically a DG without the input/output nodes of a regular DG, but
having some input/output nodes of its own (in principle: edges with one end
inside and the other end outside of the DGF), for both Booleans and bit-strings.
A DGF can be executed by the adversary, similarly to a regular DG. Again, the
adversary can (iteratively) set the inputs to the DGF and learn the outputs. The
indistinguishability and polynomiality for DGF-s is deﬁned in the same way as
for DG-s.
Deﬁnition. An occurrence of a DGF H in a DG G is a mapping ϕ from the
input and internal nodes of H to the nodes of G, such that
– if v and w are input or internal nodes of H, then there is an edge from v to
the port π of w iﬀthere is an edge from ϕ(v) to the port π of ϕ(w);
– if there is an edge from ϕ(v) to some node u in G, such that u is not the
image of some internal node of H under ϕ, then there must be an edge from
v to an output node in H.
If H and H′ have the same inputs and outputs, and ϕ is an occurrence of H
in G then we can replace this occurrence by H′ by removing from G all nodes
ϕ(v), where v is an internal node of H, and introducing the internal nodes and
edges of H′ in their stead.
Theorem 1. If polynomial DGFs H and H′ are indistinguishable, and DG G′
is obtained from polynomial G by replacing an occurrence of H with H′, then
G and G′ are indistinguishable and G′ is polynomial, too [36].
Each node of the DG corresponds to a single operation that the system may
perform. To model that some role of some protocol may be executed up to n
times, we have to analyze a DG containing n copies of that role. To model that
some role of some protocol may be executed an unbounded number of times, or
that a party can take part in unbounded number of protocol sessions, requires
inﬁnite dependency graphs. In inﬁnite dependency graphs, the set of nodes is
countably inﬁnite. Also, certain nodes (conjunction and disjunction) may have a
countable number of predecessors. A dependency graph fragment can similarly
be inﬁnite.

268
P. Laud and I. Tˇsahhirov
As the inﬁniteness of a dependency graph is typically caused from the inﬁnite
repeating of certain ﬁnite constructs, the graph is regular enough to be ﬁnitely
represented. Details can be found in [36]. Here we mention only that we are ac-
tually working with dependency graph representations (DGR-s) where each node
may represent either a single, or countably many (identiﬁed with the elements
of NX for a certain ﬁnite set X, recorded in the DGR node) nodes in the ac-
tual DG. Similarly, DGFs generalize to DGFRs — dependency graph fragment
representations.
On the choice of protocol representation. We believe that the representa-
tion based on dependency graphs will be more convenient to use than the one
based on abstract syntax trees as used by CryptoVerif. There are several reasons
for that. First, the enabling conditions for transformations are more often locally
represented in dependency graphs. Hence they should be easier to notice by the
protocol researcher (but importantly, the visualizer also has to make it easy to
locate interesting vertices and to explore their neighborhoods). Second, point-
ing at the to-be-transformed part of the protocol is very simple using a graph
representation, while it may require doing a complex selection in a textual rep-
resentation. Third and most importantly, all information is easily available in a
dependency graph of the protocol, possibly annotated with nodes carrying the
results of its static analysis. CryptoVerif contains not just the language for rep-
resenting protocols, but also a language for true facts and rewrite rules it has
collected for a protocol [13, App. C.2–C.5]. The user cannot control which facts
are derived. In an interactive tool, these facts might be added to the textual rep-
resentation of the program as annotations, but there does not necessarily exist
an obvious location in the text for them. Fourth, the graphical representation
allows certain natural transformations for which there is no equivalent in the
syntax-tree-based representation.
5
The Tool
Our tool takes as an input a protocol speciﬁed in a language remniscient to
the applied π-calculus [2], translates it into a dependency graph representation,
presents it on the screen and allows the researcher to pick a particular transfor-
mation and the occurrence of the ﬁrst DGFR. This occurrence is validated and
then replaced by the second DGFR speciﬁed by the transformation, the result is
again displayed and the researcher can choose the next transformation to apply.
There is no obvious end-point to the analysis; at some moment the researcher
can decide that the transformed protocol is now obviously secure.
We use the graph visualizer uDraw(Graph) [21,38] as the front-end of our tool.
It receives the commands to change the displayed graph from our tool, and sends
back the actions of the user — the selected nodes and edges, as well as names of
the chosen transformations. The visualizer allows the user to explore the graph
and to change its layout. Fig. 1 shows a screenshot of the visualizer after load-
ing a protocol that has just been translated from the π-calculus-like language to a

A User Interface for a Game-Based Protocol Veriﬁcation Tool
269
Fig. 1. A screenshot of the visualizer with a loaded DGR
DGR. We see that the translation procedure itself is straightforward and does
not attempt to optimize the DGR. In the visualizer, the ﬁrst row of a node shows
its ID and label, and the second row shows the elements of the (multi)set X of
its replication dimensions. This node of the DGR corresponds to NX nodes in
the actual DG.
The tool has been implemented in OCaml. The components of the tool are
its main loop (driving the interaction), the graph transformer and the various
transformations. A transformation is speciﬁed as an OCaml module describing
the initial and the ﬁnal DGFR. Additionally, it contains a method for helping
the user to choose the occurrence of the initial DGFR in the DGR. Instead
of selecting all nodes and edges comprising a DGFR, as well as specifying the
embedding of the DGFR in the DGR, the user has to select only a couple of ﬁxed
nodes/edges of the DGFR and this method will reconstruct the whole DGFR.
The graph transformer is an OCaml functor receiving a graph transformation as
an input and returning a module containing a function that takes as arguments
a DGR and the names of the selected nodes / edges, and returns the transformed
DGR (or an error message). The main loop receives the node and edge selection
commands from the visualizer, as well as the name of the transformation menu
element that the user has selected. It calls the correct graph transformation
function, ﬁnds the diﬀerence between the original and the modiﬁed DGR, and
sends that diﬀerence back to the visualizer.
5.1
Specifying a Family of Pairs of DGFRs
It makes sense to parameterize certain parts of DGFRs. For example, to ex-
press that tupling followed by projection just selects one of the inputs (modulo

270
P. Laud and I. Tˇsahhirov
control dependencies and ⊥-s): πn
i ((x1, . . . , xn)) →xi, then there should not be
a separate transformation for each n and i, but those should be the parameters
of the transformation.
The DGFR pairs are speciﬁed as OCaml modules with a certain signature.
In eﬀect, our approach can be described as a shallow embedding of DGFRs into
OCaml. A module conforming to that signature has to ﬁrst deﬁne an OCaml
data type for variable names. The variable names are used as a part in the
type for variables; the variables map to the nodes and edges of DGFRs, but also
to other values. As next, the module has to declare a mapping from variable
names to their types. There are 5+1 possible types for a variable or a variable
name — it can denote either an integer, a node, an edge, a set of dimensions, a
map of dimensions (attached to edges going from one summary node in the DGR
to another one; describing how the edges of DGR must be mapped to edges in
the DG), or an array where all elements have the same type. The type “node”
has three subtypes — a node can be either an input node, an internal node or
an output node.
Given the datatype X of variable names, the variables of type V are deﬁned
either as scalars of type X or elements (v, i), where v ∈V and i is a natural
number. The module then has to deﬁne a number of functions that map the
variables to their values, in eﬀect describing a DGFR. The following functions
have to be deﬁned
– a map from variables of type “array” to their length;
– maps from variables of type “edge” to their source and target (variables of
type “node”), dimension map (variables of type “dimension map”) and the
input port at the target node;
– maps from the variables of types “integer”, “dimension”, and “dimension
map”, giving their actual values;
– maps from variables of type “node” giving their label, and their dimension
(and also the input dimension if it the node is a contracting node).
Importantly, all those functions can call each other if necessary. Circular depen-
dencies will be detected.
The module has to specify two lists of variable names. The elements of these
lists correspond to the nodes and edges in the initial and ﬁnal DGFR, respec-
tively. During the transformation, the nodes and edges only in the ﬁrst list will
be removed and those only in the second list will be added.
In the code of the functions that the module has to deﬁne, it is possible to
ask for the actual parameters of the nodes and edges that are the values of the
variables with the names in the list deﬁning the initial DGFR. One can ask for
the actual labels and dimensions of the variables of type “node”, and dimension
maps of the variables of type “edge”.
The module has to deﬁne a validation function that tells whether the values
of initial variables (variables whose names are in the ﬁrst list) deﬁne a valid
DGFR. The function can assume that the nodes and edges are connected in the
way given by the functions we described before, but it still has to verify that the
nodes have the correct labels. If the transformation depends on it, this function

A User Interface for a Game-Based Protocol Veriﬁcation Tool
271
also has to verify that the dimensions and dimension maps of nodes and edges
are suitable.
We see that our deﬁnition of DGFRs abstracts away from the actual DGR.
Indeed, both the validation of the initial DGFR and the construction of the ﬁnal
DGFR are made in terms of DGFR variables. Only the expansion function
that the module also has to deﬁne has access to the actual DGR. The task of
this function is to assign values to the variables whose names are in the list of
variable names for the initial DGFR. For variables of type “array”, it also has to
deﬁne the length of the array. The inputs to this function are the DGR, and the
identities of certain nodes and edges of the DGR, which the expansion function
will treat as the values of certain ﬁxed DGFR variables.
5.2
The Graph Transformer
A graph transformer will be deﬁned for each of the transformations speciﬁed
as the pair of two DGFRs, but the deﬁnition is through an OCaml functor.
Given the transformation module TrM , the transformer function will take a
DGR and the node identities as arguments. First, it passes the arguments to the
expansion function of TrM and receives a mapping ϕ from initial variables to
nodes and edges of the graph. Second, it veriﬁes that the ϕ indeed constitutes
an occurrence of a DGFR in the given DGR — the internal nodes must have
all their predecessor and successor nodes also as elements of the DGFR. Third,
it invokes the validation function of TrM on the received DGFR. Fourth, it
performs the actual change of the DGFR — it deletes the nodes and edges that
occur in the initial DGFR, but not in the ﬁnal. Then it adds new nodes and
edges (corresponding to the variable names that occur in the list for the ﬁnal
DGFR, but not in the list for the initial DGFR). It calls the functions of TrM
to ﬁnd the parameters of those nodes and edges.
The graph transformer also makes sure that the values computed by the func-
tions of TrM are memoized and that the computation of a certain function on
a certain variable does not (possibly indirectly) invoke the same function on the
same variable again. Additionally, the graph transformer veriﬁes the outputs of
the functions of TrM . For example, if the result of the function returning the
dimension of a variable of type “node” is not a variable of type “dimension”
then the transformation is immediately halted. Hence the typing of variables is
enforced, albeit dynamically.
5.3
Example Analysis
Let us consider a situation where A and B have a long-term shared key KAB,
but whenever B wants to send a secret M to A, ﬁrst A generates a short-term
key k, sends it encrypted under KAB to B who then uses k to protect M. In the
conventional “arrow-notation” this protocol can be speciﬁed as follows:
A−→B : {k}KAB
B−→A : {M}k
A−→
: OK
(1)

272
P. Laud and I. Tˇsahhirov
The initial protocol (game), directly corresponding to (1), but simpliﬁed from
the output of the translator is depicted in Fig. 2. Here solid edges carry bit-strings
and dashed edges booleans. The node 43 generates the key KAB. The nodes
60–179 represent a session of A; those nodes have the (multi)set of dimensions
{A}, i.e. we are modeling an unbounded number of sessions. The ﬁrst message
is constructed by nodes 60 and 96 (the random coins for these operations are
provided by special RS-nodes 63 and 99), and sent away by the node 118; the
adversary can request it to be sent by setting (an instance of) the node 119 to
true. The second message is received in node 136, decrypted in 142, and if it
decrypts successfully (node 154) then the third message is sent in nodes 171 and
178. Similarly, nodes 196–245 represent a session of B — receiving the ﬁrst and
constructing and sending the second message.
As the node 43 is only used for encryption and decryption, we can apply
a transformation corresponding to the IND-CCA- and INT-CTXT-security of
symmetric encryption [10] to it. We select node 43 and choose “Replace a secret-
key decryption” from the menu. The resulting graph is depicted in Fig. 3.
The transformation introduced the nodes 674–680. At ﬁrst, it replaced the
encryption node 96 with the node 676 labeled SymencZ. This operation encrypts
a ﬁxed bit-string ZERO using the random coins and the key that are given to it.
The string ZERO cannot be the output of any node in the DG. The SymencZ-
node does not use the plaintext argument (60) of the original node. Still, it
should not produce output if the original plaintext has not been computed.
Hence the test (node 674) whether it has been computed is part of the control
dependency of node 676. At the decryption side, the ciphertext 196 is compared
to all computed encryptions of ZERO in node 677 (note its set of dimensions). If
one of them matches then it the corresponding plaintext (node 60) is selected as
the result of decryption by nodes (“multiplexers”) 679 and 680. The MUX-nodes
have an arbitrary (ﬁnite) number of inputs, their output is the least upper bound
of their inputs. I.e. if all inputs are ⊥then the output is ⊥and if exactly one
input is diﬀerent from ⊥then the output is equal to that input. If more than
one input is diﬀerent from ⊥(in this transformation, the number of inputs to
MUX-nodes is equal to the number of SymEnc-operations) then the result of this
operation is ⊤, denoting an inconsistency in the DG. A inconsistency means an
immediate termination of the computation; this is visible to the adversary (i.e.,
if the transformations are correct, then this can happen only with negligible
probability). Similarly, an lMUX is a “long MUX” — in a DG it is a node with
inﬁnite number of pairs of inputs (a bit-string and a boolean). If no boolean
inputs are true, it returns ⊥. If exactly one of the boolean inputs is true then it
returns the corresponding bit-string input. In a DGR, a lMUX is a contracting
node — in our example, node 679 contracts the dimension A.
We would like to apply the symmetric encryption transformation also to node
60, but it has two uses forbidding that. We get rid of the use by node 674 by
noting that SymKey succeeds if there are random coins incoming from RS, and if
the control dependency is true. The RS-node 63 always produces coins because

A User Interface for a Game-Based Protocol Veriﬁcation Tool
273
k
k
r
r
r
r
k
k
179: req
A
137: and
A
154: bit-string OK?
A
142: Symdec
A
171: Const(1)
A
136: Receive
A
172: and
A
60: Symkey
A
118: Send
A
96: Symenc
A
119: req
A
384: and
A
63: RS
A
224: true
43: Symkey
99: RS
A
220: Symenc
B
202: Symdec
B
196: Receive
B
46: RS
244: Send
B
394: and
B
75: ooor
[A]
214: Secret
B
223: RS
B
245: req
B
45: or
241: ooor
[B]
178: Send
A
Fig. 2. Initial protocol
its control dependency is true. Hence the value of node 674 equals the value of
node 384 and we get rid of this use of node 60.
The other use by node 679 eventually ends up in the SymEnc-node 220. As
MUX- and lMUX-nodes do not change the values passing through them, we can
swap them with the operations following them. Hence we can move the SymDec-
node ﬁrst to the other side of node 680 and then node 679, resulting in the graph
depicted in Fig. 4. We see that the encryption node (with new ID 704) is now
right next to the SymKey-node 60. The ability to do such swaps of operations
with multiplexers is one of the main advantages of dependency graphs.
It is instructive to consider how the second message {M}k (sent by node 244)
is computed in this graph. In the b-th round of B, this rounds secret message
Mb is encrypted with keys ka for all rounds a of A. The correct round a is then
chosen by nodes 795 and 697 by comparing the ﬁrst message received by B (node
196) in this round with the ﬁrst messages sent by A (node 118, sending the result
of node 676) in all rounds.
The next transformation steps should be obvious. After getting rid of the node
674 (described above) we apply the symmetric encryption transformation to the
key generation 60. This will get rid of the use of the Secret-node by node 704.

274
P. Laud and I. Tˇsahhirov
179: req
A
137: and
A
154: bit-string OK?
A
142: Symdec
A
171: Const(1)
A
136: Receive
A
172: and
A
60: Symkey
A
118: Send
A
119: req
A
384: and
A
63: RS
A
224: true
43: Symkey
99: RS
A
220: Symenc
B
196: Receive
B
46: RS
244: Send
B
394: and
B
75: ooor
[A]
214: Secret
B
223: RS
B
245: req
B
45: or
241: ooor
[B]
178: Send
A
674: bit-string OK?
A
675: and
A
676: SymencZ
A
677: =?
B,A
r
r
k
k
r
678: and
B,A
679: lMUX
[A]
B
680: MUX
B
r
k
Fig. 3. Applying encryption transformation to KAB
The Secret-node will then be used by the nodes replacing the decryption node
142, as it has to be returned as the plaintext. It will be an input to a multiplexer
whose output is only used by node 154. The position of node 154 will be swapped
with multiplexers, making the OK?-node an immediate successor of the Secret-
node. The use of Secret-node by an OK?-node can be transformed away and the
Secret-node had no other uses. We are left with a dead Secret-node that can be
removed. Thus the conﬁdentiality of secrets is preserved.

A User Interface for a Game-Based Protocol Veriﬁcation Tool
275
k
k
k
r
r
r
r
119: req
A
241: ooor
[B]
75: ooor
[A]
245: req
B
224: true
179: req
A
63: RS
A
45: or
46: RS
223: RS
B
99: RS
A
137: and
A
384: and
A
394: and
B
43: Symkey
136: Receive
A
60: Symkey
A
214: Secret
B
196: Receive
B
142: Symdec
A
674: bit-string OK?
A
704: Symenc
B,A
675: and
A
154: bit-string OK?
A
676: SymencZ
A
172: and
A
171: Const(1)
A
677: =?
B,A
118: Send
A
678: and
B,A
178: Send
A
705: lMUX
[A]
B
697: MUX
B
244: Send
B
Fig. 4. Moving Symenc over MUX-s
6
Conclusions and Future Work
The presented example was very simple. More complex examples require the in-
troduction of more and diﬀerent kinds of nodes, as well as extending the deﬁni-
tion of semantics of the dependency graph (but still keeping it mostly functional,
i.e. without side eﬀects) [36]. The extension serves to bring back arguments about
the order of execution. We introduce the nodes labeled Before with two Boolean
inputs u and v. The label of the node is going to change to either true or false
during the execution of the protocol, depending on the order in which u and v

276
P. Laud and I. Tˇsahhirov
become true. Such nodes require us to spend more eﬀort on proving the trans-
formations correct — it is not merely suﬃcient to prove that identical inputs to
two graph fragments lead to identical (or indistinguishable) outputs, but it is
also necessary to consider increasing sequences of inputs. See [36] for details.
The work on the analyser continues — the planned expansions include addi-
tional arithmetic operations, cryptographic primitives (diﬀerent security prop-
erties) and equivalences, as well as diﬀerent control structures. Adding loops as
a control structure would allow to apply the analyzer to various cryptographic
primitives constructed from simpler primitives, e.g. to block ciphers modes of op-
eration. Expressing loops in dependency graphs has been a researched topic [31].
But in our formalization, where we already have inﬁnite dependency graphs, the
unrolling of loops seems to be the easiest way to express them. A multiplexer
will be used to select the value of the variable from the last (in the current
run) iteration of the loop. In the dependency graph representation, a loop will
introduce an extra dimension.
The analyser should also leave a trail of its work that can later be used as a
proof that the initial and ﬁnal protocols have indistinguishable semantics. The
proof should be veriﬁable using a proof assistant, e.g. Coq. The proof would
consist of the following parts:
– The statement and proof of theorem 1. Also, the statement and proof of
transitivity of indistinguishability.
– For each of the deﬁned transformations: the proof that the two DGFs they
specify are indistinguishable.
– For each transformation step: the proof that the ﬁrst DGF occurs in the
graph before the transformation, and that the transformation replaces it
with the second DGF.
Only that last part has to be constructed separately for each of the analysed
protocols.
While our analysis procedure starts by translating an protocol speciﬁed in
an applied π-calculus like language into a DGR, the transformations we apply
can change this DGR to a shape that cannot be naturally expressed in a typical
process calculus where parallel composition and replication deﬁne the structure
of parallelly executing threads. For example, in Fig. 4, the node 704 represents a
computation done with values deﬁned in diﬀerent threads and thus can be placed
naturally neither to threads “A” nor to threads “B”. If one argues that presenting
the intermediate protocols as processes in some process calculus makes them
more comprehensible to researchers than the presentation as DGRs, then it will
make sense to search for calculi that could naturally express such computations.
CryptoVerif’s ﬁnd. . . suchthat-construction is an attempt towards that direction,
but it only allows matching, not arbitrary computations, and it is also highly
asymmetric. We desire a calculus that would represent the computation of node
704 in a way that relates it equally with threads “A” and threads “B”. At the
same time, the calculus should still ﬁx the order of execution inside a thread.

A User Interface for a Game-Based Protocol Veriﬁcation Tool
277
References
1. Abadi, M., Corin, R., Fournet, C.: Computational secrecy by typing for the pi
calculus. In: Kobayashi, N. (ed.) APLAS 2006. LNCS, vol. 4279, pp. 253–269.
Springer, Heidelberg (2006)
2. Abadi, M., Fournet, C.: Mobile values, new names, and secure communication. In:
POPL 2001, pp. 104–115 (2001)
3. Abadi, M., J¨urjens, J.: Formal eavesdropping and its computational interpretation.
In: Kobayashi, N., Pierce, B.C. (eds.) TACS 2001. LNCS, vol. 2215, pp. 82–94.
Springer, Heidelberg (2001)
4. Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the computational
soundness of formal encryption). J. Cryptology 15(2), 103–127 (2002)
5. Backes, M., Berg, M., Unruh, D.: A formal language for cryptographic pseu-
docode. In: Cervesato, I., Veith, H., Voronkov, A. (eds.) LPAR 2008. LNCS (LNAI),
vol. 5330, pp. 353–376. Springer, Heidelberg (2008)
6. Backes, M., Laud, P.: Computationally sound secrecy proofs by mechanized ﬂow
analysis. In: ACM CCS 2006, pp. 370–379 (2006)
7. Backes, M., Pﬁtzmann, B., Waidner, M.: A composable cryptographic library with
nested operations. In: ACM CCS 2003, pp. 220–230 (2003)
8. Ballance, R.A., Maccabe, A.B., Ottenstein, K.J.: The program dependence web:
A representation supporting control, data, and demand-driven interpretation of
imperative languages. In: PLDI 1990, pp. 257–271 (1990)
9. Barthe, G., Gr´egoire, B., B´eguelin, S.Z.: Formal certiﬁcation of code-based cryp-
tographic proofs. In: POPL 2009, pp. 90–101 (2009)
10. Bellare, M., Namprempre, C.: Authenticated encryption: Relations among notions
and analysis of the generic composition paradigm. In: Okamoto, T. (ed.) ASI-
ACRYPT 2000. LNCS, vol. 1976, pp. 531–535. Springer, Heidelberg (2000)
11. Bellare, M., Rogaway, P.: The security of triple encryption and a framework for
code-based game-playing proofs. In: Vaudenay, S. (ed.) EUROCRYPT 2006. LNCS,
vol. 4004, pp. 409–426. Springer, Heidelberg (2006)
12. Blanchet, B.: A computationally sound mechanized prover for security protocols.
In: IEEE S&P 2006, pp. 140–154 (2006)
13. Blanchet, B.: A Computationally Sound Mechanized Prover for Security Protocols.
Cryptology ePrint Archive, Report 2005/401 (February 2, 2007)
14. Corin, R., den Hartog, J.: A probabilistic hoare-style logic for game-based crypto-
graphic proofs. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP
2006. LNCS, vol. 4052, pp. 252–263. Springer, Heidelberg (2006)
15. Cortier, V., Kremer, S., K¨usters, R., Warinschi, B.: Computationally sound sym-
bolic secrecy in the presence of hash functions. In: Arun-Kumar, S., Garg, N. (eds.)
FSTTCS 2006. LNCS, vol. 4337, pp. 176–187. Springer, Heidelberg (2006)
16. Cortier, V., Warinschi, B.: Computationally sound, automated proofs for security
protocols. In: Sagiv, M. (ed.) ESOP 2005. LNCS, vol. 3444, pp. 157–171. Springer,
Heidelberg (2005)
17. Datta, A., Derek, A., Mitchell, J.C., Shmatikov, V., Turuani, M.: Probabilistic
polynomial-time semantics for a protocol security logic. In: Caires, L., Italiano,
G.F., Monteiro, L., Palamidessi, C., Yung, M. (eds.) ICALP 2005. LNCS, vol. 3580,
pp. 16–29. Springer, Heidelberg (2005)
18. Datta, A., Derek, A., Mitchell, J.C., Warinschi, B.: Computationally sound com-
positional logic for key exchange protocols. In: CSFW 2006, pp. 321–334 (2006)
19. Ferrante, J., Ottenstein, K.J., Warren, J.D.: The program dependence graph and
its use in optimization. ACM Trans. Program. Lang. Syst. 9(3), 319–349 (1987)

278
P. Laud and I. Tˇsahhirov
20. Fournet, C., Rezk, T.: Cryptographically sound implementations for typed
information-ﬂow security. In: POPL 2008, pp. 323–335 (2008)
21. Fr¨ohlich, M., Werner, M.: Demonstration of the interactive graph-visualization
system vinci. In: Tamassia, R., Tollis, I.G. (eds.) GD 1994. LNCS, vol. 894, pp.
266–269. Springer, Heidelberg (1995)
22. Janvier, R., Lakhnech, Y., Mazar´e, L.: Completing the picture: Soundness of formal
encryption in the presence of active adversaries. In: Sagiv, M. (ed.) ESOP 2005.
LNCS, vol. 3444, pp. 172–185. Springer, Heidelberg (2005)
23. Laud, P.: Semantics and program analysis of computationally secure information
ﬂow. In: Sands, D. (ed.) ESOP 2001. LNCS, vol. 2028, pp. 77–91. Springer, Hei-
delberg (2001)
24. Laud, P.: Handling encryption in an analysis for secure information ﬂow. In:
Degano, P. (ed.) ESOP 2003. LNCS, vol. 2618, pp. 159–173. Springer, Heidelberg
(2003)
25. Laud, P.: Symmetric encryption in automatic analyses for conﬁdentiality against
active adversaries. In: IEEE S&P 2004, pp. 71–85 (2004)
26. Laud, P.: Secrecy types for a simulatable cryptographic library. In: ACM CCS 2005,
pp. 26–35 (2005)
27. Laud, P., Vene, V.: A type system for computationally secure information ﬂow.
In: Li´skiewicz, M., Reischuk, R. (eds.) FCT 2005. LNCS, vol. 3623, pp. 365–377.
Springer, Heidelberg (2005)
28. Lincoln, P., Mitchell, J.C., Mitchell, M., Scedrov, A.: A probabilistic poly-time
framework for protocol analysis. In: ACM CCS 1998, pp. 112–121 (1998)
29. Micciancio, D., Warinschi, B.: Soundness of formal encryption in the presence of
active adversaries. In: Naor, M. (ed.) TCC 2004. LNCS, vol. 2951, pp. 133–151.
Springer, Heidelberg (2004)
30. Nowak, D.: A framework for game-based security proofs. In: Qing, S., Imai, H.,
Wang, G. (eds.) ICICS 2007. LNCS, vol. 4861, pp. 319–333. Springer, Heidelberg
(2007)
31. Pingali, K., Beck, M., Johnson, R., Moudgill, M., Stodghill, P.: Dependence ﬂow
graphs: An algebraic approach to program dependencies. In: POPL 1991, pp. 67–78
(1991)
32. Shoup, V.: Sequences of games: a tool for taming complexity in security proofs.
Cryptology ePrint Archive, Report 2004/332 (2004), http://eprint.iacr.org/
33. Smith, G.: Secure information ﬂow with random assignment and encryption. In:
FMSE 2006, pp. 33–44 (2006)
34. Sprenger, C., Backes, M., Basin, D.A., Pﬁtzmann, B., Waidner, M.: Cryptograph-
ically sound theorem proving. In: CSFW 2006, pp. 153–166 (2006)
35. Sprenger, C., Basin, D.A.: Cryptographically-sound protocol-model abstractions.
In: CSF 2008, pp. 115–129 (2008)
36. Tˇsahhirov, I.: Security Protocols Analysis in the Computational Model — Depen-
dency Flow Graphs-Based Approach. PhD thesis, Tallinn University of Technology
(2008)
37. Tˇsahhirov, I., Laud, P.: Application of dependency graphs to security protocol
analysis. In: Barthe, G., Fournet, C. (eds.) TGC 2007 and FODO 2008. LNCS,
vol. 4912, pp. 294–311. Springer, Heidelberg (2008)
38. uDraw(Graph) graph visualizer (2005),
http://www.informatik.uni-bremen.de/uDrawGraph/en/index.html
39. Volpano, D.M.: Secure introduction of one-way functions. In: CSFW 2000, pp.
246–254 (2000)

Author Index
Alcalde, Baptiste
4
Alp´ızar, Rafael
126
Armando, Alessandro
66
Banerjee, Anindya
1
Brucker, Achim D.
248
Chen, Liqun
201
Cirillo, Andrew
51
Eilers, Florian
36
ElSalamouny, Ehab
21, 141
Hamadou, Sardaouna
141
Heusser, Jonathan
96
Kamil, Allaa
233
Krishnamurthi, Shriram
217
Lafourcade, Pascal
173
Laud, Peeter
263
Lowe, Gavin
81, 233
Malacaria, Pasquale
96
Mauw, Sjouke
4, 186
McCarthy, Jay
217
Merro, Massimo
157
M¨odersheim, Sebastian A.
248
Murray, Toby
81
Nestmann, Uwe
36
Nielsen, Mogens
21
Palamidessi, Catuscia
141
Pang, Jun
186
Ponta, Serena Elisa
66
Riely, James
51
Ryan, Mark
201
Sands, David
111
Sassone, Vladimiro
21, 141
Sibilio, Eleonora
157
Smith, Geoﬀrey
126
Svenningsson, Josef
111
Terrade, Vanessa
173
Tˇsahhirov, Ilja
263
Vigier, Sylvain
173
Zhang, Chenyi
186
Zhang, Ying
186


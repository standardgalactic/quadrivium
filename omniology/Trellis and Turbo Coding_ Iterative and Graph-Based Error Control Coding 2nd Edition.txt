
TRELLIS AND 
TURBO CODING

IEEE Press
445 Hoes Lane
Piscataway, NJ 08854
IEEE Press Editorial Board
Tariq Samad, Editor in Chief
George W. Arnold 
Vladimir Lumelsky 
Linda Shafer
Dmitry Goldgof
Pui-In Mak
Zidong Wang
Ekram Hossain
Jeffrey Nanzer      
MengChu Zhou   
Mary Lanzerotti
Ray Perez
George Zobrist
Kenneth Moore, Director of IEEE Book and Information Services (BIS)
Technical Reviewers
Todd Moon, Utah State University
Claudio Sacchi, Utah State University

TRELLIS AND 
TURBO CODING
Iterative and Graph-Based Error Control Coding
Second Edition
CHRISTIAN B. SCHLEGEL
LANCE C. PÉREZ
IEEE Press

Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. All rights reserved.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to
the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at
http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
efforts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and specifically disclaim any implied
warranties of merchantability or fitness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside the
United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in
print may not be available in electronic format. For information about Wiley products, visit our
web site at www.wiley.com.
Library of Congress Cataloging-in-Publication is available.
ISBN 978-1-118-083161
Printed in the United States of America.
10  9  8  7  6  5  4  3  2  1

Contents
1
Introduction
1
1.1
Modern Digital Communications . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
The Rise of Digital Communications . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Communication Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4
Error Control Coding
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.5
Bandwidth, Power, and Complexity
. . . . . . . . . . . . . . . . . . . . . .
12
1.6
A Brief History–The Drive Towards Capacity . . . . . . . . . . . . . . . . .
20
2
Communications Basics
27
2.1
The Probabilistic Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2
Vector Communication Channels . . . . . . . . . . . . . . . . . . . . . . . .
29
2.3
Optimum Receivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.4
Matched Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.5
Message Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.6
The Complex Equivalent Baseband Model . . . . . . . . . . . . . . . . . . .
39
2.7
Spectral Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.8
Advanced Modulation Methods . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.8.1
OFDM
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.8.2
Multiple Antenna Channels (MIMO Channels) . . . . . . . . . . . .
48
2.9
A Communications System Case Study . . . . . . . . . . . . . . . . . . . . .
53
2.10 Appendix 2.A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3
Trellis-Coded Modulation
67
3.1
An Introductory Example . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2
Construction of Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.3
Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.4
Lattice Formulation of Trellis Codes . . . . . . . . . . . . . . . . . . . . . .
86
3.5
Rotational Invariance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.6
V.fast
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
v

iv
CONTENTS
3.7
The IEEE 802.3an Standard . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.8
Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4
Trellis Representations
111
4.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.2
The Parity-Check Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.3
Parity-Check Trellis Representations . . . . . . . . . . . . . . . . . . . . . . 113
4.4
Convolutional Codes and Their Trellis . . . . . . . . . . . . . . . . . . . . . 115
4.5
Minimal Trellises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.6
Minimum-Span Generator Matrices . . . . . . . . . . . . . . . . . . . . . . . 124
4.7
Systematic Construction of the PC-Trellis . . . . . . . . . . . . . . . . . . . 127
4.8
Tail-Biting Trellises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.9
The Minimal Trellis of Convolutional Codes . . . . . . . . . . . . . . . . . . 133
4.10 Fundamental Theorems from Basic Algebra . . . . . . . . . . . . . . . . . . 139
4.11 Systematic Encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
4.12 Maximum Free-Distance Convolutional Codes . . . . . . . . . . . . . . . . . 151
4.13 The Squaring Construction and the Trellis of Lattices
. . . . . . . . . . . . 154
4.14 The Construction of Reed–Muller Codes . . . . . . . . . . . . . . . . . . . . 161
4.15 A Decoding Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
4.16 Polar Codes and Their Relationship to RM Codes
. . . . . . . . . . . . . . 166
Appendix 4.A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5
Trellis and Tree Decoding
179
5.1
Background and Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.2
Tree Decoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.3
The Stack Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
5.4
The Fano Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
5.5
The M-Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
5.6
Maximum Likelihood Decoding . . . . . . . . . . . . . . . . . . . . . . . . . 197
5.7
A Posteriori Probability Symbol Decoding . . . . . . . . . . . . . . . . . . . 200
5.8
Log-APP and Approximations
. . . . . . . . . . . . . . . . . . . . . . . . . 207
5.9
Error Analysis and Distance Spectrum . . . . . . . . . . . . . . . . . . . . . 211
5.10 Random Coding Analysis of Optimal Decoding . . . . . . . . . . . . . . . . 222
5.11 Random Coding Analysis of Sequential Decoding . . . . . . . . . . . . . . . 232
5.12 Some Final Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6
Low-Density Parity-Check Codes
249
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
6.2
LDPC Codes and Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
6.3
LDPC Decoding via Message Passing . . . . . . . . . . . . . . . . . . . . . . 255
vi

CONTENTS
v
6.4
Analysis Techniques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
6.4.1
(Error) Probability Evolution for Binary Erasure Channels
. . . . . 259
6.4.2
Error Mechanism of LDPCs on BECs
. . . . . . . . . . . . . . . . . 265
6.4.3
Binary Symmetric Channels and the Gallager Algorithms . . . . . . 266
6.4.4
The AWGN Channel . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
6.5
Code Families and Construction
. . . . . . . . . . . . . . . . . . . . . . . . 281
6.5.1
Constructions with Permutation Matrices . . . . . . . . . . . . . . . 281
6.5.2
Cycle Reduction Design . . . . . . . . . . . . . . . . . . . . . . . . . 286
6.5.3
RS-based Construction . . . . . . . . . . . . . . . . . . . . . . . . . . 287
6.5.4
Repeat-Accumulate Codes . . . . . . . . . . . . . . . . . . . . . . . . 289
6.6
Encoding of LDPC Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
6.6.1
Triangular LDPC Codes . . . . . . . . . . . . . . . . . . . . . . . . . 292
6.6.2
Specialized LDPC Codes
. . . . . . . . . . . . . . . . . . . . . . . . 295
6.6.3
Approximate Triangularization . . . . . . . . . . . . . . . . . . . . . 296
Appendix 6.A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
7
Error Floors
319
7.1
The Error Floor Problem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
7.2
Dynamics of the Absorption Sets . . . . . . . . . . . . . . . . . . . . . . . . 323
7.3
Code Design for Low Error Floors
. . . . . . . . . . . . . . . . . . . . . . . 331
7.4
Impact of the Decoding Algorithm . . . . . . . . . . . . . . . . . . . . . . . 335
7.5
Importance Sampling (IS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
7.6
Computing Error Rates via Importance Sampling . . . . . . . . . . . . . . . 340
8
Turbo Coding: Basic Principles
351
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
8.2
Parallel Concatenated Convolutional Codes . . . . . . . . . . . . . . . . . . 353
8.3
Distance Spectrum Analysis of Turbo Codes . . . . . . . . . . . . . . . . . . 356
8.4
The Free Distance of a Turbo Code . . . . . . . . . . . . . . . . . . . . . . . 358
8.5
Weight Enumerator Analysis of Turbo Codes
. . . . . . . . . . . . . . . . . 364
8.6
Iterative Decoding of Turbo Codes . . . . . . . . . . . . . . . . . . . . . . . 371
8.7
EXIT Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
8.8
Serial Concatenation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
8.9
Cascaded Convolutional Codes
. . . . . . . . . . . . . . . . . . . . . . . . . 383
8.10 Weight Enumerator Analysis of SCCCs
. . . . . . . . . . . . . . . . . . . . 385
8.11 Iterative Decoding and Performance of SCCCs
. . . . . . . . . . . . . . . . 394
8.12 EXIT Analysis of Serially Concatenated Codes . . . . . . . . . . . . . . . . 397
8.13 Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
8.14 Turbo-Trellis-Coded Modulation
. . . . . . . . . . . . . . . . . . . . . . . . 402
8.15 Serial Concatenation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
vii

vi
CONTENTS
8.16 EXIT Analysis of Serial TTCM . . . . . . . . . . . . . . . . . . . . . . . . . 408
8.17 Diﬀerential-Coded Modulation
. . . . . . . . . . . . . . . . . . . . . . . . . 409
8.18 Concatenated Space–Time Coding
. . . . . . . . . . . . . . . . . . . . . . . 414
8.19 Bit-Interleaved Coded and Generalized Modulation . . . . . . . . . . . . . . 418
9
Turbo Coding: Applications
431
9.1
Interleavers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
9.2
Turbo Codes in Telecommunication Standards
. . . . . . . . . . . . . . . . 439
9.2.1
The Space Data System Standard
. . . . . . . . . . . . . . . . . . . 439
9.2.2
3G Wireless Standards . . . . . . . . . . . . . . . . . . . . . . . . . . 440
9.2.3
Digital Video Broadcast Standards . . . . . . . . . . . . . . . . . . . 443
9.3
Product Codes and Block Turbo Decoding . . . . . . . . . . . . . . . . . . . 446
9.4
Approximate APP Decoding
. . . . . . . . . . . . . . . . . . . . . . . . . . 448
9.5
Product Codes with High-Order Modulations . . . . . . . . . . . . . . . . . 451
9.6
The IEEE 802.16 Standard
. . . . . . . . . . . . . . . . . . . . . . . . . . . 453
9.7
Decoding of Polar Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
9.8
Polar Code Performance and Outlook
. . . . . . . . . . . . . . . . . . . . . 458
10 Convolutional LDPC Codes and Spatial Coupling
465
10.1 Capacity: The Ultimate Limit . . . . . . . . . . . . . . . . . . . . . . . . . . 465
10.2 Low-Density Parity-Check Convolutional Codes . . . . . . . . . . . . . . . . 467
10.2.1 New LDPC Codes from Old . . . . . . . . . . . . . . . . . . . . . . . 467
10.2.2 Decoding Convolutional LDPC Codes . . . . . . . . . . . . . . . . . 472
10.3 Spatial Coupling: A General View
. . . . . . . . . . . . . . . . . . . . . . . 474
10.4 Spatial Coupling: Convergence Analysis . . . . . . . . . . . . . . . . . . . . 482
10.4.1 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
10.4.2 Lyapunov Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
viii

Chapter 1
Introduction
1.1
Modern Digital Communications
With the advent of high-speed logic circuits and very large scale integration (VLSI), data
processing and storage equipment has inexorably moved towards employing digital tech-
niques. In digital systems, data is encoded into strings of zeros and ones, corresponding
to the on and oﬀstates of semiconductor switches. This has brought about fundamental
changes in how information is processed. While real-world data is primarily in “analog
form” of one type or another, the revolution in digital processing means that this analog
information needs to be encoded into a digital representation, e.g., into a string of ones and
zeros. The conversion from analog to digital and back are processes which have become
ubiquitous. Examples are the digital encoding of speech, picture, and video encoding and
rendering, as well as the large variety of capturing and representing data encountered in
our modern internet-based lifestyles.
The migration from analog communications of the ﬁrst half of the 20-th century to the
now ubiquitous digital forms of communications were enabled primarily by the fast-paced
advances in high-density device integration. This has been the engine behind much of the
technological progress over the last half century, initiated by the creation of the ﬁrst inte-
grated circuit (IC) by Kilby at Texas Instruments in 1958. Following Moore’s informal law,
device sizes, primarily CMOS (Complementary Metal-Oxide Semiconductors), shrink by a
factor two every two years, and computational power doubles accordingly. An impression
for this exponential growth in computing capability can be gained from Figure 1.1, which
shows the number of transistors integrated in a single circuit and the minimum device size
for progressive fabrication processes – known as implementation nodes.
While straightforward miniaturization of the CMOS devices is becoming increasingly
more diﬃcult, transistor designers have been very creative in modifying the designs to
stay on the Moore trajectory. As of 2015 we now see the introduction of 3-dimensional
1
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

2
CHAPTER 1. INTRODUCTION
transistor structures such as thin FETs, double-gated FETs, and tunnel FETs, and it is
expected that carbon nanotube devices may continue miniaturization well into the sub-10
nm range. In any case, the future for highly complex computational devices is bright.
CPU Transistor Counts 1970-2020
Process Size
Date of Introduction
1971
Moore’s paper
1965
1980
1990
2000
2008
2012
2020
2016
4,000,000,000
2,000,000,000
1,000,000,000
10,000,000,000
100,000,000
10,000,000
1,000,000
100,000
10,000
10 nm
100 nm
1 micon
2,300
Curve shows "Moore’s Law":
transistor count doubling 
every two years
4004
8008
8080
8088
286
386
486
Pentium
K5
K6
PII
PIII
K7
K6III
P4
Barton
Atom
K8
Itanium 2
Itanium 2 with 9MB cache
Core 2 Quad
Core 2 Duo
Cell
K10
G80
POWER6
Dual-Core Itanium 2
Quad-Core Itanium Tukwila
GT200
RV770
Haswell
Sandy Bridge
Core i7 g80X
Photolythographic
miniaturization
will have run its
course
Tens of billions
of transistors
350 nm
500 nm
250 nm
180 nm
65 nm
45 nm
22 nm
14 nm
7 nm
5 nm
32 nm
10 nm
90 nm
virus
red blood 
cell
Vdd
A
A
B
B
 CMOS - NAND Gate
Figure 1.1: Moore’s law is driving progress in electronic devices. Top left: A basic CMOS
switching structure. Bottom left: Moore observed his “doubling law” in 1965 and predicted
that it would continue “at least another 10 years.”
One such computational challenge is data communications: in particular data integrity,
as discussed in this book. The migration from analog to digital information processing
has opened the door for many sophisticated algorithmic methods. Digital information is

1.1. MODERN DIGITAL COMMUNICATIONS
3
treated diﬀerently in communications than analog information. Signal estimation becomes
signal detection; that is, a communications receiver need not look for an analog signal and
make a “best” estimate, it only needs to make a decision between a ﬁnite number of
discrete signals, say a one or a zero in the most basic case.
Digital signals are more
reliable in a noisy communications environment; they can usually be detected perfectly,
as long as the noise levels are below a certain threshold. This allows us to restore digital
data, and, through error correcting techniques, correct errors made during transmission.
Digital data can easily be encoded in such a way as to introduce dependency among a
large number of symbols, thus enabling a receiver to make a more accurate detection of
the symbols. This is the essence of error control coding.
Finally, there are also strong theoretical reasons behind the migration to digital pro-
cessing. Nyquist’s sampling theorem, discussed in Section 1.3, tells us that, fundamentally,
it is suﬃcient to know an analog signal at a number of discrete points in time. This opens
the door for the discrete time treatment of signals. Then, Shannon’s fundamental chan-
nel coding theorem states that the values of these discrete time samples themselves, can
contain only a ﬁnite amount of information. Therefore, only a ﬁnite amount of discrete
levels are required to capture the full information content of a signal.
The digitization of data is convenient for a number of other reasons too. The design
of signal processing algorithms for digital data is much easier than designing analog signal
processing algorithms, albeit not typically less complex. However, the abundance of such
digital algorithms, including the error control and correction techniques discussed in this
book, combined with their ease of implementation in very large scale integrated (VLSI)
circuits has led to the plethora of successful applications of error control coding we see in
practice today.
Error control coding was ﬁrst applied in deep-space communications where we are
confronted with low-power communications channels with virtually unlimited bandwidth.
On these data links, convolutional codes (Chapter 4) are used with sequential and Viterbi
decoding (Chapter 5), and the future will see the application of turbo coding. The next
successful application of error control coding was to storage devices, most notably the
compact disk player, which employs powerful Reed-Solomon codes [21] to handle the raw
error probability from the optical readout device which is too large for high-ﬁdelity sound
reproduction without error correction. Another hurdle taken was the successful application
of error control to bandwidth-limited telephone channels, where trellis-coded modulation
(Chapter 3) was used to produce impressive improvements and push transmission rates
towards the theoretical limit of the channel. Nowadays, coding is routinely applied to
satellite communications [41, 49], teletext broadcasting, computer storage devices, logic
circuits, semiconductor memory systems, magnetic recording systems, audio-video, and
WiFi systems. Modern mobile communications systems like the pan-European TDMA
digital telephony standard GSM [35], IS 95 [47], CDMA2000, IMT2000, and the new 4-th
generation LTE and LTE-A standards [63, 64] all use error control coding.

4
CHAPTER 1. INTRODUCTION
1.2
The Rise of Digital Communications
Modern digital communication theory started in 1928 with Nyquist’s seminal work on
telegraph transmission theory [36].
The message from Nyquist’s theory is that ﬁnite
bandwidth implies discrete time. That is, a signal whose bandwidth is limited can always
be represented by sample values taken at discrete time intervals. The sampling theorem of
this theory then asserts that the band-limited signal can always be reconstructed exactly
from these discrete-time samples.1 Only these discrete samples need to be processed by a
receiver since they contain all the necessary information of the entire waveform.
The second pillar to establish the supremacy of digital information processing came
precisely from Shannon’s 1948 theory. Shannon’s theory essentially establishes that the
discrete-time samples which are used to represent a bandlimited signal, could be ade-
quately described by a ﬁnite number of amplitude samples, the number of which depended
on the level of the channel noise. These two theories combined state that a ﬁnite num-
ber of levels taken at discrete time intervals are completely suﬃcient to characterize any
bandlimited signal in the presence of noise, that is, in any communication system.
With these results, technology has moved towards a complete digitization of commu-
nications systems, with error control coding being the key to realize the suﬃciency of
discrete amplitude levels. We will study Shannon’s theorem in more detail in Section 1.5.
1.3
Communication Systems
Figure 1.2 shows the basic conﬁguration of a point-to-point digital communications link.
The data to be transmitted over this link can either come from some analog source, in
which case it must ﬁrst be converted into digital format (digitized), or it can be a digital
information source. If this data is a speech signal, for example, the digitizer is a speech
codec [22]. Usually the digital data is source encoded to remove unnecessary redundancy
from the data, i.e., the source data is compressed [14]. Source encoding has the eﬀect
that the digital data which enters the encoder has statistics which resemble that of a
random symbol source with maximum entropy, i.e., all the diﬀerent digital symbols occur
with equal likelihood, and are statistically independent. The channel encoder operates
on this compressed data and introduces controlled redundancy for transmission over the
channel. The modulator converts the discrete channel symbols into waveforms which are
transmitted through the waveform channel. The demodulator reconverts the waveforms
1Since it is not shown elsewhere in this book, we present Nyquist’s sampling theorem here. It is given
by the following exact series expansion of the function s(t) which is bandlimited to [−1/2T, 1/2T]:
s(t) =
∞
i=−∞
s(iT) sinc
π
T (t −iT) ;
sinc(x) = sin(x)
x
.

1.3. COMMUNICATION SYSTEMS
5
back into a discrete sequence of received symbols, and the decoder reproduces an estimate
of the compressed input data sequence, which is subsequently reconverted into the original
signal or data sequence.
Encoder/Modulator
Waveform
Channel
Synchronization
ARQ
Error
Detection
FEC Block
Source
Compression
Digitizer
Analog Information
Digital Data 
D/A
Conversion
Source De-
compression
Analog Information
Digital Sink
Demodulator/Decoder
Figure 1.2: System diagram of a complete point-to-point communication system for digital
data. The forward error control (FEC) block is the topic of this book.
An important ancillary function at the receiver is the synchronization process. We
usually need to acquire carrier frequency and phase synchronization, as well as symbol
timing synchronization in order for the receiver to be able to operate. Synchronization is
not a topic of this book, and we will assume in most of our discussion that synchronization
has been established (with the exception of phase synchronization in the case of rotation-
ally invariant codes in Chapters 4 and 8). References [32] and [33] treat synchronization
issues in detail. Since synchronization is a relatively slow estimation process, and data
detection is a fast process, we usually have those two operations separated in real receiver
implementations as indicated in Figure 1.2. However, we would like to note at this point
that novel, iterative receiver designs are increasingly integrating these auxillary functions,
for an example of phase synchronization see Chapter 8.

6
CHAPTER 1. INTRODUCTION
Another important feature in some communication systems is automatic repeat request
(ARQ). In systems with ARQ the receiver also performs error detection, and, through
a return channel, requests retransmission of erroneous data blocks, or data blocks which
cannot be reconstructed with suﬃcient conﬁdence [25]. ARQ can usually improve the data
transmission quality substantially, but the return channel needed for ARQ is not always
available, or may be impractical. For a deep-space probe on its way to the outer rim of
our solar system, ARQ is infeasible since the return path takes too long (several hours!).
For speech-encoded signals ARQ is usually impossible for an analogous reason, since only
a maximum speech delay is of about 200 ms is acceptable. In broadcast systems, ARQ is
ruled out for obvious reasons. Error control coding without ARQ is termed forward error
correction or control (FEC). FEC is more diﬃcult to perform than simple error detection
and ARQ, but dispenses with the return channel. Oftentimes, FEC and ARQ are both
integrated into hybrid error control systems [25, 24] for data communications.
This book deals primarily with FEC—the dashed block in Figure 1.2. The reason why
we can do this relatively easily is due to diﬀerent functionalities of the various blocks just
discussed, and the fact that they operate largely autonomously from each other. Each of
them represents a separate entity with its own optimization strategy, and data is simply
passed between the diﬀerent blocks, sometimes with lithe extra mutual interaction. A no-
table point in Figure 1.2 is that the encoder/modulator and the demodulator/decoder are
combined operations. This is done to reﬂect the fact that error protection and modulation–
in the sense of choosing the discrete signal points that represent the digital data–is a pro-
cess best addressed jointly. This view of joint encoding/modulation was ﬁrst proposed by
Wozencraft and Kennedy [61] and Massey [30] and then realized with stunning results by
Ungerb¨ock [50, 51, 52] in the methods of trellis-coded modulation of the 1980’s.
Since we will assume the encoder input data to be a sequence of independent, identically
and uniformly distributed symbols (courtesy of the source compression), the single most
important parameter to optimize for the FEC block is arguably the bit and/or symbol
error rate, and we will adopt this as our criterion for the quality of an FEC system.
Note that this is not necessarily the most meaningful measure in all cases. Consider, for
example, pulse-code-modulated (PCM) speech, where an error in the most signiﬁcant bit
is signiﬁcantly more detrimental than an error in the least signiﬁcant bit. Researchers
have looked at schemes with unequal error protection for such applications (e.g., [18]).
However, such methods usually are a variation of the basic theme of obtaining a minimum
error rate. Occasionally we may have need for the frame-, or block error rate (FER),
which describes the probability that an entire block of data is incorrectly received. Most
communications protocols operate on the basis of frame errors, and frame error control
is exercised at the upper layers of a communications protocol. While of course tightly
connected, the frame-, and bit error rates do sometimes follow diﬀerent tendencies. This
will become important when we discuss error ﬂoors for very-low error rate applications,
such as those using cable modem or optical ﬁber communications.

1.4. ERROR CONTROL CODING
7
1.4
Error Control Coding
The modern approach to error control in digital communications started with the ground-
breaking work of Shannon [45], Hamming [19], and Golay [16]. While Shannon presented
a theory which explained the fundamental limits on the eﬃciency of communications
systems, Hamming and Golay were the ﬁrst to develop practical error control schemes.
A new paradigm was born, one in which errors are not synonymous with data which is
irretrievably lost; but by clever design, errors could be corrected, or avoided altogether.
This new thinking was revolutionary. Even though Shannon’s theory promised that large
improvements in the performance of communication systems could be achieved, practical
improvements had to be excavated by laborious work over half a century of intensive
research.
One reason for this lies in a fundamental shortcoming of Shannon’s theory.
While it clearly states theoretical limits on communication eﬃciency, its methodology
provides no insight on how to actually achieve these limits, since it is based on sophisticated
averaging arguments which eliminate all detailed system structure. Coding theory, on the
other hand, evolved from Hamming and Golay’s work into a ﬂourishing branch of applied
mathematics [27].
Let us see where it all started. The most famous formula from Shannon’s work is
arguably the channel capacity of an ideal band-limited Gaussian channel,2 which is given
by
C = W log2(1 + S/N) [bits/second].
(1.1)
In this formula, C is the channel capacity, that is, the maximum number of bits which
can be transmitted through this channel per unit time (second), W is the bandwidth of
the channel, and S/N is the signal-to-noise power ratio at the receiver. Shannon’s main
theorem, which accompanies (1.1), asserts that error probabilities as small as desired
can be achieved as long as the transmission rate R through the channel (in bits/second)
is smaller than the channel capacity C. This can be achieved by using an appropriate
encoding and decoding operation. However, Shannon’s theory is silent about the structure
of these encoders and decoders.
This new view was in marked contrast to early practices, which embodied the opinion
that in order to reduce error probabilities, the signal energy had to be increased, i.e., the
S/N had to be improved. Figure 1.3 shows the error performance of QPSK, a popular
modulation method for satellite channels (see Chapter 2) which allows data transmission
of rates up to 2 bits/symbol. The bit error probability (BER) of QPSK is shown as a
function of the signal-to-noise ratio S/N per dimension normalized per bit (see Section 1.3),
henceforth called SNR. It is evident that an increased SNR provides a gradual decrease in
error probability. This contrasts markedly with Shannon’s theory which promises zero(!)
error probability at a spectral eﬃciency of 2 bits/s/Hz, which is the maximum that QPSK
2The exact deﬁnitions of these basic communications concepts are given in Chapter 2.

8
CHAPTER 1. INTRODUCTION
can achieve, as long as SNR > 1.5 (1.76 dB), shattering conventional wisdom. The limit
on SNR is calculated using (1.1)–see Section 1.5.
S
h
a
n
n
on
 Ca
pacity Limit
0
1
2
3
4
5
6
7
8
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
Zero error range
QPSK
TCM
16 states
TCM
64 states
TCM
256 states
TCM
215
states
TCM
216 states
TTCM
Figure 1.3: Bit error probability of quadrature phase-shift keying (QPSK) and selected
8-PSK trellis-coded modulation (TCM), trellis-turbo-coded (TTCM), and block-turbo-
coded (BTC) systems as a function of the normalized signal-to-noise ratio.
Also shown in Figure 1.3 is the performance of several trellis-coded modulation (TCM)
and trellis-turbo-coded (TTCM) schemes using 8-ary phase-shift keying (8-PSK) (Chapter
4), and the improvement made possible by coding becomes evident. The diﬀerence in SNR
for an objective target bit error rate between a coded system and an uncoded system is
termed the coding gain. Note that the coding schemes shown in Figure 1.3 achieves these
gains without requiring more bandwidth than the uncoded QPSK system.
As we will discuss in Chapter 4, a trellis code is generated by a circuit with a ﬁnite
number of internal states. The number of these states is a direct measure of its decoding
complexity if maximum-likelihood decoding is used. Note that the two very large codes are
not maximum-likelihood decoded, they are sequentially decoded [55]. The turbo-trellis-
coded modulation (TTCM) system is based on turbo coding principles (Chapter 9) using
two small concatenated trellis codes. Coding then helps to realize the promise of Shannon’s
theory which states that for a desired error rate of Pb = 10−6 we can gain almost 9 dB
in expended signal energy over QPSK. This gain can be achieved by converting required

1.4. ERROR CONTROL CODING
9
signal power into decoder complexity, as is done by the TCM and TTCM coding methods.
Incidentally, 1948, the year Shannon published his work, is also the birth year3 of the
transistor, arguably the 20-th century’s most fundamental invention, one which allowed
the construction of very powerful, very small computing devices.
Only this made the
conversion from signal energy requirements to (system) complexity possible, giving coding
and information theory a platform for practical realizations [21].
Figure 1.4 shows an early feasibility experiment, comparing the performance of a 16-
state 8-PSK TCM code used in an experimental implementation of a single channel per
carrier (SCPC) modem operating at 64 kbits/second [49] against QPSK and the theoretical
performance established via simulations (Figure 1.3). This illustrates the viability of trellis
coding for satellite channels. Interestingly, the 8-PSK TCM modem performance comes
much closer to the theoretical performance than the original QPSK modem, achieving a
practical coding gain of 5 dB. This is due to an eﬀect where system inaccuracies, acting
similar to noise, are handled better by a coded system.
2
3
4
5
6
7
8
9
10
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
QPSK Performance
Intelsat
QPSK
Modem
IBM/DVFLR
Intelsat
Experiment
TCM
16 states
ideal channel
Figure 1.4: Measured bit error probability of (QPSK) and a 16-state 8-PSK (TCM) modem
over a 64 kbit/s satellite channel [49]. The discrepancy between the theoretical curves and
the implemented BER results are due to non-ideal behaviors of other components in the
signaling chain, collectively known as implementation loss.
3Transistor action was ﬁrst observed on December 15, 1947, but the news of the invention was not made
public until June 30, 1948.

10
CHAPTER 1. INTRODUCTION
Figure 1.5 shows the performance of selected rate R = 1/2 bits/symbol (binary) con-
volutional and turbo codes codes on an additive white Gaussian noise channel (see also
[24, 20, 37, 38, 39]). Contrary to TCM, these codes do not preserve bandwidth and the
gains in power eﬃciency in Figure 1.5 are partly obtained by a power bandwidth trade-oﬀ,
i.e., the rate 1/2 convolutional codes require twice as much bandwidth as uncoded trans-
mission. This bandwidth expansion may not be an issue in deep-space communications
or the application of error control to spread spectrum systems [53, 54]. As a consequence,
for the same complexity, a higher coding gain is achieved than with TCM. Note that the
convolutional codes reach a point of diminishing returns.
0
1
2
3
4
5
6
7
8
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
BPSK Performance
CC; 4 states
CC; 16 states
CC; 64 states
CC; 214
states
CC; 240 states
Turbo
Code
Figure 1.5: Bit error probability of selected rate R = 1/2 convolutional and turbo codes as
a function of the normalized signal-to-noise ratio. The large-state space code is decoded
sequentially, while the performance of all the other convolutional codes is for maximum-
likelihood decoding. Simulation results are taken from [20], [38], and [4]. The Shannon
limit is at Eb/N0=0 dB, and that for BPSK is at Eb/N0 = 0.19 dB.
For very low target error probabilities, tandem coding methods, called concatenated
coding, have become very popular [24, 10, 23, 4]. In classic concatenation, as illustrated in
Figure 1.6, the FEC codec is broken up into an inner and an outer code. The inner code
is most often a trellis code which performs the channel error control, and the outer code
is typically a high-rate Reed-Solomon (RS) block code. Its function it is to clean up the
residual output error of the inner code. This combination has proven very powerful, since
the error mechanism of the inner code is well matched to the error correcting capabilities

1.4. ERROR CONTROL CODING
11
of the outer system. Via this tandem construction, very low error rates are achievable.
Inner
Encoder
Inner
Decoder
Outer
Decoder
Outer
Encoder
side information
Typically:
Viterbi Decoder
Typically:
Trellis Code
Typically:
Reed-Solomon
Code
Typically:
Algebraic
Decoder
Waveform
Channel
Figure 1.6: Classic concatenated FEC coding using inner and outer codecs.
With the discovery of turbo codes [4] new concatenated structures have appeared,
shown in Figure 1.7. Serial concatenation is very similar to classic concatenation; however,
the interleaver Π and deinterleaver Π−1 are fundamental structures, discussed later in
Chapters 6 and 8. Furthermore, the outer code in these new structures is usually a very
weak error control code such as a simple parity check code. Its function is not to clean
up residual errors from the inner decoder, but, through interaction with the inner codes
to form a strong error control system, a.k.a. a turbo code.
The original turbo codes, however, used a parallel arrangement of two codes, known
as parallel concatenation, where both encoders have identical roles. Both arrangements,
parallel and serial concatenation, are decoded with the same iterative decoding procedure
which alternately invokes soft-output decoders for the two component codes. The structure
and workings of these decoders is explored in detail in Chapters 5, 6, 8, and 9.
The ﬁeld of error control and error correction coding naturally breaks into two dis-
ciplines, named somewhat inappropriately block coding and trellis coding. While block
coding, which traditionally was approached as applied mathematics, has produced the
bulk of publications in error control coding, trellis and turbo coding is favored in most
practical applications. One reason for this is the ease with which soft-decision decoding
can be implemented for trellis and turbo codes. Soft-decision is the operation whereby
the demodulator does not make hard ﬁnal decisions on the transmitted symbols or bits.
Rather, it passes the received signal values directly on to the decoder which derives prob-
abilistic information from those signals, which are then used to generate a ﬁnal estimate
of the decoded bits. There are no “errors” to be corrected; the decoder operates on re-
liability information obtained by comparing the received signals with the possible set of
transmitted signals to arrive at a decision for an entire codeword. This soft-decision pro-
cessing yields a 2 dB advantage on additive white Gaussian noise (AWGN) channels. In
many applications the trellis decoders act as “S/N transformers” (e.g., [17]), improving
the channel behavior as in concatenated coding. Ironically, many block codes can be de-

12
CHAPTER 1. INTRODUCTION
Inner
Encoder
Outer
Encoder
Soft-Output
Component Decoder
Soft-Output
Component Decoder
Soft-Output
Component Decoder
Soft-Output
Component Decoder
Typically:
Trellis Code 
Typically:
Weak Code
Waveform
Channel
Inner
Decoder
Outer
Decoder
First
Encoder
Second
Encoder
Typically:
Trellis Codes 
Waveform
Channel
First
Decoder
Second
Decoder
MUX
Parallel Concatenation:
Serial Concatenation:
Π
Π
Π
Π
Π−1
Π−1
Figure 1.7: Modern concatenated FEC coding using two component codes.
coded very successfully using decoding methods developed for trellis codes (Chapter 4),
smearing the boundaries between these two branches of error control coding.
1.5
Bandwidth, Power, and Complexity
Nyquist showed in 1928 [36] that a channel of bandwidth W (in Hz) is capable of supporting
PAM signals at a rate of 2W samples/second without causing intersymbol interference.
In other words, using Nyquist’s method of interpolating band-limited functions, there are
2W independent signal dimensions per second4. If two carriers (sin(2πfc) and cos(2πfc))
are used in quadrature, as in double side-band suppressed carrier amplitude modulation
(DSB-SC), we have W pairs of dimensions (or complex dimensions) per second. This is
the ubiquitous QAM format popular in digital radio systems (Chapter 2).
The parameter which characterizes how eﬃciently a system uses its allotted bandwidth
is the bandwidth eﬃciency η, deﬁned as
η =
Transmission Rate
Channel Bandwidth W [bits/s/Hz].
(1.2)
4A more general analysis using more sophisticated interpolation functions and a more general deﬁnition
of bandwidth reveals that there are approximately 2.4W independent signal dimensions per second [62].

1.5. BANDWIDTH, POWER, AND COMPLEXITY
13
Using Shannon’s capacity formula (1.1) and dividing by W we obtain the maximum band-
width eﬃciency for an additive white Gaussian noise channel, the Shannon limit, as
ηmax = log2

1 + S
N

[bits/s/Hz].
(1.3)
In order to calculate η, we must suitably deﬁne the channel bandwidth W. This is
obvious for some signaling schemes, like Nyquist signaling, which have a rather sharply
deﬁned bandwidth (see Chapter 2), but becomes more arbitrary for modulation schemes
with inﬁnite spectral occupancy. One commonly used deﬁnition is the 99% bandwidth
deﬁnition, i.e., W is deﬁned such that 99% of the transmitted signal power falls within the
band of width W. This 99% bandwidth corresponds to an out-of-band power of -20 dB.
The average signal power S can be expressed as
S = kEb
T
= REb,
(1.4)
where Eb is the energy per bit, k is the number of bits transmitted per symbol, and T is
the duration of a symbol. The parameter R = k/T is the transmission rate of the system
in bits/s. Rewriting the signal-to-noise power ratio S/N, where N = WN0, i.e., the total
noise power equals the one-sided noise power spectral density (N0) multiplied by the width
of the transmission band, we obtain the Shannon limit in terms of the bit energy and noise
power spectral density, given by
ηmax = log2

1 + REb
WN0

.
(1.5)
Since R/W = ηmax is the limiting spectral eﬃciency, we obtain a bound from (1.5) on the
minimum bit energy required for reliable transmission at a given spectral eﬃciency:
Eb
N0
≥2ηmax −1
ηmax
,
(1.6)
also called the Shannon bound.
If spectral eﬃciency is not at a premium, and a large amount of bandwidth is available
for transmission, we may choose to use bandwidth rather than power to increase the
channel capacity (1.1). In the limit as the signal is allowed to occupy an inﬁnite amount
of bandwidth, i.e., ηmax →0, we obtain
Eb
N0
≥
lim
ηmax→0
2ηmax −1
ηmax
= ln(2),
(1.7)
the absolute minimum bit energy to noise power spectral density required for reliable
transmission. This minimum Eb/N0 = ln(2) = −1.59 dB.

14
CHAPTER 1. INTRODUCTION
We can cheat on the Shannon limit by allowing a certain number of errors in the
following way: Assume that the original information source of rate R is being compressed
into a rate R < R. According to source coding theory, this introduces a distorting in
the sense that original information can no longer be reconstructed perfectly [14]. If the
source is binary, this compression results in a non-zero reconstruction bit error rate and
satisﬁes R = R(1 −h(BER)), where h(p) = −p log(p) −(1 −p) log(1 −p) is the binary
entropy function. As long as R < C, the channel coding system will add no extra errors,
and the only errors are due to the lossy compression. The source encoder has a rate of
1/(1 −h(BER)), and consequently the average power is
S =
REb
1 −h(BER)
(1.8)
since less energy is used to transport a bit. The actual rate over the channel is R, from
which we obtain a modiﬁed Shannon bound for non-zero bit error rates, given by
Eb
N0
≥2(1−h(BER)ηmax −1
ηmax
(1 −h(BER)).
(1.9)
This is the bound plotted in Figure 1.3 for a spectral eﬃciency of ηmax = 2 bits/s/Hz.
The implicit dependence of our formulas on the somewhat arbitrary deﬁnition of the
bandwidth W is not completely satisfactory, and we prefer to normalize these formulas per
signal dimension. Let Rd be the rate in bits/dimension, then the capacity of an additive
white Gaussian noise channel per dimension is the maximum rate/dimension at which
reliable transmission is possible. It is given by [62]
Cd = 1
2 log2

1 + 2RdEb
N0

[bits/dimension],
(1.10)
or as
Cc = log2

1 + REb
N0

[bits/complex dimension],
(1.11)
if we normalize to complex dimensions, in which case R is the rate per complex dimension.
Both (1.10) and (1.11) can easily be derived from (1.1).
Applying similar manipulations as above, we obtain the Shannon bound normalized
per dimension as
Eb
N0
≥22Cd −1
2Cd
;
Eb
N0
≥2Cc −1
Cc
.
(1.12)
Equations (1.12) are useful when the question of waveforms and pulse shaping is not a
central issue, since it allows one to eliminate these considerations by working with signal
dimensions, rather than the signals itself (see also Chapter 2). We will use (1.12) for our
comparisons.

1.5. BANDWIDTH, POWER, AND COMPLEXITY
15
The Shannon bounds (1.3) and (1.12) relate the spectral eﬃciency η to the power
eﬃciency Eb/N0, and establish fundamental limits in the trade-oﬀbetween the two primary
resources of data communications; power and spectrum. These limits hold regardless of
the signal constellation or coding method that is used for transmission.
Figure 1.8 shows (1.12)–as a solid line–as well as similar limits calculated for the cases
where the transmitted signal constellations are restricted to BPSK, QPSK, 8-PSK, 16-
PSK, and 16-QAM (see Chapter 2). As can be seen, these restrictions lead to various
degrees of loss that has to be accepted when a particular constellation is chosen. The
ﬁgure also shows the power and bandwidth eﬃciencies of some popular uncoded quadrature
constellations as well as that of a number of error control coded transmission schemes. The
trellis-coded modulation schemes used in practice, for example, achieve a power gain of
up to 6 dB without loss in spectral eﬃciency, while the more powerful coded modulation
methods such as trellis-turbo coding and block turbo coding provide even further gain. The
binary coding methods achieve a gain in power eﬃciency, but at the expense of spectral
eﬃciency with respect to the original signal constellation. The turbo-coded methods come
extremely close to capacity for η ≤1; we present such methods which can be made to
approach the Shannon bound arbitrarily closely in Chapter 8.
ConvCodes
BTC
BPSK
BPSK
Turbo
65536
Unachievable
Region
 BPSK
 QPSK
 8-PSK
 16-QAM
 32-QAM
 16-PSK
Turbo
Code 65,536
Block Turbo
Code BPSK
Block Turbo
Code 16-QAM
Block Turbo
Code 32-QAM
Block Turbo
Code 64-QAM
TCM 8-PSK
TTCM
TCM 16-PSK
Turbo
Code 65,536
(2,1,14) Conv. Code
(4,1,14) Conv. Code
Convolutional Codes
2   Seq. Dec.
14
256-states
2   -states
64          16         4 states
256         64         16        4 states
14
-1.59
0
2
4
6
8
10
12
14
0.1
0.2
0.5
1
2
5
10
Cc [bits/complex dimension]
Eb/N0[dB]
BPSK
QPSK
8PSK
16QAM
Shannon
Bound
Figure 1.8: Theoretical spectral and power eﬃciency limits for various signal constellations
and spectral eﬃciencies achieved by coded and uncoded transmission methods.

16
CHAPTER 1. INTRODUCTION
In all cases a rate reduction (traditional coding) or a signal set expansion (coded
modulation) is required in order to give the coding system the required redundancy. This
is also clearly obvious from the capacity curves for the diﬀerent signal constellations.
In order to compare these diﬀerent communications systems, we also need a parameter
expressing the performance level of a system. This parameter is the information bit error
probability Pb. For practical systems it typically falls into the range 10−3 ≥Pb ≥10−6,
though it is sometimes required to be lower, e.g., for digital TV, or much lower as for
optical and cable modem systems where BER on the order of Pb ≤10−6 are required. The
performance points in Figure 1.8 are drawn for Pb = 10−5.
Other methods which combine coding with signal shaping exist and exhibit similar
gains. For example, coded overlapped quadrature modulation (OC-QPSK) is a combined
coding and controlled intersymbol interference method [43] which causes smaller ampli-
tude variations than Nyquist signaling, and is therefore useful for systems with ampliﬁer
non-linearities, like satellite traveling wave tube (TWT) ampliﬁers.
Continuous-phase
modulation (CPM) is a constant-amplitude modulation format [1], which also has many
similarities with TCM, and derives from frequency modulation aiming at improving the
bandwidth eﬃciency by smoothing phase transitions between symbols. CPM has gone
through an evolution similar to TCM, and the reader is referred to the book by Anderson,
Aulin, and Sundberg [1], the standard reference on the subject. However, with the ad-
vent of highly linear ampliﬁers, CPM has lost one of its main selling points and has seen
signiﬁcantly less deployment then the linear modulation techniques.
The last, and somewhat hidden, player in the application of coding is complexity.
While we have shown that power and bandwidth can be captured elegantly by Shannon’s
theory, measures of complexity are much more diﬃcult to deﬁne. First there is what we
might term code complexity. In order to approach the Shannon bound, larger and larger
codes are required. In fact, Shannon et al. [46] proved the following lower bound on the
codeword error probability PB:
PB > 2−n(Esp(R)+o(N)),
Esp(R) = max
q
max
ρ>1 (E0(q, ρ) −ρR)).
(1.13)
The exponent E0(q, ρ) is called the Gallager exponent and depends on the symbol prob-
ability distribution q and the optimization parameter ρ, it is discussed in more detail in
Chapter 5, as well as in Gallager’s book on information theory [13]. The most important
point to notice is that this bound is exponential in the codelength n.
The bound is plotted for rate R = 1/2 in Figure 1.9 for BPSK modulation [44], together
with selected turbo-coding schemes and classic concatenated methods. The performance
of codes for various lengths follows the tendency of the bound, and we see a diminishing
return as the code size exceeds n ≈104 −105, beyond which only small gains are possible.
This is the reason why most practical applications of large codes target block sizes no larger
than this. On the other hand, codes cannot be shortened much below n = 104 without

1.5. BANDWIDTH, POWER, AND COMPLEXITY
17
a measurable loss in performance, which must be balanced against the overall system
performance. Implementations of near-capacity error control systems therefore have to
process blocks of 10,000 symbols or more, requiring appropriate storage and memory
management.
BTC
BPSK
Unachievable
Region
(32,26,4) Block Turbo
Code BPSK
Length=1334
4-state Turbo Code 
n=4599 concatenated 
(2,1,8) CC + RS (511,479) 
n=2040 concatenated 
(2,1,8) CC + RS (255,223) 
n=2048 concatenated 
(2,1,6) CC + RS (255,223) 
n=448
16-state
n=1334
16-state
n=2048
16-state
n=10,200
16-state
n=16,084
16-state
n=65,536
16-state
n=360
4-state
n=448
16-state
10
100
1,000
10,000
105
106
−1
0
1
2
3
4
Eb/N0
n
Figure 1.9: Block error rate PB and sphere-packing lower bound for rate R = 1/2 coded
example coding systems using BPSK. Turbo codes and selected classic concatenated coding
schemes are compared. The solid line is the Shannon limit at Eb/N0 = 0.19 dB at R = 0.5.
The other component of our complexity consideration is the computational complexity,
that is, the amount of processing that has to be performed to decode a codeword. This
is notoriously diﬃcult to measure, and parameters like the code state space or number
of multiplications may not be relevant. What ultimately counts is the size and power
consumption of a VLSI implementation, which is very much technology, architecture, and
design dependent. It suﬃces to say that what a coding theorist considers complex may
not be complex for a VLSI circuit designer. An example of this is the “folk theorem” that
an APP trellis decoder (Chapter 5) is about four times as complex as a Viterbi decoder for
the same code. Some circuit designers would argue that the trace-back implementation
of the Viterbi decoder easily compensates for the more complex arithmetic of the APP
decoder and that there is no real diﬀerence between the two in terms of implementation
complexity.

18
CHAPTER 1. INTRODUCTION
The situation of turbo codes is even more complicated to evaluate, since most of the
complexity of the decoder resides in the storage requirements of the large blocks that need
to be processed and the eﬃcient management of the this memory. The computational
units of a turbo decoder take up less than 10% of the VLSI area. Processing large blocks,
however, is inherent in eﬀective error control coding as discussed above.
More recently decoding circuits based on analog processing have emerged [26, 34, 57],
using subthreshold CMOS and bipolar multiplier circuits, both based on variations of
the well-known Gilbert cell [31]. These circuits are ideally matched to the arithmetic re-
quirements of soft-decision decoders, making use of the fundamental exponential transfer
characteristics of the transistors. Digital information is processed by small currents in the
transistor’s oﬀ-region, representing probabilities of the various discrete random variables.
Contrary to mainstream digital VLSI implementations where the component transistors
are used as on-oﬀswitches, and are optimized for that operation, analog processing oper-
ates the transistors in their oﬀ-position, and all computations are accomplished by what
digital technology calls leakage currents. Analog decoding is also diﬀerent from conven-
tional analog processing in that digital information is processed. This leads to a surprising
robustness of the analog decoder to typical analog impairments such as current mismatch,
body eﬀects, etc.
Several such decoders have already been successfully fabricated and tested, or exten-
sively simulated by a few research groups around the world, see, e.g., [58, 59]. The initial
idea was that subthreshold operation would result in substantial power savings of the de-
coding operation. While operating the transistor in the subthreshold region has indeed
a strong potential to save power (see, e.g., [6, 7]), the story regarding analog decoders is
more complex.
Zargham et al. [65] for example show that the current mirror circuit which is the base
of the Gilbert multiplier used in most of the analog designs is increasingly susceptible to
gate threshold voltage variations in the transistors that are paired to make the current
mirror. As the fabrication process shrinks, small variations in the gate size translate into
exponentially varying current errors in the Gilbert cells. This ultimately leads to a break-
down of the functionality of the decoder as a whole. Zargham et al. [65] argue that analog
decoders in sub-100 nm processes will require such large oversizing of the transistors that
going to smaller processes is not productive.
In an investigation by Winstead and Schlegel [42], the computational and energy re-
quirements of message passing decoders as discussed in Chapters 6 and 9 are examined
from fundamental viewpoints of computational theory and minimum-energy switching
principles. Since advanced message-passing decoders have a computational complexity
which is linear in the number of bits decoded, a direct energy cost measure per decoded
bit can be computed, quite irrespective of the actual code or the code family being used.
This leads to predictions of the energy cost per decoded message bit as miniaturization
processes continue to advance to ever smaller scales. Figure 1.10 shows the energy per-

1.5. BANDWIDTH, POWER, AND COMPLEXITY
19
formance attained by a number of message-passing decoders that were built by several
research and industrial groups. What is evident is that the process miniaturization is the
major driver for a reduction in the energy per bit. The ultimate limit for charged-based
computation is based on a conceptual single-charge device [66], and would attain a decod-
ing energy consumption anywhere between sub-fempto-joule to about 10 fJ/decoded bit,
depending on code and implementation parameters.
BTC
BPSK
Date of Introduction
1990
2000
2012
2016
2020
2008
Single-
Electron
Transistor
350 nm
1 nm
minimum
energy
decoder
250 nm
180 nm
130 nm
90 nm
65 nm
45 nm
22 nm
10 nm
5 nm
Analog
Analog
Decoders
Digital LDPC
Decoders
Digital Turbo
Decoders
Decoders
Digital
Decoders
sub
threshold
0.1fJ
1fJ
10fJ
100fJ
1pJ
10pJ
100pJ
1nJ
10nJ
100nJ
1μJ
Energy per decoded bit in joules
Figure 1.10: Energy consumption of various message-passing decoders and predictions for
future developments in joules per decoded bit.
While predictions are notoriously diﬃcult to make with accuracy, it appears that analog
decoders based on Gilbert-cell technology would ﬂatten out at around 10 pJ/decoded bit,
primarily due to the gate-threshold variations discussed above [65]. Digital decoders, on
the other hand, can beneﬁt from the miniaturization, and the calculations in [42] indicate
that a minimal energy cost for a digital implementation would be around 30,000kT, where
k is Boltzmann’s constant, and T is the temperature. This translates to about 0.1 fempto-
joules. Any further reductions in power consumption would need to rely on speculative
technologies such as adiabatic computation and/or quantum computing.

20
CHAPTER 1. INTRODUCTION
1.6
A Brief History–The Drive Towards Capacity
Forward error control (FEC) coding celebrated its ﬁrst success in the application of con-
volutional codes to deep-space probes in the 1960’s and 1970’s, and for quite a while
afterwards, FEC was considered an intellectual curiosity with deep-space communications
as its only viable practical application. Deep space communications is a classical case
of power-limited communications, and it serves as a picture book success story of error
control coding.
If we start with uncoded binary phase-shift keying (BPSK) as our baseline transmission
method (see Chapter 2) and assume coherent detection, we can achieve a bit error rate
of Pb = 10−5 at a bit energy-to-noise power ratio of Eb/N0 = 9.6 dB and at a spectral
eﬃciency of ideally 1 bit/dimension. From the Shannon limit in Figure 1.11, it can be
seen that 1 bit/dimension is theoretically achievable with Eb/N0 = 1.76 dB, indicating
that a power savings of nearly 8 dB is possible by applying proper coding. 8 dB is an
over 6-fold savings in transmit power, antenna size, or other aspect directly linked to the
received signal power, as, for example, 2.5 times the transmission distance.
n=1334
16-state
Galileo
BPSK
Turbo 
code
Mariner:
(32,6) biorthogonal code
with optimal decoding
(255,123) BCH code 
algebraic decoding
(2,1,6) CC
ML decoding
Pioneer: (2,1,31) CC 
sequential decoding
(4,1,14) CC with
ML-decoding
Voyager
Power Efficiency improves
Unachievable
Region
-2
-1
0
1
2
3
4
5
6
7
8
9
10
0
0.25
0.5
0.75
1
Spectral Eﬃciency [bits/dimension]
Eb
N0
Figure 1.11: Some milestones in the drive towards channel capacity achieved by the space
systems which evolved over the past 50 years as answer to the Shannon capacity challenge.
One of the earliest attempts to close the signal energy gap to the Shannon limit was
the use of a rate 6/32 biorthogonal (Reed-Muller) block code [27]. This code was used

1.6. A BRIEF HISTORY–THE DRIVE TOWARDS CAPACITY
21
on the Mariner Mars and Viking missions in conjunction with BPSK and soft-decision
maximum-likelihood decoding. This system had a spectral eﬃciency of 0.1875 bits/symbol
and achieved a bit error rate of Pb = 10−5 with an Eb/N0 = 6.4 dB. Thus, the (32,6)
biorthogonal code required 3.2 dB less power than BPSK at the cost of a ﬁve-fold increase
in the bandwidth. The performance of the 6/32 biorthogonal code is plotted in Figure
1.11, as are all the other systems discussed below.
In 1967, a new algebraic decoding technique was discovered for the popular Bose-
Chaudhuri-Hocquenghem (BCH) codes [2, 28]. This new algorithm enabled the eﬃcient
hard-decision decoding of an entire class of block codes–for example, the (255,123) BCH
code, which has a code rate of Rd ≈0.5 bits/symbol and achieves a BER of Pb = 10−5 at
Eb/N0 = 5.7 dB using algebraic decoding.
The next step was taken with the introduction of sequential decoding (see Chapter 5),
which could make use of soft-decision decoding. Sequential decoding allowed the decoding
of long constraint-length convolutional codes, and it was ﬁrst used on the Pioneer 9 mission
[11]. The Pioneer 10 and 11 missions in 1972 and 1973 both used a long constraint-length
(2,1,31), nonsystematic convolutional code (Chapter 4) [29]. A sequential decoder was
used which achieved Pb = 10−5 with Eb/N0 = 2.5 dB, and Rd = 0.5. This is only 2.5 dB
away from the capacity of the channel.
Sequential decoding has the disadvantage that the computational load is variable, and
this load grows exponentially the closer the operation point moves towards capacity (see
Chapter 4). For this and other reasons, the next generation of space systems employed
maximum-likelihood decoding. The Voyager spacecraft, launched in 1977, used a short
constraint-length (2,1,6) convolutional code in conjunction with a soft-decision Viterbi
decoder achieving Pb = 10−5 at Eb/N0 = 4.5 dB and a spectral eﬃciency of Rd = 0.5
bits/symbol. The biggest Viterbi decoder built to date [8] found application in the Galileo
mission, where a (4,1,14) convolutional code is used, yielding a spectral eﬃciency of Rd =
0.25 bits/symbol at Pb = 10−5 and Eb/N0 = 1.75 dB. The performance of this system is
2.5 dB away from the capacity limit. The systems for Voyager and Galileo are further
enhanced by the use of concatenation in addition to the convolutional inner code. An outer
(255,223) Reed-Solomon code [27] is used to reduce the required signal-to-noise ratio by
2.0 dB for the Voyager system and by 0.8 dB for the Galileo system.
More recently, Turbo-codes [3] using iterative decoding have virtually closed the ca-
pacity gap by achieving Pb = 10−5 at a spectacularly low Eb/N0 of 0.7 dB with Rd = 0.5
bits/symbol, and longer turbo codes come even closer to capacity, e.g., [48]. It is probably
appropriate to say that the half-century eﬀort to reach capacity has been achieved with
this latest invention, in particular in the regime of lower spectral eﬃciencies. With turbo
coding then, another quite unexpected step of about 2 dB right to the capacity limit was
made possible. Newer space communications systems virtually all use turbo or low-density
parity-check codes discussed in this book–see also Chapter 9.
Space applications of error control coding have met with spectacular success, and

22
CHAPTER 1. INTRODUCTION
for a long time the belief that coding was useful only for improving power eﬃciency of
digital transmission was poplar.
This attitude was thoroughly overturned by another
spectacular success of error control coding, this time for applications of data transmission
over voiceband telephone channels. Here it was not the power eﬃciency which was the
issue, but rather spectral eﬃciency, i.e., given a standard telephone channel with a ﬁxed
bandwidth and SNR, what was the maximum practical rate of reliable transmission?
The ﬁrst commercially available voiceband modem in 1962 achieved a transmission
rate of 2400 bits/s. Over the next 10 to 15 years these rates improved to 9600 bits/s,
which was then considered to be the maximum achievable rate, and eﬀorts to push the
rate higher were frustrated. Ungerb¨ock’s invention of trellis-coded modulation in the late
1970’s, however, opened the door to further, unexpected improvements. The modem rates
jumped to 14,400 bits/s and then to 19,200 bits/s using sophisticated TCM schemes [12].
The latest chapter in voiceband data modems is the establishment of the CCITT V.34
modem standard [15, 9]. The modems speciﬁed therein achieve a maximum transmission
rate of 28,800 bits/s, and extensions to V.34 to cover two new rates at 31,200 bits/s
and 33,600 bits/s have been speciﬁed.
However, at these high rates, modems operate
successfully only on a small percentage of the connections. It seems that the limits of
the voiceband telephone channel have been reached (according to [56]). This needs to be
compared to estimates of the channel capacity for a voiceband telephone channel, which
are somewhere around 30,000 bits/s.
The application of TCM was one of the fastest
migrations of an experimental laboratory system to an international standard (V.32 -
V.34) [5, 15, 9]. The trellis codes used in these advanced modems are discussed in treated
in detail in Chapter 3.
In many ways the telephone voiceband channel is an ideal playground for the appli-
cation of error control coding. Its limited bandwidth of about 3 kHz (400 Hz - 3400 Hz)
implies low data rates by modern standards. It therefore provides an ideal experimental
ﬁeld for high-complexity error control methods, which can be implemented without much
diﬃculty using current DSP technology. It is thus not surprising that coding for voiceband
channels was the ﬁrst successful application of bandwidth eﬃcient error control.
Nowadays, trellis coding in the form of bandwidth-eﬃcient TCM as well as more
conventional convolutional coding and higher-order modulation turbo-coding systems are
used for satellite communications, both geostationary and low-earth orbiting satellites, for
land-mobile and satellite-mobile services, for cellular communications networks, personal
communications services (PCS), high-frequency (HF) tropospheric long-range communi-
cations, and cable modems, and increasingly also for high-speed optical communications
systems. The Shannon capacity is now routinely the goal that is targeted for high-eﬃciency
communications systems. As an example thereof, the IEEE 802.3an standard for Ethernet
communications over twisted pair copper cables is discussed in more detail in Chapter 6.
To us it seems clear that the age of widespread application of error control coding is upon
us and every eﬃcient communications systems will entail some form of FEC.

Bibliography
[1] J.B. Anderson, T. Aulin, and C-E. Sundberg, Digital Phase Modulation, Plenum
Press, New York, 1986.
[2] E.R. Berlekamp, Algebraic Coding Theory, McGraw-Hill, New York, 1968.
[3] C. Berrou, A. Glavieux, and P. Thitimajshima, “Near Shannon limit error-correcting
coding and decoding:
Turbo-codes,” Proc. 1993 IEEE Int. Conf. on Commun.,
Geneva, Switzerland, pp. 1064–1070, 1993.
[4] C. Berrou and A. Galvieux, “Near optimal error-correcting coding and decoding:
Turbo codes,” IEEE Trans. Commun., vol. COM-44, no. 10, pp. 1261-1271, Oct.
1996.
[5] U. Black, The V Series Recommendations, Protocols for Data Communications Over
the Telephone Network, McGraw-Hill, New York, 1991.
[6] D. Bol, R. Ambroise, D. Flandre, and J.-D. Legat, “Interests and limitations of
technology scaling for subthreshold logic,” IEEE Very Large Scale Integration (VLSI)
Syst., vol. 17, no. 10, pp. 1508-1519, Oct. 2009.
[7] D. Bol, “Pushing ultra-low-power digital circuits into the nanometer era,” Ph.D. the-
sis, Universit´e catholique de Louvain, ´Ecole Polytechnique de Louvain, D´epartement
d’´Electricit´e, Dec. 2008.
[8] O.M. Collins, “The subtleties and intricacies of building a constraint length 15 con-
volutional decoder,” IEEE Trans. Commun., vol. COM-40, pp. 1810–1819, 1992.
[9] G.D. Forney, L. Brown, M.V. Eyuboglu, J.L. Moran III, “The V.34 high-speed modem
standard,” IEEE Commun. Mag., pp. 28–33, Dec. 1996.
[10] G.D. Forney, Concatenated Codes, MIT Press, Cambridge, Mass., 1966.
[11] G.D. Forney, “Final report on a study of a sample sequential decoder,” Appendix A,
Codex Corp., Watertown, MA, U.S. Army Satellite Communication Agency Contract
DAA B 07-68-C-0093, April 1968.
[12] G.D. Forney, “Coded modulation for bandlimited channels,” IEEE Information The-
ory Society Newsletter, Dec. 1990.
[13] R.G. Gallager, Information Theory and Reliable Communications, John Wiley &
Sons, New York, 1968.
23

24
BIBLIOGRAPHY
[14] R.M. Gray, Source Coding Theory, Kluwer Academic Publishers, Dordrecht, fourth
printing, 1997.
[15] CCITT Recommendations V.34.
[16] M.J.E. Golay, “Notes on digital coding,” Proc. IEEE, vol. 37, p. 657, 1949.
[17] J. Hagenauer and P. H¨oher, “A Viterbi algorithm with soft-decision outputs and its
applications,” Proc. IEEE Globecom’89, 1989.
[18] J. Hagenauer, “Rate compatible punctured convolutional codes (RCPC-codes) and
their application,” IEEE Trans. Commun., vol. COM-36, pp. 389–400, April 1988.
[19] R.W. Hamming, “Error detecting and error correcting codes,” Bell Syst. Tech. J.,
vol. 29, pp. 147–160, 1950.
[20] J.A. Heller and J.M. Jacobs, “Viterbi detection for satellite and space communica-
tions,” IEEE Trans. Commun. Technol., COM-19, pp. 835–848, Oct 1971.
[21] H. Imai et al. Essentials of Error-Control Coding Techniques, Academic Press, New
York, 1990.
[22] N.S. Jayant and P. Noll, Digital Coding of Waveforms, Prentice Hall, Englewood
Cliﬀs, NJ, 1984.
[23] K.Y. Lin and J. Lee, “Recent results on the use of concatenated Reed-Solomon/
Viterbi channel coding and data compression for space communications,” IEEE
Trans. Commun., vol. COM-32, pp. 518–523, 1984.
[24] S. Lin and D.J. Costello, Jr., Error Control Coding: Fundamentals and Applications,
Prentice Hall, Englewood Cliﬀs, NJ, 1983.
[25] S. Lin, D.J. Costello, Jr., and M.J. Miller, “Automatic-repeat-request error-control
schemes,” IEEE Commun. Mag., vol. 22, pp. 5–17, 1984.
[26] H.-A. Loeliger, F. Lustenberger, M. Helfenstein, and F. Tark¨oy, “Probability prop-
agation and decoding in analog VLSI,” IEEE Trans. Inform. Theory, pp. 837–843,
Feb. 2001.
[27] F.J. MacWilliams and N.J.A. Sloane, The Theory of Error Correcting Codes, North
Holland, New York, 1988.
[28] J.L. Massey, “Shift register synthesis and BCH decoding,” IEEE Trans. Inform. The-
ory, vol. IT-15, pp. 122–127, 1969.
[29] J.L. Massey and D.J. Costello, Jr., “Nonsystematic convolutional codes for sequential
decoding in space applications,” IEEE Trans. Commun. Technol., vol. COM-19, pp.
806–813, 1971.
[30] J.L. Massey, “Coding and modulation in digital communications,” Proc. Int. Z¨urich
Sem. Digital Commun., Z¨urich, Switzerland, March 1974, pp. E2(1)–E2(4).
[31] C. Mead, Analog VLSI and Neural Systems, Addison-Wesley, Reading, MA,1989.
[32] H. Meyr and G. Ascheid, Synchronization in Digital Communications, Vol. 1, John
Wiley & Sons, New York, 1990.

BIBLIOGRAPHY
25
[33] H. Meyr, M. Moeneclaey, and S.A. Fechtel, Digital Communication Receivers, John
Wiley & Sons, New York, 1998.
[34] M. Moerz, T. Gabara, R. Yan, and J. Hagenauer, “An analog .25 μm BiCMOS
tailbiting MAP decoder,” IEEE Proc. International Solid-State Circuits Conf., pp.
356–357, San Francisco, Feb. 2000.
[35] M. Mouly and M-B. Pautet, The GSM System for Mobile Communications, sold by
the authors, ISBN 2-9507190-0-7. 1993.
[36] H. Nyquist, “Certain topics in telegraph transmission theory,” AIEE Trans., pp. 617
ﬀ., 1946.
[37] J.P. Odenwalder, “Optimal decoding of convolutional codes,” Ph.D. thesis, University
of California, LA, 1970.
[38] J.K. Omura and B.K. Levitt, “Coded error probability evaluation for antijam com-
munication systems,” IEEE Trans. Commun., vol. COM-30, pp. 896–903, May 1982.
[39] J.G. Proakis, Digital Communications, McGraw-Hill, New York, 1989.
[40] S. Ramseier and C. Schlegel, “On the bandwidth/power tradeoﬀof trellis coded mod-
ulation schemes,” Proc. IEEE Globecom’93 (1993).
[41] S.A. Rhodes, R.J. Fang, and P.Y. Chang, “Coded octal phase shift keying in TDMA
satellite communications,” COMSAT Tech. Rev., vol. 13, pp. 221–258, 1983.
[42] C. Schlegel and C. Winstead, “From mathematics to physics: Building eﬃcient itera-
tive error control decoders,” International Symposium on Turbo Coding and Iterative
Information Processing, Sweden, Aug. 2012.
[43] C. Schlegel, “Coded overlapped quadrature modulation,” Proc. Global Conf. Com-
mun. GLOBECOM’91, Phoenix, AZ, Dec. 1991.
[44] C. Schlegel and L.C. Perez, “On error bounds and turbo codes,” IEEE Commun. Lett.,
vol. 3, no. 7, July 1999.
[45] C.E. Shannon, “A mathematical theory of communications,” Bell Syst. Tech. J., vol.
27,, pp. 379-423, July 1948.
[46] C.E. Shannon, R.G. Gallager, and E.R. Berlekamp, “Lower bounds to error probabil-
ities for coding on discrete memoryless channels,” Inform. Contr., vol. 10, pt. I, pp.
65–103, 1967, Also, Inform. Contr., vol. 10, pt. II, pp. 522-552, 1967.
[47] TIA/EIA/IS-95 interim standard, mobile station–base station compatibility standard
for dual-mode wideband spread spectrum cellular systems, Telecommunications In-
dustry Association, Washington, D.C., July 1993.
[48] S. tenBrink, “A rate one-half code for approaching the Shannon limit by 0.1 dB,”
Electron. Lett., vol. 36, no. 15, pp. 1293–1294, July 2000.
[49] G. Ungerboeck, J. Hagenauer, and T. Abdel-Nabi, “Coded 8-PSK experimental mo-
dem for the INTELSAT SCPC system,” Proc. ICDSC, 7th, pp. 299–304, 1986.

26
BIBLIOGRAPHY
[50] G. Ungerboeck, “Channel coding with multilevel/phase signals,” IEEE Trans. Inform.
Theory, vol. IT-28, no. 1, pp. 55-67, Jan. 1982.
[51] G. Ungerboeck, “Trellis-coded modulation with redundant signal sets part I: Intro-
duction,” IEEE Commun. Mag., vol. 25, no. 2, pp. 5–11, Feb. 1987.
[52] G. Ungerboeck, “Trellis-coded modulation with redundant signal sets part II: State
of the art,” IEEE Commun. Mag., vol. 25, no. 2, pp. 12–21, Feb. 1987.
[53] A.J. Viterbi, “Spread spectrum communications–myths and realities,” IEEE Com-
mun. Mag., vol. 17, pp. 11-18, May 1979.
[54] A.J. Viterbi, “When not to spread spectrum–a sequel,” IEEE Commun. Mag., vol.
23, pp. 12-17, April 1985.
[55] F.-Q. Wang and D.J. Costello, “Probabilistic construction of large constraint length
trellis codes for sequential decoding,” IEEE Trans. Commun., vol. 43, no. 9,
Sept. 1995.
[56] R. Wilson, “Outer limits,” Electronic News, May 1996.
[57] C. Winstead, J. Dai, C. Meyers, C. Schlegel, Y.-B. Kim, W.-J. Kim, “Analog MAP
decoder for (8,4) Hamming code in Subthreshold CMOS,” Advanced Research in VSLI
Conference ARVLSI, Salt Lake City, March, 2000.
[58] C. Winstead, J. Dai, S. Yu, C. Myers, R. Harrison, and C. Schlegel, “CMOS analog
MAP decoder for (8,4) Hamming code,” IEEE Journal of Solid State Circuits, vol.
29, no. 1, Jan. 2004.
[59] C. Winstead and C. Schlegel, ”Importance Sampling for SPICE-level veriﬁcation of
analog decoders”, International Symposium on Information Theory (ISIT’03), Yoko-
hama, June, 2003.
[60] A.P. Worthen, S. Hong, R. Gupta, W.E. Stark, “Performance optimization of VLSI
transceivers for low-energy communications systems,” IEEE Military Communica-
tions Conference Proceedings, MILCOM 1999, vol. 2, pp. 1434–1438,1999.
[61] J.M. Wozencraft and R.S. Kennedy, “Modulation and demodulation for probabilistic
coding”, IEEE Trans. Inform. Theory, vol. IT-12, no. 3, pp. 291–297, July, 1966.
[62] J.M. Wozencraft and I.M. Jacobs, Principles of Communication Engineering, John
Wiley & Sons, New York, 1965, reprinted by Waveland Press, 1993.
[63] 3GPP Group Website, Releases, http://www.3gpp.org/releases.
[64] 3GPP2 Group Website, http://www.3gpp2.org.
[65] M. Zargham, C. Schlegel, J. P. Chamorroc, C. Lahuecc, F. Seguinc, M. J´ez´equelc,
and V. Gaudet, “Scaling of analog LDPC decoders in sub-100 nm CMOS processes,”
Integration—the VLSI J., vol. 43, no. 4, Sept. 2010, pp. 365–377.
[66] V. Zhirnov R. Cavin, III, J. Hutchby, and G. Bourianoﬀ, “Limits to binary logic switch
scaling: A gedanken model,” Proceedings of the IEEE, vol. 91, no. 11, Nov. 2003.

Chapter 2
Communications Basics
2.1
The Probabilistic Viewpoint
Communication, the transmission of information from a sender to a receiver (destination),
is fundamentally a random experiment, particularly evident if the information is in digital
form. The sender selects one of a number of possible messages which is transmitted to
the receiver, which has to determine which message was chosen by the sender. In doing
so the receiver uses the fact that the transmitted message is chosen from a set of possible
messages known to both sender and receiver. In doing so it applies probabilistic principles
to determine which message was the most likely to have been sent. This is the fundamental
concept of the decoding process.
The signal carrying the message is typically subject to random distortions, primar-
ily noise, which adds a further complication to the transmission process. The noise in
a communications link is a fundamental property of the link, arising from a variety of
physical processes. It is often used to describe any process whose precise form is inher-
ently unknown to the receiver, irrespective of whether it is really random, or only appears
so to the receiver. Noise is not categorically a bad thing, for without noise, the concept
of communications would not truly exist, since all information could be made available
anywhere with little eﬀort. Consider the hypothetical situation where we want to move an
arbitrary amount of information virtually instantaneously. We could do this, conceptu-
ally, by transmitting a chosen voltage signal whose precise numerical representation could
carry an unlimited amount of data, for example as the inﬁnite expansion of that numerical
representation into a binary representation, whose bits are our information to transport.
All this is, of course, only thinkable if there is no noise that limits our ability to precisely
deﬁne and transmit such a signal. Noise then, in a fundamental sense, prevents informa-
tion from being omnipresent and enables the modern concepts of communications. Since
noise is a random process, it should not surprise us that communication theory draws
27
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

28
CHAPTER 2. COMMUNICATIONS BASICS
heavily from probability theory.
Now Figure 2.1 shows a simpliﬁed system diagram of such a sender/receiver commu-
nication system. The transmitter performs the random experiment of selecting one of
the M messages in the message set {m(i)}, say m(i) and transmits a corresponding signal
s(i)(t), chosen from a set of signals {s(i)(t)}. In many cases this signal is a continuous
function of time, such as a voltage level, a current, or ultimately an electromagnetic signal
that propagates from transmitter to receiver typical for modern telecommunications. The
transmission medium, irrespective of its physical representation, is generically called the
channel. In reality it may be a telephone twisted wire pair, a radio link, an underwater
acoustic link, or any other suitable arrangement, as elaborated in Chapter 1. One of the
major impairments in all communication systems is thermal noise, the quintessential of all
noise sources. Thermal noise is generated by the random motion of charged particles either
inside the receiver itself, at the signal source, or anywhere in transit. Thermal noise has
the property that it adds linearly to the received signal, which is due to the superposition
principles of electromagnetic waves,1 hence the channel model in Figure 2.1.
+
Transmitter
Receiver
m(i)
s(i)(t)
r(t)
n(t)
ˆm
Figure 2.1: Block diagram of a sender/receiver communication system used on a channel
with additive noise.
A large body of literature is available on modeling channel impairments on the trans-
mitted signal. In this book we concentrate on the almost ubiquitous case of additive white
Gaussian noise (AWGN), which is reviewed brieﬂy in Appendix 2 A. Noise is a random
quantity as discussed above, and, together with the random choice of messages, our com-
munication system becomes a joint random experiment. More precisely, using Figure 2.1,
these quantities are random processes (i.e., random functions). It is now the task of the
receiver to detect the selected message m(i) with the the best possible reliability, observing
the received signal r(t). In the context of a discrete set of messages, one speaks of message
detection and the probabilities of correct or false detection of a message. The probability
of error, which is deﬁned as the probability that the message ˆm identiﬁed by the receiver
is not the one originally chosen by the transmitter, i.e., Pe = Pr( ˆm = m(i)) is the almost
1Actually, more fundamentally it is due to the fact that the solutions to the wave equation form a linear
set, and the wave equation models most of the dominant physical channels.

2.2. VECTOR COMMUNICATION CHANNELS
29
universally accepted and widely used measure of the quality of the detection process. The
solution to the detection problem is essentially completely known for many cases and we
will subsequently present an overview of optimum receiver principles in additive white
Gaussian noise.
2.2
Vector Communication Channels
A very popular way to generate signals at the transmitter which carry our messages is to
synthesize them as a linear combination of N basis waveforms φj(t), i.e., the transmitter
selects
s(i)(t) =
N

j=1
s(i)
j φj(t)
(2.1)
as the transmitted signal for the ith message. Often the basis waveforms are chosen to be
orthonormal, that is, they fulﬁll the condition
 ∞
−∞
φj(t)φl(t)dt = δjl =
 1,
j = l,
0,
otherwise.
(2.2)
This leads to a vector interpretation of the transmitted signals, since, once the basis
waveforms are speciﬁed, s(i)(t) is completely determined by the N-dimensional vector of
numbers
s(i) =

s(i)
1 , s(i)
2 , · · · , s(i)
N

.
(2.3)
We can now visualize the signals geometrically by viewing the signal vectors s(i) in Eu-
clidean N-space, spanned by the usual orthonormal basis vectors, where each basis vector
is associated with a basis function. This geometric representation of signals is called the
signal space representation, and the vectors are called a signal constellation. The idea
is illustrated for N = 2 in Figure 2.2 for the signals s(1)(t) = sin(2πf1t)w(t), s(2)(t) =
cos(2πf1t)w(t), s(3)(t) = −sin(2πf1t)w(t), and s(4)(t) = −cos(2πf1t)w(t), where
w(t) =
 
2Es
T ,
0 ≤t ≤T,
0
otherwise,
(2.4)
and f1 = k/T is an integer multiple of the symbol time 1/T. The ﬁrst basis function is
φ1(t) =

2/T sin(2πf1t) and the second basis function is φ2(t) =

2/T cos(2πf1t). The
signal constellation in Figure 2.2 is called quadrature-phase shift-keying (QPSK).
Note that while we have lost information about the actual waveform that is used to
generate the signals si(t), we have gained a higher level of abstraction, which will make it
much easier to discuss subsequent concepts of coding and modulation. Knowledge of the
signal vector s(i) implies knowledge of the transmitted message m(i), since, in a sensible

30
CHAPTER 2. COMMUNICATIONS BASICS
system, there is a one-to-one mapping between the two.
The problem of decoding a
received waveform is therefore equivalent to recovering the signal vector s(i).
s(3)
s(1)
s(2)
s(4)
φ2
φ1
Figure 2.2: Illustration of four signals using two orthogonal basis functions.
This can be accomplished by passing the received signal waveform through a bank of
correlators, each of which correlates s(i)(t) with one of the basis functions, performing the
operation
 ∞
−∞
s(i)(t)φl(t)dt =
N

j=1
s(i)
j
 ∞
−∞
φj(t)φl(t)dt = s(i)
l ,
(2.5)
that is, the lth correlator recovers the lth component s(i)
l
of the signal vector s(i).
Later we will need the squared Euclidean distance between two signals s(i) and s(j),
given as d2
ij = |s(i) −s(j)|2, which is a measure of the noise resistance of these two signals.
Note that
d2
ij
=
N

l=1

s(i)
l
−s(j)
l
2
(2.6)
=
N

l=1
 ∞
−∞

s(i)
l
−s(j)
l

φl(t)
2
(2.7)
=
 ∞
−∞

s(i)(t) −s(j)(t)
2
dt
(2.8)
is in fact the energy of the diﬀerence signal (s(i)(t) −s(j)(t)). (It is easier to derive (2.8)
in the reverse direction, i.e., from bottom to top.)
It can be shown [28, 16] that this correlator receiver is optimal in the sense that no
relevant information is discarded and minimum error probability can be attained, even

2.3. OPTIMUM RECEIVERS
31
when the received signal contains additive white Gaussian noise. In this case the received
signal r(t) = s(i)(t) + nw(t) produces the received vector r = s(i) + n at the output
of the bank of correlators. The statistics of the noise vector n can easily be evaluated,
using the orthogonality of the basis waveforms and the noise autocorrelation function
E[nw(t)nw(t+τ)] = δ(τ)N0/2, where δ(t) is Dirac’s delta function and N0 is the one-sided
noise power spectral density (see Appendix 2.A). We obtain
E [nlnj]
=
 ∞
−∞
 ∞
−∞
E [nw(α)nw(β)] φl(α)φj(β) dαdβ
=
N0
2
 ∞
−∞
φl(α)φj(α) dα =
 N0/2,
l = j,
0,
otherwise.
(2.9)
We are pleased to see that, courtesy of the orthonormal basis waveforms, the components of
the random noise vector n are all uncorrelated. Since nw(t) is a Gaussian random process,
the sample values nl, nj are necessarily also Gaussian (For more detail see, e.g., [2]).
From the above we conclude that the components of n are independent Gaussian random
variables with common variance N0/2 and mean zero.
We have thus achieved a completely equivalent vector view of a communication system.
The advantages of this point of view are manifold. Firstly, we need not concern ourselves
with the actual choices of signal waveforms when discussing receiver algorithms, and, sec-
ondly, the diﬃcult problem of waveform communication involving stochastic processes and
continuous signal functions has been transformed into the much more manageable vector
communication system involving only random vectors and signal vectors, and thirdly, we
have gained a geometric view of communications. In this context, linear algebra, the tool
for geometric operations, plays an important role in modern communication theory.
It is interesting to note that the vector representation is ﬁnite-dimensional, with the
number of dimensions given by the dimensionality of the signal functions. The random
function space is inﬁnite-dimensional, however. The optimality of the correlator receiver
[28] shows then that we need only be concerned with that part of the noise which is pro-
jected onto the ﬁnite-dimensional signal space. All other noise components are irrelevant.
In fact, this is the way the optimality of the correlator receiver is typically proven: First
it is shown that the noise components which are not part of the ﬁnite-dimensional signal
space are irrelevant and need not be considered by the receiver. This leads directly to the
ﬁnite-dimensional geometric signal space interpretation discussed above.
2.3
Optimum Receivers
If our bank of correlators produces a received vector r = s(i)+n, then an optimal detector
will chose as message hypothesis, ˆm = m(j), the one which maximizes the conditional

32
CHAPTER 2. COMMUNICATIONS BASICS
probability
P[m(j)|r],
(2.10)
because this maximizes the overall probability of being correct, Pc, which can be seen from
Pc =

r
P[correct|r]p(r) dr,
(2.11)
that is, since p(r) is non-negative everywhere, maximizing Pc can be achieved by maxi-
mizing P[correct|r] for each received r. This is known as a maximum a posteriori (MAP)
detection.
Using Bayes’ rule, we obtain
P[m(j)|r] = P[m(j)]p(r|m(j))
p(r)
,
(2.12)
and, postulating that the signals are all used equally likely, it suﬃces to select ˆm = m(j)
such that p(r|m(j)) is maximized. This, in turn, is the maximum likelihood (ML) receiver
principle. It minimizes the signal error probability, but only for equally likely signals, in
which case MAP and ML are equivalent.
Since r = s(i) + n and n is an additive Gaussian random vector independent of the
signal s(i), we may further develop the optimum receiver using p(r|m(j)) = pn(r −s(j)),
which is an N-dimensional Gaussian density function given by
pn(r −s(j)) =
1
(πN0)N/2 exp

−|r −s(j)|2
N0

.
(2.13)
Maximizing (2.10) is now seen to be equivalent to minimizing the Euclidean distance
|r −s(j)|2
(2.14)
between the received vector and the hypothesized signal vector s(j).
The decision rule (2.14) implies decision regions D(j) for each signal point which consist
of all the points in Euclidean N-space which are closer to s(j) than any other signal point.
These regions are also known as Voronoi regions, named after Georgy Feodosevich Voronoy,
a Ukranian mathematician of the late 19th century. These decision regions are illustrated
for QPSK in Figure 2.3.
The probability of error given a particular transmitted signal s(i) can now be inter-
preted as the probability that the additive noise n carries the signal s(i) outside its decision
region D(i). This probability can be calculated as
Pe(s(i)) =

r∈D(i) pn(r −s(i)) dr.
(2.15)

2.4. MATCHED FILTERS
33
s(3)
s(1)
s(2)
s(4)
D(2)
D(1)
D(4)
D(3)
Figure 2.3: Decision regions for ML detection of equiprobable QPSK signals.
Equation (2.15) is, in general, very diﬃcult to calculate, and simple expressions exist
only for some special cases. The most important such special case is the two-signal error
probability, which is deﬁned as the probability that signal s(i) is decoded as signal s(j)
assuming that there are only these two signals. In order to calculate the two-signal error
probability we may disregard all signals except s(i), s(j). Take, for example, s(i) = s(1)
and s(j) = s(2) in Figure 2.3.
The new decision regions are D(i) = D(1) ∪D(4) and
D(j) = D(2)∪D(3). The decision region D(i) is expanded to a half-plane and the probability
of deciding on message m(j) when message m(i) was actually transmitted, know as the
pair-wise error probability, is given by
Ps(i)→s(j) = Q
⎛
⎝

d2
ij
2N0
⎞
⎠,
(2.16)
where d2
ij = |s(i) −s(j)|2 is the energy of the diﬀerence signal, and
Q(α) =
1
√
2π
 ∞
α
exp

−β2
2

dβ,
(2.17)
is a non-elementary integral, referred to as the (Gaussian) Q-function. For a more detailed
discussion, see [28].
In any case, Equation (2.16) states that the probability of error
between two signals decreases exponentially with their squared Euclidean distance dij,
due to the well-known overbound [16]
Q

d2
ij/2N0

≤(1/2) exp

−d2
ij/4N0

.
(2.18)
2.4
Matched Filters
The correlation operation (2.5) used to recover the signal vector components can be imple-
mented as a ﬁltering operation. The signal s(i)(t) is passed through a ﬁlter with impulse

34
CHAPTER 2. COMMUNICATIONS BASICS
response φl(−t) to obtain
u(t) = s(i)(t)  φl(−t) =
 ∞
−∞
s(i)(α)φl(α −t) dα.
(2.19)
If the output of the ﬁlter φl(−t) is sampled at time t = 0, Equations (2.19) and (2.5) are
identical, i.e.,
u(t = 0) = s(i)
l
=
 ∞
−∞
s(i)(α)φl(α) dα.
(2.20)
Of course some appropriate delay needs to be built into the system to guarantee that
φl(−t) is causal. We shall not be concerned with this delay in our treatise.
The maximum-likelihood receiver now minimizes |r −s(i)|2, or equivalently maximizes
2 · r · s(i) −
s(i)
2
,
(2.21)
where we have neglected the term |r|2, which is common to all the hypotheses.
The
correlation r·s(i) is the central part of (2.21) and can be implemented in two diﬀerent ways:
as the basis-function-matched ﬁlter receiver, performing a summation after correlation to
combine the diﬀerent signal components, i.e.,
r · s(i) =
N

j=1
rjs(i)
j
=
N

j=1
s(i)
j
 ∞
−∞
r(t)φj(t) dt,
(2.22)
or as the signal matched ﬁlter receiver directly computing
r · s(i) =
 ∞
−∞
r(t)
N

j=1
s(i)
j φj(t)dt =
 ∞
−∞
r(t)s(i)(t) dt.
(2.23)
The two diﬀerent receiver implementations are illustrated in Figure 2.4. Usually, if the
number of basis functions is much smaller than the number of signals, the basis-function-
matched ﬁlter implementation is preferred. Spread spectrum systems [29], where it is more
expedient to use (2.23), are notable exceptions.
The optimality of the correlation receiver (2.5) implies that both receiver structures
of Figure 2.4 are optimal also. The sampled outputs of the matched ﬁlters are therefore
suﬃcient for optimal detection of the transmitted message, and hence form what is known
as suﬃcient statistics [5].

2.5. MESSAGE SEQUENCES
35
r(t)
r(t)
r · s(1)
r · s(2)
r · s(3)
r · s(M)
φ(1)(−t)
φ(2)(−t)
φ(N)(−t)
N
j=1 rjs(i)
j
sampling at iT
r · s(1)
r · s(2)
r · s(3)
r · s(M)
s(1)(−t)
s(2)(−t)
s(3)(−t)
s(M)(−t)
Figure 2.4: The basis-function matched ﬁlter receiver shown on top and the signal matched
ﬁlter receiver shown in the bottom. M is the number of messages in the message set.
2.5
Message Sequences
In practice, our signals s(i)(t) will not be limited to a small set of messages, but consist
of large data transfer that requires the transmission of sequences of messages, analogous
to how sentences are made up of sequences of letters. The sequences of messages are then
represented by identical, time-displaced waveforms, called signal pulses. The complete
“phrase” of our message is now described by
s(i)(t) =
l

r=−l
a(i)
r p(t −rT),
(2.24)
where p(t) is some convenient signal pulse waveform, the a(i)
r
are the discrete symbol values
from our signal alphabet, which is ﬁnite, for example, binary signaling: a(i)
r
∈{−1, 1},
and 2l + 1 is the length of the sequence in symbols. The parameter T is the time delay
between successive pulses, also called the symbol period or symbol rate. The output of the
ﬁlter matched to the entire signal s(i)(t) is what we need to compute in principle and is

36
CHAPTER 2. COMMUNICATIONS BASICS
given as
y(t)
=
 ∞
−∞
r(α)
l

r=−l
a(i)
r p(α −rT −t)dα
=
l

r=−l
a(i)
r
 ∞
−∞
r(α)p(α −rT −t) dα,
(2.25)
and the sampled value of v(t) at t = 0 is given by
y(t = 0) =
l

r=−l
a(i)
r
 ∞
−∞
r(α)p(α −rT) dα =
l

r=−l
a(i)
r yr,
(2.26)
where yr =
 ∞
−∞r(α)p(α−rT) is the output of a ﬁlter matched to the pulse p(t) sampled at
time t = rT. We see that the signal-matched ﬁlter can be implemented conveniently by a
single pulse matched ﬁlter, whose output y(t) is simply sampled at multiples of the symbol
time T, a much more economical approach than a brute-force implementation of (2.25).
The time-shifted waveforms p(t−rT) may serve as orthonormal basis functions, if they
fulﬁll the orthogonality condition
 ∞
−∞
p(t −rT)p(t −hT) dt = δrh.
(2.27)
The output waveform of the pulse-matched ﬁlter p(−t) in the absence of noise is given by
z(t) =
l

r=−l
a(i)
r p(t −rT)  p(−t) =
l

r=−l
a(i)
r g(t −rT),
(2.28)
where g(t −rT) =
 ∞
−∞p(α −rT)p(α −t) dα is the composite pulse/pulse-matched ﬁlter
waveform. If (2.27) holds, z(t) is completely separable and the rth sample value z(rT) = zr
depends only on a(i)
r , i.e., zr = a(i)
r , even if the pulses p(t−rT) overlap in time. If successive
symbols a(i)
r
are chosen independently the system may then be viewed as independently
using a 1-dimensional signal constellation system 2l + 1 times. This results in tremendous
savings in complexity and represents the state of the art of digital signaling.
Condition (2.27) ensures that symbols transmitted at diﬀerent times do not interfere
with each other, that is, we have intersymbol interference-free signaling. Condition (2.27)
is known as the Nyquist criterion, which requires that the composite waveform g(t) passes
through zero at all multiples of the sampling time T, except t = 0, i.e.,
 ∞
−∞
p(t −rT)p(t −hT) dt = δrh ⇒g(rT) =
 1,
if r = 0,
0,
otherwise.
(2.29)

2.5. MESSAGE SEQUENCES
37
An example of such a composite pulse g(t) together with its Fourier transform G(f)
is shown in Figure 2.5. Note that G(f) also happens to be the energy spectrum of the
transmitted pulse p(t) since G(f) = P(f)P ∗(f) = |P(f)|2.
Fourier transform
T
−T
T
g(t)
G(f)
−1
2T
1
2T
f
t
Figure 2.5: Example of a Nyquist pulse with trapezoid frequency spectrum.
The Nyquist criterion can be translated into the frequency domain via the Fourier
transform by observing that
g(rT)
=
 ∞
−∞
G(f)e2πjfrT df
=
∞

n=−∞
 (2n+1)/2T
(2n−1)/2T
G(f)e2πjfrT df
=
 1/2T
−1/2T
∞

n=−∞
G

f + n
T

e2πjfrT df.
(2.30)
If the folded spectrum
∞

n=−∞
G

f + n
T

= T,
−1
2T ≤f ≤1
2T
(2.31)
equals a constant, T, for normalization reasons, the integral in (2.30) evaluates to g(rT) =
sin(πr)/(πr) = δ0r, i.e., the sample values of g(t) are zero at all non-zero multiples of T, as
required. Equation (2.31) is the spectral form of the Nyquist criterion for no intersymbol
interference (compare Figure 2.5).
A very popular Nyquist pulse is the spectral raised-cosine pulse whose Fourier trans-

38
CHAPTER 2. COMMUNICATIONS BASICS
form is given by
G(f) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
T,
|f| ≤1−β
2T ,
T
2

1 −sin

π(2T|f|−1)
2β

,
1−β
2T ≤|f| ≤1+β
2T ,
0,
1+β
2T ≤|f|,
(2.32)
where β ∈[0, 1] is the roll-oﬀfactor of the pulse, whose bandwidth is (1 + β)/(2T). In
practical systems, values of β around 0.3 are routinely used, and pulses with β as low as
0.1 can be approximated by realizable ﬁlters, producing spectrally compact signals.
T
−T
−3T
3T
−1
2T
1
2T
−1
T
1
T
β = 1
β = 0.5 β = 0.1
t
f
Figure 2.6: Spectral raised-cosine Nyquist pulses and their Fourier transforms.

2.6. THE COMPLEX EQUIVALENT BASEBAND MODEL
39
The impulse response corresponding to G(f) is
g(t, β) = sin(πt/T)
πt/T
cos(βπt/T)
1 −(2βt/T)2 .
(2.33)
Pulses g(t, β) and their spectra are shown in Figure 2.6 for various the roll-oﬀfactors.
These observations lead to the extremely important and popular root-Nyquist signaling
concept shown in Figure 2.7. This name stems from the fact that the actual pulse shape
used for transmission, p(t), is the inverse Fourier transform of

G(f), the Nyquist pulse.
In the case of a rectangular brick-wall frequency response, however, p(t) = g(t) = sin(πt/T)
πt/T
.
+
Root-Nyquist
P(f)
Root-Nyquist
P ∗(f)
ar
arp(t −rT)
r(t)
nw(t)
sampling
at t = rT
Figure 2.7: Communications with root-Nyquist signaling for optimal noise suppression.
While  Nyquist  signaling  achieves  excellent  spectral  eﬃciencies,  there are some imple-
mentation related diﬃculties associated with it. Since the pulses are not time duration 
limited, they need to be approximated over some ﬁnite time duration, which causes some 
spectral spill-over. A more severe problem occurs with timing errors in the sampling. 
Inaccurate timing will generate a possibly large number of adjacent symbols to interfere 
with each other’s detection, making timing very crucial. Transmission over non-ﬂat 
frequency channels also causes more severe intersymbol interference than with some other 
pulse shapes. This in-terference needs to be compensated or equalized. Equalization is 
discussed in standard textbooks on digital communications, e.g., [16, 21].
2.6
The Complex Equivalent Baseband Model
In practical applications, one often needs to shift a narrow-band signal into a higher fre-
quency band for purposes of transmission. The reason for that may lie in the transmission
properties of the physical channel, which only permits the passage of signals in certain, usu-
ally high, frequency bands. This occurs, for example, in radio transmission. The process
of shifting a signal in frequency is called modulation with a carrier frequency. Modulation
is also important for wire-bound transmission, since it allows the coexistence of several sig-
nals on the same physical medium, all residing in diﬀerent frequency bands; this is known
as frequency division multiplexing (FDM). Probably the most popular modulation method
for digital signals is quadrature double side-band suppressed carrier (DSB-SC) modulation.

40
CHAPTER 2. COMMUNICATIONS BASICS
DSB-SC modulation is a simple linear shift in frequency of a signal x(t) with low-
frequency content, called a baseband signal, into a higher frequency band by multiplying
x(t) with a cosine or sine waveform with carrier frequency f0, as shown in Figure 2.8,
giving a carrier signal
s0(t) = x(t)
√
2 cos(2πf0t),
(2.34)
where the factor
√
2 is used to make the powers of s0(t) and x(t) equal.
If our baseband signal x(t) occupies frequencies from 0 to W Hz, s0(t) occupies fre-
quencies from f0 −W to f0 + W Hz, an expansion of the bandwidth by a factor of 2. But
we quickly note that we can put another orthogonal signal, namely y(t)
√
2 sin(2πf0t), into
the same frequency band and that both baseband signals x(t) and y(t) can be recovered by
the demodulation operation shown in Figure 2.8, known as a product demodulator, where
the low-pass ﬁlters W(f) serve to reject unwanted out-of-band noise and signals. It can
be shown (see, for example, [28]) that this arrangement is optimal; i.e., no information or
optimality is lost by using the product demodulator for DSB-SC modulated signals.
+
+
+
+
+
+
+
Lowpass
Filter W(f)
Lowpass
Filter W(f)
y(t)
Lowpass
Filter W(f)
Lowpass
Filter W(f)
x(t)
√
2 sin(2πf0t)
√
2 cos(2πf0t)
√
2 sin(2πf0t)
√
2 cos(2πf0t)
nw(t)
y(t) + ny(t)
x(t) + nx(t)
Figure 2.8: Quadrature DSB-SC modulation/demodulation stages.
If the synchronization between modulator and demodulator is perfect, the two signals,
x(t), the in-phase signal, and y(t), the quadrature signal, are recovered independently
without aﬀecting each other. The DSB-SC modulated bandpass channel is then, in essence,
a dual channel for two independent signals, each of which may carry an independent data
stream.
In view of our earlier approach using basis functions, we may want to view pairs
of inputs to the two channels as a 2-dimensional signal.
Since these two dimensions
are intimately linked through the carrier modulation, and since bandpass signals are so
ubiquitous in digital communications, a complex notation for bandpass signals has been

2.6. THE COMPLEX EQUIVALENT BASEBAND MODEL
41
adopted widely. In this notation, the in-phase signal x(t) is real, the quadrature signal
jy(t) is an imaginary signal, and the bandpass signal s0(t) can be expressed as
s0(t)
=
x(t)
√
2 cos(2πf0t) −y(t)
√
2 sin(2πf0t)
=
Re

(x(t) + jy(t))
√
2e2πjf0t
,
(2.35)
where s(t) = (x(t) + jy(t)) is called the complex envelope of s0(t).
If both signals are sequences of identical pulses p(t), the complex envelope becomes
s(t) =
l

r=−l
(ar + jbr) p(t −rT) =
l

r=−l
crp(t −rT),
(2.36)
where cr is a complex (2-dimensional) number, representing both the in-phase and the
quadrature information symbol.
The noise entering the system is demodulated and produces the two low-pass noise
waveforms nx(t) and ny(t), given by
nx(t)
=
√
2
 ∞
−∞
nw(α) cos(2πf0α)w(t −α) dα;
(2.37)
ny(t)
=
√
2
 ∞
−∞
nw(α) sin(2πf0α)w(t −α) dα,
(2.38)
where w(t) = sin(2πtW)/2πtW is the impulse response of an ideal low-pass ﬁlter with
cutoﬀfrequency W.
The autocorrelation function of nx(t) (and ny(t) analogously) is
given by
E [nx(t)nx(t + τ)] = 2
 ∞
−∞
 ∞
−∞
cos(2πf0α) cos(2πf0β)w(t −α)w(t + τ −β)
×E [nw(α)nw(β)] dαdβ
=
N0
2
 ∞
−∞
(1 + cos(4πf0α)) w(t −α)w(t + τ −α) dα
=
N0
2
 ∞
−∞
w(t −α)w(t + τ −α) dα
=
N0
2
 W
−W
e−2πjfτdf = N0W sin(2πWτ)
2πWτ
,
(2.39)
where we have used Parseval’s relationships ([28], pp. 237–238) in the third step, i.e.,
the power of nx(t) equals E

n2(t)

= WN0. Equation (2.39) is the correlation function of

42
CHAPTER 2. COMMUNICATIONS BASICS
white noise after passage through a low-pass ﬁlter of bandwidth W, i.e., the multiplication
with the demodulation carrier has no inﬂuence on the statistics of the output noise nx(t)
and can be ignored. Similarly, the cross-correlation function between nx(t) and ny(t) is
evaluated as
E [nx(t)ny(t + τ)] = 2
 ∞
−∞
 ∞
−∞
cos(2πf0α) sin(2πf0β)w(t −α)w(t + τ −β)
×E [nw(α)nw(β)] dαdβ
= N0
2
 ∞
−∞
sin(4πf0α)w(t −α)w(t + τ −α) dα = 0.
(2.40)
This system can be modeled as two parallel channels aﬀected by two independent Gaussian
noise processes n(x)
w (t) and n(y)
w (t) as illustrated in Figure 2.9.
+
+
+
Root-Nyquist
P(f)
Root-Nyquist
P ∗(f)
cr
Lowpass
Filter
Lowpass
Filter
Lowpass
Filter
Lowpass
Filter
x(t)
y(t)
n(y)
w (t)
n(x)
w (t)
y(t) + ny(t)
x(t) + nx(t)
n(x)
w (t) + jn(y)
w (t)
Figure 2.9: Modeling of a DSB-SC systems in additive white Gaussian noise by two
independent AWGN channels. These two channels can be represented as one complex
channel model, shown in the lower part of the ﬁgure, and referred to as the equivalent
complex baseband model for DSB-SC modulation.
Equivalently we can use a complex model with complex noise n(t) = nx(t) + jny(t),
whose correlation is given by
E [n(t)n∗(t)] = 2N0W sin(2πWτ)
2πWτ
,
(2.41)

2.6. THE COMPLEX EQUIVALENT BASEBAND MODEL
43
which is a shorthand version of (2.39) and (2.40). We thus have arrived at the complex
equivalent baseband model also shown in Figure 2.9, which takes at its input complex
numbers cr, passes them through complex (dual) modulator and demodulator ﬁlters, and
feeds the sampled values yr into a complex receiver. Note that as long as we adopt the
convention always to use a receiver ﬁlter (P(f) in Figure 2.9), we may omit the low-pass
ﬁlter in the model, since it can be subsumed into the receiver ﬁlter, and the noise source
can be made an ideal white Gaussian noise source (nx(t) = n(x)
w (t), ny(t) = n(x)
w (t)) with
correlation function
E [n(t)n∗(t)] = N0δ(t).
(2.42)
If Nyquist pulses are used with independent complex data symbols cr, each sam-
ple is in fact an independent, 2-dimensional signal constellation, hence the ubiquity of
2-dimensional signal constellations.
Figure 2.10 shows some of the more popular 2-
dimensional signal constellations. The signal points correspond to sets of possible complex
values which cr can assume.
We now assume for the remainder of this book that all signal constellations are normal-
ized so that their average energy is unity. This requires that the complex signal point cr
needs to be multiplied with the ampliﬁcation factor √Es in order to generate the average
signal energy Es.
8-AMPM
16-QAM
64-QAM
BPSK
QPSK
8-PSK
Figure 2.10: Popular 2-dimensional signal constellations.

44
CHAPTER 2. COMMUNICATIONS BASICS
2.7
Spectral Behavior
Since bandwidth has become an increasingly treasured resource, an important parameter
of how eﬃciently a system uses its allotted bandwidth is the bandwidth eﬃciency η, deﬁned
in (1.2) as
η =
Bit Rate
Channel Bandwidth W [bits/s/Hz].
(2.43)
If raised-cosine Nyquist signaling is used with a roll-oﬀfactor of 0.3, BPSK achieves a
bandwidth eﬃciency of η = 0.884 bits/s/Hz at an Eb/N0 of 8.4 dB as shown in Figure 1.8.
The bandwidth eﬃciency of QPSK is twice that of BPSK for the same of Eb/N0, because
QPSK uses the complex dimension of the signal space. Also, 8-PSK is less power eﬃcient
than 8-AMPM (also called 8-cross) due to the equal energy constraint of the diﬀerent
signals, and the resulting smaller Euclidean distances between signal points. It can be
noted from Figure 1.8 how bandwidth can be traded for power eﬃciency and vice versa,
even without applying any coding. All performance points for uncoded signaling lie on a
line parallel to the Shannon bound.
In order to evaluate the spectral eﬃciency in (2.43), we must ﬁrst ﬁnd the power
spectrum of the complex pulse train
s(t −δ) =
l

r=−l
crp(t −rT −δ),
(2.44)
where we have introduced a random delay δ ∈[0, T[, and we further assume that the
distribution of δ is uniform. This will simplify our mathematics and has no inﬂuence on
the power spectrum, since, surely, knowledge of the delay of s(t) can have no inﬂuence on
its spectral power distribution.
The advantage of (2.44) is that, if we let l →∞, s(t −δ) can be made stationary.
Starting with the autocorrelation function
Rs(t, t + τ) = E [s∗(t)s(t + τ)] = E [s∗(t −δ)s(t + τ −δ)]
=
∞

p=−∞
∞

q=−∞
E

c∗
pcq

Eδ[p(t−pT −δ)p(t+τ −qT −δ)] ,
(2.45)
let us assume that the discrete autocorrelation of the symbols cr is stationary, i.e.,
Rcc(r) = E

c∗
qcq+r

(2.46)

2.7. SPECTRAL BEHAVIOR
45
depends only on r. This lets us rewrite (2.45) as
Rs(t, t + τ) =
∞

r=−∞
Rcc(r)
∞

q=−∞
Eδ [p(t −qT −δ)p(t + τ −(q + r)T −δ)]
=
∞

r=−∞
Rcc(r)
∞

q=−∞
1
T
 T
0
p(t−qT −δ)p(t+τ −(q+r)T −δ) dδ
=
∞

r=−∞
Rcc(r) 1
T
 T
0
p(δ)p(δ+τ −rT) dδ,
(2.47)
where the last term,
 T
0 p(δ)p(δ + τ −rT) dδ = Rpp(τ −rT), is the pulse autocorrelation
function of p(t), which depends only on τ and r, making Rs(t, t + τ) = Rs(τ) stationary.
In order to ﬁnd the power spectral density of s(t), we now merely need the Fourier
transform of Rs(τ), i.e.,
Φcc(f)
=
1
T
 ∞
−∞
∞

r=−∞
Rcc(r)Rpp(τ −rT)e−j2πfτdτ
=
1
T
∞

r=−∞
Rcc(r)e−j2πfrT
 ∞
−∞
Rpp(τ)e−j2πfτdτ
=
1
T C(f)G(f),
where
C(f) =
∞

r=−∞
Rcc(r)e−j2πfrT
(2.48)
is the spectrum-shaping component resulting from the correlation of the complex symbol
sequences cr, and G(f) = |P(f)|2 is the energy spectrum of the symbol pulse p(t) (page 37).
While the spectral factor due to the correlation of the pulses, C(f), can be used to
help shape the signal spectrum as in partial-response signaling ([16], pp. 548 ﬀ) or correl-
ative coding [18], Rcc(r) is often an impulse δ0r, corresponding to choosing uncorrelated,
zero-mean-valued symbols cr, and C(f) is ﬂat. If this is the case, the spectrum of the
transmitted signal is exclusively shaped by the symbol pulse p(t). This holds for example
for QPSK, if the symbols are chosen independently and with equal probability. In gen-
eral it holds for any constellation and symbol probabilities whose discrete autocorrelation
Rcc(r) = δ0r, and whose symbol mean2 E[cq] = 0. Constellations for which this is achieved
by the uniform probability distribution for the symbols are called symmetric constellations
2If the mean of the symbols cr is not equal to zero, discrete frequency components appear at multiples
of 1/T (see [16], Section 3.4).

46
CHAPTER 2. COMMUNICATIONS BASICS
(e.g., M-PSK, 16-QAM,64-QAM, etc.). We will see in Chapter 3 that the uncorrelated
nature of the complex symbols cr is the basis for the fact that TCM does not shape the
signal spectrum (also discussed in [2]).
2.8
Advanced Modulation Methods
2.8.1
OFDM
Orthogonal Frequency-Division Multiplexing (OFDM) is a modulation method where a po-
tentially large number of narrowband carrier signals are aggregated into a high-dimensional
modulation format. It is based on the observation that two frequency bursts
s1(t) = cos 2πf1t and s2(t) = cos 2π(f1 + Δf)t,
0 ≤t ≤T
(2.49)
are orthogonal if Δf = 1/T. OFDM is now simply the aggregation of N pairs of such
cosine and sine frequency burst, spaced at frequencies fi = f0 + i/T. While the system
with two signals may not be spectrally eﬃcient, if we aggregate a large number of carriers,
spectral eﬃciencies that exceed those achievable even with narrow Nyquist signaling are
possible. An illustration of OFDM is shown in Figure 2.11.
Subchannel
Mapping
Subchannel
DeMapping
Modulation
Parallel Channels in Frequency
Demodulation
Figure 2.11: Principle of orthogonal frequency-division multiplexing (OFDM).
The spectrum of OFDM is made up of sin(x)/x carriers that overlap in frequency, but
are orthogonal. The spectral eﬃciency of OFDM is given by
ηOFDM ≈N + 10
NΔf
× 2
T ≈2
[symbols/s/Hz],
that is, very close to ideal Nyquist signaling. A number of modern telecommunications
standards have adopted OFDM as the modulation format of choice. Apart from eﬃcient

2.8. ADVANCED MODULATION METHODS
47
spectrum utilization, OFDM oﬀers unprecedented ﬂexibility in the assignment of frequen-
cies and ﬁne tuning of data rates.
In practice, the generation of OFDM via discrete multiple carriers is very complex and
ineﬃcient, and a computationally much more eﬃcient way to generate the OFDM signal
is via the discrete Fourier transform (DFT) using the inverse DFT to transmit, along with
the DFT at the receiver. The inverse DFT computes the complex time samples sk in
s = [s0, . . . , sN−1], as
sk =
1
√
N
N−1

n=0
cne−j2πnk/N,
∀k : 0 ≤k < N
each time sample is then transmitted using a root-Nyquist transmission system as shown
in Figure 2.12, and the original symbols cn can be recovered with the DFT as
cn
=
1
√
N
N−1

k=0
skej2πkn/N,
∀n : 0 ≤n < N
=
1
N
N−1

k=0
N−1

n′=0
cn′ej2πk(n−n′)/N
=
1
N
N−1

n′=0
cn′
N−1

k=0
ej2πk(n−n′)/N



=0 for n=n′
= cn; for n = n
The transmitted symbols are thus recovered in an ideal system, and since the DFT can be
computed as a fast-fourier transform with a complexity of order O(N log(N)), substantial
complexity can be saved in modulating and demodulating.
Subchannel
Mapping
Subchannel
DeMapping
Waveform Channel
Time Samples 
in-phase
quadrature
p(t)
p(t)
p(−t)
p(−t)
DFT
Serial-Parallel
Parallel-Serial
iDFT
Figure 2.12: Implementation of an OFDM system using the discrete Fourier transform.

48
CHAPTER 2. COMMUNICATIONS BASICS
2.8.2
Multiple Antenna Channels (MIMO Channels)
As we have seen in Chapter 1, there exists a sharp limit on the achievable data rate on a
given channel, the channel capacity, and much of this book deals with coding techniques
to approach this limit. Further increases in the data carrying capacity of a channel can
only be achieved by modifying the channel itself. One method is to use many parallel
channels, rather than a single channel.
For wireless systems this is achieved by using
an array of transmit and an array of receive antennas as illustrated in Figure 2.13, which
illustrates a MIMO system for a wireless router application. This methodology has rapidly
been integrated into some modern high-capacity wireless systems, such as the IEEE WiFi
standards, and the next generation cellular systems LTE [30, 31], which, incidentally, also
use OFDM as the modulation method.
Received 
Baseband Signals
Baseband Signals
Parallel RF Stages
Parallel RF Stages
Figure 2.13: Multiple antenna communications system.
Each transmit antenna is operated by a separate DSB-SC modulated signal, and there-
fore each channel from transmit antenna i to receive antenna j is described by a complex
path gain hij. This constitutes the most simple model which does not suﬀer from inter-
symbol interference, given that the symbol rate is slower than the channel dispersion. The
composite channel can be described by the complex vector equation
yr = Hcr + nr,
(2.50)
where y = (y1r, . . . , yNrr) is the Nr-dimensional received complex vector at time r, cr =
(c1r, . . . , cNtr) is the Nt-dimensional vector of complex transmit symbols, H is the Nr ×
Nt matrix of complex path gains, and n is a Nr-dimensional noise vector.
Note that
each receive antenna is demodulated independently, hence there are Nr independent noise
sources. The mathematical model of this channel is illustrated in Figure 2.14.

2.8. ADVANCED MODULATION METHODS
49
+
+
+
cNtr
c1r
c2r
h11
hNtNr
hij
yNrr
y2r
y1r
nNrr
n2r
n1r
Transmit
Antenna
Array
Receive Array
Figure 2.14: Multiple-input multiple-output (MIMO) channel for increased capacity in
modern wireless systems.
The Shannon capacity of this channel can be calculated by applying the multi-dimensional
capacity formula [4, 6] and is given by
C = log2

det

INr +
Es
N0NNt
HH+

; [bits/channel use],
(2.51)
which is a generalization of the capacity formula (1.11) discussed in Chapter 1. Es is
the energy of the entire 2Nt-dimensional space-time symbol, which is spread over the Nt
transmit antennas. Reducing (2.51) to a single (real) dimension leads back to (1.11).
We now apply the singular-value decomposition (SVD) to the channel matrix H to
obtain a decomposition of the channel equation. The SVD [10] of this matrix is given by
H = UDV+,
(2.52)
where both U and V are unitary matrices of size Nr × Nr and Nt × Nt, and therefore
invertible, i.e., U−1 = U+. The beauty of the SVD is that the matrix D is diagonal and
contains the singular values d1 ≥· · · ≥dn ≥0; n = min(Nt, Nr) of H, which are the
square roots of the non-zero eigenvalues of HH+. (If H is a square and hermitian matrix,
the singular values are the eigenvalues.)
Simple manipulations now lead to the following decomposed equivalent channel form:
y
=
Hc + n
=
UDV+c + n,
˜y = U+y
=
D˜c + ˜n.
(2.53)
Due to the unitary nature of U, n and ˜n have the same statistics.

50
CHAPTER 2. COMMUNICATIONS BASICS
Equation (2.53) is now simply an aggregate of min(Nr, Nt) parallel channels as illus-
trated in Figure 2.15. Each channel has power dj, corresponding to the jth singular value
of H. That is, simple linear pre-, and postprocessing by multiplying the transmitted and
received symbol vectors by unitary matrices decomposes the channel into parallel chan-
nels. Alas, this is only possible if the channel is known at the transmitter. Without that
knowledge, eﬃcient transmission over the MIMO channel is much more complicated.
Compensating for the variable gain of these parallel channels as shown by the dashed
box in Figure 2.15 has no eﬀect on capacity and turns the channels into n parallel channels
aﬀected by additive noise sources with variances N0/d2
1, · · · , N0/d2
n.
Then the capacity of this set of parallel channels is given by the following [8, Section
7.5, pp. 343 ﬀ]
Theorem 2.1 The information theoretic capacity of n parallel additive white Gaussian
noise channels with noise variances N0/d2
1, . . . , N0/d2
n is given by
C =
n

j=1
log

1 +
d2
jEn
N0

=
n

j=1
log

d2
jμ
N0

(2.54)
and is achieved by the energy allocation
2σ2
n
d2n
+ En
=
μ,
σ2
n < μ
En
=
0,
σ2
n ≥μ.
(2.55)
This theorem is known as the Waterﬁlling Theorem. It says that the available energy
should be distributed such that low-noise channels receive more energy than high noise
+
+
+
+
+
+
n = min(Nt, Nr)
˜cnr
˜c1r
d1
dn
1/d1
1/dn
n1
nn
˜ynr
˜y1r
ˆynr
ˆy1r
Figure 2.15: Decomposition of a MIMO channel into parallel channels via the SVD.

2.8. ADVANCED MODULATION METHODS
51
channels. It can be visualized in Figure 2.16, where power levels are shown in black, by
thinking of the total available power as liquid which is poured into connected containers
whose base level is at the height of the noise power–think of the noise as sediment.
Power level: Diﬀerence μ −σ2
j
μ
Channels 1 through n
level: σ2
n
Figure 2.16: Illustration of the water-ﬁlling capacity theorem.
Proof: Consider
I(x; ˆy)
(1)
≤
n

j=1
I(xj; ˆyj)
(1) independent xn
(2.56)
(2)
≤

n=1
1
2 log

1 + En
σ2n




f(E)
(2) Gaussian xn
(2.57)
Since equality can be achieved in both inequalities above, the next step is to ﬁnd the
maximizing energy distribution E = [E1, . . . , EN]. We identify the following suﬃcient
and necessary conditions via constrained maximization using a Lagrange multiplier:
∂f(E)
∂En
≤
λ,
1
2(σ2n + En)
≤
λ,
σ2
n + En
≤
1
2λ = μ.
(2.58)
A more detailed version of this proof is given in [8].
Q.E.D.

52
CHAPTER 2. COMMUNICATIONS BASICS
The capacity of MIMO wireless channels has been studied in a number of papers
[6, 24, 14] and analyzed as a function of the transmission environment [26, 9, 12]. Under
favorable conditions it is possible that this channel increases the Shannon capacity by a
factor of n.
Figure 2.17 shows cumulative probability distribution of the channel capacity for indoor
measurements [11] for a 4 × 4 antenna system using quarter-length whip antennas over
a ground plane. The measurements are compared to the theoretical capacity computed
above assuming independent Rayleigh channels between antenna pairs.
While a large
number of factors impact the capacity, the measurements show that antenna spacing,
among other aspects, has a strong eﬀect. The capacity of this indoor channel in the 900-
MHz ISM band is about 3.5 times that of a single antenna channel, verifying the claims
made by theory. The results in Figure 2.17 were with an FPGA-based MIMO testbed
using orthogonal Walsh sequences to measure channel gains.
The measurements were
carried out in an indoor oﬃce building environment [12, 11].
10
12
14
16
18
20
22
24
26
28
30
0
0.2
0.4
0.6
0.8
1
Simulated
Gaussian MIMO
Antenna separation: λ/8
Antenna separation: λ/4
Antenna separation: λ/2
Signal-to-noise ratio in dB
Figure 2.17: Measured veriﬁcation of the capacity gain of MIMO systems for a 4 × 4
antenna array system using quarter-length whip antennas.
Communications over MIMO channels typically takes two approaches, space-time cod-
ing [1, 22, 23, 13] or space-time layering [6, 7]. In space-time coding the transmitted signal
is coded over space (antennas) and time to achieve maximal diversity gain. In space-time

2.9. A COMMUNICATIONS SYSTEM CASE STUDY
53
layering the signal is separated into decomposed channels by linear and/or non-linear pro-
cessing techniques. We will discuss applications of turbo coding principle to space-time
coding for MIMO channels in Chapter 8.
2.9
A Communications System Case Study
While we generally focus our discussion on error control coding only in this book, it is
important to realize that in the vast majority of applications, error control is only one
important component of a complete system. Many other pieces have to ﬁt together to
enable a physical system to transmit digital data from a transmitter to a receiver. In this
sense, the abstraction of a communications system in Figure 2.1 is just that, a maximally
reduced model used for the purpose of understanding fundamental concepts. In reality, a
communications system will look more like the one shown in Figure 2.18.
Framer
(descramb)
Framer
(scramb)
LDPC
(2048, 1723)
Encoder
DSQ 128
Mapper
DAC
Echo Canc.
3x NEXT Canc.
ADC
VGA
800 Mbaud
PAM - 16
Symbols
800 Msps
Spectrally
Flat
Transmit
Filter
(fixed)
Thomlinson-
Harashima
(THP) Precoder
LDPC
(2048, 1723)
Decoder
sync
hard
soft
T
R
HYBRID
T
R
HYBRID
T
R
HYBRID
Receive
Filter
(fixed)
+
+
Matrix
FFE
Receive Side
Transmit Side
T
R
HYBRID
T
R
HYBRID
T
R
HYBRID
T
R
HYBRID
T
R
HYBRID
+
m(i)
s(i)(t)
r(t)
h(t)
ˆm
Figure 2.18: Block diagram of the IEEE 802.3an transceiver system for communications
over 4 twisted copper cable pairs, used primarily for cabling for computer systems and
data centers.
The dark shaded blocks are the encoder and decoder systems, here a low-density parity-
check code discussed in detail in Chapter 6.
Among the other systems we ﬁnd basic
memory and latches which order bits into transmission frames and insert control signals,
the digital mapper function, precoding used to counter channel intersymbol interference
distortion, various cancelation ﬁlters, and the analog signal modulation components.

54
CHAPTER 2. COMMUNICATIONS BASICS
We start with the signal constellation. In the IEEE 802.3an standard, a special con-
stellation is used, called the dithered square constellation (DSQ). It is in essence a basic
2-dimensional constellation as in Figure 2.10, where a small amount of signal space coding
is introduced by using only every second constellation point, i.e., only using point with an
even coordinate sum. Technically, the constellation is shaped from the rotated Z2 lattice
discussed in detail in Chapter 3.
The constellation is shown in Figure 2.19, which shows the 128-point constellation
used in the standard. The bit assignment follows a set partitioned approach as we will
discuss in detail in Chapter 3, and it is arranged into coded and uncoded bits analogous
to Figure 3.16, following a lattice partition approach also discussed in Chapter 3. Here
we simply wish to present the signal constellation as an applied example of a high-density
signal set.
                
                 
       
               
                
                 
0010 0110 1110 1010
0011 0111 1111 1011
0001 0101 1101 1001
0000 0100 1100 1000
000
000
110
110
011
010
001
111
100
100
100
101
(xxx : yyyy)
select
subset
select
signalpoint
(15,15)
(-15,-15)
Figure 2.19: 128 DSQ signal set used in the IEEE 802.3an cable modem standard. The
four signal point selection bits are encoded with the LDPC code, while the subset selection
bits are uncoded, as in Figure 3.16.
The next major processing block in the system is the Tomlinson-Harashima precoder
(THP). Before we summarize its functionality, we need to take a look at the physical chan-
nel that the twisted copper cables present to the transceiver. In Figure 2.1 we assumed
that the only relevant channel distortion was additive noise, actually additive white Gaus-
sian noise as discussed earlier in this chapter. In most practical situations, the channel ﬁrst

2.9. A COMMUNICATIONS SYSTEM CASE STUDY
55
has to be conditioned into a state where the model in Figure 2.1 applies. With channels
with an uneven frequency response this requires some form of channel equalization.
Let us start by looking at the frequency response of 100 m of category 6 (Cat 6)
Ethernet cabling, shown in Figure 2.20. As is typical with wireline channels, the frequency
response drops oﬀexponentially fast with frequency, and higher frequencies require more
eﬀort to be used. A similar situation presents itself for DSL and voiceband modems.
Attenuation in dB
IEEE Measured Data
Model
-70
-60
-50
-40
-30
-20
-10
0
0
100
200
300
400
500
600
700
800 MHz
Figure 2.20: Frequency response H(f) of 100 m of category 6 Ethernet cabling.
The channel, of course, is not used directly as analog transmission medium, but via
a discrete sampled channel as discussed in Figure 2.7, where, under ideal conditions, a
signal sample (or constellation point) ar appears at the receiver distorted only by the
addition of Gaussian noise. This is true only for frequency-ﬂat transmission channels, and
a discrete impulse over the channel in Figure 2.20 with a sampling rate of 800 Msamples/s
will appear as the sample sequence shown in Figure 2.21.
The capacity of such an intersymbol interference channel can be computed as an ex-
tension of the capacity formula for the white Gaussian noise channel, which we introduced
in (1.1). If the spectrum of the noise is not ﬂat, we need to compute the capacity of the

56
CHAPTER 2. COMMUNICATIONS BASICS
Tap Amplitude
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0
5
10
15
20
25
30
Tap #
Figure 2.21: Discrete impulse response of the channel in Figure 2.20 received with a low-
pass ﬁlter with a 3-dB bandwidth of 400 MHz, and sampled at 800 Msamples/s.
channel by adding over small frequency intervals, arriving at the integral equation
C =
 W
0
log

1 + S(f)|H(f)|2
N(f)

df
(2.59)
where S(f) is the signal power spectral density, and N(f) is the noise power spectral
density, and H(f) is the channel frequency response. If we let N(f) = σ2 and S(f) = S/W,
it is easy to see that the original Shannon capacity is obtained again.
In order to evaluate (2.59) we need to optimize over the diﬀerent signal power distri-
butions which have the same energy, i.e., those with
S =
 W
0
S(f) df
constant. This is accomplished by the so-called water-ﬁlling theorem, that is, the transmit
power is distributed such that frequencies with a larger noise get less power allocated in a
process identical to the one discussed in Section 2.8.2. We obtain
S(f) =

E −
N(f)
|H(f)|2
if S(f) ≤0,
0
otherwise.
(2.60)

2.9. A COMMUNICATIONS SYSTEM CASE STUDY
57
However, in practice the transmit power spectral density S(f) is often approximately
ﬂat. This leads to a small, but insigniﬁcant, loss at the signal-to-noise power ratios of
interest. Figure 2.22 below illustrates the per sample capacities of the IEEE 802.3an cable
channel with frequency response from Figure 2.20. The following two cases are shown: (i)
Formula (2.59) and (ii) the capacity of the channel which results from THP precoding,
which suﬀers a loss of 13.26 dB w.r.t. the ideal ﬂat channel with the same overall power
S. Spectrally modulating the transmit power only has an eﬀect at signiﬁcantly lower SNR
than those of the IEEE 802.3an operating point.
Ideal Gaussian
Channel
    802.3 ISI 
Channel         
                     
   THP equalized 
Channel                     
                     
802.3 Standard
operating point              
                     
with
water-filling
THP loss: 13.26 dB
3.9 dB
7.4 dB
4
5
6
7
8
9
10
20
22
24
26
28
30
32
34
36
38
40 dB
bits per sample
Figure 2.22: Various information theoretical capacities for the cable modem channel of
Figure 2.20 as a function of the SNR S/N.
Also shown is the IEEE 802.3 Standard operating point at 23 dB slicer SNR (cor-
responding to a symbol-to-noise SNR of 37 dB. There is a gap of about 3.9 dB to the
THP capacity, as well as a 7.4 dB gap to the ISI channel capacity. About 2 dB of that
is due to the code loss, a combination of small size and the high rate. However, what is
also apparent is that investing more complex signal processing than THP is probably not
worth the eﬀort given the small additional possible gain.

58
CHAPTER 2. COMMUNICATIONS BASICS
The principle of THP is quite simple and is illustrated in Figure 2.23. The main idea
is to subtract the echoes that the channel is introducing via its impulse response (Figure
2.21) from the transmitted signal, such that the resulting signal at the receiver is free of
interference.
Feed-back
    Filter
  (causal)
+
+
  Modulo 
Operation
u(D)
x(D)
h0
h0
h1
h0
h2
h0
h3
h0
h4
h0
Figure 2.23: Principle of Tomlinson-Harashima precoding.
More precisely, if we denote the impulse response by h(D) = h0 + h1D + · · · + hLDL,
where D is the discrete delay operator, then the THP transmitter ﬁrst forms
x(D) = u(D)
h(D),
(2.61)
where u(D) is the sequence of input symbols ui, which are drawn from the discrete constel-
lation, here the one in Figure 2.19. The inverse above is realized by the feedback circuit
in Figure 2.23. If the signal x(D) is now transmitted through a channel with impulse
response h(D), the output of the channel is given by y(D) = x(D) + n(D), i.e., distortion
free.
The problem is that inverse ﬁltering can generate possibly large amplitudes and the
ﬁlter can become unstable. This is undesirable from circuit point of view. Consequently,
Tomlinson [25] introduced a modulo reduction operation, where the amplitude of x(D) is
folded back into the original range of the signal constellation. The eﬀect of this operation
at the receiver is that the received signal is given by
yi = xi + ni + kiA,
(2.62)
where ki is an integer and A is the range of the modulation, in our case A = 30 from Figure
2.19. In principle, THP now operates exactly like ideal decision-feedback equalization and

2.9. A COMMUNICATIONS SYSTEM CASE STUDY
59
we obtain a channel where additive noise is the only distortion. THP was introduced by
M. Tomlinson in 1971 [25].
As a ﬁnal point, we want to mention that the Ethernet cabling, carrying 4 twisted
strands of copper wire, also causes another kind of interference which needs to be con-
trolled.
These types of interference are the near-end crosstalk (NEXT), the far-end
crosstalk (FEXT), and channel echo, caused by transmitted signals being reﬂected along
the wires and traveling backward on the cable reach the receiver unintentionally. These
eﬀects are illustrated in Figure 2.24.
T
T
T
R
T
R
T
R
T
R
T
R
T
R
R
R
NEXT 14
NEXT 12
NEXT 13
FEXT 14
FEXT 13
FE XT 12
Fa r Echo
Near
Echo
Alien Cross talk,
EMI
HYBRID
HYBRID
HYBRID
HYBRID
HYBRID
HYBRID
HYBRID
HYBRID
Figure 2.24: Signal distortion eﬀects of the Ethernet cabling system, speciﬁcally the near-
and far-end crosstalks, and the line echo distortions.
Due to the high frequencies used in the IEEE 802.3an signaling system, NEXT, FEXT,
and echo distortions are very signiﬁcant, and need to be addressed before demodulation
and decoding can take place.
The relative interference levels of these distortions are

60
CHAPTER 2. COMMUNICATIONS BASICS
illustrated in Figure 2.25, and compared with the useful signal levels that is received at
the end of 100 m of cabling. Comparing with Figure 2.20, we see that the interference
levels can be as much as 40 dB above the levels of the target signal.
-
Interference spectral density in dB
Echo
NEXT
FEXT
-80
-70
-60
-50
-40
-30
-20
-10
0
0
100
200
300
400
500
600
700
800 MHz
Figure 2.25: Measured interference levels of NEXT, FEXT, and echo compared to useful
signal levels, which can be seen in Figure 2.20.
At the ﬁlter cutoﬀfrequency at 400 MHz, for example, the echo signal level is 35 dB
stronger then the target signal level. The NEXT levels also exceed the signal level, and the
FEXT levels are high enough such that their cancelation is necessary in order to operate
a high-density signal constellation, such as the DSQ constellation of IEEE 802.3an.
Going into further details is beyond the scope of this text, however, we wish to note
that all of these interference sources are tracked at the receiver via signal estimators,
usually employing least mean-squares (LMS) adaptive algorithms. The interfering signals
are then subtracted from the received signal to isolate the target signal. This can be done
since the original source of the interference, the transceiver’s own transmitted signal, is
available to the receiver side. As seen in Figure 2.18, these signals are fed through adaptive
ﬁlters to the receiver side, where they are used to cancel the eﬀects of NEXT, FEXT, and
the channel echos.

2.10. APPENDIX 2.A
61
As can be appreciated from Figure 2.24, this is fairly complex task since it requires 10
diﬀerent cancelation ﬁlters per lane, that is, 40 separate cancelation ﬁlters, which all need
to be trained and adapted continuously to ensure adequate cancelation. The fact that the
IEEE 802.3an transceiver does operate to speciﬁcation is a testimony to the high degree
of maturity of such cancelation methods. It is therefore not surprising that nearly 50%
of the hardware eﬀorts lies in the various cancelation ﬁlters, which, although theoretically
quite straightforward, constitute nontheless a major engineering triumph.
The details of the coding system, bit assignment, and modulation using set partitioning
principles will be discussed in detail in Section 3.7.
2.10
Appendix 2.A
The random waveform n(t) which is added to the transmitted signal in the channel of
Figure 2.1 can be modeled as a Gaussian random process in many important cases. Let us
start by approximating this noise waveform by a sequence of random rectangular pulses
w(t) of duration Ts, i.e.,
n(t) =
∞

i=−∞
niw(t −iTs −δ),
(2.63)
where the weighing coeﬃcients ni are independent Gaussian random variables, Ts is the
chosen discrete time increment and δ is some random delay, uniformly distributed in
[0, Ts[. The random delay δ is needed to make (2.63) stationary. A sample function of
n(t) is illustrated in Figure 2.26.
t
n(t)
Figure 2.26:
Sample function of the discrete approximation to the additive channel
noise n(t).

62
CHAPTER 2. COMMUNICATIONS BASICS
A stationary Gaussian process is completely determined by its mean and correlation
function (see, e.g., [5, 19]). It is also straightforward to see that n(t) as deﬁned in (2.63)
is stationary. The mean of the noise process is assumed to be zero, and the correlation
function of n(t) is then calculated as
R(τ)
=
E [n(t)n(t + τ)]
=
E
⎡
⎣
∞

i=−∞
∞

j=−∞
ninjw(t −iTs −δ)w(t + τ −jTs −δ)
⎤
⎦
=
σ2Eδ

∞

i=−∞
w(t −iTs −δ)w(t + τ −jTs −δ)

=
σ2ΔTs(τ),
(2.64)
where σ2 = E

n2
i

is the variance of the discrete noise samples ni and the expectation is
now only over the variable delay δ. This expectation is easily evaluated and ΔTs(τ) is a
triangular function as shown in Figure 2.27 below.
fourier transform
σ2/Ts
1
Ts
f
−1
Ts
−3
Ts
−5
Ts
1
Ts
3
Ts
5
Ts
τ
−Ts
Ts
R(τ)
N(f)
Figure 2.27: Correlation function R(τ) and power spectral density N(f) of the discrete
approximation to the noise function n(t).
We note that the power of the noise function, which is given by P = R(τ = 0) = 1/Ts,
increases linearly with the inverse of the sample time Ts. To see why that is a reasonable
eﬀect, let us consider the power spectral density N(f) of the random process n(t), given
by the Fourier transform of the correlation function R(τ) as
N(f) =
 ∞
−∞
R(τ)e−2πjτfdτ,
(2.65)

2.10. APPENDIX 2.A
63
where f is the frequency variable. The power density spectrum N(f) is also shown in
Figure 2.27. Note that as Ts →0 in the limit, N(f) →σ2. In the limit approximation,
(2.63) therefore models noise with an even distribution of power over all frequencies. Such
noise is called white noise in analogy to the even frequency content of white light, and we
will denote it by nw(t) henceforth. In the literature the constant σ2 = N0/2, and N0 is
known as the one-sided noise power spectral density [28].
Naturally, the model makes no physical sense in the limit, since it would imply inﬁnite
noise power. But we will be careful not to use the white noise nw(t) without some form
of low-pass ﬁltering. If nw(t) is ﬁltered, the high noise frequencies are rejected in the
stop-bands of the ﬁlter, and for the ﬁlter output it is irrelevant how we model the noise
in its stop-bands.
As n(t) →nw(t), the correlation function R(τ) will degenerate into a pulse of width
zero and inﬁnite height as Ts →0, i.e., R(τ) →σ2δ(τ), where δ(τ) is known as Dirac’s
impulse function. δ(τ) is technically not a function but a distribution. We will only need
the sifting property of δ(t), i.e.,
 ∞
−∞
δ(t −α)f(t) dt = f(α),
(2.66)
where f(t) is an arbitrary function, which is continuous at t = α. Property (2.66) is easily
proven by using (2.64) and carrying out the limit operation in the integral (2.66). In fact,
the relation
 b
a
δ(τ −α)f(τ) dτ = lim
Ts→0
 b
a
ΔTs(τ −α)f(τ) dτ = f(α),
(2.67)
for any α in the interval (a, b), can be used as a proper deﬁnition of the impulse function.
If the limit in (2.67) could be taken inside the integral
δ(τ −α) = lim
Ts→0 ΔTs(τ −α).
(2.68)
However, any function which is zero everywhere except at one point equals zero when
integrated in the Riemann sense, and hence Equation (2.68) is a symbolic equality only,
to be understood in the sense of (2.67). An introductory discussion of distributions can
be found, for example, in [15], pp. 269–282.

64
CHAPTER 2. COMMUNICATIONS BASICS

Bibliography
[1] S. Alamouti, “A simple transmit diversity technique for wireless communications,”
IEEE J. Sel. Areas Commun., vol. 16, no. 8, pp. 1451–1458, Oct. 1998.
[2] E. Biglieri, “Ungerboeck codes do not shape the signal power spectrum,” IEEE Trans.
Inform. Theory, vol. IT-32, no. 4, pp. 595–596, July 1986.
[3] R.E. Blahut, Digital Transmission of Information, Addison-Wesley, New York, 1990.
[4] T.M. Cover and J.A. Thomas, Elements of Information Theory, John Wiley & Sons,
New York, 1991.
[5] W. Feller, An Introduction to Probability Theory and Its Applications, vol. 1 and 2,
revised printing of the third edition, John Wiley & Sons, New-York, 1970.
[6] G.J. Foschini, “Layered space-time architecture for wireless communication in a fading
environment when using multi-element antennas,” Bell Labs Tech. J., vol. 1, no. 2,
pp. 41–59, Aug. 1996.
[7] G.J. Foschini and M.J. Gans, “On limits of wireless communication in a fading envi-
ronment when using multiple antennas,” Wireless Personal Commun., vol. 6, no. 3,
pp. 311–355, Mar. 1998.
[8] R.G. Gallager, Information Theory and Reliable Communication, John Wiley & Sons,
New York, 1968.
[9] D. Gesbert, H. B¨olcskei, D.A. Gore, and A.J. Paulraj, “Outdoor MIMO wireless chan-
nels: Models and performance prediction,” IEEE Trans. Commun., vol. 50, no. 12,
Dec. 2002.
[10] G.H. Golub and C.F. Van Loan, Matrix Computations, third edition, John Hopkins
University Press, Baltimore, 1996.
[11] P. Goud, R. Hang, D. Truhachev, and C. Schlegel, “A portable MIMO testbed and
selected channel measurements,” EURASIP J. Applied Sig. Proc., Special Issue on
Implementation Aspects and Testbeds for MIMO Systems, vol. 2006.
[12] P. Goud Jr., C. Schlegel, W.A. Krzymien, R. Hang, “Multiple antenna communica-
tion systems–an emerging technology,” Can. J. Electr. Comput. Eng., Special Issue
Advances in Wireless Commun. and Networking, vol. 29, no. 1/2, Jan./Apr. 2004,
pp. 51–59.
65

66
BIBLIOGRAPHY
[13] B.L. Hughes, “Diﬀerential space-time modulation,” IEEE Trans. Inform. Theory, vol.
46, no. 7, pp. 2567–2578, Nov. 2000.
[14] T.L. Marzetta and B.M. Hochwald, “Capacity of a mobile multiple-antenna commu-
nication link in Rayleigh ﬂat-fading,” IEEE Trans. Inform. Theory, vol. 45, no. 1,
pp. 139–157, Jan. 1999.
[15] A. Papoulis, The Fourier Integral and its Applications, McGraw-Hill, New York, 1962.
[16] J.G. Proakis, Digital Communications, McGraw-Hill, New York, 5th edition, 2007.
[17] S. Ramseier and C. Schlegel, “On the bandwidth/power tradeoﬀof trellis coded mod-
ulation schemes,” Proc. IEEE Globecom’93, (1993).
[18] S. Ramseier, “Bandwidth-eﬃcient correlative trellis coded modulation schemes,”
IEEE Trans. Commun., vol. COM-42, no. 2/3/4, pp. 1595–1605, 1994.
[19] K.S. Shanmugan and A.M. Breipohl, Random Signals: Detection, Estimation and
Data Analysis, John Wiley & Sons, New York, 1988.
[20] C.E. Shannon, “A mathematical theory of communications,” Bell Syst. Tech. J., vol.
27, pp. 379-423, July 1948.
[21] B. Sklar, Digital Communications: Fundamentals and Applications, Prentice Hall,
Englewood Cliﬀs, NJ, 1988.
[22] V. Takokh, N. Seshadri, and A.R. Calderbank, “Space-time codes for high data rate
wireless communication: performance criterion and code construction,” IEEE Trans.
Inform. Theory, vol. 44, no. 2, pp. 744–765, March 1998.
[23] V. Takokh, H. Jafarkhani, and A.R. Calderbank, “Space-time block codes from or-
thogonal designs,” IEEE Trans. Inform. Theory, vol. 45, no. 4, pp. 1121–1128, May
1999.
[24] E. Telatar, “Capacity of multi-antenna Gaussian channels,” Eur. Trans. Telecom-
mun., vol. 10, no. 6, Nov.–Dec. 1999.
[25] M. Tomlinson, “New automatic equaliser employing modulo arithmetic,” Electronics
Lett., vol. 7, no. 5/6, March 1971.
[26] A.L. Swindlehurst, G. German, J. Wallace, and M. Jensen, “Experimental measure-
ments of capacity for MIMO indoor wireless channels,” Proc. Third IEEE Signal
Processing Workshop, Taoyuan, Taiwan, March 20–25, 2001.
[27] A.J. Viterbi, “Error bounds for convolutional codes and an asymptotically optimum
decoding algorithm”, IEEE Trans. Inform. Theory, vol. IT-13, pp. 260-269, 1967.
[28] J.M. Wozencraft and I.M. Jacobs, Principles of Communication Engineering, John
Wiley & Sons, New York, 1965, reprinted by Waveland Press, 1993.
[29] R.L. Peterson, R.E. Ziemer, and D.E. Borth, Introduction to Spread-Spectrum Com-
munications, Prentic Hall, Englewood Cliﬀs, NJ, 1995.
[30] 3GPP Group Website, Releases, http://www.3gpp.org/releases
[31] 3GPP2 Group Website, http://www.3gpp2.org

Chapter 3
Trellis-Coded Modulation
3.1
An Introductory Example
As we have seen in Chapter 1, power and bandwidth are limited resources in modern com-
munications systems and their eﬃcient exploitation fundamentally involves an increase in
the complexity of a communication system. While there are strict limits on the power and
bandwidth resources, the complexity of systems has steadily increased to obtain eﬃciencies
ever closer to the these limits. One very successful method of reducing the power require-
ments without increase in the requirements on bandwidth was introduced by Gottfried
Ungerb¨ock [30, 31, 33, 34], subsequently termed Trellis-Coded Modulation (TCM). We
start this chapter with an illustrative example of this novel method of combining coding
and modulation.
Let us assume that we are using a standard QPSK modulation scheme which allows us
to transmit two information bits/symbol. We know from Chapter 2 that the most likely
error a decoder makes due to noise is to confuse two neighboring signals, say signal s(2)
and signal s(1) (compare Figure 2.2). This will happen with probability
Ps(1)→s(2) = Q
⎛
⎝

d2Es
2N0
⎞
⎠,
(3.1)
where d2 = 2 for a unit energy signal constellation, and Es is the average transmit energy
per symbol. Instead of using such a system to transmit one symbol at a time, the encoder
shown in Figure 3.1 is used. This encoder consists of two parts, the ﬁrst of which is a
Finite-State Machine (FSM) with a total of eight states, where state sr at time r is deﬁned
by the contents of the delay cells, or shift-registers, i.e., sr = (s(2)
r , s(1)
r , s(0)
r ). The second
part is called a signal mapper and its function is a memoryless mapping of the three bits
vr = (u(2)
r , u(1)
r , v(0)
r ) into one of the eight symbols of an 8-PSK signal set. The FSM
67
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

68
CHAPTER 3. TRELLIS-CODED MODULATION
accepts two input bits ur = (u(2)
r , u(1)
r ) at each symbol time r, and it transitions from a
state sr to one of 4 possible successor states sr+1. In this fashion the encoder generates a
sequence of output symbols x = (x0, x1, x2, . . .). Assuming that the encoder is operated
in a continuous fashion, there are four choices at each time r, which allows us to transmit
2 information bits/symbol, the same as with QPSK.
+
+
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
Finite-State Machine
Mapping Function τ
u(1)
r
u(2)
r
s(0)
r
s(1)
r
s(2)
r
v(0)
r
xr
Figure 3.1: Trellis encoder with an 8-state ﬁnite-state machine (FSM) driving a 3-bit to
8-PSK signal mapper.
A graphical interpretation of the operation of this FSM, and in fact the entire encoder,
will prove immensely useful. Since the FSM is time-invariant, it can be represented by
a state transition diagram as shown in Figure 3.2. The nodes in this transition diagram
are the states of the FSM, and the branches represent the possible transitions between
states. Each branch can now be labeled by the pair of input bits u = (u(2), u(1)) which
cause the transition, as well as by either the output triple v = (u(2), u(1), v(0)) or the
output signal x(v). (In Figure 3.2 we have used x(v), represented in decimal notation,
i.e., xdec(v) = u(2)22 + u(1)21 + v(0)20.)
If we index the state transition diagram by both the states and the time index r,
Figure 3.2 expands into the trellis diagram, or simply the trellis of Figure 3.3. It is the
two-dimensional representation of the operation of the encoder, capturing all possible state
transitions starting from an originating state (usually state 0), and terminating in a ﬁnal
state (usually also state 0). The trellis in Figure 3.3 is terminated to length L = 5. This
requires that the FSM be driven back into state 0 at time r = 5. As a consequence, the
branches at times r = 3 and 4 are predetermined, and no information is transmitted in
those two time units. Contrary to the short trellis in Figure 3.3, in practice, the length of
the trellis will be several hundred or thousands of time units, possibly even indeterminate,
corresponding to continuous operation.
When and where to terminate the trellis is a
matter of practical considerations, related to the block and frame sizes used.

3.1. AN INTRODUCTORY EXAMPLE
69
000  
010  
011  
001  
101  
110  
100  
111  
0
1
3
5
2
4
6
4
5
7
2
7
3
6
2
0
1
3
1
6
7
0
5
1
5
3
0
4 2
4
6
7
Figure 3.2: State transition diagram of the encoder from Figure 3.1. The labels on the
branches are the encoder output signals x(v), in decimal notation.
Now each path through the trellis corresponds to a unique message and is associated
with a unique message sequence (compare Section 2.5). The term trellis-coded modulation
originates from the fact that these encoded sequences consist of modulated symbols which
are associated with the branches of the trellis. Colloquially we say the symbols are attached
to the branch.
At ﬁrst sight it is not clear at all what has been gained by this complicated encoding
since the signals in the 8-PSK signal sets are much closer together than those in the QPSK
signal set and have a higher symbol error rate. The FSM, however, puts restrictions on the
symbols which can form any given valid sequence, and these restrictions can be exploited
by a smart decoder. In fact, what counts is the distance between signal sequences x, and
not the distance between individual signals. Let us then assume that such a decoder can
follow all possible sequences through the trellis, and it makes decisions between sequences.
This is illustrated in Figure 3.4 for two sequences x(e) (erroneous) and x(c) (correct).
These two sequences diﬀer in the three symbols shown. An optimal decoder will make
an error between these two sequences with probability Ps = Q

d2ecEs/2N0

, where
d2
ec = 4.586 = 2 + 0.56 + 2 is the Euclidean distance between x(e) and x(c), which,
incidentally, is much larger than the QPSK distance of d2 = 2. Going through all possible
sequence pairs x(e) and x(c), one ﬁnds that those highlighted in Figure 3.4 have the smallest

70
CHAPTER 3. TRELLIS-CODED MODULATION
0
4
2
6
0
4
2
3
7
0
5
1
5
3
7
6
State
s = 0
s = 1
s = 2
s = 3
s = 4
s = 5
s = 6
s = 7
Figure 3.3: Trellis diagram for the encoder in Figure 3.1. The code and the trellis are
terminated to length L = 5.
squared Euclidean distance, and hence the probability that the decoder makes an error
between those two sequences is the most likely error.
We now see that by virtue of carrying out sequence decoding, rather than symbol
decoding, the distances between alternatives can be increased, even though the signal
constellation used for sequence coding has a smaller minimum distance between signal
points. For this code we may decrease the symbol power by about 3.6 dB, i.e., we use less
than half the power needed for QPSK.
A more precise error analysis is not quite so simple since the possible error paths in
the trellis are highly correlated, which makes an exact analysis of the error probability
impossible for all but the most simple cases. In fact, a lot of work has gone into analyzing
the error behavior of trellis codes and we devote an large portion of Chapter 5 to this
topic. This simple example should convince us, however, that some real coding gain can
be achieved by this method with a relatively manageable eﬀort. Having said that, we must
stress that the lion share of the work is performed by the decoder, about which we will
have to say more later in Chapter 5
Figure 3.5 shows an early application of trellis coded modulation to improve the power
eﬃciency on an Intelsat data link, which used to operate uncoded QPSK signaling. The
performance of a 16-state 8-PSK TCM code used on a single channel per carrier (SCPC)
modem operating at 64 kbits/second [32] outperforms QPSK by over 5 dB. This early

3.2. CONSTRUCTION OF CODES
71
s = 0
s = 1
s = 2
s = 3
s = 4
s = 5
s = 6
s = 7
x(c) =
x(e) =
4
1
6
6
4
4
7
7
0
4
Figure 3.4: Section of the trellis of the encoder in Figure 3.1. The two solid lines depict
two possible paths with their associated signal sequences through this trellis. The numbers
on top are the signals transmitted if the encoder follows the upper path, and the numbers
at the bottom are those on the lower path.
experiment proved the viability of trellis coding for satellite channels. Interestingly, the
8-PSK TCM modem performance comes much closer to its theoretical performance limit
than does the original QPSK modem. This is due to the fact that system inaccuracies,
acting similar to noise, are handled much better by a system incorporating error control
coding.
3.2
Construction of Codes
To build a trellis code, we need an FSM to generate the trellis and a mapping of branches
into symbols to be transmitted. This is customarily done by a structure like the one in

72
CHAPTER 3. TRELLIS-CODED MODULATION
2
3
4
5
6
7
8
9
10
11
10−6
10−5
10−4
10−3
10−2
10−1
1
QPSK
Intelsat
QPSK
Modem
IBM/DVFLR
Intelsat
Experiment
TCM
16 states
ideal channel
Eb/N0[dB]
Bit Error Probability (BER)
Figure 3.5: Measured bit error probability of QPSK and a 16-state 8-PSK TCM modem
over a 64 kbit/s satellite channel [32].
Figure 3.6. The FSM part is identical to a traditional convolutional code,1 which has 2ν
states and two k bits/symbol. The mapping of trellis branches to symbols is accomplished
by the mapping function, which is an arbitrary memoryless function xr = τ(ur, sr) of the
current state of the FSM and the new input digits ur = (u(k)
r , . . . , u(1)
r ), but typically only
the output bits (ur, sr) = vr = (v(n)
r
, v(n−1)
r
, . . . , v(0)
r ) participate the mapping function
xr = τ(vr).
The second component of the encoder is the mapping function xr = τ(ur, sr), which
assigns a signal xr to each transition of the FSM. The mapping function of the trellis code
discussed in the beginning of this chapter maps three output bits from the FSM into one
8-PSK signal point as shown in Figure 3.7.
The important quantities of a signal set are the squared Euclidean distances between
pairs of signal points, since they determine the squared Euclidean distances between se-
quences, which are the important parameters of a trellis code.
A signal on a branch in the trellis xi = τ(vi) is generated by the mapping function τ,
1More precisely, the generating FSM is a convolutional encoder in the systematic feedback form (com-
pare Chapter 4).

3.2. CONSTRUCTION OF CODES
73
+
+
+
+
u(2)
r
u(1)
r
v(2)
r
v(1)
r
v(0)
r
h(2)
ν
h(2)
ν−1
h(2)
1
h(2)
0
h(1)
ν
h(1)
ν−1
h(1)
1
h(1)
0
h(0)
1
h(0)
ν−1
Mapping
Function
xr = τ(vr)
xr
Figure 3.6: Structure of a generic trellis encoder.
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
(00)
(01)
(10)
(11)
(011)
(111)
Figure 3.7: 8-PSK and QPSK signal sets with natural and Gray mapping.
given as input the branch label vi. In general, the distance between two branch signals
d2 = x1 −x22 is related to the branch labels in a complicated manner. However, if the
squared Euclidean distance between x1 and x2, given by
d2 = τ(v1) −τ(v2)2 ,
(3.2)
depends only on the diﬀerence between their branch labels, i.e.,
d2
=
τ(v1) −τ(v2)2
=

τ(v1 ⊕(−v2)



ev
) −τ(0, 0)

2
=
τ(ev) −τ(0, 0)2 ,
(3.3)
where the binary operator ⊕denotes the bit-wise EXOR operation, then the signal τ(0, 0)
can always be used as a reference signal. Signal mappings with this property are called

74
CHAPTER 3. TRELLIS-CODED MODULATION
regular. An example of a regular mapping is the QPSK signal set in Figure 3.7 used with
Gray mapping.
It is easy to see that a binary diﬀerence of (01) and (10) will always
produce a d2 = 2. The binary diﬀerence of (11) produces d2 = 4. The 8-PSK signal set
shown in Figure 3.7 uses a mapping known as natural mapping, where the signal points are
numbered sequentially in a counter clockwise fashion. This mapping is not regular, since
the binary diﬀerence v = (011) can produce the two distances d2 = 0.56 and d2 = 3.14.
It can be shown that no regular mapping exists for an 8-PSK signal set. However, clever
local averaging techniques can be used to arrive at error expression which still allow the
use of the all-zero sequence as reference sequence, see [27, Chapter 3].
Now, from Figure 3.4 we see that an error path diverges from the correct path at some
state and merges with the correct path again at a (possibly) diﬀerent state. The task of
designing a good trellis code crystallizes into designing a trellis code for which diﬀerent
symbol sequences are separated by large squared Euclidean distances. Of particular im-
portance is the minimum squared Euclidean, deﬁned as d2
free = minx(i),x(j) x(i) −x(j)2
and also called the free distance of the code. A code with a large d2
free is generally expected
to perform well, and d2
free has become the major design criterion for trellis codes. (The
free distance for the 8-PSK trellis code of Section 3.1 is d2
free = 4.586.)
One heuristic design rule [31], which was used successfully in designing codes with large
d2
free, is based on the following observation: If we assign to the branches leaving a state
signals from a subset where the distances between points are large, and likewise assign
such signals to the branches merging into a state, we are assured that the total distance is
at least the sum of the minimum distances between the signals in these subsets. For our
8-PSK code example we can choose these subsets to be QPSK signal subsets of the original
8-PSK signal set. This is done by partitioning the 8-PSK signal set into two QPSK sets
as illustrated in Figure 3.8 below. The mapping function is now chosen such that the
state information bit v(0) selects the subset and the input bits u select a signal within the
subset. Since all branches leaving a state have the same state information bit v(0), all the
branch signals are either in subset A or subset B, and the diﬀerence between two signal
sequences picks up an incremental distance of d2 = 2 over the ﬁrst branch where they
diverge. In order to achieve this, we need to make sure that the choice of u does not aﬀect
v(0), which is done by setting h(2)
0
= 0 and h(1)
0
= 0 in Figure 3.6.
To guarantee that the signal on branches merging into a state can also be chosen
from one of these two subsets, we set h(2)
ν
= 0 and h(1)
ν
= 0. This again has the eﬀect
that merging branches have the same value of the state information bit v(0). These are
Ungerb¨ock’s [31] original design rules.
We have now assured that the minimum distance between any two paths is at least
twice that of the original QPSK signal set. The values of the remaining connector coeﬃ-
cients h(2) = h(2)
1 , . . . , h(2)
ν−1, h(1) = h(1)
1 , . . . , h(1)
ν−1 and h(0) = h(0)
1 , . . . , h(0)
ν−1 are much harder
to ﬁnd, and one usually resorts to computer search programs or heuristic strategies.

3.2. CONSTRUCTION OF CODES
75
v(0)
r
= 0
v(0)
r
= 1
A
B
Figure 3.8: 8-PSK signal set partitioned into constituent QPSK signal sets.
Table 3.1 shows the best 8-PSK trellis codes found using 8-PSK with natural mapping.
The ﬁgure gives the connector coeﬃcients d2
free, the average path multiplicity Adfree of d2
free,
and the average bit multiplicity Bdfree of d2
free. Adfree is the average number of paths at
distance d2
free, and Bdfree is the average number of bit errors on those paths. Both, Adfree
and Bdfree, as well as the higher-order multiplicities, are important parameters determining
the error performance of a trellis code. This is discussed in detail in Chapter 5. (The
connector coeﬃcients are given in octal notation, e.g., h(0) = 23 = 10111, where a 1 means
connected and a 0 means no connection.)
From Table 3.1 one can see that an asymptotic coding gain (coding gain for SNR →∞
over the reference constellation which is used for uncoded transmission at the same rate) of
about 6 dB can quickly be achieved with moderate eﬀort. But, since for optimal decoding
the complexity of the decoder grows proportionally to the number of states, it becomes
very hard to go much beyond a 6 dB gain, leaving a signiﬁcant gap to the Shannon bound.
It will be up to turbo coded systems to close this gap. Since the asymptotic coding gain
is a reasonable yardstick at the bit error rates of interest, codes with a maximum of about
one thousand states seem to exploit most of what can be gained by this type of coding.
Some researchers have used diﬀerent mapping functions in an eﬀort to improve the bit
error performance which can be improved by up to 0.5 dB using 8-PSK Gray mapping (la-
bel the 8-PSK symbols successively by (000), (001), (011), (010), (110), (111), (101), (100))
as done by Du and Kasahara [6] and Zhang [41, 42]. Zhang also used another mapping
to improve the bit multiplicity, using ((000), (001), (010), (011), (110), (111), (100), (101))
as labeling. The search criterion employed involved minimizing the bit multiplicities of

76
CHAPTER 3. TRELLIS-CODED MODULATION
Number
of States
h(0)
h(1)
h(2)
d2
free
Adfree
Bdfree
Asymptotic
Coding
Gain
4
5
2
-
4.00∗
1
1
3.0 dB
8
11
2
4
4.59∗
2
7
3.6 dB
16
23
4
16
5.17∗
2.25
11.5
4.1 dB
32
45
16
34
5.76∗
4
22.25
4.6 dB
64
103
30
66
6.34∗
5.25
31.125
5.0 dB
128
277
54
122
6.59∗
0.5
2.5
5.2 dB
256
435
72
130
7.52∗
1.5
12.25
5.8 dB
512
1525
462
360
7.52∗
0.313
2.75
5.8 dB
1024
2701
1216
574
8.10∗
1.32
10.563
6.1 dB
2048
4041
1212
330
8.34
3.875
21.25
6.2 dB
4096
15201
6306
4112
8.68
1.406
11.758
6.4 dB
8192
20201
12746
304
8.68
0.617
2.711
6.4 dB
32768
143373
70002
47674
9.51
0.25
2.5
6.8 dB
131072
616273
340602
237374
9.85
6.9 dB
Table 3.1: Connectors, free squared Euclidean distance and asymptotic coding gains of
some maximum free distance 8-PSK trellis codes. The codes with an ∗were found by
exhaustive computer searches [34, 24], while the other codes were found by various heuristic
search and construction methods [24, 25]. The connector polynomials are in octal notation.
several spectral lines in the distance spectrum of a code.2 Table 3.2 gives the best 8-PSK
codes found with respect to the bit error probability.
If we go to higher-order signal sets such as 16-QAM, 32-cross, 64-QAM, etc., there are,
at some point, not enough states left such that each diverging branches lead to diﬀerent
states, and we have parallel transitions, i.e., two or more branches connect two states.
Naturally we would want to assign signals with large distances to such parallel branches,
since the probability of parallel path errors cannot be inﬂuenced by the code.
Parallel transitions actually occur also for the ﬁrst 8-PSK code in Table 3.1, whose
trellis is given in Figure 3.9. Here the parallel transitions are by choice though, not by
necessity. Note that the minimum distance path pair through the trellis has d2 = 4.56,
but that is not the most likely error to happen. All signals on parallel branches are from
a BPSK subset of the original 8-PSK set, and hence their distance is d2 = 4, which gives
2The notion of distance spectrum of a trellis code will be introduced and discussed in Chapter 5.

3.2. CONSTRUCTION OF CODES
77
Number
of States
h(0)
h(1)
h(2)
d2
free
Adfree
Bdfree
8
17
2
6
4.59∗
2
5
16
27
4
12
5.17∗
2.25
7.5
32
43
4
24
5.76∗
2.375
7.375
64
147
12
66
6.34∗
3.25
14.8755
128
277
54
176
6.59∗
0.5
2
256
435
72
142
7.52∗
1.5
7.813
512
1377
304
350
7.52∗
0.0313
0.25
1024
2077
630
1132
8.10∗
0.2813
1.688
Table 3.2: Table of 8-PSK codes using a diﬀerent mapping function [41].
the 3 dB asymptotic coding gain of the code over QPSK (d2 = 2).
0
4
2
6
3
7
1
5
1
3
7
5
2
6
0
4
0
0
2
2
Figure 3.9: 4-state 8-PSK trellis code with parallel transitions.
In general we partition a signal set into a partition chain of subsets, such that the
minimum distance between signal points in the new subsets is maximized at every level.
This is illustrated with the 16-QAM signal set and a binary partition chain (split each set
into two subsets at each level) in Figure 3.10.
Note that the partitioning can be continued until there is only one signal left in each
subset. In such a way, by following the partition path, a “natural” binary label can be
assigned to each signal point. The natural labeling of the 8-PSK signal set in Figure 3.7
(M-PSK in general) can also be generated in this way. This method of partitioning a
signal set is called set partitioning with increasing intra-subset distances. The idea is to
use these constellations for codes with parallel transitions.
An alternative way of generating parallel transitions is to choose not to encode all
the input bits in ur.
Using the encoder in Figure 3.11 with a 16-QAM constellation,

78
CHAPTER 3. TRELLIS-CODED MODULATION
0000    1000    0100    1100    0010    1010    0110    1110    0001    1001    0101    1101    0011    1011    0111    1111

v(0) = 0
v(0) = 1
v(1) = 0
v(1) = 1
v(1) = 0
v(1) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
Figure 3.10: Set partitioning of a 16-QAM signal set into subsets with increasing minimum
distance. The ﬁnal partition level used by the encoder in Figure 3.11 is the fourth level,
i.e., the subsets with two signal points each.
for example, the ﬁrst information bit u(3)
r
is not encoded and the output signal of the
FSM selects now a subset rather than a signal point (here one of the subsets at the fourth
partition level in Figure 3.10). The uncoded bit(s) select the actual signal point within the
subset. Analogously then, the encoder now has to be designed to maximize the minimum
interset distances of sequences, since it cannot inﬂuence the signal point selection within
the subsets. The advantage of this strategy is that the same encoder can be used for
all signal constellations with the same intraset distances at the ﬁnal partition level, in
particular for all signal constellation which are nested versions of each other, such as
16-QAM, 32-cross, 64-QAM, etc.
Figure 3.11 shows such a generic encoder which maximizes the minimum interset dis-
tance between sequences, and it can be used with all QAM-based signal constellations.
Only the two least signiﬁcant information bits aﬀect the encoder FSM. All other informa-
tion bits cause parallel transitions. Table 3.3 shows the coding gains achievable with such

3.2. CONSTRUCTION OF CODES
79
an encoder structure. The gains when going from 8-PSK to 16-QAM are most marked
since rectangular constellations have a somewhat better power eﬃciency than constant
energy constellations.
u(5)
r
u(4)
r
u(3)
r
u(2)
r
u(1)
r
v(5)
r
v(4)
r
v(3)
r
v(2)
r
v(1)
r
v(0)
r
h(2)
ν−1
h(2)
1
h(1)
ν−1
h(1)
1
h(0)
1
h(0)
ν−1
Mapping
Function
xr = τ(vr)
16-QAM
32-cross
64-QAM
xr
Figure 3.11: Generic encoder for QAM signal constellations.
Number
of States
Connectors
d2
free
Asymptotic Coding Gain
h(0)
h(1)
h(2)
16-
QAM/
8-PSK
32-cross/
16QAM
64-
QAM/
32-cross
Adfree
Bdfree
4
5
2
-
4.0
4.4dB
3.0dB
2.8dB
8
11
2
4
5.0
5.3 dB
4.0 dB
3.8 dB
3.656
18.313
16
23
4
16
6.0
6.1 dB
4.8 dB
4.6 dB
9.156
53.5
32
41
6
10
6.0
6.1 dB
4.8 dB
4.6 dB
2.641
16.063
64
101
16
64
7.0
6.8 dB
5.4 dB
5.2 dB
8.422
55.688
128
203
14
42
8.0
7.4 dB
6.0 dB
5.8 dB
20.328
100.031
256
401
56
304
8.0
7.4 dB
6.0 dB
5.8 dB
3.273
16.391
512
1001
346
510
8.0
7.4 dB
6.0 dB
5.8 dB
Table 3.3: Connectors and gains of maximum free distance QAM trellis codes [34].

80
CHAPTER 3. TRELLIS-CODED MODULATION
3.3
Lattices
Soon after the introduction of trellis codes and the idea of set partitioning, it was realized
that certain signal constellations and their partitioning could be described elegantly by
lattices. This formulation is a particularly convenient tool in the discussion of multidi-
mensional trellis codes. We begin by deﬁning a lattice:
Deﬁnition 3.1 An N-dimensional lattice Λ is the set of all points
Λ = {x} = {i1b1 + · · · + iNbN},
(3.4)
where x is an m-dimensional row vector (point) in Rm, b1, . . . , bN are N linearly inde-
pendent basis vectors in Rm, and i1, . . . , iN range through all integers.
A lattice is therefore something similar to a vector space, but the coeﬃcients are
restricted to be integers. Lattices have been studied in mathematics for many decades,
in particular in addressing issues such as sphere packings, the coverings, or quantization
[15, 5]. The sphere packing problem asks the question, “What is the densest way of packing
together a large number of equal-sized spheres.” The covering problem asks for the least
dense way to cover space with equal overlapping spheres and the quantization problem
addresses the problem of placing points in space so that the average second moment of
their Voronoi cells is as small as possible. A comprehensive treatise on lattices is given by
Conway and Sloane in [5]. We will use only a few results from lattice theory in our study
of mapping functions.
Not surprisingly, operations on lattices can conveniently be described by matrix oper-
ations. To this end, let
M =
⎡
⎢⎢⎢⎣
b1
b2
...
bN
⎤
⎥⎥⎥⎦
(3.5)
be the generator matrix of the lattice. All the lattice points can then be generated by iM,
where i = (i1, · · · , iN). Operations on lattices can now easily be described by operations
on the generator matrix. An example is RM, where
R =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
−1
1
1
1
−1
1
1
...
...
1
−1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(3.6)

3.3. LATTICES
81
is an operation which rotates and expands pairs of coordinates, called the rotation operator,
and is important for our purpose.
Two parameters of lattices are important, the minimum distance between points dmin
and the number of nearest neighbors, lucidly described as the kissing number τ of the
lattice. This is because if we ﬁll the m-dimensional Euclidean space with m-dimensional
spheres of radius dmin/2 centered at the lattice points, there are exactly τ spheres which
touch (kiss) any given sphere. The density Δ of a lattice is the proportion of the space
that is occupied by these touching spheres. Also, the fundamental volume V (Λ) is the
N-dimensional volume per lattice point, i.e., if we partition the total space into regions of
equal size V (Λ), each such region is associated with one lattice point.
Lattices have long been used with attempts to pack equal-sized spheres in m dimen-
sions, such that as much of the space as possible is covered by spheres. Lattices are being
investigated for the problem, since if we ﬁnd a “locally” dense packing, it is also “globally”
dense due to the linearity of the lattice.
A popular lattice is ZN, the cubic lattice, consisting of all N-tuples with integer coor-
dinates. Its minimum distance is dmin = 1 and its kissing number is τ = 2N. Trivially,
Z1 is the densest lattice in one dimension.
Another interesting lattice3 is DN, the checkerboard lattice, whose points consist of all
points whose integer coordinates sum to an even number. It can be generated from ZN
by casting out that half of all the points which have an odd integer sum. In doing so we
have increased dmin to
√
2, and the kissing number to τ = 2N(N −1), for N ≥2, and
have obtained a much denser lattice packing, which can be seen as follows. Since we need
to normalize dmin in order to compare packings, let us shrink DN by (1/
√
2)N. This puts
then
√
2
N/2 as many points as ZN into the same volume. (The denominator equals 2
since we eliminated half the points.) Therefore DN is 2N/2−1 times as dense as ZN. Note
that D2 has the same density as Z2, and it is in fact a rotated version of the latter, i.e.,
D2 = RZ2. D4, the Schl¨aﬂi lattice is the densest lattice packing in 4 dimensions. We will
return to D4 in our discussion of multi-dimensional trellis codes.
In order to describe signal constellations by lattices, we have to shift and scale the
lattice. To obtain the rectangular constellations Q from Z2, for example, we set Q =
c(Z2 + { 1
2, 1
2}), where c is an arbitrary scaling factor, usually chosen to normalize the
average symbol energy to unity, as agreed upon in Chapter 2, and the shift by (1
2, 1
2)
centers the lattice. As can be seen from Figure 3.12, such a shifted lattice can be used as
a template for all QAM constellations.
In order to extract a ﬁnite signal constellation from the lattice, a boundary is intro-
duced and only points inside the boundary are chosen. This boundary shapes the signal
constellation and aﬀects the average power used in transmitting signal points from such a
3The reason for the diﬀerent notation of ZN and DN is that ZN = Z × Z × · · · × Z, the Cartesian
product of 1-dimensional lattices Z, while DN is not.

82
CHAPTER 3. TRELLIS-CODED MODULATION
QPSK
16QAM
32cross
64QAM
256QAM
128QAM
Figure 3.12: The integer lattice Z2 as a template for the QAM constellations.
constellation. The eﬀect of this shaping is summarized in the shaping gain γs.
If we consider the rectangular constellations (see Figure 3.12), the area of the square
and the cross is given by V (Z2)2n, where 2n is the number of signal points. The average
energy Es of these two constellations can be calculated by an integral approximation for
large numbers of signal points and we obtain Es ≈2
32n for the square and Es ≈31
482n for
the cross shape. That is, the cross constellation is 31
32 or γs = 0.14 dB more energy eﬃcient
than the rectangular boundary.
The best enclosing boundary would be a circle, especially in higher dimensions. It
is easy to see that the energy savings of an N-dimensional spherical constellation over
an N-dimensional rectangular constellation is proportional to the inverse ratio of their
volumes. This gain can be calculated as [9]
γs = π(N + 2)
12
N
2

!
−2/N
,
N even.
(3.7)
In the limit, limN→∞γs = πe/6, or 1.53 dB. To obtain this gain, however, the constituent
2-dimensional signal points in our N-dimensional constellation will not be distributed
uniformly. In fact, they will tend towards a Gaussian distribution.

3.3. LATTICES
83
The fundamental coding gain4
γ(Λ) = d2
min/V (Λ)2/N
(3.8)
relates the minimum squared Euclidean distance to the fundamental volume per 2 dimen-
sions. This deﬁnition is meaningful since the volume per 2 dimensions is directly related to
the signal energy of a QAM constellation, and thus γ(Λ) expresses an asymptotic coding
gain. For example, from the preceding discussion
γ(DN) =
2
(2V (Λ))2/N = 21−2
N γ(ZN),
(3.9)
and γ(ZN) = 1 shall be used as reference henceforth. For example, γ(D4) = 2
1
2 , i.e., D4
has a coding gain of 2
1
2 or 1.51 dB over Z4. The deﬁnition (3.8) is also invariant to scaling
as one would expect.
Partitioning of signal sets can now be discussed much more conveniently with the help
of the lattice and is applicable to all signal constellations derived from that lattice. The
binary set partitioning in Figure 3.10 can be derived from the binary lattice partition
chain
Z2/RZ2/2Z2/2RZ2 = Z2/D2/2Z2/2D2.
(3.10)
Every level of the partition in (3.10) creates a sublattice Λ of the original lattice Λ as
shown in Figure 3.13. The remaining points, if we delete Λ from Λ, are a shifted version
Λ
s of Λ, called a coset. (There are p cosets in a p-ary partition.) Λ and Λ
s together make
up Λ, i.e., Λ = Λ  Λ
s. (E.g. Z2 = RZ2 (RZ2 + (1, 0)).) The partition chain (3.10)
generates 23 = 8 cosets of 2RZ2 and each coset is a translate of the ﬁnal sublattice.
A coset of a lattice Λ is denoted by Λ + c, where c is some constant vector which
speciﬁes the coset. Note that if c is in Λ, Λ + c = Λ. Mathematically speaking, we have
deﬁned an equivalence relation of points, i.e., all points equivalent to c modulo Λ are in
the same coset Λ + c.
If we now have a lattice partition Λ/Λ, and we choose c such that Λ + c ∈Λ (note:
not Λ), then every element in Λ can be expressed as
Λ = Λ +

Λ/Λ
,
(3.11)
where [Λ/Λ] is just a fancy way of writing the set of all such vectors c. Equation (3.11)
is called the coset decomposition of Λ in terms of cosets (translates) of the lattice Λ
4The fundamental coding gain is related to the center density δ used in [5] by
γ(Λ) = 4δ
2
N .
The center density of a lattice is deﬁned as the number of points per unit volume if the touching spheres
have unit radius.

84
CHAPTER 3. TRELLIS-CODED MODULATION
Z2
RZ2
2Z2
2RZ2
Figure 3.13: Illustration of the binary partition chain (3.10).
and the number of such diﬀerent cosets is the order of the partition, denoted by |Λ/
Λ′|. As an example consider the decomposition of Z2 into Z2 = RZ2 + {(0, 0), (0, 1)}.
Analogously an entire partition chain can be deﬁned, for example, for Λ/Λ′/Λ′′ we may
write Λ = Λ′′+[Λ′/Λ′′]+[Λ/Λ′], that is, every element in Λ can be expressed as an element
of Λ′′ plus a shift vector from [Λ′/Λ′′] plus a shift vector from [Λ/Λ′]. Again, for example,
Z2 = 2Z2 + {(0, 0), (1, 1)} + {(0, 0), (0, 1)}; see Figure 3.13.
The lattice partitions discussed here are all binary partitions, that is, at each level
the lattice is split into two cosets as in (3.10). The advantage of binary lattice partitions
is that they naturally map into binary representations. Binary lattice partitions will be
discussed more in Chapter 5.
Let us then consider a chain of K binary lattice partitions, i.e.,
Λ1/Λ2/ . . . ΛK,
(3.12)
for which (3.10) may serve as an example of a chain of 4 lattices. There are then 2K cosets
of ΛK whose union makes up the original lattice Λ1. Each such coset can now conveniently
be identiﬁed by a K-ary binary vector v = (v(1), · · · , v(K)), that is, each coset is given by
ΛK shifted by
c(v) =
K

i=1
v(i)c(i),
(3.13)
where c(i) is an element of Λi but not of Λi+1. The two vectors {0, c(i)} are then two
coset representatives for the cosets of Λi+1 in the binary partition Λi/Λi+1. (Compare the
example for the partition Z2/2Z2 above, where c(1) = (0, 1) and c(2) = (1, 1), see also
Figure 3.14 below). Generalizing the above to the chain of length K, the cosets of ΛK in
the partition chain Λ/ΛK are given by the linear sum (3.13).
In fact, the partition chain (3.10) is the basis for the generic encoder in Figure 3.11
for all QAM constellations. With the lattice formulation, we wish to describe our trellis
encoder in the general form of Figure 3.15. The FSM encodes k bits and the mapping

3.3. LATTICES
85
function selects one of the cosets of the ﬁnal sublattice of the partition chain, while the
n−k uncoded information bits select a signal point from that coset. The encoder can only
aﬀect the choice of cosets and therefore needs to be designed to maximize the minimum
distance between sequences of cosets, where the minimum distance between two cosets Λ
and Λ is deﬁned as min
x∈Λ
x∈Λ
|x, x|2 = min
x∈Λ |x|2 (taking the origin as reference, i.e., x = 0).
2Z2
2Z2 + (0, 1)
2Z2 + (1, 1)
2Z2 + (1, 0)
Figure 3.14: The 4 cosets of 2Z2 in the partition Z2/2Z2.
As shown in Figure 3.15 the trellis code breaks up naturally into two components. The
ﬁrst is a called a coset code and is made up of the ﬁnite state machine and the mapping into
cosets of a suitable lattice partition Λ/Λ. The second part is the choice of the actual signal
point within the chosen coset. This signal choice determines the constellation boundary
and therefore shapes the signal set. It is referred to as shaping.
k bits
n −k bits
Coset Code
FSM
Coset Selector
Λ/Λ
Shaping
Signal point
selector
xr
Figure 3.15: Generic trellis encoder block diagram using the lattice notation.

86
CHAPTER 3. TRELLIS-CODED MODULATION
3.4
Lattice Formulation of Trellis Codes
A coset code as shown in Figure 3.15 will be denoted by C(Λ/Λ; C), where C is the ﬁnite-
state machine generating the branch labels. In most known cases, C is a convolutional
code. The sequence of branch labels, the codewords of C, will be written as (vr, vr+1, . . .),
i.e., a sequence of binary k + m-tuples, where m is the number of parity bits in the
convolutional code (e.g., m = 1 for the codes generated by the encoder in Figure 3.6).
Each vj serves as a label which selects a coset of Λ at time r, and the coded part of the
encoder selects therefore a sequence of lattice cosets. The n −k uncoded bits select one
of 2n−k signal points from the coset Λ
r at time r. The sequence of branch labels vr from
the FSM are mapped by the coset selector into sequence of cosets cr.
Since the branch labels of our trellis code are now cosets containing more than one
signal point, the minimum distance of the code is min(d2
free, d2
min), where d2
free is the mini-
mum free squared Euclidean distance between any two output sequences and d2
min is the
minimum squared Euclidean distance between members of Λ, the ﬁnal sublattice of Λ.
Figure 3.16 shows the generic QAM encoder in lattice formulation, where v(0) selects
one of the two cosets of RZ2 in the partition Z2/RZ2, v(1) selects one of the two cosets of
2Z2 in the partition RZ2/2Z2 and v(2) selects one of the two cosets in the partition 2Z2/
2RZ2. (Compare also Figures 3.10 and 3.11.) The ﬁnal selected coset in the Z2/2RZ2
partition is given by
c = v(0)c(0) + v(1)c(1) + v(2)c(2),
(3.14)
where c(0) is the coset representative in the partition Z2/RZ2, and c(1) and c(2) are the
coset representatives in the partitions RZ2/2Z2 and 2Z2/2RZ2, respectively.
Finite-State Machine
Rate R = 1/2
Convolutional Encoder
u(3)
r , · · · , u(n)
r
u(2)
r
u(1)
r
v(2)
r
v(1)
r
v(0)
r
Λ + c
Z2/RZ2
RZ2/2Z2
2Z2/2RZ2
xr = τ(c, u(3)
r , · · · , u(n)
r )
xr
Figure 3.16: Lattice encoder for the 2-dimensional constellations grown from Z2.

3.4. LATTICE FORMULATION OF TRELLIS CODES
87
An important fact should be noted at this point.
Contrary to the trellis code in
Figure 3.11, the coset codes discussed in this section are group codes; i.e., two output
coset sequences c1(D) and c2(D) may be added to produce another valid output sequence
c3(D) = c1(D) + c2(D).
The reason why this is the case lies precisely in the lattice
formulation. With the lattice viewpoint we have also introduced an addition operator
which allows us to add lattice points or entire lattices. We also note that the lattices
themselves are inﬁnite, and there are no boundary problems. The linearity of the lattice
makes the entire trellis code regular, and one can use the all-zero sequence as reference
sequence.
Using this new lattice description, trellis codes can now easily be classiﬁed. Ungerb¨ock’s
original 1-, and 2-dimensional PAM codes are based on the four-way partition Z/4Z and
the eight-way partition Z2/2RZ2, respectively. The 1-dimensional codes are used with a
rate R = 1/2 convolutional code and the 2-dimensional codes with a rate R = 2/3 convo-
lutional code with the exception of the 4-state code (ν = 2) which uses a rate R = 1/ 2
code.
Table 3.4 shows these codes where the 2-dimensional codes are essentially a reproduc-
tion of Table 3.3. Also shown in the table is ND, the number of nearest neighbors in the
coset code. Note that the number of nearest neighbors is based on the inﬁnite lattice and
selecting a particular constellation from the lattice will reduce that number. As we will
discuss in more detail later the number of nearest neighbors, and in fact the number of
neighbors at a given distance in general, also aﬀect performance and sometimes codes with
a smaller d2
free can outperform codes with a larger d2
free but more nearest neighbors.
So far we have only concerned ourselves with 2-dimensional lattices as basis for our sig-
nal constellations, but in fact, this is an arbitrary restriction and multi-dimensional trellis
codes, i.e., trellis codes using lattices of dimensions larger than 2 have been constructed
and have a number of advantages. Multi-dimensional in this context is a theoretical con-
cept, since, in practice, multi-dimensional signals are transmitted as sequences of 1- or
2-dimensional signals.
We have seen earlier that in order to introduce coding we need to expand the signal
set from the original uncoded signal set. The most usual and convenient way is to double
the constellation size, i.e., introduce one bit of redundancy. Now this doubling reduces the
minimum distance within the constellation, and this reduction has to be compensated for
by the code before any coding gain can be achieved. If we use, say a 4-dimensional signal
set, this doubling causes the constituent 2-dimensional constellations to be expanded by a
factor of only
√
2 (half a bit of redundancy per 2-D constellation). The advantage is that
there is less initial loss in minimum distance within the signal set. If we consider rect-
angular signal sets derived from ZN, we obtain the following numbers. For 2 dimensions
the signal set expansion costs 3 dB in minimum distance loss, for 4-dimensional signals
the loss is 1.5 dB and for 8-dimensional signal sets it is down to 0.75 dB. We see that
the code itself has to overcome less and less signal set expansion loss. Another point in

88
CHAPTER 3. TRELLIS-CODED MODULATION
Number
of States
Λ
Λ′
d2
min
Asymptotic
Coding Gain
ND
1-dimensional codes
4
Z
4Z
9
3.52 dB
8
8
Z
4Z
10
3.98 dB
8
16
Z
4Z
11
4.39 dB
16
32
Z
4Z
13
5.12 dB
24
64
Z
4Z
14
5.44 dB
72
128
Z
4Z
16
6.02 dB
132
256
Z
4Z
16
6.02 dB
4
512
Z
4Z
16
6.02 dB
4
2-dimensional codes
4
Z2
2Z2
4
3.01 dB
4
8
Z2
2RZ2
5
3.98 dB
16
16
Z2
2RZ2
6
4.77 dB
56
32
Z2
2RZ2
6
5.77 dB
16
64
Z2
2RZ2
7
5.44 dB
56
128
Z2
2RZ2
8
6.02 dB
344
256
Z2
2RZ2
8
6.02 dB
44
512
Z2
2RZ2
8
6.02 dB
4
Table 3.4: Lattice partition, free distance, asymptotic coding gain and number of nearest
neighbors for the original Ungerb¨ock 1- and 2-dimensional trellis codes.
favor of multi-dimensional codes is that linear codes with 90-degree phase invariance can
be constructed. This will be explored in more detail in Section 3.7.
Let us consider codes over 4-dimensional rectangular signal sets as an example. The
binary partition chain we use is
Z4/D4/RZ4/RD4/2Z4/2D4/ · · ·
(3.15)
with minimum distances
1/
√
2/
√
2/2/2/2
√
2/ · · · .
(3.16)
The FSM generating the code trellis is now designed to maximize the interset distance of
sequences. No design procedure for good codes is known to date and computer searches are
usually carried out, either exhaustively or with heuristic selection and rejection rules. Note,

3.4. LATTICE FORMULATION OF TRELLIS CODES
89
however, that the computer search needs to optimize only the coset code which is linear,
and therefore we can choose v(D) = 0 as the reference sequence. Now we have a simple
mapping of vr into a distance increment d2
i , and d2
free = minc(D),c(D) c(D) −c(D)2 =
minv(D)

r d2
r(vr), where d2
r(vr) = minx∈Λs(vr) x2 is the distance from the origin to the
closest point in the coset Λs(vr) speciﬁed by vr.
Figure 3.17 illustrates the partitioning tree analogously to Figure 3.10 for the 4-
dimensional 8-way partition Λ/Λ = Z4/RD4.
The cosets at each partition are given
as unions of two 2-D cosets, i.e., D2 × D2 ∪D2 × D2 is the union of the Cartesian product
of D2 with D2 and D2 with D2, where D2 = D2 + (0, 1) is the second coset.
Z4
D2 × D2 ∪D2 × D2
D2 × D2 ∪D2 × D2
D2 × D2
D2 × D2
D2 × D2
D2 × D2
A × A ∪B × B
A × B ∪B × A
C × C ∪D × D
C × D ∪D × C
D × A ∪C × B
A × C ∪B × D
B × D ∪A × C
C × B ∪D × A
v(0) = 0
v(0) = 1
v(1) = 0
v(1) = 1
v(1) = 0
v(1) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
v(2) = 0
v(2) = 1
Figure 3.17: 8-way lattice partition tree of Z4. The constituent 2 dimensional cosets are
A = 2Z2, B = 2Z2 +(1, 1), C = 2Z2 +(0, 1) and D = 2Z2 +(1, 0) as shown in Figure 3.14.
Figure 3.18 shows a 16-state encoder using a rate R = 2/3 convolutional code to
generate a trellis with d2
free = d2
min = 4, i.e., the parallel transitions are those producing
the minimum distance.
Since the ﬁnal lattice Λ = RD4 is a rotated version of the
checkerboard lattice D4, we immediately also know the number of nearest neighbors at
d2
min = 4, which is ND = 24. The addition of the diﬀerential encoder allows the code to
be made rotationally invariant to 90 degree phase rotations. This particular encoder will
be discussed further in Section 3.7 on rotational invariance.
One philosophy, brought forth by Wei [36], is to choose a lattice partition Λ/Λ where
Λ is a denser lattice than Λ. A dense lattice Λ increases the minimum distance, and
therefore the asymptotic coding gain, however this also increases the number of nearest
neighbors. Another advantage is that this philosophy simpliﬁes code construction since
codes with fewer states may be used to achieve a given coding gain.

90
CHAPTER 3. TRELLIS-CODED MODULATION
Binary
Modulo-4
Adder
Signal
Point
Selection
Coset
Selection
u(m)
r
u(3)
r
u(2)
r
u(1)
r
v(3)
r
v(2)
r
v(1)
r
v(0)
r
x(1)
r x(2)
r
Figure 3.18: 16-state 4-dimensional trellis code using the 8-way partition shown in Figure
3.17 above. The diﬀerential encoder on the input makes the code rotationally invariant.
Tables 3.4 summarizes the best multi-dimensional trellis codes found to date. The
index in the last column refers to the source of the code. Note that the table also includes
8-dimensional codes, mainly pioneered by Wei [36], and by Calderbank and Sloane [3, 4].
Lattice
Kissing
Numbers
(Nearest
Neigh-
bors)
d2
min
Fundamental
Coding
Gain
γ(λ)
DN (Checkerboard)
2N(N −1)
2
21−2
N
E8 (Gosset)
240
4
2
H16
4
Λ16 (Barnes-Wall)
4320
8
2
3
2
Λ24 (Leech)
196560
8
4
D32 (Barnes-Wall)
208320
16
4
Table 3.5: Sublattices of ZN, their minimum distance d2
min, and their kissing number.
While we now have discussed the construction and classiﬁcation of multi-dimensional
codes, we have said little about the motivation to use more than 2 dimensions.
2-
dimensional signal constellations seem to be a natural choice since bandpass double-side-
band modulated signals naturally generate a pair of closely related dimensions, the inphase
and quadrature channels. The motivation to go to higher dimensional signal sets is not
quite so obvious, but there are two very important reasons. The ﬁrst is the smaller constel-
lation expansion factor discussed previously. The second is that multi-dimensional codes
can be made invariant to phase rotations using linear FSMs.

3.4. LATTICE FORMULATION OF TRELLIS CODES
91
All the lattices discussed, i.e., D2, D4, E8, and H16, are sublattices of the Cartesian
product lattice ZN. Signal constellations derived from these lattices are particularly useful
since a modem built for Z2, i.e., QAM constellations, can easily accommodate the lattices
without much extra eﬀort. We simply disallow the signal points in ZN which are not
points in the sublattice in question. Table 3.4 shows these useful sublattices.
Number
of States
Λ
Λ′
d2
min
Asymptotic
Coding Gain
ND
Source
4-dimensional codes
8
Z4
RD4
4
4.52 dB
44
W
16
D4
2D4
6
4.77 dB
152
C-S
64
D4
2D4
8
5.27 dB
828
C-S
16
Z4
RD4
4
4.52 dB
12
W
32
Z4
2Z4
4
4.52 dB
4
W
64
Z4
2D4
5
5.48 dB
72
W
128
Z4
2D4
6
6.28 dB
728
U
8-dimensional codes
16
Z8
E8
4
5.27 dB
316
W
32
Z8
E8
4
5.27 dB
124
W
64
Z8
E8
4
5.27 dB
60
W
128
Z8
RD8
4
5.27 dB
28
U
32
RD8
RE8
8
6.02 dB
W
64
RD8
RE8
8
6.02 dB
316
W
128
RD8
RE8
8
6.02 dB
124
W
8
E8
RE8
8
5.27 dB
764
C-S
16
E8
RE8
8
5.27 dB
316
C-S
32
E8
RE8
8
5.27 dB
124
C-S
64
E8
RE8
8
5.27 dB
60
C-S
16-dimensional codes
32
Z16
H16
4
5.64 dB
W
64
Z16
H16
4
5.64 dB
796
W
128
Z16
H16
4
5.64 dB
412
W
Table 3.6: Best multi-dimensional trellis codes based on binary lattice partitions of 4, 8,
and 16 dimensions. The source column indicates the origin of the code, i.e., codes marked
U were found in [34], those marked W in [36] and those marked C-S in [4]. The Gosset
lattice E8 and the lattice H16 are discussed in Chapter 5.

92
CHAPTER 3. TRELLIS-CODED MODULATION
3.5
Rotational Invariance
With ideal coherent detection of a DSB-SC signal (Figure 2.8), the absolute carrier phase
of the transmitted signal needs to be known at the receiver, in order to ensure the correct
orientation of the signal constellation. Before synchronization is accomplished, the received
signal has a phase oﬀset by an angle θ; i.e., the received signal, ignoring noise, is given by
(see Equation (2.34)
s0(t) = x(t)
√
2 cos(2πf0t + φ).
(3.17)
This oﬀset is typically by frequency drifts of the local oscillator, channel doppler eﬀects,
and phase noise in the system.
Let us assume for purposes of illustration that we are using an MPSK signal set. We
can then write (3.17) as
s0(t) =
√
2 cos(2πf0t + φm(t) + φ),
(3.18)
where φm(t) is the time-varying data phase. For example, for QPSK φm(t) can assume the
angles π/2, π, 3π/2 and 2π. The carrier phase tracking loop, conventionally a phase-locked
loop (PLL) circuit, ﬁrst needs to eliminate the data dependent part of the phase. In the
case of the MPSK signaling this can be accomplished by raising received signal to the Mth
power. This Mth order power device generates a spectral line cos(2πMf0+Mφm(t)+Mφ)
at Mf0. But Mφm(t) is always a multiple of 2π for MPSK modulation, and this Mth power
spectral line is now free of the data-dependent phase changes, given by cos(2πMf0 +Mφ).
A standard PLL can now used to track the phase, and the local oscillator signal is generated
by dividing the frequency of the tracking signal by M.
However, the squaring device causes a signiﬁcant squaring loss due to cross-products
between noise and signal, which manifests itself as an SNR loss in the tracking loop, and
is particularly severe for higher-order constellations [28, 14]. For 8-PSK, for example, this
squaring loss w.r.t. an unmodulated carrier is on the order of 10 dB. Due to this, decision-
directed PLL techniques are typically used in practice. Nonetheless, phase synchronization
is a challenging task, and, more recently, integrated iterative phase estimation and data
detection algorithms have shown much promise [16] (see also Chapter 12), a concept which
has been extended to complete channel estimation [26].
Even with successful phase synchronization, phase oﬀsets by multiples of constellation
symmetry angle, π/2 for QPSK, cannot be identiﬁed by a typicall PLL-based phase track-
ing circuit, leaving the carrier phase recovery system with a phase ambiguity, which have
the undesirable eﬀect that the received signal constellations may be rotated versions of
the transmitted constellations, and the trellis decoder can usually not decode properly if
the constellations are rotated. This necessitates that all possible phase rotations of the
constellation have to be tried until the correct one is found, which can cause unacceptable
delays. It is therefore desirable to design TCM systems such that they have as many phase

3.5. ROTATIONAL INVARIANCE
93
invariances as possible, that is, rotation angles which do not aﬀect decoder operation. This
will also assure more rapid resynchronization after temporary signal loss.
In decision-directed carrier phase acquisition and tracking circuits, this rotational in-
variance is even more important, since without proper output data, the tracking loop is
in an undriven random-walk situation, from which it may take a long time to recover.
This situation also can be avoided by making the trellis code invariant to constellation
rotations.
A trellis code is called rotationally invariant with respect to the rotation of constituent
constellation by an angle φ, if the decoder can correctly decode the transmitted information
sequence when the local oscillator phase diﬀers from the carrier phase by φ. Naturally,
φ is restricted to be one of the phase ambiguity angles of the recovery circuit, which is
a rotational symmetry angle of the signal constellation, i.e., an angle which rotates the
signal constellation into itself. If x is a sequence of coded symbols of a certain trellis code,
denote by xφ the symbol sequence which is obtained by rotating each symbol xr by the
angle φ. We now have the following:
Deﬁnition 3.2 A TCM code is rotationally invariant with respect to a rotation by an
angle φ, if x →xφ is a code sequence for all valid code sequences x.
This deﬁnition is rather awkward to test or work with. But we can translate it into
conditions on the transitions of the code trellis. Assume that there is a total of S states in
the code’s state space SP and that there are P subsets which result from set partitioning
the signal constellation. Assume further that the partitioning is done such that for each
phase rotation, each subset rotates into either itself or another subset; i.e., the set of
subsets is invariant under these rotations. This latter point is automatically true for 1-
and 2-dimensional constellations [36] (see, e.g., Figure 3.10) if we use the type of lattice
partitioning discussed in the previous sections. With these assumptions we now may state
the following [36, 39]
Theorem 3.1 A TCM code is rotationally invariant with respect to a rotation by an angle
φ, if there exists a (bijective) function fφ : SP →SP with the following properties: For
each transition from a state i to a state j, denote the associated signal subset by A. Denote
by Aφ the subset obtained when A is rotated by the angle φ, A
φ→Aφ. Then Aφ is the subset
associated with the transition from state fφ(i) to state fφ(j).
Proof: The situation in Theorem 3.1 is illustrated in Figure 3.19. It is easy to see that if
we concatenate successive trellis sections, for each path i, j, k, . . ., there exists a valid path
fφ(i), fφ(j), fφ(k), . . . , of rotated symbols through the trellis.
Q.E.D.
There are now two components to making a trellis code transparent to a phase rota-
tion of the signal constellation. Firstly, the code must be rotationally invariant; that is,

94
CHAPTER 3. TRELLIS-CODED MODULATION
i = 0
i
fφ(i)
i = S −1
j
fφ(j)
A
B
φ
m
fφ(m)
l
fφ(l)
Figure 3.19: Illustration of Theorem 3.1 and its proof.
the decoder will still ﬁnd a valid code sequence after rotation of x by φ. Secondly, the
rotated sequence xφ needs to map back into the same information sequence as the original
sequence. This is achieved by diﬀerentially encoding the information bits, as illustrated
below.
It turned out to be impossible to achieve rotational invariance of a code with a linear
generating FSM in conjunction with 2-dimensional signal constellations [21, 22, 17], and
hence “non-linear trellis codes” were investigated [37, 38]. Figure 3.20 shows such a non-
linear eight-state trellis code for use with QAM constellations, illustrated for use with the
32-cross constellation to transmit 4 bits/symbol, as shown in Figure 3.21. This code was
adopted in the CCITT V.32 Recommendation [18] and provides an asymptotic coding gain
of 4 dB. The three leading uncoded bits which are in plain typeface in Figure 3.21 are not
aﬀected by 90 degree rotations of the constellation. Only the last two bits, marked in bold,
are aﬀected and need to be encoded diﬀerentially, as done in the encoder. The constellation
is partitioned into the eight subsets D0, · · · , D7. Note that successive 90 degree phase
rotations take D0 →D1 →D2 →D3 →D0, and D4 →D5 →D6 →D7 →D4.
Equipped with this information we may now study a section of the trellis diagram of
this non-linear, rotationally invariant trellis code. This trellis section is shown in Figure
3.22 below, together with the state correspondence function fφ(i), for successive 90 de-
gree rotations of the signal constellations (see Theorem 3.1). It is relatively easy to see
that, if we take an example sequence D4, D0, D3, D6, D4 and rotate it by 90 degrees into
D5, D1, D0, D7, D5, we obtain another valid sequence of subsets. Careful checking reveals
that the function required in Theorem 3.1 exists, and the corresponding state relations
are illustrated in Figure 3.22.

3.5. ROTATIONAL INVARIANCE
95
Mapping
Function
u(4)
r
u(3)
r
u(2)
r
u(1)
r
v(4)
r
v(3)
r
v(2)
r
v(1)
r
v(0)
r
x(1)
r
Figure 3.20: Non-linear rotationally invariant 8-state trellis code for QAM constellations.
(00010)
(00011)
(11110)
(01111)
(00110)
(10111)
(00101)
(01100)
(11001)
(01000)
(01001)
(10000)
(00001)
(00000)
(10101)
(00100)
(01101)
(11100)
(01110)
(00111)
(10010)
(01011)
(01010)
(11011)
(10110)
(11111)
(10011)
(11010)
(10101)
(10100)
(11000)
(10001)
D0
D1
D2
D3
D4
D5
D6
D7
Figure 3.21: Example of a 32-cross constellation for the non-linear trellis code of Figure
3.20. The set partitioning of the 32-cross constellation into the eight subsets D0, . . . , D7
is also illustrated [34].

96
CHAPTER 3. TRELLIS-CODED MODULATION
D3D5D7D1
7
D2D0D6D4
6
D5D3D1D7
5
D4D6D0D2
4
D7D1D3D5
3
D6D4D2D0
2
D1D7D5D3
1
D0D2D4D6
0
Figure 3.22: Trellis section of the non-linear 8-state trellis code. The subset labeling is
such that the ﬁrst signal set is the one on the top branch, and the last signal set is on the
lowest branch, throughout all the states.
This then takes care of the rotational invariance of the code with respect to phase
rotations of multiples of 90 degrees. However, the bit v(1)
r
= u(1)
r
changes its value through
such rotations. This is where the diﬀerential encoding comes into play. In Figure 3.20, this
function is realized by the exor gate on the input line u(1)
r
and the second delay cell; i.e.,
the diﬀerential encoder is integrated into the trellis encoder. This integrated diﬀerential
encoder is indicated by the dotted loop in Figure 3.20. The coding gain of this code is 4
dB and the number of nearest neighbors is 16 [34].
The actual standards V.32 and V.33 [1] use the trellis code shown in Figure 3.23. The
code is an 8-state trellis code with 90 degree phase invariance. The V.32 standard operates
at duplex rates of up to 9600 bit/s, at a symbol rate of 2400 baud using the rotated cross
constellation of Figure 3.24 below. We leave it as an exercise to the reader to show that
the V.32 code is rotationally invariant to phase angles which are multiples of π/2.
The V.33 recommendation allows for data rates of up to 14,400 bit/s. It is designed
to operate over point-to-point, four-wire leased telephone-type circuits. It uses the same
encoder as V.32 but with an expanded 128-point cross signal constellation.
It too is
invariant to phase rotations of multiples of π/2.
While no linear trellis codes exist for 2-D constellations which are rotationally invari-
ant, such codes can be found for higher dimensional signal sets. In fact, the encoder in
Figure 3.18 generates such a code. The signal set partition chain for this encoder was
given in Figure 3.17, and a 90 degree phase rotation will cause the following signal set

3.5. ROTATIONAL INVARIANCE
97
V.33 modem only
Binary
Modulo-4
Adder
Mapping
Function
u(6)
r
u(5)
r
u(4)
r
u(3)
r
u(2)
r
u(1)
r
v(2)
r
v(1)
r
v(0)
r
xr
Figure 3.23: Non-linear 8-state trellis code used in the V.32 and V.33 recommendations.
11111
00011
00111
11011
11101
10001
01101
00001
01111
01011
10111
10011
10101
11001
00101
01001
10100
01010
00010
01000
10110
11110
11000
00000
01100
11010
10010
00100
11100
10000
00110
01110
Figure 3.24: 32-point rotated cross signal constellation used in the V.32 recommendation
for a 9600-bit/s voiceband modem.

98
CHAPTER 3. TRELLIS-CODED MODULATION
changes5: A × A ∪B × B →C × C ∪D × D →A × A ∪B × B, A × B ∪B × A →
C × D ∪D × C →A × B ∪B × A, D × A ∪C × B →B × D ∪A × C →D × A ∪C × B,
and A × C ∪B × D →C × B ∪D × A (see Figures 3.14 and 3.17). A section of the trellis
of this code is shown in Figure 3.25 below.
The state correspondences are worked out by checking through Theorem 3.1, from
where we ﬁnd that f900(0) = 4, f900(4) = 0, etc., as indicated in Figure 3.25 by the arrows.
Note that, due to the multi-dimensional nature of the signal set, 180 degree phase
rotations map the subsets back onto themselves. The remaining ambiguities can be taken
care of by diﬀerential encoding within the subsets.
A × A ∪B × B
A × B ∪B × A
C × C ∪D × D
C × D ∪D × C
A × A ∪B × B
A × B ∪B × A
C × C ∪D × D
C × D ∪D × C
0
4
0
1
6
7
2
3
4
5
Figure 3.25: Portion of the trellis section of the linear 16-state trellis code, which achieves
90 degree rotational invariance.
Wei [39] and Pietrobon et al. [22] tackled the problem of designing rotationally invariant
trellis codes for M-PSK signal constellations. This is somewhat more complicated, since the
number of phase ambiguities of the constellations are larger than for QAM constellations.
The basic philosophy, however, remains the same.
The fastest modems can achieve data transmission rates of up to 33,600 bit/s over
the public switched telephone network (PSTN). This is roughly equal to the informa-
tion theoretic capacity of an AWGN-channel with the same bandwidth. These modems
adaptively select and choose the transmission band, the trellis code, constellation shaping,
precoding and pre-emphasis for equalization, and signal set warping. These techniques
are summarized in [7]. These modems use trellis coding and are described in the standard
recommendation V.34, also nicknamed V.fast.
5Recall that the lattice is shifted by ( 1
2, 1
2) in order to obtain the constellation.

3.6. V.FAST
99
3.6
V.fast
V.34 [19], or V.fast, is an analog modem standard for full-duplex transmission at rates up
to 33.6 kbits/s over standard telephone networks. Given the average signal-to-noise ratio,
mainly due to quantization noise in the digital trunk lines, of typically between 28-38 dB,
this rate is very close to the information theoretic capacity of this channel, and V.fast
represents therefore the pinnacle of achievement and state of the art technology for digital
data transmission over voiceband channels.
Due to the wide range of signal-to-noise ratio values and other channel conditions
encountered on voiceband channels, the V.34 standard supports a large number of diﬀerent
rates, the highest supportable being negotiated between participating modems at the start
of communications session [8, 19]. V.34 uses QAM modulation like all previous voiceband
modems, but allows for an adaptive symbol rate and center frequency. The modem selects
one of six symbol rates and one of two carrier frequencies according to channel conditions.
These are 2400 (1600 Hz/1800 Hz), 2743 (1646 Hz/1829 Hz), 1680 (1680 Hz/1867 Hz),
3000 (1800 Hz/2000 Hz), 3200 (1829 Hz/1920 Hz), and 3429 (1959 Hz).
The trellis codes used in V.fast are 4-dimensional codes, whose small constellation
expansion helps guard against non-Gaussian distortions. Three codes are speciﬁed in the
standard, a 16-state 4-dimensional trellis code [36] with a coding gain of 4.2 dB, a 32-state
code [40] with a coding gain of 4.5dB, and a 64-state code [35] with a coding gain of 4.7
dB. V.34 also speciﬁes a shaping method called shell mapping in 16 dimensions to achieve
a near spherical shaping of the signal constellation.
This mapping achieves 0.8 dB of
shaping gain, which is somewhat more than half of the theoretical maximum of 1.53 dB
(see Section 3.3).
The large rates of these modems are mainly achieved by increasing the size of the
constellations used, up to 1664 points for rates of 33.6 kbits/s. The fundamental coding
approach to counter the additive noise via trellis coding is unchanged and similar to the
earlier modems discussed in this chapter. Adaptive equalization and precoding, however,
have contributed as much to the success of V.34 as has trellis-coded modulation, by equal-
izing the channel and removing distortion. Every eﬀort is made by the modem to use
the available bandwidth, even if the attenuation at the edges of the band are as severe
as 10 dB. Decision feedback equalization techniques have been chosen for this since lin-
ear equalizers generate too much noise enhancement in cases of deep attenuations. The
equalization is implemented by precoding the data via a three-tap precoding ﬁlter. More
details on precoding techniques can be found in [13].
Figure 3.26 illustrates the interplay of these diﬀerent techniques at the transmitter.
The diﬀerential encoder ensures rotational invariance to multiples of 90 degree rotations
while the shell mapper shapes the signal constellation requiring a constellation expansion
of 25%. The 4-dimensional trellis code requires a 50% constellation expansion making the
total signal constellation expansion 75%.

100
CHAPTER 3. TRELLIS-CODED MODULATION
Shell
Mapping
Diﬀerential
Encoding
Trellis
Encoder
Precoder
Non-linear
Encoder
w′
r
wr
xr
Signalpoint Mapper
Figure 3.26: Encoder block diagram of a V.fast transmitter.
The precoder and trellis encoder are seemingly switched backwards in this setup, where
the trellis encoder determines its next state from the output symbol chosen by the precoder,
rather than directly from the input symbols. This helps minimize the predistortion that
the precoder needs to add to the symbol xr in order to generate the transmitted symbol
wr. The precoder compensates for the linear distortion of the channel and the eﬀect is
that the received sequence is a simple noise trellis sequence that can be decoded by a
standard trellis decoder (Chapter 5). The non-linear encoder is optional and is designed
to counter eﬀects of non-linear distortion.
V.fast brings together a number of advanced techniques to exploit the information
carrying capacity of the telephone voiceband channel, and can rightfully be considered a
triumph of information technology. Since the information theoretic capacity does not allow
any further signiﬁcant increase in data speeds, V.fast has also been nicknamed V.last. It
is important to note in this context that V.90, the 56 kbit/s downlink modem in common
use in the 1990’s, relies on a diﬀerent technology and more importantly on a diﬀerent
channel. Since the digital trunk telephone lines carry 64 bits/s digital channels all the
way to the local switching stations, and since the twisted pair cables from the switching
stations to the end-user’s telephones is analog and, not suﬀering from the quantization
noise, has a much larger SNR, almost the entire digital data traﬃc can be transported to
the end-user by a modem which treats the trunc lines as a digital channel rather than as
a noise voiceband channel. This is precisely what V.90 does.

3.7. THE IEEE 802.3AN STANDARD
101
3.7
The IEEE 802.3an Standard
We conclude this chapter with a discussion of the physical layer of the IEEE 802.3an
standard for 10 Gigabit Ethernet transmission over twisted pair copper cables. While not
a “trellis-coded modulation” example, it is a “coded modulation” example of the latest
generation and follows the same design principles as discussed here.
To run multi-gigabit data rates on four pairs of copper cabling, the BASE-T channel,
sophisticated digital signal processing is used to eliminate the eﬀects of cross-talk between
pairs of cable and to remove the eﬀects of signal reﬂections, but the main workhorse for
the system is the coded modulation system discussed in this section.
The major diﬀerences between the IEEE 802.3an coding system and the trellis-coded
modulation methods used for the voiceband channel lies in the adoption of a large regular
LDPC code, to be discussed in detail in Chapter 6, as well as the choice of a novel 128-
point, 2-dimensional QAM constellation for achieving high spectral eﬃciency.
Figure 3.27 shows the block diagram of a the IEEE 802.3an communication system,
and we see the familiar blocks of the coded and uncoded transmit function and the signal
mapping in the 2-dimensional QAM constellation. The receiver blocks are also completely
analogous to those we discussed for trellis-based systems, consisting of the error control
code decoder and the coset decoder. The only diﬀerences are the additional 2-D rotation
of the incoming signals, which will be explained below, and the fact that the error control
decoder is an LDPC code and not a convolutional code.
The twisted-pair copper cable channel is a baseband channel and therefore the basic
signaling method is to use Pulse-Amplitude Modulation (PAM). But rather than standard
1-dimensional PAM, two successive PAM symbols are combined into a 2-dimensional sig-
nal proposed by BroadCom in [2]. The two dimensions are made up of two consecutive
one-dimensional dithered 8-PAM signal constellations. A 16-level set partitioning of this
constellation achieves enough intra-subset distance to permit uncoded transmission of 3
of the seven information bits.
The information symbol vector is therefore separated into 3 uncoded bits and 4 coded
bits, the latter using a (2048,1723) LDPC code for noise protection. The constellation
and mapping of bits is shown in Figure 3.28.
The three uncoded bits select one of 8
shaded subsets. Within each such subset, there are 16 signal points, which are labeled by
the 4 coded bits. Note that the diﬀerences in coded bits lead to signal points within a
subset which can be close and thus are prone to errors. These bits are protected by the
error control code. A change in an uncoded bit leads from one subset to another, at large
distance which provides noise immunity, the basic principle of set partitioning.
The partition chain applied to this application is D2/2Z2/2D4/4Z2 with intraset dis-
tances
√
2/2/2
√
2/4, providing a 12 dB gain on the uncoded bit on an ideal AWGN
channel. If we assume the points to lie on an integer grid with odd coordinates as in
Figure 3.28, the average power per dimension equals Pnorm = 85.

102
CHAPTER 3. TRELLIS-CODED MODULATION
Delay
Coded Information Bits
Uncoded
Bits
  Coset 
Decoder
LDPC Error Control Decoder
       Rate: (2048,1723)
LLR Generation for coded bits
Rotation
VM   +
Signals
Bit
Framing
and
Selection
LDPC Encoding
128-DSQ Mapping
Information
Bits
Uncoded
Bits
Cable Channel
h(D)
Figure 3.27: Block diagram of the IEEE 802.3an 10 Gbit/s transmission system.
Decoding of the coded bits, denoted as c1, c2, c3, c4, needs to be accomplished ﬁrst,
and we now assume that (ˆc1, ˆc2, ˆc3, ˆc4) = (c1, c2, c3, c4), i.e., no decoding errors occurred.
(Otherwise the frame will be in error independently of the uncoded bits.) The coded bits
(ˆc1, ˆc2, ˆc3, ˆc4) now identify one of 16 subsets from the signal constellation in Figure 3.28.
For discussion purposes we assume that (ˆc1, ˆc2, ˆc3, ˆc4) = (1, 1, 0, 1), in which case the
subconstellation shown by the bold circles on the left-hand side of Figure 3.29 has been
identiﬁed by the LDPC decoder. If (ˆc1, ˆc2, ˆc3, ˆc4) = (0, 0, 0, 0), the subset on the right-hand
side has been identiﬁed. The task of the coset decoder is now to extract the three uncoded
bits which are encoded in the eight signal points of each subset. In order to simplify this
process, we can translate each subset into the (0,0,0,0) subset shown on the right in Figure
3.29. To accomplish this, each signal point is shifted to the left by two units, and down
one unit in this case. Other subsets have diﬀerent shifts according to the location of the

3.7. THE IEEE 802.3AN STANDARD
103
                
                 
       
               
                
                 
0010 0110 1110 1010
0011 0111 1111 1011
0001 0101 1101 1001
0000 0100 1100 1000
000
000
110
110
011
010
001
111
100
100
100
101
(xxx : yyyy)
select
subset
select
signalpoint
(15,15)
(-15,-15)
y1
y2
Figure 3.28: 128-DSQ constellation used in 802.3an.
binary 4-tuple (ˆc1, ˆc2, ˆc3, ˆc4) in Figure 3.28. Points that fall outside the red square that
deﬁnes the base constellation are wrapped around to the opposite side of the square, as
illustrated for the points (000) and (110). This modulo wrapping can be performed at
any point in the signal processing path, but has to happen before the application of the
decision equations (3.20).
This shift is given by the following set of shift equations, where the starting point for
(c1, c2, c3, c4) = (0, 0, 0, 0) is denoted by (y1, y2).
(c1, c2, c3, c4)
Shift
(c1, c2, c3, c4)
Shift
(0, 0, 0, 0)
(y1, y2)
(1, 0, 0, 0)
(y1, y2) + (−2, 2)
(0, 0, 0, 1)
(y1, y2) + (−2, −2)
(1, 0, 0, 1)
(y1, y2) + (−8, 4)
(0, 0, 1, 0)
(y1, y2) + (−6, −6)
(1, 0, 1, 0)
(y1, y2) + (−12, 0)
(0, 0, 1, 1)
(y1, y2) + (−4, −4)
(1, 0, 1, 1)
(y1, y2) + (−10, 2)
(0, 1, 0, 0)
(y1, y2) + (−2, 2)
(1, 1, 0, 0)
(y1, y2) + (−4, 4)
(0, 1, 0, 1)
(y1, y2) + (−4, 0)
(1, 1, 0, 1)
(y1, y2) + (−6, 2)
(0, 1, 1, 0)
(y1, y2) + (−8, −4)
(1, 1, 1, 0)
(y1, y2) + (−10, −2)
(0, 1, 1, 1)
(y1, y2) + (−6, −2)
(1, 1, 1, 1)
(y1, y2) + (−8, 0)
(3.19)

104
CHAPTER 3. TRELLIS-CODED MODULATION
0000
1101
011
111
001
101
101
010
110
110
100
100
000
000
011
111
001
101
101
010
110
110
100
100
000
000
Figure 3.29: Subconstellation shift required for demodulating bits for (c1, c2, c3, c4) =
(1, 1, 0, 1). The subconstellation for (0,0,0,0) is shown on the right.
Decoding of the uncoded bits can now be accomplished eﬃciently by realizing the
relationships between the decision regions for the individual bits. Figure 3.30 shows these
decision regions for the three uncoded bits u1, u2, and u3. These decision regions are all
congruent to each other, that is, a counterclockwise rotation of the set for u1 by 90 degrees
followed by a reﬂection on the vertical axis generates the decision region and boundaries
for u3, and a simple reﬂection at the vertical axis generates the boundaries for u2.
Focussing on bit u1 for subset (c1, c2, c3, c4) = (0000), the decision region can be broken
into four areas, depending on the location of the received coordinate s2, which can fall
into four intervals: s2 :∈[−15, −7], [−7, 1], [1, 9], [9, 17]. Depending on where s2 lies, the
following decision are made for bit u1; i.e., u1 = 1
if
s2 ∈[−15, −7]
AND
s1 > s2 + 8
AND
s1 ≤s2 + 24
if
s2 ∈[−7, 1]
AND
s1 > −s2 −6
AND
s1 ≤−s2 + 10
if
s2 ∈[1, 9]
AND
s1 > s2 −8
AND
s1 ≤s2 + 8
if
s2 ∈[9, 17]
AND
s1 > −s2 + 10
AND
s1 ≤−s2 + 26
(3.20)
The decisions for the other bits are made via the same process after the received signal
sample operations discussed above, and the error probability of each uncoded bit is closely
approximated by
P(b,u) = 2Q

16 ∗8
2N0

= Q

32
σ2

= 2Q

32
85
Es
σ2

,
(3.21)
giving a 12 dB gain over the raw bit error rate of the 128-DSQ constellation alone.
This concludes our discussion of the operation of coset-based coded modulation. Nat-
urally, the performance of these systems is strongly dependent on the performance of the
underlying error control codes. This will be discussed in subsequent chapters.

3.7. THE IEEE 802.3AN STANDARD
105
                
                 
       
               
                
                 
  
               
   
   
   
   
011
111
001
101
010
110
100
000
(-15,-15)
(-15,17)
(17,-15)
(17,17)
+9
+1
-7
111
101
100
1
                
                 
       
               
                
                 
  
               
   
   
   
   
011
111
001
101
010
110
100
000
(-15,-15)
(-15,17)
(17,-15)
(17,17)
+9
+1
-7
011
111
1
101
1
                
                 
       
               
                
                 
  
               
   
   
   
   
011
111
001
101
010
110
100
000
(-15,-15)
(-15,17)
(17,-15)
(17,17)
011
111
010
1
Decision of u1
Decision of u2
Decision of u3
Figure 3.30: Decision region for uncoded bits u1, u2 and u3, derived from the prototype
decision region for u1. Simply mirroring the s1 variable allows us to use the same decision
thresholds for both.

106
CHAPTER 3. TRELLIS-CODED MODULATION
3.8
Historical Notes
While error control coding was long regarded as a discipline with applications mostly for
channel with no bandwidth limitations such as the deep-space radio channel, the trellis-
coded modulation schemes of Ungerb¨ock [30, 31, 33, 34] provided the ﬁrst strong evidence
that coding could be used very eﬀectively on bandlimited channels. Starting in the 1980’s
to this day, numerous researchers have discovered new results and new codes.
Multi-
dimensional schemes were ﬁrst used by Wei [36] and Calderbank and Sloane [3], who also
introduced the lattice viewpoint in [4]. Much of the material presented in this chapter is
adapted from Forney’s comprehensive treatments of the subject in [10, 11, 12].
The treatment of the IEEE 802.3an standard has been added here to illustrate the
expansion of coded modulation into applications other than voice-band modems and is
only one of the recent modern high-spectral eﬃciency coded modulation systems.
It
illustrates again, however, that redundant error control coding and signal set expansion
can successfully be combined to achieve spectrally eﬃcient transmission.

Bibliography
[1] U. Black, The V Series Recommendations, Protocols for Data Communications Over
the Telephone Network, McGraw-Hill, New York, 1991.
[2] G. Ungerboeck et al., “LDPC (Low-Density Parity-Check) Coded 128 DSQ (Double-
Square QAM) Constellation Modulation and Associated Labeling,” United States
Patent Application, US 2006/0045197, March 2, 2006.
[3] A.R. Calderbank and N.J.A. Sloane, “An eight-dimensional trellis code,” Proc. IEEE,
vol. 74, pp. 757–759, 1986.
[4] A.R. Calderbank and N.J.A. Sloane, “New trellis codes based on lattices and cosets,”
IEEE Trans. Inform. Theory, vol. IT-33, pp. 177–195, 1987.
[5] J.H. Conway and N.J.A. Sloane, Sphere Packings, Lattices and Groups, Springer-
Verlag, New York, 1988.
[6] J. Du and M. Kasahara, “Improvements of the information-bit error rate of trellis
code modulation systems,” IEICE, Japan, vol. E 72, pp. 609–614, May 1989.
[7] M.V. Eyuboglu, G.D. Forney, P. Dong, and G. Long, “Advanced modem techniques
for V.Fast,” Eur. Trans. Telecommun. ETT, vol. 4, no. 3, pp. 234–256, May–June
1993.
[8] G.D. Forney, Jr., L. Brown, M.V. Eyuboglu, and J.L. Moran III, “The V.34 high-
speed modem standard,” IEEE Commun. Mag., Dec. 1996.
[9] G.D. Forney, Jr., R.G. Gallager, G.R. Lang, F.M. Longstaﬀand S.U. Qureshi, “Ef-
ﬁcient modulation for band-limited channels,” IEEE J. Select. Areas Commun., vol.
SAC-2, no. 5, pp. 632–647, 1984.
[10] G.D. Forney, “Coset codes–Part I: Introduction and geometrical classiﬁcation,” IEEE
Trans. Inform. Theory, vol. IT-34, pp. 1123–1151, 1988.
[11] G.D. Forney, “Coset codes–Part II: Binary lattices and related codes,” IEEE Trans.
Inform. Theory, vol. IT-34, pp. 1152–1187, 1988.
[12] G.D. Forney, “Geometrically uniform codes,” IEEE Trans. Inform. Theory, vol. IT-
37, pp. 1241 – 1260, 1991.
107

108
BIBLIOGRAPHY
[13] G.D. Forney and M.V. Eyuboglu, “Combined equalization and coding using precod-
ing,” IEEE Commun. Mag., vol. 29, no. 12, pp. 25–34, Dec. 1991.
[14] F.M. Gardner, Phaselock Techniques, 2nd edition, John Wiley & Sons, New York,
1979.
[15] A. Gersho and R.M. Gray, Vector Quantization and Signal Compression, Kluwer
Publishing, Dortrecht, 1996.
[16] S. Howard, C. Schlegel, L. Perez, and F. Jiang, ”Diﬀerential turbo-coded-modulation
over unsynchronized channels,” Proceedings of the Wireless and Optical Communica-
tions Conference (WOC) 2002, Banﬀ, AB, Canada, July 2002.
[17] IBM Europe, “Trellis-coded modulation schemes for use in data modems transmit-
ting 3–7 bits per modulation interval,” CCITT SG XVII Contribution COM XVII,
no. D114, April 1983.
[18] IBM Europe, “Trellis-coded modulation schemes with 8-state systematic encoder and
900 symmetry for use in data modems transmitting 3–7 bits per modulation interval,”
CCITT SG XVII Contribution COM XVII, no. D180, October 1983.
[19] ITU-T Rec. V.34, “A modem operating at data signalling rates of up to 28,800 bit/s
for use on the general switched telephone network and on leased point-to-point 2-wire
telephone-type circuits,” 1994.
[20] J.L. Massey, T. Mittelholzer, T. Riedel, and M. Vollenweider, “Ring convolutional
codes for phase modulation,” IEEE Int. Symp. Inform. Theory, San Diego, CA,
Jan. 1990.
[21] S.S. Pietrobon, G.U Ungerboeck, L.C. Perez, and D.J. Costello, Jr., “Rotationally in-
variant nonlinear trellis codes for two-dimensional modulation,” IEEE Trans. Inform.
Theory, vol. IT-40, no. 6, pp. 1773–1791, Nov. 1994.
[22] S.S. Pietrobon, R.H. Deng, A. Lafanech`ere, G. Ungerboeck, and D.J. Costello, Jr.,
“Trellis-coded multidimensional phase modulation,” IEEE Trans. Inform. Theory,
vol. IT-36, pp. 63–89, Jan. 1990.
[23] S.S. Pietrobon and D.J. Costello, Jr., “Trellis coding with multidimensional QAM
signal sets,” IEEE Trans. Inform. Theory, vol. IT-39, pp. 325–336, March 1993.
[24] J.E. Porath and T. Aulin, “Fast algorithmic construction of mostly optimal trellis
codes,” Technical Report no. 5, Division of Information Theory, School of Electrical
and Computer Engineering, Chalmers University of Technology, G¨oteborg, Sweden,
1987.
[25] J.E. Porath and T. Aulin, “Algorithmic construction of trellis codes,” IEEE Trans.
Commun., vol. COM-41, no. 5, pp. 649-654, May. 1993.
[26] C. Schlegel and A. Grant, ” Diﬀerential space-time turbo codes,” IEEE Trans. Inform.
Theory, vol. 49, no. 9, Sep. 2003, pp. 2298–2306.

BIBLIOGRAPHY
109
[27] C. Schlegel and A. P´erez, Trellis and Turbo Coding, IEEE/Wiley, Hoboken, NJ, 2004.
[28] B. Skar, Digital Communications: Fundamentals and Applications, Prentice Hall,
Englewood Cliﬀs, NJ, 1988.
[29] D. Slepian, “On neighbor distances and symmetry in group codes,” IEEE Trans.
Inform. Theory, vol. IT-17, pp. 630–632, Sep. 1971.
[30] G. Ungerboeck and I. Csajka, “On improving data-link performance by increasing
channel alphabet and introducing sequence coding,” International Symposium on
Information Theory, Ronneby, Sweden, June 1976.
[31] G. Ungerboeck, “Channel coding with multilevel/phase signals,” IEEE Trans. Inform.
Theory, vol. IT-28, no. 1, pp. 55-67, Jan. 1982.
[32] G. Ungerboeck, J. Hagenauer, and T. Abdel-Nabi, “Coded 8-PSK experimental mo-
dem for the INTELSAT SCPC system,” Proc. ICDSC, 7th, pp. 299–304, 1986.
[33] G. Ungerboeck, “Trellis-coded modulation with redundant signal sets Part I: Intro-
duction,” IEEE Commun. Mag., vol. 25, no. 2, pp. 5–11, Feb. 1987.
[34] G. Ungerboeck, “Trellis-coded modulation with redundant signal sets Part II: State
of the art,” IEEE Commun. Mag., vol. 25, no. 2, pp. 12–21, Feb. 1987.
[35] L.F. Wei, “A new 4D 64-state rate-4/5 trellis code,” Cont. D19, ITU-TSG 14, Geneva,
Switzerland, Sep. 1993.
[36] L.F. Wei, “Trellis-coded modulation with multidimensional constellations,” IEEE
Trans. Inform. Theory, vol. IT-33, pp. 483 – 501, 1987.
[37] L.F. Wei, “Rotationally invariant convolutional channel coding with expanded signal
space–Part I: 180 degrees,” IEEE J. Select. Areas Commun., vol. SAC-2,, pp. 659–
672, Sep. 1984.
[38] L.F. Wei, “Rotationally invariant convolutional channel coding with expanded signal
space–Part II: nonlinear codes,” IEEE J. Select. Areas Commun., vol. SAC-2,, pp.
672–686, Sep. 1984.
[39] L.F. Wei, “Rotationally invariant trellis-coded modulations with multidimensional
M-PSK”, IEEE J. Select. Areas Commun., vol. SAC-7, no. 9, Dec. 1989.
[40] R.G.C. Williams, “A trellis code for V.fast,” CCITT V.fast Rapporteur metting,
Bath, U.K., Sep. 1992.
[41] W. Zhang, “Finite-state machines in communications,” Ph.D. thesis, University of
South Australia, Australia, 1995.
[42] W. Zhang, C. Schlegel, and P. Alexander, “The BER reductions for systematic 8PSK
trellis codes by a Gray scrambler,” Proceedings of the International Conference on
Universal Wireless Access, Melbourne, Australia, April 1994.

Chapter 4
Trellis Representations
4.1
Preliminaries
While error control coding has traditionally largely been approach using ﬁnite-ﬁeld algebra,
this book concentrates on graphical descriptions and methods for these codes.
These
graphical representations lend themselves not only as visual aids, but also as blueprints
for the algorithmic decoding methods discussed in Chapters 5, 6, and 8.
There are, in general, many diﬀerent graphical representations of any given error con-
trol code, but some representations have distinct advantages. Among the most basic, and
also richest graphical representations, are the trellises of codes discussed in this chapter.
They will lead to famous optimal trellis decoding algorithms of Chapter 5 and are fun-
damental building blocks for the iterative decoding algorithms applied so successfully to
turbo codes.
While we will not discuss algebraic code structures and decoding algorithm methods
in any length in this book, we wish to point out that such approaches have been very
successful in a number notable applications, such as compact disc (CD) storage, digital
audio recording (DAT), and high-deﬁnition television (HDTV) [22]–primarily application
where error correction is more central then error control.
Arguably, the single most important result which accounts for the use and popularity
algebraic decoding are the ﬁnite-ﬁeld methods for the class of Goppa codes [41], in par-
ticular the Berlekamp-Massey [4, 37] and Euclid’s algorithm [54]. Reed–Solomon codes,
and, to a lesser extent, Bose-Chaudhuri-Hocquenghem (BCH) codes, are the most popular
subclasses of Goppa codes. These codes can be decoded very fast with a complexity on the
order of O(d2), where d is the minimum distance of the code, using ﬁnite-ﬁeld arithmetic
which can be implemented eﬃciently in VLSI circuits. However, these eﬃcient algebraic
decoding algorithms only perform error correction, that is, the received noisy or corrupted
symbols have to be mapped into the transmitted symbol alphabet before decoding. The
111
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

112
CHAPTER 4. TRELLIS REPRESENTATIONS
direct use of the received symbols, i.e., soft-decision decoding, cannot be used directly. This
may explain why algebraic coding ﬁnds application predominantly in systems requiring
error correction. Soft-decision decoding, referred to as error control decoding, can be done
however, and several algorithms have been proposed [14, 62, 7, 39]. But these techniques
have not found much attention by practitioners of error control coding. More recently,
soft-decision decoding has seen a revival of interest [52, 20, 34, 18, 59, 60, 55, 57, 17],
in particular in connection with the trellis complexity problem of codes addressed in this
chapter.
In Chapter 5 we will discuss a number of powerful decoding techniques which make use
of the trellis structure of codes. Unlike convolutional codes, algebraic codes have tradition-
ally been constructed and decoded in purely algebraic ways, which do not lend themselves
easily to soft-decision decoding extensions. However, with the trellis representations devel-
oped in this chapter, these codes can also be described by a trellis. Soft-decision decoding
algorithms developed for trellis codes can therefore be applied. The algebraic picture of
error correction codes are extensively discussed in a number of excellent reference books
and others, [42, 33, 41, 5, 8].
4.2
The Parity-Check Matrix
One of the easiest ways to deﬁne a linear1 code of rate R = k/n is via its parity-check
matrix H = [h1, h2, . . . , hn], which is an n −k × n matrix whose jth column is hj and
has entries hij ∈GF(p), where GF(p) is the ﬁnite ﬁeld of the integers modulo p, and p
is a prime. In the simplest case the code extends over GF(2), i.e., over {0, 1}, and all
operations are modulo 2, or EXOR and AND operations.
A linear code may now be characterized by the following
Deﬁnition 4.1 A linear code C with parity check-matrix H is the set of all n-tuples (vec-
tors), or codewords x, such that
Hx = 0.
(4.1)
Algebraically, the codewords of C lie in the (right) nullspace of H.
For example, the family of single-error correcting binary Hamming codes have parity-
check matrices H whose columns are all the 2m −1 non-zero binary vectors of length
m = n −k. These codes have a rate R = k/n = (2m −m −1)/(2m −1) and a minimum
distance dmin = 3 [42]. The ﬁrst such code found by Richard Hamming in 1948 has the
1We restrict our attention to linear codes since they are the largest and most important class of block
codes, and there are no signiﬁcant performance restrictions that result from this restriction.

4.3. PARITY-CHECK TRELLIS REPRESENTATIONS
113
parity-check matrix
H[7,4] =
⎡
⎣
1
1
1
0
1
0
0
1
1
0
1
0
1
0
1
0
1
1
0
0
1
⎤
⎦= [h1 h2 h3 h4 h5 h6 h7].
(4.2)
Note that each row of H corresponds to a single parity-check equation.
Any parity check matrix can always be arranged such that H = [A|In−k] through
column and row permutations and linear combinations of rows, where In−k is the n −k
identity matrix and A has dimensions n −k × k. This has already been done in (4.2).
From this form we can obtain the systematic k × n code generator matrix
G =

Ik


−AT 
(4.3)
through simple matrix manipulations. The code generator matrix has dimensions k×n and
is used to generate directly the codewords x via xT = uT G, where u is the information
k-tuple. Algebraically, the codeword x lies in the row space of G, since they are all linear
combinations of the rows of G.
4.3
Parity-Check Trellis Representations
In this section, we will use a trellis as a visual method of keeping track of the pk codewords
of a block code C, and each distinct codeword will correspond to a distinct path through
the trellis. This idea was ﬁrst explored by Bahl, Cocke, Jelinek, and Raviv [3] in 1974,
and later also by Wolf [64] and Massey [36] in 1978. Following the approach in [3, 64] we
deﬁne the states of the trellis as follows: Let sr be the label of the state at time r, then
sr+1 = sr + xr+1hr+1 =
r

l=1
xlhl,
(4.4)
where xr+1 runs through all permissible code symbols at time r + 1. The state at the
end of time interval r + 1, sr+1, is calculated from the preceding state sr according to
(4.4). If the states sr and sr+1 are connected, they are joined in the trellis diagram by a
branch labeled with the output symbol xr+1 which caused the connection. We see that
(4.4) simply implements the parity-check equation (4.1) in a recursive fashion, since the
ﬁnal zero-state sn+1 = n
l=1 xlhl = Hx is the complete parity check equation! Since each
intermediate state sr = r
l=1 xlhl is a vector of length n −k with elements in GF(p), also
called the partial syndrome at time r, there can be at most pn−k distinct states at time r,
since this is the maximum number of distinct p-ary vectors of length n −k. We will see
that often the number of states is signiﬁcantly less than that.

114
CHAPTER 4. TRELLIS REPRESENTATIONS
At this point an example seems appropriate. Let us construct the trellis of the [7, 4]
Hamming code, whose parity check matrix is given by (4.2). The corresponding trellis
is shown in Figure 4.1.
The states sr are labeled as ternary vectors (σ1, σ2, σ3)T , in
accordance with (4.4). Note that all the path extensions which lead to states sn+1 ̸= (000)T
are dashed, since s(i)
n is the ﬁnal syndrome and must equal (000)T for x to be a codeword.
We therefore force the trellis to terminate in the zero-state at time n.
0
1
0
0
0
0
0
0
1
1
0
0
0
0
0
0
State
s = 0
s = 1
s = 2
s = 3
s = 4
s = 5
s = 6
s = 7
s0
s1
s2
s3
s4
s5
s6
s7
x1
x2
x3
x4
x5
x6
x7
Figure 4.1: Trellis diagram of the [7, 4] Hamming code. The labeling of the trellis is such
that a “ 0” causes a horizontal transition and a “ 1” causes a sloped transition. This follows
from (4.4) since xr+1 = 0 causes sr+1 = sr. Hence, only a few transitions are labeled in
the ﬁgure.
The trellis of a code represents the set of all codewords in the following sense: Each
distinct path from the starting node to the ﬁnal node is a diﬀerent and distinct codeword.
Codewords may share large portions of the trellis and only diﬀer over short sections. This
concept will be important when we discuss optimal decoding of codes on their trellis. The
complexity of these decoding algorithms will be proportional to the number of branches
and the number of states in the trellis.
For codes whose maximum number of states
is much less than the total number 2k of codewords, this trellis-based decoding is very
eﬃcient.

4.4. CONVOLUTIONAL CODES AND THEIR TRELLIS
115
4.4
Convolutional Codes and Their Trellis
In Chapter 4 we used ﬁnite-state machines as generators of our trellis, which then served
as the matrix for the trellis code. This particular way of constructing the trellis is not only
of historical importance, but it also allowed us to ﬁnd all the structural theorems presented
in Chapter 4. Later, in Chapter 5, we will prove information theoretic results which hold
for trellis code generated in this way. If the mapping of the binary output bits is into the
{−1, +1} binary set of BPSK signals, we have the case of traditional convolutional codes.
In fact, the trellis mapping function is basically omitted.
Convolutional codes have a long history. They were introduced by Elias [13] in 1955.
Since then, much theory has evolved to understand them [12, 33, 8, 61].
Figure 4.2, which is the ﬁnite-state machine from Figure 3.1, is now a convolutional
encoder which produces a convolutional code. This code maps a vector ur = (u(2)
r , u(1)
r ) of
input bits into a vector vr = (v(2)
r , v(1)
r , v(0)
r ) of output bits at time r. The rate R = 2/3
of this code is the ratio of the number of input bits to the number of output bits. If these
output bits are mapped individually into BPSK signals, we quickly see that a convolutional
code does not conserve the signal bandwidth, but requires 1/R times more bandwidth than
uncoded transmission through rate expansion.
+
+
u(1)
r
u(2)
r
s(0)
r
s(1)
r
s(2)
r
v(2)
r
v(1)
r
v(0)
r
Figure 4.2: Rate R = 2/3 convolutional encoder which was used in Chapter 3, Figure 3.1,
to generate an 8-PSK trellis code.
Convolutional codes have traditionally been used with BPSK or QPSK (Gray) map-
pings. In either case, due to the regularity of the mapper function, the Euclidean distance
between two signal sequences depends only on the Hamming distance Hd(v(1), v(2)) be-
tween the two output bit sequences (formerly branch label sequences), where the Hamming
distance between two sequences is deﬁned as the number of bit positions in which the two
sequences diﬀer. Furthermore, since the convolutional code is linear, we can choose any
one of the sequences as the reference sequence, and we need to consider only the Hamming
weights of Hw(v(1) ⊕v(2)) = Hd(v(1) ⊕v(2), 0) = Hd(v(1), v(2)).

116
CHAPTER 4. TRELLIS REPRESENTATIONS
It is interesting to note that the convolutional encoder in Figure 4.2 has an alternate
“incarnation,” which is given in Figure 4.3 below.
This form is called the controller
canonical non-systematic form, the term stemming from the fact that inputs can be used
to control the state of the encoder in a very direct way, in which the outputs have no
inﬂuence. If no feedback is present, as in Figure 4.3, then the controller canonical from
is commonly referred to as the non-systematic feedforward realization of the encoder. We
will see later in this chapter that equivalent encoders, like the ones in Figures 4.2 and 4.3,
generate the same code, although for a given input sequence the not generate the same
output sequence.
+
+
u(1)
r
u(2)
r
v(2)
r
v(1)
r
v(0)
r
Figure 4.3: Rate R = 2/3 encoder from above in controller canonical non-systematic form.
Both encoders generate the same trellis, reproduced here from Chapter 3 and shown
for a short length of L = 5 sections. One of the evident advantages of convolutional code
trellises is that they can be of arbitrary length, and their state space is well-deﬁned from
a direct map from the generating ﬁnite-state machine of code.
We will show in Section 4.9 that every convolutional code has a minimal basic encoder
of the form of Figure 4.3, an equivalent minimal systematic encoder (Figure 4.2). Minimal
in this sense means that the number of states in the ﬁnite-state machine is equal to the
number of states in the trellis–a fact which may seem obvious, but which is not in general
the case.
Additionally, there are many other possible ways of representing the encoder, a par-
ticular one, the recursive systematic realization was made popular by the introduction of
turbo codes. However, as shown later, in a certain sense only the two implementations dis-
cussed above are fundamental, and all others are easily reduced to one these realizations.
Note also that all encoder realizations generate the same code trellis, but that the labeling
of the information bits to the diﬀerent branches can change. This labeling, however, is
important in the combinations of encoders, like the one used to generate the binary turbo
codes in Chapter 8.

4.4. CONVOLUTIONAL CODES AND THEIR TRELLIS
117
0
4
2
6
0
4
2
3
7
0
5
1
5
3
7
6
s = 0
s = 1
s = 2
s = 3
s = 4
s = 5
s = 6
s = 7
Figure 4.4: Trellis diagram for the encoders in Figure 4.2 and 4.3.
In order to illustrate these concepts, it is convenient to use the deﬁnition of the D-
transform, given by
u(D) =
∞

r=s
urDr,
(4.5)
where s ∈Z guarantees that there are only ﬁnitely many negative terms in (4.5). The D-
operator can be understood as a unit-delay operator, corresponding to passing a symbol
through one delay element. Formally, (4.5) is a Laurent series with vector coeﬃcients,
which we may write as two (k in general) binary Laurent series
u(D) =

u(2)(D), u(1)(D)

.
(4.6)
The binary Laurent series form a ﬁeld2 and division is understood to be evaluated
2That is, they possess all the ﬁeld properties:
(i) Closure under addition and multiplication.
(ii) Every element a(D) possesses an additive inverse −a(D), and every element a(D) = 0 possesses a
multiplicative inverse 1/a(D).
(iii) The addition operation commutes: a(D) + b(D) = b(D) + a(D), and the multiplication operation
commutes also: a(D)b(D) = b(D)a(D).
(iv) There exists an additive unit element 0 such that a(D) + 0 = a(D), and a(D) + (−a(D)) = 0, as
well as a multiplicative unit element 1 such that a(D) · 1 = a(D), and a(D) (1/a(D)) = 1.
(v) Multiplication distributes over addition: a(D) (b(D) + c(D)) = a(D)b(D) + a(D)c(D).

118
CHAPTER 4. TRELLIS REPRESENTATIONS
by expanding the long division into positive exponents, e.g., (1 + D)/(D2 + D3 + D4) =
D−2 + 1 + D + D3 + · · · . Arithmetic with Laurent series can now be carried out in the
well-known conventional way, knowing that results from such computations are consistent
with each other.
Applying this, let us then consider a rate R = 1/2 minimal basic encoder with the
following encoding matrix
GFF (D) =

1 + D3 + D4
1 + D + D3 + D4,

(4.7)
which has the obvious non-systematic feedforward realization shown in Figure 4.5(a), also
called the controller canonical form. The equivalent systematic, or observer canonical,
encoder has the generator matrix
GFB(D) =

1
h1(D)
h0(D)

=

1
1+D+D3+D4
1+D3+D4 ,

.
(4.8)
with the systematic feedback realization shown in Figure 4.5(b).
As said above, a third realization, the recursive systematic realization based on the
controller canonical form, has been used in the context of parallel and serial concatenated
convolutional codes (turbo codes), and we now show how the recursive systematic realiza-
tion can be derived from the systematic feedback encoder GFB(D). For a rate R = 1/2
systematic feedback encoder, we can write

v(1)(D), v(0)(D)

= u(1)(D)GFB(D) =

u(1)(D), u(1)(D)h1(D)
h0(D)

.
(4.9)
Introducing the auxiliary sequence
w(D) = u(1)(D)
h0(D)
(4.10)
and using h0(D) = 1 + D3 + D4 for the selected example, we solve for
w(D) = (D4 + D3)w(D) + u(1)(D).
(4.11)
The sequence w(D) is now seen to be recursively generated by feeding it back to the input
after D = 3 and D = 4 unit time delays and adding the input sequence u(1)(D) to it. This
operation is implemented by the upper feedback part of the circuit in Figure 4.5(c). The
lower part of the recursive systematic realization implements the multiplication by h1(D)
to generate the sequence
v(0)(D) = w(D)h1(D) = u(1)
0 (D)
h0(D) h1(D),
(4.12)

4.4. CONVOLUTIONAL CODES AND THEIR TRELLIS
119
+
+
+
+
+
+
+
++
+
+
+
+
+
(a) Nonsystematic feedforward encoder realization.
(b) Systematic feedback encoder realization.
(c) Recursive systematic feedback encoder realization.
u(1)(D)
u(1)(D)
u(1)(D)
w(D)
v(1)(D)
v(0)(D)
v(1)(D)
v(0)(D)
v(1)(D)
v(0)(D)
Figure 4.5: Three encoder realizations for a rate R = 1/2 convolutional code.
which, naturally, is identical to the one generated by the systematic feedback realization.
Since the systematic feedback and recursive systematic realizations are both based on
the same encoder GFB(D), it is clear that they are not only equivalent, but identical. They
implement the identical mapping from input sequences to output sequences. This then
begs the question as to why one of these realizations would be preferred over the other.
The answer lies in the fact that the recursive systematic realization is of the controller
canonical form [28] and, as noted in Section 4.9, the state may be set by the input.

120
CHAPTER 4. TRELLIS REPRESENTATIONS
4.5
Minimal Trellises
In light of the importance the trellis plays for decoding the code, in particular the di-
rect relationship between trellis complexity the complexity of optimal decoding, we turn
our attention now to the optimality of trellis representations. We can loosely deﬁne the
complexity of a trellis as the number of branches in the trellis, which itself is strongly
related to the maximum number of states, or the state-space dimension of the trellis. As
we have seen, the complexity of the trellis of convolutional codes is directly related to
the state-space of their ﬁnite-state machine implementations, and we will discuss how to
minimize the size of that state space later in this chapter. For general codes, however, it
is not immediately clear how complex their trellis representations are.
Following the results of McEliece [40] we will now discuss bounds on the state space of
the trellis as well as its branch space and show that the parity-check trellis representation
(or PC-trellis) from Section 4.3 is the best possible in the sense that it minimizes the
number of states as well as the number of edges in the trellis for each time instant r. Since
the number of edges is the relevant measure of complexity for the decoding algorithm
(Theorem 7.2), the PC-trellis also minimizes decoding complexity.
From (4.4) we know that the set of states at time r, denoted by Sr, is given by the
mapping C →Sr
{sr(x) : sr(x) = x1h1 + · · · + xrhr}.
(4.13)
This mapping is linear due to the linearity of the code and is in general not one=-to-one.
It can be viewed as a vector space of dimension n −k, since this is the dimension of the
vectors sr(x), but is, in general, much less (see, e.g., Figure 4.1). We denote its dimension
by σr, i.e., σr = dim Sr. Note that in the case of binary codes, the dimension σr of the
states space implies that the number of states |Sr| in the trellis at time r is 2σr.
Likewise, an edge, or branch, in the trellis is speciﬁed by the state from where it
originates, the state into which it leads, and the symbol attached to the branch. Formally
we may describe the edge space Br,r+1 at time r + 1 by the triple
{br,r+1(x) = (sr(x), sr+1(x), xr+1)}.
(4.14)
Clearly, the edge mapping is also linear, and we denote its dimension by βr,r+1.
Now, since codewords are formed via x = GT u, we have
sr(x) = Hr(x1, . . . , xr) = HrGT
r u,
(4.15)
where Hr and Gr are the truncated matrices made up of the ﬁrst r columns of H and G,
respectively. So, HrGT
r is a n −k × k matrix, and we have the following lemma:
Lemma 4.1 The state space Sr at the completion of time r is the column space of HrGT
r ,
and, consequently, its dimension σr is the rank of the matrix HrGT
r .

4.5. MINIMAL TRELLISES
121
Note that we can of course span the trellis up backwards from the end by changing
the limits of the sum to l = r + 1 to n in (4.4), and then
sr(x) = Hr(xr+1, . . . , xn)T = HrG
T
r u,
(4.16)
where Hr and Gr are matrices made up of the last n−r columns of H and G, respectively.
The dimensions of HrG
T
r are also n −k × k matrix. Using the fact that the rank of a
matrix is smaller or equal to its smallest dimension and that the rank of the product of
two matrices is equal to or smaller than the rank of each factor matrix, we conclude from
(4.15), (4.16), and the deﬁnitions of Hr, Gr, Hr, and Gr
Lemma 4.2 The dimension σr of the state space Sr at the end of time r is bounded by
σr ≤min(r, n −r, k, n −k).
(4.17)
Lemma 4.2 can be seen nicely in Figure 4.1.
Now, consider all the codewords or code sequences for which xj = 0; j > r. These
sequences are called the past subcode, denoted by Pr, and are illustrated in Figure 4.6
for r = 5 for the PC-trellis for the [7, 4] Hamming code. We denote the dimension of
Pr by pr.
Likewise, we deﬁne the future subcode Fr as the set of code sequences for
which xj = 0; j ≤r, and denote its dimension by fr. The future subcode F2 for the [7, 4]
Hamming code is also shown in Figure 4.6. Clearly, both of these subcodes are linear
subcodes of the original code, i.e., they are subgroups of the original group (code) C.
past subcode P5
future subcode F2
r = 5
r = 2
Figure 4.6: Trellis diagram of the past subcode P5 and the future subcode F2 of the [7, 4]
Hamming code whose full trellis is shown in Figure 4.1.
Now, mathematically, sr(x) is a linear map C →Sr from the code space C, which is k-
dimensional, into the state space Sr at time r, which has dimension σr. Such a linear map

122
CHAPTER 4. TRELLIS REPRESENTATIONS
has a kernel,3 which is the set of code words which are mapped into the state sr(x) = 0
(i.e., all the code words whose trellis paths pass through the zero-state at time r). From
Figure 4.6 we now guess the following
Lemma 4.3 The kernel of the map sr(x) is the sum of the past and future subcodes Pr
and Fr, i.e.,
Ker(sr) = Pr ⊕Fr.
(4.18)
The kernel Ker(sr) is illustrated in Figure 4.7.
Proof: If x ∈Pr⊕Fr, it can be expressed as x = x(p)+x(f), where x(p) ∈Pr and x(f) ∈Fr.
But since x(p) ∈C, Hx(p) = 0, and hence Hr(x1, . . . , xr) = sr(x(p)) = 0.
Trivially,
sr(x(f)) = 0, and since the C →Sr is linear, we conclude sr(x) = sr(x(p) + x(f)) = 0 and
Pr ⊕Fr ⊆Ker(sr).
Conversely, assume sr(x) = 0 and let x = x(p) + x(f) again, where we choose x(p) =
(x1, . . . , xr, 0, . . . , 0) and x(f) = (0, . . . , 0, xr+1, . . . , xn). But due to sr(x) = 0 and (4.15),
we conclude that Hx(p) = 0 and therefore x(p) ∈C and x(p) ∈Pr. Likewise, applying
(4.16) gives Hx(f) = 0 and x(f) ∈C and x(f) ∈Fr. Hence Ker(sr) ⊆Pr ⊕Fr also. Q.E.D.
time: r
r + 1
Figure 4.7: Illustration of the kernel of the map C →Sr as the set of code words which
pass through sr(x) = 0.
As is the case with sr(x), br,r+1(x) is a map from the code space C into the βr,r+1-
dimensional edge space Br,r+1. Its kernel is given by the following lemma:
Lemma 4.4 The kernel of the map br,r+1(x) is the sum of the past and future subcodes
Pr and Fr+1, i.e.,
Ker(br,r+1) = Pr ⊕Fr+1.
(4.19)
3The kernel of a linear map is the subset of point which map to zero under the map. In matrix algebraic
formulation the kernel is the nullspace.

4.5. MINIMAL TRELLISES
123
Proof: From the deﬁnition of the edge mapping (4.14) we see that Ker(br,r+1) must be
contained in Ker(sr) and Ker(sr+1), as well as obey the condition xr+1 = 0. From this
the lemma follows.
Q.E.D.
The following theorem then determines the number of states and edges in the PC-
trellis.
Theorem 4.5 The number of states at depth r in the PC-trellis for a binary code is
|Sr| = 2k−pr−fr,
(4.20)
and the number of edges at depth r is given by
|Br,r+1| = 2k−pr−fr+1.
(4.21)
Proof: Since sr is a linear map from C →Sr, we may apply the rank theorem of linear
algebra [53], i.e.,
dim Ker(sr) + dim |Sr| = dim C,
(4.22)
which leads to pr + fr + σr = k and hence to (4.20). The proof of (4.21) is analogous.
Q.E.D.
We now come to the heart of this section. The optimality of the PC-trellis is asserted
by the following theorem:
Theorem 4.6 The number of states |Sr| and the number of edges |Br,r+1| at depth r in
any trellis which represents the linear block code C are bounded by
|Sr|
≥
2k−pr−fr,
(4.23)
|Br,r+1|
≥
2k−pr−fr+1.
(4.24)
Since the PC-trellis achieves the inequalities with equality, it simultaneously minimizes
both the state and the edge count at every depth r.
Proof: The proof relies on the linearity of the code. Assume that T is a trellis which
represents C, and sr(x) is a state at depth r in this trellis. Now let Csr be the subset of
codewords which pass through sr. Since every codeword must pass through at least one
state at time r,
C =

sr
Csr.
(4.25)
Now consider thecodewords x = [xp xf]T in Csr, where xp = (x1, . . . , xr) is the past
portion of x, and xf = (xr+1, . . . , xn) is the future portion. Then let x(1) =

x(1)
p x(1)
f
T
be a particular codeword in Csr, e.g., the one with minimum Hamming weight.

124
CHAPTER 4. TRELLIS REPRESENTATIONS
Now, both codewords x and x(1) pass through sr. Therefore x =

xp x(1)
f
T
and
x =

x(1)
p xf
T
are two codewords which also pass through the state sr. Hence,
x −x =

0
x(1)
f
−xf

(4.26)
is in the future code Fr, and
x −x =

x(1)
p
−xp
0

(4.27)
is in the past code Pr. We conclude that the diﬀerence of the original two codewords is
x(1) −x =

x(1)
p
−xp
x(1)
f
−xf

= x + x −2x
(4.28)
and that therefore we have
⇒x =
x + x
  
is in Pr⊕Fr
−x(1).
(4.29)
This means that every codeword x is generated from x(1) through the addition of x+x ∈
Pr ⊕Fr. Hence, x is in the coset Pr ⊕Fr −x(1). But the size of this coset is |Pr ⊕Fr|,
and therefore |Csr| ≤2pr+fr, which, together with |C| = 2k, immediately implies (4.23).
The proof of the second part of the theorem is analogous.
Q.E.D.
4.6
Minimum-Span Generator Matrices
The dimensions pr and fr can be found by inspecting the trellis, which, however, is a
rather cumbersome undertaking. In addition to this, once the trellis is constructed, the
number of states and branches at each time are known, and there is no need for the values
pr and fr anymore. There is, however, a simpler way to obtain the dimensions pr and fr
[40]. Let us consider the example of the [7, 4] Hamming code from the last section again,
whose generator matrix is given by
G[7,4] =
⎡
⎢⎢⎣
1
0
0
0
1
1
1
0
1
0
0
1
1
0
0
0
1
0
1
0
1
0
0
0
1
0
1
1
⎤
⎥⎥⎦.
(4.30)

4.6. MINIMUM-SPAN GENERATOR MATRICES
125
By adding row #3 to row #1, then row #2 to row #1, then row #4 to row #3, and ﬁnally
row #3 to row #2, we obtain the following sequence of equivalent generator matrices
(leading and trailing zeros are not shown for better readability)
G[7,4] ≡
⎡
⎢⎢⎣
1
0
1
0
0
1
1
0
0
1
1
1
0
1
0
1
1
0
1
1
⎤
⎥⎥⎦≡
⎡
⎢⎢⎣
1
1
1
0
1
1
0
0
1
1
1
0
1
0
1
1
0
1
1
⎤
⎥⎥⎦
≡
⎡
⎢⎢⎣
1
1
1
0
1
1
0
0
1
1
1
1
1
1
1
0
1
1
⎤
⎥⎥⎦≡
⎡
⎢⎢⎣
1
1
1
0
1
1
1
1
1
1
1
1
1
0
1
1
⎤
⎥⎥⎦.
(4.31)
What we have generated is an equivalent minimum span generator matrix (MSGM) [40]
for the [7, 4] Hamming code, where the span of a matrix is deﬁned as the sum of the spans
of the rows, and the span of a row is deﬁned as its length without the trailing and leading
zeros. More precisely, the span of a row vector x is deﬁned as Span(x) = R(x) −L(x),
where R(x) is the index of the rightmost non-zero entry of x, and L(x) is the index of the
leftmost non-zero entry of x. An MSGM is a generator matrix for C with minimum total
span.
An MSGM can be obtained from an arbitrary generator matrix via the following simple
algorithm:
Step 1: Find a pair of rows x(i) and x(j) in the generator matrix G such that L(x(i)) =
L(x(j)) and R(x(i)) ≤R(x(j)), or R(x(i)) = R(x(j)) and L(x(i)) ≥L(x(j)).
Step 2: If Step 1 fails and no such pair can be found go to Step 4.
Step 3: Let x(i) = x(i) + x(j), i.e., replace the row x(i) by the sum of the two rows. Go
to Step 1.
Step 4: Output G, which is now a MSGM.
We will now prove the following:
Theorem 4.7 The above algorithm always generates an MSGM.
Proof: It is obvious from Step 3 in the algorithm that at each iteration the total span is
reduced by one. The algorithm must therefore terminate in a ﬁnite number of steps, and
it stops exactly when
L(x(i))
=
L(x(j)),
R(x(i))
=
R(x(j))
(4.32)

126
CHAPTER 4. TRELLIS REPRESENTATIONS
for all rows x(i), x(j); x(i) = x(j) of G.
We now need to show that no other generator matrix G can have smaller span than
the one constructed above, which we denote by G. To this end, let x(1), . . . , x(k) be the
rows of G. Then, since G and G are equivalent,
x(j) =

x(i)∈Ij
x(i),
(4.33)
for every j, where Ij is some subset of the set of rows of G. But due to (4.32) the sum in
(4.33) can produce no cancellations at the endpoints of any member of Ij, and therefore
Span(x(j)) = max
x(i)∈Ij
R(x(i)) −min
x(i)∈Ij
L(x(i)) ≥max
x(i)∈Ij
Span(x(i)).
(4.34)
But the k vectors x(j) must contain all x(i)’s, since otherwise G has dimension less than
k and cannot generate C. Since every x(i) is represented in at least one set Ij, there exists
an ordering of the indices i such that
Span(x(j)) ≥Span(x(i))
(4.35)
for every j, and hence also
k

j=1
Span(x(j)) ≥
k

i=1
Span(x(i)),
(4.36)
which proves the theorem
Q.E.D.
We have actually proven that the matrix G has the smallest possible span for every
row (up to row permutations), i.e., it has the smallest span set. From this we immediately
deduce the following
Corollary 4.8 All MSGM’s of a given code C have the same span set.
The signiﬁcance of a MSGM lies in the fact that the dimension pr of the past subcode Pr
and the dimension fr of the future subcode Fr, respectively, can be read oﬀthe generator
matrix as explained by the following theorem.
Theorem 4.9 Given an MSGM G, we obtain
pr = |i : R(x(i)) ≤r|,
(4.37)
i.e., pr equals the number of rows in G for which the rightmost non-zero entry is at position
r or before, and
fr = |i : L(x(i)) ≥r + 1|,
(4.38)
i.e., fr equals the number of rows in G for which the leftmost non-zero entry is at position
r + 1 or later.

4.7. SYSTEMATIC CONSTRUCTION OF THE PC-TRELLIS
127
Proof: pr is the dimension of the past subcode Pr, i.e., the set of all code words ∈C which
merge with the zero-state at position r or earlier. The rows of G are a basis for the code
C; hence the rows of G which merge at time r or earlier, which are all independent, can be
used as a basis for Pr. Due to (4.32), no other row, or linear combination of rows, can be
∈Pr, since then a codeword would have non-zero entries xj, for j > r, and therefore every
codeword in Pr is a linear combination of said rows. There are exactly |i : R(x(i)) ≤r|
qualifying rows. This, together with Corollary 4.8, proves (4.37).
Analogously, the rows of G which start at position r + 1 or later generate the subcode
Fr. Since there are |i : L(x(i)) ≥r + 1| independent rows with that property is proven in
(4.38).
Q.E.D.
We summarize that the PC-trellis is optimal in that it simultaneously reduces the
number of states and edges in its trellis representation of a linear block code C, and,
hence, would logically be the trellis of choice. It remains to point out that, while the
PC-trellis is optimal for a given code, bit position permutations in a code, which generate
equivalent codes, may bring further beneﬁts. The problem of minimizing trellis complexity
allowing such bit permutations is addressed in [25, 23, 24].
We now use Theorem 4.37 to read the values of pr and fr oﬀthe MSGM G[7,4], given
in (4.31). This produces the following table:
r
0
1
2
3
4
5
6
7
pr
0
0
0
0
1
2
3
4
fr
4
3
2
1
0
0
0
0
From this table we calculate |Sr| = 2k−fr−pr and |Br,r+1| = 2k−fr+1−pr, given by
r
0
1
2
3
4
5
6
7
|Sr|
1
2
4
8
8
4
2
1
|Br,r+1|
2
4
8
16
8
4
2
-
These values correspond exactly with those in Figure 4.1, where we have constructed
the trellis explicitly.
4.7
Systematic Construction of the PC-Trellis
The construction of the PC-trellis in Section 4.3 was relatively simple because the code
was systematic and we did not have to deal with a large code. In this section we discuss a
general method to construct the PC-trellis from a MSGM of a block code. This method
was presented by McEliece in [40]. We will use the fact that the rows of G form bases for

128
CHAPTER 4. TRELLIS REPRESENTATIONS
Pr and Fr according to Theorems 4.9 and 4.5, i.e., |Sr| = 2k−pr−fr. Let us start with an
example and consider the MSGM G for the extended [8, 4] Hamming code, given by
G[8,4] =
⎡
⎢⎢⎣
1
1
1
1
0
0
0
0
0
1
0
1
1
0
1
0
0
0
1
1
1
1
0
0
0
0
0
0
1
1
1
1
⎤
⎥⎥⎦.
(4.39)
At r = 0 we have a single starting state, i.e., |S0| = 1. By inspection we see that from r = 1
to r = 3, the ﬁrst row x(1) = (11110000) is active. By that we mean that any codeword
which contains x(1) will be aﬀected by it in positions r = 1 to r = 3 + 1. Likewise, the
second row x(2) is active for r ∈[2, 6], x(3) is active for r ∈[3, 5], and x(4) is active for
r ∈[5, 7]. The basic idea of the trellis construction is that each row in the MSGM is needed
as a basis only where it is active, and each active row doubles the number of states. We
will now formalize this more precisely. Let wr be a row vector with dimension σr whose
ith entry is the ith active row. These vectors are given for G[8,4] above as
w0
w1
w2
w3
w4
w5
w6
w7
w8
-
[w(1)]
w(1)
w(2)
T
⎡
⎣
w(1)
w(2)
w(3)
⎤
⎦
T
w(2)
w(3)
T
⎡
⎣
w(2)
w(3)
w(4)
⎤
⎦
T
w(2)
w(4)
T
[w(4)]
-
We now let the states Sr be the vectors {wr} with w(i)
r
∈{0, 1}. This gives the states
of the trellis in Figure 4.8. Two states in this trellis are connected if either wr ∈wr+1 or
wr+1 ∈wr, e.g., w2 = (01) and w3 = (011) are connected since w2 = (w(1) = 0, w(2) = 1)T
is contained in w3 = (w(1) = 0, w(2) = 1, w(3) = 1).
The trellis branch labels into state wr+1 are determined by the active rows in the two
states at time r and r + 1. More precisely, let us pad the state vectors to equal length
with zeros where needed; for example, ˜w6 = [0, w(2), 0, w(4)]. Now, the branch label is
computed as
xr = [ ˜wr OR ˜wr+1] · gr
T ,
(4.40)
where OR is the bit-wise logic OR operation, and gr is the rth column in the generator
matrix. For instance, x4 on the transition (111) →(11) is given by
˜
(w(1), w(2), w(3))·gr
T =
(1110) · (1110)T = 1. Since the state vector wr indicates which active rows are present
in the codewords which pass through wr, it should be easy to see that Equation (4.40)
simply equals the symbol which these active rows generate at time r.
Now note that the states at time r = 3, r = 5, and r = 5 are redundant, since there is
only one branch leaving these states. We can therefore skip all odd-numbered states and
combine pairs of branches into single branches with two attached symbols. This results

4.8. TAIL-BITING TRELLISES
129
0
1
0
0
0
0
1
1
0
0
1
1
1
0
0
0
1
1
1
0
1
0
0
1
0
0
1
0
00
000
00
000
00
1
01
10
11
01
10
11
0
1
001
010
011
100
101
110
111
100
110
111
001
010
011
01
10
1
1
0
1
0
0
1
1
1
0
0
1
1 0
0
1
1
101
[w(1)]
[w(1), w(2)]
[w(1), w(2), w(3)]
[w(2), w(3)]
[w(2), w(3), w(4)]
[w(2), w(4)]
[w(4)]
Figure 4.8: Trellis of [8, 4] extended Hamming code, constructed in a systematic way.
in a new trellis that has a maximum number of states which is only four, i.e., half that
of the original PC-trellis. This new trellis is shown in Figure 4.9, and we will construct
this trellis again in Section 4.14 in our discussion on Reed–Muller codes.
It becomes
evident that there are many ways of graphically representing a code, and it is not always
straightforward to determine which is the best method.
4.8
Tail-Biting Trellises
The trellises we have considered so far have a well-deﬁned time axis starting at some time
r = 0 and terminating at some time r = n; n > 0. In this section we consider the case of a
tail-biting trellis, in which the index axis is circular, and the trellis “wraps” around from
the end to the beginning. This new view introduces additional degrees of freedom in the
design of the trellis for a given code, and often results in trellises which have signiﬁcantly
smaller complexities than their conventional counterparts. In fact, the square root bound
[63], which we will derive later, asserts that the number of states in a tail-biting trellis
could be as low as the square root of the number of states in the conventional trellis at its
midpoint.
The procedure to construct a tail-biting trellis is exactly analogous to the construction

130
CHAPTER 4. TRELLIS REPRESENTATIONS
[w(1), w(2)]
[w(2), w(3)]
[w(2), w(4)]
Figure 4.9: Contracted PC-trellis for the [8, 4] extended Hamming code, obtained by taking
pairs of symbols as branches.
of the PC-trellis discussed in the previous section; we only need to extend a few deﬁnitions
and generalize the construction procedure. The procedure again starts with the generator
matrix of a code with the only extension that the active span may wrap around from end
to beginning. Consider the following slightly altered generator matrix for the extended
[8,4] Hamming code, given by
G′
[8,4] =
⎡
⎢⎢⎣
1
1
1
1
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
0
1
1
1
1
0
1
1
0
0
1
1
0
⎤
⎥⎥⎦,
(4.41)
which can be constructed by moving the fourth row from (4.39) into third place and
constructing the fourth row in (4.41) by adding rows two and three.
The reason for
this transformation lies in the fact the this will allow us to construct, in fact, a minimal
tail-biting trellis.
The beginning of activity of a row is now arbitrary, and may start at any non-zero
element and extend, possibly wrapping around, to cover the last non-zero element. For
the following we deﬁne the spans in (4.41) as indicated below by the stared entries:
G′
[8,4] =
⎡
⎢⎢⎣


















⎤
⎥⎥⎦;
(4.42)
that is, the activity spans for the four rows of G′[8,4] are [1, 3], [3, 5], [5, 7], and [6, 2], which
wraps around. The state-space vectors wr for G′[8,4] are then given by

4.8. TAIL-BITING TRELLISES
131
w0 = w8
w1
w2
w3
w4
w5
w6
w7
w8

w(4)T
w(1)
w(4)
T w(1)
w(4)
T w(1)
w(2)
T 
w(2)T
w(2)
w(3)
T w(3)
w(4)
T w(3)
w(4)
T 
w(4)T
This leads to the following tail-biting trellis with maximum width four, which is smaller
than the minimal conventional trellis for this code constructed in the last section:
[w(4)]
[w(1), w(4)]
[w(1), w(4)]
[w(1), w(2)]
[w(2)]
[w(2), w(3)]
[w(3), w(4)]
[w(3), w(4)]
[w(4)]
Figure 4.10: Tail-biting trellis for the [8, 4] extended Hamming code.
There exist exactly two non-isomorphic such minimal trellises for the [8,4] extended
Hamming code [6]. Note that the states marked in black are those that ”wrap around”;
that is, a codeword ending in one of them must also start in the same state at the beginning
of the trellis.
Similar to the case of the conventional trellis, we may contract states to generate a
shorter trellis. For example, we may absorb the states at time t = 1, 3, 6, 7, since they are
mere transition states and extend the branch labels to produce the trellis in Figure 4.11.
This trellis has variable numbers of symbols on the branch labels, but it can be seen easily
that if we reverse the second 4-symbol section in the trellis in Figure 4.10, a contracted
trellis with two branch symbols on every branch results. Of course this new trellis describes
a code which is equivalent to the original code only up those symbol permutations.
[w(4)]
[w(1), w(4)]
[w(2)]
[w(2), w(3)]
[w(4)]
Figure 4.11: Contracted tail-biting trellis for the [8, 4] extended Hamming code.

132
CHAPTER 4. TRELLIS REPRESENTATIONS
We will now proceed to bring forth a strong argument that ascertains that the complex-
ity of a tail-biting trellis for a given code can be substantially smaller than the complexity
of the minimal PC-trellis for that code. But ﬁrst we note that we may interpret all trellises
as tail-biting. In the case of a single start and ending state it is simply not necessary to
remember the state across the boundary from end to beginning.
The following theorem is called the cut-set lower bound and is due to Wiberg et al.
[63, 6].
Theorem 4.10 The product of the state space sizes at time r and r′ in a tailbiting trellis
is at least as large as the maximal state space size |Sc| in a conventional trellis for the
same code, i.e.,
|Sr||Sr′| ≥|Sc|.
(4.43)
Proof: We cut the tail-biting trellis into two disjoint sections by splitting the states at r
and r′ into two and cutting the connection between them as shown in Figure 4.12.
p
a
s
t
c
od
e
fu
tu
r
e
c
o
d
e
Sr′
Sr′
Sr Sr
Figure 4.12: Illustration of the cut-set lower bound.
Call the section [r′, r] the past code, and call the section [r, r′] the future code. We
will now construct a conventional trellis from these two pieces which has a state space at
time r which is the product |Sr||Sr′| .
Let Sc = Sr×Sr′ be the Cartesian product of the state spaces at r and r′. The states Sc
will form the state space at time r of a conventional two-section trellis for the code. This
conventional trellis cannot have a smaller total state space than the minimal conventional
trellis, hence the inequality in (4.43).
It remains to show how to construct this two-section trellis. Let the set of branches
from the initial state to state Sc = (Sr, Sr′) of the conventional trellis be the set of paths
from Sr′ to Sr in the past code, and, analogously, let the set of branches from this state
(Sr′, Sr) to the ﬁnal state be the set of paths from Sr to Sr′ in the future code, i.e.,

4.9. THE MINIMAL TRELLIS OF CONVOLUTIONAL CODES
133
the conventional trellis remembers the diﬀerent states Sr′ not at the beginning, but by
extending the state Sr. Since we can combine any codeword from the past code with
any codeword from the future code as long as they share the same states Sr and Sr′,
both trellises describe the same code, but the conventional trellis has a state space of size
|Sr||Sr′|.
Q.E.D.
The following corollary is known as the square-root bound, and is a straightforward
consequence of Theorem 4.10:
Corollary 4.11 The maximum state size |Sr| in a tail-biting trellis is at least as large as
the square root of the maximum state size |Sc| in the minimal conventional trellis for the
same code, i.e.,
|Sr| ≥

|Sc|,
(4.44)
An application of Theorem 4.10 to the trellis of the [8,4] extended Hamming code in
Figure 4.8 shows that a tail-biting trellis with a state space proﬁle (2, 4, 2, 4, 2, 4, 2, 4, 2)
would be possible. However, Calderbank et. al. [6] show through reﬁned arguments that
the minimal state proﬁle for this code is (2, 4, 4, 4, 2, 4, 4, 4, 2), as we constructed in Figure
4.10. The same paper also presents a minimal 16-state tail-biting trellis for the extended
[24,12] Golay code, whose minimal conventional trellis implementation has 256 states.
However, apart from these examples of provably minimal tail-biting codes which achieve
the square root lower bound, little is known on how to systematically construct minimal
tail-biting trellises.
As a last point we wish to note that the trellis of a convolutional code can be made to
tail-bite for any desired length n by preloading the initial states of the shift registers which
generate the trellis code with the state bits it will have at time n. This is particularly
straightforward in the feed-forward realization of an encoder, since there the state bits
equal the information bits. The trellis is now no longer constrained to be driven back to
the all-zero state at time n and has thus a marginally higher rate than a terminated trellis
code.
4.9
The Minimal Trellis of Convolutional Codes
The question of state minimality in the case of convolutional codes has a slightly diﬀerent
ﬂavor than that for general codes. Of course, we can also approach the subject via the
construction of minimum span generators, and it is easily appreciated that such a trellis
has a constant state space, owing to the fact that the parity-check and generator matrices
of the convolutional code are band-diagonal.
However, given that the code is generated from a ﬁnite-state machine, as in Section
4.4, the state space of the trellis can be no larger than the number of distinct states of the
generating ﬁnite-state machine. The question of minimizing the trellis representation is

134
CHAPTER 4. TRELLIS REPRESENTATIONS
therefore the same as that of ﬁnding a minimal encoder for a given code. Such an encoder
has no redundant states, and the state space of the trellis and that of the encoder FSM
are identical.
First, we formally deﬁne a convolutional code in the following deﬁnition.
Deﬁnition 4.2 A rate R = k/n convolutional code over the ﬁeld of binary Laurent series
F2(D) is an injective (one-to-one) linear mapping of the k-dimensional vector Laurent
series u(D) ∈F k
2 (D) into the n-dimensional vector Laurent series v(D) ∈F n
2 (D), i.e.,
u(D) →v(D) :
F k
2 (D) →F n
2 (D).
(4.45)
From basic linear algebra (see, e.g., [26]) we know that any such linear map can be
represented by a matrix multiplication, in our case we write
v(D) = u(D)G(D),
(4.46)
where G(D) is known as a generator matrix of the convolutional code, or simply a gen-
erator, and consists of k × n entries gij(D) which are Laurent series. Algebraically, a
convolutional code is the image set of the linear operator G(D).
We will concentrate on delay-free generator matrices, i.e., those which have no common
multiple of D in the numerator of gij(D). In other words, a general generator matrix
Gn(D) can always be written as
Gn(D) = DiG(D),
i ≥1,
(4.47)
by pulling out the common term Di of all gij(D), where i is the delay. This restriction
does not aﬀect the generality of the results we present in this chapter.
From Deﬁnition 4.2 we see that a convolutional code is the set of output sequences,
irrespective of the particular mapping of input to output sequences, and there exist an
inﬁnite number of generator matrices for the same code. We deﬁne the equivalence of two
generator matrices in the following:
Deﬁnition 4.3 Two generator matrices G(D) and G′(D) are equivalent, written as G(D) ≡
G′(D), if they generate the same convolutional code {v(D)}, where {v(D)} is the set of
all possible output sequences v(D).
Examining Figure 4.3, we can read oﬀquite easily that
G2(D) =
 1
D2
D
D
1
0

(4.48)

4.9. THE MINIMAL TRELLIS OF CONVOLUTIONAL CODES
135
and, upon further examination,4 that
v(D)
=
u(D)G2(D) = u(D)
 1
D2
D
1
 
1
0
D
1+D3
0
1
D2
1+D3

=
u(D)

1
0
D
1+D3
0
1
D2
1+D3

= u(D)G1(D),
(4.49)
where G1(D) is the generator matrix for the encoder shown in Figure 4.2. The set of
sequences {u(D)} is identical to the set of sequences {u(D)}, since
T(D) =
 1
D2
D
1

=⇒T −1(D) =
1
1 + D3
 1
D2
D
1

(4.50)
is invertible, and u(D) = u(D)T(D).
Since we have cast our coding world into the language of linear systems, the matrix
T(D) is invertible if and only if its determinant det(T(D)) = 1 + D3 = 0. In this case the
set of possible input sequences {u(D)} is mapped onto itself; {u(D)} = {u(D)}, as long
as T(D) is a k × k matrix of full rank.
The generator matrix G1(D) above is called systematic, corresponding to the system-
atic encoder5 of Figure 4.2, since the input bits (u(2)
r , u(1)
r ) appear unaltered as (v(2)
r , v(1)
r ).
Both G1(D) and G2(D) have the same number of states, as can be seen from Figures 4.2
and 4.3, and they both generate the same code {v(D)}.
Appealing to linear systems theory again, we know that there are an inﬁnite number
of matrix representations for a given linear map, each such representation amounting to a
diﬀerent choice of bases. Another such choice, or generator, for our code is
G3(D) =
 1 + D
1 + D2
D
D + D2
1 + D
0

.
(4.51)
This new encoder has four delay elements in its realization, and therefore has more states
than G1(D) or G2(D). But there is a more serious problem with G3(D). Since we can
always transform one basis representation into another basis representation via a matrix
multiplication, we ﬁnd that
G3(D) =
1
1
0
1 + D

G2(D) = T3(D)G2(D),
(4.52)
4Note that all coeﬃcient operations are in the ﬁeld GF(2), i.e., additions are EXOR operations, and
multiplications are AND operations.
5We will use the terms generator matrix and encoder interchangeably, realizing that while G(D) may
have diﬀerent physical implementations, these diﬀerences are irrelevant from our viewpoint.

136
CHAPTER 4. TRELLIS REPRESENTATIONS
and the input sequence
u(D) =

0,
1
1 + D = 1 + D + D2 + · · ·

(4.53)
generates the output sequence
v(D) =

0,
1
1 + D

G3(D) = [D, 1, 0],
(4.54)
whose Hamming weight Hw(v(D)) = 2.
We have the unsettling case that an inﬁnite
weight (Hw(u(D)) = ∞) input sequence generates a ﬁnite weight output sequence. Such
an encoder is called catastrophic, since a ﬁnite number of channel errors in the reception of
v(D) can cause an inﬁnite number of errors in the data u(D). Such encoders are suspect.
We deﬁne formally:
Deﬁnition 4.4 An encoder G(D) for a convolutional code is catastrophic if there exists
an input sequence u(D) such that Hw(u(D)) = ∞and Hw(u(D)G(D)) < ∞.
Note that the property of being catastrophic is one of the encoder, since, while G3(D)
is catastrophic, neither G2(D) nor G1(D) are, and all generate the same code! We now
note that the problem stemmed from the fact that
T −1
3 (D) =
1
1
0
1 + D
−1
=
1
1
1+D
0
1
1+D

(4.55)
was not well-behaved, which turned G2(D) into a catastrophic G3(D). The problem was
that some of the entries of T −1
3 (D) have an inﬁnite number of coeﬃcients, i.e., they are
fractional.
Since, in the case of a catastrophic encoder, a ﬁnite-weight sequence v(D) maps into
an inﬁnite-weight sequence u(D), the encoder right inverse6 G−1(D) must have fractional
entries. We therefore require that for a “useful” encoder, G−1(D) must have no fractional
entries; in fact, all its entries are required to be polynomials in D, i.e., they have no
negative powers and only a ﬁnite number of non-zero coeﬃcients. This then is a suﬃcient
condition for G(D) to be not catastrophic. We will see later that it is also a necessary
condition. Note that both G1(D) and G2(D) have polynomial right inverses, i.e.,
G−1
1 (D) =
⎡
⎣
1
0
0
1
0
0
⎤
⎦
(4.56)
6Such an inverse must always exist, since we deﬁned a convolutional code to be a injective map, i.e.,
one which can be inverted.

4.9. THE MINIMAL TRELLIS OF CONVOLUTIONAL CODES
137
and
G−1
2 (D) =
⎡
⎣
1
D
D
1 + D2
D2
1 + D + D3
⎤
⎦
(4.57)
and are therefore not catastrophic.
Let us further deﬁne the class of basic encoders as follows:
Deﬁnition 4.5 An encoder G(D) is basic if it is polynomial and has a polynomial right
inverse.
G2(D), for example, is a basic encoder. We will use basic encoders as a main tool to
develop the algebraic theory of convolutional codes. For a basic encoder we deﬁne the
constraint length
ν =
k

i=1
max
j
(deg (gij(D))) =
k

i=1
νi.
(4.58)
Note that νi = maxj (deg (gij(D))), the maximum degree among the polynomials in row
i of G(D), corresponds to the maximum number of delay units needed to store the ith
input bits u(i)(D) in the controller canonical realization.
Again, we ﬁnd for G2(D) that ν = 3, i.e., the number of delay elements needed in the
controller canonical encoder realization shown in Figure 4.3 is three. Since the inputs are
binary, the number of states of G2(D) is S = 2ν = 8, and we see that for basic encoders
the states and the number of states are easily deﬁned and determined.
One wonders whether 2ν is the minimum number of states which are necessary to gen-
erate a certain convolutional code. To pursue this question further, we need the following
Deﬁnition 4.6 A minimal basic encoder is a basic encoder which has the smallest con-
straint length among all equivalent basic encoders.
We will see later that G2(D) is indeed minimal.
As another example of an equivalent encoder, consider
G4(D) = T4(D)G2(D)
=
1
1 + D + D2
0
1
  1
D2
D
D
1
0

(4.59)
=
1 + D + D2 + D3
1 + D
D
D
1
0

,
(4.60)
whose constraint length ν = 4. Note that, since
T4(D)−1 =
1
1 + D + D2
0
1
−1
=
1
1 + D + D2
0
1

,
(4.61)

138
CHAPTER 4. TRELLIS REPRESENTATIONS
G4(D) has a polynomial right inverse,
G−1
4 (D)
=
G−1
2 (D)
1
1 + D + D2
0
1

(4.62)
=
⎡
⎣
1
D
D
1 + D2
D2
1 + D + D3
⎤
⎦
1
1 + D + D2
0
1

(4.63)
=
⎡
⎣
1
1 + D2
D
1 + D + D3
D2
1 + D + D2 + D4
⎤
⎦.
(4.64)
We have seen above that two encoders are equivalent if and only if T(D) has a non-zero
determinant and is therefore invertible.
We may now strengthen this result for basic
encoders in the following
Theorem 4.12 Two basic encoders G(D) and G(D) are equivalent if and only if G(D) =
T(D)G(D), where T(D) is a k×k polynomial matrix with unit determinant, det(T(D)) = 1.
Proof: Since both T(D) and G(D) are polynomial, and since T(D) has full rank, we
conclude {u(D)G(D)} = {u(D)T(D)G(D)} = {u(D)G(D)}, and G(D) ≡G(D).
Conversely, if G(D) ≡G(D), T −1(D) must exist. Since G(D) is basic, it has a polyno-
mial right inverse, G−1(D), and consequently T −1(D) = G(D)G−1(D) is polynomial also.
Therefore G(D) = T −1(D)T(D)G(D) and T −1(D)T(D) = Ik, the k × k identity matrix.
But since both T(D) and T −1(D) are polynomial, det(T(D))det(T −1(D)) = det(Ik) = 1,
and det(T(D)) = 1.
Q.E.D.
We want to remark at this point that the binary polynomials in D, denoted by F[D],
form a commutative ring, i.e., they possess all the ﬁeld properties except division. Other
“famous” examples of rings are the integer numbers, Z, as well as the integer numbers
modulo m, Zm. Certain elements in a ring do have inverses: They are called units. In Z,
the units are {−1, 1}, in F[D] it is only the unit element 1. In the proof of Theorem 4.12,
we could also have used the following basic algebraic result [26, Page 96]:
Theorem 4.13 A square matrix G with elements from a commutative ring R is invertible
if and only if det(G) = ru, where ru ∈R is a unit, i.e., if and only if det(G) is invertible
in R.
A square polynomial matrix T(D) with a polynomial inverse is also called a scrambler,
since such a T(D) will simply scramble the input sequences {u(D)}, that is, relabel them.

4.10. FUNDAMENTAL THEOREMS FROM BASIC ALGEBRA
139
4.10
Fundamental Theorems from Basic Algebra
In this section we explore what formal algebra has to say about encoders.
With the
preliminaries from the last section, we are now ready for our ﬁrst major theorem. Let us
decompose the basic encoder G(D) into two parts, by splitting oﬀthe largest power terms
in each row, i.e.,
G(D) = ˜G(D) + ˆG(D) = ˜G(D) +
⎡
⎢⎢⎢⎣
Dν1
Dν2
...
Dνk
⎤
⎥⎥⎥⎦Gh,
(4.65)
where Gh is a matrix with (0, 1) entries, a 1 indicating the position where the highest
degree term Dν
i occurs in row i, for example:
G2(D) =
1
0
D
0
1
0

+
D2
D
 0
1
0
1
0
0

(4.66)
and
G4(D) =
1 + D + D2
1 + D
D
0
1
0

+
D3
D
 1
0
0
1
0
0

.
(4.67)
A minimal basic encoder is then characterized by the following theorem [27].
Theorem 4.14 A polynomial G(D) is a minimal basic encoder if and only if
(i) Gh has full rank (det(Gh) = 0), or, equivalently, if and only if
(ii) the maximum degree of all k×k subdeterminants of G(D) equals the constraint length
ν.
Proof: Assume that Gh does not have full rank. Then there exists a sum of d ≤k rows
hij of Gh such that
hi1 + hi2 + · · · + hid = 0,
(4.68)
as in (4.67). Assume now that we have ordered the indices in increasing maximum row
degrees, such that νid ≥νij, d ≥j. Adding
Dνid 
hi1 + hi2 + · · · + hid−1

(4.69)
to row id of ˆG(D) reduces it to an all zero row, and, similarly, adding
Dνid−νi1gi1(D) + Dνid−νi2gi2(D) + · · · + Dνid−νid−1gid−1(D)
(4.70)

140
CHAPTER 4. TRELLIS REPRESENTATIONS
to row id of G(D) will reduce the highest degree of row id and produce an equivalent
generator. The new generator matrix now has a constraint length which is less than that
of the original generator matrix and we have a contradiction to the original assumption
that our G(D) was minimal basic.
After some thought (working with ˜G(D)), it is quite easy to see that conditions (i)
and (ii) in the theorem are equivalent.
Q.E.D.
Since Theorem 4.14 has a constructive proof, there follows a simple algorithm to obtain
a minimal basic encoder from any basic encoder, given by the following:
Step 1: If Gh has full rank, G(D) is a minimal basic encoder and we stop, else
Step 2: let hij, j ≤k be a set of rows of Gh such that
hi1 + hi2 + · · · + hid = 0.
(4.71)
Further let gij(D) be the corresponding rows of G(D), and add
Dνid−νi1gi1(D) + Dνid−νi2gi2(D) + · · · + Dνid−νid−1gid−1(D)
(4.72)
to row id of G(D). Go to Step 1.
Note that a minimal basic encoder for a given convolutional code is not necessarily
unique. For example,
G5(D)
=
1
D
0
1
  1
D2
D
D
1
0

(4.73)
=
1 + D2
D2
D
D
1
0

(4.74)
is a basic encoder with constraint length ν = 3 equivalent to G2(D), and both are minimal.
Our treatment of convolutional encoders has focused on basic encoders so far. We will
now justify why this class of encoders is so important. But before we can continue, we
need some more basic algebra.
Figure 4.13 relates the algebraic concepts needed in the rest of this chapter.
We
start with the (commutative) ring R.
If R is commutative and has no non-zero zero
divisors, i.e., there exist no non-zero numbers r1, r2 ∈R, such that r1r2 = 0, the ring
R is called an integral domain. In an integral domain we may use the cancelation law:
r1r2 = r1r3 ⇒r2 = r3, which is something like division, but not quite as powerful.
Examples of integral domains are Z, F[D] (the ring of binary polynomials), and Zp, the
integers modulo p, when p is a prime.

4.10. FUNDAMENTAL THEOREMS FROM BASIC ALGEBRA
141
On the other hand, a subring N of a ring R is an ideal if, for every r ∈R, rN ∈N and
Nr ∈N. That is, the ideal brings every element of the original ring R into itself through
multiplication. If the ring R (and hence also N) is commutative, we deﬁne a principal
ideal N as one which is generated by all the multiples of a single element n with R, i.e.,
< n >= {rn|r ∈R}. An example of a principal ideal in Z is < 2 >, the ideal of all even
numbers. Now, if every ideal in a commutative ring R is principal, we have a principal
ideal domain. This is an algebraic structure with powerful properties, one of the most well
known of which is the following:
Theorem 4.15 (Unique Factorization Theorem) In a principal ideal domain, every
element can be factored uniquely, up to unit elements, into primes, or irreducibles. These
are elements which cannot be factored.
In Z, this is the popular and famous integer prime factorization.
Remember that
the units in Z are 1 and −1, and hence the primes can be taken positive or negative by
multiplying with the unit element −1. One usually agrees on the convention to take only
positive primes. Note that, technically, this factorization applies to ﬁelds also, (such as
Zp); but, alas, in a ﬁeld every element is a unit, and hence the factorization is not unique.
In F[D], however, the only unit is 1 and we have an unambiguous factorization.
For principal ideal domains we have the important theorem (see, e.g., [26, Page 181 ﬀ],
and [15]):
Theorem 4.16 (Invariant Factor Theorem) Given a k × n matrix P with elements
from the principal ideal domain R, P can be written as
P = AΓB,
(4.75)
where
Γ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
γ1
γ2
0
...
γk
0
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.76)
and γi|γj, if i ≤j. Furthermore, the k × k matrix A and the n × n matrix B both have
unit determinants and are therefore invertible in R. The invariant factors are given by
γi = Δi/Δi−1, where Δi is the greatest common divisor (g.c.d.) of the i×i subdeterminants
of P.

142
CHAPTER 4. TRELLIS REPRESENTATIONS
Principal Ideal Domain
Unique Factorization
(up to units) into
primes (irreducibles)
Integral Domain
Cancellation Law:
r1r2 = r1r3 ⇒r2 = r3
Ideal N
rN ∈N; Nr ∈N{r ∈R}
R is commutative
Ring R
R is commutative
no non-zero
zero divisors
Principal Ideal
< r1 >= {r1r|r ∈R}
Every ideal in principal
Figure 4.13: Diagram of ring-algebraic concepts. Some examples of rings to which these
concepts apply are the integer numbers Z, the integers modulo m, Zm and the polynomials
in D over the ﬁeld F, F[D].
The invariant factor theorem can easily be extended to rational matrices. Let R be a
k × n matrix whose entries are fractions of elements of R. Furthermore, let φ be the least
common multiple of all denominators of the entries in R. We may now apply the invariant
factor theorem to the matrix P = φR, which has all its elements in R, and obtain
φR = AΓ′B ⇒R = AΓB,
(4.77)
where the new entries of Γ are γi = γ′
i/φ, where, again, γi|γj for i ≤j, since γ′
i|γ′
j.

4.10. FUNDAMENTAL THEOREMS FROM BASIC ALGEBRA
143
Applying these concepts to our description of encoders now, we ﬁnd the following
decomposition for a generating matrix G(D) with rational entries:
G(D) = A(D)Γ(D)B(D),
(4.78)
where A(D) is a k × k scrambler and B(D) is an n × n scrambler. Now, since the last
n −k diagonal elements of Γ(D) = 0, we may strike out the last n −k rows in B(D) and
obtain a k × n polynomial matrix, which we call Gb(D). Since A(D) is invertible and
Γ(D) is invertible, G(D) ≡Gb(D), i.e., G(D) = A(D)Γ(D)Gb(D) and Gb(D) generate
the same code. But Gb(D) has entries in F[D], and, furthermore, since B(D) has unit
determinant and has a polynomial inverse, so does Gb(D). We conclude that Gb(D) is a
basic encoding matrix which is equivalent to the original rational encoding matrix G(D).
Hence the following
Theorem 4.17 Every rational encoding matrix has an equivalent basic encoding matrix.
In the process of the above discussion we have also developed the following algorithm
to construct an equivalent basic encoding matrix for any rational encoding matrix:
Step 1: Compute the invariant factor decomposition of the rational matrix G(D), given
by
G(D) = A(D)Γ(D)B(D).
(4.79)
Step 2: Delete the last n−k rows of B(D) to obtain the equivalent basic k ×n encoding
matrix Gb(D).
An algorithm to calculate the invariant factor decomposition is shown in Appendix
4.16.
While we now have a pretty good idea about basic encoders and how to obtain a
minimal version thereof, the question whether some non-polynomial encoders might have
a less complex internal structure is still daunting. We know how to turn any encoder
into a basic encoder and then into a minimal basic encoder, but do we lose anything in
doing so? We are now ready to address this question and we will see that a minimal basic
encoder is minimal in a more general sense than we have been able to show up to now. In
order to do so, we need the concept of abstract states.
An abstract state is, loosely speaking, the internal state of an encoder which produces a
particular output (v(D) in our case), if the input sequence is all zero. This is known as the
zero-input response. Diﬀerent abstract states must generate diﬀerent output sequences,
otherwise they are not distinguishable and are the same state. Obviously, diﬀerent abstract
states correspond to diﬀerent physical states (the states of the shift registers in an encoder
implementation), but diﬀerent physical states might correspond to the same abstract state.

144
CHAPTER 4. TRELLIS REPRESENTATIONS
This will appear again in Chapter 6 as state equivalence. Hence, the number of abstract
states will always be equal to or smaller than the number of physical states of a given
encoder. The number of abstract states is therefore a more basic measure of the inherent
complexity of an encoder.
In order to generate all possible abstract states, we simply use all possible input se-
quences from −∞to time unit 0, at which time we turn oﬀthe input sequences and
observe the set of possible outputs {v(D)}. This set we deﬁne as our abstract states.
To formalize, let P be the projection operator which sets the input sequence to 0 for all
non-negative time units, i.e., u(D)P = (. . . , u−2, u−1, 0, 0, 0, . . .) and let Q be the pro-
jection operator which sets the output sequence to zero for all negative time units, i.e.,
v(D)Q = (. . . , 0, 0, v0, v1, . . .). The abstract state corresponding to an input sequence
u(D) is then formally given by
vS(D) = (u(D)PG(D)) Q.
(4.80)
Our ﬁrst result is
Theorem 4.18 The number of abstract states of a minimal basic encoder is 2ν, equal to
the number of physical states.
Proof: Let
s(D) =

u(k)
−νkD−νk + · · · + u(k)
−1D−1, · · · , u(1)
−ν1D−ν1 + · · · + u(1)
−1D−1
(4.81)
be a physical state of the basic encoder in its controller canonical form. For example, for
the code from Figure 4.3, vS(D) = (u(2)
−2D−2 + u(2)
−1D−1, u(1)
−1D−1). There are 2ν diﬀerent
physical states in our basic encoder. Let us assume now that two diﬀerent physical states
s1(D) and s2(D) correspond to the same abstract state, i.e.,
v(D)Q = s1(D)G(D)Q = s2(D)G(D)Q.
(4.82)
This is equivalent to
(s1(D) −s2(D)) G(D)Q = s(D)G(D)Q = 0,
(4.83)
i.e., , there exists a non-zero state s(D) whose corresponding abstract state is 0, according
to (4.83). We will now show that the only state which fulﬁlls (4.83) is s(D) = 0, and hence
s1(D) = s2(D). This will prove the theorem by contradiction.
We need to have v(D) = 0, i.e., vi = 0 for all i ≥0, in order for (4.83) to hold. Now,
without loss of generality, assume that
νk = νk−1 = · · · = νk−l > νk−l−1 ≥· · · ≥ν1.
(4.84)

4.10. FUNDAMENTAL THEOREMS FROM BASIC ALGEBRA
145
The coeﬃcient vνk of Dνk in v(D) is then given by
v−νk =

0, . . . , 0, u(k−l)
−νk , . . . , u(k)
−νk,

Gh(D) = 0,
(4.85)
but, since Gh(D) has full rank for a minimal basic encoding matrix, we must have u(k)
−νk =
u(k−1)
−νk
= · · · = u(k−1)
−νk
= 0. Continuing by induction, we then prove v−νk−1 = 0 etc., i.e.,
that s(D) = 0, which proves the theorem.
Q.E.D.
We see that the abstract states capture the memory of an encoder G(D) in a very
general way, and we are therefore interested in ﬁnding the G(D) which minimizes the
number of abstract states, realizing that this is all we need to keep track of in the decoder.
Hence the following
Deﬁnition 4.7 A minimal encoder is an encoder G(D) which has the smallest number of
abstract states over all equivalent encoders (basic or not).
Now, if G(D) and G(D) are two equivalent encoders with abstract states vS(D) and
v
S(D), respectively, we can relate these abstract states as follows:
vS(D)
=
u(D)PG(D)Q = u(D)PT(D)G(D)Q
=
u(D)PT(D) (P + Q) G(D)Q
=
u(D)PT(D)PG(D)Q + u(D)PT(D)QG(D)Q,
(4.86)
but the ﬁrst term is an abstract state of G(D), denoted v
S(D), and the second term is a
codeword, i.e.,
v(D)
=
u(D)PT(D)QG(D)Q
(4.87)
=
u(D)PT(D)QG(D),
(4.88)
where we were allowed to drop the operator Q in the above equation, since, for any
realizable encoder, G(D) has to be a causal encoding matrix. Therefore, since u(D) =
u(D)PT(D)Q is an input sequence which starts at time 0, i.e., u
j = 0; j < 0, and since
G(D) must be causal, v(D) cannot have any non-zero negative components either. Hence
the trailing Q-operator in (4.88) is superﬂuous, and we have the representation given in
the following theorem.
Theorem 4.19
vS(D) = v
S(D) + v(D),
(4.89)
where, if G(D) is a minimal basic encoder, the representation (4.89) is unique.

146
CHAPTER 4. TRELLIS REPRESENTATIONS
Proof: To see this, assume the contrary, i.e.,
vS(mb)(D) + v(D) = v
S(mb)(D) + v(D),
(4.90)
that is,
vS(mb)(D) + v
S(mb)(D) = v
S(mb)(D) = v(D) = v(D) + v(D),
(4.91)
and v(D) is both a codeword and an abstract state v
S(mb)(D) of the minimal basic encoder.
But then
v(D) = u(D)G(D) ⇒u(D) = v(D)[G(D)]−1(D),
(4.92)
and u(D) is polynomial since [G(D)]−1 is polynomial. Furthermore, since v
S(mb)(D) =
u(D)G(D)Q, for some input sequence u(D) with no non-zero terms uj, j ≥0, and
v
S(mb)(D) = u(D)G(D)Q = u(D)G(D),
(4.93)
we obtain

u(D) + u(D)

G(D)Q = 0.
(4.94)
Using the same method as in the proof of Theorem 4.18, we can show that (u(D) + u(D)) =
0, which implies u(D) = u(D) = 0, and v
S(mb) = 0, leading to a contradiction. This proves
the theorem.
Q.E.D.
Due to vS(mb)(D) = vS(D) + v(D) and the uniqueness of (4.89), the map vS(D) →
vS(mb)(D) : vS(D) = vS(mb)(D) + v(D) is surjective (i.e., onto), and we have our next
theorem.
Theorem 4.20 The number of abstract states of an encoder G(D) is always larger than
or equal to the number of abstract states of an equivalent minimum basic encoder Gmb(D).
We also immediately notice the following
Corollary 4.21 A minimal basic encoder is a minimal encoder.
While this proves that the minimal basic encoders are desirable since they represent
an implementation with the minimum number of abstract states, more can be said about
minimal encoders in general, expressed by the following central theorem.
Theorem 4.22 G(D) is a minimal encoder if and only if
(i) its number of abstract states equals the number of abstract states of an equivalent
minimal basic encoder, or, if and only if

4.10. FUNDAMENTAL THEOREMS FROM BASIC ALGEBRA
147
(ii) G(D) has a polynomial right inverse in D and also a polynomial right inverse in
D−1.
Proof:
(i) Part (i) is obvious from Theorem 4.20.
(ii) Let u(D) be given and assume that the output sequence v(D) = u(D)G(D) is
polynomial in the inverse power D−1, i.e., vr = 0 for r > 0 and r < s, where s ≤0
is arbitrary. Then
D−1v(D)Q = 0.
(4.95)
We now show that u(D) must also be polynomial in D−1. Breaking up (4.95) into
two components, a past and a future term, we obtain
D−1v(D)Q
=
0 = D−1u(D) (P + Q) G(D)Q
=
D−1u(D)PG(D)Q + D−1u(D)QG(D)Q,
(4.96)
and, since G(D) is causal, D−1u(D)QG(D)Q = D−1u(D)QG(D) is also a codeword.
Therefore the abstract state D−1u(D)PG(D)Q is a codeword, and, following (4.91)
ﬀ., we conclude that it must be the zero codeword.
Since G(D) has full rank,
D−1u(D)QG(D) = 0 implies
D−1u(D)Q = 0,
(4.97)
i.e., u(D) contains no positive powers of D. Since v(D) = u(D)G(D) and G(D) is
delay-free, u(D) must be polynomial in D−1, i.e., ur = 0 for r < s. Therefore the
map v(D) →u(D) = G−1(D)v(D) maps polynomials in D−1 into polynomials in
D−1, and G−1(D), the right inverse of G(D), must be polynomial in D−1 also.
The fact that a minimal encoder G(D) also has a polynomial right inverse in D is
proven similarly, i.e., assume this time that v(D) = u(D)G(D) is polynomial in D.
Then
v(D) = u(D)PG(D) + u(D)QG(D),
(4.98)
where u(D)Q is a power series (only positive terms).
Then, due to causality,
u(D)QG(D) is also a power series, which implies that u(D)QG(D) must also be
a power series. But now
u(D)PG(D) = u(D)PG(D)Q
(4.99)
is again both a codeword and an abstract state, which implies that it must be the
zero codeword, i.e., u(D)PG(D) = 0. But since G(D) has full rank we conclude
that u(D)P = 0, that is, u(D) is a power series. Now from above take G−1(D−1), a

148
CHAPTER 4. TRELLIS REPRESENTATIONS
polynomial right inverse in D−1. Then G−1(D) = DsG−1(D−1) is (pseudo)-inverse
which is polynomial in D such that
u(D)Ds = v(D)G−1(D−1),
(4.100)
and hence a polynomial v(D) can only generate a polynomial u(D), i.e., u(D) has
only ﬁnitely many terms. Therefore the inverse map G−1(D) : v(D) →u(D) =
v(D)G−1(D) must also be polynomial.
Now for the reverse part of the theorem assume that G(D) has a polynomial inverse
in D−1 and in D. Assume further that the abstract state vs(D) = u(D)G(D)Q =
u(D)G(D) is a codeword, and that u(D) is polynomial in D−1 without a constant
term. vs(D) is therefore a power series and since G(D) has a polynomial inverse in
D by assumption, it follows that u(D) is also a power series. Now, using the right
inverse G−1(D−1) which is polynomial in D−1, we write
u(D)G(D)G−1(D−1)
=
u(D)G(D)QG−1(D−1),
u(D)
=
u(D)G(D)QG−1(D−1).
(4.101)
But the left-hand side in (4.101) has no negative powers in D, and the right-hand
side has no positive powers since u(D)G(D)Q has no positive powers and G−1(D−1)
is polynomial in D−1. We conclude that vs(D) = 0. Using Theorems 4.19 and 4.20,
we conclude that G(D) is minimal.
Q.E.D.
As pointed out by Johannesson and Wan, part (ii) of the theorem provides us with a
practical minimality test for encoders. Furthermore, since a minimal encoder G(D) has a
polynomial right inverse (G(D))−1, it follows immediately that G(D) is not catastrophic,
i.e., we have the following corollary.
Corollary 4.23 A minimal encoder is not catastrophic.
Johannesson and Wan pointed out the interesting fact [27] that there are minimal
encoders which are not minimal basic. Quoting their example, the basic encoder
G(D) =

1 + D
D
1
1 + D2 + D3
1 + D + D2 + D3
0

(4.102)
has constraint length ν = 4, but is not minimal basic. In fact
G(D) =
1 + D
D
1
D2
1
1 + D + D2

(4.103)
is an equivalent minimal basic encoder with constraint length ν = 3.

4.11. SYSTEMATIC ENCODERS
149
Nevertheless, G(D) has a polynomial right inverse in D−1, given by

G(D−1)
−1 =
⎡
⎣
1 + D−1 + D−2 + D−3
D−1
1 + D−1 + D−3
D−1
D−2 + D−3
D−1
⎤
⎦
(4.104)
and is therefore, according to Theorem 4.22, a minimal encoder with 23 = 8 abstract
states.
4.11
Systematic Encoders
The encoder in Figure 4.2, whose encoding matrix is
Gs(D) =

1
0
D
1+D3
0
1
D2
1+D3

,
(4.105)
is systematic, i.e., the k information bits appear unchanged in the output sequence v(D).
We now show that every code has a systematic encoder. Let us assume without loss
of generality that the code is generated by the basic encoder G(D). Then G−1(D) exists
and is a polynomial matrix. Using the invariant factor theorem we write
G−1(D) =

A(D)Γ(D)B(D)
−1 = B−1(D)Γ−1(D)A−1(D),
(4.106)
and, since B−1(D) and A−1(D) are polynomial, Γ−1(D) must be a polynomial also. This
means that all the invariant factors must be units, i.e., γi = 1, and thus the greatest
common divisor (g.c.d.) of the k × k subdeterminants of G(D) must equal 1. It follows
that there exists a k ×k subdeterminant Δk(D) of G(D), which is a delay-free polynomial
(no multiple of D), since otherwise the g.c.d. of all the k × k subdeterminant would be a
multiple of D.
We now rearrange the columns of G(D) such that the ﬁrst k columns form that matrix
T(D) whose determinant is Δk(D). Since T(D) is invertible, we form
Gs(D) = T −1(D)G(D) = [Ik|P(D)] ,
(4.107)
where P(D) is a n−k×k parity-check matrix with (possibly) rational entries. For example,
for the code from Figure 4.2 we have
P(D) =

D
1+D3
D2
1+D3

.
(4.108)
Note that the systematic encoder for a given code is unique, whereas there may exist
several diﬀerent equivalent minimal basic encoders for the same code.
Systematic encoders have another nice property, given by the following theorem.

150
CHAPTER 4. TRELLIS REPRESENTATIONS
Theorem 4.24 Every systematic encoder for a convolutional code is minimal.
Proof: The proof of this result comes easily from Theorem 4.22. Consider the inverse of
a systematic encoder, which is simply the k × k identity matrix, which, trivially, is both
polynomial in D and in D−1.
Q.E.D.
To obtain a systematic encoder for a given code, the procedure outlined above is
practical, i.e., we ﬁnd a k × k submatrix whose determinant is a polynomial which is not
a multiple of D. Then simply calculate (4.107) to obtain Gs(D).
The inverse operation is more diﬃcult. Given a systematic encoder Gs(D), we want
to ﬁnd an equivalent minimal basic encoder. We can proceed by calculating the invariant
factor decomposition of Gs(D). This will give us a basic encoder. Then we apply the
algorithm on page 140 to obtain a minimal basic encoder.
Often it is easier to apply a more ad hoc technique, based on examining the trellis
of a given code [48]. Let us take the example of Figure 4.2 again (Equation (4.105)).
This systematic encoder has 8 states, and, since systematic encoders are minimal, we are
looking for a minimal basic encoder with 8 states also. The code rate is 2/3, and therefore
the two values for ν1, ν2 must be 1 and 2 (or 0 and 3, which is quickly ruled out). The
minimal basic encoder blueprint for our code is shown in Figure 4.14.
g2,0
g2,1
g2,2
g1,0
g1,1
g2,0
g2,1
g2,2
g2,0
g2,1
g2,2
g1,0
g1,1
g1,0
g1,1
(2)
(2)
(2)
(1)
(1)
(1)
(0)
(0)
(0)
(0)
(0)
(1)
(1)
(2)
(2)
+
+
+
+
+
+
+
+ +
+
+
+
u(2)
r
u(1)
r
v(2)
r
v(1)
r
v(0)
r
Figure 4.14: Encoder blueprint for the minimal basic encoder equivalent to GS(D).
We now look at the trellis generated by the systematic encoder, whose ﬁrst few tran-
sitions originating from the zero-state are shown in Figure 4.15. We start out by trac-

4.12. MAXIMUM FREE-DISTANCE CONVOLUTIONAL CODES
151
ing paths from the zero-state back to the zero-state and map them into the connectors
g(j)
1,m and g(j)
2,m.
The ﬁrst merger occurs after two branches with the output sequence
(v0, v1) = ((010), (100)). The ﬁrst triple v0 is generated by setting g(2)
1,0 = 0, g(1)
1,0 = 1 and
g(0)
1,0 = 0. The second triple v1 is generated by setting g(2)
1,1 = 1, g(1)
1,1 = 0 and g(0)
1,1 = 0. Now
we move onto the next path. It is advantageous to choose paths for which u(1)
r
= 0, since
then the connectors g(j)
1,m will not interfere. One such path is (100), (001), (010). From this
we can determine the connectors g(2)
2,0 = 1, g(1)
2,0 = 0, and g(0)
2,0 = 0. From the second triple
we obtain g(2)
2,1 = 0, g(1)
2,1 = 0, and g(0)
2,1 = 1 and, from the third triple g(2)
2,2 = 0, g(1)
2,2 = 1, and
g(0)
2,2 = 0. Checking back with Figure 4.3, we see that we have obtained the same encoder.
(Note that choosing other paths could have generated another, equivalent encoder.) This
procedure can easily be generalized for larger encoders.
(100)
(100)
(100)
0
1
2
3
4
5
6
7
(010)
(010)
Figure 4.15: Initial trellis section generated by the systematic encoder from Figure 4.2.
4.12
Maximum Free-Distance Convolutional Codes
This chapter has developed fundamental results of convolutional codes and their encoders.
These results do not give explicit constructions for good convolutional codes and, as in the
case for trellis codes, computer searches are usually used to ﬁnd good codes [31, 46, 47].

152
CHAPTER 4. TRELLIS REPRESENTATIONS
Unlike trellis codes, convolutional code searches can be based on Hamming distance.
If the output bits v(D) of a convolutional code are mapped into the signals {−1, +1} of
a binary phase-shift keyed signal constellation, the minimum squared Euclidean distance
d2
free depends only on the number of binary diﬀerences between the closest code sequences.
This number is the minimum Hamming distance of a convolutional code, denoted by dfree.
Since convolutional codes are linear, ﬁnding the minimum Hamming distance between two
sequences v(1)(D) and v(2)(D), Hd(v(1)(D), v(2)(D)), amounts to ﬁnding the minimum
Hamming weight of any code sequence v(D).
Finding convolutional codes with large
minimum Hamming distance is as diﬃcult as ﬁnding good trellis codes with large free
distance. Most often the controller canonical form (Figure 4.3) of an encoder is preferred
in these searches, which is targeted at ﬁnding a minimal basic encoder. The procedure
is then to search for a code with the largest minimum Hamming weight by varying the
connector taps g(j)
i,m, either exhaustively or according to heuristic rules. Once an encoder
is found, it is tested for minimality, which will then ensure that it is not catastrophic.
In this fashion the codes in Tables 4.1–4.7 were found [11, 31, 33, 49]. They are the
rate R = 1/n, with n = 1, 2, . . . , 8 and R = 2/3 codes with the largest minimum Hamming
distance dfree for a given constraint length. The free distance of a convolutional code is
the most common parameter optimized in code searches. Extensive tables of a variety of
good convolutional codes may be found in Chapter 8 of [28] and the references therein.
ν
g(1)
g(0)
dfree
2
5
7
5
3
15
17
6
4
23
35
7
5
65
57
8
6
133
171
10
7
345
237
10
8
561
753
12
9
1161
1545
12
10
2335
3661
14
11
4335
5723
15
12
10533
17661
16
13
21675
27123
16
14
56721
61713
18
15
111653
145665
19
16
347241
246277
20
g(2)
g(1)
g(0)
dfree
2
5
7
7
8
3
13
15
17
10
4
25
33
37
12
5
47
53
75
13
6
133
145
175
15
7
225
331
367
16
8
557
663
711
18
9
1117
1365
1633
20
10
2353
2671
3175
22
11
4767
5723
6265
24
12
10533
10675
17661
24
13
21645
35661
37133
26
Table 4.1: Connectors and free Hamming distance of the best R = 1/2 convolutional
codes [33] on the left. The connectors are given in octal notation, e.g., g = 17 = 1111,
and connectors and dfree of the best R = 1/3 convolutional codes on the right [33].

4.12. MAXIMUM FREE-DISTANCE CONVOLUTIONAL CODES
153
g(2)
2 , g(2)
1
g(1)
2 , g(1)
1
g(0)
2 , g(0)
1
dfree
2
3, 1
1, 2
3, 2
3
3
2, 1
1, 4
3, 7
4
4
7, 2
1, 5
4, 7
5
5
14, 3
6, 10
16, 17
6
6
15, 6
6, 15
15, 17
7
7
14, 3
7, 11
13, 17
8
8
32, 13
5, 33
25, 22
8
9
25, 5
3, 70
36, 53
9
10
63, 32
15, 65
46, 61
10
Table 4.2: Connectors and dfree of the best R = 2/3 convolutional codes [33].
g(3)
g(2)
g(1)
g(0)
dfree
2
5
7
7
7
10
3
13
15
15
17
13
4
25
27
33
37
16
5
53
67
71
75
18
6
135
135
147
163
20
7
235
275
313
357
22
8
463
535
733
745
24
9
1117
1365
1633
1653
27
10
2387
2353
2671
3175
29
11
4767
5723
6265
7455
32
12
11145
12477
15537
16727
33
13
21113
23175
35527
35537
36
Table 4.3: Connectors and dfree of the best R = 1/4 convolutional codes [31].
g(4)
g(3)
g(2)
g(1)
g(0)
dfree
2
7
7
7
5
5
13
3
17
17
13
15
15
16
4
37
27
33
25
35
20
5
75
71
73
65
57
22
6
175
131
135
135
147
25
7
257
233
323
271
357
28
Table 4.4: Connectors and dfree of the best R = 1/5 convolutional codes [11].

154
CHAPTER 4. TRELLIS REPRESENTATIONS
g(5)
g(4)
g(3)
g(2)
g(1)
g(0)
dfree
2
7
7
7
7
5
5
16
3
17
17
13
13
15
15
20
4
37
35
27
33
25
35
24
5
73
75
55
65
47
57
27
6
173
151
135
135
163
137
30
7
253
375
331
235
313
357
34
Table 4.5: Connectors and dfree of the best R = 1/6 convolutional codes [11].
g(6)
g(5)
g(4)
g(3)
g(2)
g(1)
g(0)
dfree
2
7
7
7
7
5
5
5
18
3
17
17
13
13
13
15
15
23
4
35
27
25
27
33
35
37
28
5
53
75
65
75
47
67
57
32
6
165
145
173
135
135
147
137
36
7
275
253
375
331
235
313
357
40
Table 4.6: Connectors and dfree of the best R = 1/7 convolutional codes [11].
4.13
The Squaring Construction and the Trellis of Lattices
In this section we take a reverse approach to that of the previous sections, that is, we
construct lattices and codes by constructing their trellises ﬁrst.
Lattices were used in
Chapter 3 as signal sets for trellis codes, and we now learn that they, too, can be described
by trellises. To this end we introduce a general building block, the squaring construction,
which has found application mainly in the construction of lattices [9].
Let us assume that some set S of signals is the union of M disjoint subsets Ti. This
is a partition of S into M subsets, and we denote this partition by S/T. Furthermore,
g(7)
g(6)
g(5)
g(4)
g(3)
g(2)
g(1)
g(0)
dfree
2
7
7
5
5
5
7
7
7
21
3
17
17
13
13
13
15
15
17
26
4
37
33
25
25
35
33
27
37
32
5
57
73
51
65
75
47
67
57
36
6
153
111
165
173
135
135
147
137
40
7
275
275
253
371
331
235
313
357
45
Table 4.7: Connectors and dfree of the best R = 1/8 convolutional codes [11].

4.13. THE SQUARING CONSTRUCTION AND THE TRELLIS OF LATTICES
155
let d(S) be the minimum Euclidean distance between any two elements of S, i.e., d(S) =
min s1,s2∈S
(s1=s2) |s1 −s2|2, and let d(Tj) be the minimum distance between any two elements of
the subset Tj. Deﬁne d(T) = minj Tj as the minimum distance between elements in any
one of the subsets. As an example consider the lattice partition Λ = Λ′ + [Λ/Λ′] from
Chapter 3, where the subsets are the cosets of the sublattice Λ′.
We now deﬁne the squaring construction [16] as the set of signals U, given by the
following deﬁnition.
Deﬁnition 4.8 The union set U, also denoted by |S/T|2, resulting from the squaring
construction of the partition S/T is the set of all pairs (s1, s2), such that s1, s2 ∈Tj, i.e.,
s1 and s2 are in the same subset Tj.
The squaring construction can conveniently be depicted by a trellis diagram, shown in
Figure 4.16. This trellis has M states, corresponding to the M subsets Tj of S. This is
logical, since we need to remember the subset of s1 in order to restrict the choice of s2 to
the same subset.
T1
T2
T3
TM
T1
T2
T3
TM
Figure 4.16: Trellis diagram of the squaring construction.
Continuing our lattice example, let us apply the squaring construction to the lattice
partition Z2/RZ2 = Z2/D2 (compare Figure 3.13, Chapter 3), where T1 is the lattice RZ2
and T2 is its coset RZ2 + (0, 1). This is illustrated in Figure 4.17, and we see that this
construction yields D4, the Schl¨aﬂi lattice, whose points have an even coordinate sum.
(The points in RZ2 have even coordinate sums and the points in its coset RZ2 + (0, 1)
have odd coordinate sums.)
Note that in applying the squaring construction to Z2/D2 we have obtained another
lattice, D4, i.e., any two points d1, d2 ∈D4 can be added as 4-dimensional vectors to
produce d3 = d1 + d2 ∈D4, another point in the lattice. That this is so can be seen by
inspection of Figure 4.17. This property results of course from the fact that RZ2 is a
subgroup of Z2 under vector addition (i.e., a sublattice), inducing the group partition Z2/
RZ2. Our lattice examples have therefore more algebraic structure than strictly needed
for the squaring construction.

156
CHAPTER 4. TRELLIS REPRESENTATIONS
RZ2
RZ2
RZ2 + (0, 1)
RZ2 + (0, 1)
Figure 4.17: Using the squaring construction to generate D4 from the partition Z2/D2.
The squaring construction is an important tool for the following reason, stated in the
following lemma.
Lemma 4.25 Given the distances d(T) and d(S) in the partition S/T, the minimum
distance d(U) of U obeys
d(U) = min[d(T), 2d(S)].
(4.109)
Proof: There are two cases we need to examine:
(a) If the two elements u1, u2 achieving d(U) have their ﬁrst components, t1, t2 ∈Tj,
t1 = t2, in the same subset, i.e., u1 = (t1, s1), u2 = (t2, s2), where s1 = s2, then their
distance |u1 −u2|2 = d(T), and, consequently, d(U) = d(T).
(b) If their ﬁrst components t1, t2 lie in diﬀerent subsets Tj and Ti, their second compo-
nents s1, s2 must lie in the same two respective subsets by virtue of the squaring con-
struction, i.e., s1 ∈Tj and s2 ∈Ti, Since Tj = Ti, ⇒s1 = s2. Likewise, t1 = t2, and,
since t1, t2, s1, and s2 can be chosen such that d(t1, t2) = d(S) and d(s1, s2) = d(S),
we conclude that d(U) = 2d(S) in this case. This proves the lemma.
Q.E.D.
We now extend the squaring construction to two levels. Let S/T/V be a two-level
partition chain, that is, each subset Tj is made up of P disjoint subsets Vji. Let Tj × Tj =
|Tj|2 be the set of all elements (t1, t2); t1, t2 ∈Tj. Then each |Tj|2 can be represented
by the trellis in Figure 4.18, i.e., each element t1 ∈Vji can be combined with a second
element t2 from any Vji ∈Tj, and we have a 2-stage trellis with P states representing
|Tj|2 in terms of its subsets Vji. Since each |Tj|2 can be broken down into a trellis like the
one in Figure 4.18, we are led to the two-level representation of the set U shown in Figure
4.19. From the above, it should be straightforward to see that Figures 4.16 and 4.19 show
the same object with diﬀerent degrees of detail.
There is another way of looking at Figure 4.18. The paths to the ﬁrst node at the second
stage (solid paths in the ﬁgure) are in fact the set obtained by the squaring construction

4.13. THE SQUARING CONSTRUCTION AND THE TRELLIS OF LATTICES
157
of the partition Tj/Vj, denoted by Wj. Obviously, Wj is a subset of |Tj|2. In fact, |Tj|2
is the union of P sets Wji, each of which, except for Wj, is obtained from a squaring-
type construction similar to the squaring construction, but with the indices of the second
component sets permuted cyclically. This is called a twisted squaring construction.
Vj1
Vj2
Vj3
VjP
Wj1 = Wj
Wj2
Wj3
WjM
Figure 4.18: Representation of |Tj|2 = Tj × Tj, using the second level partition T/V .
V11
V12
V13
V1P
VM1
VM2
VM3
VMP
S
T
V
U
W
M
fold
P
fold
MP
fold
Figure 4.19: Representation of the original squaring construction reﬁned to the two-level
partition S/T/V and schematic representation of the induced partition U/W.
This then induces an MP-partition of the set U, denoted by U/W, as shown in Fig-
ure 4.19; i.e., the set U is partitioned into MP disjoint sets Wji. The right-hand side of
Figure 4.19 shows a schematic diagram of this construction. The partition chain S/T/V
induces a partition U/W via the squaring construction, whereby U is obtained from the
squaring construction |S/T|2 and W from |T/V |2. Since the partition S/T is M-fold and
the partition T/V is P-fold, the resulting partition U/W is MP-fold.

158
CHAPTER 4. TRELLIS REPRESENTATIONS
We now apply the squaring construction to the new partition U/W and obtain a two-
level squaring construction (Figure 4.20). This is done by concatenating back to back two
trellis sections of the type in Figure 4.19, just as in the one-level squaring construction.
We denote the two-level squaring construction by |S/T/V |4, which is, in fact, identical to
the one-level squaring construction |U/W|2. Using Lemma 4.25 twice, we get
d(|S/T/V |4)
=
min[d(W), 2d(U)]
=
min[d(V ), 2d(T), 4d(S)].
(4.110)
V11
V12
V13
V1P
VM1
VM2
VM3
VMP
V11
V12
V13
V1P
VM1
VM2
VM3
VMP
Figure 4.20: Two-level squaring construction for the partition S/T/V .
As an example of a two-level squaring construction, we start with the binary lattice
partition chain, Z2/RZ2/2Z2. The resulting lattice7 has eight dimensions and its 4-state
trellis diagram is shown in Figure 4.21. Its minimum squared Euclidean distance between
lattice points is d(|Z2/RZ2/2Z2|4) = 4. This is the famous Gosset lattice E8 [9].
This game of two-level squaring constructions may now be continued with the newly
constructed lattice, i.e., we can recursively construct the lattices Λ(n) = |Λ(n−1)/RΛ(n−
1)|2 = |Λ(n −2)/RΛ(n −2)/2Λ(n −2)|4 starting with D4 = Λ(1) = |Z2/RZ2|2 and
E8 = Λ(2) = |Z2/RZ2/2Z2|4. The sequence of these lattices is known as the sequence of
Barnes-Wall lattices ΛN = Λ(n) of dimension N = 2n+1 and minimum squared Euclidean
distance 2n = N/2 [16, 9].
The minimum squared distance results from the squaring construction, i.e., d(ΛN)=
min

d(RΛN/2), 2d(ΛN/2)

= min

d(2ΛN/4), 2d(RΛN/4), 4d(ΛN/4)

.
The distance sequence
now follows by induction, starting with d(Λ2 = Z2) = 1, and d(Λ4 = D4) = 2 and using
the fact that d(RΛ) = 2d(Λ), i.e., d(ΛN) = 2d(ΛN/2) = 4d(ΛN/4), and hence d(ΛN) = N/2.
7The fact that the resulting union set is also a lattice is argued analogously to the case of D4.

4.13. THE SQUARING CONSTRUCTION AND THE TRELLIS OF LATTICES
159
2Z2
2Z2
2Z2+(1,1)
2Z2+(1,0)
2Z2+(0,1)
2Z2
2Z2+(0,1)
2Z2+(1,1)
2Z2+(1,0)
2Z2
2Z2+(0,1)
2Z2+(0,1)
2Z2+(0,1)
2Z2+(0,1)
2Z2+(1,1)
2Z2+(1,1)
2Z2+(1,1)
2Z2+(1,1)
2Z2+(1,0)
2Z2+(1,0)
2Z2+(1,0)
2Z2+(1,0)
2Z2
2Z2
RZ2
RZ2+(1,0)
RD4+(0,0,1,1)
RD4+(0,1,1,0)
RD4
RD4+(0,1,0,1)
RD4+(0,0,1,1)
RD4+(0,1,1,0)
RD4
RD4+(0,1,0,1)
Figure 4.21: Gosset lattice E8 obtained via the two-level squaring construction.
The
subsets are the cosets Z2 + c, where c is the coset representative (Section 3.5). The lower
ﬁgure shows E8 as a single-level squaring construction from the partition U/V = D4/RD4.
Note that, as can be seen from the squaring construction, ΛN has the same minimum
distance as RΛN/2 ×RΛN/2, namely N/2, but has 2N/4 times as many lattice points (2N/4
is also the number of cosets in the ΛN/2/RΛN/2 partition). The asymptotic coding gain of
ΛN, given by γ(ΛN) = d(ΛN)/V (ΛN)2/N (see Equation (3.9)), is therefore 21/2 times that
of RΛN/2 × RΛN/2, which is also the coding gain of ΛN/2. Starting with γ(Z2) = 1 this
leads to an asymptotic coding gain of the Barnes-Walls lattices given by γ = 2n/2 = N/2,
which increases without bound. The relationships and construction of the inﬁnite sequence
of Barnes-Walls lattices is schematically represented in Figure 4.22, which also includes
the relevant parent lattices.
Since the partitions ΛN/RΛN and RΛN/2ΛN are both of the order 2N/2, the number
of states in the trellis diagram for Λ4N resulting from the two-level squaring construction

160
CHAPTER 4. TRELLIS REPRESENTATIONS
Z2
RZ2
2Z2
D4
RD4
2D4
E8
RE8
2E8
R16
RR16
2R16
R32
RR32
2R32
R1
RR1
R2
R1
RR1
2R1
R3
2
4
8
16
32
1
2
4
8
16
32
64
2
2
2
2
2
32
16
8
2
4
4
16
128
2048
215
256
16
216
256
216
1.3dB
2 states
3dB
4 states
6dB
256  states
4.8dB
16 states
dmin
dimension
Two-Level Squaring Construction
Squaring Construction
Figure 4.22: Schematic representation of the relationships of the inﬁnite sequence of
Barnes-Walls and related lattices. The small numbers indicate the order of the lattice
partition |Λ/Λ′|. The asymptotic coding gain and the number of trellis states is also given
for the main sequence of lattices.
is 2N, and therefore the number of states in the trellis of the Barnes-Walls lattices ΛN is
given by 2N/4 = 22n−1, which grows exponentially with the dimension N. Thus we have
2 states for D4, 4 states for E8, 16 states for Λ16, and then 256, 65,536, and 264 states
for Λ32, Λ64, and Λ128, respectively; and the number of states, or the complexity of the
lattice, also increases without bound.
If we want to use E8 as a code for a telephone line modem, we would choose quadrature
modulation and transmit four 2-dimensional signals to make up one 8-dimensional code
word or lattice point. A typical baud rate over telephone channels is 2400 symbols/second
(baud). To build a modem which transmits 9600 bits/s, we require 16 signal points every
two dimensions, and the total number of signal points from E8 is 216 = 65,536. In order

4.14. THE CONSTRUCTION OF REED–MULLER CODES
161
to transmit 14,400 bits/s, we already need 224 signal points, approximately 17 million. It
becomes evident that eﬃcient decoding algorithms are needed since exhaustive look-up
tables clearly become infeasible. The trellis structure of the lattices provides an excellent
way of breaking down the complexity of decoding, and we will see in Section 4.15 that a
decoder for the E8 lattice becomes rather simple indeed. Further aspects of implementation
of lattice modems are discussed by Lang and Longstaﬀ[30] and by Conway and Sloane [10].
4.14
The Construction of Reed–Muller Codes
If instead of the integer lattices, we use the binary ﬁeld GF(2) with {0, 1}, we can apply
the same squaring construction to build the entire class of Reed–Muller codes.
We start with the two-level partition chain (2, 2)/(2, 1)/(2, 0) of length-two binary
codes. (2, 2) is the binary code over GF(2) consisting of all four length-2 binary code
words. (2, 1) is its subcode consisting of the code words {(0, 0), (1, 1)}, and (2, 0) is the
trivial single code word (0, 0). Since (2, 1) is a subgroup of (2, 2) under vector addition
over GF(2), (2, 2)/(2, 1) is a true partition, and the same holds for (2, 1)/(2, 0). We now
deﬁne [16] the Reed–Muller codes RM(r, n) with parameters r, n recursively as
RM(r, n) = |RM(r, n −1)/RM(r −1, n −1)|2
(4.111)
and start the recursion with RM(−1, 1) = (2, 0), RM(0, 1) = (2, 1), and RM(1, 1) = (2, 2).
The ﬁrst two codes constructed are illustrated in Figure 4.23 below. The code RM(1, 2) =
|(2, 2)/(2, 1)|2 is a (4, 3) single parity-check code, and the code RM(0, 2) = |(2, 1)/(2, 0)|2
is the (4, 1) repetition code.
(00)
(00)
(11)
(11)
(2,1)
(2,1)
(2,1) + (01)
(2,1) + (01)
{ (00)}
   (11) 
{ (00)}
   (11) 
{ (01)}
   (10) 
{ (01)}
   (10) 
Figure 4.23: The construction of RM(1, 2) and RM(0, 2) via the squaring construction
from the binary codes (2, 2), (2, 1), and (2, 0).
Figure 4.24 shows the construction table for the Reed–Muller codes. Every column is
completed at the top with RM(n, n), the binary (2n, 2n) code consisting of all length-2n

162
CHAPTER 4. TRELLIS REPRESENTATIONS
binary vectors, and at the bottom with RM(−1, n), the binary code consisting of the single
codeword (0, . . . , 0) of length 2n. The partition orders are also indicated in the ﬁgure.
We now need to determine the code parameters of the RM(r, n) codes. Their length N
equals 2n, and, from Lemma 4.25, we establish recursively the minimum distance dmin =
2n−r. The code rate is found as follows. The partition order of |RM(r, n)/RM(r −1, n)|,
denoted by m(r, n), follows from the squaring construction and obeys the recursion
m(r, n) = m(r, n −1)m(r −1, n −1).
(4.112)
Starting this recursion with m(1, 1) = 2 and m(0, 1) = 2 leads to the partition numbers
in Figure 4.24. Converting to logarithms, i.e., M(r, n) = log2(m(r, n)), (4.112) becomes
M(r, n) = M(r, n −1) + M(r −1, n −1),
(4.113)
which, after initialization with M(1, 1) = 1 and M(0, 1) = 1, generates Pascal’s triangle,
whose numbers can also be found via the combinatorial generating function (1 +x)n. The
information rates of the RM(r, n) codes are now found easily. The rate of RM(r, n) is
given by the rate of RM(r −1, n) plus M(r, n). This generates the code rates indicated in
Figure 4.24.
RM(1,1)
2
2
2
2
16
8
2
4
2
8
64
16
2
2
(2,2) 
RM(0,1)
RM(-1,1)
RM(2,2)
RM(1,2)
RM(0,2)
RM(-1,2)
RM(3,3)
RM(1,3)
RM(1,3)
RM(0,3)
RM(-1,3)
RM(4,4)
RM(3,4)
RM(2,4)
RM(1,4)
RM(0,4)
RM(-1,4)
(4,4) 
(8,8) 
(16,16) 
(2,1) 
(4,3) 
(8,7) 
(16,15) 
(2,0) 
(4,l) 
(8,4) 
(16,11) 
(4,0) 
(8,1) 
(16,5) 
(8,0) 
(16,1) 
(16,0) 
Figure 4.24: Diagram for the construction of the Reed–Muller codes via the squaring and
two-level squaring constructions.

4.15. A DECODING EXAMPLE
163
Among the general class of Reed–Muller codes we ﬁnd the following special classes:
• RM(n −1, n) are the single parity-check codes of length 2n.
• RM(n −2, n) is the family of extended Hamming codes of length 2n with minimum
distance dmin = 4 [16, 9].
• RM(1, n) are the ﬁrst-order Reed–Muller codes of length N = 2n, rate R = (n+1)/N
and minimum distance dmin = N/2 [9].
• RM(0, n) are the repetition codes of length 2n.
Figure 4.25 below shows the four-section trellis diagrams of RM(2, 4), a [16, 11] code,
and of RM(1, 4), a [16, 5] code.
|RM(2, 2)/RM(1, 2)/RM(0, 2)|4
|RM(2, 2)/RM(1, 2)/RM(0, 2)|4
Figure 4.25: The trellis diagrams of RM(2, 4) and RM(1, 4), constructed via the two-level
squaring construction.
4.15
A Decoding Example
We will discuss general trellis decoding procedures at length in Chapter 7 and all of those
methods are applicable to trellis decoding of block codes. However, the trellises of the
codes and lattices constructed in the preceding sections lend themselves to more eﬃcient
decoder implementations due to their regular structure. The construction also resulted
in eﬃcient trellises in terms of their numbers of states. The extended Hamming codes
constructed in Section 4.14 above have N/2 states, while the PC-trellises of the original
and the extended Hamming codes, constructed according to Section 4.3, have 2n−k = N
states, a savings of a factor of 2, obtained by taking pairs of coded bits as branch symbols.
Often the trellis decoding operation can be mapped into an eﬃcient decoder.
Let
us consider the extended Hamming code RM(1, 3), whose trellis is shown in Figure 4.26.
Note that the trellis is the same as that of the Gosset lattice E8 in Figure 4.21, and, after

164
CHAPTER 4. TRELLIS REPRESENTATIONS
rearranging the states at time r = 1, identical to the trellis in Figure 4.9. Let us assume
that, as is the usual practice in telecommunications, zeros are mapped into −1’s and 1’s are
retained, i.e., we map the output signals {0, 1} of the code into a BPSK signal set and we
obtain the modiﬁed code words x′ from the original x. With this the decoder now operates
as follows. For each of the signals yi from the received signal vector y = (y1, . . . , y8), two
metrics need to be computed: one is (yi −1)2 and the other is (yi + 1)2, or equivalently
m0 = yi and m1 = −yi. We see that m0 and m1 are negatives of each other, and only one
must be calculated. Since the all-one vector x = (1, . . . , 1) is a codeword, the negative of
every modiﬁed codeword x′ is itself a codeword, as can be seen by applying the linearity
of the code and adding x to that modiﬁed codeword. The sign of the metric sum can now
be used to identify the sign of the codeword.
(00)
(2,1)
(2,1)+(10)
(10)
(01)
(11)
(00)
(10)
(01)
(11)
(00)
(00)
(11)
(11)
(01)
(01)
(10)
(10)
(00)
(00)
(11)
(11)
(01)
(01)
(10)
(10)
Figure 4.26: Trellis of the [8, 4] Extended Hamming code. This is also the trellis of the
Gosset lattice from Figure 4.21 as can be seen by comparing Figures 4.22 and 4.24.
The operation of the decoder is illustrated in Figure 4.27, where the operator O pro-
duces the sum and diﬀerence of the inputs, i.e., the outputs of the ﬁrst operator box are
y1 + y2 and y1 −y2. The ﬁrst two stages in the decoder decode the ﬁrst and the second
trellis sections, and the sign indicates whether the codeword or its complement are to be
selected. After the absolute magnitude operation, the two sections are combined. This is
done by the last section of the decoder. Finally, the selection decides through which of
the four central states the best codeword leads. Since the components have already been
decoded, this determines the codeword with the best total metric.
In order to use the above decoder to decode E8, one additional initial step needs to be

4.15. A DECODING EXAMPLE
165
added. According to Figure 4.21 and Figure 3.14, the transition of a “0” corresponds to
transmitting an odd integer, and the transition of a “1” corresponds to transmitting an
even integer. For every received yi, we therefore need to ﬁnd the closest even integer Iei
and the closest odd integer Ioi. These integers are stored and used later to reconstruct the
exact lattice point. The metrics m0i = (Ioi−yi)2 and m1i = (Iei−yi)2 are then used in the
algorithm above to ﬁnd the most likely sequence of sublattices via a decoding operation
which is identical to that of the extended Hamming code. The decoded sublattice sequence
determines now if the odd or even integers are retained, i.e., the decoded sublattices
Z2 ≡(Ioi, Io(i+1)), Z2 + (1, 1) ≡(Iei, Ie(i+1)), Z2 + (1, 0) ≡(Iei, Io(i+1)), and Z2 + (0, 1) ≡
(Ioi, Ie(i+1)). This sequence of decoded integers identiﬁes the most likely lattice point.
++
++
++
++
+-
+-
+-
+-
ABS
sign
ABS
sign
ABS
sign
ABS
sign
++
++
++
++
+-
+-
+-
+-
SUM
SUM
SUM
SUM
Select  Largest
Multiply by Signs
O
O
O
O
O
O
O
O
ABS
sign
ABS
sign
ABS
sign
ABS
sign
y1 y2
y3 y4
y5 y6
y7 y8
Figure 4.27: Schematic diagram of an eﬃcient decoder for the [8, 4] Extended Hamming
code. The operator O produces the sum yi + yj and the diﬀerence yi −yj of the inputs,
ABS forms the absolute value and SUM adds yi + yj.
Finally we wish to refer the interested reader to more in-depth material on this subject
available as recent books by Lin et al. [32] and by Honary and Markarian [21].

166
CHAPTER 4. TRELLIS REPRESENTATIONS
4.16
Polar Codes and Their Relationship to RM Codes
RM codes are binar linear codes of length N with dimenstion K, and their generator
matrices are derived from a common set of basis vectors. Let us therefore construct this
set as the nth Kronecker power of the basic kernel matrix
F2 =
 1
1
1
0

,
(4.114)
that is, we generate F ⊗n
2
. For example,
F ⊗3
2
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
0
0
0
1
0
0
0
1
1
1
1
0
0
0
0
1
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.115)
and clearly, the columns form an independent set of basis vectors. We now deﬁne RN =
F ⊗n
2
as our base matrix, and the Reed–Muller codes as well as the Polar codes are based
on RN. RM codes were discovered by Muller [44], but a decoding algorithm was ﬁrst
invented by Reed [50]. Polar codes, on the other hand, were discovered by Arikan [1] more
than 50 years later. These two ways of derivng codes from the same foundation elegantly
illustrate the shift in perspective that has taken place over the last six decades and which
is a major thread of this book.
Deﬁne the N-tuple vi = r2i, i ∈[0, n−1], where rk is the kth row of RN. So, for n = 3
we obtain
v1
=
[10101010],
v2
=
[11001100],
v3
=
[11110000].
Furthermore, the code generators also use the boolean products vk ·vl, where the products
are taken componentwise as logic AND operation.
In this fashion the RM codes are
generated by the basis vectors consisting of the products of vk up to degree r, i.e.,
RM(r, n) = {v0, v1, . . . , vn, v1 · v2, . . . , vn−1vn, . . . , higher products} .
(4.116)
By inspection we can see that this construction generates the same basis vectors as the
squaring construction from the previous section.

4.16. POLAR CODES AND THEIR RELATIONSHIP TO RM CODES
167
For example, the generator matrix for RM(3, 2) is then given by
GRM(3,2) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
v0
v1
v2
v3
v1v2
v1v3
v2v3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
1
1
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.117)
which is, of course, the [8,7] parity check code as discussed above.
The row selection in the Reed–Muller codes are based on the weight of the rows in the
matrix RN, and this leads to asymptotically unreliable codes. The polar code construction
below proceeds with a diﬀerent row selection principle.
Polar coding operates with bit channels, so let us introduce the basic channel model
y = W(x),
(4.118)
where W stands for any binary input channel, such as a simple erasure channel for example.
This channel has a mutual information denoted by IW (x; y), which in the case of an erasure
channel is simple IW (x; y) = 1 −, the erasure probability.
In the case of symmetric
output channels, IW (x; y) coincides with the capacity of the channel, which then requires
a symmetric input distribution on x.
The basis function generation matrix RN is built recursively according to (4.115), so
we recreate this process with the binary input channels attached. This is shown in Figure
4.28 for the ﬁrst two stages of the process. On the left hand, the input bits u1 and u2
are passed through F2, then each output bit is send through an identical channel W with
mutual invormation IW . We now consider the two virtual channels u1 →y and u2 →y
and compute their mutual information as
I([u1, u2]; y) = I(u2; y|u1) + I(u1; y).
(4.119)
Technically, this decomposition creates two synthetic channels, whose combined capacity
is equal to 2IW . If our example channel W is a BEC with erasure probability 0.5, then
I(u2; y|u1) = 0.75, and I(u1; y) = 0.25. This can be seen by drawing out the two channels
and realizing that for I(u1; y) the output signal is statistically independent of u1 with
probability 0.75 and therefore all the corresponding outputs are represented by an erasure
of u1. On the other hand, the channel with the higher capacity requires knowledge of u1,
the bit that passes through the lower-capacity channel. If we choose to “freeze” that bit
u1 to a ﬁxed level – either 0 or 1, the channel for the bit u2 becomes useable.
This basic observation is at the core of the phenomenon of channel polarization which
is exploited in polar codes.
Arikan [1] showed that, in general, this channel splitting

168
CHAPTER 4. TRELLIS REPRESENTATIONS
+
+
+
+
+
W
W
u1
u2
y1
y2
u1
u2
u3
u4
y1
y2
y3
y4
W
W
W
W
W −
W −
F2 =
 1
1
1
0

F4 =
⎡
⎢⎢⎣
1
1
1
1
1
0
1
0
1
1
0
0
1
0
0
0
⎤
⎥⎥⎦
binary erasure channel
0
1
0
Δ
1
Figure 4.28: Encoding principle of polar codes with the binary input channels W The
binary erasure channel (BEC) model shown is used in the text to illustrate the concepts.
generates a lower-capacity channel W −and a higher-capacity channel W +, whose sum
capacity equals the original channel capacity, such that
I(W −) ≤I(W) ≤I(W +).
(4.120)
Furthermore, in this combination we have I(W −) = I2(W).
If we study Figure 4.28 carefully, we note that the splitting argument is recursive. In
F4, two channels W −, and two channels W + are combined into a total of four channels,
denoted by W −−, W −+, W +−, W ++. We note that the combining of the two W −channels
is separate from the combining of the two W + channels, and that the channel combination
can therefore be applied recursively; from this we obtain I(W ++) = 9375, I(W +−) =
0.5625, I(W −+) = 0.4375, I(W −−) = 0.0625, for a total sum of 4I(W) = 2 bits/channel
use.
Figure 4.29, presented in [45], develops this recombination further recursively for blocks
of length N = 2n, following the pattern established by F2n. The ﬁgure nicely illustrates
the eﬀect called “polarization,” whereby the binary channels synthesized through F2n
separate into “good channels” and “bad channels,” that is, into channels with a mutual
information approaching unity and also channels whose mutual information approaches

4.16. POLAR CODES AND THEIR RELATIONSHIP TO RM CODES
169
zero. The mutual information of the respective channels can be computed recursively from
the relationships I(W +) = 2I(W) −I2(W), and I(W −) = I2(W). As N →∞this leads
to the spread of mutual information values shown in Figure 4.29, and Arikan [1] showed
that the proportion of channels whose mutual information goes to unity is exactly equal
to I(W) = δ, that is, the erasure probability of the channel.
I(W++ )
I(W–+)
I(W–)
I(W--)
I(W+–)
I(W+)
I(W)
Mutual Information
Codelength N
20
21
22
23
24
25
26
27
28
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Good Codes
Bad Codes
Figure 4.29: Illustration of the channel polarization eﬀect for a BEC with δ = 0.5.
As N increases, the channels become increasingly polarized, meaning that they con-
verge to either channels with mutual information →1 or →0, and the proportion of
channels with an intermediate mutual information goes to zero.
This is illustrated in
Figure 4.30, which plots the cumulative probability function of the Bhattacharyya bound
Z(Wn) for the polarized channels for several n. Z(Wn) is related to IWn ≥

1 −Z2(Wn)
and was used in [1] to derive the polarization eﬀect. If I(Wn) = 1, Z(Wn) = 0, and
I(Wn) = 0 →Z(Wn) = 1. Consequently polarization of the Bhattacharyya bound Z(Wn)
implies polarization of the mutual information I(Wn).
In general, Arikan [1] shows that the proportion of good channels with a mutual in-
formation that goes to unity is given by I(x; y) for any binary symmetric memoryless
channels. A polar code now consists of sending binary bits only through those good chan-
nels, and not using the bad channels, or more precisely, the bad channels are frozen to a
speciﬁc predetermined value.
This selection of the good channels is the encoding process for a polar code, or equiv-

170
CHAPTER 4. TRELLIS REPRESENTATIONS
1
1
1
0
0
0
0 0
1
0
1
0
1
0
1
1
Cumulative PDF
I(W)
n = 0
n = 5
n = 15
n →∞
Figure 4.30: Statistical illustration of polarzation via the cumulative PDF of the Bhat-
tacharyya parameter Z(Wn). For n = 0, the channel has only a value Z(Wn), while for
n →∞, Z(Wn) has a bi-modal distribution with probability mass at 0 and 1, source [19].
alently is what deﬁnes the code. Formally, encoding consists of selecting an information
vector u, by populating the unfrozen positions with the K information bits, and setting
the other postions to zero, for example. The codeword x is now found via x = uFN.
We note that this operation is in essence the Hadamard transform, which can be carried
out via the fast Hadamard transform (in structure identical to the fast fourier transform).
Encoding can therefore be done with a complexity of order O(N log(N)), or O(log(N))
operations per bit.
Constructing a polar code can be challenging, however. In the case of BEC channels,
the synthetic channel capacities of the individual bit channels can be computed recursively
as illustrated in Figure 4.28 with an eﬀort of order O(N log(N)), however, for other chan-
nels the complexity of identifying the good channels grows exponentially with codelength.
Density evolution–discussed in detail in Chapter 6–has been used for polar code constru-
tion in [43, 56]. A lower complexity version of DE, using the Gaussian approximation of
densities, has also been applied, see [58].
A last point we wish to mention is that the construction of polar codes is dependent
on the actual channel and the channel parameter, such as the erasure rate. These codes
are therefore not “universal” codes, and conceptually would have to be redesigned for
each value of the signal-to-noise ratio or erasure rate. This is, however, not a serious
impediment in practice.
As we have seen in the derivation of these codes, the channel decomposition leads to
channels which are conditioned on the knowledge of previous symbols, such as the example
of I(u2; y|u1) in (4.119), which in general will have the form I(uk; y|[u1, . . . , uk−1]). As
long as all the conditioning binary symbols [u1, . . . , uk−1 are from frozen positions, the
resulting channel mutual information directly represent that maximal rate that can be
achieved on this channel. However, in general, parts of [u1, . . . , uk−1] will be information
bearing binary digits themselves. In light of this, it is not too surprising that the decoding
method proposed by Arikan [1] is of a recursive cancellation nature; that is, previously

4.16. POLAR CODES AND THEIR RELATIONSHIP TO RM CODES
171
computed symbols [u1, . . . , uk−1] are used to deﬁne the channel for uk.
The resulting
successive cancelation algorithm has much in common with the graph-based iterative de-
coding algorithm discussed in subsequent chapters of this book, and we therefore relegate
a discussion of polar code decoding to Chapter 9.
Suﬃce to say that successive or partially parallel cancelation algorithms are the state-
of-the art decoding methods for polar codes, and in recent years have made great progress
to the point where fast algorithms have been implemented in VLSI.
Appendix 4.A
We will generate a sequence of matrix operations which turn P into the diagonal matrix
Γ according to Theorem 4.16 (see also [26, Section 3.7], or [15]).
First, let us deﬁne some elementary matrix operations of size either k × k or n × n,
depending on whether we premultiply or postmultiply P.
The ﬁrst such operation is
Tij(b) = I+bEij, where I is the identity matrix, Eij is a matrix with a single 1 in position
(i, j); i = j and 0’s elsewhere, and b ∈R, the principal ideal domain.8 Tij(b) is invertible
in R, since
Tij(b)Tij(−b) = (I + bEij) (I + (−b)Eij) = I.
(4.121)
Left multiplication of P by a k × k matrix Tij(b) adds b times the jth row to the ith row
of P, leaving the remaining rows unchanged. Right multiplication of P by an n×n matrix
Tij(b) adds b times the ith column to the jth column of P.
Next, let u be a unit in R, and deﬁne Di(u) = I + (u −1)Eii, with element u on
the ith position on the diagonal. Again, Di(u) is invertible with inverse Di(1/u). Left
multiplication with Di(u) multiplies the ith row of P by u, while right multiplication
multiplies the ith column by u.
Finally, deﬁne Qij = I −Eii −Ejj + Eij + Eji, and Qij is its own inverse.
Left
multiplication by Qij interchanges the ith and jth rows of P, while right multiplication
interchanges the i-th and j-th rows, leaving the remainder of the matrix unchanged.
Last, deﬁne
U =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
x
s
y
t
0
1
...
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.122)
where x, y, s, and t will be chosen such that the submatrix
x
s
y
t

is invertible.
8It might be helpful to think in terms of integers, i.e., R = Z.

172
CHAPTER 4. TRELLIS REPRESENTATIONS
Let us start now, and assume p11 = 0 (otherwise bring a non-zero element into its
position via elementary Qij operations), and assume further that p11 does not divide p12
(otherwise, again, move such an element into its position via elementary operations). Now,
according to Euclid’s division theorem [26], there exist elements x, y ∈R, such that
p11x + p12y = gcd(p11, p12) = d,
(4.123)
where d, the greatest common divisor of p11 and p12, can be found via Euclid’s algorithm.
Let s = p12/d and t = −p11/d in (4.122), which makes it invertible, i.e.,
x
p12/d
y
−p11/d
−1
=
p11/d
p12/d
y
−x

.
(4.124)
Multiplying P on the right by this matrix gives a matrix whose ﬁrst row is (d, 0, p13, . . . , p1k).
If d does not divide all elements of the new matrix, move such an element into position
(2, 1) via elementary matrix operation and repeat the process until p
11 divides all p
ij, for
all i, j.
Via elementary matrix operations the entire ﬁrst row and column can be cleared to
zero, leaving the equivalent matrix
P =
γ1
0
0
P
1

,
(4.125)
where γ1 = p
11, up to units, and γ1 divides every element in P
1, since the elementary
matrix operations do not aﬀect divisibility.
Iterating this procedure now, continuing with P
1 yields
P =
⎡
⎣
γ1
0
0
0
γ2
0
0
0
P
2
⎤
⎦,
(4.126)
where the divisibility γ1|γ2 is assured since γ1 divides all elements in P
1, which implies that
any elementary matrix operation preserves this divisibility, and γ2 divides every element
in P
2 .
We now simply iterate this procedure to obtain the decomposition in Theorem 4.16,
in the process producing the desired matrices A and B. It can further be shown that this
decomposition is unique in the sense that all equivalent matrices yield the same diagonal
matrix Γ (see [26], Section 3.7).
Note that this procedure also provides a proof for Theorem 4.16.

Bibliography
[1] E. Arikan, “Channel polarization:
A method for constructing capacity-achieving
codes for symmetric binary-input memoryless channels,” IEEE Trans. Inform. The-
ory, vol. 55, no. 7, July 2009, pp. 3051–3073.
[2] J.B. Anderson and S.M. Hladik, ”Tailbiting MAP decoders,” IEEE J. Select. Areas
Commun., vol. SAC-16, pp. 297–302, Feb. 1998.
[3] L.R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear codes for
minimizing symbol error rate,” IEEE Trans. Inform. Theory, vol. IT-20, pp. 284–287,
1974.
[4] E.R. Berlekamp, Algebraic Coding Theory, McGraw-Hill, New York, 1968.
[5] R.E. Blahut, Principles and Practice of Information Theory, Addison-Wesley, Read-
ing, MA, 1987.
[6] R. Calderbank, G.D. Forney, and A. Vardy, ”Minimal tail-biting trellises: The Golay
code and more,” IEEE Trans. Inform. Theory, vol. IT-45, no. 5, pp. 1435–1455, July
1999.
[7] D. Chase, “A class of algorithms for decoding block codes with channel measurement
information,” IEEE Trans. Inform. Theory, vol. IT-18, pp. 170–182, Jan. 1972.
[8] G.C. Clark and J.B. Cain, Error-Correction Coding for Digital Communications,
Plenum Press, New York, 1983.
[9] J.H. Conway and N.J.A. Sloane, Sphere Packings, Lattices and Groups, Springer-
Verlag, New York, 1988.
[10] J.H. Conway and N.J.A. Sloane, “Decoding techniques for codes and lattices, includ-
ing the Golay code and the Leech lattice,” IEEE Trans. Inform. Theory, vol. IT-32,
pp. 41–50, 1986.
[11] D.G. Daut, J.W. Modestino, and L.D. Wismer, “New short constraint length convo-
lutional code construction for selected rational rates,” IEEE Trans. Inform. Theory,
vol. IT-28, pp. 793–799, Sept. 1982.
173

174
BIBLIOGRAPHY
[12] A. Dholakia, Introduction to Convolutional Codes, Kluwer Academic Publishers,
Boston, MA, 1994.
[13] P. Elias, “Coding for noisy channels,” IRE Conv. Rec., pt. 4, pp. 37–47, 1955.
[14] G.D. Forney, Jr., “Generalized minimum distance decoding,” IEEE Trans. Inform.
Theory, vol. IT-12, pp. 125–131, April 1966.
[15] G.D. Forney, “Convolutional codes I: Algebraic structure,” IEEE Trans. Inform. The-
ory, vol. IT-16, no. 6, pp. 720–738, Nov. 1970.
[16] G.D. Forney, “Coset Codes–Part II: Binary lattices and related codes,” IEEE Trans.
Inform. Theory, vol. IT-34, no. 5, pp. 1152–1187, Sept. 1988.
[17] M.P.C. Fossorier and S. Lin, “Soft-decision decoding of linear block codes based
on ordered statistics,” IEEE Trans. Inform. Theory, vol. IT-41, pp. 1379–11396,
Sept. 1995.
[18] Y.S. Han, C.R.P. Hartman, and C.C. Chen, “Eﬃcient priority-ﬁrst search maximum-
likelihood soft-decision decoding of linear block codes,” IEEE Trans. Inform. Theory,
vol. IT-39, pp. 1514–1523, Sept. 1993.
[19] S.H. Hassani, Polarization and Spatial Coupling: Two Techniques to Boost Perfor-
mance, PhD Thesis No. 5706, ´Ecole Polytechnique F´ed´erale de Lausanne, March
2013.
[20] F. Hemmati, “Closest coset decoding of |u|u+v| codes,” IEEE J. Select. Areas Com-
mun., vol. SAC-7, pp. 982–988, Aug. 1989.
[21] B. Honary and G. Markarivan, Trellis Decoding of Block Codes: A Practical Approach,
Kluwer Academic Publisher, Boston, MA, 1997.
[22] H. Imai et al. Essentials of Error-Control Coding Techniques, Academic Press, New
York, 1990.
[23] T. Kasami et al., “On the optimum bit orders with respect to the state complexity
of trellis diagrams for binary linear codes,” IEEE Trans. Inform. Theory, vol. IT-39,
pp. 242–245, 1993.
[24] T. Kasami et. al., “On the trellis structure of block codes”, IEEE Trans. Inform.
Theory, vol. IT-39, pp. 1057–1064, 1993.
[25] A.B. Kiely, S.J. Dolinar, Jr., R.J. McEliece, L.L. Ekroot, and W. LinL, “Trellis
complexity of linear block codes,” IEEE Trans. Inform. Theory, vol. IT-42, no. 6,
Nov. 1996.
[26] N. Jacobson, Basic Algebra I, Freeman and Company, New York, 1985.
[27] R. Johannesson and Z.-X. Wan, “A linear algebra approach to minimal convolutional
encoders,” IEEE Trans. Inform. Theory, vol. IT-39, no. 4, pp. 1219–1233, July 1993.

BIBLIOGRAPHY
175
[28] R. Johannesson and K.S. Zigangirov Fundamentals of Convolutional Coding, IEEE
Press, 1999.
[29] F. Kshischang and V. Sorokine, ”On the trellis structure of block codes,” IEEE Trans.
Inform. Theory, vol. IT-41, pp. 1924–1937, Nov. 1995.
[30] G.R. Lang and F.M. Longstaﬀ, “A Leech lattice modem,” IEEE J. Select. Areas
Commun., vol. SAC-7, No. 6, Aug. 1989.
[31] K.J. Larsen, “Short convolutional codes with maximum free distance for rates 1/2,
1/3, and 1/4,” IEEE Trans. Inform. Theory, vol. IT-19,, pp. 371-372, May 1973.
[32] S. Lin (Editor) Trellises and Trellis-Based Decoding Algorithms for Linear Block
Codes, Kluwer Academic Publisher, Dordrecht, 1998.
[33] S. Lin and Daniel J. Costello, Jr., Error Control Coding: Fundamentals and Applica-
tions, Prentice Hall, Englewood Cliﬀs, NJ, 1983.
[34] N.J.C. Lous, P.A.H. Bours, and H.C.A. van Tilborg, “On maximum likelihood soft-
decision decoding of binary linear codes,” IEEE Trans. Inform. Theory, vol. IT-39,
pp. 197–203, Jan. 1993.
[35] J.H. Ma and J.K. Wolf, ”On tailbiting convolutional codes,” IEEE Trans. Commun.,
vol. COM-34, pp. 104–111, Feb. 1986.
[36] J.L. Massey, “Foundations and methods of channel coding,”Proc. Int. Conf. Inform.
Theory and Systems, NTG-Fachberichte vol. 65, pp. 148–157, 1978.
[37] J.L. Massey, “Shift-register synthesis and BCH decoding,” IEEE Trans. Inform. The-
ory, vol. IT-15, pp. 122-127, Jan. 1969.
[38] J.L. Massey and M.K. Sain, “Inverses of linear sequential circuits,” IEEE Trans.
Computers, vol. C-17, pp. 310–337, April 1968.
[39] K.R. Matis and J.W. Modestino, “Reduced-state soft-decision trellis decoding of lin-
ear block codes,” IEEE Trans. Inform. Theory, vol. IT-28, pp. 61–68, Jan. 1982.
[40] R.J. McEliece, “On the BCJR trellis for linear block codes,” IEEE Trans. Inform.
Theory, vol. IT-42, no. 4, July 1996.
[41] R. McEliece, The Theory of Information and Coding Encyclopedia of Mathematics
and its Applications, Addison-Wesley, Reading, MA, 1977.
[42] F.J. MacWilliams and N.J.A. Sloane, The Theory of Error Correcting Codes, North
Holland, New York, 1988.
[43] R. Mori and T. Tanaka, “Peformance of polar codes with the construction using
density evolution,” IEEE Commun. Lett., vol. 13, no. 7, pp. 519–521, July 2009.
[44] D.E. Muller, “Applications of boolean algebra to switching circuits design and to
error detection,” IRE Trans., EC-3, pp. 6–12, Sept. 1954.

176
BIBLIOGRAPHY
[45] K. Niu, K. Chen, J. Lin, and Q.T. Zhang, “Polar codes: Primary concepts and
practical decoding algorithms,” IEEE Comm. Mag., pp. 192–203, July 2014.
[46] J.P. Odenwalder, “Optimal decoding of convolutional codes,” Ph.D. thesis, University
of California, Los Angeles, 1970.
[47] E. Paaske, “Short binary convolutional codes with maximum free distance for rates
2/3 and 3/4,” IEEE Trans. Inform. Theory, vol. IT-20, pp. 683–689, Sept. 1974.
[48] J.E. Porath, “Algorithms for converting convolutional codes from feedback to feed-
forward form and vice versa,” Electronic Lett., vol. 25, no. 15, pp. 1008–1009, July
1989.
[49] J.G. Proakis, Digital Communications, third edition, McGraw-Hill, New York, 1995.
[50] I.S. Reed, “A class of multiple-error-correcting codes and their decoding scheme,”
IRE Trans., IT-4, pp. 38–49, Sept. 1954.
[51] M.K. Sain and J.L. Massey, “Invertibility of linear time-invariant dynamical systems,”
IEEE Trans. Automatic Control, vol. AC-14, pp. 141–149, April 1969.
[52] J. Snyders and Y. Be’ery, “Maximum likelihood soft decoding of binary block codes
and decoders for the Golay codes,” IEEE Trans. Inform. Theory, vol. IT-35,, pp.
963–975, Sept. 1989.
[53] G. Strang, Linear Algebra and Its Applications, Harcourt Brace Jovanovich, San
Diego, 1988.
[54] Y. Sugiyama, M. Kasahara, S. Hirasawa, and T. Namekawa, “A method for solving
key equation for decoding Goppa codes,” Inform. Contr., vol. 27, pp. 81-99, 1975.
[55] D.J. Taipale and M.J. Seo, “An eﬃcient soft-decision Reed–Solomon decoding algo-
rithm,” IEEE Trans. Inform. Theory, vol. IT-40, pp. 1130–1139, July 1994.
[56] I. Tal and A. Vardy, “How to construct polar codes,” IEEE Trans. Inform. Theory,
vol. 59, no. 10, pp. 6562–6582, Oct. 2013.
[57] T. Taneko, T. Nishijima, H. Inazumi, and S. Hirasawa, “Eﬃcient maximum likelihood
decoding of linear block codes with algebraic decoder,” IEEE Trans. Inform. Theory,
vol. IT-40, pp. 320–327, March 1994.
[58] P. Trifonov, “Eﬃcient design and decoding of polare codes,” IEEE Trans. Commun.,
vol. 60, no. 11, pp. 3221–3227, Nov. 2012.
[59] A. Vardy and Y. Be’ery, “More eﬃcient soft decoding of the Golay codes,” IEEE
Trans. Inform. Theory, vol. IT-37, pp. 667–672, May 1991.
[60] A. Vardy and Y. Be’ery, “Maximum likelihood soft decoding of BCH codes,” IEEE
Trans. Inform. Theory, vol. IT-40, pp. 546–554, March 1994.

BIBLIOGRAPHY
177
[61] A.J. Viterbi and J.K. Omura, Principles of Digital Communication and Coding,
McGraw-Hill, New York, 1979.
[62] E.J. Weldon, Jr., “Decoding binary block codes on q-ary output channels,” IEEE
Trans. Inform. Theory, vol. IT-17, pp. 713–718, Nov. 1971.
[63] N. Wiberg, H.-A. Loeliger, and R. K¨otter, “Codes and iterative decoding on general
graphs,” Eur. Trans. Telecommun., vol. 6, pp. 513–526, Sept. 1995.
[64] J.K. Wolf, “Eﬃcient maximum-likelihood decoding of linear block codes using a trel-
lis,” IEEE Trans. Inform. Theory, vol. IT-24, no. 1, pp. 76–80, Jan. 1978.

Chapter 5
Trellis and Tree Decoding
5.1
Background and Introduction
In this chapter we will study decoding algorithms that operate on the trellis representation
of the code. Our objective is to extract the information symbols that were encoded and
are carried by the received signals with computationally eﬃcient algorithms that lead to
decoders.
There a great variety of decoding algorithms for trellis, some heuristic and some de-
rived from well-deﬁned optimality criteria. Until very recently, the main objective of a
decoding algorithm was the successful identiﬁcation of the transmitted symbol sequence,
accomplished a sequence decoder. These sequence decoders fall into two main groups: Tree
decoders and trellis decoders. Tree decoders explore the code tree, to be deﬁned below,
and their most well-known representatives are the sequential algorithms and limited-size
breadth ﬁrst algorithms, such as the M-algorithm. Trellis decoders make use of the more
structured trellis of a code, and its main algorithm is the maximum-likelihood sequence
detector, also known widely simply as the Viterbi algorithm.
Recently, and in conjunction with the emergence of turbo coding, symbol probability
algorithms have become important. They calculate the reliability of individual transmitted
or information symbols, rather than decoding sequences. Symbol probability algorithms
are essential for the iterative algorithms used to decode large concatenated codes, such as
turbo codes. Their most popular and widely used representative is the A Posteriori Prob-
ability (APP) algorithm, also known as the BCJR algorithm, or the forward–backward
algorithm. The APP algorithm works on the trellis of the code, and it is discussed in
detail in Section 5.7.
Let us now set the stage for these decoding algorithms. In Chapters 2 and 3 we have
discussed how a trellis encoder generates a sequence x(i) = (x(i)
−l, . . . , x(i)
l ) of correlated
complex symbols x(i)
r
for message i, and how this sequence is modulated, using the pulse
179
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

180
CHAPTER 5. TRELLIS AND TREE DECODING
waveform p(t), into the (baseband) output signal
s(i)(t) =
l

r=−l
x(i)
r p(t −rT).
(5.1)
From Chapter 2 we also know the structure of the optimal decoder for such a system.
We have to build a matched ﬁlter for each possible signal s(i)(t) and select the message
which corresponds to the signal which produces the largest sampled output value.
The matched ﬁlter for s(i)(t) is given by
s(i)(−t) =
l

r=−l
x(i)
r p(−t −rT),
(5.2)
and, if r(t) is the received signal, the sampled response of the matched ﬁlter (5.2) to r(t)
is given by
r · s(i)
=
 ∞
−∞
r(t)s(i)(t) dt
=
l

r=−l
x(i)
r yr = x(i) · y,
(2.23)
where yr =
 ∞
−∞r(α)p(α −rT) dα is the output of the ﬁlter matched to the pulse p(t)
sampled at time t = rT as discussed in Section 2.5 (Equation (2.26)), and y = (y−l, . . . , yl)
is the vector of sampled signals yr.
If time-orthogonal pulses (e.g., Nyquist pulses) p(t) with unit energy (
 ∞
−∞p2(t) dt = 1)
are used, the energy of the signal s(i)(t) is given by
|s(i)|2
=
 ∞
−∞
 ∞
−∞
s(i)(α)s(i)(β) dαdβ
=
l

r=−l
|x(i)
r |2,
(5.3)
and, from (2.23), the maximum likelihood receiver will select the sequence x(i) which
maximizes
J(i) = 2
l

r=−l
Re

x(i)
r v∗
r

−
l

r=−l
|x(i)
r |2.
(5.4)
J(i) in Equation (5.4) is called the metric of the sequence x(i), and this metric is to be
maximized over all allowable choices of x(i). Clearly, exhaustive cataloguing of (5.4) for
all x(i) is inconceivable, and strategic methods for reducing this complexity to manageable
levels is precisely what the decoders in the next sections accomplish.

5.2. TREE DECODERS
181
5.2
Tree Decoders
From (5.4) we deﬁne the partial metric at time n as
J(i)
n
= 2
n

r=−l
Re

x(i)
r v∗
r

−
n

r=−l
|x(i)
r |2,
(5.5)
which allows us to rewrite (5.4) in the recursive form
J(i)
n
= J(i)
n−1 + 2Re

x(i)
n v∗
n

−|x(i)
n |2.
(5.6)
Equation (5.6) implies a tree structure which can be used to evaluate the metrics for all
the allowable signal sequences as illustrated in Figure 5.1 for the code from Figure 3.1,
Chapter 3. This tree has, in general, 2k branches leaving each node, since there are 2k
possible diﬀerent choices of the signal xn at time n. Each node is labeled with the partial
sequence1 ˜x(i) = (x(i)
−l, . . . , x(i)
n ) which leads to it. The intermediate metric J(i)
n
is also
associated with each node. A tree decoder starts at time n = −l at the single root node
with J−l = 0, and extends through the tree evaluating and storing (5.6) until time unit
n = l, at which time the largest accumulated metric identiﬁes the most likely sequence x(i).
It becomes obvious that the size of this tree grows very quickly. In fact, its ﬁnal width
is k2l+1, which is an outlandish number even for small values of l, i.e., short encoded
sequences.
We therefore need to reduce the complexity of decoding, and one way of
accomplishing this is can by performing only a partial search of the tree.
There are a number of diﬀerent approaches to tree decoding and we will discuss the
more fundamental types in the subsequent sections.
Before we tackle these decoding
algorithms, however, we wish to modify the metric such that it can take into account the
diﬀerent lengths of paths, since we will come up against the problem of comparing paths
of diﬀerent lengths.
Consider then the set XM of M partial sequences ˜x(i) with lengths {ni}, and let nmax =
max{n1, . . . , nM} be the maximum length among the M partial sequences. The decoder
must make its likelihood ranking of the paths based on the partial received sequence ˜y of
length nmax.
From (2.12) we know that an optimum receiver would choose the ˜x(i) which maximizes
P[˜x(i)|˜y] = P[˜x(i)]
−l+ni+1
r=−l
pn(yr −xr) −l+nmax
r=−l+ni p(yr)
p(˜y)
,
(5.7)
where the second product reﬂects the fact that we have no hypotheses x(i)
r
for r > ni,
since ˜x(i) extends only up to −l + ni. We therefore have to use the a priori probabilities
1We denote partial sequences by tildes to distinguish them from complete sequences or codewords.

182
CHAPTER 5. TRELLIS AND TREE DECODING
epoch:
()
(0)
(2)
(4)
(6)
(0, 0)
(0, 2)
(0, 4)
(0, 6)
(2, 0)
(2, 2)
(2, 4)
(2, 6)
(4, 0)
(4, 2)
(4, 4)
(4, 6)
(6, 0)
(6, 2)
(6, 4)
(6, 6)
(2, · · · , 0)
(2, · · · , 2)
(6, · · · , 4)
(6, · · · , 6)
−l
−l + 1
−l + 2
l
Figure 5.1: Code tree extending from time −l to time l for the code from Figure 3.1,
Chapter 3.
p(yr|x(i)
r ) = p(yr) for r > ni. Using p(˜y) = −l+nmax
r=−l
p(yr) Equation (5.7) can be rewritten
as
P[˜x(i)|˜y] = P[˜x(i)]
−l+ni

r=−l
pn(yr −x(i)
r )
p(yr)
,
(5.8)
and we see that we need not be concerned with the tail samples not aﬀected by ˜x(i).
Taking logarithms gives the “additive” metric
L(˜x(i), ˜y) =
−l+ni

r=−l
log pn(yr −x(i)
r )
p(yr)
−log
1
P[˜x(i)]
.
(5.9)
Since P[˜x(i)] = (2−k)ni is the a priori probability of the partial sequence ˜x(i), assuming
that all the inputs to the trellis encoder have equal probability, (5.9) becomes
L(˜x(i), ˜y) = L(˜x(i), y) =
−l+ni

r=−l

log pn(yr −x(i)
r )
p(yr)
−k

,
(5.10)

5.3. THE STACK ALGORITHM
183
where we have extended ˜y →y since (5.10) ignores the tail samples yr; r > ni anyhow.
The metric (5.10) was introduced for decoding tree codes by Fano [21] in 1963 and was
analytically derived by Massey [36] in 1972.
Since Equation (2.13) explicitly gives the conditional probability distribution pn(yr −
xr), the metric in (5.10) can be specialized for additive white Gaussian noise channels to
L(˜x(i), y)
=
−l+ni

r=−l
⎡
⎣log
exp

−|x(i)
r −yr|2/N0


x∈A p(x) exp (−|x −yr|2/N0) −k
⎤
⎦
=
−
−l+ni

r=−l
|x(i)
r −yr|2
N0
−cr(yr),
(5.11)
where cr(yr) = log

x∈A p(x) exp

−|x −yr|2/N0

+k is a term independent of x(i)
r
which
is subtracted from all the metrics at time r. Note that cr can be positive or negative, which
causes some of the problems with sequential decoding, as we will see later.
It is worth noting here that if the paths examined are of the same length, say n, they
all contain the same cumulative constant −−l+n
r=−l cr in their metrics, which therefore may
be discarded from all the metrics. This allows us to simplify (5.11) to
L(˜x(i), y) ≡
−l+n

r=−l
2Re

x(i)
r v∗
r

−|x(i)
r |2 = J(i)
n ,
(5.12)
by neglecting terms common to all the metrics. The metric (5.12) is equivalent to the
accumulated Euclidean distance between the the received partial sequence ˜y and the hy-
pothesized symbols on the i-th path to up to length n. The restriction to path of equal
length makes this metric much simpler than the general metric (5.10) (and (5.11)) and
ﬁnds application in the so called breadth-ﬁrst decoding algorithms which we will discuss
in subsequent sections.
5.3
The Stack Algorithm
The stack algorithm is one of the many variants of what has become known as sequential
decoding, and was introduced by Wozencraft and Reiﬀen [58] for convolutional codes. It has
subsequently experienced many changes and additions. Sequential decoding describes any
algorithm for decoding trellis codes which successively explores the code tree by moving
to new nodes from an already explored node.
From the introductory discussion in the preceding section, one way of sequential de-
coding becomes apparent. We start exploring the tree and store the metric (5.10) (or
(5.11)) for every node explored. At each stage now we simply extend the node with the

184
CHAPTER 5. TRELLIS AND TREE DECODING
largest such metric. This, in essence, is the stack algorithm ﬁrst proposed by Zigangirov
[64] and Jelinek [32]. This basic algorithm is:
Step 1: Initialize an empty stack S of visited nodes and their metrics.
Deposit the
empty partial sequence () at the top of the stack with its metric L((), y) = 0.
Step 2: Extend the node corresponding to the top entry {˜xtop, L(˜xtop, y)} by forming
L(˜xtop, y)−|xr−yr|2/N0−cr for all 2k extensions of ˜xtop →(˜xtop, xr) = ˜x(i). Delete
{˜xtop, L(˜xtop, y)} from the stack.
Step 3: Place the new entries

˜x(i), L(˜x(i), y)

from Step 2 into the stack such that the
stack remains ordered with the entry with the largest metric at the top of the stack.
Step 4: If the top entry of the stack is a path to one of the terminal nodes at depth l,
stop and select xtop as the transmitted symbol sequence. Otherwise, go to Step 2.
There are some practical problems associated with the stack algorithm. Firstly, the
number of computations which the algorithms performs is very dependent on the quality
of the channel. If we have a very noisy channel, the received sample value yr will be very
unreliable and a large number of possible paths will have similar metrics. These paths all
have to be stored in the stack and explored further. This causes a computational speed
problem, since the incoming symbols have to be stored in a buﬀer while the algorithm
performs the decoding operation. This buﬀer is now likely to overﬂow if the channel is
very noisy and the decoder will have to declare a decoding failure. This phenomenon is
explored further in Section 5.11. In practice, the transmitted data will be framed and the
decoder will declare a frame erasure if it experiences input buﬀer overﬂow.
A second problem with the stack algorithm is the increasing complexity of Step 2, i.e.,
of reordering the stack. This sorting operation depends on the size of the stack, which,
again, for very noisy channels becomes large. This problem is addressed in all practical
applications by ignoring small diﬀerences in the metric and collecting all stack entries with
metrics within a speciﬁed “quantization interval” in the same bucket. Bucket j contains
all stack entries with metrics
jΔ ≤L(˜x(i), r) ≤(j + 1)Δ,
(5.13)
where Δ is a variable quantization parameter. Incoming paths are now sorted only into
the correct bucket, avoiding the sorting complexity of the large stack. The depositing and
removal of the paths from the buckets can occur on a “last in, ﬁrst out” basis.
There are a number of variations of this basic theme. If Δ is a ﬁxed value, the number
of buckets can also grow to be large, and the sorting problem, originally avoided, reappears.
An alternative is to let the buckets vary in size, rather than in metric range. In that way,
the critical dependence on the stack size can be avoided.

5.4. THE FANO ALGORITHM
185
An associated problem with the stack is that of stack overﬂow. This is less severe
and the remedy is simply to drop the last path in the stack from future consideration.
The probability of actually loosing the correct path is very small, a much smaller problem
than that of a frame erasure. A large number of variants of this algorithm are feasible and
have been explored in the literature. Further discussion of the details of implementation
of these algorithms are found in [3, 4, 30].
5.4
The Fano Algorithm
Unlike the stack algorithm, the Fano algorithm is a depth-ﬁrst tree search procedure in
its purest form. Introduced by Fano [21] in 1963, this algorithm stores only one path and
thus, essentially, requires no storage. Its drawback is a certain loss in speed compared
to the stack algorithm for higher rates [27], but for moderate rates the Fano algorithm
decodes faster than the stack algorithm [28]. It seems that the Fano algorithm is the
preferred choice for practical implementations of sequential decoding algorithms.
Since the Fano algorithm only stores one path, it must allow for backtracking. Also,
there can be no jumping between non-connected nodes, i.e., the algorithm only moves
between adjacent nodes which are connected in the code tree. The algorithm starts at
the initial node and moves in the tree by proceeding from one node to a successor node
with a suitably large metric. If no such node can be found, the algorithm backtracks and
looks for other branches leading oﬀfrom previously visited nodes. The metrics of all these
adjacent nodes can be computed by adding or subtracting the metric of the connecting
branch and no costly storing of metrics is required. If a node is visited more than once,
its metric is recomputed. This is part of the computation/storage trade-oﬀof sequential
decoding.
The algorithm proceeds along a chosen path as long as the metric continues to increase.
It does that by continually tightening a metric threshold to the current node metric as it
visits nodes for the ﬁrst time. If new nodes along the path have a metric smaller than
the threshold, the algorithm backs up and looks at other node extensions. If no other
extensions with a metric above the threshold can be found, the value of the threshold is
decreased and the forward search is resumed. In this fashion, each node visited in the
forward direction more than once is reached with a progressively lower threshold each
time. This prevents the algorithm from getting caught in an inﬁnite loop. Eventually this
procedure reaches a terminal node at the end of the tree and a decoded symbol sequence
can be output.
Figure 5.2 depicts an example of the search behavior of the Fano algorithm. Assume
that there are two competing paths, where the solid path is the most likely sequence and
the dashed path is a competitor. The vertical height of the nodes in Figure 5.2 is used
to illustrate the values of the metrics for each node. Also assume that the paths shown

186
CHAPTER 5. TRELLIS AND TREE DECODING
are those with the best metrics, i.e., all other branches leading oﬀfrom the nodes lead to
nodes with smaller metrics. Initially, the algorithm will proceed to node A, at which time
it will start to backtrack since the metric of node D is smaller than that of node A. After
exploring alternatives and successively lowering the threshold to t1 and then to t2, it will
reach node O again and proceed along the dashed path to node B and node C. Now it
will start to backtrack again, lowering its threshold to t3 and then to t4. It will now again
explore the solid path beyond node D to node E, since the lower threshold will allow that.
From there on, the path metrics pick up again and the algorithm proceeds along the solid
path. If the threshold decrement Δ had been twice as large, the algorithm would have
moved back to node O faster, but would also have been able to move beyond the metric
dip at node F, and would have chosen the erroneous path.
O
A
B
D
C
E
tC t1
t3 t2
t4
tA
F
Δ
Figure 5.2: Illustration of the operation of the Fano algorithm when choosing between two
competing paths.
It becomes obvious that at some point the metric threshold t will have to be lowered to
the lowest metric value which the maximum likelihood path assumes, and, consequently,
a large decrement Δ allows the decoder to achieve this low threshold faster. Conversely,
if the decrement Δ is too large, t may drop to a value which allows several erroneous path
to be potentially decoded before the maximum metric path. The optimal value of the
metric threshold is best determined by experience and simulations. Figure 5.3 shows the
ﬂowchart of the Fano algorithm.
5.5
The M-Algorithm
The M-algorithm is a purely breadth-ﬁrst synchronous algorithm which moves from time
unit to time unit. It keeps M candidate paths at each iteration and deletes all others from
further consideration. At each time unit the algorithm extends all M currently held nodes
to form 2kM new nodes, from among which those M with the best metrics are retained.
Due to the breadth-ﬁrst nature of the algorithm, the metric in (5.12) can be used. The
algorithm is very simple:

5.5. THE M-ALGORITHM
187
Look forward
to best node
new metric
Jn+1 >= t
move forward
n = n+1
end of
search tree
n=l
STOP
first
visit
tighten
threshold
t=Jn
look
back
old metric
Jn-1 >= t
loosen
threshold
t=t-6
move back
n=n-1
from
worst node
Look forward
to next best node
START:
J0=0;J-1=-infinity
threshold t = 0;n=0
Yes
No
Yes
No
No
Yes
Yes
No
Yes
No
Figure 5.3: Flowchart of the Fano algorithm. The initialization of J−1 = −∞has the
eﬀect that the algorithm can lower the threshold for the ﬁrst step, if necessary.
Step 1: Initialize an empty list L of candidate paths and their metrics. Deposit the
zero-length path () with its metric L((), y) = 0 in the list. Set n = −l.
Step 2: Extend 2k partial paths ˜x(i) →(˜x(i), x(i)
r ) from each of the at most M paths
˜x(i) in the list. Delete the entries in the original list.
Step 3: Find the at most M partial paths with the best metrics among the extensions2
and save them in the list L. Delete the rest of the extensions. Set n = n + 1.
Step 4: If at the end of the tree, i.e., n = l, release the output symbols corresponding
to the path with the best metric in the list L, otherwise go to Step 2.
2Note that from two or more extensions leading to the same state (see Section 5.6), all but the one
with the best metric may be discarded. This will improve performance slightly by eliminating some paths
which cannot be the ML path.

188
CHAPTER 5. TRELLIS AND TREE DECODING
The M-algorithm appeared in the literature for the ﬁrst time in a paper by Jelinek
and Anderson [33], where it was applied to source coding. At the time, there were no real
algorithms for sequential source coding other than this, so it was viewed as miraculous.
In the early 1980s, applications of the algorithm to channel coding began to appear. The
research book by Anderson and Mohan on algorithmic source and channel coding [4], along
with [2], collects much of this material, and these are the most comprehensive sources on
the subject.
This algorithm is straightforward to implement, and its popularity is partly due to
the simple metric as compared to sequential decoding. The decoding problem with the
M-Algorithm is the loss of the correct path from the list of candidates, after which the
algorithm might spend a long time resynchronizing. This problem is usually addressed by
framing the data. With each new frame resynchronization is achieved. The computational
load of the M-algorithm is independent of the size of the code; it is proportional to M.
Unlike depth-ﬁrst algorithms, it is also independent of the quality of the channel, since M
paths are retained irrespective of the channel quality.
A variant of the M-algorithm is the so-called T-algorithm. It diﬀers from the M-
algorithm only in Step 3, where instead of a ﬁxed number M, all path with metrics
L(˜x(i), y) ≥λt −T are retained, where λt is the metric of the best path and T is some
arbitrary threshold. The T-algorithm is therefore in a sense a hybrid between the M-
algorithm and a stack-type algorithm. Its performance depends on T, but is very similar
to that of the M-algorithm, and we will not discuss it further.
The major problem with the M-algorithm is the possibility that the correct path is
not among the M retained candidates at time n. If this happens, we lose the correct path
and it usually takes a long time to resynchronize.
Since the complexity of the M-algorithm is largely independent of the code size and
constraint length, one usually chooses very long constraint-length codes to assure that
free distance of the code, dfree, is very large, and therefore errors due to neighboring
codewords are not the dominant error mechanism, correct path loss primarily determines
the performance of the decoder.
Let us then take a closer look at the probability of losing the correct path at time n.
To that end we assume that at time n −1 the correct path was among the M retained
candidates as illustrated in Figure 5.4. Each of these M nodes is extended into 2k nodes
at time n, of which M are to be retained. There are then a total of
M2k
M

ways of choosing
the new M retained paths at time n.
Let us denote the correct partial path by ˜x(c). The optimal strategy of the decoder
will then be to retain that particular set of M candidates which maximizes the probability
of containing ˜x(c). Let Cp be one of the
M2k
M

possible sets of M candidates at time n.
We wish to maximize
max
p
Pr

˜x(c) ∈Cp|˜y

.
(5.14)

5.5. THE M-ALGORITHM
189
Since all the partial paths ˜x(pj) ∈Cp, j = 1, . . . , M are distinct, the events {˜x(c) = ˜x(pj)}
are all mutually exclusive for diﬀerent j, i.e., the correct path can be at most only one of
the M diﬀerent candidates ˜x(pj). Equation (5.14) can therefore be evaluated as
max
p
Pr

˜x(c) ∈Cp|˜y

= max
p
M

j=1
Pr

˜x(c) = ˜x(pj)
j
|˜y

.
(5.15)
correct
path loss
time:  -l
        -l+1  
      -l+2        
   -l+3   . . . . .
 .    n-1      
      n    
        
 n+
1 
  
Figure 5.4: Extension of 2kM = 2 · 4 paths from the list of the best M = 4 paths. The
solid paths are those retained by the algorithm, the path indicated by the heavy line
corresponds to the correct transmitted sequence.
From (5.7), (5.9), and (5.12) we know that
Pr

˜x(c) = ˜x(pj)|˜y

∝exp

−
−l+n

r=−l

2Re

x(pj)
r
v∗
r

−|x(pj)
r
|2
,
(5.16)
where the proportionality constant is independent of ˜x(pj). The maximization in (5.14)
now becomes equivalent to (considering only the exponent from above)
max
p
Pr

˜x(c) ∈Cp|˜y

≡
max
p
M

j=1
−l+n

r=−l

2Re

x(pj)
r
v∗
r

−|x(pj)
r
|2
=
max
p
J(pj)
n
,
(5.17)

190
CHAPTER 5. TRELLIS AND TREE DECODING
i.e., we simply collect the M paths with the best partial metrics J(pj)
n
at time n. This
argument was derived by Aulin [5]. Earlier we showed that the total metric can be bro-
ken up into the recursive form of (5.6), but now we have shown that if the detector is
constrained to considering only a maximum of M path at each stage, retaining those M
paths ˜x(pj) with maximum partial metrics is the optimal strategy.
The probability of correct path loss, denoted by Pr(CPL), can now be addressed.
Follow the methodology of Aulin [5], we need to evaluate the probability that the correct
path ˜x(c) is not among the M candidate paths. This will happen if M paths ˜x(pj) = ˜x(c)
have a partial metric J(pj)
n
≥J(c)
n , or equivalently if all the M metric diﬀerences
δ(j,c)
n
= J(c)
n
−J(pj)
n
=
−l+n

r=−l

|x(pj)
r
|2 −|x(c)
r |2 −2Re

x(pj)
r
−x(c)
r

v∗
r

(5.18)
are smaller than or equal to zero. That is,
Pr(CPL|Cp) = Pr{δn ≤0},
˜x(pj) ∈Cp,
(5.19)
where δn =

δ(1,c)
n
, . . . , δ(M,c)
n

is the vector of metric diﬀerences at time n between the
correct path and the set of paths in a given set Cp, which does not contain ˜x(c). Pr(CPL|Cp)
depends on the correct path x(c) and, strictly speaking, has to be averaged over all correct
paths. We shall be satisﬁed with the correct path which produces the largest P(CPL|Cp).
In Appendix 5.C we show that the probability of losing the correct path decreases
exponentially with the signal-to-noise ratio and is overbounded by
Pr(CPL|Cp) ≤Q
⎛
⎝

d2
l
2N0
⎞
⎠.
(5.20)
The parameter d2
l depends on Cp and is known as the Vector Euclidean distance [5] of the
path ˜x(c) with respect to the M error paths ˜x(pi) ∈Cp. It is important to note here that
(5.20) is an upper bound of the probability that M speciﬁc error paths have a metric larger
than ˜x(c). The problem is ﬁnding the minimal d2
l among all sets Cp, denoted by min(d2
l ).
This is a rather complicated combinatorial problem, since essentially all combinations of
M candidates for each correct path at each time n have to be analyzed from the growing
set of possibilities. Aulin [5] has studied this problem and gives several rejection rules
which alleviate the complexity of ﬁnding d2
l , but the problem remains complex.
Note that d2
l is a non-decreasing function of M, the decoder complexity, and one way
of selecting M is to choose it such that
min(d2
l ) ≥d2
free.
(5.21)

5.5. THE M-ALGORITHM
191
This choice should guarantee that the performance of the M-algorithm is approximately
equal to the performance of maximum-likelihood decoding. To see this, let Pe(M) be the
probability of an error event (compare equation (5.66)). Then
Pe(M)
≤
Pe (1 −Pr(CPL)) + Pr(CPL)
≤
Pe + Pr(CPL),
(5.22)
where Pe is the probability that a maximum-likelihood decoder starts an error event
(see Section 5.9).
For high values of the signal-to-noise ratio, equation (5.22) can be
approximated by
Pe(M) ≈NdfreeQ
 dfree
√2N0

+ κQ
min(dl)
√2N0

,
(5.23)
where κ is some constant, which, however, is diﬃcult to determine in the general case.
Now, if (5.21) holds, the suboptimality of the M-algorithm does not dominate the error
performance for high signal-to-noise ratios.
Aulin [5] has analyzed this situation for 8-PSK trellis codes and found that, in general,
M ≈
√
S will fulﬁll condition (5.21), where S is the number of states in the code trellis.
Figure 5.5 shows the simulated performance of the M-algorithm versus M for the 64-
state trellis code from Table 3.1 with d2
free = 6.34. M = 8 meets (5.21) according to [5],
but from Figure 5.5 it is apparent that the performance is still about 1.5 dB poorer than
ML-decoding. This is attributable to the resynchronization problems and the fact that
we are operating at rather low values of the signal-to-noise ratio, where neither d2
free nor
min(d2
l ) are necessarily dominating the error performance.
Figures 5.6 and 5.7 show the empirical probability of correct path loss P(CPL) and the
BER for two convolutional codes and various values of M. Figure 5.6 shows simulation
results for the 2048-state convolutional code, ν = 11, from Table 4.1. The bit error rate
and the probability of losing the correct path converge to the same asymptotic behavior,
indicating that the probability of correct path loss and not recovery errors is the dominant
error mechanism for very large values of the signal-to-noise ratio.
Figure 5.7 shows simulation results for the ν = 15 large constraint-length code for the
same values of M. For this length code, path loss will be the dominant error scenario. We
note that both codes have a very similar error performance, demonstrating that the code
complexity has little inﬂuence.
Once the correct path is lost, the algorithm may spend a long time before it ﬁnds it
again, i.e., before the correct path is again one of the M retained paths. We use this
terminology in the following way: Technically, each path through the tree is distinct, and,
once lost, the correct path is lost forever, or at least until the end of the frame. However,
It can easily be appreciated that due to the ﬁnite memory of codes described by “small”
trellises, even after loss of the correct path, the decoder may ﬁnd a sub-tree later on, which

192
CHAPTER 5. TRELLIS AND TREE DECODING
1
2
3
4
5
6
7
8
9
10−6
10−5
10−4
10−3
10−2
10−1
1
Eb/N0[dB]
M = 2
M = 3
M = 4
M = 6
M = 8
M = 16
ML decoding
Symbol Error Probability (BER)
Figure 5.5: Simulation results for the 64-state optimal distance 8-PSK trellis codes decoded
with the M-algorithm, using M = 2, 3, 4, 5, 6, 8, and 16. The performance of maximum
likelihood decoding is also included in the ﬁgure (Source: [5]).
is identical with the correct sub-tree. Once the algorithm joins this section of the tree, we
say it has recovered the correct path. This terminology will make more sense once we use
the trellis as decoder map.
Correct path recovery, however, is a very complex problem and no complete analytical
approach exists to date. There are a few theoretical studies of the recovery problem, such
as [6], but most of the understanding into the dynamics of the decoder during a recovery
has been gained through simulation studies.
Figure 5.8 presents such a simulation showing the average number of steps taken for
the algorithm to recover the correct path for the 2048-state, ν = 11 convolutional code,
whose error performance is shown in Figure 5.6.
Each instance of the simulation was
performed such that the algorithm was initiated and run until the correct path was lost,
and then the number of steps until recovery were counted [34].
In contrast, Figure 5.9 shows the average number of steps until recovery for the rate 1/2,
ν = 11 systematic convolutional code with generator polynomials g(0) = 4000, g(1) = 7153.
This code has a free Hamming distance of only dfree = 9, but its recovery performance is
much superior to that of the non-systematic code. In fact, the average number of steps

5.5. THE M-ALGORITHM
193
0
1
2
3
4
5
6
7
8
10−6
10−5
10−4
10−3
10−2
10−1
1
Eb/N0[dB]
M = 64
M = 16
M-Algo: M = 4
M-Algo: M = 16
M-Algo: M = 64
Symbol Error Probability (BER)
Figure 5.6: Simulation results for the 2048-state, rate R = 1/2 convolutional code using
the M-algorithm, for M = 4, 16, and 64. The dashed curves are the path loss probability
and the solid curves are BER’s.
until recovery is independent of the signal-to-noise ratio, while it increases approximately
linearly with Eb/N0 for the non-systematic code. This rapid recovery results in superior
error performance of the systematic code compared to the non-systematic code, shown in
Figure 5.10. However, what can also clearly be seen in Figure 5.10 is that, for very large
values of Eb/N0, the “stronger” code will win out due to its larger free distance.
The observation that systematic convolutional codes outperform non-systematic codes
for error rates Pb
>≈10−6 has also been made by Osthoﬀet al. [38]. The reason for this lies
in the return barrier phenomenon, which can be explained with the aid of Figure 5.11. In
order for the algorithm to recapture the correct path after a correct path loss, one of the
M retained paths must correspond to a trellis state that connects to the correct state. In
Figure 5.11 we assume that the all-zero sequence is the correct sequence, and hence the
all-zero state is the correct state for all time. This assumption is made without loss of
generality for convolutional codes due to their linearity. For a feed-forward realization of a
rate 1/2 code, the only state which connects to the all-zero state is the state (0, · · · , 0, 1),
denoted by sm in the ﬁgure. In the case of a systematic code with g(1)
0
= g(1)
ν
= 1 (see
Chapter 4) the two possible branch signals are (01) and (10) as indicated in Figure 5.11.

194
CHAPTER 5. TRELLIS AND TREE DECODING
0
1
2
3
4
5
6
7
8
10−6
10−5
10−4
10−3
10−2
10−1
1
Eb/N0[dB]
M = 64
M = 16
M-Algo: M = 4
M-Algo: M = 16
M-Algo: M = 64
Symbol Error Probability (BER)
Figure 5.7: Same simulation results for the ν = 15, R = 1/2 convolutional code.
0
1
2
3
4
5
6
7
8
1
10
102
103
104
105
Eb/N0[dB]
M = 64
M = 16
M = 8
M = 4
M = 2
Average number of steps until recovery
Figure 5.8: Average number of steps until recovery for the code from Figure 5.6 [34].

5.5. THE M-ALGORITHM
195
0
1
2
3
4
5
6
7
8
1
10
102
103
104
105
Eb/N0[dB]
M = 64
M = 16
M = 8 M = 4
M = 2
Average number of steps until recovery
Figure 5.9: Average number of steps until recovery of the correct path for the systematic
convolutional code with ν = 11 ([34]).
For a non-systematic, maximum free distance code on the other hand, the two branch
signals are (11) and (00), respectively. Since the correct branch signal is (00), the prob-
ability that the metric of sf (for failed) exceeds the metric of sc equals 1/2, since both
branch signals are equidistant from the correct signal. For the non-systematic code on the
other hand, this probability equals Q(

Es/N0). This explains the dependence of the path
recovery on Eb/N0 for non-systematic codes, as well as why systematic codes recapture
the correct path faster with a recovery behavior which is independent of Eb/N0.
The M-algorithm impresses with its simplicity. Unfortunately, a theoretical under-
standing of the algorithm is not related to this simplicity at all, and it seems that much
more work in this area is needed before a coherent theory is available. This lack of a
theoretical basis for the algorithm is, however, no barrier to its implementation. Early
work on the application of the M-algorithm to convolutional codes, apart from Anderson
[2, 3, 4, 38], was presented by Zigangirov and Kolesnik [65], while Simmons and Wittke
[47], Aulin [7], and Balachandran [10], among others, have applied the M-algorithm to
continuous-phase modulation. General trellis codes have not yet seen much action from
the M-algorithm. An notable exception is [40].
It is generally felt that the M-algorithm is not a viable candidate algorithm for decod-
ing binary convolutional codes, in particular with the emergence of turbo codes and iter-
ative decoding; however, it seems to work very well with non-binary modulations such as

196
CHAPTER 5. TRELLIS AND TREE DECODING
0
1
2
3
4
5
6
7
8
10−6
10−5
10−4
10−3
10−2
10−1
1
Eb/N0[dB]
M = 64
M = 16
M = 64
M = 16
Symbol Error Probability (BER)
Figure 5.10: Simulation results for the superior 2048-state systematic code using the M-
algorithm. The dashed curves are the error performance of the same constraint length
non-systematic code from Figure 5.6 (Source [34]).
0...000
1...000
0...001
(10)
(01)
0...000
(00)
correct path
0...000
1...000
0...001
(11)
0...000
(00)
correct path
(00)
return barrier
systematic codes
non-systematic codes
sm
sf
sc
sm
sf
sc
Figure 5.11: Heuristic explanation of the return barrier phenomenon in the M-algorithm.
CPM, trellis-coded modulation, and certain multiple access systems such as code-division
multiple (CDMA) access, where it may have a place in practical implementations.

5.6. MAXIMUM LIKELIHOOD DECODING
197
5.6
Maximum Likelihood Decoding
The diﬃculty in decoding arises from the exponential size of the growing decoding tree.
In this section we will show that this tree can be reduced by merging nodes, such that the
tree only grows to a maximum size of 2S nodes, where S is the number of encoder states.
This merging leads diverging paths together again and we obtain a structure resembling
a trellis, as discussed for encoders in Chapter 4. In fact, because the paths through the
trellis of the code represent all possible codewords, it should be intuitively evident that
a decoder will also need to explore no more then all those paths, which can be done by
operating such a decoder on the trellis of the code.
In order to see how this happens, let J(i)
n−1 and J(j)
n−1 be the metrics of two nodes
corresponding to the partial sequences ˜x(i) and ˜x(j) of length n −1, respectively. Let the
encoder states which correspond to ˜x(i) and ˜x(j) at time n −1 be s(i)
n−1 and s(j)
n−1; s(i)
n−1 =
s(j)
n−1, and assume that the next extension of ˜x(i) →(˜x(i), x(i)
n ) and ˜x(j) →(˜x(j), x(j)
n ) is
such that s(i)
n = s(j)
n , i.e., the encoder states at time n are identical. See also Figure 5.12
below.
eliminate
s(i)
n−1
s(i)
n−1
s(i)
n
s(i)
n
x(i)
n
x(i)
n
x(i)
n+1 = x(j)
n+1
x(i)
n+1 = x(j)
n+1
Figure 5.12: Merging nodes.
Now we propose to merge the two nodes (˜x(i), x(i)
n ) and (˜x(j), x(j)
n ) into one node, which
we now call a (decoder) state. We retain the partial sequence which has the larger metric
Jn at time n and discard the partial sequence with the smaller metric. Ties are broken
arbitrarily. We are now ready to prove the following
Theorem 5.1 (Theorem of Non-Optimality) The procedure of merging nodes which cor-
respond to identical encoder states and then discarding the path with the smaller metric
never eliminates the maximum-likelihood path.
Theorem 5.1 is sometimes referred to as the theorem of non-optimality and allows us
to construct a maximum-likelihood decoder whose complexity is signiﬁcantly smaller than
that of an all-out exhaustive tree search.

198
CHAPTER 5. TRELLIS AND TREE DECODING
Proof: The metric at time n + k for path i can be written as
J(i)
n+k = J(i)
n +
k

h=1
β(i)
n+h
(5.24)
for every future time index n + k; 0 < k ≤l −n, where β(i)
n
= 2Re

x(i)
n v∗
n

−|x(i)
n |2 is
the metric increment, now also called the branch metric, at time n. Now, if the nodes
of path i and j correspond to the same encoder state at time n, there exists for every
possible extension of the ith path (x(i)
n+1, . . . , x(i)
n+k) a corresponding identical extension
(x(j)
n+1, . . . , x(j)
n+k) of the jth path. Let us then assume without loss of generality that the
ith path accumulates the largest metric at time l, i.e., J(i)
l
≥J(j)
l
, Therefore
J(i)
n +
n−l

h=1
β(i)
n+h ≥J(j)
n
+
n−l

h=1
β(j)
n+h,
(5.25)
and n−l
h=1 β(i)
n+h is the maximum metric sum for the extensions from node (˜x(i), x(i)
n ).
(Otherwise another path would have a higher ﬁnal metric.) But since the extensions for
both paths are identical, n−l
h=1 β(i)
n+h = n−l
h=1 β(j)
n+h and J(i)
n
≥J(j)
n . Path j can therefore
never accumulate a larger metric than path i, and we may discard it with impunity at
time n.
Q.E.D.
The tree now folds back on itself and forms a trellis with exactly S, identical to the
encoder trellis (e.g., Figure 3.3), and there are 2k paths merging in a single state at each
step. Note then that there are now at most S retained partial sequences ˜x(i), called the
survivors. The most convenient labeling convention is that each state is labeled by the
corresponding encoder state, plus the survivor which leads to it. This trellis is a replica
of the encoder trellis discussed in Chapter 4, and the task of the decoder is to retrace the
path the encoder took through this trellis. Theorem 5.1 guarantees that this procedure is
optimal. This method was introduced by Viterbi in 1967 [51, 39] in the context of analyzing
convolutional codes, and has since become widely known as the Viterbi Algorithm [23]:
Step 1: Initialize the S states of the decoder with a metric J(i)
−l = −∞and survivors
˜x(i) = {}. Initialize the starting state of the encoder, usually state i = 0, with the
metric J(0)
−l = 0. Let n = −l.
Step 2: Calculate the branch metric
βn = 2Re{xnv∗
n} −|xn|2
(5.26)
for each state s(i)
n and each extension x(i)
n .

5.6. MAXIMUM LIKELIHOOD DECODING
199
Step 3: Follow all trellis transitions s(i)
n →s(i)
n+1 determined by the encoder FSM and,
from the 2k merging paths, retain the survivor ˜x(i) for which J(i)
n+1 is maximized.
Step 4: If n < l, let n = n + 1 and go to Step 2.
Step 5: Output the survivor x(i) which maximizes J(i)
l
as the maximum-likelihood esti-
mate of the transmitted sequence.
Steps 2 and 3 are the central operations of the Viterbi algorithm and are referred
to as the Add–Compare–Select (ACS) step. That is, branch metrics are added to state
metrics, comparisons are made among all incoming branches, and the largest-metric path
is selected.
The Viterbi algorithm and the M-algorithm are both breadth-ﬁrst searches and share
some similarities. In fact, one often introduces the concept of mergers also in the M-
algorithm in order to avoid carrying along suboptimal paths. In fact, the M-algorithm
can be operated in the trellis rather than in the tree. The Viterbi algorithm has enjoyed
tremendous popularity, not only in decoding trellis codes, but also in symbol sequence
estimation over channels aﬀected by intersymbol interference [41, 24], multi-user optimal
detectors [50], and speech recognition. Whenever the underlying generating process can
be modeled as a ﬁnite-state machine, the Viterbi algorithm ﬁnds application.
A rather large body of literature deals with the Viterbi decoder, and there are a number
of good books dealing with the subject (e.g., [30, 41, 13, 52]). One of the more important
results is that it can be shown that one does not have to wait until the entire sequence is
decoded before starting to output the estimated symbols x(i)
n , or the corresponding data.
The probability that the symbols in all survivors ˜x(i) are identical for m < n −nt, where
n is the current active decoding time and nt, called the truncation length or decision depth
(Section 3.2 and Equation (4.16)), is very close to unity for nt ≈5ν. This has been shown
to be true for rate 1/2 convolutional codes [15, Page 182], but the argument can easily be
extended to general codes. We may therefore modify the algorithm to obtain a ﬁxed-delay
decoder by modifying Step 4 and 5 of the above Viterbi algorithm as follows:
Step 4: If n ≥nt, output x(i)
n−nt from the survivor ˜x(i) with the largest metric J(i)
n
as
the estimated symbol at time n −nt. If n < l −1, let n = n + 1 and go to Step 2.
Step 5: Output the remaining estimated symbols x(i)
n ; l −nt < n ≤l from the survivor
x(i) which maximizes J(i)
l .
We recognize that we may now let l →∞, i.e., the complexity of our decoder is no
longer determined by the length of the sequence, and it may be operated in a continu-
ous fashion. The simulation results in Section 5.9 were obtained with a Viterbi decoder
according to the modiﬁed algorithm.

200
CHAPTER 5. TRELLIS AND TREE DECODING
Let us spend some thoughts on the complexity of the Viterbi algorithm. Denote by E
the total number of branches in the trellis; i.e., there are S2k branches per time epoch for
a trellis generated by a rate k/n convolutional code. The complexity requirements of the
Viterbi algorithm can then be captured by the following theorem [35]
Theorem 5.2 The Viterbi algorithm requires a complexity which is linear in the number
of edges E, i.e., it performs O(E) arithmetic operations (multiplications, additions and
comparisons).
Proof: Step 2 in the Viterbi algorithm requires the calculation of βn, which needs two
multiplies and an addition, as well as the addition J(i)
n + βn for each branch. Some of the
values βn may be identical, the number of arithmetic operations is therefore larger than
E additions and less than 2E multiplications and additions.
If we denote the number of branches entering state s by ρ(s), then Step 3 in the above
algorithm requires 
states s(ρ(s) −1) ≤E/2l comparisons per time epoch. ρ(s) = 2k in
our case, and the total number of comparisons is therefore less than E, and larger than
E −2lS.
There are then together O(E) arithmetic operations required.
Q.E.D.
5.7
A Posteriori Probability Symbol Decoding
The purpose of the a posteriori probability (APP) algorithm is to compute a posteriori
probabilities on either the information bits or the encoded symbols. These probabilities
are mainly important in the iterative decoding algorithms for turbo codes discussed later
in this book. Maximizing the a posteriori probabilities by themselves leads to only minor
improvements in terms of bit error rates compared to the Viterbi algorithm. The algorithm
was originally presented by Bahl, Cocke, Jelinek, and Raviv [9] in 1972 and was used to
maximize the probability of each symbol being correct, referred to as the maximum a
posteriori probability (MAP) algorithm. However, it provided no signiﬁcant improvement
over maximum-likelihood sequence decoding and is signiﬁcantly more complex. Prior to
the age of turbo coding, it was consequently not much used.
With the invention of turbo codes in 1993, however, the situation turned, and the
APP became the major representative of the so-called soft-in soft-out (SISO) algorithms
for providing probability information of the individual symbols of a trellis code. These
probabilities are required for iterative decoding schemes and concatenated coding schemes
with soft decision decoding of the inner code, such as iterative decoding of turbo codes,
which is discussed in Chapter 8.
The APP algorithm is a representative of a general class of belief propagation algo-
rithms, which we will examine in Chapter 6, applied to the special case of trellis graphs.

5.7. A POSTERIORI PROBABILITY SYMBOL DECODING
201
Due to its importance, we will ﬁrst give a functional description of the algorithm before
deriving the formulas in detail. Figure 5.13 shows the example trellis of a short termi-
nated trellis code with seven sections. The transmitted signal is x = [x0, . . . , x6], and the
information symbols are u = [u0, . . . , u4, u5 = 0, u6 = 0], i.e., there are two tail bits that
drive the encoder back into the zero-state.
u0, x0
u1, x1
u2, x2
u3, x3
u4, x4
0, x5
0, x6
Figure 5.13: Example trellis of a short terminated trellis code.
The ultimate purpose of the algorithm is the calculation of a posteriori probabilities,
such as Pr[ur|y] or Pr[xr|y], where y is the received sequence observed at the output of
a channel, whose input is the transmitted sequence x. However, conceptually, it is more
immediate to calculate the probability that the encoder traversed a speciﬁc transition in
the trellis, i.e., Pr[sr = i, sr+1 = j|y], where sr is the state at epoch r, and sr+1 is the
state at epoch r + 1. The algorithm computes this probability as the product of three
terms:
Pr[sr = i, sr+1 = j|y]
=
1
Pr(y)Pr[sr = i, sr+1 = j, y]
=
1
Pr(y)αr−1(i)γr(j, i)βr(j).
(5.27)
The α-values are internal variables of the algorithm and are computed by the forward
recursion
αr−1(i) =

states l
αr−2(l)γr−1(i, l).
(5.28)
This forward recursion evaluates α-values at time r−1 from previously calculated α-values
at time r −2, and the sum is over all states l at time r −2 that connect with state i at
time r −1. The forward recursion is illustrated in Figure 5.14. The α-values are initiated
as α(0) = 1, α(1) = α(2) = α(3) = 0. This automatically enforces the boundary condition
that the encoder starts in state 0.

202
CHAPTER 5. TRELLIS AND TREE DECODING
u0, x0
u1, x1
u2, x2
u3, x3
u4, x4
0, x5
0, x6
i
j
Figure 5.14: Illustration of the forward recursion of the APP algorithm.
The β-values are calculated by an analogous procedure, called the backward recursion
βr(j) =

states k
βr+1(k)γr+1(k, j),
(5.29)
and initialized as β(0) = 1, β(1) = β(2) = β(3) = 0 to enforce the terminating condition
of the trellis code. The sum is over all states k at time r + 1 to which state j at time r
connects. The backward recursion is illustrated in Figure 5.15.
u0, x0
u1, x1
u2, x2
u3, x3
u4, x4
0, x5
0, x6
i
j
Figure 5.15: Illustration of the forward recursion of the APP algorithm.
The γ-values are conditional transition probabilities and are the inputs to the algo-
rithm. γr(j, i) is the joint probability that the state at time r + 1 is sr+1 = j and that yr
is received, and it is calculated as
γr(j, i) = Pr(sr+1 = j, yr|sr = i) = Pr[sr+1 = j|sr = i]Pr(yr|xr).
(5.30)
The ﬁrst term, Prp[sr+1 = j|sr = i], is the a priori transition probability and is related to
the probability of ur. In fact, in our example, the top transition is associated with ur = 1

5.7. A POSTERIORI PROBABILITY SYMBOL DECODING
203
and the bottom transition with ur = 0.
This factor can and will be used to account
for a priori probability information on the bits ur. In the sequel we will abbreviate this
transition probability by
pij = Pr(sr+1 = j|sr = i) = Pr(ur).
(5.31)
The second term, Pr(yr|xr), is simply the conditional channel transition probability, given
that symbol xr is transmitted. Note that xr is the symbol associated with the transition
from state i →j.
The a posteriori symbol probabilities Pr[ur|y] can now be calculated from the a pos-
teriori transition probabilities (5.27) by summing over all transitions corresponding to
ur = 1 and, separately, by summing over all transitions corresponding to ur = 0, to obtain
p[ur = 1|y]=
1
Pr(y)

solid
Pr[sr = i, sr+1 = j, y]
(5.32)
p[ur = 0|y]=
1
Pr(y)

dashed
Pr[sr = i, sr+1 = j, y],
(5.33)
as shown in Figure 5.16, where the solid transitions correspond to ur = 1, and the dashed
transitions correspond to ur = 0 as illustrated on the left.
u0, x0
u1, x1
u2, x2
u3, x3
u4, x4
0, x5
0, x6
Figure 5.16: Computational dynamics of the ﬁnal APP calculation.
A formal algorithm description is given at the end of this section, but ﬁrst we present
a rigorous derivation of the APP algorithm. This derivation was ﬁrst given by Bahl et
al. [9].
In the general case we will have need for the probability
qij(x) = Pr(τ(ur, sr) = x|sr = i, sr+1 = j),
(5.34)
that is, the a priori probability that the output xr at time r assumes the value x on the
transition from state i to state j. This probability is typically a deterministic function of i

204
CHAPTER 5. TRELLIS AND TREE DECODING
and j, unless there are parallel transitions, in which case xr is determined by the uncoded
information bits.
Before we proceed with the derivation, let us deﬁne the internal variables α and β by
their probabilistic meaning. These are
αr(j) = Pr(sr+1 = j, ˜y),
(5.35)
the joint probability of the partial sequence ˜y = (y−l, . . . , yr) up to and including time
epoch r and state sr+1 = j; and
βr(j) = Pr((yr+1, . . . , yl)|sr+1 = j),
(5.36)
the conditional probability of the remainder of the received sequence y given that the
state at time r + 1 is j.
With the above we now calculate
Pr(sr+1 = j, y)
=
Pr(sr+1 = j, ˜y, (yr+1, . . . , yl))
=
Pr(sr+1 = j, ˜y)Pr((yr+1, . . . , yl)|sr+1 = j, ˜y)
=
αr(j)βr(j),
(5.37)
where we have used the fact that Pr((yr+1, . . . , yl)|sr+1 = j, ˜y) =
Pr((yr+1, . . . , yl)|sr+1 = j), i.e., if sr+1 = j is known, events after time r are independent
of the history ˜y up to sr+1.
In the same way we calculate via Bayes’ expansion
Pr(sr = i, sr+1 = j, y)
=
Pr(sr = i, sr+1 = j, (y−l, . . . , yr−1), yr, (yr+1, . . . , yl))
=
Pr(sr = i, (y−l, . . . , yr−1))Pr(sr+1 = j, yr|sr = i)
×Pr((yr+1, . . . , yl)|sr+1 = j)
=
αr−1(i)γr(j, i)βr(j).
(5.38)
Now, again applying Bayes’ rule and 
b p(a, b) = p(a), we obtain
αr(j)
=

states i
Pr(sr = i, sr+1 = j, ˜y)
=

states i
Pr(sr = i, (y−l, . . . , yr−1))Pr(sr+1 = j, yr|sr = i)
=

states i
αr−1(i)γr(j, i).
(5.39)
For a trellis code started in the zero state at time r = −l we have the starting conditions
α−l−1(0) = 1, α−l−1(j) = 0,
j = 0.
(5.40)

5.7. A POSTERIORI PROBABILITY SYMBOL DECODING
205
As above, we similarly develop an expression for βr(j), i.e.,
βr(j)
=

states i
Pr(sr+2 = i, (yr+1, . . . , yl)|sr+1 = j)
=

states i
Pr(sr+2 = i, yr+1|sr+1 = j)Pr((yr+2, . . . , yl)|sr+2 = i)
=

states i
βr+1(i)γr+1(i, j).
(5.41)
The boundary condition for βr(j) is
βl(0) = 1, βl(j) = 0,
j = 0,
(5.42)
for a trellis code which is terminated in the zero state.
Furthermore, the general form of the γ-values is given by
γr(j, i)
=

xr
Pr(sr+1 = j|sr = i)Pr(xr|sr = i, sr+1 = j)Pr(yr|xr)
=

xr
pijqij(xr)pn(yr −xr),
(5.43)
where we have used the conditional density function of the AWGN channel from (2.11),
i.e., Pr(yr|xr) = pn(yr −xr). The calculation of γr(j, i) is not very complex and can most
easily be implemented by a table lookup procedure.
Equations (5.39) and (5.41) are iterative and we can now compute the a posteriori
state and transition probabilities via the following algorithm:
Step 1: Initialize α−l−1(0) = 1, α−l−1(j) = 0 for all non-zero states (j = 0) of the
encoder FSM, and βl(0) = 1, βl(j) = 0, j = 0. Let r = −l.
Step 2: For all states j calculate γr(j, i) and αr(j) via (5.43) and (5.39).
Step 3: If r < l, let r = r + 1 and go to Step 2, else r = l −1 and go to Step 4.
Step 4: Calculate βr(j) using (5.41). Calculate Pr(sr+1 = j, y) from (5.37), and Pr(sr =
i, sr+1 = j; y) from (5.27).
Step 5: If r > −l, let r = r −1 and go to Step 4.
Step 6: Terminate the algorithm and output all the values Pr(sr+1 = j, y) and Pr(sr =
i, sr+1 = j, y).

206
CHAPTER 5. TRELLIS AND TREE DECODING
Contrary to the maximum likelihood algorithm, the APP algorithms needs to go
through the trellis twice, once in the forward direction and once in the reverse direc-
tion. What is worse, all the values αr(j) must be stored from the ﬁrst pass through the
trellis. For a rate k/n convolutional code, for example, this requires 2kν2l storage locations
since there are 2kν states for each of which we need to store a diﬀerent value αr(j) at each
time epoch r. The storage requirement grows exponentially in the constraint length ν and
linearly in the block length 2l.
The a posteriori state and transition probabilities produced by this algorithm can now
be used to calculate a posteriori information bit probabilities, i.e., the probability that the
information k-tuple ur = u, where u can vary over all possible binary k-tuples. Starting
from the transition probabilities Pr(sr = i, sr+1 = j|y), we simply sum over all transitions
i →j which are caused by ur = u. Denoting these transitions by A(u), we obtain
Pr(ur = u|y) =

(i,j)∈A(u)
Pr(sr = i, sr+1 = j|y).
(5.44)
As mentioned above, another most interesting product of the APP decoder is the a
posteriori probability of the transmitted output symbol xr. Arguing analogously as above,
and letting B(x) be the set of transitions on which the output signal x can occur, we obtain
Pr(xr = x|y)
=

(i,j)∈B(x)
Pr(x|yr)Pr(sr = i, sr+1 = j|y)
=

(i,j)∈B(x)
pn(yr −xr)
p(yr)
qij(x)Pr(sr = i, sr+1 = j|y),
(5.45)
where the a priori probability of yr can be calculated via
p(yr) =

x
((i,j)∈B(x))
p(yr|x)qij(x),
(5.46)
and the sum extends over all transitions i →j.
Equation (5.45) can be much simpliﬁed if there is only one output symbol on the
transition i →j as in the introductory discussion. In that case the transition automatically
determines the output symbol, and
Pr(xr = x) =

(i,j)∈B(x)
Pr(sr = i, sr+1 = j|y).
(5.47)
One problem we have to address is that of numerical stability. The α- and β-vales in
(5.39) and (5.41) decay rapidly and will underﬂow in any ﬁxed precision implementation.
We therefore normalize both values at each epoch, i.e.,
αr(i) →
αr(i)

s αr(s),
βr(i) →
βr(i)

s βr(s).
(5.48)

5.8. LOG-APP AND APPROXIMATIONS
207
This normalization has no eﬀect on our ﬁnal results such as (5.45), since these are similarly
normalized. In fact, this normalization allows us to ignore the division by p(yr) in (5.45),
and division by Pr(y) in (5.27), (5.32), and (5.33).
5.8
Log-APP and Approximations
While the APP algorithm is concise and consists only of multiplications and additions,
current direct digital hardware implementations of the algorithm lead to complex circuits
due to many real number multiplications involved in the algorithm. In order to avoid
these multiplications, we transform the algorithm into the logarithm-domain. This results
in the so-called log-APP algorithm.
First we transform the forward recursion (5.28), (5.39) into the logarithm-domain using
the deﬁnitions
Ar(i) = log(αr(i)),
Γr(i, l) = log(γr(i, l))
(5.49)
to obtain the log-domain forward recursion
Ar−1(i) = log
 
states l
exp

Ar−2(l) + Γr−1(i, l)

.
(5.50)
Likewise the backward recursion can be transformed into the logarithm domain using the
analogous deﬁnition Br(j) = log(βr(j), and we obtain
Br(j) = log
 
states k
exp

Br+1(l) + Γr+1(k, j)

.
(5.51)
The product in (5.27) and (5.38) now turns into the simple sum
αr−1(i)γr(j, i)βr(j) →Ar−1(i) + Γr(j, i) + Br(j).
(5.52)
Unfortunately, Equations (5.50) and (5.51) contain log() and exp() functions, which
seem even more complex than the original multiplications. This is true; however, in most
cases of current practical interest, the APP algorithm is used to decode binary codes, i.e.,
there are only two branches involved at each state and therefore only sums of two terms
in (5.50) and (5.51). The logarithm of such a binary sum can be expanded as
log(exp(a) + exp(b))
=
log

exp

max(a, b)
 
1 + exp(−|a −b|)

=
max(a, b) + log
 
1 + exp(−|a −b|)

.

208
CHAPTER 5. TRELLIS AND TREE DECODING
It seems that little is gained from these manipulations, but the second term is now the
only complex operation left, and there are a number of ways to approach this. The ﬁrst,
and most complex but precise method is to store the function
log
 
1 + exp(−x)

,
x = |a −b|,
(5.53)
in a ROM lookup table. Given an example quantization of 4 bits, this is a 16 × 16 value
lookup table, which is very manageable. Figure 5.17 shows the signal ﬂow of this binary
log-domain operation on the example of a node operation in the forward recursion.
+
+
+
+
-
Comparer
Max
Min
Look-up
Table
Ar(i)
Ar−1(l)
Ar−1(l)
Γr−1(i, l)
Γr−1(i, l)
Figure 5.17: Signal ﬂow diagram of the node calculation of the Log-APP algorithm.
Finally, to binary codes the algorithm computes the log-likelihood ratio (LLR) λ(ur)
of the information bits ur using the a posteriori probabilities (5.44) as
λ(ur)
=
log
Pr(ur = 1)
Pr(ur = 0)

= log
⎛
⎜
⎜
⎜
⎝

(i,j)∈A(u=1)
αr−1(i)γr(j, i)βr(j)

(i,j)∈A(u=0)
αr−1(i)γr(j, i)βr(j)
⎞
⎟
⎟
⎟
⎠,
λ(ur)
=
log
⎛
⎜
⎜
⎜
⎝

(i,j)∈A(u=1)
exp(Ar−1(i) + Γr(j, i) + Br(j))

(i,j)∈A(u=0)
exp(Ar−1(i) + Γr(j, i) + Br(j))
⎞
⎟
⎟
⎟
⎠.
(5.54)

5.8. LOG-APP AND APPROXIMATIONS
209
The LLR is the quantity which is used in the iterative decoding algorithms of binary
turbo codes as discussed later in this book. The range of the LLR is [−∞, ∞], where a
large value signiﬁes a high probability that ur = 1.
A straightforward way of reducing the complexity of the Log-APP is to eliminate the
ROM lookup table in Figure 5.17, [42]. This has the eﬀect of approximating the forward
and backward recursions by
Ar−1(i)
=
log
 
states l
exp

Ar−2(l) + Γr−1(i, l)

≈
max
states l

Ar−2(l) + Γr−1(i, l)

(5.55)
and
Br(j)
=
log
 
states k
exp

Br+1(l) + Γr+1(k, j)

≈
max
states k

Br+1(l) + Γr+1(k, j)

(5.56)
It is very interesting to note that (5.55) is nothing else than our familiar Viterbi algorithm
for maximum-likelihood sequence decoding. Furthermore, equation (5.56) is also a Viterbi
algorithm, but operated in the reverse direction.
Analogously, the ﬁnal LLR calculation in (5.54) is approximated by
λ(ur) ≈
max
(i,j)∈A(u=1)

Ar−1(i) + Γr(j, i) + Br(j)

−
max
(i,j)∈A(u=1)

Ar−1(i) + Γr(j, i) + Br(j)

.
(5.57)
This algorithm is known the Log-Max-APP algorithm, and its big advantage is that
it only uses additions and maximization operations to approximate the LLR of ur. This
computational savings is paid for by an approximate 0.5 dB loss when these decoders are
used to decode turbo codes.
Further insight into the relationship between the Log-APP and its approximation can
be gained by considering the expressing the LLR of ur in its basic form, i.e.,
λ(ur) = log
⎛
⎜
⎜
⎜
⎜
⎝

x;(ur=1)
exp

−|y −x|2
N0


x;(ur=1)
exp

−|y −x|2
N0

⎞
⎟
⎟
⎟
⎟
⎠
,
(5.58)

210
CHAPTER 5. TRELLIS AND TREE DECODING
where the sum in the numerator extends over all coded sequences x which correspond to
information bit ur = 1, and the denominator sum extends over all x corresponding to
ur = 0.
It is quite straightforward to see that the MAX-Log-APP retains only the path in
each sum which has the best metrics, and therefore the MAX-Log-APP calculates an
approximation to the true LLR, given by
λ(ur) ≈
min
x;(ur=0)
|y −x|2
N0
−
min
x;(ur=1)
|y −x|2
N0
,
(5.59)
i.e., the metric diﬀerence between the nearest path to y with ur = 0 and the nearest path
with ur = 1. For constant energy signals this simpliﬁes to
λ(ur) ≈(x(1)
r
−x(0)
r ) · y
N0/2
,
(5.60)
where x(1)
r
= arg minx;(ur=1) |y −x|2, and x(0)
r
= arg minx;(ur=0) |y −x|2.
For high-speed turbo decoding applications requiring up to ten iterations, evaluation
of Equation (5.53) may be too complex, yet one is not readily willing to accept the half a
dB loss entailed by using the Max-Log-APP. A very eﬀective way of approximating (5.53)
is [29]
max(a, b) + log
 
1 + exp(−|a −b|)

≈max(a, b) +

0
if |a −b| > T
C
if |a −b| ≤T.
(5.61)
This simple threshold approximation is called constant-Log-APP algorithm. It is used in
the UMTS turbo code [20] and leads to a degradation with respect to the full Log-APP of
only 0.03 dB on this code [16], where the optimal parameters for this code are determined
to be C = 0.5 and T = 1.5. This reduces the ROM lookup table of the Log-APP to a
simple comparator circuit.
APP decoders are mainly used in decoders for turbo codes of various sizes.
It is
therefore desirable to make the APP algorithm itself independent of the block size of
the overall code. While the forward recursion can be performed in synchrony with the
incoming data, the backward recursion poses a problem, since the end of the block would
need to be received before it can be started. A solution lies performing a partial backward
recursion, starting some D symbol epochs in the future and using these values to calculate
the LLRs at epoch r. The basic notion of this sliding window implementation is illustrated
in Figure 5.18.

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
211
· · ·
ur−1, xr−1
ur, xr ur+1, xr+1
· · ·
βr+D(0)
βr+D(1)
βr+D(2)
βr+D(3)
Figure 5.18: Sliding window approximation to the APP algorithm.
The question now is how to initialize the values βr+D(j), and the most typical method
is to give them all the same value; a uniform initialization. Note that the exact values is
irrelevant since the LLR eliminates constants.
Note that at ﬁrst sight it seems that we have traded in a largely increased computa-
tional load, since for each forward recursion step, D backward recursion steps are needed
to ﬁnd the values of β at epoch r. However, it is computationally much more eﬃcient to
operate this algorithm in a block fashion. That is, for every D backward recursion steps,
not only a single forward step at r is executed, but a number R of forward steps. Typical
values are D = 2R, which leads to eﬃcient shared memory implementations.
5.9
Error Analysis and Distance Spectrum
The error performance analysis of trellis codes under optimal decoding (Section 5.6) is
almost exclusively based on a code’s distance spectrum used in union bounding techniques.
This approach leads to very tight bounds over a large region of signal-to-noise ratios of
practical interest.
In contrast, the same bounding techniques have had only limited success in accurately
bounding the performance of turbo codes and other iteratively decoded concatenated
codes, and a second method, based on statistical analyses, is used to complement the
distance spectrum based techniques.
As discussed in Chapter 3, the encoder for a convolutional code is a ﬁnite-state machine
(FSM), whose transitions are determined by the input data sequence. The optimum trellis
decoder uses a duplicate of this ﬁnite-state machine, which attempts to retrace the path
(i.e., the state sequence) taken by the encoder FSM. The same principles apply to all
codes, given their trellis representation is used for decoding.
Possibly the single most important measure of interest is the information bit error
probability (BER) associated with our decoder.
Unfortunately, it turns out that the

212
CHAPTER 5. TRELLIS AND TREE DECODING
calculation of the BER is extremely complex and no eﬃcient exact methods exist. We
therefore look for more accessible ways of obtaining a measure of performance. We will
proceed to consider the probability that a codeword error occurs, or, more appropriately,
for trellis codes a code sequence error occurs. Such an error happens when the decoder
follows a path in the trellis which diverges from the correct path somewhere in the trellis.
The decoder will make an error if the path it follows through its trellis does not coincide
with the path taken by the encoder. Such a scenario is illustrated in Figure 5.19, where
the decoder starts an error at time j by following an incorrect path which remerges with
the correct sequence at time j + L.
j
j + L
c
eij
incorrect path
correct path
Figure 5.19: The correct path and an error path of length L shown in the code trellis.
In general there are many overlapping error paths possible, and Figure 5.20 shows an
example trellis where the correct path is indicated by a solid line, and all possible paths,
i.e., all error paths, by dotted lines. The total length of the trellis and the code is l, and
we assume that the code is started in the zero-state and also terminated in the zero-state.
l
3
2
1
0
Figure 5.20: Trellis diagram showing a correct path and the set of possible error paths.
The probability of error, P, is the probability that any of the dotted paths in Figure
5.20 is chosen; that is, P is the probability of the union of the individual errors ei,j, given

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
213
by
P = Pr
⎛
⎝
j

i
ei,j
c
⎞
⎠,
(5.62)
where ei,j is the ith error path departing from the correct path c at time j.
To obtain an average error probability, we also need to average over all correct paths,
i.e.,
P =

c
p(c)Pr
⎛
⎝
j

i
ei,j
c
⎞
⎠,
(5.63)
where p(c) is the probability that the encoder chooses path c.
The probability in (5.63) is still diﬃcult to calculate and we use the union bound3 to
simplify the expression further to obtain
P ≤

c
p(c)

j
Pr

i
ei,j
c

.
(5.64)
If the total length l of the encoded sequence is very large, as is typically the case for
convolutional codes which don’t have a predetermined codelength, P will be become large;
in fact, it will approach unity as l →∞, and its probability may therefore not be a good
measure for the performance of a trellis code. We therefore normalize P per unit time and
consider
Pe = lim
l→∞
1
l P.
(5.65)
Since, in many cases, the inﬁnite trellis looks identical at every time unit, we can eliminate
the sum over j in (5.64) to obtain
Pe ≤

c
p(c)Pr

i
ei
c

,
(5.66)
where ei is the event that an error starts at an arbitrary but ﬁxed time unit, say j. Also,
the correct sequence c up to node j and after node j + L is irrelevant for the error path ei
of length L. Equation (5.66) can also be interpreted as the ﬁrst event error probability, i.e.,
the probability that the decoder starts its ﬁrst error event at node j. P is upperbounded
by the ﬁrst event error probability.
In order to make the bound manageable, we apply the union bound again to obtain
Pe ≤

c
p(c)

ei
Pr

ei
c

.
(5.67)
3The union bound states that the probability of the union of distinct events is smaller or equal to the
sum of the probabilities of the individual events, i.e., Pr(
Ei) ≤
i Pr(Ei).

214
CHAPTER 5. TRELLIS AND TREE DECODING
Let us denote Pr

ei
c

by Pc→ei, which, since there are only two hypotheses involved now,
is easily evaluated as (see Equation (2.14))
Pc→ei = Q

d2
ci
REb
2N0

,
(5.68)
where R = k/n is the code rate in bits/symbol, N0 is the one-sided noise power spectral
density, Eb is the energy per information bit, and d2
ci is the squared Euclidean distance
between the energy normalized signals on the error path ei and the signals on the correct
path c. Equation (5.68) is commonly called the pair-wise error probability.
The upper bound on Pe now becomes
Pe ≤

c
p(c)

ei|c
Q

d2
ci
REb
2N0

,
(5.69)
which can be rearranged into
Pe ≤

i
(d2
i ∈D)
Ad2
i Q

d2
i
REb
2N0

,
(5.70)
by counting how often each of the squared Euclidean distances d2
i occurs in (5.69) between
the signals on c and ei in a particular trellis code. D is the set of all possible such distances
d2
i , and Ad2
i is the number of times d2
i occurs is this list, termed the multiplicity of d2
i . The
multiplicity Ad2
i can be fractional since not all paths c may have the same set of distances
d2
ci with respect to their error paths. The smallest d2
i which can be found in the trellis is
called d2
free, the free squared Euclidean distance or free distance for short.
The inﬁnite set of pairs {d2, Ad2} is called the distance spectrum of the code, and we
immediately see its connection to the error probability of the code. Figure 5.21 shows
the distance spectrum of the 16-state 8-PSK code from Table 3.1, whose free squared
Euclidean distance d2
free = 5.17.
From the ﬁrst error event probability we can obtain a bound on the average bit error
probability (BER) by the following reasoning. Each error event c →ei will cause a certain
number of bit errors. If we replace Ad2 with Bd2, which is the average number of bit errors
on error paths with distance d2, we obtain a bound on the bit errors per time unit. Since
our trellis code processes k bits per time unit, the average bit error probability is bounded
by
Pb ≤

i
(d2
i ∈D)
1
kBd2
i Q

d2
i
REb
2N0

.
(5.71)

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
215
5
6
7
8
9
10
d2
0.1
1
10
102
103
104
Bd2
Ad2
{d2}
Ad2
Bd2
5.172
2.25
11.5
5.757
4.625
21.875
6
1
4
6.343
6.063
41.25
6.586
4
15
6.929
17.656
116.219
7.172
6.75
39
7.515
31.891
234.906
7.757
26.375
169.625
8
8.5
42
8.101
62.602
526.289
8.343
58.375
408.875
8.586
16.625
94.375
8.686
130.691
1163.28
8.929
134.719
1107.84
9.172
60.75
400.75
9.272
253.697
2480.54
9.414
4
17
9.515
338.688
2948.34
9.757
168.219
1253.53
9.858
511.915
5385.71
10
16.5
106
Figure 5.21: Distance spectrum of the code from Figure 3.1.
Figure 5.21 also shows the values Bd2 for this code.
Figure 5.22 is a plot of (5.71) for some popular 8-PSK Ungerb¨ock trellis codes (solid
curves) and compares it to results obtained by simulation (dashed curves). This illustrates
the tightness of (5.71), which is a good measure of the codes’ performance for Pb
<≈10−2.
Algorithms to compute a code’s distance spectrum can be generated relatively easily
from any one of the decoding algorithms described earlier in this chapter, by changing the
decoding metric to d2
ci and keeping all the merged distances and their multiplicities.
If a trellis code is regular (or geometrically uniform), the averaging over c in (5.69) is
not necessary and any code sequence may be taken as reference sequence. In the case of
geometrically uniform codes, the Voronoi regions of all the code sequences are congruent,
and hence the error probability is the same for all code sequences. For the more general case
of regular codes, the distances between sequences only depend on their label diﬀerences,
and the distance spectrum is identical for all sequences. However, many trellis codes are
not regular, and we wish to have available a more general method.
We know that both the encoder and the optimal decoder are essentially identical FSMs
denoted by M. Since we are interested in the set of squared Euclidean distances which can
occur if those two FSM’s follow diﬀerent paths, we ﬁnd it helpful to consider a new FSM
M = M ⊗M with states (p, q); p, q ∈M and outputs δ((p, q) →(p1, q1)) = d2
(p,q),(p1,q1),

216
CHAPTER 5. TRELLIS AND TREE DECODING
2
3
4
5
6
7
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
ν = 4
ν = 8
ν = 6
Figure 5.22: Bounds and bit error probabilities for three 8-PSLK codes.
where d2
(p,q),(p1,q1) is the distance increment accrued when the correct path c advances
from state p to p1 and the incorrect path ei advances from q to q1.
If the transition
(p, q) →(p1, q1) is not possible, δ((p, q) →(p1, q1)) = ∞, by deﬁnition.
Let us pause here for a moment and discuss what motivates the construction of M.
It allows us to keep track of the evolution of both the correct path c and the error path
ei. Consider, for example, the case where the correct path progresses through the states
p →p1 →p2 · · · →pL−1 →pL and the error path leads through the states q →q1 →
q2 · · · →qL−1 →qL, where q = p and qL = pL. This is illustrated in Figure 5.23 for L = 5.
c
ei
p0
p1
p2
p3
p4
p5
q1
q2
q3
q4
diverging
path pairs
parallel path pairs
merging
path pairs
Figure 5.23: The correct path and an error path of length L = 5.

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
217
Multiplying the distance increments δ((pi−1, qi−1) →(pi, qi)) of M, we obtain
L

i=1
Xδ((pi−1,qi−1)→(pi,qi)) = X
L
i=1 d2
(pi−1,qi−1),(pi,qi) = Xd2
ci,
(5.72)
that is, the total distance between c and ei appears in the exponent of our dummy base X.
Note that M is a random FSM, that is, we are not assigning any inputs to the tran-
sitions.
Each transition is taken with equal probability 1/2k, or if diﬀerent with the
probability of the transition p →p1 of the correct path c.
Now deﬁne the output transition matrix of our FSM M having the entries
B = {b(pq)(p1q1)} =
 1
2k Xδ((p,q)→(p1,q1))

=
 1
2k Xd2
(p,q),(p1,q1)

,
(5.73)
and it is quite straightforward to see that the ((p, p), (q, q))-entry of the Lth power of B is
a polynomial in X whose exponents are all the distances between path pairs originating
in (p, p) and terminating in (q, q), and whose coeﬃcients are the average multiplicities of
these distances. The weighting factor 1/2k weighs each transition with the probability that
c moves from p →p1. This achieves the weighting of the distances with the probability of
c. We now can use matrix multiplication to keep track of the distances between paths.
The size of B quickly becomes unmanageable, since it grows with the square of the
number of states in the code FSM. As an example, the matrix B for the 4-state 8-PSK
trellis code with h(0) = 5, h(1) = 4, h(2) = 2 has 16 × 16 entries4 and is given by B =
1
4
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
00
01
02
03
10
11
12
13
20
21
22
23
30
31
32
33
00
1
X2
X4
X2
X2
1
X2
X4
X4
X2
1
X2
X2
X4
X2
1
01
X3.4 X0.6 X0.6 X3.4 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4
02
X2
1
X2
X4
1
X2
X4
X2
X2
X4
X2
1
X4
X2
1
X2
03
X0.6 X3.4 X3.4 X0.6 X3.4 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X3.4
10
X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X3.4 X0.6 X0.6 X3.4
11
1
X2
X4
X2
X2
1
X2
X4
X4
X2
1
X2
X2
X4
X2
1
12
X3.4 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X3.4 X0.6 X3.4 X3.4 X0.6
13
X2
1
X2
X4
1
X2
X4
X2
X2
X4
X2
1
X4
X2
1
X2
20
X2
1
X2
X4
1
X2
X4
X2
X2
X4
X2
1
X4
X2
1
X2
21
X3.4 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X3.4 X0.6 X3.4 X3.4 X0.6
22
1
X2
X4
X2
X2
1
X2
X4
X4
X2
1
X2
X2
X4
X2
1
23
X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X3.4 X0.6 X0.6 X3.4
30
X0.6 X3.4 X3.4 X0.6 X3.4 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X0.6 X3.4 X3.4
31
X2
1
X2
X4
1
X2
X4
X2
X2
X4
X2
1
X4
X4
X2
X2
32
X3.4 X0.6 X0.6 X3.4 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4
33
1
X2
X4
X2
X2
1
X2
X4
X4
X2
1
X2
X2
X4
X2
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
4The exponents have been rounded to 1 decimal place for space reasons.

218
CHAPTER 5. TRELLIS AND TREE DECODING
We wish to partition our matrix B into a diverging, a parallel, and a merging section,
corresponding to these diﬀerent stages of an error event. For the 4-state code from above,
this partition is given by
D= 1
4
⎛
⎜
⎜
⎝
01
02
03
10
12
13
20
21
23
30
31
32
00
X2
X4
X2
X2
X2
X4
X4
X2
X2
X2
X4
X2
11
X2
X4
X2
X2
X2
X4
X4
X2
X2
X2
X4
X2
22
X2
X4
X2
X2
X2
X4
X4
X2
X2
X2
X4
X2
33
X2
X4
X2
X2
X2
X4
X4
X2
X2
X2
X4
X2
⎞
⎟
⎟
⎠,
(5.74)
P= 1
4
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
01
02
03
10
12
13
20
21
23
30
31
32
01
X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X0.6 X0.6 X0.6 X3.4
02
1
X2
X4
1
X4
X2
X2
X4
1
X4
X2
1
03
X3.4 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4
10
X3.4 X0.6 X0.6 X0.6 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6
12
X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X3.4
13
1
X2
X4
1
X4
X2
X2
X2
1
X4
X2
1
20
1
X2
X4
1
X4
X2
X2
X4
1
X4
X2
1
21
X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X3.4
23
X3.4 X0.6 X0.6 X0.6 X3.4 X0.6 X0.6 X0.6 X3.4 X3.4 X0.6 X0.6
30
X3.4 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4 X0.6 X3.4 X0.6 X0.6 X3.4
31
1
X2
X4
1
X4
X2
X2
X4
1
X4
X4
X2
32
X0.6 X0.6 X3.4 X3.4 X0.6 X0.6 X0.6 X3.4 X0.6 X0.6 X0.6 X3.4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.75)
M= 1
4
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
00
11
22
33
01
X3.4
X3.4
X3.4
X3.4
02
X2
X2
X2
X2
03
X0.6
X3.4
X0.6
X3.4
10
X3.4
X3.4
X3.4
X3.4
12
X3.4
X0.6
X3.4
X0.6
13
X2
X2
X2
X2
20
X2
X2
X2
X2
21
X3.4
X0.6
X3.4
X0.6
23
X3.4
X3.4
X3.4
X3.4
30
X0.6
X3.4
X0.6
X3.4
31
X2
X2
X2
X2
32
X3.4
X3.4
X3.4
X3.4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.76)

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
219
where the diverging matrix D is N×(N 2−N), the parallel matrix P is (N 2−N)×(N 2−N),
and the merging matrix M is (N 2 −N) × N and N is the number of states in M.
From Figure 5.23 we see that each error event starts when the correct path and the
error path diverge and terminates when the two paths merge again. In between, ei and c
are never in the same state at the same time unit, and we refer to this middle part of the
error event as the parallel section.
We may now describe all distances of error events of length exactly L by
GL = DPL−2M,
L ≥2.
(5.77)
The ((p, p), (q, q)) entry of GL is essentially a table of all weighted distances between
length-L path pairs which originate from a given state p and merge in a given state q, L
time units later. (5.77) can now be used to ﬁnd the distance spectrum of a code up to
any desired length. While (5.77) is a convenient way of describing the distance spectrum,
its calculation is best performed using an adaptation of one of the decoding algorithms
described earlier in this chapter.
The union bounds (5.70) and (5.71) are very tight for error probabilities smaller than
about 10−2 as evidenced in Figure 5.22. Its disadvantage is that we need to evaluate the
distance spectrum and also need some way of terminating the sum since (5.70) and (5.71)
have no closed form expression.
Not so for our next bound, which is traditionally referred to as the transfer-function
bound since it can also be derived via state transfer functions from a systems theory point
of view. The transfer function bound using the pair-state trellis was ﬁrst applied to TCM
in [11]. The ﬁrst step we take is loosening (5.70) by applying the inequality (see, e.g., [60,
Page 83])
Q

d2
i
REb
2N0

≤exp

−d2
i
REb
4N0

.
(5.78)
Now, using (5.77), which is essentially a table of the distances between path pairs of length
L, it is easy to see that
Pe
≤

i
(d2
i ∈D)
Ad2
i exp

−

d2
i
REb
2N0

(5.79)
=
1
N
∞

L=2
1T DPL−2M 1

X=exp −REb
4N0
,
(5.80)
where 1 = (1, 1, . . . , 1)T is an N ×1 vector of all 1’s and acts by summing all merger states
while
1
N 1T averages uniformly over all diverging states.

220
CHAPTER 5. TRELLIS AND TREE DECODING
Pulling the sum into the matrix multiplication we obtain
Pe
≤
1
N 1T D
∞

L=2
PL−2M 1

X=exp −REb
4N0
(5.81)
=
1
N 1T D (I −P)−1 M 1

X=exp −REb
4N0
.
(5.82)
The above equation involves the inversion5 of an (N2 −N) × (N2 −N) matrix, which
might be a sizeable task. In going from (5.81) to (5.82) we assumed that the inﬁnite series
in (5.81) converges. Using matrix theory, it can be shown that this series converges, if the
largest eigenvalue of P, λmax < 1 [48], and hence (I −P)−1 exists. This is always the case
for non-catastrophic codes and for Eb/N0 suﬃciently large.
Note also that it is necessary to invert this matrix symbolically in order to obtain a
closed form expression for the transfer function bound as a function of the signal-to-noise
ratio Eb/N0. In practice, one might want to apply an iterative matrix inversion technique
[8] to achieve a computationally fast procedure.
Since using (5.77) to search for all the distances up to a certain length Lmax is often
easier, it is more eﬃcient to use the tighter union bound for the distances up to a certain
path pair length Lmax and use the transfer function only to bound the tail, i.e., we break
(5.77) into two components, and obtain
Pe
≤
Lmax

L=1

c(L)
k
p(c(L)
k )

e(L)
i
|c(L)
k
Q

d2
ci
REb
2N0

+
1
N 1T D
∞

L=Lmax+1
PL−2M 1

X=exp −REb
4N0
,
(5.83)
where c(L) and e(L)
i
are a correct path and incorrect path of length exactly L. The second
term, the tail of the transfer function bound, can be overbounded by
 = 1
N 1T D
∞

L=Lmax+1
PL−2M 1

X=exp −REb
4N0
≤λLmax−1
max
1 −λmax
,
(5.84)
where λmax is the largest eigenvalue6 of P.
5The matrix I−P is sparse in most cases of interest and can therefore be inverted eﬃciently numerically
[1].
6The Perron–Frobenious theorem from linear algebra tells us that a non-negative matrix has a non-
negative largest eigenvalue [48].
A matrix is called non-negative, if every entry is a non-negative real
number, which is the case for P.

5.9. ERROR ANALYSIS AND DISTANCE SPECTRUM
221
The computation of the eigenvalues of a matrix and its inversion are closely related,
and not much seems to have been gained. The complexity of inverting I −P and ﬁnding
λmax are comparable. However, there are many eﬃcient ways of bounding λmax. And we
do not even need a very tight bound since we are only interested in the order of magnitude
of .
One straightforward way of bounding λmax we will use now is obtained by applying
Gerschgorin’s circle theorem (see Appendix 5.A). Doing this, we ﬁnd that
λmax ≤max
i

j
pij = g,
(5.85)
i.e., λmax is smaller than the the maximum row sum of P. A quick look at (5.75) shows
that g, which in general is a polynomial in X, contains the constant term 1 for our 4-state
code. This means that g ≥1 for all real X and (5.85) is not useful as a bound in (5.84).
Since working with P did not lead to a satisfactory bound, we go to P2. From basic
linear algebra we know that the maximum eigenvalue of P2 is λ2
max. Let us then apply
the Gerschgorin bound to P2, and we obtain
λmax ≤

max
i

j
p(2)
ij =

g(2).
(5.86)
Figure 5.24 shows plots of

g(2)
Lmax−1

1 −

g(2)

versus the signal-to-noise ratio Eb/N0 for various maximum lengths Lmax.
From such
plots one can quickly determine the maximum length Lmax in the trellis which needs to
be searched for the distances d2
ci in order to achieve a prescribed accuracy.
There are many approaches to minimize the state space of M (see for example [63, 62],
and [27, Section 5.5]), most of them based on ﬁnite-state machine reduction techniques.
The approach chosen in this section does not reﬂect the actual historical development
of these results. Methods to overcome the non-linearity of Ungerb¨ock trellis codes and
reducing the size of the computation FSM M from N2 states down to N were ﬁrst pre-
sented by Zehavi and Wolf [61], and independently by Rouanne and Costello [43] with
generalizations in [44] and appeared under the terminology of “quasi-regularity.” Later
a slightly diﬀerent, but equivalent approach was given by Biglieri and McLane [12], who
called the property necessary for reduction “uniformity.”

222
CHAPTER 5. TRELLIS AND TREE DECODING
4
5
6
7
8
9
10
11
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
Tail Probability 
Eb/N0[dB]
Lmax = 5
Lmax = 10
Lmax =25
Lmax =50
Lmax
=100
Figure 5.24: Plots of the bound on  (5.84) using the exact value of λmax (solid curves)
as well as the Gerschgorin bound (dotted curves) for the 4-state 8-PSK trellis code for
Lmax = 5, 10, 25, 50, 100, and 500.
5.10
Random Coding Analysis of Optimal Decoding
The calculation of the error probability is very diﬃcult for any code of decent size, and,
there is, in general, little hope of calculating an exact error probability for these codes.
Consequently, ﬁnding bounds on their performance becomes the next best thing one can
hope for. But, as we have seen, even ﬁnding bounds for speciﬁc codes is rather cumber-
some. It requires the distance spectrum of the code as discussed earlier.
If we average the performance, i.e., the block, bit, or event error probabilities over all
possible codes, assuming some distribution on the selection of codes, we ﬁnd to our great
astonishment that this average can be calculated relatively easily. This is the essence of
Shannon’s random coding bound idea [45], which we apply to trellis codes in this section.
Knowing the average of the performance of all codes, we can rest assured that there
must exist at least one code whose individual performance is better than that calculated

5.10. RANDOM CODING ANALYSIS OF OPTIMAL DECODING
223
average. This leads to the famous existence argument of good codes–and, immediately to
the dilemma that while knowing of the existence of good codes, the argument tells us little
on how to ﬁnd these good codes. Nonetheless, random coding bounds are of great interest;
they tell us about the achievable performance of good codes in general, and, together with
performance lower bounds, accurately describe the limits of performance of large codes.
They also give us a way to evaluate the inherent performance potential of diﬀerent signal
constellations.
In this section we commence with random coding bounds on the ﬁrst event error
probability of trellis codes, which we later use to derive bounds on the bit error probability.
Let us then assume that we use an ML-decoder7 which selects the hypothesized transmitted
sequence ˆx according to the highest probability metric p(y|x), where y is the received
sequence x + n, x(c) is the transmitted sequence and n is a noise sequence (see Section
2.5). This decoder makes an error if it decodes a sequence x, given that the transmitted
sequence was x. For an ML decoder this happens if and only if p(y|x) ≤p(y|x).
Let c and e be paths through the trellis. Remember that c and e describe only the
paths through the trellis (state sequences), and not the signals attached to these paths.
The signal assignment is the actual encoding function, and we will average over all these
functions. As in the case of speciﬁc trellis codes, let c be the correct path, that is, the
one taken by the encoder, and let e denote an incorrect path which diverges from c at
node j. Further, let E denote the set of all incorrect paths that diverge from c at node
j. A necessary condition for an error event to start at node j is that the incorrect path e
accumulates a higher conditional probability than the correct path c over their unmerged
segments of the trellis.
If we are using a convolutional code to generate the code trellis, the sets E for diﬀerent
correct paths are congruent. They contain the same number of paths of a certain length.
In a particular trellis code then, x is the sequence of signals assigned along the correct
path c and x is the sequence of signals assigned to e.
Using this notation, we rewrite (5.66) as
Pe ≤

c
p(c)

y
p(y|x)I

e∈E
e(p(y|x) ≥p(y|x))

dy,
(5.87)
where I(B) is an indicator function such that I(B) = 0 if B = ∅, the empty set,
I(B) = 1 if B = ∅, and e(p(y|x) ≥p(y|x)) is a path e for which p(y|x) ≥p(y|x).
I

e∈E e(p(y|x) ≥p(y|x))

simply says whether or not there exists an error path with
a posteriori probability larger than the correct path.
We now specify the structure of our trellis code to be the one shown in Figure 5.25.
The binary input vector at time r, ur, has k components and enters a feed-forward shift
7Note that the assumption of an ML decoder is nowhere necessary to prove the capacity theorem in
general; only a “good” decoder is needed, as shown by Shannon [45].

224
CHAPTER 5. TRELLIS AND TREE DECODING
register at time unit r. The shift register stores the ν −1 most recent binary input vectors.
The mapper function assigns a symbol xr as a function of the input and the state, i.e.,
xr = τ(ur, sr) = τ(ur, . . . , ur−ν−1).
ur
xr
ur−1
ur−2
ur−ν−2
ur−ν−1
Mapping
Function
Figure 5.25: Trellis encoder structure used for the random coding bound arguments.
The particular realization of the trellis code as in Figure 5.25 has been chosen merely
for convenience. The bounds which follow can be calculated for other structures as well,
but the formalism can become quite unpleasant.
The symbol xr is chosen from a signal set A of size |A| = A. The channel considered
here is a memoryless channel used without feedback, i.e., the output symbol y is described
by the conditional probability distribution p(y|x), and the conditional probability of a
symbol vector p(y|x) is then the product of the individual conditional symbol probabilities.
We will need this property later in the proof to obtain an exponential bound on the error
probability.
In order for an incorrect path e, which diverges from c at node j, to merge with the
correct path at node j + L, the last ν −1 entries in the information sequence u
j, . . . , u
j+L
associated with e must equal the last ν−1 entries in the information sequences uj, . . . , uj+L
associated with c. This has to be the case since in order for the two paths e and c to merge
at node j + L, their associated encoder states must be identical. Because an information
vector uj entering the encoder can aﬀect the output for ν time units, this is also the
maximum time it takes to force the encoder into any given state from any arbitrary starting
state. Because the remaining information bits are arbitrary, we have Np ≤(2k −1)2k(L−ν)
incorrect paths e of length L. Note that the choice of the information bits at node j is
restricted because we stipulated that the incorrect path diverges at node j, which rules
out the one path that continues to the correct state at node j + 1. This accounts for the
factor 2k −1 in the expression for Np above.
We now proceed to express (5.87) as the sum over sequences of length L, and we rewrite

5.10. RANDOM CODING ANALYSIS OF OPTIMAL DECODING
225
it as
Pe ≤
∞

L=ν

c(L)∈C(L)
p(c(L))

y
p(y|x)I
⎛
⎝

e(L)∈E(L)
e(L)(p(y|x) ≤p(y|x))
⎞
⎠dy,
(5.88)
where C(L) is the set of all correct paths c(L) of length L starting at node j and E(L) is the
set of all incorrect paths e(L) of length L unmerged with c(L) from node j to node j + L.
Note that 
L E(L) = E.
Now we observe that if an error occurs at node j, then for at least one path e
p(y|x) ≥p(y|x).
(5.89)
Therefore, for any real parameter γ ≥0

e(L)∈E(L)
p(y|x)
p(y|x)
γ
≥1.
(5.90)
We can raise both sides of (5.90) to the power of some non-negative parameter ρ ≥0 and
preserve the inequality, i.e.,
⎛
⎝

e(L)∈E(L)
p(y|x)
p(y|x)
γ
⎞
⎠
ρ
≥1.
(5.91)
This Chernoﬀbounding technique was introduced by Gallager [26] to derive random coding
theorems for block codes, and then applied to convolutional codes by Viterbi [53].
We now use (5.91) to overbound the indicator function I(·) by an exponential, i.e.,
I
⎛
⎝

e(L)∈E(L)
e(L)(p(y|x) ≤p(y|x))
⎞
⎠≤
⎛
⎝

e(L)∈E(L)
p(y|x)
p(y|x)
γ
⎞
⎠
ρ
.
(5.92)
Using (5.92) in (5.88), we obtain
Pe ≤
∞

L=ν

c(L)
p(c(L))

y
p(y|x)
⎛
⎝

e(L)∈E(L)
p(y|x)
p(y|x)
γ
⎞
⎠
ρ
dy.
(5.93)
It is worth pausing here for a moment to reﬂect on what we are doing. If we set the
parameter ρ = 1, the sum over e(L) will pull out all the way and we have in eﬀect the
union bound as in (5.67). We know, however, that the union bound is rather loose for
low signal-to-noise ratios, or, alternatively, for high rates. Hence the parameter ρ, which
allows us to tighten the bound in these areas.

226
CHAPTER 5. TRELLIS AND TREE DECODING
Since we are free to choose γ, we select γ = 1/(1 + ρ), which simpliﬁes (5.93) to
Pe ≤
∞

L=ν

c(L)
p(c(L))

y
p(y|x)1/(1+ρ)
⎛
⎝

e(L)∈E(L)
p(y|x)1/(1+ρ)
⎞
⎠
ρ
dy.
(5.94)
Pe is the event error probability of a particular code since it depends on the signal
sequences x and x of the code.
The aim of this section is to obtain a bound on an
ensemble average of trellis codes, and we therefore average Pe over all the codes in the
ensemble, i.e.,
Avg

Pe

≤
Avg
⎧
⎨
⎩
∞

L=ν

c(L)
p(c(L))

y
p(y|x)1/(1+ρ)
⎛
⎝

e(L)∈E(L)
p(y|x)1/(1+ρ)
⎞
⎠
ρ
dy
⎫
⎬
⎭,
(5.95)
where Avg{·} denotes this ensemble average.
Using the linearity of the averaging operator and noting that there are exactly N = 2kL
equiprobable paths in C(L) of length L, because at each time unit there are 2k possible
choices to continue the correct path, we obtain
Avg

Pe

≤
∞

L=ν
1
2kL Avg
⎧
⎨
⎩

c(L)

y
p(y|x)1/(1+ρ)
⎛
⎝

e(L)∈E(L)
p(y|x)1/(1+ρ)
⎞
⎠
ρ
dy
⎫
⎬
⎭.
(5.96)
To continue let X = {x1, x2, . . . , xN} be a possible assignment of signal sequences of
length L associated with the paths c(L), i.e., X is a particular code and let qLN({X}) =
qLN(x1, . . . , xN) be the probability of choosing this code. Note that E(L) ⊂C(L) since each
incorrect path is also a possible correct path. Averaging over all codes means averaging
over all signal sequences in these codes, i.e., over all assignments X. Doing this we obtain
Avg

Pe

≤
∞

L=ν
1
2kL

c(L)

y

x1
· · ·

xN
qLN(X)p(y|x)1/(1+ρ)
×
⎛
⎝

e(L)∈E(L)
p(y|x)1/(1+ρ)
⎞
⎠
ρ
dy,
(5.97)
where x is the signal sequence on c(L) and x is the one on e(L).
We can rewrite qLN(X) = qL(N−1)(X |x), where X  = X\{x} is the set of signal
sequences without x and qL(N−1)(X|x) is the probability of X\{x}, conditioned on x.

5.10. RANDOM CODING ANALYSIS OF OPTIMAL DECODING
227
Restricting ρ to the unit interval, i.e., 0 ≤ρ ≤1, allows us to apply Jensen’s inequality,
 piαρ
i ≤( piαi)ρ, to move the sum inside the exponential, and we obtain
Avg

Pe

≤
∞

L=ν
1
2kL

c(L)

y

x
q(x)p(y|x)1/(1+ρ)
×
⎛
⎜
⎜
⎜
⎜
⎜
⎝

e(L)∈E(L)

x1
· · ·

xN



(xi=x)
qL(N−1)(X |x)p(y|x)1/(1+ρ)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
ρ
dy.
(5.98)
But the inner term in (5.98) depends only on x and we can reduce (5.98) by summing
over all other signal assignments, i.e.,
Avg

Pe

≤
∞

L=ν
1
2kL

c(L)

y

x
q(x)p(y|x)1/(1+ρ)
×
⎛
⎝

e(L)∈E(L)

x
q(x|x)p(y|x)1/(1+ρ)
⎞
⎠
ρ
dy.
(5.99)
We observe that the last equation depends only on one correct and one incorrect signal
sequence. We now further assume that the signals on c(L) and e(L) are assigned inde-
pendently, i.e., q(x|x) = q(x), and that each signal is chosen independently, making
q(x) = L−1
i=j q(xj) a product. In order to make this possible we must assume that the
trellis codes are time-varying in nature, for otherwise each symbol would also depend on
the choices of the ν most recent symbols. Note also that the signal assignments x and
x can be made independently since e(L) is in a diﬀerent state than c(L) over the entire
length of the error event.
This generalization to time-varying codes is quite serious, since almost all practical
codes are time-invariant. It allows us, however, to obtain a much simpler version of the
above bound, starting with
Avg

Pe

≤
∞

L=ν
1
2kL

c(L)

y

x
L

i=1
q(xi)p(yi|xi)1/(1+ρ)
⎛
⎝

e(L)∈E(L)

x
L

i=1
q(x
i)p(yi|x
i)1/(1+ρ)
⎞
⎠
ρ
dy,
(5.100)
where we have assumed that p(y|x) = L
i=1 p(yi|xi), that is, the channel is memoryless.
In a ﬁnal step we realize that the products are now independent of the index j and the

228
CHAPTER 5. TRELLIS AND TREE DECODING
correct and incorrect path c(L) and e(L). We obtain
Avg

Pe

≤
∞

L=ν

y

x
q(x)p(y|x)1/(1+ρ)
L
×
⎛
⎝Np

x′
q(x)p(y|x)1/(1+ρ)
L⎞
⎠
ρ
dy
=
∞

L=ν
Nρ
p

y
⎛
⎝

x
q(x)p(y|x)1/(1+ρ)
1+ρ⎞
⎠
L
dy,
(5.101)
≤
∞

L=ν
(2k −1)ρ2ρk(L−ν)
⎛
⎝

y

x
q(x)p(y|x)1/(1+ρ)
1+ρ
dy
⎞
⎠
L
,
(5.102)
where we have broken up the integral over y into individual integrals over y in the last
step.
Let us now deﬁne the error exponent
E0(ρ, q) ≡−log2

y

x
q(x)p(y|x)1/(1+ρ)
1+ρ
dy,
(5.103)
where q = (q1, . . . , qA) is the probability with which x is chosen from a signal set A of size
A. This allows us to write the error bound in the standard form:
Avg

Pe

≤
(2k −1)ρ2−νE0(ρ,q)
∞

L=0
2ρkL2−LE0(ρ,q)
(5.104)
=
(2k −1)ρ2−νE0(ρ,q)
1 −2−(E0(ρ,q)−ρk) ,
ρk < E0(ρ, q),
(5.105)
where we have used the summation formula for a geometric series and the condition
ρk < E0(ρ, q) assures convergence.
Since k is the number of information bits transmitted in one channel symbol x, we
may call it the information rate in bits per channel use and denote it by the symbol R.
This gives us our ﬁnal bound on the event error probability:
Avg

Pe

= (2R −1)ρ2−νE0(ρ,q)
1 −2−(E0(ρ,q)−ρR) ,
ρ < E0(ρ, q)/R.
(5.106)
Using (5.104), we may easily obtain a bound on the average number of bit errors Pb.

5.10. RANDOM CODING ANALYSIS OF OPTIMAL DECODING
229
Since on an error path of length L there can be at most L −ν + 1 bit errors, we obtain
Avg

Pb

≤
(2R −1)ρ2−νE0(ρ,q)
∞

L=0
(L + 1)2ρRL2−LE0(ρ,q)
(5.107)
=
(2R −1)ρ2−νE0(ρ,q)

1 −2−(E0(ρ,q)−ρk)2 ,
ρ < E0(ρ, q)/R,
(5.108)
For large constraint lengths ν, only the exponent E0(ρ, q) will matter. E0(ρ, q) is a
function of ρ and we wish to explore some of its properties before interpreting our bound.
Clearly E0(ρ, q) ≥0 for ρ ≥0, since 
x q(x)p(y|x)1/(1+ρ) ≤1 for ρ ≥0, that is, our
exponent is positive, making the bound non-trivial. Furthermore, we have
Lemma 5.3 E0(ρ, q) is a monotonically increasing function of ρ.
Proof: We will apply the inequality8

i
piαr
i
 1
r
≤

i
piαs
i
 1
s
,
0 < r < s; αi ≥0,
(5.109)
which holds with equality if for some constant c: piαi = cpi for all i. Using (5.109) in the
expression for the error exponent (5.103), we obtain
E0(ρ, q)
=
−log2

y

x
q(x)p(y|x)1/(1+ρ)
1+ρ
dy
≤
−log2

y

x
q(x)p(y|x)1/(1+ρ1)
1+ρ1
dy
=
E0(ρ1, q),
(5.110)
for ρ1 > ρ > −1. Equality holds in (5.110) if and only if p(y|x) = c for all x ∈A, but
that implies that our channel has capacity C = 0. Therefore E0(ρ, q) is monotonically
increasing for all interesting cases.
Q.E.D.
Lemma 5.3 tells us that in order to obtain the best bound, we want to choose ρ as
large as possible, given the constraint in (5.106), i.e., we choose
8This inequality is easily derived from H¨older’s inequality
i
βiγi ≤
i
β
1
μ
i
μ
i
γ
1
1−μ
i
1−μ
,
0 < μ < 1; βi, γi ≥0,
by letting βi = pμ
i αr
i , γi = p1−μ
i
and μ = r/s.

230
CHAPTER 5. TRELLIS AND TREE DECODING
ρ = E0(ρ, q)
R
(1 −),
(5.111)
where  is the usual inﬁnitesimally small number.
Our next lemma says that, unfortunately, the larger ρ and hence the error exponent,
the smaller the associated rate.
Lemma 5.4 The function R(ρ) ≡E0(ρ, q)/ρ is a monotonically decreasing function of ρ.
Proof: We will use the fact that E0(ρ, q) is a ∩-convex function in ρ. This fact is well-
known and proven in [53], Appendix 3A.3. We therefore know that
∂2
∂ρ2 E0(ρ, q) ≤0, i.e.,
the function
∂
∂ρE0(ρ, q) is strictly monotonically decreasing. Let us then consider
∂
∂ρ
E0(ρ, q)
ρ
= 1
ρ2

ρ∂E0(ρ, q)
∂ρ
−E0(ρ, q)

.
(5.112)
The ﬁrst term in parentheses, ρ ∂E0(ρ,q)
∂ρ
, lies on a straight line through the origin with
slope ∂E0(ρ,q)
∂ρ
. Furthermore, E0(ρ, q), as a function of ρ also passes through the origin,
and its slope at the origin is larger than ∂E0(ρ,q)
∂ρ
for all ρ > 0. By virtue of the ∪-convexity
of E0(ρ, q), the derivative (5.112) is negative, proving the lemma.
Q.E.D.
We are therefore faced with the situation that the larger the rate R, the smaller the
maximizing ρ and, hence, the smaller the error exponent E0(ρ, q) and E0(ρ, q) →0 as
ρ →0, which can easily be seen from (5.103). Let us ﬁnd out at what limiting rate R the
error exponent approaches its value of zero, i.e., let us calculate
lim
ρ→0 R(ρ) = lim
ρ→0
E0(ρ, q)
ρ
.
(5.113)
Since this limit is of the form 0
0, we can use the rule of de l’Hˆopital and obtain the limit
lim
ρ→0 R(ρ) = ∂E0(ρ, q)
∂ρ

ρ=0
.
(5.114)
This derivative is easily evaluated and to our great surprise and satisfaction we obtain
∂E0(ρ, q)
∂ρ

ρ=0
=

y

x
q(x)p(y|x) log2

p(y|x)

x′ q(x)p(y|x)

dy = C(q),
(5.115)
the Shannon channel capacity, using the signal set A and the input probability distribution
q! (See also [18, 53]). Maximizing over the input probability distribution q will then
achieve the channel capacity C.

5.10. RANDOM CODING ANALYSIS OF OPTIMAL DECODING
231
We have now proved the important fact that random trellis codes can achieve the
Shannon capacity, and, as we will see, with a very favorable error exponent (5.103). The
bounds thus derived are an important conﬁrmation of our decision to use convolutional
codes to generate the code trellis, since they tell us that these codes can, in principle at
least, achieve channel capacity.
Let us now concern ourselves with additive white Gaussian noise (AWGN) channels,
since they are arguably the most important class of memoryless channels. For an AWGN-
channel with complex input and output symbols x and y (from (2.14)), we have
p(y|x) =
1
2πN0
exp

−|y −x|2
2N0

(5.116)
Furthermore, for ρ = 1, E0(ρ, q) can be simpliﬁed considerably by evaluating the
integral over y, i.e.,
E0(1, q)
=
−log2

y

x
q(x)
1
√2πN0
exp

−|y −x|2
4N0
2
=
−log2

x

x′
q(x)q(x)

y
1
2πN0
exp

−|y −x|2
4N0
−|y −x|2
4N0

=
−log2

x

x′
q(x)q(x) exp

−|x −x|2
4N0

= R0(q),
(5.117)
where R0(q) = E0(1, q) is known in the literature as the cutoﬀrate of the channel. It
used to be believed [59, 37] that R0 represents a “practical” limit for coded systems.
However, with the appearance of turbo codes, the signiﬁcance of R0 as a measure of
practical importance has vanished.
On AWGN-channels the uniform input symbol distribution q = 1/A, . . . , 1/A is par-
ticularly popular, and we deﬁne E0(ρ, uniform distribution) = E0(ρ) and the cutoﬀrate
R0(uniform distribution) = R0, i.e., we simply omit the probability distribution vector q.
Figure 5.26 shows E0(ρ) for an 8-PSK constellation on an AWGN-channel with signal-
to-noise ratio Es/N0 = 10 dB. The error exponent function has a constant value of R0 up
to the rate R = R0 and then rapidly drops to zero as R →C. The ﬁgure also shows the
error exponent for general codes (dashed curve), that is, the exponent in
PB < 2−NE,
E(R) = max
q
max
0≤ρ≤1 [E0(ρ, q) −ρR] ,
(5.118)
which bounds the error probability of the ensemble of all (linear) codes of length N [53,
Section 3.1]. A very thorough discussion about error exponents can be found in either [53]
or [26].

232
CHAPTER 5. TRELLIS AND TREE DECODING
E0(ρ)
R0
(2.438 bits)
ρ > 1
ρ = 1
ρ < 1
ρ = 0
C = 2.677 bits
R0
Rc
Rate R
Figure 5.26: Error exponent as a function of the rate R for an 8-PSK constellation on an
AWGN-channel at a signal-to-noise ratio of Es/N0 = 10 dB.
We discern the interesting fact that the error exponent for trellis codes is signiﬁcantly
larger than that for block codes, especially at high rates of transmission. Since the general
code error exponent multiplies the block codelength, this is taken as an indication that
the constraint length ν needed to obtain a prescribed error probability Pb is much smaller
than the blocklength of an equivalent block code. This may account for the popularity
of trellis codes and their often superior behavior on high-noise channels, since, while the
majority of academic publications are devoted to block codes, trellis (and in particular
convolutional) codes are more widespread in applications.
While trellis codes can theoretically approach capacity, this has not turned out to be
a practical avenue, since the decoding complexity of the corresponding decoders becomes
prohibitively large. The achievement of approaching capacity with manageable decoder
complexity had to await the invention of turbo codes, which, incidentally, do not rely on
maximum-likelihood decoding.
5.11
Random Coding Analysis of Sequential Decoding
In the previous section we presented random coding performance bounds for trellis codes
for maximum likelihood decoding. Since sequential decoding is not a maximum likelihood
decoding method, the results in Section 5.10 do not apply.
The error analysis of sequential decoding is very diﬃcult, and, again, we ﬁnd it easier
to generate results for an ensemble of codes via random coding arguments. The evaluation

5.11. RANDOM CODING ANALYSIS OF SEQUENTIAL DECODING
233
of the error probability is not the main problem here, since, if properly dimensioned, both
the stack and the Fano algorithm will almost always ﬁnd the same error path as the
maximum likelihood detector, and thus one might argue that we could simply let the code
constraint length become large and thus achieve the bounds from Section 5.10. In this
section then, we show that the problem of path loss will become dominant and determine
the code’s performance limits. In doing so, we will encounter R0, the computational cutoﬀ
rate, at the maximum communications rate above which the decoder fails due to buﬀer
overﬂow (or ultimately, path loss).
Xj
Xj+2
Xj+1
Figure 5.27: Incorrect subsets explored by a sequential decoder. The solid path is the
correct one.
The diﬀerence with sequential decoding is, in contrast to ML-decoding, that its com-
putational load is variable. And it is this computational load which can cause problems as
we will see. Figure 5.27 shows an example of the search procedure of sequential decoding.
The algorithm explores at each node an entire set of incorrect partial paths before ﬁnally
continuing. This set at each node includes all the incorrect paths explored by the possibly
multiple visits to that node as, for example, in the Fano algorithm. The sets X ′
j denote
the sets of incorrect signal sequences ˜x′ diverging at node j, which are searched by the
algorithm. Further, denote the number of signal sequences in X ′
j by Cj. Note that Cj is
also the number of computations that need to be performed at node j, since each new
path requires one additional metric calculation. This is the case because the algorithm
explores two extensions at each step for a binary code, both resulting in distinct extension
paths (Figure 5.27).
The problem becomes quite evident now, the number of computations at each node is
variable, and it is this distribution of the computations which we want to examine. Again,

234
CHAPTER 5. TRELLIS AND TREE DECODING
let ˜x be the partial correct path through the trellis and ˜x
j be a partial incorrect path
which diverges from ˜x at node j. Furthermore, let Ln(˜x) = L(˜x, y) be the metric of
the incorrect path at node n, and let Lm(˜x) be the metric of the correct path at node
m. A path is searched further if and only if it is at the top of the stack, and hence, if
Ln(˜x) < Lm(˜x), the incorrect path is not searched further until the metric of ˜x falls
below Ln(˜x). If
min
m≥j Lm(˜x) = λj > Ln(˜x),
(5.119)
the incorrect path ˜x is never searched beyond node n.
We may now overbound the probability that the number of computations at node j
exceeds a given value Nc by
Pr(Cj ≥Nc) ≤

x
p(x)

y
p(y|x)B
e(p(y|x) ≥λj)
 ≥Nc

dy,
(5.120)
where e(p(y|x) ≥λj) is an error path in Xj whose metric exceeds λj and |  | is the
number of such error paths. B() is a boolean function which equals 1 if the expression is
true and 0 otherwise. The function B() in (5.120) then simply equals 1 if there are more
than Nc error paths with metric larger than λi and 0 otherwise.
We now proceed to overbound the indicator function analogously to Section 5.10, by
realizing that B() = 1 if at least Nc error paths have a metric such that
Ln(˜x) ≥λj,
(5.121)
and, hence,
⎛
⎝1
Nc

x′∈X ′
j
exp

α

Ln(˜x) −λj

⎞
⎠
ρ
≥1,
(5.122)
where α and ρ are arbitrary positive constants. Note that we have extended the sum in
(5.122) over all error sequences as is customary in random coding analysis. We may now
use (5.122) to overbound the indicator function B() and we obtain
Pr(Cj ≥Nc) ≤N−ρ
c

x
p(x)

y
p(y|x)
⎛
⎝
x′∈X ′
j
exp

α

Ln(˜x) −λj)

⎞
⎠
ρ
dy
(5.123)
and, due to (5.119),
exp(−αρλi) ≤
∞

m=j
exp(−αρLm(˜x)),
(5.124)

5.11. RANDOM CODING ANALYSIS OF SEQUENTIAL DECODING
235
and we have
Pr(Cj ≥Nc)
≤
N −ρ
c

x
p(x)

y
p(y|x)
⎛
⎝
x′∈X ′
j
exp

αLn(˜x)

⎞
⎠
ρ
×
∞

m=j
exp(−αρLm(˜x)) dy.
(5.125)
Analogously to Section 5.10, let c be the correct path and let e be the incorrect path
which diverges from c at node j. Let E be the set of all incorrect paths and let E
j be the set
of incorrect paths (not signal sequences) corresponding to x
j. Again, x and x are, strictly
taken, the signal sequences assigned to the correct and incorrect path, respectively, and
˜x, ˜x are the associated partial sequences. Let then Avg{Pr(Cj ≥Nc)} be the ensemble
average of Pr(Cj ≥Nc) over all linear-trellis codes, i.e.,
Avg{Pr(Cj ≥Nc)}
≤
N −ρ
c

c
p(c)

y
p(y|x)
∞

m=j
exp(−αρLm(˜x))
×
⎛
⎝
e∈E′
j
exp(αLn(˜x))
⎞
⎠
ρ
dy.
(5.126)
Note, we have used Jensen’s inequality to pull the averaging into the second sum, which
restricts ρ to 0 ≤ρ ≤1. Since we are using time-varying random trellis codes, (5.126)
becomes independent of the starting node j, which we arbitrarily set to j = 0 now.
Observe there are at most 2kn paths e of length n in E
j = E. Using this and the
inequality9

ai
ρ
≤

aρ
i ,
ai ≥0,
0 ≤ρ ≤1,
(5.127)
9This inequality is easily shown, i.e.,
aρ
i
(
ai)ρ =
ai
ai
ρ
≥
ai
ai
= 1,
where the inequality resulted from the fact that each term in the sum is ≤1 and ρ ≤1.

236
CHAPTER 5. TRELLIS AND TREE DECODING
we obtain10
Avg{Pr(C0 ≥Nc)}
≤
N −ρ
c

c
p(c)

y
p(y|x)
∞

m=0
exp(−αρLm(˜x))
×
∞

n=0
2knρ 
exp(αLn(˜x))
ρ
dy
(5.128)
=
N−ρ
c

c
p(c)
∞

m=0
∞

n=0
2knρ
×

y
p(y|x) exp(−αρLm(˜x))

exp(αLn(˜x))
ρ
dy.
(5.129)
Now we substitute the metrics (see (5.10))
Lm(˜x)
=
m

r=0
log
p(yr|xr)
p(yr)

−k,
(5.130)
Ln(˜x)
=
n

r=0
log
p(yr|x
r)
p(yr)

−k,
(5.131)
into the expression (5.129) and use α =
1
1+ρ. Now rewrite the exponentials in (5.129) as
2knρ

y
p(y|x) exp(−αρLm(˜x))

exp(αLn(˜x))
ρ
=
⎧
⎨
⎩
2−(m−n)Ec(ρ)−m(Ece(ρ)−kρ)
if m ≥n
2−(n−m)(Ee(ρ)−kρ)−n(Ece(ρ)−kρ)
if n ≥m.
(5.132)
The exponents used above are given by
2−Ec(ρ)
=

v

x
q(x)p(v|x)
p(v|x)
p(v) 2−k
−
ρ
1+ρ
=
2k
ρ
1+ρ

v

x
q(x)p(v|x)
1
1+ρ p(v)
ρ
1+ρ
≤
2k
ρ
1+ρ
⎛
⎝

v

x
q(x)p(v|x)
1
1+ρ
1+ρ⎞
⎠
1
1+ρ
=
2k
ρ
1+ρ −
1
1+ρ E0(ρ,q) = fc,
(5.133)
10Note that it is here that we need the time-varying assumption of the codes.

5.11. RANDOM CODING ANALYSIS OF SEQUENTIAL DECODING
237
where we have used H¨older’s inequality above with βi = 
x q(x)p(v|x)
1
1+ρ , γi = p(v)
ρ
1+ρ
and λ =
1
1+ρ, and, “magically” there appears the error exponent from Section 5.10, equa-
tion (5.103)! Analogously, we also ﬁnd
2−(Ee(ρ)−kρ)
=
2kρ

v
p(v)

x′
q(x)
p(v|x)
p(v) 2−k

1
1+ρ
ρ
=
2k ρ2
1+ρ

v
p(v)
1
1+ρ

x′
q(x)p(v|x)
1
1+ρ
ρ
≤
2k ρ2
1+ρ
⎛
⎝

v

x′
q(x)p(v|x)
1
1+ρ
1+ρ⎞
⎠
ρ
1+ρ
=
2k ρ2
1+ρ −
ρ
1+ρ E0(ρ,q) = fe,
(5.134)
where λ =
ρ
1+ρ. Finally we obtain
2−(Ece(ρ)−kρ)
=
2kρ

v

x
q(x)p(v|x)

x′
q(x)
p(v|x)
p(v|x)

1
1+ρ
ρ
=
2kρ

v

x
q(x)p(v|x)
1
1+ρ

x′
q(x)p(v|x)
1
1+ρ
ρ
=
2kρ−E0(ρ,q).
(5.135)
Note now that, since 1 =
ρ
1+ρ +
1
1+ρ, we have
2kρ−E0(ρ,q) = 2k ρ2
1+ρ −
ρ
1+ρ E0(ρ,q)2k
ρ
1+ρ −
1
1+ρ E0(ρ,q) = fefc,
(5.136)
where the two factors fe and fc are deﬁned in (5.133) and (5.134).
With this we can rewrite (5.129) as
Avg{Pr(C0 ≥Nc)} ≤N −ρ
c

c
p(c)
∞

m=0
∞

n=0
fn
e fm
c .
(5.137)
The double inﬁnite sum in (5.137) converges if fe, fc < 1 and, hence from (5.136), if
ρk < E0(ρ, q) and we obtain
Avg{Pr(C0 ≥Nc)} ≤N −ρ
c
1
(1 −fe) (1 −fc).
(5.138)

238
CHAPTER 5. TRELLIS AND TREE DECODING
Similarly, it can be shown [52] that there exists a lower bound on the number of
computations, given by
Avg{Pr(Cj ≥Nc)} ≥N −ρ
c
(1 −o(Nc)) .
(5.139)
Together, (5.138) and (5.139) characterize the computational behavior of sequential
decoding. It is interesting to note that if ρ ≤1, the expectation of (5.138) and (5.139),
i.e., the expected number of computations, becomes unbounded, since
∞

Nc=1
N−ρ
c
(5.140)
diverges for ρ ≤1 or k ≥R0.
Information theory therefore tells us that we cannot
beat the capacity limit by using very powerful codes and resorting to sequential decoding,
since what happens is that as soon as the code rate reaches R0, the expected number of
computations per node tends to inﬁnity. In eﬀect our decoder fails through buﬀer overﬂow.
Further credence to R0 is given by the observation that rates R = R0 at bit error
probabilities of Pb = 10−5 −10−6 can be achieved with trellis codes. This observation was
made by Wang and Costello [54], who constructed random trellis codes for 8-PSK and
16-QAM constellations which achieve R0 with constraint lengths of 15 and 16.
5.12
Some Final Remarks
As we have seen, there are two broad classes of decoders, the depth-ﬁrst and the breadth-
ﬁrst algorithms. Many attempts have been made at comparing the respective properties
of these two basic approaches; for example, [40]—or, for convolutional codes, [52]—is an
excellent and inexhaustible source of information. Many of the random coding arguments
in [52] for convolutional codes can be extended to trellis codes with little eﬀort.
Where are we standing then?
Sequential decoding has been popular in particular
for relatively slow transmission speeds, since the buﬀer sizes can then be dimensioned
such that buﬀer overﬂow is controllable. Sequential decoding, however, suﬀers from two
major drawbacks.
Firstly, it is a “sequential” algorithm, i.e., modern pipelining and
parallelizing are very diﬃcult, if not impossible, to accomplish. Secondly, the metric used
in sequential decoding contains the “bias” term accounting for the diﬀerent path lengths.
This makes sequential decoding very channel dependent. Furthermore, this bias term may
be prohibitively complex to calculate for other than straight channel coding applications
(see, e.g., [55]).
Breadth-ﬁrst search algorithms, in particular the optimal Viterbi algorithm and the
popular M-algorithm, do not suﬀer from the metric “bias” term. These structures can
also be parallelized much more readily, which makes them good candidates for VLSI

5.12. SOME FINAL REMARKS
239
implementations. They are therefore very popular for high-speed transmission systems.
The Viterbi algorithm can be implemented with a separate metric calculator for each state.
More on the implementation aspects of parallel Viterbi decoder structures can be found
in [15, 22, 17]. The Viterbi decoder has proven so successful in applications that it is the
algorithm of choice for most applications of code decoding at present.
The M-algorithm can also be implemented exploiting inherent parallelism of the al-
gorithm, and [46] discusses an interesting implementation which avoids the sorting of the
paths associated with the basic algorithm. The M-algorithm has also been successfully
applied to multi-user detection, a problem which can also be stated as a trellis (tree) search
[56], and to the decoding of block codes.
However the importance of all these decoding algorithms, with the likely exception of
the Viterbi algorithm, is rapidly fading in favor of the APP decoding algorithms. The
impact of iterative decoding of large error control codes, which use the APP algorithms
as component (see Chapters 8 and 9 on turbo coding and related topics), has been so
revolutionary as to push other strategies into virtual obscurity.
Appendix 5.A
The following theorem is also known as Gerschgorin’s circle theorem:
Theorem 5.5 Every eigenvalue λi of a matrix P with arbitrary complex entries lies in
at least one of the circles Ci, whose centers are at pii and whose radii are ri = 
j̸=i |pij|,
i.e., ri is the i-th absolute row sum without the diagonal element.
Proof: Px = λx immediately leads to
(λ −pii)xi =

j̸=i
pijxj.
(5.141)
Taking absolute values on both sides and applying the triangle inequality, we obtain
|λ −pii| ≤

j̸=i
|pij||xj|/|xi|.
(5.142)
Now let xi be the largest component of x, then |xj|/|xi| ≤1, and |λ −pii| ≤ri.
Q.E.D.
In the application in Section 5.9, all entries pij ≥0, and hence
(λ −pii) ≤

j̸=i
pij ⇒λ ≤

j
pij.
(5.143)
Now, the largest eigenvalue λmax must be smaller than the largest row sum, i.e.,
λmax ≤max
i

j
pij.
(5.144)

240
CHAPTER 5. TRELLIS AND TREE DECODING
Appendix 5.B
In this appendix we describe an eﬃcient algorithm to produce an equivalent FSM to a
given FSM M, which has the minimal number of states among all equivalent FSMs. The
algorithm starts out assuming all states are equivalent, and then successively partitions
the sets of equivalent states until all true equivalent states are found.
The algorithm
terminates in a ﬁnite number of steps, since this reﬁnement of the partitioning must end
when each original state is in an equivalent set by itself. The algorithm performs the
following steps:
Step 1: Form a ﬁrst partition P1 of the states of M by grouping together states which
produce identical sets of outputs δ(U →U ) as we go through all transitions U →U.
Step 2: Obtain the (l + 1)th partition Pl+1 from the lth partition Pl as follows: Two
states u and v are in the same equivalent group of Pl+1 if and only if
(i) U and V are in the same equivalent group of Pl, and
(ii) for each pair of transitions U →U and V →V  which produce the same
output, U and V  are in the same equivalent set of Pl.
Step 3: Repeat Step 2 until no further reﬁnement occurs, i.e., until Pl+1 = Pl. Pl is the
ﬁnal desired partition of the states of M into equivalent states. Any member of the
group can now be used to represent the entire group.
The recursive nature of the algorithm quickly proves that it actually produces the
desired partition. Namely, if two states are in Pl, all their sequences of output sets must
be identical for l steps, since their successors are in Pl−1, etc., all the way back to P1,
which is the ﬁrst and largest group of states which produce identical sets of outputs.
Now, if Pl+1 = Pl, the successors of the above two states are in Pl also, and hence
their sequences of output sets are identical for l + 1 steps also. We conclude that their
sequences of output sets are therefore identical for all time.
The above algorithm can be extended to input and output equivalence of FSM’s. The
interested reader will ﬁnd a more complete discussion in [19, Chapter 4].
Appendix 5.C
In this appendix we calculate the vector Euclidean distance for a speciﬁc set Cp of retained
paths. Noting that δn from (5.19) is a vector of Gaussian random variables (see also Section
2.6), we can easily write down its probability density function [57], viz.
p(δn) =
1
(2π)M/2|R|1/2 exp

−1
2 (μ −δn)T R−1 (μ −δn)

(5.145)

5.12. SOME FINAL REMARKS
241
where μ is the vector of mean values given by μi = d2
i , and R is the covariance matrix of
the Gaussian random variables δn whose entries rij = E

(δ(i,c)
n
−μi)(δ(j,c)
n
−μj)

can be
evaluated as
rij =
⎧
⎪
⎨
⎪
⎩
2N0

d2
i + d2
j −d2
ij

if i = j,
4N0d2
i
if i = j
(5.146)
and where
d2
ij =
˜x(pi) −˜x(pj)
2
and
d2
i =
˜x(pi) −˜x(c) .
(5.147)
The vector μ of mean values is given by μi = d2
i .
Now the probability of losing the correct path at time n can be calculated by
Pr(CPL|Cp) =

δn≤0
p(δn)dδn.
(5.148)
Equation (5.148) is diﬃcult to evaluate due to the correlation of the entries in δn, but one
thing we know is that the area δn ≤0 of integration is convex. This allows us to place
a hyperplane through the point closest to the center of the probability density function,
μ, and overbound (5.148) by the probability that the noise carries the point μ across this
hyperplane. This results in a simple one-dimensional integral, whose value is given by
(compare also (2.14))
Pr(CPL|Cp) ≤Q
⎛
⎝

d2
l
2N0
⎞
⎠,
(5.20)
where d2
l , the Vector Euclidean distance, is given by
d2
l = 2N0 min
y≤0
(μ −y)T R−1 (μ −y),
(5.149)
and y is simply a dummy variable of minimization.
The problem of calculating (5.20) has now been transformed into the geometric prob-
lem of ﬁnding the point on the surface of the convex polytope y ≤0 which is closest to
μ using the distance measure of (5.149). This situation is illustrated in Figure 5.28 for
a 2-dimensional scenario. The minimization in (5.149) is a constrained minimization of a
quadratic form. Obviously, some of the constraints y ≤0 will be met with equality. These
constraints are called the active constraints, i.e., if y = (y(a), y(p))T is the partitioning of y
into active and passive components, y(a) = 0. This minimum is the point y0 in Figure 5.28.
The right-hand side of Figure 5.28 also shows the geometric conﬁguration when the decor-
relating linear transformation δ
n =
√
R(−1)δn is applied. The vector Euclidean distance

242
CHAPTER 5. TRELLIS AND TREE DECODING
y2
y1
y0
μ
y
0
y
1
y
2
μ
Integration Area
Overbound
Integration Area
Overbound
Figure 5.28: Illustration of the concept of the vector Euclidean distance with M = 2. The
distance between y0 and μ is d2
l . The right-hand side shows the space after decorrelation,
and d2
l equals the standard Euclidean distance between y
0 and μ.
(5.149) is invariant to such a transformation, but E

(δ(i,c)
n
−μ
i)(δ(j,c)
n
−μ
j)

= δij, i.e.,
the decorrelated metric diﬀerences are independent with unit variance each. Naturally we
may work in either space. Since the random variables δ
n are independent, equal-variance
Gaussian, we know from basic communication theory [57, Chapter 2], that the probability
that μ is carried into the shaded region of integration can be overbounded by integrating
over the half-plane not containing μ, as illustrated in the ﬁgure. This leads to (5.20).
We now have to minimize
d2
l = 2N0 min
y(p)≤0
μ(p)
μ(a)

−

y(p)
0
T R(pp) R(pa)
R(ap) R(aa)
−1μ(p)
μ(a)

−

y(p)
0

,
(5.150)
where we have partitioned μ and R analogously to y. After some elementary operations
we obtain
y(p) = μ(p) +

X(pp)−1
X(pa)μ(a) ≤0
(5.151)
and
d2
l = 2N0μ(a)T 
X(aa)−1
μ(a),
(5.152)

5.12. SOME FINAL REMARKS
243
where11
X(pp)
=

R(pp) −R(pa) 
R(aa)−1
R(ap)
−1
,
X(aa)
=

R(aa)−1 
I + R(ap)X(pp)R(pa)
,
X(pa)
=
−X(pp)R(pa) 
R(aa)−1
.
(5.153)
We are now presented with the problem of ﬁnding the active components in order to
evaluate (5.152). This is a combinatorial problem, i.e., we must test all 2M −1 = M
i=1
M
i

possible combinations of active components from the M entries in y for compatibility with
(5.151). This gives us the following procedure:
Step 1: Select all 2M −1 combinations of active components and set y(a) = 0 for each.
Step 2: For each combination for which y(p) ≤0, store the resulting d2
l from (5.152) in
a list.
Step 3: Select the smallest entry from the list in Step 2 as d2
l .
As an example, consider again Figure 5.28. The 22 −1 = 3 combinations correspond
to the points y0, y1, and y2. The point y1 does not qualify, since it violates (5.151). The
minimum is chosen between y0 and y2. This process might be easier to visualize in the
decorrelated space y, where all the distances are ordinary Euclidean distances, and the
minimization becomes obvious.
One additional complication needs to be addressed at this point.
The correlation
matrix R may be singular.
This happens when one or more entries in y are linearly
dependent on the other entries. In the context of the restriction y ≤0, we have redundant
conditions. The problem, again, is that of ﬁnding the redundant entries which can be
dropped from consideration. Fortunately, our combinatorial search helps us here. Since
we are examining all combinations of possible active components, we may simply drop
any dependent combinations which produce a singular R(aa) from further consideration
without aﬀecting d2
l .
11These equations can readily be derived from the partitioned matrix inversion lemma:
A
B
C
D
−1
=
E
F
G
H
,
where
E = A −BD−1C
−1 ; F = −EBD−1; G = −D−1CE.
and
H = D−1 + D−1CEBD−1.

244
CHAPTER 5. TRELLIS AND TREE DECODING

Bibliography
[1] F.L. Alvarado, “Manipulation and visualization of sparse matrices,” ORSA J. Com-
put., vol. 2, no. 2, Spring 1990.
[2] J.B. Anderson, ”Limited search trellis decoding of convolutional codes,” IEEE Trans.
Inform. Theory, vol. IT-35, Sept. 1989.
[3] J.B. Anderson and S. Mohan, “Sequential coding algorithms: A survey and cost
analysis,” IEEE Trans. Commun., vol. COM-32, no. 2, pp. 169–176, Feb. 1984.
[4] J.B. Anderson and S. Mohan, Source and Channel Coding: An Algorithmic Approach,
Kluwer Academic Publishers, Boston, MA, 1991.
[5] T. Aulin, “Breadth ﬁrst maximum-likelihood sequence detection,” IEEE Trans. Com-
mun., vol. COM-47, no. 2, pp. 208–216, Feb. 1999.
[6] T. Aulin, “Recovery Properties of the SA(B) Algorithm,” Technical Report No. 105,
Chalmers University of Technology, Sweden, Feb. 1991.
[7] T. Aulin, “Study of a new trellis decoding algorithm and its applications,” Final
Report, ESTEC Contract 6039/84/NL/DG, European Space Agency, Noordwijk, The
Netherlands, Dec. 1985.
[8] O. Axelsson,
“Milestones in the development of iterative solution methods,”
J. Electr. Comp. Engr., vol. 2010. doi:10.1155/2010/972794.
[9] L.R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear codes for
minimizing symbol error rate,” IEEE Trans. Inform. Theory, vol. IT-20, Mar. 1974.
[10] K. Balachandran, “Design and performance of constant envelope and non-constant
envelope digital phase modulation schemes,” Ph.D. thesis, ECSE Dept. Renselear
Polytechnic Institute, Troy, NY, Feb. 1992.
[11] E. Biglieri, “High-level modulation and coding for nonlinear satellite channels,” IEEE
Trans. Commun., vol. COM-32, pp. 616–626, May 1984.
[12] E. Biglieri and P.J. McLane, “Uniform distance and error probability properties of
TCM schemes,” IEEE Trans. Commun., vol. COM-39,, pp. 41–53, Jan. 1991.
[13] R.E. Blahut, Principles and Practice of Information Theory, Addison-Wesley, Read-
ing, MA, 1987.
245

246
BIBLIOGRAPHY
[14] P.R. Chevillat and D.J. Costello, Jr., “A multiple stack algorithm for erasurefree
decoding of convolutional codes,” IEEE Trans. Commun., vol. COM-25, pp. 1460–
1470, Dec. 1977.
[15] G.C. Clark and J.B. Cain, Error-Correction Coding for Digital Communications,
Plenum Press, New York, 1983.
[16] B. Classen, K. Blankenship, and V. Desai, “Turbo decoding with the constant-log-
MAP algorithm,” Proc. Second Int. Symp. Turbo Codes and Related Appl. (Brest,
France), pp. 467–470, Sept. 2000.
[17] O.M. Collins, “The subtleties and intricacies of building a constraint length 15 convo-
lutional decoder,” IEEE Trans. Commun., vol. COM-40, pp. 1810–1819, Dec. 1992.
[18] T. M. Cover and J. A. Thomas, Elements of Information Theory, John Wiley & Sons,
New York, 1991.
[19] P. J. Denning et al., Machines, Language and Computation, Prentice Hall, Englewood
Cliﬀs, NJ, 1978.
[20] European Telecommunications Standards Institute, “Universal mobile telecommuni-
cations system (UMTS): Multiplexing and channel coding (FDD),” 3GPP TS 125.212
version 3.4.0, pp. 14–20, September 23, 2000.
[21] R.M. Fano, “A heuristic discussion of probabilistic decoding,” IEEE Trans. Inform.
Theory, vol. IT-9, pp. 64–74, April 1963.
[22] G. Feygin and P.G. Gulak, “Architectural tradeoﬀs for survivor sequence memory
management in Viterbi decoders,” IEEE Trans. Commun., vol. COM-41, pp. 425–
429, March 1993.
[23] G.D. Forney, Jr. “The Viterbi algorithm,” Proc. IEEE, vol. 61, pp. 268–278, 1973.
[24] G.D. Forney, “Maximum-likelihood sequence estimation of digital sequences in the
presence of intersymbol interference,” IEEE Trans. Inform. Theory, vol. IT-18, pp.
363-378, May 1972.
[25] G.J. Foschini, “A reduced state variant of maximum likelihood sequence detection
attaining optimum performance for high signal-to-noise ratios,” IEEE Trans. Inform.
Theory, vol. IT-23,, pp. 605–609, Sept. 1977.
[26] R.G. Gallager, Information Theory and Reliable Communication, John Wiley & Sons,
New-York, 1968.
[27] J.M. Geist, “An empirical comparison of two sequential decoding algorithms,” IEEE
Trans. Commun., vol. COM-19, pp. 415–419, Aug. 1971.
[28] J.M. Geist, “Some properties of sequential decoding algorithms,” IEEE Trans. In-
form. Theory, vol. IT-19, pp. 519–526, July 1973.
[29] W.J. Gross and P.G. Gulak, “Simpliﬁed map algorithm suitable for implementation
of turbo decoders,” Electron. Lett., vol. 34, pp. 1577–1578, Aug. 6, 1998.
[30] S. Lin and D.J. Costello, Jr., Error Control Coding, Prentice Hall, Englewood Cliﬀs,
NJ, 1983.

BIBLIOGRAPHY
247
[31] D. Haccoun and M.J. Ferguson, “Generalized stack algorithms for decoding convolu-
tional codes,” IEEE Trans. Inform. Theory, vol. IT-21,, pp. 638–651, Nov. 1975.
[32] F. Jelinek, “A fast sequential decoding algorithm using a stack,” IBM J. Res. Dev.,
vol. 13, pp. 675–685, Nov. 1969.
[33] F. Jelinek and A.B. Anderson, “Instrumentable tree encoding of information sources,”
IEEE Trans. Inform. Theory, vol. IT-17, Jan. 1971.
[34] L. Ma, “Suboptimal decoding strategies,” MSEE thesis, University of Texas at San
Antonio, May 1996.
[35] R.J. McEliece, “On the BCJR trellis for linear block codes,” IEEE Trans. Inform.
Theory, vol. IT-42, no. 4, pp. 1072–1092, July 1996.
[36] J.L. Massey, “Variable-length codes and the Fano metric,” IEEE Trans. Inform.
Theory, vol. IT-18, pp. 196–198, Jan. 1972.
[37] J.L. Massey, “Coding and modulation in digital communications”, Proc. Int. Z¨urich
Sem. Digital Commun., Z¨urich, Switzerland, pp. E2(1)–E2(4), March 1974.
[38] H. Osthoﬀ, J.B. Anderson, R. Johannesson, and C-F. Lin, “Systematic feed-forward
convolutional encoders are better than other encoders with an M-algorithm decoder,”
IEEE Trans. Inform. Theory, vol. IT-44, no. 2, pp. 831–838, March 1998.
[39] J.K. Omura, “On the Viterbi decoding algorithm,” IEEE Trans. Inform. Theory, vol.
IT-15, pp. 177–179, Jan. 1969.
[40] G.J. Pottie and D.P. Taylor, “A comparison of reduced complexity decoding algo-
rithms for trellis codes,” IEEE J. Select. Areas Commun., vol. SAC-7, no. 9, pp.
1369–1380, Dec. 1989.
[41] J.G. Proakis, Digital Communications, McGraw-Hill, New York, 1989.
[42] P. Robertson, P. H¨oher, and E. Villebrun, “Optimal and sub-optimal maximum a
posteriori algorithms suitable for turbo decoding,” Eur. Trans. Telecommun., vol. 8,
pp. 119–125, March/April 1997.
[43] M. Rouanne and D.J. Costello, Jr., “An algorithm for computing the distance spec-
trum of trellis codes,” IEEE J. Select. Areas Commun., vol. SAC-7, no. 6, pp. 929–940,
Aug. 1989.
[44] C. Schlegel, “Evaluating distance spectra and performance bounds of trellis codes
on channels with intersymbol interference,” IEEE Trans. Inform. Theory, vol. IT-37,
No. 3, pp. 627–634, May, 1991.
[45] C.E. Shannon, “A mathematical theory of communications,” Bell Syst. Tech. J., vol.
27, pp. 379-423, July 1948.
[46] S.J. Simmons, “A nonsorting VLSI structure for implementing the (M,L) algorithm,”
IEEE J. Select. Areas Commun., vol. SAC-6, pp. 538–546, April, 1988.
[47] S.J. Simmons and P. Wittke, “Low complexity decoders for constant envelope digital
modulation,” Conf. Rec., GlobeCom, Miami, Florida, pp. E7.7.1–E7.7.5, Nov. 1982.
[48] G. Strang, Linear Algebra and Its Applications, Harcourt Brace Jovanovich, San
Diego, 1988.

248
BIBLIOGRAPHY
[49] G. Ungerboeck, “Channel coding with multilevel/phase signals,” IEEE Trans. Inform.
Theory, vol. IT-28, no. 1, pp. 55-67, Jan. 1982.
[50] S. Verd´u, “Minimum probability of error for asynchronous Gaussian multiple-access
channels,” IEEE Trans. Inform. Theory, vol. IT-32, pp. 85-96, Jan. 1986.
[51] A.J. Viterbi, “Error bounds for convolutional codes and an asymptotically optimum
decoding algorithm,” IEEE Trans. Inform. Theory, vol. IT-13, April 1969.
[52] A.J. Viterbi and J.K. Omura, Principles of Digital Communication and Coding,
McGraw-Hill, New York, 1979.
[53] A.J. Viterbi and J.K. Omura, Principles of Digital Communication and Coding,
McGraw-Hill, New Yoir, 1979.
[54] F.-Q. Wang and D.J. Costello, Jr., “Probabilistic construction of large constraint
length trellis codes for sequential decoding,” IEEE Trans. Commun., vol. COM-43,,
no. 9, pp. 2439–2448, Sept. 1995.
[55] L. Wei, L.K. Rasmussen, and R. Wyrwas, “Near optimum tree-search detection
schemes for bit-synchronous CDMA systems over Gaussian and two-path rayleigh
fading channels,” IEEE Trans. Commun., vol. COM-45, no. 6, June 1997.
[56] L. Wei and C. Schlegel, “Synchronous DS-SSMA with improved decorrelating
decision-feedback multiuser detection,” IEEE Trans. Veh. Technol., vol. VT-43, no.
3, Aug. 1994.
[57] J.M. Wozencraft and I.M. Jacobs, Principles of Communiation Engineering, John
Wiley & Sons, New York, 1965.
[58] J.M. Wozencraft and B. Reiﬀen, Sequential Decoding, M.I.T. Press, Cambridge, MA,
1961.
[59] J.M. Wozencraft and R.S. Kennedy, “modulation and demodulation for probabilistic
coding”, IEEE Trans. Inform. Theory, vol. IT-12, no. 3, pp. 291–297, July 1966.
[60] J.M. Wozencraft and I.M. Jacobs, Principles of Communication Engineering, John
Wiley & Sons, New York, 1965.
[61] E. Zehavi and J.K. Wolf, “On the performance evaluation of trellis codes,” IEEE
Trans. Inform. Theory, vol. IT-33, no. 2, pp. 196–201, March 1987.
[62] W. Zhang and C. Schlegel, “State reduction in the computation of dfree and the
distance spectrum for a class of convolutional codes,” Proceedings SICON/ICIE 93,
Singapore, Sept. 1993.
[63] W. Zhang, “Finite-state systems in mobile communications,” Ph.D. dissertation, Uni-
versity of South Australia, June 1995.
[64] K.Sh. Zigangirov, “Some sequential decoding procedures,” Prob. Pederachi Inform.,
vol. 2, pp. 13–25, 1966.
[65] K.S. Zigangirov and V.D. Kolesnik, “List decoding of trellis codes,” Problems of
Control and Information Theory, no. 6, 1980.

Chapter 6
Low-Density Parity-Check Codes
6.1
Introduction
Low-density parity-check (LDPC) codes can rightfully take their stand next to turbo
codes as the most powerful and most popular error control codes known. They oﬀer a
performance spectacularly close to theoretical limits when decoded using iterative soft-
decision algorithms. Indeed, a rate R = 1/2 LDPC code with a blocklength of 107 bits
comes to within 0.04 dB of the Shannon limit for the binary-input additive white Gaussian
noise channel at a bit error rate of 10−6 [9]. It has a decoding threshold only 0.0045 dB
away from the Shannon limit. Consequently, both LDPC and turbo codes have been called
“capacity-approaching codes,” and we shall occasionally use this informal term.
Both
coding families are “practically” capable of solving Shannon’s channel coding challenge.
Low-Density Parity-Check (LDPC) codes and an iterative decoding algorithm for them
were ﬁrst introduced by Gallager more than ﬁfty years ago [18, 19]. However, for several
decades LDPC codes were largely forgotten, arguably because computers of the time could
not simulate these codes with meaningful blocklengths at low error rates, and no theoretical
results tying these codes to the Shannon limit could be established.
Following the discovery of turbo codes (Chapter 8), LDPC codes were rediscovered
through the work of MacKay and Neal [33, 34], and quickly entered main stream interest.
LDPC codes signiﬁcantly diﬀer from the more conventional trellis and “regular” block
codes of the time. First, they are constructed in a random manner, well, in a controlled
pseudo-random manner in most cases of interest, and second, they have a decoding algo-
rithm whose complexity is linear in the block length of the code. This allows the decoding
of large codes, which is essential in approaching the Shannon capacity. When combined
with their spectacular performance, these properties makes LDPC codes a compelling class
of error control codes, and their adoption in a multitude of communications applications.
In this chapter, we explore the properties and structure of low-density parity-check
249
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

250
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
codes.
We begin with the fundamental construction of regular LDPC codes and their
representation as bipartite graphs.
The graphical representation of LDPC codes then
leads us to the notion of irregular LDPC codes, which have been used by some authors to
improve performance, and to the density evolution analysis technique (DE), which predicts
the performance of these codes. LDPC codes and their decoders can be thought of as a
prescription for selecting large random block codes, not unlike the codes suggested by
Shannon’s coding theorem. MacKay [33] showed, by applying random coding arguments
to the random sparse generator matrices of LDPC codes, that such code ensembles can
approach the Shannon limit exponentially fast in the codelength. And even at moderate
codelengths, LDPCs provide impressive performance results, some of which are illustrated
in Figure 6.1, and n denotes the codelength.
1
1.5
2
2.5
3
3.5
4
4.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
n = 200
n = 400
n = 1000
n = 2000
n = 4000
n = 20, 000
Bit Error Probability (BER)
Eb/N0[dB]
Figure 6.1: Performance results for regular (3,6) LDPC codes of rate R = 1/2 for varying
block lengths. The capacity of the channel is at Eb/N0 = 0.18 dB, and the threshold for
these codes is at 1.1 dB.
Today, LDPC codes have been adopted in most recent industry standards, including
wireless local-area networks (LANs), in IEEE 802.11n, WiMAX (IEEE 802.16e), digi-
tal video broadcasting (DVB-S2), 10GBase-T Ethernet over twisted-pair copper cabling
(IEEE 802.3an), and the International Telecommunication Union standard for networking
over power lines, phone lines, and coaxial cable (G.hn/G.9960).

6.2. LDPC CODES AND GRAPHS
251
6.2
LDPC Codes and Graphs
A linear block code C of rate R = k/n can be deﬁned in terms of an (n −k) × n parity
check matrix H = [h1, h2, . . . , hn], where each hj is a column vector of length n−k. Each
entry hij of H is an element of a ﬁnite ﬁeld GF(p). We will only consider binary codes
in this chapter, so each entry is either a ‘0’ or a ‘1’ and all operations are modulo 2. The
code C is the set of all vectors x that lie in the (right) nullspace of H, i.e., Hx = 0. Given
a parity-check matrix H, we can ﬁnd a corresponding k × n generator matrix G such that
GHT = 0. The generator matrix can be used as an encoder according to xT = uT G.
In its simplest guise, an LDPC code is a linear block code with a parity-check matrix
that is “sparse,” i.e., it has a small number of non-zero entries. The precise deﬁnition of
sparseness is as follows: An m × n matrix has nm positions, and such a matrix is called
sparse if the number of non-zero entries is of order O(n), where here n > m. That is, as we
consider sequences of ever larger matrices, the number of non-zero entries grows at most
as n. For very large such matrices, the majority of entries is therefore zero, and many
eﬃcient low complexity matrix manipulation algorithms are known for sparse matrices.
In [18, 19], Gallager proposed constructing LDPC codes by randomly placing 1’s and
0’s in a m × n parity-check matrix H subject to the constraint that each row of H had
the same number dv of 1’s and each column of H had the same number dc of 1’s. For
example, the m = 12 × n = 24 parity-check matrix shown in Figure 6.2 has dv = 6 and
dc = 3 and deﬁnes an LDPC code of length n = 24. Codes of this form are referred to
as regular (dv, dc) LDPC codes of length n. Note that such matrices are sparse for large
n since the number of nonzero entries is dvn. This is not really well visible in the rather
small example of Figure 6.2.
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
1
1
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
1
0
0
0
0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
1
0
1
0
0
0
0
1
0
0
1
0
0
0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Figure 6.2: Gallager-type low-density parity-check matrix H1.

252
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Recall that each row in a parity-check matrix represent a parity-check equation, involv-
ing some of the possible n variables. Therefore, in a (dv, dc) LDPC code, each information
bit, that is, each variable, is involved in dv parity-checks and each parity-check equation
in turn involves dc information bits. The fraction of 1’s in the parity-check matrix of
a regular LDPC code is mdc/(mn) = dc/n, which approaches zero as the block length
becomes large and leads to the sparseness discussed above.
As a ﬁrst order of business, we wish to determine the rate of the code deﬁned by H1.
Since the parity check matrix is randomly constructed, there is no guarantee that the
rows are linearly independent and the matrix has full rank. Indeed, the rank of H1 is
13 < m = 15 and this parity-check matrix actually deﬁnes a code with rate R = 7/20.
In general, such randomly constructed parity-check matrices will not be full rank and
m = n −k.
We could of course eliminate linearly dependent rows to ﬁnd a (n−k)×n parity-check
matrix, but the new matrix would no longer be regular. For LDPC codes with large n
it is convenient to retain the original parity-check matrix even if it is not full rank. The
maximum number of parity bits is then m, and 1−m/n = 1−dv/dc is the maximum rate,
also called the design rate of the code.
Having deﬁned a blueprint for a (dv, dc) regular LDPC code, we are now left to con-
struct a particular instance of a code. To do this requires the choice of dv, dc, n, and
k, which are constrained by the relationship that, for a regular code mdc = ndv, i.e.,
the number of non-zero entries of H must be the same whether calculated by rows or by
columns. Furthermore, dv must be less than dc and the rate R is less than 1. Assuming
that the block length n and the code rate are determined, we need to ﬁx appropriate values
for dv and dc. In [19], Gallager showed that the minimum distance of a “typical” regular
LDPC code increases linearly with n, provided dv ≥3. Therefore, regular LDPC codes
are constructed with dv on the order of 3 or 4, subject to the above constraints. dv < 3 is
used in some codes, but this can cause diﬃculties with the iterative decoding algorithm
as we will see later. For large block lengths, the random placement of 1’s in H such that
each row has exactly dc 1’s and each column has exactly dv 1’s requires some eﬀort, and
systematic methods for doing this have been developed [33, 34]. Gallager himself used an
approach based on permutation matrices, which we will also discuss in the sequel.
An important advance in the theory of LDPC codes is made possible by Tanner’s [45]
method of using bipartite graphs to provide a graphical representation of the parity-check
matrix. As a recognition of Tanner’s work, the bipartite graph of an LDPC code is often
referred to as its Tanner graph. A bipartite graph is a graph in which the nodes may
be partitioned into two subsets such that there are no edges connecting nodes within a
subset. In the context of LDPC codes, the two subsets of nodes are the variable nodes and
the check nodes. There is one variable node for each of the n bits in the code and there
is one check node for each of the m rows of H. An edge exists between the ith variable
node and the jth check node if and only if the entry hij = 1 in H. The bipartite graph

6.2. LDPC CODES AND GRAPHS
253
+
+
+
+
+
+
+
+
+
+
+
+
+
Variable Nodes
dc
dv
Check Nodes
Figure 6.3: Bipartite graph for the (3, 6) regular LDPC code with parity-check matrix H1.
corresponding to H1 is shown in Figure 6.3. The number of edges incident upon a node is
called the degree of the node. Thus, the bipartite graph of a (dv, dc) LDPC code contains
n variable nodes of degree dv and m check nodes of degree dc.
It is clear that the parity-check matrix can be deduced from the bipartite graph and
thus the bipartite graph can be used to deﬁne the code C. We can therefore start talking
about codes as deﬁned by a set of variable nodes, a set of check nodes, and set of edges.
Note that the pair (dv, dc), together with the codelength n, speciﬁes an ensemble of codes,
rather than any particular code. This ensemble is denoted by Cn(dv, dc). Once the degrees
of the nodes are chosen, we are still free to choose which particular connections are made.
A socket refers to a point on a node to which an edge may be attached. For example, we
say that a variable node has dv sockets, meaning dv edges may be attached to that node.
There will be a total of ndv sockets on variable nodes and mdc sockets on parity-check
nodes. This is illustrated in Figure 6.4. Clearly the number of variable node sockets must
be equal to the number of check node sockets, and a particular pattern of edge connections
can be described as a permutation π from variable node sockets to check node sockets.
We have the following deﬁnition of an LDPC code as a permutation of socket numbers:
Deﬁnition 6.1 A regular LDPC code is completely deﬁned by a permutation π(i) of the
natural numbers 1 ≤i ≤dvn. The index i refers to the socket number at the variable
nodes, and π(i) to the socket number at the check nodes to which socket i connects.
Selecting a random code from the ensemble Cn(dv, dc) therefore amounts to randomly
selecting a permutation of ndv elements. Many permutations will result in a graph which

254
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
+
+
+ +
+
+
+ +
+
+
+ +
+
+
+
Variable Nodes
sockets
sockets
Check Nodes
Permuter: H
Figure 6.4: Illustration of the concept of a socket in a Tanner graph.
contains parallel edges, i.e., in which more than one edge join the same variable and
parity-check nodes.
Note that in the parity-check matrix, an even number of parallel
edges will cancel. If they are deleted from the graph, then the degrees of some nodes will
be changed and the code will cease to be a regular LDPC code. If they are not deleted,
their presence renders the iterative decoding algorithms ineﬀective. We must therefore
make the restriction that permutations leading to parallel edges are disallowed.
It was observed by Luby et al. [29] that allowing the degrees of the diﬀerent nodes
to diﬀer can provide improved performance. They constructed R = 1/2 codes with a
performance of up to 0.5 dB better than that of regular codes for moderate-length codes
when operated on an additive white Gaussian noise channel. An irregular LDPC code
cannot be deﬁned in terms of the degree parameters dv and dc. We must instead use degree
distributions to describe the variety of node degrees in the graph. A degree distribution
γ(x) is a polynomial in x,
γ(x) =

i
γixi−1,
(6.1)
such that γ(1) = 1. That is, the coeﬃcients γi can be thought of as probabilities. These
coeﬃcients are the fraction of edges in the graph which are connected to nodes of degree
i. Note this means that γi/i is proportional to the fraction of nodes in the graph which
have i edges–an important distinction–see below.
The codelength n and two degree distributions–λ(x) and ρ(x) for the variable and
check nodes, respectively–are suﬃcient to deﬁne an ensemble Cn(λ, ρ) of irregular LDPC
codes, and an individual irregular code is speciﬁed by the following deﬁnition:

6.3. LDPC DECODING VIA MESSAGE PASSING
255
Deﬁnition 6.2 An irregular LDPC code is deﬁned by a permutation π(i) of the natural
numbers 1 ≤i ≤Ne, where Ne = n/ 
i
λi
i is the total number of edges, and two degree
distributions λ(x) and ρ(x) for the variable nodes and check nodes, respectively.
In the above deﬁnition we shall always assume that the nodes are ordered in descending
degree values, which then completely speciﬁes a given code.
The number of variable nodes in an irregular code can be computed as
n = Ne

i
λi
i ,
(6.2)
since each node of degree i combines i edges, and Ne is the number of edges in the code.
Likewise, the number of check nodes is given by
m = Ne

i
ρi
i .
(6.3)
The design rate of an irregular code is given by the dimension of the parity-check matrix,
or the size of the bipartite graph as
R = n −m
n
= 1 −m
n = 1 −

i
ρ
i

i
λi
i
.
(6.4)
Naturally, we want to rule out parallel edges also for irregular codes.
6.3
LDPC Decoding via Message Passing
One of the principal advantages of LDPC codes is that they can be decoded using an
iterative decoding algorithm whose complexity grows (only) linearly with the block length
of the code. Now that we have a graphical representation of LDPC codes, we can decode
them using belief propagation decoding algorithms (see Chapter 6). Belief propagation is
one instance of a broad class of message passing algorithms on graphs widely discussed in
literature; see, for example, [26, 17, 1, 49]. The decoding algorithms for the binary erasure
channel and the binary symmetric channel are discussed below in Sections 6.4.1 and 6.4.3,
respectively, and the ubiquitous Gaussian channel is treated in Section 6.4.4 in detail.
All message passing algorithms must respect the following rule, which was introduced
with turbo codes [6] as the extrinsic information principle:
Rule 1 (Extrinsic Information Principle)
A message sent from a node i along an edge e cannot
depend on any message previously received on edge e.

256
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
The extrinsic information principle prevents that “local” incoming information is con-
tained in the outgoing messages, which would introduce correlations in the statistical
analysis presented later in this chapter. It also has the eﬀect of slowing down convergence
of the algorithms in general.
Before stating the algorithm, it is necessary to formulate the decoding problem. An
LDPC code is constructed in terms of its parity check matrix H, which is used for decoding.
To encode an information sequence, let us call it u, it is, in principle, necessary to derive
a generator matrix G, which has the property that GHT = 0. Finding a suitable G is
greatly simpliﬁed if we ﬁrst convert H into the equivalent systematic parity check matrix
HS = [A|In−k]. As is well known, the systematic generator matrix is then given by
GS =

Ik|AT 
.
The information sequence is now encoded as xT = uT Gs. It is worth noting that en-
coding by matrix multiplication following this procedure has complexity O(n2), since Gs
is typically no longer sparse, and that therefore, in general, LDPC codes have linear de-
coding complexity, but apparently quadratic encoding complexity if we subscribe to this
logic. Methods for reducing the encoding complexity are therefore of interest and will be
discussed speciﬁcally Section 6.6 and throughout this chapter.
The codeword x is transmitted over an additive white Gaussian noise (AWGN) channel
using BPSK modulation resulting in the received sequence r = √Es(2x −1) + n (see
Chapter 2), where Es is the received energy of a single symbol (bit), and n is a vector
of n independent noise samples of variance σ2 = N0/2 each. An optimal decoder for the
AWGN channel computes the posterior probabilities of the transmitted symbols and selects
the symbol which has the maximum such probability. This is the maximum a posteriori
probability (MAP) decoder, and for binary signals it simply computes the log-likelihood
ratio
λ(xr) = log
P(xn = 1|r)
P(xn = 0|r

and makes a decision by comparing this LLR to the threshold zero. As shown in Ap-
pendix A, belief propagation on a graph, even with cycles, can closely approximate the
performance of the MAP algorithm. We now state the decoding algorithm for LDPC codes
on the AWGN channel as a belief propagation algorithm on the graph of the code.
The received signal ri is associated with the variable node i. The algorithm is not
directly working with ri, but with its log-likelihood ratio (LLR), given by
λi = log
 P(xi = 1|ri)
P(xi = −1|ri

= 4
√Es
N0
ri,
(6.5)
which is a scaled version of the received signal itself.

6.3. LDPC DECODING VIA MESSAGE PASSING
257
The algorithm operates by passing messages along the edges of the bipartite graph
of the code, starting with the “raw” bit LLRs λi, and combining these messages at the
nodes by computational functions which generate new, outgoing messages. Speciﬁcally, a
message from variable node i to check node j is represented by μi→j ∈R, and a message
from check node j to variable node i is βj→i ∈R. Let Vj\i be the set of variable nodes
which connect to check node j, excluding variable node i. Similarly, let Ci\j be the set
of check nodes which connect to variable node i, excluding check node j. The decoding
algorithm operates then as follows:
Algorithm for Decoding LDPC Codes on AWGN Channels:
Step 1: Initialize λi = 4 Es
N0 ri for each variable node.
Step 2: Variable nodes send μi→j = λi to each check node j ∈Ci.
Step 3: Check nodes connected to variable node i send
βj→i = 2 tanh−1
⎛
⎝
l∈Vi\j
tanh
λl
2
⎞
⎠.
(6.6)
Step 4: Variable nodes connected to check nodes j send
μi→j =

l∈Ci\j
βl→i.
(6.7)
Step 5: When a ﬁxed number of iterations have been completed
or the estimated codeword ˆx satisﬁes the syndrome constraint
Hˆx = 0, stop. Otherwise return to Step 3.
Figure 6.5 shows simulation results for a regular LDPC code, an irregular LDPC
code, and a turbo code of similar complexity on the AWGN channel [40]. This ﬁgure
demonstrates the power of irregular LDPC codes in terms of performance. We will have
more to say about irregularity in the next section, where the theoretical motivation and
design methodology for irregular codes are discussed. The irregular node degree, however,
can greatly increase the implementation complexity, especially if very high node degrees
are used, and also presents a problem in the error ﬂoor region, discussed later in Chapter 7.
The reader may be tempted to conclude from Figure 6.5 that irregular LDPC codes
are uniformly more powerful than other types of graph-based codes, but, in general, this
is not strictly true. Numerous designs and complexity studies have failed to reveal any

258
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
inherent advantage in one structure over another. More about the actual and perceived
superiority of one type of code over another will be said about in later sections.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
10−6
10−5
10−4
10−3
10−2
10−1
1
Shannon: 0.19dB
Irregular LDPC
Turbo Code
(3,6) Regular LDPC
Eb/N0[dB]
Bit Error Probability (BER)
Figure 6.5: Simulation results for a regular (3,6) LDPC, an optimized irregular LDPC and
a turbo code. All three codes are of rate R = 1/2 and have a block length of n = 106 [40].
The check node computation rule (6.6) is fairly complex. But for quantized messages
it is possible to map LLRs to messages in such a way that the check node rule can be
implemented with only combinational logics and an adder. Such decoders provide very
good performance with only a few bits of precision. Finite-precision node processing and
low-complexity implementations are discussed in Chapter 7. However, if we want to reduce
complexity, one immediate possibility is to use the so-called min-sum approximation, in
which we replace the check node processing rule (6.6) with
βj→i ≈min
l∈Vi\j
(|λl|)

l∈Vi\j
sign (λl).
(6.8)
Some performance loss on the order of 0.5 dB will result from this approximation, but this
loss may be justiﬁed by the overall savings in decoder complexity.

6.4. ANALYSIS TECHNIQUES
259
6.4
Analysis Techniques
We now know how to deﬁne ensembles of LDPC codes and how to decode them via
message passing, but how do we know which parameters to choose and which codes to
pick for good performance? The authors of [40, 41] use an approach they term “density
evolution” to compare the qualities of diﬀerent ensembles of regular and irregular LDPC
codes.
Density evolution has its origin in the error probability evolution formulas of
Gallager [18, 19] discussed in Section 6.4.3, and the modern extension to soft-output
channels are, in essence, a sophisticated generalization of these early methods.
The main statement that density evolution can make about LDPC codes is the follow-
ing: An LDPC code with a given degree distribution pair (λ, ρ), operated on a channel
with noise standard deviation σ has an associated threshold σ∗. The threshold is analogous
to the Shannon capacity in that the error-probability for a randomly chosen (λ(x), ρ(x))-
code used on this channel can be made arbitrarily small for a growing size of the code
if and only if σ < σ∗. One degree-distribution pair is said to be better than another if
its threshold is closer to the channel’s capacity limit, i.e., if it can tolerate a higher noise
standard deviation σ.
Threshold results for several ensembles on various channels are reported in [41]. The
qualities of diﬀerent ensembles of regular and irregular LDPC codes are studied and op-
timized via density evolution in [11, 40], where linear numerical optimization techniques
are used to ﬁnd optimal degree distributions.
One design methodology in designing good codes is to choose the ensemble with the
best threshold, from which we select a code with the largest length which can be accom-
modated in a given implementation. As is typical in random coding arguments, it turns
out that almost all codes in the ensemble perform equally well. Code design may there-
fore consist of randomly sampling a few codes from the ensemble and selecting the best
among those. Actual code selection in practice, however, is strongly determined by the
code’s performance in the error ﬂoor region, where density analysis is not applicable (see
Chapter 7).
6.4.1
(Error) Probability Evolution for Binary Erasure Channels
We ﬁrst illustrate the method of density evolution using the Binary Erasure Channel
(BEC), shown in Figure 6.6.
For regular code ensembles, it is possible to derive the
exact solution for thresholds when a simple discrete message-passing algorithm is used for
decoding [5]. The channel parameter for the BEC is the erasure probability ε.
Let A = {−1, 0, +1} denote the message alphabet, ri ∈A the received symbol at
variable node i, and di ∈A the decision at variable node i. A message from variable node
i to check node j is represented by μi→j ∈A, and a message from check node j to variable
node i is βj→i ∈A. The received signal ri is corrupted by the BEC channel according

260
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
0
1
+1
0
-1
1 −ε
1 −ε
ε
ε
Figure 6.6: Binary erasure channel model.
to random transitions illustrated in Figure 6.6, where the labels on the edges indicate the
probabilities of the edge being traversed, that is, there is a probability  that a single
symbol is erased. A decoding algorithm for the BEC now proceeds as follows [30]:
Algorithm for Decoding LDPC Codes on BEC Channels:
Step 1: Initialize di = ri for each variable node. If ri = 0 then the
received symbol i has been erased and variable i is said to be
unknown.
Step 2: Variable nodes send μi→j = di to each check node j ∈Ci.
Step 3: Check nodes connected to variable node i send βj→i =

l∈Vj\i μl→j to i.
That is, if all incoming messages are dif-
ferent from zero, the check node sends back to i the value that
makes the check consistent, otherwise it sends back a zero for
“unknown”.
Step 4: If the variable i is unknown, and at least one βj→i = 0,
the node sets di = βj→i and declares variable i to be known.
(Known variables will never have to be changed anymore.)
Step 5: When all variables are known, stop. Otherwise go to Step 2.
Under this algorithm, a check node can only output a non-zero message on an edge if
all incoming messages on other edges are non-zero. If there are not enough known variables
when decoding begins, then the algorithm will fail. We will have more to say about the
failure of LDPC codes on BEC channels later.

6.4. ANALYSIS TECHNIQUES
261
Density evolution analysis for the BEC channel amounts to studying the propagation
of erasure probabilities.
A message is called “incorrect” if it is an erasure and has a
numerical value of zero. We are now interested in determining the density of erasures
among variable nodes as the number of iterations l →∞. First, we simplify the message
alphabet to A = {0, 1}, where a zero denotes an erasure and a one means the variable is
known. As given by the decoding algorithm, the output of a check node is known if all
of its incoming messages are known, and the output of a variable node is known if one or
more of its incoming (check-node) messages are known.
Let p(l)
v
be the probability of erasures at the variable nodes at iteration l, and let p(l)
u
be the probability, or “density” of erasures at the check nodes at iteration l. Then for a
regular code, in which the node degrees are all equal, the following recursive relation can
be established:
p(l−1)
u
=
1 −

1 −p(l−1)
v
dc−1
,
p(l)
v
=
p0

p(l−1)
u
dv−1
,
(6.9)
where p0 = ε is the initial probability of an erasure. Equation (6.9) contains an important
assumption, which is that the probabilities of node u or v do not depend on themselves,
that is, there are no cycles in the local subgraph which could introduce such dependencies.
This condition is best illustrated by Figure 6.7. In this local subtree, the nodes at level
l do not appear in any of the lower levels. If this is the case, all probabilities which are
migrated upward from below can be assumed to be independent of those at higher levels.
This is the case, at least approximately, for large LDPC codes whose loops in the code’s
graph are large. Hence the eﬀorts to design LDPC codes with large loops, since otherwise
the dependencies introduced in the message passing will, in general, degrade performance.
Combining the parts in (6.9), we obtain the equation of a one-dimensional iterated
map for the probability that a variable node remains uncorrected as
p(l)
v = F(p(l−1)
v
, p0) = p0

1 −

1 −p(l−1)
v
dc−1dv−1
.
(6.10)
Figure 6.8 shows the evolution of the erasure probability for the ensemble of (3,6)
LDPC codes for a few input erasure values p0 = ε. The staircase trajectory moving left
shows the values p(l)
v
for ε = 0.4 according to Equation (6.9). The solid curves express
the function p(l)
v = f(p(l−1)
v
), computed by combining the formulas in (6.9), and illustrate
the evolution of p(l)
v
over iterations. If f(x, p0) < x, ∀x ≤ then (6.10) will converge to
p(∞)
v
= 0. Evidently, if ε is larger than a certain value, termed the decoding threshold, there
exists a non-zero ﬁx-point F(ε, x) = x of (6.10), and the decoded erasure probabilities no
longer converge to zero. Examples of the convergence function x −F(x, p0) are shown in
Figure 10.9 in Chapter 10, where we discuss spatial coupling of sparse graphs.

262
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
p(l)
v
p(l−1)
u
p(l−1)
v
Level l −1
Level l −2
Figure 6.7: Local subtree structure of an LDPC code that is generated as the connections of
a given variable node are unwrapped, allowing computation of messages through iterations.
For an irregular code with degree-distributions λ(x) and ρ(x), the degree of a particular
node is a random variable. When this is taken into account, the relation (6.9) becomes [29]
p(l−1)
u
=
1 −
dc

i=1
ρi

1 −p(l−1)
v
i−1
= 1 −ρ

1 −p(l−1)
v

,
p(l)
v
=
p0
dv

j=1
λj

p(l−1)
u
j−1
= p0λ

p(l−1)
u

.
(6.11)
(6.11) can be derived by the following simple argument: The probability that a check
node’s output is non-zero is given by
P

β(l−1) = 0

=
dc

i=1
P (i −1 inputs known | #inputs = i) P (#node degree = i)
=
dc

i=1
P

μ(l−1) = 0
i−1
P (#node degree = i),
(6.12)
where we invoked the assumption that all messages are independent and identically dis-
tributed. Because p(l)
u and p(l−1)
u
represent the probability that messages are unknown, the
ﬁrst equation of (6.11) is an immediate consequence.

6.4. ANALYSIS TECHNIQUES
263
ε = 0.5
ε = 0.4
ε = 0.3
ε = 0.2
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
F(p(l)
v , ε)
p(l)
v
Figure 6.8: Graphical illustration of the evolution of the erasure probability p(l)
v .
The same reasoning is used to obtain the second line of (6.11):
P

μ(l−1) = 0

=
dv

j=1
P (j −1 inputs unknown | node deg=j ) P(node deg=j)
=
dv

j=1
P

β(l−1) = 0
j−1
P(node deg = j) .
(6.13)
Finally, we assume that the probability that an edge incident to a check node has degree
i is equal to ρi. Similarly, the probability for an edge incident to a variable node to have
degree j is equal to λj. This gives rise to the polynomial form used in the right-hand side
of (6.11).
We proceed to determine the threshold for a code ensemble with parameters (λ, ρ) on
the BEC, by determining the conditions under which the density of erasures converges to
zero as l →∞. Based on (6.11) and appealing to Figure 6.8, this threshold happens where
the probability evolution function touches the straight line at a non-zero point, i.e., where
F(x, ε) = x. This threshold can be expressed as the supremum [5]
ε∗
=
sup {ε : F(x, ε) < x, ∀x ≤ε},
where F(x, ε)
=
ελ (1 −ρ (1 −x)).
(6.14)

264
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Not that ε∗is simply the largest erasure probability such that for all ε < ε∗, the only
solution to f(ε, x) = x is x = 0. We say colloquially that ”the code converges” in this
case.
We can express this threshold as a minimization operation as below.
Formally,
error-free decoding for a large code is possible if and only if
x = ελ [1 −ρ (1 −x)]
(6.15)
has no positive solutions for 0 < x ≤ε. Since any ﬁxed point solution x corresponds to a
unique non-zero value of ε, we study ε as a function of x:
ε(x) =
x
λ [1 −ρ (1 −x)],
(6.16)
which allows us to rewrite the threshold deﬁnition as
ε∗= min {ε(x) : ε(x) ≥x}.
(6.17)
Note that (6.17) and (6.14) are complementary, in that we approach the boundary from
above in (6.17), and therefore the minimum operator can be used, that is, the condition
describes values of ε for which (6.11) does not converge.
For a regular ensemble (dv, dc), this minimum can be found analytically. Substitute
x = 1 −y and consider the derivative of ε(1 −y), given by
d
dy

1 −y
[1 −ydc−1]dv−1

=
[(dc−1)(dv−1)−1] ydc−1−(dc−1)(dv−1)ydc−2+1 = 0,
(6.18)
which we set to zero. By Descarte’s “Rule of Signs”, the number of positive real roots for
a polynomial cannot exceed the number of sign changes in its coeﬃcients. The polynomial
in (6.18) therefore has no more than two roots. One of those roots is y = 1, the original
ﬁxpoint at x = 0. Dividing (6.18) by (y −1) yields
[(dv −1)(dc −1) −1] ydc−2 −
dc−3

i=0
yi = 0,
(6.19)
and letting s be the (only) positive real root of (6.19) the threshold for the regular LDPC
code ensemble is given by
ε∗=
1 −s
(1 −sdc−1)dv−1 .
(6.20)
As an example, the threshold for the (3, 6) regular ensemble is ε∗= 0.4294 (compare
Figure 6.8). This code has nominal rate R = 0.5. The capacity limit for the BEC at this
rate is the erasure rate εC = 1 −R = 0.5.

6.4. ANALYSIS TECHNIQUES
265
6.4.2
Error Mechanism of LDPCs on BECs
The analysis in the preceding section assumed that the code graph could be unwrapped
into a tree of depth at least equal to the number of iterations we are considering. In this
sense the analysis is asymptotic, since this condition typically requires n →∞as the
number of iterations grows.
Before discussing other channels, we turn our attention to errors in ﬁnite-sized code
ensembles. The knowledge gained in this will be used later in discussing the phenomenon
of the error ﬂoor (discussed in Chapter 7), which is a characteristic of both LDPC and
turbo codes.
Large LDPC codes are very eﬀective at correcting erasures capable of handling erasure
probabilities quite close to the maximum imposed by the Shannon capacity of the BEC
channel. However, in a ﬁnite-sized code, there are speciﬁc erasure patterns that cannot
be corrected. These patterns are related to what are known as stopping sets, [14] deﬁned
as follows:
Deﬁnition 6.3 A stopping set S is a set of variable nodes, all of
whose neighboring check nodes are connected to S at least twice.
Figure 6.9 illustrates a stopping set in our example LDPC code from Figure 6.3
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Black: Stopping Set S
Neighbors of S
Figure 6.9: Stopping set of size six nodes in the example LDPC code.
Note that the union of two stopping sets is again a stopping set, which is straightfor-
ward to see. Therefore, any arbitrary set A of variable nodes contains a unique maximal
stopping set, since if more than one set is contained in A, their union, which is larger, is
also contained in A.

266
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
If a stopping set S is erased by the channel process, the decoding algorithm from
Section 6.4.1 will fail, since in Step 3 of the algorithm, no parity-check node which is a
neighbor of S can ever send out a corrected message, since all neighbors of S see at least
two erasures from S, and therefore all their outputs will be erased. The nodes in S, in turn,
receive messages only from those neighbors, and hence keep receiving erasures irrespective
of the number of iterations performed. From this observation then, we can deduce the
following lemma:
Lemma 2 Erasure decoding will terminate at the unique
maximal stopping set contained in the erased set of variables.
While there exists a statistical analysis of stopping sets for LDPC code ensembles [14],
we will mainly use the concept of the stopping set in our discussion of the error ﬂoor on the
additive white Gaussian noise channel. For speciﬁc codes, the stopping sets will have to
be found via combinatorial search techniques, which then, however, completely determine
the code’s error performance.
6.4.3
Binary Symmetric Channels and the Gallager Algorithms
The BEC is admittedly a somewhat artiﬁcial channel model, mostly popular with in-
formation theorists who are concerned with ﬁnding closed-form solutions and structural
relationships which are harder to ﬁnd for other channel models. A more realistic channel
model is that of the binary symmetric channel (BSC) shown in Figure 6.10, which has two
inputs {0, 1} and two outputs {−1, 1}. A transmission is successful if the input equals
the output, which happens with probability 1 −ε. Conversely, an error is the inversion of
a transmitted bit and happens with probability ε. This BSC has a capacity of 1 −h(ε),
where h(ε) is the binary entropy function [13].
ε
ε
1 + ε
1 −ε
1
1
0
0
Figure 6.10: Binary symmetric channel model.
This model includes the basic BPSK
modulation step 1 →1, 0 →−1.
The following analysis was presented by Gallager [18, 19], but disappeared in obscurity,
like LDPC codes themselves, until their rediscovery. Gallager’s basic algorithm, called
Gallager A, operates as follows:

6.4. ANALYSIS TECHNIQUES
267
Gallager’s LDPC Decoding Algorithm A for BSC Channels:
Step 1: Initialize di = ri for each variable node.
Step 2: Variable nodes send μi→j = di to each check node j ∈Ci.
Step 3: Check nodes connected to variable node i send βj→i =

l∈Vj\i μl→j to i. That is, the check node sends back to i the
value that would make the parity check consistent.
Step 4: At the variable node i if dv/2 or more of the incoming
parity check messages βj→i, disagree with di, change the value
of variable i to its opposite value, i.e., di = di × (−1).
Step 5: Stop when no more variables are changing, or after a ﬁxed
number of iterations have been executed. Otherwise go to Step 2.
After specifying the algorithm, we can apply Gallager’s probability evolution analysis
to study the convergence properties of the code. Assume that the probability of error at
the variable nodes is p(l)
v , where p(0)
v
= ε. A check node will signal a correct check back
if and only if an even number of errors occur in its checked symbols. This happens with
probability1
1 −p(l)
v = 1 + (1 −2p(l)
v )dc−1
2
.
(6.21)
In turn, at the variable nodes, an error will be corrected only if b = dv/2 or more parity
checks are unsatisﬁed, which has probability
p(l)
v
dv−1

t=b
dv −1
t
 
1 + (1 −2p(l)
v )dc−1
2
t 
1 −(1 −2p(l)
v )dc−1
2
dv−1−t
,
(6.22)
where the two power terms simply express the probability of t correct parity checks and
dv −1 −t incorrect checks, respectively.
1Equation (6.21) can be derived by considering the binomial expansions (1 + p)dc−1 and (1 −p)dc−1
and adding them.

268
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
The probability of error in a variable node at the next iteration is now given by
p(l+1)
v
=
p(l)
v −p(l)
v
dv−1

t=b
dv−1
t

1+(1−2p(l)
v )dc−1
2
t
1−(1−2p(l)
v )dc−1
2
dv−1−t
+ (1−p(l)
v )
dv−1

t=b
dv−1
t

1−(1−2p(l)
v )dc−1
2
t
1+(1−2p(l)
v )dc−1
2
dv−1−t
, (6.23)
where the ﬁrst term is the probability that an erroneous variable node setting is corrected,
and the second term is the probability that a correct setting is corrupted by faulty parity-
check information.
The error probability evolution equation (6.23) is illustrated in Figure 6.11 for the
ensemble of (3,6) LDPC codes. We note that it looks similar to the erasure probability
evolution for the BEC channel, and the principle behavior of the decoder is identical.
ε = 0.05
ε = 0.04
ε = 0.03
ε = 0.02
0.01
0.02
0.03
0.04
0.05
0.01
0.02
0.03
0.04
0.05
F(p(l)
v , ε)
p(l)
v
Figure 6.11: Graphical illustration of the evolution of the variable error probability p(l)
v .
From Figure 6.11, or numerical computations, it is evident that the threshold value
for a large (3,6) LDPC code on a BSC is at or near ε∗≈0.04, while the channel capacity
threshold erasure rate for rate R = 0.5 equals ε = 0.11, derived from the BSC capacity
formula, given by CBSC = 1 −h(R). It appears that it would be quite a bit harder to
approach capacity on the BSC with LDPC codes.

6.4. ANALYSIS TECHNIQUES
269
Gallager [18, 19] has ﬁne-tuned his algorithms by allowing a maximization over the
“vote” parameter b in (6.23). This improves the threshold for the rate R = 0.5 LDPC
codes to ε∗= 0.052 using the (4,8) LDPC code ensemble.
Irregular LDPC codes have become popular because they can improve on the perfor-
mance of regular codes. In fact, adding irregularity to the node degrees has been one of
the major constructional contributions to these codes since their inception in the early
1960s. In an irregular code, the “vote” parameter depends on the degree j of the variable
node, i.e., if bj = j/2 incoming check node messages disagree with the variable node, it
is changed to the new value in Step 4 in the decoding algorithm. A check node will signal
a correct check back with probability
1 −p(l)
v =
1 + ρ

1 −2p(l)
v

2
,
(6.24)
which is a straightforward extension of (6.21) treating the node degrees as random accord-
ing to ρ(x).
Analogously, the probability that an erroneous variable is corrected is extended to
p(l)
v
dv

j=1
λj
j−1

t=bj
j −1
t
 ⎛
⎝
1 + ρ

1−2p(l)
v

2
⎞
⎠
t ⎛
⎝
1 −ρ

1−2p(l)
v

2
⎞
⎠
j−1−t



g(t,j)
,
(6.25)
and the new iterated variable node error probability is
p(l+1)
v
= p(l)
v −
dv

j=1
λj
⎡
⎣
j−1

t=bj
j −1
t
 
p(l)
v g(t, j) + (1 −p(l)
v )g(j, t)

⎤
⎦.
(6.26)
The best “vote” parameter can be found to satisfy [29]
1 −ε
ε
≤
⎡
⎣
1 + ρ

1−2p(l)
v

1 −ρ

1−2p(l)
v

⎤
⎦
2bj−j+1
,
(6.27)
where we note that 2bj −j+1 = bj −(j−1−bj) = Δb is the diﬀerence between the number
of check nodes that disagree with the variable node and that of those that agree. Equation
(6.27) therefore states that only this diﬀerence matters and needs to be optimized.
Designing irregular graphs and numerically analyzing their thresholds reveals that
better codes can be found by allowing the node degrees to vary.
In [29], the authors
present the improved irregular codes shown in Table 6.1.

270
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
dv
Code 1
Code 2
Code 3
Code 4
λ3
.123397
.093368
λ4
.555093
.346966
λ5
.496041
.284961
λ6
.173862
.124061
λ16
.321510
λ21
.077225
.159355
λ23
.252871
.400312
λ27
.068844
λ29
.109202
λ30
.119796
λ100
.293135
ρ10
1
ρ14
1
1
ρ22
1
ε∗
.0505
.0533
.0578
.0627
Table 6.1: Some irregular LDPC codes which outperform regular Gallager codes on the
BSC at rate R = 0.5. The capacity threshold is at 0.111.
6.4.4
The AWGN Channel
The additive white Gaussian noise (AWGN) channel discussed in Chapter 2 is probably
the most important representative of practical channels. In this section, we apply the
method of density evolution to the AWGN channel. An “exact” solution for its density
evolution involves several numerical steps and is quite involved. Furtherore, these give
little insight into the incept of the process [41]. A close approximation can be found with
less eﬀort if messages in the decoder are assumed to have a Gaussian distribution. This
is, in general not exact, but the Gaussian approximation greatly reduces the complexity
and produces results very close to the exact solutions [11].
As in the rest of the chapter, we assume that binary transmission is used with unit-
energy symbols from the alphabet {−1, +1}. We can also assume, without loss of gen-
erality, that the transmitted message consists only of +1s corresponding to the all-one
codeword, since the LDPC code is linear, and a digital “0” is mapped into +1 before
transmission.
A correct message bit in the decoded codeword is therefore one with a
positive sign, and an incorrect message is one with a negative sign.
The probability density function of log-likelihood messages at the output of a matched-
ﬁlter receiver is calculated from the AWGN channel equation y = √Esx + n, where
n ∼N(0, N0/2) is a zero-mean Gaussian random variable with variance σ2 = N0/2. We

6.4. ANALYSIS TECHNIQUES
271
consider the log-likelihood ratio of y, given by (6.5) as
λ = log
p(y|1)
p(y| −1) = 4
√Es
N0
y,
(6.5)
whose own PDF is Gaussian with mean mλ = 4Es/N0 and variance 16Es/N0 = 2mλ, i.e.,
f(λ) =

N0
16πEs
exp

−N0
16Es

y −4Es
N0
2
,
(6.28)
as can easily be calculated from the input signal y. This log-likelihood density fulﬁlls what
is known as the symmetry conditions
f(−λ)
=
f(λ)e−λ.
(6.29)
Condition (6.29), also called the consistency condition, implies that the density fλ is
completely determined by its mean. To simplify the analysis we now make the following
Assumption 1 The log-likelihood messages exchanged be-
tween variable and check nodes in a large LDPC code graph
have a consistent Gaussian distribution.
This consistent Gaussian assumption allows us to consider the evolution of a single
parameter, namely the mean of the distribution. Its validity has been quite well established
through extensive simulations, especially at later iterations where suﬃcient mixing has
occurred.
Since the output message is the sum of incoming messages (6.7), and noting that
messages are assumed to be independent due to the local tree structure, we obtain for the
mean of the variable node LLRs under probability propagation in the log domain
m(l)
v = m(0)
v
+ (dv −1)m(l−1)
u
,
(6.30)
where m(l)
v
is the mean of the outgoing message from the variable node at iteration l,
m(l−1)
u
is the mean of the outgoing message from a check node at iteration (l −1), and
m(0)
v
is equal to the mean of f(λ) in (6.28), i.e., m(0)
v
= 4Es/N0.
The density evolution through a check node according to (6.6) is more complicated. To
ﬁnd the evolution of the mean, we assume the messages are independent and identically
distributed (i.i.d.) and, after moving the tanh(·)−1 to the left side, we take expectations
on both sides to obtain
E

tanh
U
2

= E

tanh
V
2
dc−1
.
(6.31)

272
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
To simplify our analysis, we deﬁne a function φ as
φ (mu)
=
1 −E

tanh
U
2

=
1 −
1
4πmu

R
tanh
u
2

exp

−1
4mu
(u −mu)2

du.
(6.32)
where mu > 0. The motivation for deﬁning φ(·) is primarily computational, however
˜u = E

tanh
U
2

= Pr(U = 1) −Pr(U = −1)
(6.33)
can be interpreted as a soft bit estimate of u, since it is the expectation E [u|U], and
therefore φ (mu) is the average estimation error as per (6.32). Note that the soft bit value
is always smaller or equal to unity, and this error is therefore non-negative.
The bulk of the computational diﬃculty occurs in the integral of (6.32), and present
convenient numerical approximations to φ(·) which greatly speed up computations with
no signiﬁcant eﬀects on the accuracy of the results.
The mean of the check node’s output message in a regular LDPC code can now be
calculated from
˜u(l−1)
=

˜v(l−1)dc−1
⇒
φ

m(l−1)
u

=

1 −

1 −φ

m(l−1)
v
dc−1
.
(6.34)
The results (6.30) and (6.34) are suﬃcient to compute thresholds for regular LDPC code
ensembles. A code converges if m(l) →∞as l →∞, or, equivalently, if the soft bit error
φ (mu) →0.
If we deﬁne the estimation error at iteration l as x(l) = φ(m(l)
v ), then (6.34) and (6.30)
together deﬁne an iteration equation for that estimation error, given by
x(l) = φ
4Es
N0
+ (dv −1)φ−1
1 −

1 −x(l−1)dc−1
,
(6.35)
Succinctly, the convergence behavior of regular LPDC codes under iterative message-
passing decoding is governed by the single-parameter convergence equation
x = φ
4Es
N0
+ (dv −1)φ−1(1 −(1 −x))dc−1

= F

x, Es
N0

.
(6.36)
These convergence equations will play an important role again in Chapter 10 on spatial
coupling. The argument x is the average estimation error on a message line leaving a

6.4. ANALYSIS TECHNIQUES
273
variable node and is therefore a measure of the reliability of the LLRs in the decoding
process. x →0 signiﬁes the sign of the code’s LLR all converge to the correct transmitted
binary bit with probability one, that is, almost surely.
The exact computation of φ(·) and φ−1(·) is computationally expensive and slow. The
computation speed can be greatly improved using a few approximations. For small m, the
approximation
φ(m) ≈eαmγ+β,
(6.37)
with α = −.4527, β = .0218, and γ = .86, was proposed in [11]. For larger values of m,
the following upper and lower bounds become tight, so that their average can be used as
a good approximation for φ:
 π
me−m
4

1 −3
m

< φ(m) <
 π
me−m
4

1 −1
7m

.
(6.38)
The ﬁgure below shows a numerical comparison between the approximation
φ(m) ≈
 exp

−0.4527m0.86 + 0.0218

for m < 19.89,
 π
m exp

−m
4
 
1 −11
7m

for m ≥19.89
(6.39)
and (6.32). The diﬀerence is also plotted separately.
actual and approximation
Difference
φ(m)
10−6
10−5
10−4
10−3
10−2
10−1
1
0
10
20
30
40
50 m
(6.32) - (6.39)
Figure 6.12: Exact function φ(m) and its approximation (6.39).
The density evolution for irregular ensembles is found in analogy with the derivation
of (6.11) and the treatise above, but requires some modiﬁcations. We need to treat node
degrees as random variables. The densities of the messages entering a check node are
now Gaussian mixtures, with (6.30) and (6.34) as partial solutions, due to the fact that
diﬀerent degree variable nodes generate these messages.

274
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Speciﬁcally, the mean value of the messages leaving a degree-i variable node is
m(l−1)
v,i
= (i −1)m(l−1)
u
+ m(0)
v .
(6.40)
In turn, the check node output message for a node of degree j obeys
E

tanh
U
2

=
j−1

i=1
E

tanh
Vi
2

(6.41)
due to the independence of messages entering the check node. We now obtain
φ

m(l)
u,j

=
1 −
j−1

i=1
 dv

i=1
λi

1 −φ

m(l−1)
v,i

=
1 −

1 −
dv

i=1
λiφ

(i −1)m(l−1)
u
+ m(0)
v
j−1
,
(6.42)
and we recall that the λi are the probabilities that a variable node has i connecting edges.
To obtain the average check node output signal, we average (6.42) over the check node
degrees to obtain
m(l)
u =
dc

j=1
ρjφ−1
⎛
⎝1 −

1 −
dv

i=1
λiφ

(i −1)m(l−1)
u
+ m(0)
v
j−1⎞
⎠.
(6.43)
This is the recursive formula for the evolution of the check node mean mu. Note that
the check node output message does not have an exact Gaussian distribution. However,
these signals are mixed by the additive variable node which produces a Gaussian dis-
tributed message with higher accuracy, especially if dv is large, and hence working with
(6.43) produces results that are quite accurate.
We now continue with the determination of the thresholds for the AWGN channel,
which will inform us about the potential of LDPC codes with message passing decoding.
From our assumption that the all-zero codeword is transmitted, correct bits and messages
have positive signs. Decoding is error-free if the soft bit error φ

m(l)
v,i

with m(l)
v,i from (6.40)
goes to zero. In this case, only messages with positive sign occur, and the probability of
an incorrect bit goes to zero.
The approximate threshold for the AWGN is the limit of the parameters m(0)
v
= 4Es/N0
for which φ

m(l)
v

→0 as l →∞. We shall be content if the average soft bit error
x(l) =
dv

i=1
λiφ

m(l)
v,i

(6.44)

6.4. ANALYSIS TECHNIQUES
275
goes to zero with the iterations l →∞. Then
m(l)
u =
dc

j=1
ρjφ−1

1 −

1 −x(l−1)j−1
(6.45)
from (6.43), and substituting (6.45) into (6.44) we ﬁnally obtain the iteration equation for
irregular LPCD codes under message passing decoding:
x(l) =
dv

i=1
λiφ
⎡
⎣4Es
N0
+ (i −1)
dc

j=1
ρjφ−1

1 −

1 −x(l−1)j−1⎤
⎦,
(6.46)
where 4Es/N0 is the initial channel LLR. Equation (6.46) deﬁnes the system convergence
function for irregular codes analogous to (6.36), given as
Fi

x, Es
N0

def
=
dv

i=1
λiφ
⎡
⎣4Es
N0
+ (i −1)
dc

j=1
ρjφ−1
1 −[1 −x]j−1
⎤
⎦.
(6.47)
As before, decoding is error-free if the recursion x(l) converges to zero as l →∞. This
condition is satisﬁed if and only if Fi(x, Es/N0) < x for all x ∈(0, φ(4Es/N0)). The code
threshold is now deﬁned as the limiting signal-to-noise ratio
(Es/N0)∗= inf

s ∈R+ : Fi(x, s) −x < 0, ∀x ∈]0, φ(4Es/N0)]

.
(6.48)
Again, avoiding the inﬁmum, we can express (Es/N0)∗as the smallest root of
x −Fi(x, Es/N0) = 0.
(6.49)
Unfortunately, even with the assumption of Gaussian message distributions made in As-
sumption 1, there exists no closed-form solution for (6.49), and we have to evaluate the
thresholds using numerical methods.
Drawing the function Fi(x, Es/N0), we obtain a graphical convergence diagram shown
in Figure 6.13 which is analogous to those for the BEC and BSC models. As the diﬀerence
x −Fi(x, Es/N0) narrows as x →0 a “slow region” or “bottleneck” occurs, and many
iterations may be required to achieve ﬁnal convergence. Naturally this bottleneck becomes
more pronouncd the closer Es/N0 is to the code’s threshold.
The iterated behavior of Fi(x, 4Es/N0) is illustrated in Figure 6.13. The curves shown
are for the dv = 15 degree distribution reported in Table 6.2. The curve for Es/N0 =
−2.52 dB is at the threshold for this degree distribution. We see that Fi(x, Es/N0) →0
as the number of iterations goes to inﬁnity. The “convergence channel” x−Fi(x, Es/N0) is

276
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Es/N0 = −6.02 dB
Es/N0 = −2.52 dB
Es/N0 = −1.25 dB
Es/N0 = 0 dB
Fi(x, Es/N0)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8 x
Figure 6.13: Iterative behavior of the function h(x, Es/N0), illustrating the convergence
of LDPC codes on AWGN channels.
very narrow, especially for small values of Es/N0. This means a large number of iterations
are required to achieve the limiting performance.
Table 6.2 show degree distributions and threshold signal-to-noise ratios for rate R =
1/2 irregular LDPC codes for various maximal variable node degrees. The channel ca-
pacity limit at this rate is at Es/N0 = 0.198, illustrating the excellent, “near-capacity”
performance of these codes. The thresholds are computed according to both the exact [41]
and the Gaussian approximative density evolution [11].
The accuracy of density evolution using the Gaussian approximation follows diﬀerent
trends for regular and irregular codes. For regular codes, the accuracy has been shown to
improve with increasing dv. For irregular codes, the accuracy worsens when the maximum
variable node degree is increased [11]. The approximation is more accurate for regular
codes, but still good for irregular codes with maximal variable node degree dv of about
ten or less.
Another result found in [11] is the concentration theorem for check node degree distri-
butions. A concentrated degree distribution satisﬁes
ρ(x) = ρkxk−1 + (1 −ρk) xk.
(6.50)
A distribution of this form maximizes the rate of convergence under decoding iterations.

6.4. ANALYSIS TECHNIQUES
277
dv
4
8
9
10
11
12
15
20
30
50
λ2
.38354
.30013
.27684
.25105
.23882
.24426
.23802
.21991
.19606
.17120
λ3
.04237
.28395
.28342
.30938
.29515
.25907
.20997
.23328
.24039
.21053
λ4
.57409
.00104
.03261
.01054
.03492
.02058
.00273
λ5
.05510
.12015
λ6
.08543
.00228
λ7
01587
.06540
.05516
.00009
λ8
.41592
.01455
.04767
.16602
.15269
λ9
.43974
.01912
.04088
.09227
λ10
.43853
.01275
.01064
.02802
λ11
.43342
λ12
.40373
λ14
.00480
λ15
.37627
.01206
λ19
.08064
λ20
.22798
λ28
.00221
λ30
.28636
.07212
λ50
.25830
ρ5
.24123
ρ6
.75877
.22919
.01568
ρ7
.77081
.85244
.63676
.43011
.25475
ρ8
.13188
.36324
.56989
.73438
.98013
.64584
.00749
ρ9
.01087
.01987
.34747
.99101
.33620
ρ10
.00399
.00150
.08883
ρ11
.57497
(Es/N0)∗
-2.20
-2.56
-2.60
-2.61
-2.63
-2.64
-2.68
-2.70
-2.74
-2.76
(Es/N0)∗
GA
-2.164
-2.45
-2.50
-2.50
-2.50
-2.52
-2.52
-2.53
-2.55
-2.59
Table 6.2: Exact (Es/N0)∗and approximate (Es/N0)∗
GA thresholds for rate R = 1
2 degree
distributions. Capacity is at Es/N0 = −2.823 dB, or Eb/N0 = 0.188 dB.
Based on the concentration theorem, Chung et al. [9] designed LDPC codes for the
AWGN channel using a discretized density evolution, that is, they quantized the proba-
bility densities and propagated this quantized PDF with accuracies of up to 14 bits. The
degree distributions were then obtained via linear optimization using constraint rules. The
degree distributions of some of the optimized codes are given in Table 6.3.
The check node distribution is given as the single number ρav = (1−ρk)(k−1)+ρkk =
k −1 + ρk With the Shannon capacity at Eb/N0 = 0.188 dB, these codes come to within
0.0247, 0.0147, and 0.0045 dB of the Shannon capacity, respectively. Chung et al. [9] also
provide simulation results for block lengths of n = 107, which are reproduced in Figure
6.14, illustrating the tight predictions of the density evolution method.

278
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
dv
100
200
dv
8000
λ2
.170031
.153425
λ2
.096294
λ3
.160460
.147526
λ3
.095393
λ6
.112837
.041539
λ6
.033599
λ7
.047489
.147551
λ7
.091918
λ10
.011481
λ15
˙031642
λ11
.091537
λ20
.086563
λ18
.047938
λ50
.093896
λ19
.119555
λ70
.006035
λ26
.152978
λ100
.018375
λ27
.036131
λ150
.086919
λ55
.036379
λ400
.089018
λ56
.126714
λ900
.057176
λ100
.217056
λ2000
.085816
λ200
.179373
λ3000
.006163
λ6000
λ6000
.003028
λ8000
λ8000
.118165
ρav = k −1 + ρ
10.9375
12
18.5
(Eb/N0)∗[dB]
0.212
0.194
0.192
Table 6.3: Parameters of near-capacity optimized LDPC codes.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
10−6
10−5
10−4
10−3
10−2
10−1
1
Shannon: 0.19 dB
original turbo code
Eb/N0[dB]
Bit Error Probability (BER)
LDPC thresholds
0.204 and 0.214 dB
turbo code threshold 0.53 dB
Figure 6.14: Simulations of LDPC and the original turbo code for block lengths n = 107.

6.4. ANALYSIS TECHNIQUES
279
A graphical optimization based on EXIT analysis discussed in Chapter 8 was used in
[22] to optimize the convergence behavior of LDPC codes, which yielded the irregular code
with an error cliﬀabout 0.5 dB better than a regular code of the same rate at the same size
of n = 4000. Simulations for both the degree optimized code and a regular (3,6) LDPC
code are shown in Figure 6.15. The code with the error ﬂoor is the irregular design. It is
clearly evident that the irregular code has a lower signal-to-noise ratio threshold. However,
the error ﬂoor of the irregular code is signiﬁcantly higher than the error ﬂoor of the regular
code. Both codes are optimized in avoiding cycles (see next chapter), but the avoidance
of cycles is substantially more successful for the regular code, where all cycles of length
8 and shorter are avoided, while in the irregular code only cycles of length 6 and shorter
could be avoided.
0
0.5
1
1.5
2
2.5
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
Code Threshold at 0.7 dB
Bit/Frame Error Probability (BER)
Eb/N0[dB]
Figure 6.15: Performance results for regular and irregular cycle optimized LDPC codes of
rate R = 1/2 for a block length of 4000 bits. Dashed lines are for the block or frame error
rates, solid lines for the BER.

280
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
This leads into the question of code optimality. While degree optimization can achieve
excellent code thresholds, there is no guarantee that the code will have a low error perfor-
mance for larger signal-to-noise ratios. Indeed, codes with low thresholds seem to be more
diﬃcult to optimize in the error ﬂoor region as illustrated by the simple cycle-optimized
codes in Figure 6.15. The issue of error ﬂoor optimization is picked up and discussed in
more detail in the Chapter 7.
-2
0
-1
1
2
3
4
0.5
0.55
0.6
0.65
0.7
0.75
0.25
0.3
0.35
0.4
0.45
0.8
0.85
0.9
0.95
1
(3,6)
(3,4)
(4,6)
(4,10)
(3,9)
(3,12)
(3,15)
(3,20)
(3,30)
(3,5)
BPSK Capacity Limit
Capacity in bits/dimension
Eb/N0
[dB]
Figure 6.16: Thresholds for regular LPDC codes compared to the BPSK capacity limit.
The issue of regular versus irregular LDPC codes doesn’t stop at the error ﬂoor ques-
tion. As we have observed, for rate R = 1/2 codes, substantial gains are provided by an
irregular code over a regular code, gains in the order of half a dB or more. For lower rates,
even more can be gained, while the diﬀerence between regular and irregular coded per-
formance diminishes as the rate increases. Figure 6.16 illustrates this point by comparing
the thresholds of the best regular LDPC codes to the BPSK Shannon bound. High-rate
regular LDPC codes have thresholds less then 0.5 dB away from the BPSK capacity limit.
This accounts for the fact that in such high-rate applications, regular LDPC codes are fa-
vored, especially since they can be designed with very low error ﬂoors. The (6,32) regular
code with rate R = 0.841 of the IEEE 802.3an standard is a case in point. This code is
discussed in Section 6.5.3.

6.5. CODE FAMILIES AND CONSTRUCTION
281
6.5
Code Families and Construction
6.5.1
Constructions with Permutation Matrices
If we take a closer look at H1 from Section 6.2, we notice that the parity-check matrix
is composed of 18 sub-matrices, each of which is a permutation matrix Pij, that is, the
operation Pijx simply rearranges the elements in x like a permuter. This is illustrated
in Figure 6.17. The advantage of this arrangement is twofold: First, it is easy to see
that these codes are regular by construction.
Second, as will be discussed later, this
construction leads to simpliﬁcations of the implementation of the permuter.
We may now start by ﬁrst identifying the positions of these permutation matrices
within the larger parity-check matrix. This is done by the so-called proto-matrix whose
entries are non-negative integers. For now assume we just have the entries {0, 1} in a
m′, n′ proto-matrix Hp. To obtain the actual parity-check matrix, we expand each “1”
entry into a p × p permutation matrix, creating a m′p × n′p parity-check matrix.
As an example, consider the 3 × 4 proto-matrix Hp,1, which expands into
Hp,1 =
⎡
⎣
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎤
⎦
→H1 =
⎡
⎣
P1,1
P1,2
P1,3
P1,4
P1,5
P1,6
P2,1
P2,2
P2,3
P2,4
P2,5
P2,6
P3,1
P3,2
P3,3
P3,4
P3,5
P3,6
⎤
⎦.
(6.51)
Since the permutation matrices Pij in (6.51) can be of arbitrary size, we can generate
large parity-check matrices from small proto-matrices. In Figure 6.17 their size is 4 × 4,
but in actual applications the size of these permutation matrices is typically much larger.
The example presented in Section 6.5.3 uses permutation matrices of size p = 64.
The proto-matrix contains only the positions of the permutation matrices within the
larger parity-check matrix, but does not specify the actual Pij. In the sense that the per-
mutation matrices can be chosen arbitrarily, Hp,1 speciﬁes a family parity-check matrices.
In the case of (6.51), each position is occupied by a permutation matrix, and therefore
this contracted description may appear trivial. However, as we will see in the sequel, this
is not always the case, and many codes have been constructed using this proto-matrix
approach, or equivalent approaches [10, 16, 36, 53].
The proto-matrix has its own graphical representation, shown in Figure 6.18 and called
a protograph of an LDPC code family. Each edge represents a permuation matrix Pij of
size p × p, and each variable and check node is a set of p stacked nodes. The protograph
in Figure 6.18 corresponds to Hp,1 from above.

282
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
H1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Figure 6.17: Parity-check matrix H1 as a composite of permutation matrices.
+
+
+
Variable Nodes
Check Nodes
Figure 6.18: Bipartite graph for the (3, 4) regular LDPC code with parity-check matrix H1.
Expanding the proto-graph into a code using size-p permutation matrices now requires
replacing each entry hpij in Hp,1 by hpijPij, or, equivalently, expanding the graph in (6.18)
by duplicating it p times, and rearranging the connections from check nodes [ip, (i+1)p−1]
to variable nodes [jp, (j+1)p−1] according to Pij. This is illustrated in Figure 6.19 for H1.
This process is also call graph lifting.
Before we discuss other advantages, we brieﬂy address the impact on the implemen-
tation of LDPC codes. While the permuter network will have to be implemented as a
network of physical wires in high-speed decoders, where each node is implemented as a
separate processor, slower data rates may allow a decoder to share computational resources
among nodes. Speciﬁcally, a permutation matrix based code can be built with a ﬁxed wire

6.5. CODE FAMILIES AND CONSTRUCTION
283
+
+
+
+
+
+
+
+
+
+
+
+
+
Variable Nodes
Check Nodes
Figure 6.19: Expansion of Hp1 into H1 using permutation matrices Pij.
network for only the protograph, while permutation matrices themselves are implemented
in read–write memory blocks. Such a decoder operates in partial parallel mode, processing
blocks of data which correspond to the structure of the proto-matrix. This is illustrated
for the code H1 in Figure 6.20.
+
+
+
+
+
+
+
+
+
1, 2,
· · · ,
p
p + 1, · · · , 2p
2p + 1, · · · , 3p
3p + 1, · · · , 4p
4p + 1, · · · , 5p
5p + 1, · · · , 6p
Input LLR
Input LLR
Input LLR
Input LLR
Input LLR
Input LLR
Figure 6.20: Building block for a ﬁxed interleaver parallel implementation of H1.
A further advantage of this structure is that read–write memory elements can be used
without the danger of access collisions, since each memory block will be accessed by exactly
one read and one write operation per decoding cycle, something that is not quite obvious

284
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
from the Tanner graph of the code in Figure 6.19.
Before leaving this subject, we’ll discuss one more example of code construction via
proto-graphs. It is also an example of a more generalized code structure, which combines
turbo and LDPC code elements. The code in question is the accumulate-repeat-accumulate
(ARA) code of [10] with the deceptively simple proto-matrix
Hp,2 =
 2
1
1
2

,
(6.52)
whose protograph is shown in Figure 6.21. Two points need special explanations. First,
Hp,2 contains entries of 1 and 2, and the proto-graph contains parallel edges. While a
regular (expanded) parity-check code cannot contain parallel edges, we recall that the edges
in the protograph are placeholders for permutation matrices. An entry of “2” therefore
means there are two superimposed permutation matrices that are placed into that position.
After expansion, we need to make sure that the parity-check matrix does not contain
overlapping entries. This is illustrated in the example expansion
H2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⇐= Hp,2 =
 2
1
1
2

.
(6.53)
The second point concerns the diﬀerential encoders at both the input and the output.
The output diﬀerential encoder will be clariﬁed below when we introduce the repeat-
accumulate codes. The input diﬀerential encoder is very similar. Speciﬁcally, the ﬁrst
edge is replaced by an identity matrix, and the parallel edge is replaced by a cyclic shift
matrix, whose size p = 5 representative is given by
P5 =
⎡
⎢⎢⎢⎢⎣
1
1
1
1
1
⎤
⎥⎥⎥⎥⎦
.
(6.54)
Figure 6.22 shows an expansion of the protograph of Figure 6.21 for a small-sized
permutation matrices with p = 5. This illustrates the edge arrangements for the diﬀerential

6.5. CODE FAMILIES AND CONSTRUCTION
285
+
+
+
Interleaver
Input Differential Encoder
Output Differential 
       Encoder
Figure 6.21: Proto-building block with parallel edges for generalized LDPC codes. The
shaded variable node is punctured, i.e., they are not transmitted. It is the accumulation
variable in the input diﬀerential encoder. This is the AR3A encoder of [10].
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Variable and Input Diﬀerential Check Nodes
Diﬀerential Nodes
Interim
Variable Nodes
Check Nodes
Permuter: H2
Figure 6.22: Bipartite graph for the (3, 4) regular LDPC code with parity-check matrix H1.
Shaded nodes are punctured, a rate increase method we will encounter in Chapter 8.
parts of the code. Decoding proceeds following any of the algorithms discussed in this
chapter, realizing that there are several layers of check and parity nodes. The order in
which the messages in and out of these nodes are updated have only a minor impact on
performance as long as all nodes are updated (approximately) once each iteration.
One advantage of codes such as the ARA and RA codes—discussed below—is that
they can be encoded in a simple way as shown in Section 6.6. The repeat-accumulate
codes also form a natural link between turbo and LDPC codes, since they can easily be
interpreted as either type, and the set of diﬀerential nodes can be decoded with a trellis
APP component decoder, as is customary for turbo codes.

286
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
6.5.2
Cycle Reduction Design
As we discussed earlier, cycles in the Tanner graph of the code are cause for concern,
especially short cycles. First, the performance analysis discussed in Section 6.4 relies on
the independence of all messages down the tree. This is only true until the shortest cycle
is completed, at which time previously used messages are mixed into the message ﬂow and
the independence assumption is compromised.
More importantly however, as we will show in Chapter 7, cycles form part of the sub-
graph structures that cause the error rate to remain bounded away from zero even for large
values of the signal-to-noise ratio. Therefore, it was recognized early that one important
design criterion would be to avoid short cycles.
Most code design rules target the elimination of short cycles [24, 48] via a variety of
ad hoc search and construction algorithms. It is widely accepted that the elimination of
4-cycles and perhaps 6-cycles is essential in obtaining good performance, but the inﬂuence
of higher order cycles is more debatable, and we will present a more detailed discussion
of the error ﬂoor phenomenon and the impact of sub-graphs on it in Chapter 7. It seems
reasonable at this point to design the interleavers with cycles as large as possible.
We concentrate on cycle-aware designs for permutation-based LDPC codes. A permu-
tation matrix of size p can be speciﬁed by an integer s together with the equation
w = x + s mod p,
(6.55)
which relates the row index w with the column index x. Note that addressing the diﬀerent
check node indices in Figure 6.20 can be accomplished with a simple counter that is oﬀset
by p −s.
Let us start with a proto-matrix which fulﬁlls certain conditions, for example, let the
proto-matrix be cycle-4 free. We can then guarantee good interleavers by a technique
presented in [36], which recursively constructs the interleaver avoiding that 6 cycles are
created. Assume we have a proto-matrix that is 4-cycle free. That is, the next shortest
cycle that can possibly occur is of length 6. An illustration of this is shown in Figure 6.23
Since the row index and column index are linked by (6.55), we consider the following
diﬀerences: w1 −w6, w4 −w5, w3 −w2, and if
w1 −w6 + w4 −w5 + w3 −w2 = s1,3 −s2,3 + s2,2 −s3,2 + s1,4 −s3,4 mod p = 0 (6.56)
this is an obvious suﬃcient condition for this 6-cycle to not be possible. It is shown in [36]
that it is also a necessary condition. With this concept, [36] builds short-cycle-free LDPC
codes recursively checking and ensuring (6.56) by selecting appropriate oﬀset factors sij
for each populated position in the original proto-matrix. It appears that this method not
only produces large girth interleavers, but has a deceptively simple implementation since
the memory addresses in Figures 6.20 can be generated by simple counters each with an
oﬀset of sij. Note that the interleavers in [44] decompose into exactly the same structure.

6.5. CODE FAMILIES AND CONSTRUCTION
287
1
1
1
1
1
1
s1,3
s2,2
s2,3
s3,2
s3,4
s1,4
1
s1,3
1
s2,3
1
s2,2
1
s3,2
1
s3,4
1
s1,4
(x4, w5)
(x4, w4)
(x2, w3)
(x2, w2)
(x1, w1)
(x1, w6)
Figure 6.23: Illustration of a 6-cycle in an (LDPC) parity-check or interleaver matrix.
Many other approaches have been explored in the literature to construct large-cycle
codes, many of them relying, at least in part, on computational search techniques, such
as the progressive-edge growth algorithm in [24].
6.5.3
RS-based Construction
In this section, we discuss a speciﬁc example of a class of codes designed both from
permutation matrices and for short cycle avoidance. These codes are the Reed–Solomon
code-based LDPC code constructions from [16]. We chose this construction since it was
used for the LDPC code in 10 Gbit/s IEEE 803.2 Ethernet standard. This code is one of
the most cited and studied LDPC code, and we will return to it in the next chapter when
we discuss the error ﬂoor.
We use symbols from the Galois ﬁeld GF(ps), where p is a prime number and s is
positive integer. For details on Galois ﬁelds and their application to coding theory see
[27]. If α is a primitive element of GF(ps), then the powers 1 = α0, α1, · · · , αps−2 generate
all the elements. For ρ > 0, the Reed–Solomon code of length ps and minimum distance
ρ −1 is generated by the generator polynomial
g(X)
=
(X −α)(X −α2) · · · (X −αρ−2)
=
g0 + g1X + g2X2 + · · · + Xρ−2.
(6.57)
This code can be shortened to obtain a (ρ, minimum distance = 2, ρ −1) RS code with
generator matrix given by
GRS =
 g0
g1
g2
· · ·
1
0
0
g0
g1
g2
· · ·
1

.
(6.58)

288
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
This base code will now be used to construct 4-cycle free LDPC codes in the following
way. First note that since ρ −1 is the minimum symbol distance of (6.58), two codewords
in the code generated by GRS have at most one common symbol in same location.
We now take a codeword b = (b1, b2, . . . , bρ) from this code and replace each component
bj by its location vector z(bj), which is a binary vector of length ps −1 which has a single
one in position i, and i is the exponent of the symbol bj = αi expressed as a power of the
primitive generator element α. The element 0 = α∞may be placed at the beginning or
end of this expansion. Note that z(bj) has length ps, and we construct a length ρps binary
vector as
z(b) = (z(b1), z(b2), . . . , z(bρ)),
(6.59)
which has ρ non-zero components which equal “1” by construction. Since two codewords
of the RS code have at most one common symbol in the same location, their location
vectors have at most one non-zero component in common.
Now take a codeword b from the RS code which has weight ρ, then the codewords βb
with β ∈GF(ps) forms a subcode with minimum distance ρ. Each coset of this subcode
has the same properties of consisting of ps sequences with minimum distance ρ also. The
symbol location vectors for two codewords both taken from any of these coset therefore
have no non-zero position in common. These binary symbol location vectors are the basis
for the construction of our LDPC code parity-check matrices.
First choose any coset of the above RS subcode as the generating set. Let this be the
ith coset, and note that the ordering of cosets does not matter. We ﬁrst form the matrix
Ai over GF(2) whose rows consist of the ps symbol location vectors of the generating set.
The dimensions of Ai are ps × ρps, and it contains a total of ρps non-zero entries. Ai
has constant row weight of ρ, and column weight of 1. In fact, Ai consists of a row of ρ
permutation matrices of size ps ×ps, since each symbol bj “cycles” through all ps elements
of GF(2s), i.e.,
Ai = [Pi1, Pi2, . . . , Piρ] ,
Pij is 2s × 2s.
(6.60)
The reader may notice that we now constructed a block row of a permutation matrix-based
parity-check matrix as in Figure 6.17. All that remains is to add more block rows.
This is done by adding matrices Aj from other cosets to form the parity-check matrix
HRS =
⎡
⎢⎣
A1
...
Aγ
⎤
⎥⎦=
⎡
⎢⎣
P11
P12
· · ·
P1ρ
...
Pγ1
Pγ2
· · ·
Pγρ
⎤
⎥⎦.
(6.61)
Note that no two rows in Ai have any non-zero components in the same location, and no
two rows in diﬀerent matrices Ai and Aj have more than one non-zero component in the
same location. The Tanner graph of HRS can therefore have no 4 cycles, which can be
easily seen by applying the reasoning in Figure 6.23 to this case.

6.5. CODE FAMILIES AND CONSTRUCTION
289
HRS describes a (γ, ρ) regular LDPC code, and it can be shown that its minimum
distance is at least γ + 1, which is, however, typically a loose bound [16]. The (6,32), rate
R = 0.841 (2048,1723) RS-based code is generated from GF(26) with ρ = 32, and γ = 6 is
the code used in the IEEE 802.3 standard. Its minimum distance is at least dmin ≥7, but
its actual dmin = 14. We will return to this code in the next chapter. Other construction
examples can be found in [16].
6.5.4
Repeat-Accumulate Codes
The near-capacity performance of low density parity-check codes and the elegance of the
graphical description of the codes and the decoding algorithm led to the investigation of
other graph-based coding schemes. In this section, we consider a particularly interesting
variant of LDPC codes known as repeat-accumulate (RA) codes [15]. These codes are
important for several reasons, but a key point is that RA codes can also be viewed as a
special type of serial turbo codes (8). This is actually the more common way of treating
them. They therefore serve to illustrate the close relationship between these two classes
of codes, and, indeed, of all graph-based error control codes. As turbo codes they are the
serial concatenation of two very simple component codes. Their relationship to LDPC
codes allows them to beneﬁt from the notion of irregularity and to be analyzed using
density evolution.
Figure 6.24 shows a block diagram of a non-systematic repeat accumulate encoder in
serial concatenated form. The outer encoder is a simple repetition code with rate R = 1/q
and the inner encoder is a rate one recursive encoder with feedback polynomical 1+D, that
is, an accumulator. The inner and outer encoders are separated by the interleaver, and the
overall code rate is R = 1/q. Despite their apparent simplicity, these codes perform very
well, and for large block lengths and rates R = 1/q ≤1/3, they have thresholds within 1
dB of the Shannon limit of the AWGN channel [15]. Indeed, in Chapter 8 we will present
modiﬁed versions of the RA codes which come to within 0.1 dB of capacity.
+
R = 1/q
x
xGRA
Repitition Code
Π
1
1 + D
r
Accumulator
Figure 6.24: Block diagram of a repeat-accumulate code encoder.
The LDPC bipartite graphical representation of the RA code of Figure 6.24 is straight-
forward to ﬁnd and is best illustrated by an example. Let q equal 3 and and the check node

290
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Variable Nodes
r
dv = q
Parity Nodes
Parity-Check
Nodes
Permuter: H2
Figure 6.25: Tanner graph of an RA code with q = 3 and check node degree dc = r + 2.
parameter r be 2, also called the check node left degree; then the graph of the resulting
(LDPC) code is shown in Figure 6.25. This code has rate R = r/(r +q). From this graph,
it is clear that each variable node has degree dv = q = 3 and that each parity node has
degree 2 due to the accumulator, except for the rightmost parity variable node. Thus,
the graph of an RA code is a regular bipartite graph with degree-q information variable
nodes, degree-r + 2 check nodes, and degree-2 parity nodes.
It is evident that RA codes can also be made irregular, this was ﬁrst suggested in [25].
That is, the number of times that an information bit is repeated is varied. The resulting
Tanner graph is depicted in Figure 6.26 and is parameterized by the degree distribution
(λ1, λ2, . . . , λJ) of the information variable nodes and the left degree r of the check nodes.
Note that in the graph of an regular RA code, contrary to what is shown in Figure 6.25,
the left check node degree is typically r = 1.
In [25], the authors considered the subclass of systematic irregular RA codes with a
ﬁxed check node left degree of r. In a systematic RA code the graph is used to compute
the parity bits, which are then simply appended to the information bits to form the
codeword is x = (u1, u2, . . . , uk, x1, x2, . . . , xr).
Using linear programming techniques,
they obtained degree sequences for systematic irregular RA codes and then computed
exact and approximate thresholds. The results for codes of rate R ≈1/3 are shown in
Table 6.4. These codes are very potent and come to within approximately 0.1 dB of the
capacity limit.
RA codes form an important link between LDPC and turbo code approaches, unifying
these apparent diﬀerent methods. They are also representatives of (systematic) serially
concatenated codes, a somewhat neglected coding strategy which, however, in no way lags

6.6. ENCODING OF LDPC CODES
291
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
λ1
λ2
λJ
r
Parity Nodes
Parity-Check
Nodes
Permuter: H2
Figure 6.26: Tanner graph of an irregular RA code.
a
2
3
4
λ2
.139025
.078194
.054485
λ3
.222155
.128085
.104315
λ5
.160813
λ6
.638820
.036178
.126755
λ10
.229816
λ11
.016484
λ12
.108828
λ13
.487902
λ27
.450302
λ28
.017842
Rate
0.333364
0.333223
0.333218
(Eb/N0)∗[dB]
0.191
-0.250
-0.368
(Eb/N0) [dB]
0.293
-0.117
-0.255
Capacity (dB)
-0.4953
-0.4958
-0.4958
Table 6.4: Degree proﬁles for optimized systematic irregular repeat accumulate codes [25].
behind the turbo and LDPC codes in terms of performance or implementation eﬃciency.
6.6
Encoding of LDPC Codes
The decoding operations discussed in this chapter are all very eﬃcient in terms of com-
putational complexity, that is, their complexity grows only linear with the length n of the

292
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
code, and the number of iterations i. This is the minimal complexity with a decoding
eﬀort per bit roughly constant and independent of the size of the code, proportional only
to the number of iterations needed.
At ﬁrst glance, however, the complexity of encoding an LDPC code may be more
complex. First, we need to ﬁnd the encoding matrix, such that GHT = 0. While ﬁnding
G is quite straightforward [27], G is, in general, not sparse, and the number of non-zero
entries in G grows as n2. With it, the encoding operation uT G requires O(n2) operations,
which may be very signiﬁcant for large n.
In this remainder of this chapter we now discuss three approaches to reduce this ap-
parent encoding complexity to algorithms of order O(n). All of these use triangularization
of H in one way or other.
6.6.1
Triangular LDPC Codes
One way around this problem is to enforce a triangular structure of the parity-check
matrix and see if LDPC codes with good performance can still be constructed. Due to
the correlations enforced by the structural constraint, a random performance analysis is
not strictly possible anymore. However, constructing LDPC codes with triangular parity-
check matrices does produce codes with equivalent performance to those with completely
random parity-check matrices. This is illustrated in Figure 6.27 for a large code design
with n = 36, 993. As can be seen from the BER performance, there is little diﬀerence in
performance between a triangular and a completely random code.
If Hp is lower triangular, then the BEC decoding algorithm from Section 6.4.1 can
eﬀectively be used to perform iterative encoding in at most m = N −K steps. Consider
the following simple example:
H =
⎡
⎣
0
1
1
1
1
0
0
1
0
0
1
1
1
0
0
0
1
1
0
1
1
⎤
⎦.
(6.62)
The constraint graph for this matrix is shown in Figure 6.28.
The information bits at the top of the graph are initialized with ±1 and the parity
nodes with 0. When the BEC decoding algorithm is applied, the ﬁrst round of message
passing will cause parity-bit 1 to become known. In the second round, a non-zero message
from parity-bit 1 allows parity-bit 2 to become known. Similarly, parity bit 3 becomes
known in the third iteration. Encoding therefore takes place in m = 3 iterations, in fact,
successively going through each row constraint in the parity-check matrix. In other words,
the set of parity bit variables is never a stopping set.
The authors of [20] found that encoding can also be performed by means of iterated
approximation.
Suppose a codeword has the structure x = [xu xp], where xu is the

6.6. ENCODING OF LDPC CODES
293
information sequence and xp is the parity sequence. We can then split H into [Hu | Hp],
giving the equation
HpxT
p = HuxT
u ,
(6.63)
which we need to solve HpxT
p = HuxT
u for xT
p . To do so, we ﬁrst select an initial guess for
xp, which we’ll denote by i0. We will then use an iterative method to obtain subsequent
approximations, denoted xk. These approximations will converge to xp using the following
iteration:
xT
i+1 = (Hp + I) xT
i + HuxT
u ,
(6.64)
provided Hp is non-singular. Correct encoding results after i ≥i iterations if and only if
(Hp + I)i = 0. To apply the algorithm, we can split the code’s graph into two parts as
shown in Figure 6.28: an information-bit section and a parity-bit section. The arrows in
the graph indicate which edges will be enabled to transmit edges from check nodes in the
encoding algorithm.
The top half of the graph computes the bits in b. The bottom part only represents
the constraint in Hp, which is an m × m matrix, so the number of parity nodes (labeled
vi) is the same as the number of check nodes (labeled ci) for this part of the graph. The
encoding algorithm can then be implemented with the following algorithm:
Step 1: Set all vi = +1. Initialize information nodes to ±1 as appropriate, and propagate
messages to yield bi.
Step 2: Variable nodes send μij = bi to all j ∈Ci \ i.
Step 3: Check nodes answer with βjj = 
l∈Vj\j μlj, sent to vj only.
Step 4: Variable nodes update vi = βii. Return to Step 2 and repeat for the prescribed
number of iterations.
This result allows us to construct a code which is guaranteed to be graphically encodable
in a ﬁxed number of iterations.
In [20] a method is presented for constructing “reversible” LDPC codes, which can be
encoded in two iterations. We ﬁrst note that (Hp ⊕I)2 = I →H2
p = I. A reversible code
may then be constructed by selecting a 2 × 2 matrix A0 for which A2
0 = I. Hp can be
obtained by recursive application of one of the following rules:
Ak =
 Ak−1
I
0
Ak−1

or
Ak =
 Ak−1
0
0
Ak−1

.
(6.65)

294
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Not lower triangular
Lower triangular
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit/Frame Error Probability (BER)
Eb/N0[dB]
Figure 6.27: Performance results for a triangular and a non-triangular (random) LDPC
codes of rate R = 1/3 for a block length of 36,993 bits.
As an example, we can construct the matrix
A0 =
 1
1
0
1

⇒Hp =
⎡
⎢⎢⎣
1
1
1
0
0
1
0
1
0
0
1
1
0
0
0
1
⎤
⎥⎥⎦
(6.66)
and concatenate it with
Hu =
⎡
⎢⎢⎣
1
0
0
1
1
1
0
0
1
0
1
0
0
1
1
1
⎤
⎥⎥⎦.
(6.67)
The resulting code is an [8,4] extended Hamming code, whose graph is shown in Fig-
ure 6.29. This graph can be used for decoding as well as encoding. It may also be used to
encode with the earlier BEC algorithm.

6.6. ENCODING OF LDPC CODES
295
+
+
+
1
2
3
+
+
+
1
2
3
+
+
+
Parity Bits
Information Bits
Parity Bits
Information Bits
b1
c1
b2
c2
b3
c3
Figure 6.28: Constraint graph for the matrix in (6.62) on the left-hand side, and splitting
the code into two parts to apply iterative encoding on the right.
+
+
+
1
2
3
+
+
+
+
4
+
Parity Bits
Information Bits
b1
c1
b2
c2
b3
c3
b4
c4
Figure 6.29: Graph of the [8,4] extended Hamming code.
6.6.2
Specialized LDPC Codes
Most special LDPC codes which are easily encodable rely on a triangular structure as
well. In this section we will discuss only the RA codes discussed Section 6.5.4. Linear
complexity encoding is already obvious if we refer to the code block diagram in Figure
6.24 instead of its Tanner graph.
If we draw out the parity-check matrix of the RA code H2 from Figure 6.25, we note
that the parity section of H2 is diagonal with a single lower diagonal, which allows linear-
time encoding via the BEC decoding algorithm, or simple back-substitution.

296
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
H2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(6.68)
All other codes based on this same kind of construction, including the ARA code discussed
in Section 6.5.1, are equally encodable with linear complexity, which is best seen from their
turbo code viewpoint.
6.6.3
Approximate Triangularization
In general, when we try to ﬁnd a generator matrix for an LDPC code via the relation
GHT = 0, G will no longer be sparse. Again, suppose a codeword has the structure
x = [xu xp], where xu is the information sequence and xp is the parity sequence, and split
H into [Hu | Hp], giving the equation
HpxT
p = HuxT
u ;
(6.69)
⇒xT
p = H−1
p HuxT
u .
(6.70)
Note also that Hu and Hp are square matrices. There are a few methods available to make
this computation eﬃcient. Instead of forcing the parity-check matrix to be triangular as
in the previous cases, [42] suggest placing the matrix in “approximate lower triangular”
form in which the upper right corner is populated with 0 as shown in Figure 6.30.
A given parity-check matrix H may be placed into this form using only row and column
permutations. The distance g is referred to as the “gap.” The gap can be thought of as a
measure of the distance from H to a true lower-triangular matrix. We then split xp into

6.6. ENCODING OF LDPC CODES
297
n −m
g
n
m
m −g
g = “gap”
A
B
C
D
E
T
0
Figure 6.30: Conversion of the parity-check matrix of an LDPC code into approximate
lower triangular form.
[p1, p2], where p1 has length g and p2 has length m −g, and multiply H from the left by

I
0
−ET−1
I,

,
(6.71)
giving

A
B
T
C −ET−1 A
D −ET−1B
0

.
(6.72)
The parity-check rule then gives the following equations
AxT
u + BpT
1 + TpT
2
=
0,
(C −ET−1A)xT
u + (D −ET−1B)pT
1
=
0.
(6.73)
Now deﬁne P = D −ET−1B, and assume P is non-singular, and we arrive at
pT
1
=
P−1 
C −ET−1A

xT
u ,
pT
2
=
−T−1 
AxT
u + BpT
1

.
(6.74)
A judicious arrangement of these computations gives a sequence of complexity O(n). Only
the computation P−1 
CxT
u −ET−1AxT
u

has complexity O(g2), because P−1 is a dense
g × g matrix. The authors of [42] also show that, for suﬃciently large n, g ≤O(√n)
with high probability for a randomly chosen LDPC code. This algorithm therefore also
provides encoding with linear complexity in the codelength n. In fact, this method can be
used, for example, with the RS-based (2048,1723) code in Section 6.5.3 giving a matrix P
of size only 2 × 2.

298
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Appendix 6.A: Factor Graphs
Factor Graphs: Introduction and History
Factor graphs are graphical representations of complex functions, that is, representations
of how variables of a global function are grouped together locally. These graphical methods
have arisen as a description of the structure of probabilistic networks, where one of their
primary purpose is to provide a simple visualization of probabilistic dependencies.
The second purpose of factor graphs is to describe the network of connections among
variables by edges between nodes, along which probabilities can be exchanged locally with
the purpose of calculating global probabilistic functions. In that sense, the factor graph
represents a blueprint for a computational machine that evaluates the global function via
local processing.
Factor graphs are bipartite graphs; that is, their nodes fall into just two groups such
that there are no connections inside each group, but only between nodes in diﬀerent groups.
In the case of factor graphs, these groups are variable nodes and function nodes.
In 1981, Tanner [45, 46] deﬁned block and convolutional error-correcting codes in
terms of bipartite graphs with codeword bit nodes and constraint or subcode nodes, which
often represent parity-check constraints. These graphs were used as an eﬀective model for
code description, construction, and decoding. Tanner also presented the fundamentals of
iterative decoding on graphs.
Wiberg et al. [50] introduced analogous graphs for trellis codes by adding hidden state
variables to Tanner’s graphs. This addition extended the use of bipartite graphs to trellises
and turbo codes. Wiberg’s work [51] established such bipartite graphs, which he termed
“Tanner graphs,” as a model applicable to a wide variety of decoding algorithms, including
the Viterbi [27] or min-sum algorithm, and the BCJR [3], forward–backward [4], or APP
algorithm, discussed in Chapter 7.
The artiﬁcial intelligence community also developed graphical methods of solving prob-
abilistic inference problems, termed probability propagation. Pearl [38] presented an algo-
rithm in 1986 under the name Belief Propagation in Bayesian Networks for use in acyclic
or cycle-free graphs and applied it to models of human reasoning. An independent but
equivalent algorithm was developed by Lauritzen and Spiegelhalter [35] in 1988.
Belief or probability propagation has application in any ﬁeld where probabilistic in-
ference is used, hence also in error control decoding.
There the probabilistic value of
transmitted information symbols needs to be inferred from noisy received symbols. We
have encountered probability propagation under the name APP decoding in Chapter 7.
Information theorists noticed this connection and have shown that the turbo decoding al-
gorithm [6], which is based on the APP algorithm, is an application of belief propagation
to a graph with cycles [37].
In [26], Kschischang et al. introduced factor graph formalism, showing that belief prop-

6.6. ENCODING OF LDPC CODES
299
agation and many algorithms used in digital communications and signal processing are all
representations of a more general message-passing algorithm, the sum-product algorithm,
operating on factor graphs. This algorithm computes marginals of a global probabilistic
function in terms of local functions. The factor graph of a code is a visual expression
of this factorization into local probabilistic functions. Aji and McEliece [1] present an
equivalent but alternative formulation with their generalized distributive law (GDL). We
will discuss the sum–product algorithm in Section 6.6.3 after examining graphical models
for probabilistic inference.
Graphical Function Representation
Figure 6.31 shows the Bayesian network and the factor graph for a simple four-state
Markov chain, that is, for the probability P(A, B, C, D) = P(A)P(B|A)P(C|B)P(D|C).
The nodes represent the variables A, B, C, and D, and the edges between the nodes in-
dicate probabilistic, or functional, dependencies.
In a Bayesian network, probabilistic
conditioning is indicated by the direction of arrows between variable nodes, while the
factor graph takes a more general point of view: A small node, called a function node,
represents the function whose arguments are the variables connected to it. In Figure 6.31
these functions are P(A), P(B|A), P(C|B), and P(D|C) for our Markov chain example,
but in general the functions do not have to be conditional probabilities. We will see that
this more general formulation will be very useful in the description of the factor graphs of
error control codes.
Bayesian
Network:
A
B
C
D
Factor
Graph:
P(A)
A
P(B|A)
B
P(C|B)
C
P(D|C)
D
Figure 6.31: Graphical models for a four-state Markov chain.
Both graphical methods visually express global functions in terms of local functions.
Edges between nodes indicate all dependencies; there are no hidden dependencies. Belief
propagation, as originally introduced by Pearl [38, 39] uses the Bayesian network repre-
sentation. Graphical decoding via sum-product decoding [45, 50], functionally equivalent
to belief propagation, utilizes factor graphs or their bipartite precursors. Both represen-

300
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
tations are equivalent; however, the factor graph is more general.
Factor graphs form the computational matrix for evaluating global functions through
the communication of local messages. In order to see how this works, let us return to the
example of the Markov chain in Figure 6.31. We wish to calculate the marginal probability
distribution of D, given by
P(D)
=

A

B

C
P(A, B, C, D)
=

A

B

C
P(A)P(B|A)P(C|B)P(D|C),
(6.75)
whose “brute-force” evaluation requires nA×nB ×nC ×nD summations, where nA, nB, nC,
and nD are, respectively, the numbers of values A, B, C, and D can assume. The factor-
ization in (6.75) allows us to execute the summations locally:
P(D) =

C
P(D|C)

B
P(C|B)

A
P(B|A)P(A)



P(B)



P(C)
.
(6.76)
This local factored evaluation has only nAnB +nBnC +nCnD summations. These savings
are, of course, due to factorization in (6.75). It is obviously computationally more eﬃcient
to work with local distributions.
With this, we now wish to calculate the conditional probability P(C|D = d). Node
D plays the role of an observed value in our Markov chain, and the conditional, or a
posteriori, probability of C is the quantity of interest. Again, using basic probability, we
calculate
P(C|D = d) =

A,B P(A, B, C, D = d)
P(D = d)
= K × P(C, D = d),
(6.77)
where we need to sum out the intermediate variables A and B. We now factor the global
joint distribution into local distributions and obtain
P(C, D = d)
=

A
P(A)

B
P(B|A)P(C|B)P(D = d|C)
=
P(D = d|C)



μZ→C

B
P(C|B)

A
P(B|A) P(A)
  
μW →A



μX→B



μY →C
.
(6.78)
As illustrated in Figure 6.32, Equation (6.78) can be visualized in the factor graph of
the Markov chain as passing messages between nodes. The messages μfunction→variable are

6.6. ENCODING OF LDPC CODES
301
passed from function nodes to variable nodes. For our simple Markov chain example, the
variable nodes simply copy the message from the incoming edge to the outgoing edge.
At node C, the incoming messages are multiplied to form P(C, D = d) = BEL(C) =
μY →C μZ→C. In an analogous manner, any joint probability can be found by calculating
forward (a priori) and backward (a posteriori) messages and multiplying them to produce
the belief (BEL) of a variable node.
Note that BEL(C) is a vector whose size depends on the alphabet size of C. So for
binary C, BEL(C) = [BEL(C = −1, D = d), BEL(C = 1, D = d)], for C ∈{−1, 1}. Each
of the incoming messages is also a vector, or a list, of probability values, whose length
equals the cardinality or size of the message alphabet of the receiving variable node. Thus
μX→B = P(B) = [P(B = −1), P(B = 1)] for B ∈{−1, 1}.
node W
P(A)
A
μW→A
node X
P(B|A)
B
μX→B
node Y
P(C|B)
C
μY →C
node Z
P(D|C)
D
μZ→C
Figure 6.32: Illustration of message passing in a four-state Markov chain.
At each function node, the incoming message is multiplied by that function and a
summation over the variable on the incoming side is performed to produce the outgoing
marginalized message. For example, at node X the summation over A generates
μX→B =

A
P(B|A)μW→A.
(6.79)
No summation at W is performed, since there is no incoming message. Likewise, at node
Z no summation is performed because node D is observed, and its value is ﬁxed to D = d.
In the Markov chain, the variable nodes simply pass messages from the input to the
output and back to the input, as they only have two connecting edges per node. One edge
serves as pathway for the incoming message, while the other edge provides a path for the
outgoing message. The function nodes multiply the incoming message with their function
and sum over the variable sending the incoming message.
In a more general network, where the variable or function nodes may have multiple
incoming edges (more than two connecting edges to a node), the incoming messages are
multiplied at the function node to generate the outgoing messages. We discuss this general
algorithm in the next section.

302
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
The Sum–Product Algorithm
The sum-product algorithm implements probability calculations as message-passing in a
factor graph. It operates on local distributions, computing results at each node and passing
messages on to connecting nodes. In a general network, there are two types of messages:
1.
Function-to-Variable: At the function nodes, incoming variable-to-function mes-
sages are multiplied with that function and summed over the variables whose nodes
send them, i.e., the node variable is marginalized out to produce the outgoing mes-
sages.
For example, in Figure 6.32, the message passed from node X to node B is
P(B) =

A
P(B|A)μW→A.
2. Variable-to-Function Messages: The messages sent from the function nodes are in
the form of a priori probability or a posteriori likelihood lists. An example of the former
is μW→A = P(A), and that of the latter is μZ→C = P(D = d|C). At a variable node,
incoming messages are multiplied to generate the outgoing messages.
For an acyclic graph, i.e., for a tree graph which contains no cycles, all nodes need
to send and receive messages only once, after which the marginalized probabilities of all
variables can be calculated by multiplying all incoming messages at that variable node
to form BEL(node)= μfunction→variable. We will show later that BEL(node) is the exact
marginalized probability for acyclic graphs, while for cyclic graphs it is only approximate
as the algorithm becomes iterative along loops. Furthermore, the order of the message
passing, called the schedule, is not important for a tree graph.
Let us now study the more complex factor graph of the [8,4] extended Hamming code.
This code has the systematic parity-check matrix [32]
H[8,4] =
⎡
⎢⎢⎣
1
1
1
0
1
0
0
0
1
1
0
1
0
1
0
0
1
0
1
1
0
0
1
0
0
1
1
1
0
0
0
1
⎤
⎥⎥⎦.
(6.80)
The code has 4 information bits, which we associate with variable nodes U1, . . . , U4, and 4
parity bits which are associated with the variable nodes X5, . . . , X8. There are also 8 ob-
served output symbols, nodes Y1, . . . , Y8, which are the noisy received variables U1, . . . , X8.
The function nodes V1, . . . , V4 are the four parity-check equations described by (6.80), given
as boolean truth functions. For example, the function at node V1 is U1 ⊕U2 ⊕U3 ⊕X5 = 0
and corresponds to the ﬁrst row of H[8,4]. The complete factor graph of the parity-check
matrix representation of this code is shown below in Figure 6.33.
This network has loops, e.g., U1−V1−U2−V2−U1, and thus the sum–product algorithm
has no well-deﬁned forward–backward schedule, but many possible schedules of passing
messages between the nodes. A sensible message passing schedule will involve all nodes

6.6. ENCODING OF LDPC CODES
303
U1
U2
U3
U4
P(U1)
P(U2)
P(U3)
P(U4)
Y1
Y2
Y3
Y4
V1
V2
V3
V4
X5
X6
X7
X8
Y5
Y6
Y7
Y8
P(Y4|U4)
P(Y3|U3)
P(Y2|U2)
P(Y1|U1)
P(Y8|X8)
P(Y7|X7)
P(Y6|X6)
P(Y5|X5)
Figure 6.33: [8,4] Hamming code factor graph representation.
which are not observation nodes in a single sweep, called an iteration. Such iterations
can then be repeated until a desired result is achieved. The messages are passed exactly
according to the rules of the sum-product algorithm. More precisely, iterative probability
propagation proceeds according to the following sum–product algorithm:
Step 1: Initialize the observation nodes Yk to the observed values Yk = yk.
Step 2: Send the likelihood messages μYk→Uk = P(Yk = yk|Uk) from Yk to Uk and
μYj→Xj = P(Yj = yj|Xj) from Yj to Xj. For an AWGN channel, P(Yk = yk|Uk =
u) =
1
√N0 exp(−(yk −u)2/ N0).
Step 3: Send the a priori probability messages μUk = P(Uk) to the nodes Uk. If no a
priori information is available, assume the uniform distribution P(Uk = 1) = P(Uk =
−1) = 0.5.
Step 4: Send the variable to function messages μXj→Vj = P(Yj = yj|Xj) from the vari-
able nodes Xj to the function nodes Vj.
Steps 1–4 are executed only once at the beginning of the decoding process and “stored”
permanently on the edges. When required, the sent messages are reused during subse-
quent iterations of the inner nodes. During decoding, the evaluation of the a posteriori
probabilities of the nodes Uk now proceeds iteratively via the following steps which involve
only the inner nodes in the network, as shown in Figure 6.34:

304
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
U1
U2
U3
U4
X5
X6
X7
X8
V1
V2
V3
V4
Figure 6.34: Inner nodes participating in the iterative part of the decoding process.
Step 5: Send the messages μUk→Vj from the nodes Uk to the nodes Vj with which they
are connected. These messages are given by
μUk→Vj = μUkμYk→Uk

i=j
μVi→Uk.
(6.81)
In the ﬁrst iteration, no messages from the inner function nodes have been received
yet, and μUk→Vj = μUkμYk→Uk.
Step 6: For each outgoing edge of Vj, multiply the incoming messages and sum over all
variables which sent messages to Vj. Send the results as messages μVj→Uk from Vj
to Uk and Xk. No messages need to be sent back to the nodes Xj as long as we are
not interested in their belief probabilities.
Step 7: Calculate the belief of the variables Uk as the product
BEL(Uk)
=
μUkμYk→Uk

j
μVj→Uk = P(Uk, Y1, . . . , Y8)
∝
P(Uk|Y1, . . . , Y8).
Step 8: End the iterations or go to Step 5.
The iterations are ended by choosing a
stopping criterion, such as reaching a desired accuracy  BEL(Uk,i)−BEL(Uk,i−1) ≤
, ∀Uk, or a maximum number of iterations i = imax.

6.6. ENCODING OF LDPC CODES
305
The Factor Graph of Trellises
In this section we forge the connection between factor graphs and the trellis representations
of codes discussed earlier in this book. Not only will this lead us to more eﬃcient factor
graphs in the sense of decoding eﬃciency, but it will also establish the equivalence between
the APP algorithm discussed in Chapter 5 (Section 5.7) and the sum-product algorithm.
This will establish factor graphs as a general and convenient high-level description of trellis
codes.
Figure 6.35 shows the trellis of the code used in Chapter 5 to discuss the APP algo-
rithm. In the light of factor graph terminology, we now realize that the states S1, . . . , S8
are random variables, and we represent these random variables by variable nodes as shown
in the equivalent factor graph representation 6.36. These state nodes are no longer bi-
nary random variables as was the case in the factor graph of the [8,4] Hamming code,
but have cardinalities corresponding to the size of the state space. The function nodes Vr
now express the trellis constraints relating the previous state Sr, the new state Sr+1
and the transmitted symbol(s) Xr.
The function Vr encapsulates the two functions
Xr = τ(Ur, Sr), Sr+1 = σ(Ur, Sr). Note that the information bit Ur is implicit in the
trellis representation, determining the upper or lower path emanating from each state.
Also, the received symbols Yr are not shown in the trellis representation. It becomes ev-
ident that the factor graph is a higher level view of the trellis code, the details of which
are hidden in the function nodes Vj.
S1
S2
S3
S4
S5
S6
S7
S8
Figure 6.35: Example trellis of the short trellis code from Chapter 5.
We now establish the equivalence of the APP algorithm with the sum–product algo-
rithm. The ﬁrst three steps in the latter, according to Page 303 are preparatory in nature
and identical to the APP algorithm, if a forward–backward update schedule is used. That
is, updating starts at the state variable node S1 and proceeds along the chain to state
node S8 and then back again. The peripheral nodes Xr and Ur send and receive messages
as required by this schedule.

306
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
S1
S2
S3
S4
S5
S6
S7
S8
V1
V2
V3
V4
V5
V6
V7
X1
X2
X3
X4
X5
X6
X7
U1
U2
U3
U4
U5
Y1
Y2
Y3
Y4
Y5
Y6
Y7
Figure 6.36: Factor graph of the trellis code in Figure 6.35.
Step 4 in the sum–product algorithm generates the messages
μUr→Vr
=
P(Ur = ur),
(6.82)
μXr→Vr
=
P(Yr = yr|Xr),
(6.83)
and
μSr→Vr
=
P(Sr, Y1, . . . , Yr−1)) = [αr(0), . . . , αr(S −1)],
(6.84)
from the variable nodes Ur, Xr, and Sr, respectively, to the function node Vr. All of these
messages are vectors. For example, for a rate 1/2 binary convolutional code, (6.82) has
two entries for ur = {0, 1}, (6.83) has four entries for {00, 01, 10, 11}, and (6.84) has S
entries, an α-value for each of the S states. The function node Vr contains the probabilistic
function P(Sr+1, Xr|Sr, Ur) relating all connecting variable nodes.
At Step 5 in the sum–product algorithm, the incoming “forward” messages are marginal-
ized at the function node Vr over the sending node variables, multiplied, and forwarded
to the next state variable node Sr+1 as
αr+1
=
[αr(0), . . . , αr(S −1)]
=

Ur,Sr
μSr→Vr μXr→Vr μUr→Vr P(Sr+1|Sr, Ur, Xr),
(6.85)
where
αr+1(j) =

i
αr(i)

Ur
P(Yr = yr|Xr)P(Ur)P(Sr+1 = j, Xr|Sr = i, Ur).
(6.86)
In the general case, there would also be a sum over xr, but due to the simple trellis, Ur
and Sr uniquely determine Xr. Carrying out the sum over Ur in the above equation, we

6.6. ENCODING OF LDPC CODES
307
obtain the forward recursion for α, equivalent to (5.28):
αr+1(j) =

i
αr(i)γr+1(i, j),
(6.87)
where γr+1(i, j) = P(Sr+1 = j, Yr = yr|Sr = i) as for APP decoding.
Figure 6.37 illustrates the processing that takes place at function node Vj.
Sr
αr
αr+1
μUr→Vr
μXr→Vr
Sr+1
Vr
Ur
Xr
Figure 6.37: Function node processing during the “forward” phase.
Analogously, on the backward sweep of algorithm, the incoming messages are marginal-
ized, multiplied, and forwarded to the previous state variable Sr as
βr = [βr(0), . . . , βr(S −1)],
(6.88)
where
βr(j) =

i
βr+1(i)

Ur
P(Yr = yr|Xr)P(Ur)P(Sr+1, Xr|Sr, Ur).
(6.89)
As before, we can replace the last terms with γr+1 to get
βr(j) =

i
βr+1(i)γr+1(j, i).
(6.90)
This is the backward recursion for β developed in Chapter 5 for APP decoding (5.29).
According to Step 7, the belief or conditional probability of state Sr is calculated as
the product of the incoming messages:
P(Sr = i, Y1 = y1, . . . , YL = yL) = BEL(Sr = i) = αr(i)βr(i).
(6.91)
From the derivation in Chapter 5 for (5.27), we know that this conditional probability is
exact. Beliefs, i.e., conditional probabilities, of the other variables Ur and Xr are calculated
completely analogously, resulting in (5.44) and (5.45).

308
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
Exactness of the Sum–Product Algorithm for Trees
As we have seen, the sum–product algorithm for a trellis network is equivalent to the
APP algorithm of Section 5.7, and thus exact. That is, it generates the exact a posteriori
probabilities of the hidden variables. We are now going to show that this is true for any
network whose factor graph can be drawn as a tree.
Figure 6.38 shows an edge in a general factor graph, connecting a variable node X
with a function V . The function node V is a function of the variables X, Z1, . . . , ZN, and
the variable X is an argument in the functions V, W1, . . . , WI. The nodes Zn send the
messages μZn→V , which represent variable probabilities, in the form of lists to the function
node V . At the function node, these incoming messages are multiplied componentwise,
marginalized, and forwarded to the variable node X as
μV →X =

Z1,...,ZN
fV (X, Z1, · · · , ZN)
N

n=1
μZn→V .
(6.92)
From the right-hand side, the function nodes Wi send the messages μWi→X to the
function node X. These messages can be thought of as conditional probabilities P(X|Wi),
where Wi represents all the variables “below” the function node Wi. At node X, these
incoming messages are multiplied componentwise and forwarded to the function node V
as
μX→V =
I
i=1
μWi→X.
(6.93)
Note that both μV →X and μX→V are LX-valued message lists, where LX is the number
of values the random variable X can assume. Finally, the belief of X is calculated as
BEL(X) = μV →X
I
i=1
μWi→X = μV →XμX→V ,
(6.94)
where the product is again performed componentwise.
We will now show that the belief calculated in (6.94) is the exact joint probability
P(X, observation) if the network’s factor graph is a tree, where observation is the set of
observed random variables. The exact a posteriori probability P(X|observation) is easily
derived from the joint probability through normalization.
Without loss of generality,
assume that the node X of interest is moved to the “top” of the inverted tree as shown in
Figure 6.39. Furthermore, all observed nodes Yk are located at the bottom. Subnetworks
below the observed nodes can be ignored, since the observed nodes isolate these lower
parts, that is, if P(QY |S) = P(Q|Y )P(Y |S), then observation of Y = y leads to P(Q|Y =
y)P(Y = y|S) and Q is irrelevant for S and can be ignored.

6.6. ENCODING OF LDPC CODES
309
μV →X
V
Z1
Zn
ZN
X
μX→V
W1
Wi
WI
Figure 6.38: General edge in a factor graph.
In the ﬁrst step of the sum–product algorithm, the observed nodes Yk send P(Yk =
y|Sl) to their immediate higher variables nodes Sl.
The message that Sl forwards is,
according to (6.93), the likelihood 
k P(Yk = yk|Sl) = P(observation|Sl). Sl is in turn
connected to the next higher variable node, denoted by Tr, through the function node
P(Sl|Tr) which marginalizes the messages of all connected nodes Sl and generates

S1,··· ,SL
P(S1|Tr)P(observation|S1) · · · P(SL|Tr)P(observation|SL)
=
P(observation|Tr).
(6.95)
The T-level is now formally identical to the S-level, and, for a tree, this process can be
continued recursively up the branches to the function node Wi, which passes the message

r

Tr P(observation, Tr|X) to node X.
On the other side of the tree, the a priori side, the process starts with the free nodes
U1, . . . UL who pass a priori information to node Rh, which forwards the message

U1,··· ,UL
P(U1)P(Rh|U1) · · · P(UL)P(Rh|UL) = P(Rh)
(6.96)
to the function node P(Zn|Rh), which marginalizes out all connected Rh. Again, in a
tree this process continues up to function node Vj which passes the message μVj→X =

Z1···ZN P(X|Z1, . . . , ZN) 
n P(Zn) to node X. Node X in turn multiplies the incoming
messages and calculates
BEL(X) =

i
μWi→X

j
μVj→X,
(6.97)

310
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
BEL(X)
=

i
R

r=1

Tr
P(observation, Tr|X)
×

j

Z1
· · ·

ZN
P(X|Z1, · · · , ZN)
N

n=1
P(Zn)
=

i

j
R

r=1

Tr

Z1
· · ·

ZN
P(observation, Tr, Z1, . . . , ZN, X)
=
P(X, observation),
which, after normalization, yields the desired conditional probability.
Note that we are not restricted to having observation nodes only on one side of the
tree. The sum–product algorithm also works exactly with observation nodes on the a
priori side of the tree in addition to the observed nodes Yk. Consider a set of observations
R1 = r1, R2 = r2 at the Rh level, which we call observation O. The remaining Rh nodes
are not observed. The message forwarded to node Zn becomes

R3···RH
P(Zn|R1 = r1)P(Zn|R2 = r2)P(Zn|R3)P(R3) · · · P(Zn|RH)P(RH)
= P(Zn|R1 = r1, R2 = r2) = P(Zn|O),
(6.98)
and the conditioning on observation O simply proceeds up to node X.
Sl
Q1
Q2
Y1
. . .
YK
X
T1
..
Tr..
TR
Z1
..
Zn
..
ZR
Rh
W1
..
Wi
V1
..
Vj
U1
. . .
P(U1)
UL
P(UL)
Figure 6.39: Structure of a tree factor graph.

6.6. ENCODING OF LDPC CODES
311
The sum–product algorithm does not produce exact a posteriori probabilities for loopy
networks, i.e., networks with cycles. Exact probability propagation in a loopy network is
known to be NP-complete [12].
Nonetheless, the sum–product algorithm is used suc-
cessfully in loopy networks to perform error control decoding as shown earlier. This has
proven particularly successful in decoding turbo codes. The algorithm has also been ap-
plied successfully to interference cancellation and multiple access joint detection, both
usually described by loopy networks.
The passing of messages can occur in any arbitrary order. This does have an eﬀect
on the result, but very little is currently known about how to choose an eﬃcient update
schedule. In a tree, of course, the order of the updates is irrelevant. The convergence
behavior of this algorithm in loopy networks is also little understood and the topic of
much current research interest.
(U1, U2, U3, U4)
P(U1, U2, U3, U4)
Y1
Y2
Y3
Y4
V1
V2
V3
V4
X5
X6
X7
X8
Y5
Y6
Y7
Y8
P(Y4|U4)
P(Y3|U3)
P(Y2|U2)
P(Y1|U1)
P(Y8|X8)
P(Y7|X7)
P(Y6|X6)
P(Y5|X5)
Figure 6.40: [8,4] Hamming code factor graph representation as a tree.
Note that any given system has a large number of possible factor graphs, and one can
always generate a tree graph for any system through node contraction. That is, nodes
which form a cycle are simply lumped together and treated as a single node.
This is
illustrated for the factor graph of the [8,4] Hamming code in Figure 6.40, where we have
contracted the four information nodes into a single 16-valued node. Of course, decoding
now becomes more diﬃcult, not easier as it might seem, since passing messages through
the large node is in eﬀect the same as an exhaustive search over all 16 codewords, i.e.,

312
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
more complex than the simple binary factor graph of Figure 6.33.
If the variables are binary, i.e., two-valued with X ∈[0, 1], we can simplify the messages.
Instead of communicating the probabilities [p0, p1], we may use p0 + p1 = 1 to simplify
our representation into a single parameter. A commonly used representation is the log-
likelihood ratio (LLR) λ = ln(p0/p1). The messages change as follows when using LLR
representations.
LLR Variable Node Messages:
Referring to Figure 6.38, variable node X receives LLR messages from the function nodes
Wi in the form
μWi→X = ln (p0/p1) = λWi.
For two nodes, the probability that X = 0 equals
p0q0
p0q0+p1q1 , where p0, p1 are the proba-
bilities of W1, and q0, q1 those of W2. The fraction expresses the conditional probability
P(X) = 0, given that both incoming messages refer to the same variable X. However, the
message format is now λX = ln (P(X) = 0/P(X) = 1), and hence
P(X) = 0
=
P(X = 0|OB,C) =
p0q0
p0q0 + p1q1
,
P(X) = 1
=
P(X = 1|OB,C) =
p1q1
p0q0 + p1q1
,
λX
=
ln
p0q0
p1q1

= ln (p0/p1) + ln (q0/q1).
(6.99)
The outgoing message from node X to function node V is now
μX→V = ln(x0/x1) = ln (p0/p1) + ln(q0/q1) = μW1→X + μW2→X,
(6.100)
i.e., the LLR messages are simply added. This can easily be extended to more then two
incoming messages, and we obtain the summation of LLR rule used in (6.7).
LLR Check Node Messages:
Assume now the function at node V represent a parity check of all incoming message.
Node V receives LLR messages from variable nodes Z1 and Z2 of the form
μZ1→V = ln(p0/p1) = λz1,
μZ2→V = ln(q0/q1) = λz2.
(6.101)
The node enforces the parity-check constraint f(x, z1, z2) = [X ⊕Z1 ⊕Z2 = 0], and we
calculate that z0/z1 = p0q0+p1q1
p0q1+p1q0 , and the outgoing LLR message from check node V to
variable X is
μV →X = ln(x0/x1) = ln
p0q0 + p1q1
p0q1 + p1q0

.
(6.102)

6.6. ENCODING OF LDPC CODES
313
However, we want to be able to express the outgoing LLR message in terms of the incoming
LLR messages λz1 and λz2. This requires some trigonometric manipulations. Noting that
p0/p1 = eλz1 and q0/q1 = eλz2 and dividing both x0 and x1 by p1q1, we obtain
μV →X = ln
eλz1eλz2 + 1
eλz1 + eλz2

.
(6.103)
Factoring out e
λz1
2 e
λz2
2
from numerator and denominator gives
μV →X = ln
⎛
⎝
cosh
 λz1+λz2
2

cosh
 λz1−λz2
2

⎞
⎠.
(6.104)
Using a hyperbolic expansion for cosh(x±y) and dividing numerator and denominator by
the common term cosh
 λz1
2

cosh
 λz2
2

we obtain
μV →X = ln
⎛
⎝
1 + tanh
 λz1
2

tanh
 λz2
2

1 −tanh
 λz1
2

tanh
 λz2
2

⎞
⎠,
(6.105)
as used in [8]. With the relation tanh−1(z) = 1
2 ln

1+z
1−z

, we further obtain
μV →X = 2 tanh−1

tanh
λz1
2

tanh
λz2
2

.
(6.106)
Again, this derivation can be extended to multiple input messages and we obtain the check
node message function (6.6).
The check node LLR message can be approximated by using ln (cosh(x)) ≈|x| −ln(2)
for x  1. Thus,
μV →X ≈

λz1 + λz2
2
 −

λz1 −λz2
2
 = sgn(λz1) sgn(λz2) min(|λz1|, |λz2|) ,
(6.107)
the approximation discussed in (6.8).
To summarize: For variable nodes the outgoing LLR message is simply the sum of all
incoming LLR messages. If variable node X has degree i, the message it sends to function
node V will be
μX→V =
i

j=1
(j̸=V )
λj,
(6.108)

314
CHAPTER 6. LOW-DENSITY PARITY-CHECK CODES
For parity-check nodes the incoming LLR messages multiply together through the tanh
transformation. Check node V of degree i sends to variable node X the message
μV →X = 2 tanh−1
⎛
⎜
⎝
i
j=1
(j̸=X)
tanh
λj
2

⎞
⎟
⎠.
(6.109)
Binary factor graphs thus allow for a signiﬁcant simpliﬁcation of the computations required
at the nodes. Nonetheless, note that the outgoing messages are true LLR values given
the incoming messages; that is, if the incoming messages are exact, so are the outgoing
messages. In this sense the operations (6.108) and (6.109) are locally optimal.
It is interesting to muse that the APP algorithm used in iterative turbo decoding
can itself be viewed as an instance of belief propagation between nodes which represent
individual trellis sections. The analogy between LDPC and turbo codes can further be
strengthened using Forney’s normal graphs [17], which leads to the comparison illustrated
in Figure 6.41, showing the factor graphs for LDPC codes and standard turbo codes. Both
codes (and most others) can be represented as graph structures joined by a randomly
selected permutation Π. This graphical representation also exposes the turbo decoding
algorithm as an instance of message-passing decoding on code graphs.







LDPC Code Graph
Π
Π
Turbo Code Graph
Figure 6.41: Factor graphs for LDPC and turbo codes. The nodes in the turbo code graph
are the trellis sections of the constituent component codes–see Chapter 8.

Bibliography
[1] S.M. Aji and R.J. McEliece, “The generalized distributive law,” IEEE Trans. In-
form. Theory, pp. 325–343, March 2000.
[2] A. Anastasopoulos, “A comparison between the sum-product and the min-sum itera-
tive detection algorithms based on density evolution,” IEEE Global Telecommunica-
tions Conference 2001, GLOBECOM ’01, Vol. 2, pp. 1021–1025, Nov. 25–29, 2001.
[3] L.R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear codes for
minimizing symbol error rate,” IEEE Trans. Inform. Theory, pp. 284–287, March
1974.
[4] L.E. Baum and T. Petrie, “Statistical inference for probabilistic functions of ﬁnite-
state Markov chains,” Ann. Math. Stat., vol. 37, pp. 1559–1563, 1966.
[5] L. Bazzi, T.J. Richardson, and R. Urbanke, “Exact thresholds and optimal codes for
the binary symmetric channel and Gallager’s decoding algorithm A,” IEEE Trans. In-
form. Theory, vol. 50, no. 9, pp. 2010–2021, 2004.
[6] C. Berrou and A. Glavieux, “Near optimum error correcting coding and decoding:
Turbo-codes,” IEEE Trans. Commun., vol. 44, no. 10, pp. 1261–1271, Oct. 1996.
[7] A.J. Blanksby and C.J. Howland, “A 690-mW 1-Gb/s 1024-b, rate-1/2 low-density
parity-check code decoder,” IEEE Journal Solid-State Circuits, vol. 37, no. 2, pp.
404–412, March 2002.
[8] G. Battail, M.C. Decouvelaere and P. Godlewski, “Replication decoding,” IEEE
Trans. Inform. Theory, vol. IT-25, pp. 332–345, May 1979.
[9] S. Y. Chung, G. D. Forney, T. J. Richardson and R. Urbanke, “On the design of
low-density parity-check codes within 0.0045 dB of the Shannon limit,” IEEE Com-
mun. Lett., vol. COMM-5, no. 2, pp. 58–60, Feb. 2001.
[10] D. Divsalar, S. Dolinar, J. Thorpe, and C. Jones, “Constructing LDPC codes from
simple loop-free encoding modules,” IEEE J. Select. Areas Commun., vol. 27, no. 6,
pp. 876–888, 2009.
[11] S.-Y. Chung, T.J. Richardson, and R. Urbanke, “Analysis of sum-product decoding
of low-density parity-check codes using a Gaussian approximation,” IEEE Trans. In-
form. Theory, pp. 657–670, Feb. 2001.
315

316
BIBLIOGRAPHY
[12] G.F. Cooper, “The computational complexity of probabilistic inference using Bayesian
belief networks,” Artiﬁcial Intell., vol. 42, pp. 393–405, 1990.
[13] T.M. Cover and J.A. Thomas, Elements of Information Theory, Wiley Series in
Telecommunications, John Wiley & Sons, New York, 1991.
[14] C. Di, D. Proietti, E. Telatar, T. Richardson, and R. Urbanke, “Finite length anal-
ysis of low-density parity-check codes on the binary erasure channel,” IEEE Trans.
Inform. Theory, vol. 48, pp. 1570–1579, June 2002.
[15] D. Divsalar, H. Jin and R. J. McEliece, “Coding theorems for ‘turbo-like’ codes,”
Proceedings of the 1998 Allerton Conference, pp. 928–936, Oct. 1998.
[16] I. Djurdjevic, J. Xu, K. Abdel-Ghaﬀar, and S. Lin, “A class of low-density parity-check
codes constructed based on Reed–Solomon codes with two information symbols,”
IEEE Commun. Lett., vol. 7, no. 7, pp. 317–319, 2003.
[17] G.D. Forney, “Codes on graphs: normal realizations,” IEEE Trans. Inform. Theory,
pp. 520–548, Feb. 2001.
[18] R. G. Gallager, “Low-density parity-check codes,” IRE Trans. on Inform. Theory,
pp. 21–28, vol. 8, No. 1, Jan. 1962.
[19] R.G. Gallager, Low-Density Parity-Check Codes, MIT Press, Cambridge, MA,1963.
[20] D. Haley, A. Grant, and J. Buetefuer, “Iterative encoding of low-density parity-check
codes,” Proc. IEEE Globecom 2002, Oct. 2002.
[21] D. Haley, C. Winstead, A. Grant, V. Gaudet, and C. Schlegel, “An analog/digital
mode-switching LDPC Codec,” Proceedings of the ISCAS 2005, Kobe, Japan.
[22] S. Howard, S. Zeinoddin, C. Schlegel, “EXIT Analysis to optimize irregular low-
maximum-variable-degree LDPCs of rate 0.5,” HDCD internal report, July 2004.
[23] S. Howard, R. Hang, V. Gaudet, C. Schlegel, S. Bates, “Degree-matched check node
approximation to sum–product decoding of LDPCs,” Proc. 2005 IEEE International
Symposium on Information Theory, ISIT 2005, Adelaide, Australia, Sept. 4–9, 2005.
[24] X-Y. Hu, E. Eleftheriou, and D.M. Arnold, “Regular and irregular progressive edge-
growth tanner graphs,” IEEE Trans. Information Theory, vol. 51, no. 1, pp. 386–398,
2005.
[25] J. Jin, A. Khandekar and R. J. McEliece, “Irregular repeat-accumulate codes,”
Proceedings of the Second International Conference on Turbo Codes, pp. 125-127,
Sept. 2000.
[26] F.R. Kschischang, B.J. Frey, and H.-A. ˙Loeliger, “Factor graphs and the sum–product
algorithm,” IEEE Trans. Inform. Theory, Feb. 2001.
[27] S. Lin and D. Costello, Jr., Error Control Coding, 2nd edition, Prentice Hall, Engle-
wood Cliﬀs, 2004.
[28] M. Luby, M. Mitzenmacher, A. Shokrollahi, and D. Spielman, “Improved low-density
parity-check codes using irregular graphs,” IEEE Trans. Inform. Theory, vol. 47, no.
2, pp. 585–598, 2001.

BIBLIOGRAPHY
317
[29] M. Luby, M. Mitzenmacher, A. Shokrollahi, and D. Spielman, “Analysis of low density
codes and improved designs using irregular graphs,” Proceedings 30th ACM (Associ-
ation for Computing Machinery) STOC (Symposium on Theory of Computing), May
23–26, 1998.
[30] M. Luby, M. Mitzenmacher, A. Shokrollahi, D. Spielman, and V. Stemann, “Practical
loss-resilient codes,” Proc. 29th Annual ACM Symp. Theory of Computing, pp. 150–
159, 1997.
[31] D.J.C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge
University Press, ISBN 0-521-64298-1, 2003.
[32] F.J. MacWilliams and N.J.A. Sloane, The Theory of Error-Correcting Codes, Vol. 16,
North-Holland Mathematical Library, North-Holland, Amsterdam,1988.
[33] D. J. C. MacKay and R. M. Neal, “Near Shannon limit performance of low density
parity check codes,” IEE Electron. Lett., vol. 32, no. 18, pp. 1645–1646, Aug. 1996.
[34] D. J. C. MacKay, “Good error-correcting codes based on very sparse matrices,” IEEE
Trans. Inform. Theory, vol IT-45, no. 2, pp. 399–431, March 1999.
[35] S.L. Lauritzen and D.J. Spiegelhalter, “Local computations with probabilities on
graphical structures and their application to expert systems,” J. Royal Stat. Soc. B,
vol. 50, pp. 157–224, 1988.
[36] J. Lu and J.M.F. Moura, “Partition-and-shift LDPC codes,” IEEE Trans. Magnetics,
vol. 41, no. 10, pp. 2977–2979, Oct. 2005.
[37] R.J. McEliece, D.J.C. MacKay and J.-F. Cheng, “Turbo decoding as an instance of
Pearl’s ’belief propagation’ algorithm,” IEEE J. Select. Areas Commun., vol. SAC-16,
pp. 140–152, Feb. 1998.
[38] J. Pearl, “Fusion, propagation, and structuring in belief networks,” Artiﬁcial Intell.,
vol. 29, pp. 241–288, 1986.
[39] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer-
ence, Morgan Kaufmann Publishers, Burlington, MA 1988.
[40] T.J. Richardson, M. Shokrollahi, and R. Urbanke, “Design of capacity-approaching
irregular low-density parity-check codes,” IEEE Trans. Inform. Theory, pp. 619–637,
Feb. 2001.
[41] T.J. Richardson and R. Urbanke, “The capacity of low-density parity-check codes un-
der message-passing decoding,” IEEE Trans. Inform. Theory, pp. 599–618, Feb. 2001.
[42] T.J. Richardson and R. Urbanke, “Eﬃcient encoding of low-density parity-check
codes,” IEEE Trans. Inform. Theory, pp. 638–656, Feb. 2001.
[43] C. Schlegel and L. Perez, Trellis and Turbo Coding, IEEE/Wiley 2004, Hoboken, NJ,
ISBN 0-471-22755-2.
[44] J. Sun and O. Takeshita, “Interleavers for turbo codes using permuation polynomials
over integer rings,” IEEE Trans. Inform. Theory, vol. 51, no. 1, Jan. 2005.
[45] R. M. Tanner, “A recursive approach to low complexity codes,” IEEE Trans. Inform.
Theory, vol. 74, no. 2, pp. 533–547, Sep. 1981.

318
BIBLIOGRAPHY
[46] R.M. Tanner, Patent No. 4,295,218, “Error-correcting coding system,” Oct. 13, 1981.
[47] S. ten Brink, “Convergence behavior of iteratively decoded parallel concatenated
codes,” IEEE Trans. Commun., vol. 49, no. 10, pp. 1627–1737, Oct. 2001.
[48] T. Tian, C.R. Jones, J.D. Villasenor, and R.D. Wesel, “Selective avoidance of cycles
in irregular LDPC code construction,” IEEE Trans. Commun., vol. 52, no. 8, pp.
1242–1247, Aug. 2004.
[49] N. Wiberg, H.-A. Loeliger, and R. K¨otter, “Codes and iterative decoding on general
graphs,” Eur. Trans. Telecommun., pp. 513–525, Sept./Oct. 1995.
[50] N. Wiberg, H.-A. Loeliger and R. Koetter, “Codes and iterative decoding on general
graphs,” Eur. Trans. Telecommun., vol. 6, pp. 513–525, Sept./Oct. 1995.
[51] N. Wiberg, Codes and Decoding on General Graphs, PhD thesis, Link¨oping University,
Link¨oping, Sweden, 1996.
[52] M. Yang, W. E. Ryan, and Y. Li, “Design of eﬃciently encodable moderate-length
high-rate irregular LDPC codes,” IEEE Trans. Commun., vol. 54, no. 2, pp. 564–571,
April 2004.
[53] J. Xu, L. Chen, L. Zheng, L. Lan, and S. Lin, “Construction of low-density parity-
check codes by superposition,” IEEE Trans. Commun., vol. 53, no. 2, pp. 243–251,
Feb. 2005.
[54] H. Tang, J. Xu, and S. Lin, “Codes on ﬁnite geometries,” IEEE Trans. Inform. The-
ory, vol. 51, no. 2, pp. 572–596, Feb. 2005.

Chapter 7
Error Floors
7.1
The Error Floor Problem
As observed with the original turbo codes, LDPC codes, primarily those which are con-
structed randomly, suﬀer from an error ﬂoor. The error ﬂoor is the region of SNR where
the slope of the bit error curves exhibits to relatively shallow grade as a function of the
SNR. For large codes, the steep bit-error rate (BER) function near the code’s threshold
quite abruptly changes to ﬂat behavior. For moderate to large-sized LDPC codes, this
error ﬂoor typically sets in around bit error rates of Pb = 10−5 or 10−6; see Figure 7.1. In
the error ﬂoor region an increase of SNR has only moderate eﬀect in lowering the BER,
and for many practical purposes, this onset of the ﬂoor therefore also signiﬁes the error
correction limit of the code. There are many low-BER applications, however, that require
very low frame error rates, and this error ﬂoor is a nuisance and must be controlled.
In this chapter we discuss analytical and experimental approaches to understand and
control the error ﬂoor.
While the causes of the error ﬂoor between LDPC and turbo
codes are somewhat diﬀerent, the mechanisms to lower it both invariably start with a
careful design of the interleaver, and, in the case of the LDPC codes, a proper design of
the decoding algorithm, especially its numerical operations. In both cases we ﬁnd that
certain subgraphs of the graph of the interconnect network of the code are responsible for
the error ﬂoor, and designing this network carefully is therefore a primary approach to
controlling the error ﬂoor.
Due to the importance of error ﬂoors in low-BER applications, several authors have
studied the error ﬂoor phenomenon since the early 2000’s [8, 7, 9, 12, 5, 1, 6], and modi-
ﬁcations of the decoding algorithm have been studied to alleviate the problem and lower
the error ﬂoor, speciﬁcally targeting absorbing sets [4, 12, 5, 9, 6], which have been im-
plicated as main mechanisms for the error ﬂoor as discussed below. As we will show, the
onset of the error ﬂoor on Gaussian channels is very strongly related to the behavior of
319
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

320
CHAPTER 7. ERROR FLOORS
the algorithm on binary symmetric channels, and the dynamics of the absorbing sets fully
explain why and when the error ﬂoor becomes dominant in a given code.
Importance Sampling,
Zero Bit, Bias = 1.5
C++ Simulations -- 12 iter
linear model
including errors 
linear model 
2.5
3
3.5
4
4.5
5
5.5
6
6.5
10−16
10−14
10−12
10−10
10−8
10−6
10−4
10−2
1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 7.1: Error ﬂoor phenomenon illustrated with the IEEE 802.3an LDPC from Section
6.5.3, illustrating the onset of the error ﬂoor at about Pb = 10−10.
We also will take a computational look at the problem of the error ﬂoor and investigate
how number representations, number ranges, and mathematical operations aﬀect the error
ﬂoor in practical decoder settings where computational resources are a key concern. We
will see that the error ﬂoor depends strongly on the number representation used for the
internal log-likelihood messages circulated in the decoder, and we show that the growth
rate of the error patterns in the absorbing set can be balanced by the growth of the
LLRs external to the absorbing set and that the manifestation of the error ﬂoor can be

7.1. THE ERROR FLOOR PROBLEM
321
delayed signiﬁcantly or completely avoided. This makes the error ﬂoor, at least partially,
a computation phenomenon.
This eﬀect can be quantiﬁed both analytically and computationally using importance
sampling guided simulations, which we will discuss later in this chapter. We verify the
hypothesis that, quite unlike in the case of binary erasure or binary symmetric channels,
the error ﬂoor phenomenon for LDPC codes on AWGN channels can essentially be classi-
ﬁed as an “artifact” of inexact decoding, and not an inherent weakness of the code, and,
in the end, the only solid cause for an error ﬂoor due to the code itself are its low-weight
codewords. The main contribution of coding theory to confronting the error ﬂoor is there-
fore a good code design which strives to generate an adequate minimum distance in the
code. We begin our discussion with a look at the binary-erasure channel mechanisms, but
wish to caution the reader that the conclusion from this discourse have to be used with
care and that the results are not universally generalizable.
We have already discussed the concept of the stopping set in conjunction with decod-
ing on the binary erasure channel. These stopping sets are ﬁxed points of the decoding
algorithm and are error patterns that cannot be corrected. The concept of stopping sets
can be extended to other channel models, and we ﬁrst discuss the so-called absorption sets
which are responsible for the persistent error patterns of the decoder for signaling on the
binary symmetric channel of Section 6.4.3. The are deﬁned as follows:
Deﬁnition 7.1 An absorption set A is a set of variable nodes,
such that the majority of each variable node’s all neighboring check
nodes are connected to A an even number of times.
Figure 7.2 illustrates an absorption set in our example LDPC code from Figure 6.3.
We can now show that the Gallager algorithms from Section 6.4.3 will fail to correct
an absorption set, and that the decoder gets trapped in such a set. Assume that the
black nodes in Figure 7.2 are in error, e.g., they are logical one’s, while all other nodes
are logical zeros. In Step 3 of the algorithm in Section 6.4.3, the ﬁrst four neighbor nodes
from the left will return logical one’s to the absorption set, while the three neighbor nodes
to the right return zero’s. However, as is easily checked, at each variable node in the
absorption set, the majority of the incoming messages are logical one’s, and the nodes will
therefore retain their value of one. This cycle now continues and a stable error pattern is
established.
This is, however, where the similarities between absorption and stopping sets end. For
example, unlike in the case of the BEC, the ﬁnal absorption set in a decoding cycle does
not necessarily need to be a subset of the initial error set from the BSC channel. Very
little work on the analysis of LDPC codes over the BSC exists, since the LDPC codes do

322
CHAPTER 7. ERROR FLOORS
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Black: Absorption Set
Neighbors
Figure 7.2: Absorption set of size four nodes in an example LDPC code. Note that four
of the check nodes that are neighbors to the absorption set are doubly connected to the
set, and three nodes are singly connected–see deﬁnition above.
not perform particularly well on the BSC. However, absorption sets are important to us
since they are closely linked to the failure mechanism of the codes on AWGN channels,
the most important channel model for practical coding applications.
Before we delve deeper into the importance of absorption sets, let us recall that the
density and probability evolution algorithms discussed in the last chapter depended on the
assumption of independent signals along a tree-like network as in Figure 6.7. If the code’s
Tanner graph has loops, then nodes at level l will have appeared previously a number of
levels lower in the tree. In fact, due to the bipartite nature of the Tanner graph, only
cycles of even length exist, and cycles of length 4 are the shortest possible cycles since we
disallow parallel edges between variable and check nodes.
Since short cycles in the graph of the code cause dependencies among the message that
are assumed independent, they are also implicated with the error ﬂoor. Avoiding such
short cycles has therefore become a primary design objective and proven quite eﬀective.
The shortest cycle in the Tanner graph of a code is called the girth of the Tanner graph,
or simply of the code.
However, simply increasing the girth of a code has limited eﬀect on the error ﬂoor,
and it is not the magical solution to low error ﬂoors, in particular since creating large
girths require very large code sizes. Also, it is not just cycles which play a role, but other
closely connected subgraphs. This is where we return to the absorption sets, noting that
each absorption set contains at least one cycle, which can be seen by the following simple
argument: Depart at an arbitrary node in the absorption set A, travel to a check node
that is connected to the set an even number of times, and return to A. Repeat this process

7.2. DYNAMICS OF THE ABSORPTION SETS
323
until you return to the original node and complete a cycle. Before the cycle is completed,
it is always possible to return to a check node with an even number of connections since
the nodes in A must have a majority of connections to such neighbors.
Figure 7.3 illustrates the dominant absorption set in the IEEE 802.3an code from
Section 6.5.3, and we have isolated the unsatisﬁed, singly connected neighboring check
nodes since they play a special role in the dynamics of the set under message passing
decoding. Note also that only the check and variable nodes involved in the absorption sets
are drawn. The nomenclature used for absorption sets is typically using two parameters
a and b: a indicates the number of variable nodes in the set, and b indicates the number
of connections to unsatisﬁed check nodes. In this terminology the set in Figure 7.3 is an
(8,8) absorption set.
Singly connect check nodes (dangling checks) at top
Black
Stopping Set
Neighbors
⊞
⊞
⊞
⊞
⊞
⊞
⊞
⊞
⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞⊞
Figure 7.3: Size-8 absorption set of the IEEE 802.3 LDPC code with 8 unsatisﬁed checks.
7.2
Dynamics of the Absorption Sets
The (8,8) absorption set in Figure 7.3 is dominant in the sense that it is responsible for
the error ﬂoor of the code in Figure 7.1. There are other absorption sets in the code, as
we will see later, but those are not “visible” in the sense that their contribution to the
error ﬂoor is much smaller than that of the dominant set.
In this section we present an analysis which will allow us to explain the eﬀects of
the absorption set. The analysis is approximative, but leads to very accurate results and
has excellent predictive power–all that we really need from a theory. We start with the
illustration of eight log-likelihood (LLR) values of the absorption set variable nodes during

324
CHAPTER 7. ERROR FLOORS
a particular decoding cycle. These LLR values are shown as the eight traces in Figure
7.4. We can distinguish an initial phase where the LLRs ﬂuctuate apparently randomly,
but undergo a growth process. This phase is followed by a “bit ﬂipping” process once the
limit values of the LLR number range have been reached. In this example, this limit is
set to an arbitrary, but quite typical, value of |λmax| = 10.
30
25
20
15
3
2
5
10
LLR clipping 10 and -10
Node LLR values
iterations
Linear Growth
Bit Flipping
Figure 7.4: Trajectories of the LLR values of the variable nodes in the absorption set of
Figure 7.3 during a speciﬁc decoding cycle.
We now understand the importance of the absorption sets in the decoding of LDPC
code even over the Gaussian channel. Once the limit values of the LLRs are reached, a
signal clipping occurs. The eﬀect of this is that only the sign of the LLR value participates
in the message passing, since all values are either plus or minus the maximum. In eﬀect, the
decoder has migrated to simple bit ﬂipping, which is identical to the decoding algorithm
for the BSC channel discussed in Section 6.4.3. An absorption set caught in the bit ﬂipping
stage of the decoding algorithm will never correct itself.
The reader may wonder about the impact of the clipping value on this phenomenon,
and we will later see that large variations in the error ﬂoor can be generated simply by
changing this clipping threshold. However, some of form of clipping always occurs, even
in a ﬂoating point computer version of the algorithm. However, the subtle inﬂuence the
clipping on the error ﬂoor may account for the large disagreements by diﬀerent researchers
about where the ﬂoor lies for a particular code. The facts are that this ﬂoor depends on a

7.2. DYNAMICS OF THE ABSORPTION SETS
325
number of parameters which are decoder related, but not code related, and it is therefore
possible to steer the error ﬂoor over wide ranges with a proper decoder design.
In order to develop an analytical methodology, we denote the outgoing edge values
from the variable nodes in Figure 7.3 by xi, i.e., x1, x2, . . . , x5 leave variable node v = 0,
x6, x7, . . . , x10 variable node v = 1, etc. Collect the xi in the length-40 column vector
x, which is the vector of outgoing variable edge values in the absorption set. Likewise,
and analogously, let y denote the incoming edge values to the variable nodes, such that yj
corresponds to the reverse-direction message. Now, at iteration i = 0
x0 = λ,
(7.1)
where the initial input is the vector λ = [λ1, . . . , λ1, λ2, . . . , λ2, . . . , λ8, . . . , λ8]T of channel
intrinsics copied onto the ﬁrst outgoing messages. It undergoes the following operation at
the check node:
y0 = Cx0,
(7.2)
where C is a permutation matrix that exchanges the absorption set signals. This is a valid
approximation, since we conjecture that the messages within the absorption set converge
much slower then those external to the set. Therefore, even though the check nodes in
the code have 32 edges each, the minimum message will dominantly aﬀect the outputs,
and this is the message from within the absorption set. We will make this approximation
more precise in what follows.
At iteration i = 1 we now obtain
x1 = VCλ + λ + λ(ex)
1
,
(7.3)
where V is the variable node function matrix, i.e., each output is the sum of the other
four inputs from the check nodes plus the intrinsic input. The extrinsic inputs from the
remainder of the code graph are contained in λ(ex)
1
. Following our linear model, at iteration
i = j extrinsic signals are injected into the absorption set as
λ(ex)
j
=

λ(ex)
j1 , . . . , λ(ex)
j1 , λ(ex)
j2 , . . . , λ(ex)
j2 , . . . , λ(ex)
j8 , . . . , λ(ex)
j8
T
via the extrinsic check nodes. By induction we obtain at iteration i = I
xI =
I

i=0
(VC)iλ +
I

i=1
(VC)I−iλ(ex)
i
.
(7.4)
We can now compute the largest eigenvalue of VC and its eigenvector, since they dominate
the power of the matrix (VC)i. We ﬁnd the following result:
Lemma 7.1 The largest eigenvalue of VC for the (8, 8) set is μmax = dv −2 = 4, and its
associated eigenvector is vmax = [1, 1, . . . , 1]T.

326
CHAPTER 7. ERROR FLOORS
Proof: First write VC = 4M. By inspection M is a probability matrix, i.e., the sum of
each row equals unity. As a special case of the Perron–Frobenius theorem, it is known
that the largest eigenvalue of a probability matrix is 1; therefore the largest eigenvalue of
VC equals 4. By inspection, VC[1, 1, . . . , 1]T = 4[1, 1, . . . , 1]T.
Q.E.D.
We can now use the spectral theorem
(VC)iλ i→∞
−−−→μi
max

λTvmax

vmax
(7.5)
to obtain an estimate of the behavior of xI. If error indicator β = xI · 1 ≤0 for I →∞,
then the absorption set is in error and cannot be corrected. Combining (7.5) with (7.4),
we ﬁnd
β = λTvmax +
I

j=1

λ(ex)
j
+ λ
T
vmax
μj
max
≤0
(7.6)
or, in the case of our (8, 8) absorption set
β =
8

i=1
⎛
⎝λi +
I

j=1
λ(ex)
ji
+ λi
μj
max
⎞
⎠≤0.
(7.7)
The eigenvalue μmax = dv −2 is the gain of the absorption. It determines how rapidly
the set-extrinsic information is suppressed by the dynamics of the set in favor of the
set-internal signals.
Exact knowledge of λ(ex)
ji
is typically not available to the analysis, since these values
depend on the actual received signals. However, assuming that the code structure extrinsic
to the apsorption set operates “regularly,” we may substitute average values for the λ(ex)
ji .
By “regular” in this context, we mean that statistics of λ(ex)
ji
follow those predicted by
density evolution, and we further assume that λ(ex)
ji
has a consistent Gaussian distribution
[2, 27], with a mean equal to 2σ2. We therefore only need the mean of λ(ex)
ji , which we ﬁnd
via Gaussian density evolution discussed in Chapter 6, calculated from (6.30) and (6.34)
as
m(i)
λ(ex) = φ−1

1 −

1 −φ

mλ + (dv −1)m(i−1)
λ(ex)
dc−1
,
where mλ = 2Eb/σ2 is the mean of λi, m(i)
λ(ex) is the mean of the extrinsic signal λ(ex)
ji .
Since all the components in (7.7) are Gaussian, we can compute the probability of β

7.2. DYNAMICS OF THE ABSORPTION SETS
327
not exceeding zero after some tedious but straightforward computations as
PAS
=
Pr(β ≤0)
=
Pr

Mean(β)

Variance(β)

(7.8)
=
Q
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2mλ + 2
I

j=1
m(j)
λ(ex) + mλ
μj
max





⎛
⎝1 +
I

j=1
1
μj
max
⎞
⎠
2
mλ +
I

j=1
m(j)
λ(ex)
μ2j
max
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(7.9)
Equation (7.9) expresses the probability of an absorption set falling in error as an ex-
ponential error function, that is, analogous to a regular decision error probability. The
diﬀerence is that the remainder of the code aﬀects this error probability via its injection
of the extrinsic messages.
Before we can study this equation in more detail, Schlegel and Zhang [9] found that a
small but important reﬁnement to the formula can decisively enhance its accuracy. The
exchange of extrinsics through the matrix C is an approximation in two ways, First, during
the initial iterations, the dc −2 = 30 inputs to the check node from the set-external part
of the code are relatively small, and the entries of C are strictly less than unity, and, in
case one or an odd number of these set-external extrinsic messages has an erroneous sign,
the returned signal to the absorption set switches polarity. The ﬁrst issue is addressed as
follows. Using a Taylor series approximation, we can show that the quintessential check
node operation
tanh−1

tanh(x)
dc−2

i=1
tanh(xi)

=
dc−2

i=1
tanh(xi) x + O

x3
,
where
dc−2

i=1
tanh(xi) can be interpreted as a “check node gain.” Since we have no access
to the actual check messages, we again use the mean m(i)
μ(ex) of the signals μ(ex) from the

328
CHAPTER 7. ERROR FLOORS
variable to the check nodes for xi, to compute an average gain as
gi
=
E
⎡
⎣
dc−2

i=1
tanh
⎛
⎝m(i)
μ(ex)
2
⎞
⎠
⎤
⎦
=
E
⎡
⎣tanh
⎛
⎝m(i)
μ(ex)
2
⎞
⎠
⎤
⎦
dc−2
=

1 −φ

m(i)
μ(ex)
dc−2
,
(7.10)
where the last equality results from the deﬁnition of the density evolution function φ(·) in
(6.32). With this the probability in (7.9) is modiﬁed to the somewhat unwieldy-looking
expression
PAS = Q
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2mλ + 2
I

j=1

m(j)
λ(ex) + mλ
μj
max
j
l=1
1
gl






⎛
⎝1 +
I

j=1
j
l=1
1
glμmax
⎞
⎠
2
mλ +
I

j=1
m(j)
λ(ex)
 j
l=1
1
glμmax
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(7.11)
In order to obtain an estimate of the inﬂuence the absorption set has on the bit error rate
of the code, we need the multiplicity of this set. A restricted exhaustive search discussed in
[9] computes this value as 14,272 and also presents tables of the multiplicities per variable
node for all of the variable nodes. This multiplicity ranges from A8,8 = 37–75. We now
apply the union bound to obtain an upper bound of the impact of this absorption set on
the error ﬂoor. Given that the (8,8) absorption set is dominant in this code, this is also
an estimate of the error rate in the error ﬂoor, given as
Pb ≈A8,8PAS.
(7.12)
This estimate is plotted in Figure 7.1 and shows very close agreement with simulation
results, as well as with the importance sampling techniques discussed in Section 7.5.
The second issue of erroneous messages entering the check nodes can be handled with
similar arguments, and the details are outlined in [9]. However, the impact on the error
formula of this eﬀect is quite minor and we therefore do not discuss this further here.
Figure Figure 7.1 includes this eﬀect in the error curve labeled “linear model including
errors.”

7.2. DYNAMICS OF THE ABSORPTION SETS
329
One last issue that needs to be highlighted is that of the clipping threshold. While
the generic algorithm on Page 257 does not have any obvious computational limits, such a
limit exists in all practical implementations. First, for computational reasons the number
representation along the message lines must be limited to a few bits, typically anywhere
between 4 and 10.
This imposes both a maximum resolution as well as a maximum
value that can be represented numerically. As the LLRs grow in the course of decoding,
numerical values will be limited, or clipped at this threshold λmax. For most of the bit error
range, λmax is quite unimportant, however, in the error ﬂoor it has a very strong impact.
λmax = 10 was chosen for both the calculations as well as simulations in Figure 7.1.
The second reason is one of numerical precision.
The regular generic check node
processor computes m(i)
λ(ex), which will grow to inﬁnity as long as the code operates above
its threshold. However, using
λj→i = 2 tanh−1
⎛
⎝

l∈Vj\{i}
tanh
λl→j
2
⎞
⎠
(7.13)
in the decoding algorithm eﬀectively clips λ(ex)
i
due to numerical limitations of computing
the inverse hyperbolic tangent of a number close to unity.
This is observed in [1] as
saturation. More speciﬁcally, if p bits are available to represent the argument of tanh−1(·),
then the largest output value is approximated by (see [1])
λ = (p + 2) ln 2.
(7.14)
This equals 38.1230949 the 64-bit IEEE 754 double-precision ﬂoating-point format with
p = 53 bits of precision.
Therefore, instead of (7.13), the corrected min-sum algorithm [3] computes the check-
to-variable LLR as
λj→i =

min
l∈Vj\{i} (|λl→j|) + CT


l∈Vj\{i}
sign (λl→j),
(7.15)
where CT represents the correction term that is introduced to mitigate the loss of the
min-sign approximation introduced in (6.8). This speciﬁc correction factor was derived in
[3], but other such approaches are possible. It is shown in [3] that this correction leads to
a performance of no observable loss for a variety of LDPC codes. The factor is dependent
on the check node degree and given as
CT =

−ln(dc −1) /4,
if
minl∈Vj\{i} |λl→j| ≥3 ln (dc −1) /8,
0,
otherwise,
(7.16)

330
CHAPTER 7. ERROR FLOORS
Using (7.15) eﬀectively avoids the clipping due to numerical instability and is the preferred
practical implementation since it also oﬀers lower computational complexity than the
original check node computation (7.13).
We summarize this section with the two steps required to compute the error ﬂoor of
an LDPC code. First, we need a procedure to identify the absorption sets which are most
inﬂuential on the BER. This requires a search procedure. The authors of [9] have looked
at the IEEE 802.3an LDPC code discussed here, and identiﬁed the small absorption sets
in Table 3.1, of which the (8,8) set in Figure 7.3 is the dominant absorption set. It is
shown in [9] that the (8,8) absorption set causes the highest BER contribution and that
the next “lower” set has a BER impact that is about an order of magnitude less.
a
b
Existence
Multiplicity
Gain
< 5
No
5
10
No
6
6, 8, 10, 12
No
7
0, 2, 4, 6, 8, 10
No
12
Yes
65, 4721
3.29
14
Yes
14, 720
3
8
0, 2, 4, 6
No
8
Yes
14, 272
4
10
No
12
Yes
44, 416
3.5
14
Yes
?
3.25
16
3
9
0, 2, 4
No
6, 8, 10
?
12
Yes
?
3.67
14
3.44
16
3.22
18
3
10
0, 2, 4, 6, 8
?
10
Yes
> 192
4
12
Yes
?
3.8
14
3.6
16
3.4
18
3.2
20
3
Table 3.1: Absorption sets of IEEE 802.3an LDPC code.
1Only sets not contained in (8, 8) absorption sets are counted — see [9].

7.3. CODE DESIGN FOR LOW ERROR FLOORS
331
The second step is to apply the computational procedure outlined in this section to
calculate the impact of each absorption set to the error ﬂoor. This requires computation
of μmax and vmax numerically using the set topology, and then it uses (7.6), in general, to
ascertain the error probability of each set. The dominant set and its multiplicity typically
produce a good estimate of the error ﬂoor via the union bound.
7.3
Code Design for Low Error Floors
The need to address the error ﬂoor problem was of course recognized early on, before a
comprehensive theory of it was available. In this section we discuss a couple of approaches
which have led to successful designs, and are worth discussing.
One such successful method is to realize that not all variable nodes have the same
error rates and that an irregular LDPC code has an unequal bit error rate. In fact, due
to (6.40), nodes with a higher degree will have a more rapid accumulation of their LLR
value and thus will be more reliable. We can therefore associate nodes of high variable
degrees with information nodes, which need high error protection, and those of lower
degrees with parity nodes, whose error rates are, in principle, irrelevant. In decoding, we
are typically only interested the error rate of the information bits, and the parity bits are
discarded. This is the approach taken in the design of the so-called extended irregular
repeat accumulate codes (eIRA) studied in [11].
The parity-check matrix of these codes has a special structure, an example of which is
H =
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
,
(7.17)
where the parity section is characterized by the dual diagonal which allows very eﬃcient
recursive encoding: see Section 6.6.
Yang et al. [11] have constructed eIRA codes using optimized degree proﬁles. The
construction of the parity-check matrix simply ensured that there were no 4-cycles. The
authors noted that if they biased the distributions such that no degree-3 or no degree-4
information variable nodes existed, they could trade oﬀthe error ﬂoor and the convergence

332
CHAPTER 7. ERROR FLOORS
threshold. Figure 7.5 shows the performance of three such codes of rate R = 0.82 of size
(4161,3430) [11] whose degree distributions are given by
Code 1:
λ(x)
=
0.00007 + 0.1014x + 0.5895x2 + 0.1829x6 + 0.1262x7
ρ(x)
=
0.3037x18 + 0.6963x19
Code 2:
λ(x)
=
0.0000659 + 0.0962x + 0.9037x3
ρ(x)
=
0.2240x19 + 0.7760x20
Code 3:
λ(x)
=
0.0000537 + 0.0784x + 0.9215x4
ρ(x)
=
0.5306x24 + 0.4694x25
Another approach proposed in [10] is to lower the so-called approximate cycle extrinsic
message degree. The extrinsic message degree (EMD) is a precursor of the more complete
absorption set, which considers only cycles instead of the the entire set. While compu-
tationally of little value, the concept is easier to incorporate into the design of a code,
since, primarily, the designer works only with the short cycles of the codes, which are a
topologically much simpler structure.
The idea is similar to those we have seen aﬀecting the error performance of the ab-
sorption set, namely, a larger number of messages entering the set from the surrounding
code network lower the probability that the set falls in error–see (7.7). This concept is,
however, applied to cycles instead. Consider the cycle on the left in Figure 7.6. It is
actually a stopping set–remember that each stopping or absorption set contains at least
one cycle. As a stopping set, it has an extrinsic message degree (EMD) of zero; there
are no outside message lines connecting to the variable nodes of this set. We have come
across the concept of the extrinsic message degree in the previous section, so let us deﬁne
it formally:
Deﬁnition 7.2 The extrinsic message degree (EMD)
of a set is deﬁned as the number of connections from
the variable nodes of the set to outside (check) nodes.

7.3. CODE DESIGN FOR LOW ERROR FLOORS
333
2
2.25
2.5
2.75
3
3.25
3.5
3.75
4
4.25
10−9
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Code 3
Code 2
Code 1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 7.5: Performance of optimized eIRA codes of rate R = 0.82 with low error ﬂoors.
The EMD of a reduced set derived from the stopping set is also shown in Figure 7.6.
It has an EMD of 3. This illustrates the somewhat arbitrary nature of the deﬁnition.
Nonetheless, this deﬁnition has proved quite useful. Note that a stopping set always has
an EMD of zero, and an (a,b) absorption set has an EMD equal to b.
Working with the EMD of sets is combinatorially complex as we have seen in our
description of absorption sets, and a computationally more practical measure is needed
to make it useful for code construction. Tian et al. [10] proposed to consider the EMD
only of cycles, ignoring possible connection inside the cycle. For example, their deﬁnition
ignores the central check node in the set on the right-hand set of Figure 7.6 and gives the
associated circle an approximate EMD of 5. In general then, the approximate EMD of a
circle of length 2d simply equals
ACE =
d

i=1
(λi −2)
(7.18)

334
CHAPTER 7. ERROR FLOORS
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Smaller Set, EMD = 3
Stopping Set, EMD = 0
Figure 7.6: Extrinsic message degree of two sets.
and is thus not signiﬁcantly more complex to compute then ﬁnding the cycles in a code.
On the basis of this Approximate Cycle EMD, called the ACE of a cycle, Tian et al. [10]
proposed the construction of codes with parameters dACE and ηACE, thus deﬁned:
Deﬁnition 7.3 An LDPC code has parameters (dACE, ηACE)
if all cycles of length d ≤2dACE have ACE ≥ηACE.
This deﬁnition opens avenues to the construction of LDPCs with many diﬀerent sets
of parameters. That this procedure can produce good codes is illustrated in Figure 7.7
[10], which compares two codes of rate R = 0.5 and length 10K.
As we have seen in the preceding section, the error mechanism and its impact on the
error ﬂoor are well understood, and are based on complex topological structures which
form part of the code’s interconnect network. While construction methods such as the
ones presented in this section have been quite successful in reducing the number of such
sensitive structural weaknesses, they can never be completely avoided in codes of ﬁnite
size. We therefore turn our attention now to the importance of the decoding algorithm
itself as a contributor to the error ﬂoor of LDPC codes.

7.4. IMPACT OF THE DECODING ALGORITHM
335
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
10−9
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
no 4-cycle Code
(9,4) Code
Bit Error Probability (BER)
Eb/N0[dB]
Figure 7.7: Performance of codes of rate R = 0.82 and length N = 10, 000 constructed
with the ACE criterion. The ﬁrst code contains no 4-cycles, while the code is constructed
such that each cycle of length 18 or less has an EMD of at least 4.
7.4
Impact of the Decoding Algorithm
The error ﬂoor equation (7.11) can give clues to how the actual bit error rate will behave
as a function of the diﬀerent parameters. Primarily, the absorption set itself determines
the magnitude of the gain μmax, which is determined by the structure of the alone. It
inﬂuences how rapidly the eﬀect of the extrinsic information mλ(ex) diminishes.
In an ideal code using ﬂoating point arithmetic, mλ(ex) itself has exponential growth
as determined by density evolution and will balance the geometric growth of the eﬀect of
the absorption set gain. We glimpse the possibility that if the growth of mλ(ex) can be
sustained, the argument of (7.11) can be made arbitrarily large and hence PAS →0. That
is, in this case the impact of an absorption set, even with a strong gain, μmax can be made
to vanish.

336
CHAPTER 7. ERROR FLOORS
As we discussed in Section 7.2, for a variety of reasons, usually due to complexity
reasons, the log-likelihood messages are limited to a ﬁnite range, and therefore the growth
of μmax cannot keep up with the growth of μmax. As a consequence, the set-extrinsic mes-
sages have an eﬀect on PAS only for a few initial iterations, after which their contribution
quickly vanishes. If the set is not corrected during these initial iterations, the error will
persist no matter how many total iterations are applied to the algorithm. This eﬀect is
well known and caused by the decoder converting to eﬀective bit ﬂipping decoding.
If we are using (7.15) and (7.16) the limitation on λex is completely under the designer’s
control and not inﬂuenced by numerical issues as in (7.13). Figure 7.8 shows the error
rates due to the dominant absorption set of the IEEE 802.3an LDPC code in its error
ﬂoor region. The dashed curves plot (7.11) for I = 10 and mλ and m(i)
λ(ex) are bounded by
10, which is the clipping threshold. The circles are the numerical results of importance
sampling (IS) utilizing (7.15) with the same I and LLR clipping threshold. When the
threshold is increased to 100, the error rate decreases as predicted by this analysis. Also
shown are results for a clipping value of 1,000.
The error formula (7.11) is quite sensitive to numerical issues, however, despite this
sensitivity, importance sampling simulations and analytical results agree to within a few
tenths of a dB over the entire range of Eb/N0 values of interest. Reference [13] discusses
further details and presents additional code examples.
7.5
Importance Sampling (IS)
Software-, and even hardware-based simulation of very low error rates is often infeasible.
For example, even at a bit rate of 1 Gb/s, it would take over 11 days to produce a
single error at an error rate of 10−15, which is a typical error rate requirement for optical
transmission systems.
This is the domain of importance sampling, which elevates the
occurrence of the important event, in our case the errors.
We begin with the following general formulation of the error computation problem.
Let Pe be the quantity we wish to compute by integrating the probability density function
f(x) over the domain Ω which is associated with the “error,” i.e.,
Pe =

Ω
f(x) dx,
Ω is the domain of integration.
(7.19)
This situation is illustrated in Figure 7.9 for a one-dimensional variable and domain, in
which case the integral can of course be performed numerically. However, if the domain Ω
spans many dimensions, the describing the boundary of the domain Ω can become exceed-
ingly complex, which makes an analytical computation of (7.19) practically impossible.

7.5. IMPORTANCE SAMPLING (IS)
337
Ideal white Gaussian
noise -- FPGA 
Importance Sampling
C++ Simulations -- 12 iter
Clipping=1000
Clipping=10
Clipping
   =100
2.5
3
3.5
4
4.5
5
5.5
6
6.5
10−16
10−14
10−12
10−10
10−16
10−14
10−12
10−10
10−8
10−6
10−4
10−2
1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 7.8: Bit error rates due to the dominant absorption set of the IEEE 802.3an LDPC
code using both formula (7.11) and importance sampling (IS) with LLR clippings at 10,
100, and 1000, respectively. The iteration number is set to 10.
This is the situation encountered in computing error probabilities for many communi-
cations applications and is a situation which is addressed by importance sampling. IS

338
CHAPTER 7. ERROR FLOORS
evaluates this integral (7.19) as
Pe =

Ω
f(x) dx =

Ω
f(x)
ρ(x)ρ(x) =

Ω
w(x)ρ(x) dx,
(7.20)
where the weighting function w(x) = f(x)/ρ(x) changes the distribution of the samples
over Ω.
Of course, (7.19) is no easier to compute analytically since the boundary problem is
the same. However, often it is straightforward to determine whether a point sample lies
within the domain of integration or outside–for example, if it is an error event of a decoder.
Using sample counts, we therefore approximate
Pe ≈1
Ns
Ns

i=1
w(xi)IΩ(xi)
(7.21)
where the new random samples are drawn according to ρ(x), however, instead of the
original distribution f(x). The indicator function IΩ(xi) = 1 if xi ∈Ω and zero other-
wise formalizes that only sample in the domain of integration are counted. The count
Ns denotes the number of samples used in the approximation (7.21). The goal of im-
portance sampling is now to distort the original density function in such a way that Ns
can be minimized for a given accuracy of the estimate, and therefore simulations can be
accelerated.
Domain Ω
ρopt(x)
f(x)
ρopt throws every sample into Ω
Figure 7.9: Illustration of importance sampling for a one-dimensional function.
It can easily be seen that the optimal distortion of f(x) would use
ρopt(x) =
|f(x)|

Ω |f(x)| dx;
x ∈Ω,
(7.22)

7.5. IMPORTANCE SAMPLING (IS)
339
and lead to the constant weighting function w(x) =

Ω |f(x)| dx—which would require
only a single sample. Clearly (7.22) is not of practical interest since it requires that we
can compute the integral that we originally tried to avoid.
Once we ﬁx a ρ(x)—to be discussed below—the error is estimated via IS as
˜Pe = 1
Ns
Ns

i=1
w(xi)IΩ(xi).
(7.23)
It can be seen easily that ˜Pe is an unbiased estimator of Pe, that is E

˜Pe

= Pe, and its
variance is computed as
σ2
s
=
1
N2s
E
⎡
⎣
Ns

i=1
w(xi)IΩ(xi)
Ns

j=1
w(xj)IΩ(xj)
⎤
⎦−P 2
e
=
1
Ns
E
 Ns

i=1
w2(xi)IΩ(xi)

+ Ns −1
Ns
P 2
e −P 2
e
=
1
Ns
E
 Ns

i=1
w2(xi)IΩ(xi)

−P 2
e
Ns
.
(7.24)
Of course, the variance itself needs to be estimated since the actual error Pe is the
unknown quantity. An unbiased estimator for the variance is found as
˜σ2
s
=
1
Ns
⎛
⎝1
Ns
Ns

i=1
w2(xi)IΩ(xi) −

1
Ns
Ns

i=1
w(xi)IΩ(xi)
2⎞
⎠,
(7.25)
The performance advantage of IS is often expressed as the gain it attains over direct
Monte-Carlo simulation, i.e., ρ(x) = 1, and is expressed as
Gs = σ2
MC
σ2s
.
(7.26)
The key is to ensure that the gain Gs  1 order to save on the number of simulation runs.
The variance of Monte-Carlo simulations is computed assuming that each run is in-
dependent from other runs and has the same error probability Pe. The variance σ2
MC is
therefore the variance of a binary 0/1 random variable and found by setting w(xi) = 1 in
(7.24), or (7.25), respectively, to obtain
σ2
MC = Pe(1 −Pe)
Ns
.
(7.27)

340
CHAPTER 7. ERROR FLOORS
This is also how the error bars for error rate simulations are derived, by ﬁnding the
standard deviation ˜σs according to (7.25).
The gain Gs is therefore a measure of how much the reliability of an estimate is
improved, as well as how much computational gain is achieved over standard Monte-Carlo
sampling for the same reliability.
Figure 7.10 shows the this gain for the case of the
error rate of the [7,4] single-error correction Hamming code using maximum-likelihood
decoding in an additive white Gaussian channel to be discussed in Section 7.6. Clearly,
massive speed gains are possible with importance sampling if done correctly.
Gain for ML decoder
1
2
3
4
5
6
7
8
9
1
10
102
103
104
105
106
Eb/N0[dB]
Figure 7.10: Acceleration provided by importance sampling in simulation the bit error
probability of the [7,4] Hamming code.
7.6
Computing Error Rates via Importance Sampling
We now apply the method of importance sampling to the computation of error rates of
error control decoders. A few additional caveats need to be carefully addressed and avoided

7.6. COMPUTING ERROR RATES VIA IMPORTANCE SAMPLING
341
to obtain accurate results. If these are not properly dealt with, the results can easily be
skewed and can become useless.
Figure 7.11 shows the generic setup encountered by an error control decoder. However,
the reader should be cautioned that the simplicity of the situation in the ﬁgure does not
reﬂect how complex the situation is in general, and that such a simple description of the
decoding geometry is, in general, not possible.
We wish to compute P0 the probability that an error occurs if codeword x0 is trans-
mitted. This happens when the received signal vector y falls outside the decision region
D0 for x0, i.e., the region in received signal space for which the decoder outputs x0 as its
decoded codeword. The decision region can be the maximum likelihood decision region
discussed in Chapter 2, or any other decoder decision region. In fact, in general we do
not have a precise description of the boundary of the decision regions, and this is precisely
why we use simulation—in particular, IS. By simulating the decoder, we do not need to
know precisely where these boundaries are.
Formally, however, the error probability in question is obtained by integrating the con-
ditional channel probability density function p(y|x0) over the complement of the decision
region D0 of x0, which can be expressed as the sum over the decision regions for all other
codewords xi.
P0 =

i̸=0
Di
p(y|x0) dy =
M

i=1

Di
p(y|x0) dy
(7.28)
Generally, we can closely approximate P0 by concentrating on the most probable error
neighborhoods by restricting the explored error regions to those in the immediate proximity
of x0 as illustrated in Figure 7.11. Caution must be exercised, however, to make sure that
these dominant neighbors are suﬃciently well understood. This is not trivial in any typical-
sized modern error control code. In fact, the absence of such knowledge is precisely what
makes importance sampling such a sensitive technique in practice.
In general now, the purpose of IS is to to bias the noise towards producing signiﬁcantly
more errors, so the number of simulations can be reduced. This can be accomplished in a
number of ways:
• Excision: Certain received signal vectors are recognized as not causing an error
and can be discarded without simulating their eﬀect on the decoder. E.g., if simple
sign decoding causes all of the bits to be correct in that they produce a codeword,
the decoder will also generate the same codeword and does not need to be evoked.
Excision can sometimes be used to speed up a simulation, but will fail to help in
simulating the error ﬂoor in our case.
• Variance Scaling: The noise variance is simply increased and thus causes more

342
CHAPTER 7. ERROR FLOORS
Ω
x0
x1
x2
x3
x4
x5
x6
D0
Figure 7.11: Illustration of the decision regions followed by a decoder.
errors. One would apply the following weight function in
w(y) = σB
σ exp

−|y −x0|2 σ2
B −σ2
σ2
Bσ2

≈exp

−|y −x0|2
σ2

.
(7.29)
w(y) is exponential in the SNR. However, variance scaling does not work well in our
case. The reasons are simple. If variance scaling worked, it would be possible to
deduce a code’s error performance at high signal-to-noise ratios from its performance
in the low signal-to-noise ratio regime. This is typically not feasible, since diﬀerent
error mechanisms are responsible for the errors in the two regimes.
• Mean Translation: The most promising method, and the only one which is useful
in our case, is where the biased samples are generated according to p∗(y) = p(y−μ),

7.6. COMPUTING ERROR RATES VIA IMPORTANCE SAMPLING
343
where μ is a shift value towards the decision boundary of interest. We obtain
Pi0
=

Di
p(y|x0)
p(y −μ|x0)p(y −μ|x0) dy
=⇒Pi0
≈
1
Ns
Ns

j=1
p(y −μ|x0)
p(yj −μ|Ωx0)p(yj −μ|x0)I(yi).
(7.30)
Typically, in mean-shift importance sampling, the noise is shifted towards the decision
boundary. This shift may be approximate since the boundary may not be exactly known,
but will be somewhere in the “vicinity” of the midpoint between two codewords, i.e.,
μ = x0 + xi
2
.
Another issue that presents itself immediately with mean-shift IS is which boundary to
shift towards. As illustrated in Figure 7.12, there are typically a number of qualifying
nearest neighbors, and one really needs to separately shift towards each of them and then
combine the error probabilities into a complete estimate of Pe. This is done by obtaining
IS estimates of the probability that codeword x0 is decoded as xi, and then sum all these
contributions over the candidates that are examined, i.e.,
˜Pe =

i:neighbors
˜P0→i.
(7.31)
The diﬃculty here is to know where those dominant neighboring decision regions are
and to have an approximate estimate of their boundaries with the codeword region to be
simulated. As cautioned repeatedly, this requires a fairly detailed understanding of the
topology of the code which is to be examined.
In the example case of the [7,4] Hamming code, fortunately the geometry of the code-
words is known exactly, since the code is perfect. Concentrating on its 4 nearest neighbors
at Hamming distance dH = 3, we were able to obtain the results in Figure 7.10.
As we have cautioned the reader several times, knowledge of the codeword topologies
is vital for a proper use of importance sampling, and IS results are only reliable to extend
that this knowledge is complete. For instance, in our importance sampling simulations for
the IEEE 802.3an LDPC code in Figures 7.1 and 7.8, the implicit assumption was that the
dominant absorption set from Figure 7.3 and its multiplicities are primarily dominating
the error probabilities. This, however, may not correct for all cases, such as very large
clipping values in 7.3, and the actual minimum-distance codewords with dmin = 14 may
cause the error ﬂoor. Unfortunately, the multiplicity of the minimum distance neighbors
is unknown for this code, and hence we can only put in a coarse estimate of where this
error ﬂoor may lie—see again Figure 7.8. For a clipping value of 10, however, we can be

344
CHAPTER 7. ERROR FLOORS
Ω
x0
x1
x2
x3
x4
x5
x6
D0
Figure 7.12: Illustration of biasing the noise around codeword x0 towards codeword x6 to
generate an importance sampling error estimate ˜P0→6.
assured that the correct error mechanism has been identiﬁed since both simulations and
actual performance bit error rates coincide.
As a ﬁnal example we present IS results for the (16, 11)2 product code constructed
from two (16,11) Hamming codes. Product codes are discussed in Chapter 9. The well-
structured geometries of the product codes not only allow the application of a very regular
decoding algorithm, its codeword topology is also easy to derive from the topology of the
component code, in this case a (16,11) extended Hamming code with Amin = 15 nearest
codewords at Hamming distance dH = 4.
In particular, the minimum distance of the
product code is drmHp = d2
H = 16, and its multiplicity equals AHp = A2
H = 225. This
information is suﬃcient to compute the bit error rate caused by minimum-distance error
events, which is the dominating mechanism for this code in the error ﬂoor regime.

7.6. COMPUTING ERROR RATES VIA IMPORTANCE SAMPLING
345
This conclusion is further substantiated by the fact that IS simulation, measured hard-
ware results, and the minimum distance approximation
Pe ≈AHpQ

2dHp
N0

(7.32)
all agree, as shown in Figure 7.13.
mimimum distance
error aproximation
Importance Sampling of nearest
 
neighbor events
Commercial Hardware Simulation
using AHA’s Galaxy Simulator
0
1
2
3
4
5
6
7
10−16
10−14
10−12
10−10
10−8
10−6
10−4
10−2
1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 7.13: Low bit error evaluation for the (16, 11)2 product code using importance
sampling.
The reader may argue that in this case the IS simulations oﬀer no additional informa-
tion; however, this is not so. The error approximation (7.32) implicitly assumes the use
of a maximum-likelihood decoder which perfectly respects the ideal decision boundaries.

346
CHAPTER 7. ERROR FLOORS
x0
Figure 7.14: Illustration of the problem of overbiasing in applying importance sampling
to the low-error evaluation problem.
However, the actual implementation of product codes does not use ML decoding for com-
plexity reasons, but rather uses a form of iterative decoding and message passing between
the row an column product codes. In this case then, since the importance sampling simu-
lations are run with an iterative decoder, the IS results, in fact, conﬁrm that the decoder
will behave like a maximum-likelihood decoder even in the very low bit error regime.
Finally, we wish to discuss a couple of common pitfalls when applying importance
sampling to the evaluation of bit and frame error rates. First, and most obvious, is the
problem of overbiasing. This situation is graphically illustrated in Figure 7.14. What
happens in this situation is that the center of the noise source is shifted too far into the
erroneous decision region, highlighted as the grey hexagon in the ﬁgure. While in theory,
and with suﬃciently many samples, this does not pose a problem, in practice the estimate
of the error probability becomes biased, since the majority of the IS samples are far from
the decision boundary. That is, the area which produces the dominant errors and typically
constitutes a very small sliver near the decision boundary is not explored by suﬃcient IS
samples. The resultant error probability estimate is usually substantially smaller than the
correct value.
There is typically no clear way of ﬁnding out the correct bias value; however, if we
explore a few diﬀerent bias values, it usually becomes evident very easily where the over-
biasing phenomenon sets in. If a new bias value suddenly produces a smaller error estimate

7.6. COMPUTING ERROR RATES VIA IMPORTANCE SAMPLING
347
x0
t0
Figure 7.15: Illustration of linear continuous biasing.
than a previous bias value, we have entered the overbiasing situation. Finding a correct
bias value which causes no error in the estimate and still produces a suﬃcient speed-up
of the simulation is more of an art form than exact science and requires an attentive
experimenter.
Some researchers [8] have alleviated this issue by biasing along a continuous line. That
is, a conceptually inﬁnite number of biasing points is explored. This is shown in Figure 7.15
where the bias is located along the line t connecting x0 and t0. This method was ﬁrst
used by Richardson to quantify the eﬀect of the so-called trapping sets of LDPC codes.
Since we can possibly use all the bias points along the line t, a ﬁnite number of points
are generated according to the probability distribution of the noise component along the
line t. Since the projection of a multi-dimensional Gaussian random variable onto a single
dimension produces a standard 1-dimensional Gaussian random variable, we can therefore
write
Pe =
1
√
2πσ2
 ∞
−∞
Pe|s=x exp

−x2
2σ2

dx
(7.33)
The integral (7.33) has to be evaluated numerically, which produces ˆPe as the IS estimate
with various shifts as discussed above. Finding the point t0 is analogous to ﬁnding the
direction of bias discussed earlier, i.e., some knowledge about the decision boundaries is
required.
As a ﬁnal point, we wish to emphasize the issue of “biasing away” from the dominant

348
CHAPTER 7. ERROR FLOORS
x0
x1
D1
D2
D1
D2
x0
x1
Figure 7.16: Illustration of the problem of biasing away from the dominant error region,
which now sees a much reduced number of error events.
event. This is illustrated in Figure 7.16. The reader should note that the 2-dimensional
rendition in the ﬁgure is a poor representation of what really happens; however, it does
capture the idea of the problem. Assuming that there are two error regions D1 and D2, a
cursory examination may only identify D1, even though D2 will dominate the error ﬂoor
for higher values of the SNR, since its closest tip has a smaller Euclidean distance from
the correct codeword x0. However, if we now bias towards D1, the reader will easily see
that the probability of seeing any D2 errors is greatly diminished, and the experimenter
may be fooled into thinking that the dominant error event has been found and identiﬁed.
This underlines, once again, the importance of knowing the code’s topology adequately in
order to produce an accurate estimate of the error probabilities in the error ﬂoor regime.
Recall our discussion around Figure 7.8 in this context.
All of these eﬀects are visible only with a ﬁnite number of sample. The biasing theory
assumes an inﬁnite number of samples and produces no biased estimates as such. However,
the purpose of importance sampling is to reduce the number of simulation runs, and
therefore eﬀects due to ﬁnite numbers of samples have to be taken seriously and carefully.
While the error mechanisms in turbo codes follow similar dynamics in principle, the
fact that the node processors are stronger constituent codes than the simple repetition and
parity-check codes in LDPC systems, we observe that non-codeword errors do not appear
to make up a measurable portion of the error events. Turbo code design, therefore, has
concentrated primarily on increasing the minimum free distance of the codes. This will
be discussed in Chapters 8 and 9.

Bibliography
[1] B.K. Butler and P.H. Siegel, “Error ﬂoor approximation for LDPC codes in the
AWGN channel,” IEEE Trans. Inform. Theory, vol. 60, no. 12, pp. 7416–7441,
Dec. 2014.
[2] S.-Y. Chung, T.J. Richardson, and R. Urbanke, “Analysis of sum–product decoding
of low-density parity-check codes using a Gaussian approximation,” IEEE Trans. In-
form. Theory, pp. 657–670, Feb. 2001.
[3] S. L. Howard, C. Schlegel, and V. C. Gaudet, “Degree-matched check node decoding
for regular and irregular LDPCs,” IEEE Trans. Circuits Syst. II: Express Briefs,
vol. 53, no. 10, pp. 1054 –1058, Oct. 2006.
[4] M. Ivkovi´c, S. K. Chilappagari, and B. Vasi´c, “Eliminating trapping sets in low-
density parity-check codes by using tanner graph covers,” IEEE Trans. Inf. Theory,
vol. 54, no. 8, pp. 3763–3768, Aug. 2008.
[5] J. Kang, Q. Huang, S. Lin, and K. Abdel-Ghaﬀar, “An iterative decoding algorithm
with backtracking to lower the error-ﬂoors of LDPC codes,” IEEE Trans. Inf. Theory,
vol. 59, no. 1, pp. 64–73, Jan. 2011.
[6] G. B. Kyung and C.-C. Wang, “Finding the exhaustive list of small fully absorbing
sets and designing the corresponding low error-ﬂoor decoder,” IEEE Trans. Commun.,
vol. 60, no. 6, pp. 1487–1498, June 2012.
[7] J. Sun, O. Y. Takeshita, and M. P. Fitz, “Analysis of trapping sets for LDPC codes
using a linear system model,” in 42nd Annual Allerton Conference on Communica-
tions, Control and Computing, Oct. 2004.
[8] T.J. Richardson, “Error ﬂoors of LDPC codes,” Proc. 2003 Allerton Conference on
Communications, Control and Computing, pp. 1426–1435, Oct. 2003.
[9] C. Schlegel and S. Zhang, “On the dynamics of the error ﬂoor behavior in (regular)
LDPC codes,” IEEE Transactions on Information Theory, vol. 56, pp. 3248–3264,
June 2010.
349

350
BIBLIOGRAPHY
[10] T. Tian, C.R. Jones, J.D. Villasenor, and R.D. Wesel, “Selective avoidance of cycles
in irregular LDPC code construction,” IEEE Trans. Commun., vol. 52, no. 8, pp.
1242–1247, Aug. 2004.
[11] M. Yang, W. E. Ryan, and Y. Li, “Design of eﬃciently encodable moderate–length
high-rate irregular LDPC codes,” IEEE Trans. Commun., vol. 54, no. 2, pp. 564–571,
April 2004.
[12] L. Dolecek, Z. Zhang, V. Anantharam, M. Wainwright, and B. Nikolic, “Analysis of
absorbing sets and fully absorbing sets for array-based LDPC codes,” IEEE Trans.
Inf. Theory, vol. 56, no. 1, pp. 181–201, Jan. 2010.
[13] S. Zhang and C. Schlegel, “Controlling the error ﬂoor in LDPC decoding,” IEEE
Trans. Commun., vol 61, no. 9, pp. 3566–3575, Sept. 2013.

Chapter 8
Turbo Coding: Basic Principles
8.1
Introduction
The discovery of turbo codes introduced a paradigm shift in the error control coding
discipline, which fundamentally changed the way it will be thought of and practiced.
This shift moved focus away from maximum-likelihood decoding approaches and their
approximations to message-passing decoding in the code’s connectivity network, often
represented by the code’s graphical representation or the Tanner graph discussed earlier
in this book. The presentation, at the International Conference on Communications in
1993, of implementable error control codes that achieve Shannon’s bound to within half a
dB of signal-to-noise ratio was met with a degree of skepticism by a community accustomed
to the view that the Shannon bound represented an ideal, yet unachievable, limit.
The “near-capacity” performance of turbo codes and their novel iterative decoding
algorithm thus stimulated an explosion of research eﬀorts to understand this new coding
scheme.
From this emerged two fundamental questions regarding turbo codes.
First,
assuming optimum or near optimum decoding can be performed, why did turbo codes
perform so well, and how does the iterative decoder work [23, 52, 80]?
In practical systems, coding gains have always been limited by the technology available
for implementation. That is, while powerful error control codes at the time of the earliest
applications of FEC coding were known, it was not feasible to implement them at the
time. As technology advanced, so did the ability to implement complex coding schemes as
demonstrated by the Big Viterbi Decoder (BVD), built by the Jet Propulsion Laboratory,
which uses 214 states [70], and represented the most complex decoder realized in hardware
at that date. While the use of increasingly more complex coding schemes did reduce the
gap between real system performance and theoretical limits (see Sections 1.5 and 1.6),
these gains were not as dramatic as expected and a “law of diminishing returns” was
suspected to be at work.
351
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

352
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
The signiﬁcance of turbo codes thus can be appreciated upon observing their perfor-
mance. Figure 8.1 shows simulation results of the original rate R = 1/2 turbo code pre-
sented in [19], along with simulation results of a maximum free distance (MFD) R = 1/2,
memory ν = 14 ((2, 1, 14)) convolutional code with Viterbi decoding. The convolutional
code belongs to the family of codes chosen for the Galileo mission and decoded using the
BVD. The turbo code outperforms the (2, 1, 14) code by 1.7 dB at a bit error rate (BER) of
10−5. This comparison is stunning, especially since a detailed complexity analysis reveals
that the complexity of the turbo decoder is, in fact, smaller than that of the BVD. The
comparison of these simulation results raises two burning questions regarding the perfor-
mance of turbo codes. What are the underlying processes that allow a turbo code to have
a BER that achieves 10−5 at a signal-to-noise ratio (SNR) of 0.7 dB—a mere 0.5 dB away
from the Shannon capacity for the BPSK constellation used—only to then ﬂare out into
an “error ﬂoor,” where it hardly improves even with massively higher signal power? While
the turbo code is large, we will see that the implementation complexity of its decoder is
nonetheless quite moderate and signiﬁcantly smaller than that for the large convolutional
code.
(2,1,14) Convolutional Code
Free Distance Asymptote (2,1,14)
(37,21,65,536) Turbo Code
Free Distance Asymptote, TC
0
0.5
1
1.5
2
2.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.1: Simulated performance of the original turbo code and the code’s free distance
asymptote and the simulated performance of the (2, 1, 14) MFD convolutional code and
its free distance asymptote.

8.2. PARALLEL CONCATENATED CONVOLUTIONAL CODES
353
This chapter addresses turbo codes, or parallel concatenated convolutional codes (PC-
CCs), in detail. We begin with a simple example of a turbo encoder in order to introduce
the fundamental structure of parallel concatenated convolutional codes. This is followed
by a structural comparison of the convolutional (2, 1, 14) code with a speciﬁc turbo code
based on the original Berrou code. This comparison leads us to the conclusion that the
distance spectrum, and in particular the free distance, discussed in Chapter 3 for trellis
codes, is the appropriate tool to understand the behavior of turbo codes in the error ﬂoor
region [55]. This eﬀort leads to an interpretation that applies not only to turbo codes, but
also lends insight into designing codes in general.
We then present a more general analysis of turbo codes using transfer functions and
the notion of a probabilistic “uniform” interleaver. These techniques for analyzing the
performance of turbo codes using transfer functions were pioneered in [9], [12], and [30].
This analysis then reveals basic design rules for good turbo codes.
Having addressed the performance in the error ﬂoor region, we then proceed to the
study of the “turbo cliﬀ” region. The performance of the codes in the turbo cliﬀregion,
in particular, the onset of the turbo cliﬀ, requires an entirely diﬀerent analysis tool. It
is based on the statistical behavior of the component codes and is as such bound to the
iterative decoding algorithm. Thus, at that point we take a break from analysis and discuss
the details of the iterative decoding algorithm that makes these codes implementable.
The iterative behavior of the component decoders will be captured by measuring their
improvement of the mutual information at each iteration. These measurements will be
quantiﬁed by the extrinsic information exchange (EXIT) charts [74]–[76], discussed later
in this chapter in Section 8.7. Finally, we introduce serial concatenation as a turbo coding
alternative and discuss the application of turbo coding methods to band-limited signaling
using larger signal dimensions.
8.2
Parallel Concatenated Convolutional Codes
An encoder for a classical turbo code consists of the parallel concatenation of two or more,
usually identical, rate R = 1/2 convolutional encoders, realized in recursive systematic or
systematic feedback form,1 and a pseudorandom interleaver as illustrated in Figure 8.2.
This encoder structure is called a parallel concatenation because the two encoders operate
on the same block of input bits as compared to traditional serial concatenation where the
second encoder operates on the output bits of the ﬁrst encoder. Let us consider turbo en-
coders with two identical component convolutional encoders, though this encoding method
1The recursive systematic form is a special case of the controller canonical form while the systematic
feedback form is the observer canonical form. See Sections 4.4 and 4.9.

354
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
can easily be extended to the case where diﬀerent encoders or three or more encoders are
used [29]. However, the major conclusions and performance results are achievable with
only two component encoders.
The interleaver in Figure 8.2 is used to permute the input bits such that the two
encoders are operating on the same block of input bits, but in a diﬀerent order. The ﬁrst
encoder directly receives the input bit ur and produces the output pair (ur, v(1)
r ) while
the second encoder receives the input bit u′
r and produces the output pair (u′
r, v(2)
r ), of
which u′
r is discarded. The sequence of input bits are grouped into ﬁnite length blocks
whose length, N, equals the size of the interleaver. Since both encoders are systematic
and operate on the same set of input bits, it is only necessary to transmit the input bits
once, and the overall code has rate R = 1/3. In order to increase the rate of the code to
R = 1/2, the two parity sequences, v(1)(D) and v(2)(D), can be punctured, typically by
alternately deleting v(1)
r
or v(2)
r . We will refer to a turbo code whose component encoders
have parity-check polynomials h0(D) and h1(D), and whose interleaver is of length N, as
an (h0, h1, N) turbo code, where h0 and h1 are the octal representations of the connector
polynomials. Note that in the literature N is frequently called the block length of the code
even though this terminology is inconsistent with traditional deﬁnitions of block length.
In fact, the codelength n = N/R.
++
ur
u′
r
Interleaver
Rate 1/2 Systematic
Convolutional
Encoder
Rate 1/2 Systematic
Convolutional
Encoder
Puncturing
v(1)
r
v(2)
r
Systematic bit: v(0)
r
= ur
h1(D) = D
h0(D) = 1 + D2
Figure 8.2: Block diagram of a turbo encoder with two component encoders and an op-
tional puncturing device to increase the code rate above the base rate of R = 1/3.

8.2. PARALLEL CONCATENATED CONVOLUTIONAL CODES
355
In order to illustrate the basic principles of parallel concatenation, consider the en-
coder shown in Figure 8.2 with (2, 1, 2) constituent encoders with parity-check polynomials
h0(D) = 1 + D2 ≡5 and h1(D) = D ≡2. For purposes of illustration, assume a small
pseudorandom interleaver of size N = 16 bits which generates a (5, 2, 16) turbo code. This
interleaver can be realized as a length-16 buﬀer which is ﬁlled sequentially, with the input
bits ur. Once the buﬀer has been ﬁlled, the input bits to the second encoder, u′
r, are
obtained by reading the interleaver in a pseudorandom manner until each bit has been
read once and only once. The pseudorandom nature of the interleaver in our example is
represented by the permutation Π16 = {15, 10, 1, 12, 2, 0, 13, 9, 5, 3, 8, 11, 7, 4, 14, 6}, which
implies u′
0 = u15, u′
1 = u10, and so on.
If {u0 . . . u15} = {1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0} is the input sequence, and the in-
terleaver is represented by the permutation Π16, then the sequence u′ = Π16({u0 . . . u15}) =
{0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0} is the input to the second encoder. The trellis dia-
grams for both constituent encoders with these inputs are shown in Figure 8.3.
The
corresponding unpunctured parity sequences are {0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0} for
the ﬁrst encoder and {0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1} for the second encoder. The
resulting codeword has Hamming weight d = w(u) + w(v(1)) + w(v(2)) = 4 + 3 + 3 = 10
without puncturing. If the code is punctured up to rate R = 1/2 beginning with v(1)
0 , then
the resulting codeword has weight d = 4+3+2 = 9. If, on the other hand, the puncturing
begins with v(2)
0 , then the punctured codeword has Hamming weight 4 + 0 + 1 = 5.
States
s3
s2
s1
s0
s3
s2
s1
s0
States
Figure 8.3: Examples of detours in the constituent encoders.
This simple example illustrates several salient points concerning the structure of the
codewords in a turbo code. First, because the pseudorandom interleaver permutes the
input bits, the two input sequences u and u′ are almost always diﬀerent, though of the
same weight, and the two encoders will (with high probability) produce parity sequences
of diﬀerent weights. Second, it is easily seen that a codeword may consist of a number
of distinct detours in each encoder, as illustrated in Figure 8.3. Since the constituent

356
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
encoders are realized in recursive systematic form a non-zero input bit is required to
return the encoder to the all-zero state and thus all detours are associated with information
sequences of weight 2 or greater, at least 1 for departure and 1 for the merger. Finally,
with a pseudorandom interleaver it is highly unlikely that both encoders will be returned
to the all zero state at the end of the codeword even when the last ν bits of the input
sequence u may be used to force the ﬁrst encoder back to the all zero state, i.e., the trellis
is terminated.
If neither encoder is forced to the all zero state, i.e., no tail bits are used to terminate
the ﬁrst encoder, then the sequence consisting of N −1 zeroes followed by a one is a
valid input sequence u to the ﬁrst encoder. For some interleavers, this sequence will be
permuted to itself and u′ = u. In that case, the maximum weight of the codeword with
puncturing, and thus the free distance of the code, will be only two! For this reason,
it is common to terminate the ﬁrst encoder in the all-zero state. The ambiguity of the
ﬁnal state of the second encoder has been shown by simulation to result in negligible
performance degradation for large interleavers [1, 57]. Special interleaver structures that
result in both encoders returning to the all zero state are discussed in [3], [4], and [47].
Finding the free distance and distance spectrum of a turbo code is complicated by
the fact that parallel concatenated convolutional codes are, in general, time-varying codes
due to the interleaver. That is, for the shifted sequence2 Du(D) the ﬁrst parity sequence
is Dv(1)(D), but the input sequence to the second component encoder is not Du′(D)
(with high probability) due to the interleaver. Thus the second parity sequence is not
Dv(2)(D). Continuing with the example in Figure 8.3, if Du(D) is the input sequence,
then Π16(Du(D)) = {1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0} and the second parity sequence
is {0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0}. Thus, time shifting the input bits results in code-
words that diﬀer in both bit position and overall Hamming weight. In the next section we
will see that this simple observation has a profound eﬀect.
8.3
Distance Spectrum Analysis of Turbo Codes
In Chapter 5, the bounds on the performance of trellis codes were developed assuming an
inﬁnite-length trellis. The result is the bound of Equation (5.71) which gives the average
information bit error rate per unit time. In order to make clear the distinction between
turbo codes and convolutional codes, it is useful to consider both codes as block codes and
to redevelop the performance bounds from this point of view. To this end, the information
sequences are restricted to length N. With ﬁnite-length information sequences of length
N, a (2, 1, ν) convolutional code may be viewed as a block code with 2N codewords of
length 2(N + ν). The last ν bits input to the encoder are used to force the encoder back
2We will use the D-transform notation for sequences whenever more convenient, such as to describe
shifting of sequences.

8.3. DISTANCE SPECTRUM ANALYSIS OF TURBO CODES
357
to the all zero state, that is, to terminate the encoder, and are referred to as the tail.
The bit error rate performance of a ﬁnite-length convolutional code with maximum
likelihood (ML) decoding and binary antipodal signaling on an additive white Gaussian
noise (AWGN) channel with an SNR of Eb/N0 is upper bounded by
Pb ≤
2N

i=1
wi
N Q

di
2REb
N0

,
(8.1)
where wi and di are the information weight and total Hamming weight, respectively, of
the ith codeword. Collecting codewords of the same total Hamming weight and deﬁning
the average information weight per codeword as
˜wd = wd
Nd
,
where wd is the total information weight of all codewords of weight d and Nd is the number,
or multiplicity, of codewords of weight d, yields
Pb ≤
∞

d=dfree
Nd ˜wd
N
Q

d2REb
N0

,
where dfree is the free distance of the code.
If a convolutional code has N0
d codewords of weight d caused by information sequences
u(D) whose ﬁrst one occurs at time 0, then it also has N 0
d codewords of weight d caused by
the information sequences Du(D), N 0
d codewords of weight d caused by the information
sequences D2u(D), and so on. Thus, for dfree ≤d ≤2dfree −1, we have
lim
N→∞
Nd
N = N0
d
and
lim
N→∞˜wd = lim
N→∞
wd
Nd
= w0
d
N0
d
= ˜w0
d,
where w0
d is the total information weight of all codewords with weight d which are caused
by information sequences whose ﬁrst one occurs at time 0. For d ≥2dfree, the situation
is somewhere more complicated due to the fact that codewords can consist of multiple
error events. That is, as N →∞, Nd = N · N0
d + Nd(m) where Nd(m) is the number
of codewords of weight d made up of m error events.
As we are primarily interested
in low weight codewords, we will not try to develop more precise expressions for these
multiplicities.

358
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
The bound on the BER of a ﬁnite length terminated convolutional code with ML
decoding becomes
Pb
≤
2·dfree−1

d=dfree
N0
d ˜w0
dQ

d2REb
N0

+
∞

d=2·dfree
Nd ˜wd
N
Q

d2REb
N0

=
2·dfree−1

d=dfree
w0
dQ

d2REb
N0

+
∞

d=2·dfree
Nd ˜wd
N
Q

d2REb
N0

,
(8.2)
which is similar to the standard union bound for ML decoding as developed in Chapter
5. For this reason, eﬀorts to ﬁnd good convolutional codes for use with ML decoders have
focused on ﬁnding codes that maximize the free distance dfree and minimize the number
of free distance paths N0
free for a given rate and total encoder memory.
The performance of a turbo code with maximum likelihood decoding is also bounded
by (8.1). Collecting codewords of the same total Hamming weight, the bound on the BER
for turbo codes also becomes
Pb ≤
∞

d=dfree
Nd ˜wd
N
Q

d2REb
N0

.
(8.3)
However, in the turbo encoder the pseudorandom interleaver maps the input sequence
u(D) to u(D) and the input sequence Du(D) to the sequence (Du(D)) that is diﬀerent
from Du(D) with very high probability.
Thus, unlike convolutional codes, the input
sequences u(D) and Du(D) produce codewords with diﬀerent Hamming weights. As a
consequence of this, Nd ˜wd is much less than N for low-weight codewords. This is due to the
pseudorandom interleaver which maps low-weight parity sequences in the ﬁrst component
encoder to high weight parity sequences in the second component encoder.
Thus, for
low-weight codewords we typically ﬁnd
˜wdNd
N
 1,
where
Nd
N
(8.4)
is called the eﬀective multiplicity of codewords of weight d.
8.4
The Free Distance of a Turbo Code
For moderate and high signal-to-noise ratios, it is well known that the free distance term
in the union bound on the bit error rate performance dominates the bound. Thus, the

8.4. THE FREE DISTANCE OF A TURBO CODE
359
asymptotic performance of convolutional and turbo codes with ML decoding approaches
Pb ≈Nfree ˜wfree
N
Q

dfree
2REb
N0

,
(8.5)
where Nfree is the error coeﬃcient and ˜wfree is the average weight of the information
sequences causing free distance codewords. The right-hand side of Equation (8.5) and its
associated graph is called the free distance asymptote, Pfree, of a code.
As the error ﬂoor in Figure 8.1 occurs at moderate to high SNRs, we endeavor to
ﬁnd the free distance of turbo codes. Algorithms for ﬁnding the free distance of turbo
codes are described in [69] and [37]. These algorithms were applied to a turbo code with
the same component encoders, puncturing pattern, and interleaver size N as in [19] and
a particular pseudorandom interleaving pattern. The parity-check polynomials for this
code are h0 = D4 + D3 + D2 + D + 1 and h1 = D4 + 1, or h0 = 37 and h1 = 21 using
octal notation. This (37, 21, 65, 536) code was found to have Nfree = 3 paths with weight
dfree = 6. Each of these paths was caused by an input sequence of weight 2 and thus
˜wfree = 2. Though this result was for a particular pseudorandom interleaver, a similar
result occurs for most pseudorandom interleavers with N = 65,536.
For this particular turbo code, the free distance asymptote is given by
Pfree =
3 · 2
65, 536Q

62 · 0.5 · Eb
N0

,
where the rate loss due to the addition of a 4-bit tail to terminate the ﬁrst encoder is
ignored and
Nfree
N
=
3
65,536
is the eﬀective multiplicity. The free distance asymptote is shown plotted in Figure 8.1
along with simulation results for this code using the iterative decoding algorithm with 18
iterations, from which it can clearly be seen that the simulation results do in fact approach
the free distance asymptote for moderate and high SNRs. Since the slope of the asymptote
is essentially determined by the free distance of the code, it can be concluded that the
error ﬂoor observed with turbo codes is due to the fact that they have a relatively small
free distance and consequently a relatively ﬂat free distance asymptote.
Further examination of expression (8.5) reveals that the manifestation of the error
ﬂoor can be manipulated in two ways. First, increasing the length of the interleaver while
preserving the free distance and the error coeﬃcient will lower the asymptote without

360
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
N = 65, 536, 18 iterations
N = 16384, 18 iterations
N = 1024, 18 iterations
(37, 21)
0
0.5
1
1.5
2
2.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.4: Simulations illustrating the eﬀect of the interleaver size on the performance of
a turbo code with random interleaving.
changing its slope by reducing the eﬀective multiplicity. In this case, the performance
curve of turbo codes does not ﬂatten out until slightly higher SNRs and lower BERs are
reached. Conversely, decreasing the interleaver size while maintaining the free distance and
error coeﬃcient results in the error ﬂoor being raised and the performance curve ﬂattens
at slightly lower SNRs and higher BERs. This can be seen in simulation results for the
turbo code with varying N shown in Figure 8.4. Due to the steepness of the performance
curve in turbo cliﬀregion, very small changes in the SNR result in its onset.
If the ﬁrst constituent encoder is not forced to return to the all-zero state and the
weight 2 codewords described at the end of Section 8.2 are allowed, then the error ﬂoor is
raised to the extent that the code performs poorly even for large interleavers. If the size of
the interleaver is ﬁxed, then the error ﬂoor can be modiﬁed by increasing the free distance
of the code while preserving the multiplicity coeﬃcient. This changes the slope of the
free-distance asymptote. That is, increasing the free distance increases the slope of the
asymptote and decreasing it decreases that slope. Thus, one cannot completely disregard
free distance when constructing turbo codes. Techniques for increasing the free distance
of turbo codes are discussed in the next section, and primarily depend on the interleaved
and its design, which will be discussed in detail in the next chapter.

8.4. THE FREE DISTANCE OF A TURBO CODE
361
Note: If we compare the error ﬂoor of turbo codes to those for LDPC codes discussed
in Chapter 6, we ﬁnd that the mechanics of the former are much simpler. Even though
the iterative decoding discussed shortly is not a maximum-likelihood method in the
strict sense, the error events causing the error ﬂoor in turbo codes are nonetheless
the low-weight codewords. This is in marked contrast to the LDPC codes, where the
error ﬂoor was caused by a combination of structural and computational eﬀects.
The role that the free distance and eﬀective multiplicity play in determining the asymp-
totic performance of a turbo code is further clariﬁed by examining the asymptotic per-
formance of a convolutional code. The free distance asymptote of a convolutional code is
given by the ﬁrst term in the union bound of Equation (8.2). The maximum free distance
(2, 1, 14) code whose performance is shown in Figure 8.1 has dfree = 18, N0
free = 18, and
w0
free = 137 [24]. Thus, the free distance asymptote for this code is
Pfree = 137 · Q

182 · 0.5 · Eb
N0

,
which is also shown in Figure 8.1.
As expected, the free distance asymptote of the (2, 1, 14) code is much steeper than
the free distance asymptote of the turbo code due to the increased free distance. However,
because the eﬀective multiplicity of the free distance codewords of the turbo code, given
by (8.4), is much smaller than the multiplicity of the (2, 1, 14) code, the two asymptotes
do not cross until the SNR is much greater than Eb/N0 = 2.5 dB. and the BER of both
codes is lower than the targeted BER of many practical systems. Thus, even though the
(2, 1, 14) convolutional code is asymptotically much better than the (37, 21, 65, 536) turbo
code, the latter substantially outperforms the former at error rates and SNR values at
which many systems operate.
To emphasize the importance of using a pseudorandom interleaver with turbo codes, we
now consider a turbo code with a regular rectangular interleaver. The same constituent
encoders and puncturing pattern as in [19] are used in conjunction with a 120 × 120
rectangular interleaver.
This rectangular interleaver is realized as a 120 × 120 matrix
into which the information sequence u is written row by row.
The input sequence to
the second encoder u′ is then obtained by reading the matrix column by column.
A
120 × 120 rectangular interleaver implies an interleaver size of N = 14, 400 and thus this
is a (37, 21, 14, 400) turbo code.
Using the algorithm described in [69], this code was found to have a free distance of
dfree = 12 with a multiplicity of Nfree = 28, 900! For this code, each of the free distance
paths is caused by an information sequence of weight 4, so ˜wfree = 4. The free distance
asymptote for this code is thus given by
Pfree = 28, 900 · 4
14, 400 Q

122 · 0.5 · Eb
N0

.

362
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
The free distance asymptote is plotted in Figure 8.5 along with simulation results using
the iterative decoding algorithm of [19] with 18 iterations. This ﬁgure clearly shows that
the free distance asymptote accurately estimates the performance of the code for moderate
and high SNRs.
This code achieves a bit error rate of 10−5 at an SNR of 2.7 dB and thus performs 2 dB
worse than the (37, 21, 65, 536) turbo code with a pseudorandom interleaver even though
it has a much larger free distance. The relatively poor performance of the (37, 21, 14, 400)
turbo code with a rectangular interleaver is due to the large multiplicity of dfree paths.
This results in an eﬀective multiplicity of
Nfree
N
= 28,900
14,400 ≈2,
which is much larger than the eﬀective multiplicity of the random (37, 21, 14, 400) turbo
code. We now show that the large multiplicity is a direct consequence of the use of the
rectangular interleaver and that, furthermore, increasing the size of the interleaver does not
result in a signiﬁcant reduction in the eﬀective multiplicity of the free distance codewords.
(37, 21, 14, 400) Random
(37, 21, 14, 400) Rectangular
0
0.5
1
1.5
2
2.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.5: Comparison of simulated performance and free distance asymptote for a turbo
code with a rectangular interleaver with that of a turbo code with a random interleaver.

8.4. THE FREE DISTANCE OF A TURBO CODE
363
The free distance paths in the turbo code with the rectangular interleaver are due to
four basic information sequences of weight 4. These information sequences are depicted
in Figure 8.6 as they would appear in the rectangular interleaver. The “square” sequence
in Figure 8.6a depicts the sequence u = 1, 0, 0, 0, 0, 1, 0594, 1, 0, 0, 0, 0, 1, 0∞, where 0594
denotes a sequence of 594 consecutive zeroes and 0∞represents a sequence of zeroes that
continues to the end of the information sequence. In this case, the rectangular interleaver
maps the sequence u to itself and therefore u = u. The sequence u results in a parity
sequence v(1) from the ﬁrst constituent encoder which, after puncturing, has weight 4.
Similarly, the input sequence u = u results in a parity sequence v(2) from the second
constituent encoder which, after puncturing, also has weight 4. The weight of the codeword
is then dfree = 4 + 4 + 4 = 12. Since the “square” sequence in Figure 8.6a can appear in
(
√
N −5) × (
√
N −5) = 13, 225 distinct positions in the rectangular interleaver, and in
each case u = u and a codeword of weight dfree = 12 results, this results in 13,325 free
distance codewords. Note that for every occurrence of the “square” sequence to result in
a codeword of weight 12 the weight of both parity sequences must be invariant to which
is punctured ﬁrst.
The rectangular patterns in Figures 8.6b and 8.6c also result in weight 12 codewords.
For the corresponding two input sequences, the weight of one of the parity sequences is
aﬀected by whether or not it is punctured ﬁrst and only every other position in which the
rectangular patterns appear in the interleaver results in a codeword of weight dfree = 12.
Thus, the input sequences for the patterns in Figure 8.6b and Figure 8.6c each result in
0.5(
√
N −10) × (
√
N −5) = 6, 325 free distance codewords. For the sequence in Figure
8.6d, the weight of both parity sequences is aﬀected by which is punctured ﬁrst and only
one out of four positions in which the rectangular pattern appears in the interleaver results
in a codeword of weight dfree = 12. Consequently, this sequence results in 0.25(
√
N −10)×
(
√
N −10) = 3, 025 free distance codewords. Summing the contributions of each type of
sequence results in a total of Nfree = 28, 900 codewords of weight dfree = 12.
It is tempting to try to improve the performance of a turbo code with a rectangular
interleaver by increasing the size of the interleaver. However, all of the information se-
quences shown in Figure 8.6 would still occur in a larger rectangular interleaver, so the
free distance cannot be increased by increasing N. Also, since the number of free distance
codewords is on the order of N, increasing the size of the interleaver results in a cor-
responding increase in Nfree such that the eﬀective multiplicity Nfree/N does not change
perceptibly. We thus have the somewhat strange situation that even if N →∞, the perfor-
mance of the code in terms of bit error rate is practically unchanged. Without the beneﬁt
of a reduced eﬀective multiplicity, the free distance asymptote, and thus the error ﬂoor, of
turbo codes with rectangular interleavers is not lowered enough for them to manifest the
excellent performance of turbo codes with pseudorandom interleavers. Attempts to design
interleavers for turbo codes generally introduce structure to the interleaver and thus can
destroy the very randomness that results in such excellent performance at low SNRs.

364
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
0   0   0   0   0   0   0   0
0   1   0   0   0   0   1   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   1   0   0   0   0   1   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   1   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   1   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0  
0   0   0   1   0 
0   0   0   0   0 
0   0   0   0   0  
0   0   0   0   0   
0   0   0   0   0 
0   0   0   1   0  
0   0   0   0   0 
0   0   0   0   0   0   0   0
0   1   0   0   0   0   1   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   1   0   0   0   0   1   0
0   0   0   0   0   0   0   0
0   1   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   1   0 
0   0   0   0   0 
0   0   0   0   0 
0   0   0   0   0 
0   0   0   0   0 
0   0   0   0   0 
0   0   0   0   0  
0   0   0   0   0 
0   0   0   0   0   0   0   0
0   1   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0  
0   0   0   1   0 
0   0   0   0   0 
b n
retta
p
a n
retta
p
d n
retta
p
c n
retta
p
These positions odd
These positions even
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0
0   0   0   0   0  
0   0   0   0   0 
Figure 8.6: Input information sequences causing dfree codewords in a turbo code with a
rectangular interleaver.
8.5
Weight Enumerator Analysis of Turbo Codes3
In this section we take a closer look at the distance spectrum of turbo codes and its
eﬀect on their performance in the error region of interest. We will be using the method
of weight enumerator functions introduced in Section 5.9. For speciﬁc codes the distance
3This section can be omitted at ﬁrst reading without loss of continuity.

8.5. WEIGHT ENUMERATOR ANALYSIS OF TURBO CODES
365
spectrum has to be obtained via computer search.
Though illuminating, ﬁnding even
limited distance spectrum data is computationally expensive even for moderate interleaver
lengths. In addition, the results for a particular turbo code do not easily generalize to
the entire class of codes. In this section, the observations made concerning the distance
spectrum are formalized using a weight enumerating function analysis. In order to simplify
the notation and discussion, the turbo codes are used with identical component codes and
without puncturing. The extension to punctured codes is straightforward [69]. As with
convolutional codes and trellis codes, the primary result of this analysis will be insight
into the design of the codes rather than computable bounds on the performance.
The bound of (8.3) requires knowledge of the complete distance spectrum of a turbo
code. For parallel concatenated codes, it will be convenient to rewrite the distance spec-
trum information in the form of the Input Output Weight Enumerating Function (IOWEF)
A(W, X) =
∞

d=dfree

w
Aw,dW wXd,
(8.6)
where Aw,d is the number of codewords of weight d caused by information sequences of
weight w. Finding the IOWEF for conventional convolutional codes is frequently done
via transfer function methods as was discussed in Chapter 3. Unfortunately, ﬁnding the
exact transfer function of a turbo code is infeasible for all but trivial turbo codes due to
the pair-wise state complexity introduced by the interleaver.
In order to obtain a more manageable technique, we will compute an approximate WEF
based on the notion of a probabilistic uniform interleaver, which was introduced in [9, 11].
The fundamental idea of uniform interleaving is to consider the performance of a turbo
code averaged over all possible pseudorandom interleavers of a given length. For a given
N, there are N! possible pseudorandom interleavers and, assuming a uniform distribution,
each occurs with probability
1
N!. Let a particular interleaver map an information sequence
u of weight w to an information sequence u, also of weight w. Then there are a total of
w!(N −w)! interleavers in the ensemble of N! interleavers that perform this same mapping.
Thus, the probability that such a mapping and, hence, that the codeword that results from
the input sequences u and u occurs is
w!(N −w)!
N!
=
1
 N
w
.
(8.7)
Note that, as was the case in the random coding arguments used to prove the Shannon
capacity for trellis codes, the assumption of a uniform distribution of the interleavers is
chosen in the random interleaving argument. The derivations that follow, however, do not
constitute an existence argument as in the random coding bounds, since usingt diﬀerent
interleavers for diﬀerent weight input sequences is not excluded in the analysis.

366
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
This probabilistic interleaving argument is now used to connect the codewords of the
ﬁrst component encoder in the turbo encoder to the codewords in the second component
encoder. As (8.7) is a function of the weight of the information sequence, and parallel
concatenation demands this, some additional weight enumeration functions are necessary.
Following [12], deﬁne the Input Redundancy Weight Enumerating Function (IRWEF)
of a systematic component encoder as
AC(W, Z) =

w

z
AC
w,zW wZz,
where AC
w,z is the number of codewords of weight d = w + z generated by input sequences
of weight w and with parity sequences of weight z. Given the IRWEF of an encoder, the
IOWEF is easily found by adding the information and parity weights.
Restricting attention to information sequences of a particular weight w results in the
Conditional Weight Enumerating Function (CWEF)
AC
w(Z) =

z
AC
w,zZz.
AC
w(Z), then, is a polynomial representation of that portion of the distance spectrum of
the component code which is due to input sequences of weight w. Given the CWEFs of a
component encoder, its IRWEF is found as
AC(W, Z) =

w
W wAC
w(Z).
The goal is now to develop a relationship between the CWEFs of the component encoders
and the CWEF Aw(Z) for the overall turbo code, which may be used to bound the bit
error rate of the turbo code by
Pb ≤
N

w=wmin
w
N W wAw(Z)

W=Z=e−REb/No,
(8.8)
where wmin is the minimum weight information sequence that generates a codeword in the
terminated convolutional component code.
Using the uniform interleaver, the overall CWEF of a turbo code can now be computed
as
Aw(Z) = AC
w(Z) · AC
w(Z)
 N
w

= [AC
w(Z)]2
 N
w
 ,
where, again, we have assumed identical component encoders. The CWEF can in turn be
used to compute the transfer function bound described in Chapter 5 on the performance of

8.5. WEIGHT ENUMERATOR ANALYSIS OF TURBO CODES
367
a speciﬁc code. However, if we seek insight into turbo code design and other more general
results, additional manipulations are necessary.
This additional manipulation is primarily a consequence of the diﬀerences between
viewing a convolutional code as having an inﬁnite trellis and viewing a terminated convo-
lutional code as a block code. In the former, codewords are frequently interpreted as error
events, i.e., a path that diverges from the all zero state at a speciﬁc time and remerges for
the ﬁrst time some number of branches later. As discussed in Section 8.2, the latter view-
point results in codewords that are the concatenation of detours. This is the viewpoint
that we will ﬁnd useful in the subsequent development.
In order to obtain a more useful expression for AC
w(Z), we begin by developing an
approximate expression for the CWEF of the component encoders. Let
A(n)
w (Z) =

z
A(n)
w,zZz
be the n-event enumerating function of a component encoder. Here, A(n)
w,z represents the
number of codewords consisting of the concatenation of n detours with total information
weight w and total parity weight z. Unlike in Section 8.2, we further constrain concatena-
tion to not allow strings of zeros between the detours. Finally, assuming that the length
of the detours of interest are short compared to the interleaver length N, the CWEF of
the component encoder can be approximated as
AC
w(Z) ≈
nmaxw

n=1
 N
n

A(n)
w (Z),
where nmaxw is the maximum number of detours in a weight-w information sequence.
These detours can now be positioned by an interleaver of length N to arbitrary locations.
The overall CWEF of the turbo code with uniform interleaving can now be approxi-
mated by the combination of the pieces from both encoders as
Aw(Z) ≈
nmaxw

n1=1
nmaxw

n2=1
 N
n1
  N
n2

 N
w

A(n1)
w
(Z)A(n2)
w
(Z).
This approximation can be further simpliﬁed by approximating the binomial coeﬃcient
by N n/n! and by approximating each term in the summations by the maximum values of
n1 and n2, respectively. Doing this yields
Aw(Z) ≈
w!
(nmaxw!)2 N2nmaxw−w [Anmaxw
w
]2 .

368
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
Finally, using this approximation in the bound of (8.8) leads to
Pb ≈
N

w=wmin
W w · w ·
w!
(nmaxw!)2 N2nmaxw−w−1 [Anmaxw
w
(Z)]2
W=Z=e−REb/No
(8.9)
The approximation (8.9), although unwieldy, is now in a form that can be used to develop
design rules for turbo codes.
We begin by considering the eﬀect of the choice of the component encoder realization
on the performance of the overall code. If the component encoders are realized in non-
systematic feedforward form, then wmin = 1 and the maximum number of distinct error
events is nmaxw = w. Using this along with the approximation A(w)
w (Z) ≈[A(1)
1 (Z)]w in
(8.9) yields [12]
Pb ≈
N

w=1
W w
1
(w −1)!N w−1 
A(1)
1 (Z)
2w
W=Z=e−REb/No
.
This expression reveals that for non-recursive component encoders, the exponent of the
interleaver length N is always non-negative. That is, the eﬀective multiplicity loses the
factor of 1/N, and one of the primary reasons for the low bit error rates of turbo codes
is lost. This is intuitively satisfying since information sequences of weight 1 do, in fact,
get interleaved to delayed versions of themselves regardless of the interleaver type and
thus the resulting codewords will have high multiplicity. In addition, unlike the example
in Section 8.2, the ﬁrst and second encoders will then always generate the same parity
sequence.
The result is quite diﬀerent if recursive component encoders are used. In this case,
wmin = 2 and nmaxw = w/2. Following [12], the cases for w odd and even are treated
separately. For w = 2j + 1 odd, it is straightforward to show that the summand in (8.9)
becomes
(2j + 1)(j + 1) ·
 2j + 1
j

N−2 · W (2j+1)[A(j)
2j+1(Z)]2,
which is negligible for large interleavers due to the factor N−2, and we will no longer
consider odd weight sequences. For w = 2j, the summand becomes
(2j) ·
 2j
j

N−1 · W (2j)[A(j)
2j (Z)]2,
which has an interleaver gain factor of N−1. Thus, recursive component encoders manifest
a reduced multiplicity and an interleaver gain for all input sequences. This leads to the
ﬁrst, well-accepted design rule for turbo codes.

8.5. WEIGHT ENUMERATOR ANALYSIS OF TURBO CODES
369
Rule 1: Choose component encoders with feedback.
In order to develop the other design rules, attention is now focused only on those
information sequences of even weight since they have the smallest interleaver gain. Ap-
proximation of (8.9) then becomes
Pb ≈
N/2

j=1
(2j) ·
 2j
j

N−1 · W 2j [A(j)
2j (Z)]2
W=Z=e−REb/N0 .
(8.10)
The analysis of this expression is further simpliﬁed through use of the approximation
A(j)
2j (Z) ≈A(1)
2 (Z)j, which reduces the problem to a consideration of codewords associ-
ated with information sequences of weight-2 only. It is now necessary to brieﬂy digress
and explore the structure of codewords generated by weight 2 information sequences in
recursive encoder realizations.
In recursive encoders, codewords generated by weight 2 information sequences must
correspond to detours with a ﬁrst 1 required to leave the all-zero state and a second 1
required to return to the all-zero state. It is well known from the theory of linear feedback
shift registers that blocks of additional intermediate zeroes must traverse a cycle, that is,
a path beginning and ending in the same nonzero state, in the encoder state diagram.
Denote by zcycle the parity weight gained by traversing such a cycle. Then the overall
parity weight of a codeword generated by a weight 2 information sequence is of the form
k · zcycle + t,
(8.11)
where k is an integer and t is parity weight achieved on the diverging and remerging
transitions caused by the input 1s. Clearly, the minimum value of (8.11) is
zmin = zcycle + t,
which occurs when k = 1.
Returning to our analysis, we can now write
A(1)
2
≈
Zzmin + Z2zmin−t + Z3zmin−2t . . .
=
Zzmin
1 −Zzmin−t ,
and substituting this expression into (8.10) yields
Pb
≈
N/2

j=1
(2j) ·
 2j
j

N−1 · W 2j 
A(1)
2 (Z)
2j
W=Z=e−REb/No
=
N/2

j=1
(2j) ·
 2j
j

N−1 · W 2j

Zzmin
1 −Zzmin−t
2j
W=Z=e−REb/No
.

370
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
Finally letting W = Z results in
Pb ≈
N/2

j=1
(2j) ·
 2j
j

N−1
(Z2+2zmin)j
(1 −Zzmin−t)2j

Z=e−REb/No
,
(8.12)
determining the quantity
deﬀ= 2 + 2zmin,
which has been termed the eﬀective free distance of the turbo code [12].
The expression (8.12) demonstrates that for large interleavers, the eﬀective free dis-
tance is an important parameter aﬀecting performance in the error ﬂoor, and it is desirable
to maximize deﬀ, which is equivalent to maximizing the minimum cycle weight zmin. The
desire to maximize zmin results in two additional design rules:
Rule 2: Choose the feedback polynomial of the component encoders to be primitive.
This rule is based on the observation that the minimum cyle weight is likely to be
larger for longer cycles. Borrowing once more from the theory of linear feedback shift
registers, it is known that selecting the feedback polynomial to be primitive of degree ν
results in the maximum possible cycle length of 2ν −1. A simple proof of this rule can be
found in [12].
The third design rule is based on the observation that the eﬀective free distance occurs
when a weight 2 information sequence that generates parity weight zmin in the ﬁrst encoder
is interleaved to an information sequence that generates parity zmin in the second encoder.
In order for this to occur, the original and interleaved information sequences must corre-
spond to the shortest possible weight 2 sequence that generates a complete codeword in
both component encoders. This situation can be avoided by designing the interleaver such
that two 1’s entering the ﬁrst encoder which are separated by less than S positions are
interleaved such that they are separated by more than T positions, where T > S. More
precisely, let n1 and n2 be the indices of two 1s in the original information sequence and
let π(n1) and π(n2) be the respective indices in the interleaved sequence. An interleaver
is said to have spreading factors (S, T) if (n2 −n1) < S implies π(n2) −π(n1) ≥T. If
such an interleaver can be found and T > zmin, then this can ensure that the resulting
free distance is greater than eﬀective minimum distance. Hence:
Rule 3: Choose an interleaver with a large spreading factor.

8.6. ITERATIVE DECODING OF TURBO CODES
371
This design rule presents somewhat of a dilemma. As demonstrated in Section 8.3,
interleavers with too much regularity, such as the ubiquitous row-column interleaver, may
have good spreading factors, but actually result in poor overall code performance due to
large multiplicities. On the other hand, a pseudorandom interleaver may produce a “thin”
distance spectrum but have poor spreading factors and result in a code with a small free
distance. The solution may be found in the so-called S −−random interleavers described
in [32] which are algorithmically constructed. These interleavers begin as pseudorandom
interleavers and are manipulated to try to achieve a desired set of spreading factors. More
on interleavers and their design will be said in Chapter 9.
We now brieﬂy compare the performance of some turbo codes to illustrate the design
rules just developed. All of the codes considered are based on (2, 1, 4) component codes
with an interleaver length of N = 5000 and an overall code rate of R = 1/2. The same
puncturing pattern was used for all the codes.
The ﬁrst code is a (37, 21, 5000) code
with a pseudorandom interleaver. This code uses the same component encoders as the
original turbo code [19], which has nonprimitive feedback polynomials. This code has a
free distance of dfree = 6 with a multiplicity of 1. The second code is a (23, 33, 5000) code
with the same pseudorandom interleaver as the ﬁrst code. In this case, the component
encoder uses a primitive feedback polynomial and has dfree = 9 with a multiplicity of 1.
The last code is a (23, 33, 5000) code with a spread interleaver designed following [8]. This
code has a free distance of dfree = 18 with a multiplicity of 1.
The performance of all three of these codes is shown in Figure 8.7. Interestingly, for
low SNRs the (37, 21, 5000) code outperforms the other two codes by nearly 0.15 dB until
it manifests the error ﬂoor expected from a code with dfree = 6. The performance of the
(23, 33, 5000) codes is similar for low and moderate SNRs and both codes outperform the
(37, 21, 5000) code for BERs below 10−5. This is consistent with the higher free distance
of these codes. The code with the spread interleaver eventually outperforms the code with
the pseudorandom interleaver due to its larger free distance.
These results remind us that the design rules derived in this section, particularly
Rule 2 and Rule 3, are based on approximations that are only valid for high SNRs and
long interleaver lengths. The design rules and analysis to this point clearly do not tell
the whole story. In particular, we wish to understand the turbo cliﬀand what causes this
precipitous drop of the bit error rate that is the hallmark of turbo codes. To do this, we
must ﬁrst study the decoding algorithm, the subject of the next section.
8.6
Iterative Decoding of Turbo Codes
One quickly realizes that optimal decoding of turbo code is far too complex, and the
discoverers of turbo codes never attempted either optimal decoding or an approximation

372
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
(23; 33; 5,000) random
(23; 33; 5,000) spread
(37; 21; 5,000) random
0
0.5
1
1.5
2
2.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.7: Performance of three diﬀerent R = 1/2 turbo codes with an interleaverlength
of N = 5,000 and 20 decoder iterations.
thereof, but instead proposed a novel iterative decoder based on the a posteriori probability
(AAP) decoding algorithm of the component codes described in Chapter 5. Empirical
evidence quickly suggested that this iterative decoding algorithm performed remarkably
well in the sense that it almost always would ﬁnd the optimum decoding solution.
Even though the optimal decision rule following the maximum a posteriori decoding
principle is infeasible to implement for any code of reasonable size, it can nonetheless be
used to motivate the iterative decoding procedure invented by Berrou and Glavieux [20].
To initiate the discussion, let us start with the maximum a posteriori rule, which would
select
ˆur = max
u={0,1} P[ur = u|y(0), y(1), y(2)],
where y(0) is the received systematic bit sequence and y(m), m = 1, 2, are the received
parity sequences corresponding to the mth constituent encoder.

8.6. ITERATIVE DECODING OF TURBO CODES
373
This formula can be developed as follows:
P[ur = u|y(0), y(1), y(2)]
≡
P(y(0), y(1), y(2)|ur = u)P[ur]
=

u
(ur=u)
P(y(0), y(1), y(2)|u)P[u]
=

u
(ur=u)
P(y(2)|u)P(y(0), y(1)|u)P[u].
(8.13)
The conditioning in the second term in the summation of (8.13) can be reversed using
Bayes’ rule to obtain
P(y(0), y(1)|u)P[u] = P(y(0), y(1), u) ≡P[u|y(0), y(1)]
(8.14)
Clearly, computing (8.14) for all u is far too complex, and we simply the expression using
the distribution separation
P[u|y(0), y(1)] ≈
L

r=1
P[ur|y(0), y(1)]
where the values
P[ur|y(0), y(1)]
are seen to be the soft information produced by the ﬁrst decoder which has access only
to y(0), y(1). These are now used as a priori probability information in (8.13) delivered by
decoder #1, to which decoder #2 adds the part from received sequence y(2). The APP
calculation of (8.13) can now be approximated by
P[ur = u|y(0), y(1), y(2)]
≈

u
(ur=u)
P(y(2)|u)
L

l=1
P[ul|y(0), y(1)]
=

u
(ur=u)
P[u|y(2)]
L

l=1
P[ul|y(0), y(1)]P[ul].
In Section 5.7, it was shown that this is precisely the quantity that the APP decoder for
code #2 computes, using P[ul|y(0), y(1)]P[ul] →P[ul] as a priori probability of information
bit ul.
To proceed further it is useful to consider log-likelihood ratios of the probabilities in
question, deﬁned by
Λ(ul) = log P[ul = 1|y(0), y(1)]
P[ul = 0|y(0), y(1)],
Λs(ul) = log P[ul = 1]
P[ul = 0].

374
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
Furthermore, according to (5.27)
L(ur) = log P[ur = 1|y(0), y(1), y(2)]
P[ur = 0|y(0), y(1), y(2)]
can be calculated as
L(ur) = log

i,jA(ur=1)
γr(j, i)αr−1(i)βr(j)

i,jA(ur=0)
γr(j, i)αr−1(i)βr(j)
,
(8.15)
where the transition probability γr(i, j) is given as
γr(i, j) = P[ur|y(0), y(1)]P(y(2)
r |x(2)
r ),
(8.16)
where ur is the information bit causing the transition from i →j. Substituting (8.16) into
(8.15) and factoring yields
L(ur)
=
log

i,jA(ur=1)
P(y(2)
r |x(2)
r )αr−1(i)βr(j)

i,jA(ur=0)
P(y(2)
r |x(2)
r )αr−1(i)βr(j)
+ log P[ur = 1|y(0), y(1)]
P[ur = 0|y(0), y(1)]
=
Λ(2)
e,r + Λr,
where Λ(2)
e,r is the extrinsic information contributed by the second decoder and Λr is the a
posteriori log-likelihood ratio from the ﬁrst decoder.
Applying the same decomposition to the a posteriori probabilities produced by the
ﬁrst decoder, we obtain
L(ur) = Λ(2)
e,r + Λ(1)
e,r + Λs,
(8.17)
where
Λs = log P(y(0)
r |ur = 1)
P(y(0)
r |ur = 0)
is the a posteriori probability of the systematic bits, which are conditionally independently
distributed.
A block diagram of an iterative turbo decoder is shown in Figure 8.8, where each
APP decoder corresponds to a constituent code and generates the corresponding extrinsic
information Λ(m)
e,r for m = 1, 2 using the corresponding received sequences. The interleavers
are identical to the interleavers in the turbo encoder and are used to reorder the sequences
so that they are properly aligned at each decoder.

8.6. ITERATIVE DECODING OF TURBO CODES
375
+
+-
-
Interleaver
Deinterleaver
Interleaver
APP
Decoder
APP
Decoder
ˆur
y(0)
y(2)
y(0)
y(1)
Λ(1)
e
Λ(2)
e
Figure 8.8: Block diagram of a turbo decoder with the two constituent code decoders.
Since the separation assumption destroyed optimality of the equation, the algorithm
is iterated several times through the two decoders, each time the component decoder uses
the currently calculated a posteriori probability as input. However, direct use of (8.17)
would lead to an accumulation of “old” extrinsic information by calculating
L(1)′
r
= Λ(1)′
e,r + Λ(2)
e,r + Λ(1)
e,r + Λs



L(1)
r
.
Therefore the decoders are constrained to exchanging extrinsic information only, which is
accomplished by subtracting the input values to the APP decoders from the output values
as shown in Figure 8.8.
The extrinsic information is a reliability measure of each component decoder’s estimate
of the transmitted information symbols based on the corresponding received component
parity sequence only. Since each component decoder uses the received systematic sequence
directly, the extrinsic information allows the decoders to share information without biasing.
The eﬃcacy of this technique can be seen in Figure 8.9, which shows the performance of the
original (37; 21; 65,536) turbo code parameterized by decoder iterations. It is impressive
that the performance of the code with iterative decoding continues to improve up to 18
iterations (and possibly beyond). In the next section we discuss a statistical analysis tool
that will help shed light on the dynamics of iterative decoding of concatenated coding
systems.

376
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
18 iterations
10 iterations
6 iterations
3 iterations
2 iterations
1 iteration
0
0.5
1
1.5
2
2.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.9: Performance of the (37;21;65,536) turbo code as function of the number of
decoder iterations.
8.7
EXIT Analysis
As we have seen, the distance spectrum and the minimum distance of turbo codes can
explain the error ﬂoor and, to some extent, the threshold behavior of these codes. The
behavior of turbo codes at the onset of the turbo cliﬀis, however, better understood
by a statistical analysis called the extrinsic information transfer (EXIT) analysis, ﬁrst
presented by ten Brink [74, 75, 76]. The EXIT analysis is related to similar methods
which are based on the transfer of variances [33], as well as to density evolution analysis
discussed in Chapter 6.
The basic philosophy EXIT is to view the component decoders of the turbo decoder
(Figure 8.8) as a statistical processor which transforms an input value, i.e., the extrinsic
LLR of the information symbols, into an output value, the recomputed extrinsic LLR.
The component decoder is therefore seen as a nonlinear LLR processor, as indicated in
Figure 8.10. The input LLR is denoted by Λ(A)
e
since it takes on the function of an a priori
probability, and the output extrinsic LLR is denoted by Λ(E)
e
.
Clearly we expect the component decoder to improve the extrinsic LLR in the course of
the iterations, such that Λ(E)
e
is better than Λ(A)
e
in some sense, since otherwise iterations

8.7. EXIT ANALYSIS
377
+-
APP
Decoder
ˆur
Λ(A)
e
Λ(E)
e
Channel Inputs: Eb/N0
Figure 8.10: Component decoder seen as extrinsic LLR transformer.
will lead nowhere. The question is then how to measure the quality of the extrinsic LLR
value. Experimental measurements of Λ(m)
e
show that it is approximately Gaussian, i.e.,
Λ(m)
e
= μλu + nλ,
nλ ∼N(0, σ2
λ),
(8.18)
where u ∈{−1, 1} is the information bit whose LLR is expressed by Λ(m)
e
, μλ is a mean
shift, and nλ is a zero-mean independent Gaussian random variable with variance σ2
λ.
The question is now how to measure the reliability of Λ(m)
e
. The mutual information
I(u, Λ(m)
e
) between Λ(m)
e
and u has proven to be the most accurate and convenient measure,
but the normalized variance has also been used in several papers. The mutual information
measure has the following immediate advantages:
• The measure is bounded 0 ≤I(u, Λ(m)
e
) ≤1.
• The upper limit indicates high reliability, that is, the variance σ2
λ →0.
• The lower bound indicates low reliability, and σ2
λ →∞.
• The measure is monotonically increasing (with reliability).
In order to capture the input–output behavior of the component decoder, simulated extrin-
sic values Λ(A)
e
are generated at its input, according to the Gaussian distribution (8.18)
with independent realizations.
This independence models the eﬀect of the interleaver,
which, ideally, destroys the statistical dependence between successive input LLR values
that was introduced by the preceding component decoder.
In the case of Gaussian modelled input LLR values, the mutual information measure,
in bits, can be calculated as
IA = I(u; Λ(A)
e
) =
1
√
2πσλ
 ∞
−∞
exp

−(λ −μλ)2
2σ2
λ

(1−log2 [1+exp(−λ)]) dλ.

378
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
The mutual information of the output extrinsic LLR and the information bit u is more
complex to evaluate since Λ(E)
e
is not exactly Gaussian. Furthermore, the values of Λ(E)
e
corresponding to Λ(A)
e
have to be found via simulating the component decoder. Other
than in very simple cases, such as the rate R = 1/2 repetition code, no closed form or even
analytical formulas for the output extrinsic mutual information exist to date. The output
extrinsic information IE is therefore an empirical function of the component decoder, the
input IA, and the channel signal-to-noise ratio at which the decoder operates, formally
given by the EXIT function
IE = T(IA, Eb/N0).
To evaluate this function, the following steps are executed. First a Gaussian modelled
input LLR with mutual extrinsic information value IA is generated and then fed into
the decoder, and the output extrinsic LLRs Λ(E)
e
are captured.
Second, a numerical
evaluation of mutual information between the information symbols u and the captured
extrinsic output LLRs Λ(E)
e
yields IE, calculated as
IE = 1
2

u=±1
 ∞
−∞
pE(ξ|u) log2

2pE(ξ|u)
pE(ξ|U = −1) + pE(ξ|U = 1)

dξ,
where pE(ξ|u) is the empirical distribution of Λ(E)
e
as measured at the output of the APP
decoder. Clearly, the disadvantage of this method lies in the fact that the component
decoder needs to be simulated and that we are working with empirical distributions.
Figure 8.11 shows the EXIT function T(IA, Eb/N0) for a memory 4, 16-state, rate
R = 2/3 component convolutional code with h0(D) = 1 + D3 + D4 and h1(D) = 1 + D +
D2 + D3 + D4 (see Section 4.3). The numbers on the diﬀerent trajectories are the Eb/N0
values in dB.
Figure 8.12 shows the EXIT function T(IA, Eb/N0) for several encoder memory sizes for
a ﬁxed signal-to-noise ratio of Eb/N0 = 0.8 dB. It is interesting that no one memory order,
i.e., no one code, is superior over the entire range of values of the mutual information.
As a general trend, weaker codes are better when the input mutual information is poor,
for IA approximately less than 0.4, while stronger codes are better when the input IA is
already good. As we will see, this sheds light on why it is quite useless to try to build
stronger turbo codes by using stronger component codes.
In a complete turbo decoder the extrinsic information is now exchanged between the
component decoders, where output extrinsic LLRs become input extrinsic LLRs for the
next decoder. In our analysis these iterations are captured by a sequence of applications
of component decoder EXIT functions, that is,
0 T1
−→I(1)
E
= I(2)
A
T2
−→I(2)
E
= I(2)
A
T1
−→I(1)
E
= I(2)
A
T2
−→· · · ,

8.7. EXIT ANALYSIS
379
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
2 dB
1.5 dB
1 dB
0.5 dB
0 dB
-0.5 dB
Figure 8.11: EXIT function of a 16-state convolutional code.
where T1 is the EXIT function of decoder #1 and T2 is that of decoder #2.
The behavior of this exchange can be visualized in the EXIT chart, where the EXIT
functions of both component decoders are plotted. However, the transfer curve of decoder
#2 is reﬂected about the 45◦line to reﬂect that IE is its input mutual information, and IA
is its output mutual information. As shown in Figure 8.13, for suﬃciently large values of
Eb/N0 (in this case for Eb/N0 = 0.8 dB), these two curves leave open a channel, and they
have no intersect point other than (1,1). Since the iterations start with zero LLRs, i.e., with
zero mutual information at point (0,0) in the chart, the iterations progress by reﬂecting
from one curve to the other as illustrated in Figure 8.13.
The ﬁrst iteration through
decoder #1 takes an input value of IA = 0 and produces an output value of IE = 0.18. This
value then acts as the input to decoder #2, and after one complete iteration the mutual
information has a value of 0.22, at point (1) in the plot. Analogously, the iterations proceed
through points (2), (3), etc., until they reach IA = IE = 1 after about 11 iterations. The
curves are drawn for the 16-state component decoders of the original turbo decoder, of
which we know that it converges at Eb/N0 ≈0.7 dB, see Figure 8.9.
The trajectory plotted in Figure 8.13 is a measured characteristic of a single decoding

380
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
memory 2: (7,5)
memory 1: (3,2)
memory 8: (435,777)
memory 6: (147,117)
memory 4: (23,35)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
Eb/N0 = 0.8 dB
Figure 8.12: EXIT function for various convolutional codes.
cycle of the turbo decoder, and the accuracy with respect to the prediction of the EXIT
chart is impressive, testifying to the robustness of the method. The growing discrepancies
between the measured characteristic and the EXIT chart starting at iteration 7 are a
result of the ﬁnite-sized interleaver, in this case L = 60,000 [74]. The larger the interleaver
chosen, the more accurate the EXIT chart becomes as a prediction tool of the convergence
properties of the code (see Figure 8.14).
It is worth noting that most of these iterations occur at very high bit error rates, i.e.,
Pb > 10−2, in fact, IE ≈0.9 corresponds to Pb = 10−2. This is interesting since it teaches
us that the question of whether the decoder converges or not is decided early on, and that
even though very little improvement may be seen in terms of bit error rates for many
iterations, the decoder will converge to a low bit error rate eventually. It is clear from this
analysis that the EXIT chart is a tool to evaluate component decoders, their cooperative
statistical behavior such as onset of the turbo cliﬀ, number of iterations to convergence,
etc., but delivers no information about the error ﬂoor performance of a code and thus its
ultimate operational performance.

8.7. EXIT ANALYSIS
381
(1)
(2)
(3)
(4)
(5)
(6)
(8)
(7)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
Eb/N0 = 0.8 dB
Figure 8.13: EXIT chart combining the EXIT functions of the two decoders involved in a
turbo decoder for Eb/N0 = 0.8 dB.
As an example of using the EXIT chart to predict the onset of the turbo cliﬀbehavior,
we reduce the signal-to-noise ratio Eb/N0 for the codes discussed until the open channel
closes at Eb/N0 = 0.53 dB, which represents the pinch-oﬀsignal-to-noise ratio of this,
the original turbo code combination of decoders. The sharpness of this pinch-oﬀsignal-to-
noise ratio depends on the size of the interleaver, since ﬁnite-sized interleavers always leave
some residual correlation, in particular for larger iteration numbers, which is in violation
of the independence assumption used to generate the EXIT functions of the component
decoders.
Figure 8.14 shows the dramatic turbo cliﬀeﬀect through simulations with
diﬀerent interleaver sizes using the 16-state component codes. It becomes evident that
for very large interleaver sizes, the pinch-oﬀsignal-to-noise ratio accurately represents the
onset of the turbo cliﬀ.
ten Brink [74] has used this method to ﬁnd the pinch-oﬀsignal-to-noise ratios for
turbo codes using the component codes whose EXIT functions are given in Figure 8.12.
He presented the following table of pinch-oﬀvalues for the component code combinations:

382
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
ν
C1/C2
(3,2)
(7,5)
(13,15)
(23,37)
(67,45)
(147,117)
1
(3,2)
>2 dB
2
(7,5)
1.49 dB
0.69 dB
3
(13,15)
1.14 dB
0.62 dB
0.62 dB
4
(23,37)
1.08 dB
0.65 dB
0.64 dB
0.68 dB
5
(67,45)
0.86 dB
0.62 dB
0.66 dB
0.70 dB
0.77 dB
6
(147,117)
0.84 dB
0.63 dB
0.67 dB
0.72 dB
0.81 dB
0.84 dB
Table 8.1: Table of pinch-oﬀsignal-to-noise ratios for combinations of component codes.
Table 8.1 illustrates some interesting facts. First, it clearly demonstrates that using
larger codes, ν > 4 does not give stronger turbo codes, in fact, a combination of two strong
codes has a worse pinch-oﬀvalue than that of the best combinations. Second, it illustrates
the judicious choice of component codes chosen by the inventors of turbo codes, who chose
the ν = 4, (37,21) component codes which have a pinch-oﬀsignal-to-noise ratio of 0.53 dB
(see Figure 8.14). Much searching [74] has yielded some code combinations with slightly
better pinch-oﬀs, for example, using two (22,37) codes with a pinch-oﬀat 0.48 dB, or two
(110,141), ν = 6 codes with a pinch-oﬀat 0.42 dB, but these are small improvements.
Research into asymmetric code combinations has been carried out as well; see [14].
N=1,000,000
 60 Iterations
N=200,000
30 Iterations
N=20,000
  30 Iterations
Shannon Limit
Turbo Cliﬀ: 0.53 dB
0
0.2
0.4
0.6
0.8
1
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.14: Simulations of the turbo code using 16-state component decoders and diﬀerent
sized interleavers related to theoretical prediction of the turbo cliﬀ.

8.8. SERIAL CONCATENATION
383
8.8
Serial Concatenation
So far we have shown that the parallel concatenation of convolutional codes coupled with
an iterative decoding algorithm could perform near the capacity of an AWGN channel at
virtually all practical error rates. This required careful selection of the component codes
and the interleaver. Here we now investigate the performance of the historically more
common serial concatenation of codes in which the output of one encoder becomes the
input to the next encoder. We will see that the lessons and techniques learned from parallel
concatenation can be applied to serial concatenation with a few minor adaptations.
Serially concatenated codes were ﬁrst considered in detail by Forney [36] as a means
of constructing codes with long block lengths and decoding algorithms which could be
built with the technology at the time. In classic serial concatenation, the second decoder
typically operated on the hard decision output of the ﬁrst decoder with no reliability (soft)
information. This limited the performance of these codes, and the second decoder was
frequently constrained to the role of “cleaning up” the residual errors of the ﬁrst decoder.
In fact, classic serial concatenation is one approach to mitigating the error ﬂoor of parallel
concatenation [53], which is also used in the modern DVB-S2 standard [34] for broadband
satellite application. Perhaps the most successful example of classic serial concatenation is
the CCSDS standard which used a maximum free distance (MFD) (2, 1, 6) convolutional
code as the inner code and various Reed–Solomon block codes as outer codes. Though
still of considerable importance, this chapter will not discuss classic serial concatenation
and the reader is referred to [36, 6] for a discussion thereof.
This chapter will discuss the design and performance of serial concatenated codes
for decoding with an iterative soft-decision decoder. This coding and decoding scheme
was ﬁrst investigated in [38] and came to full fruition in the outpouring of work that
followed the discovery of turbo codes. We begin with a description of serial concatenated
convolutional codes (SCCCs) and the analysis of their performance assuming uniform
interleaving and maximum-likelihood (ML) decoding. Next, a description is given of the
modiﬁcations required to the iterative decoding algorithm of Section 8.6 for its use in a
serially concatenated system. This is followed by a performance comparison of parallel and
serial concatenated codes. Finally, we consider alternative serial concatenations, including
repeat accumulator (RA) codes, whose outstanding performance at low signal-to-noise
ratios (SNRs) is predicted by EXIT analysis.
8.9
Cascaded Convolutional Codes
Before we begin with the formal analysis of serial concatenated codes, it is worth consider-
ing a simple example of cascaded convolutional codes, which is where serial concatenation
started. This will provide us with some intuition into the more analytical results that fol-

384
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
low, as well as regarding the diﬀerences between SCCCs and other forms of concatenation.
The block diagram of a simple SCCC is shown in Figure 8.15. The outer code is a rate
Ro = 1/2 code with generator matrix
Go(D) =

1 + D2
1 + D + D2 
,
(8.19)
which speciﬁes a 4-state code with do
free = 5. The inner code is a rate Ri = 2/3 code with
generator matrix
Gi(D) =
 1 + D
D
1
1 + D
1
1 + D

,
(8.20)
which speciﬁes a 4-state code with di
free = 3. As with the parallel concatenated codes of
earlier in this chapter, the two encoders are separated by an interleaver.
+
+
+
+
+
+
+
+

Figure 8.15: Block diagram of a simple serial concatenated convolutional code (SCCC2).
The ﬁrst observation to make is that without an interleaver; i.e., if the interleaver is the
identity permutation, the concatenated code simply becomes a convolutional code with
overall generator matrix
G(D) = Go(D)Gi(D) =

D + D2
1 + D2 + D3
D2 + D3 
,
(8.21)
which speciﬁes an 8-state, rate R = 1/3 code with dfree = 7 that can be decoded using
the maximum likelihood decoding algorithm described in Chapter 5. Concatenated codes
constructed in this manner are referred to as cascaded convolutional codes. Such codes were
studied before the discovery of turbo codes and had a particularly interesting application
on the Galileo mission [31, 51].
Though the current chapter does not address cascaded codes, the cascaded code of
(8.21) does give some insight into the distance structure of the more general class of serial
concatenated convolutional codes. In particular, for a cascaded code the outer encoder
serves as a “ﬁlter” that eliminates many input sequences to the inner encoder. In the
current example, the outer encoder, by nature of its free distance, eliminates all input
sequences to Gi(D) of weight less than do
free = 5 and, by nature of its rate, eliminates

8.10. WEIGHT ENUMERATOR ANALYSIS OF SCCCS
385
2N −2N/2 of the possible input sequences of length N. Thus, the input sequences that
generate many of the low-weight codewords in the inner encoder are eliminated by the
outer encoder. This ﬁltering operation should guarantee that the overall code will have a
reasonable free distance and that free distance will increase as the component code com-
plexity increases. However, the simple cascading of maximal-free distance codes by itself
does not typically result in maximal-free distance concatenated codes; the corresponding
maximal free distance of such codes is often rather modest.
Since the ﬁltering operation is not aﬀected by the presence of an interleaver, it is
reasonable to anticipate that SCCCs will not suﬀer from the low free distance and corre-
sponding error ﬂoors that trouble turbo codes with random interleavers. This does not
imply, however, that the interleaver has no role. Without the interleaver, serial concate-
nation is a technique for generating complex convolutional codes with relatively large,
but not maximal, free distance. As shown in Chapter 5, large convolutional codes are
asymptotically good but do not oﬀer near-capacity performance at practical error rates.
In serial concatenation, the interleaver achieves “spectral thinning” as well as introducing
the randomness necessary for eﬀective iterative decoding. In order to fully understand
the role of the interleaver in the design of SCCCs, we now embark upon a more detailed
analysis again employing the concept of uniform interleaving.
8.10
Weight Enumerator Analysis of SCCCs
A block diagram of a generic serial concatenated convolutional encoder is shown in Figure
8.16. The outer encoder is of rate Ro = k/m with encoder memory νo, and the inner
encoder is of rate Ri = m/n with encoder memory νi. The interleaver is of length N and
it will be assumed for clarity of notation that N is divisible by m. The overall rate of the
SCCC is thus R = Ro × Ri = k/n if we ignore the eﬀects of trellis termination. As with
parallel concatenated convolutional codes (PCCCs), ﬁnding the exact weight enumerating
function (WEF) for a SCCC is infeasible due to the complexity of the resulting hypertrellis
and it will be suﬃcient for our purposes to ﬁnd an approximation [16].
+
+
+
+
+
+
+
+
ur
cr
c′
r
vr
Outer Encoder
Ro = k/m

Interleaver
of size N
Inner Encoder
Ri = m/n
Figure 8.16: Block diagram of a serial concatenated convolutional code.

386
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
Recall from Section 8.5 that the Input–Output Weight Enumerating Function (IOWEF)
of a code may be written as
A(W, X) =
∞

d=dfree

w
Aw,dW wXd,
(8.6)
where Aw,d was the number of codewords of weight d caused by information sequences
of weight w. Considering only codewords caused by information sequences of a certain
weight results in the Conditional Weight Enumerating Function (CWEF)
Aw(X) =

d
Aw,dXd,
and the probability of an information bit error may then be expressed as
Pb ≤
NRo

w=wmin
w
NRo
Aw(X)

X=e−REb/No
,
(8.22)
where wmin is the minimum information weight and NRo is the maximum information
weight generating a codeword in the overall code. For serial concatenated codes, it is
useful to rewrite this bound as
Pb ≤
N/Ri

d=dfree
NRo

w=wmin
w
NRo
Aw,de−dREb/No,
(8.23)
where dfree is the free distance of the overall code and N/Ri is the length of the codewords
and thus the largest possible codeword weight. Note that in these expressions the weight
of the codeword is not separated into a parity weight and an information weight as was
required in the case of parallel concatenation.
In order to derive the CWEF of the overall serial concatenation in terms of the CWEFs
of the component codes, we will once again make use of the uniform interleaver. In parallel
concatenation, a particular CWEF of the overall code is related to the CWEFs of the
two component codes for the same information weight. In serial concatenation, however,
the CWEFs of the two component codes are linked by the weight of the output of the
outer encoder, which becomes the input to the inner encoder. Thus, assuming a uniform
interleaver, we obtain
Aw,d =
N

l=do
free
Ao
w,l · Ai
l,d
 N
l

(8.24)

8.10. WEIGHT ENUMERATOR ANALYSIS OF SCCCS
387
and
Aw(X) =
N

l=do
free
Ao
w,l · Ai
l(X)
 N
l

,
where do
free is the free distance of the outer code and l is the weight of the codeword out
of the outer encoder. Given the CWEF of the overall code, a bound on the bit error rate
of the code may be obtained using (8.22).
As with parallel concatenated codes, we will ﬁnd it convenient to derive an approximate
expression for these weight enumerators. Let
A(n)
w (X) =

d
A(n)
w,dXd
be the n-event weight enumerating function of the equivalent block code of a terminated
convolutional code, where A(n)
w,d is the number of codewords of weight d due to information
sequences of weight w and which consist of n distinct detours. For large interleaver lengths
N, the number of codewords of weight d due to information sequences of weight w may
be approximated as
Aw,d ≈
nmaxw

n=1
 N
n

A(n)
w,d,
(8.25)
where nmaxw is the maximum number of detours due to information sequences of weight
w that can be concatenated in a block of length N.
Applying the approximation of (8.25) to the outer and inner code of the serial con-
catenated system yields
Ao
w,l ≈
no
maxw

no=1
 N/m
no

A(no)
w,l
and
Ai
l,d ≈
ni
maxl

ni=1
 N/m
ni

A(ni)
l,d ,
respectively. Substituting these two approximations into (8.24) leads to
Aw,d =
N

l=do
free
no
maxw

no=1
ni
maxl

ni=1
 N/m
no
  N/m
ni

 N
l

A(no)
w,l A(ni)
l,d .

388
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
We will once again make use of the approximation
 N
n

≈Nn
n! ,
(8.26)
which results in
Aw,d =
N

l=do
free
no
maxw

no=1
ni
maxl

ni=1
Nno+ni−l
l!
mno+nino!ni!A(no)
w,l A(ni)
l,d .
(8.27)
Finally, using (8.27) in the bound of (8.23) results in
Pb ≤
N/Ri

d=dfree
N·Ro

w=wmin
N

l=do
free
no
maxw

no=1
nI
maxl

ni=1
Nno+ni−l−1
l!A(no)
w,l A(ni)
l,d
mno+ni−1no!ni!
w
k e−dREb/No.
(8.28)
A judicious examination of this bound yields considerable insight into the design of SCCCs
for moderate and high SNRs. Indeed, (8.28) is the key to understanding the design rules
for SCCCs.
First, we notice that the inner four summations in (8.28) only eﬀect the multiplicity of
codewords of weight d. The outer summation and exponential term are a function of the
distance properties of the code and are independent of the inner four summations. As the
SNR becomes very large, only the dfree term is of consequence and it has the form of the
decaying exponential times a multiplicity that is dominated by N no+ni−l−1. We begin by
considering the maximum value of the exponent for the dfree term
α(dfree) = max
w,l {no + ni −l −1}
for interleavers of length N. Making the reasonable assumption that the free distance
codeword of the overall code is a single error event in the inner code and that this error
event is caused by a codeword from the outer encoder of minimum weight do
free, then
ni = 1, no = 1 and l = do
free and
α(dfree) = 1 −do
free.
Thus, provided that do
free ≥2, the SCCC will exhibit an interleaver gain as the SNR gets
large and the outer code should simply be chosen to have large free distance.
As was observed with parallel concatenated codes, asymptotic design rules and large
free distance do not fully capture the characteristics of a good code and this is true of
serial concatenation as well. It was observed in [16] and [18] that the bound of (8.28)
becomes loose compared to simulation results at higher SNRs as the interleaver length

8.10. WEIGHT ENUMERATOR ANALYSIS OF SCCCS
389
N increases. That is, for a given N there exists an SNR at which the multiplicity of
higher-weight codewords is more signiﬁcant than the free distance event. As N increases
and the code becomes spectrally dense, this threshold SNR moves away from the capacity
of the channel.
This is consistent with observations made earlier on concerning the performance of
maximum free distance convolutional codes with increasing memory. In order to charac-
terize this eﬀect, the authors in [16] consider ﬁnding the maximum value of the exponent
of N regardless of codeword weight. This may be expressed as
αmax = max
d {no + ni −l −1} = max
w,l {no + ni −l −1}
and we are primarily concerned with whether or not αmax is greater than or less than zero.
Following [16], we ﬁrst consider the case of inner encoders that do not employ feedback.
For these encoder realizations, an error event may be caused by an information sequence
of weight 1 and thus a sequence of weight l from the outer encoder may cause l distinct
detours in the inner code. When this occurs, ni = l and the exponent is maximized by
no = no
max, which results in
αmax(no feedback) = no
max + l −l −1 = no
max −1 ≥0,
(8.29)
and the corresponding multiplicity increases rapidly as N increases. The bound of (8.28)
will then diverge from the free distance asymptote at higher SNRs, which suggests that
the code will perform poorly at low and moderate SNRs.
If the outer encoder generates a codeword with weight do
free, then no = 1, l = do
free and,
for random interleavers, ni = do
free. The corresponding exponent for N is then
αdfree(feedforward) = 1 + d0
free −do
free −1 = 0
(8.30)
and the interleaver does not aﬀect the multiplicity due to N αmax = 1. Notice that in this
situation the concatenated code generates a codeword of weight d = do
freedi
free and the
concatenated code is similar to a product code! Indeed, for interleavers on the order of
several hundred bits, a search has veriﬁed that the free distance of this code combination
is equal to the product distance. Thus, the interleaver provides a signiﬁcant improvement
in free distance compared to cascaded convolutional codes, but no “turbo code eﬀect” yet.
For encoders utilizing feedback, the minimum weight of an information sequence that
generates a ﬁnite-length error event is two. Consequently, a sequence of weight l out of the
outer encoder can cause at most l/2 error events in the inner encoder and the exponent
becomes
αmax = max
w,l

no +
 l
2

−l −1

.

390
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
For an outer code with free distance do
free it is clear that the maximum number of error
events in a codeword of weight l is bounded by
no
max ≤

l
do
free

and the exponent may accordingly be bounded by
αmax ≤max
l

l
do
free

−
l + 1
2

−1

,
(8.31)
which is now a function of only l. In order to perform the maximization in (8.31), we
observe that the weight l of a codeword produced by the outer encoder must satisfy
ido
free ≤l < (i + 1)do
free
(8.32)
for some integer i. The maximum exponent occurs when l = do
free and for this case
αmax(feedback) = −
do
free + 1
2

,
(8.33)
which implies that the multiplicities of all codewords see an interleaver gain provided the
free distance of the outer code is greater than two and the inner encoder utilizes feedback.
This yields the ﬁrst design rule for SCCCs.
Rule 1: Choose the inner encoder should be in feedback form.
Further insight into the design of SCCCs may be gained by considering the minimum
weight of the codeword, denoted by dα, associated with αmax. The derivation of (8.33)
required that the number of error events in the inner code be maximized. For do
free even,
this means that there are do
free/2 error events, each due to a weight 2 input. Let d(2)
min be the
minimum weight of codewords in the inner code that are due to weight 2 input sequences.
Then the minimum weight of the codeword associated with the maximum multiplicity
exponent is
dα = do
freed(2)
min
2
.
(8.34)
If do
free is odd, then there are (do
free −3)/2 error events due to weight 2 inputs and one
error event due to a weight 3 input. In this case
dα = (do
free −3)d(2)
min
2
+ d(3)
min,
(8.35)

8.10. WEIGHT ENUMERATOR ANALYSIS OF SCCCS
391
where d(3)
min is the minimum weight of codewords in the inner code due to weight 3 input
sequences. Clearly it is desirable to have an inner code with large d(2)
min, i.e., a large eﬀective
free distance. From parallel concatenation we know that this is accomplished by choosing
the feedback polynomial to be primitive, and this becomes the second design rule.
Rule 2: Maximize inner encoder eﬀective free distance
by choosing its feedback polynomial to be primitive.
These two design rules are the same rules used to choose good component encoders for
parallel concatenated convolutional codes, and thus encoders optimized for turbo codes are
also excellent candidates for inner codes in serially concatenated systems. In choosing the
outer encoder, the primary concern is simply the free distance do
free and thus traditionally
good convolutional codes are suitable. It is well known that with traditional convolutional
codes, non-recursive encoder realizations have a slight advantage in bit error rate perfor-
mance for large SNRs due to the mapping from information sequences to codewords, and
this holds for the outer encoder in serially concatenated systems as well.
A ﬁnal design guideline is based on careful consideration of (8.35) which shows that
dα is based on both the eﬀective free distance of the inner code and d(3)
min when do
free is
odd. If the inner code can be chosen such that no odd weight input sequences generate
ﬁnite length codewords, then d(3)
min = ∞and dα = ∞. This is easily achieved by choosing
the feedback polynomial of the inner encoder to have a factor of (1 + D). It should be
noted that choosing the feedback polynomial in this manner means that it cannot be
primitive and thus this competes with the second design rule. As with turbo codes, the
choice between a primitive or non-primitive feedback polynomial is a trade-oﬀbetween
performance at low and high SNRs, respectively [46].
In order to illustrate the ﬁrst design rule, we consider the performance of two R = 1/3
serial concatenated convolutional codes with pseudorandom interleavers. Both codes use
a rate Ro = 1/2, memory ν = 2 outer code realized in non-systematic feedforward form.
The encoder for this code is
Go(D) =

1 + D2
1 + D + D2 
,
(8.36)
which describes a 4-state (2, 1, 2) code with do
free = 5. The ﬁrst SCCC uses a rate Ri = 2/3
inner code with total encoder memory ν = 2, which has a generator matrix of
G1(D) =

1
0
1+D2
1+D+D2
0
1
1+D
1+D+D2


392
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
and is realized in systematic feedback form.4 Note that the feedback polynomial is primi-
tive. Simulation results for the resulting rate R = 1/3 concatenated code, denoted SCCC1
and shown in Figure 8.17, are shown in Figure 8.18 for several interleaver lengths. This
code has do
free = 5 and from (8.33) the maximum exponent is given by
αmax = −
do
free + 1
2

= −
5 + 1
2

= −3
(8.37)
and we would expect an interleaver gain of N−3. The simulation results do indeed manifest
this gain with increasing N.
+
+
+
+
+
+

Figure 8.17: Block diagram and encoders for SCCC1.
The second serial concatenated convolutional code uses a rate Ri = 2/3 inner code
with total encoder memory ν = 2 with generator matrix
G2(D) =
 1 + D
D
1
1 + D
1
1 + D

when realized in nonsystematic feedforward form. Simulation results for the resulting rate
R = 1/3 concatenated code, denoted SCCC2 and shown in Figure 8.15, are given in Figure
8.19 for several interleaver lengths. This code clearly performs much poorer than SCCC1.
For increasing N this code exhibits an error ﬂoor despite its relatively large free dis-
tance.
In fact, simulation results suggest that the performance curves for N = 1,000
and N = 10,000 will converge for high SNR. This is not unexpected! For both codes,
it is highly likely that the free distance dfree = do
freedi
free = 15 and from (8.30) there is
no interleaver gain associated with the free distance multiplicity. Thus, the free distance
4Note that for encoders with rate R =
k
k+1 and k > 1, the recursive systematic realization is generally
not a minimal realization, but the systematic feedback realization is.

8.10. WEIGHT ENUMERATOR ANALYSIS OF SCCCS
393
asymptotes of these two codes are essentially identical. For large N, however, the pseudo-
random interleaver does have a spectral thinning eﬀect on some higher-weight codewords,
which results in the improved performance at moderate SNRs.
In order to illustrate the second design rule, we construct a third code, SCCC3, using
a non-primitive feedback inner encoder. SCCC3 uses a rate Ri = 2/3 inner code with
total encoder memory ν = 2 and generator matrix
G3(D) =

1
0
D2
1+D2
0
1
1+D+D2
1+D2

when realized in systematic feedback form. The feedback polynomial for this code, 1+D2 =
(1 + D)(1 + D), is not primitive. The performance of this code is shown in Figure 8.20
for a variety of block lengths. The asymptotic performance of SCCC3 is clearly inferior
to that of SCCC1, though it does have a small advantage at high error rates.
0
0.5
1
1.5
2
2.5
10−6
10−5
10−4
10−3
10−2
10−1
1
N = 100
N = 1,000
N = 10,000
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.18: Simulation results for the R = 1/3 code SCCC1 (inner encoder with feedback)
as a function of interleaver length.

394
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
0
0.5
1
1.5
2
2.5
10−6
10−5
10−4
10−3
10−2
10−1
1
N = 100
N = 1, 000
N = 10, 000
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.19: Simulation results for the R = 1/3 code SCCC2 (inner encoder without
feedback) as a function of interleaver length.
8.11
Iterative Decoding and Performance of SCCCs
Serially concatenated codes may be iteratively decoded in a manner very similar to that
of parallel concatenated codes.
A block diagram of the iterative decoder for a serial
concatenated code is shown in Figure 8.21. The APP decoders are the same as those
described in Chapter 5 and Section 8.6. The essential diﬀerence lies in the fact that with
serial concatenation the outer decoder does not have access to channel observations and
must rely on information received from the inner decoder.
To be precise, the inner APP decoder receives the channel log-likelihood ratio (LLR)
and the extrinsic information from the outer decoder and computes a new log-likelihood.
The output LLR of the inner APP decoder is
L(1)
r
=
log P[c
r = 1|y]
P[cr = 0|y]
=
log P[y|c
r = 1]
P[y|cr = 0] + log P[c
r = 1]
P[cr = 0] = Λ(1)′
e,r + Λ(1)′
r
,
of which only Λ(1)′
e,r
is passed to the second decoder, again for reasons of avoiding the

8.11. ITERATIVE DECODING AND PERFORMANCE OF SCCCS
395
0
0.5
1
1.5
2
2.5
10−6
10−5
10−4
10−3
10−2
10−1
1
N = 100
N = 1,000
N = 10,000
Bit Error Probability (BER)
Eb/N0[dB]
Figure 8.20: Performance of serial concatenated convolutional codes with a primitive inner
encoder (SCCC1, solid line) and a nonprimitive inner encoder (SCCC3, dashed line).
+-
+-
Inner APP
Decoder
−1

Outer APP
Decoder
Λ(1)′
e,r
Λ(1)
e,r
Λ(2)
e,r
Λ(2)′
e,r
Lr
Figure 8.21: Block diagram of the iterative decoder for serial concatenated codes.
accumulation of dated information. The outer APP decoder now computes its LLR using
the correctly deinterleaved extrinsic information from the ﬁrst decoder as its only input.
The outer decoder computes an information bit log-likelihood and a codeword bit

396
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
log-likelihood for iterative processing by the inner decoder and thus generates
L(2)
r
=
log P[cr = 1|Λ(1)
r ]
P[cr = 0|Λ(1)
r ]
=
log P[Λ(1)
r |cr = 1]
P[Λ(1)
r |cr = 0]
+ log P[cr = 1]
P[cr = 0] = Λ(2)
e,r + Λ(1)
e,r
and passes Λ(2)
e,r to the ﬁrst decoder. In this manner, serial concatenated codes can be
iteratively decoded even though they do not share the same symbol sequence. This is
based on the capability of the APP algorithms to incorporate any probabilistic prior
information whether it comes in the form of received channel data, prior information on
the information symbols, or prior information on the code symbols.
The natural question to ask at this point is how do serial concatenated convolutional
codes compare to parallel concatenated convolutional codes? We address this question
using both analysis and simulation.
The analytical comparison uses the results from
Sections 8.5 and 8.10, and the reader is reminded that these results assumed maximum
likelihood decoding and large SNRs.
For parallel concatenated convolutional codes with recursive encoders, it was shown
that for large SNRs the bit error rate could be approximated by
Pb ≈
N/2

j=1
(2j) ·
 2j
j

N−1
(Z2+2zmin)j
(1 −Zzmin−t)2j

Z=e−REb/No
,
(8.12)
The quantity deﬀ= 2 + 2zmin determines the performance of the parallel turbo code and
is generated by a weight-2 information sequence. The interleaver gain of a PCCC is ﬁxed
at N −1, and deﬀis, in general, quite small.
For serial concatenated convolutional codes, the situation is markedly diﬀerent. If we
assume that the free distance of the outer code, do
free, is even, then substituting (8.33) and
(8.34) into (8.28) and keeping only the term corresponding to αmax results in
Pb ≈KN−do
free/2 exp

−do
freed(2)
min
2
REb
No

,
(8.38)
where K is a constant proportional to the multiplicity [16]. From this expression, the
SCCCs to have an interleaver gain of N−do
free/2, and the dominant distance is do
freed(2)
min/2,
both of which increase with the outer code free distance. This suggests that SCCCs should
have a distinct advantage over PCCCs.
Figure 8.22 shows simulation results for a serial concatenated convolutional code and
a parallel concatenated code for two diﬀerent interleaver lengths.
The serial code is

8.12. EXIT ANALYSIS OF SERIALLY CONCATENATED CODES
397
SCCC1 and the PCCC is a rate R = 1/3 code based on (2, 1, 2) component codes with
h1(D) = 1 + D2 and h0(D) = 1 + D + D2. In this, Nu corresponds to the length of the
information sequence, so both codes have the same overall block length. The simulation
results shown are for seven iterations of the iterative decoding algorithm. The results
shown in Figure 8.22 clearly show the advantage of the serial concatenated code for large
SNR. The increased interleaver gain is also evident. However, the PCCC has a distinct
advantage at very low SNRs. This is an aspect of the performance that the union-bound
analysis cannot capture.
0
0.5
1
1.5
2
2.5
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
Nu = 500
Nu = 5, 000
SCCC
PCCC
Figure 8.22: Comparison of a serial concatenated turbo code and a parallel concatenated
code with the same overall block lengths and seven decoder iterations.
8.12
EXIT Analysis of Serially Concatenated Codes
As with turbo codes, the weight enumerator analysis of serial concatenated convolutional
codes provides insight into code performance for moderate to high SNRs. In order to gain
additional insight into the performance at low SNRs, we will once again rely on the EXIT

398
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
analysis of Section 8.7, which is used again to predict the onset of the turbo cliﬀfor SCCCs
and leads us to a particularly simple serial scheme with outstanding performance [75].
There are two fundamental diﬀerences between the EXIT analysis of parallel concate-
nated codes and serial concatenated codes. First, for serial concatenated codes the inner
decoder passes extrinsic information concerning its code symbols, rather than information
symbols, to the outer decoder. Thus, the EXIT function of the inner decoder,
Ii
E = Ti(IA, Eb/N0),
is diﬀerent. The EXIT function of the rate Ri = 2/3, memory-2 inner code used in SCCC1
is given in Figure 8.23 as a function of Eb/N0. Notice the large starting value of Ii
E.
-0.4 dB
0 dB
0.4 dB
0.8 dB
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
Figure 8.23: EXIT function of the rate Ri = 2/3 inner code used in SCCC1.
Second, in serial concatenation the outer code does not see the channel outputs. In-
stead, it sees the extrinsic information output of the inner encoder and consequently its
EXIT function,
Io
E = To(Ii
E),
is not a function of the SNR. The EXIT functions of several Ro = 1/2 inner codes are
given in Figure 8.24 as functions of Ii
A.

8.12. EXIT ANALYSIS OF SERIALLY CONCATENATED CODES
399
memory 2 (7,5)
memory 4 (23,35)
memory6 (133,171)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
Figure 8.24: EXIT function of several Ro = 1/2 outer codes.
As with turbo codes, the EXIT functions of the inner and outer encoders can be
combined to form an EXIT chart that predicts the behavior of the iterative decoder. For
serial concatenated codes the inner and outer codes are not identical and the EXIT chart
of the overall code is not symmetric with respect to the 45-degree line as in Figure 8.13.
The EXIT chart for SCCC1 with N = 10,000 is shown in Figure 8.25. As with turbo codes,
the accuracy of this method is impressive and this technique can be used to calculate a
pinch-oﬀsignal-to-noise ratio for serial concatenated codes.
The EXIT analysis of serial concatenated codes may be used to analyze two partic-
ularly clever alternative schemes, the so-called doped repeat scramble (DRS) codes [78]
and repeat-accumulate (RA) codes [13].5 DRS codes are eﬀective for rate R = 1/2 while
RA codes are generally eﬀective for rate R = 1/3 or lower. Indeed, RA codes have been
shown to approach the capacity of the AWGN channel as the rate approaches 0. Both of
these concatenated coding schemes have the same basic structure consisting of an outer
repetition code with rate Ro = 1/m and an inner rate Ri = 1 scrambler/accumulator.
5RA codes were discussed from an LDPC perspective in Section 6.5.4, since they can most easily be
viewed as belonging to either class of codes.

400
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IE [bits]
IA [bits]
(1)
(2) (3)
(4)
(5)
(6)
Figure 8.25: EXIT exchange chart for SCCC1 with decoder iterations for Eb/N0 = 0 dB.
The overall rate of the code is determined solely by the outer code.
The inner accumulator is simply a rate-1/2 recursive systematic convolutional encoder
that is punctured to rate 1.
For DRS codes the puncturing pattern is typically such
that one out every 100 bits is the systematic information bit, thus giving rise to the
term “doped” codes.
For RA codes no systematic information bits are used and the
inner encoder is a rate-1 recursive encoder with feedback polynomial h0(D) = 1 + D and
feedforward polynomial h1(D) = 1.
The EXIT chart of the rate R = 1/2 repetition code used in the DRS codes is decep-
tively simple and given by the diagonal line
Io
E = To(Ii
E) = Ii
E,
which is a consequence of the fact that its extrinsic soft decoder simply swaps the LLR
values, regardless of SNR. The code design problem is then to ﬁnd the appropriate inner
accumulator and puncturing pattern that result in an EXIT function Ti(IA) that stays
above this line for the lowest possible signal-to-noise ratio.
For DRS codes, extensive analysis and simulation [75] revealed that an inner code
with with feedback polynomial h0(D) = 1 + D + D2 + D3 and feedforward polynomial

8.13. VIEWPOINT
401
h1(D) = D + D2 + D3 and a doping ratio of 1 in 100 resulted in a pinch-oﬀof 0.28 dB.
The EXIT exchange chart for this code is shown in Figure 8.26. This ﬁgure shows that as
the signal-to-noise ratio is decreased, the EXIT function of the inner code gets closer and
closer to that of the outer repetition code and thus more and more iterations are required
for decoding. In order to achieve performance near the pinch-oﬀwith an interleaver length
of N = 10, 000, 100 decoder iterations are required.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Ii
E,Io
E [bits]
IA, Ii
E [bits]
Inner
Code
3 dB
2 dB
1 dB
Outer Repetition Code
Figure
8.26:
EXIT
chart
for
a
doped-repeat-scramble
code
with
inner
code
(h0(D), h1(D)) = (1 + D + D2 + D3, D + D2 + D3) and a doping ratio of 1 in 100.
8.13
Viewpoint
Turbo codes represent a paradigm shift in the 60 years of searching for the ultimate error
control codes promised by the publication of Shannon’s mathematical theory of commu-
nication. The new codes’ introduction in 1993 has shaken a community that had come
to believe that near-Shannon performance was “nearly” impossible, and now turbo codes
with simple iterative decoders allow decoding at signal-to-noise ratios virtually arbitrarily
close to the capacity limit [25, 75], at least for low-rate codes with R < 1.

402
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
While it has taken the scientiﬁc community a few years to come to terms with the
new paradigm, iterative decoding is now seen as the key to achieving limit performance,
and applications have spread to other ﬁelds such as equalization and multiple access in-
terference resolution. Turbo coding and iterative decoding have changed forever how we
approach error control coding. Gone are the waterfall error curves that gradually decline
with increasing signal-to-noise ratios. The new methods of graph-based codes and iter-
ative message passing teach pinch-oﬀsignal-to-noise ratios and turbo cliﬀbehaviors. A
data link with coding either works perfectly or does not work at all. This has far-reaching
consequences on all parts of the complex communications infrastructure, as error control
coding has become an integral part of practically all data communications in space or time
(storage).
8.14
Turbo-Trellis-Coded Modulation
As trellis-coded modulation is an extension of binary coding methods to larger signal
constellations, so is turbo-coded modulation the extension of turbo coding principles to
include larger signal constellations for operation in spectrally limited regimes. As we will
see, there are very few changes necessary to accommodate higher signaling alphabets, and
higher spectral eﬃciencies. Mainly we are dealing with multi-level symbols now, rather
than binary symbols, and the concept of log-likelihood ratios, which served us so well with
binary codes, needs to be extended.
While trellis-turbo-coded modulation (TTCM), which can be seen as the direct ex-
tension of parallel concatenation, has found only limited interest, serial concatenation of
binary error control codes with TCM is experiencing a surge of interest, in particular in the
context of space-time coding, where the larger signal alphabets are comprised of matrices
of complex symbols, as discussed in Chapter 2.
To start out with the analog of binary parallel concatenation, consider Figure 8.27,
which shows the encoder for a TTCM system proposed by Robertson and W¨orz [58]–[61].
One diﬀerence is that blocks of n-coded bits are treated as input symbols. The interleaver is
symbol-oriented, and the component encoders are trellis encoders, such as those discussed
in Chapter 3; here for n = 3. The ﬁnal sequence of transmitted symbols is generated
by selecting symbols alternately from the two encoders, i.e., x = [x(1)
0 , x(2)
1 , x(1)
2 , . . .]. In
this fashion the output symbols are punctured in the ratio 1:2 to maintain a rate R = 2
bits/symbol.
Since the encoded symbols no longer allow a separation of information (systematic) bits
and parity bits, the encoded symbols of the second component encoder are deinterleaved
before transmission. While this process could just as well be carried out at the receiver,
deinterleaving before transmission simpliﬁes syrnchronization. It has the eﬀect that the n
bits which make up the information portion of the symbols xr are transmitted in order,

8.14. TURBO-TRELLIS-CODED MODULATION
403
irrespective from which encoder the symbols originate.
+
+
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
[00, 01, 11, 10, 01, 10]
[01, 10, 01, 10, 11, 00]
[2, 4, 2, 4, 6, 0]
[0, 2, 6, 4, 2, 4]
[0, 2, 6, 4, 2, 4]
u(k)
r
u(1)
r
Interleaver
Trellis
Encoder/Mapper
Trellis
Encoder/Mapper
De-Interleaver
x(1)
r
x(2)′
r
Figure 8.27: Block diagram of a turbo-trellis encoder with two constituent encoders.
As an example, assume that a block of L = 6 pairs of symbols is encoded in a
TTCM system with 8-PSK trellis component codes, as shown in Figure 8.27.
If the
sequence of pairs of input bits [u(2)
r , u(1)
r ] is [00, 01, 11, 10, 01, 10], then the sequence of
output symbols from the ﬁrst component encoder is [0, 2, 6, 4, 2, 4] (see state-transition di-
agram in Figure 3.2). The interleaver permutes the bit pairs [u(2)
r , u(1)
r ] into the sequence
[01, 10, 01, 10, 11, 00], which is encoded by the second component encoder into the 8-PSK
symbol sequence [2, 4, 2, 4, 6, 0]. Only the bold-faced symbols are transmitted, the others
are punctured, and the ﬁnal transmitted sequence is [0, 2, 6, 4, 2, 4]. Of course L needs to
be chosen large enough for this system to have a turbo eﬀect and measurable coding gain.
The decoder is also structured analogously to the iterative decoder of a parallel con-
catenated turbo-coded system and is shown in Figure 8.28.
The sequence of received
symbols y is fed alternately to the ﬁrst or the second APP decoder, in accordance with
the puncturing order at the transmitter. If no symbol is available to be fed to the com-
ponent decoder, a one is entered in lieu of the conditional channel probability Pr(yr|xr).
This gives uniform weight to all symbols from the channel input and represents our lack
of channel information for these symbols.
Decoding proceeds analogously to standard turbo decoding, but the component APP
decoders need to be altered to accommodate the symbol structure of the received signal.
In essence, we need to extract the conditional probability of each trellis branch from the

404
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
received signal.
As discussed in Chapter 5, the only place where the received symbol
aﬀects the APP decoder is in the calculation of the branch metric
γr(j, i)
=
Pr(sr+1 = j, yr|sr = i)
=
Pr[sr+1 = j|sr = i]



transition probability
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
if symbol is punctured,
Pr(yr|xr)



channel probability
otherwise.
(5.30)
While the transition probability is simply the a priori probability Pr(ur) = Pr(k)
a (ur) of
the symbol ur causing the transition from state i to state j, the channel probability equals
the actual conditional probability Pr(yr|xr) if the received symbol yr originated from the
corresponding component decoder, or is set to 1 if that symbol was punctured, and hence
yr delivers no information on the transmitted symbol for this component decoder.
The APP decoders calculate the a posteriori probabilities of all M = 2n symbol vectors
ur, i.e., they calculate Pr(ur|y(i)); i = 1, 2. From these output a posteriori probabilities,
extrinsic probabilities Pr(i)
e (ur) are calculated by removing the a priori input probabilities
Pr(i)
a (ur) by division, i.e.,
Pr(i)
e (ur) = 1
α
Pr(ur|y(i))
Pr(i)
a (ur)
∀ur,
α =

u
Pr(u|y(i))
Pr(i)
a (u)
.
(8.39)
This is the DIV operation in Figure 8.28. The normalization factor α is used to make the
extrinsic probabilities sum up to unity. The operation in (8.39) avoids accumulation of a
priori probabilities analogously to the subtraction of the LLR in the binary turbo decoder.
In the ﬁrst half iteration no a priori information exists, and hence no information is
available during time intervals corresponding to punctured symbols. In this case, initial a
Interleaver
Deinterleaver
APP
Decoder
APP
Decoder
y
y(1)
y(2)
{Pr(2)
a (ur)}
{Pr(1)
a (ur)}
DIV
DIV
Figure 8.28: Block diagram of a turbo decoder with two constituent decoders.

8.14. TURBO-TRELLIS-CODED MODULATION
405
priori probabilities are calculated as
Pr(1)
a (ur) =

v(0)
r
p(yr| ur, v(0)
r
  
xr
),
(8.40)
that is, the unknown parity is simply averaged out.
Figure 8.29 shows simulated performance curves, taken from [58, 59, 60] for a TTCM
system using 8-state constituent 8-PSK trellis codes. The channel capacity limit for this
channel is at Eb/N0 = 1.9 dB for unconstrained input signals and increases to Eb/N0 = 2.9
dB if the channel symbols are restricted to 8-PSK. Thus the performance of this systems
comes close to the Shannon limit even for the small block length of N = 5,000 symbols.
Shannon Limit:
2.9 dB (8PSK)
1
2
3
4
5
6
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
18 iterations
8 iterations
4 iterations
8 iterations
4 iterations
1 iteration
Figure 8.29: Simulations of a turbo-trellis-coded modulation system using two 8-PSK
component trellis codes. Solid lines for L = 5,000 symbols, dashed lines for L = 1024
symbols.
Other methods of parallel concatenation also exist; [60], for example, discusses a 16-
QAM TTCM system where 2 input bits are encoded by two rate R = 1/2 trellis encoders.
The encoders are separated by the usual interleavers, which are realized independently for
each of the two input bit streams. The resulting 4 output bits are mapped into a 16-QAM

406
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
symbol, whereby the two bits from each encoder choose the signal points along either
the quadrature or the inphase axis. The advantage is that no puncturing is required, the
disadvantage, however, is the large signal set expansion required.
8.15
Serial Concatenation
Serial concatenation for trellis-coded modulation has a number of advantages. The most
obvious one is that the outer code can be a regular binary error control code, and only
the inner code needs to drive a modulator. A block diagram of this structure is shown
in Figure 8.30. A sequence of binary information symbols ur are encoded in a regular
binary encoder, whose outputs vr are sequences of symbols of m-tuples of binary coded
bits which are interleaved either symbol-wise or bit-wise and then fed into a trellis encoder
which accepts m input bits and generates the output symbol xr. Again, the example shown
in Figure 8.30 uses a rate R = 2/3 8-PSK trellis encoder as the modulation encoder.
+
+
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
ur
vr
v
r
Interleaver
Binary
Encoder
Trellis
Encoder/
Mapper
xr
Figure 8.30: Block diagram of a serial turbo encoder for turbo-coded modulation.
The decoder, too, follows the same principle as for binary serially concatenated codes,
and it is shown in Figure 8.31. The ﬁrst APP decoder for the modulation code operates as
usual with two inputs, the channel values y and the set {Pra(xr)} of a priori probabilities of
the transmitted symbols xr. The output of the modulation APP decoder are a posteriori
probabilities of the same symbols xr, denoted by {Pre(xr)}, and normalized according
to (8.39).
These a posteriori symbols probabilities are now deinterleaved to line them up with
the output symbols vr = (v(n−1)
r
, . . . , vr(0)) of the binary code. Now two strategies are
possible; the symbol probabilities can be set equal to the probability of a block vr of binary
code symbols, i.e., Pre(xr) = Pre(vr), and the APP binary code decoder operates with
symbol probabilities in calculating the branch metrics of (7.31). This is most convenient

8.15. SERIAL CONCATENATION
407
if the bits v on a branch of the binary code map directly into a symbol x. A more general
approach is to use a binary APP code decoder which operates with binary LLR values.
In order to obtain these, we need to marginalize the symbol probabilities, i.e.,
Pr(v(m)
r
= v) =

x(v)∈X
(v(m)=v)
Pre(x(v)),
(8.41)
where X is the constellation alphabet. From (8.41) the a priori LLRs for each bit to be
used by the binary APP decoder are calculated in the usual manner:
Λa(v(m)
r
) = log

Pr(v(m)
r
= 1)
Pr(v(m)
r
= 0)

.
(8.42)
On the output side of the binary APP decoder, the LLR values have to be combined
into symbol a priori values for the next iteration of the modulation code APP decoder.
First, binary bit probabilities are generated from the extrinsic LLR values as
Pr(v(m)
r
= 1) =
exp

Λe(v(m)
r
)

1 + exp

Λe(v(m)
r
)
,
Pr(v(m)
r
= 0) = 1 −Pr(v(m)
r
= 1).
(8.43)
The probability of a symbol is then simply the product of the probabilities of its constituent
binary bits, i.e.,
{Pra(xr(vr))} =
n

i=1
Pr(v(m)
r
).
(8.44)
These symbol probabilities function as a priori information for the next iteration of the
outer APP decoder.
Deinterleave
Marginalize
Interleave
Combine
APP
Decoder
Binary APP
Decoder
y
{Λa(ur)}
{Λe(ur)}
{Pra(xr)}
{Pre(xr)}
DIV
+
-
Figure 8.31: Block diagram of a turbo decoder with two constituent decoders.

408
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
8.16
EXIT Analysis of Serial TTCM
Turbo-coded modulation systems can also be analyzed following the EXIT method pre-
sented earlier. The only notable diﬀerence is that we are dealing with symbols rather than
bits, which needs additional explanation. In the binary case, probabilities could conve-
niently be expressed in a single real number using the log-likelihood ratio. For multi-level
signals, however, a single number does not suﬃce. Instead vectors of probabilities have to
be used, both in the a priori and a posteriori case.
The coded modulation APP decoder receives reliability information on two inputs,
viz. the received channel symbols y and a-priori symbol probabilities of the transmitted
symbols. Under the usual assumption of long random interleavers, the a priori probabilities
Pra(xr) are assumed to be independent. Likewise the a priori LLR values Λa(ur) used by
the binary APP decoder are assumed to belong to independent bits ur.
Unlike the binary decoder, the APP decoder for the inner modulation code at each
time instant r will generate an entire list of extrinsic output probabilities pe,r = [Pre(xr =
s(1)), . . . , Pre(xr = s(M))] for the M symbols of the TTCM constellation. Likewise, the
input probabilities will be delivered to the decoder as an M-valued vector pa,r = [Pra(xr =
s(1)), . . . , Pra(xr =s(M))] of input a priori symbol probabilities at time interval r. Both of
these M-vectors are random vectors in their own right with their own probability distri-
bution, which we shall denote by pa(pa) and pe(pe). Due to the interleaver assumption,
pe and pa are assumed of the index r.
One way to visualize this is by thinking of a hypothetical discrete-input real-vector
output channel, whose inputs are drawn from the signalling alphabet S = {s(1), . . . , s(M)}
and whose outputs are the (conditional) probability vectors pe|s(i), or pa|s(i). This hy-
pothetical channel is not so diﬀerent from a regular multi-input channel as discussed in
Chapter 2, only that it is not an additive noise channel.
The “quality” of this hypothetical channel can be measured by the mutual information
between the input X = s(i) and output pe|s(i), denoted by I(X, pe), and likewise for the a
priori symbols we deﬁne the mutual information I(X, pa) between the actual signals s(i)
and the probability distribution pa|s(i). The information measures I(X, pa) and I(X, pe)
will be used to quantify the reliability of the distributions pa and pe, i.e., their level of
uncertainty.
While the a priori distributions pa,r in the actual decoding process are generated by an
APP decoder, for the purpose of measuring the information transfer behavior of the coded
modulation APP decoder they need to be synthesized. There are many ways to do this,
and an intuitively correct way of generating them is by passing the transmitted symbols
x through a test channel of a given capacity, typically an additive white Gaussian noise
channel. This then results in the information transfer point of view illustrated in Figure
8.32. Both inputs, the channel symbols y as well as the symbol a priori probabilities,

8.17. DIFFERENTIAL-CODED MODULATION
409
are fundamentally identical in nature, since both physical and test channel are additive
white Gaussian channels. The APP decoder is simply making use of all available a priori
probability information to generate exact a posteriori probability information. The fact
that the test channel output is ﬁrst converted into probability vectors is quite irrelevant
to the decoding performance.
Physical Channel
p(y|x)
pT (y|x)
Test Channel
APP
Soft-
Output
Decoder
xr
xr
yr
pe,r
pe,r
Figure 8.32: The modulation APP decoder as a transformer of probability distributions
of the transmitted symbols xr.
The decoder operates as a memoryless symbol probability transformer—encapsulated
in the dashed box—in the iterative decoding systems. The assumed independence between
successive symbols is approximated by large interleavers.
Unfortunately due to the non-linear operation of the APP decoder, no analytical meth-
ods exist to ﬁnd the multi-dimensional distribution pe, and also the distribution pa is dif-
ﬁcult to obtain analytically. It is therefore typical to evalute these information measures
using Monte-Carlo simulations of the individual codes of interest.
Plots of I(X, pa) versus I(X, pe), i.e., (EXIT) charts, can be used to study the con-
vergence behavior of large iterative decoding systems, such as determining the onset of
the turbo cliﬀ, just as in the case of binary coded systems. Examples of EXIT charts are
presented in subsequent sections for a number of speciﬁc coded modulation systems.
8.17
Diﬀerential-Coded Modulation
An elegant method of combining coding and modulation is to use a diﬀerential encoder
as the modulation code. Diﬀerential modulation is accomplished by generating the trans-
mitted symbols as the (complex) product of the information symbol vr with a previous
transmitted symbol, i.e., xr = xr−1vr, vr, xr ∈X. The process is initiated at the beginning

410
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
of the transmission with a known symbol x0 = s(i). In the case of N-PSK the multiplica-
tion is traditional complex multiplication, but other forms are also used. In Section 8.18
we discuss systems where X is a set of unitary space–time codes.
Basic diﬀerential decoding is accomplished by extracting an estimate for vr from the
two most recently received channel symbols yr and yr−1 as
ˆvr = max
v
 [vy∗
ryr−1] .
(8.45)
However, we are more interested in using the diﬀerential code in conjunction with an
outer code in a serially concatenated coding system. Figure 8.33 shows the structure of this
serially concatenated coded modulation system. Encoding is performed on a symbol-by-
symbol level: The outer code, a binary error control code, outputs m-tuples of bits. The
stream of these binary m-tuples are passed through the interleaver and are mapped into
symbols from an M = 2m-ary symbol alphabet X. They are then diﬀerentially modulated
according to xr = xr−1vr, where D in Figure 8.33 is a single symbol delay. The mapping
from the binary symbols into modulation symbols vr is typically chosen as the natural
mapping discussed in Chapter 3. The interleaver can operate on a binary bit level, this
is advantageous especially if the binary code used is a very simple code with little error
correction capability. Both symbol- and bit-interleavers are considered in the sequel.
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
ur
Symbol
Mapper
Interleaver
Binary
Encoder
Diﬀerential
Modulator
xr
vr
D
×
Figure 8.33: Block diagram of a serial encoder for diﬀerential turbo coded modulation.
Since the diﬀerential modulator is an inﬁnite impulse response ﬁlter, it ﬁts the require-
ments of a suitable “inner code” in a serially concatenated coding system. A symbol-wise
turbo decoder [19, 20] for this is identical to that in Figure 8.31.
Such systems have
been studied for the concatenation of convolutional codes with diﬀerential BPSK reported
[41, 54], leading to very simple encoders and decoders with very good error performance
results, rivaling those of more complex Turbo codes.
Figure 8.34 shows the performance of a serially concatenated diﬀerential 8-PSK mod-
ulator with a [3,2,2] outer parity code [42, 43].
The bits of the outer parity code are

8.17. DIFFERENTIAL-CODED MODULATION
411
bit interleaved, since symbol interleaving would not generate independent inputs within
the parity-check codewords, which are only of length 3 each. The block size used was
5,000 symbols, corresponding to 10,000 information. As conjectured, the rapid drop-oﬀ
indicates the threshold behavior typical of turbo-coded systems. Note, however, that this
code suﬀers from a high dfree error ﬂoor.
50 iterations
25 iterations
10 iterations
5 iterations
3 iterations
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
dfree asymptote
Bit Error Probability (BER)
dB Eb
N0
8-PSK
Diﬀerential 8-PSK
Figure 8.34: Performance of the serially concatentated diﬀerential 8-PSK system with an
outer parity-check code, compared to regular and diﬀerential 8-PSK modulation.
Using the analysis techniques discussed in Section 8.16, we can explain the error per-
formance behavior exhibited in Figure 8.34. First, using EXIT analysis, we ﬁnd the EXIT
chart for this system using the diﬀerential 8-PSK modulator as the inner code, shown in
Figure 8.35. The input mutual information Ia = I(X, pa) is plotted on the horizontal axis
and the output extrinsic mutual information I(X, pe) on the vertical axis. Also plotted is
the EXIT curve of the [3,2,2] parity-check code used as the outer code. The axis extends
from 0 to 1 bit of mutual information; that is, the measure has been normalized per coded

412
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
bit. This EXIT chart shows that a narrow convergence channel opens at Eb/No = 3.4 dB,
exactly where the turbo cliﬀoccurs in the simulations.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
3.6 dB
Parity-
Check
Code
IAouter,IEinner
IAinner, IEouter
Dashed:
Convolutional
Code @ 5 dB
Figure 8.35: Extrinsic information transfer (EXIT) chart for serial concatenation of a
[3,2,2] parity-check code with a diﬀerential 8-PSK code.
The trajectory is drawn for
Eb/N0 = 3.6 dB. A 16-state convolutional outer code shown in dashed would not converge
until Eb/N0 > 5 dB.
The [3,2,2] parity-check code is very well matched to the diﬀerential modulator, since
their respective EXIT curves are nearly parallel. Furthermore, the APP decoder for the
[3,2,2] parity-check code is extremely simple and can be implemented, for example, in a
single ROM lookup table. From the parity-check constraint, it is quite straightforward to
calculate the output extrinsic bit log-likelihood ratio as
λE(v1) = λA(v2) + log
1 + exp(λA(v3) −λA(v2))
1 + exp(λA(v3) + λA(v2))

,
(8.46)
where v1, v2, v3 are the bits that make up a single parity-check codeword. It is important
that the interleaver between the codes uses bit-interleaving, since the parity-check code
requires bit-independent a priori probabilities.

8.17. DIFFERENTIAL-CODED MODULATION
413
The error ﬂoor is explained and quantiﬁed via a distance spectrum analysis [43]. Since
the inner code has a fully connected trellis, the shortest detour consists of two symbols.
Furthermore, the symbols on the merging path into a given state are all indentical, since
the state equals the most recently transmitted symbol. The minimum distance of the
diﬀerential 8-PSK code is therefore easily evaluated as d2
free = 0.586, which is very small
and provides no improvement over regular 8-PSK.
Including the outer code in our analysis, we ask if such short paths are permissible
by the constraints of the outer code. Table 8.2 lists all bit weights that are mapped into
length 2 detour path from the all-zero path, using the natural labeling of 8-PSK symbols.
Symbol 1
Symbol 2
d2
Weight
001
111
0.586
even
111
001
0.586
even
010
110
2
odd
110
010
2
odd
011
101
3.414
even
101
011
3.414
even
100
100
4
even
Table 8.2: Bit weights that can be mapped into pairs of diverging and merging symbols
of the diﬀerential 8-PSK trellis.
Note that since the parity-check outer code enforces an overall even weight of the
codeword, all pairs with even weight are possible, given that the interleaver maps the bits
accordingly. Furthermore, even the small d2 = 0.586 is permissible by the outer code.
Extensive experiments with random interleavers have shown that the minimum distance
of the complete code equals d2
min = 1.172 with very high probability, corresponding to two
length 2, minimum distance error events.
This can be dramatically improved by choosing a non-natural alternate mapping which
maps the triples (000), (111), (001), (100), (010), (011), (110), (101) onto the 8-PSK symbols
in a counterclockwise fashion. The weights of length 2 detours are given in Table 8.3.
The only even-weight detour has a large distance, and all of the short-distance detours
have odd weight and are thus not allowed by the outer code.
Searches with speciﬁc
interleavers have revealed that the minimum distance of the code with this mapping is
d2
min = 2.344. The performance of this mapping is the one shown in Figure 8.34.

414
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
Symbol 1
Symbol 2
d2
Weight
111
101
0.586
odd
101
111
0.586
odd
001
110
2
odd
110
001
2
odd
100
011
3.414
odd
011
100
3.414
odd
010
010
4
even
Table 8.3: Bit weights that can be mapped into pairs of diverging and merging symbols
of the diﬀerential 8-PSK trellis.
8.18
Concatenated Space–Time Coding
As a ﬁnal example we discuss the application of serial concatenation to space–time coding
for multiple-input multiple-output channels—see Chapter 2. Under certain conditions,
the channel capacity of such a MIMO channel can be increased by a factor as large as
the minimum of the number of transmit and receive ports. The information theoretic
concepts regarding multiple antenna transmission shall not be discussed here, but there
are a sequence of papers that the reader may want to explore [73, 35, 72].
We view
the multiple antenna channel as a special case of a modulator, where the modulation
symbols are Nt × Nc matrices with entries which are complex signal points from complex
constellations, and Nt is the number of transmit antennas and Nc is length of the space–
time modulation symbol, i.e., the number of columns making up the transmit matrix.
Typically each of the NtNr subchannels hij from antenna i to antenna j is subject
to independent signal fading, as discussed Chapter 2. At time r, each transmit antenna
i = 1, . . . , t sends a complex symbol cir, which is modulated onto a pulse waveform and
transmitted over the channel. Modulation and transmission are identical to the single-
antenna channel.
Taken as a vector, cr = c1r, . . . , cNtr is referred to as a space–time
symbol.
At each receive antenna j = 1, . . . , Nr, the signal is passed through a ﬁlter
matched to the pulse waveform and sampled synchronously. These samples are modeled
as yjr =

Es/Nt
Nt
i=1 hjicir + njr, where njr is a complex Gaussian noise sample. The
parameter Es is the average (received) signal power per space–time symbol, and Es/(NtN0)
is therefore the symbol signal-to-noise ratio at each receive antenna.
The transmitted space-time symbols are customarily collected into Nt ×Nc space-time
codeword (STC) matrices, C = [ct
1, . . . , ct
Nc], where rows correspond to diﬀerent transmit
antennas and columns correspond to diﬀerent times. Considering a sequence of L such

8.18. CONCATENATED SPACE–TIME CODING
415
STC transmissions C1, C2, . . . , CL, the channel can be written as
Yr =

Es
Nt
HrCr + Nr,
(8.47)
where Yr is the rth received STC, Hr is the Nr × Nt channel matrix of transmission path
gains, and Nr is an Nr × n matrix of complex noise samples.
While we will not study fading channels in any detail (see references [56, 45], for ex-
ample) multiple-antenna communication usually is studied in the context of signal fading,
where each hij is modeled as an independent Rayleigh fading process by selecting the
elements of Hr as zero-mean unit variance complex Gaussian random variables with i.i.d.
real and imaginary parts. One usually distinguishes between fast fading, in which the
Hl evolve according to a process whose dominant frequency is much faster than 1/L, but
slower than 1/Nc, and quasi-static fading, in which the Hr are constant for groups of L
code matrices corresponding to the transmission of a complete frame.
The ﬁrst example of a serially concatenated system using space–time codes, discussed
by Schlegel and Grant [63] uses a diﬀerential STC presented by Hughes [44] which is based
on unitary matrices with a group structure. A similar serially concatenated system is also
presented in [2]. In Hughes’ code, each STC word takes the form C = DG, where D is an
Nt × Nc matrix and G belongs to a group of unitary Nc × Nc matrices (GG∗= I), i.e.,
the product of any two matrices G1G2 results in another matrix G3 of the group. The
Nc columns of C are transmitted as Nc consecutive space–time symbols.
A basic example of such a code with t = n = 2 is the Quaternion Code with symbols
from the QPSK constellation. There are 8 STC words, given by
Q =

±
1
0
0
1

, ±
j
0
0
−j

, ±
 0
1
−1
0

, ±
0
j
j
0

,
(8.48)
D =
1
−1
1
1

,
(8.49)
This code is isomorphic to Hamilton’s Quaternion Group, hence its name.
The novelty of this code is that it can be diﬀerentially encoded and decoded analogous
to diﬀerential PSK. At the start of transmission, the transmitter sends the STC word
C0 = D. Thereafter, messages are diﬀerentially encoded: To send Gr ∈Q during symbol
time r, the transmitter sends
Cr = Cr−1Gr.
(8.50)
The group property of the space–time code guarantees that Cr is a codeword if Cr−1 is
a codeword. Like diﬀerential PSK, a diﬀerential receiver for Cr exists based on the two
most recent blocks. It computes [44]
ˆG = max
G∈Q  tr GY∗
rYr−1.
(8.51)

416
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
While such diﬀerential codes can be used without channel knowledge as well as provide
rapid training on time-varying channels, our purpose is to use them as inner codes in
concatenated systems. The diﬀerential nature of these codes can furthermore be exploited
in rapid channel estimation algorithms, and [63] contains a more detailed discussion of the
use of these codes in conjunction with unknown channels.
Figure 8.36 shows the EXIT chart for the system discussed using the quaternion dif-
ferential STC as the inner code (Ia = I(X, pa) on the horizontal axis and I(X, pe) on the
vertical axis) and 4, 16 and 64 state maximal free distance rate 2/3 convolutional codes
(Chapter 4) as the outer code (their input I(X, pa) is plotted on the vertical axis). The
[3,2,2] outer parity-check code from the last section is also used. The axis extend from 0
to 3 bits of mutual information, where the maximum of 3 bits corresponds to complete
knowledge of s(i) derived from pe. In the case of bit interleaving, input and output bit-
wise mutual information values are multiplied by three to normalize them to symbols. The
diﬀerential STC extrinsic information transfer curves are shown for various signal-to-noise
ratios, ranging from -1 dB to -1.6 dB in steps of 0.1 dB, and have been evaluated for
independent Rayleigh fading MIMO channels.6
From the ﬁgure, we see that we expect the turbo threshold to occur near -1.2 dB for
the 64 state outer code and around -1.3 dB for the 16- and 4-state codes. We also expect
that the 16- and 64-state outer codes to result in faster convergence, since the convergence
channel between the inner and outer curves are more open for these codes. The parity-
check code has the lowest turbo cliﬀat -1.35 dB since its extrinsic information exchange
curve is best matched to the shallow curve of the inner decoder.
Figure 8.37 compares the performance of the concatenated space–time system using the
outer codes discussed. The interleaver length was 50,000 symbols, and up to 25 decoding
iterations were performed. As predicted by the EXIT analysis, the turbo cliﬀs occur at
the thresholds of -1.35 dB, -1.3 dB and -1.2 dB respectively, and the performance sharply
follows the turbo cliﬀat exactly the predicted values. At this code rate (1 bit per channel
use), capacity is at -3.1 dB [73]. Thus the concatenated coding system can achieve virtually
error free performance at slightly more than 1.5 dB from the unconstrained MIMO channel
capacity. The more shallow drop-oﬀof the error curve of the 4-state outer convolutional
code is due to the fact that symbol interleaving rather than bit interleaving is used, which
underlines the importance of using bit interleaving for weaker outer codes.
It is quite astonishing to see such performance being achieved by the cooperation of
two component codes which are individually quite useless. Note that the [3,2,2] parity
check code by itself cannot correct any errors and that the quaternion space–time code
in its diﬀerential encoder realization is catastrophic (see Chapter 4), which also by itself
will not impress with good performance. It is diﬃcult to imagine that coding systems
6The apparent violation of Shannon’s absolute limit of Eb/N0 > −1.59 dB stems from the fact that he
signal-to-noise ratio in MIMO systems is measured per receive antenna.

8.18. CONCATENATED SPACE–TIME CODING
417
 
0
0.6
1.2
1.8
2.4
3
0
0.6
1.2
1.8
2.4
3
-1dB
-1.6dB
Parity
4-state
16-state
64-state
I(X, pe)
I(X, pa)
Figure 8.36: Extrinsic information transfer (EXIT) chart for the serial concatenation of
4-,16-, and 64-state convolutional codes and a [3,2,2] parity-check code with the diﬀerential
space–time quaternion code, the latter’s transfer curves are dashed.
with lower complexity can be found that perform this well.
The combination of such
simple component codes into a serial coding system with such strong performance and
manageable computational decoding complexity is the real contribution of the invention
of turbo coding.
Other combinations of codes have also been used successfully.
Hochwald and ten
Brink [40] have concatenated complete turbo codes with a memoryless signal mapper
for 4 × 4 and 8 × 8 antenna arrays. Their decoder also operates according to the serial
decoding principle from Figure 8.31, but due to the large complexity of the ﬁrst APP,
which has to generate LLRs for individual bits from the high-level modulated input signal,
an approximate decoder was used. Nonetheless, the results achieved even for high-level
modulation alphabets are excellent, as shown in Figure 8.38.

418
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
-1.5
-1.4
-1.3
-1.2
-1.1
-1
-0.9
-0.8
-0.7
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
Parity
4-state
16-state
64-state
pinch-oﬀ
pinch-oﬀ
pinch-oﬀ
Figure 8.37: Performance of the serially concatentated system with several outer codes for
a fast fading channel. (Note the entire scale is only 1.4 dB.)
These simulation use parallel concatenated turbo codes as outer codes, each having
memory-2 component encoders given by h1(D) = 1+D2 and feedback polynomial h0(D) =
1 + D + D2. The block size N = 18,432. As can be calculated from the modulation
alphabet and the number of transmission antennas, the spectral eﬃciencies are 4, 8, and
12 bits/channel use for QPSK, 16QAM, and 64QAM for the 4 × 4 system, and twice that
for the 8 × 8 MIMO example.
A general trend becomes evident: It is more and more diﬃcult to approach capacity
with larger modulation alphabets. The 64QAM, 8-antenna system is almost 6 dB away
from its capacity potential while the smaller constellation and fewer transmit antenna
systems come very close to channel capacity. Note also that a much more complex coding
machinery is being used than in the previous example of serial concatenation.
8.19
Bit-Interleaved Coded and Generalized Modulation
If we compare the encoding methods of Figure 8.33 with that of Figure 8.30, we note
a subtle diﬀerence.
The latter encoder assigns bits to symbols before the interleaving
process, while the encoder in Figure 8.33 interleaves the binary bit stream before assigning
(mapping) bits to symbols. This process is a form of bit-interleaved modulation, studied

8.19. BIT-INTERLEAVED CODED AND GENERALIZED MODULATION
419
0
2
4
6
8
10
12
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0[dB]
64-QAM
QPSK
16-QAM
capacity QPSK
capacity 16-QAM
capacity 64-QAM
Figure 8.38: Performance of serially concatentated space-time coded systems with outer
turbo codes for high spectral eﬃciencies, examined for 4×4 (solid lines) and 8×8 (dashed
lines) MIMO arrangements.
in [22] following the introduction of the concept by Zehavi [81]. The eﬀect of this is that
each encoded binary digit passes through its “own” independent channel if the interleaver
is large enough, such that the individual bits can be assumed to be aﬀected by independent
channel fading levels.
Now on the additive white Gaussian noise channel which forms the primary basis for
the discussions in this book, the diﬀerence between the two methods is somewhat academic,
since the goal of the encoder is to (i) create a coding system with a convergence threshold
as close to channel capacity as possible and (ii) avoid codeword pairs whose Euclidean
distance is small, which then would cause an error ﬂoor in the BER versus signal-to-noise
ratio relationship.
However, on certain other channel models, such as ideally interleaved ﬂat fading chan-
nels, the Euclidean distance is no longer the main performance criteria [26, 27, 62]. Instead,
one ﬁnds that on such channels the Hamming distance, i.e., the number of positions where
two codewords diﬀer, is the primary performance parameter. The minimum Hamming
distance of the code determines the diversity of the code and translates into the slope of
the BER versus signal-to-noise ratio behavior. Now, if one interleaves symbols, the symbol
Hamming distance is the relevant parameter, and if individual bits are interleaved, the bit
Hamming distance becomes the primary code parameter. It is quite easy to see that the

420
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
bit Hamming distance of a code can be substantially larger than its symbol Hamming dis-
tance, giving bit-interleaving a signiﬁcant edge on such channels. Reference [22] presents
cutoﬀrates for symbol and bit-interleaved coded modulation showing quantitative rate
advantages of bit interleaving for a variety of constellations from QPSK to 256 QAM.
While the original motivation for bit interleaving [which was the challenge to make
trellis-coded modulation (as in Chapter 3) more robust on interleaved ﬂat fading channels]
is perhaps no longer critical since modern codes typically have a large Hamming distance
in either symbols or bits, bit interleaving may have other attractive properties. One of
these is that separate data streams can naturally be combined as illustrated in Figure 8.39.
(000)
(010)
(110)
(100)
(011)
(101)
(111)
(001)
u1
u2
u3
Mapper
Interleaver
Interleaver
Interleaver
Binary Encoder
Binary Encoder
Binary Encoder
xr
Figure 8.39: Block diagram of a bit-interleaved modulator with separated binary streams.
The advantage of the view espoused in Figure 8.39 is that the diﬀerent binary data
streams do not necessarily need to come from the same source.
In multiple antenna
systems, for example, each data stream ui could modulate an individual antenna, with
coding and much of the receiver processing done separately and independently of the other
data streams. This principle is applied to such a MIMO system in [68]. Furthermore, the
diﬀerent binary encoders could be in separate locations, in which case the “mapper” would
be the channel itself; in this case we have a multiple-access channel—see [64] for a more
general discussion on multiple access communications.
As discussed in Chapter 3, coded modulation consists of mapping one or several binary
data streams onto signals from a ﬁnite set of such signals according to a mapping rule.
One convenient way of creating the ubiquitous PAM and QAM constellation is to ﬁrst
select one of 2B amplitude levels for each of B binary data streams. In the case of 8-PAM
with B = 3, the discrete equi-spaced amplitudes shown in Figure 8.40 are generated as the
superposition of three binary constellations, where Bit u1 has 4 times the power of Bit u0,
and Bit u2 has 16 times its power. In eﬀect, this is nothing more than set-partitioning in
reverse, where Bit u0 has the least intraset distance and u2 has the largest.
In general, any properly labeled 2B-PAM modulation can be written as the superpo-
sition of B binary antipodal amplitudes, i.e.,
x =
B−1

j=0
2juj
(8.52)

8.19. BIT-INTERLEAVED CODED AND GENERALIZED MODULATION
421
(111)
(110)
(101)
(100)
(011)
(010)
(001)
(000)
Bit 0
Bit 1
Bit 2
Figure 8.40: Pulse-amplitude modulation (PAM) as superposition of binary antipodal
modulation with a geometric power distribution.
where uj ∈{−1, 1}. If an entire sequence v of 2B-ary PAM symbols is considered, it may be
viewed as superposition of B binary data streams with powers P0, 4P0, 16P0, . . . , 4B−1P0
on binary data streams which make up the PAM symbol sequence.
In Figure 8.40 all the binary data streams modulate the same basis waveforms, and
therefore the signal points are all co-linear in the geometric signal space representation.
However, in multiple user transmission systems, the diﬀerent coordinated data streams
may not necessarily modulate the same waveforms—in fact, this may be rather hard to
achieve in practice. The concept of bit-interleaving extended to this case is illustrated in
Figure 8.41 below and has been dubbed generalized modulation in [65, 67].
u1
u2
u3
+
Interleaver
Interleaver
Interleaver
s1
s2
s3
Binary Encoder
Binary Encoder
Binary Encoder
xr
Figure 8.41: Block diagram of a generalized modulation transmission system using sepa-
rated and physically unconnected binary data streams.
This viewpoint is quite productive in that it suggests a capacity-achieving demodula-
tion method for large constellations based on cancellation. Consider the case where the
highest-power uncanceled data stream considers all lower power data streams as noise. Its

422
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES
maximum rate is then given by the mutual information
Cj = I (uj; y|uj+1, . . . , uB−1)
(8.53)
where y is the output of the channel. As long as the rate on the jth binary data stream
is Rj < Cj, it can be correctly decoded using a binary Shannon-capacity achieving code.
By virtue of (8.52), knowledge of uj+1, . . . , uB−1 implies that these data streams can be
canceled from the received signal, and Cj is the capacity of the jth binary data stream.
This thought model leads to a successive decoding and canceling method which can achieve
the mutual information rate
Csymmetric = I(v; y) =
B−1

j=0
Cj
(8.54)
by the chain rule of mutual information. Csymmetric is of course not equal to the capacity
of the additive white Gaussian noise channel y = v + n, since the input distribution of v
is uniform, rather than Gaussian distributed as required to achieve the channel capacity.
In fact, Csymmetric loses the so-called shaping gain of 1.52 dB w.r.t. the capacity of the
Gaussian channel [27], unless the basis waveforms used in the modulation process are not
equal, i.e., s1 = s2 = s3.
In [67] the authors consider the generalized PAM modulation method which operates
with random signals, rather than the more typical orthogonal bases and uses repetition
codes as binary error control codes. A two-stage iterative demodulator/decoder, which op-
erates in parallel analogously to an LDPC decoder with an iterative cancelation operation
instead of the check node processor, achieves a signal-to-noise ratio (SNR) improvement
on each of the binary data streams, such that when these are then treated by external
binary error control codes a cumulative data rate per dimension
Rd ≥0.995CGMAC −0.12
[bits/dimension]
(8.55)
can be achieved, where CGMAC is the Shannon capacity of the Gaussian multiple access
channel. That is, this system can achieve a fraction of 0.995 of channel’s information the-
oretic capacity. The result does not depend on size of the system and holds for arbitrarily
many data streams ui.
We conclude this chapter with a potential application of generalized modulation to a
multiple access system using user-speciﬁc signature sequences (see [64]). Figure 8.42 shows
the block diagram of such a multiple user transmitter—receiver chain. Each of a number
K of diﬀerent users transmits a coded binary data stream, where each bit is modulated
with a signature waveform, typically a spread-spectrum signal sk, as illustrated in the top
part of Figure 8.42. Without going into details, the resulting channel model is analogous
to complex vector channel equation (2.50) of the MIMO channel discussed in Section 2.8.2.

8.19. BIT-INTERLEAVED CODED AND GENERALIZED MODULATION
423
Signature Sequence
Signature Sequence
LLR
Extrinsic
Accumulator
Error Control
Encoder
Upsampler
Convolutional
Interleaving
Convolutional
Deinterleaving
Modulator
Transmitter
Partition Level Processing
Symbol Level Processing
Receiver
Other
Users
Loop User 3
Loop User 2
Signature Sequence
Soft
Demodulator
Error Control
Decoder
Channel
+
+
+
-
-
-
APP Symbol
Estimator
Convolutional
Interleaving
Convolutional
Interleaving
APP Symbol
Estimator
Convolutional
Interleaving
APP Symbol
Estimator
M
Signature Sequence
Signature Sequence
Matched Filter
Figure 8.42: Application of generalized modulation to a multiple user system.
The error control coding system is basically external to the system.
However, the
encoders are followed by a repetition code of rate 1/M, shown in the ﬁgure as the up-
sampling process, which acts as the error control part in the serially concatenated system
which consists of the repetition code and the multiple access channel. The partitioned
level processing block constitutes the generalized modulator and demodulator, using the
signature sequences of the diﬀerent users sk. These signature waveforms are not, and
cannot easily be made to be orthogonal. Therefore, the compound constellation at the
receiver takes on the form of a generalized modulated signal as discussed above.
Decoding is performed via iterative cancellation and remodulation after symbol LLR
combining. The partition-level processing block takes on the form of an LDPC decoder,
where “check nodes” are replaced by cancelation nodes. Details of the processing structure
and a performance analysis is beyond the scope of this text, but can be found in [65, 67,
66], and an application of the principle to multiple-antenna system is discussed in [68].
This version of generalized modulation is of considerable theoretical interest since it can
be shown that the Gaussian multiple access channel capacity can be achieved with this
structure, in conjunction with spatial coupling [79]—discussed further in Chapter 10.

424
CHAPTER 8. TURBO CODING: BASIC PRINCIPLES

Bibliography
[1] D. Arnold and G. Meyerhans, The Realization of the the Turbo-Coding System, Swiss
Federal Institute of Technology, Zurich, Switzerland, July 1995.
[2] I. Bahceci and T.M. Duman, “Combined turbo coding and unitary space–time mod-
ulation,” IEEE Trans. Commun., vol. 50, no. 8, pp. 1244–1249, Aug. 2002.
[3] A.S. Barbulescu and S.S. Pietrobon, “Interleaver design for turbo codes,” Elec-
tron. Lett., vol. 30, no. 25, p. 2107, Dec. 1994.
[4] A. S. Barbulescu and S. S. Pietrobon, “Terminating the trellis of turbo-codes in the
same state,” Electron. Lett., vol. 31, no. 1, pp. 22–23, Jan. 1995.
[5] A. S. Barbulescu and S. S. Pietrobon, “Rate compatible turbo codes,” Electron. Lett.,
vol. 31, no. 7, pp. 535–536, March 1995.
[6] V. Bhargava and S. Wicker, Reed–Solomon Codes and Their Applications, Prentice
Hall, Englewood Cliﬀs, NJ, 1988.
[7] G. Battail, C. Berrou and A. Glavieux, “Pseudo-random recursive convolutional cod-
ing for near-capacity performance,” Proceedings of the Communication Theory Mini-
Conference, Globecom ’93, Houston, Texas, pp. 23–27, Dec. 1993.
[8] G. Battail, “On random-like codes,” unpublished manuscript.
[9] S. Benedetto and G. Montorsi, “Average performance of parallel concatenated block
codes,” Electron. Lett., vol. 31, no. 3, pp. 156–158, Feb. 1995.
[10] S. Benedetto and G. Montorsi, “Performance evaluation of TURBO-codes,” Elec-
tron. Lett., vol. 31, no. 3, pg. 163–165, Feb. 1995.
[11] S. Benedetto and G. Montorsi, “Unveiling turbo codes: some results on parallel con-
catenated coding schemes,” IEEE Trans. Inform. Theory, vol. IT-42, no. 2, pp. 409–
428, March 1996.
[12] S. Benedetto and G. Montorsi, “Design of parallel concatenated convolutional codes,”
IEEE Trans. Commun., vol. COM-44, no. 5, pp. 591–600, May 1996.
425

426
BIBLIOGRAPHY
[13] S. Benedetto and G. Montorsi, “Serial concatenation of block and convolutional
codes,” IEE Electron. Lett., vol. 32, no. 13, pp. 1186–1187, June 1996.
[14] S. Benedetto, R. Garello, and G. Montorsi, “A search for good convolutional codes to
be used in the construction of Turbo codes,” IEEE Trans. Commun., vol. COM-46,
no. 9, pp. 1101–1105, Sep. 1998.
[15] S. Benedetto and G. Montorsi, “Iterative decoding of serially concatenated convolu-
tional codes,” IEE Electron. Lett., vol. 32, no. 10, pp. 887–888, May 1996.
[16] S. Benedetto, D. Divsalar, G. Montorsi, and F. Pollara. “Serial concatenation of in-
terleaved codes: Performance analysis, design, and iterative decoding,” TDA Progress
Report 42 -126, pp. 1–26, Aug. 1996.
[17] S. Benedetto, G. Montorsi, D. Divsalar, and F. Pollara. “A soft-input soft-output
maximum a posteriori (map) module to decode parallel and serial concatenated
codes,” TDA Progress Report 42-127, pp. 1–20, Nov. 1996.
[18] S. Benedetto, D. Divsalar, G. Montorsi, and F. Pollara, “Serial concatenation of inter-
leaved codes: Performance analysis, design, and iterative decoding,” IEEE Trans. In-
form. Theory, Vol. 44, no. 3, pp. 909–26, May 1998.
[19] C. Berrou, A. Glavieux, and P. Thitimajshima, “Near Shannon limit error-correcting
coding and decoding: Turbo-codes,” Proceedings 1993 IEEE International Conference
on Communication, Geneva, Switzerland, pp. 1064–1070, 1993.
[20] C. Berrou and A. Glavieux, “Near optimum error correcting coding and decoding:
Turbo-codes,” IEEE Trans. Commun., vol. 44, no. 10, pp. 1261–1271, Oct. 1996.
[21] C. Berrou and A. Glavieux, “Turbo-codes: General principles and applications,”
Proceedings of the 6th Tirrenia International Workshop on Digital Communications,
Tirrenia, Italy, pp. 215–226, Sep. 1993.
[22] G. Caire, G. Taricco, and E. Biglieri, “Bit-interleaved coded modulation,” vol. 44,
no. 3, May 1998.
[23] G. Caire, G. Taricco, and E. Biglieri, “On the convergence of the iterated decoding
algorithm,” Proceedings of the 1995 IEEE International Symposium on Information
Theory, Whistler, British Columbia, Canada, p. 472, Sep. 1995.
[24] M. Cedervall and R. Johannesson, “A fast algorithm for computing distance spec-
trum of convolutional codes,” IEEE Trans. Inform. Theory, IT-35, pp. 1146–1159,
Nov. 1989.
[25] S.Y. Chung, G.D. Forney, T.J. Richardson, and R. Urbanke, “On the design of low-
density parity-check codes within 0.0045 dB of the Shannon limit,” IEEE Comm.
Lett., vol. 16, 1999.

BIBLIOGRAPHY
427
[26] D. Divsalar and M.K. Simon, “The design of trellis coded MPSK for fading channels:
Performance criteria,” IEEE Trans. Commun., vol. 36, pp. 1004–1012, Sept. 1988.
[27] D. Divsalar and M.K. Simon, “The design of trellis coded MPSK for fading channels:
Set partitioning for optimum code design,” IEEE Trans. Commun., vol. 36, pp. 1013–
1022, Sept. 1988.
[13] D. Divsalar, H. Jin and R. J. McEliece, “Coding theorems for ‘turbo-like’ codes,”
Proceedings of the 1998 Allerton Conference, pp. 928–936, Oct. 1998.
[28] D. Divsalar and F. Pollara, “Turbo codes for PCS applications,” Proceedings of the
1995 IEEE International Conference on Communications, Seattle, Washington, June
1995.
[29] D. Divsalar and F. Pollara, “Turbo codes for deep-space communications,” JPL TDA
Progress Report 42-120, Feb. 1995.
[30] D. Divsalar, S. Dolinar, F. Pollara and R.J. McEliece, “Transfer function bounds
on the performance of turbo codes,” JPL TDA Progress Report 42-122, pp. 44–55,
Aug. 1995.
[31] S. Dolinar, “A New Code for Galileo,” TDA Progress Report PR 42-93: January–
March 1988, pp. 83–96, May 1988.
[32] S. Dolinar and D. Divsalar, “Weight distribution for turbo codes using random and
nonrandom permutations,” JPL Progress report 42-122, pp. 56–65, Aug. 1995.
[33] D. Divsalar, S. Dolinar, and F. Pollara, “Iterative turbo decoder analysis based on
density evolution,” TMO Progress Report 42-144, Feb. 2001.
[34] European Telecommunications Standards institute (ETSI) EN 302 307 V1.2.1, Euro-
pean Standard (Telecommunications series), 2009-08.
[35] G.J. Foschini, “Layered space-time architecture for wireless communication in a fading
environment when using multi-element antennas,” Bell Labs Techn. J., vol. 1, no. 2,
pp. 41–59, Aug. 1996.
[36] G. D. Forney, Concatenated Codes, MIT Press, Cambridge, MA, 1966.
[37] R. Garello, P. Pierleni and S. Benedetto, “Computing the free distance of turbo
codes and serially concatenated codes with interleavers: Algorithms and applica-
tions,” IEEE J. Select. Areas Commun., vol. 19, no. 5, pp. 800–812, May 1995.
[38] J. Hagenauer, E. Oﬀer and L. Papke, “Iterative decoding of binary block and con-
volutional codes,” IEEE Trans. Inform. Theory , vol. 42, no. 2, pp. 429–445, March
1996.
[39] C. Heegard and S. Wicker, Turbo Coding, Kluwer Academic Publishers, Boston, 1999.

428
BIBLIOGRAPHY
[40] B.M. Hochwald and S. ten Brink, “Achieving near-capacity on a multiple-antenna
channel,” IEEE Trans. Commun., vol. 51, no. 3, pp. 399, March 2003.
[41] P. H¨oher and J. Lodge, “Turbo DPSK: iterative diﬀerential PSK demodulation and
channel decoding,” IEEE Trans. Commun., vol. 47, no. 6, pp. 837–843, June 1999.
[42] S. Howard, C. Schlegel, L. P´erez, and F. Jiang, “Diﬀerential turbo coded modulation
over unsynchronized channels,” Wireless and Optical Communications Conference,
(WOC 2002) July 17–19, 2002, Banﬀ, Alberta, Canada
[43] S. Howard, C. Schlegel, “Diﬀerential turbo coded modulation with APP channel
estimation,” IEEE Trans. Commun., vol. 54, no. 8, pp. 1397–1406, Aug. 2006.
[44] B. Hughes, “Diﬀerential space-time modulation,” IEEE Trans. Inform. Theory, vol.
46, no. 7, pp. 2567–2578, Nov. 2000.
[45] W.C. Jakes, Microwave Mobile Communications, John Wiley & Sons, 1994.
[46] F. Jiang, Design and Analysis of Serially Concatenated Convolutional Codes, Master’s
Thesis, The University of Nebraska, Lincoln, Dec. 2001.
[47] O. Joerssen and H. Meyr, “Terminating the trellis of turbo-codes,” Electron. Lett.,
vol. 30, no. 16, pp. 1285–1286, Aug. 1994.
[48] P. Jung and M. Naßhan, “Performance evaluation of turbo codes for short frame
transmission systems,” Electron. Lett., vol. 30, no. 2, pp. 111–113, Jan. 1994.
[49] P. Jung and M. Naßhan, “Dependence of the error performance of turbo-codes on
the interleaver structure in short frame transmission systems,” Electron. Lett., vol.
30, no. 4, pp. 285–288, Feb. 1994.
[50] P. Jung, “Novel low complexity decoder for turbo-codes,” Electron. Lett., vol. 31, no.
2, pp. 86–87, Jan. 1995.
[51] J.W. Layland and L.L. Rauch, “Case studies of technology in the DSN: Galileo,
voyager, mariner,” in Evolution of Technology in the Deep Space Network: A History
of the Advanced Systems Program, TDA Progress Report PR 42–130, April–June
1997, Aug. 1997.
[52] R.J. McEliece, E.R. Rodemich and J.-F Cheng, “The turbo decision algorithm,”
Proceedings of the 33rd Annual Allerton Conference on Communication, Control and
Computing, Monticello, IL, Oct. 1995.
[53] D. Arnold and G. Meyerhans, The Realization of the the Turbo-Coding System, Swiss
Federal Institute of Technology, Zurich, Switzerland, July 1995.
[54] M. Peleg, I. Sason, S. Shamai, and A. Elia, “On interleaved, diﬀerentially encoded
convolutional codes,” IEEE Trans. Inform. Theory, vol. 45, no. 7, pp. 2572–2582,
Nov. 1999.

BIBLIOGRAPHY
429
[55] L. C. P´erez, J. Seghers and D. J. Costello, Jr., “A distance spectrum interpretation of
turbo codes,” IEEE Trans. Inform. Theory, vol. 42, no. 6, pp. 1698-1709, Nov. 1996.
[56] J. Proakis, Digital Communications, 1st edition Prentice Hall, Upper Saddle River,
NJ, 2007.
[57] P. Robertson, “Illuminating the structure of parallel concatenated recursive system-
atic (TURBO) codes,” Proc. GLOBECOM ’94, vol. 3, pp. 1298–1303, San Francisco,
California, Nov. 1994.
[58] R. Robertson and T. W¨orz, “Coded modulation scheme employing turbo codes,”
Electron. Lett., vol. 31, no. 18, pp. 1546–1547, Aug. 1995.
[59] R. Robertson and T. W¨orz, “A novel bandwidth eﬃcient coding scheme employing
turbo codes,” Proc. IEEE Int. Conf. on Commun. ICC’96, vol. 2, pp. 962–967, 1996.
[60] R. Robertson and T. W¨orz, “Extensions of turbo trellis coded modulation to high
bandwidth eﬃciencies,” Proc. IEEE Int. Conf. Commun., vol. 3, pp. 1251–1255, 1997.
[61] R. Robertson and T. W¨orz, “Bandwidth-eﬃcient turbo trellis-coded modulation using
punctured component codes,” IEEE J. Select. Areas Commun., vol. 16, no. 2, pp.
206–218, Feb. 1998.
[62] C. Schlegel and D.J. Costello, Jr., “Bandwidth eﬃcient coding on fading channels:
Performance analysis and code construction,” IEEE J. Select. Areas Commun., vol. 7,
pp. 1356–1368, Dec. 1989.
[63] C. Schlegel and A. Grant, “Concatenated space–time coding,” IEEE Trans. Inform.
Theory., Vol. 49, No. 9, pp. 2298-2306, Sep. 2003.
[64] C. Schlegel and A. Grant, Coordinated Multi-User Communications, Coordinated
Multiuser Communications, Springer, 2006.
[65] C. Schlegel, Z. Shi, and M. Burnashev, “Asymptotically optimal power allocation and
code selection for iterative joint detection of coded random CDMA,” IEEE Trans.
Inform. Theory, vol. 52, no. 9, pp. 4286–4295, Sept. 2006.
[66] C. Schlegel and D. Truhachev, “Multiple access demodulation in the lifted signal
graph with spatial coupling,” IEEE Trans. Inform. Theory, vol. 59, no. 4, pp. 2459–
2470, 2013.
[67] C. Schlegel, M. Burnashev, and D. Truhachev, ”Generalized superposition modula-
tion and iterative demodulation: A capacity investigation,” J. Electr. Comput. Eng.,
vol. 2010, Article ID 153540, Aug. 2010.
[68] C. Schlegel, D. Truhachev, and Z. Bagley, “Transmitter layering for multi-user MIMO
systems,” EURASIP J. Wireless Commun. Networking, vol. 2008, Article ID 372078,
Jan. 2008.

430
BIBLIOGRAPHY
[69] J. Seghers, On the Free Distance of TURBO Codes and Related Product Codes, Final
Report, Diploma Project SS 1995, Number 6613, Swiss Federal Institute of Technol-
ogy, Zurich, Switzerland, Aug. 1995.
[70] J. Statman, G. Zimmerman, F. Pollara and O. Collins, “A long constraint length
VLSI Viterbi decoder for the DSN,” TDA Progress Report 42-95, July–Sept. 1998.
[71] Y. V. Svirid, “Weight distributions and bounds for turbo codes,” Eur. Trans. Telecom-
mun., vol. 6, vo. 5, pp. 543–556, Sept–Oct. 1995.
[72] V. Tarokh, A.F. Naguib, N. Seshadri, and A.R. Calderbank, “Space–time codes for
high data rate wireless communications: Performance criterion and code construc-
tion,” IEEE Trans. Commun. Theory, pp. 744–765, March 1998.
[73] I.E. Telatar, “Capacity of multi-antenna Gaussian channels,” Eur. Trans. Telecom-
mun., vol. 10, pp. 585–595, Nov. 1999.
[74] S. ten Brink, “Convergence behavior of iteratively decoded parallel concatenated
codes,” IEEE Trans. Commun., vol. 49, no. 10, pp. 1727-1737, Oct. 2001.
[75] S. ten Brink, “A rate one-half code for approaching the Shannon limit by 0.1 dB,”
IEE Electro. Lett., vol. 36, no. 15, pp. 1293–1294, July 2000.
[76] S. ten Brink, “Convergence of iterative decoding,” IEE Electron. Lett., vol. 35, no.
15, pp. 1117–1119, June 1999.
[77] S. ten Brink, “Design of serially concatenated codes based on iterative decoding con-
vergence,” Proceedings 2nd International Symposium on Turbo Codes & Related Top-
ics, Brest, France, Sept. 2000.
[78] S. ten Brink and G. Kramer, “Design of repeat-accumulate codes for iterative de-
tection and decoding,” IEEE Trans. Signal Proc., vol. 51, no. 11, pp. 2764–2772,
Nov. 2003.
[79] D. Truhachev and C. Schlegel, “Coupling data transmission for capacity-achieving
multiple-access communications,” arXiv:1209.5785.
[80] N. Wiberg, H.-A. Loeliger, and R. K¨otter, “Codes and iterative decoding on gen-
eral graphs,” Proceedings of the1995 IEEE International Symposium on Information
Theory, p. 468, June, 1995.
[81] E. Zehavi, “8-PSK trellis codes for a Rayleigh channel,” IEEE Trans. Commun.,
vol. 40, pp. 873–884, May 1992.

Chapter 9
Turbo Coding: Applications
9.1
Interleavers
When it comes to applying turbo codes, the low error ﬂoor often caused by random
interleaving is a problem, and much of the design of the codes goes into designing the
interleavers, especially since there is a limited choice of component codes that are practical.
An interleaver Π is a mapping of indices given by a permutation. All the possible
permutations of N elements comprise a permutation group; that is, the permutations
have an algebraic group structure, with an inverse element—the reverse permutation—
an identity element, and combining two permutations always produces a third. This is
the closure of the group. Permutations can be decomposed into cycles, for example, the
interleaver Π16 = {15, 10, 1, 12, 2, 0, 13, 9, 5, 3, 8, 11, 7, 4, 14, 6} from Section 8.2 can written
in the disjoint cycle decomposition with two cycles
Π16 = {(0, 15, 6, 13, 4, 2, 1, 10, 8, 5, 0), (3, 12, 7, 9, 3)},
(9.1)
which simply means that the symbols map as follows: 1 →15 →6 →13 →4 →2 →1 →
10 →8 →5 →0 →15 into a cycle of length 8, and a second cycle of length 4.
The interleaver can also be expressed by its index mapping function
dI(i) = j,
0 ≤i, j < N,
(9.2)
which we will use most to describe the interleaves.
Let us start with the most common interleaver, the block interleaver of size N = m×n,
which functions by writing the incoming information symbols row by row into a rectangular
array. Interleaving is accomplished by reading out the information column by column.
This interleaver has the indexing function
dIB(i) = ni +
 i
m

mod N,
(9.3)
431
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

432
CHAPTER 9. TURBO CODING: APPLICATIONS
where x is the largest integer < x. This ﬂoor function makes the interleaver diﬃcult to
analyze, and a ‘linearized’ interleaver is introduced in [28] with indexing function
dIN (i) = ki + u
mod N,
(9.4)
where k and u are ﬁxed integers, and k is relatively prime to N, i.e., the greatest common
divisor of k and N, gcd(k, N) = 1. The index k is called the angular coeﬃcient of the
linear interleaver. Coeﬃcients k of approximately
√
N seem to work best, resulting in
good scattering of the index points [28]. The interleaving function j = dIN (i) has only one
solution and thus describes a proper permutation with a one-to-one mapping of elements.
This can be seen as follows, assuming that two indices map into the same interleaved
index, that is,
ki1 + u
=
ki2 + qN,
k(i1 −i2)
=
qN.
(9.5)
But, since the gcd(k, N) = 1, and |i1 −i2| < N, we conclude that i1 = i2.
Furthermore, if the input sequence has weight 2, and the two 1’s are separated by t
positions, say at position i and i + t, then the output symbols are located at
ki + u, k(i + t) + u −→τ = kt positions apart.
(9.6)
For any given τ there exists a unique value of t, and these interleaves can generate large
values of dmin, but fail to also generate low multiplicities. The latter, which we call a thin
spectrum, is paramount for the performance of the codes.
Now, if the input sequence is
u(D) =

(1 + Dt) + (1 + Dt)Dτ
Dq,
(9.7)
the corresponding output sequence is
u′(D) =

(1 + Dτ) + (1 + Dτ)Dt′
Dq′,
(9.8)
However, since
(1 + Dt) + (1 + Dt)Dτ = (1 + Dτ) + (1 + Dτ)Dt,
(9.9)
choosing τ = zmin we have a situation where the interleaver fails to break up pairs of zmin
sequences, which reappear after interleaving, causing low-weight parity sequences in both
encoders. Thus, block interleavers may achieve good values of dfree, but fail to generate
that “thin” spectrum.
The next step towards improving performance of turbo codes in the error ﬂoor region
was achieved by introducing S-random interleavers introduced in [8], which explicitly avoid

9.1. INTERLEAVERS
433
mapping close indices at the input to close indices at the output. Generation of spread
interleavers as deﬁned on Page 370 is straightforward. They are generated analogously to
random interleavers, selecting each index dI(i) randomly and then testing if |dI(i)−dI(l)| ≥
T for all i −S < l < i, i.e., by checking all the previously chosen positions. If an index
choice fails to fulﬁll this condition anywhere, it is discarded and new index is chosen. The
process continues until all N indices have been chosen. Evidently, the search time for
an appropriate interleaver increases with S and T, and the algorithm may not complete
successfully if these parameters are chose too aggressively.
However, according to [8],
values of S, T <

N/2 usually complete within reasonable time.
Spread interleavers
successfully map short weight 2 input sequences into long weight 2 input sequences and
thus avoid the problem of a low free distance as discussed earlier.
In an eﬀort to construct deterministic interleavers which have the statistical properties
of random interleavers, Takeshita and Costello [28] designed what they called quadratic
interleavers, which are deﬁned by the quadratic indexing function
dIN (i) = ki(i + 1)
2
mod N,
(9.10)
where k is an odd constant. Statistical comparisons with “ideal” uniform interleavers were
conducted for weight 1 and weight 2 sequences. Weight 1 sequences are indeed uniformly
shifted into all N possible positions as expected, while the statistics of weight 2 input se-
quences do not precisely match those expected by an ideally random interleaver. Nonethe-
less, quadratic interleavers perform well and close to random interleavers, including in the
error ﬂoor region—and therefore present no improvement over other, pseudo-randomly
chosen permutations, for example those based on maximum-length shift register designs,
so-called m-sequences, which have good random statistical properties [10]. Their main
advantage of these methods is the rapid algorithmic construction, which may have to be
performed at the receiver on the ﬂy, as code parameters change in adaptive communication
systems. They also avoid large look-up tables otherwise necessary.
Figure 9.1 shows diﬀerent interleavers for N = 512 as 2-dimensional scatter plots, i.e.,
the horizontal axis is the original index i and the vertical axis is the permuted index dI(i).
The scatter plots give a simple visual representation of a given interleaver. The left plot in
Figure 9.1 is that of a linear block interleaver, the middle plot is that of a quadratic, and
the right plot shows an S-random interleaver. A truly random interleaver has a scatter
plot which is visually indistinguishable from that of a quadratic interleaver. We see the
basic eﬀects of the interleavers in yet another way, as it becomes evident that the linear
interleaver gives a good spread, and hence a large dfree, but its regular pattern allows for
large multiplicities, while the quadratic interleaver oﬀers random-like behavior, but has
poor spreading, leading its turbo code to suﬀer from a high error ﬂoor. The S-random
interleaver, in contrast, combines a good spreading property with the random-like quality
of the index points which avoids the accumulation of large multiplicities.

434
CHAPTER 9. TURBO CODING: APPLICATIONS
Linear Interleaver
Quadratic Interleaver
S-Random Interleaver
Figure 9.1: Two-dimensional scatter plots of a linear, a quadratic, and an S-random
interleaver for a block size of N = 512.
As an example of special designed interleavers, we discuss the Dithered Relatively Prime
(DRP) interleavers invented by Crozier and Guinard [7], which represent a popular and
eﬀective interleaving method combining simpler component interleavers. There are
• An input interleaver Ia for a ﬁxed small blocks of size R. It is given as a predeﬁned
permutation of R elements.
• An output interleaver which is also ﬁxed and of size W (typically W = R).
• A central interleaver which is a simple linear interleaver over the entire block.
This interleaver, which acts a as a single permutation, is generated by two local dithering
operations and a global permutation operation. This allows the interleaver to be rep-
resented as a collection of short vectors, while the central interleaver can be calculated
recursively, allowing the overall permutation to be stored and implemented using less
memory than otherwise needed.
The arrangement of these interleaves and the combined scrambling eﬀects are shown
in Figure 9.2. The notation r([i]R) and w([i]W ) denote the integer part of i when divided
by R and W, respectively. M is the least common multiple (l.c.m) of R and W, and
represents the number of pairs of indices that need to be stored to describe the two dither
component interleaves. M is therefore a measure of their size, and as can be seen in Figure
9.3, even small values of M are suﬃcient to have a noticeable impact on the error ﬂoor
onset. M = 1 has a performance similar to a random interleaver at this code size.
The authors [7] used a their own method of generating high-spread in the inter-
leavers [4]. These examples illustrate that with suﬃcient design eﬀorts, the dfree problem

9.1. INTERLEAVERS
435
R
R
N=mR
read
dither
RP
Inter.
write
dither
N=nW
s
s + p
s + 2p
W
W
W
r
Ia = R
 i
R

+ r([i]R)
IRP = s + ip mod N
Ic = W
 i
W

+ w([i]W )
I(i) = Ic(IRP(Ia(i)))
Figure 9.2: Dithered-prime interleaved developed by Crozier and Guinand.
or random turbo codes can largely be controlled and low-weight codewords are eliminated
with appropriate hand-tailored designs.
While cycle-reduction designs like those discussed in connection with the interleavers
for LDPC codes in Section 6.5.2 are also relevant for turbo coding, the interleavers here
are more based on the two principles of randomness and spreading. The inﬂuence that the
designs have are then evaluated by ﬁnding the code’s free distance, through the methods
of short component sequences are discussed in Chapter 8.
Table 9.1 shows the impressive free distances achievable with DRP interleavers, with
values of dfree exceeding 50. The error ﬂoor behavior of these codes hence is vastly superior
to that of the original standard turbo codes. Such procedures de facto eliminate the error
ﬂoor in turbo codes.
While pseudo codewords—that is, patterns that satisfy the component code constraints
without being codewords—can exist in these codes, their impact on the error performance
appears to be negligible. This is likely due to the fact that the component codes enforce
locally consistent code sequences, thus allowing less room for non-codeword error patterns
to be practically relevant. We recall that with LDPC codes, the exact opposite was the
case in the error ﬂoor, which was dominated by non-codewords.

436
CHAPTER 9. TURBO CODING: APPLICATIONS
0
0.5
1
1.5
2
2.5
3
3.5
4
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
DRP with M = 1
random interleaver
DRP with M = 2
3GPP interleaver (see Section 9.2.2)
Frame Error Probability (FER)
DRP with M = 4
DRP with M = 8
high-spread random interleaver [4]
sphere-packing bound R = 1/3
Eb/N0 [dB]
Figure 9.3: Performance of short length-256 turbo codes with diﬀerent level of DRP
interleaves. M = R = W was chosen for simplicity.
N
M = W = R
dfree
512
1
28
512
2
36
512
4
38
512
8
44
1024
8
50
2048
8
54
4096
16
54
8192
16
58
512
2048
4096
6144
0
0.5
1
1.52
2.5
3
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Bit Error Rate
Eb
N0 [dB]
Figure 9.4: Achievable free distances of mid-range rate R = 1/3 turbo codes with DRP
interleaver designs. Source [7]. Right: Bit error performance for the code sizes indicated.

9.1. INTERLEAVERS
437
Finally, we wish to discuss a variant for interleaving, the convolutional interleaver.
Convolutional interleaving can present an advantage over block interleaving for streaming-
based applications, where the size of the system is not necessarily ﬁxed or predetermined.
Such a system will be discussed in Section 10.2 on convolutional LDPC codes. Apart
from the delay associated with the initial zero-padding, convolutional interleavers can also
signiﬁcantly reduce the delays in the overall system, as well as reduce memory requirements
by about one-half [23].
As shown in Figure 9.5, a convolutional interleaver is designed by diagonally splitting
the B × LN rectangular array of a block interleaver into two triangular structures [12].
The lower triangular part is inserted between the channel encoder and the modulator. The
upper triangular is the convolutional de-interleaver and is inserted between the channel
and demodulator.
.
.
.
.
.
.
.
.
.
.
.
.
B
(B −1)Lm
Interleaver
De-interleaver
Interleaver Part
De-interleaver Part
Block
Interleaver
Block
Interleaver
Lm
2Lm
(B −1)Lm
Lm
2Lm
(B −1)Lm
Clocked every B bits
Block interleaver is split
down into a transmitter
and a receiver side.
Figure 9.5: (B, LN) convolutional interleaver with step size LM.
This structure is referred to as a (B, LN) convolutional interleaver. B is chosen to be
larger than the average length of the burst errors expected in the channel, or some other
relevant parameter. It is known as the interleaver depth, and represents the separation
between two consecutive symbols, as they appear over the channel. LN is the maximum
delay inserted, where LN = (B −1)LM, and LM is the step size of the convolutional

438
CHAPTER 9. TURBO CODING: APPLICATIONS
interleaver. At the transmitter, the coded sequence is fed into a B × LN triangular array
of shift registers. The ith (1 ≤i ≤B) shift register has a length of (i −1)LM stages.
The registers are clocked once every B symbols and the oldest symbols in the registers
are shifted out to the channel. Both the total interleaving delay and required memory are
B(B −1)LM, as can be seen from the block representation in Figure 9.5. This is less than
half of that of an equivalent (B, BLM) block interleaver.
At the receiver, the de-inteleaver applies an inverse operation through a structure of
shift-registers, as shown in Figure 9.5. In the application of interleavers to turbo-like codes,
where randomization is important, the convolutional interleaver is typically preceded by
a random block interleaver of size B, which randomizes the inputs via an input buﬀer and
interleaver structure as shown in Figure 9.5. The eﬀects of adding this random ingress
and egress interleaves is visible in the scatter diagram of two convolutional interleaver
approaches shown in Figure 9.6, where the parameters are B = 25 and LM = 1. The
regular convolutional interleaver’s pattern is very regular and thus ill-suited for iterative
coding systems. The interleaver to the right uses 50 diﬀerent, size-B block interleavers
which are prepended to the convolutional interleaver and are cyclically applied. This makes
the overall interleaver resemble a random interleaver while maintaining the convolutional
delay constraint. The delay of the regular interleavers equals B(B −1) = 600, while that
of the randomized interleaver is B(B + 1).
0
500
1000
1500
0
200
400
600
800
1000
0
200
400
600
800
1000
0
500
1000
1500
Regular
Random
Figure 9.6: Scatter diagram of convolutional interleavers with B = 25 and LM = 1.

9.2. TURBO CODES IN TELECOMMUNICATION STANDARDS
439
9.2
Turbo Codes in Telecommunication Standards
Given their stunning peformance, it is not surprising that turbo codes have been rapidly
adopted for application in many commercial systems. This has led to a number of stan-
dards which incorporate turbo codes, ldpc codes, or other versions of graph-based, iter-
atively decoded error control codes. We use the subsequent sections to highlight speciﬁc
coding aspects with the help of a few prominent new standards that include these codes.
The goal here is not to be exhaustive in our discussion, but to bring a broad ﬂavor of how
these new codes have been adapted to a variety of speciﬁc communications situations.
9.2.1
The Space Data System Standard
Space, especially deep-space communications, has always been a test ground for new com-
munications methodologies, and has played that role for error control coding systems
for well over half a century now. The Consultative Committee for Space Data Systems
(CCSDS), which is composed of space agencies and industrial associates worldwide, was
among the ﬁrst to develop a channel coding standard based on parallel concatenated turbo
codes. This new standard was to provide an alternative to the traditional concatenated
coding system which used a 64-state convolutional code with an outer (255,223) Reed–
Solomon code, the traditional concatenated coding workhorse space communications. The
new standard based on turbo codes can provide an additional coding gain of 2.5 dB at
Pb = 10−5 if the lowest rate of R = 1/6 is used [24]. The CCSDS Telemetry Channel
Coding standard [5] uses a turbo code with two component codes with selectable rates
and block-lengths.
The encoder for this code is shown in Figure 9.7. Two rate R = 1/4 recursive convo-
lutional encoders are used to generate R = 1/2, 1/3, 1/4, and R = 1/6 turbo codes. The
two switching matrices combine these rates, where a solid circle means every symbol is
taken, and an open circle means every other symbol is taken. The feedback polynomial
h0 = D4 +D +1 = 23 of both encoders is a primitive polynomial of degree 4 and the feed-
forward polynomials are h1 = 33, h2 = 25, and h3 = 37. The various rates are achieved
by using the connections shown on the right side of Figure 9.7. For example, to achieve
R = 1/4 the four outputs are the systematic bit, the second and third parity bits from
encoder 1 and the ﬁrst parity bit from encoder 2.
Trellis termination is accomplished by ﬁrst terminating code number 1, then code
number 2, by equating the input bit to the feedback bit for four symbol clock ticks. This
causes the encoders to be loaded with the all-zero state.
The interleaver is a block permutation interleaver that has good spreading factors and,
unlike the S-random interleavers, has an algorithmic implementation and is scalable. The
interleavers are fashioned according to Berrou’s analytical algorithm [6].
The CCSDS
standard allows ﬁve interleaver lengths N1 = 1,784, N2 = 3,568, N3 = 7,136, N4 = 8,920,

440
CHAPTER 9. TURBO CODING: APPLICATIONS
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
π
Encoder number 1
Encoder number 2
Outputs
Figure 9.7: The CCSDS turbo encoder. A solid circle means every symbol is taken, and
an open circle means every other symbol is taken.
and N5 = 16,384. The ﬁrst four block lengths are chosen to be compatible with an outer
Reed–Solomon code with interleaving depths of 1, 2, 4, and 5, respectively. The largest
block length is allowed for those users requiring the most power eﬃciency and able to
tolerate a larger decoder latency. The BER and frame error rate (FER) performance of
the CCSDS turbo code with N = 8,920 and various rates is shown in Figure 9.8.
9.2.2
3G Wireless Standards
The third-generation (3G) mobile telecommunication standard [19] was developed by the
International Telecommunications Union (ITU). The system is more formally known as
the International Mobile Telecommunications 2000 (IMT-2000) system, targeting wireless
wide-band packet-switched data services of speeds up to 2 Mbit/s. There were two main
bodies undertaking work for the IMT-2000 standardization: The Third-Generation Part-
nership Project (3GPP) and 3GPP2. 3GPP addressed a wide-band CDMA (W-CDMA)
air interface, and 3GPP2 addressed the cdma2000 air interface, which evolved from the

9.2. TURBO CODES IN TELECOMMUNICATION STANDARDS
441
-0.5
0
0.5
1
1.5
10−7
10−6
10−5
10−4
10−3
10−2
10−1
R = 1/2
R = 1/3
R = 1/6
R = 1/4
Frame/Bit Error Probability (BER)
Eb/N0[dB]
Figure 9.8: Frame error (solid curves) and bit error rate (dashed curves) performance of
the CCSDS turbo code family with N = 8,920.
IS-95 interim standard. Both specify the use of turbo codes. Besides a constraint-length
9, convolutional code 3GPP also speciﬁes the rate R = 1/3 turbo code shown in Figure
9.9 using 8-state constituent component codes.
+
+
+
+
+
+
+
+
π
v
r
vr
ur
Figure 9.9: The 3GPP turbo encoder uses 8-state component codes.

442
CHAPTER 9. TURBO CODING: APPLICATIONS
The cdma2000 standard speciﬁes 3 types of convolutional codes with rates R =
1/4, 1/3, and 1/2, all with constraint-length 9.
The convolutional codes are used for
signaling channels. The turbo codes use code rates of R = 1/2, 1/3, 1/4, or R = 1/5, and
employ 8-state constituent component codes. The encoder is shown in Figure 9.10 and
reveals its similarity to the 3GPP encoder. Diﬀerent rates are achieved by puncturing.
+
+
+
+
+
+
+
+
+
+
+
+
+
+
π
v(0)
r
v(1)
r
v(2)
r
v(3)
r
ur
Figure 9.10: The cdma2000 turbo encoder also uses 8-state component codes.
For multimedia communications a wide range of possible block lengths are required,
and the 3GPP standard speciﬁes interleavers for block sizes ranging from 40 – 5,114. Due
to this, interleavers with an eﬃcient algorithmic generation method are preferred. Prime
interleavers were proposed [25] and later adopted by 3GPP [30]. The interleavers adopted
by 3GPP2 have similar structures as those for 3GPP [31].
In general, all these interleavers are based on a block interleaver, called the mother
interleaver. The information bits are written into a size m×n array, after which intra-row
and inter-row permutations are performed. The ﬁnal interleaved bits are read out column
by column. The diﬀerent interleavers diﬀer in the way these permutations are performed.
Two approaches are used to accomplish the intra-row permutations. In the so-called prime
interleavers [25, 63], the rows are basically circularly shifted by prime numbers distinct to
each row. In the interleavers for 3GPP2 the circular shifting is accomplished with a group
of angular coeﬃcients. The net eﬀect of both strategies is to make the interleaver less
regular w.r.t. to the original mother interleaver in order to avoid the high multiplicities of
the former. The interleavers are also easy to construct algorithmically, which is a primary
requirement for rapid block length changes. The visual scatter plots of examples of these
two interleaver types shown in Figure 9.11 give an impression of their randomness.

9.2. TURBO CODES IN TELECOMMUNICATION STANDARDS
443
Linear Interleaver
3GPP2 Interleaver
Prime Interleaver
Figure 9.11: Two-dimensional scatter plots of a linear and the two 3GPP interleavers for
a block size of N = 512.
9.2.3
Digital Video Broadcast Standards
The digital video broadcasting project (DVB) was founded in 1993 to create a framework
digital video services [32]. The DVB return channel via satellite RCS prescribes two coding
schemes for the traﬃc and control channels: (i) a concatenated coding method with a by-
passable outer Reed–Solomon (RS) code with an also by-passable inner non-systematic
convolutional code, and (ii) a duo-binary turbo coding method described here. During
a session the system is to stay with one of the coding methods. Both codes have been
standardized for the return channel of digital broadcasting by the DVB commitee and the
European Telecommunications Standards Institute (ETSI), aiming at providing two-way,
asymmetric broadband internet access at speeds between 144 kbits/s and 2 Mbits/s to
complement ADSL and cable modems [32]. The code used is a variation of the standard
turbo code. It uses two input bits (duo-binary) and only a single encoder that is used
alternately to generate the direct parity stream and the interleaved parity stream. Its block
diagram is shown in Figure 9.12. The input bits are alternately taken from the direct input
stream and the interleaved input stream and fed into the recursive duo-binary encoder
which produces the parity bits. The rates supported are R = 1/2, 2/5, 1/2, 2/3, 3/4, 4/5,
and R = 6/7 through various puncturing schemes. Bits are interleaved in pairs, and there
are N pairs with N ∈{48, 64, 212, 220, 228, 424, 432, 440, 752, 848, 856, 864} to provide for
short frames.
The way the duo-binary encoder works is that it ﬁrst encodes the direct information
sequence and then, in a second pass, the interleaved sequence. This is done by reusing the
same encoder. A small complication is introduced in that the codes are tail-biting, and
therefore the ﬁnal state must be found in a ﬁrst encoding pass, which then determines the

444
CHAPTER 9. TURBO CODING: APPLICATIONS
starting state for the actual encoding pass that produces the parity sequence. Decoding
is performed in the standard fashion discussed in Chapter 8.
+
+
+
+
+
π
v(1)
r
v(2)
r
u(2)
r
u(1)
r
Figure 9.12: Self-concatenated turbo encoder used in the digital video broadcast standard.
In the early 2000’s it became clear that advances in satellite technology would al-
low increased eﬃciency, and new channel coding schemes, combined with higher-order
modulation, were developed [33]. They promise a capacity gain on the order of 30% at
a given transponder bandwidth. The new coding standards are realized by a concate-
nated system with an inner BCH and an out Repeat-Accumulate (LDPC) code at rates
1/4, 1/3, 2/5, 1/2, 3/5, 2/3, 3/4, 4/5, 5/6, 8/9, and 9/10. The FEC coded block can have a
length of either n = 64,800 or n = 16,200 bits.
It’s informative to look at the error control structure of this standard as shown in
Figure 9.13. This format is very common to many implementations. The outer BCH code
is primarily intended to control the error ﬂoor, which, according to latest knowledge, is
not strictly necessary.
Bit
Mapping to
constellation
Scrambling
Pilot
Insertion
Baseband
Filtering
QAM
Inner
Encoder
Bit
Interleaver
Outer
Encoder
QPSK, 8PSK,
16APSK, 32APSK
Rates: 1/4, 1/3, 2/5, 1/2, 3/5, 
2/3, 3/4, 4/5, 5/6, 8/9, 9/10
Root-Nyquist:
_ = 0.35,0.25,0.2
Baseband
Frame
FEC Frame
PHY Layer Frame
(see Chapter 2)
(see Chapter 2)
Figure 9.13: Error control coding system for the DVB-S2 standard.

9.2. TURBO CODES IN TELECOMMUNICATION STANDARDS
445
The error control code used in the DVB-S2 standard is a repeat-accumulate code
(RA)—see Chapter 6.
This code provides an easy encoding mechanism and excellent
performance, especially in the error ﬂoor region after optimization of the interleaver. As
discussed in Chapter 8, RA codes also form a natural link between turbo and LDPC codes
and can be regarded as either. Figure 9.14 therefore shows the performance of the rate
R = 1/2 DVB-Sa code of length n = 64,800 [33] in comparison with the original turbo
code, which has approximately twice its length, and therefore fares a bit better in the
waterfall region.
The standard also provides for a reduced code length of n = 64,800 [33] to improve
latency. These two base codes are combined into a wide range of code rates (from 1/4
up to 9/10); 4 constellation sizes with spectral eﬃciencies ranging from 2 bit/s/Hz to
5 bit/s/Hz. The transmission spectrum is shaped with Nyquist signals with roll-oﬀfac-
tors 0.35, 0.25, and 0.20.
Adaptive coding and modulation (ACM) optimizing coding
modulation parameters is carried out on a frame-by-frame basis.
Berrou Turbo Code
DVB-S2 RA code
0
0.15
0.3
0.45
0.6
0.75
0.9
1.05
1.2
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
Bit Error Probability (BER)
Eb/N0 [dB]
Figure 9.14: Comparison of the DVB-S2 RA code of length 64,800 and the original Berrou
turbo code of length 2 × 65,536.

446
CHAPTER 9. TURBO CODING: APPLICATIONS
9.3
Product Codes and Block Turbo Decoding
In a few applications, such as an option in the IEEE standard 802.16 for broadband wireless
access, commercialized under the name “WiMAX,” yet another variation of graph-based
codes with iterative decoding is used. These codes are the product codes introduced by
Elias in 1954 [9], without, however, presenting an iterative or practical decoding operation.
Product codes also represent a variant of the general turbo coding principle where two
relatively weak component codes encode a block of identical, but permuted, information
symbols. The diﬀerence here is that the interleaver is regular, in fact it is a row/column
block interleaver. We have seen that for component convolutional codes such interleavers
are not very eﬃcient, however, for block component codes, this restriction does not pose
the quite the same problem. Nonetheless, as we will see, the performance of product codes
is cleary inferior to that of parallel of serial concatenated turbo codes and LDPC codes.
One plausible reason for their limited use is that they are less encumbered by patent claims
and that early implementations of decoders were readily available before the widespread
use of concatenated turbo decoders. However, we speculate that their popularity is waning
and they won’t be likely be used in future applications.
The structure of a product code is shown in Figure 9.15 and comprises a rate R2 =
k2/n2 row and a rate R1 = k1/n1 column code, often taken to be the same code. The
overall transmission rate of the product code is R = R1R2, but often the parity checks on
parity checks (dashed box in Figure 9.15) are not transmitted. This increases the rate and
produces more favorable values of Eb/N0; however, it also reduces the minimum distance
and thus the error ﬂare performance of the code.
The problem of encountering a small value of dfree does not typically occur with these
codes, since it is quite evident that the minimum distance of the code, including the parity
on parities, is the product of the minimum distances for the component codes, and is given
by dfree = dfree,rowdfree,col. Thus large minimum distance codes are easily constructed, and
product codes do not suﬀer from the problem of an error ﬂoor as do parallel concatenated
turbo codes in the absence of careful interleaver design.
Even though product codes have been well known for a long time (see, e.g., [13],
decoding them to their potential using iterative decoding principles has not been realized
until the works of Hagenauer et al. [11] and Pyndiah [20], who both used proper probability
message passing algorithms. The iterative decoder which achieves the codes full potential
is again derived from the turbo decoding principle, and only minor changes are required.
It is shown (again) in Figure 9.16.
Three types of symbols make up a coded block y = (yk, yp1, yp2), where yk are the
received k1k2 information bits, analogous to the systematic information bits in parallel
concatenation. yp1 is the k1(n2 −k2) block of received parity symbols for the row codes,
and yp2 is the k2(n1 −k1) block of parity symbols for the column codes. Each code in each

9.3. PRODUCT CODES AND BLOCK TURBO DECODING
447
n1
k1
n2
k2
Information
Bits
Column Parities
Row Parities
Figure 9.15: Structure of product codes.
+
+-
-
Block Storage
A Priori LLRs
Block Storage
Extrinsic LLRs
Block Storage
Extrinsic LLRs
Bank of
Row Block
Decoders
Bank of
Col. Block
Decoders
yk
yp1
yp2
yk
{Λ(2)
e (ur)}
{Λ(1)
e (ur)}
Figure 9.16: Block diagram of a block turbo decoder with banks of constituent soft block
component decoders.
column and each row independently decodes the information symbols, using only received
signals from its codewords plus extrinsic a priori information on the information bits. The
decoders output a posteriori extrinsic information, usually in the form of LLRs, which
are stored in memory. Iterations alternate between row decoding and column decoding.

448
CHAPTER 9. TURBO CODING: APPLICATIONS
Extrinsic information is generated only for the information bits. Despite short cycles in
the factor graphs of these codes, excellent performance is achieved. APP decoders for the
component block codes as discussed in Chapter 5 can be used, but those for suboptimal
approximate algorithms seem to be more popular due to their complexity advantage.
9.4
Approximate APP Decoding
Trellis-based APP decoding for block codes may in many cases be too complex due to the
irregular structure of the trellis of these codes. In such cases, approximate methods have
proven to be workable alternatives. On a basic level, any APP decoder needs to compute
the bit log-likelihood ratio
Λr = log
 Pr(br = 1|y)
Pr(br = −1|y)

(9.11)
for each information bit br or coded binary symbols vr. Clearly, an exhaustive way of
doing this is by calculating the sums
Pr(br = 1|y)
=

x∈X (+)
r
Pr(x|y),
(9.12)
Pr(br = −1|y)
=

x∈§(−)
r
Pr(x|y),
(9.13)
where X (+)
r
is the set of codewords x corresponding to messages with br = 1, and X (−)
r
is
the set corresponding to messages with br = −1.
These sums are prohibitively complex since there are too many terms to evaluate,
and suitable approximations need to be found. We begin by considering the likelihood
ratio (9.11) that the decoder is expected to compute, after we substitute the conditional
Gaussian probability density function Pr(y|x) =
1
(N0π)n/2 exp

−|y−x|2
N0

, giving
Λr = log
⎛
⎜
⎜
⎜
⎜
⎜
⎝

x∈X (+)
r
exp

−|y −x|2
N0


x∈X (−)
r
exp

−|y −x|2
N0

⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(9.14)
The problem is that there is no short way of calculating this sum of exponentials.
However, if we restrict the sums to only a single term each, x(+)
r
and x(−)
r
, we can
approximate (9.14) by the much simpler expression
Λr ≈1
N0

|y −x(−)
r
|2 −|y −x(+)
r
|2
.
(9.15)

9.4. APPROXIMATE APP DECODING
449
We now need to ﬁnd the two terms x(+)
r
and x(−)
r
which generate the best approximation.
This is accomplished by choosing x(+)
r
as the codeword at minimum distance to y with
br = 1, and x(−)
r
as the codeword at minimum distance to y with br = −1.
Furthermore, (9.15) can rewritten as
Λr ≈2
N0
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
yr +
n1(n2)

l=1
(l̸=r)
ylx(+)
l
pl



Λe,r:extrinsic
information
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
pl =

0,
x(+)
l
= x(−)
l
,
1,
x(+)
l
= x(−)
l
.
(9.16)
An iterative update equation can now be constructed by equating the term 2yr/N0 above
as the soft channel input value and the sum as the symbols extrinsic information, since
both represent LLR ratios. This would work well if a true APP decoder, which properly
uses a priori information, is used, and the extrinsic update algorithm discussed in Chapter
8 should be used.
However, since the component decoders which are typically used cannot make direct
use of a priori information, the following LLR update equation represents a reasonable
alternative: For subsequent decoding cycles we let
y(i)
r
←y(i−1)
r
+ αi−1Λ(i−1)
e,r
,
y(0)
r
= yr.
(9.17)
The central question is, of course, how to ﬁnd the two codewords x(−)
r
and x(−)
r
. Many
ad hoc techniques have been explored, and among them we ﬁnd the Chase Algorithm
[3]. This algorithm is conceptually simple and exploits the fact that the noise probability
decays exponentially with the magnitude of the noise. Succinctly stated, the algorithm
restricts the search to a sphere of candidate codewords around the received vector y and
draws the two codewords x(−)
r
and x(−)
r
from this reduced set, which is signiﬁcantly less
complex to search than all of Xr. The algorithm begins by identifying the least reliable
bits of a received sequence y and enumerates all corresponding binary vectors while ﬁxing
the remaining more reliable bits. These vectors form a ﬁxed number of candidates and
are decoded into candidate codewords x, from which the two reduced-size sets X (+)
r
and
X (−)
r
are then generated. However, there are a number of diﬃculties associated with this
algorithm:
a) The algorithm is ad hoc and it is diﬃcult to evaluate the performance/complexity
trade-oﬀwithout extensive simulations.
b) There is a chance that no codeword x(−)
r
(or, alternatively, no codeword x(+)
r
) is
found in the reduced set. In this case, Λr = βbr is used an approximate value, where
β is an ad hoc adjustment parameter.

450
CHAPTER 9. TURBO CODING: APPLICATIONS
c) A complex ad hoc scaling system α = [α0, . . . , αI] is necessary to optimize the
performance of the algorithm.
Nonetheless, using the Chase algorithm in this fashion as an approximation of APP
decoding produces surprisingly good performance with a manageable computational eﬀort.
Figure 9.17 shows the simulated performance of a [BCH(64,51,6)]2 code with a coding rate
of R = 0.635. At this rate, the Shannon limit is at Eb/N0 = 0.65 dB, and this limit rises
to Eb/N0 = 1.1 dB if the signal constellation is constrained to BPSK.
The simulation results were taken from [20], and the parameters were chosen as α =
[0, 0.2, 0.3, 0.5, 0.7, 0.9], β = [0.2, 0.4, 0.6, 0.8, 1, 1]. Decoding via Chase algorithm used 16
candidate vectors, changing the 4 least reliable bits.
-1
0
1
2
3
4
5
6
7
8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
BPSK
1 iteration
2 iterations
4 iterations
6 iterations
Shannon Limit 0.87 dB
Bit Error Probability (BER)
Eb/N0[dB]
Figure 9.17: Simulations of a [BCH(64,51,6)]2 product code using iterative decoding with
Chase-algorithm-based approximations to the APP algorithm.
Advanced Hardware Architectures, Inc., Pullman, WA, was an early developer who
built a series of VLSI decoders based on the concept of product codes. Their codes use

9.5. PRODUCT CODES WITH HIGH-ORDER MODULATIONS
451
extended Hamming codes as components and use both 2- and 3-dimensional arrays. The
extension of the decoding algorithm to 3 dimensions is straightforward. Table 9.1 shows
the code combinations and rates in their product line.
All codes used are Hamming
codes. Partly due to the ready availability of VLSI decoders, product codes with iterative
decoding have enjoyed some, however limited, success in standards and applications.
Code Combinations
N
K
R
(16,11) × (16,11)
256
121
0.4727
(32,26) × (16,11)
512
286
0.5586
(16,11) × (16,11) × (4,3)
1024
363
0.3545
(32,26) × (32,26)
1024
676
0.6602
(64,57) × (8,4) × (4,3)
2048
684
0.3340
(32,26) × (16,11) × (4,3)
2048
858
0.4189
(32,26) × (64,57)
2048
1482
0.7236
(32,26) × (16,11) × (8,3)
4096
1144
0.2793
(16,11) × (16,11) × (16,11)
4096
1331
0.3250
(32,26) × (32,26) × (4,3)
4096
2028
0.4951
(64,57) × (64,57)
4096
3249
0.7932
Table 9.1: Table of code combinations oﬀered by AHA corporation.
Figure 9.18 shows simulation results of a [Hamming(64,57,4)]2 code with a code rate of
R = 0.793 for a block size of n = 4096. The Shannon bound at that rate is at Eb/N0 = 1.1
dB, whereas the capacity limit using BPSK is Eb/N0 = 2 dB. The competing system is a
concatenated RS/convolutional code system with an overall rate of R = 0.790.
9.5
Product Codes with High-Order Modulations
We now apply product codes to larger constellations. This is the reason why the entire
treatment of product codes and block turbo decoding has been placed in this chapter
on turbo-coded modulation. The product encoder generates an array of output binary
symbols, and in order to use larger constellations, these binary digits from the FEC encoder
are Gray mapped onto the larger constellations.

452
CHAPTER 9. TURBO CODING: APPLICATIONS
-1
0
1
2
3
4
5
6
7
8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
BPSK
4 iterations
32 iterations
Shannon Limit 1.99dB
Bit Error Probability (BER)
Reed-Solomon with in-
ner convolutional code
and Viterbi Decoding
Rate 0.79
Eb/N0[dB]
Figure 9.18: Simulations of a [EHC(64,57,4)]2 product code using iterative decoding with
AHA’s approximation to the APP algorithm. This code has been standardized for the
IEEE 802.16 MAN standard. The competing coding system is a traditional concatenated
code with an inner convolutional code and an outer RS code.
The only signiﬁcant change w.r.t. binary modulation occurs at the decoder, where the
LLR calculation has to be revisited. Since the information symbol is “buried” in the larger
constellation, such as 8-PSK or 16-QAM, its a priori channel probability is calculated via
the marginalization
Pr(br = b|yr) =

xr∈X
(br=b)
Pr(xr = x|yr),
(9.18)
where br is the binary symbol processed by the BTC decoder, and xr is the modulated
symbol from the larger signal constellation used.
From this the bit channel likelihood ratio which is needed at the decoder input is

9.6. THE IEEE 802.16 STANDARD
453
calculated as
Λr = log
⎛
⎜
⎜
⎜
⎜
⎝

xr∈X
(br=1)
Pr(xr = x|yr)

xr∈X
(br=0)
Pr(xr = x|yr)
⎞
⎟
⎟
⎟
⎟
⎠
.
(9.19)
Note that this marginalization operation has to be performed only once before iterative
decoding begins.
Figure 9.5 shows the performance of such a product-coded modulation system using
a large 64-QAM constellation. The code used is a [Hamming(64,57,4)]2 product code,
modulated onto a 64QAM constellation using Gray mapping. This system achieves a data
rate of R = 4.76 bits/symbol with a block size of N = 683 symbols. The Shannon limit at
this rate is at Eb/N0 = 8.49 dB. The competing system is a concatenated RS-Trellis Code
system with an overall rate of R = 4.69 bits/symbol, also shown in the ﬁgure. As noted
before in the situation with space–time codes, the gap between code performance and
the Shannon bound increases with increased spectral eﬃciency. This is due to the larger
modulation alphabets which represent “weak” codes that cannot be decoded eﬃciently.
9.6
The IEEE 802.16 Standard
IEEE 802.16 is a standard for Wireless Metropolitan Area Networks (MAN), in which
product codes with iterative decoding have been standardized as optional FEC for both
the up-, and the down-link. The code standardized in 802.16 is a two-dimensional code
with one of two Hamming component codes. These are either the [64,57,4], or the [32,26,4]
extended Hamming code. The block information sizes of 3249, and 676, respectively, do
not match all the required packet sizes, and the code is matched to the diﬀerent packets
by shortening. The longer of the codes achieves a performance of only about 1.2 dB from
the Shannon limit with up to 32 iterations, and about 1.5 dB from the Shannon limit at
only 4 iterations (see Figure 9.18).
The codes are shortened in a straightforward way by deleting rows and columns until
the required block size is achieved. Deleting rows or column reduces the number of bit
that are transmitted. At the decoder these bits are then simply assumed to be zero and
are known. Decoding otherwise continues in the iterative fashion discussed in this chapter.
The shortening aﬀects the rate of the code, but has no other major eﬀect; in particular,
the minimum distance and error performance are maintained.
Turbo product codes allow for relatively high data rates. They are mainly intended
for satellite communications, wireless internet access, wireless LAN, microwave systems,

454
CHAPTER 9. TURBO CODING: APPLICATIONS
5
6
7
8
9
10
11
12
13
14
10−7
10−6
10−5
10−4
10−3
10−2
10−1
1
Uncoded 32 QAM
Product Code: 8 iterations
Shannon Limit 8.49 dB
(64QAM)
Bit Error Probability (BER)
Reed-Solomon with
16-state 2D TCM
Rate 4.69
Interleaver depth 32
Interleaver depth 32
Eb/N0[dB]
Figure 9.19: Simulations of a [EHC(64,57,4)]2 product code mapped into 64-QAM con-
stellation symbols to increase spectral eﬃciency. 32 QAM is used as comparison since it
provides roughly the same spectral eﬃciency.
and mobile communications applications. They were also investigated for future wire-line
applications such as for ADSL applications but have not been adopted there.
9.7
Decoding of Polar Codes
We introduced Polar Codes in Section 4.16 as a relative of the Reed–Muller codes, both
based on a Hadamard construction. However, Polar Codes are more general, and have been
shown to be capacity-achieving in the limit of a large block length N [1]. Unlike Reed–
Muller codes, Polar Code decoding is carried out on the code’s graph, which we construct
below. As indicated in Section 4.16, decoding proceeds bit-by-bit and is based on successive
cancelation, which is added to the otherwise familiar message passing mechanisms for
graph-based codes we discussed in this book, primarily those from Chapter 6.
We begin our discussion with the code construction graph from Figure 4.28, which we

9.7. DECODING OF POLAR CODES
455
have extended to a size N = 8 example in Figure 9.20. This code is redrawn into the
graphical diagram of Figure 9.21, where the order of the stages has been reversed, and
we have reverted to the standard notation from Chapter 6 for variable nodes as round
circles and check nodes as square boxes. With a little work it can easily be veriﬁed that
the two graphs represent the same code, but we will ﬁnd Figure 9.21 more amenable to
our discussion of the decoding procedure, and the graph now resembles a Tanner graph
for an LDPC code at least in form.
+
+
+
+
+
+
+
+
+
+
+
+
u1
u2
u3
u4
u5
u6
u7
u8
y1
y2
y3
y4
y5
y6
y7
y8
W
W
W
W
W
W
W
W
Figure 9.20: Size N = 2n = 8 example polar code.
In principle we can now apply a message passing algorithm on the graph of Figure 9.21
with any arbitrary schedule to ﬁnd the likelihood values of u1, . . . , u8 after injecting the
channel log-likelihood ratios (LLRs) λ1, . . . , λ8. However, as can readily be appreciated,
the graph in Figure 9.21 has cycles, and regular message passing as discussed in Chapter 6
may not lead to optimal performance.
In his seminal paper, Arikan [1] introduced an algorithm based on successive cancela-
tion, which we will discuss now. This algorithm, in essence, forms the basis for all variants
of Polar Code decoders. It borrows the message passing methodology from LDPC decod-
ing, with the addition of a cancelation step, which prevents the decoder from having to go
through multiple iterations. However, since the code graph has O(N log N) edges, even
this two-step cancelation algorithm has an order complexity of O(N log N), or O(log N)
per bit, which grows with N, unlike the order decoding complexity of LDPC decoders.
We begin our discussion by noting that the entire code structure in Figure 9.21 consists

456
CHAPTER 9. TURBO CODING: APPLICATIONS
+
+
+
+
+
+
+
+
+
+
+
+
u1
u2
u3
u4
u5
u6
u7
u8
λ1
λ2
λ3
λ4
λ5
λ6
λ7
λ8
Figure 9.21: Reverse ﬂow diagram of the size N = 8 polar code of Figure 9.20.
+
si,j
si+1,j
λi+1,j
λi,j
λi,j+1
λi+1,j+1
Stage j
Stage j + 1
Stage j −1
Figure 9.22: “Z”-butterﬂy which makes up the core structure of the decoding graph in
Figure 9.21. The ﬁgure shows a butterﬂy at an arbitrary stage j.
of concatenated and interlocking “Z”-butterﬂies. As shown in Figure 9.22, such a structure
has two binary inputs si,j and si+1,j, and two output bits connected with a single parity
check node. The messages through this butterﬂy are computed as follows: From the right
we receive LLR bit values from an up-stream stage j + 1, which are used to compute the
LLR values of the down-stream bits. Using the notation in Figure 9.22, we compute
λi,j
=
2 tanh−1

tanh
λi,j+1
2

tanh
λi+1,j+1
2

,
(9.20)
λi+1,j
=
(1 −2ˆsi,j)λi,j+1 + λi+1,j+1
(9.21)

9.7. DECODING OF POLAR CODES
457
Equation (9.20) is the basic check node update which computes the LLR of node
si,j, while (9.21) is the cancelation step, which depends on the knowledge of si,j, or its
decision ˆsi,j. Depending on its value, the LLR λi,j+1 is either added to, or subtracted
from, λi+1,j+1. It is at this point where the frozen bits of the polar code come into play.
Decoding now progresses from left to right with LLR propagation, and from right to
left with hard bit decision propagation. As soon as both input LLRs of a given butterﬂy
are available, the top output LLR of the previous stage can be computed (see Figure 9.22)
and as soon as the top left bit is known, the bottom left LLR can also be computed.
Butterﬂies can therefore be activated only when the required inputs are available, and the
algorithm is intrinsically sequential and potentially slow. However, substantial speed-up
can be obtained by activating non-interacting butterﬂies simultaneously. This is illustrated
in Figure 9.23 with our example code.
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Stage 1
Stage 1
Stage 2
Stage 2
Stage 3
Stage 3
Stage 1
Stage 1
Stage 2
Stage 2
Stage 3
Stage 3
Stage 1
Stage 1
Stage 2
Stage 2
Stage 3
Stage 3
4
5
6
1
2
3
Hard-Decision Updates −→
←−LLR Propgation
u1
u1
u2
u1
u2
u3
u1
u2
u3
u4
u1
u2
u3
u4
u1
u2
u3
u4
u5
λ1
λ2
λ3
λ4
λ5
λ6
λ7
λ8
Figure 9.23: Butterﬂy activation schedule for the example code of size N = 8. Solid lines
mean activation at that decoding step, grey circles mean input LLR computed, and black
circles are decided bits.
In a ﬁrst phase, 7 butterﬂies can be activated, 4 in Stage 3, 2 in Stage 2, and 1 in
Stage 1.
These are marked by solid lines in Step 1, Figure 9.23.
We have indicated
the availability of LLR information at an intermediate node by shading it in grey. The
butterﬂies have to be activated sequentially through the stages. At the end of the ﬁrst

458
CHAPTER 9. TURBO CODING: APPLICATIONS
phase, bit u1 has its ﬁnal LLR and can be decoded; however, it had better be a frozen
bit, since its LLR will be very unreliable after seven parity-check gates have “diluted” the
LLR. In fact, this is a bad channel, and we may assume u1 known and darken the circle
to indicate this in the ﬁgure.
The value of u1 can now be back-propagated to Stage 2, and the LLR from Stage 2
can be propagated to bit u2, which now also has all its LLR information. A decision on
u2 can therefore be made. It will also likely be a frozen bit and is already known. The
next forward propagation is now enabled and shown in Step 3.
At this stage the decoding pattern becomes evident. As soon as an information bit
ui has all the required information, a decision can be made, and the bit becomes known,
or frozen. Snapshots 1, 3, and 5 in Figure 9.23 illustrate the forward propagation of the
likelihood values, while snapshots 2, 4, and 6 show the back propagation of known (frozen)
bits. If a bit is already frozen by the code constraint, we technically don’t need to wait
for the LLRs but can back-propagate its value directly. This is the basis for some of the
accelerated decoding methods in the literature [15, 17, 18, 29, 21].
It is evident that this basic cancelation algorithm entails signiﬁcant computational
redundancy, which can be exploited by more careful designs. Yazdi and Kschischang [29]
introduced such a simpliﬁed decoder by putting the butterﬂies into 3 classes: zero-rate,
where both bits are known since frozen; half-rate, where one of the bits is frozen, and
full-rate, where both bits need to be computed. They report an acceleration of up to
twenty-fold, depending on code rate and size.
A fairly rich literature has sprung up regarding decoding methods, as well as imple-
mentation designs for polar codes [2, 22, 14]. In general, they are all based on variants of
the successive cancelation methodology, even though more general message passing meth-
ods using LDPC-type algorithms have been studied. However, given the nested butterﬂy
structure of the codes, general message passing is not only complex, but its performance
will strongly depend on the update schedule that is used.
9.8
Polar Code Performance and Outlook
Comparisons between Polar Code implementations and the performance of the more es-
tablished turbo and LDPC codes abound, but exact comparisons are diﬃcult due to the
many parameters that can be chosen. It is however fair to say that Polar Codes present
an interesting alternative code family with excellent performance, which is not based on
sparse graphs.
The ﬁgure below, adapted from [16], presents a performance comparison favoring polar
codes. Taken with a grain of salt, we can conclude that comparable levels of performance
can be achieved with all three types of coding, at least for half-rate codes. The codes
reported are the following: Polar a) is the standard successive cancelation algorithm and

9.8. POLAR CODE PERFORMANCE AND OUTLOOK
459
shows the poorest performance. Polar b) uses an enhanced SC decoder which allows up
to L candidate path to be explored in parallel at each level [27].
Polar c) is a CRC-
enhanced polar code, which uses a 24-bit CRC code in conjunction with the decoding
algorithm [17], and Polar d) uses a CRC enabled decoder with an adaptive list size that
progressively grows [29]. These two latter coding systems are examples of concatenated
codes which are sometimes used to improve the performance of shorter codes, like the size
N = 1024 polar codes discussed here. Strictly speaking, these codes cannot be directly
compared to the turbo codes or the other decoding algorithms, however, the reader get
the general performance picture: With enough eﬀort and engineering designs, additional
performance gains are typically possible with short and medium-sized codes. All polar
codes discussed here where constructed using a method presented by Tal and Vardy [26];
see [16].
Polar d)
Polar a)
Polar b)
Polar c)
W-CDMA
LTE turbo
0
0.5
1
1.5
2
2.5
3
3.5
10−6
10−5
10−4
10−3
10−2
10−1
1
Frame Error Probability (FER)
Eb/N0[dB]
Figure 9.24: Frame error probability of rate R = 0.5 and block length N = 1024 polar
codes with various decoding algorithms, compared to standardized turbo codes of equiv-
alent rate and size.

460
CHAPTER 9. TURBO CODING: APPLICATIONS

Bibliography
[1] E. Arikan, “Channel polarization:
A method for constructing capacity-achieving
codes for symmetric binary-input memoryless channels,” IEEE Trans. Inform. The-
ory, vol. 55, no. 7, July 2009, pp. 3051–3073.
[2] A. Balatsoukas-Stimming, A.J. Raymond, W.J. Gross, and A. Burg, “Hardware archi-
tecture for list successive cancellation decoding of polar codes,” IEEE Trans. Circuits
Systems II, vol. 6‘, no. 8, pp. 609–613, August 2014.
[3] D. Chase, “A class of algorithms for decoding block codes with channel measurement
information,” IEEE Trans. Info. Theory, pp. 170–182, Jan. 1972.
[4] S. Crozier, “New high-spread high-distance interleavers for turbo codes,” 20th Bi-
ennial Symposium on Communications, Kingston, Ontario, Canada, pp. 3–7, May
28–31, 2000.
[5] Telemetry Channel Coding, Consultative Committee on Space Data Systems, CCSDS
101.0-B-5, June 2001.
[6] Recommendations for Space Data System Standards, Consultative Committee on
Space Data Systems, CCSDS 101.0-B-6, Oct. 2002.
[7] S. Crozier and P. Guinand, “High-performance low-memory interleaver banks for
turbo-codes,” US Patent, No. 6,857,087, Feb. 15, 2005.
[8] S. Dolinar and D. Divsalar, “Weight distributions for turbo codes using random and
nonrandom permutations,” JPL TDA Prog. Rep. 42-122, pp. 56–65, Aug. 1995.
[9] P. Elias, “Error-free Coding,” IEEE Trans. Inform. Theory, vol. IT-4, pp. 29–37,
Sept. 1954.
[10] S.W Golomb, Shift Register Sequences, Holden-Day, San Francisco, 1967.
[11] J. Hagenauer, E. Oﬀer, and L. Papke, “Iterative decoding of binary block and convo-
lutional codes,” IEEE Trans. Inform. Theory, vol. 42, no. 2, pp. 429–445, Mar. 1996.
[12] S. A. Hanna, “Convolutional interleaving for digital radio communications,” IEEE
2nd International Conference on Universal Personal Communications, vol. 1, pp. 443–
447, Oct. 1993.
461

462
BIBLIOGRAPHY
[13] S. Lin and D.J. Costello, Jr., Error Control Coding: Fundamentals and Applications,
2nd edition, Prentice Hall, Upper Saddle River, NJ, 2004.
[14] C. Leroux, A.J. Raymond, G. Sarkis, I. Tal, A. Vardy, and W.J. Gross, “Hard-
ware implementation of successive-cancellation decoders for polar codes, J. Signal
Proc. Systems, vol. 69, no. 3, pp. 305–315, Dec. 2012.
[15] C. Leroux, A.J. Raymond, G. Sarkis, and W.J. Gross, “A semi-parallel successive-
cancellation decoder for polar codes,” IEEE Trans. Signal Proc., vol. 61, no. 2,
pp. 289–299, Jan. 2013.
[16] K. Niu, K. Chen, J. Lin, and Q.T. Zhang, “Polar codes: Primary concepts and
practical decoding algorithms,” IEEE Commun. Mag., pp. 192–203, Jul. 2014.
[17] K. Niu and K. Chen, “CRC-aided decoding of polar codes,” IEEE Commun. Lett.,
vol. 16, no. 10, pp. 1668–1671, Oct. 2012.
[18] K. Niu and K. Chen, “Stack decoding of polar codes,” Electron. Lett., vol. 48, no. 12,
pp. 695–697, Jun. 2012.
[19] R. Prasad, W. Mohr, and W. Konhauser, Third Generation Mobile Communication
System, Artech House, Norwood, MA, 2000.
[20] R.M. Pyndiah, “Near-optimum decoding of product codes: block turbo codes,” IEEE
Trans. Commun., vol. 46, no. 8, pp. 1003–1010, Aug. 1998.
[21] A.J. Raymond and W.J. Gross, “A scalable successive-cancellation decoder for polar
codes,“ IEEE Trans. Signal Proc., vol. 62, No. 20, pp. 5339–5347, 2014.
[22] G. Sarkis, P. Giard, A. Vardy, C. Thibeault, and W.J. Gross, “Fast polar decoders:
Algorithm and implementation,” IEEE J. Select. Areas Commun., vol. 32, no. 5,
pp. 946–957, May 2014.
[23] B. Sklar, Digital Communications Fundamentals and Applications, Prentice Hall, 2nd
edition, Englewood Cliﬀs, NJ, 2001.
[24] M.R. Soleymani, Y. Gao, and U. Uilaipornsawai, Turbo Coding for Satellite and
Wireless Communications, Kluwer Academic Publishers, Dordrecht, 2002.
[25] A. Shibutani, H. Suda, and F. Adachi, “Complexity reduction of turbo coding,” Proc.
IEEE Vehicular Technology Conference VTC’99 Fall, Sept. 1999.
[26] I. Tal and A. Vardy, “How to construct polar codes,” IEEE Trans. Inform. Theory,
vol. 59, no. 10, Oct. 2013.
[27] I. Tal and A. Vardy, “List decoding of polar codes,” IEEE Int. Symp. Inform. Theory
(ISIT), 2011, pp. 1–5..
[28] O.Y. Takeshita and D.J. Costello, Jr., “New deterministic interleaver designs for
turbo codes,” IEEE Trans. Inform. Theory, vol. 46, no. 6, pp. 1988–2006, Sept. 2000.

BIBLIOGRAPHY
463
[29] A.A. Yazdi and F.R. Kschischang, “A simpliﬁed successive-cancellation decoder for
polar codes,” IEEE Commun. Lett., vol. 15, no. 12, Dec. 2011.
[30] Multiplexing and Channel Coding (FDD), TSG RAN TS25.212 v5.4.0, March 2003.
[31] Physical Layer Standard for CDMA2000, Spread Spectrum Systems Release C,
C.S0002-C Version 1.0, May 2003.
[32] ETSI EN 301 790 V1.5.1 (2009-05) Digital Video Broadcasting (DVB); Interaction
channel for satellite distribution systems, 2009.
[33] ETSI EN 302 307 V1.2.1 (2009-08) Digital Video Broadcasting (DVB); Second gen-
eration (DVB-S2), 2012.

Chapter 10
Convolutional LDPC Codes and
Spatial Coupling
10.1
Capacity: The Ultimate Limit
As we argued in Chapters 6—9, error control coding based on sparse graphical codes
has revolutionized the ﬁeld of error control coding.
These systems now routinely are
designed for performance within a fraction of the capacity limit of the application they
are designed for. However, the inability to design codes which can provably achieve the
channel capacity has haunted theoreticians since the early days of coding theory. This,
up to recently, open-ended journey towards the Shannon capacity has been eloquently
summarized in [4]. It is therefore quite common to ﬁnd the terminology “near-capacity,”
or “capacity-approaching” coding throughout the literature as a testimony to this short-
coming. While not of much relevance to the practitioner, it nonetheless leaves the theory
of coding incomplete.
In a ﬁnal development, this issue has now begun to be settled as well. The invention
of polar codes [1] has, for the ﬁrst time, produced a class of codes which can provably
achieve the capacity of the channel, binary-input symmetric-output memoryless channels
to be precise. Polar codes were discussed in Chapter 4 as an ingenious variation of Reed–
Muller codes. While polar codes have been hailed as a theoretical leap forward, their
applications to real-world systems has to date not yet progressed signiﬁcantly beyond
test implementations.
However, much progress has been achieved in accelerating the
intrinsically slow decoding algorithm [2, 16, 18, 19], despite the sequential nature of the
algorithm. Even though the algorithm is message-passing by nature (Chapter 4), it tends
to be more complex then the algorithms for the competing LDPC code families. A main
advantage of polar codes is their theoretical approachability; that is, their performance
theory is concise and neat.
465
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

466
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
Nonetheless, as argued throughout this book, iterative, belief-propagation-based de-
coding of sparse graphical codes has virtually completely replaced all other coding meth-
ods and has literally invaded the communications standards. Communications over wire-
less local-area networks (WiFi), WiMAX, digital video broadcasting, twisted-pair cable
modems (IEEE 802.3an), networking over power and phone lines, and coaxial cable all
use LDPC coding. Clearly, the inability of the theoreticians to prove that these codes
are formally capacity-achieving has not dampened the enthusiasm with which they were
received by the technical community. This last “obstace”, if we can call it such, has now
also found a solution in the idea of spatial coupling. The work in [11, 12] has demonstrated
that spatially coupled ensembles of such sparse codes have an improved decoding thresh-
old which approaches that of maximum a posteriori decoding (MAP) of the uncoupled
ensemble. This eﬀect has been called threshold saturation, and it essentially states that if
we wish to decode, say an LDPC code, to its MAP threshold, we need to spatially couple
it with other copies of that code and then apply essentially the same message passing
decoding algorithm.
The idea of spatial coupling has quickly spread to other applications such as source
coding and multiple access communications. The main process is analogous in all these
cases, however, and therefore we can restrict ourselves to the case of coupling graph-based
error control codes. The results in Section 10.4, however, are general in nature. For a lucid
introduction to spatially coupled coding see [3]. While it has taken researchers a while
to clarify the mechanisms of spatial coupling, its eﬀects have been known for a while.
Arguably the seminal starting point of this method was the construction of terminated
convolutional low-density parity-check codes (LDPCCs) by Feldstr´’om and Zigangirov [7],
which has itself given rise to a series of developments [6, 23] on variations of convolutional
code structures. We will start our discussion of spatial coupling with the exposition of
LDPCCs in Section 10.2, largely following the original development in [7], as reinterpreted
in [3]. And while LDPCCs have their own rightful place in the pantheon of error control
codes, one of their main contribution is to usher in the concept of spatial coupling, which
we will then discuss in more generality in Section 10.3. Spatial coupling will be developed
from the concept of the protograph already encountered in Chapter 6.
While the concept of threshold saturation, i.e., the eﬀect that the convergence thresh-
old of a coupled ensemble under message passing approaches the MAP threshold of the
individual member, is a key concept, and also observed for other applications, such as
joint decoding [20], its mathematical treatment can be complex and messy. We therefore
concentrate on a more general convergence analysis of spatial coupling, which takes its
intuition from the ﬁeld of statistical physics [22]. We introduce the concept of Lyapunov
stability, which allows us to obtain a theoretically compact and well-founded viewpoint.
This approach is then used to illustrate how we can show that coupled ensembles can
reach the capacity of the transmission channel, without having to ﬁrst show threshold
saturation. This will be done in Section 10.4.

10.2. LOW-DENSITY PARITY-CHECK CONVOLUTIONAL CODES
467
10.2
Low-Density Parity-Check Convolutional Codes
10.2.1
New LDPC Codes from Old
The low-density parity-check codes discussed in Chapter 6 are linear block codes of large
size, and their sparse graphical structure allowed them to be decoded eﬀectively via mes-
sage passing.
Feldstr¨om and Zigangirov [7] argued that codes with similar properties
should also exist if one takes a convolutional code point of view. Convolutional codes, of
course, have no predeﬁned size and can be operated in a continuous manner. Similarly,
such inﬁnite sparse structures can be constructed by “unwrapping” a block LDPC code
in the following manner.
We take the parity-check matrix of the (3,6) LDPC from Figure 6.2 and cut diagonally
as shown in Figure 10.1. This produces a lower part, which we call H0, and an upper
part H1.
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
cut
Figure 10.1: Unwrapping of Gallager-type low-density parity-check matrix H.
These diagonal partitions are then pasted together as two triangular matrices H0 on top of
H1, and this structure is repeated diagonally as shown in Figure 10.2. The resulting parity-
check matrix has inﬁnite extensions in both directions and is given by Hconv shown in the
ﬁgure. Since the original matrix H is sparse, the parity-check matrix for the convolutional
LDPC code is also sparse and can therefore, in principle, be decoded in exactly the same
manner as a conventional LDPC code. We will see in Section 10.2.2 that a much more
eﬃcient pipelined decoder architecture can be designed.

468
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
Hconv =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
...
...
H0
H1
H0
H1
H0
H1
H0
H1
H0
H1
H0
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
Figure 10.2: Construction of the parity-check matrix of a convolutional LDPC code.
This new code can also be interpreted as a convolutional code (Chapter 4). In fact,
it is a time-varying convolutional code with a constraint length μ = N, where N is the
length of the originating LDPC code. Recall that the constraint length is the number of
future output symbols (bits) which are aﬀected by the current input symbol (bit).
The construction in Figure 10.2 is not unique. We could equally well have applied
the cut and paste operation to sub-matrices of H, or, equivalently, to an original code
of diﬀerent dimensions.
This is illustrated in Figure 10.3 for an original code size of
6 × 12, also using H and generating the same Hconv, but this time it is interpreted as
a band-diagonal matrix of width 3, instead of 2. Its convolutional parity-check matrix
is also shown in the ﬁgure, labeled diﬀerently, although it is the same code. It can be
appreciated that there are a large number of ways of building such convolutional LDPC
codes from given block LDPC codes, and Pusane et al. [15] have constructed extensive
lists of codes and shown through simulations that the convolutional LDPC codes thus
obtained exhibit a substantial performance gain over their block-oriented counterparts, an
important observation in the context of the concept of spatial coupling discussed in the
next section.
Of course, the code cannot be operated indeﬁnitely in practice. Therefore, it has a
starting point and generally also a termination point. This may happen after a possibly

10.2. LOW-DENSITY PARITY-CHECK CONVOLUTIONAL CODES
469
large number L of sections, analogously to the operation of a convolutional code. Tech-
nically, Hconv becomes a terminated band-diagonal matrix of size L(N −K) × LN. It
turns out, however, that this termination on one or both ends of the code has a curious
side eﬀect. Since the “local” rate, that is, the ratio of information bits to coded bits, is
smaller around the termination, we observe a locally improved bit error rate. This eﬀect
is closely linked to the convergence eﬀects of spatial coupling.
Hconv =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
...
H1
H2
H4
H3
H5
H1
H6
H2
H4
H3
H5
H1
H6
H2
H4
H3
H5
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
Figure 10.3: Alternate construction of the parity-check matrix in Figure 10.2.
Constructing the Tanner graph of a convolutional LDPC code will reveal that the connec-
tions, while being “locally” random, reﬂect the band-diagonal structure of the parity-check
matrix. It is therefore instructive to draw the graph in “sections” as shown in Figure 10.4
below, which shows the ﬁrst three sections of Hconv from Figure 10.2. It can clearly be
seen that the ﬁrst bits are checked by more low-weight check nodes than the rest of the
code. This is equivalent to assuming that the symbols (bits) to the left of the starting
point are “known,” and set to logic zero in this case. This gives these bits a higher degree
of noise resistance.
Clearly, for this eﬀect to be fully noticeable, the base code size has to be much larger
than N = 24 as in the example. In fact, the sections of the convolutional LDPC are
typically derived from a block LDPC code, and we have seen in Chapter 6 that sizes of
N ≈103 and larger are typical. This makes ﬁgures of the type of Figure 10.4 unwieldy and

470
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Figure 10.4: Tanner graph of the convolutional LDPC from Figure 10.2.
confusing. We therefore resort back to the representation of codes via their protographs,
as discussed in Section 6.5.1.
This protograph representation of the code from Figure 10.4 is shown in Figure 10.5.
We recall that each edge of the protograph represents N/2 edges, connecting N/2 variable
nodes with N/2 check nodes, each represented in the protograph by a single variable
node and check node, respectively. As discussed in Chapter 6 these N/2 pairs of nodes
are connected by a permuted arrangement of the edges, where each protograph edge
represents a diﬀerent permutation. Note that the size of the graph copies represented by
the protograph, here N/2, depends on the code and its representation.
While the representation in Figure 10.5 is compact and easy to conceptualize, we
need to keep in mind that it is somewhat more general than the original convolutional
LDPC code construction. In order to regenerate the original codes, we need to make sure
that the outermost parity-check blocks in Hconv have triangular structure, which imposes
restrictions on some of the permutations associated with the edges of the protograph.
However, from a theoretical point of view, there are little diﬀerences, and in the next
section we will focus on the analysis of the structure in Figure 10.5.
Each section in Figure 10.4 represents an N-size section of the convolutional LDPC
code, and we can now clearly identify that the nodes close to the endpoints of the structure
have better protections. The outermost parity nodes have degree of only two, the next
closer nodes have degree four, and the inner nodes have full regular degree of six.
Also note that there is a small rate loss that is associated with this termination. That

10.2. LOW-DENSITY PARITY-CHECK CONVOLUTIONAL CODES
471
is, the code’s design rate is computed from the base matrix dimensions N −K × N as
R = 1 −(L + 1)(N −K)
LN
L→∞
−→K
N,
(10.1)
and quickly approaches the design rate of the base code.
Density evolution studies to ﬁnd the decoding threshold for convolutional LDPCs have
been studied in [14] and summarized in [3] for various lengths L of the structure i n Figure
10.2. This has been done for regular (J/2J) convolutional base codes. These thresholds
are shown in Figure 10.6 below and show an interesting trend: As L →∞, regular codes
with a suﬃciently large degree proﬁle, i.e., large J, achieve a threshold that coincides
with the capacity limit of the binary-input AWGN channel at Eb/N0 = 0.19 dB at rate
R = 0.5. We will prove this capacity-achieving property of regular coupled LDPC codes
in Section 10.4.
Note also that for small L, the rate loss experienced by the convolutional LDPC code
is given by (10.1), and that it requires codes of length several tens of sections to make
marked progress in the threshold and to overtake the threshold of the block version of the
same code.
One may wonder if the eﬀort to attain these gains is worth the increased complexity of
the system. While it is true that the computational decoding eﬀort per bit is unchanged
with respect to that of the base block code, the entire code structure is now much larger.
When before we would work with block sizes of 1,000–10,000, now these are the constraint
lengths of the codes, and the actual codelengths will be on the order of 100,000 and longer.
This has to be compared with the gain of a fraction of a dB in terms of performance. Such
gains are also achievable with irregular block LDPC codes, although these require careful
control of the error ﬂoor. Perhaps in the application for streaming data or largely varying
block sizes which have to be changed on demand, these codes could be an appropriate
engineering choice; see, e.g., [21]. However, the main excitement of convolutional LDPCs
l = 1 l = 2 l = 3 l = 4
l = L
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Figure 10.5: Protograph representation of the convolutional LDPC from Figure 10.2.

472
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
(3,6)
(4,8)
(4,10)
(3,9)
(3,12)
(3,15)
(3,20)
(5,10)
(5,10) Regular
(4,8) Regular
(3,6) Regular
L=6
L=4
L=3
L=4
L=10
L=7
L=15
L=9
L=8
L=7
L=5
L=5
L=6
L=7
L=17
BPSK Capacity Limit
Capacity in bits/dimension
Eb/N0
[dB]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
-2
-1
0
1
2
3
4
Figure 10.6: Decoding thresholds for convolutional LDPC codes for various lengths L and
degree proﬁles under message-passing decoding. The open circles are the thresholds for
the regular (J,2J) block LDPCs as discussed in Chapter 6.
is in their relationship to spatial coupling, which in turn allows us to ﬁnally prove the
achievability of Shannon’s capacity with a message-passing code ensemble.
10.2.2
Decoding Convolutional LDPC Codes
In principle, decoding of convolutional LDPC codes proceeds in an manner identical to
that for their block oriented versions. That is, as discussed in Chapter 6, Page 257, check
node update rules (6.6) and variable node update rules (6.7) alternate for a number I of
iterations until convergence has occurred.
However, since Hconv can be very large, a direct application of the update rules re-
sults in signiﬁcant wastage of storage elements and excessive decoding delays. Given the
band-diagonal nature of Hconv, implementing the iterations as successive operations that
progress through the data set sequentially is much more eﬃcient [7, 21] and can be realized
with a number I of distinct and separate processor, each playing the role of an “iteration.”

10.2. LOW-DENSITY PARITY-CHECK CONVOLUTIONAL CODES
473
Due to the diagonal nature of the parity-check matrix, processors have to use overlapping
data as illustrated in Figure 10.3.
The decoder operates with banks of ﬁrst-in, ﬁrst-out register banks of size 2N, i.e.,
spanning two of the original block sizes. Each register bank has dv + 1 registers, where
the registers are attached to the “sockets” of the variable nodes. As a block of N symbols
enters the decoder, the LLR values are computed according to (6.5) and shifted into the
ﬁrst N registers of the decoder. The second N registers are the LLR values from the
previous received data block, which are shifted out of the ﬁrst N positions. The ﬁrst
register bank will continue to contain these LLRs, while the other dv register banks will
hold the messages returned by the check nodes by the variables nodes.
Registers for Processor 0
Registers for Processor 2
Registers for Processor 1
P1: parity-check 
computation
P2: parity-check 
computation
P3: parity-check 
computation
Time
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
0
0
Figure 10.7: Illustration of the pipelined processing for the LDPCC decoder from Fig-
ure 10.2.
As in the standard LDPC decoding algorithms, the LLR values are passed to the check
nodes which compute their respective output messages following (6.6). This is the “hori-
zontal” pass of the processor. Viewing Processor 1 in Figure 10.7, we see that all messages
for this operation are available in its register banks. The check nodes will now overwrite
these values, and the registers then contain the return messages from the check nodes.
Concurrently, Processors 2–I are all updating their registers with the check-to-variable

474
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
messages, using their own register banks which contain iterated messages as we shown
below.
In the step the variable nodes update the messages using (6.7). However, it has only
suﬃcient data to compute the second N variable-to-check node messages, which are up-
dated in the second half of the register banks. At the same time, Processors 2–I are also
updating their registers using in an identical manner. All registers are then shifted by N
positions, where the second N values in each 2N register bank are shifted into the ﬁrst N
positions of the registers of the subsequent processor. In this manner the dv register banks
contain further updated check-to-variable node messages for each additional processor.
Note that a processor does not exactly correspond to an iteration, since half of the
message used in the check node updates are from one iteration, while the other half
is from a previous iteration. That is, Processor i uses variable-to-check node message
corresponding to iteration i and iteration i + 1.
Further limitations are the fact that
the number of processors is not easily changed, especially during operation, and therefore
dynamic buﬀering methods, such as [17], which are often used in the application of block
LDPC codes to reduce the average number of iterations cannot easily be incorporated.
Nonetheless, a 180 nm implementation with 10 processors and N = 128 for a regular
(3,6) code achieves a throughput of 175 Mbit/s [21] with a core size of 10 mm2. Given the
rapid progress of miniaturization, we extrapolate that a 22 nm implementation could be
built with a footprint of 1 mm2 and a speed of 1 Gbit/s.
While we have spent our eﬀort in discussion the decoding and performance of convo-
lutional LDPCs, we need to at least touch on the encoding operation. As we have seen
in Chapter 6, direct encoding of block LDPC codes is not immediately obvious, and sev-
eral simplifying approaches have been utilized. However, the direct and straight-forward
encoding method of codes is appealing and accounts at least in part for the popularity of
such codes as the repeat-accumulate codes, or generally protograph based codes as in [5].
Feldstr¨om and Zigangirov present a simple convolutional encoder with time-varying taps
for their codes, which is possible given the speciﬁc code construction method they used.
Convolutional LDPC codes, or coupling codes can, of course, also be accomplished
with other base codes, such as the protograph based codes studied and designed at the
Jet Propulsion Laboratory (JPL) [5]. The advantages of coupling other than regular block
LDPC codes is also pointed out in [10], who spatially couple repeat-accumulate codes, also
achieving better thresholds for ﬁnite codelengths, as well as simpler encoding structures.
10.3
Spatial Coupling: A General View
The convolutional LPDC codes from Section 10.2 are just one special realization of the
generic concept of spatial coupling. In fact, the representation in Figure 10.5 is more
general, and we wish to use this general form for our further discussion.
Figure 10.8

10.3. SPATIAL COUPLING: A GENERAL VIEW
475
represents this concept again in a more generalized view. Each protograph, to the left,
represents a complete LDPC block code of length N. That is, for a (3,6) code, for example,
the bold lines would represent the 3N connections between N variable nodes and N/2 check
nodes.
What happens in the coupling process now is that a ﬁxed portions of these connections
are routed to the neighbor codes. The number of neighbors involved on each side of this
bipartite graph is called the coupling window W. In our example W = 5. While coupling
can, in general, involve arbitrary numbers of edges, we will concentrate on uniform edge
spreading. That is, each line in the coupled graph represents 1/w of the edges of the
original code. Edges are permuted randomly, observing however that the original node
degrees of the uncoupled codes are preserved.
Other than this, in general, no special
structures like the one encountered in Section 10.2 are assumed. Note that the “dangling”
check nodes on either side of the coupled structure implies that 1/5 of the variable nodes
in the ﬁrst and last code are ﬁxed logic “zero.” It is advantageous to think of the missing
connections into the check nodes as “known” symbols. While not critical in a practial
setup, this will help us in the analysis of the coupled system.
Individual Codes
p0f(y)
g(x)
x(l)
Coupled Ensemble
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Coupling
Figure 10.8: Protograph represntation of a coupled set of LDPC codes with coupling
window width w = 5; that is, each original code is coupled to the 4 nearest neighbors.
While there clearly exist many analogous ways of implementing the connectivity with
neighbor codes, such as the method discussed in Section 10.2, we wish to discuss the
dynamical behavior of such a coupled ensemble here, and not to dwell on potential imple-
mentation strategies and issues.
To start oﬀthe discussion, we recall the iteration equation (6.35) which describes the
statistical behavior of a regular block LDPC code on an AWGN channel from Chapter 6,

476
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
reproduced here as
x(l) = φ
4Es
N0
+ (dv −1)φ−1
1 −

1 −x(l−1)dc−1
(6.35)
where the function φ(·) is deﬁned in (6.32) and plays the role of a bit error estimate, but
only in the sense that if x(l) →0, then the average BER of a large random code ensemble
will also go to zero, and we say the code “converges.” Recall that the x(l) are related to
the message traveling from the check nodes to the variable nodes, but this is not relevant
to the analysis.
While the single code convergence formula (6.35) for the AWGN channel is perhaps
the most relevant in practice, it is often easier to discuss basic concepts with a simpler
channel model, and we therefore also consider the convergence equation for a block LDPC
code on a binary erasure channel (BEC), given in (6.10), reproduced here as
x(l) = f(x(l−1), p0) = p0

1 −

1 −x(l−1)dc−1dv−1
,
(6.10)
where x(l) here is the probability of bit erasure at iteration l in a (dv, dc) regular LDPC
block code of inﬁnitely large size N →∞.
If we abbreviate the statistical check node function in (6.10) by
g(x) = 1 −(1 −x)dc−1,
(10.2)
and abbreviate the statistical variable node function by
f(y, p0) = p0ydv−1,
(10.3)
then dynamic behavior of a regular LDPC code (6.10) is succinctly summarized by the
iteration equation
x = f(g(x), p0) = p0f(g(x)).
(10.4)
The condition of convergence (to zero error) discussed in Chapter 6 is equivalent to the
condition that (10.4) has only one ﬁxed point at x = 0, and the supremum of p0 such
that only one solution to (10.4) exists is the initial erasure probability that the code can
correct. In other words, convergence to x(∞) = 0 requires
x −f(g(x)) > 0,
∀p0 ≥x > 0.
(10.5)
This function is illustrated for a number of erasure probabilities p0 in Figure 10.9, and
from numerical experimentation we ﬁnd that the largest erasure probability that can be
corrected by the code is p0 = 0.4294.

10.3. SPATIAL COUPLING: A GENERAL VIEW
477
In the uncoupled case, each LPDC code will converge with a dynamic behavior gov-
erned by the convergence equation (10.4), which involves only a single parameter. Once
we couple the system, we now have to deal with individual convergence behaviors for each
individual system, and, in general, we need to consider a vector convergence equation of
the form
x = F (x, p0)
(10.6)
where x = [x1, . . . , xL] is the vector of the statistical variable describing all the L + 1
coupled dynamical systems.
0.1
0.2
0.3
0.4
0.5
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.10
x −f(g(x))
x
p0 = 0.35
p0 = 0.5
p0 = 0.4294
Figure 10.9: Statistical convergence functions for the (3,6) LDPC block code ensemble.
There are, as mentioned, a large number of ways to couple systems, but here we
concentrate on coupling using a uniform window of size w, in which case the variables are
updated as
x(l+1)
i
= 1
w
w−1

k=0
f
⎛
⎝1
w
w−1

j=0
g

x(l)
i+j−k, p0

⎞
⎠,
(10.7)
As we will see, the key impact of spatial coupling, as well as the complication in the
analysis, stems from the fact that the coupled systems have certain boundary conditions.
Speciﬁcally, dynamic variables that lie outside the window, i.e., values xl, l ∈[0, L], refer

478
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
to component systems that do not exist. If we inspect Figure 10.8 more closely, we see
that this eﬀect can be modeled by assuming xl = 0, ∀∈[0, L]; that is, the corresponding
variables are known. On the side we note that this can also refer to symbols that are a
priori known, such a pilot signal, frame markers, preambles, and other ﬁxed signals we
may wish to couple into the code structure.
With this model, Equation (10.7) is now completely general, and, together with the
boundary condition on the elements xl not included in x, deﬁnes a given spatially coupled
code ensemble and describes its averaged dynamic performance.
For short-hand we deﬁne the vector functions
g(x, ε) and f(x),
(10.8)
which deﬁnes the operation where the functions f(·) and g(·) are applied component-wise
to all entries of the vector x. We also deﬁne the matrix A as the band-diagonal matrix
with entries 1/w for all j ≥i and j −i ≤w, and zeros elsewhere. With this, (10.7) can
succinctly be written as the vector update equation
x(l+1) = AT f

Ag

x(l), ε

= F (x(l); ).
(10.9)
Note that if w = 0, A is the identity matrix and (10.9) reduces to the case of individual
block LDPC codes discussed in Chapter 6.
In this context, one is typically interested in the largest ε such that for any possible
initial vector x(0), lim
l→∞x(l) = 0. This parameter is typically a signal-to-noise ratio, channel
raw error rate, or a channel erasure rate as we have seen before.
Introduce the set X 0 as any set such that x ∈X 0. Since the entries of x are strictly non-
negative—since they represent an error rate, or an absolute error—the following conditions
are satisﬁed:
F (x; ε) ≤x and F (x; ε) ∈X 0.
(10.10)
Then for any l and x(l) ∈X 0 we have x(l+1) ≤x(l) and x(l+1) ∈X 0, i.e., {x(l)} is a
monotonically non-increasing sequence. If x(0) ∈X 0 then x(l) ∈X 0 for any l ≥0.
If, in particular, the initial x(0) is such that x(1) = F

x(0); ε

< x(0), then
x(2) = F

x(1); ε

< F

x(0); ε

= x(1),
and therefore x(0) > x(1) > x(2) > . . ., i.e., the sequence {x(i), i = 0, 1, 2 . . .} is strictly
monotonically decreasing and it converges to a limiting x(∞) ≥0.
The limiting x(∞) should be a stable ﬁxed point. If x(∞) is an unstable ﬁxed point,
then the system passes through that x(∞) to another ﬁxed point.
The system (10.9) shows some very interesting convergence behavior. As discussed
above, if w = 0, then all the component systems converge according to the scalar conver-
gence equations (6.35) and (6.10) for the two example cases of the AWGN channel and

10.3. SPATIAL COUPLING: A GENERAL VIEW
479
the BEC channel discussed here. However, if w ̸= 0, the boundary conditions, by acting
in an “anchoring” manner, cause the dynamical variable xl close to the edges to converge
faster then the interior points. This “perturbation” at the boundaries now propagates to
the interior of the system in a wavelike manner.
Figure 10.10 illustrates this dynamic process for an L = 100 coupled system with a
window size of w = 8. The plots show the individual values of xl at iteration number 10,
30, and 50. As can be guessed, convergence progresses inward from the two boundaries at
a constant speed until all xl = 0.
0
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
0
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
wave
wave
0
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
wave
wave
Iteration l = 10
Iteration l = 30
Iteration l = 50
Figure 10.10: Illustration of the convergence for a coupled (3,6) LDPC code ensemble with
a coupling window of w = 8.
Quite evidently, the boundary condition makes a signiﬁcant diﬀerence in the conver-
gence of the coupled system. Without the boundary, as occurs for example if the coupled
chain is closed back on itself in a ring of coupled systems, convergence of the system would
be no diﬀerent than the convergence of a single LDPC code ensemble. The intricacies of

480
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
the eﬀect of the boundary condition will be explored in Section 10.4.
A way to investigate the system (10.9) was pioneered in [22] and then developed in
[25, 26]. It is based on using the following function U(x) : x →R1, called the potential
function1:
U(x, ε) =
x

0
g(z, ε) [z −f(g(z, ε))] dz.
(10.11)
The motivation for using the function U(x, ε) in [22] was based on a continuous-time
approximation for the system (10.9), given by
dx(t)
dt
= F (x(t); ) −x(t),
t > 0,
t →∞,
(10.12)
and, in turn, on the close relation of an analog of the function U(x) to the Bethe free
energy of the system. This will be further developed shortly in Section 10.4.
For now it suﬃces to state the main result from this development, which is the following
Theorem 10.1 For large window sizes, i.e., w →∞, an iterative system of sparse graphs
coupled as in (10.9) converges to x(l) →0, if U(x) > 0; that is, the coupled system
threshold is given as
ε∗
c = sup{ε : U(x, ε) > x, ∀x ≤ε}
(10.13)
The mechanics of this theorem are illustrated in Figure 10.11, where U(x, ε) is plotted
for the coupled (3,6) LDPC code ensemble from Figure 10.9 for the BEC channel. As per
theorem, a large coupled ensemble will converge for erasure rates up to p0 = 0.48815, very
close to capacity at CBEC = 0.5.
It is not too diﬃcult to argue that ε∗
c ≥ε∗for any w, that is, coupling always improves
performance. In fact, Theorem 10.1 allows us to prove the optimality of certain message-
passing systems in coupled sparse graphs. An example is the following:
Theorem 10.2 For large window sizes, i.e., w →∞, an iterative system of sparse coupled
regular (dv, dc) LDPC codes achieves the capacity of the BEC channel, i.e.,
ε∗
c →α
for α = dv
dc
and
dv →∞
(10.14)
Equivalently, the rate of the code R = 1 −dv/dc approaches the channel capacity as in
R →1 −ε = CBEC
for ε →ε∗
c.
(10.15)
1This terminology stems from the fact that the integral in (10.29)—discussed later in this chapter—does
not depend on the curve C from 0 to x along which this integral is computed.

10.3. SPATIAL COUPLING: A GENERAL VIEW
481
0.0
0.1
0.2
0.3
0.4
0.5
0.01
0.02
0.03
0.04
U(x, p0)
x
p0 = 0.35
p0 = 0.4294
p0 = 0.48815
Figure 10.11: Potential functions for the coupled (3,6) LDPC code ensemble on the BEC
channel, compared to the convergence functions for the uncoupled codes from Figure 10.9.
Proof: For the (dv, dc)-LDPC codes the function U(x) from (10.11) takes the form
U(x, ε) = 1
dc
−(1 −x)dc
dc
−x(1 −x)dc−1 −ε
dv

1 −(1 −x)dc−1dv,
(10.16)
with f(x) = xdv−1 and g(x) = 1−(1−x)dc−1. We need to ﬁnd the maximum ε∗= ε∗(dv, dc)
such that U(x, ε∗) ≥0, x ∈[0, 1]. Consider the case dv →∞and dv/dc →α, α < 1, as in
the conditions of the theorem. We will show that ε∗(dv, dc) = α. Set ε = α and dv = dcα.
Then
dcU(x, α) = 1 −(1 −x)dc −dcx(1 −x)dc−1 −

1 −(1 −x)dc−1dcα
,
U
x(x, α) = (dc −1)(1 −x)dc−2

x −α

1 −(1 −x)dc−1dcα−1
.
Since U(1, α) = 0 and U
x(x, α) > 0, x ≥α, it makes sense to consider only x < α.
We may also exclude small x from further consideration as follows. We have
x −α

1 −(1 −x)dc−1dcα−1
≥x −α[(dc −1)x]dcα−1 ≥0,
x ≤x0 =
1
(dc −1)

1
α(dc −1)
1/(dcα−2)
,

482
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
where x0 ≥1/(2dc). Therefore U
x(x, α) ≤0, if x ≤1/(2dc). Since U(0, α) = 0, all that
remains is to consider the case x = b/dc, 1/2 < b < αdc. Since (1 −x)dc ≤e−dcx and
(1 −x)dc ≥e−dcx/(1−x), we have for 1/2 < b < αdc
dcU(b/dc, α) ≥1 −e−b −be−b −

1 −e−b/(1−α)dcα
≥o(1),
dc →∞.
Therefore
inf
0≤x≤1 U(x, α) = o(1/dc),
dc →∞,
which proves the theorem.
Q.E.D
We note that the proof of Theorem 10.2 was quite elementary, beneﬁting from the more
general results of Theorem 10.1. This is in some contrast to the machinery invoked in [12],
which proceed in a diﬀerent way. First, it is shown that the threshold of a coupled LDPC
ensemble has an improved convergence threshold ε∗
c and that this threshold approaches
the convergence threshold of maximum-likelihood decoding of the underlying block LDPC
code ensemble. This is called threshold saturation. It then remains to show under what
conditions the original code ensemble can achieve capacity under maximum likelihood
decoding. Here, we can show directly that the regular code ensemble when coupled achieves
capacity.
The phenomenon of threshold saturation and—therefore, capacity-achieving perfor-
mance if the underlying code ensemble permits this—appears to be quite general [11] and
has also been shown for other communications scenarios, such as the isotropic multiple
access channel used with uniformly random signal sets [20]. Coupling clearly is a power-
ful methodology and represents the missing link that has plagued coding theory since its
inception in the 1940’s. Finally, the theoretician can rest assured that message-passing
error control code decoding is theoretically optimal, both in performance and in order
complexity of decoding. The practitioner of coding has long been satisﬁed with the pow-
erful capabilities of message passing decoding, as evidenced by the numerous standards
that involve LDPC codes or turbo codes. If and what impact spatial coupling will have on
the application of new coding methods remains an open point at this time, but given that
the basic block versions of these codes already approach capacity to within a fraction of
a dB in many cases, we speculate that practical uses of spatial coupling may be reserved
for very special cases.
10.4
Spatial Coupling: Convergence Analysis
10.4.1
Problem Setup
In this section we present a convergence analysis based on the well-established Lyapunov
theory [13]; for a modern version see [8]. To recap the convergence problem from the

10.4. SPATIAL COUPLING: CONVERGENCE ANALYSIS
483
preceding section, we work with a set X 0 = X 0(ε) ⊆X such that for x ∈X 0, the
following conditions are satisﬁed
F (x; ε) ≤x and F (x; ε) ∈X 0.
(10.17)
That is, by virtue of our iteration problem, we are only considering the convergence case
where x is non-increasing. Note that, as can be appreciated from Figure 10.9 for example,
if we start with a suitably small x and a large value of p0, the iterations may actually
diverge to a stable point. This behavior is not of interest insofar that it does not represent
any reasonable application scenario.
Now then, for any l and x(l) ∈X 0 we have x(l+1) ≤x(l) and x(l+1) ∈X 0, i.e., {x(l)} is
a monotonically non-increasing sequence, and with x(0) ∈X 0 all x(l) ∈X 0 for any l ≥0.
If, in particular, we let the initial point be x(0), and x(1) = F

x(0); ε

< x(0), then
x(2) = F

x(1); ε

< F

x(0); ε

= x(1),
and therefore x(0) > x(1) > x(2) > . . ., i.e., the sequence {x(i), i = 0, 1, 2, . . .} is strictly
monotonically decreasing and it converges to the limiting x(∞)(x(0), ε) ≥0.
That is,
unless we happen to start at a ﬁxed point, the convergence function (10.17) can be shown
to be a strict inequality. For simplicity, we now consider the case x(0) = 1 for which those
conditions are satisﬁed from above. One can always normalize the convergence equations
such that this assumption is met. Of interest to us is the question: Under what conditions
on F (x; ε) do we have x(∞)(ε) = 0?
The limiting x(∞) should be a stable ﬁxed point. If x(∞) is an unstable ﬁxed point,
then the system passes through that x(∞) to another ﬁxed point.
10.4.2
Lyapunov Approach
Essentially, Lyapunov built a theory whereby the often exceedingly complicated study of
the behavior of individual trajectories in dynamical systems is reduced to the study of the
system properties in certain regions of interest. This is done by using Lyapunov functions
in these regions. In other words, rather than studying the behavior of the entire trajectory
of a complex dynamical system, we study its behavior in a certain neighborhood. Knowing
this, we can deduce the behavior of all trajectories in this neighborhood.
Denote by X 0 the smallest set containing x(l) for all l ≥0, and by X 1 an open set such
that X 0 ⊆X 1, but 0 ∈X 1. Denote by E0 = E \ {0}, and let E2 ⊆E0 be a subset of E0.
Deﬁnition 10.1 The solution x(l) ≡0 to (10.9) is globally asymptotically stable if
lim
l→∞x(l) = 0 for all x(0) ∈X 0.

484
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
This type of problem was approached by Lyapunov using a certain type of functions
in the region X 0 (or a region containing it). These functions are now known as Lyapunov
functions, which are given by the following
Deﬁnition 10.2 A continuous function V (x) : X →R1 is called a Lyapunov function
for the system (10.9) with x ∈X 0, ε ∈E2, if it satisﬁes the following conditions:
V (0)
=
0;
(10.18)
V (x)
>
0,
x ∈X 1;
(10.19)
V (f(g(x); ε)) −V (x)
<
0,
x ∈X 1,  ∈E2.
(10.20)
The following result is an adaptation of Lyapunov’s direct method (1892) [8, Theorem
13.2]. It gives suﬃcient conditions for global asymptotic stability of the system (10.9).
Theorem 10.3 Given that V (x) is a Lyapunov function for the system (10.9) for ε ∈E2,
the solution x(l) ≡0 is globally asymptotically stable.
That is, in the case stipulated by Theorem 10.3, the coupled system will converge to the
all-zero solution. The condition (10.19) is a bit stronger than the ﬁrst of the assumptions
in (10.10). It is important also that the condition (10.20) holds in an open neighborhood
of a limiting point of the system (10.9).
Proof: Lypunov’s Theorem 10.3 is actually quite easy to prove as follows. Suppose
trajectory x(l) does not converge to zero. Given that (10.20) is the diﬀerence of V (x)
as the process advances through iterations, V (x(l)) converges to some value, say, x(∞).
Since, per assumption, x(l) does not converge to 0, there exists some δ such that for all l,
δ ≤V (x(l)) ≤V (x(0)). Working only with continuous functions V (x), the supremum of
˙V (x) = −a < 0 is attained somewhere in X 1. Hence, for all l we have
V (x(L))
=
V (x(0)) +
L

l=1

V (x(l)) −V (x(l−1))

≤
V (x(0)) −aL,
(10.21)
which, for L > V (x(0)/a implies V (x(L)) < 0 < δ, and this is a contradiction. Conse-
quently, every trajectory x(l) converges to 0 and the theorem is proven.
q.e.d.
The next question is how to ﬁnd an appropriate Lyapunov function.
We start by
representing the system (10.9) in the form
x(l) −x(l+1) = q

x(l)
,
l = 0, 1, 2, · · · .
(10.22)

10.4. SPATIAL COUPLING: CONVERGENCE ANALYSIS
485
where q(x) = x−F (x). In order to avoid dealing with the trajectory {x(l)}, we construct
a special type of Lyapunov function for (10.22). For that purpose we ﬁrst construct such
a function for the following continuous-time analog of the system (10.22)
dx(t)
dt
= −q(x(t)),
t > 0,
t →∞,
(10.23)
where q(x(t)) = x(t) −F (x(t)).
We apply the variable gradient method for constructing Lyapunov functions to the
system (10.23) [8, Chapter 3.4]. It will be a Lyapunov function for the system (10.22) as
well.
Let V : X →R1 be a continuously diﬀerentiable function and let
h(x) =
∂V
∂x
⊤
.
i.e., h(x) is the gradient of V (x), deﬁned as
∂V
∂x =
 ∂V
∂x1
, ∂V
∂x2
, . . . , ∂V
∂xd

,
The derivative of V (x) along the trajectories of (10.23) is now simply given by
dV (x)
dt
= −∂V
∂x q(x) = −h⊤(x)q(x).
(10.24)
Next, we need to assure that condition (10.19) is fulﬁlled, that is, that we construct
h(x) such that h(x) is the gradient for a positive function.
We also need to ensure
condition (10.20), that is
dV (x)
dt
= −h⊤(x)q(x) < 0,
x ∈X,
x = 0.
(10.25)
The function V (x) can then be computed from the line integral
V (x) =
x

0
h⊤(s) ds.
(10.26)
Recall that the line integral of a gradient vector h : Rd →Rd is path independent, and
hence, integration in (10.26) can be taken along any path joining the origin to x ∈X.
It is further known [8, Proposition 3.1] that h(x) is the gradient of a real-valued
function V : Rd →R1 if and only if the Jacobian matrix ∂h/∂x is symmetric, i.e., iﬀ
∂hi
∂xj
= ∂hj
∂xi
,
i, j = 1, . . . , d.
(10.27)

486
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
Referring to (10.25), we look for h(x) of the form h(x) = B(x)q(x), where B(x) is
an n × n-positive-deﬁnite matrix. Then due to (10.25) we need (x ∈X 1, x = 0)
h(x)q(x) = q(x)B(x)q(x) > 0,
(10.28)
and from (10.26) we have
VB(x) =
x

0
[B(s)(s −f(g(s)))] ds.
(10.29)
For any positive-deﬁnite n × n-matrix B(x) the function VB(x) from (10.29) is a
Lyapunov function for the system (10.23), if condition (10.28) is satisﬁed, and VB(x) > 0.
Then
lim
t→∞x(t) = 0.
(10.30)
We now show that the function VB(x) from (10.29) is a Lyapunov function for the
system (10.9) as well. Condition (10.28) takes the form
[x −f (g(x))] B(x) [x −f (g(x))] > 0,
(10.31)
for x ∈X 1, x = 0. Speciﬁcally, if we choose B(x) = g(x), then
VB(x) = U(x),
(10.32)
which will lead us directly to the scalar potential function (10.11). As a result, we get:
For any positive-deﬁnite d × d-matrix B(x) the function VB(x) from (10.29) is Lya-
punov function for the system (10.9), if the condition (10.31) and
VB(x) > 0,
x ∈X 1,
x = 0
(10.33)
are satisﬁed. In this case
lim
l→∞x(l) = 0.
(10.34)
The key approach now is the following: Given a suitable Lyapunov function, we can
obtain a region of the parameter space  ∈E2 for which our coupled system converges.
The actual convergence region may be larger than E2 and also may depend on V (x).
Furthermore, as it turns out, E2 critically depends on the initial conditions of the coupled
system.
For example, it can quite easily be veriﬁed from (10.9) that if a coupled system is
connected back from the end to the beginning in a manner analogous to the tail-biting
constraints discussed in Chapter 4, then the point xu = (. . . , x∞, x∞, x∞, . . .) is a stable
point of the recursion (10.9), irrespective of the window size w, and x∞is the ﬁxed point

10.4. SPATIAL COUPLING: CONVERGENCE ANALYSIS
487
of the uncoupled system. Consequently, (10.5) applies and coupling has no eﬀect in this
situation, that is, the system behavior is no diﬀerent to that of the uncoupled system. The
same is true if the coupled system extends indeﬁnitely in both directions.
As alluded to above, the boundary condition of a coupled system has a profound impact
on its convergence behavior. As in the vast majority of papers published on the subject,
we study the two-sided boundary condition xi = 0, i < 0; i > L, which can also be thought
of as anchoring of the spatially coupled system. One-sided anchoring works as well, in
which case there is simply only a forward convergence propagation taking place, rather
than a bidirectional convergence as in Figure 10.10.
Formally we therefore introduce the following:
Deﬁnition 10.3 The coupled-system threshold with xi = 0, i < 0; i > L is deﬁned as
ε∗
c = sup {ε ∈E2|x∞(1; ε) = 0}.
(10.35)
Evidently ε∗
c ≥ε∗
s in general, with equality if w = 0, or, for example, if the L identical
systems are arranged in a circle such that no boundary exists as discussed above.
We will also use:
Deﬁnition 10.4 For a positive-deﬁnite matrix B the coupled-system threshold εc(B) is
deﬁned as
εc(B) = sup

ε ∈E2| min
x∈X0 VB(x) ≥0

.
(10.36)
For any positive-deﬁnite matrix B we have
ε∗
c(B) ≤ε∗
c,
(10.37)
which expresses the fact that the Lyapunov function provides only a suﬃcient condition
for convergence, not a necessary condition.
As we have calculated in Section 10.3, if
our threshold ε∗
c(B) coincides with the capacity limit of the channel, then evidently the
capacity theorem provides the necessary condition, in which case the Lyapunov function
chosen furnishes the largest convergence region E2.
Since we have already identiﬁed our Lyapunov candidate function in (10.32), we now
simply need to identify the region of validity E2 for this function. However, we run into
the following diﬃculty with (10.20), which requires a more detailed look at that condition.
Considering regions X0 that are large enough to be of interest for global convergence
in applied dynamical systems, that is, those that include actual starting points of the
decoders, we encounter ﬁxed points x∞= 0, i.e., points for which x∞−f (g(x∞, ε)) = 0.

488
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
For these points, condition (10.20) needs to be reexamined. Speciﬁcally, we must show
that in a region X0 of interest, any such ﬁxed point x∞cannot be stable, and that the
Lyapunov condition (10.20) can be relaxed to a non-strict inequality at these points. In
practice, this means that any such x will move away from x = f (g(x, ε)) due to any
disturbance, and re-enter the region convergence. In the sequel we will demonstrate what
these points are and how progress away from the non-stable ﬁxed points. This is indeed
the convergence mechanism that we observe in experiments.
For a vector x = (x−L, . . . , xi, xi+1, . . .) introduce the shift operator S such that Sx =
(0, x−L, . . . , xi−1, xi, . . .). We note that if x∞is a ﬁxed point, then Sx∞is also a ﬁxed
point. This is trivially true for xu deﬁned above. The validity of this can be veriﬁed by
realizing that the equations (10.9) are shift invariant.
This leaves us with ﬁxed points x∞for which xi = 0, i < 0; i > L. To shorten the
discussion, we assume now without any loss of generality that L →∞, that is, we eliminate
the right border. Due to the monotonicity of x(l) in l, we have x∞
i
< x∞
i+1; that is, x∞
is an increasing sequence in l. An example of this situation is shown in Figure 10.10 by
considering only the left-hand side.
Now if w is suﬃciently large, any intermediate point x∞
λ = λx∞+ (1 −λ)Sx∞
λ is also
a ﬁxed point if x∞is a ﬁxed point, i.e., x∞
λ −f (g(x∞
λ , ε)) = 0. Therefore, there exists a
continuous “ridge” xρ = x∞+ ρSx∞of such unstable ﬁxed points. It is along this ridge
that the system converges to x∞= 0 as we now show.
The Lyapunov conditions guarantee convergence of x(l) towards 0, but the ridge xρ
“intercepts” x(l), primarily since convergence along the ridge is slower. The argument
relies on the following logic: We will show that the non-zero ridge points, which are ﬁxed
points of the convergence equation, are nevertheless unstable because VB (xρ) is a saddle
point of VB (x) and not a local minimum. Therefore no xρ can be stable. Furthermore,
convergence proceeds along the ridge as what can be interpreted as a convergence wave;
see Figure 10.10.
Theorem 10.4 There exists a function w0(f, g) such that for any positive-deﬁnite matrix
B, w ≥w0(f, g), L ≥2w + 1, and ε < ε∗
c(B); the only stable ﬁxed point of the system
(10.9) is x0 = 0.
Proof.
We have x(l+1) < x(l) and VB

x(l+1)
< VB

x(l)
for all l ≥0, and the
sequence {x(l)} converges to a ﬁxed point x0 ∈xρ, which is a (local) extremum of the
function VB (x). Since x0 −εf (g(x0)) = 0), we have V 
B(x0) = 0 and therefore need to
consider second derivatives (the Hessian). Using a Taylor expansion we write
VB(x) = VB(x0)
+1
2(x −x0)T V 
B(x0)(x −x0) + O

x −x03
,

10.4. SPATIAL COUPLING: CONVERGENCE ANALYSIS
489
where
V 
B(x0) = B(x0)

I −f  (g(x0, ε))

.
In general, if the L + 1 × L + 1 matrix I −εf  (g(x0)) is not positive deﬁnite—that is, if
it has a negative eigenvalue—then x0 cannot be a local minimum of the function VB (x),
and x0 is a saddle point. For the speciﬁc system (10.9) we have
f  (g(x0)) = AT f (Ag (x, ε)) Ag (x, ε) = C (x0)
Typically, if ε is suﬃciently small then there exists only the zero ﬁxed point x0 = 0, that
is, condition (10.19) holds as a strict inequaltiy. This is somewhat trivially true for ε < ε∗;
i.e., the parameter is less than the threshold of the uncoupled system.
As ε grows, it reaches some ε1 ≥ε∗> 0 where non-zero ﬁxed point(s) x0 = 0 appear.
Since the ﬁrst derivative of VB(x) is zero only in the direction of the ridge xρ, and strictly
negative everywhere else as per (10.20), we need to examine the sign of the second deriva-
tive only in the direction of xρ, that is, letting z0 = x0 −Sx0, we study for ε > ε1 when
and if
zT
0 V 
B(x0)z0 ≤0,
(10.38)
which is equivalent to the condition
zT
0 C(x0)z0 > 1.
(10.39)
The matrix C is non-negative, i.e., all its elements are non-negative. Therefore its spectral
radius ρ(C) equals its maximal eigenvalue. Moreover, if C has a positive eigenvector, then
the corresponding eigenvalue is ρ(A) [9, Chapter 8].
If w = 1, then the ﬁxed point x0 = (x0, . . . , x0) and matrix C is diagonal with equal
diagonal elements and the scenario reduces to the uncoupled case discussed above.
If
w > 1, then the ﬁxed point x0 consists of nondecreasing components. For the form (10.9)
of the dynamical system, the ith row Ci of C is given as
Ci = εaiDi,
ci = f
g(yi)g(yi),
yi = 1
w2
w−1

k=0
w−1

j=0
x0,i+j−k,
Di = (Di,0, . . . , Di,n),
Di,j = w −|i −j|
w2
,
|i −j| ≤w,
Di,j = 0,
|i −j| ≥w.
(10.40)
Diagonal elements of C are {εci/w, i = 1, . . . , n}. For the matrix D of rows {Di} and the
matrix C of rows {Ci} we have

490
CHAPTER 10. CONVOLUTIONAL LDPC CODES AND SPATIAL COUPLING
Lemma 10.5 For any w ≥1 and L ≥2w + 1 the matrix D has the maximal eigenvalue
ρ(D) = 1. Matrix C has the maximal eigenvalue ρ(C) = ε max
i
ci.
Therefore, if ε > 1/ max
i
ci, then ρ(C) > 1, and In −εf  (g(x0)) has a negative eigen-
value as required to render x0 unstable.
Remember that we still have to verify constraint (10.33), i.e., VB(x) > 0, x = 0, which
sets the upper bound for ε. For ε satisfying both constraints, the only ﬁxed point of the
system (10.9) is x0 = 0.
It remains to clarify the condition ε > 1/ max
i
ci.
We limit ourselves here to the
following result.
Lemma 10.6 There exists a function w0(f, g) such that for any w ≥w0(f, g), L ≥2w+1,
and ε > ε∗
s the matrix In −εf  (g(x0)) has a negative eigenvalue.
From Lemma 10.6, constraint (10.33), and Deﬁnition 10.3, Theorem 10.4 follows.
We have presented this proof methodology based on Lyapunov systems theory since
we believe it represents a general methodology to treat coupled systems based on well-
known and accepted dynamic systems theory. We note that this approach presented here
in general form, but pioneered in [22, 25, 26], reduces the proof of convergence theorem to
showing the conditions for which (10.1) holds. An example of this application to the case
of iterative demodulation on Gaussian multiple access channels is discussed in [24].

Bibliography
[1] E. Arikan, “Channel polarization:
A method for constructing capacity-achieving
codes for symmetric binary-input memoryless channels,” IEEE Trans. Inform. The-
ory, pp. 879–883, June 2010.
[2] A. Balatsoukas-Stimming, A.J. Raymond, W.J. Gross, and A. Burg, “Hardware archi-
tecture for list successive cancellation decoding of polar codes,” IEEE Trans. Circuits
Systems II, vol. 61, no. 8, pp. 609–613, Aug. 2014.
[3] D.J. Costello, Jr., L. Dolecek, T.E. Fuja, J. Kliewer, D.G.M. Mitchell, and R. Smaran-
dache, “Spatially coupled sparse codes on graphs: Theory and practice,” IEEE Com-
mun. Mag., pp. 168–176, July 2014.
[4] D.J Costello, Jr. and G.D. Forney, “Channel coding: The road to channel capacity,”
Proceedings of the IEEE, vol. 95, no. 6, pp. 1150–1177, June 2007.
[5] D. Divsalar, S. Dolinar, C.R. Jones, and K. Andrews, “Capacity-approaching proto-
graph codes,” IEEE JSAC, vol. 27, no. 6, pp. 876–888, Aug. 2009.
[6] K. Engdahl and K.S. Zigangirov, “On the theory of low density convolutional codes I,”
Probl. Peredachi Inf., vol. 35, no. 4, pp. 295–310, 1999.
[7] A.J. Felstr¨om and K.S. Zigangirov, “Time-varying periodic convolutional codes with
low-density parity-check matrix,” IEEE Trans. Inform. Theory, vol. 45, no. 5,
pp. 2181–2190, Sept. 1999.
[8] W.M. Haddad and V.S. Chellaboina, Nonlinear Dynamical Systems and Control: A
Lyapunov-Based Approach. Princeton University Press, Princeton, NJ, 2008.
[9] R.A. Horn and C.R. Johnson, Matrix Analysis. Cambridge University Press, New
York, 1985.
[10] S. Johnson and G. Lechner, “Spatially coupled repeat-accumulate codes,” IEEE
Comm. Letters, vol. 17, no. 2, pp. 373–376, Feb. 2013.
[11] S. Kudekar, T. Richardson, and R. Urbanke, “Spatially coupled ensembles universally
achieve capacity under belief propagation,” IEEE Trans. Inform. Theory, vol. 59,
no. 12, pp. 7761–7813, Dec. 2013.
491

492
BIBLIOGRAPHY
[12] S. Kudekar, T. Richardson, and R. Urbanke, “Threshold saturation via spatial cou-
pling: Why convolutional LDPC ensembles perform so well over the BEC,” IEEE
Trans. Inform. Theory, vol. 57, no. 2, pp. 803–834, Feb. 2011.
[13] A.M. Lyapunov, The General Problem of the Stability of Motion, Kharkov, Mathe-
matical Society, Kharkov, Russia, 1892.
[14] M. Lentmaier, A. Sridharan, D. Costello, Jr., and K. Zigangirov, “Iterative decod-
ing threshold analysis for LDPC convolutional codes,” IEEE Trans. Inform. Theory,
vol. 56, no. 10, pp. 5274–5289, Oct. 2010.
[15] A.E. Pusane, R. Smarandache, P.O. Vontobel, and D.J. Costello, Jr., “Deriving good
LDPC convolutional codes from LDPC block codes,” IEEE Trans. Inform. Theory,
vol. 57, no. 2, pp. 835–57, Feb. 2011.
[16] A.J. Raymond and W.J. Gross, “A scalable successive-cancellation decoder for polar
codes,” IEEE Trans. Signal Proc., vol. no. 20, pp. 5339–5347, 2014.
[17] M. Rovini and A. Martinez, “On the addition of an input buﬀer to an iterative decoder
for LDPC Codes,” Proc. IEEE 65th VTC2007-Spring, pp. 1995–1999, 2007.
[18] G. Sarkis and W.J. Gross, “Implementation of Polar Decoders,” Advanced Hardware
Design for Error Correcting Codes, P. Coussy and C. Chavet, Springer, New York,
2014.
[19] G. Sarkis, P. Giard, A. Vardy, C. Thibeault, and W.J. Gross, “Fast polar decoders:
Algorithm and implementation,” IEEE J. Selected Areas Commun., vol. 32, no. 5,
pp. 946–957, May 2014.
[20] C. Schlegel and D. Truhachev, “Multiple access eemodulation in the lifted signal graph
with spatial coupling,” IEEE Trans. Inform. Theory, Vol. 59, No. 4, pp. 2459–2470,
2013.
[21] R. Swamy, S. Bates, T.L. Brandon, B.R. Cockburn, D.G. Elliott, J.C. Koob, and
Z. Chen, “Design and test of a 175-Mb/s, rate-1/2 (128,3,6) low-density parity-check
convolutional code encoder and decoder,” IEEE J.
Solid-State Circuits, vol. 42,
no. 10, pp. 2245–2256, Oct. 2007.
[22] K. Takeuchi, T. Tanaka, and T. Kawabata, “A phenomenological study on threshold
improvement via spatial coupling,” Arxiv preprint arXiv:1102.3056, 2011.
[23] R.M. Tanner, D. Sridhara, A. Sridhara, T.E. Fuja, and D.J. Costello, Jr., “LDPC
block and convolutional codes based on circulant matrices,” IEEE Trans. Inf. Theory,
vol. 50, no. 12, pp. 2966–2984, Dec. 2004.
[24] D. Truhachev and C. Schlegel, “Coupling data transmission for capacity-achieving
multiple-access communications,” arXiv:1209.5785.
[25] A. Yedla, Y-Y. Jian, P.S. Nguyen, and H.D. Pﬁster, “A Simple Proof of Threshold
Saturation for Coupled Scalar Recursions,” Arxiv preprint arXiv:1204.5703, 2012.
[26] A. Yedla, Y-Y. Jian, P.S. Nguyen, and H.D. Pﬁster, “A Simple Proof of Threshold
Saturation for Coupled Vector Recursions,” Arxiv preprint arXiv:1208.4080v2, 2012.

Index
M-algorithm, 179, 186
10GBase-T Ethernet, 250
128-DSQ constellation, 103
16-QAM, 76, 79
32-cross, 76, 79
32-point rotated cross signal, 97
3G, 440
3G standards, 440
3GPP, 440
3GPP2, 440
4-dimensional trellis, 89
6-cycle, 287
64-QAM, 76, 79
64-bit IEEE 754 double-precision, 329
absorbing set, 320, 321
absorption
dynamic set, 323
abstract
state, 145
accumulate-repeat-accumulate code, 284
ACE, 334
ACS, 199
adaptive
equalization, 99
add–compare–select, 199
additive noise, 28
additive white Gaussian noise channel
binary-input, 249
advanced hardware architectures, Inc., Pull-
man, WA, 450
Aji, 299
algebra
basic, 139
algorithm
M, 186, 239
T, 188
a posteriori probability, 179
breadth-ﬁrst, 183, 239
constant-log-APP, 210
Euclid, 111
exactness sum-product, 308
Fano, 185
log-App, 207
log-max-APP, 209
stack, 183
Viterbi, 179, 198, 240
analysis
density evolution, 250
anchoring, 479, 487
Anderson, 188
antenna
multiple channels, 48
APP, 179, 373
APP algorithm, 298
APP decoding
approximate, 448
approximate
APP decoding, 448
Approximate Cycle EMD, 334
approximate cycle extrinsic message degree,
332
approximate triangularization, 296
ARA, 284
493
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

494
INDEX
Arikan, 168, 455
ARQ, 6
asymptotic coding, 83
Aulin, 190
autocorrelation function, 41
AWGN, 232
AWGN channel, 269, 276, 476
decoding LDPC, 257
backward recursion, 202
partial, 210
Bahl, 113, 200
bandwidth
eﬃciency, 12, 43
Barnes-Wall, 90
Barnes-Walls lattice, 159
base code, 471
BASE-T channel, 101
baseband
complex equivalent model, 39
basic
algebra, 139
encoder equivalence, 138
equivalent encoding matrix, 143
minimal encoder, 116, 137, 144
basis
orthonormal, 29
vector, 80
waveforms, 29
Bayesian network, 298, 299
Bayes rule, 32
BCJR, 298
BEC, 168, 476
BEC channels
decoding LDPC codes, 260
belief, 304
belief propagation, 298, 466
Berlekamp-Massey, 111
Berrou, 372, 439
turbo code, 445
Bethe free energy, 480
Bhattacharyya
bound, 169
big Viterbi decoder, 351
Biglieri, 222
binary
partition chain, 88
binary entropy, 14
binary erasure channel, 168, 476
error probability evolution, 259
binary superposition, 420
binary symmetric channels, 266
binary-input
additive white Gaussian noise channel,
249
binary-input symmetric-output memoryless
channels, 465
bipartite
graph, 250
bit-interleaved, 418
bit-interleaved modulation, 420
block code
BCH, 21
linear, 251
Reed-Muller, 20
Reed-Solomon, 10
block turbo decoding, 446, 447
Boltzmann’s constant, 19
bound
Bhattacharyya, 169
Cherno↵, 226
random coding, 225
Shannon, 13
square root, 129
transfer-function, 220
union, 220
BPSK capacity limit, 280
branch metric, 198
BSC channel, 267, 269
butterﬂy activation, 457

INDEX
495
cable
copper, 53
twisted-pair copper, 101
cabling
CAT 6 Ethernet, 55
Calderbank, 90
cancelation law, 140
capacity, 7, 52, 465
channel, 231
Gaussian multiple access channel, 422
multi-dimensional, 49
case study
communications system, 53
CAT 6 Ethernet cabling, 55
CCITT, 22
CCITT V.32, 94
CCSDS, 383, 439, 441
Telemetry Channel, 439
CD, 111
cdma2000, 3, 440
chain
binary partition, 88
partition, 77
channel, 28
band-limited, 7
BASE-T, 101
binary erasure, 168
capacity, 231
Gaussian, 7
satellite, 7
test, 409
channels
binary symmetric, 266
capacity, 480
multiple antenna, 48
vector communication, 29
chart
EXIT, 400
Chase Algorithm, 449
check node left degree, 290
check nodes, 252
checkerboard lattice, 81
Cherno↵bound, 226
CMOS, 1
Cocke, 113, 200
code
accumulate-repeat, 284
accumulate-repeat-accumulate, 284
bit-interleaved, 418
construction, 71, 281
convolutional, 72, 115
coset, 85
di↵erential space-time, 417
ensemble, 253
Extended Hamming, 165
generator matrix, 113
Goppa, 111
Hamming, 131
irregular LDPC, 250
linear, 112
linear-trellis, 236
low-density parity-check, 21, 249
maximum free-distance convolutional, 151
minimal trellis convolutional, 133
polar, 166
product, 451
quaternion, 415
Reed–Muller, 162
Reed–Solomon, 111
spatially coupled repeat-accumulate, 474
terminated convolutional, 366
time-varying convolutional, 468
turbo, 249, 351
code design
error ﬂoors, 331
codeword
low-weight, 358
coding
asymptotic, 83
concatenated space-time, 414

496
INDEX
fundamental gain, 90
communication
deep-space, 10
system case study, 53
commutative
ring, 138, 140
compact disc, 111
complex
envelope, 41
equivalent baseband model, 39
complexity
trellis, 120
Viterbi algorithm, 200
concatenated RS-Trellis Code, 453
concatenation
classic, 10
parallel, 11, 418
serial, 11
concentration theorem, 276
concept
ring-algebraic, 142
constellation
128-DSQ, 103
dithered square, 54
constituent decoders, 403
constituent encoders, 403
construction
code, 281
PC-trellis systematic, 127
squaring, 154
twisted squaring, 157
consultative committee for space data sys-
tems, 439
contracted PC-trellis, 130
convergence, 276
analysis, 482
equation, 272
function, irregular codes, 275
convolution
codes, 115
convolutional
interleaver, 437
LDPC, 470
maximum free-distance codes, 151
convolutional code, 72
cascaded, 383
minimal trellis, 133
convolutional LDPC code, 465, 471
Conway, 80, 161
copper
twisted-pair cable, 101
copper cable, 53
correlation
function, 62
receiver, 31
coset code, 85
Costello, 222, 433
coupled dynamical system, 477
coupled-system threshold, 480, 487
coupling
spatial, 465, 466
coupling window W, 475
covering problem, 80
CPM, 16
CRC
code, 459
enabled decoder, 459
criterion
stopping, 304
cross
32-point rotated signal, 97
32, 76, 79
crosstalk
far-end, 59
near-end, 59
Crozier, 434
cubic lattice, 81
cut-set
lower bound, 132
cuto↵rate, 232

INDEX
497
computational, 234
CWEF, 386
cycle reduction, 286
dangling checks, 323
DE, 250
decision
depth, 199
region, 341
decoder
APP, 373, 395, 406
ML, 224
tree, 181
decoding
a posteriori probability, 200
convolutional LDPC codes, 471
LDPC on AWGN channels, 257
maximum likelihood, 197
polar codes, 453
sequential, 21, 183
soft-decision, 112
decoding LDPC codes
BEC channels, 260
decoding threshold, 261, 471
decomposition
singular-value, 49
deep-space, 10
degree
node, 253
degree distribution, 254
delay-free generator, 134
demodulator
product, 40
dense packing, 81
density
evolution, 471
evolution analysis, 250, 273
Gaussian function, 32
power spectral, 31
design rate, 252
DFT, 47
inverse, 47
diagram
trellis, 68
di↵erential 8-PSK, 411
di↵erential modulator, 410
di↵erential-coded modulation, 409
digital video broadcasting, 466
dimension
signal, 12
Diracs delta
function, 31
discrete fourier transform, 47
distance
Euclidean, 155
free, 74, 214
Hamming, 115
increment, 216
spectrum, 214, 356
squared Euclidean, 30
vector Euclidean, 241
distance spectrum, 214
distributive law
generalized, 299
Dithered Relatively Prime, 434
dithered square constellation, 54
diversity, 420
domain
ideal principal, 141
integral, 140
dominant absorption set, 336
doped repeat scramble, 399
DRP, 434
DRP interleaver, 436
DRS, 399
DSB-SC, 40
DSL, 55
DSQ, 54
duo-binary turbo, 442
DVB, 442

498
INDEX
return channel via satellite RCS, 442
DVB-S2, 250, 383, 444
dynamic
absorption set, 323
e↵ective free distance, 370
eﬃciency
bandwidth, 12, 43
eigenvalue, 221
largest eigenvector, 325
eIRA, 331
element
primitive, 287
Elias, 446
EMD, 332
encoder, 79
catastrophic, 136
constituent, 354
feedback component, 369
feedback form, 390
generic trellis, 85
minimal, 134, 146
minimal basic, 116, 137, 144
minimal systematic, 116
non-systematic feedforward, 116, 118
non-systematic form, 116
primitive component, 370
recursive systematic feedback, 118
systematic, 149
systematic feedback, 118
encoders
basic equivalence, 138
encoding
equivalent basic matrix, 143
enhanced SC decoder, 458
ensemble
codes, 253
envelope
complex, 41
equalization
adaptive, 99
equation
convergence, 272
equivalence
basic encoders, 138
basic encoding matrix, 143
generator matrices, 134
equivalent
complex baseband model, 39
equivalent FSM, 241
erasure decoding, 266
error
pair-wise probability, 33
error exponent, 232
error ﬂoor, 319
error ﬂoor region, 259
error ﬂoors
code design, 331
error probability evolution
binary erasure channels, 259
Ethernet, 22
CAT 6 cabling, 55
ETSI, 442
Euclid’s algorithm, 111
Euclidean
space, 29
squared distance, 30
Euclidean distance, 155
vector, 190
European Telecommunications Standards In-
stitute, 442
exactness
sum-product algorithm, 308
excision, 341
EXIT, 353, 417
analysis, 376
analysis serial TTCM, 408
analysis, serially concatenated code, 397
chart, 379, 400, 412
EXIT analysis, 279

INDEX
499
Extended Hamming code, 165
extended irregular repeat accumulate codes,
331
extrinsic
information, 374
information exchange, 353
information principle, 255
information transfer, 376
message degree, 332
factor
invariant theorem, 141
roll-o↵, 43
factor graph, 298, 303, 309
trellises, 305
fading
quasi-static, 415
Fano algorithm, 185
ﬂowchart, 186
far-end crosstalk, 59
fast fading, 417
FDM, 39
FEC, 5
feedback
recursive systematic encoder, 118
systematic encoder, 118
feedforward
non-systematic encoder, 116, 118
Feldstr¨om, 466
FET, 2
FEXT, 59
ﬁnite-state machine, 67
ﬁrst event error probability, 213
ﬁxed point solution, 264
folded spectrum, 37
form
encoder non-systematic, 116
forward recursion, 201
forward-backward, 298
fourier transform, 37
frame error, 441
frame error rate, 319
free distance, 74
asymptote, 359
e↵ective, 370, 391
maximum, 383
maximum convolutional codes, 151
turbo code, 358
FSM, 67, 211
equivalent, 241
function
autocorrelation, 41
conditional weight enumerating, 386
correlation, 62
Diracs delta, 31
Gaussian density, 32
input redundancy weight enumerating,
366
input–output weight enumerating, 386
input–output weight enumerating, 365
function node, 299
function-to-variable messages, 302
fundamental coding gain, 90
fundamental volume, 81
future
subcode, 121
Gallager, 226, 251, 321
exponent, 16
Gallager algorithms, 266
Galois ﬁelds, 287
Gaussian
consistent distribution, 271, 326
Gaussian density function, 32
Gaussian multiple access channel
capacity, 422
general edge, 309
generalized distributive law, 299
generalized modulation, 421
generator

500
INDEX
delay-free, 134
minimum-span matrix, 124
generator matrix, 80
equivalence, 134
generic trellis encoder, 85
geometrically uniform, 215
Gerschgorin cycle theorem, 222
Gilbert cell, 18
girth, 322
Glavieux, 372
global asymptotic stability, 483
Golay, 7
Goppa code, 111
Gosset, 90
Gosset lattice, 158
gradient, 485
Grant, 415
graph
bipartite, 250
graph lifting, 282
graphical function representation, 299
Gray mapping, 75, 451
greatest common divisor, 432
group
Hamilton’s quaternion, 415
GSM, 3
Guinard, 434
H¨older’s inequality, 230
Hadamard transform, 170
Hagenauer, 446
Hamiltion, 415
Hamming, 7, 112
distance, 420
Hamming code, 131, 303, 340
extended, 294
trellis, 121
Hamming distance, 115
Hamming weight, 357
minimum, 123
HDTV, 111
high-deﬁnition television, 111
high-order modulation, 451
high-spread in the interleaver, 434
historical notes, 106
Holder’s inequality, 238
Hughes, 415
ideal, 141
principal, 141
principal domain, 141
IEEE
802.11n, 250
802.16, 452, 453
802.16e, 250
802.3an, 22, 53, 101, 250, 466
802.3an LDPC, 320, 336
803.2 Ethernet, 287
implementation loss, 9
importance sampling, 328, 336, 340
IMT 2000, 3, 440
indicator function, 226
inequality
H¨older’s, 230
Jensen’s, 228
triangle, 240
information
mutual, 167, 169
inner BCH, 444
integers modulo p, 140
integral domain, 140
interference
intersymbol, 53
interference-free
intersymbol signaling, 36
interleaver, 353, 431
3GPP2, 442
block, 437
DRP, 436
linear, 442

INDEX
501
linearized, 432
mother, 442
prime, 442
probabilistic uniform, 365
quadratic, 433
random, 363
rectangular, 361
S-random, 432
spread, 433
International Telecommunications Union, 440
intersymbol
interference-free signaling, 36
intersymbol interference (ISI), 53
invariance
rotational, 92, 93
invariant
factor theorem, 141
inverse
DFT, 47
polynomial right, 137
invertible, 135
IOWEF, 365, 386
irregular
block LDPC codes, 471
LDPC code, 250, 254
irregular code
convergence function, 275
IRWEF, 366
IS, 336
iterative
matrix inversion, 221
iterative decoding, 371
SCCC, 394
iterative system
sparse graphs, 480
Jelinek, 113, 184, 188, 200
Jensen’s inequality, 228, 236
Jet Propulsion Laboratory, 474
Johannesson, 148
JPL, 474
kernel, 122
kissing number, 81
Kronecker power, 166
Lagrange multiplier, 51
Lang, 161
lattice, 80
Barnes-Walls, 159
checkerboard, 81
cubic, 81
Gosset, 158
partition tree, 89
Schl¨aﬂi, 81
trellis, 154
Laurent series, 117, 134
law
cancelation, 140
LDPC, 249
block code ensemble, 477
code graph, 314
convolutional, 470
decoding, 267
decoding convolutional codes, 471
decoding on AWGN channels, 257
encoding of codes, 291
irregular block codes, 471
irregular code, 250, 254
irregular codes, 269
irregular cycle optimized codes, 279
regular, 251
regular coupled codes, 471
repeat-accumulate, 444
reversible codes, 293
RS-based, 287
short-cycle-free codes, 286
specialized codes, 294
thresholds regular codes, 280
triangular codes, 292

502
INDEX
LDPCC, 466
least mean-squares, 60
Leech, 90
lemma
matrix inversion, 244
limit
Shannon, 10
linear
block code, 251
code, 112
linear-trellis code, 236
linearized interleaver, 432
LLR, 208, 256, 376, 394
transformer, 376
LLR check node messages, 312
LLR clippings, 336
LLR update equation, 449
LLR variable node messages, 312
LMS, 60
locally optimal, 314
log-likelihood ratio, 208, 256
Longsta↵, 161
lookup table, 208
loop
phase-locked, 92
low-density parity-check code, 21, 249
low-density parity-check convolutional codes,
467
lower bound
cut-set, 132
LTE, 3
LTE-A, 3
Lyapunov candidate function, 487
Lyapunov functions, 482
Lyapunov theory, 482
m-sequence, 433
machine
ﬁnite-state, 67
MacKay, 249
MAN, 453
MAP, 32, 256, 466
mapping
Gray, 75
natural, 74
Markov chain, 299
Massey, 113, 183
matched ﬁlter, 33, 180
matrix
code generator, 113
equivalent basic encoding, 143
generator, 80
generator equivalence, 134
minimum-span generator, 124
parity-check, 112, 251
matrix inversion lemma, 244
maximal
stopping set, 265
maximum
a posteriori probability, 256
free-distance convolutional codes, 151
maximum a posteriori, 32
maximum a posteriori decoding, 466
maximum likelihood decoding, 197
maximum likelihood receiver, 32
maximum-length shift, 433
McEliece, 120, 299
mean translation, 342
mean-shift importance sampling, 343
message
passing, 255
sequences, 35
metric
branch, 198
partial, 181, 190
MFD, 383
MIMO, 50, 52
MIMO systems, 416
min-sum approximation, 258
minimal

INDEX
503
basic encoder, 116, 137, 144
encoder, 134, 146
realization, 392
systematic encoder, 116
trellis, 120
minimal trellis
convolutional codes, 133
minimum
Hamming weight, 123
minimum-span
generator matrix, 124
mission
Galileo, 21
Mariner, 21
Pioneer, 21
Viking, 21
Voyager, 21
ML receiver, 32
model
complex equivalent baseband, 39
modulation
binary superposition, 420
di↵erental-coded, 409
generalized, 421
trellis-coded, 22, 67, 69
turbo-trellis coded, 402, 403
modulo
p integers, 140
Monte-Carlo simulations, 339
Moore’s law, 2
MPSK, 92
MSGM, 125
multi-dimensional capacity, 49
multiple antenna channels, 48
multiplexing
orthogonal frequency-division, 46
multiplicity, 214
multiplier
Lagrange, 51
mutual information, 167, 169
natural
mapping, 74
Neal, 249
near-end crosstalk, 59
NEXT, 59
node
check, 252
degree, 253
variable, 252
noise
additive, 28
one-sided, 31
Thermal, 28
white, 63
non-systematic
encoder form, 116
feedforward encoder, 116, 118
notes
historical, 106
NP-complete, 311
nullspace, 112
Nyquist, 4, 12
pulse, 37
sampling theorem, 4
Nyquist signals, 445
OFDM, 46
one-sided noise, 31
optimum
receivers, 31
orthogonal frequency-division multiplexing,
46
orthogonality, 31, 36
orthonormal basis, 29
Ostho↵, 193
overbiasing, 346
packing
dense, 81
pair-wise error probability, 33, 214

504
INDEX
pairty-check matrix, 473
PAM, 101, 420
parallel edges, 254
parietal syndrome, 113
parity-check
matrix, 112, 251
trellis, 113
Parseval’s relationships, 42
partial
metric, 190
received sequence, 181
partition
binary chain, 88
chain, 77
lattice tree, 89
set, 75
past
subcode, 121
PC-trellis
contracted, 130
systematic construction, 127
PCM, 6
Pearl, 298
performance, 436
permutation, 253
permutation group, 431
permutation matrices, 281
permuter, 281
Perron–Frobenious theorem, 221
phase-locked loop, 92
photograph, 475
PLL, 92
polar code performance, 458
polar codes, 166, 465
polarization, 169
polynomial
right inverse, 137
positive eigenvector, 489
positive-deﬁnite matrix, 487
potential function, 480
power
Kronecker, 166
power spectral density, 31
precoding
Tomlinson–Harashima, 54, 58
prime, 140
primitive
element, 287
feedback polynomial, 391
principal
ideal, 141
ideal domain, 141
probabilistic uniform interleaver, 365
probability
ﬁrst event error, 213
matrix, 326
pair-wise error, 33, 214
propagation, 298
problem
covering, 80
sphere packing, 80
processor, 473
product
code, 446, 451
demodulator, 40
proto-graph, 281, 470, 471
proto-matrix, 281
PSTN, 98
pulse
Nyquist, 37
spectral raised-cosine, 37
time-orthogonal, 180
Pyndiah, 446
QAM, 79, 420
16, 76, 79
64, 76, 79
QPSK, 7, 29
quadratic encoding complexity, 256
quadratic interleaver, 433

INDEX
505
quadrature double side-band suppressed car-
rier, 40
quadrature signal, 40
quaternion code, 415
RA, 289, 400, 445
random coding
analysis optimal decoding, 223
sequential decoding analysis, 233
random interleaver, 363
rank, 252
rate
symbol, 35
ratio
log-likelihood, 256
Raviv, 113, 200
receiver
correlator, 31
maximum-likelihood, 32, 180
ML, 32
optimum, 31
rectangular array, 431
rectangular interleaver, 361
recursive
systematic feedback encoder, 118
Reed–Muller code, 162, 453
Reed–Solomon code, 111
Reed-Muller, 465
regions
Voronoi, 32
regular, 215
coupled LDPC codes, 471
regular LDPC, 251
Rei↵en, 183
relationship
Parseval, 42
repeat-accumulate, 400
code, 284, 289, 445
LDPC, 444
return barrier, 193
Richardson, 347
ring
algebraic concepts, 142
commutative, 138, 140
Robertson, 402
roll-o↵factor, 43
ROM, 208
root-Nyquist signaling, 39
rotational invariance, 92, 93
Rouanne, 222
RS, 10
RS-based
LDPC, 287
rule
Bayes, 32
de lHˆopital, 231
S-random interleaver, 432
SCCC, 383
iterative decoding, 394
weight enumerator, 385
schedule, 302
Schl¨aﬂi lattice, 81
Schlegel, 327, 415
scrambler, 138
self-concatenated
turbo encoder, 444
sequences
message, 35
sequential decoding, 183, 234
serial concatenation, 383, 406
convolutional codes, 383
serial TTCM
EXIT analysis, 408
series
Laurent, 117, 134
set
span, 126
set partition, 75
Shannon, 7

506
INDEX
bound, 13
capacity, 20
limit, 249
shaping, 85
short cycles, 286
short-cycle-free LDPC codes, 286
signal
32-point rotated cross, 97
dimension, 12
quadrature, 40
signal-to-noise ratio, 7
pinch-o↵, 381
signaling
intersymbol interference-free, 36
root-Nyquist, 39
singular values, 49
singular-value decomposition, 49
SISO, 200
size
state, 133
sliding window, 210
Sloane, 80, 90, 161
sockets, 253
soft-decision decoding, 112
space
Euclidean, 29
state, 121
space data system standard, 439
space-time coding, 414
span
set, 126
sparse, 251
sparse graph
iterative system, 480
sparse graphical codes, 465
spatial coupling, 465, 466, 474, 482
spectral raised-cosine pulse, 37
spectrum
folded, 37
sphere
packing problem, 80
spread interleaver, 433
spreading factor, 370
square
twisted construction, 157
square root
bound, 129
squares
least mean, 60
squaring
construction, 154
stable ﬁxed point, 478, 482
stack algorithm, 183
standard, 439
CCSDS, 383
DVB-S2, 383
state
abstract, 145
space, 121
state size, 133
statistics
suﬃcient, 34
stopping
criterion, 304
stopping set, 321
maximal, 265
stopping set S, 265
subcode
future, 121
past, 121
sublattice, 83
subsets, 77
subthreshold, 18
successive cancelation, 171
successive cancelation algorithm, 458
suﬃcient statistics, 34
sum-product
exactness algorithm, 308
sumproduct algorithm, 302
SVD, 49

INDEX
507
symbol
rate, 35
syndrome
partial, 113
system
communications case study, 53
systematic
construction PC-trellis, 127
encoder, 149
feedback encoder, 118
minimal encoder, 116
recursive feedback encoder, 118
tail-biting trellis, 129, 131
Takeshita, 433
Tanner, 298
Tanner graph, 252, 322
TCM, 22, 67
television
high-deﬁnition, 111
test channel, 409
theorem
Gerschgorin, 222
Gerschgorin’s cycle, 240
invariant factor, 141
non-optimality, 197
Perron–Frobenious, 221
waterﬁlling, 50
Thermal noise, 28
thin spectrum, 432
third generation, 440
THP, 54
threshold, 259, 329
threshold saturation, 466, 482
thresholds
regular LDPC codes, 280
time-orthogonal pulse, 180
time-varying convolutional code, 468
Tomlinson–Harashima precoding, 54, 58
transfer-function bound, 220
transform
Hadamard, 170
transition matrix, 217
trapping sets of LDPC, 347
traveling wave tube, 16
tree decoder, 181
tree search
depth-ﬁrst, 185
exhaustive, 197
trellis, 115
4-dimensional, 89
coded modulation, 69
complexity, 120
diagram, 68
factor graph, 305
generic encoder, 85
Hamming code, 121
lattice, 154
minimal, 120
minimal convolutional codes, 133
parity-check, 113
tail-biting, 129, 131
trellis-based APP decoding, 448
trellis-coded modulation, 67
triangle inequality, 240
truncation length, 199
TTCM, 402
turbo cli↵, 381
turbo code, 249
BER bound, 358
distance spectrum, 356
free distance, 358
graph, 314
iterative decoding, 371
original, 352
standards, 439
weight enumerator, 364
turbo codes, 351
turbo encoder
self-concatenated, 444

508
INDEX
turbo product code, 453
turbo-trellis coded modulation, 402
twisted squaring construction, 157
twisted-pair cabale modems, 466
twisted-pair copper cable, 101
Ungerb¨ock , 67
uniformity, 222
union bound, 214
unstable ﬁxed point, 478, 482
UTI, 440
V.32, 96
V.33, 96
V.34, 22
V.90, 100
V.fast, 98, 99
variable gradient method, 485
variable nodes, 252
variable-to-function messages, 302
variance scaling, 341
vector
basis, 80
communication channels, 29
Euclidean distance, 190, 241
Viterbi, 198, 226
algorithm, 179
big decoder, 351
Viterbi algorithm, 198
complexity, 200
voiceband modem, 22
volume
fundamental, 81
Voronoi, 80
Voronoi regions, 32
W¨orz, 402
Wan, 148
waterﬁlling theorem, 50
waveforms
basis, 29
weight enumerator, 364
white noise, 63
Wiberg, 298
WiFi, 466
WiMAX, 250, 446, 466
Wolf, 113, 222
Wozencraft, 183
Yang, 331
Zehavi, 222
zero-state, 212
Zhang, 75, 327
Zigangirov, 184, 466

IEEE PRESS SERIES ON 
DIGITAL AND MOBILE COMMUNICATION
John B. Anderson, Series Editor
University of Lund
1. Wireless Video Communications: Second to Third Generation and Beyond
Lajos Hanzo, Peter J. Cherriman, and Jurgen Streit
2. Wireless Communications in the 2lst Century
Mansoor Sharif, Shigeaki Ogose, and Takeshi Hattori
3. Introduction to WLLs: Application and Deployment for Fixed and 
Broadband Services
Raj Pandya
4. Trellis and Turbo Coding
Christian B. Schlegel and Lance C. Perez
5. Theory of Code Division Multiple Access Communication
Kamil Sh. Zigangirov
6. Digital Transmission Engineering, Second Edition
John B. Anderson
7. Wireless Broadband: Conflict and Convergence
Vern Fotheringham and Shamla Chetan
8. Wireless LAN Radios: System Definition to Transistor Design
Arya Behzad
9. Millimeter Wave Communication Systems
Kao-Cheng Huang and Zhaocheng Wang
10. Channel Equalization for Wireless Communications: From Concepts to 
Detailed Mathematics
Gregory E. Bottomley
11. Handbook of Position Location: Theory, Practice, and Advances
Edited by Seyed (Reza) Zekavat and R. Michael Buehrer
12. Digital Filters: Principle and Applications with MATLAB
Fred J. Taylor
13. Resource Allocation in Uplink OFDMA Wireless Systems: Optimal 
Solutions and Practical Implementations
Elias E. Yaacoub and Zaher Dawy
14. Non-Gaussian Statistical Communication Theory
David Middleton
15. Frequency Stabilization: Introduction and Applications
Venceslav F. Kroupa
16. Mobile Ad Hoc Networking: Cutting Edge Directions, Second Edition
Stefano Basagni, Marco Conti, Silvia Giordano, and Ivan Stojmenovic
17. Techniques for Surviving the Mobile Data Explosion
Dinesh Chandra Verma and Paridhi Verma
Trellis And Turbo Coding: Iterative and Graph-Based Error Control Coding, Second Edition 
By Christian B. Schlegel and Lance C. Pérez 
Copyright © 2015 by The Institute of Electrical and Electronics Engineers, Inc. 

18. Cellular Communications: A Comprehensive and Practical Guide
Nishith D. Tripathi and Jeffrey H. Reed
19. Fundamentals of Convolutional Coding, Second Edition
Rolf Johannesson and Kamil Sh. Zigangirov
20. Trellis and Turbo Coding, Second Edition
Christian B. Schlegel and Lance C. Perez

WILEY END USER LICENSE
AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook
EULA.


Machine Learning Specialization 
Boosting 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
2 
Simple (weak) classiﬁers are good! 
©2015-2016 Emily Fox & Carlos Guestrin 
Logistic  
regression  
w. simple  
features 
Low variance.  Learning is fast! 
But high bias… 
Shallow  
decision trees 
Decision 
stumps 
Income>$100K? 
Safe 
Risky 
No 
Yes 

Machine Learning Specialization 
3 
Finding a classiﬁer that’s just right 
©2015-2016 Emily Fox & Carlos Guestrin 
Model complexity 
 
Classiﬁcation 
error 
train error 
true error 
Weak  
learner 
Need  
stronger 
learner 
Option 1:  add more features or depth 
Option 2: ????? 

Machine Learning Specialization 
4 
Boosting question 
“Can a set of weak learners be combined to  
create a stronger learner?” Kearns and Valiant (1988) 
Yes! Schapire (1990) 
Boosting 
Amazing impact:  ! simple approach  ! widely used in 
industry  ! wins most Kaggle competitions 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Ensemble classiﬁer 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
6 
A single classiﬁer 
Output: ŷ = f(x) 
-  Either +1 or -1 
©2015-2016 Emily Fox & Carlos Guestrin 
Input: x 
Classiﬁer 
Income>$100K? 
Safe 
Risky 
No 
Yes 

Machine Learning Specialization 
7 
©2015-2016 Emily Fox & Carlos Guestrin 
Income>$100K? 
Safe 
Risky 
No 
Yes 

Machine Learning Specialization 
8 
©2015-2016 Emily Fox & Carlos Guestrin 
Ensemble methods: Each classiﬁer “votes” on prediction 
xi = (Income=$120K, Credit=Bad, Savings=$50K, Market=Good)   
f1(xi) = +1  
Combine? 
F(xi) = sign(w1 f1(xi) + w2 f2(xi) + w3 f3(xi) + w4 f4(xi))  
Ensemble 
model 
Learn coeﬃcients 
Income>$100K? 
Safe 
Risky 
No 
Yes 
Credit history? 
Risky 
Safe 
Good 
Bad 
Savings>$100K? 
Safe 
Risky 
No 
Yes 
Market conditions? 
Risky 
Safe 
Good 
Bad 
Income>$100K? 
Safe 
Risky 
No 
Yes 
f2(xi) = -1  
Credit history? 
Risky 
Safe 
Good 
Bad 
f3(xi) = -1  
Savings>$100K? 
Safe 
Risky 
No 
Yes 
f4(xi) = +1  
Market conditions? 
Risky 
Safe 
Good 
Bad 

Machine Learning Specialization 
10 
©2015-2016 Emily Fox & Carlos Guestrin 
Prediction with ensemble 
w1 
2 
w2 
1.5 
w3 
1.5 
w4 
0.5 
f1(xi) = +1  
Combine? 
F(xi) = sign(w1 f1(xi) + w2 f2(xi) + w3 f3(xi) + w4 f4(xi))  
Ensemble 
model 
Learn coeﬃcients 
Income>$100K? 
Safe 
Risky 
No 
Yes 
Credit history? 
Risky 
Safe 
Good 
Bad 
Savings>$100K? 
Safe 
Risky 
No 
Yes 
Market conditions? 
Risky 
Safe 
Good 
Bad 
Income>$100K? 
Safe 
Risky 
No 
Yes 
f2(xi) = -1  
Credit history? 
Risky 
Safe 
Good 
Bad 
f3(xi) = -1  
Savings>$100K? 
Safe 
Risky 
No 
Yes 
f4(xi) = +1  
Market conditions? 
Risky 
Safe 
Good 
Bad 

Machine Learning Specialization 
12 
Ensemble classiﬁer in general 
•  Goal:  
- Predict output y   
•  Either +1 or -1 
- From input x  
•  Learn ensemble model: 
- Classiﬁers: f1(x),f2(x),…,fT(x) 
- Coeﬃcients: ŵ1,ŵ2,…,ŵT 
•  Prediction: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!

Machine Learning Specialization 
Boosting 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
14 
Training a classiﬁer 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
data 
Learn  
classiﬁer 
Predict 
ŷ = sign(f(x)) 
f(x) 

Machine Learning Specialization 
15 
Learning decision stump 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Income 
y 
A 
$130K 
Safe 
B 
$80K 
Risky 
C 
$110K 
Risky 
A 
$110K 
Safe 
A 
$90K 
Safe 
B 
$120K 
Safe 
C 
$30K 
Risky 
C 
$60K 
Risky 
B 
$95K 
Safe 
A 
$60K 
Safe 
A 
$98K 
Safe 
Credit 
Income 
y 
A 
$130K 
Safe 
B 
$80K 
Risky 
C 
$110K 
Risky 
A 
$110K 
Safe 
A 
$90K 
Safe 
B 
$120K 
Safe 
C 
$30K 
Risky 
C 
$60K 
Risky 
B 
$95K 
Safe 
A 
$60K 
Safe 
A 
$98K 
Safe 
Income? 
> $100K 
 
≤ $100K 
ŷ = Safe 
ŷ = Safe 
3 
1 
4 
3 

Machine Learning Specialization 
16 
Boosting = Focus learning on “hard” points 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
data 
Learn  
classiﬁer 
Predict 
ŷ = sign(f(x)) 
f(x) 
Learn where f(x)  
makes mistakes 
Evaluate 
Boosting: focus next 
classiﬁer on places 
where f(x) does less well  

Machine Learning Specialization 
17 
Learning on weighted data: 
More weight on “hard” or more important points 
•  Weighted dataset: 
- Each xi,yi weighted by αi 
•  More important point = higher weight αi 
•  Learning: 
- Data point j counts as αi data points 
•  E.g., αi = 2 è count point twice  
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
18 
Credit 
Income 
y 
A 
$130K 
Safe 
B 
$80K 
Risky 
C 
$110K 
Risky 
A 
$110K 
Safe 
A 
$90K 
Safe 
B 
$120K 
Safe 
C 
$30K 
Risky 
C 
$60K 
Risky 
B 
$95K 
Safe 
A 
$60K 
Safe 
A 
$98K 
Safe 
Learning a decision stump on weighted data 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit Income 
y 
Weight α 
A 
$130K 
Safe 
0.5 
B 
$80K 
Risky 
1.5 
C 
$110K 
Risky 
1.2 
A 
$110K 
Safe 
0.8 
A 
$90K 
Safe 
0.6 
B 
$120K 
Safe 
0.7 
C 
$30K 
Risky 
3 
C 
$60K 
Risky 
2 
B 
$95K 
Safe 
0.8 
A 
$60K 
Safe 
0.7 
A 
$98K 
Safe 
0.9 
Income? 
> $100K 
 
≤ $100K 
ŷ = Risky 
ŷ = Safe 
2 
1.2 
3 
6.5 
Increase weight α of harder/
misclassiﬁed points  

Machine Learning Specialization 
19 
Learning from weighted data in general 
•  Usually, learning from weighted data 
- Data point i counts as αi data points 
•  E.g., gradient ascent for logistic  
regression: 
©2015-2016 Emily Fox & Carlos Guestrin 
w(t+1)
j
 w(t)
j
+ ⌘
N
X
i=1
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘
αi 
Sum over data points 
Weigh each point by αi 
 

Machine Learning Specialization 
20 
Boosting = Greedy learning ensembles from data 
©2015-2016 Emily Fox & Carlos Guestrin 
Training data 
Predict  ŷ = sign(f1(x)) 
Learn classiﬁer 
f1(x) 
Weighted data 
Learn classiﬁer & coeﬃcient 
ŵ,f2(x) 
Predict  ŷ = sign(ŵ1 f1(x) + ŵ2 f2(x)) 
Higher weight 
for points where 
f1(x) is wrong  

Machine Learning Specialization 
AdaBoost 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
23 
AdaBoost: learning ensemble  
[Freund & Schapire 1999]  
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x) with data weights αi 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!

Machine Learning Specialization 
Computing coeﬃcient ŵt   
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
26 
AdaBoost: Computing coeﬃcient ŵt of classiﬁer ft(x) 
•  ft(x) is good è ft has low training error 
•  Measuring error in weighted data? 
- Just weighted # of misclassiﬁed points 
©2015-2016 Emily Fox & Carlos Guestrin 
Is ft(x) good? 
ŵt large 
Yes 
ŵt small 
No 

Machine Learning Specialization 
28 
Data point 
 
 
Weighted classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
(Sushi was great,      ,α=1.2) 
Learned classiﬁer 
Hide label 
Weight of correct 
Weight of mistakes 
Sushi was great 
ŷ = 
Correct! 
0 
0 
1.2 
0.5 
(Food was OK,         ,α=0.5) 
Food was OK 
Mistake! 
α=1.2 
α=0.5 

Machine Learning Specialization 
29 
Weighted classiﬁcation error 
•  Total weight of  mistakes: 
•  Total weight of all points: 
•  Weighted error measures fraction of weight of mistakes: 
- Best possible value is 0.0  
©2015-2016 Emily Fox & Carlos Guestrin 
weighted_error =                                          . 
 

Machine Learning Specialization 
30 
AdaBoost: Formula for computing  
coeﬃcient ŵt of classiﬁer ft(x) 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆwt = 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆
weighted_error(ft) 
on training data 
 
 
ŵt  
Is ft(x) good? 
Yes 
No 
1 −weighted error(ft)
weighted error(ft)

Machine Learning Specialization 
31 
AdaBoost: learning ensemble 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x) with data weights αi 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!
ˆwt = 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆

Machine Learning Specialization 
Recompute weights αi 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
34 
AdaBoost: Updating weights αi based on where  
classiﬁer ft(x) makes mistakes  
©2015-2016 Emily Fox & Carlos Guestrin 
Did ft get xi right? 
Decrease αi 
Yes 
Increase αi 
No 

Machine Learning Specialization 
36 
AdaBoost: Formula for  
updating weights αi 
©2015-2016 Emily Fox & Carlos Guestrin 
ft(xi)=yi ? 
ŵt  
Multiply αi by 
Implication 
Did ft get xi right? 
Yes 
No 
αi   ç 
αi e    ,  if ft(xi)=yi  
-ŵt 
αi e   ,   if ft(xi)≠yi  
ŵt 

Machine Learning Specialization 
37 
AdaBoost: learning ensemble 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x) with data weights αi 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!
ˆwt = 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆
αi   ç 
αi e    ,  if ft(xi)=yi  
-ŵt 
αi e   ,   if ft(xi)≠yi  
ŵt 

Machine Learning Specialization 
39 
AdaBoost: Normalizing weights αi 
©2015-2016 Emily Fox & Carlos Guestrin 
If xi often mistake, 
weight αi gets very 
large 
If xi often correct, 
weight αi gets very 
small 
Can cause numerical instability  
after many iterations 
Normalize weights to  
add up to 1 after every iteration 
 
 
↵i 
↵i
PN
j=1 ↵j

Machine Learning Specialization 
40 
AdaBoost: learning ensemble 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x) with data weights αi 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
- Normalize weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!
ˆwt = 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆
αi   ç 
αi e    ,  if ft(xi)=yi  
-ŵt 
αi e   ,   if ft(xi)≠yi  
ŵt 
↵i 
↵i
PN
j=1 ↵j

Machine Learning Specialization 
AdaBoost example 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
43 
t=1: Just learn a classiﬁer on original data 
©2015-2016 Emily Fox & Carlos Guestrin 
Learned decision stump f1(x) 
Original data 

Machine Learning Specialization 
44 
Updating weights αi  
©2015-2016 Emily Fox & Carlos Guestrin 
Learned decision stump f1(x) 
New data weights αi  
Boundary 
Increase weight αi  
of misclassiﬁed points 

Machine Learning Specialization 
45 
t=2: Learn classiﬁer on weighted data 
Learned decision stump f2(x) 
on weighted data 
Weighted data: using αi 
chosen in previous iteration 
f1(x) 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
46 
Ensemble becomes weighted  
sum of learned classiﬁers 
©2015-2016 Emily Fox & Carlos Guestrin 
= 
f1(x) 
f2(x) 
0.61 
ŵ1 
+ 0.53 
ŵ2 

Machine Learning Specialization 
47 
©2015-2016 Emily Fox & Carlos Guestrin 
Decision boundary of ensemble classiﬁer  
after 30 iterations 
training_error = 0 

Machine Learning Specialization 
AdaBoost summary 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
50 
AdaBoost: learning ensemble 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x) with data weights αi 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
- Normalize weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!
ˆwt = 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆
αi   ç 
αi e    ,  if ft(xi)=yi  
-ŵt 
αi e   ,   if ft(xi)≠yi  
ŵt 
↵i 
↵i
PN
j=1 ↵j

Machine Learning Specialization 
Boosted decision stumps 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
52 
Boosted decision stumps 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x): pick decision stump with lowest  
weighted training error according to αi 
 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
- Normalize weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!
 
 
 
 
 

Machine Learning Specialization 
53 
Finding best next decision stump ft(x) 
©2015-2016 Emily Fox & Carlos Guestrin 
Consider splitting on each feature: 
weighted_error  
= 0.2  
weighted_error  
= 0.35  
weighted_error  
= 0.3  
weighted_error  
= 0.4  
= 1
2 ln
✓1 −weighted error(ft)
weighted error(ft)
◆= 0.69  
ŵt 
Income>$100K? 
Safe 
Risky 
No 
Yes 
Credit history? 
Risky 
Safe 
Good 
Bad 
Savings>$100K? 
Safe 
Risky 
No 
Yes 
Market conditions? 
Risky 
Safe 
Good 
Bad 
ft  = 
Income>$100K? 
Safe 
Risky 
No 
Yes 

Machine Learning Specialization 
54 
Boosted decision stumps 
•  Start same weight for all points: αi = 1/N   
•  For t = 1,…,T 
- Learn ft(x): pick decision stump with lowest  
weighted training error according to αi 
 
- Compute coeﬃcient ŵt  
- Recompute weights αi  
- Normalize weights αi  
•  Final model predicts by: 
©2015-2016 Emily Fox & Carlos Guestrin 
ˆy = sign
 T
X
t=1
ˆwtft(x)
!

Machine Learning Specialization 
55 
Updating weights αi 
©2015-2016 Emily Fox & Carlos Guestrin 
= αi e-0.69 = αi /2 
= αi e0.69  =  2 αi 
, if ft(xi)=yi  
, if ft(xi)≠yi  
 αi ç 
αi e 
-ŵt 
αi e 
ŵt 
Credit 
Income 
y 
A 
$130K 
Safe 
B 
$80K 
Risky 
C 
$110K 
Risky 
A 
$110K 
Safe 
A 
$90K 
Safe 
B 
$120K 
Safe 
C 
$30K 
Risky 
C 
$60K 
Risky 
B 
$95K 
Safe 
A 
$60K 
Safe 
A 
$98K 
Safe 
Credit 
Income 
y 
ŷ 
A 
$130K 
Safe 
Safe 
B 
$80K 
Risky 
Risky 
C 
$110K 
Risky 
Safe 
A 
$110K 
Safe 
Safe 
A 
$90K 
Safe 
Risky 
B 
$120K 
Safe 
Safe 
C 
$30K 
Risky 
Risky 
C 
$60K 
Risky 
Risky 
B 
$95K 
Safe 
Risky 
A 
$60K 
Safe 
Risky 
A 
$98K 
Safe 
Risky 
Credit 
Income 
y 
ŷ 
Previous 
weight α 
New 
weight α 
A 
$130K 
Safe 
Safe 
0.5 
B 
$80K 
Risky 
Risky 
1.5 
C 
$110K 
Risky 
Safe 
1.5 
A 
$110K 
Safe 
Safe 
2 
A 
$90K 
Safe 
Risky 
1 
B 
$120K 
Safe 
Safe 
2.5 
C 
$30K 
Risky 
Risky 
3 
C 
$60K 
Risky 
Risky 
2 
B 
$95K 
Safe 
Risky 
0.5 
A 
$60K 
Safe 
Risky 
1 
A 
$98K 
Safe 
Risky 
0.5 
Credit 
Income 
y 
ŷ 
Previous 
weight α 
New 
weight α 
A 
$130K 
Safe 
Safe 
0.5 
0.5/2 = 0.25 
B 
$80K 
Risky 
Risky 
1.5 
0.75 
C 
$110K 
Risky 
Safe 
1.5 
2 * 1.5 = 3 
A 
$110K 
Safe 
Safe 
2 
1 
A 
$90K 
Safe 
Risky 
1 
2 
B 
$120K 
Safe 
Safe 
2.5 
1.25 
C 
$30K 
Risky 
Risky 
3 
1.5 
C 
$60K 
Risky 
Risky 
2 
1 
B 
$95K 
Safe 
Risky 
0.5 
1 
A 
$60K 
Safe 
Risky 
1 
2 
A 
$98K 
Safe 
Risky 
0.5 
1 
Income>$100K? 
Safe 
Risky 
No 
Yes 

Machine Learning Specialization 
Boosting convergence & overﬁtting 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
57 
Boosting question revisited 
“Can a set of weak learners be combined to  
create a stronger learner?” Kearns and Valiant (1988) 
Yes! Schapire (1990) 
Boosting 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
58 
After some iterations,  
training error of boosting goes to zero!!! 
©2015-2016 Emily Fox & Carlos Guestrin 
Training error 
Iterations of boosting 
Boosted decision stumps on toy dataset 
Training error of ensemble of  
30 decision stumps = 0% 
Training error of  
1 decision stump = 22.5% 

Machine Learning Specialization 
60 
AdaBoost Theorem 
Under some technical conditions…  
Training error of  
boosted classiﬁer → 0  
as T→∞ 
©2015-2016 Emily Fox & Carlos Guestrin 
Training error 
Iterations of boosting 
May oscillate a bit 
But will  
generally decrease, & 
eventually become 0! 

Machine Learning Specialization 
61 
Condition of AdaBoost Theorem 
Under some technical conditions…  
Training error of  
boosted classiﬁer → 0  
as T→∞ 
©2015-2016 Emily Fox & Carlos Guestrin 
Extreme example: 
No classiﬁer can  
separate a +1  
on top of -1 
Condition = At every t,  
can ﬁnd a weak learner with 
weighted_error(ft) < 0.5 
Not always 
possible 
Nonetheless, boosting often 
yields great training error  

Machine Learning Specialization 
62 
©2015-2016 Emily Fox & Carlos Guestrin 
Boosted decision stumps on loan data 
Decision trees on loan data 
39% test error 
8% training error 
Overﬁtting 
32% test error 
28.5% training error 
Better ﬁt & lower test error 

Machine Learning Specialization 
63 
Boosting tends to be robust to overﬁtting 
©2015-2016 Emily Fox & Carlos Guestrin 
Test set performance about  
constant for many iterations  
è Less sensitive to choice of T 

Machine Learning Specialization 
65 
But boosting will eventually overﬁt,  
so must choose max number of components T 
©2015-2016 Emily Fox & Carlos Guestrin 
Best test error around 31% 
Test error eventually 
increases to 33% (overﬁts) 

Machine Learning Specialization 
66 
How do we decide when to stop boosting? 
Choosing T ? 
Not on 
training data 
Never ever 
ever ever on  
test data 
Validation 
set 
Cross-
validation 
Like selecting parameters in other ML 
approaches (e.g., λ in regularization) 
©2015-2016 Emily Fox & Carlos Guestrin 
Not useful: training  
error improves  
as T increases 
If dataset is large 
For smaller data 

Machine Learning Specialization 
Summary of boosting 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
68 
Variants of boosting and related algorithms 
©2015-2016 Emily Fox & Carlos Guestrin 
There are hundreds of variants of boosting, most important: 
Many other approaches to learn ensembles, most important: 
•  Like AdaBoost, but useful beyond 
basic classiﬁcation 
Gradient 
boosting 
•  Bagging: Pick random subsets of the data 
 
 - Learn a tree in each subset 
 - Average predictions 
•  Simpler than boosting & easier to parallelize  
•  Typically higher error than boosting for same 
number of trees (# iterations T) 
Random 
forests 

Machine Learning Specialization 
69 
Impact of boosting  
(spoiler alert... HUGE IMPACT) 
• Standard approach for face 
detection, for example 
Extremely useful in  
computer vision 
• Malware classiﬁcation, credit 
fraud detection, ads click through 
rate estimation, sales forecasting, 
ranking webpages for search, 
Higgs boson detection,… 
Used by most winners of  
ML competitions  
(Kaggle, KDD Cup,…)  
• Coeﬃcients chosen manually, 
with boosting, with bagging,  
or others 
Most deployed ML systems 
use model ensembles 
©2015-2016 Emily Fox & Carlos Guestrin 
Amongst most useful   
ML methods ever created 

Machine Learning Specialization 
70 
What you can do now… 
•  Identify notion ensemble classiﬁers 
•  Formalize ensembles as the weighted 
combination of simpler classiﬁers 
•  Outline the boosting framework –  
sequentially learn classiﬁers on weighted data 
•  Describe the AdaBoost algorithm 
-  Learn each classiﬁer on weighted data 
-  Compute coeﬃcient of classiﬁer 
-  Recompute data weights 
-  Normalize weights 
•  Implement AdaBoost to create an ensemble of 
decision stumps 
•  Discuss convergence properties of AdaBoost & 
how to pick the maximum number of iterations T 
©2015-2016 Emily Fox & Carlos Guestrin 


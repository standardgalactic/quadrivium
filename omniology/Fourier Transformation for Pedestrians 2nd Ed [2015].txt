Undergraduate Lecture Notes in Physics
Tilman Butz
Fourier 
Transformation 
for Pedestrians
 Second Edition 

Undergraduate Lecture Notes in Physics

Undergraduate Lecture Notes in Physics (ULNP) publishes authoritative texts covering
topics throughout pure and applied physics. Each title in the series is suitable as a basis for
undergraduate instruction, typically containing practice problems, worked examples, chapter
summaries, and suggestions for further reading.
ULNP titles must provide at least one of the following:
• An exceptionally clear and concise treatment of a standard undergraduate subject.
• A solid undergraduate-level introduction to a graduate, advanced, or non-standard subject.
• A novel perspective or an unusual approach to teaching a subject.
ULNP especially encourages new, original, and idiosyncratic approaches to physics teaching
at the undergraduate level.
The purpose of ULNP is to provide intriguing, absorbing books that will continue to be the
reader’s preferred reference throughout their academic career.
Series editors
Neil Ashby
Professor Emeritus, University of Colorado, Boulder, CO, USA
William Brantley
Professor, Furman University, Greenville, SC, USA
Michael Fowler
Professor, University of Virginia, Charlottesville, VA, USA
Morten Hjorth-Jensen
Oslo, Norway
Michael Inglis
Professor, SUNY Suffolk County Community College, Selden, NY, USA
Heinz Klose
Professor Emeritus, Humboldt University Berlin, Germany
Helmy Sherif
Professor, University of Alberta, Edmonton, AB, Canada
More information about this series at http://www.springer.com/series/8917

Tilman Butz
Fourier Transformation
for Pedestrians
Second Edition
123

Tilman Butz
Faculty of Physics and Earth Science
University of Leipzig
Leipzig
Germany
ISSN 2192-4791
ISSN 2192-4805
(electronic)
Undergraduate Lecture Notes in Physics
ISBN 978-3-319-16984-2
ISBN 978-3-319-16985-9
(eBook)
DOI 10.1007/978-3-319-16985-9
Library of Congress Control Number: 2015935732
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2006, 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained
herein or for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)

To Renate, Raphaela, and Florentin

Preface
Fourier1 Transformation for Pedestrians. For pedestrians? Harry J. Lipkin’s famous
“Beta-decay for Pedestrians” [1], was an inspiration to me, so that’s why. Harry’s
book explains physical problems as complicated as helicity and parity violation to
“pedestrians” in an easy to understand way. Discrete Fourier transformation, by
contrast, only requires elementary algebra, something any student should be
familiar with. As the algorithm2 is a linear one, this should present no pitfalls and
should be as “easy as pie”. In spite of that, stubborn prejudices prevail, as far as
Fourier transformations are concerned, viz., that information could get lost or that
you could end up trusting a hoax; anyway, who would trust something that is all
done with “smoke and mirrors”. The above prejudices are often caused by negative
experiences, gained through improper use of ready-made Fourier transformation
programs or hardware.
This book is for all who, being laypersons—or pedestrians—are looking for a
gentle and also humorous introduction to the application of Fourier transformation,
without hitting too much theory, proofs of existence, and similar things. It is
appropriate for science students at technical colleges and universities, but also for
“mere” computer-freaks. It is also quite adequate for students of engineering and all
practical people working with Fourier transformations. Basic knowledge of inte-
gration, however, is recommended.
If this book can help to avoid prejudices or even do away with them, writing it
has been well worthwhile. Here we show how things “work”. Generally we discuss
the Fourier transformation in one dimension only. Chapter 1 introduces Fourier
series and, as part and parcel, important statements and theorems that will guide us
through the whole book. As is appropriate for pedestrians, we also cover all the
“pits and pitfalls” on the way. Chapter 2 covers continuous Fourier transformations
in great detail. Window functions are dealt with in Chap. 3 in more detail, as
1Jean Baptiste Joseph Fourier (1768–1830), French mathematician and physicist.
2Integration and differentiation are linear operators. This is quite obvious in the discrete version
(Chap. 4) and is, of course, also valid when passing on to the continuous form.
vii

understanding them is essential to avoid the disappointment caused by false
expectations. Chapter 4 is about discrete Fourier transformations, with special
regard to the Cooley–Tukey algorithm (Fast Fourier Transform, FFT). Finally,
Chap. 5 will introduce some useful examples for the ﬁltering effects of simple
algorithms. From the host of available material we only pick items that are relevant
to the recording and preprocessing of data, items that are often used without even
thinking about them. This book started as a manuscript for lectures at the Technical
University of Munich and at the University of Leipzig. That is why it is very much
a textbook and contains many worked examples—to be redone “manually”—as
well as plenty of illustrations. To show that a textbook (originally) written in
German can also be amusing and humorous, was my genuine concern, because
dedication and assiduity of their own are quite inclined to stiﬂe creativity and
imagination. It should also be fun and boost our innate urge to play. The two books
“Applications of Discrete and Continuous Fourier Analysis” [2] and “Theory of
Discrete and Continuous Fourier Analysis” [3] had considerable inﬂuence on the
makeup and content of this book, and are to be recommended as additional reading
for those “keen on theory”.
This English edition is based on the third, enlarged edition in German [4]. In
contrast to this German edition, there are now problems at the end of each chapter.
They should be worked out before going to the next chapter. However, I prefer the
word “playground” because you are allowed to go straight to the solutions, com-
piled in the Appendix, should your impatience get the better of you. In case you’ve
read the German original, there I apologized for using many new-German words,
such as “sampeln” or “wrappen”; I won’t do that here, to the contrary, they come in
very handy and make the translator’s job (even) easier. Many thanks to
Mrs. U. Seibt and Mrs. K. Schandert, as well as to Dr. T. Reinert, Dr. T. Soldner
and especially to Mr. H. Gödel (Dipl.-Phys.) for the hard work involved in turning a
manuscript into a book. Mr. St. Jankuhn (Dipl.-Phys.) did an excellent job in
proofreading and computer acrobatics.
Last but not least, special thanks go to the translator who managed to convert the
informal German style into an informal (“downunder”) English style.
Recommendations, queries, and proposals for change are welcome. Have fun
while reading, playing, and learning.
Leipzig
Tilman Butz
April 2005
viii
Preface

Preface of the Translator
More than a few moons ago I read two books about Richard Feynman’s life, and
that has made a lasting impression. When Tilman Butz asked me if I could translate
his “Fourier Transformation for Pedestrians”, I leapt at the chance—my way of
getting a bit more into science. During the rather mechanical process of translating
the German original, within its TEX-framework, I made sure I enjoyed the bits for
the pedestrians, mere mortals like myself. Of course I am biased, I have known the
author for many years—after all he is my brother.
Hamilton, New Zealand
Thomas-Severin Butz
2004
ix

Preface for the Second Enlarged Edition
The second enlarged edition is based on the ﬁrst edition with a focus on applica-
tions in signal analysis and processing. In a digital world, the discrete Fourier
transformation plays a very important role. However, in order to avoid pitfalls, it is
strongly recommended to learn about Fourier series and continuous Fourier
transformation ﬁrst. Two new chapters were added to the ﬁrst edition for pedes-
trians that like to go a little further: the ﬁrst deals with data streams and fractional
delays, a topic which is important in a variety of ﬁelds ever since the development
of fast digitizers; the second gives an introduction to tomography with focus on a
common image reconstruction algorithm, the back-projection of ﬁltered projections.
Here, we shall use the spatial coordinate x and the “angular wave number” k instead
of time t and angular frequency ω. Both topics are intimately related to Fourier
transformation and deal with modern applications. Occasionally, series and inte-
grals are needed which are beyond elementary calculus. For those who have access
to a good library, references are given to verify the results. Those who have access
to Mathematica [5] will prefer to use this tool instead. They will miss the admiration
of human ingenuity of the pre-computer era but hopefully will admire Mathematica.
Since the focus of this book is on signal processing, important issues like proofs of
existence, convergence of inﬁnite series, permutability of integration and differ-
entiation, and integrability of functions are not addressed. Those pedestrians who
want to step a little deeper into mathematics are encouraged to do so. A large
number of typos and errors have been corrected (unfortunately you never get hold
of all) and critical comments have been taken care of. It is a pleasure to thank Mr.
St. Jankuhn (Dipl.-Phys.) for his excellent work to complete the second edition and
Mr. M. Jäger (Dipl.-Inf.) for fruitful discussions on fractional delays. Recommen-
dations, queries and proposals for change are always welcome. Have fun while
reading, playing and learning.
Leipzig
Tilman Butz
xi

Contents
1
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
Even and Odd Functions. . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2
Definition of the Fourier Series . . . . . . . . . . . . . . . . . .
3
1.1.3
Calculation of the Fourier Coefficients . . . . . . . . . . . . .
5
1.1.4
Fourier Series in Complex Notation . . . . . . . . . . . . . . .
9
1.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2.2
The First Shifting Rule
(Shifting Within the Time Domain) . . . . . . . . . . . . . . .
13
1.2.3
The Second Shifting Rule
(Shifting Within the Frequency Domain). . . . . . . . . . . .
16
1.2.4
Scaling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.3
Partial Sums, Bessel’s Inequality, Parseval’s Equation. . . . . . . .
21
1.4
Gibbs’ Phenomenon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4.1
Dirichlet’s Integral Kernel. . . . . . . . . . . . . . . . . . . . . .
25
1.4.2
Integral Notation of Partial Sums . . . . . . . . . . . . . . . . .
26
1.4.3
Gibbs’ Overshoot. . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2
Continuous Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . .
33
2.1
Continuous Fourier Transformation. . . . . . . . . . . . . . . . . . . . .
33
2.1.1
Even and Odd Functions. . . . . . . . . . . . . . . . . . . . . . .
34
2.1.2
The δ-Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.1.3
Forward and Inverse Transformation. . . . . . . . . . . . . . .
35
2.1.4
Polar Representation of the Fourier Transform. . . . . . . .
41
2.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.2
The First Shifting Rule . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.3
The Second Shifting Rule . . . . . . . . . . . . . . . . . . . . . .
44
2.2.4
Scaling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
xiii

2.3
Convolution, Cross Correlation, Autocorrelation,
Parseval’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.3.1
Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.3.2
Cross Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.3.3
Autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.3.4
Parseval’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.4
Fourier Transformation of Derivatives. . . . . . . . . . . . . . . . . . .
60
2.5
Pitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.5.1
“Turn 1 into 3” . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.5.2
Truncation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3
Window Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.1
The Rectangular Window . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.1.1
Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.1.2
Intensity at the Central Peak . . . . . . . . . . . . . . . . . . . .
72
3.1.3
Sidelobe Suppression . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.1.4
3 dB-Bandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.1.5
Asymptotic Behaviour of Sidelobes . . . . . . . . . . . . . . .
75
3.2
The Triangular Window (Fejer Window). . . . . . . . . . . . . . . . .
76
3.3
The Cosine Window. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.4
The cos2-Window (Hanning) . . . . . . . . . . . . . . . . . . . . . . . . .
78
3.5
The Hamming Window. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.6
The Triplet Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.7
The Gauss Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.8
The Kaiser–Bessel Window. . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.9
The Blackman–Harris Window . . . . . . . . . . . . . . . . . . . . . . .
85
3.10
Overview over Window Functions . . . . . . . . . . . . . . . . . . . . .
86
3.11
Windowing or Convolution? . . . . . . . . . . . . . . . . . . . . . . . . .
90
4
Discrete Fourier Transformation. . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.1
Discrete Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . .
93
4.1.1
Even and Odd Series and Wrap-Around . . . . . . . . . . . .
94
4.1.2
The Kronecker Symbol or the “Discrete δ-Function” . . .
94
4.1.3
Definition of the Discrete Fourier Transformation . . . . .
96
4.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.2.2
The First Shifting Rule
(Shifting in the Time Domain). . . . . . . . . . . . . . . . . . .
101
4.2.3
The Second Shifting Rule
(Shifting in the Frequency Domain) . . . . . . . . . . . . . . .
102
4.2.4
Scaling Rule/Nyquist Frequency . . . . . . . . . . . . . . . . .
102
xiv
Contents

4.3
Convolution, Cross Correlation, Autocorrelation,
Parseval’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.3.1
Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.3.2
Cross Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.3.3
Autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.3.4
Parseval’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.4
The Sampling Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
4.5
Data Mirroring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
4.6
How to Get Rid of the “Straight-Jacket” Periodic Continuation?
By Using Zero-Padding! . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
4.7
Fast Fourier Transformation (FFT) . . . . . . . . . . . . . . . . . . . . .
125
5
Filter Effect in Digital Data Processing . . . . . . . . . . . . . . . . . . . . .
137
5.1
Transfer Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.2
Low-Pass, High-Pass, Band-Pass, Notch Filter . . . . . . . . . . . . .
139
5.3
Shifting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.4
Data Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.5
Differentiation of Discrete Data . . . . . . . . . . . . . . . . . . . . . . .
148
5.6
Integration of Discrete Data. . . . . . . . . . . . . . . . . . . . . . . . . .
149
6
Data Streams and Fractional Delays . . . . . . . . . . . . . . . . . . . . . . .
155
6.1
Fractional Delays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
6.2
Non-recursive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
6.3
Stability of Recursive Algorithms. . . . . . . . . . . . . . . . . . . . . .
164
6.4
Thiran’s All-Pass Filter for N ¼ 1 . . . . . . . . . . . . . . . . . . . . .
165
6.4.1
Impulse Response . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.4.2
Step Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.4.3
Ramp Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
7
Tomography: Backprojection of Filtered Projections . . . . . . . . . . .
173
7.1
Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
7.2
Backprojection of Filtered Projections. . . . . . . . . . . . . . . . . . .
175
Appendix: Solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
Contents
xv

Introduction
One of the general tasks in science and engineering is to record measured signals
and get them to tell us their “secrets” (information). Here we are mainly interested
in signals varying over time. They may be periodic or aperiodic, noise or also
superpositions of components. Anyway, what we are measuring is a conglomerate
of several components, which means that effects caused by the measuring-devices’
electronics and, for example, noise, get added to the signal we are actually after.
That is why we have to take the recorded signal, ﬁlter out what is of interest to us,
and process it. In many cases we are predominantly interested in the periodic
components of the signal, or the spectral content, which consists of discrete
components. For analyses of this kind Fourier transformation is particularly well
suited.
Here are some examples:
• Analysis of the vibrations of a violin string or of a bridge,
• checking out the quality of a high-ﬁdelity ampliﬁer,
• radio-frequency Fourier-transformation spectroscopy,
• optical Fourier-transformation spectroscopy,
• digital image-processing (two- and three-dimensional),
to quote only a few examples from acoustics, electronics, and optics, which also
shows that this method is not only useful for purely scientiﬁc research.
Many mathematical procedures in almost all branches of science and engi-
neering use the Fourier transformation. The method is so widely known—almost
“old hat”—that users often only have to push a few buttons (or use a few mouse-
clicks) to perform a Fourier transformation, or the lot even gets delivered “to the
doorstep, free of charge”. This user-friendliness, however, often is accompanied by
the loss of all necessary knowledge. Operating errors, incorrect interpretations and
frustration result from incorrect settings or similar blunders.
This book aims to raise the level of consciousness concerning the dos and
don’ts when using Fourier transformation programs. In order to understand how the
discrete Fourier transformation programs work it is necessary to discuss Fourier
xvii

series and continuous Fourier transformations ﬁrst. Experience shows that
mathematical laypersons will have to cope with two hurdles:
• differential and integral calculus and
• complex number arithmetic.
When deﬁning3 the Fourier series and the continuous Fourier transformation, we
cannot help using integrals, as, for example, in Chap. 3 (Window Functions). The
problem cannot be avoided, but can be mitigated using integration tables. For
example the “Oxford Users’ Guide to Mathematics” [6] will be quite helpful in this
respect. In Chaps. 4–7 elementary maths will be sufﬁcient to understand what is
going on. As far as complex number arithmetic is concerned, I have made sure that
in Chap. 1 all formulas are covered in detail, in plain and in complex notation, so
this chapter may even serve as a small introduction to dealing with complex
numbers.
For all those ready to rip into action using their PCs, the book “Numerical
Recipes” [7] is especially useful. It presents, among other things, programs for
almost every purpose, and they are commented, too.
3The deﬁnitions given in this book are similar to conventions and do not lay claim to any
mathematical rigour.
xviii
Introduction

Chapter 1
Fourier Series
Abstract This chapter introduces the mapping of periodic functions in the time
domain to a Fourier series in the frequency domain with Fourier coefﬁcients. It
shows how these coefﬁcients are calculated and gives examples. It also provides a
gentle introduction to complex notation. It addresses linearity of the transformation,
discusses shifting in the time and frequency domain as well as scaling. Parseval’s
equation is derived. Gibb’s phenomena of “ringing” are discussed.
In 1822, Jean Baptiste Joseph Fourier in his “Theorie analytique de la chaleur”
described heat transport processes in a circular geometry by a series of sine and
cosine functions. Here, we shall ﬁrst consider much simpler systems, e.g. a string
clamped at both ends like in a piano. When excited it will vibrate with its fundamental
frequencywiththeamplitudemaximuminthemiddlewhiletheendsareatrest.Thisis
half a period of a sine-wave. It can also vibrate with twice its fundamental frequency,
its so-called ﬁrst harmonic, with the midpoint at rest, too, and a positive (negative)
maximal amplitude at a quarter of the length of the string and a negative (positive) one
at three quarters. In fact, it could vibrate with all integer multiples of its fundamental
frequency. The possible modes are illustrated in Fig.4.13 when discussing the sine-
transformation. Should you happen to possess a piano, excite a string and feel that the
string one octave higher (this is a factor of two) also starts vibrating. It gets excited
by the sound (air pressure variations) produced by the ﬁrst string. The neighbouring
strings do not vibrate because their frequencies do not match.
In this chapter we want to be a little more general than a piano string. First, we want
to speak of an interval T , corresponding to the length of the string, and continue this
interval periodically. Secondly, we want to abandon the constraint that at the interval
boundaries the amplitude is zero. Hence, what we are dealing with are arbitrary but
periodic functions. Contrary to the clamped string, where we were dealing with half
a period and multiples thereof, we now require full periods and their multiples. Thus,
when shifting the interval we shall always have a complete period of the function. Let
us assume for a moment that the average of the function is zero. Then we may shift
the interval boundaries such that the function happens to be zero at the boundaries.
A joker might be tempted to put clamps there—and nobody would even notice it!
Now it is immediately clear that this arbitrary periodic function can be described
by a series of cosines of integer multiples of the fundamental frequency, eventually
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_1
1

2
1
Fourier Series
with a phase due to shifting. Instead of a cosine with a phase we can use cosines and
sines. Should the average of the function be non-zero, this can be taken care of by
a cosine with frequency zero. Finally, we shall consider periodic functions of time
(with the exception of Chap.7), e.g. signals from a measuring device.
Mapping of a Periodic Function f (t) to a Series of Fourier
Coefﬁcients Ck
1.1 Fourier Series
This section serves as a starter. Many readers may think it too easy; but it should be
read and taken seriously all the same. Some preliminary remarks are in order:
i. To make things easier to understand, the whole book will only be concerned with
functionsinthetimedomainandtheirFouriertransformsinthefrequencydomain.
This represents the most common application, and porting it to other pairings,
such as space/momentum, for example, is pretty straightforward indeed, as we
shall see in Chap.7.
ii. We use the angular frequency ω when we refer to the frequency domain. The unit
of the angular frequency is radians/second (or simpler s−1). It’s easily converted
to the frequency ν of radio-stations—for example FM 105.4 MHz—using the
following equation:
ω = 2πν.
(1.1)
The unit of ν is Hz, short for Hertz.
By the way, in case someone wants to do like H.J. Weaver, my much appreciated
role-model, and use different notations to avoid having the tedious factors 2π crop
up everywhere, don’t buy into that. For each 2π you save somewhere, there will be
more factors of 2π somewhere else. However, there are valid reasons, as detailed for
example in “Numerical Recipes” [7], to use t and ν.
In this book I’ll stick to the use of t and ω, cutting down on the cavalier use of 2π
that’s in vogue elsewhere.
1.1.1 Even and Odd Functions
All functions are either:
f (−t) = f (t) : even
(1.2)
or:
f (−t) = −f (t) : odd
(1.3)

1.1 Fourier Series
3
t
t
t
t
t
f(t)
even
odd
mixed
even
odd
part
=
+
f(t)
f(t)
f(t)
f(t)
Fig. 1.1 Examples of even, odd and mixed functions
or a “mixture” of both, i.e. even and odd parts superimposed. The decomposition
gives:
feven(t) = ( f (t) + f (−t))/2
fodd(t) = ( f (t) −f (−t))/2.
See examples in Fig.1.1.
1.1.2 Deﬁnition of the Fourier Series
Fourier analysis is often also called harmonic analysis, as it uses the trigonometric
functions sine—an odd function—and cosine—an even function—as basis functions
that play a pivotal part in harmonic oscillations.
Similar to expanding a function into a power series, especially periodic functions
may be expanded into a series of the trigonometric functions sine and cosine.
Deﬁnition 1.1 (Fourier Series)
f (t) =
∞

k=0
(Ak cos ωkt + Bk sin ωkt)
(1.4)
with ωk = 2πk
T
and B0 = 0.

4
1
Fourier Series
Here T means the period of the function f (t). The amplitudes or Fourier
coefﬁcients Ak and Bk, are determined in such a way—as we’ll see in a moment—
that the inﬁnite series is identical with the function f (t). Equation (1.4) therefore
tells us, that any periodic function can be represented as a superposition of sine- and
cosine-functions with appropriate amplitudes—with an inﬁnite number of terms, if
need be—yet using only precisely determined frequencies:
ω = 0, 2π
T , 4π
T , 6π
T , . . . .
Figure1.2 shows the basis functions for k = 0, 1, 2, 3.
Example 1.1 (“Trigonometric identity”)
f (t) = cos2 ωt = 1
2 + 1
2 cos 2ωt.
(1.5)
Trigonometric manipulation in (1.5) already determined the Fourier coefﬁcients
A0 and A2: A0 = 1/2, A2 = 1/2 (see Fig.1.3). As function cos2 ωt is an even
function, we need no Bk. Generally speaking, practically all “smooth” functions
without steps (i.e. without discontinuities) and without kinks (i.e. without disconti-
nuities in their ﬁrst derivative)—and strictly speaking without discontinuities in all
their derivatives—are limited as far as their bandwidth is concerned. This means that
a ﬁnite number of terms in the series will do for practical purposes. Often data gets
recorded using a device with limited bandwidth, which puts a limit on how quickly
f (t) can vary over time anyway.
Fig. 1.2 Basis functions of Fourier transformation: cosine (left); sine (right)

1.1 Fourier Series
5
Fig. 1.3 Decomposition of cos2 ωt into the average 1/2 and an oscillation with amplitude 1/2 and
frequency 2ω
1.1.3 Calculation of the Fourier Coefﬁcients
Before we dig into the calculation of the Fourier coefﬁcients, we need some tools.
In all following integrals we integrate from −T/2 to +T/2, meaning over an
interval with the period T that is symmetrical to t = 0. We could also pick any
other interval, as long as the integrand is periodic with period T and gets integrated
over a whole period. The letters n and m in the formulas below are natural numbers
0, 1, 2, . . . . Let’s have a look at the following:
+T/2

−T/2
cos 2πnt
T
dt =
0
for n ̸= 0
T
for n = 0 ,
(1.6)
+T/2

−T/2
sin 2πnt
T
dt = 0
for all n.
(1.7)
This results from the fact, that the areas on the positive half-plane and the ones on
the negative one cancel out each other, provided we integrate over a whole number of
periods. Cosine integral for n = 0 requires special treatment, as it lacks oscillations
and therefore areas can’t cancel out each other: there the integrand is 1, and the area
under the horizontal line is equal to the width of the interval T .
Furthermore, we need the following trigonometric identities:
cos α cos β = 1/2 [cos(α + β) + cos(α −β)],
sin α sin β = 1/2 [cos(α −β) −cos(α + β)],
sin α cos β = 1/2 [sin(α + β) + sin(α −β)].
(1.8)

6
1
Fourier Series
Using these tools we’re able to show, without further ado, that the system of basis
functions consisting of:
1, cos 2πt
T , sin 2πt
T , cos 4πt
T , sin 4πt
T , . . . ,
(1.9)
is an orthogonal system.1
Put in formulas, this means:
+T/2

−T/2
cos 2πnt
T
cos 2πmt
T
dt =
⎧
⎨
⎩
0
for n ̸= m
T/2
for n = m ̸= 0
T
for n = m = 0
,
(1.10)
+T/2

−T/2
sin 2πnt
T
sin 2πmt
T
dt =

0
for n ̸= m, n = 0
and/or m = 0
T/2
for n = m ̸= 0
,
(1.11)
+T/2

−T/2
cos 2πnt
T
sin 2πmt
T
dt = 0.
(1.12)
The right-hand side of (1.10) and (1.11) show that our basis system is not an
orthonormal system, i.e. the integrals for n = m are not normalised to 1. What’s
even worse, the special case of (1.10) for n = m = 0 is a nuisance, and will keep
bugging us again and again.
Using the above orthogonality relations, we’re able to calculate the Fourier
coefﬁcients straight away. We need to multiply both sides of (1.4) with cos ωk′t
and integrate from −T/2 to +T/2. Due to the orthogonality, only terms with k = k′
will remain; the second integral will always disappear.
This gives us:
Ak = 2
T
+T/2

−T/2
f (t) cos ωktdt
for k ̸= 0
(1.13)
and for our “special” case:
A0 = 1
T
+T/2

−T/2
f (t)dt.
(1.14)
Please note the prefactors 2/T or 1/T , respectively, in (1.13) and (1.14). Equa-
tion (1.14) simply is the average of the function f (t). The “electricians” amongst
1Similar to two vectors at right angles to each other whose dot product is 0, we call a set of basis
functions an orthogonal system if the integral over the product of two different basis functions
vanishes.

1.1 Fourier Series
7
us, who might think of f (t) as current varying over time, would call A0 the “DC”-
component (DC = direct current, as opposed to AC = alternating current). Now let’s
multiply both sides of (1.4) with sin ωk′t and integrate from −T/2 to +T/2.
We now have:
Bk = 2
T
+T/2

−T/2
f (t) sin ωktdt
for all k.
(1.15)
Equations (1.13) and (1.15) may also be interpreted like so: by weighting
the function f (t) with cos ωk′t or sin ωk′t, respectively, we “pick” the spectral
components from f (t), when integrating, corresponding to the even or odd com-
ponents, respectively, of the frequency ωk. In the following examples we’ll only
state the functions f (t) in their basic interval −T/2 ≤t ≤+T/2. They have to be
extended periodically, however, as the deﬁnition goes, beyond this basic interval.
Example 1.2 (“Constant”) See Fig.1.4 (left):
f (t) = 1
A0 = 1
“Average”
Ak = 0
for all k ̸= 0
Bk = 0
for all k (as f is even).
Example 1.3 (“Triangular function”) See Fig.1.4 (right):
f (t) =
⎧
⎪⎨
⎪⎩
1 + 2t
T
for −T/2 ≤t ≤0
1 −2t
T
for 0 ≤t ≤+T/2
.
Let’s recall: ωk = 2πk
T
A0 = 1/2 (“Average”).
t
t
f(t)
f(t)
−T
2
−T
2
+ T
2
+ T
2
Fig. 1.4 “Constant” (left); “triangular function” (right). We only show the basic intervals for
both functions

8
1
Fourier Series
For k ̸= 0 we get:
Ak = 2
T
⎡
⎢⎣
0

−T/2

1 + 2t
T

cos 2πkt
T
dt +
+T/2

0

1 −2t
T

cos 2πkt
T
dt
⎤
⎥⎦
= 2
T
0

−T/2
cos 2πkt
T
dt + 2
T
+T/2

0
cos 2πkt
T
dt



= 0
+ 4
T 2
0

−T/2
t cos 2πkt
T
dt −4
T 2
+T/2

0
t cos 2πkt
T
dt
= −8
T 2
+T/2

0
t cos 2πkt
T
dt.
In a last step we’ll use

x cos axdx = x
a sin ax + 1
a2 cos ax which ﬁnally gives
us:
Ak = 2(1 −cos πk)
π2k2
(k > 0),
(1.16)
Bk = 0
(as f is even).
A few more comments on the expression for Ak are in order:
i. For all even k Ak disappears.
ii. For all odd k we get Ak = 4/(π2k2).
iii. For k = 0 we better use the average A0 instead of inserting k = 0 in (1.16).
We could make things even simpler:
Ak =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
1
2
for k = 0
4
π2k2
for k odd
0
for k even, k ̸= 0
.
(1.17)
The series’ elements decrease rapidly while k rises (to the power of two in the case
of odd k), but in principle we still have an inﬁnite series. That’s due to the “pointed
roof” at t = 0 and the kink (continued periodically!) at ±T/2 in our function f (t).
In order to describe these kinks, we need an inﬁnite number of Fourier coefﬁcients.

1.1 Fourier Series
9
The following illustrations will show, that things are never as bad as they seem
to be:
Using ω = 2π/T (see Fig.1.5) we get:
f (t) = 1
2 + 4
π2

cos ωt + 1
9 cos 3ωt + 1
25 cos 5ωt . . .

.
(1.18)
We want to plot the frequencies of this Fourier series. Figure1.6 shows the result,
as produced, for example, by a spectrum analyser,2 if we would use our “triangular
function” f (t) as input signal.
Apart from the DC peak at ω = 0 we can also see the fundamental frequency ω
and all odd “harmonics”. We may also use this frequency-plot to get an idea about
the margins of error resulting from discarding frequencies above, say, 7ω. We will
cover this in more detail later on.
1.1.4 Fourier Series in Complex Notation
Let me give you a mild warning before we dig into this chapter: in (1.4) k starts from
0, meaning that we will rule out negative frequencies in our Fourier series.
The cosine terms didn’t have a problem with negative frequencies. The sign of
the cosine argument doesn’t matter anyway, so we would be able to go halves, like
between brothers, for example, as far as the spectral intensity at the positive frequency
kω was concerned: −kω and kω would get equal parts, as shown in Fig.1.7.
As frequency ω = 0—a frequency as good as any other frequency ω ̸= 0—has
no “brother”, it will not have to go halves. A change of sign for the sine-terms’
arguments would result in a change of sign for the corresponding series’ term. The
splitting of spectral intensity like “between brothers”—equal parts of −ωk and +ωk
now will have to be like “between sisters”: the sister for −ωk also gets 50%, but
her’s is minus 50%!
Instead of using (1.4) we might as well use:
f (t) =
+∞

k=−∞
(A
′
k cos ωkt + B
′
k sin ωkt),
(1.19)
where, of course, the following is true: A
′
−k = A
′
k, B
′
−k = −B
′
k. The formulas for
the calculation of A
′
k and B
′
k for k > 0 are identical to (1.13) and (1.15), though they
lack the extra factor 2! Equation (1.14) for A0 stays unaffected by this. This helps
us avoid to provide a special treatment for the DC-component.
2On offer by various companies—for example as a plug-in option for oscilloscopes—for a tidy sum
of money.

10
1
Fourier Series
0.5
1
−T
2
+ T
2
original
0.5
1
−T
2
+ T
2
“0th approximation”:
1
2
0.5
1
−T
2
+ T
2
1st approximation:
1
2 + 4
π2 cos ωt
0.5
1
−T
2
+ T
2
2nd approximation:
1
2 + 4
π2
cos ωt + 1
9 cos 3ωt
0.5
1
−T
2
0
+ T
2
3rd approximation:
1
2 + 4
π2
cos ωt + 1
9 cos 3ωt + 1
25 cos 5ωt
Fig. 1.5 The “triangular function” f (t) and consecutive approximations by a Fourier series with
more and more terms
Instead of (1.16) we could have used:
A
′
k = (1 −cos πk)
π2k2
,
(1.20)
which would also be valid for k = 0! To prove it, we’ll use a “dirty trick” or commit
a “venial” sin: we’ll assume, for the time being, that k is a continuous variable that
may steadily decrease towards 0. Then we apply l’Hospital’s rule to the expression of

1.1 Fourier Series
11
r
r
r
r
r
r
r
r
Ak
k
0
1
2
3
4
5
6
7
0.5
0
4
π2
4
π29
4
π225
4
π249
Fig. 1.6 Plot of the “triangular function’s” frequencies
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
Ak
k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
0.5
2
π249
2
π249
2
π225
2
π225
2
π29
2
π29
2
π2
2
π2
Fig. 1.7 Like Fig.1.6, yet with positive and negative frequencies
type “0 : 0”, stating that numerator and denominator may be differentiated separately
with respect to k until limk→0 does not result in an expression of type “0 : 0” any
more. Like so:
lim
k→0
1 −cos πk
π2k2
= lim
k→0
π sin πk
2π2k
= lim
k→0
π2 cos πk
2π2
= 1
2.
(1.21)
If you’re no sinner, go for the “average” A0 = 1/2 straight away!
Hint:InmanystandardFouriertransformationprogramsafactor2between A0 and
Ak̸=0 is wrong. This could be mainly due to the fact that frequencies were permitted
to be positive only for the basis functions, or positive and negative—like in (1.4).
The calculation of the average A0 is easy as pie, and therefore always recommended
as a ﬁrst test in case of a poorly documented program. As B0 = 0, according to the
deﬁnition, Bk is a bit harder to check out. Later on we’ll deal with simpler checks
(for example Parseval’s theorem).
Now we’re set and ready for the introduction of complex notation. In the following
we’ll always assume that f (t) is a real function. Generalising this for complex f (t)
is no problem. Our most important tool is Euler’s identity:
eiαt = cos αt + i sin αt.
(1.22)

12
1
Fourier Series
Here we use i as the imaginary unit that results in −1 when raised to the power
of two.
This allows us to rewrite the trigonometric functions as follows:
cos αt = 1
2(eiαt + e−iαt),
sin αt = 1
2i(eiαt −e−iαt).
(1.23)
Inserting into (1.4) gives:
f (t) = A0 +
∞

k=1
 Ak −iBk
2
eiωkt + Ak + iBk
2
e−iωkt

.
(1.24)
Using the short-cuts:
C0 = A0,
Ck = Ak −iBk
2
,
(1.25)
C−k = Ak + iBk
2
,
k = 1, 2, 3, . . . ,
we ﬁnally get:
f (t) =
+∞

k=−∞
Ckeiωkt,
ωk = 2πk
T .
(1.26)
Caution: For k < 0 there will be negative frequencies. (No worries, according to
our above digression!) Pretty handy that Ck and C−k are complex conjugates to each
other (see “brother and sister”.) Now Ck can be formulated just as easily:
Ck = 1
T
+T/2

−T/2
f (t)e−iωktdt
for k = 0, ±1, ±2, . . . .
(1.27)
Please note that there is a negative sign in the exponent. It will stay with us till
the end of this book, and, hopefully, for the rest of your life. Please also note that the
index k runs from −∞to +∞for Ck whereas it runs from 0 to +∞for Ak and Bk.

1.2 Theorems and Rules
13
1.2 Theorems and Rules
1.2.1 Linearity Theorem
Expanding a periodic function into a Fourier series is a linear operation. This means,
that we may use the two Fourier pairs:
f (t) ↔{Ck; ωk} and
g(t) ↔{C′
k; ωk}
to form the following linear combination:
h(t) = af (t) + bg(t) ↔{aCk + bC′
k; ωk}.
(1.28)
Thus we may easily determine the Fourier series of a function by splitting it into
items whose Fourier series we already know.
Example 1.4 (Lowered “triangular function”) The simplest example is our “trian-
gular function” from Example 1.3, though this time it is symmetrical to its base line
(see Fig.1.8): we only have to subtract 1/2 from our original function. That means,
that the Fourier series remained unchanged while only the average A0 now turned
to 0.
The linearity theorem appears to be so trivial that you may accept it at face-value
even when you have “strayed from the path of virtue”. Straying from the path of
virtue is, for example, something as elementary as squaring.
1.2.2 The First Shifting Rule (Shifting Within
the Time Domain)
Often we want to know how the Fourier series changes if we shift the function f (t)
along the time axis. This, for example, happens on a regular basis if we use a different
interval, e.g. from 0 to T , instead of the symmetrical one from −T/2 to T/2 we have
used so far. In this situation, the First Shifting Rule comes in very handy:
f(t)
T
2
t
−T
2
Fig. 1.8 “Triangular function” with average 0

14
1
Fourier Series
f (t) ↔{Ck; ωk},
f (t −a) ↔

Cke−iωka; ωk

.
(1.29)
Proof (First Shifting Rule)
Cnew
k
= 1
T
+T/2

−T/2
f (t −a)e−iωktdt = 1
T
+T/2−a

−T/2−a
f (t′)e−iωkt′e−iωkadt
′
= e−iωkaCold
k . ⊓⊔
We integrate over a full period, that’s why shifting the limits of the interval by a
does not make any difference.
The proof is trivial, the result of the shifting along the time axis not! The new
Fourier coefﬁcient results from the old coefﬁcient Ck by multiplying it with the phase
factor e−iωka. As Ck generally is complex, shifting “shufﬂes” real and imaginary
parts.
Without using complex notation we get:
f (t) ↔{Ak; Bk; ωk},
f (t −a) ↔{Ak cos ωka −Bk sin ωka; Ak sin ωka + Bk cos ωka; ωk}.
(1.30)
Two examples follow:
Example 1.5 (Quarter period shifted “triangular function”) “Triangular function”
(with average = 0) (see Fig. 1.8):
f (t) =
⎧
⎪⎪⎨
⎪⎪⎩
1
2 + 2t
T for −T/2 ≤t ≤0
1
2 −2t
T for 0 < t ≤T/2
(1.31)
with Ck =
⎧
⎨
⎩
1 −cos πk
π2k2
=
2
π2k2 for k odd
0
for k even
.
Now let’s shift this function to the right by a = T/4:
fnew = fold(t −T/4).

1.2 Theorems and Rules
15
So the new coefﬁcients can be calculated as follows:
Cnew
k
= Cold
k e−iπk/2
(k odd)
=
2
π2k2

cos πk
2 −i sin πk
2

(k odd)
(1.32)
= −2i
π2k2 (−1)
k−1
2
(k odd).
It’s easy to realise that Cnew
−k = −Cnew
k
.
In other words: Ak = 0.
Using iBk = C−k −Ck we ﬁnally get:
Bnew
k
=
4
π2k2 (−1)
k+1
2
(k odd).
Using the above shifting we get an odd function (see Fig.1.9b).
Example 1.6 (Half period shifted “triangular function”) Now we’ll shift the same
function to the right by a = T/2:
fnew = fold(t −T/2).
−T
2
+ T
2
−T
2
+ T
2
−T
2
+ T
2
f(t)
f(t)
f(t)
t
t
t
A
Ak
k
k
k
k
k
B
(a)
(b)
(c)
Fig. 1.9 a “Triangular function” (with average = 0); b right-shifted by T/4; c right-shifted by T/2

16
1
Fourier Series
The new coefﬁcients then are:
Cnew
k
= Cold
k e−iπk
(k odd)
=
2
π2k2 (cos πk −i sin πk) (k odd)
= −
2
π2k2
(k odd)
(C0 = 0 stays).
(1.33)
So we’ve only changed the sign. That’s okay, as the function now is upside-down
(see Fig.1.9c).
Warning: Shifting by a = T/4 will result in alternating signs for the coefﬁcients
(Fig.1.9b). The series of Fourier coefﬁcients, that are decreasing monotonically with
k according to Fig.1.9a, looks pretty “frazzled” after shifting the function by a =
T/4, due to the alternating sign.
1.2.3 The Second Shifting Rule (Shifting
Within the Frequency Domain)
The First Shifting Rule showed us that shifting within the time domain leads to a
multiplication by a phase factor in the frequency domain. Reversing this statement
gives us the Second Shifting Rule:
f (t) ↔{Ck; ωk},
f (t)ei 2πat
T
↔{Ck−a; ωk}.
(1.34)
In other words: a multiplication of the function f (t) by the phase factor ei2πat/T
results in frequency ωk now being related to “shifted” coefﬁcient Ck−a—instead of
the former coefﬁcient Ck. A comparison between (1.34) and (1.29) demonstrates
the two-sided character of the two Shifting Rules. If a is an integer, there won’t be
any problem if you simply take the coefﬁcient shifted by a. But what if a is not an
integer?
Strangely enough nothing serious will happen. Simply shifting like we did before
won’t work any more, but who is to keep us from inserting (k −a) into the expression
for old Ck, whenever k occurs.
(If it’s any help to you, do commit another venial sin and temporarily consider k
to be a continuous variable.) So in the case of non-integer a we didn’t really shift
Ck, but rather recalculated it using shifted k.

1.2 Theorems and Rules
17
Caution: If you have simpliﬁed a k-dependency in the expressions for Ck, for
example:
1 −cos πk =
0
for k even
2
for k odd
(as in (1.16)), you’ll have trouble replacing the “vanished” k with (k −a). In this case
there’s only one way out: back to the expressions with all k-dependencies without
simpliﬁcation.
Before we present examples, two more ways of writing down the Second Shifting
Rule are in order:
f (t) ↔{Ak; Bk; ωk} ,
f (t)e
2πiat
T
↔
1
2[Ak+a + Ak−a + i(Bk+a −Bk−a)];
(1.35)
1
2[Bk+a + Bk−a + i(Ak−a −Ak+a)]; ωk

.
Caution: This is true for k ̸= 0.
Old A0 then becomes Aa/2 + iBa/2!
This is easily proved by solving (1.25) for Ak and Bk and inserting it in (1.34):
Ak = Ck + C−k,
−iBk = Ck −C−k,
(1.36)
Anew
k
= Ck + C−k = Ak−a −iBk−a
2
+ Ak+a + iBk+a
2
,
−iBnew
k
= Ck −C−k = Ak−a −iBk−a
2
−Ak+a + iBk+a
2
,
which leads to (1.35). We get the special treatment for A0 from:
Anew
0
= Cnew
0
= A−a −iB−a
2
= A+a + iB+a
2
.
The formulas become a lot simpler in case f (t) is real. Then we get:
f (t) cos 2πat
T
↔
 Ak+a + Ak−a
2
; Bk+a + Bk−a
2
; ωk

,
(1.37)

18
1
Fourier Series
old A0 becomes Aa/2 and also:
f (t) sin 2πat
T
↔
 Bk+a −Bk−a
2
; Ak−a −Ak+a
2
; ωk

,
old A0 becomes Ba/2.
Example 1.7 (“Constant”)
f (t) = 1 for −T/2 ≤t ≤+T/2.
Ak = δk,0 (Kronecker symbol, see Sect.4.1.2) or A0 = 1, all other Ak, Bk vanish.
Of course we’ve always known that f (t) is a cosine wave with frequency ω = 0 and
therefore only required the coefﬁcient for ω = 0.
Now let’s multiply function f (t) by cos(2πt/T ), i.e. a = 1. From (1.37) we can
see:
Anew
k
= δk−1,0,
i.e.
A1 = 1 (all others are 0),
or
C1 = 1/2,
C−1 = 1/2.
So we have shifted the coefﬁcient by a = 1 (to the right and to the left, and gone
halves, like “between brothers”).
This example demonstrates that the frequency ω = 0 is as good as any other
frequency. No kidding! If you know, for example, the Fourier series of a function
f (t) and consequently the solution for integrals of the form:
+T/2

−T/2
f (t)e−iωktdt
then you already have, using the Second Shifting Rule, solved all integrals for f (t),
multiplied by sin(2πat/T ) or cos(2πat/T ). No wonder, you only had to combine
phase factor ei2πat/T with phase factor e−iωkt!
Example 1.8 (“Triangular function” multiplied by cosine) The function:
f (t) =
⎧
⎪⎪⎨
⎪⎪⎩
1 + 2t
T
for −T/2 ≤t ≤0
1 −2t
T
for 0 ≤t ≤T/2
is to be multiplied by cos(πt/T ), i.e. we shift the coefﬁcients Ck by a = 1/2 (see
Fig.1.10). The new function still is even, and therefore we only have to look after
Ak:
Anew
k
= Aold
k+a + Aold
k−a
2
.

1.2 Theorems and Rules
19
1
−T
2
T
2 t
f(t)=

1+ 2t
T
for −T/2 ≤t ≤0
1−2t
T
for 0 ≤t ≤T/2
f(t)
1
−T
2
T
2 t
g(t)=cos πt
T
g(t)
1
−T
2
T
2 t
f(t)g(t)
f(t)g(t)
Fig. 1.10 “Triangular function” (left);

cos πt
T

-function (middle); “triangular function” with

cos πt
T

-weighting (right)
We use (1.16) for the old Ak (and stop using the simpliﬁed version (1.17)!):
Aold
k
= 2(1 −cos πk)
π2k2
.
We then get:
Anew
k
= 1
2
2(1 −cos π(k + 1/2))
π2(k + 1/2)2
+ 2(1 −cos π(k −1/2))
π2(k −1/2)2

= 1 −cos πk cos(π/2) + sin πk sin(π/2)
π2(k + 1/2)2
+ 1 −cos πk cos(π/2) −sin πk sin(π/2)
π2(k −1/2)2
(1.38)
=
1
π2(k + 1/2)2 +
1
π2(k −1/2)2
Anew
0
=
Aold
1/2
2
= 2(1 −cos(π/2))
2π2  1
2
2
= 4
π2 .

20
1
Fourier Series
The new coefﬁcients then are:
A0 = 4
π2 ,
A1 = 1
π2
⎛
⎜⎝
1
!
3
2
"2 +
1
!
1
2
"2
⎞
⎟⎠= 4
π2
1
9 + 1
1

= 4
π2
10
9 ,
A2 = 1
π2
⎛
⎜⎝
1
!
5
2
"2 +
1
!
3
2
"2
⎞
⎟⎠= 4
π2
 1
25 + 1
9

= 4
π2
34
225,
A3 = 1
π2
⎛
⎜⎝
1
!
7
2
"2 +
1
!
5
2
"2
⎞
⎟⎠= 4
π2
 1
49 + 1
25

= 4
π2
74
1225 etc.
(1.39)
A comparison of these coefﬁcients with the ones without the

cos πt
T

-weighting
shows what we’ve done:
without weighting
with

cos πt
T

-weighting
A0
1
2
4
π2
A1
4
π2
4
π2
10
9
A2
0
4
π2
34
225
A3
4
π2
1
9
4
π2
74
1225.
(1.40)
We can see the following:
i. The average A0 got somewhat smaller, as the rising and falling ﬂanks were
weighted with the cosine, which, except for t = 0, is less than 1.
ii. We raised coefﬁcient A1 a bit, but lowered all following odd coefﬁcients a bit,
too. This is evident straight away, if we convert:
1
(2k + 1)2 +
1
(2k −1)2 < 1
k2
to 8k4 −10k2 + 1 > 0.
This is not valid for k = 1, yet all bigger k.
iii. Now we’ve been landed with even coefﬁcients, that were 0 before.
We now have twice as many terms in the series as before, though they go down
at an increased rate when k increases. The multiplication by cos(πt/T ) caused the
kink at t = 0 to turn into a much more pointed spike. This should actually make for
a worsening of convergence or a slower rate of decrease of the coefﬁcients. We have,
however, rounded the kink at the interval-boundary ±T/2, which naturally helps,
but we couldn’t reasonably have predicted what exactly was going to happen.

1.2 Theorems and Rules
21
1.2.4 Scaling Theorem
Sometimes we happen to want to scale the time axis. In this case, there is no need to
re-calculate the Fourier coefﬁcients. From:
f (t) ↔{Ck; ωk}
f (at) ↔{Ck; a · ωk} .
(1.41)
Here, a must be real!
For a < 1 the time axis will be stretched and, hence, the frequency axis will be
compressed. For a > 1 the opposite is true. The proof for (1.41) is easy and follows
from (1.27). Please note that we also have to stretch or compress the interval limits
because of the requirement of periodicity. Similarly, the basis function are modiﬁed
according to ωnew
k
= a · ωold
k .
Cnew
k
= a
T
+T/2a

−T/2a
f (at)e−iωnew
k
t dt = a
T
+T/2

−T/2
f (t′)e−iωold
k t′ 1
a dt′ = Cold
k .
with t′ = at
and ωnew
k
t = ωold
k t′
Here, we have tacitly assumed a > 0. For a < 0, we would only reverse the time
axis and, hence, also the frequency axis. For the special case a = −1 we have:
f (t) ↔{Ck, ωk},
f (−t) ↔{Ck; −ωk}.
(1.42)
1.3 Partial Sums, Bessel’s Inequality, Parseval’s Equation
For practical work, inﬁnite Fourier series have to get terminated at some stage,
regardless. Therefore we only use a partial sum, say until we reach kmax = N. This
Nth partial sum then is:
SN =
N

k=0
(Ak cos ωkt + Bk sin ωkt).
(1.43)
Terminating the series results in the following squared error:
δ2
N = 1
T

T
[ f (t) −SN(t)]2dt.
(1.44)

22
1
Fourier Series
The “T ” below the integral symbol means integration over a full period. This
deﬁnition will become plausible in a second if we look at the discrete version:
δ2
N = 1
N
N

i=1
( fi −si)2.
Please note that we divide by the length of the interval, to compensate for inte-
grating over the interval T . Now we know that the following is correct for the inﬁnite
series:
lim
N→∞SN =
∞

k=0
(Ak cos ωkt + Bk sin ωkt)
(1.45)
provided the Ak and Bk happen to be the Fourier coefﬁcients. Does this also have to
be true for the Nth partial sum? Isn’t there a chance the mean squared error would
get smaller, if we used other coefﬁcients instead of Fourier coefﬁcients? That’s not
the case! To prove it, we’ll now insert (1.43) and (1.44) in (1.45), leave out limN→∞
and get:
δ2
N = 1
T
⎧
⎨
⎩

T
f 2(t)dt −2

T
f (t)SN(t)dt +

T
S2
N(t)dt
⎫
⎬
⎭
= 1
T
⎧
⎨
⎩

T
f 2(t)dt
−2

T
∞

k=0
(Ak cos ωkt + Bk sin ωkt)
N

k=0
(Ak cos ωkt + Bk sin ωkt)dt
+

T
N

k=0
(Ak cos ωkt + Bk sin ωkt)
N

k=0
(A′
k cos ω′
kt + B′
k sin ω′
kt)dt
⎫
⎬
⎭
= 1
T
⎧
⎨
⎩

T
f 2(t)dt −2T A2
0 −2T
2
N

k=1
(A2
k + B2
k ) + T A2
0
+ T
2
N

k=1
(A2
k + B2
k )
)
= 1
T

T
f 2(t)dt −A2
0 −1
2
N

k=1
(A2
k + B2
k ).
(1.46)
Here we made use of the somewhat cumbersome orthogonality properties of
(1.10)–(1.12). As the A2
k and B2
k always are positive, the mean squared error will
drop monotonically as N increases.

1.3 Partial Sums, Bessel’s Inequality, Parseval’s Equation
23
Example 1.9 (Approximating the “triangular function”) The “Triangular function”:
f (t) =
⎧
⎪⎨
⎪⎩
1 + 2t
T
for −T/2 ≤t ≤0
1 −2t
T
for 0 ≤t ≤T/2
(1.47)
has the mean squared “signal”:
1
T
+T/2

−T/2
f 2(t)dt = 2
T
+T/2

0
f 2(t)dt = 2
T
+T/2

0

1 −2 t
T
2
dt = 1
3.
(1.48)
The most coarse, meaning 0th, approximation is:
S0 = 1/2, i.e.
δ2
0 = 1/3 −1/4 = 1/12 = 0.0833 . . . .
The next approximation results in:
S1 = 1/2 + 4
π2 cos ωt, i.e.
δ2
1 = 1/3 −1/4 −1/2
!
4
π2
"2
= 0.0012 . . . .
For δ2
3 we get 0.0001915 . . ., the approximation of the partial sum to the “triangle”
quickly gets better and better.
As δ2
N is always positive, we ﬁnally arrive from (1.46) at Bessel’s inequality:
1
T

T
f 2(t)dt ≥A2
0 + 1
2
N

k=1
(A2
k + B2
k ).
(1.49)
For the limiting case of N →∞we get Parseval’s equation:
1
T

T
f 2(t)dt = A2
0 + 1
2
∞

k=1
(A2
k + B2
k ).
(1.50)
Parseval’s equation may be interpreted as follows: 1/T

f 2(t)dt is the mean
squared “signal” within the time domain, or—more colloquially—the “information
content”. Fourier series don’t lose this information content: it’s in the squared Fourier
coefﬁcients.

24
1
Fourier Series
The rule of thumb therefore is:
“The information content isn’t lost.”
or
“Nothing goes missing in this house.”
Here we simply have to mention an analogy with the energy density of the elec-
tromagnetic ﬁeld: w =
1
2(E2 + B2) with ϵ0 = μ0 = 1, as often is customary
in theoretical physics. The comparison has got some weak sides, as E and B have
nothing to do with even and odd components.
Parseval’s equation is very useful: you can use it to easily sum up inﬁnite series. I
think you’d always have been curious how we arrive at formulas such as, for example,
∞

k=1
odd
1
k4 = π4
96.
(1.51)
Our “triangular function” (1.47) is behind it! Insert (1.48) and (1.17) in (1.50),
and you’ll get:
1
3 = 1
4 + 1
2
∞

k=1
odd

4
π2k2
2
(1.52)
or
∞

k=1
odd
1
k4 = 2
12 · π4
16 = π4
96.
1.4 Gibbs’ Phenomenon
So far we’ve only been using smooth functions as examples for f (t), or—like the
much-used “triangular function”—functions with “a kink”, that’s a discontinuity in
the ﬁrst derivative. This pointed kink made sure that we basically needed an inﬁnite
number of terms in the Fourier series. Now, what will happen if there is a step, a
discontinuity, in the function itself? This certainly won’t make the problem with the
inﬁnite number of elements any smaller. Is there any way to approximate such a step
by using the Nth partial sum, and will the mean squared error for N →∞approach
0? The answer is clearly “Yes and No”. Yes, because it apparently works, and no,
because Gibbs’ phenomenon happens at the steps, an overshoot or undershoot, that
doesn’t disappear for N →∞.
In order to understand this, we’ll have to dig a bit wider.

1.4 Gibbs’ Phenomenon
25
1.4.1 Dirichlet’s Integral Kernel
The following expression is called Dirichlet’s integral kernel:
DN(x) = sin

N + 1
2

x
2 sin x
2
= 1
2 + cos x + cos 2x + · · · + cos Nx.
(1.53)
The second equal sign can be proved as follows:

2 sin x
2

DN(x) = 2 sin x
2 ×
 1
2 + cos x + cos 2x + · · · + cos Nx

= sin x
2 + 2 cos x sin x
2 + 2 cos 2x sin x
2 + · · ·
+ 2 cos Nx sin x
2
= sin

N + 1
2

x.
(1.54)
Here we have used the identity:
2 sin α cos β = sin(α + β) + sin(α −β)
with α = x/2 and β = nx,
n = 1, 2, . . . , N.
By insertion, we see that all pairs of terms cancel out each other, except for the
last one.
Figure1.11 shows a few examples for DN(x). Please note that DN(x) is periodic
in 2π. This is immediately evident from the cosine notation. With x = 0 we get
DN(0) = N + 1/2, between 0 and 2π DN(x) oscillates around 0.
0.5
1.5
2.5
3.5
4.5
5.5
π
2π
x
DN(x) = 1
2 +
N

n=1
cos nx
D0
D0
D1
D2
D3
D4
D5
Fig. 1.11 DN(x) = 1/2 + cos x + cos 2x + · · · + cos Nx

26
1
Fourier Series
In the limiting case of N →∞everything averages to 0, except for x = 0 (mod-
ulo 2π), that’s where DN(x) grows beyond measure. Here we’ve found a notation
for the δ-function (see Chap.2)! Please excuse even two venial sins I’ve commit-
ted here: ﬁrst, the δ-function is a distribution (and not a function!), and second,
limN→∞DN(x) is a whole “comb” of δ-functions 2π apart.
1.4.2 Integral Notation of Partial Sums
We need a way to sneak up on the discontinuity, from the left and the right. That’s
why we insert the deﬁning equations for the Fourier coefﬁcients, (1.13)–(1.15), in
(1.43):
SN(t) = 1
T
+T/2

−T/2
f (x)dx
(k = 0)-term taken out
of the sum
+
N

k=1
2
T
+T/2

−T/2

f (x) cos 2πkx
T
cos 2πkt
T
+ f (x) sin 2πkx
T
sin 2πkt
T

dx
(1.55)
= 2
T
+T/2

−T/2
f (x)
*
1
2 +
N

k=1
cos 2πk(x −t)
T
+
dx
= 2
T
+T/2

−T/2
f (x)DN
!
2π(x−t)
T
"
dx.
Using the substitution x −t = u we get:
SN(t) = 2
T
+T/2−t

−T/2−t
f (u + t)DN
 2πu
T

du.
(1.56)
As both f and D are periodic in T , we may shift the integration boundaries by t
with impunity, without changing the integral. Now we split the integration interval
from −T/2 to +T/2:

1.4 Gibbs’ Phenomenon
27
SN(t) = 2
T
⎧
⎪⎨
⎪⎩
0

−T/2
f (u + t)DN( 2πu
T )du +
+T/2

0
f (u + t)DN( 2πu
T )du
⎫
⎪⎬
⎪⎭
(1.57)
= 2
T
+T/2

0
[ f (t −u) + f (t + u)]DN( 2πu
T )du.
Here we made good use of the fact that DN is an even function (sum over cosine
terms!).
Riemann’s localisation theorem—which we won’t prove here in the scientiﬁc
sense, but which can be understood straight away using (1.57)—states that the con-
vergence behaviour of SN(t) for N →∞only depends on the immediate proximity
to t of the function:
lim
N→∞SN(t) = S(t) = f (t+) + f (t−)
2
.
(1.58)
Here t+ and t−mean the approach to t, from above and from below. Contrary to a
continuous function with a non-differentiability (“kink”), where limN→∞SN(t) =
f (t), (1.58) means, that in the case of a discontinuity (“step”) at t, the partial sum
converges to a value that’s “half-way” there.
That seems to make sense.
1.4.3 Gibbs’ Overshoot
Now we’ll have a closer look at the unit step (see Fig.1.12):
−T
2
T
2
t
f(t)
1
2
−1
2
Fig. 1.12 Unit step

28
1
Fourier Series
f (t) =
−1/2
for −T/2 ≤t < 0
+1/2
for 0 ≤t ≤T/2
with periodic continuation.
(1.59)
At this stage we’re only interested in the case where t > 0, and t ≤T/4. The
factor in (1.57) preceding to Dirichlet’s integral kernel is:
f (t −u) + f (t + u) =
⎧
⎨
⎩
1
for 0 ≤u < t
0
for t ≤u < T/2 −t
−1
for T/2 −t ≤u < T/2
.
(1.60)
Inserting in (1.57) results in:
SN(t) = 2
T
⎧
⎪⎨
⎪⎩
t
0
DN( 2πu
T )du −
T/2

T/2−t
DN( 2πu
T )du
⎫
⎪⎬
⎪⎭
=
⎧
⎪⎨
⎪⎩
1
π
2πt/T

0
DN(x)dx −
0

−2πt/T
DN(x −π)dx
⎫
⎪⎬
⎪⎭
(with x = 2πu
T )
(with x = 2πu
T
−π).
(1.61)
Now we’ll insert the expression of Dirichlet’s kernel as sum of cosine terms and
integrate them:
SN(t) = 1
π

πt
T + sin 2πt
T
1
+ sin 2 2πt
T
2
+ · · · + sin N 2πt
T
N
−
*
πt
T −sin 2πt
T
1
+ sin 2 2πt
T
2
−· · · + (−1)N sin N 2πt
T
N
+)
(1.62)
= 2
π
N

k=1
odd
1
k sin 2πkt
T
.
This function is the expression of the partial sums of the unit step. In Fig.1.13 we
show some approximations.
Figure1.14 shows the 49th partial sum. As we can see, we’re already getting pretty
close to the unit step, but there are overshoots and undershoots near the discontinuity.
Electro-technical engineers know this phenomenon when using ﬁlters with very steep
ﬂanks: the signal “rings”. We could be led to believe that the amplitude of these
overshoots and undershoots will get smaller and smaller, provided only we make
N big enough. We haven’t got a chance! Comparing Fig.1.13 with Fig.1.14 should
have made us think twice. We’ll have a closer look at that, using the following
approximation: N is to be very big and t (or x in (1.61), respectively) very small, i.e.
close to 0.

1.4 Gibbs’ Phenomenon
29
NN
NN
=
===
12
34
−T
2
+ T
2
Fig. 1.13 Partial sum expression of unit step
T
2
−T
2
t
Fig. 1.14 Partial sum expression of unit step for N = 49
Then we may neglect 1/2 with respect to N in the numerator of Dirichlet’s kernel
and replace sin(x/2) by x/2:
DN(x) →sin Nx
x
.
(1.63)
Therefore the partial sum for large N and close to t = 0 becomes:
SN(t) →1
π
2πNt/T

0
sin z
z
dz
(1.64)
with z = Nx.

30
1
Fourier Series
That is the sine integral. We’ll get the extremes at dSN(t)
dt
!= 0. Differentiating
with respect to the upper integral boundary gives:
1
π
2πN
T
sin z
z
!= 0
(1.65)
or z = lπ with l = 1, 2, 3, . . . . The ﬁrst extreme on t1 = T/(2N) is a maximum, the
second extreme at t2 = T/N is a minimum (as can easily be seen). The extremes get
closer and closer to each other for N →∞. How big is SN(t1)? Insertion in (1.64)
gives us the value of the “overshoot”:
SN(t1) →1
π
π

0
sin z
z
dz = 1
2 + 0.0895.
(1.66)
Using the same method we get the value of the “undershoot”:
SN(t2) →1
π
2π

0
sin z
z
dz = 1
2 −0.048.
(1.67)
I bet you’ve noticed that, in the approximation of N big and t small, the value
of the overshoot or undershoot doesn’t depend on N at all any more. Therefore it
doesn’t make sense to make N as big as possible, the overshoots and undershoots
will settle at values of +0.0895 and −0.048 and stay there. We could still show that
the extremes decrease monotonically until t = T/4; thereafter they’ll be mirrored
and increase (cf. Fig.1.14). Now what about our mean squared error for N →∞?
The answer is simple: the mean squared error approaches 0 for N →∞, though
the overshoots and undershoots stay. That’s the trick: as the extremes get closer and
closer to each other, the area covered by the overshoots and the undershoots with the
function f (t) = 1/2 (t > 0) approaches 0 all the same. Integration will only come
up with areas of measure 0 (I’m sure I’ve committed at least a venial sin by putting it
this way). The moral of the story: a kink in the function (non-differentiability) lands
us with an inﬁnite Fourier series, and a step (discontinuity) gives us Gibbs’ “ringing”
to boot. In a nutshell: avoid steps wherever it’s possible!
Playground
1.1 Very Speedy
A broadcasting station transmits on 100 MHz. Calculate the angular frequency ω and
the period T for one complete oscillation. How far travels an electromagnetic pulse
(or a light pulse!) in this time? Use the vacuum velocity of light c ≈3 × 108 m/s.

Playground
31
1.2 Totally Odd
Given is the function f (t) = cos(πt/2) for 0 < t ≤1 with periodic continuation.
Plot this function. Is this function even, odd, or mixed? If it is mixed, decompose it
into even and odd components and plot them.
1.3 Absolutely True
Calculate the complex Fourier coefﬁcients Ck for f (t) = sin πt for 0 ≤t ≤1 with
periodic continuation. Plot f (t) with periodic continuation. Write down the ﬁrst four
terms in the series expansion.
1.4 Rather Complex
Calculate the complex Fourier coefﬁcients Ck for f (t) = 2 sin(3πt/2) cos(πt/2)
for 0 ≤t ≤1 with periodic continuation. Plot f (t).
1.5 Shiftily
Shift the function f (t) = 2 sin(3πt/2) cos(πt/2) = sin πt + sin 2πt for 0 ≤t ≤1
with periodic continuation by a = −1/2 to the left and calculate the complex Fourier
coefﬁcient Ck. Plot the shifted f (t) and its decomposition into ﬁrst and second parts
and discuss the result.
1.6 Cubed
Calculate the complex Fourier coefﬁcients Ck for f (t) = cos3 2πt for 0 ≤t ≤1
with periodic continuation. Plot this function. Now use (1.5) and the Second Shifting
Rule to check your result.
1.7 Tackling Inﬁnity
Derive the result for the inﬁnite series ,∞
k=1 1/k4 using Parseval’s theorem.
Hint: Instead of the triangular function try a parabola!
1.8 Smoothly
Given is the function f (t) = [1 −(2t)2]2 for −1/2 ≤t ≤1/2 with periodic
continuation. Use (1.63) and argue how the Fourier coefﬁcients Ck must depend on
k. Check it by calculating the Ck directly.

Chapter 2
Continuous Fourier Transformation
Abstract This chapter deals with the mapping of arbitrary functions in the time
domain to Fourier transformed functions in the frequency domain. The so-called
δ-function is introduced and forward and inverse transformations are deﬁned and
illustrated with examples. The polar representation of the Fourier transform is given
and shifting rules are discussed. Convolution, cross-correlation, and autocorrelation
with Parseval’s theorem are illustrated with examples. It concludes with a discussion
of pitfalls and truncation errors.
In this chapter we relax the requirement of periodicity of the function f (t). Hence,
instead of discrete Fourier coefﬁcients we end up with the continuous function F(ω).
The integration interval is the entire real axis (−∞, +∞).
Mapping of an Arbitrary Function f(t) to the Fourier-Transformed
Function F(ω)
2.1 Continuous Fourier Transformation
We’lllookatwhathappensatthetransitionfromaseries-toanintegral-representation:
Series:
Ck = 1
T
+T/2

−T/2
f (t)e−2πikt/T dt.
(2.1)
Now:
T →∞
ωk = 2πk
T
→
ω,
discrete
continuous
lim
T →∞(TCk) →
+∞

−∞
f (t)e−iωtdt.
(2.2)
Before we get into the deﬁnition of the Fourier transformation, we have to do
some homework.
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_2
33

34
2
Continuous Fourier Transformation
2.1.1 Even and Odd Functions
A function is called even, if:
f (−t) = f (t).
(2.3)
A function is called odd, if:
f (−t) = −f (t).
(2.4)
Any general function may be split into an even and an odd part. We’ve heard that
before, at the beginning of Chap.1, and of course it’s true whether the function f (t)
is periodic or not.
2.1.2 The δ-Function
The δ-function is a distribution,1 not a function. In spite of that, it’s always called
δ-function. Its value is zero anywhere except when its argument is equal to 0. In
this case it is ∞. If you think that’s too steep or pointed for you, you may prefer a
different deﬁnition:
δ(t) = lim
a→∞fa(t)
with fa(t) =

a for −1
2a ≤t ≤1
2a
0 else
.
(2.5)
Nowwehaveapulseforthedurationof−1/2a ≤t ≤1/2a withheighta andkeep
diminishing the width of the pulse while keeping the area unchanged (normalised to
1), viz. the height goes up while the width gets smaller. That’s the reason why the
δ-function often is also called impulse. At the end of the previous chapter we already
had heard about a representation of the δ-function: Dirichlet’s kernel for N →∞.
If we restrict things to the basis interval −π ≤t ≤+π, we get:
+π

−π
DN(x)dx = π, independent of N,
(2.6)
1Generalised function. The theory of distributions is an important basis of modern analysis, and
impossible to understand without additional reading. A more in-depth treatment of its theory, how-
ever, is not required for the applications in this book.

2.1 Continuous Fourier Transformation
35
and thus:
1
π lim
N→∞
+π

−π
f (t)DN(t)dt = f (0).
(2.7)
In the same way, the δ-function “picks” the integrand where the latter’s argument
is 0 during integration (we always have to integrate over the δ-function!):
+∞

−∞
f (t)δ(t)dt = f (0).
(2.8)
Another representation for the δ-function, which we’ll frequently use, is:
δ(ω) = 1
2π
+∞

−∞
eiωtdt.
(2.9)
Purists may multiply the integrand with a damping-factor, for example e−α|t|, and
then introduce limα→0. This won’t change the fact that everything gets “oscillated”
or averaged away for all frequencies ω ̸= 0 (venial sin: let’s think in whole periods
for once!), whereas for ω = 0 integration will be over the integrand 1 from −∞to
+∞, i.e. the result will have to be ∞.
2.1.3 Forward and Inverse Transformation
Let’s deﬁne:
Deﬁnition 2.1 (Forward transformation)
F(ω) =
+∞

−∞
f (t)e−iωtdt.
(2.10)
Deﬁnition 2.2 (Inverse transformation)
f (t) = 1
2π
+∞

−∞
F(ω)e+iωtdω.
(2.11)

36
2
Continuous Fourier Transformation
Caution:
i. In the case of the forward transformation, there is a minus sign in the exponent
(cf. (1.27)), in the case of the inverse transformation, this is a plus sign.
ii. In the case of the inverse transformation, 1/2π is in front of the integral, contrary
to the forward transformation.
The asymmetric aspect of the formulas has tempted many scientists to introduce
other deﬁnitions, for example to write a factor 1/√(2π) for forward as well as inverse
transformation. That’s no good, as the deﬁnition of the average F(0) =
 +∞
−∞f (t)dt
would be affected. Weaver’s representation is correct, though not widely used:
Forward transformation:
F(ν) =
+∞

−∞
f (t)e−2πiνtdt,
Inverse transformation:
f (t) =
+∞

−∞
F(ν)e2πiνtdν.
Weaver, as can be seen, doesn’t use the angular frequency ω, but rather the fre-
quency ν. This effectively made the formulas look symmetrical, though it saddles us
with many factors 2π in the exponent. We’ll stick to the deﬁnitions (2.10) and (2.11).
We now want to demonstrate, that the inverse transformation returns us to the
original function. For the forward transformation, we often will use FT( f (t)), and
for the inverse transformation we’ll use FT−1(F(ω)). We’ll start with the inverse
transformation and insert:
f (t) = 1
2π
+∞

−∞
F(ω)eiωtdω = 1
2π
+∞

−∞
dω
+∞

−∞
f (t′)e−iωt′eiωtdt′
= 1
2π
+∞

−∞
f (t′)dt′
+∞

−∞
eiω(t−t′)dω
interchange integration
(2.12)
=
+∞

−∞
f (t′)δ(t −t′)dt′ = f (t) .
Q.e.d.2 Here we have used (2.8) and (2.9). For f (t) = 1 we get:
FT(δ(t)) = 1.
(2.13)
2In Latin: “quod erat demonstrandum”, “what we’ve set out to prove”.

2.1 Continuous Fourier Transformation
37
The impulse therefore requires all frequencies with unity amplitude for its Fourier
representation (“white” spectrum). Conversely:
FT(1) = 2πδ(ω).
(2.14)
The constant 1 can be represented by a single spectral component, viz. ω = 0. No
others occur. As we have integrated from −∞to +∞, naturally an ω = 0 will also
result in inﬁnity for intensity.
We realise the dual character of the forward and inverse transformations: a very
slowly varying function f (t) will have a very high spectral density for very small
frequencies; the spectral density will go down quickly and rapidly approaches 0.
Conversely, a quickly varying function f (t) will show spectral density over a very
wide frequency range: Fig.2.1 explains this once again.
Let’s discuss a few examples now.
Example 2.1 (“Rectangle, even”)
f (t) =
1 for
−T/2 ≤t ≤T/2
0 else
.
F(ω) = 2
T/2

0
cos ωtdt = T sin(ωT/2)
ωT/2
.
(2.15)
The imaginary part is 0, as f (t) is even. The Fourier transformation of a rectan-
gular function therefore is of the type sin x
x . Some authors use the expression sinc(x)
Fig. 2.1 A slowly-varying function has only low-frequency spectral components (top); a rapidly-
falling function has spectral components spanning a wide range of frequencies (bottom)

38
2
Continuous Fourier Transformation
f(t)
t
−T
2
+ T
2
F(ω)
ω
F(ω) = T sin(ωT/2)
ωT/2
Fig. 2.2 “Rectangular function” and Fourier transformation of type sin x
x
for this case. What the “c” stands for, I don’t know.3 The “c” already has been “used
up” when deﬁning the complementary error-function erfc(x) = 1 −erf(x). That’s
why we’d rather stick to sin x
x . These functions f (t) and F(ω) are shown in Fig.2.2.
They’ll keep us busy for quite a while.
Keen readers would have spotted the following immediately: if we made the
interval smaller and smaller, and did not ﬁx f (t) at 1 in return, but let it grow at
the same rate as T decreases (“so the area under the curve stays constant”), then in
limT →∞we would have a new representation of the δ-function. Again, we get the
case where over- and undershoots on the one hand get closer to each other when
T gets smaller, but on the other hand, their amplitude doesn’t decrease. The shape
sin x
x
will stay the same. As we’re already familiar with Gibbs’ phenomenon in the
case of steps, this naturally won’t surprise us any more. Contrary to the discussion
in Sect.1.4.3 we don’t have a periodic continuation of f (t) beyond the integration
interval, i.e. there are two steps (one up, one down). It’s irrelevant that f (t) on average
isn’t 0. It’s important that for:
ω →0
sin(ωT/2)/(ωT/2) →1
(use l’Hospital’s rule or sin x ≈x for small x).
Now, we calculate the Fourier transform of important functions. Let’s start with
the Gaussian.
Example 2.2 (The normalised Gaussian) The prefactor is chosen in such a way that
the area is 1.
f (t) =
1
σ
√
2π
e−1
2
t2
σ2 .
F(ω) =
1
σ
√
2π
+∞

−∞
e−1
2
t2
σ2 e−iωtdt
(2.16)
3It stands for “sinus cardinalis”, but what is “cardinalis”? Has nothing to do with the catholic church,
I guess.

2.1 Continuous Fourier Transformation
39
=
2
σ
√
2π
+∞

0
e−1
2
t2
σ2 cos ωtdt
= e−1
2 σ2ω2.
Again, the imaginary part is 0, as f (t) is even. The Fourier transform of a Gaussian
results in another Gaussian. Note that the Fourier transform is not normalised to area
1. The 1/2 occurring in the exponent is handy (could also have been absorbed into
σ), as the following is true for this representation:
σ =
√
2 ln 2 × HWHM (half width at half maximum = HWHM)
= 1.177 × HWHM.
(2.17)
f (t) has σ in the exponent’s denominator, F(ω) in the numerator: the slimmer
f (t), the wider F(ω) and vice versa (cf. Fig.2.3).
Example 2.3 (Bilateral exponential function)
f (t) = e−|t|/τ.
F(ω) =
+∞

−∞
e−|t|/τe−iωtdt = 2
+∞

0
e−t/τ cos ωtdt =
2τ
1 + ω2τ2 .
(2.18)
As f (t) is even, the imaginary part is 0. The Fourier transform of the exponential
function is a Lorentzian (cf. Fig.2.4).
f(t)
t
f(t) =
1
σ
√
2πe−1
2
t2
σ2
1
σ
√
2π
F(ω)
ω
F(ω) = e−1
2 σ2ω2
1
Fig. 2.3 Gaussian and Fourier transform (=equally a Gaussian)

40
2
Continuous Fourier Transformation
1
f(t)
t
f(t) = e−|t|
τ
2τ
F(ω)
ω
F(ω) =
2τ
1+ω2τ 2
Fig. 2.4 Bilateral exponential function and Fourier transformation (=Lorentzian)
Example 2.4 (Unilateral exponential function)
f (t) =

e−λt for t ≥0
0
else
.
(2.19)
F(ω) =
∞

0
e−λte−iωtdt =
e−(λ+iω)t
−(λ + iω)

+∞
0
(2.20)
=
1
λ + iω =
λ
λ2 + ω2 +
−iω
λ2 + ω2 .
(2.21)
(Sorry: When integrating in the complex plane, we really should have used the
Residue Theorem4 instead of integrating in a rather cavalier fashion. The result,
however, is correct all the same.)
F(ω) is complex, as f (t) is neither even nor odd. We now can write the real and
the imaginary parts separately. The real part has a Lorentzian shape we’re familiar
with by now, and the imaginary part has a dispersion shape. Often the so-called polar
representation is used, too, so we’ll deal with that one in the next section.
Examples in physics: the damped oscillation that is used to describe the emission
of a particle (for example a photon, a γ-quantum) from an excited nuclear state with a
lifetime of τ (meaning, that the excited state depopulates according to e−t/τ), results
in a Lorentzian-shaped emission-line. Exponential relaxation processes will result
in Lorentzian-shaped spectral-lines, for example in the case of nuclear magnetic
resonance.
4The Residue Theorem is part of the theory of functions of complex variables.

2.1 Continuous Fourier Transformation
41
2.1.4 Polar Representation of the Fourier Transform
Every complex number z = a + ib can be represented in the complex plane by its
magnitude and phase ϕ (Fig.2.5):
z = a + ib =

a2 + b2 eiϕ with tan ϕ = b/a.
This allows us to represent the Fourier transform of the “unilateral” exponential
function as in Fig.2.6.
Alternatively to the polar representation we can also represent the real and imag-
inary parts separately (cf. Fig.2.7).
Please note that |F(ω)| is no Lorentzian! If you want to “stick” to this property,
you better represent the square of the magnitude: |F(ω)|2 = 1/(λ2 + ω2) is a
Lorentzian again. This representation is often also called the power representation:
|F(ω)|2 = (real part)2 + (imaginary part)2. The phase goes to 0 at the maximum of
|F(ω)|, i.e. when “in resonance”.
Warning: The representation of the magnitude as well as of the squared magnitude
does away with the linearity of the Fourier transformation!
Finally, let’s try out the inverse transformation and ﬁnd out how we return to
the “unilateral” exponential function (the Fourier transform didn’t look all that
“unilateral”!).
radius
√
a2 + b2
imaginary part
real part
ϕ
a
b
Fig. 2.5 Polar representation of a complex number z = a + ib
Fig. 2.6 Unilateral exponential function, magnitude of the Fourier transform and phase (imaginary
part/real part)

42
2
Continuous Fourier Transformation
Re{F(ω)}
ω
Im{F(ω)}
ω
Fig. 2.7 Real part and imaginary part of the Fourier transform of a unilateral exponential function
f (t) = 1
2π
+∞

−∞
λ −iω
λ2 + ω2 eiωtdω
= 1
2π
⎧
⎨
⎩2λ
+∞

0
cos ωt
λ2 + ω2 dω + 2
+∞

0
ω sin ωt
λ2 + ω2 dω
⎫
⎬
⎭
= 1
π
π
2 e−|λt| ± π
2 e−|λt|
, where “ + ” for t ≥0
“ −” for t < 0 is valid
=

e−λt for t ≥0
0
else
.
(2.22)
2.2 Theorems and Rules
2.2.1 Linearity Theorem
For completeness’ sake, once again:
f (t) ↔F(ω),
g(t) ↔G(ω),
a · f (t) + b · g(t) ↔a · F(ω) + b · G(ω).
(2.23)
2.2.2 The First Shifting Rule
We already know: shifting in the time domain means modulation in the frequency
domain:
f (t) ↔F(ω),
f (t −a) ↔F(ω)e−iωa.
(2.24)

2.2 Theorems and Rules
43
The proof is quite simple.
Example 2.5 (“Rectangular function”)
f (t) =
 1 for T/2 ≤t ≤T/2
0 else
.
(2.25)
F(ω) = T sin(ωT/2)
ωT/2
.
Fig. 2.8 “Rectangular function”, real part, imaginary part, magnitude of Fourier transform (left
from top to bottom); for the “rectangular function”, shifted to the right by T/2 (right from top to
bottom)

44
2
Continuous Fourier Transformation
Now we shift the rectangle f (t) by a = T/2 →g(t), and then get (see Fig.2.8):
G(ω) = T sin(ωT/2)
ωT/2
e−iωT/2
(2.26)
= T sin(ωT/2)
ωT/2
(cos(ωT/2) −i sin(ωT/2)).
The real part gets modulated with cos(ωT/2). The imaginary part which before
was 0, now is unequal to 0 and “complements” the real part exactly, so |F(ω)| stays
the same. Equation (2.24) contains “only” a phase factor e−iωa, which is irrelevant as
far as the magnitude is concerned. As long as you only look at the power spectrum,
you may shift the function f (t) along the time-axis as much as you want: you won’t
notice any effect. In the phase of the polar representation, however, you’ll see the
shift again:
tan ϕ = imaginary part
real part
= −sin(ωT/2)
cos(ωT/2) = −tan(ωT/2)
or ϕ = −ωT/2.
(2.27)
Don’t worry about the phase ϕ overshooting ±π/2.
2.2.3 The Second Shifting Rule
We already know: a modulation in the time domain results in a shift in the frequency
domain:
f (t) ↔F(ω),
f (t)eiω0t ↔F(ω −ω0).
(2.28)
If you prefer real modulations, you may write:
FT( f (t) cos ω0t) = F(ω + ω0) + F(ω −ω0)
2
,
(2.29)
FT( f (t) sin ω0t) = i F(ω + ω0) −F(ω −ω0)
2
.
This follows from Euler’s identity (1.22) straight away.

2.2 Theorems and Rules
45
Fig. 2.9 Fourier transform of g(t) = cos ωt in the interval −T/2 ≤t ≤T/2
Fig. 2.10 Superposition of sin x
x
sidelobes at small frequencies for negative and positive (left) and
positive frequencies only (right)

46
2
Continuous Fourier Transformation
Example 2.6 (“Rectangular function”)
f (t) =
1 for −T/2 ≤t ≤+T/2
0 else
.
F(ω) = T sin(ωT/2)
ωT/2
(cf. (2.15))
and
g(t) = cos ω0t.
(2.30)
Using h(t) = f (t) · g(t) and the Second Shifting Rule we get:
H(ω) = T
2
sin[(ω + ω0)T/2]
(ω + ω0)T/2
+ sin[(ω −ω0)T/2]
(ω −ω0)T/2

.
(2.31)
This means: the Fourier transform of the function cos ω0t within the interval
−T/2 ≤t ≤T/2 (and outside equal to 0) consists of two frequency peaks, one at
ω = −ω0 and another one at ω = +ω0. The amplitude naturally gets split evenly
(“between brothers”). If we had ω0 = 0, then we’d get the central peak ω = 0 once
again; increasing ω0 splits this peak into two peaks, moving to the left and the right
(cf. Fig.2.9).
If you don’t like negative frequencies, you may ﬂip the negative half-plane, so
you’ll only get one peak at ω = ω0 with twice (that’s the original) intensity.
Caution: For small frequencies ω0 the sidelobes of the function sin x
x
tend to “rub
shoulders”, meaning that they interfere with each other. Even ﬂipping the negative
half-plane won’t help that. Figure2.10 explains the problem.
2.2.4 Scaling Theorem
Similar to (1.41) the following is true:
f (t) ↔F(ω),
f (at) ↔1
|a| F
ω
a

.
(2.32)

2.2 Theorems and Rules
47
Proof (Scaling) Analogously to (1.41) with the difference that here we cannot stretch
or compress the interval limits ±∞:
F(ω)new = 1
T
+∞

−∞
f (at)e−iωt dt
= 1
T
+∞

−∞
f (t′)e−iωt′/a 1
a dt′
with t′ = at
= 1
|a| F(ω)old
with ω = ωold
a . ⊓⊔
Here, we tacitly assumed a > 0. For a < 0 we would get a minus sign in the
prefactor; however, we would also have to interchange the integration limits and thus
get together the factor 1
|a|. This means: stretching (compressing) the time-axis results
in the compression (stretching) of the frequency-axis. For the special case a = −1
we get:
f (t)
→F(ω),
f (−t) →F(−ω).
(2.33)
Therefore turning around the time axis (“looking into the past”) results in turning
around the frequency axis. This profound secret will stay hidden to all those unable
to think in anything but positive frequencies.
2.3 Convolution, Cross Correlation, Autocorrelation,
Parseval’s Theorem
2.3.1 Convolution
The convolution of a function f (t) with another function g(t) means:
Deﬁnition 2.3 (Convolution)
f (t) ⊗g(t) ≡
+∞

−∞
f (ξ)g(t −ξ)dξ.
(2.34)

48
2
Continuous Fourier Transformation
Please note there is a minus sign in the argument of g(t). The convolution is
commutative, distributive, and associative. This means:
commutative:
f (t) ⊗g(t) = g(t) ⊗f (t).
Here we have to take into account the sign!
Proof (Convolution, commutative) Substituting the integration variables:
f (t) ⊗g(t) =
+∞

−∞
f (ξ)g(t −ξ)dξ =
+∞

−∞
g(ξ′) f (t −ξ′)dξ′
with ξ′ = t −ξ. ⊓⊔
Distributive:
f (t) ⊗(g(t) + h(t)) = f (t) ⊗g(t) + f (t) ⊗h(t)
(Proof: Linear operation!).
Associative:
f (t) ⊗(g(t) ⊗h(t)) = ( f (t) ⊗g(t)) ⊗h(t)
(the convolution sequence doesn’t matter; proof: double integral with interchange of
integration sequence).
Example 2.7 (Convolution of a “rectangular function” with another “rectangular
function”) We want to convolve the “rectangular function” f (t) with another “rec-
tangular function” g(t):
f (t) =
1 for −T/2 ≤t ≤T/2
0 else
,
g(t) =
1 for 0 ≤t ≤T
0 else
.
h(t) = f (t) ⊗g(t).
(2.35)
According to the deﬁnition in (2.34) we have to mirror g(t) (minus sign in front of
ξ). Then we shift g(t) and calculate the overlap (cf. Fig.2.11).
We get the ﬁrst overlap for t = −T/2 and the last one for t = +3T/2 (cf.
Fig.2.12).
At the limits, where t = −T/2 and t = +3T/2, we start and ﬁnish with an
overlap of 0, the maximum overlap occurs at t = +T/2: there the two rectangles are
exactly on top of each other (or below each other?). The integral then is exactly T ;
in between the integral rises/falls at a linear rate (cf. Fig.2.13).

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
49
f (ξ)
g(−ξ)
f (ξ) · g(−ξ)
ξ
ξ
ξ
T
+T /2
T/ 2
−T/ 2
Fig. 2.11 “Rectangular function” f (ξ), mirrored rectangular function g(−ξ), overlap (from top to
bottom). The area of the overlap gives the convolution integral
f(ξ)
g(t −ξ)
g(t −ξ)
g(t −ξ)
g(t −ξ)
g(t −ξ)
ξ
ξ
ξ
ξ
ξ
ξ
+T/2
−T/2
Overlap
0%
50%
100%
50%
0%
Fig. 2.12 Illustration of the convolution process of f (t) and g(t) with t = −T/2, 0, +T/2, +T ,
+3T/2 (from top to bottom)

50
2
Continuous Fourier Transformation
−T
2
T
2
3T
2
T
t
h(t)
Fig. 2.13 Convolution h(t) = f (t) ⊗g(t)
Please note the following: the interval, where f (t) ⊗g(t) is unequal to 0, now is
twice as big: 2T ! If we had deﬁned g(t) symmetrically around 0in the ﬁrst place (I
didn’t want to do that, so we can’t forget the mirroring!), then also f (t)⊗g(t) would
be symmetrical around 0. In this case we would have convolved f (t) with itself.
Now to a more useful example: let’s take a pulse that looks like a “unilateral”
exponential function:
f (t) =

e−t/τ for t ≥0
0
else
.
(2.36)
Any device that delivers pulses as a function of time, has a ﬁnite rise-/decay-time,
which for simplicity’s sake we’ll assume to be a Gaussian (see Fig.2.14):
g(t) =
1
σ
√
2π
e−1
2
t2
σ2 .
(2.37)
That is how our device would represent a δ-function—we can’t get sharper than
that. The function g(t) therefore is the device’s resolution function, which we’ll have
to use for the convolution of all signals we want to record. An example would be the
bandwidth of an oscilloscope. We then need:
S(t) = f (t) ⊗g(t),
(2.38)
Fig. 2.14 Illustration of convolution: the Gaussian will be shifted over the unilateral exponential-
function

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
51
where S(t) is the experimental, “smeared” signal. It’s obvious that the rise at t = 0
will not be as steep, and the peak of the exponential function will get “ironed out”.
We’ll have to take a closer look:
S(t) =
1
σ
√
2π
+∞

0
e−ξ/τe−1
2
(t−ξ)2
σ2 dξ
=
1
σ
√
2π
e−1
2
t2
σ2
+∞

0
exp

−ξ
τ + tξ
σ2 −1
2ξ2/σ2




form quadratic complement
dξ
=
1
σ
√
2π
e−1
2
t2
σ2 e
t2
2σ2 e−t
τ e
σ2
2τ2
+∞

0
e
−
1
2σ2

ξ−

t−σ2
τ
2
dξ
(2.39)
=
1
σ
√
2π
e−t
τ e+ σ2
2τ2
+∞

−(t−σ2/τ)
e−
1
2σ2 ξ′2
dξ′
with ξ′ = ξ −

t −σ2
τ

= 1
2e−t
τ e+ σ2
2τ2 erfc
 σ
√
2τ
−
t
σ
√
2

.
Here, erfc(x) = 1−erf(x) is the complementary error-function with the deﬁning
equation:
erf(x) =
2
√π
x

0
e−t2dt.
(2.40)
The functions erf(x) and erfc(x) are shown in Fig.2.15.
The function erfc(x) represents a “smeared” step. Together with the factor 1/2
the height of the step is just 1. As the time in the argument of erfc(x) in (2.39) has a
negative sign, the step of Fig.2.15 is mirrored and also shifted by σ/
√
2τ. Figure2.16
shows the result of the convolution of the exponential function with the Gaussian.
Fig. 2.15 The functions erf(x) and erfc(x)

52
2
Continuous Fourier Transformation
Fig. 2.16 Result of the convolution of a unilateral exponential function with a Gaussian. Exponen-
tial function without convolution (thin line)
The following properties immediately stand out:
i. the ﬁnite time resolution ensures that there also is a signal at negative times,
whereas it was 0 before convolution,
ii. the maximum is not at t = 0 any more,
iii. what can’t be seen straight away, yet is easy to grasp, is the following: the center
of gravity of the exponential function, which was at t = τ, doesn’t get shifted at
all upon convolution. An even function won’t shift the center of gravity! Have a
go and check it out!
It’s easy to remember the shape of the curve in Fig.2.16. Start out with the expo-
nential function with a “90◦-vertical cliff”, and then dump “gravel” to the left and
to the right of it (equal quantities! it’s an even function!): that’s how you get the
gravel-heap for t < 0, demolish the peak and make sure there’s also a gravel-heap
for t > 0, that slowly gets thinner and thinner. Indeed, the inﬂuence of the step will
become less and less important if times get larger and larger, i.e.:
1
2erfc
 σ
√
2τ
−
t
σ
√
2

→1
for t ≫σ2
τ ,
(2.41)
and only the unchanged e−t/τ will remain, however with the constant factor e+ σ2
2τ2 .
This factor is always >1 because we always have more “gravel” poured downwards
than upwards.
Now we prove the extremely important Convolution Theorem:
f (t) ↔F(ω),
g(t) ↔G(ω),
h(t) = f (t) ⊗g(t) ↔H(ω) = F(ω) · G(ω),
(2.42)

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
53
i.e., the convolution integral becomes, through Fourier transformation, a product of
the Fourier-transformed ones.
Proof (Convolution Theorem)
H(ω) =

f (ξ)g(t −ξ)dξ × e−iωtdt
=

f (ξ)e−iωξ

g(t −ξ)e−iω(t−ξ)dt

dξ
↑
expanded
↑
(2.43)
=

f (ξ)e−iωξdξ × G(ω)
= F(ω) × G(ω). ⊓⊔
In the step before the last one, we substituted t′ = t−ξ. The integration boundaries
±∞did not change by doing that, and G(ω) does not depend on ξ.
The inverse Convolution Theorem then is:
f (t) ↔F(ω),
g(t) ↔G(ω),
h(t) = f (t) · g(t) ↔H(ω) =
1
2π F(ω) ⊗G(ω).
(2.44)
Proof (Inverse Convolution Theorem)
H(ω) =

f (t)g(t)e−iωtdt
=
  1
2π

F(ω′)e+iω′tdω′ × 1
2π

G(ω′′)e+iω′′tdω′′

e−iωtdt
=
1
(2π)2

F(ω′)

G(ω′′)

ei(ω′+ω′′−ω)tdt



=2πδ(ω′+ω′′−ω)
dω′dω′′
= 1
2π

F(ω′)G(ω −ω′)dω′
= 1
2π F(ω) ⊗G(ω). ⊓⊔
Caution: Contrary to the Convolution Theorem (2.42), in (2.44) there is a factor
of 1/2π in front of the convolution of the Fourier transforms.
A widely popular exercise is the de-convolution of data: the instruments’ reso-
lution function “smears out” the quickly varying functions, but we naturally want
to reconstruct the data to what they would look like if the resolution function was
inﬁnitely good—provided we precisely knew the resolution function. In principle,
that’s a good idea—and thanks to the Convolution Theorem, not a problem: you

54
2
Continuous Fourier Transformation
Fourier-transform the data, divide by the Fourier-transformed resolution function
and transform it back. For practical applications it doesn’t quite work that way. As
in real life, we can’t transform from −∞to +∞, we need low-pass ﬁlters, in order
not to get “swamped” with oscillations resulting from cut-off errors. Therefore the
advantages of de-convolution are just as quickly lost as gained. Actually, the follow-
ing is obvious: whatever got “smeared” by ﬁnite resolution, can’t be reconstructed
unambiguously. Imagine that a very pointed peak got eroded over millions of years,
so there’s only gravel left at its bottom. Try reconstructing the original peak from
the debris around it! The result might be impressive from an artist’s point of view,
an artefact, but it hasn’t got much to do with the original reality (unfortunately the
word artefact has negative connotations among scientists).
Two useful examples for the Convolution Theorem:
Example 2.8 (Gaussian frequency distribution) Let’s assume we have f (t) =
cos ω0t, and the frequency ω0 is not precisely deﬁned, but is Gaussian distributed:
P(ω) =
1
σ
√
2π
e−1
2
ω2
σ2 .
What we’re measuring then is:
∼
f (t) =
+∞

−∞
1
σ
√
2π
e−1
2
ω2
σ2 cos(ω −ω0)tdω,
(2.45)
i.e. a convolution integral in ω0. Instead of calculating this integral directly, we use
the inverse of the Convolution Theorem (2.44), thus saving work and gaining higher
enlightenment. But watch it! We have to handle the variables carefully. The time t
in (2.45) has nothing to do with the Fourier transformation we need in (2.44). And
the same is true for the integration variable ω. Therefore we rather use t0 and ω0 for
the variable pairs in (2.44). We identify:
F(ω0) =
1
σ
√
2π
e−1
2
ω2
0
σ2
1
2π G(ω0) = cos ω0t
or G(ω0) = 2π cos ω0t.
The inverse transformation of these functions using (2.11) gives us:
f (t0) = 1
2π e−1
2 σ2t2
0
(cf. (2.16) for the inverse problem; don’t forget the factor 1/2π when doing the inverse
transformation!),

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
55
g(t0) = 2π
δ(t0 −t)
2
+ δ(t0 + t)
2

(cf. (2.9) for the inverse problem; use the First Shifting Rule (2.24); don’t forget the
factor 1/2π when doing the inverse transformation!).
Finally we get:
h(t0) = e−1
2 σ2t2
0
δ(t0 −t)
2
+ δ(t0 + t)
2

.
Now the only thing left is to Fourier-transform h(t0). The integration over the
δ-function actually is fun:
∼
f (t) ≡H(ω0) =
+∞

−∞
e−1
2 σ2t2
0
δ(t0 −t)
2
+ δ(t0 + t)
2

e−iω0t0dt0
= e−1
2 σ2t2 cos ω0t.
Now, this was more work than we’d originally thought it would be. But look at
what we’ve gained in insight!
This means: the convolution of a Gaussian distribution in the frequency domain
results in exponential “damping” of the cosine term, where the damping happens
to be the Fourier transform of the frequency distribution. This, of course, is due to
the fact that we have chosen to use a cosine function (i.e. a basis function) for f (t).
P(ω) makes sure that oscillations for ω ̸= ω0 are slightly shifted with respect to
each other, and will more and more superimpose each other destructively in the long
run, averaging out to 0.
Example 2.9 (Lorentzian frequency distribution) Now naturally we’ll know imme-
diately what a convolution with a Lorentzian distribution:
P(ω) = σ
π
1
ω2 + σ2
(2.46)
would do:
∼
f (t) =
+∞

−∞
σ
π
1
ω2 + σ2 cos(ω −ω0)tdω,
h(t0) = FT−1(
∼
f (t)) = e−σt0
δ(t0 −t)
2
+ δ(t0 + t)
2

;
(2.47)
∼
f (t) = e−σt cos ω0t.

56
2
Continuous Fourier Transformation
This is a damped wave. That’s how we would describe the electric ﬁeld of a
Lorentz-shaped spectral line, sent out by an “emitter” with a life time of 1/σ.
These examples are of fundamental importance to physics. Whenever we probe
with plane waves, i.e. eiqx, the answer we get is the Fourier transform of the respective
distribution function of the object. A classical example is the elastic scattering of elec-
trons at nuclei. Here, the form factor F(q) is the Fourier transform of the distribution
function of the nuclear charge density ρ(x). The wave vector q is, apart from a
prefactor, identical with the momentum.
Example 2.10 (Gaussian convolved with Gaussian) We perform a convolution of a
Gaussian with σ1 with another Gaussian with σ2. As the Fourier transforms are Gaus-
sians again—yet with σ2
1 and σ2
2 in the numerator of the exponent—it’s immediately
obvious that σ2
total = σ2
1 + σ2
2. Therefore, we get another Gaussian with geometric
addition of the widths σ1 and σ2.
2.3.2 Cross Correlation
Sometimes we want to know if a measured function f (t) has anything in common
with another measured function g(t). Cross correlation is ideally suited to that.
Deﬁnition 2.4 (Cross correlation)
h(t) =
+∞

−∞
f (ξ)g∗(t + ξ)dξ ≡f (t) ⋆g(t).
(2.48)
Watch it: Here, there is a plus sign in the argument of g, therefore we don’t mirror
g(t). For even functions g(t), this, however, doesn’t matter.
The asterisk * means: complex conjugated. We may disregard it for real functions.
The symbol ⋆means: cross correlation, and is not to be confounded with ⊗for
convolution. Cross correlation is associative and distributive, yet not commutative.
That’s not only because of the complex-conjugated symbol, but mainly because of
the plus sign in the argument of g(t). Of course we want to convert the integral in
the cross correlation to a product by using Fourier transformation.
f (t) ↔F(ω),
g(t) ↔G(ω),
h(t) = f (t) ⋆g(t) ↔H(ω) = F(ω)G∗(ω).
(2.49)
Proof (Fourier transform of cross correlation)

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
57
H(ω) =

f (ξ)g∗(t + ξ)dξ · e−iωtdt
=

f (ξ)

g∗(t + ξ)e−iωtdt

dξ
First Shifting Rule complex conjugated with ξ = −a
(2.50)
=

f (ξ)G∗(+ω)e−iωξdξ
= F(ω)G∗(ω). ⊓⊔
Here we used the following identity:
G(ω) =

g(t)e−iωtdt
(take both sides complex conjugated)
G∗(ω) =

g∗(t)eiωtdt
(2.51)
G∗(−ω) =

g∗(t)e−iωtdt
(ω to be replaced by −ω).
The interpretation of (2.49) is simple: if the spectral densities of f (t) and g(t) are
a good match, i.e. have much in common, then H(ω) will become large on average,
and the cross correlation h(t) will also be large, on average. Otherwise if F(ω) would
be small e.g., where G∗(ω) is large and vice versa, so that there is never much left for
the product H(ω). Then also h(t) would be small, i.e. there is not much in common
between f (t) and g(t).
A maybe somewhat extreme example is the technique of “Lock-in ampliﬁcation”,
used to “dig up” small signals buried deeply in the noise. In this case we modulate
the measured signal with a so-called carrier frequency, detect an extremely narrow
spectral range—provided the desired signal does have spectral components in exactly
this spectral width—and often additionally make use of phase information, too.
Anything that doesn’t correlate with the carrier frequency, gets discarded, so we’re
only left with the noise close to the working frequency.
2.3.3 Autocorrelation
The autocorrelation function is the cross correlation of a function f (t) with itself.
You may ask, for what purpose we’d want to check for what f (t) has in common with
f (t). Autocorrelation, however, seems to attract many people in a magic manner.
We often hear the view, that a signal full of noise can be turned into something

58
2
Continuous Fourier Transformation
really good by using the autocorrelation function, i.e. the signal-to-noise ratio would
improve a lot. Don’t you believe a word of it! We’ll see why shortly.
Deﬁnition 2.5 (Autocorrelation)
h(t) =

f (ξ) f ∗(ξ + t)dξ.
(2.52)
We get:
f (t) ↔F(ω),
h(t) = f (t) ⋆f (t) ↔H(ω) = F(ω)F∗(ω) = |F(ω)|2.
(2.53)
We may either use the Fourier transform F(ω) of a noisy function f (t) and get
angry about the noise in F(ω). Or we ﬁrst form the autocorrelation function h(t)
from the function f (t) and are then happy about the Fourier transform H(ω) of
function h(t). Normally, H(ω) does look a lot less noisy, indeed. Instead of doing
it the roundabout way by using the autocorrelation function, we could have used
the square of the magnitude of F(ω) in the ﬁrst place. We all know, that a squared
representation in the ordinate always pleases the eye, if we want to do cosmetics to
a noisy spectrum. Big spectral components will grow when squared, small ones will
get even smaller (cf. New Testament, Matthew 13:12: “For to him who has will more
be given but from him who has not, even the little he has will be taken away.”). But
isn’t it rather obvious that squaring doesn’t change anything to the signal-to-noise
ratio? In order to make it “look good”, we pay the price of losing linearity.
Then,whatisautocorrelationgoodfor?Aclassicalexamplecomesfromfemtosec-
ond measuring devices. A femtosecond is one part in a thousand trillion (US)—or a
thousand billion (British)—of a second, not a particularly long time, indeed. Today,
it is possible to produce such short laser pulses. How can we measure such short
times? Using electronic stop-watches we can reach the range of 100ps; hence, these
“watches” are too slow by 5 orders of magnitude. Precision engineering does the
job! Light travels in a femtosecond a distance of about 300nm, i.e. about 1/100 of a
hair diameter. Today you can buy positioning devices with nanometer precision. The
trick: Split the laser pulse into two pulses, let them travel a slightly different optical
length using mirrors, and combine them afterwards. The detector is an “optical coin-
cidence” which yields an output only if both pulses overlap. By tuning the optical
path (using the nanometer screw!) you can “shift” one pulse over the other, i.e. you
perform a cross correlation of the pulse with itself (for purists: with its exact copy).
The entire system is called autocorrelator.
2.3.4 Parseval’s Theorem
The autocorrelation function also comes in handy for something else, namely for
deriving Parseval’s theorem. We start out with (2.52), insert especially t = 0, and

2.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
59
get Parseval’s theorem:
h(0) =

| f (ξ)|2dξ = 1
2π

|F(ω)|2dω.
(2.54)
We get the second equal sign by inverse transformation of |F(ω)|2, in which for
t = 0 the factor eiωt becomes unity.
Equation (2.54) states, that the “information content” of the function f (x)—
deﬁned as integral over the square of the magnitude—is just as large as the “infor-
mation content” of its Fourier transform F(ω)—deﬁned as integral over the square
of the magnitude of F(ω) divided by 2π. Let’s check this out straight away using an
example, namely our much-used “rectangular function”!
Example 2.11 (“Rectangular function”)
f (t) =
1 for −T/2 ≤t ≤T/2
0 else
.
We get on the one hand:
+∞

−∞
| f (t)|2dt =
+T/2

−T/2
dt = T
and on the other hand:
F(ω) = T sin(ωT/2)
ωT/2
, thus
1
2π
+∞

−∞
|F(ω)|2dω = 2T 2
2π
+∞

0
sin(ωT/2)
ωT/2
2
dω
(2.55)
= 2T 2
2π
2
T
+∞

0
sin x
x
2
dx = T
with x = ωT/2.
It’s easily understood that Parseval’s theorem contains the squared magnitudes
of both f (t) and F(ω): anything unequal to 0 has information, regardless if it’s
positive or negative. The power spectrum is important, the phase doesn’t matter. Of
course we can use Parseval’s theorem to calculate integrals. Let’s simply take the
last example for integration over
 sin x
x
2. We need an integration table for that one,
whereas integrating over 1, that’s determining the area of a square, is elementary.

60
2
Continuous Fourier Transformation
2.4 Fourier Transformation of Derivatives
When solving differential equations, we can make life easier using Fourier transfor-
mation. The derivative simply becomes a product:
f (t) ↔F(ω),
f ′(t) ↔iωF(ω).
(2.56)
Proof (Fourier transformation of derivatives with respect to t) The abbreviation FT
denotes the Fourier transformation:
FT( f ′(t)) =
+∞

−∞
f ′(t)e−iωtdt = f (t)e−iωt
+∞
−∞−(−iω)
+∞

−∞
f (t)e−iωtdt
partial integration
= iωF(ω). ⊓⊔
The ﬁrst term in the partial integration is discarded, as f (t) →0 for t →±∞.
Otherwise we would run into trouble with the integration, like we did at the end of
Sect.2.1.2. This game can go on:
FT
d f n(t)
dnt

= (iω)n F(ω).
(2.57)
For negative n we may also use the formula for integration. We can also formulate
in a simple way the derivative of a Fourier transform F(ω) with respect to the
frequency ω:
dF(ω)
dω
= −iFT(t f (t)).
(2.58)
Proof (Fourier transformation of derivatives with respect to ω)
dF(ω)
dω
=
+∞

−∞
f (t) d
dω e−iωtdt = −i
+∞

−∞
f (t)te−iωtdt = −iFT(t f (t)). ⊓⊔
Weaver [2] gives a neat example for the application of Fourier transformation:
Example 2.12 (Wave equation) The wave equation:
d2u(x, t)
dt2
= c2 d2u(x, t)
dx2
(2.59)

2.4 Fourier Transformation of Derivatives
61
can be made into an oscillation equation using Fourier transformation of the local
variable, which is much easier to solve. We assume:
U(ξ, t) =
+∞

−∞
u(x, t)e−iξxdx.
Then we get:
FT
d2u(x, t)
dx2

= (iξ)2U(ξ, t),
(2.60)
FT
d2u(x, t)
dt2

= d2
dt2 U(ξ, t),
and all together:
d2U(ξ, t)
dt2
= −c2ξ2U(ξ, t).
The solution of this equations is:
U(ξ, t) = P(ξ) cos(cξt),
where P(ξ) is the Fourier transform of the starting proﬁle p(x):
P(ξ) = FT(p(x)) = U(ξ, 0).
The inverse transformation gives us two proﬁles propagating to the left and to the
right:
u(x, t) = 1
2π
+∞

−∞
P(ξ) cos(cξt)eiξxdξ
= 1
2π
1
2
+∞

−∞
P(ξ)

eiξ(x+ct) + eiξ(x−ct)
dξ
(2.61)
= 1
2 p(x + ct) + 1
2 p(x −ct).
As we had no dispersion term in the wave equation, the proﬁles are conserved
(cf. Fig.2.17).

62
2
Continuous Fourier Transformation
Fig. 2.17 Two starting proﬁles p(x) propagating to the left and the right as solutions of the wave
equation
2.5 Pitfalls
2.5.1 “Turn 1 into 3”
Just for fun, we’ll get into magic now: let’s take a unilateral exponential function:
f (t) =

e−λt for t ≥0
0
else
with F(ω) =
1
λ + iω
(2.62)
and |F(ω)|2 =
1
λ2 + ω2 .
We put this function (temporarily) on a unilateral “pedestal”:
g(t) =
1 for t ≥0
0 else
(2.63)
with G(ω) = 1
iω .
We arrive at the Fourier transform of Heaviside’s step function g(t) from the
Fourier transform for the exponential function for λ →0. We therefore have: h(t) =
f (t) + g(t). Because of the linearity of the Fourier transformation:
H(ω) =
1
λ + iω + 1
iω =
λ
λ2 + ω2 −
iω
λ2 + ω2 −i
ω .
(2.64)
This results in:

2.5 Pitfalls
63
|H(ω)|2 =

λ
λ2 + ω2 −
iω
λ2 + ω2 −i
ω

×

λ
λ2 + ω2 +
iω
λ2 + ω2 + i
ω

=
λ2
(λ2 + ω2)2 + 1
ω2 +
ω2
(λ2 + ω2)2 +
2ω
(λ2 + ω2)ω
=
1
λ2 + ω2 + 1
ω2 +
2
λ2 + ω2
=
3
λ2 + ω2 + 1
ω2 .
Now we return |G(ω)|2 = 1/ω2, i.e. the square of the Fourier transform of the
pedestal, and have gained, compared to |F(ω)|2, a factor of 3. And we only had
to temporarily “borrow” the pedestal to achieve that?! Of course (2.64) is correct.
Returning |G(ω)|2 wasn’t. We borrowed the interference term we got when squaring
the magnitude, as well, and have to return it, too. This inference term amounts to just
2/(λ2 + ω2).
Now let’s approach the problem somewhat more academically. Assuming we have
h(t) = f (t) + g(t) with the Fourier transforms F(ω) and G(ω). We now use the
polar representation:
F(ω) = |F(ω)|eiϕ f
and
(2.65)
G(ω) = |G(ω)|eiϕg.
This gives us:
H(ω) = |F(ω)|eiϕ f + |G(ω)|eiϕg,
(2.66)
which is, due to the linearity of the Fourier transformation, entirely correct. However,
if we want to calculate |H(ω)|2 (or the square root of it), we get:
|H(ω)|2 =

|F(ω)|eiϕ f + |G(ω)|eiϕg
 
|F(ω)|e−iϕ f + |G(ω)|e−iϕg

(2.67)
= |F(ω)|2 + |G(ω)|2 + 2|F(ω)| × |G(ω)| × cos(ϕ f −ϕg) .
If the phase difference (ϕ f −ϕg) doesn’t happen to be 90◦(modulo 2π), the
interference term does not cancel. Don’t think you’re on the safe side with real Fourier
transforms. The phases are then 0, and the interference term reaches a maximum.
The following example will illustrate this:
Example 2.13 (Overlappinglines)Let’staketwospectrallines—sayofshape sin x
x —
that approach each other. H(ω) simply is a linear superposition5 of the two lines,
yet not |H(ω)|2. As soon as the two lines start to overlap, there also will be an
interference term. To use a concrete example, let’s take the function of (2.31) and,
5i.e. addition.

64
2
Continuous Fourier Transformation
|Htotal(ω)|2
ω
|H1(ω)|2 + |H2(ω)|2
ω
Fig. 2.18 Superposition of two
 sin x
x

-functions. Power representation with interference term (left);
power representation without interference term (right)
for simplicity’s sake, ﬂip the negative frequency axis to the positive axis. Then we
get:
Htotal(ω) = H1 + H2
= T
sin[(ω −ω1)T/2]
(ω −ω1)T/2
+ sin[(ω −ω2)T/2]
(ω −ω2)T/2

.
(2.68)
The phases are 0, as we have used two cosine functions cos ω1t and cos ω2t for
input. So |H(ω)|2 becomes:
|Htotal(ω)|2 = T 2
sin[(ω −ω1)T/2]
(ω −ω1)T/2
2
+
sin[(ω −ω2)T/2]
(ω −ω2)T/2
2
+ 2sin[(ω −ω1)T/2]
(ω −ω1)T/2
× sin[(ω −ω2)T/2]
(ω −ω2)T/2

(2.69)
= T 2 
|H1(ω)|2 + H∗
1 (ω)H2(ω)
+ H1(ω)H∗
2 (ω) + |H2(ω)|2
.
Figure2.18 backs up the facts: for overlapping lines, the interference term makes
sure that in the power representation the lineshape is not the sum of the power
representation of the lines. Fix: Show real and imaginary parts separately. If you
want to keep the linear superposition (it’s so useful), then you have to stay clear of
the squaring!
2.5.2 Truncation Error
We now want to look at what will happen, if we truncate the function f (t)
somewhere—preferably where it isn’t large any more—and then Fourier-transform
it. Let’s take a simple example:

2.5 Pitfalls
65
Example 2.14 (Truncation error)
f (t) =

e−λt for 0 ≤t ≤T
0
else
.
(2.70)
The Fourier transform then is:
F(ω) =
T

0
e−λte−iωtdt =
1
−λ −iω e−λt−iωt

T
0
= 1 −e−λT −iωT
λ + iω
.
(2.71)
Compared to the untruncated exponential function, we’re now saddled with the
additional term −e−λT e−iωT /(λ+iω). For large values of T it isn’t all that large, but
to our grief, it oscillates. Truncating the smooth Lorentzian gave us small oscillations
in return. Figure2.19 explains that (cf. Fig.2.7 without truncation).
The morale of the story: don’t truncate if you don’t have to, and most certainly
neither brusquely nor brutally. How it should be done—if you’ve got to do it—will
be explained in the next chapter.
Finally, an example how not to do it:
Example 2.15 (Exponential on pedestal) We’ll once again use our truncated expo-
nential function and put it on a pedestal, that’s only nonzero between 0 ≤t ≤T .
Assume a height of a:
f (t) =

e−λt for 0 ≤t ≤T
0
else
with F(ω) = 1 −e−λT e−iωT
λ + iω
,
g(t) =
a for 0 ≤t ≤T
0 else
with G(ω) = a 1 −e−iωT
iω
.
(2.72)
Here, to calculate G(ω), we’ve again used F(ω), with λ = 0. |F(ω)|2 we’ve
already met in Fig.2.19. Re{G(ω)} as well as Im{G(ω)} are shown in Fig.2.20.
Finally, in Fig.2.21 |H(ω)|2 is shown, decomposed into |F(ω)|2, |G(ω)|2 and the
interference term.
Fig. 2.19 Fourier transform of the truncated unilateral exponential function

66
2
Continuous Fourier Transformation
Fig. 2.20 Fourier transform of the pedestal
Fig. 2.21 Power representation of Fourier transform of a unilateral exponential function on a
pedestal (top left), the unilateral exponential function (top right); Power representation of the
Fourier transform of the pedestal (bottom left) and representation of the interference term (bottom
right)
For this ﬁgure we picked the function 5e−5t/T + 2 in the interval 0 ≤t ≤T .
The exponential function therefore already dropped to e−5 at truncation, the step
with a = 2 isn’t all that high either. Therefore neither |F(ω)|2 nor |G(ω)|2 look all

2.5 Pitfalls
67
that terrible either, but |H(ω)|2 does. It’s the interference term’s fault. The truncated
exponential function on the pedestal is a prototypic example for “bother” when
doing Fourier transformations. As we’ll see in Chap.3, even using window functions
would be of limited help. That’s only the—overly popular—power representation’s
and interference term’s fault.
Fix: Subtract the pedestal before transforming. Usually we’re not interested in
it anyway. For example a logarithmic representation helps, giving a straight line
for the e-function, which then becomes “bent” and runs into the background. Use
extrapolation to determine a. It would be best to divide by the exponential, too. You
are presumably interested in (possible) small oscillations only. In case you have no
data for long times, you will run into trouble. You will also get problems if you
have a superposition of several exponentials such that you won’t get a straight line
anyhow. In such cases, I guess, you will be stumped with Fourier transformation.
Here, Laplace transformation helps which we shall not treat here.
Playground
2.1 Black Magic
The Italian mathematician Maria Gaetana Agnesi—appointed in 1750 to the faculty
of the University of Bologna by the Pope—constructed the following geometric
locus, called “versiera”:
i. draw a circle with radius a/2 at (0; a/2)
ii. draw a straight line parallel to the x-axis through (0; a)
iii. draw a straight line through the origin with a slope tan θ
iv. the geometric locus of the “versiera” is obtained by taking the x-value from the
intersection of both straight lines while the y-value is taken from the intersection
of the inclined straight line with the circle.
a. Derive the x- and y-coordinates as a function of θ, i.e. in parameterised form.
b. Eliminate θ using the trigonometric identity sin2 θ = 1/(1+cot2 θ) to arrive
at y = f (x), i.e. the “versiera”.
c. Calculate the Fourier transform of the “versiera”.
2.2 The Phase Shift Knob
On the screen of a spectrometer you see a single spectral component with non-zero
patterns for the real and imaginary parts. What shift on the time axis, expressed as
a fraction of the oscillation period T , must be applied to make the imaginary part
vanish? Calculate the real part which then builds up.
2.3 Pulses
Calculate the Fourier transform of:
f (t) =
sin ω0t for −T/2 ≤t ≤T/2
0
else
with ω0 = n 2π
T/2.

68
2
Continuous Fourier Transformation
What is |F(ω0)|, i.e. at “resonance”? Now, calculate the Fourier transform of two of
such “pulses”, centered at ±Δ around t = 0.
2.4 Phase-Locked Pulses
Calculate the Fourier transform of:
f (t) =

sin ω0t for
−Δ −T/2 ≤t ≤−Δ + T/2
and + Δ −T/2 ≤t ≤+Δ + T/2
0
else
with ω0 = n 2π
T/2.
Choose Δ such that |F(ω)| is as large as possible for all frequencies ω! What is the
full width at half maximum (FWHM) in this case?
Hint: Note that now the rectangular pulses “cut out” an integer number of oscil-
lations, not necessarily starting/ending at 0, but being “phase-locked” between left
and right “pulses” (Fig.2.22).
g(t)
t
T
T
Δ
Δ
g(t)
t
−Δ −T
2
−Δ + T
2
+Δ −T
2
+Δ + T
2
t
Fig. 2.22 Two pulses 2Δ apart from each other (top). Two “phase-locked” pulses 2Δ apart from
each other (bottom)

Playground
69
2.5 Tricky Convolution
Convolve a normalised Lorentzian with another normalised Lorentzian and calculate
its Fourier transform.
2.6 Even Trickier
Convolve a normalised Gaussian with another normalised Gaussian and calculate its
Fourier transform.
2.7 Voigt Proﬁle (for Gourmets only)
Calculate the Fourier transform of a normalised Lorentzian convolved with a nor-
malised Gaussian. For the inverse transformation you need a good integration table,
e.g. [8, No 3.953.2].
2.8 Derivable
What is the Fourier transform of:
g(t) =

te−λt for 0 ≤t
0
else
.
Is this function even, odd, or mixed?
2.9 Nothing Gets Lost
Use Parseval’s theorem to derive the following integral:
 ∞
0
sin2 aω
ω2
dω = π
2 a
with a > 0.

Chapter 3
Window Functions
Abstract Various window functions are presented in their continuous formulation:
rectangular, triangular, cosine, Hanning, Hamming, triplet, Gauss, Kaiser-Bessel,
Blackman-Harris. A focus is on sidelobe suppression versus 3dB-bandwidth. An
example is given for a test-function with a comparison of different windows. The
Kaiser-Bessel window is recommended because it provides a parameter to play with
and because of its monotonically decaying sidelobes.
How much fun you get out of Fourier transformations will depend very much on the
proper use of window or weighting functions. F.J. Harris has compiled an excellent
overview of window functions for discrete Fourier transformations [9]. Here we want
to discuss window functions for the case of a continuous Fourier transformation.
Porting this to the case of a discrete Fourier transformation then won’t be a problem
any more.
In Chap.1 we learnt that we better stay away from transforming steps. But that’s
exactly what we’re doing if the input signal is available for a ﬁnite time window
only. Without fully realising what we were doing, we’ve already used the so-called
rectangular window (=no weighting) on more than a few occasions. We’ll discuss
this window in more detail shortly.
Then we’ll get into window functions where information is “switched on and off”
softly. I’ll promise right now that this can be good fun.
All window functions are, of course, even functions. The Fourier transforms of
the window function therefore don’t have an imaginary part. We require a large
dynamic range so we can better compare window qualities. That’s why we’ll use
logarithmic representations covering equal ranges. And that’s also the reason why
we can’t have negative function values. To make sure they don’t occur, we’ll use the
power representation, i.e. |F(ω)|2.
Note:
According to the Convolution Theorem, the Fourier transform of the window function rep-
resents precisely the lineshape of an undamped cosine input.
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_3
71

72
3
Window Functions
3.1 The Rectangular Window
f (t) =
1 for −T/2 ≤t ≤T/2
0 else
,
(3.1)
has the power representation of the Fourier transform:
|F(ω)|2 = T 2
sin(ωT/2)
ωT/2
2
.
(3.2)
The rectangular window and this function are shown in Fig.3.1. The ﬁrst sidelobe
is negative and all subsequent sidelobes alternate in sign.
3.1.1 Zeros
Where are the zeros of this function? We’ll ﬁnd them at ωT/2 = ±lπ with l =
1, 2, 3, . . .and without 0! The zeros are equidistant, the zero atl = 0 in the numerator
gets “plugged” by a zero at l = 0 in the denominator.1
3.1.2 Intensity at the Central Peak
Now we want to ﬁnd out how much intensity is at the central peak, and how much gets
lost in the sidebands (sidelobes). To get there, we need the ﬁrst zero at ωT/2 = ±π
or ω = ±2π/T and:
+2π/T

−2π/T
T 2
sin(ωT/2)
ωT/2
2
dω = T 2 2
T 2
π

0
sin2 x
x2
dx = 4T Si(2π)
(3.3)
where ωT/2 = x.
Here Si(x) stands for the sine integral:
x

0
sin y
y
dy.
(3.4)
1Use l’Hospital’s rule for limx→0 sin x
x .

3.1 The Rectangular Window
73
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.1 Rectangular window function and its Fourier transform in power representation (the unit
dB, “decibel”, will be explained in Sect.3.1.3)
The last equal sign may be proved as follows. We start out with:
π

0
sin2 x
x2
dx
and integrate partially with u = sin2 x and v = −1
x :
π

0
sin2 x
x2
dx = sin2 x
x

π
0
+
π

0
2 sin x cos x
x
dx
(3.5)
= 2
π

0
sin 2x
2x
dx = Si(2π)
with 2x = y.
Using Parseval’s theorem we get the total intensity:
+∞

−∞
T 2
sin(ωT/2)
ωT/2
2
dω = 2π
+T/2

−T/2
12dt = 2πT.
(3.6)
The ratio of the intensity at the central peak to the total intensity therefore is:
4T Si(2π)
2πT
= 2
π Si(2π) = 0.903.
This means that ≈90% of the intensity is in the central peak, whereas some 10%
are “wasted” in sidelobes.

74
3
Window Functions
3.1.3 Sidelobe Suppression
Now let’s determine the height of the ﬁrst sidelobe. To get there, we need:
d|F(ω)|2
dω
= 0
or also
dF(ω)
dω
= 0
(3.7)
and that’s the case when:
d
dx
sin x
x
= 0 = x cos x −sin x
x2
with x = ωT/2 or x = tan x.
Solving this transcendental equation (for example graphically or by trial and error)
gives us the smallest possible solution x = 4.4934 or ω = 8.9868/T . Inserting that
in |F(ω)|2 results in:
F
 8.9868
T
2 = T 2 × 0.04719.
(3.8)
For ω = 0 we get |F(0)|2 = T 2, the ratio of the ﬁrst sidelobe’s height to the central
peak’s height therefore is 0.04719. It’s customary to express ratios between two
values spanning several orders of magnitude in decibels (short: dB). The deﬁnition
of the decibel is:
dB = 10 log10 x
(3.9)
Quiteregularlypeopleforgettomentionwhat theratio’sbasedon,whichcancause
confusion. We’re talking about intensity-ratios, (viz. F2(ω)). If we’re referring to
amplitude-ratios,(viz. F(ω)),thiswouldmakepreciselyafactoroftwoinlogarithmic
representation! Here we have a sidelobe suppression (ﬁrst sidelobe) of:
10 log10 0.04719 = −13.2 dB.
(3.10)
3.1.4 3dB-Bandwidth
As the 10 log10(1/2) = −3.0103 ≈−3, the 3dB bandwidth tells us where the
central peak has dropped to half its height. This is easily calculated as follows:
T 2
sin(ωT/2)
ωT/2
2
= 1
2T 2.
Using x = ωT/2 we have:
sin2 x = 1
2 x2
or
sin x =
1
√
2
x.
(3.11)

3.1 The Rectangular Window
75
This transcendental equation has the following solution:
x = 1.3915,
thus
ω3dB = 2.783/T.
This gives us the total width (±ω3dB):
ω = 5.566
T
.
(3.12)
This is the slimmest central peak we can get using Fourier transformation. Any
other window function will lead to larger 3dB-bandwidths. Admittedly, it’s more
than nasty to stick more than ≈10% of the information into the sidelobes. If we
have, apart from the prominent spectral component, another spectral component,
with—say—an approx. 10dB smaller intensity, this component will be completely
smothered by the main component’s sidelobes. If we’re lucky, it will sit on the ﬁrst
sidelobe and will be visible; if we’re out of luck, it will fall into the gap (the zero)
between central peak and ﬁrst sidelobe and will get swallowed. So it pays to get rid
of these sidelobes.
Warning: This 3dB-bandwidth is valid for |F(ω)|2 and not for F(ω)! Since
one often uses |F(ω)| or the cosine-/sine-transformation (cf. Sect.4.5) one wants
the 3dB-bandwidth thereof, which corresponds to the 6dB-bandwidth of |F(ω)|2.
Unfortunately, you cannot simply multiply the 3dB-bandwidth of |F(ω)|2 by
√
2,
you have to solve a new transcendental equation. However, it’s still good as a ﬁrst
guess because you merely interpolate linearly between the point of 3dB-bandwidth
and the point of the 6dB-bandwidth. You’d overestimate the width by less than 5%.
3.1.5 Asymptotic Behaviour of Sidelobes
The sidelobes’ envelope results in the heights decreasing by 6dB per octave (that’s
a factor of 2 as far as the frequency is concerned). This result is easily derived from
(1.62). The unit step leads to oscillations which decay as 1
k , i.e. in the continuous
case as 1
ω. This corresponds to a decay of 3dB per octave. Now we are dealing with
squared magnitudes, hence, we have a decay of
1
ω2 . This corresponds to a decay of
6dB per octave. This is of fundamental importance: a discontinuity in the function
yields −6dB/octave, a discontinuity in the derivative (hence, a kink in the function)
yields −12dB/octave and so forth. This is immediately clear considering that the
derivative of the triangular function yields the step function. The derivative of 1
ω
yields
1
ω2 (apart from the sign), i.e. a factor of 2 in the sidelobe suppression. You
remember the
	
1
k2

-dependence of the Fourier coefﬁcients of the triangular function?
The “smoother” the window function starts out, the better the sidelobes’ asymptotic
behaviour will get. But this comes at a price, namely a worse 3dB-bandwidth.

76
3
Window Functions
3.2 The Triangular Window (Fejer Window)
The ﬁrst real weighting function is the triangular window:
f (t) =
⎧
⎪⎨
⎪⎩
1 + 2t/T for −T/2 ≤t ≤0
1 −2t/T for 0 ≤t ≤T/2
0
else
,
(3.13)
F(ω) = T
2
sin(ωT/4)
ωT/4
2
.
(3.14)
We won’t have to rack our brains! This is the autocorrelation function of the
triangular function (cf. Sect.2.3.1, Fig.2.12). The only difference is the interval’s
width: whereas the autocorrelation function of the rectangular function over the
interval −T/2 ≤t ≤T/2 has a width of −T ≤t ≤T , in (3.13) we only have the
usual interval −T/2 ≤t ≤T/2.
The 1/4 is due to the interval, the square due to the autocorrelation. All other
properties are obvious straight away. The triangular window and the square of this
function are shown in Fig.3.2.
The zeros are twice as far apart as in the case of the rectangular function:
ωT
4
= πl
or
ω = 4πl
T
l = 1, 2, 3, . . . .
(3.15)
The intensity at the central peak is 99.7%.
The height of the ﬁrst sidelobe is suppressed by 2 × (−13.2 dB) ≈−26.5 dB
(No wonder, if we skip every other zero!).
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.2 Triangular window and power representation of the Fourier transform

3.2 The Triangular Window (Fejer Window)
77
The 3dB-bandwidth is calculated as follows:
sin ωT
4
=
1
4√
2
ωT
4
to
ω = 8.016
T
(full width),
(3.16)
that’s some 1.44 times wider than in the case of the rectangular window.
The asymptotic behaviour of the sidelobes is −12dB/octave.
3.3 The Cosine Window
The triangular window had a kink when switching on, another kink at the maximum
(t = 0) and another one when switching off. The cosine window avoids the kink
at t = 0:
f (t) =

cos πt
T for −T/2 ≤t ≤T/2
0
else
.
(3.17)
The Fourier transform of this function is:
F(ω) = T cos ωT
2 ×

1
π −ωT +
1
π + ωT

.
(3.18)
The functions f (t) and |F(ω)|2 are shown in Fig.3.3.
At position ω = 0 we get:
F(0) = 2T
π .
For ωT →±π we get expressions of type “0 : 0”, which we calculate using
l’Hospital’s rule.
Surprise surprise: The zero at ωT = ±π was “plugged” by the expression in
brackets in (3.18), i.e. F(ω) there will stay ﬁnite and continuous. Apart from that,
the following applies:
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.3 Cosine window and power representation of the Fourier transform

78
3
Window Functions
The zeros are at:
ωT
2
= (2l + 1)π
2
,
ω = (2l + 1)π
T
,
l = 1, 2, 3, . . . ,
(3.19)
i.e. within the same distance as in the case of the rectangular window.
Here it’s not worth shedding tears for a lack of intensity at the central peak any
more. For all practical purposes it is ≈100%. We should, however, have another look
at the sidelobes because of the minorities, viz. the chance of detecting additional
weak signals.
The suppression of the ﬁrst sidelobe may be calculated as follows:
tan x
2 =
4x
π2 −x2
with the solution x ≈11.87.
(3.20)
This results in a sidelobe suppression of −23dB.
The 3dB-bandwidth amounts to:
ω = 7.47
T ,
(3.21)
a remarkable result. This is the ﬁrst time we got, through the use of a somewhat more
intelligent “window”, a sidelobe suppression of −23dB—not a lot worse than the
−26.5dB of the triangular window—and we get a better 3dB-bandwidth compared
to ω = 8.016/T for the triangular window. So it does pay to think about better
window functions. The asymptotic decay of the sidelobes is −12dB/octave, as was
the case for the triangular function.
3.4 The cos2-Window (Hanning)
The scientist Julius von Hann thought that eliminating the kinks at ±T/2 would be
beneﬁcialandproposedthecos2-window(intheUS,thissoonwascalled“Hanning”):
f (t) =

cos2 πt
T for −T/2 ≤t ≤T/2
0
else
.
(3.22)
The corresponding Fourier transform is:
F(ω) = T
4 sin ωT
2 ×

1
π −ωT/2 +
2
ωT/2 −
1
π + ωT/2

.
(3.23)
The functions f (t) and |F(ω)|2 are shown in Fig.3.4.

3.4 The cos2-Window (Hanning)
79
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.4 Hanning window and power representation of the Fourier transform
The zero at ω = 0 has been “plugged” because of sin(ωT/2)/(ωT/2) →1 and
the zeros at ω = ±2π/T get “plugged” for the same reason. The example of the
cosine window is becoming popular!
The zeros are at:
ω = ±2lπ
T ,
l = 2, 3, . . . .
(3.24)
Intensity at the central peak ≈100%.
The suppression of the ﬁrst sidelobe is −32dB.
The 3dB-bandwidth is:
ω = 9.06
T .
(3.25)
The sidelobes’ asymptotic decay is −18dB/octave.
So we get a considerable sidelobe suppression, admittedly to the detriment of the
3dB-bandwidth.
Some experts recommend to go for higher-powered cosine functions in the ﬁrst
place. This would “plug” more and more zeros near the central peak, and there will
be gains both as far as sidelobe suppression as well as asymptotic behaviour are
concerned, though, of course, the 3dB-bandwidth will get bigger and bigger. So for
the cos3-window we get:
ω = 10.4
T
(3.26)
and for the cos4-window:
ω = 11.66
T
.
(3.27)
As we’ll see shortly, there are more intelligent solutions to this problem.

80
3
Window Functions
3.5 The Hamming Window
Mr Julius von Hann didn’t have a clue that he—sorry: his window function—would
be put on a pedestal in order to get an even better window, and to add insult to injury,
his name would get mangled to “Hamming” to boot.2
f (t) =

a + (1 −a) cos2 πt
T for −T/2 ≤t ≤T/2
0
else
.
(3.28)
The Fourier transform is:
F(ω) = T
4 sin ωT
2 ×

1 −a
π −ωT/2 + 2(1 + a)
ωT/2
−
1 −a
π + ωT/2

.
(3.29)
How come there’s a “pedestal”? Didn’t we realise a few moments ago that any
discontinuity at the interval boundaries is “bad”? Just like a smidgen of arsenic may
work wonders, here a “tiny wee pedestal” can be helpful. Indeed, using parameter
a we’re able to play the sidelobes a bit. A value of a ≈0.1 proves to be good. The
plugging of the zeros hasn’t changed, as (3.29) shows. Though now, however, the
Fourier transform of the “pedestal” has saddled us with the term:
T
2 a sin(ωT/2)
ωT/2
that now gets added to the sidelobes of the Hamming window. A squaring of F(ω)
is not essential here. This on the one hand will provide interference terms of the
Hamming window’s Fourier transform, but on the other hand, the same is true for
F(ω); here all we get are positive and negative sidelobes. The absolute values of
the sidelobes’ heights don’t change. The Hamming window with a = 0.15 and the
respective F2(ω) are shown in Fig.3.5. The ﬁrst sidelobes are slightly smaller than
the second ones! Here we have the same zeros as (this is done by the sin ωT
2 , provided
the denominators don’t prevent it). For the optimal parameter a = 0.08 the sidelobe
suppression is −43dB, the 3dB-bandwidth is only ω = 8.17/T . The asymptotic
behaviour, naturally, got worse. Far from the central peak, it’s down to as little as
−6dB per octave. That’s what happens when you choose a small step!
Therefore the new strategy is: rather a somewhat worse asymptotic behaviour, if
only we manage to get a high sidelobe suppression and, at the same time, a decrease
in 3dB-bandwidth deterioration that’s as small as possible. How far one can go is
illustrated by the following example. Plant at the interval ends little “ﬂagpoles”,
i.e. inﬁnitely sharp cusps with small height. This is, of course, most easily done in
the discrete Fourier transformation. There, the “ﬂagpole” is just a channel wide. Of
2No kidding, Mr R.W. Hamming apparently did discover this window, and the von Hann window
got mangled later on.

3.5 The Hamming Window
81
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.5 Hamming window and power representation of Fourier transform
course, we get no asymptotic roll-off of the sidelobes at all. The Fourier transform of
a δ-function is a constant! However, we get the narrowest 3dB-bandwidth possible.
Such a window is called Dolph–Chebychev window, however, we won’t discuss it
any further here. Because it emphasizes the data at the interval boundaries it should
not be used in cases where those data are inaccurate or corrupted.
Before we get into more and better window functions, let’s look, just for curiosity’s
sake, at a window that creates no sidelobes at all.
3.6 The Triplet Window
The previous really set us up, so let’s try the following:
f (t) =

e−λ|t| cos2 πt
T for −T/2 ≤t ≤T/2
0
else
.
(3.30)
Deducing the expression for F(ω) is trivial, yet too lengthy (and too unimportant)
to be dealt with here.
The expression for F(ω)—if we do deduct it—stands out, as it features oscil-
lating terms (sine, cosine) though there are no more zeros. If only the λ is big
f(t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.6 Triplet window and power representation of the Fourier transform

82
3
Window Functions
enough, then there won’t even be local minima or maxima any more, and F(ω)
decays monotonically. In the case of optimum λ we can achieve an asymptotic
behaviour of −12dB/octave with a 3dB-bandwidth of ω = 9.7/T (cf. Fig.3.6).
Therefore it wasn’t such a bad idea to re-introduce a spike at t = 0. However,
there are better window functions.
3.7 The Gauss Window
A pretty obvious window function is the Gauss function. That we have to truncate it
somewhere, resulting in a small step, doesn’t worry us any more, if we look back on
our experience with the Hamming window.
f (t) =

exp
	
−1
2
t2
σ2

for −T/2 ≤t ≤+T/2
0
else
.
(3.31)
The Fourier transform reads:
F(ω) = σ
π
2 e−σ2ω2
2

erf

−iσω
√
2
+
T
2
√
2σ

+ erf

+iσω
√
2
+
T
2
√
2σ

.
(3.32)
As the error function occurs with complex arguments, though together with the
conjugate complex argument, F(ω) is real. The function f (t) with σ = 2 in units of
T/2 and |F(ω)|2 is shown in Fig.3.7. A disadvantage of the Gauss window is that
its sidelobes do not decay monotonically for all values of σ.
A Gauss function being Fourier-transformed will result in another Gauss function,
yet only when there was no truncation! If σ is sufﬁciently big, the sidelobes will
disappear: the oscillations “creep up” the Gauss function’s ﬂank. Shortly before this
happens, we get a 3dB-bandwidth of ω = 9.06/T , −64dB sidelobe suppression
and −26dB per octave asymptotic behaviour near the central peak. Not bad, but we
can do better.
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.7 Gauss window and power representation of the Fourier transform

3.8 The Kaiser–Bessel Window
83
3.8 The Kaiser–Bessel Window
The Kaiser–Bessel window is a very useful window and can be applied to various
situations:
f (t) =
⎧
⎨
⎩
I0
	
β√
1−(2t/T )2

I0(β)
for −T/2 ≤t ≤T/2
0
else
.
(3.33)
Here β is a parameter that may be chosen at will. The Fourier transform is:
F(ω) =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
2T
I0(β)
sinh

β2−ω2T 2
4


β2−ω2T 2
4
for β ≥
 ωT
2

2T
I0(β)
sin

ω2T 2
4
−β2


ω2T 2
4
−β2
for β ≤
 ωT
2

.
(3.34)
I0(x) is the modiﬁed Bessel function. A simple algorithm [10, Equations9.8.1,
9.8.2] for the calculation of I0(x) follows:
I0(x) = 1 + 3.5156229t2 + 3.0899424t4 + 1.2067492t6
+ 0.2659732t8 + 0.0360768t10 + 0.0045813t12 + ϵ,
|ϵ| < 1.6 × 10−7
with t = x/3.75, for the interval −3.75 ≤x ≤3.75,
and:
x1/2e−x I0(x) = 0.39894228 + 0.01328592t−1 + 0.00225319t−2
−0.00157565t−3 + 0.00916281t−4 −0.02057706t−5
+ 0.02635537t−6 −0.01647633t−7 + 0.00392377t−8 + ϵ,
|ϵ| < 1.9 × 10−7
with t = x/3.75, for the interval 3.75 ≤x < ∞.
The zeros are at ω2T 2/4 = l2π2+β2, l = 1, 2, 3, . . ., and they’re not equidistant.
For β = 0 we get the rectangular window, values up to β = 9 are recommended.
Figure3.8 shows f (t) and |F(ω)|2 for various values of β.
The sidelobe suppression as well as the 3dB-bandwidth as a function of β are
shown in Fig.3.9. Using this window function we get for β = 9 −70dB sidelobe
suppression with ω = 11/T and −38.5dB/octave asymptotic behaviour near the
central peak. In every respect, the Kaiser–Bessel windows is superior to the Gauss
window.

84
3
Window Functions
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Fig. 3.8 Kaiser–Bessel window for β = 0, 2, 4, 6, 8 (left) and the respective power representation
of the Fourier transform (right)

3.9 The Blackman–Harris Window
85
0
2
4
6
8
10
12
0
1
2
3
4
5
6
7
8
9
ΔωT
β
0
−10
−20
−30
−40
−50
−60
−70
0
1
2
3
4
5
6
7
8
9
β
[dB]
Fig. 3.9 Sidelobe suppression and 3dB-bandwidth for Kaiser–Bessel parameter β = 0 −9
3.9 The Blackman–Harris Window
To those of you who don’t want ﬂexibility and want to work with a ﬁxed good
sidelobe suppression, I recommend the following two very efﬁcient windows which
are due to Blackman and Harris. They have the charm to be simple: they consist of
a sum of four cosine terms as follows:
f (t) =
⎧
⎪⎨
⎪⎩
3
n=0
an cos 2πnt
T
for −T/2 ≤t ≤T/2
0
else
.
(3.35)
Please note that we have a constant, a cosine term with a full period, as well as
further terms with two and three full periods, contrary to the Sect.3.3. Here, the
coefﬁcients have the following values:
for −74 dB
for −92 dB
a0
0.40217
0.35875
a1
0.49703
0.48829
a2
0.09392
0.14128
a3
0.00183
0.01168 .
(3.36)

86
3
Window Functions
Intheoriginal publicationof Harris [9] thesumthecoefﬁcients of the −74dBwindow
is smaller than 1 by 0.00505. There must be a misprint. With the coefﬁcients listed
above the “sidelobe” suppression is signiﬁcantly worse than −74dB. If you take
a1 = 0.49708 and a2 = 0.09892 (or a2 = 0.09892 and a3 = 0.00188) you obtain
−74dB. In this case the sum of the coefﬁcients yields 1. Maybe, in 1978 there were
problems during the typing of the manuscript: the character 8 was read as 3? The
second choice is that given in [11].
Surely, you have noted that the coefﬁcients add up to 1; at the interval ends the
terms with a0 and a2 are positive, whereas the terms with a1 and a3 are negative. The
sum of the even coefﬁcients minus the sum of the odd coefﬁcients yields practically
0, i.e. there is a rather “soft” turning on with a tiny step only.
The Fourier transform of this window reads:
F(ω) = T sin ωT
2
3

n=0
an(−1)n

1
2nπ + ωT −
1
2nπ −ωT

.
(3.37)
Don’t worry, the zeros in the denominator are just “healed” by the zeros of the sine.
The zeros of the Fourier transform are given by sin ωT
2 = 0, i.e. they are the same as
for the Hanning window. The 3dB-bandwidth is ω = 10.93/T and 11.94/T for
the −74dB- and the −92dB-window, respectively; excellent performance for such
simple windows. I guess, the series expansion of the modiﬁed Bessel function I0(x)
for the appropriate values of β yields pretty much the coefﬁcients of the Blackman–
Harris windows. Because these Blackman–Harris windows differ only very little
from the Kaiser–Bessel windows with β ≈9 and β ≈11.5, respectively, (these
are the values for comparable sidelobe suppression), I do without ﬁgures. However,
the Blackman–Harris window with −92dB would have no more visible “feetlets” in
Fig.3.10 which displays to −80dB only.
3.10 Overview over Window Functions
In order to ﬁll this chapter with life we give a simple example. Given is the following
function:
f (t) = cos ωt + 10−2 cos 1.15ωt + 10−3 cos 1.25ωt
(3.38)
+ 10−3 cos 2ωt + 10−4 cos 2.75ωt + 10−5 cos 3ωt.
Apart from the dominant frequency ω there are two satellites at 1.15 and 1.25 times
ω, two harmonics—radio frequency technicians say ﬁrst and second harmonic—
at 2ω and 3ω as well as another frequency at 2.75ω. Let’s Fourier-transform this
function. Please keep in mind that we shall look at the power spectra right now, i.e.
the amplitudes squared! Hence, the signs of the amplitudes play no role. Apart from
the dominant frequency, which we will quote with 0dB intensity, we expect further
spectral components with intensities of −40, −60, −80 and −100dB.

3.10 Overview over Window Functions
87
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Rectangular window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Triangular window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Cosine window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Hanning window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Hamming window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Triplet window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Gauss window
f (t)
t
−T
2
+ T
2
0
−20
−40
−60
−80
|F(ω)|2
[dB]
ω
0
Kaiser–Bessel window
Fig. 3.10 “Overview” of the window functions

88
3
Window Functions
-160
-140
-120
-100
-80
-60
-40
-20
0
0
F
1
2
3
4
-160
-140
-120
-100
-80
-60
-40
-20
0
-160
-140
-120
-100
-80
-60
-40
-20
0
-160
-140
-120
-100
-80
-60
-40
-20
0
Hamming
rectangle
Hanning
-160
-140
-120
-100
-80
-60
-40
-20
0
Fig. 3.11 Test function from (3.38) analysed with different window functions

3.10 Overview over Window Functions
89
Figure3.11 shows what you get using different window functions. For the purists:
of course, we have used the discrete Fourier transform to be dealt with in the next
chapter, but show line-plots (we have used 128 data points, zero-padded the data,
mirrored and used a total of 4096 input data; now you can repeat it yourself!).3
The two satellites close to the dominant frequency cause the biggest problems.
On the one hand we require a window function with a good sidelobe suppression
in order to be able to see the signals with intensities of −40 and −60dB. The rec-
tangular window does’nt achieve that! You only see the dominant frequency, all
the rest is “drowned”. In addition, we require a small 3dB-bandwidth in order to
resolve the frequency which is 15% higher. This is pretty well accomplished using
the Hanning- and above all the Hamming-window (Parameter 0.08). However, the
Hamming window is unable to detect the higher spectral components which still
have lower intensities. This is a consequence of the poor asymptotic behaviour. We
are no better off with the component which is 25% higher because it has −60dB
intensity only. Here, the Blackman–Harris window with −74dB is just able to do
so. It is easy to detect the other three, still higher spectral components, regardless of
their low intensities, because they are far away from the dominant frequency if only
the sidelobes in this spectral range are not “drowning” them. Interestingly enough,
window functions with poor sidelobe suppression but good asymptotic behaviour
like the Hanning window are doing the job, as do window functions with good side-
lobe suppression and poor asymptotic behaviour like the Kaiser–Bessel window. The
Kaiser–Bessel window with the parameter β = 12 is an example (the Blackman–
Harris window with −92dB sidelobe suppression is nearly as good). The disadvan-
tage: the small satellites at 1.15- and 1.25-fold frequency show up as shoulders only.
You see that we should use different window functions for different demands. There
is no multi-purpose beast providing eggs, wool, milk and bacon! However, there are
window functions which you can simply forget. Since the Kaiser–Bessel window has
an adjustable parameter β, I recommend to try out various values of β. An important
property of this window is its monotonic decay of the sidelobes.
What can we do if we need a lot more sidelobe suppression than −100dB? Take
the Kaiser–Bessel window with a very large parameter β; you easily get much better
sidelobe suppression, of course with increasingly larger 3dB-bandwidth! There is no
escape from this “double mill”! However, despite the joy about “intelligent” window
functions you should not forget that ﬁrst you should obtain data which contain so
little noise that they allow the mere detection of −100dB-signals.
3I must confess, all the previous ﬁgures of the Fourier-transformed window functions were pre-
pared by discrete Fourier transformation and there are subtle differences compared to the analytical
formulas like those shown in Fig.4.19.

90
3
Window Functions
3.11 Windowing or Convolution?
In principle, we have two possibilities to use window functions:
i. either you weight, i.e. you multiply the input by the window function and subse-
quently Fourier-transform, or
ii. you Fourier-transform the input and convolve the result with the Fourier transform
of the window function.
According to the Convolution Theorem (2.42) we get the same result. What are
the pros and cons of both procedures? There is no easy answer to this question. What
helps in arguing is thinking in discrete data. Take, e.g. the Kaiser–Bessel window.
Let’s start with a reasonable value for the parameter β, based on considerations of
the trade-off between 3dB-bandwidth (i.e. resolution) and sidelobe suppression. In
the case of windowing we have to multiply our input data, say N real or complex
numbers, by the window function which we have to calculate at N points. After
that we Fourier-transform. Should it turn out that we actually should require a better
sidelobesuppressionandcouldtolerateaworseresolution—or viceversa—wewould
have to go back to the original data, window them again and Fourier-transform again.
The situation is different for the case of convolution: we Fourier-transform without
any bias concerning the eventually required sidelobe suppression and subsequently
convolve the Fourier data (again N numbers, however in general complex!) with the
Fourier-transformed window function, which we have to calculate for a sufﬁcient
number of points. What is a sufﬁcient number? Of course, we drop the sidelobes for
the convolution and only take the central peak! This should be calculated at least
for 5 points, better more. The convolution then actually consists of 5 (or more) mul-
tiplications and a summation for each Fourier coefﬁcient. This appears to be more
work; however, it has the advantage that a further convolution with another, say
broader Fourier-transformed window function, would not require to carry out a new
Fourier transformation. Of course, this procedure is but an approximation because
of the truncation of the sidelobes. If we included all data of the Fourier-transformed
window function including the sidelobes, we had to carry out N (complex) multipli-
cations and a summation per point, already quite a lot of computational effort, yet
still less than a new Fourier transformation. This could be relevant for large arrays,
especially in two or three dimensions like in image processing and tomography.
What happens at the edges when carrying out a convolution? We shall see in the
following chapter that we shall continue periodically beyond the interval. This gives
us the following idea: let’s take the Blackman–Harris window and continue period-
ically; the corresponding Fourier transform consists of a sum of four δ-functions,
in the discrete world we have exactly four channels which are non-zero. Where
remained the sidelobes? You shall see in a minute that in this case the points (by the
way equidistant) coincide with the zeros of the Fourier-transformed window func-
tion, except at 0! Hence, we have to carry out a convolution with just four points
only, a rather fast procedure! That’s why the Blackman–Harris window is called a
4-point-window. So after all, convolution is better? Here comes a deep sigh: there

3.11 Windowing or Convolution?
91
are so many good reasons to get rid of the periodic continuation as much as possible
by zero-padding the input data (cf. Sect.4.6), thus our neat 4-point-idea melts away
like snow in springtime sun. The decision is yours whether you prefer to weight or
to convolve and depends on the concrete case. Now it’s high time to start with the
discrete Fourier transformation!
Playground
3.1 Squared
Calculate the 3dB-bandwidth of F(ω) for the rectangular window. Compare this
with the 3dB-bandwidth F2(ω).
3.2 Let’s Gibbs Again
What is the asymptotic behaviour of the Gauss window far away from the central
peak?
3.3 Expander
The series expansion of the modiﬁed Bessel function of zeroth order is:
I0(x) =
∞

k=0
(x2/4)k
(k!)2 ,
where k! = 1 × 2 × 3 × · · · × k denotes the factorial. The series expansion for the
cosine reads:
cos(x) =
∞

k=0
(−1)k x2k
(2k)!.
Calculate the ﬁrst ten terms in the series expression of the Blackman–Harris window
with −74dB sidelobe suppression and the Kaiser–Bessel window with β = 9 and
compare the results.
Hint: Instead of pen and paper better use your PC!
3.4 Minorities
In a spectrum analyser you detect a signal at ω = 500Mrad/s in the |F(ω)|2-mode
with an instrumental full width at half maximum (FWHM) of 50Mrad/s with a
rectangular window.
a. What sampling period T did you choose?
b. What window function could you use if you were hunting a “minority” signal
which you suspect to be 20% higher in frequency and 50dB lower than the main
signal. Look at the ﬁgures in this chapter, don’t calculate too much.

Chapter 4
Discrete Fourier Transformation
Abstract This chapter deals with the discrete Fourier transformation. Here, a
periodic series in the time domain is mapped onto a periodic series in the frequency
domain. Deﬁnitions of the discrete Fourier transformation and its inverse are given.
Linearity, convolution, cross-correlation, and autocorrelation are treated as well as
Parseval’s theorem. The sampling theorem is illustrated with a simple example. Data
mirroring, cosine- and sine-transformations, as well as zero-padding are discussed.
It concludes with the Fast Fourier Transformation algorithm by Cooley and Tukey.
Mapping of a Periodic Series { fk} to the Fourier-Transformed
Series {Fj}
4.1 Discrete Fourier Transformation
Often we don’t know a function’s continuous “behaviour” over time, but only what
happens at N discrete times:
tk = kt,
k = 0, 1, . . . , N −1.
In other words: we’ve taken our “pick”, that’s “samples” f (tk) = fk at certain
points in time tk. Any digital data-recording uses this technique. So the data set con-
sists of a series { fk}. Outside the sampled interval T = Nt we don’t know anything
about the function. The discrete Fourier transformation automatically assumes, that
{ fk} will continue periodically outside the interval’s range. At ﬁrst glance this limi-
tation appears to be very annoying: maybe f (t) isn’t periodic at all, and even if f (t)
were periodic, there’s a chance that our interval happens to truncate at the wrong
time (meaning: not after an integer number of periods). How this problem can be
alleviated or practically eliminated will be shown in Sect.4.6. To make life easier,
we’ll also take for granted that N is a power of 2. We’ll have to assume the latter
anyway for the Fast Fourier Transformation (FFT) which we’ll cover in Sect.4.7.
Using the “trick” from Sect.4.6, however, this limitation will become completely
irrelevant.
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_4
93

94
4
Discrete Fourier Transformation
4.1.1 Even and Odd Series and Wrap-Around
A series is called even if the following is true for all k:
f−k = fk.
(4.1)
A series is called odd if the following is true for all k:
f−k = −fk.
(4.2)
(Here f0 = 0 is compulsory!). Any series can be broken up into an even and an odd
series. But what about negative indices? We’ll extend the series periodically:
f−k = fN−k.
(4.3)
This allows us, by adding N, to shift the negative indices to the right end of the
interval, or using another word, “wrap them around”, as shown in Fig.4.1.
Please make sure f0 doesn’t get wrapped, something that often is done by mistake.
The periodicity with period N, which we always assume as given for the discrete
Fourier transformation, requires fN = f0. In the second example—the one with the
mistake—we would get f0 twice next to each other (and apart from that, we would
have overwritten f4, truly a “mortal sin”).
4.1.2 The Kronecker Symbol or the “Discrete δ-Function”
Before we get into the deﬁnition of the discrete Fourier transformation (forward and
inverse transformation), a few preliminary remarks are in order. From the continuous
Fourier transformation eiωt we get for discrete times tk = kt, k = 0, 1, . . . , N −1
with T = Nt:
correct
wrong
Fig. 4.1 Correctly wrapped-around (top); incorrectly wrapped-around (bottom)

4.1 Discrete Fourier Transformation
95
eiωt →ei 2πtk
T
= e
2πikt
Nt
= e
2πik
N
≡W k
N.
(4.4)
Here the “kernel” is:
WN = e
2πi
N
(4.5)
a very useful abbreviation. Occasionally we’ll also need the discrete frequencies:
ω j = 2π j/(Nt),
(4.6)
related to the discrete Fourier coefﬁcients Fj (see below). The kernel WN has the
following properties:
W nN
N
= e2πin = 1
for all integer n,
(4.7)
WN is periodic in j and k with the period N.
A very useful representation of WN may be obtained in the complex plane as a
“clock-hand” in the unit circle.
The projection of the “hand of a clock” onto the real axis results in cos(2πn/N).
Like when talking about a clock-face, we may, for example, call W 0
8 “3:00 a.m.” or
W 4
8 “9:00 a.m.”. Now we can deﬁne the discrete “δ-function”:
N−1

j=0
W (k−k′) j
N
= Nδk,k′.
(4.8)
Here δk,k′ is the Kronecker symbol with the following property:
δk,k′ =
1 for k = k′
0 else
.
(4.9)
This symbol (with prefactor N) accomplishes the same tasks the δ-function had
when doing the continuous Fourier transformation. Equation(4.9) just means that, if
the hand goes completely round the clock, we’ll get zero, as we can see immediately
by simply adding the hands’ vectors in Fig.4.2, except if the hand stops at “3:00
a.m.”, a situation k = k′ can force. In this case we get N, as shown in Fig.4.3.

96
4
Discrete Fourier Transformation
real axis
imaginary axis
W 6
8
W 2
8
W 1
8
W 7
8
W 3
8
W 5
8
W 4
8
W 0
8
Fig. 4.2 Representation of W k
8 in the complex plane
W 0·0
8
W 0·1
8
W 0·2
8
W 0·3
8
W 0·4
8
W 0·5
8
W 0·6
8
W 0·7
8
Fig. 4.3 For N →∞(ﬁctitious only) we quite clearly see the analogy with the δ-function
4.1.3 Deﬁnition of the Discrete Fourier Transformation
Now we want to determine the spectral content {Fj} of the series { fk} using dis-
crete Fourier transformation. For this purpose, we have to make the transition in the
deﬁnition of the Fourier series:
c j = 1
T
+T/2

−T/2
f (t)e−2πi j/T dt
(4.10)
with f (t) periodic in T :
c j = 1
N
N−1

k=0
fke−2πi jk/N.
(4.11)
In the exponent we ﬁnd kt
Nt , meaning that t can be eliminated. The prefactor
contains the sampling raster t, so the prefactor becomes t/T = t/(Nt) =
1/N. During the transition from (4.10) to (4.11) we tacitly shifted the limits of the
interval from −T/2 to +T/2 to 0 to T , something that was okay, as we integrate over
an integer period and f (t) was assumed to be periodic in T . The sum has to come
to an end at N −1, as this sampling point plus t reaches the limit of the interval.
Therefore we get, for the discrete Fourier transformation:

4.1 Discrete Fourier Transformation
97
Deﬁnition 4.1 (Discrete Fourier transformation)
Fj = 1
N
N−1

k=0
fkW −kj
N
with
WN = e2πi/N.
(4.12)
The discrete inverse Fourier transformation is:
Deﬁnition 4.2 (Discrete inverse Fourier transformation)
fk =
N−1

j=0
FjW +kj
N
with
WN = e2πi/N.
(4.13)
Please note that the inverse Fourier transformation doesn’t have a prefactor 1/N.
A bit of a warning is called for here. Instead of (4.12) and (4.13) we also come
across deﬁnition equations with positive exponents for the forward transformation
and with negative exponent for the inverse transformation (for example in “Numeri-
cal Recipes” [7]). This doesn’t matter as far as the real part of {Fj} is concerned. The
imaginary part of {Fj}, however, changes its sign. Because we want to be consistent
with the previous deﬁnitions of Fourier series and the continuous Fourier transfor-
mation we’d rather stick with the deﬁnitions (4.12) and (4.13) and remember that, for
example, a negative, purely imaginary Fourier coefﬁcient Fj belongs to a positive
amplitude of a sine wave (given positive frequencies), as i of the forward transfor-
mation multiplied by i of the inverse transformation results in precisely a change of
sign i2 = −1. Often also the prefactor 1/N of the forward transformation is missing
(for example in “Numerical Recipes” [7]). In view of the fact that F0 is to be equal
to the average of all samples, the prefactor 1/N really has to stay there, too. As we’ll
see, also “Parseval’s theorem” will be grateful if we took care with our deﬁnition of
the forward transformation. Using relation (4.8) we can see straight away that the
inverse transformation (4.13) is correct:
fk =
N−1

j=0
FjW +kj
N
=
N−1

j=0
1
N
N−1

k′=0
fk′W −k′ j
N
W +kj
N
(4.14)
= 1
N
N−1

k′=0
fk′
N−1

j=0
W (k−k′) j
N
= 1
N
N−1

k′=0
fk′ Nδk,k′ = fk.
Before we get into more rules and theorems, let’s look at a few examples to
illustrate the discrete Fourier transformation (Fig.4.4)!

98
4
Discrete Fourier Transformation
interval
interval
F−2
F−1
F0
F0
F1
F1
F2
F3
Fig. 4.4 Fourier coefﬁcients with negative indices are wrapped to the right end of the interval
Example 4.1 (“Constant” with N = 4)
fk = 1
for k = 0, 1, 2, 3.
-
f0
f1
f2
f3
For the continuous Fourier transformation we expect a δ-function with the fre-
quency ω = 0. The discrete Fourier transformation therefore will only result in
F0 ̸= 0. Indeed, we do get, using (4.12)—or even a lot smarter using (4.8):
F0 = 1
44 = 1
F1 = 0
F2 = 0
F3 = 0.
-
F0
F1
F2
F3
As { fk} is an even series, {Fj} contains no imaginary part. The inverse transfor-
mation results in:
fk = 1 cos

2π k
40

= 1
for k = 0, 1, 2, 3.
↑
j=0
Example 4.2 (“Cosine” with N = 4)
We get, using (4.12) and W4 = i:

4.1 Discrete Fourier Transformation
99
F0 = 0 (average = 0!)
F1 = 1
4(1 + (−1)(“9:00 a.m.”) = 1
4(1 + (−1)(−1)) = 1
2
F2 = 1
4(1 + (−1)(“3:00 p.m.”) = 1
4(1 + (−1)1)
= 0
F3 = 1
4(1 + (−1)(“9:00 p.m.”) = 1
4(1 + (−1)(−1)) = 1
2.
I bet you would have noticed that, due to the negative sign in the exponent in (4.12),
we’re running around “clockwise”. Maybe those of you who’d rather use a positive
sign here, are “Bavarians”, who are well known for their clocks going backwards
(you can actually buy them in souvenir-shops). So whoever uses a plus sign in (4.12)
is out of sync with the rest of the world! What’s F3 = 1/2? Is there another spectral
component, apart from the fundamental frequency ω1 = 2π×1/4×t = π/(2t)?
Yes, there is! Of course it’s the component with −ω1, that has been wrapped-around.
We can see that the negative frequencies of FN−1 (corresponding to smallest, not
disappearing frequency ω−1) are located from the right end of the interval decreasing
to the left till they reach the center of the interval.
For real input the following applies:
FN−j = F∗
j ,
(4.15)
as we can easily deduce from (4.12). So in the case of even input the right half has
exactly the same content as the left half; in the case of odd input, the right half will
contain the conjugate complex or the same times minus as the left half. If we add
together the intensity F1 and F3 = F−1 shared “between brothers”, this results in 1,
as required by the input:
fk = 1
2ik + 1
2i3k = cos

2π k
4

for k = 0, 1, 2, 3.
Example 4.3 (“Sine” with N = 4)
Again we use (4.12) and get:

100
4
Discrete Fourier Transformation
F0 = 0
(average = 0)
F1 = 1
4(1 × “6.00 a.m.” +(−1) × “12.00 noon”) = 1
4(−i
+(−1) × i)
= −i
2
F2 = 1
4(1 × “9.00 a.m.” +(−1) × “9.00 p.m.”) = 1
4(1 × (−1) +(−1)(−1)) =
0
F3 = 1
4(1 × “12.00 noon” +(−1) × “6.00 a.m.”

	

following day
) = 1
4(1 × i
+(−1)(−i)) =
i
2
real part = 0
imaginary part:
-
F0
F1
F2
F3
−1
2
+ 1
2
If we add the intensity with a minus sign for negative frequencies, that resulted
from the sharing “between sisters”, to the one for positive frequencies, meaning F1+
(−1)F3 = −i, we get for the intensity of the sine wave (the inverse transformation
provides us with another i!) the value 1:
fk = −i
2ik + i
2i3k = sin

2π k
4

.
4.2 Theorems and Rules
4.2.1 Linearity Theorem
If we combine in a linear way { fk} and its series {Fj} with {gk} and its series {G j},
the we get:
{ fk} ↔{Fj},
{gk} ↔{G j},
a · { fk} + b · {gk} ↔a · {Fj} + b · {G j}.
(4.16)
Please always keep in mind that the discrete Fourier transformation contains only
linear operators (in fact, basic maths only), but that the power representation is no
linear operation.

4.2 Theorems and Rules
101
4.2.2 The First Shifting Rule (Shifting in the Time Domain)
{ fk} ↔{Fj}
{ fk−n} ↔{FjW −jn
N
},
n integer.
(4.17)
Ashiftinthetimedomainbyn resultsinamultiplicationbythephasefactor W −jn
N
.
Proof (First Shifting Rule)
Fshifted
j
= 1
N
N−1

k=0
fk−nW −kj
N
= 1
N
N−1−n

k′=−n
fk′W −(k′+n) j
N
with k −n = k′
= 1
N
N−1

k′=0
fk′W −k′ j
N
W −nj
N
= Fold
j
W −nj
N
. □
(4.18)
Because of the periodicity of fk, we may shift the lower and the upper summation
boundaries by n without a problem.
Example 4.4 (Shifted cosine with N = 2)
Now we shift the input by n = 1:

102
4
Discrete Fourier Transformation
4.2.3 The Second Shifting Rule (Shifting in the Frequency
Domain)
{ fk} ↔{Fj}
{ fkW −nk
N
} ↔{Fj+n},
n integer.
(4.19)
Amodulationinthetimedomainwith W −nk
N
correspondstoashiftinthefrequency
domain. The proof is trivial.
Example 4.5 (Modulated cosine with N = 2)
Now we modulate the input with W −nk
N
with n = 1, that’s W −k
2
= (−1)−k, and
get:
4.2.4 Scaling Rule/Nyquist Frequency
From Fig.4.5 we see that the highest frequency ωmax or also −ωmax corresponds to
the center of the series of Fourier coefﬁcients. This we get by inserting j = N/2 in
(4.6):
ΩNyq = π
t
“Nyquist frequency”.
(4.20)
This frequency often is also called the cut-off frequency. If we take a sample, say
every µs (t = 10−6 s), then ΩNyq is 3.14 megaradians/s (if you prefer to think in
frequencies instead of angular frequencies: νNyq = ΩNyq/2π, so here 0.5MHz).

4.2 Theorems and Rules
103
F0
FN/2
FN−1
positive
frequencies
frequencies
negative
coeﬃcients for
Fig. 4.5 Positioning of the Fourier coefﬁcients
Fig. 4.6 Two samples per period: cosine (left); sine (right)
So the Nyquist frequency ΩNyq corresponds to taking two samples per period, as
shown in Fig.4.6.
While we’ll get away with this in the case of the cosine, by the skin of our
teeth, it deﬁnitely won’t work for the sine! Here we grabbed the samples at the
wrong moment, or maybe there was no signal after all (for example because a cable
hadn’t been plugged in, or due to a power cut). In fact, the imaginary part of fk at
the Nyquist frequency always is 0. The Nyquist frequency therefore is the highest
possible spectral component for a cosine wave; for the sine it’s only up to:
ω = 2π(N/2 −1)/(Nt) = ΩNyq(1 −2/N).
Equation(4.20) is our scaling theorem, as the choice of t allows us to stretch or
compress the time axis, while keeping the number of samples N constant. This only
has an impact on the frequency scale running from ω = 0 to ω = ΩNyq. t doesn’t
appear anywhere else!
The normalisation factor we came across in (1.41) and (2.32), is done away with
here, as using discrete Fourier transformation we normalise to the number of samples
N, regardless of the sampling raster t.

104
4
Discrete Fourier Transformation
4.3 Convolution, Cross Correlation, Autocorrelation,
Parseval’s Theorem
Before we’re able to formulate the discrete versions of the (2.34), (2.48), (2.52), and
(2.54), we have to get a handle on two problems:
i. The number of samples N for the two functions f (t) and g(t) we want to convolve
or cross-correlate, must be the same. This often is not the case, for example if f (t)
is the “theoretical” signal we would get for a δ-shaped instrumental resolution
function, which, however, has to be convolved with the ﬁnite resolution function
g(t). There’s a simple ﬁx: we pad the series {gk} with zeros so we get N samples,
just like in the case of series { fk}.
ii. Don’t forget, that { fk} is periodic in N and our “padded” {gk}, too. This means
that negative indices are wrapped-around to the right end of the interval. The
resolution function g(t) mentioned in Fig.4.7, which we assumed to be sym-
metrical, had 3 samples and got padded with 5 zeros to a total of N = 8 and is
displayed in Fig.4.7.
Another extreme example:
Example 4.6 (Rectangle) We’ll remember that a continuous “rectangular function”,
when convolved with itself in the interval −T/2 ≤t ≤+T/2, results in a “triangular
function” in the interval −T ≤t ≤+T . In the discrete case, the “triangle” gets
wrapped in the area −T ≤t ≤−T/2 to 0 ≤t ≤T/2. The same happens to the
“triangle” in the area +T/2 ≤t ≤+T , which gets wrapped to −T/2 ≤t ≤0.
Therefore both halves of the interval are “corrupted” by the wrap-around, so that the
end-result is another constant (cf. Fig.4.8). No wonder! This “rectangular function”
with periodic continuation is a constant! And a constant convolved with a constant
naturally is another constant.
As long as { fk} is periodic in N, there’s nothing wrong with the fact that upon
convolution data from the end/beginning of the interval will be “mixed into” data
from the beginning/end of the interval. If you don’t like that—for whatever reasons—
rather also pad { fk} with zeros, using precisely the correct number of zeros so {gk}
won’t create overlap between f0 and fN−1 any more.
interval
interval
g0
g0
g1
g1
g−1
g7
Fig. 4.7 Resolution function {gk}: without wrap-around (left); with wrap-around (right)

4.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
105
⊗
=
−T
2
−T
2
+ T
2
+ T
2
−T
+ T
−T −T
2
+T
0
+ T
2
−T
2
+ T
2
Fig. 4.8 Convolution of a “rectangular function” with itself: without wrap-around (top); with wrap-
around (bottom)
4.3.1 Convolution
We’ll deﬁne the discrete convolution as follows:
Deﬁnition 4.3 (Discrete convolution)
hk ≡( f ⊗g)k = 1
N
N−1

l=0
flgk−l.
(4.21)
The “convolution sum” is commutative, distributive and associative. The normal-
isation factor 1/N in context: the convolution of { fk} with the “discrete δ-function”
{gk} = Nδk,0 is to leave the series { fk} unchanged. Following this rule, also a
“normalised” resolution function {gk} should respect the condition N−1
k=0 gk = N.
Unfortunately often the convolution also gets deﬁned without the prefactor 1/N.
The Fourier transform of {hk} is:
Hj = 1
N
N−1

k=0
1
N
N−1

l=0
flgk−lW −kj
N
=
1
N 2
N−1

k=0
N−1

l=0
flW −lj
N gk−lW −kj
N
W +lj
N
↑
extended
↑
(4.22)
=
1
N 2
N−1

l=0
flW −lj
N
N−1−l

k′=−l
gk′W −k′ j
N
with k′ = k −l
= FjG j.

106
4
Discrete Fourier Transformation
In our last step we took advantage of the fact that, due to the periodicity in N,
the second sum may also run from 0 to N −1 instead of −l to N −1 −l. This,
however, makes sure that the current index l has been totally eliminated from the
second sum, and we get the product of the Fourier transform Fj and G j. So we arrive
at the discrete Convolution Theorem:
{ fk} ↔

Fj

,
{gk} ↔

G j

,
{hk} = {( f ⊗g)k} ↔

Hj

=

Fj · G j

.
(4.23)
The convolution of the series { fk} and {gk} results in a product in the Fourier
space.
The inverse Convolution Theorem is:
{ fk} ↔

Fj

,
{gk} ↔

G j

,
{hk} = { fk · gk} ↔

Hj

=

N(F ⊗G) j

.
(4.24)
Proof (Inverse Convolution Theorem)
Hj = 1
N
N−1

k=0
fkgkW −kj
N
= 1
N
N−1

k=0
fkgk
N−1

k′=0
W −k′ j
N
δk,k′

	

k′-sum “artiﬁcially” introduced
=
1
N 2
N−1

k=0
fk
N−1

k′=0
gk′W −k′ j
N
N−1

l=0
W −l(k−k′)
N

	

l-sum yields Nδk,k′
=
N−1

l=0
1
N
N−1

k=0
fkW −lk
N
1
N
N−1

k′=0
gk′W −k′( j−l)
N
=
N−1

l=0
FlG j−l = N(F ⊗G) j. □

4.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
107
Example 4.7 (Nyquist frequency with N = 8)
{ fk} = {1, 0, 1, 0, 1, 0, 1, 0},
{gk} = {4, 2, 0, 0, 0, 0, 0, 2}.
-
-
The “resolution function” {gk} is padded to N = 8 with zeros and normalised to
7
k=0 gk = 8. The convolution of { fk} with {gk} results in:
{hk} =
1
2, 1
2, 1
2, 1
2, 1
2, 1
2, 1
2, 1
2

,
meaning, that everything gets “ﬂattened”, because the resolution function (here
triangle-shaped) has a full half-width of t and consequently doesn’t allow the
recording of oscillations with the period t. The Fourier transform therefore is
Hk = 1/2δk,0. Using the Convolution Theorem (4.23) we would get:
{Fj} =
1
2, 0, 0, 0, 1
2, 0, 0, 0

.
The result is easy to understand: the average is 1/2, at the Nyquist frequency we
have 1/2, all other elements are 0.
G0 = 1
1
8 × average

G1 = 1
2 +
√
2
4
1
8{4 + 2 × “4:30 a.m.” + 2 × “1:30 p.m.”}

G2 = 1
2
1
8{4 + 2 × “6:00 a.m.” + 2 × “12:00 midnight”}

G3 = 1
2 −
√
2
4
1
8{4 + 2 × “7:30 a.m.” + 2 × “10:30 a.m. next day”}

G4 = 0
1
8{4 + 2 × “9:00 a.m.” + 2 × “9:00 p.m. next day”}


108
4
Discrete Fourier Transformation
G5 = 1
2 −
√
2
4
G6 = 1
2
G7 = 1
2 +
√
2
4
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
because of real input,
hence:
{G j} =

1, 1
2 +
√
2
4 , 1
2, 1
2 −
√
2
4 , 0, 1
2 −
√
2
4 , 1
2, 1
2 +
√
2
4

.
For the product we get Hj = FjG j = {1/2, 0, 0, 0, 0, 0, 0, 0}, like we
should for the Fourier transform. If we’d taken the Convolution Theorem seriously
right from the beginning, then the calculation of G0 (average) and G4 at the Nyquist
frequency would have been quite sufﬁcient, as all other Fj = 0. The fact that the
Fourier transform of the resolution function for the Nyquist frequency is 0, precisely
means that with this resolution function we’re not able to record oscillations with the
Nyquist frequency any more. Our inputs, however, were only the frequency 0 and
the Nyquist frequency.
4.3.2 Cross Correlation
We deﬁne for the discrete cross correlation between { fk} and {gk}, similar to what
we did in (2.48):
Deﬁnition 4.4 (Discrete cross correlation)
hk ≡( f ⋆g)k = 1
N
N−1

l=0
fl · g∗
l+k.
(4.25)
If the indices at gk go beyond N −1, then we’ll simply subtract N (periodicity).
The cross correlation between { fk} and {gk}, of course, results in a product of their
Fourier transforms:
{ fk} ↔

Fj

,
{gk} ↔

G j

,
{hk} = {( f ⋆g)k} ↔

Hj

=

Fj · G∗
j

.
(4.26)
Proof (Discrete cross correlation)
Hj = 1
N
N−1

k=0
1
N
N−1

l=0
flg∗
l+kW −kj
N

4.3 Convolution, Cross Correlation, Autocorrelation, Parseval’s Theorem
109
= 1
N
N−1

l=0
fl
1
N
N−1

k=0
g∗
l+kW −kj
N
with the First Shifting Rule and complex conjugate
= 1
N
N−1

l=0
flG∗
jW −jl
N
= FjG∗
j.
□
4.3.3 Autocorrelation
Here we have { fk} = {gk}, which leads to:
hk ≡( f ⋆f )k = 1
N
N−1

l=0
fl · f ∗
l+k
(4.27)
and:
{ fk} ↔

Fj

,
{hk} = {( f ⋆f )k} ↔

Hj

=

|Fj|2
.
(4.28)
In other words: the Fourier transform of the autocorrelation of { fk} is the modulus
squared of the Fourier series {Fj} or its power representation.
4.3.4 Parseval’s Theorem
We use (4.27) for k = 0, that’s h0 (“without time-lag”), and get on the one side:
h0 = 1
N
N−1

l=0
| fl|2.
(4.29)
On the other side, the inverse transformation of {Hj}, especially for k = 0, results
in (cf. 4.13):
h0 =
N−1

j=0
|Fj|2.
(4.30)

110
4
Discrete Fourier Transformation
Put together, this gives us the discrete version of Parseval’s theorem:
1
N
N−1

l=0
| fl|2 =
N−1

j=0
|Fj|2.
(4.31)
Example 4.8 (“Parseval’s theorem” for N = 2)
{ fl} = {0, 1}
(cf. example for First Shifting Rule
Sect. 4.2.2)
{Fj} = {1/2, −1/2} (here there is only the average F0
and the Nyquist frequency at F1!)
1
2
N

l=0
| fl|2 = 1
2 × 1 = 1
2
N

j=0
|Fj|2 = 1
4 + 1
4 = 1
2.
Caution: Often the prefactor 1/N gets left out when deﬁning Parseval’s theorem.
To stay consistent with all other deﬁnitions, however, it should not be missing here!
4.4 The Sampling Theorem
When discussing the Nyquist frequency, we already mentioned that we need at least
two samples per period to show cosine oscillations at the Nyquist frequency. Now
we’ll turn the tables and claim that as a matter of principle we won’t be looking at
anything but functions f (t) that are “bandwidth-limited”, meaning, that outside the
interval [−ΩNyq, ΩNyq] their Fourier transforms F(ω) are 0. In other words: we’ll
reﬁne our sampling to a degree where we just manage to capture all the spectral
components of f (t). This seemingly “innocent” requirement implies that the peri-
odically continued function and all its derivatives are continuous everywhere, such
that its Fourier series is ﬁnite. Now we’ll skilfully “marry” formulas we’ve learned
when dealing with the Fourier series expansion and the continuous Fourier transfor-
mation with each other, and then pull the sampling theorem out of the hat. For this
purpose we’ll recall (1.26) and (1.27) which show that a periodic function f (t) can
be expanded into an (inﬁnite) Fourier series:

4.4 The Sampling Theorem
111
f (t) =
+∞

k=−∞
Ckei2πkt/T
with Ck = 1
T
T/2

−T/2
f (t)e−i2πkt/T dt.
Since F(ω) is zero outside [−ΩNyq, ΩNyq] we can continue this function periodi-
cally and expand it into an inﬁnite Fourier series. So we replace: f (t) →F(ω), t →
ω, T/2 →ΩNyq and get:
F(ω) =
+∞

k=−∞
Ckeiπkω/ΩNyq
(4.32)
with Ck =
1
2ΩNyq
+ΩNyq

−ΩNyq
F(ω)e−iπkω/ΩNyqdω.
A similar integral also occurs in the deﬁning equation for the inverse continuous
Fourier transformation:
f (t) = 1
2π
+ΩNyq

−ΩNyq
F(ω)eiωtdω.
(4.33)
The integrations boundaries are ±ΩNyq, as F(ω) is bandwidth-limited. When we
compare this with (4.32) we get:
2ΩNyqCk = 2π f (−πk/ΩNyq).
(4.34)
Once we’ve inserted this in (4.32) we get:
F(ω) =
π
ΩNyq
+∞

k=−∞
f (−πk/ΩNyq)eiπkω/ΩNyq.
(4.35)
When we ﬁnally insert this into the deﬁning equation (4.33), we get:
f (t) = 1
2π
+ΩNyq

−ΩNyq
π
ΩNyq
+∞

k=−∞
f

−πk
ΩNyq

eiπkω/ΩNyqeiωtdω

112
4
Discrete Fourier Transformation
=
1
2ΩNyq
+∞

k=−∞
f (−kt)2
+ΩNyq

0
cos ω(t + kt)dω
(4.36)
=
1
2ΩNyq
+∞

k=−∞
f (−kt)2sin ΩNyq(t + kt)
(t + kt)
.
By replacing k →−k (it’s not important in which order the sums are calculated)
we get the Sampling Theorem:
Sampling Theorem: f (t) =
+∞

k=−∞
f (kt)sin ΩNyq(t −kt)
ΩNyq(t −kt) .
(4.37)
In other words, we can reconstruct the function f (t) for all times t from the
samples at the times kt, provided the function f (t) is “bandwidth-limited”, i.e.
it contains no frequencies above ΩNyq = π/Δt. To achieve this, we only need to
multiply f (kt) with the function sin x
x
(with x = ΩNyq(t −kt)) and sum up over
all samples. The factor sin x
x
naturally is equal to 1 for t = kt, for other times, sin x
x
decays and slowly oscillates towards zero, which means, that f (t) is a composite
of plenty of
 sin x
x

-functions at the location t = kt with the amplitude f (kt).
Note that for adequate sampling with t =
π
ΩNyq each k−term in the sum in (4.37)
contributes f (kt) at the sampling points t = kt and zero at all other sampling
points whereas all terms contribute to the interpolation between sampling points.
Example 4.9 (Sampling Theorem with N = 2)
We expect:
f (t) = 1
2 + 1
2 cos ΩNyqt = cos2 ΩNyqt
2
.
The sampling theorem tells us:
f (t) =
+∞

k=−∞
fk
sin ΩNyq(t −kt)
ΩNyq(t −kt)
with fk = δk,even (with periodic continuation)
= sin ΩNyqt
ΩNyqt
+
+∞

l=1
sin ΩNyq(t −2lt)
ΩNyq(t −2lt)
+
+∞

l=1
sin ΩNyq(t + 2lt)
ΩNyq(t + 2lt)
with the substitution k = 2l

4.4 The Sampling Theorem
113
= sin ΩNyqt
ΩNyqt
+
+∞

l=1

sin 2π

t
2t −l

2π

t
2t −l

+ sin 2π

t
2t + l

2π

t
2t + l


with ΩNyqt = π
= sin ΩNyqt
ΩNyqt
+ 1
2π
+∞

l=1

t
2t + l

sin ΩNyqt +

t
2t −l

sin ΩNyqt

t
2t −l
 
t
2t + l

= sin ΩNyqt
ΩNyqt
+ sin ΩNyqt
2π
2t
2t
+∞

l=1
1

t
2t
2 −l2
= sin ΩNyqt
ΩNyqt
⎛
⎜⎝1 +
ΩNyqt
2π
2
2
+∞

l=1
1
 ΩNyqt
2π
2
−l2
⎞
⎟⎠
with [8, No 1.421.3]
= sin ΩNyqt
ΩNyqt
π ΩNyqt
2π
cot πΩNyqt
2π
= sin ΩNyqt 1
2
cos(ΩNyqt/2)
sin(ΩNyqt/2)
= 2 sin(ΩNyqt/2) cos(ΩNyqt/2)1
2
cos(ΩNyqt/2)
sin(ΩNyqt/2) = cos2 
ΩNyqt/2

. (4.38)
Please note that we actually do need all summation terms of k = −∞to k = +∞!
If we had only taken k = 0 and k = 1 into consideration, we would have got:
f (t) = 1sin ΩNyqt
ΩNyqt
+ 0sin ΩNyq(t −t)
ΩNyq(t −t)
= sin ΩNyqt
ΩNyqt
which would not correspond to the input of cos2(ΩNyqt/2). We still would have,
as before, f (0) = 1 and f (t = t) = 0, but for 0 < t < t, we wouldn’t have
interpolated correctly, as sin x
x
slowly decays for big x, while we, however, want to get
a periodic oscillation that doesn’t decay as input. You will realise, that the sampling
theorem—similar to Parseval’s equation (1.50)—is good for the summation of certain
inﬁnite series.
What happens if, for some reason or other, our sampling happens to be too coarse
and F(ω) above ΩNyq was unequal to 0? Quite simple: the spectral density above
ΩNyq will be “reﬂected” to the interval 0 ≤ω ≤ΩNyq, meaning that the true spectral
density gets “corrupted” by the part that would be outside the interval.
Example 4.10 (Not enough samples) We’ll take a cosine input and a bit less than
two samples per period (cf. Fig.4.9).

114
4
Discrete Fourier Transformation
F0
F0
F1
F1
F2
F2
F3
F3
ΩN
ΩN
“correct”
“wrong”
Fig. 4.9 Less than two samples per period (top): cosine input (solid line); “apparently” lower
frequency (dotted line). Fourier coefﬁcients with wrap-around (bottom)
ΩNyq
ΩNyq
2ΩNyq
2ΩNyq
“correct”
“wrong”
Fig. 4.10 Slightly more than one sample per period (top): cosine input (solid line); “apparently”
lower frequency (dotted line). Fourier coefﬁcients with wrap-around (bottom)
Here there are eight samples for ﬁve periods, and that means that ΩNyq has been
exceeded by 25 %. The broken line in Fig.4.9 shows that a function with only three
periods would produce the same samples within the same interval.
Therefore the discrete Fourier transformation will show a lower spectral compo-
nent, namely at ΩNyq −25 %. This will become quite obvious, indeed, when we use
only slightly more than one sample per period.
Here {Fj} produces only a very low-frequency component (cf. Fig.4.10). In other
words: spectral density that would appear at ≈2ΩNyq, appears at ω ≈0! This “cor-
ruption” of the spectral density through insufﬁcient sampling is called “aliasing”,
similar to someone acting under an assumed name. In a nutshell: When sampling,

4.4 The Sampling Theorem
115
rather err on the ﬁne side than the coarse one! Coarser rasters can always be achieved
later on by compressing data sets, but it will never work the other way round!
4.5 Data Mirroring
Often we have a situation where, on top of the samples { fk}, we also know that the
series starts with f0 = 0 or at f0 with horizontal tangent ( ∧= slope = 0). In this
case we should use data mirroring forcing a situation where the input is an odd or an
even series (cf. Fig.4.11):
odd:
f2N−k = −fk
k = 0, 1, . . . , N −1,
here we put fN = 0;
even:
f2N−k = + fk
k = 0, 1, . . . , N −1,
here fN is undetermined!
(4.39)
For odd series we put fN = 0, as would be the case for periodic continuation
anyway. For even series this is not necessarily the case. A possibility to determine
fN would be fN = f0 (as if we wanted to continue the non-mirrored data set
periodically). In our example of Fig.4.11 this would result in a δ-spike at fN, which
wouldn’t make sense. Equally, in our example fN = 0 can’t be used (another δ-
spike!). A better choice would be fN = fN−1, and even better fN = −f0 for the
present case. The optimum choice, however, depends on the respective problem. So,
for example, in the case of a cosine with window function and subsequently plenty
of zeros, fN = 0 would be the correct choice (cf. Fig.4.12).
Now the interval is twice as long! Apply the normal fast Fourier transformation
and you’ll have a lot of fun with it, even if (or maybe exactly because of it?) the
real part (in the case of odd mirroring) or the imaginary part (in the case of even
odd:
0
N −1
−(N −1)
0
N −1
0
2N −1
“wrap-around”
even:
0
N −1
−(N −1)
0
N −1
0
2N −1
“wrap-around”
Fig. 4.11 Odd/even input, forced by data mirroring

116
4
Discrete Fourier Transformation
0
N −1
0
fN
2N −1
Fig. 4.12 Example for the choice of fN
π
2π
1
−1
π
2π
k = 0
k = 1
k = 2
k = 3
Fig. 4.13 Basis functions for cosine- (left) and for sine-transformation (right)
mirroring) is full of zeros. If you don’t like that, use a more efﬁcient algorithm using
the fast sine- or cosine-transformation.
As we can see in Fig.4.13, for these sine- or cosine-transformations other basis
functions are being used than the fundamental and harmonics of the normal Fourier
transform, to model the input: also all functions with half the period will occur (the
second half models the mirror image). The normal Fourier transformation of the
mirrored input reads:
Fj =
1
2N
2N−1

k=0
fkW −kj
2N
=
1
2N
#N−1

k=0
fkW −kj
2N +
2N−1

k=N
fkW −kj
2N
$
=
1
2N
#N−1

k=0
fkW −kj
2N +
1

k′=N
f2N−k′W −(2N−k′) j
2N
$
sequence irrelevant
=
1
2N
#N−1

k=0
fkW −kj
2N +
N

k′=1
(±) fk′ W −2N j
2N
 	 
 W +k′ j
2N
$
for
even
odd

= e−2πi 2N j
2N = 1
=
1
2N
 1
−i
 N−1

k=0
fk × 2
#
cos 2πkj
2N
sin 2πkj
2N
$
+ fNe−iπ j −f0

=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
1
N
N−1

k=0
fk cos πkj
N
+ 1
2N

fNe−iπ j −f0

even
−i
N
N−1

k=0
fk sin πkj
N
odd
.

4.5 Data Mirroring
117
The expressions (1/N) N−1
k=0 fk cos(πkj/N) and (1/N) N−1
k=0 fk sin(πkj/N)
are called cosine- and sine-transformation. Please note:
i. The arguments for the cosine-/sine-function are πkj/N and not 2πkj/N! This
shows, that half periods as basis function are also allowed (cf. Fig.4.13).
ii. In the case of the sine transformation shifting of the sine boundaries from k′ =
1, 2, . . . , N towards k′ = 0, 1, . . . , N −1 is no problem, as the following has
to be true: fN = f0 = 0. Apart from the factor −i the sine transformation is
identical to the normal Fourier transformation of the mirrored input, though it
only has half as many coefﬁcients. The inverse sine transformation is identical
to the forward transformation, with the exception of the normalisation.
iii. In the case of the cosine transformation, the terms (1/2N)( fNe−iπ j −f0) stay,
except if they happen to be equal to 0. That means, that generally the cosine
transformation will not be identical to the normal Fourier transformation of the
mirrored input! (Fig.4.14).
iv. Obviously Parseval’s theorem does not apply to the cosine transformation.
v. Obviously the inverse cosine transformation is not identical to the forward trans-
formation, apart from factors.
Example 4.11 (“Constant”, N = 4)
{ fk} = 1
for all k.
The normal Fourier transformation of the mirrored input is:
F0 = 1
88 = 1,
all other Fj = 0.
f0
f0
f1
f1
f2
f2
f3
f3
f4
f5
f6
f7
best choice is f4 = 1
Fig. 4.14 Input without mirroring (left); with mirroring (right)

118
4
Discrete Fourier Transformation
Cosine transformation:
Fj = 1
4
3

k=0
cos πkj
4
=
⎧
⎨
⎩
1
44 = 1 for j = 0
1
4δ j,odd for j ̸= 0
.
Here the ﬂip-side is that, because of cos(πkj/N), the clock-hand or its projection
ontotherealaxisonlyrunaroundhalfasfast,andconsequentlyrelation (4.8)becomes
false.
The extra terms can be omitted only if f0 = fN = 0 is true, as for example in
Fig.4.15.
If you insist on using the cosine transformation, “correct” it using the term:
1
2N ( fNe−iπ j −f0).
Then you get the normal Fourier transformation of the mirrored data set, and no harm
was done. In our above example, the one with the constant input, this would look as
shown in Fig.4.16.
N =4
Fig. 4.15 Input (left); with correct mirroring (right)
cosine
transformation:
1
4
1
4
F0
F1
F2
F3
1
+
+ 1
8 1e−iπj −1
= −1
4δj,odd
−1
4
−1
4
=
1
Fig. 4.16 Cosine transformation with correcting terms

4.6 How to Get Rid of the “Straight-Jacket” Periodic Continuation? …
119
4.6 How to Get Rid of the “Straight-Jacket” Periodic
Continuation? By Using Zero-Padding!
So far, we had chosen all our examples in a way where { fk} could be continued
periodically without a problem. For example, we truncated a cosine precisely where
there was no problem continuing the cosine-shape periodically. In practice, this often
can’t be done:
i. we’d have to know the period in the ﬁrst place to be able to know where to
truncate and where not;
ii. if there are several spectral components, we’d always cut off a component at the
wrong time (for the purists: except if the sampling interval can be chosen to be
equal to the smallest common denominator of the single periods).
Example 4.12 (Truncation) See what happens for N = 4:
Without truncation error:
With maximum truncation error:

120
4
Discrete Fourier Transformation
W4 = eiπ/2
F0 = 1
4
(average)
F1 = 1
4

1 +

−1
√
2

× “6:00 a.m.” +

+ 1
√
2

× “12:00 noon”

= 1
4

1 +
i
√
2
+
i
√
2

= 1
4 +
i
2
√
2
F2 = 1
4

1 +

−1
√
2

× “9:00 a.m.” +

+ 1
√
2

× “9:00 p.m.”

= 1
4

1 + 1
√
2
−1
√
2

= 1
4
F3 = F∗
1 .
(4.40)
Two “strange ﬁndings”:
i. Through truncation we suddenly got an imaginary part, in spite of using a cosine
as input. But our function isn’t even at all, because we continue using fN = −1,
instead of fN = f0 = +1, as we originally intended to do. This function contains
an even and an odd portion (cf. Fig.4.17).
ii. We really had expected a Fourier coefﬁcient between half the Nyquist frequency
and the Nyquist frequency, possibly spread evenly over F1 and F2, and not a
constant, like we would have had to expect for the case of a δ-function as input:
but we’ve precisely entered this as “even” input.
The “odd” input is a sine wave with amplitude −1/
√
2 and therefore results in an
imaginary part of F1 = 1/2
√
2; the intensity −1/2
√
2, split “between sisters”, is to
be found at F3, the positive sign in front of Im{F1} means negative amplitude (cf.
the remarks in (4.14) about Bavarian clocks).
Instead of more discussions about truncation errors in the case of cosine inputs, we
recall that ω = 0 is a frequency “as good as any”. So we want to discuss the discrete
analog to the function sin x
x , the Fourier transform of the “rectangular function”. We
use as input:
=
1
0
−1
√
2
+ 1
√
2
even
+
odd
Fig. 4.17 Decomposition of the input into an even and an odd portion

4.6 How to Get Rid of the “Straight-Jacket” Periodic Continuation? …
121
fk =
⎧
⎨
⎩
1 for 0 ≤k ≤M
0 else
1 for N −M ≤k ≤N −1
(4.41)
and stick with period N. This corresponds to a “rectangular window” of width 2M+1
(M arbitrary, yet < N/2). Here, the half corresponding to negative times has been
wrapped onto the right end of the interval. Please note, that we can’t help but require
an odd number of fk ̸= 0 to get an even function. An example with N = 8, M = 2
is shown in Fig.4.18.
For general M < N/2 and N the Fourier transform is:
Fj = 1
N
# M

k=0
W −kj
N
+
N−1

k=N−M
W −kj
N
$
= 1
N
#
2
M

k=0
cos(2πkj/N) −1
$
.
The sum can be calculated using (1.53), which we came across when dealing with
Dirichlet’s integral kernel. We have:
1
2 + 1
2 + cos x + cos 2x + · · · + cos Mx = 1
2 +
sin

M + 1
2

x
2 sin x
2
with x = 2π j/N,
thus:
Fj = 1
N
⎛
⎜⎜⎝1 +
sin

M + 1
2
 2π j
N
sin 2π j
2N
−1
⎞
⎟⎟⎠= 1
N
⎛
⎜⎝
sin 2M + 1
N
π j
sin π j
N
⎞
⎟⎠
(4.42)
for j = 0, . . . , N −1.
f0
f−2
f−1
f1
f2
f3
f4
f5
f6
f7
f8
Fig. 4.18 “Rectangular” input using N = 8, M = 2

122
4
Discrete Fourier Transformation
j
Fig. 4.19 Equation(4.42) (points); 2M+1
N
sin x
x
with x = 2M+1
N
π j (thin line)
This is the discrete version of the function sin x
x
which we would get in the
case of the continuous Fourier transformation (cf. Fig.2.1 for our above example).
Figure4.19 shows the result for N = 64 and M = 8 in comparison to sin x
x .
What happens at j = 0? There’s a trick: j temporarily is treated like a continuous
variable and l’Hospital’s rule is applied:
F0 = 1
N
2M + 1
N

π
π/N
= 2M + 1
N
“average”.
(4.43)
We had used 2M + 1 series elements fk = 1 as input. Only in this range the
denominator can become 0.
Where are the zeros of the discrete Fourier transform of the discrete “rectangular
window”? Funny, there is no Fj, that is exactly equal to 0, as 2M+1
N
π j = lπ,
l = 1, 2, . . . or j = l
N
2M+1 and j = integer can only be achieved for l = 2M +1, and
then j already is outside the interval. Of course, for M ≫1 we may approximately
put j ≈l N
2M and then get 2M−1 “quasi-zero transitions”. This is different compared
to the function sin x
x , where there are real zeros. The oscillations around zero next to
the central peak at j = 0 decay only very slowly; even worse, after j = N/2 the
denominator starts getting smaller, and the oscillations increase again! Don’t panic:
in the right half of {Fj} there is the mirror image of the left half! What’s behind
the difference to the function sin x
x ? It’s the periodic continuation in the case of the
discrete Fourier transformation! We transform a “comb” of “rectangular functions”!
For j ≪N, i.e. far from the end of the interval, we get:
Fj = 1
N
sin 2M + 1
N
π j
π j/N
= 2M + 1
N
sin x
x
with x = 2M + 1
N
π j,
(4.44)

4.6 How to Get Rid of the “Straight-Jacket” Periodic Continuation? …
123
and that’s exactly what we’d have expected in the ﬁrst place. In the extreme case of
M = N/2 −1 we get for j ̸= 0 from (4.42):
Fj = 1
N
sin N −1
N
π j
sin(π j/N)
= −1
N eiπ j,
which we can just manage to compensate by plugging the “hole” at fN/2 (cf. Sect.4.5,
cosine transformation). Let’s take a closer look at the extreme case of large N and
large M (but with 2M ≪N). In this limit we really get the same “zeros” as in
function sin x
x . Here we have a situation somewhat like the transition from the discrete
to the continuous Fourier transformation (especially so if we only look at the Fourier
coefﬁcients Fj with j ≪N). Now we also understand why there are no sidelobes
in the case of a discrete Fourier transformation of a cosine input without truncation
errors and without zero-padding: the Fourier coefﬁcients neighbouring the central
peak are precisely where the zeros are. Then the Fourier transformation works like
a—meanwhile obsolete—vibrating-reed frequency meter. This sort of instrument
was used in earlier times to monitor the mains frequency of 50 cycles (60 cycles
in the US and some other countries). Reeds with distinctive eigen-frequencies, for
example 47, 48, 49, 50, 51, 52, 53 cycles, are activated using a mains-driven coil:
only the reed with the proper eigen-frequency at the current mains-frequency will
start vibrating, all others will keep quiet. These days, no energy supplier will get away
with supplying 49 or 51 cycles, as this would cause all inexpensive (alarm)clocks
(without quartz-control) to get out of sync. What’s true for the frequency ω = 0,
of course also is true for all other frequencies ω ̸= 0, according to the Convolution
Theorem. This means that we can only get a consistent line proﬁle of a spectral line
that doesn’t depend on truncation errors if we use zero-padding, and make it plenty
of zeros.
So here is the 1st recommendation:
Many zeros are good! N very big; 2M ≪N.
The economy and politics also obey this rule.
Since available Fourier transformation programs do not offer zero-padding you
have to do it yourself.
2nd recommendation:
Choose your sampling-interval t ﬁne enough, so that your Nyquist fre-
quency is always substantially higher than the expected spectral inten-
sity, meaning, you need Fj only for j ≪N. This should get rid of the
consequences of the periodic continuation approximately!
In Chap.3 we quite extensively discussed continuous window functions. A very
good presentation of window functions in the case of the discrete Fourier transforma-
tion can be found in Harris [9]. We’re happy to know, however, that we may transfer

124
4
Discrete Fourier Transformation
0
M
ΩNyq
N −1 −M
N −1
ﬁlling with zeros
no intensity here, i.e., sampled ﬁne enough
even input
“half” window
input with window
input with window, mirrored, ﬁlled with zeros
Fourier transform
Fig. 4.20 “Cooking recipe” for the Fourier transformation for an even input; in case of an odd
input invert the mirror image
all the properties of a continuous window function to the discrete Fourier transforma-
tion straight away, if, by using enough zeros for padding and using the low-frequency
portion of the Fourier series, we aim for the limes discrete →continuous.
So here comes the 3rd recommendation:
Do use window functions!
These three recommendations are illustrated in Fig.4.20 in an easy-to-remember
way. If you know that the input is even or odd, respectively, data mirroring is always
recommended. Should you have zero-padded your data you must window your data

4.6 How to Get Rid of the “Straight-Jacket” Periodic Continuation? …
125
yourself and eventually use mirroring before clicking the FFT button. Do not use
offered window functions, they would extend over the zero-padded region as well.
Then use the “rectangular” window, i.e. no further window.
If the input is neither even nor odd, you can force the input to become even or odd,
respectively, provided all spectral components have the same phase. The situation is
more complicated if the input contains even and odd components, i.e. the spectral
components have different phases. If these components are well separated you can
shift the phase for each component individually. If these components are not well
separated use the full window function, i.e. don’t mirror the data, than zero-padd
and Fourier transform. Now, the real and the imaginary part depend on where you
zero-padd: at the beginning, at the end, or both. In this case the power representation
is recommended.
In spite of the fact that today’s fast PCs won’t have a problem transforming very
big data sets any more, the application of the Fourier transformation got a huge boost
from the “Fast Fourier transformation” algorithm by Cooley and Tukey, an algorithm
that doesn’t grow with N 2 calculations but only N ln N.
We’ll have a closer look at this algorithm in the next section.
4.7 Fast Fourier Transformation (FFT)
Cooley and Tukey started out from the simple question: what is the Fourier transform
of a series of numbers with only one real number (N = 1)? There are at least
3 answers:
i. Accountant:
From (4.12) with N = 1 follows:
F0 = 1
1 f0W −0
1
= f0.
(4.45)
ii. Economist:
From (4.31) (Parseval’s theorem) follows:
|F0|2 = 1
1|( f0)|2.
(4.46)
Using the services of someone into law: f0 is real and even, which leads to
F0 = ± f0, and as F0 is also to be equal to the average of the series of numbers,
there’s no chance of getting a minus sign.
(A layperson would have done without all this lead-in talk!)
iii. Philosopher:
We know that the Fourier transform of a δ-function results in a constant and vice
versa. How do we represent a constant in the world of 1-term series? By using
the number f0. How do we represent in this world a δ-function? By using this
number f0. So in this world there’s no difference any more between a constant
and a δ-function. Result: f0 is its own Fourier transform.

126
4
Discrete Fourier Transformation
This ﬁnding, together with the trick to achieve N = 1 by smartly halving the input
again and again (that’s why we have to stipulate: N = 2p, p integer), (almost) saves
us the Fourier transformation. For this purpose, let’s ﬁrst have a look at the ﬁrst
subdivision. We’ll assume as given: { fk} with N = 2p. This series will get cut up in
a way that one subseries will only contain the even elements and the other subseries
only the odd elements of { fk}:
{ f1,k} = { f2k}
k = 0, 1, . . . , M −1,
{ f2,k} = { f2k+1}
M = N/2.
(4.47)
Both subseries are periodic in M.
Proof (Periodicity in M)
f1,k+M = f2k+2M = f2k = f1,k
because of 2M = N and f periodic in N.
Analogously for f2,k. □
The respective Fourier transforms are:
F1, j = 1
M
M−1

k=0
f1,kW −kj
M
,
F2, j = 1
M
M−1

k=0
f2,kW −kj
M
,
j = 0, . . . , M −1.
(4.48)
The Fourier transform of the original series is:
Fj = 1
N
N−1

k=0
fkW −kj
M
= 1
N
M−1

k=0
f2kW −2kj
N
+ 1
N
M−1

k=0
f2k+1W −(2k+1) j
N
(4.49)
= 1
N
M−1

k=0
f1,kW −kj
M
+ W −j
N
N
M−1

k=0
f2,kW −kj
M
,
j = 0, . . . , N −1.
In our last step we used:
W −2kj
N
= e−2×2πikj/N = e−2πikj/(N/2) = W −kj
M
,
W −(2k+1) j
N
= e−2πi(2k+1) j/N = W −kj
M
W −j
N .

4.7 Fast Fourier Transformation (FFT)
127
Together we get:
Fj = 1
2 F1, j + 1
2W −j
N F2, j,
j = 0, . . . , N −1,
or better:
Fj = 1
2(F1, j + F2, jW −j
N ),
Fj+M = 1
2(F1, j −F2, jW −j
N ),
j = 0, . . . , M −1.
(4.50)
Please note that in (4.50) we allowed j to run from 0 to M −1 only. In the second
line in front of F2, j we have used:
W −( j+M)
N
= W −j
N W −M
N
= W −j
N W −N/2
N
= W −j
N e−2πi N
2 /N
= W −j
N e−iπ = −W −j
N .
(4.51)
This “decimation in time” can be repeated until we ﬁnally end up with 1-term
series whose Fourier transforms are identical to the input number, as we know. The
conventional Fourier transformation requires N 2 calculations, whereas here we only
need pN = N ln N.
Example 4.13 (“Saw-tooth” with N = 2)
f0 = 0,
f1 = 1.
-
Normal Fourier transformation:
W2 = eiπ = −1
F0 = 1
2(0 + 1) = 1
2
F1 = 1
2

0 + 1 × W −1
2

= −1
2.
(4.52)
Fast Fourier transformation:
f1,0 = 0 even part →F1,0 = 0
f2,0 = 1 odd part →F2,0 = 1,
M = 1.
(4.53)
From formula (4.50) we get:
F0 = 1
2
⎛
⎜⎝F1,0 + F2,0 W 0
2
	
=1
⎞
⎟⎠= 1
2
F1 = 1
2

F1,0 −F2,0W 0
2

= −1
2.
(4.54)

128
4
Discrete Fourier Transformation
This didn’t really save all that much work so far.
Example 4.14 (“Saw-tooth” with N = 4)
f0 = 0
f1 = 1
f2 = 2
f3 = 3.
-
The normal Fourier transformation gives us:
W4 = e2πi/4 = eπi/2 = i
F0 = 1
4(0 + 1 + 2 + 3) = 3
2
“average”
F1 = 1
4

W −1
4
+ 2W −2
4
+ 3W −3
4

= 1
4
1
i + 2
−1 + 3
−i

= −1
2 + i
2
F2 = 1
4

W −2
4
+ 2W −4
4
+ 3W −6
4

= 1
4(−1 + 2 −3) = −1
2
F3 = 1
4

W −3
4
+ 2W −6
4
+ 3W −9
4

= 1
4

−1
i −2 + 3
i

= −1
2 −i
2.
(4.55)
This time we’re not using the trick with the clock, yet another playful approach.
We can skillfully subdivide the input and thus get the Fourier transform straight away
(cf. Fig.4.21).
Using 2 subdivisions, the Fast Fourier transformation gives us:
1st subdivision:
N = 4
{ f1} = {0, 2} even,
M = 2
{ f2} = {1, 3} odd.
(4.56)
2nd subdivision (M′ = 1):
f11 = 0 even ≡F1,1,0,
f12 = 2 odd ≡F1,2,0,
f21 = 1 even ≡F2,1,0,
f22 = 3 odd ≡F2,2,0.
Using (4.50) this results in ( j = 0, M′ = 1):
upper part
lower part
F1,k =
1
2 F1,1,0 + 1
2 F1,2,0, 1
2 F1,1,0 −1
2 F1,2,0

= {1, −1},
F2,k =
1
2 F2,1,0 + 1
2 F2,2,0, 1
2 F2,1,0 −1
2 F2,2,0

= {2, −1}

4.7 Fast Fourier Transformation (FFT)
129
odd
const.
δ-function
from all Fj subtract 1
2.
real part:
F0 = 8
4 = 2
F1 = F2 = F3 = 0
imaginary part:
F1 = + 1
2
F3 = −1
2
F0 = 0
(always)
F2 = 0
(Nyquist)
Fig. 4.21 Decomposition of the saw-tooth into an odd part, constant plus δ-function. Add up the
Fk, and compare the result with (4.55)
and ﬁnally, using (4.50) once again:
upper part
⎧
⎪⎪⎨
⎪⎪⎩
F0 = 1
2(F1,0 + F2,0) = 3
2,
F1 = 1
2

F1,1 + F2,1W −1
4

= 1
2

−1 + (−1) × 1
i

= −1
2 + i
2,
lower part
⎧
⎪⎪⎨
⎪⎪⎩
F2 = 1
2(F1,0 −F2,0) = −1
2,
F3 = 1
2

F1,1 −F2,1W −1
4

= 1
2

−1 −(−1) × 1
i

= −1
2 −i
2.
We can represent the calculations we’ve just done in the following diagram, where
we’ve left out the factors 1/2 per subdivision—they can be taken into account at the
end when calculating the Fj (Fig.4.22).
Here
→
↗⊕means add and
↘
→⊖subtract and W −j
4
multiply with weight W −j
4 .
This subdivision is called “decimation in time”; the scheme:

130
4
Discrete Fourier Transformation
⊕
⊕
⊕
⊕
Input
Output
F2,2,0
F2,1,0
F1,2,0
F1,1,0
F3
F2
F1
F0
F2,1
F2,0
F1,1
F1,0
W −1
4
W 0
4
Fig. 4.22 Flow-diagram for the FFT with N = 4
-
-
-
-
-
-

R⊖
⊕
W −j
N
is called “butterﬂy scheme”, which, for example, is used as a building-block in
hardware Fourier analysers. Figure4.23 illustrates the scheme for N = 16.
Those in the know will have found that the input is not required in the normal
order f0 . . . fN, but in bit-reversed order (arabic from right to left).
Example 4.15 (Bit-reversal for N = 16)
k
binary reversed results in k′
0
0000
0000
0
1
0001
1000
8
2
0010
0100
4
3
0011
1100
12
4
0100
0010
2
5
0101
1010
10
6
0110
0110
6
7
0111
1110
14
8
1000
0001
1
9
1001
1001
9
10 1010
0101
5
11 1011
1101
13
12 1100
0011
3
13 1101
1011
11
14 1110
0111
7
15 1111
1111
15

4.7 Fast Fourier Transformation (FFT)
131
0
0
0
0
0
0
0
0
0
0
0
0
4
4
4
4
0
0
2
2
4
4
6
6
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Time
Frequency
Fig. 4.23 Decimation in Time with N = 16
Computers have no problem with this bit-reversal.
At the end, let’s have a look at a simple example:
with for example
7
= W −7
16
.
Example 4.16 (Half Nyquist frequency)
fk = cos(πk/2),
k = 0, . . . , 15,
i.e.
f0 = f4 = f8 = f12 = 1,
f2 = f6 = f10 = f14 = −1,
all odd ones are 0.
The bit-reversal orders the input in such a way that we get zeros in the lower half
(cf. Fig.4.24). If both inputs of the “butterﬂy scheme” are 0, i.e. we surely get 0
at the output, we do not show the add-/subtract-crosses. The intermediate results of
the required calculations are quoted. The weights W 0
16 = 1 are not quoted for the
sake of clarity. Other powers do not show up in this example. You see, the input is
progressively “compressed” in four steps. Finally, we ﬁnd a number 8 at negative and

132
4
Discrete Fourier Transformation
−1
0
1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
k
fk
0
0
0
0
0
0
0
0
−1
−1
−1
−1
1
1
1
1
2
2
−2
−2
0
0
0
0
0
0
−4
4
⊕
⊕
⊕
⊕
⊕
⊕
⊕
⊕
8
0
0
0
0
8
8
0
0
0
0
0
0
0
0
0
0
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
k
fk
Input
Fj
Output
j
Fig. 4.24 Half Nyquist frequency
positive half Nyquist frequency each which we are allowed to add and subsequently
have to divide by 16 which ﬁnally yields the amplitude of the cosine input, i.e. 1.
Playground
4.1 Correlated
What is the cross correlation of a series { fk} with a constant series {gk}? Sketch the
procedure with Fourier transforms!
4.2 No Common Ground
Given is the series { fk} = {1, 0, −1, 0} and the series {gk} = {1, −1, 1, −1}.
Calculate the cross correlation of the two series.

Playground
133
4.3 Brotherly
Calculate the cross correlation of { fk} = {1, 0, 1, 0} and {gk} = {1, −1, 1, −1}, use
the Convolution Theorem.
4.4 Autocorrelated
Given is the series { fk} = {0, 1, 2, 3, 2, 1}, N = 6.
Calculate its autocorrelation function. Check your results by calculating the
Fourier transform of fk and of fk ⊗fk.
4.5 Shifting around
Given the following input series (see Fig.4.25):
f0 = 1,
fk = 0
for k = 1, . . . , N −1.
a. Is the series even, odd, or mixed?
b. What is the Fourier transform of this series?
c. The discrete “δ-function” now gets shifted to f1 (Fig.4.26). Is the series even,
odd, or mixed?
d. What do we get for |Fj|2?
4.6 Pure Noise
Given the random input series containing numbers between −0.5 and 0.5.
a. What does the Fourier transform of a random series look like (see Fig.4.27)?
b. How big is the noise power of the random series, deﬁned as:
N−1

j=0
|Fj|2?
(4.57)
Compare the result in the limiting case of N →∞to the signal power of the
input 0.5 cos ωt.
. . .
0
1
2
N −2 N −1
N
k
Fig. 4.25 Input-signal with a δ-shaped pulse at k = 0
. . .
0
1
2
N −2 N −1
N
k
Fig. 4.26 Input-signal with a δ-shaped pulse at k = 1

134
4
Discrete Fourier Transformation
0.5
0
−0.5
fk
k
Fig. 4.27 Random series
Fig. 4.28 Input function according to (4.58)
0
1
2
3
4
5
6
7
8
9
Fig. 4.29 Theoretical pattern (“toothbrush”) that’s to be located in the data set
4.7 Pattern Recognition
Given a sum of cosine functions as input, with plenty of superimposed noise
(Fig.4.28):
fk = cos 5πk
32 + cos 7πk
32 + cos 9πk
32 + 15(RND −0.5)
(4.58)
for k = 0, . . . , 255,
where RND is a random number1 between 0 and 1.
How do you look for the pattern Fig.4.29 that’s buried in the noise, if it represents
the three cosine functions with the frequency ratios ω1 : ω2 : ω3 = 5 : 7 : 9?
4.8 Go on the Ramp (for Gourmets only)
Given the input series:
fk = k for k = 0, 1, . . . , N −1.
1Programming languages such as, for example Turbo-Pascal, C, Fortran, … feature random gener-
ators that can be called as functions. Their efﬁciency varies considerably.

Playground
135
Is this series even, odd, or mixed? Calculate the real and imaginary part of its
Fourier transform. Check your results using Parseval’s theorem. Derive the results
for N−1
j=1 1/ sin2(π j/N) and N−1
j=1 cot2(π j/N).
4.9 Transcendental (for Gourmets only)
Given the input series (with N even!):
fk =
k
for k = 0, 1, . . . , N
2 −1
N −k for k = N
2 , N
2 + 1, . . . , N −1 .
(4.59)
Is the series even, odd, or mixed? Calculate its Fourier transform. The double-
sided ramp is a high-pass ﬁlter (cf. Sect.5.2), which immediately becomes clear
considering the periodic continuation. Use Parseval’s theorem to derive the result
for N/2
k=1 1/ sin4(π(2k −1)/N). Use the fact that a high-pass does not transfer a
constant in order to derive the result for N/2
k=1 1/ sin2(π(2k −1)/N).

Chapter 5
Filter Effect in Digital Data Processing
Abstract This chapter deals with ﬁlter effects in digital data processing. For this
purpose the transfer function is introduced. Simple ﬁlters like high-pass, low-pass,
band-pass, and notch are discussed. The effects of data shifting, data compression
as well as differentiation and integration of discrete data are shown.
In this chapter we’ll discuss only very simple procedures, such as smoothing of data,
shifting of data using linear interpolation, compression of data, differentiating data
and integrating them, and while doing that, describe the ﬁlter effect—something
that’s often not even known to our subconscience. For this purpose, the concept of
the transfer function comes in handy.
5.1 Transfer Function
We’ll take as given a “recipe” according to which the output y(t) is made up of a
linear combination of f (t) including derivatives and integrals:
y(t)

“output′′
=
+k

j=−k
a j f [ j](t)
  
“input′′
with f [ j] = d j f (t)
dt j
(negative j means integration).
(5.1)
This rule is linear and stationary, as a shift along the time axis in the input results
in the same shift along the time axis in the output.
When we Fourier-transform (5.1) we get with (2.57):
Y(ω) =
+k

j=−k
a jFT

f [ j](t)

=
+k

j=−k
a j(iω) j F(ω)
(5.2)
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_5
137

138
5
Filter Effect in Digital Data Processing
or:
Y(ω) = H(ω)F(ω)
with the transfer function H(ω) =
+k

j=−k
a j(iω) j.
(5.3)
When looking at (5.3), we immediately think of the Convolution Theorem.
According to this, we may interpret H(ω) as the Fourier transform of the output
y(t) using δ-shaped input (that’s F(ω) = 1). So weighted with this transfer func-
tion, F(ω) is translated into the output Y(ω). In the frequency domain, we can easily
ﬁlter if we choose an adequate H(ω). Here, however, we want to work in the time
domain.
Now we’ll get into number series. Please note that we’ll get derivatives only over
differences and integrals only over sums of single discrete numbers. Therefore we’ll
have to widen the deﬁnition (5.1) by including non-stationary parts. The operator
V l means shift by l:
V l yk ≡yk+l.
(5.4)
This allows us to state the discrete version of (5.1) as follows:
yk

“output”
=
+L

l=−L
al V l fk

“input”
.
(5.5)
Here, positive l stand for later input samples, and negative l for earlier input
samples. With positive l, we can’t process a data-stream sequentially in “real-time”,
we ﬁrst have to buffer L samples, for example in a shift-register, which often is
called a FIFO (ﬁrst in, ﬁrst out). These algorithms are called acausal. The Fourier
transformation is an example for an acausal algorithm.
The discrete Fourier transformation of (5.5) is:
Y j =
+L

l=−L
alFT

V l fk

=
+L

l=−L
al
1
N
N−1

k=0
fk+lW −kj
N
=
+L

l=−L
al
1
N
N−1+l

k′=l
fk′W −k′ j
N
W +lj
N
=
+L

l=−L
alW +lj
N
Fj = Hj Fj.

5.1 Transfer Function
139
Y j = Hj Fj
with Hj =
+L

l=−L
alW +lj
N
=
+L

l=−L
aleiω jlt and ω j = 2π j/(Nt).
(5.6)
Using this transfer function, which we assume to be continuous out of pure con-
venience,1 that’s H(ω) = 	+L
l=−L aleiωlt, it’s easy to understand the “ﬁlter effects”
of the previously deﬁned operations.
5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
First we’ll look into the ﬁlter effect when smoothing data. A simple 2-point algorithm
for data-smoothing would be, for example:
yk = 1
2( fk + fk+1)
(5.7)
with
a0 = 1
2,
a1 = 1
2.
This gives us the transfer function:
H(ω) = 1
2

1 + eiωt
.
(5.8)
|H(ω)|2 = 1
4(1 + eiωt)(1 + e−iωt) = 1
2 + 1
2 cos ωt = cos2 ωt
2
and ﬁnally:
|H(ω)| = cos ωt
2 .
Figure5.1 shows |H(ω)|.
This has the unpleasant effect that a real input results in a complex output. This,
of course, is due to our implicitly introduced “phase shift” by t/2.
It looks like the following 3-point algorithm will do better:
yk = 1
3 ( fk−1 + fk + fk+1)
(5.9)
with
a−1 = 1
3,
a0 = 1
3,
a1 = 1
3.
1We can always choose N to be large, so j is very dense.

140
5
Filter Effect in Digital Data Processing
Fig. 5.1 Modulus of the
transfer function for the
smoothing-algorithm of (5.7)
ΩNyq
|H(ω)|
ω
This gives us:
H(ω) = 1
3

e−iωt + 1 + e+iωt
= 1
3(1 + 2 cos ωt).
(5.10)
Figure5.2shows H(ω)andtheproblemthatforω = 2π/3t thereisazero,mean-
ing that this frequency will not get transferred at all. This frequency is (2/3)ΩNyq.
Above that, even the sign changes. This algorithm is not consistent and therefore
should not be used.
The “correct” smoothing-algorithm is as follows:
yk = 1
4 ( fk−1 + 2 fk + fk+1)
low-pass
(5.11)
with
a−1 = +1
4,
a0 = +1
2,
a1 = +1
4.
The transfer function now reads:
H(ω) = 1
4

e−iωt + 2 + e+iωt
= 1
4(2 + 2 cos ωt) = cos2 ωt
2 .
(5.12)
ΩNyq
2π
3Δt
H(ω)
ω
Fig. 5.2 Transfer function for the 3-point smoothing-algorithm as of (5.9)

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
141
ΩNyq
H(ω)
ω
logo:
Fig. 5.3 Transfer function for the low-pass
Figure5.3 shows H(ω): there are no zeros, the sign doesn’t change. Comparing
this to (5.8) and Fig.5.1, it’s obvious that the ﬁlter effect now is bigger: cos2(ωt/2)
instead of cos(ωt/2) for |H(ω)|.
Using half the Nyquist frequency we get:
H(ΩNyq/2) = cos2 π
4 = 1
2.
Therefore our smoothing-algorithm is a low-pass ﬁlter, which, admittedly, doesn’t
have a “very steep edge”, and which, at ω = ΩNyq/2, will let only half the amount
pass. So at ω = ΩNyq/2 we have −3 dB attenuation.
If our data is corrupted by low-frequency artefacts (for example slow drifts), we’d
like to use a high-pass ﬁlter. Here’s how we design it:
H(ω) = 1 −cos2 ωt
2
= sin2 ωt
2
= 1
2(1 −cos ωt)
= 1
2

1 −1
2e−iωt −1
2e+iωt

.
(5.13)
So we have: a−1 = −1/4, a0 = +1/2, a1 = −1/4, and the algorithm is:
yk = 1
4 (−fk−1 + 2 fk −fk+1)
high-pass.
(5.14)
From (5.14) we realise straight away: a constant as input will not get through
because the sum of the coefﬁcients ai is zero.
Figure5.4 shows H(ω). Here, too, we can see that at ω = ΩNyq/2 half the amount
will get through only. The experts talk of −3 dB attenuation at ω = ΩNyq/2. We
discussed in Example 4.14 the “saw-tooth”. In the frequency domain this is a high-
pass, too! In a certain image reconstruction algorithm from many projections taken at
different angles, as required in tomography, exactly such high-pass ﬁlters are in use.
They are called ramp ﬁlters. They naturally show up when transforming from carte-
sian to cylinder coordinates. We shall discuss this algorithm, called “backprojection
of ﬁltered projections”, in more detail in Chap.7.

142
5
Filter Effect in Digital Data Processing
ΩNyq
H(ω)
ω
logo:
Fig. 5.4 Transfer function for the high-pass
ΩNyq
H(ω)
ω
logo:
Fig. 5.5 Transfer function of the band-pass
If we want to suppress very low as well as very high frequencies, we need a band-
pass. For simplicity’s sake we take the product of the previously described low- and
high-pass (cf. Fig.5.5):
H(ω) = cos2 ωt
2
sin2 ωt
2
=

1
2 sin ωt
2
= 1
4 sin2 ωt = 1
4
1
2 (1 −cos 2ωt)
= 1
8

1 −1
2e−2iωt −1
2e+2iωt

.
(5.15)
So we have a−2 = −1/16, a0 = +1/8, a+2 = −1/16 and:
fk = 1
16 (−fk−2 + 2 fk −fk+2)
band-pass.
(5.16)
Now, at ω = ΩNyq/2 we have H(ΩNyq/2) = 1/4, that’s −6 dB attenuation.
If we choose to set the complement of the band-pass to 1:
H(ω) = 1 −

1
2 sin ωt
2
,
(5.17)
we’ll get a notch ﬁlter that suppresses frequencies around ω = ΩNyq/2, yet lets all
others pass (cf. Fig.5.6).

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
143
ΩNyq
H(ω)
ω
logo:
Fig. 5.6 Transfer function of the notch ﬁlter
H(ω) can be transformed to:
H(ω) = 1 −1
8 + 1
16e2iωt + 1
16e−2iωt
(5.18)
with
a−2 = +1/16, a0 = +7/8, a+2 = +1/16
and
yk = 1
16 ( fk−2 + 14 fk + fk+2)
notch ﬁlter.
(5.19)
The suppression at half the Nyquist frequency, however, isn’t exactly impressive:
only a factor of 3/4 or −1.25 dB.
Figure5.7 gives an overview/recaps all the ﬁlters we’ve covered.
How can we build better notch ﬁlters? How can we set the cut-off frequency?
How can we set the edge steepness? Linear, non-recursive ﬁlters won’t do the job.
Therefore we’ll have to look at recursive ﬁlters, where part of the output is fed back
as input. In RF-engineering this is called feedback. Live TV-shows with viewers
calling in on their phones know what (acoustic) feedback is: it goes from your
phone’s mouthpiece via plenty of wire (copper or ﬁbre) and various electronics to
the studio’s loudspeakers, and from there on to the microphone, the transmitter and
back to your TV-set (maybe using a satellite for good measure) and on to your phone’s
handset. Quite an elaborate set-up, isn’t it. No wonder we can have lots of fun letting
rip on talkshows using this kind of feedback! Video-experts may use their cameras
to achieve optical feedback by pointing it at the TV-screen that happens to show
exactly this camera and so on. (This is the modern, yet chaos-inducing, version of
the principle of the never-ending mirroring, using two mirrors opposite each other,
ΩNyq
H(ω)
ω
notch ﬁlter
high-pass
low-pass
band-pass
Fig. 5.7 Overview of the transfer functions of the various ﬁlters

144
5
Filter Effect in Digital Data Processing
like, for example, in the Mirror Hall of the Castle of Linderhof, a tourist attraction,
the “little brother” of the castle Neuschwanstein of King Ludwig II. of Bavaria.)
It’s not appropriate to discuss digital ﬁlters in depth here. We’ll only look at a
small example to glean the principles of a low-pass with a recursive algorithm. The
algorithm may be formulated in a general manner as follows:
yk =
L

l=−L
alV l fk −
M

m=−M
m̸=0
bmV m yk
(5.20)
with the deﬁnition: V l fk = fk+l (as above). We arbitrarily chose the sign in front
of the second sum to be negative; and for the same reason, we excluded m = 0 from
the sum. Both moves will prove to be very useful shortly.
For negative m the previous output is fed back to the right hand side of (5.20),
for the calculation of the new output: the algorithm is causal. For positive m the
subsequent output is fed back for the calculation of the new output: the algorithm is
acausal. Possible work-around: input and output are pushed into memory (register)
and kept in intermediate storage as long as M is big.
We may transform (5.20) into:
M

m=−M
bmV m yk =
L

l=−L
alV l fk.
(5.21)
The Fourier transform of (5.21) may be rewritten, like in (5.6) (with b0 = 1):
B jY j = A j Fj
(5.22)
with B j =
M

m=−M
bmW +mj
N
and
A j =
L

l=−L
alW +lj
N .
So the output is Y j = A j
B j Fj, and we may deﬁne the new transfer function as:
Hj = A j
B j
or
H(ω) = A(ω)
B(ω).
(5.23)
Using feedback we may, via zeros in the denominator, create poles in H(ω), or
better, using somewhat less feedback, create resonance enhancement.

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
145
Example 5.1 (Feedback) Let’s take our low-pass from (5.12) with 50% feedback of
the previous output:
yk = 1
2 yk−1 + 1
4 ( fk−1 + 2 fk + fk+1) or

1 −1
2 V −1

yk = 1
4

V −1 + 2 + V +1
fk.
(5.24)
This results in:
H(ω) = cos2(ωt/2)
1 −1
2e−iωt
.
(5.25)
Ifwedon’tcareaboutthephaseshift,causedbythefeedback,we’reonlyinterested
in:
|H(ω)| =
cos2(ωt/2)

1 −1
2 cos ωt
2
+

1
2 sin ωt
2 =
cos2(ωt/2)

5
4 −cos ωt
.
(5.26)
The resonance enhancement at ω = 0 is 2, |H(ω)| is shown in Fig.5.8, together
with the non-recursive low-pass from (5.12). We can clearly see that the edge steep-
ness got better. If we’d fed back 100% instead of 50% in (5.24), a single short input
would have been enough to keep the output “high” for good; the ﬁlter would have
been unstable. In our case, it decays like a geometric series once the input has been
taken off.
Here we’ve already taken the ﬁrst step into the highly interesting ﬁeld of ﬁlters
in the time domain. If you want to know more about it, have a look at, for example,
“Numerical Recipes” [7] and the material quoted there. But don’t forget that ﬁlters in
the frequency domain are much easier to handle because of the Convolution Theorem.
We shall discuss an interesting ﬁlter in more detail in Chap.6.
0
1
2
ΩNyq
|H(ω)|
ω
cos2(ωΔt/2)
√
5/4−cos ωΔt
cos2(ωΔt/2)
Fig. 5.8 Transfer function for the low-pass (5.12) and the ﬁlter with feedback (5.26)

146
5
Filter Effect in Digital Data Processing
5.3 Shifting Data
Let’s assume you have a data set and you want to shift it a fraction d of the sampling
interval t, say, for simplicity’s sake, using linear interpolation. So you’d rather
have started sampling d later, yet won’t (or can’t) repeat the measurements. Then
you should use the following algorithm:
yk = (1 −d) fk + d fk+1, 0 < d < 1
“shifting with
linear interpolation”.
(5.27)
The corresponding transfer function reads:
H(ω) = (1 −d) + deiωt.
(5.28)
Let’s not worry about a phase shift here; so we look at |H(ω)|2:
|H(ω)|2 = H(ω)H∗(ω)
= (1 −d + d cos ωt + i d sin ωt)(1 −d + d cos ωt −i d sin ωt)
= (1 −d + d cos ωt)2 + d2 sin2 ωt
= 1 −2d + d2 + d2 cos2 ωt + 2(1 −d)d cos ωt + d2 sin2 ωt
= 1 −2d + 2d2 + 2(1 −d)d cos ωt
= 1 + 2d(d −1) −2d(d −1) cos ωt
= 1 + 2d(d −1)(1 −cos ωt)
= 1 + 4d(d −1) sin2 ωt
2
= 1 −4d(1 −d) sin2 ωt
2 .
(5.29)
The function |H(ω)|2 is shown in Fig.5.9 for d = 0, d = 1/4 and d = 1/2.
This means: apart from the (not unexpected) phase shift, we have a low-pass effect
due to the interpolation, similar to what happened in (5.12), which is strongest for
d = 1/2. If we know that our sampled function f (t) is bandwidth-limited, we may
ΩNyq
|H(ω)|2
ω
δ = 0
δ = 1/4
δ = 1/2
Fig. 5.9 Modulus squared of the transfer function for the shifting-/interpolation-algorithm (5.27)

5.3 Shifting Data
147
use the sampling theorem and perform the “correct” interpolation, without getting
a low-pass effect. Reconstructing f (t) from samples fk, however, requires quite an
effort and often is not necessary. Interpolation algorithms requiring much effort are
either not necessary (in case the relevant spectral components are markedly below
ΩNyq), or they easily result in high-frequency artefacts. So be careful! Boundary
effects have to be treated separately.
5.4 Data Compression
Often we get the problem where data sampling had been too ﬁne, so data have to be
compressed. An obvious algorithm would be, for example:
y j ≡y2k = 1
2( fk + fk+1), j = 0, . . . , N
2
“compression”.
(5.30)
Here data set {yk} is only half as long as data set { fk}. We pretend to have extended
the sampling width t by the factor 2 and expect the average of the old samples at
the sampling point. This inevitably will lead to a phase shift:
H(ω) = 1
2 + 1
2eit.
(5.31)
If we don’t want that, we better use the smoothing-algorithm (5.12), where only
every other output is stored:
y j ≡y2k = 1
4( fk−1 + 2 fk + fk+1), j = 0, . . . , N
2
“compression”.
(5.32)
Here, there is no phase shift, the principle is shown in Fig.5.10.
Boundary effects have to be treated separately.
So we might assume, for example, f−1 = f0 for the calculation of y0. This also
applies to the end of the data set.
Fig. 5.10 Data compression algorithm of (5.32)

148
5
Filter Effect in Digital Data Processing
5.5 Differentiation of Discrete Data
We may deﬁne the derivative of a sampled function as:
d f
dt ≡yk = fk+1 −fk
t
“ﬁrst forward difference”.
(5.33)
The corresponding transfer function reads:
H(ω) =
1
t

eiωt −1

=
1
t eiωt/2 
eiωt/2 −e−iωt/2
= 2i
t sin ωt
2 eiωt/2
= iωeiωt/2 sin ωt
2
ωt/2 .
(5.34)
The exact result would be H(ω) = iω (cf. (2.56)), the second and the third factor
are due to the discretisation. The phase shift in (5.34) is a nuisance.
The “ﬁrst backward difference”:
yk = fk −fk−1
t
.
(5.35)
has got the same problem. The “ﬁrst central difference”:
yk = fk+1 −fk−1
2t
(5.36)
solves the problem with the phase shift. Here the following applies:
H(ω) =
1
2t

e+iωt −e−iωt
= iω sin ωt
ωt
.
(5.37)
Here, however, the ﬁlter effect is more pronounced, as is shown in Fig.5.11.
For high frequencies the derivative becomes more and more wrong.
Fix: Sample as ﬁne as possible, so that within your frequency realm ω ≪ΩNyq
is always true.
Fig. 5.11 Transfer function
of the “ﬁrst central
difference” (5.36) and the
exact value (thin line)
exact
1st central diﬀerence
−iH(ω)
ω
ΩNyq

5.5 Differentiation of Discrete Data
149
ΩNyq
−H(ω)
ω
exact
2nd central diﬀerence
Fig. 5.12 Transfer function of the “second central difference” (5.39) and exact value (thin line)
The “second central difference” is as follows:
yk = fk−2 −2 fk + fk+2
4t2
.
(5.38)
It corresponds to the second derivative. The corresponding transfer function is as
follows:
H(ω) =
1
4t2

e−iω2t −2 + e+iω2t
=
1
4t2 (2 cos 2ωt −2) = −1
t2 sin2 ωt
(5.39)
= −ω2

sin ωt
ωt
2
.
This should be compared to the exact expression H(ω) = (iω)2 = −ω2.
Figure5.12 shows −H(ω) for both cases.
5.6 Integration of Discrete Data
The simplest way to “integrate” data is to sum them up. It’s a bit more precise if we
interpolate between the data points. Let’s use the Trapezoidal Rule as an example:
assume the sum up to the index k to be yk, in the next step we add the following
trapezoidal area (cf. Fig.5.13):
yk+1 = yk + t
2 ( fk+1 + fk)
“Trapezoidal Rule”.
(5.40)
The algorithm is:

V 1 −1

yk = (t/2)

V 1 + 1

fk, V l is the shifting operator
of (5.4).

150
5
Filter Effect in Digital Data Processing
Fig. 5.13 Concerning the
trapezoidal rule
So the corresponding transfer function is:
H(ω) = t
2

eiωt + 1


eiωt −1

= t
2
eiωt/2 
e+iωt/2 + e−iωt/2
eiωt/2 
e+iωt/2 −e−iωt/2
(5.41)
= t
2
2 cos(ωt/2)
2i sin(ωt/2) = 1
iω
ωt
2
cot ωt
2 .
The “exact” transfer function is:
H(ω) = 1
iω
see also (2.63).
(5.42)
Heaviside’s step function has the Fourier transform 1/iω, we get that when
integrating over the impulse (δ-function) as input. The factor (ωt/2) cot(ωt/2)
is due to the discretization. H(ω) is shown in Fig.5.14.
The Trapezoidal Rule is a very useful integration algorithm.
Another integration algorithm is Simpson’s 1/3-rule, which can be derived as
follows.
Given are three subsequent numbers f0, f1, f2 and we want to put a second order
polynomial through these points:
y = a + bx + cx2
with y(x = 0) = f0 = a,
y(x = 1) = f1 = a + b + c,
y(x = 2) = f2 = a + 2b + 4c .
(5.43)
exact
trapezoidal rule
iH(ω)
ω
ΩNyq
Fig. 5.14 Transfer function for the trapezoidal rule (5.40) and exact value (thin line)

5.6 Integration of Discrete Data
151
The resulting coefﬁcients are:
a = f0,
c = f0/2 + f2/2 −f1,
b = f1 −f0 −c = f1 −f0 −f0/2 −f2/2 + f1
= 2 f1 −3 f0/2 −f2/2.
(5.44)
The integration of this polynomial of 0 ≤x ≤2 results in:
I = 2a + 4b
2 + 8c
3
= 2 f0 + 4 f1 −3 f0 −f2 + 4
3 f0 + 4
3 f2 −8
3 f1
= 1
3 f0 + 4
3 f1 + 1
3 f2 = 1
3 ( f0 + 4 f1 + f2) .
(5.45)
This is called Simpson’s 1/3-rule. As we’ve gathered up 2t, we need the step-
width 2t. So the algorithm is:
yk+2 = yk + t
3 ( fk+2 + 4 fk+1 + fk)
“Simpson’s 1/3-rule”.
(5.46)
This corresponds to an interpolation with a second order polynomial. The transfer
function is:
H(ω) = 1
iω
ωt
3
2 + cos ωt
sin ωt
and is shown in Fig.5.15.
At high frequencies Simpson’s 1/3-rule gives grossly wrong results. Of course,
Simpson’s 1/3-rule is more exact than the Trapezoidal Rule, given medium frequen-
cies, or the effort of interpolation with a second order polynomial would be hardly
worth it.
Simpson’s 1/3-rule
exact
iH(ω)
ω
ΩNyq
Fig. 5.15 Transfer function for Simpson’s 1/3-rule compared to the Trapezoidal Rule and the exact
value (thin line)

152
5
Filter Effect in Digital Data Processing
At ω = ΩNyq/2 we have, relative to H(ω) = 1/iω:
Trapezoid:
ΩNyqt
4
cot ΩNyqt
4
= π
4 cot π
4 = π
4 = 0.785 (too small),
Simpson’s-1/3:
ΩNyqt
6
2 + cos(ΩNyqt/2)
sin(ΩNyqt/2)
= π
6
2 + 0
1
= π
3 = 1.047 (too big).
Simpson’s 1/3-rule also does better for low frequencies than the Trapezoidal Rule:
Trapezoid:
ωt
2

1
ωt/2 −ωt/2
3
+ . . .

≈1 −ω2t2
12
,
Simpson’s-1/3:
ωt
3

2 + 1 −1
2ω2t2 + ω4t4
24
· · ·

ωt

1 −ω2t2
6
+ ω4t4
120
· · ·

=
1 −ω2t2
6
+ ω4t4
72 · · ·
1 −ω2t2
6
+ ω4t4
120 · · ·
≈1 + ω4t4
180
· · · .
The examples in Sects.5.2–5.6 would point us in the following direction, as far
as digital data processing is concerned:
The rule of thumb therefore is:
Do sample as ﬁne as possible!
Keep away from ΩNyq!
Do also try out other algorithms, and have lots of fun!

Playground
153
Playground
5.1 Totally Different
Given is the function f (t) = cos(πt/2) which is sampled at times tk = kt,
k = 0, 1, . . . , 5 with t = 1/3.
Calculate the ﬁrst central difference and compare it with the “exact” result for
f ′(t). Plot your results! What is the percentage error?
5.2 Simpson’s-1/3 versus Trapezoid
Given is the function f (t) = cos πt which is sampled at times tk = kt, k =
0, 1, . . . , 4 with t = 1/3.
Calculate the integral using the Simpson’s 1/3-rule and the Trapezoidal Rule and
compare your results with the exact value.
5.3 Totally Noisy
Given is a cosine input series that’s practically smothered by noise (Fig.5.16).
fi = cos π j
4 + 5(RND −0.5),
j = 0, 1, . . . , N.
(5.47)
In our example, the noise has a 2.5-times higher amplitude than the cosine sig-
nal. (The signal-to-noise ratio (power!) therefore is 0.5 : 5/12 = 1.2, see play-
ground 4.6.)
In the time spectrum (Fig.5.16) we can’t even guess the existence of the cosine
component.
Fig. 5.16 Cosine signal in totally noisy background according to (5.47)
0.4
0
f(k) 
k
Fig. 5.17 Discrete line on slowly falling background

154
5
Filter Effect in Digital Data Processing
a. What Fourier transform do you expect for series (5.47)?
b. What can you do to make the cosine component visible in the time spectrum,
too?
5.4 Inclined Slope
Given is a discrete line as input, that’s sitting on a slowly falling ground (Fig.5.17).
a. What’s the most elegant way of getting rid of the background?
b. How do you get rid of the “undershoot”?

Chapter 6
Data Streams and Fractional Delays
Abstract In this chapter the concept of data streams produced by fast digitizers
is introduced. Two simple ﬁlters to accomplish fractional delays are presented: the
Lagrange interpolator, a non-recursive ﬁlter, and the Thiran all-pass ﬁlter, a recursive
ﬁlter, both in lowest order. The group delay of these ﬁlters is discussed in detail and
illustratedwithexamples.FortheThiranall-passtheimpulse,step,andrampresponse
are shown.
This and the following chapter are for those pedestrians who want to go a little further,
like the German writer Johann Gottfried Seume who walked from Grimma, a little
town near Leipzig, Free State of Saxony, Germany, to Syrakus in Sicily, Siracusa in
Italian (“Spaziergang nach Syrakus”).
Thus far we have considered a static set of data on which we carry out actions like
high- and low-pass ﬁltering or alike. Modern digitizers analyze an analogue signal at
a certain pace, e.g. every nanosecond, and provide a stream of digital output. Light
travels about 30 cm in a nanosecond, the typical length of a ruler. Quite a handy
distance, indeed! Such digitizers are in use in a variety of ﬁelds, e.g. in acoustics
or microwave antenna arrays. Since there are different propagation delays involved
in cables for loudspeakers or antennas, the need for delays is immediately apparent.
Integer delays pose no problem: a simple shift register, a so-called FIFO (ﬁrst in,
ﬁrst out) would provide the solution. However, what about fractional delays?
6.1 Fractional Delays
We could simply interpolate the data like we did in Sect.5.3 called “shifting data”.
This is indeed a possibility and we should examine it in more detail. The linear
interpolation is merely the simplest version of the so-called Lagrange-interpolation
schemes which use polynomials for interpolation. An example for a quadratic inter-
polation (N = 2) is Simpson’s 1/3-rule.
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_6
155

156
6
Data Streams and Fractional Delays
6.2 Non-recursive Algorithms
Non-recursivealgorithmsinterpolatepastandpresentinputdata f−N, f−N+1, . . . , f0
by polynomials of degree N. The simplest non-recursive algorithm for a fractional
delay is the Lagrange interpolator for N = 1, i.e. a linear interpolation. For this case
we noticed that the amplitude of an input signal is reduced depending on the delay
and the frequency (see Fig.5.9). Thus, e.g. the Nyquist frequency is not transmitted
at all for d = 0.5. Since we are dealing with data streams we call the pace t the
“tag” and interpret d as a delay. Moreover, we did not pay attention to the associated
phase shift.
Here, we shall ﬁrst consider the phase shift, but in a rather different way. Instead
of speaking of the phase φ we shall use the quantity “group delay” deﬁned as:
τgroup = −dφ
dω .
(6.1)
This is the analogue of the inverse group velocity—e.g. in light guides—per unit
length. It measures the time delay of the amplitude envelope of a signal consisting of
various sinusoidal components produced by a linear algorithm with time invariance.
Let us have a look at the Lagrange interpolator with N = 1. The difference
equation is written as:
yk = d fk−1 + (1 −d) fk.
(6.2)
Note that we have slightly changed the nomenclature compared to (5.27) because
we now have to use terms like “past” and “present” and we do not want to use
“future” in a data stream (although this would be possible, of course, by using a
certain overall integer delay to convert “future” into “past”). The range of d is simply
0 ≤d ≤1.Themagnitudeofthetransferfunction H(ω)isnotaffectedbythischange
in nomenclature.
Let us see what we need:
First, we write the transfer function corresponding to (5.28) in the following way:
H(z) = dz−1 + (1 −d)
(6.3)
with z = eiωt, a convenient abbreviation for the moment. We shall come back
to this point later. This equation just means that you have to take the past quantity
multiplied by d and the present quantity multiplied by (1−d). In this sense, z−1 has
the same function as our shift operator V −1 deﬁned in (5.20). The real and imaginary
parts of H(z) are:
Re{H(z)} = d cos ωt + (1 −d)
Im{H(z)} = −d sin ωt.
(6.4)

6.2 Non-recursive Algorithms
157
Hence, the phase is:
φ = arctan
−d sin ωt
d cos ωt + (1 −d).
(6.5)
Differentiating the arctan-function with respect to ω is a little tedious. With the
help of:
d arctan α
dα
=
1
1 + α2
(6.6)
and a few simpliﬁcations we arrive at:
τgroup = dt

1 −2(1 −d) sin2 ωt
2


1 −4d(1 −d) sin2 ωt
2
 .
(6.7)
This function is plotted in Fig.6.1 versus ω for various values of d with t = 1.
The limiting values for ω →0 are easily seen:
τgroup →d
(in units of t).
(6.8)
For ω →ΩNyq we get:
τgroup →
d
2d −1
(in units of t).
(6.9)
For d = 1/2 the group delay seems to diverge at ΩNyq. Had we ﬁrst inserted d = 1/2
into (6.7), the frequency dependent terms in the numerator and denominator would
cancel and we would get τgroup = 1/2 in the entire frequency range. Thus, the result
depends on the order of taking the limiting values. What a nuisance! It does not
really help that under these conditions H(ΩNyq) = 0, i.e. nothing is passed at all.
The problem is still severe in the vicinity of ΩNyq for d close to 1/2. What is even
more intriguing is that τgroup becomes negative for d < 1/2 at a sufﬁciently high
frequency (always above ΩNyq/2). What is the meaning of a “negative delay”? Don’t
Fig. 6.1 The group
delay τgroup in units of t for
the N = 1 Lagrange
interpolator versus ω for
delays d in the range from
0 to 1
−3
−2
−1
0
1
2
3
ω
τgroup
ΩNyq

158
6
Data Streams and Fractional Delays
input
j
Fj
1
5
−2
5
3
5
−4
5
1
−4
5
3
5
−2
5
1
5
output for
d = 0.4
j
Fj
3
25
−4
25
5
25
−6
25
7
25
−2
25
1
25
0
−1
25
2
25
d = 0.5
j
Fj
1
10
−1
10
1
10
−1
10
1
10
1
10
−1
10
1
10
−1
10
1
10
d = 1.0
j
Fj
1
5
−2
5
3
5
−4
5
1
−4
5
3
5
−2
5
1
5
Fig. 6.2 Input signal with ω = ΩNyq and triangular envelope (top); output of the Lagrange inter-
polator with N = 1 for various delays d (subsequent to top)
panic! For the envelope of a signal which varies slowly compared to the frequency
components there is no problem in “advancing” the maximum a few tags like it
is for delaying. In order to see what cruelties the seemingly “innocent” Lagrange
interpolater with N = 1 can commit, let’s do a “drastic” example.
Example 6.1 (“Nyquist pulse”). We use as input a signal with ω = ΩNyq and a
triangular envelope (see Fig.6.2(top)). The signal contains spectral components like
those shown in Fig.3.2, now centered at ΩNyq. The upper half is simply the mirrored
left half like illustrated in Fig.4.5. This choice makes sure that we cover the range
of frequencies where τgroup is strongly frequency dependent, except for d = 0.5.
Figure6.2 illustrates the output for the delays d = 0.4, 0.5, and 1. For d = 0.5 we
immediately see that we get a delay of 0.5, the pulse envelope is now rectangular and
the amplitude is heavily attenuated. Where is the maximum of the envelope? In the
middle? Since τgroup = 0.5, independent of ω, the rectangular envelope is entirely
the result of the frequency dependent attenuation. For d = 0.4 the signal is heavily
distorted and the envelope is monotonous on one side only. It looks like the envelope

6.2 Non-recursive Algorithms
159
is shifted backwards. For d = 0.6 we get the mirror image of the case for d = 0.4.
For d = 1 we recover the input shifted by 1.
We see that not only the magnitude of the transfer function H(ω) depends on d
and ω but also the group delay. A rather unsatisfactory situation.
Thankfully, there is help regarding the magnitude of the transfer function using a
recursive algorithm, as we shall see in Sect.6.4.
In order to see that the Lagrange interpolator for N = 1 is not dull at all, let’s do
two examples.
Example 6.2 (Average) Its worth looking at a subtle detail: whats the average of the
function with the Nyquist frequency and a triangular envelope (see Fig.6.2(top))?
Have a guess! Looks like zero. Not bad, but not good enough! It is zero for even M
only where M is the number of data points below 0 and also above 0, i.e. there are
2M data points. What is it for M = odd ? Lets do the calculation by “brute force”:
Average =
1
2M

1 + 2
M−1

k=1
(−1)k M −k
M

=
1
2M

1 + 2
M−1

k=1
(−1)k −2
M
M−1

k=1
(−1)kk

=
1
2M

1 +
 −2
0

−2
M
	 M−2
2
−(M −1)
M−1
2


for even M
for odd M
=
1
2M

1 +
 −2
0

−2
M
	
−M
2
M−1
2


for even M
for odd M
=
1
2M

1 +
 −2
0

+
1
1
M −1
 for even M
for odd M
=
0
for even M
1
2M2
for odd M .
It has todowiththequestionwhether theﬁrst/last non-vanishingdatapoint is negative
or positive. Of course, for large M the difference fades away. A look to Fourier space
willshowthatthisresultcomesinquitenaturally.Tosimplifymatters,weuseperiodic
continuation, i.e. no zero-padding. Lets start with the calculation of the discrete
Fourier transform of the discrete triangular function. I’m sure you have noticed that in
Playground4.9 we have already solved a similar problem. The triangular function—
or double-sided ramp—was shifted and we required even N. With a slight change
in nomenclature we can relax this requirement. Should you happen to have solved
Playground4.9 take the following as a rehearsal. Let’s start from scratch.

160
6
Data Streams and Fractional Delays
fk =
⎧
⎪⎨
⎪⎩
M + k
M
for −M ≤k ≤0
M −k
M
for 1 ≤k ≤M −1
F0 =
1
2M

1 + 2
M−1

k=1

1 −k
M

=
1
2M

1 + 2(M −1) −2
M
M−1

k=1
k

=
1
2M

1 + 2M −2 −2
M
M(M −1)
2

=
1
2M (1 + 2M −2 −M + 1) = 1
2,
as we would have guessed right away.
The second sum is due to the young mathematician Carl Friedrich Gauß who
apparently did not want to sum up all numbers from 1 to 100 step by step, a task given
by the teacher to keep the pupils busy. Carl Friedrich noticed that by complementing
each number to add up to the maximum number he actually adds the same series but
in reversed order. That’s where the factor of two in the denominator comes from.
For j > 0 we have:
Fj =
1
2M

−1

k=−M
fk exp −iπ jk
M
+ 1 +
M−1

k=1
fk exp −iπ jk
M

=
1
2M

1 + 2
M−1

k=1

1 −k
M

cos π jk
M

(term with k = −M vanishes because f−M = 0)
=
1
2M

1 + 2
M−1

k=1
cos π jk
M −2
M
M−1

k=1
k cos π jk
M

.
The ﬁrst sum is easily evaluated using the Dirichlet kernel (1.53) with the upper
summation index being M −1. We get:
2
M−1

k=1
cos π jk
M
= 2

sin

M −1
2

x
2 sin x
2
−1
2

= −cos Mx −1 with x = π j
M .
For the second sum we start with the expression derived in Playground4.9:
M−1

k=1
sin kx = cos x
2 −cos

M −1
2

x
2 sin x
2
= 1
2

(1 −cos Mx) cot x
2 −sin Mx


6.2 Non-recursive Algorithms
161
This we differentiate with respect to x to get:
M−1

k=1
k cos kx = 1
2

−(1 −cos Mx) 1
2
sin2 x
2
+ M sin Mx cot x
2 −M cos Mx

= −1
2
M cos π j + sin2 π j
2
sin2 π j
2M
with x = π j
M .
Collecting all terms we ﬁnally arrive at:
Fj =
1
2M2
sin2 π j
2
sin2 π j
2M
.
This is the discrete analogue of the continuous result of (3.4). We can simplify this
expression further to:
F0 = 1
2
(the average;
we can use even the general formula,
let j be continuous for the moment,
and use l’Hospital’s rule)
Fj =
⎧
⎪⎨
⎪⎩
1
2M2
1
sin2 π j
2M
for odd j
0
for even j
.
The continuous Fourier transform of the triangular function had zeros so does the
discrete one. What is left over is to shift the Fourier transform of the discrete triangular
function to the Nyquist frequency because a multiplication of the function with the
Nyquist frequency by the triangular function corresponds to the convolution of the
Fourier-transformed triangular function with a discrete δ-function at the Nyquist
frequency. For the moment we are only interested in the value at zero frequency
because this gives the average of our function where we started. This means we need
the value at the end of the tail, i.e. j = −M for the unshifted Fourier-transformed
triangular function. We get:
F0 =
⎧
⎪⎨
⎪⎩
1
2M2
sin2 πM
2
sin2 π
2
=
1
2M2
for odd M
0
for even M
.
Now it is clear that the average of our input function is non-zero for odd M only.

162
6
Data Streams and Fractional Delays
Example 6.3 (Frequency comb) Now lets have a closer look at the case d = 1/2 in
Fourier space. The function looks like the Nyquist frequency, but there is a ﬂaw in
the middle. In fact, this little glitch is causing a whole comb of non-zero frequency
components in Fourier space, as we’ll see below. First we need the discrete transfer
function for the Lagrange interpolator with N = 1. Since we are not interested in
the phase shift, we need the real part only: Re{Hj} = cos2(π j/2M). For the sake of
convenience, we’ll drop the Re{} in the following. This we multiply by the Fourier-
transformed triangular function centred at the Nyquist frequency. The shifted Fourier
transform of the triangular function reads:
F0 =
⎧
⎪⎨
⎪⎩
1
2M2
for odd M
0
for even M
FM = 1
2
Fj =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
2M2
1
sin2 π(M−j)
2M
for odd M and even j = 2, 4, . . . , M −1
or
for even M and odd j = 1, 3, . . . , M −1
0
else
FM+ j = FM−j
for j = 1, 2, . . . , M −1.
The trigonometric expression can be further simpliﬁed:
sin2 π(M −j)
2M
=
⎛
⎝
=1
sin π
2 cos π j
2M −
=0
cos π
2 sin π j
2M
⎞
⎠
2
= cos2 π j
2M .
No surprise, in the continuous world, sin2 x shifted by π/2 yields cos2 x! However,
there are a few subtleties in the digital world as you can see from the little extras F0
and FM. Incidentally (or not?), this is identical to the transfer function and thus the
trigonometric expressions cancel. We ﬁnally get:
HM FM = cos2 πM
2M
1
2 = 0 (no transmission at the Nyquist frequency)
Hj Fj =
⎧
⎨
⎩
1
2M2
for odd M + j
0
else
HM+ j FM+ j = HM−j FM−j
for j = 0, . . . , M −1.
This expression means that every second Fourier coefﬁcient vanishes, in particular
the odd ones for odd M and the even ones for even M (see Fig.6.3 for M = 3 and
M = 4). And, amazingly, all non-zero coefﬁcients have the same value
1
2M2 . Thats

6.2 Non-recursive Algorithms
163
M = 3
j
Fj
0
1
2
3
4
5
6
1
18
M = 4
j
Fj
0
1
2
3
4
5
6
7
1
32
8
Fig. 6.3 Real part of Fourier transform of the input with the Nyquist frequency and triangular
envelope delayed by d = 1/2 using the Lagrange interpolator with N = 1 for M = 3 (top) and
M = 4 (bottom)
why we called this a “comb” of frequency coefﬁcients. Its the glitch in the input data
which caused it!
Just to complete the example, lets calculate directly the Fourier transform of the
output of the function with the Nyquist frequency and the triangular envelope shifted
by d = 1/2 using the Lagrange interpolator with N = 1. In order to simplify matters,
we decompose the input into a constant with value
1
2M and a function with value
−1
M at those points where the input was negative. The constant is of no interest,
it contributes to F0 only and we know this value already (=
1
2M2 for odd M). The
non-zero points of the second function appear at all odd negative values of k whereas
they appear at all even k for positive ones. Since we are interested only in the real
part of the Fourier transform we can ﬂip around the negative k’s to positive k’s and
see that we end up with a sum we know already: the Dirichlet kernel. There is a little
pitfall: for even M we have to include the term with k = −M whereas we must not
for odd M. Lets discuss even and odd M separately. For even M we get:
Fj =
1
2M

−1
M
 
cos π j + sin

M −1
2
 π
M
2 sin π j
2M
−1
2

= −
1
2M2

cos π j −1
2 cos π j −1
2

=
1
2M2 sin2 π j
2
=
⎧
⎪⎨
⎪⎩
1
2M2
for odd j
0
else
.

164
6
Data Streams and Fractional Delays
For odd M we get:
Fj =
1
2M

−1
M
 
sin

M −1
2
 π
M
2 sin π j
2M
−1
2

= −
1
2M2
1
2 cos π j −1
2

=
1
2M2 cos2 π j
2
=
⎧
⎪⎨
⎪⎩
1
2M2
for even j
0
else
.
This can be combined to:
Fj =
⎧
⎪⎨
⎪⎩
1
2M2
for odd M + j
0
else
FM+ j = FM−j
for j = 0, . . . , M −1.
This is, of course, the same result as above.
6.3 Stability of Recursive Algorithms
In the Example 5.1 we have already encountered a recursive algorithm, also called
a “ﬁlter”. We can write the transfer function as a polynomial of the above deﬁned
quantity z−1 in the numerator and another polynomial in z−1 in the denominator,
both with real-valued coefﬁcients. We shall exclude positive powers of z because
we want to use present, i.e. z0, and past data only. For the discussion of stability
of the ﬁlter we now allow z to be an arbitrary complex quantity rather than to be a
phase only. Then we get N zeros of a polynomial of degree N in the denominator,
hence the transfer function diverges. This is what is usually called a “pole”. For
N = 1 our denominator would read 1 + a1z−1 with a single pole for z−1 = −1/a1
or z = −a1. For higher N we would eventually get poles for complex values of z−1.
Let us examine what is the response of the transfer function:
H(z) =
1
1 −z−1
(6.10)
to an impulse:
fk = δk,0.
(6.11)

6.3 Stability of Recursive Algorithms
165
The corresponding difference equation reads:
yk = yk−1.
(6.12)
The output will be the Heaviside step function: once switched to unity it remains
there forever. Sometimes (6.10) is called the “z-transform” of the Heaviside function.
It is also called “accumulator” because it adds up all input data for k ≥0.
If instead of (6.10) we would use:
H(z) =
1
1 −a1z−1
with |a1| < 1
(6.13)
we would have
yk = a1yk−1
(6.14)
and the response to the impulse would decay as a geometrical series with increasing
powers of a1. For small a1, the output would decay rapidly, but actually persists
indeﬁnitely. This is why such ﬁlters are called “Inﬁnite Impulse Response” (IIR)
ﬁlters. The Lagrange interpolator, on the contrary, is a “Finite Impulse Response”
(FIR) ﬁlter.
What has to be discussed now is the stability of the ﬁlter. We have seen that (6.10)
is at the limit of stability while (6.13) is stable provided |a1| < 1. It can be shown that
IIR ﬁlters are stable provided the poles are all inside the unit circle in the complex
plane. For the stability, the zeros of the numerator polynomial play no role.
6.4 Thiran’s All-Pass Filter for N = 1
There is an algorithm which avoids the attenuation altogether, the so-called “Thiran
all-pass ﬁlter”, i.e. we have |H(z)| = 1 always, and, moreover, the group delay is
“maximally ﬂat”, i.e. it depends only weakly on ω for small ω. More precisely, all
up to the Nth derivatives at ω = 0 are 0 for a polynomial of order N. Thiran [12]
originally proposed a so-called “all-pole”-ﬁlter with a polynomial in z−1 in the
denominator and a constant numerator. Fettweis [13] showed that it is advantageous
to include a properly chosen polynomial in the numerator. This is now called the
“Thiran all-pass ﬁlter” and is discussed in detail in [14].
You may ask, what the hell is ﬁltered if everything passes? The “ﬁlter experts”
call everything a “ﬁlter” which can be described by a linear difference equation, like
the one above. It does not necessarily mean a ﬁlter in the frequency domain. You
will get used to this terminology like you are used to click to the “start” button if you
want to shut down your computer. I would have preferred a “stop” button.

166
6
Data Streams and Fractional Delays
Of course, we are dealing with a recursive ﬁlter.
Here is how it is constructed. We start with the transfer function:
H(z) = z−N DN(z)
DN(z−1)
(6.15)
Here, DN(z) denotes a polynomial in z of order N with coefﬁcients ai and the
constant a0 = 1. It is easy to show that |H(z)| = 1.
The ﬁrst factor z−N is merely a phase with magnitude 1. The numerator can be
written as DN(z) = |DN(z)|eiφD, the denominator can be written as DN(z−1) =
|DN(z)|e−iφD because z−1 is its complex conjugate z∗. Thus the quotient of the two
polynomials is again merely a phase e2iφD. This completes the proof.
Here, we shall discuss the simplest case with N = 1 only. We have:
H(z) = z−1(1 + a1z)
1 + a1z−1
= z−1 + a1
1 + a1z−1 .
(6.16)
The corresponding difference equation reads:
yk = −a1yk−1 + fk−1 + a1 fk
(6.17)
The ﬁrst term comes from the denominator (a recursive algorithm!) whereas the other
two terms come from the numerator.
Thiran [12] and Välimäki [14] quote for the maximally ﬂat ﬁlter for N = 1 the
condition for the coefﬁcient a1:
a1 = −
d
d + 2.
(6.18)
The group delay can now be written as:
τgroup = t + 2dφD
dω .
(6.19)
The ﬁrst term comes from z−1 whereas the second term comes from the quotient
DN(z)/DN(z−1).
The real and imaginary parts of the phase φD are:
Re{φD} = 1 + a1 cos ωt
Im{φD} = −a1 sin ωt.
(6.20)
After a little tedious algebra we ﬁnally get:
dφD
dω = −ta1(a1 + cos ωt)
1 + a2
1 + 2a1 cos ωt .
(6.21)

6.4 Thiran’s All-Pass Filter for N = 1
167
Fig. 6.4 The group delay
τgroup in units of t for the
N = 1 Thiran all-pass versus
ω for delays d in the range
from −1/2 to +1/2
−3
−2
−1
0
1
2
3
ΩNyq
ω
τgroup
After inserting (6.18) and using (6.19) we get:
τgroup =
t(1 + d)
1 + d(d + 2) sin2 ωt
2
.
(6.22)
This function is plotted in Fig.6.4 versus ω for various values of d with  = 1.
For ω →0 we get τgroup = 1 + d (in units of t). No wonder, even for d = 0
we get 1 tag delay because we have used an output from the past!
Here, a word about the useful range of d is necessary. Välimäki [14] recommends
−1/2 ≤d ≤+1/2 (in units of t). Don’t worry about negative delays, we just go
back to the past by half a tag and τgroup is always positive.
For d = 0 we get τgroup = 1, for d = 1/2 we get τgroup = 3/2/(1 +
5/4 sin2(ωt/2)), and for d = −1/2 we get τgroup = 1/2(1−3/4 sin2(ωt/2)), all
in units of t. For ΩNyq we get τgroup = (1+d)/(1+d(d +2)), i.e. τgroup = 2/3 and
2 for d = 1/2 and d = −1/2, respectively. Note the asymmetry between positive
and negative delays. It also shows that negative delays are giving worse results than
positive ones. In any case, there is no divergence like in the Lagrange N = 1 case.
We now give a few illustrations of the ﬁlter action.
6.4.1 Impulse Response
Let us see what is the response of the N = 1 Thiran all-pass ﬁlter to an impulse.
From (6.17) we see that the output is:
y0 = a1
yk =

1 −a2
1

(−a1)k−1,
k = 1, 2, 3, . . . .
(6.23)
This means, that—apart from the transient y0—the output decays with increasing
powers of a1. For positive a1 the sign alternates whereas for negative a1 the output
is always positive.

168
6
Data Streams and Fractional Delays
Now we show that the sum over all output data is unity, as was the input:
∞

k=0
yk = a1 + (1 −a2
1)
∞

k=0
(−a1)k.
(6.24)
The sum is easily evaluated with [8,No.0.231] yielding ﬁnally:
∞

k=0
yk = a1 + 1 −a2
1
1 + a1
= a1 + (1 −a1)(1 + a1)
1 + a1
= 1.
(6.25)
6.4.2 Step Response
Let us see what is the response of the N = 1 Thiran all-pass ﬁlter to a step, the
Heaviside step function which is zero for negative times and unity at t = 0 and
afterwards. The ﬁrst example is for d = 1/2. Figure6.5 illustrates the step response.
To start with, there is an undershoot of magnitude a1 = −1/5, followed by
rational numbers with powers of |a1| in the denominator and approaching unity
monotonously.
The step response for d = −1/2 is a little surprising at ﬁrst glance. Figure6.6
illustrates what happens.
Fig. 6.5 The step response
of the Thiran all-pass for
d = 1/2
k
yk
−2
−1
0
−1
5
1
19
25
2
119
125
3
619
625
Fig. 6.6 The step response
of the Thiran all-pass for
d = −1/2
k
yk
−2
−1
0
1
3
1
11
9
2
25
27
3
83
81

6.4 Thiran’s All-Pass Filter for N = 1
169
To start with, there is a positive output with a1 = 1/3, followed by rational
numbers with powers of a1 in the denominator and approaching unity, but now in an
oscillatory manner.
This deserves a closer look.
Apart from the transient (the ﬁrst output in the present case) the following explicit
formula can be derived from the difference equation (6.17).
yk = 1 + (−1)k+1ak
1(1 −a1),
k = 1, 2, 3, . . . .
(6.26)
Now it is easy to see why for negative a1 we have a monotonous approach to unity:
the prefactor of the second term is always negative because we can write:
(−1)k+1ak
1 = (−1)2k+1|a1|
(6.27)
and the exponent is odd.
On the other hand, for positive a1 we retain the factor (−1)k+1 which leads to
oscillations. Note that (1 −a1) is always positive in the allowed range of d.
The unit step contains all possible frequencies and the step response is what the
group delay does to all those frequency components. The integral delay action is not
immediately evident.
6.4.3 Ramp Response
More illustrative is the response of the Thiran all-pass with N = 1 to a ramp
deﬁned as:
fk =
	 k
for k = 0, 1, 2, 3, . . .
0
for k < 0
.
(6.28)
We should not bother that the fk grow indeﬁnitely. We can always stop for a given
kmax and we are interested only in smaller k, i.e. early times such that the termination
plays no role. If we normalize the fk to the largest value, i.e. fmax = 1, we see that
we actually discuss the low frequency limit.
Figures6.7 and 6.8 show the ramp response for d = 1/2 and d = −1/2, respec-
tively.
It is clear that in both cases the 45◦slope is approached rapidly (monotonously
for d = 1/2 and oscillatory for d = −1/2). What is very pleasing is that you can
directly read off the low frequency limit of the group delay τgroup = 1 + d, i.e. 3/2
for d = 1/2 and 1/2 for d = −1/2.

170
6
Data Streams and Fractional Delays
k
yk
2
1
0
1
−1
5
2
14
25
3
189
125
4
1564
625
−
−
Fig. 6.7 The ramp response of the Thiran all-pass ﬁlter for d = 1/2; the left line corresponds to
the ramp input whereas the right line indicates the input delayed by 3/2
k
yk
−2
−1
0
1
1
3
2
14
9
3
67
27
4
284
81
Fig. 6.8 The ramp response of the Thiran all-pass ﬁlter for d = −1/2; the left line corresponds to
the ramp input whereas the right line indicates the input delayed by 1/2
Again, apart from the transient, an explicit formula for yk can be derived from
the difference equation (6.17). This time, the fk grow and the algebra is a little more
tedious. Apart from the transient we arrive at the following formula:
yk = ka1 + (1 −a2
1)
k−1

i=1
(−1)i−1ai−1
1
(k −i).
(6.29)
This can be rewritten as follows:
yk = ka1 + (1 −a2
1)(−a1)k−1
k−1

i=1
i
(−a1)i .
(6.30)

6.4 Thiran’s All-Pass Filter for N = 1
171
Amazingly enough, this sum has be evaluated in the pre-computer era yielding
[8, No.0.113]:
k−1

i=1
i
(−a1)i = −
(k −1)

−1
a1
k
1 −

−1
a1

+

−1
a1

⎛
⎜⎝
1 −

−1
a1
k−1

1 −

−1
a1
2
⎞
⎟⎠.
(6.31)
We now get from (6.31) after a lengthy simpliﬁcation:
yk = k −

1 −(−a1)k 1 −a1
1 + a1
.
(6.32)
Finally, we replace a1 by d and obtain:
yk = k −(1 + d)

1 −

d
d + 2
k
.
(6.33)
For large k we get approximately:
yk = k −(1 + d)
(6.34)
from which we see immediately that yk grows like k but with a delay of 1 + d. We
also see that the approach to the ramp is monotonous for positive d (always above
the ramp values shifted by the delay, apart from the transient) whereas is oscillates
for negative d (actually oscillating around the ramp values shifted by the delay).
It was worth while going through all the tedious algebra because we directly see
the low frequency limit of τgroup = 1 + d in the output yk.
Of course, in real applications, larger values for N are in use. The coefﬁcients ai
are different, but the principle is the same. Of course, using N “past” data means
that the inevitable delay will be N. If you happen to hear the same radio program
with two different receivers, say on AM and FM, and one transmission sounds like
“echoing” the other one, you can be sure that digital audio processing was used.
Playground
6.1 What’s Your Average?
Calculate the average of the function with the Nyquist frequency and the triangular
envelope as a function of the delay d of the Lagrange interpolator with N = 1.
6.2 Late Impulse
Calculate and plot the response of the N = 1 Thiran all-pass ﬁlter to an impulse for
d = 1/2 and d = −1/2.

172
6
Data Streams and Fractional Delays
6.3 The Devil Takes the Hindmost
a. Calculate and plot the response of the N = 1 Thiran all-pass ﬁlter to the trailing
edge of the unit pulse for d = 1/2.
b. Show that the sum of all output data of a unit pulse is conserved.
c. Explain why this must be the case.
6.4 Delayed Nyquist
Calculate and plot the response of the N = 1 Thiran all-pass for an input with
cos ΩNyq starting at t = 0 and being 0 at earlier times for d = 1/2. Compare the
result with the Lagrange N = 1 interpolator.

Chapter 7
Tomography: Backprojection of Filtered
Projections
Abstract In this chapter the backprojection of ﬁltered projections, a common algo-
rithm in tomography, is presented. A simple example, the uniform disc, is discussed
in detail in its continuous version in order to illustrate how the backprojection serves
both purposes: to reconstruct the object and to “erase the shadow” where there is no
object. It is shown how in a discrete version the backprojection of ﬁltered projec-
tions is directly obtained by a convolution of the inverse Fourier transformed ramp
function and the projections.
Tomography is a method to obtain 3D-images from objects without the need to cut
the object into sections (like the greek word “tomos” would suggest). It would not
be applicable for living objects anyway. For tomography one needs particles like X-
ray photons, neutrons, high energy protons (for small objects) or even electrons (for
extremely small objects) which traverse the object and which are detected behind
the object. The contrast can, e.g., be given by absorption, i.e. not all particles arrive
at the detector, or the energy loss of the particles. Here, we discuss only the parallel
beam geometry. It could be a collimated or focussed beam (pencil geometry) which
is scanned over the object or the object is moved in front of the beam, or one can use
a broad beam and a position sensitive detector.
In the following we shall speak of “screen” for the detector and the result on the
screen is called “projection”. In order to obtain all information for the reconstruction
of the 3D-image, the object has to be rotated in front of the beam, i.e. we need
as many different projections under different angles as possible. In the following
we shall discuss a single slice only, i.e. a 2D-object (with ﬁnite thickness, if you
want). The task is to reconstruct the 2D-image from the projections. The 3D-image
is then obtained by stacking the slices. Finally, slices or cross-sections in any desired
direction can be computed from the full 3D-image of the object.
7.1 Projection
The projection is deﬁned as the path integral through the object, as illustrated in
Fig.7.1. This path integral is known as the Radon transform in order to honour the
person who ﬁrst showed how the object can be recovered from projections. It takes a
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_7
173

174
7
Tomography: Backprojection of Filtered Projections
y
x
t
y
x
t
θ
Fig. 7.1 Illustration of the path through the object for the pencil geometry. Beam parallel to y-axis
(left); beam rotated by θ (right). The distance of the path from the origin is t = x cos θ + y sin θ
while to get used to the word “transform”. We assume for simplicity that the direction
of the path plays no role, i.e. it can be carried out from “front” to “back” or vice
versa. Hence, we require projections for an angular range of 0 ≤θ ≤180◦only. This
is true only if the object does not have large density inhomogeneities. A front-only
bullet-proof vest protects against a shot aiming at your breast but not when the shot
hits your back!
The deﬁnition of the projection Pθ(t) of the object ρ(x, y) from different angles
θ is:
Pθ(t) =
+∞

−∞
ρ(x, y) δ(x cos θ + y sin θ −t) dxdy
(7.1)
Here, the δ-“function” is unity if and only if its argument is 0. In other words,
the δ-“function” ensures that only points (x, y) contribute to the integral which
are on a line with a distance t to the origin. We could rotate the sample by −θ in
Fig.7.1(right) in order to leave the beam vertical. Fortunately, we are free to rotate
the sample clockwise or counter-clockwise, so we do not have to worry about the
sign of θ. You may look up [8, No. 3.02.2] and see that the sign of θ does not matter
in the integration over θ.
Stacking all Pθ(t) versus θ we get what is called a “sinogram” because a single
mass point rotating in front of the screen would give a sinusoidal projection, even-
tually with a phase. Since the object could be considered to consist of mass points
(maybe better voxels) the sinogram is a superposition of the sinusoidal projections of
all mass points. Thus the sinogram contains all necessary information for the image
reconstruction.

7.2 Backprojection of Filtered Projections
175
7.2 Backprojection of Filtered Projections
A standard method for the image reconstruction in tomography is the so-called
“Backprojection of ﬁltered projections”. Needless to say that we require a Fourier
transformation to illustrate the method. Here, we use a spatial coordinate x and an
“angular wave number” k instead of time t and angular frequency ω as we did thus
far. A simple periodic density, e.g., will be written like
ρ(x) = cos kx.
(7.2)
It is immediately clear that a simple direct backprojection of the projection would
not do the job. We would get density in the space between the object and the screen
which would be wrong. If there is no density between the object and the screen,
the projection would not depend on the distance between object and screen at all for
parallel beams. Hence, what matters is the change of the projection with respect to the
beam direction, i.e. its spatial derivative. This is also what a rigorous treatment of the
problem would tell us. In order to avoid the awesome view of a screen penetrating the
object it is helpful to consider a single mass point within the object. The derivative
of the projection would be 0 in front of the object and would suddenly switch to
high as the voxel is touched. We could do so for all voxels in the slice and—due to
linearity—ﬁnally superimpose all results. What we get are streaks within the object
space proportional to the path integral trough the object and aligned with the path.
The superposition of all streaky patterns for all angles θ ﬁnally leads to the desired
2D-density distribution.
Now, rather than to do the inverse Radon transform, the Fourier transformation
comes into play:
Sθ(k) =
+∞

−∞
Pθ(t)e−iktdt.
(7.3)
The Fourier transformation is carried out in 1D only because of the Fourier slice
theorem, sometimes also called the projection-slice theorem or central slice theorem.
It states that the 1D-Fourier transform of a 1D-projection of a 2D-object is the same
as extracting a central slice—better a path—through the origin along ky = 0 of the
2D-Fourier transform of the object. You are encouraged to proof it in the exercises.
It is annoying that the word “slice” in the Fourier slice theorem has a different
meaning than the slice we were speaking about when stacking slices of 2D-images
in order to get a 3D-reconstruction.
Now we remember that the Fourier transform of the derivative of a function is
simply the Fourier transform of the function multiplied by iω (see (2.4)), or ik in the
present case. Hence the Fourier transform of the derivative of the projection is:
Pθ(t) = ikSθ(k).
(7.4)

176
7
Tomography: Backprojection of Filtered Projections
Since the direction of the path in the path integral (7.1) plays no role, we can use
|ik| = |k| as the multiplier. This multiplier is usually called the “ramp ﬁlter”. It is a
linear high-pass. For very long wavelength it practically suppresses Sθ(k) whereas
for high frequencies it enhances Sθ(k).
Next, we carry out the inverse Fourier transform:
Qθ(t) =
1
(2π)2
+∞

−∞
Sθ(k)|k|e+iktdk.
(7.5)
I am sure you have noticed the prefactor
1
(2π)2 . Where does it come from? Let’s argue
with the consideration of units. For the sake of simplicity we have used no units for
ρ(x). Hence, the projection Pθ(t) has the unit “length” and Sθ(k) the unit “area”.
The inverse Fourier transform is carried out by the integral of (7.5) and the angular
integral (7.6). Hence, the integral in (7.5) accounts for two dimensions and thus we
require
1
(2π)2 as prefactor. At this point I am tempted to regret not to have used 2π
times the wave number instead of the angular wave number (like 2πν instead of
ω) because there would be no prefactor at all, not in one nor in two dimensions.
However, we would have missed the insight into the secrets of what mathematicians
call “cylinder coordinates”.
Finally, we have to integrate over all angles θ:
ρ(x) =
π
0
Qθ(x cos θ + y sin θ) dθ.
(7.6)
Here, we have used t
= x cos θ + y sin θ. The last two steps represent the
“backprojection”.
This procedure looks very complicated, and in reality, nobody would follow this
recipe, as we shall see at the end of this chapter. However, it is instructive to see, how
“density” is backprojected where we want it and how it is “erased” where we don’t.
Let us do a “simple” example:
Example 7.1 (“Spherical disk” of uniform density)
ρ(r) =
⎧
⎨
⎩
1 for r ≤R
0 else
.
(7.7)
Due to symmetry it sufﬁces to consider ρ(x) only. In this case all Pθ(t) are identical
and the integration over θ should be trivial. This is true for t ≤R, but not so for the
“outside”, i.e. t larger than R, as we shall see!

7.2 Backprojection of Filtered Projections
177
The projection reads:
Pθ(t) =
⎧
⎪⎨
⎪⎩
2R

1 −
	 t
R

2 inside
0
outside
.
(7.8)
The Fourier transform of it reads:
Sθ(k) = 2
R

0
2R

1 −
 t
R
2
cos kt dt = 2πR2 J1(kR)
kR
.
(7.9)
Here, we have used [8, No. 3.752.2]. J1(kR) is a Bessel function, no surprise for
cylindrical symmetry. Don’t worry about the denominator. The J1(kR) goes to 0 with
the same power of k thus the expression remains ﬁnite for k = 0, see Fig.7.2.
The factor of 2 in front of the integral comes from the fact that we integrate over
positive t only. This is sufﬁcient for an even integrand. Similarly, we do not have to
bother about the imaginary part of the Fourier transform, it simply vanishes.
We also see that Sθ(k) is an even function because the Bessel function J1(kR) is
odd as is the denominator.
−0.1
0
0.1
0.2
0.3
0.4
0.5
5
10
15
20
25
30
J1(kR)/kR
k
Fig. 7.2 J1(kR)/kR versus k for R = 1

178
7
Tomography: Backprojection of Filtered Projections
Then we calculate with [15,5413b]:
Qθ(t) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
1
π
for t < R
−1
π
⎛
⎝
1

1−

R
t
2 −1
⎞
⎠for t ≥R
.
(7.10)
We note that for t “inside R” Qθ(t) does not depend on t. Integrating Qθ(t) from 0
to π just gives ρ(x) = 1 or ρ(r) = 1, as it should be. Thus far for the “painting” part
of the backprojection.
Surprisingly, at ﬁrst glance, Qθ(t) for t “outside R” is negative and even diverges
as t approaches R. No wonder, this is our “rubber” which erases everything out-
side. How this is done requires the integration over Qθ(t) of an arc from θ = 0 to
arccos(R/x) with the “outside” integrand and from θ = arccos(R/x) to π/2 with
the “inside” integrand, as illustrated in Fig.7.3.
ρ(x) = 2
⎛
⎝
arccos(R/x)

0
−1
π
⎛
⎝
1

1−

R
t
2 −1
⎞
⎠dθ +
π/2

arccos(R/x)
1
π dθ
⎞
⎠
= 2
⎛
⎝1
π
π/2

0
dθ −1
π
arccos(R/x)

0
1

1−

R
x cos2 θ
2
⎞
⎠dθ
(7.11)
The factor 2 in front comes from the fact that it sufﬁces to integrate up to π/2 only
due to symmetry. In the last step we have used t = x cos θ because it sufﬁces to
calculate ρ along the x-axis only. We can later replace x by r, again by symmetry.
The ﬁrst integral is trivial and yields 1, including the factor of 2 in front. The
second integral is a little tricky. We substitute t = tan θ, dθ =
	
1/
	
1 + t2

dt, and
1/ cos2 θ = 1 + t2, as suggested in [16,p. 117].
After somerearrangements weendupwiththefollowingexpressionfor thesecond
integral including the prefactor 2:
−2
π arctan
⎛
⎝
t
	 x
R

2 −1 −t2
⎞
⎠


( x
R)2−1
0
.
(7.12)
Here we used [16, 20, 236]. Inserting the upper boundary we see that the denominator
goes to 0. Hence, we ﬁnally obtain −1 because arctan ∞= π/2 and the lower
boundary does not contribute anything. Hence, we ﬁnally get ρ(x) = 0 outside, as
required. Due to rotational symmetry we can also write ρ(r) = 0 outside.

7.2 Backprojection of Filtered Projections
179
Perspective
view
Side
view
Side
view
Side
view
Top
view
1
x
Perspective
view
Side
view
Side
view
Side
view
Top
view
2R
PΘ(t)
t
Perspective
view
Side
view
Side
view
Side
view
Top
view
πR2
SΘ(k)
k
FT
Perspective
view
Side
view
Side
view
Side
view
Top
view
SΘ(k) ·| k|
k
ﬁltered
Perspective
view
Side
view
Side
view
Side
view
Top
view
1
π
QΘ(t)
t
FT−
−
1
Perspective
view
Side
view
Side
view
Side
view
Top
view
R
y
x
Perspective
view
Side
view
Side
view
Side
view
Top
view
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
ρ(x)
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
Θ
Fig. 7.3 Illustration of the “recipe” from top to bottom: projection of homogeneous disc Pθ(t); its
Fourier transform Sθ(k); Qθ(t), the inverse Fourier transform of Sθ(k) multiplied by the ramp |k|;
integration of Qθ(t) over θ for t “outside R”
We see that the expression for Qθ(t) with t “outside R” integrated over the corre-
sponding arc exactly “erases” what the expression for Qθ(t) with t “inside R” with
its corresponding complementary arc has “painted”.
Since the length of the arc outside tends to 0 the closer we come with t to R, the
“rubber” must be inﬁnitely efﬁcient. Now we understand the singularity in (7.10).
All the above individual steps are illustrated in Fig.7.3.
At the end we show, as promised, how the backprojection of ﬁltered projections is
carried out in reality. The whole effort in Fourier transforming the projection, weight-
ing, and backtransforming is completely unnecessary using the convolution theorem:
the integrand of the backtransformation is the product of the Fourier-transformed pro-
jection Sθ(k) and the “ramp” |k|. This is identical to the convolution of the projection

180
7
Tomography: Backprojection of Filtered Projections
itself (forth and back transformation compensate each other) with the inverse Fourier
transform of the “ramp”.
Now you might wonder why we did not do it like this right away. The answer is:
we cannot! The inverse Fourier transform of |k| does not exist, the integral diverges.
Don’t worry! In the discrete world we have a ﬁnite length Δx between neighbouring
data points of the projection and, hence, a kNyq. This corresponds to pixels alternating
between 1 and 0. Faster variations of the projection data simply do not exist. Hence,
the inverse Fourier transform of the ﬁnite ramp exists, of course.
WehavealreadycalculatedsomethingverysimilartotheinverseFouriertransform
of the double-sided ramp, i.e. the Fourier transform of the triangular function. The
positive sign in the exponent does not matter since we are dealing with an even
function. What should not be forgotten is that there is no prefactor
1
2M for the inverse
Fourier transform. I leave it to the exercise to calculate it.
Thus a discrete convolution is not very time consuming. What remains is the
summation over the discrete angles θ.
In practice, of course, more sophisticated equipment like fan geometry beams
or even 3D-detectors in positron annihilation tomography (PET) are used as well
as other “ﬁlters”. Often more sophisticated reconstruction algorithms like iterative
algorithms are required which could cope with complications like lateral straggling
of the beam, absorption of soft X-rays or a limited range of projection angles. We
pedestrians are happy with the basics.
Playground
7.1 Go on the Ramp, not on the Rampage!
Calculate the inverse discrete Fourier transform of the discrete double-sided ramp.
Hint: You may either use the ﬁrst shifting rule or remember the tricky Carl
Friedrich Gauß.
7.2 Slice It!
Proof the Fourier slice theorem.
Hint: Let the projection be onto the x-axis. There is no loss of generality because
we are free to choose the coordinate system.
7.3 Reconstruct It!
Suppose, we have the following object with two projections (smallest, non-trivial
symmetric image):

Playground
181
1
0
0
0
y
x
If it helps, consider a cube of uniform density and its shadow (=projection) when
illuminated with a light-beam from the x- and y-direction. 1 = there is a cube, 0 =
there is no cube (but here we have a 2D-problem).
Use a ramp ﬁlter, deﬁned as {G0 = 0, G1 = 1} and periodic continuation in order
to convolve the projection with the Fourier-transformed ramp-ﬁlter and project the
ﬁltered data back. Discuss all possible different images.
Hint: Perform convolution along the x- and y-direction consecutively.

Appendix: Solutions
Playground of Chap. 1
1.1 Very Speedy
ω = 2πν
with ν = 100 × 106 s−1
= 628.3 Mrad/s
T = 1
ν = 10 ns ; s = cT = 3 × 108 m/s × 10−8 s = 3 m.
Easy to remember: 1ns corresponds to 30cm, the length of a ruler.
1.2 Totally Odd
It is mixed since neither f (t) = f (−t) nor f (−t) = −f (t) is true. The graphical
solution is shown in Fig.A.1. The Fourier series for both feven(t) and fodd(t) are
inﬁnite series; the series for the even part decreases with 1/k2 (kink) whereas that
for the odd part decreases with 1/k (discontinuity).
1.3 Absolutely True
This is an even function! It could have been written as f (t) = | sin πt| in −∞≤
t ≤+∞as well. It is most convenient to integrate from 0 to 1, i.e. a full period of
unit length.
Ck =
1

0
sin πt cos 2πktdt
=
1

0
1
2 [sin(π −2πk)t + sin(π + 2πk)t] dt
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
183

184
Appendix: Solutions
= 1
2

(−1)cos(π −2πk)t
π −2πk

1
0
+(−1)cos(π + 2πk)t
π + 2πk

1
0

1
−1
1
f(t)
t
0.5
−1
1
feven,1(t)
t
−0.5
0.5
−1
1
fodd,1(t)
t
0.5
−1
1
feven,2(t)
t
−0.5
0.5
−1
1
fodd,2(t)
t
0.5
−1
1
feven(t)
t
−0.5
0.5
−1
1
fodd(t)
t
Fig. A.1
f (x) = cos(πt/2) for 0 ≤t ≤1, periodic continuation in the interval −1 ≤t ≤0 is
dotted; the following two graphs add up correctly for the interval 0 ≤t ≤1 but give 0 for the
interval −1 ≤t ≤0; the next two graphs add up correctly for the interval −1 ≤t ≤0 and leave
the interval 0 ≤t ≤1 unchanged; the bottom two graphs show feven(t) = feven,1(t) + feven,2(t)
and fodd(t) = fodd,1(t) + fodd,2(t) (from top to bottom)

Appendix: Solutions
185
= 1
2
(−1) cos π(1 −2k)
π −2πk
+
1
π −2πk + (−1) cos π(1 + 2k)
π + 2πk
+
1
π + 2πk

= 1
2

(−1)
⎡
⎣
=(−1)
cos π
=1
cos 2πk +
=0
sin π sin 2πk
π −2πk
⎤
⎦
+(−1)
⎡
⎣
=(−1)
cos π
=1
cos 2πk −
=0
sin π sin 2πk
π + 2πk
⎤
⎦+
2π
π2 −4π2k2

= 1
2

1
π −2πk +
1
π + 2πk +
2π
π2 −4π2k2

=
2
π −4πk2
=
2
π(1 −4k2)
f (t) =
k=0
2
π −
k=±1
4
3π cos 2πt −
k=±2
4
15π cos 4πt −
k=±3
4
35π cos 6πt −· · · .
1.4 Rather Complex
The function f (t) = 2 sin(3πt/2) cos(πt/2) for 0 ≤t ≤1 can be rewritten using a
trigonometric identity as f (t) = sin πt + sin 2πt. We have just calculated the ﬁrst
part and the linearity theorem tells us that we only have to calculate Ck for the second
part and then add both coefﬁcients. The second part is an odd function! We actually
do not have to calculate Ck because the second part is our basis function for k = 1.
Hence,
Ck =
⎧
⎨
⎩
i/2
for k = +1
−i/2 for k = −1
0
else
.
Together:
Ck =
2
π(1 −4k2) + i
2δk,1 −i
2δk,−1.
The graphical solution is displayed in Fig.A.2.
1.5 Shiftily
With the First Shifting Rule we get:
Cnew
k
= e+i2πk 1
2 Cold
k
= e+iπkCold
k
= (−1)kCold
k .

186
Appendix: Solutions
Shifted ﬁrst part:
Shifted second part:
even terms remain unchanged, odd
terms get a minus sign. We would
have to calculate:
imaginary parts for k = ±1 now
get a minus sign because the
amplitude is negative.
Ck =
1/2

−1/2
cos πt cos 2πkt dt.
FigureA.3 illustrates both shifted parts. Note the kink at the center of the interval
which results from the fact that the slopes of the unshifted function at the interval
boundaries are different (see Fig.A.2).
0
1
0
1
f(t)
t
−1
0
1
1
f(t)
t
−0.5
0
1
2
1
f(t)
t
Fig. A.2 sin πt (top); sin 2πt (middle); sum of both (bottom)

Appendix: Solutions
187
1
−1
0
1
f(t)
t
−1
1
−1
1
f(t)
t
−0.5
1
2
−1
1
f(t)
t
Fig. A.3 Shifted ﬁrst part, shifted second part, sum of both (from top to bottom)
1.6 Cubed
The function is even, the Ck are real. With the trigonometric identity cos3 2πt =
(1/4)(3 cos 2πt + cos 6πt) we get:
C0 = 0
A0 = 0
C1 = C−1 = 3/8
or
A1 = 3/4.
C3 = C−3 = 1/8
A3 = 1/4

188
Appendix: Solutions
Check using the Second Shifting Rule: cos3 2πt = cos 2πt cos2 2πt. From (1.5) we
get cos2 2πt = 1/2 + (1/2) cos 4πt, i.e. Cold
0
= 1/2, Cold
2
= Cold
−2 = 1/4.
From (1.36) with T = 1 and a = 1 we get for the real part (the Bk are 0):
C0 = A0;
Ck = Ak/2;
C−k = Ak/2,
Cold
0
= 1/2 and Cold
2
= Cold
−2 = 1/4
with Cnew
k
= Cold
k−1:
Cnew
0
= Cold
−1 = 0
Cnew
1
= Cold
0
= 1/2
Cnew
−1 = Cold
−2 = 1/4
Cnew
2
= Cold
1
= 0
Cnew
−2 = Cold
−3 = 0
Cnew
3
= Cold
2
= 1/4
Cnew
−3 = Cold
−4 = 0.
Note, that for the shifted Ck we do no longer have Ck = C−k! Let us construct the
Anew
k
ﬁrst:
Anew
k
= Cnew
k
+ Cnew
−k
Anew
0
= 0; Anew
1
= 3/4; Anew
2
= 0; Anew
3
= 1/4. In fact, we want to have
Ck = C−k, so we better deﬁne Cnew
0
= Anew
0
and Cnew
k
= Cnew
−k = Anew
k
/2.
FigureA.4 shows the decomposition of the function f (t) = cos3 2πt using a
trigonometric identity.
The Fourier coefﬁcients Ck of cos2 2πt before and after shifting using the Second
Shifting Rule as well as the Fourier coefﬁcients Ak for cos2 2πt and cos3 2πt are
displayed in Fig.A.5.
1.7 Tackling Inﬁnity
Let T = 1 and set Bk = 0. Then we have from (1.50):
1

0
f (t)2dt = A2
0 + 1
2
∞

k=1
A2
k.
We want to have A2
k ∝1/k4 or Ak ∝±1/k2. Hence, we need a kink in our function,
like in the “triangular function”. However, we do not want the restriction to odd k.
Let’s try a parabola. f (t) = t(1 −t) for 0 ≤t ≤1.

Appendix: Solutions
189
−1
0
1
1
f(t)
t
−0.5
0
0.5
1
f(t)
t
−1
0
1
1
f(t)
t
Fig. A.4 The function f (t) = cos3 2πt can be decomposed into f (t) = (3 cos 2πt + cos 6πt)/4
using a rigonometric identity
For k ̸= 0 we get:
Ck =
1

0
t(1 −t) cos 2πktdt

190
Appendix: Solutions
1
−3
−2
−1
0
1
2
3
Cold
k
k
1
−3
−2
−1
0
1
2
3
Cnew
k
k
0
1
0
1
2
3
Aold
k
k
0
1
0
1
2
3
Anew
k
k
Fig. A.5 Fourier coefﬁcients Ck for f (t) = cos2 2πt = 1/2+(1/2) cos 4πt and after shifting using
the SecondShiftingRule (toptwo).Fouriercoefﬁcients Ak for f (t) = cos2 2πt and f (t) = cos3 2πt
(bottom two)
=
1

0
t cos 2πktdt −
1

0
t2 cos 2πktdt
= cos 2πkt
(2πk)2

1
0
+t sin 2πkt
2πk

1
0
−

2t
(2πk)2 cos 2πkt +
 t2
2πk −
2
(2πk)3

sin 2πkt
 
1
0
= −

2
(2πk)2 × 1 +
 1
2πk −
2
(2πk)3

× 0 −

0 −
2
(2πk)3

× 0

= −
1
2π2k2 .

Appendix: Solutions
191
For k = 0 we get:
C0 =
1

0
t(1 −t)dt =
1

0
tdt −
1

0
t2dt
= t2
2

1
0
−t3
3

1
0
= 1
2 −1
3
= 1
6.
From the left hand side of (1.50) we get:
1

0
t2(1 −t)2dt =
1

0
(t2 −2t3 + t4)dt
= t3
3 −2t4
4 + t5
5

1
0
= 1
3 −1
2 + 1
5
= 10 −15 + 6
30
= 1
30.
Hence, with A0 = C0 and Ak = Ck + C−k = 2Ck we get:
1
30 = 1
36 + 1
2
∞

k=1

1
π2k2
2
= 1
36 +
1
2π4
∞

k=1
1
k4
or
 1
30 −1
36

2π4 =
∞

k=1
1
k4 = 36 −30
1080 2π4
= 6π4
540 = π4
90.
1.8 Smoothly
From (1.63) we know that a discontinuity in the function leads to a
 1
k

-dependence;
a discontinuity in the ﬁrst derivative leads to a

1
k2

-dependence etc.
Here, we have:
f = 1 −8t2 + 16t4
is continuous at the boundaries
f ′ = −16t + 64t3 = −16t(1 −4t2) is continuous at the boundaries
f ′′ = −16 + 192t2
is still continuous at the boundaries
f ′′′ = 384t
is not continuous at the boundaries
f ′′′ 
−1
2

= −192
f ′′′ 
+ 1
2

= +192.

192
Appendix: Solutions
Hence, we should have a

1
k4

-dependence.
Check by direct calculation. For k ̸= 0 we get:
Ck =
+1/2

−1/2
(1 −8t2 + 16t4) cos 2πktdt
= 2
1/2

0
(cos 2πkt −8t2 cos 2πkt + 16t4 cos 2πkt)dt
with a = 2πk
= 2
sin at
a
−8
 2t
a2 cos at +
t2
a −2
a3

sin at

+ t4 sin at
a
−4
a
3t2
a2 −6
a4

sin at −
t3
a −6t
a3

cos at
 
1/2
0
= 2

−8
 1
a2 (−1)k

+ 16 1
2a4 (−1)k(a2 −24)

= 2(−1)k
 8
a2 + 8
a4 (a2 −24)

= 16(−1)k

−1
a2 + 1
a2 −24
a4

= −16 × 24(−1)k
a4
= −384(−1)k
a4
= −24(−1)k
π4k4
.
For k = 0 we get:
C0 = 2
1/2

0
(1 −8t2 + 16t4)dt
= 2

t −8
3t3 + 16
5 t5
 
1/2
0
= 2
1
2 −8
3
1
8 + 16
5
1
32

= 2
1
2 −1
3 + 1
10

= 215 −10 + 3
30
= 8
15.

Appendix: Solutions
193
Playground of Chap. 2
2.1 Black Magic
FigureA.6 illustrates the construction:
i. The inclined straight line is y = x tan θ, the straight line parallel to the x-axis is
y = a. Their intersection yields x tan θ = a or x = a cot θ.
The circle is written as x2 +(y −a/2)2 = (a/2)2 or x2 + y2 −ay = 0. Inserting
x = y cot θ for the inclined straight line yields y2 cot2 θ + y2 = ay or – dividing
by y ̸= 0 – y = a/(1+cot2 θ) = a sin2 θ (the trivial solution y = 0 corresponds
to the intersection at the origin and ±∞).
ii. Eliminating θ we get y = a/(1 + (x/a)2) = a3/(a2 + x2).
iii. Calculating the Fourier transform is the reverse problem of (2.17):
F(ω) = 2
∞

0
a3
a2 + x2 cos ωxdx
= 2a3
∞

0
cos ωax′
a2 + a2x′2 adx′
with x = ax′
= 2a2
∞

0
cos ωax′
1 + x′2 dx′
= a2πe−a|ω|
the double-sided exponential. In fact, what mathematicians call the “versiera” of
Agnesi is—apart from constants—identical to what physicists call a Lorentzian.
a
2
y
x
θ
Fig. A.6 The “versiera” of Agnesi: a construction recipe for a Lorentzian with ruler and circle

194
Appendix: Solutions
What about “Black magic”? A rational function, the geometric locus of a simple
problem involving straight lines and a circle, has a transcendental Fourier trans-
form and vice versa! No surprise, the trigonometric functions used in the Fourier
transformation are transcendental themselves!
2.2 The Phase Shift Knob
We write f (t) ↔Re{F(ω)} + i Im{F(ω)} before shifting. With the First Shifting
Rule we get:
f (t −a) ↔(Re{F(ω)} + i Im{F(ω)}) (cos ωa −i sin ωa)
= Re{F(ω)} cos ωa + Im{F(ω)} sin ωa
+ i (Im{F(ω)} cos ωa −Re{F(ω)} sin ωa).
The imaginary part vanishes for tan ωa = Im{F(ω)}/Re{F(ω)} or a = (1/ω) ×
arctan(Im{F(ω)}/Re{F(ω)}). For a sinusoidal input with phase shift, i.e. f (t) =
sin(ωt −ϕ), we identify a with ϕ/ω, hence ϕ = a arctan(Im{F(ω)}/ Re{F(ω)}).
This is our “phase shift knob”. If, e.g., Re{F(ω)} were 0 before shifting, we would
have to turn the “phase shift knob” by ωa = π/2 or—with ω = 2π/T —by
a = T/4 (or 90◦, i.e. the phase shift between sine and cosine). Since Re{F(ω)}
was non-zero before shifting, less than 90◦is sufﬁcient to make the imaginary
part vanish. The real part which builds up upon shifting must be Re{Fshifted} =

Re{F(ω)}2 + Im{F(ω)}2 because |F(ω)| is unaffected by shifting and Im{Fshifted}
= 0. If you are skeptic insert tan ωa = Im{F(ω)}/Re{F(ω)} into the expression for
Re{Fshifted}:
Re{Fshifted} = Re{F(ω)} cos ωa + Im{F(ω)} sin ωa
= Re{F(ω)}
1
√
1 + tan2 ωa
+ Im{F(ω)}
tan ωa
√
1 + tan2 ωa
=
Re{F(ω)} + Im{F(ω)} Im{F(ω)}
Re{F(ω)}

1 + Im{F(ω)}2
Re{F(ω)}2
=

Re{F(ω)}2 + Im{F(ω)}2.
Of course, the “phase shift knob” does the job only for a given frequency ω.
2.3 Pulses
f (t) is odd; ω0 = n 2π
T/2 or T
2 ω0 = n2π.
F(ω) = (−i)
T/2

−T/2
sin(ω0t) sin ωtdt

Appendix: Solutions
195
= (−i)1
2
T/2

−T/2
(cos(ω0 −ω)t −cos(ω0 + ω)t) dt
= (−i)
T/2

0
(cos(ω0 −ω)t −cos(ω0 + ω)t) dt
= (−i)

sin(ω0 −ω) T
2
ω0 −ω
−sin(ω0 + ω) T
2
ω0 + ω

= (−i)
⎛
⎜⎝
=0
sin ω0 T
2 cos ω T
2 −
=1
cos ω0 T
2 sin ω T
2
ω0 −ω
−
=0
sin ω0 T
2 cos ω T
2 +
=1
cos ω0 T
2 sin ω T
2
ω0 + ω
⎞
⎟⎠
= i sin ω T
2

1
ω0 −ω +
1
ω0 + ω

= 2i sin ωT
2 ×
ω0
ω2
0 −ω2 .
At resonance: F(ω0) = −iT/2; F(−ω0) = +iT/2; |F(±ω0)| = T/2. This is easily
seen by going back to the expressions of the type sin x
x .
For two such pulses centered around ±Δ we get:
Fshifted(ω) = 2i sin ωT
2 ×
ω0
ω2
0 −ω2

eiωΔ + e−iωΔ
= 4i sin ωT
2 ×
ω0
ω2
0 −ω2 cos ωΔ
←−“modulation”.
|F(ω0)| = T if at resonance: ω0Δ = lπ. In order to maximise |F(ω)| we require
ωΔ = lπ;
l = 1, 2, 3, . . .; Δ depends on ω!
2.4 Phase-Locked Pulses
This is a textbook case for the Second Shifting Rule! Hence, we start with DC-pulses.
This function is even!
FDC(ω) =
−Δ+ T
2

−Δ−T
2
cos ωtdt +
+Δ+ T
2

+Δ−T
2
cos ωtdt = 2
Δ+ T
2

Δ−T
2
cos ωtdt
with t′ = −t we get a minus sign from dt′ and another one from the
reversal of the integration boundaries

196
Appendix: Solutions
= 2sin ωt
ω

Δ+ T
2
Δ−T
2
= 2sin ω

Δ + T
2

−sin ω

Δ −T
2

ω
= 4
ω cos ωΔ sin ω T
2 .
With (2.29) we ﬁnally get:
F(ω) = 2i
#
sin(ω + ω0) T
2 cos(ω + ω0)Δ
ω + ω0
−sin(ω −ω0) T
2 cos(ω −ω0)Δ
ω −ω0
$
= 2i
⎡
⎢⎢⎢⎢⎣
cos(ω + ω0)Δ

sin ω T
2
=1
cos ω0 T
2 + cos ω T
2
=0
sin ω0 T
2

ω + ω0
−
cos(ω −ω0)Δ

sin ω T
2
=1
cos ω0 T
2 −cos ω T
2
=0
sin ω0 T
2

ω −ω0
⎤
⎥⎥⎥⎥⎦
= 2i sin ω T
2
cos(ω + ω0)Δ
ω + ω0
−cos(ω −ω0)Δ
ω −ω0

= 2i sin ω T
2
ω2 −ω2
0
((ω −ω0) cos(ω + ω0)Δ −(ω + ω0) cos(ω −ω0)Δ).
In order to ﬁnd the extremes it sufﬁces to calculate:
d
dΔ ((ω −ω0) cos(ω + ω0)Δ −(ω + ω0) cos(ω −ω0)Δ) = 0
(ω −ω0)(−1)(ω + ω0) sin(ω + ω0)Δ −(ω + ω0)(ω −ω0) sin(ω −ω0)Δ = 0
or (ω2 −ω2
0)(sin(ω + ω0)Δ −sin(ω −ω0)Δ) = 0
or (ω2 −ω2
0) cos ωΔ sin ω0Δ = 0.
This is fulﬁlled for all frequencies ω if sin ω0Δ = 0 or ω0Δ = lπ. With this choice
we get ﬁnally:
F(ω) = 2i sin ω T
2
ω2 −ω2
0

(ω −ω0)

cos ωΔ cos ω0Δ −sin ωΔ
=0
sin ω0Δ

−(ω + ω0)

cos ωΔ cos ω0Δ + sin ωΔ
=0
sin ω0Δ


Appendix: Solutions
197
= 2i sin ω T
2
ω2 −ω2
0
(−1)l cos ωΔ × 2ω0
= 4iω0(−1)l sin ω T
2 cos ωΔ
ω2 −ω2
0
.
At resonance ω = ω0 we get:
|F(ω)| = 4ω0 lim
ω→ω0
sin ω T
2
ω2 −ω2
0
with T = 4π
ω0
= 4ω0 lim
ω→ω0
sin 2π ω
ω0
ω2
0

ω2
ω2
0 −1

with α = ω
ω0
= 4
ω0
lim
α→1
sin 2πα
(α −1)(α + 1)
with β = α −1
= 2
ω0
lim
β→0
sin 2π(β + 1)
β
= 2
ω0
lim
β→0
⎛
⎝sin 2πβ
=1
cos 2π + cos 2πβ
=0
sin 2π
β
⎞
⎠
= 2
ω0
lim
β→0
2π cos 2πβ
1
= 4π
ω0
= T.
For the calculation of the FWHM we better go back to DC-pulses!
For two pulses separated by 2Δ we get:
FDC(0) = 4T
2 lim
ω→0
sin ω T
2
ω T
2
= 2T
and |FDC(0)|2 = 4T 2.
From
 4
ω cos ωΔ sin ω T
2
2 = 1
2|FDC(0)|2 = 2T 2 we get (using Δ
T = l
4):
16 cos2 ωTl
4
sin2 ωT
2
= 2T 2ω2
with x = ωT
4
cos2 xl sin2 2x = 2x2.
For l = 1 we get:
cos2 x sin2 2x = 2x2
or cos x sin 2x =
√
2x
cos x × 2 sin x cos x =
√
2x
cos2 x sin x =
x
√
2
.

198
Appendix: Solutions
The solution of this transcendental equation yields:
ω = 4.265
T
with Δ = T
4 .
For l = 2 we get:
cos2 2x sin2 2x = 2x2
or cos 2x sin 2x =
√
2x
1
2 sin 4x =
√
2x
sin 4x = 2
√
2x.
The solution of this transcendental equation yields:
ω = 2.783
T
with Δ = T
2 .
These values for the FWHM should be compared with the value for a single DC-pulse
(see (3.12)):
ω = 5.566
T
.
The Fourier transform of such a double pulse represents the frequency spectrum
which is available for excitation in a resonant absorption experiment. In radiofre-
quency spectroscopy this is called the Ramsey technique, medical doctors would call
it fractionated medication.
2.5 Tricky Convolution
We want to calculate h(t) = f1(t)⊗f2(t). Let’s do it the other way round. We know
from the Convolution Theorem that the Fourier transform of the convolution integral
is merely a product of the individual Fourier transforms, i.e.
f1,2(t) = σ1,2
π
1
σ2
1,2 + t2
↔
F1,2(ω) = e−σ1,2|ω|.
Check:
F(ω) = 2σ
π
∞

0
cos ωt
σ2 + t2 dt
= 2
πσ
∞

0
cos ωt
1 + (t/σ)2 dt

Appendix: Solutions
199
= 2
πσ
∞

0
cos(ωσt′)
1 + t′2 σdt′
with t′ = t
σ
= 2
π
π
2 e−σ|ω| = e−σ|ω|.
No wonder, it’s just the inverse problem of (2.18).
Hence, H(ω) = exp(−σ1|ω|) exp(−σ2|ω|) = exp(−(σ1 + σ2)|ω|). The inverse
transformation yields:
h(t) = 2
2π
∞

0
e−(σ1+σ2)ω cos ωtdω
= 1
π
σ1 + σ2
(σ1 + σ2)2 + t2 ,
i.e. another Lorentzian with σtotal = σ1 + σ2.
2.6 Even Trickier
We have:
f1(t) =
1
σ1
√
2π
e
−1
2
t2
σ2
1
↔
F1(ω) = e−1
2 σ2
1ω2
and:
f2(t) =
1
σ2
√
2π
e
−1
2
t2
σ2
2
↔
F2(ω) = e−1
2 σ2
2ω2.
We want to calculate h(t) = f1(t) ⊗f2(t).
We have H(ω) = exp
 1
2

σ2
1 + σ2
2

ω2
. This we have to backtransform in order
to get the convolution integral:
h(t) = 1
2π
+∞

−∞
e−1
2

σ2
1+σ2
2

ω2e+iωtdω
= 1
π
∞

0
e−1
2

σ2
1+σ2
2

ω2 cos ωtdω
= 1
π
√π
2 1
√
2

σ2
1 + σ2
2
e
−
t2
4 1
2(σ2
1+σ2
2)
=
1
√
2π
1

σ2
1 + σ2
2
e
−1
2
t2
σ2
1+σ2
2

200
Appendix: Solutions
=
1
√
2π
1
σtotal
e
−1
2
t2
σ2
total
with σ2
total = σ2
1 + σ2
2.
Hence, it is again a Gaussian with the σ’s squared added. The calculation of the
convolution integral directly is much more tedious:
f1(t) ⊗f2(t) =
1
σ1σ22π
+∞

−∞
e
−1
2
ξ2
σ2
1 e
−1
2
(t−ξ)2
σ2
2 dξ
with the exponent:
−1
2
#
ξ2
σ2
1
+ ξ2
σ2
2
−2tξ
σ2
2
+ t2
σ2
2
$
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 ⎛
⎝ξ2 −2tξ
σ2
2
1
1
σ2
1 + 1
σ2
2
⎞
⎠+ t2
σ2
2
⎤
⎦
= −1
2
#
1
σ2
1
+ 1
σ2
2
 
ξ2 −2tξσ2
1
σ2
1 + σ2
2
+
t2σ4
1
(σ2
1 + σ2
2)2 −
t2σ4
1
(σ2
1 + σ2
2)2

+ t2
σ2
2
$
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 
ξ −
tσ2
1
σ2
1 + σ2
2
2
−(σ2
1 + σ2
2)
σ2
1σ2
2
t2σ4
1
(σ2
1 + σ2
2)2 + t2
σ2
2
⎤
⎦
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 
ξ −
tσ2
1
σ2
1 + σ2
2
2
−
t2σ2
1
σ2
2(σ2
1 + σ2
2) + t2
σ2
2
⎤
⎦
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 
ξ −
tσ2
1
σ2
1 + σ2
2
2
+ t2
σ2
2

1 −
σ2
1
σ2
1 + σ2
2
⎤
⎦
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 
ξ −
tσ2
1
σ2
1 + σ2
2
2
+ t2
σ2
2
σ2
2
σ2
1 + σ2
2
⎤
⎦
= −1
2
⎡
⎣

1
σ2
1
+ 1
σ2
2
 
ξ −
tσ2
1
σ2
1 + σ2
2
2
+
t2
σ2
1 + σ2
2
⎤
⎦
hence:
f1(t) ⊗f2(t) =
1
σ1σ22π e
−1
2
t2
σ2
1+σ2
2
+∞

−∞
e
−1
2

1
σ2
1
+ 1
σ2
2

ξ−
tσ2
1
σ2
1+σ2
2
2
dξ

Appendix: Solutions
201
with ξ −
tσ2
1
σ2
1 + σ2
2
= ξ′
=
1
σ1σ22π e
−1
2
t2
σ2
1+σ2
2
+∞

−∞
e
−1
2

1
σ2
1
+ 1
σ2
2

ξ′2
dξ′
=
1
σ1σ22π e
−1
2
t2
σ2
1+σ2
2
√π
2
2
1
√
2
 1
σ2
1 + 1
σ2
2
=
1
√
2π
e
−1
2
t2
σ2
1+σ2
2
1
σ1σ2
σ1σ2

σ2
1 + σ2
2
=
1
√
2π
1
σtotal
e
−1
2
t2
σ2
total
with σ2
total = σ2
1 + σ2
2.
2.7 Voigt Proﬁle (for Gourmets only)
f1(t) = σ1
π
1
σ2
1+t2
↔F1(ω) = e−σ1|ω|
f2(t) =
1
σ2
√
2πe
−1
2
t2
σ2
2
↔F2(ω) = e−1
2 σ2
2ω2
H(ω) = e−σ1|ω|e−1
2 σ2
2ω2.
The inverse transformation is a nightmare! Note that H(ω) is an even function.
h(t) = 1
2π 2
∞

0
e−σ1ωe−1
2 σ2
2ω2 cos ωtdω
= 1
π
1
2

2 1
2σ2
2
 1
2
exp

σ2
1 −t2
8 1
2σ2
2

×Γ (1)
⎧
⎨
⎩exp

−iσ1t
4 1
2σ2
2

D−1
⎛
⎝σ1 −it

2 1
2σ2
2
⎞
⎠
+ exp

iσ1t
4 1
2σ2
2

D−1
⎛
⎝σ1 + it

2 1
2σ2
2
⎞
⎠
⎫
⎬
⎭

202
Appendix: Solutions
= 1
2π
1
σ2
exp

σ2
1 −t2
4σ2
2
 
exp

−iσ1t
2σ2
2

D−1
σ1 −it
σ2

+ c.c.

with D−1(z) denoting a parabolic cylinder function. The complex conjugate (“c.c.”)
ensures that h(t) is real. A similar situation shows up in (3.32) where we truncate a
Gaussian. Here, we have a cusp in H(ω). What a messy lineshape for a Lorentzian
spectral line and a spectrometer with a Gaussian resolution function!
Among spectroscopists, this lineshape is known as the “Voigt proﬁle”. The par-
abolic cylinder function D−1(z) can be expressed in terms of the complementary
error function:
D−1(z) = e
z2
4
*π
2 erfc
 z
√
2

.
Hence, we can write:
h(t) =
1
2πσ2
*π
2 e
 σ1−it
σ2
2 1
4 erfc
σ1 −it
√
2σ2

e
+
σ2
1−t2
4σ2
2 e
−iσ1t
2σ2
2
+
1
2πσ2
*π
2 e
 σ1+it
σ2
2 1
4 erfc
σ1 + it
√
2σ2

e
+
σ2
1−t2
4σ2
2 e
+ iσ1t
2σ2
2
=
1
√
2π2σ2

e
1
4σ2
2
[σ2
1−2itσ1−t2+σ2
1−t2−2iσ1t]
erfc
σ1 −it
√
2σ2

+ e
1
4σ2
2
[σ2
1+2itσ1−t2+σ2
1−t2+2iσ1t]
erfc
σ1 + it
√
2σ2
 
=
1
√
2π2σ2

e
1
2σ2
2
(σ2
1−2itσ1−t2)
erfc
σ1 −it
√
2σ2

+ e
1
2σ2
2
(σ2
1+2itσ1−t2)
erfc
σ1 + it
√
2σ2

=
1
√
2π2σ2
⎧
⎨
⎩e

σ1−it
√
2σ2
2
erfc
σ1 −it
√
2σ2

+ e

σ1+it
√
2σ2
2
erfc
σ1 + it
√
2σ2
⎫
⎬
⎭
=
1
√
2π2σ2
erfc
σ1 −it
√
2σ2

e

σ1−it
√
2σ2
2
+ c.c.
2.8 Derivable
The function is mixed. We know that dF(ω)
dω
= −iFT(t f (t)) with f (t) = e−t/τ for
t ≥0 (see 2.58), and we know its Fourier transform (see 2.21) F(ω) = 1/(λ + iω).

Appendix: Solutions
203
Hence:
G(ω) = i d
dω

1
λ + iω

= i
(−i)
(λ + iω)2 =
1
(λ + iω)2
=
(λ −iω)2
(λ + iω)2(λ −iω)2 = λ2 −2iωλ −ω2
(λ2 + ω2)2
=
λ2 −ω2
(λ2 + ω2)2 −
2iωλ
(λ2 + ω2)2
= (λ2 −ω2) −2iωλ
(λ2 + ω2)2
.
Inverse transformation:
g(t) = 1
2π
∞

−∞
eiωt
(λ + iω)2 dω
Real part:
1
2π 2
∞

0
cos ωt λ2 −ω2
(λ2 + ω2)2 dω
Imaginary part:
1
2π 2
∞

0
sin ωt
(−2)ωλ
(λ2 + ω2)2 dω;
(ω sin ωt is even in ω!).
Hint: Reference [8, Nos 3.769.1, 3.769.2] ν = 2; β = λ; x = ω:
1
(λ + iω)2 +
1
(λ −iω)2 = 2(λ2 −ω2)
(λ2 + ω2)2
1
(λ + iω)2 −
1
(λ −iω)2 =
−4iωλ
(λ2 + ω2)2
∞

0
(λ2 −ω2)
(λ2 + ω2)2 cos ωtdω = π
2 te−λt
∞

0
−2iωλ
(λ2 + ω2)2 sin ωtdω = π
2 ite−λt
from real part
1
π
π
2 te−λt +
from imaginary part
1
π
π
2 te−λt
= te−λt
for t > 0.

204
Appendix: Solutions
2.9 Nothing Gets Lost
First, we note that the integral is an even function and we can write:
∞

0
sin2 aω
ω2
dω = 1
2
+∞

−∞
sin2 aω
ω2
dω.
Next, we identify sin aω/ω with F(ω), the Fourier transform of the “rectangular
function” with a = T/2 (and a factor of 2 smaller).
The inverse transform yields:
f (t) =
1/2 for −a ≤t ≤a
0
else
and
+a

−a
| f (t)|2dt = 1
42a = a
2.
Finally, Parseval’s theorem gives:
a
2 = 1
2π
+∞

−∞
sin2 aω
ω2
dω
or
∞

−∞
sin2 aω
ω2
dω = 2πa
2
= πa
or
∞

0
sin2 aω
ω2
dω = πa
2 .
Playground of Chap.3
3.1 Squared
f (ω) = T sin(ωT/2)/(ωT/2). At ω = 0 we have F(0) = T . This function drops
to T/2 at a frequency ω deﬁned by the following transcendental equation:
T
2 = T sin(ωT/2)
ωT/2

Appendix: Solutions
205
with x = ωT/2 we have x/2 = sin x with the solution x = 1.8955, hence ω3dB =
3.791/T . With a pocket calculator we might have done the following:
x
sin x
x/2
1.5
0.997
0.75
1.4
0.985
0.7
1.6
0.9995
0.8
1.8
0.9738
0.9
1.85
0.9613
0.925
1.88
0.9526
0.94
1.89
0.9495
0.945
1.895
0.9479
0.9475
1.896
0.9476
0.948
1.8955
0.94775
0.94775
The total width is ω = 7.582/T .
For F2(ω) we had ω = 5.566/T ; hence the 3 dB-bandwidth of F(ω) is a factor
of 1.362 larger than that of F2(ω), about 4% less than
√
2 = 1.414.
3.2 Let’s Gibbs Again
There are tiny steps at the interval boundaries, hence we have −6 dB/octave.
3.3 Expander
Blackman–Harris window:
f (t) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
3

n=0
an cos 2πnt
T
for −T/2 ≤t ≤T/2
0
else
.
From the expansion of the cosines we get (in the interval −T/2 ≤t ≤T/2):
f (t) =
3

n=0
an

1 −1
2!
2πnt
T
2
+ 1
4!
2πnt
T
4
−1
6!
2πnt
T
6
+ . . .

=
∞

k=0
bk
 t
T/2
2k
.

206
Appendix: Solutions
−12
−10
−8
−6
−4
−2
0
2
4
6
8
10
12
1
2
3
4
5
6
7
8
9 k
bk; br
Fig. A.7 Expansion coefﬁcients bk for the Blackman–Harris window (−74 dB) (dotted line) and
expansion coefﬁcients br for the Kaiser–Bessel window (β = 9) (solid line). There are even powers
of t only, i.e. the coefﬁcient b6 corresponds to t12
Inserting the coefﬁcients an for the −74 dB-window including the second option of
the corrections on p. 86 we get:
k
bk
0
+1.0000
1
−4.4888
2
+8.0592
3
−10.9471
4
+8.9791
5
−5.4918
6
+2.7106
7
−1.1105
8
+0.3761
9
−0.1047
The coefﬁcients are displayed in Fig.A.7. Note that at the interval boundaries
t = ±T/2 we should have ,∞
k=0 bk = 0. The ﬁrst ten terms add up to −0.0196.

Appendix: Solutions
207
Next, we calculate:
I0(z) =
∞

k=0
 1
4z2k
(k!)2
for z = 9.
k
(4.5k/k!)2
0
1.000
1
20.250
2
102.516
3
230.660
4
291.929
5
236.463
6
133.010
7
54.969
8
17.392
9
4.348
Summing up the ﬁrst ten terms, we get 1,092.5, close to the exact value of
1,093.588.
Next, we have to expand the numerator of the Kaiser–Bessel window function.
I (9) f (t) =
∞

k=0
-
81
4

1 −
 2t
T
2.k
(k!)2
=
∞

k=0
 81
4
k
(k!)2

1 −
2t
T
2k
with
2t
T
2
= y
=
∞

k=0
# 9
2
k
k!
$2
(1 −y)k
#
with binomial formula (1 −y)k =
k

r=0
k
r

(−1)r yr =
k

r=0
k!
r!(k −r)!(−y)r
$
=
∞

k=0
# 9
2
k
k!
$2
k

r=0
k!
r!(k −r)!(−y)r
=
∞

k=0
# 9
2
k
k!
$2
r=0
+
∞

k=1
# 9
2
k
k!
$2
=k
/
01
2
k!
(k −1)!(−y)1
r=1

208
Appendix: Solutions
+
∞

k=2
# 9
2
k
k!
$2
=k(k−1)/2
/
01
2
k!
2!(k −2)! y2
r=2
+
∞

k=3
# 9
2
k
k!
$2
=k(k−1)(k−2)/6
/
01
2
k!
3!(k −3)! (−y)3
r=3
+ . . .
=
∞

r=0
br
 t
T/2
2r
(Note: For integer and negative k we have k! = ±∞and 0! = 1.).
Here, the calculation of each expansion coefﬁcient br requires (in principle) the
calculation of an inﬁnite series. We truncate the series at k = 9. For r = 0 up to
r = 9 we get:
r
br
0
+1.0000
1
−4.2421
2
+8.0039
3
−8.9811
4
+6.7708
5
−3.6767
6
+1.5063
7
−0.4816
8
+0.1233
9
−0.0258
These coefﬁcients are displayed in Fig.A.7. Note, that at the interval boundaries
t = ±T/2 the coefﬁcients br do no longer have to add up to 0 exactly. FigureA.7
shows why the Blackman–Harris (−74 dB) window and the Kaiser–Bessel (β = 9)
window have similar properties.
3.4 Minorities
a. For a rectangular window we have ω = 5.566/T = 50 Mrad/s from which we
get T = 111.32 ns.
b. The suspected signal is at 600 Mrad/s, i.e. 4 times the FWHM away from the
central peak.
The rectangular window is not good for the detection. The triangular window has
a factor 8.016/5.566 = 1.44 larger FWHM, i.e. our suspected peak is 2.78 times
the FWHM away from the central peak. A glance to Fig.3.2 tells you, that this
window is also not good. The cosine window has only a factor of 7.47/5.566 = 1.34
larger FWHM, but is still not good enough. For the cos2-window we have a factor

Appendix: Solutions
209
of 9.06/5.566 = 1.63 larger FWHM, i.e. only 2.45 times the FWHM away from the
central peak. This means, that −50 dB, 2.45 times the FWHM higher than the central
peak, is still not detectable with this window. Similarly, the Hamming window is not
good enough. The Gauss window as described in Sect.3.7 would be a choice because
ωπ ∼9.06, but the sidelobe suppression just sufﬁces.
The Kaiser–Bessel window with β = 8 has ωT ∼10, but sufﬁcient sidelobe
suppression, and, of course, both Blackman–Harris windows would be adequate.
Playground of Chap.4
4.1 Correlated
{hk} = (const./N) ,N−1
l=0
fl, independent of k if ,N−1
l=0
fl vanishes (i.e. the average
is 0) then {hk} = 0 for all k, otherwise {hk} = const. × ⟨fl⟩for all k (see Fig.A.8).
4.2 No Common Ground
hk = 1
N
N−1

l=0
flg∗
l+k
fk
k
Fj
j
gk
k
Gj
j
hk
k
Hj = Fj · Gj
j
Fig. A.8 An arbitrary fk (top left) and its Fourier transform Fj (top right). A constant gk (middle
left) and its Fourier transform G j (middle right). The product of Hj = Fj G j (bottom right) and
its inverse transform hk (bottom left)

210
Appendix: Solutions
we don’t need ∗here.
h0 = 1
4( f0g0 + f1g1 + f2g2 + f3g3)
= 1
4(1 × 1 + 0 × (−1) + (−1) × 1 + 0 × (−1)) = 0
h1 = 1
4( f0g1 + f1g2 + f2g3 + f3g0)
= 1
4(1 × (−1) + 0 × 1 + (−1) × (−1) + 0 × 1) = 0
h2 = 1
4( f0g2 + f1g3 + f2g0 + f3g1)
= 1
4(1 × 1 + 0 × (−1) + (−1) × 1 + 0 × (−1)) = 0
h3 = 1
4( f0g3 + f1g0 + f2g1 + f3g2)
= 1
4(1 × (−1) + 0 × 1 + (−1) × (−1) + 0 × 1) = 0
f corresponds to half the Nyquist frequency and g corresponds to the Nyquist fre-
quency. Their cross correlation vanishes. The FT of { fk} is {Fj} = {0, 1/2, 0, 1/2},
the FT of {gk} is {G j} = {0, 0, 1, 0}. The multiplication of FjG j shows that there
is nothing in common:
FjG j = {0, 0, 0, 0} and, hence, {hk} = {0, 0, 0, 0}.
4.3 Brotherly
F0 = 1
2
F1 = 1
4

1 + 0 × e−2πi×1
4
+ 1 × e−2πi×2
4
+ 0 × e−2πi×3
4

= 1
4(1 + 0 + (−1) + 0) = 0
F2 = 1
4

1 + 0 × e−2πi×2
4
+ 1 × e−2πi×4
4
+ 0 × e−2πi×6
4

= 1
4(1 + 0 + 1 + 0) = 1
2
F3 = 0
G j = {0, 0, 1, 0}
Nyquist frequency
Hj = FjG j = {0, 0, 1/2, 0} .

Appendix: Solutions
211
Inverse transformation:
hk =
N−1

j=0
Hj W +kj
N
W +kj
4
= e
2πikj
N .
Hence:
hk =
3

j=0
Hje
2πikj
4
=
3

j=0
Hjei πkj
2
h0 = H0 + H1 + H2 + H3 = 1
2
h1 = H0 + H1 × i + H2 × (−1) + H3 × (−i) = −1
2
h2 = H0 + H1 × (−1) + H2 × 1 + H3 × (−1) = 1
2
h3 = H0 + H1 × (−i) + H2 × (−1) + H3 × i = −1
2.
FigureA.9 is the graphical illustration.
−1
−0.5
0
0.5
1
fk
k
Fj
j
−1
−0.5
0
0.5
1
gk
k
Gj
j
−1
−0.5
0
0.5
1
hk
k
Hj = Fj · Gj
j
Fig. A.9 Nyquist frequency plus const.= 1/2 (top left) and its Fourier transform Fj (top right).
Nyquist frequency (middle left) and its Fourier transform G j (middle right). Product of Hj = Fj G j
(bottom right) and its inverse transform (bottom left)

212
Appendix: Solutions
4.4 Autocorrelated
N = 6, real input:
hk = 1
6
5

l=0
fl fl+k
h0 = 1
6
5

l=0
f 2
l = 1
6(1 + 4 + 9 + 4 + 1) = 19
6
h1 = 1
6( f0 f1 + f1 f2 + f2 f3 + f3 f4 + f4 f5 + f5 f0)
= 1
6(0 × 1 + 1 × 2 + 2 × 3 + 3 × 2 + 2 × 1 + 1 × 0)
= 1
6(2 + 6 + 6 + 2) = 16
6
h2 = 1
6( f0 f2 + f1 f3 + f2 f4 + f3 f5 + f4 f0 + f5 f1)
= 1
6(0 × 2 + 1 × 3 + 2 × 2 + 3 × 1 + 2 × 0 + 1 × 1)
= 1
6(3 + 4 + 3 + 1) = 11
6
h3 = 1
6( f0 f3 + f1 f4 + f2 f5 + f3 f0 + f4 f1 + f5 f2)
= 1
6(0 × 3 + 1 × 2 + 2 × 1 + 3 × 0 + 2 × 1 + 1 × 2)
= 1
6(2 + 2 + 2 + 2) = 8
6
h4 = 1
6( f0 f4 + f1 f5 + f2 f0 + f3 f1 + f4 f2 + f5 f3)
= 1
6(0 × 2 + 1 × 1 + 2 × 0 + 3 × 1 + 2 × 2 + 1 × 3)
= 1
6(1 + 3 + 4 + 3) = 11
6
h5 = 1
6( f0 f5 + f1 f0 + f2 f1 + f3 f2 + f4 f3 + f5 f4)
= 1
6(0 × 1 + 1 × 0 + 2 × 1 + 3 × 2 + 2 × 3 + 1 × 2)
= 1
6(2 + 6 + 6 + 2) = 16
6 .
FT of { fk}: N = 6, fk = f−k = f6−k →even!

Appendix: Solutions
213
Fj = 1
6
5

k=0
fk cos 2πkj
6
= 1
6
5

k=0
fk cos πkj
3
F0 = 1
6(0 + 1 + 2 + 3 + 2 + 1) = 9
6
F1 = 1
6

1 cos π
3 + 2 cos 2π
3 + 3 cos 3π
3 + 2 cos 4π
3 + 1 cos 5π
3

= 1
6
1
2 + 2 ×

−1
2

+ 3 × (−1) + 2 ×

−1
2

+ 1 × 1
2

= 1
6
1
2 −1 −3 −1 + 1
2

= 1
6(−4) = −4
6
F2 = 1
6

1 cos 2π
3 + 2 cos 4π
3 + 3 cos 6π
3 + 2 cos 8π
3 + 1 cos 10π
3

= 1
6

−1
2 + 2 ×

−1
2

+ 3 × 1 + 2 ×

−1
2

+ 1 ×

−1
2

= 1
6(−1 −2 + 3) = 0
F3 = 1
6

1 cos 3π
3 + 2 cos 6π
3 + 3 cos 9π
3 + 2 cos 12π
3
+ 1 cos 15π
3

= 1
6(−1 + 2 × 1 + 3 × (−1) + 2 × 1 + 1 × (−1))
= 1
6(−5 + 4) = −1
6
F4 = F2 = 0
F5 = F1 = −4
6.
{F2
j } =
9
4, 4
9, 0, 1
36, 0, 4
9

.
FT({hk}):
H0 = 1
6
19
6 + 16
6 + 11
6 + 8
6 + 11
6 + 16
6

= 81
36 = 9
4
H1 = 1
6
19
6 + 16
6 cos π
3 + 11
6 cos 2π
3 + 8
6 cos 3π
3 + 11
6 cos 4π
3 + 16
6 cos 5π
3

= 4
9
H2 = 1
6
19
6 + 16
6 cos 2π
3 + 11
6 cos 4π
3 + 8
6 cos 6π
3 + 11
6 cos 8π
3 + 16
6 cos 10π
3

= 0

214
Appendix: Solutions
H3 = 1
6
19
6 + 16
6 cos 3π
3 + 11
6 cos 6π
3 + 8
6 cos 9π
3 + 11
6 cos 12π
3
+ 16
6 cos 15π
3

= 1
36
H4 = H2 = 0
H5 = H1 = 4
9.
4.5 Shifting Around
a. The series is even, because of fk = + fN−k.
b. Because of the duality of the forward and inverse transformations (apart from the
normalization factor, this only concerns a sign at e−iωt →e+iωt) the question
could also be: Which series produces only a single Fourier coefﬁcient when
Fourier-transformed, incidentally at frequency 0? A constant, of course. The
Fourier transformation of a “discrete δ-function” therefore is a constant (see
Fig.A.10).
c. The series is mixed. It is composed as shown in Fig.A.11.
d. The shifting only results in a phase in Fj, d.h., |Fj|2 stays the same.
···
F(j)
j
Fig. A.10 Answer (b)
· · ·
1
N −1N
· · ·
1
N −1
· · ·
1
N −1
=
+
Fig. A.11 Answer (c)
Fig. A.12 Real part of the Fourier transform of the random series

Appendix: Solutions
215
Fig. A.13 Imaginary part of the Fourier transform of the random series
4.6 Pure Noise
a. We get a random series both in the real part (Fig.A.12) and in the imaginary part
(Fig.A.13). Random means the absence of any structure.
So all spectral components have to occur, and they, in turn, have to be random,
otherwise the inverse transformation would generate a structure.
b. Trick: For N →∞we can imagine the random series as the discrete version of
the function f (t) = t for −1/2 ≤t ≤1/2. For this purpose we only have to
order the numbers of the random series according to their magnitudes! According
to Parseval’s theorem (4.31) we don’t have to do a Fourier transformation at all.
So with 2N + 1 samples we need:
2
2N + 1
N

k=0
 k
N
2
=
2
2N + 1
1
4N 2
(2N + 1)N(N + 1)
6
(A.1)
= N + 1
12N ;
lim
N→∞
N + 1
12N
= 1
12.
We could have solved the following integral instead:
+0.5

−0.5
t2dt = 2
+0.5

0
t2dt = 2t3
3

0.5
0
= 2
3
1
8 = 1
12.
(A.2)
Let’s compare: 0.5 cos ωt has, due to cos2 ωt = 0.5, the signal power 0.52×0.5 =
1/8.
Fig. A.14 Real part of the Fourier transform according to (4.58)

216
Appendix: Solutions
Gj
j
↓
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Fj:
G0:
G1:
G2:
Fig. A.15 Result of the cross correlation: at the position of the fundamental frequency at channel 4
the “signal” (arrow) is clearly visible; channel 0 also happens to run up, however, there is no
corresponding pattern
4.7 Pattern Recognition
It’s best to use the cross correlation. It is formed with the Fourier transform of
the experimental data Fig.A.14 and the theoretical “frequency comb”, the pattern
(Fig.4.29). As we’re looking for cosine patterns, we only use the real part for the
cross correlation.
Here, channel 36 goes up (from 128 channels to ΩNyq). The right half is the mirror
image of the left half. So the Fourier transform suggests only a spectral component
(apart from noise) at (36/128)ΩNyq = (9/32)ΩNyq. If we search for pattern Fig.4.29
in the data, we get something totally different.
The result of the cross correlation with the theoretical frequency comb leads to
the following algorithm:
G j = F5 j + F7 j + F9 j.
(A.3)
The result shows Fig.A.15.
So the noisy signal contains cosine components with the frequencies 5π(4/128),
7π(4/128), and 9π(4/128).
4.8 Go on the Ramp (for Gourmets only)
The series is mixed because neither fk = f−k nor fk = −f−k is true.
Decomposition into even and odd parts.
We have the following equations:
fk = f even
k
+ f odd
k
f even
k
= f even
N−k
for k = 0, 1, . . . , N −1.
f odd
k
= −f odd
N−k
The ﬁrst condition gives N equations for 2N unknowns. The second and third equa-
tions give each N further conditions, each appears twice, hence we have N additional
equations. Instead of solving this system of linear equations, we solve the problem
by arguing.
First, because of f odd
0
= 0 we have f even
0
= 0. Shifting the ramp downwards by
N/2 we already have an odd function with the exception of k = 0 (see Fig.A.16):

Appendix: Solutions
217
1
2
3
−4
−3
−2
−1
0
1
2
3
fk
k
1
2
3
−4
−3
−2
−1
0
1
2
3
fk,even
k
−2
−1
1
−4
−3
−2
−1
1
2
3
fk,odd
k
−2
−1
1
−4
−3
−2
−1
1
2
3
fk,shifted
k
Fig. A.16 One-sided ramp for N = 4 (periodic continuation with open circles); decomposition
into even and odd parts; ramp shifted downwards by 2 immediately gives the odd part (except for
k = 0) (from top to bottom)

218
Appendix: Solutions
f shifted
k
= k −N
2
for k = 0, 1, 2, . . . , N −1.
f shifted
−k
= f shifted
N−k
= (N −k) −N
2 = N
2 −k
= −

k −N
2

.
So we have already found the odd part:
f odd
k
= k −N
2
for k = 1, 2, . . . , N −1
f odd
0
= 0
and, of course, we have also found the real part:
f even
k
= N
2
for k = 1, 2, . . . , N −1 (compensates for the shift)
f even
0
= 0
(see above).
Real part of Fourier transform:
Re{Fj} = 1
N
N−1

k=1
N
2 cos 2πkj
N
.
Dirichlet: 1/2 + cos x + cos 2x + · · · + cos Nx = sin[(N + 1/2)x]/(2 sin[x/2]);
here we have x = 2π j/N and instead of N we go until N −1:
N−1

k=1
cos kx = sin(N −1
2)x
2 sin x
2
−1
2
=
=0
sin Nx cos x
2−
=1
cos Nx sin x
2
2 sin x
2
−1
2
= −1
2 −1
2 = −1.
Re{F0} = 1
N
N
2
(N −1)
1 2/ 0
number of terms
= N −1
2
,
Re{Fj} = −1
2.
Check:
Re{F0} +
N−1

j=1
Re{Fj} = N −1
2
−1
2(N −1) = 0.

Appendix: Solutions
219
Imaginary part of Fourier transform:
Im{Fj} = 1
N
N−1

k=1

k −N
2

sin 2πkj
N
.
For the sum over sines we need the analogue of Dirichlet’s kernel for sines. Let us
try an expression with an unknown numerator but the same denominator as for the
sum of cosines:
sin x + sin 2x + · · · + sin(N −1)x =
?
2 sin x
2
2 sin x
2 sin x + 2 sin x
2 sin 2x + . . . + 2 sin x
2 sin(N −1)x
= cos x
2 −cos 3x
2 + cos 3x
2
1
2/
0
=0
−cos 5x
2 + · · · + cos

N −3
2

x
1
2/
0
=0
−cos

N −1
2

x
= cos x
2 −cos

N −1
2

x
−→
N−1

k=1
sin kx = cos x
2 −cos

N −1
2

x
2 sin x
2
= cos x
2−
=1
cos Nx cos x
2+
=0
sin Nx sin x
2
2 sin x
2
= 0.
Hence, there remains only the term with k sin(2πkj/N). We can evaluate this sum
by differentiating the formula for Dirichlet’s kernel (Use the general formula and
insert x = 2π j/N into the differentiated formula!):
d
dx
N−1

k=1
cos kx = −
N−1

k=1
k sin kx
= 1
2

N −1
2

cos
3
N −1
2

x
4
sin x
2 −sin
3
N −1
2

x
4 1
2 cos x
2
sin2 x
2
= 1
2

N −1
2
 
=1
cos Nx cos x
2

sin x
2 −

=0
sin Nx cos x
2 −
=1
cos Nx sin x
2

1
2 cos x
2
sin2 x
2

220
Appendix: Solutions
= 1
2

N −1
2
 cos x
2
sin x
2
+ 1
2
cos x
2
sin x
2

= N
2
cos x
2
sin x
2
= N
2 cot π j
N
Im{Fj} = 1
N (−1) N
2 cot π j
N = −1
2 cot π j
N ,
j ̸= 0,
Im{F0} = 0,
ﬁnally together:
Fj =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
−1
2 −i
2 cot π j
N for j ̸= 0
N −1
2
for j = 0
.
Parseval’s theorem:
left hand side:
1
N
N−1

k=1
k2 = 1
N
(N −1)N(2(N −1) + 1)
6
= (N −1)(2N −1)
6
right hand side:
 N −1
2
2
+ 1
4
N−1

j=1

1 + i cot π j
N
 
1 −i cot π j
N

=
 N −1
2
2
+ 1
4
N−1

j=1

1 + cot2 π j
N

=
 N −1
2
2
+ 1
4
N−1

j=1
1
sin2 π j
N
hence:
(N −1)(2N −1)
6
=
 N −1
2
2
+ 1
4
N−1

j=1
1
sin2 π j
N
or 1
4
N−1

j=1
1
sin2 π j
N
= (N −1)(2N −1)
6
−(N −1)2
4
= (N −1)(2N −1)2 −(N −1)3
12
= N −1
12
(4N −2 −3N + 3)
= N −1
12
(N + 1) = N 2 −1
12

Appendix: Solutions
221
and ﬁnally:
N−1

j=1
1
sin2 π j
N
= N 2 −1
3
.
The result for ,N−1
j=1 cot2(π j/N) is obtained as follows: we use Parseval’s theo-
rem for the real/even and imaginary/odd parts separately. For the real part we get:
left hand side:
1
N
 N
2
2
(N −1) = N(N −1)
4
right hand side:
 N −1
2
2
+ N −1
4
= N(N −1)
4
.
The real parts are equal, so the imaginary parts of the left and right hand sides have
to be equal, too.
For the imaginary part we get:
left hand side:
1
N
N−1

k=1
k −N
2
2
= 1
N
N−1

k=1

k2 −kN + N 2
4

= 1
N
(N −1)N(2N −1)
6
−N(N −1)N
2
+ N 2(N −1)
4

= (N −1)(N −2)
12
right hand side:
1
4
N−1

j=1
cot2 π j
N
from which we get ,N−1
j=1 cot2 π j
N = (N −1)(N −2)/3.
4.9 Transcendental (for Gourmets only)
The series is even because:
f−k = fN−k
?= fk.
Insert N −k into (4.59) on both sides:

222
Appendix: Solutions
fN−k =
 N −k
for N −k = 0, 1, . . . , N/2 −1
N −(N −k) for N −k = N/2, N/2 + 1, . . . , N −1
or
fN−k =
 N −k for k = N, N −1, . . . , N/2 + 1
k
for k = N/2, N/2 −1, . . . , 1
or
fN−k =
k
for k = 1, 2, . . . , N/2
N −k for k = N/2 + 1, . . . , N ,
a. for k = N we have f0 = 0, so we could include it also in the ﬁrst line because
fN = f0 = 0.
b. for k = N/2 we have fN/2 = N/2, so we could include it also in the second line.
This completes the proof. Since the series is even, we only have to calculate the real
part:
Fj = 1
N
N−1

k=0
fk cos 2πkj
N
= 1
N
⎛
⎜⎝
N
2 −1

k=0
k cos 2πkj
N
+
N−1

k= N
2
(N −k) cos 2πkj
N
⎞
⎟⎠
with k′ = N −k
= 1
N
⎛
⎜⎝
N
2 −1

k=0
k cos 2πkj
N
+
1

k′= N
2
k′ cos 2π(N −k′) j
N
⎞
⎟⎠
= 1
N
⎛
⎜⎝
N
2 −1

k=0
k cos 2πkj
N
+
N
2

k′=1
k′
⎛
⎜⎜⎝cos 2πN j
N
1
2/
0
=1
cos 2πk′ j
N
+ sin 2πN j
N
1
2/
0
=0
sin 2π(−k′) j
N
⎞
⎟⎟⎠
⎞
⎟⎟⎠
= 1
N
⎛
⎜⎝
N
2 −1

k=0
k cos 2πkj
N
+
N
2

k′=1
k′ cos 2πk′ j
N
⎞
⎟⎠
= 1
N
⎛
⎜⎝2
N
2 −1

k=1
k cos 2πkj
N
+ N
2 cos π j
⎞
⎟⎠
with 2π N
2 j
N
= π j
= 2
N
N
2 −1

k=1
k cos 2πkj
N
+ 1
2(−1) j.

Appendix: Solutions
223
This can be simpliﬁed further.
How can we get this sum? Let us try an expression with an unknown numerator
but the same denominator as for the sum of cosines (“sister” analogue of Dirichlet’s
kernel):
N
2 −1

k=1
sin kx =
?
2 sin x
2
with x = 2π j
N .
The numerator of the right hand side is:
2 sin x
2 sin x + 2 sin x
2 sin 2x + · · · + 2 sin x
2 sin
 N
2 −1

x
= cos
x
2

−cos
3x
2

+ cos
3x
2

1
2/
0
=0
−· · ·
−cos
 N
2 −3
2

x + cos
 N
2 −3
2

x
1
2/
0
=0
−cos
 N
2 −1
2

x
= cos x
2 −cos N −1
2
x.
Finally we get:
N
2 −1

k=1
sin kx =
cos x
2 −cos N −1
2
x
2 sin x
2
, N = even, do not use for x = 0.
Now we take the derivative with respect to x. Let us exclude the special case of
x = 0. We shall treat it later.
d
dx
N
2 −1

k=1
sin kx =
N
2 −1

k=1
k cos kx
= 1
2

−1
2 sin x
2 +
 N −1
2

sin
 N −1
2

x

sin x
2 −

cos x
2 −cos
 N −1
2

x
 1
2 cos x
2
sin2 x
2
= 1
2
−1
2 sin2 x
2 −1
2 cos2 x
2 +
 N −1
2
 ⎛
⎝
=0
sin Nx
2
cos x
2 −cos Nx
2 sin x
2
⎞
⎠sin x
2
sin2 x
2

224
Appendix: Solutions
+1
2
⎛
⎝cos Nx
2 cos x
2 +
=0
sin Nx
2
sin x
2
⎞
⎠cos x
2
with x = 2π j
N , cos Nx
2
= cos π j = (−1) j, sin Nx
2
= sin π j = 0
= 1
2
−1
2 + N −1
2
(−1) j+1 sin2 x
2 + 1
2(−1) j cos2 x
2
sin2 x
2
= 1
2
−1
2 + (−1) j+1 N
2 sin2 x
2 −1
2(−1) j
⎛
⎝−
=−1
cos2 x
2 −sin2 x
2
⎞
⎠
sin2 x
2
= 1
2
⎛
⎜⎝
1
2 sin2 x
2

(−1) j −1

+ (−1) j+1 N
2
⎞
⎟⎠
⇒Fj = 2
N
⎛
⎜⎝(−1) j −1
2
1
2
1
sin2 π j
N
+ (−1) j+1 N
4
⎞
⎟⎠+ 1
2(−1) j
= (−1) j −1
2N sin2 π j
N
=
⎧
⎪⎨
⎪⎩
−
1
N sin2 π j
N
for j = odd
0
else
.
The special case of j = 0 is obtained from:
N
2 −1

k=1
k =
 N
2 −1
 N
2
2
= N 2
8 −N
4 .
Hence:
F0 = 2
N
 N 2
8 −N
4

+ 1
2 = N
4 .
We ﬁnally have:
Fj =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
−
1
N sin2 π j
N
for j = odd
0
for j = even, j ̸= 0
N
4
for j = 0
.

Appendix: Solutions
225
Now we use Parseval’s theorem:
l.h.s.
1
N
⎡
⎢⎢⎣2
 N
2 −1
 N
2

2
 N
2 −1

+ 1

6
+ N 2
4
⎤
⎥⎥⎦
= 1
N
⎡
⎢⎣21
2
(N −2)1
2 N(N −1)
6
+ N 2
4
⎤
⎥⎦
= 1
N
 N(N −1)(N −2) + 3N 2
12

= (N −1)(N −2) + 3N
12
= N 2 + 2
12
r.h.s.
N 2
16 +
N−1

j=1
odd
1
N 2 sin4 π j
N
with j = 2k −1
=
N/2

k=1
1
N 2 sin4 π(2k −1)
N
+ N 2
16
which gives:
N 2
12 + 1
6 =
N/2

k=1
1
N 2 sin4 π(2k −1)
N
+ N 2
16
and ﬁnally:
N/2

k=1
1
sin4 π(2k −1)
N
= N 2(N 2 + 8)
48
.
The right hand side can be shown to be an integer! Let N = 2M.
4M2(4M2 + 8)
48
= 4M24(M2 + 2)
48
= M2(M2 + 2)
3
= M(M −1)M(M + 1) + 3M2
3
= M (M −1)M(M + 1)
3
+ M2.

226
Appendix: Solutions
Three consecutive numbers can always be divided by 3!
Now we use the high-pass property:
N−1

j=0
Fj = N
4 −1
N
N−1

j=1
odd
1
sin2 π j
N
with j = 2k −1
= N
4 −1
N
N
2

k=1
1
sin2 π(2k −1)
N
.
For a high-pass ﬁlter we must have ,N−1
j=0 Fj = 0 because a zero frequency must
not be transmitted (see Chap.5). If you want, use deﬁnition (4.13) with k = 0 and
interpret fk being the ﬁlter in the frequency domain and Fj its Fourier transform.
Hence, we get:
N/2

k=1
1
sin2 π(2k −1)
N
= N 2
4 .
Since N is even, the result is always integer!
These are nice examples how a ﬁnite sum over an expression involving a trans-
cendental function yields an integer!
Playground of Chap.5
5.1 Totally Different
The ﬁrst central difference is:
“exact”
yk = fk+1 −fk−1
2t
f ′(t)
= −π
2 sin π
2 t
y0 = f1 −f−1
2/3
= f1 −f5
2/3
= 1 +
√
3/2
2/3
= 2.799 f ′(t0)
= 0
y1 = f2 −f0
2/3
= 1/2 −1
2/3
= −0.750
f ′(t1)
= −π
2 sin π
2
1
3 = −0.7854
y2 = f3 −f1
2/3
= 0 −
√
3/2
2/3
= −1.299
f ′(t2)
= −π
2 sin π
2
2
3 = −1.3603
y3 = f4 −f2
2/3
= −1/2 −1/2
2/3
= −1.500
f ′(t3)
= −π
2 sin π
2
3
3 = −1.5708
y4 = f5 −f3
2/3
= −
√
3/2 −0
2/3
= −1.299
f ′(t4)
= −π
2 sin π
2
4
3 = −1.3603
y5 = f6 −f4
2/3
= f0 −f4
2/3
= 1 + 1/2
2/3
= 2.250
f ′(t5)
= −π
2 sin π
2
5
3 = −0.7854.
Of course, the beginning y0 and the end y5 are totally wrong because of the
periodic continuation. Let us calculate the relative error for the other derivatives:

Appendix: Solutions
227
−1
1
−1
1
2
3
4
5
6
fk
k
−2
−1
1
2
3
−1
1
2
3
4
5
6
fk, yk
k
Fig. A.17 Input fk = cos πtk/2, tk = kt with k = 0, 1, . . . , 5 and t = 1/3 (top). First central
difference (bottom). The solid line is the exact derivative. y0 and y5 appear to be totally wrong.
However, we must not forget the periodic continuation of the series (see open circles in the top panel)
k = 1
exact −discrete
exact
= −0.7854 + 0.750
−0.7854
= 4.5 % too small
k = 2
4.5 % too small
k = 3
4.5 % too small
k = 4
4.5 % too small.
The result is plotted in Fig.A.17.
5.2 Simpson’s-1/3 Versus Trapezoid
The exact, trapezoidal, and Simpson’s-1/3 calculations are illustrated in Fig.A.18.
Trapezoid:
I =

f0
2 +
3

k=1
fk + f4
2

=
1
2 + 0.5 −0.5 −1 −0.5
2

= −0.75,

228
Appendix: Solutions
−1
−0.5
0
0.5
1
1
2
3
4
fk
k
−1
−0.5
0
0.5
1
1
yk
k
−1
−0.5
0
0.5
1
1
yk
k
2
3
4
2
3
4
Fig. A.18 Input fk = cos πtk, tk = kt, k = 0, 1, . . . , 4, t = 1/3 (top). Area of trapezoids to
be added up. Step width is t (middle). Area of parabolically interpolated segment in Simpson’s
1/3-rule. Step width is 2t (bottom)
Simpson’s-1/3:
I =
 f2 + 4 f1 + f0
3

+
 f4 + 4 f3 + f2
3

=
−0.5 + 4 × 0.5 + 1
3

+
−0.5 + 4 × (−1) + (−0.5)
3

= −0.833.
In order to derive the exact value we have to convert fk = cos(kπt/3) into f (t) =
cos(πt/3). Hence, we have
 4
0 cos(πt/3)dt = −0.82699.
The relative errors are:
1 −trapezoid
exact
= 1 −
−0.75
−0.82699 ⇒9.3 % too small,
1 −Simpson’s-1/3
exact
= 1 −
−0.833
−0.82699 ⇒0.7 % too large.
This is consistent with the fact that the Trapezoidal Rule always underestimates the
integral whereas Simpson’s 1/3-rule always overestimates (see Figs.5.14 and 5.15).

Appendix: Solutions
229
5.3 Totally Noisy
a. You get random noise, and additionally in the real part (because of the cosine!),
a discrete line at frequency (1/4)ΩNyq (see Figs.A.19 and A.20).
b. If you process the input using a simple low-pass ﬁlter (5.12), the time signal
already looks better as shown in Fig.A.21. The real part of the Fourier transform
of the ﬁltered function is shown in Fig.A.22.
Fig. A.19 Real part of the Fourier transform of the series according to (5.47)
Fig. A.20 Imaginary part of the Fourier transform of the series according to (5.47)
Fig. A.21 Input that has been processed using a low-pass ﬁlter according to (5.47)
Re{Fj}
j
↑
↑
Fig. A.22 Real part of the Fourier transform of the ﬁltered function yk according to Fig.A.21

230
Appendix: Solutions
5.4 Inclined Slope
a. We simply use a high-pass ﬁlter (cf. (5.13)). The result is shown in Fig.A.23.
b. For a “δ-shaped line” as input we get precisely the deﬁnition of the high-pass
ﬁlter as result. This leads to the following recommendation for a high-pass ﬁlter
with smaller undershoots:
yk = 1
8(−fk−2 −fk−1 + 4 fk −fk+1 −fk+2).
(A.4)
The result of this data processing is shown in Fig.A.24. If we keep going, we’ll
easily recognise Dirichlet’s integral kernel (1.53), that belongs to a step. The
problem here is that boundary effects are progressively harder to handle. Using
recursive ﬁlters, naturally, is much better suited to processing data.
1
0
−0.44
yk
k
Fig.A.23 DatafromFig.5.17processedusingthehigh-passﬁlter yk = (1/4)(−fk−1+2 fk−fk+1).
The “undershoots” don’t look very good
1
0
−0.42
yk
k
Fig. A.24 Data according to Fig.5.17, processed with the modiﬁed high-pass ﬁlter according to
(A.4). The undershoots get a bit smaller and wider. Progress admittedly is small, yet visible

Appendix: Solutions
231
Playground of Chap.6
6.1 What’s Your Average?
The transfer function of the Lagrange interpolator with N = 1 is unity at zero
frequency, hence the average does not depend on d. In case of doubt, calculate it by
“brute force”.
6.2 Late Impulse
See Figs.A.25 and A.26.
6.3 The Devil Takes the Hindmost
a. See Fig.A.27.
b. The undershoot at the leading edge is compensated at the trailing edge, as are all
other deviations from unity.
c. None of the spectral components is attenuated, i.e. nothing of the input gets lost,
the frequency-dependent group delay is responsible for the fact that it merely gets
redistributed. I am sure you have noticed that the Thiran all-pass ﬁlter can be used
to sum up certain inﬁnite series.
k
yk
−2
−1
0
−1
5
1
24
25
2
24
125
3
24
625
Fig. A.25 Response of the N = 1 Thiran all-pass ﬁlter to an impulse with d = 1/2
k
yk
−2
−1
0
1
3
1
8
9
2
−8
27
3
8
81
Fig. A.26 Response of the N = 1 Thiran all-pass ﬁlter to an impulse with d = −1/2

232
Appendix: Solutions
yk
k −2 k −1
k
k + 1
6
5
k + 2
6
25
k + 3
6
125
Fig. A.27 Response of the N = 1 Thiran all-pass ﬁlter to the trailing edge of a unit pulse for
d = 1/2
k
yk
−2
−1
0
−1
5
1
29
25
2
−121
125
3
629
625
4
−3121
3125
Fig. A.28 Response of the N = 1 Thiran all-pass ﬁlter to an input with cos ΩNyq starting at t = 0
and being 0 at earlier times for d = 1/2
6.4 Delayed Nyquist
Apart from a transient, the full amplitude of cos ΩNyq is approached rapidly. The
Lagrange N = 1 interpolator would not transmit the signal at all. See Fig.A.28.
Playground of Chap.7
7.1 Go on the ramp, not on the rampage!
a. First Shifting Rule
The coefﬁcients of the Fourier transform of the triangular function for a shift by
M are multiplied by W −j M
2M
= cos π j = −1 for j = 1, 3, . . . with the exception
of F0 = 1/2 which remains unchanged. Multiplying the result by 2M we get:
Fj =
⎧
⎪⎨
⎪⎩
−1
M sin2 π j
2M
for odd j
0
for even j ̸= 0
.

Appendix: Solutions
233
b. Tricky Carl Friedrich Gauß
The double-sided ramp is 1 minus the triangular function. Hence, all non-zero
Fourier coefﬁcients except j = 0 are the same as for the triangular function but
negative and we have to multiply by 2M to get F0 = 1 −1/2 = 1/2. The result
is identical, of course:
Fj =
⎧
⎪⎨
⎪⎩
−1
M sin2 π j
2M
for odd j
0
for even j
.
After all, the triangular function and the double-sided ramp complement each other
like man and woman.
7.2 Slice it!
P(x) =
 +∞
−∞ρ(x, y)dy.
FT(kx, ky) =
 +∞
−∞ρ(x, y)e−i(kx x+ky y)dxdy.
For the “central slice” we get:
FT(kx, 0) =
+∞

−∞
ρ(x, y)e−ikx xdxdy
=
+∞

−∞
⎛
⎝
+∞

−∞
ρ(x, y)dy
⎞
⎠e−ikx xdx
=
+∞

−∞
P(x)e−ikx xdx.
This is the 1D-Fourier transform of P(x).
7.3 Reconstruct it!
The inverse FT of double-sided ramp ﬁlter: (N = 2)
g0 = (G0 + G1) = 1
g1 = (G0 + G1eiπ) = −1
The convolution is deﬁned as follows:
hk = 1
2
1

l=0
flgk−l.

234
Appendix: Solutions
Image # 1:
1
0
0
0
y
x
Convolution:
Note, that the f ’s are the projections.
x-direction:
f0 = 1
f1 = 0
h0 = 1
2( f0g0 + f1g1) = 1
2 (1 × 1 + 0 × (−1)) = +1
2
h1 = 1
2( f0g1 + f1g0) = 1
2 (1 × (−1) + 0 × 1) = −1
2
y-direction:
f0 = 1
f1 = 0, hence, we get the same result as for the x -direction
h0 = +1
2
h1 = −1
2
convolved:
backprojected:
+ 1
2 + 1
2 ←
−1
2 −1
2 ←
x
+
y
↓
↓
+ 1
2 −1
2
+ 1
2 −1
2
=
+1
0
0
−1
The box with −1 is an reconstruction artefact. Use a cutoff: all negative values
do not correspond to an object.
Image # 2:
1
1
0
0
y
x

Appendix: Solutions
235
Convolution:
x -direction:
f0 = 2
f1 = 0
h0 = 1
2 (2 × 1 + 0 × (−1)) = +1
h1 = 1
2 (2 × (−1) + 0 × 1) = −1
y -direction:
f0 = 1
f1 = 1
h0 = 1
2 (1 × 1 + 1 × (−1)) = 0
h1 = 1
2 (1 × (−1) + 1 × 1) = 0
convolved:
backprojected:
+1 +1 ←
−1 −1 ←
x
+
y
↓
↓
0
0
0
0
=
+1 +1
−1 −1
Here, we have an interesting situation: the ﬁltered y-projection vanishes iden-
tically because a constant—don’t forget the periodic continuation—cannot pass
through a high-pass ﬁlter. In other words, a uniform object looks like no object
at all! All that matters is contrast!
Image # 3:
1
0
0
1
This “diagonal object” cannot be reconstructed. We would require projections
along the diagonals!
Image # 4:
1
1
1
0
is the “reverse” of image # 1.
Image # 5:
1
1
1
1
is like a white rabbit in snow or a black panther in the dark.

References
1. H.J. Lipkin, Beta-Decay for Pedestrians (North-Holland Publ, Amsterdam, 1962)
2. H.J. Weaver, Applications of Discrete and Continuous Fourier Analysis (A Wiley-Interscience
Publication, New York, 1983)
3. H.J. Weaver, Theory of Discrete and Continuous Fourier Analysis (Wiley, New York, 1989)
4. T. Butz, Fouriertransformation für Fußgänger (Teubner, Wiesbaden, 2003)
5. Wolfram Research, Inc., Mathematica, Version 10.1, Champaign, IL, USA
6. E. Zeidler (ed.), Oxford Users’ Guide to Mathematics (Oxford University Press, Oxford, 2004)
7. W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, Numerical Recipes, The Art of
Scientiﬁc Computing (Cambridge University Press, New York, 1989)
8. I.S. Gradshteyn, I.M. Ryzhik, Tables of Integrals, Series, and Products (Academic Press Inc,
San Diego, 1980)
9. F.J. Harris, Proc. IEEE 66, 51 (1978)
10. M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions (Dover Publications Inc,
New York, 1972)
11. A.H. Nuttall, IEEE Trans. Acoust. Speech Signal Process. ASSP-29, 84–91 (1981)
12. J.-P. Thiran, IEEE Trans. Circuit Theory CT-18, 659–664 (1971)
13. A. Fettweis, IEEE Trans. Audio Electroacoust. AU-20, 112–114 (1972)
14. V. Välimäki, Helsinki University of Technology, Faculty of Electrical Engineering, Laboratory
of Acoustics and Audio Signal Processing, Report 37, ISBN 951-22-2880-7, ISSN 0356-083X
(1995)
15. W. Gröbner, N. Hofreiter, Integraltafel, Zweiter Teil: Bestimmte Integrale (Springer, Wien,
1961)
16. W. Gröbner, N. Hofreiter, Integraltafel, Erster Teil: Unbestimmte Integrale (Springer, Wien,
1965)
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
237

Index
A
Acausal, 138, 144
Algorithm
acausal, 138, 144
causal, 144
of Cooley and Tukey, 125
Aliasing, 114
“All-pole”-ﬁlter, 165
“Angular wave number”, 175
Autocorrelation, 57, 133
discrete, 109
B
Backprojection of ﬁltered projections, 173,
175
Backward difference
ﬁrst, 148
Band-pass ﬁlter, 142
Basis functions
of cosine transformation, 116
of Fourier transformation, 4, 11
of sine transformation, 116
Bessel function, 83, 86, 91, 177
Bessel’s inequality, 23
Blackman–Harris window, 85, 91
Butterﬂy scheme, 130
C
Causal, 144
Central difference
ﬁrst, 148, 153
second, 149
Central slice theorem, 175
Convolution, 90, 181
discrete, 105
of functions, 47, 69
“Convolution sum”, 105
Convolution Theorem, 52, 133, 138
discrete, 106
inverse, 53
Cooley and Tukey
algorithm of, 125
Cosine transformation, 116, 118
basis functions of, 116
Cosine window, 77
cos2-window, 78
Cross correlation, 56, 132, 216
discrete, 108
D
Data
compression, 147
differentiation, 148
integration, 149
mirroring, 115
shifting, 146
smoothing, 139
streams, 155
DC-component, 7, 9
Decimation in time, 129
δ-function, 34
discrete, 94, 133
Difference equation, 156, 169, 170
Digitizer, 155
Dirichlet’s integral kernel, 25, 28, 121, 230
Dolph–Chebychev window, 81
E
Error-function, 51, 82
complementary, 38, 51
© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
239

240
Index
Euler’s identity, 11, 44
Exponential function
bilateral, 39
truncated, 67
unilateral, 41, 62
truncated, 64
F
Fan geometry beams, 180
Fast Fourier transformation, 125, 127
Feedback, 143
Fejer window, 76
FFT, 125
FIFO, 138, 155
Filter
“all-pole”-ﬁlter, 165
band-pass, 142
high-pass, 141, 230
low-pass, 141, 229
non-recursive, 143
notch, 142
overview, 143
ramp ﬁlter, 141, 176, 181
recursive, 143, 230
Thiran’s all-pass ﬁlter for N = 1, 165
Filter effect, 137
“Finite Impulse Response”, 165
FIR, 165
First in, ﬁrst out, 138, 155
Forward difference
ﬁrst, 148
Forward transformation, 35, 214
Fourier coefﬁcients, 5, 8, 21, 22
complex, 31
discrete, 95, 98
Fourier, J.B.J., vii
Fourier series, 1, 3, 13
complex notation, 9
Fourier slice theorem, 175, 180
Fourier transform, 133
inverse
of the “ramp”, 180
Fourier transformation
deﬁnition by Weaver, 36
discrete, 93
deﬁnition, 96
forward, 35
inverse, 35
of derivatives, 60
Fractional delays, 155
Full width at half maximum, 68, 91, 197
Function
even, 2, 31, 34, 69
mixed, 31, 69
odd, 2, 31, 34, 69
FWHM, 68, 91, 197
G
Gauss function, 38, 69
Gauss window, 82, 91
Gauss, C.F., 160
Gibbs’
overshoot, 27
phenomenon, 24
ringing, 30
undershoot, 28
“Group delay”, 156
H
Half width at half maximum, 39
Hamming window, 80
Hanning window, 78
Heaviside’s step function, 62, 165, 168
High-pass ﬁlter, 135, 141, 230
HWHM, 39
I
IIR, 165
Image reconstruction, 180
Imaginary part, 41
Impulse response, 167
“Inﬁnite Impulse Response”, 165
Interference term, 63–65, 67
Inverse transformation, 35, 214
K
Kaiser–Bessel window, 83, 91
“Kernel”, 94
Kronecker symbol, 94
L
L’Hospital’s rule, 10, 72, 122
Lagrange interpolator, 156
Lagrange-interpolation scheme, 155
Linear interpolation, 146
Linearity Theorem, 13, 42, 100
Lorentz function, 39, 69
Low-pass ﬁlter, 141, 229
N
Noise power, 133

Index
241
Normalisation factor, 103
Notch ﬁlter, 142
Nyquist frequency, 102, 107, 123
O
Orthogonal system, 6
Orthonormal system, 6
Oscillation equation, 61
Overlap, 48
Overshoot, 30
P
Parseval’s
equation, 23
theorem, 31, 58, 69, 135, 215
discrete, 109
Partial sums, 21
expression of unit step, 28
integral notation, 26
Path integral, 173
Pattern recognition, 134
Pedestal, 62, 65, 80
Fourier transform of, 65
Periodic continuation, 8, 31, 93, 181
PET, 180
Phase, 41
Phase factor, 14, 16, 18
Phase shift knob, 67
Pitfalls, 62
Polar representation, 41, 63
Positron annihilation tomography, 180
Power representation, 41, 64, 67, 71, 109,
125
“Projection”, 173
Projection-slice theorem, 175
R
Radon transform, 173, 175
Ramp ﬁlter, 141, 176, 181
Ramp response, 169
Random series, 133
Real part, 41
“Rectangular function”, 37
convolution of, 48, 104
shifted, 44
Rectangular window, 72, 91
3 dB-bandwidth, 74, 91
asymptotic behaviour, 75
central peak, 72
sidelobe suppression, 74
zeros, 72
Resolution function
instrumental, 104
“Resonance”, 41, 68
Resonance enhancement, 144
Response
impulse, 167
ramp, 169
step, 168
Riemann’s localisation theorem, 27
S
Sampling Theorem, 110, 112
Saw-tooth, 127, 128
decomposition, 128
Scaling Rule, 102
Scaling Theorem, 21
“Screen”, 173
Series
even, 94, 133
mixed, 133
odd, 93, 133
random, 133
Seume, J.G., 155
Shift
stationary, 137
Shifting Rule, 31
First, 13, 42, 100
Second, 16, 31, 44, 102
Sidelobes, 72
Signal-to-noise ratio, 58, 153
Simpson’s 1/3-rule, 150, 153
Sine integral, 72
Sine transformation, 116
basis functions of, 116
“Sinogram”, 174
Smoothing-algorithm, 140, 147
Stability of recursive algorithms, 164
Step Response, 168
T
Thiran’s all-pass ﬁlter for N = 1, 165
Tomography, 173
Transfer function, 137, 156
for band-pass ﬁlter, 142
for data-smoothing, 139
for high-pass ﬁlter, 141
for low-pass ﬁlter, 140
for notch ﬁlter, 143
Trapezoidal Rule, 150, 153
“Triangular function”, 9, 13, 14, 23
with weighting, 19

242
Index
Triangular function “Triangular function”, 7
Triangular window, 76
Triplet window, 81
Truncation error, 64, 119, 123
U
Undershoot, 30, 154
V
“Versiera”, 67
Voigt proﬁle, 69
W
Wave equation, 60, 61
Weighting, 20
of a function, 7
Weighting functions, 71
Window
rectangular, 91
Window functions, 71
Blackman–Harris window, 85
cosine window, 77
cos2-window, 78
Dolph–Chebychev window, 81
Fejer window, 76
Gauss window, 82
Hamming window, 80
Hanning window, 78
Kaiser–Bessel window, 83
overview, 86
rectangular window, 72
triangular window, 76
triplet window, 81
Windowing, 90
Wrap-around, 94
Z
Zero-padding, 119


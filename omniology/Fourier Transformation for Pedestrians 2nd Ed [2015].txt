Undergraduate Lecture Notes in Physics
TilmanÂ Butz
Fourier 
Transformation 
for Pedestrians
 Second Edition 

Undergraduate Lecture Notes in Physics

Undergraduate Lecture Notes in Physics (ULNP) publishes authoritative texts covering
topics throughout pure and applied physics. Each title in the series is suitable as a basis for
undergraduate instruction, typically containing practice problems, worked examples, chapter
summaries, and suggestions for further reading.
ULNP titles must provide at least one of the following:
â€¢ An exceptionally clear and concise treatment of a standard undergraduate subject.
â€¢ A solid undergraduate-level introduction to a graduate, advanced, or non-standard subject.
â€¢ A novel perspective or an unusual approach to teaching a subject.
ULNP especially encourages new, original, and idiosyncratic approaches to physics teaching
at the undergraduate level.
The purpose of ULNP is to provide intriguing, absorbing books that will continue to be the
readerâ€™s preferred reference throughout their academic career.
Series editors
Neil Ashby
Professor Emeritus, University of Colorado, Boulder, CO, USA
William Brantley
Professor, Furman University, Greenville, SC, USA
Michael Fowler
Professor, University of Virginia, Charlottesville, VA, USA
Morten Hjorth-Jensen
Oslo, Norway
Michael Inglis
Professor, SUNY Suffolk County Community College, Selden, NY, USA
Heinz Klose
Professor Emeritus, Humboldt University Berlin, Germany
Helmy Sherif
Professor, University of Alberta, Edmonton, AB, Canada
More information about this series at http://www.springer.com/series/8917

Tilman Butz
Fourier Transformation
for Pedestrians
Second Edition
123

Tilman Butz
Faculty of Physics and Earth Science
University of Leipzig
Leipzig
Germany
ISSN 2192-4791
ISSN 2192-4805
(electronic)
Undergraduate Lecture Notes in Physics
ISBN 978-3-319-16984-2
ISBN 978-3-319-16985-9
(eBook)
DOI 10.1007/978-3-319-16985-9
Library of Congress Control Number: 2015935732
Springer Cham Heidelberg New York Dordrecht London
Â© Springer International Publishing Switzerland 2006, 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciï¬cally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microï¬lms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciï¬c statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained
herein or for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)

To Renate, Raphaela, and Florentin

Preface
Fourier1 Transformation for Pedestrians. For pedestrians? Harry J. Lipkinâ€™s famous
â€œBeta-decay for Pedestriansâ€ [1], was an inspiration to me, so thatâ€™s why. Harryâ€™s
book explains physical problems as complicated as helicity and parity violation to
â€œpedestriansâ€ in an easy to understand way. Discrete Fourier transformation, by
contrast, only requires elementary algebra, something any student should be
familiar with. As the algorithm2 is a linear one, this should present no pitfalls and
should be as â€œeasy as pieâ€. In spite of that, stubborn prejudices prevail, as far as
Fourier transformations are concerned, viz., that information could get lost or that
you could end up trusting a hoax; anyway, who would trust something that is all
done with â€œsmoke and mirrorsâ€. The above prejudices are often caused by negative
experiences, gained through improper use of ready-made Fourier transformation
programs or hardware.
This book is for all who, being laypersonsâ€”or pedestriansâ€”are looking for a
gentle and also humorous introduction to the application of Fourier transformation,
without hitting too much theory, proofs of existence, and similar things. It is
appropriate for science students at technical colleges and universities, but also for
â€œmereâ€ computer-freaks. It is also quite adequate for students of engineering and all
practical people working with Fourier transformations. Basic knowledge of inte-
gration, however, is recommended.
If this book can help to avoid prejudices or even do away with them, writing it
has been well worthwhile. Here we show how things â€œworkâ€. Generally we discuss
the Fourier transformation in one dimension only. Chapter 1 introduces Fourier
series and, as part and parcel, important statements and theorems that will guide us
through the whole book. As is appropriate for pedestrians, we also cover all the
â€œpits and pitfallsâ€ on the way. Chapter 2 covers continuous Fourier transformations
in great detail. Window functions are dealt with in Chap. 3 in more detail, as
1Jean Baptiste Joseph Fourier (1768â€“1830), French mathematician and physicist.
2Integration and differentiation are linear operators. This is quite obvious in the discrete version
(Chap. 4) and is, of course, also valid when passing on to the continuous form.
vii

understanding them is essential to avoid the disappointment caused by false
expectations. Chapter 4 is about discrete Fourier transformations, with special
regard to the Cooleyâ€“Tukey algorithm (Fast Fourier Transform, FFT). Finally,
Chap. 5 will introduce some useful examples for the ï¬ltering effects of simple
algorithms. From the host of available material we only pick items that are relevant
to the recording and preprocessing of data, items that are often used without even
thinking about them. This book started as a manuscript for lectures at the Technical
University of Munich and at the University of Leipzig. That is why it is very much
a textbook and contains many worked examplesâ€”to be redone â€œmanuallyâ€â€”as
well as plenty of illustrations. To show that a textbook (originally) written in
German can also be amusing and humorous, was my genuine concern, because
dedication and assiduity of their own are quite inclined to stiï¬‚e creativity and
imagination. It should also be fun and boost our innate urge to play. The two books
â€œApplications of Discrete and Continuous Fourier Analysisâ€ [2] and â€œTheory of
Discrete and Continuous Fourier Analysisâ€ [3] had considerable inï¬‚uence on the
makeup and content of this book, and are to be recommended as additional reading
for those â€œkeen on theoryâ€.
This English edition is based on the third, enlarged edition in German [4]. In
contrast to this German edition, there are now problems at the end of each chapter.
They should be worked out before going to the next chapter. However, I prefer the
word â€œplaygroundâ€ because you are allowed to go straight to the solutions, com-
piled in the Appendix, should your impatience get the better of you. In case youâ€™ve
read the German original, there I apologized for using many new-German words,
such as â€œsampelnâ€ or â€œwrappenâ€; I wonâ€™t do that here, to the contrary, they come in
very handy and make the translatorâ€™s job (even) easier. Many thanks to
Mrs. U. Seibt and Mrs. K. Schandert, as well as to Dr. T. Reinert, Dr. T. Soldner
and especially to Mr. H. GÃ¶del (Dipl.-Phys.) for the hard work involved in turning a
manuscript into a book. Mr. St. Jankuhn (Dipl.-Phys.) did an excellent job in
proofreading and computer acrobatics.
Last but not least, special thanks go to the translator who managed to convert the
informal German style into an informal (â€œdownunderâ€) English style.
Recommendations, queries, and proposals for change are welcome. Have fun
while reading, playing, and learning.
Leipzig
Tilman Butz
April 2005
viii
Preface

Preface of the Translator
More than a few moons ago I read two books about Richard Feynmanâ€™s life, and
that has made a lasting impression. When Tilman Butz asked me if I could translate
his â€œFourier Transformation for Pedestriansâ€, I leapt at the chanceâ€”my way of
getting a bit more into science. During the rather mechanical process of translating
the German original, within its TEX-framework, I made sure I enjoyed the bits for
the pedestrians, mere mortals like myself. Of course I am biased, I have known the
author for many yearsâ€”after all he is my brother.
Hamilton, New Zealand
Thomas-Severin Butz
2004
ix

Preface for the Second Enlarged Edition
The second enlarged edition is based on the ï¬rst edition with a focus on applica-
tions in signal analysis and processing. In a digital world, the discrete Fourier
transformation plays a very important role. However, in order to avoid pitfalls, it is
strongly recommended to learn about Fourier series and continuous Fourier
transformation ï¬rst. Two new chapters were added to the ï¬rst edition for pedes-
trians that like to go a little further: the ï¬rst deals with data streams and fractional
delays, a topic which is important in a variety of ï¬elds ever since the development
of fast digitizers; the second gives an introduction to tomography with focus on a
common image reconstruction algorithm, the back-projection of ï¬ltered projections.
Here, we shall use the spatial coordinate x and the â€œangular wave numberâ€ k instead
of time t and angular frequency Ï‰. Both topics are intimately related to Fourier
transformation and deal with modern applications. Occasionally, series and inte-
grals are needed which are beyond elementary calculus. For those who have access
to a good library, references are given to verify the results. Those who have access
to Mathematica [5] will prefer to use this tool instead. They will miss the admiration
of human ingenuity of the pre-computer era but hopefully will admire Mathematica.
Since the focus of this book is on signal processing, important issues like proofs of
existence, convergence of inï¬nite series, permutability of integration and differ-
entiation, and integrability of functions are not addressed. Those pedestrians who
want to step a little deeper into mathematics are encouraged to do so. A large
number of typos and errors have been corrected (unfortunately you never get hold
of all) and critical comments have been taken care of. It is a pleasure to thank Mr.
St. Jankuhn (Dipl.-Phys.) for his excellent work to complete the second edition and
Mr. M. JÃ¤ger (Dipl.-Inf.) for fruitful discussions on fractional delays. Recommen-
dations, queries and proposals for change are always welcome. Have fun while
reading, playing and learning.
Leipzig
Tilman Butz
xi

Contents
1
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
Even and Odd Functions. . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2
Definition of the Fourier Series . . . . . . . . . . . . . . . . . .
3
1.1.3
Calculation of the Fourier Coefficients . . . . . . . . . . . . .
5
1.1.4
Fourier Series in Complex Notation . . . . . . . . . . . . . . .
9
1.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2.2
The First Shifting Rule
(Shifting Within the Time Domain) . . . . . . . . . . . . . . .
13
1.2.3
The Second Shifting Rule
(Shifting Within the Frequency Domain). . . . . . . . . . . .
16
1.2.4
Scaling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.3
Partial Sums, Besselâ€™s Inequality, Parsevalâ€™s Equation. . . . . . . .
21
1.4
Gibbsâ€™ Phenomenon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4.1
Dirichletâ€™s Integral Kernel. . . . . . . . . . . . . . . . . . . . . .
25
1.4.2
Integral Notation of Partial Sums . . . . . . . . . . . . . . . . .
26
1.4.3
Gibbsâ€™ Overshoot. . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2
Continuous Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . .
33
2.1
Continuous Fourier Transformation. . . . . . . . . . . . . . . . . . . . .
33
2.1.1
Even and Odd Functions. . . . . . . . . . . . . . . . . . . . . . .
34
2.1.2
The Î´-Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.1.3
Forward and Inverse Transformation. . . . . . . . . . . . . . .
35
2.1.4
Polar Representation of the Fourier Transform. . . . . . . .
41
2.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.2
The First Shifting Rule . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2.3
The Second Shifting Rule . . . . . . . . . . . . . . . . . . . . . .
44
2.2.4
Scaling Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
xiii

2.3
Convolution, Cross Correlation, Autocorrelation,
Parsevalâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.3.1
Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.3.2
Cross Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.3.3
Autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.3.4
Parsevalâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.4
Fourier Transformation of Derivatives. . . . . . . . . . . . . . . . . . .
60
2.5
Pitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.5.1
â€œTurn 1 into 3â€ . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.5.2
Truncation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3
Window Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.1
The Rectangular Window . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.1.1
Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.1.2
Intensity at the Central Peak . . . . . . . . . . . . . . . . . . . .
72
3.1.3
Sidelobe Suppression . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.1.4
3 dB-Bandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.1.5
Asymptotic Behaviour of Sidelobes . . . . . . . . . . . . . . .
75
3.2
The Triangular Window (Fejer Window). . . . . . . . . . . . . . . . .
76
3.3
The Cosine Window. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.4
The cos2-Window (Hanning) . . . . . . . . . . . . . . . . . . . . . . . . .
78
3.5
The Hamming Window. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.6
The Triplet Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.7
The Gauss Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.8
The Kaiserâ€“Bessel Window. . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.9
The Blackmanâ€“Harris Window . . . . . . . . . . . . . . . . . . . . . . .
85
3.10
Overview over Window Functions . . . . . . . . . . . . . . . . . . . . .
86
3.11
Windowing or Convolution? . . . . . . . . . . . . . . . . . . . . . . . . .
90
4
Discrete Fourier Transformation. . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.1
Discrete Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . .
93
4.1.1
Even and Odd Series and Wrap-Around . . . . . . . . . . . .
94
4.1.2
The Kronecker Symbol or the â€œDiscrete Î´-Functionâ€ . . .
94
4.1.3
Definition of the Discrete Fourier Transformation . . . . .
96
4.2
Theorems and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.2.1
Linearity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.2.2
The First Shifting Rule
(Shifting in the Time Domain). . . . . . . . . . . . . . . . . . .
101
4.2.3
The Second Shifting Rule
(Shifting in the Frequency Domain) . . . . . . . . . . . . . . .
102
4.2.4
Scaling Rule/Nyquist Frequency . . . . . . . . . . . . . . . . .
102
xiv
Contents

4.3
Convolution, Cross Correlation, Autocorrelation,
Parsevalâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.3.1
Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.3.2
Cross Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.3.3
Autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.3.4
Parsevalâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.4
The Sampling Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
4.5
Data Mirroring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
4.6
How to Get Rid of the â€œStraight-Jacketâ€ Periodic Continuation?
By Using Zero-Padding! . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
4.7
Fast Fourier Transformation (FFT) . . . . . . . . . . . . . . . . . . . . .
125
5
Filter Effect in Digital Data Processing . . . . . . . . . . . . . . . . . . . . .
137
5.1
Transfer Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.2
Low-Pass, High-Pass, Band-Pass, Notch Filter . . . . . . . . . . . . .
139
5.3
Shifting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.4
Data Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.5
Differentiation of Discrete Data . . . . . . . . . . . . . . . . . . . . . . .
148
5.6
Integration of Discrete Data. . . . . . . . . . . . . . . . . . . . . . . . . .
149
6
Data Streams and Fractional Delays . . . . . . . . . . . . . . . . . . . . . . .
155
6.1
Fractional Delays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
6.2
Non-recursive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
6.3
Stability of Recursive Algorithms. . . . . . . . . . . . . . . . . . . . . .
164
6.4
Thiranâ€™s All-Pass Filter for N Â¼ 1 . . . . . . . . . . . . . . . . . . . . .
165
6.4.1
Impulse Response . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.4.2
Step Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.4.3
Ramp Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
7
Tomography: Backprojection of Filtered Projections . . . . . . . . . . .
173
7.1
Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
7.2
Backprojection of Filtered Projections. . . . . . . . . . . . . . . . . . .
175
Appendix: Solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
Contents
xv

Introduction
One of the general tasks in science and engineering is to record measured signals
and get them to tell us their â€œsecretsâ€ (information). Here we are mainly interested
in signals varying over time. They may be periodic or aperiodic, noise or also
superpositions of components. Anyway, what we are measuring is a conglomerate
of several components, which means that effects caused by the measuring-devicesâ€™
electronics and, for example, noise, get added to the signal we are actually after.
That is why we have to take the recorded signal, ï¬lter out what is of interest to us,
and process it. In many cases we are predominantly interested in the periodic
components of the signal, or the spectral content, which consists of discrete
components. For analyses of this kind Fourier transformation is particularly well
suited.
Here are some examples:
â€¢ Analysis of the vibrations of a violin string or of a bridge,
â€¢ checking out the quality of a high-ï¬delity ampliï¬er,
â€¢ radio-frequency Fourier-transformation spectroscopy,
â€¢ optical Fourier-transformation spectroscopy,
â€¢ digital image-processing (two- and three-dimensional),
to quote only a few examples from acoustics, electronics, and optics, which also
shows that this method is not only useful for purely scientiï¬c research.
Many mathematical procedures in almost all branches of science and engi-
neering use the Fourier transformation. The method is so widely knownâ€”almost
â€œold hatâ€â€”that users often only have to push a few buttons (or use a few mouse-
clicks) to perform a Fourier transformation, or the lot even gets delivered â€œto the
doorstep, free of chargeâ€. This user-friendliness, however, often is accompanied by
the loss of all necessary knowledge. Operating errors, incorrect interpretations and
frustration result from incorrect settings or similar blunders.
This book aims to raise the level of consciousness concerning the dos and
donâ€™ts when using Fourier transformation programs. In order to understand how the
discrete Fourier transformation programs work it is necessary to discuss Fourier
xvii

series and continuous Fourier transformations ï¬rst. Experience shows that
mathematical laypersons will have to cope with two hurdles:
â€¢ differential and integral calculus and
â€¢ complex number arithmetic.
When deï¬ning3 the Fourier series and the continuous Fourier transformation, we
cannot help using integrals, as, for example, in Chap. 3 (Window Functions). The
problem cannot be avoided, but can be mitigated using integration tables. For
example the â€œOxford Usersâ€™ Guide to Mathematicsâ€ [6] will be quite helpful in this
respect. In Chaps. 4â€“7 elementary maths will be sufï¬cient to understand what is
going on. As far as complex number arithmetic is concerned, I have made sure that
in Chap. 1 all formulas are covered in detail, in plain and in complex notation, so
this chapter may even serve as a small introduction to dealing with complex
numbers.
For all those ready to rip into action using their PCs, the book â€œNumerical
Recipesâ€ [7] is especially useful. It presents, among other things, programs for
almost every purpose, and they are commented, too.
3The deï¬nitions given in this book are similar to conventions and do not lay claim to any
mathematical rigour.
xviii
Introduction

Chapter 1
Fourier Series
Abstract This chapter introduces the mapping of periodic functions in the time
domain to a Fourier series in the frequency domain with Fourier coefï¬cients. It
shows how these coefï¬cients are calculated and gives examples. It also provides a
gentle introduction to complex notation. It addresses linearity of the transformation,
discusses shifting in the time and frequency domain as well as scaling. Parsevalâ€™s
equation is derived. Gibbâ€™s phenomena of â€œringingâ€ are discussed.
In 1822, Jean Baptiste Joseph Fourier in his â€œTheorie analytique de la chaleurâ€
described heat transport processes in a circular geometry by a series of sine and
cosine functions. Here, we shall ï¬rst consider much simpler systems, e.g. a string
clamped at both ends like in a piano. When excited it will vibrate with its fundamental
frequencywiththeamplitudemaximuminthemiddlewhiletheendsareatrest.Thisis
half a period of a sine-wave. It can also vibrate with twice its fundamental frequency,
its so-called ï¬rst harmonic, with the midpoint at rest, too, and a positive (negative)
maximal amplitude at a quarter of the length of the string and a negative (positive) one
at three quarters. In fact, it could vibrate with all integer multiples of its fundamental
frequency. The possible modes are illustrated in Fig.4.13 when discussing the sine-
transformation. Should you happen to possess a piano, excite a string and feel that the
string one octave higher (this is a factor of two) also starts vibrating. It gets excited
by the sound (air pressure variations) produced by the ï¬rst string. The neighbouring
strings do not vibrate because their frequencies do not match.
In this chapter we want to be a little more general than a piano string. First, we want
to speak of an interval T , corresponding to the length of the string, and continue this
interval periodically. Secondly, we want to abandon the constraint that at the interval
boundaries the amplitude is zero. Hence, what we are dealing with are arbitrary but
periodic functions. Contrary to the clamped string, where we were dealing with half
a period and multiples thereof, we now require full periods and their multiples. Thus,
when shifting the interval we shall always have a complete period of the function. Let
us assume for a moment that the average of the function is zero. Then we may shift
the interval boundaries such that the function happens to be zero at the boundaries.
A joker might be tempted to put clamps thereâ€”and nobody would even notice it!
Now it is immediately clear that this arbitrary periodic function can be described
by a series of cosines of integer multiples of the fundamental frequency, eventually
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_1
1

2
1
Fourier Series
with a phase due to shifting. Instead of a cosine with a phase we can use cosines and
sines. Should the average of the function be non-zero, this can be taken care of by
a cosine with frequency zero. Finally, we shall consider periodic functions of time
(with the exception of Chap.7), e.g. signals from a measuring device.
Mapping of a Periodic Function f (t) to a Series of Fourier
Coefï¬cients Ck
1.1 Fourier Series
This section serves as a starter. Many readers may think it too easy; but it should be
read and taken seriously all the same. Some preliminary remarks are in order:
i. To make things easier to understand, the whole book will only be concerned with
functionsinthetimedomainandtheirFouriertransformsinthefrequencydomain.
This represents the most common application, and porting it to other pairings,
such as space/momentum, for example, is pretty straightforward indeed, as we
shall see in Chap.7.
ii. We use the angular frequency Ï‰ when we refer to the frequency domain. The unit
of the angular frequency is radians/second (or simpler sâˆ’1). Itâ€™s easily converted
to the frequency Î½ of radio-stationsâ€”for example FM 105.4 MHzâ€”using the
following equation:
Ï‰ = 2Ï€Î½.
(1.1)
The unit of Î½ is Hz, short for Hertz.
By the way, in case someone wants to do like H.J. Weaver, my much appreciated
role-model, and use different notations to avoid having the tedious factors 2Ï€ crop
up everywhere, donâ€™t buy into that. For each 2Ï€ you save somewhere, there will be
more factors of 2Ï€ somewhere else. However, there are valid reasons, as detailed for
example in â€œNumerical Recipesâ€ [7], to use t and Î½.
In this book Iâ€™ll stick to the use of t and Ï‰, cutting down on the cavalier use of 2Ï€
thatâ€™s in vogue elsewhere.
1.1.1 Even and Odd Functions
All functions are either:
f (âˆ’t) = f (t) : even
(1.2)
or:
f (âˆ’t) = âˆ’f (t) : odd
(1.3)

1.1 Fourier Series
3
t
t
t
t
t
f(t)
even
odd
mixed
even
odd
part
=
+
f(t)
f(t)
f(t)
f(t)
Fig. 1.1 Examples of even, odd and mixed functions
or a â€œmixtureâ€ of both, i.e. even and odd parts superimposed. The decomposition
gives:
feven(t) = ( f (t) + f (âˆ’t))/2
fodd(t) = ( f (t) âˆ’f (âˆ’t))/2.
See examples in Fig.1.1.
1.1.2 Deï¬nition of the Fourier Series
Fourier analysis is often also called harmonic analysis, as it uses the trigonometric
functions sineâ€”an odd functionâ€”and cosineâ€”an even functionâ€”as basis functions
that play a pivotal part in harmonic oscillations.
Similar to expanding a function into a power series, especially periodic functions
may be expanded into a series of the trigonometric functions sine and cosine.
Deï¬nition 1.1 (Fourier Series)
f (t) =
âˆ

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt)
(1.4)
with Ï‰k = 2Ï€k
T
and B0 = 0.

4
1
Fourier Series
Here T means the period of the function f (t). The amplitudes or Fourier
coefï¬cients Ak and Bk, are determined in such a wayâ€”as weâ€™ll see in a momentâ€”
that the inï¬nite series is identical with the function f (t). Equation (1.4) therefore
tells us, that any periodic function can be represented as a superposition of sine- and
cosine-functions with appropriate amplitudesâ€”with an inï¬nite number of terms, if
need beâ€”yet using only precisely determined frequencies:
Ï‰ = 0, 2Ï€
T , 4Ï€
T , 6Ï€
T , . . . .
Figure1.2 shows the basis functions for k = 0, 1, 2, 3.
Example 1.1 (â€œTrigonometric identityâ€)
f (t) = cos2 Ï‰t = 1
2 + 1
2 cos 2Ï‰t.
(1.5)
Trigonometric manipulation in (1.5) already determined the Fourier coefï¬cients
A0 and A2: A0 = 1/2, A2 = 1/2 (see Fig.1.3). As function cos2 Ï‰t is an even
function, we need no Bk. Generally speaking, practically all â€œsmoothâ€ functions
without steps (i.e. without discontinuities) and without kinks (i.e. without disconti-
nuities in their ï¬rst derivative)â€”and strictly speaking without discontinuities in all
their derivativesâ€”are limited as far as their bandwidth is concerned. This means that
a ï¬nite number of terms in the series will do for practical purposes. Often data gets
recorded using a device with limited bandwidth, which puts a limit on how quickly
f (t) can vary over time anyway.
Fig. 1.2 Basis functions of Fourier transformation: cosine (left); sine (right)

1.1 Fourier Series
5
Fig. 1.3 Decomposition of cos2 Ï‰t into the average 1/2 and an oscillation with amplitude 1/2 and
frequency 2Ï‰
1.1.3 Calculation of the Fourier Coefï¬cients
Before we dig into the calculation of the Fourier coefï¬cients, we need some tools.
In all following integrals we integrate from âˆ’T/2 to +T/2, meaning over an
interval with the period T that is symmetrical to t = 0. We could also pick any
other interval, as long as the integrand is periodic with period T and gets integrated
over a whole period. The letters n and m in the formulas below are natural numbers
0, 1, 2, . . . . Letâ€™s have a look at the following:
+T/2

âˆ’T/2
cos 2Ï€nt
T
dt =
0
for n Ì¸= 0
T
for n = 0 ,
(1.6)
+T/2

âˆ’T/2
sin 2Ï€nt
T
dt = 0
for all n.
(1.7)
This results from the fact, that the areas on the positive half-plane and the ones on
the negative one cancel out each other, provided we integrate over a whole number of
periods. Cosine integral for n = 0 requires special treatment, as it lacks oscillations
and therefore areas canâ€™t cancel out each other: there the integrand is 1, and the area
under the horizontal line is equal to the width of the interval T .
Furthermore, we need the following trigonometric identities:
cos Î± cos Î² = 1/2 [cos(Î± + Î²) + cos(Î± âˆ’Î²)],
sin Î± sin Î² = 1/2 [cos(Î± âˆ’Î²) âˆ’cos(Î± + Î²)],
sin Î± cos Î² = 1/2 [sin(Î± + Î²) + sin(Î± âˆ’Î²)].
(1.8)

6
1
Fourier Series
Using these tools weâ€™re able to show, without further ado, that the system of basis
functions consisting of:
1, cos 2Ï€t
T , sin 2Ï€t
T , cos 4Ï€t
T , sin 4Ï€t
T , . . . ,
(1.9)
is an orthogonal system.1
Put in formulas, this means:
+T/2

âˆ’T/2
cos 2Ï€nt
T
cos 2Ï€mt
T
dt =
â§
â¨
â©
0
for n Ì¸= m
T/2
for n = m Ì¸= 0
T
for n = m = 0
,
(1.10)
+T/2

âˆ’T/2
sin 2Ï€nt
T
sin 2Ï€mt
T
dt =

0
for n Ì¸= m, n = 0
and/or m = 0
T/2
for n = m Ì¸= 0
,
(1.11)
+T/2

âˆ’T/2
cos 2Ï€nt
T
sin 2Ï€mt
T
dt = 0.
(1.12)
The right-hand side of (1.10) and (1.11) show that our basis system is not an
orthonormal system, i.e. the integrals for n = m are not normalised to 1. Whatâ€™s
even worse, the special case of (1.10) for n = m = 0 is a nuisance, and will keep
bugging us again and again.
Using the above orthogonality relations, weâ€™re able to calculate the Fourier
coefï¬cients straight away. We need to multiply both sides of (1.4) with cos Ï‰kâ€²t
and integrate from âˆ’T/2 to +T/2. Due to the orthogonality, only terms with k = kâ€²
will remain; the second integral will always disappear.
This gives us:
Ak = 2
T
+T/2

âˆ’T/2
f (t) cos Ï‰ktdt
for k Ì¸= 0
(1.13)
and for our â€œspecialâ€ case:
A0 = 1
T
+T/2

âˆ’T/2
f (t)dt.
(1.14)
Please note the prefactors 2/T or 1/T , respectively, in (1.13) and (1.14). Equa-
tion (1.14) simply is the average of the function f (t). The â€œelectriciansâ€ amongst
1Similar to two vectors at right angles to each other whose dot product is 0, we call a set of basis
functions an orthogonal system if the integral over the product of two different basis functions
vanishes.

1.1 Fourier Series
7
us, who might think of f (t) as current varying over time, would call A0 the â€œDCâ€-
component (DC = direct current, as opposed to AC = alternating current). Now letâ€™s
multiply both sides of (1.4) with sin Ï‰kâ€²t and integrate from âˆ’T/2 to +T/2.
We now have:
Bk = 2
T
+T/2

âˆ’T/2
f (t) sin Ï‰ktdt
for all k.
(1.15)
Equations (1.13) and (1.15) may also be interpreted like so: by weighting
the function f (t) with cos Ï‰kâ€²t or sin Ï‰kâ€²t, respectively, we â€œpickâ€ the spectral
components from f (t), when integrating, corresponding to the even or odd com-
ponents, respectively, of the frequency Ï‰k. In the following examples weâ€™ll only
state the functions f (t) in their basic interval âˆ’T/2 â‰¤t â‰¤+T/2. They have to be
extended periodically, however, as the deï¬nition goes, beyond this basic interval.
Example 1.2 (â€œConstantâ€) See Fig.1.4 (left):
f (t) = 1
A0 = 1
â€œAverageâ€
Ak = 0
for all k Ì¸= 0
Bk = 0
for all k (as f is even).
Example 1.3 (â€œTriangular functionâ€) See Fig.1.4 (right):
f (t) =
â§
âªâ¨
âªâ©
1 + 2t
T
for âˆ’T/2 â‰¤t â‰¤0
1 âˆ’2t
T
for 0 â‰¤t â‰¤+T/2
.
Letâ€™s recall: Ï‰k = 2Ï€k
T
A0 = 1/2 (â€œAverageâ€).
t
t
f(t)
f(t)
âˆ’T
2
âˆ’T
2
+ T
2
+ T
2
Fig. 1.4 â€œConstantâ€ (left); â€œtriangular functionâ€ (right). We only show the basic intervals for
both functions

8
1
Fourier Series
For k Ì¸= 0 we get:
Ak = 2
T
â¡
â¢â£
0

âˆ’T/2

1 + 2t
T

cos 2Ï€kt
T
dt +
+T/2

0

1 âˆ’2t
T

cos 2Ï€kt
T
dt
â¤
â¥â¦
= 2
T
0

âˆ’T/2
cos 2Ï€kt
T
dt + 2
T
+T/2

0
cos 2Ï€kt
T
dt



= 0
+ 4
T 2
0

âˆ’T/2
t cos 2Ï€kt
T
dt âˆ’4
T 2
+T/2

0
t cos 2Ï€kt
T
dt
= âˆ’8
T 2
+T/2

0
t cos 2Ï€kt
T
dt.
In a last step weâ€™ll use

x cos axdx = x
a sin ax + 1
a2 cos ax which ï¬nally gives
us:
Ak = 2(1 âˆ’cos Ï€k)
Ï€2k2
(k > 0),
(1.16)
Bk = 0
(as f is even).
A few more comments on the expression for Ak are in order:
i. For all even k Ak disappears.
ii. For all odd k we get Ak = 4/(Ï€2k2).
iii. For k = 0 we better use the average A0 instead of inserting k = 0 in (1.16).
We could make things even simpler:
Ak =
â§
âªâªâªâªâ¨
âªâªâªâªâ©
1
2
for k = 0
4
Ï€2k2
for k odd
0
for k even, k Ì¸= 0
.
(1.17)
The seriesâ€™ elements decrease rapidly while k rises (to the power of two in the case
of odd k), but in principle we still have an inï¬nite series. Thatâ€™s due to the â€œpointed
roofâ€ at t = 0 and the kink (continued periodically!) at Â±T/2 in our function f (t).
In order to describe these kinks, we need an inï¬nite number of Fourier coefï¬cients.

1.1 Fourier Series
9
The following illustrations will show, that things are never as bad as they seem
to be:
Using Ï‰ = 2Ï€/T (see Fig.1.5) we get:
f (t) = 1
2 + 4
Ï€2

cos Ï‰t + 1
9 cos 3Ï‰t + 1
25 cos 5Ï‰t . . .

.
(1.18)
We want to plot the frequencies of this Fourier series. Figure1.6 shows the result,
as produced, for example, by a spectrum analyser,2 if we would use our â€œtriangular
functionâ€ f (t) as input signal.
Apart from the DC peak at Ï‰ = 0 we can also see the fundamental frequency Ï‰
and all odd â€œharmonicsâ€. We may also use this frequency-plot to get an idea about
the margins of error resulting from discarding frequencies above, say, 7Ï‰. We will
cover this in more detail later on.
1.1.4 Fourier Series in Complex Notation
Let me give you a mild warning before we dig into this chapter: in (1.4) k starts from
0, meaning that we will rule out negative frequencies in our Fourier series.
The cosine terms didnâ€™t have a problem with negative frequencies. The sign of
the cosine argument doesnâ€™t matter anyway, so we would be able to go halves, like
between brothers, for example, as far as the spectral intensity at the positive frequency
kÏ‰ was concerned: âˆ’kÏ‰ and kÏ‰ would get equal parts, as shown in Fig.1.7.
As frequency Ï‰ = 0â€”a frequency as good as any other frequency Ï‰ Ì¸= 0â€”has
no â€œbrotherâ€, it will not have to go halves. A change of sign for the sine-termsâ€™
arguments would result in a change of sign for the corresponding seriesâ€™ term. The
splitting of spectral intensity like â€œbetween brothersâ€â€”equal parts of âˆ’Ï‰k and +Ï‰k
now will have to be like â€œbetween sistersâ€: the sister for âˆ’Ï‰k also gets 50%, but
herâ€™s is minus 50%!
Instead of using (1.4) we might as well use:
f (t) =
+âˆ

k=âˆ’âˆ
(A
â€²
k cos Ï‰kt + B
â€²
k sin Ï‰kt),
(1.19)
where, of course, the following is true: A
â€²
âˆ’k = A
â€²
k, B
â€²
âˆ’k = âˆ’B
â€²
k. The formulas for
the calculation of A
â€²
k and B
â€²
k for k > 0 are identical to (1.13) and (1.15), though they
lack the extra factor 2! Equation (1.14) for A0 stays unaffected by this. This helps
us avoid to provide a special treatment for the DC-component.
2On offer by various companiesâ€”for example as a plug-in option for oscilloscopesâ€”for a tidy sum
of money.

10
1
Fourier Series
0.5
1
âˆ’T
2
+ T
2
original
0.5
1
âˆ’T
2
+ T
2
â€œ0th approximationâ€:
1
2
0.5
1
âˆ’T
2
+ T
2
1st approximation:
1
2 + 4
Ï€2 cos Ï‰t
0.5
1
âˆ’T
2
+ T
2
2nd approximation:
1
2 + 4
Ï€2
cos Ï‰t + 1
9 cos 3Ï‰t
0.5
1
âˆ’T
2
0
+ T
2
3rd approximation:
1
2 + 4
Ï€2
cos Ï‰t + 1
9 cos 3Ï‰t + 1
25 cos 5Ï‰t
Fig. 1.5 The â€œtriangular functionâ€ f (t) and consecutive approximations by a Fourier series with
more and more terms
Instead of (1.16) we could have used:
A
â€²
k = (1 âˆ’cos Ï€k)
Ï€2k2
,
(1.20)
which would also be valid for k = 0! To prove it, weâ€™ll use a â€œdirty trickâ€ or commit
a â€œvenialâ€ sin: weâ€™ll assume, for the time being, that k is a continuous variable that
may steadily decrease towards 0. Then we apply lâ€™Hospitalâ€™s rule to the expression of

1.1 Fourier Series
11
r
r
r
r
r
r
r
r
Ak
k
0
1
2
3
4
5
6
7
0.5
0
4
Ï€2
4
Ï€29
4
Ï€225
4
Ï€249
Fig. 1.6 Plot of the â€œtriangular functionâ€™sâ€ frequencies
âˆ’7
âˆ’6
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
5
6
7
Ak
k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
0.5
2
Ï€249
2
Ï€249
2
Ï€225
2
Ï€225
2
Ï€29
2
Ï€29
2
Ï€2
2
Ï€2
Fig. 1.7 Like Fig.1.6, yet with positive and negative frequencies
type â€œ0 : 0â€, stating that numerator and denominator may be differentiated separately
with respect to k until limkâ†’0 does not result in an expression of type â€œ0 : 0â€ any
more. Like so:
lim
kâ†’0
1 âˆ’cos Ï€k
Ï€2k2
= lim
kâ†’0
Ï€ sin Ï€k
2Ï€2k
= lim
kâ†’0
Ï€2 cos Ï€k
2Ï€2
= 1
2.
(1.21)
If youâ€™re no sinner, go for the â€œaverageâ€ A0 = 1/2 straight away!
Hint:InmanystandardFouriertransformationprogramsafactor2between A0 and
AkÌ¸=0 is wrong. This could be mainly due to the fact that frequencies were permitted
to be positive only for the basis functions, or positive and negativeâ€”like in (1.4).
The calculation of the average A0 is easy as pie, and therefore always recommended
as a ï¬rst test in case of a poorly documented program. As B0 = 0, according to the
deï¬nition, Bk is a bit harder to check out. Later on weâ€™ll deal with simpler checks
(for example Parsevalâ€™s theorem).
Now weâ€™re set and ready for the introduction of complex notation. In the following
weâ€™ll always assume that f (t) is a real function. Generalising this for complex f (t)
is no problem. Our most important tool is Eulerâ€™s identity:
eiÎ±t = cos Î±t + i sin Î±t.
(1.22)

12
1
Fourier Series
Here we use i as the imaginary unit that results in âˆ’1 when raised to the power
of two.
This allows us to rewrite the trigonometric functions as follows:
cos Î±t = 1
2(eiÎ±t + eâˆ’iÎ±t),
sin Î±t = 1
2i(eiÎ±t âˆ’eâˆ’iÎ±t).
(1.23)
Inserting into (1.4) gives:
f (t) = A0 +
âˆ

k=1
 Ak âˆ’iBk
2
eiÏ‰kt + Ak + iBk
2
eâˆ’iÏ‰kt

.
(1.24)
Using the short-cuts:
C0 = A0,
Ck = Ak âˆ’iBk
2
,
(1.25)
Câˆ’k = Ak + iBk
2
,
k = 1, 2, 3, . . . ,
we ï¬nally get:
f (t) =
+âˆ

k=âˆ’âˆ
CkeiÏ‰kt,
Ï‰k = 2Ï€k
T .
(1.26)
Caution: For k < 0 there will be negative frequencies. (No worries, according to
our above digression!) Pretty handy that Ck and Câˆ’k are complex conjugates to each
other (see â€œbrother and sisterâ€.) Now Ck can be formulated just as easily:
Ck = 1
T
+T/2

âˆ’T/2
f (t)eâˆ’iÏ‰ktdt
for k = 0, Â±1, Â±2, . . . .
(1.27)
Please note that there is a negative sign in the exponent. It will stay with us till
the end of this book, and, hopefully, for the rest of your life. Please also note that the
index k runs from âˆ’âˆto +âˆfor Ck whereas it runs from 0 to +âˆfor Ak and Bk.

1.2 Theorems and Rules
13
1.2 Theorems and Rules
1.2.1 Linearity Theorem
Expanding a periodic function into a Fourier series is a linear operation. This means,
that we may use the two Fourier pairs:
f (t) â†”{Ck; Ï‰k} and
g(t) â†”{Câ€²
k; Ï‰k}
to form the following linear combination:
h(t) = af (t) + bg(t) â†”{aCk + bCâ€²
k; Ï‰k}.
(1.28)
Thus we may easily determine the Fourier series of a function by splitting it into
items whose Fourier series we already know.
Example 1.4 (Lowered â€œtriangular functionâ€) The simplest example is our â€œtrian-
gular functionâ€ from Example 1.3, though this time it is symmetrical to its base line
(see Fig.1.8): we only have to subtract 1/2 from our original function. That means,
that the Fourier series remained unchanged while only the average A0 now turned
to 0.
The linearity theorem appears to be so trivial that you may accept it at face-value
even when you have â€œstrayed from the path of virtueâ€. Straying from the path of
virtue is, for example, something as elementary as squaring.
1.2.2 The First Shifting Rule (Shifting Within
the Time Domain)
Often we want to know how the Fourier series changes if we shift the function f (t)
along the time axis. This, for example, happens on a regular basis if we use a different
interval, e.g. from 0 to T , instead of the symmetrical one from âˆ’T/2 to T/2 we have
used so far. In this situation, the First Shifting Rule comes in very handy:
f(t)
T
2
t
âˆ’T
2
Fig. 1.8 â€œTriangular functionâ€ with average 0

14
1
Fourier Series
f (t) â†”{Ck; Ï‰k},
f (t âˆ’a) â†”

Ckeâˆ’iÏ‰ka; Ï‰k

.
(1.29)
Proof (First Shifting Rule)
Cnew
k
= 1
T
+T/2

âˆ’T/2
f (t âˆ’a)eâˆ’iÏ‰ktdt = 1
T
+T/2âˆ’a

âˆ’T/2âˆ’a
f (tâ€²)eâˆ’iÏ‰ktâ€²eâˆ’iÏ‰kadt
â€²
= eâˆ’iÏ‰kaCold
k . âŠ“âŠ”
We integrate over a full period, thatâ€™s why shifting the limits of the interval by a
does not make any difference.
The proof is trivial, the result of the shifting along the time axis not! The new
Fourier coefï¬cient results from the old coefï¬cient Ck by multiplying it with the phase
factor eâˆ’iÏ‰ka. As Ck generally is complex, shifting â€œshufï¬‚esâ€ real and imaginary
parts.
Without using complex notation we get:
f (t) â†”{Ak; Bk; Ï‰k},
f (t âˆ’a) â†”{Ak cos Ï‰ka âˆ’Bk sin Ï‰ka; Ak sin Ï‰ka + Bk cos Ï‰ka; Ï‰k}.
(1.30)
Two examples follow:
Example 1.5 (Quarter period shifted â€œtriangular functionâ€) â€œTriangular functionâ€
(with average = 0) (see Fig. 1.8):
f (t) =
â§
âªâªâ¨
âªâªâ©
1
2 + 2t
T for âˆ’T/2 â‰¤t â‰¤0
1
2 âˆ’2t
T for 0 < t â‰¤T/2
(1.31)
with Ck =
â§
â¨
â©
1 âˆ’cos Ï€k
Ï€2k2
=
2
Ï€2k2 for k odd
0
for k even
.
Now letâ€™s shift this function to the right by a = T/4:
fnew = fold(t âˆ’T/4).

1.2 Theorems and Rules
15
So the new coefï¬cients can be calculated as follows:
Cnew
k
= Cold
k eâˆ’iÏ€k/2
(k odd)
=
2
Ï€2k2

cos Ï€k
2 âˆ’i sin Ï€k
2

(k odd)
(1.32)
= âˆ’2i
Ï€2k2 (âˆ’1)
kâˆ’1
2
(k odd).
Itâ€™s easy to realise that Cnew
âˆ’k = âˆ’Cnew
k
.
In other words: Ak = 0.
Using iBk = Câˆ’k âˆ’Ck we ï¬nally get:
Bnew
k
=
4
Ï€2k2 (âˆ’1)
k+1
2
(k odd).
Using the above shifting we get an odd function (see Fig.1.9b).
Example 1.6 (Half period shifted â€œtriangular functionâ€) Now weâ€™ll shift the same
function to the right by a = T/2:
fnew = fold(t âˆ’T/2).
âˆ’T
2
+ T
2
âˆ’T
2
+ T
2
âˆ’T
2
+ T
2
f(t)
f(t)
f(t)
t
t
t
A
Ak
k
k
k
k
k
B
(a)
(b)
(c)
Fig. 1.9 a â€œTriangular functionâ€ (with average = 0); b right-shifted by T/4; c right-shifted by T/2

16
1
Fourier Series
The new coefï¬cients then are:
Cnew
k
= Cold
k eâˆ’iÏ€k
(k odd)
=
2
Ï€2k2 (cos Ï€k âˆ’i sin Ï€k) (k odd)
= âˆ’
2
Ï€2k2
(k odd)
(C0 = 0 stays).
(1.33)
So weâ€™ve only changed the sign. Thatâ€™s okay, as the function now is upside-down
(see Fig.1.9c).
Warning: Shifting by a = T/4 will result in alternating signs for the coefï¬cients
(Fig.1.9b). The series of Fourier coefï¬cients, that are decreasing monotonically with
k according to Fig.1.9a, looks pretty â€œfrazzledâ€ after shifting the function by a =
T/4, due to the alternating sign.
1.2.3 The Second Shifting Rule (Shifting
Within the Frequency Domain)
The First Shifting Rule showed us that shifting within the time domain leads to a
multiplication by a phase factor in the frequency domain. Reversing this statement
gives us the Second Shifting Rule:
f (t) â†”{Ck; Ï‰k},
f (t)ei 2Ï€at
T
â†”{Ckâˆ’a; Ï‰k}.
(1.34)
In other words: a multiplication of the function f (t) by the phase factor ei2Ï€at/T
results in frequency Ï‰k now being related to â€œshiftedâ€ coefï¬cient Ckâˆ’aâ€”instead of
the former coefï¬cient Ck. A comparison between (1.34) and (1.29) demonstrates
the two-sided character of the two Shifting Rules. If a is an integer, there wonâ€™t be
any problem if you simply take the coefï¬cient shifted by a. But what if a is not an
integer?
Strangely enough nothing serious will happen. Simply shifting like we did before
wonâ€™t work any more, but who is to keep us from inserting (k âˆ’a) into the expression
for old Ck, whenever k occurs.
(If itâ€™s any help to you, do commit another venial sin and temporarily consider k
to be a continuous variable.) So in the case of non-integer a we didnâ€™t really shift
Ck, but rather recalculated it using shifted k.

1.2 Theorems and Rules
17
Caution: If you have simpliï¬ed a k-dependency in the expressions for Ck, for
example:
1 âˆ’cos Ï€k =
0
for k even
2
for k odd
(as in (1.16)), youâ€™ll have trouble replacing the â€œvanishedâ€ k with (k âˆ’a). In this case
thereâ€™s only one way out: back to the expressions with all k-dependencies without
simpliï¬cation.
Before we present examples, two more ways of writing down the Second Shifting
Rule are in order:
f (t) â†”{Ak; Bk; Ï‰k} ,
f (t)e
2Ï€iat
T
â†”
1
2[Ak+a + Akâˆ’a + i(Bk+a âˆ’Bkâˆ’a)];
(1.35)
1
2[Bk+a + Bkâˆ’a + i(Akâˆ’a âˆ’Ak+a)]; Ï‰k

.
Caution: This is true for k Ì¸= 0.
Old A0 then becomes Aa/2 + iBa/2!
This is easily proved by solving (1.25) for Ak and Bk and inserting it in (1.34):
Ak = Ck + Câˆ’k,
âˆ’iBk = Ck âˆ’Câˆ’k,
(1.36)
Anew
k
= Ck + Câˆ’k = Akâˆ’a âˆ’iBkâˆ’a
2
+ Ak+a + iBk+a
2
,
âˆ’iBnew
k
= Ck âˆ’Câˆ’k = Akâˆ’a âˆ’iBkâˆ’a
2
âˆ’Ak+a + iBk+a
2
,
which leads to (1.35). We get the special treatment for A0 from:
Anew
0
= Cnew
0
= Aâˆ’a âˆ’iBâˆ’a
2
= A+a + iB+a
2
.
The formulas become a lot simpler in case f (t) is real. Then we get:
f (t) cos 2Ï€at
T
â†”
 Ak+a + Akâˆ’a
2
; Bk+a + Bkâˆ’a
2
; Ï‰k

,
(1.37)

18
1
Fourier Series
old A0 becomes Aa/2 and also:
f (t) sin 2Ï€at
T
â†”
 Bk+a âˆ’Bkâˆ’a
2
; Akâˆ’a âˆ’Ak+a
2
; Ï‰k

,
old A0 becomes Ba/2.
Example 1.7 (â€œConstantâ€)
f (t) = 1 for âˆ’T/2 â‰¤t â‰¤+T/2.
Ak = Î´k,0 (Kronecker symbol, see Sect.4.1.2) or A0 = 1, all other Ak, Bk vanish.
Of course weâ€™ve always known that f (t) is a cosine wave with frequency Ï‰ = 0 and
therefore only required the coefï¬cient for Ï‰ = 0.
Now letâ€™s multiply function f (t) by cos(2Ï€t/T ), i.e. a = 1. From (1.37) we can
see:
Anew
k
= Î´kâˆ’1,0,
i.e.
A1 = 1 (all others are 0),
or
C1 = 1/2,
Câˆ’1 = 1/2.
So we have shifted the coefï¬cient by a = 1 (to the right and to the left, and gone
halves, like â€œbetween brothersâ€).
This example demonstrates that the frequency Ï‰ = 0 is as good as any other
frequency. No kidding! If you know, for example, the Fourier series of a function
f (t) and consequently the solution for integrals of the form:
+T/2

âˆ’T/2
f (t)eâˆ’iÏ‰ktdt
then you already have, using the Second Shifting Rule, solved all integrals for f (t),
multiplied by sin(2Ï€at/T ) or cos(2Ï€at/T ). No wonder, you only had to combine
phase factor ei2Ï€at/T with phase factor eâˆ’iÏ‰kt!
Example 1.8 (â€œTriangular functionâ€ multiplied by cosine) The function:
f (t) =
â§
âªâªâ¨
âªâªâ©
1 + 2t
T
for âˆ’T/2 â‰¤t â‰¤0
1 âˆ’2t
T
for 0 â‰¤t â‰¤T/2
is to be multiplied by cos(Ï€t/T ), i.e. we shift the coefï¬cients Ck by a = 1/2 (see
Fig.1.10). The new function still is even, and therefore we only have to look after
Ak:
Anew
k
= Aold
k+a + Aold
kâˆ’a
2
.

1.2 Theorems and Rules
19
1
âˆ’T
2
T
2 t
f(t)=

1+ 2t
T
for âˆ’T/2 â‰¤t â‰¤0
1âˆ’2t
T
for 0 â‰¤t â‰¤T/2
f(t)
1
âˆ’T
2
T
2 t
g(t)=cos Ï€t
T
g(t)
1
âˆ’T
2
T
2 t
f(t)g(t)
f(t)g(t)
Fig. 1.10 â€œTriangular functionâ€ (left);

cos Ï€t
T

-function (middle); â€œtriangular functionâ€ with

cos Ï€t
T

-weighting (right)
We use (1.16) for the old Ak (and stop using the simpliï¬ed version (1.17)!):
Aold
k
= 2(1 âˆ’cos Ï€k)
Ï€2k2
.
We then get:
Anew
k
= 1
2
2(1 âˆ’cos Ï€(k + 1/2))
Ï€2(k + 1/2)2
+ 2(1 âˆ’cos Ï€(k âˆ’1/2))
Ï€2(k âˆ’1/2)2

= 1 âˆ’cos Ï€k cos(Ï€/2) + sin Ï€k sin(Ï€/2)
Ï€2(k + 1/2)2
+ 1 âˆ’cos Ï€k cos(Ï€/2) âˆ’sin Ï€k sin(Ï€/2)
Ï€2(k âˆ’1/2)2
(1.38)
=
1
Ï€2(k + 1/2)2 +
1
Ï€2(k âˆ’1/2)2
Anew
0
=
Aold
1/2
2
= 2(1 âˆ’cos(Ï€/2))
2Ï€2  1
2
2
= 4
Ï€2 .

20
1
Fourier Series
The new coefï¬cients then are:
A0 = 4
Ï€2 ,
A1 = 1
Ï€2
â›
âœâ
1
!
3
2
"2 +
1
!
1
2
"2
â
âŸâ = 4
Ï€2
1
9 + 1
1

= 4
Ï€2
10
9 ,
A2 = 1
Ï€2
â›
âœâ
1
!
5
2
"2 +
1
!
3
2
"2
â
âŸâ = 4
Ï€2
 1
25 + 1
9

= 4
Ï€2
34
225,
A3 = 1
Ï€2
â›
âœâ
1
!
7
2
"2 +
1
!
5
2
"2
â
âŸâ = 4
Ï€2
 1
49 + 1
25

= 4
Ï€2
74
1225 etc.
(1.39)
A comparison of these coefï¬cients with the ones without the

cos Ï€t
T

-weighting
shows what weâ€™ve done:
without weighting
with

cos Ï€t
T

-weighting
A0
1
2
4
Ï€2
A1
4
Ï€2
4
Ï€2
10
9
A2
0
4
Ï€2
34
225
A3
4
Ï€2
1
9
4
Ï€2
74
1225.
(1.40)
We can see the following:
i. The average A0 got somewhat smaller, as the rising and falling ï¬‚anks were
weighted with the cosine, which, except for t = 0, is less than 1.
ii. We raised coefï¬cient A1 a bit, but lowered all following odd coefï¬cients a bit,
too. This is evident straight away, if we convert:
1
(2k + 1)2 +
1
(2k âˆ’1)2 < 1
k2
to 8k4 âˆ’10k2 + 1 > 0.
This is not valid for k = 1, yet all bigger k.
iii. Now weâ€™ve been landed with even coefï¬cients, that were 0 before.
We now have twice as many terms in the series as before, though they go down
at an increased rate when k increases. The multiplication by cos(Ï€t/T ) caused the
kink at t = 0 to turn into a much more pointed spike. This should actually make for
a worsening of convergence or a slower rate of decrease of the coefï¬cients. We have,
however, rounded the kink at the interval-boundary Â±T/2, which naturally helps,
but we couldnâ€™t reasonably have predicted what exactly was going to happen.

1.2 Theorems and Rules
21
1.2.4 Scaling Theorem
Sometimes we happen to want to scale the time axis. In this case, there is no need to
re-calculate the Fourier coefï¬cients. From:
f (t) â†”{Ck; Ï‰k}
f (at) â†”{Ck; a Â· Ï‰k} .
(1.41)
Here, a must be real!
For a < 1 the time axis will be stretched and, hence, the frequency axis will be
compressed. For a > 1 the opposite is true. The proof for (1.41) is easy and follows
from (1.27). Please note that we also have to stretch or compress the interval limits
because of the requirement of periodicity. Similarly, the basis function are modiï¬ed
according to Ï‰new
k
= a Â· Ï‰old
k .
Cnew
k
= a
T
+T/2a

âˆ’T/2a
f (at)eâˆ’iÏ‰new
k
t dt = a
T
+T/2

âˆ’T/2
f (tâ€²)eâˆ’iÏ‰old
k tâ€² 1
a dtâ€² = Cold
k .
with tâ€² = at
and Ï‰new
k
t = Ï‰old
k tâ€²
Here, we have tacitly assumed a > 0. For a < 0, we would only reverse the time
axis and, hence, also the frequency axis. For the special case a = âˆ’1 we have:
f (t) â†”{Ck, Ï‰k},
f (âˆ’t) â†”{Ck; âˆ’Ï‰k}.
(1.42)
1.3 Partial Sums, Besselâ€™s Inequality, Parsevalâ€™s Equation
For practical work, inï¬nite Fourier series have to get terminated at some stage,
regardless. Therefore we only use a partial sum, say until we reach kmax = N. This
Nth partial sum then is:
SN =
N

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt).
(1.43)
Terminating the series results in the following squared error:
Î´2
N = 1
T

T
[ f (t) âˆ’SN(t)]2dt.
(1.44)

22
1
Fourier Series
The â€œT â€ below the integral symbol means integration over a full period. This
deï¬nition will become plausible in a second if we look at the discrete version:
Î´2
N = 1
N
N

i=1
( fi âˆ’si)2.
Please note that we divide by the length of the interval, to compensate for inte-
grating over the interval T . Now we know that the following is correct for the inï¬nite
series:
lim
Nâ†’âˆSN =
âˆ

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt)
(1.45)
provided the Ak and Bk happen to be the Fourier coefï¬cients. Does this also have to
be true for the Nth partial sum? Isnâ€™t there a chance the mean squared error would
get smaller, if we used other coefï¬cients instead of Fourier coefï¬cients? Thatâ€™s not
the case! To prove it, weâ€™ll now insert (1.43) and (1.44) in (1.45), leave out limNâ†’âˆ
and get:
Î´2
N = 1
T
â§
â¨
â©

T
f 2(t)dt âˆ’2

T
f (t)SN(t)dt +

T
S2
N(t)dt
â«
â¬
â­
= 1
T
â§
â¨
â©

T
f 2(t)dt
âˆ’2

T
âˆ

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt)
N

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt)dt
+

T
N

k=0
(Ak cos Ï‰kt + Bk sin Ï‰kt)
N

k=0
(Aâ€²
k cos Ï‰â€²
kt + Bâ€²
k sin Ï‰â€²
kt)dt
â«
â¬
â­
= 1
T
â§
â¨
â©

T
f 2(t)dt âˆ’2T A2
0 âˆ’2T
2
N

k=1
(A2
k + B2
k ) + T A2
0
+ T
2
N

k=1
(A2
k + B2
k )
)
= 1
T

T
f 2(t)dt âˆ’A2
0 âˆ’1
2
N

k=1
(A2
k + B2
k ).
(1.46)
Here we made use of the somewhat cumbersome orthogonality properties of
(1.10)â€“(1.12). As the A2
k and B2
k always are positive, the mean squared error will
drop monotonically as N increases.

1.3 Partial Sums, Besselâ€™s Inequality, Parsevalâ€™s Equation
23
Example 1.9 (Approximating the â€œtriangular functionâ€) The â€œTriangular functionâ€:
f (t) =
â§
âªâ¨
âªâ©
1 + 2t
T
for âˆ’T/2 â‰¤t â‰¤0
1 âˆ’2t
T
for 0 â‰¤t â‰¤T/2
(1.47)
has the mean squared â€œsignalâ€:
1
T
+T/2

âˆ’T/2
f 2(t)dt = 2
T
+T/2

0
f 2(t)dt = 2
T
+T/2

0

1 âˆ’2 t
T
2
dt = 1
3.
(1.48)
The most coarse, meaning 0th, approximation is:
S0 = 1/2, i.e.
Î´2
0 = 1/3 âˆ’1/4 = 1/12 = 0.0833 . . . .
The next approximation results in:
S1 = 1/2 + 4
Ï€2 cos Ï‰t, i.e.
Î´2
1 = 1/3 âˆ’1/4 âˆ’1/2
!
4
Ï€2
"2
= 0.0012 . . . .
For Î´2
3 we get 0.0001915 . . ., the approximation of the partial sum to the â€œtriangleâ€
quickly gets better and better.
As Î´2
N is always positive, we ï¬nally arrive from (1.46) at Besselâ€™s inequality:
1
T

T
f 2(t)dt â‰¥A2
0 + 1
2
N

k=1
(A2
k + B2
k ).
(1.49)
For the limiting case of N â†’âˆwe get Parsevalâ€™s equation:
1
T

T
f 2(t)dt = A2
0 + 1
2
âˆ

k=1
(A2
k + B2
k ).
(1.50)
Parsevalâ€™s equation may be interpreted as follows: 1/T

f 2(t)dt is the mean
squared â€œsignalâ€ within the time domain, orâ€”more colloquiallyâ€”the â€œinformation
contentâ€. Fourier series donâ€™t lose this information content: itâ€™s in the squared Fourier
coefï¬cients.

24
1
Fourier Series
The rule of thumb therefore is:
â€œThe information content isnâ€™t lost.â€
or
â€œNothing goes missing in this house.â€
Here we simply have to mention an analogy with the energy density of the elec-
tromagnetic ï¬eld: w =
1
2(E2 + B2) with Ïµ0 = Î¼0 = 1, as often is customary
in theoretical physics. The comparison has got some weak sides, as E and B have
nothing to do with even and odd components.
Parsevalâ€™s equation is very useful: you can use it to easily sum up inï¬nite series. I
think youâ€™d always have been curious how we arrive at formulas such as, for example,
âˆ

k=1
odd
1
k4 = Ï€4
96.
(1.51)
Our â€œtriangular functionâ€ (1.47) is behind it! Insert (1.48) and (1.17) in (1.50),
and youâ€™ll get:
1
3 = 1
4 + 1
2
âˆ

k=1
odd

4
Ï€2k2
2
(1.52)
or
âˆ

k=1
odd
1
k4 = 2
12 Â· Ï€4
16 = Ï€4
96.
1.4 Gibbsâ€™ Phenomenon
So far weâ€™ve only been using smooth functions as examples for f (t), orâ€”like the
much-used â€œtriangular functionâ€â€”functions with â€œa kinkâ€, thatâ€™s a discontinuity in
the ï¬rst derivative. This pointed kink made sure that we basically needed an inï¬nite
number of terms in the Fourier series. Now, what will happen if there is a step, a
discontinuity, in the function itself? This certainly wonâ€™t make the problem with the
inï¬nite number of elements any smaller. Is there any way to approximate such a step
by using the Nth partial sum, and will the mean squared error for N â†’âˆapproach
0? The answer is clearly â€œYes and Noâ€. Yes, because it apparently works, and no,
because Gibbsâ€™ phenomenon happens at the steps, an overshoot or undershoot, that
doesnâ€™t disappear for N â†’âˆ.
In order to understand this, weâ€™ll have to dig a bit wider.

1.4 Gibbsâ€™ Phenomenon
25
1.4.1 Dirichletâ€™s Integral Kernel
The following expression is called Dirichletâ€™s integral kernel:
DN(x) = sin

N + 1
2

x
2 sin x
2
= 1
2 + cos x + cos 2x + Â· Â· Â· + cos Nx.
(1.53)
The second equal sign can be proved as follows:

2 sin x
2

DN(x) = 2 sin x
2 Ã—
 1
2 + cos x + cos 2x + Â· Â· Â· + cos Nx

= sin x
2 + 2 cos x sin x
2 + 2 cos 2x sin x
2 + Â· Â· Â·
+ 2 cos Nx sin x
2
= sin

N + 1
2

x.
(1.54)
Here we have used the identity:
2 sin Î± cos Î² = sin(Î± + Î²) + sin(Î± âˆ’Î²)
with Î± = x/2 and Î² = nx,
n = 1, 2, . . . , N.
By insertion, we see that all pairs of terms cancel out each other, except for the
last one.
Figure1.11 shows a few examples for DN(x). Please note that DN(x) is periodic
in 2Ï€. This is immediately evident from the cosine notation. With x = 0 we get
DN(0) = N + 1/2, between 0 and 2Ï€ DN(x) oscillates around 0.
0.5
1.5
2.5
3.5
4.5
5.5
Ï€
2Ï€
x
DN(x) = 1
2 +
N

n=1
cos nx
D0
D0
D1
D2
D3
D4
D5
Fig. 1.11 DN(x) = 1/2 + cos x + cos 2x + Â· Â· Â· + cos Nx

26
1
Fourier Series
In the limiting case of N â†’âˆeverything averages to 0, except for x = 0 (mod-
ulo 2Ï€), thatâ€™s where DN(x) grows beyond measure. Here weâ€™ve found a notation
for the Î´-function (see Chap.2)! Please excuse even two venial sins Iâ€™ve commit-
ted here: ï¬rst, the Î´-function is a distribution (and not a function!), and second,
limNâ†’âˆDN(x) is a whole â€œcombâ€ of Î´-functions 2Ï€ apart.
1.4.2 Integral Notation of Partial Sums
We need a way to sneak up on the discontinuity, from the left and the right. Thatâ€™s
why we insert the deï¬ning equations for the Fourier coefï¬cients, (1.13)â€“(1.15), in
(1.43):
SN(t) = 1
T
+T/2

âˆ’T/2
f (x)dx
(k = 0)-term taken out
of the sum
+
N

k=1
2
T
+T/2

âˆ’T/2

f (x) cos 2Ï€kx
T
cos 2Ï€kt
T
+ f (x) sin 2Ï€kx
T
sin 2Ï€kt
T

dx
(1.55)
= 2
T
+T/2

âˆ’T/2
f (x)
*
1
2 +
N

k=1
cos 2Ï€k(x âˆ’t)
T
+
dx
= 2
T
+T/2

âˆ’T/2
f (x)DN
!
2Ï€(xâˆ’t)
T
"
dx.
Using the substitution x âˆ’t = u we get:
SN(t) = 2
T
+T/2âˆ’t

âˆ’T/2âˆ’t
f (u + t)DN
 2Ï€u
T

du.
(1.56)
As both f and D are periodic in T , we may shift the integration boundaries by t
with impunity, without changing the integral. Now we split the integration interval
from âˆ’T/2 to +T/2:

1.4 Gibbsâ€™ Phenomenon
27
SN(t) = 2
T
â§
âªâ¨
âªâ©
0

âˆ’T/2
f (u + t)DN( 2Ï€u
T )du +
+T/2

0
f (u + t)DN( 2Ï€u
T )du
â«
âªâ¬
âªâ­
(1.57)
= 2
T
+T/2

0
[ f (t âˆ’u) + f (t + u)]DN( 2Ï€u
T )du.
Here we made good use of the fact that DN is an even function (sum over cosine
terms!).
Riemannâ€™s localisation theoremâ€”which we wonâ€™t prove here in the scientiï¬c
sense, but which can be understood straight away using (1.57)â€”states that the con-
vergence behaviour of SN(t) for N â†’âˆonly depends on the immediate proximity
to t of the function:
lim
Nâ†’âˆSN(t) = S(t) = f (t+) + f (tâˆ’)
2
.
(1.58)
Here t+ and tâˆ’mean the approach to t, from above and from below. Contrary to a
continuous function with a non-differentiability (â€œkinkâ€), where limNâ†’âˆSN(t) =
f (t), (1.58) means, that in the case of a discontinuity (â€œstepâ€) at t, the partial sum
converges to a value thatâ€™s â€œhalf-wayâ€ there.
That seems to make sense.
1.4.3 Gibbsâ€™ Overshoot
Now weâ€™ll have a closer look at the unit step (see Fig.1.12):
âˆ’T
2
T
2
t
f(t)
1
2
âˆ’1
2
Fig. 1.12 Unit step

28
1
Fourier Series
f (t) =
âˆ’1/2
for âˆ’T/2 â‰¤t < 0
+1/2
for 0 â‰¤t â‰¤T/2
with periodic continuation.
(1.59)
At this stage weâ€™re only interested in the case where t > 0, and t â‰¤T/4. The
factor in (1.57) preceding to Dirichletâ€™s integral kernel is:
f (t âˆ’u) + f (t + u) =
â§
â¨
â©
1
for 0 â‰¤u < t
0
for t â‰¤u < T/2 âˆ’t
âˆ’1
for T/2 âˆ’t â‰¤u < T/2
.
(1.60)
Inserting in (1.57) results in:
SN(t) = 2
T
â§
âªâ¨
âªâ©
t
0
DN( 2Ï€u
T )du âˆ’
T/2

T/2âˆ’t
DN( 2Ï€u
T )du
â«
âªâ¬
âªâ­
=
â§
âªâ¨
âªâ©
1
Ï€
2Ï€t/T

0
DN(x)dx âˆ’
0

âˆ’2Ï€t/T
DN(x âˆ’Ï€)dx
â«
âªâ¬
âªâ­
(with x = 2Ï€u
T )
(with x = 2Ï€u
T
âˆ’Ï€).
(1.61)
Now weâ€™ll insert the expression of Dirichletâ€™s kernel as sum of cosine terms and
integrate them:
SN(t) = 1
Ï€

Ï€t
T + sin 2Ï€t
T
1
+ sin 2 2Ï€t
T
2
+ Â· Â· Â· + sin N 2Ï€t
T
N
âˆ’
*
Ï€t
T âˆ’sin 2Ï€t
T
1
+ sin 2 2Ï€t
T
2
âˆ’Â· Â· Â· + (âˆ’1)N sin N 2Ï€t
T
N
+)
(1.62)
= 2
Ï€
N

k=1
odd
1
k sin 2Ï€kt
T
.
This function is the expression of the partial sums of the unit step. In Fig.1.13 we
show some approximations.
Figure1.14 shows the 49th partial sum. As we can see, weâ€™re already getting pretty
close to the unit step, but there are overshoots and undershoots near the discontinuity.
Electro-technical engineers know this phenomenon when using ï¬lters with very steep
ï¬‚anks: the signal â€œringsâ€. We could be led to believe that the amplitude of these
overshoots and undershoots will get smaller and smaller, provided only we make
N big enough. We havenâ€™t got a chance! Comparing Fig.1.13 with Fig.1.14 should
have made us think twice. Weâ€™ll have a closer look at that, using the following
approximation: N is to be very big and t (or x in (1.61), respectively) very small, i.e.
close to 0.

1.4 Gibbsâ€™ Phenomenon
29
NN
NN
=
===
12
34
âˆ’T
2
+ T
2
Fig. 1.13 Partial sum expression of unit step
T
2
âˆ’T
2
t
Fig. 1.14 Partial sum expression of unit step for N = 49
Then we may neglect 1/2 with respect to N in the numerator of Dirichletâ€™s kernel
and replace sin(x/2) by x/2:
DN(x) â†’sin Nx
x
.
(1.63)
Therefore the partial sum for large N and close to t = 0 becomes:
SN(t) â†’1
Ï€
2Ï€Nt/T

0
sin z
z
dz
(1.64)
with z = Nx.

30
1
Fourier Series
That is the sine integral. Weâ€™ll get the extremes at dSN(t)
dt
!= 0. Differentiating
with respect to the upper integral boundary gives:
1
Ï€
2Ï€N
T
sin z
z
!= 0
(1.65)
or z = lÏ€ with l = 1, 2, 3, . . . . The ï¬rst extreme on t1 = T/(2N) is a maximum, the
second extreme at t2 = T/N is a minimum (as can easily be seen). The extremes get
closer and closer to each other for N â†’âˆ. How big is SN(t1)? Insertion in (1.64)
gives us the value of the â€œovershootâ€:
SN(t1) â†’1
Ï€
Ï€

0
sin z
z
dz = 1
2 + 0.0895.
(1.66)
Using the same method we get the value of the â€œundershootâ€:
SN(t2) â†’1
Ï€
2Ï€

0
sin z
z
dz = 1
2 âˆ’0.048.
(1.67)
I bet youâ€™ve noticed that, in the approximation of N big and t small, the value
of the overshoot or undershoot doesnâ€™t depend on N at all any more. Therefore it
doesnâ€™t make sense to make N as big as possible, the overshoots and undershoots
will settle at values of +0.0895 and âˆ’0.048 and stay there. We could still show that
the extremes decrease monotonically until t = T/4; thereafter theyâ€™ll be mirrored
and increase (cf. Fig.1.14). Now what about our mean squared error for N â†’âˆ?
The answer is simple: the mean squared error approaches 0 for N â†’âˆ, though
the overshoots and undershoots stay. Thatâ€™s the trick: as the extremes get closer and
closer to each other, the area covered by the overshoots and the undershoots with the
function f (t) = 1/2 (t > 0) approaches 0 all the same. Integration will only come
up with areas of measure 0 (Iâ€™m sure Iâ€™ve committed at least a venial sin by putting it
this way). The moral of the story: a kink in the function (non-differentiability) lands
us with an inï¬nite Fourier series, and a step (discontinuity) gives us Gibbsâ€™ â€œringingâ€
to boot. In a nutshell: avoid steps wherever itâ€™s possible!
Playground
1.1 Very Speedy
A broadcasting station transmits on 100 MHz. Calculate the angular frequency Ï‰ and
the period T for one complete oscillation. How far travels an electromagnetic pulse
(or a light pulse!) in this time? Use the vacuum velocity of light c â‰ˆ3 Ã— 108 m/s.

Playground
31
1.2 Totally Odd
Given is the function f (t) = cos(Ï€t/2) for 0 < t â‰¤1 with periodic continuation.
Plot this function. Is this function even, odd, or mixed? If it is mixed, decompose it
into even and odd components and plot them.
1.3 Absolutely True
Calculate the complex Fourier coefï¬cients Ck for f (t) = sin Ï€t for 0 â‰¤t â‰¤1 with
periodic continuation. Plot f (t) with periodic continuation. Write down the ï¬rst four
terms in the series expansion.
1.4 Rather Complex
Calculate the complex Fourier coefï¬cients Ck for f (t) = 2 sin(3Ï€t/2) cos(Ï€t/2)
for 0 â‰¤t â‰¤1 with periodic continuation. Plot f (t).
1.5 Shiftily
Shift the function f (t) = 2 sin(3Ï€t/2) cos(Ï€t/2) = sin Ï€t + sin 2Ï€t for 0 â‰¤t â‰¤1
with periodic continuation by a = âˆ’1/2 to the left and calculate the complex Fourier
coefï¬cient Ck. Plot the shifted f (t) and its decomposition into ï¬rst and second parts
and discuss the result.
1.6 Cubed
Calculate the complex Fourier coefï¬cients Ck for f (t) = cos3 2Ï€t for 0 â‰¤t â‰¤1
with periodic continuation. Plot this function. Now use (1.5) and the Second Shifting
Rule to check your result.
1.7 Tackling Inï¬nity
Derive the result for the inï¬nite series ,âˆ
k=1 1/k4 using Parsevalâ€™s theorem.
Hint: Instead of the triangular function try a parabola!
1.8 Smoothly
Given is the function f (t) = [1 âˆ’(2t)2]2 for âˆ’1/2 â‰¤t â‰¤1/2 with periodic
continuation. Use (1.63) and argue how the Fourier coefï¬cients Ck must depend on
k. Check it by calculating the Ck directly.

Chapter 2
Continuous Fourier Transformation
Abstract This chapter deals with the mapping of arbitrary functions in the time
domain to Fourier transformed functions in the frequency domain. The so-called
Î´-function is introduced and forward and inverse transformations are deï¬ned and
illustrated with examples. The polar representation of the Fourier transform is given
and shifting rules are discussed. Convolution, cross-correlation, and autocorrelation
with Parsevalâ€™s theorem are illustrated with examples. It concludes with a discussion
of pitfalls and truncation errors.
In this chapter we relax the requirement of periodicity of the function f (t). Hence,
instead of discrete Fourier coefï¬cients we end up with the continuous function F(Ï‰).
The integration interval is the entire real axis (âˆ’âˆ, +âˆ).
Mapping of an Arbitrary Function f(t) to the Fourier-Transformed
Function F(Ï‰)
2.1 Continuous Fourier Transformation
Weâ€™lllookatwhathappensatthetransitionfromaseries-toanintegral-representation:
Series:
Ck = 1
T
+T/2

âˆ’T/2
f (t)eâˆ’2Ï€ikt/T dt.
(2.1)
Now:
T â†’âˆ
Ï‰k = 2Ï€k
T
â†’
Ï‰,
discrete
continuous
lim
T â†’âˆ(TCk) â†’
+âˆ

âˆ’âˆ
f (t)eâˆ’iÏ‰tdt.
(2.2)
Before we get into the deï¬nition of the Fourier transformation, we have to do
some homework.
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_2
33

34
2
Continuous Fourier Transformation
2.1.1 Even and Odd Functions
A function is called even, if:
f (âˆ’t) = f (t).
(2.3)
A function is called odd, if:
f (âˆ’t) = âˆ’f (t).
(2.4)
Any general function may be split into an even and an odd part. Weâ€™ve heard that
before, at the beginning of Chap.1, and of course itâ€™s true whether the function f (t)
is periodic or not.
2.1.2 The Î´-Function
The Î´-function is a distribution,1 not a function. In spite of that, itâ€™s always called
Î´-function. Its value is zero anywhere except when its argument is equal to 0. In
this case it is âˆ. If you think thatâ€™s too steep or pointed for you, you may prefer a
different deï¬nition:
Î´(t) = lim
aâ†’âˆfa(t)
with fa(t) =

a for âˆ’1
2a â‰¤t â‰¤1
2a
0 else
.
(2.5)
Nowwehaveapulseforthedurationofâˆ’1/2a â‰¤t â‰¤1/2a withheighta andkeep
diminishing the width of the pulse while keeping the area unchanged (normalised to
1), viz. the height goes up while the width gets smaller. Thatâ€™s the reason why the
Î´-function often is also called impulse. At the end of the previous chapter we already
had heard about a representation of the Î´-function: Dirichletâ€™s kernel for N â†’âˆ.
If we restrict things to the basis interval âˆ’Ï€ â‰¤t â‰¤+Ï€, we get:
+Ï€

âˆ’Ï€
DN(x)dx = Ï€, independent of N,
(2.6)
1Generalised function. The theory of distributions is an important basis of modern analysis, and
impossible to understand without additional reading. A more in-depth treatment of its theory, how-
ever, is not required for the applications in this book.

2.1 Continuous Fourier Transformation
35
and thus:
1
Ï€ lim
Nâ†’âˆ
+Ï€

âˆ’Ï€
f (t)DN(t)dt = f (0).
(2.7)
In the same way, the Î´-function â€œpicksâ€ the integrand where the latterâ€™s argument
is 0 during integration (we always have to integrate over the Î´-function!):
+âˆ

âˆ’âˆ
f (t)Î´(t)dt = f (0).
(2.8)
Another representation for the Î´-function, which weâ€™ll frequently use, is:
Î´(Ï‰) = 1
2Ï€
+âˆ

âˆ’âˆ
eiÏ‰tdt.
(2.9)
Purists may multiply the integrand with a damping-factor, for example eâˆ’Î±|t|, and
then introduce limÎ±â†’0. This wonâ€™t change the fact that everything gets â€œoscillatedâ€
or averaged away for all frequencies Ï‰ Ì¸= 0 (venial sin: letâ€™s think in whole periods
for once!), whereas for Ï‰ = 0 integration will be over the integrand 1 from âˆ’âˆto
+âˆ, i.e. the result will have to be âˆ.
2.1.3 Forward and Inverse Transformation
Letâ€™s deï¬ne:
Deï¬nition 2.1 (Forward transformation)
F(Ï‰) =
+âˆ

âˆ’âˆ
f (t)eâˆ’iÏ‰tdt.
(2.10)
Deï¬nition 2.2 (Inverse transformation)
f (t) = 1
2Ï€
+âˆ

âˆ’âˆ
F(Ï‰)e+iÏ‰tdÏ‰.
(2.11)

36
2
Continuous Fourier Transformation
Caution:
i. In the case of the forward transformation, there is a minus sign in the exponent
(cf. (1.27)), in the case of the inverse transformation, this is a plus sign.
ii. In the case of the inverse transformation, 1/2Ï€ is in front of the integral, contrary
to the forward transformation.
The asymmetric aspect of the formulas has tempted many scientists to introduce
other deï¬nitions, for example to write a factor 1/âˆš(2Ï€) for forward as well as inverse
transformation. Thatâ€™s no good, as the deï¬nition of the average F(0) =
 +âˆ
âˆ’âˆf (t)dt
would be affected. Weaverâ€™s representation is correct, though not widely used:
Forward transformation:
F(Î½) =
+âˆ

âˆ’âˆ
f (t)eâˆ’2Ï€iÎ½tdt,
Inverse transformation:
f (t) =
+âˆ

âˆ’âˆ
F(Î½)e2Ï€iÎ½tdÎ½.
Weaver, as can be seen, doesnâ€™t use the angular frequency Ï‰, but rather the fre-
quency Î½. This effectively made the formulas look symmetrical, though it saddles us
with many factors 2Ï€ in the exponent. Weâ€™ll stick to the deï¬nitions (2.10) and (2.11).
We now want to demonstrate, that the inverse transformation returns us to the
original function. For the forward transformation, we often will use FT( f (t)), and
for the inverse transformation weâ€™ll use FTâˆ’1(F(Ï‰)). Weâ€™ll start with the inverse
transformation and insert:
f (t) = 1
2Ï€
+âˆ

âˆ’âˆ
F(Ï‰)eiÏ‰tdÏ‰ = 1
2Ï€
+âˆ

âˆ’âˆ
dÏ‰
+âˆ

âˆ’âˆ
f (tâ€²)eâˆ’iÏ‰tâ€²eiÏ‰tdtâ€²
= 1
2Ï€
+âˆ

âˆ’âˆ
f (tâ€²)dtâ€²
+âˆ

âˆ’âˆ
eiÏ‰(tâˆ’tâ€²)dÏ‰
interchange integration
(2.12)
=
+âˆ

âˆ’âˆ
f (tâ€²)Î´(t âˆ’tâ€²)dtâ€² = f (t) .
Q.e.d.2 Here we have used (2.8) and (2.9). For f (t) = 1 we get:
FT(Î´(t)) = 1.
(2.13)
2In Latin: â€œquod erat demonstrandumâ€, â€œwhat weâ€™ve set out to proveâ€.

2.1 Continuous Fourier Transformation
37
The impulse therefore requires all frequencies with unity amplitude for its Fourier
representation (â€œwhiteâ€ spectrum). Conversely:
FT(1) = 2Ï€Î´(Ï‰).
(2.14)
The constant 1 can be represented by a single spectral component, viz. Ï‰ = 0. No
others occur. As we have integrated from âˆ’âˆto +âˆ, naturally an Ï‰ = 0 will also
result in inï¬nity for intensity.
We realise the dual character of the forward and inverse transformations: a very
slowly varying function f (t) will have a very high spectral density for very small
frequencies; the spectral density will go down quickly and rapidly approaches 0.
Conversely, a quickly varying function f (t) will show spectral density over a very
wide frequency range: Fig.2.1 explains this once again.
Letâ€™s discuss a few examples now.
Example 2.1 (â€œRectangle, evenâ€)
f (t) =
1 for
âˆ’T/2 â‰¤t â‰¤T/2
0 else
.
F(Ï‰) = 2
T/2

0
cos Ï‰tdt = T sin(Ï‰T/2)
Ï‰T/2
.
(2.15)
The imaginary part is 0, as f (t) is even. The Fourier transformation of a rectan-
gular function therefore is of the type sin x
x . Some authors use the expression sinc(x)
Fig. 2.1 A slowly-varying function has only low-frequency spectral components (top); a rapidly-
falling function has spectral components spanning a wide range of frequencies (bottom)

38
2
Continuous Fourier Transformation
f(t)
t
âˆ’T
2
+ T
2
F(Ï‰)
Ï‰
F(Ï‰) = T sin(Ï‰T/2)
Ï‰T/2
Fig. 2.2 â€œRectangular functionâ€ and Fourier transformation of type sin x
x
for this case. What the â€œcâ€ stands for, I donâ€™t know.3 The â€œcâ€ already has been â€œused
upâ€ when deï¬ning the complementary error-function erfc(x) = 1 âˆ’erf(x). Thatâ€™s
why weâ€™d rather stick to sin x
x . These functions f (t) and F(Ï‰) are shown in Fig.2.2.
Theyâ€™ll keep us busy for quite a while.
Keen readers would have spotted the following immediately: if we made the
interval smaller and smaller, and did not ï¬x f (t) at 1 in return, but let it grow at
the same rate as T decreases (â€œso the area under the curve stays constantâ€), then in
limT â†’âˆwe would have a new representation of the Î´-function. Again, we get the
case where over- and undershoots on the one hand get closer to each other when
T gets smaller, but on the other hand, their amplitude doesnâ€™t decrease. The shape
sin x
x
will stay the same. As weâ€™re already familiar with Gibbsâ€™ phenomenon in the
case of steps, this naturally wonâ€™t surprise us any more. Contrary to the discussion
in Sect.1.4.3 we donâ€™t have a periodic continuation of f (t) beyond the integration
interval, i.e. there are two steps (one up, one down). Itâ€™s irrelevant that f (t) on average
isnâ€™t 0. Itâ€™s important that for:
Ï‰ â†’0
sin(Ï‰T/2)/(Ï‰T/2) â†’1
(use lâ€™Hospitalâ€™s rule or sin x â‰ˆx for small x).
Now, we calculate the Fourier transform of important functions. Letâ€™s start with
the Gaussian.
Example 2.2 (The normalised Gaussian) The prefactor is chosen in such a way that
the area is 1.
f (t) =
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
t2
Ïƒ2 .
F(Ï‰) =
1
Ïƒ
âˆš
2Ï€
+âˆ

âˆ’âˆ
eâˆ’1
2
t2
Ïƒ2 eâˆ’iÏ‰tdt
(2.16)
3It stands for â€œsinus cardinalisâ€, but what is â€œcardinalisâ€? Has nothing to do with the catholic church,
I guess.

2.1 Continuous Fourier Transformation
39
=
2
Ïƒ
âˆš
2Ï€
+âˆ

0
eâˆ’1
2
t2
Ïƒ2 cos Ï‰tdt
= eâˆ’1
2 Ïƒ2Ï‰2.
Again, the imaginary part is 0, as f (t) is even. The Fourier transform of a Gaussian
results in another Gaussian. Note that the Fourier transform is not normalised to area
1. The 1/2 occurring in the exponent is handy (could also have been absorbed into
Ïƒ), as the following is true for this representation:
Ïƒ =
âˆš
2 ln 2 Ã— HWHM (half width at half maximum = HWHM)
= 1.177 Ã— HWHM.
(2.17)
f (t) has Ïƒ in the exponentâ€™s denominator, F(Ï‰) in the numerator: the slimmer
f (t), the wider F(Ï‰) and vice versa (cf. Fig.2.3).
Example 2.3 (Bilateral exponential function)
f (t) = eâˆ’|t|/Ï„.
F(Ï‰) =
+âˆ

âˆ’âˆ
eâˆ’|t|/Ï„eâˆ’iÏ‰tdt = 2
+âˆ

0
eâˆ’t/Ï„ cos Ï‰tdt =
2Ï„
1 + Ï‰2Ï„2 .
(2.18)
As f (t) is even, the imaginary part is 0. The Fourier transform of the exponential
function is a Lorentzian (cf. Fig.2.4).
f(t)
t
f(t) =
1
Ïƒ
âˆš
2Ï€eâˆ’1
2
t2
Ïƒ2
1
Ïƒ
âˆš
2Ï€
F(Ï‰)
Ï‰
F(Ï‰) = eâˆ’1
2 Ïƒ2Ï‰2
1
Fig. 2.3 Gaussian and Fourier transform (=equally a Gaussian)

40
2
Continuous Fourier Transformation
1
f(t)
t
f(t) = eâˆ’|t|
Ï„
2Ï„
F(Ï‰)
Ï‰
F(Ï‰) =
2Ï„
1+Ï‰2Ï„ 2
Fig. 2.4 Bilateral exponential function and Fourier transformation (=Lorentzian)
Example 2.4 (Unilateral exponential function)
f (t) =

eâˆ’Î»t for t â‰¥0
0
else
.
(2.19)
F(Ï‰) =
âˆ

0
eâˆ’Î»teâˆ’iÏ‰tdt =
eâˆ’(Î»+iÏ‰)t
âˆ’(Î» + iÏ‰)

+âˆ
0
(2.20)
=
1
Î» + iÏ‰ =
Î»
Î»2 + Ï‰2 +
âˆ’iÏ‰
Î»2 + Ï‰2 .
(2.21)
(Sorry: When integrating in the complex plane, we really should have used the
Residue Theorem4 instead of integrating in a rather cavalier fashion. The result,
however, is correct all the same.)
F(Ï‰) is complex, as f (t) is neither even nor odd. We now can write the real and
the imaginary parts separately. The real part has a Lorentzian shape weâ€™re familiar
with by now, and the imaginary part has a dispersion shape. Often the so-called polar
representation is used, too, so weâ€™ll deal with that one in the next section.
Examples in physics: the damped oscillation that is used to describe the emission
of a particle (for example a photon, a Î³-quantum) from an excited nuclear state with a
lifetime of Ï„ (meaning, that the excited state depopulates according to eâˆ’t/Ï„), results
in a Lorentzian-shaped emission-line. Exponential relaxation processes will result
in Lorentzian-shaped spectral-lines, for example in the case of nuclear magnetic
resonance.
4The Residue Theorem is part of the theory of functions of complex variables.

2.1 Continuous Fourier Transformation
41
2.1.4 Polar Representation of the Fourier Transform
Every complex number z = a + ib can be represented in the complex plane by its
magnitude and phase Ï• (Fig.2.5):
z = a + ib =

a2 + b2 eiÏ• with tan Ï• = b/a.
This allows us to represent the Fourier transform of the â€œunilateralâ€ exponential
function as in Fig.2.6.
Alternatively to the polar representation we can also represent the real and imag-
inary parts separately (cf. Fig.2.7).
Please note that |F(Ï‰)| is no Lorentzian! If you want to â€œstickâ€ to this property,
you better represent the square of the magnitude: |F(Ï‰)|2 = 1/(Î»2 + Ï‰2) is a
Lorentzian again. This representation is often also called the power representation:
|F(Ï‰)|2 = (real part)2 + (imaginary part)2. The phase goes to 0 at the maximum of
|F(Ï‰)|, i.e. when â€œin resonanceâ€.
Warning: The representation of the magnitude as well as of the squared magnitude
does away with the linearity of the Fourier transformation!
Finally, letâ€™s try out the inverse transformation and ï¬nd out how we return to
the â€œunilateralâ€ exponential function (the Fourier transform didnâ€™t look all that
â€œunilateralâ€!).
radius
âˆš
a2 + b2
imaginary part
real part
Ï•
a
b
Fig. 2.5 Polar representation of a complex number z = a + ib
Fig. 2.6 Unilateral exponential function, magnitude of the Fourier transform and phase (imaginary
part/real part)

42
2
Continuous Fourier Transformation
Re{F(Ï‰)}
Ï‰
Im{F(Ï‰)}
Ï‰
Fig. 2.7 Real part and imaginary part of the Fourier transform of a unilateral exponential function
f (t) = 1
2Ï€
+âˆ

âˆ’âˆ
Î» âˆ’iÏ‰
Î»2 + Ï‰2 eiÏ‰tdÏ‰
= 1
2Ï€
â§
â¨
â©2Î»
+âˆ

0
cos Ï‰t
Î»2 + Ï‰2 dÏ‰ + 2
+âˆ

0
Ï‰ sin Ï‰t
Î»2 + Ï‰2 dÏ‰
â«
â¬
â­
= 1
Ï€
Ï€
2 eâˆ’|Î»t| Â± Ï€
2 eâˆ’|Î»t|
, where â€œ + â€ for t â‰¥0
â€œ âˆ’â€ for t < 0 is valid
=

eâˆ’Î»t for t â‰¥0
0
else
.
(2.22)
2.2 Theorems and Rules
2.2.1 Linearity Theorem
For completenessâ€™ sake, once again:
f (t) â†”F(Ï‰),
g(t) â†”G(Ï‰),
a Â· f (t) + b Â· g(t) â†”a Â· F(Ï‰) + b Â· G(Ï‰).
(2.23)
2.2.2 The First Shifting Rule
We already know: shifting in the time domain means modulation in the frequency
domain:
f (t) â†”F(Ï‰),
f (t âˆ’a) â†”F(Ï‰)eâˆ’iÏ‰a.
(2.24)

2.2 Theorems and Rules
43
The proof is quite simple.
Example 2.5 (â€œRectangular functionâ€)
f (t) =
 1 for T/2 â‰¤t â‰¤T/2
0 else
.
(2.25)
F(Ï‰) = T sin(Ï‰T/2)
Ï‰T/2
.
Fig. 2.8 â€œRectangular functionâ€, real part, imaginary part, magnitude of Fourier transform (left
from top to bottom); for the â€œrectangular functionâ€, shifted to the right by T/2 (right from top to
bottom)

44
2
Continuous Fourier Transformation
Now we shift the rectangle f (t) by a = T/2 â†’g(t), and then get (see Fig.2.8):
G(Ï‰) = T sin(Ï‰T/2)
Ï‰T/2
eâˆ’iÏ‰T/2
(2.26)
= T sin(Ï‰T/2)
Ï‰T/2
(cos(Ï‰T/2) âˆ’i sin(Ï‰T/2)).
The real part gets modulated with cos(Ï‰T/2). The imaginary part which before
was 0, now is unequal to 0 and â€œcomplementsâ€ the real part exactly, so |F(Ï‰)| stays
the same. Equation (2.24) contains â€œonlyâ€ a phase factor eâˆ’iÏ‰a, which is irrelevant as
far as the magnitude is concerned. As long as you only look at the power spectrum,
you may shift the function f (t) along the time-axis as much as you want: you wonâ€™t
notice any effect. In the phase of the polar representation, however, youâ€™ll see the
shift again:
tan Ï• = imaginary part
real part
= âˆ’sin(Ï‰T/2)
cos(Ï‰T/2) = âˆ’tan(Ï‰T/2)
or Ï• = âˆ’Ï‰T/2.
(2.27)
Donâ€™t worry about the phase Ï• overshooting Â±Ï€/2.
2.2.3 The Second Shifting Rule
We already know: a modulation in the time domain results in a shift in the frequency
domain:
f (t) â†”F(Ï‰),
f (t)eiÏ‰0t â†”F(Ï‰ âˆ’Ï‰0).
(2.28)
If you prefer real modulations, you may write:
FT( f (t) cos Ï‰0t) = F(Ï‰ + Ï‰0) + F(Ï‰ âˆ’Ï‰0)
2
,
(2.29)
FT( f (t) sin Ï‰0t) = i F(Ï‰ + Ï‰0) âˆ’F(Ï‰ âˆ’Ï‰0)
2
.
This follows from Eulerâ€™s identity (1.22) straight away.

2.2 Theorems and Rules
45
Fig. 2.9 Fourier transform of g(t) = cos Ï‰t in the interval âˆ’T/2 â‰¤t â‰¤T/2
Fig. 2.10 Superposition of sin x
x
sidelobes at small frequencies for negative and positive (left) and
positive frequencies only (right)

46
2
Continuous Fourier Transformation
Example 2.6 (â€œRectangular functionâ€)
f (t) =
1 for âˆ’T/2 â‰¤t â‰¤+T/2
0 else
.
F(Ï‰) = T sin(Ï‰T/2)
Ï‰T/2
(cf. (2.15))
and
g(t) = cos Ï‰0t.
(2.30)
Using h(t) = f (t) Â· g(t) and the Second Shifting Rule we get:
H(Ï‰) = T
2
sin[(Ï‰ + Ï‰0)T/2]
(Ï‰ + Ï‰0)T/2
+ sin[(Ï‰ âˆ’Ï‰0)T/2]
(Ï‰ âˆ’Ï‰0)T/2

.
(2.31)
This means: the Fourier transform of the function cos Ï‰0t within the interval
âˆ’T/2 â‰¤t â‰¤T/2 (and outside equal to 0) consists of two frequency peaks, one at
Ï‰ = âˆ’Ï‰0 and another one at Ï‰ = +Ï‰0. The amplitude naturally gets split evenly
(â€œbetween brothersâ€). If we had Ï‰0 = 0, then weâ€™d get the central peak Ï‰ = 0 once
again; increasing Ï‰0 splits this peak into two peaks, moving to the left and the right
(cf. Fig.2.9).
If you donâ€™t like negative frequencies, you may ï¬‚ip the negative half-plane, so
youâ€™ll only get one peak at Ï‰ = Ï‰0 with twice (thatâ€™s the original) intensity.
Caution: For small frequencies Ï‰0 the sidelobes of the function sin x
x
tend to â€œrub
shouldersâ€, meaning that they interfere with each other. Even ï¬‚ipping the negative
half-plane wonâ€™t help that. Figure2.10 explains the problem.
2.2.4 Scaling Theorem
Similar to (1.41) the following is true:
f (t) â†”F(Ï‰),
f (at) â†”1
|a| F
Ï‰
a

.
(2.32)

2.2 Theorems and Rules
47
Proof (Scaling) Analogously to (1.41) with the difference that here we cannot stretch
or compress the interval limits Â±âˆ:
F(Ï‰)new = 1
T
+âˆ

âˆ’âˆ
f (at)eâˆ’iÏ‰t dt
= 1
T
+âˆ

âˆ’âˆ
f (tâ€²)eâˆ’iÏ‰tâ€²/a 1
a dtâ€²
with tâ€² = at
= 1
|a| F(Ï‰)old
with Ï‰ = Ï‰old
a . âŠ“âŠ”
Here, we tacitly assumed a > 0. For a < 0 we would get a minus sign in the
prefactor; however, we would also have to interchange the integration limits and thus
get together the factor 1
|a|. This means: stretching (compressing) the time-axis results
in the compression (stretching) of the frequency-axis. For the special case a = âˆ’1
we get:
f (t)
â†’F(Ï‰),
f (âˆ’t) â†’F(âˆ’Ï‰).
(2.33)
Therefore turning around the time axis (â€œlooking into the pastâ€) results in turning
around the frequency axis. This profound secret will stay hidden to all those unable
to think in anything but positive frequencies.
2.3 Convolution, Cross Correlation, Autocorrelation,
Parsevalâ€™s Theorem
2.3.1 Convolution
The convolution of a function f (t) with another function g(t) means:
Deï¬nition 2.3 (Convolution)
f (t) âŠ—g(t) â‰¡
+âˆ

âˆ’âˆ
f (Î¾)g(t âˆ’Î¾)dÎ¾.
(2.34)

48
2
Continuous Fourier Transformation
Please note there is a minus sign in the argument of g(t). The convolution is
commutative, distributive, and associative. This means:
commutative:
f (t) âŠ—g(t) = g(t) âŠ—f (t).
Here we have to take into account the sign!
Proof (Convolution, commutative) Substituting the integration variables:
f (t) âŠ—g(t) =
+âˆ

âˆ’âˆ
f (Î¾)g(t âˆ’Î¾)dÎ¾ =
+âˆ

âˆ’âˆ
g(Î¾â€²) f (t âˆ’Î¾â€²)dÎ¾â€²
with Î¾â€² = t âˆ’Î¾. âŠ“âŠ”
Distributive:
f (t) âŠ—(g(t) + h(t)) = f (t) âŠ—g(t) + f (t) âŠ—h(t)
(Proof: Linear operation!).
Associative:
f (t) âŠ—(g(t) âŠ—h(t)) = ( f (t) âŠ—g(t)) âŠ—h(t)
(the convolution sequence doesnâ€™t matter; proof: double integral with interchange of
integration sequence).
Example 2.7 (Convolution of a â€œrectangular functionâ€ with another â€œrectangular
functionâ€) We want to convolve the â€œrectangular functionâ€ f (t) with another â€œrec-
tangular functionâ€ g(t):
f (t) =
1 for âˆ’T/2 â‰¤t â‰¤T/2
0 else
,
g(t) =
1 for 0 â‰¤t â‰¤T
0 else
.
h(t) = f (t) âŠ—g(t).
(2.35)
According to the deï¬nition in (2.34) we have to mirror g(t) (minus sign in front of
Î¾). Then we shift g(t) and calculate the overlap (cf. Fig.2.11).
We get the ï¬rst overlap for t = âˆ’T/2 and the last one for t = +3T/2 (cf.
Fig.2.12).
At the limits, where t = âˆ’T/2 and t = +3T/2, we start and ï¬nish with an
overlap of 0, the maximum overlap occurs at t = +T/2: there the two rectangles are
exactly on top of each other (or below each other?). The integral then is exactly T ;
in between the integral rises/falls at a linear rate (cf. Fig.2.13).

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
49
f (Î¾)
g(âˆ’Î¾)
f (Î¾) Â· g(âˆ’Î¾)
Î¾
Î¾
Î¾
T
+T /2
T/ 2
âˆ’T/ 2
Fig. 2.11 â€œRectangular functionâ€ f (Î¾), mirrored rectangular function g(âˆ’Î¾), overlap (from top to
bottom). The area of the overlap gives the convolution integral
f(Î¾)
g(t âˆ’Î¾)
g(t âˆ’Î¾)
g(t âˆ’Î¾)
g(t âˆ’Î¾)
g(t âˆ’Î¾)
Î¾
Î¾
Î¾
Î¾
Î¾
Î¾
+T/2
âˆ’T/2
Overlap
0%
50%
100%
50%
0%
Fig. 2.12 Illustration of the convolution process of f (t) and g(t) with t = âˆ’T/2, 0, +T/2, +T ,
+3T/2 (from top to bottom)

50
2
Continuous Fourier Transformation
âˆ’T
2
T
2
3T
2
T
t
h(t)
Fig. 2.13 Convolution h(t) = f (t) âŠ—g(t)
Please note the following: the interval, where f (t) âŠ—g(t) is unequal to 0, now is
twice as big: 2T ! If we had deï¬ned g(t) symmetrically around 0in the ï¬rst place (I
didnâ€™t want to do that, so we canâ€™t forget the mirroring!), then also f (t)âŠ—g(t) would
be symmetrical around 0. In this case we would have convolved f (t) with itself.
Now to a more useful example: letâ€™s take a pulse that looks like a â€œunilateralâ€
exponential function:
f (t) =

eâˆ’t/Ï„ for t â‰¥0
0
else
.
(2.36)
Any device that delivers pulses as a function of time, has a ï¬nite rise-/decay-time,
which for simplicityâ€™s sake weâ€™ll assume to be a Gaussian (see Fig.2.14):
g(t) =
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
t2
Ïƒ2 .
(2.37)
That is how our device would represent a Î´-functionâ€”we canâ€™t get sharper than
that. The function g(t) therefore is the deviceâ€™s resolution function, which weâ€™ll have
to use for the convolution of all signals we want to record. An example would be the
bandwidth of an oscilloscope. We then need:
S(t) = f (t) âŠ—g(t),
(2.38)
Fig. 2.14 Illustration of convolution: the Gaussian will be shifted over the unilateral exponential-
function

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
51
where S(t) is the experimental, â€œsmearedâ€ signal. Itâ€™s obvious that the rise at t = 0
will not be as steep, and the peak of the exponential function will get â€œironed outâ€.
Weâ€™ll have to take a closer look:
S(t) =
1
Ïƒ
âˆš
2Ï€
+âˆ

0
eâˆ’Î¾/Ï„eâˆ’1
2
(tâˆ’Î¾)2
Ïƒ2 dÎ¾
=
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
t2
Ïƒ2
+âˆ

0
exp

âˆ’Î¾
Ï„ + tÎ¾
Ïƒ2 âˆ’1
2Î¾2/Ïƒ2




form quadratic complement
dÎ¾
=
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
t2
Ïƒ2 e
t2
2Ïƒ2 eâˆ’t
Ï„ e
Ïƒ2
2Ï„2
+âˆ

0
e
âˆ’
1
2Ïƒ2

Î¾âˆ’

tâˆ’Ïƒ2
Ï„
2
dÎ¾
(2.39)
=
1
Ïƒ
âˆš
2Ï€
eâˆ’t
Ï„ e+ Ïƒ2
2Ï„2
+âˆ

âˆ’(tâˆ’Ïƒ2/Ï„)
eâˆ’
1
2Ïƒ2 Î¾â€²2
dÎ¾â€²
with Î¾â€² = Î¾ âˆ’

t âˆ’Ïƒ2
Ï„

= 1
2eâˆ’t
Ï„ e+ Ïƒ2
2Ï„2 erfc
 Ïƒ
âˆš
2Ï„
âˆ’
t
Ïƒ
âˆš
2

.
Here, erfc(x) = 1âˆ’erf(x) is the complementary error-function with the deï¬ning
equation:
erf(x) =
2
âˆšÏ€
x

0
eâˆ’t2dt.
(2.40)
The functions erf(x) and erfc(x) are shown in Fig.2.15.
The function erfc(x) represents a â€œsmearedâ€ step. Together with the factor 1/2
the height of the step is just 1. As the time in the argument of erfc(x) in (2.39) has a
negative sign, the step of Fig.2.15 is mirrored and also shifted by Ïƒ/
âˆš
2Ï„. Figure2.16
shows the result of the convolution of the exponential function with the Gaussian.
Fig. 2.15 The functions erf(x) and erfc(x)

52
2
Continuous Fourier Transformation
Fig. 2.16 Result of the convolution of a unilateral exponential function with a Gaussian. Exponen-
tial function without convolution (thin line)
The following properties immediately stand out:
i. the ï¬nite time resolution ensures that there also is a signal at negative times,
whereas it was 0 before convolution,
ii. the maximum is not at t = 0 any more,
iii. what canâ€™t be seen straight away, yet is easy to grasp, is the following: the center
of gravity of the exponential function, which was at t = Ï„, doesnâ€™t get shifted at
all upon convolution. An even function wonâ€™t shift the center of gravity! Have a
go and check it out!
Itâ€™s easy to remember the shape of the curve in Fig.2.16. Start out with the expo-
nential function with a â€œ90â—¦-vertical cliffâ€, and then dump â€œgravelâ€ to the left and
to the right of it (equal quantities! itâ€™s an even function!): thatâ€™s how you get the
gravel-heap for t < 0, demolish the peak and make sure thereâ€™s also a gravel-heap
for t > 0, that slowly gets thinner and thinner. Indeed, the inï¬‚uence of the step will
become less and less important if times get larger and larger, i.e.:
1
2erfc
 Ïƒ
âˆš
2Ï„
âˆ’
t
Ïƒ
âˆš
2

â†’1
for t â‰«Ïƒ2
Ï„ ,
(2.41)
and only the unchanged eâˆ’t/Ï„ will remain, however with the constant factor e+ Ïƒ2
2Ï„2 .
This factor is always >1 because we always have more â€œgravelâ€ poured downwards
than upwards.
Now we prove the extremely important Convolution Theorem:
f (t) â†”F(Ï‰),
g(t) â†”G(Ï‰),
h(t) = f (t) âŠ—g(t) â†”H(Ï‰) = F(Ï‰) Â· G(Ï‰),
(2.42)

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
53
i.e., the convolution integral becomes, through Fourier transformation, a product of
the Fourier-transformed ones.
Proof (Convolution Theorem)
H(Ï‰) =

f (Î¾)g(t âˆ’Î¾)dÎ¾ Ã— eâˆ’iÏ‰tdt
=

f (Î¾)eâˆ’iÏ‰Î¾

g(t âˆ’Î¾)eâˆ’iÏ‰(tâˆ’Î¾)dt

dÎ¾
â†‘
expanded
â†‘
(2.43)
=

f (Î¾)eâˆ’iÏ‰Î¾dÎ¾ Ã— G(Ï‰)
= F(Ï‰) Ã— G(Ï‰). âŠ“âŠ”
In the step before the last one, we substituted tâ€² = tâˆ’Î¾. The integration boundaries
Â±âˆdid not change by doing that, and G(Ï‰) does not depend on Î¾.
The inverse Convolution Theorem then is:
f (t) â†”F(Ï‰),
g(t) â†”G(Ï‰),
h(t) = f (t) Â· g(t) â†”H(Ï‰) =
1
2Ï€ F(Ï‰) âŠ—G(Ï‰).
(2.44)
Proof (Inverse Convolution Theorem)
H(Ï‰) =

f (t)g(t)eâˆ’iÏ‰tdt
=
  1
2Ï€

F(Ï‰â€²)e+iÏ‰â€²tdÏ‰â€² Ã— 1
2Ï€

G(Ï‰â€²â€²)e+iÏ‰â€²â€²tdÏ‰â€²â€²

eâˆ’iÏ‰tdt
=
1
(2Ï€)2

F(Ï‰â€²)

G(Ï‰â€²â€²)

ei(Ï‰â€²+Ï‰â€²â€²âˆ’Ï‰)tdt



=2Ï€Î´(Ï‰â€²+Ï‰â€²â€²âˆ’Ï‰)
dÏ‰â€²dÏ‰â€²â€²
= 1
2Ï€

F(Ï‰â€²)G(Ï‰ âˆ’Ï‰â€²)dÏ‰â€²
= 1
2Ï€ F(Ï‰) âŠ—G(Ï‰). âŠ“âŠ”
Caution: Contrary to the Convolution Theorem (2.42), in (2.44) there is a factor
of 1/2Ï€ in front of the convolution of the Fourier transforms.
A widely popular exercise is the de-convolution of data: the instrumentsâ€™ reso-
lution function â€œsmears outâ€ the quickly varying functions, but we naturally want
to reconstruct the data to what they would look like if the resolution function was
inï¬nitely goodâ€”provided we precisely knew the resolution function. In principle,
thatâ€™s a good ideaâ€”and thanks to the Convolution Theorem, not a problem: you

54
2
Continuous Fourier Transformation
Fourier-transform the data, divide by the Fourier-transformed resolution function
and transform it back. For practical applications it doesnâ€™t quite work that way. As
in real life, we canâ€™t transform from âˆ’âˆto +âˆ, we need low-pass ï¬lters, in order
not to get â€œswampedâ€ with oscillations resulting from cut-off errors. Therefore the
advantages of de-convolution are just as quickly lost as gained. Actually, the follow-
ing is obvious: whatever got â€œsmearedâ€ by ï¬nite resolution, canâ€™t be reconstructed
unambiguously. Imagine that a very pointed peak got eroded over millions of years,
so thereâ€™s only gravel left at its bottom. Try reconstructing the original peak from
the debris around it! The result might be impressive from an artistâ€™s point of view,
an artefact, but it hasnâ€™t got much to do with the original reality (unfortunately the
word artefact has negative connotations among scientists).
Two useful examples for the Convolution Theorem:
Example 2.8 (Gaussian frequency distribution) Letâ€™s assume we have f (t) =
cos Ï‰0t, and the frequency Ï‰0 is not precisely deï¬ned, but is Gaussian distributed:
P(Ï‰) =
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
Ï‰2
Ïƒ2 .
What weâ€™re measuring then is:
âˆ¼
f (t) =
+âˆ

âˆ’âˆ
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
Ï‰2
Ïƒ2 cos(Ï‰ âˆ’Ï‰0)tdÏ‰,
(2.45)
i.e. a convolution integral in Ï‰0. Instead of calculating this integral directly, we use
the inverse of the Convolution Theorem (2.44), thus saving work and gaining higher
enlightenment. But watch it! We have to handle the variables carefully. The time t
in (2.45) has nothing to do with the Fourier transformation we need in (2.44). And
the same is true for the integration variable Ï‰. Therefore we rather use t0 and Ï‰0 for
the variable pairs in (2.44). We identify:
F(Ï‰0) =
1
Ïƒ
âˆš
2Ï€
eâˆ’1
2
Ï‰2
0
Ïƒ2
1
2Ï€ G(Ï‰0) = cos Ï‰0t
or G(Ï‰0) = 2Ï€ cos Ï‰0t.
The inverse transformation of these functions using (2.11) gives us:
f (t0) = 1
2Ï€ eâˆ’1
2 Ïƒ2t2
0
(cf. (2.16) for the inverse problem; donâ€™t forget the factor 1/2Ï€ when doing the inverse
transformation!),

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
55
g(t0) = 2Ï€
Î´(t0 âˆ’t)
2
+ Î´(t0 + t)
2

(cf. (2.9) for the inverse problem; use the First Shifting Rule (2.24); donâ€™t forget the
factor 1/2Ï€ when doing the inverse transformation!).
Finally we get:
h(t0) = eâˆ’1
2 Ïƒ2t2
0
Î´(t0 âˆ’t)
2
+ Î´(t0 + t)
2

.
Now the only thing left is to Fourier-transform h(t0). The integration over the
Î´-function actually is fun:
âˆ¼
f (t) â‰¡H(Ï‰0) =
+âˆ

âˆ’âˆ
eâˆ’1
2 Ïƒ2t2
0
Î´(t0 âˆ’t)
2
+ Î´(t0 + t)
2

eâˆ’iÏ‰0t0dt0
= eâˆ’1
2 Ïƒ2t2 cos Ï‰0t.
Now, this was more work than weâ€™d originally thought it would be. But look at
what weâ€™ve gained in insight!
This means: the convolution of a Gaussian distribution in the frequency domain
results in exponential â€œdampingâ€ of the cosine term, where the damping happens
to be the Fourier transform of the frequency distribution. This, of course, is due to
the fact that we have chosen to use a cosine function (i.e. a basis function) for f (t).
P(Ï‰) makes sure that oscillations for Ï‰ Ì¸= Ï‰0 are slightly shifted with respect to
each other, and will more and more superimpose each other destructively in the long
run, averaging out to 0.
Example 2.9 (Lorentzian frequency distribution) Now naturally weâ€™ll know imme-
diately what a convolution with a Lorentzian distribution:
P(Ï‰) = Ïƒ
Ï€
1
Ï‰2 + Ïƒ2
(2.46)
would do:
âˆ¼
f (t) =
+âˆ

âˆ’âˆ
Ïƒ
Ï€
1
Ï‰2 + Ïƒ2 cos(Ï‰ âˆ’Ï‰0)tdÏ‰,
h(t0) = FTâˆ’1(
âˆ¼
f (t)) = eâˆ’Ïƒt0
Î´(t0 âˆ’t)
2
+ Î´(t0 + t)
2

;
(2.47)
âˆ¼
f (t) = eâˆ’Ïƒt cos Ï‰0t.

56
2
Continuous Fourier Transformation
This is a damped wave. Thatâ€™s how we would describe the electric ï¬eld of a
Lorentz-shaped spectral line, sent out by an â€œemitterâ€ with a life time of 1/Ïƒ.
These examples are of fundamental importance to physics. Whenever we probe
with plane waves, i.e. eiqx, the answer we get is the Fourier transform of the respective
distribution function of the object. A classical example is the elastic scattering of elec-
trons at nuclei. Here, the form factor F(q) is the Fourier transform of the distribution
function of the nuclear charge density Ï(x). The wave vector q is, apart from a
prefactor, identical with the momentum.
Example 2.10 (Gaussian convolved with Gaussian) We perform a convolution of a
Gaussian with Ïƒ1 with another Gaussian with Ïƒ2. As the Fourier transforms are Gaus-
sians againâ€”yet with Ïƒ2
1 and Ïƒ2
2 in the numerator of the exponentâ€”itâ€™s immediately
obvious that Ïƒ2
total = Ïƒ2
1 + Ïƒ2
2. Therefore, we get another Gaussian with geometric
addition of the widths Ïƒ1 and Ïƒ2.
2.3.2 Cross Correlation
Sometimes we want to know if a measured function f (t) has anything in common
with another measured function g(t). Cross correlation is ideally suited to that.
Deï¬nition 2.4 (Cross correlation)
h(t) =
+âˆ

âˆ’âˆ
f (Î¾)gâˆ—(t + Î¾)dÎ¾ â‰¡f (t) â‹†g(t).
(2.48)
Watch it: Here, there is a plus sign in the argument of g, therefore we donâ€™t mirror
g(t). For even functions g(t), this, however, doesnâ€™t matter.
The asterisk * means: complex conjugated. We may disregard it for real functions.
The symbol â‹†means: cross correlation, and is not to be confounded with âŠ—for
convolution. Cross correlation is associative and distributive, yet not commutative.
Thatâ€™s not only because of the complex-conjugated symbol, but mainly because of
the plus sign in the argument of g(t). Of course we want to convert the integral in
the cross correlation to a product by using Fourier transformation.
f (t) â†”F(Ï‰),
g(t) â†”G(Ï‰),
h(t) = f (t) â‹†g(t) â†”H(Ï‰) = F(Ï‰)Gâˆ—(Ï‰).
(2.49)
Proof (Fourier transform of cross correlation)

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
57
H(Ï‰) =

f (Î¾)gâˆ—(t + Î¾)dÎ¾ Â· eâˆ’iÏ‰tdt
=

f (Î¾)

gâˆ—(t + Î¾)eâˆ’iÏ‰tdt

dÎ¾
First Shifting Rule complex conjugated with Î¾ = âˆ’a
(2.50)
=

f (Î¾)Gâˆ—(+Ï‰)eâˆ’iÏ‰Î¾dÎ¾
= F(Ï‰)Gâˆ—(Ï‰). âŠ“âŠ”
Here we used the following identity:
G(Ï‰) =

g(t)eâˆ’iÏ‰tdt
(take both sides complex conjugated)
Gâˆ—(Ï‰) =

gâˆ—(t)eiÏ‰tdt
(2.51)
Gâˆ—(âˆ’Ï‰) =

gâˆ—(t)eâˆ’iÏ‰tdt
(Ï‰ to be replaced by âˆ’Ï‰).
The interpretation of (2.49) is simple: if the spectral densities of f (t) and g(t) are
a good match, i.e. have much in common, then H(Ï‰) will become large on average,
and the cross correlation h(t) will also be large, on average. Otherwise if F(Ï‰) would
be small e.g., where Gâˆ—(Ï‰) is large and vice versa, so that there is never much left for
the product H(Ï‰). Then also h(t) would be small, i.e. there is not much in common
between f (t) and g(t).
A maybe somewhat extreme example is the technique of â€œLock-in ampliï¬cationâ€,
used to â€œdig upâ€ small signals buried deeply in the noise. In this case we modulate
the measured signal with a so-called carrier frequency, detect an extremely narrow
spectral rangeâ€”provided the desired signal does have spectral components in exactly
this spectral widthâ€”and often additionally make use of phase information, too.
Anything that doesnâ€™t correlate with the carrier frequency, gets discarded, so weâ€™re
only left with the noise close to the working frequency.
2.3.3 Autocorrelation
The autocorrelation function is the cross correlation of a function f (t) with itself.
You may ask, for what purpose weâ€™d want to check for what f (t) has in common with
f (t). Autocorrelation, however, seems to attract many people in a magic manner.
We often hear the view, that a signal full of noise can be turned into something

58
2
Continuous Fourier Transformation
really good by using the autocorrelation function, i.e. the signal-to-noise ratio would
improve a lot. Donâ€™t you believe a word of it! Weâ€™ll see why shortly.
Deï¬nition 2.5 (Autocorrelation)
h(t) =

f (Î¾) f âˆ—(Î¾ + t)dÎ¾.
(2.52)
We get:
f (t) â†”F(Ï‰),
h(t) = f (t) â‹†f (t) â†”H(Ï‰) = F(Ï‰)Fâˆ—(Ï‰) = |F(Ï‰)|2.
(2.53)
We may either use the Fourier transform F(Ï‰) of a noisy function f (t) and get
angry about the noise in F(Ï‰). Or we ï¬rst form the autocorrelation function h(t)
from the function f (t) and are then happy about the Fourier transform H(Ï‰) of
function h(t). Normally, H(Ï‰) does look a lot less noisy, indeed. Instead of doing
it the roundabout way by using the autocorrelation function, we could have used
the square of the magnitude of F(Ï‰) in the ï¬rst place. We all know, that a squared
representation in the ordinate always pleases the eye, if we want to do cosmetics to
a noisy spectrum. Big spectral components will grow when squared, small ones will
get even smaller (cf. New Testament, Matthew 13:12: â€œFor to him who has will more
be given but from him who has not, even the little he has will be taken away.â€). But
isnâ€™t it rather obvious that squaring doesnâ€™t change anything to the signal-to-noise
ratio? In order to make it â€œlook goodâ€, we pay the price of losing linearity.
Then,whatisautocorrelationgoodfor?Aclassicalexamplecomesfromfemtosec-
ond measuring devices. A femtosecond is one part in a thousand trillion (US)â€”or a
thousand billion (British)â€”of a second, not a particularly long time, indeed. Today,
it is possible to produce such short laser pulses. How can we measure such short
times? Using electronic stop-watches we can reach the range of 100ps; hence, these
â€œwatchesâ€ are too slow by 5 orders of magnitude. Precision engineering does the
job! Light travels in a femtosecond a distance of about 300nm, i.e. about 1/100 of a
hair diameter. Today you can buy positioning devices with nanometer precision. The
trick: Split the laser pulse into two pulses, let them travel a slightly different optical
length using mirrors, and combine them afterwards. The detector is an â€œoptical coin-
cidenceâ€ which yields an output only if both pulses overlap. By tuning the optical
path (using the nanometer screw!) you can â€œshiftâ€ one pulse over the other, i.e. you
perform a cross correlation of the pulse with itself (for purists: with its exact copy).
The entire system is called autocorrelator.
2.3.4 Parsevalâ€™s Theorem
The autocorrelation function also comes in handy for something else, namely for
deriving Parsevalâ€™s theorem. We start out with (2.52), insert especially t = 0, and

2.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
59
get Parsevalâ€™s theorem:
h(0) =

| f (Î¾)|2dÎ¾ = 1
2Ï€

|F(Ï‰)|2dÏ‰.
(2.54)
We get the second equal sign by inverse transformation of |F(Ï‰)|2, in which for
t = 0 the factor eiÏ‰t becomes unity.
Equation (2.54) states, that the â€œinformation contentâ€ of the function f (x)â€”
deï¬ned as integral over the square of the magnitudeâ€”is just as large as the â€œinfor-
mation contentâ€ of its Fourier transform F(Ï‰)â€”deï¬ned as integral over the square
of the magnitude of F(Ï‰) divided by 2Ï€. Letâ€™s check this out straight away using an
example, namely our much-used â€œrectangular functionâ€!
Example 2.11 (â€œRectangular functionâ€)
f (t) =
1 for âˆ’T/2 â‰¤t â‰¤T/2
0 else
.
We get on the one hand:
+âˆ

âˆ’âˆ
| f (t)|2dt =
+T/2

âˆ’T/2
dt = T
and on the other hand:
F(Ï‰) = T sin(Ï‰T/2)
Ï‰T/2
, thus
1
2Ï€
+âˆ

âˆ’âˆ
|F(Ï‰)|2dÏ‰ = 2T 2
2Ï€
+âˆ

0
sin(Ï‰T/2)
Ï‰T/2
2
dÏ‰
(2.55)
= 2T 2
2Ï€
2
T
+âˆ

0
sin x
x
2
dx = T
with x = Ï‰T/2.
Itâ€™s easily understood that Parsevalâ€™s theorem contains the squared magnitudes
of both f (t) and F(Ï‰): anything unequal to 0 has information, regardless if itâ€™s
positive or negative. The power spectrum is important, the phase doesnâ€™t matter. Of
course we can use Parsevalâ€™s theorem to calculate integrals. Letâ€™s simply take the
last example for integration over
 sin x
x
2. We need an integration table for that one,
whereas integrating over 1, thatâ€™s determining the area of a square, is elementary.

60
2
Continuous Fourier Transformation
2.4 Fourier Transformation of Derivatives
When solving differential equations, we can make life easier using Fourier transfor-
mation. The derivative simply becomes a product:
f (t) â†”F(Ï‰),
f â€²(t) â†”iÏ‰F(Ï‰).
(2.56)
Proof (Fourier transformation of derivatives with respect to t) The abbreviation FT
denotes the Fourier transformation:
FT( f â€²(t)) =
+âˆ

âˆ’âˆ
f â€²(t)eâˆ’iÏ‰tdt = f (t)eâˆ’iÏ‰t
+âˆ
âˆ’âˆâˆ’(âˆ’iÏ‰)
+âˆ

âˆ’âˆ
f (t)eâˆ’iÏ‰tdt
partial integration
= iÏ‰F(Ï‰). âŠ“âŠ”
The ï¬rst term in the partial integration is discarded, as f (t) â†’0 for t â†’Â±âˆ.
Otherwise we would run into trouble with the integration, like we did at the end of
Sect.2.1.2. This game can go on:
FT
d f n(t)
dnt

= (iÏ‰)n F(Ï‰).
(2.57)
For negative n we may also use the formula for integration. We can also formulate
in a simple way the derivative of a Fourier transform F(Ï‰) with respect to the
frequency Ï‰:
dF(Ï‰)
dÏ‰
= âˆ’iFT(t f (t)).
(2.58)
Proof (Fourier transformation of derivatives with respect to Ï‰)
dF(Ï‰)
dÏ‰
=
+âˆ

âˆ’âˆ
f (t) d
dÏ‰ eâˆ’iÏ‰tdt = âˆ’i
+âˆ

âˆ’âˆ
f (t)teâˆ’iÏ‰tdt = âˆ’iFT(t f (t)). âŠ“âŠ”
Weaver [2] gives a neat example for the application of Fourier transformation:
Example 2.12 (Wave equation) The wave equation:
d2u(x, t)
dt2
= c2 d2u(x, t)
dx2
(2.59)

2.4 Fourier Transformation of Derivatives
61
can be made into an oscillation equation using Fourier transformation of the local
variable, which is much easier to solve. We assume:
U(Î¾, t) =
+âˆ

âˆ’âˆ
u(x, t)eâˆ’iÎ¾xdx.
Then we get:
FT
d2u(x, t)
dx2

= (iÎ¾)2U(Î¾, t),
(2.60)
FT
d2u(x, t)
dt2

= d2
dt2 U(Î¾, t),
and all together:
d2U(Î¾, t)
dt2
= âˆ’c2Î¾2U(Î¾, t).
The solution of this equations is:
U(Î¾, t) = P(Î¾) cos(cÎ¾t),
where P(Î¾) is the Fourier transform of the starting proï¬le p(x):
P(Î¾) = FT(p(x)) = U(Î¾, 0).
The inverse transformation gives us two proï¬les propagating to the left and to the
right:
u(x, t) = 1
2Ï€
+âˆ

âˆ’âˆ
P(Î¾) cos(cÎ¾t)eiÎ¾xdÎ¾
= 1
2Ï€
1
2
+âˆ

âˆ’âˆ
P(Î¾)

eiÎ¾(x+ct) + eiÎ¾(xâˆ’ct)
dÎ¾
(2.61)
= 1
2 p(x + ct) + 1
2 p(x âˆ’ct).
As we had no dispersion term in the wave equation, the proï¬les are conserved
(cf. Fig.2.17).

62
2
Continuous Fourier Transformation
Fig. 2.17 Two starting proï¬les p(x) propagating to the left and the right as solutions of the wave
equation
2.5 Pitfalls
2.5.1 â€œTurn 1 into 3â€
Just for fun, weâ€™ll get into magic now: letâ€™s take a unilateral exponential function:
f (t) =

eâˆ’Î»t for t â‰¥0
0
else
with F(Ï‰) =
1
Î» + iÏ‰
(2.62)
and |F(Ï‰)|2 =
1
Î»2 + Ï‰2 .
We put this function (temporarily) on a unilateral â€œpedestalâ€:
g(t) =
1 for t â‰¥0
0 else
(2.63)
with G(Ï‰) = 1
iÏ‰ .
We arrive at the Fourier transform of Heavisideâ€™s step function g(t) from the
Fourier transform for the exponential function for Î» â†’0. We therefore have: h(t) =
f (t) + g(t). Because of the linearity of the Fourier transformation:
H(Ï‰) =
1
Î» + iÏ‰ + 1
iÏ‰ =
Î»
Î»2 + Ï‰2 âˆ’
iÏ‰
Î»2 + Ï‰2 âˆ’i
Ï‰ .
(2.64)
This results in:

2.5 Pitfalls
63
|H(Ï‰)|2 =

Î»
Î»2 + Ï‰2 âˆ’
iÏ‰
Î»2 + Ï‰2 âˆ’i
Ï‰

Ã—

Î»
Î»2 + Ï‰2 +
iÏ‰
Î»2 + Ï‰2 + i
Ï‰

=
Î»2
(Î»2 + Ï‰2)2 + 1
Ï‰2 +
Ï‰2
(Î»2 + Ï‰2)2 +
2Ï‰
(Î»2 + Ï‰2)Ï‰
=
1
Î»2 + Ï‰2 + 1
Ï‰2 +
2
Î»2 + Ï‰2
=
3
Î»2 + Ï‰2 + 1
Ï‰2 .
Now we return |G(Ï‰)|2 = 1/Ï‰2, i.e. the square of the Fourier transform of the
pedestal, and have gained, compared to |F(Ï‰)|2, a factor of 3. And we only had
to temporarily â€œborrowâ€ the pedestal to achieve that?! Of course (2.64) is correct.
Returning |G(Ï‰)|2 wasnâ€™t. We borrowed the interference term we got when squaring
the magnitude, as well, and have to return it, too. This inference term amounts to just
2/(Î»2 + Ï‰2).
Now letâ€™s approach the problem somewhat more academically. Assuming we have
h(t) = f (t) + g(t) with the Fourier transforms F(Ï‰) and G(Ï‰). We now use the
polar representation:
F(Ï‰) = |F(Ï‰)|eiÏ• f
and
(2.65)
G(Ï‰) = |G(Ï‰)|eiÏ•g.
This gives us:
H(Ï‰) = |F(Ï‰)|eiÏ• f + |G(Ï‰)|eiÏ•g,
(2.66)
which is, due to the linearity of the Fourier transformation, entirely correct. However,
if we want to calculate |H(Ï‰)|2 (or the square root of it), we get:
|H(Ï‰)|2 =

|F(Ï‰)|eiÏ• f + |G(Ï‰)|eiÏ•g
 
|F(Ï‰)|eâˆ’iÏ• f + |G(Ï‰)|eâˆ’iÏ•g

(2.67)
= |F(Ï‰)|2 + |G(Ï‰)|2 + 2|F(Ï‰)| Ã— |G(Ï‰)| Ã— cos(Ï• f âˆ’Ï•g) .
If the phase difference (Ï• f âˆ’Ï•g) doesnâ€™t happen to be 90â—¦(modulo 2Ï€), the
interference term does not cancel. Donâ€™t think youâ€™re on the safe side with real Fourier
transforms. The phases are then 0, and the interference term reaches a maximum.
The following example will illustrate this:
Example 2.13 (Overlappinglines)Letâ€™staketwospectrallinesâ€”sayofshape sin x
x â€”
that approach each other. H(Ï‰) simply is a linear superposition5 of the two lines,
yet not |H(Ï‰)|2. As soon as the two lines start to overlap, there also will be an
interference term. To use a concrete example, letâ€™s take the function of (2.31) and,
5i.e. addition.

64
2
Continuous Fourier Transformation
|Htotal(Ï‰)|2
Ï‰
|H1(Ï‰)|2 + |H2(Ï‰)|2
Ï‰
Fig. 2.18 Superposition of two
 sin x
x

-functions. Power representation with interference term (left);
power representation without interference term (right)
for simplicityâ€™s sake, ï¬‚ip the negative frequency axis to the positive axis. Then we
get:
Htotal(Ï‰) = H1 + H2
= T
sin[(Ï‰ âˆ’Ï‰1)T/2]
(Ï‰ âˆ’Ï‰1)T/2
+ sin[(Ï‰ âˆ’Ï‰2)T/2]
(Ï‰ âˆ’Ï‰2)T/2

.
(2.68)
The phases are 0, as we have used two cosine functions cos Ï‰1t and cos Ï‰2t for
input. So |H(Ï‰)|2 becomes:
|Htotal(Ï‰)|2 = T 2
sin[(Ï‰ âˆ’Ï‰1)T/2]
(Ï‰ âˆ’Ï‰1)T/2
2
+
sin[(Ï‰ âˆ’Ï‰2)T/2]
(Ï‰ âˆ’Ï‰2)T/2
2
+ 2sin[(Ï‰ âˆ’Ï‰1)T/2]
(Ï‰ âˆ’Ï‰1)T/2
Ã— sin[(Ï‰ âˆ’Ï‰2)T/2]
(Ï‰ âˆ’Ï‰2)T/2

(2.69)
= T 2 
|H1(Ï‰)|2 + Hâˆ—
1 (Ï‰)H2(Ï‰)
+ H1(Ï‰)Hâˆ—
2 (Ï‰) + |H2(Ï‰)|2
.
Figure2.18 backs up the facts: for overlapping lines, the interference term makes
sure that in the power representation the lineshape is not the sum of the power
representation of the lines. Fix: Show real and imaginary parts separately. If you
want to keep the linear superposition (itâ€™s so useful), then you have to stay clear of
the squaring!
2.5.2 Truncation Error
We now want to look at what will happen, if we truncate the function f (t)
somewhereâ€”preferably where it isnâ€™t large any moreâ€”and then Fourier-transform
it. Letâ€™s take a simple example:

2.5 Pitfalls
65
Example 2.14 (Truncation error)
f (t) =

eâˆ’Î»t for 0 â‰¤t â‰¤T
0
else
.
(2.70)
The Fourier transform then is:
F(Ï‰) =
T

0
eâˆ’Î»teâˆ’iÏ‰tdt =
1
âˆ’Î» âˆ’iÏ‰ eâˆ’Î»tâˆ’iÏ‰t

T
0
= 1 âˆ’eâˆ’Î»T âˆ’iÏ‰T
Î» + iÏ‰
.
(2.71)
Compared to the untruncated exponential function, weâ€™re now saddled with the
additional term âˆ’eâˆ’Î»T eâˆ’iÏ‰T /(Î»+iÏ‰). For large values of T it isnâ€™t all that large, but
to our grief, it oscillates. Truncating the smooth Lorentzian gave us small oscillations
in return. Figure2.19 explains that (cf. Fig.2.7 without truncation).
The morale of the story: donâ€™t truncate if you donâ€™t have to, and most certainly
neither brusquely nor brutally. How it should be doneâ€”if youâ€™ve got to do itâ€”will
be explained in the next chapter.
Finally, an example how not to do it:
Example 2.15 (Exponential on pedestal) Weâ€™ll once again use our truncated expo-
nential function and put it on a pedestal, thatâ€™s only nonzero between 0 â‰¤t â‰¤T .
Assume a height of a:
f (t) =

eâˆ’Î»t for 0 â‰¤t â‰¤T
0
else
with F(Ï‰) = 1 âˆ’eâˆ’Î»T eâˆ’iÏ‰T
Î» + iÏ‰
,
g(t) =
a for 0 â‰¤t â‰¤T
0 else
with G(Ï‰) = a 1 âˆ’eâˆ’iÏ‰T
iÏ‰
.
(2.72)
Here, to calculate G(Ï‰), weâ€™ve again used F(Ï‰), with Î» = 0. |F(Ï‰)|2 weâ€™ve
already met in Fig.2.19. Re{G(Ï‰)} as well as Im{G(Ï‰)} are shown in Fig.2.20.
Finally, in Fig.2.21 |H(Ï‰)|2 is shown, decomposed into |F(Ï‰)|2, |G(Ï‰)|2 and the
interference term.
Fig. 2.19 Fourier transform of the truncated unilateral exponential function

66
2
Continuous Fourier Transformation
Fig. 2.20 Fourier transform of the pedestal
Fig. 2.21 Power representation of Fourier transform of a unilateral exponential function on a
pedestal (top left), the unilateral exponential function (top right); Power representation of the
Fourier transform of the pedestal (bottom left) and representation of the interference term (bottom
right)
For this ï¬gure we picked the function 5eâˆ’5t/T + 2 in the interval 0 â‰¤t â‰¤T .
The exponential function therefore already dropped to eâˆ’5 at truncation, the step
with a = 2 isnâ€™t all that high either. Therefore neither |F(Ï‰)|2 nor |G(Ï‰)|2 look all

2.5 Pitfalls
67
that terrible either, but |H(Ï‰)|2 does. Itâ€™s the interference termâ€™s fault. The truncated
exponential function on the pedestal is a prototypic example for â€œbotherâ€ when
doing Fourier transformations. As weâ€™ll see in Chap.3, even using window functions
would be of limited help. Thatâ€™s only theâ€”overly popularâ€”power representationâ€™s
and interference termâ€™s fault.
Fix: Subtract the pedestal before transforming. Usually weâ€™re not interested in
it anyway. For example a logarithmic representation helps, giving a straight line
for the e-function, which then becomes â€œbentâ€ and runs into the background. Use
extrapolation to determine a. It would be best to divide by the exponential, too. You
are presumably interested in (possible) small oscillations only. In case you have no
data for long times, you will run into trouble. You will also get problems if you
have a superposition of several exponentials such that you wonâ€™t get a straight line
anyhow. In such cases, I guess, you will be stumped with Fourier transformation.
Here, Laplace transformation helps which we shall not treat here.
Playground
2.1 Black Magic
The Italian mathematician Maria Gaetana Agnesiâ€”appointed in 1750 to the faculty
of the University of Bologna by the Popeâ€”constructed the following geometric
locus, called â€œversieraâ€:
i. draw a circle with radius a/2 at (0; a/2)
ii. draw a straight line parallel to the x-axis through (0; a)
iii. draw a straight line through the origin with a slope tan Î¸
iv. the geometric locus of the â€œversieraâ€ is obtained by taking the x-value from the
intersection of both straight lines while the y-value is taken from the intersection
of the inclined straight line with the circle.
a. Derive the x- and y-coordinates as a function of Î¸, i.e. in parameterised form.
b. Eliminate Î¸ using the trigonometric identity sin2 Î¸ = 1/(1+cot2 Î¸) to arrive
at y = f (x), i.e. the â€œversieraâ€.
c. Calculate the Fourier transform of the â€œversieraâ€.
2.2 The Phase Shift Knob
On the screen of a spectrometer you see a single spectral component with non-zero
patterns for the real and imaginary parts. What shift on the time axis, expressed as
a fraction of the oscillation period T , must be applied to make the imaginary part
vanish? Calculate the real part which then builds up.
2.3 Pulses
Calculate the Fourier transform of:
f (t) =
sin Ï‰0t for âˆ’T/2 â‰¤t â‰¤T/2
0
else
with Ï‰0 = n 2Ï€
T/2.

68
2
Continuous Fourier Transformation
What is |F(Ï‰0)|, i.e. at â€œresonanceâ€? Now, calculate the Fourier transform of two of
such â€œpulsesâ€, centered at Â±Î” around t = 0.
2.4 Phase-Locked Pulses
Calculate the Fourier transform of:
f (t) =

sin Ï‰0t for
âˆ’Î” âˆ’T/2 â‰¤t â‰¤âˆ’Î” + T/2
and + Î” âˆ’T/2 â‰¤t â‰¤+Î” + T/2
0
else
with Ï‰0 = n 2Ï€
T/2.
Choose Î” such that |F(Ï‰)| is as large as possible for all frequencies Ï‰! What is the
full width at half maximum (FWHM) in this case?
Hint: Note that now the rectangular pulses â€œcut outâ€ an integer number of oscil-
lations, not necessarily starting/ending at 0, but being â€œphase-lockedâ€ between left
and right â€œpulsesâ€ (Fig.2.22).
g(t)
t
T
T
Î”
Î”
g(t)
t
âˆ’Î” âˆ’T
2
âˆ’Î” + T
2
+Î” âˆ’T
2
+Î” + T
2
t
Fig. 2.22 Two pulses 2Î” apart from each other (top). Two â€œphase-lockedâ€ pulses 2Î” apart from
each other (bottom)

Playground
69
2.5 Tricky Convolution
Convolve a normalised Lorentzian with another normalised Lorentzian and calculate
its Fourier transform.
2.6 Even Trickier
Convolve a normalised Gaussian with another normalised Gaussian and calculate its
Fourier transform.
2.7 Voigt Proï¬le (for Gourmets only)
Calculate the Fourier transform of a normalised Lorentzian convolved with a nor-
malised Gaussian. For the inverse transformation you need a good integration table,
e.g. [8, No 3.953.2].
2.8 Derivable
What is the Fourier transform of:
g(t) =

teâˆ’Î»t for 0 â‰¤t
0
else
.
Is this function even, odd, or mixed?
2.9 Nothing Gets Lost
Use Parsevalâ€™s theorem to derive the following integral:
 âˆ
0
sin2 aÏ‰
Ï‰2
dÏ‰ = Ï€
2 a
with a > 0.

Chapter 3
Window Functions
Abstract Various window functions are presented in their continuous formulation:
rectangular, triangular, cosine, Hanning, Hamming, triplet, Gauss, Kaiser-Bessel,
Blackman-Harris. A focus is on sidelobe suppression versus 3dB-bandwidth. An
example is given for a test-function with a comparison of different windows. The
Kaiser-Bessel window is recommended because it provides a parameter to play with
and because of its monotonically decaying sidelobes.
How much fun you get out of Fourier transformations will depend very much on the
proper use of window or weighting functions. F.J. Harris has compiled an excellent
overview of window functions for discrete Fourier transformations [9]. Here we want
to discuss window functions for the case of a continuous Fourier transformation.
Porting this to the case of a discrete Fourier transformation then wonâ€™t be a problem
any more.
In Chap.1 we learnt that we better stay away from transforming steps. But thatâ€™s
exactly what weâ€™re doing if the input signal is available for a ï¬nite time window
only. Without fully realising what we were doing, weâ€™ve already used the so-called
rectangular window (=no weighting) on more than a few occasions. Weâ€™ll discuss
this window in more detail shortly.
Then weâ€™ll get into window functions where information is â€œswitched on and offâ€
softly. Iâ€™ll promise right now that this can be good fun.
All window functions are, of course, even functions. The Fourier transforms of
the window function therefore donâ€™t have an imaginary part. We require a large
dynamic range so we can better compare window qualities. Thatâ€™s why weâ€™ll use
logarithmic representations covering equal ranges. And thatâ€™s also the reason why
we canâ€™t have negative function values. To make sure they donâ€™t occur, weâ€™ll use the
power representation, i.e. |F(Ï‰)|2.
Note:
According to the Convolution Theorem, the Fourier transform of the window function rep-
resents precisely the lineshape of an undamped cosine input.
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_3
71

72
3
Window Functions
3.1 The Rectangular Window
f (t) =
1 for âˆ’T/2 â‰¤t â‰¤T/2
0 else
,
(3.1)
has the power representation of the Fourier transform:
|F(Ï‰)|2 = T 2
sin(Ï‰T/2)
Ï‰T/2
2
.
(3.2)
The rectangular window and this function are shown in Fig.3.1. The ï¬rst sidelobe
is negative and all subsequent sidelobes alternate in sign.
3.1.1 Zeros
Where are the zeros of this function? Weâ€™ll ï¬nd them at Ï‰T/2 = Â±lÏ€ with l =
1, 2, 3, . . .and without 0! The zeros are equidistant, the zero atl = 0 in the numerator
gets â€œpluggedâ€ by a zero at l = 0 in the denominator.1
3.1.2 Intensity at the Central Peak
Now we want to ï¬nd out how much intensity is at the central peak, and how much gets
lost in the sidebands (sidelobes). To get there, we need the ï¬rst zero at Ï‰T/2 = Â±Ï€
or Ï‰ = Â±2Ï€/T and:
+2Ï€/T

âˆ’2Ï€/T
T 2
sin(Ï‰T/2)
Ï‰T/2
2
dÏ‰ = T 2 2
T 2
Ï€

0
sin2 x
x2
dx = 4T Si(2Ï€)
(3.3)
where Ï‰T/2 = x.
Here Si(x) stands for the sine integral:
x

0
sin y
y
dy.
(3.4)
1Use lâ€™Hospitalâ€™s rule for limxâ†’0 sin x
x .

3.1 The Rectangular Window
73
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.1 Rectangular window function and its Fourier transform in power representation (the unit
dB, â€œdecibelâ€, will be explained in Sect.3.1.3)
The last equal sign may be proved as follows. We start out with:
Ï€

0
sin2 x
x2
dx
and integrate partially with u = sin2 x and v = âˆ’1
x :
Ï€

0
sin2 x
x2
dx = sin2 x
x

Ï€
0
+
Ï€

0
2 sin x cos x
x
dx
(3.5)
= 2
Ï€

0
sin 2x
2x
dx = Si(2Ï€)
with 2x = y.
Using Parsevalâ€™s theorem we get the total intensity:
+âˆ

âˆ’âˆ
T 2
sin(Ï‰T/2)
Ï‰T/2
2
dÏ‰ = 2Ï€
+T/2

âˆ’T/2
12dt = 2Ï€T.
(3.6)
The ratio of the intensity at the central peak to the total intensity therefore is:
4T Si(2Ï€)
2Ï€T
= 2
Ï€ Si(2Ï€) = 0.903.
This means that â‰ˆ90% of the intensity is in the central peak, whereas some 10%
are â€œwastedâ€ in sidelobes.

74
3
Window Functions
3.1.3 Sidelobe Suppression
Now letâ€™s determine the height of the ï¬rst sidelobe. To get there, we need:
d|F(Ï‰)|2
dÏ‰
= 0
or also
dF(Ï‰)
dÏ‰
= 0
(3.7)
and thatâ€™s the case when:
d
dx
sin x
x
= 0 = x cos x âˆ’sin x
x2
with x = Ï‰T/2 or x = tan x.
Solving this transcendental equation (for example graphically or by trial and error)
gives us the smallest possible solution x = 4.4934 or Ï‰ = 8.9868/T . Inserting that
in |F(Ï‰)|2 results in:
F
 8.9868
T
2 = T 2 Ã— 0.04719.
(3.8)
For Ï‰ = 0 we get |F(0)|2 = T 2, the ratio of the ï¬rst sidelobeâ€™s height to the central
peakâ€™s height therefore is 0.04719. Itâ€™s customary to express ratios between two
values spanning several orders of magnitude in decibels (short: dB). The deï¬nition
of the decibel is:
dB = 10 log10 x
(3.9)
Quiteregularlypeopleforgettomentionwhat theratioâ€™sbasedon,whichcancause
confusion. Weâ€™re talking about intensity-ratios, (viz. F2(Ï‰)). If weâ€™re referring to
amplitude-ratios,(viz. F(Ï‰)),thiswouldmakepreciselyafactoroftwoinlogarithmic
representation! Here we have a sidelobe suppression (ï¬rst sidelobe) of:
10 log10 0.04719 = âˆ’13.2 dB.
(3.10)
3.1.4 3dB-Bandwidth
As the 10 log10(1/2) = âˆ’3.0103 â‰ˆâˆ’3, the 3dB bandwidth tells us where the
central peak has dropped to half its height. This is easily calculated as follows:
T 2
sin(Ï‰T/2)
Ï‰T/2
2
= 1
2T 2.
Using x = Ï‰T/2 we have:
sin2 x = 1
2 x2
or
sin x =
1
âˆš
2
x.
(3.11)

3.1 The Rectangular Window
75
This transcendental equation has the following solution:
x = 1.3915,
thus
Ï‰3dB = 2.783/T.
This gives us the total width (Â±Ï‰3dB):
Ï‰ = 5.566
T
.
(3.12)
This is the slimmest central peak we can get using Fourier transformation. Any
other window function will lead to larger 3dB-bandwidths. Admittedly, itâ€™s more
than nasty to stick more than â‰ˆ10% of the information into the sidelobes. If we
have, apart from the prominent spectral component, another spectral component,
withâ€”sayâ€”an approx. 10dB smaller intensity, this component will be completely
smothered by the main componentâ€™s sidelobes. If weâ€™re lucky, it will sit on the ï¬rst
sidelobe and will be visible; if weâ€™re out of luck, it will fall into the gap (the zero)
between central peak and ï¬rst sidelobe and will get swallowed. So it pays to get rid
of these sidelobes.
Warning: This 3dB-bandwidth is valid for |F(Ï‰)|2 and not for F(Ï‰)! Since
one often uses |F(Ï‰)| or the cosine-/sine-transformation (cf. Sect.4.5) one wants
the 3dB-bandwidth thereof, which corresponds to the 6dB-bandwidth of |F(Ï‰)|2.
Unfortunately, you cannot simply multiply the 3dB-bandwidth of |F(Ï‰)|2 by
âˆš
2,
you have to solve a new transcendental equation. However, itâ€™s still good as a ï¬rst
guess because you merely interpolate linearly between the point of 3dB-bandwidth
and the point of the 6dB-bandwidth. Youâ€™d overestimate the width by less than 5%.
3.1.5 Asymptotic Behaviour of Sidelobes
The sidelobesâ€™ envelope results in the heights decreasing by 6dB per octave (thatâ€™s
a factor of 2 as far as the frequency is concerned). This result is easily derived from
(1.62). The unit step leads to oscillations which decay as 1
k , i.e. in the continuous
case as 1
Ï‰. This corresponds to a decay of 3dB per octave. Now we are dealing with
squared magnitudes, hence, we have a decay of
1
Ï‰2 . This corresponds to a decay of
6dB per octave. This is of fundamental importance: a discontinuity in the function
yields âˆ’6dB/octave, a discontinuity in the derivative (hence, a kink in the function)
yields âˆ’12dB/octave and so forth. This is immediately clear considering that the
derivative of the triangular function yields the step function. The derivative of 1
Ï‰
yields
1
Ï‰2 (apart from the sign), i.e. a factor of 2 in the sidelobe suppression. You
remember the
	
1
k2

-dependence of the Fourier coefï¬cients of the triangular function?
The â€œsmootherâ€ the window function starts out, the better the sidelobesâ€™ asymptotic
behaviour will get. But this comes at a price, namely a worse 3dB-bandwidth.

76
3
Window Functions
3.2 The Triangular Window (Fejer Window)
The ï¬rst real weighting function is the triangular window:
f (t) =
â§
âªâ¨
âªâ©
1 + 2t/T for âˆ’T/2 â‰¤t â‰¤0
1 âˆ’2t/T for 0 â‰¤t â‰¤T/2
0
else
,
(3.13)
F(Ï‰) = T
2
sin(Ï‰T/4)
Ï‰T/4
2
.
(3.14)
We wonâ€™t have to rack our brains! This is the autocorrelation function of the
triangular function (cf. Sect.2.3.1, Fig.2.12). The only difference is the intervalâ€™s
width: whereas the autocorrelation function of the rectangular function over the
interval âˆ’T/2 â‰¤t â‰¤T/2 has a width of âˆ’T â‰¤t â‰¤T , in (3.13) we only have the
usual interval âˆ’T/2 â‰¤t â‰¤T/2.
The 1/4 is due to the interval, the square due to the autocorrelation. All other
properties are obvious straight away. The triangular window and the square of this
function are shown in Fig.3.2.
The zeros are twice as far apart as in the case of the rectangular function:
Ï‰T
4
= Ï€l
or
Ï‰ = 4Ï€l
T
l = 1, 2, 3, . . . .
(3.15)
The intensity at the central peak is 99.7%.
The height of the ï¬rst sidelobe is suppressed by 2 Ã— (âˆ’13.2 dB) â‰ˆâˆ’26.5 dB
(No wonder, if we skip every other zero!).
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.2 Triangular window and power representation of the Fourier transform

3.2 The Triangular Window (Fejer Window)
77
The 3dB-bandwidth is calculated as follows:
sin Ï‰T
4
=
1
4âˆš
2
Ï‰T
4
to
Ï‰ = 8.016
T
(full width),
(3.16)
thatâ€™s some 1.44 times wider than in the case of the rectangular window.
The asymptotic behaviour of the sidelobes is âˆ’12dB/octave.
3.3 The Cosine Window
The triangular window had a kink when switching on, another kink at the maximum
(t = 0) and another one when switching off. The cosine window avoids the kink
at t = 0:
f (t) =

cos Ï€t
T for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.17)
The Fourier transform of this function is:
F(Ï‰) = T cos Ï‰T
2 Ã—

1
Ï€ âˆ’Ï‰T +
1
Ï€ + Ï‰T

.
(3.18)
The functions f (t) and |F(Ï‰)|2 are shown in Fig.3.3.
At position Ï‰ = 0 we get:
F(0) = 2T
Ï€ .
For Ï‰T â†’Â±Ï€ we get expressions of type â€œ0 : 0â€, which we calculate using
lâ€™Hospitalâ€™s rule.
Surprise surprise: The zero at Ï‰T = Â±Ï€ was â€œpluggedâ€ by the expression in
brackets in (3.18), i.e. F(Ï‰) there will stay ï¬nite and continuous. Apart from that,
the following applies:
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.3 Cosine window and power representation of the Fourier transform

78
3
Window Functions
The zeros are at:
Ï‰T
2
= (2l + 1)Ï€
2
,
Ï‰ = (2l + 1)Ï€
T
,
l = 1, 2, 3, . . . ,
(3.19)
i.e. within the same distance as in the case of the rectangular window.
Here itâ€™s not worth shedding tears for a lack of intensity at the central peak any
more. For all practical purposes it is â‰ˆ100%. We should, however, have another look
at the sidelobes because of the minorities, viz. the chance of detecting additional
weak signals.
The suppression of the ï¬rst sidelobe may be calculated as follows:
tan x
2 =
4x
Ï€2 âˆ’x2
with the solution x â‰ˆ11.87.
(3.20)
This results in a sidelobe suppression of âˆ’23dB.
The 3dB-bandwidth amounts to:
Ï‰ = 7.47
T ,
(3.21)
a remarkable result. This is the ï¬rst time we got, through the use of a somewhat more
intelligent â€œwindowâ€, a sidelobe suppression of âˆ’23dBâ€”not a lot worse than the
âˆ’26.5dB of the triangular windowâ€”and we get a better 3dB-bandwidth compared
to Ï‰ = 8.016/T for the triangular window. So it does pay to think about better
window functions. The asymptotic decay of the sidelobes is âˆ’12dB/octave, as was
the case for the triangular function.
3.4 The cos2-Window (Hanning)
The scientist Julius von Hann thought that eliminating the kinks at Â±T/2 would be
beneï¬cialandproposedthecos2-window(intheUS,thissoonwascalledâ€œHanningâ€):
f (t) =

cos2 Ï€t
T for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.22)
The corresponding Fourier transform is:
F(Ï‰) = T
4 sin Ï‰T
2 Ã—

1
Ï€ âˆ’Ï‰T/2 +
2
Ï‰T/2 âˆ’
1
Ï€ + Ï‰T/2

.
(3.23)
The functions f (t) and |F(Ï‰)|2 are shown in Fig.3.4.

3.4 The cos2-Window (Hanning)
79
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.4 Hanning window and power representation of the Fourier transform
The zero at Ï‰ = 0 has been â€œpluggedâ€ because of sin(Ï‰T/2)/(Ï‰T/2) â†’1 and
the zeros at Ï‰ = Â±2Ï€/T get â€œpluggedâ€ for the same reason. The example of the
cosine window is becoming popular!
The zeros are at:
Ï‰ = Â±2lÏ€
T ,
l = 2, 3, . . . .
(3.24)
Intensity at the central peak â‰ˆ100%.
The suppression of the ï¬rst sidelobe is âˆ’32dB.
The 3dB-bandwidth is:
Ï‰ = 9.06
T .
(3.25)
The sidelobesâ€™ asymptotic decay is âˆ’18dB/octave.
So we get a considerable sidelobe suppression, admittedly to the detriment of the
3dB-bandwidth.
Some experts recommend to go for higher-powered cosine functions in the ï¬rst
place. This would â€œplugâ€ more and more zeros near the central peak, and there will
be gains both as far as sidelobe suppression as well as asymptotic behaviour are
concerned, though, of course, the 3dB-bandwidth will get bigger and bigger. So for
the cos3-window we get:
Ï‰ = 10.4
T
(3.26)
and for the cos4-window:
Ï‰ = 11.66
T
.
(3.27)
As weâ€™ll see shortly, there are more intelligent solutions to this problem.

80
3
Window Functions
3.5 The Hamming Window
Mr Julius von Hann didnâ€™t have a clue that heâ€”sorry: his window functionâ€”would
be put on a pedestal in order to get an even better window, and to add insult to injury,
his name would get mangled to â€œHammingâ€ to boot.2
f (t) =

a + (1 âˆ’a) cos2 Ï€t
T for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.28)
The Fourier transform is:
F(Ï‰) = T
4 sin Ï‰T
2 Ã—

1 âˆ’a
Ï€ âˆ’Ï‰T/2 + 2(1 + a)
Ï‰T/2
âˆ’
1 âˆ’a
Ï€ + Ï‰T/2

.
(3.29)
How come thereâ€™s a â€œpedestalâ€? Didnâ€™t we realise a few moments ago that any
discontinuity at the interval boundaries is â€œbadâ€? Just like a smidgen of arsenic may
work wonders, here a â€œtiny wee pedestalâ€ can be helpful. Indeed, using parameter
a weâ€™re able to play the sidelobes a bit. A value of a â‰ˆ0.1 proves to be good. The
plugging of the zeros hasnâ€™t changed, as (3.29) shows. Though now, however, the
Fourier transform of the â€œpedestalâ€ has saddled us with the term:
T
2 a sin(Ï‰T/2)
Ï‰T/2
that now gets added to the sidelobes of the Hamming window. A squaring of F(Ï‰)
is not essential here. This on the one hand will provide interference terms of the
Hamming windowâ€™s Fourier transform, but on the other hand, the same is true for
F(Ï‰); here all we get are positive and negative sidelobes. The absolute values of
the sidelobesâ€™ heights donâ€™t change. The Hamming window with a = 0.15 and the
respective F2(Ï‰) are shown in Fig.3.5. The ï¬rst sidelobes are slightly smaller than
the second ones! Here we have the same zeros as (this is done by the sin Ï‰T
2 , provided
the denominators donâ€™t prevent it). For the optimal parameter a = 0.08 the sidelobe
suppression is âˆ’43dB, the 3dB-bandwidth is only Ï‰ = 8.17/T . The asymptotic
behaviour, naturally, got worse. Far from the central peak, itâ€™s down to as little as
âˆ’6dB per octave. Thatâ€™s what happens when you choose a small step!
Therefore the new strategy is: rather a somewhat worse asymptotic behaviour, if
only we manage to get a high sidelobe suppression and, at the same time, a decrease
in 3dB-bandwidth deterioration thatâ€™s as small as possible. How far one can go is
illustrated by the following example. Plant at the interval ends little â€œï¬‚agpolesâ€,
i.e. inï¬nitely sharp cusps with small height. This is, of course, most easily done in
the discrete Fourier transformation. There, the â€œï¬‚agpoleâ€ is just a channel wide. Of
2No kidding, Mr R.W. Hamming apparently did discover this window, and the von Hann window
got mangled later on.

3.5 The Hamming Window
81
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.5 Hamming window and power representation of Fourier transform
course, we get no asymptotic roll-off of the sidelobes at all. The Fourier transform of
a Î´-function is a constant! However, we get the narrowest 3dB-bandwidth possible.
Such a window is called Dolphâ€“Chebychev window, however, we wonâ€™t discuss it
any further here. Because it emphasizes the data at the interval boundaries it should
not be used in cases where those data are inaccurate or corrupted.
Before we get into more and better window functions, letâ€™s look, just for curiosityâ€™s
sake, at a window that creates no sidelobes at all.
3.6 The Triplet Window
The previous really set us up, so letâ€™s try the following:
f (t) =

eâˆ’Î»|t| cos2 Ï€t
T for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.30)
Deducing the expression for F(Ï‰) is trivial, yet too lengthy (and too unimportant)
to be dealt with here.
The expression for F(Ï‰)â€”if we do deduct itâ€”stands out, as it features oscil-
lating terms (sine, cosine) though there are no more zeros. If only the Î» is big
f(t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.6 Triplet window and power representation of the Fourier transform

82
3
Window Functions
enough, then there wonâ€™t even be local minima or maxima any more, and F(Ï‰)
decays monotonically. In the case of optimum Î» we can achieve an asymptotic
behaviour of âˆ’12dB/octave with a 3dB-bandwidth of Ï‰ = 9.7/T (cf. Fig.3.6).
Therefore it wasnâ€™t such a bad idea to re-introduce a spike at t = 0. However,
there are better window functions.
3.7 The Gauss Window
A pretty obvious window function is the Gauss function. That we have to truncate it
somewhere, resulting in a small step, doesnâ€™t worry us any more, if we look back on
our experience with the Hamming window.
f (t) =

exp
	
âˆ’1
2
t2
Ïƒ2

for âˆ’T/2 â‰¤t â‰¤+T/2
0
else
.
(3.31)
The Fourier transform reads:
F(Ï‰) = Ïƒ
Ï€
2 eâˆ’Ïƒ2Ï‰2
2

erf

âˆ’iÏƒÏ‰
âˆš
2
+
T
2
âˆš
2Ïƒ

+ erf

+iÏƒÏ‰
âˆš
2
+
T
2
âˆš
2Ïƒ

.
(3.32)
As the error function occurs with complex arguments, though together with the
conjugate complex argument, F(Ï‰) is real. The function f (t) with Ïƒ = 2 in units of
T/2 and |F(Ï‰)|2 is shown in Fig.3.7. A disadvantage of the Gauss window is that
its sidelobes do not decay monotonically for all values of Ïƒ.
A Gauss function being Fourier-transformed will result in another Gauss function,
yet only when there was no truncation! If Ïƒ is sufï¬ciently big, the sidelobes will
disappear: the oscillations â€œcreep upâ€ the Gauss functionâ€™s ï¬‚ank. Shortly before this
happens, we get a 3dB-bandwidth of Ï‰ = 9.06/T , âˆ’64dB sidelobe suppression
and âˆ’26dB per octave asymptotic behaviour near the central peak. Not bad, but we
can do better.
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.7 Gauss window and power representation of the Fourier transform

3.8 The Kaiserâ€“Bessel Window
83
3.8 The Kaiserâ€“Bessel Window
The Kaiserâ€“Bessel window is a very useful window and can be applied to various
situations:
f (t) =
â§
â¨
â©
I0
	
Î²âˆš
1âˆ’(2t/T )2

I0(Î²)
for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.33)
Here Î² is a parameter that may be chosen at will. The Fourier transform is:
F(Ï‰) =
â§
âªâªâªâªâªâ¨
âªâªâªâªâªâ©
2T
I0(Î²)
sinh

Î²2âˆ’Ï‰2T 2
4


Î²2âˆ’Ï‰2T 2
4
for Î² â‰¥
 Ï‰T
2

2T
I0(Î²)
sin

Ï‰2T 2
4
âˆ’Î²2


Ï‰2T 2
4
âˆ’Î²2
for Î² â‰¤
 Ï‰T
2

.
(3.34)
I0(x) is the modiï¬ed Bessel function. A simple algorithm [10, Equations9.8.1,
9.8.2] for the calculation of I0(x) follows:
I0(x) = 1 + 3.5156229t2 + 3.0899424t4 + 1.2067492t6
+ 0.2659732t8 + 0.0360768t10 + 0.0045813t12 + Ïµ,
|Ïµ| < 1.6 Ã— 10âˆ’7
with t = x/3.75, for the interval âˆ’3.75 â‰¤x â‰¤3.75,
and:
x1/2eâˆ’x I0(x) = 0.39894228 + 0.01328592tâˆ’1 + 0.00225319tâˆ’2
âˆ’0.00157565tâˆ’3 + 0.00916281tâˆ’4 âˆ’0.02057706tâˆ’5
+ 0.02635537tâˆ’6 âˆ’0.01647633tâˆ’7 + 0.00392377tâˆ’8 + Ïµ,
|Ïµ| < 1.9 Ã— 10âˆ’7
with t = x/3.75, for the interval 3.75 â‰¤x < âˆ.
The zeros are at Ï‰2T 2/4 = l2Ï€2+Î²2, l = 1, 2, 3, . . ., and theyâ€™re not equidistant.
For Î² = 0 we get the rectangular window, values up to Î² = 9 are recommended.
Figure3.8 shows f (t) and |F(Ï‰)|2 for various values of Î².
The sidelobe suppression as well as the 3dB-bandwidth as a function of Î² are
shown in Fig.3.9. Using this window function we get for Î² = 9 âˆ’70dB sidelobe
suppression with Ï‰ = 11/T and âˆ’38.5dB/octave asymptotic behaviour near the
central peak. In every respect, the Kaiserâ€“Bessel windows is superior to the Gauss
window.

84
3
Window Functions
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Fig. 3.8 Kaiserâ€“Bessel window for Î² = 0, 2, 4, 6, 8 (left) and the respective power representation
of the Fourier transform (right)

3.9 The Blackmanâ€“Harris Window
85
0
2
4
6
8
10
12
0
1
2
3
4
5
6
7
8
9
Î”Ï‰T
Î²
0
âˆ’10
âˆ’20
âˆ’30
âˆ’40
âˆ’50
âˆ’60
âˆ’70
0
1
2
3
4
5
6
7
8
9
Î²
[dB]
Fig. 3.9 Sidelobe suppression and 3dB-bandwidth for Kaiserâ€“Bessel parameter Î² = 0 âˆ’9
3.9 The Blackmanâ€“Harris Window
To those of you who donâ€™t want ï¬‚exibility and want to work with a ï¬xed good
sidelobe suppression, I recommend the following two very efï¬cient windows which
are due to Blackman and Harris. They have the charm to be simple: they consist of
a sum of four cosine terms as follows:
f (t) =
â§
âªâ¨
âªâ©
3
n=0
an cos 2Ï€nt
T
for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
(3.35)
Please note that we have a constant, a cosine term with a full period, as well as
further terms with two and three full periods, contrary to the Sect.3.3. Here, the
coefï¬cients have the following values:
for âˆ’74 dB
for âˆ’92 dB
a0
0.40217
0.35875
a1
0.49703
0.48829
a2
0.09392
0.14128
a3
0.00183
0.01168 .
(3.36)

86
3
Window Functions
Intheoriginal publicationof Harris [9] thesumthecoefï¬cients of the âˆ’74dBwindow
is smaller than 1 by 0.00505. There must be a misprint. With the coefï¬cients listed
above the â€œsidelobeâ€ suppression is signiï¬cantly worse than âˆ’74dB. If you take
a1 = 0.49708 and a2 = 0.09892 (or a2 = 0.09892 and a3 = 0.00188) you obtain
âˆ’74dB. In this case the sum of the coefï¬cients yields 1. Maybe, in 1978 there were
problems during the typing of the manuscript: the character 8 was read as 3? The
second choice is that given in [11].
Surely, you have noted that the coefï¬cients add up to 1; at the interval ends the
terms with a0 and a2 are positive, whereas the terms with a1 and a3 are negative. The
sum of the even coefï¬cients minus the sum of the odd coefï¬cients yields practically
0, i.e. there is a rather â€œsoftâ€ turning on with a tiny step only.
The Fourier transform of this window reads:
F(Ï‰) = T sin Ï‰T
2
3

n=0
an(âˆ’1)n

1
2nÏ€ + Ï‰T âˆ’
1
2nÏ€ âˆ’Ï‰T

.
(3.37)
Donâ€™t worry, the zeros in the denominator are just â€œhealedâ€ by the zeros of the sine.
The zeros of the Fourier transform are given by sin Ï‰T
2 = 0, i.e. they are the same as
for the Hanning window. The 3dB-bandwidth is Ï‰ = 10.93/T and 11.94/T for
the âˆ’74dB- and the âˆ’92dB-window, respectively; excellent performance for such
simple windows. I guess, the series expansion of the modiï¬ed Bessel function I0(x)
for the appropriate values of Î² yields pretty much the coefï¬cients of the Blackmanâ€“
Harris windows. Because these Blackmanâ€“Harris windows differ only very little
from the Kaiserâ€“Bessel windows with Î² â‰ˆ9 and Î² â‰ˆ11.5, respectively, (these
are the values for comparable sidelobe suppression), I do without ï¬gures. However,
the Blackmanâ€“Harris window with âˆ’92dB would have no more visible â€œfeetletsâ€ in
Fig.3.10 which displays to âˆ’80dB only.
3.10 Overview over Window Functions
In order to ï¬ll this chapter with life we give a simple example. Given is the following
function:
f (t) = cos Ï‰t + 10âˆ’2 cos 1.15Ï‰t + 10âˆ’3 cos 1.25Ï‰t
(3.38)
+ 10âˆ’3 cos 2Ï‰t + 10âˆ’4 cos 2.75Ï‰t + 10âˆ’5 cos 3Ï‰t.
Apart from the dominant frequency Ï‰ there are two satellites at 1.15 and 1.25 times
Ï‰, two harmonicsâ€”radio frequency technicians say ï¬rst and second harmonicâ€”
at 2Ï‰ and 3Ï‰ as well as another frequency at 2.75Ï‰. Letâ€™s Fourier-transform this
function. Please keep in mind that we shall look at the power spectra right now, i.e.
the amplitudes squared! Hence, the signs of the amplitudes play no role. Apart from
the dominant frequency, which we will quote with 0dB intensity, we expect further
spectral components with intensities of âˆ’40, âˆ’60, âˆ’80 and âˆ’100dB.

3.10 Overview over Window Functions
87
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Rectangular window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Triangular window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Cosine window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Hanning window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Hamming window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Triplet window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Gauss window
f (t)
t
âˆ’T
2
+ T
2
0
âˆ’20
âˆ’40
âˆ’60
âˆ’80
|F(Ï‰)|2
[dB]
Ï‰
0
Kaiserâ€“Bessel window
Fig. 3.10 â€œOverviewâ€ of the window functions

88
3
Window Functions
-160
-140
-120
-100
-80
-60
-40
-20
0
0
F
1
2
3
4
-160
-140
-120
-100
-80
-60
-40
-20
0
-160
-140
-120
-100
-80
-60
-40
-20
0
-160
-140
-120
-100
-80
-60
-40
-20
0
Hamming
rectangle
Hanning
-160
-140
-120
-100
-80
-60
-40
-20
0
Fig. 3.11 Test function from (3.38) analysed with different window functions

3.10 Overview over Window Functions
89
Figure3.11 shows what you get using different window functions. For the purists:
of course, we have used the discrete Fourier transform to be dealt with in the next
chapter, but show line-plots (we have used 128 data points, zero-padded the data,
mirrored and used a total of 4096 input data; now you can repeat it yourself!).3
The two satellites close to the dominant frequency cause the biggest problems.
On the one hand we require a window function with a good sidelobe suppression
in order to be able to see the signals with intensities of âˆ’40 and âˆ’60dB. The rec-
tangular window doesâ€™nt achieve that! You only see the dominant frequency, all
the rest is â€œdrownedâ€. In addition, we require a small 3dB-bandwidth in order to
resolve the frequency which is 15% higher. This is pretty well accomplished using
the Hanning- and above all the Hamming-window (Parameter 0.08). However, the
Hamming window is unable to detect the higher spectral components which still
have lower intensities. This is a consequence of the poor asymptotic behaviour. We
are no better off with the component which is 25% higher because it has âˆ’60dB
intensity only. Here, the Blackmanâ€“Harris window with âˆ’74dB is just able to do
so. It is easy to detect the other three, still higher spectral components, regardless of
their low intensities, because they are far away from the dominant frequency if only
the sidelobes in this spectral range are not â€œdrowningâ€ them. Interestingly enough,
window functions with poor sidelobe suppression but good asymptotic behaviour
like the Hanning window are doing the job, as do window functions with good side-
lobe suppression and poor asymptotic behaviour like the Kaiserâ€“Bessel window. The
Kaiserâ€“Bessel window with the parameter Î² = 12 is an example (the Blackmanâ€“
Harris window with âˆ’92dB sidelobe suppression is nearly as good). The disadvan-
tage: the small satellites at 1.15- and 1.25-fold frequency show up as shoulders only.
You see that we should use different window functions for different demands. There
is no multi-purpose beast providing eggs, wool, milk and bacon! However, there are
window functions which you can simply forget. Since the Kaiserâ€“Bessel window has
an adjustable parameter Î², I recommend to try out various values of Î². An important
property of this window is its monotonic decay of the sidelobes.
What can we do if we need a lot more sidelobe suppression than âˆ’100dB? Take
the Kaiserâ€“Bessel window with a very large parameter Î²; you easily get much better
sidelobe suppression, of course with increasingly larger 3dB-bandwidth! There is no
escape from this â€œdouble millâ€! However, despite the joy about â€œintelligentâ€ window
functions you should not forget that ï¬rst you should obtain data which contain so
little noise that they allow the mere detection of âˆ’100dB-signals.
3I must confess, all the previous ï¬gures of the Fourier-transformed window functions were pre-
pared by discrete Fourier transformation and there are subtle differences compared to the analytical
formulas like those shown in Fig.4.19.

90
3
Window Functions
3.11 Windowing or Convolution?
In principle, we have two possibilities to use window functions:
i. either you weight, i.e. you multiply the input by the window function and subse-
quently Fourier-transform, or
ii. you Fourier-transform the input and convolve the result with the Fourier transform
of the window function.
According to the Convolution Theorem (2.42) we get the same result. What are
the pros and cons of both procedures? There is no easy answer to this question. What
helps in arguing is thinking in discrete data. Take, e.g. the Kaiserâ€“Bessel window.
Letâ€™s start with a reasonable value for the parameter Î², based on considerations of
the trade-off between 3dB-bandwidth (i.e. resolution) and sidelobe suppression. In
the case of windowing we have to multiply our input data, say N real or complex
numbers, by the window function which we have to calculate at N points. After
that we Fourier-transform. Should it turn out that we actually should require a better
sidelobesuppressionandcouldtolerateaworseresolutionâ€”or viceversaâ€”wewould
have to go back to the original data, window them again and Fourier-transform again.
The situation is different for the case of convolution: we Fourier-transform without
any bias concerning the eventually required sidelobe suppression and subsequently
convolve the Fourier data (again N numbers, however in general complex!) with the
Fourier-transformed window function, which we have to calculate for a sufï¬cient
number of points. What is a sufï¬cient number? Of course, we drop the sidelobes for
the convolution and only take the central peak! This should be calculated at least
for 5 points, better more. The convolution then actually consists of 5 (or more) mul-
tiplications and a summation for each Fourier coefï¬cient. This appears to be more
work; however, it has the advantage that a further convolution with another, say
broader Fourier-transformed window function, would not require to carry out a new
Fourier transformation. Of course, this procedure is but an approximation because
of the truncation of the sidelobes. If we included all data of the Fourier-transformed
window function including the sidelobes, we had to carry out N (complex) multipli-
cations and a summation per point, already quite a lot of computational effort, yet
still less than a new Fourier transformation. This could be relevant for large arrays,
especially in two or three dimensions like in image processing and tomography.
What happens at the edges when carrying out a convolution? We shall see in the
following chapter that we shall continue periodically beyond the interval. This gives
us the following idea: letâ€™s take the Blackmanâ€“Harris window and continue period-
ically; the corresponding Fourier transform consists of a sum of four Î´-functions,
in the discrete world we have exactly four channels which are non-zero. Where
remained the sidelobes? You shall see in a minute that in this case the points (by the
way equidistant) coincide with the zeros of the Fourier-transformed window func-
tion, except at 0! Hence, we have to carry out a convolution with just four points
only, a rather fast procedure! Thatâ€™s why the Blackmanâ€“Harris window is called a
4-point-window. So after all, convolution is better? Here comes a deep sigh: there

3.11 Windowing or Convolution?
91
are so many good reasons to get rid of the periodic continuation as much as possible
by zero-padding the input data (cf. Sect.4.6), thus our neat 4-point-idea melts away
like snow in springtime sun. The decision is yours whether you prefer to weight or
to convolve and depends on the concrete case. Now itâ€™s high time to start with the
discrete Fourier transformation!
Playground
3.1 Squared
Calculate the 3dB-bandwidth of F(Ï‰) for the rectangular window. Compare this
with the 3dB-bandwidth F2(Ï‰).
3.2 Letâ€™s Gibbs Again
What is the asymptotic behaviour of the Gauss window far away from the central
peak?
3.3 Expander
The series expansion of the modiï¬ed Bessel function of zeroth order is:
I0(x) =
âˆ

k=0
(x2/4)k
(k!)2 ,
where k! = 1 Ã— 2 Ã— 3 Ã— Â· Â· Â· Ã— k denotes the factorial. The series expansion for the
cosine reads:
cos(x) =
âˆ

k=0
(âˆ’1)k x2k
(2k)!.
Calculate the ï¬rst ten terms in the series expression of the Blackmanâ€“Harris window
with âˆ’74dB sidelobe suppression and the Kaiserâ€“Bessel window with Î² = 9 and
compare the results.
Hint: Instead of pen and paper better use your PC!
3.4 Minorities
In a spectrum analyser you detect a signal at Ï‰ = 500Mrad/s in the |F(Ï‰)|2-mode
with an instrumental full width at half maximum (FWHM) of 50Mrad/s with a
rectangular window.
a. What sampling period T did you choose?
b. What window function could you use if you were hunting a â€œminorityâ€ signal
which you suspect to be 20% higher in frequency and 50dB lower than the main
signal. Look at the ï¬gures in this chapter, donâ€™t calculate too much.

Chapter 4
Discrete Fourier Transformation
Abstract This chapter deals with the discrete Fourier transformation. Here, a
periodic series in the time domain is mapped onto a periodic series in the frequency
domain. Deï¬nitions of the discrete Fourier transformation and its inverse are given.
Linearity, convolution, cross-correlation, and autocorrelation are treated as well as
Parsevalâ€™s theorem. The sampling theorem is illustrated with a simple example. Data
mirroring, cosine- and sine-transformations, as well as zero-padding are discussed.
It concludes with the Fast Fourier Transformation algorithm by Cooley and Tukey.
Mapping of a Periodic Series { fk} to the Fourier-Transformed
Series {Fj}
4.1 Discrete Fourier Transformation
Often we donâ€™t know a functionâ€™s continuous â€œbehaviourâ€ over time, but only what
happens at N discrete times:
tk = kt,
k = 0, 1, . . . , N âˆ’1.
In other words: weâ€™ve taken our â€œpickâ€, thatâ€™s â€œsamplesâ€ f (tk) = fk at certain
points in time tk. Any digital data-recording uses this technique. So the data set con-
sists of a series { fk}. Outside the sampled interval T = Nt we donâ€™t know anything
about the function. The discrete Fourier transformation automatically assumes, that
{ fk} will continue periodically outside the intervalâ€™s range. At ï¬rst glance this limi-
tation appears to be very annoying: maybe f (t) isnâ€™t periodic at all, and even if f (t)
were periodic, thereâ€™s a chance that our interval happens to truncate at the wrong
time (meaning: not after an integer number of periods). How this problem can be
alleviated or practically eliminated will be shown in Sect.4.6. To make life easier,
weâ€™ll also take for granted that N is a power of 2. Weâ€™ll have to assume the latter
anyway for the Fast Fourier Transformation (FFT) which weâ€™ll cover in Sect.4.7.
Using the â€œtrickâ€ from Sect.4.6, however, this limitation will become completely
irrelevant.
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_4
93

94
4
Discrete Fourier Transformation
4.1.1 Even and Odd Series and Wrap-Around
A series is called even if the following is true for all k:
fâˆ’k = fk.
(4.1)
A series is called odd if the following is true for all k:
fâˆ’k = âˆ’fk.
(4.2)
(Here f0 = 0 is compulsory!). Any series can be broken up into an even and an odd
series. But what about negative indices? Weâ€™ll extend the series periodically:
fâˆ’k = fNâˆ’k.
(4.3)
This allows us, by adding N, to shift the negative indices to the right end of the
interval, or using another word, â€œwrap them aroundâ€, as shown in Fig.4.1.
Please make sure f0 doesnâ€™t get wrapped, something that often is done by mistake.
The periodicity with period N, which we always assume as given for the discrete
Fourier transformation, requires fN = f0. In the second exampleâ€”the one with the
mistakeâ€”we would get f0 twice next to each other (and apart from that, we would
have overwritten f4, truly a â€œmortal sinâ€).
4.1.2 The Kronecker Symbol or the â€œDiscrete Î´-Functionâ€
Before we get into the deï¬nition of the discrete Fourier transformation (forward and
inverse transformation), a few preliminary remarks are in order. From the continuous
Fourier transformation eiÏ‰t we get for discrete times tk = kt, k = 0, 1, . . . , N âˆ’1
with T = Nt:
correct
wrong
Fig. 4.1 Correctly wrapped-around (top); incorrectly wrapped-around (bottom)

4.1 Discrete Fourier Transformation
95
eiÏ‰t â†’ei 2Ï€tk
T
= e
2Ï€ikt
Nt
= e
2Ï€ik
N
â‰¡W k
N.
(4.4)
Here the â€œkernelâ€ is:
WN = e
2Ï€i
N
(4.5)
a very useful abbreviation. Occasionally weâ€™ll also need the discrete frequencies:
Ï‰ j = 2Ï€ j/(Nt),
(4.6)
related to the discrete Fourier coefï¬cients Fj (see below). The kernel WN has the
following properties:
W nN
N
= e2Ï€in = 1
for all integer n,
(4.7)
WN is periodic in j and k with the period N.
A very useful representation of WN may be obtained in the complex plane as a
â€œclock-handâ€ in the unit circle.
The projection of the â€œhand of a clockâ€ onto the real axis results in cos(2Ï€n/N).
Like when talking about a clock-face, we may, for example, call W 0
8 â€œ3:00 a.m.â€ or
W 4
8 â€œ9:00 a.m.â€. Now we can deï¬ne the discrete â€œÎ´-functionâ€:
Nâˆ’1

j=0
W (kâˆ’kâ€²) j
N
= NÎ´k,kâ€².
(4.8)
Here Î´k,kâ€² is the Kronecker symbol with the following property:
Î´k,kâ€² =
1 for k = kâ€²
0 else
.
(4.9)
This symbol (with prefactor N) accomplishes the same tasks the Î´-function had
when doing the continuous Fourier transformation. Equation(4.9) just means that, if
the hand goes completely round the clock, weâ€™ll get zero, as we can see immediately
by simply adding the handsâ€™ vectors in Fig.4.2, except if the hand stops at â€œ3:00
a.m.â€, a situation k = kâ€² can force. In this case we get N, as shown in Fig.4.3.

96
4
Discrete Fourier Transformation
real axis
imaginary axis
W 6
8
W 2
8
W 1
8
W 7
8
W 3
8
W 5
8
W 4
8
W 0
8
Fig. 4.2 Representation of W k
8 in the complex plane
W 0Â·0
8
W 0Â·1
8
W 0Â·2
8
W 0Â·3
8
W 0Â·4
8
W 0Â·5
8
W 0Â·6
8
W 0Â·7
8
Fig. 4.3 For N â†’âˆ(ï¬ctitious only) we quite clearly see the analogy with the Î´-function
4.1.3 Deï¬nition of the Discrete Fourier Transformation
Now we want to determine the spectral content {Fj} of the series { fk} using dis-
crete Fourier transformation. For this purpose, we have to make the transition in the
deï¬nition of the Fourier series:
c j = 1
T
+T/2

âˆ’T/2
f (t)eâˆ’2Ï€i j/T dt
(4.10)
with f (t) periodic in T :
c j = 1
N
Nâˆ’1

k=0
fkeâˆ’2Ï€i jk/N.
(4.11)
In the exponent we ï¬nd kt
Nt , meaning that t can be eliminated. The prefactor
contains the sampling raster t, so the prefactor becomes t/T = t/(Nt) =
1/N. During the transition from (4.10) to (4.11) we tacitly shifted the limits of the
interval from âˆ’T/2 to +T/2 to 0 to T , something that was okay, as we integrate over
an integer period and f (t) was assumed to be periodic in T . The sum has to come
to an end at N âˆ’1, as this sampling point plus t reaches the limit of the interval.
Therefore we get, for the discrete Fourier transformation:

4.1 Discrete Fourier Transformation
97
Deï¬nition 4.1 (Discrete Fourier transformation)
Fj = 1
N
Nâˆ’1

k=0
fkW âˆ’kj
N
with
WN = e2Ï€i/N.
(4.12)
The discrete inverse Fourier transformation is:
Deï¬nition 4.2 (Discrete inverse Fourier transformation)
fk =
Nâˆ’1

j=0
FjW +kj
N
with
WN = e2Ï€i/N.
(4.13)
Please note that the inverse Fourier transformation doesnâ€™t have a prefactor 1/N.
A bit of a warning is called for here. Instead of (4.12) and (4.13) we also come
across deï¬nition equations with positive exponents for the forward transformation
and with negative exponent for the inverse transformation (for example in â€œNumeri-
cal Recipesâ€ [7]). This doesnâ€™t matter as far as the real part of {Fj} is concerned. The
imaginary part of {Fj}, however, changes its sign. Because we want to be consistent
with the previous deï¬nitions of Fourier series and the continuous Fourier transfor-
mation weâ€™d rather stick with the deï¬nitions (4.12) and (4.13) and remember that, for
example, a negative, purely imaginary Fourier coefï¬cient Fj belongs to a positive
amplitude of a sine wave (given positive frequencies), as i of the forward transfor-
mation multiplied by i of the inverse transformation results in precisely a change of
sign i2 = âˆ’1. Often also the prefactor 1/N of the forward transformation is missing
(for example in â€œNumerical Recipesâ€ [7]). In view of the fact that F0 is to be equal
to the average of all samples, the prefactor 1/N really has to stay there, too. As weâ€™ll
see, also â€œParsevalâ€™s theoremâ€ will be grateful if we took care with our deï¬nition of
the forward transformation. Using relation (4.8) we can see straight away that the
inverse transformation (4.13) is correct:
fk =
Nâˆ’1

j=0
FjW +kj
N
=
Nâˆ’1

j=0
1
N
Nâˆ’1

kâ€²=0
fkâ€²W âˆ’kâ€² j
N
W +kj
N
(4.14)
= 1
N
Nâˆ’1

kâ€²=0
fkâ€²
Nâˆ’1

j=0
W (kâˆ’kâ€²) j
N
= 1
N
Nâˆ’1

kâ€²=0
fkâ€² NÎ´k,kâ€² = fk.
Before we get into more rules and theorems, letâ€™s look at a few examples to
illustrate the discrete Fourier transformation (Fig.4.4)!

98
4
Discrete Fourier Transformation
interval
interval
Fâˆ’2
Fâˆ’1
F0
F0
F1
F1
F2
F3
Fig. 4.4 Fourier coefï¬cients with negative indices are wrapped to the right end of the interval
Example 4.1 (â€œConstantâ€ with N = 4)
fk = 1
for k = 0, 1, 2, 3.
-
f0
f1
f2
f3
For the continuous Fourier transformation we expect a Î´-function with the fre-
quency Ï‰ = 0. The discrete Fourier transformation therefore will only result in
F0 Ì¸= 0. Indeed, we do get, using (4.12)â€”or even a lot smarter using (4.8):
F0 = 1
44 = 1
F1 = 0
F2 = 0
F3 = 0.
-
F0
F1
F2
F3
As { fk} is an even series, {Fj} contains no imaginary part. The inverse transfor-
mation results in:
fk = 1 cos

2Ï€ k
40

= 1
for k = 0, 1, 2, 3.
â†‘
j=0
Example 4.2 (â€œCosineâ€ with N = 4)
We get, using (4.12) and W4 = i:

4.1 Discrete Fourier Transformation
99
F0 = 0 (average = 0!)
F1 = 1
4(1 + (âˆ’1)(â€œ9:00 a.m.â€) = 1
4(1 + (âˆ’1)(âˆ’1)) = 1
2
F2 = 1
4(1 + (âˆ’1)(â€œ3:00 p.m.â€) = 1
4(1 + (âˆ’1)1)
= 0
F3 = 1
4(1 + (âˆ’1)(â€œ9:00 p.m.â€) = 1
4(1 + (âˆ’1)(âˆ’1)) = 1
2.
I bet you would have noticed that, due to the negative sign in the exponent in (4.12),
weâ€™re running around â€œclockwiseâ€. Maybe those of you whoâ€™d rather use a positive
sign here, are â€œBavariansâ€, who are well known for their clocks going backwards
(you can actually buy them in souvenir-shops). So whoever uses a plus sign in (4.12)
is out of sync with the rest of the world! Whatâ€™s F3 = 1/2? Is there another spectral
component, apart from the fundamental frequency Ï‰1 = 2Ï€Ã—1/4Ã—t = Ï€/(2t)?
Yes, there is! Of course itâ€™s the component with âˆ’Ï‰1, that has been wrapped-around.
We can see that the negative frequencies of FNâˆ’1 (corresponding to smallest, not
disappearing frequency Ï‰âˆ’1) are located from the right end of the interval decreasing
to the left till they reach the center of the interval.
For real input the following applies:
FNâˆ’j = Fâˆ—
j ,
(4.15)
as we can easily deduce from (4.12). So in the case of even input the right half has
exactly the same content as the left half; in the case of odd input, the right half will
contain the conjugate complex or the same times minus as the left half. If we add
together the intensity F1 and F3 = Fâˆ’1 shared â€œbetween brothersâ€, this results in 1,
as required by the input:
fk = 1
2ik + 1
2i3k = cos

2Ï€ k
4

for k = 0, 1, 2, 3.
Example 4.3 (â€œSineâ€ with N = 4)
Again we use (4.12) and get:

100
4
Discrete Fourier Transformation
F0 = 0
(average = 0)
F1 = 1
4(1 Ã— â€œ6.00 a.m.â€ +(âˆ’1) Ã— â€œ12.00 noonâ€) = 1
4(âˆ’i
+(âˆ’1) Ã— i)
= âˆ’i
2
F2 = 1
4(1 Ã— â€œ9.00 a.m.â€ +(âˆ’1) Ã— â€œ9.00 p.m.â€) = 1
4(1 Ã— (âˆ’1) +(âˆ’1)(âˆ’1)) =
0
F3 = 1
4(1 Ã— â€œ12.00 noonâ€ +(âˆ’1) Ã— â€œ6.00 a.m.â€

	

following day
) = 1
4(1 Ã— i
+(âˆ’1)(âˆ’i)) =
i
2
real part = 0
imaginary part:
-
F0
F1
F2
F3
âˆ’1
2
+ 1
2
If we add the intensity with a minus sign for negative frequencies, that resulted
from the sharing â€œbetween sistersâ€, to the one for positive frequencies, meaning F1+
(âˆ’1)F3 = âˆ’i, we get for the intensity of the sine wave (the inverse transformation
provides us with another i!) the value 1:
fk = âˆ’i
2ik + i
2i3k = sin

2Ï€ k
4

.
4.2 Theorems and Rules
4.2.1 Linearity Theorem
If we combine in a linear way { fk} and its series {Fj} with {gk} and its series {G j},
the we get:
{ fk} â†”{Fj},
{gk} â†”{G j},
a Â· { fk} + b Â· {gk} â†”a Â· {Fj} + b Â· {G j}.
(4.16)
Please always keep in mind that the discrete Fourier transformation contains only
linear operators (in fact, basic maths only), but that the power representation is no
linear operation.

4.2 Theorems and Rules
101
4.2.2 The First Shifting Rule (Shifting in the Time Domain)
{ fk} â†”{Fj}
{ fkâˆ’n} â†”{FjW âˆ’jn
N
},
n integer.
(4.17)
Ashiftinthetimedomainbyn resultsinamultiplicationbythephasefactor W âˆ’jn
N
.
Proof (First Shifting Rule)
Fshifted
j
= 1
N
Nâˆ’1

k=0
fkâˆ’nW âˆ’kj
N
= 1
N
Nâˆ’1âˆ’n

kâ€²=âˆ’n
fkâ€²W âˆ’(kâ€²+n) j
N
with k âˆ’n = kâ€²
= 1
N
Nâˆ’1

kâ€²=0
fkâ€²W âˆ’kâ€² j
N
W âˆ’nj
N
= Fold
j
W âˆ’nj
N
. â–¡
(4.18)
Because of the periodicity of fk, we may shift the lower and the upper summation
boundaries by n without a problem.
Example 4.4 (Shifted cosine with N = 2)
Now we shift the input by n = 1:

102
4
Discrete Fourier Transformation
4.2.3 The Second Shifting Rule (Shifting in the Frequency
Domain)
{ fk} â†”{Fj}
{ fkW âˆ’nk
N
} â†”{Fj+n},
n integer.
(4.19)
Amodulationinthetimedomainwith W âˆ’nk
N
correspondstoashiftinthefrequency
domain. The proof is trivial.
Example 4.5 (Modulated cosine with N = 2)
Now we modulate the input with W âˆ’nk
N
with n = 1, thatâ€™s W âˆ’k
2
= (âˆ’1)âˆ’k, and
get:
4.2.4 Scaling Rule/Nyquist Frequency
From Fig.4.5 we see that the highest frequency Ï‰max or also âˆ’Ï‰max corresponds to
the center of the series of Fourier coefï¬cients. This we get by inserting j = N/2 in
(4.6):
Î©Nyq = Ï€
t
â€œNyquist frequencyâ€.
(4.20)
This frequency often is also called the cut-off frequency. If we take a sample, say
every Âµs (t = 10âˆ’6 s), then Î©Nyq is 3.14 megaradians/s (if you prefer to think in
frequencies instead of angular frequencies: Î½Nyq = Î©Nyq/2Ï€, so here 0.5MHz).

4.2 Theorems and Rules
103
F0
FN/2
FNâˆ’1
positive
frequencies
frequencies
negative
coeï¬ƒcients for
Fig. 4.5 Positioning of the Fourier coefï¬cients
Fig. 4.6 Two samples per period: cosine (left); sine (right)
So the Nyquist frequency Î©Nyq corresponds to taking two samples per period, as
shown in Fig.4.6.
While weâ€™ll get away with this in the case of the cosine, by the skin of our
teeth, it deï¬nitely wonâ€™t work for the sine! Here we grabbed the samples at the
wrong moment, or maybe there was no signal after all (for example because a cable
hadnâ€™t been plugged in, or due to a power cut). In fact, the imaginary part of fk at
the Nyquist frequency always is 0. The Nyquist frequency therefore is the highest
possible spectral component for a cosine wave; for the sine itâ€™s only up to:
Ï‰ = 2Ï€(N/2 âˆ’1)/(Nt) = Î©Nyq(1 âˆ’2/N).
Equation(4.20) is our scaling theorem, as the choice of t allows us to stretch or
compress the time axis, while keeping the number of samples N constant. This only
has an impact on the frequency scale running from Ï‰ = 0 to Ï‰ = Î©Nyq. t doesnâ€™t
appear anywhere else!
The normalisation factor we came across in (1.41) and (2.32), is done away with
here, as using discrete Fourier transformation we normalise to the number of samples
N, regardless of the sampling raster t.

104
4
Discrete Fourier Transformation
4.3 Convolution, Cross Correlation, Autocorrelation,
Parsevalâ€™s Theorem
Before weâ€™re able to formulate the discrete versions of the (2.34), (2.48), (2.52), and
(2.54), we have to get a handle on two problems:
i. The number of samples N for the two functions f (t) and g(t) we want to convolve
or cross-correlate, must be the same. This often is not the case, for example if f (t)
is the â€œtheoreticalâ€ signal we would get for a Î´-shaped instrumental resolution
function, which, however, has to be convolved with the ï¬nite resolution function
g(t). Thereâ€™s a simple ï¬x: we pad the series {gk} with zeros so we get N samples,
just like in the case of series { fk}.
ii. Donâ€™t forget, that { fk} is periodic in N and our â€œpaddedâ€ {gk}, too. This means
that negative indices are wrapped-around to the right end of the interval. The
resolution function g(t) mentioned in Fig.4.7, which we assumed to be sym-
metrical, had 3 samples and got padded with 5 zeros to a total of N = 8 and is
displayed in Fig.4.7.
Another extreme example:
Example 4.6 (Rectangle) Weâ€™ll remember that a continuous â€œrectangular functionâ€,
when convolved with itself in the interval âˆ’T/2 â‰¤t â‰¤+T/2, results in a â€œtriangular
functionâ€ in the interval âˆ’T â‰¤t â‰¤+T . In the discrete case, the â€œtriangleâ€ gets
wrapped in the area âˆ’T â‰¤t â‰¤âˆ’T/2 to 0 â‰¤t â‰¤T/2. The same happens to the
â€œtriangleâ€ in the area +T/2 â‰¤t â‰¤+T , which gets wrapped to âˆ’T/2 â‰¤t â‰¤0.
Therefore both halves of the interval are â€œcorruptedâ€ by the wrap-around, so that the
end-result is another constant (cf. Fig.4.8). No wonder! This â€œrectangular functionâ€
with periodic continuation is a constant! And a constant convolved with a constant
naturally is another constant.
As long as { fk} is periodic in N, thereâ€™s nothing wrong with the fact that upon
convolution data from the end/beginning of the interval will be â€œmixed intoâ€ data
from the beginning/end of the interval. If you donâ€™t like thatâ€”for whatever reasonsâ€”
rather also pad { fk} with zeros, using precisely the correct number of zeros so {gk}
wonâ€™t create overlap between f0 and fNâˆ’1 any more.
interval
interval
g0
g0
g1
g1
gâˆ’1
g7
Fig. 4.7 Resolution function {gk}: without wrap-around (left); with wrap-around (right)

4.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
105
âŠ—
=
âˆ’T
2
âˆ’T
2
+ T
2
+ T
2
âˆ’T
+ T
âˆ’T âˆ’T
2
+T
0
+ T
2
âˆ’T
2
+ T
2
Fig. 4.8 Convolution of a â€œrectangular functionâ€ with itself: without wrap-around (top); with wrap-
around (bottom)
4.3.1 Convolution
Weâ€™ll deï¬ne the discrete convolution as follows:
Deï¬nition 4.3 (Discrete convolution)
hk â‰¡( f âŠ—g)k = 1
N
Nâˆ’1

l=0
flgkâˆ’l.
(4.21)
The â€œconvolution sumâ€ is commutative, distributive and associative. The normal-
isation factor 1/N in context: the convolution of { fk} with the â€œdiscrete Î´-functionâ€
{gk} = NÎ´k,0 is to leave the series { fk} unchanged. Following this rule, also a
â€œnormalisedâ€ resolution function {gk} should respect the condition Nâˆ’1
k=0 gk = N.
Unfortunately often the convolution also gets deï¬ned without the prefactor 1/N.
The Fourier transform of {hk} is:
Hj = 1
N
Nâˆ’1

k=0
1
N
Nâˆ’1

l=0
flgkâˆ’lW âˆ’kj
N
=
1
N 2
Nâˆ’1

k=0
Nâˆ’1

l=0
flW âˆ’lj
N gkâˆ’lW âˆ’kj
N
W +lj
N
â†‘
extended
â†‘
(4.22)
=
1
N 2
Nâˆ’1

l=0
flW âˆ’lj
N
Nâˆ’1âˆ’l

kâ€²=âˆ’l
gkâ€²W âˆ’kâ€² j
N
with kâ€² = k âˆ’l
= FjG j.

106
4
Discrete Fourier Transformation
In our last step we took advantage of the fact that, due to the periodicity in N,
the second sum may also run from 0 to N âˆ’1 instead of âˆ’l to N âˆ’1 âˆ’l. This,
however, makes sure that the current index l has been totally eliminated from the
second sum, and we get the product of the Fourier transform Fj and G j. So we arrive
at the discrete Convolution Theorem:
{ fk} â†”

Fj

,
{gk} â†”

G j

,
{hk} = {( f âŠ—g)k} â†”

Hj

=

Fj Â· G j

.
(4.23)
The convolution of the series { fk} and {gk} results in a product in the Fourier
space.
The inverse Convolution Theorem is:
{ fk} â†”

Fj

,
{gk} â†”

G j

,
{hk} = { fk Â· gk} â†”

Hj

=

N(F âŠ—G) j

.
(4.24)
Proof (Inverse Convolution Theorem)
Hj = 1
N
Nâˆ’1

k=0
fkgkW âˆ’kj
N
= 1
N
Nâˆ’1

k=0
fkgk
Nâˆ’1

kâ€²=0
W âˆ’kâ€² j
N
Î´k,kâ€²

	

kâ€²-sum â€œartiï¬ciallyâ€ introduced
=
1
N 2
Nâˆ’1

k=0
fk
Nâˆ’1

kâ€²=0
gkâ€²W âˆ’kâ€² j
N
Nâˆ’1

l=0
W âˆ’l(kâˆ’kâ€²)
N

	

l-sum yields NÎ´k,kâ€²
=
Nâˆ’1

l=0
1
N
Nâˆ’1

k=0
fkW âˆ’lk
N
1
N
Nâˆ’1

kâ€²=0
gkâ€²W âˆ’kâ€²( jâˆ’l)
N
=
Nâˆ’1

l=0
FlG jâˆ’l = N(F âŠ—G) j. â–¡

4.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
107
Example 4.7 (Nyquist frequency with N = 8)
{ fk} = {1, 0, 1, 0, 1, 0, 1, 0},
{gk} = {4, 2, 0, 0, 0, 0, 0, 2}.
-
-
The â€œresolution functionâ€ {gk} is padded to N = 8 with zeros and normalised to
7
k=0 gk = 8. The convolution of { fk} with {gk} results in:
{hk} =
1
2, 1
2, 1
2, 1
2, 1
2, 1
2, 1
2, 1
2

,
meaning, that everything gets â€œï¬‚attenedâ€, because the resolution function (here
triangle-shaped) has a full half-width of t and consequently doesnâ€™t allow the
recording of oscillations with the period t. The Fourier transform therefore is
Hk = 1/2Î´k,0. Using the Convolution Theorem (4.23) we would get:
{Fj} =
1
2, 0, 0, 0, 1
2, 0, 0, 0

.
The result is easy to understand: the average is 1/2, at the Nyquist frequency we
have 1/2, all other elements are 0.
G0 = 1
1
8 Ã— average

G1 = 1
2 +
âˆš
2
4
1
8{4 + 2 Ã— â€œ4:30 a.m.â€ + 2 Ã— â€œ1:30 p.m.â€}

G2 = 1
2
1
8{4 + 2 Ã— â€œ6:00 a.m.â€ + 2 Ã— â€œ12:00 midnightâ€}

G3 = 1
2 âˆ’
âˆš
2
4
1
8{4 + 2 Ã— â€œ7:30 a.m.â€ + 2 Ã— â€œ10:30 a.m. next dayâ€}

G4 = 0
1
8{4 + 2 Ã— â€œ9:00 a.m.â€ + 2 Ã— â€œ9:00 p.m. next dayâ€}


108
4
Discrete Fourier Transformation
G5 = 1
2 âˆ’
âˆš
2
4
G6 = 1
2
G7 = 1
2 +
âˆš
2
4
â«
âªâªâªâªâªâªâªâ¬
âªâªâªâªâªâªâªâ­
because of real input,
hence:
{G j} =

1, 1
2 +
âˆš
2
4 , 1
2, 1
2 âˆ’
âˆš
2
4 , 0, 1
2 âˆ’
âˆš
2
4 , 1
2, 1
2 +
âˆš
2
4

.
For the product we get Hj = FjG j = {1/2, 0, 0, 0, 0, 0, 0, 0}, like we
should for the Fourier transform. If weâ€™d taken the Convolution Theorem seriously
right from the beginning, then the calculation of G0 (average) and G4 at the Nyquist
frequency would have been quite sufï¬cient, as all other Fj = 0. The fact that the
Fourier transform of the resolution function for the Nyquist frequency is 0, precisely
means that with this resolution function weâ€™re not able to record oscillations with the
Nyquist frequency any more. Our inputs, however, were only the frequency 0 and
the Nyquist frequency.
4.3.2 Cross Correlation
We deï¬ne for the discrete cross correlation between { fk} and {gk}, similar to what
we did in (2.48):
Deï¬nition 4.4 (Discrete cross correlation)
hk â‰¡( f â‹†g)k = 1
N
Nâˆ’1

l=0
fl Â· gâˆ—
l+k.
(4.25)
If the indices at gk go beyond N âˆ’1, then weâ€™ll simply subtract N (periodicity).
The cross correlation between { fk} and {gk}, of course, results in a product of their
Fourier transforms:
{ fk} â†”

Fj

,
{gk} â†”

G j

,
{hk} = {( f â‹†g)k} â†”

Hj

=

Fj Â· Gâˆ—
j

.
(4.26)
Proof (Discrete cross correlation)
Hj = 1
N
Nâˆ’1

k=0
1
N
Nâˆ’1

l=0
flgâˆ—
l+kW âˆ’kj
N

4.3 Convolution, Cross Correlation, Autocorrelation, Parsevalâ€™s Theorem
109
= 1
N
Nâˆ’1

l=0
fl
1
N
Nâˆ’1

k=0
gâˆ—
l+kW âˆ’kj
N
with the First Shifting Rule and complex conjugate
= 1
N
Nâˆ’1

l=0
flGâˆ—
jW âˆ’jl
N
= FjGâˆ—
j.
â–¡
4.3.3 Autocorrelation
Here we have { fk} = {gk}, which leads to:
hk â‰¡( f â‹†f )k = 1
N
Nâˆ’1

l=0
fl Â· f âˆ—
l+k
(4.27)
and:
{ fk} â†”

Fj

,
{hk} = {( f â‹†f )k} â†”

Hj

=

|Fj|2
.
(4.28)
In other words: the Fourier transform of the autocorrelation of { fk} is the modulus
squared of the Fourier series {Fj} or its power representation.
4.3.4 Parsevalâ€™s Theorem
We use (4.27) for k = 0, thatâ€™s h0 (â€œwithout time-lagâ€), and get on the one side:
h0 = 1
N
Nâˆ’1

l=0
| fl|2.
(4.29)
On the other side, the inverse transformation of {Hj}, especially for k = 0, results
in (cf. 4.13):
h0 =
Nâˆ’1

j=0
|Fj|2.
(4.30)

110
4
Discrete Fourier Transformation
Put together, this gives us the discrete version of Parsevalâ€™s theorem:
1
N
Nâˆ’1

l=0
| fl|2 =
Nâˆ’1

j=0
|Fj|2.
(4.31)
Example 4.8 (â€œParsevalâ€™s theoremâ€ for N = 2)
{ fl} = {0, 1}
(cf. example for First Shifting Rule
Sect. 4.2.2)
{Fj} = {1/2, âˆ’1/2} (here there is only the average F0
and the Nyquist frequency at F1!)
1
2
N

l=0
| fl|2 = 1
2 Ã— 1 = 1
2
N

j=0
|Fj|2 = 1
4 + 1
4 = 1
2.
Caution: Often the prefactor 1/N gets left out when deï¬ning Parsevalâ€™s theorem.
To stay consistent with all other deï¬nitions, however, it should not be missing here!
4.4 The Sampling Theorem
When discussing the Nyquist frequency, we already mentioned that we need at least
two samples per period to show cosine oscillations at the Nyquist frequency. Now
weâ€™ll turn the tables and claim that as a matter of principle we wonâ€™t be looking at
anything but functions f (t) that are â€œbandwidth-limitedâ€, meaning, that outside the
interval [âˆ’Î©Nyq, Î©Nyq] their Fourier transforms F(Ï‰) are 0. In other words: weâ€™ll
reï¬ne our sampling to a degree where we just manage to capture all the spectral
components of f (t). This seemingly â€œinnocentâ€ requirement implies that the peri-
odically continued function and all its derivatives are continuous everywhere, such
that its Fourier series is ï¬nite. Now weâ€™ll skilfully â€œmarryâ€ formulas weâ€™ve learned
when dealing with the Fourier series expansion and the continuous Fourier transfor-
mation with each other, and then pull the sampling theorem out of the hat. For this
purpose weâ€™ll recall (1.26) and (1.27) which show that a periodic function f (t) can
be expanded into an (inï¬nite) Fourier series:

4.4 The Sampling Theorem
111
f (t) =
+âˆ

k=âˆ’âˆ
Ckei2Ï€kt/T
with Ck = 1
T
T/2

âˆ’T/2
f (t)eâˆ’i2Ï€kt/T dt.
Since F(Ï‰) is zero outside [âˆ’Î©Nyq, Î©Nyq] we can continue this function periodi-
cally and expand it into an inï¬nite Fourier series. So we replace: f (t) â†’F(Ï‰), t â†’
Ï‰, T/2 â†’Î©Nyq and get:
F(Ï‰) =
+âˆ

k=âˆ’âˆ
CkeiÏ€kÏ‰/Î©Nyq
(4.32)
with Ck =
1
2Î©Nyq
+Î©Nyq

âˆ’Î©Nyq
F(Ï‰)eâˆ’iÏ€kÏ‰/Î©NyqdÏ‰.
A similar integral also occurs in the deï¬ning equation for the inverse continuous
Fourier transformation:
f (t) = 1
2Ï€
+Î©Nyq

âˆ’Î©Nyq
F(Ï‰)eiÏ‰tdÏ‰.
(4.33)
The integrations boundaries are Â±Î©Nyq, as F(Ï‰) is bandwidth-limited. When we
compare this with (4.32) we get:
2Î©NyqCk = 2Ï€ f (âˆ’Ï€k/Î©Nyq).
(4.34)
Once weâ€™ve inserted this in (4.32) we get:
F(Ï‰) =
Ï€
Î©Nyq
+âˆ

k=âˆ’âˆ
f (âˆ’Ï€k/Î©Nyq)eiÏ€kÏ‰/Î©Nyq.
(4.35)
When we ï¬nally insert this into the deï¬ning equation (4.33), we get:
f (t) = 1
2Ï€
+Î©Nyq

âˆ’Î©Nyq
Ï€
Î©Nyq
+âˆ

k=âˆ’âˆ
f

âˆ’Ï€k
Î©Nyq

eiÏ€kÏ‰/Î©NyqeiÏ‰tdÏ‰

112
4
Discrete Fourier Transformation
=
1
2Î©Nyq
+âˆ

k=âˆ’âˆ
f (âˆ’kt)2
+Î©Nyq

0
cos Ï‰(t + kt)dÏ‰
(4.36)
=
1
2Î©Nyq
+âˆ

k=âˆ’âˆ
f (âˆ’kt)2sin Î©Nyq(t + kt)
(t + kt)
.
By replacing k â†’âˆ’k (itâ€™s not important in which order the sums are calculated)
we get the Sampling Theorem:
Sampling Theorem: f (t) =
+âˆ

k=âˆ’âˆ
f (kt)sin Î©Nyq(t âˆ’kt)
Î©Nyq(t âˆ’kt) .
(4.37)
In other words, we can reconstruct the function f (t) for all times t from the
samples at the times kt, provided the function f (t) is â€œbandwidth-limitedâ€, i.e.
it contains no frequencies above Î©Nyq = Ï€/Î”t. To achieve this, we only need to
multiply f (kt) with the function sin x
x
(with x = Î©Nyq(t âˆ’kt)) and sum up over
all samples. The factor sin x
x
naturally is equal to 1 for t = kt, for other times, sin x
x
decays and slowly oscillates towards zero, which means, that f (t) is a composite
of plenty of
 sin x
x

-functions at the location t = kt with the amplitude f (kt).
Note that for adequate sampling with t =
Ï€
Î©Nyq each kâˆ’term in the sum in (4.37)
contributes f (kt) at the sampling points t = kt and zero at all other sampling
points whereas all terms contribute to the interpolation between sampling points.
Example 4.9 (Sampling Theorem with N = 2)
We expect:
f (t) = 1
2 + 1
2 cos Î©Nyqt = cos2 Î©Nyqt
2
.
The sampling theorem tells us:
f (t) =
+âˆ

k=âˆ’âˆ
fk
sin Î©Nyq(t âˆ’kt)
Î©Nyq(t âˆ’kt)
with fk = Î´k,even (with periodic continuation)
= sin Î©Nyqt
Î©Nyqt
+
+âˆ

l=1
sin Î©Nyq(t âˆ’2lt)
Î©Nyq(t âˆ’2lt)
+
+âˆ

l=1
sin Î©Nyq(t + 2lt)
Î©Nyq(t + 2lt)
with the substitution k = 2l

4.4 The Sampling Theorem
113
= sin Î©Nyqt
Î©Nyqt
+
+âˆ

l=1

sin 2Ï€

t
2t âˆ’l

2Ï€

t
2t âˆ’l

+ sin 2Ï€

t
2t + l

2Ï€

t
2t + l


with Î©Nyqt = Ï€
= sin Î©Nyqt
Î©Nyqt
+ 1
2Ï€
+âˆ

l=1

t
2t + l

sin Î©Nyqt +

t
2t âˆ’l

sin Î©Nyqt

t
2t âˆ’l
 
t
2t + l

= sin Î©Nyqt
Î©Nyqt
+ sin Î©Nyqt
2Ï€
2t
2t
+âˆ

l=1
1

t
2t
2 âˆ’l2
= sin Î©Nyqt
Î©Nyqt
â›
âœâ1 +
Î©Nyqt
2Ï€
2
2
+âˆ

l=1
1
 Î©Nyqt
2Ï€
2
âˆ’l2
â
âŸâ 
with [8, No 1.421.3]
= sin Î©Nyqt
Î©Nyqt
Ï€ Î©Nyqt
2Ï€
cot Ï€Î©Nyqt
2Ï€
= sin Î©Nyqt 1
2
cos(Î©Nyqt/2)
sin(Î©Nyqt/2)
= 2 sin(Î©Nyqt/2) cos(Î©Nyqt/2)1
2
cos(Î©Nyqt/2)
sin(Î©Nyqt/2) = cos2 
Î©Nyqt/2

. (4.38)
Please note that we actually do need all summation terms of k = âˆ’âˆto k = +âˆ!
If we had only taken k = 0 and k = 1 into consideration, we would have got:
f (t) = 1sin Î©Nyqt
Î©Nyqt
+ 0sin Î©Nyq(t âˆ’t)
Î©Nyq(t âˆ’t)
= sin Î©Nyqt
Î©Nyqt
which would not correspond to the input of cos2(Î©Nyqt/2). We still would have,
as before, f (0) = 1 and f (t = t) = 0, but for 0 < t < t, we wouldnâ€™t have
interpolated correctly, as sin x
x
slowly decays for big x, while we, however, want to get
a periodic oscillation that doesnâ€™t decay as input. You will realise, that the sampling
theoremâ€”similar to Parsevalâ€™s equation (1.50)â€”is good for the summation of certain
inï¬nite series.
What happens if, for some reason or other, our sampling happens to be too coarse
and F(Ï‰) above Î©Nyq was unequal to 0? Quite simple: the spectral density above
Î©Nyq will be â€œreï¬‚ectedâ€ to the interval 0 â‰¤Ï‰ â‰¤Î©Nyq, meaning that the true spectral
density gets â€œcorruptedâ€ by the part that would be outside the interval.
Example 4.10 (Not enough samples) Weâ€™ll take a cosine input and a bit less than
two samples per period (cf. Fig.4.9).

114
4
Discrete Fourier Transformation
F0
F0
F1
F1
F2
F2
F3
F3
Î©N
Î©N
â€œcorrectâ€
â€œwrongâ€
Fig. 4.9 Less than two samples per period (top): cosine input (solid line); â€œapparentlyâ€ lower
frequency (dotted line). Fourier coefï¬cients with wrap-around (bottom)
Î©Nyq
Î©Nyq
2Î©Nyq
2Î©Nyq
â€œcorrectâ€
â€œwrongâ€
Fig. 4.10 Slightly more than one sample per period (top): cosine input (solid line); â€œapparentlyâ€
lower frequency (dotted line). Fourier coefï¬cients with wrap-around (bottom)
Here there are eight samples for ï¬ve periods, and that means that Î©Nyq has been
exceeded by 25 %. The broken line in Fig.4.9 shows that a function with only three
periods would produce the same samples within the same interval.
Therefore the discrete Fourier transformation will show a lower spectral compo-
nent, namely at Î©Nyq âˆ’25 %. This will become quite obvious, indeed, when we use
only slightly more than one sample per period.
Here {Fj} produces only a very low-frequency component (cf. Fig.4.10). In other
words: spectral density that would appear at â‰ˆ2Î©Nyq, appears at Ï‰ â‰ˆ0! This â€œcor-
ruptionâ€ of the spectral density through insufï¬cient sampling is called â€œaliasingâ€,
similar to someone acting under an assumed name. In a nutshell: When sampling,

4.4 The Sampling Theorem
115
rather err on the ï¬ne side than the coarse one! Coarser rasters can always be achieved
later on by compressing data sets, but it will never work the other way round!
4.5 Data Mirroring
Often we have a situation where, on top of the samples { fk}, we also know that the
series starts with f0 = 0 or at f0 with horizontal tangent ( âˆ§= slope = 0). In this
case we should use data mirroring forcing a situation where the input is an odd or an
even series (cf. Fig.4.11):
odd:
f2Nâˆ’k = âˆ’fk
k = 0, 1, . . . , N âˆ’1,
here we put fN = 0;
even:
f2Nâˆ’k = + fk
k = 0, 1, . . . , N âˆ’1,
here fN is undetermined!
(4.39)
For odd series we put fN = 0, as would be the case for periodic continuation
anyway. For even series this is not necessarily the case. A possibility to determine
fN would be fN = f0 (as if we wanted to continue the non-mirrored data set
periodically). In our example of Fig.4.11 this would result in a Î´-spike at fN, which
wouldnâ€™t make sense. Equally, in our example fN = 0 canâ€™t be used (another Î´-
spike!). A better choice would be fN = fNâˆ’1, and even better fN = âˆ’f0 for the
present case. The optimum choice, however, depends on the respective problem. So,
for example, in the case of a cosine with window function and subsequently plenty
of zeros, fN = 0 would be the correct choice (cf. Fig.4.12).
Now the interval is twice as long! Apply the normal fast Fourier transformation
and youâ€™ll have a lot of fun with it, even if (or maybe exactly because of it?) the
real part (in the case of odd mirroring) or the imaginary part (in the case of even
odd:
0
N âˆ’1
âˆ’(N âˆ’1)
0
N âˆ’1
0
2N âˆ’1
â€œwrap-aroundâ€
even:
0
N âˆ’1
âˆ’(N âˆ’1)
0
N âˆ’1
0
2N âˆ’1
â€œwrap-aroundâ€
Fig. 4.11 Odd/even input, forced by data mirroring

116
4
Discrete Fourier Transformation
0
N âˆ’1
0
fN
2N âˆ’1
Fig. 4.12 Example for the choice of fN
Ï€
2Ï€
1
âˆ’1
Ï€
2Ï€
k = 0
k = 1
k = 2
k = 3
Fig. 4.13 Basis functions for cosine- (left) and for sine-transformation (right)
mirroring) is full of zeros. If you donâ€™t like that, use a more efï¬cient algorithm using
the fast sine- or cosine-transformation.
As we can see in Fig.4.13, for these sine- or cosine-transformations other basis
functions are being used than the fundamental and harmonics of the normal Fourier
transform, to model the input: also all functions with half the period will occur (the
second half models the mirror image). The normal Fourier transformation of the
mirrored input reads:
Fj =
1
2N
2Nâˆ’1

k=0
fkW âˆ’kj
2N
=
1
2N
#Nâˆ’1

k=0
fkW âˆ’kj
2N +
2Nâˆ’1

k=N
fkW âˆ’kj
2N
$
=
1
2N
#Nâˆ’1

k=0
fkW âˆ’kj
2N +
1

kâ€²=N
f2Nâˆ’kâ€²W âˆ’(2Nâˆ’kâ€²) j
2N
$
sequence irrelevant
=
1
2N
#Nâˆ’1

k=0
fkW âˆ’kj
2N +
N

kâ€²=1
(Â±) fkâ€² W âˆ’2N j
2N
 	 
 W +kâ€² j
2N
$
for
even
odd

= eâˆ’2Ï€i 2N j
2N = 1
=
1
2N
 1
âˆ’i
 Nâˆ’1

k=0
fk Ã— 2
#
cos 2Ï€kj
2N
sin 2Ï€kj
2N
$
+ fNeâˆ’iÏ€ j âˆ’f0

=
â§
âªâªâªâªâ¨
âªâªâªâªâ©
1
N
Nâˆ’1

k=0
fk cos Ï€kj
N
+ 1
2N

fNeâˆ’iÏ€ j âˆ’f0

even
âˆ’i
N
Nâˆ’1

k=0
fk sin Ï€kj
N
odd
.

4.5 Data Mirroring
117
The expressions (1/N) Nâˆ’1
k=0 fk cos(Ï€kj/N) and (1/N) Nâˆ’1
k=0 fk sin(Ï€kj/N)
are called cosine- and sine-transformation. Please note:
i. The arguments for the cosine-/sine-function are Ï€kj/N and not 2Ï€kj/N! This
shows, that half periods as basis function are also allowed (cf. Fig.4.13).
ii. In the case of the sine transformation shifting of the sine boundaries from kâ€² =
1, 2, . . . , N towards kâ€² = 0, 1, . . . , N âˆ’1 is no problem, as the following has
to be true: fN = f0 = 0. Apart from the factor âˆ’i the sine transformation is
identical to the normal Fourier transformation of the mirrored input, though it
only has half as many coefï¬cients. The inverse sine transformation is identical
to the forward transformation, with the exception of the normalisation.
iii. In the case of the cosine transformation, the terms (1/2N)( fNeâˆ’iÏ€ j âˆ’f0) stay,
except if they happen to be equal to 0. That means, that generally the cosine
transformation will not be identical to the normal Fourier transformation of the
mirrored input! (Fig.4.14).
iv. Obviously Parsevalâ€™s theorem does not apply to the cosine transformation.
v. Obviously the inverse cosine transformation is not identical to the forward trans-
formation, apart from factors.
Example 4.11 (â€œConstantâ€, N = 4)
{ fk} = 1
for all k.
The normal Fourier transformation of the mirrored input is:
F0 = 1
88 = 1,
all other Fj = 0.
f0
f0
f1
f1
f2
f2
f3
f3
f4
f5
f6
f7
best choice is f4 = 1
Fig. 4.14 Input without mirroring (left); with mirroring (right)

118
4
Discrete Fourier Transformation
Cosine transformation:
Fj = 1
4
3

k=0
cos Ï€kj
4
=
â§
â¨
â©
1
44 = 1 for j = 0
1
4Î´ j,odd for j Ì¸= 0
.
Here the ï¬‚ip-side is that, because of cos(Ï€kj/N), the clock-hand or its projection
ontotherealaxisonlyrunaroundhalfasfast,andconsequentlyrelation (4.8)becomes
false.
The extra terms can be omitted only if f0 = fN = 0 is true, as for example in
Fig.4.15.
If you insist on using the cosine transformation, â€œcorrectâ€ it using the term:
1
2N ( fNeâˆ’iÏ€ j âˆ’f0).
Then you get the normal Fourier transformation of the mirrored data set, and no harm
was done. In our above example, the one with the constant input, this would look as
shown in Fig.4.16.
N =4
Fig. 4.15 Input (left); with correct mirroring (right)
cosine
transformation:
1
4
1
4
F0
F1
F2
F3
1
+
+ 1
8 1eâˆ’iÏ€j âˆ’1
= âˆ’1
4Î´j,odd
âˆ’1
4
âˆ’1
4
=
1
Fig. 4.16 Cosine transformation with correcting terms

4.6 How to Get Rid of the â€œStraight-Jacketâ€ Periodic Continuation? â€¦
119
4.6 How to Get Rid of the â€œStraight-Jacketâ€ Periodic
Continuation? By Using Zero-Padding!
So far, we had chosen all our examples in a way where { fk} could be continued
periodically without a problem. For example, we truncated a cosine precisely where
there was no problem continuing the cosine-shape periodically. In practice, this often
canâ€™t be done:
i. weâ€™d have to know the period in the ï¬rst place to be able to know where to
truncate and where not;
ii. if there are several spectral components, weâ€™d always cut off a component at the
wrong time (for the purists: except if the sampling interval can be chosen to be
equal to the smallest common denominator of the single periods).
Example 4.12 (Truncation) See what happens for N = 4:
Without truncation error:
With maximum truncation error:

120
4
Discrete Fourier Transformation
W4 = eiÏ€/2
F0 = 1
4
(average)
F1 = 1
4

1 +

âˆ’1
âˆš
2

Ã— â€œ6:00 a.m.â€ +

+ 1
âˆš
2

Ã— â€œ12:00 noonâ€

= 1
4

1 +
i
âˆš
2
+
i
âˆš
2

= 1
4 +
i
2
âˆš
2
F2 = 1
4

1 +

âˆ’1
âˆš
2

Ã— â€œ9:00 a.m.â€ +

+ 1
âˆš
2

Ã— â€œ9:00 p.m.â€

= 1
4

1 + 1
âˆš
2
âˆ’1
âˆš
2

= 1
4
F3 = Fâˆ—
1 .
(4.40)
Two â€œstrange ï¬ndingsâ€:
i. Through truncation we suddenly got an imaginary part, in spite of using a cosine
as input. But our function isnâ€™t even at all, because we continue using fN = âˆ’1,
instead of fN = f0 = +1, as we originally intended to do. This function contains
an even and an odd portion (cf. Fig.4.17).
ii. We really had expected a Fourier coefï¬cient between half the Nyquist frequency
and the Nyquist frequency, possibly spread evenly over F1 and F2, and not a
constant, like we would have had to expect for the case of a Î´-function as input:
but weâ€™ve precisely entered this as â€œevenâ€ input.
The â€œoddâ€ input is a sine wave with amplitude âˆ’1/
âˆš
2 and therefore results in an
imaginary part of F1 = 1/2
âˆš
2; the intensity âˆ’1/2
âˆš
2, split â€œbetween sistersâ€, is to
be found at F3, the positive sign in front of Im{F1} means negative amplitude (cf.
the remarks in (4.14) about Bavarian clocks).
Instead of more discussions about truncation errors in the case of cosine inputs, we
recall that Ï‰ = 0 is a frequency â€œas good as anyâ€. So we want to discuss the discrete
analog to the function sin x
x , the Fourier transform of the â€œrectangular functionâ€. We
use as input:
=
1
0
âˆ’1
âˆš
2
+ 1
âˆš
2
even
+
odd
Fig. 4.17 Decomposition of the input into an even and an odd portion

4.6 How to Get Rid of the â€œStraight-Jacketâ€ Periodic Continuation? â€¦
121
fk =
â§
â¨
â©
1 for 0 â‰¤k â‰¤M
0 else
1 for N âˆ’M â‰¤k â‰¤N âˆ’1
(4.41)
and stick with period N. This corresponds to a â€œrectangular windowâ€ of width 2M+1
(M arbitrary, yet < N/2). Here, the half corresponding to negative times has been
wrapped onto the right end of the interval. Please note, that we canâ€™t help but require
an odd number of fk Ì¸= 0 to get an even function. An example with N = 8, M = 2
is shown in Fig.4.18.
For general M < N/2 and N the Fourier transform is:
Fj = 1
N
# M

k=0
W âˆ’kj
N
+
Nâˆ’1

k=Nâˆ’M
W âˆ’kj
N
$
= 1
N
#
2
M

k=0
cos(2Ï€kj/N) âˆ’1
$
.
The sum can be calculated using (1.53), which we came across when dealing with
Dirichletâ€™s integral kernel. We have:
1
2 + 1
2 + cos x + cos 2x + Â· Â· Â· + cos Mx = 1
2 +
sin

M + 1
2

x
2 sin x
2
with x = 2Ï€ j/N,
thus:
Fj = 1
N
â›
âœâœâ1 +
sin

M + 1
2
 2Ï€ j
N
sin 2Ï€ j
2N
âˆ’1
â
âŸâŸâ = 1
N
â›
âœâ
sin 2M + 1
N
Ï€ j
sin Ï€ j
N
â
âŸâ 
(4.42)
for j = 0, . . . , N âˆ’1.
f0
fâˆ’2
fâˆ’1
f1
f2
f3
f4
f5
f6
f7
f8
Fig. 4.18 â€œRectangularâ€ input using N = 8, M = 2

122
4
Discrete Fourier Transformation
j
Fig. 4.19 Equation(4.42) (points); 2M+1
N
sin x
x
with x = 2M+1
N
Ï€ j (thin line)
This is the discrete version of the function sin x
x
which we would get in the
case of the continuous Fourier transformation (cf. Fig.2.1 for our above example).
Figure4.19 shows the result for N = 64 and M = 8 in comparison to sin x
x .
What happens at j = 0? Thereâ€™s a trick: j temporarily is treated like a continuous
variable and lâ€™Hospitalâ€™s rule is applied:
F0 = 1
N
2M + 1
N

Ï€
Ï€/N
= 2M + 1
N
â€œaverageâ€.
(4.43)
We had used 2M + 1 series elements fk = 1 as input. Only in this range the
denominator can become 0.
Where are the zeros of the discrete Fourier transform of the discrete â€œrectangular
windowâ€? Funny, there is no Fj, that is exactly equal to 0, as 2M+1
N
Ï€ j = lÏ€,
l = 1, 2, . . . or j = l
N
2M+1 and j = integer can only be achieved for l = 2M +1, and
then j already is outside the interval. Of course, for M â‰«1 we may approximately
put j â‰ˆl N
2M and then get 2Mâˆ’1 â€œquasi-zero transitionsâ€. This is different compared
to the function sin x
x , where there are real zeros. The oscillations around zero next to
the central peak at j = 0 decay only very slowly; even worse, after j = N/2 the
denominator starts getting smaller, and the oscillations increase again! Donâ€™t panic:
in the right half of {Fj} there is the mirror image of the left half! Whatâ€™s behind
the difference to the function sin x
x ? Itâ€™s the periodic continuation in the case of the
discrete Fourier transformation! We transform a â€œcombâ€ of â€œrectangular functionsâ€!
For j â‰ªN, i.e. far from the end of the interval, we get:
Fj = 1
N
sin 2M + 1
N
Ï€ j
Ï€ j/N
= 2M + 1
N
sin x
x
with x = 2M + 1
N
Ï€ j,
(4.44)

4.6 How to Get Rid of the â€œStraight-Jacketâ€ Periodic Continuation? â€¦
123
and thatâ€™s exactly what weâ€™d have expected in the ï¬rst place. In the extreme case of
M = N/2 âˆ’1 we get for j Ì¸= 0 from (4.42):
Fj = 1
N
sin N âˆ’1
N
Ï€ j
sin(Ï€ j/N)
= âˆ’1
N eiÏ€ j,
which we can just manage to compensate by plugging the â€œholeâ€ at fN/2 (cf. Sect.4.5,
cosine transformation). Letâ€™s take a closer look at the extreme case of large N and
large M (but with 2M â‰ªN). In this limit we really get the same â€œzerosâ€ as in
function sin x
x . Here we have a situation somewhat like the transition from the discrete
to the continuous Fourier transformation (especially so if we only look at the Fourier
coefï¬cients Fj with j â‰ªN). Now we also understand why there are no sidelobes
in the case of a discrete Fourier transformation of a cosine input without truncation
errors and without zero-padding: the Fourier coefï¬cients neighbouring the central
peak are precisely where the zeros are. Then the Fourier transformation works like
aâ€”meanwhile obsoleteâ€”vibrating-reed frequency meter. This sort of instrument
was used in earlier times to monitor the mains frequency of 50 cycles (60 cycles
in the US and some other countries). Reeds with distinctive eigen-frequencies, for
example 47, 48, 49, 50, 51, 52, 53 cycles, are activated using a mains-driven coil:
only the reed with the proper eigen-frequency at the current mains-frequency will
start vibrating, all others will keep quiet. These days, no energy supplier will get away
with supplying 49 or 51 cycles, as this would cause all inexpensive (alarm)clocks
(without quartz-control) to get out of sync. Whatâ€™s true for the frequency Ï‰ = 0,
of course also is true for all other frequencies Ï‰ Ì¸= 0, according to the Convolution
Theorem. This means that we can only get a consistent line proï¬le of a spectral line
that doesnâ€™t depend on truncation errors if we use zero-padding, and make it plenty
of zeros.
So here is the 1st recommendation:
Many zeros are good! N very big; 2M â‰ªN.
The economy and politics also obey this rule.
Since available Fourier transformation programs do not offer zero-padding you
have to do it yourself.
2nd recommendation:
Choose your sampling-interval t ï¬ne enough, so that your Nyquist fre-
quency is always substantially higher than the expected spectral inten-
sity, meaning, you need Fj only for j â‰ªN. This should get rid of the
consequences of the periodic continuation approximately!
In Chap.3 we quite extensively discussed continuous window functions. A very
good presentation of window functions in the case of the discrete Fourier transforma-
tion can be found in Harris [9]. Weâ€™re happy to know, however, that we may transfer

124
4
Discrete Fourier Transformation
0
M
Î©Nyq
N âˆ’1 âˆ’M
N âˆ’1
ï¬lling with zeros
no intensity here, i.e., sampled ï¬ne enough
even input
â€œhalfâ€ window
input with window
input with window, mirrored, ï¬lled with zeros
Fourier transform
Fig. 4.20 â€œCooking recipeâ€ for the Fourier transformation for an even input; in case of an odd
input invert the mirror image
all the properties of a continuous window function to the discrete Fourier transforma-
tion straight away, if, by using enough zeros for padding and using the low-frequency
portion of the Fourier series, we aim for the limes discrete â†’continuous.
So here comes the 3rd recommendation:
Do use window functions!
These three recommendations are illustrated in Fig.4.20 in an easy-to-remember
way. If you know that the input is even or odd, respectively, data mirroring is always
recommended. Should you have zero-padded your data you must window your data

4.6 How to Get Rid of the â€œStraight-Jacketâ€ Periodic Continuation? â€¦
125
yourself and eventually use mirroring before clicking the FFT button. Do not use
offered window functions, they would extend over the zero-padded region as well.
Then use the â€œrectangularâ€ window, i.e. no further window.
If the input is neither even nor odd, you can force the input to become even or odd,
respectively, provided all spectral components have the same phase. The situation is
more complicated if the input contains even and odd components, i.e. the spectral
components have different phases. If these components are well separated you can
shift the phase for each component individually. If these components are not well
separated use the full window function, i.e. donâ€™t mirror the data, than zero-padd
and Fourier transform. Now, the real and the imaginary part depend on where you
zero-padd: at the beginning, at the end, or both. In this case the power representation
is recommended.
In spite of the fact that todayâ€™s fast PCs wonâ€™t have a problem transforming very
big data sets any more, the application of the Fourier transformation got a huge boost
from the â€œFast Fourier transformationâ€ algorithm by Cooley and Tukey, an algorithm
that doesnâ€™t grow with N 2 calculations but only N ln N.
Weâ€™ll have a closer look at this algorithm in the next section.
4.7 Fast Fourier Transformation (FFT)
Cooley and Tukey started out from the simple question: what is the Fourier transform
of a series of numbers with only one real number (N = 1)? There are at least
3 answers:
i. Accountant:
From (4.12) with N = 1 follows:
F0 = 1
1 f0W âˆ’0
1
= f0.
(4.45)
ii. Economist:
From (4.31) (Parsevalâ€™s theorem) follows:
|F0|2 = 1
1|( f0)|2.
(4.46)
Using the services of someone into law: f0 is real and even, which leads to
F0 = Â± f0, and as F0 is also to be equal to the average of the series of numbers,
thereâ€™s no chance of getting a minus sign.
(A layperson would have done without all this lead-in talk!)
iii. Philosopher:
We know that the Fourier transform of a Î´-function results in a constant and vice
versa. How do we represent a constant in the world of 1-term series? By using
the number f0. How do we represent in this world a Î´-function? By using this
number f0. So in this world thereâ€™s no difference any more between a constant
and a Î´-function. Result: f0 is its own Fourier transform.

126
4
Discrete Fourier Transformation
This ï¬nding, together with the trick to achieve N = 1 by smartly halving the input
again and again (thatâ€™s why we have to stipulate: N = 2p, p integer), (almost) saves
us the Fourier transformation. For this purpose, letâ€™s ï¬rst have a look at the ï¬rst
subdivision. Weâ€™ll assume as given: { fk} with N = 2p. This series will get cut up in
a way that one subseries will only contain the even elements and the other subseries
only the odd elements of { fk}:
{ f1,k} = { f2k}
k = 0, 1, . . . , M âˆ’1,
{ f2,k} = { f2k+1}
M = N/2.
(4.47)
Both subseries are periodic in M.
Proof (Periodicity in M)
f1,k+M = f2k+2M = f2k = f1,k
because of 2M = N and f periodic in N.
Analogously for f2,k. â–¡
The respective Fourier transforms are:
F1, j = 1
M
Mâˆ’1

k=0
f1,kW âˆ’kj
M
,
F2, j = 1
M
Mâˆ’1

k=0
f2,kW âˆ’kj
M
,
j = 0, . . . , M âˆ’1.
(4.48)
The Fourier transform of the original series is:
Fj = 1
N
Nâˆ’1

k=0
fkW âˆ’kj
M
= 1
N
Mâˆ’1

k=0
f2kW âˆ’2kj
N
+ 1
N
Mâˆ’1

k=0
f2k+1W âˆ’(2k+1) j
N
(4.49)
= 1
N
Mâˆ’1

k=0
f1,kW âˆ’kj
M
+ W âˆ’j
N
N
Mâˆ’1

k=0
f2,kW âˆ’kj
M
,
j = 0, . . . , N âˆ’1.
In our last step we used:
W âˆ’2kj
N
= eâˆ’2Ã—2Ï€ikj/N = eâˆ’2Ï€ikj/(N/2) = W âˆ’kj
M
,
W âˆ’(2k+1) j
N
= eâˆ’2Ï€i(2k+1) j/N = W âˆ’kj
M
W âˆ’j
N .

4.7 Fast Fourier Transformation (FFT)
127
Together we get:
Fj = 1
2 F1, j + 1
2W âˆ’j
N F2, j,
j = 0, . . . , N âˆ’1,
or better:
Fj = 1
2(F1, j + F2, jW âˆ’j
N ),
Fj+M = 1
2(F1, j âˆ’F2, jW âˆ’j
N ),
j = 0, . . . , M âˆ’1.
(4.50)
Please note that in (4.50) we allowed j to run from 0 to M âˆ’1 only. In the second
line in front of F2, j we have used:
W âˆ’( j+M)
N
= W âˆ’j
N W âˆ’M
N
= W âˆ’j
N W âˆ’N/2
N
= W âˆ’j
N eâˆ’2Ï€i N
2 /N
= W âˆ’j
N eâˆ’iÏ€ = âˆ’W âˆ’j
N .
(4.51)
This â€œdecimation in timeâ€ can be repeated until we ï¬nally end up with 1-term
series whose Fourier transforms are identical to the input number, as we know. The
conventional Fourier transformation requires N 2 calculations, whereas here we only
need pN = N ln N.
Example 4.13 (â€œSaw-toothâ€ with N = 2)
f0 = 0,
f1 = 1.
-
Normal Fourier transformation:
W2 = eiÏ€ = âˆ’1
F0 = 1
2(0 + 1) = 1
2
F1 = 1
2

0 + 1 Ã— W âˆ’1
2

= âˆ’1
2.
(4.52)
Fast Fourier transformation:
f1,0 = 0 even part â†’F1,0 = 0
f2,0 = 1 odd part â†’F2,0 = 1,
M = 1.
(4.53)
From formula (4.50) we get:
F0 = 1
2
â›
âœâF1,0 + F2,0 W 0
2
	
=1
â
âŸâ = 1
2
F1 = 1
2

F1,0 âˆ’F2,0W 0
2

= âˆ’1
2.
(4.54)

128
4
Discrete Fourier Transformation
This didnâ€™t really save all that much work so far.
Example 4.14 (â€œSaw-toothâ€ with N = 4)
f0 = 0
f1 = 1
f2 = 2
f3 = 3.
-
The normal Fourier transformation gives us:
W4 = e2Ï€i/4 = eÏ€i/2 = i
F0 = 1
4(0 + 1 + 2 + 3) = 3
2
â€œaverageâ€
F1 = 1
4

W âˆ’1
4
+ 2W âˆ’2
4
+ 3W âˆ’3
4

= 1
4
1
i + 2
âˆ’1 + 3
âˆ’i

= âˆ’1
2 + i
2
F2 = 1
4

W âˆ’2
4
+ 2W âˆ’4
4
+ 3W âˆ’6
4

= 1
4(âˆ’1 + 2 âˆ’3) = âˆ’1
2
F3 = 1
4

W âˆ’3
4
+ 2W âˆ’6
4
+ 3W âˆ’9
4

= 1
4

âˆ’1
i âˆ’2 + 3
i

= âˆ’1
2 âˆ’i
2.
(4.55)
This time weâ€™re not using the trick with the clock, yet another playful approach.
We can skillfully subdivide the input and thus get the Fourier transform straight away
(cf. Fig.4.21).
Using 2 subdivisions, the Fast Fourier transformation gives us:
1st subdivision:
N = 4
{ f1} = {0, 2} even,
M = 2
{ f2} = {1, 3} odd.
(4.56)
2nd subdivision (Mâ€² = 1):
f11 = 0 even â‰¡F1,1,0,
f12 = 2 odd â‰¡F1,2,0,
f21 = 1 even â‰¡F2,1,0,
f22 = 3 odd â‰¡F2,2,0.
Using (4.50) this results in ( j = 0, Mâ€² = 1):
upper part
lower part
F1,k =
1
2 F1,1,0 + 1
2 F1,2,0, 1
2 F1,1,0 âˆ’1
2 F1,2,0

= {1, âˆ’1},
F2,k =
1
2 F2,1,0 + 1
2 F2,2,0, 1
2 F2,1,0 âˆ’1
2 F2,2,0

= {2, âˆ’1}

4.7 Fast Fourier Transformation (FFT)
129
odd
const.
Î´-function
from all Fj subtract 1
2.
real part:
F0 = 8
4 = 2
F1 = F2 = F3 = 0
imaginary part:
F1 = + 1
2
F3 = âˆ’1
2
F0 = 0
(always)
F2 = 0
(Nyquist)
Fig. 4.21 Decomposition of the saw-tooth into an odd part, constant plus Î´-function. Add up the
Fk, and compare the result with (4.55)
and ï¬nally, using (4.50) once again:
upper part
â§
âªâªâ¨
âªâªâ©
F0 = 1
2(F1,0 + F2,0) = 3
2,
F1 = 1
2

F1,1 + F2,1W âˆ’1
4

= 1
2

âˆ’1 + (âˆ’1) Ã— 1
i

= âˆ’1
2 + i
2,
lower part
â§
âªâªâ¨
âªâªâ©
F2 = 1
2(F1,0 âˆ’F2,0) = âˆ’1
2,
F3 = 1
2

F1,1 âˆ’F2,1W âˆ’1
4

= 1
2

âˆ’1 âˆ’(âˆ’1) Ã— 1
i

= âˆ’1
2 âˆ’i
2.
We can represent the calculations weâ€™ve just done in the following diagram, where
weâ€™ve left out the factors 1/2 per subdivisionâ€”they can be taken into account at the
end when calculating the Fj (Fig.4.22).
Here
â†’
â†—âŠ•means add and
â†˜
â†’âŠ–subtract and W âˆ’j
4
multiply with weight W âˆ’j
4 .
This subdivision is called â€œdecimation in timeâ€; the scheme:

130
4
Discrete Fourier Transformation
âŠ•
âŠ•
âŠ•
âŠ•
Input
Output
F2,2,0
F2,1,0
F1,2,0
F1,1,0
F3
F2
F1
F0
F2,1
F2,0
F1,1
F1,0
W âˆ’1
4
W 0
4
Fig. 4.22 Flow-diagram for the FFT with N = 4
-
-
-
-
-
-

RâŠ–
âŠ•
W âˆ’j
N
is called â€œbutterï¬‚y schemeâ€, which, for example, is used as a building-block in
hardware Fourier analysers. Figure4.23 illustrates the scheme for N = 16.
Those in the know will have found that the input is not required in the normal
order f0 . . . fN, but in bit-reversed order (arabic from right to left).
Example 4.15 (Bit-reversal for N = 16)
k
binary reversed results in kâ€²
0
0000
0000
0
1
0001
1000
8
2
0010
0100
4
3
0011
1100
12
4
0100
0010
2
5
0101
1010
10
6
0110
0110
6
7
0111
1110
14
8
1000
0001
1
9
1001
1001
9
10 1010
0101
5
11 1011
1101
13
12 1100
0011
3
13 1101
1011
11
14 1110
0111
7
15 1111
1111
15

4.7 Fast Fourier Transformation (FFT)
131
0
0
0
0
0
0
0
0
0
0
0
0
4
4
4
4
0
0
2
2
4
4
6
6
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
Time
Frequency
Fig. 4.23 Decimation in Time with N = 16
Computers have no problem with this bit-reversal.
At the end, letâ€™s have a look at a simple example:
with for example
7
= W âˆ’7
16
.
Example 4.16 (Half Nyquist frequency)
fk = cos(Ï€k/2),
k = 0, . . . , 15,
i.e.
f0 = f4 = f8 = f12 = 1,
f2 = f6 = f10 = f14 = âˆ’1,
all odd ones are 0.
The bit-reversal orders the input in such a way that we get zeros in the lower half
(cf. Fig.4.24). If both inputs of the â€œbutterï¬‚y schemeâ€ are 0, i.e. we surely get 0
at the output, we do not show the add-/subtract-crosses. The intermediate results of
the required calculations are quoted. The weights W 0
16 = 1 are not quoted for the
sake of clarity. Other powers do not show up in this example. You see, the input is
progressively â€œcompressedâ€ in four steps. Finally, we ï¬nd a number 8 at negative and

132
4
Discrete Fourier Transformation
âˆ’1
0
1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
k
fk
0
0
0
0
0
0
0
0
âˆ’1
âˆ’1
âˆ’1
âˆ’1
1
1
1
1
2
2
âˆ’2
âˆ’2
0
0
0
0
0
0
âˆ’4
4
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
âŠ•
8
0
0
0
0
8
8
0
0
0
0
0
0
0
0
0
0
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
k
fk
Input
Fj
Output
j
Fig. 4.24 Half Nyquist frequency
positive half Nyquist frequency each which we are allowed to add and subsequently
have to divide by 16 which ï¬nally yields the amplitude of the cosine input, i.e. 1.
Playground
4.1 Correlated
What is the cross correlation of a series { fk} with a constant series {gk}? Sketch the
procedure with Fourier transforms!
4.2 No Common Ground
Given is the series { fk} = {1, 0, âˆ’1, 0} and the series {gk} = {1, âˆ’1, 1, âˆ’1}.
Calculate the cross correlation of the two series.

Playground
133
4.3 Brotherly
Calculate the cross correlation of { fk} = {1, 0, 1, 0} and {gk} = {1, âˆ’1, 1, âˆ’1}, use
the Convolution Theorem.
4.4 Autocorrelated
Given is the series { fk} = {0, 1, 2, 3, 2, 1}, N = 6.
Calculate its autocorrelation function. Check your results by calculating the
Fourier transform of fk and of fk âŠ—fk.
4.5 Shifting around
Given the following input series (see Fig.4.25):
f0 = 1,
fk = 0
for k = 1, . . . , N âˆ’1.
a. Is the series even, odd, or mixed?
b. What is the Fourier transform of this series?
c. The discrete â€œÎ´-functionâ€ now gets shifted to f1 (Fig.4.26). Is the series even,
odd, or mixed?
d. What do we get for |Fj|2?
4.6 Pure Noise
Given the random input series containing numbers between âˆ’0.5 and 0.5.
a. What does the Fourier transform of a random series look like (see Fig.4.27)?
b. How big is the noise power of the random series, deï¬ned as:
Nâˆ’1

j=0
|Fj|2?
(4.57)
Compare the result in the limiting case of N â†’âˆto the signal power of the
input 0.5 cos Ï‰t.
. . .
0
1
2
N âˆ’2 N âˆ’1
N
k
Fig. 4.25 Input-signal with a Î´-shaped pulse at k = 0
. . .
0
1
2
N âˆ’2 N âˆ’1
N
k
Fig. 4.26 Input-signal with a Î´-shaped pulse at k = 1

134
4
Discrete Fourier Transformation
0.5
0
âˆ’0.5
fk
k
Fig. 4.27 Random series
Fig. 4.28 Input function according to (4.58)
0
1
2
3
4
5
6
7
8
9
Fig. 4.29 Theoretical pattern (â€œtoothbrushâ€) thatâ€™s to be located in the data set
4.7 Pattern Recognition
Given a sum of cosine functions as input, with plenty of superimposed noise
(Fig.4.28):
fk = cos 5Ï€k
32 + cos 7Ï€k
32 + cos 9Ï€k
32 + 15(RND âˆ’0.5)
(4.58)
for k = 0, . . . , 255,
where RND is a random number1 between 0 and 1.
How do you look for the pattern Fig.4.29 thatâ€™s buried in the noise, if it represents
the three cosine functions with the frequency ratios Ï‰1 : Ï‰2 : Ï‰3 = 5 : 7 : 9?
4.8 Go on the Ramp (for Gourmets only)
Given the input series:
fk = k for k = 0, 1, . . . , N âˆ’1.
1Programming languages such as, for example Turbo-Pascal, C, Fortran, â€¦ feature random gener-
ators that can be called as functions. Their efï¬ciency varies considerably.

Playground
135
Is this series even, odd, or mixed? Calculate the real and imaginary part of its
Fourier transform. Check your results using Parsevalâ€™s theorem. Derive the results
for Nâˆ’1
j=1 1/ sin2(Ï€ j/N) and Nâˆ’1
j=1 cot2(Ï€ j/N).
4.9 Transcendental (for Gourmets only)
Given the input series (with N even!):
fk =
k
for k = 0, 1, . . . , N
2 âˆ’1
N âˆ’k for k = N
2 , N
2 + 1, . . . , N âˆ’1 .
(4.59)
Is the series even, odd, or mixed? Calculate its Fourier transform. The double-
sided ramp is a high-pass ï¬lter (cf. Sect.5.2), which immediately becomes clear
considering the periodic continuation. Use Parsevalâ€™s theorem to derive the result
for N/2
k=1 1/ sin4(Ï€(2k âˆ’1)/N). Use the fact that a high-pass does not transfer a
constant in order to derive the result for N/2
k=1 1/ sin2(Ï€(2k âˆ’1)/N).

Chapter 5
Filter Effect in Digital Data Processing
Abstract This chapter deals with ï¬lter effects in digital data processing. For this
purpose the transfer function is introduced. Simple ï¬lters like high-pass, low-pass,
band-pass, and notch are discussed. The effects of data shifting, data compression
as well as differentiation and integration of discrete data are shown.
In this chapter weâ€™ll discuss only very simple procedures, such as smoothing of data,
shifting of data using linear interpolation, compression of data, differentiating data
and integrating them, and while doing that, describe the ï¬lter effectâ€”something
thatâ€™s often not even known to our subconscience. For this purpose, the concept of
the transfer function comes in handy.
5.1 Transfer Function
Weâ€™ll take as given a â€œrecipeâ€ according to which the output y(t) is made up of a
linear combination of f (t) including derivatives and integrals:
y(t)

â€œoutputâ€²â€²
=
+k

j=âˆ’k
a j f [ j](t)
  
â€œinputâ€²â€²
with f [ j] = d j f (t)
dt j
(negative j means integration).
(5.1)
This rule is linear and stationary, as a shift along the time axis in the input results
in the same shift along the time axis in the output.
When we Fourier-transform (5.1) we get with (2.57):
Y(Ï‰) =
+k

j=âˆ’k
a jFT

f [ j](t)

=
+k

j=âˆ’k
a j(iÏ‰) j F(Ï‰)
(5.2)
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_5
137

138
5
Filter Effect in Digital Data Processing
or:
Y(Ï‰) = H(Ï‰)F(Ï‰)
with the transfer function H(Ï‰) =
+k

j=âˆ’k
a j(iÏ‰) j.
(5.3)
When looking at (5.3), we immediately think of the Convolution Theorem.
According to this, we may interpret H(Ï‰) as the Fourier transform of the output
y(t) using Î´-shaped input (thatâ€™s F(Ï‰) = 1). So weighted with this transfer func-
tion, F(Ï‰) is translated into the output Y(Ï‰). In the frequency domain, we can easily
ï¬lter if we choose an adequate H(Ï‰). Here, however, we want to work in the time
domain.
Now weâ€™ll get into number series. Please note that weâ€™ll get derivatives only over
differences and integrals only over sums of single discrete numbers. Therefore weâ€™ll
have to widen the deï¬nition (5.1) by including non-stationary parts. The operator
V l means shift by l:
V l yk â‰¡yk+l.
(5.4)
This allows us to state the discrete version of (5.1) as follows:
yk

â€œoutputâ€
=
+L

l=âˆ’L
al V l fk

â€œinputâ€
.
(5.5)
Here, positive l stand for later input samples, and negative l for earlier input
samples. With positive l, we canâ€™t process a data-stream sequentially in â€œreal-timeâ€,
we ï¬rst have to buffer L samples, for example in a shift-register, which often is
called a FIFO (ï¬rst in, ï¬rst out). These algorithms are called acausal. The Fourier
transformation is an example for an acausal algorithm.
The discrete Fourier transformation of (5.5) is:
Y j =
+L

l=âˆ’L
alFT

V l fk

=
+L

l=âˆ’L
al
1
N
Nâˆ’1

k=0
fk+lW âˆ’kj
N
=
+L

l=âˆ’L
al
1
N
Nâˆ’1+l

kâ€²=l
fkâ€²W âˆ’kâ€² j
N
W +lj
N
=
+L

l=âˆ’L
alW +lj
N
Fj = Hj Fj.

5.1 Transfer Function
139
Y j = Hj Fj
with Hj =
+L

l=âˆ’L
alW +lj
N
=
+L

l=âˆ’L
aleiÏ‰ jlt and Ï‰ j = 2Ï€ j/(Nt).
(5.6)
Using this transfer function, which we assume to be continuous out of pure con-
venience,1 thatâ€™s H(Ï‰) = 	+L
l=âˆ’L aleiÏ‰lt, itâ€™s easy to understand the â€œï¬lter effectsâ€
of the previously deï¬ned operations.
5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
First weâ€™ll look into the ï¬lter effect when smoothing data. A simple 2-point algorithm
for data-smoothing would be, for example:
yk = 1
2( fk + fk+1)
(5.7)
with
a0 = 1
2,
a1 = 1
2.
This gives us the transfer function:
H(Ï‰) = 1
2

1 + eiÏ‰t
.
(5.8)
|H(Ï‰)|2 = 1
4(1 + eiÏ‰t)(1 + eâˆ’iÏ‰t) = 1
2 + 1
2 cos Ï‰t = cos2 Ï‰t
2
and ï¬nally:
|H(Ï‰)| = cos Ï‰t
2 .
Figure5.1 shows |H(Ï‰)|.
This has the unpleasant effect that a real input results in a complex output. This,
of course, is due to our implicitly introduced â€œphase shiftâ€ by t/2.
It looks like the following 3-point algorithm will do better:
yk = 1
3 ( fkâˆ’1 + fk + fk+1)
(5.9)
with
aâˆ’1 = 1
3,
a0 = 1
3,
a1 = 1
3.
1We can always choose N to be large, so j is very dense.

140
5
Filter Effect in Digital Data Processing
Fig. 5.1 Modulus of the
transfer function for the
smoothing-algorithm of (5.7)
Î©Nyq
|H(Ï‰)|
Ï‰
This gives us:
H(Ï‰) = 1
3

eâˆ’iÏ‰t + 1 + e+iÏ‰t
= 1
3(1 + 2 cos Ï‰t).
(5.10)
Figure5.2shows H(Ï‰)andtheproblemthatforÏ‰ = 2Ï€/3t thereisazero,mean-
ing that this frequency will not get transferred at all. This frequency is (2/3)Î©Nyq.
Above that, even the sign changes. This algorithm is not consistent and therefore
should not be used.
The â€œcorrectâ€ smoothing-algorithm is as follows:
yk = 1
4 ( fkâˆ’1 + 2 fk + fk+1)
low-pass
(5.11)
with
aâˆ’1 = +1
4,
a0 = +1
2,
a1 = +1
4.
The transfer function now reads:
H(Ï‰) = 1
4

eâˆ’iÏ‰t + 2 + e+iÏ‰t
= 1
4(2 + 2 cos Ï‰t) = cos2 Ï‰t
2 .
(5.12)
Î©Nyq
2Ï€
3Î”t
H(Ï‰)
Ï‰
Fig. 5.2 Transfer function for the 3-point smoothing-algorithm as of (5.9)

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
141
Î©Nyq
H(Ï‰)
Ï‰
logo:
Fig. 5.3 Transfer function for the low-pass
Figure5.3 shows H(Ï‰): there are no zeros, the sign doesnâ€™t change. Comparing
this to (5.8) and Fig.5.1, itâ€™s obvious that the ï¬lter effect now is bigger: cos2(Ï‰t/2)
instead of cos(Ï‰t/2) for |H(Ï‰)|.
Using half the Nyquist frequency we get:
H(Î©Nyq/2) = cos2 Ï€
4 = 1
2.
Therefore our smoothing-algorithm is a low-pass ï¬lter, which, admittedly, doesnâ€™t
have a â€œvery steep edgeâ€, and which, at Ï‰ = Î©Nyq/2, will let only half the amount
pass. So at Ï‰ = Î©Nyq/2 we have âˆ’3 dB attenuation.
If our data is corrupted by low-frequency artefacts (for example slow drifts), weâ€™d
like to use a high-pass ï¬lter. Hereâ€™s how we design it:
H(Ï‰) = 1 âˆ’cos2 Ï‰t
2
= sin2 Ï‰t
2
= 1
2(1 âˆ’cos Ï‰t)
= 1
2

1 âˆ’1
2eâˆ’iÏ‰t âˆ’1
2e+iÏ‰t

.
(5.13)
So we have: aâˆ’1 = âˆ’1/4, a0 = +1/2, a1 = âˆ’1/4, and the algorithm is:
yk = 1
4 (âˆ’fkâˆ’1 + 2 fk âˆ’fk+1)
high-pass.
(5.14)
From (5.14) we realise straight away: a constant as input will not get through
because the sum of the coefï¬cients ai is zero.
Figure5.4 shows H(Ï‰). Here, too, we can see that at Ï‰ = Î©Nyq/2 half the amount
will get through only. The experts talk of âˆ’3 dB attenuation at Ï‰ = Î©Nyq/2. We
discussed in Example 4.14 the â€œsaw-toothâ€. In the frequency domain this is a high-
pass, too! In a certain image reconstruction algorithm from many projections taken at
different angles, as required in tomography, exactly such high-pass ï¬lters are in use.
They are called ramp ï¬lters. They naturally show up when transforming from carte-
sian to cylinder coordinates. We shall discuss this algorithm, called â€œbackprojection
of ï¬ltered projectionsâ€, in more detail in Chap.7.

142
5
Filter Effect in Digital Data Processing
Î©Nyq
H(Ï‰)
Ï‰
logo:
Fig. 5.4 Transfer function for the high-pass
Î©Nyq
H(Ï‰)
Ï‰
logo:
Fig. 5.5 Transfer function of the band-pass
If we want to suppress very low as well as very high frequencies, we need a band-
pass. For simplicityâ€™s sake we take the product of the previously described low- and
high-pass (cf. Fig.5.5):
H(Ï‰) = cos2 Ï‰t
2
sin2 Ï‰t
2
=

1
2 sin Ï‰t
2
= 1
4 sin2 Ï‰t = 1
4
1
2 (1 âˆ’cos 2Ï‰t)
= 1
8

1 âˆ’1
2eâˆ’2iÏ‰t âˆ’1
2e+2iÏ‰t

.
(5.15)
So we have aâˆ’2 = âˆ’1/16, a0 = +1/8, a+2 = âˆ’1/16 and:
fk = 1
16 (âˆ’fkâˆ’2 + 2 fk âˆ’fk+2)
band-pass.
(5.16)
Now, at Ï‰ = Î©Nyq/2 we have H(Î©Nyq/2) = 1/4, thatâ€™s âˆ’6 dB attenuation.
If we choose to set the complement of the band-pass to 1:
H(Ï‰) = 1 âˆ’

1
2 sin Ï‰t
2
,
(5.17)
weâ€™ll get a notch ï¬lter that suppresses frequencies around Ï‰ = Î©Nyq/2, yet lets all
others pass (cf. Fig.5.6).

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
143
Î©Nyq
H(Ï‰)
Ï‰
logo:
Fig. 5.6 Transfer function of the notch ï¬lter
H(Ï‰) can be transformed to:
H(Ï‰) = 1 âˆ’1
8 + 1
16e2iÏ‰t + 1
16eâˆ’2iÏ‰t
(5.18)
with
aâˆ’2 = +1/16, a0 = +7/8, a+2 = +1/16
and
yk = 1
16 ( fkâˆ’2 + 14 fk + fk+2)
notch ï¬lter.
(5.19)
The suppression at half the Nyquist frequency, however, isnâ€™t exactly impressive:
only a factor of 3/4 or âˆ’1.25 dB.
Figure5.7 gives an overview/recaps all the ï¬lters weâ€™ve covered.
How can we build better notch ï¬lters? How can we set the cut-off frequency?
How can we set the edge steepness? Linear, non-recursive ï¬lters wonâ€™t do the job.
Therefore weâ€™ll have to look at recursive ï¬lters, where part of the output is fed back
as input. In RF-engineering this is called feedback. Live TV-shows with viewers
calling in on their phones know what (acoustic) feedback is: it goes from your
phoneâ€™s mouthpiece via plenty of wire (copper or ï¬bre) and various electronics to
the studioâ€™s loudspeakers, and from there on to the microphone, the transmitter and
back to your TV-set (maybe using a satellite for good measure) and on to your phoneâ€™s
handset. Quite an elaborate set-up, isnâ€™t it. No wonder we can have lots of fun letting
rip on talkshows using this kind of feedback! Video-experts may use their cameras
to achieve optical feedback by pointing it at the TV-screen that happens to show
exactly this camera and so on. (This is the modern, yet chaos-inducing, version of
the principle of the never-ending mirroring, using two mirrors opposite each other,
Î©Nyq
H(Ï‰)
Ï‰
notch ï¬lter
high-pass
low-pass
band-pass
Fig. 5.7 Overview of the transfer functions of the various ï¬lters

144
5
Filter Effect in Digital Data Processing
like, for example, in the Mirror Hall of the Castle of Linderhof, a tourist attraction,
the â€œlittle brotherâ€ of the castle Neuschwanstein of King Ludwig II. of Bavaria.)
Itâ€™s not appropriate to discuss digital ï¬lters in depth here. Weâ€™ll only look at a
small example to glean the principles of a low-pass with a recursive algorithm. The
algorithm may be formulated in a general manner as follows:
yk =
L

l=âˆ’L
alV l fk âˆ’
M

m=âˆ’M
mÌ¸=0
bmV m yk
(5.20)
with the deï¬nition: V l fk = fk+l (as above). We arbitrarily chose the sign in front
of the second sum to be negative; and for the same reason, we excluded m = 0 from
the sum. Both moves will prove to be very useful shortly.
For negative m the previous output is fed back to the right hand side of (5.20),
for the calculation of the new output: the algorithm is causal. For positive m the
subsequent output is fed back for the calculation of the new output: the algorithm is
acausal. Possible work-around: input and output are pushed into memory (register)
and kept in intermediate storage as long as M is big.
We may transform (5.20) into:
M

m=âˆ’M
bmV m yk =
L

l=âˆ’L
alV l fk.
(5.21)
The Fourier transform of (5.21) may be rewritten, like in (5.6) (with b0 = 1):
B jY j = A j Fj
(5.22)
with B j =
M

m=âˆ’M
bmW +mj
N
and
A j =
L

l=âˆ’L
alW +lj
N .
So the output is Y j = A j
B j Fj, and we may deï¬ne the new transfer function as:
Hj = A j
B j
or
H(Ï‰) = A(Ï‰)
B(Ï‰).
(5.23)
Using feedback we may, via zeros in the denominator, create poles in H(Ï‰), or
better, using somewhat less feedback, create resonance enhancement.

5.2 Low-Pass, High-Pass, Band-Pass, Notch Filter
145
Example 5.1 (Feedback) Letâ€™s take our low-pass from (5.12) with 50% feedback of
the previous output:
yk = 1
2 ykâˆ’1 + 1
4 ( fkâˆ’1 + 2 fk + fk+1) or

1 âˆ’1
2 V âˆ’1

yk = 1
4

V âˆ’1 + 2 + V +1
fk.
(5.24)
This results in:
H(Ï‰) = cos2(Ï‰t/2)
1 âˆ’1
2eâˆ’iÏ‰t
.
(5.25)
Ifwedonâ€™tcareaboutthephaseshift,causedbythefeedback,weâ€™reonlyinterested
in:
|H(Ï‰)| =
cos2(Ï‰t/2)

1 âˆ’1
2 cos Ï‰t
2
+

1
2 sin Ï‰t
2 =
cos2(Ï‰t/2)

5
4 âˆ’cos Ï‰t
.
(5.26)
The resonance enhancement at Ï‰ = 0 is 2, |H(Ï‰)| is shown in Fig.5.8, together
with the non-recursive low-pass from (5.12). We can clearly see that the edge steep-
ness got better. If weâ€™d fed back 100% instead of 50% in (5.24), a single short input
would have been enough to keep the output â€œhighâ€ for good; the ï¬lter would have
been unstable. In our case, it decays like a geometric series once the input has been
taken off.
Here weâ€™ve already taken the ï¬rst step into the highly interesting ï¬eld of ï¬lters
in the time domain. If you want to know more about it, have a look at, for example,
â€œNumerical Recipesâ€ [7] and the material quoted there. But donâ€™t forget that ï¬lters in
the frequency domain are much easier to handle because of the Convolution Theorem.
We shall discuss an interesting ï¬lter in more detail in Chap.6.
0
1
2
Î©Nyq
|H(Ï‰)|
Ï‰
cos2(Ï‰Î”t/2)
âˆš
5/4âˆ’cos Ï‰Î”t
cos2(Ï‰Î”t/2)
Fig. 5.8 Transfer function for the low-pass (5.12) and the ï¬lter with feedback (5.26)

146
5
Filter Effect in Digital Data Processing
5.3 Shifting Data
Letâ€™s assume you have a data set and you want to shift it a fraction d of the sampling
interval t, say, for simplicityâ€™s sake, using linear interpolation. So youâ€™d rather
have started sampling d later, yet wonâ€™t (or canâ€™t) repeat the measurements. Then
you should use the following algorithm:
yk = (1 âˆ’d) fk + d fk+1, 0 < d < 1
â€œshifting with
linear interpolationâ€.
(5.27)
The corresponding transfer function reads:
H(Ï‰) = (1 âˆ’d) + deiÏ‰t.
(5.28)
Letâ€™s not worry about a phase shift here; so we look at |H(Ï‰)|2:
|H(Ï‰)|2 = H(Ï‰)Hâˆ—(Ï‰)
= (1 âˆ’d + d cos Ï‰t + i d sin Ï‰t)(1 âˆ’d + d cos Ï‰t âˆ’i d sin Ï‰t)
= (1 âˆ’d + d cos Ï‰t)2 + d2 sin2 Ï‰t
= 1 âˆ’2d + d2 + d2 cos2 Ï‰t + 2(1 âˆ’d)d cos Ï‰t + d2 sin2 Ï‰t
= 1 âˆ’2d + 2d2 + 2(1 âˆ’d)d cos Ï‰t
= 1 + 2d(d âˆ’1) âˆ’2d(d âˆ’1) cos Ï‰t
= 1 + 2d(d âˆ’1)(1 âˆ’cos Ï‰t)
= 1 + 4d(d âˆ’1) sin2 Ï‰t
2
= 1 âˆ’4d(1 âˆ’d) sin2 Ï‰t
2 .
(5.29)
The function |H(Ï‰)|2 is shown in Fig.5.9 for d = 0, d = 1/4 and d = 1/2.
This means: apart from the (not unexpected) phase shift, we have a low-pass effect
due to the interpolation, similar to what happened in (5.12), which is strongest for
d = 1/2. If we know that our sampled function f (t) is bandwidth-limited, we may
Î©Nyq
|H(Ï‰)|2
Ï‰
Î´ = 0
Î´ = 1/4
Î´ = 1/2
Fig. 5.9 Modulus squared of the transfer function for the shifting-/interpolation-algorithm (5.27)

5.3 Shifting Data
147
use the sampling theorem and perform the â€œcorrectâ€ interpolation, without getting
a low-pass effect. Reconstructing f (t) from samples fk, however, requires quite an
effort and often is not necessary. Interpolation algorithms requiring much effort are
either not necessary (in case the relevant spectral components are markedly below
Î©Nyq), or they easily result in high-frequency artefacts. So be careful! Boundary
effects have to be treated separately.
5.4 Data Compression
Often we get the problem where data sampling had been too ï¬ne, so data have to be
compressed. An obvious algorithm would be, for example:
y j â‰¡y2k = 1
2( fk + fk+1), j = 0, . . . , N
2
â€œcompressionâ€.
(5.30)
Here data set {yk} is only half as long as data set { fk}. We pretend to have extended
the sampling width t by the factor 2 and expect the average of the old samples at
the sampling point. This inevitably will lead to a phase shift:
H(Ï‰) = 1
2 + 1
2eit.
(5.31)
If we donâ€™t want that, we better use the smoothing-algorithm (5.12), where only
every other output is stored:
y j â‰¡y2k = 1
4( fkâˆ’1 + 2 fk + fk+1), j = 0, . . . , N
2
â€œcompressionâ€.
(5.32)
Here, there is no phase shift, the principle is shown in Fig.5.10.
Boundary effects have to be treated separately.
So we might assume, for example, fâˆ’1 = f0 for the calculation of y0. This also
applies to the end of the data set.
Fig. 5.10 Data compression algorithm of (5.32)

148
5
Filter Effect in Digital Data Processing
5.5 Differentiation of Discrete Data
We may deï¬ne the derivative of a sampled function as:
d f
dt â‰¡yk = fk+1 âˆ’fk
t
â€œï¬rst forward differenceâ€.
(5.33)
The corresponding transfer function reads:
H(Ï‰) =
1
t

eiÏ‰t âˆ’1

=
1
t eiÏ‰t/2 
eiÏ‰t/2 âˆ’eâˆ’iÏ‰t/2
= 2i
t sin Ï‰t
2 eiÏ‰t/2
= iÏ‰eiÏ‰t/2 sin Ï‰t
2
Ï‰t/2 .
(5.34)
The exact result would be H(Ï‰) = iÏ‰ (cf. (2.56)), the second and the third factor
are due to the discretisation. The phase shift in (5.34) is a nuisance.
The â€œï¬rst backward differenceâ€:
yk = fk âˆ’fkâˆ’1
t
.
(5.35)
has got the same problem. The â€œï¬rst central differenceâ€:
yk = fk+1 âˆ’fkâˆ’1
2t
(5.36)
solves the problem with the phase shift. Here the following applies:
H(Ï‰) =
1
2t

e+iÏ‰t âˆ’eâˆ’iÏ‰t
= iÏ‰ sin Ï‰t
Ï‰t
.
(5.37)
Here, however, the ï¬lter effect is more pronounced, as is shown in Fig.5.11.
For high frequencies the derivative becomes more and more wrong.
Fix: Sample as ï¬ne as possible, so that within your frequency realm Ï‰ â‰ªÎ©Nyq
is always true.
Fig. 5.11 Transfer function
of the â€œï¬rst central
differenceâ€ (5.36) and the
exact value (thin line)
exact
1st central diï¬€erence
âˆ’iH(Ï‰)
Ï‰
Î©Nyq

5.5 Differentiation of Discrete Data
149
Î©Nyq
âˆ’H(Ï‰)
Ï‰
exact
2nd central diï¬€erence
Fig. 5.12 Transfer function of the â€œsecond central differenceâ€ (5.39) and exact value (thin line)
The â€œsecond central differenceâ€ is as follows:
yk = fkâˆ’2 âˆ’2 fk + fk+2
4t2
.
(5.38)
It corresponds to the second derivative. The corresponding transfer function is as
follows:
H(Ï‰) =
1
4t2

eâˆ’iÏ‰2t âˆ’2 + e+iÏ‰2t
=
1
4t2 (2 cos 2Ï‰t âˆ’2) = âˆ’1
t2 sin2 Ï‰t
(5.39)
= âˆ’Ï‰2

sin Ï‰t
Ï‰t
2
.
This should be compared to the exact expression H(Ï‰) = (iÏ‰)2 = âˆ’Ï‰2.
Figure5.12 shows âˆ’H(Ï‰) for both cases.
5.6 Integration of Discrete Data
The simplest way to â€œintegrateâ€ data is to sum them up. Itâ€™s a bit more precise if we
interpolate between the data points. Letâ€™s use the Trapezoidal Rule as an example:
assume the sum up to the index k to be yk, in the next step we add the following
trapezoidal area (cf. Fig.5.13):
yk+1 = yk + t
2 ( fk+1 + fk)
â€œTrapezoidal Ruleâ€.
(5.40)
The algorithm is:

V 1 âˆ’1

yk = (t/2)

V 1 + 1

fk, V l is the shifting operator
of (5.4).

150
5
Filter Effect in Digital Data Processing
Fig. 5.13 Concerning the
trapezoidal rule
So the corresponding transfer function is:
H(Ï‰) = t
2

eiÏ‰t + 1


eiÏ‰t âˆ’1

= t
2
eiÏ‰t/2 
e+iÏ‰t/2 + eâˆ’iÏ‰t/2
eiÏ‰t/2 
e+iÏ‰t/2 âˆ’eâˆ’iÏ‰t/2
(5.41)
= t
2
2 cos(Ï‰t/2)
2i sin(Ï‰t/2) = 1
iÏ‰
Ï‰t
2
cot Ï‰t
2 .
The â€œexactâ€ transfer function is:
H(Ï‰) = 1
iÏ‰
see also (2.63).
(5.42)
Heavisideâ€™s step function has the Fourier transform 1/iÏ‰, we get that when
integrating over the impulse (Î´-function) as input. The factor (Ï‰t/2) cot(Ï‰t/2)
is due to the discretization. H(Ï‰) is shown in Fig.5.14.
The Trapezoidal Rule is a very useful integration algorithm.
Another integration algorithm is Simpsonâ€™s 1/3-rule, which can be derived as
follows.
Given are three subsequent numbers f0, f1, f2 and we want to put a second order
polynomial through these points:
y = a + bx + cx2
with y(x = 0) = f0 = a,
y(x = 1) = f1 = a + b + c,
y(x = 2) = f2 = a + 2b + 4c .
(5.43)
exact
trapezoidal rule
iH(Ï‰)
Ï‰
Î©Nyq
Fig. 5.14 Transfer function for the trapezoidal rule (5.40) and exact value (thin line)

5.6 Integration of Discrete Data
151
The resulting coefï¬cients are:
a = f0,
c = f0/2 + f2/2 âˆ’f1,
b = f1 âˆ’f0 âˆ’c = f1 âˆ’f0 âˆ’f0/2 âˆ’f2/2 + f1
= 2 f1 âˆ’3 f0/2 âˆ’f2/2.
(5.44)
The integration of this polynomial of 0 â‰¤x â‰¤2 results in:
I = 2a + 4b
2 + 8c
3
= 2 f0 + 4 f1 âˆ’3 f0 âˆ’f2 + 4
3 f0 + 4
3 f2 âˆ’8
3 f1
= 1
3 f0 + 4
3 f1 + 1
3 f2 = 1
3 ( f0 + 4 f1 + f2) .
(5.45)
This is called Simpsonâ€™s 1/3-rule. As weâ€™ve gathered up 2t, we need the step-
width 2t. So the algorithm is:
yk+2 = yk + t
3 ( fk+2 + 4 fk+1 + fk)
â€œSimpsonâ€™s 1/3-ruleâ€.
(5.46)
This corresponds to an interpolation with a second order polynomial. The transfer
function is:
H(Ï‰) = 1
iÏ‰
Ï‰t
3
2 + cos Ï‰t
sin Ï‰t
and is shown in Fig.5.15.
At high frequencies Simpsonâ€™s 1/3-rule gives grossly wrong results. Of course,
Simpsonâ€™s 1/3-rule is more exact than the Trapezoidal Rule, given medium frequen-
cies, or the effort of interpolation with a second order polynomial would be hardly
worth it.
Simpsonâ€™s 1/3-rule
exact
iH(Ï‰)
Ï‰
Î©Nyq
Fig. 5.15 Transfer function for Simpsonâ€™s 1/3-rule compared to the Trapezoidal Rule and the exact
value (thin line)

152
5
Filter Effect in Digital Data Processing
At Ï‰ = Î©Nyq/2 we have, relative to H(Ï‰) = 1/iÏ‰:
Trapezoid:
Î©Nyqt
4
cot Î©Nyqt
4
= Ï€
4 cot Ï€
4 = Ï€
4 = 0.785 (too small),
Simpsonâ€™s-1/3:
Î©Nyqt
6
2 + cos(Î©Nyqt/2)
sin(Î©Nyqt/2)
= Ï€
6
2 + 0
1
= Ï€
3 = 1.047 (too big).
Simpsonâ€™s 1/3-rule also does better for low frequencies than the Trapezoidal Rule:
Trapezoid:
Ï‰t
2

1
Ï‰t/2 âˆ’Ï‰t/2
3
+ . . .

â‰ˆ1 âˆ’Ï‰2t2
12
,
Simpsonâ€™s-1/3:
Ï‰t
3

2 + 1 âˆ’1
2Ï‰2t2 + Ï‰4t4
24
Â· Â· Â·

Ï‰t

1 âˆ’Ï‰2t2
6
+ Ï‰4t4
120
Â· Â· Â·

=
1 âˆ’Ï‰2t2
6
+ Ï‰4t4
72 Â· Â· Â·
1 âˆ’Ï‰2t2
6
+ Ï‰4t4
120 Â· Â· Â·
â‰ˆ1 + Ï‰4t4
180
Â· Â· Â· .
The examples in Sects.5.2â€“5.6 would point us in the following direction, as far
as digital data processing is concerned:
The rule of thumb therefore is:
Do sample as ï¬ne as possible!
Keep away from Î©Nyq!
Do also try out other algorithms, and have lots of fun!

Playground
153
Playground
5.1 Totally Different
Given is the function f (t) = cos(Ï€t/2) which is sampled at times tk = kt,
k = 0, 1, . . . , 5 with t = 1/3.
Calculate the ï¬rst central difference and compare it with the â€œexactâ€ result for
f â€²(t). Plot your results! What is the percentage error?
5.2 Simpsonâ€™s-1/3 versus Trapezoid
Given is the function f (t) = cos Ï€t which is sampled at times tk = kt, k =
0, 1, . . . , 4 with t = 1/3.
Calculate the integral using the Simpsonâ€™s 1/3-rule and the Trapezoidal Rule and
compare your results with the exact value.
5.3 Totally Noisy
Given is a cosine input series thatâ€™s practically smothered by noise (Fig.5.16).
fi = cos Ï€ j
4 + 5(RND âˆ’0.5),
j = 0, 1, . . . , N.
(5.47)
In our example, the noise has a 2.5-times higher amplitude than the cosine sig-
nal. (The signal-to-noise ratio (power!) therefore is 0.5 : 5/12 = 1.2, see play-
ground 4.6.)
In the time spectrum (Fig.5.16) we canâ€™t even guess the existence of the cosine
component.
Fig. 5.16 Cosine signal in totally noisy background according to (5.47)
0.4
0
f(k) 
k
Fig. 5.17 Discrete line on slowly falling background

154
5
Filter Effect in Digital Data Processing
a. What Fourier transform do you expect for series (5.47)?
b. What can you do to make the cosine component visible in the time spectrum,
too?
5.4 Inclined Slope
Given is a discrete line as input, thatâ€™s sitting on a slowly falling ground (Fig.5.17).
a. Whatâ€™s the most elegant way of getting rid of the background?
b. How do you get rid of the â€œundershootâ€?

Chapter 6
Data Streams and Fractional Delays
Abstract In this chapter the concept of data streams produced by fast digitizers
is introduced. Two simple ï¬lters to accomplish fractional delays are presented: the
Lagrange interpolator, a non-recursive ï¬lter, and the Thiran all-pass ï¬lter, a recursive
ï¬lter, both in lowest order. The group delay of these ï¬lters is discussed in detail and
illustratedwithexamples.FortheThiranall-passtheimpulse,step,andrampresponse
are shown.
This and the following chapter are for those pedestrians who want to go a little further,
like the German writer Johann Gottfried Seume who walked from Grimma, a little
town near Leipzig, Free State of Saxony, Germany, to Syrakus in Sicily, Siracusa in
Italian (â€œSpaziergang nach Syrakusâ€).
Thus far we have considered a static set of data on which we carry out actions like
high- and low-pass ï¬ltering or alike. Modern digitizers analyze an analogue signal at
a certain pace, e.g. every nanosecond, and provide a stream of digital output. Light
travels about 30 cm in a nanosecond, the typical length of a ruler. Quite a handy
distance, indeed! Such digitizers are in use in a variety of ï¬elds, e.g. in acoustics
or microwave antenna arrays. Since there are different propagation delays involved
in cables for loudspeakers or antennas, the need for delays is immediately apparent.
Integer delays pose no problem: a simple shift register, a so-called FIFO (ï¬rst in,
ï¬rst out) would provide the solution. However, what about fractional delays?
6.1 Fractional Delays
We could simply interpolate the data like we did in Sect.5.3 called â€œshifting dataâ€.
This is indeed a possibility and we should examine it in more detail. The linear
interpolation is merely the simplest version of the so-called Lagrange-interpolation
schemes which use polynomials for interpolation. An example for a quadratic inter-
polation (N = 2) is Simpsonâ€™s 1/3-rule.
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_6
155

156
6
Data Streams and Fractional Delays
6.2 Non-recursive Algorithms
Non-recursivealgorithmsinterpolatepastandpresentinputdata fâˆ’N, fâˆ’N+1, . . . , f0
by polynomials of degree N. The simplest non-recursive algorithm for a fractional
delay is the Lagrange interpolator for N = 1, i.e. a linear interpolation. For this case
we noticed that the amplitude of an input signal is reduced depending on the delay
and the frequency (see Fig.5.9). Thus, e.g. the Nyquist frequency is not transmitted
at all for d = 0.5. Since we are dealing with data streams we call the pace t the
â€œtagâ€ and interpret d as a delay. Moreover, we did not pay attention to the associated
phase shift.
Here, we shall ï¬rst consider the phase shift, but in a rather different way. Instead
of speaking of the phase Ï† we shall use the quantity â€œgroup delayâ€ deï¬ned as:
Ï„group = âˆ’dÏ†
dÏ‰ .
(6.1)
This is the analogue of the inverse group velocityâ€”e.g. in light guidesâ€”per unit
length. It measures the time delay of the amplitude envelope of a signal consisting of
various sinusoidal components produced by a linear algorithm with time invariance.
Let us have a look at the Lagrange interpolator with N = 1. The difference
equation is written as:
yk = d fkâˆ’1 + (1 âˆ’d) fk.
(6.2)
Note that we have slightly changed the nomenclature compared to (5.27) because
we now have to use terms like â€œpastâ€ and â€œpresentâ€ and we do not want to use
â€œfutureâ€ in a data stream (although this would be possible, of course, by using a
certain overall integer delay to convert â€œfutureâ€ into â€œpastâ€). The range of d is simply
0 â‰¤d â‰¤1.Themagnitudeofthetransferfunction H(Ï‰)isnotaffectedbythischange
in nomenclature.
Let us see what we need:
First, we write the transfer function corresponding to (5.28) in the following way:
H(z) = dzâˆ’1 + (1 âˆ’d)
(6.3)
with z = eiÏ‰t, a convenient abbreviation for the moment. We shall come back
to this point later. This equation just means that you have to take the past quantity
multiplied by d and the present quantity multiplied by (1âˆ’d). In this sense, zâˆ’1 has
the same function as our shift operator V âˆ’1 deï¬ned in (5.20). The real and imaginary
parts of H(z) are:
Re{H(z)} = d cos Ï‰t + (1 âˆ’d)
Im{H(z)} = âˆ’d sin Ï‰t.
(6.4)

6.2 Non-recursive Algorithms
157
Hence, the phase is:
Ï† = arctan
âˆ’d sin Ï‰t
d cos Ï‰t + (1 âˆ’d).
(6.5)
Differentiating the arctan-function with respect to Ï‰ is a little tedious. With the
help of:
d arctan Î±
dÎ±
=
1
1 + Î±2
(6.6)
and a few simpliï¬cations we arrive at:
Ï„group = dt

1 âˆ’2(1 âˆ’d) sin2 Ï‰t
2


1 âˆ’4d(1 âˆ’d) sin2 Ï‰t
2
 .
(6.7)
This function is plotted in Fig.6.1 versus Ï‰ for various values of d with t = 1.
The limiting values for Ï‰ â†’0 are easily seen:
Ï„group â†’d
(in units of t).
(6.8)
For Ï‰ â†’Î©Nyq we get:
Ï„group â†’
d
2d âˆ’1
(in units of t).
(6.9)
For d = 1/2 the group delay seems to diverge at Î©Nyq. Had we ï¬rst inserted d = 1/2
into (6.7), the frequency dependent terms in the numerator and denominator would
cancel and we would get Ï„group = 1/2 in the entire frequency range. Thus, the result
depends on the order of taking the limiting values. What a nuisance! It does not
really help that under these conditions H(Î©Nyq) = 0, i.e. nothing is passed at all.
The problem is still severe in the vicinity of Î©Nyq for d close to 1/2. What is even
more intriguing is that Ï„group becomes negative for d < 1/2 at a sufï¬ciently high
frequency (always above Î©Nyq/2). What is the meaning of a â€œnegative delayâ€? Donâ€™t
Fig. 6.1 The group
delay Ï„group in units of t for
the N = 1 Lagrange
interpolator versus Ï‰ for
delays d in the range from
0 to 1
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Ï‰
Ï„group
Î©Nyq

158
6
Data Streams and Fractional Delays
input
j
Fj
1
5
âˆ’2
5
3
5
âˆ’4
5
1
âˆ’4
5
3
5
âˆ’2
5
1
5
output for
d = 0.4
j
Fj
3
25
âˆ’4
25
5
25
âˆ’6
25
7
25
âˆ’2
25
1
25
0
âˆ’1
25
2
25
d = 0.5
j
Fj
1
10
âˆ’1
10
1
10
âˆ’1
10
1
10
1
10
âˆ’1
10
1
10
âˆ’1
10
1
10
d = 1.0
j
Fj
1
5
âˆ’2
5
3
5
âˆ’4
5
1
âˆ’4
5
3
5
âˆ’2
5
1
5
Fig. 6.2 Input signal with Ï‰ = Î©Nyq and triangular envelope (top); output of the Lagrange inter-
polator with N = 1 for various delays d (subsequent to top)
panic! For the envelope of a signal which varies slowly compared to the frequency
components there is no problem in â€œadvancingâ€ the maximum a few tags like it
is for delaying. In order to see what cruelties the seemingly â€œinnocentâ€ Lagrange
interpolater with N = 1 can commit, letâ€™s do a â€œdrasticâ€ example.
Example 6.1 (â€œNyquist pulseâ€). We use as input a signal with Ï‰ = Î©Nyq and a
triangular envelope (see Fig.6.2(top)). The signal contains spectral components like
those shown in Fig.3.2, now centered at Î©Nyq. The upper half is simply the mirrored
left half like illustrated in Fig.4.5. This choice makes sure that we cover the range
of frequencies where Ï„group is strongly frequency dependent, except for d = 0.5.
Figure6.2 illustrates the output for the delays d = 0.4, 0.5, and 1. For d = 0.5 we
immediately see that we get a delay of 0.5, the pulse envelope is now rectangular and
the amplitude is heavily attenuated. Where is the maximum of the envelope? In the
middle? Since Ï„group = 0.5, independent of Ï‰, the rectangular envelope is entirely
the result of the frequency dependent attenuation. For d = 0.4 the signal is heavily
distorted and the envelope is monotonous on one side only. It looks like the envelope

6.2 Non-recursive Algorithms
159
is shifted backwards. For d = 0.6 we get the mirror image of the case for d = 0.4.
For d = 1 we recover the input shifted by 1.
We see that not only the magnitude of the transfer function H(Ï‰) depends on d
and Ï‰ but also the group delay. A rather unsatisfactory situation.
Thankfully, there is help regarding the magnitude of the transfer function using a
recursive algorithm, as we shall see in Sect.6.4.
In order to see that the Lagrange interpolator for N = 1 is not dull at all, letâ€™s do
two examples.
Example 6.2 (Average) Its worth looking at a subtle detail: whats the average of the
function with the Nyquist frequency and a triangular envelope (see Fig.6.2(top))?
Have a guess! Looks like zero. Not bad, but not good enough! It is zero for even M
only where M is the number of data points below 0 and also above 0, i.e. there are
2M data points. What is it for M = odd ? Lets do the calculation by â€œbrute forceâ€:
Average =
1
2M

1 + 2
Mâˆ’1

k=1
(âˆ’1)k M âˆ’k
M

=
1
2M

1 + 2
Mâˆ’1

k=1
(âˆ’1)k âˆ’2
M
Mâˆ’1

k=1
(âˆ’1)kk

=
1
2M

1 +
 âˆ’2
0

âˆ’2
M
	 Mâˆ’2
2
âˆ’(M âˆ’1)
Mâˆ’1
2


for even M
for odd M
=
1
2M

1 +
 âˆ’2
0

âˆ’2
M
	
âˆ’M
2
Mâˆ’1
2


for even M
for odd M
=
1
2M

1 +
 âˆ’2
0

+
1
1
M âˆ’1
 for even M
for odd M
=
0
for even M
1
2M2
for odd M .
It has todowiththequestionwhether theï¬rst/last non-vanishingdatapoint is negative
or positive. Of course, for large M the difference fades away. A look to Fourier space
willshowthatthisresultcomesinquitenaturally.Tosimplifymatters,weuseperiodic
continuation, i.e. no zero-padding. Lets start with the calculation of the discrete
Fourier transform of the discrete triangular function. Iâ€™m sure you have noticed that in
Playground4.9 we have already solved a similar problem. The triangular functionâ€”
or double-sided rampâ€”was shifted and we required even N. With a slight change
in nomenclature we can relax this requirement. Should you happen to have solved
Playground4.9 take the following as a rehearsal. Letâ€™s start from scratch.

160
6
Data Streams and Fractional Delays
fk =
â§
âªâ¨
âªâ©
M + k
M
for âˆ’M â‰¤k â‰¤0
M âˆ’k
M
for 1 â‰¤k â‰¤M âˆ’1
F0 =
1
2M

1 + 2
Mâˆ’1

k=1

1 âˆ’k
M

=
1
2M

1 + 2(M âˆ’1) âˆ’2
M
Mâˆ’1

k=1
k

=
1
2M

1 + 2M âˆ’2 âˆ’2
M
M(M âˆ’1)
2

=
1
2M (1 + 2M âˆ’2 âˆ’M + 1) = 1
2,
as we would have guessed right away.
The second sum is due to the young mathematician Carl Friedrich GauÃŸ who
apparently did not want to sum up all numbers from 1 to 100 step by step, a task given
by the teacher to keep the pupils busy. Carl Friedrich noticed that by complementing
each number to add up to the maximum number he actually adds the same series but
in reversed order. Thatâ€™s where the factor of two in the denominator comes from.
For j > 0 we have:
Fj =
1
2M

âˆ’1

k=âˆ’M
fk exp âˆ’iÏ€ jk
M
+ 1 +
Mâˆ’1

k=1
fk exp âˆ’iÏ€ jk
M

=
1
2M

1 + 2
Mâˆ’1

k=1

1 âˆ’k
M

cos Ï€ jk
M

(term with k = âˆ’M vanishes because fâˆ’M = 0)
=
1
2M

1 + 2
Mâˆ’1

k=1
cos Ï€ jk
M âˆ’2
M
Mâˆ’1

k=1
k cos Ï€ jk
M

.
The ï¬rst sum is easily evaluated using the Dirichlet kernel (1.53) with the upper
summation index being M âˆ’1. We get:
2
Mâˆ’1

k=1
cos Ï€ jk
M
= 2

sin

M âˆ’1
2

x
2 sin x
2
âˆ’1
2

= âˆ’cos Mx âˆ’1 with x = Ï€ j
M .
For the second sum we start with the expression derived in Playground4.9:
Mâˆ’1

k=1
sin kx = cos x
2 âˆ’cos

M âˆ’1
2

x
2 sin x
2
= 1
2

(1 âˆ’cos Mx) cot x
2 âˆ’sin Mx


6.2 Non-recursive Algorithms
161
This we differentiate with respect to x to get:
Mâˆ’1

k=1
k cos kx = 1
2

âˆ’(1 âˆ’cos Mx) 1
2
sin2 x
2
+ M sin Mx cot x
2 âˆ’M cos Mx

= âˆ’1
2
M cos Ï€ j + sin2 Ï€ j
2
sin2 Ï€ j
2M
with x = Ï€ j
M .
Collecting all terms we ï¬nally arrive at:
Fj =
1
2M2
sin2 Ï€ j
2
sin2 Ï€ j
2M
.
This is the discrete analogue of the continuous result of (3.4). We can simplify this
expression further to:
F0 = 1
2
(the average;
we can use even the general formula,
let j be continuous for the moment,
and use lâ€™Hospitalâ€™s rule)
Fj =
â§
âªâ¨
âªâ©
1
2M2
1
sin2 Ï€ j
2M
for odd j
0
for even j
.
The continuous Fourier transform of the triangular function had zeros so does the
discrete one. What is left over is to shift the Fourier transform of the discrete triangular
function to the Nyquist frequency because a multiplication of the function with the
Nyquist frequency by the triangular function corresponds to the convolution of the
Fourier-transformed triangular function with a discrete Î´-function at the Nyquist
frequency. For the moment we are only interested in the value at zero frequency
because this gives the average of our function where we started. This means we need
the value at the end of the tail, i.e. j = âˆ’M for the unshifted Fourier-transformed
triangular function. We get:
F0 =
â§
âªâ¨
âªâ©
1
2M2
sin2 Ï€M
2
sin2 Ï€
2
=
1
2M2
for odd M
0
for even M
.
Now it is clear that the average of our input function is non-zero for odd M only.

162
6
Data Streams and Fractional Delays
Example 6.3 (Frequency comb) Now lets have a closer look at the case d = 1/2 in
Fourier space. The function looks like the Nyquist frequency, but there is a ï¬‚aw in
the middle. In fact, this little glitch is causing a whole comb of non-zero frequency
components in Fourier space, as weâ€™ll see below. First we need the discrete transfer
function for the Lagrange interpolator with N = 1. Since we are not interested in
the phase shift, we need the real part only: Re{Hj} = cos2(Ï€ j/2M). For the sake of
convenience, weâ€™ll drop the Re{} in the following. This we multiply by the Fourier-
transformed triangular function centred at the Nyquist frequency. The shifted Fourier
transform of the triangular function reads:
F0 =
â§
âªâ¨
âªâ©
1
2M2
for odd M
0
for even M
FM = 1
2
Fj =
â§
âªâªâªâ¨
âªâªâªâ©
1
2M2
1
sin2 Ï€(Mâˆ’j)
2M
for odd M and even j = 2, 4, . . . , M âˆ’1
or
for even M and odd j = 1, 3, . . . , M âˆ’1
0
else
FM+ j = FMâˆ’j
for j = 1, 2, . . . , M âˆ’1.
The trigonometric expression can be further simpliï¬ed:
sin2 Ï€(M âˆ’j)
2M
=
â›
â
=1
sin Ï€
2 cos Ï€ j
2M âˆ’
=0
cos Ï€
2 sin Ï€ j
2M
â
â 
2
= cos2 Ï€ j
2M .
No surprise, in the continuous world, sin2 x shifted by Ï€/2 yields cos2 x! However,
there are a few subtleties in the digital world as you can see from the little extras F0
and FM. Incidentally (or not?), this is identical to the transfer function and thus the
trigonometric expressions cancel. We ï¬nally get:
HM FM = cos2 Ï€M
2M
1
2 = 0 (no transmission at the Nyquist frequency)
Hj Fj =
â§
â¨
â©
1
2M2
for odd M + j
0
else
HM+ j FM+ j = HMâˆ’j FMâˆ’j
for j = 0, . . . , M âˆ’1.
This expression means that every second Fourier coefï¬cient vanishes, in particular
the odd ones for odd M and the even ones for even M (see Fig.6.3 for M = 3 and
M = 4). And, amazingly, all non-zero coefï¬cients have the same value
1
2M2 . Thats

6.2 Non-recursive Algorithms
163
M = 3
j
Fj
0
1
2
3
4
5
6
1
18
M = 4
j
Fj
0
1
2
3
4
5
6
7
1
32
8
Fig. 6.3 Real part of Fourier transform of the input with the Nyquist frequency and triangular
envelope delayed by d = 1/2 using the Lagrange interpolator with N = 1 for M = 3 (top) and
M = 4 (bottom)
why we called this a â€œcombâ€ of frequency coefï¬cients. Its the glitch in the input data
which caused it!
Just to complete the example, lets calculate directly the Fourier transform of the
output of the function with the Nyquist frequency and the triangular envelope shifted
by d = 1/2 using the Lagrange interpolator with N = 1. In order to simplify matters,
we decompose the input into a constant with value
1
2M and a function with value
âˆ’1
M at those points where the input was negative. The constant is of no interest,
it contributes to F0 only and we know this value already (=
1
2M2 for odd M). The
non-zero points of the second function appear at all odd negative values of k whereas
they appear at all even k for positive ones. Since we are interested only in the real
part of the Fourier transform we can ï¬‚ip around the negative kâ€™s to positive kâ€™s and
see that we end up with a sum we know already: the Dirichlet kernel. There is a little
pitfall: for even M we have to include the term with k = âˆ’M whereas we must not
for odd M. Lets discuss even and odd M separately. For even M we get:
Fj =
1
2M

âˆ’1
M
 
cos Ï€ j + sin

M âˆ’1
2
 Ï€
M
2 sin Ï€ j
2M
âˆ’1
2

= âˆ’
1
2M2

cos Ï€ j âˆ’1
2 cos Ï€ j âˆ’1
2

=
1
2M2 sin2 Ï€ j
2
=
â§
âªâ¨
âªâ©
1
2M2
for odd j
0
else
.

164
6
Data Streams and Fractional Delays
For odd M we get:
Fj =
1
2M

âˆ’1
M
 
sin

M âˆ’1
2
 Ï€
M
2 sin Ï€ j
2M
âˆ’1
2

= âˆ’
1
2M2
1
2 cos Ï€ j âˆ’1
2

=
1
2M2 cos2 Ï€ j
2
=
â§
âªâ¨
âªâ©
1
2M2
for even j
0
else
.
This can be combined to:
Fj =
â§
âªâ¨
âªâ©
1
2M2
for odd M + j
0
else
FM+ j = FMâˆ’j
for j = 0, . . . , M âˆ’1.
This is, of course, the same result as above.
6.3 Stability of Recursive Algorithms
In the Example 5.1 we have already encountered a recursive algorithm, also called
a â€œï¬lterâ€. We can write the transfer function as a polynomial of the above deï¬ned
quantity zâˆ’1 in the numerator and another polynomial in zâˆ’1 in the denominator,
both with real-valued coefï¬cients. We shall exclude positive powers of z because
we want to use present, i.e. z0, and past data only. For the discussion of stability
of the ï¬lter we now allow z to be an arbitrary complex quantity rather than to be a
phase only. Then we get N zeros of a polynomial of degree N in the denominator,
hence the transfer function diverges. This is what is usually called a â€œpoleâ€. For
N = 1 our denominator would read 1 + a1zâˆ’1 with a single pole for zâˆ’1 = âˆ’1/a1
or z = âˆ’a1. For higher N we would eventually get poles for complex values of zâˆ’1.
Let us examine what is the response of the transfer function:
H(z) =
1
1 âˆ’zâˆ’1
(6.10)
to an impulse:
fk = Î´k,0.
(6.11)

6.3 Stability of Recursive Algorithms
165
The corresponding difference equation reads:
yk = ykâˆ’1.
(6.12)
The output will be the Heaviside step function: once switched to unity it remains
there forever. Sometimes (6.10) is called the â€œz-transformâ€ of the Heaviside function.
It is also called â€œaccumulatorâ€ because it adds up all input data for k â‰¥0.
If instead of (6.10) we would use:
H(z) =
1
1 âˆ’a1zâˆ’1
with |a1| < 1
(6.13)
we would have
yk = a1ykâˆ’1
(6.14)
and the response to the impulse would decay as a geometrical series with increasing
powers of a1. For small a1, the output would decay rapidly, but actually persists
indeï¬nitely. This is why such ï¬lters are called â€œInï¬nite Impulse Responseâ€ (IIR)
ï¬lters. The Lagrange interpolator, on the contrary, is a â€œFinite Impulse Responseâ€
(FIR) ï¬lter.
What has to be discussed now is the stability of the ï¬lter. We have seen that (6.10)
is at the limit of stability while (6.13) is stable provided |a1| < 1. It can be shown that
IIR ï¬lters are stable provided the poles are all inside the unit circle in the complex
plane. For the stability, the zeros of the numerator polynomial play no role.
6.4 Thiranâ€™s All-Pass Filter for N = 1
There is an algorithm which avoids the attenuation altogether, the so-called â€œThiran
all-pass ï¬lterâ€, i.e. we have |H(z)| = 1 always, and, moreover, the group delay is
â€œmaximally ï¬‚atâ€, i.e. it depends only weakly on Ï‰ for small Ï‰. More precisely, all
up to the Nth derivatives at Ï‰ = 0 are 0 for a polynomial of order N. Thiran [12]
originally proposed a so-called â€œall-poleâ€-ï¬lter with a polynomial in zâˆ’1 in the
denominator and a constant numerator. Fettweis [13] showed that it is advantageous
to include a properly chosen polynomial in the numerator. This is now called the
â€œThiran all-pass ï¬lterâ€ and is discussed in detail in [14].
You may ask, what the hell is ï¬ltered if everything passes? The â€œï¬lter expertsâ€
call everything a â€œï¬lterâ€ which can be described by a linear difference equation, like
the one above. It does not necessarily mean a ï¬lter in the frequency domain. You
will get used to this terminology like you are used to click to the â€œstartâ€ button if you
want to shut down your computer. I would have preferred a â€œstopâ€ button.

166
6
Data Streams and Fractional Delays
Of course, we are dealing with a recursive ï¬lter.
Here is how it is constructed. We start with the transfer function:
H(z) = zâˆ’N DN(z)
DN(zâˆ’1)
(6.15)
Here, DN(z) denotes a polynomial in z of order N with coefï¬cients ai and the
constant a0 = 1. It is easy to show that |H(z)| = 1.
The ï¬rst factor zâˆ’N is merely a phase with magnitude 1. The numerator can be
written as DN(z) = |DN(z)|eiÏ†D, the denominator can be written as DN(zâˆ’1) =
|DN(z)|eâˆ’iÏ†D because zâˆ’1 is its complex conjugate zâˆ—. Thus the quotient of the two
polynomials is again merely a phase e2iÏ†D. This completes the proof.
Here, we shall discuss the simplest case with N = 1 only. We have:
H(z) = zâˆ’1(1 + a1z)
1 + a1zâˆ’1
= zâˆ’1 + a1
1 + a1zâˆ’1 .
(6.16)
The corresponding difference equation reads:
yk = âˆ’a1ykâˆ’1 + fkâˆ’1 + a1 fk
(6.17)
The ï¬rst term comes from the denominator (a recursive algorithm!) whereas the other
two terms come from the numerator.
Thiran [12] and VÃ¤limÃ¤ki [14] quote for the maximally ï¬‚at ï¬lter for N = 1 the
condition for the coefï¬cient a1:
a1 = âˆ’
d
d + 2.
(6.18)
The group delay can now be written as:
Ï„group = t + 2dÏ†D
dÏ‰ .
(6.19)
The ï¬rst term comes from zâˆ’1 whereas the second term comes from the quotient
DN(z)/DN(zâˆ’1).
The real and imaginary parts of the phase Ï†D are:
Re{Ï†D} = 1 + a1 cos Ï‰t
Im{Ï†D} = âˆ’a1 sin Ï‰t.
(6.20)
After a little tedious algebra we ï¬nally get:
dÏ†D
dÏ‰ = âˆ’ta1(a1 + cos Ï‰t)
1 + a2
1 + 2a1 cos Ï‰t .
(6.21)

6.4 Thiranâ€™s All-Pass Filter for N = 1
167
Fig. 6.4 The group delay
Ï„group in units of t for the
N = 1 Thiran all-pass versus
Ï‰ for delays d in the range
from âˆ’1/2 to +1/2
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Î©Nyq
Ï‰
Ï„group
After inserting (6.18) and using (6.19) we get:
Ï„group =
t(1 + d)
1 + d(d + 2) sin2 Ï‰t
2
.
(6.22)
This function is plotted in Fig.6.4 versus Ï‰ for various values of d with  = 1.
For Ï‰ â†’0 we get Ï„group = 1 + d (in units of t). No wonder, even for d = 0
we get 1 tag delay because we have used an output from the past!
Here, a word about the useful range of d is necessary. VÃ¤limÃ¤ki [14] recommends
âˆ’1/2 â‰¤d â‰¤+1/2 (in units of t). Donâ€™t worry about negative delays, we just go
back to the past by half a tag and Ï„group is always positive.
For d = 0 we get Ï„group = 1, for d = 1/2 we get Ï„group = 3/2/(1 +
5/4 sin2(Ï‰t/2)), and for d = âˆ’1/2 we get Ï„group = 1/2(1âˆ’3/4 sin2(Ï‰t/2)), all
in units of t. For Î©Nyq we get Ï„group = (1+d)/(1+d(d +2)), i.e. Ï„group = 2/3 and
2 for d = 1/2 and d = âˆ’1/2, respectively. Note the asymmetry between positive
and negative delays. It also shows that negative delays are giving worse results than
positive ones. In any case, there is no divergence like in the Lagrange N = 1 case.
We now give a few illustrations of the ï¬lter action.
6.4.1 Impulse Response
Let us see what is the response of the N = 1 Thiran all-pass ï¬lter to an impulse.
From (6.17) we see that the output is:
y0 = a1
yk =

1 âˆ’a2
1

(âˆ’a1)kâˆ’1,
k = 1, 2, 3, . . . .
(6.23)
This means, thatâ€”apart from the transient y0â€”the output decays with increasing
powers of a1. For positive a1 the sign alternates whereas for negative a1 the output
is always positive.

168
6
Data Streams and Fractional Delays
Now we show that the sum over all output data is unity, as was the input:
âˆ

k=0
yk = a1 + (1 âˆ’a2
1)
âˆ

k=0
(âˆ’a1)k.
(6.24)
The sum is easily evaluated with [8,No.0.231] yielding ï¬nally:
âˆ

k=0
yk = a1 + 1 âˆ’a2
1
1 + a1
= a1 + (1 âˆ’a1)(1 + a1)
1 + a1
= 1.
(6.25)
6.4.2 Step Response
Let us see what is the response of the N = 1 Thiran all-pass ï¬lter to a step, the
Heaviside step function which is zero for negative times and unity at t = 0 and
afterwards. The ï¬rst example is for d = 1/2. Figure6.5 illustrates the step response.
To start with, there is an undershoot of magnitude a1 = âˆ’1/5, followed by
rational numbers with powers of |a1| in the denominator and approaching unity
monotonously.
The step response for d = âˆ’1/2 is a little surprising at ï¬rst glance. Figure6.6
illustrates what happens.
Fig. 6.5 The step response
of the Thiran all-pass for
d = 1/2
k
yk
âˆ’2
âˆ’1
0
âˆ’1
5
1
19
25
2
119
125
3
619
625
Fig. 6.6 The step response
of the Thiran all-pass for
d = âˆ’1/2
k
yk
âˆ’2
âˆ’1
0
1
3
1
11
9
2
25
27
3
83
81

6.4 Thiranâ€™s All-Pass Filter for N = 1
169
To start with, there is a positive output with a1 = 1/3, followed by rational
numbers with powers of a1 in the denominator and approaching unity, but now in an
oscillatory manner.
This deserves a closer look.
Apart from the transient (the ï¬rst output in the present case) the following explicit
formula can be derived from the difference equation (6.17).
yk = 1 + (âˆ’1)k+1ak
1(1 âˆ’a1),
k = 1, 2, 3, . . . .
(6.26)
Now it is easy to see why for negative a1 we have a monotonous approach to unity:
the prefactor of the second term is always negative because we can write:
(âˆ’1)k+1ak
1 = (âˆ’1)2k+1|a1|
(6.27)
and the exponent is odd.
On the other hand, for positive a1 we retain the factor (âˆ’1)k+1 which leads to
oscillations. Note that (1 âˆ’a1) is always positive in the allowed range of d.
The unit step contains all possible frequencies and the step response is what the
group delay does to all those frequency components. The integral delay action is not
immediately evident.
6.4.3 Ramp Response
More illustrative is the response of the Thiran all-pass with N = 1 to a ramp
deï¬ned as:
fk =
	 k
for k = 0, 1, 2, 3, . . .
0
for k < 0
.
(6.28)
We should not bother that the fk grow indeï¬nitely. We can always stop for a given
kmax and we are interested only in smaller k, i.e. early times such that the termination
plays no role. If we normalize the fk to the largest value, i.e. fmax = 1, we see that
we actually discuss the low frequency limit.
Figures6.7 and 6.8 show the ramp response for d = 1/2 and d = âˆ’1/2, respec-
tively.
It is clear that in both cases the 45â—¦slope is approached rapidly (monotonously
for d = 1/2 and oscillatory for d = âˆ’1/2). What is very pleasing is that you can
directly read off the low frequency limit of the group delay Ï„group = 1 + d, i.e. 3/2
for d = 1/2 and 1/2 for d = âˆ’1/2.

170
6
Data Streams and Fractional Delays
k
yk
2
1
0
1
âˆ’1
5
2
14
25
3
189
125
4
1564
625
âˆ’
âˆ’
Fig. 6.7 The ramp response of the Thiran all-pass ï¬lter for d = 1/2; the left line corresponds to
the ramp input whereas the right line indicates the input delayed by 3/2
k
yk
âˆ’2
âˆ’1
0
1
1
3
2
14
9
3
67
27
4
284
81
Fig. 6.8 The ramp response of the Thiran all-pass ï¬lter for d = âˆ’1/2; the left line corresponds to
the ramp input whereas the right line indicates the input delayed by 1/2
Again, apart from the transient, an explicit formula for yk can be derived from
the difference equation (6.17). This time, the fk grow and the algebra is a little more
tedious. Apart from the transient we arrive at the following formula:
yk = ka1 + (1 âˆ’a2
1)
kâˆ’1

i=1
(âˆ’1)iâˆ’1aiâˆ’1
1
(k âˆ’i).
(6.29)
This can be rewritten as follows:
yk = ka1 + (1 âˆ’a2
1)(âˆ’a1)kâˆ’1
kâˆ’1

i=1
i
(âˆ’a1)i .
(6.30)

6.4 Thiranâ€™s All-Pass Filter for N = 1
171
Amazingly enough, this sum has be evaluated in the pre-computer era yielding
[8, No.0.113]:
kâˆ’1

i=1
i
(âˆ’a1)i = âˆ’
(k âˆ’1)

âˆ’1
a1
k
1 âˆ’

âˆ’1
a1

+

âˆ’1
a1

â›
âœâ
1 âˆ’

âˆ’1
a1
kâˆ’1

1 âˆ’

âˆ’1
a1
2
â
âŸâ .
(6.31)
We now get from (6.31) after a lengthy simpliï¬cation:
yk = k âˆ’

1 âˆ’(âˆ’a1)k 1 âˆ’a1
1 + a1
.
(6.32)
Finally, we replace a1 by d and obtain:
yk = k âˆ’(1 + d)

1 âˆ’

d
d + 2
k
.
(6.33)
For large k we get approximately:
yk = k âˆ’(1 + d)
(6.34)
from which we see immediately that yk grows like k but with a delay of 1 + d. We
also see that the approach to the ramp is monotonous for positive d (always above
the ramp values shifted by the delay, apart from the transient) whereas is oscillates
for negative d (actually oscillating around the ramp values shifted by the delay).
It was worth while going through all the tedious algebra because we directly see
the low frequency limit of Ï„group = 1 + d in the output yk.
Of course, in real applications, larger values for N are in use. The coefï¬cients ai
are different, but the principle is the same. Of course, using N â€œpastâ€ data means
that the inevitable delay will be N. If you happen to hear the same radio program
with two different receivers, say on AM and FM, and one transmission sounds like
â€œechoingâ€ the other one, you can be sure that digital audio processing was used.
Playground
6.1 Whatâ€™s Your Average?
Calculate the average of the function with the Nyquist frequency and the triangular
envelope as a function of the delay d of the Lagrange interpolator with N = 1.
6.2 Late Impulse
Calculate and plot the response of the N = 1 Thiran all-pass ï¬lter to an impulse for
d = 1/2 and d = âˆ’1/2.

172
6
Data Streams and Fractional Delays
6.3 The Devil Takes the Hindmost
a. Calculate and plot the response of the N = 1 Thiran all-pass ï¬lter to the trailing
edge of the unit pulse for d = 1/2.
b. Show that the sum of all output data of a unit pulse is conserved.
c. Explain why this must be the case.
6.4 Delayed Nyquist
Calculate and plot the response of the N = 1 Thiran all-pass for an input with
cos Î©Nyq starting at t = 0 and being 0 at earlier times for d = 1/2. Compare the
result with the Lagrange N = 1 interpolator.

Chapter 7
Tomography: Backprojection of Filtered
Projections
Abstract In this chapter the backprojection of ï¬ltered projections, a common algo-
rithm in tomography, is presented. A simple example, the uniform disc, is discussed
in detail in its continuous version in order to illustrate how the backprojection serves
both purposes: to reconstruct the object and to â€œerase the shadowâ€ where there is no
object. It is shown how in a discrete version the backprojection of ï¬ltered projec-
tions is directly obtained by a convolution of the inverse Fourier transformed ramp
function and the projections.
Tomography is a method to obtain 3D-images from objects without the need to cut
the object into sections (like the greek word â€œtomosâ€ would suggest). It would not
be applicable for living objects anyway. For tomography one needs particles like X-
ray photons, neutrons, high energy protons (for small objects) or even electrons (for
extremely small objects) which traverse the object and which are detected behind
the object. The contrast can, e.g., be given by absorption, i.e. not all particles arrive
at the detector, or the energy loss of the particles. Here, we discuss only the parallel
beam geometry. It could be a collimated or focussed beam (pencil geometry) which
is scanned over the object or the object is moved in front of the beam, or one can use
a broad beam and a position sensitive detector.
In the following we shall speak of â€œscreenâ€ for the detector and the result on the
screen is called â€œprojectionâ€. In order to obtain all information for the reconstruction
of the 3D-image, the object has to be rotated in front of the beam, i.e. we need
as many different projections under different angles as possible. In the following
we shall discuss a single slice only, i.e. a 2D-object (with ï¬nite thickness, if you
want). The task is to reconstruct the 2D-image from the projections. The 3D-image
is then obtained by stacking the slices. Finally, slices or cross-sections in any desired
direction can be computed from the full 3D-image of the object.
7.1 Projection
The projection is deï¬ned as the path integral through the object, as illustrated in
Fig.7.1. This path integral is known as the Radon transform in order to honour the
person who ï¬rst showed how the object can be recovered from projections. It takes a
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9_7
173

174
7
Tomography: Backprojection of Filtered Projections
y
x
t
y
x
t
Î¸
Fig. 7.1 Illustration of the path through the object for the pencil geometry. Beam parallel to y-axis
(left); beam rotated by Î¸ (right). The distance of the path from the origin is t = x cos Î¸ + y sin Î¸
while to get used to the word â€œtransformâ€. We assume for simplicity that the direction
of the path plays no role, i.e. it can be carried out from â€œfrontâ€ to â€œbackâ€ or vice
versa. Hence, we require projections for an angular range of 0 â‰¤Î¸ â‰¤180â—¦only. This
is true only if the object does not have large density inhomogeneities. A front-only
bullet-proof vest protects against a shot aiming at your breast but not when the shot
hits your back!
The deï¬nition of the projection PÎ¸(t) of the object Ï(x, y) from different angles
Î¸ is:
PÎ¸(t) =
+âˆ

âˆ’âˆ
Ï(x, y) Î´(x cos Î¸ + y sin Î¸ âˆ’t) dxdy
(7.1)
Here, the Î´-â€œfunctionâ€ is unity if and only if its argument is 0. In other words,
the Î´-â€œfunctionâ€ ensures that only points (x, y) contribute to the integral which
are on a line with a distance t to the origin. We could rotate the sample by âˆ’Î¸ in
Fig.7.1(right) in order to leave the beam vertical. Fortunately, we are free to rotate
the sample clockwise or counter-clockwise, so we do not have to worry about the
sign of Î¸. You may look up [8, No. 3.02.2] and see that the sign of Î¸ does not matter
in the integration over Î¸.
Stacking all PÎ¸(t) versus Î¸ we get what is called a â€œsinogramâ€ because a single
mass point rotating in front of the screen would give a sinusoidal projection, even-
tually with a phase. Since the object could be considered to consist of mass points
(maybe better voxels) the sinogram is a superposition of the sinusoidal projections of
all mass points. Thus the sinogram contains all necessary information for the image
reconstruction.

7.2 Backprojection of Filtered Projections
175
7.2 Backprojection of Filtered Projections
A standard method for the image reconstruction in tomography is the so-called
â€œBackprojection of ï¬ltered projectionsâ€. Needless to say that we require a Fourier
transformation to illustrate the method. Here, we use a spatial coordinate x and an
â€œangular wave numberâ€ k instead of time t and angular frequency Ï‰ as we did thus
far. A simple periodic density, e.g., will be written like
Ï(x) = cos kx.
(7.2)
It is immediately clear that a simple direct backprojection of the projection would
not do the job. We would get density in the space between the object and the screen
which would be wrong. If there is no density between the object and the screen,
the projection would not depend on the distance between object and screen at all for
parallel beams. Hence, what matters is the change of the projection with respect to the
beam direction, i.e. its spatial derivative. This is also what a rigorous treatment of the
problem would tell us. In order to avoid the awesome view of a screen penetrating the
object it is helpful to consider a single mass point within the object. The derivative
of the projection would be 0 in front of the object and would suddenly switch to
high as the voxel is touched. We could do so for all voxels in the slice andâ€”due to
linearityâ€”ï¬nally superimpose all results. What we get are streaks within the object
space proportional to the path integral trough the object and aligned with the path.
The superposition of all streaky patterns for all angles Î¸ ï¬nally leads to the desired
2D-density distribution.
Now, rather than to do the inverse Radon transform, the Fourier transformation
comes into play:
SÎ¸(k) =
+âˆ

âˆ’âˆ
PÎ¸(t)eâˆ’iktdt.
(7.3)
The Fourier transformation is carried out in 1D only because of the Fourier slice
theorem, sometimes also called the projection-slice theorem or central slice theorem.
It states that the 1D-Fourier transform of a 1D-projection of a 2D-object is the same
as extracting a central sliceâ€”better a pathâ€”through the origin along ky = 0 of the
2D-Fourier transform of the object. You are encouraged to proof it in the exercises.
It is annoying that the word â€œsliceâ€ in the Fourier slice theorem has a different
meaning than the slice we were speaking about when stacking slices of 2D-images
in order to get a 3D-reconstruction.
Now we remember that the Fourier transform of the derivative of a function is
simply the Fourier transform of the function multiplied by iÏ‰ (see (2.4)), or ik in the
present case. Hence the Fourier transform of the derivative of the projection is:
PÎ¸(t) = ikSÎ¸(k).
(7.4)

176
7
Tomography: Backprojection of Filtered Projections
Since the direction of the path in the path integral (7.1) plays no role, we can use
|ik| = |k| as the multiplier. This multiplier is usually called the â€œramp ï¬lterâ€. It is a
linear high-pass. For very long wavelength it practically suppresses SÎ¸(k) whereas
for high frequencies it enhances SÎ¸(k).
Next, we carry out the inverse Fourier transform:
QÎ¸(t) =
1
(2Ï€)2
+âˆ

âˆ’âˆ
SÎ¸(k)|k|e+iktdk.
(7.5)
I am sure you have noticed the prefactor
1
(2Ï€)2 . Where does it come from? Letâ€™s argue
with the consideration of units. For the sake of simplicity we have used no units for
Ï(x). Hence, the projection PÎ¸(t) has the unit â€œlengthâ€ and SÎ¸(k) the unit â€œareaâ€.
The inverse Fourier transform is carried out by the integral of (7.5) and the angular
integral (7.6). Hence, the integral in (7.5) accounts for two dimensions and thus we
require
1
(2Ï€)2 as prefactor. At this point I am tempted to regret not to have used 2Ï€
times the wave number instead of the angular wave number (like 2Ï€Î½ instead of
Ï‰) because there would be no prefactor at all, not in one nor in two dimensions.
However, we would have missed the insight into the secrets of what mathematicians
call â€œcylinder coordinatesâ€.
Finally, we have to integrate over all angles Î¸:
Ï(x) =
Ï€
0
QÎ¸(x cos Î¸ + y sin Î¸) dÎ¸.
(7.6)
Here, we have used t
= x cos Î¸ + y sin Î¸. The last two steps represent the
â€œbackprojectionâ€.
This procedure looks very complicated, and in reality, nobody would follow this
recipe, as we shall see at the end of this chapter. However, it is instructive to see, how
â€œdensityâ€ is backprojected where we want it and how it is â€œerasedâ€ where we donâ€™t.
Let us do a â€œsimpleâ€ example:
Example 7.1 (â€œSpherical diskâ€ of uniform density)
Ï(r) =
â§
â¨
â©
1 for r â‰¤R
0 else
.
(7.7)
Due to symmetry it sufï¬ces to consider Ï(x) only. In this case all PÎ¸(t) are identical
and the integration over Î¸ should be trivial. This is true for t â‰¤R, but not so for the
â€œoutsideâ€, i.e. t larger than R, as we shall see!

7.2 Backprojection of Filtered Projections
177
The projection reads:
PÎ¸(t) =
â§
âªâ¨
âªâ©
2R

1 âˆ’
	 t
R

2 inside
0
outside
.
(7.8)
The Fourier transform of it reads:
SÎ¸(k) = 2
R

0
2R

1 âˆ’
 t
R
2
cos kt dt = 2Ï€R2 J1(kR)
kR
.
(7.9)
Here, we have used [8, No. 3.752.2]. J1(kR) is a Bessel function, no surprise for
cylindrical symmetry. Donâ€™t worry about the denominator. The J1(kR) goes to 0 with
the same power of k thus the expression remains ï¬nite for k = 0, see Fig.7.2.
The factor of 2 in front of the integral comes from the fact that we integrate over
positive t only. This is sufï¬cient for an even integrand. Similarly, we do not have to
bother about the imaginary part of the Fourier transform, it simply vanishes.
We also see that SÎ¸(k) is an even function because the Bessel function J1(kR) is
odd as is the denominator.
âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
5
10
15
20
25
30
J1(kR)/kR
k
Fig. 7.2 J1(kR)/kR versus k for R = 1

178
7
Tomography: Backprojection of Filtered Projections
Then we calculate with [15,5413b]:
QÎ¸(t) =
â§
âªâªâªâªâ¨
âªâªâªâªâ©
1
Ï€
for t < R
âˆ’1
Ï€
â›
â
1

1âˆ’

R
t
2 âˆ’1
â
â for t â‰¥R
.
(7.10)
We note that for t â€œinside Râ€ QÎ¸(t) does not depend on t. Integrating QÎ¸(t) from 0
to Ï€ just gives Ï(x) = 1 or Ï(r) = 1, as it should be. Thus far for the â€œpaintingâ€ part
of the backprojection.
Surprisingly, at ï¬rst glance, QÎ¸(t) for t â€œoutside Râ€ is negative and even diverges
as t approaches R. No wonder, this is our â€œrubberâ€ which erases everything out-
side. How this is done requires the integration over QÎ¸(t) of an arc from Î¸ = 0 to
arccos(R/x) with the â€œoutsideâ€ integrand and from Î¸ = arccos(R/x) to Ï€/2 with
the â€œinsideâ€ integrand, as illustrated in Fig.7.3.
Ï(x) = 2
â›
â
arccos(R/x)

0
âˆ’1
Ï€
â›
â
1

1âˆ’

R
t
2 âˆ’1
â
â dÎ¸ +
Ï€/2

arccos(R/x)
1
Ï€ dÎ¸
â
â 
= 2
â›
â1
Ï€
Ï€/2

0
dÎ¸ âˆ’1
Ï€
arccos(R/x)

0
1

1âˆ’

R
x cos2 Î¸
2
â
â dÎ¸
(7.11)
The factor 2 in front comes from the fact that it sufï¬ces to integrate up to Ï€/2 only
due to symmetry. In the last step we have used t = x cos Î¸ because it sufï¬ces to
calculate Ï along the x-axis only. We can later replace x by r, again by symmetry.
The ï¬rst integral is trivial and yields 1, including the factor of 2 in front. The
second integral is a little tricky. We substitute t = tan Î¸, dÎ¸ =
	
1/
	
1 + t2

dt, and
1/ cos2 Î¸ = 1 + t2, as suggested in [16,p. 117].
After somerearrangements weendupwiththefollowingexpressionfor thesecond
integral including the prefactor 2:
âˆ’2
Ï€ arctan
â›
â
t
	 x
R

2 âˆ’1 âˆ’t2
â
â 


( x
R)2âˆ’1
0
.
(7.12)
Here we used [16, 20, 236]. Inserting the upper boundary we see that the denominator
goes to 0. Hence, we ï¬nally obtain âˆ’1 because arctan âˆ= Ï€/2 and the lower
boundary does not contribute anything. Hence, we ï¬nally get Ï(x) = 0 outside, as
required. Due to rotational symmetry we can also write Ï(r) = 0 outside.

7.2 Backprojection of Filtered Projections
179
Perspective
view
Side
view
Side
view
Side
view
Top
view
1
x
Perspective
view
Side
view
Side
view
Side
view
Top
view
2R
PÎ˜(t)
t
Perspective
view
Side
view
Side
view
Side
view
Top
view
Ï€R2
SÎ˜(k)
k
FT
Perspective
view
Side
view
Side
view
Side
view
Top
view
SÎ˜(k) Â·| k|
k
ï¬ltered
Perspective
view
Side
view
Side
view
Side
view
Top
view
1
Ï€
QÎ˜(t)
t
FTâˆ’
âˆ’
1
Perspective
view
Side
view
Side
view
Side
view
Top
view
R
y
x
Perspective
view
Side
view
Side
view
Side
view
Top
view
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Ï(x)
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
Î˜
Fig. 7.3 Illustration of the â€œrecipeâ€ from top to bottom: projection of homogeneous disc PÎ¸(t); its
Fourier transform SÎ¸(k); QÎ¸(t), the inverse Fourier transform of SÎ¸(k) multiplied by the ramp |k|;
integration of QÎ¸(t) over Î¸ for t â€œoutside Râ€
We see that the expression for QÎ¸(t) with t â€œoutside Râ€ integrated over the corre-
sponding arc exactly â€œerasesâ€ what the expression for QÎ¸(t) with t â€œinside Râ€ with
its corresponding complementary arc has â€œpaintedâ€.
Since the length of the arc outside tends to 0 the closer we come with t to R, the
â€œrubberâ€ must be inï¬nitely efï¬cient. Now we understand the singularity in (7.10).
All the above individual steps are illustrated in Fig.7.3.
At the end we show, as promised, how the backprojection of ï¬ltered projections is
carried out in reality. The whole effort in Fourier transforming the projection, weight-
ing, and backtransforming is completely unnecessary using the convolution theorem:
the integrand of the backtransformation is the product of the Fourier-transformed pro-
jection SÎ¸(k) and the â€œrampâ€ |k|. This is identical to the convolution of the projection

180
7
Tomography: Backprojection of Filtered Projections
itself (forth and back transformation compensate each other) with the inverse Fourier
transform of the â€œrampâ€.
Now you might wonder why we did not do it like this right away. The answer is:
we cannot! The inverse Fourier transform of |k| does not exist, the integral diverges.
Donâ€™t worry! In the discrete world we have a ï¬nite length Î”x between neighbouring
data points of the projection and, hence, a kNyq. This corresponds to pixels alternating
between 1 and 0. Faster variations of the projection data simply do not exist. Hence,
the inverse Fourier transform of the ï¬nite ramp exists, of course.
WehavealreadycalculatedsomethingverysimilartotheinverseFouriertransform
of the double-sided ramp, i.e. the Fourier transform of the triangular function. The
positive sign in the exponent does not matter since we are dealing with an even
function. What should not be forgotten is that there is no prefactor
1
2M for the inverse
Fourier transform. I leave it to the exercise to calculate it.
Thus a discrete convolution is not very time consuming. What remains is the
summation over the discrete angles Î¸.
In practice, of course, more sophisticated equipment like fan geometry beams
or even 3D-detectors in positron annihilation tomography (PET) are used as well
as other â€œï¬ltersâ€. Often more sophisticated reconstruction algorithms like iterative
algorithms are required which could cope with complications like lateral straggling
of the beam, absorption of soft X-rays or a limited range of projection angles. We
pedestrians are happy with the basics.
Playground
7.1 Go on the Ramp, not on the Rampage!
Calculate the inverse discrete Fourier transform of the discrete double-sided ramp.
Hint: You may either use the ï¬rst shifting rule or remember the tricky Carl
Friedrich GauÃŸ.
7.2 Slice It!
Proof the Fourier slice theorem.
Hint: Let the projection be onto the x-axis. There is no loss of generality because
we are free to choose the coordinate system.
7.3 Reconstruct It!
Suppose, we have the following object with two projections (smallest, non-trivial
symmetric image):

Playground
181
1
0
0
0
y
x
If it helps, consider a cube of uniform density and its shadow (=projection) when
illuminated with a light-beam from the x- and y-direction. 1 = there is a cube, 0 =
there is no cube (but here we have a 2D-problem).
Use a ramp ï¬lter, deï¬ned as {G0 = 0, G1 = 1} and periodic continuation in order
to convolve the projection with the Fourier-transformed ramp-ï¬lter and project the
ï¬ltered data back. Discuss all possible different images.
Hint: Perform convolution along the x- and y-direction consecutively.

Appendix: Solutions
Playground of Chap. 1
1.1 Very Speedy
Ï‰ = 2Ï€Î½
with Î½ = 100 Ã— 106 sâˆ’1
= 628.3 Mrad/s
T = 1
Î½ = 10 ns ; s = cT = 3 Ã— 108 m/s Ã— 10âˆ’8 s = 3 m.
Easy to remember: 1ns corresponds to 30cm, the length of a ruler.
1.2 Totally Odd
It is mixed since neither f (t) = f (âˆ’t) nor f (âˆ’t) = âˆ’f (t) is true. The graphical
solution is shown in Fig.A.1. The Fourier series for both feven(t) and fodd(t) are
inï¬nite series; the series for the even part decreases with 1/k2 (kink) whereas that
for the odd part decreases with 1/k (discontinuity).
1.3 Absolutely True
This is an even function! It could have been written as f (t) = | sin Ï€t| in âˆ’âˆâ‰¤
t â‰¤+âˆas well. It is most convenient to integrate from 0 to 1, i.e. a full period of
unit length.
Ck =
1

0
sin Ï€t cos 2Ï€ktdt
=
1

0
1
2 [sin(Ï€ âˆ’2Ï€k)t + sin(Ï€ + 2Ï€k)t] dt
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
183

184
Appendix: Solutions
= 1
2

(âˆ’1)cos(Ï€ âˆ’2Ï€k)t
Ï€ âˆ’2Ï€k

1
0
+(âˆ’1)cos(Ï€ + 2Ï€k)t
Ï€ + 2Ï€k

1
0

1
âˆ’1
1
f(t)
t
0.5
âˆ’1
1
feven,1(t)
t
âˆ’0.5
0.5
âˆ’1
1
fodd,1(t)
t
0.5
âˆ’1
1
feven,2(t)
t
âˆ’0.5
0.5
âˆ’1
1
fodd,2(t)
t
0.5
âˆ’1
1
feven(t)
t
âˆ’0.5
0.5
âˆ’1
1
fodd(t)
t
Fig. A.1
f (x) = cos(Ï€t/2) for 0 â‰¤t â‰¤1, periodic continuation in the interval âˆ’1 â‰¤t â‰¤0 is
dotted; the following two graphs add up correctly for the interval 0 â‰¤t â‰¤1 but give 0 for the
interval âˆ’1 â‰¤t â‰¤0; the next two graphs add up correctly for the interval âˆ’1 â‰¤t â‰¤0 and leave
the interval 0 â‰¤t â‰¤1 unchanged; the bottom two graphs show feven(t) = feven,1(t) + feven,2(t)
and fodd(t) = fodd,1(t) + fodd,2(t) (from top to bottom)

Appendix: Solutions
185
= 1
2
(âˆ’1) cos Ï€(1 âˆ’2k)
Ï€ âˆ’2Ï€k
+
1
Ï€ âˆ’2Ï€k + (âˆ’1) cos Ï€(1 + 2k)
Ï€ + 2Ï€k
+
1
Ï€ + 2Ï€k

= 1
2

(âˆ’1)
â¡
â£
=(âˆ’1)
cos Ï€
=1
cos 2Ï€k +
=0
sin Ï€ sin 2Ï€k
Ï€ âˆ’2Ï€k
â¤
â¦
+(âˆ’1)
â¡
â£
=(âˆ’1)
cos Ï€
=1
cos 2Ï€k âˆ’
=0
sin Ï€ sin 2Ï€k
Ï€ + 2Ï€k
â¤
â¦+
2Ï€
Ï€2 âˆ’4Ï€2k2

= 1
2

1
Ï€ âˆ’2Ï€k +
1
Ï€ + 2Ï€k +
2Ï€
Ï€2 âˆ’4Ï€2k2

=
2
Ï€ âˆ’4Ï€k2
=
2
Ï€(1 âˆ’4k2)
f (t) =
k=0
2
Ï€ âˆ’
k=Â±1
4
3Ï€ cos 2Ï€t âˆ’
k=Â±2
4
15Ï€ cos 4Ï€t âˆ’
k=Â±3
4
35Ï€ cos 6Ï€t âˆ’Â· Â· Â· .
1.4 Rather Complex
The function f (t) = 2 sin(3Ï€t/2) cos(Ï€t/2) for 0 â‰¤t â‰¤1 can be rewritten using a
trigonometric identity as f (t) = sin Ï€t + sin 2Ï€t. We have just calculated the ï¬rst
part and the linearity theorem tells us that we only have to calculate Ck for the second
part and then add both coefï¬cients. The second part is an odd function! We actually
do not have to calculate Ck because the second part is our basis function for k = 1.
Hence,
Ck =
â§
â¨
â©
i/2
for k = +1
âˆ’i/2 for k = âˆ’1
0
else
.
Together:
Ck =
2
Ï€(1 âˆ’4k2) + i
2Î´k,1 âˆ’i
2Î´k,âˆ’1.
The graphical solution is displayed in Fig.A.2.
1.5 Shiftily
With the First Shifting Rule we get:
Cnew
k
= e+i2Ï€k 1
2 Cold
k
= e+iÏ€kCold
k
= (âˆ’1)kCold
k .

186
Appendix: Solutions
Shifted ï¬rst part:
Shifted second part:
even terms remain unchanged, odd
terms get a minus sign. We would
have to calculate:
imaginary parts for k = Â±1 now
get a minus sign because the
amplitude is negative.
Ck =
1/2

âˆ’1/2
cos Ï€t cos 2Ï€kt dt.
FigureA.3 illustrates both shifted parts. Note the kink at the center of the interval
which results from the fact that the slopes of the unshifted function at the interval
boundaries are different (see Fig.A.2).
0
1
0
1
f(t)
t
âˆ’1
0
1
1
f(t)
t
âˆ’0.5
0
1
2
1
f(t)
t
Fig. A.2 sin Ï€t (top); sin 2Ï€t (middle); sum of both (bottom)

Appendix: Solutions
187
1
âˆ’1
0
1
f(t)
t
âˆ’1
1
âˆ’1
1
f(t)
t
âˆ’0.5
1
2
âˆ’1
1
f(t)
t
Fig. A.3 Shifted ï¬rst part, shifted second part, sum of both (from top to bottom)
1.6 Cubed
The function is even, the Ck are real. With the trigonometric identity cos3 2Ï€t =
(1/4)(3 cos 2Ï€t + cos 6Ï€t) we get:
C0 = 0
A0 = 0
C1 = Câˆ’1 = 3/8
or
A1 = 3/4.
C3 = Câˆ’3 = 1/8
A3 = 1/4

188
Appendix: Solutions
Check using the Second Shifting Rule: cos3 2Ï€t = cos 2Ï€t cos2 2Ï€t. From (1.5) we
get cos2 2Ï€t = 1/2 + (1/2) cos 4Ï€t, i.e. Cold
0
= 1/2, Cold
2
= Cold
âˆ’2 = 1/4.
From (1.36) with T = 1 and a = 1 we get for the real part (the Bk are 0):
C0 = A0;
Ck = Ak/2;
Câˆ’k = Ak/2,
Cold
0
= 1/2 and Cold
2
= Cold
âˆ’2 = 1/4
with Cnew
k
= Cold
kâˆ’1:
Cnew
0
= Cold
âˆ’1 = 0
Cnew
1
= Cold
0
= 1/2
Cnew
âˆ’1 = Cold
âˆ’2 = 1/4
Cnew
2
= Cold
1
= 0
Cnew
âˆ’2 = Cold
âˆ’3 = 0
Cnew
3
= Cold
2
= 1/4
Cnew
âˆ’3 = Cold
âˆ’4 = 0.
Note, that for the shifted Ck we do no longer have Ck = Câˆ’k! Let us construct the
Anew
k
ï¬rst:
Anew
k
= Cnew
k
+ Cnew
âˆ’k
Anew
0
= 0; Anew
1
= 3/4; Anew
2
= 0; Anew
3
= 1/4. In fact, we want to have
Ck = Câˆ’k, so we better deï¬ne Cnew
0
= Anew
0
and Cnew
k
= Cnew
âˆ’k = Anew
k
/2.
FigureA.4 shows the decomposition of the function f (t) = cos3 2Ï€t using a
trigonometric identity.
The Fourier coefï¬cients Ck of cos2 2Ï€t before and after shifting using the Second
Shifting Rule as well as the Fourier coefï¬cients Ak for cos2 2Ï€t and cos3 2Ï€t are
displayed in Fig.A.5.
1.7 Tackling Inï¬nity
Let T = 1 and set Bk = 0. Then we have from (1.50):
1

0
f (t)2dt = A2
0 + 1
2
âˆ

k=1
A2
k.
We want to have A2
k âˆ1/k4 or Ak âˆÂ±1/k2. Hence, we need a kink in our function,
like in the â€œtriangular functionâ€. However, we do not want the restriction to odd k.
Letâ€™s try a parabola. f (t) = t(1 âˆ’t) for 0 â‰¤t â‰¤1.

Appendix: Solutions
189
âˆ’1
0
1
1
f(t)
t
âˆ’0.5
0
0.5
1
f(t)
t
âˆ’1
0
1
1
f(t)
t
Fig. A.4 The function f (t) = cos3 2Ï€t can be decomposed into f (t) = (3 cos 2Ï€t + cos 6Ï€t)/4
using a rigonometric identity
For k Ì¸= 0 we get:
Ck =
1

0
t(1 âˆ’t) cos 2Ï€ktdt

190
Appendix: Solutions
1
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Cold
k
k
1
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Cnew
k
k
0
1
0
1
2
3
Aold
k
k
0
1
0
1
2
3
Anew
k
k
Fig. A.5 Fourier coefï¬cients Ck for f (t) = cos2 2Ï€t = 1/2+(1/2) cos 4Ï€t and after shifting using
the SecondShiftingRule (toptwo).Fouriercoefï¬cients Ak for f (t) = cos2 2Ï€t and f (t) = cos3 2Ï€t
(bottom two)
=
1

0
t cos 2Ï€ktdt âˆ’
1

0
t2 cos 2Ï€ktdt
= cos 2Ï€kt
(2Ï€k)2

1
0
+t sin 2Ï€kt
2Ï€k

1
0
âˆ’

2t
(2Ï€k)2 cos 2Ï€kt +
 t2
2Ï€k âˆ’
2
(2Ï€k)3

sin 2Ï€kt
 
1
0
= âˆ’

2
(2Ï€k)2 Ã— 1 +
 1
2Ï€k âˆ’
2
(2Ï€k)3

Ã— 0 âˆ’

0 âˆ’
2
(2Ï€k)3

Ã— 0

= âˆ’
1
2Ï€2k2 .

Appendix: Solutions
191
For k = 0 we get:
C0 =
1

0
t(1 âˆ’t)dt =
1

0
tdt âˆ’
1

0
t2dt
= t2
2

1
0
âˆ’t3
3

1
0
= 1
2 âˆ’1
3
= 1
6.
From the left hand side of (1.50) we get:
1

0
t2(1 âˆ’t)2dt =
1

0
(t2 âˆ’2t3 + t4)dt
= t3
3 âˆ’2t4
4 + t5
5

1
0
= 1
3 âˆ’1
2 + 1
5
= 10 âˆ’15 + 6
30
= 1
30.
Hence, with A0 = C0 and Ak = Ck + Câˆ’k = 2Ck we get:
1
30 = 1
36 + 1
2
âˆ

k=1

1
Ï€2k2
2
= 1
36 +
1
2Ï€4
âˆ

k=1
1
k4
or
 1
30 âˆ’1
36

2Ï€4 =
âˆ

k=1
1
k4 = 36 âˆ’30
1080 2Ï€4
= 6Ï€4
540 = Ï€4
90.
1.8 Smoothly
From (1.63) we know that a discontinuity in the function leads to a
 1
k

-dependence;
a discontinuity in the ï¬rst derivative leads to a

1
k2

-dependence etc.
Here, we have:
f = 1 âˆ’8t2 + 16t4
is continuous at the boundaries
f â€² = âˆ’16t + 64t3 = âˆ’16t(1 âˆ’4t2) is continuous at the boundaries
f â€²â€² = âˆ’16 + 192t2
is still continuous at the boundaries
f â€²â€²â€² = 384t
is not continuous at the boundaries
f â€²â€²â€² 
âˆ’1
2

= âˆ’192
f â€²â€²â€² 
+ 1
2

= +192.

192
Appendix: Solutions
Hence, we should have a

1
k4

-dependence.
Check by direct calculation. For k Ì¸= 0 we get:
Ck =
+1/2

âˆ’1/2
(1 âˆ’8t2 + 16t4) cos 2Ï€ktdt
= 2
1/2

0
(cos 2Ï€kt âˆ’8t2 cos 2Ï€kt + 16t4 cos 2Ï€kt)dt
with a = 2Ï€k
= 2
sin at
a
âˆ’8
 2t
a2 cos at +
t2
a âˆ’2
a3

sin at

+ t4 sin at
a
âˆ’4
a
3t2
a2 âˆ’6
a4

sin at âˆ’
t3
a âˆ’6t
a3

cos at
 
1/2
0
= 2

âˆ’8
 1
a2 (âˆ’1)k

+ 16 1
2a4 (âˆ’1)k(a2 âˆ’24)

= 2(âˆ’1)k
 8
a2 + 8
a4 (a2 âˆ’24)

= 16(âˆ’1)k

âˆ’1
a2 + 1
a2 âˆ’24
a4

= âˆ’16 Ã— 24(âˆ’1)k
a4
= âˆ’384(âˆ’1)k
a4
= âˆ’24(âˆ’1)k
Ï€4k4
.
For k = 0 we get:
C0 = 2
1/2

0
(1 âˆ’8t2 + 16t4)dt
= 2

t âˆ’8
3t3 + 16
5 t5
 
1/2
0
= 2
1
2 âˆ’8
3
1
8 + 16
5
1
32

= 2
1
2 âˆ’1
3 + 1
10

= 215 âˆ’10 + 3
30
= 8
15.

Appendix: Solutions
193
Playground of Chap. 2
2.1 Black Magic
FigureA.6 illustrates the construction:
i. The inclined straight line is y = x tan Î¸, the straight line parallel to the x-axis is
y = a. Their intersection yields x tan Î¸ = a or x = a cot Î¸.
The circle is written as x2 +(y âˆ’a/2)2 = (a/2)2 or x2 + y2 âˆ’ay = 0. Inserting
x = y cot Î¸ for the inclined straight line yields y2 cot2 Î¸ + y2 = ay or â€“ dividing
by y Ì¸= 0 â€“ y = a/(1+cot2 Î¸) = a sin2 Î¸ (the trivial solution y = 0 corresponds
to the intersection at the origin and Â±âˆ).
ii. Eliminating Î¸ we get y = a/(1 + (x/a)2) = a3/(a2 + x2).
iii. Calculating the Fourier transform is the reverse problem of (2.17):
F(Ï‰) = 2
âˆ

0
a3
a2 + x2 cos Ï‰xdx
= 2a3
âˆ

0
cos Ï‰axâ€²
a2 + a2xâ€²2 adxâ€²
with x = axâ€²
= 2a2
âˆ

0
cos Ï‰axâ€²
1 + xâ€²2 dxâ€²
= a2Ï€eâˆ’a|Ï‰|
the double-sided exponential. In fact, what mathematicians call the â€œversieraâ€ of
Agnesi isâ€”apart from constantsâ€”identical to what physicists call a Lorentzian.
a
2
y
x
Î¸
Fig. A.6 The â€œversieraâ€ of Agnesi: a construction recipe for a Lorentzian with ruler and circle

194
Appendix: Solutions
What about â€œBlack magicâ€? A rational function, the geometric locus of a simple
problem involving straight lines and a circle, has a transcendental Fourier trans-
form and vice versa! No surprise, the trigonometric functions used in the Fourier
transformation are transcendental themselves!
2.2 The Phase Shift Knob
We write f (t) â†”Re{F(Ï‰)} + i Im{F(Ï‰)} before shifting. With the First Shifting
Rule we get:
f (t âˆ’a) â†”(Re{F(Ï‰)} + i Im{F(Ï‰)}) (cos Ï‰a âˆ’i sin Ï‰a)
= Re{F(Ï‰)} cos Ï‰a + Im{F(Ï‰)} sin Ï‰a
+ i (Im{F(Ï‰)} cos Ï‰a âˆ’Re{F(Ï‰)} sin Ï‰a).
The imaginary part vanishes for tan Ï‰a = Im{F(Ï‰)}/Re{F(Ï‰)} or a = (1/Ï‰) Ã—
arctan(Im{F(Ï‰)}/Re{F(Ï‰)}). For a sinusoidal input with phase shift, i.e. f (t) =
sin(Ï‰t âˆ’Ï•), we identify a with Ï•/Ï‰, hence Ï• = a arctan(Im{F(Ï‰)}/ Re{F(Ï‰)}).
This is our â€œphase shift knobâ€. If, e.g., Re{F(Ï‰)} were 0 before shifting, we would
have to turn the â€œphase shift knobâ€ by Ï‰a = Ï€/2 orâ€”with Ï‰ = 2Ï€/T â€”by
a = T/4 (or 90â—¦, i.e. the phase shift between sine and cosine). Since Re{F(Ï‰)}
was non-zero before shifting, less than 90â—¦is sufï¬cient to make the imaginary
part vanish. The real part which builds up upon shifting must be Re{Fshifted} =

Re{F(Ï‰)}2 + Im{F(Ï‰)}2 because |F(Ï‰)| is unaffected by shifting and Im{Fshifted}
= 0. If you are skeptic insert tan Ï‰a = Im{F(Ï‰)}/Re{F(Ï‰)} into the expression for
Re{Fshifted}:
Re{Fshifted} = Re{F(Ï‰)} cos Ï‰a + Im{F(Ï‰)} sin Ï‰a
= Re{F(Ï‰)}
1
âˆš
1 + tan2 Ï‰a
+ Im{F(Ï‰)}
tan Ï‰a
âˆš
1 + tan2 Ï‰a
=
Re{F(Ï‰)} + Im{F(Ï‰)} Im{F(Ï‰)}
Re{F(Ï‰)}

1 + Im{F(Ï‰)}2
Re{F(Ï‰)}2
=

Re{F(Ï‰)}2 + Im{F(Ï‰)}2.
Of course, the â€œphase shift knobâ€ does the job only for a given frequency Ï‰.
2.3 Pulses
f (t) is odd; Ï‰0 = n 2Ï€
T/2 or T
2 Ï‰0 = n2Ï€.
F(Ï‰) = (âˆ’i)
T/2

âˆ’T/2
sin(Ï‰0t) sin Ï‰tdt

Appendix: Solutions
195
= (âˆ’i)1
2
T/2

âˆ’T/2
(cos(Ï‰0 âˆ’Ï‰)t âˆ’cos(Ï‰0 + Ï‰)t) dt
= (âˆ’i)
T/2

0
(cos(Ï‰0 âˆ’Ï‰)t âˆ’cos(Ï‰0 + Ï‰)t) dt
= (âˆ’i)

sin(Ï‰0 âˆ’Ï‰) T
2
Ï‰0 âˆ’Ï‰
âˆ’sin(Ï‰0 + Ï‰) T
2
Ï‰0 + Ï‰

= (âˆ’i)
â›
âœâ
=0
sin Ï‰0 T
2 cos Ï‰ T
2 âˆ’
=1
cos Ï‰0 T
2 sin Ï‰ T
2
Ï‰0 âˆ’Ï‰
âˆ’
=0
sin Ï‰0 T
2 cos Ï‰ T
2 +
=1
cos Ï‰0 T
2 sin Ï‰ T
2
Ï‰0 + Ï‰
â
âŸâ 
= i sin Ï‰ T
2

1
Ï‰0 âˆ’Ï‰ +
1
Ï‰0 + Ï‰

= 2i sin Ï‰T
2 Ã—
Ï‰0
Ï‰2
0 âˆ’Ï‰2 .
At resonance: F(Ï‰0) = âˆ’iT/2; F(âˆ’Ï‰0) = +iT/2; |F(Â±Ï‰0)| = T/2. This is easily
seen by going back to the expressions of the type sin x
x .
For two such pulses centered around Â±Î” we get:
Fshifted(Ï‰) = 2i sin Ï‰T
2 Ã—
Ï‰0
Ï‰2
0 âˆ’Ï‰2

eiÏ‰Î” + eâˆ’iÏ‰Î”
= 4i sin Ï‰T
2 Ã—
Ï‰0
Ï‰2
0 âˆ’Ï‰2 cos Ï‰Î”
â†âˆ’â€œmodulationâ€.
|F(Ï‰0)| = T if at resonance: Ï‰0Î” = lÏ€. In order to maximise |F(Ï‰)| we require
Ï‰Î” = lÏ€;
l = 1, 2, 3, . . .; Î” depends on Ï‰!
2.4 Phase-Locked Pulses
This is a textbook case for the Second Shifting Rule! Hence, we start with DC-pulses.
This function is even!
FDC(Ï‰) =
âˆ’Î”+ T
2

âˆ’Î”âˆ’T
2
cos Ï‰tdt +
+Î”+ T
2

+Î”âˆ’T
2
cos Ï‰tdt = 2
Î”+ T
2

Î”âˆ’T
2
cos Ï‰tdt
with tâ€² = âˆ’t we get a minus sign from dtâ€² and another one from the
reversal of the integration boundaries

196
Appendix: Solutions
= 2sin Ï‰t
Ï‰

Î”+ T
2
Î”âˆ’T
2
= 2sin Ï‰

Î” + T
2

âˆ’sin Ï‰

Î” âˆ’T
2

Ï‰
= 4
Ï‰ cos Ï‰Î” sin Ï‰ T
2 .
With (2.29) we ï¬nally get:
F(Ï‰) = 2i
#
sin(Ï‰ + Ï‰0) T
2 cos(Ï‰ + Ï‰0)Î”
Ï‰ + Ï‰0
âˆ’sin(Ï‰ âˆ’Ï‰0) T
2 cos(Ï‰ âˆ’Ï‰0)Î”
Ï‰ âˆ’Ï‰0
$
= 2i
â¡
â¢â¢â¢â¢â£
cos(Ï‰ + Ï‰0)Î”

sin Ï‰ T
2
=1
cos Ï‰0 T
2 + cos Ï‰ T
2
=0
sin Ï‰0 T
2

Ï‰ + Ï‰0
âˆ’
cos(Ï‰ âˆ’Ï‰0)Î”

sin Ï‰ T
2
=1
cos Ï‰0 T
2 âˆ’cos Ï‰ T
2
=0
sin Ï‰0 T
2

Ï‰ âˆ’Ï‰0
â¤
â¥â¥â¥â¥â¦
= 2i sin Ï‰ T
2
cos(Ï‰ + Ï‰0)Î”
Ï‰ + Ï‰0
âˆ’cos(Ï‰ âˆ’Ï‰0)Î”
Ï‰ âˆ’Ï‰0

= 2i sin Ï‰ T
2
Ï‰2 âˆ’Ï‰2
0
((Ï‰ âˆ’Ï‰0) cos(Ï‰ + Ï‰0)Î” âˆ’(Ï‰ + Ï‰0) cos(Ï‰ âˆ’Ï‰0)Î”).
In order to ï¬nd the extremes it sufï¬ces to calculate:
d
dÎ” ((Ï‰ âˆ’Ï‰0) cos(Ï‰ + Ï‰0)Î” âˆ’(Ï‰ + Ï‰0) cos(Ï‰ âˆ’Ï‰0)Î”) = 0
(Ï‰ âˆ’Ï‰0)(âˆ’1)(Ï‰ + Ï‰0) sin(Ï‰ + Ï‰0)Î” âˆ’(Ï‰ + Ï‰0)(Ï‰ âˆ’Ï‰0) sin(Ï‰ âˆ’Ï‰0)Î” = 0
or (Ï‰2 âˆ’Ï‰2
0)(sin(Ï‰ + Ï‰0)Î” âˆ’sin(Ï‰ âˆ’Ï‰0)Î”) = 0
or (Ï‰2 âˆ’Ï‰2
0) cos Ï‰Î” sin Ï‰0Î” = 0.
This is fulï¬lled for all frequencies Ï‰ if sin Ï‰0Î” = 0 or Ï‰0Î” = lÏ€. With this choice
we get ï¬nally:
F(Ï‰) = 2i sin Ï‰ T
2
Ï‰2 âˆ’Ï‰2
0

(Ï‰ âˆ’Ï‰0)

cos Ï‰Î” cos Ï‰0Î” âˆ’sin Ï‰Î”
=0
sin Ï‰0Î”

âˆ’(Ï‰ + Ï‰0)

cos Ï‰Î” cos Ï‰0Î” + sin Ï‰Î”
=0
sin Ï‰0Î”


Appendix: Solutions
197
= 2i sin Ï‰ T
2
Ï‰2 âˆ’Ï‰2
0
(âˆ’1)l cos Ï‰Î” Ã— 2Ï‰0
= 4iÏ‰0(âˆ’1)l sin Ï‰ T
2 cos Ï‰Î”
Ï‰2 âˆ’Ï‰2
0
.
At resonance Ï‰ = Ï‰0 we get:
|F(Ï‰)| = 4Ï‰0 lim
Ï‰â†’Ï‰0
sin Ï‰ T
2
Ï‰2 âˆ’Ï‰2
0
with T = 4Ï€
Ï‰0
= 4Ï‰0 lim
Ï‰â†’Ï‰0
sin 2Ï€ Ï‰
Ï‰0
Ï‰2
0

Ï‰2
Ï‰2
0 âˆ’1

with Î± = Ï‰
Ï‰0
= 4
Ï‰0
lim
Î±â†’1
sin 2Ï€Î±
(Î± âˆ’1)(Î± + 1)
with Î² = Î± âˆ’1
= 2
Ï‰0
lim
Î²â†’0
sin 2Ï€(Î² + 1)
Î²
= 2
Ï‰0
lim
Î²â†’0
â›
âsin 2Ï€Î²
=1
cos 2Ï€ + cos 2Ï€Î²
=0
sin 2Ï€
Î²
â
â 
= 2
Ï‰0
lim
Î²â†’0
2Ï€ cos 2Ï€Î²
1
= 4Ï€
Ï‰0
= T.
For the calculation of the FWHM we better go back to DC-pulses!
For two pulses separated by 2Î” we get:
FDC(0) = 4T
2 lim
Ï‰â†’0
sin Ï‰ T
2
Ï‰ T
2
= 2T
and |FDC(0)|2 = 4T 2.
From
 4
Ï‰ cos Ï‰Î” sin Ï‰ T
2
2 = 1
2|FDC(0)|2 = 2T 2 we get (using Î”
T = l
4):
16 cos2 Ï‰Tl
4
sin2 Ï‰T
2
= 2T 2Ï‰2
with x = Ï‰T
4
cos2 xl sin2 2x = 2x2.
For l = 1 we get:
cos2 x sin2 2x = 2x2
or cos x sin 2x =
âˆš
2x
cos x Ã— 2 sin x cos x =
âˆš
2x
cos2 x sin x =
x
âˆš
2
.

198
Appendix: Solutions
The solution of this transcendental equation yields:
Ï‰ = 4.265
T
with Î” = T
4 .
For l = 2 we get:
cos2 2x sin2 2x = 2x2
or cos 2x sin 2x =
âˆš
2x
1
2 sin 4x =
âˆš
2x
sin 4x = 2
âˆš
2x.
The solution of this transcendental equation yields:
Ï‰ = 2.783
T
with Î” = T
2 .
These values for the FWHM should be compared with the value for a single DC-pulse
(see (3.12)):
Ï‰ = 5.566
T
.
The Fourier transform of such a double pulse represents the frequency spectrum
which is available for excitation in a resonant absorption experiment. In radiofre-
quency spectroscopy this is called the Ramsey technique, medical doctors would call
it fractionated medication.
2.5 Tricky Convolution
We want to calculate h(t) = f1(t)âŠ—f2(t). Letâ€™s do it the other way round. We know
from the Convolution Theorem that the Fourier transform of the convolution integral
is merely a product of the individual Fourier transforms, i.e.
f1,2(t) = Ïƒ1,2
Ï€
1
Ïƒ2
1,2 + t2
â†”
F1,2(Ï‰) = eâˆ’Ïƒ1,2|Ï‰|.
Check:
F(Ï‰) = 2Ïƒ
Ï€
âˆ

0
cos Ï‰t
Ïƒ2 + t2 dt
= 2
Ï€Ïƒ
âˆ

0
cos Ï‰t
1 + (t/Ïƒ)2 dt

Appendix: Solutions
199
= 2
Ï€Ïƒ
âˆ

0
cos(Ï‰Ïƒtâ€²)
1 + tâ€²2 Ïƒdtâ€²
with tâ€² = t
Ïƒ
= 2
Ï€
Ï€
2 eâˆ’Ïƒ|Ï‰| = eâˆ’Ïƒ|Ï‰|.
No wonder, itâ€™s just the inverse problem of (2.18).
Hence, H(Ï‰) = exp(âˆ’Ïƒ1|Ï‰|) exp(âˆ’Ïƒ2|Ï‰|) = exp(âˆ’(Ïƒ1 + Ïƒ2)|Ï‰|). The inverse
transformation yields:
h(t) = 2
2Ï€
âˆ

0
eâˆ’(Ïƒ1+Ïƒ2)Ï‰ cos Ï‰tdÏ‰
= 1
Ï€
Ïƒ1 + Ïƒ2
(Ïƒ1 + Ïƒ2)2 + t2 ,
i.e. another Lorentzian with Ïƒtotal = Ïƒ1 + Ïƒ2.
2.6 Even Trickier
We have:
f1(t) =
1
Ïƒ1
âˆš
2Ï€
e
âˆ’1
2
t2
Ïƒ2
1
â†”
F1(Ï‰) = eâˆ’1
2 Ïƒ2
1Ï‰2
and:
f2(t) =
1
Ïƒ2
âˆš
2Ï€
e
âˆ’1
2
t2
Ïƒ2
2
â†”
F2(Ï‰) = eâˆ’1
2 Ïƒ2
2Ï‰2.
We want to calculate h(t) = f1(t) âŠ—f2(t).
We have H(Ï‰) = exp
 1
2

Ïƒ2
1 + Ïƒ2
2

Ï‰2
. This we have to backtransform in order
to get the convolution integral:
h(t) = 1
2Ï€
+âˆ

âˆ’âˆ
eâˆ’1
2

Ïƒ2
1+Ïƒ2
2

Ï‰2e+iÏ‰tdÏ‰
= 1
Ï€
âˆ

0
eâˆ’1
2

Ïƒ2
1+Ïƒ2
2

Ï‰2 cos Ï‰tdÏ‰
= 1
Ï€
âˆšÏ€
2 1
âˆš
2

Ïƒ2
1 + Ïƒ2
2
e
âˆ’
t2
4 1
2(Ïƒ2
1+Ïƒ2
2)
=
1
âˆš
2Ï€
1

Ïƒ2
1 + Ïƒ2
2
e
âˆ’1
2
t2
Ïƒ2
1+Ïƒ2
2

200
Appendix: Solutions
=
1
âˆš
2Ï€
1
Ïƒtotal
e
âˆ’1
2
t2
Ïƒ2
total
with Ïƒ2
total = Ïƒ2
1 + Ïƒ2
2.
Hence, it is again a Gaussian with the Ïƒâ€™s squared added. The calculation of the
convolution integral directly is much more tedious:
f1(t) âŠ—f2(t) =
1
Ïƒ1Ïƒ22Ï€
+âˆ

âˆ’âˆ
e
âˆ’1
2
Î¾2
Ïƒ2
1 e
âˆ’1
2
(tâˆ’Î¾)2
Ïƒ2
2 dÎ¾
with the exponent:
âˆ’1
2
#
Î¾2
Ïƒ2
1
+ Î¾2
Ïƒ2
2
âˆ’2tÎ¾
Ïƒ2
2
+ t2
Ïƒ2
2
$
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 â›
âÎ¾2 âˆ’2tÎ¾
Ïƒ2
2
1
1
Ïƒ2
1 + 1
Ïƒ2
2
â
â + t2
Ïƒ2
2
â¤
â¦
= âˆ’1
2
#
1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾2 âˆ’2tÎ¾Ïƒ2
1
Ïƒ2
1 + Ïƒ2
2
+
t2Ïƒ4
1
(Ïƒ2
1 + Ïƒ2
2)2 âˆ’
t2Ïƒ4
1
(Ïƒ2
1 + Ïƒ2
2)2

+ t2
Ïƒ2
2
$
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
2
âˆ’(Ïƒ2
1 + Ïƒ2
2)
Ïƒ2
1Ïƒ2
2
t2Ïƒ4
1
(Ïƒ2
1 + Ïƒ2
2)2 + t2
Ïƒ2
2
â¤
â¦
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
2
âˆ’
t2Ïƒ2
1
Ïƒ2
2(Ïƒ2
1 + Ïƒ2
2) + t2
Ïƒ2
2
â¤
â¦
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
2
+ t2
Ïƒ2
2

1 âˆ’
Ïƒ2
1
Ïƒ2
1 + Ïƒ2
2
â¤
â¦
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
2
+ t2
Ïƒ2
2
Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
â¤
â¦
= âˆ’1
2
â¡
â£

1
Ïƒ2
1
+ 1
Ïƒ2
2
 
Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
2
+
t2
Ïƒ2
1 + Ïƒ2
2
â¤
â¦
hence:
f1(t) âŠ—f2(t) =
1
Ïƒ1Ïƒ22Ï€ e
âˆ’1
2
t2
Ïƒ2
1+Ïƒ2
2
+âˆ

âˆ’âˆ
e
âˆ’1
2

1
Ïƒ2
1
+ 1
Ïƒ2
2

Î¾âˆ’
tÏƒ2
1
Ïƒ2
1+Ïƒ2
2
2
dÎ¾

Appendix: Solutions
201
with Î¾ âˆ’
tÏƒ2
1
Ïƒ2
1 + Ïƒ2
2
= Î¾â€²
=
1
Ïƒ1Ïƒ22Ï€ e
âˆ’1
2
t2
Ïƒ2
1+Ïƒ2
2
+âˆ

âˆ’âˆ
e
âˆ’1
2

1
Ïƒ2
1
+ 1
Ïƒ2
2

Î¾â€²2
dÎ¾â€²
=
1
Ïƒ1Ïƒ22Ï€ e
âˆ’1
2
t2
Ïƒ2
1+Ïƒ2
2
âˆšÏ€
2
2
1
âˆš
2
 1
Ïƒ2
1 + 1
Ïƒ2
2
=
1
âˆš
2Ï€
e
âˆ’1
2
t2
Ïƒ2
1+Ïƒ2
2
1
Ïƒ1Ïƒ2
Ïƒ1Ïƒ2

Ïƒ2
1 + Ïƒ2
2
=
1
âˆš
2Ï€
1
Ïƒtotal
e
âˆ’1
2
t2
Ïƒ2
total
with Ïƒ2
total = Ïƒ2
1 + Ïƒ2
2.
2.7 Voigt Proï¬le (for Gourmets only)
f1(t) = Ïƒ1
Ï€
1
Ïƒ2
1+t2
â†”F1(Ï‰) = eâˆ’Ïƒ1|Ï‰|
f2(t) =
1
Ïƒ2
âˆš
2Ï€e
âˆ’1
2
t2
Ïƒ2
2
â†”F2(Ï‰) = eâˆ’1
2 Ïƒ2
2Ï‰2
H(Ï‰) = eâˆ’Ïƒ1|Ï‰|eâˆ’1
2 Ïƒ2
2Ï‰2.
The inverse transformation is a nightmare! Note that H(Ï‰) is an even function.
h(t) = 1
2Ï€ 2
âˆ

0
eâˆ’Ïƒ1Ï‰eâˆ’1
2 Ïƒ2
2Ï‰2 cos Ï‰tdÏ‰
= 1
Ï€
1
2

2 1
2Ïƒ2
2
 1
2
exp

Ïƒ2
1 âˆ’t2
8 1
2Ïƒ2
2

Ã—Î“ (1)
â§
â¨
â©exp

âˆ’iÏƒ1t
4 1
2Ïƒ2
2

Dâˆ’1
â›
âÏƒ1 âˆ’it

2 1
2Ïƒ2
2
â
â 
+ exp

iÏƒ1t
4 1
2Ïƒ2
2

Dâˆ’1
â›
âÏƒ1 + it

2 1
2Ïƒ2
2
â
â 
â«
â¬
â­

202
Appendix: Solutions
= 1
2Ï€
1
Ïƒ2
exp

Ïƒ2
1 âˆ’t2
4Ïƒ2
2
 
exp

âˆ’iÏƒ1t
2Ïƒ2
2

Dâˆ’1
Ïƒ1 âˆ’it
Ïƒ2

+ c.c.

with Dâˆ’1(z) denoting a parabolic cylinder function. The complex conjugate (â€œc.c.â€)
ensures that h(t) is real. A similar situation shows up in (3.32) where we truncate a
Gaussian. Here, we have a cusp in H(Ï‰). What a messy lineshape for a Lorentzian
spectral line and a spectrometer with a Gaussian resolution function!
Among spectroscopists, this lineshape is known as the â€œVoigt proï¬leâ€. The par-
abolic cylinder function Dâˆ’1(z) can be expressed in terms of the complementary
error function:
Dâˆ’1(z) = e
z2
4
*Ï€
2 erfc
 z
âˆš
2

.
Hence, we can write:
h(t) =
1
2Ï€Ïƒ2
*Ï€
2 e
 Ïƒ1âˆ’it
Ïƒ2
2 1
4 erfc
Ïƒ1 âˆ’it
âˆš
2Ïƒ2

e
+
Ïƒ2
1âˆ’t2
4Ïƒ2
2 e
âˆ’iÏƒ1t
2Ïƒ2
2
+
1
2Ï€Ïƒ2
*Ï€
2 e
 Ïƒ1+it
Ïƒ2
2 1
4 erfc
Ïƒ1 + it
âˆš
2Ïƒ2

e
+
Ïƒ2
1âˆ’t2
4Ïƒ2
2 e
+ iÏƒ1t
2Ïƒ2
2
=
1
âˆš
2Ï€2Ïƒ2

e
1
4Ïƒ2
2
[Ïƒ2
1âˆ’2itÏƒ1âˆ’t2+Ïƒ2
1âˆ’t2âˆ’2iÏƒ1t]
erfc
Ïƒ1 âˆ’it
âˆš
2Ïƒ2

+ e
1
4Ïƒ2
2
[Ïƒ2
1+2itÏƒ1âˆ’t2+Ïƒ2
1âˆ’t2+2iÏƒ1t]
erfc
Ïƒ1 + it
âˆš
2Ïƒ2
 
=
1
âˆš
2Ï€2Ïƒ2

e
1
2Ïƒ2
2
(Ïƒ2
1âˆ’2itÏƒ1âˆ’t2)
erfc
Ïƒ1 âˆ’it
âˆš
2Ïƒ2

+ e
1
2Ïƒ2
2
(Ïƒ2
1+2itÏƒ1âˆ’t2)
erfc
Ïƒ1 + it
âˆš
2Ïƒ2

=
1
âˆš
2Ï€2Ïƒ2
â§
â¨
â©e

Ïƒ1âˆ’it
âˆš
2Ïƒ2
2
erfc
Ïƒ1 âˆ’it
âˆš
2Ïƒ2

+ e

Ïƒ1+it
âˆš
2Ïƒ2
2
erfc
Ïƒ1 + it
âˆš
2Ïƒ2
â«
â¬
â­
=
1
âˆš
2Ï€2Ïƒ2
erfc
Ïƒ1 âˆ’it
âˆš
2Ïƒ2

e

Ïƒ1âˆ’it
âˆš
2Ïƒ2
2
+ c.c.
2.8 Derivable
The function is mixed. We know that dF(Ï‰)
dÏ‰
= âˆ’iFT(t f (t)) with f (t) = eâˆ’t/Ï„ for
t â‰¥0 (see 2.58), and we know its Fourier transform (see 2.21) F(Ï‰) = 1/(Î» + iÏ‰).

Appendix: Solutions
203
Hence:
G(Ï‰) = i d
dÏ‰

1
Î» + iÏ‰

= i
(âˆ’i)
(Î» + iÏ‰)2 =
1
(Î» + iÏ‰)2
=
(Î» âˆ’iÏ‰)2
(Î» + iÏ‰)2(Î» âˆ’iÏ‰)2 = Î»2 âˆ’2iÏ‰Î» âˆ’Ï‰2
(Î»2 + Ï‰2)2
=
Î»2 âˆ’Ï‰2
(Î»2 + Ï‰2)2 âˆ’
2iÏ‰Î»
(Î»2 + Ï‰2)2
= (Î»2 âˆ’Ï‰2) âˆ’2iÏ‰Î»
(Î»2 + Ï‰2)2
.
Inverse transformation:
g(t) = 1
2Ï€
âˆ

âˆ’âˆ
eiÏ‰t
(Î» + iÏ‰)2 dÏ‰
Real part:
1
2Ï€ 2
âˆ

0
cos Ï‰t Î»2 âˆ’Ï‰2
(Î»2 + Ï‰2)2 dÏ‰
Imaginary part:
1
2Ï€ 2
âˆ

0
sin Ï‰t
(âˆ’2)Ï‰Î»
(Î»2 + Ï‰2)2 dÏ‰;
(Ï‰ sin Ï‰t is even in Ï‰!).
Hint: Reference [8, Nos 3.769.1, 3.769.2] Î½ = 2; Î² = Î»; x = Ï‰:
1
(Î» + iÏ‰)2 +
1
(Î» âˆ’iÏ‰)2 = 2(Î»2 âˆ’Ï‰2)
(Î»2 + Ï‰2)2
1
(Î» + iÏ‰)2 âˆ’
1
(Î» âˆ’iÏ‰)2 =
âˆ’4iÏ‰Î»
(Î»2 + Ï‰2)2
âˆ

0
(Î»2 âˆ’Ï‰2)
(Î»2 + Ï‰2)2 cos Ï‰tdÏ‰ = Ï€
2 teâˆ’Î»t
âˆ

0
âˆ’2iÏ‰Î»
(Î»2 + Ï‰2)2 sin Ï‰tdÏ‰ = Ï€
2 iteâˆ’Î»t
from real part
1
Ï€
Ï€
2 teâˆ’Î»t +
from imaginary part
1
Ï€
Ï€
2 teâˆ’Î»t
= teâˆ’Î»t
for t > 0.

204
Appendix: Solutions
2.9 Nothing Gets Lost
First, we note that the integral is an even function and we can write:
âˆ

0
sin2 aÏ‰
Ï‰2
dÏ‰ = 1
2
+âˆ

âˆ’âˆ
sin2 aÏ‰
Ï‰2
dÏ‰.
Next, we identify sin aÏ‰/Ï‰ with F(Ï‰), the Fourier transform of the â€œrectangular
functionâ€ with a = T/2 (and a factor of 2 smaller).
The inverse transform yields:
f (t) =
1/2 for âˆ’a â‰¤t â‰¤a
0
else
and
+a

âˆ’a
| f (t)|2dt = 1
42a = a
2.
Finally, Parsevalâ€™s theorem gives:
a
2 = 1
2Ï€
+âˆ

âˆ’âˆ
sin2 aÏ‰
Ï‰2
dÏ‰
or
âˆ

âˆ’âˆ
sin2 aÏ‰
Ï‰2
dÏ‰ = 2Ï€a
2
= Ï€a
or
âˆ

0
sin2 aÏ‰
Ï‰2
dÏ‰ = Ï€a
2 .
Playground of Chap.3
3.1 Squared
f (Ï‰) = T sin(Ï‰T/2)/(Ï‰T/2). At Ï‰ = 0 we have F(0) = T . This function drops
to T/2 at a frequency Ï‰ deï¬ned by the following transcendental equation:
T
2 = T sin(Ï‰T/2)
Ï‰T/2

Appendix: Solutions
205
with x = Ï‰T/2 we have x/2 = sin x with the solution x = 1.8955, hence Ï‰3dB =
3.791/T . With a pocket calculator we might have done the following:
x
sin x
x/2
1.5
0.997
0.75
1.4
0.985
0.7
1.6
0.9995
0.8
1.8
0.9738
0.9
1.85
0.9613
0.925
1.88
0.9526
0.94
1.89
0.9495
0.945
1.895
0.9479
0.9475
1.896
0.9476
0.948
1.8955
0.94775
0.94775
The total width is Ï‰ = 7.582/T .
For F2(Ï‰) we had Ï‰ = 5.566/T ; hence the 3 dB-bandwidth of F(Ï‰) is a factor
of 1.362 larger than that of F2(Ï‰), about 4% less than
âˆš
2 = 1.414.
3.2 Letâ€™s Gibbs Again
There are tiny steps at the interval boundaries, hence we have âˆ’6 dB/octave.
3.3 Expander
Blackmanâ€“Harris window:
f (t) =
â§
âªâªâªâ¨
âªâªâªâ©
3

n=0
an cos 2Ï€nt
T
for âˆ’T/2 â‰¤t â‰¤T/2
0
else
.
From the expansion of the cosines we get (in the interval âˆ’T/2 â‰¤t â‰¤T/2):
f (t) =
3

n=0
an

1 âˆ’1
2!
2Ï€nt
T
2
+ 1
4!
2Ï€nt
T
4
âˆ’1
6!
2Ï€nt
T
6
+ . . .

=
âˆ

k=0
bk
 t
T/2
2k
.

206
Appendix: Solutions
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
8
10
12
1
2
3
4
5
6
7
8
9 k
bk; br
Fig. A.7 Expansion coefï¬cients bk for the Blackmanâ€“Harris window (âˆ’74 dB) (dotted line) and
expansion coefï¬cients br for the Kaiserâ€“Bessel window (Î² = 9) (solid line). There are even powers
of t only, i.e. the coefï¬cient b6 corresponds to t12
Inserting the coefï¬cients an for the âˆ’74 dB-window including the second option of
the corrections on p. 86 we get:
k
bk
0
+1.0000
1
âˆ’4.4888
2
+8.0592
3
âˆ’10.9471
4
+8.9791
5
âˆ’5.4918
6
+2.7106
7
âˆ’1.1105
8
+0.3761
9
âˆ’0.1047
The coefï¬cients are displayed in Fig.A.7. Note that at the interval boundaries
t = Â±T/2 we should have ,âˆ
k=0 bk = 0. The ï¬rst ten terms add up to âˆ’0.0196.

Appendix: Solutions
207
Next, we calculate:
I0(z) =
âˆ

k=0
 1
4z2k
(k!)2
for z = 9.
k
(4.5k/k!)2
0
1.000
1
20.250
2
102.516
3
230.660
4
291.929
5
236.463
6
133.010
7
54.969
8
17.392
9
4.348
Summing up the ï¬rst ten terms, we get 1,092.5, close to the exact value of
1,093.588.
Next, we have to expand the numerator of the Kaiserâ€“Bessel window function.
I (9) f (t) =
âˆ

k=0
-
81
4

1 âˆ’
 2t
T
2.k
(k!)2
=
âˆ

k=0
 81
4
k
(k!)2

1 âˆ’
2t
T
2k
with
2t
T
2
= y
=
âˆ

k=0
# 9
2
k
k!
$2
(1 âˆ’y)k
#
with binomial formula (1 âˆ’y)k =
k

r=0
k
r

(âˆ’1)r yr =
k

r=0
k!
r!(k âˆ’r)!(âˆ’y)r
$
=
âˆ

k=0
# 9
2
k
k!
$2
k

r=0
k!
r!(k âˆ’r)!(âˆ’y)r
=
âˆ

k=0
# 9
2
k
k!
$2
r=0
+
âˆ

k=1
# 9
2
k
k!
$2
=k
/
01
2
k!
(k âˆ’1)!(âˆ’y)1
r=1

208
Appendix: Solutions
+
âˆ

k=2
# 9
2
k
k!
$2
=k(kâˆ’1)/2
/
01
2
k!
2!(k âˆ’2)! y2
r=2
+
âˆ

k=3
# 9
2
k
k!
$2
=k(kâˆ’1)(kâˆ’2)/6
/
01
2
k!
3!(k âˆ’3)! (âˆ’y)3
r=3
+ . . .
=
âˆ

r=0
br
 t
T/2
2r
(Note: For integer and negative k we have k! = Â±âˆand 0! = 1.).
Here, the calculation of each expansion coefï¬cient br requires (in principle) the
calculation of an inï¬nite series. We truncate the series at k = 9. For r = 0 up to
r = 9 we get:
r
br
0
+1.0000
1
âˆ’4.2421
2
+8.0039
3
âˆ’8.9811
4
+6.7708
5
âˆ’3.6767
6
+1.5063
7
âˆ’0.4816
8
+0.1233
9
âˆ’0.0258
These coefï¬cients are displayed in Fig.A.7. Note, that at the interval boundaries
t = Â±T/2 the coefï¬cients br do no longer have to add up to 0 exactly. FigureA.7
shows why the Blackmanâ€“Harris (âˆ’74 dB) window and the Kaiserâ€“Bessel (Î² = 9)
window have similar properties.
3.4 Minorities
a. For a rectangular window we have Ï‰ = 5.566/T = 50 Mrad/s from which we
get T = 111.32 ns.
b. The suspected signal is at 600 Mrad/s, i.e. 4 times the FWHM away from the
central peak.
The rectangular window is not good for the detection. The triangular window has
a factor 8.016/5.566 = 1.44 larger FWHM, i.e. our suspected peak is 2.78 times
the FWHM away from the central peak. A glance to Fig.3.2 tells you, that this
window is also not good. The cosine window has only a factor of 7.47/5.566 = 1.34
larger FWHM, but is still not good enough. For the cos2-window we have a factor

Appendix: Solutions
209
of 9.06/5.566 = 1.63 larger FWHM, i.e. only 2.45 times the FWHM away from the
central peak. This means, that âˆ’50 dB, 2.45 times the FWHM higher than the central
peak, is still not detectable with this window. Similarly, the Hamming window is not
good enough. The Gauss window as described in Sect.3.7 would be a choice because
Ï‰Ï€ âˆ¼9.06, but the sidelobe suppression just sufï¬ces.
The Kaiserâ€“Bessel window with Î² = 8 has Ï‰T âˆ¼10, but sufï¬cient sidelobe
suppression, and, of course, both Blackmanâ€“Harris windows would be adequate.
Playground of Chap.4
4.1 Correlated
{hk} = (const./N) ,Nâˆ’1
l=0
fl, independent of k if ,Nâˆ’1
l=0
fl vanishes (i.e. the average
is 0) then {hk} = 0 for all k, otherwise {hk} = const. Ã— âŸ¨flâŸ©for all k (see Fig.A.8).
4.2 No Common Ground
hk = 1
N
Nâˆ’1

l=0
flgâˆ—
l+k
fk
k
Fj
j
gk
k
Gj
j
hk
k
Hj = Fj Â· Gj
j
Fig. A.8 An arbitrary fk (top left) and its Fourier transform Fj (top right). A constant gk (middle
left) and its Fourier transform G j (middle right). The product of Hj = Fj G j (bottom right) and
its inverse transform hk (bottom left)

210
Appendix: Solutions
we donâ€™t need âˆ—here.
h0 = 1
4( f0g0 + f1g1 + f2g2 + f3g3)
= 1
4(1 Ã— 1 + 0 Ã— (âˆ’1) + (âˆ’1) Ã— 1 + 0 Ã— (âˆ’1)) = 0
h1 = 1
4( f0g1 + f1g2 + f2g3 + f3g0)
= 1
4(1 Ã— (âˆ’1) + 0 Ã— 1 + (âˆ’1) Ã— (âˆ’1) + 0 Ã— 1) = 0
h2 = 1
4( f0g2 + f1g3 + f2g0 + f3g1)
= 1
4(1 Ã— 1 + 0 Ã— (âˆ’1) + (âˆ’1) Ã— 1 + 0 Ã— (âˆ’1)) = 0
h3 = 1
4( f0g3 + f1g0 + f2g1 + f3g2)
= 1
4(1 Ã— (âˆ’1) + 0 Ã— 1 + (âˆ’1) Ã— (âˆ’1) + 0 Ã— 1) = 0
f corresponds to half the Nyquist frequency and g corresponds to the Nyquist fre-
quency. Their cross correlation vanishes. The FT of { fk} is {Fj} = {0, 1/2, 0, 1/2},
the FT of {gk} is {G j} = {0, 0, 1, 0}. The multiplication of FjG j shows that there
is nothing in common:
FjG j = {0, 0, 0, 0} and, hence, {hk} = {0, 0, 0, 0}.
4.3 Brotherly
F0 = 1
2
F1 = 1
4

1 + 0 Ã— eâˆ’2Ï€iÃ—1
4
+ 1 Ã— eâˆ’2Ï€iÃ—2
4
+ 0 Ã— eâˆ’2Ï€iÃ—3
4

= 1
4(1 + 0 + (âˆ’1) + 0) = 0
F2 = 1
4

1 + 0 Ã— eâˆ’2Ï€iÃ—2
4
+ 1 Ã— eâˆ’2Ï€iÃ—4
4
+ 0 Ã— eâˆ’2Ï€iÃ—6
4

= 1
4(1 + 0 + 1 + 0) = 1
2
F3 = 0
G j = {0, 0, 1, 0}
Nyquist frequency
Hj = FjG j = {0, 0, 1/2, 0} .

Appendix: Solutions
211
Inverse transformation:
hk =
Nâˆ’1

j=0
Hj W +kj
N
W +kj
4
= e
2Ï€ikj
N .
Hence:
hk =
3

j=0
Hje
2Ï€ikj
4
=
3

j=0
Hjei Ï€kj
2
h0 = H0 + H1 + H2 + H3 = 1
2
h1 = H0 + H1 Ã— i + H2 Ã— (âˆ’1) + H3 Ã— (âˆ’i) = âˆ’1
2
h2 = H0 + H1 Ã— (âˆ’1) + H2 Ã— 1 + H3 Ã— (âˆ’1) = 1
2
h3 = H0 + H1 Ã— (âˆ’i) + H2 Ã— (âˆ’1) + H3 Ã— i = âˆ’1
2.
FigureA.9 is the graphical illustration.
âˆ’1
âˆ’0.5
0
0.5
1
fk
k
Fj
j
âˆ’1
âˆ’0.5
0
0.5
1
gk
k
Gj
j
âˆ’1
âˆ’0.5
0
0.5
1
hk
k
Hj = Fj Â· Gj
j
Fig. A.9 Nyquist frequency plus const.= 1/2 (top left) and its Fourier transform Fj (top right).
Nyquist frequency (middle left) and its Fourier transform G j (middle right). Product of Hj = Fj G j
(bottom right) and its inverse transform (bottom left)

212
Appendix: Solutions
4.4 Autocorrelated
N = 6, real input:
hk = 1
6
5

l=0
fl fl+k
h0 = 1
6
5

l=0
f 2
l = 1
6(1 + 4 + 9 + 4 + 1) = 19
6
h1 = 1
6( f0 f1 + f1 f2 + f2 f3 + f3 f4 + f4 f5 + f5 f0)
= 1
6(0 Ã— 1 + 1 Ã— 2 + 2 Ã— 3 + 3 Ã— 2 + 2 Ã— 1 + 1 Ã— 0)
= 1
6(2 + 6 + 6 + 2) = 16
6
h2 = 1
6( f0 f2 + f1 f3 + f2 f4 + f3 f5 + f4 f0 + f5 f1)
= 1
6(0 Ã— 2 + 1 Ã— 3 + 2 Ã— 2 + 3 Ã— 1 + 2 Ã— 0 + 1 Ã— 1)
= 1
6(3 + 4 + 3 + 1) = 11
6
h3 = 1
6( f0 f3 + f1 f4 + f2 f5 + f3 f0 + f4 f1 + f5 f2)
= 1
6(0 Ã— 3 + 1 Ã— 2 + 2 Ã— 1 + 3 Ã— 0 + 2 Ã— 1 + 1 Ã— 2)
= 1
6(2 + 2 + 2 + 2) = 8
6
h4 = 1
6( f0 f4 + f1 f5 + f2 f0 + f3 f1 + f4 f2 + f5 f3)
= 1
6(0 Ã— 2 + 1 Ã— 1 + 2 Ã— 0 + 3 Ã— 1 + 2 Ã— 2 + 1 Ã— 3)
= 1
6(1 + 3 + 4 + 3) = 11
6
h5 = 1
6( f0 f5 + f1 f0 + f2 f1 + f3 f2 + f4 f3 + f5 f4)
= 1
6(0 Ã— 1 + 1 Ã— 0 + 2 Ã— 1 + 3 Ã— 2 + 2 Ã— 3 + 1 Ã— 2)
= 1
6(2 + 6 + 6 + 2) = 16
6 .
FT of { fk}: N = 6, fk = fâˆ’k = f6âˆ’k â†’even!

Appendix: Solutions
213
Fj = 1
6
5

k=0
fk cos 2Ï€kj
6
= 1
6
5

k=0
fk cos Ï€kj
3
F0 = 1
6(0 + 1 + 2 + 3 + 2 + 1) = 9
6
F1 = 1
6

1 cos Ï€
3 + 2 cos 2Ï€
3 + 3 cos 3Ï€
3 + 2 cos 4Ï€
3 + 1 cos 5Ï€
3

= 1
6
1
2 + 2 Ã—

âˆ’1
2

+ 3 Ã— (âˆ’1) + 2 Ã—

âˆ’1
2

+ 1 Ã— 1
2

= 1
6
1
2 âˆ’1 âˆ’3 âˆ’1 + 1
2

= 1
6(âˆ’4) = âˆ’4
6
F2 = 1
6

1 cos 2Ï€
3 + 2 cos 4Ï€
3 + 3 cos 6Ï€
3 + 2 cos 8Ï€
3 + 1 cos 10Ï€
3

= 1
6

âˆ’1
2 + 2 Ã—

âˆ’1
2

+ 3 Ã— 1 + 2 Ã—

âˆ’1
2

+ 1 Ã—

âˆ’1
2

= 1
6(âˆ’1 âˆ’2 + 3) = 0
F3 = 1
6

1 cos 3Ï€
3 + 2 cos 6Ï€
3 + 3 cos 9Ï€
3 + 2 cos 12Ï€
3
+ 1 cos 15Ï€
3

= 1
6(âˆ’1 + 2 Ã— 1 + 3 Ã— (âˆ’1) + 2 Ã— 1 + 1 Ã— (âˆ’1))
= 1
6(âˆ’5 + 4) = âˆ’1
6
F4 = F2 = 0
F5 = F1 = âˆ’4
6.
{F2
j } =
9
4, 4
9, 0, 1
36, 0, 4
9

.
FT({hk}):
H0 = 1
6
19
6 + 16
6 + 11
6 + 8
6 + 11
6 + 16
6

= 81
36 = 9
4
H1 = 1
6
19
6 + 16
6 cos Ï€
3 + 11
6 cos 2Ï€
3 + 8
6 cos 3Ï€
3 + 11
6 cos 4Ï€
3 + 16
6 cos 5Ï€
3

= 4
9
H2 = 1
6
19
6 + 16
6 cos 2Ï€
3 + 11
6 cos 4Ï€
3 + 8
6 cos 6Ï€
3 + 11
6 cos 8Ï€
3 + 16
6 cos 10Ï€
3

= 0

214
Appendix: Solutions
H3 = 1
6
19
6 + 16
6 cos 3Ï€
3 + 11
6 cos 6Ï€
3 + 8
6 cos 9Ï€
3 + 11
6 cos 12Ï€
3
+ 16
6 cos 15Ï€
3

= 1
36
H4 = H2 = 0
H5 = H1 = 4
9.
4.5 Shifting Around
a. The series is even, because of fk = + fNâˆ’k.
b. Because of the duality of the forward and inverse transformations (apart from the
normalization factor, this only concerns a sign at eâˆ’iÏ‰t â†’e+iÏ‰t) the question
could also be: Which series produces only a single Fourier coefï¬cient when
Fourier-transformed, incidentally at frequency 0? A constant, of course. The
Fourier transformation of a â€œdiscrete Î´-functionâ€ therefore is a constant (see
Fig.A.10).
c. The series is mixed. It is composed as shown in Fig.A.11.
d. The shifting only results in a phase in Fj, d.h., |Fj|2 stays the same.
Â·Â·Â·
F(j)
j
Fig. A.10 Answer (b)
Â· Â· Â·
1
N âˆ’1N
Â· Â· Â·
1
N âˆ’1
Â· Â· Â·
1
N âˆ’1
=
+
Fig. A.11 Answer (c)
Fig. A.12 Real part of the Fourier transform of the random series

Appendix: Solutions
215
Fig. A.13 Imaginary part of the Fourier transform of the random series
4.6 Pure Noise
a. We get a random series both in the real part (Fig.A.12) and in the imaginary part
(Fig.A.13). Random means the absence of any structure.
So all spectral components have to occur, and they, in turn, have to be random,
otherwise the inverse transformation would generate a structure.
b. Trick: For N â†’âˆwe can imagine the random series as the discrete version of
the function f (t) = t for âˆ’1/2 â‰¤t â‰¤1/2. For this purpose we only have to
order the numbers of the random series according to their magnitudes! According
to Parsevalâ€™s theorem (4.31) we donâ€™t have to do a Fourier transformation at all.
So with 2N + 1 samples we need:
2
2N + 1
N

k=0
 k
N
2
=
2
2N + 1
1
4N 2
(2N + 1)N(N + 1)
6
(A.1)
= N + 1
12N ;
lim
Nâ†’âˆ
N + 1
12N
= 1
12.
We could have solved the following integral instead:
+0.5

âˆ’0.5
t2dt = 2
+0.5

0
t2dt = 2t3
3

0.5
0
= 2
3
1
8 = 1
12.
(A.2)
Letâ€™s compare: 0.5 cos Ï‰t has, due to cos2 Ï‰t = 0.5, the signal power 0.52Ã—0.5 =
1/8.
Fig. A.14 Real part of the Fourier transform according to (4.58)

216
Appendix: Solutions
Gj
j
â†“
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Fj:
G0:
G1:
G2:
Fig. A.15 Result of the cross correlation: at the position of the fundamental frequency at channel 4
the â€œsignalâ€ (arrow) is clearly visible; channel 0 also happens to run up, however, there is no
corresponding pattern
4.7 Pattern Recognition
Itâ€™s best to use the cross correlation. It is formed with the Fourier transform of
the experimental data Fig.A.14 and the theoretical â€œfrequency combâ€, the pattern
(Fig.4.29). As weâ€™re looking for cosine patterns, we only use the real part for the
cross correlation.
Here, channel 36 goes up (from 128 channels to Î©Nyq). The right half is the mirror
image of the left half. So the Fourier transform suggests only a spectral component
(apart from noise) at (36/128)Î©Nyq = (9/32)Î©Nyq. If we search for pattern Fig.4.29
in the data, we get something totally different.
The result of the cross correlation with the theoretical frequency comb leads to
the following algorithm:
G j = F5 j + F7 j + F9 j.
(A.3)
The result shows Fig.A.15.
So the noisy signal contains cosine components with the frequencies 5Ï€(4/128),
7Ï€(4/128), and 9Ï€(4/128).
4.8 Go on the Ramp (for Gourmets only)
The series is mixed because neither fk = fâˆ’k nor fk = âˆ’fâˆ’k is true.
Decomposition into even and odd parts.
We have the following equations:
fk = f even
k
+ f odd
k
f even
k
= f even
Nâˆ’k
for k = 0, 1, . . . , N âˆ’1.
f odd
k
= âˆ’f odd
Nâˆ’k
The ï¬rst condition gives N equations for 2N unknowns. The second and third equa-
tions give each N further conditions, each appears twice, hence we have N additional
equations. Instead of solving this system of linear equations, we solve the problem
by arguing.
First, because of f odd
0
= 0 we have f even
0
= 0. Shifting the ramp downwards by
N/2 we already have an odd function with the exception of k = 0 (see Fig.A.16):

Appendix: Solutions
217
1
2
3
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
fk
k
1
2
3
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
fk,even
k
âˆ’2
âˆ’1
1
âˆ’4
âˆ’3
âˆ’2
âˆ’1
1
2
3
fk,odd
k
âˆ’2
âˆ’1
1
âˆ’4
âˆ’3
âˆ’2
âˆ’1
1
2
3
fk,shifted
k
Fig. A.16 One-sided ramp for N = 4 (periodic continuation with open circles); decomposition
into even and odd parts; ramp shifted downwards by 2 immediately gives the odd part (except for
k = 0) (from top to bottom)

218
Appendix: Solutions
f shifted
k
= k âˆ’N
2
for k = 0, 1, 2, . . . , N âˆ’1.
f shifted
âˆ’k
= f shifted
Nâˆ’k
= (N âˆ’k) âˆ’N
2 = N
2 âˆ’k
= âˆ’

k âˆ’N
2

.
So we have already found the odd part:
f odd
k
= k âˆ’N
2
for k = 1, 2, . . . , N âˆ’1
f odd
0
= 0
and, of course, we have also found the real part:
f even
k
= N
2
for k = 1, 2, . . . , N âˆ’1 (compensates for the shift)
f even
0
= 0
(see above).
Real part of Fourier transform:
Re{Fj} = 1
N
Nâˆ’1

k=1
N
2 cos 2Ï€kj
N
.
Dirichlet: 1/2 + cos x + cos 2x + Â· Â· Â· + cos Nx = sin[(N + 1/2)x]/(2 sin[x/2]);
here we have x = 2Ï€ j/N and instead of N we go until N âˆ’1:
Nâˆ’1

k=1
cos kx = sin(N âˆ’1
2)x
2 sin x
2
âˆ’1
2
=
=0
sin Nx cos x
2âˆ’
=1
cos Nx sin x
2
2 sin x
2
âˆ’1
2
= âˆ’1
2 âˆ’1
2 = âˆ’1.
Re{F0} = 1
N
N
2
(N âˆ’1)
1 2/ 0
number of terms
= N âˆ’1
2
,
Re{Fj} = âˆ’1
2.
Check:
Re{F0} +
Nâˆ’1

j=1
Re{Fj} = N âˆ’1
2
âˆ’1
2(N âˆ’1) = 0.

Appendix: Solutions
219
Imaginary part of Fourier transform:
Im{Fj} = 1
N
Nâˆ’1

k=1

k âˆ’N
2

sin 2Ï€kj
N
.
For the sum over sines we need the analogue of Dirichletâ€™s kernel for sines. Let us
try an expression with an unknown numerator but the same denominator as for the
sum of cosines:
sin x + sin 2x + Â· Â· Â· + sin(N âˆ’1)x =
?
2 sin x
2
2 sin x
2 sin x + 2 sin x
2 sin 2x + . . . + 2 sin x
2 sin(N âˆ’1)x
= cos x
2 âˆ’cos 3x
2 + cos 3x
2
1
2/
0
=0
âˆ’cos 5x
2 + Â· Â· Â· + cos

N âˆ’3
2

x
1
2/
0
=0
âˆ’cos

N âˆ’1
2

x
= cos x
2 âˆ’cos

N âˆ’1
2

x
âˆ’â†’
Nâˆ’1

k=1
sin kx = cos x
2 âˆ’cos

N âˆ’1
2

x
2 sin x
2
= cos x
2âˆ’
=1
cos Nx cos x
2+
=0
sin Nx sin x
2
2 sin x
2
= 0.
Hence, there remains only the term with k sin(2Ï€kj/N). We can evaluate this sum
by differentiating the formula for Dirichletâ€™s kernel (Use the general formula and
insert x = 2Ï€ j/N into the differentiated formula!):
d
dx
Nâˆ’1

k=1
cos kx = âˆ’
Nâˆ’1

k=1
k sin kx
= 1
2

N âˆ’1
2

cos
3
N âˆ’1
2

x
4
sin x
2 âˆ’sin
3
N âˆ’1
2

x
4 1
2 cos x
2
sin2 x
2
= 1
2

N âˆ’1
2
 
=1
cos Nx cos x
2

sin x
2 âˆ’

=0
sin Nx cos x
2 âˆ’
=1
cos Nx sin x
2

1
2 cos x
2
sin2 x
2

220
Appendix: Solutions
= 1
2

N âˆ’1
2
 cos x
2
sin x
2
+ 1
2
cos x
2
sin x
2

= N
2
cos x
2
sin x
2
= N
2 cot Ï€ j
N
Im{Fj} = 1
N (âˆ’1) N
2 cot Ï€ j
N = âˆ’1
2 cot Ï€ j
N ,
j Ì¸= 0,
Im{F0} = 0,
ï¬nally together:
Fj =
â§
âªâªâªâ¨
âªâªâªâ©
âˆ’1
2 âˆ’i
2 cot Ï€ j
N for j Ì¸= 0
N âˆ’1
2
for j = 0
.
Parsevalâ€™s theorem:
left hand side:
1
N
Nâˆ’1

k=1
k2 = 1
N
(N âˆ’1)N(2(N âˆ’1) + 1)
6
= (N âˆ’1)(2N âˆ’1)
6
right hand side:
 N âˆ’1
2
2
+ 1
4
Nâˆ’1

j=1

1 + i cot Ï€ j
N
 
1 âˆ’i cot Ï€ j
N

=
 N âˆ’1
2
2
+ 1
4
Nâˆ’1

j=1

1 + cot2 Ï€ j
N

=
 N âˆ’1
2
2
+ 1
4
Nâˆ’1

j=1
1
sin2 Ï€ j
N
hence:
(N âˆ’1)(2N âˆ’1)
6
=
 N âˆ’1
2
2
+ 1
4
Nâˆ’1

j=1
1
sin2 Ï€ j
N
or 1
4
Nâˆ’1

j=1
1
sin2 Ï€ j
N
= (N âˆ’1)(2N âˆ’1)
6
âˆ’(N âˆ’1)2
4
= (N âˆ’1)(2N âˆ’1)2 âˆ’(N âˆ’1)3
12
= N âˆ’1
12
(4N âˆ’2 âˆ’3N + 3)
= N âˆ’1
12
(N + 1) = N 2 âˆ’1
12

Appendix: Solutions
221
and ï¬nally:
Nâˆ’1

j=1
1
sin2 Ï€ j
N
= N 2 âˆ’1
3
.
The result for ,Nâˆ’1
j=1 cot2(Ï€ j/N) is obtained as follows: we use Parsevalâ€™s theo-
rem for the real/even and imaginary/odd parts separately. For the real part we get:
left hand side:
1
N
 N
2
2
(N âˆ’1) = N(N âˆ’1)
4
right hand side:
 N âˆ’1
2
2
+ N âˆ’1
4
= N(N âˆ’1)
4
.
The real parts are equal, so the imaginary parts of the left and right hand sides have
to be equal, too.
For the imaginary part we get:
left hand side:
1
N
Nâˆ’1

k=1
k âˆ’N
2
2
= 1
N
Nâˆ’1

k=1

k2 âˆ’kN + N 2
4

= 1
N
(N âˆ’1)N(2N âˆ’1)
6
âˆ’N(N âˆ’1)N
2
+ N 2(N âˆ’1)
4

= (N âˆ’1)(N âˆ’2)
12
right hand side:
1
4
Nâˆ’1

j=1
cot2 Ï€ j
N
from which we get ,Nâˆ’1
j=1 cot2 Ï€ j
N = (N âˆ’1)(N âˆ’2)/3.
4.9 Transcendental (for Gourmets only)
The series is even because:
fâˆ’k = fNâˆ’k
?= fk.
Insert N âˆ’k into (4.59) on both sides:

222
Appendix: Solutions
fNâˆ’k =
 N âˆ’k
for N âˆ’k = 0, 1, . . . , N/2 âˆ’1
N âˆ’(N âˆ’k) for N âˆ’k = N/2, N/2 + 1, . . . , N âˆ’1
or
fNâˆ’k =
 N âˆ’k for k = N, N âˆ’1, . . . , N/2 + 1
k
for k = N/2, N/2 âˆ’1, . . . , 1
or
fNâˆ’k =
k
for k = 1, 2, . . . , N/2
N âˆ’k for k = N/2 + 1, . . . , N ,
a. for k = N we have f0 = 0, so we could include it also in the ï¬rst line because
fN = f0 = 0.
b. for k = N/2 we have fN/2 = N/2, so we could include it also in the second line.
This completes the proof. Since the series is even, we only have to calculate the real
part:
Fj = 1
N
Nâˆ’1

k=0
fk cos 2Ï€kj
N
= 1
N
â›
âœâ
N
2 âˆ’1

k=0
k cos 2Ï€kj
N
+
Nâˆ’1

k= N
2
(N âˆ’k) cos 2Ï€kj
N
â
âŸâ 
with kâ€² = N âˆ’k
= 1
N
â›
âœâ
N
2 âˆ’1

k=0
k cos 2Ï€kj
N
+
1

kâ€²= N
2
kâ€² cos 2Ï€(N âˆ’kâ€²) j
N
â
âŸâ 
= 1
N
â›
âœâ
N
2 âˆ’1

k=0
k cos 2Ï€kj
N
+
N
2

kâ€²=1
kâ€²
â›
âœâœâcos 2Ï€N j
N
1
2/
0
=1
cos 2Ï€kâ€² j
N
+ sin 2Ï€N j
N
1
2/
0
=0
sin 2Ï€(âˆ’kâ€²) j
N
â
âŸâŸâ 
â
âŸâŸâ 
= 1
N
â›
âœâ
N
2 âˆ’1

k=0
k cos 2Ï€kj
N
+
N
2

kâ€²=1
kâ€² cos 2Ï€kâ€² j
N
â
âŸâ 
= 1
N
â›
âœâ2
N
2 âˆ’1

k=1
k cos 2Ï€kj
N
+ N
2 cos Ï€ j
â
âŸâ 
with 2Ï€ N
2 j
N
= Ï€ j
= 2
N
N
2 âˆ’1

k=1
k cos 2Ï€kj
N
+ 1
2(âˆ’1) j.

Appendix: Solutions
223
This can be simpliï¬ed further.
How can we get this sum? Let us try an expression with an unknown numerator
but the same denominator as for the sum of cosines (â€œsisterâ€ analogue of Dirichletâ€™s
kernel):
N
2 âˆ’1

k=1
sin kx =
?
2 sin x
2
with x = 2Ï€ j
N .
The numerator of the right hand side is:
2 sin x
2 sin x + 2 sin x
2 sin 2x + Â· Â· Â· + 2 sin x
2 sin
 N
2 âˆ’1

x
= cos
x
2

âˆ’cos
3x
2

+ cos
3x
2

1
2/
0
=0
âˆ’Â· Â· Â·
âˆ’cos
 N
2 âˆ’3
2

x + cos
 N
2 âˆ’3
2

x
1
2/
0
=0
âˆ’cos
 N
2 âˆ’1
2

x
= cos x
2 âˆ’cos N âˆ’1
2
x.
Finally we get:
N
2 âˆ’1

k=1
sin kx =
cos x
2 âˆ’cos N âˆ’1
2
x
2 sin x
2
, N = even, do not use for x = 0.
Now we take the derivative with respect to x. Let us exclude the special case of
x = 0. We shall treat it later.
d
dx
N
2 âˆ’1

k=1
sin kx =
N
2 âˆ’1

k=1
k cos kx
= 1
2

âˆ’1
2 sin x
2 +
 N âˆ’1
2

sin
 N âˆ’1
2

x

sin x
2 âˆ’

cos x
2 âˆ’cos
 N âˆ’1
2

x
 1
2 cos x
2
sin2 x
2
= 1
2
âˆ’1
2 sin2 x
2 âˆ’1
2 cos2 x
2 +
 N âˆ’1
2
 â›
â
=0
sin Nx
2
cos x
2 âˆ’cos Nx
2 sin x
2
â
â sin x
2
sin2 x
2

224
Appendix: Solutions
+1
2
â›
âcos Nx
2 cos x
2 +
=0
sin Nx
2
sin x
2
â
â cos x
2
with x = 2Ï€ j
N , cos Nx
2
= cos Ï€ j = (âˆ’1) j, sin Nx
2
= sin Ï€ j = 0
= 1
2
âˆ’1
2 + N âˆ’1
2
(âˆ’1) j+1 sin2 x
2 + 1
2(âˆ’1) j cos2 x
2
sin2 x
2
= 1
2
âˆ’1
2 + (âˆ’1) j+1 N
2 sin2 x
2 âˆ’1
2(âˆ’1) j
â›
ââˆ’
=âˆ’1
cos2 x
2 âˆ’sin2 x
2
â
â 
sin2 x
2
= 1
2
â›
âœâ
1
2 sin2 x
2

(âˆ’1) j âˆ’1

+ (âˆ’1) j+1 N
2
â
âŸâ 
â‡’Fj = 2
N
â›
âœâ(âˆ’1) j âˆ’1
2
1
2
1
sin2 Ï€ j
N
+ (âˆ’1) j+1 N
4
â
âŸâ + 1
2(âˆ’1) j
= (âˆ’1) j âˆ’1
2N sin2 Ï€ j
N
=
â§
âªâ¨
âªâ©
âˆ’
1
N sin2 Ï€ j
N
for j = odd
0
else
.
The special case of j = 0 is obtained from:
N
2 âˆ’1

k=1
k =
 N
2 âˆ’1
 N
2
2
= N 2
8 âˆ’N
4 .
Hence:
F0 = 2
N
 N 2
8 âˆ’N
4

+ 1
2 = N
4 .
We ï¬nally have:
Fj =
â§
âªâªâªâªâªâ¨
âªâªâªâªâªâ©
âˆ’
1
N sin2 Ï€ j
N
for j = odd
0
for j = even, j Ì¸= 0
N
4
for j = 0
.

Appendix: Solutions
225
Now we use Parsevalâ€™s theorem:
l.h.s.
1
N
â¡
â¢â¢â£2
 N
2 âˆ’1
 N
2

2
 N
2 âˆ’1

+ 1

6
+ N 2
4
â¤
â¥â¥â¦
= 1
N
â¡
â¢â£21
2
(N âˆ’2)1
2 N(N âˆ’1)
6
+ N 2
4
â¤
â¥â¦
= 1
N
 N(N âˆ’1)(N âˆ’2) + 3N 2
12

= (N âˆ’1)(N âˆ’2) + 3N
12
= N 2 + 2
12
r.h.s.
N 2
16 +
Nâˆ’1

j=1
odd
1
N 2 sin4 Ï€ j
N
with j = 2k âˆ’1
=
N/2

k=1
1
N 2 sin4 Ï€(2k âˆ’1)
N
+ N 2
16
which gives:
N 2
12 + 1
6 =
N/2

k=1
1
N 2 sin4 Ï€(2k âˆ’1)
N
+ N 2
16
and ï¬nally:
N/2

k=1
1
sin4 Ï€(2k âˆ’1)
N
= N 2(N 2 + 8)
48
.
The right hand side can be shown to be an integer! Let N = 2M.
4M2(4M2 + 8)
48
= 4M24(M2 + 2)
48
= M2(M2 + 2)
3
= M(M âˆ’1)M(M + 1) + 3M2
3
= M (M âˆ’1)M(M + 1)
3
+ M2.

226
Appendix: Solutions
Three consecutive numbers can always be divided by 3!
Now we use the high-pass property:
Nâˆ’1

j=0
Fj = N
4 âˆ’1
N
Nâˆ’1

j=1
odd
1
sin2 Ï€ j
N
with j = 2k âˆ’1
= N
4 âˆ’1
N
N
2

k=1
1
sin2 Ï€(2k âˆ’1)
N
.
For a high-pass ï¬lter we must have ,Nâˆ’1
j=0 Fj = 0 because a zero frequency must
not be transmitted (see Chap.5). If you want, use deï¬nition (4.13) with k = 0 and
interpret fk being the ï¬lter in the frequency domain and Fj its Fourier transform.
Hence, we get:
N/2

k=1
1
sin2 Ï€(2k âˆ’1)
N
= N 2
4 .
Since N is even, the result is always integer!
These are nice examples how a ï¬nite sum over an expression involving a trans-
cendental function yields an integer!
Playground of Chap.5
5.1 Totally Different
The ï¬rst central difference is:
â€œexactâ€
yk = fk+1 âˆ’fkâˆ’1
2t
f â€²(t)
= âˆ’Ï€
2 sin Ï€
2 t
y0 = f1 âˆ’fâˆ’1
2/3
= f1 âˆ’f5
2/3
= 1 +
âˆš
3/2
2/3
= 2.799 f â€²(t0)
= 0
y1 = f2 âˆ’f0
2/3
= 1/2 âˆ’1
2/3
= âˆ’0.750
f â€²(t1)
= âˆ’Ï€
2 sin Ï€
2
1
3 = âˆ’0.7854
y2 = f3 âˆ’f1
2/3
= 0 âˆ’
âˆš
3/2
2/3
= âˆ’1.299
f â€²(t2)
= âˆ’Ï€
2 sin Ï€
2
2
3 = âˆ’1.3603
y3 = f4 âˆ’f2
2/3
= âˆ’1/2 âˆ’1/2
2/3
= âˆ’1.500
f â€²(t3)
= âˆ’Ï€
2 sin Ï€
2
3
3 = âˆ’1.5708
y4 = f5 âˆ’f3
2/3
= âˆ’
âˆš
3/2 âˆ’0
2/3
= âˆ’1.299
f â€²(t4)
= âˆ’Ï€
2 sin Ï€
2
4
3 = âˆ’1.3603
y5 = f6 âˆ’f4
2/3
= f0 âˆ’f4
2/3
= 1 + 1/2
2/3
= 2.250
f â€²(t5)
= âˆ’Ï€
2 sin Ï€
2
5
3 = âˆ’0.7854.
Of course, the beginning y0 and the end y5 are totally wrong because of the
periodic continuation. Let us calculate the relative error for the other derivatives:

Appendix: Solutions
227
âˆ’1
1
âˆ’1
1
2
3
4
5
6
fk
k
âˆ’2
âˆ’1
1
2
3
âˆ’1
1
2
3
4
5
6
fk, yk
k
Fig. A.17 Input fk = cos Ï€tk/2, tk = kt with k = 0, 1, . . . , 5 and t = 1/3 (top). First central
difference (bottom). The solid line is the exact derivative. y0 and y5 appear to be totally wrong.
However, we must not forget the periodic continuation of the series (see open circles in the top panel)
k = 1
exact âˆ’discrete
exact
= âˆ’0.7854 + 0.750
âˆ’0.7854
= 4.5 % too small
k = 2
4.5 % too small
k = 3
4.5 % too small
k = 4
4.5 % too small.
The result is plotted in Fig.A.17.
5.2 Simpsonâ€™s-1/3 Versus Trapezoid
The exact, trapezoidal, and Simpsonâ€™s-1/3 calculations are illustrated in Fig.A.18.
Trapezoid:
I =

f0
2 +
3

k=1
fk + f4
2

=
1
2 + 0.5 âˆ’0.5 âˆ’1 âˆ’0.5
2

= âˆ’0.75,

228
Appendix: Solutions
âˆ’1
âˆ’0.5
0
0.5
1
1
2
3
4
fk
k
âˆ’1
âˆ’0.5
0
0.5
1
1
yk
k
âˆ’1
âˆ’0.5
0
0.5
1
1
yk
k
2
3
4
2
3
4
Fig. A.18 Input fk = cos Ï€tk, tk = kt, k = 0, 1, . . . , 4, t = 1/3 (top). Area of trapezoids to
be added up. Step width is t (middle). Area of parabolically interpolated segment in Simpsonâ€™s
1/3-rule. Step width is 2t (bottom)
Simpsonâ€™s-1/3:
I =
 f2 + 4 f1 + f0
3

+
 f4 + 4 f3 + f2
3

=
âˆ’0.5 + 4 Ã— 0.5 + 1
3

+
âˆ’0.5 + 4 Ã— (âˆ’1) + (âˆ’0.5)
3

= âˆ’0.833.
In order to derive the exact value we have to convert fk = cos(kÏ€t/3) into f (t) =
cos(Ï€t/3). Hence, we have
 4
0 cos(Ï€t/3)dt = âˆ’0.82699.
The relative errors are:
1 âˆ’trapezoid
exact
= 1 âˆ’
âˆ’0.75
âˆ’0.82699 â‡’9.3 % too small,
1 âˆ’Simpsonâ€™s-1/3
exact
= 1 âˆ’
âˆ’0.833
âˆ’0.82699 â‡’0.7 % too large.
This is consistent with the fact that the Trapezoidal Rule always underestimates the
integral whereas Simpsonâ€™s 1/3-rule always overestimates (see Figs.5.14 and 5.15).

Appendix: Solutions
229
5.3 Totally Noisy
a. You get random noise, and additionally in the real part (because of the cosine!),
a discrete line at frequency (1/4)Î©Nyq (see Figs.A.19 and A.20).
b. If you process the input using a simple low-pass ï¬lter (5.12), the time signal
already looks better as shown in Fig.A.21. The real part of the Fourier transform
of the ï¬ltered function is shown in Fig.A.22.
Fig. A.19 Real part of the Fourier transform of the series according to (5.47)
Fig. A.20 Imaginary part of the Fourier transform of the series according to (5.47)
Fig. A.21 Input that has been processed using a low-pass ï¬lter according to (5.47)
Re{Fj}
j
â†‘
â†‘
Fig. A.22 Real part of the Fourier transform of the ï¬ltered function yk according to Fig.A.21

230
Appendix: Solutions
5.4 Inclined Slope
a. We simply use a high-pass ï¬lter (cf. (5.13)). The result is shown in Fig.A.23.
b. For a â€œÎ´-shaped lineâ€ as input we get precisely the deï¬nition of the high-pass
ï¬lter as result. This leads to the following recommendation for a high-pass ï¬lter
with smaller undershoots:
yk = 1
8(âˆ’fkâˆ’2 âˆ’fkâˆ’1 + 4 fk âˆ’fk+1 âˆ’fk+2).
(A.4)
The result of this data processing is shown in Fig.A.24. If we keep going, weâ€™ll
easily recognise Dirichletâ€™s integral kernel (1.53), that belongs to a step. The
problem here is that boundary effects are progressively harder to handle. Using
recursive ï¬lters, naturally, is much better suited to processing data.
1
0
âˆ’0.44
yk
k
Fig.A.23 DatafromFig.5.17processedusingthehigh-passï¬lter yk = (1/4)(âˆ’fkâˆ’1+2 fkâˆ’fk+1).
The â€œundershootsâ€ donâ€™t look very good
1
0
âˆ’0.42
yk
k
Fig. A.24 Data according to Fig.5.17, processed with the modiï¬ed high-pass ï¬lter according to
(A.4). The undershoots get a bit smaller and wider. Progress admittedly is small, yet visible

Appendix: Solutions
231
Playground of Chap.6
6.1 Whatâ€™s Your Average?
The transfer function of the Lagrange interpolator with N = 1 is unity at zero
frequency, hence the average does not depend on d. In case of doubt, calculate it by
â€œbrute forceâ€.
6.2 Late Impulse
See Figs.A.25 and A.26.
6.3 The Devil Takes the Hindmost
a. See Fig.A.27.
b. The undershoot at the leading edge is compensated at the trailing edge, as are all
other deviations from unity.
c. None of the spectral components is attenuated, i.e. nothing of the input gets lost,
the frequency-dependent group delay is responsible for the fact that it merely gets
redistributed. I am sure you have noticed that the Thiran all-pass ï¬lter can be used
to sum up certain inï¬nite series.
k
yk
âˆ’2
âˆ’1
0
âˆ’1
5
1
24
25
2
24
125
3
24
625
Fig. A.25 Response of the N = 1 Thiran all-pass ï¬lter to an impulse with d = 1/2
k
yk
âˆ’2
âˆ’1
0
1
3
1
8
9
2
âˆ’8
27
3
8
81
Fig. A.26 Response of the N = 1 Thiran all-pass ï¬lter to an impulse with d = âˆ’1/2

232
Appendix: Solutions
yk
k âˆ’2 k âˆ’1
k
k + 1
6
5
k + 2
6
25
k + 3
6
125
Fig. A.27 Response of the N = 1 Thiran all-pass ï¬lter to the trailing edge of a unit pulse for
d = 1/2
k
yk
âˆ’2
âˆ’1
0
âˆ’1
5
1
29
25
2
âˆ’121
125
3
629
625
4
âˆ’3121
3125
Fig. A.28 Response of the N = 1 Thiran all-pass ï¬lter to an input with cos Î©Nyq starting at t = 0
and being 0 at earlier times for d = 1/2
6.4 Delayed Nyquist
Apart from a transient, the full amplitude of cos Î©Nyq is approached rapidly. The
Lagrange N = 1 interpolator would not transmit the signal at all. See Fig.A.28.
Playground of Chap.7
7.1 Go on the ramp, not on the rampage!
a. First Shifting Rule
The coefï¬cients of the Fourier transform of the triangular function for a shift by
M are multiplied by W âˆ’j M
2M
= cos Ï€ j = âˆ’1 for j = 1, 3, . . . with the exception
of F0 = 1/2 which remains unchanged. Multiplying the result by 2M we get:
Fj =
â§
âªâ¨
âªâ©
âˆ’1
M sin2 Ï€ j
2M
for odd j
0
for even j Ì¸= 0
.

Appendix: Solutions
233
b. Tricky Carl Friedrich GauÃŸ
The double-sided ramp is 1 minus the triangular function. Hence, all non-zero
Fourier coefï¬cients except j = 0 are the same as for the triangular function but
negative and we have to multiply by 2M to get F0 = 1 âˆ’1/2 = 1/2. The result
is identical, of course:
Fj =
â§
âªâ¨
âªâ©
âˆ’1
M sin2 Ï€ j
2M
for odd j
0
for even j
.
After all, the triangular function and the double-sided ramp complement each other
like man and woman.
7.2 Slice it!
P(x) =
 +âˆ
âˆ’âˆÏ(x, y)dy.
FT(kx, ky) =
 +âˆ
âˆ’âˆÏ(x, y)eâˆ’i(kx x+ky y)dxdy.
For the â€œcentral sliceâ€ we get:
FT(kx, 0) =
+âˆ

âˆ’âˆ
Ï(x, y)eâˆ’ikx xdxdy
=
+âˆ

âˆ’âˆ
â›
â
+âˆ

âˆ’âˆ
Ï(x, y)dy
â
â eâˆ’ikx xdx
=
+âˆ

âˆ’âˆ
P(x)eâˆ’ikx xdx.
This is the 1D-Fourier transform of P(x).
7.3 Reconstruct it!
The inverse FT of double-sided ramp ï¬lter: (N = 2)
g0 = (G0 + G1) = 1
g1 = (G0 + G1eiÏ€) = âˆ’1
The convolution is deï¬ned as follows:
hk = 1
2
1

l=0
flgkâˆ’l.

234
Appendix: Solutions
Image # 1:
1
0
0
0
y
x
Convolution:
Note, that the f â€™s are the projections.
x-direction:
f0 = 1
f1 = 0
h0 = 1
2( f0g0 + f1g1) = 1
2 (1 Ã— 1 + 0 Ã— (âˆ’1)) = +1
2
h1 = 1
2( f0g1 + f1g0) = 1
2 (1 Ã— (âˆ’1) + 0 Ã— 1) = âˆ’1
2
y-direction:
f0 = 1
f1 = 0, hence, we get the same result as for the x -direction
h0 = +1
2
h1 = âˆ’1
2
convolved:
backprojected:
+ 1
2 + 1
2 â†
âˆ’1
2 âˆ’1
2 â†
x
+
y
â†“
â†“
+ 1
2 âˆ’1
2
+ 1
2 âˆ’1
2
=
+1
0
0
âˆ’1
The box with âˆ’1 is an reconstruction artefact. Use a cutoff: all negative values
do not correspond to an object.
Image # 2:
1
1
0
0
y
x

Appendix: Solutions
235
Convolution:
x -direction:
f0 = 2
f1 = 0
h0 = 1
2 (2 Ã— 1 + 0 Ã— (âˆ’1)) = +1
h1 = 1
2 (2 Ã— (âˆ’1) + 0 Ã— 1) = âˆ’1
y -direction:
f0 = 1
f1 = 1
h0 = 1
2 (1 Ã— 1 + 1 Ã— (âˆ’1)) = 0
h1 = 1
2 (1 Ã— (âˆ’1) + 1 Ã— 1) = 0
convolved:
backprojected:
+1 +1 â†
âˆ’1 âˆ’1 â†
x
+
y
â†“
â†“
0
0
0
0
=
+1 +1
âˆ’1 âˆ’1
Here, we have an interesting situation: the ï¬ltered y-projection vanishes iden-
tically because a constantâ€”donâ€™t forget the periodic continuationâ€”cannot pass
through a high-pass ï¬lter. In other words, a uniform object looks like no object
at all! All that matters is contrast!
Image # 3:
1
0
0
1
This â€œdiagonal objectâ€ cannot be reconstructed. We would require projections
along the diagonals!
Image # 4:
1
1
1
0
is the â€œreverseâ€ of image # 1.
Image # 5:
1
1
1
1
is like a white rabbit in snow or a black panther in the dark.

References
1. H.J. Lipkin, Beta-Decay for Pedestrians (North-Holland Publ, Amsterdam, 1962)
2. H.J. Weaver, Applications of Discrete and Continuous Fourier Analysis (A Wiley-Interscience
Publication, New York, 1983)
3. H.J. Weaver, Theory of Discrete and Continuous Fourier Analysis (Wiley, New York, 1989)
4. T. Butz, Fouriertransformation fÃ¼r FuÃŸgÃ¤nger (Teubner, Wiesbaden, 2003)
5. Wolfram Research, Inc., Mathematica, Version 10.1, Champaign, IL, USA
6. E. Zeidler (ed.), Oxford Usersâ€™ Guide to Mathematics (Oxford University Press, Oxford, 2004)
7. W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, Numerical Recipes, The Art of
Scientiï¬c Computing (Cambridge University Press, New York, 1989)
8. I.S. Gradshteyn, I.M. Ryzhik, Tables of Integrals, Series, and Products (Academic Press Inc,
San Diego, 1980)
9. F.J. Harris, Proc. IEEE 66, 51 (1978)
10. M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions (Dover Publications Inc,
New York, 1972)
11. A.H. Nuttall, IEEE Trans. Acoust. Speech Signal Process. ASSP-29, 84â€“91 (1981)
12. J.-P. Thiran, IEEE Trans. Circuit Theory CT-18, 659â€“664 (1971)
13. A. Fettweis, IEEE Trans. Audio Electroacoust. AU-20, 112â€“114 (1972)
14. V. VÃ¤limÃ¤ki, Helsinki University of Technology, Faculty of Electrical Engineering, Laboratory
of Acoustics and Audio Signal Processing, Report 37, ISBN 951-22-2880-7, ISSN 0356-083X
(1995)
15. W. GrÃ¶bner, N. Hofreiter, Integraltafel, Zweiter Teil: Bestimmte Integrale (Springer, Wien,
1961)
16. W. GrÃ¶bner, N. Hofreiter, Integraltafel, Erster Teil: Unbestimmte Integrale (Springer, Wien,
1965)
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
237

Index
A
Acausal, 138, 144
Algorithm
acausal, 138, 144
causal, 144
of Cooley and Tukey, 125
Aliasing, 114
â€œAll-poleâ€-ï¬lter, 165
â€œAngular wave numberâ€, 175
Autocorrelation, 57, 133
discrete, 109
B
Backprojection of ï¬ltered projections, 173,
175
Backward difference
ï¬rst, 148
Band-pass ï¬lter, 142
Basis functions
of cosine transformation, 116
of Fourier transformation, 4, 11
of sine transformation, 116
Bessel function, 83, 86, 91, 177
Besselâ€™s inequality, 23
Blackmanâ€“Harris window, 85, 91
Butterï¬‚y scheme, 130
C
Causal, 144
Central difference
ï¬rst, 148, 153
second, 149
Central slice theorem, 175
Convolution, 90, 181
discrete, 105
of functions, 47, 69
â€œConvolution sumâ€, 105
Convolution Theorem, 52, 133, 138
discrete, 106
inverse, 53
Cooley and Tukey
algorithm of, 125
Cosine transformation, 116, 118
basis functions of, 116
Cosine window, 77
cos2-window, 78
Cross correlation, 56, 132, 216
discrete, 108
D
Data
compression, 147
differentiation, 148
integration, 149
mirroring, 115
shifting, 146
smoothing, 139
streams, 155
DC-component, 7, 9
Decimation in time, 129
Î´-function, 34
discrete, 94, 133
Difference equation, 156, 169, 170
Digitizer, 155
Dirichletâ€™s integral kernel, 25, 28, 121, 230
Dolphâ€“Chebychev window, 81
E
Error-function, 51, 82
complementary, 38, 51
Â© Springer International Publishing Switzerland 2015
T. Butz, Fourier Transformation for Pedestrians, Undergraduate Lecture
Notes in Physics, DOI 10.1007/978-3-319-16985-9
239

240
Index
Eulerâ€™s identity, 11, 44
Exponential function
bilateral, 39
truncated, 67
unilateral, 41, 62
truncated, 64
F
Fan geometry beams, 180
Fast Fourier transformation, 125, 127
Feedback, 143
Fejer window, 76
FFT, 125
FIFO, 138, 155
Filter
â€œall-poleâ€-ï¬lter, 165
band-pass, 142
high-pass, 141, 230
low-pass, 141, 229
non-recursive, 143
notch, 142
overview, 143
ramp ï¬lter, 141, 176, 181
recursive, 143, 230
Thiranâ€™s all-pass ï¬lter for N = 1, 165
Filter effect, 137
â€œFinite Impulse Responseâ€, 165
FIR, 165
First in, ï¬rst out, 138, 155
Forward difference
ï¬rst, 148
Forward transformation, 35, 214
Fourier coefï¬cients, 5, 8, 21, 22
complex, 31
discrete, 95, 98
Fourier, J.B.J., vii
Fourier series, 1, 3, 13
complex notation, 9
Fourier slice theorem, 175, 180
Fourier transform, 133
inverse
of the â€œrampâ€, 180
Fourier transformation
deï¬nition by Weaver, 36
discrete, 93
deï¬nition, 96
forward, 35
inverse, 35
of derivatives, 60
Fractional delays, 155
Full width at half maximum, 68, 91, 197
Function
even, 2, 31, 34, 69
mixed, 31, 69
odd, 2, 31, 34, 69
FWHM, 68, 91, 197
G
Gauss function, 38, 69
Gauss window, 82, 91
Gauss, C.F., 160
Gibbsâ€™
overshoot, 27
phenomenon, 24
ringing, 30
undershoot, 28
â€œGroup delayâ€, 156
H
Half width at half maximum, 39
Hamming window, 80
Hanning window, 78
Heavisideâ€™s step function, 62, 165, 168
High-pass ï¬lter, 135, 141, 230
HWHM, 39
I
IIR, 165
Image reconstruction, 180
Imaginary part, 41
Impulse response, 167
â€œInï¬nite Impulse Responseâ€, 165
Interference term, 63â€“65, 67
Inverse transformation, 35, 214
K
Kaiserâ€“Bessel window, 83, 91
â€œKernelâ€, 94
Kronecker symbol, 94
L
Lâ€™Hospitalâ€™s rule, 10, 72, 122
Lagrange interpolator, 156
Lagrange-interpolation scheme, 155
Linear interpolation, 146
Linearity Theorem, 13, 42, 100
Lorentz function, 39, 69
Low-pass ï¬lter, 141, 229
N
Noise power, 133

Index
241
Normalisation factor, 103
Notch ï¬lter, 142
Nyquist frequency, 102, 107, 123
O
Orthogonal system, 6
Orthonormal system, 6
Oscillation equation, 61
Overlap, 48
Overshoot, 30
P
Parsevalâ€™s
equation, 23
theorem, 31, 58, 69, 135, 215
discrete, 109
Partial sums, 21
expression of unit step, 28
integral notation, 26
Path integral, 173
Pattern recognition, 134
Pedestal, 62, 65, 80
Fourier transform of, 65
Periodic continuation, 8, 31, 93, 181
PET, 180
Phase, 41
Phase factor, 14, 16, 18
Phase shift knob, 67
Pitfalls, 62
Polar representation, 41, 63
Positron annihilation tomography, 180
Power representation, 41, 64, 67, 71, 109,
125
â€œProjectionâ€, 173
Projection-slice theorem, 175
R
Radon transform, 173, 175
Ramp ï¬lter, 141, 176, 181
Ramp response, 169
Random series, 133
Real part, 41
â€œRectangular functionâ€, 37
convolution of, 48, 104
shifted, 44
Rectangular window, 72, 91
3 dB-bandwidth, 74, 91
asymptotic behaviour, 75
central peak, 72
sidelobe suppression, 74
zeros, 72
Resolution function
instrumental, 104
â€œResonanceâ€, 41, 68
Resonance enhancement, 144
Response
impulse, 167
ramp, 169
step, 168
Riemannâ€™s localisation theorem, 27
S
Sampling Theorem, 110, 112
Saw-tooth, 127, 128
decomposition, 128
Scaling Rule, 102
Scaling Theorem, 21
â€œScreenâ€, 173
Series
even, 94, 133
mixed, 133
odd, 93, 133
random, 133
Seume, J.G., 155
Shift
stationary, 137
Shifting Rule, 31
First, 13, 42, 100
Second, 16, 31, 44, 102
Sidelobes, 72
Signal-to-noise ratio, 58, 153
Simpsonâ€™s 1/3-rule, 150, 153
Sine integral, 72
Sine transformation, 116
basis functions of, 116
â€œSinogramâ€, 174
Smoothing-algorithm, 140, 147
Stability of recursive algorithms, 164
Step Response, 168
T
Thiranâ€™s all-pass ï¬lter for N = 1, 165
Tomography, 173
Transfer function, 137, 156
for band-pass ï¬lter, 142
for data-smoothing, 139
for high-pass ï¬lter, 141
for low-pass ï¬lter, 140
for notch ï¬lter, 143
Trapezoidal Rule, 150, 153
â€œTriangular functionâ€, 9, 13, 14, 23
with weighting, 19

242
Index
Triangular function â€œTriangular functionâ€, 7
Triangular window, 76
Triplet window, 81
Truncation error, 64, 119, 123
U
Undershoot, 30, 154
V
â€œVersieraâ€, 67
Voigt proï¬le, 69
W
Wave equation, 60, 61
Weighting, 20
of a function, 7
Weighting functions, 71
Window
rectangular, 91
Window functions, 71
Blackmanâ€“Harris window, 85
cosine window, 77
cos2-window, 78
Dolphâ€“Chebychev window, 81
Fejer window, 76
Gauss window, 82
Hamming window, 80
Hanning window, 78
Kaiserâ€“Bessel window, 83
overview, 86
rectangular window, 72
triangular window, 76
triplet window, 81
Windowing, 90
Wrap-around, 94
Z
Zero-padding, 119


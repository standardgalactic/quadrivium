
Information Security
Management Handbook
Sixth Edition
Volume 4

OTHER INFORMATION SECURITY BOOKS FROM AUERBACH
Building an Enterprise-Wide Business 
Continuity Program
Kelley Okolita 
ISBN 978-1-4200-8864-9
Critical Infrastructure: Homeland Security 
and Emergency Preparedness,  
Second Edition
Robert Radvanovsky and Allan McDougall 
ISBN 978-1-4200-9527-2 
Data Protection: Governance,  
Risk Management, and Compliance 
David G. Hill
ISBN 978-1-4398-0692-0 
Encyclopedia of Information Assurance
Edited by Rebecca Herold and Marcus K. Rogers
ISBN 978-1-4200-6620-3
The Executive MBA in Information Security
John J. Trinckes, Jr. 
ISBN 978-1-4398-1007-1
FISMA Compliance: Principles and Best 
Practices
Patrick D. Howard
ISBN 978-1-4200-7829-9 
HOWTO Secure and Audit Oracle 10g  
and 11g
Ron Ben-Natan
ISBN 978-1-4200-8412-2
Information Security Management: 
Concepts and Practice 
Bel G. Raggad 
ISBN 978-1-4200-7854-1
Information Security Policies and 
Procedures: A Practitioner‚Äôs Reference, 
Second Edition 
Thomas R. Peltier
ISBN 978-0-8493-1958-7
Information Security Risk Analysis,  
Third Edition 
Thomas R. Peltier
ISBN 978-1-4398-3956-0
Information Technology Control and Audit, 
Third Edition
Sandra Senft and Frederick Gallegos
ISBN 978-1-4200-6550-3
Intelligent Video Surveillance:  
Systems and Technology
Edited by Yunqian Ma and Gang Qian
ISBN 978-1-4398-1328-7 
Managing an Information Security and 
Privacy Awareness and Training Program, 
Second Edition
Rebecca Herold
ISBN 978-1-4398-1545-8 
Mobile Device Security: A Comprehensive 
Guide to Securing Your Information in  
a Moving World
Stephen Fried 
ISBN 978-1-4398-2016-2 
Secure and Resilient Software Development
Mark S. Merkow and Lakshmikanth Raghavan
ISBN 978-1-4398-2696-6 
Security for Service Oriented 
Architectures 
Bhavani Thuraisingham 
ISBN 978-1-4200-7331-7
Security of Mobile Communications
Noureddine Boudriga
ISBN 978-0-8493-7941-3 
Security of Self-Organizing Networks: 
MANET, WSN, WMN, VANET
Edited by Al-Sakib Khan Pathan
ISBN 978-1-4398-1919-7
Security Patch Management
Felicia M. Wetter
ISBN 978-1-4398-2499-3
Security Risk Assessment Handbook:  
A Complete Guide for Performing Security 
Risk Assessments, Second Edition
Douglas Landoll
ISBN 978-1-4398-2148-0
Security Strategy: From Requirements  
to Reality
Bill Stackpole and Eric Oksendahl
ISBN 978-1-4398-2733-8
Vulnerability Management
Park Foreman
ISBN 978-1-4398-0150-5
AUERBACH PUBLICATIONS
www.auerbach-publications.com
5P0SEFS$BMMr'BY
E-mail: orders@crcpress.com

Information Security
Management Handbook
Edited by
)BSPME'5JQUPO$*441r.JDLJ,SBVTF$*441
Sixth Edition
7PMVNF

Auerbach Publications
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
¬© 2010 by Taylor and Francis Group, LLC
Auerbach Publications is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-4398-1903-6 (Ebook-PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid-
ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the Auerbach Web site at
http://www.auerbach-publications.com 

v
Contents
Preface ..................................................................................................................................ix
Editors ..................................................................................................................................xi
1: 
DOMAIN 
ACCESS CONTROL
Access Control Administration
 1 Back to the Future ........................................................................................................3
PAUL A. HENRY
2: 
DOMAIN 
TELECOMMUNICATIONS AND NETWORK SECURITY
Communications and Network Security
 2 Adaptive Th reats and Defenses...................................................................................29
SEAN M. PRICE
 3 Achieving a Global Information Systems Transformation (GIST): 
Foundations for Infrastructure 2.0 via Standards-Based Interoperability: 
IF-MAP and Beyond ............................................................................................. 45
DAVID O‚ÄôBERRY
 4 A Primer on Demystifying U.S. Government Networks ............................................59
SAMUEL CHUN
Network Attacks and Countermeasures
 5 Antispam: Bayesian Filtering .....................................................................................75
GEORGES J. JAHCHAN
3: INFORMATION SECURITY AND RISK MANAGEMENT
DOMAIN 
Security Management Concepts and Principles
 6 Measuring Information Security and Privacy Training 
and Awareness Eff ectiveness .......................................................................................87
REBECCA HEROLD

vi ‚óæ Contents
 7 Managing Mobile Device Security ...........................................................................107
E. EUGENE SCHULTZ AND GAL SHPANTZER
 8 Establishing an Information Security Program for Local Government ...................127
ROBERT K. PITTMAN, Jr.
Policies, Standards, Procedures, and Guidelines
 9 A Business Case for ISO 27001 Certifi cation ...........................................................141
TOM CARLSON AND ROBERT FORBES
10 Achieving PCI DSS Compliance: A Compliance Review .........................................149
BONNIE GOINS PILEWSKI AND CHRISTOPHER A. PILEWSKI
Risk Management
11 
Leveraging IT Control Frameworks for Compliance ...............................................169
TODD FITZGERALD
12 Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù .......179
KEN M. SHAURETTE
13 Th e Outsourcing of IT: Seeing the Big Picture .........................................................193
FOSTER HENDERSON
14 Understanding Information Risk Management .......................................................209
TOM CARLSON AND NICK HALVORSON
15 Th e Sarbanes‚ÄìOxley Revolution: Hero or Hindrance ..............................................219
SETH KINNETT
4: 
DOMAIN 
APPLICATION SECURITY
System Development Controls
16 Data Loss Prevention Program ................................................................................229
POWELL HAMILTON
17 Data Reliability: Trusted Time Stamps ....................................................................245
JEFF STAPLETON
18 Security in the .NET Framework .............................................................................259
JAMES D. MURRAY
5: CRYPTOGRAPHY
DOMAIN 
Crypto Concepts, Methodologies, and Practices
19 Cryptography: A Unifying Principle in Compliance Programs ...............................281
RALPH SPENCER POORE

Contents ‚óæ vii
6: SECURITY ARCHITECTURE AND DESIGN
DOMAIN 
Principles of Computer and Network Organizations, Architectures, and 
Designs
20 Best Practices in Virtualization Security .................................................................291
SHANIT GUPTA
21 Everything New Is Old Again ..................................................................................325
ROBERT M. SLADE
7: OPERATIONS SECURITY
DOMAIN 
Operations Controls
22 A Brief Summary of Warfare and Commercial Entities ...........................................335
ROB SHEIN
23 Information Destruction Requirements and Techniques .........................................347
BEN ROTHKE
8:  BUSINESS CONTINUITY PLANNING AND 
DOMAIN 
DISASTER RECOVERY PLANNING
Business Continuity Planning
24 Integrated Business Continuity Planning ................................................................357
JAMES C. MURPHY
25 CERT/BERT: Community and Business Emergency Response ...............................397
CARL JACKSON
9: LAW, REGULATIONS, COMPLIANCE, AND INVESTIGATION
DOMAIN 
Major Categories of Computer Crime
26 Cyberstalking ...........................................................................................................413
MICKI KRAUSE NOZAKI
Incident Handling
27 Is Software Write Blocking a Viable Alternative to Hardware Write Blocking 
in Computer Forensics..............................................................................................425
PAUL A. HENRY
10: PHYSICAL SECURITY
DOMAIN 
Elements of Physical Security
28 Protection of Sensitive Data .....................................................................................449
SANDY BACIK

viii ‚óæ Contents
29 Water Leakage and Flooding ...................................................................................457
SANDY BACIK
30 Site Selection and Facility Design Considerations ...................................................463
SANDY BACIK
31 An Overview of IP-Based Video Surveillance ..........................................................471
LEO KAHNG
Index .................................................................................................................................485
Information Security Management Handbook, Sixth Edition: 
Comprehensive Table of Contents .....................................................................................495

ix
Preface
Th e days of wringing hands and warning of ‚Äúfear, uncertainty, and doubt‚Äù have given way to 
thoughtful and intelligent approaches to protecting information. Th is is due, in great part, to 
the adoption of comprehensive and far-reaching standards that foster the practice of integrating 
security into the business.
So, although some may still be ‚Äúatwitter‚Äù with loud and dramatic cries for enhanced, strength-
ened, and enforced security, many organizations are realizing the benefi ts of embedding appropriate 
controls into ongoing operations, thereby yielding eff ective and effi  cient safeguards.
Enter the Information Security Management Handbook, which for over a decade has off ered 
a virtual toolset of essays and dissertations addressing people, processes, and technologies. Th e 
information herein is practical, useful, and hands-on. Th e chapters are written by dedicated and 
committed authors who seek to share their ‚Äúbeen there, done that‚Äù stories with those who may 
benefi t from them. Within each of the chapters, you will fi nd personal histories and problem solv-
ing that each author has been gracious enough to share. We thank them.
Further, the handbook‚Äôs mission is to be used by a wide audience. Yes, the chapters are of sub-
stantial value to the security professional; however, they also address issues applicable to managers, 
executives, attorneys, risk managers, technology operators, and beyond. So, read hearty. If you 
learn one thing or fi nd one idea to apply, we have succeeded.
As always, we wish you the best.
Harold F. Tipton
Micki Krause Nozaki


xi
Editors
Harold F. Tipton, currently an independent consultant, was a past president of the International 
Information System Security Certifi cation Consortium and a director of computer security for 
Rockwell International Corporation, Seal Beach California for about 15 years. He initiated the 
Rockwell computer and data security program in 1977 and then continued to administer, develop, 
enhance, and expand the program to accommodate the control needs produced by technological 
advances until his retirement from Rockwell in 1994.
Tipton has been a member of the Information Systems Security Association (ISSA) since 1982. 
He was the president of the Los Angeles Chapter in 1984, and the president of the national orga-
nization of ISSA (1987‚Äì1989). He was added to the ISSA Hall of Fame and the ISSA Honor Role 
in 2000.
Tipton was a member of the National Institute for Standards and Technology (NIST), the 
Computer and Telecommunications Security Council, and the National Research Council Secure 
Systems Study Committee (for the National Academy of Science). He received his BS in engi-
neering from the U.S. Naval Academy and his MA in personnel administration from George 
Washington University, Washington, District of Columbia; he also received his certifi cate in com-
puter science from the University of California, Irvine, California. He is a certifi ed information 
system security professional (CISSP), ISSAP, and ISSMP.
He has published several papers on information security issues for
Auerbach Publishers‚ÄîHandbook of Information Security Management
Data Security Management
Information Security Journal
National Academy of Sciences‚ÄîComputers at Risk
Data Pro Reports
Elsevier
ISSA ‚ÄúAccess‚Äù Magazine
He has been a speaker at all the major information security conferences including the following: 
Computer Security Institute, the ISSA Annual Working Conference, the Computer Security 
Workshop, MIS Conferences, AIS Security for Space Operations, DOE Computer Security 
Conference, National Computer Security Conference, IIA Security Conference, EDPAA, UCCEL 
Security & Audit Users Conference, and Industrial Security Awareness Conference.
He has conducted/participated in information security seminars for (ISC)2¬Æ, Frost & 
Sullivan, UCI, CSULB, System Exchange Seminars, and the Institute for International Research. 

xii ‚óæ Editors
He participated in the Ernst & Young video ‚ÄúProtecting Information Assets.‚Äù He is currently serv-
ing as the editor of the Handbook of Information Security Management (Auerbach). He chairs the 
(ISC)2 CBK Committees and the QA Committee. He received the Computer Security Institute‚Äôs 
Lifetime Achievement Award in 1994 and the (ISC)2‚Äôs Hal Tipton Award in 2001.
Micki Krause Nozaki, MBA, CISSP, has held positions in the information security profession for 
the past 20 years. Krause was named one of the 25 most infl uential women in the fi eld of informa-
tion security by industry peers and Information Security magazine as part of their recognition of 
Women of Vision in the fi eld of information technology (IT) security. She received the Harold F. 
Tipton Award in recognition of sustained career excellence and outstanding contributions to the 
profession.
She has held several leadership roles in industry-infl uential groups, including the Information 
Systems Security Information (ISSA) and the International Information Systems Security 
Certifi cation Consortium (ISC)2, and is a passionate advocate for professional security leadership.
She is also a reputed speaker, published author, and coeditor of the Information Security 
Management Handbook series.

DOMAIN
 
1
ACCESS CONTROL
Access Control 
Administration


3
1
Chapter 
Back to the Future
Paul A. Henry
Network security appears (at least to the author), in some respects, to have come full circle. Many of 
today‚Äôs so-called innovations in network security can in fact, at least in part, be traced back to having orig-
inally been implemented in one form or another in the decades-old Orange Book standards. Th e author, 
having worked with a fi rewall vendor that in the early 1990s had developed the fi rst (and only) fi rewall 
to achieve an Orange Book ‚ÄúB Level‚Äù certifi cation, gained a perhaps unique‚Äîfi rsthand perspective‚Äîof 
the security benefi ts of the components of an Orange Book‚Äìcompliant security implementation.
Ironically, many of the features of Orange Book that were shunned in the commercial market-
place decades ago are now being embraced in one form or another in security implementations as 
the only sensible solutions to the environment we fi nd ourselves in today. Perhaps Orange Book 
requirements were simply decades ahead of their time.
Contents
Revisiting Orange Book .............................................................................................................. 4
Offi  cial Overview of Orange Book Classes .................................................................................. 4
Security Architecture Models in the Era of Orange Book ............................................................ 5
An Unoffi  cial View of Orange Book Classes ................................................................................ 7
Positive Security Model ........................................................................................................... 9
Positive Security Model: Gateway Considerations .............................................................. 9
Positive Security Model: Antivirus Considerations and Application Control 
Considerations ..................................................................................................................10
Negative Security Model ........................................................................................................11
Application Control ..........................................................................................................11
Mandatory Protection ............................................................................................................18
Mandatory Security in the Mainstream .............................................................................19
Use of Data Classifi cation Labeling ....................................................................................... 22
Covert Channels ................................................................................................................... 22
In Closing .........................................................................................................................25
About the Author ...................................................................................................................... 26

4 ‚óæ Information Security Management Handbook
In our eff orts to solve the most pressing issues that we face in network security today, perhaps a 
trip ‚ÄúBack to the Future‚Äù and a reexamination of the security provisions of the decades-old Orange 
Book are in order.
Revisiting Orange Book
In the late 1980s‚Äìearly 1990s, the methodologies that were core components of trusted 
computer systems often referred to as Orange Book‚Äìbased security were adopted by a small 
number of network security product vendors. While no one can argue that adopting Orange 
Book security did not provide for a higher level of attainable security, the commercial 
marketplace literally shunned them as overkill, administratively burdensome, and relegated 
it as old technology.
For those of us working for security product vendors at that time, it was widely felt that 
anything above an Orange Book B1 level was simply not achievable and sustainable in a commercial 
security product. See Figure 1.1.
Offi cial Overview of Orange Book Classes
Class (D): Minimal protection
Th is class is reserved for those systems that have been evaluated but that fail to meet the require-
ments for a higher evaluation class.
Class (C1): Discretionary security protection
Th e Trusted Computing Base (TCB) of a class (C1) system nominally satisfi es the discretionary 
security requirements by providing separation of users and data. It incorporates some form of 
credible controls capable of enforcing access limitations on an individual basis, that is, ostensibly 
suitable for allowing users to be able to protect project or private information and to keep other 
users from accidentally reading or destroying their data. Th e class (C1) environment is expected to 
be one of cooperating users processing data at the same level(s) of sensitivity.
Class (C2): Controlled access protection
Systems in this class enforce a more fi nely grained discretionary access control than (C1) systems, 
making users individually accountable for their actions through login procedures, auditing of 
security-relevant events, and resource isolation.
High security
 
A1 Verifi ed design
 
B3 
Security domains
 
B2 
Structured protection
 
B1 
Labeled security protection
 
C2 
Controlled access protection
 
C1 
Discretionary security protection
 
 
D 
Minimal protection
No security
Figure 1.1 Orange Book Security Classes. (From Department of Defense, Trusted Computer 
System Evaluation Criteria, DOD 5200.28-STD, December 1985, Appendix C, pp. 88‚Äì89.)
‚ñ≤

Back to the Future ‚óæ 5
Class (B1): Labeled security protection
Class (B1) systems require all the features required for class (C2). In addition, an informal state-
ment of the security policy model, data labeling, and mandatory access control over named subjects 
and objects must be present. Th e capability must exist for accurately labeling exported informa-
tion. Any fl aws identifi ed by testing must be removed.
Class (B2): Structured protection
In class (B2) systems, the TCB is based on a clearly defi ned and documented formal security 
policy model that requires the discretionary and mandatory access control enforcement found 
in class (B1) systems to be extended to all subjects and objects in the ADP system. In addi-
tion, covert channels are addressed. Th e TCB must be carefully structured into protection-critical 
and non-protection-critical elements. Th e TCB interface is well defi ned and the TCB design and 
implementation enable it to be subjected to more thorough testing and more complete review. 
Authentication mechanisms are strengthened, trusted facility management is provided in the form 
of support for system administrator and operator functions, and stringent confi guration manage-
ment controls are imposed. Th e system is relatively resistant to penetration.
Class (B3): Security domains
Th e class (B3) TCB must satisfy the reference monitor requirements that it mediate all accesses of 
subjects to objects, be tamperproof, and be small enough to be subjected to analysis and tests. To 
this end, the TCB is structured to exclude code not essential to security policy enforcement, with 
signifi cant system engineering during TCB design and implementation directed toward mini-
mizing its complexity. A security administrator is supported, audit mechanisms are expanded to 
signal security-relevant events, and system recovery procedures are required. Th e system is highly 
resistant to penetration.
Class (A1): Verifi ed design
Systems in class (A1) are functionally equivalent to those in class (B3) in that no additional archi-
tectural features or policy requirements are added. Th e distinguishing feature of systems in this 
class is the analysis derived from formal design specifi cation and verifi cation techniques and the 
resulting high degree of assurance that the TCB is correctly implemented. Th is assurance is devel-
opmental in nature, starting with a formal model of the security policy and a formal top-level 
specifi cation (FTLS) of the design. In keeping with extensive design and development analysis of 
the TCB required of systems in class (A1), more stringent confi guration management is required 
and procedures are established for securely distributing the system to sites. A system security 
administrator is supported.
Security Architecture Models in the Era of Orange Book
 
1. Bell‚ÄìLa Padula
 
 Th e Bell‚ÄìLa Padula confi dentiality model provides the ‚Äúmandatory‚Äù component of a mandatory 
access control system with the following mandatory access control parameters:
 
a. Top Secret level subjects
 
i. Top secret level subject can create as well as write only top secret level objects
 
ii. Can read top secret level objects as well as lower sensitivity level objects‚Äîsecret and 
confi dential
 
iii. Cannot write ‚Äúdown‚Äù to lower sensitivity level object‚Äîsecret and confi dential

6 ‚óæ Information Security Management Handbook
 
b. Secret level subjects
 
i. Secret level subject can create as well as write secret level objects and top secret level 
objects
 
ii. Cannot read ‚Äúup‚Äù in top secret level objects
 
iii. Can read secret level objects as well as lower sensitivity level objects ‚Äî confi dential
 
iv. Cannot write ‚Äúdown‚Äù to lower sensitivity level object‚Äîconfi dential
 
c. Confi dential level subjects
 
a. Confi dential level subject can create as well as write confi dential level objects as well 
as secret and top secret level objects
 
b. Can read only confi dential level objects
 
c. Cannot read ‚Äúup‚Äù in top secret or secret level objects
A common theme among applications of mandatory access control is the ‚ÄúNo read up‚ÄîNo write 
down‚Äù policy applied to each subject‚Äôs sensitivity level. Th is is the ‚Äúmandatory‚Äù part of mandatory access 
control.
It is the implementation of the Bell‚ÄìLa Padula security model:
 
i. Simple security property
 
 Th e subject cannot read information from an object with a higher sensitivity level than the 
subject‚Äôs
 
ii. Star property
 
 Th e subject cannot write information to an object with a sensitivity level that is lower than 
the subject‚Äôs
 
2. Biba
 
 Th e Biba formal model was written by K.J. Biba in 1977 and is the basis for the ‚Äúintegrity‚Äù: 
aspects of the mandatory access control model. Th e Biba formal model provides for three 
primary rules:
 
a. An access control subject cannot access an access control object that has a lower integ-
rity level
 
b. An access control subject cannot modify an access control object that has a higher 
integrity level
 
c. An access control subject cannot request services from an access control object that has 
a higher integrity level
 
3. Clark‚ÄìWilson
 
 Th e Clark‚ÄìWilson formal model was written by Dr. David D. Clark and David R. Wilson in 
1987, was updated in 1989, and like the Biba formal model, it addresses integrity. However, 
unlike the Biba formal model, the Clark‚ÄìWilson formal model extends beyond limiting 
access to the access control object by adding integrity considerations to the processes that 
occur while using the access control object.
Th e Clark‚ÄìWilson formal model eff ectively provides for the integrity of the access control object 
by controlling the process that can create or modify the access control object.
Further, the Clark‚ÄìWilson formal model also provides for the separation of duties. Th is 
aspect of the Clark‚ÄìWilson formal model establishes guidelines that require that no single per-
son should perform a task from beginning to end and that the task should be accomplished by 
two or more people to mitigate the potential for fraud in one person performing the task alone.

Back to the Future ‚óæ 7
 
 Other considerations of Clark‚ÄìWilson:
 
a. Well-formed transaction
 
 Th e well-formed transaction is the basis of the Clark‚ÄìWilson model and provides for integrity 
through the use of rules and certifi cations applied to data as it is processed through various 
states. A well-formed transaction also employs the use of separation of duties whereby the 
implementer of a transaction and the certifi er of a transaction must be separate entities.
 
b. Access Triple
 
 Historically, the Clark‚ÄìWilson Triple referred to the relationship between an authenti-
cated user, the programs that operate on the data items, and the data itself. Similarly, an 
Access Triple refers to an authenticated user having permission to use a given program 
upon a specifi c set of data.
 
4. Brewer‚ÄìNash‚ÄîChinese Wall
 
 Th e Chinese Wall adds an additional element‚Äîthe interrelationships of data to other mod-
els. In an example of the addition of a Chinese Wall to the Bell‚ÄìLa Padula, not only would a 
given user be restricted to only accessing a specifi c set of data, but a further consideration of 
what other data sets the user had previously accessed would be examined before permitting 
access to the data. In an example of Clark‚ÄìWilson augmented with a Chinese Wall, not only 
is access to data restricted to a given process, but consideration is also given to which other 
data the processes had been used upon.
An Unoffi cial View of Orange Book Classes
C1, C2‚ÄîSimple enhancement of existing systems that did not break applications
B1‚ÄîRelatively simple enhancement of existing systems that will break some applications
B2‚ÄîRelatively major enhancement of existing systems that will break many applications
B3‚ÄîSystems that failed A1 certifi cation
A1‚ÄîComplete top-down design and implementation of a new system from scratch
While originally written for military system usage, the security classifi cations that were at the 
very core of Orange Book are today, decades later, being adopted and being used within current 
generation network security products. A few, perhaps ‚Äúoverly simplifi ed,‚Äù examples we will discuss 
in this chapter are
Positive security model
 
‚óæ
Mandatory protection
 
‚óæ
Use of data classifi cation labeling
 
‚óæ
Covert channels
 
‚óæ
Driven by the changing threat environment
If we look back at 1988, only a single advisory was published by CERT for the entire year. In 2000, 
for the fi rst time in history, BugTraq reported that the number of new vulnerabilities reported 
monthly had exceeded 100 (Figure 1.2). By 2006, the number of annual vulnerabilities cataloged 
unoffi  cially by CERT had grown to 8064 (Figure 1.3). By 2007, obfuscation of malware had 
become a common practice, and by 2009, due to the use of obfuscation, the number of unique 
samples of malware found in the wild exceeded 5,500,000 samples annually (Figure 1.4).

8 ‚óæ Information Security Management Handbook
Under 10,000 unique samples
200,000
0
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
400,000
600,000
800,000
1,000,000
5,000,000
6,000,000
Number of unique samples
Almost 5.5 million unique samples
Figure 1.4 By 2007, the number of unique samples of malware found in the wild exceeded 
5,500,000 samples annually.
1998‚Äì01
1998‚Äì02
0
50
100
vulns
1998‚Äì03
1998‚Äì04
1998‚Äì05
1998‚Äì06
1998‚Äì07
1998‚Äì08
1998‚Äì09
1998‚Äì10
1998‚Äì11
1998‚Äì12
1999‚Äì01
1999‚Äì02
1999‚Äì03
1999‚Äì04
1999‚Äì05
1999‚Äì06
1999‚Äì07
1999‚Äì08
1999‚Äì09
1999‚Äì10
1999‚Äì11
1999‚Äì12
2000‚Äì01
2000‚Äì02
2000‚Äì03
2000‚Äì04
2000‚Äì05
2000‚Äì06
2000‚Äì07
2000‚Äì08
2000‚Äì09
2000‚Äì10
2000‚Äì11
Figure 1.2 In 2000, BugTraq reported that the number of new vulnerabilities reported monthly 
had exceeded 100.
1000.00
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
-
2000.00
3000.00
4000.00
5000.00
6000.00
7000.00
8000.00
9000.00
Vulnerabilities
Figure 1.3 By 2006, the number of annual vulnerabilities recorded by CERT had grown to 8064.

Back to the Future ‚óæ 9
Positive Security Model
In a positive security model‚Äìbased defense, the security administrator must fi rst confi gure the 
security product (such as a fi rewall) to match the business needs of the organization.
Denied by default‚Äîsimply put if not confi gured as ‚Äúknown good,‚Äù is determined to be nec-
essary to meet the business needs of the organization and is explicitly allowed to pass by formal 
policy, a given packet is simply blocked by default. Th e security aff orded by a positive security 
model‚Äìbased fi rewall includes multiple layers of defense:
Dramatically reduces the organization‚Äôs threat envelope by allowing only those packets that 
 
‚óæ
meet the business needs of the organization to pass
Provides for complete protocol validation thereby eliminating entire classes of attacks, such 
 
‚óæ
as buff er overfl ows (common in day zero attacks)
Provides for application anomaly detection
 
‚óæ
Positive Security Model: Gateway Considerations
While clearly a more secure alternative to the negative security model, the positive security model 
failed to gain popularity not due to a fl aw in the model but in the authors opining because the 
vendors supporting it failed to keep up with the rapidly growing number of applications that it 
needed to support in order to work in the business environment. Simply put, if the vendor did not 
fully support the application, the Positive Model‚Äìbased security product would in fact break the 
application by not allowing it‚Äôs associated traffi  c to pass, and eff ectively the organization utilizing 
the product was not able to conduct their normal business.
If we look back to 1988, only a handful of applications were necessary to be supported in order 
to conduct business:
SMTP
FTP
HTTP
Finger
Telnet
Gopher
Today, the numbers of application that are used in the enterprise environment have nearly reached 
1000. Failure to provide support for any one of these applications could very well prevent an enter-
prise from conducting its normal business.
In the early days of Firewalls, vendors that provided application proxy technologies were the 
fi rst to use the positive security model in a pitch for their products. But as the number of applica-
tions began to quickly outnumber those they could provide full application proxy support for, 
they themselves began to include negative security model‚Äìbased technologies within their prod-
ucts in order to simply make them useable. Quickly, the line blurred between positive and negative 
security models for these products as you ended up with a limited positive security model for those 
applications proxies that could work eff ectively in a given environment, and a negative security 
model in the form of nothing more than a packet fi lter with signatures (IDS) layered on top of it 
to provide for the identifi cation of known bad packets to support those applications that could not 
be supported in a positive security model.

10 ‚óæ Information Security Management Handbook
Obfuscation renders negative security model‚Äìbased fi rewalls obsolete: In a negative security 
model, all traffi  c is allowed to fl ow freely and only that traffi  c that is identifi ed as ‚Äúbad‚Äù is blocked. 
Back in 1999 when the rate of new vulnerabilities were running at only 25 new vulnerabilities 
reported each month, negative security model‚Äìbased fi rewalls could off er a reasonable level of risk 
mitigation as vendors could ‚Äúkeep up‚Äù in creating new defensive signatures. In today‚Äôs threat envi-
ronment with an ever-increasing number of reported application vulnerabilities combined with 
the use of obfuscation, we are seeing 5,500,000 unique samples of malware annually. Keeping 
up with the number of necessary signatures has become a trying, if not impossible, task for those 
responsible for creating the necessary defensive signatures.
Th is signature problem was clearly exacerbated with hacking tools like ‚ÄúVOMM‚Äù also referred 
to as ‚ÄúEvade-O-Mastic‚Äù that obfuscates Web-based exploits rendering their attack completely 
undetectable by negative security model fi rewalls and their associated signatures. Negative secu-
rity model fi rewalls are also plagued with being unable to detect self-mutating exploits like the 
Straton worm, which automatically alters its program code at a faster rate, than vendors can create 
new signatures. In October 2006, the Straton worm was the most prevalent worm reportedly seen 
by security vendors on the Internet. In the month of October alone, over 300 diff erent variants 
of the Straton worm were detected. Today‚Äôs exploit obfuscation tools and self-modifying exploits 
have eff ectively rendered negative security model fi rewalls as well as other signature-based defenses 
such as Intrusion Prevention Systems (IPS) as obsolete.
From a fi rewall and positive security model perspective, Orange Book was clearly decades 
ahead of its time. Only now, decades later, in 2009 does a vendor seem to be ‚Äúahead of the curve‚Äù 
and able to support the nearly 1000 applications that may be found within the enterprise and 
provide for a positive security model without breaking the applications necessary for the enterprise 
to conduct their daily business.
Positive Security Model: Antivirus Considerations 
and Application Control Considerations
Malware regularly slips through current defenses wreaking havoc within enterprise networks. 
Antivirus products are struggling today with their inability to keep up with both the shear number of 
new virus and worms (malware) that are spawned out of the dramatic increase in newly reported vul-
nerabilities as well as the stealth employed by current technology self-mutating malware. Antivirus 
products have grown to be too dependent upon signatures for detection of malware and have not 
placed enough emphasis on newer technologies such as advanced heuristics that can detect malware 
without using an associated signature. Another issue has arisen that is increasing the diffi  culty of 
antivirus products from aff ording a reasonable level of risk mitigation‚Äîtargeted attacks. In a 
targeted attack, the malware is not broadly distributed across the Internet, the delivery is reduced to 
a fi nite number of targets. Antivirus vendors have grown accustomed to the luxury of the broad dis-
tribution of malware, aff ording them the opportunity to capture and reverse engineer the malware 
for signature creation early in the malware life cycle. In a targeted attack, it is highly unlikely that 
signature-dependent antivirus vendors will be able to capture a sample of the malware in order to 
create a defensive signature, hence they will be unable to off er any defensive capability. Th e failure 
of antivirus products to operate eff ectively without the use of signatures has perpetuated the rise in 
day zero attacks. Th is antivirus signature issue combined with the increased prevalence of targeted 
attacks, as compared to traditional broadly distributed malware, is quickly rendering many antivirus 
solutions obsolete.

Back to the Future ‚óæ 11
Many are now beginning to recognize that the era of other negative security model‚Äìbased 
products, such as traditional signature-based antivirus, is quickly nearing its end. A Positive 
Model‚Äìbased alternative known as White-Listing or Application Control is quickly gaining 
popularity as a replacement for traditional antivirus solutions.
Table 1.1 displays the results of a recent test of AV software by Virus Bulletin. At fi rst glance, 
one may conclude that a rating of 99.8% is quite eff ective. However, consider that the rating when 
applied to the 1,164,662 samples used in the test still allowed 2,329 pieces of malware through to 
infect a network. Consider the worst-case performance reported at 65.5% that left 401,808 pieces 
of malware through in the testing. Now, to fully appreciate the scope of the issue, apply the ratings 
from the testing to real-world numbers, such as the current reported run rate of actual unique mal-
ware samples at 5,500,000 annually. With 99.8% eff ectiveness in your AV solution, you are still 
potentially allowing 11,000 pieces of malware to slip through‚Äîmore than enough to devastate 
and/or wreck havoc in your network.
Negative Security Model
Also known as default allow‚ÄîIn contrast to a positive security model is of course the negative secu-
rity model. Th e most popular network security products to date historically have been those that 
worked in a negative security model. Simply put, they rely on their ability to identify undesirable/
known bad traffi  c and prevent it from entering. It is very much like having a list at a country‚Äôs port 
of entry, which identifi es known criminals. When people travel into the country, their passports are 
checked against this list, and if they are not on it, they are allowed in. Th is design is eff ective to the 
degree that it catches known criminals, but what about those who have not yet committed any acts of 
terror, or have not yet been caught for their crimes, or who should also be considered a risk because 
of their associations or reputation? In fact, it is not always possible to determine whether somebody, 
or in the case of the network, a particular packet of traffi  c, is undesirable based on known parameters. 
Th e most eff ective security policy revolves around one statement: ‚ÄúTrust no one.‚Äù Th at is why the best 
fi rewalls operate on a ‚Äúpositive‚Äù security model, which denies all access unless it is explicitly allowed.
Application Control
A current generation implementation of the Positive Security Model
Application Control is quickly emerging to complement and even, perhaps, to replace traditional 
antivirus solutions. Rather than relying on the constant creation of new signatures to protect 
assets from emerging threats in a Black List or negative security model, Application Control uses 
a positive security model or white list approach. In the simplest of terms, controls are established 
to permit or deny all applications and supporting scripts and macros on all workstations across the 
enterprise. Th is approach reminds the author of the Orange Book Default Deny methodology (in 
limited respects)‚Äîif the application, script, or macro is not explicitly approved via the Application 
Control policy and confi rmed via hash, it by default is not permitted to execute.
From a risk mitigation perspective, the time has clearly come for the shift from the negative 
security model to the positive security model. In the simplest of terms, the number of new and 
potentially bad things that must be blocked in a negative security model implementation now easily 
outweigh those that need to be permitted to facilitate the business needs of an organization. From 
an administrative burden perspective, the tide has turned and today it is simply more eff ective to 
manage the ‚Äúknown good‚Äù than to keep up with the explosive growth of the ‚Äúknown bad.‚Äù

12 ‚óæ Information Security Management Handbook
Table 1.1 Results of Test of AV Software by Virus 
Bulletin
Product
Malware on Demand (%)
AntiVir (Avira)
99.80
Avast! (Alwil)
99.30
AVG
95.80
AVK 2008 (G Data) (1)
99.20
AVK 2009 (G Data) (2)
99.80
BitDefender 2008
97.70
BitDefender 2009
97.60
CA-AV (VET)
65.50
ClamAV
88.50
Dr Web
84.90
eScan
97.80
Fortinet-GW
92.60
F-Prot (Frisk)
94.80
F-Secure 2008
98.20
F-Secure 2009
99.20
Ikarus
99.50
K7 Computing
92.10
Kaspersky
98.40
McAfee
93.60
Microsoft
97.70
Nod32 (Eset)
94.40
Norman
96.30
Norton 2008 (Symantec)
97.80
Norton 2009 (Symantec)
98.70
Panda 2008
86.40
Panda 2009
91.80
Rising
83.40
Sophos
97.50
Trend Micro
91.30

Back to the Future ‚óæ 13
Th e fi ve phases of the implementation of an application control solution are
 
1. Discovering and monitoring the application ecosystem
 
2. Assigning rights
 
3. Pilot rollout
 
4. Enforcing protection
 
5. Fine-tuning the application ecosystem
Discovering and Monitoring the Application Ecosystem
Th ere are multiple approaches to establishing a baseline. One could use a third-party database 
of known good application hashes or simply create a custom database by scanning clean and 
known good machines (Figure 1.5) that had not been connected to the network or public 
Internet that contained the applications necessary to complete the business objectives of the 
organization.
Assigning Rights
In an Application Control solution, assigning rights can be as simple as assigning all validated 
applications and supporting scripts to a ‚ÄúEveryone Group‚Äù or can be accomplished with a high 
level of granularity for individual users and/or groups by leveraging existing LDAP, Active 
Directory, or eDirectory resources (Figure 1.6). Application Control solutions often provide for 
integration with IT change control solutions to reduce the administrative burden of ongoing 
system maintenance.
Pilot Rollout
A pilot group of users is normally selected to test the completeness and accuracy of the white list 
and system confi guration. A good selection for a pilot group would be a set of users that does not 
include development and/or IT maintenance‚Äìrelated workstations as they typically run the most 
nonstandard applications. In the initial phase of a pilot program, the solution is operated in a 
monitor-and-report-only mode and does not implement enforcement.
Table 1.1 (continued) Results of Test of AV 
Software by Virus Bulletin
Product
Malware on Demand (%)
TrustPort
99.50
VBA32
90.50
VirusBuster
89.00
WebWasher-GW (3)
99.70
ZoneAlarm
97.80
Source: Data from Virus Bulletin, September 2008. 
http://www.virusbtn.com/news/2008/09_02

14 ‚óæ Information Security Management Handbook
Figure 1.5 A unique SHA-1 signature is calculated for each binary fi le, together with the fi le-
name, path, size, and product version. This information is recorded on the Lumension server 
whitelist, defi ning what programs can run on all selected computers.
Figure 1.6 The User Explorer module lets you use the Microsoft Active Directory to map users 
and groups to the whitelist.

Back to the Future ‚óæ 15
Enforcement
In the pilot program, careful monitoring of exceptions will allow reconfi guration of policies to 
provide for operation with minimal exception alerts. As the exception list shrinks to a manageable 
level, the pilot can switch to an enforcement mode, whereby those applications that are not explic-
itly permitted for users/groups are denied, and exception reports are generated and made available 
to the management console, where adjustments can be made to the operating policy (Figure 1.7).
Once you are comfortable that you have developed a manageable confi guration, the pilot 
can begin to be rolled out across the enterprise. Departments can simply be added initially in a 
reporting-only mode. Once confi rmed to be confi gured properly, whereby a minimum level of 
exceptions are reported, the department can be switched to an enforcement mode of operation.
Fine-Tuning the Application Ecosystem
User awareness is a big part of a successful implementation of application control. Employees must be 
alerted as to which applications are approved for operation on departmental workstations and which 
are not. If you have not prepared your employees for the change, your help desk could be overloaded. 
In addition, alerts can be confi gured to be displayed on the users‚Äô workstation to alert the users when 
they have attempted to run an application or script that is not permitted by policy. A carefully crafted 
message can go a long way in reducing help desk calls for assistance (Figure 1.8).
To facilitate an eff ective and manageable application control solution, the following capabilities 
need to be addressed by the application control vendor:
Automated application discovery that provides fl exible options to update white lists
 
‚óæ
Spread check mechanisms that can automatically disable applications when it is discovered 
 
‚óæ
that too many users may have used local authorization to enable an application that poten-
tially places the organization at risk
Figure 1.7 Use warning messages to explain why applications won‚Äôt run after blocking is turned 
on‚Äîperhaps with more information than ‚ÄúNo, Bob!‚Äù.

16 ‚óæ Information Security Management Handbook
Active directory integration to facilitate automatic maintenance of users and groups used 
 
‚óæ
within the application control policies
Automatic authorization of vendor software updates to eliminate the risk of automatically 
 
‚óæ
restricting user community‚Äôs access to frequently updated applications
Script and macro protection to extend policy enforcement beyond applications to their sup-
 
‚óæ
porting scripts and associated macros
Flexible fi le authorization to identify new fi les to be included in the authorization database
 
‚óæ
Local authorization for trusted power users to off er fl exibility without giving up administra-
 
‚óæ
tive control; any locally authorized application is reported to the administrator for review
Off -line protection to ensure that remote or disconnected users are constantly protected by 
 
‚óæ
keeping a local copy of respective application hashes and remissions on each machine
Standard fi le defi nitions that include classifi cations of all preloaded applications across all 
 
‚óæ
supported operating systems
Th e only question remaining for moving to a positive security model in the use of application control is 
perhaps where it is best to implement it. Some would suggest application control is best handled at the 
gateway to reduce administrative burden and focus protection on a single set of protective devices. In 
the authors opinion, it is this kind of thinking that has brought us to the perilous point we are at today, 
whereby hackers that are able to pierce the perimeter defenses have an open reign within our networks. 
Th at thought along with the current increasing insider threat leads the author to conclude that applica-
tion control is best accomplished in a layered approach both at the gateway and on the desktop.
An example of the benefi t of positive security model‚Äìbased application control:
Th e scenario:
Small network < 100 Windows XP Machines
Current antivirus at the gateway and on the desktop with the latest signatures
Firewall with a rule to permit internal users with Internet access over ports 80 and 443
URL fi lter to block access to known malicious Web sites
 
1. Th e internal user while surfi ng the Internet is redirected to an offi  cial-looking Web site that 
initiates a fake security scan of the users PC.
 
2. Th e URL was not blocked by the URL fi lter as the Web site had not yet been classifi ed as a 
malicious Web site.
 
3. Th e page displayed on the user‚Äôs PC (Figure 1.9) did not include the typical browser tool bar 
and the user assumed it was an offi  cial company application scanning his PC.
Figure 1.8 You can write custom warning messages for users when they attempt to launch an 
application that is not on the whitelist.

Back to the Future ‚óæ 17
 
4. When the scan was completed the user in the interest of quickly getting back to work 
selected to remove the malware that reportedly was found on the PC.
 
5. Upon selecting ‚ÄúRemove,‚Äù the user‚Äôs PC unknowingly downloaded malicious executable 
fi les to the user‚Äôs PC.
 
6. Th e gateway antivirus server and the user‚Äôs desktop antivirus software did not block the 
download as the downloaded fi les were using obfuscation and the antivirus signatures 
had not yet been updated to contain signatures for this new and seemingly one-of-a-kind 
obscured application.
 
7. Application control automatically blocked the operation of the executables by default (Figure 
1.10) because they were on part of the permitted applications that the user had administra-
tive permission to execute. Further, application control blocked the malware‚Äôs attempted 
execution of a restricted command.
Ironically, the user‚Äôs application aware fi rewall vendor, URL fi lter vendor, and antivirus ven-
dor claims their products are proactive in that they will automatically protect the user from 
malware. Unfortunately the fi rewall did not block the malware as it did not yet have a signature 
for the unique malware in it‚Äôs internal application fi lter database, the URL fi lter did not block 
the user‚Äôs access to the URL that contained the malicious page as it did not yet have the proper 
classifi cation for the URL that the user was redirected to, and the user‚Äôs antivirus solution did 
not block access to the malware as it lacked a signature for the new and unique malware that 
was delivered. Th e user‚Äôs so-called ‚ÄúProactive Defenses‚Äù were unable to be anything that resem-
bled ‚ÄúProactive.‚Äù Th e only real ‚ÄúProactive Defense‚Äù the user experienced was their last line of 
defense‚Äîtheir positive security model‚Äìbased application control.
Figure 1.9 Rogue security software is a type of misleading application that pretends to be 
legitimate security software; however, it provides little or no protection and may install the very 
malicious code it purports to protect against.

18 ‚óæ Information Security Management Handbook
Continue with
Mandatory protection
 
‚óæ
Use of data classifi cation labeling
 
‚óæ
Covert channel analysis
 
‚óæ
Mandatory Protection
Mandatory protection in the form of the enforcement of the rule of least privilege was a principle 
component of the Orange Book. It clearly defi nes least privilege as a principle that ‚Äúrequires that 
each subject in a system be granted the most restrictive set of privileges needed for the performance 
of authorized tasks. Th e application of this principle limits the damage that can result from acci-
dent, error, or unauthorized use.‚Äù
Perhaps one of the most recent highly visible, albeit arguably failed, implementations of man-
datory protection would have to be Microsoft Windows Vista user account control (UAC). Many 
would argue that one of the reasons Vista has not enjoyed the popularity obtained by its prede-
cessor Windows XP is specifi cally due to the implementation of UAC. In fact, reportedly, a vast 
majority of Vista users have disabled UAC. An example of the annoying interruptions provided by 
UAC is the pop-up received when a user would attempt to open the common command prompt 
with administrative privilege (Figure 1.11) or attempt to run any program with administrative 
privilege (Figure 1.12).
Th e problem with UAC stems from Microsoft‚Äôs legacy of giving every user administrative 
rights by default. Simply put, UAC is perhaps Microsoft‚Äôs fi rst attempt at breaking away from the 
tradition of every user being an administrator by default. When a common user has standard user 
Figure 1.10 Positive security model-based application control automatically blocked the oper-
ation of the rogue software executables by default because they were not part of the permitted 
applications that the user had administrative permission to execute.

Back to the Future ‚óæ 19
rights in Windows Vista, UAC will pop up each and every time the user attempts to do anything 
where their administrative rights are necessary. Perhaps it is too much nagging interaction with 
the user that fueled the distain for UAC. In a Mac or Unix environment, the user by default does 
not have administrative rights, hence the user is only prompted when they attempt to perform a 
task that actually requires administrative rights and they are silently prevented from doing other 
things that may require administrative rights.
Mandatory Security in the Mainstream
Th e U.S. government‚Äôs Information Security Automation Program (ISAP) is an initiative to pro-
vide for the standardization of technical security operations. Using tools that support the Security 
Content Automation Protocol (SCAP) enables literally pushing standard policies out to every 
Figure 1.12 An example of the annoying interruptions provided by UAC is the pop-up received 
when a user would attempt to run any program with administrative privilege.
Figure 1.11 An example of the annoying interruptions provided by UAC is the pop-up received 
when a user would attempt to open the common command prompt with administrative 
privilege.

20 ‚óæ Information Security Management Handbook
desktop within an organization and then monitoring those machines for compliance with the 
policy. While you simply will not fi nd the words ‚Äúmandatory security‚Äù within the ISAP specifi -
cations, clearly, it is a tool that can in fact be used to provide for an enterprise-wide baseline of 
mandatory security.
It is the policies that are used that in fact aff ord the baseline of the deployed security confi gura-
tion‚Äîmandatory security. Further, ongoing monitoring of compliance can provide for automated 
compliance reporting that has become a requirement of standards such as the Federal Desktop 
Core Confi guration (FDCC) and the Payment Card Industry (PCI-DSS) and other custom policy 
implementations.
A cursory review of a current FDCC policy confi guration would include approximately 300 
security-related requirements in a Windows XP or Vista computer. Th e immediate impact felt by 
users in an environment where FDCC policy is enforced is as follows:
Password changes will be more frequent. Instead of every 90 days, your password will have 
 
‚óæ
to be changed every 60 days.
Your login will not be saved when you log on. You will need to fi ll in the login and the pass-
 
‚óæ
word each time you log on to your computer.
Administrative privileges will be taken away, which means you will not be able to download 
 
‚óæ
new applications. Unless you obtain a waiver to have these privileges, you will need to open a 
ticket with the Help Desk and have them work with your local IT support to make changes 
to your computer or install software.
Some applications may not work properly because they require administrative access to the 
 
‚óæ
operating system, application directories, and registry keys. For example, there is a known 
problem with Visual Studio Suite accessing fi les that only an administrator can access. It has 
also been reported that in some cases, Remedy is unable to access the user preferences, which 
are stored in the user‚Äôs profi le, which requires administrator access.
Checklists are freely available for various FDCC confi gurations and are downloadable for review 
at http://checklists.nist.gov/chklst_detail.cfm?confi g_id=129.
An example of the processes involved in automating mandatory security across an entire enter-
prise using a SCAP-compliant product (Figure 1.13):
3
4
1
2
Figure 1.13 An example of the processes involved in automating mandatory security across an 
entire enterprise using a SCAP-compliant product.

Back to the Future ‚óæ 21
 
1. Manage Security Confi guration Policy: Defi ne, edit, and import/export security confi gura-
tion policies and best practices by leveraging the SCAP. Automatically map these regulatory 
or internal security policies to your own agent policy set, enabling you to standardize and 
secure your endpoint confi gurations and easily demonstrate compliance. Th anks to open 
standards, security specifi cations can also be added or edited to create custom security con-
fi guration policies.
 
2. Assess Policy Compliance by Group and Device: Apply desired security specifi cations to 
your network device groups and application confi gurations. Automatically (or manually, 
where applicable) assess policy compliance with security confi guration specifi cations for 
device groups as well as individual devices.
 
3. Report Policy Compliance Results: Demonstrate policy compliance by reporting confi gura-
tion status against regulations and industry standards such as FDCC and PCI-DSS, as well 
as customized policies.
 
4. Enforce Policy Compliance: Achieve and maintain compliance with security confi guration 
policies and best practices, leveraging automated remediation and policy enforcement.
With policies deployed across the enterprise monitoring of compliance can be an administrative 
burden, however, current generation SCAP implementations provide for a centralized graphical 
user interface (GUI) that can dramatically reduce administrative burden (Figure 1.14). Further, 
the centralized GUI provides for the necessary reporting capabilities that are a common compo-
nent in today‚Äôs regulatory environment.
Some of the very same mandatory security requirements that were perhaps principle compo-
nents of the original Orange Book requirements two decades ago have clearly fi nally found their 
way in to broad use today across both government and private commercial networks. One can only 
wonder what impact would have occurred had mandatory security become a reality back when 
the Orange Book requirements were fi rst made available. Clearly, the unquestionable benefi ts of 
Figure 1.14 Current generation SCAP implementations provide for a centralized graphical user 
interface (GUI) that can dramatically reduce administrative burden.

22 ‚óæ Information Security Management Handbook
mandatory security and its components such as the enforcement of the rule of least privilege would 
have seriously altered the threat landscape we face today.
A recent study by BeyondTrust found that 92% of critical Microsoft vulnerabilities could 
have been stopped or mitigated by simply eliminating the practice of giving users ‚Äúadministra-
tor‚Äù rights. Th e study also found that eliminating administrator rights would have stopped or 
mitigated
94% of Microsoft Offi  ce vulnerabilities reported in 2008
89% of Internet Explorer vulnerabilities reported in 2008
53% of Microsoft Windows vulnerabilities reported in 2008.
Use of Data Classifi cation Labeling
Th e usage of a data classifi cation and labeling scheme was a core component of Orange Book 
security. Used in part to enforce mandatory access control (MAC) in environments requiring high 
levels of security, such as government or military systems. With MAC, the inherent problems of 
trying to rely upon each system owner to properly control access to each access control object is 
eliminated by having the system participate in applying a mandatory access policy (the system 
owner applies the ‚Äúneed to know‚Äù element). Th is policy aff ords typically three object classifi cation 
levels: top-secret, secret, and confi dential. Each access control system subject (users and programs) 
is assigned clearance labels, and access control system objects are assigned sensitivity labels. Th e 
system then automatically provides the correct access rights based upon comparing the object and 
subject labels. Mandatory access controls allow multiple security levels of both objects and subjects 
to be combined in one system securely.
Today, data classifi cation and labeling schemes are proving to be benefi cial in data leakage 
prevention (DLP) systems as well as the automation of rediscovery and deduplication eff orts.
A data classifi cation and labeling system can go a long way in making organization‚Äôs eff orts to 
identify and secure their data more eff ective. Th e use of unique labels for fi les containing sensitive 
information can make it easier to fi nd them, whether they are at rest or in transit.
Th ese unique labels can be literally used like digital watermarks, and you can write IDS rules 
to identify them when the data is in transit. You can also use regular expressions for the tools that 
are used to search data at rest, using them to identify these labels and to help audit computers that 
should not be storing a particular classifi cation of data.
Labeling data in eDiscovery and deduplication has been proven to be benefi cial in reducing the 
complexity of fulfi lling rediscovery eff orts within large organizations. Th e shear volume of data 
contained within the enterprise has seen explosive growth (Figure 1.15). It has become common 
today for data labeling for data classifi cation to become part of the normal business data life cycle, 
(Figure 1.16) and in at least some abstract respect has its heritage traceable back to Orange Book 
security initiatives.
Covert Channels
Covert channels have long been the enabler of communications for the command and control of 
botnets and most recently have been adopted for the theft of data in data leakage incidents.
A ‚Äúcovert channel‚Äù can be described as ‚ÄúAny communications channel that can be exploited by 
a process to transfer information in a manner that violates the system‚Äôs security policy.‚Äù Essentially, 
it is a method of communication that is not part of an actual computer system‚Äôs design but can 

Back to the Future ‚óæ 23
be used to transfer information to users or system processes that normally would not be allowed 
access to the information.
Covert channel exploits typically require a malicious client or server program operating on a 
PC outside the protected network and a malicious server or client program operating on a server 
inside the protected network.
Th e malicious PC outside the protected network would encapsulate the desired protocol within 
a given protocol that is allowed by the security policy of the protected network‚Äôs fi rewall. Th e 
malicious PC on the outside of the protected network would then transmit this allowed protocol 
Use 
records
Business
information
life cycle
Consider
retention
needs
Discard
unneeded
records
Label and
organize
records
Store and
backup
records
Create
records
Figure 1.16 The normal business data lifecycle.
Digital information
created, captured, replicated worldwide
Sensors
Digital TV
RFID, toys
MP3 players, VolP
GPS, digital cameras
Camera phones, e-mail
medical imaging, laptops
data center applications, games
satellite images, ATMs, scanners
DVDs, digital radio, DLP theaters, telematics
Peer-to-peer, instant messaging, video conferencing
CAD/CAM,  industrial machinery, security systems, appliances
2006
2007
2008
2009
2010
2011
1800
exabytes
180
exabytes
10√ógrowth in 5 years!
Figure 1.15 The shear volume of data contained within the enterprise has seen explosive 
growth. (Courtesy of IDC White Paper, The Diverse and Exploding Digital Universe, spon-
sored by EMC, Framingham, MA, March 2008.)

24 ‚óæ Information Security Management Handbook
through the fi rewall directed to the IP address of the server running the malicious receiving pro-
gram inside the protected network.
Th e receiving program would strip off  the transport protocol, thereby leaving the original 
malicious data in its original protocol form. Th ese packets would either then be used by the server 
running the receiving program or be automatically sent to a predetermined IP address of another 
server or PC within the network.
Covert channels are not a new methodology; in fact the theoretical dangers of covert chan-
nels were fi rst addressed in the National Computer Security Center‚Äôs (NCSC) Trusted Computer 
System Evaluation Criteria (TCSEC) as early as in 1983 and 1985.
Later in 1990, as covert channels moved from the realm of theoretical to possible, in France, 
Germany, the Netherlands, and the United Kingdom, a testing methodology for covert channels 
was developed and published: Information Technology Security Evaluation Criteria (ITSEC). In the 
mid-1990s, as covert channels moved from the realm of possible to probable, many papers were published 
outside of government that explicitly detailed covert channel exploits at the application level and, in 
many cases, provided working source code to build a fully functional covert channel.
One of the most recent adaptions of covert channels involved a data theft incident whereby 
stolen credit card data was simply added to the payload of a DNS request to move the stolen data 
out of the network without raising any suspicion from the network administrators.
Covert channels, in their simplest form, involve encapsulating a particular protocol that 
if used directly would raise suspicion over a protocol that is considered normally permitted 
traffi  c. A timely example would be IRC traffi  c: if an administrator saw IRC traffi  c fl owing 
across the network, it would immediately raise suspicion as it has long been associated with 
nefarious activities. It is trivial to encapsulate the IRC traffi  c over the normally permitted 
HTTP traffi  c that fl ows across the network and escapes the detection of the network‚Äôs security 
mechanisms.
Simply put, fi rewall vendors have known since the late 1980s about the risk of covert channels 
and yet only a select few actually did anything defi nitive, such as adhere to Orange Book stan-
dards for risk mitigation of Covert Channels. Th e problem is much broader when you consider 
that the vast majority of security mechanisms actually aid in facilitating Covert Channels by 
strictly adhering to a port-centric view of network traffi  c.
A good example of this failure would be the evolution of Instant Messaging (IM) and our early 
reliance on blocking the specifi c ports used by the specifi c IM client. IM client providers quickly 
realized that continued use of specifi c ports would allow security administrators to block their 
usage, so they adopted the capability to use not only the typically defi ned port associated with 
the IM client but also literally any port it could fi nd open and available through the network‚Äôs 
defenses. Today, you will fi nd nearly all IM applications tunneling their traffi  c over the most com-
monly open port in a network gateway‚Äîport 80 (normally associated with HTTP).
Th e shortsightedness that a port-centric view brings to fi rewalls is still a problem today as the 
most popular fi rewalls currently in use are limited to applying their protective policies only against 
the specifi c port numbers normally associated with a specifi c traffi  c fl ow. Once you create a rule 
to allow traffi  c to fl ow through your defenses, such as opening port 80 to allow your users to have 
internet access with their browser, any traffi  c including traffi  c from any malicious applications that 
may be residing within your network have the ability to use that open port and simply encapsulate 
their traffi  c on top of the HTTP protocol and evade your ability to block it.
Historically, the author is only aware of a small number of legacy fi rewalls such as the Cyber 
Guard Firewall and Secure Computing Sidewinder Firewall (who‚Äôs legacies can ironically be 
traced back to Orange Book) that aff ord the necessary application awareness to allow a specifi c 

Back to the Future ‚óæ 25
application to use a specifi c protocol and to automatically deny by default any ability for a foreign 
protocol to be encapsulated across that permitted protocol. However, these fi rewalls are severely 
limited by the small number of protocols they can fully support, and in the current environment, 
some simply fi nd them unusable as they have a tendency to break applications that they are not 
fully aware of.
Today, a diff erent approach in next generation fi rewalls is quickly taking shape: Th e PaloAlto 
Firewall, which was noted earlier in this chapter as having the ability to control any of the nearly 
1000 applications necessary to conduct business in today‚Äôs enterprise environment, can be found 
operating literally over any port or service. Simply put, it does not classify traffi  c based solely on a 
given port or service but identifi es the protocol using a combination of heuristics and learned behav-
ior on the wire regardless of any inference to a specifi c port or service. Th is eliminates the port-centric 
issue of traditional fi rewalls and returns our ability to construct a workable deny-by-default approach 
to network security, whereby only those applications specifi cally permitted by policy are permitted 
to traverse the network regardless of the port or service it happens to be operating over.
Another promising approach to the covert channel issue takes a completely diff erent approach 
to the problem. Rather than trying to control the ability of a malicious or undesired application 
from being able to move traffi  c across the network, Lumension Application Control eliminates 
the ability of the application itself from being able to operate on the workstation in the fi rst place. 
Th e application fi ngerprint and centralized management approach utilized all but eliminates the 
administrative burden one might associate with the methodology. Using this approach brings 
back the ability to construct an enterprise network that is able to operate in a default deny state 
for any application that is not explicitly permitted by the organization‚Äôs security policy. Able to 
support thousands of current generation and legacy applications, it can operate without the issue 
of legacy solutions that suff ered from a severe limitation of their breadth of the applications associ-
ated with a modern enterprise.
With the ability returning to eff ectively enforce a default deny policy for the enterprise, we can 
again begin to address the covert channel issues identifi ed decades ago in the Orange Book. For most, 
the remaining question is where is it best to address the issue‚Äîat the gateway or on the endpoint? In 
the others, opinions have been long convinced that a layered approach to network security is the only 
eff ective approach and the issue should be addressed both at the gateway and on the endpoint.
In Closing
Long ago, those responsible for network security for those networks connected to the public 
Internet made a decision in a relatively immature and low-threat environment where unique 
threats on the wire could be measured in the hundreds annually. Th at ease of use and performance 
were more important than the ability to control a wide range of seemingly distant security threats. 
Th e security provisions of the Orange Book were relegated as being too diffi  cult to implement, 
too CPU intensive to off er acceptable performance, and for the most part relegated as ‚Äúold school 
technology‚Äù with no place in the modern enterprise.
In our current threat environment where threats are now exceeding over 5,500,000 unique 
malicious threats annually on the wire, current generation solutions for antivirus and fi rewalling 
have been found to be nothing short of completely overwhelmed and simply unable to scale to 
meet our future defensive needs and are eff ectively obsolete. Further, many of the risks originally 
addressed by the security provisions outlined in the Orange Book standards have come to fruition 
and unfortunately remain even today, two decades later, unsolved by the most popular solutions 
available in the security marketplace.

26 ‚óæ Information Security Management Handbook
Perhaps the author is simply showing his age by taking a nostalgic look back on what could 
have been‚Ä¶.. but cannot help but wonder what the threat environment we face today would be 
like if we had simply adopted the principles outlined in the Orange Book some two decades ago. 
It would most certainly not resemble the mess we currently fi nd ourselves in today.
About the Author
Paul A. Henry, MCP+I, MCSE, CCSA, CCSE, CISSP-ISSAP, CISM, CISA, CIFI, CCE, ACE, 
GCFA, is president of Forensics & Recovery LLC, Ocala, Florida.

DOMAIN 
2
TELECOMMUNICATIONS 
AND NETWORK 
SECURITY
Communications 
and Network Security


29
2
Chapter 
Adaptive Threats and Defenses
Sean M. Price
Th e survival of living organisms is often dependent on their ability to compensate for changes in 
their environment. Th e ability of an organism to compensate for changes encountered is referred to 
as adaptation. Predominately, the methods of adaptation involve changes in the organism‚Äôs behavior, 
physical characteristics, or both. Some creatures are able to learn new skills or tricks that allow them 
to cope when changes occur. In other cases, an organism might undergo a genetic mutation that 
provides it with a slight advantage over its rivals allowing it to survive better given the changed condi-
tions. Adaptation can also occur with the combination of altered behaviors and new mutations. Th e 
ability to adapt is also exhibited in the cyber realm by threats and defenses. Th is chapter is primarily 
focused on the adaptability of attacker malware and defender security tools.
Th reats and defenses have evolved over the years. Th e emergence of the fi rst forms of malware 
and hacker tools was followed by defensive tools and techniques. As new methods of attack are 
pursued, defensive measures arise to counter the threat. Th is constant struggle between attackers 
and defenders is sometimes referred to as an ongoing arms race (Carlsson and Jacobsson 2005). 
Th e goals of attackers and defenders are equally opposed to each other. Attackers seek to exploit 
Contents
Evolution of Th reats and Defenses............................................................................................. 30
Adapting Th reats .................................................................................................................. 30
Behavioral Changes ...........................................................................................................31
Th reat Mutations ............................................................................................................. 34
Adaptive Defenses ................................................................................................................. 36
Behavior Modifi cation ...................................................................................................... 36
Defense Mutations ........................................................................................................... 38
Defensive Adaptation Weaknesses .................................................................................... 39
Strengthening Defensive Adaptations ................................................................................41
About the Author ...................................................................................................................... 42
References ................................................................................................................................. 42

30 ‚óæ Information Security Management Handbook
a system while the defenders attempt to prevent compromises. Th e objectives for each of these 
competitors could be summarized with the following:
Th reat objectives
Discover new weaknesses
 
‚óæ
Exploit new and old vulnerabilities
 
‚óæ
Hide presence
 
‚óæ
Retain a foothold in compromised systems
 
‚óæ
Defense objectives
Counteract known threats
 
‚óæ
Detect deviations from normal activity
 
‚óæ
Identify abuse of the system
 
‚óæ
Mitigate known vulnerabilities
 
‚óæ
Evolution of Threats and Defenses
Over time the objectives of threats and defenses has not changed much. However, the methods 
used to achieve their objectives have substantially evolved. In the early days, threats were single 
purpose and could be generally categorized according to its attack vector. Initially, the taxon-
omy of malware was predominately marked by viruses, worms, backdoors, keystroke loggers, and 
Trojan horses. Human threats included hackers, crackers, and social engineers. Adaptations soon 
appeared with the emergence of malware such as spyware and remote-access Trojans. Similarly, 
the human threat evolved with the new uses of spam and phishing techniques. More recently, 
threats and defenses began to exhibit adaptability by use techniques from diff erent categories 
(Geer 2005). Th e use of multiple categories is regarded as a compound threat or defense.
Attackers quickly learned that combining attack vectors enabled deeper penetration and more 
automation. Malware authors began to incorporate a variety of attack methods into their code. 
Instead of a worm simply infecting one system after another through a single exploit, it would 
drop packages enabling further compromise of the system. Bots, for example, are a recent evolu-
tionary step in malware that are perhaps the most troubling. Th ey automate much of the manual 
activity previously accomplished with hacker tools.
To a lesser extent compound defenses have emerged. Many security products now incorpo-
rate multiple defensive measures such as antivirus, anti-spyware, phishing fi lters, spam blockers, 
and fi rewalls (Greiner 2006). Th ese eff orts appear to be more about consolidation and rivalry 
between the products of security vendors as opposed to focused eff orts to compete against mali-
cious code. Th e impact of compound defenses seems much less substantial than the eff ect of 
compound attacks.
Adapting Threats
Th e ability of a threat to retain its relevance is strongly tied to its capability to adapt. Automated 
and external human threats often exploit a weakness to gain further access within a system. As 
weaknesses are corrected or countermeasures put in place, the relevance of a threat is diminished. 
Th reats must change their malware to adapt to these changes that prevent or restrict their tools 
from performing their devious tasks.

Adaptive Threats and Defenses ‚óæ 31
It is important to bear in mind that automated threats such as malware are largely dependent 
on the hackers that code them. Aside from polymorphism, malware adaptations are strongly linked 
to human intervention. Yet the source code infl uencing the polymorphism is a human-generated 
response that is driven by the need to adapt as a method of detection evasion (Christodorescu 
and Jha 2004). In the not too distant future, malware integrated with machine intelligence may 
be capable of generating original source code, discovering new vulnerabilities, and create unique 
methods to exploit any vulnerability. Although some might suggest that view is close to real-
ity, there is little evidence to suggest that machine intelligence is close to achieving this level of 
abstract cognition. Nevertheless, it is likely that some malware will incorporate some or all of these 
attributes on a limited scale.
Th e evolutionary nature of threats is manifested from two points of view. First, changes in the 
behavior of the threat provide one method in which an adaptation can be achieved. In this regard, 
behavior refl ects the actions malware takes to achieve its objectives. Relevant actions include fi le 
operations, registry manipulation, and network activity (Lakhotia et al. 2005). Process spawning 
is also pertinent to behavior as well. Th e second point of view is that of mutation. Th e predominate 
manifestations of a mutation involves changes to the code logic or structure of the binary. Changes 
in behavior will likely induce mutations in the underlying code. However, a mutation is also a 
tactical maneuver supporting adaptations that allow it to avoid detection.
Behavioral Changes
Th e actions and activity of a threat is an indicator of its behavior. Th e longer a threat uses the 
same behavior the more likely it is that defenses will detect and deploy countermeasures against 
it. Th reats, therefore, adapt by changing the methods and techniques used to attack and retain a 
stronghold in a victim system. Continuous adaptations in malware, such as bots, are a common 
occurrence (Holz 2005).
Attack Vectors
Attackers regularly seek new methods to accomplish their objectives. An attack vector is the meth-
ods and techniques used to exploit a particular vulnerability. It is essentially the cumulative steps 
taken to exploit the fl aw. For any given vulnerability there may exist a multitude of ways to exploit 
it (Ma et al. 2006). Th reats can adapt their attack vector behavior by modifying the actions pur-
sued to compromise the system.
Vulnerability Exploitation
A threat agent may attempt to exploit one or more vulnerabilities to achieve its objective. In time, 
due to awareness and fl aw remediation, a targeted vulnerability might disappear, become irrel-
evant, or prove too diffi  cult to eff ectively exploit. To remain pertinent, the threat must be capable 
of choosing diff erent vulnerabilities to attack. A changing list of vulnerabilities to choose from 
provides the threat with a means to alter its attack behavior. Th e aff ect of this behavior enables the 
threat to adapt to environments where some vulnerabilities are mitigated. Bots, in particular, are 
often coded with the capability to exploit multiple vulnerabilities (Geer 2005). Having the ability 
to select among vulnerability options can also make it more diffi  cult for defensive mechanisms to 
target a particular threat.

32 ‚óæ Information Security Management Handbook
Command and Control
Much of the malware in the wild today rely on some form of command and control. Th is 
enables the threat agent to communicate with and direct the activities of the malware. With 
respect to botnets, command and control is recognized as an important aspect of their value 
(Schaff er 2006). Th e three main types of communication methods can be categorized as 
follows:
Independent‚ÄîIn this category, a malware opens a communication channel and listens for 
 
‚óæ
commands. Th e listening activity could be TCP, UDP, or both types of ports. In these cases 
the malware threat does not know where its command will come from.
Centralized‚ÄîSome malware know how to contact their master. Th is could be a particular 
 
‚óæ
Web site or e-mail address, but the most common is an Internet Relay Chat area. In these 
cases the malware looks to a primary address to receive commands.
Decentralized‚ÄîOne trend among attackers is to organize the malware as a collective entity. 
 
‚óæ
Th is has the advantage of ensuring the malware can survive and can make it more diffi  cult 
to detect the command and control origin. Th is type of control is similar to peer-to-peer 
(P2P) networks.
A threat can adapt its behavior within each of these categories. For instance, malware using an 
independent method of command and control could change the port on which it is listening. 
It may also change the application protocol used, imitating other known services or something 
completely novel. Centralized threats can exhibit behavior change by contacting diff erent or new 
centralized command centers to obtain instructions. Lastly, decentralized malware might try to 
mimic legitimate P2P, change its underlying application protocol, or use encryption. It is worth 
noting that a suffi  ciently ‚Äúintelligent‚Äù malware agent might be capable of selecting among all three 
categories. Th is type of behavior could make it more diffi  cult for network monitoring to detect its 
presence.
System Interaction
In most cases, a threat exploiting a weakness results in the appearance of executable binaries on the 
compromised system. Th e binaries are usually standalone, but could be attached to other objects, 
in the case of a virus. Th ese malicious software components may interact with the system in a 
variety of ways. Some of the most common instances follow:
Executables‚ÄîAn executable fi le runs as an independent process. Th e threat might be con-
 
‚óæ
tained entirely in the executable or it may rely on it as a way to initiate other activities such 
as downloading other malware.
Extensions‚ÄîA malicious library could be used to extend existent malware or have it loaded 
 
‚óæ
into legitimate applications allowing it run more discretely.
Injections‚ÄîSimilar to extensions, a library might be injected into the execution space of 
 
‚óæ
another process. Some viruses create their own thread of execution within the host pro-
cess. Although it is not truly an injection, the execution of a virus has many of the same 
implications.
Rootkits and drivers‚ÄîMalware at this level has the capability to hide its activities or that of 
 
‚óæ
participating malicious processes.

Adaptive Threats and Defenses ‚óæ 33
Th reats can alter their behavior by interacting with the system through any and all of the aforemen-
tioned methods. By changing the way a threat interacts with a system it increases the likelihood that 
it will either avoid detection or make it more diffi  cult to remove.
Storage and Confi guration
Th e code enabling the threat to execute on a system must exist somewhere in storage and also 
requires some confi guration method to prompt its activation. Th reats will alter their behavior by a 
variety of methods that obfuscate their storage locations. Th ey may change their fi le names 
and/or extensions to hide their presence. Confi guration entries are generally needed to assure that 
the threat is launched regularly.
Obscure fi le and directory names‚ÄîTh is can include names that are randomized. Some 
 ‚óæ
threats create random names while others select from a pre-populated list within the malware 
package.
Alteration of registry entries‚ÄîSome threats create their own entries or rely on the entries of 
 
‚óæ
legitimate software.
Alternate data streams‚ÄîMalware hiding in an alternate data stream is not easily observed 
 
‚óæ
with standard system tools.
Changes to confi gurations fi les‚ÄîTh is is similar to the methods used for registry entries.
 
‚óæ
Masquerading as legitimate fi les‚ÄîA threat might select a fi le name that is similar to legiti-
 
‚óæ
mate software. In some cases, the threat might actually rename or replace a legitimate binary 
fi le. Th e malware then is loaded whenever calls are made to the replaced fi le. In these cases, 
the threat will proxy the requests to the actual library whether it is in another fi le or is 
included with the threat itself.
Malware will often change their storage locations and confi guration methods to remain a step or 
two ahead of defensive countermeasures. Th is adaptive behavior of modern threats enables it to 
persist within a compromised system.
Recruiting
A small number of threat agents can multiple their capabilities by duplicating their eff orts. Th e 
common denominator of this behavior is to entice people to execute their malicious code. Malware 
are increasingly acting as the intermediary between the human threat agent and the human victim. 
A number of enticements are commonly used to recruit new victims.
Trojaned freeware‚ÄîAn off er for a new free toy with hidden strings attaching to the victim‚Äôs 
 
‚óæ
system.
Pornography‚ÄîTh e promise of a gratuitous glimpse into an act of indiscretion through a link 
 
‚óæ
or attachment.
Financial‚ÄîA lure to easy riches that turns out to be true for the attacker.
 
‚óæ
Spyware‚ÄîTh e participant is promised reduced rates or access to a particular application by 
 
‚óæ
using a particular software product. Often times, much more is disclosed than what was 
agreed to.
Scareware‚ÄîTh e end user is informed that their system is infected with malware and encour-
 
‚óæ
aged to download an antivirus tool to help clean their system. Th ey are commonly enticed 

34 ‚óæ Information Security Management Handbook
to purchase the fake antivirus product. Th is is reportedly a big income generator for many 
attacks.
Phishing‚ÄîMasquerading as a legitimate entity, such as an online bank, but really duping 
 
‚óæ
the e-mail recipient into disclosing their private information.
Problem solving‚ÄîUnwitting participants solve reverse Turing test problems, such as 
 
‚óæ
CAPTCHAs, that are too diffi  cult for malware to deduce. Often this is used with other 
enticements such as pornography.
Threat Mutations
Code updates to malware has parallel attributes to evolution in living organisms. In time, a given 
piece of malware must adapt or it will be more readily recognized by defensive measures such as 
antivirus and anti-spyware tools. Mutations in this regard are essential for the malware to main-
tain relevance. Mutation to avoid detection is a common malware tactic (Edge et al. 2006). Th e 
following summarizes some of the reasons why threat mutations are an aspect of adaptation.
Defeat Signatures
An unchanging or static nature of an attacker makes detection easier over time. Researchers and 
security product vendors constantly seek the telltale signs and behaviors of attackers. Once the 
attributes are learned the data is compiled into tools and techniques that can be used to detect the 
presence of an attacker. Th reats must, therefore, change their code and alter techniques to defeat 
signature analysis. New versions of the malware or polymorphic techniques are common methods 
used to defeat signature based defenses (Hsu et al. 2006).
Code Improvements
Some malware is just plain buggy. It is not uncommon for malware to cause poor performance or 
even disrupt applications (Schmidt and Arnett 2005). In recent years, the shift from hacking by 
rogue amateurs to those of nation-states and organized crime are accompanied by improved code 
reliability. Today malware is less likely to aff ect performance. However, it is important to con-
sider that operating system improvements may have also contributed to more stable performance 
even when misbehaving applications are present. In any case, attackers will regularly attempt to 
upgrade their malware allowing it to better adapt to its environment.
Detection Avoidance
Overtime malware methods have become more sophisticated. Much of the eff orts for improve-
ments are related to techniques that hide the presence of the malicious code. Malware adaptations 
are increasingly disguising their activities to mimic legitimate system and network activity (Borders 
et al. 2006). Several years ago much of the malware resided in a single executable or may have 
included a small number of libraries. Th e existences of these tools were often readily observable in 
the fi le system and could be seen as distinct processes when executing. Th e next evolutionary jump 
emerged as add-ons to existing products. Malware increased the ability to cloak their activities by 
taking advantage of legitimate software features that enable extensions. Examples of these include 
system hooks, add-ons for offi  ce productivity software, and browser helper objects. By running 
as a loaded module the malware avoids detection of some process monitoring techniques, but are 

Adaptive Threats and Defenses ‚óæ 35
still observable through system tools. In case of extensions the malware plays by the rules of the 
operating system. In contrast, other techniques such as vulnerability exploits and process injec-
tion are used to force a target application to run the attackers code of choice. Th ese approaches to 
detection avoidance are more stealthy and not easily identifi ed. Th e latest evolution of detection 
avoidance involves the use of rootkits and system drivers. Th ese methods allow the tool itself and 
accomplice malicious processes to operate largely undetectable by most system and security tools. 
Malware with stealthy capabilities hide their behavior by intercepting and fi ltering application 
programming interface (API) calls that could be used to reveal their presence (Wang et al. 2005). 
Th e trend of malware and attackers has gone from brazen attacks defacing popular sites or sensa-
tional attacks against a well-known Internet presence to discrete compromises as chilling as any 
clandestine espionage activity could achieve.
Added Capabilities
New features incorporated into malware increase its value and potentially expand the infl uence 
of an attacker. Increased capabilities represent a maturation of the malware, which is a type of 
adaptation for survival. For example, an update might give the attacker the capability to scan other 
hosts for weakness or act as a relay for other malicious activity. Increased capability enables the 
threat to adapt to an environment and potentially sustain or propagate its existence.
New Objectives
An attacker may periodically change targets or attack vectors. Th is is a common occurrence in 
botnets where the bot-herder rents out the zombies to service their customer requests (Geer 2005). 
Th e ability to change objectives is a tactical adaptation that makes for a superior weapon. Older 
malware usually had limited objectives that were not altered. Nowadays malware can attack new 
targets using diff erent vectors or exploits through the receipt of software modules embedded with 
the new objectives and commands (McLaughlin 2004).
Upgrade Survival
Overtime, systems are upgraded or reinstalled. Th reats must be able to adapt to new technologies 
for the relevance to remain. As an example, a system owner might migrate from one technology 
(i.e., e-mail client or Web browser) to another. An adaptable threat will be able to accommodate 
the change and continue unimpeded if the migration represents a vector for exploitation. Upgrades 
to the underlying operating system can also aff ect the ability of the threat to endure. Adaptable 
threats anticipate or respond to these changes through code changes that allow their existence to 
continue. Although, a threat outside of a supply chain injection may not be capable of surviving 
a fresh installation of the OS or applications, it can attempt to persist by incorporating itself with 
legitimate applications and data. A threat that carefully infuses itself with data and applications 
targeted by managed backups enable malware longevity due to upgrades.
Self-Preservation
It is not uncommon for malware to disrupt, disable, or destroy security controls in a system. 
Some aggressive threats will alter access controls to protect themselves. Others reportedly disable 
host-based fi rewalls and antivirus software (Abu Rajab et al. 2006). Th is sort of activity ensures 

36 ‚óæ Information Security Management Handbook
communications with the threat agent will remain intact. Yet, other malware may be so bold as to 
delete programs or audit data that could be used to detect or disrupt it. Th e techniques and meth-
ods used for self-preservation must adapt according to changes in technology and the environment 
of the compromised system.
Competition
Imagine that a zombie computer is under the infl uence of diff erent bot-masters. Serving multiple 
masters might produce erratic behavior on the machine. Th ere appears to be an unspoken consen-
sus in the evil realm of malware creators that a zombie should not exhibit personality disorders. 
Th is perceived consensus is most likely imaginary. In reality, some malware attack and remove 
other malware (Osorio and Klopman 2006). Additionally, some malware reportedly patch exist-
ing vulnerabilities (Abu Rajab et al. 2006). Th e reasons for this competition probably include 
rivalry, dominance, or economic advantage. In this regard competition among living organisms 
spills over into the cyber realm and is witnessed as malware on malware attacks. Th e escalation of 
malware competition is yet another dimension of the adaptable nature of threats.
Adaptive Defenses
Agile defenses are necessary to counteract adaptable threats. Defensive countermeasures are con-
tinuously challenged by the rapid changes they must deal with. On one hand defenses must adapt 
to changes in their environment. Network expansions and new technology can easily introduce 
exploitable weaknesses. On the other hand, threat aggressiveness continues to escalate. Th e rapid 
evolution of malware puts continuous pressure on defenses to adapt. From the perspective of a 
defender, adaptation is an imperative that must meet the challenges of environmental changes 
while remaining competitive with adaptive threats.
Attackers continuously conduct new and inventive assaults on network defenders. New attack 
methods brought about by malware adaptability are met with adaptive defenses. Th e discoveries of 
new attacks are often shared in the security community. Conjectured exploits by security research-
ers or actual exploits discovered in the Internet are reported by numerous public and private orga-
nizations. Th is new information is often integrated into defensive countermeasures resulting in an 
adaptation to the threat.
Defenses exhibit adaptation through behavior modifi cation and mutations. Th e objectives of 
defenses tend to be more reactionary to threat activity. In contrast, the adaptations of threats are 
more exploratory and proactive. In this regard, defense adaptations tend to lag those of threats.
Behavior Modifi cation
Defensive controls face more challenges than do their nemeses. Tools used to defend a system 
require behavior modifi cations to account for changes to the environment as well as proliferating 
and changing threats. Behavior modifi cations entail the methods used to accommodate the rapid 
changes in the organization, technology, and known threats.
Frequency
Adaptive defenses may alter the period with which they conduct their surveillance. For instance, 
vulnerability scanners might ordinarily be used on a monthly basis. If the organization is 

Adaptive Threats and Defenses ‚óæ 37
experiencing substantial growth or more frequent compromises then the frequency of the control 
is increased. Th e timeframe between the moments of detection activity presents opportunity for a 
threat to attack a system. Increasing the frequency behavior is an adaptive approach to counteract 
rising malware instances.
Breadth
Security controls are not always deployed in every possible location in a network due to resource 
constraints. Furthermore, a control might also peer into a narrow band of activity in its attempt 
to identify attacks. In some cases, the purview of a control can be expanded to cover a larger area. 
Th is could be through increased instances in a system or through expansion of the band of activity 
monitored. Altering the breadth of a control impacts its behavior. Essentially, an increase in the 
horizontal nature of the control adapts the behavior of the defensive mechanism to detect mali-
cious activity.
Depth
Viewing activity in a system from the perspective of the Open Systems Interconnection (OSI) 
model provides a vertical perspective regarding the behavior of a security control. A security con-
trol might ordinarily operate at only one layer of the model. An adaptive defensive tool might 
occasionally perform inspections at other layers of the model to detect attacks or actual com-
promises. Th is type of capability demonstrates a change in behavior that could be very useful in 
detecting adaptive threats.
Indicators
Many defensive mechanisms rely on precompiled indicators or signatures of known attacks. 
Adaptable defenses have the capability to compare system activity against new and prior indicators 
to detect active attacks. Adaptability through indicators is one of the most predominate behavior 
modifi cations of defensive components.
Baselines
Well-managed systems have a number of documented baselines. Th ese include baselines for hard-
ware, software, network connectivity, and confi gurations. Defensive tools with the capability to 
detect system components and confi gurations can validate baselines. Security controls with the 
ability to make comparisons between system changes and a documented baseline exhibit adapt-
able behavior.
Learning
Perhaps the most intriguing representation of behavior modifi cation occurs when a secu-
rity control actually learns something. Machine learning techniques are commonly found 
in security tools designed to look for anomalous activity. Two common implementations 
of machine learning are used by intrusion detection and spam fi ltering. Intrusion detection 
products make use of neural networks and support vector machine algorithms (Mukkamala 

38 ‚óæ Information Security Management Handbook
and Sung 2003). Spam fi lters typically use Bayesian techniques (Pelletier et al. 2004). In both 
cases, the tools learn what is normal versus what is not and raise alerts when anomalous 
features are encountered.
Interactions
In the near future, technologically advanced security controls will receive data from multiple 
sources. Th is will provide the security control with the ability to form a more coherent picture 
of the cyber landscape. A security control with this advanced capability would be able to make 
predictions or advise human counterparts and other participating security controls of the current 
security state of the system. According to the situational awareness from the infl ux of data these 
advanced tools will exhibit behavior unlike its archaic predecessors. Th e interaction among these 
tools will form a collective that shares threat information and alters its behavior accordingly. For 
now, we rely on the interactions and sharing among humans to infl uence the most robust defensive 
control‚Äîthe information system security professional.
Defense Mutations
Most of the behavior modifi cations are realized through mutations of the aff ected defensive control. 
Adaptation by way of mutation is for the most part straight forward regarding security controls. 
It is important to note that a mutation need not necessarily be compiled. Th e inclusion of any sort of 
logic enabling adaptability qualifi es as a mutation. Some of the most prominent mutations employed 
to achieve adaptability follow.
Signatures
Features, behaviors, and characteristics of malware and indicators of threat activity comprise 
attack signatures. In many instances the fi les containing the signature information are compiled 
into a library or proprietary data fi le. From this perspective, the inclusion of the new signatures 
results in a mutation of the defensive tool.
Rule Sets
Th is type of mutation is comprised of multiple ‚Äúif‚Äìthen‚Äù statements. Rule sets permit the logi-
cal evaluation of witnessed activity. Some rule sets are ordered to form logic trees. Th is allows 
for granular decisions based on collected information. Changes in the environment or attacker 
behavior may require changes in rule sets. Altering rule sets according to changes or trends enables 
adaptability through this type of mutation.
Thresholds
Cumulative events can be used to activate security controls. For instance, a control monitoring 
access control failures might not raise an alert unless the number of failures exceeds 10 events in 
less than 1s. A threshold of this type is designed to detect malware behaviors given the successively 
repetitive failures in a period of time too fast to be driven by a human manipulating a graphi-
cal user interface. Th e composition and attributes of thresholds can be changed to adapt to new 
threats or changes in the behaviors of known malware.

Adaptive Threats and Defenses ‚óæ 39
Defensive Adaptation Weaknesses
Th e traditional model used by defenders has been to shore up weaknesses or adapt to threats by 
deploying new or modifi ed tools counteracting the particular threat. Often times this can be over 
an extended period of time after a vulnerability is disclosed and exploit code is available. Sadly, 
adaptive defenses are primarily reactionary. Defensive measures usually target specifi c types of 
attacks deemed imminent. Rarely, will an organization incorporate a new defensive measure that 
is not focused on a particular threat or attack vector that has not been experienced by the organi-
zation or industry. Th e main reason for this line of thinking has to do with risk. An organization 
may deem a particular threat, likelihood, or loss to be minimal. Due to the prevalence of risk 
management by way of qualitative assessments, coupled with the ever-present problem of scare 
resources, it is not uncommon for managers to be optimistic about their level or risk. As such, 
adaptive or forward thinking defenses are not commonly deployed. Th is has the unfortunate side 
eff ect of causing defensive countermeasures to play catch-up with the attackers. Th is is evident by 
the relentless cycle of patching and signature updating.
Specifi city
Defensive measures often rely heavily on specifi c signatures. Th e eff ort required to adapt to a new 
threat may be greater than that needed by the threat. Signature development can also require 
substantial time and eff ort to compile that can signifi cantly lag a threat that is rapidly propagating 
(Edge et al. 2006). Considering the time and resources needed to detect, develop, and deploy an 
adaptation for a given threat it seems that attackers have the upper economic hand.
Timeliness
Th e creation of a countermeasure for a given threat may be well after signifi cant damage occurs 
(Cui et al. 2005). In some cases this reactive adaptation can be too little too late. It is now common 
for exploits for previously unknown vulnerabilities to be found in the wild (Levy 2006). In this 
regard defensive adaptations are entirely reactive with respect to weaknesses and exploits.
Growth Rate
New threats are beginning to emerge at a rate faster than security defensive measures can adapt. 
A recent estimate claims that the volume of malicious code exceeds the production of legitimate 
software (Nachenberg 2008). One implication of this growth rate is that defenses may consume 
substantially more resources to determine if a threat is present or not. Th is reduction in effi  ciency 
will likely inhibit the ability of the defensive measures to adequately adapt to the ever-increasing 
number of threats. Furthermore, the sheer volume may also imply that a larger number of malware 
is circulating in the wild that are unknown to defensive product vendors. Th is is to suggest that 
false negatives (malware missed by detectors) will increase. A substantial growth rate has the eff ect 
of overwhelming our defenses by a numerically superior enemy.
Environment
Changes in the system environment present unique challenges. Growing organizations regularly 
add new network equipment to accommodate a growing user base. New technologies enabling 

40 ‚óæ Information Security Management Handbook
increased productivity may also include new vulnerabilities. Defenses must not only adapt to 
system growth but new technologies as well.
Search Space
Adaptations that attempt to characterize what is normal in a system often fail due to complexity. 
Defensive techniques such as anomaly detection are commonly designed to look at everything to 
identify features that are not normal. Th is requires the defensive tool to look at the entire uni-
verse of possibilities. Tuning is often used to increase performance by reducing the search space. 
However, the search space is often still too large allowing false positives to persist. In some cases, 
items ignored by the rule set can be abused by attackers and thus avoid detection.
Constraints
Whereas malware authors are free to attempt anything desired to achieve their objectives, defenders 
are much more constrained. Th e adaptability of defensive measures is reduced due to factors in 
their environment.
Financial‚ÄîAdaptability often requires a monetary tradeoff . Whether the cost involves 
 
‚óæ
time, people, or materials the lack of suffi  cient fi nancial resources can constrain defense 
adaptations.
Personnel‚ÄîAdequately trained people must be assigned to monitor, respond, and manage 
 
‚óæ
defensive controls. Adaptations that are diff erent from those existing are impacted by the 
abilities of those assigned responsibility. A superior adaptation is of little use if the end users 
are unable to implement it properly.
Performance‚ÄîAn adaptation must not severely degrade system performance. Whereas 
 
‚óæ
malware can be careless about performance issues, defensive measures with performance 
problems are often unacceptable even when they provide an important adaptation to a class 
of threats.
Usability‚ÄîDefensive adaptations that are eff ective, but too diffi  cult to use will not fi nd 
 
‚óæ
favor with those who need them most. Complicated adaptations will be abandoned or cir-
cumvented by humans who are attempting to accomplish a particular task.
Management‚ÄîA properly managed system implements change control. However, this can 
 
‚óæ
impede deployment of the adaptable defense. An adaptable control might be altogether 
avoided if it is perceived to be too diffi  cult to manage.
Operations‚ÄîEff ective defenses contribute to security operations. An adaptable defense that 
 
‚óæ
exists in a silo inhibits the fl ow of security operations.
Design‚ÄîIdeal security controls are built-in rather than bolt-on. Unfortunately, most adapt-
 
‚óæ
able defenses are bolt-on. Integration eff orts may be hampered by the complexity of the 
tool.
Perceptions‚ÄîQualitative risk assessments might lead management to conclusions that a 
 
‚óæ
particular adaptable defense is unnecessary. Perceptions based on insuffi  cient or inaccurate 
information inhibit the acquisition and deployment of adaptable defenses.
Th ese constraints burden defensive adaptability. Constraints impact the ability of a defensive 
control to compete with the unbridled capabilities of malware encountered. Th e competition 
between adaptable threats and defenses are becoming increasingly unbalanced in the favor of 

Adaptive Threats and Defenses ‚óæ 41
malware. In this regard, attackers have a distinct advantage that is evident by the continued 
rise in compromises and data losses.
Strengthening Defensive Adaptations
Security is fi rst and foremost a people problem. Weaknesses in systems are going to occur. All 
security problems have their root in people. Some programmers will make mistakes when coding 
that is further missed by reviews and quality assurance. System integrators will occasionally put 
things together incorrectly. System administrators will introduce confi guration errors or fail to fol-
low procedures. Users will also make honest mistakes and fall victim to an attacker‚Äôs trickery. Let 
us not forget that malware is an off spring of the warped eff orts of bad people. All sorts of unsavory 
individuals such as criminals, spies, and terrorists are ultimately directing the actions of malware. 
Our goal as security professionals should be to not only employ adaptive defensive technology, but 
also to establish adaptive operations that are proactive. In this regard, the reactive nature of our 
current adaptable defenses can be augmented with techniques and processes that are prepared for 
the worst. Consider some of the following during the design, implementation, and management 
of security operations for information systems.
Anticipate Compromises
Develop an attitude that the best plans will eventually be circumvented. Manage stakeholder 
expectations by advocating proactive measures that can be used as early warning detection of 
failed countermeasures. Note areas within the system that are at higher risk for compromise and 
conduct more frequent reviews.
Response Plans
Contingency planning and incident response are invaluable tools that can be used to prepare for 
the eventual compromise in a system. Having a plan is great, but it is only as good as those who 
are suffi  ciently familiar with its guidance. Th e plans should be regularly practiced and updated 
when weaknesses are discovered. Ensure the plans address the actions required to clean the system 
and restore normal operations.
Penetration Testing
Periodically attempt to break into your system. Hire reputable professionals to do the same. Use 
some of the same tools attackers use to compromise a system. Penetration testing should be used 
to exercise contingency and incident response plans.
Operational Alternatives
Few, if any, software products have proven impervious to vulnerabilities. Unfortunately, new vul-
nerabilities seem to be reported weekly for some products. Critical vulnerabilities with exploitable 
code in the wild may subject an organization to unacceptable risk. At such times it may be prudent 
to deploy or have ready other products that can be used instead of the one with a critical vulner-
ability. Require the use of alternative applications until all instances of the vulnerable product 
are appropriately patched. Consider altering access controls on the aff ected application to prevent 

42 ‚óæ Information Security Management Handbook
intentional or accidental use. Th e downside to operational alternatives is increased management 
complexity and cost. Th e cost of a potential exposure and cleanup should be compared with the 
periodic licensing and management expenses.
Defense in Depth
Traditionally, defense in depth relies on the overlapping of policy, people, and technological 
countermeasures (Price 2008). Although this is a good idea it is proving too shallow. For instance, 
systems are often protected from malware by a policy that requires antivirus tools that are regularly 
updated, people to confi gure and use the tools, and the tools themselves deployed on workstations 
and servers. Th e problem with this approach becomes apparent when all of the mechanisms fail. 
Rather than use another antivirus product it would be better to implement secondary controls that 
could be used to detect and/or prevent virus propagation. Access controls, auditing, least privilege, 
network segregation, and intrusion detection are just some of the tools that can serve double duty 
to detect and defend against malware. But, they must be properly implemented and monitored to 
suffi  ciently detect the failure of the primary defense in depth controls.
Monitor for Changes
Ensure system managers have complete listings of the authorized hardware devices, software 
components, and their confi gurations in the system. Frequent sweeps of these aspects of the system 
should be conducted. Any changes not found in the listings should be immediately investigated. 
Issues identifi ed should be corrected if inappropriate or included in the system listings if autho-
rized. Identifying unauthorized changes to hardware, software, and confi guration baselines is the 
most eff ective way to determine the existence of adaptable threats.
About the Author
Sean M. Price, CISA, CISSP, is an independent security researcher and consultant living in 
northern Virginia.
References
Abu Rajab, M., Zarfoss, J., Monrose, F., and Terzis, A. 2006. A multifaceted approach to understanding 
the botnet phenomenon. Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement, 
Miami Beach, FL, pp. 41‚Äì52.
Borders, K., Zhao, X., and Prakash, A. 2006. Siren: Catching evasive malware. Proceedings of the 2006 IEEE 
Symposium on Security and Privacy, Berkeley, CA, pp. 85‚Äì91.
Carlsson, B. and Jacobsson, A. 2005. On contamination in information ecosystems. Proceedings of the 38th 
Hawaii International Conference on Systems Sciences, Bigland, HI, vol. 7, p. 185b.
Christodorescu, M. and Jha, S. 2004. Testing malware detectors. ACM SIGSOFT Software Engineering Notes, 
29(4), 34‚Äì44.
Cui, W., Katz, R. H., and Tan, W. 2005. Design and implementation of an extrusion-based break-in detector for 
personal computers. Proceedings of the 21st Annual Computer Security Applications Conference, Tuscon, 
AZ, pp. 361‚Äì370.
Edge, K. S., Lamont, G. B., and Raines, R. A. 2006. A retrovirus inspired algorithm for virus detection 
and optimization. Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation, 
Seattle, WA, pp. 103‚Äì110.

Adaptive Threats and Defenses ‚óæ 43
Geer, D. 2005. Malicious bots threaten network security. Computer, 38(1), 18‚Äì20.
Greiner, L. 2006. Th e new face of malware. netWorker, 10(4), 11‚Äì13.
Holz, T. 2005. A short visit to the bot zoo. IEEE Security and Privacy, 3(3), 76‚Äì79.
Hsu, F., Chen, H., Ristenpart, T., Li, J., and Su, Z. 2006. Back to the future: A framework for automatic 
malware removal and system repair. Proceedings of the 22nd Annual Computer Security Applications 
Conference, Miami Beach, FL, pp. 257‚Äì268.
Lakhotia, A., Kumar, E. U., and Venable, M. 2005. A method for detecting obfuscated calls in malicious 
binaries. IEEE Transactions on Software Engineering, 31(11), 956‚Äì968.
Levy, E. 2006. Worst-case scenario. IEEE Security and Privacy, 4(5), 71‚Äì73.
Ma, J., Dunagan, J., Wang, H. J., Savage, S., and Voelker, G. M. 2006. Finding diversity in remote code 
injection exploits. Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement, Rio de 
Janeiro, Brazil, pp. 53‚Äì64.
McLaughlin, L. 2004. Bot software spreads, causes new worries. IEEE Distributed Systems Online, 5(6), 1‚Äì5.
Mukkamala, S. and Sung, A. H. 2003. A comparative study of techniques for intrusion detection. Proceedings 
of the 15th IEEE International Conference on Tools with Artificial Intelligence, San Diego, CA, 
pp. 570‚Äì577.
Nachenberg, C. 2008. Tomorrow‚Äôs AV marks the good, the bad, and the long tail. Downloaded May 12, 2009 
from http://www.infosectoday.com/Articles/Whitelisting.htm
Osorio, F. C. and Klopman, Z. 2006. An initial analysis and presentation of malware exhibiting swarm-
like behavior. Proceedings of the 2006 ACM Symposium on Applied Computing, Dijon, France, 
pp. 323‚Äì329.
Pelletier, L., Almhana, J., and Choulakian, V. 2004. Adaptive fi ltering of SPAM. Proceedings of the 2nd Annual 
Conference on Communication Networks and Services Research, Fredericton, NB, Canada, pp. 218‚Äì224.
Price, S. M. 2008. Extending the McCumber cube to model network defense. ISSA Journal, September, 6(9), 
14‚Äì18.
Schaff er, G. P. 2006. Worms, and viruses and botnets, oh my! IEEE Security and Privacy, 4(3), 52‚Äì58.
Schmidt, M. B. and Arnett, K. P. 2005. Spyware: A little knowledge is a wonderful thing. Communications of 
the ACM, 48(8), 67‚Äì70.
Wang, Y., Beck, D., Vo, B., Roussev, R., and Verbowski, C. 2005. Detecting stealth software with strider 
ghostbuster. Proceedings of the International Conference on Dependable Systems and Networks, Edinburgh, 
U.K., pp. 368‚Äì377.


45
3
Chapter 
Achieving a Global 
Information Systems 
Transformation (GIST): 
Foundations for Infrastructure 
2.0 via Standards-Based 
Interoperability: 
IF-MAP and Beyond
David O‚ÄôBerry
Contents
GIST: Th e New World Order .................................................................................................... 46
At the Edge of a Digital Abyss .............................................................................................. 46
‚ÄúScrew the Whales, Save the Plankton‚Äù ................................................................................. 46
‚ÄúPermeation with Representation‚Äù .........................................................................................47
IF-MAP: In the Beginning ........................................................................................................ 48
Th at Second Step Is a Dooooooozy ....................................................................................... 49
Complex Graphs May Emerge in MAP ................................................................................ 50
Evolution of ‚ÄúTh reat State Databases‚Äù: SETI for Security 
Coming to a Computer Near You ......................................................................................52
Standards-Based Information Sharing: Enhancing Participation and Agility ..........................55
About the Author ...................................................................................................................... 58
References ................................................................................................................................. 58

46 ‚óæ Information Security Management Handbook
GIST: The New World Order
At the Edge of a Digital Abyss
Th is chapter has to touch on so many topics that there is simply no way I will be able to do so to the 
level I want in the space allotted at the depth I need to do so. What that means is that I am going to set 
the table for where we are today while trying to not sound like ‚ÄúChicken Little.‚Äù When all hope looks 
lost and you think I am some kind of doomsday cultist, I am going to briefl y discuss the foundation 
pieces that are in place as well as those rapidly evolving. Th e standards will be discussed conceptually 
as well as approached in a more actionable form where possible. It is clear we are at a tipping point for 
the open standards-based eff orts and they are becoming not only more critical but also more practical 
to implement. As a whole, they are also forcing us to think diff erently in order to architect, collaborate, 
and then rapidly deploy realistic capabilities to begin to recover from where we are at present.
‚ÄúScrew the Whales, Save the Plankton‚Äù
Th e current state of the digital ecosystem is rather scary when taken in the context of just how impor-
tant it is to the functioning of civilizations throughout the world. For many years, we have been in 
a situation where we have constantly been reacting to situations that occur versus taking a proactive 
stance. I have previously written on the topic of how the ‚Äúthreat cycle‚Äù and ‚Äúproduct cycle‚Äù really have 
nothing to do with one another yet are often treated as one and the same by businesses that sell you 
tools to solve your problems. Th at has been how we have always functioned really because money most 
times equals resources and that equation is balanced pretty signifi cantly to the side of the vendors. Th is 
has been acceptable to many in the past because the prime concern was pretty simply interoperability 
and effi  ciency or the lack thereof. Th e problem is that industry realized the real problems very late in the 
game and some still have not really acknowledged that things have changed. Th e larger vendors really 
do not want things to change because they have often times spent extensive resources to guarantee their 
stockholders some security in the various markets in which they participate. Unfortunately, fi duciary 
duty to shareholders and moral duty to the health of the overall world economic system run counter to 
one another in many instances. What the industry must realize is that the challenges have now changed 
signifi cantly and cannot be looked at as anything other than a systemic cause and eff ect ecosystem. It 
is a system of dominoes driven by consumer confi dence that very much resembles a frail and very tall 
house of cards at times. Having witnessed the fi nancial system debacle fi rst hand with many of you in 
2008, the thing that people seem to remember most was the feeling of helplessness and how profoundly 
they despised being in a situation where they had little or no control over the outcome.
We quickly followed that with signifi cant malware challenges in late 2008 and early 2009, 
which shook the foundations of what the world knew related to the techno-industrial revolution. 
Th e stark realization that the world was not only at the whim of a seemingly out of control tech-
nological beast but truly at the mercy of the criminal networks that wielded it, many of which had 
a better business model than most companies, hit home when we sat waiting to see what would 
happen April 1, 2009. Currently we are faced with a situation where the drivers for our world 
economy are the points that are least protected. Essentially, the majority of the companies that 
are in the security space are only in places in which they can make money. Th at makes sense on a 
microeconomic level but certainly not from a more macroeconomic point of view. Eff ectively we 
are a globally consumer-driven society and even countries that do not contribute nearly as much to 
the consumption side do oftentimes participate from the production perspective. What that means, 
in a nutshell, is that when people stop spending money we nearly immediately feel the eff ects 
and stability falters quickly. So with no real protection, the base of the food chain that drives the 

Achieving a Global Information Systems Transformation (GIST) ‚óæ 47
economies of the world is the most suspect, the most infested with malware, the most helpless, and 
the most at risk. Th at is not a recipe for success. To top that off , we are actually forced to root for the 
crime networks that have a profi t motive to write good code because God forbid someone without 
a reason to keep the money fl owing were to get access, through a vulnerability, to one of the larger 
botnets or information stores that are gathered by them. If that were to happen, then all it would 
take is a simple transaction involving small amounts of money and thousands of compromised 
bank accounts. Th at money could even be put back the next day but an attached message to the 
world would read, ‚ÄúWe put your money back this time but can take it away at any point we choose.‚Äù 
Imagine the potential run on the banks that could cause. It could possibly make the fi nancial crisis 
of 2008 look like a very small blip on the radar. It could be a crippling blow to many economies 
and the house of cards would, more than likely, just come down. It looks grim but a group light on 
marketing but heavy on capabilities has stepped up its eff orts over the last decade to fi ll the breach 
and potentially make a real diff erence from an open and interoperable point of view.
‚ÄúPermeation with Representation‚Äù
Trusted Computing Group (TCG) has been toiling in relative obscurity but has been and continues 
to be critical to the evolution and progression of several key standards that form a signifi cant por-
tion of the foundation we will discuss and they need to build on going forward to have a chance to 
turn the tide. Specifi c working groups like Trusted Network Connect (TNC) are just one arm of 
what the group as a whole is working on. At the same time this specifi c area has taken on an incred-
ible amount of importance over the last few years as the technology curve has accelerated while the 
education curve has seemingly fallen off . Figure 3.1 is a chart that shows the status as of the end 
IWG TNC document roadmap
Infrastructure
architecture:
Part II: Integrity management
Infrastructure
architecture:
Part I:
Interoperability architecture
Migration
and backup
Certificate
profiles v1.0
Trust
credentials
IF-IMC
IF-IMV
IF-PTS
IF-TNCCS
IF-M
IF-T
IF-PEP
IF-MAP
TNC
use cases
Other
use cases
SKAE
TNC
architecture
IWG
use cases
Integrity
report schema
Core
integrity schema
RIMM
schema
Credentials
TNC IF-MAP binding for SOAP
specification version 1.0
TCG copyright
.....
Figure 3.1 TNC roadmap.

48 ‚óæ Information Security Management Handbook
of 2008 beginning of 2009 for the TNC‚Äôs Document Roadmap. Th is was the initial document 
pulled directly from the TCG‚Äôs Interface Metadata Access Point (IF-MAP) specifi cation version 1.0. 
Establishing the initial spec was critical to progressing the initiative but at the same time uptake was 
not signifi cant enough to really make an impression on the ecosystem as a whole. Th at changed in 
2009 as specs were not only modifi ed in some signifi cant ways but existing specs were fi ne tuned 
and empowered with open standards-based abilities that heretofore had been mostly contained 
within the domain of proprietary systems. Th e power of that concept may at fi rst not seem to grab 
you but when examined in the context of how slowly current standards develop as well as what it 
takes for smaller entities to contribute, the potential positive eff ect is amplifi ed considerably. Th at 
being said, it makes sense to see what IF-MAP is in real-world terms, how it and corresponding 
specifi cations can be used, and why it really has a chance to make a signifi cant diff erence.
IF-MAP: In the Beginning
At the most basic of levels, IF-MAP is an open standards-based repository of information about a 
variety of subjects. Specifi cally, it provides a server mechanism that is a real-time enabler of access 
to network devices as well as their state and activities. Th e specifi cation itself initially denoted three 
diff erent capabilities, including IF-MAP Publish, IF-MAP Subscribe, IF-MAP Poll, and IF-MAP 
Search. Th ese capabilities run over the wire, using Simple Object Access Protocol (SOAP), against 
an IF-MAP Server that in turn houses but does not validate information from all devices with 
the ability to publish to it. Th e initial metadata specifi cations defi ned for network devices include 
IP-to-MAC binding, Layer-2 Location, Security Events, Device Attributes, Authentication 
Information, and Access Request Information. Th ey were defi ned and initially published in the 
TNC IF-MAP binding for SOAP specifi cation 1.0 revision 25. Individually they are somewhat 
signifi cant, but when you begin to put them together, they become much more compelling.
As with a TCG standard, at publication it was both open and free to use without a fee or 
requirement for membership in TCG or any other alliances. As previously stated, at release ini-
tial uptake existed in a plug-fest type of demonstration between a handful of vendors. Th e real 
power of IF-MAP though begins to show when you think about it and as adoption becomes 
broader across the entire security landscape. In the past the model involved a great deal of random-
ness related to vendors and their relationships to foster interworkings between various products. 
Essentially throughout history we have had to ‚Äúhope‚Äù vendors supported a certain product or con-
cern ourselves with a great deal of customization in order to extract, import, correlate, decipher, 
and then report on critical issues. While it is possible to pull this off , it is not unlike the entire 
problem facing our industry today, which centers around some type of assumed knowledge that 
simply may or may not exist. Call it the ‚ÄúWeenie Factor‚Äù or whatever you want, but it cripples us. 
Steve Hanna and I have had this conversation on and off  for years and I asked him if it would be 
okay to share some of it. It goes something like a Letterman top ten list:
Are you a security nerd? Here are a few telltale signs:
You have WPA2 enabled on your home wireless
 
‚óæ
You use a radius server for the above because PSK is ‚Äúweak‚Äù
 
‚óæ
Your home backups are encrypted
 
‚óæ
You have the encrypted copies stored in multiple off -site locations
 
‚óæ
Your Web passwords are generated using an algorithm that you invented yourself
 
‚óæ
You SSH back to servers on your home network
 
‚óæ

Achieving a Global Information Systems Transformation (GIST) ‚óæ 49
You have custom rules on your personal fi rewall one of which had DOS‚Äôd you at some point
 
‚óæ
You regularly run more than three diff erent malware detection programs
 
‚óæ
You have or had a certifi cate for your personal e-mail
 
‚óæ
You installed disk encryption on your child‚Äôs laptop
 
‚óæ
If you answered yes to more than one of these items, you are a security nerd. Th ere is nothing 
wrong with that. I am one too and proud of it! At the same time, we must realize that most people 
are not like us. For most people, security does not make them feel good. Security is a necessary 
evil, something that they must deal with to get their job done. It is oftentimes just an evil or a 
block or a failure of the system as a whole. It is not unlike the ‚Äú12:00 syndrome‚Äù with VCR‚Äôs. 
Somewhere we fell off  the bus and now that has worked its way up into the data centers and enter-
prises themselves. Instead of continuing to try and out ‚Äútech‚Äù a problem, we should seek to fi nd 
effi  ciency through supporting interoperability without excuses. In our daily lives it should be our 
job to give normal people the security that they need while remaining as unobtrusive as possible. 
If we could do our jobs perfectly, we would design information systems that could protect them-
selves automatically. Users could spend less time trying to remember long passwords that change 
every month and complex incantations to connect back to the VPN. Th ey could focus on their 
business, using smart cards and biometrics to log in. Th eir security duties would be focused on 
not falling prey to social engineering. Th at is plenty hard enough for most users and yet instead of 
moving that way by opening up the ecosystem, we have gone almost the exact opposite in many 
instances because we have allowed ourselves be cut off  and put into silos by the same folks trying 
to sell us products to solve the problems they tell us we have. Th at seems imbalanced to me and 
that imbalance creates a large portion of the ecosystem issues in which we are currently mired.
Th e current reality we are stuck with in many cases is instead again ‚Äúhoping‚Äù that the vendors 
stay happy with one another and praying that the next patch to whichever system needs it does not 
randomly break something that we have no visibility on inside a black box they created. Instead, 
with IF-MAP you become the pilot and not just the passenger in that deterministically you are 
able to use the power built within the protocol to automatically aggregate and associate informa-
tion in near real time and in many cases real time from a variety of resources. It does not require 
vendors to be friends, it does not require them to like one another, it does not even require them 
to like you or even care you exist. Instead, it relies on their product to perform as it says it is going 
to perform and then prove that performance in a potentially heterogeneous but truly interoperable 
network environment. It gives the smaller vendors, through predefi ned data types and vendor-
specifi c extensions, a target that cannot be moved by the behemoths in order to create lock-in and 
it stimulates innovation by allowing the community to potentially contribute to it‚Äôs own security.
That Second Step Is a Dooooooozy
While most people can agree in principle on something that seems so upfront and sensible, the real 
power is in not only paying lip-service to concepts but in putting rubber to the road. Broad mag-
nanimous concepts make people feel good inside as they wax poetically about how Company C is 
connecting ‚ÄúTh e Human Network‚Äù, while Company A to Z are ‚Äúhear no evil‚Äù and ‚Äúsee no evil‚Äù 
with the third monkey seemingly missing. Luckily, Google found that monkey‚Ä¶at least for now! 
Seriously though, practical implementation of what evidently must be a radical concept to some 
companies, practitioners, and industry pundits maybe needs to be broken down to the simplest of 
examples in order for it to stick. Again, I decided to borrow from Steve Hanna simply because at it‚Äôs 
simplest and least extensive level this is the minimum that IF-MAP brings to the security table.

50 ‚óæ Information Security Management Handbook
Let‚Äôs walk through a simple use case to see the benefi ts of IF-MAP. A user connects to a 
network through a VPN or NAC system, passing through identity checks and endpoint 
health checks. If the user is allowed on the network, the VPN or NAC system uses the 
IF-MAP protocol to store information about the user and their endpoint into the MAP. If 
an IDS later sees an endpoint device attacking someone or sending spam or engaging in 
some other undesirable behavior, the IDS can use the IF-MAP protocol to fi nd informa-
tion about that device that was previously stored in the MAP (such as the identity of the 
device‚Äôs user). Th e IDS can even store an event into the MAP, reporting the bad behavior. 
If the VPN or NAC system has subscribed to notifi cations for such events, the MAP will 
notify the VPN or NAC system of the bad behavior using the IF-MAP protocol‚Ä¶1
So now, we see in Figure 3.2 a simple implementation that has a great deal of value in an IF-MAP 
based world. Capability-wise it goes up exponentially from there based on the power of the archi-
tecture to embrace products through nearly open-ended extensibility and interworking. One of 
the most signifi cant aspects of the design is that from the simple diagram above, incredibly exten-
sive maps can be developed as needs arise. Th is is made possible in part by the implementation of 
a true MAP instead of just a rehash of an extensible directory. Th e noticeable diff erences include 
heteroarchical versus hierarchical design, a true search capability without a separate catalog or 
registration server, and a much more scalable dynamic read/write capability that can be updated 
from nearly anywhere. Th ose of you familiar with X.500 and its much more popular child LDAP 
know the challenges associated with hierarchical directories once they reach a certain size and that 
issue coupled with the vast potentially chaotic looking mapping that could very well be required 
in each network/internetwork called for the heteroarchical approach.
Complex Graphs May Emerge in MAP
Figure 3.3, while clearly representing just how capable the IF-MAP protocol is related to any 
number of decision support criteria, makes my head hurt. It serves the dual purpose of also mak-
ing me thankful for those that work in our fi eld that enjoy that sort of thing. Keep in mind that 
while we are concentrating on the security aspect here, the controlled chaos of IF-MAP‚Äôs model 
Network
access
requestor
Network
access
enforcer
Network
access
authority
TNC
server
AAA Server
Edge switch/
access Ô¨Årewall/
VPN gateway
Supplicant/
VPN client, etc.
TNC
client
Network
access 
layer
Integrity
evaluation
layer
Integrity
measurement
layer
Integrity
measurement
veriÔ¨Åers
IF-M
IF-MAP
IF-MAP
Flow
controllers
Sensors
Others
IDS, interior
Ô¨Årewalls, etc
IF-MAP
IF-MAP
Metadata
access point
IF-MAP
IF-IMC
IF-IMV
IF-TNCCS
IF-PEP
IF-T
AR
PEP
PDP
MAP
MAP
server
MAPC
Integrity
measurement
collectors
Figure 3.2 TNC architecture with IF-MAP integrated‚Äîkeeping it simple.

Achieving a Global Information Systems Transformation (GIST) ‚óæ 51
would fi t nearly any type of data required in organizations and will probably be considered for 
those purposes at some point in the future (Figure 3.3).
Mike Fratto wrote
IF-MAP provides a standardized framework for network and security devices to 
publish device state data‚Äîsuch as IP address, authentication, or virtually any mean-
ingful information‚Äîto a central repository that can be used by other applications. 
Th is repository can be used for security, asset management, discovery, or any other 
purpose.2
Stuart Bailey, the CTO and Founder of Infoblox synthesized it best:
MAP is like a MySpace or Facebook for enterprise infrastructure security pieces that 
each component publishes and subscribes to.3
Bailey said:
Th is is a community of security infrastructure devices where each device can allow its 
circle to know what it sees on the network, and share information.3
Mike‚Äôs description of the ‚Äúpotential promise‚Äù of IF-MAP especially coupled with Bailey‚Äôs analogy 
should hit home for practitioners because each goes to the heart of what our profession is about 
in many instances, context. Everything has to be considered in its own context and IF-MAP fi ts 
access-request-mac
mac-
address=
00:11:22:3
3:44:55
ip-mac
ip-mac
identity =
john.smith
role = finance
and employee
authenticated-as
authenticated-by
layer2-information
VLAN = 978
port = 12
ip-address
=
192.0.2.55
ip-address
=
192.0.2.221
device-attribute=
av-signatures-out-
of-date
access-request-
device
device=
111:55
ip-address
=192.0.2.7
access-
request=
111:33
ip-address
=
192.0.2.60
Figure 3.3 Complex diagrams may emerge using IFMAP.

52 ‚óæ Information Security Management Handbook
that way of looking at things. It has the ability to transcend a simple database to be a true open 
standards-based repository of all types of data that in turn will create a potential economy of scale 
that should lower costs and raise skill levels across the board.
Evolution of ‚ÄúThreat State Databases‚Äù: SETI for Security 
Coming to a Computer Near You
Deployment of protections coupled with correlation of the information coming back from our 
increasingly complicated environments is pretty much make or break going forward. In Chapter 1 
‚ÄúEnhanced Security through Open Standards: A path to a Stranger Global Digital Ecosystems,‚Äù 
I mentioned and defi ned the fl exible ‚ÄúEndpoints and Flowpoints‚Äù concept in a fairly generic way. 
Th is type of fl ex defense concept can adapt as you go to attempt to combat the increasingly intense 
evolution of threat and attack vectors in the next few years. Th e Mississippi River analogy still 
holds here as the sheer size and the overwhelming fl ood of information combine to overwhelm 
most if not all typical castle and moat type defenses. You cannot dam the Mississippi River but 
you can dam some tributaries while you watch the Mississippi and that type of thinking needs to 
be heavily considered for future deployments in networks. Single go/no go calls on endpoints as 
they come in to the network are nearly worthless in this new environment and various pieces of 
information need to be collected and then combined and assessed at speeds that we do not even 
understand yet. Again while this has existed in some form for years, the changes possible when 
enabled via an agile scalable framework like IF-MAP are very powerful. By sharing this informa-
tion freely amongst various devices from all of the companies in your network, activity can be 
correlated with users to identify behavior that are anomalous eventually allowing the entity to 
decide the best course of action and then to enable it to take that action. Watching this diverse 
information stream with concepts from the past would have been nearly impossible based on both 
false positives as well as simply the inability of many of the practitioners in the fi eld to fully grasp 
the nuances of the required skill sets for sometimes incredibly intricate manipulation of the inter-
faces necessary to be successful. I remember having this conversation with several of my friends 
and colleagues and we always came back to the question of why IF-MAP was necessary and why 
it mattered so much. I would bring up threats and use cases and they would detail the various 
things they had to approximately do the same thing. We would venture into harder core concepts 
and they would detail the sometimes fairly complicated data extracts and imports, the array of 
custom fi lters, the various pieces of command and control within their networks most of the time 
from the same company, and invariably we would broach the amount of care they had to take to 
make sure anything they bought fi t this model. After this exercise with several friends, it became 
painfully clear to most of them that while they could do this it could not be and should not be an 
assumed skill set or even valid method of operation for our profession going forward. It simply 
did not scale.
Mike Fratto wrote:
At the center of an IF-MAP-enabled system is the IF-MAP server, which stores state 
information. As a device changes over time, records are updated. Note that this is not 
a historical database‚Äîstate data is only as current as the last update. Other network 
devices, like a NAC policy decision point, then query the IF-MAP server to determine 
if a host, say, successfully authenticated and completed a DHCP exchange, before 
allowing it to communicate on the network.2

Achieving a Global Information Systems Transformation (GIST) ‚óæ 53
IF-MAP uses a data model based on associating identifi ers logically. For example, 
an IP address, a MAC address, and a user name are identifi ers that, when linked, 
bind together to classify a host. Metadata is then used to describe these links. Th e 
relationship between a user name and an access request might have multiple metadata 
attached, including the user‚Äôs role and the user name provided to authenticate.
Th is data is searchable by IF-MAP clients. Identifi ers may be linked, facilitating 
searches so an IF-MAP client can discover how a particular device authenticated to 
the network, which user name was entered, and what IP addresses are bound to a given 
MAC address. Th is is a huge benefi t‚Äîtoday there‚Äôs no standardized way to, say, ask a 
Radius server which clients have authenticated, or query a DHCP server as to which 
leases have been given out.
As we move to drastically increase the visibility points throughout, that scalability becomes critical. 
As the next iterations of NBAD evolve we need to be able to truly innovate in the development of 
our strategies in order to enable the network to trigger responses to stimuli that begin to capture a 
lot more information at the fi rst hint of trouble. Th e increased participation of both traditional and 
nontraditional endpoints in the security environment around them will allow, through IF-MAP, for 
the profession to potentially graduate to a ‚ÄúTh reat State Database‚Äù as one manifestation. IF-MAP is 
a cornerstone of this type of concept but is only one of the tools in the box. As things like Common 
Uniform Driver Architecture evolve, projects like SETI can be imitated with the goal of thousands 
of individual clients participating in a grid working on the distributed task of a more secure ecosys-
tem. Open source clients that serve various functions would all be IF-MAP enabled and could then 
possibly fi ll dual roles of both subscribing to information to strengthen their overall posture as well 
as publishing what they see around them. Eff ectively they each become individual sensors in their 
own right. Th is in turn allows for a weighting of input and potentially a furtherance of the vetting of 
information going into the repositories. Th is is a distributed peer review of sorts and while the input 
would have to be vetted to ascertain trust levels, it could be another indicator stored in the MAP that 
gives an additional clue as to the state of the network based more-so on what the individual clients 
see around and less on what they tell the network about themselves. Th is weighting of input could be 
adjusted as warranted and would potentially yield an important piece of the equation. Th e concept 
of ‚ÄúFederated Security‚Äù is not really new but the achievability of it has certainly been a bit of a pipe 
dream for some time. A truly comprehensive picture of not only your enterprise but of the portions 
of partner enterprises that matter to you is a potential force magnifi er for defense. Real-time infor-
mation fl owing to an open standards-based repository allows you the ability to quickly focus eff orts 
and resources potentially before an area becomes a concern. Th is on the fl y tuning would simply not 
have been even conceivable in heterogeneous networks a few years ago and yet with the IF-MAP and 
TCG‚Äôs newest TNC specifi cations, it seems like we are fi nally at the cusp of a true OODA or Boyd‚Äôs 
Loop interactive decision cycle based on timely feedback to each action and decision.
It Is Full of Stars‚Ä¶.
Last year I wrote:
Basically in this environment, we are watching the sampled data at diff erent points in 
the system and as heuristics continue to evolve, we can decide whether or not to focus 
the more intensive capabilities of the forensic aspect of that network on our trouble spots. 
With the proliferation of headless attack vectors like printers, iPods, and iPhones there has 
to be a way of distributing the load of the visibility so that we have early warning indicators 

54 ‚óæ Information Security Management Handbook
before things overwhelm the core. It has been proven that core defense simply does not 
work because of the drastic increase in bandwidth coming from the edge, the huge liabil-
ity located in the endpoint including what data it sees and what it does with that data, and 
new valid ‚Äúmalware-like‚Äù software that serves valid business purposes while fraying the 
nerves of the security team. As we continue to move toward the evolution of more transient 
and distributed network security supplicants on these clients, we need to concern ourselves 
with network design that allows for the inclusion of this data in real time so that when it 
does get here we do not have to rip and replace yet again. Buying switches that are sFlow 
capable should be on the agenda while paying attention to both standards adherence in 
the past as well as roadmap postures for the future again comes to the forefront. Paying 
attention to how a router supports NetFlow and whether or not the company is really par-
ticipating in the standards associated with communications in general should become a 
main criterion for our discussions. Recognizing how we are going to put that information 
to use and what we need to do in order to further the evolution of the industry needs to 
become a prime consideration if we are every to get ahead of the curve.4
IF-MAP paved the way, establishing a conceptual ‚Äúbeachhead‚Äù and TCG wasted no time by fol-
lowing up this year with the release of three additional potentially powerful specifi cations as well 
as updates to three already in place. Th ese are late additions to this chapter and cannot be explored 
fully right now since they are not in full production. Having said that, the potential for them when 
looked at as a group is staggering. First out of the gate is the IF-T Binding to TLS 1.0 (IF-TTLS). 
Th is is potentially the little engine that could as it relates to enablement of communications across 
the broad spectrum of IP networks. Th e sexiest part about this specifi cation is that it does not 
require 802.1x so will function on any IP Network to enable health monitoring via intelligence 
embedded on a client (preinstalled or installed on the fl y) that can initiate its own communication 
with a service menu through a new mechanism available through a modifi cation of the IF-TNCCS 
specifi cation that updated the XML-based session protocol. Feature wise, the IF-T Binding to TLS 
enables checking of a machine already on an IP network versus one not yet attached. It also allows 
very verbose assessments of endpoints due to the fact that a TLS connection generally provides 
larger bandwidth than tunneled EAP as well as support for many round trips as in the case when 
practitioners use TPM-based attestation in addition to a TNC assessment. Another benefi t of 
using IF-T is that both the TNC-compliant client and TNC-compliant server are able to initiate 
an assessment when either believes an event has occurred that warrants an updated assessment.
Th e strength of IF-TTLS would be enough of a revision in 1 year if it stood by itself but 
TCG went further in attempting to help our profession do its job. First off  to support IF-TTLS 
as well as anything else new that comes out in a distributed form, they released Federated TNC 
1.0 (FTNC). Now that any IP addressable machine with a client can be assessed, they needed a 
standards-based way to transfer that information between security domains. TCG chose SAML 
assertions as the method to transfer TNC results between security domains creating three new 
SAML profi les. Th e three profi les were Roaming Assessment Profi le, Web Assessment Profi le, and 
FTNC Attribute Profi le. Although only in a version 1.0, the benefi ts to users and practitioners 
comes through both improved endpoint assessment capabilities that in turn increases overall orga-
nizational security. At the same time, higher trust at host organizations could possibly result into 
a more substantial menu of available services. Potentially single sign-on is possible and if not that 
then at least simplifi ed sign-on. You are no longer treated as an outsider to other host organiza-
tions once you have been vetted in this manner and so effi  ciency has strong potential to increase. 
Potentially overlooked in the signifi cantly increased visibility from both the host and home site 

Achieving a Global Information Systems Transformation (GIST) ‚óæ 55
is the potential to reuse and, therefore, potentially extract further value from existing SAML and 
TNC-related technologies. Th is goes back directly to the community wanting to participate in 
its own safety and survival. Reuse gives you more time to innovate, which in turn gives you and 
others better code to reuse, continuing to cycle.
Finally, the third new specifi cation sounds boring on the surface until you realize just how criti-
cal profi les are to mitigation of threats from headless, and in this case clientless is defi ned as ‚Äúwithout 
a head,‚Äù attack vectors throughout the network. Previously much of this was done in nonstandard 
ways and hence was hard to replicate and to secure from one organization to the next. Clientless 
Endpoint Support Profi le 1.0 (CESP) attempts to standardize how all endpoints that attach to a 
network without clients will be treated on a class-by-class basis. At its most basic, it describes what 
PEPs and PDPs should do to support clientless endpoints. Th e benefi ts are substantial with clientless 
endpoints like printers, VoIP phones, etc., being allowed to interoperate while maintaining a much 
improved security posture through the use of MAC-AUTH and correlation of data. Th ese profi les 
can be transported and reused and while currently there is no community creation capability, that 
too is a goal in order to really raise the value proposition of participation.
With the initial IF-MAP standard, TCG seemed to be staying pretty much to itself with attempts 
to branch out in order to push standards in certain areas. With the release of these three new speci-
fi cations and the adjustments to the older ones, TCG has shown that it is here to make a real dif-
ference. Th e goal is evidently ‚ÄúTNC Everywhere‚Äù and so to that eff ect they no longer consider 
themselves limited to just WAN and LAN. Th ey now support standards-based roaming through 
FTNC while stepping up as the fi rst standards-based profi le enabler for any and all devices that run 
on today‚Äôs networks. Slowly but surely they are moving from the foundation up the stack and at each 
year the standards have gotten increasingly relevant to the digital ecosystem as a whole.
With everything you have read so far, this is a no brainer right? Only if consumers of the 
technology take an active role and push forward without shying away or giving up and letting the 
vendors continue to lead us. We need to get to that 50/50 partnership sooner than later.
Standards-Based Information Sharing: Enhancing 
Participation and Agility
Lead, follow, or‚Ä¶ok wait just lead or follow!
But follow who? Are we supposed to continue to let it occur as it has in the past number of years within 
the industry or are we supposed to really reclaim some type of control of our own destiny. I believe 
we are tired of being in silos that keep us from communicating and, therefore, cooperating with one 
another to the benefi t of those that would like to sell us a product to solve a problem we may not have 
but they thought we would 18 months ago. Did you follow that? We know that coordinated security 
allows better automation, more room to respond, better resiliency, and better overall results in general. 
In the past, intentionally, the lack of communication allowed us to be separated from one another and 
eff ectively kept in silos. With those odds, it was no wonder we consistently lost our way and fell back 
into the old patterns. Blogs and other social media outlets are changing that aspect, breaking down the 
silos, and possibly enabling the next thrust forward from a capability standpoint for open standards:
In November of 2008, Chris Hoff  wrote a set of blog posts over a couple of days initially based 
on the security state, or lack thereof, of Cloud Computing. He wrote
I described the need for a new security model, methodology and set of technologies 
in the virtualized and cloud computing realms built to deal with the dynamic and 
distributed nature of evolving computing:

56 ‚óæ Information Security Management Handbook
Th is basically means that we should distribute the sampling, detection and preven-
tion functions across the entire networked ecosystem, not just to dedicated security 
appliances; each of the end nodes should communicate using a standard signaling and 
telemetry protocol so that common threat, vulnerability and eff ective disposition can 
be communicated up and downstream to one another and one or more management 
facilities.5
Th e second post came a couple days later after Hoff  had been pointed to IF-MAP by a current 
TCG member and IF-MAP supporter. Th is time he writes
IF-MAP is a standardized real-time publish/subscribe/search mechanism which utilizes 
a client/server, XML-based SOAP protocol to provide information about network 
security objects and events including their state and activity:
IF-MAP extends the TNC architecture to support standardized, dynamic data inter-
change among a wide variety of networking and security components, enabling customers 
to implement multi-vendor systems that provide coordinated defense-in-depth.
Today‚Äôs security systems‚Äîsuch as fi rewalls, intrusion detection and prevention 
systems, endpoint security systems, data leak protection systems, etc.‚Äîoperate as 
‚Äúsilos‚Äù with little or no ability to ‚Äúsee‚Äù what other systems are seeing or to share their 
understanding of network and device behavior.
Th is limits their ability to support coordinated defense-in-depth. In addition, cur-
rent NAC solutions are focused mainly on controlling network access, and lack the 
ability to respond in real-time to post-admission changes in security posture or to pro-
vide visibility and access control enforcement for unmanaged endpoints. By extending 
TNC with IF-MAP, the TCG is providing a standard-based means to address these 
issues and thereby enable more powerful, fl exible, open network security systems.
While the TNC was initially designed to support NAC solutions, extending the 
capabilities to any security product to subscribe to a common telemetry and informa-
tion exchange/integration protocol is a fantastic idea.
I‚Äôm really interested in how many vendors outside of the NAC space are 
including IF-MAP in their roadmaps. While IF-MAP has potential in conven-
tional non-virtualized infrastructure, I see a tremendous need for it in our move to 
Infrastructure 2.0
Integrating, for example, IF-MAP with VM-Introspection capabilities (in VMsafe, 
XenAccess, etc.) would be fantastic as you could tie the control planes of the hypervi-
sors, management infrastructure, and provisioning/governance engines with that of 
security and compliance in near-time with virtualization and Cloud Computing.5
If he sees it then what is the deal? Chris is a smart guy and at every turn when I describe the concept 
to people, I at least get a positive response of some sort whether people agree that it is the right 
way to go or not. Th e positive responses with no real follow-through confounded me until I found 
another post by Steve Hanna that described the exact phenomenon that we see each and every 
time something gets traction.
Steve writes
Chris asks which vendors are supporting IF-MAP in their products. I have found that 
standards adoption follows the classic innovation adoption lifecycle. Innovators are the 

Achieving a Global Information Systems Transformation (GIST) ‚óæ 57
vendors and customers that have the vision and foresight to see where things must go. 
Th ey are the fi rst to create and adopt new technology. For IF-MAP, that group includes 
the folks who developed the IF-MAP spec and demonstrated implementations at Interop 
Vegas in April: ArcSight, Aruba Networks, Infoblox, Juniper Networks, Lumeta, and 
nSolutions. Next come Early Adopters, Early Majority, Late Majority, and Laggards. It 
takes at least a year for each stage: six months to turn prototypes into products and six 
months for the next generation of adopters to catch on. Th at‚Äôs the timescale we‚Äôve seen 
for the other TNC standards. So I expect to see Innovator vendors shipping products 
that implement IF-MAP in the next few months and Innovator customers deploying 
those products in the months after that. Th en will come Early Adopters and so on.1
I thought the graph he provided potentially unintentionally highlighted the real issue that has 
existed for many years. Th e graphic he provided is the standard picture of a typical innovation 
adoption life cycle. Th e challenge is that in technology we have a skewed graphic because probably 
close to half of the early adopters oftentimes are de facto locked in before they make that leap to 
early majority (Figure 3.4).
Th at is what we simply cannot allow to happen at this point in time. I will not go back over 
the challenges we face. I will not rehash the fact that we are standing at the edge of a digital abyss 
or revisit the fact that we are a consumer-driven economy whose actual main drivers are the most 
at risk‚Ä¶errmm‚Ä¶ Anyway, what I will say is that the single best way to increase the number of 
products that support IF-MAP is for customers to demand and then buy those products. We need 
to reward both the vendors who are innovators and who have used their resources to try and partici-
pate on a fi eld they know will be level to all of those wishing to uphold the standard. We need to be 
supportive of the community aspect around which local centers of excellence can grow and not only 
‚ÄúCounter-Valence‚Äù
Downward pressure
exerted by large or
incumbent vendors
through various means
amplifies risk aversion
inherent in human nature
‚ÄúEdge‚Äù
Early majority/
pragmatists and
doers
‚ÄúWe Support the
Standards... kinda‚Äù
Suddenly large
incumbent vendors say
they want to play fair?
Trust a T-rex?
‚ÄúGroundhog Day‚Äù
Biz opportunity ‚Äúplayed 
out‚Äù and standards on 
‚Äúlife support‚Äù, move on to
the next target market
‚ÄúOff Edge‚Äù
Laggards and resisters
‚ÄúTrailing Edge‚Äù
Late majority/
followers
‚ÄúDefacto Smackdown‚Äù
T-rex incumbent
vendors let ‚Äúfriends‚Äù play
by ‚Äútheir‚Äù rules, smaller
vendors try to co-exist
on the scraps to survive
‚ÄúLeading Edge‚Äù
Progressives and
visionaries
Time
The
chasm of fear
‚ÄúBleeding
Edge‚Äù
Innovators,
enthusiasts and
risk takers
Market growth
Figure 3.4 Innovation adoption life cycle. (Adapted from Moore, G., Crossing the Chasm: 
Marketing and Selling High-Tech products to Maintain Customers, Harper Business, New York, 
1991; and Hanna, S., The adoptive curve for IF-MAP, Got the NAC, November 11, 2008.)

58 ‚óæ Information Security Management Handbook
support, but actively pursue interactions across all of the lines that have been used as silo makers 
for us in the past. We are not Government, Health, and Education (GHE) or Small and Medium 
Business (SMB) or Large Enterprise, or Legal, or any of the titles vendors attach to us in order to 
better order their lives. What we are instead is one profession attempting to bridge a gap that we 
helped create. In order to even have a shot at doing that though, we need to fi nd the things that we 
can make a utility and execute on them. We need to be able to rely on the community around us to 
assist so that each of us does not always continue to invent the wheel over and over again. We need 
a fulcrum in order to generate the leverage necessary to make a diff erence in what we do.
Mike Fratto wrote
Whether IF-MAP achieves broad adoption depends on whether vendors actively develop 
integration points. IF-MAP has all the features necessary to aggregate host informa-
tion in a standardized format, including enabling vendors to add their own attributes. 
Th e potential for integration is, simply, enormous. (Mike Fratto, InformationWeek)
I simply cannot say it any better than Mike does above. To that end, many of those quoted here 
and involved within the TCG, open group, the press, non-profi ts, and various government orga-
nizations have begun working toward a more active and hopefully successful role in the evolution 
of the standards process going forward. Currently we have to approach it from all sides. Customer 
advocacy, contract language, advisory councils, blogs, articles, information community based 
assistance, sites like opengroup.org, demandstandards.org, and truststc.org all have to be sup-
ported by our profession. It is through that coordination that we can really drive the point home 
and potentially snap us forward and over the early adopter‚Äôs chasm. None of this is easy and with 
that in mind we should not make it harder than it already is by holding on to our sacred cows, 
old concerns, and fears that ubiquity of skill sets will make us less desirable to our organizations. 
Th e security profession has to move forward and drag some folks with us because otherwise the 
repercussions have the potential to be incredibly severe.
About the Author
David O‚ÄôBerry, CSSLP, CISSP-ISSAP, ISSMP, MCNE, CNE-I, and CSPM, is director of 
information technology systems and services for the South Carolina Department of Probation, 
Parole and Pardon Services (SCDPPPS), Columbia, South Carolina.
References
 
1. Hanna, S. Th e Adoption Curve for IF-MAP, Got the NAC, November 11, 2008. http://forums.juniper.
net/t5/user/viewprofi lepage/userid/2969;jsessionid=124D8B42617DFA28E80BFC B18FF53E91.
 
2. Kerner, S. M. NAC 2.0 takes shape under networking giants, Internetnews.com, April 28, 2008. 
http://www.internetnews.com/infra/article.php/3743346.
 
3. O‚ÄôBerry, D. Enhanced security through open standards: A path to a stronger global digital ecosys-
tem, Information Security Management Handbook, 6th edn., vol. 2, H. F. Tipton and M. Krause (eds.). 
Auerbach, New York, 2009.
 
4. Fratto, M. Tech road map: IF-MAP protocol, InformationWeek, July 7, 2008.
 
5. Hoff , C. I can haz TCG IF-MAP in your security product, please‚Ä¶, Rational Survivability Blog, 
November 10, 2008. http://www.rationalsurvivability.com/blog/?p=78.

59
4
Chapter 
A Primer on Demystifying 
U.S. Government Networks*
Samuel Chun
Introduction: 9-11 Changes Everything Including Government IT
One of the immediate impacts of the events following 9-11 for government information infra-
structures was the passage of the Intelligence Reform and Terrorism Prevention Act (IRTPA) 
of 2004. Former President George W. Bush‚Äôs signature on December 17, 2004 on the IRTPA 
signaled a radical change in direction for intelligence eff orts by encouraging and fostering inter-
agency cooperation. One of the most interesting aspects of IRTPA was that it clearly attempted 
* Th is chapter does not contain any classifi ed information about government networks or government network 
operations. All information in this chapter is freely available to the public.
Contents
Introduction: 9-11 Changes Everything Including Government IT ............................................59
Diff erences and Similarities ....................................................................................................... 60
Classifi ed versus Unclassifi ed versus Controlled but Unclassifi ed versus Declassifi ed 
Explained .............................................................................................................................. 62
Classifi ed Information ...................................................................................................... 63
Th e Clearance Process: Vetting the Individual .................................................................. 64
Agency Independence ...................................................................................................... 66
Commonly Used Government Networks: Th e Alphabet Soup ....................................................67
Bell‚ÄìLa Padula Still Rules ..................................................................................................... 68
Cross Domain Solutions: Across Organizational Boundaries ............................................ 69
Multilevel Security: Across Classifi cation Levels ............................................................... 70
Th e Future: Information-Sharing Environment = CDS + MLS ........................................ 70
About the Author ...................................................................................................................... 71

60 ‚óæ Information Security Management Handbook
to address the problems found by the 9-11 Commission in information sharing by codifying what 
was thought to be a solution into law.
Section 1016(a) (1) (A) of the IRTPA (Information Sharing) mandates that the President shall
 (A) Create an information sharing environment for sharing of terrorism information in a manner 
consistent with national security and with applicable legal standards relating to privacy and civil 
liberties;
 (B) designate the organizational and management structures that will manage the ISE; and
 (C) determine and enforce the policies, directives, and rules that will govern the content and usage 
of the ISE.
Further, in Section (E), IRTPA mandates that the newly created and appointed Program 
Manager for the Information-Sharing environment (PM-ISE) of DNI must employ an approach 
that emphasizes control over access to data, in addition to networks and systems that contain it, 
without compromising security. Section (E) also states that the ISE must facilitate the sharing of 
information within and across all security levels. To summarize simply, the President is mandated 
to create an environment where information (including those that are classifi ed) can be shared eas-
ily to all appropriate stakeholders without compromising security. In short, the law dictates that 
the U.S. government agencies changed from an ‚Äúattitude of need to know‚Äù to a ‚Äúresponsibility to 
share‚Äù with enabling technologies to support this monumental change.
While it is easy to state on legislation that ‚Äúeveryone must share more,‚Äù it is an enormous 
challenge to federal agencies to actually accomplish as nearly a half century of mandates and laws 
have created a richly complex environment that contains the most sensitive (including those that 
could result in grievous damage to the U.S. nation-state) in ‚Äústove-piped‚Äù enclaves with controls 
and cultures intended to inhibit the sharing of information. Even with the passage of the IRPTA 
in 2004, government networks, especially those that serve predominantly classifi ed missions such 
as defense and intelligence, are riddled with the results of insular practices and inadequate infor-
mation-sharing technologies of the past. For example, it is very common to have ‚Äúair gapped‚Äù 
or ‚Äúphysically separated‚Äù networks that require users to have multiple PCs that are connected 
to entirely separate networks infrastructures (which include switches, servers, software, identi-
ties, etc.). Consequently, the PM-ISE reported in 2007 in his PM-ISE Enterprise Architecture 
Framework document that the current lack of capabilities in sharing and securing of information 
‚Äúcauses proliferation of assets, ranging from multiple desktop machines for end-users to multiple 
server racks and associated networking equipment in back-offi  ce server rooms. Th is trend will 
continue for the foreseeable future.‚Äù
Differences and Similarities
While it is easy to assume that U.S. federal government information and communications 
infrastructures are identical to commercial networks in approach and technologies, the fact 
of the matter is that while it may use similar or even the same technologies (Microsoft, Cisco, 
and HP are some of the biggest federal government technology vendors) there are diff erences 
that newcomers to federal government agency IT departments struggle with when exposed 
to U.S. government networks. One of the primary goals of this chapter is to be a primer for 
those that are interested in how the U.S. federal government information infrastructures were 

A Primer on Demystifying U.S. Government Networks ‚óæ 61
created and some of the challenges associated in moving into a more sharing environment. 
Th is chapter is by no means the defi nitive treatise in how government information infrastructures 
operate but it is intended to give a newcomer or someone interested in joining government IT 
departments a topical overview.
Presented below is a general summary of the relative diff erences between commercial and 
federal government networks.
Technologies: Th e technologies deployed by federal agencies diff er widely, but since the 1980s, they 
have gone generally toward a more COTS route in providing IT for the agency users. Ubiquitous 
technology vendors such as HP, Microsoft, Cisco, Dell, Oracle, and EMC make hundreds of 
millions in business with the federal government under Federal Acquisition Regulations (FAR) 
and Defense Federal Acquisition Regulations (DFAR) each year. In addition, there are numerous 
purpose built and proprietary systems and applications in place to support specifi c missions. 
In addition, large-scale legacy systems based on older programmatic languages such as COBOL 
still exist servicing millions of citizens each year. By and large, there are numerous eff orts across 
the federal government to modernize based on customization of commercial products.
Processes and regulations: Federal agencies tend to be more policy and process driven than com-
mercial organizations. Th is is mainly due to the myriad of regulations and directives that can 
apply to IT infrastructures. For example, compliance with the Federal Information Security 
Management Act of 2002 is a rigorous exercise that each government agency expends considerable 
eff ort in adhering to. In addition, mandates from organizations such as Offi  ce of Management 
and Budget (OMB) and department-specifi c policies generated by agencies such as DISA (Defense 
Information Systems Agency) exert a lot of processes and policies that need to be complied with 
by agency IT departments.
People: Th e practitioners and technical resources is where commercial and public sector share 
common ground. Th e technical expertise required in working in a federal government environ-
ment is not that dissimilar since the technologies tend to be ubiquitous. As a matter of fact, U.S. 
Department of Defense is the only federal government organization that imposes a certifi cation 
requirement for security practitioners under the 8570.01 mandate. All of the 8570.01 certifi cations 
required for compliance to 8570 are commercially based (CISSP, SSCP, etc.) However, there is a 
level of vetting that does not exist in commercial environments since the risks involved can be 
extremely high. Th is type of vetting is usually reserved for access to the most sensitive of information‚Äî
classifi ed information.
Nature of risk: While the missions of the hundreds of federal agencies vary, there are a few where 
the risks involved far exceed that of any commercial interests. Th ere are organizations in government 
whose sole mission is the defense of the state. Th e risks facing these organizations by inappropriate 
disclosure, lack of system resiliency, or data integrity can be catastrophic and can result in grievous 
bodily harm, loss of life, or damage to the security of the nation. It is not surprising that there 
are strictly controlled environments that are closed off  from public and even private access without 
the ‚Äúneed to know.‚Äù Th ese environments, while small in number, do carry extraordinary risks, and 
the cultures of the organizations that use these environments consequently tend to be more risk 
averse. Th ese classifi ed networks, while they use classic commercially available software and hard-
ware, are strictly controlled and closed from public scrutiny. Th ese networks, due to the manner 
in which they were architected and those that are authorized to work within and with it, present 
some of the most challenging barriers to a more agile, sharing information environment.

62 ‚óæ Information Security Management Handbook
Classifi ed versus Unclassifi ed versus Controlled 
but Unclassifi ed versus Declassifi ed Explained
One of the fi rst questions that will be asked when joining a government contracting company 
is ‚Äúdo you have a security clearance?‚Äù A candidate‚Äôs answer will have enormous impact on the 
programs and projects that the practitioner will have access to when working in federal govern-
ment. While most of the federal government enterprise works in an unclassifi ed fashion (although 
 ‚Äúoffi  cial use only‚Äù policies certainly do apply) using fairly well-known technologies, having a secu-
rity clearance qualifi es you to potentially work on projects on behalf of government programs at 
that person‚Äôs clearance level and below.
Before getting into the particulars of the various levels of how a program (and its information) 
can be classifi ed, it is important to describe how the access to sensitive, classifi ed informa-
tion is handled in the United States. It is important to fi rst understand some basic defi nitions 
involved.
Classifi ed information: Classifi ed information refers to information that has been categorized and 
restricted in access to individuals or groups that has the appropriate security clearances (defi ned 
later) and requisite ‚Äúneed to know‚Äù to the information. Th ere are three levels of classifi cation in 
the United States: Confi dential, Secret, and Top Secret. Executive Order (EO) 1329 signed by 
President George W. Bush in March of 2003 is the governing presidential order that regulates 
classifi ed information. EO 1329 is an amendment to EO 12958, which in itself is an amendment 
to a long line of presidential orders dating back to President Dwight Eisenhower‚Äôs order from 
the 1950s.
Unclassifi ed information: Unclassifi ed information refers to information that has not been classifi ed. 
Th at does not imply that all unclassifi ed information is releasable. See the section on Controlled 
Unclassifi ed Information.
Declassifi ed/declassifi cation: Declassifi ed information is information that has had their security 
classifi cation removed. Until the introduction of the concept of ‚Äúautomatic declassifi cation‚Äù by 
EO 12958, classifi ed information was presumed to be of permanent value to the federal govern-
ment. EO 12958 basically introduced a regular ‚Äútime limit‚Äù of 25 years in which information that 
were legally transferred by agencies and the executive offi  ce to the National Archives would be 
presumed declassifi ed unless the agency that originated them acted to exempt them in compliance 
with the provisions of EO 12958. Th is had a tremendous impact on declassifying information 
as 1.33 billion pages have been declassifi ed according to the FY2006 Report to the President, 
Information Security Oversight Offi  ce.
Controlled unclassifi ed information (CUI ): In May of 2008, the Offi  ce of the President issued 
a memorandum to the heads of the executive departments and agencies with the subject of 
‚ÄúDesignation and Sharing of Controlled Unclassifi ed Information.‚Äù Th e intent of this memo-
randum was to replace the Sensitive But Unclassifi ed (SBU) designation with a framework that 
is intended to be more rigorous. In order for the information to be considered CUI, it must 
(a) not meet the standards for National Security Classifi cation under EO 12958, as amended; 
(b) be pertinent to the national interests of the United States or to the important interests of 
entities outside the Federal Government; and (c) under law or policy require protection from 
unauthorized disclosure, special handling safeguards, or prescribed limits on exchange or 
dissemination.

A Primer on Demystifying U.S. Government Networks ‚óæ 63
Figure 4.1 pictorially represents the four terms:
Classifi ed Information
As mentioned earlier, classifi ed information is generally referred to as ‚Äúclassifi ed‚Äù; however, they 
are actually categorized further into three levels of sensitivity: Confi dential, Secret, and Top Secret. 
EO 13292 dictates that ‚ÄúExcept as otherwise provided by statute, no other terms shall be used to 
identify United States classifi ed information.‚Äù Th at means that according to EO 13292 there are 
no further ways to classify information than Confi dential, Secret, and Top Secret.
One of the important things to note is that there are standards that are used to determine if 
information should be classifi ed. Not every bit of information used by government needs to be 
classifi ed in some manner. Th e following statements are excerpted directly from EO 13292 on the 
standards that are to be used to classify information. Only if all of the following conditions are met 
the information may be originally classifi ed:
 
1. ‚ÄúAn original classifi cation authority is classifying the information.‚Äù
 
2. ‚ÄúTh e information is owned by, produced by or for, or is under the control of the United 
States Government.‚Äù
 
3. ‚ÄúTh e information falls within one or more of the categories of information listed. Information 
shall not be considered for classifi cation unless it concerns:
 
(a) military plans, weapons systems, or operations;
 
(b) foreign government information;
 
(c) 
intelligence activities (including special activities), intelligence sources or methods, or 
cryptology;
 
(d) foreign relations or foreign activities of the United States, including confi dential sources;
 
(e) 
scientifi c, technological, or economic matters relating to the national security, which 
includes defense against transnational terrorism;
 
(f) United States Government programs for safeguarding nuclear materials or facilities;
Unclassified
Controlled unclassified
Classified
Classified
Unclassified
Declassification
Figure 4.1 Classifi cation versus unclassifi ed overview.

64 ‚óæ Information Security Management Handbook
 
(g) vulnerabilities or capabilities of systems, installations, infrastructures, projects, plans, 
or protection services relating to the national security, which includes defense against 
transnational terrorism;
 
(h) weapons of mass destruction.‚Äù
 
4. ‚ÄúTh e original classifi cation authority determines that the unauthorized disclosure of the 
information reasonably could be expected to result in damage to the national security, which 
includes defense against transnational terrorism, and the original classifi cation authority is 
able to identify or describe the damage.‚Äù
Confi dential versus Secret versus Top Secret versus 
Top Secret Compartmentalized Explained
Once information is deemed to meet the standards as described under the governing EO for clas-
sifi cation, the level of sensitivity or classifi cation is determined by the classifi cation authority based 
on a set of standards. Th e standards are based on the potential impact to national security if an 
inappropriate disclosure were to occur. Th ey are written as
Top Secret: ‚Äúshall be applied to information, the unauthorized disclosure of which reasonably 
could be expected to cause exceptionally grave damage to the national security that the original 
classifi cation authority is able to identify or describe.‚Äù
Secret: ‚Äúshall be applied to information, the unauthorized disclosure of which reasonably could be 
expected to cause serious damage to the national security that the original classifi cation authority 
is able to identify or describe.‚Äù
Confi dential: ‚Äúshall be applied to information, the unauthorized disclosure of which reasonably 
could be expected to cause damage to the national security that the original classifi cation author-
ity is able to identify or describe.‚Äù
Top Secret level information (and programs) can be so sensitive and potentially impactful 
to national security that it can have additional requirements placed on the people that access it 
through the clearance process. For example, Sensitive Compartmented Information (SCI) requires 
that access be granted only to that compartment so that others that are not part of that compart-
ment do not have access to the classifi ed information regardless of their security clearance. Each 
compartment may impose additional requirements on the individuals that need access with a 
common one being a polygraph (full or lifestyle only). A diagram of the classifi cation levels is 
presented in Figure 4.2.
The Clearance Process: Vetting the Individual
While granting access to individuals to unclassifi ed information by agencies is straightforward, 
granting access to classifi ed information is actually complex. Th e security clearance process is the 
method used to determine an individual‚Äôs trustworthiness and reliability before adjudicating them 
to be fi t to receive access to classifi ed information. While both government organizations and 
commercial companies that do business with government agencies can request a security clearance 
for an individual, only the government can grant a security clearance.
Clearances are granted (or adjudicated by an agency adjudicative authority) after a back-
ground investigation is performed by either the Offi  ce of Personnel Management (OPM) or the 

A Primer on Demystifying U.S. Government Networks ‚óæ 65
Defense Security Service (DSS). Depending on the clearance level requested, investigations can 
involve computerized checks of fi nancial records and criminal history all the way to personal 
interviews of neighbors and individual polygraphs. Clearances, predictably, are mostly requested 
and granted at three levels: Confi dential, Secret, and Top Secret.
Although security clearances ‚Äúroughly‚Äù map to classifi cation levels of Confi dential, Secret, and 
Top Secret, individual agencies often choose to only recognize clearances adjudicated from their 
own agencies (which is actually technically against policy) or issue their own versions. For exam-
ple, the U.S. Department of Energy grants ‚ÄúL‚Äù or ‚ÄúQ‚Äù clearances that roughly map to ‚ÄúSecret‚Äù and 
‚ÄúTop Secret‚Äù levels; however, they are not generally portable to other agencies. Another notable 
example is that workers that work in or for the U.S. Department of Treasury or Securities and 
Exchange Commission can receive a clearance called ‚ÄúPublic Trust‚Äù that does not map to any of 
the National Security Clearances and is not portable for access to classifi ed information anywhere 
due to its less rigorous background investigations used for adjudication.
Seems OK‚Ä¶ What Are the Problems?
While the approach and processes for classifying information and clearing individuals seem reason-
able, anyone involved in federal government information technology will readily admit that there 
are defi nite challenges with the system. One of the biggest issues in the past has been the lengthy 
time required for background investigations by OPM and DSS and adjudicators to grant clear-
ances. While much progress has been made in recent years to shorten the wait time for applicants, 
Unclassified
Unclassified
Confidential
Secret
Top
Secret
Controlled
unclassified
Classified
Classified
Declassification
Figure 4.2 Classifi ed information overview.

66 ‚óæ Information Security Management Handbook
it could still take months or even years for individuals to receive full clearances at the highest levels. 
OPM and DSS along with agencies are still working at the writing of this chapter in shortening the 
average wait times; however, the demand for cleared IT security professionals far exceeds supply 
and are one of the most sought after practitioners in the federal government IT support industry.
Agency Independence
One of the most interesting aspects of U.S. federal government IT is the level of independence 
exercised by agencies. Whether it is due to the diverse missions of the organizations or decades of 
independence, agency CIO‚Äôs have remarkable power to shape their IT infrastructures. Even within 
Cabinet level departments such as Defense, Justice, Treasury and Homeland Security there are 
a myriad of component agencies who operate semiautonomously. While overarching regulations 
such as FISMA and OMB mandates (such as OMB-13 on the federal desktop core confi guration 
or more commonly referred to as FDCC) are complied with, other technology investments or pro-
grams are generally funded led by the individual agency themselves and generally not their cabi-
net level CIO‚Äôs. Looking across a large cabinet level department or the entire federal enterprise is 
similar to looking at a jigsaw puzzle: the shapes of pieces may look diff erent but they are intended 
to work together for a common mission‚Äîcitizen service and safety.
Consequently, the IT infrastructure of a specifi c agency may look remarkably diff erent than 
another even under the same Cabinet level secretary. Consequently, there are technological issues 
in interoperability and sharing information. As agencies have evolved their IT infrastructures inde-
pendently, there have been widely varying approaches to such vendors, technologies, and services 
as directory/authentication, e-mail, Web services, and storage, and even security. While there have 
been recent eff orts to have a more standardized approach to IT with initiatives such as the Federal 
Enterprise Architecture (FEA) and FDCC, much work remains on a coordinated eff ort for IT.
Why Do I Have 5 PCs and Log In 5 Times Every AM?
Compounding the issue of a loosely coordinated IT strategy across the federal government is that 
agencies that are involved in maintaining national security have to have networks that are designed 
to protect classifi ed information. Th ese networks are hierarchical depending on the level of sensitiv-
ity or classifi cation. An agency, depending on their mission, may have all levels or a subset or none. 
Figure 4.3 represents three hypothetical agencies and the potential networks they may contain.
Internet
Agency A
Agency B
Agency C
Internet
Unclassified intranet
Unclassified intranet
Internet
Unclassified intranet
Confidential
network
Secret
network
Secret
network
Top
Secret
Figure 4.3 Differences between agencies.

A Primer on Demystifying U.S. Government Networks ‚óæ 67
In many civilian agencies, the number of networks is less of a problem. However, in environments 
where all levels of classifi ed information are necessary to fulfi ll a mission (mainly in intelligence, 
homeland security, and national defense) the networking issues become complex. For example, 
due to the nature of the classifi ed networks, each tend to be ‚Äúair gapped‚Äù (physically separated 
from others) and ‚Äúclosed‚Äù (data fl ows between levels are extremely tightly controlled based on 
Mandatory Access Control policies). End users in these environments have multiple PCs in their 
work areas with separate connections to each of the networks and have multiple IDs and resources 
to work within the classifi cation levels. It is not unusual for analysts at some agencies to have more 
than 5 PCs connected to a switch box to access multiple classifi ed and unclassifi ed networks. 
Even applications that run on these networks are not exempt from the requirement for separa-
tion. For example, even the most successful of information-sharing programs such as the Central 
Intelligence Agency‚Äôs Intellipedia runs separately at each classifi cation level with varying content 
at each level.
Commonly Used Government Networks: The Alphabet Soup
One of the most challenging aspects of working in the federal government IT is its use of acro-
nyms. Th e following presents some of the most commonly referred to federal government net-
works and their associated acronyms:
NIPRNet (commonly referred to as ‚Äúnipper‚Äù): Non-Secure Internet Protocol Router Network 
is a network used primarily by the U.S. Department of Defense to exchange unclassifi ed 
information. It is predominantly serviced by the .mil domain and is similar in use as other 
civilian agency internal networks.
SIPRNet (commonly referred to as ‚Äúsipper‚Äù): Secret Internet Protocol Router Network is 
an IP network used predominantly by the Department of Defense and the members of the 
Intelligence Community to transfer classifi ed information up to the Secret level. It is physi-
cally separated from NIPRNet due to its use to transfer classifi ed information. SPIRNet is 
serviced by the .smil and .sgov domains.
JWICS (commonly referred to as ‚Äújaywicks‚Äù): Joint Worldwide Intelligence Communications 
System is a network used predominantly by the members of the Intelligence Community 
and the Department of Defense to transfer information up to Top Secret (and compart-
mented) level.
HSDN: Homeland Secure Data Network announced in 2004 to be a replacement for various 
Departmental WANs. It will operate at the Top Secret level with plans to support at TS/
SCI level.
INTELINK: Web-based intranet information-sharing service used in the intelligence commu-
nity. Th ere are multiple versions at each classifi cation level including an unclassifi ed version 
called INTELINK-U.
CENTRIXS (commonly referred to as ‚Äúcentricks‚Äù): Combined Enterprise Regional 
Information Exchange System is a collection of classifi ed networks used by the U.S. 
Department of Defense and Coalition allies for exchanging information in support of joint/
coalition military operations. It is considered the de facto infrastructure for multinational 
information sharing. Th ere are many versions of CENTRIXS with each having a widely rang-
ing assortment of coalition allies for a common mission. For example, CENTRIXS-MCFI 

68 ‚óæ Information Security Management Handbook
(Multination Coalition Forces Iraq) is used by Global War on Terror (GWOT) allies in 
Iraq while the CENTRISXS-GCTF (Global Counter-Terrorism Forces) network services 
the U.S. military and allied partners for GWOT in a separate network. One of the long-
term challenges that CENTRIXS will need to address is the creation of multiple physically 
separated networks that may introduce redundancy and enclaves that may not be able to 
communicate with each other (lack of cross domain access).
TFIN (commonly referred to as ‚Äúteefi n‚Äù): Treasury Foreign Intelligence Network is a net-
work operating at the Top Secure/SCI level in support of the Treasury Department‚Äôs Offi  ce 
of Terrorism and Financial Intelligence with the ‚Äútwin aims of safeguarding the fi nancial 
system against illicit use and combating rogue nations, terrorist facilitators, weapons of mass 
destruction (WMD) proliferators, money launderers, drug kingpins, and other national 
security threats.‚Äù
Bell‚ÄìLa Padula Still Rules!
Th e Bell‚ÄìLa Padula Model (BPM) has been the foundation of military and intelligence access 
control for decades. Since the early 1970s, classifi ed networks have been built based on Bell‚ÄìLa 
Padula‚Äôs approach to security‚Äîno read up and no write down. BPM forbids users at a certain clas-
sifi cation level from reading content at a higher classifi cation level (e.g., users in Secret networks 
cannot read/access in Top Secret networks) and downgrading information (e.g., users in Secret 
networks cannot create information in Unclassifi ed networks). Conversely, users in lower classifi -
cation levels can create or send information to higher classifi cation levels (users in Secret networks 
can transfer fi les to Top Secret networks). Th e Bell‚ÄìLa Padula Model in eff ect creates a one-way 
information fl ow eff ect with information that is easy to send up but almost impossible to send 
down, as shown in Figure 4.4:
It is easy for users at lower classifi cation networks to e-mail and transfer fi les to higher level 
networks while information transfer down to lower classifi cation level must be done through a 
more rigorous (i.e., manual and slower) process. An electrical engineering analogy of a diode is 
Confidential
User
Secret
Near one-way information flow
Top
Secret
Figure 4.4 Bell‚ÄìLa Padula and classifi ed information fl ow.

A Primer on Demystifying U.S. Government Networks ‚óæ 69
often used to describe the guard technologies that protect information fl ow between classifi cation 
levels as diodes only allow an electrical charge to fl ow in one direction.
With the global and time-sensitive nature of homeland security and national defense, it is not 
diffi  cult to imagine how this type of access control mechanism can inhibit urgent information 
sharing. For example, intelligence gathered by a defense organization overseas can take days to 
reach state and local law enforcement offi  cials as the information must not only go through the 
appropriate vetting process but it must also traverse the various classifi cation levels up and down 
(with delays occurring on the down side) to an unclassifi ed state and local agency (e.g., local EMS), 
as shown in Figure 4.5:
While these examples and diagrams grossly oversimplify the challenges, agility in informa-
tion sharing is an identifi ed issue within and between agencies. Th is is one of the reasons for the 
PM-ISE clause in the IRTPA of 2004 and the creation of the Offi  ce of the Director of National 
Intelligence. Cultural and political issues aside, there are real technological problems with the ease 
and speed in which critical information can be shared.
Cross Domain Solutions: Across Organizational Boundaries
Th ere are two technologies that are extremely sought after in government to facilitate informa-
tion sharing. Th e fi rst is a cross domain solution (CDS). CDSs are generally intended to address 
the problem of being able to share information with ease (no disruptive technologies for end 
users) and confi dence (without fear of inappropriate disclosure) across agency and even national 
boundaries. With unclassifi ed, releasable information, the document can simply be e-mailed or 
put on a Web site for download. However, classifi ed information is an entirely diff erent matter 
since access must be restricted to specifi c individuals at specifi c periods, requiring auditing and 
logging of access in addition to restrictions on derivative content and in appropriate dissemina-
tion. Technologically, this is a complex issue as agencies themselves have widely diff ering informa-
tion infrastructure, much less separate nations. In addition, classifi ed networks tend to be closed, 
so rather than trying to create a secure information-sharing infrastructure, it is currently much 
Unclassified
Confidential
Secret
Urgent
information
Declassification
Top
Secret
Figure 4.5 Challenges of information sharing through classifi cation levels.

70 ‚óæ Information Security Management Handbook
easier, technologically, to create completely separate networks for joint mission delivery (e.g., 
coalition war fi ghting, multinational law enforcement, and public‚Äìprivate sector partnerships). 
Consequently, cross domain technologies are one of the most sought after solutions in the public 
sector (Figure 4.6).
Multilevel Security: Across Classifi cation Levels
Th e challenges associated with working across multiple classifi cation levels was described earlier. 
Perhaps it is due to the small niche market or the complex technologies required or even the risk 
associated with developing a solution for this market, but industry has not been entirely success-
ful in off ering a series of robust solutions for the problem to government agencies. Th ere have 
been notable successes such as the NetTop program with the NSA where a combination of VM 
technologies, Linux-based hosts, Microsoft Windows, and commercially manufactured PCs were 
integrated to perform across multiple security levels (MLS). However, successes have been rare as 
only a handful of providers have been able to achieve true MLS and have the solution be accred-
ited for production implementation. While there continues to be innovation and research aimed 
at true MLS at diff erent commercial companies, progress will likely continue to be slow compared 
to research and development aimed at producing products for a much broader market. Th e simpli-
fi ed diagram below presents hypothetical MLS solutions where users and PCs are connected either 
through multiple physical connections to the various classifi cation levels from a single PC or rout-
ing/switching/VPN technologies are used to provide a single connection from a PC to multiple 
levels. In addition, secure information fl ow both up and down classifi cation levels would also be 
ideal (Figure 4.7).
The Future: Information-Sharing Environment = CDS + MLS
While it is diffi  cult to envision any near-term solution to the MLS and CDS problems that cur-
rently exist, almost everyone uniformly agrees that a truly agile information-sharing environment, 
where a cleared user with the appropriate need to know on a single desktop can create, send, and 
Internet
Agency A
Allied partner B
Unclassified intranet
Confidential
network
Top
Secret
Secret
network A
Secure cross
domain
Secret
network B
Unclassified intranet
Internet
Information
sharing
Figure 4.6 Cross domain information sharing.

A Primer on Demystifying U.S. Government Networks ‚óæ 71
receive classifi ed information across classifi cation levels across domains, needs to be the future. 
A logical representation of such an environment where Cross Domain and Multilevel Security 
capabilities are combined is presented in Figure 4.8:
In this environment, information fl ows to the appropriate authorized users across jurisdic-
tional boundaries (United States to allied nations) and across classifi ed networks to ensure that 
the right individual has access to the information as quickly as possible. Th is is an antithesis to 
the ‚Äústove-piped‚Äù networks where information is easily enclaved due to the fear of inappropriate 
disclosure and lack of enabling technologies. While an environment such as this is envisioned and 
much eff ort is being expended to create it, it would be interesting to revisit the progress made by 
industry and government in the years to come.
About the Author
Samuel Chun, CISSP, is the director of the Cyber Security Practice for HP Enterprise Services, 
Plano, Texas, U.S. Public Sector.
Confidential
Unclassified
Secure information flow
User 2
User 1
Secret
Top
Secret
Figure 4.7 Multilevel access.
Internet
Agency A
User A
Allied partner B
Allied user B
Unclassified intranet
Internet
Unclassified intranet
Confidential
network
Secret
network A
Secure information flow
Secret
network B
Top
Secret
Figure 4.8 Cross domain and multilevel combined.


Network Attacks and 
Countermeasures


75
5
Chapter 
Antispam: Bayesian Filtering
Georges J. Jahchan
Spamming is defi ned as the abuse of electronic messaging to indiscriminately send unsolicited 
bulk messages. Its most recognized form is e-mail spam, with IM, Usenet newsgroups, Web search 
engines, blogs, wiki, online classifi ed ads, mobile phone messaging, Internet forum, and junk fax 
transmissions being the other spam spread vectors.
Come to think of it, spam is an inexpensive method of advertising; it costs little more than the 
maintenance of mailing lists and that of the equipment and Internet connectivity to send spam 
e-mails. In other words, any individual with access to a computer that is connected to the Internet 
is a potential spammer. Spam has grown to alarming proportions, with the intent of spammers 
reaching far beyond advertising into cybercrime territory with pharming, phishing, botnets, root-
kits, and other malicious uses, all of which can potentially have serious consequences for the victims 
(identity theft and/or fi nancial loss).
Th e spam problem has grown to the point where even small businesses cannot aff ord to ‚Äúlook 
the other way.‚Äù Businesses of all sizes consider e-mail a mission-critical application. It is relied 
upon for internal and external business communications. Management expects e-mail to be highly 
available, accessible when and where needed, and secure. To that end, measures to eff ectively 
manage spam are a business imperative.
Contents
Bayesian Spam Filtering Process .................................................................................................76
How Bayes‚Äô Th eorem Is Utilized by Spam Filters ....................................................................... 77
Paul Graham ............................................................................................................................. 78
SpamProbe for MacOS .............................................................................................................. 79
Annoyance Filter ....................................................................................................................... 80
SpamAssassin .............................................................................................................................81
Conclusion ................................................................................................................................ 83
About the Author ...................................................................................................................... 84
Reference................................................................................................................................... 84

76 ‚óæ Information Security Management Handbook
Information Week, in its February 3, 2005 edition estimated the global cost of spam back in 
2005 at $21.58 billion annually.* Ferris Research estimated the cost of spam in 2007 at a stagger-
ing $100 billion.‚Ä†
Spam does not advertise itself as such, quite the contrary, spammers try to disguise the e-mails 
to encourage or entice recipients to read them. Early spam detection methods relied on keyword 
searches in the title, headers, or body of the message. Reliability was low and maintenance cost was 
high. Spammers quickly learned to outsmart keyword searches.
Additional controls were devised‚Ä°: whitelists, blacklists, and Bayesian fi lters. Whitelisting only 
delivers messages from e-mails or domains that have been preapproved by the recipient. Blacklists 
automatically fi lter e-mails based on identifi ed individual e-mails, domains, or source server IP 
addresses, blocking known off enders. Bayesian fi ltering uses complex statistical techniques to clas-
sify e-mails based on content from past e-mails. A content-dependent score is assigned to every 
e-mail. E-mails exceeding a predefi ned score threshold are tagged as spam.
In this chapter, we analyze in detail how generic Bayesian spam fi lters work, probing into vari-
ous methods of applying Bayes‚Äô theorem. A handful of Bayesian fi ltering implementations (that 
have been reported as particularly eff ective¬ß) Paul Graham‚Äôs, SpamProbe for MacOS, CRM114, 
SpamAssassin (SA), and Annoyance Filter¬∂ will be examined in detail.
Bayesian Spam Filtering Process
A Bayesian spam e-mail fi lter relies on the likelihood or probability of encountering certain words 
in spam e-mails that one is unlikely to fi nd in legitimate e-mails. Th ese words are not known in 
advance, and the Bayesian spam fi lter must be trained to recognize them. Initial training is per-
formed by analyzing at least 200 known spam and 200 ham (known legitimate) e-mails. During 
training, the fi lter will adjust the probabilities that each word will appear in spam or in legitimate 
e-mails in its database. Th ese word probabilities contribute to the calculation of a spam score 
(a numeric or a percentage value) to the e-mail, based on the weight and frequency of occurrence of 
certain words in the e-mail. Th e contribution is called posterior probability. Posterior probability is 
computed using Bayes‚Äô theorem.
Th e probability that an e-mail is spam is then computed for all words in the e-mail. If that 
total exceeds a preset value, the e-mail is tagged as spam (for example, ***Spam*** may be appended 
to the beginning of the subject line). Client-side fi lters look for these tags in the subject line and 
automatically move tagged e-mails to the junk folder without further processing.
Some implementations of Bayesian fi lters can optionally be confi gured to act earlier in the 
e-mail chain by ‚Äúblacklisting‚Äù certain e-mails. As soon as suffi  cient information is received to 
identify an e-mail as blacklisted, the fi lter resets and closes the SMTP connection, thereby saving 
bandwidth that would otherwise be wasted in receiving spam e-mails. Th e disadvantage is that 
unlike tagging, the recipients are neither aware of such e-mails which do not reach their mail-
boxes, nor are they notifi ed of their existence.
* http://www.informationweek.com/news/security/vulnerabilities/showArticle.jhtml?articleID=59300834
‚Ä† http://www.ferris.com/research-library/industry-statistics/
‚Ä° http://articles.directorym.net/Th e_Fight_Against_Spam_San_Jose_CA-r906929-San_Jose_CA.html
¬ß http://www.linux.com/articles/32353
¬∂ http://www.fourmilab.ch/annoyance-fi lter/

Antispam: Bayesian Filtering ‚óæ 77
How Bayes‚Äô Theorem Is Utilized by Spam Filters
Bayes‚Äô Th eorem is used to
 
1. Compute the probability that a message is spam, based on the presence of a given word in 
the message
 
2. Compute the probability that the message is spam on the basis of all of its words (or a subset 
thereof)
 
3. Identify the message as blacklisted on the basis of its header (or part thereof) and/or part of 
its body
 
4. Analyze rare words
Th e formula that the software uses to determine spam is derived from Bayes‚Äô theorem reproduced below:
(
)
(
)
(
)
(
)
|
( )
|
|
( )
|
(
)
‚ãÖ
=
‚ãÖ
+
‚ãÖ
Pr W S
Pr S
Pr S W
Pr W S
Pr S
Pr W H
Pr H
where
Pr (S|W ) is the probability that a message is spam, knowing that the banned word is in it
Pr (S) is the overall probability that any given message is spam
Pr (W|S) is the probability that a keyword appears in spam messages
Pr (H) is the overall probability than any given message is not spam (is ‚Äúham‚Äù)
Pr (W|H ) is the probability that the keyword appears in ham messages
Plugging in numerical values to illustrate the theorem, assuming a spam keyword appears in 60% 
of spam and in 10% of ham, and that 55% of all e-mails are spam, the probability that a message 
with that keyword in it is spam is
or
0.6 0.55
0.88
88%
0.6 0.55
0.1 0.45
‚àó
=
‚àó
+
‚àó
Th e example above provides a use case for a single keyword. In a typical e-mail, multiple keywords 
can be found, each with its own set of values. Th e overall potential of an e-mail being spam is the 
combination of the individual computed keyword probabilities.
Th ere are several methods to combine the individual keyword probabilities. Th e ‚Äúnaive‚Äù 
method assumes that the occurrences of the individual keywords are independent:
1
2
1
2
(1
1) (1
2) (1
)
p
p
pN
p
p
p
pN
p
p
pN
‚ãÖ
=
‚ãÖ
+
‚àí
‚ãÖ
‚àí
‚ãÖ
‚àí
‚Ä¶
‚Ä¶
where
p is the probability that the message is spam
p1 is the probability p(S|W1) that it is a spam knowing it contains a fi rst keyword
p2 is the probability p(S|W2) that it is a spam knowing it contains a second keyword
pN is the probability p(S|WN) that it is a spam knowing it contains an Nth keyword
Th e result p is compared to a preset threshold; if it exceeds that threshold, the e-mail is most likely 
spam, and is tagged accordingly. In the English language, the ‚Äúnaive‚Äù assumption is, however, 
incorrect, as the probability of fi nding an adjective is aff ected by the probability of having a noun.

78 ‚óæ Information Security Management Handbook
Another method used in some Bayesian spam fi lters is to ignore all keywords whose spamic-
ity (see below) is close to 0.5 as they add little value to the decision. Th e words that are evaluated 
are those whose spamicity is close to zero (found mostly in ham) or close to 1.0 (found mostly in 
spam). A variation thereof is to only retain the words with the top/bottom N spamicity values on 
either side of the scale.
Marcov Random Fields (MRF) [1] is yet another method of zeroing in on spam. Implemented 
with tweaks and hacks in CRM114, it considers all relations between neighboring words to mat-
ter, for neighborhoods with variable window size (for example, up to 3, 4, 5, 6 words). A technical 
discussion of MRF is beyond the scope of this document.
Paul Graham
Paul Graham in ‚ÄúA Plan for Spam‚Äù* suggests an improved algorithm he later describes in Better 
Bayesian Filtering.‚Ä† Paul starts with a relatively large set of 4000 spam and 4000 ham e-mails. He 
then scans the entire text including headers and embedded html and javascript, of each message. 
He considers alphanumeric characters, dashes, apostrophes, and dollar signs to be part of tokens, 
and everything else to be a token separator. He counts the number of times each token (currently 
ignoring case) occurs in each set. At this stage, he ends up with two large hash tables, one for each 
set, mapping tokens to the number of occurrences. Next, he creates a third hash table, this time 
mapping each token to the probability that an e-mail containing it is a spam, which he calculates 
as follows (in Lisp):
(let
 
(
 
 
(g (* 2 (or (gethash word good) 0) ) )
 
 
(b (or (gethash word bad) 0) )
 
)
 
(unless
 
 
(< (+ g b) 5)
 
 
(max .01
 
 
 
   (min .99
 
 
 
 
  (float
 
 
 
 
 
(/
 
 
 
 
 
 
(min 1 (/ b nbad) )
 
 
 
 
 
 
(+
 
 
 
 
 
 
 
  (min 1 (/ g ngood) )
 
 
 
 
 
 
 
  (min 1 (/ b nbad) ) )
 
 
 
 
 
 
)
 
 
 
 
 
)
 
 
 
 
)
 
 
 
)
 
 
)
 
)
* http://www.paulgraham.com/spam.html
‚Ä† http://www.paulgraham.com/better.html

Antispam: Bayesian Filtering ‚óæ 79
where word is the token whose probability is being calculated, good and bad are the hash tables 
created in the fi rst step, and ngood & nbad are the number of non-spam and spam messages, 
respectively. In order to reduce false positives, the number in good is doubled. Th e number of 
e-mails in each set is used instead of their combined length in calculating spam probabilities.
When a new mail arrives, it is scanned into tokens, and the most interesting 15 tokens, where 
interesting is measured by how far their spam probability is from a neutral 0.5, are used to calcu-
late the probability that the mail is spam. If probs is a list of the 15 individual probabilities, the 
combined probability is calculated as (in Lisp):
(let
 
( (prod (apply #‚Ä≤* probs) ) )
 
(/ prod (+ prod (apply #‚Ä≤* (mapcar #‚Ä≤(lambda (x) (- 1 x) ) probs) ) ) )
)
By trial and error, Paul found that words not in the hash table worked best when assigned a prob-
ability of 0.4 as it is more likely to be a clean word.
In other methods, words that the Bayesian fi lter is encountering for the fi rst time (or which 
were not encountered in a suffi  cient number of e-mails during the training phase) present a special 
challenge. Applying the standard Bayes theorem may lead to null numerator and denominator in 
the case of a fi rst encounter. In the formula below, it is assumed that the classifi cation between 
ham and spam for the given keyword is a random variable with beta distribution*:
(
)
( )
( |
)
|
‚ãÖ
+
‚ãÖ
‚Ä≤
=
+
s
n
s
n
Pr S
Pr S W
Pr S W
where
Pr‚Ä≤(S|W ) is the corrected probability for the message to be spam, knowing that it contains a 
given keyword
s is the strength we give to background information about incoming spam
Pr(S) is the probability of any incoming message to be spam
n is the number of occurrences of this word during the learning phase
Pr(S|W ) is the spamicity of this word, which is defi ned as
(
)
(
| )
|
(
| )
(
|
)
=
+
Pr W S
Pr S W
Pr W S
Pr W H
SpamProbe for MacOS
Brian Burton, the author of SpamProbe for MacOS based his implementation on Paul‚Äôs ideas with 
some tweaks and changes to improve the algorithm‚Äôs eff ectiveness. Th e SpamProbe tokenizer under-
stands HTML entity references as well as Base64 and Quoted Printable URL encoding. It allows 
a small set of characters to appear within terms (‚Äú.‚Äù, ‚Äú,‚Äù, ‚Äú + ‚Äù, ‚Äú-‚Äù, ‚Äú_‚Äù, ‚Äú$‚Äù) and treats all others as 
white space. Pure numbers are ignored but numbers linked by punctuation are retained. Tokens 
* http://en.wikipedia.org/wiki/Beta_distribution

80 ‚óæ Information Security Management Handbook
containing punctuation are further broken into sub-terms by repeatedly separating the leading por-
tion of the term, a technique used to extract the domain name from fully qualifi ed host names in 
URLs. SpamProbe uses Paul‚Äôs technique of classifying some terms based on where they were found. 
Specifi cally, particular terms from the subject, to, and cc headers are specially fl agged.
By default, SpamProbe scores word pairs (phrases) as terms. Longer phrases generally provide 
an increase in accuracy, but with diminishing returns and at a signifi cant cost in disk space. While 
Paul‚Äôs fi lter only allows each term to appear in the array once, SpamProbe allows terms to appear 
more than once and uses larger arrays, by default 27 terms which repeat twice.
Annoyance Filter
Annoyance fi lter implements an adaptive Bayesian fi lter, which distinguishes junk mail from 
legitimate mail by scanning archives of each and calculating the probability for each word which 
appears a statistically signifi cant number of times in the body of text that the word will appear 
in junk mail. Larger mail archives lead to more eff ective results (upward of 99% eff ectiveness has 
been reported). Annoyance fi lter is command-line driven and highly tunable, which makes it 
diffi  cult to use for most users.
Th e excerpt below is the section of the annoyance fi lter manual that deals with Bayesian fi lter 
tuning/parameters:
‚Äìbiasmail n
Th e frequency of words appearing in legitimate mail is infl ated by the fl oating point factor n, 
which defaults to 2. Th is biases the classifi cation of messages in favor of ‚Äúfalse negatives‚Äù‚Äîjunk 
mail deemed legitimate, while reducing the probability of ‚Äúfalse positives‚Äù (legitimate mail errone-
ously classifi ed as junk, which is bad). Th e higher the setting of ‚Äìbiasmail, the greater the bias in 
favor of false negatives will be.
‚Äìbinword n
Binary character streams (for example, attachments of application-specifi c fi les, including the execut-
able code of worm and virus attachments) are scanned and contiguous sequences of alphanumeric 
ASCII characters n characters or longer are added to the list of words in the message. Th e dollar sign 
(‚Äú$‚Äù) is considered an alphanumeric character for these purposes, and words may have embedded 
hyphens and apostrophes, but may not begin or end with those characters. If ‚Äìbinword is set to zero, 
scanning of binary attachments is disabled entirely. Th e default setting is fi ve characters.
‚Äìnewword n
Th e probability that a word seen in mail that does not appear in the dictionary (or appeared too few 
times to assign it a probability with acceptable confi dence) is indicative of junk is set to n. Th e default 
is 0.2‚Äîthe odds are that novel words are more likely to appear in legitimate mail than in junk.
‚Äìphraselimit n
Limit the length of phrases assembled according to the ‚Äìphrasemin and ‚Äìphrasemax options to n charac-
ters. Th is permits ignoring ‚Äúphrases‚Äù consisting of gibberish from mail headers and undecoded content. 
In most cases, these items will be discarded by a ‚Äìprune in any case, but skipping them as they are 
generated keeps the dictionary from bloating in the fi rst place. Th e default value is 48 characters.
‚Äìphrasemin n
Calculate probabilities of phrases consisting of a minimum of n words. Th e default of 1 calculates 
probabilities for single words.

Antispam: Bayesian Filtering ‚óæ 81
‚Äìphrasemax n
Calculate probabilities of phrases consisting of a maximum of n words. The default of 1 
calculates probabilities for single words. If you set this too large, the dictionary may grow 
to an absurd size.
‚Äìprune
After loading the dictionary from ‚Äìmail and ‚Äìjunk folders, this option discards words which 
appear suffi  ciently infrequently that their probability cannot be reliably estimated. One usually 
‚Äìprunes the dictionary before using ‚Äìwrite to save it for subsequent runs.
‚Äìsigwords n
Th e probability that a message is junk will be computed based on the individual probabilities of 
the n words with extremal probabilities, that is, probabilities most indicative of junk or mail. Th e 
default is 15, but there is no obvious optimal setting for this parameter; it depends in part on the 
average length of messages you receive.
‚Äìsloppyheaders
To evade fi ltering programs, some junk mail is sent with MIME part headers which violate the 
standard but which most mail clients accept anyway. Th is option causes such messages to be 
parsed as a browser would, at the cost of standards compliance. If ‚Äìsloppyheaders is used, it should 
be specifi ed both when building the dictionary and when testing messages.
‚Äìthreshjunk n
Set the threshold for classifying a message as junk to the fl oating point probability value n. Th e default 
threshold is 0.9; messages scored above ‚Äìthreshjunk are deemed junk.
‚Äìthreshmail n
Set the threshold for classifying a message as legitimate mail to the fl oating point probability value 
n. Th e default threshold is 0.9, with messages scored below ‚Äìthreshmail deemed legitimate. Note that 
you may leave a gap between the ‚Äìthreshmail and ‚Äìthreshjunk values (although it makes no sense to set 
‚Äìthreshmail higher). Mail scored between the two thresholds will then be judged of uncertain status.
SpamAssassin*
SpamAssassin (hereinafter referred to as SA) is an open source and widely used in all aspects of 
e-mail management. You can readily fi nd SA in use in both e-mail clients and servers, on many 
diff erent operating systems, fi ltering incoming as well as outgoing e-mail, and implementing a 
very broad range of policy actions. SA also forms the basis for numerous commercial antispam 
products available on the market today.
SA is a set of Perl programs that utilizes combined scores from various checks to determine if 
a given message is spam. SA checks are based on Perl regular expressions.
Th e Bayesian classifi er in SA tries to identify spam by looking at what are called tokens: words or 
short character sequences that are commonly found in spam or ham. If I have handed 100 messages 
to SA, learn that they have the phrase ‚Äúreplica watches,‚Äù and am told it that those are all spam, then 
when the 101st message comes in with the words ‚Äúreplica‚Äù and ‚Äúwatches,‚Äù the Bayesian classifi er will 
be pretty sure that the new message is spam and will increase the spam score of that message.
* http://wiki.apache.org/spamassassin/SpamAssassin

82 ‚óæ Information Security Management Handbook
While other Bayesian fi lters require a large number of ham and spam messages to get started, SA 
can do with as little as 200 ham and 200 spam messages. In addition, custom rules can be created 
to address particular user needs, for example, rules targeting pill spams or foreign languages.
A sample diet pill spam set of rules by Matt Kettler* is reproduced below for reference:
#diet
body __ DRUGS _ DIET _ PHEN /\bphentermine\b/i
#phentermine
body __ DRUGS _ DIET1 
/(?:\b|\s)[ _ \W]{0,3}p[ _ \W]{0,3}h[ _ \W]{0,3}[e3\xE8- 
\xEB][ _ \W]{0,3}n[ _ \W]{0,3}t[ _ \W]{0,3}[e3\xE8-
\xEB][ _ \W]{0,3}r[ _ \W]{0,3}m[ _ \W]{0,3}[i1!|l\xEC-\xEF][ _ \W]{0,3}n[ _ \W]{0,3}[e3\xE8-
\xEB][ _ \W]{0,3}(?:\b|\s)/i
#ionamin
body __ DRUGS _ DIET2 
/(?:\b|\s)_ {0,3}[i1!|l\xEC-\xEF][ _ \W]?o[ _ \W]?n[ _ \W]?[a4\xE0-
\xE6@][ _ \W]?m[ _ \W]?[i1!|l\xEC-\xEF][ _ \W]?n _ {0,3}\b/i
#bontril
body __ DRUGS _ DIET3 /\bbontril\b/i
#phendimetrazine
body __ DRUGS _ DIET4 /\bphendimetrazine\b/i
#diethylpropion, generic of Tenuate, uncommon in spam
body __ DRUGS _ DIET5 /\bdiethylpropion\b/i
#Meridia
body __ DRUGS _ DIET6 /(?:\b|\s)[ _ \W]{0,3}M[ _ \W]{0,3}[e3\xE8-
\xEB][ _ \W]{0,3}r[ _ \W]{0,3}[i1!|l\xEC-\xEF][ _ \W]{0,3}d[ _ \W]{0,3}[i1!|l\xEC-
\xEF][ _ \W]{0,3}[a4\xE0-\xE6@][ _ \W]{0,3}(?:\b|\s)/i
#tenuate
body __ DRUGS _ DIET7 /\b _ {0,3}t[ _ \W]?[e3\xE8-
\xEB][ _ \W]?n[ _ \W]?u[ _ \W]?a[ _ \W]?t[ _ \W]?[e3\xE8-\xEB] _ {0,3}(?:\b|\s)/i
#didrex
body __ DRUGS _ DIET8 /\b _ {0,3}d[ _ \W]?[i1!|l\xEC-
\xEF][ _ \W]?d[ _ \W]?r[ _ \W][e3\xE8-\xEB[ _ \W]?xx? _ {0,3}\b/i
#adipex
body __ DRUGS _ DIET9 /\b _ {0,3}a[ _ \W]?d[ _ \W]?[i1!|l\xEC-
\xEF][ _ \W]?p[ _ \W]?[e3\xE8-\xEB][ _ \W]?x _ {0,3}\b/i
#xenical
body __ DRUGS _ DIET10 /\b _ {0,3}x?x[ _ \W]?[e3\xE8-\xEB][ _ \W]?n[ _ \W]?[i1!|l\xEC-
\xEF][ _ \W]?c[ _ \W]?[a4\xE0-\xE6@][ _ \W]?l _ {0,3}\b/i
meta DRUGS _ DIET ( __ DRUGS _ DIET1 || __ DRUGS _ DIET2 || __ DRUGS _ DIET3 ||
__ DRUGS _ DIET4 || __ DRUGS _ DIET5 || __ DRUGS _ DIET6 || __ DRUGS _ DIET7
|| __ DRUGS _ DIET8 || __ DRUGS _ DIET9 || __ DRUGS _ DIET10 )
describe DRUGS _ DIET Refers to a diet drug
meta DRUGS _ DIET _ OBFU ( __ DRUGS _ DIET1 && ! __ DRUGS _ DIET _ PHEN)
describe DRUGS _ DIET _ OBFU Obfuscated reference to a diet drug
SA processes body rules in series (one after the other), and that can take up to 65% of the total 
time analyzing messages. To speed things up in the latest release at the time of this writing (3.2.0), 
a parallel processing plug-in has been proposed ‚Äúre2c‚Äù (a compiler of Perl regular expressions into 
* http://mysite.verizon.net/mkettler_sa/antidrug.cf

Antispam: Bayesian Filtering ‚óæ 83
C code which implements a parallel matching DFA state machine) and ‚Äúre2xs‚Äù which converts 
basic Perl regular expressions into input for ‚Äúre2c‚Äù and generates a Perl XS module.
An additional speed-up feature has been introduced in version 3.2.0: short circuit* (SC). SC 
enables an early spam decision if a spam score is achieved early in rule processing. Th e concept 
introduces another factor in processing: rule order. Th e administrator needs to specify which rules 
they want to allow to short-circuit the scan and the rule order can be specifi ed in the confi gura-
tion. Th e best practice is to run fast reliable rule fi rst and short-circuit if hit, followed by ‚Äúless 
cheap‚Äù reliable rules, followed by the rest. A sample is provided below for reference:
# local whitelists, or mails via trusted hosts
meta SC _ HAM (USER _ IN _ WHITELIST||USER _ IN _ DEF _ WHITELIST||ALL _ TRUSTED)
priority SC _ HAM 
-1000
shortcircuit SC _ HAM 
ham
score SC _ HAM 
-20
# slower, network-based whitelisting
meta SC _ NET _ HAM (USER _ IN _ DKIM _ WHITELIST||USER _ IN _ SPF _ WHITELIST)
priority SC _ NET _ HAM 
-500
shortcircuit SC _ NET _ HAM 
ham
score SC _ NET _ HAM 
-20
# run Spamhaus tests early, and shortcircuit if they fire
meta SC _ SPAMHAUS (RCVD _ IN _ XBL||RCVD _ IN _ SBL||RCVD _ IN _ PBL)
priority SC _ SPAMHAUS 
-400
shortcircuit SC _ SPAMHAUS 
spam
score SC _ SPAMHAUS 
20
Conclusion
Th is chapter presented a simplifi ed math theory of Bayes‚Äô Th eorem in general, outlining represen-
tations of the theorem that are relevant to spam fi ltering.
It has also delved into examples of various implementations of Bayes‚Äô Th eorem, each utilizing 
a diff erent method to implement it. Irrespective of the details of any particular implementation, 
the initial eff ectiveness of any Bayesian spam fi lter is largely dependent on the corpus of spam 
and ham e-mails that are used to ‚Äútrain‚Äù it to categorize e-mails into spam or ham, based on the 
analysis of their content. Parameters such as the number of words utilized to categorize e-mail can 
also impact the fi lter‚Äôs eff ectiveness and aff ect its performance. Such tuning is largely a matter of 
trial and error, each solution suggesting out-of-the-box default settings that have been proven to 
work in test environments.
In practice, most solutions implement one or more additional techniques of spam detection, 
including but not limited to DNS block lists, header and text analysis (heuristics), and collabora-
tive fi ltering databases (out of the scope of this article) in an eff ort to improve the overall spam 
fi ltering eff ectiveness of the solution.
* http://wiki.apache.org/spamassassin/ShortcircuitingRuleset

84 ‚óæ Information Security Management Handbook
Th ere is no single best solution for any particular scenario. Th ere are simply too many variables 
that aff ect the performance of spam fi lters (starting with proper tuning). Initial implementation 
tends to be labor intensive; however, once a solution has been carefully tuned (that on its own can 
be time consuming) and trained, spam fi lters continuously learn as they go, improving their own 
eff ectiveness.
About the Author
Georges J. Jahchan, CISA, CISM, BS7799 lead auditor, is currently working as a senior infra-
structure management consultant in the Middle East, North Africa, and Pakistan with Computer 
Associates (CA).
Reference
 
1. Spam fi ltering using a Markov random fi eld model with variable weighting schemas, in Proceedings of the 
Fourth IEEE International Conference on Data Mining (ICDM‚Äô 04), pp. 347‚Äì350, 0-7695-2142-8/04 
$ 20.00, IEEE, Brighton, U.K.

DOMAIN 
3
INFORMATION 
SECURITY AND RISK 
MANAGEMENT
Security Management 
Concepts and Principles


87
6
Chapter 
Measuring Information 
Security and Privacy Training 
and Awareness Effectiveness*
Rebecca Herold
Grades Indicate Improvement Needs as Well as Strengths
When I was growing up, there was a trend in many schools to not give grades, but instead just 
give a ‚Äúpass‚Äù or ‚Äúneeds improvement‚Äù mark in an eff ort not to damage the ‚Äúdelicate self-esteem‚Äù 
of children. My father, who taught and was also school superintendent for a few decades, 
believed that was hogwash. If you do not show how well students are doing in a subject for their 
age and grade level, how will you be able to know where improvement is necessary? How will 
* Th is is based upon an updated excerpt of the ‚ÄúEvaluate education eff ectiveness‚Äù chapter from the book Managing 
an Information Security and Privacy Awareness and Training Program by Rebecca Herold, 2005, published by 
Auerbach Publications.
Contents
Grades Indicate Improvement Needs as Well as Strengths.......................................................... 87
Business Drivers for Evaluating Eff ectiveness ............................................................................. 88
Components for Successfully Evaluating Eff ectiveness ............................................................... 89
Evaluation Areas for Your Awareness Program ....................................................................... 89
Evaluation Methods .............................................................................................................. 92
Evaluating Education Eff ectiveness: Intangible Benefi ts .................................................... 93
Evaluating the Eff ectiveness of Specifi c Awareness and Training Methods ......................... 94
Eff ectiveness Evaluation Methods ..................................................................................... 98
Evaluating the Eff ectiveness of Awareness Newsletters ...................................................... 98
Education Eff ectiveness Evaluation Checklist ..................................................................103
About the Author .....................................................................................................................105

88 ‚óæ Information Security Management Handbook
you be able to know the specifi c areas in which a student needs to study more or, diff erently, to 
understand the subject better? How will you be able to know where curriculum needs improve-
ment? How will you be able to tell if a teacher may not be living up to the standards established 
if you see all students are either doing very poorly, or if they are all doing exceptionally well 
across the board? As you can probably tell, we defi nitely had grades, including the minuses and 
pluses, within our school.
While a notable portion of the population would rather not establish any manner of measuring how 
well someone is doing, or how well some initiative is performing, in order to spare feelings, not measur-
ing eff ectiveness is not good business. If you do not measure where your organization is as compared 
to where it started (your benchmark), or how close it is to meeting goals, you will not be able to clearly 
show how much change has occurred as a result of implementing certain business processes. Th is also 
applies to information security and privacy programs. If you do not establish some measurements, 
rankings, ratings, evaluations, metrics, key performance indicators (KPIs), or whatever label you want 
to use, you cannot demonstrate to your business leaders the value of your eff orts to the business.
Th ere has been a large amount of disagreement lately about what the correct ‚Äúterm‚Äù for the 
various types of evaluations should be called. It does not really matter whether or not a specifi c 
international evaluation term exists. What does matter is how you defi ne the evaluations you are 
making within your organization that you clearly communicate those defi nitions and then con-
sistently follow them.
To demonstrate the need for evaluating program eff ectiveness, I am going to focus on measur-
ing the eff ectiveness of information security and privacy awareness and training initiatives. Th ese 
same concepts can be used for other types of initiatives.
Business Drivers for Evaluating Effectiveness
Th e goal of information security and privacy awareness and training should ultimately be to 
change personnel work habits so that they work in a more secure manner and protect the privacy 
of personally identifi able information (PII).
On August 22, 2007, the European Network and Information Security Agency (ENISA) 
released a study, ‚ÄúInformation security awareness initiatives: Current practice and the measure-
ment of success.‚Äù It provides nice documentation validating this need to measure the eff ectiveness 
of information security education eff orts to improve business. It details the awareness methods 
that have worked and not worked, and the various experiences of a wide range of organizations.
Th ere are many business drivers for maintaining documented measurements to track the eff ec-
tiveness of your information security and privacy program. Documented measurements
Point out where you need to make improvements within your program
 
‚óæ
Highlight what is working well within your program
 
‚óæ
Show how much you have improved within specifi c areas of your program
 
‚óæ
Validate to stakeholders the value of information security and privacy initiatives
 
‚óæ
Demonstrate the need to invest in information security and privacy
 
‚óæ
Demonstrate due care processes are in place and are actively being followed to meet compli-
 
‚óæ
ance requirements
Raise the awareness of information security and privacy issues
 
‚óæ
Highlight information security and privacy risk areas
 
‚óæ
Facilitate making better business decisions
 
‚óæ

Measuring Training and Awareness Effectiveness ‚óæ 89
Components for Successfully Evaluating Effectiveness
Th ere are many methods that can be used to evaluate the eff ectiveness of information security and 
privacy program initiatives. Do not get stuck only looking at the very specifi c, narrowly scoped 
technical measurements. While these types of metrics are useful, you also need to look at the big 
picture: your business enterprise.
How are information security and privacy initiatives and eff orts impacting your business? You 
must always keep in mind that information security and privacy exists within your organization to 
support and protect your business and customers. Create and communicate your evaluations and 
measurements in terms of your business whenever possible.
When you are contemplating and planning for how you will perform your evaluations, be sure 
to include and document the following components:
Evaluation topics
 
‚óæ
Evaluation areas
 
‚óæ
Evaluation methods
 
‚óæ
Tangible benefi ts
 
‚óæ
Intangible benefi ts
 
‚óæ
Do not get carried away and have too many measurements, though. After you brainstorm and 
document all your possible measurements, determine the ones most important for your business. 
You do not want to overwhelm your business leaders with too many measurements or they will not 
pay attention to any of them. Determine the ones that will resonate most with business.
Also determine how often to take the measurements. You will need to choose what is most 
appropriate for each of the topics. Some measurements will be appropriate to do once a year, and 
others will need to be done once a week to be meaningful.
You will also need to regularly reevaluate the measurements you have chosen and fi ne-tune 
them, replace them, or completely do away with them. As your business changes over time as a 
result of your training and awareness eff orts, your metrics will also need to be modifi ed.
Let us dig deeper into more of the specifi cs of evaluations for the rest of this chapter.
Evaluation Areas for Your Awareness Program
It is important to know, and demonstrate to your business leaders, that your information security 
and privacy awareness eff orts are valuable and are making an impact on the way your personnel do 
business. Before you can create measurements, grades, or eff ectiveness ratings, though, you need 
to identify the areas within which you will be looking for these metrics.
Verduin and Clark identifi ed eight areas of evaluation for learning (Distance Learning. San 
Francisco: Jossey Bass, 1991); access, relevancy, quality, learning outcomes, impact, cost eff ec-
tiveness, knowledge generation, and ‚Äúgeneral to specifi c.‚Äù I fi nd them useful when measuring 
the success of information security and privacy education eff orts. Tailor them to facilitate the 
evaluation of your own organizational education programs by considering the questions listed 
with each area.
By answering the questions for each of these areas, you will be better able to identify the types 
of metrics you should create to answer the questions, as well as focus better on how to communi-
cate those metrics. I will not cover how to generate the metrics in this chapter; that is a big topic 
and will be best discussed in a future article.

90 ‚óæ Information Security Management Handbook
 
1. Access:
 
a. What groups are you targeting for your education eff orts? List the groups that handle 
PII, use your networks, communicate directly with your customers, and so on. Check 
with other areas in your organization; use your information security and/or privacy 
oversight group if you have one. Ask them, are there groups missing?
 
b. Are all members of the target groups participating in the training off ered to them? Why 
or why not? Are all personnel participating in awareness events? How many of the per-
sonnel have access to attend the awareness events? Are all personnel reading awareness 
communications? Do all personnel have access to awareness communications?
 
c. Are you providing appropriate delivery methods for your target audiences? Can all your 
target audience access your training and awareness materials and participate in your 
delivery methods?
 
2. Relevancy:
 
a. Is your education program relevant to your organization‚Äôs business goals and expecta-
tions? Do your information security education messages have a clear link to the business 
goals? Do you explain how your privacy eff orts support business eff orts?
 
b. Are your training and awareness messages and information relevant to the participants‚Äô 
job responsibilities? Do you clearly communicate where information security actions 
occur within the normal execution of business transactions? Do you relate how privacy 
can be impacted by misuse of PII?
 
c. Does your education program have a noticeable impact on business practices? How 
have personnel changed the way they handle and protect PII as a result of train-
ing and awareness? Is your training content appropriate for your target participants? 
Does your training cover information security and privacy regulatory and policy 
requirements?
 
3. Quality:
 
a. Does the quality of your information security and awareness materials eff ectively deliver 
the intended message? Do your communications capture the attention of your personnel 
throughout the training period? Does the quality of your training materials contribute 
to your learners‚Äô success?
 
b. Do your trainers and teachers deliver quality education? Do they know how to inter-
actively adjust to the abilities and experiences of their learners? Do they have enough 
background and understanding of information security and privacy to be able to answer 
the learners‚Äô questions?
 
c. Were the conditions right for learning? Were the learners encouraged by management to 
participate in training? Did the learners indicate that, in their subjective opinion, they 
were satisfi ed with the quality of the training?
 
4. Learning outcomes:
 
a. Is the amount of time allowed for learning appropriate for successfully understanding 
the message? What do your learners say about the usefulness and eff ectiveness of your 
training and awareness activities? Do you speak to the learners about the expected out-
comes of your education activities?
 
b. Do you tell the learners how their job activities should change as a result of taking the 
training? What did the learners actually learn, as evidenced through observable actions 
or through feedback from quizzes or follow-up surveys? Did your learners indicate they 
truly learned something from taking the training?

Measuring Training and Awareness Effectiveness ‚óæ 91
 
5. Impact:
 
a. What is the impact of your education program on your entire organization? Were secu-
rity and privacy activities and habits noticeably changed in a positive way following 
training and awareness activities?
 
b. Were more information security incidents reported following training and aware-
ness activities? Do personnel ask the information security and privacy areas more 
questions?
 
c. What are the long-term impacts? Did the training methods promote the desired infor-
mation security and privacy skills? Did job performance improve? What was the trend 
related to noticeable personnel work changes following each training session?
 
d. Do you assist managers with determining their own workforce performance changes? 
Do you provide managers with communications to make them aware of the personnel 
changes they should notice and document following the training? Do you create related 
statistics to support and validate training and awareness funds?
 
6. Cost eff ectiveness:
 
a. What time requirements are involved? Is the length of time necessary to take the train-
ing appropriate for not disrupting business work? How much time do your awareness 
activities take? Are education activities off ered during normal work hours, during lunch, 
before and after work, and so on?
 
b. What are the costs of the materials? Are the costs within budget? Are there ways in which cost 
can be reduced by partnering with other departments, or using donations from outside entities?
 
c. How many people are in your targeted groups? How was training delivered? Did it allow 
for all personnel within the targeted groups to attend?
 
d. Are you using inside or outside training and awareness resources, or both? What is the 
value of the method of awareness activity or training session you used compared to other 
awareness and training options?
 
7. Knowledge generation:
 
a. Do you understand and document specifi cally what is important for your personnel 
to know about information security and privacy? Do you understand and document 
specifi cally what is important for your managers to know?
 
b. Do you understand what works and what does not work within your education 
program? Are you actually utilizing your evaluation results?
 
c. Do you account for all types of learners: visual, audio, and kinesthetic (hands-on)? 
Do you provide eff ective multimedia training methods? Do you provide awareness 
communications to account for all types of learners?
 
d. Do you assist employees in determining their own performance success for implement-
ing the information they received? Do you compile trend data to assist instructors in 
improving both information security learning and teaching?
 
8. General to specifi c:
 
a. Do your instructors give learners enough information to allow them to evaluate their 
own success in implementing what they learn?
 
b. Are learners told overall goals for information security and privacy, along with the spe-
cifi c actions necessary to achieve them? Are information security and privacy goals and 
actions realistic and relevant to the business?
 
c. What is the necessary prerequisite general and specifi c information security and privacy 
knowledge for your personnel?

92 ‚óæ Information Security Management Handbook
Measurements can be developed within each of these eight areas to demonstrate the value of infor-
mation security and privacy activities for your business.
Evaluation Methods
Consider using a combination of the following 18 methods for determining the eff ectiveness of 
security and privacy education within your organization.
Be sure to discuss the methods with your legal department and labor unions prior to imple-
mentation to make sure you are not violating any applicable laws, labor union requirements, or 
employee policies.
 
1. Videotape your training sessions, and review and critique to identify where you can improve 
delivery, content, organization, etc.
 
2. Give quizzes immediately following training to measure comprehension.
 
3. Distribute a security and privacy awareness survey to all personnel or to a representative 
sample. Do this prior to training to establish a baseline, then following training to help 
determine training eff ectiveness.
 
4. Send follow-up questionnaires to people who have attended formal training approxi-
mately 4‚Äì6 months afterward to determine how well they have retained the information 
presented.
 
5. Monitor the number of compliance infractions for each issue for which you provide training. 
Is this number decreasing or increasing? Why are they increasing or decreasing? Increased 
infractions may at fi rst be a sign that the program has some weaknesses, but an increase in 
reporting infractions may instead be a result of increased awareness, which would be a suc-
cess sign.
 
6. Measure security and privacy knowledge as part of yearly job performance appraisals.
 
7. Place feedback and suggestion forms on an appropriate intranet Web site.
 
8. Track the number and type of security and privacy incidents that occur before and after the 
training and awareness activities.
 
9. Conduct spot checks of personnel behavior. For instance, walk through work areas and note 
if workstations are logged in while unattended or if patient information printouts are not 
adequately protected.
 10. Record user IDs and completion status for Web- and network-based training. Send a tar-
geted questionnaire to those who have completed the online training.
 11. Have training participants fi ll out evaluation forms at the end of the class.
 12. Identify the percentage of your target groups that participate in training.
 13. Determine if you had an adequate number of instructors with the necessary level of expertise 
for the corresponding training topic.
 14. Determine if the training materials addressed all your goals and objectives. Identify the gaps 
and make a plan to fi ll them.
 15. Review training logs to see trends in attendance.
 16. Tape or fi lm participants performing their work after training to determine if they are utiliz-
ing the skills taught.
 17. Administer occasional tests to personnel. Use multiple choice, short answer, essay tests, or a 
combination of these. Avoid using true-or-false tests.
 18. Perform interviews with past training participants as well as personnel who have not yet 
been trained. Use structured and unstructured interview sessions.

Measuring Training and Awareness Effectiveness ‚óæ 93
Evaluating Education Effectiveness: Intangible Benefi ts
A successful information security and privacy awareness and training program will not only result 
in tangible benefi ts, but also intangible ones.
Intangible benefi ts are positive results that cannot be given monetary or numeric values, or 
would involve too complex calculations to create such values. However, intangible benefi ts have a 
great impact on your organization and on your bottom line. Th ey can also be used as additional 
evidence of an awareness and training program‚Äôs success.
Th e following are common intangible benefi ts of eff ective information security and privacy 
education programs:
Increased compliance
 
‚óæ
Decreased security incidents and privacy breaches
 
‚óæ
Increased job satisfaction
 
‚óæ
Increased organizational commitment
 
‚óæ
Improved work climate
 
‚óæ
Fewer employee complaints
 
‚óæ
Fewer employee grievances
 
‚óæ
Reduction of employee stress
 
‚óæ
Increased employee tenure
 
‚óæ
Reduced employee lateness
 
‚óæ
Reduced absenteeism
 
‚óæ
Reduced employee turnover
 
‚óæ
Increased innovation
 
‚óæ
Increased customer satisfaction
 
‚óæ
Decreased customer dissatisfaction
 
‚óæ
Enhanced community image
 
‚óæ
Enhanced investor image
 
‚óæ
Fewer customer complaints
 
‚óæ
Faster customer response time
 
‚óæ
Increased customer loyalty
 
‚óæ
Improved teamwork
 
‚óæ
Increased cooperation
 
‚óæ
Confl ict reduction
 
‚óæ
Improved decisiveness
 
‚óæ
Improved communication
 
‚óæ
A 1998 Gallup Organization study* of 2 million employees within 700 companies, based upon 
a survey of entirely all workers, concluded that employer-sponsored training and education is 
viewed by employees as a plus in recruitment and contributes signifi cantly to retention, and that 
employees want more training, particularly in technology, communications, and management. 
Other fi ndings within this survey include
Eighty percent indicated training is important or very important in keeping them as employees.
 ‚óæ
Only 50% indicated that the current training received from employers exceeded their expec-
 
‚óæ
tations indicating that training and awareness quality can be improved.
* Employees speak out on job training: Findings of a new nationwide study (1998). Th e Gallup Organization, 
Survey Research Division.

94 ‚óæ Information Security Management Handbook
Determining Intangible Benefi ts of Training and Awareness
To determine the impact of information security and privacy awareness and training activities 
within your organization, include questions similar to those shown in Figures 6.1 and 6.2 in 
evaluation surveys, interviews, and focus group discussions involving your employees, training 
participants, managers, and trainers.
Evaluating the Effectiveness of Specifi c Awareness and Training Methods
Th ere are many ways to evaluate the eff ectiveness of the wide range of training and awareness 
activities. Some will work better than the others, depending upon your organization, situation, 
and regulatory requirements.
Th e following are some examples of evaluating the eff ectiveness of a couple of specifi c training 
and awareness methods. Modify these as necessary for your specifi c education and delivery methods.
Evaluating the Effectiveness of Computer-Based Training Modules
Computer-based training (CBT) modules can be very eff ective for some types of training, but be 
inappropriate or ineff ective for other types.
Whether or not you should use CBT depends on your target audience, the topic, and the 
amount of interaction, feedback, and inquiry necessary. In general, here are the benefi ts and draw-
backs of CBT education in a corporate setting:
Employees and Training Participants
 1. Are you more satisfi ed with your job and support for your responsibilities as a result of this training?
 2. Do you feel more committed to supporting the goals of the organization as a result of this training?
 3.  Do you believe that the work climate is better as a result of information security and privacy awareness 
activities?
 4. Do you feel less work-related stress as a result of the information security and privacy training?
 5. Do you believe your job advancement opportunities have improved as a result of this training?
 6. Do the awareness and training activities motivate you to come to work each day?
 7. Has the training given you ideas on how to improve information security and privacy within your own team?
 8.  Do you believe better information security and privacy practices will result in increased customer 
satisfaction?
 9.  Do you believe the information security and privacy awareness and training eff orts enhance the organization‚Äôs 
community image?
 10.  Do you believe the information security and privacy awareness and training eff orts enhance the organization‚Äôs 
investor image?
 11. Has the training resulted in better teamwork with your coworkers?
 12. Has the training provided you with the ability to make better decisions related to information security?
Figure 6.1 Sample training participant survey.

Measuring Training and Awareness Effectiveness ‚óæ 95
Benefi ts
 
‚óæ
:
Can be taken by participants at a time most convenient to them.
 
‚àí
Does not require trainer interaction.
 
‚àí
Is cost eff ective.
 
‚àí
Participants can work at their own pace.
 
‚àí
Participants do not have someone watching over them.
 
‚àí
No associated travel expenses for trainers.
 
‚àí
More interesting for some participants than classroom training.
 
‚àí
Takes less time for the participant than classroom training.
 
‚àí
Th e participant can retake the portions of the CBT where more instruction is 
 ‚àí
necessary.
Managers, Trainers, and HR
 1.  Do you see increased job satisfaction in your personnel as a result of information security and privacy 
awareness and training eff orts?
 2.  Do you believe your personnel have increased organizational commitment as a result of information security 
and privacy awareness and training activities and participation?
 3. Has your work climate improved as a result of information security and privacy awareness and training eff orts?
 4. Do you notice fewer employee complaints regarding information security?
 5. Do you receive fewer employee grievances concerning information security?
 6.  Do you believe your personnel have reduced their stress as a result of information security and privacy 
awareness and training activities?
 7.  Do you believe employees will stay with the organization longer in part because of the information security 
and privacy awareness and training eff orts?
 8.  Have your personnel improved their punctuality and absenteeism in part because of information security and 
privacy awareness and training activities?
 9.  Have you witnessed increased innovation among your team members following information security and 
privacy awareness and training participation?
 10.  Do you believe that there is increased customer satisfaction because of the changes in how your personnel 
communicate with them as a result of training and awareness eff orts?
 11.  Do you believe the organization has enhanced their community image as a result of providing information 
security and privacy awareness and training activities?
 12.   Have you received fewer customer complaints following implementation of information security and privacy 
awareness and training activities?
 13. Are customer complaints resolved more quickly since implementing awareness and training?
 14.  Do you believe the organization‚Äôs information security and privacy eff orts will result in greater customer 
loyalty?
 15. Do you notice increased personnel cooperation following training and awareness activities?
 16. Have your information security and privacy communications with personnel improved?
Figure 6.2 Sample management survey.

96 ‚óæ Information Security Management Handbook
Drawbacks
 
‚óæ
:
No human instructor interaction so may seem too impersonal.
 
‚àí
Little, if any, individualization for each person‚Äôs capability to understand.
 
‚àí
Not as interactive.
 
‚àí
Trying to fi nd human help when necessary during the training may be hard.
 
‚àí
Th e CBT may be poorly constructed.
 
‚àí
Th e topic may not be best taught via CBT.
 
‚àí
Errors within CBT content will be communicated and propagated to the learners.
 
‚àí
Technology problems can occur with CBTs because of bandwidth, network, and similar 
 ‚àí
problems.
When Does CBT Make Sense?
A few examples indicating CBT is generally more practical than classroom- or lecture-style train-
ing includes when
Procedural or hard-skills training is required.
 
‚óæ
A ‚Äúsafe‚Äù or more comfortable learning environment is needed.
 
‚óæ
Learners are geographically dispersed.
 
‚óæ
Quick roll out of training is required.
 
‚óæ
Consistency in training delivery and materials is required.
 
‚óæ
Th e training topic must meet standards on an ongoing basis.
 
‚óæ
Training is legislated, regulated, or must be based upon best practices and given to large 
 
‚óæ
numbers or geographically dispersed participants.
Training can eff ectively be self-directed.
 
‚óæ
A large number, such as more than 100 learners, must be trained.
 
‚óæ
Launching CBT
Perform a needs analysis for training requirements to determine if the learners are receiving train-
ing for informational use, where a live instructor would be more benefi cial, or for skills progress, 
where interaction with the computer would increase learning. Th en proceed as follows:
 
1. Perform a task analysis to determine the type of information that needs to be included 
within the CBT.
 
2. Design the learning objectives for the CBT. Th e objectives give the learner an idea of the out-
come, conditions, and how he or she will be evaluated in the CBT program. Th e objectives give 
the developer of the CBT parameters to match cognitive skills being communicated within the 
material.
 
3. Design the CBT screen designs. Create user-friendly screens with several characteristics; 
they should be simple as well as informative.
 
4. Ideally, the participant should be able to perform some sort of activity like playing a 
video, giving an answer to a question, or placing something on the screen by using the 
drag-and-drop on each screen.
 
5. Give the CBT participant some control over the learning experience as is appropriate. For 
example, for advanced or high-level topics, give the participant the option of choosing how 
many examples he or she wants to be given or the density of the topic context.

Measuring Training and Awareness Effectiveness ‚óæ 97
 
6. In lower level or basic participant situations, the participant may grasp the knowledge better 
if the program is ‚Äúin charge,‚Äù so that it leads the participant through the module in a very 
structured way, consistent from one person to another.
 
7. Feedback is a very important part of a CBT program. Th oughtful, informative feedback is 
an essential component of CBT programs and can be formative and summative feedback or 
evaluations. In formative evaluations, the participant‚Äôs knowledge is tested on the facts that 
were just given, whereas a summative evaluation tests the participant about the complete 
CBT module. Feedback words can be standard such as ‚Äúcorrect‚Äù or ‚Äúincorrect.‚Äù For correct 
answers, restate the idea to reinforce learning. For incorrect answers, provide an explanation 
and the correct answer. Th en, place the reworded question again later in the module so the 
participant can be confi dent the concept is understood.
 
8. Other important considerations:
 
a. Course organization
 
b. Screen composition
 
c. Colors and graphics
 
d. Text placement
 
e. Wording
 
f. White space
 
g. Text justifi cation (left or center)
 
h. Navigation toolbar
 
i. Consistency
 
9. Perform a quality assurance review of the fi nished CBT to ensure that the fi nished product 
successfully meets the goals of the course. Use trainers, subject matter experts (SMEs), and 
a target learner or two as reviewers.
 10. Obtain executive sponsorship and visible support for the training.
 11. Communicate this to all your target participants. It is most eff ective for the communication 
to come from the executive sponsor.
 12. Identify your primary contacts in each team and department where you are launching the 
training. Communicate with them your timelines for completing the training, along with 
directions on how the training needs to be presented, implemented, and documented within 
their area.
 13. Obtain feedback from the primary contacts.
 14. Review quiz and CBT-module results.
 15. Perform business impact analysis to determine how the training aff ected the personnel in 
their daily job performance.
Managing CBT Participation
Th e key to ensuring high participation in a CBT is to get the support and cooperation of your 
identifi ed primary contacts throughout the organization, typically managers, who will instruct 
their personnel to take the CBT.
Have your information security and privacy champion send messages to your primary con-
 
‚óæ
tacts asking them for their cooperation.
Send your primary contact a CBT implementation information package, including
 
‚óæ
Overview of the CBT along with learning objectives
 
‚àí
Target audiences
 
‚àí

98 ‚óæ Information Security Management Handbook
Timeline for completion
 
‚àí
Responsibilities for the contacts and personnel participants
 
‚àí
Data collection forms
 
‚àí
Sample memos for the contact to send to the participants about the training
 ‚àí
Collect data forms from contacts on a date designated in the timeline.
 
‚óæ
Follow up with contacts who have not responded in a week of the target date. Give them 
 
‚óæ
one more week to have their personnel complete the training and for them to submit their 
data collection forms.
One month following completion of the CBT, send evaluation surveys to contacts to 
 
‚óæ
determine what impact the training had on their personnel.
Update the CBT content according to feedback, quiz, test results, and other identifi ed 
 
‚óæ
factors from your six levels of evaluation forms.
Effectiveness Evaluation Methods
Some possibilities include
Testing each participant‚Äôs knowledge acquisition with an online exam following completion 
 
‚óæ
of the module.
Testing each participant‚Äôs knowledge acquisition with an online quiz following each section 
 
‚óæ
of the module.
Controlling access to the exam questions and exam results.
 
‚óæ
Calculating the number of participants who completed the module.
 
‚óæ
Calculating the high, low, mean, and mode fi nal CBT-module test results.
 
‚óæ
Calculating the high, low, mean, and mode results for each of the quizzes.
 
‚óæ
Identifying questions that had a low success rate, reviewing them, and determin-
 
‚óæ
ing if the question is bad or if the concept was not clearly communicated by the CBT 
curriculum.
Compiling reports for all the above; monitoring participation progress as well as areas of 
 
‚óæ
concern that may need additional training or updated content.
Obtaining feedback via surveys, interviews, and focus groups from the managers in the areas 
 
‚óæ
where the training was given.
Obtaining feedback via surveys, interviews, and focus groups from the CBT participants 
 
‚óæ
1‚Äì3 months following completion of the CBT.
Evaluating the Effectiveness of Awareness Newsletters
Sampling and surveys are two methods of eff ectiveness evaluation that lend themselves best to 
determining the eff ectiveness of many awareness activities, such as electronic awareness newslet-
ters that are targeted at a large population.
Sampling
Sampling is drawing information from a subset of your target group, the people you want to read 
the newsletter, to estimate the characteristics of the entire target population. Sampling is a good 
choice for newsletter eff ectiveness evaluation when

Measuring Training and Awareness Effectiveness ‚óæ 99
You cannot collect data from the entire population to whom the newsletter is targeted.
 
‚óæ
You do not have the time to interview the large number of targeted individuals.
 
‚óæ
You do not have the travel budget to visit all the target population.
 
‚óæ
Some people in the target group are diffi  cult to reach or contact.
 
‚óæ
You do not have enough qualifi ed staff  to conduct interviews or compile surveys from 
 
‚óæ
everyone.
Sampling Procedures
Use the following steps to do sampling:
Identify your target population
 
‚óæ
Create a list of all members within the population
 
‚óæ
Determine your sampling approach
 
‚óæ
Determine your sample size
 
‚óæ
Identify your sample participants
 
‚óæ
Sampling Approaches
Probability sampling
 
‚óæ
. Ensures every member of the target population has an equal chance of 
being selected for the sample.
Simple random sampling
 
‚àí
. Determine what percentage of the target population to contact, 
and then randomly choose from the entire population. Th is is the most straightforward, 
but not frequently used, approach. It is often diffi  cult to get a list of the entire popula-
tion. Some of the employees in the list may have left the organization. Th e clerical eff ort 
to draw the sample may be very time consuming if the population is large. And it may 
not be more appropriate to select some individuals and not others.
Stratifi ed random sampling
 
‚àí
. Divide the entire target population into groups based on such 
characteristics as geographic location, personnel levels, departments, etc. Th en, choose 
an identifi ed percentage from each of these groups (strata). Th is enables the evaluator to 
analyze the data of diff erent subgroups and to compare and contrast the fi ndings.
Cluster sampling
 ‚àí
. First sample a large subgroup, then sample from within the subgroup. 
For example, select 6 out of a total of 12 fi eld offi  ces, and then within each of the six 
chosen offi  ces, draw a random sample of employees to contact. Th is is useful for reduc-
ing costs and the time required to survey people across many groups and locations.
Non-probability sampling
 
‚óæ
. Th is method does not provide information that can be general-
ized with confi dence to the entire population. Th e results may be biased and could aff ect the 
usefulness of the fi ndings. However, it is easier than the probability sampling methods.
Convenience sampling
 
‚óæ
. Contacting personnel who are most accessible for feedback. Th e eval-
uator does not know if these people have characteristics that bias the outcome. For example, 
asking for volunteers to participate may result in people who have a specifi c motivation to 
give feedback that would be completely diff erent than if others were contacted who did not 
have the same motivations.
Purposive sampling
 
‚óæ
. Individuals are selected because of their position, experience, knowledge, 
or attitudes. Because the sample is not randomly selected, the fi ndings cannot be generalized 
beyond those who participated.

100 ‚óæ Information Security Management Handbook
Snowball sampling
 
‚óæ
. Contact identifi ed departments, individuals, etc., and ask them for sug-
gestions for individuals to include in the sample. It is particularly useful when a list of names 
is diffi  cult to obtain in any other way. When choosing your sampling method, take into 
consideration the following issues:
Budget, such as money is available for travel, hiring consultants, interviewers, postage, 
 
‚àí
audiotapes, and other applicable materials
Size of sample population
 
‚àí
Geographical locations of the population
 
‚àí
Availability of the list of all possible people within the population
 
‚àí
Data collection methods
 
‚àí
How much variance exists within the population
 ‚àí
Determining Sample Size
You need to determine how many participants are enough. Th e larger the number, the more 
representative the results will be for the entire population. Th e sample size should depend on the 
following factors:
Th e rarity of the event being evaluated
 
‚óæ
. If the event is rare, then a larger sample size should be 
used. For example, if you want feedback for an event that happens once a year, you should 
use a larger sample size than if you are getting feedback for an event that happens once a 
month.
Available resources
 
‚óæ
. If you have suffi  cient time, resources, and personnel, draw a large 
sample.
Th e degree of precision needed
 
‚óæ
. Th e impact of the evaluation must be considered. For exam-
ple, if the evaluation involves whether the training led to reducing the amount of fi nancial 
fraud resulting from access to customer data, then you will want a larger sample so you 
can be more accurate in determining if the benefi ts of the training outweigh the side 
eff ects.
Whether or not the fi ndings will be generalized
 
‚óæ
. If generalization is not a goal, then the number 
depends on the key evaluation questions, the number of methods being used to collect data, 
and what decision makers believe is suffi  cient to use the fi ndings. If generalization is a goal, 
then you will likely need a probability sampling method.
Figure 6.3 gives you an idea of how large your sample size needs to be to have a confi dence level (assur-
ance that the results represent the total target population) of 95% with a range of 5% and 10%.
Survey Composition
Create your survey to best obtain the feedback you need for your information security and pri-
vacy newsletters or other awareness or training activity for which you are using surveys. Solicit 
the opinions, beliefs, and feedback from your target group. To improve the response rate for your 
surveys to determine the eff ectiveness of newsletters, keep the following in mind.
Keep the survey as short and easy to answer as possible.
 
‚óæ
Stick with yes/no, rating (Likert), or multiple-choice answers.
 
‚óæ
Consider using a combination of questions.
 
‚óæ

Measuring Training and Awareness Effectiveness ‚óæ 101
Use terms that the participants will understand.
 
‚óæ
Phrase items in the same manner that participants speak them.
 
‚óæ
Do not use ‚Äúand/or‚Äù in survey items.
 
‚óæ
Avoid using acronyms.
 
‚óæ
Do not use double negatives. For example do NOT use a question like the following:
 
‚óæ
‚ÄúDo you believe that trainees should not have to pay for their own training? Yes or No‚Äù
If the participant answers ‚ÄúNo,‚Äù it actually means, ‚ÄúYes, trainees should pay for their own 
training.‚Äù
Avoid wording that suggests answers or biases responses in one direction. For example, do 
 
‚óæ
not start a question with ‚ÄúIsn‚Äôt it true that ‚Ä¶‚Äù
Avoid leading or loaded questions. Th is is one that leads the respondent to answer diff erently 
 
‚óæ
if the question were worded in a diff erent way.
Avoid ‚Äúdouble-barreled‚Äù questions that ask for more than one piece of information in the 
 
‚óæ
question. For example, do not ask, ‚ÄúIs this newsletter interesting and useful? Yes or No.‚Äù
Use plenty of white space to make the survey easy to read.
 
‚óæ
Group items into logical sections.
 
‚óæ
Provide clear, simple, and brief directions and instructions.
 
‚óæ
Make it easy to return the survey, for example, via online form submission, e-mail, self-
 
‚óæ
addressed, stamped envelope, etc.
Provide advance communication, preferably from the information security and privacy 
 
‚óæ
sponsor, that the survey will be taking place.
Population
Required Precision ¬±5%
Required Precision ¬±10%
Total Number of People
Sample Size
Sample Size
50
 44
33
75
 63
42
100
 80
49
150
108
59
200
132
65
300
168
73
400
196
78
500
217
81
1,000
277
88
3,000
340
93
5,000
356
94
10,000
369
95
Figure 6.3 Determining responding sampling size with confi dence level of 95%.

102 ‚óæ Information Security Management Handbook
Clearly communicate the reason for the survey. For example:
 
‚óæ
To improve the applicability of the information to personnel job responsibilities
 
‚àí
To learn the topics that are of most concern to personnel
 
‚àí
To discover information security and privacy risks that were not yet known
 
‚àí
To ensure that personnel are reading the newsletters
 
‚àí
To improve the quality of the newsletters
 ‚àí
Indicate who will see the results of the survey.
 
‚óæ
Describe how the results will be used.
 
‚óæ
Pilot the survey before wide distribution. Th is will allow you to identify, remove, or revise 
 
‚óæ
confusing and unnecessary items.
Notify personnel that the survey is coming within an issue or two of the newsletter and ask 
 
‚óæ
for their participation.
Provide an estimate of the time needed to complete the survey.
 
‚óæ
Allow participants to remain anonymous. You will likely want to attach some other type of 
 
‚óæ
stratifi cation if you do this, such as job level, department, etc.
Ask managers to support the survey and encourage participation.
 
‚óæ
If applicable for sampling method, communicate to the target audience that they are part of 
 
‚óæ
a carefully selected sample, and that their participation is very important.
Use one or two follow-up reminders.
 
‚óæ
Send the survey on behalf of executive management, including the leader‚Äôs signature if possible.
 ‚óæ
Provide incentives for completing and returning the survey, for example, a coupon for a free 
 
‚óæ
personal pan pizza, a drawing from those participating for a free day of vacation, etc.
Send a summary of the survey results to your participants.
 
‚óæ
Take advantage of including survey questions within existing surveys, such as existing HR 
 
‚óæ
employee satisfaction surveys, whenever possible.
Survey Questions
Th ink about the purpose and goals of the information security and privacy newsletter or the 
awareness event or training off ering for which you are surveying. Construct the survey questions 
to determine if those goals are met. For example, consider including questions similar to the fol-
lowing if these are some of your newsletter goals:
How well does the newsletter communicate recent information security and privacy inci-
 
‚óæ
dents within the company?
How well does the newsletter communicate incidents outside the company, which present a 
 
‚óæ
risk to the organization?
How well does the newsletter communicate the organization‚Äôs information security and 
 
‚óæ
privacy policies?
How often do you read the information security and privacy newsletter?
 
‚óæ
What are the reasons you do not read, or rarely read, the newsletter?
 
‚óæ
What topics would you like to see included within the newsletter?
 
‚óæ
What do you feel is the most helpful information currently within the newsletters?
 
‚óæ
What do you feel is the least helpful information currently within the newsletters?
 
‚óæ
Do you feel the newsletters should be published more often, less often, or is the current 
 
‚óæ
publication rate just about right?

Measuring Training and Awareness Effectiveness ‚óæ 103
Which of the following types of information security and privacy newsletters do you believe 
 
‚óæ
are most benefi cial? One that goes to all corporate personnel or those that are department-
specifi c and tailored?
What form of newsletter do you prefer and are most likely to read? E-mail-, paper-, or 
 
‚óæ
 Web-based?
Survey Administration
Here is a step-by-step high-level process for you to follow for administering surveys:
 
1. Identify and document the survey participants along with contact information.
 
2. Prepare the surveys for distribution.
 
3. For mailed surveys, prepare return envelopes.
 
4. Compose a cover letter to accompany each survey. It is ideal to have the executive sponsor 
sign the letter. Th e cover letter should state the purpose, give instructions, and other items 
as applicable from the list of 30 items provided earlier.
 
5. For mailed surveys, prepare envelopes for mailing.
 
6. Create a survey tracking form to record each participant‚Äôs name, the date the survey was 
sent, and when it was received.
 
7. Record the receipt of surveys as they are returned. Even if they are anonymous, you can 
count the forms to determine how many participants have responded. You will need this to 
determine the response rate.
 
8. Follow up with those who do not return the survey to help increase your response rate.
Education Effectiveness Evaluation Checklist
Create a checklist to help you keep on track with establishing baseline measurements, delivery 
measurements, and impact measurements that you collect within the education eff ectiveness eval-
uation framework. For a complete set of evaluation forms and charts, see my book Managing an 
Information Security and Privacy Awareness and Training Program.
Use any of the forms in conjunction with this checklist to help you plan and implement your 
own, customized information security and privacy education evaluation framework.
 
1. Establish the information security and privacy training and awareness schedule.
 
2. Identify your goals for each training and awareness activity.
 
3. Obtain executive support for the awareness and training program.
 
4. Create an inventory detailing your training and awareness activities and associated informa-
tion. Use this inventory to track progress with your education program.
 
5. Identify your awareness and training contacts for each location.
 
6. Send your contacts the training and awareness schedule, along with a memo from your 
executive education sponsor.
 
7. Create an eff ectiveness evaluation framework for each training and awareness event/activity 
and fi ll in the information you have so far.
 
8. Th ree weeks before each education activity, send your contacts a baseline pre-evaluation form.
 
9. Two weeks before the activity, follow up with contacts who have not yet returned their 
completed baseline pre-evaluation form.

104 ‚óæ Information Security Management Handbook
 10. Send the appropriate contacts the information privacy and security education eff ectiveness 
evaluation at the time the activity is scheduled to occur.
 11. One week following the activity, follow up with contacts who have not yet returned their 
completed information security education eff ectiveness evaluation.
Consistently Measured Evaluations
Some believe that unless you can assign an exact number to your measurements, your measurements 
are not meaningful. Poppycock! Th ere are many ways in which you can measure your eff ectiveness; 
through metrics, percentages, ratings, rankings, KPIs, grades, and any other type of label you 
want to use. Th e most important consideration to make them truly useful is that they need to be 
consistently measured and applied.
Some argue that meaningful evaluations should not be subjective. Yes, there are many meth-
ods of evaluating eff ectiveness that can and should be objective. Specifi c measurements, such as 
the cost in dollars, the number of hours used, the numbers of questions correctly answered, and 
so on, are valuable. However, there are also some very valuable measures that are necessarily sub-
jective. Consistent and useful measurements can be obtained if your subjective measurements are 
clearly defi ned, and examples provided.
For example, it is important to know that the management in each of the departments are 
supporting awareness and training eff orts. However, trying to measure this with an objective 
numerical value is hard, if even possible, to do. Management support is not a mathematically accu-
rate concept. But you can determine a measurement for management support based upon clearly 
observable management characteristics and actions. Some possible measurements for information 
security and privacy education management support include the following:
 
1. Unacceptable. Management did not make awareness communications available to staff . 
Management did not send staff  to available and applicable information security and privacy 
training sessions. If you want a value assigned instead of the ‚ÄúUnacceptable‚Äù label, use the 
value ‚Äú1.‚Äù
 
2. Needs improvement. Management sends some, but not all, staff  to information security and 
privacy training sessions. Management occasionally, but inconsistently, provides awareness 
communications to the staff . If you want a value, use ‚Äú2.‚Äù
 
3. Satisfactory. Management consistently sends most staff  to information security and privacy 
training off erings. Management consistently gives staff  access to awareness communica-
tions. Use the value ‚Äú3‚Äù if you want.
 
4. Better than expected. Management sends all staff to training regularly and always to 
usually returns training evaluation forms. Management actively and visibly encour-
ages all staff to participate in awareness events. The value ‚Äú4‚Äù would correlate with 
this label.
 
5. Role model. Management has incorporated information security and privacy training into 
staff  job requirements and performance appraisals. Management urges staff  to create infor-
mation security and privacy awareness communications tailored to their own areas, along 
with participating in corporate awareness activities.
Subjective evaluations can tell a lot about awareness eff orts in addition to other metrics if they are 
consistently applied.

Measuring Training and Awareness Effectiveness ‚óæ 105
Your Measurements Are Unique to Your Organization
Eff ective use of information security and privacy measurements can have a profound impact on 
your business. As you gain a better understanding of your business and move closer to achieving 
important goals, your day-to-day work will become easier and your staff  will be more accountable 
for the measurements that matter. You will make sound information security and privacy decisions 
based upon consistently generated measurements that are created in the context of business.
I see too many organizations try to use a cookie-cutter approach to establishing information 
security and privacy measurements. I see too many vendors pushing their cookie-cutter metrics 
onto organizations, only to subsequently have the organizations realize they are trying to measure 
something that is not applicable to them.
Th e axiom generally attributed to Peter Drucker holds true for information security and pri-
vacy eff orts, ‚ÄúYou can‚Äôt manage it if you can‚Äôt measure it.‚Äù You also cannot be successful in your 
eff orts if you do not maintain measurements. Organizations must establish metrics based upon 
their own unique organizational characteristics. Th ey can use ideas obtained from others, but their 
ultimate metrics must be customized to fi t their organization.
About the Author
Rebecca Herold, CISM, CISA, CISSP, FLMI, is an information privacy, security, and compliance 
consultant, author, and instructor.


107
7
Chapter 
Managing Mobile 
Device Security
E. Eugene Schultz and Gal Shpantzer
Contents
Introduction .............................................................................................................................108
Advantages of Mobile Computing ............................................................................................109
Mobile Computing Risks .........................................................................................................109
Susceptibility to the Same Attacks as Conventional Systems ................................................109
‚ÄúAlways-on‚Äù Connections .....................................................................................................110
Reduced Ability to Control Devices .....................................................................................110
Reduced Ability to Monitor User Actions........................................................................110
Operation Independently of an Organization‚Äôs Network 
and Network Security Features ...............................................................................110
Increased Risk to an Organization‚Äôs Conventional Network(s) .........................................110
Increased Diffi  culty of Installing Software and Patches, as Well as of Troubleshooting .....111
Increased Diffi  culty of Confi guration Management .........................................................111
Increased Diffi  culty of Performing Backups and Restores ................................................111
Elevated Risk of Virus/Worm/Trojan Horse Infections ....................................................111
Elevated Risk of Shoulder Surfi ng ...................................................................................111
Elevated Risk of Use of Unauthorized Software ...............................................................111
Elevated Risk of Unauthorized Integrity Changes to Sensitive/Proprietary Data ..............112
Elevated Risk of Mobile Computing Devices Being 
Used without Owner‚Äôs Knowledge ....................................................................... 112
Increased Risk of Unauthorized Interception of Communications ...................................112
Increased Risk of Denial of Service ..................................................................................112
Ease of Downloading Illegal Music or Movies .................................................................113
Risks due to Built-in Cameras and Microphones .............................................................113

108 ‚óæ Information Security Management Handbook
Introduction
Of all the recent trends in technology, few have been as pervasive as the growth of mobile comput-
ing technology. Whereas not too many years ago smart phones, personal data assistants (PDAs), 
and wireless networks were somewhat of a rarity, the opposite is very much true today.
Many types of mobile computing devices now exist. Laptops, not exactly newcomers to 
the mobile computing arena, continue to be prevalent, but the number of smart phones now 
owned and used both in the business and personal arena is growing disproportionately. PDAs 
continue to be popular, as are multifunction devices such as BlackBerry devices. Detachable 
media such as universal serial bus (USB) devices (‚Äúfl ash drives‚Äù) are also very much part of the 
mobile device mix.
Mobile computing exists in many contexts. As many organizations have moved increasingly 
to ‚Äúoffi  celess‚Äù environments, mobile computing has bridged the gap resulting from not always having 
wired technology available. Th e same applies to telecommuting; telecommuters are increasingly 
connecting to organizations‚Äô networks from wireless networks. ‚ÄúRoad warriors‚Äù such as sales 
personnel may connect to numerous wireless networks using one or more mobile devices during 
a typical workday. In larger organizations, employees are working in distributed environments, 
using wireless to connect to a central network while they are in a meeting in one building and 
then connecting via wireless when they change physical locations. Th e use of detachable media 
such as USB fl ash drives to transfer fi les and executables from one computing system to another 
constitutes yet another common use of mobile computing technology.
Increased Diffi  culty of e-Discovery and Archiving ...........................................................113
Increased Diffi  culty of Forensics and Investigation Eff orts ...............................................113
Management Strategies .............................................................................................................114
Creating a Strategy or Plan for Dealing with the Mobile Computing Security .....................114
Performing a Risk Analysis for the Mobile Computing Environment ..................................115
Creating a Breakout Policy for Mobile Computing Security .................................................116
Ownership ......................................................................................................................116
Usage Authorization ........................................................................................................116
Selecting and Implementing Controls ..................................................................................118
Types of Controls ............................................................................................................118
Controls Selection Criteria ..............................................................................................119
Creating Security Standards and Procedures for Mobile 
Computing Security Technology .................................................................................. 121
What Standards Need to Cover .......................................................................................121
What Procedures Need to Cover .....................................................................................121
Training and Awareness ...................................................................................................... 123
Intrusion Detection ............................................................................................................ 123
Incident Response ............................................................................................................... 124
Business Continuity Planning ............................................................................................. 124
e-Discovery ......................................................................................................................... 124
Compliance Monitoring ......................................................................................................125
Conclusion ...............................................................................................................................125
About the Authors ....................................................................................................................125

Managing Mobile Device Security ‚óæ 109
Advantages of Mobile Computing
Mobile computing is becoming increasingly essential in a wide variety of usage contexts because 
of the many advantages that this technology off ers. In a nutshell, the advantages boil down to 
increased productivity and effi  ciency. Users with mobile computing devices can quickly and con-
veniently connect to a network, enabling them to engage in a plethora of tasks, including quickly 
accessing fi les, executables, applications, and databases all of which can be used for a myriad of 
purposes. Data portability is often a particularly critical consideration. Mobile users can bring 
data stored on mobile computing devices with them to use in a wide variety of situations such as 
sales calls and corporate briefi ngs; they can also analyze such data at their convenience. Th e fact 
that today‚Äôs mobile devices now typically off er many hundreds of gigabytes of storage makes data 
access via mobile devices even more advantageous. Additionally, wireless technology allows users 
to quickly and conveniently connect to organizations‚Äô networks to check e-mail and participate in 
instant messaging to keep abreast of the latest developments.
Without mobile computing, the physical location of network connection spots becomes a huge 
obstacle to users. Users would otherwise have to wait until they are able to connect via conven-
tional methods, usually via wired networks, but in some cases via Internet cafes (with all of the 
associated security risks). Most individuals need constant Internet connectivity, and in the busi-
ness world, having this level of connectivity is critical to profi tability.
Cost savings are another signifi cant advantage of mobile computing. Th e cost associated with 
wired networks is considerably higher than for wireless networks. Wireless networking precludes 
the need to install and maintain network cabling within buildings. Additionally, smart phones 
and PDAs are often suffi  cient for mobile users‚Äô needs; the cost of these devices is well below 
conventional computing systems. Furthermore, the ease and speed with which users of mobile 
computing devices can connect to networks typically results in sizable productivity gains.
Finally, mobile computing devices can also deliver functions that ‚Äúnormal‚Äù computers cannot. 
For example, some mobile computing devices support open communication with wireless switches 
confi gured to serve as load request proxies between mobile computing devices and one or more 
mobile services server(s) on wireless networks. A mobile services server furnishes provisioning data 
to each wireless switch, which can then transport these data to each mobile computing device. 
Th e provisioning data are used to set security parameters on each mobile computing device and to 
confi gure software applications, thereby maximizing their security. Additionally, phone services 
are supported on many mobile computing devices, and many also have a built-in camera.
Mobile Computing Risks
With all the advantages of mobile computing and mobile computing devices also come a wide 
variety of security-related risks. Th ese risks are described in this section.
Susceptibility to the Same Attacks as Conventional Systems
Previously, applications and technologies for mobile users were diff erent from traditional ones. 
Before they became PDAs with powerful computing capabilities and storage capacity, cell phones 
were in reality just mobile phones with specialized operating systems and very limited functional-
ity. Security risks were primarily tied to three issues: (1) the privacy of data that fl owed in the ‚Äúair‚Äù 

110 ‚óæ Information Security Management Handbook
on its way to and from the organization‚Äôs network; (2) the need for organizations to open ports in 
their fi rewalls to let the data in; and (3) the risk that thieves might steal the unprotected mobile 
computing devices themselves, thereby gaining access to the data on them. Serious risk from 
earlier mobile devices was limited because their functionality and storage capacity were very lim-
ited. Risks previously in large part applied to Wireless Application Protocol (WAP)-enabled cell 
phones, PDAs and wireless data entry systems, and attackers required a diff erent skill set from the 
one needed to attack other types of computers. Th e opposite is now true. As mobile devices have 
become more powerful, they are now subject to same type of attacks as conventional computers, 
since they now have more complete operating systems and popular, widely deployed applications, 
making them vulnerable to the same malware and interactive attacks that have plagued Windows 
and other systems for years.
‚ÄúAlways-on‚Äù Connections
Today, being connected 24/7 is a critical productivity issue. At the same time, however, an 
‚Äúalways-on‚Äù connection is an almost ideal target for an attacker, who has virtually unlimited time 
to launch attacks. In contrast, conventional workstations are often if not usually turned off  after 
normal work hours.
Reduced Ability to Control Devices
Mobile devices are often out of the immediate control of an organization‚Äôs security staff , creating 
a plethora of complications and risks, including the following.
Reduced Ability to Monitor User Actions
Overseeing mobile employees‚Äô behavior and ensuring that they adhere to their employer‚Äôs security 
policies and procedures is normally much more diffi  cult.
Operation Independently of an Organization‚Äôs 
Network and Network Security Features
Most of the monitoring and fi ltering tools that organizations use to enforce policies and to pro-
tect systems and devices within networks are geared toward the central offi  ce, not the mobile 
computing environment. Th ese tools are built into the organization‚Äôs network(s), and are thus 
very diffi  cult for users to circumvent when users directly access the network(s). Th e opposite is, 
however, true of other networks to which mobile users connect. Additionally, mobile devices are 
used outside of the offi  ce, where they are not as protected as at the corporate offi  ce, resulting in a 
higher likelihood of malware infections and other security breaches. At some point in time, they 
are connected to the corporate network, where they may introduce malware that would otherwise 
have been blocked by network security mechanisms such as fi rewalls and virus walls.
Increased Risk to an Organization‚Äôs Conventional Network(s)
New entry points into an organization‚Äôs network(s) are created by mobile device access to the 
network. Th ese entry points, most of which are not likely to be nearly as security as conventional 
entry points, comprise new potential avenues of attack.

Managing Mobile Device Security ‚óæ 111
Increased Diffi culty of Installing Software 
and Patches, as Well as of Troubleshooting
Given that mobile devices are so often out of the reach of system administrators, performing 
normal maintenance and software (including software with security functionality) or patch instal-
lation is usually considerably more diffi  cult. Troubleshooting installations that do not go well is 
also generally much more complex for the same reason. Additionally, the sheer variety of mobile 
devices, from laptops to PDAs, creates complexity challenges for organizations. Management 
must consider the following when selecting mobile devices for standardization, or for exceptions 
to standards.
Increased Diffi culty of Confi guration Management
Confi guration management of mobile computing devices is more diffi  cult for the same reason that 
installing and troubleshooting software and patches on these devices is.
Increased Diffi culty of Performing Backups and Restores
Many mobile devices are not backed up as frequently as desktops and servers at corporate head-
quarters. Th is is due to several limiting factors: One is the now frequently mentioned problem of 
mobile computing devices not being accessible to system and network administrators. Another is 
ignorance and neglect on the part of mobile computing users, who may not know how to make a 
backup, or if they do, may overlook the importance of doing so. Bandwidth and processing power 
are other factors, in that they create a large backup window for networked backup systems. Th is 
forces mobile users who know how to make backups and who are motivated to do so to use cum-
bersome local backup methods such as USB drives or even smaller devices. Th e same limitations 
also apply to performing system restores.
Elevated Risk of Virus/Worm/Trojan Horse Infections
Mobile devices sometimes lack full-function malware protection available on their conventional 
workstation counterparts. If the packages are available, they are not necessarily centrally managed 
by the corporation. In the case of certain mobile devices, there are still no malware protection 
products available. Furthermore, even if antivirus software is installed, mobile device users often 
delay downloading the latest updates because downloading updates is slow in bandwidth-poor 
wireless networks.
Elevated Risk of Shoulder Surfi ng
Shoulder surfi ng (e.g., at airports and in airplanes) is a particularly dangerous threat with mobile 
computing devices.
Elevated Risk of Use of Unauthorized Software
Many mobile devices, even if owned by an organization, are self-administered by end users. Th is 
gives end users complete control over the confi guration of installed operating systems and applica-
tions, as well as the ability to install further applications, often from untested third parties that 

112 ‚óæ Information Security Management Handbook
may not be fully compatible with the operating system, built-in applications, and third-party man-
agement and security applications installed on the device. Legitimate applications installed by the 
end user are often not as rigorously tested for interoperability and security defects before they are 
sold to the public. Th ey are also not updated as often for security patches as the applications from 
the larger software publishers, who have formalized security development processes that take into 
account prerelease architecture and post-release security patches. Additionally, users may unknow-
ingly download software that appears useful, but that is in reality malicious. Furthermore, mobile 
user actions often cannot be monitored by system and network administrators; so users may be 
able to download peer-to-peer software without being detected.
Elevated Risk of Unauthorized Integrity Changes 
to Sensitive/Proprietary Data
Even in organization with strong controls in the mobile computing arena, a certain amount of 
sensitive or proprietary data is likely to be downloaded to mobile devices. Because these devices 
cannot normally be protected as well as conventional computing systems, the likelihood of unau-
thorized integrity changes due to malware infections, unauthorized access to mobile devices‚Äô hard 
drives, user error, or attacks that result in data modifi cation is higher.
Elevated Risk of Mobile Computing Devices Being 
Used without Owner‚Äôs Knowledge
Mobile devices are inherently more subject to unauthorized use than their desktop counterparts at 
an organization‚Äôs offi  ce(s). In a normal offi  ce, physical security is more stringent than in the home 
offi  ce or on the road. A given desktop machine at a typical offi  ce is not accessible to a multitude 
of people who are not employees and who are thus not in an organization‚Äôs building itself. An 
executive with a corporate laptop often leaves the laptop around the house; children and friends 
often engage in Web surfi ng and other activity on this computer, thereby exposing the computer 
to additional threats such as malware at malicious Web sites.
Increased Risk of Unauthorized Interception of Communications
Mobile devices are designed to use wireless networks at an organization‚Äôs offi  ce(s) or in a wireless 
hotspot in a foreign country. Communicating back to the corporate network via a Virtual Private 
Network (VPN) is not always possible with every mobile device, however. Some PDAs are, for 
example, initially released as ‚Äúhot‚Äù consumer items. It may take a generation or two for basic enter-
prise-level security features to be built into these devices. Additionally, even if a wireless network 
is encrypted, the encryption that is used may be so weak that cryptanalysis of the message content 
may require only seconds. Th e risk of unauthorized interception of network communications from 
mobile computing devices is especially high when the mobile user is in foreign countries where 
business travelers are targeted by local governments and businesses for intellectual property theft 
and competitive intelligence.
Increased Risk of Denial of Service
Denial of service is a special worry in mobile computing environments. One of the major reasons 
is that wireless networks are subject to a variety of very diffi  cult-to-prevent attacks, such as traffi  c 

Managing Mobile Device Security ‚óæ 113
fl ooding and frequency jamming.* Also, as mentioned earlier, fi rewalls, intrusion prevention sys-
tems, and other barriers at the entrance to many organizations‚Äô networks generally fi lter the over-
whelming majority of the malicious incoming traffi  c (such as multitudes of malformed packets 
designed to produce denial-of-service conditions).
Ease of Downloading Illegal Music or Movies
Bandwidth shaping and URL fi ltering at the corporate offi  ce is a standard way to conserve band-
width resources, monitor employee productivity, and prevent objectionable, copyrighted, and 
illegal material from settling into the corporate network. Mobile devices are less controllable 
than the desktop workstations at a typical organization‚Äôs offi  ce(s), since the mobile device‚Äôs traf-
fi c is not usually forced through the corporate proxy server and/or URL fi lters when a mobile 
device is not directly connected to the organization‚Äôs network(s). Th erefore, mobile users are 
generally able to more readily connect to fi le-sharing sites in which illegal music or movies can 
be downloaded.
Risks Due to Built-in Cameras and Microphones
Many mobile devices have built-in cameras and microphones. Cameras pose elevated security 
risk because pictures of documents and computer screens are one way to purloin information 
onto personally owned devices. Cameras may also be remotely controlled via malware and 
take pictures or video clips without the legitimate end user‚Äôs awareness. Microphones built 
into mobile devices can also turn mobile devices into a tool for eavesdropping on sensitive 
conversations.
Increased Diffi culty of e-Discovery and Archiving
Records management within organizations is a growing concern for those in charge of IT and legal 
departments. IT and legal counsel must work together to formulate and implement defensible and 
repeatable policies and procedures that are properly responsive to legal requests for discovery in 
litigation. e-Discovery is becoming an increasingly important component of records management. 
Knowing exactly what fi les exist and where (i.e., in which particular server or workstation) each is 
stored is diffi  cult in conventional computing environments; in mobile computing environments, 
this endeavor is even more diffi  cult.
Increased Diffi culty of Forensics and Investigation Efforts
Incident response and internal investigations in the digital realm are now a fairly routine part of 
normal operations in many organizations. Nonstandard devices such as many types of mobile 
computing devices become technical and legal challenges if the proper forensics hardware, soft-
ware, and standard procedures to successfully and defensibly extract data from the device are not 
available.
* Frequency jamming is fl ooding a particular frequency channel with noise, thus overwhelming normal com-
munications that occur on that channel.

114 ‚óæ Information Security Management Handbook
Case Study in Complexity and Unwelcome Surprises: The iPhone
Security managers sometimes have to make exceptions to policies that ban unsupported devices. The iPhone 
is a wildly popular mobile device that is rapidly taking over market share from traditional PDA players, despite 
a lack of availability of some security features and third-party tools, especially as compared to the Blackberry, 
Palm, and Pocket PC platforms that have had more time to mature in the security realm. The iPhone has 
become more enterprise friendly with features added to the product after its initial release. However, there is 
an interesting twist to the iPhone story. The iPhone requires installation of iTunes, an Apple application, on 
a laptop or desktop, to activate the phone and to install software updates. Apple‚Äôs iTunes also comes with 
Quicktime, a media player program. So, having an iPhone requires installation and management of a separate 
Web-enabled desktop application and a media player on a desktop or laptop system. These two additional 
applications have numerous vulnerabilities. To further complicate things, in early 2008, Apple bundled its 
Safari Internet browser along with the iTunes application, so installing iTunes also installs Safari onto the 
desktop/laptop system that runs iTunes. The iPhone example demonstrates that the mobile device itself is not 
the only security-related problem for the enterprise: There are other security-related issues such as whether or 
not e-mail sync capabilities to exchange servers exist, whether network encryption is available, and whether 
antivirus functionality is present; these must be carefully evaluated and addressed. Furthermore, ISMs must 
also adequately understand and deal with the way devices such as the iPhone are deployed and managed as 
well as the particular types of preinstalled software on these devices. ISMs need to work with owners of these 
devices to make informed decisions concerning costs versus benefi ts that will ultimately drive mobile device 
security standards such as standards for iPhones.
Management Strategies
Given the number and magnitude of security-related risks inherent in mobile computing, miti-
gating these risks should be a very high priority for the organizations that use it. Unfortunately, 
too often due to failure to genuinely understand the seriousness and pervasiveness of mobile 
computer‚Äìrelated risks, this is not the case. In other instances, ISMs may genuinely understand 
these risks, but for one reason or another they have deployed piecemeal approaches and/or purely 
technical solutions, both of which will almost invariably result in unacceptably large amounts of 
residual risk.
Th e better alternative is to use a systematic and comprehensive top-down approach. Th is 
approach should include the following steps and/or components.
Creating a Strategy or Plan for Dealing with the 
Mobile Computing Security
Th is fi rst step in mitigating mobile computing risk is achieving a high-level understanding of 
mobile computing and its potential advantages and disadvantages with respect to business and/or 
mission drivers within an organization. After achieving and documenting this level of understand-
ing and communicating it to senior-level management and obtaining feedback, the ISM should 
create a high-level strategy or plan for securing mobile computing environments. Th is strategy 
should describe the basic objectives in dealing with mobile computing risks as well as the types of 
policy or policies and standards that will be necessary in combating these risks. It should also state 
the types of resources (monetary, personnel, and technological) that are likely to be necessary to 
achieve the long-range goal and why, the types of obstacles that are likely to be encountered and 
what to do if each one manifests itself, and how progress toward achieving the long-range goal 
(mitigating mobile computing security risk to an acceptable level) will be monitored. Th e ISM 

Managing Mobile Device Security ‚óæ 115
should brief senior management concerning the major elements in this plan and should once again 
obtain their feedback and modify the strategy or plan accordingly.
Th e strategy or plan for dealing with mobile computing security must be aligned with the 
strategy or plan for the information security practice as a whole. Just as the overall information 
security strategy or plan must be aligned with critical business and/or operational drivers, so 
should the strategy or plan for achieving mobile computing security. If the overall information 
security strategy calls for tolerating risks that other organizations might not be willing to accept 
because a business is aggressively pursuing profi tability with all the associated business risks, the 
strategy for security mobile computing environments must refl ect a similar posture. In this case, 
a mobile computing strategy might allow use of mobile devices that are not all that conducive 
to security because the devices might uniquely support a new, aggressive sales initiative. Th e 
same principle applies to an organization that is risk adverse regarding business risks‚Äîin this 
case, only mobile devices that are conducive to achieving high levels of security are likely to be 
suitable.
Performing a Risk Analysis for the Mobile Computing Environment
One of the next critical steps is performing a comprehensive risk analysis that covers mobile com-
puting devices, networks, and operations. Th is step is critical in that if performed correctly, it 
enables the ISM and others to genuinely recognize and understand the types of risks that mobile 
computing poses and the magnitude of each risk. For example, an organization that routinely 
assigns laptop computing systems to employees who develop marketing strategies must evaluate 
associated risks such as the cost of current and new strategies falling into the hands of competitors 
due to loss or theft of the laptops and using vulnerable mail servers other than those owned and 
operated by the organization to send and receive mail while employees are away from the offi  ce.
Th e risk analysis process should include vulnerability and threat analyses* in an organization‚Äôs 
mobile computing environment. Additionally, risk analysis (including a vulnerability and threat 
analysis) for this environment should not be a one-time activity, but should (as in the case of an 
overall risk analysis) rather be repeatedly performed at appropriately spaced intervals (e.g., once a 
year). Ideally, a risk analysis on an organization‚Äôs mobile computing environment will as soon as 
possible be integrated into the overall risk analysis process.
Finally, the scope of a risk analysis for the mobile computing environment must be appropriate 
if the risk analysis is to produce valid and meaningful results. Many types of mobile computing 
technology currently exist. Given the many vulnerabilities and threats that exist for each type of 
mobile computing technology, it is tempting to conduct a risk analysis for mobile technologies 
that are not currently used, but that are likely to be used sometime in the future. Yielding to this 
temptation is extremely unwise, however, in that risks associated with technologies that are yet to 
be implemented cannot be genuinely assessed and understood. Newly implemented technologies 
* To the surprise of many information security professionals and others, a number of vulnerabilities in each par-
ticular type and make of mobile computing device usually exists. Th ese vulnerabilities can allow unauthorized 
access to fi le systems, cause denial of service, allow for unauthorized capture and or decryption of encryption 
keys, and much more. Obtaining vulnerability information may, however, be considerably more diffi  cult than 
with conventional operating systems and applications. Some mobile device vendors have over the years been 
very unresponsive to security-related vulnerabilities found in their products; others have gone out of the way to 
communicate these vulnerabilities to their customers and to patch them.

116 ‚óæ Information Security Management Handbook
must instead be assessed for risk at their time of their implementation as part of the overall change 
management process. At the same time, however, proactively collecting and evaluating information 
about the possible security implications of new and developing mobile technologies is essential.
Creating a Breakout Policy for Mobile Computing Security
Another critical step in suitably mitigating mobile computer risks is creating a policy‚Äîa 
‚Äúbreakout‚Äù policy for using mobile devices. Th ere should be one high-level information security 
policy for each organization and other more-specifi c (‚Äúbreakout‚Äù) policies that provide details 
about provisions within the high-level policy and also provide direction concerning specifi c 
areas such as what constitutes acceptable use of computing systems are usually also necessary. 
Given the level of risk involved, mobile computing now warrants creating a breakout policy for 
mobile computing security. Th is policy should address many important issues, which include 
the following.
Ownership
A provision that states that every mobile computing device is the exclusive property of the orga-
nization that has bought and/or issued it should be included in this policy. Individuals to whom 
these devices are issued should also be advised that each device is subject to immediate reposses-
sion by the owning organization for any reason.
Usage Authorization
Who may use mobile devices and for what reasons? Must mobile computing device usage be 
approved in advance by a cognizant manager, and if so, must devices that are used for offi  cial 
company business be issued by the organization, or may personally owned devices be used?
Use of Approved Software
Must all software installed on each mobile device be on an organization‚Äôs approved product list, or 
may other software also be installed and used?
Required Data Protection Measures
Who is the owner of the data that reside in each mobile computing device and what responsibili-
ties does this person have in protecting the data? For example, any restrictions concerning the 
types of fi les that can and cannot be stored on and transferred to/from mobile computing devices 
need to be stated. Additionally, is disk encryption required, and if so, what type and strength of 
encryption is required? Th e same applies to encryption of data transmitted over networks. What 
measures are required to safeguard as well as escrow encryption keys?
Required Physical Security Measures
Th is part of a mobile computing breakout policy should cover many physical security issues. At 
a minimum, it should prohibit leaving mobile devices in any location in which the likelihood 
of theft is higher than normal (e.g., in a locked or unlocked car). In the case of laptops that 

Managing Mobile Device Security ‚óæ 117
store sensitive and/or proprietary information, more extreme measures such as installation of GPS 
devices that allow for stolen laptops to be tracked are appropriate.
Usage Location Restrictions
Where can and cannot mobile devices be taken and used? For example, may devices such as smart 
phone be taken on international travel? May devices that have built-in cameras be used in areas of 
buildings where sensitive and/or proprietary information is discussed and/or printed out?
Responsibility and Accountability of/for Devices
A mobile computing policy should also state that each person who owns any mobile computing 
device is accountable for that device and that that person should use all reasonable measures to 
prevent the theft or damage of that device. Each person must upon discovery of a potentially miss-
ing device immediately report this to a designated entity or group such as physical security.
Training and Awareness: Requirements and Content
As will be discussed in more depth shortly, training and awareness requirements for mobile com-
puting device users and IT administrators need to be included in the mobile computing security 
breakout policy. At a minimum, these individuals should be required to take some kind of training 
concerning risks associated with mobile computing, required logical and physical security mea-
sures (including any procedures that must be followed), prohibited types of usage, and what to do 
if a device is lost or stolen before they actually start using any mobile computing device. Requiring 
IT administrators to receive special training concerning how to install and maintain antivirus and 
anti-spyware software in these devices, how to patch vulnerabilities, how to make backups, and 
how to detect and respond to security breaches in these devices is also highly advisable.
Acceptable Use Policy
Acceptable use provisions for traditional and mobile computing overlap considerably, yet a sepa-
rate acceptable use policy may be necessary because of diff erences between the two computing 
environments. For example, in the mobile computing environment, ‚Äúpiggybacking,‚Äù i.e., obtain-
ing a wireless network connection by connecting to a nearby wireless network that has weak or no 
security, is a real temptation to mobile computing users. Th e closest thing to ‚Äúpiggybacking‚Äù in 
a conventional network environment is fi nding a unsecured terminal and then using it to access 
a network to which a negligent user has already been authenticated. In most cases, however, the 
need to ‚Äúpiggyback‚Äù is much greater with mobile users who are away from their normal computing 
environment. Defi ning and explicitly forbidding ‚Äúpiggybacking‚Äù is thus appropriate in a mobile 
computing security breakout policy. Similarly, because of the typical ease of using mobile devices 
to download movies and music without authorization,* specifi c admonitions against doing so need 
to also be included in such a policy.
* Firewalls that block peer-to-peer access to fi le sharing sites are often used in conventional networking environ-
ment, thereby greatly reducing users‚Äô ability to illegally download movies and music. However, once mobile 
users are away from the offi  ce, they can usually connect to such sites without the intervention of the organiza-
tion‚Äôs fi rewalls.

118 ‚óæ Information Security Management Handbook
Consequences of Policy Violations
Th e consequences (anything from warnings to reprimands to termination of employment) for 
violating this policy must also be delineated.
Selecting and Implementing Controls
What types of controls are suitable for risk mitigation in which types of mobile computing envi-
ronments and devices and what criteria should be used? Th is section answers these questions.
Types of Controls
Th e types of controls selected depend on the particular environment or device in question. For 
example, if an organization cannot control mobile device access to a wireless network (such as in 
the case of home users gaining access to an organization‚Äôs network via wireless networks in their 
own homes or if users access local networks at airports to access their organization‚Äôs mail and 
calendar servers), the following kinds of controls are appropriate:
Antivirus and anti-spyware software
 
‚óæ
Secure network protocols (SSH, SFTP, and so on)
 
‚óæ
Secure e-mail
 
‚óæ
VPNs for all remote connections
 
‚óæ
Policy-enforcing software (e.g., software that drops connections in which peer-to-peer pro-
 
‚óæ
tocols are used)
Personal fi rewalls
 
‚óæ
Hard drive encryption
 
‚óæ
Host-based intrusion detection and intrusion prevention
 
‚óæ
If, however, an organization controls mobile device access to wireless networks, the following 
kinds of controls are appropriate:
Network isolation‚Äîeither segregating wireless networks from conventional networks or 
 
‚óæ
making wireless networks part of virtual LANs* (VLANs)
Scaling back power on wireless transmitters to reduce the likelihood of ‚Äúwar driving,‚Äù dis-
 
‚óæ
covering unprotected access points by driving or walking around a physical area with a 
device or computer that can pick up wireless transmissions
Having strong authentication at all access points
 
‚óæ
Deploying secure gateways
 
‚óæ
Secure network protocols
 
‚óæ
Secure e-mail
 
‚óæ
Encrypting all network traffi  c (e.g., using WPA
 
‚óæ
‚Ä† and ultimately to the IEEE 802.11i 
standard‚Ä°
* Virtual LANs or VLANs are local area networks in which computers are not within the same physical LAN, 
but yet the switch that sends network traffi  c to and from them acts as if they are.
‚Ä† WPA is wireless protected access, a specifi cation for interoperable security enhancements in wireless networks.
‚Ä° An IEEE (Institute of Electrical and Electronics Engineers) standard for a security protocol for wireless net-
works that was developed to replace older, less secure encryption protocols.

Managing Mobile Device Security ‚óæ 119
Hard drive encryption
 
‚óæ
Media access control (MAC) address fi ltering, such that only pre-known, approved MAC 
 
‚óæ
addresses are allowed to connect to the wireless network
Making all names of access points and disabling service set identifi ers* (SSIDs) diffi  cult to 
 
‚óæ
guess
Disabling SSID broadcasts
 
‚óæ
VPNs for all connections
 
‚óæ
Network access control (NAC) to reduce the likelihood that virus-infected and vulnerabil-
 
‚óæ
ity-laden devices can connect to the network
Deploying servers that provide security at intermediate connection points, especially when 
 
‚óæ
wireless devices also support Voice over IP (VoIP)
Host-based intrusion detection and intrusion prevention
 
‚óæ
Network-based intrusion detection and intrusion prevention tools
 
‚óæ
Potential controls for Bluetooth devices include the following:
Antivirus and anti-spyware software
 
‚óæ
Keep any Bluetooth network(s) separate from other networks
 
‚óæ
Application-level security controls‚Äîthese are especially important in mobile computing 
 
‚óæ
security because applications that run on mobile devices are often developed without much 
concern for security
Vendor products that associate user identities to the keys used in Bluetooth link encryption
 
‚óæ
Servers that enforce security-related procedures and mechanisms (e.g., biometric authentica-
 
‚óæ
tion, end-to-end encryption, and so forth)
NAC
 
‚óæ
Hard drive encryption
 
‚óæ
Backup software in which backups are regularly initiated by clients within devices
 
‚óæ
Note that in the previous examples, obtaining an acceptable level of security would not neces-
sitate implementing all or possibly even most of the listed controls. If nothing else, the lack of 
fi nancial resources that information security practices usually face would prevent purchasing 
most or all of these controls. At the same time, however, implementing only one or two of these 
controls would not be a wise strategy given the need for defense-in-depth or layered defenses. 
Multiple layers of controls need to be in place in case one or more security controls are bypassed 
or defeated.
Controls Selection Criteria
Potential controls need to be evaluated on the basis of numerous criteria, including the 
following.
Effectiveness
Some types of technical controls for mobile computing devices are simply more eff ective 
in mitigating certain kinds of risks than others and are thus, all things considered, better 
* An ID that is used to distinguish one wireless local area network (LAN) from another.

120 ‚óæ Information Security Management Handbook
candidates for selection and deployment. For example, personal fi rewalls for these devices 
provide better protection by preventing intrusions into these devices than do passwords used 
in the authentication of these devices.
Strength of Authentication
Surprisingly, some controls for mobile computing devices off er relatively weak (if any) authentica-
tion. Strength of authentication should be a major consideration during the selection of controls 
for these devices.
Defense in Depth
Whenever practicable, controls should be selected and deployed in accordance with the defense-
in-depth principle. Multiple layers of defense can thwart attacks in which one or more controls 
have been defeated or bypassed.
Compatibility with Other Software and Features 
on Each Mobile Computing Device
Technical controls for each mobile computing device need to run compatibly with other software 
running on the same device as well as features built into that device. Some kinds of antivirus soft-
ware may, for example, falsely detect anti-spyware software as malware and may thus isolate it in 
the same manner that it isolates viruses it detects.
Performance Considerations
Although most mobile computing devices‚Äô processor speed and random access memory (RAM) 
have increased substantially in recent years, most of these devices do not have as good performance 
as standard desktop machines, let alone servers. Installing additional software and/or enabling 
features that hurt performance can cause mobile devices to become intolerable. Selecting technical 
controls that do not interfere signifi cantly with performance is thus essential.
Ease of Installation and Maintenance
Given that mobile devices are frequently used in places where they are not accessible to system 
administrators, ease of installation and maintenance is a particularly important consideration.
Usability
Using many types of mobile computing devices is not particularly easy from a usability point of 
view. Technical controls that are implemented on these devices should thus not compound usabil-
ity problems.
Cost
Th e fi nancial cost of technical controls should be reasonable. Th is is an especially important consid-
eration given the sheer number of mobile devices that organizations are currently using. Th is number 

Managing Mobile Device Security ‚óæ 121
will also certainly only grow in the future. Th e ISM should also consider that up front costs are only 
the fi rst of a number of costs over the life cycle of a security product; maintenance and upgrade costs 
must also be considered.
Vendor Considerations
Vendor reputation, support, and reliability should also have a major bearing on the decision 
whether or not to buy a particular technical control for mobile computing device security.
All things considered, however, the major driver for selecting controls should be costs versus 
benefi ts. Ideally, a control should yield many times more benefi ts in terms of amount of risk miti-
gation than the cost in terms money, disruption, and so on.
Creating Security Standards and Procedures 
for Mobile Computing Security Technology
Once controls have been selected and implemented, standards and procedures for the technology 
used to counter mobile computing risks should follow soon.
What Standards Need to Cover
Standards must specify acceptable parameters for mobile computing security technology, such as 
the number of days between updates of antivirus software. At the same time, however, standards 
are likely to vary greatly, depending on the policy provisions for mobile computing security, the 
particular context(s) in which mobile computing is used, and the particular types of mobile com-
puting technology that are being used within an organization.
What Procedures Need to Cover
Procedures must as much as possible be focused on the system and network administrators who 
administer mobile devices, because these individuals will in most cases be the ones who have 
suffi  cient technical skills to be able to perform that many tasks (e.g., installing software and 
performing maintenance tasks) that need to be completed for the sake of security. Given the 
propensity for user error combined with users‚Äô general lack of technical knowledge, procedures 
for mobile computing security must also be designed to keep users out of the loop as much as 
possible.
Procedures need to at a minimum address all of the following issues.
Inventorying All Mobile Computing Devices
Several studies indicate that a substantial (up to 40%) percentage of mobile computing devices 
are lost or stolen within 2 years of their purchase. Additional studies show that a disproportionate 
percentage (up to 75%) of all mobile device thefts is committed by insiders (employees, contrac-
tors, and so forth). Regularly (e.g., once a month) inventorying all mobile computing devices is 
thus an extremely critical theft and loss mitigation measure. Procedures need to establish ways 
that those charged with keeping inventories can locate each mobile computing device at specifi ed 
time intervals and to report missing devices. Procedures should include escalation methods to be 
used when business-critical devices are missing.

122 ‚óæ Information Security Management Handbook
Enabling Security-Related Features
Mobile computing devices tend to be small in physical size. Functions (including security-related 
functions) thus tend to be represented by small icons or may be buried in menu structures that 
may be as deep as fi ve or six levels. Consequently, enabling security-related features tends to be 
diffi  cult. Procedures must specify the features to be enabled, how to enable them, and how to set 
any associated parameters in accordance with standards.
Installing Security Software
Only a few years ago, security software such as antivirus software and anti-spyware software 
was not widely available for mobile computing devices, but now it is. However, as in the case of 
enabling security-related features, installing security software on mobile computing devices tends 
to not be very intuitive. Procedures must thus delineate the steps involved in security software 
installation as well as how to set parameters that determine how the software works.
Penetration Testing
One of the major failings in security mobile computing devices and wireless networks is neglecting 
to conduct penetration testing, yet penetration testing is one of the best ways to fi nd vulnerabili-
ties in mobile computing devices. Th e ISM must ensure that thorough procedures for penetration 
testing these devices as well as procedures for eliminating any vulnerabilities that are found are 
developed.
The Patch Management Process
Patching mobile computing devices is one of the most diffi  cult tasks in the IT arena. Th e reason 
is that mobile computing devices are so often out of the direct control of the IT staff  within an 
organization. In contrast, conventional computing devices are normally put in one fi xed loca-
tion within a building and connected to a particular network and subnet, things that are very 
conducive to remotely pushing patches into these devices using a patch server. Procedures must 
specify how and how often patches for mobile computing devices are to be installed. Th e number 
of vendor products that simplifi es patch management for mobile computing devices has slowly 
grown over time. Some of these products check for the presence of each device on an organization‚Äôs 
network. If the device is found, a patch server checks the device to determine whether each patch 
has been installed. If not, the patch server automatically installs each missing patch.
How and How Often to Review Audit Log Data
Many types of mobile computing devices now have their own audit logs. Capturing audit data 
does not do any good, however, unless someone or something (e.g., a security information and 
event management tool) analyzes the data. Procedures must thus state how and how frequently 
audit/log data need to be reviewed as well as archived.
Performing Backups and Restores
Mobile computing devices are ‚Äústrange beasts‚Äù in that although more of them have complete operat-
ing systems than ever before, some of their functions are less straightforward than for conventional 

Managing Mobile Device Security ‚óæ 123
operating systems. Backups and restores are two of these functions. Whereas attaching tape drives 
to conventional systems for the purpose of backing up or restoring them is generally trivial, the 
same is by no means true for most mobile computing devices. Special procedures and solutions for 
backups and restores therefore need to be created for many types of these devices.
Responding to Incidents Such as When Mobile Computing 
Devices Become Lost or Compromised
Finally, given the above average probability that mobile computing devices will be lost or stolen 
or will have a data security breach, procedures for responding to such incidents need to be 
created. Although these procedures are likely to be very similar to incident response proce-
dures for conventional computing devices, the threat profi les and eccentricities associated with 
mobile computing devices dictate the need for special steps. For example, eradicating a virus 
in a mobile computing device for which no antivirus software currently exists is diff erent from 
eradicating a virus in a Windows PC.
Training and Awareness
Ensuring that mobile computing device users and system and network administrators who install 
and maintain these devices have suffi  cient training is a critical part of achieving adequate control 
over mobile computing risks. Users must at a minimum be taught the provisions of the breakout 
policy for mobile computer security so that they are more likely to do what is required of them 
and to avoid doing what is prohibited. Users must also be told not to download fi les that are sent 
to their mobile devices unless they are expecting the fi les to be sent, and also to avoid surfi ng 
unknown Web sites with these devices. System and network administrators need training con-
cerning initial confi gurations of mobile devices, how to properly install additional software (such 
as security tools), how to install patches, how to back these devices up, and much more. Th e fact 
that mobile devices are so often completely out of the reach of system and network administrators 
coupled with the eccentricities and mysteries that go with many of these devices makes training 
for these individuals a much more important than average requirement.
Intrusion Detection
Mainstream network-based intrusion detection systems (IDSs) such as Snort are capable of iden-
tifying some attacks against mobile computing devices. Th ese tools generally have no trouble 
detecting denial-of-service attacks, attempts by viruses and worms to scan other systems and 
devices, and many other kinds of malicious activity. However, many attacks against mobile com-
puting devices are against specifi c types of mobile devices, each of which has its own particular 
vulnerability profi le. In this case, mainstream IDSs do not fare very well‚Äîthe missed detection 
rate tends to be very high. Th e best solution is to purchase and install host-based IDSs designed 
specifi cally for each type of mobile computing device that is being used. Th is solution is, however, 
usually limited in that when a host-based IDS detects an attack against a mobile device, the fact 
that the mobile device is often not connected to an organization‚Äôs network frequently results in 
the alert that is sent to a central IDS within the organization‚Äôs network being dropped or blocked 
by some intermediate barrier such as a fi rewall at the entrance to the network being used by the 
mobile user or a spam wall. Additionally, although host-based IDSs for mobile devices are avail-
able for a number of these devices, they are not available for many others. Th e bottom line is that 

124 ‚óæ Information Security Management Handbook
at this point in time it is not likely that an intrusion detection eff ort in connection with mobile 
computing is likely to be very successful. Still, getting started is better than doing nothing at all, 
and planning for the future, when host-based IDSs for a larger proportion of mobile devices are 
likely to be available, is necessary.
Incident Response
Incident response for attacked and/or compromised mobile computing devices is another extremely 
challenging endeavor. As discussed earlier, the basic problem is that mobile devices are so fre-
quently out of the direct control of an organization‚Äôs system and network administrators, often 
rendering basic incident response measures such as immediately isolating compromised systems 
impossible. Additionally, the previously discussed problem of computer forensics for mobile com-
puting devices being more diffi  cult than normal, in large part due to a dearth of forensics products 
designed for many of these devices, presents even more challenges to mobile computing inci-
dent response eff orts. Nevertheless, the ISM should ensure that incident response procedures for 
mobile computing devices are written on a best eff ort basis, frequently tested, and appropriately 
distributed.
Business Continuity Planning
Th e ISM must also consider mobile devices in business continuity planning. One of the most 
overlooked facts in the IT arena is the importance of many mobile computing devices to an orga-
nization‚Äôs business interests. Consider, for example, the importance of a CEO‚Äôs PDA on which 
notes related to strategic planning are stored. Th e loss of this PDA could possibly have extremely 
adverse consequences for the organization, not merely because the PDA might fall into the hands 
of competitors, but also because of the potential value of the intellectual property that the CEO 
may or may not be able to recreate at a later point in time. Although mobile computing devices do 
not fi t into classic business continuity and disaster recovery models very well, at a minimum these 
devices can and should be regularly backed up. As mentioned previously, special procedures for 
backing up mobile computing devices need to be created and updated as necessary.
e-Discovery
Th e ISM must select and implement solutions for challenges (as imperfect as they may be) that 
mobile computing creates for e-Discovery. Having a mobile computing device policy that limits 
the types of fi les that can be stored on these devices is the right place to start. Th is policy should 
in most cases state that fi les containing personal and/or fi nancial data cannot be stored on mobile 
computing devices. Th e disposition of documents, e-mail messages, instant  messenger (IM) 
 conversations, and other communications to and from the mobile device should all  follow the 
 corporate records management policies for retention. Th is can be a daunting technical task, because 
many mobile devices are self-administered and lack centrally managed software that works with 
archiving backend systems. Lamentably, the end user will thus have to  properly synchronize the 
device to a corporate server, desktop, or laptop that will then be incorporated into the archiving and 
discovery process. Similarly, destruction and disposal of information stored on mobile  computing 
devices should be considered. Alternatively, the ISM can use data de- duplication technology to 
ensure that one and only one copy of each fi le is stored and that it is stored in a known location, 
and having periodic audits of fi les stored on mobile computing devices. Additionally, a few vendor 

Managing Mobile Device Security ‚óæ 125
tools can help with the e-Discovery challenge in mobile computing environments by indexing and 
cataloging fi les stored on mobile devices and then reporting the fi le names and paths to a central 
e-Discovery server. Th is technology is in its relative infancy; nevertheless, it may be suitable for 
the current needs of some organizations, and it promises to become considerably better in the 
future.
Compliance Monitoring
Creating policies, standards, and procedures related to mobile computing security does little good 
if there is no enforcement that accompanies it. Th e ISM is the fi rst line of enforcement, so the 
ISM must (in conjunction with the IT organization and other stakeholders) systematically check 
whether the requirements in the policies, standards, and procedures for mobile computing secu-
rity are being met. Th e ISM must also use good judgment in determining whether or not requested 
variances should be approved, and must be reasonable in creating action plans for dealing with 
out-of-compliance situations in which it is not practical for business or other reasons to comply by 
an established date.
Conclusion
Mobile computing is an extremely dynamic arena, as is virtually every aspect associated with 
it, security very much included. As such, the ISM must at a minimum devote a great deal of 
eff ort to understanding the boundaries within the organization‚Äôs network in which and out of 
which mobile computing exists and how these boundaries change over time. Th e ISM must con-
stantly track mobile computing usage and learn of new mobile computing technologies that seem 
to inevitably surface within an organization so that new security-related risks can be identifi ed, 
evaluated, and mitigated. Th e types and values of information that are remotely stored on mobile 
computing devices and transmitted in mobile computing environments as well as the applications 
that run in mobile computing environments will inevitably change over time; these, too, need to 
be tracked and the resulting risk needs to be considered and, if justifi ed by a cost-benefi t analysis, 
mitigated. Ensuring that new, appropriate provisions are added as needed to an organization‚Äôs 
mobile computing security policy is also imperative.
Reducing security risk associated with mobile devices and mobile computing environments 
to an acceptable level is a very diffi  cult task, one in which so far very few organizations have 
been truly successful. As the use of both wireless networking and handheld mobile devices in 
organizations continues to grow, the struggle to achieve this goal in the face of signifi cant 
budget constraints is likely to become even more challenging. Th e ISM must thus now more 
than ever before assume a much more active role in ensuring that a process for planning and 
phasing in proper controls for mobile computing is in place.
About the Authors
E. Eugene Schultz, PhD, CISM, CISSP, is the chief technology offi  cer at Emagined Security, an 
information security consultancy based in San Carlos, California.
Gal Shpantzer is a trusted advisor to chief security offi  cers of Fortune 500 corporations, Silicon 
Valley start-ups, large universities, and national nonprofi ts organizations.


127
8
Chapter 
Establishing an Information 
Security Program for 
Local Government
Robert K. Pittman, Jr.
Introduction
Many of us are survivors based on experiences and challenges that all of us face throughout life. 
Th ese challenges include the careers and jobs that we work and spend an enormous amount of 
time on a daily basis for many decades that eventually lead to a gratifying retirement. At least, 
this is the hope and goal. Employees that seem to be fl ourishing with satisfying retirement pack-
ages are employed at various levels of government (e.g., federal, state, and local). Government 
or public-sector jobs diff er from corporations or private-sector jobs in numerous facets. Some of 
these diff ering facets are providing an enormous amount of services to citizens and constituents 
throughout our nation.
Th e consumers of government businesses are essentially supported by the public. Th e public is 
comprised of its citizens, constituents, and businesses such as nonprofi ts, corporations, including 
government agencies at all levels. Th ere is a relationship among everyone that involves citizens in 
Contents
Introduction ............................................................................................................................ 127
Organizational Governance ......................................................................................................129
Organizational Culture and Behavior ...................................................................................... 130
Th e Information Security Executive in the Organization ..........................................................132
Information Security Policies, Standards, Procedures, and Guidelines...................................... 134
Th e Information Security Organization ....................................................................................135
Conclusion ...............................................................................................................................137
About the Author .....................................................................................................................138

128 ‚óæ Information Security Management Handbook
terms of governments and associated organizations interrelating their programs and services on 
behalf of the public. At least, this is one of the goals of government, since they are providing ser-
vices that a corporate business would not even consider. Th e plethora of services being provided to 
the public and its citizens are social services, general government, health care, and public safety.
Some of the countless government social services programs are addressing and supporting 
low-income families, foster care, emancipated youths, and general relief payments for food and 
housing for the disadvantaged. Other services consist of property value assessment, property tax 
payment, requests for a birth certifi cate, marriage license, or a death certifi cate, as well as sim-
ply registering to vote, in part, constitutes general government services. Our citizens, from the 
time they are brought into this world and throughout their lives will require health-care services. 
Medical and mental-health care including public health issues will always be of the highest 
concern to all levels of government that involve all ages. It may seem obvious that public safety 
services are at the top of the list with heath care as well. Th e security of our homeland, borders 
and ports protection, law enforcement, and protecting our loved ones is an area where government 
visibly plays a signifi cant role.
All of the aforementioned government services are provisioned externally to the citizens. 
Th e perspective on services provided internally would be contrary to corporate, in terms of the 
existence and loyalty of a signifi cant amount of employee‚Äôs labor unions (i.e., civil service rules), 
attractive sustained retirement packages, consistent health and dental benefi ts, career and job 
advancements within the same government level where opportunities exist at diff erent depart-
ments, branches, and agencies, and are knowingly supporting a cause or the greater good.
Regardless of what lens we use to view government and corporate, the lens illustrates that 
obvious diff erences do exist. Many of the services to the public are intangible where government 
employees have a fond appreciation.
By viewing government at the 80,000 ft level and viewing through the looking glass, diff er-
ences exist from an employee and organization perspective. Local government (i.e., county and 
city) organizations have diff erences in contrast to corporations. Obviously, corporate stock shares 
and job security are some of those diff erences. However, establishing an information security pro-
gram has signifi cant diff erences within local governments contrary to corporations.
To establish an information security program in local government involves an array of focal 
points that must be addressed within the initial 18 months by the chief information security offi  -
cer (CISO), chief security offi  cer (CSO), or information security manager (ISM). In some recent 
information security forums and industry writings, the term chief risk offi  cer (CRO) may have 
a signifi cant role as well. It is imperative that these focal points are addressed in terms of having 
them established and adopted by the organization:
Enterprise information security policies
 
‚óæ
Information security steering committee (ISCC)
 
‚óæ
Enterprise information security program
 
‚óæ
Enterprise information security strategy
 
‚óæ
Identify the organization health level based on an information security risk assessment
 
‚óæ
Enterprise and departmental (or agencies) computer emergency response teams (DCERTs)
 
‚óæ
Enterprise security engineering teams (SETs)
 
‚óæ
Each of the above focal points can be categorized as your ‚ÄúLucky 7.‚Äù Th roughout this chapter these 
points will be referred as Lucky 7. For that information security professional that addresses these points 
will be ‚Äúlucky,‚Äù and the others will not be as ‚Äúlucky‚Äù in terms of continued employment with that 

Establishing an Information Security Program for Local Government ‚óæ 129
particular organization since the primary responsibility exists in the information security unit. Th is 
may sound harsh. However, at the end of the day, the organization‚Äôs business and services being 
provided to the citizens and constituents have the expectation that their confi dential, sensitive, 
and personally identifi able information (PII) is secured and protected. It is the job of the informa-
tion security professional to accept the challenge and responsibility to ensure that the organization 
stays away from any press or media release announcing a data breach, or perhaps, a breach of trust. 
As information security practitioners are aware, there has been a plethora of announcements in the 
press and media on organizations (public and private sectors) that experienced computer security 
breaches. Th ese are in corporate America, colleges and universities, health-care organizations, as 
well as the 26 million veterans‚Äô records with PII that was the responsibility of the federal govern-
ment Veteran‚Äôs Administration (public sector) and T. J. Maxx‚Äôs record setting 45.7 million credit 
and debit card owners (private sector).
Organizational Governance
It seems more apparent that the public sector leverages a security-related event to promote an 
information security program, or at the minimum, obtain a funding source to support a project 
or initiative. Despite the consequences of failure or compromise, security governance is still a 
muddle. It is poorly understood and ill-defi ned, and, therefore, means diff erent things to dif-
ferent people. Essentially, security governance is a subset of enterprise or corporate governance. 
Moreover, one could identify governance as security responsibilities and practices, strategies and 
objectives for security, risk assessment and management, resource management for security, and 
compliance with legislation, regulations, security policies, and rules.
Information security governance is ‚Äúthe establishment and maintenance of the control environment 
to manage the risks relating to the confi dentiality, integrity, and availability of information and its 
supporting processes and systems.‚Äù
From a local government perspective, in terms of county government that is governed by a 
fi ve-member board of supervisors and the chief executive offi  cer (CEO). Th e CISO, departmental 
information security offi  cers (DISOs), and the ISSC or a security council comprises the informa-
tion security governance.
A federated organizational structure is the norm for the majority of local government 
organizations. If the discussion is county or city government, there are numerous business 
units or departments that serve unique and diff ering business purposes. Because of these 
unique business units, comprehensible governance is vital to the success of any information 
security program. Th is governance involves a strategic organization framework (Figure 8.4) 
that provides a clear illustration of the involved players. Th e ‚ÄúSecurity Triangle‚Äù or Figure 
8.3 is a framework that is doable for the CISOs organization regardless if their information 
technology is decentralized, centralized, or a managed-security service. Additionally, a local 
government organization can be deemed as an organization with 30 or more corporations, in 
terms of having 30+ county departments with distinct businesses as they serve their respective 
constituents‚Äô (Figure 8.1).
Th e Security Triangle must be supported by the organization‚Äôs senior management; however, 
articulation of this support should be achieved by the development of board-adopted policies. 
Th ese policies are similar to the corporate world where the board of directors and CEO can adopt 
policies. However, information standards and procedures can be approved by an information 
security council.

130 ‚óæ Information Security Management Handbook
Organizational Culture and Behavior
Bruce Schneier is an internationally renowned security technologist and author, as well as the go-to 
security expert for business leaders and policy makers. Currently, he is the chief security technol-
ogy offi  cer for BT Managed Security Solutions. He states, in his book Beyond Fear, Th inking 
Sensibly about Security in an Uncertain World, that security is all about people: not only the people 
who attack systems, but the people who defend those systems. If we are to have any hope of making 
security work, we need to understand these people and their motivations. We have already discussed 
attackers; now we have to discuss defenders.
Schneier also states that good security has people in charge. People are resilient. People can 
improvise. People can be creative. People can develop on-the-spot solutions. People are the 
strongest point in a security process. When a security system succeeds in the face of a new or 
coordinated or devastating attack, it is usually due to the eff orts of people.
If it was not obvious prior to reading this chapter, it should be obvious now that people play a 
signifi cant role as part of any information security program. And it is the culmination of people 
that defi nes organizational behavior and its culture. Organizational culture is the culture that 
exists in an organization, something akin to a societal culture. It is composed of many intangible 
phenomena, such as values, beliefs, assumptions, perceptions, behavioral norms, artifacts, and 
patterns of behavior. It is the unseen and unobserved force that is always behind the organiza-
tional activities that can be seen and observed. Organizational culture is a social energy that moves 
people to act. ‚ÄúCulture is to the organization what personality is to the individual‚Äîa hidden, yet 
unifying theme that provides meaning, direction, and mobilization.‚Äù
Organizations are assumed to be rational‚Äìutilitarian institutions whose primary purpose is 
the accomplishment of established goals (i.e., information security strategy and initiatives). People 
in positions of formal authority set goals. Th e personal preferences of organization employees are 
restrained by systems of formal rules (e.g., policies, standards, and procedures), authority, and 
norms of rational behavior.
Th ese patterns of assumptions continue to exist and infl uence behaviors in an organization 
because they repeatedly have led people to make decisions that ‚Äúworked in the past.‚Äù With repeated 
use, the assumptions slowly drop out of people‚Äôs consciousness but continue to infl uence organi-
zational decisions and behaviors even when the environment changes and diff erent decisions are 
needed. Th ey become the underlying, unquestioned, but largely forgotten, reasons for ‚Äúthe way 
we do things here‚Äù‚Äîeven when the ways may no longer be appropriate. Th ey are so basic, so 
ingrained, and so completely accepted that no one thinks about or remembers them.
Public Sector
Private Sector
Public Corporation
Director
Owner
Board of directors
Deputy director/branch manager
Vice-president
Executive management
Division chief
Manager
Middle management
Section manager
Manager
Supervisory management
Associate
Employees
Employees
Figure 8.1 Public sector versus private sector and corporate organizations.

Establishing an Information Security Program for Local Government ‚óæ 131
In the public sector, it seems that almost every employee has worked at least 20 years or more. 
In retrospect, they may have only worked many years less. Reality sits in when many employees 
consistently echo the aforementioned phase, ‚Äúthe way we do things here.‚Äù Being the CISO attempt-
ing to implement one of your many information security initiatives, this echo seems to sound loudly 
increasing exponentially with the employees that will be aff ected by implementing a change to their 
environment. Th is type of behavior illustrates the presence of a strong organizational culture.
A strong organizational culture can control organizational behavior. For example, an organizational 
culture can block an organization from making changes that are needed to adapt to new informa-
tion technologies. From the organizational culture perspective, systems of formal rules, authority, 
and norms of rational behavior do not restrain the personal references of organization employees. 
Instead, they are controlled by cultural norms, values, beliefs, and assumptions. In order to under-
stand or predict how an organization will behave under varying circumstances, one must know and 
understand the organization‚Äôs patterns of basic assumptions‚Äîits organizational culture.
Organizational cultures diff er for several reasons. Some organizational cultures are more distinc-
tive than others. Some organizations have strong, unifi ed, pervasive cultures, whereas others have 
weaker or less pervasive ones; some cultures are quite pervasive, whereas others may have many sub-
cultures existing in diff erent functional or geographical areas.
In contrast, there are some ‚Äúprescriptive aphorisms‚Äù or ‚Äúspecifi c considerations in changing 
organizational cultures‚Äù; when this occurs, your information security program (i.e., Lucky 7) 
along with its processes and practices will fl ourish with positive outcomes:
Capitalize on propitious moments
 
‚óæ
Combine caution with optimism
 
‚óæ
Understand resistance to culture change
 
‚óæ
Change many elements, but maintain some continuity and synergy
 
‚óæ
Recognize the importance of a planned implementation
 
‚óæ
Select, modify, and create appropriate cultural forms
 
‚óæ
Modify socialization tactics
 
‚óæ
Locate and cultivate innovative leadership
 
‚óæ
Altered organizational culture is merely the fi rst‚Äîbut essential‚Äîstep in reshaping organizations 
to become more fl exible, responsive, and customer driven. Changing an organizational culture is 
not a task to be undertaken lightly, but can be achieved over time.
Organizational Target Group
Desired Behavior
Board of supervisors/city council
Endorsement
Executive management
Priority
Middle management
Resources
Supervisory mangement
Support
Employees
Diligence
Constituents/consumers
Trust
Security program
Execution
Figure 8.2 Stakeholders‚Äô desired behavior.

132 ‚óæ Information Security Management Handbook
Organizational cultures are just one of the major tenets that constraints the establishment 
and building of an information security program. As a CISO, or an information security practi-
tioner within your organization, you should have had at least some interaction with the various 
target groups (i.e., stakeholders) to grasp an awareness of their behavior. Figure 8.2 illustrates the 
expected behavior of the target groups and the desired behavior to assist in driving your program. 
Th is chart will provide benefi ts when you rate each stakeholder from your perspective.
The Information Security Executive in the Organization
Th e University of California at Los Angeles (UCLA) legendary men‚Äôs head basketball coach John 
Wooden, wrote ‚Äúthere is a choice you have to make in everything you do, so keep in mind that 
in the end, the choice you make makes you.‚Äù Nowhere is this more evident than the relation-
ships that are established throughout your organization as well as external to the organization. 
Surround yourself with people who add value to you and encourage you. At the minimum, having 
photographs or prints hanging from your offi  ce walls of individuals that have achieved greatness, 
regardless of what industry will provide an added psychological benefi t when tough decisions 
must be made. Having established strong relationships is an excellent indicator of a strong CISO; 
however, staying visible in the organization is equally important.
People can trace the successes and failures in their lives to their most signifi cant relationships. 
Establishing relationships are part of our livelihood in terms of family, personal, professionally, 
and business. Moreover, as the CISO, when establishing an information security program and 
chairing an ISCC meeting with your security peers or colleagues in your organization, those rela-
tionships are imperative to your success. Eff ective CISOs have learned how to gain the trust and 
confi dence of the executive team. Th e CISO must remember that security is easy to sell if the focus 
is on the benefi ts to the company.
Th e CISO is the information security executive (i.e., executive management); regardless if we 
are referencing public or private (i.e., Corporate American) sector organizations. An organization‚Äôs 
CISO must address the big picture and must rely on timely and actionable risk information that 
enhances their ability to make decisions that will drive local government effi  ciencies and opera-
tional eff ectiveness.
In local government, many CISOs are using a matrix reporting structure and either report to 
the chief information offi  cer (CIO) or the CEO, and ultimately the city manager, board of super-
visors (Board), or city council (Council). Actually, this matrix model can only function in this 
fashion as long as no operations responsibilities are incorporated. In other words, the daily opera-
tional activities and tasks would collide, at the minimum, with the strategic and tactical mindset 
of the information security practitioner. Th is model has brought this author numerous successful 
implementations of information security projects and initiatives.
However, there are many other ways to organize the security function and they all have advan-
tages and disadvantages. Better CISOs understand it is not important how security is organized, 
or the hierarchical structure. Th e key to success is the support structure the CISO is able to build 
among the executive team. However, the manner in which security is organized will change the 
methods and processes a CISO will use to be successful. Eff ective CISOs will adapt their approach 
to the most advantageous organizational structure. Th e two primary organization structures most 
common are (1) matrix structure, in which the CISO is an enterprise-level (or corporate level for 
private sector) organization and the security staff  report in the business lines, or (2) the CISO has 
direct responsibility for the implementation and operations of security.

Establishing an Information Security Program for Local Government ‚óæ 133
Smart CISOs understand that they do not need to have all the security staff  in their direct 
reporting line. Be ready for decentralization. Being a powerful CISO is not about how many staff  
you manage; it is about how many staff  you can infl uence. Drive the diff erence of security any way 
you can‚Äîthrough direct staff , matrix staff , and supporting staff  to reach the security program 
goals and initiatives. Large organizations have already implemented a matrix organization or are 
seriously reviewing how to manage the business lines more eff ectively. Be prepared to manage in 
a matrix organization.
Regardless of the reporting structure, decisions must be made to eliminate press clippings in 
tomorrow‚Äôs local newspaper, or perhaps the national news. Th e CISO cannot be risk-averse. All 
information security practitioners should think quantitatively. Th is does not necessarily mean 
doing calculations. Rather, it means thinking about things in terms of the balance of arguments, 
the force of each of which depends on some magnitude.
Some local government organizations are forward-thinking companies that have recognized 
that business and information technology (IT) executives (e.g., CIO, CISO, or Chief Technology 
Offi  cer) need to establish standardized, repeatable ways to identify, prioritize, measure, and reduce 
business and technology risks, both collaboratively and eff ectively. Moreover, security executives 
who were accustomed to working in their own silo must now consider all business-related risk 
areas to align initiatives (e.g., business and applications system migration projects and customer-
based applications to enhance e-government/e-services) properly with exposures.
Collaboration and communication is sunshine on its brightest day. Team relationships and/or 
team meetings are training gold nuggets. If the opportunity exists, inviting individuals to attend 
selected meetings within your security program can go a long way to helping them understand the 
scope and breadth of security. Make them an honorary member of the team. Th is has been done 
on several occasions to break through the myopia barrier. In addition, if other groups will let you 
attend a team meeting or two, go for it. Th is seems very simple, and is, but can be unbelievably 
powerful.
It is very true that there is success in numbers from an empirical perspective, where teams 
can drive your information security program. Th ere are two types of teams that should be imple-
mented to support an information security program: proactive and reactive.
We call the proactive measures teams SETs. All of these teams develop and review policies, 
standards, procedures, and guidelines. Th ese teams are usually experienced and knowledgeable 
in terms of the technical, culture, and organizational perspectives. Th ese teams address host 
strengthening and isolation, policy and operating procedures, malware defense, and application 
security, to name a few. However, there will be opportunities where a proactive team will be 
formed to address a point-in-time project. For example, our implementation of an Internet con-
tent fi lter was a win‚Äìwin because of the formulation of a SET from development of the technical 
specifi cations to enterprise deployment. Once deployed throughout the organization, the team 
was no longer required.
A reactive team addresses an enterprise-wide CERT. Th is team reacts to situations that 
potentially impact or have impacted the enterprise network, servers, applications, workstations, 
etc. Th is is reactive in nature. However, the use of a structured methodology while respond-
ing, resolving, and reporting the incident is vital. Th e use of well-maintained and clearly writ-
ten documentation (e.g., narratives, matrixes, and diagrams) for responding to incidents, and 
using a standardized incident reporting form are crucial. It may be obvious that by defi ning 
the various types of information security incidents to report will provide one of the numerous 
performance metrics that can be established to measure a portion of the operational aspects of 
your program.

134 ‚óæ Information Security Management Handbook
Information Security Policies, Standards, 
Procedures, and Guidelines
One of the major components of an information security program is the formulation, collabo-
ration, and adoption of information security policies. Th ese written policies cannot survive without 
associated supporting standards, procedures (some private-sector organizations uses standard 
operating procedures or SOP), and guidelines. Personally, having clear, distinct, and physically 
separated policies, standards, and procedures would provide benefi ts to your overall information 
security program.
Charles Cresson Wood, well known in the information security industry as a leader for infor-
mation security policy development, has emphasized to segregate information that has diff erent 
purposes. Specifi cally, one should formulate diff erent documents for policy, standards, procedures, 
and guidelines. Th is structure provides numerous benefi ts to the responsible owner of these docu-
ments in terms of ease of modifi cation to maintain currency and relevance, reviews and approvals 
are more effi  cient, and requests for any single type of document can be distributed on a need-to-
know basis that protects the security and privacy of the written information, where applicable.
Policy is defi ned as the rules and regulations set by the organization. Policies are laid down 
by the management in compliance with applicable law, industry regulations, and the decisions 
of enterprise leaders and stakeholders. Policies, standards, and procedures are mandatory; guide-
lines are optional. However, policies can be used to clearly defi ne roles and responsibilities of the 
information security program, including the CISOs, steering committee, etc. Moreover, policies 
are written in defi nite language and require compliance. Failure to conform to policy can result in 
disciplinary action, termination of employment, and even legal action.
Information security policy governs how an organization‚Äôs information is to be protected against 
breaches of security. Familiar examples of policy include requirements for establishing an informa-
tion security program, ensuring that all laptops are deployed with automatic hard disk encryption 
software, employees‚Äô Internet usage, security awareness and training, malware (e.g., anti-virus, anti-
spam, and anti-spyware) defense, and computer incident reporting for employees, to name a few.
Information security standards can be an accepted specifi cation for software, hardware, or 
human actions. Th ese standards can be de facto, as well, when they are so widely used that new 
applications routinely respect their conventions. However, the written format is preferred and 
recommended from the perspective of an information security professional and information tech-
nology professionals including auditors.
A software standard can address a specifi c vendor‚Äôs solution for anti-virus software protection. 
In fact, from a defense-in-depth perspective an organization may be standardized on two vendor‚Äôs 
solutions. If particular organizations have implemented all Cisco Systems, Incorporated (Cisco) 
network devices, they could conclude that their hardware standard for network infrastructure is 
Cisco. Th ere are many standards to address human actions or even their behavior. For example, to 
address a potential computer security breach, a standard will address actions to be performed by 
specifi c employees‚Äô roles, responsibilities, and timelines for an appropriate response.
Procedures prescribe how people are to behave in implementing policies. For example, a policy 
might stipulate that all confi dential and private data network communications from employees 
working or traveling and desire to connect externally to the enterprise network must be encrypted. 
Th is would constitute previously identifi ed software and hardware (perhaps an adopted standard 
for communicating externally) required to be implemented based on policy. Th e corresponding 
procedure (the ‚Äúhow-to‚Äù) would explain in detail each step required to initiate a secure connection 
using a particular virtual private network (VPN) or some other technology.

Establishing an Information Security Program for Local Government ‚óæ 135
Policies, standards, and procedures as previously stated are mandatory. However, guidelines 
are not mandatory. Guidelines could be used as a documented standard or procedure where invari-
ably in the future could be transformed and adopted into a standard or procedure. Establishing 
guidelines assists in identifying the usefulness, and the trial of specifi c security controls for future 
adoption. For example, if an organization prefers the usage of Windows Mobile operating sys-
tem for all mobile devices and there is a small community within the organization that prefers 
the proprietary Blackberry device, a guideline would be feasible to address appropriate security 
controls for the Blackberry device, where a standard would address appropriate security controls 
for all Windows Mobile devices. Eventually, the Blackberry security controls guideline would be 
transformed into a standard after a greater acceptance within the organization was achieved. Th is 
eliminates the use of a de facto standard in this example.
All documents should use suitable policy resources including the aforementioned Charles 
Cresson Wood‚Äôs Information Security Policy Made Easy, government (e.g., National Institute of 
Standards and Technology (NIST), National Security Agency (NSA) Security Guidelines, and 
RFC 2196), industry bodies (e.g., International Standards Organization (ISO) 17799/27002 
and CoBIT), and commercial (e.g., Microsoft) organizations in preparing to formulate policies and 
standards.
Th e writing style should state what employees can do and what they cannot do, use of short 
sentences, written at a tenth grade level similar to the model newspapers use, review and improve 
(i.e., Sunset date), or adapt policies regularly, circulate drafts showing changes in policies to stakeholders 
and interested participants prior to adoption, and articulate major changes to senior management 
(e.g., Department Heads, Counsel, CIOs, and Privacy Offi  cers) within the enterprise.
The Information Security Organization
Th e organizational culture and behavior, the CISO as the information security executive, and the 
organization structure are the dependent variables in establishing an information security pro-
gram. Th e framework that has been proved at numerous local governments west of the Mississippi 
River, regardless of workforce size is the ‚ÄúSecurity Triangle‚Äù (Figure 8.3). Th is framework has paid 
dividends in having clearly defi ned roles and responsibilities, while addressing defense and off ense 
strategies. In other words, these strategies are the previously stated reactive and proactive teams 
that allow for continual collaboration with stakeholders vertically and horizontally throughout the 
public-sector organization.
Th e following Information Security Strategic Organization diagram (i.e., Security Triangle) 
depicts an example from a local government (i.e., county government). It illustrates the CISO at 
the top of the organization that may report to a CIO or CEO, as previously stated. Th e ISSC is 
composed of the DISOs. Th is will provide a forum for all information security-related collaboration 
and decision-making. Th is deliberative body will weigh the balance between heightened security 
and departments performing their individual business. Th e ISSC responsibilities will be to
Develop, review, and recommend information security policies
 
‚óæ
Develop, review, and approve best practices, standards, guidelines, and procedures
 
‚óæ
Coordinate interdepartmental communication and collaboration
 
‚óæ
Coordinate countywide education and awareness
 
‚óæ
Coordinate countywide purchasing and licensing
 
‚óæ
Adopt security standards
 
‚óæ

136 ‚óæ Information Security Management Handbook
Th e DISOs are responsible for departmental security initiatives and eff orts to comply with county-
wide information security policies and activities. Th ey also represent their departments on the 
ISSC. To perform these duties, the DISO must be established at a level that provides management 
visibility, management support, and objective independence. DISO responsibilities include
Representing their department on the ISSC
 
‚óæ
Developing departmental information security systems
 
‚óæ
Developing departmental information security policies, procedures, and standards
 
‚óæ
Advising the department head on security-related issues
 
‚óæ
Department security awareness programs
 
‚óæ
Conducting system security audits
 
‚óæ
Th e countywide CCERT will respond to information security events that aff ect several departments 
within the county that must be coordinated and planned. Th e CCERT is comprised of member-
ship from the various departments, and are often members of the DCERT. Th e CCERT team 
meets biweekly to review the latest threats and vulnerabilities, and ensure that membership data 
is kept current. Th e CISO participates in their activities as well as leads the response to cyber-
related events. Eff orts include improved notifi cation and communication process, and ensure that 
weekend and after-hours response is viable. Additionally, training will be conducted to provide 
forensic capabilities to the CCERT team members, but specifi c to incident response in terms of 
maintaining chain of custody of electronic evidence.
Th e information security strategic framework (Figure 8.4) developed to support a local government 
is designed to address organization, people, processes and technology, as they relate to information 
security. Th e strategy is based on the principle that security is not a one-time event; but, must be 
ISSC
Information security steering committee
CISO
Chief information security officer
CCERT
Countywide computer
emergency response team
Technical staff
SET
Security engineering teams
County departments
DISO
DISO
DISO
DISO
DISO
DISO
Figure 8.3 Information security strategic organization ‚ÄúSecurity Triangle.‚Äù

Establishing an Information Security Program for Local Government ‚óæ 137
a continuously improving process, an emergent process that addresses changes in business require-
ments, technology changes, new threats and vulnerabilities, and a need to maintain currency with 
regard to software release levels at all levels within the security network, server, and client arena. It 
also is based on the realization that perfect security is an impossible goal and that eff orts to secure 
systems must be based on cost of protective measures versus risk of loss.
As the CISO or ISM, many of these protective measures are identifi ed in an information 
security strategy, as a necessity. A documented strategy that is annually reviewed is imperative to 
ensure currency of the projects and initiatives for that particular fi scal year. It is prudent that as 
the information security practitioner you align your security projects, initiatives, and activities 
with the annual budget process of the organization. Th is will provide a means and awareness to 
senior management that funding is mandatory to sustain a true information security program 
that will reduce risk. Th is strategy must clearly articulate the mission and vision of the program. 
Additionally, information security program goals and objectives are articulated in the strategy, in 
terms of short- and near-term timelines. For example, your high-level goals can be derived from 
the 12 information security domains that are articulated in the ISO 27002 standard. Th e objec-
tives will support the stated goal that should apply to your organization‚Äôs required level of security 
protections. Th e strategy will assist in the CISO‚Äôs ability to achieve the stated goals and objectives 
over a defi ned period of time.
Conclusion
Today‚Äôs information security professional practitioner is increasingly being challenged in numerous 
facets that are warranted based on the numerous threats and vulnerabilities that exists in the 
world. Government organizations are among us and throughout the world, as well. Rather, we 
are discussing local government or private-sector organization challenges, some specifi c areas are 
unique to government, such as the diversity of businesses and services under a single organization 
Culture 
Human factors 
Architecture 
Organization  
People 
Technology 
Process
‚Ä¢Detect 
‚Ä¢Respond 
Governance 
Engagement 
Enabling & 
Figure 8.4 Information security strategic framework.

138 ‚óæ Information Security Management Handbook
(i.e., county or city government), the type of businesses that warrants diff ering security and pri-
vacy protections, multiple legislations and regulations that sanctions departments within a local 
government organization, and perhaps most of all the culture issue because of the civil service 
rules that provides diffi  culty when employee termination is being considered.
Th e CISO responsibilities range from establishing and sustaining relationships with execu-
tive management, learning about the organization‚Äôs culture and behavior, constantly being visible 
and communicating the security message throughout the organization, having formulated clearly 
defi ned policies, standards, and procedures, and establishing a governance structure that com-
prises and establishes a successful information security program.
In today‚Äôs global society, a career path defi nitely exists for information security practitioners 
that would ultimately lead to holding a position as a CISO or CSO. Th is chapter including and 
this book will hopefully provide dividends throughout your career as a practitioner. However, hav-
ing a business acumen, IT experience, and enormous leadership skills are a few of the major tenets 
in striving to be an outstanding and successful CISO. On the other hand, IT training curricu-
lum does not usually include managerial skills such as leadership, team building, collaboration, 
risk assessment, communication skills including negotiations, as well as psychology, philosophy 
courses, are not usually provided.
About the Author
Robert K. Pittman, Jr., MPA, is a public sector employee and a doctoral degree candidate at the 
University of Southern California, Los Angeles, California.

Policies, Standards, 
Procedures, and Guidelines


141
9
Chapter 
A Business Case for ISO 
27001 Certifi cation
Tom Carlson and Robert Forbes
Introduction
While your organization‚Äôs marketing and sales teams attempt to leverage security as a market 
diff erentiator, information security leadership faces the daunting challenge of ‚Äúdoing more with 
less.‚Äù Th is chapter sets out the benefi ts and provides a business case for an information security 
management system (ISMS) that conforms to the ISO 27001 standard.
Background
ISO 27001, the internationally accepted and recognized standard for ISMSs, was developed and 
supported by the member nations of the International Organization of Standardization (ISO), 
an organization chartered by the United Nations. Th e ISO 27000 series of standards evolved 
from the British Standard BS 7799. Originally published in 1995, Part One of BS 7799, the 
Code of Practice (aka the implementation guide), is now the basis for ISO 27002 (formerly 
known as ISO 17799). Part Two of BS 7799, fi rst published in 1998 is the auditable ISMS set of 
specifi cations, now embodied in ISO 27001. Th ere are other standards in the series, both pub-
lished and in progress, covering ISMS implementation guidance (27003), information security 
metrics (27004), risk management (27005), and a guide to Information Security Management 
auditing (27008).
Contents
Introduction .............................................................................................................................141
Background ..............................................................................................................................141
Intended Use ............................................................................................................................142
Conclusion ...............................................................................................................................148
About the Author .....................................................................................................................148

142 ‚óæ Information Security Management Handbook
Intended Use
ISO 27001 is intended to provide guidance on how to manage information security for an orga-
nization. To expand on this, the ISO standard is focused on an organization as a whole, including 
all information types, systems, people, policies, processes, and technologies.*
An ISMS built and certifi ed to ISO 27001, in addition to its internal benefi ts to the organization, can 
also provide defensible due diligence for potential clients, users, and/or other parties. Th e latter sections of 
this chapter will demonstrate a number of benefi ts resulting from implementation of the standard.
First, let us look in detail at the ISMS and how it can be used by an organization to ‚Äúpackage for success.‚Äù
We can all agree that although we have been practicing information security for a long time, 
the management of information security has been inconsistent at best. Th e concept of a quality-
based ISMS, codifi ed in the ISO 27001 standard, is a classic example of interdiscipline synergy, 
porting proven quality management techniques into the security discipline.
What is a management system?
Management: direction or control
System: a collection of practices bundled to provide some form of service
Management System: that collection of practices to direct or control the provision of a service
Note that from the defi nitions above, the service provided could be any service from any program. 
For the purposes of this chapter, the services will be information security services.
Once an organization makes the decision to proactively direct or control its information secu-
rity activities (i.e., managing rather than practicing), an ISMS can be crafted. Th is is where we can 
borrow valuable lessons from, believe it or not, the discipline of packaging.
What are some typical functions of packaging?
It presents the product to the end user in an appealing manner
 
‚óæ
It protects the product during transport and storage
 
‚óæ
It provides instructions on how to set up and use the product
 
‚óæ
It informs the end user on where to turn for help
 
‚óæ
With these functions in mind, let us look at some common scenarios with ‚Äúbad‚Äù packaging. Any 
parent who has been severely challenged as they attempted to assemble a child‚Äôs new toy, will 
understand the impact of ‚Äúbad‚Äù packaging.
Bad Packaging Example
Problem
Appearance
Dingy box with no pictures or 
motivating text
No appeal to procure the 
product
Communication
Incoherent documentation
Message not getting across
Direction
Assembly instructions with no 
illustrations or insuffi cient detail
Inconsistency in assembly
Structure
Poorly labeled piece parts
Not clear what part is what
Packing
Damaged cartons
Missing or unusable piece parts
Feedback
Unresponsive/inadequate Customer 
support
No recourse
* Note that organizations may choose to certify all or a reduced section of their environment.

A Business Case for ISO 27001 Certifi cation ‚óæ 143
Packaging has vastly improved over the years, although bad examples certainly still exist. In contrast, 
let us look at an organization that is well known for their packaging‚Äîthe Swedish modular furniture 
merchandiser, IKEA.
IKEA Packaging
Result
Appearance
Pictures of the product in action and 
features list
Creation of desire and 
motivation
Communication
Documentation tailored to the audience 
in both proper language and content
Clear and unambiguous 
knowledge transfer
Direction
Clear instructions with suffi cient detail to 
ensure success
Consistency in assembly
Structure
Each part clearly labeled, usually with a 
spare part or two just in case
No confusion regarding 
what goes where
Packing
Engineered cartons
Safe arrival of all piece parts
Feedback
Help desk
When all else fails, the ability 
to reach out for help
Note how IKEA (and certainly others) have overcome traditional packaging problems and derived 
a marketing benefi t, as well as usable products. Th rough the use of focus groups and customer feed-
back, their products are attractively packaged, with a high probability of customer satisfaction.
By now, you are probably saying to yourself, ‚ÄúWhat does this have to do with information security?‚Äù
As a tactical initiative to manage a strategic information security program, an ISMS must
Be supported by those that use or are bound by it
 
‚óæ
Be easy to use and maintain for those that are aff ected
 
‚óæ
Have a mechanism for stakeholder satisfaction
 
‚óæ
Returning to our packaging analogy, here are some typical ‚Äúbad‚Äù packaging issues common in 
the security arena:
Management Example
Problem
Appearance
No benefi t identifi ed for stakeholders
No appeal to participate
Communication
Documentation written with 
inconsistent or inappropriate use of 
terminology
Message not getting across
Direction
No defi ned processes
Inconsistency in execution
Structure
Undefi ned taxonomy
Not clear what controls goes 
where and at what level of detail
Packing
Best practice (‚Äúbest guess‚Äù) based 
selection of controls
Inadequate or missing controls
Feedback
Unknown/undefi ned responsibilities
No accountability

144 ‚óæ Information Security Management Handbook
Here are some typical solutions enacting by nothing more than proper packaging of existing practices 
already being done by most information security organizations:
ISMS Packaging
Result
Appearance
The Service Delivery model provides 
for clear defi nition of benefi ts and can 
be integrated into the ITILa service 
catalog
Services can be ‚Äúmarketed‚Äù 
within the organization with 
clearly defi ned benefi ts. In 
addition, service satisfaction 
can be measured
Communication
A glossary of terms assists in linguistic 
consistency
Clear communication
Direction
Defi ned and documented enterprise 
standards and processes
Clear direction at all levels as 
to what to do and how to do it
Structure
Well-defi ned documentation taxonomy
Identifi cation of audiences 
and audience needs
Packing
Risk-based selection of controls
Defensible and measurable 
controls
Feedback
Clearly defi ned roles and 
responsibilities
Accountability
a ITIL: Information Technology Infrastructure Library: Concepts and policies for managing 
information technology infrastructure, development, and operations.
Note how eff ective ISMS packaging has overcome traditional management problems and creates 
both stakeholder appeal, as well as tangible management benefi ts. Th rough the inclusion of 
stakeholders in a facilitated process, as well as attention to stakeholder feedback, the ISMS is both 
comprehensive and comprehensible, with a high probability of stakeholder satisfaction.
Standards such as the ISO 27001 and ISO 9001 intentionally specify only the requirements 
of a management system, and are implementation-neutral. ISMS implementations may therefore 
vary both in look and feel. Proper packaging, however, makes a huge contribution to success.
Once the decision is made to do more than practice information security, the next logical con-
clusion is to create an ISMS based upon the proven quality concepts embedded in ISO 27001. But 
not all management systems are created equal. Although an ISMS may meet the requirements of 
ISO 27001, proper packaging can make the diff erence between a ‚Äúlip service‚Äù management system 
and a true management system that brings actual added value to the organization.
Th en, and only then, will an organization yield the benefi ts as detailed below:
 
1. Market diff erentiation
 
 Th e ISO 27001 certifi cation is accepted globally, and its adoption rate in the United States 
while still not comparable to some other nations, is on the rise.* Organizations, large and 
small, have felt increasing pressure from current customers, potential customers, and regula-
tors, to adopt a defensible, risk-based ISMS, as opposed to abiding by the customary and 
* See www.isocertifi cates.com for a complete listing of organizations who have attained the ISO 27001 
certifi cation.

A Business Case for ISO 27001 Certifi cation ‚óæ 145
vague reliance on ‚Äúbest practices‚Äù or other standards that are not specifi c to the discipline 
of information security, e.g., SAS 70 Type II. Th e eff ort involved in raising the maturity of 
the security program to a certifi able level is proof to clients and potential clients that your 
organization is actively maintaining its information security posture.
Benefi t: Th e ability to stand apart from your competition. Attaining ISO 27001 certifi cation 
means joining an exclusive group of companies and is a highly eff ective market diff erentia-
tor for your company. Your competitors are most likely already looking at or moving toward 
ISO 27001 certifi cation. You can get there fi rst.
 Bottom line impact:
 
a. Increased selling opportunities by off ering a mature and capable ISMS, certifi ed to an 
international standard.
 
b. A greater potential to land business where touting your company‚Äôs security is a critical 
element, including opportunities to work with clients seeking to do business with a 
company that has a certifi ed security program already in place.
 
2. Proactive versus reactive security management
 
 ISO 27001 provides a set of criteria in the form of management system requirements and 
control objectives that are based on intelligent and risk-based practice from various indus-
tries and countries. Organizations can then use these criteria as the basis to determine what 
they should be doing to manage information security, and the fl exibility to decide how. Th is 
allows the information security function to be proactive in developing, deploying, manag-
ing, and maintaining an information security program. Information security is no longer 
forced into a constant ‚Äúfi re-fi ghting‚Äù mode and the usual lack of effi  ciencies is avoided.
In turn, a proactive, defensible approach to information security yields a reduction in 
response eff ort to the rising volume of information security questionnaires that an orga-
nization receives from clients and potential clients. Given the increasingly cumbersome 
regulatory environment, detailed inquiries are often defended as ‚Äúdoing due diligence,‚Äù 
even though such inquiries impose a signifi cant time and workload burden on the receiv-
ing organization. However, with proactive security management, the organization has a 
ready answer to any and all security questions, and has no need to ‚Äúreinvent the wheel‚Äù 
every time a new inquiry is received. Often, customers are willing to accept the ISO 
27001 certifi cation in lieu of answering lengthy and proprietary questionnaires. Further, 
security-conscious organizations are hesitant to provide detailed information regarding 
implemented controls; thus, a comprehensive response such as ‚ÄúWe are ISO 27001 certifi ed‚Äù 
is preferred.
 
 Benefi t: Holding an ISO 27001 certifi cation is widely accepted proof of a reliable, defensible, 
standards-based information security posture. It confi rms to both management and clients 
that your organization is proactively managing its security control responsibilities.
 
 Bottom line impact: Reduced eff ort and time to respond to inquiries; shortening the sales 
cycle, and reducing the number of audit or review cycles, thereby increasing effi  ciencies.
 
3. Information risk management
 
 ISO 27001, with its process-based and risk-driven approach, provides a mechanism to inte-
grate information security into your company‚Äôs overall risk management strategy. Using 
the common language of risk management, business executives can now be presented with 
information security in its proper context of asset protection and risk mitigation, without a 
need to explain the intricacies or jargon of the discipline.
 
 Benefi t: By making information security decisions on the defensible basis of risk management, 
the information security practitioner and business manager can employ a common terminology. 

146 ‚óæ Information Security Management Handbook
In addition, the information security function becomes more integrated with the organization as 
a whole.
 
 Bottom line impact: Increased understanding and acceptance of the role of information 
security in the organization‚Äôs overall risk management strategy.
 
4. Time-based assurance
 
 Adoption of the ISO standard requires implementation of an ongoing management compo-
nent or ‚Äúcontinuous process improvement.‚Äù Organizations are required to not only identify 
what is in place now, but monitor, review, and change controls if the environment dictates 
such change. ISO 27001, like other ISO management standards, is based on the W. Edwards 
Deming model of Plan, Do, Check, Act (PDCA) to achieve continuous improvement.
If your organization must respond to customer security inquiries, there is nearly always a 
requirement for annual renewal or periodic review. Once certifi ed under ISO, the ISMS will 
be subject to annual surveillance audits and recertifi cation every 3 years. Th ese independent 
audits performed by the certifying authority off er proof to your management and your clients 
that the ISMS is operating in a satisfactory manner with continuous improvement.
 
 Benefi t: ISO 27001 certifi cation is a dynamic process, requiring at least annual audits and 
periodic renewal of the certifi cation. Th is off ers independent proof of ISMS adequacy 
and the ongoing benefit of continuous process improvement. It offers clients and 
management proof that the ISMS continues to meet due diligence.
 Bottom line impact:
 
a. Proves to management that the program is operating eff ectively and has a positive return 
on investment.
 
b. Reduces the eff ort to provide ongoing compliance assurance to customers and 
regulators.
 
5. Process defi nition and metrics
 
 Another benefi t of ISO 27001 is its requirement to defi ne information security services and 
the supporting processes. For some organizations, it will be the fi rst time they have thor-
oughly addressed and defi ned the structure of their information security group. In other 
cases, the implementation of the standard yields defi ned process fl ows and assigned respon-
sibilities for services delivered both to ‚Äúcustomers‚Äù within the organization and for services 
delivered to information security by other parts of the organization, such as Information 
Technology, Human Resources, Audit and Legal Counsel. By defi ning processes, inputs, 
outputs, and responsibilities, the role of information security is emphasized and awareness is 
increased across the organization.
Process defi nition also yields an unambiguous basis for security metrics. Th ese metrics 
are essential to measure both the eff ectiveness of the program and its progress through the 
PDCA or continuous improvement cycle.
 
 Benefi t: Management gains a clear window into the results of its security investment, and 
better insight info which security processes are working well and which need improvement. 
Th is increased visibility helps to make the case for the information security group and often 
can serve as a model for other parts of the organization.
 
 Bottom line impact: Concrete results and metrics help to justify security budgets. Better 
management understanding of the challenges and opportunities faced by the information 
security function leads potentially to both a larger role in the organization and the ability 
to at least sustain and possibly increase management funding. Moreover, metrics can be 
used to demonstrate opportunities to streamline processes and make more effi  cient use of 
available resources.

A Business Case for ISO 27001 Certifi cation ‚óæ 147
 
6. Consistent third-party governance, risk, and compliance (GRC) management
 
 Consistency between internal and external parties is another challenge organizations face 
today, and the problem is only getting worse. How can you make sure that your require-
ments are being implemented, measured, managed, and communicated? Contract or service 
agreement language often does not address specifi c requirements for the preservation of 
information confi dentiality, integrity, and availability. A supplier risk assessment or audit 
can check to see if security expectations are adequately met, but by itself, this activity does 
not communicate the actual requirements or criteria.
With an ISO 27001-based ISMS, third-party requirements, specifi cations, empower-
ment, and communication are an integral part of the system. Th ese elements can then be 
provided to the third parties or service providers. What does this mean? It means that you 
can raise your level of assurance by knowing that the third parties are ‚Äúon the same page‚Äù 
as your company. Suppliers are able to deliver services at desired levels and with processes 
and security measures which are defi ned, visible, and accountable to you.
 
 Benefi t: Clear communication of security requirements to third parties and scheduled 
periodic reviews of compliance with such requirements.
 
 Bottom line impact: Th ird parties with a full understanding of requirements can provide 
more accurate pricing for services and are not ‚Äúsurprised‚Äù near the end of the contract process 
with unanticipated demands. Periodic compliance assessments become a scheduled part of 
third-party governance with specifi c, stated objectives and increased focus on defi ned 
remediation tasks where necessary.
 
7. Legal and regulatory compliance
 
 Th e legal and regulatory environment is increasingly more rigorous, and unfortunately, increasingly 
more burdensome. Recently introduced law and regulation often requires a risk-based approach 
and informed-choice decision making to achieve compliance. Both of these qualities are inherent 
in an ISO 27001 ISMS, along with a defi ned responsibility for the Legal department to advise 
security of pending legislation. A risk-based, structured approach to security management, policies 
and standards, means accommodating shifts in the regulatory environment can often be accom-
plished as part of the normal review and update cycle rather than an ad hoc, reactive mode. When 
changes are required, they can be accomplished incrementally rather than as a major overhaul.
 
 Benefi t: Th e risk-based decision making inherent in an ISO 27001 ISMS means the system 
shares a common basis with many new legal requirements. Changes to the ISMS can be 
made in an orderly, incremental fashion.
 
 Bottom line impact: Legal and regulatory compliance is accomplished through an ongoing 
change process, often using maintenance cycles rather than unplanned eff orts or forced 
reaction. Disruption to the business is lessened, and compliance is achieved through simple 
alignment rather than repetitive and unplanned reengineering of security policies, stan-
dards, and practices.
 
8. Defensibility
 
 ISO 27001 begins by requiring organizations to defi ne a risk methodology, then to perform an 
assessment of their security practices based on this methodology. With the risk assessment in 
hand, information security and management together make informed choices regarding which 
controls must be applied, and justify these choices. Th e list of controls in Annex A of the standard 
are not simply ‚Äúbest practices‚Äù but rather a set of independent, reasoned choices formulated and 
signed off  on by more than 170 countries. Within the context of the ISMS, each choice can be 
defended on the basis of evaluated risks and defi ned controls. Th ere is no ‚Äúgray area‚Äù and no 
reliance on individual interpretation of security practices, no matter how well intended.

148 ‚óæ Information Security Management Handbook
 
 Benefi t: Referencing decision making to an independent standard and valid risk assessment 
means the organization can easily defend and justify its choices to management, customers, 
and regulators.
 
 Bottom line impact: Using a defi ned and defensible set of information security controls 
means reduced eff ort and confusion in explaining security choices. Th is can shorten audit 
cycles and provide important reassurance to both management and clients that information 
security is based on informed-choice decisions, not just common practices.
Conclusion
Th e future of assurance for information security and security risk management lies with the utilization 
of proactive frameworks, based upon internationally recognized standards. By providing defensible, 
risk-driven, and process-based information security practices in a manner that is packaged for success, 
the organization can achieve the following goals:
 
1. Increased ability to earn and maintain business from its customers
 
2. Th e ability to diff erentiate its services from those of its competitors
 
3. Speed to compliance in the legal and regulatory environment
 
4. Better alignment with management requirements and allotted resources
 
5. More comprehensive and ongoing governance over third-party services
 
6. Concrete metrics to justify security budgets
About the Author
Tom Carlson, CISSP, is a principal consultant and ISMS practice lead for Information Security 
Management Systems (ISMS) and ISO 27001 certifi cation.

149
10
Chapter 
Achieving PCI 
DSS Compliance: 
A Compliance Review
Bonnie Goins Pilewski and Christopher A. Pilewski
Contents
Payment Card Industry Data Security Standard .......................................................................150
Change Control and Confi guration Management ....................................................................151
Implementing Controls for PCI DSS Compliance ...................................................................157
Approvals (Control Process) .................................................................................................157
Leadership (Section 6, ISO 27002) ......................................................................................157
Exceptions (Section 5, ISO 27002) ......................................................................................157
Cyber Security Policy (Section 5, 27002) .............................................................................158
Asset Management (Section 7, 27002) .................................................................................158
Awareness (Section 8, ISO 27002) .......................................................................................159
Training (Section 8, ISO 27002) .........................................................................................159
Personnel Risk Assessment (Sections 4 and 8, ISO 27002) ..................................................159
Physical Access Controls (Section 9, ISO 27002) .................................................................159
Monitoring Physical Access (Section 9, ISO 27002) ............................................................159
Logging Physical Access (Section 9, ISO 27002) ..................................................................160
Disposal or Redeployment (Section 9, 27002) .....................................................................160
Maintenance and Testing (Section 9, ISO 27002) ................................................................160
Electronic Security Perimeter (Section 10, ISO 27002) ........................................................160
Monitoring Electronic Access (Sections 10, 11, and 12, ISO 27002) ...................................161
Cyber Vulnerability Assessment (Sections 10 and 12, ISO 27002) .......................................161
Test Procedures (Section 10, ISO 27002).............................................................................162
Ports and Services (Section 10, ISO 27002) .........................................................................162
Security Patch Management (Section 10, ISO 27002) .........................................................162

150 ‚óæ Information Security Management Handbook
Government, corporate, and industry compliance has become a mainstay of business life for the 
world‚Äôs population. Th is is due to the recognized need for secured operations, facilitated through 
the implementation of appropriate controls and governance mechanisms. Th is also includes secu-
rity standards for corporations, such as the Payment Card Industry Data Security Standard (PCI 
DSS), sponsored by Visa. Details on the organization‚Äôs Web site state that ‚Äúbeginning January 1, 
2008, Visa will implement a series of mandates to eliminate the use of non-secure payment appli-
cations from the Visa payment system. Th ese mandates require acquirers to ensure their merchants 
and agents do not use payment applications known to retain prohibited data elements and require 
the use of payment applications that adhere to Visa‚Äôs Payment Application Best Practices (PABP). 
PABP-compliant applications help merchants and agents mitigate compromises, prevent storage 
of prohibited data and support overall compliance with the Payment Card Industry Data Security 
Standard (PCI DSS) and the Visa U.S.A. Inc. Operating Regulations.‚Äù
Th e purpose of this chapter is to educate the security practitioner in the fi eld regarding PCI 
DSS compliance requirements (Standards). Information based on the international standards (i.e., 
ISO 17799/27002 and ISO 20000), best practices, and control frameworks (i.e., CobiT, NIST) is 
also provided in the discussion of the implementation of controls within the standard.
Security practitioners and organizations alike should take note that, while this chapter is tar-
geted toward compliance with the PCI DSS, the recommendations for implementation detailed 
within could be expanded and used by the organization in building or enhancing its enterprise 
security program.
Payment Card Industry Data Security Standard
Requirement 1: Install and maintain a fi rewall confi guration to protect cardholder data
As stated in the PCI DSS version 1.2, ‚Äúall systems must be protected from unauthorized access 
from untrusted networks, whether entering the system via the Internet as e-commerce, employees‚Äô 
Internet access through desktop browsers, employees‚Äô e-mail access, dedicated connection such 
as business to business connections, via wireless networks, or via other sources. Often, seemingly 
insignifi cant paths to and from untrusted networks can provide unprotected pathways into key 
systems. Firewalls are a key protection mechanism for any computer network.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Malicious Software Prevention (Section 10, ISO 27002) .....................................................162
Security Status Monitoring (Section 10, ISO 27002) ...........................................................163
Access Control (Section 11, ISO 27002) .............................................................................163
Account Management (Section 11, ISO 27002) ..................................................................164
Cyber Security Incident Response Plan (Section 13, ISO 27002) ........................................164
Recovery Plans (Section 14, ISO 27002) .............................................................................165
Exercises (Section 14, ISO 27002) .......................................................................................165
Plan Maintenance (Section 14, ISO 27002) ........................................................................165
Backup and Restore (Section 14, ISO 27002) ......................................................................165
Testing Backup Media (Section 14, ISO 27002) ..................................................................166
Conclusion ...............................................................................................................................166
About the Authors ....................................................................................................................166
References ................................................................................................................................166

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 151
Establish fi rewall and confi guration standards (1.1): Components include the following: a for-
mal change control process that ensures approval is obtained and testing occurs for all changes 
to the fi rewall confi gurations, including rulesets, and for network connections; a current network 
diagram, including wireless networks, is in place that represents the location of connections to 
cardholder data; fi rewalls present at the boundary between the zoned architecture, that is, fi rewalls 
in place at all Internet ingress/egress points and between perimeters of the DMZ, particularly at 
the boundary between the DMZ and cardholder systems and data; detailed information about 
the individuals and groups responsible for administration of cardholders systems and data; a full 
formal and documented business justifi cation for open ports and services; and the requirement for 
formal and documented review of all fi rewall confi gurations and rulesets every 6 months.
Build a fi rewall confi guration that restricts connections between untrusted networks, i.e., 
external and any system components in the cardholder data environment (1.2): Components 
include the following: the restriction of both inbound and outbound traffi  c to only that which 
is necessary for the environment; ensure that router confi guration fi les are properly secured and 
synchronized; and implement perimeter fi rewalls between any wireless networks and allow only 
the traffi  c required for the cardholder environment, denying all other traffi  c.
Prohibit direct public access between the Internet and any system component in the card-
holder data environment (1.3): Components include the following: implement a DMZ to 
restrict protocols traveling from the public zone (i.e., the Internet) to the secured zone (i.e., 
cardholder data is present); restrict public traffi  c to the DMZ only (i.e., public access to 
the secured zone is prohibited); prohibit any direct routes between the public zone and the 
secured zone; prohibit the passing of internal addresses from the public zone to the DMZ; 
outbound traffi  c from the secured zone must pass through IP addresses in the DMZ only (i.e., 
no direct outbound traffi  c from the secured zone to the public zone is allowed); implement 
stateful inspection (i.e., dynamic packet fi ltering); locate any databases containing cardholder 
data in the secured area; implement IP masquerading (i.e., NAT, PAT) to prevent internal IP 
addresses from being exposed to the public zone.
Install personal fi rewall software on any mobile and/or employee-owned computers with direct 
connectivity to the Internet (for example, laptops used by employees), which are used to access 
the organization‚Äôs network (1.4): Th is component ensures that mobile and computing devices, 
particularly those not within control of the organization, maintain at minimum the same level of 
protection as organizational assets.
Change Control and Confi guration Management
Th e foundation for the implementation of robust change control and confi guration management 
processes must start with formal, documented, and approved policies, standards, and procedures 
detailing the processes. Th e practitioner may wish to use ISO 20000 or IT Infrastructure Library 
(ITIL) practices to build an internal change control and confi guration management capability. It 
is important to note that provisions for emergency changes must also be formally documented and 
approved by senior management.
Implementation guidance for this eff ort can be found in Section 9 (Control processes) of the 
Code of Practice for Information Technology Service Management (ISO 20000-2).
Requirement 2: Do not use vendor-supplied defaults for system passwords and other security parameters
As stated in version 1.2 of the PCI DSS, ‚ÄúMalicious individuals (external and internal to a com-
pany) often use vendor default passwords and other vendor default settings to compromise systems. 

152 ‚óæ Information Security Management Handbook
Th ese passwords and settings are well known by hacker communities and are easily determined 
via public information.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Default passwords provided by hardware and software vendors must always be changed by the 
organization (2.1): Components include the following: all default settings in wireless networks must 
be changed and strong encryption for authentication and transmission must be implemented.
Standards confi gurations must be developed for all cardholder system components (2.2): 
Components include the following: all cardholder systems must be dedicated, with the data segre-
gated; disable all unnecessary ports, services, and functions on cardholder systems; ensure security 
functions are enabled on cardholder systems to prevent unauthorized use.
Encrypt all access to cardholder systems that is not performed directly at the console (2.3). 
Th is ensures that data streams cannot be read by unauthorized persons or systems.
If cardholder systems are collocated with a shared hosting provider, the organization must 
meet requirements as dictated by Visa. In particular, cardholder systems and data must be segre-
gated from other systems at the collocation (2.4). Th is ensures that the confi dentiality and integ-
rity of cardholder data are not breached by unauthorized persons or systems.
Requirement 3: Protect stored cardholder data
As stated in version 1.2 of the PCI DSS, ‚ÄúProtection methods such as encryption, truncation, mask-
ing, and hashing are critical components of cardholder data protection. If an intruder circumvents 
other network security controls and gains access to encrypted data, without the proper cryptographic 
keys, the data is unreadable and unusable to that person. Other eff ective methods of protecting 
stored data should be considered as potential risk mitigation opportunities. For example, methods 
for minimizing risk include not storing cardholder data unless absolutely necessary.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Keep cardholder data storage to a minimum. Develop a data retention and disposal policy. 
Limit storage amount and retention time to that which is required for business, legal, and/or regu-
latory purposes, as documented in the data retention policy (3.1). Th is will ensure that cardholder 
data can be properly maintained and protected by the organization.
Authentication data must not be stored after authorization, regardless of the protection mech-
anisms used (3.2). Components include the following: storing the full contents of any of the 
magnetic strip data from the card is strictly prohibited; the card verifi cation code (CVC) or value 
(CVV) may not be stored in any media post-authentication; storage of the personal identifi cation 
number (PIN) or encrypted PIN block is strictly prohibited.
Account numbers (PAN) must be masked when displayed (3.3). Th is ensures that the account 
number will not be accidentally divulged to unauthorized persons through shoulder surfi ng.
Th e PAN must not be readable when stored (3.4). Components include the following: in the 
event disk encryption is used to protect the PAN, the decryption keys cannot be tied to a user or 
system account.
Cryptographic keys must be protected against disclosure and/or misuse (3.5). Components 
include the following: keys must be restricted to as few custodians as possible; keys must be 
securely stored while in use and in escrow.
Ensure that all policies, standards, procedures, and processes for key generation, key distribu-
tion, key management, implementation, storage, revocation and escrow are formally documented 
and implemented (3.6), to include acknowledgement of responsibility for these functions.

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 153
Requirement 4: Encrypt transmission of cardholder data across open, public networks
As stated in version 1.2 of the PCI DSS, ‚ÄúSensitive information must be encrypted during trans-
mission over networks that are easily accessed by malicious individuals. Misconfi gured wireless 
networks and vulnerabilities in legacy encryption and authentication protocols can be continued 
targets of malicious individuals who exploit these vulnerabilities to gain privileged access to card-
holder data environments.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
During transmission of cardholder data over an open and/or public network, strong encryption 
and security protocols must be used (4.1). Th is includes transmission from wireless networks.
Account numbers must never be sent unencrypted over any medium (4.2).
Requirement 5: Use and regularly update antivirus software or programs
As stated in version 1.2 of the PCI DSS, ‚ÄúMalicious software, commonly referred to as ‚Äúmalware‚Äù‚Äî
including viruses, worms, and Trojans‚Äîenters the network during many business approved activ-
ities including employees‚Äô e-mail and use of the Internet, mobile computers, and storage devices, 
resulting in the exploitation of system vulnerabilities. Anti-virus software must be used on all 
systems commonly aff ected by malware to protect systems from current and evolving malicious 
software threats.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Implement antivirus software on all systems that house cardholder data (5.1): ensure that these 
programs can detect, recover, and protect against all malware.
Ensure that all installed antivirus software is activated, running eff ectively and is generating 
audit logs (5.2).
Requirement 6: Develop and maintain secure systems and applications
As stated in version 1.2 of the PCI DSS, ‚ÄúUnscrupulous individuals use security vulnerabilities to 
gain privileged access to systems. Many of these vulnerabilities are fi xed by vendor-provided secu-
rity patches, which must be installed by the entities that manage the systems. All critical systems 
must have the most recently released, appropriate software patches to protect against exploitation 
and compromise of cardholder data by malicious individuals and malicious software.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Ensure that all systems holding cardholder data are patched as required, with critical patches 
installed no later than 30 days after issuance (6.1). Th is will assist in ensuring that exposures are 
corrected within operating systems and applications.
Build a vulnerability assessment process (6.2). Th is will help the organization to continue 
to identify exposure (vulnerabilities) as they exist in the organization‚Äôs network, infrastructure, 
applications, and physical facilities.
Develop software in accordance with the Software Development Lifecycle (SDLC), industry best 
practices and the PCI DSS (6.3). Components include the following: testing of all security patches, to 
include validation of input, error handling, secure cryptographic mechanisms, secure communications 
and role-based access control implementation; separation of test and development environments, as well 
as administration and use of these environments; the use of test data only (no live account numbers) for 
testing in the development/test environment; segregation of administration of these environments; and 
the removal of any test data prior to going live with the systems or updates; removal of any custom items 
introduced into the code prior to production; review of the source code for any vulnerabilities.

154 ‚óæ Information Security Management Handbook
Implement change control procedures to any system components (6.4), including documenta-
tion of impact, senior management approval, testing, and a formal back-out plan.
Develop all code based on secure coding standards, such as those present in the OWASP stan-
dards (6.5), including, cross-site scripting (XSS), injections, malicious code execution, information 
data leakage, unsecure direct object references, cross site request forgery, broken authentication man-
agement, unsecure cryptographic access and communications, and failure to restrict URL access.
Establish vulnerability assessment methods for applications (6.6); this will assist the organiza-
tion in performing a comprehensive vulnerability management review.
Requirement 7: Restrict access to cardholder data by business need to know
As stated in version 1.2 of the PCI DSS, ‚ÄúTo ensure critical data can only be accessed by autho-
rized personnel, systems and processes must be in place to limit access based on need to know 
and according to job responsibilities. ‚ÄúNeed to know‚Äù is defi ned as the case when access rights are 
granted to only the least amount of data and privileges needed to perform a job.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Limit access to cardholder systems and data to those with ‚Äúneed-to-know‚Äù (7.1). Limits to 
access include limited access to privileged accounts (i.e., administrator accounts), assignment 
of permissions based on role, authorization for receiving permission to access an organization‚Äôs 
resources and the implementation of software to assist with provisioning access control.
Establish control of systems that have multiple users based on users‚Äô need to know (7.2) and 
include coverage of system components, assignment of access based on role, and setting a ‚Äúdeny-
all‚Äù default setting.
Requirement 8: Assign a unique id to each person with computer access
As stated in version 1.2 of the PCI DSS, ‚ÄúAssigning a unique identifi cation (ID) to each person 
with access ensures that each individual is uniquely accountable for his or her actions. When such 
accountability is in place; actions taken on critical data and systems are performed by, and can be 
traced to, known and authorized users.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
All users must have a unique user ID to log into cardholder data and systems (8.1).
Along with this ID, a user must also have either a password or two-factor authentication 
mechanisms to assist with login (8.2).
Incorporate two-factor authentication for remote access (network-level access originating from 
outside the network) to the network by employees, administrators, and third parties (8.3).
Render all passwords unreadable in storage and transmission through the use of strong encryp-
tion (8.4).
Ensure proper user authentication and password management is implemented for non-consumer 
users and administrators on all system components (8.5)
Requirement 9: Restrict physical access to cardholder data
As stated in version 1.2 of the PCI DSS, ‚ÄúAny physical access to data or systems that house 
cardholder data provides the opportunity for individuals to access devices or data and to remove 
systems or hardcopies, and should be appropriately restricted.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Facility entry controls should be used as a means to restrict and monitor traffic into 
and out of facilities containing cardholder systems and data (9.1). Components include the 

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 155
following: use of properly placed video cameras to record traffic; restriction on the use of 
publicly available network jacks; restriction of the use of handheld devices and wireless 
networks.
Create, implement, maintain, monitor and enforce policies, standards and procedures for 
visitors (9.2). Th is allows the organization to promote a process for tracking visitors to facilities, 
particularly those which contain cardholder systems or data.
Ensure that visits are properly processed (9.3). Components include the following: ensure visi-
tors are authorized; ensure visitors are given a badge to wear during the entire time on the site; 
ensure that visitors are asked to return the badge prior to leaving the facility.
Implement a manual or automated visitor log to ‚Äúcheck in‚Äù visitors to the facility (9.4). 
Information captured should include name, company/title, escort, time in, and time out. Logs 
should be retained for at least 3 months.
Backups of media should be securely stored, preferably at an off site location such as the orga-
nization‚Äôs collocation site for business continuity (9.5).
Any media, be it electronic or paper, must be securely stored if it contains cardholder data (9.6).
Maintain control over all media distribution (i.e., both internal and external) (9.7). Th is 
includes media classifi cation so it can be properly handled and transporting the media by trusted 
courier.
Management must approve any move of cardholder data from its secured location (9.8).
Storage of and access to cardholder data must be strictly controlled (9.9). Th e maintenance of 
data inventory logs facilitates this process.
Cardholder data and media must be destroyed when no longer required for business purposes. 
Hardcopy materials should be appropriately shredded, burned, or made into pulp to render the 
data unreadable. Electronic data must likewise be rendered unreadable through permanent wiping 
or full media destruction, as appropriate.
Requirement 10: Track and monitor all access to network resources and cardholder data
As stated in version 1.2 of the PCI DSS, ‚ÄúLogging mechanisms and the ability to track user activities 
are critical in preventing, detecting, or minimizing the impact of a data compromise. Th e presence 
of logs in all environments allows thorough tracking, alerting, and analysis when something does 
go wrong. Determining the cause of a compromise is very diffi  cult without system activity logs.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the 
Standard:
Implement a process that links an individual user to administrative permissions (10.1) so that 
these individuals can be tracked and held accountable for any access violations correctly attributed 
to them.
Implement automated audit trails (10.2) that track: individual access to cardholder systems 
and data; monitor all audit trails from administrative-level users; track all audit log access; the 
use of authentication and identifi cation mechanisms; initialization of audit logs; and creation and 
deletion of system-level objects.
Record audit trail entries for all system components for each event (10.3). Include user iden-
tifi cation, date and time, type of event, success or failure indication, the origin of event, and the 
identity (or name) of the aff ected resource.
Synchronize all critical system clocks and times (10.4).
Secure audit trails against tampering (10.5).
Review all logs for system component issues at least once per day (10.6).
Retain the audit trail for at least 1 year, with 3 months present for review and analysis (10.7).

156 ‚óæ Information Security Management Handbook
Requirement 11: Regularly test security systems and processes
As stated in version 1.2 of the PCI DSS, ‚ÄúVulnerabilities are being discovered continually by mali-
cious individuals and researchers, and being introduced by new software. System components, 
processes, and custom software should be tested frequently to ensure security controls continue to 
refl ect a changing environment.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the Standard:
Perform wireless network vulnerability assessments at least quarterly (11.1) to determine expo-
sures present in the environment; if this is not possible, implement a wireless IDS/IPS and follow 
the monitoring processes mentioned prior in this Standard.
Run internal and external network vulnerability scans at least quarterly or whenever there is a 
change in the network status (11.2) to determine exposures present in the environment.
Perform internal and external network and application penetration testing at least quarterly or 
whenever a signifi cant change is introduced into the network to determine exposures present in 
the environment (11.3).
Monitor all network traffi  c with intrusion detection or intrusion prevention systems to alert 
authorized persons of any issues with traffi  c or intrusion (11.4).
Deploy fi le integrity software that alerts authorized parties of tampering with cardholder fi les 
and data (11.5). Ensure that fi le comparisons are performed at least weekly.
Requirement 12: Maintain a policy that addresses information security for employees and contractors
As stated in version 1.2 of the PCI DSS, ‚ÄúA strong security policy sets the security tone for the 
whole company and informs employees what is expected of them. All employees should be aware 
of the sensitivity of data and their responsibilities for protecting it. For the purposes of this requirement, 
‚Äúemployees‚Äù refers to full-time and part-time employees, temporary employees and personnel, and 
contractors and consultants who are ‚Äúresident‚Äù on the company‚Äôs site.‚Äù
Th e following components contribute to this requirement, as stated in Version 1.2 of the Standard:
Create, implement, maintain, monitor and enforce a formal and documented security pol-
icy which addresses PCI DSS concerns, requires a threat, risk and vulnerability assessment and 
includes a review at least once per year or when the environment changes (12.1).
Create, implement, maintain, monitor and enforce formal and documented standards and 
procedures which operationalize the security policy (12.2). Ensure there is a provision for review 
and update of the deliverables at least annually or when there is a change to the environment.
Create, implement, maintain, monitor and enforce formal and documented appropriate use 
policies for the technologies used with cardholder systems and data (12.3). Th is includes formal 
and documented management approval, authentication for technology use, a list of all devices 
and authorized users, labeling of devices with the owner, his or her contact information and the 
purpose of the device, acceptable use and network location, a list of approved products, automatic 
disconnection from remote access after a prescribed period of discontinued use, activation of ven-
dor products for remote maintenance only when needed to perform the job function.
Prohibit the download of cardholder data to local machines or mobile devices (12.4) when 
connecting remotely.
Assign security responsibilities to an owner (12.5), to include creation, implementation, main-
tenance, monitoring and enforcement of a formal and documented security policy; the monitoring 
and subsequent analysis of security alerts, as well as distribution to the proper resource(s); creation, 
implementation, maintenance, monitoring and enforcement of a formal and documented incident 
management and reporting plan; administration of user accounts; monitoring and controlling of 
all access to cardholder data.

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 157
Create, implement and maintain a security awareness training program (12.6) that is presented 
to resources at last annually and is formally acknowledged by all staff  and contractors.
Perform employee background checks on all staff  working with cardholder data and systems 
(12.7).
If cardholder data is shared with a service provider (12.8), prepare a list of all service provid-
ers, maintain a formal and documented service-level agreement for cardholder systems and data, 
ensure vendors are screened to determine if they are appropriate to task and develop a formal and 
documented program to monitor service from the vendor.
Create, implement, maintain, monitor and enforce a formal and documented incident response 
plan (12.9), in order to be prepared for internal or external attacks.
Implementing Controls for PCI DSS Compliance
Approvals (Control Process)
Approvals can be completed and documented through the use of either an automated workfl ow 
process or a manual review cycle. Proper documentation includes authorized signature on the 
confi guration itself or on a cover sheet that is attached to the requirement or change control deliverables. 
An archived and/or electronic copy of the approval should be securely maintained for future use. 
Th is recommendation can be used for all areas of the PCI DSS which require such approvals.
Leadership (Section 6, ISO 27002)
Assignment of accountable senior management for completion of PCI compliance is critical to the 
organization‚Äôs success. While it is not mandatory that the same resource be dedicated throughout 
the life of the compliance initiative, dedication provides continuity and stability to this important 
initiative. Formal documentation of the assignment, along with signatures from senior manage-
ment, should be completed as part of this compliance objective.
Exceptions (Section 5, ISO 27002)
While it is advisable to construct policies, standards, procedures, plans and programs such that 
they may govern the organization‚Äôs PCI compliance function without the need for exceptions, it 
is clear that an exception over time may be required. Th e organization should implement a formal 
and documented exception process that is followed for every exception requested. Information 
documenting the exception should, at minimum, include
Requestor
Department
Date of request
Aff ected policy deliverable
Scope of the exception
Reason for the exception request
Documentation to support the need for the exception
Signature of departmental manager indicating the exception has been reviewed and is autho-
rized by departmental management

158 ‚óæ Information Security Management Handbook
Approval from senior management prior to authorizing the exception and noting it in the 
policy deliverable
Th e exception process should itself be reviewed annually, approved by senior management and 
updated as required.
Cyber Security Policy (Section 5, 27002)
Paramount to communicating expectations to staff , business partners and third parties is the 
creation of a comprehensive security policy, relative to people, process, data, technology, and facili-
ties. Th e policy should encompass all identifi ed areas of the Code of Practice for Information 
Security Management (ISO 17799/27002).
Sample policies are available in abundance on the Web, covering a variety of topics. 
Policies are communication of management expectations and should not, therefore, detail 
how to perform a function; this is left to procedural documentation. A policy should contain, 
at minimum
Introduction or background
Purpose
Scope
Defi nitions
Policy statement
References, including corresponding policies, standards, and procedures
Information on the version and eff ective dates
Approval by senior management
It is important to note that processes for implementation, maintenance (including annual review 
and update), monitoring for eff ectiveness and enforcement must also be considered and, if imple-
mented, reviewed, and updated at least annually.
Asset Management (Section 7, 27002)
In its truest sense, asset management surrounds the data (information) classifi cation eff ort 
within the organization, as it applies to cardholder information assets. Once classifi cation is 
completed, proper controls must be applied to protect the information. Th e implementation 
guidance from ISO 27002, Section 7 (Asset Management) can assist the organization in build-
ing an appropriate classifi cation scheme and performing the assessment of information assets. 
Policies for proper labeling and handling, based on the classifi cation, should be included either 
in the security policy or implemented as a stand-alone policy requirement. Formal and docu-
mented standards and procedures should also be created, approved by senior management and 
implemented. Note that all this documentation will also require maintenance, monitoring, 
and enforcement.
In terms of applying controls to the assets for the purposes of protection, there are a number of 
frameworks, in addition to the ISO standards, that can be used to determine a desired control set 
for PCI compliance. Controls detailed in the CobiT framework, as well as controls described in 
NIST Special Publication 800:53A, will give the organization a control set from which to select its 
protections. Once controls are selected, they should be formally documented, planned, approved 
by senior management and applied to the assets.

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 159
Awareness (Section 8, ISO 27002)
Periodic security awareness training within an organization can be accomplished through a variety 
of means, including formalized in-class and computer-based training. Regardless of the method 
selected, a formalized and documented approach should be implemented. A good reference for the 
building of this program is available through NIST Special Publication 800:50. It is important to 
note that, in this case, awareness training must be provided for all staff  with PCI responsibilities 
and/or access to PCI data.
Training (Section 8, ISO 27002)
Th e same methodology as mentioned above can be used for the completion of a PCI training pro-
gram for aff ected staff . Content will vary, as the purpose of this training program is to socialize staff  
with the requirements for PCI compliance and to establish their roles in the compliance objective.
Personnel Risk Assessment (Sections 4 and 8, ISO 27002)
All staff  with access to PCI data and/or systems must be assessed for risk prior to the granting of 
access to these systems. Th ere are a variety of methods for performance of this objective; it is advis-
able for the resource or team performing this function to enlist the help of the Human Resources 
and Facilities departments to complete this requirement.
Access control considerations for this part of the PCI standard have been discussed earlier in 
this chapter.
Physical Access Controls (Section 9, ISO 27002)
Organizations are required to document, implement, and operate controls for the provision of 
physical access to cardholder systems and data. Acceptable controls for physical access include
Proximity cards
Security personnel, such as a guard contingent, centralized operations center and so on
Specialty locks (magnetic remote access control locks, mantraps, and restricted access locks)
Devices that promote two-factor authentication, such as biometric devices, tokens, and so on
Implementation guidelines for this work can be reviewed in Section 9 (Physical and Environmental 
Security) of the Code of Practice for Information Security Management (ISO 17799 (27002)), 
NIST Special Publications 800:53A, 800:39, DS5: Ensure Systems Security, DS12: Manage the 
Physical Environment (CobiT).
Monitoring Physical Access (Section 9, ISO 27002)
Physical access to cardholder systems and data must be continuously monitored (24 √ó 7 √ó 365). 
Monitoring should include both automated controls, such as alarm systems, and human controls, 
such as review of physical access points by PCI cleared personnel. Monitoring should be formal 
and documented.
Implementation guidelines for this work can be reviewed in Section 9 (Physical and 
Environmental Security) and Section 10 (Communications and Operations Management) of 

160 ‚óæ Information Security Management Handbook
the Code of Practice for Information Security Management (ISO 17799 (27002)), NIST Special 
Publications 800:53A, 800:39, DS5: Ensure Systems Security, DS12: Manage the Physical 
Environment (CobiT).
Logging Physical Access (Section 9, ISO 27002)
Physical access to PCI secured areas must be logged. Acceptable methods for logging include
Automated logging, such as those produced by use of proximity cards
Camera/DVR recording of entrance to and exit from the PCI physical access points
Manual review of visitor logs, recordings, and log fi les generated by automated means
Logging must identify the individual gaining access and the time of access. Logging must be 
performed 24 √ó 7 √ó 365.
Implementation guidelines for this work can be reviewed in Section 9 (Physical and Environmental 
Security) and Section 10 (Communications and Operations Management) of the Code of Practice 
for Information Security Management (ISO 17799 (27002)), NIST Special Publications 800:53A, 
800:39, DS5: Ensure Systems Security, DS12: Manage the Physical Environment (CobiT).
Disposal or Redeployment (Section 9, 27002)
Requirements in this area of the PCI standards mandate that cardholder systems set for either 
disposal or redeployment undergo the permanent removal of PCI information from the system. 
Th e processes must be formally documented. In addition, records of removal must be maintained 
for these systems.
Implementation guidelines for this work can be reviewed in Section 9.2.6 (Secure disposal or 
reuse of equipment) of the Code of Practice for Information Security Management (ISO 17799 
(27002)), NIST Special Publications 800:53A, 800:39 and DS12: Manage the physical environ-
ment (CobiT).
Maintenance and Testing (Section 9, ISO 27002)
In order to determine that PCI physical access controls are implemented properly and are 
operating eff ectively, they must be tested and maintained. Th is testing and maintenance cycle 
cannot exceed 1 year. Any records related to outages must be retained for a minimum of 1 year 
from the outage.
Implementation guidelines for this work can be reviewed in Section 9 (Physical and 
Environmental Security) of the Code of Practice for Information Security Management (ISO 
17799 (27002)), NIST Special Publications 800:53A, 800:39, DS5: Ensure Systems Security, 
DS12: Manage the Physical Environment (CobiT).
Electronic Security Perimeter (Section 10, ISO 27002)
In any secure architecture, creation of zones is essential to ensure that assets requiring height-
ened protection are segmented off  from more public-facing areas of the network. In addition, the 
secured zone will often receive expanded control implementation. Paramount to this activity for PCI 
DSS is the segmentation of cardholder systems and data from non-cardholder systems and data. 

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 161
When the architecture has been implemented, a detailed network diagram should be completed 
to accurately refl ect the zones, including assets and access points to those zones.
Implementation guidelines for access control can be obtained from Section 10 (Communications 
and Operations Security and Section 11 (Access Control) of the Code of Practice for Information 
Security Management (ISO 17799 (27002)), NIST Special Publications 800:53A, 800:39 and 
DS-5: Ensure Systems Security (CobiT).
Monitoring Electronic Access (Sections 10, 11, and 12, ISO 27002)
For the purposes of this section, monitoring refers to the tracking of unauthorized access into 
the electronic security perimeter, through access points to the network and to dialups using non-
routable protocols. Implementation of monitoring tools that provide automated alerts is highly 
desirable; however, it is equally important that logs generated by the monitoring are periodically 
and formally reviewed by skilled staff . Th is function can also be outsourced, should the organization 
desire. At minimum, network-based monitoring must be undertaken; however, the organization may 
also opt to conduct host-based monitoring as a part of this eff ort.
Implementation guidelines for this work can be reviewed in Section 10.10 (Monitoring) of 
the Code of Practice for Information Security Management (ISO 17799 (27002)), NIST Special 
Publications 800:53A, 800:39, DS5: Ensure Systems Security, ME1: Monitor and Evaluate IT 
Performance and ME2: Monitor and Evaluate Internal Control (CobiT).
Cyber Vulnerability Assessment (Sections 10 and 12, ISO 27002)
In order to determine whether PCI assets are appropriately protected, a technical vulnerability 
assessment must be performed at least annually on these assets. It is also highly recommended that 
the organization perform both internal and external technical security assessments for this eff ort. 
A baseline PCI assessment could include
Network discovery, using tools such as IPSonar, Nmap, others
Discovery of all access points to the secure (PCI) zone, using tools such as Internet Scanner
Port and services scanning, using tools such as ISS and Microsoft Baseline Analyzer
Scanning for default accounts and passwords, using tools such as Dumpsec
Password cracking, using tools such as John the Ripper, Cain, and Abel
Scanning SNMP community strings, using tools such as Internet Scanner
Confi guration reviews (manual process) of network devices, servers and workstations, as 
included in the PCI assets for the organization (automated or manual)
Any additional assessments services desired by the organization (such as social engineering, 
penetration testing using tools such as Metasploit, application security testing using tools 
such as AppScan, and so on)
Post-assessment, the results of the assessment, along with a description of the method for conduct-
ing the assessment, must be formally documented and presented to the organization for review 
and comment, as appropriate. Th e National Security Agency Information Assurance Method 
(NSA IAM) and NSA Information Evaluation Method (NSA IEM) are good examples of a uni-
fi ed assessment system; information on this method is publicly available at the NSA Web site.
Guidelines for this work can be reviewed in Section 12.6 (Technical Vulnerability Management) 
of the Code of Practice for Information Security Management (ISO 17799 (27002)), NIST 

162 ‚óæ Information Security Management Handbook
Special Publications 800:53A, 800:39, DS5: Ensure Systems Security, PO9: Assess and Manage 
IT Risks (CobiT) and Section 6.6.3 (Security Risk Assessment Practices) in the Code of Practice 
for Information Technology Service Management (ISO 20000-2).
Test Procedures (Section 10, ISO 27002)
It is imperative that any changes to systems containing cardholder data are tested to ensure that 
they do not adversely impact operations or any other system. Th ese include confi guration changes, 
such as the implementation of security patches, service packs, operating system upgrades, and so 
on. Formalized and documented test procedures are required for compliance in this area. All test-
ing performed must refl ect the production environment for the PCI.
Implementation guidelines for this work can be reviewed in Section 10 (Communications and 
Operations Management) of the Code of Practice for Information Security Management (ISO 
17799 (27002)), NIST Special Publications 800:53A, 800:39, DS4: Ensure Continuous Service, 
DS5: Ensure Systems Security, DS9: Manage the Confi guration (CobiT).
Ports and Services (Section 10, ISO 27002)
In order to secure systems appropriately, the ‚Äúhardening‚Äù of hosts must be completed. Th is entails 
the disabling of ports and services that are unnecessary for normal or emergency operations. If it 
is not possible to disable a port or service due to technical infeasibility or for business reasons, it is 
necessary to implement compensating controls to provide the same level of protection as would be 
achieved through disabling of the port or service.
Implementation guidelines for this work can be reviewed in Section 10 (Communications and 
Operations Management) of the Code of Practice for Information Security Management (ISO 17799 
(27002)), NIST Special Publications 800:53A, 800:39, DS4: Ensure Continuous Service, DS5: Ensure 
Systems Security, DS9: Manage the Confi guration (CobiT) and Section 9 (Control processes) of the 
Code of Practice for Information Technology Service Management (ISO 20000-2).
Security Patch Management (Section 10, ISO 27002)
Th is objective requires the organization to evaluate relevant security patches for applicability and 
implement them as appropriate. In the event that an organization decides not to implement a relevant 
patch, it must formally document the justifi cation for doing so and indicate the compensating 
controls put in place to protect the PCI assets.
Implementation guidelines for this work can be reviewed in Section 10 (Communications and 
Operations Management) of the Code of Practice for Information Security Management (ISO 17799 
(27002)), NIST Special Publications 800:53A, 800:39, DS4: Ensure Continuous Service, DS5: Ensure 
Systems Security, DS9: Manage the Confi guration (CobiT) and Section 9 (Control processes) of the 
Code of Practice for Information Technology Service Management (ISO 20000-2).
Malicious Software Prevention (Section 10, ISO 27002)
PCI assets must be protected from viruses and malicious code. Antivirus and malicious software 
tools must be implemented as a result. If for any reason the tools cannot be implemented, the 
organization must document compensating controls that provide the required protection for 
the assets.

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 163
Implementation guidelines for this work can be reviewed in Section 10 (Communications 
and Operations Management) of the Code of Practice for Information Security Management 
(ISO 17799 (27002)), NIST Special Publications 800:53A, 800:39 and DS5: Ensure Systems 
Security.
Security Status Monitoring (Section 10, ISO 27002)
Monitoring in this area of the PCI standards is related to the monitoring of systems versus the 
network. Th is is referred to as security event monitoring. Th is monitoring is typically performed 
in the server and workstation environments. Event logs can be sent to an aggregated log server for 
easier review and maintenance. Dependent upon the operating system, this can be done with or 
without an additional software package for log forwarding. Th is work can be performed in-house 
or outsourced. Th e organization must document the approach, implement automated alerting 
where possible (manual where not) and ensure that logs generated are securely stored for 90 days, 
with the exception of logs documenting reportable incidents, which must be securely stored based 
on the organization‚Äôs retention policy for incidents.
Implementation guidelines for this work can be reviewed in Section 10.10 (Monitoring) of 
the Code of Practice for Information Security Management (ISO 17799 (27002)), NIST Special 
Publications 800:53A, 800:39 and DS5: Ensure Systems Security (CobiT).
Access Control (Section 11, ISO 27002)
Access control for PCI should include considerations not only for electronic access control, but 
for physical access control as well. As a foundation, formal and documented policies, standards 
and procedures for performance of this work should be completed and approved by senior man-
agement. Th e policies may be incorporated into the security policy or may appear as stand-alone 
policies, as desired. In addition, processes performed must be formally detailed and approved as 
well. At a minimum, the following processes should be included:
Th e process for determination and assignment of permissions to staff  with need-to-know for 
PCI compliance
Th e process for creation, implementation, maintenance (including periodic review), and moni-
toring of access control lists for both electronic access and physical access to PCI assets
Th e process for periodic of review and update of electronic and physical access lists
Th e processes for provisioning of electronic and physical access, preferably based upon role 
(RBAC), including management authorization
Th e processes for deprovisioning of electronic and physical access, including management 
authorization
Th e processes for creation and assignment of access control mechanisms (such as proximity 
cards, tokens or fobs, and so on)
Th e processes for third-party electronic and physical access
Inputs to these processes would likely come from Human Resources (relative to role assignment, 
which is typically based upon job function or description), Facilities and Corporate Security 
(physical access).
Th is requirement also refl ects the need for proper control of access, through implementation 
of protections and confi guration management. In particular, the organization should ensure that 

164 ‚óæ Information Security Management Handbook
network devices, such as fi rewalls, are confi gured with rules that implicitly deny and explicit allow; 
ports and services that are not necessary for job function are disabled; appropriate use banners are 
implemented on PCI assets; and that protections implemented are formally documented.
Implementation guidelines for access control can be obtained from Section 11 (Access Control) 
of the Code of Practice for Information Security Management (ISO 17799(27002)), NIST Special 
Publications 800:53A, 800:39 and DS-5: Ensure Systems Security (CobiT).
Account Management (Section 11, ISO 27002)
While this objective shares commonality with the Access Control requirements mentioned prior 
in the chapter, there are additional requirements to be met for Account Management. Th e previ-
ous Access control section delineated the need for a formal and documented provisioning process 
to be carried out by authorized personnel. Th e same is true for meeting this compliance objective. 
In addition, there is a stated need to perform a review of access, at least annually, in a formal and 
documented fashion. Formal procedures should be documented to detail mechanisms for per-
forming this review.
Th e requirements for handling of shared, generic, and vendor default passwords are also noted 
here. Where possible, these passwords must be changed to a unique, strong password. If there is a 
business justifi cation or technical infeasibility, it must be demonstrated that there are compensating 
controls to address this issue and to protect PCI assets. Note that a policy for password maintenance 
is required for PCI compliance.
Implementation guidelines for this work can be reviewed in Section 11 (Access Control) of 
the Code of Practice for Information Security Management (ISO 17799 (27002)), NIST Special 
Publications 800:53A, 800:39 and DS5: Ensure Systems Security.
Cyber Security Incident Response Plan (Section 13, ISO 27002)
Th e Incident Response Plan must minimally include
Procedures to help to diff erentiate among PCI events, incidents, and reportable security 
incidents
Identifi cation of the PCI Incident Response Team, along with documentation of the members‚Äô 
responsibilities
Formal documentation of PCI incident handling procedures, and communication plans.
A formal and documented process for reporting all security incidents.
Formal and documented process for updating the Incident Response Plan.
Formal and documented process for an (at least) annual review of the Incident Response Plan
Formal and documented process for (at least) annual testing of the Incident Response Plan is 
required, to include at least a desktop drill, a full simulation exercise, and, if possible, the 
response to an actual security incident.
Implementation guidelines for this work can be reviewed in Section 13 (Information Security 
Incident Management) of the Code of Practice for Information Security Management (ISO 17799 
(27002)), Section 8 (Resolution processes) of the Code of Practice for Information Technology 
Service Management (ISO 20000-2), NIST Special Publications 800:53A, 800:39, DS5: Ensure 
Systems Security, DS8: Manage the Service Desk and Incidents and DS10: Manage Problems 
(CobiT).

Achieving PCI DSS Compliance: A Compliance Review ‚óæ 165
Recovery Plans (Section 14, ISO 27002)
A recovery plan (i.e., Business Continuity Plan) is required for all PCI assets. Th is Plan must be 
reviewed at least annually. Th is Plan must include specifi c actions for disasters, based on both 
severity and duration of the disaster, as well as responsibilities for those personnel aff ected by 
the Plan.
Implementation guidelines for this work can be reviewed in Section 14 (Business Continuity 
Practices) of the Code of Practice for Information Security Management (ISO 17799 (27002)), Th e 
Code of Practice for Business Continuity Management (BS 25999-1), NIST Special Publications 
800:53A, 800:39, DS4: Ensure Continuous Service, DS10: Manage Problems and DS11: Manage 
Data (CobiT).
Exercises (Section 14, ISO 27002)
A formal and documented process for (at least) annual testing of the Recovery Plan is required, to 
include at least a desktop drill, a full simulation exercise, and, if possible, the documented response 
to an actual disaster.
Implementation guidelines for this work can be reviewed in Section 14 (Business Continuity 
Management) of the Code of Practice for Information Security Management (ISO 17799 
(27002)), Th e Code of Practice for Business Continuity Management (BS 25999-1), NIST Special 
Publications 800:53A, 800:39, DS4: Ensure Continuous Service, DS10: Manage Problems and 
DS11: Manage Data (CobiT).
Plan Maintenance (Section 14, ISO 27002)
Any lessons learned as a result of a recovery should be incorporated into the Recovery Plan. Th is 
information should be added to the Recovery Plan in such a way that information which is still 
viable is preserved. Changes must be communicated to staff , either through formal training or by 
computer-based means.
Implementation guidelines for this work can be reviewed in Section 14 (Business Continuity 
Practices) of the Code of Practice for Information Security Management (ISO 17799 (27002)), Th e 
Code of Practice for Business Continuity Management (BS 25999-1), NIST Special Publications 
800:53A, 800:39, DS4: Ensure Continuous Service, DS10: Manage Problems and DS11: Manage 
Data (CobiT).
Backup and Restore (Section 14, ISO 27002)
Processes and procedures for backups and restores must be formally documented. It is also advisable 
to test these procedures to ensure they correctly capture these critical functions. Any documentation 
contributing to this work could also be included in this requirement.
Implementation guidelines for this work can be reviewed in Section 14 (Business Continuity 
Practices) of the Code of Practice for Information Security Management (ISO 17799 (27002)), Th e 
Code of Practice for Business Continuity Management (BS 25999-1), NIST Special Publications 
800:53A, 800:39, DS4: Ensure Continuous Service, DS10: Manage Problems and DS11: Manage 
Data (CobiT).

166 ‚óæ Information Security Management Handbook
Testing Backup Media (Section 14, ISO 27002)
Testing of backup media is essential to ensure that the media is viable and restores can be appro-
priately completed in a timely fashion. Backup media must be tested at least annually.
Implementation guidelines for this work can be reviewed in Section 14 (Business Continuity 
Practices) of the Code of Practice for Information Security Management (ISO 17799 (27002)), Th e 
Code of Practice for Business Continuity Management (BS 25999-1), NIST Special Publications 
800:53A, 800:39, DS4: Ensure Continuous Service, DS10: Manage Problems and DS11: Manage 
Data (CobiT).
Conclusion
Th e security, infrastructure, and compliance requirements set forth in the PCI DSS standards 
can present challenges for the organization required to comply. Fortunately for the practitioner, 
there are many resources, vetted by the international community, to assist with implementation. 
A considered selection of controls can also make the job much easier for both the practitioner and 
the organization. Regardless of the control implementation chosen, the organization should allow 
suffi  cient time, resources, and dollars for a robust compliance eff ort.
About the Authors
Bonnie Goins Pilewski, MSIS, CISSP, NSA IAM, ISS, is a senior security strategist at Isthmus 
Group, Inc., Madison, Wisconsin, where she is the co-practice leader for IGI‚Äôs security practice.
Christopher A. Pilewski, CCSA, CPA/E, FSWCE, FSLCE, MCP, is a senior security strategist 
at Isthmus Group, Inc., Madison, Wisconsin.
References
BS 25999, Part 1: Code of Practice for Business Continuity Management, 2006.
ISO/IEC 17799, International Standard: Code of Practice for Information Security, 2005.
ISO/IEC 27002, International Standard: Code of Practice for Information Security, 2007.
ISO/IEC 20000, Part 2: Code of Practice for Information Technology, 2005.
Payment Card Industry Data Security Standard version 1.2, Visa.com, 2008.

Risk Management


169
11
Chapter 
Leveraging IT Control 
Frameworks for Compliance
Todd Fitzgerald
A variety of laws and regulations have surfaced over the past decade in an attempt to strengthen the 
security of information stored within the companies to which the information assets are entrusted. 
As a result of the laws that have been enacted, various security control ‚Äústandards‚Äù and ‚Äúframe-
works‚Äù have evolved and become popular means to meet the requirements of the laws. Since laws 
and regulations are intentionally developed at a higher, ‚Äúwhat needs to happen‚Äù level vs. the ‚Äúhow 
to secure the information‚Äù level, the standards and control frameworks become valuable tools to 
ensure that security is planned, organized, implemented, tested, and monitored.
Governance, risk, and compliance (GRC) is a term that has been embraced primarily by the 
vendor community in recent years in recognition of the fact that companies are struggling with 
the plethora of controls that must be implemented to meet the extensive requirements of the laws 
and regulations. Governance is simply the structure, policies, and practices that are put in place 
by the organization to ensure that the controls are adequately communicated, carried out, and 
Contents
So, What Are the Control Frameworks .....................................................................................170
Th e World Operates on Standards .......................................................................................172
Standards Are Dynamic .......................................................................................................172
Th e ‚ÄúHow‚Äù Is Typically Left up to Us ..................................................................................173
Key Question: Why Does the Standard Exist .......................................................................174
Compliance Is Not Security‚Ä¶ But It Is a Good Start ..........................................................174
Integration of Standards and Control Frameworks ...............................................................174
Value of Audits ....................................................................................................................175
Final Th oughts: Control Framework Convergence ...............................................................175
About the Author .....................................................................................................................176
Suggested Reading ....................................................................................................................176

170 ‚óæ Information Security Management Handbook
enforced by engaging direction and support at the appropriate organizational level. Risk is the act 
of making informed decisions about the losses that the company is willing to accept given a breach 
of security and building the appropriate mitigating risk strategies to reduce the risk to acceptable 
levels defi ned by the business. Compliance is ensuring that the controls are being adhered to on an 
ongoing basis, thereby increasing the likelihood of a reduction of risk and increased adherence to 
the governance intended by the organization.
Th e three components of GRC are necessary for adequate security controls; however, implement-
ing them does not ensure that a security program is adequate. Compliance is a necessary control 
which has been recognized by governments for centuries. Criminal acts, by their very nature, are 
forms of noncompliance with the laws that are in place. Take driving a car for example. As a teenager 
obtains his or her driver‚Äôs license, the diligent parent warns about the downside of not following 
the laws, reckless driving, speeding, and paying attention to parking and vehicle regulations. Th e 
teenager says, ‚ÄúSure dad, no problem‚Äù and forgets 5 min later as they morph into their busy teenage 
social network of friends and peer pressure, away from the constant mom/dad reminders. Th ey do 
not realize at the time the consequences of their actions. Or, maybe they do subconsciously, but it is 
not the most important thought in their daily ‚Äúwork life.‚Äù Time goes on, piling up speeding tickets, 
tickets for excessive window tinting, unpaid parking tickets‚Ä¶ until one day, they have the opportu-
nity to pay their own car insurance! Th e parent at that point transfers the risk to the child, and then 
the learning of true cost of noncompliance begins. Th e risk is ultimately acknowledged and accepted, 
and new mitigating strategies are put in place, such as better driving. Organizations are made up 
of many busy ‚Äúteenagers,‚Äù each of which are infl uenced by their peer work groups and need to be 
educated as to the future costs of noncompliance to the security controls. Adopting a control frame-
work is a good start; however compliance must be addressed as an ongoing, deliberate strategy.
So, What Are the Control Frameworks?
Control frameworks and security standards are often interchangeable terms depending upon the 
creator. Just to confuse things further, ISO27001 posits an Information Security Management 
‚Äúsystem‚Äù (ISMS) and the controls are contained within the ISO27002 ‚ÄúCode of Practice.‚Äù Th e 
National Institute of Standards and Technology (NIST) Special Publication 800-53, entitled 
Recommended Security Control for Federal Information Systems, breaks the controls into 17 
control ‚Äúfamilies‚Äù and three ‚Äúclasses‚Äù (Managerial, Operational, Technical) of controls. COBIT 
defi nes a framework the same as a Control Framework, which is defi ned as a tool for business process 
owners that facilitates the discharge of their responsibilities through the provision of a supporting 
control model. Alternatively, COBIT defi nes a standard as a business practice or technology 
product that is an accepted practice endorsed by the enterprise or IT management team.
For the purposes of this discussion, control frameworks, controls, and standards are interchange-
able, as the ‚Äúintent‚Äù of each of them is to provide some defi nition to a practice or set of practices 
which if performed, will protect the organization‚Äôs information assets. Th ese consist of documented, 
executed, tested, implemented, and monitored controls which reduce the risk of threats succeeding 
against the company vulnerabilities.
Th e following are some examples of the control frameworks/standards which address information 
security requirements:
Health Insurance Portability and Accountability Act (HIPAA): Th e fi nal rule for adopting security 
standards was published in February 20, 2003, which required a series of administrative, techni-
cal, and physical security procedures for entities to use to assure the confi dentiality of Protected 

Leveraging IT Control Frameworks for Compliance ‚óæ 171
Health Information (PHI). Th e standard was intentionally non-technology specifi c and intended 
to provide scalability to small providers and large providers alike.
Federal Information Security Management Act of 2002 (FISMA): Th e primary purpose is to provide 
a comprehensive framework for ensuring the eff ectiveness of security controls over information 
resources that support federal operations and assets. Th e law also provided funding for NIST to 
develop the minimum necessary controls required to provide adequate security. Th e government 
publishes an annual report card based upon their assessment of compliance with the framework.
National Institute of Standards and Technology (NIST) Recommended Security Controls for Federal 
Information Systems (800-53): Th e standards and guidelines reference the minimum set of controls 
that must be implemented to protect the federal system based upon the risk level determined. 
Implementation of the 17 families of security controls establishes a level of ‚Äúsecurity due diligence‚Äù 
for the federal agencies and the contractors which perform work for the government. Th ese stan-
dards are very comprehensive, freely available, and an excellent resource to supplement the other 
control frameworks.
Federal Information System Controls Audit Manual (FISCAM): Issued by the General Accounting 
Offi  ce, this provides guidance for Information Systems auditors to evaluate the IT controls used 
in support of fi nancial statement audits. Th is is not an audit standard, but is included here because 
auditors are typically testing the control environment in government audits using this standard. 
Th ere has been increased emphasis on the use of NIST 800-53 controls and the NIST 800-53A 
Assessments, however FISCAM is still utilized by government auditors and, therefore, it is worth-
while to understand the contents.
ISO/IEC 27001:2005 Information Security Management Systems Requirements: Provides a model 
for establishing, implementing, operating, monitoring, reviewing, maintaining, and improving an 
ISMS. Th is was an evolution from British Standard BS7799-2 and ISO17799.
ISO/IEC 27002:2005 Information Technology Security Techniques‚ÄîCode of Practice for Information 
Security Management: Provides 11 security control clauses (Security Policy, Organizing Information 
Security, Asset Management, Human Resources Security, Physical and Environmental Security, 
Communications and Operations Management, Access Control, Information Systems Acquisition, 
Development and Maintenance, Incident Management, Business Continuity Management, and 
Compliance). Th e code of practice specifi es the controls necessary and the implementation guidance 
by specifying the controls that may be chosen to build the ISMS specifi ed through application of 
ISO/IEC 27001:2005.
Control Objectives for Information and Related Technology (COBIT): A framework and supporting 
toolset that allow managers to bridge the gap with respect to control requirements, technical 
issues and business risks, and communicate that level of control to stockholders. COBIT can be 
used to integrate other standards as an umbrella framework. COBIT gained increasing popular-
ity through implementation to demonstrate compliance with Sarbanes‚ÄìOxley regulations, which 
were enacted in 2002 to require management and the external auditor to report on internal 
controls over fi nancial reporting.
Payment Card Industry Data Security Standard (PCI DSS): A set of comprehensive requirements 
for enhancing payment account security, formed by several major credit card issuers, to facilitate 
the broad adoption of a comprehensive security standard.
Information Technology Infrastructure Library (ITIL): ITIL is a set of books published by the British 
government‚Äôs Stationary Offi  ce between 1989 and 1992 to improve IT service management. Th e 
framework contains a set of best practices for IT core operational processes such as change, release 

172 ‚óæ Information Security Management Handbook
and confi guration management, incident and problem management, capacity and availability 
management, and IT fi nancial management. ITIL‚Äôs primary contribution is showing how the 
controls can be implemented for the service management IT processes.
Security Technical Implementation Guides (STIGS) and National Security Agency (NSA) Guides: 
Confi guration standards for Department of Defense Information Assurance, however freely avail-
able and used as the basis for technical standards for many private organizations. Th ese standards, 
if implemented, support many of the high-level requirements specifi ed within requirements such 
as FISMA, HIPAA, PCI, NIST, GLBA, COBIT, ISO27001, etc.
The World Operates on Standards
Th e obvious fact about standards is that they are useful and the world is made up of many of them, 
from the minimum weight in the passenger seat that must be met before the airbag protection will 
become active, to the specifi cation of the size of a #8 screw, to the standard formats for electronic 
data interchange of electronic transactions between healthcare providers, payers, and clearing-
houses. Standards ensure that products are built to specifi cations and allow us to ‚Äúsimplify‚Äù the 
complexity of the world by creating a common deliverable and common language. Imagine if 
every time a manufacturer wanted a product built they had to design a screw that could be poten-
tially diff erent from any other screw a manufacturer created! Not only would this process be very 
expensive, it would also be very time consuming for the customer and the supplier, and would be 
very error-prone. Non-standardized processes also slow down the delivery of the product or ser-
vice. Henry Ford recognized many years ago that there were effi  ciencies and increases in quality by 
creating vehicles which looked the same and were painted the same color (black). While they were 
actually available in other colors, the primary color produced was black for effi  ciency. Imagine if 
stoplights were each made with diff erent colors to represent stop‚Äìslowdown‚Äìgo. Imagine road-
ways that used diff erent types of striping to indicate passing vs. non-passing lanes based upon 
the state that you lived in! Th e world would be very chaotic with each individual interpreting the 
colors and passing lanes as they drove, many times potentially making the wrong decision.
Sometimes we like the standards, sometimes we do not. Sometimes, they just do not make intui-
tive sense to us, nor do they seem eff ective. For example, the Transportation Security Administration 
(TSA) originally did not allow nail clippers on the airplane, and reversed the decision in 2005 after 
negative public opinion. Lighters were also subsequently allowed in July, 2007 by the Federal Aviation 
Administration. Laws, regulations, and the standards which support them are sometimes developed 
without the extensive analysis of their necessity, or in reaction to a major event and the need to ‚Äúdo 
something,‚Äù only to be rescinded later for their lack of eff ectiveness. Th is is understood, as government 
and private industry must react to make demands and situations, making decisions on the data avail-
able at the time. In defense of the TSA, decisions to limit what was brought on an airplane had to be 
made quickly on the heels of September 11, 2001, and their focus was on objects which had the poten-
tial to harm. Th us, the standard of ‚Äúno nail clippers‚Äù was enacted. Liquid restrictions were placed on 
travelers due to an incident where the chemicals could be used to create explosives. By the time of this 
publication, due to new technology scanning, the ‚Äúno liquid‚Äù standard may also disappear.
Standards Are Dynamic
Over time, the standards evolve, and they change to meet the societal and technological needs. 
While the intent of many security standards appears to stay the same over time, the underly-
ing technologies that must be supported are constantly changing. Just as in the ‚Äúno liquids‚Äù on 

Leveraging IT Control Frameworks for Compliance ‚óæ 173
airplanes were fi rst introduced, and then evolved into ‚Äúas long as the liquids are 3oz or less and fi t 
in a 1qt baggie,‚Äù and then may morph into ‚Äúno requirements at all‚Äù due to investments in more 
advanced scanning technology, information security standards also need to change.
Most control frameworks are written at a higher, broader level, which provides fl exibility to 
implement controls to satisfy the specifi c technological request. For example, the ISO27002:2005 
Information Technology Security Techniques (Code of Practice) control 10.5.1d indicates that 
‚Äúthe back-ups should be stored in a remote location, at a suffi  cient distance to escape any damage 
from a disaster at the main site.‚Äù Th is leaves much interpretation up to the implementer of the 
standard‚Äîhow far away is far enough? Before Hurricane Katrina infl icted extensive damage on 
New Orleans, Louisiana, and other surrounding areas in 2005, many individuals felt that storage 
a few miles away was suffi  cient. Today, when companies are assessing their business continuity 
plans, they typically point to Katrina, and quickly decide that 50‚Äì100+ miles away would greatly 
reduce the risk. Others have invested in new replication technologies and the availability of 
inexpensive storage to ensure availability of the information.
Changing environments necessitate the ability to change the implementation strategies to 
meet the lower cost of technology, increased eff ectiveness of controls, and conformance to emerging 
regulations.
The ‚ÄúHow‚Äù Is Typically Left up to Us
As the aforementioned example illustrates, the good news is that the standards may be written 
to be fl exible over time. Th e bad news is that they are written to be fl exible over time. In other 
words, standards often lack the specifi city of the ‚Äúhow‚Äù that would be useful to implementing the 
standard. Obviously, this is by design; however, it leaves the implementer of the standard to ‚Äúfi gure 
out‚Äù based upon the available alternatives what the best method of implementation should be for 
their particular environment and cost constraints.
Th e ‚Äúbest practices‚Äù terminology has received criticism over the past several years, as the beauty 
is in the eye of the beholder. A practice that works for one organization may not fi t for another. 
One organization may implement a policy banning USB drives due to their small size, while 
another may allow them as long as the contents are automatically encrypted with the company-
approved software. Still another may prohibit their use by policy to most users, but allow adoption 
by those which establish a business need (as specifi ed in ISO27002:2005 10.7.1f), as well as tak-
ing the additional step of controlling access through active directory authorization and a vendor 
product. Which is the ‚Äúbest practice‚Äù? It depends on the organizational culture, appetite for risk, 
cost constraints, etc. It may also be the case that the individuals within the organization do not 
have access to sensitive information, thus limiting the exposure.
Th erefore, the ‚Äúbest practice‚Äù for an organization must take in many factors not defi ned within 
the individual standard. Typically, an organization would be prudent to follow the trends within 
their particular vertical industry, and pay attention to what the ‚Äúherd‚Äù is doing. If 70% of the sheep 
are heading for the hills, it may be worth heading in that direction. It is also important to understand 
why, as the 10% going another direction (assuming 20% are standing still), may be headed to a bet-
ter best practice. In the tape backup example, maybe the 10% that are utilizing online, high-speed 
compressed disk-to-disk backup strategies are the ‚Äúbest practice‚Äù that is right for the organization.
Whatever these practices are named for our individual organizations, each must recognize that 
the practices must satisfy the standard and where they do not, suffi  cient business justifi cation and 
risk acceptance must be documented. In this manner, the standards become the reference point 
for making informed business decisions.

174 ‚óæ Information Security Management Handbook
Key Question: Why Does the Standard Exist?
Before deciding the ‚Äúhow‚Äù to implement the standard, it is a useful exercise to examine the selected 
control within the standard and analyze why does this control exist in the fi rst place? What threat 
is it addressing? What would the risk be to my organization if I decided to ignore addressing 
the control? In other words, how is implementing the control increasing the security, protection, 
or information assurance of the information assets within the organization? Understanding the 
‚Äúwhat if I don‚Äôt‚Äù can quickly lead to a deeper understanding of the ‚Äúintent‚Äù of the control, vs. trying 
to ensure compliance with every detail of the control.
For example, if there is a control within the standard which says that logs of activity to the system 
must be retained for 1 year, access must be restricted to only those with a need to know, understand-
ing why this standard exists will contribute to ‚Äúhow‚Äù it should be implemented. If the intent of the 
control is to be able to go back and analyze incidents, then the individuals who need read access are 
the systems security operations team, or those responding to the incidents. Th e fi les may also need 
to be online if there is a frequent occurrence of investigation. Alternatively, the logs may not be able 
to be reviewed due to resource (human) constraints, and may necessitate the investment in a security 
incident management tool which aggregates and correlates the information.
Understanding the intent of the control also assists in interpreting the terminology used within 
the control. Th e standards are promulgated by many diff erent organizations, committees, and 
geographic representations. Th e NIST uses terminology in the 800-53 standard (Recommended 
Security Controls for Federal Information Systems) with roots in the Government Sector that 
would be familiar to many accustomed to working for or contracting with government agencies. 
Contrast that with the IT Governance Institute‚Äôs COBIT framework that is reviewed by an inter-
nationally represented committee.
Compliance Is Not Security‚Ä¶ But It Is a Good Start
Checking off  each of the controls specifi ed within a standard is analogous to completing 
the weekend honey-do list at home‚Ä¶ it may be ‚Äúdone‚Äù at the end of the weekend, but wait for 
the household auditor to see if it was done ‚Äúwell.‚Äù When the Health Insurance Portability and 
Accountability Act (HIPAA), Gramm‚ÄìLeach‚ÄìBliley Act (GLBA), Federal Information Security 
Management Act (FISMA), Payment Card Industry Data Security Standards (PCI), and other 
regulations arrived on the scene, some organizations reviewed the ‚ÄúCompliancy with the stan-
dard‚Äù as the primary goal, and subsequently created a checklist approach to satisfying the controls 
with the minimum that would be needed for ‚Äúcompliancy,‚Äù without the benefi t of a real risk assess-
ment. Th e danger in this is that the security controls implemented may prove to be ineff ective to 
addressing the vulnerabilities of the organization and the threats that they face.
However, even though compliance with standards may not be suffi  cient to mitigate the risk 
level to an acceptable level for the organization, the fact that the organization is adopting a control 
framework provides the opportunity to create a baseline and enhance the security level over time. 
Without such a framework in place, there is less chance that the environment will be secure, as 
items can be missed too easily.
Integration of Standards and Control Frameworks
Each of the standards and control frameworks contributes in their own way and the astute security 
professional will become familiar with each of them. COBIT provides an excellent overall framework 

Leveraging IT Control Frameworks for Compliance ‚óæ 175
which ties business goals, governance drivers, business outcomes and IT resources, processes, and 
goals together. ISO27001 provides a nice framework for establishing the ISMS, ensuring that risks 
are assessed, controls are implemented, management is actively involved, and the documentation 
is up to date. NIST 800-53 provides the detailed controls with tailored enhancements with 
the specifi cations for assessing the controls (800-53A document). HIPAA Final Federal Security 
Rule provides the framework for implementing protection for the Healthcare vertical industry. Th e 
Federal Information Security Management Act (FISMA) relies on the controls specifi ed by NIST to 
comply with the regulation instead of creating a new set of controls. Th e ITIL provides the control 
areas for providing eff ective and effi  cient service delivery, and overlaps the security areas specifi ed in 
the other control areas.
Several organizations, such as NIST and the IT Governance Institute have recognized the 
commonality of these standards as evidenced by their work in mapping controls between HIPAA, 
NIST 800-53, COBIT, FISMA, ITIL, and others. While the wording, level of the control, and 
measures for assessment may have diff erent criteria, there is much commonality amongst the 
controls. For example, controls regarding confi guration management and the need to develop 
baselines may not be specifi cally called out in HIPAA and FISMA as they are in ISO27001 and 
NIST, however, the need for securing the computing devices are represented within the controls 
for technical controls and the implementation of systems security plans.
Value of Audits
Once a control framework or set of standards has been chosen and implemented, it is imperative 
that the framework be audited on a regular basis internally and externally. Gaps in process are typi-
cally uncovered during these audits, and if these gaps are mitigated quickly, over time, the security 
program becomes more complete. Care must be taken to address reasonable risk, as it is rare that an 
organization will execute every control every time. What is important is that there are mitigating 
or compensating controls that catch the anomalies before they become major issues, and prompt 
follow-up and correction actions are taken. Audit testing of the control frameworks will take many 
forms including interviewing, determining, and testing samples; performing vulnerability assess-
ments; reviewing policies and procedures; and conducting external penetration tests.
Each audit should be viewed as an opportunity to determine the eff ectiveness of the control 
framework and potentially modify the existing controls.
Final Thoughts: Control Framework Convergence
Why cannot we have just one standard? On the surface, this appears to be a simple, logical 
question. As Eckhart Tolle promotes in his book, A New Earth, individuals create stress in their 
lives by not accepting ‚Äúwhat is.‚Äù Th e reality is that laws and regulations will continue to emerge 
from diff erent organizations and as security professionals, adapting to the emerging laws and 
regulations and applying the appropriate standards and control frameworks will be key. Th is 
is not to say that effi  ciencies cannot be gained, as controls can be implemented which would 
support multiple control frameworks. Sometimes, a control only needs to be tweaked to satisfy 
multiple controls/standards.
Over time, the practices that are common do emerge and become generally accepted. Even as 
recent as a few years ago, laptops were not universally encrypted by companies, with IT depart-
ments citing the expense, complexity, lack of necessity, fi les stored on the network by company 
policy, etc. So what was the tipping point that changed the practice to companies encrypting 

176 ‚óæ Information Security Management Handbook
the laptops? It was when the veteran‚Äôs administration lost a laptop containing information on 
26.5 million veterans in 2006 which caused public outrage at the situation. Today, few compa-
nies would want to admit that they are not encrypting their laptops due to the shift in the herd 
mentality to encryption. Th e sheep have headed for the hills, and the slow sheep are vulnerable 
to being left behind. Upon reviewing the standards and control frameworks, it is clear that the 
requirements for protecting ‚Äúmobile devices and media‚Äù were specifi ed prior to 2006. It could 
be argued that proactive attention to these frameworks would save much in the long run. Th e 
Veterans Administration suff ered in terms of public reputation and fi nancially ($20 million 
lawsuit to settle claims), all which could have been avoided. What happened to this government 
agency could happen to any of our organizations if the controls are not in place. Adherence to 
the control frameworks and standards increases the likelihood that breaches will not have a 
devastating eff ect on the confi dentiality, integrity, or availability of the information assets.
Control frameworks and standards provide the roadmap to build a successful information 
security program. Once in place, continuous review of the policies, standard operating procedures, 
and implementation of the controls will enhance the eff ectiveness of the program. Monitoring 
through audits accompanied by corrective actions and tracking enables refi nement of the control 
framework and standards to reduce the risk of a security event impacting the business in a signifi -
cant way. Th ink of the various security control frameworks as each contributing in some way to 
the infrastructure of a super 6-lane freeway. Rather than managing our security programs by our-
selves, on an old gravel road at 20 miles per hour, it is time to get on the superhighway supported 
by the strong plethora of control frameworks and standards and enjoy the ride!
About the Author
Todd Fitzgerald, CISSP, CISA, CISM, is the director of systems security and systems security 
offi  cer for United Government Services, LLC, Milwaukee, Wisconsin.
Suggested Reading
Cobit 4.1, IT Governance Institute, http://www.itgi.org
Defense Information Systems Agency (DISA), Security Technical Implementation Guides (STIGS), http://
iase.disa.mil/stigs/stig
Federal Information Security Management Act of 2002 (FISMA), November 27, 2002, http://csrc.nist.gov/
drivers/documents/FISMA-fi nal.pdf
Federal Information System Controls Audit Manual (FISCAM), GAO/AIMD-12.19.6, January 1999, http://
www.gao.gov/special.pubs/12_19_6.pdf
GAO/AIMB-12.19.6, Federal Information Systems Controls Audit Manual (FISCAM), January 1999, 
http://gao.gov/special.pubs/ai12.19.6.pdf
Guide for Assessing Controls In Federal Information Systems, http://csrc.nist.gov/publications/nistpubs/800-
53A/SP800-53A-fi nal-sz.pdf
Health Insurance Portability and Accountability Act (HIPAA), August 21, 1996, http://aspe.os.dhhs.gov/
admnsimp/nprm/sec13.htm
Information Technology Infrastructure Library, http://www.itil-offi  cialsite.com/home/home.asp
ISO/IEC 27001:2005 Information Security Management Systems Requirements, http://www.iso.org/iso/
iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=42103

Leveraging IT Control Frameworks for Compliance ‚óæ 177
ISO/IEC 27002:2005 Information Technology Security Techniques‚ÄîCode of Practice for Information 
Security Management, International Organization for Standardization (ISO), http://www.iso.org/iso/
iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50297
National Institute of Standards and Technology, Special Publications, http://csrc.nist.gov/publications/
PubsSPs.html
National Security Agency, Security Confi guration Guides, http://www.nsa.gov/snac
Recommended Security Controls for Federal Information Systems, Special Publication 800-53, http://csrc.
nist.gov/publications/PubsSPs.html
Seventh Report Card on Computer Security, http://republicans.oversight.house.gov/FISMA
Tolle, E., A New Earth: Awakening to Your Life‚Äôs Purpose, Penguin, New York, 2005.


179
12
Chapter 
Rats in the Cellar and Bats 
in the Attic, ‚ÄúNot Enough 
Depth to My Security‚Äù
Ken M. Shaurette
Introduction
What comes to mind when you think of security? I guess that would depend on what one is trying 
to secure.
For an organization, technology has come a long way over the past decade, which has helped to 
detect network intrusions or breaches of security to keep critical organizational data confi dential. 
Contents
Introduction .............................................................................................................................179
You Bat‚Äôch You .........................................................................................................................180
Vulnerability Assessments and Penetration Testing ...................................................................183
Penetration Testing versus Vulnerability Assessment .............................................................183
Th e Penetration Test ............................................................................................................184
Th e Vulnerability Assessment ...............................................................................................184
Comparing and Contrasting ................................................................................................185
A Hacker‚Äôs Process ....................................................................................................................185
Social Engineering ...............................................................................................................185
Physical Security ..................................................................................................................186
Password and E-Mail/Web Site Spoofi ng .............................................................................186
Wireless Environment..........................................................................................................187
External Perimeter ...............................................................................................................189
Internal Network .................................................................................................................190
Conclusion ...............................................................................................................................190
About the Author .....................................................................................................................191

180 ‚óæ Information Security Management Handbook
With technology we now have ways to look at literally a mountain of data to determine if an incident 
is occurring or has occurred.
For an individual protecting their home, there have been advances as well. However, there are 
still the same variables that continue to cause the organization or home to be at risk. Th ose variables 
are people, time, and exposure to new and changing threats.
I would like to share a couple of stories, draw some analogies to how we secure our organiza-
tions, add some perspective to common security processes that every organization evaluates when 
performing network penetration testing and maybe, just maybe, you will fi nd some advice based 
on real-world experience related to managing risk with vulnerability assessments and penetration 
testing (Figure 12.1).
You Bat‚Äôch You
It was early summer in northern Wisconsin and I planned a trip to the cabin. Th is is the time 
of year to travel north to the cabin and get things ready for the summer. Cabins need summer 
preparations just like at the end of the season when they need preparations before winter. Th ere 
are a number of chores and preparations that must be completed so the family can enjoy another 
summer at the cabin. Th ere is always a checklist of items, which include chopping wood for camp-
fi res and clearing paths for nature walks or to provide open paths for riding the all terrain vehicle 
(ATV), but once all the chores are completed it is well worth the eff ort. Th e cabin becomes a 
refuge for rest and relaxation from the stress of hectic work schedules and the strains of day-to-day 
risk management‚Äîhelping organizations protect their critical assets.
Similar eff orts are necessary when comparing winter and summer preparations to the 
activities that an organization undertakes to protect their network. Th e diff erence in the 
Intrusion
prevention system 
Hacker
Cracker
Confidential data
Perimeter
Figure 12.1 Real-world risk management.

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 181
corporation is that the preparations must be proactive, constant, and there is no time to relax. 
An organization must perform processes such as updating antivirus and undergo continuous 
eff orts to patch applications, databases, and operating systems against new and changing 
vulnerabilities‚Äô (just like keeping the cabin ready to survive winter or ready to handle a busy 
summer). Performing vulnerability assessment (VA) and penetration tests are like walking 
around the cabin, checking the trails, and making sure that basic cabin facilities are function-
ing properly to protect the cabin and its contents from criminal activity (thieves and kids who 
might like to just cause property damage) and even mother nature‚Äôs creatures (bats and mice) 
and conditions (storms and snow). Th ese preparations in an organization not only provide the 
organization their competitive ability to grow but also protect the confi dentiality, integrity, 
and availability of information assets to comply with customer expectations or regulated and 
legislated requirements.
After a very busy day working around the cabin, I was just about ready to settle in for the 
evening, relax by the fi re, and kick up my feet. I was relaxing in the living room by the fi re with 
the lights off  just enjoying its warm glow. Suddenly, I saw something out of the corner of my eye 
fl y past my head. It took me a couple of seconds to be able to process what I just saw. I jumped to 
my feet quickly and turned on the lights.
Hmmm, there it was, just as I thought, a bat. For me personally, bats are not one of my most 
favorite creatures; in fact, even though I consider myself a hunter and active outdoors person, I am 
very squeamish when it comes to bats. I had to ask myself, fi rst, how am I going to get this thing 
out of the cabin and second, and more important; how did it get in the cabin in the fi rst place? 
Without fi guring out number two I was going to continue having problems even if I took care of 
this incident.
Sound familiar to your risk management eff orts at your organization? Deciding how 
to handle an incident and reevaluating your current protections?
I quickly assembled my anti-bat tools in preparation of doing battle. Anti-bat tools, well I grabbed 
a pair of leather gloves, two of the kids butterfl y nets, and my trusty work boot, just in case I had 
to do a little fast footwork with this critter. Remember I really don‚Äôt like bats?
Sound familiar to assembling your incident response team, equipping them with the tools 
they need for incident management and to protect the organization from the event?
Just so you can get a clear picture of how I looked. I was now wearing a pair of blue jeans, a long 
sleeve fl annel shirt, boots, leather gloves, a hat and I had a butterfl y net in each hand. I know it‚Äôs 
a myth about bats fl ying in your hair, but my earlier experience with it fl ying by my head had me 
thinking it‚Äôs better to be safe then sorry.
I contained the bat to the living room by closing all doors to every room, so I was now ready 
to go on the off ensive. If you have had an experience with a bat before, you probably already have 
guessed that I much underestimated the speed and agility of my foe.
What? Butterfl y nets used as my combat tools against a bat. It is important when han-
dling an incident to be prepared to have the proper tools in order to properly handle 
the incident and protect the organization combating an intruder.

182 ‚óæ Information Security Management Handbook
I found it hanging on the wall in the corner of the living room. Just as I was getting close it took 
off  and began fl ying around the cabin again. Th ose butterfl y nets came in handy as I found myself 
swinging with both nets simultaneously only to come up empty handed. I chased the intruder 
(bat) around the cabin living room area for about an hour until it suddenly fl ew into a very small 
hole near the fi replace where the rock wall fi rewall connects to the wood interior siding. It had 
apparently made its escape.
Monitoring systems and event management help identify and track an incident, pos-
sibly even giving evidence of an intruder‚Äôs actions.
I was very tired from the day‚Äôs chores and with the incident with the bat intruder, so I called it a 
day and went off  to get some sleep. However, I did make sure to put some basic protections in place 
for the bedroom. I made sure the door was closed and added one more thing to the list of chores 
for tomorrow, which was to fi gure out how the bat intruder got into the cabin.
Regular vulnerability and penetration assessments can help to identify potential areas 
of concern where an attacker in the corporate environment might exploit to access 
information assets.
Okay, it is clear my home incident response process needs a little more planning, but it is hard to plan 
for all possible scenarios. For your organization, the important thing is to have an incident response 
plan defi ned. Once a process is in place and steps are defi ned, your focus is clearer and you are less 
likely to miss something, especially if you are gathering evidence that might need to support litiga-
tion at some point in the future. Th is way you are not relying solely on the skill of the staff  to react 
and make proper decisions quickly under a probably stressful situation to resolve the incident.
Th e next morning I needed to do an external perimeter assessment of the house, as well as 
investigating possibly holes inside, especially by the fi replace. I could identify a small hole where 
the screened-in porch connects to the house. It is on the same wall where the chimney for the fi re-
place is located. I proceeded to climb up onto the porch roof and inspect the external perimeter 
where the porch was an add-on after original construction. I found that bats had chewed a small 
hole in the area where the porch integrated to the cabin and this was where they were able to gain 
access to the interior of the cabin. Th ere were also some gaps along the eaves of the house which 
were probably caused over time by lack of regular maintenance and the eff ects of ice and cold 
winters. Th is evidence shows, like in our corporate computing environments, that maintenance is not 
something we can just do a couple times a year, it must be ongoing.
After I climbed down from the roof, I headed to the local hardware store to get the tools I did 
not have to begin patching things, close the holes.
I returned ready to complete the necessary maintenance to eliminate the vulnerabilities that 
had made it possible for an intruder to invade the cabin. I climbed back up on the roof of the 
porch and began patching. My fi rst priority was to patch the hole where the roof connected to 
the cabin. Th is would minimize the potential for future intruders. Suddenly I heard a scratching 
sound and little squeaking noises coming from the eaves of the main roof. I subsequently pounded 
on the eave with my fi st and several bats came pouring out of the gaps. (Did I mention that I 
didn‚Äôt particularly like bats?) Needless to say I was a little startled by this. I started waving my 
arms rapidly to protect myself, like they were attacking me. It is scary how little room there is on 
a 12 ft by 12 ft porch roof to make a quick get away unless I was willing to risk a 10 ft drop, so my 
movement was limited.

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 183
Sometimes in our corporate environments budget and other resources limit our 
ability to protect our systems to best practices. We have to make do to the best of 
our ability within the limitations that are posed to us. Management, regulations and 
legislation will still expect that adequate protections are in place.
Once everything fi nally settled down and there appeared to be no bats around the roof or in the 
eaves, I steadied my chalk gun and plugged every gap, crease, and hole. I eliminated all of the 
vulnerability that I could fi nd. After the maintenance project on the porch roof was complete, 
I called it a day, fi guring I deserved a little time to rest and relax. Now, I can look back, since this 
incident occurred a few years ago and I, fortunately, have not had a reoccurrence since.
Just as all of us face day-to-day challenges of protecting our homes from intrusion, organiza-
tions face constant battles to protect their critical and confi dential information. Nothing sends a 
chill down a CEO‚Äôs spine like hearing about a security breach. Yep, the one where your fi rst reac-
tion is to take a deep relaxing breath, in through the nose out through the mouth just like you were 
getting ready to deal with a bat.
Organizations need to be on constant vigilance, keeping watch on potential intruders, moni-
toring and analyzing the perimeter and the interior for vulnerabilities. Th ankfully, organizations 
have the technology available that can make the tremendous burden of keeping a constant vigil 
over vulnerabilities easier.
However, organizations cannot let their guard down, just like the story about the bat intruder, 
vulnerabilities were minimized, but one day my wife opened the door and let a mouse into our house. 
Th is action reminds us that people need to be aware and careful in their actions to avoid opening any 
holes unintentionally making our environments vulnerable even if only for a few moments.
Th e remainder of this chapter will continue to discuss the vigilance and some of the measures 
that organizations take to minimize vulnerability and manage risk to the confi dentiality, integrity, 
and availability of their data.
Vulnerability Assessments and Penetration Testing
Planning and preparation are keys for any risk assessment. Vulnerability assessment in conjunction 
with penetration testing help when doing complete diligence to test both technical and process-
oriented controls protecting an organization‚Äôs data.
Penetration Testing versus Vulnerability Assessment
For years there has remained a certain amount of confusion about the diff erence between a network 
penetration test (PenTest) and a network VA. Too often, they are classifi ed as the same thing or 
one is confused for the other when in fact they are diff erent. One common misrepresentation is the 
PenTest is from the outside and a VA is of the inside. While penetration testing does sound a lot 
more exciting, experience has told us that most organizations need to start with a comprehensive 
VA and not the more intrusive PenTest.
While they are similar projects and can cover many of the same areas; a critical diff erence 
is that a PenTest is typically more aggressive and more intrusive. Th e PenTest involves trying to 
break into components of the computing environment to prove they are vulnerable. Essentially, 
the PenTest in that sense is providing evidence that the VA and associated processes to perform 
maintenance, patch and mitigate identifi ed vulnerabilities are eff ective.

184 ‚óæ Information Security Management Handbook
A concern with a PenTest is that it can be more risky, since the tester may exploit fl aws in hard-
ware, database, Web, or software components of the production environment, which could cause 
instability and potential outages.
The Penetration Test
A penetration test is a method of evaluating the security of a computer system or network by 
simulating an attack by a malicious hacker. PenTest should be a goal-oriented test. Sometimes, 
a PenTest will identify a trophy, the target that signifi es success of a penetration. Th e process 
involves active analysis for weaknesses, technical fl aws, or vulnerabilities. Th e idea is to attack a 
predetermined target. It is like trying to break into the vault, fi ll a bag with money, and walk out 
of the bank without being stopped. It does not matter how many ways it could be done, the ques-
tion is whether it can be done. Analysis is carried out from the position of a potential attacker, and 
could involve active exploitation of vulnerabilities. Th e extent to which penetration was successful 
along with vulnerabilities found is reported with an assessment of their impact and recommenda-
tions for mitigation. Pen testing can help to fi nd the mistakes that other approaches might miss, 
such as a problem with confi guration or architecture that could get overlooked in other reviews. 
Th e PenTest usually goes deeper than most audits. Be prepared for the diff erent impact that a 
PenTest will have on the time resources of target environment personnel.
In reality, the PenTest cannot fi nd all vulnerabilities in a target environment. Limitations will 
be based on the resources and constraints of a test. Th ese limitations may include
Scope of testing (e.g., internal, external, applications, human fi rewall)
 
‚óæ
Time, this includes both elapsed time and how many hours are contracted for an ethical 
 
‚óæ
attacker to attempt to penetrate.
Access by the PenTest, this is important to the objective of the testing and to some level 
 
‚óæ
dictated by scope of testing.
Methods and expertise of the tester.
 
‚óæ
Testing that will not result in a denial of service. Th is can limit diversionary attacks and 
 
‚óæ
other activities that might make an attack more likely to be successful.
The Vulnerability Assessment
A VA is more like a review or an audit. VA is the step an attacker performs to identify vulnerabilities, 
internal or external including network, system, database, and application or Web site components. 
A key distinction is the VA identifi es vulnerabilities versus exploiting the worst of them. It is like 
walking around the bank looking for unlocked doors or open windows. It does not matter if a 
particular goal is completed; the challenge is to identify as many likely areas of exposure that 
could lead to a variety of compromised assets. Vulnerabilities that are found can be categorized, 
prioritized for risk, and reported with their potential impact, likelihood, and recommendations 
for mitigation.
Some potential purposes for your VA include
Demonstrate due care
 
‚óæ
‚Äì Infrastructure patching
‚Äì Intrusion prevention
‚Äì Software development

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 185
Security awareness
 
‚óæ
Discovery
 
‚óæ
To justify funding
 
‚óæ
Comparing and Contrasting
Which one delivers an organization the most value? Answering that depends on the maturity of your 
risk management. Unless vulnerability assessments have been performed for a while and vulner-
abilities actively corrected, organizations will be much better served by conducting the vulnerability 
assessment. Th e reason being is that a PenTest is typically more time consuming and more expensive. 
A VA answers the question: ‚ÄúWhat are our weaknesses and how do we fi x them?‚Äù A PenTest typically 
answers the question: ‚ÄúCan someone exploit our vulnerabilities and to what extent can they access 
information?‚Äù A VA works to improve security posture and develop a more mature, integrated secu-
rity program, whereas a PenTest‚Äôs most common use might be to prove to doubting management that 
vulnerabilities can be exploited, by showing evidence of the break-in (the Trophy). Depending on the 
tools used during a high-quality VA, the need to do PenTesting can be minimized. Th e VA tool may 
perform some basic PenTesting, such as testing default passwords.
Most organizations should start with vulnerability assessments, act on the results; and when 
confi dent with mitigation, consider the penetration test. Th e PenTest should be performed by a 
diff erent third party than the VA. As components of an overall risk management program, both 
vulnerability assessments and penetration tests can have their purpose and should be performed 
regularly (the VA at least annually, more often in an environment such as PCI/DSS) to ensure 
continuous risk management.
A Hacker‚Äôs Process
Let us take a look at the process a hacker could take in breaching an organization‚Äôs network. 
Plan, discover, attempt penetration, more discovery, more penetration attempts and the process is 
repeated until they are either successful, they give up, or get caught. Th en they move onto the next 
target (trophy) identifi ed in the planning stage.
Th is is very similar or is actually the same process that is involved with the penetration test as 
described above.
Th e one thing a hacker typically has on their side is time and if they can be patient. It is often 
only a matter of time before they will be successful. Th e same is often true with the PenTest, 
scoping how many hours will be used and the elapsed time are important to a test‚Äôs eff ectiveness. 
Given enough time to test during a penetration test will typically result in fi nding a vulnerability 
to penetrate and with suffi  cient elapse time the testing can be done in such a way that it is very 
unlikely that it will ever be detectable.
Let us take a brief look at how diff erent types of penetration tests are performed, and what tools 
are involved from the planning stages to execution. Th ere are four functional areas to consider: exter-
nal perimeter, internal network, wireless environment, and social engineering.
Social Engineering
Social Engineering, more positively identifi ed as security awareness testing, is a method that 
can be used to educate the organization‚Äôs employees to have good judgment when encountering 

186 ‚óæ Information Security Management Handbook
anomalous activity. Just like testing the perimeter fi rewall, it is important to test the ‚Äúhuman 
fi rewall,‚Äù the employee. Th is kind of activity targets the vulnerability I alluded to earlier when 
I mentioned my wife letting the mouse in the house. Providing my wife greater awareness of 
how to be more careful so she can prevent an incident like the mouse coming in the door of the 
house. While testing my wife to be more careful might be a little diffi  cult and not real practi-
cal, testing employees to ensure they follow good data handling is a valuable step to improve 
protection of data.
Physical Security
One area of social engineering testing involves physical visits that look for vulnerabilities in an 
organization‚Äôs nonpublic presence to view information that might be left in plain sight. A social 
engineer might start by reviewing and evaluating various physical access points to the facility. Th e 
test might use some of the following types of attack scenarios, including costumes to dress accord-
ingly for the role:
Help desk employee
 
‚óæ
Executive attending a management meeting
 
‚óæ
Delivery person
 
‚óæ
High-profi le IT vendor representative
 
‚óæ
Fire extinguisher service personnel
 
‚óæ
Maintenance or janitorial staff 
 
‚óæ
Th is test can identify the internal availability of ‚Äúlive‚Äù network ports that could be used for tech-
nology based penetration activities after an attacker might gain physical access. A social engineer 
could place a rogue wireless access point (WAP) or a specifi cally confi gured laptop on the network 
and attempt to obtain a network connection and penetrate the network from that device. Physical 
access may even be authorized, such as a vendor or auditor connecting a WAP for ease of printing, 
or maybe just having their laptop‚Äôs wireless active and insecure.
Password and E-Mail/Web Site Spoofi ng
Another social engineering attack might test a sample of personnel using the following types of 
tests:
Telephone-based social engineering‚ÄîSometimes also called pre-texting, this type of an 
 
‚óæ
attack‚Äôs goal could be to elicit login credentials, or directly get personal data on a customer.
E-mail and fake Web sites‚ÄîA social engineering test might fi nd e-mail addresses that 
 
‚óæ
employees have posted on the Internet using their company e-mail address. Th is may be 
in technical support chat (i.e., Listserv) rooms and on social networks (e.g., Facebook, 
MySpace, Twitter, and LinkedIn). Many e-mail addresses that could be valuable targets are 
often available on corporate Web sites. Th e goal of this type of testing would be to determine 
how an employee would deal with unsolicited actions from unknown sources.
Once these activities are complete, an organization may have a good idea how their existing secu-
rity awareness plan is working. Experience shows that most organizations that have undergone 
this type of service were surprised. A talented social engineer has the ability to take advantage 

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 187
of the trust that many people inherently have to help, especially in organizations that are ser-
vice oriented. Kevin Mitnick, renowned as a hacker, in reality was not at all technical, but was 
very good at exploiting people‚Äôs trust and also at masquerading in positions to gain trust to get 
information.
For a hacker, social engineering activities are just one step in the process. Obtaining key pieces 
of information, such as types of applications or systems or a user‚Äôs credentials to use in order to 
gain access, increases a penetration‚Äôs potential for success. Th is information is almost always used 
for future penetration.
Th is reminds me of another story about the cabin. One of the other joys of owning a cabin in 
northern Wisconsin is that each fall and spring there is the unglamorous task of eliminating mice. 
Mostly they are discovered in the cellar but on occasion I will see one in the house. A mouse is 
somewhat an unpleasant creature and I can tolerate them better than bats; however, they can cause 
a great deal of damage and if not taken care of, costs could mount.
I fi nd myself continually checking the perimeter of the cabin looking for possible entry points 
(vulnerabilities) and closing them one at a time. But still each year as the cabin ages new vulner-
abilities are discovered, an ever so small crease, hole, or crack that provides an opening for mice to 
get in. Th ese types of things are expected, so plan for it, dealing with the vulnerability by adding 
insulation (applying network, operating system, application, or database patches), replacing a door 
or fi lling in the gaps with some caulk (upgrades to newer vendor releases or replacing an out-of-
date fi rewall).
As mentioned earlier, my wife went to feed our Old English sheepdog one morning, not think-
ing (a lapse in awareness), she had left the bag of open dog food on the deck the night before. 
When she stepped outside with the food dish she left the door open and in her fi rst scoop of food 
out pops a mouse, jumps down onto the deck, and scurries right into the house. You could almost 
hear the mouse say, ‚ÄúTh ank You!‚Äù Of course the alarm is sounded, my wife and daughter imme-
diately begin to start screaming, the dog starts barking and running around the perimeter of the 
kitchen alerting me that an intruder has entered. We all recognized these as the more frustrating 
situations when we just hold the door open for those pesky mice, the intruders to come right it 
(employees open malicious attachments, visit unauthorized Web sites, or choose poor passwords).
As you would expect, my incident response team was immediately put into motion, I dropped 
everything and began to hunt the little critter down. Fortunately, with the Old English sheepdog 
circling the kitchen barking, the mouse froze under the table and I was able to capture it with my 
trusty butterfl y net and I place it back outside. It is almost like the dog herded the mouse to 
confi ne where it would go to under the table. Imagine this as one of the secondary layers in the 
organization‚Äôs defense in depth. I should not really be that surprised an Old English sheepdog is bred 
to herd sheep. I think if we would have still had our Yellow Lab, she would have just ran it down 
stepped on it and ate it. Labs will eat just about anything, both food and not food items. Trust me 
on that, I have many stories to tell. Each diff erent type of dog can have diff erent approaches but 
either would have been just as eff ective. Just like diff erent security technologies handle intrusions 
or specifi c potential vulnerabilities diff erently.
Wireless Environment
Wireless is an extension of an organization‚Äôs internal network. It can extend an organization‚Äôs 
internal network outside the perimeter. At this point, almost everyone is aware that wireless is 
inherently insecure, but with proper security can be made more secure. Wireless technology is easy 
to set up but can be a bit more diffi  cult to adequately secure. However, there are secure wireless 

188 ‚óæ Information Security Management Handbook
confi gurations that make penetrating the wireless networks harder and makes it take more time 
and eff ort for the attacker. Some of them are basic changes in default confi guration settings.
As is with any type of penetration testing, the fi rst part of performing a Wireless PenTest 
focuses on external WLAN (wireless LAN) Discovery. Th e goal is to look for active APs (access 
points), workstations, and laptops associated with the target organization and attempt to identify 
the confi guration of those devices. Th e Discovery includes determining if any open WAPs are 
available, if confi guration errors exist and the security level and encryption type that may be 
confi gured on the APs. Th ere are many tools available for use during the discovery. One such tool 
from the Back Track 3 suite is called Airodump-ng. Th is free tool collects much of the information 
needed to perform additional attacks and identifi es potential attack vectors. Other noncommercial 
tools that can be used for discovery include Kismet, Netstumbler, Ministumbler, Commview, 
Wifi  Hopper, Wirelessmon, Airopeek, and Wellenreiter.
Th e process for attacking a wireless environment is essentially the same whether it is being 
performed internally or externally from the parking lot or street. Once the discovery step is complete, 
a variety of next steps could be taken. Th ose include asking several ‚Äúifs‚Äù:
 
1. If there is an unencrypted wireless network, then sniff  and port scan IP range and continue 
to gather information of workstation or laptops connected to that network. Look for bridged 
systems that are both connected to the wired network and wireless network.
 
2. If the Wireless environment is using WEP encryption, then attempt a WEP attack. Th e biggest 
challenge for cracking WEP is collecting enough initialization vectors. Th ree tools (Airodump, 
Aireplay, and Aircrack) running in conjunction with each other can be very eff ective in crack-
ing any WEP key. One only needs time to collect the initialization vector required to initiate 
the cracking process. Th at is what Aireplay is used for. Once an active access point and associated 
workstation or laptop is identifi ed, Aireplay can be used to send authentication and de-authen-
tication requests to assist in generating traffi  c quickly. Both penetration testers and/or hackers 
do not like waiting around too long if they do not have to.
 
3. If the Wireless environment is using WPA/WPA2 encryption, then capture EAPOL hand-
shake and attempt a WPA/WPA 2 dictionary attack. Th e same challenges apply to cracking 
WPA as WEP. Th e tool that would be used would be Aircrack.
 
4. If LEAP is being used for the wireless network, then attempt to break LEAP by using 
Airodump to capture the traffi  c and use Asleap to collect any authentication requests and 
crack the password.
 
5. If the discovery identifi es a client confi guration where wireless clients are ‚Äúlistening‚Äù in peer-
to-peer mode, known as AD-HOC, then‚Ä¶. To clarify this issue, when a user reviews a list of 
wireless access points by which to connect, occasionally an AD-HOC ‚Äúbase station‚Äù or access 
point (typically it is just another wireless client in AD-HOC mode) will appear in the list. If 
the user either manually attempts to connect to this AD-HOC base station (or confi gures the 
client to connect to ANY base station, infrastructure or AD-HOC), the user‚Äôs wireless client 
will store that attempted association. Th en, when the user moves the wireless client to a new 
location and there are no access points that the current confi guration selects in priority, the 
client will attempt to search for whatever it was connected to last. If the previous connection 
attempt was to an AD-HOC base station, the client will, in eff ect, become an AD-HOC 
base station in its attempt to search for its missing partner. Because this behavior occurs auto-
matically and sometimes when the computer that hosts the wireless client is also connected 
to a wired network through the Ethernet interface, an attacker could connect to the wireless 
AD-HOC base station and compromise the computer and its wired Ethernet interface.

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 189
 
6. If you were able to observe wireless clients that were ‚Äúroaming‚Äù for a wireless access point to 
which to connect, then in most cases, these wireless clients are also connected to the secure, 
internal, wired network via an Ethernet adapter. Th en one could create a mobile, promiscu-
ous access point that rapidly rotates the broadcast SSID of observed client beacons in order 
for ‚Äúfree roaming‚Äù clients to connect. Th e mobile access point could also host a DHCP 
server so that clients were automatically given an IP address, to which the ‚Äúattacker‚Äù could 
scan each unsuspecting client and exploit vulnerabilities.
As you can see, if there is a will there is a way when it comes to successfully penetrating a wireless 
network. It is simply a function of time.
External Perimeter
Attacking or attempting to penetrate the external perimeter starts similarly to that of a wireless or 
internal penetration attack that is beginning with reconnaissance and discovery. External PenTest 
activities typically include Google searches for e-mail addresses, reviewing technical forums, and 
blog posts. Th e goal is to identify e-mail addresses that might be used to populate user lists for 
authenticator enumeration. Other activities include
Running WHOIS searches to identify IP address ranges and getting confi rmation of ownership.
 ‚óæ
Performing an initial assessment of Web-based applications accessible by BROWSER only. 
 
‚óæ
Th is involves rendering publicly accessible browser pages to identify ‚Äúlow-hanging fruit‚Äù and 
establish a prioritized testing framework.
Looking at DNS resolution to identify resolution and reverse resolution of IP addresses and 
 
‚óæ
corresponding ‚Äúfully qualifi ed domain names‚Äù (FQDN) to also look for discrepancies or 
anomalies.
Doing port mapping using redundant NMAP scans and performing port scans against 
 
‚óæ
identifi ed targets.
One of the next steps in the external penetration attack would be to perform specialized scans 
against Web servers. Tools that can be used are commercial scanners such as WebInspect, 
Nikto, Paros proxy, WebScarab, Wisker/libwisker, Burpsuite, Wikto, Acunetix WVX, Appscan, 
WFUZZ, N-Stealth, and potentially others, depending on the Web server or application server 
discovered. Th ese scans are intended to identify information about the supporting architecture, 
such as internal IP addresses, application servers, and databases running on the inner application 
tiers. Th e tools can identify various vulnerabilities including poor coding practices in Web appli-
cation programming, and dangerous fi les/CGIs.
Th ese types of scans can be ‚Äúnoisy/loud‚Äù in terms of network attack activities. If an organization 
has an intrusion detection system (IDS), the activities might be noticeable and the organization could 
quickly respond appropriately to prevent penetration. Th at is unless secure socket layer (SSL) ports are 
leveraged to do the scans.
Most attackers will identify which servers off er SSL and will launch specialized scans on these 
systems fi rst to maintain a ‚Äústealth‚Äù level of activity to avoid being detected. If the attacker is 
unable to identify if an organization has an IDS, then the servers that do not off er SSL will usually 
be attacked last, if at all, to continue to work undetected.
Th e penetration tester or attacker would also perform an assessment of non-Web exposed 
services. Th is would involve review of exposed banners, leveraging acquired username lists in 

190 ‚óæ Information Security Management Handbook
an attempt to authenticate, attempting to bypass authentication, and an attempt to compromise 
exposed services. Th e goal of using the service(s) will be to access internal resources. Tool selection 
and strategy will depend upon the exposed services that are discovered.
Internal Network
An internal penetration test or attack follows a similar format as an external. An internal attacker 
typically has an advantage. Th eir advantage is that they often understand the systems and the 
placement of the valuable assets that they wish to attack because they are often an employee with 
credentials to access resources associated with their role within the company. So there is time on 
their side to gain access to confi dential information they are not supposed to access.
If the attacker is not an employee, they might start by locating an open ‚Äúactive‚Äù network jack 
in a location that would not be too conspicuous. Th e tester or attacker would then start infor-
mation gathering and vulnerability analysis on devices that are visible on the internal network. 
Typical devices that are discovered by the penetration tester include servers, workstations, data-
bases, printers, fi rewalls, routers, Web servers, e-mail servers, and ftp servers.
Th e tools used would be a port scanner such as NMAP and a vulnerability scanner like 
Nessus.
Based on an attacker‚Äôs results and the targets that are identifi ed, the exploit activities could 
be performed. Th e specifi c attack vectors are dependent upon exposed services and vulnerabilities 
associated with the exposed hosts and services. Subsequent penetration attack strategy and tool 
selection depends on the discovery.
Conclusion
Vulnerability testing (i.e., assessment‚ÄîVA) and penetration testing (i.e., assessment‚ÄîPenTest) are 
not the same thing! Th ey have much in common, are both a form of assessment, and when done 
correctly will help an organization manage their risk. Th ey both have their place and should be 
part of a robust risk management program. Th e simplest diff erence in a ‚Äúpure‚Äù sense is that the VA 
is less intrusive than the Pentest. Maturity of vulnerability management is very important before 
stepping into a Pentest. Pentesting will typically include the processes of scanning for vulnerabili-
ties (i.e., discovery), which is the main process of the VA.
What are some of the motivators for VAs and Pentests?
Compliance/fear
 
‚óæ
Means to justify other initiatives
 
‚óæ
New management eager to learn
 
‚óæ
‚ÄúTrue believers‚Äù
 
‚óæ
Anticipated benefi ts to a vulnerability assessment/PenTest can include
Learning something new
 
‚óæ
Validating and quantifying concerns
 
‚óæ
Standardizing communication of vulnerabilities
 
‚óæ
Establishing common language and tools
 
‚óæ
Satisfying the auditors and regulators
 
‚óæ

Rats in the Cellar and Bats in the Attic, ‚ÄúNot Enough Depth to My Security‚Äù ‚óæ 191
Th e vendor selection process is an important step. Before selection, an organization must exercise 
due diligence when choosing a vendor to ensure the ethics and morals of not only the organization, 
but the individuals performing the assessment. Th is should include background and reference 
checks. Be sure to select a vendor who not only has employees with the skill to compromise the 
latest exploits but who can be trusted. Th ese authors are not for hiring the ‚Äúreformed‚Äù hacker, but 
we will leave that discussion for another day and time. If penetration testing is successful, you will 
want to have confi dence that if any data is breached it will be handled in a secure and professional 
manner. You want to know that the exploits will not be published or talked about except in the 
management report to your organization.
Information security technology can be a life saver when identifying vulnerabilities and to 
stop many security incidents, but the processes of incident response plans are also crucial to mini-
mizing potential impact. Th is means not just the technology of intrusion detection, but what to 
do if an attempt is detected. Every organization‚Äôs security awareness and training programs must 
include reminders and proactive measures to keep not just the technology fi rewall, but the human 
fi rewall well prepared for the potential attack.
Oh. Did I ever tell you about the problem I had with bees‚Ä¶
About the Author
Ken M. Shaurette, CISSP, CISA, CISM, IAM, is an engagement manager in Technology Risk 
Manager Services at Jeff erson Wells, Inc. in Madison, Wisconsin.


193
13
Chapter 
The Outsourcing of IT: 
Seeing the Big Picture
Foster Henderson
Outsourcing information technology (IT) jobs has benefi ts if properly managed and conducted 
in a manner that doesn‚Äôt jeopardize the loss of strategic or corporate assets. Unfortunately, the 
common practice is to solely focus on reducing costs and not applying the same amount of 
attention or diligence in addressing security issues. Th is problem encompasses ethical issues, 
losses in revenue in the long run, and a clear and present danger to our national interests. If we as 
Contents
Background ..............................................................................................................................194
Today‚Äôs Business Environment (Globalization) .........................................................................195
Outsourcing Types ...................................................................................................................196
Risk Management ....................................................................................................................197
Intellectual Property .................................................................................................................197
Data Security/Protection ..........................................................................................................198
Turnover of Personnel ..............................................................................................................199
Insider Th reats ..........................................................................................................................199
Espionage Cases ...................................................................................................................... 200
Th e Business Case and Expected Cost Savings ......................................................................... 202
Business Impact ....................................................................................................................... 202
Ethics ...................................................................................................................................... 203
National Interest (United States and Others) ........................................................................... 204
Mitigations of Risks (Outsourcing) ......................................................................................... 205
Trusted Foundry ................................................................................................................. 205
Trusted Software ................................................................................................................. 205
Conclusion .............................................................................................................................. 206
About the Author .................................................................................................................... 206
References ............................................................................................................................... 206

194 ‚óæ Information Security Management Handbook
a nation continue sending high-technology jobs overseas, should we trust the electronics or 
software when it appears in military applications, hardware, or other critical infrastructures 
that support a nation so dependent on technology?
Background
Th e Central Intelligence Agency (CIA) stated within its World Factbook of 2008 the following:
Th e US has the largest and most technologically powerful economy in the world, 
with a per capita [gross domestic product] GDP of $48,000. In this market-oriented 
economy, private individuals and business fi rms make most of the decisions, and the 
federal and state governments buy needed goods and services predominantly in the 
private marketplace. (CIA, 2008).
Th e GDP is defi ned as the fi nal goods or services a country produces within a given year (CIA, 
2008). An example of the United States‚Äôs technological power is evident throughout twentieth-
century history.
Th e United States as a nation is responsible for a majority of the greatest twentieth-century inven-
tions that propelled its technologically powerful economy. Below is a sampling of those inventions:
Computers
 
‚óæ
Integrated circuits
 
‚óæ
Lasers
 
‚óæ
Fiber optics
 
‚óæ
Telephone (traditional and cellular)
 
‚óæ
Air conditioning and refrigeration
 
‚óæ
Television
 
‚óæ
Airplanes
 
‚óæ
Internet (National Academy of Engineering, 2009)
 
‚óæ
Th e CIA estimates the world‚Äôs GDP at $70.65 trillion; the United States‚Äôs GDP is the second larg-
est in the world, surpassed by the European Union by $380 billion, and is estimated to be nearly 
$14.58 trillion or 21% of the world‚Äôs estimated GDP (CIA, 2008). Th e next closest country is 
China at $7.8 trillion or 53% of the United States‚Äôs GDP (CIA, 2008).
Melton (2004) wrote at the end of the Cold War that the former Soviet Union was not able to 
adapt to the new global economy and lacked the telecommunications infrastructure to compete as 
a modern superpower; yet, it maintained a nuclear arsenal to destroy the world. Th e Cold War era 
has been replaced with the digital byte in today‚Äôs economic and digital warfare. Future superpow-
ers will be those nations that successfully compete in today‚Äôs economic environment.
From a monetary viewpoint, how can lesser developed nations or businesses from those nations 
seek to compete in today‚Äôs environment? How can they narrow the gap between their country and 
the United States, for example? One strategy is to simply obtain the technology they cannot aff ord 
to develop themselves. Th ink about it from a business point of view. It makes sense. Call it what 
you like: borrow, steal, copy, or acquire a technology. Let someone else expend valuable resources 
to perform the research and development of a product and simply pick up on the tail end with the 
production. Th is leads to my next point.

The Outsourcing of IT: Seeing the Big Picture ‚óæ 195
Today‚Äôs Business Environment (Globalization)
Business is war. Clausewitz (1832) stated in his chapter ‚ÄúArt or Science of War,‚Äù the following:
It is a confl ict of great interests which is settled by bloodshed and only in that 
it is diff erent from others. It would be better, instead of comparing it with any 
Art, to liken it to business competition, which is also a confl ict of human inter-
ests and activities; and it is still more like State policy [politics], which again on 
its part, may be looked upon as a kind of business competition on a great scale 
(pp. 202‚Äì203).
I used the quote from a classic book to illustrate today‚Äôs global business environment with all its 
underlying motivations and risks, which must be considered. For example, in a Federal Computer 
Week article titled the ‚ÄúOutsourcing Hole,‚Äù French wrote the following:
A terrorist cell looking for an advantage against the powerful U.S. military trains a 
group to be software programmers, who then infi ltrate companies that have sent their 
software development work overseas. Working for those companies, the programmers 
surreptitiously put vulnerabilities in software (French, 2004a).
In too many instances, chief executive offi  cers (CEOs), chief information offi  cers (CIOs), or other 
decision makers do not account for the strategic impact that their decision to outsource IT will 
have on their own company or country. Th ese decisions are being made solely on the basis of 
the expected cost savings without taking into account other potential problems or issues associ-
ated with outsourcing IT. Specifi cally, these decision makers tend to overlook critical information 
about the topics listed below:
Turnover of personnel
 
‚óæ
Insider threats
 
‚óæ
Data security/protection
 
‚óæ
Espionage cases
 
‚óæ
Business cases
 
‚óæ
Cost reduction expectations
 
‚óæ
Business impact
 
‚óæ
Ethics
 
‚óæ
Project risks
 
‚óæ
Cultural barriers
 
‚óæ
Infrastructure risks
 
‚óæ
National interests (includes geopolitical)
 
‚óæ
If one doesn‚Äôt buy French‚Äôs analysis as a possibility, here‚Äôs another example of the risk.
In February 2002, nearly 6 months after September 11, 2001, Paul Wolfowitz, Deputy Secretary 
of Defense, signed Department of Defense (DoD) Directive (DoDD) 8000.01, Management of 
DoD Information Resources and Information Technology, [policy] that mandated the Services 
and individual organizations, which comprise the DoD, to perform the following actions sum-
marized below:

196 ‚óæ Information Security Management Handbook
Before applying IT resources, determine whether or not if private industry or another gov-
 
‚óæ
ernment organization could provide the same product or services at a lower cost.
Maximize the use of commercial-off -the-shelf (COTS) products for military acquisitions 
 
‚óæ
related to DoD IT and national security systems (NSS).
NSS is defi ned as U.S. government telecommunications or information related to intelli-
 
‚àí
gence, cryptological activities, command and control of military forces, equipment or con-
trol of a weapon or weapon system, or fruition of military or intelligence missions.
Outsource non-core or inherently non-government business functions to another govern-
 
‚óæ
ment organization or private sector when it made good business case or decision.
Custom designed products shall be avoided or isolated to minimize severe impacts to project 
 
‚óæ
schedules or budgets.
On February 10, 2009, the above policy was canceled, updated, and renamed DoDD 8000.01, 
Management of the Department of Defense Information Enterprise (ASD NII/DoD CIO, 2009). 
Th is policy is consistent with DoD‚Äôs existing acquisition policies and now states:
Acquisitions shall allocate risk between the U.S. Government and contractors
 
‚óæ
Tie contract payments to performance
 
‚óæ
When applicable, take maximum advantage of COTS technology
 
‚óæ
Information solutions are to be structured, brief, with a delivered and measurable net benefi t
 ‚óæ
Outsource noncore or nongovernmental functions to another U.S. Government entity or 
 
‚óæ
the private sector when it makes appropriate business case (ASD NII/DoD CIO, 2009)
Th e benefi ts of COTS can be illustrated throughout IT history. Xerox developed the fi rst personal 
computer, but it was too expensive to market because everything was developed from scratch. 
IBM‚Äôs personal computer (PC) was released in August 12, 1981, and was IBM‚Äôs fi rst computer 
built from off -the-shelf parts distributed by outside vendors. It used an existing Intel 8086 chip 
that IBM had licensing rights from Intel to manufacture (Bellis, 1999a). Th e development of 
IBM‚Äôs PC is an early example of outsourcing, where Microsoft developed a 16-bit operating sys-
tem (OS) that it called MS-DOS. A more modern example is Boeing outsourcing to Chinese 
vendors, which has had a major consequence, as discussed later in this chapter.
Outsourcing Types
Bierce and Kenerson (2007) defi ne outsourcing as ‚Äúthe transfer or delegation to an external service 
provider the operation and day-to-day management of a business process. Th e customer receives 
a service that performs a distinct business function that fi ts into the customer‚Äôs overall business 
operations.‚Äù Th ey state there are two principal types of outsourcing: traditional and greenfi eld. 
Th ey defi ne those respective terms as follows:
‚Ä¶employees of an enterprise cease to perform the same jobs to the enterprise. Rather, 
tasks are identifi ed that need to be performed, and the employees are normally hired by 
the service provider. In ‚Äúgreenfi eld‚Äù outsourcing, the enterprise changes its business pro-
cesses without any hiring of personnel by the service provider. For example, the enterprise 
might hire a startup company to provide a new service, such as wireless remote comput-
ing, that was not previously managed internally (Bierce and Kenerson, P.C., 2007).

The Outsourcing of IT: Seeing the Big Picture ‚óæ 197
Now, from that defi nition we can see the more commonly known types of outsourced services, 
technology services and business process outsourcing (BPO). For further clarifi cation, outsourc-
ing practice terms ‚Äúnearshore‚Äù and ‚Äúoff shoring‚Äù will be used. Th ese words refer to geographical 
locations according to whether the outsourced work is sent outside the country. Th e term ‚Äúoff -
shoring‚Äù primarily used within the United States refers to those jobs or services sent overseas as a 
result of outsourcing (Pfannenstein and Tsai, 2004). A key benefi t is that the labor pool is cheaper 
overseas when compared to wages within the United States to perform the same task. Nearshore 
outsourcing refers to those locations where outsourced work occurs within the same four major 
times zones as the United States (Rao, 2004). Another term that won‚Äôt be used in this chapter 
but is a used business practice is ‚Äúinsourcing.‚Äù Rao (2004) described insourcing as ‚Äúattempts to 
reap the dual advantages of low-cost off shore environments‚Äù (p. 20). Specifi cally, some U.S. com-
panies will hire foreign technology workers as employees at established off shore IT centers who 
receive the same training, tools, support, and follow the same developing outlines as their U.S. 
counterparts (Rao, 2004). Th e one exception is that these workers come from poorer countries 
and are paid less than their U.S. counterparts. Th e reason for this practice is to eliminate some 
of the risks associated with outsourcing IT, specifi cally software development, to third-party 
vendors (Rao, 2004).
Risk Management
‚ÄúAs soon as you have anything of value, you are at risk of losing it‚Äîit is just that simple‚Äù (Tiller, 
2004, p. 1064). As added value along the same thought process: what are you protecting, and why 
is it important to protect? If an individual, corporation, or country does not know what is worth 
protecting, then the security plan is fl awed from the start. Risk management involves fi rst iden-
tifying the risk, knowing the probability for it occurring, and then being able to respond to that 
threat or vulnerability. Th ere are four things an individual, company, or nation can do with risk. 
You can ignore it, accept it, transfer it (insurance policy), or mitigate it. Mitigation involves reduc-
ing risk to an acceptable level where the consequences of that event are recoverable. For example, 
outsourcing touches upon the one key item or one common thread this nation and its underlying 
businesses was built upon that propels our economy: intellectual property (IP).
Intellectual Property
Merriam-Webster‚Äôs Dictionary of Law (1996) defi nes IP as ‚Äúproperty that derives from the work of 
the mind or intellect; specifi cally: an idea, invention, trade secret, process, program, data, formula, 
patent, copyright, or trademark or application, right, or registration relating thereto‚Äù (Dictionary.com, 
2009). As previously mentioned, there were nine bulletized items briefl y mentioned at the 
beginning of this chapter (i.e., within Today‚Äôs Business Environment subheading). Th e fi rst four 
bullets will be discussed in detail because they are integrally connected to the one item most 
corporations are attempting to protect: IP. As a reminder, the four items were
Turnover of personnel
 
‚óæ
Insider threats
 
‚óæ
Data security/protection
 
‚óæ
Espionage cases
 
‚óæ

198 ‚óæ Information Security Management Handbook
History is a teacher that provides a useful method for understanding the value of IP and why it 
should be protected. For example, Bill Gates, cofounder of Microsoft, and Steve Jobs, cofounder 
of Apple, both took tours of Xerox‚Äôs Palo Alto Research Center. Jobs fi rst visited the site and was 
inspired by the work Xerox was performing with the Graphic User Interface (GUI) involving pull-
down menus and point and click features (Bellis, 1999b). Bill Gates was hired to work on software 
for the Macintosh, which was released in 1984, and Gates released Windows 1.0 in 1985. Mesa 
(1998) states
‚Ä¶.Bill Gates feared Apple would sue him due to the fact that his OS was looking a lot 
like the Mac OS. So on November 22, 1983, John Sculley, then CEO of Apple, signed 
an agreement to allow Microsoft to use Mac OS technology in exchange for further 
development of Microsoft software for the Mac. Th is single event would be one of the 
biggest mistakes in the history of the micro-computing industry.
When Steve Jobs complained that Gates had stolen designs from the Mac, Gates‚Äô reply appearing 
in MacWeek was ‚ÄúHey, Steve, just because you broke into Xerox‚Äôs house before I did and took the 
TV doesn‚Äôt mean I can‚Äôt go in later and take the stereo‚Äù (Mesa, 1998). Later, Bill Gates worked 
on the next OS for IBM PC, which was OS/2 to replace the MS-DOS OS. Th e deal went bad 
and as a result Gates took that information from working for IBM and turned it into what is now 
known as Windows NT. IBM later sued and lost due to contractual language within the licensing 
agreements.
Fortunately, the aftermath from the lawsuits still benefi ted the U.S. economy (i.e., among 
Apple and Microsoft, and Microsoft and IBM) and jobs were created within this country as a 
result of a disputed IP owner(s); however, it can be assumed Apple and IBM wouldn‚Äôt see it that 
way. What if this 1980s timeline were moved up 20 years and occurred in a foreign country where 
a U.S. company was off shoring? Are companies conducting background investigations on off shor-
ing employees and who would be the IP owner if this occurred today?
Data Security/Protection
IP is something a majority of companies have diffi  culty grappling with or even understanding. 
Michael Croy (2004) stated ‚ÄúMost business managers say their data must be highly available, but 
few identify which data needs to be highly accessible and which data can be stored in less acces-
sible locations‚Äù (p. 20). Th is point is important because with off shoring, most companies use data 
centers. Th ese data centers are usually located in remote regions where the infrastructure isn‚Äôt 
in the company‚Äôs control, but a vendor‚Äôs control located in another country. Also consider the 
fact that in 2003, it was estimated that 1 out of every 500 data centers experienced a computer 
disaster (Margeson, 2003), and application failure was the number one reason in most outages 
(Roden, 2004). ‚ÄúTh e loss of critical knowledge is seen as the greatest source of work-force related 
risk around outsourcing‚Äù (Pfannenstein and Tsai, 2004, p. 75). While this statement was specifi -
cally focusing on the intellectual assets that will be discussed shortly, it also refers to loss of digital 
information.
For example, the importance of data security and protection should become evident by dis-
cussing Boeing‚Äôs outsourcing with China. It was previously mentioned that airplanes made the 
National Academy of Engineering‚Äôs twentieth century‚Äôs greatest engineering achievements list. 
According to Owen Herrnstadt (2008), Boeing has worked with China for three decades in various 

The Outsourcing of IT: Seeing the Big Picture ‚óæ 199
business dealings to include manufacturing training, raw materials, and parts. As a condition for 
Boeing to sell its 747 to Air China, parts of the plane had to be assembled in China (Herrnstadt, 
2008). During his Congressional testimony (U.S. Congress, 2008), Herrnstadt stated that there 
has been technology transfers ‚Äúoff sets‚Äù (as described previously within Pfannenstein and Tsai‚Äôs 
(2004) article). China Daily (2007) stated, ‚ÄúChina is now Boeing company‚Äôs largest foreign 
supplier of parts, with US $2.5 billion of live contracts for aircraft products, a senior Boeing 
offi  cial said here on Th ursday‚Äù (para 1). More troubling is that earlier USA Today (2004) reported, 
‚ÄúChina studies building its own large aircraft.‚Äù In addition, on March 27, 2007, Cameron et al., 
(2007) wrote an article in the Financial Times titled ‚ÄúChina to challenge Boeing and Airbus.‚Äù Th e 
Financial Times article stated that China was going to pursue building its own airplanes rather 
than buy from Airbus or Boeing.
Strictly focusing on raw data (i.e., the electronic form in bytes), what guarantees are in place to 
ensure that data in transit (transmission) or storage is being properly protected from prying eyes? 
Th ere are specifi c regulations that have privacy requirements for U.S.-based companies. Below are 
a few regulatory requirements:
Th e Privacy Act of 1974
 
‚óæ
Health Insurance Portability and Accountability Act of 1996 (HIPAA)
 
‚óæ
Sarbanes‚ÄìOxley Act of 2002 (Sox) (public company accounting reform)
 
‚óæ
European Data Protection Directive (Rao, 2004)
 
‚óæ
California SB 1386 (Blum, 2004)
 
‚óæ
Gramm‚ÄìLeach Bliley Act (Blum, 2004)
 
‚óæ
Failure to comply with the above regulatory requirements can result in steep fi nancial or disclosure 
penalties (Blum, 2004).
Turnover of Personnel
Bangalore, India, has experienced rapid growth as a result of being one of the larger IT outsourcing 
hot spots, which has created a dynamic labor market. Turnover rates for that region are reported 
to be from 15% to 20% and there are associated indirect IT costs as a result of training new 
employees and loss of knowledge associated with turnover (Davison, 2004). Also any knowledge 
or experience gained working in a particular project (i.e., what does work and doesn‚Äôt work), the 
best business practices to use, etc., is lost when an employee leaves. Overby (2003) stated the cost 
for turnover of vendors is as high as 35% in India. A Deutsche Bank research conducted by Schaaf 
(2005) stated that India‚Äôs employee turnover in call centers is 50%, and among IT providers the 
range of turnover is from 15% to 30%. Th ere is also the question what‚Äôs to stop an individual from 
working with another competitor on a similar project and using what they learned from the previ-
ous job in their new position with a new employer?
Insider Threats
By law, companies have to provide employees 60 days notifi cation prior to outsourcing their jobs. 
Most of those employees who would be notifi ed in this instance are probably aware that the main 
reason they are losing their jobs is to reduce the company‚Äôs payroll. Insider threats pose more 

200 ‚óæ Information Security Management Handbook
security consequences than an outside hacker because an insider is an employee who has more 
access rights or privileges to company assets than a stranger not associated with the company. It is 
safe to assume that insider threats would increase due to the backlash associated with employees 
losing their jobs because of outsourcing. For example, a credit union employee with system admin-
istrator access was terminated and had his account disabled. Unfortunately, the credit union forgot 
to remove the individual‚Äôs remote network access or change the password that led to the credit 
union‚Äôs network being sabotaged and unavailable for three days. Th e 2004 Computer Security 
Institute (CSI)/FBI Computer Crime and Security Survey was based upon 481 participants and 
59% of those respondents reported insider abuse of their network totaled $10,601,055 (CSI/FB1, 
2004). CSI‚Äôs 2008 Computer Crime and Security Survey stated that insider abuse was the second-
most frequently occurring problem reported by 44% of its 420 respondents; down from 2007s 59% 
of previous respondents reporting insider abuse (CSI, 2008).
Espionage Cases
Sun Tzu‚Äôs Art of War is a timeless classic written around 500 BC. Th e last chapter in his book is 
titled, Employment of Secret Agents. Sun Tzu‚Äôs fourth principle states, ‚Äú‚Ä¶foreknowledge cannot 
be elicited from spirits, nor from gods, nor by analogy with past events, nor from calculations. It 
must be obtained from men who know the enemy situation‚Äù (Tzu, 500 BC, p. 145). Michaelson 
(2001) further elaborated with a modern Sun Tzu interpretation stating, ‚ÄúWhile you get their 
secrets, protect your own‚Äù (p. 132).
As a result of economic espionage, one company in particular no longer exists: Ellery Systems, 
Inc. (Texas A & M, n.d.). Ellery was working on distributed computing technology software. An 
employee from Ellery Systems went to China and returned, resigned, and proceeded to download 
the source code and sent it to a friend in China. Th e employee later admitted to meeting with 
Chinese offi  cials. Th e cost to develop the source code was $950,000with an estimated market value 
in the billions (Texas A & M, n.d.). In November 2001, two men were arrested in San Francisco‚Äôs 
airport en route to China caught with suitcases packed with microchip designs, equipment, and 
other trade secrets (Konrad, 2003). Court documents stated that the Chinese government wrote 
in documents discovered in the homes of the suspects indicating their work is ‚Äúextremely useful to 
the development of China‚Äôs integrated circuit industry‚Äù (Konrad, 2003).
Weiss (n.d.) discusses several interesting espionage facts within an article that are summarized 
and highlighted below:
Th e Soviets performed ‚Äúsuccessful clandestine eff ort to obtain technical and scientifi c knowl-
 
‚óæ
edge from the West‚Äù (Weiss, G, n.d.).
Th e operation process was code named ‚ÄúLine X‚Äù
 
‚àí
A Soviet citizen visiting Boeing‚Äôs facility had glue on his shoes to obtain metallurgic 
 
‚óæ
samples.
A Soviet delegation applied for a visa to visit a light-emitting diode watch manufacturer; 
 
‚óæ
however, three days before their arrival they expanded the trip to include computer and 
semiconductor fi rms.
Th e intentional last-minute change prevented the DoD the necessary time to object and 
 
‚àí
this maneuver was within legal constraints; thus nothing could be done
In the early 1970s, the Soviets proposed purchasing 50 planes from fi nancially strapped 
 
‚óæ
Lockheed with the stipulation Lockheed build and equip those planes in their country.

The Outsourcing of IT: Seeing the Big Picture ‚óæ 201
Line X obtained thousands of documents, samples, and technology in radar, computers, and 
 
‚óæ
semiconductors.
In the mid-1990s, the Soviet science advisor wrote the Soviets trailed the West 15 years 
 
‚óæ
in military and civil technology and a Soviet-made supercomputer was the most glaring 
absence illustrating this fact (Weiss, n.d.).
Th e United States caught onto the Soviets‚Äô Line X operation and deliberately slipped faulty pro-
grammed computer chips, a rejected NASA space shuttle design, stealth technology, and more 
into what the Soviets were covertly acquiring from U.S. companies and the U.S. government 
(Weiss, n.d). Th e various stolen technologies eventually made their way into military equipment 
and turbines used within the Soviet‚Äôs gas pipeline (Safi re, 2004). Th e New York Times printed 
an article based upon Th omas Reed‚Äôs 2004 book and Gus Weiss‚Äô historical account. Th e eff ects 
of sending the Soviets bogus technology led to an estimated three-kiloton blast in the Siberian 
wilderness in 1982. Th is occurred because the computer chips and software was designed to pass 
Soviet quality control tests, yet when put into operation, exceeded the gas pipeline‚Äôs tolerance 
(Safi re, 2004).
Some of this may appear like something from a Tom Clancy novel or as in the case with the 
Soviets, it is ancient history and someone may say that the Cold War is over and that this does 
not apply in today‚Äôs climate. During the Cold War, the United States sent naval submarines to 
tap into Soviet cables lying on the ocean fl oors (Drew et al., 1999). Th e point is why bother using 
outdated and expensive Cold War tactics when there is off shoring or the Internet in which nearly 
everything is connected? Michaelson (2001) wrote in the Art of War for Managers, ‚ÄúTh e vision of 
what the organization wants must be planned with an awareness of reality‚Äù (p. 3). Furthermore, 
on the same page, he further states
A common mistake is to consider planning as only a mental process, an idea in our 
head that simply looks at the past and adjust for the future. If your plan is not in writ-
ing, you do not have a plan at all. Instead, you only have a dream, a vision, or perhaps 
even a nightmare (Michaelson, 2001, p. 3).
Michaelson (2001) went on to state the following:
Th ere is evidence that Asian nations readily apply Sun Tzu‚Äôs lessons on ‚Äúsecret 
agents.‚Äù A survey of 1300 companies conducted by the American Society of 
Industrial Security revealed China and Japan head the list of foreign countries pos-
sessing the greatest economic espionage threat. However next in line were France, 
Great Britain, and Canada (p. 132).
A more recent assessment within the Annual Report to Congress on Foreign Economic Collection 
and Industrial Espionage, FY [U.S. fi scal year] 07 (2008) stated, ‚ÄúMany European countries 
appear to face industrial espionage threats from China, Russia, and others that are similar to 
threats facing the United States, according to the media‚Äù (p. 2).
Th e semiconductor industry has changed its strategy from building new plants in Taiwan and 
instead has chosen to locate them in China (Manufacturing & Technology News, 2003a). China has 
changed its value-added tax to a 14-point advantage in their favor, which is attracting the semicon-
ductor industry to that country. A Manufacturing & Technology News (2003b) article indicated, 
‚ÄúTh e U.S. government should require that China provide full protection for intellectual property 

202 ‚óæ Information Security Management Handbook
and increase enforcement of IP because lack of enforcement enables the piracy of semiconductor 
designs.‚Äù Manufacturing & Technology News published an article describing a debate over defense 
off sets within Congress. An off set is when foreign countries require transfer of technology or 
manufacturing capacity in order to receive a contract (i.e., from American companies). Th e term 
‚Äúeconomic bribery‚Äù was used to describe this practice by one senator (Manufacturing & Technology 
News, 2004a,b).
The Business Case and Expected Cost Savings
Most of the information recently discussed is overlooked in comparison with saving a U.S. dol-
lar. Verton (2003) indicated a 2003 Gardner study, which concluded that in 2004 ‚Äú80 percent of 
U.S. companies will consider outsourcing critical IT services, including software development, to 
countries such as India, Pakistan, Russia and China‚Äù. CIO magazine conducted a survey with its 
readers in January 2004, and 49% of the survey‚Äôs 340 respondents indicated outsourcing was the 
main reason to lower costs (Ware, 2004).
Earlier it was stated there are four ways to treat risk: ignore it, accept it, transfer it, or miti-
gate it. Th e CSI/FBI 2004 Computer Crime Survey indicated 72% of the respondents do not 
have an external insurance policy to transfer cyber security risks (CSI/FBI, 2004). Th e 2008 
CSI Computer Crime Survey indicated a mild increase from 29% (2007‚Äôs results) to 34% of the 
respondents stating they have insurance policies (CSI, 2008). And some of the risk management 
issues discussed earlier, in regard to complying with regulatory compliance statues, may actu-
ally contradict the initial expected cost savings. Blum (2004) briefl y discussed within his article 
the fact that companies may fi nd that the expected cost savings may evaporate when proper risk 
management practices, such as conducting audits on software code and background checks on 
off shore employees, are carried out. Kliem (2004) wrote this regarding outsourcing ‚Ä¶. ‚Äúthese risks 
are potentially more numerous and disabling, with the possibility of off setting the potential gains 
from outsourcing in the long run‚Äù (p. 23). Specifi cally, the initial expected or cost savings from 
outsourcing will be reduced when you take into account the severance pay, monitoring the con-
tract performance, providing oversight, legal consultation expenses for the awarding the contract, 
relocations cost, and dual payrolls (i.e., before a corporation transfers U.S. jobs overseas).
Business Impact
‚ÄúOrganizations simply lack the means and experiential research to assign value to the knowledge 
they are transferring and receiving‚Äù (Feeney et al., 2004, p. 7). If organizations or individuals 
don‚Äôt understand the value assigned to knowledge, more commonly known or referred to as ‚ÄúIP 
assets,‚Äù the business impact of outsourcing is unknown and won‚Äôt be known until it may be too 
late. Businesses are either simply unaware, ignoring, or simply accepting the risks. Security is 
overhead to most companies and it aff ects the total net profi t. Without knowing all the risks and 
benefi ts associated with a decision to outsource and properly manage risks, the same outcome 
or sequence of events discussed earlier (e.g., from Ellery Systems, Inc.; Xerox; Apple Computer; 
IBM; and possibly Boeing in the distant future now having to compete against China) will be 
repeated again.
To use the Boeing example again, it was stated that Boeing‚Äôs latest project the 787 is two years 
behind schedule due to the various problems with its outsourced partners and is making plans to 

The Outsourcing of IT: Seeing the Big Picture ‚óæ 203
bring the work back in-house amid pressure customers may cancel orders (Weber, 2009). If an 
individual is under the assumption that this is an isolated case or that Boeing is being singled out, 
far from the contrary. Overby (2006) wrote,
In the past year alone, 47 percent of companies have prematurely ended an outsourc-
ing arrangement, according to research by Diamond Management and Technology 
Consultants. Forty-three percent of them brought the work back in-house, indicating it 
may not have been a good decision to farm out the function in the fi rst place (para 4).
Ethics
‚ÄúWe have one of the few societies, the only one I can think of right off hand, where your health care 
is tied to your job, so that when an American company has to hire, they have to think about health 
care‚Äù (Congressman Frank, 2004, p. H2054). People are losing their jobs as a result of globaliza-
tion and old jobs are being lost or destroyed while new ones are being created. Th e problem is that 
while new jobs are being gained or created, it is often at a reduced pay scale and loss of benefi ts 
(Congressman Frank, 2004). In the past, education was one method of ensuring job stability or 
security within this country. ‚ÄúTh e jobs being outsourced are the jobs we used to retrain people 
for‚Äù (Congressman Frank, 2004, p. H2057). President Obama (CBS News, 2009) stated. ‚Ä¶. ‚Äúthat 
over the last decade, the average worker, the average family have seen their wages and incomes fl at. 
Even at times where supposedly we were in the middle of an economic boom, as a practical matter, 
their incomes didn‚Äôt go up‚Äù (p. 7).
Th e Economic Policy Institutes‚Äô briefi ng paper (Herrnstadt, 2008), which was referenced 
within Herrnstadt‚Äôs Congressional testimony (U.S. Congress, 2008), briefl y stated the following:
Within the United States, over 3 million jobs lost due to off sets the past few years
 
‚óæ
Sited a U.S. government report that over 16,000 jobs lost each year between 2002 and 2005 
 
‚óæ
due to defense industry off sets
Sited a 1996 GAO study where McDonnell Douglas machine tools were shipped to 
 
‚óæ
China‚Äôs business (i.e., another off set) that was transferred to a Chinese producer of military 
equipment
Sited an Aerospace Industry Commission report stating aerospace account for 15% of the 
 
‚óæ
United States‚Äôs GDP
Nearly 500,000 aerospace jobs lost since 1990 due to off sets (Herrnstadt, 2008)
 
‚àí
Th e intent of the H-1B and L-1 visa was to allow U.S. companies to recruit foreign employees as 
a result of the shortage of technical people during the ‚Äú.com‚Äù boom. However, the H-1B and L-1 
visa program is now being abused because companies are employing foreign workers at reduced 
salaries rather than available unemployed high-tech U.S. workers (e.g., Silicon Valley area). For 
example, Humphries was a 1996 Stanford graduate who lost her quality assurance engineering job 
at Palm (i.e., maker of the popular handheld PDA and Palm One software). Humphries described 
how she had to train her Indian replacement and before a Congressional hearing was quoted in 
the article saying ‚Äú‚Ä¶.we have companies more concerned about the bottom line than they are in 
preserving U.S. jobs‚Äù (Manufacturing & Technology News, 2003a,b). Interestingly, a fi nding from 
a CIO magazine survey indicated only 12% of its respondents stated their reason to use outsourcing 
resulted from a lack of qualifi ed workers or scarce skills (Ware, 2004). Ironically, McCormack (2005) 

204 ‚óæ Information Security Management Handbook
within his article detailed that the Defense Science Board stated DoD was the most logically to 
lead and encourage a national solution to save the U.S. semiconductor industry rather than them 
making a recommendation to Congress or the commerce department as it‚Äôs a private sector issue 
that started the problem. Private corporations such as Dell, Apple, HP, etc., manufacture laptops 
either in Taiwan or China, and Intel corp. has manufacturing plants across the globe.
Sectors such as law enforcement, the intelligence community, or the DoD routinely require 
processing confi dential to highly sensitive data (i.e., classifi ed). Yet, it is common knowledge that 
this group uses COTS software for their OS and databases. Th e ethical question to pose is this: 
if a specifi c vendor is a key provider or source for software used in this particular business process 
climate is that vendor informing its clients that the software is being outsourced? Th is issue was 
partially addressed within a DoD Interim Defense Acquisition Guidebook. Unfortunately, the proce-
dures listed are not mandatory and are at the discretion of acquisition program managers (French, 
2004a,b). Within the IT security profession, program managers are notorious for their focus on 
cost-cutting principles and not using that same diligence as in cutting costs to address security 
issues because that increases the costs for their project. French (2004a,b) wrote an article titled 
‚ÄúSetting Boundaries‚Äù describing a summary of the security rules for foreign affi  liates involved with 
software development based upon the previously mentioned policy. It is located below:
Vendors must indicate whether foreign affi  liates participated in software development
 
‚óæ
Foreign affi  liates employed as contractors to DoD to develop or modify code must have 
 
‚óæ
security clearances (i.e., extensive background investigations performed) equivalent to the 
level the software‚Äôs intended use
DoD software coded by foreign affi  liates must be reviewed by software quality assurance 
 
‚óæ
employees for malicious content (French, 2004a,b)
National Interest (United States and Others)
As previously mentioned, it is common knowledge that the DoD and U.S. government heavily use 
Microsoft‚Äôs operating system (OS) software and COTS computers. It was announced in February 
2003 that Microsoft would provide its OS source code to China and other countries. Apparently, 
Congress had no issue in allowing Microsoft to release the source code, and in a partial defense, 
Microsoft was responding to the open source OS threat that was eating into its market share. As it 
may not be perfectly clear, the security concern is this: since Microsoft released its source code the 
question is whether or not the intelligence community, DoD, or law enforcement agencies or other 
federal departments have experienced increased attempts by someone writing virus or malicious 
code attempting to penetrate those IT systems based upon weaknesses in the Microsoft source 
code? What does this have to do with outsourcing?
IT professionals at a security conference in South Carolina questioned the reliance on off -
shore software developers, which is ultimately putting companies and the U.S. economy at risk 
(Verton, 2003). Attendees at the security conference raised common concerns about work being 
sent to China. Verton (2003) wrote ‚Äú‚Ä¶.China has a signifi cant economic espionage program that 
targets U.S. technology.‚Äù Furthermore, another voiced concern was that some software was being 
developed in ‚Äú‚Ä¶.countries in Southeast Asia, particularly Malaysia and Indonesia where terror-
ist networks are known to exist‚Äù (Verton, 2003). During the same technology meeting it was 
acknowledged that Oracle outsources some of its software developmental work to companies based 
in India and China; however they are only granted read access (Verton, 2003). Just a thought, 

The Outsourcing of IT: Seeing the Big Picture ‚óæ 205
but wasn‚Äôt Abraham Lincoln reportedly self-taught from reading? A Government Accountability 
Offi  ce (GAO) report stated ‚ÄúDoD offi  cials‚Äô control over software, particularly which goes 
into weapons platforms is lacking‚Äù (French, 2004a,b). In addition, the Offi  ce of the National 
Counterintelligence Executive (2008) key fi nding to Congress stated:
Th e United States remains the prime target for foreign economic collection and 
industrial espionage by virtue of its global technological leadership and innovation. 
Collectors from across the globe‚Äîprivate businessmen, scientists, engineers, students, 
and foreign military and intelligence offi  cers‚Äîengaged in economic collection activi-
ties against the United States in Fiscal Year 2007 (FY 2007), according to information 
amassed by the Counterintelligence (CI) Community. While collectors came from a 
large number of countries, those from fewer than 10 nations, including both allies and 
adversaries, accounted for the bulk of targeting activity (p. ii).
Hopefully, it is clearly evident that outsourcing not only in values U.S. interests, which corporate 
leaders should consider, but also those of other nations or companies who have their own agendas.
Mitigations of Risks (Outsourcing)
Trusted Foundry
Th e National Security Agency and DoD have voiced similar security issues and concerns over 
the shift of semiconductor manufacturing capacity off shore (Manufacturing & Technology News, 
2004a,b). As previously mentioned, the Defense Science Board, recommended that DoD take 
the lead to save the U.S. semiconductor industry (McCormack, 2005). Th e NSA and DoD are 
funding a ‚ÄúTrusted Foundry‚Äù initiative owned by IBM. Th e deal with IBM will allow NSA and 
DoD to move away from their heavy reliance on COTS technology and to obtain designs of 
various technologies with strict guidelines and procedures to ensure reliability of that technol-
ogy (Manufacturing & Technology News, 2004a,b). As a follow-up to his previous article, and in 
response to the Defense Science Board‚Äôs earlier recommendation, McCormack wrote the DoD 
has stepped up the program to include microelectronics in its supply chain and that 10 companies 
were accredited as ‚Äútrusted‚Äù suppliers (2008).
Trusted Software
Th e federal government has provided a number of valuable resources to IT professionals through 
the National Information Assurance Partnership (NIAP). NIAP involves the initiative to increase 
information technology security by collaborating with industry in security testing, research, and 
the development of information assurance methodologies. From NIAP came the Common Criteria 
Evaluation and Validations Scheme (CCEVS), which is jointly managed by the National Security 
Agency and National Institute of Standards and Technology (NIST). Th e CCEVS established a 
national program for the evaluation of information technology products. Th is program is known as 
Common Criteria (CC) and it is identifi ed as International Organization for Standardization (ISO) 
15408. Under CC there are seven protection profi les. A fi rm understanding of the CC‚Äôs protection 
profi les, which also include seven evaluation assurance levels (EAL), is important for various reasons 
(Henderson and Craig-Henderson, 2004). ‚ÄúTh e CC will permit comparability between the results of 

206 ‚óæ Information Security Management Handbook
independent security evaluations‚Äù (ISO, 1999). In laymen‚Äôs terms there are strict guidelines that must 
be followed and documented and these must be verifi ed by an independent lab.
Investigation
Nothing takes the place of truly informed decision. Th oughtful and strategic planning has to 
occur fi rst before diving in and claiming money can be saved. A review of the Boeing case, dis-
cussed several times, is a perfect example. Specifi cally, if an organizational management performs 
the following:
Carefully inventories and catalogs its IP and assigns values
 
‚óæ
Performs the necessary background checks of outsourced personnel
 
‚óæ
Checks the political climate, safety, of the outsourced host‚Äôs infrastructure
 
‚óæ
Prepares a project risk management plan
 
‚óæ
Performs a return on investment with clearly defi ned metrics to measure success
 
‚óæ
Th at organization won‚Äôt be within the 47% of companies to prematurely end an outsourced deal. 
Instead, it will be within the 53% of successful companies (Overby, 2006).
Conclusion
Outsourcing IT jobs has benefi ts, if properly managed and conducted in a manner that doesn‚Äôt jeopar-
dize the loss of strategic or corporate assets. Having a clear inventory or understanding of what an orga-
nization‚Äôs intellectual property is and properly managing and having a plan for protecting those ‚Äúassets‚Äù 
is when an informed decision to outsource IT jobs can be rationally made. Otherwise, in the somewhat 
near future, those companies outsourcing today without the proper planning are not only creating their 
competition but are also continuing to create harm to their own nation in the long run.
About the Author
Foster Henderson, CISM, CISSP senior recovery planner, NSA IAM, is an information assurance 
(IA) consultant for the United States Department of Defense. He is currently an information 
assurance manager covering a wide range of IA matters.
References
ASD (NII)/DoD CIO (2009). Department of Defense Directive Number 8000.1, Management of the 
Department of Defense Information Enterprise. Retrieved March 22, 2009, from http://www.dtic.mil/
whs/directives/corres/pdf/800001p.pdf
Bellis, M. (1999a). Inventors of the modern computer: Th e history of the IBM PC. Retrieved December 4, 
2004, from http://inventors.about.com/library/weekly/aa031599.htm
Bellis, M. (1999b). Inventors of the modern computer: Th e history of the graphical user interface or 
GUI‚ÄîTh e Apple Lisa. Retrieved December 4, 2004, from http://inventors.about.com/library/weekly/
aa043099.htm
Bierce and Kenerson, P.C. (2007). Outsourcing law. Retrieved March 26, 2009, from http://www.outsourcing-law.
com/what_is_outsourcing.htm
Blum, D. (2004). Weigh risks of off shore outsourcing. Network World. Retrieved November 2, 2004, from 
http://www.networkworld.com/columnists/2004/0308blum.html?fsrc=rss-outsourcing

The Outsourcing of IT: Seeing the Big Picture ‚óæ 207
Cameron, D., Done, K., and McGregor R. (2007). China to challenge Boeing and Airbus. Financial 
Times. Retrieved March 29, 2009, from http://www.ft.com/cms/s/0/62138fe6-d575-11db-a5c6-
000b5df10621.html?nclick_check=1
CBS News. (2009). Transcript: Obama press conference March 24, 2009: Th e President Takes questions 
from the White House Press Corps. CBS News. Retrieved March from http://www.cbsnews.com/sto-
ries/2009/03/25/politics/100days/main4891818.shtml
China Daily. (2007). China becomes largest foreign supplier of Boeing parts. China Daily. Retrieved March 
27, 2009, from http://www.chinadaily.com.cn/china/2007- 09/07/content_6088635.htm
Clausewitz, C.V. (1832). Vom kreige: Translation published by Routledge & Kegan Paul Ltd 1908. Art or 
Science of War (pp. 202‚Äì203). London, U.K.: Penguin Classics.
Common Criteria: Common Criteria for information technology security evaluation Part 1: Introduction 
and general model. (2004). National Institute of Standard and Technology.
Croy, M. (2004). Th e business value of data. Disaster Recovery Journal, Summer 2004, 17(3), 20.
CSI/FBI Computer Crime and Security Survey. (2004). Federal Bureau of Investigation. Retrieved September 
15, 2004, from www.go.csi.com
CSI Computer Crime and Security Survey. (2008). CSI. Retrieved March 25, 2006, from http://www.gocsi.
com/forms/csi_survey.jhtml
Davison, D. (2004). Top 10 risks of off shore outsourcing. CIO magazine. Retrieved November 2, 2004, from 
http://searchcio.techtarget.com/news/article/0,289142,sid182_gci950602,00.html
Department of Defense Directive Number 8000.1, Management of DoD Resources and Information 
Technology. (2002). Assistant Secretary of Defense (Command, Control, Communications, and 
Intelligence {ASD (C3I)}).
Dictionary.com. (n.d.). Intellectual property. Retrieved March 21, 2009, from http://dictionary.reference.
com/browse/property?db = legal&q = property
Drew, A., Drew, C., and Sontag, S. (1999). Blind Man‚Äôs Bluff : Th e Untold Story of the American Submarine 
Espionage. New York: Perennial.
Feeney, D., Hindle, J., Lacity, M., and Willcocks, L. (2004). IT and business process outsourcing: Th e knowledge 
potential. Information Security Management. Retrieved November 15, 2004, from www.ism-journal.com
Frank, B. (2004). Employment problems in America. U.S. Congressional Record‚ÄìHouse, pp. H2054‚ÄìH2059.
French, M. (2004a). Th e outsourcing hole. Federal Computer Weekly. Retrieved October 25, 2004, from 
http://www.fcw.com/fcw/articles/2004/0719/pol-outsource-07-19-04.asp
French, M. (2004b). Setting boundaries. Federal Computer Weekly. Retrieved November 2, 2004, from http://
www.fcw.com/fcw/articles/2004/0719/pol-outsource2-07-19-04.asp
Henderson, F. and Craig-Henderson, K. (2004). Security architecture and models. In Krause, M. and 
Tipton, H. (eds.), Information Security Management Handbook, 5th edn. (pp. 1531‚Äì1554), Boca Raton, 
FL: Auerbach Publications.
Herrnstadt, O. (2008). Off sets and the lack of a comprehensive U.S. policy. Economic Policy Institute. 
Retrieved March 29, 2009, from http://www.sharedprosperity.org/bp201/bp201.pdf
Insider Th reat Study: Illicit cyber activity in the banking and fi nance sector. (2004). United States Secret 
Service/CERT Coordination Center/SEI.
International Organization for Standardization, ISO/IEC 15408-1:1999. Information technology‚ÄîSecurity 
techniques‚ÄîEvaluation criteria for IT security‚ÄîPart 1: Introduction and general model, 1999.
Kliem, R. (2004). Managing the risks of off shore IT development projects. Information Security Management. 
Retrieved November 15, 2004, from www.ism-journal.com.
Konrad, R. (2003). Judge to hear motions in Silicon Valley economic espionage case. Th e Detroit News 
Technology. Retrieved March 23, 2009, from http://www.highbeam.com/doc/1P1-85920329.html
Manufacturing & Technology News. (2003a). Engineers fear off shore outsourcing is contributing to high jobless 
rates. Retrieved November 6, 2004, from http://www.manufacturingnews.com/news/03/1104/art1.html
Manufacturing & Technology News. (2003b). How China is quickly capturing the world‚Äôs semiconductor indus-
try. Retrieved November 6, 2004, from http://www.manufacturingnews.com/news/03/0804/art1.html
Manufacturing & Technology News. (2004a). $600 million over 10 years for IBM‚Äôs ‚ÄòTrusted Foundry‚Äô chip 
industry‚Äôs shift overseas elicits National Security Agency, Defense Department response. Retrieved 
October 19, 2004, from http://www.manufacturingnews.com/news/04/0203/art1.html

208 ‚óæ Information Security Management Handbook
Manufacturing & Technology News. (2004b). ‚ÄòBuy American‚Äô raises its head once again; Th is time in a 
battle over defense ‚Äòoff sets.‚Äô Retrieved October 19, 2004, from http://www.manufacturingnews.com/
news/04/0707/art1.html
McCormack, R. (2005). Defense Science Board tells military to develop a grand strategy to save the U.S. 
semiconductor industry. Manufacturing & Technology News. Retrieved March 25, 2009, from http://
www.manufacturingnews.com/news/05/0422/art1.html
Margeson, B. (2003). Th e human side of data loss. Disaster Recovery Journal, Spring 2003, 16(2), 48. Retrieved 
from http://www.drj.com/articles/spr03/1602-08.html
Melton, H.K. (2004). Spies in the digital age. Cable News Network (CNN). Retrieved November 29, 2004, 
from http://www.cnn.com/SPECIALS/cold.war/experience/spies/melton.essay/
Mesa, A. (1998). Apple and the GUI. Retrieved December 4, 2004, from http://applemuseum.bott.org/sections/
gui.html
Michaelson, G.A. (2001). Sun Tzu: Th e Art of War for Managers, Avon, MA: Adams Media Corp.
National Academy of Engineering. (2009). Greatest engineering achievements of the 20th century. Retrieved 
March 27, 2009, from http://www.greatachievements.org/
Offi  ce of the National Counterintelligence Executive. (2008) Annual report to congress on foreign economic 
collection and industrial espionage, FY07. Retrieved March 27, 2009, from http://www.ncix.gov/
publications/reports/fecie_all/fecie_2007/FECIE_2007.pdf
Overby, S. (2003). Th e hidden cost of off shore outsourcing: Moving jobs overseas can be a much more expen-
sive proposition than you may think. CIO magazine. Retrieved October 25, 2004, from http://www.
cio.com/article/29654/Th e_Hidden_Costs_of_Off shore_Outsourcing.
Overby, S. (2006). How to say no to outsourcing. CIO magazine. Retrieved March 28, 2009, from http://
www.cio.com/article/25356/How_to_Say_No_to_Outsourcing?page=1
Pfannenstein, L.L. and Tsai, R. (2004). Off shore outsourcing: Current and future eff ects on American IT 
industry. Retrieved November 15, 2004, from www.ism-journal.com
Rao, M. (2004). Key issues for global IT sourcing: Country and individual factors. Retrieved November 15, 
2004, from www.ism-journal.com
Roden, K. (2004). Building a business case for disaster recovery planning. Disaster Recovery Journal, Summer 
2004, 17(3), 76‚Äì77.
Safi re, W. (2004). Th e Farwell dossier. Th e New York Times. Retrieved October 19, 2004, from http://courses.
dce.harvard.edu/‚àºcscie160/FarewellDosier.html
Schaaf, J. (2005). Outsourcing to India: Crouching tiger set to pounce. Deutsche Bank Research. Retrieved 
March 
25, 
2009, 
from 
http://www.dbresearch.com/PROD/DBR_INTERNET_EN-PROD/
PROD0000000000192125.pdf
Tiller, J. (2004). Outsourcing security. In Krause, M. and Tipton, H. (eds.), Information Security Management 
Handbook, 5th edn. (p. 1064), Boca Raton, FL: Auerbach Publications.
Texas A & M Research Foundation. (n.d.). Espionage killed the company. Retrieved December 11, 2004, 
from http://rf-web.tamu.edu/security/Security%20Guide/Spystory/Ellery.htm
Tzu, S. (500 BC). Th e Art of War: Translated and with an Introduction by Samuel. B. Griffi  th, Oxford, U.K.: 
Oxford University Press.
United States Congressional Congress. (2008). Testimony of Owen E. Herrnstadt. USCC. Retrieved March 28, 
2009, from http://www.uscc.gov/hearings/2008hearings/transcripts/08_07_16_trans/herrnstadt.pdf
USA Today. (2004). China studies building its own large aircraft. USA Today. Retrieved March 29, 2009, 
from http://www.usatoday.com/travel/news/2004-03-15-china-jets_x.htm
Verton, D. (2003). Off shore coding work raises security concerns. Computer World. Retrieved March 16, 2010, 
from http://www.computerworld.com/s/article/80935/Off shore coding work raises security concerns
Weber, J. (2009). Boeing to Rein in Dreamliner outsourcing. Business Week. Retrieved March 29, 2009, from 
http://www.businessweek.com/bwdaily/dnfl ash/content/jan2009/db20090116_971202.htm
Weiss, G.W. (n.d.). Doping the Soviets: Th e farewell dossier. Retrieved October 19, 2004, from https://www.cia.
gov/library/center-for-the-study-of-intelligence/csi-publications/csi-studies/studies/96unclass/farewell.htm
Th e World Factbook. (2008). Central Intelligence Agency. Retrieved March 25, 2009, from https://www.cia.
gov/library/publications/the-world-factbook/index.html

209
14
Chapter 
Understanding Information 
Risk Management
Tom Carlson and Nick Halvorson
What Is Information Risk Management?
Th e discipline of information security may be considered as a subset of an organization‚Äôs overall 
risk management strategy. Information security is a focused initiative to manage risk to infor-
mation in any form. Risk management concepts, when applied to information risk are readily 
managed within the context of an information security management system, or ISMS. An ISMS 
Contents
What Is Information Risk Management .................................................................................. 209
Why Information Risk Management ........................................................................................210
Background ..............................................................................................................................210
How Is Information Risk Management Implemented ..............................................................210
Th e Nature of Risk ..............................................................................................................210
Strategic Risk ..................................................................................................................210
Tactical Risk ....................................................................................................................210
Operational Risk .............................................................................................................210
Th e Process of Risk Management .........................................................................................210
Step 1: Set Scope .............................................................................................................211
Step 2: Identify Risk ........................................................................................................211
Step 3: Quantify Risk ......................................................................................................213
Step 4: Address Risk ........................................................................................................214
Step 5: Mitigate Risk .......................................................................................................215
Step 6: Measure Performance ..........................................................................................216
Conclusion ...............................................................................................................................218
About the Authors ....................................................................................................................218

210 ‚óæ Information Security Management Handbook
is a process-based program management approach and furnishes a framework from which to 
administer risk management processes.
Why Information Risk Management?
Robust risk management processes identify and quantify areas of information risk, and allow for 
the development of a comprehensive and focused risk treatment plan.
A clearly defi ned risk assessment methodology is a mandatory component in legal/regulatory 
 
‚óæ
compliance.
Th e corresponding risk treatment plan documents informed choice, decision making, and 
 
‚óæ
organizational due diligence.
Background
Business is all about risk and the management of any enterprise is in some manner enterprise 
risk management. Traditionally, the term risk management has been focused on fi nancial and 
fi duciary business risk. Th e increased importance of information confi dentiality, integrity, and 
availability in managing an enterprise has caused the recognition that information risk is equally 
concerning the health and well-being of an enterprise and should be managed accordingly.
How Is Information Risk Management Implemented?
The Nature of Risk
Risk may be strategic, tactical, or operational.
Strategic Risk
Strategic risk is risk to the existence or profi t of the organization and may or may not have informa-
tion security signifi cance. Such risk includes regulatory compliance and fi duciary responsibility, as 
well as risk to the revenue and reputation to the organization.
Tactical Risk
Tactical risk is risk to the information security program‚Äôs ability to mitigate relevant strategic risk 
to information. Such program risk includes the ability to identify relevant regulations, identify 
and justify control objectives, and justify information security initiatives.
Operational Risk
Operational risk is concerned with the ability to implement the tactical risk‚Äìbased control objectives. 
Such risk includes budget, timelines, and technologies.
The Process of Risk Management
In its most basic form, the Risk management process is a closed loop, or iterative, providing a 
feedback mechanism for continuous process improvement (Figure 14.1).

Understanding Information Risk Management ‚óæ 211
Th e current ISO 27005 standard addresses the application of this process as an information security 
technique. A process-based ISMS provides the framework within which to implement this technique.
Step 1: Set Scope (Figure 14.2)
Information Security Program
A comprehensive information security program should address strategic, tactical, and operational 
risk. An information security program is a strategic risk initiative, managed by a tactical risk‚Äìbased 
ISMS. Th is structure allows ready identifi cation and mitigation of operational risk. For example:
Th e scope of strategic risk is enterprise wide and focused on the risk-mitigating services 
 
‚óæ
required by the enterprise.
Th e scope of tactical risk is program wide and focused on the risk-mitigating processes 
 
‚óæ
required by the strategic services.
Th e scope of operational risk is based upon a discrete domain that stores, transmits, or 
 
‚óæ
processes information in any form. Th is domain-specifi c risk is focused on the people, pro-
cedure, and products that integrate into risk-mitigating processes.
An ISMS-based information security program is conducive to scoping and managing multiple risk 
domains while simultaneously identifying and maintaining both vertical alignment and horizon-
tal dependencies (Figure 14.3).
Step 2: Identify Risk (Figure 14.4)
Threat Forecasting
Th reats are negative events that occur when a vulnerability or weakness is exploited. Th reat fore-
casting is a proactive process to predict future risk based on identifi ed or perceived vulnerability.
Th reats span the organization at all levels.
Th reats may be strategic, or enterprise wide such as regulatory noncompliance.
 
‚óæ
Th reats may be tactical based upon organizational vulnerabilities such as ineff ective programs.
 ‚óæ
Th reats may be operational based upon technical vulnerabilities.
 
‚óæ
Quantify
risk
Mitigate
risk
Address
risk
Measure
performance
v
Identify risk
Set scope
Figure 14.1 Risk management process.
Set scope
Identify risk
Address
risk
Quantify
risk
Measure
performance
Mitigate
risk
Figure 14.2 Set scope.

212 ‚óæ Information Security Management Handbook
Th reat forecasting examines multiple information sources, or sensors. Th reat sensors may include
Legal/regulatory analysts
 
‚óæ
Program reviews
 
‚óæ
Technical bulletins from vendors or analysts
 
‚óæ
Th e potential rate of change to the threat environment must be considered, and may drive the 
frequency of triggering the threat forecasting processes. For example, a strategic threat such as 
noncompliance with emerging regulations typically has a longer tolerable reaction time than an 
operational threat such as emerging technical vulnerabilities.
Incident Evaluation
Incidents are threats that have occurred, or in other words, a vulnerability has been exploited 
to cause an event resulting in an incident. Incident evaluation, although triggered reactively, is 
proactive because of the ‚Äúlessons learned‚Äù that can be utilized to both identify the underlying 
vulnerabilities and predict future probability of reoccurrence. Forensic, or ‚Äúroot cause‚Äù analysis 
Set scope
v
Identify risk
Quantify
risk
Address
risk
Mitigate
risk
Measure
performance
Figure 14.4 Identify risk.
Call center
Service 1
Service 2
Information security program
Risk management policy
Service 3
Service 4
Service 5
Domain specific information security management system (ISMS)
Operational risk
Tactical risk
Strategic risk
Server
room
Collection
center
Server
room 2
Collection
center 2
Managed domains
Supporting security services
Vision
Figure 14.3 ISMS-based information security program.

Understanding Information Risk Management ‚óæ 213
will illuminate technical and procedural weaknesses, and performance analysis will illuminate 
effi  ciency and eff ectiveness weakness.
Step 3: Quantify Risk (Figure 14.5)
Risk Assessment
Th e processes of threat forecasting and incident evaluation identify relevant threats and vulner-
abilities; however, relevant threats and vulnerabilities are not necessarily risks. Identifi ed threats 
and vulnerabilities must be quantifi ed to determine the existence and magnitude of risk within the 
applicable environment. Quantifi ed risk allows for defensible prioritization of remediation eff orts 
as well as informed choice (defensible) decision making.
Assessment Scope
Strategic assessment
Strategic risk assessments look at enterprise business processes that span multiple domains. Not all 
assessed business processes have information risk.
Tactical assessment
Tactical risk assessments look at the ability of the information security program to identify and 
mitigate relevant strategic risk to information.
Operational assessment
Operational risk assessments look at a domain‚Äôs ability to meet tactical control objectives in 
protecting specifi c information assets. Technical vulnerability assessments are an example of a 
specifi cally focused type of operational risk assessment.
Assessment Framework ‚Äî A risk assessment framework assists in maintaining structure 
during the risk assessment process since it may be diffi  cult to make sense of the diverse collec-
tion of threats and vulnerabilities that fl ow from ‚Äúworst-case‚Äù scenario brainstorming. A risk 
assessment framework allows both organization of thought and recognition of relationships 
between this diverse collection of threats and vulnerabilities. Starting with the premise that 
information risk is based upon breaches of confi dentiality, integrity, and availability, a risk 
assessment framework can be further subdivided into, for example, intentional and accidental 
components. Further subdivisions result in creation of a ‚Äúthreat tree‚Äù that allows organized 
‚Äúcataloging‚Äù of risk, and enhances the ability to ask and analyze appropriate risk questions. 
For example:
Th reat: Breach of confi dentiality
Intentional disclosure
 
‚óæ
Vulnerability
 
‚àí
: Poorly vetted employees
Set scope
Identify risk
Quantify
risk
Address
risk
Mitigate
risk
Measure
performance
v
Figure 14.5 Quantify risk.

214 ‚óæ Information Security Management Handbook
Unintentional disclosure
 
‚óæ
Vulnerability
 
‚àí
: Unencrypted information
Vulnerability
 ‚àí
: Ineff ective media disposal
Note the structured thought process resulting in discrete vulnerabilities being mapped to a 
common threat.
Risk Quantum ‚Äî Risk quantifi cation is based upon identifi cation of relevant variables that are then 
incorporated into a risk rating algorithm. A quantitative assessment requires much more eff ort than 
a qualitative assessment, but may be necessary when, for example, using the resultant risk rating to 
make fi nancial (quantitative) decisions. Typical qualitative risk quantifi cation utilizes two independent 
variables, probability (likelihood), and harm (impact). Risk-rating algorithms vary in sophistication 
depending on the level of detail and accuracy required to be furnished by the assessment.
Probability ‚Äî Probability may be seen as having three attributes. Total probability must take 
into consideration all aspects:
Frequency
 
‚óæ
: how often the scenario can be expected to occur
Simplicity
 
‚óæ
: the level of eff ort required to create the scenario
Motive
 
‚óæ
: the determination of the attacker
Frequency and simplicity are relevant for each vulnerability, whereas motive is relevant to the orga-
nization. For example, an externally facing fi rewall has a high probability of penetration attempts 
(frequency) but a low probability of success (simplicity). A defense contractor or fi nancial institu-
tion may generate more focused attention than a home PC user (motive).
Harm ‚Äî Harm is the impact successful execution of the event would cause the organization. 
Since harm is many times aligned to a particular tangible asset, another view sometimes used in 
risk assessment is value where value is perceived in terms of availability, and harm perceived as 
absence. Th is view is more common in enterprise business process risk assessment.
Raw Risk ‚Äî Th e identifi ed vulnerabilities quantifi ed through an algorithm (of your choice) 
utilizing the independent variables of probability and harm constitute raw risk, or risk before the 
application of controls. Raw risk serves as a baseline of your threat exposure or risk environment. 
Raw risk also acts as the basis of ‚Äúbefore and after‚Äù views, modifi ed as controls are factored in to 
calculate residual (post-control) risk. An unacceptable level of raw risk serves as the justifi cation 
for implementing mitigating controls.
Step 4: Address Risk (Figure 14.6)
Risk Tolerance
Having identifi ed and evaluated the risks attached to specifi c vulnerabilities, the risks must be 
addressed. Decisions on risk are based upon the organization‚Äôs risk tolerance thresholds and 
include the following options:
Avoid Risk ‚Äî Risk may possibly be avoided, for example, by relocating a data center.

Understanding Information Risk Management ‚óæ 215
Transfer Risk ‚Äî Risk may be transferred to someone with a higher risk tolerance, for example, 
an insurance company.
Accept Risk ‚Äî Risk may be accepted although diligence requires care regarding
Who is authorized to accept what level of risk
 
‚óæ
How is risk acceptance based upon ‚Äúinformed choice decision making‚Äù
 
‚óæ
Whether the aggregation of accepted risk remains tolerable
 
‚óæ
Mitigate Risk ‚Äî Risk may be mitigated to an acceptable level through the application of 
compensating controls.
Step 5: Mitigate Risk (Figure 14.7)
It is not practical to completely eliminate risk, only to reduce risk to an acceptable level.
Control Objectives
Control objectives serve as the glue to bind specifi c vulnerabilities to specifi c controls. Defi ning 
control objectives is the fi rst step in deriving the corresponding control requirements to mitigate 
the risk associated with the vulnerability. Control objectives give a risk-based justifi cation to 
allocation of resources.
Selection of Controls
Once control requirements have been derived from control objectives, tangible controls may be 
selected.
Discretionary Controls ‚Äî Discretionary controls are controls that can weigh cost versus benefi ts. 
In general, the cost of mitigating a risk needs to be balanced with the benefi ts obtained. Th is is 
Set scope
v
Identify risk
Quantify
risk
Address
risk
Mitigate
risk
Measure
performance
Figure 14.6 Address risk.
Set scope
v
Identify risk
Quantify
risk
Address
risk
Mitigate
risk
Measure
performance
Figure 14.7 Mitigate risk.

216 ‚óæ Information Security Management Handbook
essentially a cost-benefi t analysis on ‚Äúat what cost‚Äù the risk is acceptable. It is important to consider 
all direct and indirect costs and benefi ts whether tangible or intangible and measured in fi nancial or 
other terms. More than one option can be considered and adopted either separately or in combina-
tion. For example, mitigating controls such as support contracts may reduce risk to a certain degree, 
with residual risk transferred via appropriate insurance or risk fi nancing.
Mandatory Controls ‚Äî Mandatory controls diff er from discretionary controls in that cost has 
no bearing on the selection of mandatory controls. Th ese are controls that must be implemented in 
order to mitigate specifi c risks. Th ere may be no risk acceptance option due, for example, to legal 
and regulatory requirements.
Risk Treatment
Development of Action Plan ‚Äî Th e organization requires a treatment plan in order to describe 
how the chosen controls will be implemented. Th e treatment plan should be comprehensive and 
should document all necessary information about
Proposed actions, priorities, or time plans
 
‚óæ
Resource requirements
 
‚óæ
Roles and responsibilities of all parties involved in the proposed actions
 
‚óæ
Performance measures
 
‚óæ
Reporting and monitoring requirements
 
‚óæ
Action plans may have strategic, tactical, and operational components, and should be in line with 
the culture, values, and perceptions of all stakeholders.
Approval of Action Plan ‚Äî As with all management plans, initial approval is not suffi  cient 
to ensure the eff ective implementation of the action plan. Senior management support is critical 
throughout the entire life cycle of the plan. By its nature, an ISMS is an empowerment vehicle for 
risk treatment, with clear trickle down authority documenting management support and authori-
zation to the highest levels.
Implementation of Action Plan ‚Äî An important responsibility of the action plan owner is to 
identify requirements and procure necessary resources to implement the plan. Th is may include 
such tangibles as people, process, and products, the component parts selected to meet the required 
control objectives. In the event that available resources such as budgets are not suffi  cient, the risk of 
not implementing the action plan must ultimately be accepted by someone. Th e risk management 
model allows transference of risk to a willing risk acceptor, and the ISMS framework provides the 
means of transference.
Step 6: Measure Performance (Figure 14.8)
A critical success factor (CSF) for the risk management process is to strategically reduce risk to 
an acceptable level. A key performance indicator is the tactical ability to reach this steady state, 
or equilibrium, through the judicious selection and deployment of effi  cient and eff ective controls. 
Operational metrics can be used to evaluate control effi  ciency and eff ectiveness.

Understanding Information Risk Management ‚óæ 217
Risk Metrics
Th ere are various types of risk metrics that may benefi t the information security program.
Process Metrics ‚Äî A process by defi nition has a CSF defi ning the successful execution of the 
process. Th e CSF is evaluated via process key performance indicators. Key performance indicators 
are evaluated via process metrics. Whereas process design deals with process eff ectiveness, pro-
cess execution deals with process effi  ciency. For example, a risk-mitigating operational ‚Äúincident 
response‚Äù process (a reactive control) has been designed to be tactically eff ective, but the perfor-
mance indicators look at operational effi  ciency factors such as ‚Äútime to respond.‚Äù
Program Metrics ‚Äî Program metrics typically measure process eff ectiveness. Th ese tactical pro-
cess eff ectiveness metrics require a ‚Äúhistory‚Äù against which to measure, with value being enhanced 
by history length. Th is type of evaluation is synergistic with maturity modeling since maturity 
modeling is by nature history based.
Environmental Metrics ‚Äî Environmental metrics are of value when trying to evaluate an orga-
nization‚Äôs risk profi le and resultant risk strategy. For example, a response process (reactive control) 
may be triggered frequently, giving insight into the external environment. Th is metric says nothing 
about the effi  ciency or eff ectiveness of the information security program, but may add justifi cation 
to its existence or tactics.
Control Attributes
Controls in this context may be seen to have two independent attributes
Maturity ‚Äî As risk treatment progresses, controls remain in varying degrees of maturity. Factoring in 
the maturity level of the various types of controls on a standardized scale allows the ability to quantify 
eff ectiveness in progress toward meeting control objectives and the resultant reduction of risk.
Weight ‚Äî Controls may be considered:
Directive
 
‚óæ
Preventive
 
‚óæ
Detective
 
‚óæ
Reactive
 
‚óæ
In some environments, there is merit in weighting the value of a specifi c category of con-
trol. For example, in a risk-intolerant environment such as the nuclear industry, a preventive 
control may be far more valued than detective and reactive controls and should be weighted 
accordingly.
Set scope
Identify risk
Quantify
risk
Address
risk
Mitigate
risk
Measure
performance
v
Figure 14.8 Measure performance.

218 ‚óæ Information Security Management Handbook
Residual Risk
Residual risk is the risk that remains after risk treatment. Residual risk is derived from raw risk, with 
an algorithm typically utilizing risk-mitigating control attributes to modify the raw risk environment. 
Untreated residual risk is essentially de facto accepted risk. Since the objective of the iterative risk 
management process is to reduce residual risk to an acceptable level, the risk management process 
may therefore require multiple passes to reach this goal. For example, a vulnerability management 
process that tracks the system patching life cycle may require multiple iterations before an acceptable 
residual risk of 5% unpatched (95% patched) is achieved.
Conclusion
Information security is a focused application of risk management, managing risk to information in 
any form based upon the risk criteria of confi dentiality, integrity, and availability. An information 
security program is hence a subset of an organization‚Äôs risk management program, and is readily 
managed within the context of a process-based ISMS. ISMS and risk assessment frameworks add 
structure to the information security program, clearly delineating risk roles and responsibilities. 
A process-based approach is repeatable, defensible, and extensible, off ering metrics to optimize 
effi  ciency and eff ectiveness while reducing risk to an acceptable level.
About the Authors
Tom Carlson, CISSP, is a principal consultant and ISMS practice lead for Information Security 
Management Systems (ISMS) and ISO 27001 certifi cation.
Nick Halvorson, CISSP, is the information security manager for Merrill Corporation headquar-
tered in St. Paul, Minnesota.

219
15
Chapter 
The Sarbanes‚ÄìOxley 
Revolution: Hero 
or Hindrance?
Seth Kinnett
A headache. A frustration. A nightmare. Each of these terms comments on the seemingly universal 
sentiment over Sarbanes‚ÄìOxley (SOX). A review of available journal literature surrounding SOX reveals 
the frustrations facing companies aff ected by SOX. Supporters of heavy regulation might dismiss these 
responses as typical or expected reactions by the business world. Certainly regulatory requirements 
are not always embraced by the corporate community, but SOX appears to have rattled them even more 
than ever. But the purpose of this essay is not to debate the worth of the regulations, nor to justify them. 
Rather, we will begin our discussion by examining the impacts that SOX compliance has wrought 
upon various institutions. Having established the vast environment aff ected by the SOX legislation, we 
will then turn to particulars of implementation such as pain points, costs, and frustrations, including 
those by actual people within the organizations aff ected. Finally, we will synthesize this information to 
determine the high-level impact and potential benefi ts of SOX on organizations and how it will aff ect 
their ability‚Äîbetter or worse‚Äîto thrive in the evolving business environment.
To begin, it is important to understand that SOX is not a narrow initiative. In fact, ‚Äúaccording 
to the Public Company Accounting Oversight Board, 15,000 U.S. companies, 1,200 non-U.S. 
based companies and 1,423 accounting fi rms spread across 76 countries are aff ected by SOX‚Äù 
(Braganza and Franken 2007). Th is example is eye-raising to many who falsely assume SOX applies 
only to fi nancial institutions. In fact, SOX applies to fi nancial reporting, which is something that 
every company‚Äîto some degree or other‚Äîconducts as part of its business. While fi nancial insti-
tutions may have additional requirements or challenges, SOX is a far-reaching initiative aff ecting 
Contents
About the Author .................................................................................................................... 226
References ............................................................................................................................... 226

220 ‚óæ Information Security Management Handbook
many organizations. ‚ÄúIn particular, Section 404, which deals with management‚Äôs assessment of 
internal controls, aff ects CIOs and information technology departments‚Äù (Brganza and Franken 
2007). We will examine fi nancial institutions in some level of detail, however, as an emblematic 
institution that fi nds itself complying with the SOX mandates, but we will also draw examples 
from other industries. Th ere are a number of considerations surrounding SOX that apply to mul-
tiple business verticals. By way of providing background, we should note that SOX is a piece of 
regulatory legislation passed by the U. S. Congress in 2002. Th e impetus for the legislation came 
most publicly from scandals, but also ‚Äúbusiness structures and reporting processes have become 
increasingly complex, leading to redundancies and ineffi  ciencies‚Äù (Giniat and Saporito 2007). 
Put another way, the enactment of SOX refl ected a ‚Äúneed to enhance the independence of SEC 
fi nancial statement audits by shifting responsibility from audit oversight and auditor selection 
to an audit committee composed of three independent directors, and prohibiting auditors from 
providing certain consultation services to clients‚Äù (Drexler 2006). Th e regulation has a number of 
components and nuances that are of import to companies, but at a high level, ‚Äúthe main eff ect of 
Sarbanes‚ÄìOxley is to move the ultimate responsibility for the accuracy of fi nancial reports from 
the outside auditor to the company‚Äôs management‚Äù (Needleman 2008). Th e SOX environment 
as 2008 draws to a close is an evolved one as more and more organizations gain their footing in 
implementing controls and developing an appropriate compliance strategy. Most companies had 
knee-jerk, reactive responses to SOX. Th ey began hammering their IT departments to implement 
the required controls. Indeed, ‚Äúmuch of SOX addresses IT functions, as IT is an integral com-
ponent of fi nancial transaction processing in any enterprise‚Äù (Needleman 2008). Th is response, 
while perhaps eff ective in the short term, is dangerous. As time continues to pass and it becomes 
increasingly clear that SOX is not going away, ‚Äúcontrols must not be viewed as a single event, but 
as a changing process that must be monitored and reviewed on a regular basis‚Äù (Kumar 2006). 
While the ‚ÄúBand-aid‚Äù approach may have worked in 2003 and 2004 and even 2005, most com-
panies now already have addressed the basics. ‚ÄúNow that companies have completed the initial 
implementation of SOX controls, it is time to create a framework for ongoing, sustainable SOX 
compliance‚Äù (Kumar 2006). Th e issue of control specifi city within SOX is one that has been the 
subject of some debate.
While Sarbanes‚ÄìOxley may have initially been viewed as a panacea for better report-
ing, some of its critics argue that the initial implementation of the law has focused too 
much on the detailed process-level controls instead of on the company-level monitor-
ing controls. In eff ect, the critics contend, there has been too much focus on a cover-
age-based approach instead of a risk-based approach (Giniat and Saporito 2007).
Th roughout this discussion we will examine a combination of specifi c, IT control considerations 
as well as broader initiatives such as company culture and how it may aff ect an environment of 
compliance.
To the former, one spillover benefi t of regulation has been the economic benefi t to software 
companies that have responded to SOX by deploying new tools to provide the controls so many 
technology departments desperately need. Applications such as the Axentis Financial GRC Suite 
‚Äú[provide] base services, including organization management, knowledge management and com-
munication management, as well as reporting and analytics, and audit trail tracking. To these 
base services, the application adds risk and control management, which directly addresses Section 
404 concerns‚Äù (Needleman 2006). While it is never a good idea to view a software application as 
a complete business solution, these applications can contribute toward positive discussions about 

The Sarbanes‚ÄìOxley Revolution: Hero or Hindrance? ‚óæ 221
how to shape an organization toward a culture of compliance and accountability. Th ose two words 
underscore the very purpose of SOX and bring us back to a remembrance of the unfortunate issues 
that brought about the legislation in the fi rst place.
It is important to place this sort of discussion within a practical industry context so as to frame 
it in as real an environment as possible. Th e fi nancial industry will serve well in this regard. We 
will examine high-level fraud and compliance in this fi eld as well as the specifi c impacts, costs, 
and benefi ts that compliance has placed on this vertical. First, it is critical to note that this is an 
industry that certainly contributed to the advent of SOX. As an example, ‚Äúin the 2006 ACFE 
Report to the Nation on Occupational Fraud & Abuse, 14 percent of the 1134 fraud cases reported 
by the Certifi ed Fraud Examiners who investigated them were from the banking/fi nancial ser-
vices industry‚Äù (Magliozzi 2007‚Äì2008). Since then, they‚Äîlike all verticals‚Äîhave had time to 
change their ways and respond to the eff ects of regulation. As we noted above, this is the time for 
companies to transition away from their control-based responses and look toward a sustainable 
strategy. Th is will be a challenge, but one which may prove very benefi cial. ‚ÄúMany fi nancial insti-
tutions, including asset management companies, hedge funds and companies undergoing their 
initial registration process, operate with manually intense processes. Automating these processes 
can yield improvement in the quality of the control environment, while reducing signifi cantly 
the cost of SOX compliance‚Äù (Magliozzi 2007‚Äì2008). Still, the challenge of moving IT resources 
away from the day-to-day grind of support and reactivity toward long-term process-improvement 
initiatives is a large one. Particularly in this environment in which fi nancial institutions are strug-
gling simply to survive, the short-term costs of strategic initiatives are perceived as far greater 
than the long-term benefi ts. To make matters worse ‚Äúfor fi nancial institutions, the cost of SOX 
compliance is exacerbated by the costs associated with the myriad other laws and regulations with 
which they must comply‚Äù (Magliozzi 2007‚Äì2008). Th ere is a need‚Äîif for no other reason than 
sanity‚Äîto view SOX in a more positive light than refl exes dictate. Smart companies will use the 
SOX regulations to help them update their systems, processes, and security. ‚ÄúA fi nancial institu-
tion should aim to make the SOX eff ort effi  cient while improving overall quality over time‚Ä¶. 
Positive change can occur as companies move from a manual, detective and ad hoc state to a more 
evolved systems-based, preventive and managed state‚Äù (Magliozzi 2007‚Äì2008). But unfortunately 
many fi nancial institutions have not adopted this model. In an article written in late 2007 for Bank 
Accounting & Finance Magazine, author Rick Magliozzi made it clear that progress still needs to 
be made, writing that ‚Äúreducing the cost of SOX compliance and improving the quality of the key 
business processes are needed if the fi nancial institution is to remain competitive in today‚Äôs mar-
ketplace‚Äù (Magliozzi 2007‚Äì2008). One bit of relief may be in store for banks and fi nancial insti-
tutions, however, in the form of greater fl exibility on the part of the SEC as they have ‚Äúendorsed 
the recommendations of the agency‚Äôs professional staff  to eliminate ‚Äòwaste and duplication‚Äô in 
companies‚Äô compliance with the Sarbanes‚ÄìOxley Act‚Äù (SEC‚Ä¶. 22). Of course, it may be some 
time before all the effi  ciencies are put into place, and organizations rarely have the time, energy, or 
resources to take steps back to strategically analyze these kinds of issues. We will now take a closer 
look inside the inner workings of an organization and examine some of the issues facing executives 
as they strive to comply with SOX. Considering the political and power-play issues inherent in 
this kind of situation, companies may fi nd that the specifi c technical requirements of SOX are, 
in fact, the least of their worries.
In a truly fascinating article, authors Ashley Braganza and Arnoud Franken studied the rela-
tionships between C-level executives as they interact within the context of implementing and 
executing on SOX compliance. Th e study is very revealing into the sorts of political and power 
games that take place within the corporate environment, and the SOX challenge is certainly not 

222 ‚óæ Information Security Management Handbook
immune from these games. Th is examination provides a very distinct insight into our discussion, 
as it allows us to deviate from simply analyzing a particular business environment and the tech-
nical and operational business considerations surrounding SOX compliance and allows a brief 
examination of the human function within this process. It is important for everyone involved in 
the compliance process to understand the human element, for it is the most diffi  cult to mediate or 
to predict its impact. In their study, the authors examined CEOs, CFOs, CIOs, and auditors.
Early in their article they reveal that they ‚Äúuse power relationships as the basis for [their] analysis 
because previous empirical research shows that CIOs have relatively little power in organizations‚Äù 
(Braganza and Franken 2007). In one example cited in their study, the authors described the 
power relationships taking place between the CIO and the CFO in the context of the 404 SOX 
requirements. ‚ÄúTh e CFO‚Äôs infl uence is directed at pushing 404 requirements on to the IT depart-
ment; whereas the CIO seeks to create the environment for the fi nance department to understand 
IT‚Äù (Braganza and Franken 2007). In this same example, the CIO of the company in question 
is part of an organization in which he does not have direct access to the CEO, ‚Äúwhich can be of 
concern especially where the CIO has a signifi cant role to play in the implementation of 404. 
Moreover, it implies the CIO has access to the CEO via the CFO, which gives the CFO a political 
edge over the CIO‚Äù (Braganza and Franken 2007). An organization with this type of structure 
is in trouble. Sarbanes‚ÄìOxley is clearly a complicated process that requires an effi  cient and intel-
ligent use of company resources. Th e amount of people involved and the challenges implicit simply 
in achieving compliance are numbered and great, and there is absolutely no room for political 
posturing among C-level executives. By marginalizing the CIO, the CEO of this organization 
has actually hurt his company overall by failing to acknowledge the critical role that technology 
plays in not just SOX compliance, but in making the organization run smoothly each and every 
day. Other examples in this study, while exhibiting some level of variance, basically underscore 
the larger point that the CIO is‚Äîas the authors suggested at the beginning of the study‚Äîat the 
bottom of the proverbial totem pole when it comes to executives. It is diffi  cult to draw conclusions 
about how or why this hierarchy evolved or how it has persisted. Auditors, armed with this infor-
mation, should be encouraged to form strong relationships with the CIO and to be understanding 
of his position in the fi rm. Th e CIO is an important player and, when treated as more than just 
an implementer of controls, may have valuable insights to lend to the SOX compliance process. 
Th e authors of the study suggest that ‚ÄúCIOs widen their base of relationships and develop new 
internal power and infl uence relationships when implementing 404 compliance‚Äù (Braganza and 
Franken 2007).
We have now examined what SOX is: a number of control-specifi c issues that companies must 
understand in order to comply with the regulation, and external challenges such as politics and 
power games that executives might fi nd themselves in during this process. Ultimately, however, 
the government has a responsibility to impose regulations to support the best interests of the 
people. Clearly, in a capitalist system, it is imperative that companies are reporting accurate and 
honest fi nancial data and it is in the best interest of the public for them to do so. We must also 
consider, however, the impact of these regulations on companies and ensure that the benefi ts real-
ized by regulation do not outweigh the costs‚Äîboth direct and indirect‚Äîon companies.
In an article for Strategic Finance magazine, Paul Sharman likens the SOX process to a long 
car trip. In this analogy, ‚Äúthe fi nal destination Congress defi ned was ‚Äòmore reliable auditor certi-
fi ed fi nancial disclosures.‚Äô Th e travel planner is the Securities and Exchange Commission (SEC) 
and its new subsidiary, the Public Accounting Oversight Board (PCAOB)‚Äù (Sharman 2007). In 
Sharman‚Äôs estimation, the trip is not going very well, with costs outweighing those predicted by 
lawmakers. ‚ÄúAs a result, the U.S. has been slowly losing ground to other countries whose markets 

The Sarbanes‚ÄìOxley Revolution: Hero or Hindrance? ‚óæ 223
aren‚Äôt as cumbersome and restrictive‚Äù (Sharman 2007). During diffi  cult economic times it can 
be tempting to rollback regulations in favor of corporate growth and profi tability. What many 
neglect to realize is that it may be the previous lack of regulation that caused circumstances to 
turn so dire in the fi rst place. Realizing, however, that it is diffi  cult to deny the challenges that 
SOX has placed upon American businesses, it is more reasonable to suggest targeted reforms to 
streamline compliance requirements without sacrifi cing quality and accountability. We now turn 
to outlining some specifi c shortcomings that others have analyzed and which would appear to be 
positive targets for reform.
Areas for improvement exist throughout the SOX compliance process, from documenta-
tion requirements to external auditor accountability and standards. To the latter, ‚ÄúIMA and FEI 
(Financial Executives International) research indicates there is massive variability between various 
audit pass/fail graders. Even graders from the same CPA fi rm vary in their views on how much and 
what kind of controls are enough to pass. Th e grading criteria that auditors are using are far from 
clear and haven‚Äôt been empirically validated as appropriate‚Äù (Sharman 2007). Th is is not such a 
surprising fi nding. It is challenging to strike a balance between mandating specifi cs and trusting 
professionals to use their expertise to make decisions. Depending on the research, however, it may be 
useful to defi ne a more targeted list of requirements for auditors. Th ere appears to be a large degree 
of dissatisfaction with the SEC across multiple areas of the SOX process. Some research suggests that 
the SEC was too reliant on existing systems and processes and did not provide targeted means to 
address specifi c new challenges brought about by SOX. According to Sharman, ‚Äúthe SEC in eff ect 
abdicated responsibility for providing adequate guidance for management to assess and report on 
internal control by assigning the PCAOB the responsibility to develop external auditor guidance in 
the form of Auditing Standard No. 2 (AS2). Th e problem is that the PCAOB‚Äôs mission and guidance 
are intended to support the external auditors‚Äîusing decades-old thinking and very traditional audit 
standards‚Äù (Sharman 2007). Couple this anti-executive orientation with the political implications we 
already discussed facing executives, and it is not too diffi  cult to see how SOX can become a signifi -
cant challenge for businesses. Reforms to this portion of the regulation should focus on developing 
a new innovative guidance for executives, clearly outlining high-level points that they can follow. 
Th ese do not necessarily have to replace lower-level auditor specifi cations, but executives are used to 
big-picture views throughout their jobs, and SOX regulators should strive to capitalize on this where 
possible. Th e SEC has already begun to respond to ‚Äúsuggestions‚Äù for improvement. After the SEC 
eased some compliance requirements, the response was generally favorable.
‚ÄúTo a great extent, the new SEC rule and the new auditing standard by the Public 
Company Accounting Oversight Board deliver what we‚Äôve hoped,‚Äù says George 
Yungmann, senior vice president of fi nancial standards at the National Association of 
Real Estate Investment Trusts based in Washington, D.C. Th e new rules allow compa-
nies to conduct a top-down, risk-based analysis of their internal controls, says Yungmann, 
rather than getting into a hyper-detailed review. ‚ÄúIt brings us up out of the weeds,‚Äù he 
says, ‚Äúand the new rules are intended to be scalable for companies.‚Äù (Filisko, 2007)
Th ese are the kinds of adjustments that are understandable and necessary for a young mandate such 
as SOX. Businesses could assist in this improvement by providing targeted feedback and analysis as 
opposed to brute force lobbying or complaining, which has largely been the status quo.
Despite the frustrations we have examined so far, SOX is not entirely a negative initiative. 
Consider that organizations may even be better off  for having been forced to comply with SOX. 
One article notes that ‚Äúwhile SOX compliance can seem like an added burden, the upside is that 

224 ‚óæ Information Security Management Handbook
creating a SOX-compliant credit function pays off  handsomely in terms of achieving repeatable 
and transparent processes and procedures, increased effi  ciencies, and more accurate reporting‚Äù 
(10 Credit Pros‚Ä¶ 2). It is important to understand how SOX‚Äôs best intentions can actually make 
businesses better. Th e beginning of making this a reality comes from the organizational culture, 
and ‚Äúorganizations should not regard voluntary compliance with the Sarbanes‚ÄìOxley Act as an end 
in itself; it should be part of a larger enterprise risk management initiative‚Äù (Giniat and Saporito 
2007). Authors Edward Giniat and Joseph Saporito examine the not-for-profi t healthcare industry 
as one that could benefi t from a voluntary SOX compliance initiative. Th ey argue that ‚Äúmany of 
the unfortunate surprises that have hurt healthcare organizations‚Äô reputations recently could have 
been avoided‚Äîor at least anticipated‚Äîby more eff ective risk management and more transparent 
reporting‚Äù (Giniat and Saporito 2007). SOX certainly can provide the catalyst toward a greater 
organizational awareness into information security and security controls. For some industries, 
they may not have explored security to a very deep degree at all. SOX can be an important fi rst step 
toward a more secure organization, and public demand has begun to support the push for tighter 
accountability. As noted in a magazine catering to accountants, one author writes that
Non-SEC-registered entities, including governments and not-for-profi t organiza-
tions, face pressures similar to those present in for-profi t corporations to mismanage 
accounting, mislead their auditors, or infl uence auditor judgment with lucrative con-
sulting projects. Th ird parties for nonregistrants, such as banks, venture capitalists, 
hedge funds, and regulators, are just as vulnerable to fi nancial reporting abuses as are 
investors in publicly traded companies (Drexler 2006).
Th is public trend is the kind of nudge that companies often need to begin a process as challeng-
ing as SOX compliance. One elusive benefi t of SOX is the attention to fraud and audit concerns 
that have plagued many smaller businesses in the past but which never received suffi  cient national 
attention. At one end of the extreme, ‚Äúdiscoveries of fraud in Nassau County‚Äôs school system led 
New York State Comptroller Alan Hevesi to reinstitute the state‚Äôs school audit department and hire 
89 auditors‚Äù (Drexler 2006). We must caution, however, that SOX is not the end of the process, 
but rather the beginning and ‚ÄúSOX compliance by itself is not suffi  cient to account for risk where 
the root causes are in operations, compliance, and strategic activities‚Äù (Giniat and Saporito 2007). 
Rather than operate reactively and scrambling to implement specifi c SOX controls‚Äîas many 
companies have that we have explored thus far‚Äîorganizations would be well benefi ted ‚Äúto incor-
porate SOX compliance into a larger, integrated enterprise risk management (ERM) approach that 
tailors their SOX compliance to their specifi c needs while ensuring that other types of risk also 
receive critical attention. In this way, they can apply a coordinated approach to addressing many 
of the root causes of risk facing their organizations‚Äù (Giniat and Saporito 2007). Th is larger enter-
prise initiative can be the strategic driver or umbrella under which SOX tactical measures may 
operate. ERM may have been viewed as a ‚Äúnice to have‚Äù in the past, but one benefi t of SOX is that 
security best practices are much more widely known and implemented than they may have been 
in the past. In addition to the benefi ts of the controls themselves, ‚ÄúERM engages leadership‚Ä¶ by 
ensuring that they use a common language and methodology to identify, measure, prioritize and 
manage risk. It also creates a framework and process to improve the focus and effi  ciency of gov-
ernance and to link the risks the organization faces to its strategy and decision-making process‚Äù 
(Giniat and Saporito 2007). Th is strategic approach is also more cost-eff ective to companies as it 
exposes them to fundamental principles of information security such as confi dentiality, integrity, 
and information availability. As Giniat writes, ‚ÄúERM helps the organization‚Äôs executive leadership 

The Sarbanes‚ÄìOxley Revolution: Hero or Hindrance? ‚óæ 225
understand diverse risks and place those risks on a common platform that takes into account each 
risk‚Äôs likelihood of occurring and its potential magnitude of impact should it occur‚Äù (Giniat and 
Saporito 2007).
We have seen that SOX can conceivably bring a number of benefi ts to an organization, despite 
the perceived hassles. While many organizational leaders might admit that in the long run they 
agree SOX can help their organization, they resent the short-term costs and challenges. Th ese 
are reasonable concerns, particularly for small businesses. A study conducted about the banking 
industry can help to provide a representative example of the impacts of SOX compliance on the 
company bottom line. Th e banking industry is a particularly interesting example since it is quite 
familiar with regulations and compliance issues. ‚ÄúTh e Federal Deposit Insurance Corporation 
Improvement Act (FDICIA) of 1991 required signifi cant auditing, corporate reporting and gov-
ernance reforms for all banking institutions with more than $500 million in assets‚Äù (Borgia and 
Siegel 2008). FDICIA was perhaps the weightiest regulation for banks prior to SOX. Even though 
‚ÄúSOX was modeled after FDICIA‚Ä¶, the provisions of SOX go far beyond those of the model 
and require an audit of the internal controls of an SEC registrant throughout the year‚Äù (Borgia 
and Siegel 2008). We have already examined the circumstances surrounding SOX‚Äôs development, 
but the profi tability fi gures found in this bank study are important to be noted and they do drive 
forward the case for intelligent reform. Th e study compared public and private banks. Th e latter, 
being excluded from the SOX requirement, were hypothesized to be more profi table than public 
banks that had to expend capital on SOX controls and compliance.
Th e results of the study are fairly telling about how SOX aff ects profi tability of public 
bank holding companies. ROA increased only 4% for public companies from 2001 
to 2003, but it increased twice as much (8%) for private companies. ROE showed 
even greater diff erences for public and private entities. For public companies, ROE 
increased 3%, whereas private companies experienced an 8% increase. Clearly, SOX 
had a negative initial impact on the profi tability of public bank holding companies 
(Borgia and Siegel 2008).
While we do not have access to studies for every industry vertical, it is clear that SOX compliance 
weighs on the company budget. ‚ÄúTh e total average cost of Sarbanes‚ÄìOxley Section 404 compli-
ance reached $1.7 million last year [2007], according to a newly released survey by Financial 
Executives international. Th e poll of 183 companies found that total audit fees for U.S. acceler-
ated fi lers averaged $3.6 million, a slight increase of 1.8 percent from the previous year‚Äù (Costs‚Ä¶ 7). 
It is important to note that corporate expenses are not necessarily cause for public concern. 
Considering the reputations most corporations in America have today, many people would be 
inclined to shrug with disinterest when presented with these types of fi gures. Th e danger comes 
when, in the aggregate, companies and particularly small businesses begin to fi nd their budgets 
weighed down with these types of recurrent fees that prevent them from adequately taking care 
of their employees or participating and contributing to civic initiatives. Americans have a vested 
interest in the success of their economy, so streamlining an initiative like SOX has the potential 
to help both businesses and consumers.
Overall, SOX‚Äîlike any piece of regulatory legislation‚Äîis not perfect. It is also in its infancy, 
so we should not be terribly surprised that it still has fl aws that need to be corrected in order to 
provide a reasonable and appropriate regulatory environment that will achieve its primary goal of 
mandating accurate and reliable fi nancial reporting. A balance must always exist between regula-
tors and the regulated, and there are bound to be pain points throughout the process. Changes, 

226 ‚óæ Information Security Management Handbook
while slow, have begun to trickle down from the SEC as business pains and shortcomings of the 
existing SOX rules become clearer and more widely known. Yet businesses continue to paint a 
relatively grim picture of SOX. Even the most vigorous supporters of regulation* must admit SOX 
has not been the most eff ective or intelligent mandate to come out of Washington. Corporations 
must strike a balance and foster a culture that embraces risk management initiatives rather than 
treating them as a chore that must be completed, as many companies still do. Where items need 
to be reformed, they ought to be and they should be reformed rapidly so as to ease the potential 
burden to companies who, like the American people, are struggling. To some, SOX reform can-
not come quickly enough and it is viewed as an imperative to the prosperity of the United States 
enterprise. ‚ÄúIf America‚Äôs global competitiveness and position as the world‚Äôs preeminent capital 
market is to be maintained, the time to make sweeping changes to the current SOX regime is now. 
If U.S.-listed companies want change, the time to lobby for real change is now‚Äù (Sharman 2007). 
With the dawn of a new era of politics and American civics, we may just fi nd that capitalizing on 
the benefi ts of SOX and making it into a positive economic driver for businesses is truly within 
our reach.
About the Author
Seth Kinnett is a senior analyst for Goldman Sachs Asset Management, Chicago, Illinois, where 
he specializes in business analysis, process improvement, and data mining.
References
Borgia, C. and Siegel, P. H. How the Sarbanes‚ÄìOxley Act is aff ecting profi tability in the banking industry. 
Th e CPA Journal, pp. 13‚Äì14, August 2008.
Braganza, A. and Franken, A. SOX, compliance, and power relationships. Communications of the ACM, 50, 
97‚Äì102, September 2007.
Costs analysis of 404. Practical Accountant, p. 7, June 2008.
Drexler, P. M. Could Sarbanes‚ÄìOxley benefi t non-SEC-registrant audits? Th e CPA Journal, pp. 6‚Äì9, June 
2006.
Filisko, G. M. SOX auditing rules eased. National Real Estate Investor, p. 118, July 2007.
Giniat, E. and Saporito, J. Sarbanes‚ÄìOxley: Impetus for enterprise risk management. Healthcare Financial 
Management, pp. 65‚Äì70, August 2007.
How 10 credit pros meet the challenge of SOX compliance. Managing Credit, Receivables & Collections 
newsletter, p. 2, December 2007.
Kumar, S. Finding a framework for sustainable SOX compliance. Pulp & Paper, September 2006.
Magliozzi, R. Th e dawn of Sarbanes‚ÄìOxley effi  ciency and eff ectiveness: A fi nancial institution perspective. 
Bank Accounting & Finance, pp. 43‚Äì46, December 2007‚ÄìJanuary 2008.
Needleman, T. SOX compliance software‚ÄîLots to choose from. Accounting Today, September 2006.
Needleman, T. SOX compliance: More than a single approach. Accounting Today, pp. 22‚Äì27, September 
8‚Äì21, 2008
SEC supports streamlined SOX compliance. Practical Accountant, www.webCPA.com.
Sharman, P. A. Th e winding road of SOX compliance. Strategic Finance, pp. 8‚Äì10, January 2007.
* Such as this essay‚Äôs author.

DOMAIN 
4
APPLICATION SECURITY
System Development 
Controls


229
16
Chapter 
Data Loss Prevention Program
Powell Hamilton
One of the major challenges for today‚Äôs IT security professional is having a strong awareness of 
the type of data and value of the data they are protecting. No matter how robust the technology, 
or how vigorous the monitoring systems, intellectual property (IP) can fi nd a way to leak onto less 
secure systems and devices for which they are not intended.
Traditional information technology (IT) strategies have focused on the external threats that 
emphasize broader protection methods and often overlook internal threats and vulnerabilities. 
Th ese strategies assume a malicious person is willing and motivated to break through the network 
Contents
What Is Data Leakage ............................................................................................................. 230
Common Sources of Data Leakage .......................................................................................... 230
What Is Data Loss Prevention ..............................................................................................231
Components of a Data Loss Prevention Program .................................................................231
DLP Governance ........................................................................................................... 232
Risk Assessment ............................................................................................................. 232
Regulatory and Privacy Requirements .............................................................................233
Data Classifi cation ......................................................................................................... 234
Business Segment Detail Data Analysis .......................................................................... 236
Policies, Standards, Procedures ....................................................................................... 238
Data Discovery............................................................................................................... 238
Remediation Processes .................................................................................................... 239
Training and Awareness ...................................................................................................241
Emergence of Departments ................................................................................................. 242
Legal Challenges ...................................................................................................................... 243
Federal and State Laws ........................................................................................................ 243
DLP Resource Challenges ................................................................................................... 244
Benefi ts of a Data Loss Prevention Program ........................................................................ 244
Conclusion .............................................................................................................................. 244

230 ‚óæ Information Security Management Handbook
barriers and penetrate critical systems. Th is ‚ÄúBarbarians at the Gate‚Äù mentality loses the focus that 
the main asset to protect is data.
Protecting the data, rather than protecting the system, is the risk-based approach that guides 
security practitioners to focus on security controls on systems processing, transferring, and storing 
high-value or sensitive data. Th e problem is most IT professionals have little knowledge of what 
type of data their systems are processing and what value is placed on the data. Th erefore, the IT 
professional focuses on protecting computer systems, rather than data protection.
To compound the problem, business owners (i.e., data owners or data custodians) often do not 
have a strong understanding of the value of their own data. Th erefore, little emphasis is placed on 
security controls and safeguards.
Th e old phrase used in criminal cases, ‚ÄúFollow the Money,‚Äù can be slightly altered in the busi-
ness environment to ‚ÄúFollow the Data.‚Äù If an organization follows the data, they can focus their 
security controls on these systems.
What Is Data Leakage?
What is meant by data leakage is easy to understand. Data leakage is the separation of IP from its 
intended place of storage. Most of the time, the IP is stored in a location with less security controls, 
than where it was intended. Data leakage can occur in many ways. Th e most common method is 
for an employee to violate corporate policy and copy the IP to a less secure system or their personal 
computer or removable device. Other methods include human error, technology mishaps, system 
misconfi guration, sabotage from a disgruntled employee, or possibly, a system breach from a hacker.
Common Sources of Data Leakage
Some leakage may be intentional and others may be from human error or a system misconfi gura-
tion. Th e following outlines some common examples of data leakage:
An employee needs to create a report. Th e employee extracts the data from a secure system and 
 
‚óæ
conducts the analysis on a less secure system, such as desktop or their notebook device. After 
the analysis has been complete, the employee does not properly dispose the information.
A new or upgraded application is being implemented on a test system. Personal identifi able 
 
‚óæ
information (PII) is used to ensure the system is working properly. After the tests are com-
pleted, the PII is not removed or disposed of.
Processes for conducting secure backups are not established. Th e backup tapes are stored in 
 
‚óæ
a nonsecure environment and a curious intruder removes the tape to examine the content.
Outdated hardware is donated to a charitable organization. Before the system is delivered, 
 
‚óæ
the hard drive is not properly cleaned and sensitive information is not removed.
A home-grown application is developed to interface to the public. Th e application developer 
 
‚óæ
lacks secure coding experience and writes the program with leakage errors. Th ese types of 
applications can provide a malicious hacker with unauthorized access to sensitive data.
In some cases, the improper confi guration settings or inadequate security controls for shared 
 
‚óæ
drives have been implemented. Th e permissions of the fi le and directory structure allow 
anyone to access the information. Th e organization has a loosely guarded policy toward its 
propriety information, whereby data becomes easily accessible to everyone.
A disgruntled employee with privileged access to sensitive data may act maliciously and steal infor-
 ‚óæ
mation. Th e information is copied to a nonsecure system or device (such as a memory stick).

Data Loss Prevention Program ‚óæ 231
Although it is common for data leakage incidents to occur internally, it is also common for data to 
leak due to hackers, social engineering, phishing attacks, and even dumpster diving. Th ese attacks 
can spread sensitive information from system to system.
To cope with data leakage, many organizations have implemented several security solutions. 
Solutions can range from establishing policies and procedures, increasing education and aware-
ness, to building a full Data Loss Prevention (DLP) program.
What Is Data Loss Prevention?
DLP, also known as Data Loss Protection, is the process and methodology to detect and prevent the 
unauthorized transmission or disclosure of sensitive information. DLP depends on a combination 
of people, processes, and technology as its strategic control foundation. Th ese control elements work 
together to help ensure data is utilized in its intended manner.
DLP is one of the least understood technologies. Before businesses think to implement 
a sound DLP program, they must have a solid discovery tool to drive the eff ort. Although a 
discovery tool is important, there are several components needed to build a successful DLP 
program.
Components of a Data Loss Prevention Program
Th e potential legal liability and damage to brand reputation from exposure of sensitive data has 
encouraged security leaders to implement a DLP program. A DLP program can be constructed in 
many ways. Th e following diagram illustrates a unique model.
Policies,
standards,
and procedures
Data
classification
Regulatory
and privacy
compliance
Risk
assessment
Data
governance
Training
and
awareness
Remediation
processes
Data
discovery
DLP
program
Most DLP solutions rely on technology. Although technology is an important aspect, it takes 
manpower resources (or personnel resources) and processes to build a holistic DLP program.

232 ‚óæ Information Security Management Handbook
DLP Governance
A fi rst step in a DLP program is to establish Governance. DLP Governance encompasses the 
overall management of the DLP program. From a security standpoint, DLP governance is the act 
of protecting data and monitoring the fl ow of where the data travels. Although this sounds easy to 
accomplish, this can be a challenging task within a large organization.
A governance program includes a governing body or a committee to defi ne policies and pro-
cedures and a plan to implement those procedures. In a large organization, this group needs to 
consist of individuals who have a strong understanding of the organization‚Äôs industry, business 
objectives, internal processes, and the corporate culture.
Th is group is not responsible for managing data directly, but is responsible for creating the 
rules (policies) and the methods (procedures) for storing, accessing, and handling data. Along 
with creating policies and procedures, the group needs to defi ne the responsibilities of the owners 
and/or custodians of the data and outline the accountability for the data, specifi cally how the data 
is processed, stored, archived, and transmitted internally and externally.
Th e makeup of this group should include multiple departments (i.e., HR, Legal, Compliance, 
Information Systems, and IT Security). Th e relationship, roles, and responsibilities of the group 
are discussed within the ‚ÄúEmergence of departments‚Äù section of this chapter. Th is group will need 
to establish a working structure to help resolve complex issues. Th e mishandling of data can cause 
apprehension and panic, even if it was conducted inadvertently.
Th e structure of a DLP governance initiative will vary not only with the size of the organization, 
but with the desired objectives it hopes to accomplish. For example, an organization may want to 
only identify and track the movement of sensitive data. Once an alert has been delivered, the off end-
er‚Äôs management will be notifi ed to address the issues. In a stricter environment, the sensitive data 
may be blocked and quarantined and the off ender may be disciplined with stringent punishment.
Overall, the governance of a DLP program is an important fi rst step for establishing a DLP 
program. Similar to many unsuccessful intrusion detection system (IDS) implementations, 
a DLP system without a structure and governance will more than likely fail.
Risk Assessment
Conducting a risk assessment is a good second step in any DLP program. Th e main purpose for 
a risk assessment is to identify all types of data within your network and to identify threat and 
vulnerabilities related to this data. Non-Public Data (fi nancial, business, HR, legal, and regulatory 
data), PII (social security numbers, credit card information, personal health data), and IP (patents, 
trademarks, design plans) are examples of data that need to be identifi ed. Once this information 
has been identifi ed, a fl ow analysis needs to be conducted to identify all systems and devices the 
data either resides on or fl ows through.
For example, the HR department may utilize employee information. Th is information is 
stored on a centralized server utilizing a second server with a proprietary database. To access this 
information, the HR employee connects their intranet Web browser to the server (i.e., three-tier 
architecture). In this simple scenario, the devices transferring and storing data are the employee‚Äôs 
desktop workstation, network components connecting to the server, the server itself, and a sepa-
rate server maintaining the proprietary database. Each of these systems needs to be evaluated to 
determine threats and vulnerabilities that may put the data at risk.
Th is exercise needs to be conducted for all types of data being utilized within the organization. 
A comprehensive DLP solution ultimately has to protect all potential risk points in your organization.

Data Loss Prevention Program ‚óæ 233
Th ere are several approaches and methods available for conducting an accurate risk assessment. 
Th e following Internet sites provide excellent information to accomplish this goal:
Control Objectives for Information Technology (COBIT): Developed by IT auditors and made available 
through the Information Systems Audit and Control Association (ISACA). COBIT provides a frame-
work for assessing a security program, developing a performance baseline, and measuring performance 
over time. Additional information can be obtained on the Internet at www.isaca.org/cobit.htm.
NIST Documents on Risk Assessment: Th e U.S. National Institute of Standards and Technology has 
published several documents that outline frameworks for conducting technology risk assessment and 
evaluating information security. Additional information can be obtained on the Internet at csrc.nist.gov.
Operationally Critical Th reat, Asset, and Vulnerability Evaluation (OCTAVE): Developed by the 
Computer Emergency Response Team at Carnegie Mellon University. OCTAVE provides mea-
sures based on accepted best practices for evaluating security programs. Additional information 
can be obtained on the Internet at www.cert.org/octave.
Common Criteria (International Organization for Standardization (ISO) 27001): Th e common 
criteria represent an international standard for testing the eff ectiveness (functionality and assur-
ance) of most security products/systems. Additional information can be obtained (purchased) 
on the Internet at www.iso.org.
SysTrust: Developed by the American Institute of Certifi ed Public Accountants and the Canadian 
Institute of Chartered Public Accountants. SysTrust provides a framework for evaluating controls 
for information systems assurance. Additional information can be obtained on the Internet at 
www.aicpa.org/assurance/systrust/index.htm.
Regulatory and Privacy Requirements
Identifying regulatory and privacy requirements is essential for an organization to ensure that the 
organization is compliant and privacy goals and confi dentiality policies are supported by its prac-
tices, thereby protecting confi dential information from abuse and the organization from liability 
and public relations problems.
In the past, regulatory requirements were usually issued by the federal or state government. 
Today, regulatory requirements are not only mandated by public bodies, but now private organi-
zations are driving forced mandates as well (i.e., payment card industry‚ÄîPCI). Just about every 
industry sector has some type of regulatory and/or privacy requirements. A few of the more com-
mon requirements are as follows:
21 CFR Part 11 (FDA)
 
‚óæ
FCRA (Fair Credit Reporting Act)
 
‚óæ
Australian Privacy Legislation
 
‚óæ
Federal Energy Regualtory Commission (FERC)
 
‚óæ
BBB Online (privacy 
 
‚óæ
standards)
Federal Law for Data Protection (BDSG)
 
‚óæ
California AB 1950
 
‚óæ
FERPA (Family Educational Rights and Privacy Act)
 
‚óæ
California SB 1386
 
‚óæ
FIPPA (Freedom of Information and Protection of 
 
‚óæ
Privacy Act)
Canadian Privacy Act
 
‚óæ
Gramm‚ÄìLeach‚ÄìBliley Act (GLBA)
 
‚óæ
Children‚Äôs Online Privacy 
 
‚óæ
Protection Act (COPPA)
Health Insurance Portability and Accountability Act 
 
‚óæ
(HIPAA)
Data Protection Act 1998 (UK)
 
‚óæ
Other State and Local Privacy Regulations
 ‚óæ

234 ‚óæ Information Security Management Handbook
Electronic Communications 
 
‚óæ
Privacy Act of 1986 (ECPA)
Payment Card Industry Data Security Standards 
 
‚óæ
(PCI DSS)
European Union (EU) Data 
 ‚óæ
Privacy Directive
Sarbanes‚ÄìOxley Act
 
‚óæ
FACT
 ‚óæ
 Act (Fair and Accurate 
Credit Transactions)
Having a strong understanding of what regulatory requirements apply to your organization and 
what types of security controls are required, need to be identifi ed. Most organizations do not have 
a strong understanding of their requirements, or their interpretations of those requirements are 
diff erent from the regulators. Th us, most organizations are operating in a noncompliance mode.
A successful DLP program needs to take initial steps to identify and understand their regula-
tory requirements. Th e following table is an example chart for collection and tracking regulatory 
requirements and privacy issues.
Regulatory 
and Privacy 
Requirements
HIPAA
SB1386/AB700
PCI
EU Data Privacy 
Directive
Sarbanes‚Äì
Oxley Act
GLBA
Departments
Human 
resource
√ó
√ó
√ó
√ó
√ó
Legal
√ó
√ó
√ó
√ó
√ó
√ó
Compliance
√ó
√ó
√ó
√ó
√ó
√ó
Marketing and 
sales
√ó
√ó
Information 
systems
√ó
√ó
√ó
√ó
√ó
√ó
Manufacturing
√ó
Information 
systems
√ó
√ó
√ó
√ó
√ó
√ó
Once all the regulatory and privacy requirements of a company have been identifi ed, the organiza-
tion now has a stronger understanding of the type and the amount of information it must protect.
Data Classifi cation
Data classifi cation is the process of classifying information data according to its value and sensitiv-
ity to the organization. Data classifi cation provides the proper prioritization of an organization‚Äôs 
assets and resources, which will result in the appropriate level of controls to be applied to each 
system, accordingly.

Data Loss Prevention Program ‚óæ 235
Data should be categorized in terms of sensitivity within an organization‚Äôs environment 
(i.e., public, confi dential, secret, and private, etc.). Business requirements should drive the 
classifi cation process and should be directly related to data classes.
Once data requirements have been established and IP has been identifi ed, classifi cation catego-
ries can be assigned. A typical data classifi cation program should conduct the following:
Develop a standard or policy for data classifi cation.
 
‚óæ
Identify data type by departments.
 
‚óæ
Identify administrator/custodian/users for each data type.
 
‚óæ
Identify systems maintaining, processing, or storing each data type.
 
‚óæ
Specify the criteria of how the data will be classifi ed and labeled.
 
‚óæ
Create an enterprise awareness program.
 
‚óæ
Th e data classifi cation program will add additional controls to limit the access and movement of 
sensitive data, reducing the amount of data that is leaked within the organization.
Risk Rating Factors
Th e following risk rating factors can be used to measure the level of risks for data:
Data element
Data elements are the information an organization desires to protect. They 
can be fi nancial records, marketing information, human resource records, etc.
Data value
This risk factor is based on the value of the data. It considers the fi nancial 
impact if the data that was exposed is altered or lost.
Threat agents
Threat agents consider the human-caused or technology-based events 
such as the accidental deletion of records or denial of service attacks by 
cybercriminals.
Confi dentiality 
risk
The confi dentiality risk element is the risk of exposure. This risk rating 
considers the harm to the organization if the information were to be disclosed. 
Categories of ‚ÄúInternal Use Only,‚Äù ‚ÄúConfi dential,‚Äù and ‚ÄúMost Confi dential‚Äù 
have been assigned to each data item. These categories match the existing data 
classifi cation descriptions in place and are based solely upon input from the 
business areas. This is the sensitivity element.
Integrity risk
The integrity risk element rates the risk of unauthorized modifi cation or 
deletion of information.
Availability 
risk
The availability risk element considers the loss of productivity due to 
systems or data not being available. This is the criticality element.
Data 
classifi cation
The Data Classifi cation element is an overall rating of the potential 
exposure. This rating summarizes the other elements so a comparison and 
prioritization can be performed.
Risk Rating Scale
Th e following rating chart describes the criteria used to rate the value of data and the risk rating 
within the environment.

236 ‚óæ Information Security Management Handbook
Rating
Defi nition
Low
Low: Th e likelihood that the identifi ed risk will have 
a signifi cant impact on the business is remote
L
M
Low/medium: Th e likelihood that the identifi ed risk 
will have a signifi cant impact on the business is a low 
possibility
Medium
Medium: Th e likelihood that the identifi ed risk will 
have a signifi cant impact on the business is possible
M
H
Medium/high: Th e likelihood that the identifi ed risk 
will have a signifi cant impact on the business is 
probable
High
High: Th e likelihood that the identifi ed risk will have 
a signifi cant impact on the business is a high 
possibility
Data Classifi cation Level
Th e following is an example of Data Classifi cation schema:
Private: Information that is highly sensitive and is an internal document. For example, pending 
mergers or acquisitions, investment strategies, or research information that could seriously dam-
age the organization if such information were lost or made public. Information classifi ed as 
Private has very restricted distribution and must be protected at all times. Security at this level is 
the highest possible.
Confi dential: Information that, if made public or even shared around the organization, could seriously 
impede the organization‚Äôs operations and is considered critical to its ongoing operations. Examples 
would include accounting information, business plans, sensitive customer information, medical 
studies, and similar highly sensitive data. Such information should not be copied or removed from the 
organization‚Äôs operational control without specifi c authority. Security at this level should be very high.
Internal: Information not approved for general circulation outside the organization where its loss 
would inconvenience the organization or management but where disclosure is unlikely to result in 
fi nancial loss or serious damage to credibility. Examples would include internal memos, minutes 
of meetings, and internal project reports. Security at this level is controlled but normal.
Public: Information in the public domain; annual reports, press statements, etc., which has been 
approved for public use. Security at this level is minimal.
Business Segment Detail Data Analysis
Th e following is an example of a chart listing detailed data analysis of the type of data and relevant 
risk ratings.

Data Loss Prevention Program ‚óæ 237
Data 
Element
Depart.
Owner
Application
Storage 
Type
Server
Data Value
Number 
of Th reat 
Agent
Confi d. Risk
Integrity 
Risk
Availability 
Risk
Data Class.
HR data
HR
Fred 
Th omas
People soft
DB
Server1
High
L
M
High
M
H
Medium
Private
Network 
share
All users
Mark 
Swanson
Network 
share (Prod)
Network 
share
Server2
Medium
M
H
L
M
L
M
L
M
Confi dential
Legal 
transcripts
Legal
Brad 
Wong
Network 
share (legal)
Network 
share
Legal 
server3
High
M
H
High
Low
Medium
Private

238 ‚óæ Information Security Management Handbook
Policies, Standards, Procedures
Sound policies, standards, and procedures are fundamentals for an eff ective DLP strategy. Th ey 
ensure that the organization‚Äôs data is protected at a level appropriate to its value. It is critical not only 
to create sound policies, standards, and procedures, but also to ensure that they are updated on a 
regular basis.
Within the realm of DLP, policies are the starting point before a company can establish stan-
dards and procedures, which allow an organization‚Äôs DLP solution to operate more securely and 
effi  ciently. Standards are mandatory activities, actions, rules, and regulations designed to provide 
the DLP policies with the support, structure, and specifi c direction required to be meaningful and 
eff ective. Procedures spell out the specifi cs of how the DLP policies and the supportive standards 
will actually be implemented in an operating environment.
Your DLP policies should be based on your compliance and privacy needs. Some examples 
would include
Acceptable Use Policy
 
‚óæ
Information Security Roles and 
 
‚óæ
Responsibilities
Computer, Telephone and Network 
 
‚óæ
Usage Policy
Logon and Authentication Policy
 
‚óæ
Contractors and Th ird Parties Policy
 
‚óæ
Personnel Security Policy
 
‚óæ
Exceptions to Policy
 
‚óæ
Physical Security Policy
 
‚óæ
Change and Problem Management 
 
‚óæ
Policy
Remote Access Policy
 
‚óæ
External Network Connections Policy
 
‚óæ
Risk Management and Information 
 
‚óæ
Classifi cation Policy
Glossary of Terms
 
‚óæ
Security Awareness Policy
 
‚óæ
Hard Copy Information Policy
 
‚óæ
Software Compliance Policy
 
‚óæ
Incident Reporting Policy
 
‚óæ
System Security Audit and Review Policy
 
‚óæ
Information Contingency Policy
 
‚óæ
Once your policies have been created, policies search rule sets will be created to support the data 
discovery phase.
Policy creation is not a one-time operation. DLP policies should include technological require-
ments, as well as business requirements. For a discovery tool to be eff ective, IT and business units 
must collaborate on developing policies that protect the company‚Äôs assets, but are also fl exible 
enough to allow employees to be successful with their job requirements.
Once policies are created and rule sets are established, the IT team should expect to engage 
the business units to help update the policies on a regular basis, using feedback from the user com-
munity. One method to obtain user feedback is to collect explanations of why a policy-breaking 
action should be allowed to occur. Th is feedback will help strengthen policies and will provide an 
overall view of the business functions needed within the environment.
Data Discovery
Data discovery (also known as e-discovery) refers to any process in which electronic data is 
assessed. In the process of data discovery, data of all types can be searched. Th is can include 

Data Loss Prevention Program ‚óæ 239
electronic mail (e-mail), text, images fi les, databases, spreadsheets, Web sites, and computer pro-
grams. Digital data can be electronically searched with ease, whereas paper documents must be 
scrutinized manually. To help support a data discovery eff ort, most technicians utilize a data 
discovery product.
Data discovery products are tools that help organizations identify sensitive information by 
matching predefi ned patterns or algorithms to fl ag matches to those defi nitions. Th ere are many 
diff erent types of products architectures. Some are software-only, some are appliance based, some 
require user credentials, and some require agents to be installed on target systems. Regardless of 
their form, most of these products are based on either scanning ‚Äúdata-in-motion‚Äù on the network 
or scanning ‚Äúdata-at-rest‚Äù (i.e., data fi les or databases) on servers, workstations, desktop, and even 
laptop systems.
Once data discovery has identifi ed a policy violation, the system can be confi gured to handle 
the event in a variety of ways. In the case of ‚Äúdata-in-motion,‚Äù the administrator can be alerted and 
the data can be blocked or quarantined for further investigation. In the case of ‚Äúdata-at-rest,‚Äù the 
administrator can be alerted and a copy of the data can be obtained. In either case, sensitive data 
can be captured and addressed according to the organization‚Äôs internal procedures.
Regardless of the amount of security controls implemented, the chance of IP leaking out is 
highly likely. Th is is why a data discovery assessment should be conducted on a periodic basis. 
Data discovery is one of the key elements of a DLP program. Access to a strong discovery tool and 
knowledgeable staff  can assist most organizations in implementing a solid DLP program.
Remediation Processes
Th ere are several process steps within the remediation phase. Th ese steps are visually displayed below.
Identification
Classification
Notification
Response
Identifi cation
Before a policy violation can be remediated, it fi rst needs to be identifi ed. A major challenge with 
DLP is determining which data is real-leaked data and which data is a false- positive detection. 
In today‚Äôs business environment, the amount of data traveling through the network and stored 
on disk drives is almost overwhelming. Nevertheless, the challenge needs to be managed and pro-
cesses need to be in place beforehand. Should a violation be discovered, proper methodology can 
be implemented and the cause of the breach accurately determined.

240 ‚óæ Information Security Management Handbook
Should a violation be discovered, enough information needs to be collected to determine the 
next steps.
Classifi cation
Once the data has been identifi ed, it needs to be classifi ed as far as the type of data and its sever-
ity level. Th e type of data will support notifi cation steps (i.e., who needs to be notifi ed) and the 
severity will help determine the urgency for addressing the issue. Th e classifi cation schema should 
leverage the data classifi cation model discussed earlier in this chapter. An example classifi cation 
chart would be as follows:
Departments
Urgency
Private
Confi dential
Internal
Public
Human resource
√ó
Legal
√ó
Compliance
√ó
Marketing and sales
√ó
Information systems
Manufacturing
√ó
Information systems
√ó
where
Severity
Action
Private
Immediate contact needs to occur. Data should be blocked or quarantined.
Confi dential
Contact needs to occur within a timely manner. Data should be blocked, 
quarantined, or at least a copy of the data taken.
Internal
Urgency is low; the contacts should receive a voice mail or e-mail notifi cation. 
A copy of the data should be taken for future reference.
Public
Public data is not an issue. No action required.
Notifi cation
Based on the type of data and its severity, determined in the previous step, the notifi cation step 
will identify the proper owner of data and the urgency to take action. Th is information should be 
predetermined and escalation procedures should be developed to hand off  the issue to the data‚Äôs 
owners. Notifi cation procedures should, at a minimum, contain the following:
Department
 
‚óæ
Contact
 
‚óæ
DLP role
 
‚óæ
Offi  ce/cell number
 
‚óæ

Data Loss Prevention Program ‚óæ 241
Response
Once a violation has been identifi ed, classifi ed, and the correct people have been notifi ed, the vio-
lation must be responded based on the classifi cation and severity level. Part of the response would 
include an assessment and an investigation.
Assessment begins as soon as a violation is identifi ed. Th e relevant DLP team members need 
to collect all relevant data and work with business owners to determine the business impact on the 
organization. Th e DLP team members and the data owners need to coordinate a plan to effi  ciently 
contain the distribution of the sensitive data and address the off enders to ensure the violation does 
not occur again.
At a minimum, the DLP team and data owners should
Identify all types of data the off ender has access to
 
‚óæ
Start logging and recording the off ender‚Äôs activity
 
‚óæ
Suspend access to additional sensitive data
 
‚óæ
Investigation is undertaken to determine the method and extent of the incident. Th e method is 
the technical vehicle allowing data to be transferred. Th e extent includes the boundary and the 
magnitude of the circulation of the data.
During the investigation process, ensure the following information is captured:
Th e data and time of the violation
 
‚óæ
Th e source of the data
 
‚óæ
Th e destination of the data
 
‚óæ
Data violation
 
‚óæ
Regulatory Compliance‚ÄîOne of the outcomes of the assessment and investigation phases is 
to determine if any steps need to be taken to meet regulatory requirements. (i.e., ‚ÄúCalifornia 
SB1386‚ÄîProtection of Personal Data‚Äù).
Th e DLP Leaders will work with Compliance and Legal Counsel to determine if compliance 
violations have occurred. If violations have occurred, the DLP Leaders should call an incident 
meeting with the DLP team, Compliance, Legal Counsel, and Executive Management. Th is meeting 
should determine the following:
Determine what data was aff ected
 
‚óæ
Determine if outside help is required
 
‚óæ
Determine if law enforcement should be notifi ed
 
‚óæ
Determine the best notifi cation method to use
 
‚óæ
Training and Awareness
It is important for an eff ective DLP solution to interact with the organization‚Äôs employees so 
that they have a strong understanding of why certain activities are inappropriate and could 
be harmful for the organization. Not all violations are conducted with a harmful intent. An 
employee may want to work at home and e-mail sensitive data to their personal, less secure 
public account. Although the intent is good, the action is not. Ongoing education will help 
reinforce correct behavior and provide the employee with guidance on how to correctly handle 
sensitive data.

242 ‚óæ Information Security Management Handbook
When organizations educate and highlight the dangers of data loss, violations are reduced 
dramatically. Over time, as the employees become more familiar with corporate policy, the overall 
security awareness practices increase throughout the organization.
Emergence of Departments
With the technology advancements of DLP tools, the emergence of multiple departments becomes 
necessary to ensure data is handled in an appropriate manner. Functional relationships need to be 
defi ned so sensitive information is not exposed to the wrong person. Th e following model displays 
an example of a DLP department relationship structure.
Human
resource
department
Legal
department
Compliance
department
Information
systems
department
Information
security
department
DLP
program
Although it depends on the organization‚Äôs structure, the above model displays a common relation-
ship (i.e., interest) structure, with each department responsible for a particular role and responsibility.
Legal Department: Th e Legal department must be involved in any DLP program. Th e legal depart-
ment will have the role and responsibility for creating and/or approving policies and procedures. Th e 
fi rst set of policies the legal department must create and/or approve is who has access to certain types 
of discovered information. For example, the legal department needs to create a policy ensuring that IT 
personal are not reviewing HR data. Th e legal department will also need to create a policy and a process 
for policy violations. With today‚Äôs technology, it is easy to identify policy violations. Th e diffi  cult part is 
acting on the policy and determining the level of damage to the organization. Overall, the legal depart-
ment needs to provide legal oversight to ensure the DLP program is not being misused.
Human Resources Department: Th e Human Resource (HR) department‚Äôs involvement would be that 
of a reviewer of data and enforcement. As a reviewer, HR policies violations need to be fl agged and 
assessed by an HR representative. Sensitive information such as salaries, social security data, and other 
PII needs to be restricted and protected. As an enforcer, the HR department would be required to 
handle any personnel issues, which may lead to a reprimand or possible termination of employment.
Compliance Department: Th e Compliance department will have the role and responsibility to 
ensure the handling of data does not violate any compliance requirements. For example, the trans-
fer of HIPAA or PCI data from its secure location to a less secure, unauthorized location could 
violate a compliance requirement. An employee may not be aware that their actions violate com-
pliance; therefore, an intervention may help protect the company and educate the employee.
Information Systems Department: Th e Information Systems (IS) department‚Äôs role and responsibil-
ity is to install and maintain tools that are used to identify violations of DLP policies. In the case 

Data Loss Prevention Program ‚óæ 243
of a network solution, the IS department would provide logical access to the network and would 
provide physical accommodations for the DLP hardware.
Security Department: Th e Security department is the owner of the DLP program. As the owner, it 
would also be the functional custodian of the tools used to monitor and/or discover policy viola-
tions. Although it would be the application owner, this does not mean it would receive and take 
action for all the alerts. As stated earlier, each designated group (i.e., Legal, HR, Compliance, etc.) 
would receive and manage their own issues.
Legal Challenges
Employee monitoring of their staff ‚Äôs activities has always occurred within the workplace environ-
ment. Early monitoring was as simple as walking the fl oor to ensure the employee was at their 
desk working away. Today, due to technological advancements, the workplace environment has 
widened its boundaries. Virtual environments have allowed employees to work in remote locations 
and, in some cases, work at their home. Although technology has provided the employee with the 
freedom to work remotely, technology has also provided the employer with tools to monitor their 
activities, without the employee‚Äôs knowledge.
Th e practice of monitoring employee‚Äôs activities without their knowledge has become a con-
troversial issue. Although most organizations have their employees sign an acceptable use policy, 
privacy expectations can be out-of-alignment between the employee and the employer.
Federal and State Laws
Th ere are several federal and state laws governing workplace privacy. Th e most popular federal 
law is the Electronic Communication Privacy Act of 1986 (ECPA). ECPA was enacted to extend 
federal wiretap laws to new forms of communication. Th e prior law, the Omnibus Crime Control 
and Safe Streets Act of 1968, protected only those communications that could be overheard by an 
individual. Th e statue did not address communication technologies such as e-mail, instant mes-
saging, fi le transmittals, faxes, pagers, and cellular or cordless telephones.
Th is later legislation disallows employers to monitor their employee‚Äôs communications. 
Although it is disallowed, the ECPA has several exceptions for monitoring electronic commu-
nications. Th e three most relevant to the workplace are (1) where one party (i.e., the employee) 
consents, (2) where the provider of the communication service can monitor the communications, 
and (3) where the monitoring is conducted in the ordinary course of business.
On October 26, 2001 new legislation was enacted, called the Uniting and Strengthening 
America by Providing Appropriate Tools Required to Intercept and Obstruct Terrorism (USA 
PATRIOT) Act. Th is new law amended a number of laws, including the ECPA. Th e changes 
made by the PATRIOT Act to ECPA addresses two situations, (1) the manner in which government 
authorities can compel disclosure, and (2) whether or not an organization can make a voluntary 
disclosure to government authorities.
Th e USA PATRIOT Act also amends ECPA by adding a new voluntary disclosure exception 
for emergency situations. Under this exception, if a service provider reasonably believes that an 
emergency exists involving immediate danger (i.e., death or serious physical injury), assistance 
from and disclosure of information to a law enforcement agency is justifi ed.
Every state has some type of privacy law as well. Th ese laws, also known as tort laws, are often 
viewed as the primary sources of protection for privacy of electronic communications. Th e most 
common tort that would apply is the tort of invasion of privacy.

244 ‚óæ Information Security Management Handbook
Th e biggest controversy with workplace privacy laws is the reasonable expectation of privacy. 
Most employees have a diff erent expectation of privacy than their employers. An organization 
must take signifi cant steps to ensure the expectation of privacy is understood.
Th e Legal department should be heavily involved in every DLP program. Th eir lack of involve-
ment could easily lead to an unsuccessful program if the structure, processes, and procedures were 
ill-defi ned and an employee‚Äôs expectations of privacy are not properly communicated. Even if an 
organization constructs a solid program on the front-end, back-end issues with handling a policy 
violation can be challenging, especially from a legal standpoint.
Disciplinary action taken against an employee violating policy can always be a challenge. Th e 
organization needs to ensure all policies are in order and that the employee is well informed about 
the policy and its consequences.
DLP Resource Challenges
Today, implementing a DLP program can be cost-prohibitive for most organizations. Th e cost of 
procuring a product and deploying dedicated security resources to manage and investigate alerts 
has been limited to large enterprises. Mid-tier organizations traditionally resort to improving their 
policies and procedures, increasing employee awareness, and in some cases, implementing some 
type of e-mail or fi le encryption. Th e simple fact is, most mid-tier organizations have not been able 
to aff ord the emerging technology and do not have the IT resources to manage a program.
With advancements in technology, an increasing number of DLP vendors, and increased 
awareness of the value of a DLP program, costs are reducing and next-generation solutions are 
becoming economically feasible.
Benefi ts of a Data Loss Prevention Program
Th e following illustrates some of the benefi ts for creating a DLP program:
Prevent data leakage
 
‚óæ
 
 Prevent accidental or malicious loss of data by insiders or hackers, even when data is disguised.
Reduce cost of investigation and reputation
 
‚óæ
 
 Reducing costs in investigating data loss can also reduce the cost of rebuilding damage to an 
organization‚Äôs reputation.
Facilitate early risk detection and mitigation
 
‚óæ
 
 Implementing a DLP program will help identify data leakage and will help ensure informa-
tion is in its proper place.
Increased comfort level with senior management
 
‚óæ
 
 Implementing DLP controls will help assure senior management that proper security safe-
guards have been implemented, allowing them to concentrate on other critical business issues.
Conclusion
A DLP program reduces the risk of exposure of sensitive information. Although some of the DLP pro-
cesses and procedures may appear to be redundant to existing security safeguards, creating a separate 
program would strengthen existing security controls and would initiate new methods for detecting data 
leakage. Implementation of a program will also provide management with greater control and under-
standing of what type of data is being transferred through the network. Organizations faced with com-
pliance requirements need to be proactive and initiate a program to address compliance obligations.

245
17
Chapter 
Data Reliability: Trusted 
Time Stamps
Jeff Stapleton
Th e technology shift from pen and ink documents to electronic bits has dramatically increased 
data fl ows and information quantity during the past 70 years, but at the same time has adversely 
aff ected its quality. Th e information age initially required data entry operators to translate pen and 
ink information into data bits, but today information sources are the data bits and conversely the 
paper is now printed afterward, relying on the data bits interpreted by software programs. Th is 
software abstraction layer introduces a data reliance problem that ultimately can only be addressed 
using cryptography to implement trusted time stamp technology.
Contents
Pen and Ink Reliability ............................................................................................................ 246
Evolution to Electronic Data ................................................................................................... 246
Electronic Data Unreliability ....................................................................................................247
Cryptography Schemes .............................................................................................................249
Data Encryption ..................................................................................................................249
Message Authentication Code ..............................................................................................250
Encrypted Hash ...................................................................................................................250
Keyed Hash .........................................................................................................................251
Digital Signature ..................................................................................................................252
Trusted Time Stamps ...........................................................................................................253
Standardization ........................................................................................................................256
Conclusion ...............................................................................................................................256
About the Author .....................................................................................................................257
References ................................................................................................................................257

246 ‚óæ Information Security Management Handbook
Pen and Ink Reliability
More than 5000 thousand years ago, the Sumerians are thought to have invented the written lan-
guage using styli and clay tablets. Th e Egyptians and Mayans often captured their hieroglyphics in 
stone, while the Chinese used turtle shells and ox bones. Regardless of the language, such physical 
media have a certain permanence about it. Even today, thousands of years later, we can still read 
these carvings. Th e Egyptians are also accredited with using papyrus and ink as early as 5000 years 
ago, which the ancient Romans adopted over 2500 years ago. Papyrus writing, clay tablets, wooden 
tablets, and waxed tablets may not have the same permanence as stone carvings; but nonetheless 
their physical characteristics provide a sense of authenticity and integrity, which has transferred to 
the modern-day paper and ink. Indeed, important documents requiring integrity were captured on 
paper using pen and ink, such as accounting ledgers and contracts, with handwritten signatures and 
personal seals (e.g., signet rings and hand stamp devices in wax or ink) providing authenticity.
Pen and ink documents have inherent characteristics; they are tangible and have tactile feed-
back. An individual can feel the paper, smell the ink, and read the document with an unaided eye. 
Paper documents are static and stable such that appropriate procedures can preserve their custody 
(e.g., chain of evidence). A document can be fi led away and locked up in a fi ling cabinet. Paper 
records are to some extent indelible and durable, such that alteration or unintentional destruction 
can be easily detected. Th e paper, the ink, and the manufacturing processes can be enhanced to 
such an extent that governments use it to print currency.
Th e originality of a pen and ink document is perceived to have worth, merit and reliance. 
Ancient scribes painstakingly used to copy documents by hand, sentence by sentence, word for 
word, letter by letter. Today, paper documents can be copied using modern imaging systems, they 
can then be mailed or faxed to multiple parities, and stored in fi ling cabinets for later reference. 
Th e authenticity of an original document is often determined by handwritten signatures that are 
distinguishable from copies.
Evolution to Electronic Data
Prior to computer availability, information was captured and processed by humans using pen and 
ink documents. Th e availability of electronic computing in the 1940s and the commercialization 
of mainframe computers in the 1950s ushered in the need to transform pen and ink informa-
tion into electronic data. Data entry operators read handwritten or typed documents as the data 
source and keyed information for computer processing. Th e 1960s introduced the minicomputer, 
which led to the transition of data capture from documents to data capture directly into computer 
systems. Th e microcomputer in the 1970s completed the data-capture transition, and inevitably 
transformed the paper document source from humans using pen and ink to printing documents 
from electronic bits. Th e data fl ow has literally reversed the direction ‚Äúpaper then computer‚Äù to 
‚Äúcomputer then paper,‚Äù and subtly transformed the reliance from paper to electronic data. It is this 
transformation that lost the inherent characteristics of pen and ink and introduced the unreliable 
nature of electronic data, adversely aff ecting paper documents.
Data bits can exist in volatile memory such that an unexpected loss of power will allow the 
data to fade away over a relatively short period of time. For longer periods data bits are stored 
on magnetic or physical media depending on the retention or the usage requirements. Magnetic 
media include computer hard drives, tapes, fl ash memory on smart cards, and even magnetic 
stripe on credit cards. Physical media include CD-ROM, DVD, and other optic-based tokens. 

Data Reliability: Trusted Time Stamps ‚óæ 247
Regardless of the medium, such data bits cannot be read directly; rather, humans rely on electron-
ics and software to read, interpret, and display the information. Further, data bits can be perfectly 
duplicated (each bit is either a ‚Äú0‚Äù or a ‚Äú1‚Äù) an infi nite number of times without the ability to 
discriminate between the original and a copy.
Th e article Spoliation of Digital Evidence [1] discusses the evolution of more accurate forensics 
tools to determine the provenance of evidence, as well as likelihood of spoliation. Court cases are now 
addressing digital evidence issues, such as computer equipment ownership, preserving unaltered 
electronic data, controls over digital data to prevent spoliation, and even meta-data, which is infor-
mation about data such as modifi cation dates and fi le sizes. As the courts become more familiar 
with digital evidence and forensics experts continue to testify as expert witnesses, organizations‚Äô 
need for proactive integrity controls (versus reactive processes) will continue to increase and even-
tually will become a core element of the enterprise security policy and practices.
Electronic Data Unreliability
Application software is designed to read, interpret, display, and even edit data fi les. Data fi les have 
a specifi c format and syntax such that suffi  cient modifi cations may corrupt the data to the extent 
that the designated application can no longer read the fi le. However, adversarial modifi cations 
may change the semantics of the fi le without aff ecting the format or syntax. Examples abound 
ranging from e-mails to contracts in which a dollar amount may be changed, a decision might be 
reversed, or a date could be manipulated. In many cases the modifi cation would go undetected, 
while with others the change triggered an unexpected event and scrutiny.
Th e Sarbanes‚ÄìOxley Act of 2002 (Public Law No. 107-204) commonly called ‚ÄúSOX‚Äù was 
passed in response to a number of major corporate and accounting irregularities. Essentially SOX 
requires that publicly traded companies evaluate and disclose the eff ectiveness of their internal 
controls as they relate to fi nancial reporting, and that appropriate controls over fi nancial data be 
implemented where such controls are found to be lacking. Th ere are several reported instances 
of time manipulation and data modifi cation, where fi nancial statements, audit reports, letters of 
credit were falsifi ed and backdated to provide the appearance of compliance. See the Information 
Assurance Consortium‚Äôs Wall of Shame [2] for details.
Th e article ‚ÄúLife after Sarbanes‚ÄìOxley‚Äù [3] discusses the ‚Äúdata-generation event‚Äù (DGE) separate 
from data ‚Äúviews‚Äù and the unreliability of paper printouts. Th e authors take the following positions:
First, it may be argued that the provision by the enterprise of views for audit, and the cor-
 
‚óæ
responding audit of views by auditors, constitute prima facie violations of Sarbanes-Oxley 
Section 302. Th is is because certifi cations are not being made on reliable business documents but 
on unreliable views.
Second, the government and the courts traditionally have recognized that internal control 
 
‚óæ
requirements apply to electronic information-security infrastructures. If material weaknesses (for 
example, the ready ability of an insider to reset a system clock and undetectably alter, delete, or 
substitute digital accounting records) could lead to a material misstatement (as it has in the cases 
previously cited), a Sarbanes‚ÄìOxley violation might ensue.
Backdating stock options is the granting of restricted employee stock options by a company at 
an exercise price equal to the value on the date that the grant is apparently made; however, the 
date chosen for the grant date is cherry-picked to select a date when the price of the stock was 

248 ‚óæ Information Security Management Handbook
advantageously lower after the fact. Th ese events incorporate time manipulation where the stock 
option letter is backdated such that the statement appears to be issued on the older grant date 
rather than the current date. Th is practice gained the attention of the Wall Street Journal (WSJ) 
and the Securities Exchange Commission (SEC). For more information refer to ‚ÄúTh e options 
backdating fi asco‚Äù [4] presentation at the RSA 2007 Conference. Not an illegal practice per se, 
although the legal profession is still debating this point, the impact to the company and stock-
holders of having to reissue earning statements to the SEC and the Internal Revenue Service (IRS) 
has been in the multimillions. See the Information Assurance Consortium‚Äôs Wall of Shame [2] for 
details.
Mutual funds‚Äô late trading incorporates data modifi cation where trades that have been held past 
the fi nal bell are submitted in a batch to the stock exchange. Th e practice of late trading is currently an 
acceptable SEC process but the consequences of altering the trades after the fact are well recognized by 
the securities industry. See the Information Assurance Consortium‚Äôs Wall of Shame [2] for details.
Th ese days software developers are notorious for bugs and vendors are well known for issuing 
endless patches. Information technology (IT) professionals are constantly on guard for current 
security patches, they rely on quality assurance (QA) practices to ensure that software is well 
behaved, and use antivirus software to prevent unauthorized software modifi cation. However 
IT operates under the presumption that the executable software has not been intentionally 
manipulated during the software development life cycle (SDLC). See the Information Assurance 
Consortium‚Äôs Wall of Shame [2] for details.
Sensitive data such as health-care information is susceptible to manipulation as demonstrated 
in the example x-rays shown in Figure 17.1. Th e newly created fi le might further be backdated and 
substituted for the original. Such time manipulation and data modifi cation could be used to grant 
insurance coverage on an otherwise declined policy, provide justifi cation for unwarranted medical 
treatment, or any number of other health-care-based fraud instances.
Further, electronic data can easily be copied and duplicated an infi nite number of times with 
no distinction between the original and a copy. Clearly when the data is software related this can 
be a manufacturing benefi t; however, replication might allow unauthorized distribution. Such 
(a)
(b)
Figure 17.1 Manipulating zeros and ones. The digitized full-body bone scan on the left (a) shows 
cancerous lesions on two ribs in the second and fourth images. The altered image on the right (b) 
shows the patient miraculously cured thanks to the advanced treatment called Adobe Photoshop¬Æ. 
(From B. Rothke, et al., LAW-403, RSA Conference, San Francisco, CA, 2007.)

Data Reliability: Trusted Time Stamps ‚óæ 249
may be the case of downloadable or physically distributed software. Further, when the data is 
business information or unauthorized duplication of software, replication might allow the loss 
of intellectual property. It is often desirable to identify the compromised point and authorized 
party for purpose of reparation. However, when data can be modifi ed or backdated by the fi rst party 
providing information or second party receiving or replicating information, no parties can rely on 
the data unless a third party can verify the information.
Ironically, additional controls are necessary in order for electronic data to reclaim reliance 
that has become lost with outmoded pen and ink documents. Data in storage can be protected by 
physical barriers and access-control measures in a controlled environment. However, access con-
trols can fail, and whenever the data is removed from its controlled environment (e.g., whenever it 
is viewed) the stored information requires cryptographic integrity solutions. In all circumstances, 
data in transit can only be protected using cryptographic solutions.
Cryptography Schemes
Th ere are several well-known and established uses of cryptography with its advantages and disad-
vantages. Each cryptography scheme is presented and its characteristics are described.
Data Encryption
Data encryption is the best understood use of cryptography. A modern encryption algorithm 
such as Triple DES or AES uses a symmetric key to encrypt cleartext and produce ciphertext, 
and the same key is used to decrypt ciphertext and recover the cleartext. Encryption provides 
data confi dentiality, which can be defi ned as the protection of information from unauthorized 
disclosure. Two-party transmission as shown in Figure 17.2‚Äîsymmetric encryption is where the 
sender (A) and receiver (B) have previously and securely exchanged a symmetric encryption key. 
Th e sender encrypts the cleartext, transmits the ciphertext to the receiver, and the receiver decrypts 
the ciphertext to recover the cleartext. Any third-party eavesdropping on the transmission can 
only access the ciphertext. However, note that encryption by itself cannot provide integrity as 
the eavesdropper can modify the transmission and the receiver might not detect the modifi cation 
depending on the format and syntax of the cleartext.
Ciphertext
Cleartext
Cleartext
Receiver
Ciphertext
Encrypt
Decrypt
B
Sender
A
Secret
Secret
Previously established symmetric keys
Figure 17.2 Symmetric encryption.

250 ‚óæ Information Security Management Handbook
Message Authentication Code
A message authentication code (MAC) is a symmetric scheme that uses encryption to provide 
rudimentary data integrity and limited authentication. Two-party transmission as shown in 
Figure 17.3‚Äîmessage authentication code is where the sender (A) and receiver (B) have securely 
exchanged a symmetric MAC key. Th e sender encrypts the cleartext, derives a MAC from the 
ciphertext, and transmits both the cleartext and the MAC to the receiver. Th e receiver encrypts 
the cleartext, derives a MAC from the ciphertext, and compares the MAC received with the 
newly generated MAC. If the two match, then neither the MAC nor the cleartext message has 
been modifi ed and the receiver has validated the data integrity to itself. Further, the receiver has 
authenticated the sender to itself since only the two key holders could have generated the MAC. 
However, note that the MAC does not provide authentication to a third party in the case of a 
dispute since either the sender or the receiver could have generated the MAC and consequently 
the cleartext.
Encrypted Hash
An encrypted hash is a combination of using a hash function with data encryption to achieve 
both data confi dentiality and data integrity. A good hash function is a compression algorithm that 
demonstrates several important properties including the following:
High compression rate, which means that the length of the output must be signifi cantly 
 
‚óæ
shorter than the length of the input. Th is is necessary so that very large inputs are reduced 
to outputs that are small enough to use with other cryptographic schemes.
One-way function, which simply means that the input cannot be determined from the out-
 
‚óæ
put. Th is also implies that an inverse function does not exist or is infeasible to calculate.
Low collision rate, which basically means that the number of possible outputs from an 
 
‚óæ
infi nite number of inputs have equal probability. For example, a bad hash function is one 
that maps every input to the same output value. Conversely a good hash function has the 
characteristic that the output is mathematically representative of the input.
Two-party transmission as shown in Figure 17.4‚Äîencrypted hash is where the sender (A) and 
receiver (B) have previously and securely exchanged a symmetric encryption key. Th e sender fi rst 
Sender
Cleartext
MAC
MAC
Encrypt
Ciphertext
Ciphertext
Cleartext
Receiver
Secret
Decrypt
Match
A
B
Secret
Previously established symmetric keys
Figure 17.3 Message authentication code.

Data Reliability: Trusted Time Stamps ‚óæ 251
generates a hash value of the cleartext, encrypts the hash value and the cleartext, and transmits the 
ciphertext to the receiver. Th e receiver decrypts the ciphertext to recover the hash value and the 
cleartext, generates a hash from the cleartext, and compares the hash value received with the newly 
generated hash value. If the two match, then the ciphertext has not been modifi ed and the receiver 
has validated the data integrity to itself. Further, the receiver has authenticated the sender to itself 
since only the two key holders could have generated the ciphertext. However, the encrypted hash 
has the same issue as the MAC, as the ciphertext does not provide authentication to a third party 
in the case of a dispute since either the sender or the receiver could have generated the ciphertext 
and consequently the cleartext and the hash value.
Keyed Hash
Th e keyed hash is similar to an encrypted hash except instead of encrypting the cleartext and hash 
value together, the cleartext is combined (not encrypted) with the symmetric key and then the 
hash function is used to generate a hash value. Th e combination algorithm uses several ‚Äúexclusive 
or‚Äù functions (XOR) to create the input to the hash function so that the output is a ‚Äúkeyed hash‚Äù 
value. Two-party transmission as shown in Figure 17.5‚Äîkeyed hash is where the sender (A) and 
receiver (B) have previously and securely exchanged a symmetric encryption key. Th e sender 
generates the ‚Äúkeyed hash‚Äù from the cleartext, and transmits both the cleartext and the ‚Äúkeyed 
hash‚Äù to the receiver. Th e receiver generates a ‚Äúkeyed hash‚Äù from the cleartext, and compares the 
‚Äúkeyed hash‚Äù received with the newly generated ‚Äúkeyed hash.‚Äù If the two match, then neither 
the ‚Äúkeyed hash‚Äù nor the cleartext has been modifi ed and the receiver has validated the data 
integrity to itself. Further, the receiver has authenticated the sender to itself since only the two key 
holders could have generated the ‚Äúkeyed hash.‚Äù However, the ‚Äúkeyed hash‚Äù has the same issue as 
the MAC, as the ‚Äúkeyed hash‚Äù does not provide authentication to a third party in the case of a dis-
pute since either the sender or the receiver could have generated the ‚Äúkeyed hash‚Äù and consequently 
the cleartext.
Previously established symmetric keys
Secret
Sender
Cleartext
Hash
Hash
value
A
B
Ciphertext
Encrypt
Ciphertext
Hash
Cleartext
Cleartext
Receiver
Hash
Match
Secret
Decrypt
Figure 17.4 Encrypted hash.

252 ‚óæ Information Security Management Handbook
Digital Signature
A digital signature is an asymmetric scheme that uses a hash function and a digital signature algo-
rithm to provide enhanced integrity and authentication. Two-party transmission as shown in Figure 
17.6‚Äîdigital signature the sender (A) has an asymmetric key pair, designated as the public key and 
the private key, and the sender has established its public key with the receiver (B). Th e sender creates 
Sender
Previously established asymmetric public key
Hash
value
Hash
Digital signature
Verify
Receiver
Hash
Cleartext
Cleartext
Hash
value
Public
B
Sign
A
Private
Public
Figure 17.6 Digital signature.
Sender
Receiver
A
B
Previously established symmetric keys
Hash
Hash
Hash
value
Hash
value
Cleartext
Cleartext
Match
XOR
XOR
Secret
Secret
Figure 17.5 Keyed hash.

Data Reliability: Trusted Time Stamps ‚óæ 253
a hash value from the cleartext using a hash function; generates a digital signature using the hash 
value, the private key, and the sign function; and transmits the cleartext and the digital signature. 
Th e receiver creates a hash value from the cleartext using the same hash function, and verifi es the 
digital signature using a verify function. Note that the digital signature can only be generated using 
the private key that is under the control of the sender, but the digital signature can be verifi ed by 
anyone using the sender‚Äôs public key. Since only the sender can generate the digital signature, the 
cleartext has authenticity provable to any third party. However, the integrity is impermanent since 
the sender could modify and backdate the cleartext with a new valid digital signature. Diff erent 
relying parties might receive dissimilar information, or the same relying parties might receive dis-
continuous information, whose valid digital signatures convey a false sense of reliability [5].
From an information-security perspective, data reliability must be based on a cryptographic 
technology whose integrity scheme can be validated by any independent third party to a verifi able 
time source [6]. Trusted time stamps are in fact such an integrity scheme that can provide not only 
data reliability, but also have the capacity for enhancing digital signatures.
Trusted Time Stamps
Trusted time stamps can use one of several cryptographic methods but for this explanation the 
method described will be using digital signatures. An independent Time Stamp Authority (TSA) 
provides time stamp tokens (TST) to requestors, which relying parties can depend upon. Two-
party transmission as shown in Figure 17.7‚ÄîTST provided by the TSA has an asymmetric key 
pair, designated as the public key and the private key, and the TSA has established its public 
Public
TSA
Private
Hash
Cleartext
Time stamp token (TST)
Generate digital signature
Generate time stamp
Includes hash value
Hash
Timestamp
Signature
Hash
Hash
Validate hash value
Validate digital signature
Timestamp
Signature
Hash
Timestamp
Signature
Sender
Receiver
Previously established asymmetric public key
TSA
Public
Public
TSA
A
TSA
B
value
Calibrate
Calibrate
NMI
ITA
Figure 17.7 Time stamp token.

254 ‚óæ Information Security Management Handbook
key with both the sender (A) and the receiver (B). Th e sender (who is the requestor for this sce-
nario) generates a hash of the cleartext and submits a request with the hash to the TSA. Th e TSA 
generates a TST by incorporating the sender‚Äôs hash value, generating a time stamp from its clock, 
and generating a digital signature using its asymmetric private key. Th e TSA returns the TST 
to the sender. Note that the TSA never has access to the original cleartext. Th e sender can then 
transmit the TST and the cleartext to the receiver (who is the relying party for this scenario). Th e 
receiver fi rst verifi es that the TST corresponds to the cleartext by validating the hash value, and 
then validates the TST by validating the TSA‚Äôs digital signature using the TSA public key which 
verifi es the binding between the hash value and the time stamp. Th e receiver can then rely on the 
time and integrity of the cleartext.
Th e time stamp is reliable as it is derived from the TSA clock. Th e TSA clock is calibrated to 
a National Measurement Institute (NMI), which similarly calibrates its clock to the International 
Timing Authority (ITA). Th e ITA is the Bureau International des Poids et Mesures (BIPM) near 
Paris, France [7]. Th e United States actually has two NMI, the National Institute of Standards and 
Technology (NIST) Time and Frequency Division [8] and the United States Naval Observatory 
(USNO) [9]. Note that the Global Positioning System (GPS) clocks are calibrated to the NIST 
and USNO atomic clocks and, therefore, GPS can be used by a TSA to calibrate its clock and 
generate a time calibration report.
Unlike digital signatures the sender cannot modify and backdate the cleartext. Th e TST 
ensures the reliance of the data integrity, the authenticity of the TSA, and the trustworthiness 
of the time stamp. Similar to digital signatures the TST is verifi able by any third party using 
the TSA‚Äôs public key. Th e receiver can now truly rely on the data, hence the term ‚Äúdata reliance‚Äù 
versus ‚Äúdata integrity.‚Äù Further, trusted time stamp technology can also be used to extend the 
reliability of public key infrastructures (PKI) by enhancing the sender‚Äôs digital signature.
It is important to recognize that trusted time stamping technology is not a panacea. Th e 
TST must be applied to the workfl ow at the proper sequence of events, otherwise the data reliance is 
no more useful as locking the proverbial barn door after the horses have already left. For exam-
ple, a requestor could generate several diff erent messages prior to an event and obtain a TST for 
each message. Th e optimal message could then be chosen and used after the event. A classic case 
might be late trading. In this scenario an unscrupulous trader generates two messages, a ‚Äúbuy‚Äù 
message and a ‚Äúsell‚Äù message, and obtains a TST for both. Depending on the outcome of the 
targeted stock, the optimal message is submitted after the closing bell, but the TST indicates a 
pre-closing time. Th e TST provides reliance that the message was generated before the closing 
bell, but in this case it does not provide anything useful regarding the actual submission of the 
order. Th e TST obtained by the trader before the order was submitted is an improper imple-
mentation. Rather the TST should be obtained by the stock broker at the time of submission 
to thwart the unscrupulous trader. Of course an unscrupulous stockbroker could practice the 
same deception, so the problem has not been solved; it has only been shifted in the workfl ow. 
A proper solution for this scenario might include a TST at every message-exchange point in 
the workfl ow between participating entities. Note that this scenario discussion has ignored the 
fact that the TSA is required to log all TST generation events [10] such that the extra messages 
would easily be discoverable in an investigation should collusion between multiple parties 
result in fraud.
Two-party transmission as shown in Figure 17.8‚Äîsignature TST is where the TSA has an 
asymmetric key pair and the sender likewise has an asymmetric key pair. TSA has established its 
public key with both the sender (A) and the receiver (B), and the sender has established its public 

Data Reliability: Trusted Time Stamps ‚óæ 255
key with the receiver. Th e sender generates a digital signature from the cleartext using its asym-
metric private key and submits the digital signature (instead of just hash) to the TSA. Th e TSA 
generates a TST using its normal process, the diff erence being that the TST contains the sender‚Äôs 
signature, and returns the TST to the sender as usual. Th e sender can then transmit the TST and 
the cleartext to the receiver. Th e receiver fi rst verifi es that the TST corresponds to the cleartext by 
validating the sender‚Äôs digital signature using the sender‚Äôs public key and then validates the TST 
by validating the TSA‚Äôs digital signature using the TSA public key. Th e receiver can not only rely 
on the integrity of the cleartext as it pertains to the time stamp, but also knows with certainty 
when the sender signed the cleartext. Since the TST is verifi able by any third party using both the 
TSA‚Äôs and the sender‚Äôs public keys, the receiver now has strong authentication and data integrity 
demonstrable to a verifi able time source.
Th e concept of a cryptographically secure digital timestamp (CSDT) was described in 
‚ÄúPreserving public key hierarchy‚Äù [11]. Th e CSDT is essentially a proprietary implementation 
of a TST customized for a specifi c application environment, namely the issuance of a public key 
certifi cate by a Certifi cation Authority (CA). Both schemes have a requestor (for the CSDT it is 
the CA), the equivalence of a TSA (for the CSDT it is called a Timing Authority), and a relying 
party (for the CSDT it is the certifi cate users). Th e CSDT provides the additional assertion as to 
when the CA signed the public key certifi cate such that in the unlikely event of a CA root asym-
metric key pair being compromised, the relying party might still trust the existing certifi cate. Th is 
of course presumes that the point in time at which the compromise occurred is known such that 
any certifi cate issued prior to the compromise might still be trusted.
TSA
TSA
TSA
Private
Signature
Sign
Hash value
Timestamp  token (TST)
Signature
Timestamp
Signature
Signature
Timestamp
Signature
Signature
Timestamp
Signature
Hash
Public
TSA
Public
Public
ÀùAÀù
Public
ÀùAÀù
ÀùAÀù
Private
Sender
Cleartext
Cleartext
Receiver
A
B
TSA
Public
Figure 17.8 Signature TST.

256 ‚óæ Information Security Management Handbook
Standardization
TSA technology, policy, practices, and processes are codifi ed in the National American Standard 
X9.95 Trusted Time Stamp Management and Security [10]. Th is standard is an enhancement of two 
other standards, the Internet Engineering Task Force (IETF) Request for Comment (RFC) 3161 
Internet X.509 PKI Time Stamp Protocol (TSP) [12] and the international standard ISO/IEC 
18014 Information Technology‚ÄîSecurity Techniques‚ÄîTime-Stamping Services [13]. Th ere are 
signifi cant diff erences between the documents as detailed in the Trusted Time Stamp Standards 
[14] and summarized here.
X9.95 includes digital signatures, message authentication codes (MAC), linked tokens, and 
 
‚óæ
the transient key method. ISO/IEC 18014 does not include the transient key method and 
RFC 3161 only supports digital signatures.
X9.95 includes roles and responsibilities for four entities, the Time Source Entity (TSE) such 
 
‚óæ
as the NMI and the ITA, the Time Stamp Authority (TSA), the TST requestor and the TST 
verifi er. ISO/IEC 18014 does not include the TSE and RFC 3161 only address the TSA.
X9.95 contains over 150 specifi c and detailed requirements, whereas ISO/IEC 18014 has 22 
 
‚óæ
and RFC 3161 has 9 requirement statements.
X9.95 provides both Abstract Syntax Notation One (ASN.1) and Extended Markup 
 
‚óæ
Language (XML) defi nition for various objects including the Time Calibration Report, the 
Time Stamp Request, the Time Stamp Response, the Time Stamp Token, the Verifi cation 
Request and the Verifi cation Response. ISO/IEC 18014 and RFC 3161 provide ANS.1 defi -
nitions for a subset of the same objects, however the objects are interoperable.
X9.95 has complete message fl ows and error handling for Time Calibration, Time Stamp 
 
‚óæ
Acquisition, and Time Stamp Verifi cation. ISO/IEC 18014 and RFC 3161 do not provide 
any message fl ows or error handling, but error codes are defi ned.
X9.95 provides example policy and practice statements, whereas ISO/IEC 18014 and RFC 
 
‚óæ
3161 do not address the topics.
X9.95 provides 22 control objectives and 237 evaluation criterion for use by a qualifi ed pro-
 
‚óæ
fessional to assess (or audit) a TSA or other trusted time stamp implementation entity. ISO/
IEC 18014 and RFC 3161 do not provide any assessment or evaluation material.
Note that ISO, IEC, and X9 standards undergo 5-year reviews and revisions so it is likely that 
the diff erentials listed above will change in the near future. Further note that there are other 
proprietary content integrity and time-and-date stamp services that likewise rely on various cryp-
tographic solutions. As of this writing, there are no regulatory or industry requirements for using 
trusted time stamps or for existing TSAs undergoing formal assessments based on any industry 
standards beyond the usual accounting practices such as the Statement on Auditing Standards No. 
70 (SAS 70) for third-party service organizations.
Conclusion
Regulatory and legal precedents are occurring today that are harbingers for proactive data integ-
rity controls. Cryptography is a key element in achieving data confi dentiality, data integrity, and 
authenticity for electronic data; however, not all cryptographic schemes are equivalent. Symmetric 
cryptography can provide limited data integrity and authenticity between participating parties but 

Data Reliability: Trusted Time Stamps ‚óæ 257
cannot provide such assurance to a third party. Asymmetric cryptography can provide authen-
ticity provable to a third party but with impermanent data integrity. However, data integrity 
must include a trustworthy time element, and, therefore, to distinguish between ‚Äúdata integrity‚Äù 
methods in use today and the advanced trusted time stamps, the term ‚Äúdata reliance‚Äù has been 
introduced. Trusted time stamps are the next-generation technology that can provide data reliance 
services that are demonstrable to independent third parties.
About the Author
Jeff  Stapleton is the CTO with Cryptographic Assurance Services with over 25 years experience 
in the cryptography, security, fi nancial, and healthcare industries.
References
 
1. S. W. Teppler, Spoliation of digital evidence: A changing approach to challenges and sanctions, Th e Scitech 
Lawyer, 6(2), FALL 2007, Section of Science & Technology Law American Bar Association.
 
2. Information Assurance Consortium, Wall of Shame, www.infoassurance.org
 
3. S. W. Teppler Esq., B. Nearon, J. Stanley Esq., and J. Burton Esq., Life after Sarbanes-Oxley: Th e 
merger of information security and accountability, Jurimetrics J. 45 379‚Äì412 2005.
 
4. B. Rothke, B. Nearon, S. Teppler Esq., and J. Stapleton, Th e options backdating fi asco: Time-based 
data control issues leads to compliance problems, shareholder lawsuits and criminal indictments, 
LAW-403, RSA Conference, San Francisco, CA, 2007.
 
5. J. Stapleton, P. Doyle, and S. Teppler Esq., Digital signature paradox, 6th IEEE Information Assurance 
Workshop, West Point, NY, April 2005.
 
6. J. Stapleton, Digital signatures are not enough, ISSA J. 4 1 January 2006.
 
7. Bureau International des Poids et Mesures, http://www.bipm.org/
 
8. Time and Frequency Division, National Institute of Standards and Technology, http://tf.nist.gov/
 
9. United States Naval Observatory, http://www.usno.navy.mil/
 10. American National Standard X9.95 Trusted Time Stamp Management and Security, 2005 www.x9.org
 11. G. C. Grabow, Preserving public key hierarchy, in H. F. Tipton and M. Krause (eds), Information 
Security Handbook, 6th edn., New York: Auerbach Publications, 2007.
 12. RFC 3161 Internet X.509 PKI Time Stamp Protocol (TSP), August 2001 www.ietf.org
 13. ISO/IEC 18014 Information Technology‚ÄîSecurity Techniques‚ÄîTime-Stamping Services, www.iso.org; 
www.infoassurance.org
 14. J. Stapleton, Trusted time stamp standards: A comparison and guideline of the American national standard 
X9.95 trusted time stamp management and security, Information Assurance Consortium, 2007.


259
18
Chapter 
Security in the .NET 
Framework
James D. Murray
Contents
Trustworthy Computing ..........................................................................................................261
Releases of the .NET Framework and Windows .......................................................................261
Java and .NET ........................................................................................................................ 262
.NET Framework Security ...................................................................................................... 263
.NET Security by Domain .................................................................................................. 263
Security Inside the .NET Framework ...................................................................................... 263
Common Language Runtime ............................................................................................. 263
Managed Code ................................................................................................................265
Unmanaged Code ...........................................................................................................265
Assemblies .......................................................................................................................265
Strong-Named Assemblies ...............................................................................................265
Type Safety ..................................................................................................................... 266
Bounds Checking ........................................................................................................... 266
Security Exception Management .................................................................................... 266
Garbage Collection ........................................................................................................ 266
Code Access Security ...........................................................................................................267
Evidence-Based Security ..................................................................................................267
Evidence..........................................................................................................................267
Security Policies ............................................................................................................. 268
Application Domains .......................................................................................................... 268
Application Security ................................................................................................................ 268
.NET Framework Security Namespaces .............................................................................. 269
Isolated Storage ....................................................................................................................270
Secure Strings ......................................................................................................................270

260 ‚óæ Information Security Management Handbook
Since its formal introduction in 2002, the Microsoft.NET Framework has become the standard 
development platform for mobile and desktop applications, Web sites and Internet services, and 
enterprise-class software solutions running on Microsoft Windows. Th e .NET Framework itself 
is a managed environment used to provide a structured development platform for creating and 
managing many types of software solutions.
Microsoft created the .NET Framework in response to the need of a standardized platform for
Designing general-purpose, network-distributed, and Web-based applications
 
‚óæ
Providing platform-independent services for those applications to use
 
‚óæ
Managing (monitoring and controlling) applications running in a Windows environment
 
‚óæ
Th e ultimate purpose of the .NET Framework is to free developers from making decisions such 
as writing codes based on system dependencies, creating nonstandard services, and implementing 
low-level security, and instead focus on development based on the business goals of the needed 
solution. As stated on the Microsoft Developer Network (MSDN) Web site, the .NET Framework 
is designed to fulfi ll the following objectives:
To provide a consistent object-oriented programming environment whether object code 
 
‚óæ
is stored and executed locally, executed locally but Internet distributed, or executed 
remotely.
Security Architecture and Design .............................................................................................270
Windows Host Security .......................................................................................................270
COM and DCOM ..............................................................................................................271
ASP .NET ...........................................................................................................................271
Levels of Trust ................................................................................................................ 272
Access Control ........................................................................................................................ 272
Authentication .................................................................................................................... 272
Role-Based Security ........................................................................................................ 272
Authorization ..................................................................................................................... 273
Accounting ......................................................................................................................... 273
Network Security .................................................................................................................... 273
Secure Network Communications Protocols ....................................................................... 273
Security Support Provider Interface .....................................................................................274
Cryptography ...........................................................................................................................274
Microsoft Cryptographic Application Programming Interface .............................................274
Cryptographic Service Providers ......................................................................................274
Changes in CAPI ............................................................................................................274
Encryption and Decryption .................................................................................................275
Cryptographic Hashing .......................................................................................................275
Digital Signing ....................................................................................................................275
Public-Key Cryptography Standards ....................................................................................275
Random Number Generation ..............................................................................................276
Data Protection API ............................................................................................................276
.NET Framework Security Issues ..............................................................................................276
Conclusion .............................................................................................................................. 277
About the Author .................................................................................................................... 277

Security in the .NET Framework ‚óæ 261
To provide a code-execution environment that minimizes software deployment and version-
 
‚óæ
ing confl icts.
To provide a code-execution environment that promotes safe execution of code, including a 
 
‚óæ
code created by an unknown or semi-trusted third party.
To provide a code-execution environment that eliminates the performance problems of 
 
‚óæ
scripted or interpreted environments.
To make the developer experience consistent across widely varying types of applications, 
 
‚óæ
such as Windows-based and Web-based applications.
To build all communication on industry standards to ensure that codes based on the .NET 
 
‚óæ
Framework can integrate with any other code.
It is interesting to note that all of these objectives have security implications, either directly or 
indirectly. Security in a computing system is often expressed as a trust in the precision, reliability, 
and confi dentiality of the system. Th is fact is realized by Microsoft and is expressed in Microsoft‚Äôs 
Trustworthy Computing initiative.
Trustworthy Computing
In 2002, Microsoft launched its Trustworthy Computing initiative in an eff ort to ensure the 
delivery of secure, private, reliable, and helpful computing experiences for all users. Th e four key 
goals of this initiative are
Security to ensure the confi dentiality, integrity, and availability of a system and its data, and 
 
‚óæ
to provide resilience to attack.
Privacy to provide reliable control over how the information stored by a system is accessed 
 
‚óæ
and protected.
Reliability to guarantee that a computing system is dependable, is available when needed, 
 
‚óæ
and performs at the level expected.
Business practices that help discover eff ective and responsible solutions to business cus-
 
‚óæ
tomer‚Äôs problems with product, services, or interactions.
Th e .NET Framework supports Trustworthy Computing by allowing developers to write .NET 
applications that make use of Windows libraries and services, which provide technical support for 
these four goals. Developers using the .NET Framework may create applications that support host 
and user authentication, information privacy, and ensure reliable operation and resiliency against 
errors and attacks.
Releases of the .NET Framework and Windows
Since the fi rst beta release of .NET in 2000, there have been subsequence releases of .NET (1.1, 
2.0, 3.0, and 3.5) that have extended the capabilities of the .NET Framework, including the 
security framework. Th erefore, not all security features detailed in this article are available in all 
versions of the .NET Framework.
It is also useful to know that not every release of the .NET Framework runs on every version 
of Microsoft Windows, or even comes preinstalled on most versions of Windows:

262 ‚óæ Information Security Management Handbook
.NET Framework 1.0 was released in January 2002, and ran only on Windows 98, Windows 
 
‚óæ
NT 4.0, Windows 2000, and Windows XP. .NET 1.x also runs on later releases of Windows, 
but is never preinstalled on any Windows distribution.
.NET Framework 1.1 was released in April 2003, and came preinstalled on Windows Server 
 
‚óæ
2003. Release 1.1 added support for security features such as Code Access Security and 
levels of trust.
.NET Framework Version 2.0 was released in November 2005, and featured many additions 
 
‚óæ
and updates, especially to the .NET security engine. Th is is the last version of .NET to sup-
port Windows 2000, Windows 98, and Windows Me. No distribution of Windows includes 
.NET 2.0 preinstalled.
.NET Framework 3.0 was released in November 2006, and came preinstalled on Windows 
 
‚óæ
Vista and Windows Server 2008, and can only be installed on Windows Server 2003 with 
Service Pack 1 (and later) and Windows XP with Service Pack 2 (and later). Release 3.0 
contains new major features, but no major architectural changes.
.NET Framework 3.5 was released in November 2007, and comes preinstalled only on 
 
‚óæ
Windows Vista with Service Pack 1 (and later). Th is release includes mostly fi xes and 
improvements, but nothing security related.
.NET Framework 4.0 is due to be released in 2009.
 
‚óæ
Improvements in security are present in each release of .NET from 1.0 through 2.0. .NET 3.x uses 
the same Common Language Runtime as .NET 2.0 and has no signifi cant changes in security 
that requires updating applications from .NET 2.0. To make full use of .NET security features, it 
is recommended that .NET applications and all Windows systems use (at least) .NET 2.0.
Java and .NET
Th e .NET Framework is conceptually identical in many respects to the Java platform. Th e Java 
Virtual Machine, Java byte codes, and Java class libraries have equivalents in .NET as the .NET 
Common Language Runtime, Common Intermediate Language, and the Framework Class 
Library. Both Java and .NET can run applications that are either compiled before execution or 
Just-In-Time compiled as they are executed. In addition, both support applications developed 
to run from a command line shell, a GUI shell (the desktop), in a Web browser, or a mobile 
device.
Some notable diff erences between Java and .NET are
Java has been widely ported to many operating platforms (e.g., Windows, UNIX, Linux, OS X), 
 ‚óæ
while Microsoft‚Äôs implementation of .NET is only available for Microsoft Windows.
Microsoft supports the implementation of hundreds of programming languages that use the 
 
‚óæ
.NET Framework, while Sun Microsystems only supports the use of the Java language for 
the Java platform.
Portions of the .NET Framework are formalized by the ISO/IEC and ECMA as international 
 ‚óæ
standards, while Sun Microsystems exclusively controls the Java standard.
Most of the Java software platform is licensed as free and open source under the GNU 
 
‚óæ
General Public License, while the .NET Framework remains largely closed source and 
proprietary to Microsoft.

Security in the .NET Framework ‚óæ 263
Java is a third-party software for Microsoft Windows, while Microsoft itself develops the 
 
‚óæ
.NET Framework. Th erefore, the .NET Framework may be perceived as having better inte-
gration and accessibility to the Microsoft Windows environment than does Java.
All the similarities and diff erences aside, both Java and .NET have many of the same goals, includ-
ing developing better, useful, and more secure applications more quickly by writing less code, and 
distributing all applications more easily and securely.
.NET Framework Security
Th e .NET Framework provides a variety of security-related features and services that developers 
may choose to include in the design of their .NET applications. Services such as authentication, 
private network communications, secure data storage, and data encryption are available to all 
.NET applications as needed.
.NET also provides security features that run transparently in all .NET-based applications, 
requiring little or no confi guration by the developer or system administrator. Th e continual 
monitoring of a running .NET application for invalid input, illegal access, and security policy 
violations are performed continually at the application level.
.NET also provides a security model in which all .NET applications must run. Th e model 
determines what permissions are granted to the code running in a .NET application and defi nes 
and enforces the security boundaries of each .NET application. Th is security model runs indepen-
dently of the Windows user context-based model.
.NET Security by Domain
To better understand the security features of the .NET Framework, it is useful to categorize them 
using the (ISC)2 CISSP Common Body of Knowledge (CBK). Exhibit 18.1 lists the major .NET 
security features and their corresponding CISSP CBK domains.
Security Inside the .NET Framework
How .NET security works must be understood through the features and mechanisms that 
form the components of the .NET Framework itself. Some components, like assemblies and the 
Common Language Runtime, are indispensable parts of .NET security. Security features, such 
as user authentication and string encryption, are only used if needed by an application. Still other 
mechanisms, such as garbage collection, are not specifi cally security features, but are very impor-
tant for the secure operation of a .NET application.
Common Language Runtime
Th e .NET Common Language Runtime (CLR) is the virtual runtime environment that con-
trols the execution of .NET code on the Windows platform. Th e CLR provides all the services 
essential for running a .NET application including memory management, thread manage-
ment, type checking, exception handling, garbage collection, and security. Th e CLR itself is 

264 ‚óæ Information Security Management Handbook
Microsoft‚Äôs implementation of the Common Language Infrastructure (CLI) standard defi ned 
in ISO/IEC 23271:2006.
Th e CLR acts as the security watchdog of the .NET environment. It constantly checks for 
illegal instructions that would aff ect the integrity of the application or violate the security policy 
of the hosting environment. If the code in a .NET application attempts to commit an illegal 
Exhibit 18.1 .NET Security Features in Relation to the CISSP CBK Domains
Security Domain
Supporting Features
Benefi ts
Access control
Role-based security
Authentication
Host-based access control
Authorization
Logging and event reporting
Accountability
Application security
Code access security
Resiliency to error, misuse, and 
attacks
Applications domains
Input validation
Isolated storage
Secure strings
Cryptography
Secret key cryptography
Privacy and confi dentiality of 
data
Public key cryptography
Authentication and non-
repudiation of data ownership
Cryptographic hashing
Use of common and formalized 
standards of cryptography and 
cryptosystems
Digital signatures
Random number generation
Security API interfaces
Security architecture and 
design
Host-based security 
integration
Integration and conformance 
to Windows platform security
COM/DCOM integration
ASP.NET security
Network security
Secure network protocols
Use of common and accepted 
standards of secure network 
communications
Security provider interfaces
PKI interface

Security in the .NET Framework ‚óæ 265
operation, either accidentally or on purpose, it is prevented from doing so by the CLR. Th e secu-
rity operation of the CLR is a compliment to the security policies of the Windows-operating 
system and does not override host-based or domain-based security mechanisms.
Managed Code
Managed code is any application code whose execution is controlled directly by the CLR. 
Applications written using a .NET language are not compiled to native machine code, and, there-
fore, are not directly executed by the CPU. Instead, .NET applications are compiled to Common 
Intermediate Language (CIL, formerly known as Microsoft Intermediate Language, or MSIL) and 
Just-In-Time (JIT) compiled to native machine code at run-time. Th e JIT stage is when checks for 
security and consistency are performed.
Managed code also does not support the use of unmanaged pointer types in application code. 
Memory pointers are a constant source of problems involving uninitialized memory references 
(wild pointers), code stepping beyond memory object boundaries (overfl ow or overruns), and 
pointing to data objects that no longer exist in memory (dangling pointers). Such problems cause 
memory faults in a program‚Äôs execution. Th ey are also a possible source of security vulnerabilities 
from arbitrary code execution attacks that change the memory location stored by the pointer. 
None of these problems exists in managed .NET applications.
Unmanaged Code
Unmanaged code is any application code executed outside of the control of the CLR. Th is includes 
all programs compiled to a binary image, all Windows-operating system APIs, and all COM/
DCOM components. When unmanaged code is called by a managed .NET application, the oper-
ation of the unmanaged code is not controlled by the CLR; therefore there can be no assurance of 
secure and correct operation of the unmanaged code by the CLR. For reasons of both reliability 
and security, it is often a development constraint that .NET applications are designed to only use 
managed code by only calling classes and methods implemented in the .NET Framework.
Assemblies
In .NET, an assembly is a library that stores the managed code used by a .NET application. An 
assembly contains the pre-complied CIL code, class libraries, and localized resources used by the 
application. An assembly will also contain metadata used to identify the object types, attributes, 
and security permissions in the managed code, and versioning, locality, and deployment informa-
tion about the .NET application. An assembly itself is a set of fi les JIT compiled and executed by 
the CLR to run a .NET application.
Strong-Named Assemblies
Code signing is used to enhance the security of .NET assembles. When an assembly is signed with 
a digital signature, it is said to have a strong name. A strong name is created from the assembly‚Äôs 
text name, version number, locale information, a public key, and a digital signature. A strong name 
provides a unique identity for an assembly, allowing integrity checking to be performed before the 
assembly is loaded into memory. If the integrity check fails, a security exception is generated and 
the assembly is not loaded.

266 ‚óæ Information Security Management Handbook
Assembly authentication is provided using an Authenticode signature. Authenticode enables 
signing an assembly using an X.509 certifi cate stored in a Public Key Infrastructure (PKI) reposi-
tory. Th e assembly‚Äôs publisher signs the assembly using a private key, and the authentication is 
checked using the corresponding public key stored in a trusted certifi cate authority. If the pub-
lisher‚Äôs identify cannot be verifi ed, the .NET application user is asked (via a pop-up dialog box) if 
the publisher is trusted or not. Both strong names and Authenticode are especially important for 
identifi cation of assemblies deployed in mobile and Web-based environments.
Strong names also help prevent a long-standing problem with Windows involving multiple 
versions of the same library installed on the same computer. Th is ‚ÄúDLL Hell‚Äù causes applications 
to load the wrong version of a shared library fi le, which can cause security problems and often the 
source of program failures. Strong names allow a specifi c .NET application to use only the version 
of an assembly it is built for, and for multiple version of the same assembly to be installed on a 
single computer and not to cause problems.
Type Safety
Type safety is the ability to prevent operations on an object that is not appropriate for its data size 
or type, such as writing a string value to an integer object. Such operations can cause memory 
violations that may result in the unstable operation of the application or cause the application to 
crash. In rare circumstance, such errors are also exploitable as security vulnerabilities.
Type safety verifi cation is provided both at compile-time by the .NET language used to write 
the application and at run-time by the CLR. Coding errors that could cause a type safety problem 
are fl agged as warnings or errors by the .NET compiler. Type safety also provides memory safety by 
preventing the referencing of arbitrary memory locations by a .NET application. Th is includes the 
execution of code on the program stack or heap commonly used in buff er overfl ow attacks.
Bounds Checking
Bounds checking is the ability of an object to autonomously control the allocation of memory it 
uses to store data. String and StringBuilder are examples of .NET Framework classes that perform 
their own bounds checking. If a string is assigned to an object instantiated from either of these 
classes, the object will check if its internal buff er is large enough to store the sting data. If it is not, 
either the size of the buff er will be increased or an exception will be thrown indicating suffi  cient 
storage space does not exist. In either result, a buff er overfl ow condition will not occur.
Security Exception Management
Security exceptions are events generated when the CLR detects a security error in a .NET application. 
Security exceptions can result from authentication failures, insuffi  cient permission to perform 
an operation, invalid strong name detection, and from arithmetic overfl ows and underfl ows. 
Developers are responsible for writing code in their applications that handle all security exceptions 
generated by the .NET Framework.
Garbage Collection
Garbage collection is the mechanism that allocates and releases memory managed by the CLR. 
Although not considered a security feature, garbage collection does prevent some common security 

Security in the .NET Framework ‚óæ 267
vulnerabilities, such as memory leaks and premature deallocation, found in programming envi-
ronments where developers are required to control memory deallocation explicitly in their code. 
Garbage collection does not securely overwrite memory before it is deallocated, nor has it control 
over unmanaged memory or resources, pointer types, and static variables. Th e application devel-
oper must, therefore, implement extra security measures if the secure destruction of data objects 
is required.
Code Access Security
Code Access Security (CAS) is a security model in .NET that enforces the access limitations an 
assembly has to specifi c protected resources and restricted operations. Diff erent assemblies in a 
.NET application may have diff erent levels of trust depending on their publisher and origin of 
their code. CAS uses the integral security features of the CLR to perform higher-level adminis-
trative operations on running code, such as authenticating assemblies before they are loaded and 
checking permissions when code attempts to access resources. CAS is, therefore, an abstraction of 
both the CLR and the features of .NET used to prevent managed code from performing opera-
tions for which it is not trusted.
Evidence-Based Security
Windows uses a user-based security model in which all operations are granted or denied based 
on the security context of the user performing the operation. Th e CAS uses an evidence-based 
security model based on identifying information provided by assemblies and trusted application 
domain hosts. Th e evidence-based security model is independent of the security context of the 
user, and is applied on top of Windows‚Äô user-based security and does not replace it.
Evidence
Evidence is information obtained from an application domain host or .NET assembly and com-
pared to system or user security policies to determine what permissions the host or assembly has. 
Assembly evidence is created when the assembly is coded and built by the developers, but before it 
is signed. Evidence that may be present in an assembly includes
Th e strong name (digital signature) validated by the CLR when the assembly is loaded.
 
‚óæ
A cryptographic hash value of the assembly.
 
‚óæ
Th e Authenticode X.509v3 signature of the software publisher.
 
‚óæ
Th e physical folder and path where the assembly‚Äôs fi les are located.
 
‚óæ
Th e URL of the Web site from which an assembly originates.
 
‚óæ
Th e URL where an assembly is located.
 
‚óæ
Th e Internet Explorer security zone of an assembly.
 
‚óæ
Custom information added to an assembly by the application developers. Th is can include 
 
‚óæ
simple data, such as date the assembly was approved for release, or stronger data, such as 
additional public keys to be used for authentication.
A trusted application domain (e.g., Web browser, Web server, operating system shell) hosting the .NET 
application passes both its own evidence and the assembly‚Äôs evidence to the CLR when the assembly 
is loaded. Th e evidence is evaluated against the current enterprise, machine, application, and user 

268 ‚óæ Information Security Management Handbook
security policies to determine the permission the assembly will have. If the host is not trusted to 
control evidence, the CLR uses the same security permission it uses for the host itself.
Not all evidence is regarded with equal trust. Assembly evidence is not considered as trust-
worthy as evidence provided by a trusted host. Strong names and Authenticode signatures are also 
considered stronger evidence because they are diffi  cult to modify and forge.
Security Policies
CAS enables system administrators to confi gure security policies allowing the CLR to determine 
what operations an assembly is permitted to perform. Th e policies are a mapping of assembly evi-
dence to a set of operations that the assembly‚Äôs code has authorization to perform. For example, an 
assembly originating from a trusted and authenticated source may have the ability to write to the 
Windows event logs, while an unauthorized assembly originating from the Internet may not.
Managed code can make permissions requests, allowing an assembly to request only the level 
of permission it needs to perform its operations. Code that cannot be granted its minimum per-
mission request is not loaded, and the application generates a security exception when it is fi rst 
started and not at a later time when the application is running.
Like the Windows security model, .NET policies can be confi gured for diff erent scopes, including 
enterprise, machine, user, and application domains. Policies are set by the system administrator 
and are evaluated by the CLR when the application is run. Th e CLR determines the trust of the 
assembly from the permission and assigns a level of trust from a predefi ned set of permissions 
defi ned for a .NET application.
Application Domains
Operating systems provide isolation by running each application in a separate process. Th is isola-
tion provides a security boundary that does not allow a process to inadvertently access the memory 
or private resources of other processes. Th e CLR can provide further security boundaries by creat-
ing application domains within a single process. Each application domain isolates objects within an 
application from each other, much the same way processes are isolated by the operating system.
Application domains are a primary mechanism used by CAS for enforcing security boundaries 
and checking run-time security. When an assembly is loaded by the CLR, it is placed in its own 
application domain and cannot access objects in any other application domains unless it is trusted 
to do so. Each application domain, therefore, has its own security permissions and level of trust. An 
assembly in an application domain can be unloaded without aff ecting all other application domains. 
Th is ability allows applications domains to perform sandboxing of potentially unsafe code, but also 
increase application performance and control application memory utilization as well.
Application Security
.NET applications, components, and services are built on top of a standard library called the 
Framework Class Library (FCL). Th e FCL contains thousands of classes and methods that extend 
functionality to all applications that use it. Many implicit security operations are implemented 
directly in the code in the FCL, including parameter value checking and access permission restriction. 
All .NET applications, therefore, contain security-minded code, although a .NET application 
developer may never write a single line of code with security in mind.

Security in the .NET Framework ‚óæ 269
.NET Framework Security Namespaces
Th e FCL orders related classes into groups called namespaces. All .NET classes are referenced 
within the context of a namespace to isolate them into functional groups, and to prevent any 
ambiguity between classes that have the same name. Namespaces are very similar in concept 
to folders in a fi le system, with fi les being the classes. Th e FCL itself can be regarded as the 
Application Programming Interface (API) for .NET itself.
Th e base namespace for most security functions is System.Security. Several other namespaces 
also contain security-related classes. Th e following is a listing of all of the security-related 
namespaces in the FCL:
System.IO.IsolatedStorage
 
‚óæ
 provides access to secure storage locations for isolating data by the 
context of the current user, domain, and assembly creating the store.
System.Net.Security
 
‚óæ
 provides authentication and encryption for secure network communica-
tion between hosts using SSL.
System.Runtime
 
‚óæ
 provides control over services used by the .NET security engine, including 
garbage collection.
System.Security
 
‚óæ
 is the base class for all CLR security functions.
System.Security.AccessControl
 
‚àí
 provides auditing and rule-based access control over secur-
able objects.
System.Security.Authentication
 
‚àí
 defi nes the type of authentication to be used with an SSL 
connection.
System.Security.Cryptography
 ‚àí
 provides the base cryptographic services of .NET, includ-
ing encryption and decryption, cryptographic hashing, message authentication, and 
random number generation.
System.Security.Cryptography.Pkcs
‚Ä¢ 
 enables applications to use the Public Key 
Cryptography Standards (PKCS) supported by .NET, including message signing, 
key exchange, requesting certifi cates, and PKI.
System.Security.Cryptography.X509Certifi cates
‚Ä¢ 
 enables applications to use X.509 v.3 
certifi cates for reading and distributing public keys.
System.Security.Cryptography.Xml
‚Ä¢ 
 enables applications to create and validate XML 
digital signatures to provide integrity or message authentication for data.
System.Security.Permissions
 
‚àí
 provides controlled access to operations and resources based 
on security permissions and Windows security policy.
System.Security.Policy
 
‚àí
 allows the CLR to determine operation based on security policy 
using code groups, membership conditions, and evidence.
System.Security.Principle
 
‚àí
 defi nes a principal (Windows group or user) that represents the 
security context under which manage code is run.
System.Security.RightsManagement
 
‚àí
 provides an application access to the Microsoft 
Windows Active Directory Rights Management Services (AD RMS) for controlling 
application-created content.
System.Security.SecurityException
 ‚àí
 provides notifi cation when a security error is detected 
by the CLR.
System.Web.Security
 
‚óæ
 provides security features specifi cally for ASP.NET managed code in 
Web server applications. Security features includes Windows, Forms, and Microsoft Passport 
authentication, URL and fi le authorization, and access to Active Directory.

270 ‚óæ Information Security Management Handbook
Isolated Storage
Th e .NET Framework supports the concept of isolated storage for the creation of safe, private, 
virtual fi le systems for each .NET application. Isolated storage can be used when a computer‚Äôs fi le 
system does not provide suffi  cient isolation or protection for a user‚Äôs data from untrusted code, or 
an application is not suffi  ciently trusted to allow it access to the Windows fi le system and registry. 
A typical use of isolated storage is to store application settings and user preferences, so they are 
hidden from access by untrusted code. Th e use of isolated storage also removes the need for the 
developer to write code for creating fi le names, checking fi le paths, and verifying that the applica-
tion has read and write fi le permissions.
Every assembly run under the user‚Äôs context is given its own private area in the user profi le folder 
on the local system disk in which to store data. Access to this folder is based on either application 
identity or user identity, and conditional access is based on assembly evidence and application domain. 
Roaming profi les are used with isolated storage to allow an application‚Äôs isolated stores to travel with 
the user‚Äôs roaming profi le. System administrators can control isolated storage by confi guring the size 
(quota) of the allocated space, clearing the space, and setting the security policies. Administrators also 
have unrestricted access to all of the current user‚Äôs isolated stores.
Secure Strings
A common security problem with running applications is that they often store sensitive informa-
tion as readable clear-text in memory and do not properly dispose of the information after it is no 
longer needed. Th is type of data leakage is sometimes observed in programs that accept a password 
for authentication and then fail to overwrite the password before removing it from memory. In 
.NET, string data is stored in memory as clear-text that is not automatically overwritten before it 
is removed from memory, leaving the string data to remain in memory even after the string object 
has been destroyed and garbage collection has occurred.
To remedy the possibility of string data exposure, the .NET Framework provides a SecureString 
class that is used to store string data in memory in an encrypted data format. To a .NET applica-
tion, SecureString objects behave as normal strings, with the string data itself being encrypted 
when it is initialized and modifi ed. A SecureString object can store data as either read‚Äìwrite or 
read-only, and can be forced to clear its data and immediately remove it from memory without 
waiting for garbage collection to occur. Th e data in a SecureString object is also not visible or 
accessible via COM.
Security Architecture and Design
Th e .NET Framework provides both a security model and architecture of its own and integration 
with the security architecture of the Windows operating system. Th e .NET concept of application 
security is built on the top of the security model for the entire Windows platform. .NET is not 
inherently capable of bypassing or overriding Windows system or network security settings.
Windows Host Security
Th e Windows security policy supports the concept of security zones. Th ese predefi ned zones are 
used to segregate the location of code based on its physical location and levels of trust of its region. 

Security in the .NET Framework ‚óæ 271
Th e zones are My Computer, Intranet, Trusted, Internet, Untrusted, and NoZone, and are con-
fi gured using the Internet Options applet in Control Panel, or the Security tab on the Microsoft 
Internet Explorer options panel.
Zones allow the CLR to determine how to trust code based on its identity. Th is is especially 
important for ASP.NET applications, whose code originates partially from the local machine and 
partially in an environment accessed via a Web browser. Th e CLR can run the local code with 
full trust, while allowing only partial trust for the mobile code running in the browser, eff ectively 
sandboxing the mobile application from the local environment.
COM and DCOM
Th e Component Object Model (COM) is the Microsoft protocol and interface standard for inter-
process communications between software components residing in diff erent processes on the same 
machine. ActiveX is an interface model based on COM that allows Windows programs to run 
hosted in Web browsers, and is identical in concept to Java applets.
Although the .NET Remoting feature is a functional replacement for COM, .NET is fully 
capable of communicating securely and reliably with unmanaged COM components. .NET 
WinForm and WebForm controls off er a much more secure replacement for ActiveX components. 
Th e versioning and signing of .NET assemblies removes the ‚ÄúDLL Hell‚Äù created by COM compo-
nents. And .NET assemblies can expose COM interfaces of their own.
Th e Distributed Component Object Model (DCOM) is the Microsoft protocol and interface 
standard for communications between software components residing on separate, network-connected 
machines. DCOM supports an RPC encryption protocol that provides encryption of all packets in 
an RPC channel. Th e Remoting and XML Web Services features of the .NET Framework are the 
functional replacement of DCOM (and COM+) in .NET applications.
ASP .NET
ASP.NET (Active Server Pages for .NET) is an application framework used by developers for the 
creation of Web applications, Web sites, and Web services. ASP.NET is built on top of the .NET 
Framework, and uses .NET the same way that other .NET applications do.
Security features typically employed by ASP.NET applications include authenticating, regis-
tering, and managing users; authorizing access to Web site features; and provisioning application, 
provider, and database confi gurations. Many of these high-level features are implemented directly 
in ASP.NET and not in .NET itself. However, ASP.NET does benefi t greatly from the security 
features inherent in the .NET Framework.
Th e security concerns specifi c in ASP.NET are those of any Web site, including SQL Injection 
and Cross Site Scripting (XSS) attacks. ASP.NET contains features used to prevent these types 
of attacks, such as automatic validation of all HTTP GET and POST Request variables, disal-
lowing the illegal modifi cation of pre-populated data controls, and preventing directory traversal 
that would read‚Äìwrite outside of the document root. Input validation applied by ASP.NET on 
WebForm fi eld data defeats malformed URL and SQL Injection attacks.
Web Services provide cross-platform communication between heterogeneous systems, and 
are used quite extensively in Service-Oriented Architecture (SOA) and Software as a Service 
(SaaS) designs. Th e security of Web Services is greatly increased by using the authentication 
and data encryptions features available in the System.Web.Security namespace of the .NET 
Framework.

272 ‚óæ Information Security Management Handbook
Levels of Trust
In .NET 1.0, all ASP.NET Web applications ran with full trust, making them unable to apply 
the rules of Code Access Security. Starting with .NET 1.1, the system administrator is allowed to 
confi gure the trust level of an ASP.NET application, ranging from full unrestricted access to no 
access execute-only permission. Allowing diff erent parts of a .NET application to operate at dif-
fering levels of trust blocks the more vulnerable parts of an application, such as a WebForm, from 
accessing the more privileges features of the .NET Framework while not impairing the access by 
more trusted code in the same ASP.NET application.
Access Control
Access control uses the operations of authentication, authorization, and accounting to identify a 
subject; determine what objects the subject can and cannot access; and record an audit trail of the 
subject‚Äôs actions. Th e .NET Framework provides features allowing applications to perform these 
access control operations as needed.
Authentication
Th e .NET Framework supports three methods of user authorization and authentication: host based, 
role based, and custom. Host-based authentication uses Windows authentication to determine the 
identity and permission of a user based on a Windows local or domain user account. Role-based 
authentication allows users to be authenticated based on a role, which can be application defi ned or 
based on Windows groups. Custom authentication allows the developer to implement an application-
specifi c authentication scheme.
ASP.NET is fully integrated with the authentication mechanism used for a Windows domain 
or network using NT LAN Manager authentication, or Kerberos for single sign-on capability. 
Other authentication schemes available to ASP.NET applications include digest authentication 
with MD5, using digital certifi cates to mutually authenticate both the client and server, and cus-
tom authentication schemes created by developers.
Role-Based Security
.NET contains a generic, role-based security mechanism that allows the CLR to make authen-
tication and authorization decisions based on the identity and roles of a principle running 
the application. A principle is typically a Windows user account, with the groups to which 
the user belongs acting as the roles. Using Windows user accounts to determine access per-
missions for an application allows an administrator to change the permissions without also 
requiring an update and redistribution of the application. Principles can also be subjects in 
other systems, such as Active Directory or SQL Server, or a customized identity unrelated to 
user accounts.
Role-based security verifi es that a principle has a role that allowed it to perform a requested 
operation, such as creating a fi le or loading an assembly. Role-based security is commonly used 
in ASP.NET applications; however, any type of .NET application may use role-based security as 
needed.

Security in the .NET Framework ‚óæ 273
Authorization
Authorization is the process of determining what access permission an authenticated principle has 
concerning a specifi c object. In Windows, applications are authorized to access specifi c objects based 
on the context of the user, group, or computer successfully authorized by the application. Access to 
objects is granted based on the authenticated user‚Äôs NTFS permission. Th ese permissions are ACL 
based and can use either an ‚Äúallow all, except‚Ä¶‚Äù or a ‚Äúdeny all, except‚Ä¶‚Äù permissions scheme.
ASP.NET applications may also use NTFS permissions to access objects as if they were users 
logged on locally to Windows or on to the Windows domain. Authorization rules can also be 
specifi ed in Web services, and access granted or denied to a specifi c object based on user name or 
role specifi ed in a URL.
Accounting
Accounting refers to the ability of an application to create an audit trail of its activities in the form 
of a log fi le or log records in a database. Th e .NET Framework provides access to the Windows 
event-logging service for reporting application information to the Windows administrator. 
Developers may design .NET applications to report suspicious user activity, including attempted 
unauthorized access, the use of privileged features, and changes in the user‚Äôs security context. Th e 
.NET Framework also supports the services needed for implementing event-reporting protocols, 
such as syslog and SNMP.
Network Security
Th e .NET Framework provides applications access to network security services available on the 
Windows platform. Standardized network encryption and authentication services are available for 
low-level TCP/IP sockets and higher-level network and Internet communications. .NET applica-
tions may create encrypted communication channels, access secure Web sites and Web services, use 
Public Key Infrastructure services for authenticating and decrypting data, and securely exchange 
information with non-Windows-based network hosts. All secure networking protocols supported 
by .NET are found in Windows 2000/XP/Vista and later releases of Windows.
Secure Network Communications Protocols
Th e Hypertext Transfer Protocol Secure (HTTPS) is the use of the Hypertext Transfer Protocol 
(HTTP) over an encrypted network security protocol, such as SSL or TLS. Th e use of HTTPS 
is to ensure confi dentiality of HTTP transactions and to prevent eavesdropping and man-in-the-
middle attacks. SSL and TLS are used with certifi cate-based authentication systems and provide 
packet traffi  c encryption. Although commonly used by ASP.NET applications, any .NET applica-
tion may utilize network protocols associated with the Internet and the World Wide Web.
SSL, TLS, and HTTPS provide a secure, point-to-point communications channel between 
host applications. For creating a secure, host-to-host communications channel, .NET provides 
interfaces to protocols such as IP Security (IPSec) and Layer 2 Tunneling Protocol (L2TP). IPSec 
is an open standards communications framework that supports network-level peer and origin 
authentication, data integrity and confi dentiality, and packet replay protection. L2TP is used to 

274 ‚óæ Information Security Management Handbook
create Virtual Private Network (VPN) connectivity between two hosts and IPSec is used to create 
a secure (authenticated and encrypted) communications channel.
Security Support Provider Interface
Th e Security Support Provider Interface (SSPI) is an interface to the Windows security system 
enabling Windows applications to create secure communications channels. Protocols available 
include NTLM, Kerberos v5, digest authentication (using MD5), and SSL. A .NET application 
can also use SSPI for encryption and authentication for TCP channels that it creates between hosts 
over a network or between processes on the same machine. SSPI is the Microsoft implementation 
of the Generic Security Service API (GSSAPI) standard documented in RFCs 1508 and 1509.
Cryptography
Cryptography is an essential technology for supporting the confi dentiality and integrity of data, 
authorization and non-repudiation of subjects, and access control of objects. Th e .NET Framework 
enables applications to use many of the standardized cryptographic algorithms commonly used 
for secure data storage and network communications. Also supported is the ability to extend the 
.NET Framework with custom third-party and developer-created cryptographic features.
Microsoft Cryptographic Application Programming Interface
Most cryptographic services are not implemented within the .NET Framework itself, but instead 
reside within the Windows operating system. Th e Microsoft Cryptographic Application Programming 
Interface (CAPI, formerly known as the Microsoft CryptoAPI) is a Windows API use for providing 
cryptographic services to the Windows operating system and applications. Th e .NET Framework 
provides a managed interface to CAPI services for use by .NET applications.
CAPI includes symmetric and asymmetric encryption algorithms, cryptographic key genera-
tion, message digests, digital signing, and random number generation. Also provided is support 
for network protocols that used cryptography, including SSL, TLS, and IPSec. Cryptographic 
devices, such as Smart Card interfaces used to provide single sign-on capability to a Windows 
workstation or application, are also supported.
Cryptographic Service Providers
Th e .NET Framework provides applications access to CAPI cryptographic services through the 
use of Cryptographic Service Providers (CSP). CSPs are the implementation of the cryptographic 
services installed and used on Windows, and are what FIPS publications refer to as cryptographic 
modules. Th e .NET Framework provides an interface to CSPs and enables .NET applications to 
access CAPI services using managed code.
Changes in CAPI
Th e features in CAPI itself are improved both in new releases of Windows and through peri-
odic updates. Th e actual cryptographic services available to a .NET application will depend on 
the release of the .NET Framework the application uses and the release and patch level of the 

Security in the .NET Framework ‚óæ 275
Windows system the application is running on. For example, Windows Vista includes compliance 
with FIPS 140-1 Level 1 for cryptographic module support, but compliance with the same stan-
dard was not available in Windows XP until the release of Service Pack 3.
Windows Vista also introduced an update to CAPI know as Cryptography API: Next Generation 
(CNG). CNG provides improvements for legacy CAPI cryptographic algorithms and supports a 
number of new algorithms from NSA Suite B, including Elliptic curve cryptography (ECC), Elliptic 
Curve Digital Signature Algorithm (ECDSA), and Elliptic Curve Diffi  e‚ÄìHellman (ECDH). Service 
packs and other updates to Windows also provide bug fi xes and security features to CAPI.
Encryption and Decryption
Th rough CAPI, the .NET Framework supports the use of several standard algorithms capable of 
encrypting and decrypting data in a fi le stream, memory stream, or network stream. Symmetric 
algorithms supported include RC2, RC4, DES, Triple DES, AES, and Rijndael. Asymmetric 
algorithms supported include RSA, DSA, and Diffi  e‚ÄìHellman (key agreement). Th e actual algo-
rithms available to any .NET application on any Windows system depends upon the release of the 
.NET Framework and the CSPs installed.
Cryptographic Hashing
Cryptographic hashing is an integrity-checking mechanism that allows a unique, fi xed-sized value 
(the message digest) to be generated from a corresponding collection of digital data, such as an 
e-mail message or image fi le. Th e .NET Framework supports the use of many standardized cryp-
tographic hashing algorithms, including MD2, MD4, MD5, SHA-1, SHA-256, SHA-384, SHA-
512, and RIPEMD-160.
Cryptographic hashing algorithms that also use a secret key in combination with the digital data 
input are known as Hash-based Message Authentication Codes (HMAC), or simply as keyed hashes. 
Using a secret key, HMAC algorithms can verify both the integrity and the authenticity of digi-
tal data. HMAC algorithms supported by the .NET Framework include HMAC-MD5, HMAC-
SHA-1, HMAC-SHA-256, HMAC-SHA-384, HMAC-SHA-512, and HMAC-RIPEMD-160.
All cryptographic hashing algorithms supported by the .NET Framework are available to all 
.NET applications, and are used by many other security protocols supported by .NET, including 
SSL, TLS, and IPSec. Th e .NET Framework also supports the ability of developers to create their 
own cryptographic hashing and keyed hash algorithms.
Digital Signing
Digital signing is a primary method for verifying the integrity and authenticating the sender of 
digital information. A digital signature is a combination of cryptographic hashing (message digest) 
and public key (asymmetric) cryptography. Th e .NET Framework supports digital signing using 
the Digital Signature Algorithm (DSA) as described by FIPS 186-2 Digital Signature Standard 
(DSS), and Elliptic Curve DSA (ECDSA) described in ANSI X9.62-2005.
Public-Key Cryptography Standards
Public-Key Cryptography Standards (PKCS) is a collection of cryptography standards created by 
RSA security for promoting and expediting the implementation of systems that use public key 

276 ‚óæ Information Security Management Handbook
cryptography. Th e .NET Framework supports several PKCS key types, including those used for 
creating public/private key pairs and the Digital Signature Algorithm (PKCS #1), Diffi  e‚ÄìHellman 
key exchange algorithm (PKCS #3), signing and encrypting messages for PKI (PKCS #7), 
cryptographic tokens for Smart Cards and single sign-on (PKCS #11), and using the Personal 
Information Exchange Syntax Standard (PKCS #12).
Random Number Generation
A Random Number Generator (RNG) is an implementation of an algorithm used for creating sequences 
of random, non-repeating values. An RNG is considered strong if there is an extremely small chance 
that a duplicate set of values will be produced by multiple calls to the RNG. Cryptographic algo-
rithms are used to ensure the strength (or quality) of random numbers produced by a RNG.
CAPI implements an RNG that provides a cryptographically secure pseudo-random number 
generator (CSPRNG). (Th e term pseudo is used to indicate that the sequence of values generated 
is not truly random, and is instead only an approximation of random values.) CSPRNG is suit-
able for creating cryptographic key streams and initialization vectors for stream and block ciphers, 
and for generating passwords, salts, nonces, and one-time pads. CSPRNG is based on the RNG 
specifi ed in the FIPS 186-2 standard for Windows 2000, XP, and Vista, or on NIST SP 800-90 
for Windows Vista with Service Pack 1 and Windows Server 2008.
Data Protection API
Th e .NET Framework also provides managed access to other security APIs not associated with 
CAPI. Th e Data Protection API (DPAPI) is an operating system service and a legacy API, available 
since Windows 2000, and used to protect data such as passwords, encryption keys, and database 
connection strings using encryption. Th e main advantage of using DPAPI is that it enables Windows 
applications to use data encryption without the need of managing secret encryption keys.
.NET Framework Security Issues
Th e .NET Framework itself is software, and, therefore, .NET and all the applications built using 
it are subject to the same possible vulnerabilities and exploits as any other software installed on 
the Windows platform. Misconfi guration, excessive security permissions, design and coding 
errors, and fl aws in legacy Windows components can all possibly cause security-related vulner-
abilities. Even certain aspects of the design of the .NET Framework itself leaves it open to possible 
attacks.
A search through the vulnerability advisories at Secunia.com shows past releases of .NET to have 
a variety of security-related problems, including boundary errors aiding the execution of arbitrary 
code, buff er overfl ows resulting in denial of service, and invalid data allowing the bypassing of secu-
rity restrictions. Th ese types of vulnerabilities do not only exist in .NET, but are also found in many 
unmanaged Windows components (DCOM, GDI+, WMI, etc.) called by the .NET Framework.
Vulnerabilities also result from misconfi guration of security-related features when software is 
written, built, installed, or run. Developers may make mistakes in designing and implementing 
code that results in security vulnerability that the .NET Framework cannot possible compensate 
for. Not understanding the need for encryption, authentication, restricted code access, or requir-
ing an application to be run with unnecessarily elevated privileges are all security design issues.

Security in the .NET Framework ‚óæ 277
.NET applications themselves can aide in their own attack. If .NET assemblies are distributed 
as unobfuscated CIL code, or with debugging information included, they will be much easier to 
reverse-engineer and possibly alter for malicious purposes. Application diagnostic messages can 
contain information that can be very useful to attackers, such as physical path names and compo-
nent version numbers. Applications that are designed or confi gured or provide verbose diagnostics 
logging‚Äîperhaps written to the Windows event log or a log fi le‚Äîmay be leaking information 
aiding in their own misuse.
Th e possibility of yet undiscovered security-related problems in the .NET Framework stresses 
the need to update Windows machines with the latest services packs, hotfi xes, and monthly 
Microsoft updates as they become available. Both system administrators and application develop-
ers are responsible for testing applications for proper and secure behavior before any new system or 
application updates are distributed into a production environment.
Conclusion
Th e .NET Framework is a vast and feature-packed platform for the creation of the smallest pro-
grams to the largest software solutions. .NET provides transparent security to make the default 
operation of .NET applications secure, and provides services making the deliberate implementa-
tion of security-related features quick and easy.
However, using .NET for developing applications is not a panacea for all Windows platform 
security issues. Although the .NET Framework assists developers in writing secure application, 
it can do very little to ensure that developers always follow best practices for write secure code, 
including the appropriate security mechanisms in the application‚Äôs design, and that security is 
considered and implemented at every stage of the development process.
Many aspects of application security are outside of the scope of the .NET Framework, includ-
ing risk mitigation, threat modeling, following best practices, and writing secure code. Design 
decisions, such as not to use data encryption or user authentication, are not the concern of .NET. 
Th ere is also very little .NET can do about an administrator‚Äôs choice to allow the use of excessive 
permissions, weak passwords, or revoked certifi cates. Th e .NET Framework is software platform 
used for helping developers create secure Windows applications, and not an iron hand of security 
that prevents developers from creating and running insecure applications.
About the Author
James D. Murray, GSEC, CISSP-ISSMP, CISA, CISM, is the information security offi  cial for the Offi  ce 
of MMIS Services in the NC Department of Health and Human Services (DHHS), and information 
security lead within the NC Medicaid Management Information System development project.


DOMAIN
 
5
CRYPTOGRAPHY
Crypto Concepts, 
Methodologies, and Practices


281
19
Chapter 
Cryptography: 
A Unifying Principle in 
Compliance Programs
Ralph Spencer Poore
Cryptography: A Unifying Principle in Compliance Programs
Compliance programs generally require one or more of the following attributes: confi dentiality, 
integrity, and assurance of the identities of entities involved. Cryptographic measures are uniquely 
qualifi ed to implement these attributes. Th is chapter describes how the tools of cryptography can 
act as a unifying principle in compliance programs.
Compliance Regimes
Compliance with regulatory, quasi-regulatory, and contract requirements and industry standards fi rst 
requires an understanding of what applies to your enterprise. In an ideal world, your legal depart-
ment would know this and advise the enterprise accordingly. In practice, however, this is almost 
Contents
Cryptography: A Unifying Principle in Compliance Programs .................................................281
Compliance Regimes ...........................................................................................................281
Privacy ........................................................................................................................... 282
Integrity ......................................................................................................................... 282
Authentication ............................................................................................................... 282
Cryptographic Key Management ........................................................................................ 282
Unifying Cryptographic Compliance .................................................................................. 285
Summary ................................................................................................................................. 287
About the Author .................................................................................................................... 287
Further Resources .................................................................................................................... 287

282 ‚óæ Information Security Management Handbook
never the case. Th e vastness of legal jurisdictions, specialties, and industry standards overwhelms 
most corporate legal departments. Th ey, in turn, depend on compliance offi  cers, department man-
agement, corporate management, internal audit, and specialty areas (e.g., information security, HR, 
and IT) to provide them with specifi c compliance program information that they can research.
Although the spectrum of compliance programs is vast, the chapter focuses on two areas: 
protection of privacy and of identity. Privacy is clearly important in many industries including 
health care and fi nancial services‚Äîtwo heavily regulated industries with mandatory compliance 
regimes. Th e ability to identify authoritatively is essential to both commercial and governmental 
transactions. Identity theft is a major and escalating problem.
Table 19.1 lists examples of cryptography-related compliance requirements, i.e., sources of 
rules that either require cryptography explicitly or have an implicit and practical requirement for 
cryptography. Note that several states (e.g., California, Massachusetts, and Nevada) have laws that 
provide their citizens protection regardless of where the data actually resides.
Once an enterprise understands the compliance programs it must put in place, the enterprise 
should take a unifi ed approach to policy, standards, procedures, and technology. Th is can save 
time and money. Th e old 80/20 rule applies: 80% is common to almost all compliance programs 
and 20% is unique. Th is also applies to cryptographic security measures.
Privacy
In support of privacy requirements, encryption can limit constructive access to information. Th is 
is an essential tool in protecting information in transit. It is also a useful tool in protecting stored 
information on removable media (including laptop computers). Many compliance regimes have 
privacy as one required element. Health Insurance Portability and Accountability Act (HIPAA), 
for example, has an entire body of regulations on privacy. Th e European Union and its member 
countries also have extensive privacy rules.
Integrity
Cryptography can provide message or transaction integrity. In business transactions, the integrity 
of the transaction is often more important than its secrecy. Th e use of digital signatures, message 
authentication codes (MAC), and similar cryptography-based error-detection mechanisms can 
reduce fraud by preventing undetected changes to messages. Th e integrity of underlying account-
ing transactions, for example, is an important element of Sarbanes‚ÄìOxley (SOX).
Authentication
Authenticating the source, the destination, or the authority of a transaction is important in many 
compliance regimes. Cryptography can play an important role in accomplishing this. Th e binding 
of information to an identity may provide a relying party with a level of trust on which to base a 
business relationship. Access control depends on authentication of an identity. Password, security 
token, or biometric identity systems often use cryptography as an essential element.
Cryptographic Key Management
Cryptographic security measures depend on several factors including selecting the appropriate 
algorithms for the intended purpose, securely implementing the cryptography, and properly man-
aging the cryptographic key life cycle.

Cryptography: A Unifying Principle in Compliance Programs ‚óæ 283
Table 19.1 Cryptographic-Related Compliance Requirements
Law, Regulation, Standard
Requirement
Cryptography
Gramm‚ÄìLeach‚ÄìBliley Act 
(15 U.S.C. ¬ß¬ß 6801 et seq.)
Requires administrative, 
technical, and physical 
safeguards to maintain the 
security, confi dentiality, and 
integrity of the information
Implicit
Health Insurance Portability 
and Accountability Act (Pub. 
Law No. 104-191 ¬ß¬ß262,264: 45 
C.F.R. ¬ß¬ß160-164)
Regulations issued on both 
privacy and security
Implicit in privacy rules; 
explicit in security rules
Privacy Act (5 U.S.C. ¬ß552a)
Requires the establishment 
of appropriate 
administrative, technical, 
and physical safeguards to 
ensure the security and 
confi dentiality of records.
Implicit
Fair and Accurate Credit 
Transactions Act of 2003 
(FACT Act) (Public Law 
108-159)
Protection of account 
numbers and expiration 
dates on receipts
Explicit (but alternatives may 
be possible)
Sarbanes‚ÄìOxley Act of 2002 
(SOX) (Public Law 107-204)
Sec. 802 addresses altering 
documents by amending 18 
USC ¬ß1519
Implicit
OMB Memorandum 06-16 
Protection of Sensitive Agency 
Information (06-23-2006)
Provides a security checklist 
for use by federal agencies 
to protect personally 
identifi able information (PII) 
during transmission, storage, 
and remote access. The 
checklist contains fi ve 
mandatory and four 
conditional action items 
depending on whether PII is 
transported, stored, or 
accessed remotely
Explicit
OMB Memorandum 07-16 
Breach of Personally 
Identifi able Information (May 
22, 2007)
Required agencies to 
develop and implement a 
breach notifi cation policy 
and identifi ed three ‚Äúsimple 
and cost-effective steps‚Äù to 
reduce PII breach risks 
which included using 
encryption.
Explicit
(continued)

284 ‚óæ Information Security Management Handbook
Table 19.1 (continued) Cryptographic-Related Compliance Requirements
Law, Regulation, Standard
Requirement
Cryptography
California: Security Breach 
Notice‚ÄîCivil Code sections 
1798.29, 1798.82, and 1798.84. 
[SB 1386 (2003) & AB 1950 
(2004)]
SB 1386 states that any 
breach of the security of the 
data must be reported in the 
most expedient time 
possible following the 
discovery of the breach to 
any resident of California 
whose unencrypted 
personal information was, or 
is reasonably believed to 
have been, acquired by an 
unauthorized person. Theft 
of encrypted data is 
specifi cally exempted in SB 
1386 and AB 1950
Explicit
Massachusetts: Standards for 
the Protection of Personal 
Information of Residents of the 
Commonwealth (201 CMR 
17.00)
This law requires all portable 
personal data about any 
Massachusetts resident to be 
encrypted. The law applies to 
data transmitted over public 
networks and to data stored 
on a laptop or on any type of 
removable memory device
Explicit
Nevada: Restrictions on 
transfer of personal 
information through electronic 
transmission (NRS 597.970 
(January 10, 2008))
‚ÄúA business in this State shall 
not transfer any personal 
information of a customer 
through an electronic 
transmission other than a 
facsimile to a person outside 
of the secure system of the 
business unless the business 
uses encryption to ensure 
the security of electronic 
transmission.‚Äù
Explicit
Payment Card Industry (PCI) 
Data Security Standards 
(DSS), Version 1.2 (October 
2008)
Cryptography is specifi cally 
cited as a means of 
protecting sensitive 
information
Explicit (but alternatives may 
be possible)
PIN Security Compliance 
(‚ÄúTG-3‚Äù)
Required by interchange 
networks, e.g., NYCE¬Æ 
Payments Network, PULSE¬Æ 
Network, and STAR¬Æ Debit & 
ATM Network. This is all 
about encryption
Explicit

Cryptography: A Unifying Principle in Compliance Programs ‚óæ 285
As shown in Figure 19.1, organizations use cryptography often without intention. Th at is, off -
the-shelf products on which organizations rely use cryptography, and the defaults are rarely what 
management would choose if developing a cryptographic security policy. Having a documented 
policy on the application of cryptography to an organization‚Äôs compliance program is one step in 
unifying the otherwise disparate elements of compliance regimes.
Unifying Cryptographic Compliance
Th e process for developing a unifi ed cryptographic compliance program begins with collecting 
and creating documentation. Here are recommended steps:
 
1. Document an inventory of crypto-using products in your organization. Include information 
on the cryptographic functions supported, the business purpose for the product, and the 
following technical data:
 
a. Cryptographic algorithm or algorithms supported by the product
 
b. Key lengths used/supported by the product
 
c. Cryptographic testing criteria (if any) met by the product (e.g., FIPS 140, ISO/IEC 
15408, and X9.24)
 
2. Produce a network diagram that includes all paths that support some cryptographic security 
measure. Indicate protocol, form of cryptography used (e.g., symmetric key or asymmetric 
(‚Äúpublic‚Äù) key), and algorithm used (e.g., two-key triple-DES, AES, or RC4). Give the keys 
names that you can reference in your documentation.* Th is diagram is especially important 
Trade secrets
Remote access
Payments
Identity
Authentication
Access control
Secrecy
Privacy
Privacy
Data leakage
Payments
Payments
Privacy
Data integrity
Trade secrets
Data leakage
Payments
Privacy
Payments
Trade secrets
Data leakage
Identity
Payments
Government
Healthcare
Transportation
Sports
Manufacturing
Financial
services
Entertainment
Retail
Privacy
Privacy
7
9
B
3
1
E
F
9
9
2
 
4
5
b
c
6
3
2
D
3
F
2
7
C
F
B
A
B
3
E
D
1
8
8
1
C
E
2
A
6
7
9
9
B
E
2
2
D
6
1
8
4
D
0
0
3
B
2
1
1
A
5
6
C
7
7
8
2
3
C
C
3
2
A
R
F
I 
  
 I
R
  
  
E
M
I
/
C
  
  
S
S
L
  
  
V
P
N
  
  
I
P
  
  
S
E
C
  
  
X
M
L
  
  
A
E
S
  
  
R
S
A
  
  
E
C
C
  
  
B
C
/
D
R
  
  
S
O
A
  
  
P
K
I 
  
 
D
E
S
  
  
C
M
S
 
A
5
6
C
7
7
8
2
3
C
C
3
2
A
7
9
B
4
5
A
B
3
E
D
1
8
8
1
C
E
2
A
6
7
9
9
B
E
2
2
D
6
1
8
4
D
0
0
3
B
2
1
1
D
0
0
3
B
2
1
1
 
4
5
A
B
3
E
D
1
8
8
1
C
E
2
A
6
7
9
9
B
E
2
2
D
6
1
8
4
7
9
9
B
E
4
5
A
B
3
E
D
1
8
8
1
C
E
2
A
6
Figure 19.1 Business uses of cryptography depicts the many forms and uses of cryptography in 
various industries. (Courtesy of Cryptographic Assurance Services LLC, Arlington, TX, 2009.)

286 ‚óæ Information Security Management Handbook
for compliance regimes that allow network segmentation as a scope-narrowing principle (e.g., 
PCI DSS). Some compliance regimes require a network diagram as part of their compliance 
reporting (e.g., PIN Security Compliance‚Äîcommonly referred to as a TG-3 Assessment).
 
3. Identify cryptography that is done in cryptographic hardware (e.g., host security module 
[HSM], tamper-resistant security module [TRSM], or physically secure device [PSD]). For 
some compliance regimes such devices are required.
 
4. Identify cryptography that is done in software. Determine how the cryptographic keys are 
generated, stored, and managed. Th is step will often identify weaknesses that may require 
remediation.
 
5. Collect copies of all documented procedures associated with cryptography in your organization. 
Th is may include vendor manuals, local processes, computerized scripts, logs, and control forms.
 
6. Determine which business processes fall under which compliance regimes. For those that 
require (or benefi t from) cryptography determine if the cryptography in place addresses the 
requirement.
 
7. Ensure that compliance documentation refl ects the applicable contribution made by 
cryptography.
Operational processes associated with the cryptographic key life cycle are also unifying principles. 
In accordance with ISO standards (ISO 11568-1 and ISO 11568-4) a cryptographic key life cycle 
consists of the stages shown in Figure 19.2. Th ose stages are briefl y described here.
Generation: Key generation involves the creation of a new key for subsequent use.
Storage: Key storage involves the holding of a key in one of the permissible forms.
Backup: Key backup occurs when a protected copy of a key is kept in storage during its opera-
tional use for potential recovery.
Distribution and loading: Key distribution and loading is the process by which a key is manu-
ally or electronically transferred into a secure cryptographic device.
Archival
Destruction
Replacement
Use
Generation
Storage
Backup
Distribution/
loading
Termination
Deletion
Figure 19.2 Cryptographic key management life cycle. (Courtesy of Cryptographic Assurance 
Services LLC, Arlington, TX, 2009.)

Cryptography: A Unifying Principle in Compliance Programs ‚óæ 287
Use: Key use occurs when a key is employed for the cryptographic purpose for which it was 
intended.
Replacement: Key replacement occurs when one key is substituted for another when the original 
key is known or suspected to be compromised or the end of its operational life is reached.
Destruction: Key destruction ensures that an instance of a key in one of the permissible key 
forms no longer exists at a specifi c location. Information may still exist at the location from 
which the key may be feasibly reconstructed for subsequent use.
Deletion: Key deletion is the process by which an unwanted key, and information from which 
the key may be reconstructed, is destroyed at its operational storage/use location. A key may 
be deleted from one location and continue to exist at another, e.g., for archival purposes.
Archive: Key archive is the process by which a key that is no longer in operational use at any 
location is stored. Archival of keys is usually for evidential purposes, i.e., when the busi-
ness has reason to believe that a future legal action may require the ability to reconstruct 
an historic transaction or prove its authenticity. Archived keys should have clear retention 
periods.
Termination: Key termination occurs when a key is no longer required for any purpose, and all 
copies of the key and information required to regenerate or reconstruct the key have been 
deleted (irrecoverably) from all locations wherever they existed.
By having a uniform set of policies and procedures for cryptographic key management, the orga-
nization saves time and money in development, training, and compliance assessment, and often 
through standardization that permits fewer products and associated maintenance.
Summary
Cryptography is pervasive. Th e use of cryptographic security measures underlies many compliance 
regimes. Intentional, documented use of cryptography can provide the foundation for unifi cation 
of compliance programs. Unifi ed compliance programs save time and money, reduce complexity 
and increase reliability.
About the Author
Ralph Spencer Poore, CFE, CISA, CISSP, CHS-III, CTGA, QSA, is the chief cryptologist for 
Cryptographic Assurance Services LLC, Arlington, Texas.
Further Resources
ANSI & ISO Standards, webstore.ansi.org
Compliance White Papers, it.toolbox.com/vendors/white-papers/view-results/?term=compliance
Cryptographic White Papers, www.cryptographicassuranceservices.com; www.cryptography.com/
resources/whitepapers/index.html
NIST Cryptographic Materials, csrc.nist.gov
Privacy White Papers, epic.org/


DOMAIN
 
6
SECURITY ARCHITECTURE 
AND DESIGN
Principles of Computer and 
Network Organizations, 
Architectures, and Designs


291
20
Chapter 
Best Practices 
in Virtualization Security
Shanit Gupta
Contents
Main Components ...................................................................................................................293
ESX Host ............................................................................................................................293
Patches ........................................................................................................................... 294
Server Management........................................................................................................ 294
Security Tools ................................................................................................................. 294
Hardening Steps for ESX Host ................................................................................................ 295
Controlled Root Access ....................................................................................................... 295
Strong Password Policy ....................................................................................................... 295
Password Aging ................................................................................................................... 295
Password Complexity and Lock Out ................................................................................... 296
Authorization Controls for Privileged Operations ............................................................... 297
Substitute User Privileges ............................................................................................... 297
Perform Actions as Super User ....................................................................................... 297
Active Directory Integration ............................................................................................... 297
Disable Unnecessary Services on the Host ........................................................................... 298
Secure SNMP Confi guration .............................................................................................. 299
Confi gure Proper Logging .................................................................................................. 300
Log File Size ................................................................................................................... 300
Remote Logging ............................................................................................................. 300
Time Synchronization .....................................................................................................301
Maintain File System Integrity .............................................................................................301
Disable Auto Mount on for USB Devices ........................................................................... 302
Confi guring ESX Host ............................................................................................................ 303
Network Isolation ............................................................................................................... 303
Management .................................................................................................................. 303

292 ‚óæ Information Security Management Handbook
Operational Networks .................................................................................................... 303
ESX Virtual Switch Tagging (VST Mode) ...................................................................... 304
Layer 2 Security Settings ..................................................................................................... 306
MAC Address Changes .................................................................................................. 306
Forged Transmissions ..................................................................................................... 306
Promiscuous Mode ......................................................................................................... 306
Boot Time Passwords ..................................................................................................... 307
Require Certifi cate Validation ............................................................................................. 308
Replace Certifi cate on ESX ............................................................................................ 308
Replace Certifi cate on VirtualCenter .............................................................................. 308
Enable Certifi cate Checking on the VirtualCenter ......................................................... 309
Re-Encrypt the Database Password ................................................................................. 309
Confi gure Virtual Infrastructure Clients to Verify Certifi cates .........................................310
Storage Security ........................................................................................................................310
Zoning ................................................................................................................................310
Logical Units Masking .........................................................................................................311
Raw Device Mapping ..........................................................................................................311
Secure iSCSI Deployment ...................................................................................................311
Network Isolation of iSCSI .............................................................................................311
Require Challenge Handshake Authentication Protocol to Connect to iSCSI ......................312
Security Banner ........................................................................................................................312
ESX Host ............................................................................................................................312
Console and SSH Access ......................................................................................................312
VirtualCenter ...........................................................................................................................313
VirtualCenter Host Controls ...............................................................................................313
Implement Network Isolation ..............................................................................................313
Firewall ................................................................................................................................313
Disable Unnecessary Services ...............................................................................................314
Authentication and Access Control ......................................................................................314
Roles and Permissions ..........................................................................................................315
Database Security ................................................................................................................315
Logging and Monitoring ......................................................................................................316
Virtual Machine .......................................................................................................................316
Virtual versus Physical World...............................................................................................317
Use Templates ......................................................................................................................317
Loss of Virtual Machines .....................................................................................................318
Resource Overcommitment .................................................................................................318
Isolation Based on Trust Levels ............................................................................................318
Virtual Machine Isolation Settings .......................................................................................319
Disable Copy and Paste .......................................................................................................319
Limit Log File Size .............................................................................................................. 320
Limit VMX File Size ........................................................................................................... 320
Disable Unnecessary Devices .............................................................................................. 320
Other Confi guration Settings ..............................................................................................321
Tools for Virtualization ............................................................................................................321
Virtualization Malware ........................................................................................................321
Detecting Virtualization ..................................................................................................... 322

Best Practices in Virtualization Security ‚óæ 293
If you want to be on the cutting edge then be ready to bleed. Virtualization in many ways is on 
the cutting edge of technology. More importantly, it continues to transform our IT infra-
structure that was not possible using the physical environment. While the technology of 
virtualization is widely respected, the ability to secure a virtualization infrastructure is pas-
sionately debated.
I am of the opinion that virtualization by itself does not make the system more or less secure. 
Th e risks that we see on a physical environment manifest themselves a little diff erently in a virtual-
ized environment. However, it is important to note that virtualization introduces new layers. Each 
layer increases the attack surface that can result in attack and compromise. Th e enterprise-wide 
adoption of the technology is fairly recent, which increases the fear of the unknown. In this chapter, 
we go through some of the tactical and strategic steps that can be taken to better secure the vir-
tualization infrastructure.
Main Components
VirtualCenter
 
‚óæ
Virtual machine
 
‚óæ
ESX Host
 
‚óæ
Data storage
 
‚óæ
Policy and compliance thoughts
 
‚óæ
Virtualization security products
 
‚óæ
Strategic thoughts
 
‚óæ
ESX Host
Th e core of the ESX Host is the virtualization layer. Th is is a lightweight kernel that is capable of 
virtualizing the hardware and running multiple operating systems simultaneously on the same 
physical system. Th is virtualization layer is popularly known as the hypervisor. Th e hypervisor 
along with the other support modules make up the ESX Host. It is easy to see why the hypervi-
sor or in fact the ESX Host would be a valuable target. Once the Host is compromised, all virtual 
machines running on the Host can be compromised.
While it seems that the ESX Host would be a most likely target of attack and penetration, there 
are multiple design decisions that help bolster the security of the hypervisor. Th e most important 
Future of Virtualization Security ......................................................................................... 322
CPU and Memory Scanning ............................................................................................... 323
Network Monitoring .......................................................................................................... 323
Process Monitoring and Execution ...................................................................................... 323
Storage Scanning ................................................................................................................ 323
Agent-Free Security ............................................................................................................. 323
Comprehensive and Integrated Network Security ............................................................... 323
Security Bottlenecks Can Be Reduced ................................................................................. 323
Concerns about VMsafe ......................................................................................................324
About the Author .....................................................................................................................324
References ................................................................................................................................324

294 ‚óæ Information Security Management Handbook
consideration is to maintain the guest isolation. VMware embedded several design and implemen-
tation controls to ensure that one guest cannot compromise another guest of the Host in ways 
that would otherwise be not possible in a physical environment. Additionally, a small footprint 
and the highly optimized kernel that is designed specifi cally for virtualization limits the attack 
surface of the ESX Host. However, besides the hypervisor, we have several other components in 
the ESX Host that have a network interface attached to them and if compromised can jeopardize 
the integrity and confi dentiality of all virtual machines on the ESX Host.
ESX Host‚Äôs service console is developed from the base image of Red Hat Enterprise Linux 
3 Server, Update 6. While the service console feels like a Linux system, it is heavily custom-
ized and lightweight. It, therefore, should not be treated like a regular Linux console. Many 
Red-Hat Package Manager (RPM) packages that would otherwise be available on a Linux system 
will not be available on the ESX service console. Even if they are available on ESX Host 
the administrator cannot assume them to function like regular RPM packages because they 
may be customized to meet the hypervisor requirements. Considering the factors mentioned 
above, the administrator should pay special attention while deploying any tools that have not 
specifi cally been developed for ESX Host.
Patches
Do not apply patches released by Red Hat on ESX Host. While this may seem like a contradic-
tion to security best practices, Red Hat patches may not be compatible with your ESX Host 
and should, therefore, not be applied on the same. An administrator should subscribe to security 
notifi cations issues by VMware. Th ese notifi cations can be signed up at http://www.vmware.com/
security under ‚ÄúSign-up for Security Notifi cations.‚Äù
Server Management
Th e redhat-config-* commands that are used to manage a Red Hat system will not be avail-
able on the ESX Host. Similarly, there is no graphical interface like the X server present. Most 
confi guration and server management will need to be performed using vmfstools and esxcfg-* 
commands present on the ESX Host. If the user prefers a user interface, consider the use of VI Client 
or the VirtualCenter for confi guring the ESX Host. While many confi gurations can be performed 
using these tools, the administrator will need to use the service console for some advanced confi gura-
tions and hardening.
Security Tools
Many vulnerability scanners and security analyzers are not designed to work with the ESX Host 
and the hypervisor. In most cases, these analyzers will detect the ESX Host to be a Linux server 
and identify vulnerabilities that are not applicable for ESX Host. Th ey may also off er steps and 
procedures to remediate the vulnerability. It is not recommended to follow these steps as they may 
not apply for ESX. Unless the scanners are designed for ESX Host, it is not recommended to use 
them to analyze the ESX Host.
If the administrator follows the security best practices detailed in the hardening guide and/or 
in sections below, it will not be necessary to deploy host-based security tools like antivirus, IDS, 
or IPS. Unless they are designed specifi cally to work with the ESX Host, it is recommended they 
are not deployed on the ESX system.

Best Practices in Virtualization Security ‚óæ 295
Hardening Steps for ESX Host
Th is section details some of the confi guration best practices that we can follow to better pro-
tect the ESX Host and thereby the virtual machines running on it. We will assume the use of 
VirtualCenter for managing the ESX Host. Most confi gurations mentioned below can be per-
formed using VirtualCenter. Some may need the administrator to have Console access or SSH 
access to the ESX Host.
Controlled Root Access
Limiting root user access is the fi rst and most important step toward securing ESX Host. Th e root 
user controls the ESX Host, which in turn controls every other VM. It is, therefore, easy to under-
stand the signifi cance of this account.
By default, remote access to root account using SSH is disabled. If for any reason remote access 
for root is enabled, disable the root login by modifying the /etc/ssh/sshd _ config fi le and 
change the line from PermitRootLogin yes to PermitRootLogin no.
Note: Before performing this step, ensure that you integrated the ESX Host with active direc-
tory or have created a lower privilege user account on the system that can remote login the system. 
If remote access for root is disabled and no other accounts are confi gured, then you will not be able 
to remote access the server.
We also need to confi gure the sudo and/or su privileges for the regular user to be able to per-
form some privileged operations. Th e use of sudo provides better accountability of user actions as 
all sudo operations are logged. In contrast, if su is used, then only the fact that the user elevated 
the privileges will be logged.
Strong Password Policy
Th e basic principle of strong password policy is important even with ESX. Th e two biggest factors 
of a strong password policy are
 
1. Password aging
 
2. Password complexity and lockout
Password Aging
Th e steps to change the default password aging restrictions on ESX are
Step 1: Log on to the service console and gain privileged access (root or equivalent) on the Host
Step 2: Make the change using the following commands
To change the maximum number of days a user can keep a password:
 
‚óæ
esxcfg-auth ‚Äì‚Äìpassmaxdays = <number _ of _ days>
 
‚àí
where 
 
‚àí
<number _ of _ days> is the maximum number of days before password 
expiration.
To change the minimum number of days between password changes:
 
‚óæ
esxcfg-auth ‚Äì‚Äìpassmindays = <number _ of _ days>
 
‚àí
where 
 
‚àí
<number _ of _ days> is the minimum number of days between password 
changes.

296 ‚óæ Information Security Management Handbook
To change the warning time before a password change:
 
‚óæ
esxcfg-auth ‚Äì‚Äìpasswarnage = <number _ of _ days>
 
‚àí
where 
 
‚àí
<number _ of _ days> is the number of days of advanced warning a user 
receives before a password change is due.
Password Complexity and Lock Out
Th e ESX server uses the pam _ cracklib.so plug-in to set the rules that users must observe 
when creating passwords and to check password strength during the creation process. Th e default 
rules provided by pam _ cracklib.so plug-in may not be enough for some high-risk environ-
ments. In such cases, the administrator can confi gure the environment and set custom rules.
To change the password complexity requirement, the privileged user can enter
esxcfg-auth ‚Äì‚Äìusecrack=<retries> <minimum _ length> <lc _ credit> 
<uc _ credit> <d _ credit> <oc _ credit>
where
<retries> is the number of retries the user is allowed before ESX Server locks them out of 
password change mode.
minimum _ length is the minimum number of characters a user must enter to make the 
password acceptable. Th is number is the total length before any length credits are applied.
One length credit is always applied so, in eff ect, the password length is one character less than the 
minimum _ length parameter you specify. Because the pam _ cracklib.so plug-in does not 
accept passwords of fewer than six characters, calculate the minimum _ length parameter so that 
users cannot drop the password length below six as a result of subtracting the length credits.
lc _ credit is the number by which the minimum _ length parameter is reduced if 
the user includes at least one lowercase character in the password.
uc _ credit is the number by which the minimum _ length parameter is reduced if the 
user includes at least one uppercase character.
d _ credit is the number by which the minimum _ length parameter is reduced if 
the user includes at least one digit.
oc _ credit is the number by which the minimum _ length parameter is reduced if the 
user includes at least one special character, such as an underscore or dash.
For further protection, you can enforce account lockout after too many unsuccessful login 
attempts. To confi gure the ESX service console to disable the account after three unsuccessful 
login attempts, add the following lines to /etc/pam.d/system-auth:
auth required /lib/security/pam _ tally.so no _ magic _ root
account required /lib/security/pam _ tally.so deny=3
no _ magic _ root
To create the fi le for logging failed login attempts, execute the following commands:
touch /var/log/faillog
chown root:root /var/log/faillog
chmod 600 /var/log/faillog
If the pam _ cracklib.so plug-in does not provide suffi  cient password strength enforcer 
for your environment, then you can consider the use of pam _ passwdqc.so plug-in. All avail-
able options for the plug-in can be viewed by man pam_passwdqc.

Best Practices in Virtualization Security ‚óæ 297
Authorization Controls for Privileged Operations
Substitute User Privileges
Th e ability to become the super user is a powerful feature and should, therefore, be closely guarded. 
Once a user elevates his privileges to super user privileges, then the commands executed by the 
user may not be logged and, therefore, go unidentifi ed. Only the members of the wheel group 
should be allowed to become root after having successfully authenticated to the system. Th is can 
be achieved by editing the /etc/pam.d/su fi le. Uncomment the line that reads
auth required /lib/security/$ISA/pam _ wheel.so use _ uid
Th is confi guration setting will only allow members of wheel group to change their eff ective 
user identifi er to root. Only allow very limited users to be part of the wheel group.
Perform Actions as Super User
Th e use of sudo is a safer way to grant administrators access to privileges commands without hav-
ing to give them root access or even making them part of the trusted group. Th e permissions for 
users of diff erent groups and the commands they can execute can be controlled by editing the 
/etc/sudoers fi le. Th e list of commands and groups depend on the environment and the user 
groups created. Security best practices to consider while confi guring the sudo users are
Create a special group and only allow members of that group to use sudo.
 
‚óæ
Do NOT permit users to execute su using sudo.
 
‚óæ
Use aliases to confi gure user roles and apply necessary authorization control. It is easier to 
 
‚óæ
add users to aliases than to add and remove them from every sudo command specifi cation.
Use the global authentication scheme that is confi gured using the esxcfg-auth command. 
 
‚óæ
Th is is set by using service=system-auth as the authentication means in /etc/pam.d/
sudo file.
Require user to enter their password while performing sudo operations.
 
‚óæ
Enable extended logging for all sudo operations performed by the user.
 
‚óæ
Active Directory Integration
Many organizations tend to use the Active Directory for central user management and adminis-
tration. ESX Host includes services that can integrate with the existing directory services.
Th ere are several advantages of integrating ESX Host with the directory services. Most signifi -
cant of them include the following:
 
1. Large enterprises tend to have an established environment with many users and creating 
trust with an existing authentication authority is easier than recreating all required users in 
ESX Host.
 
2. Reducing the number of authentication systems reduces the number of user names and 
passwords each user must remember. Users are more likely to follow recommended 
practices for creating strong passwords when they have fewer passwords to remember. 
Reusing any password, even a strong one, for multiple accounts eff ectively weakens the 
security of an authentication system. Th is can be avoided by using a centralized user 
management system.

298 ‚óæ Information Security Management Handbook
Th e esxcfg-auth tool makes authentication with Active Directory possible by confi guring a plug-
gable authentication module (PAM) and modifying the ESX Server system‚Äôs confi guration.
To accomplish directory services integration, execute the following commands:
Step 1:
# esxcfg-auth ‚Äì‚Äìenablead ‚Äì‚Äìaddomain=DOMAINAME.com ‚Äì‚Äìaddc=DC1.
DOMAINAME.com
Th is confi gures the global authentication scheme to use active directory‚Äìbased authentication 
for the domain DOMAINAME.com with the domain controller DC1.DOMAINAME.com.
Step 2:
# useradd UserName1
Th is will create a user with user name UserName1 with permission to use the service console.
Disable Unnecessary Services on the Host
Services increase the footprint of any software especially the services with network access. Every 
open port in the system increases the attack surface thereby increasing the potential for miscon-
fi guration and compromise. By default, ESX Host limits the number of services and does not start 
any clear text-based communication channels. Th e default fi rewall setting only allows a list of 
known services to use the associated incoming and outgoing ports.
Th e list of services started by default on the ESX server are listed in the screen shot below.

Best Practices in Virtualization Security ‚óæ 299
All other incoming and outgoing ports are explicitly blocked by the fi rewall on the ESX 
system. If any ports need to be opened on the ESX system, they can be opened using the com-
mand esxcfg-fi rewall ‚Äìo <port,tcp|udp,in|out,name>
Th e table below lists the services and the ports that can be blocked provided the condition 
specifi ed is fulfi lled.
Service Name
Identifi cation in 
esxcfg-fi rewall 
Command
Port Number
Traffi c Type
Disable Condition
CIM service 
location 
protocol
CIMSLP
427
Incoming and 
outgoing UDP 
and TCP
If not using CIM-
based software for 
monitoring or 
management
NFS client
nfsClient
111, 2049
Outgoing TCP 
and UDP
If not mounting 
NFS-based storage in 
the service console
VMware 
consolidate 
backup
VCB
443, 902
Outgoing TCP
If not using VCB for 
backup
CIM over HTTP
CIMHTttpServer
5988
Incoming TCP
If not using CIM-
based software for 
monitoring or 
management
CIM over 
HTTPS
CIMHttpServer
5989
Incoming TCP
If not using CIM-
based software for 
monitoring or 
management
Licensing
LicenseClient
2700, 27010
Outgoing TCP
If using only host-
based licensing
SSH server
sshServer
22
Incoming TCP
If all management is 
done via 
VirtualCenter, VI 
Client, or through 
third party agents
VirtualCenter 
agent
vpxHeartBeats
902
Outgoing UDP
If not managed by 
VirtualCenter
Secure SNMP Confi guration
SNMP agent on ESX Host can allow remote management tools to monitor status information on 
many critical operations. Further, it is possible to obtain confi guration information about the virtual 
machines and the state of hardware components like CPU, Network, Disk, and other failures. 
While ESX supports SNMP version 1, 2c, and 3, it is recommended to only use SNMP version 3, 
which provides authentication and privacy of messages between the agent and the management 

300 ‚óæ Information Security Management Handbook
console. In the least, administrators should consider confi guring a non-default community string 
and restricting the hosts that are allowed to query.
Th e link http://net-snmp.sourceforge.net/tutorial/tutorial-5/demon/snmpd.html provides 
detailed steps to confi gure SNMP using the snmpconf script. Th e administrator can choose com-
munity names, specify the restricted host list, and also confi gure SNMP v3 using the script. Th e 
script is available on ESX 3.5.
Confi gure Proper Logging
System logs are a key resource in identifying technical and security issues. Th ey help reconstruct 
a chronological order of the actions performed on the system. Monitoring tools rely on logs 
collected on the system to identify any unusual or unsafe activity. It is, therefore, vital that the 
logging on ESX be set according to recommended best practices.
Log File Size
Log fi les should be restricted in size but at the same time confi gured to collect an adequate amount 
of information. If the log fi les are not restricted in size, then they may keep growing and eventually 
exhaust all available disk space thereby missing important log information and causing a denial-
of-service attack. If the log fi le size is too small, then important logs may get overwritten.
Th e key is, therefore, to choose a log fi le size that will not exhaust disk space and cannot be 
fi lled up easily thereby resulting in loss of information.
Th e settings for log fi les vmkernel, vmksummary, and vmkwarning can be confi gured in the 
directory /etc/logrotate.d/. It is recommended that the vmk* fi le size be raised from 200 k 
to 4096 k. Further enable compression on the log fi les. Th e duration to keep the logs may be 
governed by the corporate policy.
Similar setting can be made for /etc/logrotate.conf master log confi guration fi le. It 
also dictates the defaults for log fi le rotation, log fi le compression, and the duration for which older 
log fi les should be stored and maintained.
Remote Logging
Th e syslog daemon performs logging on the ESX system. By default, the log fi les are stored under 
/var/log directory. Syslog can be confi gured to send the logs to a remote logging server. Th e 
use of a dedicated logging server is recommended in a production environment. Th is reduces 
the likelihood of log compromise in the event that the ESX server is compromised or corrupted. 
Central management of the logs also makes it easier for a monitoring tool to gain a holistic view 
of the activities and identify malicious activities on the system and network.
Syslog behavior is controlled by the confi guration fi le /etc/syslog.conf. Th e logs can be 
sent to a remote log server by adding the name of the host where the log fi les should be sent, for 
example, to send the storage monitor-related messages to the server logserver.companyname.com 
add @<logserver.companyname.com> at the end of the line local4.*. Th is will look like local4.* 
@<logserver.companyname.com>
Similarly, the other entries in the syslog can be confi gured to send logs to the log server. An 
important point to keep in mind is that the log fi les are transmitted in clear to the log server and so 
the log server and the ESX server should be hosted on an isolated and restricted network to prevent 
the compromise of log information.

Best Practices in Virtualization Security ‚óæ 301
After the syslog.conf fi le confi guration is complete, the syslog daemon needs to be sent 
the HUP signal to reread the confi g fi le. Th e command to do the same is
#kill ‚ÄìSIGHUP ‚Äò/bin/cat /var/run/syslogd.pid‚Äô
Time Synchronization
To ensure that all events are recorded in the right order, it is important to keep time synchroniza-
tion between all devices. It is, therefore, best to use the same relative time source, and the source 
should be confi gured to an agreed upon time standard such as UTC. Th is confi guration makes 
it easier to correlate the user actions between diff erent log fi les. Th e time synchronization can be 
confi gured using an NTP system by following the instructions listed below:
 
1. Logon to the console and gain super user privileges
 
2. Make a copy of /etc/ntp.conf for backup
 
3. Add the following lines to the /etc/ntp.conf fi le to select NTP servers from a pool
 
 restrict 127.0.0.1
 
 restrict default kod nomodify notrap
 
 server 0.pool.ntp.org
 
 server 1.pool.ntp.org
 
 server 2.pool.ntp.org
 
4. Edit fi le /etc/ntp/step-tickers and add the following lines
 
 server 0.pool.ntp.org
 
 server 1.pool.ntp.org
 
 server 2.pool.ntp.org
 
5. Enable NTP access through the fi rewall
 
 esxcfg-fi rewall ‚ÄìenableService ntpClient
 
6. Restart the NTP service
 
 service ntpd restart
 
7. Confi gure the service to auto start
 
 chkconfi g ‚Äìlevel 345 ntpd on
 
8. Synchronize hardware clock to NTP synchronized local clock
 
 hwclock ‚Äìsystohc
Refer the VMware knowledge-based article for more details on NTP confi guration. http://
kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&extern
alId=1339.
An automated script to set up the NTP can be obtained at http://www.vmcolonel.net/?p=13
Maintain File System Integrity
Th ere are several important confi gurations and binary fi les on the ESX Host that are not intended 
to be changed very often. It may, therefore, be useful to have a fi le integrity verifi er to ensure that 
the fi les were not edited out of the change control cycle. Th e fi le checksum can be calculated at the 
end of confi guration changes. Th e results can be stored securely offl  ine. Periodically, the checksum 
can be verifi ed to ensure that the fi les have not been tampered with because of unauthorized access 
or changes. Many diff erent factors can be considered to calculate the checksum including the fi le 

302 ‚óæ Information Security Management Handbook
contents, size, location, and fi le permissions. Commercial products available for ESX can help 
implement this security measure.
Some key fi les and folders to consider for integrity checks are
/etc/fstab
 
‚óæ
/etc/group
 
‚óæ
/etc/grub.conf
 
‚óæ
/etc/host.conf
 
‚óæ
/etc/hosts
 
‚óæ
/etc/hosts.allow
 
‚óæ
/etc/hosts.deny
 
‚óæ
/etc/krb.conf
 
‚óæ
/etc/krb5.conf
 
‚óæ
/etc/krb.realms
 
‚óæ
/etc/logrotate.conf
 
‚óæ
/etc/logrotate.d/
 
‚óæ
/etc/login.defs
 
‚óæ
/etc/modules.conf
 
‚óæ
/etc/motd
 
‚óæ
/etc/nscd.conf
 
‚óæ
/etc/ntp
 
‚óæ
/etc/nsswitch.conf
 
‚óæ
/etc/ntp.conf
 
‚óæ
/etc/openldap/ldap.conf
 
‚óæ
/etc/pam.d/system-auth
 
‚óæ
/etc/passwd
 
‚óæ
/etc/profile
 
‚óæ
/etc/resolv.conf
 
‚óæ
/etc/securetty
 
‚óæ
/etc/ssh/sshd _ config
 
‚óæ
/etc/snmp
 
‚óæ
/etc/sudoers
 
‚óæ
/etc/shadow
 
‚óæ
/etc/vmware
 
‚óæ
‚ÄîMultiple confi guration and kernel fi les
Besides monitoring these fi les for integrity, it is also advisable to periodically backup these fi les. 
In the event that a compromise is detected, it will be easier to restore the system if system backup 
is available.
Disable Auto Mount on for USB Devices
ESX Host automatically mounts the USB devices that may be connected to the server. Th e driv-
ers are preloaded to detect and mount a USB device attached to ESX Host. Th is may allow a user 
with physical access to the ESX Host to connect a USB device and execute malicious code on the 
server. It is advisable to disable auto mount on USB devices. A privileged user will still be able to 
manually mount the USB device if it is connected to the server. Th is will prevent an attacker from 
attaching devices with malicious code.

Best Practices in Virtualization Security ‚óæ 303
To disable to the auto mount on USB devices, edit the /etc/modules.conf fi le and place 
a # symbol before the line usb-controller.
Confi guring ESX Host
Network Isolation
Systems attached to a network are at considerable higher risk of attack and compromise. Th e risk 
is increased if the systems are accessible to untrusted and potentially malicious users. Th e attacks 
do not always originate from external users. In many instances, internal users or even privileged 
administrators in the organization may pose a risk. It is, therefore, best to reduce the attack surface 
of the virtualized infrastructure.
Network isolation is perhaps one of the most important and eff ective ways to increase the secu-
rity of virtualization infrastructure. While it seems like a straightforward and intuitive solution, it 
is one of the most common misconfi gurations in the virtualized world. Much of it can be attrib-
uted to insuffi  cient planning and incorrect design of the virtual networks. In the next section, we 
look at some of the security best practices and procedures to isolate the virtual networks.
During the confi guration of ESX, an administrator has the option of creating a default virtual 
machine port. If this option is chosen, a virtual machine port group is created on the same network 
interface as the service console. It is easy to see how this can allow a malicious virtual machine 
to sniff  sensitive and often unencrypted (logs, VMotion) information belonging to the service 
console. It can also allow a virtual machine to launch man-in-the-middle attacks against the con-
sole and disrupt network operations. Considering these threats, it is recommended to isolate the 
network operations for the virtual infrastructure.
To begin with, we will start identifying the diff erent networks segments. Broadly, we can 
divide the networks into two segments.
Management
Th is network will be used to troubleshoot and confi gure the virtual infrastructure. Th e systems 
that need to be on the management segment are
 
1. ESX Hosts
 
2. VirtualCenter
 
3. Client tools like VI Client, custom, or third-party tools using VI SDK
 
4. Syslog server
 
5. Database server
 
6. Backup servers
 
7. Host monitoring systems
 
8. Any third-party management systems
 
9. Storage systems
Operational Networks
Th e operational network is used by the virtual machines running on the ESX Host(s) to perform 
their business goals. Th e systems on the operational networks will be

304 ‚óæ Information Security Management Handbook
 
1. Virtual machines
 
2. Security devices (IDS/IPS/DLP, etc.)
 
3. Other systems (physical or virtual)
It is important to note that not all management systems will need to be on the same network seg-
ment. Similarly, the trust relationship within systems on the operational network will also have 
multiple levels and diff erent categories. Th ey will, therefore, need to be grouped and confi gured in 
more granular segments based on the organization network.
Th e more important need is to isolate systems based on their roles and the trust relationship 
with other systems on the network. For instance, the traffi  c communicated between the ESX Host 
and the Syslog server is unencrypted and unauthenticated, which may allow a malicious system on 
the same network to compromise the integrity of the logs. Similarly, the virtual machine state that 
is transmitted from one ESX Host to the other during a VMotion process is unencrypted, which 
may allow man-in-the-middle attacks. Th ese systems, therefore, cannot be hosted on a network 
that is accessible by other untrusted and potentially malicious systems.
All systems that are not exclusively operated by the privileged administrators can be considered 
untrusted and should not share the network with managed hosts.
Network isolation can be achieved using one of the two diff erent ways:
 
1. VLAN (virtual local network) tagging for diff erent isolated networks
 
2. Physical segregation of networks using diff erent virtual switches and physical uplinks
It may be useful to note that not all experts trust the VLAN tagging to be secure and eff ective. While 
VLAN tagging by itself does not off er security, it is a good fi rst step to isolate the networks. With correct 
network confi guration, it is possible to isolate systems on diff erent trust levels on diff erent VLANs.
Th e other option of physical isolation provides a much more conceptually straightforward way 
to isolate networks. However, this comes at a much higher price of additional routers. Further, it 
requires multiple physical network interfaces as there is a one-to-one mapping between VLAN 
port and physical port.
ESX Virtual Switch Tagging (VST Mode)
In virtual switch tagging, multiple port groups are created on each virtual switch. Each port group 
created in a virtual switch can be assigned a unique VLAN ID. All systems connected to a particu-
lar port group with uniquely assigned VLAN IDs are isolated from systems on other port groups 
on the same or diff erent virtual switch. If two port groups in the same virtual switch are assigned 
the same VLAN ID, then the traffi  c between the 2 port groups is shared.
Virtual network adapters associated with virtual machines may then be confi gured to connect 
to these user-defi ned port groups. Th e virtual adapters connected using a user-defi ned port group 
inherit and abide by the policies defi ned within the port group.
A VLAN ID between the ranges of 1‚Äì4095 can be specifi ed. It is recommended to not use 
VLAN IDs of 1, 1001‚Äì1024, and 4095. Th ese VLAN IDs are reserved for default VLAN, Cisco 
VLANS, and virtual guest tagging mode. Th e use of VLAN ID 1 can cause a denial of service as 
ESX drops traffi  c with this VLAN ID. VLAN ID 4095 causes the port group to use trunk mode. 
Th is is useful in the event that the operating systems are installed with VLAN drivers to manage 
their own VLAN tags. While this is rare, there are some conceivable reasons for doing so. Most 
organizations do not use this feature and, therefore, the VLAN ID 4095 should not be used.

Best Practices in Virtualization Security ‚óæ 305
It is important to clearly label all port groups to avoid any misunderstanding and miscon-
fi guration of systems. Th is will decrease the likelihood of virtual machines being attached to 
unauthorized networks. It is also important to note that if VMotion is currently in use between 
diff erent Hosts, then the Hosts should have the same port group label. If the port group labels do 
not match, VMotion will fail.

306 ‚óæ Information Security Management Handbook
Layer 2 Security Settings
ESX provides 3 options to enforce Layer 2 (OSI Stack) security measures on the virtual machines. 
Th ey are
 
1. MAC address changes
 
2. Forged transmissions
 
3. Promiscuous mode
MAC Address Changes
When a virtual machine is instantiated, it can have one or more network interfaces. Each network 
interface is assigned a MAC address at the time of creation. Th ese MAC addresses can be assigned 
statically or dynamically by the administrator.
Th e operating system and the administrator typically have the ability to change the MAC 
address for any network interface on any operating system. When a new MAC is assigned, net-
work packets are stamped with the MAC address that is assigned to that network interface. Th is 
can allow a malicious virtual machine to masquerade as another system on the network and 
receive their network traffi  c.
By default, the ESX Host does not prevent a virtual machine to change its MAC address. ESX 
Host provides an option to reject these MAC address changes. If this option is set to ‚ÄúReject,‚Äù 
ESX will not allow the operating system to change the initial MAC address. If the virtual machine 
tries to change the MAC address, ESX will disable that virtual port on the virtual machine. Th e 
port will be re-enabled only when the virtual machine sets the MAC to the initial value.
Th ere are very few legitimate purposes for changing the MAC address once the system has 
been successfully deployed and is operational. It is, therefore, recommended to change the setting 
to ‚ÄúReject‚Äù to not allow the virtual machine to change its MAC Address.
Forged Transmissions
It is similar to MAC address changes in the sense that the virtual machine will send packets using 
a MAC address that was not initially assigned to it. As opposed to MAC address changes, the vir-
tual machine is not permanently changing its MAC. It is only sending out network packets with 
a forged MAC address to spoof the identity of a victim.
By default, the ESX Host does not prevent a virtual machine from sending packets with forged 
MAC addresses. Th is can allow systems to perform ARP spoofi ng and enable man-in-the-middle 
attacks against other systems. If the option is set to ‚ÄúReject,‚Äù the ESX network stack compares the 
MAC address stamped on a packet with a virtual machine‚Äôs actual MAC. If the addresses do not 
match, the packet is dropped.
Once again, there are very few scenarios where a system may need to send packets with a 
forged MAC address. Th ey should be treated as exceptions and by default all virtual switches 
(including the port groups) should be confi gured to reject forged transmissions.
Promiscuous Mode
Many network cards can be placed in the promiscuous mode. In a regular mode, the network card 
only accepts the packets destined to it and ignores all other packets. However, if the card is placed 
in a promiscuous mode, the network card accepts all network packets passing through the system. 

Best Practices in Virtualization Security ‚óæ 307
Th is will allow the system to sniff  traffi  c belonging to other systems. Some of this traffi  c may be 
sensitive and unencrypted. Th is will, therefore, allow the ‚Äúsniff er‚Äù to gain access to sensitive infor-
mation that he should otherwise not have access to.
By default, ESX Host is confi gured to reject a virtual adapter‚Äôs request to operate in promiscuous 
mode. Th is will prevent a virtual machine‚Äôs network adapter from gaining access to traffi  c that is not 
destined to it. While this setting can be changed, it is recommended to not change this setting.
Th ere are some legitimate uses for enabling promiscuous mode for monitoring systems, for 
example, the IDS system, tracking and troubleshooting system, or for debugging purposes. 
However, they are more of an exception. Th ese systems could be placed in a separate port group 
and the promiscuous mode can be enabled only for that port group while it is set to reject for all 
other switches and groups.
Boot Time Passwords
GRUB Password
Physical access to systems by an attacker can be damaging. Most organizations, therefore, take 
adequate measures to physically secure systems from intruders. However, not all attack scenarios 
can be prevented, especially when the data center access is shared with many other administrators 
or if a hosting service is in use. An attacker with physical access to the system can boot the system 
into single user mode and gain root access. It is, therefore, advisable to use a GRUB password to 
prevent an unauthorized user from booting into single user mode. Th e user will be prompted for 
this password if they try to boot the system into a single user mode. To set a GRUB password, 
follow the steps below:
Type grub at the command line. Ensure that you have root privileges.
 
‚óæ
 
 #grub
Type md5crypt to generate an MD5 hash of the password. Th is prevents clear text access to 
 
‚óæ
the password. When prompted for the Password, enter the password you want to use.
 
 grub> md5crypt
 
 Password: **********
 
 Encrypted: $1$Pdmiq$W7rfsJjSEfzuOzKzbsPx21
Encrypted: provides the MD5 hash of the password entered.
 
‚óæ
Add the line password -md5 <Hashed Password> to the fi le 
 
‚óæ
/boot/grub/grub.conf. 
Make sure that you copy the hash provided by md5crypt correctly.
Now if any user tries to edit the GRUB options, the user will be prompted for a password.
BIOS Password
Another way to compromise a system with physical access is to boot the system using a remov-
able device. An attacker can reboot the ESX system, change the BIOS settings, and make 
the fi rst boot device the removable media. He can then attach the removable media which 
has a Linux distribution with Rescue mode. Once the attacker boots in Rescue mode, he can 
mount the ESX fi le system and change the root user‚Äôs password. Th is will allow him to gain 
complete control of the ESX Host. It is, therefore, important to set up a BIOS password that 
makes it diffi  cult for an attacker to change the boot priority and gain super user access on the 
ESX Host.

308 ‚óæ Information Security Management Handbook
It is common knowledge that BIOS passwords can be circumvented by an attacker with physi-
cal access. Th ey act more as a deterrent that make it harder for an attacker to gain access to the 
system. To prevent against a dedicated attacker with physical access, full disk encryption may off er 
the best protection.
Require Certifi cate Validation
All components in a virtualization infrastructure rely heavily on SSL to ensure the confi dentiality and 
integrity of information. All communication from the client tools (VI Client, VI API, VirtualCenter, 
Web Access) rely on the administrators to deploy certifi cates signed by a trusted third party. Without 
trusted certifi cates, all communication is vulnerable to man-in-the-middle attacks. Further, it is very 
hard for users to diff erentiate between a self-signed certifi cate used by a component from a certifi cate 
warning issued because of a man-the-middle attack launched by an attacker.
During the installation of the virtualization components, self-signed certifi cates are deployed to 
get the system up and running. Extended use of self-signed certifi cates can raise the risk of attack 
and compromise through a man-in-the-middle attack. It is, therefore, recommended to deploy cer-
tifi cates that are signed by a trusted third party. Once certifi cates signed by trusted third party are 
deployed, the system should be confi gured to reject any self-signed or invalid certifi cates.
Replace Certifi cate on ESX
To replace the certifi cate on the ESX host, follow the steps mentioned below:
 
1. Log in the ESX as root
 
2. Change directory to /etc/vmware/ssl/
 
3. Backup the fi les rui.crt to rui.crt.bak and rui.key to rui.key.bak (rui.key 
is the private key and rui.crt is the certifi cate)
 
4. Upload the new certifi cate and the private key and place them in /etc/vmware/ssl/ 
with the name rui.crt and rui.key, respectively
 
5. Ensure that the owner of the both the fi les is user and group root
 
6. Ensure that the permission on the certifi cate fi le remain 644 (read/write for root, read for 
group, read for others)
 
7. Ensure that the permission on the private key fi le remain 400 (read for root, no permissions 
for anyone else)
 
8. Restart the web server by executing /etc/init.d/mgmt-vmware restart
Replace Certifi cate on VirtualCenter
To replace the certifi cate on the VirtualCenter
 
1. Log on the system that has VirtualCenter. You should have administrative privilege on the 
system
 
2. Go to C:\Documents and Settings\All Users\Application Data\VMware\VMware\
VirtualCenter\SSL\
 
3. Backup the fi les rui.crt to rui.crt.bak and rui.key to rui.key.bak (rui.key 
is the private key and rui.crt is the certifi cate)
 
4. Besides the certifi cate and the private key, you will also need a PFX (Personal Information 
Exchange) format fi le for the Windows system

Best Practices in Virtualization Security ‚óæ 309
 
5. Create the PFX fi le using openssl
 
 openssl pkcs12 -export -in rui.crt -inkey rui.key -name FQDNforVirtualCenter -out rui.pfx
 
6. Upload the new certifi cate, private key and the PFX fi le in C:\Documents and Settings\All 
Users\Application Data\VMware\VMware\VirtualCenter\SSL\
 
7. Restart the VirtualCenter management service to load the new certifi cates
Enable Certifi cate Checking on the VirtualCenter
To enable certifi cate checking
 
1. Log in to a VirtualCenter server using the VI Client.
 
2. Click Administration > VirtualCenter Management Server Confi guration.
 
3. Click SSL Settings in the left pane and enable the Check host certifi cates checkbox.
 
4. Click OK.
Re-Encrypt the Database Password
Once the default certifi cates are replaced with custom certifi cates signed by a trusted third party, 
the VirtualCenter will not start the next time. Th is is because the password used to connect to the 
database is encrypted using the old certifi cates. For this purpose, it is important to re-encrypt the 
password using the new certifi cate.
To do this, follow the steps mentioned below:
 
1. Stop the VirtualCenter service
 
2. Open command prompt and change directory to C:\Program Files\VMware\Infrastructure\
VirtualCenter Server\
 
3. Run vpxd.exe ‚àíp
 
4. Enter the database password when prompted
 
5. Start the VirtualCenter service again

310 ‚óæ Information Security Management Handbook
Confi gure Virtual Infrastructure Clients to Verify Certifi cates
Once the valid certifi cates are deployed on the Hosts and the VirtualCenter, an administrator can 
enable certifi cate validation for the VI Client. Th is is only relevant if the user had chosen ‚ÄúDo 
not display any security warnings for ‚Ä¶‚Äù was initially checked. Now that the valid certifi cates are 
installed, it is important to no longer ignore the security warnings.
Th e security warning preferences are set in [HKEY_CURRENT_USER\Software\VMWare\
Virtual Infrastructure Client\Preferences\UI\SSLIgnore\] of the system running VI Client. Under 
this key is the name of all hosts for which the security warning will no longer be displayed.
Remove the VirtualCenter and Hosts that are now confi gured with a valid SSL certifi cate. Th e 
SSL warning should no longer appear for these systems as the certifi cates used by these systems are 
signed by a trusted third party.
It is important to note that all client communication initiated from a Linux system will be 
vulnerable to a man-in-the-middle attack. Th is is because the client tools on a Linux system do 
not perform certifi cate validation. Even if a valid certifi cate is deployed on the ESX Host and the 
VirtualCenter, the client tools will not be able to diff erentiate a valid certifi cate from an invalid 
one. It is, therefore, recommended to not use a Linux system to operate the client tools including 
VI Client, VI Perl Toolkit, VI SDK, Web Access, and RCLI.
Storage Security
ESX Host can store a Virtual Machine on local SCSI disks, a Fiber Channel storage area network, 
iSCSI storage area network (SAN), or via Network File System (NFS). In the most common use 
cases, virtual machines are stored as .vmdk (Virtual Machine Disk format) fi les within an ESX 
server‚Äôs VMFS (Virtual Machine File System) datastore.
While the security considerations for storage fi les on local SCCI disks and NFS are fairly com-
mon best practices, security considerations concerning storage area network are more involved. 
Two common methods of increasing the security for a SAN are by logical partitioning of informa-
tion in diff erent segments through the use of zoning and masking.
Zoning
A zone divides and partitions the available resources into multiple logical groups. A group of servers 
with similar business need to typically access the same zone. Zone switches access control HBAs 
(host bus adapters). Th ey control which HBA can connect to which service processor. A server 
cannot access devices in outside zones. SAN switches also restrict SAN traffi  c within each zone. 
An example use of the zones is to create diff erent zones for trusted and untrusted environments. 

Best Practices in Virtualization Security ‚óæ 311
Th e ESX systems in a test environment can be confi gured to use a diff erent zone from the ESX 
systems in a production environment. Th is will reduce the risk of attack and compromise.
Logical Units Masking
Logical units (LUN) masking is used to implement access control on the fi les that the users are 
aware of and have access to. It restricts users from accessing fi les that are present on the same 
storage unit. LUN masking is enabled at the storage array level by admitting only specifi c fabric 
WWNs (World Wide Name) to access the LUN within the array. By default, the Host that has 
access to a given fi ber port can potentially access all LUNs accessible by that port. To restrict 
access, access control is enforced on the HBA using a masking utility that is part of the HBA. 
Th is masking utility allows editing the WWNs visible to a host down to the set authorized for 
that host.
Raw Device Mapping
ESX supports mapping of a raw storage device to a virtual machine using a proxy fi le present on 
the VMFS volume. Th is proxy fi le contains all metadata used to redirect disk access to the raw 
storage device. It allows a virtual machine to access the details of the underlying LUN. A virtual 
machine can pass raw SCSI commands to the LUN. Th e ability to pass raw SCSI commands to a 
LUN is very powerful and should be restricted to only privileged and highly trusted systems.
Secure iSCSI Deployment
An ESX Host can be confi gured to use the iSCSI-based storage area network instead of the fi ber 
channel. An iSCSI is a specialized high-speed network-based protocol that connects the server 
to a storage system using Ethernet. Th e availability of storage devices over the Ethernet increases 
their attack surface considerably. Incorrectly confi gured iSCSI devices can potentially allow an 
unauthorized user to connect and mount a LUN which he should otherwise not have access to. 
Th is poses a signifi cantly higher risk when compared to a fi ber channel‚Äìbased SAN. If fi ber chan-
nel is used, a virtual machine has no means to access the SAN directly. Th e virtual machine is not 
provided the virtual fi ber channel HBAs. Th ey can only access the disks that have been assigned 
to a virtual machine using virtual SCSI adapters. Considering the security profi le, a fi ber channel 
may be considered as a safer storage system.
Th ere are well-established hardening best practices that can be employed to make iSCSI devices 
secure from attack and compromise.
Network Isolation of iSCSI
Th e most important step to securing the iSCSI device is to isolate it from an untrusted network. 
An iSCSI device should only be accessible on an isolated or private LAN where limited trusted 
systems are hosted. Th e use of a separate VLAN may serve well to isolate the storage network.
If the iSCSI device is accessible to virtual machines or other untrusted systems, then they 
are open to attack and compromise. All iSCSI communication except Challenge Handshake 
Authentication Protocol (CHAP) is clear text and can be compromised using a man-in-the-middle 
attack. It is, therefore, recommended to create a separate isolated network for ESX and iSCSI 
communication.

312 ‚óæ Information Security Management Handbook
Th e management console of the device should also be protected from unauthorized access. 
Administrators should only be able to access the management console from a management VLAN 
or restricted systems.
Require Challenge Handshake Authentication 
Protocol to Connect to iSCSI
ESX only supports the use of CHAP authentication to connect to iSCSI devices. It does not 
support other authentication protocols like Kerberos, Secure Remote Protocol, or public key 
encryption. Further, bidirectional authentication is not supported. Only the initiator can 
authenticate to iSCSI devices and each initiator can only support one set of credentials for all 
targets.
It is recommended to confi gure the iSCSI SAN to require authentication from the initiators. 
Th is will ensure that any attacker with access to the iSCSI device will not be able to connect to 
and mount the LUNs that he should not have access to.
Security Banner
US Department of Defense recommends the use of warning banners. Th e banner should indicate 
the name of the organization that owns the system and the fact that the system may be monitored. 
Th e banner should also indicate that by proceeding to use the system the user is consenting to the 
monitoring terms stated.
Th e organization‚Äôs legal team should review and approve the content of the warning banner. 
For the most part, warning banners should be consistent across the organization expect for any 
location or system-specifi c messages. A sample message can be found at http://www.usdoj.gov/
criminal/cybercrime/s&sappendix2002.htm
ESX Host
Web Access
To confi gure the warning banner on the ESX Host Web interface, embed the banner in the fi le 
/usr/lib/vmware/hostd/docroot/index.html
Similarly, the fi le /usr/lib/vmware/webAccess/tomcat/apache-tomcat-5.5.26/webapps/ui/WEB-
INF/jsp/scriptedInstall/login.jsp will need to be modifi ed to embed the security banner in the 
login page.
Console and SSH Access
To enable the security banner for console and the SSH access, embed the banner in the follow-
ing fi les:
/etc/issue
/etc/issue.net
/etc/banner
/etc/motd
/etc/issue.emergency,

Best Practices in Virtualization Security ‚óæ 313
Further edit the fi le /etc/ssh/sshd_confi g and replace line
#Banner /some/path
with
Banner /etc/banner
Restart SSH
service sshd restart
VirtualCenter
VirtualCenter is the central management station that allows administration of all ESX Hosts and 
the VirtualMachines. It mediates several advanced features available on the Virtual Infrastructure 
like High Availability, Distributed Resource Scheduler, and VMotion.
When an ESX server is added to a VirtualCenter, the VirtualCenter creates a powerful vpxuser 
account on the ESX Host. Th is account is used by VirtualCenter to perform all the operations on ESX 
Hosts. ESX server, therefore, has a trust relationship with the VirtualCenter. If the VirtualCenter is 
compromised, then all ESX Hosts and the virtual machines on each ESX Host may be at risk. Th e 
security of VirtualCenter, therefore, warrants very stringent security standards.
VirtualCenter Host Controls
VirtualCenter needs to run on a Windows Server 2000 or newer and all security controls that 
would otherwise be applicable to a Windows Server should be implemented for a VirtualCenter. 
Th is includes and is not limited to antivirus, antispyware, intrusion detection systems, and a 
fi rewall.
An administrator can also consider running the VirtualCenter as a virtual machine. Th e key 
benefi t of running the VirtualCenter as a virtual machine is to provide high availability (HA) and 
take snapshots. Th e HA feature ensures that the VirtualCenter is migrated to a diff erent ESX Host 
in the event of a server failure. Similarly, system snapshots can help the administrator restore the 
VirtualCenter if a system patch or an application makes the system unstable.
Implement Network Isolation
VirtualCenter only needs to communicate with the Virtual Infrastructure components like the ESX 
Host, Licensing Server, Update Manager, and other virtualization add-ons. VirtualCenter should, 
therefore, be hosted on an isolated management network that is separate from an operational net-
work. Regular users should not be able to connect to VirtualCenter. Further, during normal operation, 
VirtualCenter does need to access the Internet. A combination of isolated network with no Internet 
connectivity will signifi cantly decrease the attack surface for VirtualCenter.
Firewall
VirtualCenter can receive network communication from many diff erent interfaces. Th ese include 
the VI Client, the Web Interface, the Remote CLI, the Scripting Toolkit and the third party 
Addons that use the SDK. Not all of these network interfaces may currently be in use. Further, 
there may be other ports like NETBIOS that may not be used.

314 ‚óæ Information Security Management Handbook
Th e use of fi rewalls can be considered to disable or even restrict access to these ports. Th e fi re-
wall can be confi gured to only allow a restricted set of systems to access the management interface. 
Th e list of services and the ports used by VirtualCenter are mentioned below.
Port Number
TCP or UDP
Incoming or Outgoing
Service Description
80
TCP
Incoming
HTTP access for non-encrypted 
traffi c. By default redirects the user 
to port 443.
443
TCP
Incoming
HTTP access using SSL.
All VirtualServer clients use this 
port including the VI Client, Web 
interface, and SDK. This is the only 
required port for VI Client and 
VirtualCenter communication.
902
TCP and UDP
Incoming TCP, 
outgoing UDP
ESX Access and Heartbeat
27000‚Äì27010
TCP
Incoming and 
outgoing
Licensing transactions. Applicable 
if Licensing Server is installed on 
the same host.
Disable Unnecessary Services
VirtualCenter should be hosted on a separate system with very few other shared services. Ideally, 
only the VirtualCenter and the Licensing Server should be hosted on this system. All other 
services that are not necessary to support the VirtualCenter and the Licensing Server should be 
disabled. Th is can include services like IIS Web server, printer services, e-mail server, and dhcp 
server. Unnecessary services and applications increase the number of input points that can be 
used by an attacker to compromise the system. Th erefore, the number of input points in a system 
should be kept to the lowest possible.
Authentication and Access Control
Authentication to VirtualCenter may be supported through local Windows-based authentication 
or through integration with ActiveDirectory. If the VirtualCenter host is not part of an Active 
Directory domain, then local system administrators will have access to the VirtualCenter. If the 
host is a member of Active Directory or a domain controller, then administrators on the domain 
have administrative access on VirtualCenter.
Th ere are several advantages of using VirtualCenter as a member of Active Directory Domain. 
Some of the key ones are
 
1. Centralized management of users and roles. Users could be made part of a group or a role. 
Th is limits the number of user accounts that need to be tracked and changed in the event of 
a change in user role.
 
2. Password policy enforcement: It is easier to establish and enforce one good password policy 
throughout the domain than to confi gure several individual policies for every environment.

Best Practices in Virtualization Security ‚óæ 315
 
3. Ease of use: Th e user is likely to choose one good password with adequate complexity than 
choose several diff erent ones with varying requirements. If a user has many accounts, then 
he may end up writing his credentials down which may be easily accessible to others.
 
4. It is possible to enable pass-through authentication for a VI Client using a short to ‚ÄúC:\
Program Files\VMware\Infrastructure\Virtual Infrastructure Client\Launcher\VpxClient.
exe‚Äù -passthroughAuth -s virtualcenter.domainname.com
If the VirtualCenter is not part of the domain controller, then the users can still be confi gured on 
VirtualCenter using local Windows accounts. It is desirable to create a separate Windows account 
with regular user privileges and make that user an administrator for VirtualCenter. Th is will elim-
inate the need to provide Windows administrative account to the VirtualCenter administrator. 
Where possible, it is recommended to integrate VirtualCenter with the Active Directory domain 
for all the advantages mentioned above.
Roles and Permissions
VirtualCenter supports a very matured and sophisticated system of roles and privileges. Th ese roles 
and privileges determine a user authorization on the objects. An administrator can confi gure the 
roles and privileges to control user permissions at a very granular level. Roles and permissions can 
be confi gured to set authorizations for various operational and administrative tasks on the avail-
able object. Th e objects may be a data center, clusters, hosts, virtual machines, or any resource 
group.
By default, VirtualCenter supports two groups of roles: One is system roles and the other is 
sample roles. Th e system roles are no access, read-only, and administrator. System roles cannot be 
edited or deleted. Users and groups can be added or deleted from system role.
Th e sample roles are provided as suggestions. Th e permission for the sample roles can be modi-
fi ed to meet an organization‚Äôs needs. Further, administrators can create custom roles to meet the 
requirements of the environment. It is important to create roles that only support the intended 
tasks and do not provide the users with elevated privileges.
Th e task of assigning roles and privileges to a large base of users can be a fairly involved. A user 
can gain privileges from the permissions that were applied hierarchically downward to an object. 
Permissions can also be overridden by setting diff erent permissions on the lower object. Finally, a 
user may be a member of two or more roles and groups. In such cases, a union of the permissions 
may take eff ect. A combination of these diff erent rules can easily cause confusion resulting in 
unintentional consequences. It is, therefore, recommended to plan user roles and permissions well 
before making actual confi gurations in VirtualCenter.
Database Security
Th e databases supported by VirtualCenter include Microsoft MSDE, SQL 2000/2005 SP1, and 
Oracle. Th e use of MSDE is not recommended for a production environment. Some of the security 
considerations while using the database are
 
1. Install the database on a separate server with adequate network controls like VLANs and 
fi rewalls.
 
2. Harden the database by removing unnecessary procedure calls, unnecessary databases and 
users, and installing all the applicable security patches.

316 ‚óæ Information Security Management Handbook
 
3. Shared database use with other applications is not recommended. Other applications may 
have user accounts with privileges not restricted to their own database. Malicious applica-
tion owners or an SQL injection vulnerability in other applications can result in complete 
compromise of Virtual Infrastructure.
 
4. Confi gure VirtualCenter to use an account with restricted privileges. Th is user should only 
be able to
 
a. Select, update, insert, and delete queries
 
b. Execute stored procedures
Logging and Monitoring
VirtualCenter governs the important confi guration settings for the virtualization infrastructure. 
It is, therefore, important to log and monitor the actions performed on the VirtualCenter. While 
most of the actual confi guration is stored on the database, some important confi guration and logs 
reside on the VirtualCenter‚Äôs fi le system. An organization‚Äôs logging policy might require periodical 
backup and monitoring of these fi les.
Th e logs fi les are located at the following locations:
VirtualCenter Log Files
C:\Documents and Settings\All Users\Application 
Data\VMware\VMwareVirtualCenter\Logs\*
VirtualCenter Web Server Log Files
C:\Program Files\VMware\Infrastructure\
VirtualCenter Server\tomcat\logs\*
License Manager
C:\WINDOWS\Temp\lmgrd.log
A menu option in VirtualCenter management console allows the administrator to back up all 
VirtualCenter and ESX Host logs using the VI Client.
Select File > Export > Export Diagnostic Data
If connected to an ESX Host, the logs for ESX Host will be collected.
 
‚óæ
If connected to a VirtualCenter Server, the user can choose the ESX Host(s) to gather the 
 
‚óæ
logs from.
Another convenient option is to run the Generate VirtualCenter Server log bundle command 
from the VMware program fi le menu. Th is will capture all troubleshooting and debugging infor-
mation including the relevant Windows registry entries, confi guration fi les, and all log fi les for 
VirtualCenter.
Th is task can be easily automated by the administrator to periodically obtain the information 
and compare the state to the previous confi guration.
Virtual Machine
Th e virtual machines running in a virtualized environment should not be treated any diff erent 
from the virtual machines running in a physical environment. Th ere may be a few exceptions for cases 
like clustering services and virtual machines using Guest Tagging for VLANs but for the most part, 
all security considerations applied for physical systems should also be applied for virtual machines.

Best Practices in Virtualization Security ‚óæ 317
Virtual versus Physical World
Systems are not inherently more or less secure because they are virtualized. Th e security of a vir-
tual system largely depends on the confi guration and security measures applied on the system. 
Virtualization adds a few new attack vectors and also provides a few measures to bolster the 
security of a virtual machine but these measures are independent of the need to secure the virtual 
machine itself. It is, therefore, important to apply all security controls of the physical world to a 
virtualized system. Th ese controls may include and are not restricted to the following:
 
1. Harden the guest operating system. Default operating system installations are deployed 
with unnecessary services, default accounts, and in some cases weak fi le permissions. It is, 
therefore, important to follow a system hardening guide to lockdown the system.
 
2. Deploy security solutions like antivirus, antispyware, fi rewall, intrusion detection systems, 
and monitoring agents. It may also be important to connect them to a central management 
console that monitors and updates these tools on a periodic basis.
 
3. Patch all systems regularly. In the current environment, by the time a vendor releases a secu-
rity patch, the vulnerability is often being exploited in the wild. It is, therefore, important to 
patch systems as soon as possible. Th is does not eliminate the need to test the patches well 
to ensure that they do not cause any confl icts with critical applications. Th e use of virtual-
ization can make it easier to take snapshots and revert back in case any unexpected event 
occurred.
Use Templates
Templates provide a fast and effi  cient way to create new virtual machines. A template image is a 
base image of a virtual image that can be used to create and deploy live virtual machines. Th ere 
are several advantages of using templates. Some of the key ones are
 
1. Templates can be confi gured to be a hardened base operating system with all necessary secu-
rity controls, latest patches and service packs, and all security policies pre-confi gured. Th is 
ensures that the systems are better secured when initially deployed.
 
2. Reduced deployment time. Th e time needed to deploy a new virtual machine is reduced 
with the use of templates. Th e templates will probably have the necessary tools, applications, 
and drivers installed thereby reducing the likelihood of confi guration errors and the deploy-
ment time needed for new systems.
 
3. It is easier to update templates along with the applications and the security tools to the latest 
available service packs, patches, and updates. Th e server update process that would other-
wise take several diff erent individual tasks is now part of the template.
Th ere are some security considerations that need to be considered with the use of templates. Some 
of the key things to consider with the use of templates are as follows:
 
1. License violations: Templates are mobile, activated guest images that can contain commercial 
licensed software. It is easy to make several copies of the template without purchasing the 
necessary number of licenses. Th is can result in unintentional license violation. While this 
does not directly result in compromise of a system, it can have fi nancial and public relations 
impact on an organization.

318 ‚óæ Information Security Management Handbook
 
2. Hidden secrets in templates: Live virtual machines generated from templates may have 
embedded hidden secrets that can be exploited by a malicious user to gain access to other 
systems on the network. Th e most common technique employed by an internal penetra-
tion tester to gain access to privileged systems on the internal network is password reuse. 
Templates could potentially contain services running as privileged accounts. If a malicious 
user has administrative control on the virtual machine, then by dumping the LSA secrets 
the administrator on the virtual machine can gain access to clear text passwords of the ser-
vice accounts. Th ese credentials could then be replayed on other systems to gain privileged 
access to them. Similarly, if the password to administrator (or root) account is the same on 
all systems, then a user with administrator-level access on the system can crack the pass-
word hashes. In some scenarios, the user may not even need to crack the hash. Th ey can 
directly pass the hash to gain privileged access to other systems.
Loss of Virtual Machines
At a fundamental level, virtual machines are comprised of a bunch of fi les. If an attacker can 
gain access to the vmdk fi les, then he can easily upload the complete virtual image on a remote 
private server. Similarly, an attacker can mount the vmdk(s) using the VMDK Disk Mount util-
ity and directly write to the underlying fi le system. Th is can allow an attacker with access to 
virtual machine fi les to inject code and replace system fi les with backdoors. It is, therefore, impor-
tant to secure the storage system and ensure that the users do not gain access to VMDK fi les. 
Organizations can also consider the use of full disk encryption techniques to ensure that an unau-
thorized user cannot manipulate the virtual machines even if he has access to the VMDK fi les.
Resource Overcommitment
Using the virtual infrastructure, it is possible for the administrators to overcommit on the resources 
between virtual machines. Th is means that the ESX administrator can allocate virtual machines 
memory and CPU power such that the sum of individual assignments can exceed the total avail-
able resources. While this confi guration allows optimal utilization of resources, the successful 
operation of the system hinges on the assumption that virtual machines will play nice and only 
consume resources when they need them. Further, by default, all virtual machines have the same 
priority on the resources. However, not all virtual systems may be equally critical. If any virtual 
machine acts malicious, then it can easily consume all available resources thereby causing a denial-
of-service attack on more critical systems. It is, therefore, important to set minimum limits on the 
resources that should be available for the key critical systems. Th is will ensure that these systems 
will at least have the minimum resources required to be stable. It is also important to monitor the 
resource consumption by virtual machines to ensure that no machine is trying to consume all 
resources. Further, an administrator can set tasks to take necessary action like suspend, shutdown, 
or reboot a virtual machine in the event that a virtual machine is causing performance degradation 
to other systems. Similarly, an administrator can set up alerts to notify them if unusual resource 
consumption is detected.
Isolation Based on Trust Levels
VMware has incorporated several architectural designs to enforce virtual machine isolation. 
Th e Hypervisor is designed to ensure that one virtual machine cannot access resources belonging 

Best Practices in Virtualization Security ‚óæ 319
to other virtual machine in a manner that is not possible in the physical environment. While 
that works well most of the time, security researchers periodically identify vulnerabilities that can 
potentially allow an attacker on a virtual machine to break the isolation and gain privileged access on 
another virtual machine. Th ese vulnerabilities eventually get patched but there is always a window of 
exposure between the time that the vulnerability is discovered and the vulnerability is patched.
Organizations that require a very high level of security can consider a defense-in-depth strat-
egy. Th e defense-in-depth strategy recommends only hosting virtual machines on the same trust 
level on an ESX Host. While this approach provides better protection against less trustable systems, 
it does come at the additional hardware and operational costs.
While the isolation provided by VMware can be considered strong and the ESX administra-
tors can only deploy systems on the same trust level on the same ESX Hosts, all systems are still 
interconnected through the network. Th e most eff ective isolation technique will be to isolate systems 
based on network segments.
All virtual machines on the same network segment are open to attack and compromise from 
other virtual systems present on the same network. A malicious user on one virtual machine can 
perform attacks like sniffi  ng traffi  c, ARP spoofi ng, IP spoofi ng, and MAC spoofi ng. Th ese attacks 
can alter the traffi  c fl ow, provide clear text access to sensitive information, or help an attacker 
launch a man-in-the-middle attack.
Considering the diff erent risk level of systems on the same network, it is important to segment 
the network based on the business roles, trust levels, and criticality of systems. Virtual machines 
can be separated by diff erent physical network connections or through the use of VLANs.
Virtual Machine Isolation Settings
Th ere are several isolation settings that can be made for a virtual machine or a group of virtual 
machines that can provide defense-in-depth measures. It may be advisable to perform these hard-
ening steps on all virtual machines unless there is a legitimate business purpose for not doing so.
Disable Copy and Paste
By default, the Copy and Paste feature is enabled for all virtual machines. Th is allows a user with 
remote access to a virtual machine console to copy and paste information between the virtual 
machine and the system where remote access is running (host). While this is a convenient feature, 
it allows the processes and users on the virtual machine to access the clipboard of the host system. 
If a user on the host system copies any sensitive information to the clipboard and moves focus to 
the remote virtual machine, the virtual machine process can gain access to the information on the 
clipboard. Users on the host system will not even be aware that the processes in the virtual system 
are able to access the information on the clipboard. It is, therefore, recommended to disable the 
Copy and Paste feature between the host and the virtual machine.
To disable Copy and Paste feature, make the following confi guration changes
Name
Value
isolation.tools.copy.disable
True
isolation.tools.paste.disable
True
isolation.tools.setGUIOptions.enable
False

320 ‚óæ Information Security Management Handbook
Limit Log File Size
Th e logs created during virtual machine operation are written to the vmware.log fi le. Th is fi le 
is present on the same VMFS volume as other fi les for the virtual machines. All troubleshooting 
and debugging information from the virtual machine is written to this fi le. A malicious virtual 
system can potentially exploit this feature of logging information to consume all available disk 
space and cause a denial-of-service attack. To eff ectively mitigate the disk exhaustion attack, the 
administrator can set a limit on the total size and the number of log fi les that will be created. Th e 
key is to choose a fi le size and number of fi les that collect adequate information but do not tax 
the storage space. VMware recommends 10 diff erent log fi les with maximum size of 100 kB each. 
Th is confi guration will collect most of the important information needed for logging and debug-
ging purposes and will not consume too much system resources.
Th e system will automatically enforce the log fi le checks and will delete the old log fi les in the 
event that the maximum fi le size and fi le number is reached for all fi les.
Name
Value
log.rotateSize
100,000
log.keepOld
10
Limit VMX File Size
Virtual Machines can provide system-specifi c information to ESX Host that is written to VMX fi le. 
Th ese are name‚Äìvalue pairs that have no predefi ned format and can be of any length. If no upper limit 
size is specifi ed on the name‚Äìvalue pairs, then the VMX fi le can grow out-of-bounds and it can result 
in a denial-of-service attack. In ESX 3.5, the size for this GuestInfo is limited to 1 MB by default.
An administrator can change this setting using the tools.setInfo.sizeLimit setting in the VMX 
confi guration fi le. Th e absence of this setting places a default size limit of 1 MB.
A more stringent setting disallows the guest system from writing any name‚Äìvalue pairs to the 
VMX fi le. Th is may be a recommended setting for some production systems that are not expected 
to change during the course of operation. It can be set using the setting
isolation.tools.setinfo.disable true
Disable Unnecessary Devices
Th e administrators can decide which devices are needed for the virtual machine and only allow 
connection to those devices. Other devices like serial and parallel port and CD/DVD drive may 
not be needed on the virtual system.
Some of these removable devices may have residual information. If the virtual machine is 
allowed access to these devices, then the users on the virtual machine can potentially gain access 
to this information.
Device Name
Confi guration Parameter <X> 
Signifi es a Device Identifi er
Value (True or False)
Floppy drive
Floppy<X>.present
False
Serial port
Serial<X>.present
False
Parallel port
Parallel<X>.present
False

Best Practices in Virtualization Security ‚óæ 321
All unnecessary devices that are not needed should not be enabled for the virtual machine. 
To disable the use of a device, the device presence can be set to false.
Other Confi guration Settings
Users within the virtual machine should not be able to connect or disconnect the available devices. 
Only the administrators should be able to confi gure the virtual machine with devices. To prevent 
a user within a virtual machine from connecting or disconnecting a device, the administrator can 
set the confi guration
isolation.device.connectable.disable = ‚ÄúTRUE‚Äù
isolation.device.edit.disable = ‚ÄúTRUE‚Äù
Lower privileged users within the virtual machine can invoke the disk-shrinking utility to 
reclaim unused space and decrease the size of a virtual disk. Repeated use of disk shrinking can 
cause the virtual disk to defragment and shrink thereby becoming unavailable and causing a 
denial-of-service attack on all users. To prevent users from invoking the disk-shrinking utility, 
confi gure the following settings
isolation.tools.diskWiper.disable = ‚ÄúTRUE‚Äù
isolation.tools.diskShrink.disable = ‚ÄúTRUE‚Äù
Tools for Virtualization
Virtualization Malware
A lot is being debated about virtualization security and will continue to be the case. Over the last 
4 years, several proof-of-concept malware and virtualization detection tools have been released. 
One of the virtualization-specifi c infection techniques is known as Hyperjacking. In this attack, 
a malware converts an operating system into a virtual machine and operates as Hypervisor. It is 
transparent to the applications and operating system itself. Some of the prominent virtualization-
based malware discussed in the community are Blue Pill, Vitriol, and SubVirt.
Blue Pill is a virtualization-based malware originally targeting the Windows Vista operating 
system. Since then, the project has evolved and now works on diff erent architectures and platforms. 
In simplistic terms, the Blue Pill concept SubVirts the base operating system or the Hypervisor and 
moves it to a virtual machine. Once infected, the malware itself will run like a Hypervisor between 
the host operating system (could be a hypervisor) and the hardware. Th e authors of this proof of 
concept malware claim it to be 100% undetectable. One of the main reasons that the malware is 
claimed to be undetectable is because it uses no system hooks, therefore, the traditional methods of 
rootkit detection no longer apply.
Th ese claims have been heavily challenged in the community. Many claim that the techniques 
that would be used to inject the malware into the Hypervisor or the base operating system can 
be detected and stopped. Some others claim to have the technology to detect virtualization-based 
malware like Blue Pill.
Th ere are other Hyperjacking tools like SubVirt and Vitriol. Vitriol developed by Dino Dai 
Zovi uses Intel‚Äôs virtualization technology (VT-x) to infect the Mac OS X. SubVirt was a similar 
proof of concept virtualization malware developed by researchers at the University of Michigan 
targeting the Virtual PC and VMware Workstation on x86 platform.

322 ‚óæ Information Security Management Handbook
Th e main idea for all these rootkits remains the same. Some of them are more weaponized 
than others.
SubVirt: http://www.eecs.umich.edu/virtual/papers/king06.pdf
Detecting Virtualization
Most commercial virtualization solutions make no attempt to hide the fact that the guest operat-
ing system is running as a virtual machine. Th ere are many diff erent ways to detect virtualization. 
Th ese include looking for artifacts inside the virtualized environment. Some of the easily identifi -
able signs include
Process, fi les, and registry keys present inside the guest operating system.
 
‚óæ
Inside memory. Several operating system structures are located on diff erent address ranges 
 
‚óæ
in virtual machine when compared to a virtual machine. In particular, the location of inter-
rupt descriptor table register (IDTR) provides a good indication of the kind of virtualization 
environment used. Th e other operating system structures like global descriptor table and 
local descriptor table can also be referenced to determine the virtualized environment.
Virtualization-specifi c hardware. Th is includes motherboard make and model, MAC 
 
‚óæ
addresses, USB controller, audio controller, and SCSI devices.
Invoke nonstandard instructions and observe the behavior. Some nonstandard x86 instruc-
 
‚óæ
tions are supported by VMware, Virtual PC, and Xen.
Th e timing discrepancy between two operations performed in physical and virtual mode 
 
‚óæ
can also be used to detect the presence of virtualization.
A non-privileged user can use any combination of the methods listed above or more to detect 
operations in a virtual machine. However, the signifi cance of such detection is getting smaller. In 
the past, some malware detected operation in a virtual machine and terminated itself. Th is was 
done to avoid being monitored. Th e assumption was that the virtual machines with their unique 
ability to take snapshots and revert to snapshots will be used to study malware.
Virtualization is now increasingly being adopted in a production environment and its use is 
only expected to grow. In the future, instead of terminating themselves, malware and attackers 
can use the virtualization detection techniques to launch attacks targeting the specifi c virtualized 
operating environment.
Some of the popular virtualization detection tools are
 
1. RedPill
 
2. Nopill
 
3. ScoopyNG (Scoopy Doo and Jerry combined)
 
4. VMDetect
Future of Virtualization Security
In February 2008, VMware announced the release of VMsafe security technology. VMware part-
nered with several security vendors that can leverage the powerful features of VMsafe APIs to 
provide security solutions that were not available in the physical world.
Th e VMsafe technology opens the operations on ESX Host to provide security vendors much 
more fi ne-grained access to Hypervisor and the operations on the virtual machine. VMsafe-based 
security solutions will be able to provide several unique features.

Best Practices in Virtualization Security ‚óæ 323
CPU and Memory Scanning
Using the VMsafe APIs, the authorized security product will be able to monitor CPU state and 
virtual machine memory pages for all the virtual systems on an ESX system.
Network Monitoring
All network communications through any virtual switch can be monitored using VMsafe. Th is 
will include traffi  c for all virtual machines and the ESX Host.
Process Monitoring and Execution
VMsafe APIs will allow process execution control within a virtual machine. Th is will allow the 
security tools to monitor the modules loaded, executables launched, and the operations performed 
by all binaries.
Storage Scanning
All virtual machines‚Äô storage fi les can be mounted, scanned, and edited using the VMsafe APIs. 
Security tools will be able to scan for malicious content within the fi le system. Further, all fi le 
system operations can also be governed.
Th ere are several benefi ts of VMsafe APIs that make the technology promising.
Agent-Free Security
Th e key benefi t of using VMsafe-based products will be the agent-less security. Administrators will 
no longer need to deploy a security tool on every operating system. One of the major challenges 
with the physical world is that once a system is compromised, it is diffi  cult to trust any operation 
on that system. In most cases, malware and security tools operate at the same permission level. 
Th is allows the malware to disable the security software and render it useless. With the use of 
VMsafe technology, agents will no longer be needed to be deployed on the virtual machines. Th ey 
will operate outside the infl uence of the malware, which can allow more eff ective control and 
higher standards of security.
Further, agents will not need to be deployed, updated, and maintained on every system. Any 
virtual system present on the ESX Host could automatically be part of the protection profi le.
Comprehensive and Integrated Network Security
All network traffi  c from all virtual machines across diff erent ESX Hosts can be integrated and moni-
tored using the VMsafe technology. Security products can enforce more granular network policies for 
virtual machines. Malicious content can be blocked or fi ltered before it actually reaches the virtual 
machines. Further, the need to deploy complex and expensive hardware devices can be reduced.
Security Bottlenecks Can Be Reduced
Most security tools tend to be resource intensive. Th ey tend to have a large footprint and consume 
valuable resources. In some cases, this causes a contention of resources on a system that delays 
critical business operations. With the use of VMsafe technology, most security operations are 

324 ‚óæ Information Security Management Handbook
performed without consuming the resources of the virtual machine. Th e scanning and monitoring 
of the content and operations are performed outside the scope of a virtual machine.
Th is is especially important for critical business systems that support high-priority operations. Even 
if the security tool fails or crashes, this will not directly aff ect the operation of the virtual system.
Concerns about VMsafe
Th e powerful features of VMsafe if abused by malicious systems can result in signifi cant damage 
to the infrastructure. Some of the most obvious concerns are
 
1. VMsafe APIs will allow the security vendors to perform some very powerful operations on 
the ESX and the virtual machines. What prevents a malicious virtual machine from mas-
querading itself as a security tool and invoking the same function?
 
2. To perform some of the security features that were mentioned, the security tools will have 
direct access to the core functioning of the Hypervisor. While measures can be taken to run 
the operations outside the context of the Hypervisor, the ability to infl uence CPU, memory, 
network, and storage will raise concerns about Hypervisor integrity.
Given the importance of security, we can be sure that there will be considerable thought given to 
these and several other security concerns arising with the use of VMsafe. We can also be sure that 
there will be design fl aws, bugs, and vulnerabilities. While we trust the vendors to design the prod-
ucts securely, it will be our responsibility in the security community to verify and test these claims.
About the Author
Shanit Gupta is a principal consultant at Foundstone, Mission Viejo, California.
References
http://www.vmware.com/pdf/esx3_esxcfg_auth_tn.pdf
http://www.vmware.com/support/esx21/doc/esx21admin_snmpagents.html
http://www.inlab.de/balanceng/vi3_35_25_3_server_confi g.pdf
http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1003070
http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1517
http://www.vmware.com/pdf/vi_vcserver_certifi cates.pdf
http://pubs.vmware.com/vi35/server_confi g/wwhelp/wwhimpl/js/html/wwhelp.htm?href=sc_cover.1.1.html
http://www.cisecurity.org/tools2/vm/CIS_VMware_ESX_Server_Benchmark_v1.0.pdf
http://www.accessmylibrary.com/coms2/summary_0286‚Äì9909942_ITM
http://www.vmware.com/pdf/esx25_rawdevicemapping.pdf
http://www.vmware.com/pdf/esx_lun_security.pdf
http://iase.disa.mil/stigs/stig/esx_server_stig_v1r1_fi nal.pdf
http://www.vmware.com/pdf/vi3_vc_roles.pdf
http://www.eecs.umich.edu/virtual/papers/king06.pdf
https://www.blackhat.com/presentations/bh-usa-06/BH-US-06-Zovi.pdf
http://viops.vmware.com/home/docs/DOC-1079
http://www.iseclab.org/papers/detection.pdf
http://handlers.sans.org/tliston/Th wartingVMDetection_Liston_Skoudis.pdf
http://www.off ensivecomputing.net/fi les/active/0/vm.pdf
http://www.vmware.com/technology/security/vmsafe/usecases.html

325
21
Chapter 
Everything New Is Old Again
Robert M. Slade
Once upon a time, it was fun going to computer trade shows. All the vendors were doing new 
things. Novel ideas sprouted like weeds. It was exciting.
But that was once upon a time, very long ago.
Somewhat more recently I was doing a course on ‚ÄúEmerging Technology‚Äù for a local 
college. As the course went on, the students kept asking why we weren‚Äôt studying this or 
that technology. Basically, it was because those technologies weren‚Äôt actually new or emerg-
ing: they were old technologies under new names. I introduced the class to Slade‚Äôs Law of 
Computer History: those who do not learn the lessons of computer history are doomed to buy 
it all again‚Äîrepackaged.
Contents
Virtualization .......................................................................................................................... 326
Cloud Computing .............................................................................................................. 326
CloudAV .........................................................................................................................327
ROSI ........................................................................................................................................327
Intelligent Automation .............................................................................................................327
Native Client ........................................................................................................................... 328
Convergence ........................................................................................................................... 328
Baked in Security .................................................................................................................... 328
Web 2.0 ...................................................................................................................................329
VDI .........................................................................................................................................329
How to Spot the Next Big Th ing that Spots Next Big Th ings ...................................................329
Vulnerability Management ...................................................................................................... 330
Compliance ............................................................................................................................. 330
Etc.  ......................................................................................................................................... 330
Conclusion ...............................................................................................................................331
About the Author .....................................................................................................................331
References ................................................................................................................................331

326 ‚óæ Information Security Management Handbook
Don‚Äôt believe me? You think Twitter is the latest new thing? We used to have a system that 
allowed you to publish your current activities and plans, and made it possible for anyone (who was 
interested) to access them. It was called fi nger.
Into blogging are you? You like the fact that people with similar interests can fi nd each other, 
post their thoughts and opinions, and comment on those of others? We used to do that on some-
thing called Usenet News.
So, you‚Äôre a 13th-level sorcerous battle maid on World of Warcraft, having built up a stock 
of magical pruning saws? Other than the graphics, what‚Äôs the diff erence between that and 
rogue?
Allow me to present a few more examples.
Virtualization
Virtually everyone has heard the ‚Äúnew‚Äù term ‚Äúvirtualization.‚Äù Th at‚Äôs because virtually every 
vendor has jumped on the virtualization bandwagon. Virtually anything can be virtualized, it 
seems.
Also, virtually nobody can agree on what virtualization really means. Virtualization seems to 
be a confl ation of two old ideas: virtual machines (what do you think VM and VMS stood for?), 
and distributed computing (which is now being sold as ‚Äúcloud computing,‚Äù an amazingly cloudy 
concept that‚Äôll be dealt with in a moment).
We used virtual machines a lot in the old days, and they were great for security. We used 
them as goat or bait machines for viruses. Very secure way to protect yourself when dealing with 
dangerous software.
Of course nowadays they use virtualization in some virtually explosive ways. Like putting your 
Kerberos KDC on the same physical box as your Web server.
Cloud Computing
At one time, everybody stored their programs and information in some mysterious, far off , 
ill-defi ned entity. It was called a ‚Äúmainframe,‚Äù or sometimes just ‚Äúthe computer.‚Äù Nobody stored 
anything locally: you really couldn‚Äôt, on a terminal.
So then we got microcomputers, or personal computers, or desktop machines. And people 
stored everything locally. And then found out that there were problems with not making backups, 
getting viruses, and stuff  like that.
In the mid-1990s, the Internet was getting big, thanks to this new technology called ‚Äúthe 
World Wide Web.‚Äù And people started to write programs that ran in the application you used 
to access the Web, called a browser. And all of a sudden people had this wonderful idea that you 
wouldn‚Äôt need to store your programs locally anymore, they could be stored, in bits and pieces, all 
over the Internet. And so could your data!
And everyone who has ever looked at a network diagram knows what the Internet is. It‚Äôs a 
cloud.
Well, when people started to actually try and store programs and data on the net (in the cloud), 
they got into a big fi ght over whether they would use a freely available and open technology, or one 
from Th e Vendor Who Controlled Everything‚ÄîTrust Us. And then they decided that maybe it 
really wasn‚Äôt that great an idea.
Until it came back with a new name‚Ä¶

Everything New Is Old Again ‚óæ 327
CloudAV
And, even within the idea of cloud computing there are specialties.
A few media sources seem to be picking up a press release from the University of Michigan [1]. 
Th is reports on ‚ÄúCloudAV,‚Äù a project and a series of papers about having antivirus detection run 
‚Äúin the cloud‚Äù rather than on the PC [2].
As usual, there seems to be some misunderstanding about what is going on here. CloudAV is 
not really a new approach, it is simply the use of multiple scanners, which the AV research com-
munity has advocated for years. It‚Äôs like having a bunch of scanners installed on your desktop, 
or a system like Virustotal [3], with the exception that the scanners run on diff erent computers 
so you get a bit of performance advantage (absent the bandwidth lag/drain for submitting fi les to 
multiple systems).
ROSI
Return on Investment. (In security.)
Return on Security Investment.
It doesn‚Äôt exist. But it keeps on coming back from the grave, no matter how many times we 
try to kill it.
OK, we can say that spending money on security keeps you from losing more money somewhere 
down the road. But ‚Äúinvesting‚Äù in security isn‚Äôt going to bring in revenue. (Well, unless you want to 
try and estimate the extra business you got from being a ‚Äúsafe‚Äù company to do business with.)
We can also try to do quantitative risk assessment or analysis, and, therefore, be able to do 
something of a cost/benefi t analysis of individual controls. (Of course, a lot of people have done a 
‚Äúquick and dirty‚Äù qualitative cost/benefi t analysis, and fi gured out that the cost of doing a quan-
titative risk analysis outweighs any possible benefi t in terms of the greater accuracy of your cost/
benefi t analysis.)
Undeterred, now that we are in an economic quagmire, the media has started to spin ROSI as 
the way to ensure that you get the most out of your security investment.
Columnists love fairy tales‚Ä¶
Intelligent Automation
Wait a minute, I thought automation, by defi nition, was supposed to involve some measure of 
intelligence?
Oh, but it‚Äôs a specialized form of automation.
First we have to go way, way back. Back to the days of random security technologies, when you 
had all kinds of diff erent security technologies. And they all had to be managed. Separately.
And then, oh joy, someone (either Marcus Ranum or Steve Bellovin, take your pick) invented 
fi rewalls! And we wouldn‚Äôt have to manage security anymore! And there was rejoicing!
Until we fi gured out that we were going to have to manage the fi rewalls.
And then someone invented Intrusion Detection Systems! And there was rejoicing!
Until we fi gured out that we were going to have to manage the IDS.
And then some marketing department invented IPS. And by this time, becoming jaded, we were 
asking questions. Like, what‚Äôs the diff erence between IDS and IPS. (Oh, really? An IPS prevents 

328 ‚óæ Information Security Management Handbook
a packet getting through, rather than just detecting it? Th en what‚Äôs the diff erence between an IPS 
and a fi rewall? Oh, really? An IPS is more intelligent? How so? Well, depends on which marketing 
department you ask. Th at‚Äôs what you get for using terms invented by marketing departments.)
But that ‚Äúintelligent‚Äù business seems to have had a bit of magic in it. We‚Äôve always had network 
monitoring, of one sort or another. For a long time we‚Äôve had tools to help us sort through our logs 
(after all, even IDS is only a form of real-time log analysis). And people have been trying to sell 
us all ‚Äúmanagement‚Äù systems, to help with the work of, well, managing all the security bits and 
pieces. So why not get a log analysis package, bolt on a few other items (maybe virus scanning or 
something), and call the whole thing ‚Äúintelligent!‚Äù
Hey, presto! A new marketing term!
Native Client
Google has garnered a lot of interest with its radically new idea, released under the name Native 
Client. You can read the announcement [4] and download the research paper (in PDF [5]).
Th at idea sounded so familiar I just knew it had to have been done before.
It has. It‚Äôs just a dressed up version of an activity monitor. Th e oldest form of antivirus technol-
ogy actually implemented. In fact, it dates back to the days just slightly before the fi rst PC viruses, 
when people were trying to prevent damage by some of the early PC trojans that were being shared 
on BBSs.
Or, if they take it far enough, and if you like, you can call it a form of virtual machine. And 
we are back to where we started this article.
Convergence
Or, converged communications, if you prefer.
I mean, c‚Äômon. We‚Äôve had VoIP for a while. (Before that we had H.323. Even before that we 
had Internet telephony, although it didn‚Äôt work all that terribly well.)
Of course, from our perspective in security, convergence is a great thing for job security. Just 
think, we can take all the problems we have in networking, and all the problems we have in tele-
phony, and roll them up into one big insecurity.
(Surprise, surprise: bad guys are breaking into home and small offi  ce VoIP PBXs and using 
them to make telemarketing calls [6].) (Most recently with Skype.) (Although don‚Äôt get me wrong: 
I‚Äôve nothing against Asterisk per se, and I‚Äôm sure it‚Äôs a great system if well managed.)
Baked in Security
Now, believe me, I have only the greatest of sympathy with the intent of this phrase. Yes, I agree 
that we‚Äôve been hamstrung and hampered by insecurities due to sloppy programming, and we 
desperately need to have more secure software development practices.
It‚Äôs just nothing new, that‚Äôs all.
I mean, we‚Äôve been preaching this for years. Decades, really. Ask any old programmer what he, 
she, or it was taught way back in the old days.
Structured programming. Top-down programming. Th e waterfall method.

Everything New Is Old Again ‚óæ 329
And documentation. I especially like internal documentation. If you don‚Äôt like documenta-
tion, you can have a moment of pity for my (occasional) programming students. When they hand 
in a project it has to have internal documentation in the source code, and it has to be clear and 
make sense. (Th ey lose marks if they don‚Äôt and it doesn‚Äôt.) As far as I‚Äôm concerned, if you can‚Äôt say 
what you are doing, you don‚Äôt know what you are doing.
And if you know what you are doing, you do it right.
Web 2.0
Or, social networking, if you prefer.
Let‚Äôs face it, the net is social. Th e money that went into creating computer networks, and the 
Internet itself, may have been intended for specifi c purposes, but as soon as it was there, people, 
being people, were being social.
As soon as the Internet was out of the test bed (and probably before that), and even before it 
was known as the Internet, people were using e-mail. A lot. For social things. What are the longest 
running Usenet ‚Äúnews‚Äù groups and mailing lists of any types? Lists of jokes and discussions of sci-
ence fi ction. Social stuff . (Yeah, the SF geeks are pretty antisocial, by ‚Äúnormal‚Äù standards, but for 
them this stuff  is the ultimate in sociability.)
So, what‚Äôs new? Oh, ‚Äúsocial‚Äù networks have the users generate content? What do you think 
mailing lists are? OK, blogs make it a bit easier to search archives. But archives of mailing lists have 
been around for a while too. (And this ‚Äúeasier‚Äù stuff  is highly subjective. Some blogs can be pretty 
diffi  cult to plow through in order to fi nd content of interest.)
And what about the Internet itself? It‚Äôs the last word in user-created content. Th e protocols 
and programs that run the net were primarily created by individual users, seeing something they 
wanted to do, and writing something that would do it. As Dave Clark famously put it, ‚ÄúWe believe 
in: rough consensus and running code.‚Äù
Works pretty good, doesn‚Äôt it?
VDI
Argh! YASMA! (Yet Another Stupid Marketing Acronym.) VDI pops up in my e-mail. And when 
I search for it (using two kettles worth of carbon emissions), what do I fi nd? ‚ÄúVirtual desktop 
infrastructure.‚Äù In other words, thin client, or cloud computing, or just plain virtualization.
It is to weep.
How to Spot the Next Big Thing that Spots Next Big Things
A new company is telling everyone which new companies are worth investing in [7]. Is this some-
thing we should get into?
‚ÄúTh e software measures the ‚Äòbuzz‚Äô surrounding a company via blogs and media reports along 
with a variety of factors including website traffi  c.‚Äù
We should all blog and Twitter about this.
Th en we should all blog about how blogging is so last year.

330 ‚óæ Information Security Management Handbook
Vulnerability Management
Yes, we have to know, assess, analyze, and manage vulnerabilities. Yes, it is a complicated task. So 
now this is the next big thing, is it?
Well, when you look at it, it is the same task we have always had to do under the name ‚Äúrisk 
management.‚Äù Except that it is in a smaller compass and more limited extent and application. 
Nothing particularly wrong with concentrating on one aspect at a time‚Äîas long as you realize 
that is what you are doing. Not thinking that you are somehow seeing something new.
Compliance
‚ÄúComply‚Äù used to be a verb. It used to mean that you would follow instructions.
Not any more. Now ‚Äúcompliance‚Äù is the important thing. You have to ‚Äúhave compliance.‚Äù 
(Preferably at least fi ve yards‚Äô worth.)
And what kind of compliance are we to have? Well, as Tanenbaum has said, the great thing 
about computer standards is that there are so many of them. And, if you don‚Äôt like those, there will 
be more next year. We have the ISO 27000 family (which basically says you have to do informa-
tion security). Th ere is PCI DSS (which basically says you have to do information security). Th ere 
is COBIT (with the middle three letters in small caps, which basically says you have somebody 
demonstrate that you are doing information security).
One of the defi nite biggies, recently, is the Sarbanes‚ÄìOxley Act of 2002 [8]. Th is act basically 
says that, if you have a company that is traded on the stock market, you have to tell people how 
much money you are making. (Th is allows people to bet on whether you will continue to make 
money. Th at‚Äôs what the stock market is all about: betting.) And it even says that you have to have 
internal controls on whether you are being accurate about how much money you are making. 
Section 404 (say, isn‚Äôt that the code for ‚Äúfi le not found?‚Äù) says that you have to have controls on 
your systems.
(A moment of thought for this ‚Äúinternal controls‚Äù business. If someone is going to lie about 
how much money they are making, what makes you think they won‚Äôt lie about whether or not 
they are telling the truth?)
What a radically new idea! Have controls on our information systems! (Hmmmm, no, wait, I‚Äôve 
heard the word ‚Äúcontrols‚Äù used before. It had something to do with doing information security‚Ä¶) 
Anyway, it‚Äôs a new idea to have it legislated, right? Well, not really. Way back in 1977, the Americans 
had this thing called the Foreign Corrupt Practices Act [9], which talked about the internal controls 
you had to have on your accounting books and systems. As a matter of fact, at about the same time 
the Committee of Sponsoring Organizations of the Treadway Commission created something called 
COSO (which stood for Committee of Sponsoring Organizations) that has been suggested as guidance 
for internal controls for doing information security. Th e banking industry even got into the game with 
the Basel II guidelines (which is kind of ironic in view of that Foreign Corrupt Practices business).
Etc.
Is crimeware new? Of course not. Even phishing is just using the same kind of login trojan as has 
been around for as long as we‚Äôve had login screens. Th e ever-so-modern cross-site-request-forgery 
(CSRF) is just a form of the man-in-the-middle attack that we‚Äôve known about forever. Recently, 

Everything New Is Old Again ‚óæ 331
I saw someone use the term ‚Äútime to protection,‚Äù which is just another way of saying plain old 
‚Äúwindow of opportunity.‚Äù Even endpoint security (or Network Access Control [NAC] or Data 
Leakage Protection [DLP]) isn‚Äôt new: 20 years ago Sophos had something called D-Fence that 
implemented the concept (although in a simpler, and therefore more robust, way).
Conclusion
Sorry, marketing guys. Th ere are new ideas out there. But you have to go and fi nd them. Not just 
dress up the same old ideas (that didn‚Äôt work before) and give them a new name.
About the Author
Robert M. Slade, CISSP, is a data communications and security specialist from North Vancouver, 
British Columbia, Canada.
References
 
1. http://www.ns.umich.edu/htdocs/releases/story.php?id=6666
 
2. http://www.eecs.umich.edu/fj group/cloudav/
 
3. http://www.virustotal.com/
 
4. http://google-code-updates.blogspot.com/2008/12/native-client-technology-for-running.html
 
5. http://nativeclient.googlecode.com/svn/trunk/nacl/googleclient//native_client/documentation/nacl_
paper.pdf
 
6. http://www.networkworld.com/news/2008/120608-fbi-criminals-auto-dialing-with-hacked.html
 
7. http://news.bbc.co.uk/go/em/-/2/hi/technology/7900463.stm
 
8. www.sec.gov/about/laws/soa2002.pdf
 
9. http://www.usdoj.gov/criminal/fraud/docs/statute.html


DOMAIN 
7
OPERATIONS SECURITY
Operations Controls


335
22
Chapter 
A Brief Summary of Warfare 
and Commercial Entities
Rob Shein
Th roughout past history, civilian commercial entities have not been the primary targets of warfare 
and have even been avoided as targets. In the earliest days when such groups existed, they did not 
make feasible targets in and of themselves. Such organizations existed within the physical bound-
aries of nation-states, such that attacks upon them could only be conducted within the scope 
of much larger, comprehensive attacks upon the nation-states themselves, or the castles/cities in 
which they were located. Th e concept of weakening an enemy by focusing on causing economic 
impact exclusive of signifi cant loss of life simply did not exist, and even if one were to focus mili-
tary eff orts on disruption of commercial activity, it inevitably involved a focus on killing civilians. 
In those days, the only means of warfare was kinetic warfare, using spears, swords, ballistic weap-
ons, explosives, and so on. Non-kinetic warfare, also known as cyber warfare, was not an option as 
there simply was no digital infrastructure through or against which to leverage attacks.
Contents
Non-Kinetic Warfare and Civilian Exposure ............................................................................ 336
Diff erentiation between Nation-State and Cybercriminal Actors ..............................................337
Addressing the Th reat: Private Organizations on the Front Line ...............................................337
Th e Other Side of the Coin: Cyber Partisans ............................................................................338
Regulatory Eff orts to Defend Critical Infrastructure ................................................................ 340
Information Warfare Doctrine: Th e View of Data as Both Sword and Castle ........................... 340
Th e Evolution of Technology and Impact on Vulnerability to Cyber Warfare .......................... 341
Addressing the Th reat .............................................................................................................. 342
Cyber Warfare as a Th reat to Small/Medium Civilian Organizations and Individuals .......... 343
Cyber Warfare as a Targeted Th reat for Non-State Causes ................................................... 343
Cyber Warfare as a Th reat to Organizations of Interest to Nation-State Actors .................... 344
Fear vs. Reality: Cyber Warfare in the Press and in Reality ....................................................... 344
About the Author .................................................................................................................... 346

336 ‚óæ Information Security Management Handbook
Trade, both between and within nation-states and cities, was conducted using material goods, 
which in and of themselves were transported by people. As a result, the notion of warfare even for 
the specifi c purpose of halting or impeding such trade necessarily involved direct attacks on civil-
ians. Th is fact remained in eff ect from the feudal era on until the late twentieth century, and as a 
result, by the time warfare (especially aerial warfare in general and bombing in particular) allowed 
for the capability to target commercial entities specifi cally, the Fourth Geneva Convention, pro-
vided some degree of protection. Th ere have been exceptions to the degree with which nations 
have followed the Geneva Conventions, but these exceptions have tended to stand out as just 
that: exceptional events, accidents (such as a bomber crew targeting the wrong building through 
genuine human error), or the misbehavior of nation-states that were judged to be barbaric for 
their actions. Despite these outlying events, the general fact has been that nations have sought 
to target counterforce (military) targets and avoid damage to countervalue (civilian) targets. As 
kinetic warfare has evolved, this diff erentiation has only grown; the advent of precision-guided 
munitions has reduced the civilian death toll from bombing raids to numbers so low as to have 
been unimaginable during earlier confl ict. Where once entire neighborhoods would be bombed 
in the course of attacks on a single building of military value, it is now considered a tragedy if a 
single civilian building is destroyed as a result of human error or incorrect information. In a sense, 
the protection of civilian industry was a benefi cial side eff ect of the Geneva Conventions, given 
the fact that one could not deliberately attack a commercial enterprise without physically harming 
or killing its employees. Such organizations also experienced reduced risk from their geographic 
distance from theaters of warfare, and their attack surface was relatively small compared to that 
of the military itself. A shop owner need not fear the destruction of his business by a war that was 
fought thousands of miles away.
In the late twentieth century, this began to change. Now, as the methods, processes, and doctrine 
around cyber warfare have evolved, the above-described world has nearly reversed itself. Attacks 
using non-kinetic means are nonlethal in nature, and do not even incur physical harm; as such, the 
Fourth Geneva Conventions do not apply. Furthermore, while the IT infrastructure of the military 
is often sequestered (with varying success, admittedly) into enclaves, private industry is heavily inter-
connected with a great deal of exposure to the digital world and all of its inhabitants.
Non-Kinetic Warfare and Civilian Exposure
Th ere exists a larger problem with the evolution of non-kinetic warfare as a form of low-intensity 
confl ict during peacetime. While non-kinetic warfare off ers the potential for impact without 
loss of life, it also broadens the battlefi eld in a fashion that has not been seen since the advent of 
the airplane. Even worse, it has extended the theater of combat to organizations that have never 
before been responsible for defending themselves against nation-state aggressors. Most confl ict 
on the globe is considered ‚Äúlow intensity,‚Äù meaning that it takes the form of guerrilla warfare, 
insurgency, special operations, and other such means. Even current wars between the United 
States and its enemies in Iraq and Afghanistan may be considered this, from the perspective of its 
enemies, since they themselves do not engage in large military maneuvers on defi ned fronts. Th e 
days of two large armies amassing their forces to face off  on a battlefi eld with clear battle lines are 
no more, except between two smaller powers in a regional confl ict of only local interest.
What this means in broader terms is that the world‚Äôs major powers have an incentive and model 
through which to conduct non-kinetic warfare against potential adversaries, even in peacetime. 
Between themselves, this category of nation-states typically participate in low-intensity confl ict 

A Brief Summary of Warfare and Commercial Entities ‚óæ 337
through clandestine operations and special warfare to avoid becoming enmeshed into full-fl edged 
confl ict, and the additional deniability that inevitably comes from information warfare makes 
cyber warfare an attractive means of confl ict. Furthermore, the overwhelming military superiority 
of the United States‚Äîin terms of kinetic warfare‚Äîprovides an equally overwhelming incentive 
for smaller nations to adopt cyber warfare for other reasons. Simply put, cyber warfare provides an 
economically cheap means of asymmetric warfare that is unlikely to incur a conventional military 
response from a much larger power.
Differentiation between Nation-State and Cybercriminal Actors
Th ere are several things about cyber warfare that diff erentiate it from hacking related to other 
motivations. Originally, hackers (or ‚Äúvintage hackers,‚Äù as they shall be described here) were people 
with extraordinary expertise and talent, but typically benevolent motivations. It was not uncom-
mon for a hacker to notify the sysadmin of a compromised system as soon as a hack was successful, 
both informing them of the way they gained access and of how to prevent it in the future. Th e key 
motivation was a quest for knowledge and greater expertise, combined with a lack of a legitimate 
outlet for their skills. While their actions were unquestionably illegal, there nonetheless existed 
a consistent morality to these individuals, and they rarely caused the havoc they were capable of. 
Later came the time of the ‚Äúscript kiddie,‚Äù once Internet access became commonplace, hacking 
tools became more widespread, and a far lesser degree of skill was needed to break into vulner-
able systems. Th ese individuals lacked the expertise or moral fi ber found in their predecessors, 
typically defacing Web sites with profane messages just to gain bragging rights. Dealing with this 
group has been little more than a matter of implementing best practices for security, as the threat 
posed by them has proven to be particularly sophisticated. Most recently, criminal organizations 
have adopted hacking as a means toward generating revenue through extortion, embezzlement, or 
identity theft. Th is threat has been gaining in sophistication and scope, and still poses an evolving 
challenge to both individual people and private organizations.
A nation-state leveraging off ensive cyber warfare with hostile intent, however, embodies the 
worst aspects of all three groups. Th e sophistication and expertise of the vintage hacker, the indis-
criminate scope of the script kiddie, and the targeted hostile intent to maximize damage of the 
cybercriminal combine. In addition, cyber warfare units of military and intelligence organizations 
are furnished with unprecedented resources. Th e vintage hackers and script kiddies both did their 
work on a shoestring budget; while criminal organizations are better funded, they still have lim-
ited resources plus a signifi cant need to avoid capture and prosecution. A nation-state‚Äôs off ensive 
cyber warfare assets, however, have plentiful resources and training, and no fear of criminal pros-
ecution for their acts. Th ey operate within save enclaves from which they have little fear of facing 
retribution for whatever they may do. Th e morality of their acts is typically limited to that of the 
government they serve; as two of the more sophisticated cyber warfare actors are North Korea and 
China, this is a chilling thought indeed.
Addressing the Threat: Private Organizations on the Front Line
So, the question becomes this: what can be done about managing the risk imposed by these develop-
ments? Even the largest multinational private companies have never had more than a limited capability 
to address the challenges of warfare, even when operating in confl ict regions. Smaller organizations are 

338 ‚óæ Information Security Management Handbook
still grappling with the threat imposed by cybercrime and nuisance hacking, neither of which typically 
represents the same degree of threat posed by a motivated nation-state actor. In contrast to information 
about typical hacking elements, where information is plentiful and openly available, information about 
the true capability and intent of cyber warfare elements is typically classifi ed and not available for public 
consumption. So, while fi nancial organizations and entities that process large number of credit card 
transactions have been properly forewarned that they are being targeted by criminal organizations, for 
example, other industries may not be aware of the fact that they are being targeted by the operators of 
other countries for reasons not directly related to their core business.
Fortunately, while the motives and degree of sophistication possessed by attackers may vary, 
the nature of vulnerability does not. Technical vulnerability to one form of attack is the same 
regardless of whomever may seek to exploit it. Th e challenge is that a more sophisticated and 
determined actor will leverage vulnerabilities in combination to greater eff ect while more capably 
evading detection. Additionally, while major private organizations (like members of the Fortune 
100) will likely themselves be targeted directly and subjected to the full brunt of an attack, smaller 
organizations need only be more secure than the norm to avoid signifi cant attacks.
Th e governments of many nations are aware of this new form of risk that their citizenry now 
faces, and steps are being taken in an eff ort to manage the risk. Th e current Obama administration 
in the United States is making bold moves toward a national policy to improve the cybersecurity 
in the private sector, for example. How eff ective such measures will be has yet to be seen, as the 
organizations and individuals tasked with such things have historically been given few tools with 
which to aff ect any true measure of change. Even within civilian government, positions typically 
tasked with responsibility for cybersecurity on a broad scale have lacked any kind of budgetary 
control, therefore rendering them incapable of imposing or facilitating change.
The Other Side of the Coin: Cyber Partisans
Despite the fact that many nation-states have invested signifi cant resources into developing their 
off ensive cyber warfare capabilities, to date most activities seem to have been carried out by sym-
pathetic civilians, apparently with little more than tacit and indirect support from the nation 
on whose behalf they act. Examples of this include the coordinated attacks by Russian citizens 
(some not even located in Russia) on Estonia and Georgia in 2008. Chinese military doctrine 
allows for and welcomes this manner of leveraging civilian actors, and the People‚Äôs Liberation 
Army has even carried out military cyber warfare exercises involving use of these units. Th is 
has tended to keep the impact of attacks limited, either in terms of duration or in terms of 
strategic impact, and denies the attackers the benefi ts of state-funded vulnerability research 
related to exotic technologies like Smart Grid/advanced metering infrastructure (AMI), embed-
ded devices, or SCADA environments. An appropriate description of the potential role of such 
civilian hackers acting in support of a nation-state is of partisans. In the case of cyber warfare, 
they are able to act in the nation of their enemy, unlike partisans of the more traditional form 
in kinetic warfare who act as an insurgent resistance. Otherwise, the metaphor holds, as these 
‚Äúcyber partisans‚Äù strike at targets of opportunity from the digital woods to foment disorganization 
and chaos among those they consider to be the enemy.
Th is concept is not so new, in a manner. In 2001, after a Chinese fi ghter collided with a U.S. 
electronic surveillance plane off  the coast of the Hainan Peninsula, a ‚Äúhacker war‚Äù erupted between 
Chinese and United States‚Äìfriendly hackers. In that day and age, the distributed denial-of-service 
attack was relatively new and infrequently employed; instead the method of choice was Web site 

A Brief Summary of Warfare and Commercial Entities ‚óæ 339
defacement. Vulnerabilities still abounded among public Web servers, cybercrime had not come to 
fruition, and there was still great social value placed within the underground hacking community 
on bragging rights gained by defacing random Web sites. Ironically, in hindsight, it is considered 
likely that this confl ict did not develop organically, but rather as the result of the questionable 
journalism of Michelle Delio, who authored a story in Wired magazine claiming that Chinese 
hackers were preparing to unleash attacks upon the United States in retaliation for American hege-
mony. Acting in response to the article, Western hackers attacked Chinese sites, and thus the ‚Äúwar‚Äù 
began. Later, Delio stated that the war was over‚Ä¶ and as simply as that, the exchanges seemed to 
cease. (It is worth noting that Delio was later exposed as having manufactured information for a 
number of her articles, and is no longer used by Wired magazine as a contributor.) ‚ÄúJericho‚Äù of the 
organization Attrition.org titled the whole aff air with the phrase, ‚ÄúWag the Delio,‚Äù at a presenta-
tion given at the Black Hat Briefi ngs. Other examples of mass Web site defacements in support 
of one national cause or another relate to Pakistani interests, the Palestinian‚ÄìIsraeli confl ict, and 
curiously enough a mass dispute in South America over which country produces the best ‚Äúpisco,‚Äù 
which is a liquor distilled from grapes in both Peru and Chile. Th e same ease with which nation-
states attack each other with cyber warfare also translates to irate private citizens with the desire to 
overreact on a global scale. It may well be that unlike traditional terrorists, such people have not 
endured more than annoyance by the general populace because there has yet to be any signifi cant 
loss of life, or other reason to see them as more than a nuisance. As would be expected given the 
trivial basis for some of these exchanges, the true motivation is to infl ict damage on information 
technology assets, with the nationalist motivation merely providing a thin veneer of legitimacy 
in the eyes of the attacker. Unfortunately, as cyber warfare increasingly becomes a tool of formal 
nation-states, the problems posed by such people will become more apparent.
One of the great challenges posed by the acts of such individuals is the diffi  culty they add to 
the task of attributing attacks to nation-state entities. When one cannot be sure that an attack is 
committed by a nation, or simply on behalf of that nation by sympathetic elements, it becomes 
diffi  cult to wield the tools of international accountability. Even more troubling is that such peo-
ple could conceivably trigger larger confl icts or adversely aff ect ongoing diplomatic negotiations 
between nations, since they cannot be assumed to recognize the true and full impact of their acts. 
Just as peace negotiations have often been disrupted or even halted by a single act of aggression 
using kinetic violence, so might similar discussions be threatened by an act of cyber warfare, 
particularly since the aggressors would be unlikely to claim responsibility for their acts. It is not 
diffi  cult to imagine a regime that is trying to appease two diff erent constituencies going to the 
negotiating table, but also fomenting such attacks so as to avoid alienating too many of its support-
ers; the mere possibility of such a thing in (for example) Pakistan would be enough to jeopardize 
talks with other nations should an independent group carry out such an attack on their own, even 
without even tacit approval from their own country.
Two recent examples highlight how tenuous the connection can be between the individual 
hackers and the countries on whose behalf they act. In the recent attack on Georgia by pro-
Russian activists, one of the two coordinating Web sites behind the attacks (www.stopgeorgia.ru) 
was actually hosted by a small Russian company that, in turn, had leased its server from a London 
shell company which operated out of a mail drop; that company is owned by a Russian national 
living in the Netherlands which leased a server block from a major hosting and services fi rm in 
Texas. Th e participants of the forum themselves were from multiple nations, apparently joined 
by little more than Russian patriotism and the ability to wreak havoc with Georgian networks 
through the use of distributed denial-of-service attacks and vulnerability research. Finally, within 
these forums, there was a ‚Äújourneyman-apprentice‚Äù approach, whereby the more experienced and 

340 ‚óæ Information Security Management Handbook
capable hackers took on a leadership role, and subordinate tasks were meted out to less seasoned 
actors. In a situation like this, assigning responsibility, blame, or criminal liability is an obvious 
nightmare.
Another way in which such behavior clouds matters is where the actors for cybercrime and 
cyber warfare overlap. It follows that a populace that is highly represented with regard to criminal 
operations like the establishment and operation of botnets, illicit online activities, and cyber-based 
fraud will also serve just as well for cyber warfare. Th e problem that arises is in the motivation that 
some governments have in protecting such enterprises to some degree, so as to maintain a capabil-
ity for later use should the need arise.
Regulatory Efforts to Defend Critical Infrastructure
Th ere is one way in which government agencies and industry coalitions can aff ect change to 
cybersecurity in the private sector. Regulatory standards related to security can be developed and 
implemented as a driver toward greater security. In many cases, the standards act mostly to cre-
ate a driver for funding and support of cybersecurity within organizations; in other cases, they 
provide guidance as to best practices and requirements to get to a more secure state. Th ere have 
been few regulatory standards yet that have much direct relevance to the threat posed by cyber 
warfare, but one such set of standards is put forth by the North American Electricity Reliability 
Corporation (NERC). A particular subset of the NERC standards, known as NERC Critical 
Infrastructure Protection (or ‚ÄúNERC CIP,‚Äù as the standards are known), focus on information 
security for critical assets related to the generation, management, and transmission of electricity 
in North America.
NERC CIP is comprised of nine standards, CIP-001 through CIP-009. CIP-001 is rarely 
discussed, as it merely describes the need for a process whereby sabotage is reported to the appro-
priate entity within the Department of Energy. But CIP-002 through CIP-009 cover the gamut 
of information security practices, from user security awareness training and personnel security to 
backup procedures. Currently in its fi rst iteration, NERC CIP is currently succinct and lacking in 
specifi c details or guidance on many topics. A new iteration is currently in development, which is 
expected to refl ect a radical shift in methodology toward a framework based around NIST stan-
dard 800‚Äì53. In addition, the scope of NERC‚Äôs requirements is expanding to include additional 
forms of power generation and transmission assets.
Information Warfare Doctrine: The View 
of Data as Both Sword and Castle
Th e concept of attacking cyber infrastructure using logical attacks is not diffi  cult to grasp, but 
simply performing such attacks for their own sake fails to elevate one‚Äôs eff ect (or relevance) above 
that of the chaos-inducing cyber partisans discussed earlier. Th e true benefi t of any form of war-
fare lies in its integration with other forms. Th is is an already established doctrine in terms of 
kinetic warfare doctrine, whereby troops on the ground move after aerial attacks have severely 
damaged enemy emplacements, which in turn were fi rst observed using various reconnaissance 
and intelligence-gathering methods. While attacking, the troops have the ability to call upon 
artillery strikes, close air support, or armor to support their mission. Th is is known as ‚Äúcombined 

A Brief Summary of Warfare and Commercial Entities ‚óæ 341
warfare,‚Äù and is the norm on today‚Äôs battlefi eld. But what happens when the concept of informa-
tion‚Ä¶ both as a weapon and as an objective to be attacked or captured‚Ä¶comes into play? Th e 
Chinese People‚Äôs Liberation Army (PLA) has been a pioneer in thought around this question, and 
while their doctrine is still evolving they are remarkably open in their thinking, at least to those 
who can read Mandarin.
Th ere exist two primary objectives that compete for primacy in the context of information 
warfare. One is the control of information, either in the sense of gaining access to it or denying 
access to it. Th e other is infl uence over that information. Th e two concepts may sound vague and 
unrelated to warfare until one considers the way in which they can be applied. For example, deny-
ing access to information could take on the form of using logical attacks to cause an air defense 
system‚Äôs radar to lie; if the enemy cannot perceive the intrusion into its airspace of an invading 
force, that becomes a remarkable tactical advantage to the invader as it would provide obscurity 
about the scale and composition of the attack while maintaining total surprise until the last pos-
sible minute. If the same eff ect were to be sought using kinetic warfare (i.e., bombing the radar 
installations) then the element of surprise would be lost, and the only benefi t would be denial of 
information about how the attack was progressing at the early stages. To apply the alternate objec-
tive (infl uence over information) would be to cause the radar systems to false positive at times, 
showing things that are not there. Eventually, the information produced by the radar systems 
would be considered so unreliable as to be nearly worthless, thus degrading the quality of decisions 
made based upon that data. Th is seems like the lesser of the two approaches until one recognizes 
that it is far easier to make fake objects show up on a screen than it is to selectively hide the ones 
that you wish to keep hidden.
Most notable is that even formal Chinese information warfare doctrine does not distinguish 
between countervalue and counterforce targets in terms of escalation. It is not considered a more 
aggressive act to attack a bank or other civilian target (countervalue) than it would be to restrict 
the scope of an attack to military targets (counterforce), for example. In fact, the result of this 
aspect of doctrine tends to favor attacks against private organizations for the numerous reasons 
listed earlier. Furthermore, this reality has been acknowledged by leading members of the Chinese 
cyber warfare community on many occasions.
The Evolution of Technology and Impact 
on Vulnerability to Cyber Warfare
As a result of recent spikes in the price of petroleum products, attention focused on energy conserva-
tion. Fortunately, a number of technologies have recently come into maturity to address such a need, 
and as a result new phrases have started appearing in the vocabulary of the news: ‚ÄúSmart Grid,‚Äù 
‚ÄúAMI,‚Äù ‚ÄúSmart Metering,‚Äù ‚ÄúDemand Response,‚Äù and so on. Th ese refer to a set of enabling technolo-
gies, which provide the ability to do things that were never before possible with the power grid:
AMI: Uses ‚Äúsmart meters‚Äù that monitor electrical usage at individual homes in 15 min 
increments (instead of the 3 month increments that are the current norm) and throttle 
power consumption by noncritical devices like air conditioners and dishwashers in 
response to spikes in demand (or unforeseen drops in power generation). Th ese com-
municate back to the central power utility company via wireless protocols of various 
forms, and communicate with devices inside the home using protocols like ZigBee. 

342 ‚óæ Information Security Management Handbook
Th ese meters have a feature called ‚Äúremote disconnect,‚Äù which permits a utility to 
toggle power delivery to a home or offi  ce with a command sent to the meter, saving 
cost and time needed to handle disconnects and reconnects. Th is feature also facili-
tates services like payment in advance for power, much like a prepaid cell phone; when 
the balance runs low, the household is alerted so that they may top off  their balance 
with more funds.
Smart Grid: Refers to a set of technologies that provide additional control capabil-
ity to the power grid using devices like ‚Äúreclosers‚Äù (which can remotely force tripped 
power connections to be reestablished) and capacitors. By leveraging the information 
gleaned from AMI, Smart Grid allows routing of power around downed lines and 
better management of the power generated by uncertain sources of generation (like 
windmills), among other things.
Th ere are obvious security ramifi cations to replacing current power meters (which are mechanical 
in design) with computerized systems that have the ability to turn appliances off  while they report 
back wirelessly to a power company. Th ese implications have also been covered by the media, 
including one story where a vulnerability in key management for a specifi c brand of AMI meter 
was discovered. Unfortunately, there is more hype than truth to the discussion of these vulnerabil-
ities at the moment. Even more unfortunate is that sooner or later, signifi cant vulnerabilities will 
likely be uncovered. Either way, this expansion of information technology into a realm of infra-
structure provides new opportunities for attackers to wreak havoc from afar. By gaining access to 
the ‚Äúhead end‚Äù system of an AMI infrastructure, which accepts data from and sends commands 
to the meters, it would be possible to trigger a mass disconnect of tens or even hundreds of thou-
sands of meters simultaneously. Such an event is called a ‚Äúmass load-shedding event,‚Äù and would 
cause an outage similar in both nature and scale to the power outage suff ered in the Northeastern 
United States in 2003.
Th e good news is that the potential for abuse of these technologies has not gone unnoticed. 
A number of organizations have sprung up to address security with Smart Grid and AMI solu-
tions, and the off erings put forth by some vendors are also quite promising. Standards around 
communications and data security, a taxonomy for defi ning security domains within AMI infra-
structure, and a vibrant working group dedicated to the discussion of security requirements all 
exist and are proving to be viable in addressing the risk. Th e power grid will remain a target of 
interest to hostile actors, and successful breaches have occurred outside the United States, but the 
picture is not nearly as gloomy as it could be, and it is getting better as time passes.
Addressing the Threat
While single private organizations have few options against a determined cyber warfare attacker 
(above and beyond proper information security practices), as stated earlier it will be uncommon 
for a foreign actor to be focused specifi cally on any single company in particular. Instead it ends up 
being more like the joke about two men running from a bear, where the punch line states ‚ÄúI only 
need to outrun you.‚Äù Th ere is a great deal of protection aff orded the fact that nation-states rarely 
take such bold action unless there is a specifi c and deliberate reason, and globalization greatly 
narrows the potential number of reasons to attack a corporation or civilian organization of any 
signifi cant size (while smaller ones are quite unlikely to pose much interest to foreign nations at 
all). And while the behavior of cyber partisans is not so measured and restrained, they are largely 
not of great impact unless they band together and work in concert.

A Brief Summary of Warfare and Commercial Entities ‚óæ 343
Which brings the threat to three diff erent forms. One, an organization that is smaller, rela-
tively immature in information security measures, and thus useful as a stepping stone in attacks 
on other organizations. Th e second is of organizations that, for some reason, have gained the atten-
tions of groups with nationalist, environmental, or other motivators. Th e third contains organiza-
tions which themselves are tightly linked to national drivers and infrastructure. Examples of this 
third group include defense contractors, fi nancial institutions, and public utilities.
Cyber Warfare as a Threat to Small/Medium 
Civilian Organizations and Individuals
Within the fi rst group, as stated above, the primary goal of an attacker would be merely to gain 
a foothold in their infrastructure for the facilitation of attacks on other organizations. Th is tac-
tic is nothing new, and a more granular form of it takes place even within well-secured larger 
organizations. Incident response teams have noticed that many attackers choose to take control 
of relatively unimportant IT assets, and remain dormant until the time comes to exploit the 
control they already have. Th is takes place only when there is suffi  cient cause for them to reveal 
the penetration and tip their hand. Th e same can and does happen on a national scale, where 
smaller, less-defended environments are used for the staging of attacks against more vigilant 
targets; this allows for some degree of obfuscation regarding the source of the attack, and adds 
fl exibility should the intended fi nal target notice an attack and start blocking the networks from 
which it originates. Th us, in a time of open, no-holds-barred cyber warfare between any two 
factions, this segment of the population would be more heavily hit than normal, both in terms 
of the number of attacks and the eff ect of already-compromised machines being more heavily 
leveraged to perform attacks.
Cyber Warfare as a Targeted Threat for Non-State Causes
Th e second group involves a greater risk of facing a determined attacker, but still lacks the risk 
inherent in a coherent, well-coordinated attack by a large or well-supported group. Still, an attacker 
who is bound and determined to bring harm to or gain entry to a target is far more dangerous 
than one who is merely looking for a target of opportunity. Th ese organizations will tend to be 
larger, and thus better protected, but not themselves useful targets for cyber warfare. In the event 
of greater confl ict between nations, however, the equation changes for organizations within this 
group. Th e largest of multinational corporations are themselves tightly bound to foreign nations 
with signifi cant cyber warfare capabilities for outsourcing and manufacturing; this almost pro-
vides a kind of hostage situation whereby an attack upon them would inevitably (and quickly) 
incur harm upon the attacking nation. Companies from Accenture to General Electric to General 
Motors all rely heavily upon their operations in other nations. Th is, combined with the ways in 
which our economies interact, would not only serve to cause any harm to be shared by both the 
attacker and the target, but in some cases would actually cause far greater harm to the attacker‚Äôs 
economy. In the recent worldwide downturn, it has become apparent just how slim a margin the 
economic powers of Asia have been maintaining in their fi ght to compete globally; once things 
slipped backward even a small amount, that margin was eliminated and disaster ensued on a 
regional scale. Th e same event would be triggered by a successful and devastating attack on a large 
multinational corporation by China, for example, except in this case only China would suff er 
the impact, and the other countries in the region (particularly Taiwan) would actually benefi t, as 
they would pick up the slack. Th e phrase, ‚ÄúGlobalization stops wars,‚Äù is at least as true with cyber 

344 ‚óæ Information Security Management Handbook
warfare as it is with kinetic warfare. For those few nations who possess a signifi cant cyber warfare 
capacity but lack signifi cant economic ties to the rest of the world (the best example of this is 
North Korea), their lack of economic ties is accompanied by both a lack of large-scale connectiv-
ity to the rest of the world and a lack of large groups of motivated actors in other countries. As a 
result, an attack by such a nation would be easily stopped merely by severing the links between 
that nation and the rest of the Internet.
Cyber Warfare as a Threat to Organizations 
of Interest to Nation-State Actors
Th is fi nal group has the most to fear from cyber warfare, given that they comprise organizations 
that would be specifi cally targeted by the disciplined, well-resourced actors of nation-state enti-
ties. Fortunately, these are also typically organizations that have had to face a signifi cant and 
sophisticated threat model to begin with for other reasons (e.g., the same fi nancial organizations 
that would be attacked by another nation for countervalue economic impact tend to have a lot 
of money, and have always been targeted for purposes of theft and fraud) and, therefore, have 
highly evolved defensive capabilities. In addition, this segment of the population holds the fewest 
members, has the highest level of collaboration with offi  cials in the intelligence and defense sec-
tors, and is likely to get the quickest and most eff ective response from government agencies in the 
event of an attack. In some situations (particularly organizations that have some degree of overlap 
with the defense industry) attacks on this segment of the population would fall under the defi ni-
tion of counterforce attacks. In even rarer cases, these organizations are already adept at defensive 
cyber warfare operations, since they already provide services to the military and government in 
that capacity. So, while the potential threat to this group is the greatest, they are also far better 
prepared than any other component of private industry.
Fear vs. Reality: Cyber Warfare in the Press and in Reality
On a fi nal closing note, it is wise to discuss the diff erential between what is likely to occur and 
what some reports in the popular media envision in terms of cyber warfare and how it would 
be conducted. Some of the visions come from books or fi lm, and, therefore, cannot be seriously 
faulted; after all, these are venues of entertainment, not education. But news media tends to fol-
low similar plotlines in their conceptualization of cyber warfare, to the detriment of popular 
perception and, eventually, eff orts to prepare for the future. So this chapter will close with a bit 
of debunking.
Th e fi rst basic rule of cyber warfare is this: cyber warfare rarely causes new things to occur. 
What instead is more probable is that an attack, at most, could cause something minor that 
happens occasionally to happen a great deal at once, either in terms of scale or in terms of 
frequency. Th e challenge there is that since these are things that can happen for other reasons, 
there are usually already ways to prevent them or mitigate their impact. Th e classic example of 
this concept is the ‚Äúgreen lights in all directions‚Äù idea. Th is has both shown up in popular fi lm 
and in the dire warnings of ‚Äúexperts‚Äù on the topic of cyber warfare. Th e idea is that as more cit-
ies integrate networks over their municipal operations, including stoplights and traffi  c manage-
ment systems, a hacker could take control of the network and cause traffi  c lights to show a green 
signal in all directions at once, causing car crashes. Th e truth of the matter is that many things 

A Brief Summary of Warfare and Commercial Entities ‚óæ 345
have been proven to cause a traffi  c signal to attempt to do this, like corrosion, rodent infestation, 
human error in installation, electrical failure, and so on. As a result, the circuitry of these traffi  c 
light controllers is designed with an inherent failsafe. Should the controller attempt to display 
a pattern that would be considered dangerous (like all green lights in all directions), it fails into 
a failsafe mode, with blinking yellow or red lights in all directions. Anyone with signifi cant 
driving experience has seen this phenomenon. Getting control of the signal controller via the 
Internet will not override this circuitry; it is inherent to the wiring of the signal itself, to make 
it as reliable a failsafe as possible.
Another fallacy pertains to the production of food. One example was the warning that hackers 
could take control of the machines involved in the production of children‚Äôs cereal and increase the 
amount of iron being put into the food until it would be toxic. Th is also fails as a threat when one 
considers the real-world situation, and how such an event would actually play out. For one thing, 
the additive in food used to provide supplemental iron (iron sulfate) is used in trace amounts nor-
mally; to poison someone with it, the amount would have to be increased by multiple orders of 
magnitude. Consider also the fact that iron sulfate is dark green in color. Th e cereal would have a 
rather peculiar appearance, which I doubt would go unnoticed by people working at the produc-
tion facility, much less the child presented with a bowl full of the stuff . Even more interesting is 
the odor and fl avor of iron sulfate, which is not in the least bit appetizing. Getting a child to eat a 
bowl full of cereal laced with enough of it to harm him would only be possible in a household so 
draconian that the child would probably be tough enough to eat barbed wire and still survive the 
experience in the fi rst place. And fi nally, it would not go without notice that the mixing machines 
would be going through iron sulfate at an unprecedented rate, requiring refi lls thousands of times 
more often than normal. So again, when one considers the operational world in which this attack 
would need to be successful, one can see that it would have very limited chances of success on any 
level, for a wide number of reasons.
Another fabled attack that is coming to the forefront in the news is the notion that attackers 
could take the entire Internet off -line. Oddly enough, this one has more truth to it than fi ction, in 
that such a thing is conceivable. Testifying before the United States Senate in 1999, ‚ÄúMudge‚Äù of 
the group L0pht stated that he could take the Internet down within approximately 30 min. Th is 
was later borne out to be entirely plausible, as 4 years later a series of vulnerabilities in a protocol 
called BGP (Border Gateway Protocol) were revealed. Th ere were earlier indications of these fl aws, 
going back to that same year when a person known as ‚ÄúBatz‚Äù (who was a friend of Mudge) gave a 
talk on ‚ÄúSecurity Issues Aff ecting Internet Transit Points and Backbone Providers,‚Äù during which 
he detailed how attacks using BGP could result in the rerouting or even denial of traffi  c routing 
between major components of the Internet. An analogy would be the global destruction of all 
points of travel across bodies of water, mountains, desert, or other impassable terrain.
Th e problem with this scenario was detailed by Mudge in the sentence that followed the proc-
lamation that hackers could demolish the Internet so quickly. He posed the simple question of 
asking why they would do such a thing, and sever the links to rich sources of information instead 
of exploiting them. And this point still holds true today. While most of the issues surrounding 
BGP have been addressed, there are probably other issues waiting to be found. But for a cyber 
warrior to ‚Äútake down‚Äù the Internet makes little sense; it would be like an invading army blowing 
up a bridge that still lay before them.
In some cases, for brute force reasons (such as the attacks on Estonia and Georgia by hackers 
sympathetic to Russian causes), a limited version of this may be performed against a single nation, 
but for larger countries with signifi cant connectivity to other parts of the world, such a thing is 

346 ‚óæ Information Security Management Handbook
not feasible without causing numerous eff ects to friendly networks. Th e amount of traffi  c needed 
to perform a denial-of-service attack against the entire United States, for example, would cause 
backscatter traffi  c that would more than overwhelm the rest of the Internet, including the net-
works of the attacking country. Furthermore, the earlier-described economic interdependence of 
nations provides a strong disincentive to perform this kind of attack at all. And above all else, the 
cyber warfare doctrines of all companies with suffi  ciently advanced capabilities to perform such 
an attack would instead dictate that they exploit access to resources, rather than cut off  the ability 
to continue to do so.
About the Author
Rob Shein is a cyber security architect for HP‚Äôs Security and Privacy Professional Services divi-
sion, where he provides security consulting to a wide range of clients in the private and public 
sector.

347
23
Chapter 
Information Destruction 
Requirements and Techniques
Ben Rothke
Th e inability to discard worthless items even though they appear to have no value is known as 
compulsive hoarding syndrome. If the eccentric Collyer brothers had a better understanding of 
destruction practices, they likely would not have been killed by the very documents and newspa-
pers they obsessively collected.
While most organizations do not hoard junk and newspapers like Homer and Langley Collyer did, 
they do need to keep information such as employee personnel records, fi nancial statements, contracts 
and leases, and more. Given the vast amount of paper and digital media that amasses over time, eff ec-
tive information destruction policies and practices are now a necessary part of doing business and will 
likely save organizations time, eff ort and heartache, legal costs, as well as embarrassment and more.
In December 2007, the Federal Trade Commission (FTC) announced a $50,000 settlement 
with American Mortgage Company of Northbrook, Illinois, over charges the company violated 
the FTC‚Äôs disposal, safeguards, and privacy rules by failing to properly dispose of documents con-
taining consumers‚Äô credit and personally identifi able information. In announcing the settlement, 
the FTC put all companies on notice that it is taking such failures seriously.
A $50,000 settlement might seem low when measured against the potential for fi nancial harm 
to individuals as a result of the company‚Äôs negligence, but in addition to the negative PR for 
Contents
Every Organization Has Data Th at Needs to Be Destroyed ..................................................... 348
Just Trash It All: Th e Enron Approach ..................................................................................... 349
Regulatory Issues ......................................................................................................................350
Hard Copies Should Be Destroyed on a Formal and Regular Basis .......................................351
Security Containers .............................................................................................................351
In-House or Outsource ........................................................................................................352
Conclusions .............................................................................................................................353
About the Author .....................................................................................................................353

348 ‚óæ Information Security Management Handbook
American Mortgage, the settlement includes an obligation to obtain an audit, every 2 years for the 
next 10 years, from a qualifi ed, independent, third-party professional to ensure that its security 
program meets the standards of the order. Any similar failures by this company during the next 
decade will be met with more severe punishment. Th at, indeed, is a very costly lesson.
In today‚Äôs litigious environment, there are a plethora of aggressive lawyers that would love to 
devour your organization for failure to take due care around document and media destruction.
Th is chapter looks at the key areas to ensure that your organization does not fall prey to such 
lawyers when it comes to the physical destruction of documents and records.
Every Organization Has Data That Needs to Be Destroyed
Besides taxes, what unites every business is that they possess highly sensitive information that 
should not be seen by unauthorized persons. While some documents can be destroyed minutes 
after printing, regulations may require others to be archived from a few years to permanently.* But 
between these two ends of the scale, your organization can potentially have a large volume of hard 
copy data occupying space as a liability, both from a legal and information security perspective.
Depending on how long you have been in business, the number of physical sites and the number 
of people you employ, it is possible to have hundreds of thousands, if not millions, of pages of hard 
copy stored throughout your company‚Äîmuch of which is confi dential data that can be destroyed.
Th e National Association of Corporate Directors provides some excellent guidelines in their 
Record Retention and Document Destruction Policy. From trademark registrations, safety records, 
to retirement and pension records and much more, there is a lot that needs to be retained. But once 
that retention period is over, much of those documents can be destroyed. Below is a partial list‚Ä† of 
the types of information that absolutely should be shredded when no longer needed:
Account Records
Activity Sheets
Advertising
Applications
Appraisals
Bank statements
Bids and quotes
Budgets
Business plans
Canceled checks
Client lists
Contact lists
Corporate tax records
Correspondence
Customer records
Disciplinary reports
Educational reports
Expense reports
Financial statements
Forecasts
Formulas, product 
plans, and tests
General service 
information
Health and safety 
reports
Internal reports
Legal documents
Lottery tickets
Magnetic media
Maps and blueprints
Marketing plans
Medical records
Microfi lm/
microfi che
New product 
information
Payroll documents
Performance 
appraisals
Personnel fi les
Plastic credit and ID 
cards
* Some basic records retention schedules can be found at http://www.shredquick.com/pdfs/Records_Retention_
Schedule.pdf
‚Ä† From http://www.shredquick.com/whatshred.asp

Information Destruction Requirements and Techniques ‚óæ 349
(continued)
Account Records
Activity Sheets
Advertising
Applications
R&D reports
Sales forecasts
Specifi cation 
drawings
Strategic reports
Strategies
Supplier PO‚Äôs
Supplier reports
Supplier 
specifi cations
Test scores/class 
rosters
Training information
Treatment programs
Encryption key 
management 
information
Besides the regulatory and ethical issues around keeping those hard copies secure, the reality 
is that many of your competitors would love to get their hands on the documents that you are 
throwing out. And even if your competitors are not combing through your dumpsters, others may 
do so and attempt to sell your secrets to your competitors.
For those who think that dumpster diving is a security threat of the past, check out Steve 
Hunt‚Äôs fascinating video Scoring big in corporate dumpster diving. He recently did a dumpster 
dive in Chicago and found confi dential wire transfer information, a laptop, and others treasures in 
the dumpster. His adventure took all of 3 min and he astutely advises companies to do their own 
dumpster diving tests.
In addition, the current recession means that organizations may have to deal with disgruntled 
and angry employees as well as those who think their job or company will soon be eliminated. 
With that, the risk of misuse of sensitive information is even greater.
Simply put, eff ective document destruction practices prevent information from falling into 
the wrong hands. Perhaps the most pervasive example of this is credit card charge receipts, which 
are retrieved from trash bins by dumpster divers often with the intent of using the information for 
online or telephone orders. Many businesses discard such payment information without eff ective 
destruction controls. If such controls are not used, the information unearthed from the post-fraud 
investigation could be extremely embarrassing to explain to customers, and it could also turn into 
a PR nightmare or an expensive legal problem.
Just Trash It All: The Enron Approach
Once made aware of the need, many organizations take a knee jerk reaction by gathering all stored 
hard copies and simply disposing of them. But that does not solve the problem for a number of 
reasons.
First, there are legal and regulatory requirements that mandate that paper documents be retained 
for specifi c periods of time. Additionally, throwing things directly into the dumpster exposes com-
panies to dumpster divers. As detailed above, dumpsters can be a great source of information.
Th ere is another reason why the trashing of daily records without appropriate destruction is 
dangerous. If you simply throw out trash and it gets into your competitors‚Äô hands, they can easily 
correlate and learn about your business activities.
By way of example, SIM software can take seemingly disparate log items and correlate them 
into an active attack; so too with your trash. Your daily activities are similarly manifest in your 

350 ‚óæ Information Security Management Handbook
trash. From daily activities, phone records, travel plans, RFP submissions, memos, and much 
more, your business can be exposed if this information is not properly destroyed.
If Enron is the poster child for inappropriate document destruction, those organizations seek-
ing to do document destruction precisely should consider obtaining the Media Disposal Toolkit 
from Network Frontiers. Th e toolkit contains everything an organization needs to know about 
data disposal. It includes a spreadsheet of unifi ed common controls, work breakdown structure 
with processes and procedures, and a data deletion management documentation on the policies 
and standards that organizations must adhere to in order to be in compliance with global regula-
tory mandates.
Regulatory Issues
Various regulations must be taken into consideration also. For example, Sarbanes‚ÄìOxley (SOX) 
addresses the destruction of business records and documents and turns intentional document 
destruction into a process that must be carefully monitored. If the process is not followed, execu-
tives can fi nd themselves under indictment. Having formally documented data retention, policies 
are a requirement.
SOX raises the legal stakes for destruction of corporate documents and includes numerous 
provisions that create and enhance criminal penalties for corporate fraud and obstruction of jus-
tice. SOX section 1102 makes it a crime, punishable by fi ne and imprisonment for up to 20 years, 
to corruptly alter, destroy, mutilate or conceal a record, document, or other object with the intent 
to impair the object‚Äôs integrity or availability or use in an offi  cial proceeding or to obstruct or 
impede an offi  cial proceeding. SOX section 802 states that ‚Äúwhoever knowingly alters, destroys, 
mutilates, conceals, covers up, falsifi es, or makes a false entry in any record, document, or tangible 
object with intent to impede, obstruct, or infl uence the investigation or proper administration of 
any matter within the jurisdiction of any department or agency of the United States‚Ä¶ or in rela-
tion to or contemplation of any such matter or case, shall be fi ned under this title, imprisoned not 
more than 20 years, or both.‚Äù
Another relevant regulation around disposal is the Fair and Accurate Credit Transactions 
Act of 2003 (FACTA). Enacted in June 2005 requires businesses and individuals to take 
appropriate measures to dispose of sensitive information derived from consumer reports. Any 
business or individual who uses a consumer report for a business purpose is subject to the 
requirements of the Disposal Rule, a part of FACTA that calls for the proper disposal of infor-
mation in consumer reports and records to protect against unauthorized access to or use of the 
information.
Th e rule applies to people and both large and small organizations that use consumer reports, 
including consumer reporting companies, lenders, insurers; employers; landlords; government 
agencies; mortgage brokers, car dealers; attorneys; private investigators; debt collectors; individu-
als who pull consumer reports on prospective home employees, such as nannies or contractors; and 
entities that maintain information in consumer reports as part of their role as a service provider to 
other organizations covered by the rule.
A benefi t of having a formal document destruction process and using product such as the 
Media Disposal Toolkit is that since you are doing document destruction properly, your organiza-
tion does not have to worry about every new regulation, as such practices are likely compliant with 
whatever new regulation comes out.

Information Destruction Requirements and Techniques ‚óæ 351
Hard Copies Should Be Destroyed on a Formal and Regular Basis
Imagine you are the manager of a large medical practice, which is being sued after 10,000 pages of 
medical records found their way into the hands of an investigative reporter or thief. When asked 
by the plaintiff ‚Äôs lawyer how you get rid of hard copies, an answer such as ‚ÄúLenny the computer 
guy does it whenever he can‚Äù is akin to pleading guilty. In contrast, ‚ÄúWe have an outside bonded, 
National Association of Information Destruction (NAID) certifi ed company empty our secu-
rity containers and shred the contents on a weekly basis‚Äù will likely shield you from signifi cant 
liability.
Th e issue also is not necessarily how often the data is destroyed; rather, whether it is done on 
a formal basis, based on risk factors specifi c to the organization. As part of eff ective oversight, a 
formal system of information destruction must be created and implemented. If data destruction 
is indeed performed in a formal, documented manner, and your destruction schedule is done on 
a scheduled basis, the plaintiff ‚Äôs lawyers will have much less to use, which could likely be judged 
positively by a jury.
Two good examples of formalized procedures are the confi dential document handling pro-
cedures from Purdue University and the Iowa State University document destruction operating 
plan. A Google search will give you many more, which you can use as a base for your program.
One of the most important aspects of a formal plan for information destruction is consistency. 
If an organization is inconsistent in what it destroys, this shows a lack of due diligence, in addition 
to the appearance of attempting to hide something.
As part of this formal process, realize also that there are many elements to data destruction 
that must be built into the process. One of them is the concept of a data destruction moratorium. 
Th e reason for this is that there are times when an organization must stop its data destruction 
activities. If a legal discovery request is received, policies must be in place to ensure that all orga-
nized and periodic data destruction activities must immediately be placed on hold until the Legal 
Department determines whether these destruction activities jeopardize sought-after data.
As to a formal process, there was a company that used a goat as their document shredder. 
While perhaps eff ective from a shredding perspective, it is clearly not a best practice approach, 
nor is it likely their lawyers signed off  on that method. A goat eating away at paper is fi ne for the 
farside, but has no place in a formal document disposal process.
Security Containers
As the need for information destruction has caught on, the ubiquitous security containers from 
companies such as Shred-it are found in many organizations. It is a good idea to have such con-
tainers readily available so staff  can easily dispose of information that is no longer needed.
Containers generally come in three sizes:
Executive consoles
 
‚óæ
: Generally used in high-profi le environments. Th ey have front loading 
which frees up the top space for offi  ce equipment and the doors swing open for easy removal 
and can be keyed alike. Approximate measurements are 40‚Ä≥ √ó 19‚Ä≥ √ó 19‚Ä≥.
Large containers
 
‚óæ
: Ninety six gallon security containers are used for heavy document produc-
tion centers, purging sites, warehouses and high-traffi  c offi  ces are especially popular for 
overfl ow conditions. Approximate measurements are 43‚Ä≥ √ó 24‚Ä≥ √ó 37‚Ä≥. Th ey have the capac-
ity to hold up to 15 boxes of paper.

352 ‚óæ Information Security Management Handbook
Bulk containers
 
‚óæ
: Used for larger production centers, areas that generate large quantities of 
confi dential data and some e-scrap material. Approximate measurements are 38‚Ä≥ √ó 43‚Ä≥ √ó 
29‚Ä≥ and can accommodate up to 650 + lbs of material.
As part of a security awareness program, make sure that employees are trained in the proper dis-
posal and destruction of sensitive materials. You want to make sure that employees place papers 
in these designated locked destruction containers and not in trash bins, recycle bins, or other 
publicly accessible locations. Also, make sure that they do not place materials that do not need to 
be shredded in these bins. Since many destruction companies charge by the bin or pound, placing 
documents in these bins that do not need to be shredded is a waste of money.
Some organizations use these secure information containers only for sensitive, but not highly 
confi dential or secret information. Some organizations have polices that require highly confi den-
tial or secret information, because it is so sensitive, to be immediately destroyed. Th is lessens the 
risk that someone could break into a locked destruction container, or even steal the whole con-
tainer and then break into it at another location.
In-House or Outsource
Document destruction, like other services, can be done in-house or outsourced. Which is the best 
way to go? Like every decision, the correct answer is the proverbial‚Äîit depends.
Th ere are two predominant types of shredding services available: plant-based (off -site) and 
mobile (on-site).
Mobile-based shredding
 
‚óæ
: Mobile shredders have the actual shredders on the truck itself. 
Mobile shredding companies provide bins or consoles for their customers and on scheduled 
days, the truck arrives at the place of business and the customer service representative (CSR) 
collects the bins, or console bags, takes them to the truck, and shreds the material on the 
customer‚Äôs premises. After completion, the CSR will typically leave a certifi cate of destruc-
tion. Since the shredding operation is done on the customer‚Äôs property, it is assumed to be 
more secure since nothing leaves unshredded. Often the customer will board the truck to 
ensure their sensitive material is indeed being destroyed.
Plant-based shredding
 
‚óæ
: Th is is a typical off -site service where the plant has large industrial 
shredders. On the scheduled day, the CSR collects the bins or console bags, places them in 
his secure truck, and transports them back to the remote plant where the bins are unloaded 
into a secured area. Th e collected bins are later staged for shredding, which can occur days 
later. Some view this as an insecure method since the documents may be left unattended. 
One other major caveat is that plant-based shredders may sort the material to maximize 
its recycling value which can put your organization at risk. Some of these off -site shred-
ding companies are simply glorifi ed recycling companies that get top dollar for recycling 
paper, your paper. Since their staff  will sort the documents, they have the opportunity to 
take them. So before you choose a plant-based service, make sure you investigate them 
accordingly.
When dealing with an outsourcer, ensure that they are NAID certifi ed. NAID is an indepen-
dent organization that certifi es destruction companies. Its certifi cation program checks a shredding 
company‚Äôs compliance in 22 critical areas, including everything from shred size to employee back-
ground checks. When it comes to something as critical as information destruction‚Äîcaveat emptor. 

Information Destruction Requirements and Techniques ‚óæ 353
Unscrupulous shredding companies will claim to be NAID certifi ed just to get your business. Make 
sure to ask for a copy of their NAID Certifi ed certifi cate as proof of their standing.
So what it depends gives you the right solution? Th ere are potential security issues with both 
solutions. Mobile shredding is done with the CSR alone there and since the CSR is alone on the 
truck, they may have access to your confi dential material.
With a plant-based approach, various plant employees have access to the material during the 
sort process. A paper sorter could conceal a sensitive document on his person and leave the prop-
erty with it.
Th e bottom line is that either solution requires an amount of trust, but the fi nal decision must 
be customer-based on what they feel the most secure solution is. Th is decision, like most, is a trade-
off  between the level of security and cost.
A third solution is to do it yourself. While this may seem cheaper in the short term, it can often 
be more expensive. And if you do it internally, there must be policies and procedures to ensure that 
destruction of sensitive information must be performed only with approved destruction methods 
including shredders or other equipment approved by the information security department.
Irrespective if you use a mobile-based shredding or a plant-based shredding service, ensure that 
the service provider is NAID certifi ed and that all documents are secured until they are destroyed. 
A good service level agreement (SLA) is to make sure documents are completely destroyed within 
24 h and a certifi cate of destruction is provided upon completion of this process.
Conclusions
It is clear that document destruction in today‚Äôs world must be part of a good system of business 
processes.
But the bottom line is that if your organization is not careful about what they do not dispose 
of, it could become your competitors‚Äô good fortune and your worst corporate nightmare.
About the Author
Ben Rothke, CISSP, CISM, PCI QSA, is a New York City‚Äìbased senior security consultant with 
BT INS, Santa Clara, California.


DOMAIN
 
8
BUSINESS CONTINUITY 
PLANNING AND 
DISASTER RECOVERY 
PLANNING
Business Continuity Planning


357
24
Chapter 
Integrated Business 
Continuity Planning
James C. Murphy
Contents
Challenges in Responsibility Scope and Terminology ...............................................................361
Chapter Preview ...................................................................................................................... 362
Overriding Perspectives ........................................................................................................... 363
Human Safety Is Most Important ....................................................................................... 363
Process/Service Continuity Is the Emphasis ........................................................................ 364
Information Is the Focus ..................................................................................................... 364
Disasters Are No Longer Local ........................................................................................365
Organizational Responsibilities.................................................................................................365
Executive-Level Coordination ............................................................................................. 366
Mandatory Internal Collaboration ...................................................................................... 366
Strategic Planning ............................................................................................................... 366
Operational/Tactical Planning .............................................................................................367
Plan Maturity Measurement ................................................................................................367
Initial Plan Creation ....................................................................................................... 368
Plan Testability/Changeability ........................................................................................ 368
Plan Integration ............................................................................................................. 368
Integrated Plans as Part of Organizational Identity ......................................................... 368
Organizational Assessment ...................................................................................................... 369
Business Characterization ................................................................................................... 369
Existing Documented Internal Business Continuity Plans/Policies/Procedures .....................370
Existing Internal Business Continuity Responsibility Structure ............................................370
Existing External Relationships ............................................................................................370
Integrated Business Continuity Management Plans/Processes ...................................................371
Coordination Plan ...............................................................................................................372
Strategic Components .....................................................................................................372
Tactical Components ......................................................................................................373

358 ‚óæ Information Security Management Handbook
Th is chapter is written for information security professionals charged with creating or improving 
the existing business continuity plan (BCP) within his or her organization. Th is chapter assumes 
that such professionals have a general working knowledge of the information security common 
body of knowledge, and yet may need a supplementary document detailing the general concepts. 
Th e emphasis within the chapter is less on what are the components of the BCP and the specifi c 
organizational task forces or committees, but on how to accomplish the necessary preparations 
and tasks.
Business continuity management (BCM) planning is a body of knowledge that does not lack 
for resources. Table 24.1 is only a sampling of available documentation and plans that are available 
(albeit obtaining many of the standards documents require a fee). Th e list in this table is intended 
only as a short beginning; dozens of other sources and sets of documentation are available. Th ough 
there are many organizations contributing to the professionalism of BCM planning, two have 
great history and presence, the Disaster Recovery Institute International (DRII) and the British 
Continuity Institution (BCI). Both have robust certifi cation programs and have contributed sets 
Communications Plan .........................................................................................................373
Strategic Components .....................................................................................................373
Tactical Components ......................................................................................................373
Risk Management Plan ........................................................................................................374
Strategic Components .....................................................................................................374
Tactical Components ......................................................................................................376
Safety and Emergency Response Plan .................................................................................. 377
Strategic Components .................................................................................................... 377
Tactical Components ..................................................................................................... 377
Internal Incident Management Plan.....................................................................................378
Strategic Components .....................................................................................................378
Tactical Components ......................................................................................................379
Process Continuity Plan .......................................................................................................379
Strategic Components .....................................................................................................379
Tactical Components ..................................................................................................... 380
Information Continuity Plan .............................................................................................. 380
Strategic Components .................................................................................................... 380
Tactical Components ..................................................................................................... 383
Summary of Planning ......................................................................................................... 384
Crisis Onset and Response .......................................................................................................385
Crisis Recognition .............................................................................................................. 390
Organizational Impact ........................................................................................................ 390
Scope Impact ...................................................................................................................... 390
Declaration ..........................................................................................................................393
Emergency Response ...........................................................................................................393
Incident Management ......................................................................................................... 394
Information Continuity ...................................................................................................... 394
Process Continuity .............................................................................................................. 394
Recovery Termination ......................................................................................................... 394
About the Author .....................................................................................................................395

Integrated Business Continuity Planning ‚óæ 359
Table 24.1 Selected Business Continuity Management Resources
Organizations
Business Continuity Institute, http://www.thebci.org/
Disaster Recovery Institute International, https://www.drii.org/
Government plans
National Incident Management System, 2008, http://www.fema.gov/pdf/emergency/nims/
NIMS_core.pdf
National Response Framework, 2008, http://www.fema.gov/pdf/emergency/nrf/nrf-core.pdf
The National Strategy for Pandemic Infl uenza Implementation Plan, 2006, http://pandemicfl u.
gov/plan/federal/pandemic-infl uenza-implementation.pdf
National Infrastructure Protection Plan, 2009, http://www.dhs.gov/xlibrary/assets/NIPP_Plan.
pdf
Treasury Board of Canada Secretariat, Operational Security Standard, Business Continuity 
Planning (BCP) Program, http://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=12324
Public Safety Canada, Emergency management, http://www.publicsafety.gc.ca/prg/em/
index-eng.aspx
International standards
DRII‚ÄîProfessional Practices for Business Continuity Planners, https://www.drii.org/
professionalprac/index.php
BCI‚ÄîGeneral Practice Guidelines, http://www.thebci.org/gpg.htm
NIST SP 800-100, Information Security Handbook: A Guide for Managers, 2006, NIST SP 800-34, 
Contingency Planning Guide for Information Technology Systems, 2002, http://csrc.nist.gov/
publications/PubsSPs.html
ASIS International‚ÄîOrganizational Resilience: Preparedness and Continuity Management 
Best Practices Standard, http://www.abdi-secure-ecommerce.com/asis/ps-907-37-1842.aspx
National Fire Protection Association 1600: Standard on Disaster/Emergency Management and 
Business Continuity Programs, 2007, http://www.nfpa.org/aboutthecodes/AboutTheCodes.
asp?DocNum=1600
BS 25999-1:2006: Business continuity management, Part 1: Code of practice, BS 25999-2:2007: 
Business continuity management, Part 2: Specifi cation, http://www.pas56.com/index.htm
Standards Australia‚ÄîHB 221:2004, Business Continuity Management, HB 292-2006, 
A practitioners guide to business continuity management, http://www.saiglobal.com/shop/
script/PortalBusiness.asp
Canada Standards Association Z1600, Emergency management and business continuity 
programs, http://www.shopcsa.ca/onlinestore/GetCatalogDrillDown.asp?Parent=4773
Singapore: Technical Reference 19: 2005: Business Continuity Management, http://psbcorp.
com/BCM2007.aspx

360 ‚óæ Information Security Management Handbook
of practices and guidelines that have become de facto industry standards. Th e U.S. government, 
through the Department of Homeland Security (DHS) has established a set of national plans 
and a framework that has great potential for off ering a controlled regional response to crises of all 
kinds. Th e Canadian government also has a similar set of plans and structures. Several standards 
are available for use, and more are underway.
In July of 2008, the U.S. DHS made an agreement with ANSI-ASQ National Accreditation 
Board (ANAB) to implement certifi cation requirements for the private sector. Th is was based on 
Title IX of Public Law 110-53 Implementing the Recommendations of the 9/11 Commission Act of 2007 
(http://www.fema.gov/news/newsrelease.fema?id=45280). Th e ultimate result will be an accreditation 
and certifi cation program for U.S. private sector organizations. Based on that, ASIS International 
and the British Standards Institution announced plans for developing an American BCM Standard. 
Th e fi rst meeting for drafting the plan was held in January, 2009, in Alexandria, VA (http://www.
asisonline.org/guidelines/guidelines.htm), and the draft is under development and review.*
Standards are foundational for a security professional to develop a framework for the BCM 
framework. Appropriate compliance will off er assurance to the organizational stakeholders (and 
stockholders) that the framework will meet industry-wide expectations for safety, security, and 
prompt attention to organizational process restoration. In attempts to address an extensive range of 
issues and conditions, standards off er an idealized approach to BCM planning, and often business 
professionals will assume that a selected standard document actually depicts a single, comprehen-
sive plan that can be introduced, established, and implemented as a single organizational eff ort. 
Unfortunately, those engaged in this process may assume the need to start from scratch with the 
selected standard, building a set of plans and structures according to the standard. Such an eff ort 
may inadvertently disregard existing organizational documentation and processes that are vital to 
the ultimate BCP, and which may result in duplications of eff ort or redundant plan components.
Th erefore, in addressing compliance, security professionals need fi rst to be aware of any exist-
ing plans and work eff orts that will aff ect or be aff ected by the larger BCP. Th ey also need to adapt 
the components of the standards to meet the specifi c organizational characteristics and require-
ments in order to make the BCP usable within the organization. Standards may be comprehensive 
and detailed, but often the standards documentation does not provide the depth of detail and 
pragmatic approaches necessary to tailor the plan to the organization. Th is usability is the basis for 
the design of this chapter. A reasonably trained, organizationally experienced security professional 
can hopefully adapt the concepts within this chapter to craft an appropriately targeted plan.
A large organization‚Äîcharacterized by a campus of several buildings and possibly several 
locations across a community or the country, and with a workforce size of several hundred to 
several thousand‚Äîwill have opportunity for more distribution of the responsibilities of a BCP 
into separate plans and committees. Smaller organizations‚Äîcharacterized by a single or very few 
locations in one (or perhaps less than one) building, and with a workforce size of a few dozen to 
a few hundred‚Äîwill have fewer committees and plans but no less responsibility for the sake of 
the future of the organization and its relationships. Small organizations which lease or rent part 
(or all) of a building will have to coordinate some of the response plans and sequences with the 
building owner/manager, but the responsibilities and expectations of the lease contract pertaining 
to crisis response should be clarifi ed and documented as part of the planning.
Every business initiative deserves to be under scrutiny for cost benefi ts and savings, and BCM 
planning must be included. An effi  cient BCM planning process can not only enable recovery and 
* Full disclosure‚Äîat the time of writing this chapter, this author is a reviewer of the U.S. BCM Standard development 
process as administered by ASIS and BCM.

Integrated Business Continuity Planning ‚óæ 361
restoration after major and minor crises, but it can also drive a top-to-bottom review of practices, pro-
cesses, underlying information dependencies, and the supporting technical infrastructure. An existing 
business process that is not easily recoverable for any number of reasons may be a target for reduction or 
elimination. While not a major decision factor, process recoverability may prove to be a new consideration 
for existing and new business initiatives. Another suggested new consideration may be data retention 
effi  ciency. In part because storage technology has become much more aff ordable, organizational data 
and information has begun to accumulate beyond the terabyte into the petabyte range. Such capacity is 
proving to be more and more diffi  cult to back up routinely. With additional pressures from e-discovery 
requirements, eliminating unnecessary stored data will undoubtedly enhance data recoverability.
Th e plan process itself must also be effi  ciently conceived and administered, especially as 
it becomes a repeatable, cyclical program, fi nding its place among other major organizational 
 initiatives. Let it not be unsaid that a well-crafted BCM plan will more than prove its worth to an 
organization during and after a major crisis, and the absence of such a plan will almost certainly 
mean the demise of the organization. Let it also not be unsaid that a BCM plan is a vital  component 
of any organization that seeks to protect its information assets through time and in anticipation of 
interruptions, large and small. A BCM plan is not simply a control for residual risk.
Challenges in Responsibility Scope and Terminology
Most information security professionals have understood by experience and training that the recovery 
of technology and restoration of systems and data (the classic disaster recovery planning) was primarily 
the responsibility of the technical or security organizational units. Within the past decade, as the exec-
utive level has been confronted with requirements for more visible involvement with data protection, 
other organizational units have become involved. Information auditors are being trained to administer 
business continuity processes, sometimes including the information protection and recovery. Because 
of recent major regional crises, safety and emergency response professionals are being trained to address 
disaster recovery from a hazards management perspective. In fact, all three professional groups‚Äîaudi-
tors, information security professionals, and emergency response professionals‚Äîare being trained to 
assume responsibility for establishing and managing a command and control center at the onset of a 
major crisis. Without integrated planning, coordination, and communication, the tragic scene of each 
of the three groups claiming the center stage in a crisis could be more real than imagined. Th is vitally 
important collaboration is the primary driver behind the details of this chapter.
Even the use of the term security professional can lead to confusion, as this author discovered recently. 
At a recent conference of health and safety professionals, I presented a seminar encouraging cooperation 
between ‚Äúsafety and security professionals‚Äù in which I intended to introduce concepts of information 
security and off er points of potential collaboration. To my surprise at the apparent frustration of some 
of the attendees, I was informed that some of the audience expected the presentation to address safety 
and police or guard professionals! I strongly suggest as we professionals involved in protecting data 
interact with diverse audiences that we identify ourselves as information security professionals!
Th e term disaster recovery has long been associated with the recovery of and restored access 
to the computer, network, information components, and business data systems of an organi-
zation. As the awareness of the need increased for a recovery plan for the larger organization, 
BCM planning became the inclusive term, with disaster recovery as one of the subordinate 
plans. However, many organizations with a strong physical security or safety component have 
long associated the terms disaster, crisis, and emergency as near synonyms in making plans for 
response and recovery, which has often led to problems in planning and coordinating disaster 

362 ‚óæ Information Security Management Handbook
recovery programs. I suggest that the term disaster recovery should be given back to the safety/
emergency response organizations and business units, and that the historic single process be 
divided to emphasize the appropriate ownership and responsibilities. I suggest process continu-
ity, to describe the restoration of business processes that rely on information components. Th is 
is within the scope of responsibility of organizational unit data owners. Information continuity 
describes the restoration of access to systems, servers, and networks that support those business 
processes as more appropriate and descriptive of the actual eff orts.
Th ere is also a potential problem in the use of the term incident. For several years within 
the information security domain, and the context of information security and BCM planning, 
the term incident is associated with an event (which can be any interaction with the informa-
tion technology (IT) and resources) that produces a negative impact. Such an impact can be a 
compromised password, a stolen laptop, a successful Internet attack, the presence of an unrec-
ognized person, and many others. Th at defi nition is relatively small in scope, and is intended 
to encourage the prevention and detection of activities before they precipitate larger crises. Th e 
problem is that certain national and international business continuity initiatives have begun 
to use the term  incident to be nearly synonymous with crisis or emergency. Th e U.S. DHS has 
created the massive and well-crafted National Incident Management System (NIMS) and the 
system‚Äôs inclusive components, e.g., incident action plan, incident command post, incident 
command system, and incident management team‚Äîall using the defi nition of incident to mean 
crisis or emergency (see Table 24.1). Also, the National Fire Protection Association (NFPA) 1600 
Standard on disaster/emergency management and business continuity programs (see Table 24.1) 
uses the term incident in the same context as the NIMS. Other BCM standards and planning 
organizations use the same context for the term incident (e.g., BS 25999-1, Code of Practice]. 
And, safety professionals are adopting the same terminology of the NIMS (e.g., American 
Society of Safety Engineers publication Emergency Incident Management Systems, http://www.
asse.org/cartpage.php?link=emergency).
Even though information security professionals have defi ned and used the term incident for an 
arguably longer time than the business continuity and risk management professionals, the over-
whelming usage of term defi nitions by the business continuity professional domain, may prove 
that the momentum is moving away from the original information security concept, and recon-
ciliation of the defi nition will be diffi  cult. CERT (Web site link) uses the term computer security 
incident response, which may be the appropriate alternative. I suggest another alternative, since 
computer security incidents are not the only incidents that require attention: internal incident (e.g., 
internal incident management), in order to distinguish the concept from the usage by the business 
continuity domain, retain the familiar information security referential usage with little modifi ca-
tion and include the broader scope of internal problems that could lead to larger problems.
Chapter Preview
Overriding Perspectives
Organizational Responsibilities
Organizational Assessment
Integrated BCM Plans/Processes
Crisis Onset and Response

Integrated Business Continuity Planning ‚óæ 363
Th is chapter has fi ve major sections, beyond the Introduction, each with several subsections. Th e 
fi rst section, ‚ÄúOverriding Perspectives,‚Äù gives an indication of some of the concepts or emphases‚Äî
human safety, information access restoration, and process restoration. Also increasingly, major 
disasters are no longer local to individual organizations, but require interaction among local and 
regional organizations and control centers. In the second section ‚ÄúOrganizational Responsibilities‚Äù, 
I describe the importance of executive participation‚Äînot simply approval and funding. And, the 
integrated participation throughout all units of the organization is vital. For large organizations, 
no single organizational unit can perform all the responsibilities of the BCM plan alone, although 
the BCP needs to have a single point of origin and contact.
In the third section ‚ÄúOrganizational Assessment,‚Äù I emphasize capturing the core orga-
nizational characteristics and the existing documentation and organizational structures as 
an effi  cient means of beginning the BCM planning. Th is will also include documenting the 
breadth of external relationships including local community neighbors and regional crisis 
recovery groups, as well as business relationships. In some organizational units, plans may 
already exist that address specifi c continuity processes. Within the fourth section, ‚ÄúIntegrated 
BCM Plans/Processes,‚Äù I present the details of the individual plans and how they can be 
integrated. I describe the plans and responsible business units as conceptual. Not all orga-
nizations will be the same size or have the same  structure, and some may combine processes 
and planning eff orts under a single business unit. Security professionals need to be more 
concerned about addressing the concepts than whether named committees and plans match 
those documented here.
Because most eff orts described here are plans, I suggest that all plans include a strategic (long 
range, 5‚Äì10 or more years) and a tactical (within one business year or season) aspect, and suggest 
some simple measures of maturity for the plans. Within each of the plans, I indicate the respon-
sible organizational unit, and any teams or special units created by the plans. In response to the 
growing concerns internationally for pandemic events, including naturally spreading diseases and 
bioterrorism, I integrate components to address these needs within the appropriate plans.
Finally, in the section ‚ÄúCrisis Onset and Response‚Äù, I suggest a sequence of cascading events 
for an actual crisis response. Th is sequence will also identify the points of integration among the 
plans and components.
Overriding Perspectives
Business Continuity Management Planning has context!
Overriding Perspectives
Human Safety Is Most Important
Process/Service Continuity Is the Emphasis
Information Is the Focus
Disasters Are No Longer Local!
Human Safety Is Most Important
Th e human component is arguably the most important resource of any size organization. Without 
doubt, therefore, the most important perspective in developing the organizational BCP is human 

364 ‚óæ Information Security Management Handbook
safety and protection. Th is includes not only the workforce of the organization, but the extended 
families of the workforce individuals, and the people in the near vicinity of the organization who 
may be aff ected by emergency circumstances originating within the organization. Th is is certainly 
apparent to manufacturing or large healthcare organizations, where the understanding for com-
pliance to regulatory mandates for workforce or care recipients permeates throughout the life of 
the organization. Th is should also be apparent to organizations of all sizes and industry sectors. 
Most large organizational safety units will already have plans and processes for emergency recog-
nition, response, and evacuation. Many organizations may not have considered planning for work 
activities in the absence of a facility. Some large organizations may be in a position to provide vital 
temporary shelter for their own workforce and for nearby individuals as well.
After intense, destructive disasters, many of the organizational workforce may be lost in the 
disaster, and many others may relocate from the area of the disaster. Plans must account for 
the loss of workforce after large disasters.
Pandemic diseases and bioterrorism activity may result in mandatory quarantine episodes 
where workforce members may be compelled to stay away from the organizational buildings or 
campus, or some may be trapped by circumstances within the buildings.
Process/Service Continuity Is the Emphasis
Proactive organizations have developed change management strategies for relocation, expansion (or 
reduction), mergers, and acquisitions. Each of these activities include process continuation plans dur-
ing the changes. Within a crisis situation, the process continuity planning is similar, but response and 
implementation must be at a much more rapid pace. Th ese plans developed under the larger umbrella 
BCM plan must emphasize defi nitive and comprehensible steps for restoring how the organization will 
reorganize and restart vital business processes. Although most of the processes will have an information 
component, some may not actually involve data or information, but the steps still need to be defi ned. 
Th e steps must be clear for workforce members who may be new to the organization, and they also 
must defi ne working in temporary locations and include manual capabilities, if necessary.
Process continuity planning may off er opportunities for identifying eff orts that may need reen-
gineering for better recoverability. If a defi ned process is tied to outmoded technology or represents 
duplication or redundancy or is considered no longer critical, process consolidation or elimination may 
be the answer. It will be important to address any organizational processes and services which may be 
part of a chain of eff orts or vital service on which other organizations and communities depend.
Information Is the Focus
In developing the larger BCP, the focus on planning must be on the organizational data and 
information. Without the workforce, the organization will have a prolonged recovery; without 
the data and information, the organization will not recover! Regulatory requirements for health 
and safety of the workforce, such as OSHA or HIPAA, warn that the inability to produce 
health and safety information on demand can result in fi nes, even if the organization has suf-
fered loss from a major disaster.
Data and information defi ne the organization, regardless of the primary products or services 
provided as income sources. Protecting the organizational information is of highest importance, 
and that protection should be part of every business cycle, not just as a part of long-term plan-
ning. Minimally, organizations must enable data backups to remote locations, a practice that has 
been standard in all IT management units for decades. Successful BCM planning also includes 

Integrated Business Continuity Planning ‚óæ 365
contingencies for replacement of organizational technology‚Äîservers, workstations, and networks. 
However in the aftermath of recent large regional disasters in many parts of the world, it has been 
seen that the restoration of technology may take much longer that anticipated, because the com-
petitive demand across the region will be extremely high. At that time, recovery contracts with 
technology vendors may be useless! Such a bottleneck in the supply of technology will undoubt-
edly skew previously documented planned time objectives for recovery and restoration of business 
processes. Even more consequential was the loss of workforce, not necessarily from injury or death, 
but from permanent departure.
Th erefore, it cannot be understated that it is of highest importance that business data and infor-
mation must be the primary target of a successful BCP. Even if restoration of technology hardware 
is much slower than anticipated and even if the workforce is much reduced and takes longer to 
restore, as long as the historic and current (at least to the date of the last backup) information is 
still available and protected in a remote location, then the organization will have a much better 
opportunity to return to viability.
Disasters Are No Longer Local!
Most historic BCM planning have concentrated on the organizational building or campus, but it 
is clear from recent crises that other organizations as well as local and regional communities are 
aff ected by the disaster or crisis at an organizational location. A crisis on a single fl oor of a building 
can block access to other untouched tenants in the same building. A fi re in a single building can 
certainly spread to other buildings in an urban setting. Organizations that manufacture or store 
hazardous chemicals or other products constitute a threat to local communities if the crisis is a fi re 
or a fl ood that can disseminate dangerous products.
In the United States, the recent crises over the past several years has resulted in National 
Infrastructure Planning, and FEMA has established extensive training for individuals and pro-
fessionals in how to respond to regional crises. State and local governments have also established 
homeland security programs that mandate appropriate planning among government agencies. 
Several local nongovernment organizations have brought the concepts of emergency planning to 
schools and private family homes. Private organizations are also being encouraged to develop plan-
ning relationships with these public and community groups. Organizational BCM planning must 
include the contact information for the government and community eff orts and make allowances 
for including some of the contacts in regional planning.
Organizational Responsibilities
No single organizational unit can do it all.
Organizational Responsibilities
Executive-Level Coordination
Mandatory Internal Collaboration
Strategic Planning
Operational/Tactical Planning
Plan Maturity Measurement

366 ‚óæ Information Security Management Handbook
Executive-Level Coordination
Many large organizations have entered into the BCM planning by setting aside an internal 
committee to pull all the pieces and documentation together in a grand eff ort that often takes 
many months. Sometimes the committee is led by a consultant, either an individual or a team. 
Depending on the size of the organization and the existing status of some of the supporting plan-
ning and documentation, this type of eff ort may be necessary. However, in most medium to large 
organizations, it should not be assumed that the responsibilities for planning, implementing, and 
participating in the creation, evaluation, and testing of the BCP can be accomplished by a single 
committee or at least in a reasonably acceptable time frame.
All BCP standards (no exception) will assert that successful planning and preparation require 
commitment and participation from the executive level of the organization. Where appropriate, 
a commitment of support from the board of directors would be most appreciated and benefi cial, since 
the directors often are the ultimate source for funding. Ultimately, a committee formed with executives 
as the primary participants will be in the best position to act as the overriding steering committee of all 
other committees and plans. Th e executives would reduce the eff ectiveness of the ongoing management 
of the BCP if their participation was only as a token appearance at the beginning of the eff ort. It is not 
enough for the executives only to give permission, or simply to open the purse strings. Executives must 
play a part in the overall planning and the ongoing administration of a successful BCP, even if some of 
the tasks are delegated to a subset of the executives or their direct reports. For organizations of all sizes, 
the persistent participation of executives will set the proper tone for the participation of all levels.
Mandatory Internal Collaboration
Organizations of all sizes have complexities that refl ect diff erences in responsibilities and therefore 
division of skills and labor. Each organizational business unit will also have diff erent information 
foundations and diff erent information relationships among each other and among external part-
ners. Th e BCP must include participation from all organizational units whose product or service 
and foundational information are critically important to the organization‚Äôs integrity. Since one of 
the outcomes of a complete BCP is the safety of the organizational workforce, the complete work-
force should be aware of the plan as it is crafted and refi ned.
Th e BCM plan and the subordinate, integrated plans must be initiated before major disasters 
occur. Th is is most obvious to practicing business continuity professionals, but regretfully, some 
organizational executives tend to postpone plans until the cost is prohibitive and the organization 
is limited in its ability to withstand or recover from a crisis. Th e most eff ective planning is made a 
part of the organizational business life cycle along with other accepted support activities. In fact, 
BCM planning within a given organizational unit should not be seen as expanding the unit‚Äôs work 
responsibilities, but as a process for documenting the work responsibilities with an emphasis on 
recoverability. As referenced above, the planning must account for the overriding principles‚Äî
human safety, process continuity, information focus, and community responsibility.
Strategic Planning
All of the subordinate plans need to be integrated as other traditional organizational plans. Since 
most major organizational subunits have strategic as well as tactical or operational planning, so 
should the eff orts involved in creating the BCP. Strategic plans for some business processes may 

Integrated Business Continuity Planning ‚óæ 367
project 5, 10, or even 50 years in the future; therefore, by nature some of the strategic planning 
may outlive members of the planning committees. Strategic plans may document repeatable or 
cyclical processes, which may span months or years, and which include long-term organizational 
changes and maintenance requiring capital expense commitments. Th ese processes are often pri-
marily administrative, including organizational hierarchy changes, responses to workforce growth 
(or shrinkage), and evolving regulatory response. Across a wide variety of industries, research 
activities also necessitate a strategic view since outcomes of research are unpredictable.
Organizations craft tragic plans for long-term external relationships, such as business partners, 
competitors, suppliers, customers, and regulatory agencies. Information and business continuity 
will require the same strategic relationship planning as well. Suppliers of hardware and software 
technology will evolve along with their products, and suppliers of technical services will change as 
the technology and information requirements of the organization change. Th e evolving technol-
ogy environment will be paralleled by the evolving threats to information from the Internet in 
general and from individuals who directly target organizational information for monetary gain. 
Strategic plans involving information protection and continuity will need to be more attuned to 
the stages within organizational information life cycles, as the information migrates from inputs 
to outputs, archives, and even elimination.
Operational/Tactical Planning
All network-based organizations are actively involved in tactical or operational planning for tech-
nology and information management as part of periodic (annual, if not more frequent) business 
cycle changes. Organizational growth and decline will result in increase or decrease in technical 
service requirements and increasing emphases on information retention will aff ect the local and 
remote storage requirements for data protection. Th is type of planning is usually tied strongly to 
the annual operational budget, with fl exibility for short-term operational planning to replenish 
supplies and replace outmoded or worn equipment.
Operational planning also prepares for short-duration interruptions to information access or 
availability due to power problems or human accidents. Th is planning includes adequate staffi  ng 
for answering internal calls for assistance, to keep systems, servers, and other network components 
up to date with the latest upgrades and patches, and to monitor the networks for component 
failure or intrusion attempts. Organizational safety professionals also create and maintain tactical 
plans for hazards management and human protection from industrial accidents. Many of these 
incidents may be addressed by service contracts with vendors supporting various technology com-
ponents, which also require monitoring for service compliance.
Plan Maturity Measurement
Organizations are facing the need to become more responsible in maintaining and improving 
plans, whether strategic or tactical. Critical plans aff ecting core business functionality can no 
longer be static and thus require measurements to track improvement and maturity through time. 
Th is is increasingly important to ensure that the plans outlive committee memberships and to track 
more eff ectively the changes in the organizational information and technology requirements.
Th ere are certainly several standards-based maturity tracking disciplines that address various 
aspects of organizational processes, including software design and development, and overall orga-
nizational information management. Many of the standard eff orts require investment in external 
consulting to implement and coordinate over time. I suggest here a simple view consisting of 

368 ‚óæ Information Security Management Handbook
four levels of maturity that summarizes the expectations of most of the standards and allows for 
various-sized organizations to ensure the consistency of the individual plans involved in the overall 
BCM planning.
Initial Plan Creation
Poorly documented, loosely coupled processes and ad hoc conceptual plans do not constitute a 
plan suffi  cient to address targeted needs and responsibilities. Th e rule of thumb for many concep-
tual plans and policies has been that if it is not written down and authorized, it does not exist. 
For the BCM planning, there will be several interrelated plans, each of which will have separate 
but complementary domains within the larger BCP. Th e fi rst stage for all of them has to be that 
the plans are documented and appropriately authorized by the executive level. Th is also applies 
to the integration plan that ties all the individual plans into an operable master BCM plan.
Plan Testability/Changeability
Once a plan has been created and documented, it remains in a conceptual mode that will need to 
be tested and managed within its own organizational domain or unit. Only by examining the plan 
in context, if only a tabletop discussion, can the eff ectiveness and effi  ciencies be validated. Th ese 
tests must be within a consistent periodicity that is appropriate to the organization‚Äôs size and dis-
tribution. Some smaller organizations may be able to test once each year or business cycle. Larger 
organizations may require several individual tests of business units throughout the year. Tests will 
be most eff ective if the problems identifi ed will result in appropriate modifi cations, which must be 
made within a controlled, auditable process. Modifi cations also include updates based on organi-
zational and personnel changes through the periodic testing cycles.
Plan Integration
As individual plans are created and tested, the actual integration across the larger organization 
will not occur without the participation of the senior management executive levels. As will be 
explained in more detail below, the integration of the individual plans require attention and over-
sight only found at the decision-making, expense distribution levels of the organization. Th is 
oversight can begin as a separate plan from the organizational domain plans and can form the 
foundation for the creation, documentation, and testing of all the other plans. Th e testing of 
the integration plan itself will certainly change as the more localized plans are established and 
tested. Smaller organizations cannot ignore the need to integrate the individual plans, even if the 
organizational size means that most of the plans may be created, documented, and tested by a 
single organizational unit.
Integrated Plans as Part of Organizational Identity
Ideally, an organization will establish a mature BCM planning mode and methodology when the 
BCP is considered integral to the organizational culture. When BCM planning changes and test 
results are made part of annual organization reports and presentations to the public as well as to 
the board of directors, and the contributions of individual executives to the integrated planning 
and management of the BCP are appropriately recognized, then all the rest of the workforce will 
accept the importance of BCM to the future of the organization.

Integrated Business Continuity Planning ‚óæ 369
Organizational Assessment
Business Continuity Planning is rarely de novo
Organizational Assessment
Business Characterization
Existing Documented Internal BCM 
Plans/Policies/Procedures
Existing Internal BCM Structure
Existing External Relationships
Business Characterization
As mentioned earlier, common characteristics of many existing BCM standards include the ideal-
ized breadth and scope of the components of the plan, which is justifi ed by the need to address 
BCM planning in a large variety of organizational sizes, structures, and industry sectors. Because 
of such a characterization, each organization must be attentive to document its unique character-
istics, especially as they will aff ect how the organization is reestablished and restored after a major 
crisis. Documenting these characteristics will be critical to the recovery process, especially if much 
of the recovery is handled and administered by individuals who are not part of the organization. 
Undoubtedly, much of this documentation will already exist but perhaps in inconsistent forms 
and formats.
Th e organizational documentation begins with core business values‚Äîunique points of sen-
sitivity that may be part of the organizational philosophy, industry sector, and/or products or 
services produced. Many of the points of sensitivity are because of the organizational regulatory 
environment constraining the industry sector, e.g., health care, manufacturing, waste manage-
ment, construction, etc., all which have specifi c requirements for products or services produced, 
protection of records, and responsibilities for surrounding communities. Each organization will 
have goals and objectives surrounding the products or services produced, in areas such as quality, 
capacity, growth in size, and breadth of products. Based on those goals and constrained by the 
regulations, organizations may have business development plans such as expansions, mergers, and 
acquisitions, some of which may be actively underway when a major crisis strikes. All of these 
documents will be necessary to restore the organization, especially if commitments and contracts 
have been established.
For each of the products and services, and for the business goals and plans, there will be a set of 
supporting data and information foundations, including external data sources, internal data stores 
and archives, and specifi c information processing and protecting requirements. Th ese data/infor-
mation foundations are often taken for granted until a crisis demonstrates their vital importance. 
Establishing a BCM planning process may off er an important opportunity to document the ties 
between business processes and goals and the data/information foundational requirements. Integral 
to the information foundation is the technology‚Äîhardware, networks, and software‚Äîrequired 
to render the information useful. Vital to the information and technology are the people‚Äîthe IT 
professionals that implement and maintain the information environments. Documenting all of this 
can be part of the periodic organizational impact assessment, described further in Section 24.4. 
Such an exercise can be useful in heightening the awareness through all levels of the organization 

370 ‚óæ Information Security Management Handbook
to the importance of the information and support environments and for identifying potentially 
unnecessary or obsolete practices and technology that can be de-emphasized or eliminated.
Finally, organizational characteristics include the actual physical location or locations of all 
buildings, owned or leased, and all important topographic or geographic features, which may 
aff ect crisis management. Th ese include points of transportation ingress and egress, nearby bodies 
of water, nearby potentially hazardous terrain, e.g., dry forests, rocky slopes, geologic faults, larger, 
taller buildings, etc. Again, much of this may exist in various formats, but documenting these 
characteristics will help in anticipating potential crises, and/or consequences of crises.
Existing Documented Internal Business 
Continuity Plans/Policies/Procedures
In attempting to implement the components within a given standard as specifi c requirements to 
be explicitly fulfi lled, individual organizations may overlook or ignore existing sets of plans, how-
ever well documented and organized, that actually address important components of the BCM 
planning. Th e organization may already have BCPs and disaster recovery plans in various orga-
nizational units. Th ese can be vitally important jump starts for developing a larger integrated 
BCM plan. Larger organizations may already have risk management planning documentation, 
and associated plans for annual audits and assessments. Some organizations may have formal 
privacy/quality/regulatory responses, based on industry sector or product/service characteris-
tics. Manufacturing organizations undoubtedly will have safety and emergency response plans 
based on regulatory requirements, as well as a robust workforce awareness and training program. 
Hopefully, most organizations, large and small, will have existing enterprise information security 
policies and plans that may include security incident response management and information pro-
tection and continuity planning.
Existing Internal Business Continuity Responsibility Structure
If such plans and documentation exist, then undoubtedly some measure of organizational 
 structure exists as well. Th ere may be individuals already identifi ed with some of the plans and 
processes described above. Th e existing structures need not have the same titles and role descrip-
tions outlined in some of the Standards documents, but they do need to be related to the functions 
expected by the Standards documents. Even if some structure exists, for a successful BCM plan, 
the structure must be tied to the organizational hierarchy in a way that elevates the importance of 
the roles and ties it to the decision makers and spenders.
Organizations that have less structure will need to identify (beyond the decision makers and 
spenders) the responsibilities for privacy/regulatory response, compliance assessment and audit, 
safety and physical security, information security, and data protection (which may be distinct from 
IT support). Th e common factor in most of these roles is that though there may be individuals 
with delegated responsibility to implement them, the foundational responsibility for each is within 
the executive level of the organization. Th is again highlights the importance of tying the overall 
BCM planning to the executive level.
Existing External Relationships
As highlighted in Section 24.1, disasters are no longer local! Even when the crisis is within a single 
building, organizations still do not respond in isolation. Th is cannot be underemphasized based 

Integrated Business Continuity Planning ‚óæ 371
on the recent regional disasters. As part of the responsible gathering of organizational character-
istics, the collection must include external relationships. Most organizations will already have 
documentation about supply chain or business partners. It will be vital to understand and clarify 
the responsibilities (on both sides of the contract) during a major crisis. Th is is also vital for restor-
ing the organizational roles and relationships after the crisis, whether in a temporary recovery 
location or in a permanently restored setting. Critical vendors who provide vital raw materials and 
services must also be documented, including technology vendors and support organizations. Th e 
documentation must certainly include contact information but also license and service numbers 
and contract responsibilities.
Beyond the business requirements, external relationships include the local community 
 surrounding the organizational buildings or campus. Building and keeping ‚Äúgood neighbor‚Äù 
 relationships will become critically important during and after major crises. If the crisis or emer-
gency is localized to an organizational building, it may be necessary to disrupt the local com-
munity to bring crisis response, such as fi re fi ghting equipment, through local communities. For 
larger organizations, and especially health care organizations, the surrounding community may 
seek shelter or other types of services that may be lost in the surrounding community. Larger 
organizations may consider stocking levels of canned goods, dry goods, and bottled water for such 
opportunities. Organizational workforce members undoubtedly will include parents of school-age 
children, thus it will be of major benefi t to identify school locations and be ready to assist work-
force parents with their family concerns.
Finally, organizations must become aware of federal, state, and local emergency response 
eff orts, and especially local organizations that seek to bridge the response planning of govern-
ments, the community, and the private business/industry. If a crisis is regional, such a bridge will 
be vital to ensuring a coordinated response across all sectors. Large and small organizations must 
take the time to become acquainted with such eff orts, and to become involved in community-wide 
crisis response planning.
Integrated Business Continuity Management Plans/Processes
Out of many, one!
Integrated BCM Plans/Processes
Coordination Plan
Communications Plan
Risk Management Plan
Internal Incident Management Plan
Safety and Emergency Response Plan
Process Continuity Plan
Information Continuity Plan
All organizations, regardless of size, will require an anchor plan for BCM, and the accompa-
nying anchor of control and administration. From that, a number of organizational unit-specifi c 
plans will be coordinated. In this section, I suggest six organizational plans, which address com-
ponents and processes required for successful recovery after major crises. Each organization may 

372 ‚óæ Information Security Management Handbook
name these plans or concepts diff erently, or divide or combine them in diff erent ways, all based on 
the organizational size and complexity.
For each plan, I suggest the organizational unit responsible for creating and/or administrating 
the plan, and describe the accompanying committee, if appropriate. I also recommend strategic 
considerations, which will carry the plan through subsequent years or business cycles, which will 
include pandemic issues that each plan must address. It should become immediately evident that 
all the plans and concomitant committees interact‚Äîone committee will gather information 
that feeds the plans for another committee, and much of the responsibility of the executive coor-
dination committee (ECC) involves assimilating and maintaining reports and other output from 
the other plans. Finally, I recommend tactical components for each plan to be addressed within 
each business cycle, or as a part of regular operational processes. Th e tactical components will 
change much more frequently than the strategic components.
Th e actual plan will be the summary documentation describing appropriately the committee 
makeup and the associated strategic and tactical components. Th erefore, the complete BCM plan 
will be the compiled documentation from all the plans. Th e documentation of the combined plan 
must be dynamically maintained and protected, allowing for authorized periodic updates and 
modifi cations, and accessible for timely review, testing, and prompt implementation as needed. 
A variety of business continuity systems and tools are available for use and adaptation; but the 
BCM plan must determine the software and tools, and not the other way around! Th e driving 
determinations for tools and storage of the plan are usability and accessibility.
Coordination Plan
Th e coordination plan is the anchor for all other plans and processes for the BCM plan. 
Organizational senior executives must initiate and implement this plan and be responsible for lead-
ing this eff ort through an ongoing BCM ECC, which will include representatives from the other 
plans and from various organizational units as needed.
Strategic Components
Th e ECC establishes the command and control team (CCT), which is the group that administers the 
BCM plan during and after a crisis onset from a predetermined command center. Th e ECC is 
the ongoing presence of the CCT; it may consist of rotating members, some of whom may be on the 
CCT as well. Th e CCT will not be a standing team, but will take form at the onset of a major crisis, 
and it will be responsible for all organizational decision-making during and after the crisis. Its composi-
tion must be defi ned and representative roles must be identifi ed. Th e most important responsibility of 
this ECC, and ultimately the CCT, is to keep abreast of the legal responsibilities that the organization 
will face during a major disruptive crisis, such as regulatory requirements for personnel safety, product 
or service quality, and protected information accessibility.
Th e CCT will establish the emergency responsibility hierarchy, initiate the communica-
tions plan, and determine the allocation of funds and the availability of any insurance claims 
in anticipation of an emergency. Th e ECC will be responsible for selecting the command and 
control center location and the temporary relocation site for the organizational recovery, if nec-
essary, based on recommendations from other organizational units. Th e ECC will determine 
the staff  augmentation requirements during the recovery stages, with input from organizational 
units. Based on the funds and insurance, the ECC will determine the spending plan during and 
after the crisis onset.

Integrated Business Continuity Planning ‚óæ 373
For pandemic considerations, the ECC must understand the legal liability if data cannot be 
recovered in a timely manner from a quarantined location, and must document the data protec-
tion requirements for the organization during a reduced workforce due to quarantine or social 
distancing mandates.
Th e ECC will also formally establish the other BCM plans, if they are not already in force, 
and will initiate the communication strategies among the other committees and with external 
contacts.
Tactical Components
Th e ECC must collect, organize, and maintain the local and regional contacts for community, 
state, and federal crisis management groups, and identify representatives for periodic meetings of 
the groups. Th e committee will also track the maintenance of all the other plans, insuring that 
documentation is updated, and plans are tested appropriately. Th e most important tactical con-
sideration for the ECC is to defi ne the requirements for declaring a formal disaster or emergency, 
and to declare the initiation of the crisis response plans. Th is must be a single decision, made with 
participation from other organizational committees.
Communications Plan
Th is plan establishes the eyes, ears, and mouth for the CCT, and is also implemented by the 
 organizational senior executives, with appropriate unit representatives. Th is plan establishes 
the communications team (COMT) who will defi ne and document organizational communica-
tions at all levels prior to and especially during a major crisis.
Strategic Components
Th e COMT will also implement and administer the organizational training unit to address a 
broad range of training including personnel safety, information security, incident response, and 
emergency recognition and response. Many manufacturing and construction organizations may 
already have a strong safety training and awareness program that can be expanded to include the 
other subject matter. For BCM planning, the more the workforce is aware of emergency response, 
the more effi  cient that response will be.
Th is plan will also document organizational relationships on all levels‚Äîworkforce, custom-
ers, vendors, partners, and supply chains‚Äîand supply the contacts and procedures for emergency 
circumstances. Th e CCT will be the point of origination for offi  cial communications to the media 
from the organization during a crisis response.
Tactical Components
Th e COMT will insure that all contacts are documented and that the lists are maintained. Th e 
team will also research technologies and procedures for keeping in contact with workforce mem-
bers and their families, including hotlines, call trees, and various messaging services. Th e training 
unit will insure that the workforce is made aware of the conditions that could lead to a major 
crisis, and the appropriate ways to respond, including recognizing security incidents, following 
the proper chain of notifi cation, and following emergency evacuation planning based on periodic 
training and review programs.

374 ‚óæ Information Security Management Handbook
Risk Management Plan
Th e risk management plan will be administered by the organizational units that address 
internal audit, privacy, or quality control. Of course, organizational size and complexity will 
 determine the specifi c unit responsible for risk management. Th is plan will also be addressed in 
more detail within another chapter of this book. Characteristically, the plan assigns a compli-
ance/internal audit (CIA) unit to address regulatory responsibilities. For the BCM planning 
purposes, the risks involved with managing the information component of the processes are 
the primary scope. Also for the BCM plan, assessing positive outcomes of risks is a potential 
distraction, since the most eff ective use of BCM planning resources is to control, if not prevent 
negative outcomes. Evaluation for positive outcomes belongs much higher or earlier in the orga-
nizational decision process.
Strategic Components
Much of the strategic work of the risk management plan is to measure, for each organizational 
business process, the components of risk, and from those components, to analyze the risk for each 
of the processes.
Assessment of Criticality
Th is component determines the relative importance, or value, of each business process asset to the 
mission and goals of the organization. First, the specifi c process must be described suffi  ciently for 
replacement or temporary staff  to activate the process. Th en, the data/information foundation 
for the process must be documented. Th is includes the types and capacity of data and informa-
tion, the server and network technology, as well as the support staff . In terms of the BCM plan, 
the criticality of the process is the overall cost of the loss of the process, which is evaluated from 
three perspectives:
Actual Value of the Process
 
‚óæ
. Th e importance of the process to the organizational bottom line; 
and the eff ect of the loss of the process to the continuation of the organization.
Replacement Cost
 
‚óæ
. Th is includes not only the organizational staff  and activities engaged in 
implementing the process, but the technology and staff  supporting the data management 
involved with the process.
Regulatory or Legal Liability
 
‚óæ
. Th e impact of the interruption of the process or service to the 
neighborhood or community, the eff ect on partner/customer contracts, and the regulatory 
penalties for delaying or failing to restore the process and its associated information.
From this assessment, the business processes and their supporting information and technology can 
be ranked by value and/or cost of replacement.
For each system(s) supporting the process, certain targets can also be determined during this 
assessment. Th e recovery time objective (RTO) is the time required to restore a system or process 
to functionality, and the recovery point objective (RPO) is the desired state or point in (recent past) 
time at which the system is to be restored. Another target often included is the minimum time to 
recover (MTR), which takes into account the logistics involved in recovering the technology, data, 
and systems. In the event of a pandemic crisis, this assessment must account for the eff ects of a 
reduced workforce on this process.

Integrated Business Continuity Planning ‚óæ 375
Assessment of Threat Ecology (Sources, Activities, Probability)
Th e complete threat assessment must document the sources, the activities involved with this 
threats, and the probability of each threat occurring within the organizational setting. Th e threat 
sources for most of the processes at the same location will be the same, with minor exceptions. 
Generally, threats are summarized under three main categories:
Human
 
‚óæ
: Directly personal activities, including internal and external sources involving social 
engineering, unauthorized access or behavior (e.g., apparent hallway strangers, ‚Äúdumpster 
diving‚Äù), errors, theft, vandalism, and terrorism.
Environmental
 
‚óæ
: Weather-related threats, other natural disasters such as fi re, earthquakes, 
and pandemic diseases.
Technology-
 
‚óæ
 or Information-Based: Malicious software of all kinds, whether from external 
Internet-based sources and/or internal accidental or deliberate activities. Th ese also include 
malicious use of wireless technology, unauthorized network sniffi  ng, email phishing, and 
spamming.
Of course, most technology-based threats are perpetrated by humans, but the exploitation of 
technological skills and the application of technological controls diff erentiates these from the 
directly personal human activities. Most of the threat sources are easily identifi ed using commonly 
available sources. Documenting the threat activities, whether human or network-related, will help 
determine the controls or countermeasures for the threats. Th reat probability is frequently assessed 
in a subjective measurement (e.g., high, medium, low), but can be measured with more precision. 
Th is assessment must include those threats that aff ect human safety as well as threats to the data 
and information.
Assessment of Vulnerabilities
Organizational vulnerabilities are the openings, faults, or absences, accidental or deliberate, which 
allow threats to aff ect negatively the information resource and interfere with the process or service. 
Vulnerabilities can be categorized in three ways:
Physical
 
‚óæ
 or Structural: Building or room problems that allow some of the personal and/
or environmental threats to act. Th ese can include broken or absent entry controls, doors, 
or window locks lighting (for visibility); building construction problems, new incomplete 
construction or older deteriorating structures; and location proximity to potential natural 
threats, such as steep slopes with loose rock or lacking vegetation, lakes or rivers prone to 
fl ood, or regions with recent history of wind storms.
Technology
 
‚óæ
- or Information-Based: Network, hardware, or software problems that allow 
threats to penetrate the systems and servers. Specifi c examples are out of date or poorly 
designed operating systems, applications or systems or devices, poor system software 
change management, poor access management, and poor network and server protection 
management.
Th e absence of 
 
‚óæ
Operational Structures/Processes/Practices: Th e absence of process barriers may 
inadvertently permit human- or technology-based threats to act or penetrate. Th ese can 
include incomplete (e.g., unsigned) or missing policies and procedures, poorly defi ned or 
undocumented processes, all of which can permit or fail to detect successful threats.

376 ‚óæ Information Security Management Handbook
Based on the assessment, it may be possible to aggregate vulnerabilities in ways to enable effi  cient 
implementation of controls, e.g., many network vulnerabilities can be controlled with intrusion 
detection/prevention devices, or uniform access control processes can control unauthorized access 
to diff erent systems; many physical vulnerabilities can be controlled by consistent door locks and 
lighting.
Assessment of Controls Suffi ciency
Th ere are several controls categorization schemes available to document and correlate the controls 
for the vulnerabilities and threats. Th e selection of a scheme will depend on the organization‚Äôs 
industry sector or regulatory requirements. By defi nition, the control of a threat is essentially 
the control of a vulnerability through which the threat is actualized; therefore, in the context of 
providing protection and controlling access to the data and information, the most appropriate 
targets of the controls and countermeasures are the vulnerabilities as indicated above. Controls or 
countermeasures for asset loss cost of restoration can include reducing the technology costs of the 
restored process, e.g., moving to less expensive platforms or systems requiring fewer person-hours 
of support.
After the recommended controls are identifi ed, and an initial gap analysis is performed, the 
controls effi  ciency assessment is intended to follow up on the gap analysis by determining if an 
active controls assessment is actually in place, and if there are any required controls that are 
absent.
Analysis of Risk from Assessments
To be precise, in an information management context, risks themselves are neither identifi ed 
nor are they directly managed. Th reats, threat probabilities, vulnerabilities, and the value (or 
loss impact) of the information component of the business process can be directly measured and 
 managed. Risks associated with specifi c processes can be determined, or calculated by the combi-
nations of the components. Th e classic equation defi nes the assessment process succinctly:
 
Risk = Th reat (Probability) √ó Vulnerability √ó Asset Value (loss)
Risk is then identifi ed as the probability of a monetary loss. Th e complete analysis means that risk 
is calculated for all organizational business processes. From this analysis, the value/cost ranking of 
the business processes can be augmented by the probability of the threats and cost of vulnerability 
controls. Th is analysis can indicate the high-value, high-risk processes, which can lead to a prior-
ity of process recovery for the BCM plan. Th e same analysis can indicate the data or information 
supporting the processes that can be classifi ed by protection importance, which could indicate 
the most confi dential data or information deserving the highest protection from integrity loss or 
unauthorized access, and requiring the highest availability.
Tactical Components
For risk management, the tactical processes include planning and implementing annual internal 
assessments, as well as assessments for changes in the organization that aff ect the protection of 
critical assets and the associated data/information. Internal and external audits will be admin-
istered or coordinated from the CIA unit or other appropriate organizational units. Each of the 

Integrated Business Continuity Planning ‚óæ 377
assessments and audits will undoubtedly result in changes and updates to the overall risk manage-
ment plan or subsets of the larger plan.
Safety and Emergency Response Plan
Undoubtedly, most manufacturing or construction organizations will have robust plans for pro-
tecting people during and after a major crisis. Other organizations, depending upon size and 
complexity, will have physical security and/or safety plans. Th is planning eff ort is administered by 
the organizational safety/physical security (SPS) unit. Characteristically, in most organizations, 
the safety plan is implemented and maintained by a single or a set of safety/emergency response 
teams (SERT).
Strategic Components
Th e most important strategic component is that people come fi rst! People are gradually being 
acknowledged as the most important and valuable organizational resource, therefore providing 
for the safety and protection of people during and after crises off ers a strategic advantage to the 
restoration of the organization. SPS units administer physical site control, assessing, and control-
ling potential threats to human safety. Organizations that deal in potentially harmful materials 
or substances, either as raw materials or products, will conduct hazards assessments to protect 
people from the substances or materials. Th e SPS unit also will determine criteria for recogniz-
ing emergencies, whether they are internal or external. Th is group also develops evacuation plans 
from within structures and possibly egress plans away from dangerous locations. Also, the SPS 
unit will develop the temporary relocation plan to account for restoration of processes and services 
immediately after the crisis.
For pandemic issues, the SPS unit will need to defi ne and plan quarantine evacuation plans. 
Th ese diff er from rapid, emergency evacuations because they may allow for slow egress from build-
ings or campuses. Th is type of slow evacuation will allow for gathering personal material, laptops, 
and documents that will be important in continuity of processes. Also, this type of evacuation will 
allow for a phased lock-down of buildings and information resource locations and allow for appro-
priate restrictions to be implemented. In the case that individuals will be left inside for technical 
support eff orts, the SPS unit must also prepare stores of food and water.
Tactical Components
As part of ongoing organizational support, the SPS unit will work with the IT unit to administer 
the organization‚Äôs requirements for power and internal climate conditioning to protect the tech-
nology resources. Appropriate internal engineering management will help reduce energy-based 
threats and vulnerabilities, potentially reducing the impact of a major crisis. Th e SPS unit must 
be prepared to tailor the emergency response, drawing from the overall strategic plan and based 
on the nature of the crisis and its onset. Th e SERT will be responsible for implementing the tem-
porary relocation plan at the onset of a crisis. During the crisis, the SERT will have the primary 
responsibility for building and site safety.
After a crisis onset and the move to the temporary location, the SPS unit and teams will be 
responsible for administering the cleanup, decontamination, and/or disposal of damaged  structures 
and materials. Finally, this unit will plan and manage the return from the temporary location and 
conditions to a restored or new organizational location.

378 ‚óæ Information Security Management Handbook
Internal Incident Management Plan
Th e growing danger to organizational integrity posed by the increasingly intense threats from 
malicious software and the human perpetrators of this software have increased the importance of 
the organization internal incident response team (IIRT) and plan. Th is plan is primarily initiated 
from within the IT unit with close involvement and cooperation of the SPS unit and because of 
anticipated legal complications, the senior executives. Th is collaboration is responsible for devel-
oping the internal incident response plan and the closely aligned forensics assessment plan. Th e 
activities of the IIRT are predominantly tactical and operational as they respond to notices from 
all quarters about lesser and greater incidents.
Strategic Components
Th e strategic activities of the team entail developing and maintaining plans that will address 
crucial circumstances, such as human threat activities, internal and external network attacks, 
and legal consequences of the responses to these activities. Th ese carefully prepared plans and 
procedures will need to be reviewed and vetted by the executives and by legal representatives, 
certainly internal but perhaps external as well. Ideally, this team is a separate group from SPS 
and IT units, which may not be the case in smaller organizations. Working with the IT group, 
the IIRT will develop an escrow protection plan for sensitive system passwords, encryption keys, 
and lists of authorized administrators and delegates who will be able to acquire the lists and 
use them in the response and recovery phases of a major crisis. Working with IT, the IIRT will 
develop a structured and authorized process for acquiring the escrowed information and a pro-
cess for allowing the authorized individuals to enter the building to assess and document the 
status of the infrastructure. Th e IIRT will develop plans and procedures to coordinate the collec-
tion of internal incidents from various sources. Any observable interruption of information-based 
services must be documented and correlated with other events or internal incidents to determine 
patterns or sequences that may help control a problem and prevent it from escalating and/or 
repeating in the future.
Process steps for a response plan can be found from a number of resources, but all have similar 
sequences:
Prepare
 
‚óæ
: Th e development of the plan represents the preparation. All IIRT members will 
need to become skilled at following the details of the plan.
Strengthen
 
‚óæ
: Based on the preparations and in cooperation with IT and SPS units, the inter-
nal infrastructure can be strengthened with appropriate countermeasures and controls that 
will eliminate or reduce the impact of incidents. Also, working with the organizational 
training unit, a set of awareness and training programs can be developed from the plan 
that will bring the workforce members into appropriate participation with the overall 
response.
Detect
 
‚óæ
: Discovering, identifying, analyzing, and reporting the types of incidents will be an 
integral part of the training for the workforce; anyone should be able to notice, assess the sig-
nifi cance, and report on occurrences as soon as possible. Th e IIRT must be the single point 
of contact, even if the team will not necessarily address or correct every internal incident. 
Th e uniform collection process will be vital for correlating events that may at fi rst notice 
seem unrelated.

Integrated Business Continuity Planning ‚óæ 379
Contain/eliminate
 
‚óæ
: Working with other organizational units, the IIRT will direct the appro-
priate response to the incident or sets of incidents, including how to capture the most amount 
of data and evidence about the incident, when to block the source of the incidents, and how 
to remove the eff ects of the incident from the infrastructure. Th e IIRT will determine how 
to control the evidence, whether physical or digital so that the evidence can be useful if a 
legal or criminal investigation ensues.
Restore
 
‚óæ
: For information-based incident activities, the IIRT will work with the IT unit 
to determine the recovery from the activities, assuring that the sequence of events is 
 documented, potential witnesses are identifi ed, and evidence is appropriately collected and 
protected. After such eff orts are underway or under control, then the internal and external 
networks can be restored and if backups are necessary, aff ected servers and workstations can 
be recovered.
Report
 
‚óæ
: No internal incident response is complete until all the activities are documented and 
reported to the organizational executives and if appropriate, the external legal or criminal 
authorities. Th is will also include appropriately handing over any documentation or evi-
dence for the activities.
Review
 
‚óæ
: Every response activity off ers a chance to review the internal incident response plan 
and make corrections or additions, and to enhance the training to the workforce.
Tactical Components
Th e most signifi cant burden of the IIRT will be in the ongoing tracking of the reported network 
and systems intrusions, evaluating server and system halts and interrupts, responding to personal 
observations by workforce members, and correlating all the activities. Th e team will also review 
system, network, and other activity logs collected by various audit logging systems to evaluate 
activities and patterns. Th e IIRT will also constantly review Internet threat assessments and soft-
ware vulnerabilities as reported by various Internet-based security organizations.
Process Continuity Plan
Th e organizational business unit managers are identifi ed as the data owners for the information 
resource supporting the business processes. After a major crisis, the data owners will actually 
direct the restoration of the systems based on the backups preserved for this purpose. Th e IT group 
will certainly provide the technical capability for the restoration. Each unit data owner will create 
a restoration plan for validating the incorporation of the backup data into the restored system and 
determining the availability of the system. Depending on the size of the organization, there may 
be many process continuity plans!
Strategic Components
Th e primary strategic component for each unit data owner in developing the restoration plan is 
the criticality assessment of risk management plan. Th e unit data owners will participate in the 
criticality assessment, which will not only determine the priority of the restoration among other 
processes, but also the RTO, RPO, MTR, and any other restoration objective appropriate to the 
specifi c process. Additionally, the unit data owners will work with the IT unit and the system ven-
dors to arrange for systems and usage licenses and keys for restoration on new technology.

380 ‚óæ Information Security Management Handbook
Tactical Components
At the declaration of the recovery plan by the CCT, each organizational business unit will fi rst 
implement the unit emergency organizational hierarchy for structural continuity. Th is will also 
enable the continuity of regulatory response and restore the policies and procedures specifi c to the 
unit. As the recovery proceeds, the unit data owners will coordinate with IT for the activation and 
confi guration of the temporary network and infrastructure. According to priority sequence, the 
backed up data and information, and new copies of the application software will be restored to the 
temporary infrastructure. Th e unit data owners will be responsible for testing the restored systems 
and authorizing the restoring of the process to the appropriate user community.
Th e unit data owners will work with existing staff  or temporary workers to carry out the 
processes in coordination with the organizational staffi  ng plans. As the permanent organization 
location is restored, the unit data owners will initiate a similar set of restoration plans to move back 
to the permanent location.
Information Continuity Plan
Th is plan is clearly within the scope of the IT unit, though the activities are dependent on input 
from the CIA unit (including auditors and legal staff ), the SPS unit, and the organizational busi-
ness unit data owners. In fact the information continuity plan cannot exist in isolation‚Äîit is by 
defi nition and make up an integrated plan. In many larger organizations, the information security 
management (ISM) unit, which addresses the planning for overall data protection, may be sepa-
rated hierarchically from the IT unit, which administers the networks and server infrastructure, 
implements the security practices, and acts as stewards of the organizational data, as directed by 
the units referenced above.
Th e IT unit will have the expertise to keep abreast of advances in the various technical arenas 
and will add value by proactively recommending changes while supporting the existing infrastruc-
ture. It will also perform the backups and archives of data as required, and in direct relation to the 
internal incident management team and the SERT, the IT staff  will act as eyes and ears to respond 
immediately and report problems as they occur. Th e IT unit ideally will identify an IT Recovery 
Team (ITRT) to be at the forefront of crisis recovery implementation to restore data to the tem-
porary infrastructure. Th is team will develop and maintain the information continuity plan for 
responding to a crisis. Historically, the information continuity plan has been a complex activity, 
coordinating technology, vendors, backup media, and workforce demand, but it is important to 
recognize that the primary goal of this plan is to restore access to data and information. Keeping this 
goal in mind can help simplify decisions about how data is protected in general, and choices about 
the replacement architecture and infrastructure.
Th e preparation and documentation of the information continuity plan can require an invest-
ment in time if little has been documented historically and based on organizational size. Several 
external service agencies off er consulting services to initiate and document the plan. Many of these 
will also be able to participate at varying levels of eff ort at the onset of the crisis and the activation 
of the plan.
Strategic Components
Most of the strategic components of this plan refl ect the actual strategic responsibilities of the IT 
unit. As the IT staff  track advances in technology, evaluation and adoption is a function of such 

Integrated Business Continuity Planning ‚óæ 381
things as business direction, capital cost, and eff ects of infrastructure change. From a continuity 
perspective, other considerations for new technology must be resistance to interruptions (short 
duration, less than one workday) and recoverability (from short or long interruptions), which may 
be less a function of new devices and more a reorienting of existing processes and perhaps creativ-
ity in architecture and infrastructure planning. Targets for reorientation and creativity include the 
following areas.
Decoupling
Th is refers to the separation of server and networking components that classically were contained 
within a single cabinet. Th is is refl ected in tiered architectural designs where the processing func-
tion is separated from input/output and storage, e.g., a single console connecting to redundant 
servers which are also connected (along with other redundant pairs of servers) to a storage area net-
work (SAN). Th ese multiply connected sets of redundancies constitute a tiered architecture such as 
service oriented architecture (SOA). With other design considerations, this can potentially enable 
replacement or addition of component parts without noticeable interruptions to services and data.
Redundancy
Storage redundancy has been available for years, and is continually being refi ned with more stor-
age capacity and more intelligence within the units to detect problems. As server technology 
shrinks in size and cost (rack mounts and blades), capabilities for clustering, either in pairs or 
more, and automatic failover after failure are becoming more aff ordable. Many larger organiza-
tions are building redundancy in network architecture, including redundant network hubs (at a 
distance) and redundant pathways. Higher network speeds and protocols are allowing for remote 
mirrored servers off ering a complete duplication of the server environment. Virtual technology is 
not itself physical redundancy, but again, with the reduction in size, it is conceivable to construct 
redundant virtual servers.
Th e challenge for process recovery is to document all confi gurations and where remote mirror-
ing is implemented, documenting the latency and daily diff erences in active data. As redundancy 
and virtual capabilities increase, the cost of software and system licenses will change and vendor 
service contracts must account for appropriate licenses and keys for duplicate servers and server 
images.
Resilience/Disaster Tolerance
Innovations in server and operating system technology bring the potential for near immediate 
recognition of power loss and data preservation, including automatic shutdown and recovery after 
power restoration. With redundancies throughout the architecture, it is possible to conceive of no 
network downtime or zero-time recovery, which would be important for threats leading to short-
duration interrupts, but less so for major crises. New technologies are also bring better fi re and 
water protection to server and array cabinets, cabling, and server rooms and buildings.
Remote Storage
With the higher network speeds and protocols, remote storage and backups are more aff ord-
able both in time and money. As described above, organizations can more effi  ciently implement 

382 ‚óæ Information Security Management Handbook
remote data mirroring or fi le shadowing. With the reduced cost for storage, organizations may 
consider retaining idle arrays at distant locations for rapid recovery needs. Based on experiences in 
recent environmental crises, most large, and many small or medium organizations are establishing 
remote centers for backup and archive, either with separate organizational locations or perhaps 
through reciprocal agreements with business partners. Smaller organizations also have access to 
remote storage service agencies, whose services may include simple data backups, data archives, 
and data recovery hot- or warm-sites.
Serendipitously, storing mirrored or daily backed up data may off er a sea change in informa-
tion recovery. As long as the remote location is suffi  ciently distant, and the stored systems are 
duplicates of the primary location, and the data is no older than one workday prior to the crisis 
onset, this practice may simplify the recovery process. Once a recovery work site is established, 
including work space, workstations, and network connectivity, then the time consuming and 
cumbersome processes‚Äîacquiring new servers, recovering backup media from a remote location, 
initiating all operating systems and applications, and fi nally reloading the data itself‚Äîmay be 
obviated by simply pointing the recovery site network to the remote server location.
Desktop Simplifi cation
Many organizations have suff ered data loss of various scopes because of organizationally sensitive 
data being stored on desktop workstations. As a response, many have attempted a variety of data 
backup schemes from implementing mounted server directories to actual remote backup of many 
or all workstations. Th e situation has grown much more complex with portable storage devices and 
mobile computing becoming part of the organizational practices. Some  organizations are turning 
to networked terminals, a technology that harkens back to pre-desktop workstation times. In the 
strictest architecture, the desktop device will have no internal storage, and no external storage 
connections. All desktop software and data will be mounted remotely at start-up and all data will 
be kept in protected storage arrays within the organizational data center. One of the technologies 
supporting that is the blade PC, which is conceptually similar to blade servers. Th is has the poten-
tial for greatly simplifying the management of large numbers of desktop workstations, since the 
blade PCs will be much more easily updated, protected, and serviced. Additionally, virtualization 
off ers the potential for PC images to exist on data center servers. Virtualization on desktop PCs 
may off er fl exibility in application processing, but may not help with the protection, updates, and 
potential data loss since the workstations themselves remain on workforce desktops.
Outsource All Server and Storage Management
Large and medium organizations may have an historic investment in technology to such a degree 
that duplicate storage and server capability is too expensive. Th ese organizations will continue to 
rely on contract recovery service vendors who will provide fee-based, on-demand server facilities 
(cold-, warm-, or hot-sites) or other vendor-supplied recovery services. As infrastructures evolve 
toward smaller server sizes, some of the large organizations are beginning to create recovery ser-
vices among remotely located data centers, each serving the recovery needs of the other location.
Smaller organizations, especially those who are recent start-ups with little historic technol-
ogy, may opt for the complete outsourcing of all technology devices and support. Service agen-
cies are expanding to provide complete computing and data management services to small and 
medium organizations, even supplying and supporting desktop workstations. All backups, data 
recoveries, and technical support could be handled by the service agency, and potentially, the 

Integrated Business Continuity Planning ‚óæ 383
customer organization may need no salaried technical workforce. Another innovative alternative 
to outsourcing is the concept of cloud computing, where all processing capability and data resides 
within a remote vendor‚Äôs domain, allowing a customer organization to connect through secure 
Web interfaces to the vendor‚Äôs systems services.
Such arrangements with remote vendors have great promise and may reduce technology manage-
ment costs. However, they must not be adopted hastily; they will require carefully created contracts 
specifying service scope, service response timeframes, change management, data protection require-
ments, assurance of data protection, and regulatory and legal liabilities for failure to full the contract.
For pandemic issues, there are two main considerations for the IT support group:
Reduced Workforce
In an active pandemic, several workforce staff  may be absent, whether from suff ering from actual 
symptoms or from the practice of social distancing, which reduces the active workforce on site to 
reduce the opportunity for infective agent spread. Strategic plans will need to include how to sup-
port critical systems (from the risk management plan assessment of criticality) with a reduced staff . 
Th is may be addressed by appropriate cross training of staff  to be able to fi ll in at diff erent roles and 
timeframes. Th e process and procedure steps must be documented so that temporary staff  who 
may not be familiar can at least follow instructions. It will be important to identify the minimum 
systems necessary to keep the organization‚Äôs service processes in action, some systems and servers 
may be able to be shut down temporarily. A reserve of supplies‚Äîpaper, tapes, print cartridges, 
etc.‚Äîshould be kept separately from the standard set of supplies in the case that external service 
agencies may be unavailable. In the case of a quarantine declaration, food and water supplies 
should also be kept strictly for that purpose. Finally, IT will need a shutdown triage plan, allowing 
for a careful, controlled backup and shutdown of the servers if the staff  is reduced, or if the organi-
zation is completely shut down. Th is correlates with the quarantine evacuation, described within 
the safety and emergency response plan, during which no structural damage occurs.
Remote Workforce
In the event that the organization is shut down, but the servers and networks are still functioning, 
it may be necessary to implement a remote workforce plan where staff  connect to the network 
through the Internet. Th is will require capacity planning for the network hub, since many more 
workforce staff  will be making remote connections. New policies and procedures will be needed to 
ensure secure connections and access control procedures are in force. Potentially, new workstation 
or laptop technology will be needed to ensure high-speed connectivity, either wired or wireless. It 
may be necessary to increase the voice connectivity as well.
One consequence of a high volume of remote users will be the capacity of the public Internet 
access services. Many vendors may not be able to handle such a peak load increase, especially if 
the eff ects of the pandemic are widespread. Organizations may consider contracts for multiple ISP 
alternatives or working together with the local community to arrange for ad hoc rapid bandwidth 
increases during pandemic events.
Tactical Components
Th e primary tactical/operational goal of the IT unit is to prevent (or reduce the potential for) 
 service interruption to data and information fl ow within the organization. Th is involves  addressing 
the organizational business process technology and information requirements, and  responding to 

384 ‚óæ Information Security Management Handbook
the regulatory requirements for restricted access to private data. For the IT workforce, this means 
the active implementation of the strategic plans for managing the architecture, infrastructure, 
and processes. Th e day-to-day activities feed back to the strategic planning with new requests 
and recommendations based on growing organizational needs. From the information continuity 
perspective, these activities target reducing vulnerabilities to interruptions, small and large, and 
enhance the overall recoverability of the technology components.
One of the primary means of controlling service interruptions is often overlooked before it is 
too late. Inadequate staffi  ng is indeed a vulnerability, if the workforce is overloaded and is not able 
to address service requests in a timely manner. If increased staffi  ng is diffi  cult, then service con-
tracts with external agencies is a viable alternative. Historically, the IT workforce were considered 
prime targets for reduction and/or outsourcing, based on relative ignorance of the decision mak-
ers. Where there is a heavy investment of technology infrastructure, internal staff  who develop 
familiarity over time with the technology and the organizational requirements prove to be much 
more effi  cient in responding to problems than outsourced, temporary staff .
Th e technology itself requires constant attention and periodic maintenance (another function 
of adequate staffi  ng) to allow proactive anticipation of problems instead of retroactive cleanup. As 
the infrastructure within a single data center increases, the power and environmental conditioning 
requirements will also increase, which highlights the importance of strategic views on technology 
changes. Loss of power or failure to maintain the appropriate temperature can result in the loss of 
the complete technical infrastructure.
Whether or not staffi  ng problems can be resolved, other service interruption controls can reduce 
interruptions. Implementing the strategic perspectives on redundancy in the technology components 
and devices will reduce single points of failure in the infrastructure; if one of a paired set of servers 
fails, the failover capability can provide near seamless continuity of service. With redundancy, the 
service maintenance and upgrades can be performed separately while the other member of the pair 
is kept active. Redundancy can also be implemented in connectivity‚Äîeach of the paired servers can 
have paired connections to the storage switches, which also can have paired connections to the stor-
age array. Th e decoupling of the infrastructure components allows for replacing or adding process-
ing technology or storage capacity without bringing down the infrastructure. Server virtualization 
potentially reduces the number of independent server units, and therefore the complexity of the 
infrastructure. It can also reduce the staffi  ng requirements needed to maintain separate servers.
Backup of data and information can be a labor-intensive eff ort, especially if tapes and other 
small media are incorporated. Server-to-server backups, whether adjacent or remote, can reduce 
the live system impact of backups, and allow the data on the backup server to be separately copied 
to external media, if necessary. Automated, remote server backups can provide strong data protec-
tion in anticipation of a major crisis, but may require additional staffi  ng for support.
As part of the maintenance of the organizational infrastructure, recoverability can be enhanced 
by periodically reducing the infrastructure variability, simplifying the support eff ort, and elimi-
nating out-of-date software and hardware technologies. Vendors characteristically will curtail or 
eliminate support on products that are too far out of date. Older applications that have little use 
are targets for assumption into new technologies and systems.
Summary of Planning
Apart from the coordination plan, all the other plans represent ongoing active processes within 
the various organizational business units. Th e coordination plan is specifi cally for activation at 
the onset of a major crisis, but it still needs to be developed and documented by the executives. 

Integrated Business Continuity Planning ‚óæ 385
Th e ECC should periodically tap into the planning schemes, including testing results and status 
of documentation, of all the other units and the subordinate plans. Th at itself is one of the most 
important disciplines within the BCM planning program, and one of the reasons that the over-
all coordination must be at the executive level, if not at the board level. If planning control and 
accountability is not mandated, the subordinate planning programs will slip in priority within the 
organizational units.
Th e biggest challenge for all the plans will be to fi nd ways to test appropriately‚Äîif the plans 
are not tested, then they are not viable. For large organizations, it may be necessary to establish 
a formal testing program that operates year-round and sequentially tests the major plans and the 
organizational unit process recovery plans. For most of the major plans described above, the most 
effi  cient testing may be to have detailed tabletop walk-through tests, which include all major 
participants (even from other business units) and which allows a testing leader to take the plan 
through various crisis scenarios and validate the documentation for process fl ow and complete-
ness. One of the scenarios may be to scale back from a complete recovery implementation if in fact 
the crisis is more regional than organizational. Th e organizational representatives may participate 
in regional crisis response activities even if the physical structure and information resources of 
the organization are still intact. Live testing for many of the plans, such as safety evacuations 
and hazard containment may be required by regulations. Testing data backup media is extremely 
important, even if it is not a formal system recovery test. It may also be important to test selected 
system recovery processes for the highest critical systems.
As stated in this chapter, and in many more documents and venues, people are the most important 
target for crisis recovery, and they are the most important resource for implementing the  recovery. 
It will be increasingly necessary for organizations to develop a culture of  security‚Äîpersonal safety, 
data privacy, hazard containment, and recognition of potential  problems. Organizations that seek 
longevity cannot aff ord an ignorant workforce, and the elimination of ignorance rests in the hands 
of the executives and board of the organization.
Crisis Onset and Response
Today is too late!
Crisis Onset and Response
Crisis Recognition
Organizational Impact
Scope Impact
Declaration
Emergency Response
Incident Management
Information Continuity
Process Continuity
Recovery Termination
Th is section suggests several sequence steps after the onset of a major crisis (see Table 24.2). 
Th ese are idealized, and meant to represent a wide range of decisions and responsibilities that all 

386 ‚óæ Information Security Management Handbook
Table 24.2 Integrated Business Continuity Plan Outlines
Coordination Plan
Communications Plan
Risk Management Plan
Safety and Emergency 
Response Plan
Responsibility
Senior executives
Senior executives
Privacy/quality/audit unit(s)
Safety/physical security 
unit(s)
Participation
Unit representatives
Unit representatives
IT, safety units
Training unit
Team/
committee
Executive coordination 
committee
Communications team
Compliance/internal 
assessment/audit units
Safety/emergency response 
team(s)
Command/control team
Training unit
Strategic 
components
Establishment of other 
plans
Workforce awareness 
training
For each defi ned business 
process:
People fi rst!
Legal responsibilities
Relationship 
communication
Criticality assessment
Safety/evacuation training
Emergency responsibility 
hierarchy
Media communication
Threat ecology assessment
Site control
Spending plans
Regulatory communication
Vulnerability assessment
Hazards assessment
Staff augmentation plans
Controls suffi ciency 
assessment
Emergency recognition
Temporary relocation site
Analysis of risk from 
assessments
Evacuation plans 
coordination
Communications plan

Integrated Business Continuity Planning ‚óæ 387
Pandemic 
issues
Legal liability for 
unrecoverable data
Consequences of reduced 
workforce (each defi ned 
business process)
Quarantine evacuation
Requirements for data 
protection
Phased lock-down
Controlled facility access
Emergency food/water stores
Tactical 
components
Identifi cation of local/
regional contacts
Community coordination
Periodic Review
Tailored emergency 
response
Defi nition of disaster/
emergency
Internal messaging
Annual internal assessments
Temporary relocation
Declaration of disaster/
emergency
Workforce family 
communication
Annual external audits
Building/site safety
Assessment/maintenance 
of other plans
Communications 
technology
Timely updates to plans
Cleanup, disposal, and 
decontamination
Termination of recovery 
activities
Return from temporary 
location
(continued)

388 ‚óæ Information Security Management Handbook
Table 24.2 (continued) Integrated Business Continuity Plan Outlines
Incident Management Plan
Process Continuity Plan
Information Continuity Plan
Responsibility
IT, safety units; senior 
executives
IT, unit representatives
IT unit
Participation
Unit representatives
Organizational unit 
process and data owners
Safety unit representatives
Team/
committee
Internal incident response 
team
Process continuity team(s)
IT recovery team
Strategic 
components
Structured, authorized 
response
Criticality assessment of 
risk management plan
Creative architecture/infrastructure
Coordination of incident 
collection
Organizational 
development strategic 
plans
Technology redundancy
Escrow/protect passwords, 
keys, critical lists
Regulatory compliance 
plans
Technology resilience/disaster tolerance
Forensics preparation
Remote data storage
Outsource server and storage 
management
Pandemic 
issues
Plans for reduced workforce/social 
distancing
Plans for remote workforce; increased 
network/Web access

Integrated Business Continuity Planning ‚óæ 389
Incident Management Plan
Process Continuity Plan
Information Continuity Plan
Tactical 
components
Network/systems 
intrusions
Initiate emergency 
organizational hierarchy
Crisis/service interruption prevention
Personal observations
Provide procedure 
continuity
Control technology/service interruption
System halts/interrupts
Coordinate with IT for 
temporary technology 
restoration
Reduce islands of technology
Threat assessment
Restore services/processes 
in temporary 
confi guration
Reduce technology variability/
obsolescence
Initiate temporary staffi ng 
plan
Timing of temporary 
structure breakdown
Return from temporary 
state or location

390 ‚óæ Information Security Management Handbook
organi zations will need to determine prior to the actual crisis. Depending on the size, organizations 
may combine some of the responsibilities under a more appropriate set of organizational units, and 
rearrange the sequence of events to suit the organizational structure.
Crisis Recognition
It is clearly understood that the most prevalent crises that trigger response and recovery plans are 
weather related (e.g., see the FEMA map links: http://www.fema.gov/hazard/index.shtm). Fires 
are perhaps the next major category of crises that trigger a formal response. Although the preva-
lence of Internet malware and network attacks are increasing, it is neither clear how many of those 
attacks trigger formal crisis response and recovery plans, nor is it clear if such attacks reach the 
same breadth and scope as environmental, weather, or expansive fi re-related crises. Nonetheless, 
each organization will have to be diligent in tracking and responding to low-level incidents and 
Internet attack attempts lest they result in a loss of processing capability within the organizational 
network.
A crisis may be recognized by anyone in the organization, but if the processes are in place 
the report will make its way through either the IIRT or the SERT to the ECC, and if appropriate 
to the CCT. Of course, for regional crises, news from the public media may reach all  committees 
and the workforce at nearly the same time. From any source, the crisis recognition needs to be 
brought as soon as possible to the ECC who will call the CCT to action, which will offi  cially 
 initiate and administer the response (Table 24.3).
Organizational Impact
Th e fi rst consideration is the impact of the crisis, internal to the organization and external 
to the surrounding community and region. Th e CCT will simultaneously solicit organiza-
tional impact reports from the ITRT and the SERT for the state of structural and safety 
problems and the condition of the network and infrastructure. Th e severity of these reports 
(or lack thereof) may signal the quick end of the crisis and the CCT activities. If the severity 
is suffi  cient (which most certainly is a subjective determination), then the CCT will initi-
ate the response plan. Th e fi rst eff ort will be to instruct the Legal and CIA Units to evaluate 
the organizational responsibilities during and immediately after the crisis response. At this 
point, the CCT will determine from the reports whether or not the crisis involves a pandemic 
circumstance. If so, the team will implement the pandemic plan and initiate the appropriate 
evacuation or quarantine response.
Scope Impact
At the same time as the organizational impact review is initiated, the CCT will instruct the 
COMT to contact external crisis teams for the impact beyond the organization. Th e COMT 
will be the offi  cial voice of contact for the organization under the direction of the CCT. Th e 
COMT will also instruct the predetermined organizational representatives to report to regional 
command centers for participating in the regional response and to keep the COMT abreast of 
the community and regional activities. Th e COMT will also contact all subordinate plan teams 
for preparation of activity.

Integrated Business Continuity Planning ‚óæ 391
Table 24.3 Crisis Onset Response Sequence
Responsible Unit(s)
Activity/Role/Function
Crisis recognition
Internal incident 
response team
Observe/report internal incidents
Implement internal incident response plan
Safety/emergency 
team
Observe/report external or internal crises
Organizational 
impact
Coordination team
Receive/collate internal and external crisis alerts
Contact safety and IT units for effects on people, 
structures, information
Contact legal, compliance units for regulatory 
effects and responsibilities
Determine pandemic effects‚Äîevacuation, 
quarantine, remote/reduced workforce
Alert all responsible teams/units
Internal incident 
response team
Track/report correlated incidents
Scope impact
Communications 
team
Contact local/regional centers for breadth and 
intensity of crisis
Delegate participants to regional local crisis 
control center
Declaration
Command and 
control team
Declare offi cial response implementation
Establish command and control center; activate 
relocation hierarchy
Based on predetermined thresholds, initiate 
response and recovery plans
Initiate emergency spending plan‚Äîaccounting, 
payroll, expenses
Initiate remote location readiness plan
Acquire/establish networks, workstations, 
communications, furniture, etc.
Communications 
team
Contact workforce, establish family 
communication capability
Contact partners, customers, health care 
providers, media
Contact vendors and initiate technology 
replacement and license adjustment
Distribute/coordinate temporary 
communications technology
(continued)

392 ‚óæ Information Security Management Handbook
Table 24.3 (continued) Crisis Onset Response Sequence
Responsible Unit(s)
Activity/Role/Function
Emergency 
response
Safety/emergency 
team
Hazard assessment‚Äîascertain human effects of 
exposure to hazards
Structural assessment‚Äîevaluate damage effects 
on people and processes
Set organizational perimeter protection
Establish ingress (rescue, fi refi ghters) and egress 
(evacuation) pathways
Coordinate immediate personal aid‚Äîhealth, 
food, shelter
Environmental/Pandemic assessments:
Initiate evacuation/quarantine plans (time of day, 
intensity of Pandemic)
Coordinate food, shelter needs of internally 
quarantined groups
Incident 
management
Internal incident 
response team
Assess potential for information leakage, theft at 
organizational site
Acquire escrowed lists for timely and 
appropriate use
Monitor temporary network infrastructure for 
intrusions
IT recovery team
Collaborate with Internal Incident Response 
Team for technical support
Information 
continuity
IT recovery team
Activate recovery location technology‚Äîservers, 
workstations, network connectivity, vendor 
licenses
Initiate system builds‚ÄîOS, 
licensed software, etc.
Acquire/reload backup data
Process 
continuity
Unit data owners
Test and activate systems (predetermined RTO/
MTR sequence)
Initiate production processes in recovery location
IT recovery team
Coordinate testing and production loads with 
unit data owners
Recovery 
termination
Command and 
control team
Determine restoration completion of 
permanent site
After other team activities (below):
Declare return to permanent site

Integrated Business Continuity Planning ‚óæ 393
Declaration
Th e CCT will be the sole source for the formal declaration of the crisis in order to activate the 
organizational recovery plan. Depending on the size of the organization, the decision may be 
made by one person or the whole team, but a single source timing and of the decision is imperative. 
Th e declaration by prior understanding sets in motion a series of cascading events, triggers the 
initiation of all the subordinate plans; no separate command is to be expected by the subordinate 
plan administrators. However, if the crisis conditions have not interrupted all services and struc-
tures, some of the subordinate plan administrators may be advised to scale back pending further 
developments.
Th e CCT will activate the recovery hierarchy and establish the command and control center, 
which must be away from the crisis location. Th en the team will initiate the emergency spending 
and accountability plans and set in motion the necessary insurance claims. Th en, the temporary 
relocation site can be prepared, and all previously defi ned replacement technology, network infra-
structure, furniture, and communications capability can be acquired, installed, and confi gured. 
If the temporary relocation site is provided by a service agency, then the relocation contract and 
plans will be implemented. If a temporary workforce supplement is necessary, then the CCT will 
initiate contact with temporary staffi  ng agencies.
Th e declaration will also trigger the COMT to contact the workforce with the prearranged 
process, and seek to provide communication and coordinate assistance to families. Th e COMT 
will alert all organizational partners, customers, debtors, etc., to the status of the ongoing response 
and recovery. As the temporary relocation site is being prepared, the COMT will also contact 
vendors for replacement technology and license adjustment, based on previously collected instruc-
tions from the IT Tech Support Staff . If appropriate, the COMT will acquire and distribute the 
temporary communications technology, again based on previously arranged plans.
Emergency Response
Th e SERT will be the primary on-site contacts for ascertaining the potential hazards within the 
organizational location, including toxic hazards and structural damage. Within the location, 
the team will set an organizational perimeter to prevent anyone from entering, whether part of 
Table 24.3 (continued) Crisis Onset Response Sequence
Responsible Unit(s)
Activity/Role/Function
Safety/emergency 
team
Determine/report safety of restored 
permanent site
IT recovery team
Initiate planning for restoration from temporary 
location
Restore networks, workstations, servers, licenses 
from temporary location
Unit data owners
Initiate planning for restoration of production 
processes from temporary location
Initiate production processes in restored 
permanent location

394 ‚óæ Information Security Management Handbook
the workforce or not. Th e team will also coordinate with local and regional authorities for estab-
lishing pathways for ingress of emergency vehicles and the egress of the workforce, especially any 
who may be injured. Where needed, the team will implement or coordinate the personal aid for 
injuries and temporary shelter and food.
If there are pandemic conditions, the SERT will initiate the appropriate quarantine and evacu-
ation plans, based on advice or mandates from local and/or regional authorities. Th e team will 
also release and distribute food and other needs for workforce who may be remaining within the 
buildings for predetermined reasons.
Incident Management
Th e IIRT will continue vigilance for any other threats that may exploit the structural and 
 technical vulnerabilities associated with the crisis onset. Th is team may be involved with forensics 
 assessments and gathering evidence especially if the crisis appears to be of internal origin, such as a 
fi re or an explosion. Th e team will also initiate the monitoring of the temporary location network 
prior to restoring the replacement systems and data.
If there is damage to the network infrastructure, the team will work with the ITRT to estimate 
the damages and to identify recoverable systems and data storage. If the organizational network 
is still intact, the team will work with the IT Technical Support to heighten the intrusion detec-
tion and prevention capabilities of the monitoring devices. Representatives of both the internal 
incident management team and the ITRT will follow the procedures to recover any escrowed lists 
of passwords or encryption keys for use during the recovery period.
Information Continuity
Th e eff orts of the ITRT refl ect the classic disaster recovery planning involved with restoring access 
to data and information. Once the recovery site is set up with replacement servers, workstations, 
and network connectivity, and the vendor licenses and keys are acquired, the ITRT will pre-
pare the architecture for organizational use. Th e team will load the appropriate operating systems 
and tools, and build the applications systems required. Following that, the team will acquire the 
backup data media and restore the data to the temporary infrastructure.
Process Continuity
In close coordination with the ITRT and the information continuity plan, the organizational 
business unit data owners will assist in the reconfi guration of the application systems and test the 
systems with the restored backup data. Th e prearranged RTO and MTR sequences will determine 
the order of the processes. Only the unit data owners can release the production systems for user 
activities.
Recovery Termination
Th e CCT will necessarily keep abreast of both the temporary relocation eff orts and the 
 reestablishment or reconstruction of the original location. When the original location is close 
to completion, the CCT will initiate the plans to terminate the temporary recovery activity and 

Integrated Business Continuity Planning ‚óæ 395
return to the restored location. Th e process will be quite similar to the temporary location recov-
ery activities, with hopefully much less need for safety and hazards protection. Th e team will 
instruct the SERT and ITRT to assess the safety of the restored site and to initiate the prepara-
tion of the restored site networks and infrastructure. Once confi gured, the unit data owners will 
again test and authorize the restored systems for production status. Th e relocation back to the 
restored location will also follow a predetermined sequence, based on the size and complexity of 
the organization.
About the Author
James C. Murphy, GSEC, CISSP-ISSMP, CISA, and CISM, is the information security offi  cial 
for the Offi  ce of MMIS Services in the NC Department of Health and Human Services (DHHS), 
and information security lead within the NC Medicaid Management Information System devel-
opment project.


397
25
Chapter 
CERT/BERT: Community and 
Business Emergency Response
Carl Jackson
Contents
Foreword ................................................................................................................................. 398
Background ............................................................................................................................. 398
Incident Command System ................................................................................................ 399
Multiagency Coordination Systems..................................................................................... 399
Public Information ............................................................................................................. 400
Private Sector Integration ........................................................................................................ 400
Integration into Crisis Management Plans .......................................................................... 400
Community and Business Emergency Response Teams ........................................................... 400
Th e Role of Community Emergency Response Teams ..........................................................401
Business Emergency Response Teams Program ........................................................................ 402
Th e Role of BERT .............................................................................................................. 402
Why BERT......................................................................................................................... 402
Benefi ts of BERT to the Enterprise ..................................................................................... 402
Establishing a BERT Program ................................................................................................. 403
BERT Costs ........................................................................................................................ 404
BERT Training ................................................................................................................... 405
Timing of BERT Training ................................................................................................... 405
Equipping the BERT Teams ............................................................................................... 405
Executive Management Communications ........................................................................... 406
Summary ................................................................................................................................. 408
About the Author .................................................................................................................... 408
References ............................................................................................................................... 408

398 ‚óæ Information Security Management Handbook
Foreword
Th e Department of Homeland Security defi nes a fi rst-responder1 as those individuals who in the 
early stages of an incident are responsible for the protection and preservation of life, property, 
evidence, and the environment, including emergency response providers.
Today, there is no accurate estimate of the number of trained and certifi ed fi rst-responders 
in the United States. Th e U.S. Department of Defense has warned of the piecemeal manner in 
which the federal government‚Äôs assistance program for fi rst-responders was initially created and 
expanded across agencies and departments. Th is largely uncoordinated eff ort has resulted in a 
number of problems and shortcomings in the eff ort to improve the nation‚Äôs overall preparedness. 
According to Richard Falkenrath, former senior director of policy and plans at the Offi  ce of 
Homeland Security and currently the deputy commissioner of counterterrorism of the New York 
City Police Department, ‚ÄúTh e specifi cs of the program have been determined not by any guiding 
strategic concept but by discrete, uncoordinated legislative and appropriations and administrative 
initiatives.‚Äù2
Also, according to Bruce Baughman, director of FEMA‚Äôs Offi  ce of National Preparedness 
(ONP), ‚ÄúEven the best prepared states and localities do not possess adequate resources to respond 
to the full range of terrorist threats.‚Äù3 In fact, over the past 5 years, numerous reports have identi-
fi ed problems with the federal eff ort to train fi rst-responders and have called for a reform of the 
numerous federal assistance programs created to support these professionals.
For some time, many private sector executive managers, event crisis coordinators, and conti-
nuity planning professionals have become increasingly aware of the gap that exists between the 
ratio of employees in their organizations versus the number of immediately available civil fi rst-
responders.
At this writing, the U.S. First Responders Association (http://www.usfra.org/), a nonprofi t network 
of fi refi ghters, EMS, rescue, police offi  cers, military, and civilian support teams, estimates that there is far 
in excess of 17,000 open positions in these professions. Th ese positions are for all categories of position 
titles and agency openings relating to support and execution of fi rst-responder responsibilities.
Background
Subsequent to the September 11, 2001 attacks on the United States, the newly formed Department 
of Homeland Security4 (DHS), established the National Incident Management System (NIMS). 
In DHS‚Äôs words, ‚ÄúNIMS provides a systematic, proactive approach to guide departments and 
agencies at all levels of government, nongovernmental organizations, and the private sector to 
work seamlessly to prevent, protect against, respond to, recover from, and mitigate the eff ects of 
incidents, regardless of cause, size, location, or complexity, in order to reduce the loss of life and 
property and harm to the environment.‚Äù
Th e NIMS command and management component is comprised of the following elements:
Incident command system (ICS)
 
‚óæ
Multiagency coordination systems (MACS)
 
‚óæ
Public information
 
‚óæ
Taken together, all three of these elements of top-down and unifi ed command and control infra-
structure are the most visible aspects of the DHS incident management infrastructure and are 
normally executed with a sense of urgency during times of crisis.

CERT/BERT: Community and Business Emergency Response ‚óæ 399
Incident Command System
Th e ICS component of NIMS is designed as a standardized, on-scene, all-hazards incident man-
agement approach that
Allows for the integration of facilities, equipment, personnel, procedures, and communica-
 
‚óæ
tions operating within a common organizational structure
Enables a coordinated response among various jurisdictions and functional agencies, both 
 
‚óæ
public and private
Establishes common processes for planning and managing resources
 
‚óæ
Th e command and control organization and communications structure of the ICS is utilized by all 
levels of the government including federal, state, tribal, and local. As well ICS has been voluntarily 
adopted by many nongovernmental and private sector organizations (i.e., businesses,  educational 
organizations, etc.). ICS is applicable across disciplines providing a structured command and con-
trol environment comprising fi ve major functional areas: command, operations, planning, logistics, 
and fi nance/administration. All of these functional areas may or may not be used based on the 
needs of the response personnel reacting to a particular incident.
Multiagency Coordination Systems
Th e MACS component of NIMS allows all levels of government and all disciplines to work 
together and coordinate activities much more effi  ciently and eff ectively. Multiagency coordina-
tion occurs across the diff erent disciplines of the ICS structure involved in incident management, 
across jurisdictional lines, or across levels of government. Th is multiagency coordination has been 
utilized on a regular basis whenever personnel from diff erent agencies interact in such activities as 
preparedness, prevention, response, recovery, and mitigation (Figure 25.1).
On-scene multiagency
coordination
As
incident
grows and
transitions,
coordination
moves off-scene
Requests and
information
Support and
coordination
Off-scene EOCs and/or
MAC groups
Incident
command/
unified
command
Liaison
officer
Incident
command/
unified
command
Dispatch
centers
Jurisdictional
EOCs
MAC
Groups
DOCs
Figure 25.1 Multiagency coordination systems overview. (From Federal Emergency Management 
Agency, http://www.fema.gov/emergency/nims/MultiagencyCoordinationSystems.shtm)

400 ‚óæ Information Security Management Handbook
Public Information
Th e public information component of NIMS consists of the processes, procedures, and systems to 
communicate timely, accurate, and accessible information on the incident‚Äôs cause, size, and current 
situation to the public, responders, and additional stakeholders (both directly aff ected and indirectly 
aff ected). Public information must be coordinated and integrated across jurisdictions, agencies, and 
organizations; among federal, state, tribal, and local governments; and the private sector.
Th e government provides a Glossary of NIMS related terms at the following URL for anyone inter-
ested in further understanding (http://training.fema.gov/EMIWeb/IS/ICSResource/Glossary.htm).
Private Sector Integration
So, what is the relevance of NIMS to the private sector organizations like businesses and educational 
organizations?
NIMS provides an eff ective and effi  cient emergency response command and control structure 
that can be rather easily adopted by private sector organizations in planning for and documenting 
crisis management plans.
Integration into Crisis Management Plans
Private sector organization crisis management planning is an integral part of the continuity plan-
ning process of the enterprise. Its most important objectives are to provide managers with a quickly 
activated and streamlined mechanism to assist them in managing the enterprise during the crisis/
disaster situation. Th ey do this by providing resource support and more importantly in facilitating 
communications with both internal crisis and recovery teams as well as with critical external par-
ties (the press, key stakeholders, civil authorities, important customers, clients and suppliers, etc.).
Should a crisis arise, organization‚Äôs crisis management team(s) must be activated to manage the 
crisis until conclusion. Typically, this support would be in the form of facilitated communications, 
resource allocation and access, and any other help required by the business and/or information 
technology recovery teams to facilitate rapid recovery and continuation of time-critical business 
process functionality.
Adopting and integrating the ICS concept into the company‚Äôs crisis management structure 
gives the management a leg up on carrying out these vital objectives. Adherence to the ICS com-
mand and control model provides company crisis management with signifi cant benefi ts and will 
enhance all aspects of response and recovery.
Utilization of the ICS model during times of crisis by businesses promotes the coordinated 
exchange of information that is so critical to rapid crisis response and mitigation. Th is provides 
business management, crisis management, and continuity planning teams with a valuable and 
structured synchronized means to exchange vital information, both internally and externally.
Community and Business Emergency Response Teams
Given the NIMS and ICS background discussion above, it should be apparent at this point there 
are many advantages of adopting the ICS process in the private sector. As a way of providing pri-
vate sector organizations with a model of how to most eff ectively achieve this, company managers 
should merely look to what many communities have done along these lines.

CERT/BERT: Community and Business Emergency Response ‚óæ 401
Referred to as the community emergency response team (CERT 5) program, dozens of public 
entities have established CERT programs that serve as an outstanding example of how best to 
systematize community crisis response processes for a widespread or regional crisis. CERT is a key 
supporting component of the overall NIMS infrastructure with its ability to help communities 
organize management structure and communication paths. A properly implemented CERT pro-
gram provides communities with the tools necessary to help them react to emergencies in a rapid 
and synchronized way utilizing ICS. CERT has enjoyed a widespread implementation among 
state, county, city, and other governmental agencies within the United States.
The Role of Community Emergency Response Teams
Th e CERT program is designed to teach people about crisis and disaster preparedness and accom-
panying hazards that could seriously impact their communities and it provides training in essen-
tial disaster response skills, like
Fire safety
 
‚óæ
Light search and rescue
 
‚óæ
CERT team organization
 
‚óæ
Disaster medical operations
 
‚óæ
Th rough the CERT training that potential team members receive, they will be prepared to assist 
other folks in their neighborhood or workplace should disaster arise until professional fi rst-
responders are available, which in some cases could be hours or even days.
As volunteers go through the CERT training, they gain a much better understanding of the 
possible threats to their communities so that they may take the most appropriate steps to reduce 
the impact. Should a crisis occur that overwhelms local community fi rst-responders, CERT mem-
bers are then in a position to provide aid and comfort until help arrives. And even when assistance 
is available, CERT members can provide helpful information to fi rst-responders accordingly.
Th rough participation in CERT, community leaders and volunteers are acknowledging that 
without outside assistance, they must be prepared to help themselves until signifi cant aid can 
be brought in from the outside. Th is also acknowledges the fact that following a major disaster, 
fi rst-responders who normally respond rapidly to supply fi re and medical services simply will 
not be able to meet the overwhelming demand for life-safety services. A simple fact of life under 
these circumstances is that the numbers of casualties, potential communication interruptions or 
even complete failures, possible street blockages, etc., will all combine to severely hamper fi rst-
responders from providing aid.
CERT organization resources are available on the Web. For instance, an organization called 
Citizen Core6 (http://www.citizencorps.gov/cert/index.shtm) recommends a number of steps to 
start a CERT within their Web site:
Identify the program goals that CERT will meet and the resources available to conduct the 
 
‚óæ
program in your area.
Gain approval from appointed and elected offi  cials to use CERT as a means to prepare citi-
 
‚óæ
zens to care for themselves during a disaster when services may not be adequate.
Identify and recruit potential participants. Naturals for CERT are community groups, busi-
 
‚óæ
ness and industry workers, and local government workers.
Train CERT instructor cadre.
 
‚óæ

402 ‚óæ Information Security Management Handbook
Conduct CERT sessions.
 
‚óæ
Conduct refresher training and exercises with CERTs.
 
‚óæ
On the Web site referenced above Citizen Core also provides guidance on
Starting a CERT program
 
‚óæ
Registering a CERT
 
‚óæ
Obtaining CERT specifi c training
 
‚óæ
Finding CERT resources
 
‚óæ
Locating other CERT organizations
 
‚óæ
Th e well respected Citizen Core CERT National Newsletter can also be found at that URL.
Business Emergency Response Teams Program
The Role of BERT
As with community CERT teams, likewise, businesses must also acknowledge that without out-
side assistance, they must prepare themselves to help themselves until signifi cant assistance can 
be brought in from the outside. An outgrowth of the CERT concept, the American Red Cross 
and other organizations have been advocating the formation of Business Emergency Response 
Teams (BERTs) within the private sector. Business, educational organizations, and others are 
being asked to consider the implementation of a BERT organization. BERT is very closely 
related to and structured in a similar manner for businesses as the CERT program is for govern-
ment entities.
Why BERT?
Sometimes, unfortunately, organizational management may not be aware of the need for a 
BERT program internally. Th ere are several tools the practitioner can use to begin educating 
management on the need for BERT. For instance, Figure 25.2 presents a convincing argument 
by addressing the wide disparity of community fi rst-responders on duty at any given time versus 
the numbers of citizen casualties who would need assistance. Such a wide divergence of numbers 
demonstrate that in a signifi cant regional or area wide disaster, civil fi rst-responders would be 
badly stretched to the point of breaking, and citizens would be forced into taking care of them-
selves as well as their neighbors.
Benefi ts of BERT to the Enterprise
Th ere are several benefi ts that can accrue to an organization that implements and maintains a suc-
cessful BERT program. Th ese benefi ts include, but are certainly not limited to, the following:
Enhanced State OSHA compliance
 
‚óæ
Lowered insurance related costs
 
‚óæ
BERT recognition by local fi re and police agencies (potentially)
 
‚óæ

CERT/BERT: Community and Business Emergency Response ‚óæ 403
Fulfi lling responsibility to company employees
 
‚óæ
Having the company location named as a 
 
‚óæ
Disaster Triage Drop-Off  * location
Paramedic/ambulance colocation
 
‚àí
Medical personnel marshalling area
 ‚àí
Workers compensation credit (potentially)
 
‚óæ
State emergency services credit (potentially)
 
‚óæ
Corporate citizenship (to the local community)
 
‚óæ
Establishing a BERT Program
At the outset, it is wise to establish a BERT Management Team with the organization to oversee 
the development and implementation of the program. Components of BERT program formation 
might include
Obtaining executive sponsor support and leadership
 
‚óæ
Identifi cation of company locations in need of BERT
 
‚óæ
* Potentially making the company eligible to be recognized and used by local fi re and medical agencies as a 
medical marshalling area during times of emergency. Th e reasoning is that if medical agencies understand that 
BERT training and triage is utilized at this location, they may be more likely to designate the location as a mar-
shalling area as they would understand that the company‚Äôs personnel are trained in triage and have performed 
adequate screening of critically injured people. Th is is as opposed to a company where no prescreening has 
occurred, thereby requiring medical authorities to perform triage themselves, stretching scarce resources, and 
thereby slowing response.
Situation: The  American Red Cross and the State of California OES tell us that in the event of a major Southern
California earthquake, we should all expect to and be prepared to be on our own for at least 72 h.
‚Ä¢
Facts:
Company locations
#1
#2
#3
Population:
~84,000
~48,000
~428,000
Visitors/company employees:
~10,000
Total population:
~94,000
~48,000
~428,000
On duty police officers:
12
20
35
On duty fire fighters:
39
10
29
Total first responders:
51
30
64
Ratio:
1800:1
1600:1
6700:1
‚Ä¢
We need to answer these questions:
‚Ä¢
What do we do if there is a major disaster event at one of our locations?
‚Ä¢
What happens if we are on our own?
‚Ä¢
What do we do it there are multiple casualties?
‚Ä¢
What happens if 911 is down?
‚Ä¢
Do we have enough trained internal first responders?
‚Ä¢
Do we have enough supplies?
‚Ä¢
Assist in answering management‚Äôs question: ‚ÄúAre we ready?‚Äù
Why a company BERT?
Figure 25.2 Why a company BERT?

404 ‚óæ Information Security Management Handbook
Preparing a written program description including defi nition of a BERT charter, outlining the 
 ‚óæ
BERT organization structure, participant and volunteer roles and responsibilities, funding, 
training plans, internal awareness communications plans, etc.
Identifi cation of volunteer business emergency responders who work in the business and are 
 
‚óæ
familiar with the area
Preparation of draft management communications regarding BERT
 
‚óæ
Analysis of existing business emergency response plans and groups within the enterprise that 
 
‚óæ
support those plans (i.e., company security, facilities management, IT emergency response and 
business continuity planners, etc.)
Organizing and oversight of BERT training
 
‚óæ
Development of BERT maintenance plans
 
‚óæ
Continued long-term administration of the BERT program
 
‚óæ
BERT Costs
One of the very fi rst questions that will be asked will be on the topic of cost. An organization that 
has not attempted to form a BERT in the past will naturally be concerned about the expenses 
involved for start-up. Figure 25.3 estimates expenses for a company with three locations with 
roughly 3000 employees spread across those locations.
As can be seen in this very simple example, the estimated costs are allocated to two cat-
egories: the fi rst being the cost of initial start-up, while the second is an estimate of annual 
maintenance expenses. Of course, there are important cost elements that have been left off  
this simple example: the cost of employee time in training and participating in BERT-related 
events, etc.
Company BERT program
Location 1
Location 2
Location 3
Notes
Costs
BERT materials
15,000
$
15,000
$
15,000
$
Extinguishers, radios, backpacks, etc.
Misc BERT consulting
10,000
$
10,000
$
10,000
$
CPR training, Haz-Mat abatement, etc.
Primary BERT training
15,000
$
15,000
$
15,000
$
Outside consulting
Contingency
5000
$
5000
$
5000
$
Unknowns
45,000
$
45,000
$
45,000
$
135,000
$
Company BERT program
Location 1
Location 2
Location 3
Notes
Costs
Refresher training
2000
$
2000
$
2000
$
Certification fees‚ÄîRed Cross
Materials
2000
$
2000
$
2000
$
New and replacement
Training
6000
$
6000
$
6000
$
New BERT members
Consulting
5000
$
5000
$
5000
$
Annual drill assistance
Contingency
3000
$
3000
$
3000
$
Unknowns
18,000
$
18,000
$
18,000
$
54,000
$
Estimated first year BERT start-up costs
Estimated annual BERT maintenance costs
Figure 25.3 BERT start-up and maintenance cost estimates.

CERT/BERT: Community and Business Emergency Response ‚óæ 405
BERT Training7
Th e typical American Red Cross BERT course is generally 28 h long for each volunteer that 
trains them in fi rst-responder disciplines. Th e course prepares business fi rst-responder teams to 
respond during emergency and disaster situations. It consists of seven 4 h modules:
Introduction to emergency preparedness
 
‚óæ
Emergency response
 
‚óæ
Medical module 1
 
‚óæ
Medical module 2
 
‚óæ
Search and rescue
 
‚óæ
Team organization and management
 
‚óæ
Course review and disaster simulation
 
‚óæ
Timing of BERT Training
Th e following sample BERT training schedule timeline (Figure 25.4) is a MS Visio representation 
of how an organization with two diff erent geographical locations may want to address training at 
both with scarce training resources:
Equipping the BERT Teams8
To learn more and fi nd out how you can help your community, please contact citizencorps.gov. 
Th e following CERT checklist, which is also very applicable to a checklist that BERT team mem-
bers might use, is recommended by Citizen Corps (www.citizencorps.gov/cert/downloads/training/
PM-CERT-AD-Unit1Rev2.doc):
1/18/2010
9/16/2010
1/18/2010‚Äì3/4/2010
BERT volunteer
indentification
2/1/2010
3/1/2010
4/1/2010
5/1/2010
6/1/2010
7/1/2010
8/1/2010
9/1/2010
5/12/2010
Supervised
BERT team
final drill (#1)
3/4/2010‚Äì5/13/2010
BERT training
sessions
(location #1)
6/16/2010‚Äì8/22/2010
BERT training
sessions
(location #2)
8/20/2010
Supervised
BERT team
final drill (#2)
Figure 25.4 BERT training schedule.

406 ‚óæ Information Security Management Handbook
COMMUNITY EMERGENCY RESPONSE TEAM MEMBER 
EQUIPMENT/SUPP/FIRST AID CHECKLIST
Th e following equipment and supplies are recommended as a minimum supply cache 
for all CERT teams. Th e equipment and supplies should be maintained at or near the team 
staging area.
EQUIPMENT/SUPPLY
Nylon/canvas bag with shoulder strap
 
‚óæ
Water (two canteens/bottles per search and rescue team)
 
‚óæ
Dehydrated foods
 
‚óæ
Water purifi cation tablets
 
‚óæ
Work gloves (leather)
 
‚óæ
Goggles
 
‚óæ
Dust masks
 
‚óæ
Flashlight or miner‚Äôs lamp
 
‚óæ
Batteries and extra bulbs
 
‚óæ
Secondary fl ashlight or light sticks
 
‚óæ
Utility knife
 
‚óæ
Note pads
 
‚óæ
Markers:
 
‚óæ
Th in point
 
‚àí
Th ick point
 
‚àí
Pens
 
‚óæ
Duct tape
 
‚óæ
Masking tape (2 in.)
 
‚óæ
Crescent wrench
 
‚óæ
First aid pouch containing:
4 √ó 4 gauze dressings (6)
 
‚óæ
Abdominal pads (4)
 
‚óæ
Triangular bandages (4)
 
‚óæ
Band-Aids
 
‚óæ
Roller bandage
 
‚óæ
Scissors
 
‚óæ
Executive Management Communications
Once executive and mid-level managers are on board, the BERT management team can take 
over to provide initial organization, preparation of budgets, selection of volunteers, training, 
and the like.
Box 25.1 is an example of the wording of an executive management communication intended 
for all managers within the enterprise.

CERT/BERT: Community and Business Emergency Response ‚óæ 407
BOX 25.1 SAMPLE EXECUTIVE MANAGEMENT 
BERT PROGRAM ANNOUNCEMENT
Date:
Good Morning (Afternoon);
Given threat of serious regional disaster scenarios in our area, Executive Management is 
pleased to announce the initiation of a Business Emergency Response Team (BERT) program 
at Company.
Th e primary purpose of the BERT program is to put us in the position to help us help 
ourselves should severe regional emergencies, like earthquakes, result in signifi cant delays by 
local city, county, or state fi rst-responders to come to our aid.
Under the BERT program, we will provide training to individuals who have volunteered 
to help respond to emergency situations at company locations.
When emergencies happen, Company Business Emergency Response Team members 
will have been trained to provide critical support to fi rst-responders, provide immedi-
ate assistance to victims, and organize spontaneous volunteers for our primary business 
locations.
Members of the Company BERT Management Team will be asking for volunteers within 
the next few weeks. We are anticipating that each of the company locations will require 
approximately (number here) BERT members.
Th e Business Emergency Response Team training is free, will be conducted during business 
hours, and is comprised of seven 4 h modules:
Session 1: Introduction/course overview/basic emergency preparedness
Session 2: Fire suppression/evacuation basics
Session 3: Search and rescue
Session 4: Triage/stabilization and extrication/medical operations/disaster fi rst aid
Session 5: Medical operations/disaster fi rst aid II
Session 6: Team roles and operational organization/NIMS‚ÄîSIMS system orientation
Session 7: Final drill exercise
Following completion of initial training, BERT members will meet periodically to ensure 
they stay current on training and to participate in periodic drills as needed.
On behalf of the executive management, please note that as we move through the initial 
stages of forming the company BERT program, we are asking the cooperation of all man-
agers in assisting in the identifi cation of volunteers, and then for allowing selected BERT 
volunteers to participate fully in BERT training and other activities. It is not anticipated that 
the time needed by BERT volunteers will in any way be onerous.
Our BERT Management Team will be utilizing the company intranet site and other commu-
nication mediums to reach out to company BERT volunteers and their managers very soon.
Should you have any questions, please feel free to contact ______________.
Best Regards,
/CEO signature/

408 ‚óæ Information Security Management Handbook
Summary
Th e 9/11 attacks as well as the many natural and man-made disasters of late have all combined to 
raise awareness and recognition that our civil fi rst-responders may well be hard pressed to provide 
enough aid in a timely manner. We know that even under optimum circumstances, there is often 
a very serious defi cit of civil fi rst-responders as compared to the numbers of people that would 
require life-safety and/or medical attention immediately following either an area wide or regional 
disaster or a major terrorist attack.
One solution discussed has been the adoption and adherence to an ICS command and control 
model. A properly implemented ICS together with CERT/BERT team formation will provide 
enterprise crisis management with signifi cant benefi ts and will enhance all aspects of response 
and recovery. Utilization of the ICS model during times of crisis by businesses promotes the 
coordinated exchange of information that is so critical to rapid crisis response and mitigation and, 
thereby, provides business management, crisis management, and continuity planning teams with 
a valuable and structured and synchronized means to exchange vital information, both internally 
and externally.
BERT and CERT are about readiness, people helping people, rescuer safety, and doing 
the greatest good for the greatest number. BERT/CERT is a positive and realistic approach 
to emergency and disaster situations where citizens will be initially on their own and their 
actions can make a life-and-death diff erence to us, our loved ones, coworkers, and/or neigh-
bors. Th rough training, citizens can manage utilities and put out small fi res; treat the three 
killers by opening airways, controlling bleeding, and treating for shock; provide basic medical 
aid; search for and rescue victims safely; and organize themselves and spontaneous volunteers 
to be eff ective.
FEMA defi nes an emergency as ‚Äúany unplanned event that can cause deaths or signifi cant 
injuries to employees, customers, or the public; or that can shut down your business, disrupt 
operations, cause physical or environmental damage, or threaten the facility‚Äôs fi nancial standing 
or public image.‚Äù
To help prepare ourselves to withstand such a serious unplanned event, this chapter has 
attempted to provide potential solutions to prepare ourselves to help ourselves. And, at the end of the 
day, this preparedness may be all we have that stands between us and tragedy.
About the Author
Carl Jackson, CISSP, CBCP, is the former director of the Business Continuity Program at Pacifi c 
Life Insurance Company in Newport Beach, California.
References
 
1. Section 2 of the Homeland Security Act of 2002 (6 U.S.C. 101).
 
2. R. A. Falkenrath, Th e problems of preparedness: Challenges facing the U.S. Domestic Preparedness 
Program. BCSIA Discussion Paper 2000-28, ESDP Discussion Paper ESDP-2000-05, John F. Kennedy 
School of Government, Harvard University, December 2000, p. 4.
 
3. B. Baughman, Testimony before the Subcommittee on Military Procurement, Committee on Armed 
Services, U.S. Senate, March 5, 2002.

CERT/BERT: Community and Business Emergency Response ‚óæ 409
 
4. Department of Homeland Security, http://www.dhs.gov/xprepresp/
 
5. CERT, http://www.citizencorps.gov/cert/
 
6. Citizen Core, http://www.citizencorps.gov/cert/index.shtm
 
7. Business Emergency Response Team Training [BERTT],Orange County, California, http://oc-redcross.
org/Education/course.aspx?c=3744
 
8. CERTT and BERTT Kits & Equipment for Community Emergency Response Teams, http://www.
cpr-savers.com/emergency/cert-kits-equipment.html (CPR Savers & First Aid Supply LLC)


DOMAIN 
9
LAW, REGULATIONS, 
COMPLIANCE, 
AND INVESTIGATION
Major Categories 
of Computer Crime


413
26
Chapter 
Cyberstalking
Micki Krause Nozaki
Introduction
In 1998, J. Reid Meloy wrote: ‚ÄúStalking is an old behavior, but a new crime.‚Äù [1]
A decade later, we can state: Cyberstalking is a new behavior and an even newer crime. Th is 
chapter will address the what, who, why, where, and how of cyberstalking, including the many 
defi nitions of the word and the behaviors demonstrated by cyberstalkers; where and how cyber-
stalking is done; who propagates this potentially criminal behavior and why; the laws surrounding 
this relatively recent phenomenon; and, fi nally, the steps to consider should someone fi nd them-
selves in the unfortunate situation as the victim of a cyberstalker.
Robert Lloyd-Goldstein, MD, JD, in his contribution to the aforementioned anthology states: 
‚ÄúUnsurprisingly, the growth of new technologies, such as the proliferation of computers and the 
ubiquity of the world wide web, has been accompanied by the emergence of electronic surveil-
lance, email stalking and internet harassment, among other forms of online crime.‚Äù
Contents
Introduction .............................................................................................................................413
What Is Cyberstalking? .............................................................................................................414
How Is It Done? .......................................................................................................................415
Who Cyberstalks? .....................................................................................................................415
Why Cyberstalk? ......................................................................................................................416
Cyber-Behaviors .......................................................................................................................416
Cyberstalking and the Law .......................................................................................................417
Cyberstalking Laws in Other Countries ....................................................................................419
How Can I Tell If I‚Äôm a Cyberstalking Victim? .........................................................................419
How Can We Be on the Alert? .................................................................................................419
Keeping Your Children Safe Online ........................................................................................ 420
What If I Th ink I‚Äôm a Victim? ................................................................................................. 420
References ................................................................................................................................421

414 ‚óæ Information Security Management Handbook
As Figure 26.1 demonstrates, the pervasive growth and ubiquity of technology presents increasing and 
omnipresent methods of electronic communication, each of which off ers an avenue for cyberstalking.
Cyberstalking‚Äîthe word alone conjures up visions of a creepy-looking, shifty-eyed, mal-intended 
marauder, roving about, preparing to molest or attack some unsuspecting, innocent victim.
In many ways, this is not too far from the truth.
Stalking, in its traditional sense, is the obsessive following, observing, or contacting of another 
person, or the obsessive attempt to engage in any of these activities. It‚Äôs been around for as long as 
man could well stalk. Stalking victims broadly range from ordinary citizens such as Joan 6-pack 
to public fi gures such as John Lennon and Jodie Foster.
Cyberstalking, on the other hand, is a relatively new occurrence and incidents are rarely 
publicized, that is to say, cyberstalking is typically an anonymous crime.
What Is Cyberstalking?
According to the U.S. Department of Justice, cyberstalking is
‚Äú‚Ä¶ the use of the Internet, e-mail, or other electronic communications devices to stalk 
another person. Stalking generally involves harassing or threatening behavior that an 
individual engages in repeatedly, such as following a person, appearing at a person‚Äôs 
home or place of business, making harassing phone calls, leaving written messages or 
objects, or vandalizing a person‚Äôs property. Most stalking laws require that the per-
petrator make a credible threat of violence against the victim; others include threats 
against the victim‚Äôs immediate family; and still others require only that the alleged 
stalker‚Äôs course of conduct constitute an implied threat.‚Äù[2]
Little statistical data has been compiled on cyberstalking incidents (e.g. see Figure 26.2) 
Nonetheless, what data does exist forecasts daunting news as shown in a report from the United 
States Department of Justice [2]:
Voice mail
Instant
messaging
Web sites
e-mail
Cell phone
Chat
rooms
Figure 26.1 Stalking opportunities abound.

Cyberstalking ‚óæ 415
Th ere may be tens or even hundreds of thousands of cyberstalking victims in the United 
 
‚óæ
States
Almost 25% of stalking incidents among college-age women involved cyberstalking
 
‚óæ
Th e numbers are not surprising, given the decreasing expense and thereby increased availability of 
computers and online services. More and more individuals are purchasing computers and logging 
onto the Internet. Moreover, since cyberstalking is done behind the scenes, that is, anonymously, 
a veil of perceived safety protects the perpetrator.
How Is It Done?
Cyberstalkers target their victims through chat rooms, message boards, discussion forums, and 
e-mail. Th eir behaviors take many forms including threatening or obscene e-mail; sending the 
victim a stream of spam or junk mail; live chat harassment or fl aming; leaving improper messages 
on message boards; sending malware, e.g., viruses; sending unsolicited e-mail; tracing another 
person‚Äôs computer and Internet activity; and electronic identity theft.
Similar to the eff ects of stalking off -line, victims of online stalking report it as a terrifying 
experience. Police reports indicate that many times, cyberstalking situations evolve into off -line 
stalking, and a victim may experience abusive and excessive phone calls, vandalism, threatening 
or obscene mail, trespassing, and physical assault.
Who Cyberstalks?
As Figure 26.3 shows, anyone can be online stalker, although the majority are male and the major-
ity of their victims are female.
Cyberstalking is often perpetrated by people that are known to the victim. In fact, as shown 
in Figure 26.4, almost 50% of the victims knew their stalker.
2000
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
2001
2002
2003
2004
2005
2006
2007
e-mail
Message board
IM
Chat
Web site
Other
Figure 26.2 Primary sources of cyberstalking. Other includes auctions, personals, online dating, 
virus, hacking, greeting cards, gaming, mailing list, Webcam, blogs, guestbook, spyware, trojans. 
(Courtesy of haltabuse.org)

416 ‚óæ Information Security Management Handbook
Th ey may be coworkers, former spouses, or frustrated 
suitors whose advances were ignored or rejected. Th ey could 
also be fans or groupies, especially when a cyber-celebrity 
or well-known chat room or discussion board leader is 
involved.
Why Cyberstalk?
Bad people do bad things. Th ere are just as many predators on the 
Web as there are in real life. Th e only diff erence is the manner in 
which the behavior is manifested. What is diff erent is the meth-
ods they use to victimize. As in real life, a minority of predators 
are abusing technology to prey on the innocent. Electronic stalking behavior is really just a refl ection of 
the world in which we live.
Cyberstalkers are often driven by revenge, hate, anger, jealousy, obsession, and mental illness. 
Sometimes, the perpetrator intends to teach the victim a lesson in netiquette. Often, the victim is 
merely in the wrong place at the wrong time, or has made a comment or expressed an opinion that the 
stalker dislikes.
Sitting alone behind a computer lends itself to a feeling of security, a sense of anonymity, which 
allows one to demonstrate behaviors not comfortably displayed face to face. Cyberstalkers can be rude, 
insensitive, angry, passionate, exuberant, etc. In fact, it can be said that the experience of online com-
munications can be likened to the consumption of alcohol‚Äîthe lowering of inhibitions and an increase 
in directness.
Cyber-Behaviors
Cyberstalkers meet or target their victims by using a multitude of available technologies including 
search engines, online forums, bulletin and discussion boards, chat rooms, and very recent off erings 
such as MySpace, Facebook, or Friendster. Th ese social networks are essentially online communities, 
which allow for the propagation (and potential abuse) of personal information. Victims of cyber-
stalking may not even know that they are being stalked.
2003
2002
0%
10%
20%
30%
40%
50%
60%
70%
80%
2004
2005
2006
2007
Female
Male
Unknown
Figure 26.3 Gender of victim. (Courtesy of haltabuse.org)
Yes
No
Figure 26.4 Cyberstalker known 
to victim (2007). (Courtesy of 
haltabuse.org)

Cyberstalking ‚óæ 417
Cyberstalkers oftentimes research individuals to feed their obsessions and curiosity as seen 
from the following excerpt from an article in the November 2008 Marie Claire magazine.
My boyfriend‚Äôs ex-wife is a 29 year old, 5-foot-9 costume designer and lover of whis-
key, Hitchcock and Lena Horne. She‚Äôs agnostic, wants children someday, and has a 
ruby-lipped smile. How do I know all this? I cyberstalked her. [3]
More commonly they will post defamatory or derogatory statements about their target on Web 
pages, message boards, and in guest books hoping to get a reaction or response from their victim, 
thereby initiating contact. Once they get a reaction from the victim, they will typically attempt to 
track or follow the victim‚Äôs Internet activity.
Classic cyberstalking behavior includes the tracing of the victim‚Äôs Internet address in an 
attempt to verify their home or place of employment. Online dating Web sites are another com-
mon way for cyberstalkers to ‚Äúmeet‚Äù their prey. All too often, however, ‚Äúthere is a false degree 
of safety assumed by women looking for love online,‚Äù as reported in a 2007 study led by Paige 
Padgett from the University of Texas. [4]
Examining the choices women made when meeting men from online personal ads for 
friendships, love, and sex, Dr. Padgett concluded that the high frequency and intensity of 
e-mail communication, prior to meeting face to face, lent itself to an accelerated form of inti-
macy on behalf of her participants. Said Padgett, ‚ÄúTh is may have aff ected women‚Äôs decisions 
to engage in risky sexual behavior.‚Äù
Cyberstalking and the Law
With personal information becoming readily available to an increasing number of people through 
the Internet and other advanced technology, state legislators are addressing the problem of stalkers 
who harass and threaten their victims over the Internet.
Although cyberstalking in the United States is addressed at the federal level (the current U.S. 
Federal Anti-Cyber-Stalking law (47 USC sec 223)), the predominance of regulation can be found 
at the state level. Th e Violence Against Women Act, passed in 2000, made cyberstalking a part of 
the federal interstate stalking statute. Still, there remains a lack of legislation at the federal level 
to specifi cally address cyberstalking, leaving the majority of legislative prohibitions against cyber-
stalking at the state level.
Most stalking laws require that the perpetrator make a credible threat of violence against the 
victim; others include threats against the victim‚Äôs immediate family; and still others require the 
alleged stalker‚Äôs course of conduct constitute an implied threat. While some conduct involving 
annoying or menacing behavior might fall short of illegal stalking, such behavior may be a prelude 
to stalking and violence and should be treated seriously.
Online identity stealth blurs the line on infringement of the rights of would-be victims to 
identify their perpetrators. Th ere is a debate on how Internet use can be traced without infringing 
on protected civil liberties.
The first U.S. cyberstalking law went into effect in 1999 in California. Other states 
include prohibition against cyberstalking in their harassment or stalking legislation. In 
Florida, HB 479 was introduced in 2003 to ban cyberstalking. This was signed into law on 
October 2003.

418 ‚óæ Information Security Management Handbook
Some states in the United States have begun to address the issue of cyberstalking (National 
Center for Victims of Crime):
Alabama, Arizona, Connecticut, Hawaii, Illinois, New Hampshire, and New York have 
 
‚óæ
included prohibitions against harassing electronic, computer or e-mail communications in 
their harassment legislation.
Alaska, Florida, Oklahoma, Wyoming, and California have incorporated  electronically 
 
‚óæ
communicated statements as conduct constituting stalking in their anti-stalking 
laws.
Texas enacted the 
 
‚óæ
Stalking by Electronic Communications Act, 2001.
Missouri revised its state harassment statutes to include stalking and harassment by telephone 
 ‚óæ
and electronic communications (as well as cyber-bullying) after the Megan Meier suicide case 
of 2006. [5]
A few states have both stalking and harassment statutes that criminalize threatening and 
 
‚óæ
unwanted electronic communications.
Other states have laws other than harassment or anti-stalking statutes that prohibit misuse 
 
‚óæ
of computer communications and e-mail, while others have passed laws containing broad 
language that can be interpreted to include cyberstalking behaviors.
Most stalking statutes require that the perpetrator make a real or implied ‚Äúcredible threat‚Äù of 
violence against the victim or the victim‚Äôs family. Th e laws are suffi  ciently disparate as the threat 
factors, which leads to some level of interpretation as to whether the cyberstalking is reportable. 
However, most agree that there are common factors, as shown in Table 26.1.
Table 26.1 Common Factors in Cyberstalking
Key Factor
Description
False accusations
Posting false information about a person on public 
Web sites
Attempts to gather information 
about the victim
Monitoring the victim‚Äôs online activities; tracing the 
victim‚Äôs Internet address; and soliciting personal 
information about the victim from the victim‚Äôs family, 
friends, or acquaintances
Encouraging others to harass 
the victim
Involving third parties in the harassment
False victimization
Claiming that the victim is actually doing the harassment
Attacks on data and equipment
Attempting to damage the victim‚Äôs electronic equipment 
by, e.g., sending an e-mail message infected with malware
Ordering goods and services
Subscribing and sending goods to victim‚Äôs home or 
workplace
Arranging to meet
Setting up in-person meetings with their victims (typically 
done with younger victims)
Source: Courtesy of National Center for Victims of Crime, Washington, DC.

Cyberstalking ‚óæ 419
Cyberstalking Laws in Other Countries
Other countries have begun to include online abuse in their anti-stalking legislation. In Australia, 
the Stalking Amendment Act (1999) includes the use of any form of technology to harass a target 
as forms of ‚Äúcriminal stalking.‚Äù In the United Kingdom, the Malicious Communications Act 
(1998) classifi ed cyberstalking as a criminal off ense.
How Can I Tell If I‚Äôm a Cyberstalking Victim?
When identifying cyberstalking ‚Äúin the fi eld,‚Äù and particularly when considering whether to 
report it to any kind of legal authority, the following features or combination of features can be 
considered to characterize a true stalking situation:
Malice
 
‚óæ
Premeditation
 
‚óæ
Repetition
 
‚óæ
Distress
 
‚óæ
Obsession
 
‚óæ
Vendetta
 
‚óæ
No legitimate purpose
 
‚óæ
Personally directed
 
‚óæ
Disregarded warnings to stop
 
‚óæ
Harassment
 
‚óæ
Th reats
 
‚óæ
How Can We Be on the Alert?
Th ere are several good resources on the Internet that provide information in anti-stalking.
Here are a few important pointers to help you understand how to thwart cyberstalking:
Th e following checklist gives you information on how to avoid becoming a victim
Maintain vigilance over physical access to your computer and other Web-enabled devices. 
 
‚óæ
Cyberstalkers use software and hardware devices (sometimes attached to the back of your 
PC without your knowledge).
Be sure you always log out of your computer programs when you step away from the com-
 
‚óæ
puter and use a screensaver with a password. Your entire family should develop the same 
good habits.
Make sure to practice good password management and security‚Äînever share your pass-
 
‚óæ
words with others. And be sure to change your passwords frequently!
Use search engines such as ‚ÄúGoogle‚Äù to search for yourself and your family members 
 
‚óæ
now and then to check on what‚Äôs available about you and your kids online. Don‚Äôt 
be shy about searching social networks and be sure to remove anything private or 
inappropriate.
Get rid of calendars or itineraries regarding your future travels from Web sites and social 
 ‚óæ
networks.

420 ‚óæ Information Security Management Handbook
If you suspect that someone is using spyware software to track your everyday activities, and 
 
‚óæ
you feel as if you‚Äôre in danger, seek help.
As always, use good, updated security software to prevent someone from getting spyware 
 
‚óæ
onto your computer via a phishing attack or an infected Web page.
Limit the amount of personal information you provide for online resources such as social 
 
‚óæ
networks.
Keeping Your Children Safe Online
Keep the computer in a central family location, not in the child‚Äôs room.
 
‚óæ
Get to know your children‚Äôs online friends.
 
‚óæ
Screen e-mail with all younger children.
 
‚óæ
Help your children keep computing online in balance.
 
‚óæ
If you can‚Äôt be home with them when they‚Äôre online, use child protection software to help 
 
‚óæ
keep an eye on them.
Make sure they understand that they should never meet anyone in real life that they met 
 
‚óæ
online without parents in attendance.
What If I Think I‚Äôm a Victim?
 
1. In order to locate local victim service professionals who may be able to off er assistance, safety 
suggestions, and information and referrals, contact the Helpline of the National Center 
for Victims of Crime at 1-800-FYI-CALL, 8:30 a.m.‚Äì8:30 p.m., Monday through Friday, 
Eastern Standard Time.
 
2. Th e Privacy Rights Clearinghouse
 
 3100 5th Avenue., Suite B
 
 San Diego, CA 92103
 
 (619) 298-3396
 
3. Resources on the Internet:
 
a. National Center for Victims of Crime Stalking Resource Center
 
b. National Network to End Domestic Violence (NNEDV)
 
c. Working to Halt Online Abuse (WHOA)‚Äîwhoa@haltabuse.org
 
d. CyberAngels
 
e. Safety Ed International
 
f. Electronic Privacy Information Center (EPIC)
 
g. Online Privacy Alliance
 
h. Network Solutions WHOIS‚Äîhelps determine contents of domain name registration
 
4. Your local prosecutor‚Äôs offi  ce, law enforcement, or state Attorney General‚Äôs offi  ce. Check 
in the Blue Pages of your local phone book under the appropriate section heading of either 
‚ÄúLocal Government,‚Äù ‚ÄúCounty Government,‚Äù or ‚ÄúState Government.‚Äù
 
5. Consumer reports online: http://www.consumerreports.org/electronics-computers/resource-
center/cyber-insecurity/cyber-insecurity-hub.htm (must be a subscriber of Consumer 
Reports)

Cyberstalking ‚óæ 421
References
 
1. J. R. Meloy (ed.) (1998). Th e Psychology of Stalking: Clinical and Forensic Perspectives. Academic Press, 
San Diego, CA, ISBN 0124905617, 9780124905610, 327 pp.
 
2. U.S. Department of Justice (August 1999). Cyberstalking: A new challenge for law enforcement 
and industry, a report from the Attorney General to the Vice President. U.S. Department of Justice, 
Washington, DC, pp. 2, 6.
 
3. A. Whitefi eld-Madrano (2008). Confessions: Google stalking, Marie Claire, November 2008.
 
4. P. Padgett (2007). Personal safety and sexual safety for women using online personal ads, Sexuality 
Research and Social Policy, 4(2); 27‚Äì37.
 
5. S. Michels, Prosecutors bringing charges under law inspired by Megan Meir suicide, http://abcnews.
go.com/Th eLaw/story?id = 6520260&page=1, December 24, 2008. Accessed March 8, 2010.


Incident Handling


425
27
Chapter 
Is Software Write Blocking 
a Viable Alternative to 
Hardware Write Blocking 
in Computer Forensics?
Paul A. Henry
Hardware write blockers have historically been the only choice in protecting the integrity of evi-
dence in computer forensics. Th is chapter explores some of the increasing competitive issues and the 
often overlooked total cost of ownership considerations that are driving some within the industry 
to seek out software-based alternatives. Further, it explores the viability of using current generation 
software write blocking as an alternative to hardware write blocking by providing the details of the 
authors actual ‚Äúhands-on‚Äù testing of a software write blocker to validate its forensic soundness.
Contents
My Personal Observations ....................................................................................................... 426
Exploring Software-Based Alternatives ..................................................................................... 426
Taking a Software Write Blocker for a Test Drive ..................................................................... 427
But Is Software Write Blocking Really Forensically Sound ....................................................... 428
Forensic Validation of SAFE Block XP Version 1.1 .................................................................. 428
Step 1: Prepare the Media ................................................................................................... 428
Step 2: Test the Media ........................................................................................................ 432
Step 3: Activate SAFE Block Write Blocking Device ........................................................... 436
Step 4: Test the Write Blocking Device ............................................................................... 437
Step 5: Check for Any Changes in the Media ..................................................................... 440
Conclusion .............................................................................................................................. 442
About the Author .................................................................................................................... 445

426 ‚óæ Information Security Management Handbook
My Personal Observations
If you have been in computer forensics for any length of time you already have had to put together 
quite a collection of hardware write blocking devices for drive imaging and for knock-and-look 
events. But it never seems to be enough; fi rst you have to deal with the issue of legacy drive inter-
faces that you may still run into today such as SCSI, IDE/ATA, and today‚Äôs newer SATA drives. 
We have to consider the myriad of new small form factor laptop drives, the new solid state drives 
appearing in current generation laptops and let us not forget all of the external drives with their 
USB, IEEE1394, and eSATA interfaces. You are constantly buying new hardware write blockers 
in order to be able to simply keep up with the drives you are faced with imaging.
For a commercial forensic practice there is another consideration; you have to keep in mind 
that you are competing with other computer forensic service vendors that may have already 
upgraded to today‚Äôs latest eSATA interface‚Äìbased write blockers. You could very well fi nd yourself 
priced out of competition because of your slower legacy acquisition speeds. So for anyone off ering 
commercial forensic services, it is not just simply necessary to keep up with the evolution of hard 
drives themselves but also a matter of upgrading write blockers to take advantage of faster interface 
speed, or worst case, lowering one‚Äôs billable imaging rates in order to remain competitive.
Evolution of drive acquisition speeds (actual not theoretical):
USB 2.0
30 MB/s
Firewire 400
35 MB/s
Firewire 800
75 MB/s
eSATA-150
115 MB/s
For a small forensics shop, keeping up to date with hardware write blockers can get expensive. In order 
to be able to handle the common drive situations you will run into today, you actually need multiple 
hardware write blockers, and then there is the issue of the evolving interfaces to your PC and keeping 
your imaging services competitive.
Buying used write blocking gear to try to keep your costs down is not necessarily a viable solu-
tion either. I recently purchased a hardware write blocker/high-speed imager myself on eBay. Th e 
particular device ‚Äúnew‚Äù (in the confi guration I bought it in) was worth just under $5000 and I was 
able to pick it up for only $2000. I validated it against a handful of drives with known hashes‚Äî
speed was great averaging around 3.6 GB/min and the hashes all matched (forensically sound) 
and all seemed well. However, the very fi rst job I used it on just weeks later, it died‚Äîno available 
power for the target drive. I contacted the vendor and knowing it was out of the 1 year warranty I 
asked about a repair, and the price I was quoted was $600 regardless of what was wrong with it‚ÄîI 
was told this was the company policy. What appeared to be nothing more than perhaps a $5 com-
ponent was going to cost me $600 and if I opened the case and tried to repair it myself, I would be 
banished to ‚Äúvoided warranty land‚Äù forever‚Äînever then being able to resell the device.
Exploring Software-Based Alternatives
For me, it seemed to be a good time to perhaps reconsider hardware write blockers in general as 
really being my only choice in producing forensically sound images. Looking at the constantly 
changing environment in drives and interfaces while also considering the investment I already have 

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 427
in lab and portable forensic servers, using a forensically sound software write blocker is beginning 
to look like a viable alternative. All of my forensic servers already have removable IDE‚ÄîSATA 
drive trays and multiple USB‚ÄîIEEE1394 and eSATA ports. I wondered if I could fi nd a software 
write blocker solution that would work across literally anything I could connect to my forensic 
servers. I could reduce the costs associated with both maintaining my existing hardware write 
blockers and the cost of upgrading them to remain competitive from a performance perspective.
I spoke to peers and also to a few hardware acquisition/write blocker vendors. Th e big areas of push 
back they pointed out to me in considering moving to a software write blocker are performance, fea-
tures, and forensic soundness. Well performance seemed easy enough to test, I only found one feature 
that a hardware product off ered‚Äîthe ability to search for keywords during an acquisition‚Äîa feature I have 
yet to have a client request, so all that remained was testing for forensic soundness, something that is 
already a best practice whenever you buy any new hardware write blocker or upgrade its fi rmware. So 
taking a closer look at software write blockers was now a high priority for me.
I read through the reports at NIST on software write blocker validation and it certainly seemed 
to me that software write blockers are perhaps coming of age. Further research on the Internet led 
me to SAFE Block XP 1.1 from Forensic Soft. I read the test results from the NIST write blocking 
suite on the Web site, looked at the price $219 and decided to take a closer look using their free 
trial off er (http://www.forensicsoft.com/catalog/index.php). I downloaded the install software and 
fi lled out the registration form so a temporary license key could be sent via e-mail. Th is was a sim-
ple process, and literally I was up and running in minutes ready to write block any IDE (PATA & 
SATA), SCSI, FC, SAS, USB, or IEEE1394 drive I could connect to my portable forensic server. I 
ran the SAFE Block software for a couple of weeks simply getting myself used to the GUI and its 
overall operation and to see if it had any negligible impact on my system‚Äîwithout relying upon 
it as a write blocker for any case-related forensic evidence.
Taking a Software Write Blocker for a Test Drive
My fi rst impression of the GUI was pretty good‚Äîall of my interfaces for not only my boot hard 
drive but all of my removable drives and my external ports could be quickly write protected, and 
just as importantly, I could password protect my confi guration. I chose to set up my confi gura-
tion to automatically block literally anything I could plug in to the machine by default but to also 
remember anything beyond the default that I had changed. Th at is, default block but then allow a 
particular USB drive to be written to‚Äîshut down, reboot, and have it come back to the previous 
state with access allowed to the USB drive I permitted earlier, but anything else I then plugged in 
was automatically write protected.
What about performance? I dropped a 250 GB SATA drive into one of my removable trays 
and decided to see how well it performed when imaging. Having lived in a world of imaging hard 
drives at rates that seem to range from as little as 1 MB/s to a best case of around 15 MB/s using a 
hardware write block and a USB connection, I was only expecting a marginal improvement. Using 
FTK Imager and with the software write block enabled for the physical drive in the SAFE Block XP 
GUI, I was able to image a 250 GB SATA drive in exactly 69 min. Th at works out to a rate of just 
under 60 MB/s‚Äîthat is close to the 3.6 GB/min. I have only been averaging getting 3.6 from my 
dedicated ‚Äúhigh-end‚Äù stand-alone acquisition hardware. You have to consider that the same drive 
would have taken me between 6 and 8 h to image using a USB connection with a hardware write 
blocker. I then tried to validate the image I created against the drive, the MD5 hash validation ran 
at over 90 MB/s as well‚Äîunheard of speed in a write block, USB, or even IEEE1394 world.

428 ‚óæ Information Security Management Handbook
But Is Software Write Blocking Really Forensically Sound?
Rather than simply accepting the vendor‚Äôs claims, I decided to test it myself with a SATA drive. 
I used the NCFS fi ve-step procedure for validating a write blocker outlined in the HELIX docu-
mentation and adjusted it for the specifi cs of the hardware confi guration of my portable forensic 
workstation.
 
1. Prepare the media.
 
a. Insert the drive into the removable drive tray.
 
b. Wipe drive and validate.
 
c. Format the drive.
 
d. Copy data to the drive.
 
e. Delete a portion of the data on the drive.
 
f. Since all of my drives are confi gured to use write caching for performance, I added the 
extra step of fl ush the drive write cache using the MS SysInternals ‚ÄúSync‚Äù program.
 
g. Image and MD5 Hash the drive to a folder called ‚ÄúStep-1‚Äù.
 
2. Test the media.
 
a. Copy additional data to the drive.
 
b. Delete a portion of the data that was written to the drive.
 
c. Flush the drive cache.
 
d. Image and MD5 Hash the drive to a folder called ‚ÄúStep-2‚Äù.
 
e. Compare the MD5 Hash for the drive in folder ‚ÄúStep-1‚Äù with the MD5 Hash for the 
image in folder ‚ÄúStep-2.‚Äù Th e media had in fact been changed so the MD5 Hash values 
should be diff erent.
 
3. Activate the write blocking device.
 
a. Activate the software write block for the drive under test.
 
4. Test the write blocking device.
 
a. Attempt to copy data to the drive.
 
b. Attempt to delete data from the drive.
 
c. Attempt to format the drive.
 
d. Flush the drive cache.
 
e. Image and MD5 Hash the drive to a folder called ‚ÄúStep-5‚Äù.
 
5. Check for any changes in the media.
 
a. Compare the MD5 Hash for the image in folder Step-2 and the MD5 Hash for the image 
contained in folder Step-5. If the write block is forensically sound the MD5 Hashes will 
match validating that the write block prevented any changes to the drive.
Forensic Validation of SAFE Block XP Version 1.1
Step 1: Prepare the Media
My portable forensic server uses removable drive trays, so I shut down and replaced the XP boot 
drive with my Fedora boot drive and booted into Fedora Linux (confi gured for ‚ÄúNOSMP‚Äù). I used 
SMART to wipe the drive with all ‚Äú0‚Äù characters and verifi ed via Hex view that the drive had been 
properly wiped. Rebooted to XP and formatted the drive for NTFS (Figure 27.1).

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 429
Figure 27.1 Step 1 in forensic validation of SAFE Block XP Version 1.1 to prepare the media.
I set the drive letter to F: and assigned the drive label as ‚ÄúSB_Test‚Äù (Figure 27.2).
Figure 27.2 The drive letter set to F: and assigned the drive label as ‚ÄúSB_Test‚Äù.

430 ‚óæ Information Security Management Handbook
I then verifi ed that software write block was not enabled for the drive under test (Figure 27.3).
Figure 27.3 Verifi cation that software write block was not enabled for the drive under test.
I copied the Program Files directory (folder) from the portable forensic workstation C: drive to 
the F: drive.
I copied the \system directory (folder) from the portable forensic workstation C: drive to the F: 
drive.
I then deleted the \system directory from drive F:
I ran the sync command from the DOS prompt on the F: drive (Figure 27.4).
Figure 27.4 The system after running the sync command from the DOS prompt on the F: drive.

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 431
I then created a physical image (Figure 27.5) of the drive using FTK Imager and selected Raw (dd) 
image (Figure 27.6).
Figure 27.5 Creation of a physical image of the drive using FTK Imager.
Figure 27.6 Selection Raw (dd) image.
I set the destination as folder ‚ÄúStep-1‚Äù and imaged drive F: (Figure 27.7).

432 ‚óæ Information Security Management Handbook
Figure 27.7 Destination set as folder ‚ÄúStep-1‚Äù and F: drive imaged.
Note the 57.268 MB/s speed.
Image complete for Step-1 and the MD5 Hash was recorded as:
2a2363a3640b57760e3bcbdcbf25fcd6
Step 2: Test the Media
I added additional fi les to the F: drive (Figure 27.8).

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 433
Figure 27.8 Additional fi les added to the F: drive.
I then deleted a portion of the fi les that I had just been added (Figure 27.9).

434 ‚óæ Information Security Management Handbook
Figure 27.9 A portion of recently added fi les deleted.
I fl ushed the drive cache (Figure 27.10).
Figure 27.10 The drive cache fl ushed.

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 435
I created a physical image (Figure 27.11) of the drive using FTK Imager and selected Raw (dd) 
image (Figure 27.12).
Figure 27.11 FTK Imager used to create a physical image of the drive.
Figure 27.12 Raw (dd) image selected.
I set the destination as folder ‚ÄúStep-2‚Äù and imaged the F: drive (Figure 27.13).

436 ‚óæ Information Security Management Handbook
Figure 27.13 The destination set as folder ‚ÄúStep-2‚Äù and the F: drive imaged.
I recorded image MD5 Hash d1e2a8e6e99475eb53d9a3ca52600dc9
I then compared the MD5 hashes for the 2 images:
MD5 Hash from Folder Step-1 2a2363a3640b57760e3bcbdcbf25fcd6
MD5 Hash from Folder Step-2 d1e2a8e6e99475eb53d9a3ca52600dc9
As expected, the MD5 Hashes do not match as the drive had in fact been changed.
Step 3: Activate SAFE Block Write Blocking Device
I enabled write blocking for the drive within the GUI for SAFE Block XP (Figure 27.14).

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 437
Figure 27.14 Write blocking enabled for the drive within the GUI for SAFE Block XP.
Step 4: Test the Write Blocking Device
I attempted to copy the folder ‚ÄúForensics‚Äù from the C: drive to the F: drive (Figure 27.15) and 
received the following error message:
Figure 27.15 Error message received after attempt to copy the folder ‚ÄúForensics‚Äù from the C: 
drive to the F: drive.
I attempted to drag and drop the Intel folder from the C: drive to the F: drive (Figure 27.16) and 
received the following error message:

438 ‚óæ Information Security Management Handbook
Figure 27.16 Error message received after attempt to drag and drop the Intel folder from the 
C: drive to the F: drive.
Figure 27.17 Error message received after attempt to delete a fi le from the F: drive.
Attempted to delete a fi le from the F: drive (Figure 27.17).
I then attempted to format the F: drive at the DOS prompt and DOS responded by informing 
me that the media was write protected. I then attempted to delete all fi les from the F: drive at 
the DOS prompt (Figure 27.18) and again DOS responded by informing me that the media is 
write protected.
Figure 27.18 Following an attempt to delete all fi les from the F: drive at the DOS prompt, DOS 
responded saying that the media is write protected.

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 439
I fl ushed all data in the drive cache for drive F: (Figure 27.19)
Figure 27.19 All data in the drive cache for the F: drive fl ushed.
I created a physical image (Figure 27.20) of the F: drive using FTK Imager and selected Raw (dd) 
image (Figure 27.21).
Figure 27.20 A physical image of the F: drive created using FTK Imager.

440 ‚óæ Information Security Management Handbook
Figure 27.21 Raw (dd) image selected.
I set the destination as folder ‚ÄúStep-5‚Äù and imaged the F: drive (Figure 27.22). Note the acquisition speed 
of 53.969 MB/s while operating against a write protected drive. Th at works out to 3.23 GB/min.
Figure 27.22 The destination set as folder ‚ÄúStep-5‚Äù and the F: drive imaged. Note the acquisi-
tion speed of 53.969 MB/s while operating against a write protected drive, which works out to 
3.23 GB/min.
I recorded the image MD5 Hash d1e2a8e6e99475eb53d9a3ca52600dc9
Step 5: Check for Any Changes in the Media
I then compared the MD5 Hash from the image in folder Step-2 with the image in folder Step-5:
MD5 Hash from Folder Step-2 d1e2a8e6e99475eb53d9a3ca52600dc9
MD5 Hash from Folder Step-5 d1e2a8e6e99475eb53d9a3ca52600dc9

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 441
Th e Hashes match the drive and did not change‚ÄîSAFE Block XP version 1.1 has been found to be 
forensically sound.
I then removed the drive and imaged it with my hardware imager to see what the performance 
diff erence might be and to verify the drive MD5 Hash. I confi gured the imager to use the 
UDMA5 speed selection‚Äîit automatically then adjusts the speed to work best with the source 
and target drive. Th e target drive was a 3 GB/s 1 TB SATA drive. Note that the hardware 
imager was operating with an acquisition speed of 2.42 GB/min (Figure 27.23) and we previ-
ously attained an acquisition speed of 3.23 GB/min while imaging behind the software write 
block (Figure 27.22).
Figure 27.23 The hardware imager was operating with an acquisition speed of 2.42 GB/min.

442 ‚óæ Information Security Management Handbook
Figure 27.24 Imaging completed.
Th e MD5 Hash of the image of the drive produced behind the software write block matched that 
produced by the hardware imager (Figure 27.24).
Conclusion
Software write blocking as presented in the testing of SAFEBlock XP 1.1 is a viable and forensi-
cally sound alternative to current hardware write blocking solutions. When one considers the 
total cost of ownership and ongoing maintenance and warranty costs of hardware write blocking 
devices, the dramatically reduced cost of software write blocking presents an undeniable business 
case for consideration.
Complete acquisition report from folder Step 1
Created By AccessData¬Æ FTK¬Æ Imager 2.5.4.16 080324
Case Information: Test of SAFE Block software write block
Case Number: sbtest001
Evidence Number: sbtest001
Unique Description:
Examiner: Paul A. Henry
Notes:
Information for D:\Step-1\sbtest001:
Physical Evidentiary Item (Source) Information:
[Drive Geometry]

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 443
Cylinders: 30,515
Tracks per Cylinder: 255
Sectors per Track: 63
Bytes per Sector: 512
Sector Count: 490,234,752
[Physical Drive Information]
Drive Model: Maxtor 7Y250M0
Drive Serial Number: 36593039374d4547202020202020202020202020
Drive Interface Type: IDE
Source data size: 239372 MB
Sector count: 490234752
[Computed Hashes]
MD5 checksum: 2a2363a3640b57760e3bcbdcbf25fcd6
SHA1 checksum: d176fce35286a23059acbc5bd376ecac0b2f1a5c
Image Information:
Acquisition started: Th u Sep 11 23:17:31 2008
Acquisition fi nished: Fri Sep 12 00:44:25 2008
Segment list:
D:\Step-1\sbtest001.001
Image Verifi cation Results:
Verifi cation started: Fri Sep 12 00:44:26 2008
Verifi cation fi nished: Fri Sep 12 01:25:26 2008
MD5 checksum: 2a2363a3640b57760e3bcbdcbf25fcd6: verifi ed
SHA1 checksum: d176fce35286a23059acbc5bd376ecac0b2f1a5c: verifi ed
Complete acquisition report from folder Step-2
Created By AccessData¬Æ FTK¬Æ Imager 2.5.4.16 080324
Case Information: Test of SAFE Block software write block
Case Number: sbtest002
Evidence Number: sbtest002
Unique Description:
Examiner: Paul A. Henry
Notes:
Information for D:\Step-2\sbtest002:
Physical Evidentiary Item (Source) Information:
[Drive Geometry]
Cylinders: 30,515
Tracks per Cylinder: 255
Sectors per Track: 63
Bytes per Sector: 512
Sector Count: 490,234,752
[Physical Drive Information]
Drive Model: Maxtor 7Y250M0
Drive Serial Number: 36593039374d4547202020202020202020202020
Drive Interface Type: IDE

444 ‚óæ Information Security Management Handbook
Source data size: 239372 MB
Sector count: 490234752
[Computed Hashes]
MD5 checksum: d1e2a8e6e99475eb53d9a3ca52600dc9
SHA1 checksum: dd4a1294cb768c1dad2235d30a6904c4ff c04d14
Image Information:
Acquisition started: Fri Sep 12 09:33:19 2008
Acquisition fi nished: Fri Sep 12 11:00:02 2008
Segment list:
D:\Step-2\sbtest002.001
Image Verifi cation Results:
Verifi cation started: Fri Sep 12 11:00:02 2008
Verifi cation fi nished: Fri Sep 12 11:46:23 2008
MD5 checksum: d1e2a8e6e99475eb53d9a3ca52600dc9 : verifi ed
SHA1 checksum: dd4a1294cb768c1dad2235d30a6904c4ff c04d14 : verifi ed
Complete acquisition report from folder Step-5
Created By AccessData¬Æ FTK¬Æ Imager 2.5.4.16 080324
Case Information: Test of SAFE Block software write block
Case Number: sbtest003
Evidence Number: sbtest003
Unique Description:
Examiner: Paul A. Henry
Notes:
Information for D:\Step-5\sbtest003:
Physical Evidentiary Item (Source) Information:
[Drive Geometry]
Cylinders: 30,515
Tracks per Cylinder: 255
Sectors per Track: 63
Bytes per Sector: 512
Sector Count: 490,234,752
[Physical Drive Information]
Drive Model: Maxtor 7Y250M0
Drive Serial Number: 36593039374d4547202020202020202020202020
Drive Interface Type: IDE
Source data size: 239372 MB
Sector count: 490234752
[Computed Hashes]
MD5 checksum: d1e2a8e6e99475eb53d9a3ca52600dc9
SHA1 checksum: dd4a1294cb768c1dad2235d30a6904c4ff c04d14
Image Information:
Acquisition started: Fri Sep 12 12:49:00 2008
Acquisition fi nished: Fri Sep 12 14:15:55 2008

Is Software Write Blocking a Viable Alternative to Hardware Write Blocking ‚óæ 445
Segment list:
D:\Step-5\sbtest003.001
Image Verifi cation Results:
Verifi cation started: Fri Sep 12 14:15:55 2008
Verifi cation fi nished: Fri Sep 12 15:12:45 2008
MD5 checksum: d1e2a8e6e99475eb53d9a3ca52600dc9 : verifi ed
SHA1 checksum: dd4a1294cb768c1dad2235d30a6904c4ff c04d14 : verifi ed
About the Author
Paul A. Henry, CISSP, MCP+I, MCSE, CCSA, CCSE, CFSA, CFSO, CISM, CISA, is senior 
vice president of CyberGuard Corporation, Deerfi eld Beach, Florida.


DOMAIN 
10
PHYSICAL SECURITY
Elements of Physical 
Security


449
28
Chapter 
Protection of Sensitive Data
Sandy Bacik
What constitutes sensitive data for an enterprise: paper copies of forms, faxes, employee data, 
insurance information, support contracts, intellectual property, or client data? How does the 
enterprise physically protect sensitive data in hard copy form or removable media? Locked closet, 
desk, or cabinet? Are there any environmental concerns with those areas? Many organizations 
are digitizing their hard copy fi les for more effi  cient and eff ective storage, but some regulations 
for records retention still require an organization to retain the hard copies. And many times, an 
enterprise will store electronic copies on removable media.
Th e amount of data that a staff  member comes across daily can be enormous. It is not possible 
to protect all the data that a staff  member can come across. Th e enterprise needs to document what 
constitutes sensitive data (data classifi cation policy) and identify the level of protection required. 
Th is chapter discusses the physical (not logical through access control) protection of sensitive data 
and what to consider in the environment.
Temperature
One of the main environmental threats to equipment and sensitive data is temperature. Th e  generally 
accepted, ideal temperature is between 68¬∞F and 74¬∞F (20¬∞C‚Äì24¬∞C) for storage of  electrical equipment 
Contents
Temperature ............................................................................................................................ 449
Humidity .................................................................................................................................450
Hard Copy Deterioration .........................................................................................................450
Light  ........................................................................................................................................451
Data-in-Motion and Data-at-Rest ............................................................................................451
Destruction of Sensitive Data ...................................................................................................452
Current Monitoring Practices ...................................................................................................452
Conclusion ...............................................................................................................................453
About the Author .....................................................................................................................455

450 ‚óæ Information Security Management Handbook
and paper. Excessive heat degrades network performance and causes downtime. As the temperature 
increases, a heat sink fan works harder to cool the central processing unit (CPU). Continuous over-
working causes the fan to fail, leading to equipment overheating. A machine shuts down when 
it reaches an unsafe temperature in order to prevent permanent damage. When that happens, an 
administrator must then be located, day or night, go to the machine, and reboot it after it has cooled. 
Consequently, services hosted by a down machine are unavailable until it is restarted, which can take 
minutes or hours. If the services are critical, revenues can be lost, users cannot login, and communi-
cations are interrupted. If the equipment shutdown is not done properly, data can be lost.
Excessive heat and rapid temperature changes damage equipment. Together, heat and moisture 
accelerate the breakdown of materials used in microchips, motherboards, and hard drives, which 
ages equipment more rapidly. Heat-damaged equipment must be replaced, increasing the cost of 
network maintenance. Controlling temperature is becoming more important and more diffi  cult 
because of changes in equipment design and greater use of network services. As old equipment 
is replaced with new equipment, that new equipment has more power and cooling requirements 
because it runs faster and hotter. New equipment also has smaller and more condensed circuit 
board, thus trapping heat in a smaller space. Th e smaller, more effi  cient, equipment is then packed 
tighter. Th e increased density increases the amount of heat dissipated within the rack and data 
center. Increased network usage also increases heat, so as usage levels change during the day, so 
does the temperature and the need for cooling. For networks that operate near capacity 24 h a day, 
every day of the year, there is little, if any, time for machines to cool down.
Strong temperature controls that include training, monitoring, and testing the temperature 
devices will ensure that equipment will have longevity in the production environment.
Humidity
Another main environmental threat to network equipment and sensitive data is humidity. 
Temperature and humidity have been shown to be interdependent. Humidity, too much  dampness 
or moisture in the air, can cause water damage to electronics, paper, and computer equipment. 
Humidity can be natural or man-made. Rapid temperature increases can increase humidity, while 
rapid drops can cause water in humid air to condense on equipment. Some causes for high humid-
ity are a mixture of hot and cold air, leaky pipes, and an increase of water used in day-to-day 
activities. Th e relative humidity should be between 40% and 50%.
High humidity levels can produce condensation problems within a data center and other offi  ce 
storage areas. Condensation occurs when humidity levels are too high or when there is a rapid tem-
perature drop and then the enterprise can potentially have water running along pipes. Condensation 
inside equipment can cause rust, short circuits, or deposits of dirt and minerals that ruin equipment.
Like a temperature control, a companion humidity control device that includes training, mon-
itoring, and testing the temperature devices will help ensure that equipment will continue to have 
longevity in the production environment.
Hard Copy Deterioration
Where there is moisture due to high humidity, there can be biological growths such as molds or 
fungi, insects and rodents infestations. Biological agents attack paper and other organic materi-
als when both temperature and humidity are not regulated properly. Mold spores and fungi can 

Protection of Sensitive Data ‚óæ 451
remain suspended in the air until they fi nd suitable conditions for their living habits. Mold and 
fungi can result in the staining and deterioration of organic materials. It is a common experience 
to note that mold and fungi growth can occur more readily on items that are tightly packed and 
have stagnant pockets of moist air, which favors mold and fungi growth.
In addition to high temperature and humidity, staff ‚Äôs cleaning negligence can favor mold and 
fungi, as well as the growth and proliferation of insects. Th is negligence can result in the following:
Accumulations of dirt and dust from poor or careless housekeeping practices on materials 
 
‚óæ
and electronics
Trails of foodstuff  in storage and exhibit areas due to staff  leaving items behind
 
‚óæ
Opening or closing of air vents or poorly sealed windows and doors
 
‚óæ
Poor ventilation in and around the materials and equipment
 
‚óæ
Rodents and insects can be some of the worst enemies of books and other organic materials. 
Insects are attracted to the proteins and carbohydrates in the form of paste, starches, or other 
organic substances. Damage can vary from a few markings and holes to complete destruction.
Light
Another cause of deterioration of sensitive data can be light. Th e types of materials that are subject to 
damage by light are pigments and dyestuff , including inks, paper, and other cellulose materials, and 
various other organic materials holding copies of sensitive data. Inks and dyestuff  fade when exposed 
to light. Unfortunately, coloring in pictures and forms fade selectively, some disappearing while oth-
ers remain unchanged, which means that the color relationships of hard copies can be distorted.
Serious paper deterioration is caused by cellulose oxidation that comes through ultraviolet rays 
(like sunlight) and fl uorescent light. Two changes aff ect hard copies: embrittlement and deteriora-
tion. Embrittlement is paper whitening and color fading of certain inks and paper. Deterioration 
is the oxygenation that occurs when paper reacts to the air and turns yellow or brownish, like old 
newspapers you might fi nd in an attic or basement. One other thing with light damage, paper 
continues to degrade after the light source has been removed.
Data-in-Motion and Data-at-Rest
With sensitive data there are two types of data that may need encryption: data-in-motion and 
data-at-rest. Data-in-motion is data that is in transit between two points or data in transmission. 
Data-in-motion comprises data moving over LANs, WANs, the Internet, etc. Data-in-motion can 
also be in motion when stored on removable media and being transported to another location. 
Data-at-rest is the data at the endpoints of transmission. Th is can be data stored in applications, 
databases, fi les, etc. One thing to remember is that the encryption of data-in-motion does not 
necessarily protect data-at-rest.
Where and when should sensitive data be encrypted? Possibly, all the time, depending upon 
the regulations and enterprise standards. When the decision to encrypt is made, the data owner 
and system owner need to decide where and how to implement encryption. While the author can-
not recommend specifi c encryption software or methods, the enterprise, data owner, and  system 
owner should document business requirements for encryption. Some of the business requirements 
should consider the following:

452 ‚óæ Information Security Management Handbook
Risks of sensitive data disclosure
 
‚óæ
Amount of sensitive data
 
‚óæ
Frequency of sensitive data changes
 
‚óæ
Cost of the encryption and storage solution
 
‚óæ
Burden of the maintenance of the encryption and storage on staff 
 
‚óæ
Destruction of Sensitive Data
When sensitive data has come to the end of its useful life, is in surplus, or needs to be destroyed 
per a records retention standard, methods and processes need to be in place. If the sensitive data is 
stored on media, then methods and processes need to be in place for media destruction or reuse.
Th e most common methods of destroying paper media are individual shredders, shred bins, 
and confi dential destruction bins. Many times, individual departments will purchase shredders 
because they work with sensitive data on a daily basis. Should an enterprise determine the need 
for multiple shred and confi dential destruction bins with contracted services, the enterprise needs 
to understand the use of the bins and contracted third parties need to meet the enterprise needs. 
If an enterprise contracts services for destruction of paper contained in bins, on at least an annual 
basis the enterprise should test or follow the contracted services to ensure the paper is stored and 
destroyed per contact requirements.
Many times, certain departments will store sensitive data on removable media. Removable media 
is one of the hardest things to control within an enterprise, because it can accidently disappear and 
never be found or it can walk off  enterprise premises and be used for another enterprise‚Äôs competi-
tive advantage. Th ings like a USB drive can be reused many times, therefore, an enterprise needs 
to have standards on how to reuse removable media. Depending on the removable media, physical 
destruction can include crushing, shredding, incinerating, or otherwise rendering the physical media 
unusable. If the media is to be reused, then processes need to be in place to eliminate the original data 
from the removable media, such as low-level formatting or completely overwriting the data.
Current Monitoring Practices
In a typical business, three groups monitor the environment: system and network administrators, 
security personnel, and facility maintenance employees. Network administrators often rely on a 
single thermometer and subjective notions about ‚Äúcomfort‚Äù to control the temperature of server 
rooms and data centers. In addition, security personnel and facility maintenance departments 
monitor areas outside of the server rooms and also check the environmental controls within the 
data center on a ‚Äúregular basis.‚Äù Th is ‚Äúregular basis‚Äù should be at least daily, yet, many times, is 
only quarterly or when there is a reported problem. Th ese three groups usually attempt to coor-
dinate their eff orts, but each maintain separate systems, practices, and habits. Ultimately, system 
and network administrators are primarily responsible for protecting hardware. Th is approach has 
the following weaknesses:
Staff  are not trained to recognize all threats‚ÄîDamage caused by the environment can be 
 
‚óæ
subtle or attributed to other causes. Accelerated equipment aging due to heat or condensa-
tion occurs over years and is often written off  as a natural process (i.e., ‚Äúequipment just wears 
out‚Äù). Condensation, rust, and heat damage is usually hidden inside machines, out of sight.

Protection of Sensitive Data ‚óæ 453
Nonstandard (inconsistent) processes for all staff ‚ÄîTh e room thermometers are checked 
 
‚óæ
only when the environment feels too hot or cold. Unfortunately, the sense of a ‚Äúcomfortable‚Äù 
temperature and humidity level varies from person to person.
No 7 √ó 24 √ó 365 monitoring activities‚ÄîEnvironmental threats can occur 24 h a day, every 
 
‚óæ
day of the year. Staff  are not always in the data center, especially on nights and weekends. 
Depending on staffi  ng levels and schedules, server room environments can be unmonitored 
up to 65% of the time during an average week.
It is not my job‚ÄîAnother gap can occur because of shared responsibilities. Facilities might 
 
‚óæ
be monitoring for water leaks and fl ooding, but they rely on system and network adminis-
trators and security personnel to review every time they enter the data center. Frequently, 
one person will not look, because they think someone else is doing it. Or someone will see 
something and report it to the wrong person. Th e vulnerabilities develop and potential prob-
lems are never investigated until it is too late.
No automated environmental tracking‚ÄîTemperature and humidity levels constantly 
 
‚óæ
change. Without a condition logging, an administrator cannot identify changes through 
trending metrics. Th erefore, these problems continue for days or months, while time and 
money is wasted investigating false causes and solutions.
Staff  have so many daily duties, they only focus on catastrophes, not daily problems‚Äî
 
‚óæ
Enterprises want to avoid catastrophes, but they do little to protect from threats that slowly 
damage hardware or promote preventative maintenance, such as detecting gradual tempera-
ture increases that indicate a need to clean fans or air fi lters.
An eff ective server environment monitoring system addresses the weaknesses in the current prac-
tice of having personnel monitor the environment. Enterprises need a combination of manual and 
automated monitoring to protect all sensitive data.
Conclusion
Any changes to environmental conditions anywhere sensitive data is stored can impact its future 
use and cause potential damage to that sensitive data recovery. Environmental monitoring includes 
temperature, lighting, humidity, airfl ow, and cleanliness. To start, a sensitive data policy should 
be created that is similar to Exhibit 28.1. Regular controls for environmental conditions include
Changing fi lters
 
‚óæ
HVAC maintenance
 
‚óæ
UPS maintenance and testing
 
‚óæ
Maintenance and testing of environmental controls
 
‚óæ
Proper and thorough cleaning
 
‚óæ
Manual and automated environmental monitoring of sensitive data can provide the following benefi ts:
Control equipment maintenance costs: In a stable environment, equipment lasts longer, and 
 
‚óæ
less equipment is damaged and needs to be replaced. Sometimes, the savings from not hav-
ing to replace equipment can pay for the cost of the monitoring system.
Longer lead time to fi x a small problem: Early warnings permit staff  to respond to an issue 
 
‚óæ
before it becomes a disaster.

454 ‚óæ Information Security Management Handbook
Reduced production downtime: Hardware used in good consistent environmental condi-
 
‚óæ
tions operates more effi  ciently and eff ectively, reducing the number of outages.
Environmental data logs for trending analysis: Reporting and monitoring the environmental 
 
‚óæ
log data ensures stable conditions and also makes available more data when an investigation 
is required.
Exhibit 28.1 Sensitive Data Policy
Title
Sensitive Data Policy
Part Number:
PL00550
Revision:
1.0
Effective:
20050930
Owner:
CSO
Last Review:
20070501
Th is electronic document supersedes all previous electronic and printed documents or oral statements 
regarding this policy.
MYC Policies are subject to change at the sole discretion of MYC management.
Scope
Th is policy applies to any data that has been classifi ed by MyCompany (MYC) as sensitive data 
and is stored on any media or medium.
Purpose
Th e purpose of this policy is to provide behavioral guidance to MYC staff  or any party who is 
contractually bound to handle sensitive data produced by MYC, who produce, or have access to 
MYC sensitive data.
Defi nitions
See the information assurance glossary.
Responsibility
Contracted parties: Must be provided with suffi  cient training and supporting reference 
 
‚óæ
materials to allow them to properly protect and otherwise manage MYC information.
Department heads: Will be responsible for authorizing access to sensitive data and will per-
 
‚óæ
form regular access reviews against MYC sensitive data.
Information security: Will perform regular risk and compliance reviews against MYC infor-
 
‚óæ
mation and will coordinate any information incidents.
Information technology: Will maintain the technology required for information assurance.
 
‚óæ
Management: Must make sure that information is protected in a manner that is at least as 
 
‚óæ
secure as other organizations in the same industry handling the same type of information 
and as required by law.
Staff : Must be provided with suffi  cient training and supporting reference materials to allow 
 
‚óæ
them to properly protect and otherwise manage MYC information.
Policy
For MYC departments, department heads are responsible for implementing additional precau-
tions to be used by any individuals who have access to sensitive data. Parties that are contractually 
bound will be responsible for implementing additional precautions to be used by their staff . For 
contractually bound parties, additional precautions include

Protection of Sensitive Data ‚óæ 455
Limited access (on-site): All areas that contain sensitive data should not be accessible to all 
 
‚óæ
staff . All areas that contain sensitive data must not provide unsupervised access to third par-
ties. Department heads or their designee will work with information and physical security to 
control access to areas containing sensitive data. Areas that cannot be locked cannot be used 
to store sensitive data. Department heads or their designee will identify individuals who 
have a need to access these areas to perform their job function, and will communicate the 
names of these individuals and their required access to Information and Physical Security. 
When leaving their area containing sensitive data, staff , to the best of their ability, must 
properly put away and secure sensitive data.
Limited access (remote): Non-MYC spaces used by contractually bound third parties should 
 
‚óæ
only be accessible by individuals the third party has approved to access the sensitive data. All 
areas that contain sensitive area must not provide unsupervised access to the third parties. 
Areas that cannot be locked cannot be used to store sensitive data. When leaving the area in 
which sensitive data is stored, staff , to the best of their ability, must properly put away and 
secure sensitive data.
Maintenance and cleaning staff : For departments that contain sensitive data, the depart-
 
‚óæ
ment head or their designee will determine if it is practical to request that the cleaning staff  
perform their duties during normal business hours, while the area is staff ed. If none of the 
department employees are present, the cleaning staff  will not clean that area that contains 
sensitive data at that time. If a department has stored sensitive data but receives cleaning 
services after hours, then the staff  shall, to the best of their ability, properly put away and 
secure sensitive data before ending their workday. Cleaning staff  will not have master keys 
for any areas that they clean during normal business hours.
Copiers, printers, and fax machines: Department heads or their designee will work with infor-
 ‚óæ
mation technology, and information and physical security to ensure that all printers and fax 
machines that output sensitive data will be in a limited access area. In areas where this is not pos-
sible, staff , to the best of their ability, will not leave printed or faxed sensitive data unattended.
Shredding/confi dential containers: For departments that handle sensitive data, the depart-
 
‚óæ
ment head or their designee will work with Procurement to ensure that the department has 
access to a secured repository in which they can deposit sensitive data to be shredded.
Questions about this policy
If you have questions about this policy, please contact the Information or Physical Security teams.
Violations
Unauthorized access, disclosure, duplication, modifi cation, diversion, destruction, loss, misuse, or 
theft of MYC information by staff , willingly and deliberately, may result in the loss of access to 
computer and/or network resources and may include termination and legal prosecution. Disciplinary 
measures are on a case-by-case basis.
___________________________
/Name/, Chief Security Offi  cer
About the Author
Sandy Bacik, CISSP, ISSMP, CISM, CHS-III, has over 22 years experience in information 
 security and various information technology positions.


457
29
Chapter 
Water Leakage and Flooding
Sandy Bacik
Mother Nature is always a force to be reckoned with.
 
1. December 26, 2004, an undersea Indian Ocean earthquake and the following tsunami 
(‚ÄúAsian Tsunami‚Äù or ‚ÄúBoxing Day Tsunami‚Äù) off  the west coast of Sumatra, Indonesia, 
killed more than 225,000 people in 11 countries, and inundated coastal communities with 
waves up to 30 m (100 ft) high. Indonesia, Sri Lanka, India, and Th ailand were hardest hit. 
Th e reconstruction and recovery would probably take between 5 and 10 years. Industrial 
fi shery is the major economic activity that provides direct employment to about 250,000 
people. Th e fi shery industry is a dynamic export-oriented sector, generating substantial for-
eign exchange earnings. Preliminary estimates indicate that 66% of the fi shing fl eet and 
industrial infrastructure in coastal regions have been destroyed by the wave surges, which 
will have adverse economic eff ects both at local and national levels. How many enterprise 
infrastructures were never restored or totally recovered from fl ooding?
 
2. August 25‚Äì27, 2006, Hurricane Katrina hits Florida, the Gulf of Mexico, and Louisiana, 
and then returns to make a second landfall with more disastrous results and more repercus-
sions in the months afterward. Whole infrastructures were destroyed. How many businesses 
never recovered and how many enterprises recovered and considered the recovery from 
fl ooding successful?
 
3. October 2007, Tropical Storm Noel turns into a hurricane and Grijalva River bursts 
its banks and fl oods the state capital of Villahermosa with up to 8 ft of muddy and foul 
Contents
Types of Water Damage ............................................................................................................459
What Can Water Damage Do ..................................................................................................459
Humidity and Condensation ................................................................................................... 460
Current Monitoring Practices .................................................................................................. 460
Conclusion .............................................................................................................................. 460
About the Author .....................................................................................................................461

458 ‚óæ Information Security Management Handbook
water. To make matters worse, the federal electric utility decided to release water from 
the lake behind the Penitas hydroelectric dam. At the height of the fl ooding four-fi fths of 
Tabasco was submerged. Villahermosa was fl ooded for more than a week. With natural 
fl ooding, enterprises have time to activate their business continuity and disaster recovery 
plans. In this situation, the Red Cross, the Mexican armed forces, and private sector 
companies set up basic communication and other electronic systems to ensure com-
munications were active. Companies still had their IT infrastructure destroyed by the 
fl oodwaters and were able to eventually recover after power and communications systems 
returned to normal.
Yet, man-made water disasters can be just as deadly for a business.
 
1. July 15, 2002, Walnut Creek, California, fi re broke out in the corporate headquar-
ters of WildPackets. Monthly backup tapes were stored off site, but June‚Äôs tapes were still 
onsite. Despite a fi reproof safe, the tapes were not usable due to smoke and water dam-
age. WildPackets had to use May‚Äôs monthly backups to recover their systems. A week later 
WildPackets was in a new building and resuming shipments.
 
2. July 9, 2008, Lucas Oil Stadium had 20 roof drains break sending much water to the lower 
levels of the building just weeks before the grand opening. While damage was confi ned 
to less than 1% of the building, the lower building levels contained electrical boxes, tele-
phone closets, and the data center. Recovery was completed and the opening on was held on 
August 16th as scheduled.
Water damage is one of the most problematic and commonly experienced forms of disaster 
damage. Causes of water damage include natural fl ooding, burst or leaky pipes, fi re hoses, 
and humidity. To better protect an enterprise from water damage, it is necessary to know 
what it is and how it can occur. Th e recovery from water damage is varied depending upon 
how soon the incident was discovered. Th e main thing to remember with water damage is 
to act fast before it becomes a disaster. For example, your enterprise decided to expand (or 
contract), so larger (or smaller) areas are required to store electronic equipment and hard copy 
data. Let us say a new internal location is selected for the new server room. Now imagine, 
you fi nd a large enough area with good ventilation; there exists dry pipes for extinguishing 
fi res and the air-conditioning units do not have their drip pipes running over the network 
equipment. Th e fl oor is on a raised platform. Good considerations for an initial design. And 
the move of equipment starts. Oops! Th e pipes from the restrooms on the fl oors above run 
directly over the new server room and one of the pipes bursts due to a plumbing mistake. 
What type of activities or safety measures should be put in place? Was there anything in the 
design that would have prevented any water-carrying pipes from running directly over the new 
server room? IT and management need to understand and take into consideration water leak-
age and fl ooding because they do not need to live in a fl ood plan area to have a fl ood or water 
leak into their equipment rooms. Water leakage and fl ooding need to be addressed within an 
environmental safety policy and program. Water leaks and fl ooding are not an everyday occur-
rence and we know that water and electricity do not mix and can cause signifi cant damages. 
We will discuss issues and possible resolutions for locating server rooms and critical computer 
equipment away from a water source or transport pipes and other water type issues within the 
environment.

Water Leakage and Flooding ‚óæ 459
Types of Water Damage
In most environments, there are three classifi cations of water damage:
Clean water: Th is type of water damage does not pose any health risk to humans and is really 
 
‚óæ
just annoying.
Grey water: Th is type of water damage could eventually pose a health risk to humans and 
 
‚óæ
contains degrees of chemical, biological, or physical contaminants.
Black water: Th is type of water damage (such as sewage) contains highly unsanitary agents 
 
‚óæ
and can impact human health.
Reviewing the pipe and hose systems within the enterprise can assist in limiting the risk of water 
damage.
What Can Water Damage Do
Many times, network and system administrators focus on protecting network devices from 
logical security attacks and connectivity failures, often inadvertently missing the ever-present 
danger of environmental threats. Th ese threats include temperature, humidity, and water leaks. 
Environmental issues can damage equipment, slow performance, and force hardware to shut 
down. Th e costs of water and environmental threats can include the following:
Loss of revenues due to unavailable equipment
 
‚óæ
Replacement of damaged equipment, including additional administrative time to investi-
 
‚óæ
gate and fi x problems
Lower productivity due to downtime
 
‚óæ
When staff  observe water leakage and fl ooding, staff  should look in all directions within the data 
center (above, below, inside, and just outside) to determine if there is any potential source for the 
damage, instead of just what is in front of them. Some of the items we must look at are as follows:
HVAC units
 
‚óæ
Outside water sources (ponds, lakes, streams, water mains)
 
‚óæ
Pipe entrance into the room or building and transport pipe locations
 
‚óæ
Restrooms or cafeterias/cafes
 
‚óæ
Fire hydrants
 
‚óæ
Server room wet pipes
 
‚óæ
Sometimes, staff  can create water type situations in the data center by
Adjusting the heat or air conditioning while working in the server room and forgetting to 
 
‚óæ
reset it when they leave, making the HVAC and other ventilation equipment work harder.
Placing boxes in front of vents ‚Äútemporarily‚Äù and forgetting to move them, which blocks 
 
‚óæ
airfl ow, again making equipment work harder.
Similarly, cleaning crews sometimes close or open doors that should be left open or closed 
 
‚óæ
for ventilation, making the HVAC and ventilation equipment work harder.

460 ‚óæ Information Security Management Handbook
Proper planning moves equipment away from condensation, drip, and water pipes that might burst 
or leak. Blocked ventilation systems can cause condensation if they are overworked and the moist air 
is not removed quickly. If ventilation vents are located above or behind machines, condensation can 
form small water pools that no one may notice until it is too late. Small amounts of water near air 
intakes raise humidity levels and can fi ll servers with moisture. Depending on the angle of the pipe, 
water can travel long distances behind walls or over rooms for a long time before it is noticed and 
any action is taken. Cables and wires within the data center are often located beneath fl oor panels. 
Th e cords are generally kept safe from being unplugged, but monitoring the physical status is dif-
fi cult. Water leaks may have happened for a long time before anyone lifts a fl oor tile. Besides possible 
power outages, this situation can also break down insulation and cause performance degradation.
Humidity and Condensation
Th e main environmental threats to network equipment are temperature and humidity. Th e generally 
accepted ideal temperature is between 68¬∞F and 74¬∞F (20¬∞C‚Äì24¬∞C) for storage of electrical equipment 
and paper. When the temperature is normal, the relative humidity (i.e., the amount of water in the air) 
should be between 40% and 50%. High humidity levels can produce condensation problems within 
a data center. Condensation occurs when humidity levels are too high or when there is a rapid drop 
in temperature. Besides the potential of water running along pipes, condensation inside equipment 
causes rust, short circuits, or deposits of dirt and minerals that ruin equipment. Th ere exist moisture-
absorbing circuit boards that can expand and contract with changes in relative humidity levels, but 
constant expansion and contraction can break microelectronic circuits and edge connectors.
With high humidity levels (above 60%) and persistent high temperatures, the conditions are 
ideal for fungus growth. Besides aff ecting human health, fungus can also clog the machine‚Äôs 
airfl ow and promote heat retention and condensation. Fungus retains moisture and promotes cor-
rosion, which can damage circuits and motherboards.
To assist with humidity issues, there is humidity-measuring and -controlling equipment within 
the data centers, which can be adjusted according to one‚Äôs requirements. Th is can be the humid-
ity monitoring or cooling equipment, which has an integrated mechanism for humidity control. 
Current trends recommend having separate equipment for humidity control rather than letting air 
conditioners handle it on their own.
Current Monitoring Practices
Like the current monitoring practices described in Chapter 28, water leaks and fl ooding need 
to be included in that monitoring with the humidity and water level sensors. Staff  need to walk 
the entire data center on a daily basis, including lifting fl oor and ceiling tiles to check if there is 
any additional dampness or clogging. Enterprises need a combination of manual and automated 
monitoring to protect all sensitive data.
Conclusion
In conclusion, the most common environmental monitoring practices include having staff  just 
observe and report problems. Th is practice has some inherent weaknesses, such as changing con-
ditions, not recognizing threats, not knowing how to track environmental threats, and focusing 

Water Leakage and Flooding ‚óæ 461
on major incidents and not daily threats that may damage equipment. Automated environmental 
monitoring systems help prevent the damages caused by environmental threats, such as tempera-
ture, humidity, and water leakage, that could destroy network components in a data center or 
network equipment room. Automated systems can send alerts through fl ashing lights, buzzers, 
and messages via SNMP traps, e-mail, and the system‚Äôs Web-based administrative interface. Is 
there any way to stop all water-based risk within a server room environment? Probably not, but 
the enterprise needs to mitigate the water-based risk and have compensating controls in place just 
in case. Like the results of a combination of automated and manual monitoring documented in 
Chapter 28, water leaks and fl ooding need to be included in that monitoring with the humidity 
and water level sensors.
About the Author
Sandy Bacik, CISSP, ISSMP, CISM, CHS-III, has over 22 years experience in information security 
and various information technology positions.


463
30
Chapter 
Site Selection and Facility 
Design Considerations
Sandy Bacik
Information technology (IT) has been complaining to facilities about additional power and air 
requirements needed for the server room within enterprise headquarters. IT has documented the 
current and future environmental requirements for production equipment. Th e business park, where 
headquarters is located, is having new construction being performed for other business expansions. 
Th e power company while digging a new trench cuts the power to the whole business park. No 
problem, the generator kicks on and the server room equipment is continuing merrily on the pro-
duction processing schedule. Th e power company gets the power back on and the generator senses 
the power and switches back to regular power. Oops! Th e building uninterruptible power supply 
(UPS) breaker trips and the server room does not have an additional UPS in place and does not have 
a separate power source. All 200-plus network devices and servers hard crash, because the building 
breaker tripped and the generator did not come back on because it sensed the line power. IT needs 
72 continuous hours to recover and bring all systems back online. Could this have been prevented? 
Were there simple items that should and could have been in place for business continuity and disaster 
recovery? While the answer is yes, there may have been communication problems between IT and 
facilities, or there might have been budget issues as to who should have paid for the upgrades.
Contents
Business Requirements and Th reats ......................................................................................... 464
Build or Select an Existing Facility ...........................................................................................465
Physical and Material Supplies ................................................................................................ 466
Facility ................................................................................................................................ 466
Environmental .................................................................................................................... 468
Eff ective Physical Security ................................................................................................... 469
TIA-942 ............................................................................................................................. 469
Conclusion ...............................................................................................................................470
About the Author .....................................................................................................................470

464 ‚óæ Information Security Management Handbook
Th is chapter will walk through checklists and discuss what-ifs on selecting a site and designing 
a facility to store the enterprise production and test equipment. It reviews what the ideal environ-
mental conditions should be to ensure IT obtains the most productivity from equipment. At some 
time or the other, IT will need to discuss the movement of corporate equipment to a larger or 
smaller location due to budget, size, or support. Facilities may be the group who signs the contract 
for the location, but IT must be involved with documenting the business requirements for keeping 
the network and services up and running. Every enterprise, at some point in its history or in the 
future, will consider building its own server room or data center, or outsourcing the facility to a 
supplier. Th e enterprise might be outgrowing its existing internal facility, it may be reviewing to 
outsource the hosting, or it may be compressing the facility. When an enterprise starts looking for 
a site or a data center‚Äìhosting facility, it must look at where the site is going to be located and the 
facility design. Th is chapter also includes questions that need to be considered as the enterprise 
develops business requirements and items that need to be included when performing a facility 
audit, as well as issues that need to be taken into account when reviewing a site and facility to host 
its assets.
Business Requirements and Threats
In recent years, trends have existed for data center expansion or consolidation and for going 
green. For many enterprises it does not make sense to have the enterprise headquarters in one 
geographic location and the data center in another. Th is is mostly because of the cost and logis-
tics of doing business in another location, where the infrastructure and personnel costs might 
just exceed the value of the data center site location. If the business has a continuity and disaster 
recovery plan, then it may outsource to a co-location facility in another part of the country. 
Centralized data centers have assisted enterprises in achieving cost savings and eff ective and 
effi  cient production activities. When an enterprise determines that a new data center or server 
room location is required, it needs to look at the cost savings and business requirements as well 
as the vulnerabilities of that location, such as
Population density and the amenities in the location
 
‚óæ
Environmental concerns
 
‚óæ
Proximity to possible hazardous events‚Äînatural or man-made
 
‚óæ
Existence (or not) of early-warning systems and communication methods
 
‚óæ
Readiness of emergency preparedness of the location
 
‚óæ
Construction styles, building codes, and regulations of the location
 
‚óæ
Cultural and support factors
 
‚óæ
Business and government climate, volatility, and insurance
 
‚óæ
Real estate and fi nancial environment
 
‚óæ
Enterprise threats come in all shapes, sizes, durations, and forewarnings. If a data center becomes 
unavailable, direct and indirect losses can occur. Direct losses translate into additional costs and 
lost revenue. Indirect losses can include legal, contractual, regulatory, and customer obligations 
and costs. Th reats can be regional, national, or international. When looking at a facility, most 
businesses think only about regional threats. Th reats can be natural, unintentional, or intentional. 
Again, most businesses look at natural threats and potentials for an intentional threat. As the 

Site Selection and Facility Design Considerations ‚óæ 465
enterprise builds or expands a facility, it needs to think about implementing safeguards to reduce 
the likelihood or frequency of risks and threats to limit the damage level that could be sustained 
and survived. A pre-assessment and strong design against threats, risks, and vulnerabilities will be 
more cost eff ective than correcting any damage. When performing a review or building business 
requirements, the enterprise should focus on the inherent risks and threats to equipment and data. 
At a high level, it must determine the requirements for
 
1. Physical and material supplies‚Äînumber of components, terminals, desks, chairs, containers, 
tapes, disks, paper supplies, waste, cabling, wiring, cabinets, and network equipment
 
2. Facility location‚Äîbuilding, room(s), work space, storage area
 
3. Environmental‚Äîair conditioning and fl ow, fi re suppression, electricity, communication, 
water, power, backup power, and lighting
For these three high-level requirements, the enterprise must review and determine which of the 
following six security essentials they have to accomplish to meet the requirements for site selection 
and facility design:
 
1. Demarcate by established defi ned borders to create a defendable space.
 
2. Prohibit entry and exit through a limited number of portals.
 
3. Deter threatening entities by ensuring the building appears strong and solid and deny easy 
access to keys, information, badges, windows, and doors.
 
4. Delay unauthorized entry by creating sound alarms.
 
5. Monitor detection systems in case an intruder gets past the physical barriers.
 
6. Communicate alarms immediately to an entity that is prepared to respond and react to an 
intrusion.
When an enterprise looks at a specifi c geographical location, it needs to look at the area and see 
that the considerations mentioned below meet the enterprise policy and culture, because this is 
a crucial decision that will impact effi  ciency, reliability, security, and service levels. Some of the 
items to consider as part of business requirements are as follows:
Human resources and staff -related issues (culture and quality of life)
 
‚óæ
Network connectivity and redundancy
 
‚óæ
Utilities (availability, emergency shutoff s)
 
‚óæ
Security and safety of equipment, data, and staff 
 
‚óæ
Business service availability in the community
 
‚óæ
Real estate and business market conditions
 
‚óæ
Real estate and building acquisition contractual requirements
 
‚óæ
Build or Select an Existing Facility
Whether an enterprise is expanding/consolidating, the enterprise also needs to determine if it will 
be using the existing facility, relocating to another facility, or building its own facility. Later in 
the chapter, issues such as TIA-942, facility telecommunication, and cabling requirements that 
an enterprise must be aware of are discussed. Th is site must fi t into current requirements and any 

466 ‚óæ Information Security Management Handbook
future expansion/consolidation of the enterprise. Check with your current carriers for assistance 
to determine if they will continue to fi t your needs. Before the enterprise decides to use an existing 
facility or build a new facility, it needs to document the following related to existing equipment:
How much power is being used by the facility and equipment? http://www.formalogy.com/
 
‚óæ
iphone.html
How much equipment is being used or will be used for foot printing the space 
 
‚óæ
requirements?
How much cooling and airfl ow equipment will be needed to ensure a controlled 
 
‚óæ
environment?
How much bandwidth is being used and will be used for processing?
 
‚óæ
Are there any special structural requirements for storing the equipment?
 
‚óæ
How long would we need to and could go on generator power, similar to a business impact 
 
‚óæ
analysis?
Is one of the company‚Äôs mission statements to go toward a green enterprise environment?
 
‚óæ
Executive management should know the direction the business will be going toward in the future 
and they can provide hints toward expansion or consolidation. Can I recommend the use of an 
existing facility or building a new facility? No, because this decision will depend on the business 
requirements and the mission and vision of the enterprise.
Physical and Material Supplies
IT staff  must often remain in the data center for hours at a stretch performing work, and need an 
area to work and access the network. What is the function of the facility? Is it just going to host 
networking and production computer equipment? What are the types and number of assets that 
will be hosted in a data center facility? Assets can range from fi rewalls and other network devices 
to production servers containing the enterprise‚Äôs most critical information. First and foremost, the 
enterprise must develop a list of assets that are going to be hosted and the environmental require-
ments of those assets. If staff  are going to reside, even part-time, within the facility, work areas 
and supplies will need to be included in the facility requirements. Depending on the size of the 
facility, requirements for extra cabling, wiring, and hard drive space may be needed. Determining 
the square footage required for equipment, cabinets, and supplies (‚Äúfoot printing‚Äù) will assist the 
enterprise in selecting the size of the facility.
Facility
Before getting into more detail, a physical site needs to be selected to host the equipment. When 
an enterprise evaluates a site or decides to use an internal facility, the following items need to be 
taken into consideration to limit the risk and threats to enterprise assets.
 
1. General construction:
 
a. Can the structure withstand the regional disasters? What are the natural regional disas-
ters? What are some regular man-made regional disasters?
 
b. How many controlled and uncontrolled ingresses and egresses exist?
 
c. Is the structure new or has it been retrofi tted?

Site Selection and Facility Design Considerations ‚óæ 467
 
d. Does the construction company have an excellent reputation for service?
 
e. Is there a gated entrance and how is it controlled and monitored?
 
f. Can the exterior and interior doors be easily compromised? What are the alerts when 
this happens? What about the door frames?
 
g. What utility grid does the site reside in? Are there redundant utilities for the site, such as 
power generators?
 
h. How soundproof is the facility?
 
i. Is there much glass on the exterior? Are there many windows in the facility for unauthor-
ized access or for viewing activity within the facility?
 
j. Does the facility contain raised fl oors? What about a manhole or underground opening 
to access the facility?
 
k. Does the facility contain signage: warning signs, exits, emergency lights?
 
l. What are the hazards of the facility?
 
m. What are the intrusion alerts and how can they be set off ?
 
n. When the fi re alarm goes off , do all the doors unlock within the building, including the 
data center or the enterprise cage? If so, what is the process for keeping the data center 
secure during a fi re alarm incident?
 
o. Remember: Just because a building is built to code does not mean it is a good design for 
the enterprise assets.
 
2. Site vulnerabilities:
 
a. Is the site located in a high-crime area? Will the site feel safe when leaving after-
hours?
 
b. Is the area outside well lit or hazardous when leaving after sunset? Does the outside 
lighting illuminate critical areas (fi re escapes, ground-level windows and doors, alley-
ways)? Is there auxiliary power for the external lighting? Can the lighting be easily 
compromised?
 
c. Is the facility internally well lit during the day, as well as the night? Is there auxiliary 
power for the internal lighting?
 
d. How close are the emergency services (police, fi re)?
 
e. Does the site attract unwanted attention?
 
f. Is the site marked as a hosting facility for random violence?
 
g. If the enterprise places their assets on the site, who can access the facility and can some-
one see and monitor activities of staff ?
 
h. Can someone drive a truck through the wall of the site and damage internal assets?
 
3. Site protections:
 
a. Where are the security cameras located and how many are there? What are the closed-
circuit television (CCTV) aspects of the facility? How are they being monitored?
 
b. Are the security cameras tape or digital? What is the retention for the video footage? 
Does it meet the enterprise record‚Äôs retention requirements?
 
c. Are the rooms locked independently?
 
d. Do guards regularly walk the facility for unsecured areas?
 
e. Are there environmental controls: smoke detectors, fi re detectors, fi re suppression, and 
heat and humidity sensors?
 
f. Will tampering with any of the environmental controls create alerts to appropriate personnel?
 
g. What are the access controls? Biometrics? Mantraps? Proximity cards?
 
h. If badges are used, is it easy to distinguish a visitor from a staff  member?
 
i. How are visitors and regular staff  logged for entry and exit to the facility?

468 ‚óæ Information Security Management Handbook
 
j. How is the enterprise alerted when there is a facility compromise?
 
k. Can the site perform regular maintenance without aff ecting the enterprise operations?
 
l. Does the facility have layered security controls: facility site, facility shell, data cages/rooms, 
work areas?
 
m. When a facility staff  member is terminated or turns in their resignation, what controls 
are in place to ensure access is removed and passwords have been modifi ed?
 
n. What is the screening method for facility staff  members? Are the guards and staff  rotated 
on a regular basis to ensure there are fresh eyes monitoring the environment?
 
o. Are facility staff  members provided special training for fi rearms, CPR, fi rst aid, fi re 
safety, etc.?
 
4. Adjacent or nearby buildings:
 
a. How well maintained are the nearby buildings?
 
b. Can someone compromise an adjacent building and acquire access to the enterprise 
facility?
If a server room is going to be used instead of a hosting facility, the following requirements will 
need to be considered:
 
1. Does the room have full-height fi reproof walls to close access through false ceiling tiles?
 
2. Does the server room have separate environmental controls from the enterprise building? 
Power, air, fi re suppression?
 
3. Who maintains and controls the access to the room?
 
4. When the fi re alarm goes off , do all the doors unlock within the building, including the 
data center? If so, what is the process for keeping the data center secure during a fi re alarm 
incident?
Environmental
After determining the physical and material supplies that will be housed in the facility, a list of the 
environmental requirements is needed:
 
1. Does the facility power meet the enterprise current and future equipment power needs?
 
2. Is the airfl ow and air conditioning suffi  cient for the current and future layout of the 
equipment?
 
3. Is the humidity for the current and future environment regulated and monitored?
 
4. Is the power supply suffi  cient for the current and future layout of the equipment?
 
5. Is the fi re suppression spread throughout the facility and is it adequate to cover the enter-
prise equipment? Or are there automatic sprinklers for fi re suppression and no protection for 
equipment when the sprinklers are activated?
 
6. What type of communication lines are in the place for the network devices and servers?
 
7. What about voice communications for staff  who may be working at the facility?
 
8. What are the personal facilities and rest areas for staff  working at the facility?
 
9. Is there backup power and environmental controls? Are they tested on a regular basis?
 10. How does the facility respond to power spikes or brownouts?
 11. Are there any static controls for the equipment?

Site Selection and Facility Design Considerations ‚óæ 469
Effective Physical Security
Th is section is more of a listing of things to consider and question on the facility design when 
building a facility/data center or server, and even when using a hosting provider for the enterprise 
data center.
Whether the enterprise has regulatory requirements for auditing computing facilities or not, 
it should implement regular processes to perform its own facility auditing. Some facility auditing 
requirements are as follows:
 
1. If the facility is hosted, can the enterprise obtain an annual risk assessment or SAS70 Type 
II audit from the hosting facility?
 
2. Obtain and review a listing of authorized staff  to the facility. Is it reviewed and approved on 
a regular basis? Who performs the authorization? Determine who is responsible for the enter-
prise staff  access control and interview them to ensure they understand their responsibility.
 
3. Obtain and review a list of application software, operating system version, and hardware 
with their function and owner that reside in the facility. Assess the equipment maintenance, 
change management, and confi guration management processes for the facility. Th is may 
include interviews with the enterprise staff  responsible for the equipment.
 
4. Walk through the physical safeguards with the enterprise staff  for accessing and maintain-
ing the equipment in the facility. Th is would also include device and server console access 
and controls.
 
5. Obtain and review the standard operating procedures for the facility as performed by enter-
prise staff .
 
6. Obtain and review the business continuity and disaster recovery procedures for the enter-
prise staff  operating the facility.
 
7. Review the facility installation and maintenance of the environmental controls.
 
8. Obtain and review the enterprise procedures for incident handling at the facility; also include 
the hosted facilities incident response procedures.
 
9. Obtain procedures for routine testing of environmental facility controls.
 10. Walk through the facility; is there ease of, but secure, access to the facility? Can enterprise 
staff  perform their functions within the facility easily and securely?
 11. Remember the following for a basic checklist for a facility/data center/server room:
 
a. Dedicated and secured space
 
b. Reliable environment conditions
 
c. Clean work area
 
d. Limited access
 
e. Active monitoring
 
f. Operational procedures
 
g. Trained staff 
TIA-942
For a generally accepted standard for facilities, please review TIA-942‚Äîa standard developed 
by the Telecommunications Industry Association (TIA) to defi ne guidelines for planning and 
building facilities (http://www.tiaonline.org/standards/). Th e TIA-942 specifi cation references 
data center requirements for applications and procedures such as

470 ‚óæ Information Security Management Handbook
Network architecture
 
‚óæ
Electrical design
 
‚óæ
File storage, backup, and archiving
 
‚óæ
System redundancy
 
‚óæ
Network access control and security
 
‚óæ
Database management
 
‚óæ
Web hosting
 
‚óæ
Application hosting
 
‚óæ
Content distribution
 
‚óæ
Environmental control
 
‚óæ
Protection against physical hazards (fi re, fl ood, windstorm)
 
‚óæ
Power management
 
‚óæ
Th e principal advantages of designing data centers in accordance with TIA-942 include standard 
nomenclature, fail-safe operation, protection against natural or man-made disasters, and long-
term expandability and scalability.
Conclusion
Th ere are several things that need to be considered when selecting a site to support the enterprise. 
Documenting business requirements is a must before selecting a location for the data center. Most 
enterprise strategic plans only look ahead for up to three years, yet when looking for a data center 
site the enterprise needs to see much farther into the future. While the above are a sampling of 
questions to consider, you can also use something like the National Institute of Standards and 
Technology‚Äôs Risk Management Guide for Information Technology Systems (SP 800‚Äì30). Th ese 
documented requirements will then provide the enterprise with a site that will have effi  cient and 
eff ective productivity for a long period of time.
About the Author
Sandy Bacik, CISSP, ISSMP, CISM, CHS-III, has over 22 years experience in information security 
and various information technology positions.

471
31
Chapter 
An Overview of IP-Based 
Video Surveillance
Leo Kahng
Th e information security industry has a long-standing history in Internet protocol (IP)‚Äìbased 
tools, utilities, and communications, but this is not so much the case in the physical security 
realm, which is in the midst of a transition to IP. In 2007, Forrester Research stated in its report 
called ‚ÄúTrends 2007: Physical and Logical Security Convergence‚Äù that ‚Äú‚Ä¶the physical security 
environment has been dominated by analog, stand-alone control systems with limited intercon-
nectivity, digital communications, or integrated management capability.‚Äù Physical security is often 
emphasized as one of the fi rst layers of defense when addressing information systems, but it is a sig-
nifi cant and critical element of security practices that can have a great impact on the networking 
foundation and information security architecture. In fact, it is signifi cant enough that the certifi ed 
information systems security professional (CISSP) examination identifi es physical (environmen-
tal) security as one of its ten common body of knowledge (CBK) domains. Th is is then subdivided 
into layered physical defense and entry points, and site location principles.
Contents
Challenges and Pain Points with Traditional Video Surveillance ...............................................472
IP Video Surveillance Today .....................................................................................................473
Current Best Practices and Network Design Considerations for IP Video Surveillance .............475
Th e Nature of Video Surveillance Traffi  c ..............................................................................475
Performance and Storage Considerations .............................................................................476
Multicast: Just for a Quick Recap ....................................................................................... 477
Quality of Service and Network Design Principles .............................................................. 477
Port-Based Security ..............................................................................................................481
Case Study: Retail ....................................................................................................................481
Summary ................................................................................................................................. 483
About the Author .................................................................................................................... 483

472 ‚óæ Information Security Management Handbook
Among the various physical security methods, such as physical facilities access, access con-
trol, and network admission control, one of the most traditional and widely used elements of 
physical security is that of video surveillance, which, until lately, has been largely based on analog 
technologies and site-based data-handling practices. Th is chapter covers key concepts and design 
considerations regarding video surveillance deployed over IP networks, as it realizes some unique 
benefi ts over previous surveillance systems, as well as new methods of data handling that IP 
transport enables. Leveraging the reach and fl exibility of IP presents a range of advantages and 
enhancements that can increase responsiveness, reduce casualties and loss, and reduce the costs 
associated with the implementation of a robust and resilient surveillance solution.
Challenges and Pain Points with Traditional Video Surveillance
Th ough we will not delve into the very origins of video surveillance, much of what is available today 
evolved from closed-circuit television (CCTV), which can arguably be dated as far back as 1942 when 
Siemens AG used such a system to remotely monitor the launching of V2 rockets to observe launch 
characteristics and any malfunctions or errant behavior. CCTV has since then managed to fi nd its 
way into law enforcement organizations to fi ght crime, followed by a migration into the commercial 
sector, primarily in banks and retail, to monitor facilities access patterns and deter theft. In recent 
times, one of the most demanding applications for video surveillance is in the gaming industry where 
nonstop scrutiny in facilities such as casinos is of the highest priority for gaming houses to protect 
their interests from the less trustworthy. Additionally, in the past few years, a heightened awareness of 
physical security is steadily growing in the state and local government space, and especially in the K-12 
and higher education environment. One such example is that of traffi  c monitoring cameras. However, 
one of the most widely covered tragic incidents of the recent past was the shooting at Virginia Tech, 
which unfortunately exposed a greater need for safety and security awareness in the education sector.
When we examine traditional video surveillance systems, several defi ciencies are exposed that 
drive the need for a highly connected and converged solution that IP can bring forth.
Local access only
 
‚óæ
: Th e majority of existing legacy CCTV surveillance systems are almost 
entirely closed. Th e infrastructure, the video capture, camera management, and storage of 
the video feeds are all conducted locally, on-site. Remote access to these systems does not 
exist and the operation of these systems requires a skilled on-site staff  to man an operations 
center or security offi  ce for as long as the surveillance is necessary. Th e transfer of informa-
tion to and from the site is performed by manual means.
Low level of threat detection
 
‚óæ
: When security personnel have to observe the video streams that 
are being piped to monitors and recording systems, visual fatigue can set in very quickly. 
Studies have shown that up to a 95% decrease in event detection can occur after only 22 min 
of consecutive viewing. When events are missed, it is then up to review teams to evaluate 
hours of recorded material to identify and assess threats, which may have already damaged 
the integrity of a secure facility, or created a loss of assets.
Investigation delays
 
‚óæ
: With surveillance systems that rely on video tape or other removable 
media that is housed on-site, investigations of security incidents can be greatly delayed. 
For instance, the correct media has to be found (locate a tape in archive), then it has to be 
shipped to the reviewing party, then the media has to be reviewed and analyzed. Essentially, 
once the media is in the reviewers‚Äô hands, they have to watch as much video as was recorded 
and rely on their vision and alertness to detect the events that are in question.

An Overview of IP-Based Video Surveillance ‚óæ 473
Number of monitoring locations can be very limited
 
‚óæ
: Traditional surveillance systems require 
dedicated cabling plants. Often, these were expensive coaxial cable (most popular) or fi ber-
optic cable infrastructures. In order to place more cameras in strategically advantageous 
locations, cable needed to be pulled and back-hauled to the central command center for 
video monitoring.
Lacking ease of scalability
 
‚óæ
: In order to add more cameras, more matrix switching is required to 
handle the additional inputs and output streams, which often required a hardware upgrade 
or replacement.
Many other elements of traditional video surveillance can be pointed out, but the ones listed above 
represent some of the more popular pain points or challenges that are faced. One common theme 
that is present in the above-listed challenges is a very high degree of human interaction required 
to operate and manage traditional CCTV solutions.
IP Video Surveillance Today
When we approach video surveillance from an IP standpoint, we immediately begin to incorporate 
features and utilities in the network infrastructure to augment the physical security enhancements 
that video monitoring enables. For instance,
Taking advantage of an existing infrastructure
 
‚óæ
: IP-based physical security leverages the net-
work, which often already exists and provides a physical infrastructure that can be adapted 
and/or easily expanded to present more video camera coverage options.
Leverages network security features
 
‚óæ
: Use the authentication, authorization, and access control 
features native to the network security platforms in place. Monitor rogue device implantation 
in real time and implement policy-based countermeasures.
Enable the use of wired and wireless video surveillance cameras
 
‚óæ
: Anytime, anywhere coverage. 
Allows one to overcome the challenges associated with cabling where it is prohibitively diffi  cult 
or costly to place a surveillance camera.
Digitally record video
 
‚óæ
: Provide immediate access to recorded events and enable real-time 
incident response, investigation, and resolution.
Monitoring and controls can be transferred to any point in the network
 
‚óæ
: No longer does one 
have to be physically colocated with the video surveillance infrastructure to manipulate and 
manage the video system. Provides true remote access to video streams, camera control, 
system management, and recorded video.
Ability to distribute management and recording of video
 ‚óæ
: Eliminates the ‚Äúsilo‚Äù eff ect and prevents 
systems from becoming a single point of failure.
One very innovative and important advent in the evolution of video surveillance is that of video 
content analysis (VCA), which is more commonly referred to today as video analytics. As mentioned 
earlier, in traditional video surveillance, the eff ectiveness of persons tasked with monitoring live 
video feeds, or reviewing recorded video footage, can dramatically decrease in as little as 22 min. 
Video analytics leverages automated algorithms and detection parameters to identify movement, 
anomalies, or behavior patterns within a particular fi eld of view. For example, Figure 31.1 depicts 
an IP surveillance camera tracking movement within the fi eld of view in a parking garage, where 
a person is spending time around a particular vehicle.

474 ‚óæ Information Security Management Handbook
Using video analytics, human intervention can be greatly reduced, thus reducing the number 
of fatigue-related errors while allowing more effi  cient management of the video surveillance system. 
Video analytics enables real-time analysis and detection of security events from many diff erent 
cameras, simultaneously, identifying events as they occur and providing tools to analyze the situation 
and even trigger responses, such as notifi cations, alerts, and alarms. Some of the most common 
algorithms and behavior detection scenarios are
Erratic/suspicious behavior
 
‚óæ
: Spending an unusual amount of time within one area or a repeat-
ing pattern of movement around vehicles in a parking garage.
Congestion detection
 
‚óæ
: Too many people in a particular space.
Motion detection
 
‚óæ
: An object, animal, or person crossing a particular fi eld of view.
Abandoned object detection
 
‚óæ
: Parcels or luggage left unattended at an airport, for example.
Opposing fl ow
 
‚óæ
: Objects or persons moving opposite the normal direction of fl ow‚Äîa car 
going the wrong way down a one-way street.
Shape-based detection
 
‚óæ
: Automobile detection, detection of persons or animals, unusual size 
objects in the fi eld of view.
Missing object detection
 
‚óæ
: Detection of something removed from a scene.
Video tripwire
 
‚óæ
: Alert based on detection of a breach of a defi ned boundary. Th is can be as 
simple as motion detection, or identifying movement or the presence of an entity in just a 
portion of the camera‚Äôs fi eld of view. Also, video tripwires can be utilized to trigger loitering 
alarms where a suspicious person may be lurking by a secure door waiting to enter a facility 
as someone from inside is leaving.
Th e current state of developments in the IP video surveillance realm negate the majority of the 
defi ciencies encountered in traditional video surveillance systems, while reducing the level of 
human intervention and taking advantage of the IP network to provide real-time, anytime, any-
where monitoring for the betterment of physical security.
Figure 31.1 Sample video analytics for motion or loitering.

An Overview of IP-Based Video Surveillance ‚óæ 475
Current Best Practices and Network Design 
Considerations for IP Video Surveillance
Note: In this section, we will examine several elements of consideration for design best practices. 
Th ough this is not a comprehensive overview, the topics of coverage are: the nature of video sur-
veillance traffi  c, performance and storage considerations, multicast, quality of service and network 
design principles, and port-based security.
When considering the deployment of an IP-based video surveillance solution, many factors 
come into the picture that are not relevant to traditional surveillance topologies. One of the most 
obvious shifts in thought processes for design and best practices is the transport platform over 
which the video transmissions will traverse. We are now looking at an IP network to provide the 
communications infrastructure. With this in mind, some key best practices and network design 
considerations for IP video surveillance have been developed by Cisco Systems, which is beginning 
to challenge the traditional physical security brands by approaching this market from a network-
ing perspective. Much of this segment, and those following immediately after, will be based on the 
design guidance set forth by Cisco Systems.
With a focus on the IP network, several factors come into consideration. Some of the most 
signifi cant elements will be discussed in this section.
The Nature of Video Surveillance Traffi c
Surveillance traffi  c imposes demands on the networking infrastructure in the form of constant 
as well as variable bit-rate video, which have diff erent implications for bandwidth utilization. 
Constant bit-rate video defi nes bandwidth for each video stream, which can be useful for capacity 
planning and storage, but can also occasionally waste bandwidth. Variable bit-rate video changes 
as diff erences in the video stream are captured. Typically, this results in lower overall bandwidth 
consumption, but traffi  c can be bursty, which makes it diffi  cult to plan for capacity. To put this 
into perspective, let us examine Figure 31.2, which depicts constant bit-rate metrics based on video 
resolution.
To put video resolution into perspective, Figure 31.3 outlines respective resolution fi gures, 
represented as pixel measurements of width and height.
Frame rate
1.5
2
3
3.75
5
7.5
10
15
30
450,000
600,000
770,000
935,000
1,050,000
1,400,000
1,700,000
2,200,000
3,000,000
230,000
315,000
410,000
475,000
525,000
750,000
900,000
1,100,000
1,600,000
155,000
200,000
260,000
300,000
330,000
400,000
530,000
600,000
850,000
CIF
2 CIF
4 CIF or D1
Notes:   CBR rate guidelines by resolution and frame rate. Values in bits per second.
Resolution
Figure 31.2 Constant bit-rate bandwidth consumption guidelines.

476 ‚óæ Information Security Management Handbook
Video is also categorized into live video as well as prerecorded video. Live video is clearly delay 
sensitive and quality of service (QoS) is very important in maintaining the integrity of live video 
streams. In most cases, transport is typically UDP (best eff ort). Recorded video is not adversely 
aff ected by delay, and reviews of the recorded video streams can be scheduled or ad hoc. Typically, 
transport is TCP based and the location of recording devices is of importance to understand where 
the information should be retrieved.
Performance and Storage Considerations
When addressing video performance, parameters such as resolution and frame rate have a direct 
impact on both bandwidth and storage requirements. For instance, if we consider a video capture 
practice like dual-streaming, where live viewing locally and remotely need to be facilitated, one 
would be best served using high resolution and frame rates for local live viewing, while remote 
viewing can be adjusted to lower frame rates depending on available bandwidth. Also, especially 
for storage of captured video streams, the two leading methods are as follows:
Centrally located
 
‚óæ
: Centralized storage is typically deployed in a single environmentally con-
trolled facility with close proximity to personnel who provide the technical support and 
management of the surveillance and storage systems. Often, these centralized storage mod-
els provide the advantage of potentially reducing operational costs due to only having a 
single location where the video is stored. However, in order to pass on video to other loca-
tions, or collect video from multiple locations in a central storage facility, there is likely more 
video (data) fl owing across WAN links.
Distributed storage
 
‚óæ
: A distributed model for the storage of video surveillance streams positions 
storage to be located where events are happening, and within close proximity to local sur-
veillance teams who can monitor and react to events in real time. Scalability can be achieved 
by using multiple smaller systems and there is not a single point of failure. Distributed stor-
age also reduces the bandwidth requirements on the LAN and WAN links; however, there 
Resolution
PAL
NTSC
QCIF
176 √ó 144
176 √ó 120
VGA
640 √ó 480
640 √ó 480
SVGA
800 √ó 600
800 √ó 600
XGA
1024 √ó 768
1024 √ó 768
CIF
352 √ó 288
352 √ó 240
2 CIF
704 √ó 288
704 √ó 240
4 CIF
704 √ó 576
704 √ó 480
D1
720 √ó 576
720 √ó 480
Figure 31.3 Video resolution represented in pixels.

An Overview of IP-Based Video Surveillance ‚óæ 477
are downsides like economies of scale‚Äîthe cost per hour of storage with multiple smaller 
systems may be higher. Also, overall operational costs can be higher due to the required 
maintenance of more devices.
Multicast: Just for a Quick Recap
Unicast
 
‚óæ
: Communications between one source address and one destination host address.
Broadcast
 
‚óæ
: Communication from one host address, typically to all hosts on a network seg-
ment or broadcast destination.
Multicast
 
‚óæ
: Communications where one host sends one copy of each packet being sent to a 
special address that is then used by several hosts interested in receiving this information. 
Th e receiving hosts are members of a designated multicast group and can join or leave the 
group dynamically, be members of one or multiple groups, and can be located anywhere on 
the network.
 
  
Multicast is often desirable for IP video surveillance applications since increased effi  cien-
cies can be realized. For instance, lower CPU utilization is often observed since senders 
and receivers only receive requested data. Obviously, network utilization is optimal, when 
compared to replicating multiple streams. However, the nature of multicast relies on UDP, 
rather than TCP, for transport; thus drops can occur and are often expected since there is 
not a built-in receipt mechanism or congestion avoidance.
Quality of Service and Network Design Principles
QoS is a critical technology that must be employed to maintain the integrity and effi  cacy of IP 
video surveillance. QoS when employed in IP networks allows a systems administrator to prioritize 
and protect video surveillance traffi  c from all other types of traffi  c that traverse the network. Also, 
proper deployment of QoS prevents the degradation of service events like packet loss, latency, and 
jitter, which represent the eff ects of mismanaged traffi  c congestion and will often result in poor 
video quality, sometimes a complete loss of video. Let us consider some of the impacts of these 
congestion events as they pertain to IP video surveillance.
Packet loss
 
‚óæ
‚ÄîVideo surveillance decoders may be able to tolerate some degree of packet loss, 
but the quality of the image will degrade. However, since surveillance video is often required 
to reconstruct critical information, there is not a specifi c amount of packet loss that can be 
deemed as ‚Äúacceptable.‚Äù IP video surveillance packet loss may represent itself as shown in 
Figure 31.4.
Latency
 
‚óæ
‚ÄîTh is measure of delay represents the time required to encode, transmit, buff er, 
and decode the video. Th is delay sensitivity can be more critical for pan/tilt/zoom (PTZ) 
cameras due to their movement. A popular best-practice metric is to optimize total end-to-
end delay to under 500 ms, with encoding and decoding best done under 250 ms, leaving 
1‚Äì50 ms for LAN transport, and 100‚Äì150 ms for WAN transport.
Jitter
 
‚óæ
‚ÄîRepresented as delay variation, accounts for diff erences in the end-to-end delay for 
IP packets for a given video surveillance stream. Packets can over- or underrun the avail-
able buff er space, aff ecting the quality of playback. Th is eff ect can cause additional frames 

478 ‚óæ Information Security Management Handbook
to be stored in the decoder‚Äôs buff er, potentially reordering the frame sequence, introducing 
latency, and can result in dropped packets. Jitter is often recommended to be kept under 
10 ms whenever possible.
Innovated by Cisco Systems, the hierarchical network design principle is often used in the deploy-
ment of IP video surveillance and leverages the advantages inherent to the core, distribution, and 
access layers design. A typical topology is shown in Figure 31.5.
QoS implementation for IP video surveillance traffi  c requires classifi cation, marking, queuing, 
and scheduling provisions. Th e access layer defi nes the edge of the network, or the ingress point, 
where traffi  c must be classifi ed and marked for further treatment in the architecture. Th e edge 
devices may be able to rely on confi gurable Diff erentiated Services Code Point (Diff Serv Code 
Point, or DSCP) markings that originate from the end devices themselves, that is, video surveil-
lance cameras. Access layer switches are then confi gured to trust these markings and preserve 
them upon ingress.
When looking at the actual classifi cation and markings, the preferred methods for marking pack-
ets are for class of service (CoS), which is performed at Layer 2 of the OSI model, and DSCP marking, 
which is conducted at Layer 3. Let us briefl y examine CoS marking at Layer 2 (Figure 31.6).
An Ethernet frame can be marked at Layer 2 (trunked) with their relative importance by 
setting the 802.1p user priority bits of the 802.1Q header. Since only three bits are available for 
802.1p marking, eight COSs can be marked, 0‚Äì7.
Figure 31.4 Examples of video surveillance packet loss.

An Overview of IP-Based Video Surveillance ‚óæ 479
Access
WAN
WAN
Data center
Distribution
Distribution
Access
Campus
Core
Figure 31.5 Core-distribution-access design.
Three bits used for CoS
(802.1p user priority)
Ethernet frame
802.1 Q/p
header
CoS
7
6
5
4
3
2
1
0
Application
Reserved
Routing
Voice
Video
Call signaling
Bulk data
Critical data
Best effort data
Pream.
SFD
SA
Type
TAG
4 bytes
PRI
CFI
VLAN ID
PT
Data
FCS
DA
Figure 31.6 CoS marking.

480 ‚óæ Information Security Management Handbook
If we now examine a DSCP marking at Layer 3, we see where several key pieces of information 
are coded into the packet (Figure 31.7).
Here we see information written into the type of service (ToS) byte. Th e three most signifi cant 
bits of the ToS byte are called IP precedence (IPP). Th e six most signifi cant bits of the ToS byte are 
called the DSCP, with the remaining two bits left for fl ow control. IPP values defi ne eight levels 
of marking, which some fi nd too restrictive and much prefer the 6-bit/64-value DSCP model. 
Figure 31.8 summarizes the recommendations for marking:
We must also consider the implications for WAN traffi  c. QoS for video surveillance across 
the wide-area network primarily accounts for queuing and scheduling. In particular, prior-
ity queue, or low latency queuing (LLQ) and class-based weighted fair queuing (CBWFQ). 
Priority or LLQ is employed to immediately service the highest priority traffi  c to minimize 
delay and prevent bandwidth starvation. CBWFQ will sort traffi  c based on markings and 
weighting them for servicing, while also providing traffi  c shaping for events like recorded 
video bursts across the WAN. For example, delay-, loss-, and jitter-sensitive traffi  c such as live 
video and audio surveillance and PTZ camera control will use a priority or LLQ algorithm. 
IP precedence
7
6
5
4
3
2
1
0
DiffServ code point (DSCP)
IP ECN
Unused
Standard IPv4
DiffServ extensions
ToS
byte
Version
length
Len
ID
Offset
TTL
Proto
FCS
IP SA
IP DA
Data
Figure 31.7 IPP and DSCPs.
Layer 3 Classifi cation
Traffi c Type
IPP
PHB
DSCP
CoS
Live video surveillance
4
CS4
32
4
Live audio surveillance
4
CS4
32
4
PTZ/control
4
CS4
32
4
Recorded video 
surveillance
4
AF41
34
Recorded audio 
surveillance
4
AF41
34
4
System administration
2
CS2
16
2
Figure 31.8 QoS baseline marking recommendations for IP video surveillance.

An Overview of IP-Based Video Surveillance ‚óæ 481
Traffi  c that is more tolerant of fl uctuations like recorded video and system administration can 
use CBWFQ algorithms.
With respect to network design for IP video surveillance, QoS, when implemented properly, 
can greatly increase effi  ciencies and the performance of surveillance systems, whether traffi  c is 
traversing the LAN and/or WAN. QoS is not a substitute for the proper provisioning of adequate 
link speeds but, conversely, provisioning a high level of bandwidth on a link is also not a replace-
ment for properly confi guring QoS on the network. When balanced in concert, these elements of 
networking work well together to provide optimal performance while not wasting bandwidth or 
network resources.
Port-Based Security
Another best-practice focuses on port-level security where end devices such as the actual IP video 
surveillance cameras attach. IEEE 802.1x is a client-server based access control and authentication 
protocol that restricts unauthorized devices from connecting to a LAN through publicly acces-
sible network ports. 802.1x provides this port-based network admission control for both wired and 
wireless endpoints. Used in conjunction with wireless network security, many security features 
are provided to protect wireless network access. Wireless security options include WEP, WPA, 
WPA2-PSK, and WPA2-Enterprise. Employing strict port-based security policies in addition to 
existing network security measures helps preserve the integrity of not only the network, but also 
the IP video surveillance cameras and what they are privy to viewing.
Case Study: Retail
One the largest consumers of video surveillance technologies is the retail industry, which primar-
ily uses surveillance to monitor theft patterns. In the 18th Annual Retail Th eft Survey conducted 
in 2005, thieves stole over $5.8 billion from 24 surveyed retailers. Th is information was extracted 
from a survey that covered 13,313 stores with retail sales exceeding $519 billion that year. Th is 
shrinkage from theft accounts for about 2%‚Äì3% of revenues in most cases, and only 2%‚Äì3% of 
stolen merchandise is ever recovered. Using a combination of IP video surveillance, video stream 
storage, and video analytics, the retailer was able to respond much faster to real-time events and 
analyze events without personnel manning every video feed. Leveraging technologies like video 
analytics allowed security offi  cers to concentrate on events to augment loss prevention, rather than 
scouring hours of video trying to locate a place in time where something happened. Additionally, 
employing tactics such as remote monitoring and operations of the surveillance system, and remote 
access to stored video greatly enhanced the productivity of the security staff . An example topology 
is represented in Figure 31.9.
In addition to the enhanced security and loss prevention, retailers have also discovered that 
surveillance systems are very eff ective in optimizing store operations and staff  effi  ciency. For 
example, being able to monitor the number of open checkout stations as compared to the number 
of patrons standing in line allows the real-time analysis of operations effi  ciency and eff ectiveness. 
Should more staff  be called in to speed up checkout, or should some staff  be allocated to other 
departments when things are slow at the registers? Th ese questions are easily answered in real time 
to optimize the effi  ciency of operations.

482 ‚óæ Information Security Management Handbook
Local security
20 fps video
Analytic A
Analytic C
Analytic B
Event storage
1 fps video
1 fps video
30 fps video
15 fps video
Storage
Corporate
Vendor
1 fpm video
Main
entrance
Downtown
Desserts
& Coffee
Salad bar
North Side
Trattoria
Tea & coffee
Walk-in
beer
cooler
Fifth Street
Seafood
Lamar
Street
Greens
Nut roaster
Grocery
valetStairs to
garage
Outdoor market
Stairs
to plaza
level
Entrance
Checkout
Exit
Dining area
Seafood
soups & salads
Soups
Candy
Island
Rest rooms
Figure 31.9 Retail IP video surveillance solution.

An Overview of IP-Based Video Surveillance ‚óæ 483
Summary
In closing this overview of IP video surveillance, we are able to see some of the inherent advantages 
of deploying an IP-based surveillance system over traditional and/or analog solutions. We can 
also observe the effi  ciencies realized and potential cost savings when implementing an intelligent 
surveillance solution that leverages the existing network infrastructure. Also, when employing an 
IP video surveillance solution, we can clearly see that the system becomes a truly integral part of 
the overall networking platform and security practice, involving more than just video cameras 
and dedicated storage. As physical security endpoints are integrated into the network, the same 
diligence is required in properly managing security policies and integrity of the environment.
About the Author
Leo Kahng is a consulting systems engineer for Cisco‚Äôs U.S. Public Sector Sales theater, focusing 
on strategic business developments eff orts in the education markets.


485
Index
A
Access control, 6, 22; see also Discretionary access 
control; Mandatory access control (MAC)
Adaptive defenses
behavior modifi cation, 36
breadth and depth, 37
compromises anticipation, 41
constraints, 40‚Äì41
defense in depth, 42
defense mutations, 38
defensive adaptation weaknesses, 39
environment, 39‚Äì40
frequency, 36‚Äì37
growth rate, 39
indicators and baselines, 37
interactions, 38
penetration testing and response plans, 41
rule sets, 38
search space, 40
signatures, 38
specifi city and timeliness, 39
strengthening defensive adaptations, 41
thresholds, 38
Adaptive threats
attack vectors and behavioral changes, 31
code improvements, 34
command and control, 32
competition, 36
defeat signatures, 34
detection avoidance, 34‚Äì35
recruiting, 33‚Äì34
self-preservation, 35‚Äì36
storage and confi guration, 33
system interaction, 32‚Äì33
threat mutations, 34
vulnerability exploitation, 31
Administrator rights, 22
Advanced metering infrastructure (AMI), 338, 341‚Äì342
‚ÄúAlways-on‚Äù connections, 110
AMI, see Advanced metering infrastructure
Antispam, see Bayesian fi lter
Application control solution
phases
application ecosystem, discovering 
and monitoring, 13, 14
assigning rights, 13, 14
enforcement, 15
fi ne-tuning, application ecosystem, 15, 16
pilot rollout, 13
vendors, 15‚Äì16
Attack vectors, 31
B
Baked in security, 328‚Äì329
Band-aid approach, 220
Bayesian fi lter
annoyance fi lter, 80‚Äì81
Bayes theorem utilization, 77‚Äì78
blacklisting, 76
MacOS, 79‚Äì80
Paul Graham, 78‚Äì79
process, 76
SpamAssassin, 81‚Äì83
Bell‚ÄìLa Padula model (BPM), 5‚Äì6, 68‚Äì69
Biba formal model, 6
Breakout policy, 116‚Äì118
Brewer‚ÄìNash‚ÄîChinese Wall, 7
Business continuity planning; see also Integrated business 
continuity plan
business emergency response teams (BERTs) 
program, 402‚Äì407
community emergency response teams (CERTs), 
role of, 401‚Äì402
crisis management plan, integration, 400
incident command system (ICS), 399
MS PPT, 402, 403
multiagency coordination systems (MACS), 399
National Incident Management System (NIMS), 
399‚Äì400
private sector integration, 400

486 ‚óæ Index
Business data lifecycle, 22, 23
Business emergency response teams (BERTs) program
benefi ts of, 402‚Äì403
establishment
components, 403‚Äì404
costs, 404‚Äì405
equipment, 405‚Äì406
executive management communications, 
406‚Äì407
timing of, 405
training, 405
role of, 402
Buzz, 329
C
CAS, see Code access security (CAS)
Central Intelligence Agency (CIA), 194
CERT, see Community emergency response teams
Chinese Wall, 7
Clark‚ÄìWilson formal model, 6‚Äì7
Closed-circuit television (CCTV), 472‚Äì473
CloudAV, 327
Cloud computing, 326
CLR, see Common language runtime (CLR)
Code access security (CAS)
evidence, 267‚Äì268
security policies, 268
user-based security, 267
COM and DCOM, 271
Commercial entities, see Cyber warfare
Common language runtime (CLR)
assemblies, 265
bounds checking, 266
CISSP CBK domains, 264
garbage collection, 266‚Äì267
managed code, 265
security exception management, 266
strong-named assemblies, 265‚Äì266
type safety, 266
unmanaged code, 265
Communications and network security
adaptive defenses, 36‚Äì42
adaptive threats, 31‚Äì36
heteroarchical vs. hierarchical design, 50
innovation adoption lifecycle, 50
TNC architecture, 50
TNC roadmap, 47
trusted computing group (TCG), 47
Trusted Network Connect (TNC), 47‚Äì48
U.S. Government networks demystifi cation
Bell‚ÄìLa Padula model (BPM), 68‚Äì69
controlled unclassifi ed information (CUI), 62
cross domain solution (CDS), 69‚Äì70
Intelligence Reform and Terrorism Prevention 
Act (IRTPA), 59‚Äì60
Communications plan, 373
Community emergency response teams (CERT), 
401‚Äì402
Compliance, 330
Computer-based training (CBT) modules, 94‚Äì98
Computer crime, see Computer forensics; Cyberstalk
Computer forensics
hardware write blocking devices, 426
high-end stand-alone acquisition hardware, 427
NCFS procedure, 428
software write blocker, 427
validation of
media changes, 440‚Äì442
media preparation, 428‚Äì432
media tests, 432‚Äì436
safe block activation, 436‚Äì437
write blocking device tests, 437‚Äì440
Controlled unclassifi ed information (CUI), 62
Control Objectives for Information and Related 
Technology (COBIT), 171
Convergence, 328
Coordination plan, 372‚Äì373
Covert channels, 22‚Äì25
Crisis management plan, integration, 400
Cross domain solution (CDS), 69‚Äì70
Cryptographically secure digital timestamp (CSDT) 
concept, 255
Cryptography
compliance regimes, 281‚Äì282
data protection API, 276
digital signing, 275
encryption and decryption, 275
hashing, 275
key management, 282‚Äì285
Microsoft Cryptographic Application 
Programming Interface (Microsoft 
CryptoAPI), 274‚Äì275
public-key cryptography standards, 275‚Äì276
random number generation, 276
schemes
data encryption, 249
digital signature, 252‚Äì253
encrypted hash, 250‚Äì251
keyed hash, 251‚Äì252
message authentication code (MAC), 250
unifying compliance
developing process, 285‚Äì286
operational processes, 286‚Äì287
Cyber partisans, 338‚Äì340
Cyberstalk
anti-stalking, 419‚Äì420
behaviors, 416‚Äì417
criminal stalking, 419
defi nition, 414‚Äì415
features, 419
gender of victim, 415‚Äì416
and law, 417‚Äì418
opportunities, 414

Index ‚óæ 487
predators and perpetrator, 416
safe online, 420
victims, 415‚Äì416
Cyber warfare
cyber partisans, 338‚Äì340
defend critical infrastructure, regulatory eff orts, 340
fear vs. reality, 344‚Äì346
information warfare doctrine, 340‚Äì341
nation-state and cybercriminal actors, diff erentiation, 
337
non-kinetic and civilian exposure, 336‚Äì337
private organizations, front line, 337‚Äì338
technology and impact, evolution, 341‚Äì342
threats
nation-state actors, 344
non-state causes, 343‚Äì344
small/medium civil organizations, 343
D
Data classifi cation and labeling scheme, 22, 23
Data destruction moratorium, 351
Data loss prevention (DLP) program
components of
business segment detail data analysis, 236‚Äì237
data classifi cation, 234‚Äì236
data discovery, 238‚Äì239
governance, 232
personal identifi able information (PII), 230
policies, standards, procedures, 238
regulatory and privacy requirements, 233‚Äì234
remediation processes, 239‚Äì241
risk assessment, 232‚Äì233
training and awareness, 241‚Äì242
data leakage, 230
emergency departments, 242‚Äì243
information technology (IT), 229‚Äì230
legal challenges
benefi ts of, 244
federal and state laws, 243‚Äì244
resource challenges, 244
sources, 230
Data reliability
cryptography schemes
data encryption, 249
digital signature, 252‚Äì253
encrypted hash, 250‚Äì251
keyed hash, 251‚Äì252
message authentication code (MAC), 250
trusted time stamps, 253‚Äì255
electronic data unreliability
data-generation event (DGE), 247
electronic data, 246‚Äì249
Sarbanes‚ÄìOxley Act, 247
sensitive data, 248
pen and ink reliability, 246
standardization, 256
Decoupling, 381
Department of Defense (DoD), 195‚Äì196
Disaster recovery planning, see Business continuity 
planning; Integrated business continuity plan
Discretionary access control, 4, 215‚Äì216
dot NET framework security, see .NET framework 
security
E
E-discovery, 124‚Äì125
Enron approach, 349‚Äì350
Enterprise risk management (ERM) approach, 
224‚Äì225
ESX host
confi guring
certifi cate validation, 308‚Äì310
layer 2 security settings, 306‚Äì308
network isolation, 303‚Äì305
hardening steps
active directory integration, 297‚Äì298
confi gure proper logging, 300‚Äì301
controlled root access, 295
fi le system integrity, 301‚Äì302
password aging, 295‚Äì296
password complexity and lock out, 296
privileged operations, 297
secure SNMP confi guration, 299‚Äì300
services, 298‚Äì299
strong password policy, 295
USB devices, 302‚Äì303
patches, 294
security tools, 294
server management, 294
F
Fear vs. reality, 344‚Äì346
Federal Deposit Insurance Corporation Improvement 
Act (FDICIA), 225
Federal Information Security Management Act 
(FISMA), 171
Federal Information System Controls Audit Manual 
(FISCAM), 171
Fire-fi ghting mode, 145
G
GIST, see Global information systems transformation
Global information systems transformation (GIST)
digital abyss, 46
interface metadata access point (IF-MAP)
heteroarchical vs. hierarchical design, 50
simple object access protocol (SOAP), 48
threat state databases, evolution, 52‚Äì55
TNC architecture, 50
weenie factor, 48

488 ‚óæ Index
standards-based information sharing
cloud computing, 55
innovation adoption lifecycle, 56, 57
silos, 56
threat and product cycle, 46
TNC roadmap, 47
trusted computing group (TCG), 47
Trusted Network Connect (TNC), 47‚Äì48
Globalization
COTS benefi ts, 196
Department of Defense (DoD), 195‚Äì196
Governance, risk, and compliance (GRC), 169‚Äì170
H
Hacker‚Äôs process
external perimeter, 189‚Äì190
internal network, 190
password and e-mail/web site spoofi ng, 186‚Äì187
physical security, 186
social engineering, 185‚Äì186
wireless environment, 187‚Äì189
Hardware write blocking devices, 426
Health Insurance Portability and Accountability Act 
(HIPAA), 170‚Äì171
I
ICS, see Incident command system
IDS, see Intrusion detection systems
Incident command system (ICS), 399
Information continuity plan, 381‚Äì385
Information destruction requirements and techniques, 
companies
data destruction moratorium, 351
Enron approach, 349‚Äì350
record retention and document destruction policy, 348
Sarbanes‚ÄìOxley (SoX), 350
security containers, companies
bulk containers, 352
executive consoles, 351
large containers, 352
Information technology (IT) outsourcing
business case and cost savings, 202
business impact, 202‚Äì203
Central Intelligence Agency (CIA), 194
data security/protection, 198‚Äì199
espionage cases
facts of, 200‚Äì201
semiconductor industry, 201‚Äì202
ethics, 203‚Äì204
globalization
COTS benefi ts, 196
Department of Defense (DoD), 195‚Äì196
insider threats, 199‚Äì200
intellectual property, 197‚Äì198
national interest, 204‚Äì205
personnel turnover, 199
risk management, 197
risk mitigations
rusted foundry, 205
software, 205‚Äì206
types, 196‚Äì197
Information risk management
defi nition of, 210
nature of
operational risk, 210
strategic risk, 210
tactical risk, 210
process of
address risk, 214‚Äì215
identify risk, 211‚Äì213
information security program, 211
measure performance, 216‚Äì218
mitigate risk, 215‚Äì216
quantify risk, 213‚Äì214
Information security and privacy training, evaluation 
eff ectiveness
business drivers, 88
components
access and relevancy, 90
awareness newsletters, 98‚Äì103
computer-based training (CBT) modules, 94‚Äì98
determination, intangible benefi ts, 94
education checklist, 103‚Äì104
education, intangible benefi ts, 93
impact and cost eff ectiveness, 91
quality and learning outcomes, 90
specifi c awareness and training methods, 94
Information technology (IT) control frameworks
audit values, 175
best practices terminology, 173
compliance, 174
control framework convergence, 175‚Äì176
Control Objectives for Information and Related 
Technology (COBIT), 171
Federal Information Security Management Act 
(FISMA), 171
Federal Information System Controls Audit Manual 
(FISCAM), 171
governance, risk, and compliance (GRC), 169‚Äì170
Health Insurance Portability and Accountability Act 
(HIPAA), 170‚Äì171
Information Security Management Systems 
Requirements, 171
Information Technology Infrastructure Library 
(ITIL), 171‚Äì172
National Institute of Standards and Technology 
(NIST), 171
National Security Agency (NSA) guides, 172
Payment Card Industry Data Security Standard 
(PCI DSS), 171
Security Technical Implementation Guides (STIGS), 
172

Index ‚óæ 489
standards and control frameworks integrations, 
174‚Äì175
world-wide usage of standards, 172
Information Technology Infrastructure Library (ITIL), 
171‚Äì172
Innovation adoption lifecycle, 56, 57
Integrated business continuity plan
communications plan, 373
coordination plan
strategic components, 372‚Äì373
tactical components, 373
crisis onset and response
crisis recognition, 386
declaration, 393
emergency response, 393‚Äì394
incident management, 394
organizational impact, 386
process and information continuity, 394
recovery termination, 394‚Äì395
scope impact, 386
sequence, 391‚Äì393
information continuity plan
decoupling, 381
desktop simplifi cation, 382
planning schemes, 384‚Äì385
reduced workforce, 383
redundancy, 381
remote storage, 381‚Äì382
remote workforce, 383
resilience/disaster tolerance, 381
server and storage management, 382‚Äì383
strategic components, 380‚Äì381
tactical components, 383‚Äì384
internal incident management plan
strategic components, 378‚Äì379
tactical components, 379
management resources, 358, 359
organizational assessment
business characterization, 369‚Äì370
existing documented internal business continuity 
plan, 370
existing external relationships, 370‚Äì371
existing internal business continuity 
responsibility structure, 370
organizational responsibilities
executive-level coordination, 366
mandatory internal collaboration, 366
operational/tactical plan, 367
plan maturity measurement, 367‚Äì368
strategic plan, 366‚Äì367
outlines, 387‚Äì390
overriding perspectives
human safety, 363‚Äì364
organizational BCM plan, 365
organizational data and information, 
364‚Äì365
process and service continuity, 364
process continuity plan
strategic components, 379
tactical components, 380
risk management plan
controls suffi  ciency assessment, 376
criticality assessment, 374
risk analysis assessment, 376
strategic components, 374
tactical components, 376‚Äì377
threat ecology assessment, 375
vulnerabilities assessment, 375‚Äì376
safety and emergency response plan, 377
scope and terminology responsibility
crisis onset and response, 363
information and process continuity, 362
organizational assessment, 363
safety and security professionals, 361
Intellectual property, 197‚Äì198
Intelligent automation, 327‚Äì328
Internal incident management plan, 378‚Äì379
Internet Explorer vulnerabilities, 22
Internet protocol (IP)-based video surveillance
algorithms and behavior detection, 474
best practices and network design considerations
multicast, 477
performance and storage considerations, 476‚Äì477
port-based security, 481
quality of service and principles, 477‚Äì481
surveillance traffi  c, 475‚Äì476
closed-circuit television (CCTV), 472‚Äì473
features and utilities, 473
motion or loitering, 473, 474
retail, 481‚Äì482
Intrusion detection systems (IDS), 327‚Äì328
ISO 27001 certifi cation
bad packaging, 143‚Äì144
consistent third-party governance, 147
defensibility, 147‚Äì148
fi re-fi ghting mode, 145
information risk management, 145‚Äì146
legal and regulatory compliance, 147
lip service, 144
management system, 142
market diff erentiation, 144‚Äì145
packaging, typical functions, 142
proactive vs. reactive security management, 145
process defi nition and metrics, 146
time-based assurance, 146
L
Layer 2 security settings, 306‚Äì308
Lip service, 144
Local government, information security program
establishment, 128‚Äì129
executive, 132‚Äì133
guidelines, 135

490 ‚óæ Index
organization, 135‚Äì137
organizational culture and behavior, 130‚Äì132
organizational governance, 129‚Äì130
policies and standards, 134
procedures, 134
strategic framework, 136, 137
Logical units masking (LUN), 311
M
MACS, see Multiagency coordination systems
Mandatory access control (MAC), 5, 6, 22, 216
Mandatory protection (security), 18‚Äì22
Mass load-shedding event, 342
Message authentication code (MAC), 250
Microsoft Offi  ce vulnerabilities, 22
Microsoft Windows vulnerabilities, 22
Mobile-based shredding, 352
Mobile device security
advantages, 109
management strategies
breakout policy, creation, 116‚Äì118
business continuity planning, 124
compliance monitoring, 125
controls, selection and implementation, 118‚Äì121
creation, 114‚Äì115
e-discovery, 124‚Äì125
incident response, 124
intrusion detection, 123‚Äì124
risk analysis, perform, 115‚Äì116
standards and procedures, 121‚Äì123
training and awareness, 123
risks
‚Äúalways-on‚Äù connections, 110
control devices, reduced ability, 110‚Äì113
conventional systems, attacks, 109‚Äì110
sidebar, 114
Multiagency coordination systems (MACS), 399
N
NAID, see National Association of Information 
Destruction
National Association of Information Destruction 
(NAID), 351‚Äì353
National Incident Management System (NIMS), 
399‚Äì400
National Institute of Standards and Technology (NIST), 
171
National Security Agency (NSA) guides, 172
Native client, 328
NCFS procedure, 428
Negative security model, application control
application ecosystem, discovering and monitoring, 
13, 14
assigning rights, 13, 14
enforcement, 15
fi ne-tuning, application ecosystem, 15, 16
pilot rollout, 13
vendors, 15‚Äì16
.NET framework security
access control
accounting, 273
authentication, 272
authorization, 273
application domains, 268
application security
isolated storage, 270
.NET framework security namespaces, 269
secure strings, 270
architecture and design
ASP .NET, 271‚Äì272
COM and DCOM, 271
Windows host security, 270‚Äì271
code access security (CAS)
evidence, 267‚Äì268
security policies, 268
user-based security, 267
common language runtime (CLR)
assemblies, 265
bounds checking, 266
CISSP CBK domains, 264
garbage collection, 266‚Äì267
managed code, 265
security exception management, 266
strong-named assemblies, 265‚Äì266
type safety, 266
unmanaged code, 265
cryptography
cryptographic hashing, 275
data protection API, 276
digital signing, 275
encryption and decryption, 275
Microsoft Cryptographic Application 
Programming Interface, 274‚Äì275
public-key cryptography standards, 275‚Äì276
random number generation, 276
domain, 263
issues, 276‚Äì277
Java, 262‚Äì263
network security
secure network communications protocols, 273‚Äì274
security support provider interface, 274
Trustworthy Computing, 261
windows, 261‚Äì262
Network attacks and countermeasures, see Bayesian fi lter
NIMS, see National Incident Management System
Normal business data lifecycle, 22, 23
O
Operational/tactical plan, 367
Operations security, see also Cyber warfare
data destruction moratorium, 351

Index ‚óæ 491
information destruction requirements and 
techniques, companies, 248‚Äì352
mobile-based shredding, 352
National Association of Information Destruction 
(NAID), 351‚Äì353
plant-based shredding, 352
record retention and document destruction policy, 348
Sarbanes‚ÄìOxley (SOX), 350
Orange Book‚Äìbased security
architecture models
Bell‚ÄìLa Padula, 5‚Äì6
Biba, 6
Brewer‚ÄìNash‚ÄîChinese Wall, 7
Clark‚ÄìWilson, 6‚Äì7
classes, 4‚Äì5
covert channels, 22‚Äì25
mandatory protection, 18‚Äì22
negative security model, application control
application ecosystem, discovering 
and monitoring, 13, 14
assigning rights, 13, 14
enforcement, 15
fi ne-tuning, application ecosystem, 15, 16
pilot rollout, 13
vendors, 15‚Äì16
positive security model
antivirus considerations and application control 
considerations, 10‚Äì12
example, 16‚Äì18
gateway considerations, 9‚Äì10
threat environment, 7‚Äì8
usage of data classifi cation and labeling scheme, 22, 23
Organizational BCM plan, 365
Organizational responsibilities
executive-level coordination, 366
mandatory internal collaboration, 366
operational/tactical plan, 367
plan maturity measurement, 367‚Äì368
strategic plan, 366‚Äì367
P
Payment Card Industry Data Security Standard 
(PCI DSS), 171
change control and confi guration management
access tracking/monitoring, 155
antivirus software/programs updation, 153
cardholder data encryption, 153
cardholder data protection, 152
information security addressing policy, 156‚Äì157
restricted access, 154
secure systems and applications, 153‚Äì154
system passwords and security parameters, 151‚Äì152
testing, security systems and processes, 156
unique identifi cation (ID), 154
fi rewall and confi guration standards, 151
implementing controls
access control, 163‚Äì164
account management, 164
approvals, 157
asset management, 158
awareness, 159
backup and restore, 165
cyber security incident response plan, 164
cyber security policy, 158
cyber vulnerability assessment, 161‚Äì162
disposal or redeployment, 160
electronic security perimeter, 160‚Äì161
exceptions, 157‚Äì158
exercises, 165
leadership, 157
logging physical access, 160
maintenance and testing, 160
malicious software prevention, 162‚Äì163
monitoring electronic access, 161
monitoring physical access, 159‚Äì160
personnel risk assessment, 159
physical access controls, 159
planning maintenance, 165
ports and services, 162
recovery plans, 165
security patch management, 162
security status monitoring, 163
testing backup media, 166
test procedures, 162
training, 159
Internet and system component, 151
PCI DSS compliance, see Payment Card Industry Data 
Security Standard (PCI DSS)
Pen and ink reliability, 246
Penetration testing (PenTest), 184
Personal identifi able information (PII), 230
Physical security elements
algorithms and behavior detection, 474
best practices and network design considerations, 
475‚Äì481
closed-circuit television (CCTV), 472‚Äì473
sensitive data protection
data-in-motion and data-at-rest, 451‚Äì452
destruction of, 452
hard copy deterioration, 450‚Äì451
humidity, 450
light, 451
monitoring practices, 452‚Äì453
policy, 453‚Äì455
temperature, 449‚Äì450
site selection and facility design considerations
build or select, 465‚Äì466
business requirements and threats, 464‚Äì465
physical and material supplies, 468‚Äì470
water leakage and fl ooding
humidity and condensation, 460
types of, 459
water and environmental threats costs, 459‚Äì460

492 ‚óæ Index
Plant-based shredding, 352
Positive security model
antivirus considerations and application control 
considerations, 10‚Äì12
example, 16‚Äì18
gateway considerations, 9‚Äì10
Predators and perpetrator, 416
Prescriptive aphorisms, 131
Private sector integration, 400
Process continuity plan, 379‚Äì380
Public sector vs. private sector and corporate 
organizations, 129, 130
R
Record retention and document destruction policy, 348
Redundancy, 381
Return on security investment (ROSI), 327
Risk management
addressing, 214‚Äì215
identifi cation
incident evaluation, 212‚Äì213
threat forecasting, 211‚Äì212
information security program, 211
IT outsourcing
business case and cost savings, 202
business impact, 202‚Äì203
data security/protection, 198‚Äì199
espionage cases, 200‚Äì202
ethics, 203‚Äì204
globalization, 195‚Äì196
insider threats, 199‚Äì200
intellectual property, 197‚Äì198
national interest, 204‚Äì205
personnel turnover, 199
risk mitigations, 205‚Äì206
types, 196‚Äì197
IT control frameworks
audits value, 175
control framework convergence, 175‚Äì176
importance of, 172
standards and control frameworks integration, 
174‚Äì175
mitigation
control objectives, 215
controls selection, 215‚Äì216
risk treatment, 216
nature of, 210
performance measurement, 216
control attributes, 217
residual risk, 218
risk metrics, 217
planning, 374‚Äì376
quantifi cation
assessment framework, 213‚Äì214
assessment scope, 213
raw risk, 214
risk assessment, 213
risk quantum, 214
Sarbanes‚ÄìOxley (SOX) revolution, 219‚Äì226
security processes and organization
hacker‚Äôs process, 185‚Äì190
vulnerability assessments and penetration testing, 
183‚Äì185
Rogue security software, 16‚Äì17
ROSI, see Return on security investment
S
Safety and emergency response plan, 374‚Äì376
Sarbanes‚ÄìOxley (SOX), 350
band-aid approach, 220
enterprise risk management (ERM) approach, 224‚Äì225
Federal Deposit Insurance Corporation Improvement 
Act (FDICIA), 225
fi nancial institutions, 221
fi nancial report, 219‚Äì220
Public Accounting Oversight Board (PCAOB), 
222‚Äì223
Securities and Exchange Commission (SEC) rules, 
222‚Äì223
Sarbanes‚ÄìOxley Act, 247
Script kiddie, 337
Securities and Exchange Commission (SEC) rules, 
222‚Äì223
Security architecture and design
baked in security, 328‚Äì329
buzz, 329
CloudAV, 327
cloud computing, 326
compliance, 330
convergence, 328
intelligent automation, 327‚Äì328
intrusion detection systems (IDS), 327‚Äì328
native client, 328
return on security investment (ROSI), 327
virtual desktop infrastructure (VDI), 329
virtualization security, 326
ESX host, 291‚Äì310
security banner, 312‚Äì313
storage security, 310‚Äì312
tools, 321‚Äì324
virtual machine, 316‚Äì321
vulnerability management, 330
Web 2.0, 329
World Wide Web, 326
Security Content Automation Protocol (SCAP), 19‚Äì22
Security management concepts and principles
information security and privacy training, evaluation 
eff ectiveness
access and relevancy, 90
awareness newsletters, 98‚Äì103
business drivers, 88
computer-based training (CBT) modules, 94‚Äì98

Index ‚óæ 493
determination, intangible benefi ts, 94
education checklist, 103‚Äì104
education, intangible benefi ts, 93
impact and cost eff ectiveness, 91
quality and learning outcomes, 90
specifi c awareness and training methods, 94
local government, information security program
establishment, 128‚Äì129
organization, 135‚Äì137
prescriptive aphorisms, 131
procedures, 134
Security Triangle, 129, 135, 136
mobile device security
advantages, 109
‚Äúalways-on‚Äù connections, 110
breakout policy, 116‚Äì118
business continuity planning, 124
iPhone, 114
Security patch management, 162
Security processes and organization
hacker‚Äôs process
external perimeter, 189‚Äì190
internal network, 190
password and e-mail/web site spoofi ng, 
186‚Äì187
physical security, 186
social engineering, 185‚Äì186
wireless environment, 187‚Äì189
penetration testing (PenTest), 184
real-world risk management, 180
vulnerability assessments (VA), 184‚Äì185
wireless environment
AD-HOC base station, 188
PenTest, 188
process of, 188‚Äì189
WEP encryption, 188
Security Technical Implementation Guides (STIGS), 172
Security Triangle, 129, 135, 136
Sensitive data protection
data-in-motion and data-at-rest, 451‚Äì452
destruction of, 452
hard copy deterioration, 450‚Äì451
humidity, 450
light, 451
monitoring practices, 452‚Äì453
policy, 453‚Äì455
temperature, 449‚Äì450
Site selection and facility design considerations
build or select, 465‚Äì466
business requirements and threats, 464‚Äì465
physical and material supplies
eff ects, 468‚Äì469
environmental, 468
facility, 466‚Äì468
TIA-942, 469‚Äì470
Smart Grid, 342
Smart meters, 31‚Äì32
Software write blocker, 427
System development controls
data loss prevention program
components, 231‚Äì242
data leakage, 230
emergency departments, 242‚Äì243
legal challenges, 243‚Äì244
data reliability
cryptography schemes, 249‚Äì255
electronic data, 246‚Äì249
pen and ink reliability, 246
standardization, 256
.NET framework security
access control, 272‚Äì273
application domains, 268
application security, 268‚Äì270
code access security, 267‚Äì268
common language runtime (CLR), 263‚Äì267
cryptography, 274‚Äì276
network security, 273‚Äì274
security architecture and design, 270‚Äì272
Trustworthy Computing, 261
T
TCG, see Trusted Computing Group
Th reat environment, 7‚Äì8
Th reats and defenses, evolution
adaptive defenses
anticipate compromises, 41
behavior modifi cation, 36
breadth and depth, 37
constraints, 40‚Äì41
defense in depth, 42
defense mutations, 38
defensive adaptation weaknesses, 39
environment, 39‚Äì40
frequency, 36‚Äì37
growth rate, 39
indicators and baselines, 37
interactions, 38
penetration testing and response plans, 41
rule sets, 38
search space, 40
signatures, 38
specifi city and timeliness, 39
strengthening defensive adaptations, 41
thresholds, 38
adaptive threats
attack vectors and behavioral changes, 31
code improvements, 34
command and control, 32
competition, 36
defeat signatures, 34
detection avoidance, 34‚Äì35
recruiting, 33‚Äì34
self-preservation, 35‚Äì36

494 ‚óæ Index
storage and confi guration, 33
system interaction, 32‚Äì33
threat mutations, 34
vulnerability exploitation, 31
Th reats, cyber warfare
nation-state actors, 344
non-state causes, 343‚Äì344
small/medium civil organizations, 343
TIA-942, 469‚Äì470
Time Stamp Authority (TSA), 253‚Äì254
TNC, see Trusted Network Connect
TNC architecture, 50
TNC roadmap, 47
Trusted Computing Base (TCB), 4‚Äì5
Trusted Computing Group (TCG), 47
Trusted Network Connect (TNC), 47‚Äì48
Trusted time stamps (TST)
cryptographically secure digital timestamp (CSDT) 
concept, 255
Time Stamp Authority (TSA), 253‚Äì254
U
U.S. government networks demystifi cation
agency independency, 66
Bell‚ÄìLa Padula model (BPM), 68‚Äì69
classifi ation vs. unclassifi ed, 63
classifi ed information, 62‚Äì64
controlled unclassifi ed information (CUI ), 62
cross domain solution (CDS), 69‚Äì70
declassifi ed/declassifi cation, 62
diff erences and similarities, 60‚Äì61
information-sharing environment, 71
Intelligence Reform and Terrorism Prevention Act 
(IRTPA), 59‚Äì60
multilevel security, 70‚Äì71
unclassifi ed information, 62
V
VDI, see Virtual desktop infrastructure
Virtual center host controls, 313
Virtual desktop infrastructure (VDI), 329
Virtualization security, 326
ESX host
confi guring, 303‚Äì310
hardening steps, 295‚Äì303
patches, 294
security tools, 294
server management, 294
security banner
console and SSH access, 312‚Äì313
ESX host, 312
storage security
Challenge Handshake Authentication Protocol to 
iSCSI, 312
logical units masking (LUN), 311
raw device mapping, 311
secure iSCSI deployment, 311‚Äì312
zoning, 310‚Äì311
tools
agent-free security, 323
comprehensive and integrated network security, 
323
CPU and memory scanning, 323
detection, 321
future, 322
network monitoring, 323
process monitoring and execution, 323
security bottlenecks, 323‚Äì324
storage scanning, 323
virtualization malware, 321‚Äì322
VMsafe, 324
virtual center
authentication and access control, 
314‚Äì315
database security, 315‚Äì316
disabling unnecessary services, 314
fi rewall, 313‚Äì314
host controls, 313
implementing network isolation, 313
logging and monitoring, 316
roles and permissions, 315
virtual machine
confi guration settings, 321
copy and paste features, 319
disabling unnecessary devices, 
320‚Äì321
isolation based on trust levels, 318‚Äì319
isolation settings, 319
log fi le size, 320
loss of, 318
resource overcommitment, 318
templates, 317‚Äì318
virtual vs. physical world, 317
VMX fi le size, 320
Vulnerability, 7‚Äì8, 10, 22
management, 330
Vulnerability assessments (VA), 184‚Äì185
W
Water leakage and fl ooding
humidity and condensation, 460
types of, 459
water and environmental threats costs, 
459‚Äì460
Web 2.0, 329
Wireless environment
AD-HOC base station, 188
PenTest, 188
process of, 188‚Äì189
WEP encryption, 188
World Wide Web, 326

495
Information Security 
Management Handbook, 
Sixth Edition: Comprehensive 
Table of Contents
Domain 1 Access Control Systems
Title
Volume 1
Volume 2
Volume 3
Volume 4
1.1 Access Control Techniques
A Look at RFID Security, Ben Rothke
x
New Emerging Information Security 
Technologies and Solutions, Tara Chand
x
Sensitive or Critical Data Access Controls, 
Mollie E. Krehnke and David Krehnke
x
An Introduction to Role-Based Access 
Control, Ian Clark
x
Smart Cards, Jim Tiller
x
A Guide to Evaluating Tokens, Joseph T. 
Hootman
x
Controlling FTP: Providing Secured Data 
Transfers, Chris Hare
x
Authentication Tokens, Paul A. Henry
x
Authentication and the Role of Tokens, 
Jeff Davis
x
(continued)

496 ‚óæ Comprehensive Table of Contents
Domain 1 (continued) Access Control Systems
Title
Volume 1
Volume 2
Volume 3
Volume 4
Expanding PKI-Based Access Control 
Capabilities with Attribute Certifi cates, 
Alex Golod
x
1.2 Access Control Administration
Back to the Future, Paul A. Henry
x
End Node Security and Network Access 
Management: Deciding among Different 
Strategies, Franjo Majstor
x
Identity Management: Benefi ts and 
Challenges, Lynda L. McGhie
x
Blended Threat Analysis: Passwords and 
Policy, Daniel D. Houser
x
Accountability, Dean R. Bushmiller
x
Five Components to Identity Management 
Systems, Kevin Castellow
x
1.3 Identifi cation and Authentication Techniques
Enhancing Security through Biometric 
Technology, Stephen D. Fried
x
Single Sign-On for the Enterprise, Ross 
A. Leo
x
1.4 Access Control Methodologies and Implementation
Centralized Authentication Services 
(RADIUS, TACACS, DIAMETER), Bill 
Stackpole
x
An Introduction to Secure Remote Access, 
Christina M. Bird
x
1.5 Methods of Attack
Hacker Tools and Techniques, Ed Skoudis
x
A New Breed of Hacker Tools and Defenses, 
Ed Skoudis
x
Breaking News: The Latest Hacker Attacks 
and Defenses, Ed Skoudis
x
Counter-Economic Espionage, Craig A. 
Schiller
x

Comprehensive Table of Contents ‚óæ 497
Domain 1 (continued) Access Control Systems
Title
Volume 1
Volume 2
Volume 3
Volume 4
Rootkits: The Ultimate Malware Threat, 
E. Eugene Schultz and Edward Ray
x
Security Weaknesses of System and 
Application Interfaces Used to Process 
Sensitive Information, Sean Price
x
1.6 Monitoring and Penetration Testing
Insight into Intrusion Prevention Systems, 
Gildas Deograt-Lumy
x
Penetration Testing, Stephen D. Fried
x
Domain 2 Telecommunications and Network Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
2.1 Communications and Network Security
Adaptive Threats and Defenses, Sean Price
x
Achieving Global Information Systems 
Transformation (GIST) through Standards: 
Foundations for Standards-Based Network 
Visibility via IF-MAP and Beyond, David 
O‚ÄôBerry
x
A Primer on Demystifying U.S. Government 
Networks, Samuel W. Chun
x
Network Security Utilizing an Adaptable 
Protocol Framework, Robby Fussell
x
The Five W‚Äôs and Designing a Secure, 
Identity-Based, Self-Defending Network 
(5W Network), Samuel W. Chun
x
Maintaining Network Security: Availability 
via Intelligent Agents, Robby Fussell
x
PBX Firewalls: Closing the Back Door, 
William A. Yarberry, Jr.
x
Network Security Overview, Bonnie A. 
Goins and Christopher A. Pilewski
x
Putting Security in the Transport: TLS, 
Chris Hare
x
(continued)

498 ‚óæ Comprehensive Table of Contents
Domain 2 (continued) Telecommunications and Network Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
WLAN Security Update, Franjo Majstor
x
Understanding SSL, Chris Hare
x
Packet Sniffers and Network Monitors, 
James S. Tiller and Bryan D. Fish
x
Secured Connections to External Networks, 
Steven F. Blanding
x
Security and Network Technologies, 
Chris Hare
x
Wired and Wireless Physical Layer Security 
Issues, James Trulove
x
Network Router Security, Steven F. Blanding
x
What‚Äôs Not So Simple about SNMP?, 
Chris Hare
x
Network and Telecommunications Media: 
Security from the Ground Up, Samuel Chun
x
Security and the Physical Network Layer, 
Matthew J. Decker
x
Wireless LAN Security Challenge, 
Frandinata Halim and Gildas Deograt
x
ISO/OSI and TCP/IP Network Model 
Characteristics, George G. McBride
x
Facsimile Security, Ben Rothke
x
Mobile Data Security, George McBride
x
Integrated Security through Open 
Standards: A Path to Enhanced Network 
Visibility, David O‚ÄôBerry
x
2.2 Internet, Intranet and Extranet Security
VoIP Security Issues, Anthony Bruno
x
An Examination of Firewall Architectures, 
Paul A. Henry
x
Voice over WLAN, Bill Lipiczky
x
Spam Wars: How to Deal with Junk E-Mail, 
Al Bredenberg
x
Secure Web Services: Holes and Fillers, 
Lynda L. McGhie
x

Comprehensive Table of Contents ‚óæ 499
Domain 2 (continued) Telecommunications and Network Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
IPSec Virtual Private Networks, James S. 
Tiller
x
Internet Security: Securing the Perimeter, 
Douglas G. Conorich
x
Application-Layer Security Protocols for 
Networks, Bill Stackpole
x
Application Layer: Next Level of Security, 
Keith Pasley
x
Security of Communication Protocols and 
Services, William Hugh Murray
x
An Introduction to IPSec, Bill Stackpole
x
VPN Deployment and Evaluation Strategy, 
Keith Pasley
x
Comparing Firewall Technologies, Per 
Thorsheim
x
Cookies and Web Bugs: What They Are and 
How They Work Together, William T. 
Harding, Anita J. Reed, and Robert L. Gray
x
Security for Broadband Internet Access 
Users, James Trulove
x
Network Content Filtering and Leak 
Prevention, Georges J. Jahchan
x
Web Application Firewalls, Georges J. 
Jahchan
x
2.3 E-Mail Security
Instant Messaging Security Issues, William 
Hugh Murray
x
2.4 Secure Voice Communications
Voice Security, Chris Hare
x
Secure Voice Communications, Valene 
Skerpac
x
2.5 Network Attacks and Countermeasures
Deep Packet Inspection Technologies, 
Anderson Ramos
x
(continued)

500 ‚óæ Comprehensive Table of Contents
Domain 2 (continued) Telecommunications and Network Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
Wireless Penetration Testing: Case Study 
and Countermeasures, Christopher 
Pilewski
x
Auditing the Telephony System: Defenses 
against Communications Security Breaches 
and Toll Fraud, William A. Yarberry, Jr.
x
Insecurity by Proxy, Micah Silverman
x
Wireless Security, Charles R. Hudson and 
Chris R. Cunningham
x
Packet Sniffers: Use and Misuse, Steve A. 
Rodgers
x
ISPs and Denial-of-Service Attacks, K. 
Narayanaswamy
x
The Ocean Is Full of Phish, Todd Fitzgerald
x
Botnets, Robert M. Slade
x
Antispam: Bayesian Filtering, Georges J. 
Jahchan
x
Domain 3 Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
3.1 Security Management Concepts and Principles
Bits to Bytes to Boardroom, Micki Krause
x
Information Security Governance, Todd 
Fitzgerald
x
Corporate Governance, David Krehnke
x
IT Governance Institute (ITGI) Overview, 
Molly Krehnke
x
Top Management Support Essential for 
Effective Information Security, Kenneth J. 
Knapp and Thomas E. Marshall
x
Managing Security by the Standards: An 
Overview and Primer, Bonnie A. Goins
x
Information Security for Mergers and 
Acquisitions, Craig A. Schiller
x

Comprehensive Table of Contents ‚óæ 501
Domain 3 (continued) Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
Information Security Governance, Ralph 
Spencer Poore
x
Belts and Suspenders: Diversity in 
Information Technology Security, Jeffrey 
Davis
x
Building Management Commitment 
through Security Councils, Todd Fitzgerald
x
Validating Your Business Partners, Jeff 
Misrahi
x
Measuring ROI on Security, Carl F. Endorf
x
The Human Side of Information Security, 
Kevin Henry
x
Integrated Threat Management, George G. 
McBride
x
Understanding Information Security 
Management Systems, Tom Carlson
x
Security Management, Ken Buszta
x
It Is All about Control, Chris Hare
x
Collaborating Information Security and 
Privacy to Create Effective Awareness and 
Training, Rebecca Herold
x
Security Information and Event 
Management (SIEM) Technology, E. Eugene 
Schultz
x
Managing Mobile Device Security, E. Eugene 
Schultz and Gal Shpantzer
x
Establishing an Information Security 
Program for Local Government, Robert 
Pittman
x
3.2 Change Control Management
Patch Management 101: It Just Makes Good 
Sense! Lynda McGhie
x
Security Patch Management Process, Felicia 
M. Nicastro
x
(continued)

502 ‚óæ Comprehensive Table of Contents
Domain 3 (continued) Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
Confi guration Management: Charting the 
Course for the Organization, Mollie E. 
Krehnke and David C. Krehnke
x
3.3 Data Classifi cation
Understanding Information Risk 
Management, Tom Carlson and Nick 
Halvorson
x
Information Classifi cation: A Corporate 
Implementation Guide, Jim Appleyard
x
Ownership and Custody of Data, William 
Hugh Murray
x
Developing and Conducting a Security Test 
and Evaluation, Sean M. Price
x
Enterprise Security Management, George 
McBride
x
A Matter of Trust, Ray Kaplan
x
Trust Governance in a Web Services World, 
Daniel D. Houser
x
3.4 Risk Management
The Role of Information Security in the 
Enterprise Risk Management Structure, Carl 
Jackson and Mark Carey
x
Technology Convergence and Security: A 
Simplifi ed Risk Management Model, Ken M. 
Shaurette
x
Using Quasi-Intelligence Resources to 
Protect the Enterprise, Craig A. Schiller
x
Information Risk Management: A Process 
Approach to Risk Diagnosis and Treatment, 
Nick Halvorson
x
Department-Level Transformation, R. Scott 
McCoy
x
Setting Priorities in Your Security Program, 
Derek Schatz
x
Why and How Assessment of Organization 
Culture Shapes Security Strategies, Don 
Saracco
x

Comprehensive Table of Contents ‚óæ 503
Domain 3 (continued) Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
Information Security Risk Assessment, 
Samantha Thomas Cruz
x
Risk Management and Analysis, Kevin 
Henry
x
New Trends in Information Risk 
Management, Brett Regan Young
x
Cyber-Risk Management: Technical and 
Insurance Controls for Enterprise-Level 
Security, Carol A. Siegel, Ty R. Sagalow, and 
Paul Serritella
x
A Look Ahead, Samantha Thomas
x
The Insider Threat: A View from the 
Outside, Todd Fitzgerald
x
Pod Slurping, Ben Rothke
x
The USB (Universal Security Burden) 
Nightmare: Pod-Slurping and Other High 
Storage Capacity Portable Device 
Vulnerabilities, Kenneth F. Belva
x
Diary of a Security Assessment: ‚ÄúPut That in 
Your Pipe and Smoke It!‚Äù Ken M. Shaurette
x
3.5 Policies, Standards, Procedures and Guidelines
Committee of Sponsoring Organizations 
(COSO), Mignona Cote
x
Toward Enforcing Security Policy: 
Encouraging Personal Accountability for 
Corporate Information Security Policy, 
John O. Wylder
x
The Security Policy Life Cycle: Functions 
and Responsibilities, Patrick D. Howard
x
People, Processes, and Technology: A 
Winning Combination, Felicia M. Nicastro
x
Building an Effective Privacy Program, 
Rebecca Herold
x
Establishing an E-Mail Retention Policy: 
Preventing Potential Legal Nightmares, 
Stephen Fried
x
(continued)

504 ‚óæ Comprehensive Table of Contents
Domain 3 (continued) Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
Ten Steps to Effective Web-Based Security 
Policy Development and Distribution, Todd 
Fitzgerald
x
Roles and Responsibilities of the 
Information Systems Security Offi cer, 
Carl Burney
x
Organizing for Success: Some Human 
Resources Issues in Information Security, 
Jeffrey H. Fenton and James M. Wolfe
x
Information Security Policies from the 
Ground Up, Brian Shorten
x
Policy Development, Chris Hare
x
Training Your Employees to Identify 
Potential Fraud and How to Encourage 
Them to Come Forward, Rebecca Herold
x
Planning for a Privacy Breach, Rebecca 
Herold
x
A Business Case for ISO 27001 Certifi cation, 
Tom Carlson and Robert Forbes
x
Achieving PCI DSS Compliance: A 
Compliance Review, Bonnie A. Goins and 
Christopher A. Pilewski
x
The Sarbanes‚ÄìOxley Revolution: Hero or 
Hindrance? Seth Kinnett
x
Leveraging IT Control Frameworks for 
Compliance, Todd Fitzgerald
x
Rats in the Cellar and Bats in the Attic, 
‚ÄúNot Enough Depth to My Security‚Äù 
Ken M. Shaurette
x
3.6 Security Awareness Training
Measuring Information Security and Privacy 
Training and Awareness Effectiveness, 
Rebecca Herold
x
Change That Attitude: The ABCs of a 
Persuasive Security Awareness Program, 
Sam Chun
x
Maintaining Management‚Äôs Commitment, 
William Tompkins
x

Comprehensive Table of Contents ‚óæ 505
Domain 3 (continued) Information Security and Risk Management
Title
Volume 1
Volume 2
Volume 3
Volume 4
Making Security Awareness Happen, Susan 
D. Hansche
x
Beyond Information Security Awareness 
Training: It Is Time to Change the Culture, 
Stan Stahl
x
3.7 Security Management Planning
The Outsourcing of IT: Seeing the Big 
Picture, Foster Henderson
x
Overview of an IT Corporate Security 
Organization, Jeff Davis
x
Make Security Part of Your Company‚Äôs DNA, 
Ken M. Shaurette
x
Building an Effective and Winning Security 
Team, Lynda McGhie
x
When Trust Goes beyond the Border: 
Moving Your Development Work Offshore, 
Stephen Fried
x
Maintaining Information Security during 
Downsizing, Thomas J. Bray
x
The Business Case for Information Security: 
Selling Management on the Protection of 
Vital Secrets and Products, Sanford 
Sherizen
x
How to Work with a Managed Security 
Service Provider, Laurie Hill McQuillan
x
Considerations for Outsourcing Security, 
Michael J. Corby
x
Achieving NERC Compliance: A Compliance 
Review, Bonnie Goins Pilewski and 
Christopher A. Pilewski
x
3.8 Ethics
The Ethical and Legal Concerns of Spyware, 
Janice C. Sipior, Burke T. Ward, and 
Georgina R. Roselli
x
Ethics and the Internet, Micki Krause
x
Computer Ethics, Peter S. Tippett
x
(continued)

506 ‚óæ Comprehensive Table of Contents
Domain 4 Application Development Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
4.1 Application Issues
Application Service Provider Security: 
Ensuring a Secure Relationship for the 
Client and the ASP, Stephen D. Fried
x
Stack-Based Buffer Overfl ows, Jonathan S. 
Held
x
Web Application Security, Mandy Andress
x
Security for XML and Other Metadata 
Languages, William Hugh Murray
x
XML and Information Security, Samuel C. 
McClintock
x
Application Security, Walter S. Kobus, Jr.
x
Covert Channels, Anton Chuvakin
x
Security as a Value Enhancer in Application 
Systems Development, Lowell Bruce 
McCulley
x
Open Source versus Closed Source, Ed 
Skoudis
x
A Look at Java Security, Ben Rothke
x
Neural Networks and Information 
Assurance Uses, Sean M. Price
x
Information Technology Infrastructure 
Library and Security Management Overview, 
David McPhee
x
Adaptation: A Concept for Next-Generation 
Security Application Development, Robby 
S. Fussell
x
Quantum Computing: Implications for 
Security, Robert M. Slade
x
Mashup Security, Mano Paul
x
Format String Vulnerabilities, Mano Paul
x
4.2 Databases and Data Warehousing
Refl ections on Database Integrity, William 
Hugh Murray
x

Comprehensive Table of Contents ‚óæ 507
Domain 4 (continued) Application Development Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
Digital Signatures in Relational Database 
Applications, Mike R. Prevost
x
Security and Privacy for Data Warehouses: 
Opportunity or Threat? David Bonewell, 
Karen Gibbs, and Adriaan Veldhuisen
x
4.3 Systems Development Controls
Data Loss Prevention Program, Powell 
Hamilton
x
Data Reliability: Trusted Time Stamps, Jeff 
Stapleton
x
Security in the .NET Framework, James D. 
Murray
x
Building and Assessing Security in the 
Software Development Lifecycle, George G. 
McBride
x
Avoiding Buffer Overfl ow Attacks, Sean 
Price
x
Secure Development Life Cycle, Kevin 
Henry
x
System Development Security 
Methodology, Ian Lim and Ioana V. Bazawan
x
Software Engineering Institute Capability 
Maturity Mode, Matt Nelson
x
Enterprise Security Architecture, William 
Hugh Murray
x
Certifi cation and Accreditation 
Methodology, Mollie E. Krehnke and David 
C. Krehnke
x
System Development Security 
Methodology, Ian Lim and Ioana V. Carastan
x
Methods of Auditing Applications, David C. 
Rice and Graham Bucholz
x
4.4 Malicious Code
Fast Scanning Worms, Paul A. Henry
x
(continued)

508 ‚óæ Comprehensive Table of Contents
Domain 4 (continued) Application Development Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
Organized Crime and Malware, Michael 
Pike
x
Net-Based Malware Detection: A 
Comparison with Intrusion Detection 
Models, Robert M. Slade
x
Malware and Computer Viruses, Robert M. 
Slade
x
An Introduction to Hostile Code and Its 
Control, Jay Heiser
x
A Look at Java Security, Ben Rothke
x
4.5 Methods of Attack
Hacking Methods, Georges J. Jahchan
x
Enabling Safer Deployment of Internet 
Mobile Code Technologies, Ron Moritz
x
Domain 5 Cryptography
Title
Volume 1
Volume 2
Volume 3
Volume 4
5.1 Use of Cryptography
Auditing Cryptography: Assessing System 
Security, Steve Stanek
x
Three New Models for the Application of 
Cryptography, Jay Heiser
x
5.2 Cryptographic Concepts, Methodologies, and Practices
Cryptography: A Unifying Principle in 
Compliance Programs, Ralph Spencer 
Poore
x
Cryptographic Transitions, Ralph Spencer 
Poore
x
Blind Detection of Steganographic Content 
in Digital Images Using Cellular Automata, 
Sasan Hamidi
x
An Overview of Quantum Cryptography, 
Ben Rothke
x

Comprehensive Table of Contents ‚óæ 509
Domain 5 (continued) Cryptography
Title
Volume 1
Volume 2
Volume 3
Volume 4
Elliptic Curve Cryptography: Delivering 
High-Performance Security for 
E-Commerce and Communications, 
Paul Lambert
x
Cryptographic Key Management Concepts, 
Ralph Spencer Poore
x
Message Authentication, James S. Tiller
x
Fundamentals of Cryptography and 
Encryption, Ronald A. Gove
x
Steganography: The Art of Hiding Messages, 
Mark Edmead
x
An Introduction to Cryptography, Javek 
Ikbal
x
Hash Algorithms: From Message Digests to 
Signatures, Keith Pasley
x
A Look at the Advanced Encryption 
Standard (AES), Ben Rothke
x
Message Digest, Ralph Spencer Poore
x
Quantum Computing: The Rise of the 
Machine, Robby Fussell
x
5.3 Private Key Algorithms
Principles and Applications of 
Cryptographic Key Management, William 
Hugh Murray
x
5.4 Public Key Infrastructure (PKI)
Preserving Public Key Hierarchy, Geoffrey C. 
Grabow
x
PKI Registration, Alex Golod
x
Encryption Key Management in Large-Scale 
Network Deployments, Franjo Majstor and 
Guy Vancollie
x
5.5 System Architecture for Implementing Cryptographic Functions
Implementing Kerberos in Distributed 
Systems, Joe Kovara and Ray Kaplan
x
(continued)

510 ‚óæ Comprehensive Table of Contents
Domain 5 (continued) Cryptography
Title
Volume 1
Volume 2
Volume 3
Volume 4
5.6 Methods of Attack
Methods of Attacking and Defending 
Cryptosystems, Joost Houwen
x
Domain 6 Security Architecture and Design
Title
Volume 1
Volume 2
Volume 3
Volume 4
6.1 Principles of Computer and Network Organizations, Architectures, and Designs
Enterprise Assurance: A Framework 
Explored, Bonnie A. Goins
x
Creating a Secure Architecture, Christopher 
A. Pilewski and Bonnie A. Goins
x
Common Models for Architecting an 
Enterprise Security Capability, Matthew J. 
Decker
x
The Reality of Virtual Computing, Chris 
Hare
x
Service-Oriented Architecture and Web 
Services Security, Glenn J. Cater
x
Analysis of Covert Channels, Ralph Spencer 
Poore
x
Security Architecture of Biological Cells: An 
Example of Defense in Depth, Kenneth J. 
Knapp and R. Franklin Morris, Jr.
x
ISO Standards Draft Content, Scott Erkonen
x
Security Frameworks, Robert M. Slade
x
Information Flow and Covert Channels, 
Sean Price
x
Securing Data at Rest: From Smartphones to 
Tapes Defi ning Data at Rest, Sam Chun and 
Leo Kahng
x
Best Practices in Virtualization Security, 
Shanit Gupta
x
Everything New Is Old Again, Robert M. 
Slade
x

Comprehensive Table of Contents ‚óæ 511
Domain 6 (continued) Security Architecture and Design
Title
Volume 1
Volume 2
Volume 3
Volume 4
6.2 Principles of Security Models, Architectures and Evaluation Criteria
Formulating an Enterprise Information 
Security Architecture, Mollie E. Krehnke and 
David C. Krehnke
x
Security Architecture and Models, Foster J. 
Henderson and Kellina M. Craig-
Henderson
x
The Common Criteria for IT Security 
Evaluation, Debra S. Herrmann
x
6.3 Common Flaws and Security Issues: System Architecture and Design
Common System Design Flaws and Security 
Issues, William Hugh Murray
x
Domain 7 Operations Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
7.1 Concepts
Security Considerations in Distributed 
Computing: A Grid Security Overview, 
Sasan Hamidi
x
Managing Unmanaged Systems, Bill 
Stackpole and Man Nguyen
x
Storage Area Networks Security Protocols 
and Mechanisms, Franjo Majstor
x
Operations: The Center of Support and 
Control, Kevin Henry
x
Why Today‚Äôs Security Technologies Are So 
Inadequate: History, Implications, and 
New Approaches, Steven Hofmeyr
x
Operations Security and Controls, Patricia 
A.P. Fisher
x
7.2 Resource Protection Requirements
The Nebulous Zero Day, Rob Slade
x
(continued)

512 ‚óæ Comprehensive Table of Contents
Domain 7 (continued) Operations Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
Understanding Service Level Agreements, 
Gilbert Held
x
Physical Access Control, Dan M. Bowers
x
7.3 Auditing
Auditing the Electronic Commerce 
Environment, Chris Hare
x
7.4 Intrusion Detection
Improving Network-Level Security through 
Real-Time Monitoring and Intrusion 
Detection, Chris Hare
x
Intelligent Intrusion Analysis: How Thinking 
Machines Can Recognize Computer 
Intrusions, Bryan D. Fish
x
7.5 Operations Controls
Directory Security, Ken Buszta
x
Patch Management 101: It Just Makes Good 
Sense! Lynda McGhie
x
Security Patch Management: The Process, 
Felicia M. Nicastro
x
Validating Tape Backups, Sandy Bacik
x
A Brief Summary of Warfare and 
Commercial Entities, Rob Shein
x
Information Destruction Requirements and 
Techniques, Ben Rothke
x
Domain 8 Business Continuity and Disaster Recovery Planning
Title
Volume 1
Volume 2
Volume 3
Volume 4
Section 8.1 Business Continuity Planning
Developing Realistic Continuity Planning 
Process Metrics, Carl B. Jackson
x

Comprehensive Table of Contents ‚óæ 513
Domain 8 (continued) Business Continuity and Disaster Recovery Planning
Title
Volume 1
Volume 2
Volume 3
Volume 4
Building Maintenance Processes for 
Business Continuity Plans, Ken Doughty
x
Identifying Critical Business Functions, 
Bonnie A. Goins
x
Selecting the Right Business Continuity 
Strategy, Ken Doughty
x
Contingency Planning Best Practices and 
Program Maturity, Timothy R. Stacey
x
Reengineering the Business Continuity 
Planning Process, Carl B. Jackson
x
The Role of Continuity Planning in the 
Enterprise Risk Management Structure, 
Carl Jackson
x
Determining Business Unit Priorities in 
Business Continuity Management, Kevin 
Henry
x
Continuity Program Testing, Maintenance, 
Training and Awareness, Carl Jackson
x
Integrated Business Continuity Planning, 
James C. Murphy
x
CERT/BERT: Community and Business 
Emergency Response, Carl B. Jackson
x
Section 8.2 Disaster Recovery Planning
Contingency at a Glance, Ken M. Shaurette 
and Thomas J. Schleppenbach
x
The Business Impact Assessment Process 
and the Importance of Using Business 
Process Mapping, Carl Jackson
x
Testing Business Continuity and Disaster 
Recovery Plans, James S. Mitts
x
Restoration Component of Business 
Continuity Planning, John Dorf and Martin 
Johnson
x
Business Resumption Planning and Disaster 
Recovery: A Case History, Kevin Henry
x
(continued)

514 ‚óæ Comprehensive Table of Contents
Domain 8 (continued) Business Continuity and Disaster Recovery Planning
Title
Volume 1
Volume 2
Volume 3
Volume 4
Business Continuity Planning: A 
Collaborative Approach, Kevin Henry
x
Section 8.3 Elements of Business Continuity Planning
The Business Impact Assessment Process, 
Carl B. Jackson
x
Domain 9 Legal, Regulations, Compliance and Investigations
Title
Volume 1
Volume 2
Volume 3
Volume 4
Section 9.1 Information Law
Sarbanes‚ÄìOxley Compliance: A Technology 
Practitioner‚Äôs Guide, Bonnie A. Goins
x
Health Insurance Portability and 
Accountability Act Security Rule, Lynda L. 
McGhie
x
Jurisdictional Issues in Global 
Transmissions, Ralph Spencer Poore
x
An Emerging Information Security 
Minimum Standard of Due Care, Robert 
Braun and Stan Stahl
x
ISPs and Accountability, Lee Imrey
x
The Case for Privacy, Michael J. Corby
x
Liability for Lax Computer Security in DDoS 
Attacks, Dorsey Morrow
x
Compliance Assurance: Taming the Beast, 
Todd Fitzgerald
x
Section 9.2 Investigations
Operational Forensics, Michael J. Corby
x
Computer Crime Investigation and 
Computer Forensics, Thomas Welch
x
What Happened? Kelly J. Kuchta
x
Section 9.3 Major Categories of Computer Crime
Potential Cyber Terrorist Attacks, Chris 
Hare
x

Comprehensive Table of Contents ‚óæ 515
Domain 9 (continued) Legal, Regulations, Compliance and Investigations
Title
Volume 1
Volume 2
Volume 3
Volume 4
The Evolution of the Sploit, Ed Skoudis
x
Computer Crime, Christopher A. Pilewski
x
Phishing: A New Twist to an Old Game, 
Stephen D.Fried
x
It‚Äôs All about Power: Information Warfare 
Tactics by Terrorists, Activists, and 
Miscreants, Gerald L. Kovacich, Andy Jones, 
and Perry G. Luzwick
x
Bluesnarfi ng, Mano Paul
x
Cyberstalking, Micki Krause Nozaki
x
Section 9.4 Incident Handling
Social Engineering: The Human Factor in 
Information Assurance, Marcus K. Rogers
x
Privacy Breach Incident Response, Rebecca 
Herold
x
Security Event Management, Glenn Cater
x
DCSA: A Practical Approach to Digital Crime 
Scene Analysis, Marcus K. Rogers
x
What a Computer Security Professional 
Needs to Know about E-Discovery and 
Digital Forensics, Larry R. Leibrock
x
How to Begin a Non-Liturgical Forensic 
Examination, Carol Stucki
x
Honeypot Essentials, Anton Chuvakin
x
Managing the Response to a Computer 
Security Incident, Michael Vangelos
x
Cyber-Crime: Response, Investigation, and 
Prosecution, Thomas Akin
x
Enterprise Incident Response and Digital 
Evidence Management and Handling, 
Marcus K. Rogers
x
Security Information Management Myths 
and Facts, Sasan Hamidi
x
(continued)

516 ‚óæ Comprehensive Table of Contents
Domain 9 (continued) Legal, Regulations, Compliance and Investigations
Title
Volume 1
Volume 2
Volume 3
Volume 4
Virtualization and Digital Investigations, 
Marcus K. Rogers and Sean C. Leshney
x
Is Software Write Blocking a Viable 
Alternative to Hardware Write Blocking in 
Computer Forensics? Paul A. Henry
x
Domain 10 Physical (Environmental) Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
10.1 Elements of Physical Security
Perimeter Security, R. Scott McCoy
x
Melding Physical Security and Traditional 
Information Systems Security, Kevin Henry
x
Physical Security for Mission-Critical 
Facilities and Data Centers, Gerald Bowman
x
Physical Security: A Foundation for 
Information Security, Christopher Steinke
x
Physical Security: Controlled Access and 
Layered Defense, Bruce R. Matthews
x
Computing Facility Physical Security, Alan 
Brusewitz
x
Closed-Circuit Television and Video 
Surveillance, David Litzau
x
Mantraps and Turnstiles, R. Scott McCoy
x
Halon Fire Suppression Systems, Chris Hare
x
Crime Prevention through Environmental 
Design, Mollie Krehnke
x
Data Center Site Selection and Facility 
Design Considerations, Sandy Bacik
x
Protection of Sensitive Data, Sandy Bacik
x
Water Leakage and Flooding, Sandy Bacik
x
Site Selection and Facility Design 
Considerations, Sandy Bacik
x
An Overview of IP-Based Video 
Surveillance, Leo Kahng
x

Comprehensive Table of Contents ‚óæ 517
Domain 10 (continued) Physical (Environmental) Security
Title
Volume 1
Volume 2
Volume 3
Volume 4
10.2 Technical Controls
Types of Information Security Controls, 
Harold F. Tipton
x
10.3 Environment and Life Safety
Workplace Violence: Event Characteristics 
and Prevention, George Richards
x
Physical Security: The Threat after 
September 11, 2001, Jaymes Williams
x




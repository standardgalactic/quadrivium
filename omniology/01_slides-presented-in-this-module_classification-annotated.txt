Machine Learning Specialization 
Classiﬁcation:  
Analyzing Sentiment 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Predicting sentiment by topic:  
An intelligent restaurant  
review system 
2 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
3	  
It’s a big day & I want to book a table at  
a nice Japanese restaurant 
©2015 Emily Fox & Carlos Guestrin 
Seattle has many 
★★★★  
sushi restaurants 
What are people 
saying about  
the food?  
the ambiance?...  

Machine Learning Specialization 
4	  
Positive reviews not positive about everything 
©2015 Emily Fox & Carlos Guestrin 
Sample review: 
Watching the chefs create 
incredible edible art made  
the experience very unique.  
My wife tried their ramen  
and it was pretty forgettable.  
All the sushi was delicious!   
Easily best sushi in Seattle. 
Experience 
 
 

Machine Learning Specialization 
5	  
From reviews to topic sentiments 
©2015 Emily Fox & Carlos Guestrin 
Experience  
★★★★	  
Ramen 
★★★	  
Sushi 
★★★★★	  
Novel intelligent  
restaurant review app 
Easily best sushi  
in Seattle. 
All reviews  
for restaurant 

Machine Learning Specialization 
6	  
Intelligent restaurant review system 
©2015 Emily Fox & Carlos Guestrin 
All reviews  
for restaurant 
 
 
 
 
The seaweed salad was just OK, 
vegetable salad was just ordinary. 
I like the interior decoration and 
the blackboard menu on the wall.  
My wife tried their ramen and  
it was pretty forgettable.  
The service is somewhat hectic. 
Easily best sushi in Seattle. 
All the sushi was delicious. 
The sushi was amazing, and  
the rice is just outstanding. 
Break all reviews  
into sentences 

Machine Learning Specialization 
7	  
Core building block  
 
©2015 Emily Fox & Carlos Guestrin 
Easily best sushi in Seattle. 
Sentence Sentiment 
Classiﬁer 
Easily best sushi in Seattle. 

Machine Learning Specialization 
8	  
Intelligent restaurant review system 
©2015 Emily Fox & Carlos Guestrin 
All reviews  
for restaurant 
 
 
 
 
The seaweed salad was just OK, 
vegetable salad was just ordinary. 
I like the interior decoration and 
the blackboard menu on the wall.  
My wife tried their ramen and  
it was pretty forgettable.  
The service is somewhat hectic. 
Easily best sushi in Seattle. 
All the sushi was delicious. 
The sushi was amazing, and  
the rice is just outstanding. 
Break all reviews  
into sentences 
Easily best sushi in Seattle. 
All the sushi was delicious. 
The sushi was amazing, and  
the rice is just outstanding. 
 
 
 
 
Select sentences 
about “sushi” 
 
 
 
 
Sentence  
Sentiment 
Classiﬁer 
 
 
 
 
Sushi 
★★★★★	  
Average 
predictions 
Easily best 
sushi  
in Seattle. 
Most  
& 

Machine Learning Specialization 
Classiﬁer applications 
9 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
10	  
Classiﬁer 
©2015 Emily Fox & Carlos Guestrin 
Sentence  
from  
review 
Classiﬁer 
MODEL 
Input:  x 
Output:  y  
Predicted  
class 

Machine Learning Specialization 
11	  
Example multiclass classiﬁer 
Output y has more than 2 categories 
©2015 Emily Fox & Carlos Guestrin 
Education 
Finance 
Technology 
Input:  x 
Webpage 
Output:  y  

Machine Learning Specialization 
12	  
Spam ﬁltering  
Input:  x 
Output:  y  
Not spam 
Spam 
©2015 Emily Fox & Carlos Guestrin 
Text of email, 
sender, IP,… 

Machine Learning Specialization 
13	  
Image classiﬁcation 
©2015 Emily Fox & Carlos Guestrin 
Input:  x 
Image pixels 
Output:  y 
Predicted object 

Machine Learning Specialization 
14	  
Personalized medical diagnosis 
©2015 Emily Fox & Carlos Guestrin 
Disease 
Classiﬁer 
MODEL 
Input:  x 
Healthy 
Flu 
… 
Cold 
Pneumonia 
Output:  y 

Machine Learning Specialization 
Reading your mind 
15 
©2015 Emily Fox & Carlos Guestrin 
“Hammer” 
“House” 

Machine Learning Specialization 
Linear classiﬁers 
16 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
17	  
Representing classiﬁers 
©2015 Emily Fox & Carlos Guestrin 
Sentence  
from  
review 
Classiﬁer 
MODEL 
Input:  x 
Output:  y  
Predicted class 
How does it work??? 

Machine Learning Specialization 
18	  
 
 
Count positive & negative words 
in sentence  
 
If number of positive words >  
  number of negative words: 
  ŷ =  
Else: 
  ŷ =  
List of positive 
words 
List of negative 
words 
great, awesome, 
good, amazing,… 
bad, terrible, 
disgusting, sucks,… 
©2015 Emily Fox & Carlos Guestrin 
Sentence  
from  
review 
Input:  x 
Simple threshold classiﬁer  

Machine Learning Specialization 
19	  
 
 
Count positive & negative words 
in sentence  
 
If number of positive words >  
  number of negative words: 
  ŷ =  
Else: 
  ŷ =  
List of positive 
words 
List of negative 
words 
great, awesome, 
good, amazing,… 
bad, terrible, 
disgusting, sucks,… 
©2015 Emily Fox & Carlos Guestrin 
Sushi was 
great, the 
food was 
awesome, 
but the 
service was 
terrible.  
Simple threshold classiﬁer  
2 
1 

Machine Learning Specialization 
20	  
Problems with threshold classiﬁer 
•  How do we get list of  
positive/negative words? 
•  Words have diﬀerent  
degrees of sentiment: 
- Great > good 
- How do we weigh  
diﬀerent words? 
•  Single words are not enough: 
- Good è Positive 
- Not good è Negative 
©2015 Emily Fox & Carlos Guestrin 
Addressed 
by learning 
a classiﬁer  
Addressed 
by more 
elaborate 
features 

Machine Learning Specialization 
21	  
A (linear) classiﬁer 
•  Will use training data to learn a weight  
for each word  
©2015 Emily Fox & Carlos Guestrin 
Word 
Weight 
good 
  1.0 
great 
  1.5 
awesome 
  2.7 
bad 
-1.0 
terrible 
-2.1 
awful 
-3.3 
restaurant, the, we, where, … 
  0.0 
… 
 … 

Machine Learning Specialization 
22	  
Scoring a sentence 
©2015 Emily Fox & Carlos Guestrin 
Word 
Weight 
good 
  1.0 
great 
  1.2 
awesome 
  1.7 
bad 
-1.0 
terrible 
-2.1 
awful  
-3.3 
restaurant, the,  
we, where, … 
  0.0 
… 
 … 
Input x: 
Sushi was great,  
the food was awesome,  
but the service was terrible.  
Called a linear classiﬁer, because output is weighted sum of input. 

Machine Learning Specialization 
23	  
 
 
Score(x) = weighted count of 
  
  
  words in sentence 
 
If Score (x) > 0: 
  ŷ =  
Else: 
  ŷ =  
Word 
Weight 
… 
… 
©2015 Emily Fox & Carlos Guestrin 
Sentence  
from  
review 
Input:  x 
Simple linear classiﬁer  

Machine Learning Specialization 
Decision boundaries 
24 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
25	  
Suppose only two words had non-zero weight 
©2015 Emily Fox & Carlos Guestrin 
Word 
Weight 
awesome 
  1.0 
awful  
-1.5 
awesome 
0 
1 
2 
3 
4 
… 
awful 
0 
1 
2 
3 
4 
… 
Sushi was awesome,  
the food was awesome,  
but the service was awful.  
Score(x) = 1.0 #awesome – 1.5 #awful 

Machine Learning Specialization 
26	  
Decision boundary example 
©2015 Emily Fox & Carlos Guestrin 
Word 
Weight 
awesome 
  1.0 
awful  
-1.5 
awesome 
awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Score(x) = 1.0 #awesome – 1.5 #awful 
Score(x) > 0 
Score(x) < 0 

Machine Learning Specialization 
27	  
Decision boundary separates 
positive & negative predictions 
•  For linear classiﬁers: 
- When 2 weights are non-zero  
è line 
- When 3 weights are non-zero  
è plane 
- When many weights are non-zero 
è hyperplane  
•  For more general classiﬁers  
è more complicated shapes 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Training and evaluating  
a classiﬁer  
28 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
29	  
Training a classiﬁer = Learning the weights 
©2015 Emily Fox & Carlos Guestrin 
Data 
(x,y) 
(Sentence1,     ) 
(Sentence2,     ) 
… 
Training 
set 
Test  
set 
Learn 
classiﬁer 
Evaluate?  
Word 
Weight 
good 
  1.0 
awesome 
  1.7 
bad 
-1.0 
awful  
-3.3 
… 
 … 

Machine Learning Specialization 
30	  
Test example 
 
 
Classiﬁcation error 
©2015 Emily Fox & Carlos Guestrin 
(Sushi was great,      ) 
Learned classiﬁer 
Hide label 
Correct 
Mistakes 
Sushi was great	  
ŷ = 
Correct! 
0 
0 
1 
1 
(Food was OK,      ) 
Food was OK	  
Mistake! 

Machine Learning Specialization 
31	  
Classiﬁcation error & accuracy 
•  Error measures fraction of mistakes 
- Best possible value is 0.0  
•  Often, measure accuracy 
- Fraction of correct predictions 
- Best possible value is 1.0 
©2015 Emily Fox & Carlos Guestrin 
error =                            . 
 
accuracy=                            . 
 

Machine Learning Specialization 
What’s a good accuracy? 
32 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
33	  
What if you ignore the sentence, and just guess? 
•  For binary classiﬁcation: 
- Half the time, you’ll get it right! (on average) 
 è accuracy = 0.5 
•  For k classes, accuracy = 1/k 
- 0.333 for 3 classes, 0.25 for 4 classes,… 
 
©2015 Emily Fox & Carlos Guestrin 
At the very, very, very least,  
you should healthily beat random…  
Otherwise, it’s (usually) pointless… 

Machine Learning Specialization 
34	  
•  One class is more common than others 
•  Beats random (if you know the majority class) 
Is a classiﬁer with 90% accuracy good? Depends… 
2010 data shows:  
“90% emails sent are spam!” 
Predicting every email is spam 
gets you 90% accuracy!!! 
Majority class prediction  
Amazing performance when 
there is class imbalance  
(but silly approach) 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
35	  
So, always be digging in and asking the  
hard questions about reported accuracies 
•  Is there class imbalance? 
•  How does it compare to a simple,  
baseline approach? 
- Random guessing 
- Majority class 
- … 
•  Most importantly:  
what accuracy does my application need? 
- What is good enough for my user’s experience? 
- What is the impact of the mistakes we make? 
 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
False positives, false negatives,  
and confusion matrices 
36 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
37	  
Types of mistakes 
©2015 Emily Fox & Carlos Guestrin 
True 
Positive 
False 
Positive 
(FP) 
False 
Negative 
(FN) 
True 
Negative 
True label 
Predicted label 
True 
Positive 
False 
Negative 
(FN) 
False 
Positive 
(FP) 
True 
Negative 

Machine Learning Specialization 
38	  
Cost of diﬀerent types of mistakes can be 
diﬀerent (& high) in some applications 
©2015 Emily Fox & Carlos Guestrin 
Spam 
ﬁltering 
Medical 
diagnosis 
False 
negative 
False 
positive 
Annoying 
Email lost 
Disease  
not treated 
Wasteful 
treatment 

Machine Learning Specialization 
39	  
Confusion matrix –  
  binary classiﬁcation 
©2015 Emily Fox & Carlos Guestrin 
True 
Positive 
False 
Positive 
(FP) 
False 
Negative 
(FN) 
True 
Negative 
True label 
Predicted label 

Machine Learning Specialization 
40	  
Confusion matrix –  
  multiclass classiﬁcation 
©2015 Emily Fox & Carlos Guestrin 
Healthy 
Cold 
Flu 
Healthy 
Cold 
Flu 
True label 
Predicted label 

Machine Learning Specialization 
Learning curves: 
How much data do I need?  
 
41 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
42	  
How much data does a model need to learn? 
•  The more the merrier J  
- But data quality is most important factor 
•  Theoretical techniques sometimes  
can bound how much data is needed  
- Typically too loose for practical application 
- But provide guidance 
•  In practice: 
- More complex models require more data 
- Empirical analysis can provide guidance 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
43	  
Learning curves 
©2015 Emily Fox & Carlos Guestrin 
Amount of training data 
Test error 

Machine Learning Specialization 
44	  
Is there a limit?   
Yes, for most models… 
©2015 Emily Fox & Carlos Guestrin 
Amount of training data 
Test error 
Bias of model 

Machine Learning Specialization 
45	  
More complex models tend to have less bias… 
Sentiment classiﬁer using  
single words can do OK, but…  
Never classiﬁes correctly:  
“The sushi was not good.” 
More complex model:  
consider pairs of words (bigrams) 
©2015 Emily Fox & Carlos Guestrin 
Word 
Weight 
good 
+1.5 
not good -2.1 
Less bias è  
potentially more accurate,  
needs more data to learn 

Machine Learning Specialization 
46	  
Models with less bias tend to  
need more data to learn well,  
but do better with suﬃcient data 
©2015 Emily Fox & Carlos Guestrin 
Amount of training data 
Test error 
Classiﬁer based  
on single words 

Machine Learning Specialization 
Class probabilities 
47 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
48	  
How conﬁdent is your prediction? 
•  Thus far, we’ve outputted a prediction 
•  But, how sure are you about the prediction? 
- “The sushi & everything  
 else were awesome!” 
- “The sushi was good,  
 the service was OK.”  
©2015 Emily Fox & Carlos Guestrin 
Deﬁnite  
Not sure 
Many classiﬁers provide a conﬁdence level: 
 
   P(y|x) 
 
 Extremely useful in practice 
Output label 
Input sentence 
P(y=+|x) = 0.99  
P(y=+|x) = 0.55  

Machine Learning Specialization 
Summary of classiﬁcation 
49 
©2015 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
50	  
What you can do now… 
•  Identify a classiﬁcation problem and 
some common applications 
•  Describe decision boundaries and linear 
classiﬁers 
•  Train a classiﬁer  
•  Measure its error 
- Some rules of thumb for good accuracy 
•  Interpret the types of error associated 
with classiﬁcation 
•  Describe the tradeoﬀs between model 
bias and data set size 
•  Use class probability to express degree 
of conﬁdence in prediction  
©2015 Emily Fox & Carlos Guestrin 


Canadian
Mathematical Society
Soci√©t√© math√©matique
du Canada
CMS/CAIMS Books in Mathematics
Alexander Melnikov
A Course 
of Stochastic 
Analysis

CMS/CAIMS Books in Mathematics
Volume 6
Series Editors
Karl Dilcher
Department of Mathematics and Statistics, Dalhousie University, Halifax,
NS, Canada
Frithjof Lutscher
Department of Mathematics, University of Ottawa, Ottawa, ON, Canada
Nilima Nigam
Department of Mathematics, Simon Fraser University, Burnaby, BC, Canada
Keith Taylor
Department of Mathematics and Statistics, Dalhousie University, Halifax,
NS, Canada
Associate Editors
Ben Adcock
Department of Mathematics, Simon Fraser University, Burnaby, BC, Canada
Martin Barlow
University of British Columbia, Vancouver, BC, Canada
Heinz H. Bauschke
University of British Columbia, Kelowna, BC, Canada
Matt Davison
Department of Statistical and Actuarial Science, Western University, London,
ON, Canada
Leah Keshet
Department of Mathematics, University of British Columbia, Vancouver,
BC, Canada
Niky Kamran
Department of Mathematics and Statistics, McGill University, Montreal,
QC, Canada
Mikhail Kotchetov
Memorial University of Newfoundland, St. John‚Äôs, Canada
Raymond J. Spiteri
Department of Computer Science, University of Saskatchewan, Saskatoon,
SK, Canada

CMS/CAIMS Books in Mathematics is a collection of monographs and graduate-
level textbooks published in cooperation jointly with the Canadian Mathematical
Society-Societ√© math√©matique du Canada and the Canadian Applied and Industrial
Mathematics Society-Societ√© Canadienne de Math√©matiques Appliqu√©es et Indus-
trielles. This series offers authors the joint advantage of publishing with two major
mathematical societies and with a leading academic publishing company. The se-
ries is edited by Karl Dilcher, Frithjof Lutscher, Nilima Nigam, and Keith Taylor.
The series publishes high-impact works across the breadth of mathematics and its
applications. Books in this series will appeal to all mathematicians, students and
established researchers. The series replaces the CMS Books in Mathematics series
that successfully published over 45 volumes in 20 years.

Alexander Melnikov
A Course of Stochastic
Analysis
123

Alexander Melnikov
Department of Mathematical
and Statistical Sciences
University of Alberta
Edmonton, AB, Canada
ISSN 2730-650X
ISSN 2730-6518
(electronic)
CMS/CAIMS Books in Mathematics
ISBN 978-3-031-25325-6
ISBN 978-3-031-25326-3
(eBook)
https://doi.org/10.1007/978-3-031-25326-3
Mathematics Subject ClassiÔ¨Åcation: 60, 62
¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciÔ¨Åcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microÔ¨Ålms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciÔ¨Åc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afÔ¨Åliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The foundation of the modern probability theory was done by A. N. Kolmogorov in
his monograph appeared as ‚ÄúGrundbegriffe der Wahrscheinlichkeitsrechtung‚Äù in
1933. Since that time we use the notion of a probability space (X; F ; P), where X is
an abstract set of elementary outcomes x of a random experiment, F is a family of
events, and P is a probability measure deÔ¨Åned on F . In that book, there is a special
remark about the property of independence as a speciÔ¨Åc feature of the theory of
probability. Due to this property, one can use deterministic numerical characteris-
tics like mean and variance to describe the behavior of families of random variables.
It was later recognized that dynamics of random events can be exhaustively
determined considering t and x together and a random process as a function
of these two variables on the basis of information Ô¨Çow Ft, t  0.
The fruitful idea was extremely important for a general theory of random pro-
cesses initiated in the middle of the twentieth century. The central notion of this
theory is a stochastic basis (X; F ; √∞Ft√û; P), i.e. a probability space equipped with an
information Ô¨Çow or Ô¨Åltration (Ft). In such a setting, deterministic numerical char-
acteristics induced by the independence property are replaced by their conditional
versions with respect to Ô¨Åltration (Ft). So, a predictability has appeared as a driver
of extension of stochastic calculus to the biggest possible class of processes called
semimartingales. These processes admit the full description in predictable terms.
Moreover, they unify processes with discrete and continuous time. As a result, we
arrive to a nice transformation of the theory of probability and stochastic processes
to a wider area which is called stochastic analysis.
The primary goal of the book is to deliver basic notions, facts, and methods of
stochastic analysis using a uniÔ¨Åed methodology, sufÔ¨Åciently strong and complete,
and giving interesting and valuable implementations in mathematical Ô¨Ånance and
statistics of random processes. There are a lot of examples considered to illustrate
theoretical concepts discussed in line with problems for students aligned with
material. Moreover, the list of supplementary problems with hints and solutions,
covering both important theoretical statements and purely technical problems
intended to motivate a deeper understanding of stochastic analysis is provided at the
end of the book. The book can be considered as a textbook for both senior
v

undergraduate and graduate courses on stochastic analysis/stochastic processes. It
certainly can be helpful for undergraduate and graduate students, instructors as well
as for experts in stochastic analysis and its applications.
The book is based on the lecturers given by the author in different times at
Lomonosov
Moscow
State
University,
State
University-Higher
School
of
Economics, University of Copenhagen, and at the University of Alberta. The author
is grateful to his Ph.D. students Ilia Vasilev, Andrey Pak, and Pounch Mohammadi
Nejad at the Mathematical Finance Program of the University of Alberta for their
kind help preparation of this book.
Edmonton, Canada
Alexander Melnikov
vi
Preface

Contents
1
Probabilistic Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Classical theory and the Kolmogorov axiomatics . . . . . . . . . . .
1
1.2
Probabilistic distributions and the Kolmogorov consistency
theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2
Random variables and their quantitative characteristics . . . . . . . . .
13
2.1
Distributions of random variables . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Expectations of random variables . . . . . . . . . . . . . . . . . . . . . . .
18
3
Expectations and convergence of sequences of random
variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.1
Limit behavior of sequences of random variables
in terms of their expected values . . . . . . . . . . . . . . . . . . . . . . .
21
3.2
Probabilistic inequalities and interconnections between
types of convergence of random variables. . . . . . . . . . . . . . . . .
26
4
Weak convergence of sequences of random variables . . . . . . . . . . .
31
4.1
Weak convergence and its description in terms
of distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.2
Weak convergence and Central Limit Theorem. . . . . . . . . . . . .
34
5
Absolute continuity of probability measures and conditional
expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5.1
Absolute continuity of measures and the Radon-Nikodym
theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5.2
Conditional expectations and their properties . . . . . . . . . . . . . .
43
6
Discrete time stochastic analysis: basic results . . . . . . . . . . . . . . . .
49
6.1
Basic notions: stochastic basis, predictability
and martingales. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
6.2
Martingales on Ô¨Ånite time interval . . . . . . . . . . . . . . . . . . . . . .
55
6.3
Martingales on inÔ¨Ånite time interval . . . . . . . . . . . . . . . . . . . . .
59
vii

7
Discrete time stochastic analysis: further results
and applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
7.1
Limiting behavior of martingales with statistical
applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
7.2
Martingales and absolute continuity of measures. Discrete
time Girsanov theorem and its Ô¨Ånancial application . . . . . . . . .
74
7.3
Asymptotic martingales and other extensions of martingales . . .
76
8
Elements of classical theory of stochastic processes . . . . . . . . . . . . .
81
8.1
Stochastic processes: deÔ¨Ånitions, properties and classical
examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
8.2
Stochastic integrals with respect to a Wiener process . . . . . . . .
91
8.3
The Ito processes: Formula of changing of variables,
theorem of Girsanov, representation of martingales . . . . . . . . . .
98
9
Stochastic differential equations, diffusion processes
and their applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
9.1
Stochastic differential equations . . . . . . . . . . . . . . . . . . . . . . . .
107
9.2
Diffusion processes and their connection with SDEs
and PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
9.3
Applications to Mathematical Finance and Statistics
of Random Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
9.4
Controlled diffusion processes and applications to option
pricing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
10
General theory of stochastic processes under
‚Äúusual conditions‚Äù . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
10.1
Basic elements of martingale theory . . . . . . . . . . . . . . . . . . . . .
139
10.2
Extension of martingale theory by localization
of stochastic processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
10.3
On stochastic calculus for semimartingales . . . . . . . . . . . . . . . .
159
10.4
The Doob-Meyer decomposition: proof and related remarks . . .
169
11
General theory of stochastic processes in applications . . . . . . . . . .
175
11.1
Stochastic mathematical Ô¨Ånance . . . . . . . . . . . . . . . . . . . . . . . .
175
11.2
Stochastic Regression Analysis . . . . . . . . . . . . . . . . . . . . . . . .
182
12
Supplementary problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
viii
Contents

Acronyms and Notation
√∞X; F; P√û
Probability space
√∞Fn√ûn¬º0;1...
Filtration
√∞X; F; √∞Fn√û; P√û
Stochastic basis
Rd
d-dimensional Euclidian space
B(Rd)
Borel r-algebra on Rd
(Rd, B(Rd))
Borel space
A √æ
Class of increasing integrable processes
A
Class of processes with integrable variation
Aloc
+
Class of increasing locally integrable processes
Aloc
Class of processes with locally integrable variation
V
Class of processes with Ô¨Ånite variation
LP
Space of random variables with Ô¨Ånite p-moment
O
Optional r-algebra
P
Predictable r-algebra
M
Set of uniformly integrable martingales
M2
Set of square integrable martingales
Mloc
Set of local martingales
Mloc
2
Set of locally square integrable martingales
EX
Expected value of random variable X
Var(X )
Variance of X
E(X|A)
Conditional expected value of random variable X with respect to
r-algebra A
Xn !
P X
Convergence in probability
Xn ! X(a.s.)
Convergence almost surely
Xn !
d X
Convergence in distribution
Xn !
Lp
X
Convergence in LP space
Xn !
w X
Weak convergence
ix

LawP(X)
Distribution of X w.r. to P
Law(X, P)
Distribution of X with respect to measure P
~P << P
Absolute continuity of measure ~P with respect to P
~P * P
Equivalence of measures ~P and P
n
k


¬º Ck
n
Number of combinations from n by k
hX; Yi
Joined quadratic characteristic of X and Y
[X, Y]
Joined quadratic bracket X and Y
¬Ω¬Ωs; r
Stochastic integral with Ô¨Ånite limits s and r
¬Ω¬Ωs
Graph of s
V+
Class of increasing processes
hX; Xi
Quadratic characteristic (compensator) of X
conv(f1, f2, ‚Ä¶)
Linear convolution of vectors f1, f2, ‚Ä¶
A [ B
Union of sets A and B
A \ B
Intersection of sets A and B
Ac
Complement of set A
P(A)
Probability of set A
e(X)
Stochastic exponent of X
P(A|A)
Conditional probability of A w.r. to r-algebra A
cov(X, Y)
Covariance of random variables X and Y
a ^ b
Min(a, b)
a _ b
Max(a, b)
a+
Max(a, 0)
a‚àí
Max(‚àía, 0)
k:kP
Norm in space LP
lX(t,a)
Local time of W at level a during [0, t]
{x : Xt !}
Set of x for which there exists a Ô¨Ånite limit limt! ‚àûXt (x)
x
Acronyms and Notation

Chapter 1
Probabilistic Foundations
Abstract In the Ô¨Årst chapter, Kolmogorov‚Äôs axioms of the theory of probability are
presented. The choice of the system of the axioms is explained in context of the
Caratheodory theorem. DiÔ¨Äerent probability spaces as well as probability distribu-
tions are introduced too. The famous Kolmogorov consistency theorem is formu-
lated, and as a consequence, a brief construction of the Wiener measure is shown
(see [1], [6], [7], [10], [15], [19], [40], and [45]).
1.1
Classical theory and the Kolmogorov axiomatics
Classical theory of probability deals with a Ô¨Ånite probability space, i.e. it creates a
mathematical model for any random (stochastic) experiment with Ô¨Ånite number of
possible outcomes (elementary events, results) œâ ‚ààŒ©, |Œ©| = N < ‚àû, where Œ© is the
whole set of outcomes.
Any other events A, B, and C are simply called events. We cannot be certain
of the event A under consideration before a random experiment. Nevertheless, we
expect to have a quantitative measure of such a possibility. In classical probability,
the problem of Ô¨Ånding this measure can be solved perfectly.
Denote œâ1, ..., œâN elementary outcomes and deÔ¨Åne p1, ..., pN corresponding pos-
sibilities for them. Any other and more complicated event A can be reconstructed
with the help of œâ1, ..., œâN. Hence, to determine a measure of possibility for event
A, one can put
P(A) =

i:œâi ‚ààA
pi.
The situation is diÔ¨Äerent if Œ© is not Ô¨Ånite. Consider the well-known random exper-
iment called InÔ¨Ånite coin trial with outcomes H(head) and T(tail), and with the
probability of success (H) p, p ‚àà(0, 1).
It is quite clear that in this case the set Œ© can be chosen as follows
Œ© = {œâ : œâ = (a1, a2, ...), ai = 0 or 1}.
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_1
1

2
1
Probabilistic Foundations
The natural question arises: How big is this set? Mathematically speaking, we should
Ô¨Ånd the power |Œ©| of this set.
It is well-known that any real number a ‚àà[0, 1) can be represented uniquely in
the form
a = a1
2 + a2
22 + ...,
this representation provides a one-to-one correspondence between [0, 1) and Œ©:
a ‚àà[0, 1) ‚áîœâ = (a1, a2, ...) ‚ààŒ©.
If we follow to the classical probability theory we should Ô¨Ånd the way to deÔ¨Åne the
probability for any elementary event œâ. It is equivalent to a random choice of a point
from the semi-interval [0, 1). Taking into account a symmetry of such trials, we must
conclude that all elementary events œâ are uniformly possible, hence,
p = p(œâ) = const = c.
Further, we have
1 = |[0, 1)| = Probability of Œ© =

œâ‚ààŒ©
p(œâ) = ‚àû
because the set Œ© is not countable. So, we must deÔ¨Åne p(œâ) = c = 0, however, this
choice is not satisfactory because, for example, for A = [0, 1/2) we have
1/2 = Probability of A = P(A) =

œâ‚ààA
p(œâ),
and we arrive to a contradiction again.
Based on these considerations, one can conclude that the approach of classical
theory of probability must be seriously transformed:
The probability should be well-deÔ¨Åned instead of elementary outcomes for a
reasonable collection of events of Œ©.
The natural candidate for this type of collection is given in the following deÔ¨Ånition.
DeÔ¨Ånition 1.1 A collection A of subsets of given set Œ© is called an algebra if the
following conditions are fulÔ¨Ålled
1. Œ© ‚ààA,
2. A, B ‚ààA ‚áíA ‚à™B ‚ààA, A ‚à©B ‚ààA,
3. A ‚ààA ‚áíAc = ¬ØA = Œ© \ A ‚ààA.
Using this deÔ¨Ånition we can realize the idea described above as follows.
DeÔ¨Ånition 1.2 A function Œº : A ‚Üí[0, ‚àû] is a Ô¨Ånite-additive measure on A, if for
any disjoint sets A, B ‚ààA :
Œº(A ‚à™B) = Œº(A) + Œº(B).

1.1
Classical theory and the Kolmogorov axiomatics
3
In case of Œº(Œ©) < ‚àûthis measure is called Ô¨Ånite. In particular case Œº(Œ©) = 1 such
a measure is called a (Ô¨Ånite-additive) probability measure. If there is a partition of
Œ© = ‚à™‚àû
n=1Œ©n, Œ©k ‚à©Œ©m = ‚àÖ, k  m, Œ©n ‚ààA such that Œº(Œ©n) < ‚àû, n = 1, 2, ..., then
Œº is œÉ-Ô¨Ånite.
Based on these deÔ¨Ånitions we arrive to the Ô¨Årst probability model that will be
called a probability space of the Ô¨Årst level:
(Œ©, A, P), where A is an algebra, and P is a probability (Ô¨Ånite-additive measure
with P(Œ©) = 1).
Assume that a Ô¨Ånite additive probability P on the probability space of the Ô¨Årst
level (Œ©, A, P) is countably additive (œÉ-additive). It means that for any sequence of
disjoint events An ‚ààA with ‚à™‚àû
n=1An ‚ààA the following equality holds:
P(‚à™‚àû
n=1An) =
‚àû

n=1
P(An).
In this case we call this measure a probability (probability measure). Let us count a
list of natural properties of probability:
1. ‚àÖ‚ààA ‚áíP(‚àÖ) = 0;
2. A, B ‚ààA ‚áíP(A ‚à™B) = P(A) + P(B) ‚àíP(A ‚à©B);
3. A, B ‚ààA, A ‚äÜB ‚áíP(A) ‚â§P(B);
4. An ‚ààA, n = 1, 2, ..., ‚à™‚àû
n=1An ‚ààA ‚áíP(‚à™‚àû
n=1An) ‚â§‚àû
n=1 P(An).
Problem 1.1 Prove the properties (1)‚Äì(4).
The idea of the proof is shown here in case of property (1): For any A ‚ààA we have
‚àÖ‚à©A ‚ààA. Further, using the Ô¨Ånite-additivity of P and equality ‚àÖ‚à™Œ© = Œ© we obtain
1 = P(‚àÖ‚à™Œ© = Œ©) = P(‚àÖ) + P(Œ©) = P(‚àÖ) + 1,
and, hence, P(‚àÖ) = 0.
The following theorem contains conditions under which a Ô¨Ånite-additive proba-
bility measure is countable-additive.
Theorem 1.1 Let P be a Ô¨Ånitely additive set function on the algebra A with P(Œ©) =
1. Then the following sentences are equivalent:
1. P is a probability (œÉ-additive);
2. P is continuous from below:
P(‚à™‚àû
n=1An) = lim
n‚Üí‚àûP(An) f or A1 ‚äÜA2 ‚äÜ..., ‚à™‚àû
n=1An ‚ààA;
3. P is continuous from above:
P(‚à©‚àû
n=1An) = lim
n‚Üí‚àûP(An) f or A1 ‚äáA2 ‚äá..., ‚à©‚àû
n=1An ‚ààA;
4. P is continuous at ‚àÖ:

4
1
Probabilistic Foundations
lim
n‚Üí‚àûP(An) = 0 f or A1 ‚äáA2 ‚äá..., ‚à©‚àû
n=1An = ‚àÖ.
Proof We only show the implication (1) ‚áí(2) because the other statements can be
proved in a similar way.
Let us represent ‚à™‚àû
n=1An = A1 ‚à™(A2 \ A1) ‚à™(A3 \ A2) ‚à™... as a union of disjoint
events. Therefore,
P(‚à™‚àû
n=1An) = P(A1) + P(A2) ‚àíP(A1) + ... = lim
n‚Üí‚àûP(An).
The Ô¨Årst probability model (Œ©, A, P) looks very constructive. But in the frame-
work of this model we cannot operate with events which are combinations of count-
able numbers of these events. To avoid the disadvantages of the probability space of
the Ô¨Årst level we introduce the following notions.
DeÔ¨Ånition 1.3 A system F of subsets of Œ© is called a œÉ-algebra (sigma-algebra)
if F is an algebra and for any sequence An ‚ààF, n = 1, 2, ..., their unions and
intersections belong to F :
An ‚ààF ‚áí‚à™‚àû
n=1An and ‚à©‚àû
n=1 An ‚ààF .
The couple (Œ©, F ) is called a measurable space. Let us investigate interconnections
of algebras and œÉ-algebras. First, we note that
F‚àó= {‚àÖ, Œ©} and F ‚àó= {A, A ‚äÜŒ©} = {all subsets of Œ©}
are both algebras and œÉ‚àíalgebras. Second, we can Ô¨Åx a set A ‚äÜŒ© and deÔ¨Åne another
algebra (and a œÉ-algebra also) FA = {A, Ac, ‚àÖ, Œ©}, which is generated by event A.
This way of construction of algebras and œÉ‚àíalgebras admits a natural generalization.
Lemma 1.1 Let E be a collection of sets in Œ©, then there are the smallest algebra
and the smallest œÉ‚àíalgebra containing E.
Idea of the proof. It is clear that E ‚äÜF ‚àó. Therefore, there exist at least one algebra
and one œÉ‚àíalgebra with the desirable property. DeÔ¨Åne Œ±(E) and œÉ(E) as a collection
of sets that belong to every algebra (correspondently, œÉ‚àíalgebra) containing E.
DeÔ¨Ånition 1.4 A collection of subsets of Œ© is called a monotonic class if for any
increasing (decreasing) sequence of events their union (intersection) belong to this
class.
Denote Œº(E) a minimal monotonic class containing E. It turns out, an algebra A
is a œÉ-algebra if and only if it is a monotonic class.
Idea of the proof. The direct implication of this statement is clear because
œÉ‚àíalgebra is a monotonic class. The inverse implication can be proved as fol-
lows. If (An)n=1,2,... is an arbitrary sequence of subsets Œ© from a monotonic class
A, we can deÔ¨Åne a monotonic class A, and a monotonic sequence (Bn)n=1,2,... with
Bn = ‚à™n
i=1Ai ‚ààA, and then Bn ‚Üë‚à™‚àû
i=1Ai ‚ààA. Using this statement we can easily
prove that Œº(A) = œÉ(A) if A is an algebra.

1.1
Classical theory and the Kolmogorov axiomatics
5
Example 1.1 (Borel space). Let us take R = R1 = (‚àí‚àû, ‚àû) as Œ©, and consider subin-
tervals
(a, b] = {x ‚ààR : a < x ‚â§b} for ‚àí‚àû‚â§a < b < ‚àû,
Let A be a system of subsets A of the form:
A =
n

i=1
(ai, bi] = ‚à™n
i=1(ai, bi], n = 1, 2, ...
Including to A the empty set ‚àÖ, we can see that A is an algebra. Moreover, we can
easily deÔ¨Åne a measure on A : l0(A) = n
i=1(bi ‚àíai). In particular,
l0((a, b]) = b ‚àía = the length of (a, b].
We note also that A is not a œÉ-algebra because An = (0, 1 ‚àí1/n] ‚ààA, but the union
‚à™‚àû
n=1An = (0, 1)  A.
Even in the framework of this example the following problem arises:
Determine a minimal œÉ-algebra B(R) containing A and a measure on the measur-
able space (R, B(R)) which is called the Borel space.
Problem 1.2 Prove that
1. B(R) is equivalent to a minimal œÉ‚àíalgebra containing all intervals (a, b];
2. B(R) contains (a, b), [a, b], {a}.
Existence of B(R) is obvious, but an extension of l0 from A to B(R) is a real math-
ematical problem, and its solution is the famous Lebesgue measure l on (R, B(R)).
To avoid real technical diÔ¨Éculties we would like to get a solution of this problem
from the fundamental theorem of Caratheodory:
Theorem 1.2 If Œ© is a space with an algebra A and with a œÉ-additive measure Œº0
on (Œ©, A), then there exists a unique measure Œº on (Œ©, œÉ(A)) such that
Œº(A) = Œº0(A) for all A ‚ààA.
Based on all these preparations and theorems 1.1 and 1.2 one can arrive to the
following natural axioms proposed by Kolmogorov for deÔ¨Ånition of the probability
space of the second level (or simply, probability space):
An ordered triple (Œ©, F, P) with Œ© as a set of elements œâ, F as a œÉ‚àíalgebra on
Œ©, P as a probability measure (probability) on F .
To go further we give some other examples of measurable spaces playing an
important role in many our further constructions and statements.
Example 1.2 (Rd, B(Rd)) is a multidimensional (d ‚â•1) Borel space.
Take a set I = d
i=1 Ii, Ii = (ai, bi], i = 1, ..., d, which is called a rectangle with
sides Ii. The smallest œÉ‚àíalgebra containing all such sets is denoted B(Rd).

6
1
Probabilistic Foundations
Instead of rectangels I, one can consider the sets B = d
i=1 Bi, Bi ‚ààB(R1), i =
1, 2, ..., d. The minimal œÉ‚àíalgebra containing all such sets is
B(Rd) = B(R1) ‚äó... ‚äóB(R1)

d
.
Problem 1.3 Using an induction method, prove the above equality.
Example 1.3 Denote
R‚àû= {x = (x1, x2, ...), ‚àí‚àû< xi < +‚àû},
C(I) = C(I1 √ó ... √ó In) = {x : x = (x1, x2, ...), x1 ‚ààI1, ..., xn ‚ààIn}.
It is clear that the cylinders C satisfy the consistency property
C(I1 √ó ... √ó In) = C(I1 √ó ... √ó In √ó R).
In this case, the system of the cylinders is an algebra. The smallest œÉ‚àíalgebra
containing all cylinders is B(R‚àû), and we arrive to a measurable space (R‚àû, B(R‚àû)).
Example 1.4 For T = [0, ‚àû) we deÔ¨Åne RT the space of all real-valued functions
x, and Ct1,...,tn(I1 √ó ... √ó In) = {x : xt1 ‚ààI1, ..., xtn ‚ààIn}. The smallest œÉ-algebra
containing all such cylinders is B(RT). So, we arrive to the measurable space
(RT, B(RT)). Let us note a structural property of this space: For any A ‚ààB(RT)
there exist t1 < t2 < . . . and a Borel set B ‚ààB(R‚àû) such that
A = {x : (xt1, xt2, . . .) ‚ààB}.
1.2
Probabilistic distributions and the Kolmogorov
consistency theorem
After these theoretical considerations and the examples given above, we can put the
problem:
How to construct a probability measure on given measurable space?
In the framework of Example 1.1 one can take a Borel set A ‚ààB(R) in the form
A = (‚àí‚àû, x], x ‚ààR1,
and deÔ¨Åne the function
F(x) = P((‚àí‚àû, x]),
where P is a probability measure on the Borel space.
We can note that the function F = F(x) satisÔ¨Åes the following obvious properties
1. F is non-decreasing,

1.2
Probabilistic distributions and the Kolmogorov consistency theorem
7
2. F(‚àí‚àû) = limx‚Üí‚àí‚àûF(x) = 0,
F(+‚àû) = limx‚Üí+‚àûF(x) = 1,
3. F is a right-continuous function with Ô¨Ånite left-limits.
Such a function, i.e. satisfying (1)-(3), is called a distribution function.
It is now clear that the problem of a construction of the probability can be solved
if we have a distribution function F. In this case we can deÔ¨Åne a Ô¨Ånite additive set
function P0 on algebra A :
P0(A) =
n

i=1
(F(bi) ‚àíF(ai)),
A =
n

i=1
(ai, bi] ‚ààA.
We can prove that P0 is œÉ‚àíadditive, and according to the Caratheodory theorem P0
admits a unique extension P to the whole space (R1, B(R1)).
In particular, we can consider the measurable space ([0, 1], B([0, 1])) and the
distribution function
F(x) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
0,
x < 0,
x,
x ‚àà[0, 1],
1,
x > 1.
Then the set function l((a, b]) = F(b) ‚àíF(a) = b ‚àía is the length of the interval
(a, b] presents the Lebesgue measure on this space.
Further, starting at the distribution function F = F(x) which is a piecewise con-
stant with change in values at points x1, x2, . . ., ŒîF(xi) > 0, we deÔ¨Åne
pi = P({xi}) = ŒîF(xi) > 0,
‚àû

i=1
P({xi}) = 1.
This is a measure concentrated at x1, x2, . . ., and usually the sequence (p1, p2, . . . )
is called a discrete distribution. Sometimes, the table
 p1 . . . pn . . .
x1 . . . xn . . .

is also called a discrete distribution. The corresponding distribution function F is
also called discrete. Below we have the most well-known discrete distributions:
1. Uniform discrete distribution:

x1 . . . xN
1/N . . . 1/N

;
2. Bernoulli distribution:
 x1 x2
p1 p2

, p1 + p2 = 1;
3. Binomial distribution with parameter p:

0 . . . i . . . n
n
i
pi(1 ‚àíp)n‚àíi

, p ‚àà(0, 1);

8
1
Probabilistic Foundations
4. Poisson distribution with parameter Œª:
 0 . . . i . . . n . . .
Œªi
i! e‚àíŒª

, Œª > 0.
If the distribution function F admits the integral representation
F(x) =
‚à´x
‚àí‚àû
f (y)dy,
where a non-negative function (density) f (y) satisÔ¨Åes the integral condition
‚à´‚àû
‚àí‚àûf (y)dy = 1, then the distribution and its distribution function are absolute con-
tinuous.
Let us count the most important absolute continuous distributions as examples:
1. Uniform distribution on [a, b] :
f (y) =
1
b ‚àía, y ‚àà[a, b];
2. Normal distribution with parameters Œº and œÉ2 (N ‚àº(Œº, œÉ2)) :
f (y) = (2œÄœÉ2)‚àí1/2exp

‚àí(y ‚àíŒº)2
2œÉ2

, Œº ‚ààR1, œÉ > 0, y ‚ààR1;
3. Gamma distribution:
f (y) = yŒ±‚àí1e‚àíy/Œ≤
Œì(Œ±)Œ≤Œ± , y ‚â•0, Œ± > 0, Œ≤ > 0;
In particular, for Œ≤ = 1/Œª, and Œ± = 1, we get an exponential distribution with
parameter
Œª ‚â•0 : f (y) = Œªe‚àíŒªy, y ‚â•0;
for Œ± = n/2, and Œ≤ = 2, we get the Chi-squared distribution with
f (y) = 2‚àín/2yn/2‚àí1e‚àíy/2/Œì(n/2), y ‚â•0, n = 1, 2, . . . ;
4. Student distribution (t-distribution):
f (y) = Œì((n + 1)/2)
(nœÄ)1/2Œì(n/2)(1 + y2/n)‚àí(n+1)/2, y ‚ààR1, n = 1, 2, . . . ;
5. Cauchy distribution with parameter Œ∏ :
f (y) =
Œ∏
œÄ(y2 + Œ∏2), y ‚ààR1, Œ∏ > 0.
Besides discrete and absolute continuous distribution functions (measures, dis-
tributions) there are distribution functions which are continuous, but the set of their
points of increasing has the Lebesgue measure zero. These types of distributions
(measures) are called singular.

1.2
Probabilistic distributions and the Kolmogorov consistency theorem
9
Let us explain it here with the help of the famous Cantor function. We deÔ¨Åne the
functions
F1(x) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
linear function,
between (0, 0) and (1/3, 1/2),
1/2,
x ‚àà[1/3, 2/3],
linear function,
between (2/3, 1/2) and (1, 1), x ‚àà[2/3, 1],
F2(x) =
‚éß‚é™‚é™‚é™‚é™‚é™‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é™‚é™‚é™‚é™‚é™‚é©
linear function,
between (0, 0) and (1/9, 1/4), x ‚àà[0, 1/9],
1/4,
x ‚àà[1/9, 2/9],
linear function,
between (2/9, 1/4) and (1/3, 1/2), x ‚àà(2/9, 1/3),
1/2,
x ‚àà[1/3, 2/3],
On (2/3, 1)
is similar to the interval (0, 1/3),
and so on. The sequence (Fn) converges to a non-decreasing continuous function
FC(x), called the Cantor function. We can calculate the length of intervals on which
FC(x) is constant and Ô¨Ånd
1
3 + 2
9 + 4
27 + . . . = 1
3
‚àû

n=0
2
3
n
= 1.
So, the Lebesgue measure l(N) = 0, where N is the set of points of increasing of
FC.
Denote Œº the measure, generated by the Cantor function FC and Ô¨Ånd that Œº(N) = 1
because Œº(N) = FC(1) = 1. It means that Œº and l are singular, and this fact is denoted
as l‚ä•Œº.
It is possible to give a general description of arbitrary distribution functions F :
F(x) = Œ±1F1(x) + Œ±2F2(x) + Œ±3F3(x),
where Œ±i ‚â•0, i = 1, 2, and 3, 3
i=1 Œ±i = 1, F1 is discrete, F2 is absolutely continu-
ous, and F3 is singular.
Let us pay our attention to a d-dimensional Borel space (Rd, B(Rd)). If P is
a probability measure on this space, we can deÔ¨Åne the d-dimensional distribution
function:
Fd(x1, . . ., xd) = P((‚àí‚àû, x1] √ó . . . √ó (‚àí‚àû, xd]) = P((‚àí‚àû, x]).
Problem 1.4 Prove that
1. Fd(+‚àû, . . ., +‚àû) = 1;
2. Fd(x1, . . ., xd) ‚Üí0 if at least one of x1, . . ., xd converges to ‚àí‚àû;
3. DeÔ¨Åne operator (for i = 1, . . ., d, ai < bi)
Œîai,bi Fd(x1, . . ., xd) = Fd(x1, . . ., bi, . . ., xd) ‚àíFd(x1, . . ., ai, . . ., xd),
then

10
1
Probabilistic Foundations
Œîa1,b1 . . . Œîad,bd Fd(x1, . . ., xd) = P((a, b]),
where (a, b] = (a1, b1] √ó . . . √ó (ad, bd].
In fact, there is a one-to-one correspondence between d-dimensional distribution
functions and probabilities on the space (Rd, B(Rd)).
Let us give some examples of multidimensional distribution functions:
1. If F1, . . ., Fd are the distribution functions on R1, then Fd(x1 . . . xd) =
d
i=1 Fi(xi) deÔ¨Ånes a d-dimensional distribution function.
2. If
Fi(xi) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
0,
xi < 0,
xi,
0 ‚â§xi ‚â§1,
1,
xi > 1,
then Fd(x1, . . ., xd) = x1 . . . xd corresponds to the d-dimensional Lebesgue mea-
sure.
3. A d-dimensional absolute continuous distribution function Fd(x1 . . . xd) is
deÔ¨Åned by the same manner as in the real line case:
Fd(x1, . . ., xd) =
‚à´x1
‚àí‚àû
. . .
‚à´xd
‚àí‚àû
fd(y1 . . . yd)dy1 . . . dyd,
where fd isthedensityfunction,i.e. fd ‚â•0and
‚à´‚àû
‚àí‚àû. . .
‚à´‚àû
‚àí‚àûfd(y1 . . . yd)dy1 . . . dyd =
1.
The most important case is the multidimensional Normal distribution function. It
is deÔ¨Åned by the density
fd(x1 . . . xd) = (det A)1/2
(2œÄ)d/2 exp
‚éß‚é™‚é®
‚é™‚é©
‚àí1
2

1‚â§i,j ‚â§d
aij(xi ‚àíŒºi)(xj ‚àíŒºj)
‚é´‚é™‚é¨
‚é™‚é≠
,
where A = (aij) is the inverse matrix of a symmetric positive deÔ¨Ånite matrix B.
In particular, in case d = 2, we have
f2(x1, x2) =
1
2œÄœÉ1œÉ2(1 ‚àíœÅ2)1/2 exp

‚àí
1
2(1 ‚àíœÅ2)

(x1 ‚àíŒº1)2
œÉ2
1
‚àí2œÅ (x1 ‚àíŒº1)(x2 ‚àíŒº2)
œÉ1œÉ2
+ (x2 ‚àíŒº2)2
œÉ2
2

where œÉ1, œÉ2 > 0, Œº1, Œº2 ‚ààR1, |œÅ| < 1.
Now we need to consider the case of space (R‚àû, B(R‚àû)). Let us take a set
B ‚ààB(Rn) and consider a cylinder
Cn(B) = {x ‚ààR‚àû: (x1, . . ., xn) ‚ààB}.
If P is a probability measure on (R‚àû, B(R‚àû)), we can deÔ¨Åne Pn(B) =
P(Cn(B)), n = 1, 2, . . . a sequence of probability measures on space (Rn, B(Rn)).
By construction we have
Pn+1(B √ó R1) = Pn(B),

1.2
Probabilistic distributions and the Kolmogorov consistency theorem
11
which is called the consistency condition. Now we can formulate the famous theorem
of Kolmogorov which is fundamental for foundations of probability theory.
Theorem 1.3 (Consistency theorem of Kolmogorov). Let (Pn)n=1,2,... be a system
of probability measures on (Rn, B(Rn)) correspondingly, n = 1, 2, . . ., satisfying
the consistency property. Then there exists a unique probability measure P on
(R‚àû, B(R‚àû)) such that
P(Cn(B)) = Pn(B).
We can give an example how to construct this sequence (Pn). To do this we start
with the sequence of 1-dimensional distribution functions (Fn(x))n=1,2,..., x ‚ààR1.
Further, we construct another sequence of distribution functions as follows
F1(x1) = F1(x1), x1 ‚ààR1;
F2(x1, x2) = F1(x1) ¬∑ F2(x2), x1, x2 ‚ààR1; etc.
Denote P1, P2, . . . probability measures on (R1, B(R1)), (R2, B(R2)), . . ., which
correspond to the distribution functions F1, F2, . . ..
We can also consider the space (R[0,‚àû), B(R[0,‚àû))) for which one can state a
version of consistency theorem in the same way as before. Using this theorem one
can construct an extremely important measure, called the Wiener measure.
Denote (œÜt(y|x))t ‚â•0 a family of Normal (Gaussian) densities of y for a Ô¨Åxed x :
œÜt(y|x) =
1
(2œÄt)1/2 exp

‚àí(y ‚àíx)2
2t

, y ‚ààR1.
For each t1 < t2 < . . . < tn and B = n
i=1 Ii, Ii = (ai, bi], ai < bi, we deÔ¨Åne the
measure
P(t1,t2,...,tn)(B) =
‚à´
I1
. . .
‚à´
In
œÜt1(x1|0)œÜt2‚àít1(x2|x1) . . . œÜtn‚àítn‚àí1(xn|xn‚àí1)dx1 . . . dxn.
Furthermore, the measure P on cylinder sets can be deÔ¨Åned as follows:
P(Ct1...tn(I1 √ó . . . √ó In)) = P(t1...tn)(I1 √ó . . . √ó In)),
The family of measures (P(t1...tn))n=1,2,... is consistent. Hence, according to The-
orem 1.3 the measure P can be extended from cylinder sets to the whole space
(R[0,‚àû), B(R[0,‚àû)).

Chapter 2
Random variables and their quantitative
characteristics
Abstract In the second chapter random variables are introduced and investigated
in the framework of axiomatic of Kolmogorov. It is shown a connection of proba-
bility distributions and distributions of random variables as well as their distribution
functions. The notion of the Lebesgue integral is given in context of deÔ¨Ånition of
moments of random variables (see [1], [7], [10], [15], [19], [21], [40], and [45]).
2.1
Distributions of random variables
P. L. Chebyshev was the Ô¨Årst who introduced the notion of random variables as
functions of elementary outcomes in the theory of probability. Here we develop this
topic in the framework of probability spaces of the second level.
DeÔ¨Ånition 2.1 Let (Œ©, F, P) be a probability space and (R1, B(R1)) be a Borel space.
Consider a mapping
X : (Œ©, F ) ‚Üí(R1, B(R1)).
For a given set B ‚ààB(R1) the set X‚àí1(B) = {œâ : X(œâ) ‚ààB} is called the inverse
image of B.
The mapping X is called a random variable (measurable function) if for any
B ‚ààB(R1) :
X‚àí1(B) ‚ààF .
(2.1)
Let us note that it is useful to allow to X can take values ¬±‚àû. In this case we call a
random variable as an extended random variable.
The notion of random variables is very productive as illustrated in the following
example.
Example 2.1 Let us Ô¨Åx a set A ‚ààF and deÔ¨Åne the indicator of A as follows:
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_2
13

14
2
Random variables and their quantitative characteristics
IA(œâ) =

1,
œâ ‚ààA,
0,
œâ ‚ààAc.
DeÔ¨Ånitely, IA is a mapping from the space (Œ©, F ) to (R1, B(R1)). Then for any Borel
set B we have

{œâ : IA(œâ) ‚ààB} = A ‚ààF if B contains 1 only,
{œâ : IA(œâ) ‚ààB} = Ac ‚ààF if B contains 0 only,
otherwise, we have ‚àÖor Œ©. Hence, the condition (2.1) is fulÔ¨Ålled for IA, and this
mapping is a random variable taking two values 0 and 1. Therefore, the set of all
random events is included to the set of random variables.
Problem 2.1 Assume A1, . . ., An are disjoint events from F such that ‚à™n
i=1Ai =
n
i=1 Ai = Œ©. DeÔ¨Åne a mapping X with values x1, . . ., xn on the sets A1, . . ., An,
correspondingly. Prove that X = n
i=1 xiIAi is a random variable. Let us call X a
simple random variable.
Further, if X takes countable numbers of values x1, . . ., xn, . . . on disjoint sets
A1, . . ., An, . . . with ‚àû
1 xiIAi, then X = ‚àû
1 xiIAi is called a discrete random vari-
able.
Condition (2.1) can be relaxed with the help of the next useful lemma.
Lemma 2.1 Assume E is a system of subsets such that œÉ(E) = B(R1). Then mapping
X is a random variable if and only if
X‚àí1(E) ‚ààF for all E ‚ààE.
Proof The direct implication is trivial. To prove the inverse implication we deÔ¨Åne a
system Y of those Borel sets B for which X‚àí1 ‚ààF . Further, we have equalities
X‚àí1(‚à™Œ±BŒ±) = ‚à™Œ±(X‚àí1(BŒ±)),
X‚àí1(‚à©Œ±BŒ±) = ‚à©Œ±(X‚àí1(BŒ±)),
(X‚àí1(BŒ±))c = X‚àí1(Bc
Œ±).
It follows from here that Y is a œÉ-algebra and E ‚äÜB(R1). Hence, B(R1) = œÉ(E) ‚äÜ
œÉ(Y) = Y and Lemma is proved.
‚ñ°
Based on Lemma 2.1 and the previous description of the Borel space we arrive
to the corollary.
Corollary 2.1 The function X is a random variable ‚áîX‚àí1((‚àí‚àû, x]) ‚ààF for all
x ‚ààR1. This statement is true if we replace (‚àí‚àû, x] to (‚àí‚àû, x).
Now we arrive to deÔ¨Ånition of the most important quantitative characteristic of
random variable X.

2.1
Distributions of random variables
15
DeÔ¨Ånition 2.2 Probability measure PX on a Borel space (R1, B(R1)) deÔ¨Åned as
PX(B) = P(X‚àí1(B)), B ‚ààB(R1),
(2.2)
is called a distribution (probability distribution) of X.
Applying equality (2.2) to the set B = (‚àí‚àû, x], x ‚ààR1, we arrive to the distribu-
tion function (cumulative distribution function) of X:
FX(x) = P{œâ : X(œâ) ‚â§x}.
(2.3)
In the previous sections we already studied arbitrary distributions on (R1, B(R1)). A
very similar illustration can be found in the context of random variables.
Example 2.2 1. For discrete random variable X :
PX(B) =

i:xi ‚ààB
p(xi), p(xi) = P(œâ : X(œâ) = xi) = ŒîFX(xi), B ‚ààB(R1);
2. For continuous random variable X its distribution function FX(x) is continuous;
3. For absolute continuous random variable X its distribution function FX(x) has
an integral representation with the density fX(x):
FX(x) =
‚à´x
‚àí‚àû
fX(y)dy.
Usually, any measurable function
œï : (R1, B(R1)) ‚Üí(R1, B(R1)),
is called Borelian. Using Borelian functions and a random variable, one can construct
many other random variables. This is the essence of the next important lemma.
Lemma 2.2 Let œï be a Borelian function and X be a random variable on a proba-
bility space (Œ©, F, P), then the mapping
Y = œï ‚ó¶X = œï(X),
is a random variable.
Proof For an arbitrary B ‚ààB(R1) we have
{œâ : Y(œâ) ‚ààB} = {œâ : œï(X(œâ)) ‚ààB} =
{œâ : X(œâ) ‚ààœï‚àí1(B)} ‚ààF .
‚ñ°
It means that Y is a random variable. In particular, the functions x+, x‚àí, |x| etc. are
random variables.
Let us note that there is a necessity to study multidimensional random variables
or random vectors.

16
2
Random variables and their quantitative characteristics
The corresponding deÔ¨Ånition is straightforward. This is a measurable mapping
X : (Œ©, F ) ‚Üí(Rd, B(Rd)), d ‚â•1.
In this case we have
X(œâ) = (X1(œâ), . . ., Xd(œâ)),
where Xi, i = 1, . . ., d, are one-dimensional random variables. We can deÔ¨Åne a dis-
tribution function of X as follows
FX(x1, . . ., xd) = P(œâ : X1(œâ) ‚â§x1, . . ., Xd(œâ) ‚â§xd).
DeÔ¨Ånition 2.3 We say that X1, . . ., Xd are independent if
FX(x1, . . ., xd) =
d

i=1
FXi(xi).
The following theorem is very important in providing many probabilistic construc-
tions.
Theorem 2.1 For any random variable (extended random variable) X there exists
a sequence of simple random variables X1, X2, . . ., |Xi| ‚â§|X| such that Xn(œâ) ‚Üí
X(œâ) for all œâ ‚ààŒ©. For a non-negative X such a sequence (Xn)n=1,2,... can be
constructed as a non-decreasing sequence.
Proof Assume X ‚â•0 and deÔ¨Åne for n = 1, 2, . . . simple random variables as follows
Xn(œâ) =
n¬∑2n

i=1
i ‚àí1
2n Ii,n(œâ) + nI{X(œâ)‚â•n}(œâ),
(2.4)
where Ii,n = I{œâ: i‚àí1
2n ‚â§X(œâ)< i
2n }.
The general case follows from here if we represent X = X+ ‚àíX‚àí.
‚ñ°
Problem 2.2 Prove that for (extended) random variables X1, X2, . . . the mappings
supn Xn, infn Xn, lim infn Xn, lim supn Xn are (extended) random variables.
As a hint to the corresponding solution we note that for supn Xn we have
{œâ : sup
n
Xn > x} = ‚à™n{œâ : Xn > x} ‚ààF, x ‚ààR1.
Theorem 2.2 Let (Xn)n=1,2,... be a sequence of (extended) random variables. Then
X(œâ) = limn‚Üí‚àûXn(œâ) is an (extended) random variable.
Proof Follows from Problem 2.2 and the next equalities: for any x ‚ààR1

2.1
Distributions of random variables
17
{œâ : X(œâ) < x} ={œâ : lim
n‚Üí‚àûXn(œâ) < x}
={œâ : lim sup
n
Xn(œâ) = lim inf
n
Xn(œâ)} ‚à©{œâ : lim sup
n
Xn(œâ) < x}
=Œ© ‚à©{œâ : lim sup
n
Xn(œâ) < x}
={œâ : lim sup
n
Xn(œâ) < x} ‚ààF .
Combining previous facts about limiting approximation of given (extended) random
variables X andY with the help of sequences of simple random variables (Xn)n=1,2,...
and (Yn)n=1,2,... correspondingly, we can prove the following convergence properties:
1. limn‚Üí‚àû(Xn(œâ) ¬± Yn(œâ)) = X(œâ) ¬± Y(œâ), œâ ‚ààŒ©;
2. limn‚Üí‚àû(Xn(œâ) ¬∑ Yn(œâ)) = X(œâ) ¬∑ Y(œâ), œâ ‚ààŒ©;
3. limn‚Üí‚àû(Xn(œâ) ¬∑ ÀúY‚àí1
n (œâ)) = X(œâ) ¬∑ Y‚àí1(œâ), œâ ‚ààŒ©,
where
ÀúYn(œâ) = Yn(œâ) +
1
n I{œâ:Yn(œâ)=0}.
Now, we focus on the question of how one random variable can be represented by
another.
DeÔ¨Ånition 2.4 For a given random variable X the family of events
((œâ : X(œâ) ‚ààB))B‚ààB(R1),
is called a œÉ-algebra FX generated by X.
Problem 2.3 Prove that FX is a œÉ‚àíalgebra.
Theorem 2.3 Assume a random variableY is measurable with respect to œÉ-algebra
FX. Then there exists a Borelian function œï such that
Y = œï ‚ó¶X.
Proof Consider the set of FX-measurable functions Y = Y(œâ). Denote ÀúDX the set
of FX-measurable functions of the form œï ‚ó¶X. It is clear that ÀúDX ‚äÜDX. To prove
the inverse inclusion, DX ‚äÜÀúDX, consider a set A ‚ààFX and Y(œâ) = IA(œâ).
Note that A = X‚àí1(B) for some B ‚ààB(R1), and Y = IA(œâ) = IB(X(œâ)) ‚ààÀúDX.
Further, we consider functions Y of the form n
i=1 ciIAi, where ci ‚ààR1, Ai ‚ààFX,
and Ô¨Ånd that Y ‚ààÀúDX.
For an arbitrary FX-measurable functionY we construct a sequence of simple FX-
measurable functions Yn such that Y = limn Yn. We already know that Yn = œïn(X)
for some Borelian functions œïn and œïn(X) ‚ÜíY(œâ) as n ‚Üí‚àû. Now, we take the set
B = {x : limn‚Üí‚àûœïn(x) exists} ‚ààB(R1) and deÔ¨Åne a Borelian function
œï(x) =

limn œïn(x),
x ‚ààB,
0,
x  B.

18
2
Random variables and their quantitative characteristics
ThenY(œâ) = limn‚Üí‚àûœïn(X(œâ)) = œï(X(œâ)) for all œâ ‚ààŒ© and we obtain ÀúDX ‚äÜDX.
‚ñ°
2.2
Expectations of random variables
In this section we provide a construction of another important and convenient quan-
titative characteristic of a random variable X on a probability space (Œ©, F, P).
We start such a construction from the simplest case when
X(œâ) =
n

i=1
xi ¬∑ IAi(œâ),
n

i=1
Ai = Œ©, xi ‚ààR1.
(2.5)
DeÔ¨Ånition 2.5 For the simple random variable X with representation (2.5) we deÔ¨Åne
its expected value (expectation) EX as follows
EX =
n

i=1
xiP(Ai).
Problem 2.4 a) For simple random variables X1, . . ., Xm and real numbers
a1, . . ., am we have E m
i=1 aiXi = m
i=1 aiEXi (linearity of expected values).
b) For two simple random variables X ‚â§Y we have EX ‚â§EY (monotonicity of
expected values).
Let X be a non-negative random variable. According to Theorem 2.1 one can
construct a sequence of simple random variables X(œâ)
n
‚ÜíX(œâ), n ‚Üí‚àû, for all
œâ ‚ààŒ©. Further, due to Problem 2.4, the sequence of expected values (EXn)n=1,2,...
is non-decreasing, and therefore limn‚Üí‚àûEXn does exist (Ô¨Ånite number or +‚àû). It
gives a possibility to give the following deÔ¨Ånition.
DeÔ¨Ånition 2.6 The expected value (expectation) of a non-negative random variable
X is deÔ¨Åned as
EX = lim
n‚Üí‚àûEXn.
Problem 2.5 1) If there exists another sequence of simple random variables
ÀúXn(œâ) ‚ÜëX(œâ), n ‚Üë‚àû, œâ ‚ààŒ©; then limn‚Üí‚àûE ÀúXn = EX.
2) If 0 ‚â§X(œâ) ‚â§Y(œâ) for all œâ ‚ààŒ©, then EX ‚â§EY.
There is a standard way of expected value extensions to random variables tak-
ing both positive and negative values. We can represent X as diÔ¨Äerence of its
positive and negative parts: X = X+ ‚àíX‚àí, where as usual X+ = max(0, X) and
X‚àí= max(‚àíX, 0). In this case, we say that EX does exist if EX+ or EX‚àí< ‚àûand
EX = EX+ ‚àíEX‚àí.
One can say that EX is Ô¨Ånite if both EX¬± < ‚àû, and E|X| = E(X+ + X‚àí) = EX+ +
EX‚àí< ‚àû. In this case we also call X integrable.

2.2
Expectations of random variables
19
There is another denotation for expected values:
EX =
‚à´
Œ©
XdP.
This denotation came from functional analysis, where the notion of expected value of
a measurable function X is called the Lebesgue integral. As a matter of fact we note
that the Lebesgue integral generalizes the Riemann integral at least in two directions:
1. It is constructed on the measurable space without any metric structure.
2. It is well deÔ¨Åned for any measurable bounded function X. But construction of the
Riemann integral and its existence depends on the power of the set of discontinuity
DX of X, and the set DX should have the Lebesgue measure zero.
A standard example of function ‚Äúintegrable by Lebesgue and non-integrable by
Riemann‚Äù is the Dirichlet function on [0, 1] :
X(œâ) =

1
i f œâ ‚ààQ,
0
i f œâ ‚ààR \ Q,
where Q is the set of rational numbers.
Let us formulate several natural properties of expected values as a problem
because their proofs are straightforward.
Problem 2.6 1) Ec ¬∑ X = cEX if EX does exist and c ‚ààR.
2) EX ‚â§EY if X(œâ) ‚â§Y(œâ), œâ ‚ààŒ©, EX > ‚àí‚àûor EY < ‚àû.
3) |EX| ‚â§E|X| if EX does exist.
4) If EX exists, then EX ¬∑ IA exists for each A ‚ààF . Moreover, if EX is Ô¨Ånite, then
EXIA is Ô¨Ånite too.
5) If X and Y are non-negative or integrable, then E(X + Y) = EX + EY.
ToformulateotherimportantpropertiesofexpectationsweneedthenextdeÔ¨Ånition.
DeÔ¨Ånition 2.7 We say that a property holds almost surely (a.s.) if there exists a set
N ‚ààF such that
a) the property holds for every element œâ ‚ààŒ© \ N,
b) P(N) = 0.
6) If X = 0 (a.s.), then EX = 0.
7) If X = Y (a.s.) and E|X| < ‚àû, then E|Y| < ‚àûand EX = EY.
8) If X ‚â•0 and EX = 0, then X = 0 (a.s.).
9) If X and Y are integrable and EX ¬∑ IA ‚â§EY ¬∑ IA for all A ‚ààF, then X ‚â§Y (a.s.).
Let us give a solution of subproblem 8 in the above list of Problem 2.6:
Denote A = {œâ : X(œâ) > 0} and An = {œâ : X(œâ) ‚â•1
n } ‚ÜëA as n ‚Üí‚àû.
We note that 0 ‚â§XIAn ‚â§XIA, and hence EXIAn ‚â§EX = 0. Next, we have 0 =
EXIAn ‚â•1
n P(An). Hence, P(An) = 0, n = 1, 2, . . ., and P(A) = limn‚Üí‚àûP(An) = 0.
Another useful property is the formula of change of variables in the Lebesgue
integral. Let X be a random variables and œÜ be a Borelian function which is integrable

20
2
Random variables and their quantitative characteristics
with respect to the distribution PX of X. Then for any B ‚ààB(R) the formula of change
of variables is true:
‚à´
B
œÜ(x)dPX =
‚à´
X‚àí1(B)
œÜ(X(œâ))dP.
(2.6)
In particular, the formula (2.6) for B = R1 is reduced to the formula for calculation
of expectation of X :
EœÜ(x) =
‚à´
Œ©
œÜ(X)dP =
‚à´‚àû
‚àí‚àû
œÜ(x)dPX =
‚à´‚àû
‚àí‚àû
œÜ(x)dFX,
(2.7)
where FX is a distribution function of X and the integral
‚à´‚àû
‚àí‚àûœÜ(x)dFX is the
Lebesgue-Stiltjes integral.
Scheme of the proof of the formula (2.6) is below
Take œÜ(x) = IC(x), C ‚ààB(R1), and (2.6) is reduced to PX(B ‚à©C) = P(X‚àí1(B) ‚à©
X‚àí1(C)) which follows from the deÔ¨Ånition of PX and the equality:
X‚àí1(B) ‚à©X‚àí1(C) = X‚àí1(B ‚à©C).
Next steps are obvious: a non-negative simple function œÜ etc.
If œÜ(x) = xk, k = 1, 2, . . ., the expectation EXk is called the k-th moment of X
with the help of its distribution and its distribution function. Suppose EX = Œº and
œÜ(x) = (x ‚àíŒº)k, then corresponding moments are called centered moments. The
second centered moment is called the variance of X :
Var(X) = E(X ‚àíŒº)2,
and it is one of the key measures of the dispersion of values of X around the mean
value Œº.
The other common measures of this type are
skewness = E(X ‚àíŒº)3
(Var(x))3/2
and
kurtosis = E(X ‚àíŒº)4
(Var(X))2 .
Problem 2.7 Let X be a normal random variable with parameters Œº and œÉ2. Find
EX,Var(X), skewness and kurtosis of X.

Chapter 3
Expectations and convergence of
sequences of random variables
Abstract In the third chapter asymptotic properties of sequences of random vari-
ables are studied. Lemma of Fatou and the Lebesgue dominated convergence theorem
are presented as permanent technical tools of stochastic analysis. It is also emphasized
the role of a uniform integrability condition of families of random variables. Classi-
cal probabilistic inequalities of Chebyshev, Jensen and Cauchy-Schwartz are proved.
It is shown how these inequalities work to investigate interconnections between dif-
ferent types of convergence of sequences of random variables. In particular, the large
numbers law (LNL) is derived for the case of independent identically distributed
random variables (see [1], [7], [10], [15], [19], [40], and [45]).
3.1
Limit behavior of sequences of random variables in
terms of their expected values
One of the main questions here is how to take the limit under the sign of expectation.
The Ô¨Årst result exploits a monotonicity assumption. That is why the corresponding
claim is called Monotonicity convergence theorem.
Theorem 3.1 Let (Xn)n=1,2,... be a sequence of random variables then we have
1. if there are random variables X andY such that Xn ‚â•Y, EY > ‚àí‚àû, Xn ‚ÜëX, n ‚Üë
‚àû, then EXn ‚ÜëEX, n ‚Üë‚àû;
2. if therearerandomvariables X andY suchthat Xn ‚â§Y, EY < ‚àû, Xn ‚ÜìX, n ‚Üë‚àû,
then EXn ‚ÜìEX, n ‚Üë‚àû.
Proof Let us only prove the Ô¨Årst part of the theorem because the second part can
be derived in the same way. Consider only case Y ‚â•0, and approximate Xi for
each i = 1, 2, . . . by a sequence (Xn
i )n=1,2,... of simple random variables. DeÔ¨Åne
Xn = max1‚â§i‚â§n Xn
i and note that
Xn‚àí1 ‚â§Xn = max
1‚â§i‚â§n Xn
i ‚â§max
1‚â§i‚â§n Xi = Xn.
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_3
21

22
3
Expectations and convergence of sequences of random variables
Denote Z = limn‚Üí‚àûXn and Ô¨Ånd that X = Z because Xi ‚â§Z ‚â§X for all i = 1, 2, . . . .
This inequality follows from Xn
i ‚â§Xn ‚â§Xn, i = 1, . . ., n, by taking the limit as
n ‚Üí‚àû. The random variables (Xn)n=1,2,... are simple, then
EX = EZ = lim
n‚Üí‚àûEXn ‚â§lim
n‚Üí‚àûEXn.
On the other hand, since Xn ‚â§Xn+1 ‚â§X we have that
EX ‚â•lim
n‚Üí‚àûEXn
and, hence,
EX = lim
n‚Üí‚àûEXn.
Problem 3.1 Give the proof in general case when Xn ‚â•Y and EY > ‚àí‚àû.
The next theorem is the most exploited result about taking the limit under the
expectation sign. It is called the Fatou Lemma.
Theorem 3.2
1. Assume Xn ‚â•Y and EY > ‚àí‚àû, then
E lim inf
n‚Üí‚àûXn ‚â§lim inf
n‚Üí‚àûEXn.
2. Assume Xn ‚â§Y and EY < ‚àû, then
lim sup
n‚Üí‚àû
EXn ‚â§E lim sup
n‚Üí‚àû
Xn.
3. Assume |Xn| ‚â§Y and EY < ‚àû, then
E lim inf
n‚Üí‚àûXn ‚â§lim inf
n‚Üí‚àûEXn ‚â§lim sup
n‚Üí‚àû
EXn ‚â§E lim sup
n‚Üí‚àû
Xn.
Proof In case (1) we deÔ¨Åne a sequence of random variables Zn = infm‚â•n Xm and
Ô¨Ånd that Zn ‚Üëlim infn‚Üí‚àûXn = sup infm‚â•n Xn and Zn ‚â•Y. Further we have that
lim inf
n‚Üí‚àûXn = lim inf
m‚â•n Xm = lim
n‚Üí‚àûZn,
and applying Theorem 3.1 we get the Ô¨Årst claim of the Theorem.
The case (2) is derived in a similar way. The case (3) is just a combination of (1)
and (2).
‚ñ°
Let us give the following deÔ¨Ånition of convergence of almost surely (a.s).
DeÔ¨Ånition 3.1 We say that sequence (Xn)n=1,2,... converges to a random variable X
almost surely (a.s.), if
P(œâ : Xn(œâ) ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX(œâ)) = 1.

3.1
Limit behavior of sequences of random variables in terms of their expected values
23
We introduce the following denotations in this case: Xn
a.s.
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX or Xn ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àû
X (a.s.).
The following natural question arises here:
Under what conditions one can provide convergence of expected values if Xn ‚Üí
X(a.s.)?
The classical result is the dominated convergence theorem of Lebesgue.
Theorem 3.3 Assume that Xn ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX (a.s.) and |Xn| ‚â§Y, EY < ‚àû. Then E|X| <
‚àû, and EXn ‚ÜíEX, n ‚Üí‚àû. Moreover, the sequence (Xn)n=1,2,... converges to X in
space L1, i.e., E|Xn ‚àíX| ‚Üí0, n ‚Üí‚àû.
Proof Using the Fatou Lemma we obtain E lim infn‚Üí‚àûXn = lim infn‚Üí‚àûEXn =
lim supn‚Üí‚àûEXn = E lim supn‚Üí‚àûXn = EX. It is clear that |X| ‚â§Y, and therefore
E|X| < ‚àû. Taking into account inequality |Xn ‚àíX| ‚â§2Y and applying the Fatou
Lemma again, we obtain the last statement of the theorem.
‚ñ°
Nowweintroducetheweakestconditiontoprovidetheconvergenceofexpectedvalues.
DeÔ¨Ånition 3.2 A sequence of random variables (Xn)n=1,2,... is uniformly integrable,
if
sup
n
E|Xn|I{œâ:|Xn |>c} ‚Üí0, c ‚Üí‚àû.
(3.1)
Problem 3.2 If a sequence of |Xn| ‚â§Y, n = 1, 2, . . ., EY < ‚àû, then (Xn)n=1,2,... is
uniformly integrable.
Theorem 3.4 If a sequence (Xn)n=1,2,... is uniformly integrable, then
1. E lim infn‚Üí‚àûXn ‚â§lim infn‚Üí‚àûEXn ‚â§lim supn‚Üí‚àûEXn ‚â§E lim supn‚Üí‚àûXn.
2. if also Xn ‚ÜíX (a.s.), n ‚Üí‚àû, then E|X| < ‚àûand EXn ‚ÜíEX and E|Xn ‚àí
X| ‚Üí0, n ‚Üí‚àû.
Proof To prove (1) we take c > 0 and Ô¨Ånd that
EXn = EXnI{Xn<‚àíc} + EXnI{Xn ‚â•‚àíc}.
(3.2)
Using (3.1) for œµ > 0 we obtain
sup
n
|EXnI{Xn<‚àíc}| < œµ.
Let us apply the Fatou Lemma to the second term of (3.2) and get
lim inf
n
EXnI{Xn ‚â•‚àíc} ‚â•E lim inf XnI{Xn ‚â•‚àíc}.
(3.3)
Taking into account that
XnI{Xn ‚â•‚àíc} ‚â•Xn,
we obtain from (3.3) the following inequality

24
3
Expectations and convergence of sequences of random variables
lim inf
n
EXn ‚â•E lim inf
n
Xn ‚àíœµ.
The proof of part (2) is similar to the proof of Theorem 3.3.
‚ñ°
Theorem 3.5 Assume the sequence of integrable non-negative random variables
(Xn)n=1,2,... converges to a random variable X. Then EXn ‚ÜíEX < ‚àû, n ‚Üí‚àû, if
and only if (Xn)n=1,2,... is uniformly integrable.
Proof The inverse implication follows from Theorem 3.4. To prove the direct impli-
cation we consider the set A = {a ‚ààR1 : P(œâ : X(œâ) = a) > 0} and Ô¨Ånd that
XnI{Xn<a} ‚ÜíXI{X<a}, n ‚Üí‚àû,
for each a ‚ààA and (XnI{Xn<a})n=1,2,... is uniformly integrable.
By Theorem 3.4 we obtain
EXnI{Xn<a} ‚ÜíEXI{X<a},
for a ‚ààA and hence,
EXnI{Xn ‚â•a} ‚ÜíEXI{X ‚â•a},
for a  A.
For each œµ > 0 we take a0  A large enough that
EXI{X ‚â•a0 } < œµ/2.
Choose N0 so large to provide inequalities (for all n ‚â•N0):
EXnI{Xn ‚â•a0 } ‚â§EXI{X ‚â•a0 } + œµ/2,
and EXnI{Xn ‚â•a0 } ‚â§œµ. Now we choose a1 ‚â•a0 large enough to have the following
inequality (for all n ‚â§N0):
EXnI{Xn ‚â•a1 } ‚â§œµ.
Finally, we obtain
sup
n
EXnI{Xn ‚â•a1 } ‚â§œµ,
which means the condition (3.1).
Let us note the following necessary condition of (3.1).
Problem 3.3 If the sequence (Xn)n=1,2,... is uniformly integrable, then
sup
n
E|Xn| < ‚àû.
The following problem provides a convenient suÔ¨Écient condition for (3.1).
Problem 3.4 Let (Xn)n=1,2,... be a sequence of integrable random variables and
G = G(u), u ‚â•0, be a non-negative increasing function such that

3.1
Limit behavior of sequences of random variables in terms of their expected values
25
lim
u‚Üí‚àû
G(u)
u
= ‚àû, sup
n
EG(|Xn|) < ‚àû.
Then (Xn)n=1,2,... is uniformly integrable.
Hint. Let œµ > 0 and G = supn EG(Xn). For large enough c and t ‚â•c, G(t)/t ‚â•
G/œµ. Then E|Xn|I{|Xn |‚â•c} ‚â§œµ
G EG(|Xn|)I{|Xn |‚â•c} ‚â§œµ.
To conclude our discussion of the notion of uniform integrability, we present
another necessary and suÔ¨Écient condition in the following theorem.
Theorem 3.6 A sequence of random variables (Xn)n=1,2,... is uniformly integrable
if and only if (3.3) is true and
sup
n
E|Xn|IA ‚Üí0 as P(A) ‚Üí0.
(3.4)
Proof The direct implication follows from the next considerations. For c > 0 we
have
E|Xn|IA =E|Xn|IA‚à©{|Xn |‚â•c} + E|Xn|IA‚à©{|Xn |<c}
‚â§E|Xn|I{|Xn |‚â•c} + cP(A).
Taking c large enough, we obtain for a Ô¨Åxed œµ > 0 :
sup
n
E|Xn|IA‚à©{|Xn |‚â•c} ‚â§œµ/2.
(3.5)
From (3.5) with P(A) ‚â§œµ/(2c) we get
sup
n
E|Xn|IA ‚â§œµ,
which means (3.4). To prove the inverse implication we take P(A) < Œ¥ so that
E|Xn|IA ‚â§œµ uniformly over n = 1, 2, . . . . Let us note that
E|Xn| ‚â•E|Xn|IA‚à©{|Xn |‚â•c} ‚â•cP(|Xn| ‚â•c).
Then
sup
n
P(|Xn| ‚â•c) ‚â§1
c sup
n
E|Xn| ‚Üí0, c ‚Üí‚àû.
So, we can take as A = {|Xn| ‚â•c} for large enough c and get
sup
n
E|Xn|I{|Xn |‚â•c} ‚â§œµ,
which is equivalent to (3.1).
‚ñ°

26
3
Expectations and convergence of sequences of random variables
3.2
Probabilistic inequalities and interconnections between
types of convergence of random variables
We start with some important and well-known probabilistic inequalities.
Chebyshev inequality: For a non-negative random variable X
P(œâ : X(œâ) ‚â•œµ) ‚â§EX
œµ , œµ > 0.
In particular, for arbitrary random variable X with Ô¨Ånite EX and Var(X) :
P(œâ : X(œâ) ‚â•œµ) ‚â§EX2
œµ2
and
P(œâ : |X(œâ) ‚àíEX| ‚â•œµ) ‚â§Var(X)
œµ2
.
For the proof we just note that
EX ‚â•EXI{X ‚â•œµ } ‚â•œµEI{X ‚â•œµ } = œµP(œâ : X ‚â•œµ).
Cauchy-Schwartz inequality: Let X and Y be random variables which are inte-
grable in square: EX2 < ‚àû, EY2 < ‚àû, i.e. X,Y ‚ààL2. Then E|XY| < ‚àûand
E|XY| ‚â§(EX2EY2)1/2.
Proof Without loss of generality we assume that EX2 > 0 and EY2 > 0. Consider
transformed random variables
ÀúX = X/(EX2)1/2, ÀúY = Y/(EY2)1/2.
Note that
2| ÀúX ÀúY| ‚â§ÀúX2 + ÀúY2,
and obtain
2E| ÀúX ÀúY| ‚â§E ÀúX2 + E ÀúY2 = 2,
or E| ÀúX ÀúY| ‚â§1.
‚ñ°
Jensen inequality: Let a Borelian function g = g(x) be convex downward and
Eg(|X|) < ‚àû. Then
g(EX) ‚â§Eg(X).
Proof Let us consider only the case of smooth function g ‚ààC2 with g‚Ä≤(x) ‚â•0 for
all x ‚ààR1. Using the Taylor decomposition at Œº = EX, we have
g(x) = g(Œº) + g‚Ä≤(Œº)(x ‚àíŒº) + g‚Ä≤‚Ä≤(Œ∏)(x ‚àíŒº)2
2
,
(3.6)
where Œ∏ is between x and Œº.

3.2
Probabilistic inequalities and interconnections . . .
27
Putting x = X and taking expectation in (3.6) we get the desirable inequality. ‚ñ°
In context of the Cauchy-Schwartz inequality we want to emphasize one important
case when the inequality is transformed to equality.
Theorem 3.7 Let X and Y be integrable independent random variables, i.e.
FXY(x, y) = FX(x) ¬∑ FY(y). Then E|XY| < ‚àûand
EXY = EXEY.
(3.7)
Proof We start with non-negative X and Y, constructing sequences (Xn)n=1,2,... and
(Yn)n=1,2,... of discrete random variables such that
Xn =
‚àû

m=0
m
n I{ m
n ‚â§X(œâ)< m+1
n
}, n = 1, 2, . . .
(Yn is similar)
Xn ‚â§X, Yn ‚â§Y, |Xn ‚àíX| ‚â§1
n, |Yn ‚àíY| ‚â§1
n.
Since X and Y are integrable we get that EXn ‚ÜíEX, EYn ‚ÜíEY, n ‚Üí‚àû, by the
Lebesgue dominated convergence theorem. Due to the independence assumption we
have
EXnYn =

m,l
ml
n2 EI{ m
n ‚â§X(œâ)< m+1
n
}I{ l
n ‚â§Y(œâ)< l+1
n }
=

m,l
ml
n2 EI{ m
n ‚â§X(œâ)< m+1
n
}EI{ l
n ‚â§Y(œâ)< l+1
n }
=EXnEYn.
Let us note that for n = 1, 2, . . .
|EXY ‚àíEXnYn| ‚â§E|XY ‚àíXnYn|
‚â§E|X||Y ‚àíYn| + E|Yn||X ‚àíXn|
‚â§EX
n + E(Y + 1/n)
n
‚Üí0 as n ‚Üí‚àû.
Therefore,
EXY = lim
n EXnYn = lim
n EXn lim
n EYn = EXEY < ‚àû.
General case (3.7) can be treated in a similar way if we note equalities: X = X+ ‚àí
X‚àí, Y = Y+ ‚àíY‚àí, XY = X+Y+ ‚àíX‚àíY+ ‚àíX+Y‚àí+ X‚àíY‚àí.
‚ñ°

28
3
Expectations and convergence of sequences of random variables
The brilliant Chebyshev inequality has a lot of applications, and one of the most
important corollaries is the Large Numbers Law (LNL).
Theorem 3.8 Let (Yn)n=1,2,... be a sequence of identically distributed independent
random variables with mean Œº and variance œÉ2, and Sn = n
i=1 Yi. Then for any
œµ > 0
P

œâ :

Sn
n ‚àíŒº
 ‚â•œµ

‚Üí0, n ‚Üí‚àû.
(3.8)
The proof of (3.8) follows from the Chebyshev inequality and Theorem 3.6.
Denoting ÀúYi = Yi ‚àíŒº, i = 1, 2, . . ., we calculate
E
 Sn
n ‚àíŒº
2
= 1
n2 E
 n

i=1
ÀúYi
2
= 1
n2
	
E
n

i=1
ÀúY2
i +

ij
E ÀúYi ÀúYj

= 1
n2
	
nœÉ2 +

ij
E ÀúYi ÀúYj

= œÉ2
n .
and obtain that
P

œâ :

Sn
n ‚àíŒº
 ‚â•œµ

‚â§
E

Sn
n ‚àíŒº
2
œµ2
= œÉ2
nœµ2 ‚Üí0, n ‚Üí‚àû.
Let us give the following deÔ¨Ånition.
DeÔ¨Ånition 3.3 A sequence of random variables (Xn)n=1,2,... converges to a random
variable X in probability, if for any œµ > 0
P(œâ : |Xn ‚àíX| ‚â•œµ) ‚Üí0, n ‚Üí‚àû.
(3.9)
We denote it as follows Xn
P‚àí‚Üí
n X or Xn
P‚àí‚ÜíX, n ‚Üí‚àû.
Using this deÔ¨Ånition one can reformulate Theorem 3.8 as convergence in proba-
bility of normed sums Xn = Sn/n to the constant Œº as n ‚Üí‚àû.
We also introduce two types of convergence of random variables (Xn)n=1,2,... to a
random variable X involving expected values.
DeÔ¨Ånition 3.4 We say that (Xn) converges to X weakly (weak convergence), if for
any bounded continuous function f :
E f (Xn) ‚ÜíE f (X), n ‚Üí‚àû.
(3.10)
DeÔ¨Ånition 3.5 Assume Xn, n = 1, 2, . . . and X belongs to the space Lp with Ô¨Ånite
p-moment, i.e. E|Xn|p < ‚àû, E|X|p < ‚àû. We say that Xn converges to X in space

3.2
Probabilistic inequalities and interconnections . . .
29
Lp, if E|Xn ‚àíX|p ‚Üí0, n ‚Üí‚àû. This convergence is denoted as Xn
Lp
‚àí‚àí‚Üí
n
or Xn
Lp
‚àí‚àí‚Üí,
n ‚Üí‚àû.
Now we are ready to discuss how the convergence (a.s.) (or convergence with
probability 1), deÔ¨Åned before and these convergences are related to each other.
It is not diÔ¨Écult to construct a sequence (Xn)n=1,2,... of random variables which
converges to X for all œâ ‚ààŒ©, and hence, almost surely.
For example, if X is a random variable and (an)n=1,2,... is a sequence of positive
numbers converges to zero, then a sequence Xn(œâ) = (1 ‚àían)X(œâ) converges to X
for all œâ ‚ààŒ©.
Further, convergence with probability one provides both convergence in proba-
bility and the weak convergence.
If Xn ‚ÜíX (a.s.), n ‚Üí‚àû, then |Xn ‚àíX| ‚Üí0 (a.s.), n ‚Üí‚àû, and for œµ > 0
I{œâ:|Xn‚àíX |‚â•œµ } ‚Üí0 (a.s.) as well. Taking expectation we obtain P(œâ : |Xn ‚àíX| ‚â•
œµ) = EI{œâ:|Xn‚àíX |‚â•œµ } ‚Üí0, n ‚Üí‚àû.
The weak convergence follows due to deÔ¨Ånition and the Lebesgue dominated
convergence theorem.
Convergence in space Lp is failed when the moment of p-th order does not exist.
Convergence in probability implies the weak convergence.
To prove it assume that Xn
P‚àí‚Üí
n X, f is a bounded continuous function, | f (x)| ‚â§
c, x ‚ààR1, œµ > 0. We can choose a big enough N such that P(|X| > N) ‚â§œµ/(4c).
Take Œ¥ > 0 so that | f (x) ‚àíf (y)| ‚â§œµ/(2c) for |x| ‚â§N and |x ‚àíy| < Œ¥. Then
|E f (Xn) ‚àíE f (X)| ‚â§E| f (Xn) ‚àíf (X)|I{œâ:|Xn‚àíX |>Œ¥} + E| f (Xn) ‚àíf (X)|I{œâ:|Xn‚àíX |‚â§Œ¥}.
(3.11)
The Ô¨Årst term in the right hand side of (3.10) is dominated by 2cP(œâ : |Xn ‚àíX| >
Œ¥) ‚Üí0, n ‚Üí‚àû.
E| f (Xn) ‚àíf (X)|I{œâ:|Xn‚àíX |‚â§Œ¥} =E| f (Xn) ‚àíf (X)|I{œâ:|Xn‚àíX |‚â§Œ¥}I{œâ:|X |‚â§N }
+ E| f (Xn) ‚àíf (X)|I{œâ:|Xn‚àíX |‚â§Œ¥}I{œâ:|X |>N }
‚â§œµ + 2cP(œâ : |Xn ‚àíX| > Œ¥) < 2œµ
for a large enough n.

Chapter 4
Weak convergence of sequences of
random variables
Abstract The chapter four is devoted in a systematic study of a weak convergence
of sequences of random variables. It is shown the equivalence between a weak con-
vergence and convergence in distribution. It is shown that a weak compactness and
tightness for families of probability distributions (Prokhorov‚Äôs theorem) are equiva-
lent. It is discussed a connection between characteristic functions and distributions
of random variables. The method of characteristic functions is applied to prove
the Central Limit Theorem (CLT) for sums of independent identically distributed
random variables (see [6], [15], [21], and [40]).
4.1
Weak convergence and its description in terms of
distributions
In the previous section we recognized that weak convergence corresponds to the word
‚Äúweak‚Äù because all other types of convergence provide the weak one. It turns out
this type of convergence plays the most signiÔ¨Åcant role in the theory of probability.
This is why we study it here in more details.
We start with the following considerations. Let (Xn)n=1,2,... be a sequence of
independent Bernoulli‚Äôs random variables taking values 1 and 0 with probabilities p
and q, p + q = 1, correspondingly. In this case, the law of large numbers (Theorem
3.7) is reduced to the following
¬ØSn = Sn
n
P‚àí‚Üíp, n ‚Üí‚àû.
(4.1)
Let us deÔ¨Åne distribution functions
Fn(x) = P(œâ : ¬ØSn ‚â§x) and F(x) =

1, x ‚â•p,
0, x < p,
x ‚ààR1.
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_4
31

32
4
Weak convergence of sequences of random variables
We know also from the previous section that
E f ( ¬ØSn) ‚ÜíE f (p), n ‚Üí‚àû,
(4.2)
for any bounded continuous function f .
Further, denote PFn and PF the distributions on (R1, B(R1)) which correspond to
Fn and F, respectively. Using formula of change of variables in expectations (2.7),
we can rewrite (4.2) in two equivalent forms:
‚à´
R1 f (x)dPFn ‚Üí
‚à´
R1 f (x)dPF, n ‚Üí‚àû,
‚à´
R1 f (x)dFn ‚Üí
‚à´
R1 f (x)dF, n ‚Üí‚àû,
(4.3)
for any bounded continuous function f .
The limit relations (4.3) allow to speak about the weak convergence of distribu-
tions PFn and distribution functions Fn to PF and F in the sense (4.3).
We also note that
Fn(x) ‚ÜíF(x), n ‚Üí‚àû,
(4.4)
for all x ‚ààR1 \ {p}, and hence, the weak convergence can be characterized with the
help of distribution functions as their convergence (4.4) generalized in certain way.
Namely, for an arbitrary sequence of random variables (Xn)n=1,2,... and a ran-
dom variable X with distribution functions (Fn)n=1,2,... and F we may consider a
convergence
Fn(x) ‚ÜíF(x), n ‚Üí‚àû,
for all points of continuity of F, x ‚ààR1.
Such type of convergence of Xn to X we will call a convergence in distribution
or convergence in law and denote Xn
d‚àí‚ÜíX, n ‚Üí‚àû.
Theorem 4.1
Xn
w‚àí‚ÜíX, n ‚Üí‚àû‚áîXn
d‚àí‚ÜíX, n ‚Üí‚àû.
Proof We show the direct implication only. The inverse implication is the Helly‚Äôs
selection principle application. That is why we omit it here.
For Ô¨Åxed x ‚ààR1 and for each integer Œ± ‚â•1 we deÔ¨Åne two bounded continuous
functions
fŒ± = fŒ±(y) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
1,
y ‚â§x,
Œ±(x ‚àíy) + 1,
x < y < x + Œ±‚àí1,
0,
x + Œ±‚àí1 ‚â§y,
f ‚àí
Œ± = f ‚àí
Œ± (y) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
1,
x ‚àíŒ±‚àí1 ‚â•y,
Œ±(x ‚àíy),
x ‚àíŒ±‚àí1 < y ‚â§x,
0,
x ‚â§y.
Functions fŒ± and f ‚àí
Œ± admit the limits:

4.1
Weak convergence and its description in terms of distributions
33
lim
Œ±‚Üí‚àûfŒ±(y) = I(‚àí‚àû,x](y) and lim
Œ±‚Üí‚àûf ‚àí
Œ± (y) = I(‚àí‚àû,x)(y).
(4.5)
Moreover, applying the Lebesgue dominated convergence theorem we get
lim
Œ±‚Üí‚àûE fŒ±(X) =EI(‚àí‚àû,x](X) = P(œâ : X(œâ) ‚â§x) = F(x),
lim
Œ±‚Üí‚àûE f ‚àí
Œ± (X) =EI(‚àí‚àû,x)(X) = P(œâ : X(œâ) < x) = F(x‚àí).
(4.6)
Further, due to Xn
w‚àí‚ÜíX, n ‚Üí‚àû, we have for fŒ± and f ‚àí
Œ± the limiting relations:
lim
n‚Üí‚àûE fŒ±(Xn) =E fŒ±(X),
lim
n‚Üí‚àûE f ‚àí
Œ± (Xn) =E f ‚àí
Œ± (X).
By construction of fŒ± and f ‚àí
Œ± we obtain
E f ‚àí
Œ± (Xn) ‚â§Fn(x) ‚â§E fŒ±(Xn).
Hence, for any Œ± ‚â•1 the following inequalities are true:
E f ‚àí
Œ± (X) ‚â§lim inf
n‚Üí‚àûFn(x) ‚â§lim sup
n‚Üí‚àû
Fn(x) ‚â§E fŒ±(X).
(4.7)
Combining (4.5)-(4.7) we arrive to the inequalities
F(x‚àí) ‚â§lim inf
n‚Üí‚àûFn(x) ‚â§lim sup
n‚Üí‚àû
Fn(x) ‚â§F(x).
which provide convergence Fn(x) ‚ÜíF(x), n ‚Üí‚àû, if x is a point of continuity of
F.
‚ñ°
Let us pay also a brief attention to a characterization of weak convergence of prob-
ability distributions (Pn)n=1,2,... on space (R1, B(R1)).
We start with a very simple observation. Let P and ÀúP be two diÔ¨Äerent probability
measures on (R1, B(R1)). DeÔ¨Åne a sequence of probability measures (Pn)n=1,2,... as
follows
P2n = P and P2n+1 = ÀúP,
and we arrive to conclusion that such a sequence (Pn)n=1,2,... does not converge
weakly.
Another observation: we deÔ¨Åne a sequence of probability measures such that
Pn({n}) = 1 and, hence, Pn(R1) = 1. On the other hand, limn‚Üí‚àûPn((a, b]) = 0 for
any a < b ‚ààR1.
What does it mean in terms of distribution functions?
Obviously, Pn has the distribution function
Fn(x) =

1,
x ‚â•n,
0,
x < n, n = 1, 2, . . .,

34
4
Weak convergence of sequences of random variables
and for every x ‚ààR1
lim
n‚Üí‚àûFn(x) = G(x) = 0.
Therefore, the limit above is not a distribution function. It means that the set
of distribution functions is not compact. These observations lead us to the next
deÔ¨Ånitions.
DeÔ¨Ånition 4.1 A collection of probability measures (PŒ±) is relatively compact,
if every sequence of measures from this collection admits a subsequence which
converges weakly to a probability measure.
DeÔ¨Ånition 4.2 A collection of probability measures (PŒ±) is tight, if for every œµ > 0
there exists a compact set K ‚äÜR1 such that
sup
Œ±
PŒ±(R1 \ K) ‚â§œµ.
Similar deÔ¨Ånitions can be reproduced word by word for a collection of distribution
functions (FŒ±).
The classical Prokhorov theorem below speaks us that both these notions are
equivalent.
Theorem 4.2 Let (PŒ±) be a family of probability measures on (R1, B(R1)). Then
this family is relatively compact if and only if it is tight.
Proof We give the proof of the direct implication only. Assume that (PŒ±) is not tight.
Hence, there exist œµ > 0 such that for any compact K ‚äÜR1 :
sup
Œ±
PŒ±(R1 \ K) > œµ.
Taking as K compact intervals In = [‚àín, n], n = 1, 2, . . ., we arrive to a sequence of
measures (PŒ±n) such that PŒ±n(R1 \ In) > œµ, n = 1, 2, . . . .
Further, since (PŒ±) is relatively compact, we can select from (PŒ±n) a subse-
quence (PŒ±n k ) such that (PŒ±n k )
w‚àí‚ÜíÀúP, k ‚Üí‚àû, where ÀúP is a probability measure on
(R1, B(R1)).
Now we have
œµ ‚â§lim sup
k‚Üí‚àû
PŒ±n k (R1 \ In) ‚â§ÀúP(R1 \ In),
but it is not possible as n ‚Üí‚àûand ÀúP is a probability measure.
‚ñ°
4.2
Weak convergence and Central Limit Theorem
We already know that distribution function F is a key quantitative characteristics of
a random variable on given probability space (Œ©, F, P). Moreover, it is clear from

4.2
Weak convergence and Central Limit Theorem
35
the Kolmogorov consistency theorem that there always exist such a probability space
and a random variable with given distribution. To be more illustrative we would like
to describe a particular way how this procedure can be realized.
We assume for simplicity that F(x) is strictly increasing. Then we choose the space
([0, 1], B([0, 1]), l) as a probability space. Let us deÔ¨Åne the following random variable
X(œâ) =

F‚àí1(œâ)
if œâ ‚àà[0, 1], œâ = F(x)
0 if œâ < 0 or œâ > 1.
It is clear that X is B([0, 1])‚àímeasurable and if œâ‚Ä≤ ‚Üîx‚Ä≤ and œâ ‚Üîx, we obtain
P(œâ‚Ä≤ : X(œâ‚Ä≤) ‚â§x) = P(œâ‚Ä≤ : F‚àí1(œâ‚Ä≤) ‚â§x) =
= P(œâ‚Ä≤ : œâ‚Ä≤ ‚â§F(x)) = P(œâ‚Ä≤ : œâ‚Ä≤ ‚â§œâ) = F(x),
i.e. X has the distribution function F(x).
We also want to emphasize the following additional properties of expectations
and variances.
Problem 4.1 Prove that
1. E n
i=1 aiXi = n
i=1 aiEXi for integrable random variables X1, . . ., Xn and real
numbers a1, . . ., an, n = 1, 2, . . .
2. E 	n
i=1 aiXi = 	n
i=1 aiEXi for independent random variables X1, . . ., Xn, n =
1, 2, . . .
3. Var(aX + b) = a2 Var(X) for a random variable X with well-deÔ¨Åned variance
and real numbers a, b
4. Var(n
i=1 aiXi) = n
i=1 a2
i Var(Xi) for independent random variables X1, . . ., Xn
and real numbers a1, . . ., an, n = 1, 2, . . .
Let us pay more attention to normal (or Gaussian) random variables. Denote
p(x) = pŒº,œÉ2(x) =
1
‚àö
2œÄœÉ
exp

‚àí(x ‚àíŒº)2
2œÉ2

the density of a normal random variable with parameters Œº and œÉ2, i.e. N(Œº, œÉ2).
We note the following:
1. If Œº = 0 and œÉ2 = 1, then the random variable Z = N(0, 1) is called standard.
2. The integral
‚à´‚àû
‚àí‚àûe‚àíx2/2dx =
‚àö
2œÄ is known as the Poisson integral. Therefore,
‚à´‚àû
‚àí‚àûp0,1(x)dx =
1
‚àö
2œÄ
‚àö
2œÄ = 1, which means that p0,1 is the density of some prob-
ability distribution.
3. EZ =
‚à´‚àû
‚àí‚àûxp0,1(x)dx = 0 because h(x) = x is the odd function, and
‚à´‚àû
‚àí‚àûy2e‚àíy2/2
dy =
‚àö
2œÄ for the even function h(x) = x2.
Hence,
Var(Z) = EZ2 ‚àí(EZ)2 = EZ2 =
1
‚àö
2œÄ
‚à´‚àû
‚àí‚àû
y2e‚àíy2/2dy = 1.

36
4
Weak convergence of sequences of random variables
4. Standardization procedure.
A random variable X = N(Œº, œÉ2) = œÉZ + Œº or, equivalently, Z = X‚àíŒº
œÉ . Hence,
EX = E(œÉZ + Œº) = œÉEZ + Œº = Œº and Var(X) = Var(œÉZ + Œº) = œÉ2Var(Z) =
œÉ2, and we get a nice probabilistic interpretation of parameters Œº and œÉ2 as
mean and variance of X.
5. Let X be a normal random variable with parameters Œº and œÉ2. Consider its
exponential transformationY = eX. This equality can be rewritten in a logarithmic
form X = lnY.
In this case Y is called log-normal, and its density has the form
l(y) =
1
‚àö
2œÄœÉ
exp

‚àí(ln y ‚àíŒº)2
2œÉ2

y‚àí1, y ‚àà(0, ‚àû).
Problem 4.2 Denote ŒºY and œÉ2
Y the mean, and variance of Y respectively. Prove
that
ŒºY =EY = exp(Œº + œÉ2/2),
œÉ2
Y =Var(Y) = exp(2Œº + œÉ2)(eœÉ2 ‚àí1).
To give an alternative description of moments of a random variable and its distribu-
tion function we introduce the notion of characteristic function.
DeÔ¨Ånition 4.3 Let X be a random variable. Then the function œÜX(t) = EeiX, i =
‚àö
‚àí1, is called a characteristic function of X.
To simplify our further considerations we assume that the distribution function
FX(x) = F(x) admits a density fX(x). Now we note the following:
First, the characteristics function does exist because of
E|eitx| =
‚à´‚àû
‚àí‚àû
|eitx| fX(x)dx =
‚à´‚àû
‚àí‚àû
fX(x)dx = 1 < ‚àû.
Second, if E|X|m < ‚àû, m = 1, 2, . . ., then the characteristic function admits m con-
tinuous derivatives such that
EX = iœÜ‚Ä≤
X(0), EX2 = ‚àíœÜ‚Ä≤‚Ä≤
X(0), . . ., EXm = imœÜ(m)
X (0).
(4.8)
Relations (4.8) follow from direct calculations of the corresponding integrals:
œÜ‚Ä≤
X(t) =
‚à´‚àû
‚àí‚àû
ixeitx fX(x)dx
and, hence,
œÜ‚Ä≤
X(0) = i
‚à´‚àû
‚àí‚àû
xei¬∑0¬∑x fX(x)dx = iEX etc.
Example 4.1
1. For the Bernoulli random variable X taking values 1 and 0 with
probabilities p and q respectively, p + q = 1, we have

4.2
Weak convergence and Central Limit Theorem
37
œÜX(t) = eit¬∑0q + eit¬∑1p = peit + q.
Hence, œÜ‚Ä≤
X(t) = peit ¬∑ i and œÜ‚Ä≤
X(0) = ip.
2. For the Poisson random variable with parameter Œª > 0 we have
œÜX(t) = EeitX =
‚àû

m=0
eitmP(œâ : X(œâ) = m) =
=
‚àû

m=0
eitm Œªm
mW e‚àíŒª =
‚àû

m=0
(Œªeit)m
mW e‚àíŒª =
= e‚àíŒªeŒªeit = eŒª(eit ‚àí1)
and œÜ‚Ä≤
X(0) = Œªi.
3. For the normal random variable X with parameters Œº and œÉ2 we have using
standardized random variable Z :
œÜZ(t) =
‚à´‚àû
‚àí‚àû
eitx
1
‚àö
2œÄ
e‚àíx2/2dx
=
‚à´‚àû
‚àí‚àû
cos(tx)
‚àö
2œÄ
e‚àíx2/2dx + i
‚à´‚àû
‚àí‚àû
sin(tx)
‚àö
2œÄ
e‚àíx2/2dx
= 1
‚àö
2œÄ
‚à´‚àû
‚àí‚àû
cos(tx)e‚àíx2/2dx
(4.9)
DiÔ¨Äerentiating both side of (4.9) and using the integration by parts we obtain
œÜ‚Ä≤
Z(t) = 1
‚àö
2œÄ
‚à´‚àû
‚àí‚àû
(‚àíx sin(tx))e‚àíx2/2dx
= ‚àí1
‚àö
2œÄ
‚à´‚àû
‚àí‚àû
t cos(tx)e‚àíx2/2dx = ‚àítœÜZ(t).
Hence,
œÜ‚Ä≤
Z(t)/œÜZ(t) = ‚àít.
(4.10)
Integrating the equation (4.10), we get ln œÜZ(t) = ‚àít2/2 + c, c = const, and
therefore, œÜZ(t) = ece‚àít2/2 with ec = 1 due to œÜZ(0) = 1. Finally, we have
œÜZ(t) = e‚àít2/2.
The case of arbitrary X = N(Œº, œÉ2) is considered with the help of standardization
procedure X = Œº + œÉZ. As a result, we arrive to the formula
œÜX(t) = exp

itŒº ‚àít2œÉ2
2

.

38
4
Weak convergence of sequences of random variables
Now we want to discuss a correspondence between weak convergence (convergence
in distribution, in law) and the convergence of characteristic functions.
There is a simple suÔ¨Écient condition for convergence in distribution. Namely,
assume ( fn)n=1,2,... is a sequence of densities of random variables (Xn)n=1,2,... such
that fn(x) ‚Üíf (x), n ‚Üí‚àû, implies the convergence of their distribution functions
Fn(x) ‚ÜíF(x), n ‚Üí‚àû. In fact, this result from Calculus is true for any bounded
function h(x) :
‚à´‚àû
‚àí‚àû
h(x) fn(x)dx ‚Üí
‚à´‚àû
‚àí‚àû
h(x) f (x)dx, n ‚Üí‚àû.
Taking h(x) = eitx in (4.11) we get
œÜXn(t) ‚ÜíœÜX(t), n ‚Üí‚àû.
The result (4.12) implies the convergence Fn(x) ‚ÜíF(x), n ‚Üí‚àû, too. Summarizing
all these Ô¨Åndings we arrive to the following methodology to prove the Central Limit
Theorem (CLT) of the Theory of Probability.
Theorem 4.3 Let (Xn)n=1,2,... be a sequence of independent identically distributed
(iid) random variables with EXn = Œº and VarXn = œÉ2. Denote
Sn =
n

m=1
Xm, Yn = Sn ‚àínŒº
œÉ‚àön .
Then
Yn
d‚àí‚ÜíY, n ‚Üí‚àû,
(4.11)
where Y is a standard normal random variable N(0, 1).
This is a version of the CLT for iid random variables.
Proof First of all, we note that the characteristic function of the sum of independent
random variables is equal to the product of characteristic functions of these random
variables. Using this property, we have
œÜYn(t) =œÜ
1
œÉ‚àön
n
m=1(Xm‚àíŒº)(t) = œÜn
m=1(Xm‚àíŒº)(t/œÉ‚àön)
=
n

m=1
œÜZXm‚àíŒº(t/œÉ‚àön)
=(œÜ(t/œÉ‚àön))n, n = 1, 2, . . .
(4.12)
Further, E(Xm ‚àíŒº) = 0, E(Xm ‚àíŒº)2 = œÉ2 and
œÜ‚Ä≤(t) = iE(Xm ‚àíŒº)eit(Xm‚àíŒº), œÜ‚Ä≤(0) = 0,
œÜ‚Ä≤‚Ä≤(t) = ‚àíE(Xm ‚àíŒº)2eit(Xm‚àíŒº), œÜ‚Ä≤‚Ä≤(0) = ‚àíœÉ2.

4.2
Weak convergence and Central Limit Theorem
39
Hence, we can expand œÜ in a Taylor expansion at point t = 0 and Ô¨Ånd that
œÜ(t) = 1 + 0 ‚àíœÉ2t2
2
+ t2Œî(t),
where Œî(t) ‚Üí0, t ‚Üí0. Therefore, using (4.14) we get
œÜYn(t) = œÜn(t/œÉ‚àön) = exp(n log œÜ(t/œÉ‚àön)) = exp(n log(1 ‚àít2
2n + t2
nœÉ2 Œî(t)),
and hence limn‚Üí‚àûœÜYn(t)e‚àít2/2 which is the characteristics function of N(0, 1).
‚ñ°
A more general version of the CLT is usually formulated under the Lindeberg
Condition (L).
Let (Xn)n=1,2,... be a sequence of independent random variables with EXn =
Œºn, VarXn = œÉ2
n. We say that the sequence (Xn)n=1,2,... satisÔ¨Åes the condition (L) if
for any œµ > 0
 n

m=1
œÉ2
m
‚àí1
n

m=1
‚à´

x:|x‚àíŒºm |‚â•œµ(n
m=1 œÉ2m)
1/2(x ‚àíŒºm)2dFXm(x) ‚Üí0, n ‚Üí‚àû.
Under the conditions above
Sn ‚àíESn
(VarSn)1/2
d‚àí‚ÜíN(0, 1), n ‚Üí‚àû.
The proof of this version of the CLT is provided by the same method as the proof of
Theorem 4.3.

Chapter 5
Absolute continuity of probability
measures and conditional expectations
Abstract In this chapter a special attention is devoted to the absolute continuity
of measures. It is shown how this notion and the Radon-Nikodym theorem work to
deÔ¨Åne conditional expectations. The list of properties of conditional expectations are
given here. In particular, it is emphasized the optimality in the mean-square sense
of conditional expectations (see [7], [15], [19], [40], [41] and [45]).
5.1
Absolute continuity of measures and the
Radon-Nikodym theorem
Let (Œ©, F, P) be a probability space and X be a random non-negative variable. DeÔ¨Åne
a set function
ÀúP(A) =
‚à´
A
XdP = EXIA, A ‚ààF .
(5.1)
Let us take A = ‚à™‚àû
i=1Ai, Ai ‚à©Aj = ‚àÖ, i  j. Using a linearity of expected values we
Ô¨Ånd that ÀúP from (5.1) is a Ô¨Ånite-additive measure. Further, with the help of monotonic
convergence theorem we have
ÀúP(A) =EXIA = EXI‚à™‚àû
i=1 Ai =
=E
‚àû

n=1
XIAn =
‚àû

n=1
EXIAn =
‚àû

n=1
ÀúP(An),
and therefore ÀúP is countable additive.
In case of arbitrary random variable we can use the standard representation
X = X+ ‚àíX‚àí. Assume that one of expected values EX+ or EX‚àíis Ô¨Ånite. In such a
case one can deÔ¨Åne the following signed-measure
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_5
41

42
5
Absolute continuity of probability measures and conditional expectations
ÀúP(A) =
‚à´
A
X+dP ‚àí
‚à´
A
X‚àídP = ÀúP+(A) ‚àíÀúP‚àí(A), A ‚ààF,
(5.2)
that satisÔ¨Åes the next property
P(A) = 0 ‚áíÀúP(A) = 0.
(5.3)
Theproof of (5.3) is standard. For simplerandomvariablewithvalues x1, . . ., xn, X =
n
i=1 xiIAi we have
ÀúP(A) = EXIA =
n

i=1
xiP(Ai ‚à©A) = 0, P(A) = 0.
For arbitrary random variable X we monotonically approximate X by a sequence of
simple random variables Xn ‚ÜëX, n ‚Üí‚àû, and take a limit according to monotonic
convergence theorem:
ÀúP(A) = EXIA = lim
n‚Üí‚àûEXnIA = 0.
DeÔ¨Ånition 5.1 Relation (5.3) between two measures P and ÀúP (not necessarily prob-
ability measures) is called the absolute continuity of ÀúP with respect to P, and denoted
ÀúP ‚â™P. Moreover, if P ‚â™ÀúP then measures P and ÀúP are equivalent (P ‚àºÀúP).
It turns out, one can characterize the absolute continuity property with the help of
relations (5.1)-(5.2). This fundamental fact is known as Radon-Nikodym theorem.
Theorem 5.1 Let (Œ©, F ) be a measurable space. Let Œº be a œÉ-additive measure
and Œª be a signed measure which is absolute continuous with respect to Œº, Œª =
Œª1 ‚àíŒª2, where one of Œª1 or Œª2 is Ô¨Ånite. Then there exists F -measurable function
Z = Z(œâ), œâ ‚ààŒ©, with values in [‚àí‚àû, +‚àû] such that
Œª(A) =
‚à´
A
Z(œâ)dŒº, A ‚ààF .
(5.4)
Moreover, the function Z is uniquely deÔ¨Åned up to sets of Œº-measure zero.
In representation (5.4) the function Z is called the Radon-Nikodym density or
derivative and denoted Z = dŒª
dŒº .
As the Ô¨Årst consequence of Theorem 5.1 we get a convenient rule of changing
of measure in expected values. Let X be a random variable on a probability space
(Œ©, F, P) and ÀúP ‚â™P with the Radon-Nikodym density Z. Then we formally obtain:
ÀúEX =
‚à´
Œ©
Xd ÀúP =
‚à´
Œ©
X d ÀúP
dP dP =
‚à´
Œ©
XZdP = EXZ.

5.2
Conditional expectations and their properties
43
5.2
Conditional expectations and their properties
Now we are ready to introduce the conditional expected value of random variable X
with respect to a œÉ-algebra Y ‚äÜF . In this case we also call Y sub-œÉ-algebra of F .
DeÔ¨Ånition 5.2 Let X be a non-negative random variable and Y be a sub-œÉ-algebra
of F . Then a random variable E(X|Y) is called the conditional expectation of X
with respect to Y if E(X|Y) is Y-measurable and
EXIA = E (E(X|Y)IA)
(5.5)
for every A ‚ààY.
In general case, we decompose X = X+ ‚àíX‚àíand assume that one of random vari-
ables E(X+|Y) and E(X‚àí|Y) is Ô¨Ånite, and deÔ¨Åne
E(X|Y) = E(X+|Y) ‚àíE(X‚àí|Y).
(5.6)
Existence of E(X|Y) in (5.6) follows from the Radon-Nikodym theorem. To prove it
we deÔ¨Åne a (signed) measure ÀúP(A) = EXIA on a measurable space (Œ©, Y) such that
ÀúP ‚â™P. According to Theorem 5.1 there exists a unique Radon-Nikodym density
which can be denoted here E(X|Y):
ÀúP(A) =
‚à´
A
E(X|Y)dP.
Let us note that the above equality coincides with (5.5).
Using deÔ¨Ånition of E(X|Y) we can easily deÔ¨Åne:
Conditional Probability w.r. to Y
P(B|Y) = E(IB|Y), B ‚ààF,
conditional variance w.r. to Y
Var(X|Y) = E

(X ‚àíE(X|Y))2|Y

.
It is often the œÉ-algebra Y is generated by another random variable Y. Therefore,
we can deÔ¨Åne conditional expected value and conditional probability X w.r. to Y as
follows
E(X|Y) = E(X|Y), P(B|Y) = P(B|Y), B ‚ààF,
where Y = YY.
Let us count the list of properties of conditional expectations.
1. If X = c (a.s.), then E(X|Y) = c (a.s.),
2. If X ‚â§Y (a.s.), then E(X|Y) ‚â§E(Y|Y) (a.s.),
3. |E(X|Y)| ‚â§E(|X| |Y) (a.s.),

44
5
Absolute continuity of probability measures and conditional expectations
4. For random variables X and Y and constants a and b we have
E(aX + bY|Y) = aE(X|Y) + bE(Y|Y) (a.s.),
5. E(X|F‚àó) = EX (a.s.),
6. E(X|F ) = X (a.s.),
7. EE(X|Y) = EX,
8. If sub-œÉ-algebras Y1 ‚äÜY2, then
E (E(X|Y2)|Y1) = E(X|Y1) (a.s.),
9. If sub-œÉ-algebras Y2 ‚äÜY1, then
E (E(X|Y2)|Y1) = E(X|Y2) (a.s.),
10. We say that two sub-œÉ-algebras Y1 and Y2 are independent, if for any B1 ‚ààY1
and B2 ‚ààY2 we have P(B1 ‚à©B2) = P(B1)P(B2),
For a random variable X and a sub-œÉ-algebra Y we say that X does not depend
on Y if YX and Y are independent. In this case we have
E(X|Y) = EX (a.s.),
if EX is well-deÔ¨Åned.
11. For random variables X andY such thatY is Y-measurable, E|X| < ‚àû, E|XY| <
‚àû, we have
E(XY|Y) = YE(X|Y) (a.s.)
12. For
a
(generalized)
sequence
of
random
variables
(Xn)n=1,2,...
such
that
|Xn| ‚â§Y,
n = 1, 2, . . .,
EY < ‚àû
and
Xn ‚ÜíX,
n ‚Üí‚àû,
a.s.
We
have E(Xn|Y) ‚ÜíE(X|Y), (a.s.) n ‚Üí‚àû,
13. If
Xn ‚â•Y,
EY > ‚àí‚àû
and
Xn ‚ÜëX,
n ‚Üí‚àû,
a.s.,
then
E(Xn|Y) ‚Üë
E(X|Y), (a.s.) n ‚Üí‚àû,
14. If
Xn ‚â§Y,
EY < ‚àû
and
Xn ‚ÜìX,
n ‚Üí‚àû,
a.s.,
then
E(Xn|Y) ‚Üì
E(X|Y), (a.s.) n ‚Üí‚àû,
15. If Xn ‚â•Y, EY > ‚àí‚àû, then
E(lim inf
n‚Üí‚àûXn|Y) ‚â§lim inf
n‚Üí‚àûE(Xn|Y), (a.s.)
16. For a non-negative random variables (Xn)n=1,2,... we have
E
 ‚àû

n=1
Xn|Y

=
‚àû

n=1
E(Xn|Y) (a.s.)
Let us show ways of proof of such properties. In case 2) we have for any A ‚ààY that
‚à´
A
XdP ‚â§
‚à´
A
YdP.

5.2
Conditional expectations and their properties
45
Therefore, we get from (5.5) that
‚à´
A
E(X|Y)dP ‚â§
‚à´
A
E(Y|Y)dP,
which means that E(X|Y) ‚â§E(Y|Y) (a.s.)
In case (8) we have for any A ‚ààY1 ‚äÜY2 that
‚à´
A
E(X|Y1)dP =
‚à´
A
XdP =
‚à´
A
E(X|Y2) =
‚à´
A
E (E(X|Y2)|Y1) dP,
which certiÔ¨Åes the statement in (8).
Below we provide some comments and detailed calculations of conditional expec-
tations.
Consider E(X|Y) for two (integrable) random variables X and Y. According to
our deÔ¨Ånition E(X|Y) is YY-measurable, and by the representation theorem there
exists a Borelian function œÜ(¬∑) such that
œÜ(Y(œâ)) = E(X|Y)(œâ), œâ ‚ààŒ©.
(5.7)
For A ‚ààYY we get from (5.7):
‚à´
A
XdP =
‚à´
A
E(X|Y)dP =
‚à´
A
œÜ(Y)dP.
(5.8)
Further, taking A = Y‚àí1(B), B ‚ààB(R1) we have
‚à´
Y‚àí1(B)
œÜ(Y)dP =
‚à´
B
œÜ(y)dPY,
and hence
‚à´
Y‚àí1(B)
XdP =
‚à´
B
œÜ(y)dPY.
(5.9)
Having equalities (5.7)-(5.9) we can take œÜ(y) = E(X|Y = y), y ‚ààR1.
To provide more details one can consider a special case when the pair (X,Y)
admits a joint density fXY(x, y). In such a case we can put
fX |Y(x|y) = fxy(x, y)
fY(y)
and Ô¨Ånd that for C ‚ààB(R1)
P(X ‚ààC|Y = y) =
‚à´
C
fX |Y(x|y)dx
and
E(X|Y = y) =
‚à´
R1 x fX |Y(x|y)dx.

46
5
Absolute continuity of probability measures and conditional expectations
This is a reason that the function fX |Y(x|y) is called a density of a conditional
distribution (conditional density).
In case of discrete random variables X and Y with values x1, x2, . . . and y1, y2, . . .
correspondingly, denote Y = œÉ(Y) = YY and (œâ : Y(œâ) = yi) = Di, i = 1, 2, . . .
Sets Di, i = 1, 2, . . . are atoms for P in the sense that P(Di) > 0 and any subset
A ‚äÜDi has a probability zero or its completion to Di (i.e. Di \ A) has a probability
zero. We can deÔ¨Åne
E(X|Di) = EXIDi
P(Di) , i = 1, 2, . . .
Then the calculation of E(X|Y) is reduced to the claim:
E(X|Y) = E(X|YY) = E(X|Di) (a.s.)
on the atom Di, i = 1, 2, . . .
To connect these deÔ¨Ånitions and results to the general deÔ¨Ånition we need to check
(5.5). In particular, we have
EE(X|Y) =
‚àû

i=1
E(X|Di)P(Di)
=
‚àû

i=1
E(XIDi) =
‚àû

i=1
‚àû

j=1
xjP(X = xj,Y = yi)
=
‚àû

j=1
‚àû

i=1
xjP(X = xj,Y = yi)
=
‚àû

j=1
xj
‚àû

i=1
P(X = xj,Y = yi) =
‚àû

j=1
xjP(X = xj)
=EX.
Let‚Äôs take note of that calculations of E(X|Y1, . . .,Yn) can be given in the same way.
Finally, we demonstrate an important application of E(X|Y). We will interpret Y
as an observable variable and X as a non-observable. This is a typical situation in
many areas, including option pricing theory, where X is a pay-oÔ¨Äof option and Y is
a stock price.
How can we estimate X based on observations of Y to provide the optimality of
the estimate?
A satisfactory solution of the problem can be given as follows.
Let œÜ = œÜ(x), x ‚ààR1, be a Borelian function. Taking œÜ(Y) we get an estimate for
X, and we should choose from a variety of such estimates an optimal estimate. As a
criteria we can take E(X ‚àíœÜ(Y))2 and deÔ¨Åne the optimal estimate œÜ‚àó(Y) as follows
E(X ‚àíœÜ‚àó(Y))2 = inf
œÜ E(X ‚àíœÜ(Y))2,
where we assume EX2 < ‚àûand EœÜ2(Y) < ‚àû.

5.2
Conditional expectations and their properties
47
Theorem 5.2 The
optimal
estimate
has
the
following
representation
œÜ‚àó(x) = E(X|Y = x).
Proof Taking œÜ‚àó(Y) = E(X|Y) we have for any other estimate œÜ :
E(X ‚àíœÜ(Y))2 =E
	
(X ‚àíœÜ‚àó(Y)) + (œÜ‚àó(Y) ‚àíœÜ(Y))

2
=E(X ‚àíœÜ‚àó(Y))2 + 2E(X ‚àíœÜ‚àó(Y))(œÜ‚àó(Y) ‚àíœÜ(Y)) + E(œÜ‚àó(Y) ‚àíœÜ(Y))2
=E(X ‚àíœÜ‚àó(Y))2 + E(œÜ‚àó(Y) ‚àíœÜ(Y))2 + 2E
	
E (X ‚àíœÜ‚àó(Y))(œÜ‚àó(Y) ‚àíœÜ(Y)) |Y

=E(X ‚àíœÜ‚àó(Y))2 + E(œÜ‚àó(Y) ‚àíœÜ(Y))2
‚â•E(X ‚àíœÜ‚àó(Y))2.

Chapter 6
Discrete time stochastic analysis: basic
results
Abstract Chapter 6 is completely devoted to a discrete time stochastic analysis.
It contains the key notions adapted to discrete time like stochastic basis, Ô¨Åltration,
predictability, stopping times, martingales, sub- and supermartingales, local densities
of probability measures, discrete stochastic integrals and stochastic exponents. It
is stated the Doob decomposition for stochastic sequences, maximal inequalities,
and other Doob‚Äôs theorems. The developed martingale technique is further applied
to prove several asymptotical properties for martingales and submartingales (see
[1], [7], [8], [10], [15], [40], and [45]).
6.1
Basic notions: stochastic basis, predictability and
martingales
We have seen already that a sequence of random variables X1, X2, . . ., Xn can be
interpreted as a n-times repetition (realization) of the underlying random experiment.
One can note that the order of appearance of new information is important. For
example, such order and such information are valuable in stock exchange trading.
One can also observe that a numeration of Xi may not be directly connected to
real (physical) time, because an operation time is often exploited instead of real
time in Ô¨Ånance. As we know, to provide an accurate work with random variables
we need to assume that these random variables are deÔ¨Åned on some probability
space (Œ©, F, P). In this setting, for each Ô¨Åxed outcome œâ ‚ààŒ© one can observe
a sequence of numbers X1(œâ), X2(œâ), . . ., Xn(œâ), . . ., called a trajectory or sample
path. It may present a behavior of stock prices or indexes over given time interval. So,
thinking in this way, we must emphasize the diÔ¨Äerence between probability theory
and stochastic/random processes. It can be roughly explained as follows. If theory of
probability studies probabilities of occurrence of random events, including the events
connected to random variables, theory of stochastic processes considers probabilities
of occurrence of trajectories and families of trajectories of stochastic processes. Such
a diÔ¨Äerence calls for a ‚Äúsupplementary equipment‚Äù of given probability space with
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_6
49

50
6
Discrete time stochastic analysis: basic results
an information Ô¨Çow or Ô¨Åltration. It is a non-decreasing family (Fn)n=0,1,... together
with (Œ©, F, (Fn)n=0,1,..., P) called a stochastic basis. Usually, the main œÉ-algebra
F is determined by the Ô¨Åltration (Fn)n=0,1,... in the sense that F‚àû= œÉ(‚à™nFn) = F .
We sometimes will count that F0 is trivial, i.e. F0 = {‚àÖ, Œ©}. Filtration creates a
new class of random variables, called stopping times. A random variable œÑ : Œ© ‚Üí
{0, 1, . . ., n, . . ., ‚àû} is called a stopping time if for each n = 0, 1, . . .
{œâ : œÑ(œâ) ‚â§n} ‚ààFn,
i.e. each value n is taken based on information until time n without having information
from the future time.
We can interpret a stopping time œÑ as a random time, and therefore we can speak
about information FœÑ before this time œÑ. It is formally realized as follows:
FœÑ = {A ‚ààF‚àû: A ‚à©{œâ : œÑ(œâ) ‚â§n} ‚ààFn},
and FœÑ is a œÉ-algebra.
If two stopping times œÑ and œÉ are connected to each other through inequality
œÑ ‚â§œÉ (a.s.), then obviously FœÑ ‚äÜFœÉ.
Now if (Xn)n=0,1,... is a sequence of random variables on (Œ©, F, (Fn)n=0,1,..., P)
with information Ô¨Çow (Fn) and Xn ‚àíFn‚àímeasurable for each n = 0, 1, . . .. In this
case we will call (Xn) adapted to (Fn)n=0,1,..., or stochastic.
For a stochastic sequence (Xn)n=0,1,... and a stopping time œÑ we deÔ¨Åne
XœÑ =
‚àû

n=0
XnI{œÑ=n} + X‚àûI{œÑ=‚àû},
where X‚àûmay be a Ô¨Åxed constant or X‚àû= limn‚Üí‚àû= Xn (a.s.), if the limit does
exist.
The random variable XœÑ is a ‚Äúsuperposition‚Äù of a stochastic sequence and œÑ, and
hence, XœÑ ‚àíFœÑ‚àímeasurable. We can go even further if we take a stochastic sequence
(Xn)n=0,1,... and a non-decreasing sequence of stopping times (œÑn)n=0,1,... and deÔ¨Åne
a new sequence of random variables
Yn = XœÑn, œÑn ‚â§œÑn+1.
It is quite natural to call (œÑn) as a time change and (Yn) as a time changed sequence.
The idea of ‚Äútime change‚Äù is very productive for Stochastic Analysis. For example,
in Mathematical Finance this type of time change is often based on market volatility,
and is called the ‚Äúoperation time‚Äù.
Problem 6.1 Prove that
1. œÑ is a stopping time if and only if {œÑ = n} ‚ààFn for all n.
2. FœÑ is a œÉ‚àíalgebra, if œÑ is a stopping time.
3. FœÑ ‚äÜFœÉ if stopping times œÑ ‚â§œÉ(œâ) (a.s.).
4. XœÑ is FœÑ-measurable if œÑ is a stopping time.
5. œÑ ‚àßœÉ is a stopping time.

6.1
Basic notions: stochastic basis, predictability and martingales
51
We can also consider (Œ©, F, (Fn), P) as a system of probability spaces
(Œ©, F0, P0), (Œ©, F1, P1), . . ., where P0 on F0 is just a restriction of P to F0 and
so on. In this framework we can consider another probability measure ÀúP and ÀúPn as
its restriction to Fn. Assume that ÀúPn ‚â™Pn, n = 0, 1, . . . and denote Zn = d ÀúPn
dPn the
density of ÀúPn with respect to Pn, n = 0, 1, . . . . The sequence (Zn)n=0,1,... is called a
local density of ÀúP with respect to P, and for each n = 0, 1, . . . we have
EZn =
‚à´
Œ©
ZndP =
‚à´
Œ©
ZndPn = ÀúPn(Œ©) = 1.
Further, for any A ‚ààFn‚àí1 we formally have that
‚à´
A
ZndP =
‚à´
A
ZndPn =
‚à´
A
d ÀúPn
dPn
dPn = ÀúPn(A) = ÀúPn‚àí1(A) =
‚à´
A
Zn‚àí1dP.
It means that (a.s.)
E(Zn|Fn‚àí1) = Zn‚àí1, n = 1, 2, . . .
(6.1)
We can also ask the question:
Is it possible to calculate for some integrable random variable Y the conditional
expectation ÀúE(Y|Fn‚àí1) via E(Y|Fn‚àí1)?
Again, take A ‚ààFn‚àí1 and Ô¨Ånd that
‚à´
A
E(Y Zn|Fn‚àí1)dP =
‚à´
A
Y ZndP =
‚à´
A
Yd ÀúPn
=
‚à´
A
Yd ÀúPn‚àí1 =
‚à´
A
ÀúE(Y|Fn‚àí1)d ÀúPn‚àí1
=
‚à´
A
ÀúE(Y|Fn‚àí1)dPn‚àí1 =
‚à´
A
ÀúE(Y|Fn‚àí1)Zn‚àí1dP,
and hence,
ÀúE(Y|Fn‚àí1) = Z‚àí1
n‚àí1E(Y Zn|Fn‚àí1) (a.s.)
(6.2)
Relation (6.2) is called a rule of change of probability in conditional expectations.
Besides adapted sequences of random variables, we can introduce sequences of
random variables which are in between deterministic and stochastic sequences. We
say that an adapted sequence (An)n=0,1,... is predictable, if An-Fn‚àí1-measurable for
n = 1, 2, . . .. We also can take the property (6.1) to introduce the whole class of
stochastic sequences called martingales. We say that an integrable adapted sequence
(Mn)n=0,1,... is a martingale, if
E(Mn|Fn‚àí1) = Mn‚àí1 (a.s.) f or n = 1, 2, . . . .
(6.3)
Relation (6.3) can be rewritten as follows E(Mn ‚àíMn‚àí1|Fn‚àí1) = 0 (a.s.) It means
the sequence Yn = Mn ‚àíMn‚àí1 presents so-called a martingale-diÔ¨Äerence.

52
6
Discrete time stochastic analysis: basic results
We can see also that the local density (Zn)n=0,1,... in (6.1) is a martingale
with respect to P. These two types of stochastic sequences is a natural basis
for many others. To demonstrate this claim, we consider an arbitrary integrable
stochastic sequence (Xn)n=0,1,... with X0 = 0 (a.s.) for simplicity. DeÔ¨Åne for each
n = 1, 2, . . ., ŒîXn = Xn ‚àíXn‚àí1 and write an obvious equality
ŒîXn = ŒîXn ‚àíE(ŒîXn|Fn‚àí1) + E(ŒîXn|Fn‚àí1) = ŒîMn + ŒîAn;
(6.4)
where
ŒîMn = ŒîXn ‚àíE(ŒîXn|Fn‚àí1),
ŒîAn = E(ŒîXn|Fn‚àí1).
It is clear from (6.4) that for n = 0, 1, . . .
Xn = Mn + An,
(6.5)
where Mn = 
i‚â§n ŒîMi and An = 
i‚â§n ŒîAi; and therefore, (Mn)n=0,1,... is a martin-
gale and (An)n=0,1,... is predictable.
Such a unique decomposition (6.5) is called the Doob decomposition of
(Xn)n=0,1,... In particular, it is true for sequences (Xn)n=0,1,... satisfying conditions
E(Xn|Fn‚àí1) ‚â•Xn‚àí1 (a.s.)
and
E(Xn|Fn‚àí1) ‚â§Xn‚àí1 (a.s.)
Such sequences are called submartingales and supermartingales and in their decom-
positions (6.5) ŒîAn ‚â•0 and ŒîAn ‚â§0 (a.s.), n = 1, 2, . . ., correspondingly.
Remark 6.1 Given a martingale (Xn)n=0,1,... there is a simple way of constructing
submartingales (supermartingales). Suppose œÜ is a convex downward measurable
function such that E|œÜ(Xn)| < ‚àû, n = 0, 1, . . . Then Jensen‚Äôs inequality implies that
(œÜ(Xn))n=0,1,... is a submartingale.
Let us consider a martingale (Mn)n=0,1,... such that EM2
n < ‚àû, n = 0, 1, . . ., then
it is called square-integrable. Further, due to the Jensen‚Äôs inequality Xn = M2
n is a
submartingale. Using the Doob decomposition (6.5) one can conclude that
M2
n = mn + ‚ü®M, M‚ü©n, n = 0, 1, . . .,
where (mn)n=0,1,... is a martingale and (‚ü®M, M‚ü©n)n=0,1,... is a non-decreasing pre-
dictable sequence called the quadratic characteristic (compensator) of M. More-
over,
‚ü®M, M‚ü©n =
n

i=1
E((ŒîMi)2|Fi‚àí1), ‚ü®M, M‚ü©0 = 0,

6.1
Basic notions: stochastic basis, predictability and martingales
53
and
E

(Mk ‚àíMl)2|Fl

=E(M2
k ‚àíM2
l |Fl)
=E(‚ü®M, M‚ü©k ‚àí‚ü®M, M‚ü©l|Fl), l ‚â§k,
and EM2
n = E‚ü®M, M‚ü©n, n = 0, 1, . . .. It is possible to deÔ¨Åne a measure of correlation
between two square-integrable martingales (Mn)n=0,1,... and (Nn)n=0,1,... :
‚ü®M, N‚ü©n = 1
4{‚ü®M + N, M + N‚ü©n ‚àí‚ü®M ‚àíN, M ‚àíN‚ü©n},
n = 0, 1, . . ., which is called the joint quadratic characteristic of (Mn)n=0,1,... and
(Nn)n=0,1,....
It is almost obvious to show that the sequence (MnNn ‚àí‚ü®M, N‚ü©n)n=0,1,... is a
martingale, and if ‚ü®M, N‚ü©n = 0 for all n = 0, 1, . . ., then such martingales are called
orthogonal.
Further, the following property that connects the martingale property and absolute
continuity of probability measures is related to formula (6.2) and can be referred to
as a discrete time version of the Girsanov theorem.
Let (Mn)n=0,1,..., M0 = 0, be a martingale with respect to the original measure
P and assume E|ZnZ‚àí1
n‚àí1ŒîMn| < ‚àû, n = 1, 2, . . .. DeÔ¨Åne ( ÀúMn)n=0,1,..., ÀúM0 = 0, by
relations
Œî ÀúMn = ŒîMn ‚àíE(ZnZ‚àí1
n‚àí1ŒîMn|Fn‚àí1), n = 1, 2, . . . .
Using the rule of change of measure (6.2), we obtain
ÀúE(Œî ÀúMn|Fn‚àí1) = ÀúE(ŒîMn ‚àíE(ZnZ‚àí1
n‚àí1ŒîMn|Fn‚àí1)|Fn‚àí1)
= ÀúE(ŒîMn|Fn‚àí1) ‚àíÀúE(ŒîMn|Fn‚àí1) = 0 (a.s.),
which implies that ( ÀúMn)n=0,1,... is a martingale with respect to ÀúP ‚â™P with the local
density (Zn)n=0,1,....
Let us use predictable and martingale stochastic sequences to construct more
complicated objects. For a predictable (Hn)n=0,1,... and a martingale (mn)n=0,1,...
deÔ¨Åne a discrete stochastic integral
H ‚àómn =
n

i=0
HiŒîmi.
(6.6)
If martingale (mn)n=0,1,... is square-integrable, sequence (Hn)n=0,1,... is predictable
and EH2
nŒî‚ü®m, m, ‚ü©n < ‚àû, n = 0, 1, . . ., then (H ‚àóMn)n=0,1,... is a square-integrable
martingale with quadratic characteristic
‚ü®H ‚àóm, H ‚àóm‚ü©n =
n

i=0
H2
i Œî‚ü®m, m‚ü©i.

54
6
Discrete time stochastic analysis: basic results
Further, let (Mn)n=0,1,... be a Ô¨Åxed square-integrable martingale, then one
can consider all square-integrable martingales (Nn)n=0,1,... that are orthogonal to
(Mn)n=0,1,.... Introduce a family of square-integrable martingales of the form
Xn = Mn + Nn.
(6.7)
On the other hand, any square-integrable martingale (Xn)n=0,1,... can be written in the
form (6.7), where the orthogonal term (Nn)n=0,1,... satisÔ¨Åes (6.6) with the martingale
(mn)n=0,1,... that is orthogonal to the given martingale (Mn)n=0,1,.... Such a version
of decomposition (6.7) is referred to as the Kunita-Watanabe decomposition.
Discrete stochastic integrals are related to discrete stochastic diÔ¨Äerential equa-
tions. Solutions of such equations are used in modeling the dynamics of asset prices
in Ô¨Ånancial markets.
Consider a stochastic sequence (Un)n=0,1,... with U0 = 0 and deÔ¨Åne a new stochas-
tic sequence (Xn)n=0,1,... with X0 = 1 by
ŒîXn = Xn‚àí1ŒîUn, n = 1, 2, . . . .
(6.8)
Solution of (6.8) has the form
Xn =
n

i=1
(1 + ŒîUi) = En(U), n = 1, 2, . . .,
and is called a stochastic exponential.
One can consider a non-homogeneous version of equation (6.8)
ŒîXn = ŒîNn + Xn‚àí1ŒîUn, X0 = N0,
(6.9)
where (Nn)n=0,1,... is a given sequence.
Solution of (6.9) is determined with the help of stochastic exponential as follows
Xn = En(U)

N0 +
n

i=1
E‚àí1
i (U)ŒîUi
	
.
Let us list very helpful properties of stochastic exponentials.
1. E‚àí1
n (U) = En(‚àíU‚àó), where ŒîU‚àó
n =
ŒîUn
1+ŒîUn, ŒîUn  ‚àí1;
2. (En(U))n=0,1,... is a martingale if and only if (Un)n=0,1,... is a martingale;
3. En(U) = 0 (a.s.) for n ‚â•œÑ0 = inf(i : Ei(U) = 0);
4. For two stochastic sequences (Un) and (Vn) the next multiplication rule of stochas-
tic exponentials is true:
En(U)En(V) = En(U + V + [U,V]),
where Œî[U,V]n = ŒîUnŒîVn.

6.2
Martingales on Ô¨Ånite time interval
55
6.2
Martingales on Ô¨Ånite time interval
Here we study stochastic sequences on the interval [0, N] = {0, 1, . . ., N}. The Ô¨Årst
important theorem is devoted to an interesting characterization of the notion of a
martingale.
Theorem 6.1 Let (Xn)n=0,1,...,N be an integrable stochastic sequence on a stochastic
basis (Œ©, F, (Fn)n=0,1,...,N, P). Then the following statements are true
1) The sequence (Xn)n=0,1,...,N is a martingale if and only if Xn = E(XN |Fn) (a.s.)
for all n = 0, 1, . . ., N.
2) If for all stopping times œÑ the equality EXœÑ = EX0 is fulÔ¨Ålled then (Xn)n=0,1,...,N
is a martingale.
Proof 1) For a direct implication, using the deÔ¨Ånition of conditional expectations
and their telescopic property, we have that (a.s.)
E(XN |FN‚àí2) = E (E(XN |FN‚àí1)|FN‚àí2) = E(XN‚àí1|FN‚àí2) = XN‚àí2, etc.
For an inverse implication we also use the deÔ¨Ånition and the telescopic property and
Ô¨Ånd that for all n (a.s.)
E(XN |FN‚àí1) = E (E(XN |FN)|FN‚àí1) = E(XN |FN‚àí1) = XN‚àí1, etc.
2) For a Ô¨Åxed n ‚â§N and A ‚ààFn we deÔ¨Åne the following stopping time
œÑA(œâ) =

n
i f œâ ‚ààA,
N
i f œâ  A.
According to the assumption we have
EX0 = EXœÑA =EXœÑAIA + EXœÑAIAc
=EXnIA + EXN IAc .
It means that E(XN |Fn) = Xn (a.s.) and by (a) we conclude that (Xn)n=0,1,...,N is a
martingale.
‚ñ°
Theorem 6.1 gives us an understanding about a strong connection between martin-
gales and stopping times. The next statement which has a special name as ‚Äúoptional
sampling theorem‚Äù of Doob tells us more about these connections.
Theorem 6.2 Let (Xn)n=0,1,...,N be a martingale (submartingale, supermartingale)
and œÑ1 ‚â§œÑ2 (a.s.) are stopping times, then
E(XœÑ2|FœÑ1) = XœÑ1 (a.s.)

56
6
Discrete time stochastic analysis: basic results
(E(XœÑ2|FœÑ1) ‚â•XœÑ1 and E(XœÑ2|FœÑ1) ‚â§XœÑ1 (a.s.), respectively). In particular EXœÑ1 =
EXœÑ2.
Proof of this theorem, which generalizes the obvious property for deterministic
times, readily follows from next considerations.
For A ‚ààFœÑ1, n ‚â§N and B = A ‚à©{œÑ1 = n} in order to prove
‚à´
A XœÑ2dP =
‚à´
A XœÑ1dP
we need to prove that ‚à´
B‚à©{œÑ2 ‚â•n}
XœÑ2dP =
‚à´
B‚à©{œÑ2 ‚â•n}
XndP.
It follows from the next equalities
‚à´
B‚à©{œÑ2 ‚â•n}
XndP =
‚à´
B‚à©{œÑ2=n}
XndP +
‚à´
B‚à©{œÑ2>n}
XndP
=
‚à´
B‚à©{œÑ2=n}
XndP +
‚à´
B‚à©{œÑ2>n}
E(Xn+1|Fn)dP
=
‚à´
B‚à©{œÑ2=n}
XndP +
‚à´
B‚à©{œÑ2>n}
Xn+1dP
=
‚à´
B‚à©{n‚â§œÑ2 ‚â§n+1}
XndP +
‚à´
B‚à©{œÑ2 ‚â•n+2}
Xn+2dP = . . . =
=
‚à´
B‚à©{œÑ2 ‚â•n}
XœÑ2dP.
It turns out for martingales (submartingales, supermartingales) one can get inequal-
ities called maximal (or the Kolmogorov-Doob inequalities) which are stronger than
the Chebyshev inequality.
Theorem 6.3 1. If (Xn)n=0,1,...,N is a submartingale, then for any Œª > 0 :
P(œâ : max
n‚â§N Xn ‚â•Œª) ‚â§EX+
N
Œª
.
2. If (Xn)n=0,1,...,N is a supermartingale, then for any Œª > 0 :
P(œâ : max
n‚â§N Xn ‚â•Œª) ‚â§EX0 + EX‚àí
N
Œª
.
3. If (Xn)n=0,1,...,N is a martingale, then for any Œª > 0 :
P(œâ : max
n‚â§N |Xn| ‚â•Œª) ‚â§E|XN |
Œª
.
Proof Let us deÔ¨Åne the following stopping time
œÑ = inf{n ‚â§N : Xn ‚â•Œª},

6.2
Martingales on Ô¨Ånite time interval
57
where we put œÑ = N if the set in brackets above is ‚àÖ.
Denote A = {œâ : maxn‚â§N Xn ‚â•Œª} and Ô¨Ånd that
A ‚à©{œÑ = n} = {œÑ = n} ‚ààFn,
if n < N and
A ‚à©{œÑ = N} ‚ààFN,
due to A ‚ààFN.
Hence, A ‚ààFœÑ.
To prove the statement (1) of the theorem we derive from Theorem 6.2 that
EXœÑIA ‚â§EXN IA
because stopping time œÑ ‚â§N.
If the event A occurs, then XœÑ ‚â•Œª and XœÑIA ‚â•ŒªIA. Therefore,
ŒªP(A) ‚â§EXN IA ‚â§EX+
N IA ‚â§EX+
N,
and we obtain (1).
Further, due to 0 ‚â§œÑ ‚â§N we have from Theorem 6.2
EX0 ‚â•EXœÑ = EXœÑIA + EXœÑIAc ‚â•EXœÑIA + EXN IAc .
Hence, we get statement (2) after next calculations
ŒªP(A) ‚â§EXœÑIA ‚â§EX0 ‚àíEXN IAc
=EX0 + E(‚àíXN)IAc ‚â§EX0 + E(‚àíXN)+IAc
‚â§EX0 + EX‚àí
N.
The last statement (3) is a combination of (1) and (2).
‚ñ°
We Ô¨Ånish this section by the well-known and helpful results of Doob about estima-
tion of number of upcrossings (downcrossings) of given interval by submartingales
and supermartingales.
Let a < b be real numbers and (Xn)n=0,1,...,N be a stochastic sequence. DeÔ¨Åne the
following sequence of stopping times:
œÑ0 = 0, œÑ1 = inf{n > 0 : Xn ‚â§a},
œÑ2 = inf{n > œÑ1 : Xn ‚â•b}, . . .,
œÑ2m‚àí1 = inf{n > œÑ2m‚àí2 : Xn ‚â§a},
œÑ2m = inf{n > œÑ2m‚àí1 : Xn ‚â•b},
where œÑk = N if the set in brackets above is ‚àÖ.

58
6
Discrete time stochastic analysis: basic results
It is clear that between œÑ2m‚àí1 and œÑ2m that is an upcrossing of the interval (a, b)
by the sequence (Xn) occurs.
DeÔ¨Åne the following random variable
Œ≤X
+ (N, a, b) =

0,
i f œÑ2 > N,
max{m : œÑ2m ‚â§N},
i f œÑ2 ‚â§N,
which is called the number of upcrossings of (a, b) by the sequence (Xn) during the
time interval [0, N].
DeÔ¨Ånition of notion of downcrossings is given in a similar way. Denote the
corresponding number of downcrossings by Œ≤X
‚àí(N, a, b).
Theorem 6.4 Let (Xn)n=0,1,...,N be a submartingale and Œ≤X
¬± (N, a, b) be the number
of upcrossings (downcrossings) of the interval (a, b). Then
EŒ≤X
+ (N, a, b) ‚â§E(XN ‚àía)+
b ‚àía
,
(6.10)
EŒ≤X
‚àí(N, a, b) ‚â§E(XN ‚àíb)+
b ‚àía
.
(6.11)
Proof We prove only (6.10) in view a symmetry of formulas (6.10) and (6.11). Let us
reduce the initial problem to estimation of number of upcrossings by a non-negative
submartingale ((Xn ‚àía)+)n=0,1,...,N of the interval (0, b ‚àía). Moreover, we can put
a = 0 and prove that
EŒ≤X
+ (N, 0, b) ‚â§EXN
b
,
(6.12)
for a non-negative submartingale (Xn)n=0,1,2,...,N, X0 = 0.
DeÔ¨Åne for i = 1, 2, . . . a sequence of random variables:
œÜi =

1,
i f œÑm < i ‚â§œÑm+1 for some non-even m,
0,
i f œÑm < i ‚â§œÑm+1 for some even m.
It follows from deÔ¨Ånitions of Œ≤X
+ (N, a, b) and œÜi that
bŒ≤X
+ (N, 0, b) ‚â§
N

i=1
œÜi(Xi ‚àíXi‚àí1).
Let us note that
{œÜi = 1} = ‚à™non‚àíeven m({œÑm < i}  {œÑm+1 < i}) ‚ààFi‚àí1,
and therefore by properties of conditional expected values and submartingales

6.3
Martingales on inÔ¨Ånite time interval
59
bEŒ≤X
+ (N, 0, b) ‚â§E
N

i=1
œÜi(Xi ‚àíXi‚àí1)
=
N

i=1
‚à´
{œÜi }
(Xi ‚àíXi‚àí1)dP
=
N

i=1
‚à´
{œÜi }
E(Xi ‚àíXi‚àí1|Fi‚àí1)dP
‚â§
N

i=1
EE(Xi ‚àíXi‚àí1|Fi‚àí1)
=(EX1 ‚àíEX0) + (EX2 ‚àíEX1) + . . . + (EXN ‚àíEXN‚àí1) = EXN.
and we arrive to (6.12), and further to (6.10).
‚ñ°
Corollary 6.1 Let (Xn)n=0,1,...,N be a supermartingale and Œ±X
+ (N, a, b) be the num-
ber of upcrossings of the interval (a, b). Then
EŒ±X
+ (N, a, b) ‚â§E(a ‚àíXN)+
b ‚àía
.
(6.13)
Proof The inequality (6.13) follows from (6.11) because Œ±X
+ (N, a, b) can be inter-
preted as the number of downcrossings of the interval (‚àíb, ‚àía) by the submartingale
(‚àíXn)n=0,1,...,N :
EŒ±X
+ (N, a, b) = EŒ≤‚àíX
‚àí(N, ‚àíb, ‚àía) ‚â§E(‚àíXN ‚àí(‚àía))+
(‚àía) ‚àí(‚àíb)
= E(a ‚àíXN)+
b ‚àía
.
‚ñ°
6.3
Martingales on inÔ¨Ånite time interval
Here
martingales
(submartingales,
supermartingales)
are
studied
for
Z+ = {0, 1, . . ., n, . . .}. Statements of Theorem 6.3 and Theorem 6.4 are transformed
by taking the limits as N ‚Üí‚àû, and we get the following list of inequalities (6.14)-
(6.19):
P(œâ : sup
n
Xn ‚â•Œª) ‚â§supn EX+
n
Œª
, Œª > 0,
(6.14)
for a submartingale (Xn)n=0,1,... with supn EX+
n < ‚àû;
P(œâ : sup
n
Xn ‚â•Œª) ‚â§EX0 + supn EX‚àí
n
Œª
, Œª > 0,
(6.15)

60
6
Discrete time stochastic analysis: basic results
for a supermartingale (Xn)n=0,1,... with supn EX‚àí
n < ‚àû;
P(œâ : sup
n
|Xn| ‚â•Œª) ‚â§supn E|X|n
Œª
, Œª > 0,
(6.16)
for a martingale (Xn)n=0,1,...,N with supn E|X|n < ‚àû;
EŒ≤X
+ (‚àû, a, b) ‚â§supn E(Xn ‚àía)+
b ‚àía
,
(6.17)
for a submartingale (Xn)n=0,1,... with supn E(Xn ‚àía)+ < ‚àû;
EŒ≤X
‚àí(‚àû, a, b) ‚â§supn E(Xn ‚àíb)+
b ‚àía
.
(6.18)
for a submartingale (Xn)n=0,1,... with supn E(Xn ‚àíb)+ < ‚àû;
EŒ±X
+ (‚àû, a, b) ‚â§supn E(a ‚àíXn)+
b ‚àía
.
(6.19)
for a supermartingale (Xn)n=0,1,... with supn E(a ‚àíXn)+ < ‚àû.
The Doob optional stopping (sampling) theorem admits a natural extension to Z+
too.
Theorem 6.5 Let (Xn)n=0,1,... be a uniformly integrable martingale (submartingale,
supermartingale) and œÑ be a Ô¨Ånite stopping time. Then E|XœÑ| < ‚àûand
EXœÑ = EX0 (EXœÑ ‚â•EX0, EXœÑ ‚â§EX0).
(6.20)
Moreover, for Ô¨Ånite stopping times œÑ ‚â§œÉ (a.s.):
E(XœÉ|FœÑ) = XœÑ (a.s.), (E(XœÉ|FœÑ) ‚â•XœÑ (a.s.), E(XœÉ|FœÑ) ‚â§XœÑ (a.s.)). (6.21)
Proof Both formulas (6.20) and (6.21) are proved with the help of limit arguments.
Let us only provide the proof of (6.20).
For a Ô¨Åxed N denote œÑN = œÑ ‚àßN which is a bounded stopping time, and by
Theorem 6.2 we have
EX0 = EXœÑN .
(6.22)
Let us note from (6.22) that
EXœÑN = 2EX+
œÑN ‚àíEXœÑN ‚â§2EX+
œÑN ‚àíEX0.
(6.23)
Further, using (6.23) and a submartingale property of (X+
n )n=0,1,... we obtain that

6.3
Martingales on inÔ¨Ånite time interval
61
EX+
œÑN =
N

j=0
EX+
j I{œÑN =j } +
‚à´
{œÑ>N }
X+
N dP
‚â§
N

j=0
EX+
j I{œÑN =j } + EX+
N I{œÑ>N }
=EX+
N ‚â§E|XN | ‚â§sup
n
E|Xn|,
and, hence,
E|XœÑN | ‚â§3 sup
n
E|Xn| < ‚àû.
(6.24)
Using (6.24) and the Fatou lemma we get
E|XœÑ| ‚â§lim sup
N
E|XœÑN | ‚â§4 sup
n
E|Xn| < ‚àû.
Let us note that for a Ô¨Ånite stopping time œÑ we have
lim
n‚Üí‚àûE|Xn|I{œÑ>n} = 0,
(6.25)
because (Xn)n=0,1,... is uniformly integrable and P(œâ : œÑ(œâ) > n) ‚Üí0, n ‚Üí‚àû.
We can write a decomposition
XœÑ = XœÑ‚àßn + (XœÑ ‚àíXn)I{œÑ>n},
and its average
EXœÑ = EXœÑ‚àßn + EXœÑI{œÑ>n} ‚àíEXnI{œÑ>n},
(6.26)
In the equation (6.26) EXœÑ‚àßn = EX0, because (XœÑ‚àßn) is a martingale.
The second term of (6.26)
E(XœÑ)I{œÑ>n} =
‚àû

i=n+1
EXiI{œÑ=i} ‚Üí0, n ‚Üí‚àû,
because the series
EXœÑ =
‚àû

i=0
EXiI{œÑ=i} converges.
The third term of (6.26) converges to zero because (6.25).
‚ñ°
Let us prove a key theorem of Doob about (a.s.)-convergence of submartingales.
Theorem 6.6 Let (Xn)n=0,1,... be a submartingale satisfying condition
sup
n
E|Xn| < ‚àû.
(6.27)
Then there exists an integrable random variable X‚àû= limn‚Üí‚àûXn (a.s.).

62
6
Discrete time stochastic analysis: basic results
Proof Assume that this limit does not exist. It means that
P(œâ : lim sup
n‚Üí‚àû
Xn(œâ) > lim inf
n‚Üí‚àûXn(œâ)) > 0.
(6.28)
The set in (6.28) can be written
{œâ : lim sup
n
Xn > lim inf
n
Xn} = ‚à™a<b,a,b‚ààQ{œâ : lim sup
n
Xn > b > a > lim inf
n
Xn},
(6.29)
where Q ‚äÜR1 is the set of rational numbers.
It follows from (6.28)-(6.29) that for some rational numbers a < b we have
P(œâ : lim sup
n
Xn > b > a > lim inf
n
Xn) > 0.
(6.30)
We also note that for each n ‚ààZ
EX+
n ‚â§E|Xn| = 2EX+
n ‚àíEXn ‚â§2EX+
n ‚àíEX0,
and Ô¨Ånd that for submartingale (Xn)n‚ààZ the following conditions are equivalent:
sup
n
E|Xn| < ‚àû‚áîsup
n
EX+
n < ‚àû.
(6.31)
Applying (6.17) together with (6.31) we obtain
EŒ≤X
+ (‚àû, a, b) ‚â§supn EX+
n + |a|
b ‚àía
< ‚àû.
(6.32)
Condition (6.32) is a contradiction with the assumption (6.30) and (6.28). Hence,
there exists X‚àû= limn‚Üí‚àûXn (a.s.) which is integrable by the Fatou lemma and
E|X‚àû| ‚â§supn E|Xn| < ‚àû.
‚ñ°
Corollary 6.2 (a) If (Xn)n=0,1,... is a non-negative martingale, then supn E|Xn| =
supn EXn = EX0 < ‚àûand therefore there exists integrable X‚àû= limn‚Üí‚àûXn (a.s.).
(b) If (Xn)n=0,1,... is a non-positive submartingale, then there exists integrable
X‚àû= limn‚Üí‚àûXn (a.s.) and (Xn, Fn)n=0,1,...,‚àûis also submartingale, where F‚àû=
œÉ(‚à™‚àû
n=1Fn).
Proof Statement (a) is obvious. In case (b) one can apply the Fatou lemma and get
EX‚àû= E lim
n Xn ‚â•lim sup
n
EXn ‚â•EX0 > ‚àí‚àû.
The submartingale property follows from relations:
E(X‚àû|Fm) = E(lim
n Xn|Fm) ‚â•lim sup
n
E(Xn|Fm) ‚â•Xm (a.s.), m = 0, 1, . . . .
‚ñ°
To study another type of convergence of martingales (submartingales, supermartin-
gales) which is L1-convergence we give the following example.

6.3
Martingales on inÔ¨Ånite time interval
63
Example 6.1 Let (Yn)n=0,1,... be a sequence of independent random variables such
that
Yn =

2
with probability 1/2,
0
with probability 1/2.
DeÔ¨Åne Xn = n
i=0 Yi and Fn = œÉ(Y0, . . .,Yn). It is clear that (Xn)n=0,1,... is a martin-
gale with EXn = 1, and hence, Xn ‚ÜíX‚àû= 0 (a.s.).
At the same time we have E|Xn ‚àíX‚àû| = EXn = 1 and therefore, this martingale
does not converge in space L1.
It means that the condition supn E|Xn| < ‚àû, appeared in Doob‚Äôs theorem about
(a.s.)-convergence, is not enough to achieve L1-convergence.
Theorem 6.7 If (Xn)n=0,1,... is a uniformly integrable submartingale, then there exi-
sts an integrable random variable X‚àûsuch that Xn ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX‚àû(a.s.) and in space L1.
Moreover, (Xn)n=0,1,...,‚àûwill be a submartingale with respect to (Fn)n=0,1,...,‚àû,
where F‚àû= œÉ(‚à™‚àû
n=1Fn).
Proof Existence of X‚àûand convergence (a.s.) follows from Theorem 6.6. Further,
take c > 0 and represent
EXn = EXnI{Xn<‚àíc} + EXnI{Xn ‚â•‚àíc}.
(6.33)
For any œµ > 0 due to uniform integrability (Xn) we can choose c > 0 large enough
to provide inequality
sup
n
|EXnI{Xn<‚àíc}| < œµ,
(6.34)
for the Ô¨Årst term in (6.33).
We also note that
XnI{Xn ‚â•‚àíc} ‚â•Xn,
and by the Fatou lemma we obtain that
lim inf
n
EXnI{Xn ‚â•‚àíc} ‚â•E lim inf
n
XnI{Xn ‚â•‚àíc} ‚â•E lim inf
n
Xn.
Hence, due to (6.34) we get from the above inequalities
lim inf
n
EXn ‚â•E lim inf
n
Xn ‚àíœµ.
We can prove in a similar way
lim sup
n
EXn ‚â§E lim sup
n
Xn,
and further just repeat the same steps of the proof of the Lebesgue dominated
convergence theorem and Ô¨Ånd that E|Xn ‚àíX‚àû| ‚Üí0, n ‚Üí‚àû.
To Ô¨Ånish the proof we take A ‚ààFn, and for m ‚â•n we have
EIA|Xm ‚àíX‚àû| ‚Üí0, m ‚Üí‚àû,

64
6
Discrete time stochastic analysis: basic results
‚à´
A
XmdP ‚Üí
‚à´
A
X‚àûdP, m ‚Üí‚àû,
{
‚à´
A XmdP}m‚â•n is non-decreasing.
Hence, for any A ‚ààFn we have
‚à´
A
XndP ‚â§
‚à´
A
XmdP ‚â§
‚à´
A
X‚àûdP =
‚à´
A
E(X‚àû|Fn)dP,
which certiÔ¨Åes a submartingale property.
‚ñ°
Corollary 6.3 Let (Xn)n=0,1,... be a submartingale with supn E|Xn|p < ‚àû, p > 1.
Then there exists an integrable random variable X‚àûsuch that Xn ‚ÜíX‚àû, n ‚Üí‚àû,
(a.s.) and in L1.
Theorem 6.8 (Levy‚Äôs theorem). Consider an integrable random variable X on
stochastic basis (Œ©, F, (F )n=0,1,..., P). Then
E(X|Fn) ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûE(X|F‚àû) (a.s.) and in L1.
Proof Denote Xn = E(X|Fn), n = 0, 1, . . . and Ô¨Ånd for a > 0 and b > 0 that
‚à´
{|Xn |‚â•a
|Xn|dP ‚â§
‚à´
{|Xn |‚â•a
E(|Xn||Fn)dP =
‚à´
{|Xn |‚â•a}
|X|dP
=
‚à´
{|Xn |‚â•a}‚à©{|Xn |‚â§b}
|X|dP +
‚à´
{|Xn |‚â•a}‚à©{|Xn |>b}
|X|dP
‚â§bP(|Xn| ‚â•a) +
‚à´
{{|Xn |>b}
|X|dP
‚â§b
aE|X| +
‚à´
{{|Xn |>b}
|X|dP.
So, taking b ‚Üí‚àûand after a ‚Üí‚àûwe get
lim
a‚Üí‚àûsup
n
E|Xn|I{|Xn |‚â•a} = 0,
which means that (Xn)n=0,1,... is uniformly integrable. Applying Theorem 6.7 we get
the statement of this theorem.
‚ñ°
Corollary 6.4 A uniformly integrable stochastic sequence (Xn)n=0,1,... on stochastic
basis (Œ©, F, (Fn)n=0,1,..., P) is a martingale ‚áîthere exists an integrable random
variable X such that Xn = E(X|Fn), n = 0, 1, . . ..
Proof The inverse implication ‚áêis just the Levy theorem (Theorem 6.8). The direct
implication ‚áíis just a consequence of Theorem 6.7 if we take X = X‚àû.
‚ñ°
Problem 6.2 In Levy‚Äôs theorem prove that
X‚àû= E(X|F‚àû).

Chapter 7
Discrete time stochastic analysis: further
results and applications
Abstract In this chapter a characterization of sets of convergence of martingale is
given in predictable terms. As a consequence, the strong LNL for square-integrable
martingales is proved. This result is applied for derivation of strong consistency
of the least-squared estimates in the framework of regression model with martin-
gale errors. Moreover, the CLT for martingales is stated, and further this theorem
together with the martingale LNL is applied to derive the asymptotic normality
and strong consistency of martingale stochastic approximation procedures. A dis-
crete version of the Girsanov theorem is given here with its further application for
derivation of a discrete time Bachelier option pricing formula. In the last section,
the notion of a martingale is extended in several directions: from asymptotic mar-
tingales and local martingales to martingale transforms and generalized martingales
(see [4], [8], [12], [13], [15], [26], [30], [34], and [40]).
7.1
Limiting behavior of martingales with statistical
applications
Let us investigate limiting behavior of martingales. We start with some facts about
sets of convergence of martingales and submartingales.
DeÔ¨Ånition 7.1 Let (Xn)n=0,1,... be a stochastic sequence. Denote {œâ : Xn ‚Üí} the
set of those œâ ‚ààŒ© such that Xn(œâ) converges to a Ô¨Ånite limit as n ‚Üí‚àû.
We also say that A ‚äÜB ‚ààF (a.s.), if P(A ‚à©Bc) = 0, and A = B (a.s.) if A ‚äÜB
(a.s.) and B ‚äÜA (a.s.).
Lemma 7.1 Let (Xn)n=0,1,... be a square integrable martingale. Then (a.s.)
{œâ : ‚ü®X, X‚ü©‚àû< ‚àû} ‚äÜ{œâ : Xn ‚Üí}
(7.1)
Proof For a positive a ‚ààR we deÔ¨Åne a stopping time
œÑa = inf{n : ‚ü®X, X‚ü©n+1 ‚â•a},
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_7
65

66
7
Discrete time stochastic analysis: further results and applications
assuming that inf{‚àÖ} = ‚àû. One can observe that ‚ü®X, X‚ü©œÑa ‚â§a and the stopped
sequence (XœÑa
n )n=0,1,... = (XœÑa‚àßn)n=0,1,... is a square integrable martingale such that
E(XœÑa‚àßn)2 = E(XœÑa
n )2 = E‚ü®XœÑn, XœÑn‚ü©n = E‚ü®X, X‚ü©œÑa‚àßn ‚â§a.
According to Theorem 6.6, P{œâ : XœÑn
n
‚Üí} = 1. We also note that XœÑn
n
= XœÑa‚àßn = Xn
on the set {œÑa = ‚àû} = {œâ : ‚ü®X, X‚ü©‚àû‚â§a}.
Further, (a.s.) {œâ : œÑa = ‚àû} ‚äÜ{œâ : Xn ‚Üí} and ‚à™a>0{œÑa = ‚àû} ‚äÜ{œâ : Xn ‚Üí}.
Hence, (a.s.)
{œâ : ‚ü®X, X‚ü©‚àû<‚àû}= ‚à™a>0 {œâ : ‚ü®X, X‚ü©‚àû‚â§a} = ‚à™a>0{œâ : œÑa = ‚àû} ‚äÜ{œâ : Xn ‚Üí},
and we get (7.1).
‚ñ°
Lemma 7.2 (Stochastic Kronecker‚Äôs lemma). Let (An)n=0,1,..., A0 > 0, be a pre-
dictable non-decreasing sequence, (Mn)n=0,1,... be a square integrable martingale
and
Nn =
n

i=0
A‚àí1
i ŒîMi.
Then (a.s.)
Œ©‚Ä≤ = {œâ : A‚àû= ‚àû} ‚à©{œâ : Nn ‚Üí} ‚äÜ{œâ : A‚àí1
n Mn ‚Üí}.
(7.2)
Proof First of all, we note the following formula of ‚Äúsummation by parts‚Äù:
AnXn = A0X0 +
n

i=1
AiŒîXi +
n

i=1
Xi‚àí1ŒîAi,
(7.3)
for two stochastic sequences (An)n=0,1,... and (Xn)n=0,1,....
Applying the formula (7.3) to (An)n=0,1,... and (Nn)n=0,1,... we represent
n

i=1
AiŒîNi = AnNn ‚àí
n

i=1
Ni‚àí1ŒîAi,
and using n
i=1 AiŒîNi = Mn we obtain
A‚àí1
n Mn = A‚àí1
n
n

i=1
AiŒîNi = A‚àí1
n

AnNn ‚àí
n

i=1
Ni‚àí1ŒîAi

= A‚àí1
n
n

i=1
(Nn ‚àíNi‚àí1)ŒîAi.
(7.4)
Further, for œµ > 0, deÔ¨Åne
nœµ =

sup{i : |N‚àû‚àíNi‚àí1| ‚â§œµ}
on the set {Nn ‚ÜíN‚àû},
0,
on Œ© \ {Nn ‚ÜíN‚àû}.

7.1
Limiting behavior of martingales with statistical applications
67
By (7.4) on the set {œâ : Nn ‚ÜíN‚àû} we have
|A‚àí1
n Mn| =|A‚àí1
n
n

i=1
(Nn ‚àíNi‚àí1)ŒîAi|
‚â§A‚àí1
n |
n

i=1
(Nn ‚àíNi‚àí1)ŒîAi|
‚â§A‚àí1
n
n‚àßnœµ

i=1
|Nn ‚àíNi‚àí1|ŒîAi +
n

i=n‚àßnœµ
[|N‚àû‚àíNn| + |N‚àû‚àíNi‚àí1|] ŒîAi

‚â§A‚àí1
n

const.Anœµ sup
i
|Ni| + const.An|N‚àû‚àíNn| + const.œµ An

.
(7.5)
Hence, for almost all œâ ‚ààŒ©‚Ä≤ one can Ô¨Ånd n‚Ä≤
œµ(œâ) such that in (7.5) for n ‚â•n‚Ä≤
œµ(œâ) :
|A‚àí1
n Mn(œâ)| ‚â§const.(œâ) ¬∑ œµ, and therefore (7.2).
‚ñ°
Theorem 7.1 (Strong LNL for martingales). Let (An)n=0,1,... be non-negative pre-
dictable non-decreasing sequence and (Mn)n=0,1,..., M0 = 0, be a square integrable
martingale such that (a.s.) A‚àû= ‚àûand (a.s.)
‚àû

n=1
Œî‚ü®M, M‚ü©n
A2n
< ‚àû.
(7.6)
Then A‚àí1
n Mn ‚Üí0 (a.s.), n ‚Üí‚àû.
Proof Due to assumptions (7.6) and Lemma 7.1 we observe in Lemma 7.2 P(Œ©‚Ä≤) = 1,
and, hence, we get the statement of the theorem.
‚ñ°
Corollary 7.1 (Strong LNL of Kolmogorov). Let (Yn)n=1,2,... be a sequence of inde-
pendent random variables with EYn = 0 and œÉ2
n = EY2
n such that
‚àû

n=1
œÉ2
n
n2 < ‚àû.
(7.7)
Then for a sequence Xn = n
i=1 Yi the following strong large numbers law is true:
(a.s.) Xn
n ‚Üí0, n ‚Üí‚àû.
Proof We put An = n, Mn = Xn and Ô¨Ånd that the Kolmogorov variance condition
(7.7) is transformed to the condition (7.6) of Theorem 7.1 and we get the claim.
‚ñ°
Let us give an interesting and valuable application of strong LNL for martingales to
Regression Analysis.
Example 7.1 Suppose the observations are performed at times n = 0, 1, . . . and obey
the formula
xn = fnŒ∏ + en,
(7.8)

68
7
Discrete time stochastic analysis: further results and applications
where Œ∏ ‚ààR is an unknown parameter, (en)n=0,1,... is a martingale-diÔ¨Äerence and
( fn)n=0,1,... is a predictable regressor.
One can consider a structural least squares estimate (LS-estimate) in the frame-
work of regression model (7.8):
Œ∏n =
	 n

i=0
f 2
i

‚àí1
n

i=0
fixi.
(7.9)
Denote Dn = E(e2
n|Fn‚àí1) and Fn = n
i=0 f 2
i and assume that Fn ‚Üí‚àû(a.s.), n ‚Üí‚àû,
and ‚àû
i=0 F‚àí2
i
Di < ‚àû(a.s.). Then according to Theorem 7.1 LS-estimate (7.9) is
strongly consistent in the sense that Œ∏n ‚ÜíŒ∏ (a.s.), n ‚Üí‚àû.
Remark 7.1 If in the model (7.8) we take fn = xn‚àí1, then we arrive to the Ô¨Årst
order autoregression model for which the condition ‚àû
1 x2
i‚àí1 = ‚àûis well-known in
Regression Analysis as a standard guarantee for consistency of LS-estimates.
Below we give some additional facts about asymptotic behavior of martingales and
submartingales.
DeÔ¨Ånition 7.2 A stochastic sequence (Xn)n=0,1,... ‚ààC+, if for each œÑa = inf{n ‚â•
0 : Xn > a}, a > 0 :
E(ŒîXœÑa)+I{œâ:œÑa<‚àû} < ‚àû.
Problem 7.1 Prove that the condition C+ is true for any stochastic sequence with
E supn |ŒîXn| < ‚àû.
Theorem 7.2 a) Assume a submartingale with its Doob decomposition Xn = X0 +
An + Mn, n = 0, 1, . . . belongs to a class C+, then (a.s.)
{Xn ‚Üí} = {sup
n
Xn < ‚àû} ‚äÜ{A‚àû< ‚àû}.
b) Assume a submartingale Xn = X0 + An + Mn, n = 0, 1, . . . is non-negative,
then (a.s.)
{A‚àû< ‚àû} ‚äÜ{Xn ‚Üí} ‚äÜ{sup
n
Xn < ‚àû}.
c) If a non-negative submartingale belongs to C+, then (a.s.)
{A‚àû< ‚àû} = {Xn ‚Üí} = {sup
n
Xn < ‚àû}.
d) Let (Yn)n=0,1,... be a non-negative stochastic sequence with the structure
Yn = Y0 + A‚Ä≤
n + Mn ‚àíA2
n, n = 0, 1, . . .,
where (A‚Ä≤
n)n=0,1,... and (A2
n)n=0,1,... are non-decreasing predictable sequences and
(Mn)n=0,1,... is a martingale. Then (a.s.)
{A‚Ä≤
‚àû< ‚àû} ‚äÜ{Yn ‚Üí} = {sup
n
A2
‚àû< ‚àû}.

7.1
Limiting behavior of martingales with statistical applications
69
Proof a) It is clear that (a.s.) {Xn ‚Üí} ‚äÜ{supn Xn < ‚àû}. To prove the inverse inclu-
sion, we have
sup
n
EX+
œÑa‚àßn ‚â§a + EX+
œÑa IœÑa<‚àû‚â§2a + E(ŒîXœÑa)+I{œÑa<‚àû} < ‚àû.
Hence, the stopped submartingale (XœÑa
n )n=0,1,... = (XœÑa‚àßn)n=0,1,... converges (a.s.) as
n ‚Üí‚àû, and {œÑa = ‚àû} ‚äÜ{Xn ‚Üí} (a.s.).
Due to ‚à™a>0{œÑa = ‚àû} = {supn Xn < ‚àû} we get
{sup
n
Xn < ‚àû} = {Xn ‚Üí} (a.s.).
b) DeÔ¨Åne œÉa = inf(n : An+1 > a), a > 0, and get
EXn‚àßœÉa = EAn‚àßœÉa ‚â§a.
Hence supn EXn‚àßœÉa ‚â§a < ‚àû, and due to positivity of (Xn)n=0,1,... we can apply the
Doob convergence theorem and Ô¨Ånd that
{A‚àû‚â§a} ‚äÜ{œÉa = ‚àû} = {Xn ‚Üí} (a.s.)
and therefore,
{A‚àû< ‚àû} = ‚à™a>0{A‚àû‚â§a} ‚äÜ{Xn ‚Üí} (a.s.).
c) is a combination of (a) and (b).
d) Let us rewrite Yn = Xn ‚àíA2
n, n = 0, 1, . . . where Xn = X0 + A‚Ä≤
n + Mn.X0 = Y0,
is a non-negative submartingale due to 0 ‚â§Yn = Xn ‚àíA2
n, 0 ‚â§A2
n ‚â§Xn (a.s.), and
(a.s.) {A‚Ä≤
‚àû< ‚àû} ‚äÜ{Xn ‚Üí} ‚äÜ{A2
‚àû< ‚àû} by (b). Hence, (a.s.) {A‚Ä≤
‚àû< ‚àû} ‚äÜ{Yn ‚Üí
} ‚à©{A2
‚àû< ‚àû}.
‚ñ°
Corollary 7.2 Assume a martingale (Xn)n=0,1,... satisÔ¨Åes to the following condition
E supn |ŒîXn| < ‚àû. Then (a.s.)
Œ© = {Xn ‚Üí} ‚à™{lim inf
n
Xn = ‚àí‚àû, lim sup
n
Xn = +‚àû}.
(7.10)
Proof We apply Theorem 7.2 to ¬±(Xn)n=0,1,... and get (a.s.)
{lim sup
n
Xn < ‚àû} = {sup
n
Xn < ‚àû} = {Xn ‚Üí},
{lim inf
n
Xn > ‚àí‚àû} = {inf
n Xn > ‚àí‚àû} = {Xn ‚Üí},
which leads to (7.10).
‚ñ°
We already studied the Large Numbers Law for martingales and a very nice gen-
eralization of the corresponding results for sums of independent random variables
observed.
A natural question arises.

70
7
Discrete time stochastic analysis: further results and applications
Is it possible to get a similar version of the Central Limit Theorem (CLT) for
martingales?
The next theorem gives a positive answer to this question.
Theorem 7.3 Let(Xn)n=0,1,...beamartingale-diÔ¨Äerencesuchthat(a.s.)E(X2
n|Fn‚àí1) =
1 and E(|Xn|3|Fn‚àí1) ‚â§C < ‚àû. Then for a martingale Mn = n
i=1 Xi the CLT is true:
Yn = Mn
‚àön
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûZ ‚àºN(0, 1)
Proof Here, as in Chapter 4 for a random variable X we deÔ¨Åne its characteristic
function œÜX(t) = Eeitx, t ‚ààR, and denote
œÜn,j(t) = E

eit
Xj
‚àön |Fj‚àí1

.
Let us write the Taylor decomposition
eit
Xj
‚àön = 1 + it Xj
‚àön ‚àít2
2n X2
j ‚àí
it3
6n3/2 ¬ØX3
j ,
(7.11)
with a random reminder bounded as follows 0 ‚â§¬ØXj ‚â§|Xj|.
Taking the conditional expectation in (7.11) and exploiting a martingale-
diÔ¨Äerence assumption we obtain the corresponding representation for œÜn,j(t) :
œÜn,j(t) = 1 + it E(Xj|Fj‚àí1)
‚àön
‚àít2
2nE(X2
j |Fj‚àí1) ‚àí
it3
6n3/2 E( ¬ØX3
j |Fj‚àí1).
(7.12)
It follows from (7.12) that
œÜn,j(t) ‚àí1 ‚àít2
2n =
t3
6n3/2 E( ¬ØX3
j |Fj‚àí1).
Hence, for m ‚â§n we obtain from (7.10) that
Eeit Mm
‚àön = Eeit Mm‚àí1
‚àön eit Xm
‚àön =E

eit Mm‚àí1
‚àön E

eit Xm
‚àön |Fm‚àí1

=Eeit Mm‚àí1
‚àön œÜn,m(t)
=Eeit Mm‚àí1
‚àön

1 ‚àít2
2n ‚àí
it3
6n3/2 ¬ØX3
j

.
(7.13)
Let us rewrite (7.11) as follows
E

eit Mm
‚àön ‚àí(1 ‚àít2
2neit Mm‚àí1
‚àön

= Eeit Mm‚àí1
‚àön
it3
6n3/2 ¬ØX3
j .
(7.14)
Using the boundedness of the third moment of (Xn)n=0,1,... we derive from (7.13)
that

7.1
Limiting behavior of martingales with statistical applications
71
E

eit Mm
‚àön ‚àí(1 ‚àít2
2n)eit Mm‚àí1
‚àön

‚â§E
eit Mm‚àí1
‚àön

|t|3
6n3/2 E

|Xm|3|Fm‚àí1

‚â§c |t|3
6n3/2 .
(7.15)
Let us Ô¨Åx t ‚ààR and choose large enough n ‚â•t2
2 to provide the following inequality
0 ‚â§1 ‚àít2/2n ‚â§1.
For such t and n we obtain from (7.13)-(7.15) that
(1 ‚àít2
2n)n‚àímEeit Mm‚àí1
‚àön
‚àí(1 ‚àít2
2n)n‚àím+1Eeit Mm‚àí1
‚àön
 ‚â§c |t|3
6n3/2 .
(7.16)
Let us note that
Eeit Mm
‚àön ‚àí(1 ‚àít2
2n)n =
n

m=1

(1 ‚àít2
2n)n‚àímEeit Mm‚àí1
‚àön
‚àí(1 ‚àít2
2n)n‚àím+1Eeit Mm‚àí1
‚àön

.
(7.17)
Relations (7.16)-(7.17) for n ‚â•t2/2 lead to the following inequality
Eeit Mm
‚àön ‚àí(1 ‚àít2
2n)n
 ‚â§nC |t|3
6n3/2 = C |t|3
6‚àön.
(7.18)
Due to the right hand side of (7.18) tends to zero, now one can conclude that
lim
n‚Üí‚àûEeit Mm
‚àön = e‚àít2/2,
(7.19)
where we used the well-known fact that
lim
n‚Üí‚àû

1 ‚àít2
2n
n
= e‚àít2/2.
Relation (7.19) shows that characteristic functions ofYn converge to the characteristic
function of N(0, 1). Hence, Yn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûZ ‚àºN(0, 1).
‚ñ°
Another area of applications of a martingale LNL and a martingale CLT is
stochastic approximation algorithms. The classical theory of stochastic approxima-
tion is concerned with the problem to construct a stochastic sequence (Œ∏n)n=0,1,... that
converges in some probabilistic sense to a unique root Œ∏ of the regression equation
R(Œ∏) = 0, Œ∏ ‚ààR,
(7.20)
where R is a regression function.
An approximate solution of (7.20) is given by the Robbins-Monro procedure
Œ∏n = Œ∏n‚àí1 ‚àíŒ≥nyn,
(7.21)

72
7
Discrete time stochastic analysis: further results and applications
with the sequence (yn) satisfying to equation
yn = R(Œ∏n‚àí1) + en,
(7.22)
where (en)n=0,1,... are errors of observations usually modeled by a sequence of
independent random variables with mean zero and bounded variance, (Œ≥n)n=0,1,... is
a sequence of positive real numbers converging to 0.
The convergence of the procedure (7.21)-(7.22) for a continuous linearly bounded
regression function R(x) with R(x)(x ‚àíŒ∏) > 0 for all x ‚ààR is guaranteed by the
following classical conditions:
‚àû

1
Œ≥k = ‚àû,
(7.23)
‚àû

1
Œ≥2
k < ‚àû.
(7.24)
Let us demonstrate how the martingale technique and convergence work here and
how classical stochastic approximation results are extended to models (7.21)-(7.22)
with martingale errors (en)n=0,1,... and predictable sequences (Œ≥n)n=0,1,....
Example 7.2 We consider for simplicity the case of linear function R(x) = Œ≤(x ‚àí
Œ∏), Œ≤ > 0, assuming in (7.22) that (en)n=0,1,... is a martingale-diÔ¨Äerence with Een = 0
and E(e2
n|Fn‚àí1) ‚â§Œæ < ‚àû(a.s.). Regarding (Œ≥n) we assume that (Œ≥n) is predictable,
0 < Œ≥n ‚â§Œ≤‚àí1 (a.s.) and conditions (7.23)-(7.24) are fulÔ¨Ålled almost surely. One can
rewrite the algorithm (7.21)-(7.22) as follows (Œ∏ = 0 for simplicity)
ŒîŒ∏n = ‚àíŒ≥nŒ≤Œ∏n‚àí1 ‚àíŒ≥nen.
(7.25)
Moreover, (7.25) can be rewritten in the form of an inhomogeneous linear stochastic
diÔ¨Äerential equation (6.9) with Xn = Œ∏n, ŒîNn = ‚àíŒ≥nen, ŒîUn = ‚àíŒ≤Œ≥n, and En(‚àíŒ≤Œ≥)
is a stochastic exponential of sequence (Un).
In these denotations the solution of (7.25) is expressed as follows
Œ∏n = Xn =En(U)X0 + En(U)
n

1
E‚àí1
i (U)ŒîNi
=En(‚àíŒ≤Œ≥)Œ∏0 + En(‚àíŒ≤Œ≥)
n

1
E‚àí1
i (‚àíŒ≤Œ≥)Œ≥iei.
(7.26)
According to the assumption Œ≥n < Œ≤‚àí1 (a.s.) the stochastic exponential En(‚àíŒ≤Œ≥) =
n
1(1 ‚àíŒ≤Œ≥i) is positive (a.s.) if (7.23) is fulÔ¨Ålled (a.s.).
Further, the Ô¨Årst term of the right hand side of (7.26) tends to zero (a.s.), n ‚Üí‚àû,
because En(‚àíŒ≤Œ≥) ‚Üí0 (a.s.), n ‚Üí‚àû.
The structure of the second term of the right hand side of (7.26) is exactly
(see Theorem 7.1) as in the strong LNL for martingales with A‚àí1
n = En(‚àíŒ≤Œ≥) and
Mn = n
1 E‚àí1
i (‚àíŒ≤Œ≥)Œ≥iei.

7.1
Limiting behavior of martingales with statistical applications
73
So, we have to check the condition (7.6) in this case:
‚àû

1
A‚àí2
n Œî‚ü®M, M‚ü©n =
n

1
E2
n(‚àíŒ≤Œ≥)E‚àí2
n (‚àíŒ≤Œ≥)Œ≥2
nE(e2
n|Fn‚àí1) ‚â§Œæ
‚àû

1
Œ≥2
n < ‚àû(a.s.)
Applying Theorem 7.1 we get the convergence Œ∏n ‚ÜíŒ∏ (a.s.), n ‚Üí‚àû.
Let us show how the martingale CLT (see Theorem 7.3) works to study asymptotic
normality properties of algorithms (7.21)-(7.22). Under some reasonable simpliÔ¨Å-
cations, we will show it to avoid technical diÔ¨Éculties.
Example 7.3 We consider the linear model (7.21)-(7.22) assuming Œ≥n = Œ±
n, Œ± >
0, en ‚àºN(0, œÉ2) and independent, and n must be greater than Œ±Œ≤ to provide a
positivity of En(‚àíŒ≤Œ≥).
Problem 7.2 Prove that under the assumptions above
En(‚àíŒ≤Œ≥) ‚â°En(‚àíŒ≤Œ±) ‚àºn‚àíŒ≤Œ±, n ‚Üí‚àû.
(7.27)
To derive an asymptotic normality of the procedure (Œ∏n) we multiply (7.26) by n1/2
and obtain
‚àönŒ∏n = ‚àönEn(‚àíŒ≤Œ±)Œ∏0 ‚àí‚àönEn(‚àíŒ≤Œ±)
n

1
Œ±
k E‚àí1
k (‚àíŒ≤Œ±)ek.
(7.28)
It follows from (7.27) that ‚àönEn(‚àíŒ≤Œ±) ‚àºn1/2‚àíŒ≤Œ±, and, hence, under additional
assumption 2Œ≤Œ± > 1 we Ô¨Ånd the Ô¨Årst term of the right side of (7.28) tends to zero
(a.s.), n ‚Üí‚àû. The second term of (7.28) has a normal distribution. Therefore, we
need to calculate its limiting variance.
As n ‚Üí‚àûwe have
nE2
n(‚àíŒ≤Œ±)
n

1
E‚àí2
k (‚àíŒ≤Œ±)Œ±2œÉ2
k2
‚Üí
Œ±2œÉ2
(2Œ≤Œ± ‚àí1).
(7.29)
Thus we obtain from (7.29) that
‚àönŒ∏n
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûN

0,
Œ±2œÉ2
2Œ≤Œ± ‚àí1

.
Remark 7.2 One can develop this approach to a non-linear regression functions
R(x) = Œ≤(x ‚àíŒ∏) + U(x), where U(x) = O((x ‚àíŒ∏)2), and prove both strong consis-
tency and asymptotic normality of stochastic approximation procedures (7.21)-
(7.22). We note also that Lemma 7.2 works for such extension.

74
7
Discrete time stochastic analysis: further results and applications
7.2
Martingales and absolute continuity of measures.
Discrete time Girsanov theorem and its Ô¨Ånancial
application
We start this section with a martingale characterization of absolute continuity
of a probability measure ÀúP with respect to measure P on given stochastic basis
(Œ©, F, (Fn)n=0,1,..., P). It was already shown in Section 6.1 that under assumption
of ‚Äúlocal absolute continuity‚Äù ÀúP ‚â™loc P the corresponding local density Zn = d ÀúPn
dPn
is a martingale w.r. to P. The next theorem states conditions under which a ‚Äúlocal
absolute continuity‚Äù is transformed to ‚Äúabsolute continuity‚Äù of ÀúP w.r. to P.
Theorem 7.4 Assume that ÀúP ‚â™loc P and (Zn)n=0,1,..., Z0 = 1, is a local density
d ÀúPn
dPn . Then the following statements are equivalent:
1) ÀúP ‚â™P,
2) (Zn)n=0,1,... is uniformly integrable,
3) ÀúP(œâ : supn Zn < ‚àû) = 1.
Proof (1) ‚áí(3): According to Theorem 6.6 there exists Z‚àû= limn‚Üí‚àûZn P-a.s.
Due to ÀúP ‚â™P such a limit does exist ÀúP-a.s.. Hence, ÀúP(œâ : supn Zn < ‚àû) = 1.
(3) ‚áí(2): For a constant c > 0 we have
EZnI{œâ:Zn>c} = ÀúP(œâ : Zn > c) ‚â§ÀúP(œâ : sup
i
Zi > c) ‚Üí0, c ‚Üí‚àû.
(2) ‚áí(1): Due to the uniform integrability of (Zn)n=0,1,... we can apply Theorem
6.7 and Ô¨Ånd the existence Z‚àû= limn‚Üí‚àûZn P-a.s., and E|Zn ‚àíZ‚àû| ‚Üí0, n ‚Üí‚àû.
Further, for any A ‚ààFm and for n ‚â•m, we can write a martingale property of
(Zn)n=0,1,... as follows
ÀúP(A) = EIAZm = EIAZn.
(7.30)
It follows from L1-convergence that one can take a limit as n ‚Üí‚àûin the equality
(7.30) and obtain
ÀúP(A) = EIAZ‚àû,
i.e. Z‚àû= d ÀúP
dP .
‚ñ°
Now we show a method of construction of probability measure ÀúP and its (local)
density w.r. to P. Aiming it we formulate one of the simplest versions of the Girsanov
theorem.
Suppose (œµn)n=0,1,... is a sequence of independent random variables œµn ‚àºN(0, 1).
DeÔ¨Åne F0 = {‚àÖ, Œ©},
Fn = œÉ(œµ1, . . ., œµn},
n = 1, 2, . . ., a bounded predictable
sequence (Œ±n)n=0,1,... and Z0 = 1,
Zn = exp

‚àí
n

1
Œ±kœµk ‚àí1
2
n

1
Œ±2
k

, n = 1, 2, . . .
(7.31)
Problem 7.3 Prove that (Zn)n=0,1,... is a martingle with EZn = 1.

7.2
Martingales and absolute continuity of measures. Discrete time . . .
75
Theorem 7.5 DeÔ¨Åne a new probability measure ÀúPN(A) = EIAZN, A ‚ààFN. Then
Àúœµn = Œ±n + œµn, n = 1, . . ., N, is a sequence of independent standard normal random
variables w.r. to ÀúPN.
Proof For real numbers (Œªn)n=1,...,N we have using the rule of changing of measures,
properties of conditional expectations and the form of characteristic function for
standard normal random variables:
ÀúENei N
1 Œªn Àúœµn =EZNei N
1 Œªn Àúœµn
=E

ei N‚àí1
1
Œªn Àúœµn ZN‚àí1E

eiŒªN (Œ±N +œµN )+Œ±N œµN ‚àíŒ±2
N /2|FN‚àí1

=E

ei N‚àí1
1
Œªn Àúœµn ZN‚àí1

e‚àíŒª2
N /2 = . . . = e‚àí1/2 N
1 Œª2
n.
It means that characteristic function of N
1 Œªn Àúœµn is a product of characteristic func-
tions of standard normal random variables and we get the statement of the theorem.
‚ñ°
Remark 7.3 One can extend Theorem 7.5 to inÔ¨Ånite the time interval. According to
Theorem 7.4 we need provide a condition to guarantee that (Zn)n=0,1,... is uniformly
integrable. Usually, it is achieved with the help of the Novikov type condition
E exp(1/2 ‚àû
1 Œ±2
n) < ‚àû.
Let us give an example of application of this theorem to Mathematical Finance.
Example 7.4 We assume that Ô¨Ånancial market consists of two assets (Bn)n=0,1,...,N
and (Sn)n=0,1,...,N (bank account and stock prices relatively). We put for simplicity
that Bn ‚â°1 (interest rate is zero) and
ŒîSn = Sn ‚àíSn‚àí1 = Œ±n + œµn, S0 > 0, n = 1, . . ., N.
(7.32)
We consider a standard Ô¨Ånancial contract ‚Äúcall option‚Äù with a strike K. The holder
of the contract has the right to buy a stock share by the price K at the maturity date
N. In terms of having the right - he /she must pay a premium CN at time 0. The basic
problem here is to determine CN.
The market (7.32) can be considered as the discrete time Bachelier model with
unit volatility. It is well-established that the premium must be calculated to avoid
arbitrage. According to this Ô¨Ånancial no-arbitrage principle CN is determined as
expected value of pay-oÔ¨Ä(SN ‚àíK)+ w.r. to a risk-neutral measure ÀúPN given in the
Girsanov theorem.
To provide concrete calculations we formulate an auxiliary fact for a Normal
distribution as a problem.
Problem 7.4 Prove that
E(a + bœµ)+ = aŒ¶(a/b) ‚àíbœÜ(a/b),
(7.33)
where œµ ‚àºN(0, 1), a ‚ààR, b > 0, œÜ = œÜ(x) is a standard normal density and Œ¶(x) =
‚à´x
‚àí‚àûœÜ(y)dy.

76
7
Discrete time stochastic analysis: further results and applications
To calculate CN we apply (7.33) and obtain the following discrete time formula
of Bachelier
CN = ÀúEN(SN ‚àíK)+ = ÀúEN
	
S0 +
N

1
Œ±i +
n

1
œµi ‚àíK

+
=E
	
S0 ‚àíK +
N

1
œµi

+
= E(S0 ‚àíK +
‚àö
Nœµ)+
=(S0 ‚àíK)Œ¶
 S0 ‚àíK
‚àö
N

+
‚àö
NœÜ
 S0 ‚àíK
‚àö
N

.
7.3
Asymptotic martingales and other extensions of
martingales
We start with an idea to avoid conditional expected values to extend the notion of
martingale. Let us assume that (Œ©, F, (Fn)n=0,1,..., P) be a stochastic basis on which
all stochastic sequences are considered. Denote Tb the set of all bounded stopping
times on this stochastic basis. We deÔ¨Åned before that two stopping times œÑ ‚â§œÉ if
œÑ(œâ) ‚â§œÉ(œâ) (a.s.). With this deÔ¨Ånition the set Tb is a directed set Ô¨Åltering to the
right.
DeÔ¨Ånition 7.3 A stochastic sequence of integrable random variables (Xn)n=0,1,... is
called an asymptotic martingale (amart), if the family (net) (EXœÑ)œÑ‚ààTb converges.
Remark 7.4 It follows from the deÔ¨Ånition of amart (Xn)n=0,1,... that the set (EXœÑ)œÑ‚ààTb
is bounded. It is also clear that a linear combinations of amarts is an amart.
According to this deÔ¨Ånitions and Theorem 6.6 and Theorem 6.7, we can conclude
that martingales belong to the class of asymptotic martingales. In this case a natural
question arises:
‚ÄúWhat elements of previously developed martingale theory can be extended to a
wider class of asymptotic martingales?‚Äù
If another Ô¨Åltration (Gn)n=0,1,... is included to Ô¨Åltration (Fn)n=0,1,... i.e. Gn ‚äÜFn, n =
0, 1, . . ., then each stopping time w.r. to (Gn) will be a stopping time w.r. to (Fn).
Therefore, every amart (Xn)n=0,1,... w.r. to (Fn) will be amart w.r. to (Gn) if it is
adapted to (Gn). Moreover, from every (Fn)-amart (Xn) one can construct a (Gn)-
amart (Yn) if we put Yn = E(Xn|Gn), n = 0, 1, . . . because of EXœÑ = EYœÑ for all
œÑ ‚ààTb.
Further, we have seen the role of ‚Äúmaximal inequalities‚Äù in martingale theory. It
turns out, such inequality can be stated in more general setting.
Lemma 7.3 Assume that a stochastic sequence (Xn)n=0,1,... such that supœÑ‚ààTb
E|XœÑ| < ‚àû. Then for Œª > 0 :
P

sup
n
|Xn| > Œª

‚â§Œª‚àí1 sup
œÑ‚ààTb
E|XœÑ|.
(7.34)

7.3
Asymptotic martingales and other extensions of martingales
77
Proof For a Ô¨Åxed number N we deÔ¨Åne the set A = {œâ : sup0‚â§n‚â§N |Xn| > Œª} and a
bounded stopping time
œÉ(œâ) =

inf(n ‚â§N : |Xn(œâ)| > Œª)
i f œâ ‚ààA,
N
i f œâ  A.
Then
sup
œÑ‚ààTb
E|XœÑ| ‚â•E|XœÉ| ‚â•ŒªP(A).
(7.35)
Taking the limit in (7.35) as N ‚Üí‚àûwe get (7.34).
‚ñ°
Lemma 7.4 If (Xn)n=0,1,... and (Yn)n=0,1,... are L1-bounded amarts, then (max
(Xn,Yn))n=0,1,... and (min(Xn,Yn))n=0,1,... are amarts.
Proof We consider only the case of Zn = max(Xn,Yn) due to a symmetry. First of
all, we prove that the net (EZœÑ)œÑ‚ààTb is bounded. To do it take an arbitrary œÑ ‚ààTb and
choose n ‚â•œÑ. DeÔ¨Åne
œÑX =

œÑ,
i f XœÑ ‚â•0,
n,
i f XœÑ < 0.
œÑY =

œÑ,
i f YœÑ ‚â•0,
n,
i f YœÑ < 0.
and Ô¨Ånd that
EZœÑ ‚â§EXœÑI{XœÑ ‚â•0} + EYœÑI{YœÑ ‚â•0}
=EXœÑX ‚àíEXnI{XœÑ <0} + EYœÑY ‚àíEYnI{YœÑ <0}
‚â§sup
œÉ‚ààTb
EXœÉ + sup
m
E|Xm| + sup
œÉ‚ààTb
EYœÉ + sup
m
E|Ym|,
and we arrive to the conclusion.
Let us prove now that (Zn)n=0,1,... is an amart. For œµ > 0 we can choose œÑ0 ‚ààTb
such that
|EXœÉ ‚àíEXœÑ| < œµ, |EYœÉ ‚àíEYœÑ| < œµ
(7.36)
for œÉ, œÑ ‚â•œÑ0. Further, due to the boundedness of (EZœÑ)œÑ‚ààTb one can choose œÑ1 ‚â•œÑ0
such that if œÉ ‚â•œÑ0, then
EZœÉ ‚â§EZœÑ1 + œµ.
(7.37)
For a bounded stopping time œÉ ‚â•œÑ1 we deÔ¨Åne A = {XœÑ1 < YœÑ1} and œÉ1 ‚ààTb as
follows
œÉ1 =

œÑ1
on A,
œÉ
on Ac.
Then we obtain

78
7
Discrete time stochastic analysis: further results and applications
EXœÑ1 =EZœÑ1IAc + EXœÑ1IA,
(7.38)
EXœÉ1 =EXœÉIAc + EXœÑ1IA.
(7.39)
Subtracting (7.38) from (7.37) we get with the help of (7.35) that
EZœÑ1IAc = EXœÉIAc + EXœÑ1 ‚àíEXœÉ1 ‚â§EZœÉIAc + œµ.
(7.40)
We can write the same relations as (7.38) and (7.39) for (Yn) :
EYœÑ1 =EZœÑ1IAc + EYœÑ1IA,
EYœÉ1 =EYœÉIAc + EYœÑ1IA.
and Ô¨Ånd
EZœÑ1IA = EYœÉIA + EYœÑ1 ‚àíEYœÉ1 ‚â§EZœÉIA + œµ.
(7.41)
Combining (7.40) and (7.41), we obtain inequality
EZœÑ1 ‚â§EZœÉ + 2œµ
which leads together with (7.36) that
|EZœÉ ‚àíEZœÑ1| ‚â§2œµ,
and, hence, the net (EZœÑ)œÑ‚ààTb is the Cauchy net.
‚ñ°
Applying Lemma 7.3 and Lemma 7.4 we arrive to the following theorem.
Theorem 7.6 Let (Xn)n=0,1,... be amart with supn E|Xn| < ‚àû. Then
1) its positive (negative) parts X+
n (X‚àí
n ) and its absolute value |Xn| are
L1‚àíbounded amarts,
2) for each Œª ‚â•0 the sequence
XŒª
n = sign(Xn)ŒªI|Xn |>Œª + XnI|Xn |‚â§Œª,
3) supœÑ‚ààTb E|XœÑ| < ‚àûand supn |Xn| < ‚àû(a.s.).
The next statement can be considered as a version of the optional sampling
theorem.
Theorem 7.7 Let (Xn)n=0,1,... be an amart for (Fn)n=0,1,... and let (œÑm)m=0,1,...
be a non-decreasing sequence of bounded stopping times for (Fn)n=0,1,.... Then
(XœÑm)m=0,1,... is an amart for (FœÑm)m=0,1,....
Proof For Ô¨Åxed œµ > 0 we can choose N such that |EXœÑ ‚àíEXœÑ‚Ä≤| < œµ for all bounded
stopping times œÑ and œÑ‚Ä≤ ‚â•N. Denote œÑ‚àû= limm‚Üí‚àûœÑm (a.s.) and Ô¨Ånd that XœÑm‚àßN ‚Üí
XœÑ‚àû‚àßN (a.s.), m ‚Üí‚àû, and
E sup
m
|XœÑm‚àßN | ‚â§E max(|X1, |, . . ., |XN |) < ‚àû.

7.3
Asymptotic martingales and other extensions of martingales
79
Therefore, by the dominated convergence theorem the sequence (XœÑm‚àßN)m=0,1,... is
an amart. Taking a big enough M so that |EXœÑœÉ‚àßN ‚àíEXœÑœÉ‚Ä≤‚àßN | for all bounded
stopping times œÉ and œÉ‚Ä≤ ‚â•M for (FœÑm)m=0,1,... we have
|EXœÑœÉ ‚àíEXœÑœÉ‚Ä≤ | ‚â§|EXœÑœÉ‚à®N ‚àíEXœÑœÉ‚Ä≤‚à®N | + |EXœÑœÉ‚àßN ‚àíEXœÑœÉ‚Ä≤‚àßN | ‚â§œµ + œµ = 2œµ.
‚ñ°
Example 7.5 An integrable stochastic sequence (Xn)n=0,1,... is called a quasimartin-
gale if
‚àû

n=0
E|Xn ‚àíE(Xn+1|Fn)| < ‚àû.
(7.42)
Condition (7.42) is trivial in case of a martingale. In view (7.42) for given œµ > 0 one
can Ô¨Ånd a big enough N such that
‚àû

n=N
E|Xn ‚àíE(Xn+1|Fn)| ‚â§œµ.
(7.43)
Take œÑ ‚ààTb such that N ‚â§œÑ ‚â§M we have
|EXœÑ ‚àíEXM | =

M

m=N
E(Xm ‚àíXM)I{œÑ=m}

=

M

m=N
M‚àí1

n=m
E(Xn ‚àíXn+1)I{œÑ=m}

=
M‚àí1

n=N
n

m=N
E |Xn ‚àíE(Xn+1|Fn))| I{œÑ=m}

‚â§
‚àû

n=N
E|Xn ‚àíE(Xn+1|Fn)| ‚â§œµ.
If œÑ1, œÑ2 ‚â•N, then choose M ‚â•œÑ1 ‚à®œÑ2 and get
|EXœÑ1 ‚àíEXœÑ2| ‚â§|EXœÑ1 ‚àíEXM | + |EXœÑ2 ‚àíEXM | ‚â§2œµ.
Hence, the net (EXœÑ)œÑ‚ààTb is Cauchy and converges.
So, any quasimartingale is an amart.
The deÔ¨Ånition of amarts is certainly directed to provide their asymptotic behavior
similar to martingales. Below we conÔ¨Årm these expectations.
Lemma 7.5 Let (Xn)n=0,1,... be a stochastic L1-bounded sequence. Then the follow-
ing are equivalent:
(1) (Xn)n=0,1,... converges (a.s.),
(2) (Xn)n=0,1,... is an amart.

80
7
Discrete time stochastic analysis: further results and applications
Proof the implication (1) ‚áí(2) follows from the dominated convergence theorem.
To prove that (2) ‚áí(1) we put X‚àó= lim sup Xn and X‚àó= lim infn Xn. Then we
can Ô¨Ånd sequences œÑn, œÉn ‚ààTb with œÑn ‚Üë‚àû, œÉn ‚Üë‚àûsuch that XœÑn ‚ÜíX‚àó(a.s.) and
XœÉn ‚ÜíX‚àó(a.s.). Applying again the dominated convergence theorem, we obtain
that
E(X‚àó‚àíX‚àó) = lim
n E(XœÑn ‚àíXœÉn) = 0.
It means X‚àó= X‚àó(a.s.).
‚ñ°
Theorem 7.8 Let (Xn)n=0,1,... be an amart with supn E|Xn| < ‚àû. Then (Xn)n=0,1,...
converges (a.s.).
Proof By Theorem 7.6, supn |Xn| < ‚àû(a.s.) and therefore P{œâ : supn |Xn| > Œª} is
arbitrary small if Œª is big enough. By Theorem 7.7, XŒª
n is an amart, and by Lemma
7.5, it converges (a.s.). Taking Œª ‚Üë‚àûwe get (a.s.)-convergence of (Xn).
‚ñ°
In the end of this Section we would like to mention some other extensions of the
notion of martingales.
DeÔ¨Ånition 7.4 A stochastic sequence (Xn)n=0,1,..., X0 = 0, is a local martingale if
there exists a sequence of stopping times (œÑm)m=1,2,... increasing to +‚àûsuch that
(XœÑm‚àßn)n=0,1,... is a martingale for each m = 1, 2, . . . . The sequence (œÑm) is called a
localizing sequence.
DeÔ¨Ånition 7.5 A stochastic sequence (Xn)n=0,1,..., X0 = 0, is a generalized martin-
gale if (a.s.)
{œâ : E(X+
n |Fn‚àí1) < ‚àû} ‚à™{œâ : E(X‚àí
n |Fn‚àí1) < ‚àû} = Œ©,
and for n = 1, 2, . . .
E(Xn|Fn‚àí1) = Xn‚àí1
(a.s.)
DeÔ¨Ånition 7.6 A stochastic sequence (Xn)n=0,1,..., X0 = 0, is a martingale transform
(stochastic integral) if it admits the following representation
Xn =
n

m=0
HmŒîMm,
where (Mn)n=0,1,..., M0 = 0, is a martingale, (Hn)n=0,1,... is predictable, H0 = ŒîM0 =
M0 = 0.
All these generalizations are directed to relax the condition of integrability of
stochastic sequences. All these classes of stochastic sequences are coincide. We omit
the proof of this statement here.

Chapter 8
Elements of classical theory of stochastic
processes
Abstract This chapter contains a general notion of random processes with continu-
ous time. It is given in context of the Kolmogorov consistency theorem. The notion of
a Wiener process with variety of its properties are also presented here. Its existence
is stated by two ways: with the help of the Kolmogorov theorem as well as with the
help of orthogonal functional systems. Besides the Wiener process as a basic process
for many others, the Poisson process is also considered here. Stochastic integration
with respect to Wiener process is developed for a class of progressively measurable
functions. It leads to the Ito processes, the Ito formula, the Girsanov theorem and
representation of martingales (see [5], [6], [14], [17], [21], [35], [41], and [44]).
8.1
Stochastic processes: deÔ¨Ånitions, properties and
classical examples
In classical theory the notion of a stochastic process is associated with a family
of random variables (Xt) on a probability space (Œ©, F, P) with values in space
Rd, d ‚â•1, where parameter t ‚àà[0, ‚àû) = R+. Usually for simplicity we put d = 1. It
is supposed that for time parameter t ‚ààR+ we have a random variable Xt(œâ), œâ ‚àà
Œ©. On the other hand, one can Ô¨Åx œâ ‚ààŒ© and get a function X.(œâ) : R+ ‚ÜíRd,
which is called a trajectory. This point of view opens up the possibility of studying
stochastic processes that have been exhaustively determined by their distributions
as probability measures on the functional space (R[0,‚àû), B[0,‚àû)). Usually a family
of Ô¨Ånite dimensional distributions Pt1,...,tn(B1, . . ., Bn), 0 ‚â§t < t1 < t2 < . . . < tn <
‚àû, Bi ‚ààB(Rd), i = 1, . . ., n, is exploited as the basic probabilistic characteristic of
a stochastic process. A natural question arises in this regard. Is there a stochastic
process (Xt(œâ)) such that P(Xt1 ‚ààB1, . . ., Xtn ‚ààBn) = Pt1,...,tn(B1, . . ., Bn). As it
was noted in Chapter 1, this question is solved with the help of the Kolmogorov
theorem if for the system (Pt1,...,tn) the following consistency conditions are fulÔ¨Ålled:
1. Pt1,...,tn(B1, . . ., Bi‚àí1, ¬∑, Bi+1, . . ., Bn), i = 1, 2, . . ., n, is a probability measure
on (Rd, B(Rd);
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_8
81

82
8
Elements of classical theory of stochastic processes
2. Pt1,...,tn(B1, . . ., Bn) = Pti1,...,tin (Bi1, . . ., Bin), where (i1, . . ., in) is arbitrary
permutation of numbers (1, 2, . . ., n);
3. Pt1,...,tn‚àí1,tn(B1, . . ., Bn‚àí1, Rd) = Pt1,...,tn‚àí1(B1, . . ., Bn‚àí1). In such a case there
exists a probability measure PX in the functional space (R[0,‚àû), B[0,‚àû)) and the
process (Xt(œâ)) is constructed as Xt(œâ) = œât, where œât is the value of function
œâ. ‚ààR[0,‚àû) at time t. Measure PX will be a distribution of (Xt(œâ)) which has Pt1,...,tn
as a system of Ô¨Ånite dimensional distributions of this process.
After these necessry explanations we introduce the notion of a Wiener process or
Brownian motion.
DeÔ¨Ånition 8.1 A stochastic process (Wt)t ‚â•0 such that
1) W0 = 0 (a.s.),
2) W.(œâ) ‚ààC[0, ‚àû) for almost all œâ ‚ààŒ©,
3)
P(Wt1 ‚ààB1, . . ., Wtn ‚ààBn) =
‚à´
B1 . . .
‚à´
Bn p(t, 0, x1)p(t2 ‚àít1, x1, x2) . . . p(tn‚àí
tn‚àí1, xn‚àí1, xn)dx1 . . . dxn,
for
arbitrary
0 < t1 < t2 < . . . tn,
B1, . . ., Bn ‚ààB(R),
where
p(t, x, y) =
1
‚àö
2œÄt exp

‚àí(x‚àíy)2
2t

is called a Wiener process.
It follows directly from the deÔ¨Ånition that Wt ‚àºN(0, t) for every Ô¨Åxed t ‚ààR+.
Let us formulate the Ô¨Årst properties of the process (Wt)t ‚â•0 :
(1) EWtWs = s ‚àßt for arbitrary s, t ‚ààR+;
(2) E(Wt ‚àíWs)2 = |t ‚àís|;
(3) E(Wt ‚àíWs)4 = 3(t ‚àís)2.
We only prove (2), leaving (1) and (3) as problems.
Problem 8.1 Prove properties (1) and (3) of (Wt)t ‚â•0.
Assume that s < t and Ô¨Ånd that
EWsWt =
‚à´‚àû
‚àí‚àû
‚à´‚àû
‚àí‚àû
xyp(s, 0, x)p(t ‚àís, x, y)dxdy
=
‚à´‚àû
‚àí‚àû
xp(s, 0, x)
‚à´‚àû
‚àí‚àû
yp(t ‚àís, x, y)dy

dx
=
‚à´‚àû
‚àí‚àû
xp(s, 0, x)
‚à´‚àû
‚àí‚àû
(x + u)p(t ‚àís, x, x + u)du

dx
=
‚à´‚àû
‚àí‚àû
xp(s, 0, x)
‚à´‚àû
‚àí‚àû
(x + u)p(t ‚àís, 0, u)du

dx
=
‚à´‚àû
‚àí‚àû
xp(s, 0, x)

x
‚à´‚àû
‚àí‚àû
p(t ‚àís, 0, u)du + u
‚à´‚àû
‚àí‚àû
p(t ‚àís, 0, u)du

dx
=
‚à´‚àû
‚àí‚àû
xp(s, 0, x) (x + 0) dx
=
‚à´‚àû
‚àí‚àû
x2p(s, 0, x)dx = s.
Now we calculate for s < t and B ‚ààB(R)

8.1
Stochastic processes: deÔ¨Ånitions, properties and classical examples
83
P(Wt ‚àíWs ‚ààB) =
‚à´
{(x,y):y‚àíx‚ààB}
p(s, 0, x)p(t ‚àís, x, y)dxdy
=
‚à´‚àû
‚àí‚àû
p(s, 0, x)
‚à´
{y:y‚àíx‚ààB}
p(t ‚àís, x, y)dy

dx
=
‚à´‚àû
‚àí‚àû
p(s, 0, x)
‚à´
B
p(t ‚àís, x, x + u)du

dx
=
‚à´‚àû
‚àí‚àû
p(s, 0, x)
‚à´
B
p(t ‚àís, 0, u)du

dx
=
‚à´‚àû
‚àí‚àû
p(s, 0, x)dx
‚à´
B
p(t ‚àís, 0, u)du =
‚à´
B
p(t ‚àís, 0, u)du
and we obtain that Wt ‚àíWs ‚àºN(0, t ‚àís).
As a result, we arrive to the fourth property of Wt :
(4) For 0 =t 0 < t1 < . . . < tn the increments Wt1 ‚àíWt0, . . ., Wtn ‚àíWtn‚àí1 of a
Wiener process are independent random variables.
Proof Due to a ‚Äúnormality‚Äù of increments we need to prove that they are uncorre-
lated. Let us take r < s < t < u and Ô¨Ånd that
E(Wu ‚àíWt) = EWuWs ‚àíEWtWs ‚àíEWuWr + EWtWr = s ‚àís ‚àír + r = 0.
‚ñ°
To go further, we introduce Ô¨Åltration, generated by a Wiener process Ft =
œÉ(Ws, s ‚â§t). We assume that Ft is complete, i.e. it contains all sets of P‚àímeasure
zero.
DeÔ¨Ånition 8.2 A process (Mt)t ‚â•0 satisfying conditions
1) Mt ‚àíFt-measurable,
2) Mt is integrable,
3) E(Mt|Fs) = Ms (a.s.) for all s < t,
is called a martingale.
After this deÔ¨Ånition we formulate the martingale property of a Wiener process. If
in (3) the equality is replaced by the inequality ‚â•(‚â§), then (Mt) is a submartingale
(supermartingale).
(5) Processes (Wt)t ‚â•0 and (W2
t ‚àít)t ‚â•0 are martingales w.r.to (F )t ‚â•0.
Remark 8.1 In fact, the inverse statement to (5) is well-known as Levy‚Äôs character-
ization of a Wiener process.
(6) The Doob maximal inequality for a Wiener process (Wt)t ‚â•0 has the following
form: for t > 0,
E max
s‚â§t |Ws|2 ‚â§4E|Wt|2.
(8.1)
To prove it we consider a subdivision of the interval [0, t] as of the interval [0, t] as
t(n)
k
= kt
2n, 0 ‚â§k ‚â§2n, and deÔ¨Åne a square integrable martingale with discrete time

84
8
Elements of classical theory of stochastic processes
Mk = Mn
k = W kt
2n , F n
k = F kt
2n , 0 ‚â§k ‚â§2n.
For (|Mk|) we have, using its submartingale property that
E(max
k
|Mk|)2 =2
‚à´‚àû
0
yP(max
k
|Mk| > y)dy
‚â§2
‚à´‚àû
0
E|M2n |I(maxk |Mk |‚â•y)dy
=2
‚à´‚àû
0
‚à´
(maxk |Mk |‚â•y)
|M2n |dP

dy
=2
‚à´
Œ©
|Mk
2n
‚à´maxk |Mk |
0
dy

dP
=2
‚à´
Œ©
|M2n | max
k
|Mk|dP = 2E|M2n | max
k
|Mk|
‚â§2(E(M2n)2)1/2

E(max
k
|Mk|)2
1/2
.
As a result, we obtain that
(E(max
k
|Mk|)2)1/2 ‚â§2(E(M2n)2)1/2,
and, hence,
E max
k
|Mn
k |2 ‚â§E(max
k
|Mn
k |)2 ‚â§4E(M2n)2 = 4E|Wt|2.
(8.2)
Taking the limit as n ‚Üí‚àûin (8.2) and using the continuity of trajectories of a
Wiener process we arrive to inequality (8.1).
(7) For any positive constant c the process Wc
t = c‚àí1Wc2t is a Wiener process.
This property of a Wiener process is called self-similarity.
To prove the self-similarity of a Wiener process one can use the Levy characteri-
zation of this process.
(8) Existence of a Wiener process follows from the consistency theorem of Kol-
mogorov (see Chapter 1). However, such a reference is incomplete because this
theorem guarantees existence of this process only in space (R[0,‚àû), B[0,‚àû)). To make
the proof complete one can use another theorem of Kolmogorov:
Suppose (Xt)t ‚â•0 is a stochastic process satisfying the following condition
E|Xt ‚àíXs|Œ± ‚â§C|t ‚àís|1+Œ≤ for all t, s ‚â•0,
and for some Œ± > 0, Œ≤ > 0, 0 < C < ‚àû. Then the process (Xt) admits a continuous
modiÔ¨Åcation i.e. there exists a continuous process (Yt) such that P(Xt = Yt) = 1 for
all t ‚ààR+.
Property (3) of a Wiener process (Wt) means that this theorem can be applied,
and therefore, the process (Wt) can be identiÔ¨Åed with its continuous modiÔ¨Åcation.

8.1
Stochastic processes: deÔ¨Ånitions, properties and classical examples
85
(9) Now we give some other characterizations of trajectories of (Wt)t ‚â•0. The con-
tinuity property brings an element of a regularity of trajectories. But this description
of sample paths properties is clearly incomplete as it is shown below.
Let us analyze the Ô¨Årst question of diÔ¨Äerentiability of trajectories of (Wt)t ‚â•0. We
take t ‚â•0 and consider the ratio RW
Œî (t) = Wt+Œît‚àíWt
Œît
, where Œît ‚Üí0. One can easily
observe that
ERW
Œî (T) = EWt+Œît ‚àíEWt
Œît
= 0,
and
VarRW
Œî (t) = (Œît)‚àí2Var(Wt+Œît ‚àíWt) = (Œît)‚àí2Œît = (Œît)‚àí1 ‚Üí‚àûas Œît ‚Üí0.
It shows that W. is not diÔ¨Äerentiable in the L2‚àísense. Moreover, one can prove
that almost all trajectories of W are non-diÔ¨Äerentiable.
Let us analyze the second question of how vary the trajectories of (Wt)t ‚â•0 are.
This property is characterized with the help of the notion of p-variation of given
function f : [0,T] ‚ÜíR, p ‚â•1 :
We say that
lim sup
Œît‚Üí0
n

i=1
| f (ti) ‚àíf (ti‚àí1)|p, 0 = t0 < t1 < . . . < tn = T,
is a p‚àívariation of f on [0,T].
Usually, the Ô¨Årst and second variations are most important in this regard.
For the second variation of (Wt)t ‚â•0. We have the following observations.
In view of independent increments we obtain for 0 = t0 < t1 < . . . < tn = T that
E 	n
i=1(Wti ‚àíWti‚àí1)2 = 	n
i=1 E(Wti ‚àíWti‚àí1)2 = 	n
i=1 Var(Wti ‚àíWti‚àí1) = 	n
i=1
(ti ‚àíti‚àí1) = T.
Further, with the help of (3):
Var
n

i=1
(Wti ‚àíWti‚àí1)2 =
n

i=1
Var(Wti ‚àíWti‚àí1)2
=
n

i=1

E(Wti ‚àíWti‚àí1)4 ‚àí

E(Wti ‚àíWti‚àí1)22
=
n

i=1

3(ti ‚àíti‚àí1)2 ‚àí(ti ‚àíti‚àí1)2
=2
n

i=1
(ti ‚àíti‚àí1)2.
Therefore,

86
8
Elements of classical theory of stochastic processes
E
 n

i=1
(Wti ‚àíWti‚àí1)2 ‚àíT
2
= Var
 n

i=1
(Wti ‚àíWti‚àí1)2

=2
n

i=1
(ti ‚àíti‚àí1)2 ‚â§2T max
0<i‚â§n(ti ‚àíti‚àí1) ‚Üí0,
and we conclude that the second variation of (Wt) on [0,T] converges to the length
of this interval in L2sense, and hence, in the sense of convergence in probability. To
investigate the Ô¨Årst variation of (Wt) we consider the probability P(œâ : 	n
i=1 |Wti ‚àí
Wti‚àí1| > N). First of all, we note that E 	n
i=1 |Wti ‚àíWti‚àí1| =

2
œÄ
	n
i=1
‚àöti ‚àíti‚àí1 ‚Üí
‚àûas max(ti ‚àíti‚àí1) ‚Üí0, and
Var
n

i=1
|Wti ‚àíWti‚àí1| =
n

i=1
(1 ‚àí2
œÄ )(ti ‚àíti‚àí1) = (1 ‚àí2
œÄ )T.
Further, for E 	n
i=1 |Wti ‚àíWti‚àí1| > n with the help of the Chebyshev inequality
we get
P(œâ :
n

i=1
|Wti ‚àíWti‚àí1 | ‚â§N) ‚â§P(œâ :
n

i=1
|Wti ‚àíWti‚àí1 | ‚àíE
n

i=1
|Wti ‚àíWti‚àí1 | ‚â•E
n

i=1
|Wti ‚àíWti‚àí1 | ‚àíN)
‚â§Var(
n

i=1
|Wti ‚àíWti‚àí1 |)/(E
n

i=1
|Wti ‚àíWti‚àí1 | ‚àíN)2 ‚Üí0,
as max(ti ‚àíti‚àí1) ‚Üí0.
It means that the Ô¨Årst variation of (Wt) converges in probability to ‚àû.
Remark 8.2 It is possible to prove that all mentioned results are true in the sense of
(a.s.)-convergence.
Another process that plays an important role in both theory and applications is a
Poisson process.
DeÔ¨Ånition 8.3 The process (Nt)t ‚â•0 is called a Poisson process with parameter Œª > 0,
if the following conditions are satisÔ¨Åed:
1) N0 = 0. (a.s.),
2) its increments Nt1 ‚àíNt0, . . ., Ntn ‚àíNtn‚àí1 are independent random variables for
any sundivision 0 ‚â§t0 < t1 < . . . < tn < ‚àû,
3) Nt ‚àíNs is a random variable that has a Poisson distribution with parameter
Œª(t ‚àís) :
P(œâ : Nt ‚àíNs = i) = e‚àíŒª(t‚àís) (Œª(t ‚àís))i
i!
, 0 ‚â§s ‚â§t < ‚àû, i = 0, 1, . . . .
Let us present some properties of sample paths of this process.

8.1
Stochastic processes: deÔ¨Ånitions, properties and classical examples
87
First of all, we note that its almost all trajectories are non-decreasing, because for
s ‚â§t :
P(œâ : Nt ‚àíNs ‚â•0) =
‚àû

i=0
P(œâ : Nt ‚àíNs = i) =
‚àû

i=0
e‚àíŒª(t‚àís) (Œª(t ‚àís))i
i!
= e‚àíŒª(t‚àís)
‚àû

i=0
(Œª(t ‚àís))i
i!
= e‚àíŒª(t‚àís)eŒª(t‚àís) = 1.
The process (Nt)t ‚â•0 is stochastically continuous: in the sense Nt ‚ÜíNs in proba-
bility as t ‚Üís. This property is an obvious consequence of the application of the
Chebyshev inequality. It is an interesting eÔ¨Äect when a jumping process satisÔ¨Åes a
continuity property. Moreover, one can say something about a diÔ¨Äerentiability of
trajectories of (Nt)t ‚â•0 in the sense of convergence in probability, i.e.
Nt+Œît ‚àíNt
Œît
p
‚àí‚àí‚àí‚àí‚Üí
Œît‚Üí0 0.
(8.3)
Problem 8.2 Prove relation (8.3).
Further, one can derive from P(œâ : Ns ‚â§Nt) = 1 for all s ‚â§t that there is a right-
continuous modiÔ¨Åcation of (Nt), almost all trajectories of which are non-decreasing
integer-valued functions with unit jumps.
Let us note a martingale property of (Nt)t ‚â•0. To do this we introduce a natural
Ô¨Åltration (Ft)t ‚â•0, generated by (Nt)t ‚â•0 and completed by sets of P‚àímeasure zero.
We note that
ENt =
‚àû

i=0
iP(œâ : Nt = i) =
‚àû

i=0
ie‚àíŒªt (Œªt)i
i!
=(Œªt)e‚àíŒªt
‚àû

i=1
(Œªt)i‚àí1
(i ‚àí1)! = (Œªt)e‚àíŒªteŒªt = Œªt.
Let us deÔ¨Åne a new process Mt = Nt ‚àíŒªt and Ô¨Ånd that for s ‚â§t due to indepen-
dence (Nt ‚àíNs) of Fs :
E(Mt|Fs) =E(Nt ‚àíŒªt|Fs) = E(Nt ‚àíNs + Ns ‚àíŒªt|Fs)E(Nt ‚àíNs|Fs) + Ns ‚àíŒªt
=E(Nt ‚àíNs) + Ns ‚àíŒªt = Œª(t ‚àís) + Ns ‚àíŒªt = Ns ‚àíŒªs = Ms.
Hence, (Mt)t ‚â•0 is a martingale w.r.to (Ft)t ‚â•0.
We have seen that both processes (Wt)t ‚â•0 and (Nt) are stochastically continu-
ous processes with independent increments. The following example shows why a
consideration of stochastic processes with independent values is not productive.
Example 8.1 Let (Xt)t ‚â•0 be a family of independent random variables with the same
density f = f (x) ‚â•0,
‚à´‚àû
‚àí‚àûf (x)dx = 1. Then for a Ô¨Åxed s ‚â•0, and t  s, œµ > 0 we
have that

88
8
Elements of classical theory of stochastic processes
P(œâ : |Xt ‚àíXs| ‚â•œµ) =
‚à´‚à´
|x‚àíy|‚â•œµ
f (x) f (y)dxdy.
(8.4)
The integral in the right hand side of equality (8.4) converges (as œµ ‚Üí0) to
‚à´‚à´
xy
f (x) f (y)dxdy =
‚à´‚àû
‚àí‚àû
‚à´‚àû
‚àí‚àû
f (x) f (y)dxdy.
(8.5)
The relation (8.5) shows that for some œµ > 0 the probability in the left hand side
of (8.4) does not converge to zero as t ‚Üís. It means the process with independent
values is not stochastically continuous even.
The existence of a Wiener process was derived from the Kolmogorov consistency
theorem. Such a derivation does not look constructive. That is why we want to
demonstrate a direct way of construction of a Wiener process, based on orthogonal
systems of functions of Haar and Schauder, and sequences of independent normal
random variables.
Let us deÔ¨Åne a system of functions (Hk(t))k=1,2,..., t ‚àà[0, 1], called the Haar
system:
H1(t), H2(t) = I[0,2‚àí1](t) ‚àíI(2‚àí1,1](t), . . .,
Hk(t) = 2n/2 
I(an,k, an,k + 2‚àín‚àí1] ‚àíI(an,k+2‚àín‚àí1,an,k+2‚àín](t)

where wn < k ‚â§2n+1, an,k = 2‚àín(k ‚àí2n ‚àí1), n = 1, 2, . . . .
In the space L2([0, 1], B(0, 1), dt) with the scalar product ‚ü®f, g‚ü©=
‚à´1
0 fsgsds,
f, g ‚ààL2, the system (Hk(t))k=1,2... is complete and orthogonal. Hence,
f =
‚àû

k=1
‚ü®f, Hk‚ü©Hk, g =
‚àû

k=1
‚ü®g, Hk‚ü©Hk, ‚ü®f, g‚ü©=
‚àû

k=1
‚ü®f, Hk‚ü©‚ü®g, Hk‚ü©.
Using the system (Hk(t))k=1,2,... one can construct the Schauder system
(Sk(t))k=1,2,... as follows
Sk(t) =
‚à´t
0
Hk(y)dy = ‚ü®I[0,t], Hk‚ü©, t ‚àà[0, 1].
Lemma 8.1 If a sequence of real numbers ak = O(kœµ), k ‚Üí‚àû, for some œµ ‚àà
(0, 1/2), then the series 	‚àû
k=1 akSk(t) converges uniformly on [0, 1], and, hence,
it is a continuous function.
Proof Denote Rm = supt
	
k>2m |ak|Sk(t), m = 1, 2, . . ., and note that |ak| ‚â§ckœµ
for all k ‚â•1 and some constant c > 0. Therefore, for all t and n = 1, 2, . . . we obtain
that

2n<k<2n+1
|ak|Sk(t) ‚â§c2(n+1)œµ

2n<k<2n+1
Sk(t)
‚â§c2(n+1)œµ2‚àín/2‚àí1
‚â§c2œµ‚àín(1/2‚àíœµ).
(8.6)

8.1
Stochastic processes: deÔ¨Ånitions, properties and classical examples
89
It follows from here that Rm ‚â§c2œµ 	
n‚â•m 2‚àín(1/2‚àíœµ) ‚Üí0 as m ‚Üí‚àû.
‚ñ°
Lemma 8.2 Let (Œæk)k=1,2,... be a sequence of standard normal random variable on
a complete probability space (Œ©, F, P). Then for every constant c >
‚àö
2 and almost
all œâ ‚ààŒ© there exists a number N0 = N0(œâ, c) such that |Œæk(œâ)| < c
‚àö
ln k for all
k ‚â•N0.
Proof For a standard normal random variable Œæ ‚àºN(0, 1) and x > 0 we have that
P(œâ : Œæ ‚â•x) =(2œÄ)‚àí1/2
‚à´‚àû
x
e‚àíy2/2dy
=(2œÄ)‚àí1/2
‚à´‚àû
x

‚àí1
y

de‚àíy2/2
=(2œÄ)‚àí1/2

x‚àí1e‚àíx2/2 ‚àí
‚à´‚àû
x
y‚àí2e‚àíy2/2dy

‚â§x‚àí1(2œÄ)‚àí1/2(x‚àí1e‚àíx2/2)
(8.7)
and, hence,
P(œâ : |Œæ| ‚â•x) ‚â§x‚àí1(2œÄ)1/2e‚àíx2/2.
Applying the above inequality we can estimate the series below as follows

k ‚â•2
P

œâ : |Œæk| ‚â•c(ln k)1/2
‚â§c‚àí1(2/œÄ)1/2 
k ‚â•2
k‚àíc2/2(ln k)‚àí1/2 < ‚àû,
and the statement of Lemma 8.2 follows from the Borel-Cantelli lemma.
‚ñ°
Theorem 8.1 Let (Œæk)k=1,2,... be a sequence of independent standard normal random
variables (Œ©, F, P). DeÔ¨ÅneastochasticprocessWt = W(t, œâ) = 	‚àû
k=1 Œæk(œâ)Sk(t), t ‚àà
[0, 1], œâ ‚ààŒ©. Then (Wt)t ‚àà[0,1] is a Wiener process.
Proof First of all, we note that (Wt) has continuous trjectories due to Lemma 8.1
and 8.2.
Denote Zn(t) = 	n
k=1 ŒækSk(t), t ‚àà[0, 1], and Ô¨Ånd that
E|Zn+m(t) ‚àíZn(t)|2 =E

n+m

k=n+1
ŒækSk(t)

2
=
n+m

k=n+1
n+m

l=n+1
Sk(t)Sl(t)EŒækŒæl
=
n+m

k=n+1
S2
k(t).
(8.8)
Further, 	‚àû
k=1 S2
k(t) ‚â§	‚àû
k=1
2‚àík/2‚àí12 < ‚àû, and, hence, due to a completeness of
L2(Œ©, F, P) we arrive to conclusion that there exists a limit Z(t) of Zn(t) in this
space such that

90
8
Elements of classical theory of stochastic processes
E|Zn(t) ‚àíZ(t)|2 ‚Üí0, n ‚Üí‚àû, t ‚àà[0, 1].
This limit Z(t) coincides with Wt up to equivalence and Zn(t)
t‚àí‚ÜíWt in L2‚àísense.
Now due to EZn(t) = 0 we obtain that
|EWt| = |EWt ‚àíEZn(t)| ‚â§

E|Wt ‚àíZn(t)|21/2
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àû0,
which means that EWt = 0.
Using a continuity property of the scalar product we Ô¨Ånd that
‚ü®Zn(s), Zn(t)‚ü©L2(Œ©) ‚Üí‚ü®Ws, Wt‚ü©L2(Œ©) = EWsWt = Cov(Ws, Wt), s, t ‚àà[0, 1].
On the other hand, we can use the independence of (Œæk)k=1,2,... and EŒæk = 0, EŒæ2
k = 1,
and observe that
‚ü®Zn(s), Zn(t)‚ü©L2(Œ©) = EZn(s)Zn(t) =
n

k=1
Sk(s)Sk(t)EŒæ2
k ‚Üí
‚àû

k=1
Sk(t)Sk(t), n ‚Üí‚àû.
Applying the Parseval equality for scalar products we Ô¨Ånally derive that
‚àû

k=1
Sk(s)Sk(t) =
‚àû

k=1
‚ü®Hk, I[0,s]‚ü©‚ü®Hk, I[0,t]‚ü©= ‚ü®I[0,s], I[0,t]‚ü©= min(s, t) = Cov(Ws, Wt).
Let us prove that (Wt)t ‚àà[0,1] is a Gaussian process. Take Œº = (Œº1, . . ., Œºn) ‚àà
Rn, t1, . . ., tn ‚àà[0, 1] and deÔ¨Åne Y = 	n
m=1 ŒºmWtm. We have now that
Y =
n

m=1
Œºm
‚àû

k=1
ŒækSk(tm) =
‚àû

k=1
bkŒæk,
where bk = bk(Œº1 . . . Œºn, t1, . . ., tn) = 	n
m=1 ŒºmSk(tm). Consider YN = 	N
k=1 bkŒæk ‚àº
N(0, œÉ2
N), œÉ2
N = 	N
k=1 b2
k and Ô¨Ånd that
EY2
N = œÉ2
N ‚ÜíEY2 = œÉ2, N ‚Üí‚àû,
in the sense of L2(Œ©). Therefore, we have also the convergence in distribution
YN ‚àí‚Üí
d Y, N ‚Üí‚àû. Hence, we have that the convergence of
œÜY(Œª) = exp(‚àíœÉ2
NŒª2/2) ‚Üíexp(‚àíœÉ2Œª2/2) = œÜY(Œª), Œª ‚ààR,
i.e. Y ‚àºN(0, sœÉ2).
Finally, we can extend this construction from the unit time interval to the whole
[0, ‚àû) as follows
W(t, œâ) =

Wt = W(t, œâ) = W1(t, œâ),
t ‚àà[0, 1),
	k
j=1 Wj(1, œâ) + Wk+1(t ‚àík, œâ),
t ‚àà[k, k + 1), k = 1, 2, . . .,

8.2
Stochastic integrals with respect to a Wiener process
91
where Wn = (Wn(t))t ‚àà[0,1] be a sequence of independent Wiener processes.
‚ñ°
8.2
Stochastic integrals with respect to a Wiener process
Let (Œ©, F, P) be a complete probability space, (Wt)t ‚â•0 be a Wiener process and
(Ft)t ‚â•0 be a complete Ô¨Åltration generated by this process.
We describe here a scheme of construction of a stochastic integral
I( f ) =
‚à´‚àû
0
fs(œâ)dWs
(8.9)
for any Ft-adapted and B(0, ‚àû) √ó F -measurable random function/process ft(œâ) =
f (t, œâ) integrable in square with respect to dt √ó dP :
E
‚à´‚àû
0
f 2
s ds < ‚àû.
(8.10)
The class of these random functions is denoted S2, and the stochastic integral
(8.9) must be a linear and isometric operator from L2(R+ √ó Œ©, B(R+) √ó F, dt √ó dP)
to L2(Œ©, F, dP). We start its construction from the class of step functions f ‚ààS2
0 ‚äÜ
S2. For each function f there exists a subdivision 0 = t0 ‚â§t1 ‚â§. . . ‚â§tn < ‚àûsuch
that fti-Fti- measurable and square-integrable, and ft = fti for t ‚àà[ti, ti+1), i ‚â§n,
whereas ft = 0 for t ‚â•tn. Hence, the weighted sum
n‚àí1

i=0
fti I[ti,ti+1)(t) = ft(œâ).
(8.11)
For function ft(œâ), deÔ¨Åned by (8.11) we introduce a stochastic integral I( f ) as
follows
I( f ) =
‚à´‚àû
0
fSdWs =
n‚àí1

i=0
fti(Wti+1 ‚àíWti).
(8.12)
It is almost obvious that the deÔ¨Ånition of I( f ) in (8.12) does not depend on changes
of subdivisions (ti), and it is a linear mapping from S2
0 to L2(Œ©, F, P), i.e.
I(Œ± f + Œ≤g) = Œ±I( f ) + Œ≤I(g),
(8.13)
where Œ±, Œ≤ ‚ààR, f, g ‚ààS2
0.
Using properties (1)-(2) and (4) of a Wiener process (Wt)t ‚â•0 and properties of
conditional expected values, we obtain that

92
8
Elements of classical theory of stochastic processes
EI2(f ) =E
‚é°‚é¢‚é¢‚é¢‚é¢‚é£
n‚àí1

i=0
f 2
ti (Wti+1 ‚àíWti )2 + 2

j<i
ftj (Wtj+1 ‚àíWtj )fti (Wti+1 ‚àíWti )
‚é§‚é•‚é•‚é•‚é•‚é¶
=E
‚é°‚é¢‚é¢‚é¢‚é¢‚é£
n‚àí1

i=0
f 2
ti E

(Wti+1 ‚àíWti )2 |Fti

+ 2

j<i
ftj (Wtj+1 ‚àíWtj )fti E (Wti+1 ‚àíWti )|Fti
‚é§‚é•‚é•‚é•‚é•‚é¶
=
n‚àí1

i=0
Ef 2
ti (ti+1 ‚àíti) = E
‚à´‚àû
0
f 2
s ds.
(8.14)
The
isometry
relation
(8.14)
means
that
the
norms
||I( f )||L2(Œ©,F,P)
and
|| f ||L2(R+√óŒ©,B(R+)√óF,dtdP) are coincide. Using similar reasoning as in proving (8.14)
one can derive for f, g ‚ààS2
0 that
EI( f )I(g) = E
‚à´‚àû
0
fsgsds.
(8.15)
Equalities (8.13)-(8.15) show that the stochastic integral deÔ¨Åned in (8.12) is a
linear and isometric operator.
For function f ‚ààS2
0 we deÔ¨Åne a stochastic integral over interval [0, t), t > 0, as
follows
It( f ) = I(I[0,t) f ) =
‚à´t
0
fsdWs.
(8.16)
It turns out, (It( f ))t ‚â•0 is a martingale with respect to (Ft)t ‚â•0.
To prove it we Ô¨Åx t > 0, and for simplicity, assume that t = tk.
Then we have
I[0,t) fs = 	n‚àí1
i=0 fti I[ti,ti+1)(t)
and
It( f ) = 	n‚àí1
i=0 fti(Wti+1 ‚àíWti),
which
are
Ft‚àímeasurable. Further, if s ‚â§t and s = tu and t = tk, then
E(It( f ) ‚àíIs( f )|Fs) =
k‚àí1

i=u
E  ftiE(Wti+1 ‚àíWti |Fti)|Fs
 = 0.
For function f ‚ààS0 one can rewrite
It( f ) =
n‚àí1

i=0
fti(Wti+1‚àßt ‚àíWti‚àßt),
and notice that the process (It( f ))t ‚â•0 is continuous.
Now we take a monotonic sequence (sj) such that 0 ‚â§s0 ‚â§s1 ‚â§. . . ‚â§sm < ‚àû.
Then the sequence (Isj ( f ), Fj)j=0,...,m is a martingale for which the Doob inequality
is true
E sup
0‚â§j ‚â§m
I2
sj ( f ) ‚â§4EI2
sm( f ) ‚â§4E
‚à´sm
0
f 2
t dt ‚â§4E
‚à´‚àû
0
f 2
t dt.
(8.17)
Considering (sj) rational and using the continuity of the trajectories of It( f ), we
arrive to the Doob inequality for f ‚ààS2
0 taking sup over t ‚ààR+ in (8.17):

8.2
Stochastic integrals with respect to a Wiener process
93
E sup
t
I2
t ( f ) ‚â§4E
‚à´‚àû
0
f 2
t dt.
(8.18)
One can extend the operator It( f ) from S2
0 to its closure S2. Any function f ‚ààS2
can be approximated by a sequence fn ‚ààS2
0.
A concrete construction of such a sequence ( fn)n=1,2,... ‚ààS2
0 can be provided as
follows. For any function f ‚ààS2 we deÔ¨Åne
fn(t)=

n
‚à´k/n
(k‚àí1)/n f (s)ds,
for t‚àà[k/n, (k + 1)/n), k=1, 2, . . ., n2 ‚àí1, n=1, 2, . . .
0,
otherwise.
Using the Jensen inequality we can Ô¨Ånd that
‚à´(k+1)/n
k/n
| fn(t)|2dt = n

‚à´k/n
(k‚àí1)/n
f (t)dt

2
‚â§
‚à´k/n
(k‚àí1)/n
| f (t)|2dt, k = 1, 2, . . .
Hence, (a.s.)
‚à´‚àû
0
| fn(t)|2dt ‚â§
‚à´‚àû
0
| f (t)2dt < ‚àû,
‚à´‚àû
0
| f (t) ‚àífn(t)|2dt ‚â§2
‚à´‚àû
0

| f (t)|2 + | fn(t)|2
dt ‚â§4
‚à´‚àû
0
| f (t)|2dt < ‚àû.
Now, for arbitrary N = 1, 2, . . . we easily observe that E
‚à´n
0 | f ‚àífn(t)|2dt ‚Üí
0, n ‚Üí‚àû, by construction of ( fn)n=1,2,.... Further, we have
‚à´‚àû
0
| f (t) ‚àífn(t)|2dt =
‚à´N
0
| f (t) ‚àífn(t)|2dt +
‚à´‚àû
N
| f (t) ‚àífn(t)|2dt
‚â§
‚à´N
0
| f (t) ‚àífn(t)|2dt + 2
‚à´‚àû
N

| f (t)|2 + | fn(t)|2
dt
‚â§
‚à´N
0
| f (t) ‚àífn(t)|2dt + 2
‚à´‚àû
N

| f (t)|2 +
‚à´‚àû
N‚àí1/n
| fn(t)|2

dt
‚â§
‚à´N
0
| f (t) ‚àífn(t)|2dt + 2
‚à´‚àû
N‚àí1
| f (t)|2dt.
(8.19)
The term
‚à´‚àû
N‚àí1 | f (t)|2dt ‚Üí0 as N ‚Üí‚àû, and by the dominated convergence theorem
E
‚à´‚àû
0
| f (t) ‚àífn(t)|2dt ‚Üí0 as n ‚Üí‚àû.
Then for m and n ‚ààZ+ we have
||I( fm) ‚àíI( fn)||L2(Œ©) = ||I( fm ‚àífn)||L2(Œ©) = || fm ‚àífn||S2.
It means that (I( fn))n‚â•1 is a fundamental sequence in a complete space L2(Œ©, F, P).
Hence, it converges to some element of this space:
I( f ) = lim
n‚Üí‚àûI( fn),
(8.20)

94
8
Elements of classical theory of stochastic processes
and we call I( f ) a stochastic integral of f ‚ààS2. We also deÔ¨Åne for t ‚â•0
‚à´t
0
fsdWs =
‚à´‚àû
0
I[0,t)(s) fsdWs,
(8.21)
and we prove that the stochastic integral (8.21) admits a continuous modiÔ¨Åcation.
To prove it we take a sequence fn = fn(t, œâ) ‚ààS2
0 such that
E
‚à´‚àû
0
| f (s) ‚àífn(s)|2ds ‚â§2‚àín, n = 1, 2, . . .,
For each t ‚â•0 we have in space L2(R+ √ó Œ©, B(0, ‚àû) √ó F, dtdP) that
I[0,t)(s)( f ) = I[0,t)(s) f1 + . . . + I[0,t)(s)( fn+1 ‚àífn) + . . .,
(8.22)
The equality (8.22) by continuity in the sense of this space is transformed to the next
one
‚à´t
0
f (s)dWs =
‚à´t
0
f1(s)dWs + . . . +
‚à´t
0
| fn+1(s) ‚àífn(s))dWs + . . .,
(8.23)
In the right hand side of (8.23) each term is continuous as functions fn ‚ààS2
0. There-
fore, to prove the continuity of stochastic integral (8.21) it is enough to provide a
uniform convergence (a.s.) of series (8.21). First, due to the Doob inequality we
obtain that
E sup
t ‚â•0

‚à´t
0
( fn+1 ‚àífn(s)) dWs

2
‚â§4E
‚à´‚àû
0
( fn+1(s) ‚àífn(s))2 ds
(8.24)
‚â§16 ¬∑ 2‚àín.
(8.25)
Second, combining (8.24) with the Chebyshev inequality, we get
P

sup
t

‚à´t
0
( fn+1(s) ‚àífn(s)) dWs
 ‚â•n‚àí2

‚â§16n42‚àín.
(8.26)
Application of (8.25) together with the Borel-Cantelli lemma certiÔ¨Åes that the series
(8.23) converges (a.s.).
Finally, we can conclude that the extension of It( f ) from space S2
0 to S2 leaves its
basic properties:
(1) Linearity,
(2) Isometry,
(3) Martingality,
(4) Continuity of trajectories.
Before starting to state other properties of stochastic integrals we introduce a
special class of random variables needed here.

8.2
Stochastic integrals with respect to a Wiener process
95
Let œÑ be a random variable taking values in [0, ‚àû]. We call it a stopping time
on a stochastic basis (Œ©, F, (F )t ‚â•0, P), generated by a Wiener process (Wt)t ‚â•0, if
{œâ : œÑ(œâ) ‚â§t} ‚ààFt for every t ‚ààR+.
A natural example is the Ô¨Årst hitting time of the point a by Wt :
œÑ =œÑa = inf{t : Wt ‚â•a},
œÑ =‚àû, if the set in brackets {.} is empty.
Indeed, one can observe that
{œâ : œÑ(œâ) ‚â§t} = Œ© \ {œâ : œÑ(œâ) > t}.
(8.27)
Further, we have a relation
{œâ : œÑ(œâ) > t} = {œâ : max
s‚â§t Ws(œâ) < a}.
(8.28)
Let us note that
max
s‚â§t Ws =
sup
r ‚â§t,r ‚ààQ
Wr,
(8.29)
where Q is the set of rational numbers, and hence maxs‚â§t Ws is Ft‚àímeasurable.
Using continuity of trajectories of Wt and relations (8.27)-(8.29) we arrive to con-
clusion that {œâ : œÑ(œâ) ‚â§t} ‚ààFt.
Let us connect a stochastic integral It( f ) = Xt =
‚à´t
0 fsdWs, f ‚ààS2, and a stopping
time œÑ. We have the two following equalities which are very useful:
XœÑ =
‚à´‚àû
0
Is<œÑ fsdWs =
‚à´‚àû
0
Is‚â§œÑ fsdWs (a.s.),
(8.30)
E
‚à´œÑ
0
f sdWs
2
= E
‚à´œÑ
0
f 2
s ds (Wald‚Äôs identity),
(8.31)
To prove (8.30) we start with a stopping time œÑ taking discrete values t1, t2, . . .
and Ô¨Ånd that Is<tk fs = Is<œÑ fs on the set {œâ : œÑ = tk}.
On each such set (a.s.)
XœÑ = Xtk =
‚à´‚àû
0
I(s<tk) fsdWs =
‚à´‚àû
0
I(s<œÑ) fsdWs.
Hence, it is true (a.s.) on Œ© = ‚à™k{œÑ = tk}. To prove (8.30) for an arbitrary stopping
time œÑ we approximate œÑ by a sequence of discrete stopping times
œÑn = 2‚àí2[2nœÑ] + 2‚àín,
such that œÑ ‚â§œÑn and œÑn ‚àíœÑ ‚â§2‚àín. Further, due to a continuity of Xt we get conver-
gence XœÑn ‚ÜíXœÑ (a.s.) and by the dominated convergence theorem

96
8
Elements of classical theory of stochastic processes
E
‚à´‚àû
0
I{s<œÑ} fsdWs ‚àí
‚à´‚àû
0
Is<œÑn fsdWs
2
= E
‚à´‚àû
0
I{œÑ‚â§s<œÑn } f 2
s ds ‚Üí0, n ‚Üí‚àû.
Therefore, we obtain (8.30), and using isometry property we get (8.31):
EX2
œÑ = E
‚à´‚àû
0
Is<œÑ f 2
s ds = E
‚à´œÑ
0
f 2
s ds.
It turns out a certain extension of the notion of a stochastic integral with respect
to a Wiener process can be done using the following localization procedure. It is
given with the help of stopping times.
Consider the set of B(0, ‚àû) √ó F ‚àímeasurable adapted random functions f =
ft(œâ) such that
‚à´T
0
f 2
s ds < ‚àû(a.s.) for all T ‚ààR+.
(8.32)
The family of such functions satisfying (8.32) is denoted by S.
We deÔ¨Åne a sequence of stopping time (œÑn)n=1,2,... such that
œÑn = inf

t :
‚à´t
0
f 2
s ds ‚â•n

.
One can easily observe that (a.s.)
‚à´œÑn
0
f 2
s ds ‚â§n
and
œÑn ‚Üë‚àû, n ‚Üë‚àû,
and
I{t<œÑn } f ‚ààS2
for
b = 1, 2, . . . .
It follows from here that
Xn
t =
‚à´t
0
I{s<œÑn } fsdWs,
is well deÔ¨Åned for n = 1, 2, . . . one can also note that
Xn
t (œâ) = Xm
t (œâ),
for m ‚â•n, 0 ‚â§t ‚â§œÑn(œâ) and œâ ‚ààŒ©‚Ä≤, P(Œ©‚Ä≤) = 1.
Moreover, the sequence (Xn
t )n=1,2,... converges uniformly on Ô¨Ånite time intervals.
All arguments above give a possibility to deÔ¨Åne a stochastic integral for each
f ‚ààS as follows
‚à´t
0
fsdWs = lim
n‚Üí‚àû
‚à´t
0
Is<œÑn fsdWS.
(8.33)
Stochastic integral (8.33) for f ‚ààS admits a continuous modiÔ¨Åcation and saves a
linearity property. But it is not a martingale anymore. Nevertheless, the martingale
property is fulÔ¨Ålled locally:
‚à´t‚àßœÑn
0
fsdWs

t ‚â•0
is a square-integrable martingale for n = 1, 2, . . .

8.2
Stochastic integrals with respect to a Wiener process
97
Remark 8.3 Let us make a remark regarding Ô¨Ånite-dimensional processes.
We say that (Wt) is a d‚àídimensional Wiener process ifWt = (W1
t , . . ., W d
t ), where
(Wi
t ), i = 1, . . ., d, are one-dimensional independent Wiener process.
For d‚àídimensional random function ft = ( f 1
t , . . ., f d
t ) with components from S
we deÔ¨Åne
‚à´t
0
fsdWs =
d

i=1
‚à´t
0
f i
s dWi
s.
As far as a stochastic integration of matrix-valued random functions œÉt = (œÉi
t ), i =
1, . . ., d, j = 1, . . ., d, and œÉij ‚ààS we deÔ¨Åne
‚à´t
0 œÉsdWs as a d‚àídimensional process
with the i‚àíth coordinate 	d
j=1
‚à´t
0 œÉij
s dW j
s .
Remark 8.4 The construction of a stochastic integral developed in this section was
given by K. Ito. That is why this integral is often called the Ito integral. A similar
construction was given by R. Stratonovich who proposed to use a diÔ¨Äerent class of
integral sums:
N‚àí1

i=0
f (t‚àó
i )(Wti+1iWti),
where t‚àó
i = 1
2(ti+1 + ti).
The corresponding stochastic integral is called the Stratonovich integral.
Remark 8.5 It is well-known that the construction of the Rieman integral as a limit
of the integral sums does not depend on the choice of intermediate points inside of
subdivision intervals. It was shown in Remark 8.4 that the construction of a stochastic
integral is not invariant with respect to such a choice. Let us give another example
of this type. Take f (t) = W(t) = Wt and consider a subdivision of [0,T] :
0 = t0 < t1 < . . . < tn = T, tj = T j
n , j = 1, . . ., n.
For this subdivision we deÔ¨Åne two integral sums as follows
I(n)
1
=
n

i=1
Wti‚àí1(Wti ‚àíWti‚àí1) and I(n)
2
n

i=1
Wti(Wti ‚àíWti‚àí1).
For two real numbers a and b we note that a(b ‚àía) = 1
2(b2 ‚àía2) ‚àí1
2(a ‚àíb)2 and
b(b ‚àía) = 1
2(b2 ‚àía2) + 1
2(a ‚àíb)2.
Applying these elementary equalities to the integral sums we obtain that
I(n)
1
= 1
2
n

i=1
(W2
ti ‚àíW2
ti‚àí1 ‚àí1
2
n

i=1
(Wti ‚àíWti‚àí1)2 = 1
2W2
T ‚àí1
2
n

i=1
(Wti ‚àíWti‚àí1)2,
I(n)
2
= 1
2
n

i=1
(W2
ti ‚àíW2
ti‚àí1 ‚àí1
2
n

i=1
(Wti ‚àíWti‚àí1)2 = 1
2W2
T + 1
2
n

i=1
(Wti ‚àíWti‚àí1)2.

98
8
Elements of classical theory of stochastic processes
Hence,
EI(n)
1
= 1
2T ‚àí1
2E
n

i=1
(Wti ‚àíWti‚àí1)2 ‚ÜíT
2 ‚àíT
2 = 0, n ‚Üí‚àû,
EI(n)
2
= 1
2T + 1
2E
n

i=1
(Wti ‚àíWti‚àí1)2 ‚ÜíT
2 + T
2 = T, n ‚Üí‚àû,
and we observe that L2-limits of integral sums are diÔ¨Äerent.
Remark 8.6 It is very often another type of measurability of f is exploited in the
deÔ¨Ånition of stochastic integrals, i.e. f is progressively measurable in the sense: for
each t
f : ([0, t] √ó Œ©, B(0, t) √ó Ft) ‚Üí(R, B(R)).
Without loss of generality we can count S2
0, S2, S as classes of progressively measur-
able random functions.
8.3
The Ito processes: Formula of changing of variables,
theorem of Girsanov, representation of martingales
Let us take two progressively measurable (or Ft‚àíadapted and B(0, ‚àû) √ó F ‚àí
measurable) (bt(œâ))t ‚â•0 and (œÉt(œâ))t ‚â•0 satisfying the next integrability conditions:
for each T > 0 (a.s.)
‚à´T
0
|bs(œâ)|ds < ‚àû,
‚à´T
0
|œÉs(œâ)|2ds < ‚àû.
(8.34)
Conditions (8.34) imply a possibility to deÔ¨Åne a new stochastic process
Xt = X0 +
‚à´t
0
bsds +
‚à´t
0
œÉsdWs,
(8.35)
where X0 is a Ô¨Ånite (a.s.) random variable, (Wt)t ‚â•0 is a Wiener process generating
Ô¨Åltration (Ft)t ‚â•0.
The process (Xt)t ‚â•0 is called the Ito process. It is very convenient to write equality
(8.35) in the following diÔ¨Äerential form
dXt = btdt + œÉtdWt.
(8.36)
In the above representation (8.36) we can speak about dXt as a stochastic diÔ¨Äer-
ential of (Xt)t ‚â•0.
We will study the class of the Ito processes using both form (8.35) and (8.36).
As the Ô¨Årst result playing a very important role in Stochastic Analysis is the formula
of changing of variables (the Ito formula) for the class of smooth functions F(t, x),

8.3
The Ito processes: Formula of changing of variables . . .
99
having one continuous derivative ‚àÇF
‚àÇt and two continuous derivatives ‚àÇF
‚àÇx and ‚àÇ2F
‚àÇx2 ,
i.e. F ‚ààC1,2.
We start with the next lemma.
Lemma 8.3 Let (Wt)t ‚â•0 be a Wiener process, then for each t ‚â•0 (a.s.)
W2
t = 2
‚à´t
0
WsdWs + t.
(8.37)
Proof Without loss of generality we consider only t = 1 and Ô¨Ånd that in the sense
of convergence in probability (P‚àílim):
‚à´t
0
WsdWs = lim
n‚Üí‚àû

k<n
W k
n

W k+1
n ‚àíW k
n

= lim
n‚Üí‚àû

k<n
1
2

W k+1
n + W k
n
 
W k+1
n ‚àíW k
n

‚àí

k<n
1
2

W k+1
n ‚àíW k
n
2

.
(8.38)
It is clear that as n ‚Üí‚àû
	
k<n
1
2

W k+1
n + W k
n
 
W k+1
n ‚àíW k
n

= 1
2
	
k<n

W2
k+1
n ‚àíW2
k
n

‚Üí1
2W2
1,
	
k<n

W k+1
n ‚àíW k
n
2
‚Üí1
2.
Hence, we can get from (8.38) the equality (8.37).
‚ñ°
The next step is the formula for the product of two Ito‚Äôs processes (Xi
t )t ‚â•0, i = 1, 2,
with coeÔ¨Écients bi, œÉi correspondingly.
Lemma 8.4 For each t ‚â•0 (a.s.)
dX1
t X2
t = X1
t dX2
t + X2
t dX1
t + œÉ1
t œÉ2
t dt.
(8.39)
Proof Dividing [0, t] with the help of a subdivision (tk)k ‚â§n with the diameter
maxk ‚â§n |tk ‚àítk‚àí1| ‚Üí0, n ‚Üí‚àû, we can reduce the proof to the ‚Äúsmall‚Äù intervals
(tk, tk+1) of this subdivision.
For such small enough interval we have a discrete analog of (8.39):
X1
tk+1 X2
tk+1 ‚àíX1
tk X2
tk =
‚à´tk+1
tk
X1
s

b2
sds + œÉ2
s dWs

+
‚à´tk+1
tk
X2
s

b1
sds + œÉ1
s dWs

+
‚à´tk+1
tk
œÉ1
s œÉ2
s ds,
where we can think about bi, œÉi, i = 1, 2 as constants. Therefore, the problem is
reduced to the following four cases:
(1) X1
t = t, X2
t = t,

100
8
Elements of classical theory of stochastic processes
(2) X1
t = t, X2
t = Wt,
(3) X1
t = Wt, X2
t = t,
(4) X1
t = Wt, X2
t = Wt.
Case (1) is the usual diÔ¨Äerentiation case.
Case (4) is Lemma 8.3.
Case (2)-(3) are equivalent, and we need to show that for each t ‚â•0 (a.s.)
tWt =
‚à´t
0
Wsds +
‚à´t
0
sdWs,
which is true due to the following obvious relation: for t = 1
1 ¬∑ W1 =P ‚àílim
n‚Üí‚àû

k<n

 k + 1
n
W k+1
n ‚àík
nW k
n

=P ‚àílim
n‚Üí‚àû

k<n
k
n

W k+1
n ‚àíW k
n

+

k<n
1
nW k+1
n

.
So, we arrive to the equality (8.39).
‚ñ°
Lemma 8.5 For any polynomial function Pn(x) of degree n = 1, 2, . . . we have
dPn(Wt) = P‚Ä≤
n(Wt)dWt + 1
2P‚Ä≤‚Ä≤
n (Wt)dt.
(8.40)
Proof It is suÔ¨Écient to prove (8.40) for Pn(x) = xn. We can use the induction method
to prove that
dWn
t = nWn‚àí1
t
dWt + n(n ‚àí1)
2
Wn‚àí2
t
dt.
(8.41)
For n = 1 the formula (8.41) is trivial. Further, we take X1
t = Wn
t and X2
t = Wt, use
Lemma 8.4 with
b1
t = n(n ‚àí1)
2
Wn‚àí1
t
, œÉ1
t = nWn‚àí2
t
, b2
t = 0, œÉ2
t = 1,
and obtain (8.41):
dWn+1
t
=Wn
t dWt + Wt

nWn‚àí1
t
dWt + n(n ‚àí1)
2
Wn‚àí2
t
dt

+ nWn‚àí1
t
dt
=(n + 1)Wn
t dWt + n(n + 1)
2
Wn‚àí1
t
dt.
‚ñ°
Corollary 8.1 If F = F(x) is a continuously diÔ¨Äerentiable function with continuous
derivatives F‚Ä≤(x) and F‚Ä≤‚Ä≤(x), then for t ‚â•0 (a.s.)

8.3
The Ito processes: Formula of changing of variables . . .
101
dF(Wt) = F‚Ä≤(Wt)dWt + 1
2F‚Ä≤‚Ä≤(Wt)dt.
(8.42)
To prove (8.42) we uniformly (on compact intervals) approximate F(x) by a sequence
of polynomials Pn such that
F(x) = lim
n‚Üí‚àûPn(x), F‚Ä≤(x) = lim
n‚Üí‚àûP‚Ä≤
n(x), F‚Ä≤‚Ä≤(x) = lim
n‚Üí‚àûP‚Ä≤‚Ä≤
n (x)
and take the limit in the formula (8.40).
Corollary 8.2 Assume F : R+ √ó R ‚ÜíR is a function of class C1,2, then for t ‚â•0
(a.s.)
dF(t, Wt) =

 ‚àÇF
‚àÇt (t, Wt) + 1
2
‚àÇ2F
‚àÇx2 (t, Wt)

dt + ‚àÇF
‚àÇx (t, Wt)dWt.
(8.43)
The Ô¨Årst step of the proof is the case F(t, x) = F1(t)F2(x). Then (8.43) follows
from Lemma 8.4 and Corollary 8.1. The general case of F can be derived by
approximation of F(t, x) by a sequence of functions Fn(t, x) = 	
k ‚â§n F1,n(t)F2,n(x)
with ‚àÇFn
‚àÇt , ‚àÇFn
‚àÇx , ‚àÇ2Fn
‚àÇx2 converge uniformly on compact sets.
Remark 8.7 Let us deÔ¨Åne the local time of W at the level a during [0, t]:
lW(t, a) = lim
œµ‚Üí0
1
2œµ
‚à´t
0
I(a‚àíœµ,a+œµ)(Ws)ds,
which exists (a.s.) and in the sense of L2-space. Using this notion, one can get a
version of the Ito formula for non-diÔ¨Äerentiable functions, say, F(x) = |x| :
|Wt ‚àía| ‚àí|W0 ‚àía| =
‚à´t
0
sgn(Ws ‚àía)dWs + lW(t, a),
where
sgn(x) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
1,
x > 0,
0,
x = 0,
‚àí1,
x < 0.
Such a formula is called as the Tanaka formula.
The Ô¨Ånal formula (for Ito‚Äôs processes) is given in the next theorem.
Theorem 8.2 Assume F ‚ààC1,2 and
dXt = bt(œâ)dt + œÉt(œâ)dWt.
Then for t ‚â•0 (a.s.)
dF(t, Xt) =

 ‚àÇF
‚àÇt (t, Xt) + bt
‚àÇF
‚àÇx (t, Xt) + œÉ2
t
2
‚àÇ2F
‚àÇx2 (t, Xt)

dt + œÉt
‚àÇF
‚àÇx (t, Xt)dWt.
(8.44)

102
8
Elements of classical theory of stochastic processes
The formula (8.44) admits a generalization to a multidimensional case. Namely,
assume F : R+ √ó Rk ‚ÜíR of class C1,2, the process (Xt) in (8.35) is deÔ¨Åned by
a vector-function bt(œâ) = b(t, œâ) = (b1
t (œâ, . . ., bk
t (œâ)) and a matrix-value function
œÉt(œâ) = œÉ(t, œâ) = (œÉij
t (œâ))i‚â§k,j ‚â§d:
Xi
t = Xi
0 +
‚à´t
0
bi
sds +
‚à´t
0
d

j=1
œÉij
s dW j
s, i = 1, . . ., k,
and Wt = (W1
t , . . ., W d
t ) is a d‚àídimensional Wiener process. Then for t ‚â•0 (a.s.)
dF(t, Xt) =

‚àÇF
‚àÇt (t, Xt) +
k

i=1
‚àÇF
‚àÇxi (t, Xt)bi
t + 1
2
k

i,j=1
‚àÇ2F
‚àÇxi‚àÇxj
(t, Xt)
(8.45)
√ó
d

l=1
œÉil
t œÉlj
t

dt +
d

j=1
 k

i=1
‚àÇF
‚àÇxi
(t, Xt)œÉij
t

dW j
t .
(8.46)
Example 8.2 (Stochastic exponential) Consider the Ito process (8.35). DeÔ¨Åne the
process
Et(X) = exp
!
Xt ‚àí1
2
‚à´t
0
œÉ2
s ds
"
.
(8.47)
Applying the Ito formula to function F(t, x) = e‚àíœÉ2
2 tex, (t, x) ‚ààR+ √ó R, we Ô¨Ånd that
dEt(X) = Et(X)dXt, E0(X) = 1.
(8.48)
We will call (Et(X))t ‚â•0 a stochastic exponent of process (Xt).
Let us count the following properties of stochastic exponentials (8.47)-(8.48).
Problem 8.3 1. Et(X) > 0 (a.s.), t ‚ààR+;
2.
1
Et(X) = Et( ÀúX), (a.s.) t ‚ààR+, where ÀúXt = Xt ‚àí
‚à´t
0 œÉ2
s ds,
3. If bt = 0 a.s., t ‚ààR+, then (Et(X))t ‚â•0 is a (local) martingale;
4. Et(X1)Et(X2) = Et
X1 + X2 + [X1, X2] (a.s.), t ‚ààR+, where dXi
t = bi
tdt +
œÉidWt, d[X1, X2]t = œÉ1
t œÉ2
t dt, i = 1, 2, and this property is called a multiplication
rule of stochastic exponentials.
We already demonstrated how important and useful the Ito formula. The next
consideration has an incredible meaning for Stochastic Analysis and its applications.
Let us consider a d‚àídimensional random function bt = (b1
t, . . ., bd
t ) belonging to the
space S (or S2) and deÔ¨Åne a new process

8.3
The Ito processes: Formula of changing of variables . . .
103
Zt = exp
!‚à´t
0
bsdWs ‚àí1
2
‚à´t
0
|bs|2ds
"
= exp
 d

i=1
‚à´t
0
bi
sdWi
s ‚àí1
2
d

i=1
‚à´t
0

bi
s
2
ds
#
,
(8.49)
Z0 =1.
The process (8.49) satisÔ¨Åes the following properties
(1) dZt = ZtbtdWt, Z0 = 1;
(2) (Zt)t ‚â•0 is a supermartingale, which is a martingale, if (bt)t ‚â•0 is bounded or
EZt = 1, t ‚ààR+;
(3) if EZT = 1 for some T > 0, then (Zt)t ‚â§T is a martingale, and for any sequence
bn ‚ààS(S2) with the property
‚à´T
0 |bn ‚àíb|2ds ‚Üí0, n ‚Üí‚àû, (a.s.) the corresponding
sequence ZT(bn) converges to ZT(b) in the space L1.
To prove (1) we just apply the Ito formula. For the proof of (2) we consider a
sequence of stopping times
œÑn = inf

t :
‚à´t
0
|bs|2Z2
s ds ‚â•n

and Ô¨Ånd that the process
‚à´t
0
I(s<œÑn)bsZsdWs

t ‚â•0
is a martingale, as well as (Zt‚àßœÑn)t ‚â•0.
For t1 ‚â§t2 we have (a.s.)
E Zt2‚àßœÑn |Ft1
 = Zt1‚àßœÑn, n=1,2,...
and as n ‚Üí‚àûwe get by Fatou‚Äôs Lemma that
E(Zt2|Ft1) ‚â§Zt1 (a.s.),
which certiÔ¨Åes that (Zt)t ‚â•0 is a supermartingale, and therefore
E exp
‚à´t
0
bsdWs ‚àí1
2
‚à´t
0
|bs|2ds

‚â§1.
(8.50)
To prove (3) we Ô¨Årst note that for |bs| ‚â§K < ‚àûwe derive from (8.50) that

104
8
Elements of classical theory of stochastic processes
E
‚à´t
0
|bs|2Z2
s ds ‚â§K2E
‚à´t
0
Z2
Sds
=K2
‚à´t
0
EZ2
s (2b) exp
‚à´t
0
|bu|2du

ds
‚â§K2
‚à´t
0
eK2sds < ‚àû,
and conclude that
‚à´t
0 bsZsds

t ‚â•0 and (Zt)t ‚â•0 are martingales.
The claim (3) follows from claim (2) above, where EZT(b) = 1 and ZT(bn)
P‚àí‚Üí
ZT(b), n ‚Üí‚àû. As well as E|ZT(bn) ‚àíZT(b)| ‚Üí0, n ‚Üí‚àû, and hence Zt(bn) =
E(ZT(bn)|Ft), t ‚â§T, a.s. Taking the limit in the above equality as n ‚Üí‚àûwe get the
martingale property of (Zt(b))t ‚â§T .
Now we are ready to formulate a beautiful Girsanov Theorem.
Theorem 8.3 Assume (Wt)t ‚â§T is a Wiener process on a probability space (Œ©, F, P).
Let b be a random function from space S (or S2) such that EZT = 1. DeÔ¨Åne process
Xt = Wt ‚àí
‚à´t
0 bsds. Then
(1) a random variable ZT deÔ¨Ånes a new probability measure ÀúP, equivalent to P,
i.e. ÀúP << P and P << P with the density ZT = d ÀúP
dP ;
(2) The process (Xt) is a Wiener process with respect to ÀúP.
Proof. For simplicity, consider a bounded function (bt). For 0 ‚â§t0 ‚â§t1 ‚â§. . . ‚â§
tn = T and Ô¨Åxed (Œªj)j=0,...,n‚àí1 from Rd we deÔ¨Åne the process Œªs as iŒªj on [tj, tj+1).
Further, we obtain that
ÀúE exp
‚éß‚é™‚é®
‚é™‚é©
i
n‚àí1

j=0
Œªj(Xtj+1 ‚àíXtj )
‚é´‚é™‚é¨
‚é™‚é≠
=E exp
!‚à´T
0
ŒªsdWs ‚àí
‚à´T
0
Œªsbsds
"
ZT(b)
=EZT(Œª + b) exp
1
2
‚à´T
0
Œª2
sds

=e1/2
‚à´T
0 Œª2
sds
which corresponds to a Gaussian distribution. Taking into account X0 = 0, we arrive
to the claim (2) of the theorem.
To Ô¨Ånish this section we study the structure of any martingale (Mt)t ‚â•0 on stochastic
basis (Œ©, F, (Ft = F W
t
)t ‚â•0, P). It turns out, the set of all martingales on the stochastic
basis, generated by a Wiener process (Wt)t ‚â•0 is determined by stochastic integrals.
Namely, any square integrable martingale (Mt)t ‚â•0, M0 = 0, admits
Mt =
‚à´t
0
œÜsdWs (a.s.), t ‚â•0,
(8.51)
where (œÜt)t ‚â•0 is uniquely deÔ¨Åned progressively measurable function from space S2.
The relation (8.51) is called a martingale representation. Let us prove it for a Ô¨Ånite

8.3
The Ito processes: Formula of changing of variables . . .
105
interval [0,T]. For a deterministic function Œ≤ ‚ààL2(0,T) we deÔ¨Åne the Girsanov
exponent
Zt = Zt(Œ≤) = exp
!‚à´t
0
Œ≤udu ‚àí1
2
‚à´t
0
Œ≤2
udu
"
, Z0 = 1
Let us note that
Z2
t (Œ≤) = Zt(2Œ≤)exp
!‚à´t
0
Œ≤2
udu
"
,
and EZt(2Œ≤) ‚â§1 for all t ‚â§T.
Hence, we obtain
EZ2
t (Œ≤) ‚â§exp
!‚à´t
0
Œ≤2
udu
"
< ‚àû.
LetX2 be a class of random variables X from the space L2 = L2(Œ©, F = F W
T , P) = L2
such that X =
‚à´T
0 œÜsdWs with a progressively measurable œÜ and
‚à´T
0 EœÜ2
sds < ‚àû.
Further, if Y is an arbitrary random variable from L2 such that EY = 0 and Y ‚ä•X2 in
the L2-sense, then EY.ZT(Œ≤) = 0 for all Œ≤ ‚ààL2(0,T). Therefore taking an arbitrary
subdivision (ti), i = 1, ..., n of the interval [0,T] and constructing an arbitrary step
function with (ti), we obtain that E(Y|Wt1, ...,Wtn) = 0. Hence, Y = 0 (a.s.) due to
FT = F W
T . The last step of the proof is clear. We take MT as X ‚ààX2 and using the
martingale and stochastic integral properties, obtain
Mt = E(MT |Ft) =
‚à´t
0
œÜsdWs.

Chapter 9
Stochastic diÔ¨Äerential equations,
diÔ¨Äusion processes and their applications
Abstract The chapter presents stochastic diÔ¨Äerential equations (SDEs) and their
connections with diÔ¨Äusion processes and partial diÔ¨Äerential equations (PDEs). The
existence and uniqueness of solutions of SDEs are proved under Lipschitz‚Äôs con-
ditions. Two important processes (Geometric Brownian Motion (GBM) and the
Ornstein-Uhlenbeck process) are constructed on this theoretical base. The diÔ¨Äer-
ence between ordinary diÔ¨Äerential equations and SDEs are discussed. As a part of
this discussion, the existence of a solution (weak solution) of any SDE with mea-
surable bounded drift coeÔ¨Écient and unit diÔ¨Äusion is proved with the help of the
miracle Girsanov theorem. Moreover, it is shown by mean of the method of mono-
tonic approximations that such a solution will be strong if the grift coeÔ¨Écient is
a bounded piece-wise smooth function. DiÔ¨Äusion processes are deÔ¨Åned as Markov
processes for which their transition densities satisfy the asymptotic properties of
Kolmogorov. The backward and forward equations of Kolmogorov are derived. A
connection between SDEs and PDEs are stated with the help of the Feynman-Kac
theorem. Absolute continuity of distributions of diÔ¨Äusion processes is studied with
the help of the Girsanov theorem. A special attention is paid to the class of controlled
diÔ¨Äusion processes for which the Hamilton-Jacobi-Bellman optimality equation is
derived. It is shown how the theory of diÔ¨Äusion processes and SDEs are helpful
in mathematical Ô¨Ånance (Bachelier and Black-Scholes models) and in statistics of
random processes (see [3], [5], [14], [17], [21], [22], [23], [24], [25], [30], [35], [39],
[41], [42], and [44]).
9.1
Stochastic diÔ¨Äerential equations
Let b(t, x) and œÉ(t, x) be (t, x)‚àímeasurable locally bounded functions from R+ √ó
R ‚ÜíR, i.e. these functions are bounded on each compact set of R+ √ó R.
If a continuous Ft-adapted process X(t, œâ) = Xt(œâ) such that for all t ‚â•0 (a.s.)
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_9
107

108
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Xt(œâ) = X0(œâ) +
‚à´t
0
b(s, Xs)ds +
‚à´t
0
œÉ(s, Xs)dWs,
(9.1)
where X0(œâ) is a Ô¨Ånite random variable, measurable w.r. to F0, (Wt)t ‚â•0 is a Wiener
process, then it is called a solution of (9.1). We rewrite (9.1) in an equivalent
diÔ¨Äerential form
dXt = bt(Xt)dt + œÉt(Xt)dWt.
(9.2)
We say that such a solution is unique if for any other solution ( ÀúXt) we have
P(œâ : sup
t
|Xt(œâ) ‚àíÀúXt(œâ)| > 0) = 0.
(9.3)
Theorem 9.1 Assume that coeÔ¨Écients b(t, x) and œÉ(t, x) of stochastic equations
(9.1)-(9.2) satisfy the conditions:
1) For each T ‚â•0 there exists a constant kT such that
|b(t, x)| + |œÉ(t, x)| ‚â§kT(1 + |x|)
for (t, x) ‚àà[0,T] √ó R;
2) For all c > 0 there exists a constant lc > 0 such that
|b(t, x) ‚àíb(t, y)| + |œÉ(t, x) ‚àíœÉ(t, y)| ‚â§lc|x ‚àíy|
for all (t, x) ‚àà[0, c] √ó [‚àíc, c].
Then the equation (9.1)-(9.2) admits one and only one solution.
Proof First, we note that conditions 1)-2) of this theorem are called a linear growth
and a local Lipschitz conditions, correspondingly. Second, we give the proof for
a standard Lipschitz condition (lc = l) and for a Ô¨Ånite time interval [0,T], and for
X0(œâ) = x ‚ààR.
Let us prove a uniqueness of solutions. Assume X(t, œâ) and ÀúX(t, œâ) are two
solutions with the initial condition X(0, œâ) = ÀúX(0, œâ) = x. Then we have
ÀúX(t) ‚àíX(t) =
‚à´t
0

b(s, ÀúX(s)) ‚àíb(s, X(s))

ds +
‚à´t
0

œÉ(s, ÀúX(s)) ‚àíœÉ(s, X(s))

dWs.
Further, using the Cauchy-Schwartz inequality and the isometry property, we obtain
for each t ‚àà[0,T] that

9.1
Stochastic diÔ¨Äerential equations
109
E| ÀúX(t) ‚àíX(t)|2 ‚â§2E
‚à´t
0

b(s, ÀúX(s)) ‚àíb(s, X(s))

ds
2
(9.4)
+ 2E
‚à´t
0

œÉ(s, ÀúX(s)) ‚àíœÉ(s, X(s))

dWs
2
‚â§2tE
‚à´t
0

b(s, ÀúX(s)) ‚àíb(s, X(s))
2 ds + 2E
‚à´t
0

œÉ(s, ÀúX(s)) ‚àíœÉ(s, X(s))
2 ds
‚â§2(t + 1)l2
‚à´t
0
E| ÀúX(s) ‚àíX(s)|2ds.
(9.5)
To continue we need the following Gronwall lemma:
If œÜ(t) ‚â•0 is an integrable function satisfying inequality for all t ‚â•0 and Ô¨Åxed
constants A and B :
œÜ(t) ‚â§A + B
‚à´t
0
œÜ(s)ds.
(9.6)
Then œÜ(t) ‚â§AeBt for all t ‚â•0.
To prove Grownwall lemma we just diÔ¨Äerentiate the logarithm of the right hand
side of (9.6):

ln

A + B
‚à´t
0
œÜ(s)ds
‚Ä≤
=
BœÜ(t)
A + B
‚à´t
0 œÜ(s)ds
‚â§B,
ln

A + B
‚à´t
0
œÜ(s)ds

‚â§ln A + Bt,
A + B
‚à´t
0
œÜ(s)ds ‚â§AeBt.
In relations (9.5) the function œÜ(t) = E| ÀúX(t) ‚àíX(t)|2 and A = 0, B = 2(T + 1)l2.
Hence, by the Gronwall lemma we obtain E| ÀúX(t) ‚àíX(t)|2 = 0. We derive from here
that for the set QT of rational numbers of [0,T] that
P

sup
t ‚ààQT
| ÀúX(t) ‚àíX(t)| = 0
	
= 1.
Due to a continuity of processes X(t) and ÀúX(t) we have
1 = P(sup
QT
| ÀúX(t) ‚àíX(t)| = 0) = P( sup
t ‚àà[0,T]
| ÀúX(t) ‚àíX(t)| = 0),
which means that X and ÀúX are equivalent.
To prove the existence of a solution of (9.1)-(9.2), we apply the Picard method of
successive approximations:

110
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
X0(t) =x,
Xn(t) =x +
‚à´t
0
b(s, Xn‚àí1(s))ds +
‚à´t
0
œÉ(s, Xn‚àí1(s))dWs.
(9.7)
For the sequence of approximations (9.7) we get for t ‚àà[0,T] that
|Xn(t)|2 ‚â§3|x|2 + 3




‚à´t
0
b(s, Xn‚àí1(s))ds




2
+ 3
‚à´t
0
b(s, Xn‚àí1(s)dWs
2
.
(9.8)
Inequality (9.8) implies for t ‚â§T that
E|Xn(t)|2 ‚â§3|x|2 + LT
‚à´t
0
E|Xn‚àí1(s)|2ds,
(9.9)
where LT depends on kT .
Taking supk ‚â§n in (9.9) we derive from Gronwall‚Äôs Lemma that
sup
k ‚â§n
E|Xk(t)|2 ‚â§3|x|2eT LT, t ‚àà[0,T].
(9.10)
Further, we have that
Xn+1(t) ‚àíXn(t) =
‚à´t
0

b(s, Xn(s)) ‚àíb(s, Xn‚àí1(s))

ds
+
‚à´t
0

œÉ(s, Xn(s)) ‚àíœÉ(s, Xn‚àí1(s))

dWs.
and applying a similar estimation as the proof of uniqueness, we get for t ‚â§T :
E|Xn+1(t) ‚àíXn(t)|2 ‚â§KT
‚à´t
0
E|Xn(s) ‚àíXn‚àí1(s)|2ds,
(9.11)
E|X1(t) ‚àíX0(t)|2 ‚â§E
‚à´t
0
b(s, x))ds +
‚à´t
0
œÉ(s, x)dWs
2
‚â§CTt,
(9.12)
with some constants CT and KT .
Using the induction we derive from (9.11) -(9.12) that
E|Xn+1(t) ‚àíXn(t)|2 ‚â§CT Kn
T
tn+1
(n + 1)!.
(9.13)
Due to (9.13) we conclude that the series
X0(t) +
‚àû

n=0

Xn+1(t) ‚àíXn(t)

converges uniformly to continuous process X(t) in the L2-sense.
To prove that this process is a solution of (9.1)-(9.2) we note for t ‚àà[0,T] that

9.1
Stochastic diÔ¨Äerential equations
111
E
‚à´t
0
[b(s, X(s)) ‚àíb(s, Xn(s))] ds +
‚à´t
0
[œÉ(s, X(s)) ‚àíœÉXn(s))] dWs
2
‚â§KT
‚à´T
0
E|X(s) ‚àíXn(s)|2ds.
Using this inequality, we obtain that
E

X(t) ‚àíx ‚àí
‚à´t
0
b(s, X(s))ds +
‚à´t
0
œÉ(s, X(s))dWs
2
‚â§2E|X(t) ‚àíXn(t)|2
+ 2E
‚à´t
0
b(s, Xn‚àí1(s))ds +
‚à´t
0
œÉ(s, Xn‚àí1(s))dWs ‚àí
‚à´t
0
b(s, X(s))ds ‚àí
‚à´t
0
œÉ(s, X(s))dWs
2
‚â§2E|X(t) ‚àíXn(t)|2 + 2KT E|X(t) ‚àíXn‚àí1(t)|2 ‚Üí0,
as n ‚Üí‚àû.
‚äì‚äî
Remark 9.1 To emphasize a dependence of the limit of successive approximations
from an initial point x (correspondently, y) denote it by Xx(t, œâ) (correspondently,
Xy(t, œâ)). Similar to inequality (9.10) one can derive for t ‚àà[0,T] :
E|Xx(t) ‚àíXy(t)|2 ‚â§const(T)|x ‚àíy|2et¬∑const(T),
E|Xx(t) ‚àíXx(t + Œît)|2 ‚â§const(T)|1 + |x|2|2et¬∑const(T).
Due to these inequalities the function Xx(t) can be chosen measurable w.r. to x.
Example 9.1 Consider a linear SDE
dXt = ŒºXtdt + œÉXtdWt,
(9.14)
where X0 ‚àíF0-measurable Ô¨Ånite random variable. According to Theorem 8.1 the
equation (9.14) has a unique solution. To Ô¨Ånd a concrete formula for Xt we rewrite
(9.1) in the form
dXt = (c + œÉ2/2)Xtdt + œÉXtdWt,
(9.15)
where c = Œº ‚àíœÉ2/2.
Applying the Ito formula to X0 exp{ct + œÉWt} = ÀúXt we Ô¨Ånd that
d ÀúXt =d

X0ect+œÉWt

=

cX0ect+œÉWt + œÉ2
2 X0ect+œÉWt

dt + œÉX0ect+œÉWt dWt
=

c + œÉ2
2

ÀúXtdt + œÉ ÀúXtdWt.

112
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
So, using the uniqueness of solution of (9.14) we arrive to conclusion that Xt = ÀúXt
(a.s.) for all t ‚â•0. Hence,
Xt = X0ect+œÉWt = X0e(Œº‚àíœÉ2/2)t+œÉWt,
(9.16)
which is called a Geometric Brownian Motion (GBM). The GBM (9.16) is exploited
as a stock price model in mathematical Ô¨Ånance.
Example 9.2 Consider another linear stochastic diÔ¨Äerential equation
dXt = ‚àíŒ±Xtdt + œÉdWt,
(9.17)
X0 is F0-measurable Ô¨Ånite random variable, Œ± ‚ààR.
Let us transform (Xt) with the help of function f (t, x) = eŒ±t.x, x ‚ààR. By the Ito
formula we can Ô¨Ånd a stochastic diÔ¨Äerential of Yt = f (t, Xt) :
dYt =( f ‚Ä≤
t ‚àíŒ±Xt f ‚Ä≤
x + 1
2œÉ2 f ‚Ä≤‚Ä≤
xx)dt + œÉ f ‚Ä≤
xdWt
= Œ±eŒ±t Xt ‚àíŒ±eŒ±t Xt
 dt + œÉeŒ±tdWt = œÉeŒ±tdWt.
It leads to Yt = X0 + œÉ
‚à´t
0 eŒ±sdWs and hence
Xt = e‚àíŒ±tYt = e‚àíŒ±t X0 + œÉe‚àíŒ±t
‚à´t
0
eŒ±sdWs.
This is a famous Ornstein-Uhlenbeck process, which is called a Vasicek model for
interest rate modeling in mathematical Ô¨Ånance.
We observed here that for SDEs with Lipschitz-type coeÔ¨Écients existence and
uniqueness of solutions are stated in a similar way as for ordinary diÔ¨Äerential equa-
tions. A natural question arises:
Are there new results in this theory, which are diÔ¨Äerent from the deterministic theory?
We start our answer with the following nice result of Girsanov.
Theorem 9.2 Let b(t, x), (t, x) ‚àà[0,T] √ó Rd, d ‚â•1, be a bounded measurable func-
tion. Then there exists a probability space (Œ©, F, P), a continuous process (Xt) and
a Wiener process on this space such that (Xt)t ‚â§T is a solution of the SDE
dXt = b(t, Xt)dt + dWt, X0 = 0, t ‚â§T.
(9.18)
Proof Take any probability space (Œ©, F, ÀúP) with a Wiener process (Xt)t ‚â§T . DeÔ¨Åne
another process
Wt = Xt ‚àí
‚à´t
0
b(s, Xs)ds,
(9.19)
and a new probability measure P with the density w.r.to ÀúP:

9.1
Stochastic diÔ¨Äerential equations
113
dP
d ÀúP = exp
‚à´T
0
b(s, Xs)dXs ‚àí1
2
‚à´T
0
|b(s, Xs)|2ds.

By the Girsanov theorem, the process (Wt)t ‚â§T is a Wiener process on the probability
space (Œ©, F, P). Hence, if we rewrite the equality (9.19) as
Xt =
‚à´t
0
b(s, Xs)ds + Wt, t ‚â§T,
we arrive to conclusion that (Xt)t ‚â§T is a solution of (9.18).
‚äì‚äî
Remark 9.2 The result of Theorem 9.2 guarantees existence of a solution of (9.18)
for any bounded measurable function b(t, x). It is well-known that the ordinary
diÔ¨Äerential equation dxt = b(t, xt)dt does not admit a solution for any bounded
measurable function b(t, x).
Remark 9.3 Comparing solutions in Theorem 9.1 and 9.2 we can observe the fol-
lowing diÔ¨Äerence. The solution in Theorem 9.1 was constructed on given probability
space. On the other hand, the solution in Theorem 9.2 was built on some probability
space. They are naturally called strong and weak solutions, respectively, because any
strong solution will be also a weak solution, but not vice versa.
Example 9.3 Let (Œ≤t) be a Brownian Motion (BM) with Œ≤0 = y ‚ààR. DeÔ¨Åne a pro-
cess Bt =
‚à´t
0 sgn(Œ≤S)dŒ≤s, where
sgn(x) =

1,
x > 0,
‚àí1,
x ‚â§0.
The process (Bt) is a BM by the Levy characterization. Further, we note that by a
property of stochastic integral
y +
‚à´t
0
sgn(Œ≤s)dBs =y +
‚à´t
0
(sgn(Œ≤s))2dŒ≤s
=y +
‚à´t
0
dŒ≤s = y + Œ≤t ‚àíy = Œ≤t.
Hence, (Œ≤t) solves the SDE
dXt = sgn(Xt)dBt, X0 = y,
(9.20)
in the ‚Äúweak‚Äù sense. Moreover, any other solution will be again a BM, and in the
sense of its distribution such a solution is unique (weak uniqueness). But taking
y = 0 we Ô¨Ånd that (Œ≤t) and (‚àíŒ≤t) solve the equation (9.20) with some BM (Bt) and
X0 = 0. It means there is no uniqueness as (9.5).
Let us develop the method of monotonic approximations, which is powerful enough
to prove existence of strong solutions without the Lipschitz conditions. We start with
the simplest version of the comparison theorem of strong solutions of SDEs.

114
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Lemma 9.1 Assume that bi = bi(x), x ‚ààR, i = 1, 2, are bounded continuous func-
tions, and Xi, i = 1, 2, are strong (continuous) solutions of the equations
dXi
t = bi(Xi
t )dt + dWt, Xi
0 = 0, t ‚â•0.
Then the inequality b1(x) < b2(x) for all x ‚ààR implies that X1
t (œâ) ‚â§X2
t (œâ) for
almost all œâ ‚ààŒ© and all t ‚â•0.
Proof For each œâ ‚ààÀúŒ©, P( ÀúŒ©) = 1, we deÔ¨Åne
Œ∏ =

inf{t > 0 : X1
t (œâ) = X2
t (œâ)},
‚àûotherwise.
Note that X2
t (œâ) ‚àíX1
t (œâ) =
‚à´t
0

b2(X2
s (œâ)) ‚àíb1(X1
s (œâ))

ds = Œît(œâ). Due to con-
tinuity of bi(x), Xi
t (œâ), i = 1, 2, and assumption b1(x) < b2(x), x ‚ààR, we Ô¨Ånd
that Œî0(œâ) > 0, Œît(œâ) is continuous and Œît(œâ) > 0 for all t < Œ∏(œâ). If Œ∏(œâ) = ‚àû,
then X2
t (œâ) = X1
t (œâ), and Lemma is proved. Otherwise, we have Œ∏(œâ) < ‚àûand
ŒîŒ∏(œâ) = X2
Œ∏ ‚àíX1
Œ∏ = 0. It gives a possibility to reproduce for Œ∏ the same consider-
ation as for t = 0 and Ô¨Ånd that X2
t (œâ) ‚àíX1
t (œâ) = Œît(œâ) > 0 for a neighborhood of
Œ∏(œâ), etc. Therefore, X2
t (œâ) ‚àíX1
t (œâ) = Œît(œâ) > 0 for all t ‚â•0.
‚äì‚äî
Theorem 9.3 Consider a homogeneous SDE (9.18) and suppose that the drift coef-
Ô¨Åcient bt(x) = b(x) satisÔ¨Åes the following conditions:
(1) b(x) has a Ô¨Ånite number of points of discontinuity {x1, . . ., xN };
(2) b(x) is piece-wise smooth, i.e. for any x ‚ààR \ {x1, . . ., xN } the derivative
b‚Ä≤(x) exists;
(3) |b(x)| and |b‚Ä≤(x)| ‚â§K < ‚àû.
Then the equation (9.18) has a strong solution.
Proof The Ô¨Årst step is to show that b(x) can be approximated by a sequence of
functions bn(x) such that a strong solution of the equations
dXn(t, œâ) = bn(Xn(t, œâ))dt + dW(t, œâ),
(9.21)
Xn(0, œâ) = 0, t ‚àà[0,T], n = 1, 2, . . .,
does exist and is unique. Then the properties of these solutions are studied with
the aim to show that the sequence (Xn(t, œâ))n=1,2,... converges to a continuous
Ft‚àíadapted process X‚àû(t, œâ) which is the desired solution of the limit equation
(9.18).
To construct the auxiliary sequence of functions (bn(x))n=1,2,... we take neigh-
borhoods Vr(xi) of the points x1 < . . . < xN of radius r ‚â§1
4 min1‚â§i‚â§N‚àí1 |xi+1 ‚àíxi|
and set
Àúbn(x) = b(x),
x ‚ààR \ ‚à™N
i=1Vr(xi).

9.1
Stochastic diÔ¨Äerential equations
115
It suÔ¨Éces now to build Àúbn(x) in the neighborhood Vr(xi), i = 1, 2, . . ., N. Consider
Vr(xi) and let b(xi‚àí) > b(xi+) for limits from the left and right side of xi corre-
spondingly.
It follows from the smoothness of b(x) that there exists a deterministic sequence
(Œ±i
n)n=1,2,... such that Œ±i
n ‚ààVr(xi), Œ±i
n > xi, Œ±i
n ‚Üìxi as n ‚Üí‚àû, while the continuous
function Àúbn(x) is determined as follows
Àúbn(x) =b(x) for x ‚ààVr(xi) ‚à©

(xi‚àí1, xi) ‚à™[Œ±i
n, xi+1)

,
Àúbn(xi) =b(xi‚àí),
Àúbn(x) is linear on [xi, Œ±i
n] satisfying the condition Àúbn(x) ‚â•b(x), x ‚àà(xi, Œ±i
n).
If b(xi‚àí) < b(xi+), the construction of Àúbn(x) in Vr(xi) is analogous. Finally, we
deÔ¨Åne bn(x) = Àúbn(x) + 1/n, n = 1, 2, . . ., with desirable properties for us:
bn(x) > bn+1(x), x ‚ààR,
bn(x) ‚Üìb(x) as n ‚Üí‚àû, x ‚ààR \ {x1, . . ., xN },
bn(x) satisÔ¨Åes a Lipschitz condition; and simultaneously for n = 1, 2, . . .
|bn(x)| ‚â§K + 1 < ‚àû.
Applying Theorem 9.1 to (9.21) with bn(x), constructed above, we get the existence
(and uniqueness) of strong solutions Xn(t), which are continuous and Ft‚àíadapted.
Further, according to Lemma 9.1 we obtain the system of inequalities
X1(t, œâ) ‚â•X2(t, œâ) ‚â•. . . ‚â•Xn(t, œâ) ‚â•. . . (a.s.), t ‚â§T.
Therefore, there exists a Ft-adapted process X‚àû(t, œâ) such that X‚àû(t, œâ) =
limn‚Üí‚àûXn(t, œâ) (a.s.), t ‚àà[0,T].
‚äì‚äî
Lemma 9.2 The family of continuous functions (Xn(¬∑, œâ))n=1,2,..., œâ ‚ààÀúŒ©, P( ÀúŒ©) = 1,
is uniformly bounded and equicontinuous.
Proof For each œâ from ÀúŒ©, P( ÀúŒ©) = 1, we have for all n = 1, 2, . . . that
|Xn(t, œâ)| ‚â§




‚à´t
0
bn(Xn(s, œâ))ds




 + |W(t, œâ)|
(9.22)
‚â§
‚à´t
0
|bn(Xn(s, œâ))|ds + |W(t, œâ)|
(9.23)
‚â§(K + 1)T + max
0‚â§t ‚â§T |W(t, œâ)|,
(9.24)
which means that (Xn(¬∑, œâ))n=1,2,... is uniformly bounded.
Further, for œâ ‚ààÀúŒ©, P( ÀúŒ©) = 1, Œ¥ > 0, we obtain

116
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
œâXn(¬∑,œâ)(Œ¥) =
sup
s,t, |t‚àís|<Œ¥
|Xn(t, œâ) ‚àíXn(s, œâ)|
‚â§
sup
s,t, |t‚àís|<Œ¥




‚à´t
s
bn(Xn(u, œâ)du




 + |Wn(t, œâ) ‚àíW(s, œâ)|
‚â§(K + 1)Œ¥ + œâW(¬∑,œâ)(Œ¥).
Hence, for almost all œâ we have as Œ¥ ‚Üí0 that supn œâXn(¬∑,œâ)(Œ¥) ‚â§(K + 1)Œ¥ +
œâW(¬∑,œâ)(Œ¥) ‚Üí0.
In view of Lemma 9.2 and the Ascoli-Arzela test there exists (for œâ ‚ààÀúŒ©, P( ÀúŒ©) = 1)
a subsequence n‚Ä≤(œâ) such that
sup
0‚â§t ‚â§T
|Xn‚Ä≤(œâ)(t, œâ) ‚àíX‚àû(t, œâ)| ‚Üí0, n‚Ä≤(œâ) ‚Üí‚àû,
and therefore almost all paths of X‚àû(t, œâ) are continuous on [0,T].
Nowweput œÜn(t, œâ) = bn(W(t, œâ)), n = 1, 2, . . ., œÜ‚àû(t, œâ) = b(W(t, œâ)) anddeÔ¨Åne
probability measures (Pn)n=1,2,... and P‚àûwith the help of the Girsanov exponents:
dPn
dP = exp
‚à´T
0
œÜn(s, œâ)dW(s, œâ) ‚àí1
2
‚à´T
0
œÜ2
n(s, œâ)ds

,
dP‚àû
dP = exp
‚à´T
0
œÜ‚àû(s, œâ)dW(s, œâ) ‚àí1
2
‚à´T
0
œÜ2
‚àû(s, œâ)ds

.
Problem 9.1 Prove that for y ‚ààR and t ‚àà[0,T]
Pn(œâ : W(t, œâ) ‚â§y) = P(œâ : Xn(t, œâ) ‚â§y),
(9.25)
Pn(œâ : W(t, œâ) ‚â§y) ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûP(œâ : W(t, œâ) ‚â§y),
(9.26)
Pn(œâ : Xn(t, œâ) ‚â§y) ‚ÜíP(œâ : X‚àû(t, œâ) ‚â§y).
(9.27)
Relations (9.25)-(9.27) yield
P(œâ : X‚àû(t, œâ) ‚â§y) ‚ÜíP‚àû(œâ : W(t, œâ) ‚â§y).
(9.28)
Lemma 9.3 If A is a Borel set in R with the Lebesgue measure zero (l(A) = 0), then
P(œâ : l(t ‚àà[0,T] : X‚àû(t, œâ) ‚ààA) = 0) = 1.
(9.29)
Proof For Ô¨Åxed œµ > 0 we Ô¨Årst apply Chebyshev‚Äôs inequality and Fubini‚Äôs theorem,
then we use (9.28) to derive that

9.2
DiÔ¨Äusion processes and their connection with SDEs and PDEs
117
P(œâ : l(t ‚àà[0,T] : X‚àû(t, œâ) ‚ààA) > œµ) ‚â§œµ‚àí1E
‚à´T
0
I(t:X‚àû(t,œâ)‚ààA)dt
=œµ‚àí1
‚à´T
0
EI(t:X‚àû(t,œâ)‚ààA)dt
=œµ‚àí1
‚à´T
0
‚à´
Œ©
I(t:X‚àû(t,œâ)‚ààA)dP

dt
=œµ‚àí1
‚à´T
0
‚à´
Œ©
I(t:W(t,œâ)‚ààA)dP‚àû

dt
=œµ‚àí1
‚à´T
0
P‚àû(œâ : W(t, œâ) ‚ààA)dt = 0.
Since œµ > 0 is arbitrary, we obtain (9.29).
‚äì‚äî
Let us deÔ¨Åne now the following process
X(t, œâ) =
‚à´t
0
b(X‚àû(s, œâ))ds + W(t, œâ)
and consider the diÔ¨Äerence
Xn(t, œâ) ‚àíX(t, œâ) =
‚à´t
0
bn(Xn(s, œâ))ds ‚àí
‚à´t
0
b(X‚àû(s, œâ))ds.
Applying Lemma 9.3 with A = {x1, . . ., xN } we Ô¨Ånd for almost all t ‚àà[0,T] w.r.to
the Lebesgue measure and almost all œâ ‚ààŒ© that
lim
n‚Üí‚àûbn(Xn(t, œâ)) = b(X‚àû(t, œâ)).
Further, we derive from here by using the Lebesgue dominated convergence theorem
that
‚à´t
0
bn(Xn(s, œâ))ds ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àû
‚à´t
0
b(X‚àû(s, œâ))ds,
and, hence, for t ‚àà[0,T] and P-a.s.
X(t, œâ) = X‚àû(t, œâ),
which completes the proof of Theorem 9.3.
9.2
DiÔ¨Äusion processes and their connection with SDEs and
PDEs
A general conception of a Markov process (Xt)t ‚â•0 on given stochastic basis
(Œ©, F, (Ft)t ‚â•0, P) can be described as follows. The characteristic (Markov) property

118
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
of such adapted process means that
P(œâ : X(t, œâ) ‚ààB|Fs) = P(œâ : X(t, œâ) ‚ààB|X(s, œâ))
(9.30)
for every B ‚ààB(R) and all s < t.
It is convenient to treat (9.30) with the help of a transition probability function
P(s, x, t, B) satisfying conditions:
(1) It is a measurable function of x ‚ààR,
(2) It is a probability measure on (R, B(R)) such that P(s, x, s, B) = IB(x),
(3)
P(s, x, u, B) =
‚à´
P(s, x, t, dy)P(t, y, u, B).
(9.31)
for x ‚ààR, B ‚ààB(R), 0 ‚â§s < t < u.
The equation (9.31) is called the Kolmogorov-Chapman equation, and the process
(Xt)t ‚â•0 is a Markov random function (in a restricted sense). Substituting x by X(s, œâ)
in (9.30) and using properties of conditional expectations we get
P(s, X(s, œâ), u, B) = E(P(t, X(t, œâ), u, B)|Fs) =
‚à´
P(t, y, u, B)P(s, X(s, œâ), t, dy)
which coincides with (9.31) under x = X(s, œâ)
We know that many properties of stochastic processes can be described using
their
Ô¨Ånite-dimensional distributions. Let us derive the formula of a Ô¨Ånite-
dimensional distribution of (X(t)) in terms of its transition probability function.
Putting Ik = IBk (X(tk, œâ)), 0 ‚â§t1 < t2 < . . . < tk < . . . tn, Bk ‚ààB(R) we obtain by
backward integration:
P(œâ : X(t1, œâ) ‚ààB1, . . ., X(tn, œâ) ‚ààBn) =EI1 . . . In‚àí1E(In |Ftn‚àí1)
=EI1 . . . In‚àí1
‚à´
Bn
P(tn‚àí1, X(tn‚àí1, œâ), tn, dyn)
=EI1 . . . In‚àí1
‚à´
Bn‚àí1
P(tn‚àí2, X(tn‚àí2, œâ), tn‚àí1, dyn‚àí1)
√ó
‚à´
Bn
P(tn‚àí1, X(tn‚àí1, œâ), tn, dyn)
=EI1
‚à´
B2
P(t1, X(t1, œâ), t2, dy2) √ó . . .
√ó
‚à´
Bn
P(tn‚àí1, X(tn‚àí1, œâ), tn, dyn).
(9.32)
In the expression above we can treat
‚à´
B2
P(t1, X(t1, œâ), t2, dy2) . . .
‚à´
Bn
P(tn‚àí1, X(tn‚àí1, œâ), tn, dyn)
(9.33)

9.2
DiÔ¨Äusion processes and their connection with SDEs and PDEs
119
as a conditional expectation
P(œâ : X(t2, œâ)) ‚ààB2, . . ., X(tn, œâ) ‚ààBn|X(t1, œâ) = x1).
The expressions (9.32)-(9.33) present a consistent system of Ô¨Ånite-dimensional dis-
tributions of the process X(t, œâ) for Ô¨Åxed t1 and x1. So, to provide a full description
of this system it is reasonable to have a family of probability spaces (Œ©, F, Pt,x).
So, in case (9.32)-(9.33) we arrive to the probabilities Pt,x(œâ : X(t2, œâ) ‚ààB2, . . .,
X(tn, œâ) ‚ààBn) expressed with the help of P(s, x, t, B). We also can see that the
process X(t, œâ) can be started at each t ‚ààR+ from each point x ‚ààR. The process
(X(t, œâ)) in such a framework is called a Markov family, but we will call it also a
Markov process. All these arguments speak us that in the theory of Markov processes
it is necessary to describe the class of transition probability functions.
Before further steps in developments of this theory we give some examples and
problems.
First of all, (Wt)t ‚â•0 presents an example of a Markov process, because its Ô¨Ånite-
dimensional distributions were given in its deÔ¨Ånition via transition probability func-
tion.
Problem 9.2 Prove that (|Wt|)t ‚â•0 is a Markov process.
Hint. Its transition function:
P(s, x, t, B) =
1

2œÄ(t ‚àís)
‚à´
B

e‚àí(y‚àíx)2
2(t‚àís) + e
(y+x)2
2(t‚àís)

dy
Example 9.4 Let (Yn)n=1,2,... be a sequence of independent random variables with
a positive density p(x). DeÔ¨Åne X0 = 0, X1 = Y+
1 , X2 = (Y1 + Y2)+, . . . The process
(Xn)n=0,1,... is non-Markov. To show it we take x > 0 and consider the next conditional
probability
P(œâ : X3 > 0|X2 = 0, X1 = x) =
‚à´0
‚àí‚àû
‚à´‚àû
0
p(y ‚àíx)p(z ‚àíy)dydz.
This probability depends on x, and therefore, the process (Xn) can not be Markov.
Problem 9.3 Let (Yn)n=1,2,... be a sequence of independent r.v.‚Äôs with the density
p(x) > 0, x ‚ààR. DeÔ¨Åne the process (Y0 = 0)
Xt = (Y1 + . . . + Yk)(t ‚àík) + (Y1 + . . . + Yk+1)(k + 1 ‚àít)
for k ‚â§t ‚â§k + 1, t ‚â•0. Prove that such a process is not Markov.
Remark 9.4 The transition probability function P(s, x, t, B) is homogeneous, if
P(s + h, x, t + h, B) = P(s, x, t, B) for any positive h. Hence, P(¬∑, ¬∑, ¬∑, ¬∑) depends on
the diÔ¨Äerence (t ‚àís) only and one can use another transition probability function
P(t, x, B) for which the Kolmogorov-Chapman equation has the form

120
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
P(t + s, x, B) =
‚à´
R
P(t, x, dy)P(s, y, B).
DeÔ¨Ånition 9.1 Markov process X(t, œâ) with its transition probability function
P(s, x, t, B) satisfying the properties (1)-(3) is called a diÔ¨Äusion process, if there
exist measurable functions b(t, x) and œÉ2(t, x), (t, ) ‚ààR+ √ó R such that for œµ > 0 and
Vœµ = {y ‚ààR : |y ‚àíx| > œµ}
lim
h‚Üí0
1
h P(t, x, t + h,Vœµ) = 0,
(9.34)
lim
h‚Üí0
1
h
‚à´
|y‚àíx |‚â§œµ
(y ‚àíx)P(t, x, t + h, dy) = b(t, x),
(9.35)
lim
h‚Üí0
1
h
‚à´
|y‚àíx |‚â§œµ
(y ‚àíx)2P(t, x, t + h, dy) = œÉ2(t, x).
(9.36)
Functions b(t, x) and œÉ2(t, x) are called drift and diÔ¨Äusion coeÔ¨Écients.
It turns out relations (9.34)-(9.36) admit an adequate charazterization with the
help of the following diÔ¨Äerential operator (generator of X(t, œâ)):
LtœÜ(x) = a(t, x)‚àÇœÜ(x)
‚àÇx
+ 1
2œÉ(t, x)‚àÇ2œÜ(x)
‚àÇx2
.
(9.37)
in class of twice continuously diÔ¨Äerentiable functions œÜ.
Lemma 9.4 For all bounded functions œÜ described above
‚à´
R
|œÜ(y) ‚àíœÜ(x)|P(t, x, t + h, dy) = hLtœÜ(x) + o(h)
(9.38)
is fulÔ¨Ålled ‚áê‚áí(9.34)-(9.36).
Proof To prove (9.38), we decompose function œÜ at point x : for |y ‚àíx| < œµ
‚à´
R
|œÜ(y) ‚àíœÜ(x)|P(t, x, t + h, dy) = Œ±œµ(h) + hLtœÜ(x) + hŒ≥œµ,
where Lt is deÔ¨Åned by (9.37), Œ±œµ(h)h‚àí1 ‚Üí0, œµ > 0, and Œ≥œµ ‚Üí0 as œµ ‚Üí0.
Hence, we get (9.38). To prove the direct implication we take the function œÜx0(x) =
1 ‚àíexp ‚àí|x ‚àíx0|4 and Ô¨Ånd that
‚àÇ
‚àÇx œÜx0(x)|x=x0 = 0 and
‚àÇ2
‚àÇx2 œÜx0(x)|x=x0 = 0.
It implies (9.34).
Now we take function œÜx0(y) = y ‚àíx0 for |y ‚àíx0| < 1 extending this function to
be a bounded twice continuously diÔ¨Äerentiable function. For such a function and
0 < œµ < 1 we get
‚à´
|y‚àíx0 |‚â§œµ
(y ‚àíx0)P(t, x0, t + h, dy) = a(t, x)h + o(h)
(9.39)

9.2
DiÔ¨Äusion processes and their connection with SDEs and PDEs
121
and hence (9.35). To prove (9.36) we take a square of function in the previous
construction (9.39).
‚äì‚äî
Let us Ô¨Ånd diÔ¨Äerential equations for transition probabilities of diÔ¨Äusion processes,
known as the Kolmogorov backward and forward equations.
For some bounded continuous function œÜ(x) and T > 0 we deÔ¨Åne the following
function
uT(t, x) =
‚à´
R
œÜ(y)P(t, x,T, dy).
Theorem 9.4 Assume that (9.34)-(9.36) are uniformly satisÔ¨Åed on bounded time
intervals with continuous functions b(t, x) and œÉ2(t, x). Let function uT(t, x) be (t, x)-
continuous together with derivatives ‚àÇuT
‚àÇx and ‚àÇ2uT
‚àÇx2 . Then uT(t, x) has the derivative
w.r.to t and satisÔ¨Åes the equation
‚àÇuT(t, x)
‚àÇt
+ LtuT(t, x) = 0,
(9.40)
with the boundary condition uT(T, x) = œÜ(x).
Proof The boundary condition is fulÔ¨Ålled due to (9.34). Further, for h > 0 we have
by Lemma 8.4 that
uT(t ‚àíh, x) =
‚à´
R
uT(t, y)P(t ‚àíh, x, t, dy)
=uT(t, x) +
‚à´
R
[uT(t, y) ‚àíuT(t, x)] P(t ‚àíh, x, t, dy)
=hLt‚àíhuT(t, x) + uT(t, x) + o(h),
whence
1
h [uT(t ‚àíh, x) ‚àíuT(t, x)] = Lt‚àíhuT(t, x) + o(h)
h .
This implies (9.40).
‚äì‚äî
In fact, the equation (9.40) can be called as the Kolmogorov backward equation. But
usually it is formulated as the equation for the density p(s, z, t, y) of the transition
probability function P(s, x, t, B) :
p(s, x, t, y)dy = P(s, x, t, dy).
So, we arrive to the backward Kolmogorov equation for p(s, x, t, y) :
‚àÇp(s, x, t, y)
‚àÇs
+ b(s, x, )‚àÇp(s, x, t, y)
‚àÇx
+ œÉ2(s, x)
2
‚àÇ2p(s, x, t, y)
‚àÇx2
= 0.
(9.41)
On the other hand, one can write also the forward Kolmogorov equation or the
Fokker-Planck equation:
‚àÇp(s, x, t, y)
‚àÇt
+ ‚àÇ
‚àÇy (b(t, y)p(s, x, t, y)) ‚àíœÉ2
2‚àÇy2

œÉ2(t, y)p(s, x, t, y)

= 0.
(9.42)

122
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
So, the same function p(s, x, t, y) plays the role a fundamental solution for two
diÔ¨Äerential equations:
(9.41) as the function of (s, x) and
(9.42) as the function of (t, y).
Let us come back to the stochastic diÔ¨Äerential equation (9.1) and its solution X(t)
derived under the Lipschitz conditions in Theorem 9.1. Applying the same reasoning
as in Theorem 9.1 we can prove that there exists a unique solution Xs,x(t) of the
equation (t ‚â•s) :
Xs,x(t) = x +
‚à´t
s
b(u, Xs,x(u))du +
‚à´t
s
œÉ(u, Xs,x(u))dWu.
(9.43)
Putting for B ‚ààB(R)
P(s, x, t, B) = P(Xs,x(t) ‚ààB),
(9.44)
we arrive to conclusion that the solution X(t) of (9.1) is a Markov process with the
transition probability function (9.44).
To prove this claim we note that Xs,X(s)(t) is a solution of (9.43). We just note
from (9.1) and Remark 9.1 that
X(t, œâ) ‚àíX(s, œâ) =
‚à´t
s
b(u, X(u, œâ)du +
‚à´t
s
œÉ(u, X(u, œâ)dWu.
Thus, X(t, œâ) = Xs,X(s,œâ)(t). Further, the œÉ-algebra Fs is generated by Wu, u ‚â§s.
Let us take arbitrary bounded Fs-measurable r.v. Œ∏. Then for any bounded function
g(x) we obtain that
EŒ∏g(X(t, œâ)) =EŒ∏g Xs,X(s,œâ)(t)
=EŒ∏E g Xs,x(t)
x=X(s,œâ)
=EŒ∏
‚à´
R
g(y)P(s, x, t, dy)

x=X(s,œâ)
=EŒ∏
‚à´
R
g(y)P(s, X(s, œâ), t, dy),
where we used the independence of Xs,x(t) and Fs because Xs,x(t) depends on
W(u) ‚àíW(s) for u ‚â•s only. Hence,
E(g(X(t, œâ))|Fs) =
‚à´
R
g(y)P(s, X(s, œâ), t, dy),
and we get the claim.
Remark 9.5 As a methodological result one can conclude that diÔ¨Äusion processes
are described in two ways: as a class of Markov processes with transition probabilities
determined by the drift and diÔ¨Äusion coeÔ¨Écients and as solutions of SDEs. Let us
discuss other interesting connections. We consider a stochastic diÔ¨Äerential equation

9.2
DiÔ¨Äusion processes and their connection with SDEs and PDEs
123
with coeÔ¨Écients b = b(t, x) and œÉ = œÉ(t, x) :
dXt = b(t, Xt)dt + œÉ(t, Xt)dWt.
As before, we deÔ¨Åne the following diÔ¨Äerential operator based on functions b and œÉ :
LœÜ = 1
2œÉ2 ‚àÇ2œÜ
‚àÇx2 + b‚àÇœÜ
‚àÇx .
Having a smooth function F ‚ààC1,2 we can transform (Xt) with the help of the Ito
formula as follows
F(Xt) = F(X0) +
‚à´t
0
 ‚àÇF
‚àÇs + LF(Xs)

ds +
‚à´t
0
œÉ(s, Xs)‚àÇF
‚àÇx (Xs)dWs.
(9.45)
Equality (9.45) is a smooth transformation of the diÔ¨Äusion process (Xt). Such a
problem was Ô¨Årst formulated and solved by Kolmogorov in 1931: a new process
Yt = F(Xt) will be again a diÔ¨Äusion process with the drift bY = ‚àÇF
‚àÇt + LF(Xt) and
the diÔ¨Äusion coeÔ¨Écient œÉY = œÉ ‚àÇF
‚àÇx (Xt). Putting together both these formulas we
arrive to the Ito formula in the form (9.45). That‚Äôs why the formula (9.45) can be
also called the Kolmogorov-Ito formula.
Remark 9.6 We can also put the Cauchy problem for the parabolic diÔ¨Äerential
equation: Ô¨Ånd a smooth function v = v(t, x) : [0,T] √ó R ‚ÜíR such that
‚àÇv
‚àÇt + Lv = 0, (t, x) ‚àà[0,T] √ó R
with v(T, x) = f (x), x ‚ààR.
This boundary value problem is solved in the theory of partial diÔ¨Äerential equa-
tions (PDEs) under wide conditions. It turns out one can write a probabilistic form
of this solution using the theory of diÔ¨Äusion processes. We take a diÔ¨Äusion process
(Xt) with a generator L and
v(t, x) = Et,x f (XT), Xt = x.
(9.46)
To get formula (9.46), called the Feynman-Kac representation, we apply the Ito
formula to v(t, Xt) and obtain that
dv(t, Xt) =
 ‚àÇv
‚àÇt + Lv

dt + œÉ ‚àÇv
‚àÇx dWt.
It is clear that v(t, Xt) is a martingale, and therefore
v(t, Xt) = E( f (XT)|Ft) = Et,x f (XT)



x=Xt .
Let us investigate absolute continuity of distributions of diÔ¨Äusion processes. We
shall do it in the form of solutions of stochastic diÔ¨Äerential equations.

124
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Assume (X(t))t ‚â§T is a continuous random process on a probabilistic space
(Œ©, F, P). Denote C[0,T] the space of continuous functions on [0,T] and B[0,t]
is a œÉ-algebra in this space, generated by cylinders. We deÔ¨Åne a distribution of
(X(t))t ‚â§T as the measure on (C[0,T], B[0,T])
ŒºX(B) = P{œâ : X(¬∑, œâ) ‚ààB}.
It means that the measure ŒºX is just an image of P under mapping
X(¬∑, œâ) : Œ© ‚ÜíC[0,T].
Assume, there is a probability measure ÀúP << P, i.e. for A ‚ààF
ÀúP(A) =
‚à´
A
Z(œâ)dP,
where Z is a measurable non-negative random variable such that
‚à´
Œ©
Z(œâ)dP = 1.
We can consider the process (Xt)t ‚â§T under a new measure ÀúP, and denote this process
( ÀúXt)t ‚â§T . Then
dŒº ÀúX
dŒºX
(X(¬∑, œâ)) = E(Z(œâ)|F X),
(9.47)
where F X is a œÉ-algebra, generated by the process (Xt)t ‚â§T . The equality (9.47) is
almost obvious because for A ‚ààB[0,T] we have
Œº ÀúX(A) = ÀúP(œâ : X(¬∑, œâ) ‚ààA)
=
‚à´
A
Z(œâ)dP
=EZ(œâ)IA(X(¬∑, œâ))
=EIA(X(¬∑, œâ))E(Z(œâ)|F X)
=
‚à´
A
E(Z(œâ)|F X)ŒºX(dx).
Assume that two diÔ¨Äusion processes (Xi(t))t ‚â§T, i = 1, 2, satisfy the following stochas-
tic diÔ¨Äerential equations
dXt(t) =bi(t, Xi(t))dt + œÉ(t, Xi(t))dWt,
Xi(0) =x ‚ààR.
(9.48)
Theorem 9.5 Assume the coeÔ¨Écients of equations (9.48) are bounded and satisfy
the Lipschitz conditions, and there exists a bounded continuous function Œ∏ = Œ∏(t, x)
such that œÉ(t, x) > 0 and

9.2
DiÔ¨Äusion processes and their connection with SDEs and PDEs
125
b2(t, x) ‚àíb1(t, x) = Œ∏(t, x)œÉ(t, x),
(t, x) ‚àà[0,T] √ó R. Then ŒºX2 << ŒºX1 and
dŒºX2
dŒºX1
(X1(¬∑, œâ)) = exp
‚à´T
0
Œ∏(s, X1(s))dWs ‚àí1
2
‚à´T
0
Œ∏2(s, X1(s))ds

.
(9.49)
Proof Obviously, we can apply here the Girsanov theorem with the measure ÀúP and
Z(œâ) = d ÀúP
dP = exp
‚à´T
0
Œ∏(s, X1)dWs ‚àí1
2
‚à´T
0
Œ∏2(s, X1)ds

,
and due to boundedness of coeÔ¨Écients EZ(œâ) = 1.
Hence, ÀúP(Œ©) = 1 and the process
ÀúW(t) = W(t) ‚àí
‚à´t
0
Œ∏(s, X1(s))ds
is a Wiener process w.r. to ÀúP.
Let us rewrite the process (X1(t))t ‚â§T as follows, using (9.5):
X1(t) ‚àíx =
‚à´t
0
b1(s, X1)ds +
‚à´t
0
œÉ(s, X1)dWs
=
‚à´t
0
b1(s, X1)ds +
‚à´t
0
œÉ(s, X1)

d ÀúWs + Œ∏(s, X1)ds

=
‚à´t
0
[b1(s, X1) + Œ∏(s, X1)œÉ(s, X1)] ds +
‚à´t
0
œÉ(s, X1)d ÀúWs
=
‚à´t
0
b2(s, X1)ds +
‚à´t
0
œÉ(s, X1)d ÀúWs.
.
(9.50)
Representation (9.50) certiÔ¨Åes that (X1(t))t ‚â§T coincides with the solution of the
equation
d ÀúX2(t) = b2(t, ÀúX2(t))dt + œÉ(t, ÀúX2(t))d ÀúWt
on the space (Œ©, F, ÀúP). Hence, distributions Œº ÀúX2 and ŒºX2 are the same, and applying
(9.47) we obtain
dŒºX2
dŒºX1
(X1(¬∑, œâ)) = E

Z(œâ)|F X1

.
(9.51)
Let us prove that Z(œâ) is F X1-measurable and therefore the formula (9.49) is fulÔ¨Ålled.
To establish it we represent Œ∏(t, x) as follows
Œ∏(t, x) = b2(t, x) ‚àíb1(t, x)
œÉ(t, x)

126
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
due to assumption œÉ(t, x) > 0 for (t, x) ‚àà[0,T] √ó R. Further, for a subdivision
0 = s0 < s1 < . . . < sn = t we have (in the sense of convergence in probability) that
Wt = lim
n‚Üí‚àû
n‚àí1

i=0
œÉ‚àí1(si, X1(si))

X1(si+1) ‚àíX1(si) ‚àí
‚à´si+1
si
b1(s, X1(s))ds

.
(9.52)
The relation (9.52) follows from observation that
n‚àí1

i=0
œÉ‚àí1(si, X1(si))

X1(si+1) ‚àíX1(si) ‚àí
‚à´si+1
si
b1(s, X1(s))ds

=
n‚àí1

i=0
œÉ‚àí1(si, X1(si))œÉ(s, X1(s))dWs,
which due to continuity of coeÔ¨Écients tends to Wt in probability as maxi Œîsi ‚Üí0
by properties of stochastic integrals.
‚äì‚äî
Example 9.5 Consider processes Wt, Xt = 10 + Wt and Yt = 3Wt, t ‚àà[0, 1]. Let us
deÔ¨Åne a functional on C[0, 1], f (x(¬∑)) = x(0). For the process Xt this functional
takes value 10 with probability 1, but for other processes Wt and Yt this functional
takes value 0 with probability one. Hence, ŒºX is singular w.r.to ŒºW and ŒºY. We also
note that ŒºW and ŒºY are singular too. It follows from the fact that in probability
n
i=1(Wti ‚àíWti‚àí1)2 ‚Üí1 as the diameter of subdivision 0 = t0 < t1 < . . . < tn = 1
tends to 1. A similar limit for (Yt) will be equal to 9.
Remark 9.7 As it was noted in Remark 8.3 for Ito‚Äôs processes, a similar theory of
stochastic diÔ¨Äerential equations with a multidimensional Wiener process, a vector-
valued coeÔ¨Écient band a matrix-valued coeÔ¨Écient œÉ can be developed. Respectively,
it is connected with diÔ¨Äusion processes with a vector-valued drift b, a matrix-valued
diÔ¨Äusion a = œÉœÉ‚àóand a generator Lu = 
i bi ‚àÇu
‚àÇxi + 1
2

i,j aij
‚àÇ2u
‚àÇxi‚àÇx j .
9.3
Applications to Mathematical Finance and Statistics of
Random Processes
In Ô¨Ånancial context, a Wiener process was mathematically introduced and developed
by L. Bachelier. His model of price evolution of stocks has the following simple
form:
St = S0 + Œºt + œÉWt, t ‚â§T,
(9.53)
where Œº ‚ààR, œÉ > 0, and (Wt) is a Wiener process on given stochastic basis
(Œ©, F, (F W
t
), P).
Besides price dynamics of a risky asset (9.53) we assume for simplicity that
evolution of non-risky asset (bank account) is trivial, i.e. Bt = 1.

9.3
Applications to Mathematical Finance and Statistics of Random Processes
127
One of the main subject of Mathematical Finance is option pricing. We consider
only standard and the most exploited contracts which are called Call and Put options.
These derivative securities (or simply, derivatives) give the right to the holder to buy
(Call option) and to sell (Put option) a stock at maturity time T. To get such a
derivative it is necessary to buy it by some price at time t ‚â§T. Denote such prices
for call and put options for t = 0 by CT and PT, correspondently. It is convenient
to identify these options with their pay-oÔ¨Äfunctions at maturity time as (ST ‚àíK)+
and (K ‚àíST)+. The problem is to Ô¨Ånd so-called fair price CT(PT) in the beginning
(t = 0) of the contract period. According to the theory of option pricing such a price
CT (PT) is calculated as E‚àó(ST ‚àíK)+ (correspondingly, E‚àó(K ‚àíST)+), where E‚àóis
the expectation with respect to a measure P‚àósuch that the process (St)t ‚â§T is a P‚àó-
martingale. In case of the model (9.53) such martingale measure P‚àóis determined
by the Girsanov exponent
Z‚àó
T = exp

‚àíŒº
œÉWT ‚àí1
2
 Œº
œÉ
2
T

,
and according to the Girsanov theorem, the process
W‚àó
t = Wt + Œº
œÉ t
is a Wiener process w.r. to P‚àó. Hence, for distributions LawP‚àóand LawP w.r. to
measures P‚àóand P relatively we have equality
LawP‚àó(S0 + Œºt + œÉWt; t ‚â§T) = LawP(S0 + œÉWt; t ‚â§T).
(9.54)
Theorem 9.6 In the framework of the Bachelier model (9.53) the initial price of a
call option is determined by the formula
CT = (S0 ‚àíK)Œ¶
 S0 ‚àíK
œÉ
‚àö
T

+ œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

,
(9.55)
where Œ¶(x) =
‚à´x
‚àûœÜ(y)dy, œÜ(x) =
1
‚àö
2œÄ e‚àíx2/2.
In particular, for S0 = K we have CT = œÉ

T
2œÄ .
To prove (9.55) we use (9.54) and self-similarity property of (Wt) and rewrite CT
as follows
CT =E‚àó(ST ‚àíK)+ = E‚àó(S0 + ŒºT + œÉWT ‚àíK)+ =
=E‚àó(S0 + œÉWT ‚àíK)+ = E‚àó(S0 + œÉ
‚àö
TW1 ‚àíK)+
=E‚àó(S0 ‚àíK + œÉ
‚àö
TŒæ)+
(9.56)
where W1 = Œæ ‚àºN(0, 1).

128
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Denote a = S0 ‚àíK and b = œÉ
‚àö
T and obtain form (9.56) that
CT =E(a + bŒæ)+ =
‚à´‚àû
‚àía/b
(a + bx)œÜ(x)dx
=aŒ¶(a/b) + b
‚à´‚àû
‚àía/b
xœÜ(x)dx
=aŒ¶(a/b) ‚àíb
‚à´‚àû
‚àía/b
dœÜ(x)
=aŒ¶(a/b) + bœÜ(a/b)
=(S0 ‚àíK)Œ¶
 S0 ‚àíK
œÉ
‚àö
T

+ œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

.
Moreover, using the Markov property of (St) we can Ô¨Ånd the price of call option
C(t, St) at any time t ‚â§T taking conditional expected value of (ST ‚àíK)+ with respect
to Ft :
C(t, St) =E‚àó(ST ‚àíK)+|Ft

=E‚àó(ST ‚àíK)+|St

=Et,x(ST ‚àíK)+


St=x
=E(at + btŒæ)+
=atŒ¶(at/bt) + btœÜ(at/bt),
where at = St ‚àíK, b = œÉ
‚àö
T ‚àít.
Therefore,
C(t, St) = (St ‚àíK)Œ¶
 St ‚àíK
œÉ
‚àö
T ‚àít

+ œÉ
‚àö
T ‚àítœÜ
 St ‚àíK
œÉ
‚àö
T ‚àít

.
(9.57)
Applying the Ito formula in (9.57) and using a martingale property of E‚àó((ST ‚àí
K)+|Ft) we arrive to
dC(t, St) = ‚àÇC
‚àÇSt
dSt +
 ‚àÇC
‚àÇt + 1
2œÉ2 ‚àÇ2C
‚àÇS2
t

dt
and a PDE
‚àÇC
‚àÇt (t, x) + 1
2œÉ2 ‚àÇ2C(t, x)
‚àÇx2
= 0.
(9.58)
with the boundary condition
C(T, x) = (x ‚àíK)+.
The equation (9.58) gives the opportunity to apply methods of PDEs in pricing of
option.

9.3
Applications to Mathematical Finance and Statistics of Random Processes
129
Remark 9.8 As far as the put price PT it follows from the put-call parity: for r = 0
CT = PT + S0 ‚àíK.
(9.59)
The parity (9.59) follows from the next elementary equality
(x ‚àíK)+ = (K ‚àíx) + x ‚àíK, x, K ‚â•0.
(9.60)
Putting ST instead of x in (9.60), and taking expected value w.r.to P‚àówe arrive to
(9.59).
The Bachelier model (9.53) has at least one disadvantage that prices can take
negative values that contradicts their Ô¨Ånancial sense. To make a reasonable improve-
ment of the model P. Samuelson (1965) proposed to transform (9.53) with the help
of an exponential function. The resulting model
St = S0 exp

(Œº ‚àíœÉ2/2)t + œÉWt

(9.61)
became the name of Geometric Brownian Motion (GBM).
Applying the Ito formula to (9.61) we get
dSt = St(Œºdt + œÉdWt), S0 > 0.
(9.62)
The model of Ô¨Ånancial market in the form (9.62) is called the Black-Scholes
model. As in case of the Bachelier model, we assume here Bt = 1 for simplicity.
As in the case of the Bachelier model, we use the Girsanov theorem with the same
Girsanov exponent Z‚àó
T, the martingale measure P‚àóand a Wiener process W‚àó
t w.r. to
P‚àówe can conclude that
LawP‚àó(œÉWt, t ‚â§T) = LawP‚àó(œÉW‚àó
t , t ‚â§T) = LawP(œÉWt, t ‚â§T)
and, hence,
LawP‚àó(St; t ‚â§T) = LawP

S0e‚àíœÉ2/2t+œÉWt,t ‚â§T 
.
It gives a possibility to calculate the price CT of a Call option (ST ‚àíK)+ in the model
(9.62) taking expected value w.r. to P‚àó:
CT =E‚àó(ST ‚àíK)+ = E(S0e‚àíœÉ2/2T+œÉWT ‚àíK)+
=E(S0e‚àíœÉ2/2T+œÉ
‚àö
TW1 ‚àíK)+
=E(aebŒæ‚àíb2/2 ‚àíK)+
=aŒ¶

ln( a
K ) + 1
2b2
b
	
‚àíKŒ¶

ln( a
K ) ‚àí1
2b2
b
	
,
(9.63)
where Œæ ‚àºN(0, 1), a = S0, b = œÉ
‚àö
T.
The formula (9.63) is the famous formula of Black and Scholes for call option. A
similar formula for put option is derived with the help of put-call parity.

130
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Example 9.6 To recognize how close prices of call options in the model of Bachelier
and the model of Black and Scholes we consider the simplest equations for them:
dSB
t =S0œÉdWt,
dSBS
t
=SBS
t
œÉdWT, t ‚â§T, œÉ > 0.
We put S0 = K, and in this case we compare prices CB
T and CBS
T , for which
0 ‚â§CB
T ‚àíCBS
T
‚â§
S0
12
‚àö
2œÄ
œÉ3T3/2 = O((œÉ
‚àö
T)3).
(9.64)
To prove (9.64) we note the following inequalities: ey ‚â•1 + y for all y, and, hence,
y2/2 ‚â•1 ‚àíe‚àíy2/2 for all y. Using these inequalities we obtain that
0 ‚â§CB
T ‚àíCBS
T
=
 S0
‚àö
2œÄ
x ‚àíS0

Œ¶
 x
2

‚àíŒ¶

‚àíx
2

x=œÉ
‚àö
T
= S0
‚àö
2œÄ
‚à´x/2
‚àíx/2

1 ‚àíe‚àíy2/2
dy

x=œÉ
‚àö
T
‚â§S0
‚àö
2œÄ
‚à´x/2
‚àíx/2
y2/2dy



x=œÉ
‚àö
T
= S0
‚àö
2œÄ
x3/12



x=œÉ
‚àö
T
=
S0
12
‚àö
2œÄ
œÉ3T3/2 = O((œÉ
‚àö
T)3).
Assuming œÉ
‚àö
T << 1 in (9.64) we can observe that call prices in both models
are pretty close to each others. In particular, for T = 1/2 and œÉ = 2.4% we obtain
(œÉ
‚àö
T)3 ‚âÉ5 ¬∑ 10‚àí7 and, therefore,
S0
12
‚àö
2œÄ ‚àí5 ¬∑ 10‚àí7 ‚âÉ1.6 ¬∑ 10‚àí8S0.
Let us show how methods of stochastic processes work in solving of statistical
problems.
Assume that the observation process has the following structure
Xt =
‚à´t
0
fsdsŒ∏ + Wt,
(9.65)
where ( ft)t ‚â•0 is a progressively measurable process for which P(œâ :
‚à´t
0 f 2
s ds < ‚àû) =
1, t ‚â•0, Œ∏ ‚ààR is an unknown parameter to be estimated based on observations of
the process (Xt)t ‚â•0, and (Wt)t ‚â•0 is a Wiener process.
The model (9.65) is a continuous time version of of a regression model in discrete
time. So, it is quite natural to use the least squares estimates for estimation of Œ∏. In
the case of model (9.65) such estimate has the following structure

9.3
Applications to Mathematical Finance and Statistics of Random Processes
131
Œ∏t =
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
fsdXs.
We need to investigate some properties of estimates (9.3). We demonstrate it in
the form of the following example.
Example 9.7 Assume that (a.s)
0 < c2 ‚â§f 2
t (œâ) ‚â§C2 < ‚àû,
(9.66)
for every t > 0. We can provide a ‚Äústochastic representation‚Äù of Œ∏t using (9.65) as
follows
Œ∏t =
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
fsdXS
=
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
f 2
s dsŒ∏ +
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
fsdWs,
and Ô¨Ånd that
Œ∏t ‚àíŒ∏ =
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
fsdWs.
(9.67)
Using representation (9.67), condition (9.66) and the Chebyshev inequality, we obtain
for œµ > 0 that
P(œâ : |Œ∏t ‚àíŒ∏| ‚â•œµ) ‚â§œµ‚àí2E
‚à´t
0
f 2
s ds
‚àí1 ‚à´t
0
fsdWs
	2
‚â§œµ‚àí2c‚àí4t‚àí2E
‚à´t
0
fsdWs
2
=œµ‚àí2c‚àí4t‚àí2E
‚à´t
0
f 2
s ds
‚â§œµ‚àí2c‚àí4t‚àí2C2t
=œµ‚àí2 C2
c4 t‚àí1 ‚Üí0,
t ‚Üí‚àû.
It means that Œ∏t is a consistent estimate of parameter Œ∏.
Example 9.8 The least square estimate (9.3) is consistent, as was shown in the
previous Example, but it does not speak us about the accuracy of these estimates.
One can modify them with the help of a specially chosen stopping times to get an
estimate with Ô¨Åxed accuracy. To do this we Ô¨Åx a positive number H and deÔ¨Åne
œÑH = inf

t :
‚à´t
0
f 2
s ds ‚â•H

.
Assume
‚à´t
0 f 2
s ds ‚Üí‚àû(a.s.) as t ‚Üí‚àû, to prove that

132
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
P(œâ : œÑH(œâ) < ‚àû) = 1.
Further, deÔ¨Åne a sequential least squares estimate
ÀÜŒ∏H = H‚àí1
‚à´œÑH
0
fsdXs.
(9.68)
It follows from (9.68) that
E ÀÜŒ∏H =H‚àí1E
‚à´œÑH
0
fsdXs
=H‚àí1E
‚à´œÑH
0
f 2
s dsŒ∏ + H‚àí1E
‚à´œÑH
0
fsdWs
=H‚àí1HŒ∏ + H ¬∑ 0 = Œ∏.
So, the estimate ÀÜŒ∏H is unbiased.
To estimate the accuracy of ÀÜŒ∏H we use its variance:
Var ÀÜŒ∏H =E( ÀÜŒ∏H ‚àíŒ∏)2
=E

H‚àí1
‚à´œÑH
0
fsdXs ‚àíŒ∏
2
=H‚àí2E
‚à´œÑH
0
fsdWs
2
=H‚àí2E
‚à´œÑH
0
f 2
s ds
=H‚àí2 ¬∑ H = H‚àí1.
(9.69)
The relation (9.69) shows that accuracy of the estimate ÀÜŒ∏H can be controlled with
the help of the level H.
9.4
Controlled diÔ¨Äusion processes and applications to
option pricing
Suppose there is a family of diÔ¨Äusion processes Xt = XŒ±
t
satisfying a stochastic
diÔ¨Äerential equation
dXt := dXŒ±
t = bŒ±(t, XŒ±
t )dt + œÉŒ±(t, XŒ±
t )dWt, X0 = x.
(9.70)
Here bŒ± and œÉŒ± are functions satisfying some reasonable conditions for existence
and uniqueness of solutions of (9.70). Parameter Œ± is a control process adapted to

9.4
Controlled diÔ¨Äusion processes and applications to option pricing
133
Ô¨Åltration (Ft)t ‚â•0. The equation (9.70) is also called a stochastic control system. We
will call the process (Xt) a controlled diÔ¨Äusion process.
For estimating the quality of control Œ± we introduce a function f Œ±(t, x), (t, x) ‚àà
R+ √ó R. The process Œ± takes values in a domain D ‚äÜR. Function f Œ± can be inter-
preted as the density of the value Ô¨Çow. Then the total value on the interval [0, t] will
be identiÔ¨Åed with
‚à´t
0 f Œ±(XŒ±
s )ds which is assumed to be well deÔ¨Åned.
Let us put the problem and provide some heuristic explanations of its solution for
a time homogeneous stochastic control system (9.70). Denoting
vŒ±(x) = E0,x
‚à´‚àû
0
f Œ±(XŒ±
s )ds

= Ex
‚à´‚àû
0
f Œ±(XŒ±
s )ds

,
we deÔ¨Åne the optimal control Œ±‚àó, if
v(x) = sup
Œ±
vŒ±(x) = vŒ±‚àó(x), x ‚ààR.
(9.71)
In the theory of controlled diÔ¨Äusion processes the following Hamilton-Jacobi-
Bellman principle of optimality is exploited to determine v(x) :
v(x) = sup
Œ±
Ex
‚à´t
0
f Œ±(XŒ±
s )ds + v(XŒ±
t )

.
(9.72)
Let us explain a motivation for using (9.72). We rewrite the total value using strategy
(control) Œ± on [0, ‚àû) as follows
‚à´‚àû
0
f Œ±(XŒ±
s )ds =
‚à´t
0
f Œ±(XŒ±
s )ds +
‚à´‚àû
t
f Œ±(XŒ±
s )ds.
(9.73)
If this strategy was used only up to moment t, then the Ô¨Årst term in the right-hand
side of (9.73) represents the value on the interval [0, t]. Suppose the controlled
process XŒ±
t = Xt has the value y = Xt at time t. If we wish to continue the control
process after time t with the goal of maximization of the value over the whole time
interval [0, ‚àû), then we have to maximize Ey
‚à´t
0 f Œ±(XŒ±
s )ds

, where Œ± also denotes
the continuation of the control process to [t, ‚àû). Changing variable s = t + u, u ‚â•0,
and using independence and stationarity of increments of the Wiener process W, we
obtain
EXt
‚à´‚àû
0
f Œ±(XŒ±
s )ds = vŒ±(Xt) ‚â§v(Xt).
Thus, a strategy that is optimal after time t, gives the average value such that
Ex
‚à´t
0
f Œ±(XŒ±
s )ds ‚â•vŒ±(x).
(9.74)
One can choose Œ±s, s ‚â§t, so that the corresponding value is close enough to the
average value. Therefore, taking supremum of both sides (9.74), we arrive to the
HJB-principle of optimality (9.72), and we will call v(x) the value function.

134
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Moreover, one can rewrite (9.72) in a diÔ¨Äerential form if the value function is
smooth enough. Applying the Ito formula, we obtain
v(Xt) = v(x) +
‚à´t
0
 ‚àÇv
‚àÇx bŒ±(Xs) + 1
2œÉ2
Œ±
‚àÇ2v
‚àÇx2 (Xs)

ds +
‚à´t
0
œÉŒ±
‚àÇv
‚àÇx (Xs)dWs. (9.75)
Since the last term in the right-hand side of (9.75) is a martingale, then we get
from (9.72) that
v(x) = sup
Œ±
Ex
‚à´t
0
 ‚àÇv
‚àÇx bŒ±(Xs) + œÉ2
Œ±
2
‚àÇ2v
‚àÇx2 (Xs) + f Œ±(Xs)

ds + v(x)

,
and hence,
sup
Œ±
[LŒ±v(x) + f Œ±(x)] = 0,
(9.76)
where LŒ±v = bŒ± ‚àÇv
‚àÇx + 1
2œÉ2
Œ±
‚àÇ2v
‚àÇx2 .
The equation (9.76) is usually referred to as HJB-diÔ¨Äerential equation.
Keeping in mind an adequate application of stochastic control theory in option
pricing we would like reformulate it for the inhomogeneous case and for a Ô¨Ånite
interval time [0,T]. We consider the following stochastic control system
dXs =bŒ±(s, Xs)ds + œÉŒ±(s, Xs)dWs,
Xt =x, s ‚àà[t,T],
(9.77)
where Œ± is a D-valued adapted process. Then the optimal control problem is to
maximize (minimize) the value function
vŒ±(t, x) =Et,x
‚à´T
t
f Œ±(s, Xs)ds + g(XT)

,
v(t, x) = sup
Œ±
vŒ±(t, x),
(9.78)
where Xs = XŒ±
s satisÔ¨Åes (9.77) and function g determines the terminal value.
For system (9.77) with the value function (9.78) the corresponding HJB-equation
is derived in the form
‚àÇv(t, x)
‚àÇt
+ sup
Œ±
LŒ±v(t, x) = 0,
v(T, x) = g(x), x ‚ààR.
(9.79)
Let us show how this mathematical technique works for option pricing. We start with
the Bachelier model with stochastic volatility (with interest rate r = 0):
St = S0 + Œºt + œÉtWt, t ‚àà[0,T],
(9.80)

9.4
Controlled diÔ¨Äusion processes and applications to option pricing
135
where S0, œÉ2
t = œÉ2 + (‚àí1)Nt Œ¥œÉ2,
Œ¥œÉ2 < œÉ2,
(Nt)t ‚â§T is a Poisson process with
intensity Œª > 0.
It is well-known that pricing of option with pay-oÔ¨Äfunction g leads to the interval
of non-arbitrage prices of option with end points
C‚àó= inf
P‚àóE‚àóg(ST) and C‚àó= sup
P‚àóE‚àóg(ST),
(9.81)
where P‚àóruns a family of martingale measures for the model (9.80).
Number C‚àóand C‚àóare called (initial) lower and upper price of option. For
arbitrary time t before maturity date T such prices are determined as follows
v(t, St) = sup
P‚àóE‚àó(g(ST)|Ft) and
u(t, St) = inf
P‚àóE‚àó(g(ST)|Ft).
(9.82)
Due to a Markov property of the process (Xt) from (9.80) and a parametrization
of martingale measures P‚àó= P‚àó(Œ±), Œ±2
t = œÉ2 + (‚àí1)Nt Œ¥œÉ2, formulas (9.81)-(9.82)
can be rewritten as
v(t, St) = sup
Œ±
E‚àó(g(ST)|St) and
u(t, St) = inf
Œ± E‚àó(g(ST)|St).
(9.83)
Let us consider v(t, St) only because the case of the lower price u(t, St) (9.83) can be
treated in the same way. Now we rewrite model (9.80) via P‚àó(Œ±) :
dSt = Œ±(t, St)dW‚àó
t ,
(9.84)
where W‚àóis a Wiener process w.r. to P‚àó.
According the Ito formula we obtain
dv(t, St) =
 ‚àÇv(t, St)
‚àÇt
+ 1
2
‚àÇ2v(t, St)
‚àÇx2
Œ±2

dt + Œ± ‚àÇv(t, St)
‚àÇx
dW‚àó
t .
Hence, the equality (9.83) can be rewritten as
v(t, St) = sup
Œ±
Et,St

v(t, St) +
‚à´T
t
 ‚àÇv
‚àÇs + Œ±2
2
‚àÇ2v
‚àÇx2

+
‚à´T
t
Œ± ‚àÇv
‚àÇx dW‚àó
s

.
(9.85)
Using the martingale property of the last term of (9.85) we arrive to the equation
0 = sup
Œ±
E‚àó
t,St
‚à´T
t
 ‚àÇv
‚àÇs + Œ±2
2
‚àÇ2v
‚àÇx2

ds

(9.86)
Divide both sides of (9.86) by T ‚àít and let T ‚Üít we get

136
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
0 = sup
Œ±
 ‚àÇv(t, St)
‚àÇt
+ Œ±2
2
‚àÇ2v(t, St)
‚àÇx2

.
(9.87)
Similarly, we can obtain that
0 = inf
Œ±
 ‚àÇu(t, St)
‚àÇt
+ Œ±2
2
‚àÇ2u(t, St)
‚àÇx2

.
(9.88)
Putting to (9.87)-(9.88) Œ±2 = œÉ2 + (‚àí1)Nt Œ¥œÉ2, we determine D = (œÉ2 ‚àíŒ¥œÉ2,
œÉ2 + Œ¥œÉ2) and
0 =‚àÇv(t, St)
‚àÇt
+ 1
2
‚àÇ2v(t, St)
‚àÇx2

œÉ2 + sgn
 ‚àÇ2v
‚àÇx2 (t, St)

Œ¥œÉ2

0 =‚àÇu(t, St)
‚àÇt
+ 1
2
‚àÇ2u(t, St)
‚àÇx2

œÉ2 ‚àísgn
 ‚àÇ2u
‚àÇx2 (t, St)

Œ¥œÉ2

.
So, we arrive to the following theorem of pricing of a call option in the model (9.80).
Theorem 9.7 The bounds of non-arbitrage prices of a call option for any t ‚â§T are
calculated as solutions of the HJB-equations:
‚àÇv(t, St)
‚àÇt
+ 1
2œÉ2 ‚àÇ2v(t, St)
‚àÇx2
+ 1
2




‚àÇ2v
‚àÇx2




 Œ¥œÉ2 = 0,
v(T, x) = (x ‚àíK)+;
(9.89)
‚àÇu(t, St)
‚àÇt
+ 1
2œÉ2 ‚àÇ2u(t, St)
‚àÇx2
‚àí1
2




‚àÇ2u
‚àÇx2




 Œ¥œÉ2 = 0,
u(T, x) = (x ‚àíK)+.
(9.90)
The next theorem shows how equations (9.89) and (9.90) can be solved approxi-
mately by means of the small perturbations method.
Theorem 9.8 Assume that Œ¥œÉ2 << œÉ2, then the initial upper and lower prices of a
call option with strike price K admit approximations given by the formulas
ÀÜC‚àó= (S0 ‚àíK)Œ¶
 S0 ‚àíK
œÉ
‚àö
T

+ œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

+ Œ¥œÉ2
2œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

,
(9.91)
ÀÜC‚àó= (S0 ‚àíK)Œ¶
 S0 ‚àíK
œÉ
‚àö
T

+ œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

‚àíŒ¥œÉ2
2œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

,
(9.92)
where œÜ(x) =
1
‚àö
2œÄ e‚àíx2/2, Œ¶(x) =
‚à´x
‚àí‚àûœÜ(y)dy.
Proof Both formulas (9.91) and (9.92) are derived in the same way using the method
of small perturbations from the PDEs theory. Deriving (9.91) only we represent v(t, x)
as follows
v(t, x) = v0(t, x) + v1(t, x)Œ¥œÉ2 + v2(t, x)(Œ¥œÉ2)2 + . . .
(9.93)

9.4
Controlled diÔ¨Äusion processes and applications to option pricing
137
Let us make a change of variable Œ∏ = œÉ2(T ‚àít) and obtain v(t, x) = V(Œ∏, x) and
‚àÇv
‚àÇt = ‚àíœÉ2 ‚àÇV
‚àÇŒ∏ .
Hence, the HJB-equation (9.89) is transformed to the following one
‚àÇV
‚àÇŒ∏ = 1
2
‚àÇ2V
‚àÇx2 + Œ¥œÉ2
2œÉ2




‚àÇ2V
‚àÇx2




, V(0, x) = (x ‚àíK)+.
(9.94)
To solve (9.94) we rewrite (9.93) in new variables ignoring terms of the order greater
one:
V(Œ∏, x) = V0(Œ∏, x) + V1(Œ∏, x)Œ¥œÉ2 + o(Œ¥œÉ2)..
(9.95)
Plugging (9.95) to the equation (9.94) we get equations for V0 and V1 :
‚àÇV0
‚àÇŒ∏ =1
2
‚àÇ2V0
‚àÇx2 , V0(0, x) = (x ‚àíK)+,
‚àÇV1
‚àÇŒ∏ =1
2
‚àÇ2V1
‚àÇx2 +
1
2œÉ2




‚àÇ2V0
‚àÇx2




, V1(0, x) = 0.
(9.96)
The Ô¨Årst equation in (9.96) is exactly the Bachelier equation, and we know its solution
(see, for example, (9.57)). Moreover, we have for derivatives of V0 that
‚àÇV0
‚àÇx =Œ¶
 x ‚àíK
‚àö
Œ∏

+ (x ‚àíK)œÜ
 x ‚àíK
‚àö
Œ∏
 1
‚àö
Œ∏
+ (x ‚àíK)œÜ
 x ‚àíK
‚àö
Œ∏
 ‚àíx + K
‚àö
Œ∏
= Œ¶
 x ‚àíK
‚àö
Œ∏

,
‚àÇ2V0
‚àÇx2 =œÜ
 x ‚àíK
‚àö
Œ∏
 1
‚àö
Œ∏
‚â•0.
Therefore, the second equation in (9.96) is transformed to the next one
‚àÇV1
‚àÇŒ∏ = 1
2
‚àÇ2V1
‚àÇx2 +
1
2œÉ2‚àö
Œ∏
œÜ
 x ‚àíK
‚àö
Œ∏

,
for which we Ô¨Ånd that V1(Œ∏, x) =
‚àö
Œ∏
2œÉ2 œÜ(Œ∏, x).
Finally, we arrive to conclusion that
v(0, S0) ‚âàV(œÉ2T, S0) = (S0 ‚àíK)Œ¶
 S0 ‚àíK
œÉ
‚àö
T

+ œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

+ Œ¥œÉ2
2œÉ
‚àö
TœÜ
 S0 ‚àíK
œÉ
‚àö
T

which proves the formula (9.91).
‚äì‚äî
Problem 9.4 Consider the Black-Scholes model with stochastic volatility and zero
interest rate
dSt =St (Œºdt + œÉtdWt), S0 > 0,
œÉ2
t =sœÉ2 + (‚àí1)Nt Œ¥œÉ2, Œ¥œÉ2 < œÉ2, t ‚àà[0,T].
(9.97)

138
9
Stochastic diÔ¨Äerential equations, diÔ¨Äusion processes and their applications
Using a similar reasoning as in Theorem 9.7, derive the following HJB-equations
for the upper and lower call option prices
‚àÇv
‚àÇt + 1
2œÉ2x2 ‚àÇ2v
‚àÇx2 + 1
2




‚àÇ2v
‚àÇx2




 x2Œ¥œÉ2 = 0,
v(T, x) = (x ‚àíK)+;
‚àÇu
‚àÇt + 1
2œÉ2x2 ‚àÇ2u
‚àÇx2 ‚àí1
2




‚àÇ2u
‚àÇx2




 x2Œ¥œÉ2 = 0,
u(T, x) = (x ‚àíK)+.
Problem 9.5 Assuming in the model (9.97) that Œ¥œÉ2 << œÉ2, Ô¨Ånd the Ô¨Årst order
approximations for initial lower and upper prices C‚àóand C‚àóof call option:
ÀÜC‚àó= S0Œ¶
 ln S0/K + œÉ2T/2
œÉ
‚àö
T

‚àíKŒ¶
 ln S0/K ‚àíœÉ2T/2
œÉ
‚àö
T

+ KŒ¥œÉ2
2œÉ2 œÉ
‚àö
TœÜ
 ln S0/K ‚àíœÉ2T/2
œÉ
‚àö
T

,
ÀÜC‚àó= S0Œ¶
 ln S0/K + œÉ2T/2
œÉ
‚àö
T

‚àíKŒ¶
 ln S0/K ‚àíœÉ2T/2
œÉ
‚àö
T

‚àíKŒ¥œÉ2
2œÉ2 œÉ
‚àö
TœÜ
 ln S0/K ‚àíœÉ2T/2
œÉ
‚àö
T

.

Chapter 10
General theory of stochastic processes
under ‚Äúusual conditions‚Äù
Abstract Chapter 10 is devoted to a systematic exposition of a continuous time
version of stochastic analysis under ‚Äúusual conditions‚Äù with its standard notions like
a stochastic basis, Ô¨Åltration, stopping times, random sets, predictable and optional
sigma-algebras etc. It is shown how the discrete time martingale theory as well as
a pure continuous time theory of diÔ¨Äusion processes are generalized for so-called
cadlag processes. Using the predictable notion of a compensator the fundamental
Doob-Meyer theorem is formulated for the class of sub- and supermartingales of class
D. The full version of stochastic integration of predictable processes with respect
to square-integrable martingale is developed. Moreover, diÔ¨Äerent decompositions of
such martingales are proved as well as the Kunuta-Watanabe inequality. It is shown
how the theory can be extended with the help of localization procedures (local
martingales, processes with locally integrable variation, semimartingales). The Ito
formula is proved for semimartingales. SDEs with respect to semimartingales are
studied including the existence and uniqueness of solutions of such equations with
the Lipschitz coeÔ¨Écients (see [2], [8], [9], [16], [18], [20], [26], [33], [36], and [37]).
10.1
Basic elements of martingale theory
We will operate here with a complete probability space (Œ©, F, P) equipped with a
non-decreasing family of œÉ-algebras Ft ‚äÜF satisfying the ‚Äúusual conditions‚Äù:
1) Fs ‚äÜFt, s ‚â§t, Ft = Ft+ = ‚à©œµ>0Ft+œµ;
2) Ft contains all the P-null sets of F .
Let us call such a stochastic basis (Œ©, F, (F )t ‚â•0, P) standard.
In stochastic analysis, a special place is occupied by the set of stopping times.
That is why we study this notion in a more general setting as before.
DeÔ¨Ånition 10.1 A non-negative random variable œÑ : Œ© ‚ÜíR+ ‚à™{‚àû} is a stopping
time, if for any t ‚â•0
{œâ : œÑ(œâ) ‚â§t} ‚ààFt.
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_10
139

140
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
We also deÔ¨Åne a œÉ-algebra
FœÑ = {A ‚ààF‚àû= œÉ (‚à™t ‚â•0Ft) : A ‚à©{œÑ ‚â§t} ‚ààFt}
as a set of all events occurred before œÑ.
Let us formulate some properties of s.t.‚Äôs as the following problem.
Problem 10.1 1) If œÑ and œÉ are s.t.‚Äôs, then
œÑ ‚à®œÉ = max{œÑ, œÉ} and
œÑ ‚àßœÉ = min{œÑ, œÉ} are s.t.‚Äôs.
2) If (œÑn)n=1,2,... is a monotone sequence of s.t.‚Äôs, then
œÑ = lim
n‚Üí‚àûœÑn is a s.t.
3) If œÑ is a s.t., then FœÑ is a œÉ-algebra.
4) For two s.t.‚Äôs œÑ ‚â§œÉ we have FœÑ ‚äÜFœÉ.
5) Let œÑ be a s.t. and A ‚ààFœÑ, then
œÑA =

œÑ
on A,
‚àû
on Ac
is a s.t.
6) Let œÑ be a s.t., then there exists a monotonic sequence of s.t.‚Äôs œÑn > œÑ such that
limn‚Üí‚àûœÑn = œÑ (a.s.).
DeÔ¨Ånition 10.2 A s.t. œÑ is called predictable, if there exists a non-decreasing
sequence (œÑn)n=1,2,... of s.t.‚Äôs such that
lim
n‚Üí‚àû= œÑ (a.s.), œÑn < œÑ (a.s.) on {œÑ > 0}.
In this case we say that (œÑn)n=1,2,... announces œÑ. Moreover, we denote FœÑ‚àí=
œÉ ‚à™‚àû
n=1FœÑn
 as a collection of events occurring strictly before œÑ.
Problem 10.2 1) If A ‚ààFœÑ‚àí, then œÑA is a predictable s.t.
2) If a.s. œÉ < œÑ, then
FœÉ ‚äÜFœÑ‚àí‚äÜFœÑ.
DeÔ¨Ånition 10.3 For a s.t. œÑ we deÔ¨Åne a subset of R+ √ó Œ©
‚ü¶œÑ‚üß= {(t, œâ) : t = œÑ(œâ) < ‚àû}
as the graph of œÑ.
DeÔ¨Ånition 10.4 If the graph ‚ü¶œÑ‚üßcan be embedded to a countable union of graphs
of predictable s.t.‚Äôs, then a s.t. œÑ is accessible. It means that Œ© = ‚à™nAn, and on each

10.1
Basic elements of martingale theory
141
element An of such partition œÑ is announced by a sequence of s.t.‚Äôs (œÑn,m)m=1,2,.... So,
a s.t. œÑ will be predictable if the sequence (œÑn,m) can be chosen without dependence
on n.
If a s.t. œÑ such that P(œâ : œÑ = œÉ < ‚àû) = 0 for every predictable s.t. œÉ, then œÑ is
totally inaccessible.
Let us note that every s.t. œÑ can be decomposed as follows. There exists a unique
(up to zero probability) set A ‚ààFœÑ such that œÑA is accessible and œÑAc is totally
inaccessible, A ‚äÜ{œâ : œÑ(œâ) < ‚àû}.
We need to connect these notion with the notion of a stochastic process. We
understand a stochastic process X = Xt(œâ) = X(t, œâ) as a mapping
X : R+ √ó Œ© ‚ÜíRd.
For simplicity we consider the case d = 1. Another stochastic process Y is a modiÔ¨Å-
cation of X, if for arbitrary t ‚â•0 :
P{œâ : Xt  Yt} = 0.
Two stochastic processes X and Y are indistinguishable, if
P{œâ : Xt(œâ) = Yt(œâ) for all t} = 1.
Example 10.1 To demonstrate a diÔ¨Äerence between the notion ‚ÄúmodiÔ¨Åcation‚Äù and
‚Äúindistinguishability‚Äù we give a standard example. Take Xt := 0 and
Yt =

0,
t  œÑ,
1,
t = œÑ,
where a r.v. œÑ is exponentially distributed with parameter Œª > 0. Then for Ô¨Åxed t0 we
have P(Xt0 = Yt0) = P(œÑ  t0) = 1, but P(Xt = Yt for all t) = 0.
DeÔ¨Ånition 10.5 If X is a measurable mapping from (R+ √ó Œ©, B(R+) √ó F ) to
(R, B(R)), then such a stochastic process is measurable.
If for t ‚ààR+ a r.v. Xt is Ft-measurable, then the process X is adapted (to Ô¨Åltration
(Ft)).
For each t ‚â•0 we can consider a restriction of X to the set [0, t] √ó Œ©. If such
restriction is B([0, t]) √ó Ft-measurable, then the process X is called progressively
measurable. A family of sets A ‚äÜR+ √ó Œ© such that the indicator process X(t, œâ) =
IA(t, œâ) is progressively measurable is a œÉ-algebra of progressively measurable sets
Œ†.
In case of a progressively measurable process X we have the following important
property: XœÑ is FœÑ-measurable for any s.t. œÑ, where X‚àûmust be F‚àû-measurable .
Let us note that for any progressively measurable set A a random variable
DA(œâ) = inf{t : (t, œâ) ‚ààA},

142
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
DA(œâ) = ‚àûif the set {¬∑} = ‚àÖ, is a stopping time which is called the debut of A.
One can note that every adapted and measurable process X admits a progressively
measurable modiÔ¨Åcation. It is convenient to operate with a stochastic process process
X almost all trajectories of which are right-continuous and admit left-limits at each
time. Such processes are called cadlag. It turns out such adapted cadlag process X is
progressively measurable. This fact follows from the next standard considerations.
For t > 0, n = 1, 2, . . ., i = 0, 1, . . ., 2n ‚àí1, s ‚â§t we put X(n)
0 (œâ) = X0(œâ)
X(n)
s (œâ) = X (i+1)t
2n (œâ) for it
2n < s ‚â§i + 1
2n t.
(10.1)
According to (10.1) we get X(n) which is B(0, t) √ó Ft-measurbale and due to the right-
continuity limn‚Üí‚àûX(n)
s (œâ) = Xs(œâ) for (s, œâ) ‚àà[0, t] √ó Œ©. Hence, X is progressively
measurable.
Further, deÔ¨Åne the random variables
œÑ1,0 = 0, œÑ1,1 = inf(t > 0 : |ŒîXt| ‚â•1), œÑ1,2 = inf(t > œÑ1,1 : |ŒîXt| ‚â•1), . . .
œÑk,0 = 0, . . ., œÑk,n = inf

t > œÑk,n‚àí1 : 1
k ‚â§|ŒîXt| <
1
k ‚àí1

, . . .,
(10.2)
where as usual ŒîXt = Xt ‚àíXt‚àí.
These random variables are stopping times because (Xt) and (Xt‚àí) are progres-
sively measurable. Moreover, the set
U = {(t, œâ) : ŒîXt(œâ)  0} ‚äÜ‚à™k ‚â•1,n‚â•1‚ü¶œÑk,n‚üß
and even can be embedded to the union of graphs ‚ü¶œÉn‚üßand ‚ü¶œÑn‚üß, where (œÉn) are
predictable s.t.‚Äôs, (œÑn) are totally inaccessible s.t.‚Äôs such that P(œÉn = œÉm < ‚àû) = 0
and P(œÑn = œÑm < ‚àû) = 0, n  m.
Besides Œ† we need to introduce two additional œÉ-algebras on the space [0, ‚àû) √ó Œ©.
We shall do it with the help of the notion of stochastic intervals. These are examples
of random set related to the stopping times œÉ and œÑ :
‚ü¶œÉ, œÑ‚ü¶= {(t, œâ) : œÉ(œâ) ‚â§t < œÑ(œâ)},
‚ü¶œÉ, œÑ‚üß= {(t, œâ) : œÉ(œâ) ‚â§t ‚â§œÑ(œâ) < ‚àû},
and so on, and also their graphs ‚ü¶œÉ‚üß= ‚ü¶œÉ, œÉ‚üß, ‚ü¶œÑ‚üß= ‚ü¶œÑ, œÑ‚üß.
The Ô¨Åltration (Ft)t ‚â•0 induces in [0, ‚àû) √ó Œ© besides Œ† the predictable and the
optional œÉ-algebras P and O :
P =œÉ{‚ü¶0A‚üß, ‚ü¶0, œÑ‚üß: A ‚ààF0, œÑ is a s.t.}, ‚ü¶0A‚üß= {0} √ó A,
O =œÉ{‚ü¶0, œÑ‚ü¶: œÑ is a s.t.}.
The processes which are measurable w.r. to P (correspondently, O) are called pre-
dictable (optional).

10.1
Basic elements of martingale theory
143
Problem 10.3 Let (œÑi) is a non-decreasing sequence of s.t.‚Äôs and bounded random
functions œÜ‚àó
0 and œÜi are F0 and FœÑi-measurable correspondently. DeÔ¨Åne a stochastic
process
Xt(œâ)œÜ‚àó
0(œâ)I{0}(t) +
n‚àí1

i=0
œÜi(œâ)I‚üßœÑi,œÑi+1‚üß(t, œâ).
(10.3)
Prove that P is generated by all processes of type (10.3).
Remark 10.1 One can prove that P is generated by processes of type (10.3) with
deterministic œÑi = ti. Moreover, P is generated by all left-continuous (continuous)
adapted processes.
It is clear that for the left-continuous process X and a predictable s.t. œÑ XœÑI{œÑ<‚àû}-
FœÑ‚àí-measurable, and therefore such a statement will be true for all predictable
processes and times.
Remark 10.2 If A is a predictable set (A ‚ààP), then its debut is a s.t. Moreover, DA
is predictable and A \ ‚ü¶DA, ‚àû‚ü¶‚ààP, if ‚ü¶DA‚üß‚äÜA.
Remark 10.3 As far as the times œÑk,n of the n-th jump of size |ŒîXt| ‚àà
 1
k,
1
k‚àí1
	
, for an
adapted predictable process X (see (10.2)) they are predictable, U = ‚à™n‚â•1,k ‚â•1‚ü¶œÑk,n‚üß,
and XœÑn,k -FœÑn,k‚àí-measurable, according to Remark 10.1.
To go further, we will study two classes of stochastic processes: processes with Ô¨Ånite
variation and martingales.
DeÔ¨Ånition 10.6 1) The process A = (At)t ‚â•0 is increasing, if A0 = 0 (a.s.), adapted
and cadlag, As ‚â§At (a.s.) for s ‚â§t.
2) The process B = (Bt)t ‚â•0 is the process with Ô¨Ånite variation, if B0 = 0 (a.s.),
adapted and cadlag, for each œâ ‚ààŒ© the trajectory B¬∑(œâ) has Ô¨Ånite variation on each
compact interval.
Each process with Ô¨Ånite variation can be decomposed into diÔ¨Äerence of two
increasing processes and vice versa. For any bounded (non-negative) Borel func-
tion f (s) one can deÔ¨Åne the Lebesgue-Stieltjes integral
‚à´t
0 f (s)dBs(œâ). We denote
‚à´t
0 f (s)|dBs| the integral of f under the variation of B, and
‚à´t
0 |dBs| will be equal to
the variation of B on [0, t].
DeÔ¨Ånition 10.7 We call an increasing process A integrable if E
‚à´t
0 dAs < ‚àû. Cor-
respondently, a process B has integrable variation if E
‚à´‚àû
0
|dBs| < ‚àû. The corre-
sponding classes of such processes we denote A+ and A.
The process B with Ô¨Ånite variation can be decomposed as follows
Bt = Bc
t +

s‚â§t
ŒîBs,
(10.4)
where Bc is a continuous process with Ô¨Ånite variation and 
s‚â§t |ŒîBs| ‚â§
‚à´t
0 |dBs| <
‚àû.

144
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Moreover,
in
decomposition
(10.4)
its
discontinuous
part

s‚â§t ŒîBs =

n‚â•1 ŒîBœÑn I{œÑn ‚â§t } with some sequence of s.t.‚Äôs (œÑn)n‚â•1. If the process B is pre-
dictable, then œÑn are predictable too, and ŒîBœÑn ‚àíFœÑn‚àí-measurable. Hence, any pre-
dictable process with Ô¨Ånite variation admits the form (10.4) as
Bt = Bc
t +

n‚â•1
œÜnI{œÑn ‚â§t },
(10.5)
where œÜn ‚àíFœÑn‚àí-measurable and 
n‚â•1 |œÜn|I{œÑn ‚â§t } exists for all t ‚ààR+.
In fact, if B satisÔ¨Åes (10.5) then it is a predictable process with Ô¨Ånite variation.
DeÔ¨Ånition 10.8 A martingale is an adapted integrable process M = (Mt)t ‚â•0 such
that
E(Mt|Fs) = Ms (a.s.) f or all t ‚â•s ‚â•0.
(10.6)
If in (10.6) the ‚Äú=‚Äù is changed by ‚Äú‚â•‚Äù (correspondently, ‚Äú‚â§‚Äù), then we have a
submartingale (supermartingale).
Without loss of generality we always can count these processes cadlag, because under
‚Äúusual conditions‚Äù every submartingale admits the right-continuous modiÔ¨Åcation if
and only if its expected value is right-continuous.
We can transform any martingale (Mt)t ‚â•0 to a submartingale (œÜ(Mt))t ‚â•0 with
the help of a convex function œÜ such that E|œÜ(Mt)| < ‚àû, t ‚ààR+, using Jensen‚Äôs
inequality.
As in discrete time, we can deÔ¨Åne for given process X = (Xt)t ‚â•0 the following
notions. Let a < b be real numbers and F be a Ô¨Ånite subset of R+. For each (restricted
to F) trajectory of process (Xt(œâ))t ‚â•0 one can deÔ¨Åne the number of upcrossings
UF(a, b, X¬∑(œâ)) of the interval [a, b]. To do it we take
œÑ1(œâ) = inf(t ‚ààF : Xt(œâ) ‚â§a) and for j = 1, 2, . . .
œÉj(œâ) = inf(t ‚ààF : t ‚â•œÑj(œâ), Xt(œâ) > b),
œÑj+1(œâ) = inf(t ‚ààF : t ‚â•œÉj(œâ), Xt(œâ) < a),
and deÔ¨Åne UF(a, b, X¬∑(œâ)) the largest j such that œÉj(œâ) < ‚àû.
For an inÔ¨Ånite set G ‚äÜR+ one can deÔ¨Åne
UG(a, b, X¬∑(œâ)) = sup{UF(a, b, X¬∑(œâ)) : F ‚äÜG, F is Ô¨Ånite}.
Let us formulate the following Doob inequalities for continuous time submartingales.
Theorem 10.1 Assume X = (Xt)t ‚â•0 is a submartingale, [t1, t2] ‚äÜR+, Œª > 0. Then
1) P

œâ : supt1 ‚â§t ‚â§t2 Xt ‚â•Œª

‚â§
EX+
t2
Œª ;
2) P

œâ : inft1 ‚â§t ‚â§t2 Xt ‚â§‚àíŒª

‚â§
EX+
t2EXt1
Œª
;
3) EU[t1,t2](a, b, X¬∑(œâ)) ‚â§
EX+
t2+|a|
b‚àía
4)E

supt1 ‚â§t ‚â§t2 Xt
	 p
‚â§

p
p‚àí1
	
EX p
t2, p > 1,
for a non-negative (Xt) wih EX p
t < ‚àû.

10.1
Basic elements of martingale theory
145
The derivation of inequalities of the above theorem is based on limiting arguments,
right-continuity of X and discrete time versions of these results.
Let us pay attention to other important results of martingale theory.
Theorem 10.2 If X = (Xt)t ‚â•0 is a submartingale with supt ‚â•0 EX+
t < ‚àû, then there
exists a r.v. X‚àû‚àíF‚àû-measurable and integrable such that X‚àû(œâ) = limt‚Üí‚àûXt(œâ)
(a.s.).
Proof For n = 1, 2, . . . and a < b ‚ààR we have from Theorem 9.1 that
EU[0,n](a, b, X¬∑(œâ)) ‚â§EX+
n + |a|
b ‚àía
.
(10.7)
Taking the limit as n ‚Üí‚àûin (10.7) we get
EU[0,‚àû[(a, b, X¬∑(œâ)) ‚â§supt EX+
n + |a|
b ‚àía
.
(10.8)
Denote Aa,b = {œâ : U[0,‚àû)(a, b, X¬∑(œâ)) = ‚àû} and Ô¨Ånd from (10.8) that P(Aa,b) = 0.
Hence,
P(A) = P(‚à™a,b‚ààQAa,b) = 0.
The
set
A
contains
the
set
{œâ : lim supt‚Üí‚àûXt(œâ) > lim inft‚Üí‚àûXt(œâ)}.
Now, for œâ ‚ààŒ© \ A there exists
X‚àû(œâ) = lim
t‚Üí‚àûXt(œâ) (a.s.)
Due to E|Xt| = 2EX+
t ‚àíEXt ‚â§2 supt EX+
t ‚àíEX0 and Fatou‚Äôs lemma we obtain that
E|X‚àû| < ‚àû.
‚ñ°
Corollary 10.1 Let X = (Xt)t ‚â•0 be a martingale. Then the following statements are
equivalent
1) (Xt)t ‚â•0 is uniformly integrable;
2) Xt converges to X‚àûin L1;
3) There exists X‚àû‚ààL1 such that Xt = E(X‚àû|Ft).
Corollary 10.2 Let X = (Xt)t ‚â•0 be a nonnegative supermartingale. Then there
exists an integrable r.v. X‚àû= limt‚Üí‚àûXt (a.s.).
Corollary 10.3 Let X = (Xt)t ‚àà[0,‚àû] be a submartingale on extended real line R+ ‚à™
{‚àû}. Then for stopping times œÉ ‚â§œÑ
E(XœÑ|FœÉ) ‚â•XœÉ(a.s.)
(10.9)
Proof Consider the following sequence of discrete stopping times
œÉn(œâ) =

œÉ(œâ),
œÉ = ‚àû,
kn
2 ,
k‚àí1
2n ‚â§œÉ(œâ) <
k
2n .
Similar sequence is constructed for œÑ, and by construction and conditions of the
theorem œÉn ‚â§œÑn, n = 1, 2, . . . . Applying (10.9) with œÉn and œÑn, we derive (10.9) by
taking the limit as n ‚Üí‚àû.
‚ñ°

146
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Corollary 10.4 Let X = (Xt)t ‚â•0 be a submartingale and œÉ ‚â§œÑ be stopping times.
Then
1) (XœÑ‚àßt)t ‚â•0 is a submartingale w.r.to (Ft);
2) E(XœÑ‚àßt|FœÉ) ‚â•XœÉ‚àßt (a.s.) for all t ‚â•0.
Corollary 10.5 If M = (Mt)t ‚â•0 is a uniformly integrable martingale and œÑ is a
predictable stopping time, then E(ŒîMœÑ|FœÑ‚àí) = 0.
To prove it we consider a sequence of stopping times (œÑn)n=1,2,... announcing œÑ.
Applying Theorem 10.2 to this sequence we get MœÑn = E(M‚àû|FœÑn) = E(MœÑ|FœÑn).
Taking the limit in this equality we obtain (a.s.)
MœÑ‚àí= lim
n‚Üí‚àûMœÑn = lim
n‚Üí‚àûE(MœÑ|FœÑn) = E(MœÑ|FœÑ‚àí).
Denote M the class of uniformly integrable martingales.
Assume M ‚ààM and A ‚ààA+, then the process X = M ‚àíA is a supermartingale,
satisfying the condition (D), i.e. the class of random variables {XœÑI{œÑ<‚àû} : œÑ is a
s.t.} is uniformly integrable. Hence, such a construction leads to a supermartingale
X ‚ààD. The inverse statement is the famous Meyer‚Äôs theorem:
Any supermartingale X ‚ààD admits the following decomposition of Doob-Meyer:
for t ‚â•0 (a.s.)
Xt = Mt ‚àíAt,
(10.10)
where M ‚ààM and A is a predictable process from A+. The decomposition (10.10)
is unique in the class of predictable increasing processes.
Remark 10.4 If a supermartingale X ‚ààD and B is the predictable increasing process
from the Doob-Meyer decomposition, then we can calculate the jumps of B as
follows. First of all, the moments of jumps of B are predictable. For any such a
moment œÑ we can apply Corollary 10.4 with the uniformly integrable martingale
X = M + B and get
0 = E(ŒîMœÑ|FœÑ‚àí) = E(ŒîXœÑ + ŒîBœÑ|FœÑ‚àí) = E(ŒîXœÑ|FœÑ‚àí) + ŒîBœÑ,
and hence ŒîBœÑ = ‚àíE(ŒîXœÑ|FœÑ‚àí).
Let us note that for every A ‚ààA+ the process (‚àíA) is a supermartingale of class
(D).
Applying to (‚àíA) its Doob-Meyer decomposition, we Ô¨Ånd a unique predictable
process B ‚ààA+ such that B ‚àíA ‚ààM.
DeÔ¨Ånition 10.9 The process B is called the compensator of A.
These considerations can be extended to the class V of processes with integrable
variation. Let A ‚ààV, then there exists a unique predictable process B ‚ààV with the
property: B ‚àíA ‚ààM. We will call B the compensator of A.
Remark 10.5 a) If œÑ is a totally inaccessible stopping time and œÜ is an integrable FœÑ-
measurable function, then the compensator of At = œÜI{œÑ‚â§t } is a continuous process
from class V.

10.1
Basic elements of martingale theory
147
b) If œÑ is predictable and œÜ is an integrable FœÑ-measurable function, then the
compensator of At = œÜI{œÑ‚â§t } is the process Bt = E(œÜ|FœÑ‚àí)I{œÑ‚â§t }.
The rest of this Section is devoted to square-integrable martingales.
DeÔ¨Ånition 10.10 A martingale M is square integrable if supt EM2
t < ‚àû. The class
of such martingales is denoted of M2.
We count some properties of such martingales in the following theorem.
Theorem 10.3 Let M ‚ààM2, then the next properties are fulÔ¨Ålled:
(1) (M2
t )t ‚â•0 is a submartingale;
(2) there exists M‚àû= limt‚Üí‚àûMt (a.s.) and in L2;
(3)E supt M2
t ‚â§4 supt EM2
t = 4EM2
‚àû;
(4) E 
t(ŒîMt)2 ‚â§lim inf(t1<...<tn) E (Mti ‚àíMti‚àí1)2 = EM2
‚àû< ‚àû.
Proof Statements (1)-(3) come from Jensen‚Äôs inequality and Theorem 10.1 and 10.2.
The last one 4) follows from the Fatou lemma, where liminf is provided via directed
sets if subdivisions (t1 < . . . < tn).
‚ñ°
Let us note that due to the Doob inequality M2 is a submartingale from class
(D), and hence, by the Doob-Meyer decomposition there exists a unique predictable
A ‚ààA+ such that M2 ‚àíA ‚ààM.
The process A is denoted ‚ü®M, M‚ü©and is called a quadratic characteristic (com-
pensator) of M ‚ààM2.
Remark 10.6 We can cover the jumps of M ‚ààM2 by a sequence of predictable s.t.‚Äôs
(œÑn)n=1,2,... and a sequence of totally inaccessible s.t.‚Äôs (œÉn)n=1,2,...
For each n = 1, 2, . . . we deÔ¨Åne processes Cn
t = ŒîMœÑn I{œÑn ‚â§t } and Dn
t =
ŒîMœÉn I{œÉn ‚â§t }.
Due to Remark 10.5 in case Cn
t we have E(ŒîMœÑn |FœÑn‚àí)I{œÑn ‚â§t } = 0 ¬∑ I{œÑn ‚â§t } = 0,
and hence, it is a square integrable martingale. Another process Dn
t has a compensator
ÀúDn
t which is a continuous process with integrable variation. So, ÀÜDn
t = Dn
t ‚àíÀúDn
t ‚ààM
and Œî ÀÜDn
œÉn = ŒîMœÉn.
To investigate the structure of class M2 we need several auxiliary facts, which
we formulate as Lemmas.
Lemma 10.1 Let A and B ‚ààA, A ‚àíB be a martingale and œÜ be a bounded (non-
negative) predictable function. Then
E
‚à´‚àû
0
œÜsd(A ‚àíB)s = 0.
(10.11)
Proof It is clear that relation (10.11) is fulÔ¨Ålled for the step function (see (10.3) and
Remark 10.1)
œÜt = œÜ(t) = œÜ‚àó
0I(t)
{0} +
n‚àí1

i=0
œÜi(œâ)I(ti,ti+1](t),
and extension to predictable bounded functions is straightforward.
‚ñ°

148
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Lemma 10.2 If a : R+ ‚ÜíR+ is the right-continuous increasing function, a(0) = 0,
and c(s) = inf(t : a(t) > s), then c(s) is the increasing right-continuous satisfying
the equality
‚à´‚àû
0
f (s)da(s) =
‚à´‚àû
0
f (c(s))I{c(s)<‚àû}ds
(10.12)
for any bounded (non-negative) measurable function f .
Again, the proof is standard. We check (10.12) for f (s) = I(0,t](s) and so on.
Lemma 10.3 For any bounded martingale M and process A ‚ààA the following
equality is true
EA‚àûM‚àû= E
‚à´‚àû
0
MsdAs.
(10.13)
Proof Using (10.12) with A and c, we obtain (10.13) from the following chain of
equalities
E
‚à´‚àû
0
MsdAs =E
‚à´‚àû
0
Mcs I{cs<‚àû}ds
=
‚à´‚àû
0
EMcs I{cs<‚àû}ds
=
‚à´‚àû
0
E M‚àûI{cs<‚àû}
 ds
=E
‚à´‚àû
0
M‚àûI{cs<‚àû}ds
=E
‚à´‚àû
0
M‚àûdAs
=EM‚àûdA‚àû.
The claim of the next Lemma was proved before for discrete time martingales.
Lemma 10.4 Let (Lt)t ‚àà[0,‚àû] be an uniformly integrable process, L0 = 0. If ELœÉ = 0
for any stopping time œÉ, then L is a martingale.
Proof We deÔ¨Åne a s.t.
œÉ =

t
on A,
‚àû
on Ac,
where A is a Ô¨Åxed set from Ft. Then we obtain that
‚à´
A LtdP +
‚à´
Ac L‚àûdP = 0, which
can be transformed for œÉ = ‚àûto the equality
‚à´
A L‚àûdP +
‚à´
Ac L‚àûdP, i.e. (Lt) is a
martingale.
‚ñ°
Lemma 10.5 For
M ‚ààM2
and
any
predictable
s.t.
œÑ
the
process
ŒîCt = ŒîMœÑI{œÑ‚â§t } ‚ààM2, and for any N ‚ààM2 the process
Lt = CtNt ‚àíŒîCœÑŒîNœÑI{œÑ‚â§t } ‚ààM.

10.1
Basic elements of martingale theory
149
Proof The Ô¨Årst claim follows from Remark 10.6. Further, for the process (Lt) we
have
sup
t
|Lt| ‚â§sup
t
|Ct| sup
t
|Nt| + |ŒîCœÑ||ŒîNœÑ| ‚ààL1,
and for any s.t. œÉ and Nt‚àßœÉ = NœÉ
t by Lemma 10.3
EC‚àûNœÉ
‚àûN =ECœÉNœÉ = E
‚à´‚àû
0
NœÉ
s dCs = ENœÉ
œÑ ŒîCœÑ
=EŒîNœÉ
œÑ ŒîCœÑ = EŒîNœÑŒîCœÑI{œÉ‚â§œÑ},
where the equality ENœÑ
œÉ‚àíŒîCœÉ = E

NœÑ
œÉ‚àíE(ŒîCœÉ|FœÉ‚àí)

= 0 was used. We arrive to
conclusion that ELœÑ = 0 for any s.t. œÑ. Now the claim follows from Lemma 10.4. ‚ñ°
Lemma 10.6 Let M ‚ààM2 and œÉ be a totally inaccessible s.t. Let Dt = ŒîMœÉI{œÉ}
with compensator ÀúDt and ÀÜDt = Dt ‚àíÀúDt. Then
(1) The process ÀÜD ‚ààM2 and E ÀÜD2
‚àû‚â§5E(ŒîMœÉ)2;
(2) For any N ‚ààM2 the process
Lt = DtNt ‚àíŒîDœÉŒîNœÉI{œÉ‚â§t } ‚ààM.
Proof (1) It is suÔ¨Écient to consider Dt = œÜI{œÉ‚â§t } with non-negative function œÜ, FœÉ-
measurable and from L2. Otherwise, one can consider separately ŒîM+
œÉ and ŒîM‚àí
œÉ.
For function |œÜ| ‚â§a we have
E ÀúD2
‚àû=2E
‚à´‚àû
0
ÀúDsd ÀúDs = 2E
‚à´t
0
ÀúDsdDs
‚â§2E
‚à´‚àû
0
ÀúD‚àûdDs = 2EœÜ ÀúD‚àû‚â§2aE ÀúD‚àû< ‚àû,
where the formula f 2(‚àû) = 2
‚à´‚àû
0
f (s)df (s) is used. Hence, ÀúD‚àû‚ààL2 and E ÀúD2
‚àû‚â§
2EœÜ ÀúD‚àû‚â§2||œÜ||L2|| ÀúD‚àû||L2 and E ÀúD2
‚àû‚â§4EœÜ2 for a non-negative bounded function
œÜ.
In general case, we consider œÜn = œÜ ‚àßn and processes ÀúDn
t and Dn
t = œÜnI{œÉ‚â§t }.
Correspondently, ÀúDn+1
t
‚àíÀúDn
t is the compensator of the increasing continuous process
we have
E( ÀúDn
‚àû)2 ‚â§4E(œÜn)2 ‚â§4EœÜ2 < ‚àû.
(10.14)
The non-decreasing sequence ÀúDn
‚àûconverges, and according to (10.14) the limit
B‚àûwill be in L2, as in case Bt = limn ÀúDn
t . Each sample function ÀúDn
t (œâ) converges to
Bt(œâ) uniformly, as 0 ‚â§ÀúDn+
t
‚àíÀúDn
t ‚â§ÀúDn+1
‚àû
‚àíÀúDn
‚àû. Hence, the process Bt is continu-
ous, and by taking the limit in L1 in the equality E(Dn
t ‚àíÀúDn
t |Fs) = Dn
s ‚àíÀúDn
s, s ‚â§t,
we arrive to conclusion that Dt ‚àíBt ‚ààM. Therefore, Bt = ÀúDt, and E ÀúD2
‚àû= EB2
‚àû‚â§
4EœÜ2 < ‚àûand the martingale ÀÜD = D ‚àíÀúD ‚ààM2.
(2) As in Lemma 10.5, the process L is uniformly integrable. Take a stopping
time œÑ and applying Lemma 10.3 to ÀÜD and NœÑ, we obtain

150
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
E(DœÑ ‚àíÀúDœÑ)NœÑ = E
‚à´‚àû
0
NœÑ
t d(D ‚àíÀúD)t

= ENœÑ
œÉŒîDœÉ ‚àíE
‚à´‚àû
0
NœÑ
t d ÀúDt
= ENœÑ
œÉŒîDœÉ ‚àíE
‚à´‚àû
0
NœÑ‚àí
t‚àíd ÀúDt

= E

ŒîNœÉŒîDœÉI{œÉ‚â§t }

.
Finally, applying Lemma 10.4 we show that L is a martingale.
‚ñ°
DeÔ¨Ånition 10.11 A martingale M ‚ààM2 is called purely discontinuous, if M0 = 0,
and if for any continuous martingale N ‚ààM2 their product M ¬∑ N is a martingale.
Let us denote M2,c and M2,d subclasses of M2 of continuous and purely discon-
tinuous martingales. Now we are ready to formulate the key theorem for square
integrable martingales.
Theorem 10.4 Any martingale M ‚ààM2 admits the following unique decomposition
M = Mc + Md,
(10.15)
where Mc ‚ààM2,c and Md ‚ààM2,d.
Let us call Mc in (10.15) the continuous part of
M, and Md the compensated
sum of the jumps of M or the purely discontinuous part of M.
Proof Using Remark 10.6 with Lemma 10.5 and Lemma 10.6 we can see that
Cn ÀÜDn, Cn ÀÜDm, CnCm, ÀÜDn ÀÜDm (m  m) are martingales. Denote for N = 1, 2,
. . ., Y N
t
= N
1 Cn
t + N
1
ÀÜDn and Ô¨Ånd from Lemma 10.5 and Lemma 10.6 that for
N ‚â§N ‚Ä≤ :
E(Y N‚Ä≤
‚àû‚àíY N
‚àû)2 = E
 N‚Ä≤

N+1
(Cn
‚àû)2 +
N‚Ä≤

N+1
( ÀÜDn
‚àû)2

‚â§5E
 N‚Ä≤

N+1

(ŒîMœÑn)2 + (ŒîMœÉn)2	
.
(10.16)
It follows from (10.16) and Theorem 9.4 that
E sup
t
(Y N‚Ä≤
t
‚àíY N
t )2 ‚â§5E
 N‚Ä≤

N+1

(ŒîMœÑn)2 + (ŒîMœÉn)2	
.
It implies 
N E supt |Y N+1
t
‚àíY N
t |2 < ‚àûfor some subsequence (Y N), which we
denoted Y N again. By the Borel-Cantelli lemma, there exists Yt = limN Y N
t
(a.s.)
uniformly over t, and it is a limit in L2 too. So, the process Yt is cadlag with the same
jumps as M, and Y ‚ààM2.
‚ñ°
Now for a continuous martingale X ‚ààM2 the process XY N is a martingale by
Lemma 10.5 and Lemma 10.6. The convergenceY N
t
toYt in L2 implies a convergence
XtY N
t
to XtYt in L1. Hence, it is a martingale, and Md = Y is a purely discontinuous
martingale. Finally, Mc = M ‚àíMd is a continuous martingale, and we get (10.15).
Toprovetheuniquenessofsuchdecomposition,weassumethattherearetwodecom-
positions M = Mc + Md = ¬ØMc + ¬ØMd, and Mc ‚àí¬ØMc = ¬ØMd ‚àíMd is continuous and
purely discontinuous. Hence, (Md
t ‚àí¬ØMd
t )2 = E(Md
0 ‚àí¬ØMd)2 = 0 for all t ‚â•0.

10.2
Extension of martingale theory by localization of stochastic processes
151
DeÔ¨Ånition 10.12 Let M ‚ààM2, M0 = 0, Mc and Md are its continuous and purely
discontinuous parts of M. DeÔ¨Åne [M, M]t = ‚ü®Mc, Mc‚ü©t + 
s‚â§t(ŒîMs)2 and call it a
quadratic brackets of M.
We can note that (Mc
t )2 ‚àí‚ü®Mc, Mc‚ü©t, (Md
t )2 ‚àí
s‚â§t(ŒîMs)2 and McMd belong
to M. Hence, M2 ‚àí[M, M] ‚ààM, and ‚ü®M, M‚ü©is the compensator of [M, M].
Remark 10.7 If M0  0, then we can think of M0 as the jump of M at t = 0 and
deÔ¨Åne
[M, M] = ‚ü®Mc ‚àíM0, Mc ‚àíM0‚ü©+ M2
0 +

s‚â§t
(ŒîMs)2.
Remark 10.8 Let us list some properties such quadratic characteristics of square
integrable martingales.
DeÔ¨Åne for M, N ‚ààM, M0, N0 = 0, their joint quadratic characteristics (brackets,
compensators):
‚ü®M, N‚ü©t =1
2 (‚ü®M + N, M + N‚ü©t ‚àí‚ü®M, M‚ü©t ‚àí‚ü®N, N‚ü©t),
[M, N]t =1
2 ([M + N, M + N]t ‚àí[M, M]t ‚àí[N, N]t) .
(1) The process ‚ü®M, N‚ü©is the unique predictable process such that MN ‚àí
‚ü®M, N‚ü©‚ààM.
(2) For any stopping time œÑ we have
‚ü®MœÑ, N‚ü©= ‚ü®M, NœÑ‚ü©= ‚ü®MœÑ, NœÑ‚ü©= ‚ü®M, N‚ü©œÑ
and similar equalities are true for the quadratic brackets.
(3) [M, N] = ‚ü®Mc, Nc‚ü©+ 
s‚â§t ŒîMsŒîNs.
(4) MN ‚àí[M, N] ‚ààM.
Analysing class M of square integrable martingales we observe that a Wiener
process W does not belong to M, because EW2
t = t ‚Üí‚àûas t ‚Üí‚àû. In the next
section it will be shown how to extend M to include this key process to a bigger
class for which a very nice theory can be developed.
10.2
Extension of martingale theory by localization
of stochastic processes
We start with the following localization procedure. Let H be a class of stochastic
processes X = (Xt)t ‚â•0. DeÔ¨Åne Hloc the set of stochastic processes Y = (Yt)t ‚â•0 such
that for each of them we can Ô¨Ånd a sequence of stopping times œÑn ‚Üë‚àû, n ‚Üí‚àû,
such that the process YœÑn = (Yt‚àßœÑn)t ‚â•0 is in H for any n = 1, 2, . . . . The sequence
(œÑn)n=1,2,... is called localizing (fundamental) for the process Y. For example, in case
of classes Mloc and M2
loc we will call the corresponding processes local martingales
and locally square-integrable martingales.

152
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
DeÔ¨Ånition 10.13 We say that a s.t. œÑ reduces M ‚ààMloc if MœÑ ‚ààM.
Problem 10.4 Prove that for M ‚ààMloc the following statements are true:
(1) A s.t. œÉ reduces M ‚áîMœÉ ‚ààD.
(2) If a s.t. œÑ reduces M and a s.t. œÉ ‚â§œÑ, then œÉ reduces M.
(3) If s.t.‚Äôs œÉ and œÑ reduce M, then max(œÉ, œÑ) = œÉ ‚à®œÑ reduces M.
Hint: to prove (3) one can use the equality
MœÉ‚à®œÑ = MœÉ + MœÑ ‚àíMœÉ‚àßœÑ.
The next theorem shows that the localization does not expand the class of local
martingales.
Theorem 10.5 If M is locally a local martingale, then M is a local martingale.
Proof Denote a localizing sequence (œÑn)n=1,2,... such that MœÑn is a local martingale.
Denote R the set of all stopping times œÑ reducing M, and Œ≥ = esssupœÑ‚ààR)œÑ. There
exists a sequence œÉn ‚ààR which converges to Œ≥ almost surely. Let us make the
sequence (œÉn)n=1,2,... non-decreasing with the help of (3) of Problem 10.4. By the
deÔ¨Ånition Œ≥ majorates œÑn ‚Üí‚àû(n ‚Üí‚àû), and, hence, Œ≥ = ‚àû(a.s.), and (œÉ)n‚â•1 is
fundamental for M.
‚ñ°
Problem 10.5 (1) Let B be a process with locally integrable variation, i.e. B ‚ààAloc.
Prove that there exists a unique predictable process A ‚ààAloc such that B ‚àíA ‚àà
Mloc. We call A a compensator of B.
(2) Prove that the predictable process B with Ô¨Ånite variation has a locally bounded
variation.
(3) A process with Ô¨Ånite variation has a compensator ‚áîits variation is locally
integrable.
Remark 10.9 There is a simple way to check that the variation is locally integrable.
Namely, if for the process A with Ô¨Ånite variation there exists a localizing sequence
(œÑn) such that E sups‚â§œÑn |ŒîAs| < ‚àû, n = 1, 2, . . ., then the variation of A is locally
integrable. To verify this test we deÔ¨Åne a sequence of s.t.‚Äôs
œÉn = œÑn ‚àßinf

t :
‚à´t
0
|dAs| ‚â•n

, n = 1, 2, . . .
and Ô¨Ånd that
‚à´
‚ü¶0,œÉn‚üß
|dAs| ‚â§n + |ŒîAœÉn | ‚â§n + sup
s‚â§œÑn
|ŒîAs| ‚ààL1.
The next result plays a key role in many construction for local martingales. That
is why it is called a fundamental lemma in the literature.
Lemma 10.7 Let M ‚ààMloc, then
(1) M‚àó
t = sups‚â§t |Mt| ‚ààA+
loc,
(2) Mt = Ut + Vt, where U ‚ààMloc, |ŒîUt| ‚â§1, t ‚â•0,V ‚ààMloc ‚à©V.

10.2
Extension of martingale theory by localization of stochastic processes
153
Proof (1) Take a localizing sequence of s.t.‚Äôs (œÉn)n=1,2,... for M, and make elements
of this sequence Ô¨Ånite as follows œÉn ‚ÜíœÉn ‚àßn, n = 1, 2, . . . . Further, deÔ¨Åne s.t.‚Äôs
ÀúœÉn = œÉn ‚àßinf{t : |Mt| ‚â•n} ‚â§œÉn, n = 1, 2, . . .,
and obtain that M ÀúœÉn is integrable due to the uniform integrability of (MœÉn‚àßt)t ‚â•0.
On. stochastic interval ‚ü¶0, ÀúœÉn ‚ü¶we have |Mt| ‚â§n and M‚àó
ÀúœÉn ‚â§n + |M ÀúœÉn | belongs to
L1. Hence, (M‚àó
t )t ‚â•0 is locally integrable.
(2) Denote (At) = 
s‚â§t ŒîMsI{|ŒîMs |>1/2} which is Ô¨Ånite for almost all œâ. Using
this process and (ÀúœÉn), we deÔ¨Åne a new sequence of s.t.‚Äôs
œÑn = ÀúœÉn ‚àßinf{t :
‚à´t
0
|dAs| ‚â•n} ‚â§œÉn, n = 1, 2, . . . .
It is a localizing sequence such that
‚à´
‚ü¶0,œÑn‚üß
|dAt| ‚â§n + |ŒîAœÑn | ‚â§n + 2M‚àó
œÑn ‚ààL1.
Therefore, there is a compensator B for A, and V = A ‚àíB ‚ààMloc ‚à©V. Because
of predictability of B, its jumps are happen at predictable times œÑ only. Stopping
processes by sequence ÀúœÉn we get now
|ŒîB ÀúœÉn
œÑ | =
E

ŒîA ÀúœÉn
œÑ |FœÑ‚àí
	
=
E

ŒîM ÀúœÉn
œÑ
‚àíŒîM ÀúœÉn
œÑ
I{|ŒîMœÑ |<1/2}|FœÑ‚àí
	 ‚â§0 + 1/2.
Hence, the jumps of Ut = Mt ‚àíVt verify the following inequalities
|ŒîUt| ‚â§|Œî(M ‚àíA)t| + |ŒîBt| ‚â§1.
Moreover, the sequence of stopping times deÔ¨Åned as ÀúœÑn = inf{t : |Ut ‚â•n} is local-
izing, and |U ÀúœÑn
t | ‚â§n + 1, n = 1, 2, . . . .
‚ñ°
Below we use this fundamental lemma to prove a decomposition of any local
martingale into the sum of continuous and purely discontinuous if for any continuous
local martingale N ‚ààMloc their product MN ‚ààMloc.
Lemma 10.8 Let M ‚ààMloc ‚à©V. Then
(1) Vt

s‚â§t ŒîMs is a process with locally integrable variation and Mt = Vt ‚àí
ÀúVt, t ‚â•0.
(2) For any bounded continuous martingale N, the product MN ‚ààMloc.
Proof By Lemma 10.7 the process M‚àó
t ‚ààA+
loc. By Remark 10.9 the variation of
the process V is locally integrable. Therefore, there is a compensator ÀúV for V,
and the local martingale M ‚àí(V ‚àíÀúV) is continuous, and its variation is locally
integrable according to Lemma 10.7 and Remark 10.9. It means M ‚àí(V ‚àíÀúV)

154
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
admits a compensator, but due to a continuity of M ‚àí(V ‚àíÀúV) it has to be zero as
M ‚àí(V ‚àíÀúV) ‚ààMloc. As a result, M = V ‚àíÀúV.
(2) Let N be a bounded continuous martingale and let the variations of V and ÀúV
are already integrable (otherwise, we can choose a localizing sequence of s.t.‚Äôs and
work with stopped processes). For a s.t. œÑ with the help of Lemma 10.1 and Lemma
10.3 we obtain
EMœÑNœÑ = EM‚àûNœÑ
‚àû= E
‚à´‚àû
0
NœÑ
s dMs
= E
‚à´‚àû
0
NœÑ
s dVs ‚àíE
‚à´‚àû
0
NœÑ
s d ÀúVs
= E
‚à´‚àû
0
NœÑ
s dVs ‚àíE
‚à´‚àû
0
NœÑ
s dVs
= 0.
By Lemma 10.4 we arrive to conclusion that MN is a martingale.
‚ñ°
Theorem 10.6 Let M ‚ààMloc, then
Mt = Mc
t + Md
t ,
(10.17)
where Mc ‚ààMc
loc (continuous local martingales) and Md ‚ààMd
loc (a class of purely
discontinuous local martingales). The decomposition (10.17) is unique.
Proof According to the fundamental lemma we represent M = N + U with a locally
bounded local martingale N and a martingale with Ô¨Ånite variation U. Taking a
localizing sequence of s.t.‚Äôs (œÑn) we get NœÑn as a bounded martingale for each
n = 1, 2, . . . . Applying Theorem 10.5 we obtain NœÑn = (NœÑn)c + (NœÑn)d, and using
the uniqueness of such decomposition, we obtain that (NœÑn)c = (NœÑn+1)c on ‚ü¶0, œÑn‚üß.
Now putting
Nc =
‚àû

n=1
(NœÑn)cI‚üßœÑn‚àí1,œÑn‚üß
we get a continuous local martingale as well as a purely discontinuous local
martingale Nd = N ‚àíNc. As a result, we arrive to the following decomposition
M = Nc + (Nd + U), where Nd + U is a purely discontinuous local martingale due
to Lemma 10.8.
As far as the uniqueness of (10.17), we can assume that M = Xc + Xd = Mc +
Md. Then the equality Mc ‚àíXc = Xd ‚àíMd leads to conclusion that locally Mc ‚àí
Xc = Xd ‚àíMd = 0 by Theorem 10.5.
‚ñ°
Remark 10.10 For M ‚ààM ‚ààM2
loc we can apply the Doob-Meyer decomposition
(for M2) locally and construct its compensator ‚ü®M, M‚ü©‚ààA‚àó
loc, which is predictable
and M2 ‚àí‚ü®M, M‚ü©‚ààMloc. In particular such compensator is well deÔ¨Åned for any
continuous local martingale because Mc
loc ‚äÜM2
loc.

10.2
Extension of martingale theory by localization of stochastic processes
155
The exact extension of this approach to the whole class of local martingales
is not possible. Nevertheless, one can deÔ¨Åne for M ‚ààMloc a similar quadratic
characterization [M, M] as follows
[M, M]t = ‚ü®Mc, Mc‚ü©t +

s‚â§t
(ŒîMs)2,
where Mc is a continuous part of M.
The formula (10.2) shows that the process [M, M] is increasing, M2 ‚àí[M, M] ‚àà
Mloc, but it is not predictable anymore.
To provide the correctness of the deÔ¨Ånition of process [M, M] by formula (10.2)
we need to prove that 
s‚â§t(ŒîMs)2 converges for all t.
Again,weusethefundamentallemmatorepresent M = U + V.Further,
s‚â§t |ŒîVs|
converges and therefore 
s‚â§t |ŒîVs|2 converges too. Taking a localizing sequence (œÑn)
for U we obtain that E 
s‚â§œÑn |ŒîUs|2 < ‚àûand for almost all œâ

s‚â§œÑn
|ŒîMs| ‚â§2

s‚â§œÑn
|ŒîUs|2 + 2

s‚â§œÑn
|ŒîVs|2
converges.
Let us join together classes of local martingales and processes with Ô¨Ånite variation
to create a bigger family of stochastic processes.
DeÔ¨Ånition 10.14 A process X is called a semimartingale, if
X = X0 + M + A,
(10.18)
where X0 is an F0-measurable r.v., M ‚ààMloc, A ‚ààV.
The next auxiliary result speaks us when the semimartingale admits a unique
representation in its deÔ¨Ånition.
Lemma 10.9 Assume X is a semimartingale with bounded jumps |ŒîXt| ‚â§a < ‚àû.
Then X can be represented in a unique way as
X = X0 + M + A,
(10.19)
where X0 is an F0-measurable r.v., M ‚ààMloc, A is a predictable process with Ô¨Ånite
variation.
Proof Let us write the semimartingale X = X0 + N + B with N ‚ààMloc and B ‚ààV,
and with the uniquely deÔ¨Åned r.v. X0-F0-measurable. We can estimate the jumps of
B as follows
|ŒîBt| ‚â§|ŒîNt| + |ŒîXt| ‚â§2N‚àó
t + a, t ‚â•0.
So, the increasing process Yt = sups‚â§t |ŒîBs| ‚ààA+
loc, and by Remark 10.9 the
variation of B is also locally integrable, and, hence, there exists its compensator A.
Let us put M = N + B ‚àíA ‚ààMloc and Ô¨Ånd that X = X0 + M + A. If there are two

156
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
representations (10.19) for X = X0 + M + A = X0 + M‚Ä≤ + A‚Ä≤, then A ‚àíA‚Ä≤ ‚ààMloc
and therefore A‚Ä≤ is a compensator of A and due to predictability A = A‚Ä≤.
Now we are ready to prove that the class of semimartingales can not be extended
by localization procedures.
Theorem 10.7 Any stochastic process X which is locally a semimartingale is a
semimartingale.
Proof Denote (œÑn)n=1,2,... a localizing sequence of stopping times, i.e. XœÑn is a semi-
martingaleforeachn = 1, 2, . . .DeÔ¨ÅnetheprocessYt = 
s‚â§t ŒîXsI{ŒîXs| ‚â•1andnote
thatY hasÔ¨Ånitevariation.Therefore,thediÔ¨Äerence XœÑn ‚àíYœÑn isasemimartingalewith
jumps bounded by one. Let us use Lemma 10.9 and write the unique representation
XœÑn ‚àíYœÑn = X0 + M(n) + A(n) with M(n) ‚ààMloc, A(n) ‚ààV andpredictable.Further,
using the uniqueness of such representation we obtain on ‚ü¶0.œÑn‚üßthat
A(n) = A(n+1)
and
M(n) = M(n+1).
So, we get the processes A = 
n A(n)I‚üßœÑn‚àí1,œÑn‚üßand M = 
n M(n)I‚üßœÑn‚àí1,œÑn‚üß.
‚ñ°
The process A is predictable process with Ô¨Ånite variation, M is locally a local
martingale, and according to Theorem 10.6 belongs to class Mloc. As a result, we
conclude that X is a semimartingale.
Another result of this type is presented in the following theorem.
Theorem 10.8 Let X be a cadlag adapted process and there exist a sequence of
s.t.‚Äôs (œÑn) and a sequence of semimartingales Y(n) such that
(1) limn‚Üí‚àûœÑn = ‚àû(a.s.)
(2) X = Y(n) on ‚ü¶0, œÑn‚ü¶, n = 1, 2, . . .
Then X is a semimartingale.
Proof For each n = 1, 2, . . . we have
Xt‚àßœÑn = Y(n)
t‚àßœÑn ‚àíY(n)
œÑn I{œÑn ‚â§t } + XœÑn I{œÑN ‚â§t },
and therefore, XœÑn is a semimartingale. Now, if we can transform (œÑn) to an increas-
ing sequence, then we prove a semimartingale property of X according to Theorem
10.8. The trick to make it is standard. If œÑ and œÉ are s.t.‚Äôs, and XœÉ and XœÑ are semi-
martingales, then XœÉ‚àßœÑ and XœÉ‚à®œÑ = XœÉ + XœÑ ‚àíXœÉ‚àßœÑ are both semimartingales
too.
‚ñ°
Representation (10.18) for a semimartingale X may not be unique. Nevertheless,
there is a continuous martingale component of X which does not depend on such rep-
resentation.ItgivesapossibilitytodeÔ¨Åneaquadraticbracketof X thatisaveryhelpful
characteristic and tool for semimartingales. It is provided by the next theorem.
Theorem 10.9 Let X be a semimartingale with representation (10.18).
Then
(1) Mc in (10.18) does not depend on the particular representation (10.18);
(2) 
s‚â§t(ŒîXs)2 converges a.s. for all t ‚â•0.

10.2
Extension of martingale theory by localization of stochastic processes
157
Proof (1) Indeed, if X = X0 + M + A = X0 + N + B with M, N ‚ààMloc, A, B ‚ààV,
then the diÔ¨Äerence M ‚àíN ‚ààMloc ‚à©V, and according to Lemma 10.8 M ‚àíN is
purely discontinuous and (M ‚àíN)c = 0. Therefore, Mc = Nc and we denote this
component Xc and call it the continuous local martingale part of X. The proof of
(2) is provided in the same way as for local martingales.
‚ñ°
DeÔ¨Ånition 10.15 Let X be a semimartingale. Then the increasing process
[X, X]t = ‚ü®Xc, Xc‚ü©t +

s‚â§t
(ŒîXs)2, t ‚â•0,
(10.20)
is well deÔ¨Åned and is called the quadratic bracket of X. If we have another semi-
martingale Y = (Yt)t ‚â•0, we can deÔ¨Åne a joint bracket
[X,Y]t = ‚ü®Xc,Y c‚ü©t +

s‚â§t
ŒîXsŒîYs
(10.21)
with similar properties.
Example 10.2 Let X be a supermartingale, then it is a semimartingale. To show it we
use Lemma 10.9. We consider the stepped supermartingale Xn
t = Xt‚àßn, n = 1, 2, . . .
which can be represented as follows
Xn
t = E(Xn|Ft) + Xn
t ‚àíE(Xn|Ft) = E(Xn|Ft) + Zt,
where Z is a non-negative supermartingale. For arbitrary s.t. œÉ we obtain E|ZœÉ| < ‚àû.
Moreover, deÔ¨Åne s.t.‚Äôs
œÑn = inf(t : Z‚àó
t ‚â•n) ‚àßn, Z‚àó
t = sup
s‚â§t
|Zs|
and on ‚ü¶o, œÑn‚üßwe have Z‚àó
t ‚â§n + |ZœÑn | ‚ààL1, which certiÔ¨Åes that Z ‚ààD, and by the
Doob-Meyer decomposition (10.10) can be rewritten as Z = M ‚àíAwith a martingale
M and a predictable process A with Ô¨Ånite variation. Hence, Z is a semimartingale,
and X is also a semimartingale.
Example 10.3 Assume X is a cadlag process with independent increments. Then
Zt = 
s‚â§t ŒîXsI{ŒîXs |‚â•1} is the process with independent increments and with Ô¨Ånite
variation. DeÔ¨Åne Y = X ‚àíZ and observe that it has jumps bounded by one, and
again, the process with independent increments. Hence, Yt ‚àíEYt is a martingale,
and we arrive to conclusion: X is a semimartingale if and only if EYt has the Ô¨Ånite
variation.
Example 10.4 Let X be an integrable right-continuous adapted process deÔ¨Åned on
[0, ‚àû]. We call it a quasimartingale on [0, ‚àû], if
Var(X) =
sup
0‚â§t0<...<tn ‚â§‚àû
E
 n

0
E(Xti+1 ‚àíXti |Fti)


< ‚àû.

158
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
The quasimartingale X admits the Fisk decomposition X = M + Y ‚àíZ, where M is
a martingale, Y and Z are non-negative supermartingales on [0, ‚àû]. Hence, it is a
semimartingale.
Now we want to discuss the question about semimartingales and change of equiv-
alent probability measures.
Theorem 10.10 If P and Q are equivalent probability measures on a measurable
space (Œ©, F ), then each semimartingale with respect to P will be a semimartingale
w.r. to Q.
Proof is based on the following lemmas.
Lemma 10.10 Let N be a positive local martingale w.r. to Q. Then N‚àí1 is a semi-
martingale w.r. to Q.
Proof If Nt ‚â•a > 0, then N‚àí1
t
‚â§a‚àí1, and therefore EN‚àí1
t
< ‚àû, t ‚â•0. Note that
x‚àí1 is convex. By the Jensen inequality N‚àí1
t
will be a submartingale, and hence, it is
a semimartingale w.r. to Q by Example 10.2. In general case we consider a sequence
of convex functions fn(x) such that fn(x) = 1
x on [1/n, ‚àû) and [0, 1/n] the graph
of fn is the tangent to the curve y = 1/x at the point 1/n. For each t ‚â•0 we have
E fn(Nt) < ‚àû, which implies that Yn = fn(N) is a submartingale w.r. to Q for each
n = 1, 2, . . . . Taking s.t.‚Äôs œÑn = inf(t : Nt ‚â§1/n) we get a localizing sequence such
that Yn = 1/N on ‚ü¶0, œÑn‚üß. So, by Theorem 10.9 1/N is a semimartingale.
‚ñ°
Lemma 10.11 The product of semimartingales is a semimartingales.
Proof Let us prove it (it is enough) for X2. We use X = M + V, where M is a
locally bounded martingale, and V is a process with Ô¨Ånite variation, as stated in the
fundamental lemma.
Taking a localizing sequence (œÑn)n=1,2,... we get MœÑn as a bounded martingale,
VœÑn with bounded variation on ‚ü¶0, 1/n‚üß. Let us show that (MœÑn + BœÑn)2 is a semi-
martingale. The term (MœÑn)2 is a submartingale. As far as (VœÑn)2 we consider a
subdivision 0 ‚â§t0 < t1 < . . . < tn = t of [0, t] and Ô¨Ånd that

i
(VœÑn
ti+1)2 ‚àí(VœÑn
ti )2 =

i
VœÑn
ti+1 + VœÑn
ti
 VœÑn
ti+1 ‚àíVœÑn
ti

‚â§2
‚à´t
0
dVœÑn
s


i
VœÑn
ti+1 ‚àíVœÑn
ti


‚â§2
‚à´t
0
dVœÑn
s

2
.
To treat the term MœÑnVœÑn, we note that MœÑn
t
= E(M+
œÑn |Ft) ‚àíE(M‚àí
œÑn |Ft) and VœÑn
is the diÔ¨Äerence of two increasing processes. It means that we can consider only the
case of non-negative MœÑn and VœÑn. The process MœÑn
t
VœÑn
t
‚àíŒîVœÑn I{œÑn ‚â§t }
 = MœÑn
t Un
t
is a submartingale, and MœÑnVœÑn = MœÑnUn on ‚ü¶0, œÑn‚üß. The application of Theorem
10.9 completes the proof of Lemma.
‚ñ°

10.3
On stochastic calculus for semimartingales
159
Let us come back to the proof of Theorem. Denote Z‚àû= dQ
dP on (Œ©, F ) and
Ô¨Ånd that Zt = EP(Z‚àû|Ft) is a martingale w.r. to P and Z‚àûis positive P and Q-a.s.
Obviously, X is a local martingale w.r. to P ‚áîXZ‚àí1 is a local martingale w.r. to Q.
Writing X as the product X
Z Z we get the statement from Lemma 10.10 and Lemma
10.11.
Remark 10.11 We deÔ¨Åned [X, X] by (10.20). It turns out one can prove that
[X, X]t = P ‚àílimn
n
1(Xti ‚àíXti‚àí1)2, and it is invariant regarding equivalent changes
of measures. Similar fact is true also for [X,Y] (see (10.21)).
10.3
On stochastic calculus for semimartingales
We extend here Ito‚Äôs calculus to bigger classes of processes deÔ¨Åned on a standard
stochastic basis (Œ©, F, (Ft)t ‚â•0, P). A scheme of such extension will be presented
sequentially for square-intgerable martingales, local martingales and semimartin-
gales.
As we know (see, for example, Corollary 10.1 and Remark 10.7), any M ‚ààM2
is described as Mt = E(M‚àû|Ft), where M‚àû‚ààL2(Œ©, F, P). It creates a possibility to
think about M2 as a Hilbert space with the scalar product (M, N) = EM‚àûN‚àûand
the norm ||M|| = (EM2
‚àû)1/2 for M, N ‚ààM2.
Let us Ô¨Åx M ‚ààM2 and deÔ¨Åne
S2(M) =

œÜ : œÜ is predictable , E
‚à´‚àû
0
œÜ2
sd[M, M]s < ‚àû

with the norm ||œÜ||S2(M) =

E
‚à´‚àû
0 œÜ2
sd[M, M]s
	1/2
.
In this space we consider a subspace S2
step(M) of predictable processes of the
form
œÜ = œÜ‚àó
0I{0} +
n‚àí1

i=1
œÜiI(ti,ti+1,
where œÜ‚àó
0 ‚àíF0-measurable, œÜi ‚àíFti-measurable, i = 1, 2, . . ., n.
for œÜ ‚ààS2
step(M) we deÔ¨Åne the stochastic integral of œÜ w.r. to M as the process
of M2 :
(œÜ ‚ó¶M)t =
‚à´‚àû
0
I(s,t](s)œÜsdMs =
n

i=1
œÜi(Mti+1‚àßt ‚àíMti‚àßt), t ‚â•0.
(10.22)
Using relation (10.22) we can easily obtain the following properties of œÜ ‚ó¶M :
(1) Isometry: ||œÜ ‚ó¶M|| = ||œÜ||S2(M).
(2) Continuity of (œÜ ‚ó¶M)t if M is continuous.
(3) Indistinguishability of jumps Œî(œÜ ‚ó¶M)t and œÜtŒîMt, t > 0.

160
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
The subspace S2
step(M) is dense in S2(M), and as in case of stochastic integration
w.t. to a Wiener process, one can extend the deÔ¨Ånition of œÜ ‚ó¶M to the whole space
S2(M) with the properties (1)-(3) above.
Now we want to provide a more tight connection of œÜ ‚ó¶M with quadratic brack-
ets of M and œÜ ‚ó¶M. Creating such a connection the following Kunite-Watanabe
inequality is useful.
Lemma 10.12 Let M and N ‚ààM2, and œÜ and œà be bounded measurable processes.
Then
‚à´‚àû
0
|œÜs||œàs||d[M, N]s| ‚â§
‚à´‚àû
0
œÜ2
sd[M, M]s
1/2 ‚à´‚àû
0
œà2
s d[N, N]s
1/2
. (10.23)
To prove (10.23), which is an analog of the Cauchy-Schwartz inequality, we consider
for Œª ‚ààR and s ‚â§t the diÔ¨Äerence
[M + ŒªN, M + ŒªN]t ‚àí[M + ŒªN, M + ŒªN]s.
It is non-negative, and, hence,
|[M, N]t ‚àí[M, N]s|2 ‚â§([M, M]t ‚àí[M, M]s) ([N, N]t ‚àí[N, N]s) .
Take œÜ = n
i=1 œÜiI(ti,ti+1], œà = m
j=1 œàjI(tj,tj+1] with bounded r.v.‚Äôs œÜi and œàj, we
obtain from the above inequality for brackets (10.23). After this we extend (10.23)
to the class of all bounded measurable functions œÜ and œà using limit arguments, as
usual.
Theorem 10.11 For M ‚ààM2 and œÜ ‚ààS2(M) the stochastic integral œÜ ‚ó¶M is a
unique L ‚ààM2 such that for any N ‚ààM2 and any predictable bounded œà
(1) [L, N] = œÜ ‚ó¶[M, N],
(2) œà ‚ó¶[œÜ ‚ó¶M, N] = œàœÜ ‚ó¶[M, N] = [œàœÜ ‚ó¶M, N] and œà ‚ó¶(œÜ ‚ó¶M) = œàœÜ ‚ó¶M,
(3) (œÜ ‚ó¶M)c = œÜ ‚ó¶Mc, (œÜ ‚ó¶M)d = œÜ ‚ó¶Md.
In particular, for a s.t. œÑ and process œà = I‚üß0,œÑ‚üßwe have equalities
(œÜ ‚ó¶M)œÑ =
‚à´œÑ
0
œÜsdMs = (œà ‚ó¶(œÜ ‚ó¶M))‚àû= ((œàœÜ) ‚ó¶M)‚àû
=
‚à´‚àû
0
I‚üß0,œÑ‚üßœÜsdMs.
Proof Due
to the properties
of
œÜ ‚ó¶M
and
Lemma 10.12
the
mapping
œÜ ‚ÜíE

(œÜ ‚ó¶M)‚àûN‚àû‚àí
‚à´‚àû
0 œÜsd[M, N]s
	
is continuous on S2(M) for any N ‚ààM2.
Further, It = (œÜ ‚ó¶M)tNt ‚àí
‚à´t
0 œÜsd[M, N]s is an uniformly integrable process,
and for any s.t. œÑ EIœÑ = 0. Hence, by Lemma 9.4 the process (It) is a martingale,
and we arrive to the statement (1). As far as the uniqueness, assume L and L‚Ä≤ ‚àà
M2 and [L, N] = œÜ ‚ó¶[M, N] for any N ‚ààM. Then we have [L ‚àíL‚Ä≤, N] = 0 and

10.3
On stochastic calculus for semimartingales
161
[L ‚àíL‚Ä≤, L ‚àíL‚Ä≤] = 0. Hence, (L ‚àíL‚Ä≤)2 is a martingale and we obtain equality L = L‚Ä≤.
Other statements (2)-(3) are almost obvious.
‚ñ°
Now we show the way of construction of stochastic integral for M ‚ààMloc and
a locally bounded predictable process œÜ, i.e. œÜœÑn I(œÑn>0) is bounded for a localizing
sequence of s.t.‚Äôs (œÑn)n=1,2,....
To deÔ¨Åne such a stochastic integral œÜ ‚ó¶M we use the fundamental lemma, when
M = U + V with U ‚ààM2
loc, V is a local martingale with bounded variation. Then
for a localizing sequence (œÑn)n=1,2,... for œÜ, U,V we deÔ¨Åne
(œÜ ‚ó¶M)t =
‚à´t
0
œÜœÑn
s dUœÑn
s
+
‚à´t
0
œÜœÑn
s dVœÑn
s
on ‚ü¶0, œÑn‚üß. To continue the construction we need the following lemma.
Lemma 10.13 (1) If V is a martingale with integrable variation and œÜ is predictable
with E
‚à´‚àû
0
|œÜs|dVs| < ‚àû, then the Lebesgue-Stieltjes integral
‚à´t
0 œÜsdVs ‚ààM.
(2) if V ‚ààM2 and œÜ is bounded, then the Stochastic integral (œÜ ‚ó¶V)t and the
Lebesgue-Stieltjes integral
‚à´t
0 œÜsdVs are indistinguishable.
Proof We prove statement (1) only because second statement can be proved
in a similar manner. DeÔ¨Åne S‚Ä≤(V) the set of predictable processes œÜ such that
E
‚à´‚àû
0
|œÜs|dVs| < ‚àû. Then ||œÜ||S‚Ä≤(V) = E
‚à´‚àû
0
|œÜs||dVs| deÔ¨Ånes the norm of this space.
If ||œÜn ‚àíœÜ||S‚Ä≤(V) ‚Üí0, n ‚Üí‚àû, then
E

sup
t

‚à´t
0
œÜn
s dVs ‚àí
‚à´t
0
œÜsdVs


‚â§E
‚à´‚àû
0
|œÜn
s ‚àíœÜs||dVs|
‚â§||œÜn ‚àíœÜ||S‚Ä≤(V) ‚Üí0, n ‚Üí‚àû.
Using these Ô¨Åndings we make a transition from step-predictable functions to all
functions of space S‚Ä≤(V).
‚ñ°
Coming back to the deÔ¨Ånition of œÜ ‚ó¶M by relation (10.3) one can concluded with
the help of Lemma 10.13 that œÜ ‚ó¶M ‚ààMloc and its deÔ¨Ånition does not depend on
the decomposition M = U + V. Moreover, we arrive to statements which are similar
to (1)-(3) of Theorem 10.12 if we replace class M2 by Mloc.
The Ô¨Ånal step of construction of stochastic integral is the case of semimartingale.
Assume œÜ is a predictable locally bounded process and X is a semimartingale of the
form
X = X0 + M + A,
(10.24)
where X0 is a Ô¨Ånite F0-measurable r.v., M ‚ààMloc, A ‚ààV.
Let us deÔ¨Åne
(œÜ ‚ó¶X)t = (œÜ ‚ó¶M)t +
‚à´t
0
œÜsdAs,
(10.25)
where
‚à´t
0 œÜsdAs is the Lebesgue-Stieltjes integral.

162
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
According to Lemma 10.13 the deÔ¨Ånition (10.25) of (œÜ ‚ó¶X) does not depend on
the decomposition (10.24). As a result, let us formulate the list of natural properties
of œÜ ‚ó¶X :
(1) œÜ ‚ó¶X is a semimartingale,
(2) (œÜ ‚ó¶X)c = œÜ ‚ó¶Xc,
(3) jumps Œî(œÜ ‚ó¶X)t and œÜtŒîXt are indistinguishable,
(4) (œÜ ‚ó¶X)œÑ =
‚à´‚àû
0
I‚üß0,œÑ‚üßœÜsdXs =
‚à´œÑ
0 œÜsdXs for any Ô¨Ånite s.t. œÑ.
Remark 10.12 It is interesting to note that there is no hope to go beyond semimartin-
gales in extension of stochastic integration.
Let X1, . . ., Xd be d semimartingales taking the values in R. It is convenient
to collect them to a d-dimensional process Xt = (X1
t , . . ., Xd
t ), called a d-valued
semimartingale.
Let F : E = Rd ‚ÜíR, d ‚â•1, be a twice continuously diÔ¨Äerentiable function.
Then one can transform X with the help of this smooth function. It turns out the
transformed process F(Xt) will be a semimartingale satisfying the next generalized
Ito‚Äôs formula:
F(Xt) = F(X0) +
d

i=1
‚à´t
0
DiF(Xs‚àí)dXi,c
s
+1
2

1‚â§i,j,‚â§d
‚à´t
0
DiDjF(Xs‚àí)d[Xi,c, X j,c]s
+

s‚â§t

F(Xs) ‚àíF(Xs‚àí) ‚àí
d

i=1
DiF(Xs‚àí)ŒîXi
s

,
(10.26)
where DiF(x) = ‚àÇF(x)
‚àÇi
= ‚àÇF(x1,...,xd)
‚àÇxi
, DiDjF(x) = ‚àÇ2F(x)
‚àÇxi‚àÇx j , i, j = 1, . . ., d.
Remark 10.13 Let us note that processes DiF(Xs‚àí) and DiDjF(Xs‚àí) are predictable
and locally bounded, and therefore all integrals in (10.26) are well-deÔ¨Åned.
DeÔ¨Åning the localizing sequence œÑn = inf{t : |Xt| ‚â•n}, we obtain that |Xt| and
|Xt‚àí‚â§n on ‚ü¶o, œÑn‚ü¶, n = 1, 2, . . . Further, letting Kn = sup|x |‚â§n

i,j |DiDjF(x)|,
weget with the help of the Taylor decomposition that
|

s<œÑn
(F(Xs) ‚àíF(Xs‚àí) ‚àí
d

i=1
DiF(Xs‚àí)ŒîXi
s)| ‚â§Kn
2

i

sœÑn
|ŒîXi
s|2.
Hence, for almost all œâ both series

s<œÑn
|F(Xs) ‚àíF(Xs‚àí) ‚àí
d

i=1
DiF(Xs‚àí)ŒîXi
s)|
and

10.3
On stochastic calculus for semimartingales
163

s‚â§œÑn
|F(Xs) ‚àíF(Xs‚àí) ‚àí
d

i=1
DiF(Xs‚àí)ŒîXi
s)|
converge, and we can conclude that

s‚â§t
|F(Xs) ‚àíF(Xs‚àí) ‚àí
d

i=1
DiF(Xs‚àí)ŒîXi
s)|
converges for all t > 0.
This observation generates the idea that the formula (10.26) can be proved without
big technical diÔ¨Éculties in between moments of jumps, i.e. in a pure continuous case.
This is the reason that we prove (10.26) for a one-dimensional continuous semi-
martingale Xt = X0 + Mt + At, M ‚ààMc
loc, A ‚ààV. Using a localization procedure
we can reduce the proof to the case when continuous processes M, A, [M, M] are
bounded as well as X0, F, F‚Ä≤ and F‚Ä≤‚Ä≤, and the upper bound for |F|, |F‚Ä≤|, |F‚Ä≤‚Ä≤| is
denoted by C.
Let us use the Taylor decomposition
F(y) ‚àíF(x) = (y ‚àíx)F‚Ä≤(x) + 1
2(y ‚àíx)2F‚Ä≤‚Ä≤(x) + o(x, y)
where o(x, y) ‚â§œµ(|y ‚àíx|)|y ‚àíx|2 and œµ
is a non-decreasing function with
limt‚Üí0 œµ(t) = 0.
Take a > 0 and deÔ¨Åne a sequence of s.t.‚Äôs (œÑn)n=0,1,... as follows
œÑ0 = 0, . . ., œÑi+1 = t ‚àß(œÑi + a) ‚àßinf{s > œÑi : Ms ‚àíMœÑi > a,
[M, M]s ‚àí[M, M]œÑi > a or |As ‚àíAœÑi | > a}, . . .
Obviously, for each œâ : œÑi(œâ) = t except for a Ô¨Ånite number of i, and we can write
the following equality using the Taylor formula above:
F(Xt) ‚àíF(X0) =

i
F(XœÑi+1 ‚àíF(XœÑi )
=

i

F‚Ä≤(XœÑi )(XœÑi+1 ‚àíXœÑi ) + 1
2 F‚Ä≤‚Ä≤(XœÑi+1)(XœÑi+1 ‚àíXœÑi )2 + o(XœÑi, XœÑi+1

.
(10.27)
For the martingale part of (10.27) we have

164
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
E
‚à´t
0
F‚Ä≤(Xs)dMs ‚àí

i
F‚Ä≤(XœÑi)(MœÑi+1 ‚àíMœÑi)
2
=E

i
‚à´œÑi+1
œÑi
F‚Ä≤(Xs) ‚àíF‚Ä≤(XœÑi)2 d[M, M]s

‚â§E

sup
i
|F‚Ä≤(Xs) ‚àíF‚Ä≤(XœÑi |2 ¬∑ [M, M]t

‚Üí0
(10.28)
as a ‚Üí0.
The term of (10.27) with process A can be treated similarly, and 
i F‚Ä≤(XœÑi)(AœÑi ‚àí
AœÑi) converges to
‚à´t
0 F‚Ä≤(Xs)dAs in L1-sense.
The second term in (10.27) we represent as a sum of three components.
For the Ô¨Årst component we have
|

i
F‚Ä≤‚Ä≤(XœÑi)(AœÑi+1 ‚àíAœÑi)2| ‚â§C sup
i
|AœÑi+1 ‚àíAœÑi |
‚à´t
0
|dAs|
‚â§Ca
‚à´t
0
|dAs| ‚Üí0, a ‚Üí.
(10.29)
for the second component we obtain
|

i
F‚Ä≤‚Ä≤(XœÑi)(AœÑi+1) ‚àíAœÑi)(MœÑi+1 ‚àíMœÑi)| ‚â§C sup
i
|MœÑi+1 ‚àíMœÑi |
‚à´t
0
|dAs|
‚â§Ca
‚à´t
0
|dAs| ‚Üí0, a ‚Üí0.
(10.30)
Taking into account that M2 ‚àí[M, M] is a martingale, we get for the third component
that
E
‚é°‚é¢‚é¢‚é¢‚é¢‚é£

i
F‚Ä≤‚Ä≤(XœÑi)(MœÑi+1 ‚àíMœÑi)2 ‚àí

i
F‚Ä≤‚Ä≤(XœÑi)([M, M]œÑi+1 ‚àí[M, M]œÑi)
2‚é§‚é•‚é•‚é•‚é•‚é¶
=

i
E

(F‚Ä≤‚Ä≤(XœÑi))2 
(MœÑi+1 ‚àíMœÑi)2 ‚àí([M, M]œÑi+1 ‚àí[M, M]œÑi)
	2
‚â§2C2E

sup
i
|MœÑi+1 ‚àíMœÑi |2M2
t

+ 2C2E

sup
i
|[M, M]œÑi+1 ‚àí[M, M]œÑi | ¬∑ [M, M]t

‚â§2C2a2EM2
t + 2C2aE[M, M]t ‚Üí0, a ‚Üí0.
(10.31)
The remaining term in (10.27) can be treated as follows

10.3
On stochastic calculus for semimartingales
165
E

i
o(XœÑi, XœÑi+1) ‚â§E

i
(XœÑi+1 ‚àíXœÑi)2œµ(|XœÑi+1 ‚àíXœÑi |)

‚â§E

2œµ(2a)

i
(AœÑi+1 ‚àíAœÑi)2 + 2œµ(2a)

i
(MœÑi+1 ‚àíMœÑi)2

‚â§2œµ(2a)E

a
‚à´t
0
|dAs| + M2
t

‚Üí0, a ‚Üí0.
(10.32)
Putting together relations (10.28)-(10.32) we complete the proof.
Now we are ready to consider stochastic diÔ¨Äerential equations with respect to
semimartingale. The main result is contained in the next theorem.
Theorem 10.12 Let N = M + A be a semimartingale, M ‚ààMloc, A ‚ààV. Assume
function F : R+ √ó Œ© √ó R ‚ÜíR satisÔ¨Åes the conditions
(1) f (s, œâ, ¬∑) is a Lipschitz function with a constant K;
(2) f (s, ¬∑, x) is Fs-measurable;
(3) f (¬∑, œâ, ¬∑) is a continuous function.
Then the SDE w.r. to N
Xt(œâ) = X0(œâ) +
‚à´t
0
f (s, œâ, Xs‚àí(œâ))dNs(œâ)
(10.33)
has a unique solution as a cadlag adapted process with the initial value X0(œâ) ‚àíF0-
measurable Ô¨Ånite random variable.
Proof Obviously, (1)-(3) guarantee that the integral in the right-hand side of (10.33)
is well-deÔ¨Åned. The proof includes a couple of steps. We start with the assumption
that processes
[M, M]t and
‚à´t
0
|dAs| are bounded by b > 0
(10.34)
and
| f (t, œâ, 0)| ‚â§c for all t, œâ.
(10.35)
Let H be a class of cadlag processes X such that X‚àó= supt |Xt| ‚ààL2, X0 = 0.
DeÔ¨Åne the norm in this space ||X|| = ||X‚àó||L2.
Denote for x ‚ààH
U(X)t =
‚à´t
0
f (s, œâ, Xs‚àí)dNs.
Lemma 10.14 The process U(x) ‚ààH if X ‚ààH. Moreover, for processes X and
Y ‚ààH we have
||U(X) ‚àíU(Y)|| ‚â§h||X ‚àíY||, h = K(2
‚àö
b + b).
(10.36)

166
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Proof For zero-process we have
U(0)t =
‚à´t
0
f (s, œâ, 0)dMs +
‚à´t
0
f (s, œâ, 0)dAs = Lt + Vt.
Process L ‚ààMloc, and due to (10.34)-(10.35)
[L, L]‚àû=
‚à´‚àû
0
f 2(s, œâ, 0)d[M, M]s ‚â§c2b,
(10.37)
and E(L‚àó)2 ‚â§4c2b by Doob‚Äôs inequality.
‚ñ°
For process Vt we get from (10.34)-(10.35) that
V‚àó‚â§
‚à´‚àû
0
|dVs| ‚â§
‚à´‚àû
0
| f (s, œâ, 0)||dAs| ‚â§cb
(10.38)
and hence E(V‚àó)2 ‚â§c2b2. Relations (10.37)-(10.38) show that U(X) ‚ààH. Further,
denote Z = X ‚àíY and Ô¨Ånd that
U(X)t ‚àíU(Y)t =
‚à´t
0
( f (Xs‚àí) ‚àíf (Ys‚àí))dMs +
‚à´t
0
( f (Xs‚àí) ‚àíf (Ys‚àí))dAs
=L‚Ä≤
t + V ‚Ä≤
t , L‚Ä≤ ‚ààMloc, V ‚Ä≤ ‚ààV.
Using similar reasonings as in (10.37)-(10.38) we obtain for processes L‚Ä≤ and V ‚Ä≤
that
‚ñ°
[L‚Ä≤, L‚Ä≤]‚àû=
‚à´‚àû
0
( f (Xs‚àí‚àíf (Ys‚àí))2d[M, M]s ‚â§K2b(Z‚àó)2,
(10.39)
E((L‚Ä≤)‚àó)2 ‚â§4K2bE(Z‚àó)2 and
V‚àó‚â§
‚à´‚àû
0
K|Zs||dAs| ‚â§KbZ‚àó.
(10.40)
It follows from (10.39)-(10.40) that
||U(X) ‚àíU(Y)|| ‚â§K||X ‚àíY||(b + 2
‚àö
b),
and we get (10.36).
Lemma 10.15 Assume conditions (10.34)-(10.35) are satisÔ¨Åed, and h = K(b +
2
‚àö
b) < 1. Then there exists a unique cadlag adapted process X solving the equation
Xt =
‚à´t
0
f (s, œâ, Xs‚àí)dNs.
Proof Due to h < 1 there exists a unique solution X from space H. Let us note from
conditions (1)-(2) of the theorem that
|ŒîNs| ‚â§|ŒîMs| + |ŒîAs| = (Œî[M, M]s)1/2 + |ŒîAs| ‚â§
‚àö
2b + b.

10.3
On stochastic calculus for semimartingales
167
So, if Z is a cadlag adapted process, Zt =
‚à´t
0 f (s, œâ, Zs‚àí)dNs, we can deÔ¨Åne stopping
times œÑZ
n = inf(t : |Zt| ‚â•n), n = 1, 2, . . ., and get
ŒîZœÑZ
n = f (œÑZ
n , ZœÑz
n‚àí)ŒîNœÑZ
n and
|ŒîZœÑZ
œÑn | ‚â§(c + nK)(2b +
‚àö
2b).
Hence, the process Z is locally bounded and locally in space H. Due to uniqueness
in H we obtain Z = X.
‚ñ°
Lemma 10.16 Assume that condition (10.34) is fulÔ¨Ålled and h = K(b + 2
‚àö
b) < 1,
then there exists a unique cadlag adapted process X solving equation
Xt =
‚à´t
0
f (s, œâ, Xs‚àí)dNs.
Proof Let
œÑn = inf(t : | f (t, œâ, 0)| ‚â•n),
n = 1, 2, . . .
and
fn(t, œâ, x) =
f (t, œâ, x)I{0<t ‚â§œÑn }. Obviously, functions fn satisfy (10.35) with c = n, and each
equationY n
t =
‚à´t
0 fn(s, œâ,Y n
s‚àí)dNs has a unique solution. Due to fn+1 = fn on ‚üß0, œÑn‚üß
we have Y n = Y n+1 on ‚ü¶0, œÑn‚üß.
Further, an adapted process X solves the equation Xt =
‚à´t
0 f (s, œâ, Xs‚àí)dNs ‚áî
for each œÑn we have XœÑn
t
=
‚à´t
0 f (s, œâ, XœÑn
s‚àíNœÑn
s . Let us deÔ¨Åne now the process X as
follows X = Y n on ‚ü¶0, œÑn‚üß, X0 = 0, and Ô¨Ånd the unique solution.
‚ñ°
Lemma 10.17 Assume a semimartingale N has jumps |ŒîNt| ‚â§b/4 and both b, h =
K(b + 2
‚àö
b) < 1. Then the equation Xt =
‚à´t
0 f (s, œâ, Xs‚àí)dNs has one and only one
solution.
Proof Under the conditions of Lemma we can write N = M + A, M ‚ààMloc and A
is a predictable process with locally bounded variation. Without loss of generality
we can assume that M ‚ààM and predictable process A has integrable variation. Then
|ŒîAœÑ| = |E(ŒîNœÑ|FœÑ‚àí) ‚àíE(ŒîMœÑ|FœÑ‚àí)| = |EŒîNœÑ|FœÑ‚àí)| ‚â§b/4
and we have a boundedness of Œî[M, M]t = (ŒîMT)2 and |ŒîAt| by b/4. Denote
Dt =
‚à´t
0 |dAs| and put œÑ0 = 0, . . ., œÑn = inf(t > œÑn‚àí1 : Dt ‚àíDœÑn‚àí1 ‚â•b/2). We have
‚à´
‚üßœÑn,œÑn+1‚üßd[M, M]s ‚â§b and
‚à´
‚üßœÑn,œÑn+1‚üß|dAs| ‚â§b for all n = 1, 2, . . .
By Lemma 9.16 there is one and only one solution on ‚ü¶0, œÑ1‚üß, and denote it by
X1. At the interval ‚üßœÑ1, œÑ2‚üßwe have equation
Xt = X1
œÑ1 +
‚à´
‚üßœÑ1,t‚üß
f (s, œâ, Xs‚àí)dNs.
Obviously, X solves (86) ‚áîY = X ‚àíX1
œÑ1 is a solution of the equation
Yt =
‚à´t
0
f (s, œâ,Ys‚àí+ X1
œÑ1)dNs.

168
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
So, we get a unique solution X2 at this interval and so on.
‚ñ°
Let us come back to the proof of the theorem. We assume that b as in Lemma
10.17 and we have a non-decreasing sequence of s.t.‚Äôs œÑ1 ‚â§œÑ2 ‚â§. . . ‚â§œÑn ‚â§. . . at
which ŒîNt| > b/4. Construct the following semimartingale
N1
t = MtI(t<œÑ1) + NœÑ1‚àíI(t ‚â•œÑ1).
A cadlag process X is a solution of (10.33) on ‚ü¶0, œÑ1‚üß‚áîthe process X1
t = XtI(t<œÑ1) +
XœÑ1‚àíI(t ‚â•œÑ1) is a solution on ‚ü¶0, œÑ1‚üßof the equation X1
t = X0
‚à´t
0 f (s, œâ, X1
s‚àí)dN1
s . On
the interval ‚ü¶0, œÑ1‚üßthe process N1 satisÔ¨Åes the conditions of Lemma 10.17, and
hence the last SDE has a unique cadlag adapted solution X1 on ‚ü¶0, œÑ1‚üß. We can
continue this construction on ‚ü¶œÑ1, œÑ2‚üßand so on.
Example 10.5 Let us consider the linear SDE:
Zt = 1 +
‚à´t
0
Zs‚àídNs.
(10.41)
We know from Theorem 10.12 that a unique solution of the equation (10.41) exists.
It turns out it admits a special form which is similar to the Girsanov exponent:
Zt = exp{Nt ‚àí1
2[Nc, Nc]t}
 
s‚â§t
(1 + ŒîNs)e‚àíŒîNs .
(10.42)
The form of solution in (10.42) is called a stochastic exponent or the Doleans
exponent, denoted by Et(N). To prove this, we denoteYt = Nt ‚àí1
2[Nc, Nc]t and Xt =
!
s‚â§t(1 + ŒîNs)e‚àíŒîNs, and apply Ito‚Äôs formula to the function F(Yt, Xt) = exp(Yt)Xt.
Problem 10.6 Prove the properties of stochastic exponents w.r. to semimartingales:
(1) Let Et(N) be a stochastic exponent w.r. to a semimartingale N with ŒîNt 
‚àí1,
t ‚â•0. Then
1
Et(N) = Et(‚àíN‚àó), where N‚àó
t = Nt ‚àí[Nc, Nc]t ‚àí
s‚â§t
(ŒîNs)2
1+ŒîNs .
stochastic exponent w.r. to a semimartingale N with ŒîNt  ‚àí1, t ‚â•0. Then
1
Et(N) = Et(‚àíN‚àó), where N‚àó
t = Nt ‚àí[Nc, Nc]t ‚àí
s‚â§t
(ŒîNs)2
1+ŒîNs .
(2) If N inMloc, then Et(N) ‚ààMloc.
(3) Et(N) = 0 for (t, œâ) ‚àà‚ü¶œÑ, ‚àû‚ü¶, where œÑ = inf(t : ŒîNt = ‚àí1).
(4) Et(U)Et(V) = Et(U + V + [U,V]), where U and V are semimartingale. This
property is called the rule of multiplication of stochastic exponents.
Remark 10.14 The equation (10.33) admits further generalizations. One can prove
an analog of Theorem 9.13 for equation
dXt(œâ) = b(t, œâ, Xt‚àídAt + œÉ(t, œâ, Xt‚àí)dMt,
(10.43)
where M ‚ààMloc, A ‚ààV, both coeÔ¨Écients b and œÉ are Lipschitz‚Äôs functions, and
b(t, œâ, x) is optional and œÉ(t, œâ, x) is predictable as functions of (t, œâ).

10.4
The Doob-Meyer decomposition: proof and related remarks
169
Let us note that the equation 10.43 can be also considered for a multidimensional
semimartingale X = A + M, and for matrix-valued coeÔ¨Écients b and œÉ.
Remark 10.15 The equation (10.33) admits further important results. One can prove
an analog of comparison Lemma 9.1 for stochastic diÔ¨Äerential equations (10.43) with
a predictable increasing process A = (At)t ‚â•0.
Moreover the Ito‚Äôs formula (10.26) admits a further generalization to a class of convex
functions F. The corresponding formula is similar to (10.26) where the second term
in the right hand side is replaced by a continuous process V from V. The process V
admits a representation with the help of a local time of X.
10.4
The Doob-Meyer decomposition: proof and related
remarks
The leading idea of this short book as it was stated in preface dictates the way of
exposition avoiding too long proofs and too many technical details. That is why
the proof of the Doob-Meyer decomposition was omitted in Section 10.1. However,
the proof of this fundamental fact of stochastic analysis must be presented in an
appropriate manner.
It is necessary to note that the initial Doob-Meyer decomposition in continuous time
was given by Meyer in the form
Xt = X0 + Mt + At,
t ‚â•0,
(10.44)
where X = (Xt)t ‚â•0 is a submartingale of class (D), M = (Mt)t ‚â•0, M0 = 0, is a
martingale, A = (At)t ‚â•0, A0 = 0, is a ‚Äúnatural‚Äù increasing process.
The notion ‚Äúnatural‚Äù means that for every bounded cadlag martingale m = (mt)m‚â•0
on a standard stochastic basis (Œ©, F, (Ft)t ‚â•0, P) on the following equality is satisÔ¨Åed
E
‚à´t
0
msdAs = E
‚à´t
0
ms‚àídAs.
(10.45)
This characterization of A looks very artiÔ¨Åcial in comparison with discrete time,
where A is predictable. Nevertheless, since that time there were done several proofs
of the Doob-Meyer decomposition in these terms, including the proof of Rao who
used the limiting arguments for transition from discrete time to continuous time
decomposition. Doleans was the Ô¨Årst who noted that the notions ‚Äúnatural‚Äù and
‚Äúpredictable‚Äù are equivalent in (10.44).
This section is devoted to the proof of (10.44) in such a new fashion, and as in
many other books we work with this form only. Nevertheless, before providing the
proof which is based on the paper [2], we want to compare notions ‚Äúnatural‚Äù and
‚Äúpredictable‚Äù.
First of all, one can show that Emt At = E
‚à´t
0 msdAs and therefore, the condition
(10.45) is equivalent to the next one

170
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Emt At = E
‚à´t
0
ms‚àídAs.
(10.46)
Problem 10.7 Prove (for discrete time) that an increasing process A is natural ‚áê‚áí
A is predictable, i.e. An-Fn‚àí1-measurable, n ‚â•1.
Hint: We rewrite (10.46) for the discrete time case as follows
EmnAn = E
n

1
mk‚àí1ŒîAk.
It follows from this condition that Emn(An ‚àíE(An|Fn‚àí1)) = 0 and even E|An ‚àí
E(An|Fn‚àí1) = 0 which leads to predictability of A.
The opposite implication follows from the equality E n
1 AkŒîmk = 0 and the repre-
sentation
n

1
AkŒîmk = Anmn ‚àímn‚àí1ŒîAn ‚àímn‚àí2ŒîAn‚àí1 ‚àí...
which leads to EAnmn = E n
1 mk‚àí1ŒîAk.
Let us show that the decomposition (10.44) is unique in the class of natural processes
A. To prove it we assume that there exist two such decompositions for X with
martingales M‚Ä≤, M‚Ä≤‚Ä≤, and with the natural increasing processes A‚Ä≤, and A‚Ä≤‚Ä≤. Hence,
M‚Ä≤
t + A‚Ä≤
t = M‚Ä≤‚Ä≤
t + A‚Ä≤‚Ä≤
t for all t ‚â•0. DeÔ¨Åne process Nt = A‚Ä≤
t ‚àíA‚Ä≤‚Ä≤
t = M‚Ä≤
t ‚àíM‚Ä≤‚Ä≤
t , which
is a martingale with Ô¨Ånite variation.
Further, for every bounded martingale m = (mt)t ‚â•0, we have due to (10.46) that
Emt.(A‚Ä≤
t ‚àíA‚Ä≤‚Ä≤
t ) = E
‚à´t
0
ms‚àídNs = lim
n‚Üí‚àûE
kn

j
mt(n)
j‚àí1(Nt(n)
j
‚àíNt(n)
j‚àí1),
where (t(n)
k )k=0,...,kn is a sequence of subdivision of (0,T] with diameter Œît(n)
k
‚Üí
0, n ‚Üí‚àû. Obviously, Emt(n)
j‚àí1ŒîNt(n)
j
= 0, and, hence, Emt.(A‚Ä≤
t ‚àíA‚Ä≤‚Ä≤
t ) = 0, t ‚â•0. To
Ô¨Ånish the proof we take an arbitrary integrable random variable Œ∂ and deÔ¨Åne a
martingale Œ∂t = E(Œ∂|Ft). Then EŒ∂.(A‚Ä≤
t ‚àíA‚Ä≤‚Ä≤
t ) = 0 for every such random variable
which leads to conclusion that A‚Ä≤
t = A‚Ä≤‚Ä≤
t (a.s) for all t.
Let us prove the Doob-Meyer decomposition (10.44) assuming for simplicity that
t ‚àà[0, 1].
Denote D = {D1, D2, ..., Dn, ...}, where n = 1, 2, ..., Dn = {0, 1
2n, 2
2n, ..., j
2j , ...}, the
dyadic numbers of the interval [0, 1].
Consider the process X = (Xt)t ‚â•0 on this discrete time, and write the Doob-Meyer
decomposition for t ‚ààDn:
Xt = Mn
t + An
t ,
(10.47)
An
t ‚àíAn
t‚àí1
2n = E(Xt ‚àíXt‚àí1
2n |Ft‚àí1
2n ),
Mn
t = Xt ‚àíAn
t .

10.4
The Doob-Meyer decomposition: proof and related remarks
171
Here Mn = (Mn
t )t ‚ààDn is a martingale, An = (An
t )t ‚ààDn is increasing and predictable
with respect to (Ft)t ‚ààDn.
The idea is to take a limit in (10.47), and as a result, to obtain (10.44).
To make such limiting transition carefully we need the following lemma.
Lemma 10.18 Assume ( fn)n‚â•1 is a uniformly integrable sequence of random vari-
ables on the probability space (Œ©, F, P). Then there exists gn ‚ààconv{ fn, fn+1, ...}
which converges in space L1(Œ©, F, P).
We will use here a version of the Komlos Lemma:
If ( fn)n‚â•1 is a bounded sequence on a Hilbert space, then for each n one can pick
gn ‚ààconv{ fn, fn+1, ...} such that (gn)n‚â•1 will converge in the norm ‚à•.‚à•2 of this
space.
To Prove this lemma, we deÔ¨Åne the Ô¨Ånite number
a = sup
n
inf{‚à•g‚à•2 : g ‚ààconv{ fn, fn+1, ...}}
For each n we can take some gn ‚ààconv{ fn, fn+1, ...} with the norm ‚à•gn‚à•2 ‚â§a + 1
n.
For a Ô¨Åxed Œµ > 0, and a big enough n we obtain for all m, k ‚â•n that
‚à•(gk + gm)/2‚à•2 > a ‚àíŒµ.
We get that (gn) is fundamental due to
‚à•(gk ‚àígm)‚à•2
2 = 2‚à•gk‚à•2
2 + 2‚à•gm‚à•2
2 ‚àí‚à•(gk + gm)‚à•2
2 ‚â§4(a + 1
n)2 ‚àí4(a ‚àíŒµ)2,
and therefore this sequence converges.
Proof of lemma 10.18: We truncate fn and denote truncated functions by f (i)
n
=
fn.I{|fn |‚â§i}, n = 1, 2, .... Then for every i we get a bounded sequence ( f (i)
n )n‚â•1 for
which the Komlos lemma can be applied in the following manner.
Problem 10.8 For each n = 1, 2, ... there exist convex weights (Œªn
n, ..., Œªn
Nn) such
that Nn
j=n Œªn
j f (i)
j
converge on space L2(Œ©) for each i = 1, 2, ....
Hint: Use the Komlos lemma to Ô¨Ånd convex weights Œªn
n, ..., Œªn
Nn with the convergent
sum Nn
j=n Œªn
j f (i)
j
as n ‚Üí‚àûfor i = 1, 2, ..., m and apply a diagonalization procedure
to get a desirable result.
Now using uniform integrability of ( fn) we obtain that f (i)
n converges uniformly in n
to fn in L1(Œ©) as i ‚Üí‚àû, and therefore uniformly in n:
Nn

j=n
Œªn
j f (i)
j
‚Üí
Nn

j=n
Œªn
j fj
in
L1(Œ©)
as
i ‚Üí‚àû.
Lemma 10.19 The sequence (Mn
1 )n‚â•1 is uniformly integrable.

172
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
Proof Without loss of generality we can put X1 = 0 and Xt ‚â§0 for t ‚â§1, because
there is an obvious transition from Xt to Xt ‚àíE(X1|Ft). In such a case Mn
1 = ‚àíAn
1
and
Xn
œÑ = ‚àíE(An
1 |FœÑ) + An
œÑ
(10.48)
for every s.t. œÑ w.r. to (Ft)t ‚ààDn.
Let us derive from assumption X ‚àà(D) that (An
1)n‚â•1 is uniformly integrable. DeÔ¨Åne
the following s.t.
œÑn(c) = inf{(j ‚àí1)/2n : An
j
2n > c} ‚àß1,
c > 0,
and note
{œÑn(c) < 1} ‚äÜ{œÑn(c
2) < 1},
An
œÑn(c) ‚â§c.
It follows from (10.48) that
XœÑn(c) ‚â§‚àíE(An
1 |FœÑn(c)) + c.
Thus,
EAn
1.I{An
1 >c} =EE(An
1 |FœÑn(c)).I{œÑn(c)<1}
‚â§c.P(œÑn(c) < 1) ‚àíEXœÑn(c).I{œÑn(c)<1}
and by (10.48)
E(‚àíXœÑn( c
2 )).I{œÑn( c
2 )<1} =E(An
1 ‚àíAn
œÑn( c
2 )).I{œÑn( c
2 )<1}
‚â•E(An
1 ‚àíAn
œÑn( c
2 )).I{œÑn(c)<1}
‚â•c
2P{œÑn(c) < 1}.
These inequalities imply that
EAn
1.I{An
1 >c} ‚â§‚àí2EXn
œÑn( c
2 ).I{œÑn(c)<1} ‚àíEXn
œÑn(c).I{œÑn(c)<1}.
(10.49)
We also note
P{œÑn(c) < 1} = P{An
1 > c} ‚â§1
c EAn
1 = ‚àí1
c E(Mn
1 ) = ‚àí1
c EX0,
and conclude that
lim
c‚Üí‚àûP(œÑn(c) < 1) = 0
uniformly in n.
Assumption X ‚àà(D) together with (10.49) imply that (An
1)n‚â•1 is uniformly inte-
grable as well as (Mn
1 )n‚â•1 = (X1 ‚àíAn
1)n‚â•1.
Now we are ready for a limiting transition as n ‚Üí‚àûin (10.47).
First we extend Mn to the whole interval [0, 1] by setting Mn
t = E(Mn
1 |Ft). Accord-
ing to Lemma 10.18 and Lemma 10.19 there exist M ‚ààL1(Œ©) and convex weights

10.4
The Doob-Meyer decomposition: proof and related remarks
173
(Œªn
j )j=n,...,Nn such that Mn
1 (Œª) = Nn
j=n Œªn
j M j
1 converges to M on space L1(Œ©). More-
over, by Jensen‚Äôs inequality for each t ‚àà[0, 1]:
Mn
t (Œª) ‚ÜíMt = E(M1|Ft)
as
n ‚Üí‚àû.
Secondly, we extend An to [0, 1] by setting An = 
t ‚ààDn An
t .I(t‚àí‚àí1
2n ,t], t ‚àà[0, 1],n =
1, 2, ..., and denote An(Œª) = Nn
j=n Œªn
j Aj.
Then the cadlag process
At = Xt ‚àíMt,
where
t ‚àà[0, 1],
satisÔ¨Åes the following relations
An
t (Œª) = (Xt ‚àíMn
t (Œª)) ‚Üí(Xt ‚àíMt) = At
in
L1(Œ©)
for
t ‚ààD, n ‚Üí‚àû.
So, we Ô¨Ånd a subsequence Ank
t (Œª) ‚ÜíAt (a.s.) which we identify with An
t (Œª) ‚ÜíAt
(a.s.). These standard considerations lead us to conclude that A is (a.s.) increasing
on D, and therefore on the whole interval [0, 1] due to its right-continuity.
‚ñ°
Lemma 10.20 Process A is predictable.
Proof: In fact we can restrict ourself by the proof that for all t ‚àà[0, 1] (a.s.)
lim
n sup An
t (Œª) = At,
(10.50)
because the process An and An(Œª) are left-continuous and adapted, and hence they
are predictable.
Problem 10.9 Let fn and f be increasing functions form [0, 1] to R1 such that f is
right-continuous and f (t) = limn‚Üí‚àûfn(t), t ‚ààD. Prove that
lim
n sup fn(t) ‚â§f (t)
for all
t ‚àà[0, 1],
(10.51)
lim
n fn(t) = f (t),
if f is continuous at
t ‚àà[0, 1].
(10.52)
Hint: By the right-continuity (tk ‚ààD, tk > t):
f (t) = lim
k‚Üí‚àûf (tk) = lim
k‚Üí‚àû( lim
n‚Üí‚àûfn(tk)) ‚â•lim
n sup fn(t),
since limn sup fn(tk) ‚â•limn sup fn(t).
Due to (10.51) and (10.52) the relation (10.50) can be broken at discontinuity points
of A only. The process A is cadlag, and as was shown in Section 10.1 the points of
jumps of A can be exhausted by a countable sequence of stopping times (see (10.2)
and related remarks). So, it is suÔ¨Écient for (10.50) that limn sup An
œÑ(Œª) = AœÑ for all
s.t. œÑ.
By (10.51) limn
œÑ(Œª) ‚â§AœÑ and An
œÑ(Œª) ‚â§An
1(Œª) which converges to A, on space L1(Œ©).
Hence, by Fatou‚Äôs lemma

174
10
General theory of stochastic processes under ‚Äúusual conditions‚Äù
lim
n inf EAn
œÑ ‚â§lim
n sup EAn
œÑ(Œª) ‚â§E lim
n sup An
œÑ(Œª) ‚â§EAœÑ.
To prove that limn EAn
œÑ = EAœÑ we set œÉn = inf{t ‚ààDn : t ‚â•œÑ}. Then œÉn ‚ÜìœÑ as
n ‚Üí‚àûand An
œÑ = An
œÉn.
The process X belongs to class (D) and therefore
EAn
œÑ = An
œÉn = EXœÉn ‚àíEM0 ‚ÜíEXœÑ ‚àíEM0 = EAœÑ.
Lemma 10.20 is proved and hence the decomposition (10.44) is true with a martingale
M and an increasing predictable process A.

Chapter 11
General theory of stochastic processes in
applications
Abstract The main goal of this chapter is to show how the general theory developed
before can be applied to mathematical Ô¨Ånance and statistics of random processes.
In the area of mathematical Ô¨Ånance a semimartingale Ô¨Ånancial market model is
introduced. Applying to this general model the technique of stochastic exponents the
fundamental questions of arbitrage and completeness of such a market are studied.
These results have a number of corollaries for modeling and option pricing (Black-
Scholes model and formula, Cox-Ross-Rubinstein model and formula etc). In the
area of statistics of random processes the technique developed above gives a possi-
bility to introduce semimartingale models. It is shown that classical discrete time and
continuous time models of stochastic approximation are embedded in a semimartin-
gale scheme. Moreover, it is proved that semimartingale stochastic approximation
procedures are strong consistent and asymptotically normal under very wide condi-
tions. In case of semimartingale regression the structural least-squared estimates are
strong consistent and their sequential versions satisfy the important Ô¨Åxed accuracy
property (see [3], [4], [11], [13], [18], [23], [30], [31], [32], [34], and [43]).
11.1
Stochastic mathematical Ô¨Ånance
Suppose that besides the original measure on a stochastic basis (Œ©, F, (Ft)t ‚â•0, P),
we are given a measure ÀúP locally equivalent to P and with local density (Zt)t ‚â•0.
The equivalence implies strict positivity of Zt (P-a.s.), t ‚â•0. Therefore, one can
deÔ¨Åne a local martingale N = (Nt)t ‚â•0 with respect to P as a stochastic integral
to P as a stochastic integral Nt =
‚à´t
0 Z‚àí1
s‚àídZs. It leads to a stochastic exponent
form of Zt = Et(N). We already know that a local martingale is transformed to a
semimartingale (not a local martingale) under an equivalent change of measure. The
next lemma contains a result of this type. To formulate it we denote Mloc(P) and
Mloc( ÀúP) classes of local martingales w.r. to P and ÀúP correspondently.
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_11
175

176
11
General theory of stochastic processes in applications
Lemma 11.1 Let X = (Xt)t ‚â•0 be a semimartingale on (Œ©, F, (Ft)t ‚â•0, P), then
XZ ‚ààMloc(P) ‚áíX ‚ààMloc( ÀúP).
(11.1)
The proof of Lemma 11.1 can be given using a standard scheme. First, it is proved for
martingales. Second, using localization one can extend it to local martingales. The
Ô¨Årst step is given with the help of change of probability in conditional expectations,
which was stated before for a discrete time. A continuous time version of such change
of measure is proved in the same way.
In fact, we prove (11.1) below in the framework of a semimartingale model of
Ô¨Ånancial markets. We deÔ¨Åne a (B, S)-market as a collection of two positive semi-
martingales B and S on given stochastic basis (Œ©, F, (Ft)t ‚â•0, P). The values of Bt
and St are interpreted as the prices of a non-risky asset B and a risky asset S.
A pair œÄt = (Œ≤t, Œ≥t)t ‚â•0 of stochastic processes with a predictable second compo-
nent is deÔ¨Åned as a portfolio of investment strategy. The portfolio has a capital
XœÄ
t = XœÄ
t (x) = Œ≤tBt + Œ≥tSt, where XœÄ
0 = Œ≤0B0 + Œ≥0S0 = x ‚ààR, t ‚â•0.
The ratio Xt = St
Bt is called the discounted price of S, and the ratio X œÄ
t
Bt is called
the discounted capital of the strategy œÄ at the time t ‚â•0.
In the set of all strategies we distinguish those portfolios œÄ such that
XœÄ
t
Bt
=
XœÄ
0
B0
+
‚à´t
0
Œ≥ud
 S
B

u
.
(11.2)
We call them self-Ô¨Ånancing and denote their collection SF. We say that (B, S)-
market admits an arbitrage at time T > 0 if there exists ÀúœÄ ‚ààSF such that X ÀúœÄ
0 =
0, X ÀúœÄ
t ‚â•0, t ‚â§T, P-a.s., and P(œâ : X ÀúœÄ
T (œâ) > 0) > 0. Such a strategy ÀúœÄ is called the
arbitrage strategy. Any probability measure ÀúP locally equivalent to P is called a
local martingale measure, if the discounted price

St
Bt

‚ààMloc( ÀúP).
Denote the set of such measures M(X, P). It is well-known that for a semimartin-
gale (B, S)-market the absence of arbitrage is characterized as M(X, P)  ‚àÖ.
By a contingent claim with exercise time T, we understand any non-negtaive
FT-measurable random variable f . We remark that if such a claim f represents a
pay-oÔ¨Äof an option, then the option is called a European option. A strategy œÄ ‚ààSF
is said to be a hedge (hedging strategy) for f (for the option with the pay-oÔ¨Äf ) if
XœÄ
T (x) ‚â•f (a.s.) for some initial capital x. If in some class of hedging strategies œÄ,
there is a strategy œÄ‚àósuch that XœÄ‚àó
t
‚â§XœÄ
t (a.s.) for all t ‚àà[0,T], then œÄ‚àóis called a
minimal hedge (in this class). Usually, the minimal hedge coincides with a replicating
strategy œÄ‚àófor which XœÄ‚àó
T = f (a.s.). In this case, f is called attainable. The (B, S)-
market is said to be complete if any contingent claim is attainable. For many Ô¨Ånancial
markets, this notion is equivalent to the uniqueness of a local martingale measure P‚àó,
i.e. M(X, P) = {P‚àó}. Otherwise, the market is called incomplete. Correspondingly,
pricing of an option with pay-oÔ¨Äf in complete and incomplete markets is achieved
with the help of martingale measures as follows:

11.1
Stochastic mathematical Ô¨Ånance
177
C( f ) = E‚àóf
BT
and C‚àó( f ) =
sup
ÀúP‚ààM(X,P)
ÀúE f
B,
(11.3)
where C( f ) is the fair price and C‚àó( f ) is the so-called upper price of such an option.
So, we can see that an investigation of conditions under which ÀúP ‚ààM(X, P) is
extremely important for Mathematical Finance. Let us show how it can be done for
a semimartingale (B, S)-market.
Suppose that a (B, S)-market is determined by the two equations
Bt =B0 +
‚à´t
0
Bu‚àídhu, Œîhu > ‚àí1,
St =S0 +
‚à´t
0
Su‚àídHu, ŒîHu > ‚àí1,
(11.4)
where h and H are given semimartingales.
Using stochastic exponents we can rewrite (11.4) as follows
Bt = B0Et(h) and St = S0Et(H).
The problem in the model (11.4) is to determine conditions under which a measure
P‚àóequivalent to P takes the process X = S
B into a local martingale, i.e.
P‚àó‚ààM(X, P) ‚áîX = S
B ‚ààMloc(P‚àó).
(11.5)
Let us check Ô¨Årst when measure P is a martingale one for the market (11.4). Using
the properties of stochastic exponents we have
Xt =X0Et(H)E‚àí1
t (h) = X0Et(H)Et(‚àíh‚àó)
=X0E

H ‚àíh + ‚ü®hc, hc‚ü©+
 (Œîh)2
1 + Œîh ‚àí‚ü®Hc, hc‚ü©‚àí
 ŒîHŒîh
1 + Œîh

=X0Et

H ‚àíh + ‚ü®hc, hc ‚àíHc‚ü©+
 Œîh(Œîh ‚àíŒîH)
1 + Œîh

.
(11.6)
Denote
Œ®t(h, H) = HT ‚àíht + ‚ü®hc, hc ‚àíHc‚ü©t +

s‚â§t
Œîh(Œîh ‚àíŒîH)
1 + Œîh
and rewrite (11.6) in the form of the following stochastic diÔ¨Äerential equation
Xt = X0 +
‚à´t
0
Xs‚àídŒ®s(h, H).
(11.7)

178
11
General theory of stochastic processes in applications
Hence, X ‚ààMloc(P) if Œ®(h, H) ‚ààMloc(P). These considerations generate the idea
how to Ô¨Ånd conditions to provide that X ‚ààMloc(P‚àó) for a local martingale measure
P‚àówith the local density
Zt = dP‚àó
t
dPt
= Et(N), Nt =
‚à´t
0
Z‚àí1
s‚àídZs ‚ààMloc(P).
We just need to recognize when XZ = XE(N) ‚ààMloc(P), and apply (11.1) of
Lemma 11.1.
In view (11.6)-(11.7) we have
XtEt(N) = X0Et(Œ®(h, H))Et(N),
and by the properties of stochastic exponents obtain that
XtEt(N) = X0Et(Œ®(h, H, N)),
(11.8)
where
Œ®t(h, H, N) = Ht ‚àíht + Nt + ‚ü®(h ‚àíN)c, (h ‚àíH)c‚ü©t +

s‚â§t
(Œîhs ‚àíŒîNs)(Œîhs ‚àíŒîHs)
1 + Œîhs
.
Using relation (11.8), we arrive to the following theorem.
Theorem 11.1 Let X = S
B in the model (11.4). Then the following claims are true
(1) If Œ®(h, H) ‚ààMloc(P), then X ‚ààMloc(P),
(2) If Œ®(h, H, N) ‚ààMloc(P), then X ‚ààMloc(P‚àó).
The above theorem presents a convenient methodology of Ô¨Ånding of martingale
measures for the model (11.4). Let us demonstrate this methodology for several
partial cases of (11.4).
Example 11.1 The Black-Scholes model:
dBt =rBtdt, B0 = 1,
dSt =St(Œºdt + œÉdWt), S0 > 0,
(11.9)
where r, Œº ‚ààR+, œÉ > 0, W = (Wt)t ‚â•0 is a Wiener process.
In the model (11.9), ht = rt, Ht = Œºt + œÉWt. Due to W is the sole source of
randomness, we take N in the form Nt = œÜWt. To use Theorem 11.1 we Ô¨Ånd that
Œ®(h, H, N) =Œºt + œÉWt ‚àírt + œÜWt + œÜœÉt
=(Œº ‚àír + œÜœÉ)t + (œÉ + œÜ)Wt.
Therefore, the condition (2) of Theorem 11.1 is satisÔ¨Åed if Œº ‚àír + œÜœÉ = 0 or œÜ =
‚àíŒº‚àír
œÉ , and we can construct the local density
Zt = dP‚àó
t
dPt
= Et(N) = exp
	
‚àíŒº ‚àír
œÉ
Wt ‚àí1
2
 Œº ‚àír
œÉ
2
t

.
(11.10)

11.1
Stochastic mathematical Ô¨Ånance
179
Using the Girsanov exponent (11.10) we Ô¨Ånd the martingale measure P‚àóunder
which W‚àó
t = Wt + Œº‚àír
œÉ t will be a Wiener process by the Girsanov theorem. So, if
f = (ST ‚àíK)+ is a contingent claim, then its initial price C( f ) can be calculated as
follows
C( f ) = E‚àó(ST ‚àíK)+
erT
= S0Œ¶(d+) ‚àíKe‚àírTŒ¶(d‚àí),
(11.11)
d¬± = ln(S0/K)+(r¬±œÉ2/2)T
œÉ
‚àö
T
, and we again arrive to the Black-Scholes formula.
Example 11.2 The Merton model:
dBt =rBtdt, B0 = 1,
dSt =St‚àí(Œºdt ‚àíŒΩdŒ†t),
(11.12)
where r, Œº ‚ààR+, ŒΩ < 1, Œ† = (Œ†t)t ‚â•0 is a Poisson process with parameter Œª > 0.
In the model (11.12) ht = rt, Ht = Œºt ‚àíŒΩŒ†t, and the martingale Nt will be chosen
as Nt = œà(Œ†t ‚àíŒªt). Further,
Œ®(h, H, N) =Œºt ‚àíŒΩŒ†t ‚àírt + œà(Œ†t ‚àíŒªt) ‚àíœàŒΩŒ†t
=(Œº ‚àír ‚àíŒΩŒª ‚àíœàŒΩŒª)t ‚àíŒΩ(Œ†t ‚àíŒªt) + œà(Œ†t ‚àíŒªt) ‚àíœàŒΩ(Œ†t ‚àíŒªt).
Hence, the condition (2) of Theorem 11.1 is fulÔ¨Ålled if Œº ‚àír ‚àíŒΩŒª ‚àíœàŒΩŒª = 0 or
œà = Œº‚àír
ŒΩŒª ‚àí1 is the unique solution. The uniqueness means that a martingale measure
P‚àóis also unique, and its local density has the following exponential form
Zt = dP‚àó
t
dPt
= Et(N) = exp [(Œª ‚àíŒª‚àó)t + (ln Œª‚àó‚àíln Œª)Œ†t],
(11.13)
where Œª‚àó= Œº‚àír
ŒΩ
is a parameter of Œ†t under measure P‚àó.
Using (11.13) we calculate a call option price C in the model (11.12) as follows.
C = E‚àó(ST ‚àíK)+
erT
.
It is clear that
ST
BT
= S0
B0
exp [‚àíŒΩŒ†T + ŒΩŒª‚àóT]

t ‚â§T
(1 ‚àíŒΩŒîŒ†t)eŒΩŒîŒ†t
= S0
B0
exp [Œ†T ln(1 ‚àíŒΩ) + ŒΩŒª‚àóT] .
(11.14)
Using (11.14) we Ô¨Ånd that

180
11
General theory of stochastic processes in applications
C =E‚àó
 ST
BT
‚àíK
BT
+
=
‚àû

n=0
e‚àíŒª‚àóT (Œª‚àóT)n
n!

S0en ln(1‚àíŒΩ)+ŒΩŒª‚àóT ‚àíB‚àí1
T K
+
.
(11.15)
Denote
n0 = inf

n : S0en ln(1‚àíŒΩ)+ŒΩŒª‚àóT ‚â•K
BT

=
ln(K/S0) ‚àíŒºT
ln(1 ‚àíŒΩ)

,
Œ®p(x, y) =
‚àû

n=x
e‚àíy yn
n! ,
and Ô¨Ånd from (11.15) that
C =S0
‚àû

n=n0
e‚àíŒª‚àóT+Œª‚àóŒΩT en ln(1‚àíŒΩ) (Œª‚àóT)n
n!
‚àíKe‚àírT Œ®p(n0, Œª‚àóT)
=S0
‚àû

n=n0
e‚àíŒª‚àó(ŒΩ‚àí1)T (Œª‚àó(1 ‚àíŒΩ)T)n
n!
‚àíKe‚àírT Œ®p(n0, Œª‚àóT)
=S0Œ®p(n0, Œª‚àó(1 ‚àíŒΩ)T) ‚àíKe‚àírT Œ®p(n0, Œª‚àóT).
(11.16)
Formula (11.16) is called the Merton formula for the price of a call option in the
model (11.12).
Putting together models (11.9) and (11.12) we get a jump-diÔ¨Äusion market model:
Bt =rBtdt, B0 = 1,
dSt =St‚àí(Œºdt + œÉdWt ‚àíŒΩdŒ†t), S0 > 0.
(11.17)
To calculate process Œ®(h, H, N) in this case we choose Nt = œÜWt + œàŒ†t and Ô¨Ånd
that
Œ®t(h, H, N) = (Œº ‚àír ‚àíŒΩŒª + œÜœÉ ‚àíœàŒª)t + martingale.
Hence, Œº ‚àír + œÜœÉ ‚àíŒªŒΩ(1 + œà) = 0 to make Œ®(h, H, N) a martingale. But this
equation has inÔ¨Ånitely many solutions (œÜ, œà), and therefore, the model (11.15) admits
inÔ¨Ånitely many martingale measures, i.e. the market (11.17) is incomplete.
Let us consider a discrete-time model, called a Binomial market or the Cox-Ross-
Rubinstein model. We show that such a model is embedded in the model (11.4).
Example 11.3 The model we are talking represents a kind of Binomial random walk:

11.1
Stochastic mathematical Ô¨Ånance
181
ŒîBn =Bn ‚àíBn‚àí1 = rBn‚àí1, B0 > 0,
ŒîSn =Sn ‚àíSn‚àí1 = œÅnSn‚àí1, S0 > 0,
(11.18)
where (œÅn)n=1,2,... is a sequence of independent random variables taking two values
b > a with probabilities p and q = 1 ‚àíp, p ‚àà(0, 1).
Assume also that ‚àí1 < a < r < b. Putting Bt = Bn on [n, n + 1) (we do the same
with St, Ft, . . .), we transform (11.18) to the model (11.4). This standard procedure
allows to apply the theory developed for the semimartingale model.
In the case under consideration
Œîhn = r, ŒîHn = œÅn, ŒîNn = œàn(œÅn ‚àíŒº), Œº = EœÅn,
and
(1 + r)ŒîŒ®n(h, H, N) = ŒîHn ‚àíŒîhn + ŒîNn + ŒîNnŒîHn.
Hence, the martingality property Œ®n means that
E(œÅn ‚àír + œàn(œÅn ‚àíŒº) + œàn(œÅn ‚àíŒº)œÅn|Fn‚àí1) = 0,
which leads to
œàn = ‚àíŒº ‚àír
œÉ2 ,
where œÉ2 = Var(œÅn).
As a result, we can construct a local density of a martingale measure P‚àóhere in
the form of stochastic exponent
Zn = En

‚àíŒº ‚àír
œÉ2

(œÅn ‚àíŒº)

.
(11.19)
As in previous examples we want to derive the price of a call option (SN ‚àíK)+,
where N ‚â•1. According to the general theory the price C can be calculated as
C =E‚àó(1 + r)‚àíN(SN ‚àíK)+
=(1 + r)‚àíNE‚àó(SN ‚àíK)+
=(1 + r)‚àíNEEN

‚àíŒº ‚àír
œÉ2

(œÅk ‚àíŒº)

(SN ‚àíK)I{SN >K },
(11.20)
where we used (11.19).
DeÔ¨Åne k0 = min

k ‚â§N : S0(1 + b)k(1 + a)N‚àík > K

and Ô¨Ånd that k0 ln(1 +
b) + (N ‚àík0) ln(1 + a) > ln K
S0 or k0 =

ln
K
S0(1+a)N

ln 1+b
1+a

+ 1, where [x] is an
integer part of x ‚ààR.
Denote p‚àó= r‚àía
b‚àía and derive for the term with K in (11.20) using elementary
equalities Œº = p(b ‚àía) + a, œÉ2 = (b ‚àía)2p(1 ‚àíp) that

182
11
General theory of stochastic processes in applications
(1 + r)‚àíNEEN

‚àíŒº ‚àír
œÉ2

(œÅk ‚àíŒº)

KI{SN >K }
=K(1 + r)‚àíN
N

k=k0
N
k
 
1 ‚àíŒº ‚àír
œÉ2 (b ‚àíŒº)
k
pk

1 ‚àíŒº ‚àír
œÉ2 (a ‚àíŒº)
 N‚àík
(1 ‚àíp)N‚àík
=K(1 + r)‚àíN
N

k=k0
N
k
  p‚àó
p
k
pk
 1 ‚àíp‚àó
1 ‚àíp
 N‚àík
(1 ‚àíp)N‚àík
=K(1 + r)‚àíN
N

k=k0
N
k

(p‚àó)k(1 ‚àíp‚àó)N‚àík.
(11.21)
To calculate the term with SN = S0EN( œÅk) in (11.20) we use a multiplication rule
of stochastic exponents:
(1 + r)‚àíN EEN

‚àíŒº ‚àír
œÉ2

(œÅk ‚àíŒº)

SN I{SN >K}
=S0(1 + r)‚àíN EEN

‚àíŒº ‚àír
œÉ2

(œÅk ‚àíŒº) +

œÅk ‚àíŒº ‚àír
œÉ2

(œÅk ‚àíŒº)œÅk

I{SN >K}
=S0(1 + r)‚àíN
N

k=k0
N
k
 
1 ‚àíŒº ‚àír
œÉ2 (b ‚àíŒº) + b ‚àíŒº ‚àír
œÉ2 (b ‚àíŒº)b
k
pk
√ó

1 ‚àíŒº ‚àír
œÉ2 (a ‚àíŒº) + a ‚àíŒº ‚àír
œÉ2 (a ‚àíŒº)a
 N‚àík
(1 ‚àíp)N‚àík
=S0(1 + r)‚àíN
N

k=k0
N
k
  p‚àó
p (1 + b)
k
pk
 1 ‚àíp‚àó
1 ‚àíp (1 + a)
 N‚àík
(1 ‚àíp)N‚àík
=S0(1 + r)‚àíN
N

k=k0
N
k
 
p‚àó1 + b
1 + r
k 
(1 ‚àíp‚àó) 1 + a
1 + r
 N‚àík
.
(11.22)
Introducing the notations Àúp = 1+b
1+r p‚àóand B(j, N, p) = N
k=j
N
k
pk(1 ‚àíp)N‚àík and
putting together (11.21)-(11.22) we get from (11.20) the Cos-Ross-Rubinstein for-
mula for the initial price of a call option
C = S0B(k0, N, Àúp) ‚àíK(1 + r)‚àíNB(k0, N, p‚àó).
11.2
Stochastic Regression Analysis
Proposed and developed below an extension of classical regression models and
techniques is based on those theoretical Ô¨Åndings that was delivered in the previous
chapter, and can be called a Stochastic Regression Analysis.

11.2
Stochastic Regression Analysis
183
We start with the classical problem of stochastic approximation. It consists of a
construction of a stochastic sequence Œ∏n or a stochastic process Œ∏t that converges in
some probabilistic sense to unique root Œ∏ ‚ààR of the regression equation
R(Œ∏) = 0,
(11.23)
where R is a regression function.
In classical theory, a solution to (11.23) is given by the Robbins-Monro procedure
Œ∏n = Œ∏n‚àí1 ‚àíŒ≥nyn, n = 1, 2, . . .,
(11.24)
where the sequence of observations yn is such that
yn = R(Œ∏n‚àí1) + Œæn,
(11.25)
(Œæn)n=1,2,... is a sequence of independent random variables or martingale-diÔ¨Äerences,
(Œ≥n)n=1,2,... is a numerical positive sequence converging to zero.
The convergence (a.s.) of the procedure (11.24)-(11.25) for a continuous linearly
bounded regression function R(x) such that
R(x)(x ‚àíŒ∏) > 0 for all x  Œ∏
is guaranteed by Ô¨Åniteness of the variance (conditional variance) of observation
errors Œæn and the following conditions:
‚àû

n=1
Œ≥n = ‚àû,
(11.26)
‚àû

n=1
Œ≥2
n < ‚àû.
(11.27)
DiÔ¨Äusion analogues of (11.24)-(11.25) and (11.26)-(11.27) are
dŒ∏t = ‚àíŒ≥tR(Œ∏t)dt ‚àíŒ≥tdWt,,
(11.28)
‚à´‚àû
0
Œ≥sds = ‚àû,,
(11.29)
‚à´‚àû
0
Œ≥2
sds < ‚àû,
(11.30)
where Wt is a Wiener process and Œ≥t is a positive deterministic function tending to
zero as t ‚Üí‚àû.
Under conditions (11.29)-(11.30) the procedure Œ∏t converges to Œ∏ as t ‚Üí‚àû.
The leading idea to generalize (11.24)-(11.25) and (11.28) consists in the possi-
bility of describing such stochastic algorithms as strong solutions of some special
classes of stochastic diÔ¨Äerential equations with respect to semimartingales. It leads

184
11
General theory of stochastic processes in applications
to a generalized Robbins-Monro procedure as a process Œ∏t satisfying the stochastic
diÔ¨Äerential equation
Œ∏t = Œ∏0 ‚àí
‚à´t
0
Œ≥sR(Œ∏s‚àí)das ‚àí
‚à´t
0
Œ≥sdms,
(11.31)
where Œ∏0 is a Ô¨Ånite F0-measurable random variable, a predictable process a ‚àà
A+
loc, m ‚ààM2
loc, and Œ≥ is a predictable process decreasing to zero (a.s.) as t ‚Üí‚àû.
To simplify the demonstration how martingale methods work here we consider a
linear case only: R(x) = Œ≤(x ‚àíŒ∏), Œ≤ > 0.
In this case (11.31) is reduced to
Œ∏t ‚àíŒ∏ = Œ∏0 ‚àíŒ∏ ‚àí
‚à´t
0
Œ≥sŒ≤(Œ∏s‚àí‚àíŒ∏)das ‚àí
‚à´t
0
Œ≥sdms.
(11.32)
Let us assume that (a.s.)
‚à´‚àû
0
Œ≥sdas = ‚àû,
(11.33)
‚à´‚àû
0
Œ≥2
sd‚ü®m, m‚ü©s < ‚àû.
(11.34)
DeÔ¨Åne the following stochastic exponent
Et(‚àíŒ≤Œ≥ ¬∑ a) = Et

‚àíŒ≤
‚à´
Œ≥sdas

,
and assume that Œ≤Œ≥tŒîat < 1 to provide that Et(‚àíŒ≤Œ≥ ¬∑ a) > 0 (a.s.).
Applying the Ito formula to E‚àí1
t (‚àíŒ≤Œ≥ ¬∑ a)(Œ∏t ‚àíŒ∏) we arrive from (11.32) to
Œ∏t ‚àíŒ∏ = Et(‚àíŒ≤Œ≥ ¬∑ a)(Œ∏0 ‚àíŒ∏) ‚àíEt(‚àíŒ≤Œ≥ ¬∑ a)
‚à´t
0
Œ≥sE‚àí1
s (‚àíŒ≤Œ≥ ¬∑ a)dms.
(11.35)
The Ô¨Årst term in the right-hand side of (11.35) converges to zero (a.s.) as t ‚Üí‚àû
because of Et(‚àíŒ≤Œ≥ ¬∑ a) ‚Üí0 (a.s.), t ‚Üí‚àû. Second term in (11.35) is treated by
the arguments of the Large Numbers Law for square integrable martingales. Using
(11.34) we have that (a.s.)
‚à´‚àû
0
E2
s(‚àíŒ≤Œ≥ ¬∑ a)Œ≥2
sE2
s(‚àíŒ≤Œ≥ ¬∑ a)d‚ü®m, m‚ü©s =
‚à´‚àû
0
Œ≥2
sd‚ü®m, m‚ü©s < ‚àû.
Hence, the second term of (11.35) converges to zero (a.s.) as t ‚Üí‚àû. As a result,
we get the convergence (a.s.) Œ∏t ‚ÜíŒ∏ as t ‚Üí‚àû.
To investigate an asymptotic normality of the procedure (11.32) we simplify the
situation assuming that at is a deterministic function and mt is a Gaussian martingale
satisfying conditions:

11.2
Stochastic Regression Analysis
185
‚ü®m, m‚ü©t = œÉ2at, œÉ2 > 0, Œ≥t =
Œ±
1 + at
, Œ± > 0, Œ≤Œ± < 1,
at ‚Üë, t ‚Üí‚àû,

0<s<‚àû
 Œîas
1 + as
2
< ‚àû.
Under these conditions we have
Et(‚àíŒ≤Œ≥ ¬∑ a) = Et(‚àíŒ≤Œ±) ‚àº(1 + at)‚àíŒ≤Œ± as t ‚Üí‚àû.
Multiplying (11.35) by (1 + at)1/2 we obtain
(1 + at)1/2(Œ∏t ‚àíŒ∏) = (1 + at)1/2Et(‚àíŒ≤Œ±)(Œ∏0 ‚àíŒ∏)
‚àí(1 + at)1/2Et(‚àíŒ≤Œ±)
‚à´t
0
Œ±
1 + as
E‚àí1
s (‚àíŒ≤Œ±)dms.
(11.36)
Let us note that
(1 + at)1/2Et(‚àíŒ≤Œ±) ‚àº(1 + at)1/2‚àíŒ≤Œ± as t ‚Üí‚àû,
and therefore, under 2Œ≤Œ± > 1 we obtain that the Ô¨Årst term of (11.36) converges to
zero (a.s.) as t ‚Üí‚àû.
The second term of (11.36) has a Gaussian distribution, and hence, we need to
calculate the asymptotic value of the variance of this term:
(1 + at)E2
t (‚àíŒ≤Œ±)
‚à´t
0
E2
s(‚àíŒ≤Œ±) Œ±2œÉ2
1 + as
das ‚Üí
Œ±2œÉ2
2Œ≤Œ± ‚àí1 as t ‚Üí‚àû.
(11.37)
Finally, we can arrive to conclusion that
(1 + at)1/2(Œ∏t ‚àíŒ∏)
d
‚àí‚àí‚àí‚àí‚Üí
t‚Üí‚àûN

0,
Œ±2œÉ2
2Œ≤Œ± ‚àí1

.
(11.38)
As a consequence of (11.38) we get the well-known classical results from the theory
of stochastic approximation for discrete and continuous time (11.24)-(11.25) and
(11.28):
n1/2(Œ∏n ‚àíŒ∏)
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûN

0,
Œ±2œÉ2
2Œ≤Œ± ‚àí1

,
t1/2(Œ∏t ‚àíŒ∏)
d
‚àí‚àí‚àí‚àí‚Üí
t‚Üí‚àûN

0,
Œ±2œÉ2
2Œ≤Œ± ‚àí1

.
Another important problem of regression analysis is the problem of estimation of
the unknown parameter of a linear regression function.

186
11
General theory of stochastic processes in applications
Suppose we observe the process Xt having the following structure
Xt =
‚à´t
0
fsdasŒ∏ + Mt,
(11.39)
where a ‚ààA+
loc and predictable, M ‚ààM2
loc, a predictable function ft such that,
‚à´t
0 f 2
s das = Ft < ‚àû(a.s.), t ‚â•0, Œ∏ ‚ààR is an unknown parameter.
DeÔ¨Åne the following structural Least Squares estimate as follows
Œ∏t = F‚àí1
t
‚à´t
0
fsdXs,
(11.40)
where we assume that Ft > 0 (a.s.) and Ft ‚ààA+
loc and predictable.
Assuming that Ft ‚Üí‚àû(a.s.) as t ‚Üí‚àû, we rewrite (11.40):
Œ∏t =F‚àí1
t
‚à´t
0
f 2
s dasŒ∏ + F‚àí1
t
‚à´t
0
fsdMs
=Œ∏ + F‚àí1
t
‚à´t
0
fsdMs.
(11.41)
Obviously, we can study the asymptotic behaviour of Œ∏t with the help of the Large
Numbers Law. To apply such LNL we Ô¨Ånd from (11.41) that (a.s.)
‚à´‚àû
0
F‚àí2
s d
‚à´t
0
fsdMs,
‚à´t
0
fsdMs

=
‚à´‚àû
0
F‚àí2
s
f 2
s d‚ü®M, M, ‚ü©s < ‚àû.
(11.42)
So, if Ft ‚Üí‚àû(a.s.), t ‚Üí‚àû, and (11.42) is fulÔ¨Ålled then Œ∏t ‚ÜíŒ∏ (a.s.) t ‚Üí‚àû.
Let us continue our study of estimates (11.40), by considering their sequential
analog. To do this in the model (11.39) we assume that
d‚ü®M, M‚ü©t
dat
‚â§ŒæŒ≥t,
(11.43)
‚à´t
0
Œ≥‚àí1
s f 2
s das ‚ààA+
loc and predictable,
where Œæ is a positive r.v. and Œ≥t is a positive predictable process.
Next, for H > 0 we put
œÑH = inf
	
t :
‚à´t
0
Œ≥‚àí1
s f 2
s das ‚â•H

with œÑH = ‚àûif the set in {¬∑} is empty.
On the set {œÑH < ‚àû} we deÔ¨Åne a random variable Œ≤H by the relation
‚à´
(0,œÑH )
Œ≥1
s f 2
s das + Œ≤HŒ≥‚àí1
œÑH f 2
œÑH ŒîaœÑH = H,
(11.44)

11.2
Stochastic Regression Analysis
187
and we put Œ≤H = 0 on the set {œÑH = ‚àû}. Then Œ≤H ‚àà[0, 1) and it is a FœÑH‚àí-measurable
random variable.
Let us deÔ¨Åne the following sequential Least Squares estimate
ÀÜŒ∏H = H‚àí1
‚à´
(0,œÑH )
Œ≥‚àí1
s fsdXs + Œ≤HŒ≥‚àí1
œÑH fœÑH ŒîXœÑH

.
(11.45)
The next theorem shows a nice property of ÀÜŒ∏H called a Ô¨Åxed accuracy.
Theorem 11.2 Suppose (11.43)-(11.45) are fulÔ¨Ålled, EŒæ < ‚àûand
‚à´‚àû
0 Œ≥‚àí1
s f 2
s das =
‚àû(a.s.). Then P{œâ : œÑH < ‚àû} = 1, E ÀÜŒ∏H = Œ∏, and
Var ÀÜŒ∏H ‚â§H‚àí1EŒæ.
(11.46)
Proof The
Ô¨Åniteness
(a.s.)
of
œÑH
follows
from
the
relation
{œÑH ‚â§T} =

œâ :
‚à´T
0 Œ≥‚àí1
s f 2
s das ‚â•H

. Using (11.45) we Ô¨Ånd that
ÀÜŒ∏H = Œ∏ + H‚àí1NœÑH,
(11.47)
where Nt =
‚à´t
0 I(s<œÑH )Œ≥‚àí1
s fsdMs + I{t=œÑH }Œ≤HŒ≥‚àí1
œÑH fœÑH ŒîMœÑH .
Since the process Nt is a stochastic integral with respect to M ‚ààM2
loc, by the
properties of stochastic integrals we obtain that
‚ü®N, N‚ü©t =
‚à´t
0
I(s<œÑH )Œ≥‚àí1
s f 2
s d‚ü®M, M, ‚ü©s + I{t=œÑH }Œ≤2
HŒ≥‚àí2
œÑH f 2
œÑH Œî‚ü®M, M‚ü©œÑH
(11.48)
Hence, by (11.44) we get
‚ü®N, N‚ü©œÑH = Œæ
‚à´
(0,œÑH )
Œ≥‚àí1
s f 2
s das + Œ≤HŒ≥‚àí1
œÑH f 2
œÑH ŒîaœÑH

= ŒæH,
and consequently, Nt‚àßœÑH ‚ààM ‚à©M2 and
ENœÑH = 0, EN2
œÑH = HEŒæ,
which leads to (11.46).
Example 11.4 Let us consider the Ô¨Årst order autoregression model:
xt = Œ∏xt‚àí1 + et, x0 = 0, t = 1, 2, . . .,
where (et)t=1,2,... is a martingale-diÔ¨Äerence w.r. to (Ft) and E(e2
t |Ft‚àí1) ‚â§Œæ, EŒæ < ‚àû.
Then (11.43) is fulÔ¨Ålled with Œ≥t = 1, and the estimate ÀÜŒ∏H takes the form

188
11
General theory of stochastic processes in applications
ÀÜŒ∏H =H‚àí1
œÑH ‚àí1

k=1
xk xk‚àí1 + Œ≤H xœÑH xœÑH ‚àí1

,
œÑH = inf

n :
n

k=1
x2
k‚àí1 ‚â•H

,
Œ≤H =x‚àí2
œÑH

H ‚àí
œÑH ‚àí1

k=1
x2
k

.
Embedding the present model in the general model (11.39) in a standard way, we get
E ÀÜŒ∏H = Œ∏, Var ÀÜŒ∏H ‚â§H‚àí1EŒæ.

Chapter 12
Supplementary problems
Abstract The list below contains problems which are related to all chapters of the
book. Some of them are numerical and some others are pure theoretical, but in
any case they are helped for both students and instructors. Students can improve
their understanding and scope. Instructors can transform most of the problems for
teaching and examination purposes. The following references might be useful to
create detailed solutions (see [1], [5], [7], [10], [11], [13], [14], [15], [16], [17], [18],
[21], [22], [23], [24], [27], [28], [29], [30], [31], [35], [37], [43], [44], and [45]).
Problem 12.1 Let X be a standard normal random variable and Y be a Bernoulli random
variable such that P(Y = 1) = P(Y = ‚àí1) = 1
2, and X,Y are independent. Prove that
(a) Z = Y X ‚àºN(0, 1),
(b) Z and X are uncorrelated, but they are not independent.
Solution: The claim (a) is obvious. To prove (b) we calculate cov(X, Z) = E(XZ) =
EX2EY = 1 ¬∑ 0 = 0.
Further,
P(X ‚â•1, Z ‚â•1) = P(X ‚â•1,Y = 1) = P(X ‚â•1) ¬∑
P(Y = 1) = 1
2P(X ‚â•1). But P(X ‚â•1) ¬∑ P(Z ‚â•1) = (P(X ‚â•1))2 and P(X ‚â•1) ‚âÉ
0.1587, and, hence, 1
2P(X ‚â•1)  (P(X ‚â•1))2.
‚ñ†
Problem 12.2 Assume that stochastic process Xt = eWt , where W = (Wt)t ‚â•0 is a
standard Wiener process. Calculate the drift and diÔ¨Äusion coeÔ¨Écients of process
X = (Xt)t ‚â•0:
b(x) = lim
h‚Üí0 h‚àí1 [E(Xt+h ‚àíXt|Xt = x)],
œÉ2(x) = lim
h‚Üí0 h‚àí1 
E

(Xt+h ‚àíXt)2|Xt = x

for t, x ‚ààR1
+
Hint: Use the fact that
Wt+h ‚àíWt ‚àºN(0,
‚àö
h),
h > 0
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3_12
189

190
12
Supplementary problems
‚ñ†
Problem 12.3 Let Xi ‚àºN(Œºi, œÉ2
i ), i = 1, 2, and X1 + X2 ‚àºN(Œº1 + Œº2, œÉ2
1 + œÉ2
2 +
2œÅœÉ1œÉ2). Then cov(X1, X2) = œÅœÉ1œÉ2 and corr(X1, X2) = œÅ.
Hint: Use the formula for bivariate normal distribution.
‚ñ†
Problem 12.4 (Theorem of Slutsky)
Let r.v‚Äôs Xn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX and the sequence of real numbers an ‚àí‚Üía ‚ààR1. Then Xn +
an
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX + a and anXn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûaX
Solution:
Let us note for ‚àÄŒµ > 0, x ‚ààR1, that
P(Xn + an ‚â§x) =P(Xn + an ‚â§x, |an ‚àía| < Œµ) + P(Xn + an ‚â§x, |an ‚àía| ‚â•Œµ) ‚â§
‚â§P(Xn ‚â§x ‚àía + Œµ) + P(|an ‚àía| ‚â•Œµ).
It follows from here for all points x ‚àía + Œµ of continuity of distribution function Fx
that
lim
n sup FXn+an(x) ‚â§Fx(x ‚àía + Œµ).
So, due to arbitrary choice of Œµ > 0 we derive
lim
n‚Üí‚àûFXn+an(x) = FX(x ‚àía) = FX+a(x)
for all x at which FX+a is continuous.
The second claim is proved in the same way.
‚ñ†
Problem 12.5 Let Xn ‚àºN(Œºn, œÉ2
n) and Œºn ‚ÜíŒº, œÉ2
n ‚ÜíœÉ2, n ‚Üí‚àû. Then Xn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àû
X ‚àºN(Œº, œÉ2).
Solution:
Denote Zn = Xn‚àíŒºn
œÉn
‚àºN(0, 1) and, hence, Zn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûZ ‚àºN(0, 1). Using Problem
12.4, we obtain that
Xn = œÉnZn + Œºn
d
‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àûX = œÉZ + Œº.
‚ñ†
Problem 12.6 (Borel-Cantelli lemma) Let (An)n=1,2,... be a sequence of events,
and C =
‚àû
m=1

n=m An. Then

12
Supplementary problems
191
1. P(C) = 0, if
‚àû
n=1
P(An) < ‚àû;
2. P(C) = 1, if (An)n=1,2,... are independent and
‚àû
n=1
P(An) = ‚àû.
Solution: 1. We have P(C) ‚â§P
	 ‚àû
n=m An

‚â§
‚àû
n=m P(An) for each n = 1, 2, ... Take
Œµ > 0 and Ô¨Ånd NŒµ big enough that
‚àû
n=NŒµ
P(An) < Œµ. Hence, for all N > NŒµ we obtain
P(C) < Œµ.
2. Let us note that P((
‚àû
n=m An)c) ‚â§P(
‚àû
n=m Ac
n) ‚â§P(
m+M

n=m Ac
n) for any M > 0.
But (An) are independent, and, hence, P((
‚àû
n=m An)c) ‚â§
m+M

n=m (1 ‚àíP(An)) ‚â§
exp

‚àí
m+M

n=m P(An)

, where we used the inequality 1 ‚àíx ‚â§e‚àíx for x ‚àà[0, 1]. The
claim follows from the inequality above for probabilities.
‚ñ†
Problem 12.7 Let Œº =
‚àû
n=1
Œ±nPn, where (Pn) and (Œ±n) are sequences of probability
measures and positive numbers respectively. DeÔ¨Åne ŒΩ =
‚àû
n=1
Œ≤nQn, where (Qn) and
(Œ≤n) are sequences of probability measures Qn ‚â™Pn and non-negative numbers.
Prove that ŒΩ ‚â™Œº
Solution:
For any A with Œº(A) = 0 we have
‚àû
1
Œ±nPn = Œº(A) = 0 and, hence, Pn(A) = 0 for
all n. So, Qn(A) = 0 and therefore ŒΩ(A) =
‚àû
n=1
Œ≤nQn = 0 and we get ŒΩ ‚â™Œº.
‚ñ†
Problem 12.8 Let ([0, 1], B(0, 1), l) be a Borel space with the Lebesgue probability
measure l, and X and Y be random variables: X(œâ) = 2œâ2 and
Y(œâ) =
‚éß‚é™‚é™‚é™‚é®
‚é™‚é™‚é™‚é©
0, œâ ‚àà[0, 1
3]
2, œâ ‚àà[ 1
3, 2
3]
1, œâ ‚àà[ 2
3, 1]
.
Find E(X|Y).
Solution: For œâ ‚àà[0, 1
3] we have E(X|Y)(œâ) =
1
3‚à´
0
xdP
P([0, 1
3 ]) = 1
1
3
‚à´1
3
0 2œâ2dœâ = 2
27. Values
of E(X|Y) on other sets [ 1
3, 2
3] and [ 2
3, 1] can be determined in the same way: 14
27 and
38
27 correspondingly.
‚ñ†

192
12
Supplementary problems
Problem 12.9 Let (Œµn)n=1,2,...,N be a sequence of independent random variables
with values +1 and ‚àí1 taking with probability 1
2. DeÔ¨Åne Xn = (‚àí1)n cos œÄ n
k=1 Œµk
 ,
n = 1, 2, ..., N and prove that (Xn)n=1¬∑¬∑¬∑N is a martingale with respect to a natural
Ô¨Åltration Fn = F Œµ
n = F X
n .
Solution:
We
represent
the
sequence
(Xn)
as
follows:
Xn = (‚àí1)n 1
2

eiœÄ n
1 Œµk + e‚àíiœÄ n
1 Œµk 
, i =
‚àö
‚àí1.
Using
independence
of
(Œµn)1¬∑¬∑¬∑N
we
have
E(Xn|Fn‚àí1) = 1
2(‚àí1)n

EeiœÄŒµn ¬∑ eiœÄ n‚àí1
1
Œµk + Ee‚àíiœÄŒµn ¬∑ e‚àíiœÄ n‚àí1
1
Œµk

.
Further, applying an obvious relation EeiœÄŒµn = 1
2(eiœÄ + e‚àíiœÄ) = ‚àí1 we obtain
E(Xn|Fn‚àí1) = (‚àí1)n‚àí1
2

eiœÄ n‚àí1
1
Œµk + e‚àíiœÄ n‚àí1
1
Œµk

= (‚àí1)n‚àí1cos

œÄ
n‚àí1

1
Œµk

= Xn‚àí1.
‚ñ†
Problem 12.10 Let values and joint distribution of random variables X and Y are
given in the table
X
Y
-0.1
0
0.1
-0.2
0.1
0
0.4
0.1
0.3
0.1
0.1
Find marginal distributions of X and Y, average of Y and E(Y|X).
Solution: We have from the table above that
P(X = ‚àí0.2) = 0.1 + 0.4 = 0.5,
P(X = 0.1) = 0.3 + 0.1 + 0.1 = 0.5,
P(Y = ‚àí0.1) = 0.1 + 0.3 = 0.4,
P(Y = 0) = 0.1, P(Y = 0.1) = 0.4 + 0.1 = 0.5
which give us marginal distributions. We also derive from the above equalities that
EY = ‚àí0.1 ¬∑ 0.4 + 0.1 ¬∑ 0.5 = 0.01.
To calculate the conditional expectation E(Y|X) we write
E(Y|X) = E(Y|X = ‚àí0.2) ¬∑ I{X=‚àí0.2} + E(Y|X = 0.1) ¬∑ I{X=0.1}.
Calculating
E(Y|X = ‚àí0.2) = ‚àí0.1 ¬∑ P(Y = ‚àí0.1|X = ‚àí0.2) + 0.1 ¬∑ P(Y = 0.1|X = ‚àí0.2) =
=‚àí0.1 ¬∑ 0.1 + 0.1 ¬∑ 0.4
0.5
= 0.06,
and similarly

12
Supplementary problems
193
E(Y|X = 0.1) = ‚àí0.1 ¬∑ 0.3 + 0.1 ¬∑ 0.1
0.5
= ‚àí0.04,
we obtain
E(Y|X) = 0.06 ¬∑ I{X=‚àí0.2} ‚àí0.04 ¬∑ I{X=0.1}.
‚ñ†
Problem 12.11 Let (Xn)n=1,2,¬∑¬∑¬∑ be a sequence of independent random variables
such that P(Xn = 1) = p, P(Xn = ‚àí1) = 1 ‚àíp, 1 < p < 1
2. Show that the following
stochastic sequences are martingales with respect to a natural Ô¨Åltration (Fn)n=1,2,¬∑¬∑¬∑
generated by (Xn).
(a) Mn =
n
k=1
Xk ‚àín ¬∑ (2p ‚àí1);
(b) Yn = M2
n ‚àí4np(1 ‚àíp);
(c) Zn = ( 1‚àíp
p )
n
1
Xk.
Hint: Check the martingale property.
‚ñ†
Problem 12.12 Let X0 be a random variable such that P(X0 = 2) = P(X0 = 0) = 1
2.
DeÔ¨Åne Xn = n ¬∑ Xn‚àí1,
n = 1, 2, ¬∑ ¬∑ ¬∑ and Mn = Xn ‚àíEXn. Prove that (Mn) is not a
martingale with respect to the natural Ô¨Åltration (Fn).
Solution: We observe that E(Mn|Fn‚àí1) = E(Xn|Fn‚àí1) ‚àín! = n ¬∑ Xn‚àí1 ‚àín! =
n(Xn‚àí1 ‚àí(n ‚àí1)!) = n ¬∑ Mn‚àí1  Mn‚àí1.
‚ñ†
Problem 12.13 Find a stochastic diÔ¨Äerential for the process Xt = (
‚àö
123 + 1
2Wt)2,
where (Wt) is a Wiener process.
Solution: Here Xt = f (Wt) with the function f (x) = (
‚àö
123 + 1
2 x)2. The Ô¨Årst and
second derivatives of this function are f ‚Ä≤(x) = 2(
‚àö
123 + 1
2 x), f ‚Ä≤‚Ä≤(x) = 1. Therefore,
using the Ito formula, the stochastic diÔ¨Äerential of (Xt) is
dXt = f ‚Ä≤(Wt)dWt + 1
2 f ‚Ä≤‚Ä≤(Wt)dt
= 2(
‚àö
123 + 1
2Wt)dWt + 1
2dt.
‚ñ†
Problem 12.14 Let (Nt)t ‚â•0 be a Poisson process with intensity Œª = 1. Prove that
(Nt ‚àít)2 ‚àít is a martingale with respect to natural Ô¨Åltration generated by (Nt).
Calculate E
3‚à´
1
Ntdt ¬∑
4‚à´
2
Ntdt

194
12
Supplementary problems
Hint: In the Ô¨Årst case, please, check a martingale property. In the second case the
answer is 34 1
3.
‚ñ†
Problem 12.15 Check whether the processes are martingales
(a) Xt = W3
t ‚àí3tWt;
(b) Xt = Wt + 287t;
(c) Xt = e
t
2 sin(Wt),
where (Wt)t ‚â•0 is a Wiener process.
Solution:
(a) We have here Xt = f (t,Wt) with the function f (t, x) = x3 ‚àí3tx. This function
has the partial derivatives
‚àÇ
‚àÇx f (t, x) = 3x2 ‚àí3t, ‚àÇ2 f (t, x)
‚àÇx2
= 6x,
‚àÇ
‚àÇt f (t, x) = ‚àí3x.
Therefore, using the Ito formula, we derive
dXt = (3W2
t ‚àí3t)dWt +
	
‚àí3Wt + 1
2(6Wt)

dt = (3W2
t ‚àí3t)dWt.
Therefore, (Xt) is a martingale as a stochastic integral has a martingale property.
In case (b) we have EX0 = 0 and for instance EX1 = 287. This implies that (Xt)
cannot be a martingale, since martingales have constant expectations.
For (c) we have Xt = f (t, Wt) with function f (t, x) = e
t
2 ¬∑ sin(x) and its partial
derivatives
‚àÇ
‚àÇx f (t, x) = e
t
2 ¬∑ cos(x), ‚àÇ2 f (t, x)
‚àÇx2
= ‚àíe
t
2 ¬∑ sin(x),
‚àÇ
‚àÇt f (t, x) = 1
2e
t
2 ¬∑ sin(x).
Using the Ito formula we get
dXt = e
t
2 ¬∑ cos(Wt)dWt +
1
2e
t
2 ¬∑ sin(Wt) + 1
2(‚àíe
t
2 ¬∑ sin(Wt))

dt =
= e
t
2 ¬∑ cos(Wt)dWt
which certiÔ¨Åes that (Xt) is a martingale.
‚ñ†
Problem 12.16 Provide a condition on the mapping œï : R1 ‚àí‚ÜíR1 under which
œï(œÑ) remains a stopping time, where œÑ is a stopping time.
Solution: Suppose the mapping œï meets the following conditions:
(a) œï is injective,

12
Supplementary problems
195
(b) œï([0, ‚àû)) = [0, ‚àû),
(c) œï order-preserving and t ‚â§œï(t) for all t ‚àà[0, ‚àû).
Condition (a) ensures that the inverse mapping œï‚àí1 : œï(R1) ‚àí‚ÜíR1 is deÔ¨Åned. Con-
dition (b) ensures that œï‚àí1(t) is well-deÔ¨Åned and positive for all t ‚â•0. Condition
(c) together with the previous two conditions (a) and (b) gives that whenever œÑ is a
stopping time
{œï(œÑ) ‚â§t} = {œÑ ‚â§œï‚àí1(t)} ‚ààFœï‚àí1(t) ‚äÜFt.
For instance, if œï : [0, ‚àû) ‚àí‚Üí[0, ‚àû) is a strictly increasing function satisfying
t ‚â§œï(t) for all t ‚â•0, and œï is diÔ¨Äerentiable with œï‚Ä≤(t) ‚â•1 for all t, œï(0) = 0,
then by the mean value theorem we get œï(t) ‚â•œï‚Ä≤(c) ¬∑ t ‚â•t, and the above holds.
‚ñ†
Problem 12.17 Considerthefunctionp(t, x, y) =
1
‚àö
2œÄt exp

‚àí(x‚àíy)2
2t

, x, y ‚ààR1, t ‚àà
R1
t , representing the transition density of a Wiener process (Wt). Prove the p(t, x, y)
satisÔ¨Åes to the PDE:
‚àÇ
‚àÇt p(t, x, y) = 1
2
‚àÇ2
‚àÇy2 p(t, x, y).
Solution: On one hand side we have
‚àÇ
‚àÇt p(t, x, y) = p(t, x, y)

‚àí1
2t + (x ‚àíy)2
2t

.
On the other hand, diÔ¨Äerentiating with respect to y, we get
‚àÇ2
‚àÇy2 p(t, x, y) =p ¬∑ (x ‚àíy)2
t
+ p ¬∑ (‚àí1
t )
=p(t, x, y)
(x ‚àíy)2
t
‚àí1
t

.
Then it is clear that p(t, x, y) satisÔ¨Åes the above diÔ¨Äerential equation.
‚ñ†
Problem 12.18 Prove that every non-negative local martingale is a supermartingale.
Hint: Apply the Fatou lemma.
‚ñ†
Problem 12.19 Let (Xn)n=0,1,...,N be a submartingale with E(X+
N)p < ‚àû, p > 1.
Prove the Doob inequality
E(max
n
X+
n )p ‚â§(
p
p ‚àí1)
p
E(X+
N)p,
X+
N = max(0, XN).
Solution: Denote X‚àó
N = maxn‚â§N X+
N and note that
Œª.P(X‚àó
N > Œª) ‚â§EX+
N.I{X‚àó
N >Œª},
Œª > 0.

196
12
Supplementary problems
Multiplying the above inequality by Œªp‚àí2 and integrating over (0, ‚àû), we obtain
‚à´‚àû
0
Œªp‚àí1P(X‚àó
N > Œª)dŒª ‚â§EX+
N.
‚à´X‚àó
N
0
Œªp‚àí2dŒª ‚â§
1
p ‚àí1EX+
N.(X‚àó
N)p‚àí1
Due
to
‚à´‚àû
0 Œªp‚àí1P(X‚àó
N > Œª)dŒª = 1
p
‚à´‚àû
0 ŒªpdP(X‚àó
N ‚â§Œª)
we
get
E(X‚àó
N)p ‚â§
p
1‚àípEX+
N.(X‚àó
N)p‚àí1, and with the help of the H√∂lder-inequality we derive
EX+
N.(X‚àó
N)p‚àí1 ‚â§(E(X+
N)p)
1
p (E(X‚àó
N)p)
p‚àí1
p
and hence
E(X‚àó
N)p ‚â§
p
p ‚àí1(E(X+
N)p)
1
p (E(X‚àó
N)p)
p‚àí1
p .
It leads to the desirable inequality.
‚ñ†
Problem 12.20 Give an example of a non-right-continuous Ô¨Åltration and a martin-
gale which is not right-continuous.
Solution: For a positive real number a > 0 deÔ¨Åne Œ© = {a, ‚àía}, P(a) = P(‚àía) = 1
2,
and the Bernoulli random variable
X =

a
with probability
1
2,
‚àía
with probability
1
2.
DeÔ¨Åne Xt =

0
i f t ‚â§t0,
X
i f t > t0
, t ‚â•0, and Ft =

{‚àÖ, Œ©}
i f t ‚â§t0,
FX = œÉ(X)
i f t > t0
.
Process (Xt) is a martingale w.r. to (Ft). Both (Xt) and (Ft) are not right-continious.
‚ñ†
Problem 12.21 Let (An)n‚â•0 be a predictable (d √ó d)-matrix-valued sequence of
random variables such that ŒîAn = An ‚àíAn‚àí1 is positive-deÔ¨Åned, n ‚â•1, Œª min(A)
and Œª max(A) are minimal and maximal eigenvalues of A. Let (Mn)n‚â•0 be a d-
dimensional square-martingale with the quadratic characteristic << M, M >>n=
(‚ü®Mi, M j‚ü©n)i,j=1,..,d. Prove an analog of lemma 7.1 for d-dimensional case: (a.s)
{œâ : Œªmin(A‚àû) = ‚àû} ‚à©{œâ : N ‚Üí} ‚à©{œâ : lim
n sup Œª max
Œª min (An) < ‚àû} ‚äÜ{œâ : A‚àí1
n Mn ‚Üí0}.
Hint: Adapt the proof of lemma 7.1 to this multidimensional case.
‚ñ†
Problem 12.22 Let (An) and (Mn) be as in the previous problem, Œª min(An) ‚Üí‚àû
(a.s.), limn sup Œª max
Œª min (An) < ‚àû(a.s.), and (a.s.)
‚àû

n=1
tr (A‚àí1
n )Œî<< M, M >>n(A‚àí1
n ) < ‚àû.

12
Supplementary problems
197
Then (a.s.) A‚àí1
n Mn ‚Üí0, n ‚Üí‚àû.
Hint: Adapt the proof of theorem 7.1 to this multidimensional case.
‚ñ†
Problem 12.23 Let us consider a polynomial transformation Pn(t).Qm(wt) of a
Wiener process (wt)t ‚â•0, where Pn and Qm are polynomials of degrees n and m
relatively. Determine when this transformation leads to a martingale.
Hint: Use the Ito formula.
‚ñ†
Problem 12.24 Let (Xn)n=1,2,... be a sequence of independent random variables
with the density
fa(x) =

e‚àíax
i f x ‚â•0,
0
i f x < 0.
Find the parameter a to provide that Zn = n
i=1 Xi is a martingale w.r. to a natural
Ô¨Åltration (FX
n )n=1,2,....
Hint: Use the conditions
‚à´‚àû
0
fa(x)dx = 1 and EXn = 1.
‚ñ†
Problem 12.25 Assume Xn ‚àºN(Œºn, œÉ2
n), n = 1, 2, ..., and Xn
L2
‚àí‚àí‚ÜíX, n ‚Üí‚àû. Then
X ‚àºN(Œº, œÉ2), where Œº = limn‚Üí‚àûŒºn, œÉ2 = limn‚Üí‚àûœÉ2
n.
Solution: It follows from the L2-convergence that Œºn ‚ÜíŒº = EX and VarXn ‚Üí
VarX = œÉ2, n ‚Üí‚àû. Hence, for an arbitrary Œª ‚ààR1 we obtain
EeiŒªX = lim
n‚Üí‚àûEeiŒªXn = lim
n‚Üí‚àûeiŒºnŒª‚àíœÉ2n
2 Œª2 = eiŒºŒª‚àíœÉ2
2 Œª2.
‚ñ†
Problem 12.26 Let b = b(x) and œÉ = œÉ(x) be bounded functions from R1 to R1 such
thatb ‚ààC1(R1),œÉ ‚ààC2(R1).Assume(Wt)t ‚â•0 isawienerprocess.ThentheItoprocess
dXt = b(Xt)dt + œÉ(Xt).dWt,
can be rewritten in the form of the Stratonovich stochastic integral
dXt = (b(Xt) ‚àí1
2œÉ(Xt)œÉ‚Ä≤(Xt))dt + œÉ(Xt).dWt.
Show also that for Xt = Wt and F ‚ààC2(R1) the Ito formula F(Xt) ‚àíF(X0) =
‚à´t
0 F‚Ä≤(Xs)dXs + 1
2
‚à´t
0 F‚Ä≤‚Ä≤(Xs)ds admits the following form in terms of Stratonovich
integral F(Xt) ‚àíF(X0) =
‚à´t
0 F‚Ä≤(Xs).dXs.
Hint: Use deÔ¨Ånitions of these integrals.
‚ñ†

198
12
Supplementary problems
Problem 12.27 Let us consider the exponential transformation of a probability
measure PT, T > 0 to a measure function P‚àó
T such that
dP‚àó
T
dPT
= eaW ‚àí
T +bT+c = ZT,
where (Wt)t ‚â§T is a Wiener process. Determine parameters a, b, and c under which
P‚àó
T will be a probability measure.
Hint: Use the condition EZT = 1.
‚ñ†
Problem 12.28 ProvethattheÔ¨ÅrstandsecondvariationsofaWienerprocess(Wt)t ‚â•0
converge (a.s.) to inÔ¨Ånity and to the length of time interval correspondingly.
Solution: For a Ô¨Åxed T > 0 we deÔ¨Åne a subdivision tn
i = iT.2‚àín, i ‚â§2n, n ‚â•1 of
the interval [0,T]. DeÔ¨Åne
FVn =
2n‚àí1

i=0
|Wtn
i+1 ‚àíWtn
i | =
2n‚àí1

i=0
|ŒîWtn
i |
and
SVn =
2n‚àí1

i=0
(ŒîWtn
i )2,
n ‚â•1.
For SVn using properties of (Wt) we have
E(SVn ‚àíT)2 =E
2n‚àí1

i=0

(ŒîWn
ti)
2
‚àíT.2‚àín2
=
2n‚àí1

i=0
E

(ŒîWn
ti)
2
‚àíT.2‚àín2
=2n.2.(T.2‚àín)2
=2.T2.2‚àín
and
  
‚àû

n=1
|SVn ‚àíT|
  
L2 ‚â§
‚àû

n=1
‚à•SVn ‚àíT‚à•L2
‚â§
‚àö
2T.
‚àû

n=1
2
‚àín
2 < ‚àû.
Hence, SVn ‚ÜíT (a.s.) as n ‚Üí‚àûdue to ‚àû
n=1 |SVn ‚àíT| < ‚àû(a.s.).
Regarding
FVn
for
each
œâ ‚ààŒ©
we
have
SVn(œâ) ‚â§(maxi |ŒîWtn
i (œâ)|).
2n‚àí1
i=0
|ŒîWtn
i (œâ)| ‚â§(maxi |ŒîWtn
i (œâ)|).FVn(œâ). It follows from this inequality for
paths of (Wt)t ‚â•0 that limn‚Üí‚àûinf FVn(œâ) cannot be Ô¨Ånite, otherwise SVn ‚àí‚àí‚àí‚àí‚Üí
n‚Üí‚àû0.

12
Supplementary problems
199
‚ñ†
Problem 12.29 Let (Mt)t ‚â•0 be a continuous square integrable martingale with a
Ô¨Ånite Ô¨Årst variation FVt(M). Show that Mt = M0 (a.s.) for all t ‚â•0.
Hint: Let (tn
j ) be a Ô¨Ånite partition of [0, t] with maxj Œîtn
j ‚Üí0 as n ‚Üí‚àû. Then (a.s.)

j
|ŒîMtn
j |2 ‚â§FVt(M(œâ)). max
j
|ŒîMtn
j (œâ)| ‚Üí0,
n ‚Üí‚àû,
and
< M, M >t= 0
(a.s.).
Hence, Mt = M0 (a.s.).
‚ñ†
Problem 12.30 Let M = (Mt)t ‚â•0 be a continuous local martingale and
Zt(M) = exp{Mt ‚àí1
2 ‚ü®M, M‚ü©t},
t ‚àà[0, ‚àû].
Assume that (Krylov‚Äôs condition):
lim
Œµ‚Üí0 inf Œµ. log E exp{1 ‚àíŒµ
2
‚ü®M, M‚ü©‚àû} < ‚àû.
Prove that EZ‚àû(M) = 1, i.e. the exponential local martingale (Zt(M))t ‚â•0 is uniformly
integrable. Show that the condition above is wider than the Novikov condition
Ee
1
2 ‚ü®M,M ‚ü©‚àû< ‚àû.
Hint: First of all we note that EZ‚àû(M) ‚â§1 as a consequence of a supermartingale
property of (Zt(M)) and the Fatou lemma. Using the H√∂lder inequality we can derive
for a constant c > 0 that
1 = EZ‚àû((1 ‚àíŒµ)M) =Ee(1‚àíŒµ)(M‚àû‚àí1
2 ‚ü®M,M ‚ü©‚àû).e
(1‚àíŒµ)Œµ
2
‚ü®M,M ‚ü©‚àû.I{‚ü®M,M ‚ü©‚àû‚â§c}
+Ee(1‚àíŒµ)(M‚àû‚àí1
2 ‚ü®M,M ‚ü©‚àû).e
(1‚àíŒµ)Œµ
2
‚ü®M,M ‚ü©‚àû.I{‚ü®M,M ‚ü©‚àû>c}
‚â§(EZ‚àû(M))1‚àíŒµ.

Ee
(1‚àíŒµ)Œµ
2
‚ü®M,M ‚ü©‚àû.I{‚ü®M,M ‚ü©‚àû‚â§c}
Œµ
+(EZ‚àû(M).I{‚ü®M,M ‚ü©‚àû>c})1‚àíŒµ.

Ee
(1‚àíŒµ)Œµ
2
‚ü®M,M ‚ü©‚àû
Œµ
.
By taking the limit when Œµ ‚Üí0 in the above inequality, we arrive to
1 ‚â§EZ‚àû(M) + const.EZ‚àû(M).I{‚ü®M,M ‚ü©‚àû>c}.
Taking the limit as c ‚Üí‚àû, we get EZ‚àû(M) ‚â•1.
‚ñ†
Problem 12.31 Show that every process (At) with Ô¨Ånite variation can be represented
as the diÔ¨Äerence of two increasing processes.

200
12
Supplementary problems
Hint: Use the representation
At = 1
2(|A|t + At ‚àí1
2(|A|t ‚àíAt)),
where |A|t is the variation of A on [0, t].
‚ñ†
Problem 12.32 Let (Xt)t ‚â•0 be a semimartingale and let (At) be a process of Ô¨Ånite
variation. Prove that
[X, A]t =

s‚â§t
(ŒîXt)(ŒîAs).
In particular, [X, A] = 0, if (At) or (Xt) is continuous.
Hint: Use the limiting arguments dividing [0, t] by a subdivision tn
j = jt2‚àín,
j = 0, ..., 2n, n ‚â•1.
‚ñ†
Problem 12.33 Prove the Levy characterization of a Wiener process (Remark 8.1).
Solution: Let us prove that the following three statements are equivalent for a con-
tinuous martingale (Xt)t ‚â•0, X0 = 0:
(1) Process (Xt) is a standard Wiener process on the underlying stochastic basis;
(2) Process (X2
t ‚àít)t ‚â•0 is a martingale;
(3) Process (Xt)t ‚â•0 has a quadratic variation [X, X]t = t.
To prove (1) ‚áí(2) we just observe that E|X2
t ‚àít| < ‚àûfor each t ‚â•0 and for s ‚â§t,
E(X2
t ‚àít|Fs) = E(X2
t ‚àíX2
s ) + X2
s ‚àí(t ‚àís) ‚àís|Fs
 = X2
s ‚àís due to the properties
of a Wiener process (Xt).
For the proof of the second implication (2) ‚áí(3) we note that (X2
t )t ‚â•0 is a submartin-
gale with the Doob-Meyer decomposition X2
t = (X2
t ‚àít) + t. Hence, [X, X]t =<
X, X >t= t.
The third implication (3) ‚áí(1) can be stated as follows. First, we prove that incre-
ments of (Xt)t ‚â•0 are Gaussian. Applying the Ito formula to eiŒªx+ Œª2
2 t = f (t, x), t ‚â•0,
and Œª > 0, we obtain
df (t, Xt) = ‚àÇ
‚àÇt f (t, Xt)dt + ‚àÇ
‚àÇx f (t, Xt)dXt + 1
2
‚àÇ2
‚àÇx2 f (t, Xt)d[X, X]t
=1
2Œª2 f (t, Xt)dt + iŒª f (t, Xt)dXt + (‚àíŒª2
2 ) f (t, Xt)dt
=iŒª f (t, Xt)dXt,
and therefore, ( f (t, Xt))t ‚â•0 is a martingale. Further
E(eiŒªXt+ 1
2 Œª2t|Fs) = eiŒªXs+ 1
2 Œª2s,
and
EeiŒª(Xt‚àíXs) = E(E(eiŒª(Xt‚àíXs)|Fs)) = e‚àí1
2 Œª2(t‚àís),

12
Supplementary problems
201
which means that Xt ‚àíXs ‚àºN(0, t ‚àís), t ‚â•s.
To Ô¨Ånish the proof we show that the increments Xt2 ‚àíXt1, ..., Xtn ‚àíXtn‚àí1 are indepen-
dent for any subdivision of time 0 ‚â§t1 < t2 < t3 < ... < tn‚àí1 < tn. We have
E(eiŒª,Xt1+iŒª2(Xt2‚àíXt1)+...+iŒªn(Xtn ‚àíXtn‚àí1)) = e‚àí
Œª2
1
2 t1 √ó ... √ó e‚àí1
2 Œª2
n(tn‚àítn‚àí1),
which states independence of increments of (Xt). So, (Xt)t ‚â•0 is a Wiener process.
‚ñ†
Problem 12.34 Prove that any submartingale X = (Xt)t ‚â•0 on a standard stochastic
basis (Œ©, F, (Ft)t ‚â•0, P) admits the right-continuous modiÔ¨Åcation, if EXt is right-
continuous.
Hint: Using limiting arguments together with a submartingale property we have
Xt ‚â§E(Xt+|Ft) (a.s.), t ‚â§0. But Ft = Ft+, hence Xt ‚â§Xt+ (a.s.), and due to EXt =
EXt+ we get the result.
‚ñ†
Problem 12.35 Let
M = (Mt)t ‚â•0
be
a
martingale
on
a
stochastic
basis
(Œ©, F, (Ft)t ‚â•0, P). Show that a predictability property for the process œÜ = (œÜt) is
vital in getting the martingale property for a stochastic integral
‚à´t
0 œÜsdMs.
Hint: Take a rich enough probability space (Œ©, F, P) to accommodate two random
variables: œÑ ‚â•0 with P(œÑ ‚â§t) = t ‚àß1 and a Bernoulli random variable Y such that
P(Y = 1) = P(Y = ‚àí1) = 1
2. DeÔ¨Åne Mt = Y.I{œÑ‚â§t } and Ô¨Åltration Ft = FM
t . In this
case
‚à´t
0 MsdMs is not a martingale.
‚ñ†
Problem 12.36 Let continuous processes A ‚ààA+
loc and M ‚ààMloc are deÔ¨Åned on a
standard stochastic basis (Œ©, F, (Ft)t ‚â•0, P). Assume that bi = bi(x), x ‚ààR1, i = 1, 2,
are bounded continuous functions and Xi, i = 1, 2, are (strong) continuous solutions
of the stochastic diÔ¨Äerential equations w.r. to a semimartingaleYt = At + Mt,Y0 = 0 :
dXi
t = bi(Xi
t )dAt + dMt,
where Xi
0 = x ‚ààR1.
Prove that the inequality b1(x) < b2(x) for all x ‚ààR1 implies X1
t (œâ) ‚â§X2
t (œâ) (a.s.)
for all t ‚â•0.
Hint: Apply the method of proof similar to Lemma 9.1.
‚ñ†
Problem 12.37 Consider a stochastic diÔ¨Äerential equation as in problem 12.36
dXt = b(Xt)dAt + dMt,
where X0 = x ‚ààR1, t ‚â§T.
Assume that b1 = b(x), x ‚ààR1 satisÔ¨Åes conditions of Theorem 9.3 and A <<
‚ü®M, M‚ü©. Prove that this equation admits at least one (strong) solution.

202
12
Supplementary problems
Hint: Adapt the method of proof of Theorem 9.3 for this case.
‚ñ†
Problem 12.38 Let dXt = Œºdt + œÉdWt + ŒΩdNt, where W and N be a Wiener pro-
cess and a Poisson process, respectively, with intensity Œª > 0, Œº, œÉ, ŒΩ ‚ààR1. Let (œÑi)
be the moments of the jumps of N. Prove the Ito formula for F ‚ààC2:
F(Xt) = F(X0) +
‚à´t
0
F‚Ä≤(Xs)dXs + 1
2
‚à´t
0
F‚Ä≤‚Ä≤(Xs‚àí)œÉ2ds ‚àí
Nt

i=1
F‚Ä≤(XœÑi‚àí)ŒîXœÑi +
Nt

i=1
(F(XœÑi ) ‚àíF(XœÑi‚àí)).
Hint: Adapt the Ito formula for semimartingales in this case.
‚ñ†
Problem 12.39 Let N = (Nt)t ‚â•0 be a Poisson process with intensity Œª > 0 and
Œ± = (Œ±t) be a bounded deterministic function.
DeÔ¨Åne the process
Lt = exp{
‚à´t
0
Œ±sd(Ns ‚àíŒªs) +
‚à´t
0
(1 + Œ±s ‚àíeŒ±s)Œªds}
and prove that L = (L0
t )t ‚â•0 is a martingale satisfying the equation
dLt = Lt‚àí(eŒ±t ‚àí1)d(Nt ‚àíŒªt).
Hint: Use the Ito formula.
‚ñ†

References
1. Baldi P.: An introduction through theory and exercises.tochastic calculus. universitext, (2017)
2. Beiglboeck M., Schachermayer W., and Veliyev B.: A short proof of the doob-meyer theorem.
Stochastic Processes and their applications, vol. 122, no. 4, pp. 1204-1209, (2012)
3. Bishwal., Jaya PN.: Parameter Estimation in Stochastic Differential Equations. Springer-
Verlag, Berlin- Heidelberg, (2008)
4. Borkar., Vivek S.: Stochastic approximation: A dynamical systems viewpoint. Cambridge Uni-
versity Press, Cambridge, (2008)
5. Borodin., A. N.: Stochastic processes. Birkhauser, (2018)
6. Bulinski, A. V., Shiryayev, A. N.: Theory of stochastic processes. Fizmatlit, Moscow, (2005)
7. √áinlar, E.: Probability and Stochastics. Springer, vol. 261, (2011)
8. Cohen, S. N., Elliott, R. J.: Stochastic calculus and applications. 2nd Edition, Springer-Science,
(2015)
9. Dol√©ans‚ÄìDade, C.: Stochastic processes and stochastic differential equations. in Stochastic
Differential Equations, Springer-Verlag, pp. 7-73, (2010)
10. Durrett, R.: Essentials of Stochastic Processes. 3rd Edition. Springer, (2018)
11. Eberlein, E., and Kallsen, J.: Mathematical Finance. Springer, (2019)
12. Edgar, G. A., Sucheston, L.: Amarts: A class of asymptotic martingales. a. discrete parameter.
Journal of Multivariate Analysis, vol. 6, pp. 193-221, (1976)
13. Etheridge, A.: A Course in Financial Calculus. Cambridge University Press, (2002)
14. Ikeda, N., and Watanabe, S.: Stochastic Differential Equations and Diffusion Processes. 2nd
Edition. North-Holland, (1989)
15. Jacod, J., and Protter, P.: Probability Essentials. 2nd Edition. Springer, (2003)
16. Kallianpur, G., and Karandikar, R. L.: Introduction to option pricing theory. Springer Science
& Business Media, (2012)
17. Karatzas, I., and Shreve, S.: Brownian motion and stochastic calculus. Springer, New York,
(1998)
18. Klebaner, F. C.: Introduction to stochastic calculus with applications. World ScientiÔ¨Åc Pub-
lishing Company, (2012)
19. Kolmogorov, A. N.: Foundations of the Theory of Probability. 2nd Edition. Chelsea, New York,
(1956)
20. Kruglov, V. M.: Stochastic Processes. Academy, Moscow, (2013)
21. Krylov, N. V.: Introduction to the theory of random processes. Providence: American Mathe-
matical Soc., (2002)
22. Krylov, N. V.: Controlled diffusion processes. Springer-Verlag, (1980)
¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to
Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3
203

204
References
23. Lamberton, D., and Lapeyre, B.: Introduction to Stochastic Calculus Applied to Finance. Chap-
man & Hall/CRC, (1996)
24. Le Gall, J.-F.: Brownian motion, martingales, and stochastic calculus. Springer, (2016)
25. Liptser, R. Sh., and Shiryaev, A. N.: Statistics of random processes. Springer, 2nd Ed, (2001)
26. Liptser, R. Sh, and Shiryaev, A. N.: Theory of martingales. Kluwer Academic Publishers,
(1989)
27. Melnikov, A. V.: On solutions of stochastic equations with driving semimartingales. Proceeding
of the third European young statisticians meeting, Catholic University, Leuven, pp. 120-124,
(1983)
28. Melnikov, A. V.: On strong solutions of stochastic differential equations with nonsmooth coef-
Ô¨Åcients. Theory Probab. Appl., vol. 24, no. 1, pp. 146-149, (1979)
29. Melnikov, A. V.: On the theory of stochastic equations in components of semimartingles.
Sbornik Math, vol. 38, no. 3, pp. 381-394, (1981)
30. Melnikov,A.V.:Stochasticdifferentialequations:SingularityofcoefÔ¨Åcients,regressionmodels
and stochastic approximation. Russian Math Surveys, vol. 51, no. 5, pp. 43-136, (1996)
31. Melnikov, A. V., and Novikov, A. A.: Sequential inferences with Ô¨Åxed accuracy for semimartin-
gales. Theory of Probability & Its Applications, vol. 33, no. 3, pp. 446-459, (1989)
32. Melnikov, A. V., and Shiryayev, A. N.: Criteria for the absence of arbitrage in the Ô¨Ånancial
market. Frontiers in pure and applied probability II: proceedings of the Fourth Russian-Finnish
Symposium on Probability Theory and Mathematical Statistics, pp. 121-134, (1996)
33. Meyer, P.-A.: Probability and potential. Blaisdell Publ.Company, (1966)
34. Nevel‚Äôson, M. B., and Has‚Äôminskii, R. Z.: Stochastic approximation and recursive estimation.
AMS, Providence, (1976)
35. √òksendal, B.: Stochastic differential equations. 5th Edition. Springer, (2000)
36. Protter, P. E.: Stochastic Integration and Differential Equations. 2nd Edition. Springer, (2005)
37. Revuz, D., and Yor, M.: Continuous Martingales and Brownian Motion. 2nd Edition. Springer-
verlag, (1999)
38. Schachermayer, W., and Teichmann, J.: How close are the option pricing formulas of bachelier
and black-merton-scholes?. Mathematical Finance, vol. 18, no. 1, pp. 155-170, (2008)
39. Shiryaev, A. N.: Essentials of Stochastic Finance. World ScientiÔ¨Åc, (1999)
40. Shiryaev, A. N.: Probability. 2nd Edition. Springer, (1996)
41. Skorokhod, A. V.: Lectures on the theory of stochastic processes. Utrecht: VSP, (1996)
42. Tikhonov, A. N., Vasileva, A. B., and Sveshnikov, A. G.: Differential Equations. Springer,
(1985)
43. Valkeila, E., and Melnikov, A. V.: Martingale models of stochastic approximation and their
convergence. Theory of Probability & Its Applications, vol. 44, no. 2, pp. 333-360, (2000)
44. Wentzell, A. D.: A Course in the Theory of Stochastic Processes. McGraw-Hill, (1981)
45. Williams, D.: Probability and Martingales. Cambridge University Press, (1991)

Index
A
Absolute continuity, 41
local, 74
Absolute continuity of measures, 41
Accessible stopping time, 140
Algebra, 2
Atom, 46
Autoregression model, 68, 187
B
Bachelier
discrete model, 75
formula, 75
Bank account, 75
Bernoulli distribution, 7
Binomial market model, 180
Black-Scholes
formula, 129
model, 129
Borel function, 143
Borel space, 5, 13
Borel-Cantelli lemma, 89
Brownian motion, 82
C
Cadlag process, 142
Call option, 127, 128
Cantor function, 9
Capital of strategy, 176
Cauchy-Schwartz inequality, 27
Central Limit Theorem, 31
Change of measure, 53
Change of time, 50
Change of variables formula, 19
Characteristic function, 36
Chebyshev inequality, 26
Class D, 139
Comparison theorem, 113
Compensator, 52, 146, 147
Complete probability space, 83
Conditional expectation, 43
Controlled diffusion process, 132
Convergence
weak, 38
Convergence of Random Variables, 28
Cox-Ross-Rubinstein model, 180
Cylinder, 6
D
Debut, 142
Diffusion coefÔ¨Åcient, 120
Diffusion process, 120
Dirichlet function, 19
Discrete distribution, 7
Discrete stochastic integral, 53
Distribution
Bernoulli, 7
Binomial, 7
density function, 10
discrete, 7
Ô¨Ånite-dimensional, 118
function, 7
Normal, 8
Poisson, 8
Uniform, 7, 8
Doleans exponent, 168
Doob decomposition, 52
Doob inequalities, 144
¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to
Springer Nature Switzerland AG 2023
A. Melnikov, A Course of Stochastic Analysis, CMS/CAIMS Books in Mathematics 6,
https://doi.org/10.1007/978-3-031-25326-3
205

206
Index
Doob-Meyer decomposition, 146
Downcrossing, 58
Drift coefÔ¨Åcient, 114
E
Elementary event, outcome, 1
Equivalence of measures, 104
Existence of a solution, 109
Expectation, 18
Extended Random Variable, 13
F
Fair price, 127
Fatou lemma, 22
Feynman-Kac representation, 123
Filtration, 50
Financial market, 75, 129
Finite-additive measure, 2
Finite-dimensional
process, 97
Finite-dimensional distribution, 118
First variation, 86
Fisk decomposition, 158
Fixed accuracy property, 175
Fokker-Planck equation, 121
Formula
Bachelier, 76
Ito, 98
Merton, 180
Function
Borel, 15
Cantor, 9
characteristic, 36
Dirichlet, 19
Functional space, 81
Fundamental sequence, 93
G
Gaussian process, 90
General theory of stochastic processes, 139
Generalized martingale, 80
Generator, 120
Geometric Brownian Motion, 112, 129
Girsanov theorem, 74
Graph, 140
Gronwall lemma, 109
H
Haar system, 88
Hamilton-Jacobi-Bellman principle, 133
Hamilton-Jacoby-Bellman equation, 138
Helly principle, 32
I
Incomplete market, 176
Independent Random Variables, 16
Indicator, 13
Indistinguishable processes, 141
Inequality
Cauchy-Schwartz, 26
Chebyshev, 26
Jensen, 26
Kolmogorov-Doob, 56
Kunite-Watanabe, 160
Information Ô¨Çow, 50
Integrable, 18
Integral
Lebesgue, 19
Interval of non-arbitrage prices, 135
Inverse image, 13
Isometric property, 92
Ito formula, 98
Ito process, 98
Ito stochastic integral, 97
J
Jensen inequality, 26
Joint Quadratic characteristic, 53
Jump-diffusion model, 180
K
Kolmogorov
backward, forward equation, 121
consistency theorem, 6
variance condition, 67
Kolmogorov-Chapman equation, 118
Kolmogorov-Doob inequality, 56
Kunita-Watanabe
decomposition, 54
inequality, 160
Kurtosis, 20
L
Large Numbers Law, 28
Least-squares estimate, 68
Lebesgue
dominated convergence theorem, 23
integral, 19
Lebesgue measure, 5
Levy theorem, 64

Index
207
Lindeberg condition, 39
Lipschitz condition, 108
Local Lipschitz conditions, 108
Local martingale, 80, 151
Localization, 96
Localizing sequence, 80
Locally square-integrable martingale, 151
Lower option price, 135
LS-estimate, 68
M
Markov process, 117
Martingale, 51
generalized, 80
local, 80, 151
locally square-integrable, 151
Square integrable, 52
Martingale difference, 51
Martingale measure, 127
Martingale representation, 104
Mathematical Ô¨Ånance, 112, 126, 127
Maturity time, 127
Measurable
mapping, 16
Measurable space, 4
Measure, 1
Ô¨Ånite-additive, 2
Lebesgue, 5
Martingale, 127
Wiener, 11
Merton formula, 180
Method of monotonic approximations, 113
ModiÔ¨Åcation
continuous, 84
right-continuous, 87
Monotonic class, 4
Multiplication rule, 182
N
Normal distribution, 8
Novikov condition, 75
O
Optimal control, 133
Option
call, 75
Optional Sampling Theorem, 55
Optional sigma-algebra, 142
Ornstein-Uhlenbeck process, 112
P
Parseval identity, 90
Partial Differential Equation, 123
Poisson distribution, 8
Polynomial, 101
Predictable process, 143
Predictable sequence, 52
Predictable sigma-algebra, 142
Predictable stopping time, 141
Principle of optimality, 133
Probability measure, 3
Probability space, 3, 5
Process with Ô¨Ånite variation, 143
Progressively measurable, 98
Prokhorov theorem, 34
Purely discontinuous martingale, 150
Put-call parity, 129
Q
Quadratic bracket, 151
Quadratic characteristic, 52, 151
Quasimartingale, 79
R
Radon-Nikodym
density,derivative, 42
theorem, 42
Random set, 142
Random variable, 13
extended, 13
uniformly integrable, 23
Regression
analysis, 67
function, 71
Regression model, 68
Relative compactness, 34
Replicating strategy, 176
S
Sample path, 49
Scalar product, 88
Schauder system, 88
Semimartingale, 155
Sequential estimate, 132
Sigma-algebra, 4
optional, 142
predictable, 142
Singular distribution, measure, 8
Skewness, 20
Small perturbations method, 136
Step function, 91

208
Index
Stochastic Approximation, 71, 183
Stochastic basis, 55
Stochastic calculus, 159
Stochastic Differential Equation, 107
Stochastic exponential, 54, 168
multiplication rule, 168
Stochastic interval, 142
Stochastic Kronecker‚Äôs Lemma, 66
Stochastic regression analysis, 182
Stochastic volatility, 134
Stochastically continuous process, 87
Stock price, 46, 112
Stopped process, 154
Stopping time, 50
accessible, 140
graph, 140
predictable, 140
totally inaccessible, 141
Strategy, 176
Stratonovich integral, 97
Strong solution, 113
Subdivision, 83
Submartingale, 52
Supermartingale, 52
T
Theorem
Caratheodory, 5
CLT, 38
Comparison, 113
Girsanov, 74
Kolmogorov consistency, 6
Lebesgue dominated convergence, 27
Levy, 64
Optional Sampling, 55
Prokhorov, 34
Radon-Nikodym, 42
Theory of Probability, 1
Tightness, 34
Totally inaccessible stopping time, 141
Transition probability function, 118
U
Uniform distribution, 8
Uniformly integrable random variable, 23
Uniqueness of solutions, 112, 132
Upcrossing, 57
Upper option price, 135
Usual conditions, 139, 144
V
Value function, 133
Variance, 20
W
Wald identity, 95
Weak convergence, 38
Weak solution, 113
Wiener measure, 11


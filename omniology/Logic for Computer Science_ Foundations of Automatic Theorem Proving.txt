Logic For Computer Science
Foundations of
Automatic Theorem Proving
Copyright 2003, Jean H. Gallier
June 2003

Jean Gallier
University of Pennsylvania
Department of Computer and Information Science
200 South 33rd Street
Philadelphia, Pa 19104
USA
e-mail: jean@saul.cis.upenn.edu

To Anne, my wife,
Mia, Philippe and Sylvie, my children,
and my mother, Simone


Preface (2003 Edition)
This is a slighty revised version of the 1985 edition of my logic book. Many ty-
pos and errors have been corrected and the line drawings have been improved.
Most mistakes were minor, except for a subtle error in Theorem 4.3.3. Indeed,
the second part of the theorem about the complexity of the proof tree T ob-
tained from a resolution refutation D is false: It is not necessarily the case
that the number of leaves of T is less than or equal to the number of resolution
steps in D. As a consequence, Lemma 4.3.4 is also false.
In this revised edition, we simply removed all statements about the com-
plexity of the conversion of resolution refutations into proof trees. Hopefully,
this part of the book is now correct, as well as the rest of it!
Some parts of the book have now aged, in particular, the parts about
PROLOG. Also, eighteen years later, I would prefer to present some of the
material in a diﬀerent order and in a diﬀerent manner. In particular, I would
separate more clearly the material on the resolution method (Chapter 4) from
the more proof-theory oriented material. However, I consider this too big a
task and this mildly revised version will have to do (at least, for now!).
Ideally, a number of topics should also be covered, for example, some
basics of constructive logic, linear logic, temporal logic and model checking.
Again, this is too formidable a task for a single individual and I hope that
readers enjoy this new version of my book anyway.
It should be noted that this book is of “preTEX vintage.” This means
that LaTEX was not available at the time this book was written, which implies
that I had to write macros (stolen and adapted from D. Knuth blue TEX
book) to do chapter headings, etc. I also had to adapt the infamous macro
v

vi
Preface (2003 Edition)
makeindex to produce the dreaded index (which was turned into ﬁnal form
using a PASCAL program!). Thus, I indulged in the practice of the change of
catcode to turn the symbol ˆ into an active character, among other strange
things! Nevertheless, I am grateful to Knuth for producing TEX. Without it,
this book would not be alive.
In retrospect, I realize how much I was inspired by, and thus, how much
I owe, Gerhard Gentzen, Jacques Herbrand, Stephen Kleene, Gaisi Takeuti,
Raymond Smullyan, Peter Andrews and last but not least, Jean-Yves Girard.
You have my deepest respect for your seminal and inspiring work and I thank
all of you for what you taught me.
Philadelphia, June 2003
Jean Gallier

Preface (1985 Edition)
This book is intended as an introduction to mathematical logic, with an em-
phasis on proof theory and procedures for constructing formal proofs of for-
mulae algorithmically.
This book is designed primarily for computer scientists, and more gen-
erally, for mathematically inclined readers interested in the formalization of
proofs, and the foundations of automatic theorem-proving.
The book is self contained, and the level corresponds to senior under-
graduates and ﬁrst year graduate students. However, there is enough material
for at least a two semester course, and some Chapters (Chapters 6,7,9,10) con-
tain material which could form the basis of seminars. It would be helpful, but
not indispensable, if the reader has had an undergraduate-level course in set
theory and/or modern algebra.
Since the main emphasis of the text is on the study of proof systems and
algorithmic methods for constructing proofs, it contains some features rarely
found in other texts on logic. Four of these features are:
(1) The use of Gentzen Systems;
(2) A Justiﬁcation of the Resolution method via a translation from a
Gentzen System;
(3) A presentation of SLD-resolution and a presentation of the foundations
of PROLOG;
(4) Fast decisions procedures based on congruence closures.
vii

viii
Preface (1985 Edition)
A fruitful way to use this text is to teach PROLOG concurently with the
material in the book, and ask the student to implement in PROLOG some of
the procedures given in the text, in order to design a simple theorem-prover.
Even though the main emphasis of the book is on the design of proce-
dures for constructing formal proofs, the treatment of the semantics is per-
fectly rigorous. The following paradigm has been followed: Having deﬁned
the syntax of the language, it is shown that the set of well-formed formulae
is a freely generated inductive set. This is an important point, which is often
glossed over. Then, the concepts of satisfaction and validity are deﬁned by
recursion over the freely generated inductive set (using the “unique homo-
morphic extension theorem”, which can be rigorously justiﬁed). Finally, the
proof theory is developped, and procedures for constructing proofs are given.
Particular attention is given to the complexity of such procedures.
In our opinion, the choice of Gentzen systems over other formal systems
is pedagogically very advantageous. Gentzen-style rules reﬂect directly the
semantics of the logical connectives, lead naturally to mechanical proof pro-
cedures, and have the advantage of having duality “built in”. Furthermore, in
our opinion, Gentzen systems are more convenient than tableaux systems or
natural deduction systems for proof-theoretical investigations (cut-free proofs
in particular).
In three years of teaching, I have found that Gentzen-like
systems were very much appreciated by students.
Another good reason for using a formal system inspired from Gentzen (a
sequent calculus), is that the completeness theorem is obtained in a natural
and simple way. Furthermore, this approach even yields a program (the search
procedure) for constructing a proof tree for a valid formula. In fact, in our pre-
sentation of the completeness theorem (inspired by Kleene, Kleene 1967), the
search for a proof tree is described by a program written in pseudo-PASCAL.
We also show how a proof procedure for ﬁrst-order logic with equality can be
developed incrementally, starting with the propositional case.
The contents of the book are now outlined.
Chapter 1 sets the goals of the book.
Chapter 2 has been included in order to make the book as self contained
as possible, and it covers the mathematical preliminaries needed in the text.
It is recommended to refer to this Chapter only when needed, as opposed to
reading it entirely before proceeding to Chapter 3.
Propositional logic is studied in Chapter 3. This includes the syntax and
semantics of propositional logic. Gentzen systems are introduced as a method
for attempting to falsify a proposition. The completeness theorem is shown
as well as some of its consequences (the conjunctive and disjunctive normal
forms). By introducing inﬁnite sequents, the extended completeness theorem
and the compactness theorem are obtained. An informal exposition of the
complexity classes P, NP, and of the concept of NP-completeness is given at
the end of the Chapter.

Preface (1985 Edition)
ix
The resolution method for propositional logic is presented in Chapter
4. This Chapter uses a new approach for proving the completeness of resolu-
tion. Completeness is obtained by deﬁning a special Gentzen System whose
completeness follows easily from the results of Chapter 3, and giving an al-
gorithm for converting proofs in the special Gentzen Systems into resolution
refutations. Some complexity issues are also examined.
Chapter 5 is devoted to ﬁrst-order logic. The syntax and semantics are
presented. This includes the notions of ﬁrst-order languages, structures, and
models. Gentzen systems are extended to deal with quantiﬁers and equal-
ity. The concept of a Hintikka set is also introduced. It is shown that every
Hintikka set is satisﬁable in a model whose domain is (a quotient of) a term
algebra. This result, together with a generalization of the search procedure, is
used to derive the main theorems of ﬁrst-order logic: completeness, compact-
ness, model existence, L¨owenheim-Skolem theorems. One of the main themes
in this Chapter is that the search procedure is a “Hintikka set builder”.
Chapter 6 is devoted to Gentzen’s “Cut elimination Theorem” and some
of its applications. A simple semantic proof derived from the completeness
theorem is given for the Gentzen System LK. An entirely proof-theoretic (con-
structive) argument is also given for a simpler system G1nnf. This proof due
to Schwichtenberg has the advantage that it also yields a precise upper bound
on the length of cut-free proofs obtained from a proof with cut. This result
is then extended to a system with equality.
A constructive proof of Craig’s Interpolation Theorem is given, and
Beth’s Deﬁnability Theorem and Robinson’s Joint Consistency Theorem are
also proved. This Chapter contains more advanced material than the previous
Chapters.
Chapter 7 is devoted to Gentzen’s “Sharpened Hauptsatz”, Herbrand’s
Theorem, and the Skolem-Herbrand-G¨odel Theorem.
As Chapter 6, this
Chapter contains more advanced material. Gentzen’s “Sharpened Hauptsatz”
for prenex sequents is proved constructively, using proof transformation tech-
niques. A version of the “Sharpened Hauptsatz” is also proved constructively
for sequents consisting of formulae in NNF. To prove this result, a new Gentzen
system with quantiﬁer rules applying to certain subformulae is deﬁned. This
version of the “Sharpened Hauptsatz” for sequents in NNF appears to be
new. Using these results, constructive versions of Herbrand’s Theorem are
proved, as well as Andrews’s version of the Skolem-Herbrand-G¨odel Theorem
(Andrews 1981). The class of primitive recursive functions and the class of
recursive functions are also brieﬂy introduced.
In Chapter 8, the resolution method for ﬁrst-order logic is presented. A
recursive uniﬁcation algorithm inspired from Robinson’s algorithm (Robinson
1965) is presented. Using results from Chapter 4 and the Skolem-Herbrand-
G¨odel Theorem, the completeness of ﬁrst-order resolution is
shown, using the “lifting technique”. Paramodulation is also brieﬂy discussed.

x
Preface (1985 Edition)
Chapter 9 is devoted to SLD-resolution and the foundations of PRO-
LOG. Using techniques from Chapter 4, the completeness of SLD-resolution is
shown, by translating proofs in a certain Gentzen system into SLD-refutations.
This approach appears to be new. Logic programs are deﬁned, and a model-
theoretic semantics is given. It is shown that SLD-resolution is a sound and
complete computational proof procedure for logic programs. Most of this ma-
terial can only be found in research papers, and should be useful to readers
interested in logic programming.
In Chapter 10 (the last Chapter), a brief presentation of many-sorted
ﬁrst-order logic is given. This presentation should be suﬃcient preparation
for readers interested in the deﬁnition of abstract data types, or computing
with rewrite rules. Finally, an extension of the congruence closure method of
Nelson and Oppen (Nelson and Oppen 1980) to the many-sorted case and its
application to fast decision procedures for testing the validity of quantiﬁer-free
formulae are presented.
This book grew out of a number of class notes written for a graduate
course in logic for computer scientists, taught at the University of Pennsylva-
nia (CIS581). The inspiration for writing the book came from Sheila Greibach
(my advisor at UCLA) and Ronald Book (my “supervisor” at UCSB, while I
was a “Post Doc”), who convinced me that there is no better way to really
know a topic than writing about it.
I wish to thank my colleagues Saul Gorn, Dale Miller and Alex Pelin
for reading the manuscript very carefully, and for many helpful comments.
I also wish to thank my students William Dowling, Tomas Isakowitz, Harry
Kaplan, Larry Krablin, Francois Lang, Karl Schimpf, JeﬀStroomer, Stan
Raatz and Todd Rockoﬀfor their help in “debugging” the manuscript. This
includes reporting of typos, stylistic improvements, additional exercises, and
correction of mistakes.
Philadelphia, July 1985
Jean Gallier

How To Use This
Book As A Text
This book is written at the level appropriate to senior undergraduate and ﬁrst
year graduate students in computer science, or mathematics. The prereque-
sites are the equivalent of undergraduate-level courses in either set theory,
abstract algebra, or discrete structures.
All the mathematical background
necessary for the text itself is contained in Chapter 2, and in the Appendix.
Some of the most diﬃcult exercises may require deeper knowledge of abstract
algebra.
Most instructors will ﬁnd it convenient to use Chapter 2 on a “call by
need” basis, depending on the background of the students. However, to the
authors’s experience, it is usually desirable to review the material contained
in Sections 2.1, 2.2 and 2.3.
To help the instructor make up a course, at the end of this section we give
a graph showing the dependence of the Sections and Chapters. This graph
only applies to the text itself, but not to the exercises, which may depend on
any earlier Sections.
The core of the subject which, in the author’s opinion, should be part
of any course on logic for computer science, is composed of Sections 3.1, 3.2,
3.3 (excluding 3.3.5), 3.4, 3.5, 5.1, 5.2, 5.3, 5.4, 5.5.
The Sections which are next in priority (as core Sections) are 3.6, 5.6,
6.1, 6.2, 6.3, 7.1, 7.2, 7.3, 7.5.
More advanced topics suitable for seminars are covered in Sections 6.4,
6.5, 6.6, 6.7, 7.4, 7.6 and in Chapter 10.
Sections marked with a star (∗) give a glimpse of topics only sketched
in this book. They can be omitted at ﬁrst reading.
xi

xii
How To Use This Book As A Text
Some results from Section 2.4 are required in Chapter 5. However, in
order to shorten Chapter 2, this material as well the material on many-sorted
algebras has been made into an Appendix. Similarly, to be perfectly rigorous,
Chapter 8 depends on Section 7.6 (since the Skolem-Herbrand-G¨odel Theorem
proved in Section 7.6 is used to prove the completeness of resolution). How-
ever, if the emphasis of the course is on theorem-proving techniques rather
than on foundations, it is possible to proceed directly from Section 5.5 to
Chapter 8 after having covered Chapter 4). The instructor may simply quote
the Herbrand-Skolem-G¨odel Theorem from Section 7.6, without proof.
Hence, depending on the time available and the level of the class, there
is ﬂexibility for focusing more on automatic theorem-proving methods, or
more on foundations. A one semester course emphasizing theorem-proving
techniques may consist of the core, plus Chapter 4, Chapter 8, and possibly
part of Chapter 9. A one semester course emphasizing foundations may consist
of the core, plus Chapter 6 and Chapter 7.
The ideal situtation is to teach the course in two semesters, with au-
tomatic theorem-proving techniques ﬁrst.
The second semester covers the
foundations, and ﬁnishes with a more complete coverage of Chapter 9, Chap-
ter 10, and possibly some material on decision procedures, or on rewrite rules.
It is also possible to use Chapter 6 and Chapter 7 as the core of a seminar
on analytic versus non-analytic proofs.
Problems are usually found at the end of each Section. The problems
range from routine to very diﬃcult. Diﬃcult exercises or exercises requiring
knowledge of material not covered in the text are marked with a star (∗). Very
diﬃcult exercises are marked with a double star (∗∗). A few programming
assignments have been included.
Some historical remarks and suggestions for further reading are included
at the end of each Chapter. Finally the end of a proof is indicated by the
symbol
(box). The word “iﬀ” is used as an abbreviation for “if and only
if”.

How To Use This Book As A Text
xiii
DEPENDENCY OF SECTIONS
7.6
7.7
7.5
7.4
9.5
9.4
9.3
9.2
9.1
8.5
8.4
8.3
8.2
8.1
8.6
7.3
7.2
7.1
6.3
6.2
6.1
6.5
6.4
6.6
6.7
5.6
5.5
5.4
5.3
5.2
5.1
4.1
4.2
4.3
3.6
3.5
3.4
3.3
3.2
3.1
2.3
2.2
2.1
1
2.4
2.5
10.6
10.5
10.4
10.3
10.2
10.1

TABLE OF CONTENTS
Chapter 1: INTRODUCTION, 1
Chapter 2: MATHEMATICAL PRELIMINARIES, 4
2.1
Relations, Functions, Partial Orders, Induction, 4
2.1.1
Relations, 4
2.1.2
Partial Functions, Total Functions, 5
2.1.3
Composition of Relations and Functions, 5
2.1.4
Injections, Surjections, Bijections, 6
2.1.5
Direct Image, Inverse Image, 6
2.1.6
Sequences, 6
2.1.7
Natural Numbers and Countability, 7
2.1.8
Equivalence Relations, 7
2.1.9
Partial and Total Orders, 8
2.1.10
Well-Founded Sets and Complete Induction, 9
2.1.11
Restrictions and Extensions, 12
2.1.12
Strings, 12
2.2
Tree Domains and Trees, 13
2.2.1
Tree Domains, 13
2.2.2
Trees, 14
2.2.3
Paths, 15
2.2.4
Subtrees, 15
xiv

TABLE OF CONTENTS
xv
2.2.5
Tree Replacement, 15
2.2.6
Ranked Alphabets and Σ-Trees, 16
2.3
Inductive Deﬁnitions, 17
2.3.1
Inductive Closures, 17
2.3.2
Freely Generated Sets, 20
2.3.3
Functions Deﬁned Recursively over Freely Generated In-
ductive Sets, 22
PROBLEMS, 24
Chapter 3: PROPOSITIONAL LOGIC, 28
3.1
Introduction, 28
3.2
Syntax of Propositional Logic, 31
3.2.1
The Language of Propositional Logic, 32
3.2.2
Free Generation of PROP, 32
PROBLEMS, 36
3.3
Semantics of Propositional Logic, 39
3.3.1
The Semantics of Propositions, 39
3.3.2
Satisﬁability, Unsatisﬁability, Tautologies, 42
3.3.3
Truth Functions and Functionally Complete Sets of Con-
nectives, 45
3.3.4
Logical Equivalence and Boolean Algebras, 48
∗3.3.5
NP-Complete Problems, 50
PROBLEMS, 54
3.4
Proof Theory of Propositional Logic: The Gentzen System G′, 60
3.4.1
Basic Idea: Searching for a Counter Example, 60
3.4.2
Sequents and the Gentzen System G′, 62
3.4.3
Falsiﬁable and Valid Sequents, 64
3.4.4
Axioms, Deduction Trees, Proof Trees, Counter Example
Trees, 65
3.4.5
Soundness of the Gentzen System G′, 67
3.4.6
The Search Procedure, 68
3.4.7
Completeness of the Gentzen System G′, 71
3.4.8
Conjunctive and Disjunctive Normal Form, 73
3.4.9
Negation Normal Form, 74
PROBLEMS, 76
3.5
Proof Theory for Inﬁnite Sequents: Extended Completeness of G′,
82
3.5.1
Inﬁnite Sequents, 82
3.5.2
The Search Procedure for Inﬁnite Sequents, 83

xvi
TABLE OF CONTENTS
3.5.3
K¨onig’s Lemma, 89
3.5.4
Signed Formulae, 89
3.5.5
Hintikka Sets, 90
3.5.6
Extended Completeness of the Gentzen System G′, 92
3.5.7
Compactness, Model Existence, Consistency, 94
3.5.8
Maximal Consistent Sets, 95
PROBLEMS, 97
3.6
More on Gentzen Systems: The Cut Rule, 109
3.6.1
Using Auxiliary Lemmas in Proofs, 109
3.6.2
The Gentzen System LK′, 110
3.6.3
Logical Equivalence of G′, LK′, and LK′ −{cut}, 112
3.6.4
Gentzen’s Hauptsatz for LK′ (Cut elimination theorem
for
LK′), 113
3.6.5
Characterization of Consistency in LK′, 114
PROBLEMS, 115
Notes and Suggestions for Further Reading, 116
Chapter 4: RESOLUTION IN PROPOSITIONAL LOGIC, 117
4.1
Introduction, 117
4.2
A Special Gentzen System, 118
4.2.1
Deﬁnition of the System GCNF ′, 118
4.2.2
Soundness of the System GCNF ′, 121
4.2.3
Completeness of the System GCNF ′, 123
PROBLEMS, 124
4.3
The Resolution Method for Propositional Logic, 126
4.3.1
Resolution DAGs, 126
4.3.2
Deﬁnition of the Resolution Method for Propositional
Logic, 128
4.3.3
Soundness of the Resolution Method, 131
4.3.4
Converting GCNF ′-proofs into Resolution Refutations
and Completeness, 131
4.3.5
From Resolution Refutations to GCNF ′-proofs, 137
PROBLEMS, 142
Notes and Suggestions for Further Reading, 144
Chapter 5: FIRST-ORDER LOGIC, 146
5.1
INTRODUCTION, 146

TABLE OF CONTENTS
xvii
5.2
FIRST-ORDER LANGUAGES, 147
5.2.1
Syntax, 147
5.2.2
Free Generation of the Set of Terms, 150
5.2.3
Free Generation of the Set of Formulae, 151
5.2.4
Free and Bound Variables, 153
5.2.5
Substitutions, 155
PROBLEMS, 156
5.3
SEMANTICS OF FIRST-ORDER LANGUAGES, 158
5.3.1
First-Order Structures, 158
5.3.2
Semantics of Formulae, 159
5.3.3
Satisfaction, Validity, and Model, 162
5.3.4
A More Convenient Semantics, 163
5.3.5
Free Variables and Semantics of Formulae, 169
5.3.6
Subformulae and Rectiﬁed Formulae, 171
5.3.7
Valid Formulae Obtained by Substitution in Tautologies,
173
5.3.8
Complete Sets of Connectives, 175
5.3.9
Logical Equivalence and Boolean Algebras, 176
PROBLEMS, 179
5.4
Proof Theory of First-Order Languages, 187
5.4.1
The Gentzen System G for Languages Without Equality,
187
5.4.2
Deduction Trees for the System G, 188
5.4.3
Soundness of the System G, 189
5.4.4
Signed Formulae and Term Algebras (no Equality Sym-
bol), 192
5.4.5
Reducts, Expansions, 194
5.4.6
Hintikka Sets (Languages Without Equality), 194
5.4.7
Completeness: Special Case of Languages Without Func-
tion Symbols and Without Equality, 197
PROBLEMS, 205
5.5
Completeness for Languages with Function Symbols and no
Equality, 207
5.5.1
Organizing the Terms for Languages with Function Sym-
bols and no Equality, 207
5.5.2
The Search Procedure for Languages with Function Sym-
bols and no Equality, 209
5.5.3
Completeness of the System G (Languages Without
Equality), 214
5.5.4
L¨owenheim-Skolem, Compactness, and Model Existence
Theorems for Languages Without Equality, 217
5.5.5
Maximal Consistent Sets, 218

xviii
TABLE OF CONTENTS
PROBLEMS, 219
5.6
A Gentzen System for First-Order Languages With Equality, 230
5.6.1
Hintikka Sets for Languages with Equality, 230
5.6.2
The Gentzen System G= (Languages With Equality), 236
5.6.3
Soundness of the System G=, 239
5.6.4
Search Procedure for Languages With Equality, 239
5.6.5
Completeness of the System G=, 241
5.6.6
L¨owenheim-Skolem, Compactness, and Model Existence
Theorems for Languages With Equality, 243
5.6.7
Maximal Consistent Sets, 243
5.6.8
Application of the Compactness and L¨owenheim-Skolem
Theorems: Nonstandard Models of Arithmetic, 244
PROBLEMS, 246
Notes and Suggestions for Further Reading, 255
Chapter 6: GENTZEN’S CUT ELIMINATION THEOREM AND
APPLICATIONS, 256
6.1
Introduction, 256
6.2
Gentzen System LK for Languages Without Equality, 257
6.2.1
Syntax of LK, 257
6.2.2
The Logical Equivalence of the Systems G, LK, and LK−
{cut}, 259
PROBLEMS, 261
6.3
The Gentzen System LKe with Equality, 262
6.3.1
Syntax of LKe, 262
6.3.2
A Permutation Lemma for the System G=, 263
6.3.3
Logical equivalence of G=, LKe, and LKe Without Es-
sential Cuts: Gentzen’s Hauptsatz for LKe Without Es-
sential Cuts, 266
PROBLEMS, 268
6.4
Gentzen’s Hauptsatz for Sequents in NNF, 269
6.4.1
Negation Normal Form, 269
6.4.2
The Gentzen System G1nnf, 270
6.4.3
Completeness of G1nnf, 272
6.4.4
The Cut Elimination Theorem for G1nnf, 273
6.4.5
The System G1nnf
=
, 281
6.4.6
The Cut Elimination Theorem for G1nnf
=
, 282
PROBLEMS, 284

TABLE OF CONTENTS
xix
6.5
Craig’s Interpolation Theorem, 287
6.5.1
Interpolants, 287
6.5.2
Craig’s Interpolation Theorem Without Equality, 288
6.5.3
Craig’s Interpolation Theorem With Equality, 292
PROBLEMS, 294
6.6
Beth’s Deﬁnability Theorem, 295
6.6.1
Implicit and Explicit Deﬁnability, 295
6.6.2
Explicit Deﬁnability Implies Implicit Deﬁnability, 296
6.6.3
Beth’s Deﬁnability Theorem, Without Equality, 297
6.6.4
Beth’s Deﬁnability Theorem, With Equality, 299
PROBLEMS, 299
6.7
Robinson’s Joint Consistency Theorem, 300
PROBLEMS, 301
Notes and Suggestions for Further Reading, 302
Chapter 7: GENTZEN’S SHARPENED HAUPTSATZ;
HERBRAND’S THEOREM, 303
7.1
Introduction, 303
7.2
Prenex Normal Form, 305
PROBLEMS, 309
7.3
Gentzen’s Sharpened Hauptsatz for Prenex Formulae, 310
7.3.1
Pure Variable Proofs, 310
7.3.2
The Permutability Lemma, 314
7.3.3
Gentzen’s Sharpened Hauptsatz, 320
PROBLEMS, 324
7.4
The Sharpened Hauptsatz for Sequents in NNF, 325
7.4.1
The System G2nnf, 325
7.4.2
Soundness of the System G2nnf, 328
7.4.3
A Gentzen-like Sharpened Hauptsatz for G2nnf, 330
7.4.4
The Gentzen System G2nnf
=
, 336
7.4.5
A Gentzen-like Sharpened Hauptsatz for G2nnf
=
, 337
PROBLEMS, 337
7.5
Herbrand’s Theorem for Prenex Formulae, 338
7.5.1
Preliminaries, 338
7.5.2
Skolem Function and Constant Symbols, 339
7.5.3
Substitutions, 342
7.5.4
Herbrand’s Theorem for Prenex Formulae, 344

xx
TABLE OF CONTENTS
PROBLEMS, 353
7.6
Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF, 355
7.6.1
Skolem-Herbrand-G¨odel’s Theorem in Unsatisﬁability
Form, 355
7.6.2
Skolem Normal Form, 357
7.6.3
Compound Instances, 359
7.6.4
Half of a Herbrand-like Theorem for Sentences in NNF,
360
7.6.5
Skolem-Herbrand-G¨odel’s Theorem (Sentences in NNF),
361
7.6.6
Comparison of Herbrand and Skolem-Herbrand-G¨odel
Theorems, 365
PROBLEMS, 367
∗7.7
The Primitive Recursive Functions, 369
7.7.1
The Concept of Computability, 369
7.7.2
Deﬁnition of the Primitive Recursive Functions, 371
7.7.3
The Partial Recursive Functions, 372
7.7.4
Some Primitive Recursive Functions, 373
PROBLEMS, 374
Notes and Suggestions for Further Reading, 375
Chapter 8: RESOLUTION IN FIRST-ORDER LOGIC, 376
8.1
Introduction, 376
8.2
Formulae in Clause Form, 378
8.3
Ground Resolution, 379
8.4
Uniﬁcation and the Uniﬁcation Algorithm, 381
8.4.1
Uniﬁers and Most General Uniﬁers, 381
8.4.2
The Uniﬁcation Algorithm, 383
PROBLEMS, 394
8.5
The Resolution Method for First-order Logic, 395
8.5.1
Deﬁnition of the Method, 395
8.5.2
Soundness of the Resolution Method, 398
8.5.3
Completeness of the Resolution Method, 400
PROBLEMS, 405
8.6
A Glimpse at Paramodulation, 407
Notes and Suggestions for Further Reading, 409

TABLE OF CONTENTS
xxi
Chapter 9: SLD-RESOLUTION AND LOGIC PROGRAMMING
(PROLOG), 410
9.1
Introduction, 410
9.2
GCNF ′-Proofs in SLD-form, 411
9.2.1
The Case of Deﬁnite Clauses, 411
9.2.2
GCNF ′-Proofs in SLD-Form, 413
9.2.3
Completeness of Proofs in SLD-Form, 413
PROBLEMS, 421
9.3
SLD-Resolution in Propositional Logic, 422
9.3.1
SLD-Derivations and SLD-Refutations, 422
9.3.2
Completeness of SLD-Resolution for Horn Clauses, 425
PROBLEMS, 427
9.4
SLD-Resolution in First-Order Logic, 427
9.4.1
Deﬁnition of SLD-Refutations, 428
9.4.2
Completeness of SLD-Resolution for Horn Clauses, 431
PROBLEMS, 432
9.5
SLD-Resolution, Logic Programming (PROLOG), 433
9.5.1
Refutations as Computations, 433
9.5.2
Model-Theoretic Semantics of Logic Programs, 434
9.5.3
Correctness of SLD-Resolution as a Computation Proce-
dure, 439
9.5.4
Completeness of SLD-Resolution as a Computational
Procedure, 444
9.5.5
Limitations of PROLOG, 445
PROBLEMS, 445
Notes and Suggestions for Further Reading, 447
Chapter 10: MANY-SORTED FIRST-ORDER LOGIC, 448
10.1
Introduction, 448
10.2
Syntax, 448
10.2.1
Many-Sorted First-Order Languages, 449
10.2.2
Free Generation of Terms and Formulae, 452
10.2.3
Free and Bound Variables, Substitutions, 452
PROBLEMS, 452
10.3
Semantics of Many-Sorted First-Order Languages, 453
10.3.1
Many-Sorted ﬁrst-Order Structures, 453

xxii
TABLE OF CONTENTS
10.3.2
Semantics of Formulae, 453
10.3.3
An Alternate Semantics, 455
10.3.4
Semantics and Free Variables, 456
10.3.5
Subformulae and Rectiﬁed Formulae, 456
PROBLEMS, 456
10.4
Proof Theory of Many-Sorted Languages, 456
10.4.1
Gentzen System G for Many-Sorted Languages Without
Equality, 456
10.4.2
Deduction Trees for the System G, 458
10.4.3
Soundness of the System G, 458
10.4.4
Completeness of G, 458
10.5
Many-Sorted First-Order Logic With Equality, 458
10.5.1
Gentzen System G= for Languages with Equality, 458
10.5.2
Soundness of the System G=, 459
10.5.3
Completeness of the System G=, 459
10.5.4
Reduction of Many-Sorted Logic to One-Sorted Logic,
459
PROBLEMS, 460
10.6
Decision Procedures Based on Congruence Closure, 460
10.6.1
Decision Procedure for Quantiﬁer-free Formulae Without
Predicate Symbols, 460
10.6.2
Congruence Closure on a Graph, 461
10.6.3
The Graph Associated With a Conjunction, 462
10.6.4
Existence of the Congruence Closure, 467
10.6.5
Decision Procedure for Quantiﬁer-free Formulae, 467
10.6.6
Computing the Congruence Closure, 471
PROBLEMS, 474
Notes and Suggestions for Further Reading, 476
APPENDIX, 477
2.4
Algebras, 477
2.4.1
Deﬁnition of an Algebra, 477
2.4.2
Homomorphisms, 478
2.4.3
Subalgebras, 479
2.4.4
Least Subalgebra Generated by a Subset, 479
2.4.5
Subalgebras Freely Generated by a Set X, 480
2.4.6
Congruences, 481
2.4.7
Quotient Algebras, 482
2.5
Many-Sorted Algebras, 483
2.5.1
S-Ranked Alphabets, 483

TABLE OF CONTENTS
xxiii
2.5.2
Deﬁnition of a Many-Sorted Algebra, 483
2.5.3
Homomorphisms, 484
2.5.4
Subalgebras, 484
2.5.5
Least Subalgebras, 485
2.5.6
Freely Generated Subalgebras, 485
2.5.7
Congruences, 486
2.5.8
Quotient Algebras, 487
2.5.9
Many-Sorted Trees, 487
PROBLEMS, 488
REFERENCES, 490
INDEX OF SYMBOLS, 495
INDEX OF DEFINITIONS, 498
SUBJECT INDEX, 502

Chapter 1
Introduction
Logic is concerned mainly with two concepts: truth and provability. These
concepts have been investigated extensively for centuries, by philosophers,
linguists, and mathematicians. The purpose of this book is by no means to
give a general account of such studies.
Instead, the purpose of this book
is to focus on a mathematically well deﬁned logical system known as ﬁrst-
order logic (and, to some extent, many-sorted logic), and prove some basic
properties of this system. In particular, we will focus on algorithmic methods
for proving theorems (often referred to as automatic theorem proving).
Every logical system consists of a language used to write statements also
called propositions or formulae. Normally, when one writes a formula, one has
some intended interpretation of this formula in mind. For example, a formula
may assert a true property about the natural numbers, or some property that
must be true in a data base. This implies that a formula has a well-deﬁned
meaning or semantics. But how do we deﬁne this meaning precisely? In logic,
we usually deﬁne the meaning of a formula as its truth value. A formula can
be either true (or valid) or false.
Deﬁning rigorously the notion of truth is actually not as obvious as it
appears. We shall present a concept of truth due to Tarski. Roughly speaking,
a formula is true if it is satisﬁed in all possible interpretations. So far, we have
used the intuitive meaning of such words as truth, interpretation, etc. One of
the objectives of this book is to deﬁne these terms rigorously, for the language
of ﬁrst-order logic (and many-sorted ﬁrst-order logic). The branch of logic
in which abstract structures and the properties true in these structures are
studied is known as model theory.
Once the concept of truth has been deﬁned rigorously, the next question
1

2
1/Introduction
is to investigate whether it is possible to ﬁnd methods for deciding in a ﬁnite
number of steps whether a formula is true (or valid). This is a very diﬃcult
task. In fact, by a theorem due to Church, there is no such general method
for ﬁrst-order logic.
However, there is another familiar method for testing whether a formula
is true: to give a proof of this formula.
Of course, to be of any value, a proof system should be sound, which
means that every provable formula is true.
We will also deﬁne rigorously the notion of proof, and proof system
for ﬁrst-order logic (and many-sorted ﬁrst-order logic). The branch of logic
concerned with the study of proof is known as proof theory.
Now, if we have a sound proof system, we know that every provable
formula is true. Is the proof system strong enough that it is also possible to
prove every true formula (of ﬁrst-order logic)?
A major theorem of G¨odel shows that there are logical proof systems in
which every true formula is provable. This is referred to as the completeness
of the proof system.
To summarize the situation, if one is interested in algorithmic meth-
ods for testing whether a formula of ﬁrst-order logic is valid, there are two
logical results of central importance: one positive (G¨odel’s completeness the-
orem), the other one negative (Church’s undecidability of validity). Roughly
speaking, G¨odel’s completeness theorem asserts that there are logical calculi
in which every true formula is provable, and Church’s theorem asserts that
there is no decision procedure (procedure which always terminates) for decid-
ing whether a formula is true (valid). Hence, any algorithmic procedure for
testing whether a formula is true (or equivalently, by G¨odel’s completeness
theorem, provable in a complete system) must run forever when given certain
non-true formulae as input.
This book focuses on G¨odel’s positive result and its applications to
automatic theorem proving. We have attempted to present a coherent ap-
proach to automatic theorem proving, following a main thread: Gentzen-like
sequent calculi. The restriction to the positive result was dictated mostly by
the lack of space. Indeed, it should be stressed that Church’s negative result
is also important, as well as other fundamental negative results due to G¨odel.
However, the omission of such topics should not be a severe inconvenience to
the reader, since there are many texts covering such material (see the notes
at the end of Chapter 5).
In spite of the theoretical limitation imposed by Church’s result, the goal
of automatic theorem proving (for short, atp) is to ﬁnd eﬃcient algorithmic
methods for ﬁnding proofs of those formulae that are true.
A fairly intuitive method for ﬁnding such algorithms is the completeness
proof for Gentzen-like sequent calculi. This approach yields a complete pro-
cedure (the search procedure) for proving valid formulae of ﬁrst-order logic.

1 Introduction
3
However, the search procedure usually requires an enormous amount of space
and time and it is not practical. Hence, we will try improve it or ﬁnd more
eﬃcient proof procedures.
For this, we will analyze the structure of proofs carefully. Fundamental
results of Gentzen and Herbrand show that if a formula is provable, then it
has a proof having a certain form, called a normal form.
The existence of such normal forms can be exploited to reduce the size
of the search space that needs to be explored in trying to ﬁnd a proof. Indeed,
it is suﬃcient to look for proofs in normal form.
The existence of normal forms is also fundamental because it reduces the
problem of ﬁnding a proof of a ﬁrst-order formula to the problem of ﬁnding a
proof of a simpler type of formula, called a proposition. Propositions are much
simpler than ﬁrst-order formulae. Indeed, there are algorithms for deciding
truth. One of the methods based on this reduction technique is the resolution
method, which will be investigated in Chapters 4 and 8.
Besides looking for general methods applying to the class of all true (ﬁrst-
order) formulae, it is interesting to consider subclasses for which simpler or
more eﬃcient proof procedures exist. Indeed, for certain subclasses there may
be decision procedures. This is the case for propositions, and for quantiﬁer-
free formulae. Such cases are investigated in Chapters 3 and 10 respectively.
Unfortunately, even in cases in which algorithms exist, another diﬃculty
emerges.
A decision procedure may take too much time and space to be
practical. For example, even testing whether a proposition is true may be
very costly. This will be discussed in Chapter 3.
Automatic theorem proving techniques can be used by computer sci-
entists to axiomatize structures and prove properties of programs working
on these structures. Another recent and important role that logic plays in
computer science, is its use as a programming language and as a model of
computation.
For example, in the programming language PROLOG, pro-
grams are speciﬁed by sets of assertions. In such a programming language, a
computation is in fact a proof, and the output of a program is extracted from
the proof. Promoters of such languages claim that since such programs are
essentially logical formulae, establishing their correctness is trivial. This is not
quite so, because the concept of correctness is relative, and the semantics of a
PROLOG program needs to be expressed in a language other than PROLOG.
However, using logic as a vehicle for programming is a very interesting idea
and should be a selling point for skeptics. This use of logic will be investigated
in Chapter 9.

Chapter 2
Mathematical
Preliminaries
This chapter is devoted to mathematical preliminaries. This fairly lengthy
chapter has been included in order to make this book as self-contained as
possible. Readers with a ﬁrm mathematical background may skim or even
skip this chapter entirely. Classroom experience shows that anyone who is
not acquainted with the material included in Section 2.3 should probably
spend some time reading Sections 2.1 to 2.3. In any case, this chapter can be
used as a library of useful facts and may be consulted whenever necessary.
Since trees, inductive deﬁnitions and the deﬁnition of functions by re-
cursion play an important role in logic, they will be deﬁned carefully. First,
we review some basic concepts and establish the terminology and notation
used in this book. It is assumed that the reader is familiar with the basic
properties of sets. For more details, the reader may consult Enderton, 1972;
Enderton, 1977; Lewis and Papadimitriou, 1981; or Suppes, 1972.
2.1 Relations, Functions, Partial Orders, Induction
First, we review the concepts of Cartesian product, tuple and relation.
2.1.1 Relations
Given two sets A and B (possibly empty), their Cartesian product denoted
by A × B is the set of ordered pairs
{< a, b > | a ∈A, b ∈B}.
4

2.1 Relations, Functions, Partial Orders, Induction
5
Given any ﬁnite number of sets A1,...,An, the Cartesian product
A1 × ... × An is the set of ordered n-tuples
{< a1, ..., an > | ai ∈Ai, 1 ≤i ≤n}
(An ordered n-tuple < a1, ..., an > is also denoted by (a1, ..., an).)
A binary relation between A and B is any subset R (possibly empty) of
A × B.
Given a relation R between A and B, the set
{x ∈A | ∃y ∈B < x, y >∈R},
is called the domain of R and denoted by dom(R). The set
{y ∈B | ∃x ∈A < x, y >∈R}
is called the range of R and is denoted by range(R).
When A = B, a relation R beween A and A is also called a relation on
(or over) A. We will also use the notation xRy as an alternate to (x, y) ∈R.
2.1.2 Partial Functions, Total Functions
A relation R between two sets A and B is functional iﬀ, for all x ∈A, and
y, z ∈B, (x, y) ∈R and (x, z) ∈R implies that y = z.
A partial function is a triple f =< A, G, B >, where A and B are
arbitrary sets (possibly empty) and G is a functional relation (possibly empty)
between A and B, called the graph of f.
Hence, a partial function is a functional relation such that every argu-
ment has at most one image under f. The graph of a function f is denoted
as graph(f). When no confusion can arise, a function f and its graph are
usually identiﬁed.
A partial function f =< A, G, B > is often denoted as f : A →B. For
every element x in the domain of a partial function f, the unique element y
in the range of f such that (x, y) ∈graph(f) is denoted by f(x). A partial
function f : A →B is a total function iﬀdom(f) = A. It is customary to call
a total function simply a function.
2.1.3 Composition of Relations and Functions
Given two binary relations R between A and B, and S between B and C,
their composition denoted by R ◦S is a relation between A and C deﬁned by
the following set of ordered pairs:
{(a, c) | ∃b ∈B, (a, b) ∈R and (b, c) ∈S}.

6
2/Mathematical Preliminaries
Given a set A, the identity relation of A is denoted by IA and is the
relation {(x, x) | x ∈A}. Note that IA is also a total function.
Given a relation R between A and B, its converse is the relation between
B and A denoted by R−1 deﬁned by the set
{(b, a) ∈B × A | (a, b) ∈R}.
Given two partial or total functions f : A →B and g : B →C, with
f =< A, G1, B > and g =< B, G2, C >, their composition denoted by f ◦g
(or f.g, or fg), is the partial or total function deﬁned by < A, G1 ◦G2, C >.
Notice that according to our notation, f ◦g(x) = g(f(x)), that is, f is applied
ﬁrst. Note also that composition is associative.
2.1.4 Injections, Surjections, Bijections
A function f : A →B is injective (or one to one) iﬀ, for all x, y ∈A,
f(x) = f(y) implies that x = y.
A function f : A →B is surjective (or onto) iﬀ, for all y ∈B, there is
some x ∈A such that f(x) = y. Equivalently, the range of f is the set B.
A function is bijective iﬀit is both injective and surjective.
It can be shown that a function f is surjective if and only if there exists a
function g : B →A such that g ◦f = IB. If there exists a function g : B →A
such that f ◦g = IA, then f : A →B is injective. If f : A →B is injective
and A ̸= ∅, then there exists a function g : B →A such that f ◦g = IA. As
a consequence, it can be shown that a function f : A →B is bijective if and
only if there is a unique function f −1 called its inverse such that f ◦f −1 = IA
and f −1 ◦f = IB.
2.1.5 Direct Image, Inverse Image
Given a (partial) function f : A →B, for every subset X of A, the direct
image (or for short, image) of X under f is the set
{y ∈B | ∃x ∈X, f(x) = y}
and is denoted by f(X). For every subset Y of B, the inverse image of Y
under f is the set
{x ∈A | ∃y ∈Y, f(x) = y}
and is denoted by f −1(Y ).
Warning: The function f may not have an inverse.
Hence, f −1(Y )
should not be confused with f −1(y) for y ∈B, which is only deﬁned when f
is a bijection.

2.1 Relations, Functions, Partial Orders, Induction
7
2.1.6 Sequences
Given two sets I and X, an I-indexed sequence (or sequence) is any function
A : I →X, usually denoted by (Ai)i∈I. The set I is called the index set. If
X is a set of sets, (Ai)i∈I is called a family of sets.
2.1.7 Natural Numbers and Countability
The set of natural numbers (or nonnegative integers) is denoted by N and is
the set {0, 1, 2, 3, ...}. A set A is countable (or denumerable) iﬀeither A = ∅or
there is a surjection h : N →A from N onto A, countably inﬁnite iﬀthere is a
bijection h : N →A. Otherwise, A is said to be uncountable. The cardinality
of a countably inﬁnite set is denoted by ω. The set of positive integers is
denoted by N+.
For every positive integer n ∈N+, the set {1, ..., n} is
denoted as [n], and [0] denotes the empty set. A set A is ﬁnite iﬀthere is a
bijection h : [n] →A for some natural number n ∈N. The natural number
n is called the cardinality of the set A, which is also denoted by |A|. When
I is the set N of natural numbers, a sequence (Ai)i∈I is called a countable
sequence, and when I is some set [n] with n ∈N, (Ai)i∈I is a ﬁnite sequence.
2.1.8 Equivalence Relations
A binary relation R ⊂A × A is reﬂexive iﬀfor all x ∈A, (x, x) ∈R.
The relation R is symmetric iﬀfor all x, y ∈A, (x, y) ∈R implies that
(y, x) ∈R.
The relation R is transitive iﬀfor all x, y, z ∈A, (x, y) ∈R and (y, z) ∈R
implies that (x, z) ∈R.
The relation R is an equivalence relation if it is reﬂexive, symmetric and
transitive. Given an equivalence relation R on a set A, for every x ∈A, the
set {y ∈A | (x, y) ∈R} is the equivalence class of x modulo R and is denoted
by [x]R, or xR, or simply [x] or x. The set of equivalence classes modulo R
is the quotient of A by R and is denoted by A/R. The set A/R is also called
a partition of A, since any two distinct equivalence classes are nonempty and
disjoint, and their union is A itself. The surjective function hR : A →A/R
such that hR(x) = [x]R is called the canonical function associated with R.
Given any relation R on a set A, we deﬁne the powers of R as follows:
For every integer n ≥0,
R0 = IA, R1 = R, and Rn+1 = Rn ◦R.
The union
R+ =

n≥1
Rn

8
2/Mathematical Preliminaries
called the transitive closure of R is the smallest transitive relation on A con-
taining R, and
R∗=

n≥0
Rn
is called the reﬂexive and transitive closure of R and is the smallest reﬂexive
and transitive relation on A containing R. It is obvious that R+ = R ◦R∗=
R∗◦R, and that R∗= IA ∪R+. Thus, it can also be shown that for any
relation R on a set A, (R ∪R−1)∗is the least equivalence relation containing
R.
2.1.9 Partial and Total Orders
A relation R on a set A is antisymmetric iﬀfor all x, y ∈A, (x, y) ∈R and
(y, x) ∈R implies that x = y.
A relation R on a set A is a partial order iﬀR is reﬂexive, transitive and
antisymmetric.
Given a partial order R on a set A, the pair < A, R > is called a partially
ordered set (or poset). A partial order is often denoted by the symbol ≤.
Given a partial order ≤on a set A, given any subset X of A, X is a
chain iﬀfor all x, y ∈X, either x ≤y, or y ≤x.
A partial order ≤on a set A is a total order (or a linear order) iﬀA is
a chain.
Given a partial order ≤on a set A, given any subset X of A, an element
b ∈A is a lower bound of X iﬀfor all x ∈X, b ≤x. An element m ∈A is an
upper bound of X iﬀfor all x ∈X, x ≤m. Note that b or m may or may not
belong to X. It can be easily shown that a lower bound (resp. upper bound)
of X in X is unique. Hence the following deﬁnition is legitimate.
An element b ∈X is the least element of X iﬀfor all x ∈X, b ≤x.
An element m ∈X is the greatest element of X iﬀfor all x ∈X, x ≤m. In
view of the above remark, least and greatest elements are unique (when they
exist).
Given a subset X of A, an element b ∈X is minimal in X iﬀfor all
x ∈X, x ≤b implies that x = b. An element m ∈X is maximal in X if for all
x ∈X, m ≤x implies that m = x. Contrary to least and greatest elements,
minimal or maximal elements are not necessarily unique.
An element m ∈A is the least upper bound of a subset X, iﬀthe set of
upper bounds of X is nonempty, and m is the least element of this set. An
element b ∈A is the greatest lower bound of X if the set of lower bounds of
X is nonempty, and b is the greatest element of this set.
Although the following fundamental result known as Zorn’s lemma will
not be used in the main text, it will be used in some of the problems. Hence,

2.1 Relations, Functions, Partial Orders, Induction
9
this result is stated without proof. For details and the proof, the reader is
referred to Suppes, 1972; Levy, 1979; or Kuratowski and Mostowski, 1976.
Theorem 2.1.1
(Zorn’s lemma) Given a partially ordered set < A, ≤>, if
every (nonempty) chain in A has an upper bound, then A has some maximal
element.
2.1.10 Well-Founded Sets and Complete Induction
A very general induction principle holds for the class of partially ordered sets
having a well-founded ordering. Given a partial order ≤on a set A, the strict
order < associated with ≤is deﬁned as follows:
x < y if and only if x ≤y and x ̸= y.
A partially ordered set < A, ≤> is well-founded iﬀit has no inﬁnite
decreasing sequence (xi)i∈N, that is, sequence such that xi+1 < xi for all
i ≥0.
The following property of well-founded sets is fundamental.
Lemma 2.1.1
Given a partially ordered set < A, ≤>, < A, ≤> is a well-
founded set if and only if every nonempty subset of A has a minimal element.
Proof : First, assume that < A, ≤> is well-founded.
We proceed by
contradiction. Let X be any nonempty subset of A, and assume that X does
not have a minimal element. This means that for any x ∈X, there is some
y ∈X such that y < x, since otherwise there would be some minimal x ∈X.
Since X is nonempty, there is some x0 in X. By the above remark, there is
some x1 ∈X such that x1 < x0. By repeating this argument (using induction
on N), an inﬁnite decreasing sequence (xi) can be deﬁned in X, contradicting
the fact that A is well-founded. Hence, X must have some minimal element.
Conversely, assume that every nonempty subset has a minimal element.
If an inﬁnite decreasing sequence (xi) exists in A, (xi) has some minimal
element xk. But this contradicts the fact that xk+1 < xk.
The principle of complete induction (or structural induction) is now de-
ﬁned. Let (A, ≤) be a well-founded poset, and let P be a property of the
set A, that is, a function P : A →{false, true}. We say that P(x) holds if
P(x) = true.
Principle of Complete Induction
To prove that a property P holds for all z ∈A, it suﬃces to show that,
for every x ∈A,
(∗) if x is minimal, or P(y) holds for all y < x,
(∗∗) then P(x) holds.
The statement (∗) is called the induction hypothesis, and the implication

10
2/Mathematical Preliminaries
for all x, (∗) implies (∗∗)
is called the induction step. Formally, the induction principle can be
stated as:
(CI)
(∀x ∈A)[(∀y ∈A)(y < x ⊃P(y)) ⊃P(x)] ⊃(∀z ∈A)P(z)
Note that if x is minimal, then there is no y ∈A such that y < x, and
(∀y ∈A)(y < x ⊃P(y)) is true. Hence, P(x) has to be shown to be true
for every minimal element x. These cases are called the base cases. Complete
induction is not valid for arbitrary posets (see the problems) but holds for
well-founded sets as shown in the following lemma.
Lemma 2.1.2
The principle of complete induction holds for every well-
founded set.
Proof : We proceed by contradiction. Assume that (CI) is false. Then,
(1)
(∀x ∈A)[(∀y ∈A)(y < x ⊃P(y)) ⊃P(x)]
is true and
(2)
(∀z ∈A)P(z)
is false, that is,
(∃z ∈A)(P(z) = false)
is true.
Hence, the subset X of A deﬁned by
X = {x ∈A | P(x) = false}
is nonempty. Since A is well founded, by lemma 2.1.1, X has some minimal
element b. Since (1) is true for all x ∈A, letting x = b,
(3)
[(∀y ∈A)(y < b ⊃P(y)) ⊃P(b)]
is true. If b is also minimal in A, there is no y ∈A such that y < b and so,
(∀y ∈A)(y < b ⊃P(y))
holds trivially and (3) implies that P(b) = true, which contradicts the fact
that b ∈X. Otherwise, for every y ∈A such that y < b, P(y) = true, since
otherwise y would belong to X and b would not be minimal. But then,
(∀y ∈A)(y < b ⊃P(y))
also holds and (3) implies that P(b) = true, contradicting the fact that b ∈X.
Hence, complete induction is valid for well-founded sets.

2.1 Relations, Functions, Partial Orders, Induction
11
As an illustration of well-founded sets, we deﬁne the lexicographic or-
dering. Given a partially ordered set (A, ≤), the lexicographic ordering <<
on A × A induced by ≤is deﬁned a follows: For all x, y, x′, y′ ∈A,
(x, y) << (x′, y′) if and only if either
x = x′ and y = y′,
or
x < x′
or
x = x′ and y < y′.
We leave as an exercise the check that << is indeed a partial order on A × A.
The following lemma will be useful.
Lemma 2.1.3
If < A, ≤> is a well-founded partially ordered set, the lexi-
cographic ordering << on A × A is also well founded.
Proof : We proceed by contradiction. Assume that there is an inﬁnite
decreasing sequence (< xi, yi >)i∈N in A × A. Then, either,
(1) There is an inﬁnite number of distinct xi, or
(2) There is only a ﬁnite number of distinct xi.
In case (1), the subsequence consisting of these distinct elements forms
a decreasing sequence in A, contradicting the fact that ≤is well founded. In
case (2), there is some k such that for all i ≥k, xi = xi+1. By deﬁnition of
<<, the sequence (yi)i≥k is a decreasing sequence in A, contradicting the fact
that ≤is well founded. Hence, << is well founded on A × A.
As an illustration of the principle of complete induction, consider the
following example in which it is shown that a function deﬁned recursively is
a total function.
EXAMPLE 2.1.1
(Ackermann’s function) The following function A : N × N →N known
as Ackermann’s function is well known in recursive function theory for
its extraordinary rate of growth. It is deﬁned recursively as follows:
A(x, y) = if x = 0 then y + 1
else if y = 0 then A(x −1, 1)
else A(x −1, A(x, y −1))
It is actually not obvious that such a recursive deﬁnition deﬁnes a partial
function, but this can be shown. The reader is referred to Machtey and
Young, 1978; or Rogers, 1967, for more details.
We wish to prove that A is a total function. We proceed by com-
plete induction over the lexicographic ordering on N × N.

12
2/Mathematical Preliminaries
The base case is x = 0, y = 0. In this case, since A(0, y) = y + 1,
A(0, 0) is deﬁned and equal to 1.
The induction hypothesis is that for any (m, n), A(m′, n′) is deﬁned
for all (m′, n′) << (m, n), with (m, n) ̸= (m′, n′).
For the induction step, we have three cases:
(1) If m = 0, since A(0, y) = y + 1, A(0, n) is deﬁned and equal to
n + 1.
(2) If m ̸= 0 and n = 0, since (m −1, 1) << (m, 0) and (m −1, 1) ̸=
(m, 0), by the induction hypothesis, A(m −1, 1) is deﬁned, and so
A(m, 0) is deﬁned since it is equal to A(m −1, 1).
(3) If m ̸= 0 and n ̸= 0, since (m, n −1) << (m, n) and (m, n −1) ̸=
(m, n), by the induction hypothesis, A(m, n −1) is deﬁned. Since
(m−1, y) << (m, z) and (m−1, y) ̸= (m, z) no matter what y and
z are, (m −1, A(m, n −1)) << (m, n) and (m −1, A(m, n −1)) ̸=
(m, n), and by the induction hypothesis, A(m −1, A(m, n −1)) is
deﬁned. But this is precisely A(m, n), and so A(m, n) is deﬁned.
This concludes the induction step. Hence, A(x, y) is deﬁned for all
x, y ≥0.
2.1.11 Restrictions and Extensions
We deﬁne a partial ordering ⊆on partial functions as follows: f ⊆g if and
only if graph(f) is a subset of graph(g). We say that g is an extension of f
and that f is a restriction of g. The following lemma will be needed later.
Lemma 2.1.4
Let (fn)n≥0 be a sequence of partial functions fn : A →B
such that fn ⊆fn+1 for all n ≥0. Then, g = (A,  graph(fn), B) is a partial
function. Furthermore, g is the least upper bound of the sequence (fn).
Proof : First, we show that G =  graph(fn) is functional. Note that for
every (x, y) ∈G, there is some n such that (x, y) ∈graph(fn). If (x, y) ∈G
and (x, z) ∈G, then there is some m such that (x, y) ∈graph(fm) and some
n such that (x, z) ∈graph(fn). Letting k = max(m, n), since (fn) is a chain,
we have (x, y) ∈graph(fk) and (x, z) ∈graph(fk). But since graph(fk) is
functional, we must have y = z. Next, the fact that each relation graph(fn) is
contained in G is obvious since G =  graph(fn). If h is any partial function
such that graph(fn) is a subset of graph(h) for all n ≥0, by deﬁnition of a
union, G =  graph(fn) is a subset of h. Hence, g is indeed the least upper
bound of the chain (fn).
2.1.12 Strings
Given any set A (even inﬁnite), a string over A is any ﬁnite sequence u : [n] →
A, where n is a natural number. It is customary to call the set A an alphabet.

2.2 Tree Domains and Trees
13
Given a string u : [n] →A, the natural number n is called the length of u and
is denoted by |u|. For n = 0, we have the string corresponding to the unique
function from the empty set to A, called the null string (or empty string), and
denoted by eA, or for simplicity by e when the set A is understood. Given
any set A (even inﬁnite), the set of all strings over A is denoted by A∗. If
u : [n] →A is a string and n > 0, for every i ∈[n], u(i) is some element of A
also denoted by ui, and the string u is also denoted by u1...un.
Strings can be concatenated as follows. Given any two strings u : [m] →
A and v : [n] →A, (m, n ≥0), their concatenation denoted by u.v or uv is
the string w : [m + n] →A such that:
w(i) =

u(i)
if 1 ≤i ≤m;
v(i −m)
if m + 1 ≤i ≤m + n.
One veriﬁes immediately that for every string u, u.e = e.u = u.
In
other words, viewing concatenation as an algebraic operation on the set A∗
of all strings, e is an identity element. It is also obvious that concatenation is
associative, but not commutative in general.
Given a string u, a string v is a preﬁx (or head) of u if there is a string
w such that u = vw. A string v is a suﬃx (or tail) of u if there is a string
w such that u = wv. A string v is a substring of u if there are strings x and
y such that u = xvy. A preﬁx v (suﬃx, substring) of a string u is proper if
v ̸= u.
2.2 Tree Domains and Trees
In order to deﬁne ﬁnite or inﬁnite trees, we use the concept of a tree domain
due to Gorn (Gorn, 1965).
2.2.1 Tree Domains
A tree domain D is a nonempty subset of strings in N∗
+ satisfying the condi-
tions:
(1) For each u ∈D, every preﬁx of u is also in D.
(2) For each u ∈D, for every i ∈N+, if ui ∈D then, for every j,
1 ≤j ≤i, uj is also in D.
EXAMPLE 2.2.1
The tree domain
D = {e, 1, 2, 11, 21, 22, 221, 222, 2211}
is represented as follows:

14
2/Mathematical Preliminaries
e
↙
↘
1
2
↙
↙
↘
11
21
22
↙
↘
221
222
↓
2211
2.2.2 Trees
Given a set Σ of labels, a Σ-tree (for short, a tree) is a total function t : D →Σ,
where D is a tree domain.
The domain of a tree t is denoted by dom(t). Every string u in dom(t)
is called a tree address or a node.
EXAMPLE 2.2.2
Let Σ = {f, g, h, a, b}. The tree t : D →Σ, where D is the tree domain
of example 2.2.1 and t is the function whose graph is
{(e, f), (1, h), (2, g), (11, a), (21, a), (22, f), (221, h), (222, b), (2211, a)}
is represented as follows:
f
↙
↘
h
g
↙
↙
↘
a
a
f
↙
↘
h
b
↓
a
The outdegree (sometimes called ramiﬁcation) d(u) of a node u is the
cardinality of the set {i | ui ∈dom(t)}. Note that the outdegree of a node can
be inﬁnite. Most of the trees that we shall consider will be ﬁnite-branching,
that is, for every node u, d(u) will be an integer, and hence ﬁnite. A node
of outdegree 0 is called a leaf . The node whose address is e is called the root
of the tree. A tree is ﬁnite if its domain dom(t) is ﬁnite. Given a node u in
dom(t), every node of the form ui in dom(t) with i ∈N+ is called a son (or
immediate successor) of u.

2.2 Tree Domains and Trees
15
Tree addresses are totally ordered lexicographically as follows: u ≤v if
either u is a preﬁx of v or, there exist strings x, y, z ∈N∗
+ and i, j ∈N+, with
i < j, such that u = xiy and v = xjz. In the ﬁrst case, we say that u is an
ancestor (or predecessor) of v (or u dominates v) and in the second case, that
u is to the left of v. If y = e and z = e, we say that xi is a left brother (or left
sibling) of xj, (i < j). Two tree addresses u and v are independent if u is not
a preﬁx of v and v is not a preﬁx of u.
2.2.3 Paths
A ﬁnite path with source u and target v is a ﬁnite sequence of nodes u0,u1,...,un
such that u0 = u, un = v, and for all j, 1 ≤j ≤n, uj = uj−1ij for some
ij ∈N+. The length of a path u0, u1, ..., un is n (n ≥0). When n = 0, we
have the null path from u to u (of length 0). A branch (or chain) is a path
from the root to a leaf. An inﬁnite path with source u is an inﬁnite sequence
of nodes u0,u1,...,un,..., such that u0 = u and, for all j ≥1, uj = uj−1ij for
some ij ∈N+.
Given a ﬁnite tree t, the height of a node u in dom(t) is equal to
max({length(p) | p is a path from u to a leaf}). The depth of a ﬁnite tree
is the height of its root (the length of a longest path from the root to a leaf).
2.2.4 Subtrees
Given a tree t and a node u in dom(t), the subtree rooted at u (also called
scope) is the tree t/u whose domain is the set {v | uv ∈dom(t)} and such
that t/u(v) = t(uv) for all v in dom(t/u).
Another important operation is the operation of tree replacement (or
tree substitution).
2.2.5 Tree Replacement
Given two trees t1 and t2 and a tree address u in t1, the result of replacing
t2 at u in t1, denoted by t1[u ←t2], is the function whose graph is the set of
pairs
{(v, t1(v)) | u is not a preﬁx of v} ∪{(uv, t2(v)}.
EXAMPLE 2.2.3
Let t1 and t2 be the trees deﬁned by the following diagrams:

16
2/Mathematical Preliminaries
Tree t1
f
↙
↘
h
g
↙
↙
↘
a
a
f
↙
↘
h
b
↓
a
Tree t2
g
↙
↘
a
b
The tree t1[22 ←t2] is deﬁned by the following diagram:
f
↙
↘
h
g
↙
↙
↘
a
a
g
↙
↘
a
b
2.2.6 Ranked Alphabets and Σ-Trees
In many situations, it is desirable to have a standard set of symbols to name
operations taking a speciﬁed number of arguments. Such a set is called a
ranked alphabet (or simply stratiﬁed alphabet, or signature).
A ranked alphabet is a set Σ together with a rank function r : Σ →N.
Every symbol f ∈Σ has a rank (or arity) r(f) indicating the ﬁxed number of
arguments of f. Symbols of arity 0 are also called constants. For every n ≥0,
Σn denotes the subset of Σ consisting of the function symbols of rank n.
If the set Σ of labels is a ranked alphabet, a Σ-tree is a function t :
dom(t) →Σ as before, with the additional requirement that for every node u
in dom(t), d(u) = r(t(u)). In other words, the outdegree of a node is equal to
the rank of its label.

2.3 Inductive Deﬁnitions
17
EXAMPLE 2.2.4
Let Σ = {a, b, +, ∗}, where a, b have rank 0, and +, ∗have rank 2. The
following is a Σ-tree:
+
↙
↘
a
∗
↙
↘
a
b
The set of all Σ-trees is denoted by CTΣ and the set of all ﬁnite trees
by TΣ. Every one-node tree labeled with a constant a is also denoted by a.
2.3 Inductive Deﬁnitions
Most objects used in logic or computer science are deﬁned inductively. By
this we mean that we frequently deﬁne a set S of objects as:
The smallest set of objects containing a given set X of atoms, and closed
under a given set F of constructors.
The purpose of this section is to deﬁne rigorously what the above sen-
tence means.
2.3.1 Inductive Closures
Let us begin with an example.
EXAMPLE 2.3.1
Let V = {x0, x1, ...} be a countable set of variables, let X = V ∪{0, 1}, let
+ and ∗two binary function symbols, let “(” denote the left parenthesis
and “)” the right parenthesis.
We wish to deﬁne the set EXPR of
arithmetic expressions deﬁned using the variables in V , the constants
0,1, and the operators + and ∗. The following deﬁnition is often given:
An arithmetic expression E is one of the following expressions:
(1) A variable in V , or 0, or 1;
(2) If E1 and E2 are arithmetic expressions, then so are (E1 +E2) and
(E1 ∗E2);
(3) An expression is an arithmetic expression only if it is obtained by
applications of clauses (1) and (2).
In such a deﬁnition called an inductive deﬁnition, clause (1) deﬁnes the
atoms, clause (2) asserts some closure conditions, and clause (3) is supposed
to assert that the set EXPR of arithmetic expressions is the smallest set of

18
2/Mathematical Preliminaries
expressions containing the atoms and closed under the operations described
in (2). However, it is by no means clear that (1),(2),(3) really deﬁne a set,
and that this set is the smallest set having properties deﬁned by clauses (1)
and (2).
The problem with the above deﬁnition is that the universe of all possible
expressions is not deﬁned, and that the operations deﬁned by (2) are not
clearly deﬁned either. This can be remedied as follows. Let Σ be the alphabet
V ∪{0, 1, (, ), +, ∗}, and A = Σ∗be the set of all strings over Σ. The set A
is the universe of all possible expressions.
Note that A contains a lot of
expressions that are not arithmetic expressions, and the purpose of the above
inductive deﬁnition is to deﬁne the subset EXPR of Σ∗describing exactly all
arithmetic expressions. We deﬁne the functions H+ and H∗from A × A to A
as follows: For all strings u, v ∈A,
H+(u, v) = (u + v)
H∗(u, v) = (u ∗v)
The string (u + v) is the string obtained by concatenating the symbol
“(”, the string u, the symbol +, the string v, and the symbol “)”, and similarly
for the string (u ∗v). Also, note that H+ and H∗are deﬁned for all strings
in A, and not just legal arithmetic expressions. For example, if u = 0 and
v = ∗), H+(u, v) = (0 + ∗)), which is not a legal arithmetic expression.
We say that a subset Y of A is closed under H+ and H∗if for all u, v ∈Y ,
H+(u, v) ∈Y and H∗(u, v) ∈Y . We are now in a position to give a precise
deﬁnition of the set EXPR of arithmetic expressions. We deﬁne EXPR as
the least subset of A containing X and closed under H+ and H∗. The only
remaining problem is that we have not shown that such a set actually exists.
This can be shown in two ways that turn out to be equivalent as we will prove
shortly. The ﬁrst method which might be called a top-down method, is to
observe that:
(1) The family C of all subsets Y of A that contain X and are closed
under H+ and H∗is nonempty, since A satisﬁes these properties;
(2) Given any family of subsets of A containing X and closed under H+
and H∗, the intersection of this family also contains X and is closed under
H+ and H∗.
Hence, the least subset X+ of A containing X and closed under H+ and
H∗is the intersection of the family C.
The bottom-up method is to deﬁne a sequence EXPRi of subsets of A
by induction as follows:
EXPR0 = V ∪{0, 1};
EXPRi+1 = EXPRi ∪{H+(u, v), H∗(u, v)|u, v ∈EXPRi}, for i ≥0.
We let X+ =  EXPRi. We shall show below that X+ = X+ and
therefore, EXPR is equal to X+.

2.3 Inductive Deﬁnitions
19
Generalizing the method described in example 2.3.1, we give the follow-
ing general deﬁnition.
Let A be a set, X ⊂A a subset of A, and F a set of functions f : An →A,
each having some arity n > 0. We say that a set Y is inductive on X, iﬀX
is a subset of Y and Y is closed under the functions in F, that is: For every
function f : An →A in F, for every y1, ..., yn ∈Y , f(y1, ..., yn) is also in Y .
Clearly, A itself is inductive on X. The intersection of all inductive sets on
X is also closed under F and it is called the inductive closure of X under F.
Let us denote the inductive closure of X by X+.
If X is nonempty, since every inductive set on X contains X and there is
at least one inductive set on X (namely A), X+ is nonempty. Note that X+
is the least inductive set containing X. The above deﬁnition is what we might
call a top-down deﬁnition. Frequently, X+ is called the least set containing
X and closed under F. There is also a bottom-up and more constructive way
of characterizing X+. The sequence of sets (Xi)i≥0 is deﬁned by induction as
follows:
X0 = X and
Xi+1 = Xi ∪{f(x1, ..., xn) | f ∈F, x1, ..., xn ∈Xi, n = r(f)}.
It is clear that Xi ⊆Xi+1 for all i ≥0. Let
X+ =

i≥0
Xi.
Lemma 2.3.1
X+ = X+.
Proof : First we show that X+ is inductive on X. Since X0 = X, X+
contains X. Next, we show that X+ is closed under F. For every f in F of
arity n > 0 and for all x1, ..., xn ∈X+, by deﬁnition of X+ there is some i
such that x1, ..., xn are all in Xi, and since f(x1, ..., xn) ∈Xi+1 (by deﬁnition),
f(x1, ..., xn) ∈X+. Since X+ is inductive on X and X+ is the least inductive
set containing X, X+ is a subset of X+.
To prove that X+ is a subset of X+, we prove by induction that Xi is
a subset of X+ for every i ≥0. But this is obvious since X+ is closed under
F. Hence, we have shown that X+ = X+.
The following induction principle for inductive sets is very useful:
Induction Principle for Inductive Sets
If X+ is the inductive closure of X under F, for every subset Y of X+,
if Y contains X and is closed under F, then Y = X+.
Lemma 2.3.2
The induction principle for inductive sets holds.

20
2/Mathematical Preliminaries
Proof : By hypothesis, Y is inductive on X. By lemma 2.3.1, X+ = X+
which is the least inductive set containing X. Hence, X+ is a subset of Y .
But Y is contained in X+, so Y = X+.
As an illustration of the induction principle, we prove that every arith-
metic expression in EXPR has the same number of left and right parentheses.
Let Y be the subset of EXPR consisting of all expressions having an equal
number of left and right parentheses. Note that Y contains X since neither
the variables nor 0 nor 1 contain parentheses. Y is closed under H+ and H∗
since these function introduce matching parentheses. Hence, by the induction
principle, Y = EXPR.
2.3.2 Freely Generated Sets
One frequently needs to deﬁne functions recursively over an inductive closure.
For example, one may want to deﬁne the process of evaluating arithmetic
expressions.
EXAMPLE 2.3.2
Let E be the arithmetic expression ((x0 + 1) ∗x1). Assume that we
want to evaluate the value of the expression E for the assignment to the
variables given by x0 = 2, x1 = 3. Naturally, one will ﬁrst compute the
value of (x0+1), which is (2+1) = 3, and then the value of ((x0+1)∗x1)
which is (3 ∗3) = 9. Suppose that we now make the problem slightly
more complicated.
We want a method which, given any assignment
v : V ∪{0, 1} →N of natural numbers to the variables such that v(0) = 0
and v(1) = 1, allows us to evaluate any expression E. The method is to
evaluate expressions recursively. This means that we deﬁne the function
v : EXPR →N such that:
(0) v(E) = v(xi), if E is the variable xi; v(0) = 0, v(1) = 1;
(1) v(E) = v(E1) + v(E2), if E is (E1 + E2);
(2) v(E) = v(E1) ∗v(E2), if E is (E1 ∗E2).
Note that v is an extension of v, and in fact, it can be shown that
it is the unique extension of v satisfying (1) and (2).
However, it is
not obvious that there is a function v satisfying (0),(1),(2), and if such
a function exists, it is not clear that it is unique. The existence and
uniqueness of the function v is a consequence of special properties of
the inductive closure EXPR. In fact, given an inductive closure X+
deﬁned by a set X and a set F of functions, it is not always possible
to deﬁne recursively a function extending a given function v : X →B
(for some set B). We refer the reader to the problems of this chapter
for a counter example. It turns out that functions are properly deﬁned
by recursion on an inductive closure exactly when this inductive closure
is freely generated. The set EXPR of expressions happens to be freely
generated, and this is the reason functions are well deﬁned by recursion.

2.3 Inductive Deﬁnitions
21
To give an intuitive explanation of what freely generated means, observe
that the bottom-up deﬁnition of X+ suggests that each element of X+ can
be represented by a set of trees. Indeed, each atom, that is, each element
x of X, is represented by the one-node tree labeled with that element, and
each element a = f(x1, ..., xn) ∈Xk+1 is represented by all trees of the form
f(t1, ..., tn), where each subtree ti is any of the trees representing xi. Each
element of X+ is usually represented by many diﬀerent trees.
Roughly speaking, an inductive closure X+ is freely generated by X and
F if every element a of X+ is represented by a unique tree.
EXAMPLE 2.3.3
Let A = {a, b, c} and ∗: A × A →A be the function deﬁned by the
following multiplication table:
∗
a
b
c
a
a
b
c
b
b
c
a
c
c
a
b
Since c = ∗(b, b) and a = ∗(∗(b, b), b), the inductive closure of X = {b}
is A.
The element a is represented by the trees ∗(b, c), ∗(∗(b, b), b),
∗(∗(a, b), c), ∗(∗(a, b), ∗(b, b)), and in fact by inﬁnitely many trees. As a
consequence, A is not freely generated by X.
Technically, the deﬁnition of free generation is as follows.
Let A be a set, X a subset of A, F a set of functions on A, and X+ the
inductive closure of X under F. We say that X+ is freely generated by X and
F if the following conditions hold:
(1) The restriction of every function f : Am →A in F to Xm
+ is injective.
(2) For every f : Am →A, g : An →A in F, f(Xm
+ ) is disjoint from
g(Xn
+) whenever f ̸= g.
(3) For every f : Am →A in F and every (x1, ..., xm) ∈Xm
+ ,
f(x1, ..., xm) /∈X.
Let X−1 = ∅. We now show the following lemma.
Lemma 2.3.3
If X+ is freely generated by X and F, then for every i ≥0,
Xi−1 ̸= Xi and f(x1, ..., xn) /∈Xi, for every f in F of arity n and every
(x1, ..., xn) ∈Xn
i −Xn
i−1.
Proof : We proceed by induction on i ≥0. This is obvious for i = 0 since
X−1 = ∅, X0 = X and by condition (3). For i > 0, we prove by induction on k,
0 ≤k ≤i, that if (x1, ..., xn) ∈Xn
i −Xn
i−1, then f(x1, ..., xn) /∈Xk. For k = 0,
this follows from condition (3). Now, assume that if (x1, ..., xn) ∈Xn
i −Xn
i−1,
then f(x1, ..., xn) /∈Xk, for 0 ≤k ≤i −1. If f(x1, ..., xn) ∈Xk+1, then

22
2/Mathematical Preliminaries
f(x1, ..., xn) ∈Xk+1 −Xk.
By condition (2) and the deﬁnition of Xk+1,
there is some (y1, ..., yn) in Xn
k such that f(x1, ..., xn) = f(y1, ..., yn). Since
f is injective on Xn
+, we have xm = ym for 1 ≤m ≤n. Hence, we have
(x1, ..., xn) ∈Xn
k for k < i, contradicting the hypothesis that (x1, ..., xn) ∈
Xn
i −Xn
i−1. Therefore, f(x1, ..., xn) /∈Xk+1, establishing the induction step
on k. But this also shows that Xi ̸= Xi+1, concluding the induction step on
i.
It should be noted that conditions (1),(2),(3) apply to the restrictions
of the functions in F to X+. Indeed, there are cases in which the functions
in F are not injective on A, and f(Am) ∩g(An) ̸= ∅for distinct functions
f, g, but conditions (1),(2),(3) hold and X+ is freely generated. See problem
3.2.5. Lemma 2.3.3 can be used to formalize the statement that X+ is freely
generated by X and F iﬀevery element has a unique tree representation.
However, in order to deﬁne precisely what representation by trees means, it is
necessary to show that trees are freely generated, and to deﬁne a function from
trees to X+ using theorem 2.3.1 proved next. For details of this representation,
the reader is referred to the problems.
In logic, terms, formulae, and proofs are given by inductive deﬁnitions.
Another important concept is that of a function deﬁned recursively over an
inductive set freely generated.
2.3.3 Functions Deﬁned Recursively over Freely Gener-
ated Inductive Sets
Let A be a nonempty set, X a subset of A, F a set of functions on A, and X+
the inductive closure of X under F. Let B be any nonempty set, and let G be
a set of functions over the set B, such that there is a function d : F →G that
associates with each function f of arity n in F, the function d(f) : Bn →B
in G (d need not be a bijection).
Theorem 2.3.1
(Unique homomorphic extension theorem) If X+ is freely
generated by X and F, for every function h : X →B, there is a unique
function h : X+ →B such that:
(1) For all x ∈X, h(x) = h(x);
For every function f of arity n > 0 in F, for every x1, ..., xn ∈Xn
+,
(2) h(f(x1, ..., xn)) = g(h(x1), ...,h(xn)), where g = d(f).
The diagram below illustrates the fact that h extends h. The function
η is the inclusion function of X into X+.
X
η
−→
X+
h ↘
h
B

2.3 Inductive Deﬁnitions
23
The identities (1) and (2) mean that h is a homomorphism, which is often
called the unique homomorphic extension of h. Clause (2) can be described
by the following commutative diagram:
Xn
+
f
−→
X+
hn
h
Bn
−→
d(f)
B
In the above diagram, the function hn is deﬁned by hn(x1, ..., xn) =
(h(x1), ...,h(xn)). We say that this diagram is commutative if the composition
f ◦h is equal to the composition hn ◦g.
Proof : We deﬁne by induction a sequence of functions hi : Xi →B
satisfying conditions (1) and (2) restricted to Xi. We set h0 = h. Given hi,
let hi+1 have the graph:
{(f(x1, ..., xn), g(hi(x1), ..., hi(xn))) | (x1, ..., xn) ∈Xn
i −Xn
i−1, f ∈F}∪
graph(hi)
(with g=d(f).)
We have to check that this graph is indeed functional. Since X+ is freely
generated, by lemma 2.3.3, f(x1, ..., xn) ∈Xi+1 −Xi whenever (x1, ..., xn) ∈
Xn
i −Xn
i−1, (i ≥0), and we only have to check functionality for the ﬁrst
part of the union. Since the elements of G are functions, by lemma 2.3.3,
the only possibility for having (x, y) ∈graph(hi) and (x, z) ∈graph(hi) for
some x ∈Xi+1 −Xi, is to have x = f(x1, ..., xm) = f ′(y1, ..., yn) for some
(x1, ..., xm) ∈Xm
i −Xm
i−1, (y1, ..., yn) ∈Xn
i −Xn
i−1 and for some constructors
f and f ′ in F.
Since f(Xm
+ ) and f ′(Xn
+) are disjoint whenever f ̸= f ′,
f(x1, ..., xm) = f ′(y1, ..., yn) implies that f = f ′ and m = n. Since every
f ∈F is injective on Xn
+, we must also have xj = yj for every j, 1 ≤j ≤n.
But then, y = z = g(x1, ..., xn), with g = d(f), showing functionality. Using
lemma 2.1.4, h = 
i≥0 hi is a partial function. Since dom(h) =  dom(hi) =
 Xi = X+, h is total on X+. Furthermore, it is clear by deﬁnition of the hi
that h satisﬁes (1) and (2). To prove that h is unique, for any other function
h′ satisfying (1) and (2), it can be easily shown by induction that h and h′
agree on Xi for all i ≥0. This proves the theorem.
EXAMPLE 2.3.4
Going back to example 2.3.2, the set A is Σ∗, the set F of functions is
{H+, H∗}, the set B is N, the set G consists of addition and multipli-
cation on the natural numbers, and the function d : F →G is given by
d(H+) = addition and d(H∗) = multiplication. It can be shown that
EXPR is freely generated by V ∪{0, 1} and {H+, H∗}, but this is not
obvious. Indeed, one has to prove rigorously conditions (1),(2),(3), for
the functions H+ and H∗, and this requires some work. A proof can

24
2/Mathematical Preliminaries
be given by adapting the method used in theorem 3.2.1, and we leave
it as an exercise.
Since EXPR is freely generated, for any function
v : V ∪{0, 1} →N such that v(0) = 0 and v(1) = 1, by theorem 2.3.1,
there is a unique function v extending v which is a homomorphism.
Later on when we deﬁne satisfaction in ﬁrst-order logic, we will need to
deﬁne the concept of a structure, and we will have to reformulate slightly the
notion of an inductive closure. This can be done conveniently by introducing
the concept of an algebra. Since this material is only used in Chapter 5 and
Chapter 10, it has been included in an appendix.
PROBLEMS
2.1.1.
Show the following properties:
(a) If there exists a function g : B →A such that f ◦g = IA, then
f : A →B is injective. If f : A →B is injective and A ̸= ∅, then
there exists a function g : B →A such that f ◦g = IA.
(b) A function f is surjective if and only if there exists a function
g : B →A such that g ◦f = IB.
(c) A function f : A →B is bijective if and only if there is a function
f −1 called its inverse such that f ◦f −1 = IA and f −1 ◦f = IB.
2.1.2.
Prove that a function f : A →B is injective if and only if, for all
functions g, h : C →A, g ◦f = h ◦f implies that g = h. A function
f : A →B is surjective if and only if for all functions g, h : B →C,
f ◦g = f ◦h implies that g = h.
2.1.3.
Given a relation R on a set A, prove that R is transitive if and only
if R ◦R is a subset of R.
2.1.4.
Given two equivalence relations R and S on a set A, prove that if
R ◦S = S ◦R, then R ◦S is the least equivalence relation containing
R and S.
2.1.5.
Prove that R+ = 
n≥1 Rn is the smallest transitive relation on A
containing R, and R∗= 
n≥0 Rn is the smallest reﬂexive and tran-
sitive relation on A containing R. Prove that for any relation R on a
set A, (R ∪R−1)∗is the least equivalence relation containing R.
2.1.6.
Show that complete induction is not valid for posets that are not
well-founded by giving a counter example.
2.1.7.
Let (A, ≤) and (B, ≤′) be two partially ordered sets.
A function
f : A →B is monotonic if, for all a, b ∈A, a ≤b implies f(a) ≤′ f(b).
(a) Show that the composition of monotonic functions is monotonic.

PROBLEMS
25
(b) Show that if f is monotonic and m is the least element of a subset
S of A, then f(m) is the least element of f(S).
(c) Give a counter example showing that if f is monotonic and m is
the least element of A, then f(m) is not necessarily the least element
of B. Give a counter example showing that if m is a minimal element
of S, then f(m) is not necessarily a minimal element of f(S), even if
f is surjective.
∗2.1.8.
Given a set A, a multiset over A is an unordered collection of ele-
ments of A that may have multiple occurrences of identical elements.
Formally, a multiset over A may be deﬁned as a function M : A →N,
where N denotes the set of nonnegative integers. An element a in A
has exactly n occurrences in M iﬀM(a) = n. In particular, a does
not belong to M iﬀM(a) = 0. Let M(A) denote the set of ﬁnite
multisets over A, that is, the set of functions M : A →N such that
M(a) ̸= 0 only for ﬁnitely many a ∈A. Two (ﬁnite) multisets M and
M ′ are equal iﬀevery element occurring exactly n times in A also
occurs exactly n times in B.
Multiset union and multiset diﬀerence is deﬁned as follows: Given two
multisets M1 : A →N and M2 : A →N, their union is the multiset
M : A →N, such that for all x ∈A, M(x) = M1(x) + M2(x). The
union of M1 and M2 is also denoted as M1 ∪M2. The difference
of M1 and M2 is the multiset M : A →N such that for all x ∈A,
M(x) = M1(x)−M2(x) if M1(x) ≥M2(x), M(x) = 0 otherwise. The
diﬀerence of M1 and M2 is also denoted as M1 −M2. A multiset M1
is a submultiset of a multiset M2 if for all x ∈A, M1(x) ≤M2(x).
If A is partially ordered by ⪯, the relation << on the set M(A) of
ﬁnite multisets is deﬁned as follows:
M << M ′ iﬀM is obtained from M ′ by removing zero or more
elements from M ′, and replacing each such element x by zero or any
ﬁnite number of elements from A, each strictly less than x (in the
ordering ⪯).
Formally, M << M ′ iﬀM = M ′, or there exist some ﬁnite multisets
X, Y with X a nonempty submultiset of M ′ such that, M = (M ′ −
X) ∪Y , and for all y ∈Y , there is some x ∈X such that y ≺x.
(a) If A = N = {0, 1, 2, ...} and ⪯is the natural ordering on N, give
examples of pairs of multisets related by the relation <<.
(b) Prove that << is a partial ordering.
(c) Assume that ⪯is well founded. To prove that << is also well
founded, we shall proceed by contradiction as follows. Assume that

26
2/Mathematical Preliminaries
there is an inﬁnite decreasing sequence M0 >> M1 >> ... >>
Mn >> Mn+1 >> ..., and Mn ̸= Mn+1 for all n ≥0 (M >> N
holds iﬀN << M holds). We build a tree in the following fashion:
Begin with a root node whose immediate descendants are labeled with
the elements of M0. Since M0 >> M1 (and M0 ̸= M1), there exist
multisets X and Y with X a nonempty multiset of M0, such that
M1 = (M0 −X) ∪Y , and for every y ∈Y , there is some x ∈X and
y ≺x. For each y in Y , choose some x in X such that y ≺x, and add
a successor node labeled y to the node corresponding to that x. For
every remaining x in X (element that is dropped and replaced by no
elements at all), add a successor labeled with the special symbol ⊥.
This last step guarantees that at least one new node is added to the
tree for every multiset Mn in the sequence. This is necessary in case
Y is empty. Repeat the process for M1 >> M2, M2 >> M3, and so
on. Let T be the resulting tree.
Note that by construction, the elements on any path form a strictly
decreasing sequence in A (we can assume that ⊥is less than any
element in A).
(i) Prove that the tree T is inﬁnite and that each node has a ﬁnite
number of successors.
Then, by K¨onig’s lemma (if a tree is ﬁnite
branching and inﬁnite then it contains an inﬁnite path), there must
be an inﬁnite path in T.
(ii) Prove that there is a path in T corresponding to an inﬁnite de-
creasing sequence of elements in A. Conclude that the partial ordering
<< is well founded.
2.2.1.
Let t be a tree and let u and v be two independent tree addresses
in dom(t) (that is, u is not a preﬁx of v and v is not a preﬁx of u).
Prove that for any trees t1 and t2,
t[u ←t1][v ←t2] = t[v ←t2][u ←t1].
2.3.1.
Let A = {a, b, c} and ∗: A × A →A be the function deﬁned by the
following table:
∗
a
b
c
a
a
b
c
b
b
c
a
c
c
a
b
(a) Show that the inductive closure of X = {b} is A.

PROBLEMS
27
(b) If N denotes the set of nonnegative integers and + is addition
(of integers), show that there is some function h : X →N which
does not have any homomorphic extension to A (a function g is a
homomorphic extension of h if, g(b) = h(b) and g(x∗y) = g(x)+g(y),
for all x, y ∈A).
Find an inﬁnite set that is the inductive closure of a ﬁnite set, but is
not freely generated.
2.3.2.
Show that if X+ is freely generated by X and F, then Xn
i −Xn
i−1 ̸=
(Xi −Xi−1)n. Show that if X+ is not freely generated, then Xn
i −
Xn
i−1 = (Xi −Xi−1)n is possible.
∗2.3.3.
Recall from Subsection 2.2.6 that TΣ denotes the set of all ﬁnite
Σ-trees over the ranked alphabet Σ.
Every function symbol f of
rank n > 0 deﬁnes the function f : T n
Σ →TΣ as follows: For every
t1, t2, ..., tn ∈TΣ, f(t1, t2, ..., tn) is the tree denoted by ft1t2...tn and
whose graph is the set of pairs
{(e, f)} ∪
i=n

i=1
{(iu, ti(u)) | u ∈dom(ti)}.
The tree ft1...tn is the tree with f at the root and ti as the subtree
at address i.
(a) Prove that TΣ is freely generated by the set Σ0 of constant symbols
in Σ and the functions f deﬁned above.
Hint: See the proof of lemma 2.4.2 in the Appendix.
Let A be a set, X a subset of A, F a set of functions on A, and X+
the inductive closure of X under F. We deﬁne the ranked alphabet
Σ as follows:
Σ0 = X,
Σn = {f | f ∈F of rank n}.
(b) Prove that the unique homomorphic extension h : TΣ →X+ of
the inclusion function J : X →X+ is surjective. We say that a tree
t ∈TΣ represents an element x ∈X+ iﬀh(t) = x.
(c) Prove that X+ is freely generated by X and F iﬀh is a bijection.
∗2.3.4.
Prove that EXPR is freely generated by V ∪{0, 1} and {H+, H∗}.
Hint: Use the proof technique of theorem 3.2.1.

Chapter 3
Propositional Logic
3.1 Introduction
Every logic comprises a (formal) language for making statements about ob-
jects and reasoning about properties of these objects. This view of logic is very
general and actually we will restrict our attention to mathematical objects,
programs, and data structures in particular. Statements in a logical language
are constructed according to a predeﬁned set of formation rules (depending
on the language) called syntax rules.
One might ask why a special language is needed at all, and why English
(or any other natural language) is not adequate for carrying out logical reason-
ing. The ﬁrst reason is that English (and any natural language in general) is
such a rich language that it cannot be formally described. The second reason,
which is even more serious, is that the meaning of an English sentence can be
ambiguous, subject to diﬀerent interpretations depending on the context and
implicit assumptions. If the object of our study is to carry out precise rigor-
ous arguments about assertions and proofs, a precise language whose syntax
can be completely described in a few simple rules and whose semantics can
be deﬁned unambiguously is required.
Another important factor is conciseness. Natural languages tend to be
verbose, and even fairly simple mathematical statements become exceedingly
long (and unclear) when expressed in them. The logical languages that we
shall deﬁne contain special symbols used for abbreviating syntactical con-
28

3.1 Introduction
29
structs.
A logical language can be used in diﬀerent ways. For instance, a language
can be used as a deduction system (or proof system); that is, to construct
proofs or refutations. This use of a logical language is called proof theory. In
this case, a set of facts called axioms and a set of deduction rules (inference
rules) are given, and the object is to determine which facts follow from the
axioms and the rules of inference. When using logic as a proof system, one is
not concerned with the meaning of the statements that are manipulated, but
with the arrangement of these statements, and speciﬁcally, whether proofs or
refutations can be constructed. In this sense, statements in the language are
viewed as cold facts, and the manipulations involved are purely mechanical, to
the point that they could be carried out by a computer. This does not mean
that ﬁnding a proof for a statement does not require creativity, but that the
interpetation of the statements is irrelevant. This use of logic is similar to
game playing. Certain facts and rules are given, and it is assumed that the
players are perfect, in the sense that they always obey the rules. Occasionally,
it may happen that following the rules leads to inconsistencies, in which case
it may be necessary to revise the rules.
However, the statements expressed in a logical language often have an
intended meaning.
The second use of a formal language is for expressing
statements that receive a meaning when they are given what is called an in-
terpretation. In this case, the language of logic is used to formalize properties
of structures, and determine when a statement is true of a structure. This
use of a logical language is called model theory.
One of the interesting aspects of model theory is that it forces us to
have a precise and rigorous deﬁnition of the concept of truth in a structure.
Depending on the interpretation that one has in mind, truth may have quite
a diﬀerent meaning. For instance, whether a statement is true or false may
depend on parameters.
A statement true under all interpretations of the
parameters is said to be valid. A useful (and quite reasonable) mathematical
assumption is that the truth of a statement can be obtained from the truth
(or falsity) of its parts (substatements). From a technical point of view, this
means that the truth of a statement is deﬁned by recursion on the syntactical
structure of the statement. The notion of truth that we shall describe (due to
Tarski) formalizes the above intuition, and is ﬁrmly justiﬁed in terms of the
concept of an algebra presented in Section 2.4 and the unique homomorphic
extension theorem (theorem 2.4.1).
The two aspects of logic described above are actually not independent,
and it is the interaction between model theory and proof theory that makes
logic an interesting and eﬀective tool. One might say that model theory and
proof theory form a couple in which the individuals complement each other.
To illustrate this point, consider the problem of ﬁnding a procedure for listing
all statements true in a certain class of stuctures. It may be that checking
the truth of a statement requires an inﬁnite computation. Yet, if the class of

30
3/Propositional Logic
structures can be axiomatized by a ﬁnite set of axioms, we might be able to
ﬁnd a proof procedure that will give us the answer.
Conversely, suppose that we have a set of axioms and we wish to know
whether the resulting theory (the set of consequences) is consistent, in the
sense that no statement and its negation follow from the axioms.
If one
discovers a structure in which it can be shown that the axioms and their con-
sequences are true, one will know that the theory is consistent, since otherwise
some statement and its negation would be true (in this structure).
To summarize, a logical language has a certain syntax, and the meaning,
or semantics, of statements expressed in this language is given by an interpre-
tation in a structure. Given a logical language and its semantics, one usually
has one or more proof systems for this logical system.
A proof system is acceptable only if every provable formula is indeed
valid. In this case, we say that the proof system is sound. Then, one tries to
prove that the proof system is complete. A proof system is complete if every
valid formula is provable. Depending on the complexity of the semantics of a
given logic, it is not always possible to ﬁnd a complete proof system for that
logic. This is the case, for instance, for second-order logic. However, there
are complete proof systems for propositional logic and ﬁrst-order logic. In the
ﬁrst-order case, this only means that a procedure can be found such that, if the
input formula is valid, the procedure will halt and produce a proof. But this
does not provide a decision procedure for validity. Indeed, as a consequence
of a theorem of Church, there is no procedure that will halt for every input
formula and decide whether or not a formula is valid.
There are many ways of proving the completeness of a proof system.
Oddly, most proofs establishing completeness only show that if a formula A is
valid, then there exists a proof of A. However, such arguments do not actually
yield a method for constructing a proof of A (in the formal system). Only the
existence of a proof is shown. This is the case in particular for so-called Henkin
proofs. To illustrate this point in a more colorful fashion, the above situation is
comparable to going to a restaurant where you are told that excellent dinners
exist on the menu, but that the inexperienced chef does not know how to
prepare these dinners. This may be satisfactory for a philosopher, but not for
a hungry computer scientist! However, there is an approach that does yield a
procedure for constructing a formal proof of a formula if it is valid. This is the
approach using Gentzen systems (or tableaux systems). Furthermore, it turns
out that all of the basic theorems of ﬁrst-order logic can be obtained using
this approach. Hence, this author feels that a student (especially a computer
scientist) has nothing to lose, and in fact will reap extra beneﬁts by learning
Gentzen systems ﬁrst.
Propositional logic is the system of logic with the simplest semantics.
Yet, many of the concepts and techniques used for studying propositional logic
generalize to ﬁrst-order logic. Therefore, it is pedagogically sound to begin by

3.2 Syntax of Propositional Logic
31
studying propositional logic as a “gentle” introduction to the methods used
in ﬁrst-order logic.
In propositional logic, there are atomic assertions (or atoms, or propo-
sitional letters) and compound assertions built up from the atoms and the
logical connectives, and, or, not, implication and equivalence. The atomic
facts are interpreted as being either true or false. In propositional logic, once
the atoms in a proposition have received an interpretation, the truth value
of the proposition can be computed.
Technically, this is a consequence of
the fact that the set of propositions is a freely generated inductive closure.
Certain propositions are true for all possible interpretations. They are called
tautologies.
Intuitively speaking, a tautology is a universal truth.
Hence,
tautologies play an important role.
For example, let “John is a teacher,” “John is rich,” and “John is a
rock singer” be three atomic propositions. Let us abbreviate them as A,B,C.
Consider the following statements:
“John is a teacher”;
It is false that “John is a teacher” and “John is rich”;
If “John is a rock singer” then “John is rich.”
We wish to show that the above assumptions imply that
It is false that “John is a rock singer.”
This amounts to showing that the (formal) proposition
(∗) (A and not(A and B) and (C implies B)) implies (not C)
is a tautology. Informally, this can be shown by contradiction. The statement
(∗) is false if the premise (A and not(A and B) and (C implies B)) is true
and the conclusion (not C) is false. This implies that C is true. Since C is
true, then, since (C implies B) is assumed to be true, B is true, and since A is
assumed to be true, (A and B) is true, which is a contradiction, since not(A
and B) is assumed to be true.
Of course, we have assumed that the reader is familiar with the semantics
and the rules of propositional logic, which is probably not the case. In this
chapter, such matters will be explained in detail.
3.2 Syntax of Propositional Logic
The syntax of propositional logic is described in this section. This presentation
will use the concept of an inductive closure explained in Section 2.3, and the
reader is encouraged to review it.

32
3/Propositional Logic
3.2.1 The Language of Propositional Logic
Propositional formulae (or propositions) are strings of symbols from a count-
able alphabet deﬁned below, and formed according to certain rules stated in
deﬁnition 3.2.2.
Deﬁnition 3.2.1
(The alphabet for propositional formulae) This alphabet
consists of:
(1) A countable set PS of proposition symbols: P0,P1,P2...;
(2) The logical connectives: ∧(and), ∨(or), ⊃(implication), ¬ (not),
and sometimes ≡(equivalence) and the constant ⊥(false);
(3) Auxiliary symbols: “(” (left parenthesis), “)” (right parenthesis).
The set PROP of propositional formulae (or propositions) is deﬁned as
the inductive closure (as in Section 2.3) of a certain subset of the alphabet of
deﬁnition 3.2.1 under certain operations deﬁned below.
Deﬁnition 3.2.2
Propositional formulae. The set PROP of propositional
formulae (or propositions) is the inductive closure of the set PS ∪{⊥} under
the functions C¬, C∧, C∨, C⊃and C≡, deﬁned as follows: For any two strings
A, B over the alphabet of deﬁnition 3.2.1,
C¬(A) = ¬A,
C∧(A, B) = (A ∧B),
C∨(A, B) = (A ∨B),
C⊃(A, B) = (A ⊃B) and
C≡(A, B) = (A ≡B).
The above deﬁnition is the oﬃcial deﬁnition of PROP as an inductive
closure, but is a bit formal. For that reason, it is often stated less formally as
follows:
The set PROP of propositions is the smallest set of strings over the
alphabet of deﬁnition 3.2.1, such that:
(1) Every proposition symbol Pi is in PROP and ⊥is in PROP;
(2) Whenever A is in PROP, ¬A is also in PROP;
(3) Whenever A, B are in PROP, (A ∨B), (A ∧B), (A ⊃B) and
(A ≡B) are also in PROP.
(4) A string is in PROP only if it is formed by applying the rules
(1),(2),(3).
The oﬃcial inductive deﬁnition of PROP will be the one used in proofs.

3.2 Syntax of Propositional Logic
33
3.2.2 Free Generation of PROP
The purpose of the parentheses is to ensure unique readability; that is, to
ensure that PROP is freely generated on PS. This is crucial in order to give
a proper deﬁnition of the semantics of propositions. Indeed, the meaning of a
proposition will be given by a function deﬁned recursively over the set PROP,
and from theorem 2.4.1 (in the Appendix), we know that such a function exists
and is unique when an inductive closure is freely generated.
There are other ways of deﬁning the syntax in which parentheses are un-
necessary, for example the preﬁx (or postﬁx) notation, which will be discussed
later.
It is necessary for clarity and to avoid contradictions to distinguish be-
tween the formal language that is the object of our study (the set PROP of
propositions), and the (informal) language used to talk about the object of
study. The ﬁrst language is usually called the object language and the second,
the meta-language. It is often tedious to maintain a clear notational distinc-
tion between the two languages, since this requires the use of a formidable
number of symbols. However, one should always keep in mind this distinction
to avoid confusion (and mistakes !).
For example, the symbols P, Q, R, ... will usually range over proposi-
tional symbols, and the symbols A, B, C, ... over propositions. Such symbols
are called meta-variables.
Let us give a few examples of propositions:
EXAMPLE 3.2.1
The following strings are propositions.
P1,
P2,
(P1 ∨P2),
((P1 ⊃P2) ≡(¬P1 ∨P2)),
(¬P1 ≡(P1 ⊃⊥)),
(((P1 ⊃P2) ∧¬P2) ⊃¬P1),
(P1 ∨¬P1).
On the other hand, strings such as
(()), or
(P1 ∨P2)∧
are not propositions, because they cannot be constructed from PS and
⊥and the logical connectives.
Since PROP is inductively deﬁned on PS, the induction principle (of
Section 2.3) applies. We are now going to use this induction principle to show
that PROP is freely generated by the propositional symbols (and ⊥) and the
logical connectives.

34
3/Propositional Logic
Lemma 3.2.1
(i) Every proposition in PROP has the same number of left
and right parentheses.
(ii) Any proper preﬁx of a proposition is either the empty string, a
(nonempty) string of negation symbols, or it contains an excess of left paren-
theses.
(iii) No proper preﬁx of a proposition can be a proposition.
Proof : (i) Let S be the set of propositions in PROP having an equal
number of left and right parentheses. We show that S is inductive on the set
of propositional symbols and ⊥. By the induction principle, this will show
that S = PROP, as desired. It is obvious that S contains the propositional
symbols (no parentheses) and ⊥. It is also obvious that the rules in deﬁnition
3.2.2 introduce matching parentheses and so, preserve the above property.
This concludes the ﬁrst part of the proof.
(ii) Let S be the set of propositions in PROP such that any proper preﬁx
is either the empty string, a string of negations, or contains an excess of left
parentheses. We also prove that S is inductive on the set of propositional
symbols and ⊥.
First, it is obvious that every propositional symbol is in
S, as well as ⊥. Let us verify that S is closed under C∧, leaving the other
cases as an exercise. Let A and B be in S. The nonempty proper preﬁxes of
C∧(A, B) = (A ∧B) are:
(
(C
where C is a proper preﬁx of A
(A
(A∧
(A ∧C
where C is a proper preﬁx of B
(A ∧B
Applying the induction hypothesis that A and B are in S, we obtain the
desired conclusion.
Clause (iii) of the lemma follows from the two previous properties. If a
proper preﬁx of a proposition is a proposition, then by (i), it has the same
number of left and right parentheses. If a proper preﬁx has no parentheses, it
is either the empty string or a string of negations, but neither is a proposition.
If it has parentheses, by property (ii), it has an excess of left parentheses, a
contradiction.
The above lemma allows us to show the theorem:
Theorem 3.2.1
The set PROP of propositions is freely generated by the
propositional symbols in PS, ⊥, and the logical connectives.
Proof : First, we show that the restrictions of the functions C¬, C∧, C∨,
C⊃and C≡to PROP are injective. This is obvious for C¬ and we only check
this for C∧, leaving the other cases as an exercise. If (A ∧B) = (C ∧D),
then A ∧B) = C ∧D). Either A = C, or A is a proper preﬁx of C, or C is

3.2 Syntax of Propositional Logic
35
a proper preﬁx of A. But the last two cases are impossible by lemma 3.2.1.
Then ∧B) = ∧D), which implies B = D.
Next, we have to show that the ranges of the restrictions of the above
functions to PROP are disjoint. We only discuss one case, leaving the others
as an exercise. For example, if (A ∧B) = (C ⊃D), then A ∧B) = C ⊃D).
By the same reasoning as above, we must have A = C. But then, we must
have ∧=⊃, which is impossible. Finally, since all the functions yield a string
of length greater than that of its arguments, all the conditions for being freely
generated are met.
The above result allows us to deﬁne functions over PROP recursively.
Every function with domain PROP is uniquely determined by its restriction
to the set PS of propositional symbols and to ⊥. We are going to use this fact
in deﬁning the semantics of propositional logic. As an illustration of theorem
3.2.1, we give a recursive deﬁnition of the set of propositional letters occurring
in a proposition.
EXAMPLE 3.2.2
The function symbols : PROP →2PS is deﬁned recursively as follows:
symbols(⊥) = ∅,
symbols(Pi) = {Pi},
symbols((B ∗C)) = symbols(B) ∪symbols(C), for ∗∈{∧, ∨, ⊃, ≡},
symbols(¬A) = symbols(A).
For example,
symbols(((P1 ⊃P2) ∨¬P3) ∧P1)) = {P1, P2, P3}.
In order to minimize the number of parentheses, a precedence is assigned
to the logical connectives and it is assumed that they are left associative.
Starting from highest to lowest precedence we have:
¬
∧
∨
⊃, ≡.
EXAMPLE 3.2.3
A ∧B ⊃C is an abbreviation for ((A ∧B) ⊃C),
A ∨B ∧C an abbreviation for (A ∨(B ∧C)), and
A ∨B ∨C is an abbreviation for ((A ∨B) ∨C).

36
3/Propositional Logic
Parentheses can be used to disambiguate expressions. These conventions
are consistent with the semantics of the propositional calculus, as we shall see
in Section 3.3.
Another way of avoiding parentheses is to use the preﬁx notation. In
preﬁx notation, (A ∨B) becomes ∨AB, (A ∧B) becomes ∧AB, (A ⊃B)
becomes ⊃AB and (A ≡B) becomes ≡AB.
In order to justify the legitimacy of the preﬁx notation, that is, to show
that the set of propositions in preﬁx notation is freely generated, we have to
show that every proposition can be written in a unique way. We shall come
back to this when we consider terms in ﬁrst-order logic.
PROBLEMS
3.2.1.
Let PROP be the set of all propositions over the set PS of proposi-
tional symbols. The depth d(A) of a proposition A is deﬁned recur-
sively as follows:
d(⊥) = 0,
d(P) = 0, for each symbol P ∈PS,
d(¬A) = 1 + d(A),
d(A ∨B) = 1 + max(d(A), d(B)),
d(A ∧B) = 1 + max(d(A), d(B)),
d(A ⊃B) = 1 + max(d(A), d(B)),
d(A ≡B) = 1 + max(d(A), d(B)).
If PSi is the i-th stage of the inductive deﬁnition of PROP = (PS ∪
{⊥})+ (as in Section 2.3), show that PSi consists exactly of all propo-
sitions of depth less than or equal to i.
3.2.2.
Which of the following are propositions? Justify your answer.
¬¬¬P1
¬P1 ∨¬P2
¬(P1 ∨P2)
(¬P1 ⊃¬P2)
¬(P1 ∨(P2 ∧(P3 ∨P4) ∧(P1 ∧(P3 ∧¬P1) ∨(P4 ∨P1)))
(Hint: Use problem 3.2.1, lemma 3.2.1.)
3.2.3.
Finish the proof of the cases in lemma 3.2.1.

PROBLEMS
37
3.2.4.
The function sub : PROP →2P ROP which assigns to any proposition
A the set sub(A) of all its subpropositions is deﬁned recursively as
follows:
sub(⊥) = {⊥},
sub(Pi) = {Pi}, for a propositional symbol Pi,
sub(¬A) = sub(A) ∪{¬A},
sub((A ∨B)) = sub(A) ∪sub(B) ∪{(A ∨B)},
sub((A ∧B)) = sub(A) ∪sub(B) ∪{(A ∧B)},
sub((A ⊃B)) = sub(A) ∪sub(B) ∪{(A ⊃B)},
sub((A ≡B)) = sub(A) ∪sub(B) ∪{(A ≡B)}.
Prove that if a proposition A has n connectives, then sub(A) contains
at most 2n + 1 propositions.
3.2.5.
Give an example of propositions A and B and of strings u and v such
that (A ∨B) = (u ∨v), but u ̸= A and v ̸= B. Similarly give an
example such that (A ∨B) = (u ∧v), but u ̸= A and v ̸= B.
∗3.2.6.
The set of propositions can be deﬁned by a context-free grammar,
provided that the propositional symbols are encoded as strings over a
ﬁnite alphabet. Following Lewis and Papadimitriou, 1981, the symbol
Pi, (i ≥0) will be encoded as PI...I$, with a number of I’s equal
to i. Then, PROP is the language L(G) deﬁned by the following
context-free grammar G = (V, Σ, R, S):
Σ = {P, I, $, ∧, ∨, ⊃, ≡, ¬, ⊥}, V = Σ ∪{S, N},
R = {N →e,
N →NI,
S →PN$,
S →⊥,
S →(S ∨S),
S →(S ∧S),
S →(S ⊃S),
S →(S ≡S),
S →¬S}.
Prove that the grammar G is unambiguous.
Note: The above language is actually SLR(1). For details on parsing
techniques, consult Aho and Ullman, 1977.
3.2.7.
The set of propositions in preﬁx notation is the inductive closure of
PS ∪{⊥} under the following functions:

38
3/Propositional Logic
For all strings A, B over the alphabet of deﬁnition 3.2.1, excluding
parentheses,
C∧(A, B) = ∧AB,
C∨(A, B) = ∨AB,
C⊃(A, B) =⊃AB,
C≡(A, B) =≡AB,
C¬(A) = ¬A.
In order to prove that the set of propositions in preﬁx notation is
freely generated, we deﬁne the function K as follows:
K(∧) = −1; K(∨) = −1; K(⊃) = −1; K(≡) = −1; K(¬) = 0;
K(⊥) = 1; K(Pi) = 1, for every propositional symbol Pi.
The function K is extended to strings as follows: For every string
w1...wk (over the alphabet of deﬁnition 3.2.1, excluding parentheses),
K(w1...wk) = K(w1) + ... + K(wk).
(i) Prove that for any proposition A, K(A) = 1.
(ii) Prove that for any proper preﬁx w of a proposition, K(w) ≤0.
(iii) Prove that no proper preﬁx of a proposition is a proposition.
(iv) Prove that the set of propositions in preﬁx notation is freely
generated.
3.2.8.
Suppose that we modify deﬁnition 3.2.2 by omitting all right paren-
theses. Thus, instead of
((P ∧¬Q) ⊃(R ∨S)),
we have
((P ∧¬Q ⊃(R ∨S.
Formally, we deﬁne the functions:
C∧(A, B) = (A ∧B,
C∨(A, B) = (A ∨B,
C⊃(A, B) = (A ⊃A,
C≡(A, B) = (A ≡A,
C¬(A) = ¬A.
Prove that the set of propositions deﬁned in this fashion is still freely
generated.

3.3 Semantics of Propositional Logic
39
3.3 Semantics of Propositional Logic
In this section, we present the semantics of propositional logic and deﬁne the
concepts of satisﬁability and tautology.
3.3.1 The Semantics of Propositions
The semantics of the propositional calculus assigns a truth function to each
proposition in PROP.
First, it is necessary to deﬁne the meaning of the
logical connectives. We ﬁrst deﬁne the domain BOOL of truth values.
Deﬁnition 3.3.1
The set of truth values is the set BOOL = {T, F}. It is
assumed that BOOL is (totally) ordered with F < T.
Each logical connective X is interpreted as a function HX with range
BOOL. The logical connectives are interpreted as follows.
Deﬁnition 3.3.2
The graphs of the logical connectives are represented by
the following table:
P
Q H¬(P) H∧(P, Q) H∨(P, Q) H⊃(P, Q) H≡(P, Q)
T T
F
T
T
T
T
T F
F
F
T
F
F
F T
T
F
T
T
F
F F
T
F
F
T
T
The logical constant ⊥is interpreted as F.
The above table is what is called a truth table. We have introduced the
function HX to distinguish between the symbol X and its meaning HX. This
is a heavy notational burden, but it is essential to distinguish between syntax
and semantics, until the reader is familiar enough with these concepts. Later
on, when the reader has assimilated these concepts, we will often use X for
HX to simplify the notation.
We now deﬁne the semantics of formulae in PROP.
Deﬁnition 3.3.3
A truth assignment or valuation is a function v : PS →
BOOL assigning a truth value to all the propositional symbols. From theo-
rem 2.4.1 (in the Appendix), since PROP is freely generated by PS, every
valuation v extends to a unique function v : PROP →BOOL satisfying the
following clauses for all A, B ∈PROP:

40
3/Propositional Logic
v(⊥) = F,
v(P) = v(P), for all P ∈PS,
v(¬A) = H¬(v(A)),
v((A ∧B)) = H∧(v(A), v(B)),
v((A ∨B)) = H∨(v(A), v(B)),
v((A ⊃B)) = H⊃(v(A), v(B)),
v((A ≡B)) = H≡(v(A), v(B)).
In the above deﬁnition, the truth value v(A) of a proposition A is deﬁned
for a truth assignment v assigning truth values to all propositional symbols,
including inﬁnitely many symbols not occurring in A. However, for any for-
mula A and any valuation v, the value v(A) only depends on the propositional
symbols actually occurring in A. This is justiﬁed by the following lemma.
Lemma 3.3.1
For any proposition A, for any two valuations v and v′ such
that v(P) = v′(P) for all proposition symbols occurring in A, v(A) = v′(A).
Proof : We proceed by induction. The lemma is obvious for ⊥, since ⊥
does not contain any propositional symbols. If A is the propositional symbol
Pi, since v(Pi) = v′(Pi), v(Pi) = v(Pi), and v′(Pi) = v′(Pi), the Lemma holds
for propositional symbols.
If A is of the form ¬B, since the propositional symbols occurring in B
are the propositional symbols occurring in A, by the induction hypothesis,
v(B) = v′(B). Since
v(A) = H¬(v(B))
and
v′(A) = H¬(v′(B)),
we have
v(A) = v′(A).
If A is of the form (B ∗C), for a connective ∗∈{∨, ∧, ⊃, ≡}, since the
sets of propositional letters occurring in B and C are subsets of the set of
propositional letters occurring in A, the induction hypothesis applies to B
and C. Hence,
v(B) = v′(B)
and
v(C) = v′(C).
But
v(A) = H∗(v(B), v(C)) = H∗(v′(B), v′(C)) = v′(A),
showing that
v(A) = v′(A).
Using lemma 3.3.1, we observe that given a proposition A containing the
set of propositional symbols {P1, ..., Pn}, its truth value for any assignment v
can be computed recursively and only depends on the values v(P1), ..., v(Pn).

3.3 Semantics of Propositional Logic
41
EXAMPLE 3.3.1
Let
A = ((P ⊃Q) ≡(¬Q ⊃¬P)).
Let v be a truth assignment whose restriction to {P, Q} is v(P) = T,
v(Q) = F. According to deﬁnition 3.3.3,
v(A) = H≡(v((P ⊃Q)), v((¬Q ⊃¬P))).
In turn,
v((P ⊃Q)) = H⊃(v(P), v(Q))
and
v((¬Q ⊃¬P)) = H⊃(v(¬Q), v(¬P)).
Since
v(P) = v(P)
and
v(Q) = v(Q),
we have
v((P ⊃Q)) = H⊃(T, F) = F.
We also have
v(¬Q) = H¬(v(Q)) = H¬(v(Q))
and
v(¬P) = H¬(v(P)) = H¬(v(P)).
Hence,
v(¬Q) = H¬(F) = T,
v(¬P) = H¬(T) = F
and
v((¬Q ⊃¬P)) = H⊃(T, F) = F.
Finally,
v(A) = H≡(F, F) = T.
The above recursive computation can be conveniently described by a
truth table as follows:
P
Q ¬P
¬Q (P ⊃Q) (¬Q ⊃¬P) ((P ⊃Q) ≡(¬Q ⊃¬P))
T F
F
T
F
F
T
If v(A) = T for a valuation v and a proposition A, we say that v satisﬁes
A, and this is denoted by v |= A. If v does not satisfy A, we say that v falsiﬁes
A, and this is denoted by, not v |= A, or v ̸|= A.
An expression such as v |= A (or v ̸|= A) is merely a notation used in
the meta-language to express concisely statements about satisfaction. The
reader should be well aware that such a notation is not a proposition in the
object language, and should be used with care. An illustration of the danger

42
3/Propositional Logic
of mixing the meta-language and the object language is given by the deﬁnition
(often found in texts) of the notion of satisfaction in terms of the notation
v |= A. Using this notation, the recursive clauses of the deﬁnition of v can be
stated informally as follows:
v ̸|=⊥,
v |= Pi iﬀv(Pi) = T,
v |= ¬A iﬀv ̸|= A,
v |= A ∧B iﬀv |= A and v |= B,
v |= A ∨B iﬀv |= A or v |= B,
v |= A ⊃B iﬀv ̸|= A or v |= B,
v |= A ≡B iﬀ(v |= A iff v |= B).
The above deﬁnition is not really satisfactory because it mixes the object
language and the meta-language too much.
In particular, the meaning of
the words not, or, and and iﬀis ambiguous.
What is worse is that the
legitimacy of the recursive deﬁnition of “|=” is far from being clear. However,
the deﬁnition of |= using the recursive deﬁnition of v is rigorously justiﬁed by
theorem 2.4.1 (in the Appendix).
An important subset of PROP is the set of all propositions that are
true in all valuations. These propositions are called tautologies.
3.3.2 Satisﬁability, Unsatisﬁability, Tautologies
First, we deﬁne the concept of a tautology.
Deﬁnition 3.3.4
A proposition A is valid iﬀv(A) = T for all valuations v.
This is abbreviated as |= A, and A is also called a tautology.
A proposition is satisﬁable if there is a valuation (or truth assignment)
v such that v(A) = T. A proposition is unsatisﬁable if it is not satisﬁed by
any valuation.
Given a set of propositions Γ, we say that A is a semantic consequence
of Γ, denoted by Γ |= A, if for all valuations v, v(B) = T for all B in Γ implies
that v(A) = T.
The problem of determining whether any arbitrary proposition is satis-
ﬁable is called the satisﬁability problem. The problem of determining whether
any arbitrary proposition is a tautology is called the tautology problem.
EXAMPLE 3.3.2
The following propositions are tautologies:
A ⊃A,

3.3 Semantics of Propositional Logic
43
¬¬A ⊃A,
(P ⊃Q) ≡(¬Q ⊃¬P).
The proposition
(P ∨Q) ∧(¬P ∨¬Q)
is satisﬁed by the assignment v(P) = F, v(Q) = T.
The proposition
(¬P ∨Q) ∧(¬P ∨¬Q) ∧P
is unsatisﬁable. The following are valid consequences.
A, (A ⊃B) |= B,
A, B |= (A ∧B),
(A ⊃B), ¬B |= ¬A.
Note that P ⊃Q is false if and only if both P is true and Q is false. In
particular, observe that P ⊃Q is true when P is false.
The relationship between satisﬁability and being a tautology is recorded
in the following useful lemma.
Lemma 3.3.2
A proposition A is a tautology if and only if ¬A is unsatisﬁ-
able.
Proof : Assume that A is a tautology. Hence, for all valuations v,
v(A) = T.
Since
v(¬A) = H¬(v(A)),
v(A) = T
if and only if
v(¬A) = F.
This shows that for all valuations v,
v(¬A) = F,
which is the deﬁnition of unsatisﬁability. Conversely, if ¬A is unsatisﬁable,
for all valuations v,
v(¬A) = F.
By the above reasoning, for all v,
v(A) = T,
which is the deﬁnition of being a tautology.

44
3/Propositional Logic
The above lemma suggests two diﬀerent approaches for proving that
a proposition is a tautology.
In the ﬁrst approach, one attempts to show
directly that A is a tautology.
The method in Section 3.4 using Gentzen
systems illustrates the ﬁrst approach (although A is proved to be a tautology
if the attempt to falsify A fails). In the second approach, one attempts to
show indirectly that A is a tautology, by showing that ¬A is unsatisﬁable.
The method in Chapter 4 using resolution illustrates the second approach.
As we saw in example 3.3.1, the recursive deﬁnition of the unique exten-
sion v of a valuation v suggests an algorithm for computing the truth value
v(A) of a proposition A. The algorithm consists in computing recursively the
truth tables of the parts of A. This is known as the truth table method.
The truth table method clearly provides an algorithm for testing whether
a formula is a tautology: If A contains n propositional letters, one constructs
a truth table in which the truth value of A is computed for all valuations
depending on n arguments. Since there are 2n such valuations, the size of
this truth table is at least 2n. It is also clear that there is an algorithm for
deciding whether a proposition is satisﬁable: Try out all possible valuations
(2n) and compute the corresponding truth table.
EXAMPLE 3.3.3
Let us compute the truth table for the proposition A = ((P ⊃Q) ≡
(¬Q ⊃¬P)).
P
Q ¬P
¬Q (P ⊃Q) (¬Q ⊃¬P) ((P ⊃Q) ≡(¬Q ⊃¬P))
F F
T
T
T
T
T
F T
T
F
T
T
T
T F
F
T
F
F
T
T T
F
F
T
T
T
Since the last column contains only the truth value T, the proposition
A is a tautology.
The above method for testing whether a proposition is satisﬁable or a
tautology is computationally expensive, in the sense that it takes an exponen-
tial number of steps. One might ask if it is possible to ﬁnd a more eﬃcient
procedure. Unfortunately, the satisﬁability problem happens to be what is
called an NP-complete problem, which implies that there is probably no fast
algorithm for deciding satisﬁability. By a fast algorithm, we mean an algo-
rithm that runs in a number of steps bounded by p(n), where n is the length
of the input, and p is a (ﬁxed) polynomial. NP-complete problems and their
signiﬁcance will be discussed at the end of this section.
Is there a better way of testing whether a proposition A is a tautology
than computing its truth table (which requires computing at least 2n entries,
where n is the number of proposition symbols occurring in A)? One possibility

3.3 Semantics of Propositional Logic
45
is to work backwards, trying to ﬁnd a truth assignment which makes the
proposition false. In this way, one may detect failure much earlier. This is
the essence of Gentzen systems to be discussed shortly.
As we said at the beginning of Section 3.3, every proposition deﬁnes a
function taking truth values as arguments, and yielding truth values as results.
The truth function associated with a proposition is deﬁned as follows.
3.3.3 Truth Functions and Functionally Complete Sets
of Connectives
We now show that the logical connectives are not independent. For this, we
need to deﬁne what it means for a proposition to deﬁne a truth function.
Deﬁnition 3.3.5
Let A be a formula containing exactly n distinct propo-
sitional symbols. The function HA : BOOLn →BOOL is deﬁned such that,
for every (a1, ..., an) ∈BOOLn,
HA(a1, ..., an) = v(A),
with v any valuation such that v(Pi) = ai for every propositional symbol Pi
occurring in A.
For simplicity of notation we will often name the function HA as A. HA
is a truth function. In general, every function f : BOOLn →BOOL is called
an n-ary truth function.
EXAMPLE 3.3.4
The proposition
A = (P ∧¬Q) ∨(¬P ∧Q)
deﬁnes the truth function H⊕given by the following truth table:
P
Q ¬P
¬Q (P ∧¬Q) (¬P ∧Q) (P ∧¬Q) ∨(¬P ∧Q))
F F
T
T
F
F
F
F T
T
F
F
T
T
T F
F
T
T
F
T
T T
F
F
F
F
F
Note that the function H⊕takes the value T if and only if its arguments
have diﬀerent truth values. For this reason, it is called the exclusive OR
function.
It is natural to ask whether every truth function f can be realized by
some proposition A, in the sense that f = HA. This is indeed the case. We say
that the boolean connectives form a functionally complete set of connectives.

46
3/Propositional Logic
The signiﬁcance of this result is that for any truth function f of n arguments,
we do not enrich the collection of assertions that we can make by adding a
new symbol to the syntax, say F, and interpreting F as f. Indeed, since f
is already deﬁnable in terms of ∨, ∧, ¬, ⊃, ≡, every extended proposition
A containing F can be converted to a proposition A′ in which F does not
occur, such that for every valuation v, v(A) = v(A′).
Hence, there is no
loss of generality in restricting our attention to the connectives that we have
introduced. In fact, we shall prove that each of the sets {∨, ¬}, {∧, ¬}, {⊃, ¬}
and {⊃, ⊥} is functionally complete.
First, we prove the following two lemmas.
Lemma 3.3.3
Let A and B be any two propositions, let {P1, ..., Pn} be the
set of propositional symbols occurring in (A ≡B), and let HA and HB be the
functions associated with A and B, considered as functions of the arguments
in {P1, ..., Pn}. The proposition (A ≡B) is a tautology if and only if for all
valuations v, v(A) = v(B), if and only if HA = HB.
Proof : For any valuation v,
v((A ≡B)) = H≡(v(A), v(B)).
Consulting the truth table for H≡, we see that
v((A ≡B)) = T
if and only if
v(A) = v(B).
By lemma 3.3.1 and deﬁnition 3.3.5, this implies that HA = HB.
By constructing truth tables, the propositions listed in lemma 3.3.4 be-
low can be shown to be tautologies.
Lemma 3.3.4
The following properties hold:
|= (A ≡B) ≡((A ⊃B) ∧(B ⊃A));
(1)
|= (A ⊃B) ≡(¬A ∨B);
(2)
|= (A ∨B) ≡(¬A ⊃B);
(3)
|= (A ∨B) ≡¬(¬A ∧¬B);
(4)
|= (A ∧B) ≡¬(¬A ∨¬B);
(5)
|= ¬A ≡(A ⊃⊥);
(6)
|=⊥≡(A ∧¬A).
(7)
Proof : We prove (1), leaving the other cases as an exercise. By lemma
3.3.3, it is suﬃcient to verify that the truth tables for (A ≡B) and ((A ⊃
B) ∧(B ⊃A)) are identical.

3.3 Semantics of Propositional Logic
47
A
B
A ⊃B
B ⊃A
(A ≡B)
((A ⊃B) ∧(B ⊃A))
F
F
T
T
T
T
F
T
T
F
F
F
T
F
F
T
F
F
T
T
T
T
T
T
Since the columns for (A ≡B) and ((A ⊃B) ∧(B ⊃A)) are identical,
(1) is a tautology.
We now show that {∨, ∧, ¬} is a functionally complete set of connectives.
Theorem 3.3.1
For every n-ary truth function f, there is a proposition A
only using the connectives ∧, ∨and ¬ such that f = HA.
Proof : We proceed by induction on the arity n of f. For n = 1, there
are four truth functions whose truth tables are:
P
1
2
3
4
F
T
F
F
T
T
T
F
T
F
Clearly, the propositions P ∨¬P, P ∧¬P, P and ¬P do the job. Let f
be of arity n + 1 and assume the induction hypothesis for n. Let
f1(x1, ..., xn) = f(x1, ..., xn, T)
and
f2(x1, ..., xn) = f(x1, ..., xn, F).
Both f1 and f2 are n-ary. By the induction hypothesis, there are propositions
B and C such that
f1 = HB
and
f2 = HC.
But then, letting A be the formula
(Pn+1 ∧B) ∨(¬Pn+1 ∧C),
where Pn+1 occurs neither in B nor C, it is easy to see that f = HA.
Using lemma 3.3.4, it follows that {∨, ¬}, {∧, ¬}, {⊃, ¬} and {⊃, ⊥}
are functionally complete. Indeed, using induction on propositions, ∧can be
expressed in terms of ∨and ¬ by (5), ⊃can be expressed in terms of ¬ and ∨
by (2), ≡can be expressed in terms of ⊃and ∧by (1), and ⊥can be expressed
in terms of ∧and ¬ by (7). Hence, {∨, ¬} is functionally complete. Since ∨
can be expressed in terms of ∧and ¬ by (4), the set {∧, ¬} is functionally
complete, since {∨, ¬} is. Since ∨can be expressed in terms of ⊃and ¬ by
(3), the set {⊃, ¬} is functionally complete, since {∨, ¬} is. Finally, since ¬

48
3/Propositional Logic
can be expressed in terms of ⊃and ⊥by (6), the set {⊃, ⊥} is functionally
complete since {⊃, ¬} is.
In view of the above theorem, we may without loss of generality restrict
our attention to propositions expressed in terms of the connectives in some
functionally complete set of our choice. The choice of a suitable functionally
complete set of connectives is essentially a matter of convenience and taste.
The advantage of using a small set of connectives is that fewer cases have to
be considered in proving properties of propositions. The disadvantage is that
the meaning of a proposition may not be as clear for a proposition written in
terms of a smaller complete set, as it is for the same proposition expressed
in terms of the full set of connectives used in deﬁnition 3.2.1. Furthermore,
depending on the set of connectives chosen, the representations can have very
diﬀerent lengths. For example, using the set {⊃, ⊥}, the proposition (A ∧B)
takes the form
(A ⊃(B ⊃⊥)) ⊃⊥.
I doubt that many readers think that the second representation is more per-
spicuous than the ﬁrst!
In this book, we will adopt the following compromise between mathe-
matical conciseness and intuitive clarity. The set {∧, ∨, ¬, ⊃} will be used.
Then, (A ≡B) will be considered as an abbreviation for ((A ⊃B)∧(B ⊃A)),
and ⊥as an abbrevation for (P ∧¬P).
We close this section with some results showing that the set of propo-
sitions has a remarkable structure called a boolean algebra. (See Subsection
2.4.1 in the Appendix for the deﬁnition of an algebra.)
3.3.4 Logical Equivalence and Boolean Algebras
First, we show that lemma 3.3.3 implies that a certain relation on PROP is
an equivalence relation.
Deﬁnition 3.3.6
The relation ≃on PROP is deﬁned so that for any two
propositions A and B, A ≃B if and only if (A ≡B) is a tautology. We say
that A and B are logically equivalent, or for short, equivalent.
From lemma 3.3.3, A ≃B if and only if HA = HB. This implies that
the relation ≃is reﬂexive, symmetric, and transitive, and therefore it is an
equivalence relation. The following additional properties show that it is a
congruence in the sense of Subsection 2.4.6 (in the Appendix).
Lemma 3.3.5 For all propositions A,A′,B,B′, the following properties hold:
If A ≃A′ and B ≃B′,
then for ∗∈{∧, ∨, ⊃, ≡},
(A ∗B) ≃(A′ ∗B′)
and
¬A ≃¬A′.

3.3 Semantics of Propositional Logic
49
Proof : By deﬁnition 3.3.6,
(A ∗B) ≃(A′ ∗B′)
if and only if
|= (A ∗B) ≡(A′ ∗B′).
By lemma 3.3.3, it is suﬃcient to show that for all valuations v,
v(A ∗B) = v(A′ ∗B′).
Since
A ≃A′
and
B ≃B′
implies that
v(A) = v(A′)
and
v(B) = v(B′),
we have
v(A ∗B) = H∗(v(A), v(B)) = H∗(v(A′), v(B′)) = v(A′ ∗B′).
Similarly,
v(¬A) = H¬(v(A)) = H¬(v(A′)) = v(¬A′).
In the rest of this section, it is assumed that the constant symbol ⊤
is added to the alphabet of deﬁnition 3.2.1, yielding the set of propositions
PROP ′, and that ⊤is interpreted as T. The proof of the following properties
is left as an exercise.
Lemma 3.3.6 The following properties hold for all propositions in PROP ′.
Associativity rules:
((A ∨B) ∨C) ≃(A ∨(B ∨C))
((A ∧B) ∧C) ≃(A ∧(B ∧C))
Commutativity rules:
(A ∨B) ≃(B ∨A)
(A ∧B) ≃(B ∧A)
Distributivity rules:
(A ∨(B ∧C)) ≃((A ∨B) ∧(A ∨C))
(A ∧(B ∨C)) ≃((A ∧B) ∨(A ∧C))
De Morgan’s rules:
¬(A ∨B) ≃(¬A ∧¬B)
¬(A ∧B) ≃(¬A ∨¬B)
Idempotency rules:
(A ∨A) ≃A
(A ∧A) ≃A
Double negation rule:
¬¬A ≃A
Absorption rules:
(A ∨(A ∧B)) ≃A
(A ∧(A ∨B)) ≃A
Laws of zero and one:
(A∨⊥) ≃A
(A∧⊥) ≃⊥
(A ∨⊤) ≃⊤
(A ∧⊤) ≃A
(A ∨¬A) ≃⊤(A ∧¬A) ≃⊥

50
3/Propositional Logic
Let us denote the equivalence class of a proposition A modulo ≃as [A],
and the set of all such equivalence classes as BP ROP . We deﬁne the operations
+, ∗and ¬ on BP ROP as follows:
[A] + [B] = [A ∨B],
[A] ∗[B] = [A ∧B],
¬[A] = [¬A].
Also, let 0 = [⊥] and 1 = [⊤].
By lemma 3.3.5, the above functions (and constants) are independent of
the choice of representatives in the equivalence classes. But then, the proper-
ties of lemma 3.3.6 are identities valid on the set BP ROP of equivalence classes
modulo ≃. The structure BP ROP is an algebra in the sense of Subsection 2.4.1
(in the Appendix). Because it satisﬁes the identities of lemma 3.3.6, it is a
very rich structure called a boolean algebra. BP ROP is called the Lindenbaum
algebra of PROP. In this book, we will only make simple uses of the fact
that BP ROP is a boolean algebra (the properties in lemma 3.3.6, associativity,
commutativity, distributivity, and idempotence in particular) but the reader
should be aware that there are important and interesting consequences of this
fact. However, these considerations are beyond the scope of this text. We
refer the reader to Halmos, 1974, or Birkhoﬀ, 1973 for a comprehensive study
of these algebras.
∗3.3.5 NP-Complete Problems
It has been remarked earlier that both the satisﬁability problem (SAT) and
the tautology problem (TAUT) are computationally hard problems, in the
sense that known algorithms to solve them require an exponential number of
steps in the length of the input. Even though modern computers are capable
of performing very complex computations much faster than humans can, there
are problems whose computational complexity is such that it would take too
much time or memory space to solve them with a computer. Such problems are
called intractable. It should be noted that this does not mean that we do not
have algorithms to solve such problems. This means that all known algorithms
solving these problems in theory either require too much time or too much
memory space to solve them in practice, except perhaps in rather trivial cases.
An algorithm is considered intractable if either it requires an exponential
number of steps, or an exponential amount of space, in the length of the
input. This is because exponential functions grow very fast. For example,
210 = 1024, but 21000 is equal to 101000log102, which has over 300 digits!
A problem that can be solved in polynomial time and polynomial space is
considered to be tractable.
It is not known whether SAT or TAUT are tractable, and in fact, it is
conjectured that they are not. But SAT and TAUT play a special role for

3.3 Semantics of Propositional Logic
51
another reason.
There is a class of problems (NP) which contains
problems for which no polynomial-time algorithms are known, but for which
polynomial-time solutions exist, if we are allowed to make guesses, and if we
are not charged for checking wrong guesses, but only for successful guesses
leading to an answer. SAT is such a problem. Indeed, given a proposition A, if
one is allowed to guess valuations, it is not diﬃcult to design a polynomial-time
algorithm to check that a valuation v satisﬁes A. The satisﬁability problem
can be solved nondeterministically by guessing valuations and checking that
they satisfy the given proposition. Since we are not “charged” for checking
wrong guesses, such a procedure works in polynomial-time.
A more accurate way of describing such algorithms is to say that free
backtracking is allowed. If the algorithm reaches a point where several choices
are possible, any choice can be taken, but if the path chosen leads to a dead
end, the algorithm can jump back (backtrack) to the latest choice point,
with no cost of computation time (and space) consumed on a wrong path
involved. Technically speaking, such algorithms are called nondeterministic.
A nondeterministic algorithm can be simulated by a deterministic algorithm,
but the deterministic algorithm needs to keep track of the nondeterministic
choices explicitly (using a stack), and to use a backtracking technique to
handle unsuccessful computations.
Unfortunately, all known backtracking
techniques yield exponential-time algorithms.
In order to discuss complexity issues rigorously, it is necessary to deﬁne
a model of computation. Such a model is the Turing machine (invented by
the mathematician Turing, circa 1935). We will not present here the theory
of Turing Machines and complexity classes, but refer the interested reader to
Lewis and Papadimitriou, 1981, or Davis and Weyuker, 1983. We will instead
conclude with an informal discussion of the classes P and NP.
In dealing with algorithms for solving classes of problems, it is conve-
nient to assume that problems are encoded as sets of strings over a ﬁnite
alphabet Σ. Then, an algorithm for solving a problem A is an algorithm for
deciding whether for any string u ∈Σ∗, u is a member of A. For example,
the satisﬁability problem is encoded as the set of strings representing satisﬁ-
able propositions, and the tautology problem as the set of strings representing
tautologies.
A Turing machine is an abstract computing device used to accept sets
of strings. Roughly speaking, a Turing machine M consists of a ﬁnite set of
states and of a ﬁnite set of instructions. The set of states is partitioned into
two subsets of accepting and rejecting states.
To explain how a Turing machine operates, we deﬁne the notion of an
instantaneous description (ID) and of a computation. An instantaneous de-
scription is a sort of snapshot of the conﬁguration of the machine during a
computation that, among other things, contains a state component. A Turing
machine operates in discrete steps. Every time an instruction is executed, the
ID describing the current conﬁguration is updated. The intuitive idea is that

52
3/Propositional Logic
executing an instruction I when the machine is in a conﬁguration described by
an instantaneous description C1 yields a new conﬁguration described by C2.
A computation is a ﬁnite or inﬁnite sequence of instantaneous descriptions
C0, ..., Cn (or C0, ..., Cn, Cn+1, ..., if it is inﬁnite), where C0 is an initial in-
stantaneous description containing the input, and each Ci+1 is obtained from
Ci by application of some instruction of M. A ﬁnite computation is called
a halting computation, and the last instantaneous description Cn is called a
ﬁnal instantaneous description.
A Turing machine is deterministic, if for every instantaneous description
C1, at most one instantaneous description C2 follows from C1 by execution of
some instruction of M. It is nondeterministic if for any ID C1, there may be
several successors C2 obtained by executing (diﬀerent) instructions of M.
Given a halting computation C0, ..., Cn, if the ﬁnal ID Cn contains an
accepting state, we say that the computation is an accepting computation,
otherwise it is a rejecting computation.
A set A (over Σ) is accepted deterministically in polynomial time if there
is a deterministic Turing machine M and a polynomial p such that, for every
input u,
(i) u ∈A iﬀthe computation C0, ..., Cn on input u is an accepting
computation such that n ≤p(|u|) and,
(ii) u /∈A iﬀthe computation C0, ..., Cn on input u is a rejecting com-
putation such that n ≤p(|u|).
A set A (over Σ) is accepted nondeterministically in polynomial time if
there is a nondeterministic Turing machine M and a polynomial p, such that,
for every input u, there is some accepting computation C0, ..., Cn such that
n ≤p(|u|).
It should be noted that in the nondeterministic case, a string u is rejected
by a Turing machine M (that is, u /∈A) iﬀevery computation of M is either a
rejecting computation C0, ..., Cn such that n ≤p(|u|), or a computation that
takes more than p(|u|) steps on input u.
The class of sets accepted deterministically in polynomial time is denoted
by P, and the class of sets accepted nondeterministically in polynomial time
is denoted by NP. It is obvious from the deﬁnitions that P is a subset of
NP. However, whether P = NP is unknown, and in fact, is a famous open
problem.
The importance of the class P resides in the widely accepted (although
somewhat controversial) opinion that P consists of the problems that can
be realistically solved by computers. The importance of NP lies in the fact
that many problems for which eﬃcient algorithms would be highly desirable,
but are yet unknown, belong to NP. The traveling salesman problem and
the integer programming problem are two such problems among many others.

3.3 Semantics of Propositional Logic
53
For an extensive list of problems in NP, the reader should consult Garey and
Johnson, 1979.
The importance of the satisﬁability problem SAT is that it is NP-
complete. This implies the remarkable fact that if SAT is in P, then P = NP.
In other words, the existence of a polynomial-time algorithm for SAT implies
that all problems in NP have polynomial-time algorithms, which includes
many interesting problems apparently untractable at the moment.
In order to explain the notion of NP-completeness, we need the concept
of polynomial-time reducibility. Deterministic Turing machines can also be
used to compute functions. Given a function f : Σ∗→Σ∗, a deterministic
Turing machine M computes f if for every input u, there is a halting com-
putation C0, ..., Cn such that C0 contains u as input and Cn contains f(u) as
output. The machine M computes f in polynomial time iﬀthere is a polyno-
mial p such that, for every input u, n ≤p(|u|), where n is the number of steps
in the computation on input u. Then, we say that a set A is polynomially re-
ducible to a set B if there is a function f : Σ∗→Σ∗computable in polynomial
time such that, for every input u,
u ∈A
if and only if
f(u) ∈B.
A set B is NP-hard if every set A in NP is reducible to B.
A set B is
NP-complete if it is in NP, and it is NP-hard.
The signiﬁcance of NP-complete problems lies in the fact that if one
ﬁnds a polynomial time algorithm for any NP-complete problem B, then
P = NP. Indeed, given any problem A ∈NP, assuming that the determin-
istic Turing machine M solves B in polynomial time, we could construct a
deterministic Turing machine M ′ solving A as follows. Let Mf be the deter-
ministic Turing machine computing the reduction function f. Then, to decide
whether any arbitrary input u is in A, run Mf on input u, producing f(u),
and then run M on input f(u). Since u ∈A if and only if f(u) ∈B, the above
procedure solves A. Furthermore, it is easily shown that a deterministic Tur-
ing machine M ′ simulating the composition of Mf and M can be constructed,
and that it runs in polynomial time (because the functional composition of
polynomials is a polynomial).
The importance of SAT lies in the fact that it was shown by S. A. Cook
(Cook, 1971) that SAT is an NP-complete problem. In contrast, whether
TAUT is in NP is an open problem. But TAUT is interesting for two other
reasons. First, it can be shown that if TAUT is in P, then P = NP. This
is unlikely since we do not even know whether TAUT is in NP. The second
reason is related to the closure of NP under complementation. NP is said to
be closed under complementation iﬀfor every set A in NP, its complement
Σ∗−A is also in NP.
The class P is closed under complementation, but this is an open prob-
lem for the class NP. Given a deterministic Turing machine M, in order to

54
3/Propositional Logic
accept the complement of the set A accepted by M, one simply has to create
the machine M obtained by swapping the accepting and the rejecting states of
M. Since for every input u, the computation C0, ..., Cn of M on input u halts
in n ≤p(|u|) steps, the modiﬁed machine M accepts Σ∗−A is polynomial
time. However, if M is nondeterministic, M may reject some input u because
all computations on input u exceed the polynomial time bound p(|u|). Thus,
for this input u, there is no computation of the modiﬁed machine M which
accepts u within p(|u|) steps. The trouble is not that M cannot tell that u is
rejected by M, but that M cannot report this fact in fewer than p(|u|) steps.
This shows that in the nondeterministic case, a diﬀerent construction is re-
quired. Until now, no such construction has been discovered, and is rather
unlikely that it will. Indeed, it can be shown that TAUT is in NP if and only
if NP is closed under complementation. Furthermore, since P is closed under
complementation if NP is not closed under complementation, then NP ̸= P.
Hence, one approach for showing that NP ̸= P would be to show that
TAUT is not in NP. This explains why a lot of eﬀort has been spent on the
complexity of the tautology problem.
To summarize, the satisﬁability problem SAT and the tautology problem
TAUT are important because of the following facts:
SAT ∈P
if and only if
P = NP;
TAUT ∈NP
if and only if
NP is closed under complementation;
If TAUT ∈P, then P = NP;
If TAUT /∈NP, then NP ̸= P.
Since the two questions P = NP and the closure of NP under comple-
mentation appear to be very hard to solve, and it is usually believed that their
answer is negative, this gives some insight to the diﬃculty of ﬁnding eﬃcient
algorithms for SAT and TAUT. Also, the tautology problem appears to be
harder than the satisﬁability problem. For more details on these questions,
we refer the reader to the article by S. A. Cook and R. A. Reckhow (Cook
and Reckhow, 1971).
PROBLEMS
3.3.1.
In this problem, it is assumed that the language of propositional logic
is extended by adding the constant symbol ⊤, which is interpreted as
T. Prove that the following propositions are tautologies by construct-
ing truth tables.
Associativity rules:
((A ∨B) ∨C) ≡(A ∨(B ∨C))
((A ∧B) ∧C) ≡(A ∧(B ∧C))

PROBLEMS
55
Commutativity rules:
(A ∨B) ≡(B ∨A)
(A ∧B) ≡(B ∧A)
Distributivity rules:
(A ∨(B ∧C)) ≡((A ∨B) ∧(A ∨C))
(A ∧(B ∨C)) ≡((A ∧B) ∨(A ∧C))
De Morgan’s rules:
¬(A ∨B) ≡(¬A ∧¬B)
¬(A ∧B) ≡(¬A ∨¬B)
Idempotency rules:
(A ∨A) ≡A
(A ∧A) ≡A
Double negation rule:
¬¬A ≡A
Absorption rules:
(A ∨(A ∧B)) ≡A
(A ∧(A ∨B)) ≡A
Laws of zero and one:
(A∨⊥) ≡A
(A∧⊥) ≡⊥
(A ∨⊤) ≡⊤
(A ∧⊤) ≡A
(A ∨¬A) ≡⊤(A ∧¬A) ≡⊥
3.3.2.
Show that the following propositions are tautologies.
A ⊃(B ⊃A)
(A ⊃B) ⊃((A ⊃(B ⊃C)) ⊃(A ⊃C))
A ⊃(B ⊃(A ∧B))
A ⊃(A ∨B)
B ⊃(A ∨B)
(A ⊃B) ⊃((A ⊃¬B) ⊃¬A)
(A ∧B) ⊃A
(A ∧B) ⊃B
(A ⊃C) ⊃((B ⊃C) ⊃((A ∨B) ⊃C))
¬¬A ⊃A
3.3.3.
Show that the following propositions are tautologies.
(A ⊃B) ⊃((B ⊃A) ⊃(A ≡B))
(A ≡B) ⊃(A ⊃B)
(A ≡B) ⊃(B ⊃A)
3.3.4.
Show that the following propositions are not tautologies. Are they
satisﬁable? If so, give a satisfying valuation.
(A ⊃C) ⊃((B ⊃D) ⊃((A ∨B) ⊃C))
(A ⊃B) ⊃((B ⊃¬C) ⊃¬A)

56
3/Propositional Logic
3.3.5.
Prove that the propositions of lemma 3.3.4 are tautologies.
∗3.3.6.
Given a function f of m arguments and m functions g1, ..., gm each
of n arguments, the composition of f and g1, ..., gm is the function h
of n arguments such that for all x1, ..., xn,
h(x1, ..., xn) = f(g1(x1, ..., xn), ..., gm(x1, ..., xn)).
For every integer n ≥1, we let P n
i , (1 ≤i ≤n), denote the projection
function such that, for all x1, .., xn,
P n
i (x1, ..., xn) = xi.
Then, given any k truth functions H1, ..., Hk, let TFn be the inductive
closure of the set of functions {H1, ..., Hk, P n
1 , ..., P n
n } under compo-
sition. We say that a truth function H of n arguments is deﬁnable
from H1, ..., Hk if H belongs to TFn.
Let Hd,n be the n-ary truth function such that
Hd,n(x1, ...., xn) = F
if and only if
x1 = ... = xn = F,
and Hc,n the n-ary truth function such that
Hc,n(x1, ..., xn) = T
if and only if
x1 = ... = xn = T.
(i) Prove that every n-ary truth function is deﬁnable in terms of H¬
and some of the functions Hd,n, Hc,n.
(ii) Prove that H¬ is not deﬁnable in terms of H∨, H∧, H⊃, and H≡.
∗3.3.7.
Let Hnor be the binary truth function such that
Hnor(x, y) = T
if and only if
x = y = F.
Show that Hnor = HA, where A is the proposition (¬P ∧¬Q). Show
that {Hnor} is functionally complete.
∗3.3.8.
Let Hnand be the binary truth function such that Hnand(x, y) = F
if and only if x = y = T. Show that Hnand = HB, where B is the
proposition (¬P ∨¬Q). Show that {Hnand} is functionally complete.
∗3.3.9.
An n-ary truth function H is singulary if there is a unary truth
function H′ and some i, 1 ≤i ≤n, such that for all x1, ..., xn,
H(x1, ..., xn) = H′(xi).
(i) Prove that if H is singulary, then every n-ary function deﬁnable
in terms of H is also singulary. (See problem 3.3.6.)

PROBLEMS
57
(ii) Prove that if H is a binary truth function and {H} is functionally
complete, then either H = Hnor or H = Hnand.
Hint: Show that H(T, T) = F and H(F, F) = T, that only four
binary truth functions have that property, and use (i).
3.3.10. A substitution is a function s : PS →PROP. Since PROP is freely
generated by PS and ⊥, every substitution s extends to a unique
function s : PROP →PROP deﬁned by recursion. Let A be any
proposition containing the propositional symbols {P1, ..., Pn}, and s1
and s2 be any two substitutions such that for every Pi ∈{P1, ..., Pn},
the propositions s1(Pi) and s2(Pi) are equivalent (that is s1(Pi) ≡
s2(Pi) is a tautology).
Prove that the propositions s1(A) and s2(A) are equivalent.
3.3.11. Show that for every set Γ of propositions,
(i) Γ, A |= B
if and only if
Γ |= (A ⊃B);
(ii) If
Γ, A |= B
and
Γ, A |= ¬B,
then
Γ |= ¬A;
(iii) If
Γ, A |= C
and
Γ, B |= C,
then
Γ, (A ∨B) |= C.
3.3.12. Assume that we consider propositions expressed only in terms of the
set of connectives {∨, ∧, ¬}. The dual of a proposition A, denoted by
A∗, is deﬁned recursively as follows:
P ∗= P,
for every propositional symbol P;
(A ∨B)∗= (A∗∧B∗);
(A ∧B)∗= (A∗∨B∗);
(¬A)∗= ¬A∗.
(a) Prove that, for any two propositions A and B,
|= A ≡B
if and only if
|= A∗≡B∗.
(b) If we change the deﬁnition of A∗so that for every propositional
letter P, P ∗= ¬P, prove that A∗and ¬A are logically equivalent
(that is, (A∗≡¬A) is a tautology).
3.3.13. A literal is either a propositional symbol P, or the negation ¬P of a
propositional symbol. A proposition A is in disjunctive normal form
(DNF) if it is of the form C1 ∨...∨Cn, where each Ci is a conjunction
of literals.
(i) Show that the satisﬁability problem for propositions in DNF can
be decided in linear time. What does this say about the complexity

58
3/Propositional Logic
of any algorithm for converting a proposition to disjunctive normal
form?
(ii) Using the proof of theorem 3.3.1 and some of the identities of
lemma 3.3.6, prove that every proposition A containing n proposi-
tional symbols is equivalent to a proposition A′ in disjunctive normal
form, such that each disjunct Ci contains exactly n literals.
∗3.3.14. Let H⊕be the truth function deﬁned by the proposition
(P ∧¬Q) ∨(¬P ∧Q).
(i) Prove that ⊕(exclusive OR) is commutative and associative.
(ii) In this question, assume that the constant for false is denoted
by 0 and that the constant for true is denoted by 1. Prove that the
following are tautologies.
A ∨B ≡A ∧B ⊕A ⊕B
¬A ≡A ⊕1
A ⊕0 ≡A
A ⊕A ≡0
A ∧1 ≡A
A ∧A ≡A
A ∧(B ⊕C) ≡A ∧B ⊕A ∧C
A ∧0 ≡0
(iii) Prove that {⊕, ∧, 1} is functionally complete.
∗3.3.15. Using problems 3.3.13 and 3.3.14, prove that every proposition A is
equivalent to a proposition A′ which is either of the form 0, 1, or
C1 ⊕... ⊕Cn, where each Ci is either 1 or a conjunction of positive
literals. Furthermore, show that A′ can be chosen so that the Ci are
distinct, and that the positive literals in each Ci are all distinct (such
a proposition A′ is called a reduced exclusive-OR normal form).
∗3.3.16. (i) Prove that if A′ and A′′ are reduced exclusive-OR normal forms of
a same proposition A, then they are equal up to commutativity, that
is:
Either
A′ = A′′ = 0, or A′ = A′′ = 1, or
A′ = C′
1 ∧... ∧C′
n,
A′′ = C′′
1 ∧... ∧C′′
n,
where each C′
i is a permutation of some C′′
j (and conversely).
Hint: There are 22n truth functions of n arguments.

PROBLEMS
59
(ii) Prove that a proposition is a tautology if and only if its re-
duced exclusive-OR normal form is 1. What does this say about the
complexity of any algorithm for converting a proposition to reduced
exclusive-OR normal form?
∗3.3.17. A set Γ of propositions is independent if, for every A ∈Γ,
Γ −{A} ̸|= A.
(a) Prove that every ﬁnite set Γ has a ﬁnite independent subset ∆
such that, for every A ∈Γ, ∆|= A.
(b) Let Γ be ordered as the sequence < A1, A2, .... >. Find a sequence
Γ′ =< B1, B2, ... > equivalent to Γ (that is, for every i ≥1, Γ |= Bi
and Γ′ |= Ai), such that, for every i ≥1, |= (Bi+1 ⊃Bi), but
̸|= (Bi ⊃Bi+1). Note that Γ′ may be ﬁnite.
(c) Consider a countable sequence Γ′ as in (b). Deﬁne C1 = B1, and
for every i ≥1, Cn+1 = (Bn ⊃Bn+1). Prove that ∆=< C1, C2, ... >
is equivalent to Γ′ and independent.
(d) Prove that every countable set Γ is equivalent to an independent
set.
(e) Show that ∆need not be a subset of Γ.
Hint: Consider
{P0, P0 ∧P1, P0 ∧P1 ∧P1, ...}.
∗3.3.18. See problem 3.3.13 for the deﬁnition of a literal. A proposition A is
a basic Horn formula iﬀit is a disjunction of literals, with at most
one positive literal (literal of the form P). A proposition is a Horn
formula iﬀit is a conjunction of basic Horn formulae.
(a) Show that every Horn formula A is equivalent to a conjunction of
distinct formulae of the form,
Pi,
or
¬P1 ∨.... ∨¬Pn, (n ≥1),
or
¬P1 ∨.... ∨¬Pn ∨Pn+1, (n ≥1),
where all the Pi are distinct. We say that A is reduced.
(b) Let A be a reduced Horn formula A = C1 ∧C2 ∧... ∧Cn, where
the Ci are distinct and each Ci is reduced as in (a).
Since ∨is
commutative and associative (see problem 3.3.1), we can view each
conjunct Ci as a set.

60
3/Propositional Logic
(i) Show that if no conjunct Ci is a positive literal, or every conjunct
containing a negative literal also contains a positive literal, then A is
satisﬁable.
(ii) Assume that A contains some conjunct Ci having a single posi-
tive literal P, and some conjunct Cj distinct from Ci, such that Cj
contains ¬P. Let Di,j be obtained by deleting ¬P from Cj. Let A′
be the conjunction obtained from A by replacing Cj by the conjunct
Di,j, provided that Di,j is not empty.
Show that A is satisﬁable if and only if A′ is satisﬁable and Di,j is
not empty.
(iii) Using the above, prove that the satisﬁability of a Horn formula
A can be decided in time polynomial in the length of A.
Note: Linear-time algorithms are given in Dowling and Gallier, 1984.
3.4 Proof Theory of Propositional Logic: The Gentzen
System G′
In this section, we present a proof system for propositional logic and prove
some of its main properties: soundness and completeness.
3.4.1 Basic Idea: Searching for a Counter Example
As we have suggested in Section 3.3, another perhaps more eﬀective way of
testing whether a proposition A is a tautology is to search for a valuation
that falsiﬁes A. In this section, we elaborate on this idea. As we progress
according to this plan, we will be dealing with a tree whose nodes are la-
beled with pairs of ﬁnite lists of propositions. In our attempt to falsify A, the
tree is constructed in such a way that we are trying to ﬁnd a valuation that
makes every proposition occurring in the ﬁrst component of a pair labeling
a node true, and all propositions occurring in the second component of that
pair false. Hence, we are naturally led to deal with pairs of ﬁnite sequences
of propositions called sequents. The idea of using sequents originates with
Gentzen, although Gentzen’s motivations were quite diﬀerent. A proof sys-
tem using sequents is very natural because the rules reﬂect very clearly the
semantics of the connectives. The idea of searching for a valuation falsifying
the given proposition is simple, and the tree-building algorithm implementing
this search is also simple. Let us ﬁrst illustrate the falsiﬁcation procedure by
means of an example.
EXAMPLE 3.4.1
Let
A = (P ⊃Q) ⊃(¬Q ⊃¬P).

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
61
Initially, we start with a one-node tree labeled with the pair
(<>, < (P ⊃Q) ⊃(¬Q ⊃¬P) >)
whose ﬁrst component is the empty sequence and whose second compo-
nent is the sequence containing the proposition A that we are attempt-
ing to falsify. In order to make A false, we must make P ⊃Q true and
¬Q ⊃¬P false. Hence, we build the following tree:
(< P ⊃Q >, < ¬Q ⊃¬P >)
(<>, < (P ⊃Q) ⊃(¬Q ⊃¬P) >)
Now, in order to make P ⊃Q true, we must either make P false or Q
true. The tree must therefore split as shown:
(<>, < P, ¬Q ⊃¬P >)
(< Q >, < ¬Q ⊃¬P >)
(< P ⊃Q >, < ¬Q ⊃¬P >)
(<>, < (P ⊃Q) ⊃(¬Q ⊃¬P) >)
We continue the same procedure with each leaf. Let us consider the
leftmost leaf ﬁrst. In order to make ¬Q ⊃¬P false, we must make ¬Q
true and ¬P false. We obtain the tree:
(< ¬Q >, < P, ¬P >)
(<>, < P, ¬Q ⊃¬P >)
(< Q >, < ¬Q ⊃¬P >)
(< P ⊃Q >, < ¬Q ⊃¬P >)
(<>, < (P ⊃Q) ⊃(¬Q ⊃¬P) >)
But now, in order to falsify the leftmost leaf, we must make both P and
¬P false and ¬Q true. This is impossible. We say that this leaf of the
tree is closed. We still have to continue the procedure with the rightmost
leaf, since there may be a way of obtaining a falsifying valuation this
way. To make ¬Q ⊃¬P false, we must make ¬Q true and ¬P false,
obtaining the tree:
(< ¬Q >, < P, ¬P >)
(<>, < P, ¬Q ⊃¬P >)
(< Q, ¬Q >, < ¬P >)
(< Q >, < ¬Q ⊃¬P >)
(< P ⊃Q >, < ¬Q ⊃¬P >)
(<>, < (P ⊃Q) ⊃(¬Q ⊃¬P) >)

62
3/Propositional Logic
This time, we must try to make ¬P false and both Q and ¬Q false,
which is impossible. Hence, this branch of the tree is also closed, and
our attempt to falsify A has failed. However, this failure to falsify A is
really a success, since, as we shall prove shortly, this demonstrates that
A is valid!
Trees as above are called deduction trees. In order to describe precisely
the algorithm we have used in our attempt to falsify the proposition A, we
need to state clearly the rules that we have used in constructing the tree.
3.4.2 Sequents and the Gentzen System G′
First, we deﬁne the notion of a sequent.
Deﬁnition 3.4.1
A sequent is a pair (Γ, ∆) of ﬁnite (possibly empty) se-
quences Γ =< A1, ..., Am >, ∆=< B1, ..., Bn > of propositions.
Instead of using the notation (Γ, ∆), a sequent is usually denoted as
Γ →∆. For simplicity, a sequence < A1, ..., Am > is denoted as A1, ..., Am.
If Γ is the empty sequence, the corresponding sequent is denoted as →∆; if
∆is empty, the sequent is denoted as Γ →. and if both Γ and ∆are empty,
we have the special sequent →(the inconsistent sequent). Γ is called the
antecedent and ∆the succedent.
The intuitive meaning of a sequent is that a valuation v makes a sequent
A1, ..., Am →B1, ..., Bn true iﬀ
v |= (A1 ∧... ∧Am) ⊃(B1 ∨... ∨Bn).
Equivalently, v makes the sequent false if v makes A1, ..., Am all true and
B1, ..., Bn all false.
It should be noted that the semantics of sequents suggests that instead
of using sequences, we could have used sets. We could indeed deﬁne sequents
as pairs (Γ, ∆) of ﬁnite sets of propositions, and all the results in this section
would hold. The results of Section 3.5 would also hold, but in order to present
the generalization of the tree construction procedure, we would have to order
the sets present in the sequents anyway. Rather than switching back and forth
between sets and sequences, we think that it is preferable to stick to a single
formalism. Using sets instead of sequences can be viewed as an optimization.
The rules operating on sequents fall naturally into two categories: those
operating on a proposition occurring in the antecedent, and those on a propo-
sition occurring in the succedent. Both kinds of rules break the proposition
on which the rule operates into subpropositions that may also be moved from
the antecedent to the succedent, or vice versa. Also, the application of a rule
may cause a sequent to be split into two sequents. This causes branching in
the trees. Before stating the rules, let us mention that it is traditional in logic
to represent trees with their root at the bottom instead of the root at the

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
63
top as it is customary in computer science. The main reason is that a tree
obtained in failing to falsify a given proposition can be viewed as a formal
proof of the proposition. The proposition at the root of the tree is the logical
conclusion of a set of inferences, and it is more natural to draw a proof tree in
such a way that each premise in a rule occurs above its conclusion. However,
this may be a matter of taste (and perhaps, aesthetics).
In the rest of this section, it will be assumed that the set of connectives
used is {∧, ∨, ⊃, ¬}, and that (A ≡B) is an abbreviation for (A ⊃B) ∧(B ⊃
A), and ⊥an abbreviation for (P ∧¬P).
Deﬁnition 3.4.2 The Gentzen system G′. The symbols Γ, ∆, Λ will be used
to denote arbitrary sequences of propositions and A, B to denote propositions.
The inference rules of the sequent calculus G′ are the following:
Γ, A, B, ∆→Λ
Γ, A ∧B, ∆→Λ (∧: left)
Γ →∆, A, Λ
Γ →∆, B, Λ
Γ →∆, A ∧B, Λ
(∧: right)
Γ, A, ∆→Λ
Γ, B, ∆→Λ
Γ, A ∨B, ∆→Λ
(∨: left)
Γ →∆, A, B, Λ
Γ →∆, A ∨B, Λ (∨: right)
Γ, ∆→A, Λ
B, Γ, ∆→Λ
Γ, A ⊃B, ∆→Λ
(⊃: left)
A, Γ →B, ∆, Λ
Γ →∆, A ⊃B, Λ (⊃: right)
Γ, ∆→A, Λ
Γ, ¬A, ∆→Λ (¬ : left)
A, Γ →∆, Λ
Γ →∆, ¬A, Λ (¬ : right)
The name of every rule is stated immediately to its right. Every rule
consists of one or two upper sequents called premises and of a lower sequent
called the conclusion. The above rules are called inference rules. For ev-
ery rule, the proposition to which the rule is applied is called the principal
formula, the propositions introduced in the premises are called the side for-
mulae, and the other propositions that are copied unchanged are called the
extra formulae.
Note that every inference rule can be represented as a tree with two
nodes if the rule has a single premise, or three nodes if the rule has two
premises. In both cases, the root of the tree is labeled with the conclusion of
the rule and the leaves are labeled with the premises. If the rule has a single
premise, it is a tree of the form
(1)
S1
(e)
S2

64
3/Propositional Logic
where the premise labels the node with tree address 1, and the conclusion
labels the node with tree address e. If it has two premises, it is a tree of the
form
S3
S1
S2
(1)
(2)
(e)
where the ﬁrst premise labels the node with tree address 1, the second premise
labels the node with tree address 2, and the conclusion labels the node with
tree address e.
EXAMPLE 3.4.2
Consider the following instance of the ⊃:left rule:
A, B →P, D
Q, A, B →D
A, (P ⊃Q), B →D
In the above inference, (P ⊃Q) is the principal formula, P and Q are
side formulae, and A, B, D are extra formulae.
A careful reader might have observed that the rules (⊃:left), (⊃:right),
(¬:left), and (¬:right) have been designed in a special way. Notice that the
side proposition added to the antecedent of an upper sequent is added at
the front, and similarly for the side proposition added to the succedent of an
upper sequent. We have done so to facilitate the generalization of the search
procedure presented below to inﬁnite sequents.
We will now prove that the above rules achieve the falsiﬁcation procedure
sketched in example 3.4.1.
3.4.3 Falsiﬁable and Valid Sequents
First, we extend the concepts of falsiﬁability and validity to sequents.
Deﬁnition 3.4.3
A sequent A1, ..., Am →B1, ..., Bn is falsiﬁable iﬀthere
exists a valuation v such that
v |= (A1 ∧... ∧Am) ∧(¬B1 ∧... ∧¬Bn).
A sequent as above is valid iﬀfor every valuation v,
v |= (A1 ∧... ∧Am) ⊃(B1 ∨... ∨Bn).
This is also denoted by
|= A1, ..., Am →B1, ..., Bn.

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
65
If m = 0, the sequent
→B1, ..., Bn is falsiﬁable iﬀthe proposition
(¬B1 ∧... ∧¬Bn) is satisﬁable, valid iﬀthe proposition (B1 ∨... ∨Bn) is
valid. If n = 0, the sequent A1, ..., Am →
is falsiﬁable iﬀthe proposition
(A1 ∧... ∧Am) is satisﬁable, valid iﬀthe proposition (A1 ∧... ∧Am) is not
satisﬁable. Note that a sequent Γ →∆is valid if and only if it is not falsiﬁable.
Lemma 3.4.1
For each of the rules given in deﬁnition 3.4.2, a valuation v
falsiﬁes the sequent occurring as the conclusion of the rule if and only if v
falsiﬁes at least one of the sequents occurring as premises. Equivalently, v
makes the conclusion of a rule true if and only if v makes all premises of that
rule true.
Proof : The proof consists in checking the truth tables of the logical
connectives. We treat one case, leaving the others as an exercise. Consider
the (⊃:left) rule:
Γ, ∆→A, Λ
B, Γ, ∆→Λ
Γ, (A ⊃B), ∆→Λ
For every valuation v, v falsiﬁes the conclusion if and only if v satisﬁes
all propositions in Γ and ∆, and satisﬁes (A ⊃B), and falsiﬁes all propositions
in Λ. From the truth table of (A ⊃B), v satisﬁes (A ⊃B) if either v falsiﬁes
A, or v satisﬁes B. Hence, v falsiﬁes the conclusion if and only if, either
(1) v satisﬁes Γ and ∆, and falsiﬁes A and Λ, or
(2) v satisﬁes B, Γ and ∆, and falsiﬁes Λ.
3.4.4 Axioms, Deduction Trees, Proof Trees, Counter
Example Trees
The central concept in any proof system is the notion of proof . First, we
deﬁne the axioms of the system G′.
Deﬁnition 3.4.4
An axiom is any sequent Γ →∆such that Γ and ∆
contain some common proposition.
Lemma 3.4.2
No axiom is falsiﬁable. Equivalently, every axiom is valid.
Proof : The lemma follows from the fact that in order to falsify an axiom,
a valuation would have to make some proposition true on the left hand side,
and that same proposition false on the right hand side, which is impossible.
Proof trees are given by the following inductive deﬁnition.

66
3/Propositional Logic
Deﬁnition 3.4.5 The set of proof trees is the least set of trees containing all
one-node trees labeled with an axiom, and closed under the rules of deﬁnition
3.4.2 in the following sense:
(1) For any proof tree T1 whose root is labeled with a sequent Γ →∆,
for any instance of a one-premise inference rule with premise Γ →∆and
conclusion Λ →Θ, the tree T whose root is labeled with Λ →Θ and whose
subtree T/1 is equal to T1 is a proof tree.
(2) For any two proof trees T1 and T2 whose roots are labeled with
sequents Γ →∆and Γ′ →∆′ respectively, for every instance of a two-premise
inference rule with premises Γ →∆and Γ′ →∆′ and conclusion Λ →Θ, the
tree T whose root is labeled with Λ →Θ and whose subtrees T/1 and T/2
are equal to T1 and T2 respectively is a proof tree.
The set of deduction trees is deﬁned inductively as the least set of trees
containing all one-node trees (not necessarily labeled with an axiom), and
closed under (1) and (2) as above.
A deduction tree such that some leaf is labeled with a sequent Γ →∆
where Γ, ∆consist of propositional letters and are disjoint is called a counter-
example tree. The sequent labeling the root of a proof tree (deduction tree) is
called the conclusion of the proof tree (deduction tree). A sequent is provable
iﬀthere exists a proof tree of which it is the conclusion. If a sequent Γ →∆
is provable, this is denoted by
⊢Γ →∆.
EXAMPLE 3.4.3
The deduction tree below is a proof tree.
P, ¬Q →P
¬Q →¬P, P
→P, (¬Q ⊃¬P)
Q →Q, ¬P
¬Q, Q →¬P
Q →(¬Q ⊃¬P)
(P ⊃Q) →(¬Q ⊃¬P)
→(P ⊃Q) ⊃(¬Q ⊃¬P)
The above tree is a proof tree obtained from the proof tree
P, ¬Q →P
¬Q →¬P, P
→P, (¬Q ⊃¬P)
Q →Q, ¬P
¬Q, Q →¬P
Q →(¬Q ⊃¬P)
(P ⊃Q) →(¬Q ⊃¬P)

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
67
and the rule
(P ⊃Q) →(¬Q ⊃¬P)
→(P ⊃Q) ⊃(¬Q ⊃¬P)
In contrast, the deduction tree below is a counter-example tree.
P →Q
→(P ⊃Q)
Q →P
Q, ¬P →
¬P →¬Q
→(¬P ⊃¬Q)
→(P ⊃Q) ∧(¬P ⊃¬Q)
The above tree is obtained from the two counter-example trees
P →Q
→(P ⊃Q)
Q →P
Q, ¬P →
¬P →¬Q
→(¬P ⊃¬Q)
and the rule
→(P ⊃Q)
→(¬P ⊃¬Q)
→(P ⊃Q) ∧(¬P ⊃¬Q)
It is easily shown that a deduction tree T is a proof tree if and only if
every leaf sequent of T is an axiom.
Since proof trees (and deduction trees) are deﬁned inductively, the in-
duction principle applies. As an application, we now show that every provable
sequent is valid.
3.4.5 Soundness of the Gentzen System G′
Lemma 3.4.3 Soundness of the system G′. If a sequent Γ →∆is provable,
then it is valid.
Proof : We use the induction principle applied to proof trees. By lemma
3.4.2, every one-node proof tree (axiom) is valid. There are two cases in the
induction step.
Case 1: The root of the proof tree T has a single descendant. In this
case, T is obtained from some proof tree T1 and some instance of a rule
S1
S2

68
3/Propositional Logic
By the induction hypothesis, S1 is valid. Since by Lemma 3.4.1, S1 is valid if
and only if S2 is valid, Lemma 3.4.3 holds.
Case 2: The root of the proof tree T has two descendants. In this case,
T is obtained from two proof trees T1 and T2 and some instance of a rule
S1
S2
S3
By the induction hypothesis, both S1 and S2 are valid. Since by lemma 3.4.1,
S3 is valid if and only if both S1 and S2 are, lemma 3.4.3 holds.
Next, we shall prove the fundamental theorem for the propositional se-
quent calculus G′. Roughly speaking, the fundamental theorem states that
there exists a procedure for constructing a candidate counter-example tree,
and that this procedure always terminates (is an algorithm). If the original
sequent is valid, the algorithm terminates with a tree which is in fact a proof
tree. Otherwise, the counter-example tree yields a falsifying valuation (in fact,
all falsifying valuations). The fundamental theorem implies immediately the
completeness of the sequent calculus G′.
3.4.6 The Search Procedure
The algorithm searching for a candidate counter-example tree builds this tree
in a systematic fashion. We describe an algorithm that builds the tree in
a breadth-ﬁrst fashion. Note that other strategies for building such a tree
could be used, (depth-ﬁrst, in particular). A breadth-ﬁrst expansion strat-
egy was chosen because it is the strategy that works when we generalize the
search procedure to inﬁnite sequents. We will name this algorithm the search
procedure.
Let us call a leaf of a tree ﬁnished iﬀthe sequent labeling it is either an
axiom, or all propositions in it are propositional symbols. We assume that a
boolean function named ﬁnished testing whether a leaf is ﬁnished is available.
A proposition that is a propositional symbol will be called atomic, and other
propositions will be called nonatomic. A tree is ﬁnished when all its leaves
are ﬁnished.
The procedure search traverses all leaves of the tree from left to right as
long as not all of them are ﬁnished. For every unﬁnished leaf, the procedure
expand is called. Procedure expand builds a subtree by applying the appro-
priate inference rule to every nonatomic proposition in the sequent labeling
that leaf (proceeding from left to right). When the tree is ﬁnished, that is
when all leaves are ﬁnished, either all leaves are labeled with axioms or some
of the leaves are falsiﬁable. In the ﬁrst case we have a proof tree, and in the
second, all falsifying valuations can be found.
Deﬁnition 3.4.6
Procedure search.
The input to search is a one-node
tree labeled with a sequent Γ →∆. The output is a ﬁnished tree T called a
systematic deduction tree.

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
69
Procedure Search
procedure search(Γ →∆: sequent; var T : tree);
begin
let T be the one-node tree labeled with
Γ →∆;
while not all leaves of T are ﬁnished
do
T0 := T;
for each leaf node of T0
(in lexicographic order of tree addresses) do
if not finished(node) then
expand(node, T)
endif
endfor
endwhile;
if all leaves are axioms
then
write (‘T is a proof of Γ →∆’)
else
write (‘Γ →∆is falsiﬁable’)
endif
end
Procedure Expand
procedure expand(node : tree-address; var T : tree);
begin
let A1, ..., Am →B1, ..., Bn be the label of node;
let S be the one-node tree labeled with
A1, ..., Am →B1, ..., Bn;
for i := 1 to m do
if nonatomic(Ai) then
S := the new tree obtained from S by
applying to the descendant of Ai in
every nonaxiom leaf of S the
left rule applicable to Ai;
endif
endfor;
for i := 1 to n do
if nonatomic(Bi) then
S := the new tree obtained from S by
applying to the descendant of Bi in
every nonaxiom leaf of S the
right rule applicable to Bi;
endif
endfor;
T := dosubstitution(T, node, S)
end

70
3/Propositional Logic
The function dosubstitution yields the tree T[node ←S] obtained by
substituting the tree S at the address node in the tree T. Since a sequent
A1, ..., Am →B1, ..., Bn is processed from left to right, if the propositions
A1, ..., Ai−1 have been expanded so far, since the propositions Ai, ..., Am,
B1, ..., Bn are copied unchanged, every leaf of the tree S obtained so far is
of the form Γ, Ai, ..., Am →∆, B1, ..., Bn. The occurrence of Ai following Γ
is called the descendant of Ai in the sequent. Similarly, if the propositions
A1, ..., Am, B1, ..., Bi−1 have been expanded so far, every leaf of S is of the
form Γ →∆, Bi, ..., Bn, and the occurrence of Bi following ∆is called the
descendant of Bi in the sequent. Note that the two for loops may yield a tree
S of depth m + n.
A call to procedure expand is also called an expansion step. A round is
the sequence of expansion calls performed during the for loop in procedure
search, in which each unﬁnished leaf of the tree (T0) is expanded. Note that
if we change the function ﬁnished so that a leaf is ﬁnished if all propositions
in the sequent labeling it are propositional symbols, the procedure search will
produce trees in which leaves labeled with axioms also consist of sequents in
which all propositions are atomic. Trees obtained in this fashion are called
atomically closed.
EXAMPLE 3.4.4
Let us trace the construction of the systematic deduction tree (which is
a proof tree) for the sequent
(P ∧¬Q), (P ⊃Q), (T ⊃R), (P ∧S) →T.
The following tree is obtained at the end of the ﬁrst round:
Γ →∆
Q, P, ¬Q, P, S →T, T
Q, P, ¬Q, (P ∧S) →T, T
R, Q, P, ¬Q, P, S →T
R, Q, P, ¬Q, (P ∧S) →T
Q, P, ¬Q, (T ⊃R), (P ∧S) →T
P, ¬Q, (P ⊃Q), (T ⊃R), (P ∧S) →T
(P ∧¬Q), (P ⊃Q), (T ⊃R), (P ∧S) →T
where Γ →∆= P, ¬Q, (T ⊃R), (P ∧S) →P, T.
The leaves Q, P, ¬Q, P, S →T, T and R, Q, P, ¬Q, P, S →T are not
axioms (yet). Note how the same rule was applied to (P ∧S) in the
nodes labeled Q, P, ¬Q, (P ∧S) →T, T and R, Q, P, ¬Q, (P ∧S) →T,
because these occurrences are descendants of (P ∧S) in (P ∧¬Q),(P ⊃
Q),(T ⊃R), (P ∧S) →T. After the end of the second round, we have
the following tree, which is a proof tree.

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
71
Γ →∆
Q, P, P, S →Q, T, T
Q, P, ¬Q, P, S →T, T
Q, P, ¬Q, (P ∧S) →T, T
R, Q, P, P, S →Q, T
R, Q, P, ¬Q, P, S →T
R, Q, P, ¬Q, (P ∧S) →T
Q, P, ¬Q, (T ⊃R), (P ∧S) →T
P, ¬Q, (P ⊃Q), (T ⊃R), (P ∧S) →T
(P ∧¬Q), (P ⊃Q), (T ⊃R), (P ∧S) →T
where Γ →∆= P, ¬Q, (T ⊃R), (P ∧S) →P, T.
It should also be noted that the algorithm of deﬁnition 3.4.6 could be
made more eﬃcient.
For example, during a round through a sequent we
could delay the application of two-premise rules. This can be achieved if a
two-premise rule is applied only if a sequent does not contain propositions to
which a one-premise rule applies. Otherwise the expand procedure is called
only for those propositions to which a one-premise rule applies. In this fashion,
smaller trees are usually obtained. For more details, see problem 3.4.13.
3.4.7 Completeness of the Gentzen System G′
We are now ready to prove the fundamental theorem of this section.
Theorem 3.4.1
The procedure search terminates for every ﬁnite input se-
quent. If the input sequent Γ →∆is valid, procedure search produces a
proof tree for Γ →∆; if Γ →∆is falsiﬁable, search produces a tree from
which all falsifying valuations can be found.
Proof : Deﬁne the complexity of a proposition A as the number of logical
connectives occurring in A (hence, propositional symbols have complexity 0).
Given a sequent A1, ..., Am →B1, ..., Bn, deﬁne its complexity as the sum
of the complexities of A1, ..., Am,B1, ..., Bn.
Then, observe that for every
call to procedure expand, the complexity of every upper sequent involved in
applying a rule is strictly smaller than the complexity of the lower sequent
(to which the rule is applied). Hence, either all leaves will become axioms or
their complexity will become 0, which means that the while loop will always
terminate. This proves termination. We now prove the following claim.
Claim: Given any deduction tree T, a valuation v falsiﬁes the sequent
Γ →∆labeling the root of T if and only if v falsiﬁes some sequent labeling a
leaf of T.
Proof of claim: We use the induction principle for deduction trees. In
case of a one-node tree, the claim is obvious. Otherwise, the deduction tree

72
3/Propositional Logic
is either of the form
T2
S2
S1
where the bottom part of the tree is a one-premise rule, or of the form
T2
S2
T3
S3
S1
where the bottom part of the tree is a two-premise rule. We consider the
second case, the ﬁrst one being similar. By the induction hypothesis, v falsiﬁes
S2 if and only if v falsiﬁes some leaf of T2, and v falsiﬁes S3 if and only if v
falsiﬁes some leaf of T3. By lemma 3.4.1, v falsiﬁes S1 if and only if v falsiﬁes
S2 or S3. Hence, v falsiﬁes S1 if and only if either v falsiﬁes some leaf of T2
or some leaf of T3, that is, v falsiﬁes some leaf of T.
As a consequence of the claim, the sequent Γ →∆labeling the root
of the deduction tree is valid, if and only if all leaf sequents are valid. It is
easy to check that search builds a deduction tree (in a breadth-ﬁrst fashion).
Now, either Γ →∆is falsiﬁable, or it is valid.
In the ﬁrst case, by the
above claim, if v falsiﬁes Γ →∆, then v falsiﬁes some leaf sequent of the
deduction tree T. By the deﬁnition of ﬁnished, such a leaf sequent must be
of the form P1, ..., Pm →Q1, ..., Qn where the Pi and Qj are propositional
symbols, and the sets {P1, ..., Pm} and {Q1, ..., Qn} are disjoint since the
sequent is not an axiom. Hence, if Γ →∆is falsiﬁable, the deduction tree
T is not a proof tree. Conversely, if T is not a proof tree, some leaf sequent
of T is not an axiom.
By the deﬁnition of ﬁnished, this sequent must be
of the form P1, ..., Pm →Q1, ..., Qn where the Pi and Qj are propositional
symbols, and the sets {P1, ..., Pm} and {Q1, ..., Qn} are disjoint. The valuation
v which makes every Pi true and every Qj false falsiﬁes the leaf sequent
P1, ..., Pm →Q1, ..., Qn, and by the above claim, it also falsiﬁes the sequent
Γ →∆. Therefore, we have shown that Γ →∆is falsiﬁable if and only if
the deduction tree T is not a proof tree, or equivalently, that Γ →∆is valid
if and only if the deduction tree T is a proof tree. Furthermore, the above
proof also showed that if the deduction tree T is not a proof tree, all falsifying
valuations for Γ →∆can be found by inspecting the nonaxiom leaves of T.
Corollary
Completeness of G′. Every valid sequent is provable. Further-
more, there is an algorithm for deciding whether a sequent is valid and if so,
a proof tree is obtained.
As an application of the main theorem we obtain an algorithm to convert
a proposition to conjunctive (or disjunctive) normal form.

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
73
3.4.8 Conjunctive and Disjunctive Normal Form
Deﬁnition 3.4.7
A proposition A is in conjunctive normal form (for short,
CNF) if it is a conjunction C1 ∧... ∧Cm of disjunctions Ci = Bi,1 ∨... ∨Bi,ni,
where each Bi,j is either a propositional symbol P or the negation ¬P of
a propositional symbol. A proposition A is in disjunctive normal form (for
short, DNF) if it is a disjunction C1 ∨... ∨Cm of conjunctions Ci = Bi,1 ∧
... ∧Bi,ni, where each Bi,j is either a propositional symbol P or the negation
¬P of a propositional symbol.
Theorem 3.4.2
For every proposition A, a proposition A′ in conjunctive
normal form can be found such that |= A ≡A′. Similarly, a proposition A′′
in disjunctive normal form can be found such that |= A ≡A′′.
Proof : Starting with the input sequent →A, let T be the tree given
by the algorithm search. By theorem 3.4.1, either A is valid in which case
all leaves are axioms, or A is falsiﬁable. In the ﬁrst case, let A′ = P ∨¬P.
Clearly, |= A ≡A′. In the second case, the proof of theorem 3.4.1 shows that
a valuation v makes A true if and only if it makes every leaf sequent true. For
every nonaxiom leaf sequent A1, ..., Am →B1, ..., Bn, let
C = ¬A1 ∨... ∨¬Am ∨B1... ∨Bn
and let A′ be the conjunction of these propositions. Clearly, a valuation v
makes A′ true if and only if it makes every nonaxiom leaf sequent A1, ..., Am →
B1, ..., Bn true, if and only if it makes A true. Hence, |= A ≡A′.
To get an equivalent proposition in disjunctive normal form, start with
the sequent A →. Then, a valuation v makes A →false if and only if v makes
at least some of the sequent leaves false. Also, v makes A →false if and only
if v makes A true. For every nonaxiom sequent leaf A1, ..., Am →B1, ..., Bn,
let
C = A1 ∧... ∧Am ∧¬B1 ∧... ∧¬Bn
and let A′′ be the disjunction of these propositions. We leave as an exercise
to check that a valuation v makes some of the non-axiom leaves false if and
only if it makes the disjunction A′′ true. Hence |= A ≡A′′.
EXAMPLE 3.4.5
Counter-example tree for
→(¬P ⊃Q) ⊃(¬R ⊃S).

74
3/Propositional Logic
P →R, S
→R, S, ¬P
¬R →S, ¬P
→¬P, ¬R ⊃S
Q →R, S
¬R, Q →S
Q →¬R ⊃S
¬P ⊃Q →¬R ⊃S
→(¬P ⊃Q) ⊃(¬R ⊃S)
An equivalent proposition in conjunctive normal form is:
(¬Q ∨R ∨S) ∧(¬P ∨R ∨S).
EXAMPLE 3.4.6
Counter-example tree for
(¬P ⊃Q) ⊃(¬R ⊃S) →.
→P, Q
¬P →Q
→¬P ⊃Q
R →
→¬R
S →
¬R ⊃S →
(¬P ⊃Q) ⊃(¬R ⊃S) →
An equivalent proposition in disjunctive normal form is:
S ∨R ∨(¬P ∧¬Q).
We present below another method for converting a proposition to con-
junctive normal form that does not rely on the construction of a deduction
tree. This method is also useful in conjunction with the resolution method
presented in Chapter 4. First, we deﬁne the negation normal form of a propo-
sition.
3.4.9 Negation Normal Form
The set of propositions in negation normal form is given by the following
inductive deﬁnition.
Deﬁnition 3.4.8 The set of propositions in negation normal form (for short,
NNF) is the inductive closure of the set of propositions {P, ¬P | P ∈PS}
under the constructors C∨and C∧.

3.4 Proof Theory of Propositional Logic: The Gentzen System G′
75
More informally, propositions in NNF are deﬁned as follows:
(1) For every propositional letter P, P and ¬P are in NNF;
(2) If A and B are in NNF, then (A ∨B) and (A ∧B) are in NNF.
Lemma 3.4.4
Every proposition is equivalent to a proposition in NNF.
Proof : By theorem 3.3.1, we can assume that the proposition A is ex-
pressed only in terms of the connectives ∨and ∧and ¬. The rest of the proof
proceeds by induction on the number of connectives. By clause (1) of deﬁ-
nition 3.4.8, every propositional letter is in NNF. Let A be of the form ¬B.
If B is a propositional letter, by clause (1) of deﬁnition 3.4.8, the property
holds. If B is of the form ¬C, by lemma 3.3.6, ¬¬C is equivalent to C, by
the induction hypothesis, C is equivalent to a proposition C′ in NNF, and by
lemma 3.3.5, A is equivalent to C′, a proposition in NNF. If B is of the form
(C ∨D), by lemma 3.3.6, ¬(C ∨D) is equivalent to (¬C ∧¬D). Note that
both ¬C and ¬D have fewer connectives than A. Hence, by the induction
hypothesis, ¬C and ¬D are equivalent to propositions C′ and D′ in NNF. By
lemma 3.3.5, A is equivalent to (C′ ∧D′), which is in NNF. If B is of the
form (C ∧D), by lemma 3.3.6, ¬(C ∧D) is equivalent to (¬C ∨¬D). As in
the previous case, by the induction hypothesis, ¬C and ¬D are equivalent to
propositions C′ and D′ in NNF. By lemma 3.3.5, A is equivalent to (C′ ∨D′),
which is in NNF. Finally, if A is of the form (B ∗C) where ∗∈{∧, ∨}, by the
induction hypothesis, C and D are equivalent to propositions C′ and D′ in
NNF, and by lemma 3.3.5, A is equivalent to (C′ ∗D′) which is in NNF.
Lemma 3.4.5 Every proposition A (containing only the connectives ∨,∧,¬)
can be transformed into an equivalent proposition in conjunctive normal form,
by application of the following identities:
¬¬A ≃A
¬(A ∧B) ≃(¬A ∨¬B)
¬(A ∨B) ≃(¬A ∧¬B)
A ∨(B ∧C) ≃(A ∨B) ∧(A ∨C)
(B ∧C) ∨A ≃(B ∨A) ∧(C ∨A)
(A ∧B) ∧C ≃A ∧(B ∧C)
(A ∨B) ∨C ≃A ∨(B ∨C)
Proof : The proof of lemma 3.4.4 only uses the ﬁrst three tautologies.
Hence, given a proposition A, we can assume that it is already in NNF. We
prove by induction on propositions that a proposition in NNF can be converted
to a proposition in CNF using the last four tautologies. If A is either of the
form P or ¬P, we are done. If A is of the form (B ∨C) with B and C in NNF,
by the induction hypothesis both B and C are equivalent to propositions B′
and C′ in CNF. If both B′ and C′ consist of a single conjunct, (B′ ∨C′) is

76
3/Propositional Logic
a disjunction of propositional letters or negations of propositional letters and
by lemma 3.3.5, A is equivalent to (B′ ∨C′) which is in CNF. Otherwise, let
B′ = B′
1 ∧... ∧B′
m and C′ = C′
1 ∧... ∧C′
n, with either m > 1 or n > 1.
By repeated applications of the distributivity and associativity rules (to be
rigorous, by induction on m + n),
(B′ ∨C′) = (B′
1 ∧... ∧B′
m) ∨(C′
1 ∧... ∧C′
n)
≃((B′
1 ∧... ∧B′
m) ∨C′
1) ∧... ∧((B′
1 ∧... ∧B′
m) ∨C′
n)
≃

{(B′
i ∨C′
j) | 1 ≤i ≤m, 1 ≤j ≤n}.
The resulting proposition is in CNF, and by lemma 3.3.5, A is equivalent to a
proposition in CNF. If A is of the form (B ∧C) where B and C are in NNF,
by the induction hypothesis, B and C are equivalent to propositions B′ and
C′ in CNF. But then, (B′∧C′) is in CNF, and by lemma 3.4.5, A is equivalent
to (B′ ∧C′).
The conjunctive normal form of a proposition may be simpliﬁed by using
the commutativity rules and the idempotency rules given in lemma 3.3.6. A
lemma similar to lemma 3.4.5 can be shown for the disjunctive normal form
of a proposition.
EXAMPLE 3.4.7
Consider the proposition
A = (¬P ⊃Q) ⊃(¬R ⊃S).
First, we eliminate ⊃using the fact that (¬B ∨C) is equivalent to
(B ⊃C). We get
(¬(¬¬P ∨Q)) ∨(¬¬R ∨S).
Then, we put this proposition in NNF. We obtain
(¬P ∧¬Q) ∨(R ∨S).
Using distributivity we obtain
(¬P ∨R ∨S) ∧(¬Q ∨R ∨S),
which is the proposition obtained in example 3.4.5 (up to commutativ-
ity). However, note that the CNF (or DNF) of a proposition is generally
not unique. For example, the propositions
(P ∨Q) ∧(¬P ∨R)
and
(P ∨Q) ∧(¬P ∨R) ∧(Q ∨R)
are both in CNF and are equivalent.

PROBLEMS
77
PROBLEMS
3.4.1.
Give proof trees for the following tautologies:
A ⊃(B ⊃A)
(A ⊃B) ⊃((A ⊃(B ⊃C)) ⊃(A ⊃C))
A ⊃(B ⊃(A ∧B))
A ⊃(A ∨B)
B ⊃(A ∨B)
(A ⊃B) ⊃((A ⊃¬B) ⊃¬A)
(A ∧B) ⊃A
(A ∧B) ⊃B
(A ⊃C) ⊃((B ⊃C) ⊃((A ∨B) ⊃C))
¬¬A ⊃A
3.4.2.
Using counter-example trees, give propositions in conjunctive and dis-
junctive normal form equivalent to the following propositions:
(A ⊃C) ⊃((B ⊃D) ⊃((A ∨B) ⊃C))
(A ⊃B) ⊃((B ⊃¬C) ⊃¬A)
3.4.3.
Recall that ⊥is a constant symbol always interpreted as F.
(i) Show that the following equivalences are valid.
¬A ≡(A ⊃⊥)
(A ∨B) ≡((A ⊃⊥) ⊃B)
(A ≡B) ≡(A ⊃B) ∧(B ⊃A)
(A ∧B) ≡¬(¬A ∨¬B)
(ii) Show that every proposition A is equivalent to a proposition A′
using only the connective ⊃and the constant symbol ⊥.
(iii) Consider the following Gentzen-like rules for propositions over
the language consisting of the propositional letters, ⊃and ⊥.
The symbols Γ, ∆, Λ denote ﬁnite arbitrary sequences of propositions
(possibly empty):
Γ, ∆→A, Λ
B, Γ, ∆→Λ
Γ, (A ⊃B), ∆→Λ
A, Γ →B, ∆, Λ
Γ →∆, (A ⊃B), Λ
Γ →∆, Λ
Γ →∆, ⊥, Λ

78
3/Propositional Logic
The axioms of this system are all sequents of the form Γ →∆where
Γ and ∆contain a common proposition, and all sequents of the form
Γ, ⊥, ∆→Λ.
(a) Prove that for every valuation v, the conclusion of a rule is fal-
siﬁable if and only if one of the premises is falsiﬁable, and that the
axioms are not falsiﬁable.
(b) Prove that the above Gentzen-like system is complete.
(c) Convert (P ⊃Q) ⊃(¬Q ⊃¬P) to a proposition involving only
⊃and ⊥. Give a proof tree for this proposition in the above system.
3.4.4.
Let C and D be propositions and P a propositional letter. Prove that
the following equivalence (the resolution rule) holds by giving a proof
tree:
(C ∨P) ∧(D ∨¬P) ≡(C ∨P) ∧(D ∨¬P) ∧(C ∨D)
Show that the above also holds if either C or D is missing.
3.4.5.
Give Gentzen-like rules for equivalence (≡).
3.4.6.
Give proof trees for the following tautologies:
Associativity rules:
((A ∨B) ∨C) ≡(A ∨(B ∨C))
((A ∧B) ∧C) ≡(A ∧(B ∧C))
Commutativity rules:
(A ∨B) ≡(B ∨A)
(A ∧B) ≡(B ∧A)
Distributivity rules:
(A ∨(B ∧C)) ≡((A ∨B) ∧(A ∨C))
(A ∧(B ∨C)) ≡((A ∧B) ∨(A ∧C))
De Morgan’s rules:
¬(A ∨B) ≡(¬A ∧¬B)
¬(A ∧B) ≡(¬A ∨¬B)
Idempotency rules:
(A ∨A) ≡A
(A ∧A) ≡A
Double negation rule:
¬¬A ≡A
Absorption rules:
(A ∨(A ∧B)) ≡A
(A ∧(A ∨B)) ≡A
Laws of zero and one:
(A∨⊥) ≡A
(A∧⊥) ≡⊥
(A ∨⊤) ≡⊤
(A ∧⊤) ≡A
(A ∨¬A) ≡⊤(A ∧¬A) ≡⊥

PROBLEMS
79
3.4.7.
Instead of deﬁning logical equivalence (≃) semantically as in deﬁnition
3.3.6, let us deﬁne ≃proof-theoretically so that, for all A, B ∈PROP,
A ≃B if and only if ⊢(A ⊃B) ∧(B ⊃A) in G′.
Prove that ≃is an equivalence relation satisfying the properties of
lemma 3.3.5 (Hence, ≃is a congruence).
3.4.8.
Give Gentzen-like rules for the connective ⊕(exclusive-or), where H⊕
is the binary truth function deﬁned by the proposition (P ∧¬Q) ∨
(¬P ∧Q).
∗3.4.9.
The Hilbert system H for propositional logic is deﬁned below. For
simplicity, it is assumed that only the connectives ∧, ∨, ⊃and ¬ are
used.
The axioms are all propositions given below, where A,B,C denote
arbitrary propositions.
A ⊃(B ⊃A)
(A ⊃B) ⊃((A ⊃(B ⊃C)) ⊃(A ⊃C))
A ⊃(B ⊃(A ∧B))
A ⊃(A ∨B),
B ⊃(A ∨B)
(A ⊃B) ⊃((A ⊃¬B) ⊃¬A)
(A ∧B) ⊃A,
(A ∧B) ⊃B
(A ⊃C) ⊃((B ⊃C) ⊃((A ∨B) ⊃C))
¬¬A ⊃A
There is a single inference rule, called modus ponens given by:
A
(A ⊃B)
B
Let {A1, ..., Am} be any set of propositions. The concept of a de-
duction tree (in the system H) for a proposition B from the set
{A1, ..., Am} is deﬁned inductively as follows:
(i) Every one-node tree labeled with an axiom B or a proposition B
in {A1, ..., Am} is a deduction tree of B from {A1, ..., Am}.
(ii) If T1 is a deduction tree of A from {A1, ..., Am} and T2 is a de-
duction tree of (A ⊃B) from {A1, ..., Am}, then the following tree is
a deduction tree of B from {A1, ..., Am}:
T1
A
T2
(A ⊃B)
B

80
3/Propositional Logic
A proof tree is a deduction tree whose leaves are labeled with axioms.
Given a set {A1, ..., Am} of propositions and a proposition B, we use
the notation A1, ..., Am ⊢B to denote that there is deduction of B
from {A1, ..., Am}. In particular, if the set {A1, ..., Am} is empty, the
tree is a proof tree and we write ⊢B.
(i) Prove that modus ponens is a sound rule, in the sense that if both
premises are valid, then the conclusion is valid. Prove that the system
H is sound; that is, every proposition provable in H is valid.
(ii) Prove that for every proposition A, ⊢(A ⊃A).
Hint: Use the axioms A ⊃(B ⊃A) and (A ⊃B) ⊃((A ⊃(B ⊃
C)) ⊃(A ⊃C)).
(iii) Prove the following:
(a)
A1, ..., Am ⊢Ai, for every i, 1 ≤i ≤m.
(b)
If A1, ..., Am ⊢Bi for every i, 1 ≤i ≤m and
B1, ..., Bm ⊢C, then A1, ..., Am ⊢C.
∗3.4.10. In this problem, we are also considering the proof system H of problem
3.4.9. The deduction theorem states that, for arbitrary propositions
A1, ..., Am, A, B,
if A1, ..., Am, A ⊢B, then
A1, ..., Am ⊢(A ⊃B).
Prove the deduction theorem.
Hint: Use induction on deduction trees. The base case is relatively
easy. For the induction step, assume that the deduction tree is of the
form
T1
B1
T2
(B1 ⊃B)
B
where the leaves are either axioms or occurrences of the propositions
A1,...,Am,A. By the induction hypothesis, there are deduction trees
T ′
1 for (A ⊃B1) and T ′
2 for (A ⊃(B1 ⊃B)), where the leaves of T ′
1
and T ′
2 are labeled with axioms or the propositions A1, ..., Am. Show
how a deduction tree whose leaves are labeled with axioms or the
propositions A1, ..., Am can be obtained for (A ⊃B).
∗3.4.11. In this problem, we are still considering the proof system H of problem
3.4.9. Prove that the following meta-rules hold about deductions in

PROBLEMS
81
the system H: For all propositions A,B,C and ﬁnite sequence Γ of
propositions (possibly empty), we have:
Introduction
Elimination
⊃
If Γ, A ⊢B,
A, (A ⊃B) ⊢B
then Γ ⊢(A ⊃B)
∧
A, B ⊢(A ∧B)
(A ∧B) ⊢A
(A ∧B) ⊢B
∨
A ⊢(A ∨B)
If Γ, A ⊢C and Γ, B ⊢C
then Γ, (A ∨B) ⊢C
¬
If Γ, A ⊢B and Γ, A ⊢¬B
¬¬A ⊢A
then Γ ⊢¬A
(double negation elimination)
(reductio ad absurdum)
A, ¬A ⊢B
(weak negation elimination)
Hint: Use problem 3.4.9(iii) and the deduction theorem.
∗3.4.12. In this problem it is shown that the Hilbert system H is complete, by
proving that for every Gentzen proof T of a sequent →A, where A
is any proposition, there is a proof in the system H.
(i) Prove that for arbitrary propositions A1, ..., Am, B1, ..., Bn,
(a) in H, for n > 0,
A1, ..., Am, ¬B1, ..., ¬Bn ⊢P ∧¬P if and only if
A1, ..., Am, ¬B1, ..., ¬Bn−1 ⊢Bn, and
(b) in H, for m > 0,
A1, ..., Am, ¬B1, ..., ¬Bn ⊢P ∧¬P if and only if
A2, ..., Am, ¬B1, ..., ¬Bn ⊢¬A1.
(ii) Prove that for any sequent A1, ..., Am →B1, ..., Bn, if
A1, ..., Am →B1, ..., Bn
is provable in the Gentzen system G′ then
A1, ..., Am, ¬B1, ..., ¬Bn ⊢(P ∧¬P)
is a deduction in the Hilbert system H. Conclude that H is complete.
Hint: Use problem 3.4.11.
3.4.13. Consider the modiﬁcation of the algorithm of deﬁnition 3.4.6 obtained
by postponing applications of two-premise rules. During a round, a

82
3/Propositional Logic
two-premise rule is applied only if a sequent does not contain proposi-
tions to which a one-premise rule applies. Otherwise, during a round
the expand procedure is called only for those propositions to which a
one-premise rule applies.
Show that theorem 3.4.1 still holds for the resulting algorithm. Com-
pare the size of the proof trees obtained from both versions of the
search algorithm, by trying a few examples.
3.4.14. Write a computer program (preferably in PASCAL or C) implement-
ing the search procedure of deﬁnition 3.4.6.
3.5 Proof Theory for Inﬁnite Sequents: Extended Com-
pleteness of G′
In this section, we obtain some important results for propositional logic (ex-
tended completeness, compactness, model existence) by generalizing the pro-
cedure search to inﬁnite sequents.
3.5.1 Inﬁnite Sequents
We extend the concept of a sequent Γ →∆by allowing Γ or ∆to be count-
ably inﬁnite sequences of propositions. The method of this section is very
important because it can be rather easily adapted to show the completeness
of a Gentzen system obtained by adding quantiﬁer rules to G′ for ﬁrst-order
logic (see Chapter 5).
By suitably modifying the search procedure, we can generalize the main
theorem (theorem 3.4.1) and obtain both the extended completeness theorem
and the compactness theorem. The procedure search is no longer an algorithm
since it can go on forever in some cases. However, if the input sequent is
valid, a ﬁnite proof will be obtained. Also, if the input sequent is falsiﬁable,
a falsifying valuation will be (nonconstructively) obtained.
We will now allow sequents Γ →∆in which Γ or ∆can be countably
inﬁnite sequences. It is convenient to assume that Γ is written as A1, ..., Am, ...
(possibly inﬁnite to the right) and that ∆is written as B1, ..., Bn, ... (possibly
inﬁnite to the right). Hence, a sequent will be denoted as
A1, ..., Am, ... →B1, ..., Bn, ...
where the lists on both sides of →are ﬁnite or (countably) inﬁnite.
In order to generalize the search procedure, we need to deﬁne the func-
tions head and tail operating on possibly inﬁnite sequences. Let us denote
the empty sequence as <>.
head(<>) =<>; otherwise head(A1, ..., Am, ...) = A1.

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
83
tail(<>) =<>; otherwise tail(A1, ..., Am, ...) = A2, ..., Am, ... .
In particular, tail(A1) =<>. The predicate atomic is deﬁned such that
atomic(A) is true if and only if A is a propositional symbol.
3.5.2 The Search Procedure for Inﬁnite Sequents
Every node of the systematic tree constructed by search is still labeled with
a ﬁnite sequent Γ →∆. We will also use two global variables L and R, which
are possibly countably inﬁnite sequences of propositions. The initial value of
L is tail(Γ0) and the initial value of R is tail(∆0), where Γ0 →∆0 is the
initial sequent.
A leaf of the tree is an axiom (or is closed) iﬀits label Γ →∆is an
axiom. A leaf is ﬁnished iﬀeither
(1) it is closed, or
(2) the sequences L and R are empty and all propositions in Γ, and ∆
are atomic. The new versions of procedures search and expand are given as
follows.
Deﬁnition 3.5.1
Procedure search.
procedure search(Γ0 →∆0 : sequent; var T : tree);
begin
L := tail(Γ0); Γ := head(Γ0);
R := tail(∆0); ∆:= head(∆0);
let T be the one-node tree labeled with Γ →∆;
while not all leaves of T are ﬁnished do
T0 := T;
for each leaf node of T0
(in lexicographic order of tree addresses) do
if not finished(node) then
expand(node, T)
endif
endfor;
L := tail(L); R := tail(R)
endwhile;
if all leaves are closed
then
write (‘T is a proof of Γ0 →∆0’)
else
write (‘Γ0 →∆0 is falsiﬁable’)
endif
end
The input to search is a one-node tree labeled with a possibly inﬁnite

84
3/Propositional Logic
sequent Γ →∆. Procedure search builds a possibly inﬁnite systematic deduc-
tion tree using procedure expand.
Procedure expand is modiﬁed as follows: For every leaf u created during
an expansion step, if Γ →∆is the label of u, the ﬁnite sequent Γ →∆is
extended to Γ, head(L) →∆, head(R).
At the end of a round, the heads
of both L and R are deleted.
Hence, every proposition will eventually be
considered.
Procedure Expand
procedure expand(node : tree-address; var T : tree);
begin
let A1, ..., Am →B1, ..., Bn be the label of node;
let S be the one-node tree labeled with
A1, ..., Am →B1, ..., Bn;
for i := 1 to m do
if nonatomic(Ai) then
S := the new tree obtained from S by
applying to the descendant of Ai in
every nonaxiom leaf of S the
left rule applicable to Ai;
(only the sequent part is modiﬁed,
L and R are unchanged)
endif
endfor;
for i := 1 to n do
if nonatomic(Bi) then
S := the new tree obtained from S by
applying to the descendant of Bi in
every nonaxiom leaf of S the
right rule applicable to Bi;
(only the sequent part is modiﬁed,
L and R are unchanged)
endif
endfor;
for each nonaxiom leaf u of S do
let Γ →∆be the label of u;
Γ′ := Γ, head(L);
∆′ := ∆, head(R);
create a new leaf u1, son of u,
labeled with the sequent Γ′ →∆′
endfor;
T := dosubstitution(T, node, S)
end
If search terminates with a systematic tree whose leaves are all closed,

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
85
we say that search terminates with a closed tree. Note that a closed tree is
ﬁnite by deﬁnition. We will also need the following deﬁnitions.
Deﬁnition 3.5.2
Given a possibly inﬁnite sequent Γ →∆, a valuation v
falsiﬁes Γ →∆if
v |= A
for every proposition A in Γ, and
v |= ¬B
for every proposition B in ∆. We say that Γ →∆is falsiﬁable.
A valuation v satisﬁes Γ →∆if, whenever
v |= A
for every proposition A in Γ, then there is some proposition B in ∆such that
v |= B.
We say that Γ →∆is satisﬁable.
The sequent Γ →∆is valid if it is satisﬁed by every valuation. This is
denoted by
|= Γ →∆.
The sequent Γ →∆is provable if there exist ﬁnite subsequences C1, ..., Cm
and D1, ..., Dn of Γ and ∆respectively, such that the ﬁnite sequent
C1, ..., Cm →D1, ..., Dn
is provable. This is denoted by
⊢Γ →∆.
Note that if an inﬁnite sequent is provable, then it is valid. Indeed, if
Γ →∆is provable, some subsequent C1, ..., Cm →D1, ..., Dn is provable, and
therefore valid. But this implies that Γ →∆is valid, since D1, ..., Dn is a
subsequence of ∆.
EXAMPLE 3.5.1
Consider the sequent Γ0 →∆0 where
Γ0 =< P0, (P0 ⊃P1), (P1 ⊃P2), ..., (Pi ⊃Pi+1), ... >,
and
∆0 =< Q, P3 > .

86
3/Propositional Logic
Initially,
Γ =< P0 >,
L =< (P0 ⊃P1), (P1 ⊃P2), ..., (Pi ⊃Pi+1), ... >,
∆=< Q >,
and
R =< P3 > .
At the end of the ﬁrst round, we have the following tree:
P0, (P0 ⊃P1) →Q, P3
P0 →Q
Note how (P0 ⊃P1), the head of L, was added in the premise of the top
sequent, and P3, the head of R, was added to the conclusion of the top
sequent. At the end of this round,
L =< (P1 ⊃P2), ..., (Pi ⊃Pi+1), ... >
and R =<> .
At the end of the second round, we have:
P0 →P0, Q, P3
P1, P0, (P1 ⊃P2) →Q, P3
P1, P0 →Q, P3
P0, (P0 ⊃P1) →Q, P3
P0 →Q
We have
L =< (P2 ⊃P3), ..., (Pi ⊃Pi+1), ... >
and R =<> .
At the end of the third round, we have the tree:
P0 →P0, Q, P3
P1, P0 →P1, Q, P3
P2, P1, P0, (P2 ⊃P3) →Q, P3
P2, P1, P0 →Q, P3
P1, P0, (P1 ⊃P2) →Q, P3
P1, P0 →Q, P3
P0, (P0 ⊃P1) →Q, P3
P0 →Q
We have
L =< (P3 ⊃P4), ..., (Pi ⊃Pi+1), ... >
and R =<> .

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
87
At the end of the fourth round, we have the closed tree:
Π1
P1, P0 →P1, Q, P3
P2, P1, P0 →P2, Q, P3
P3, P2, P1, P0 →Q, P3
P2, P1, P0, (P2 ⊃P3) →Q, P3
P2, P1, P0 →Q, P3
P1, P0, (P1 ⊃P2) →Q, P3
P1, P0 →Q, P3
P0, (P0 ⊃P1) →Q, P3
P0 →Q
where
Π1 = P0 →P0, Q, P3.
The above tree is not quite a proof tree, but a proof tree can be con-
structed from it as follows. Starting from the root and proceeding from
bottom-up, for every sequent at depth k in which a proposition of the
form head(L) or head(R) was introduced, add head(L) after the right-
most proposition of the premise of every sequent at depth less than k,
and add head(R) after the rightmost proposition of the conclusion of
every sequent at depth less than k:
Π2
Π3
P2, P1, P0 →P2, Q, P3
P3, P2, P1, P0 →Q, P3
P2, P1, P0, (P2 ⊃P3) →Q, P3
P2, P1, P0, (P2 ⊃P3) →Q, P3
P1, P0, (P1 ⊃P2), (P2 ⊃P3) →Q, P3
P1, P0, (P1 ⊃P2), (P2 ⊃P3) →Q, P3
P0, (P0 ⊃P1), (P1 ⊃P2), (P2 ⊃P3) →Q, P3
P0, (P0 ⊃P1), (P1 ⊃P2), (P2 ⊃P3) →Q, P3
with
Π2 = P0, (P1 ⊃P2), (P2 ⊃P3) →P0, Q, P3
and
Π3 = P1, P0, (P2 ⊃P3) →P1, Q, P3.
Then, delete duplicate nodes, obtaining a legal proof tree:

88
3/Propositional Logic
Π2
Π3
P2, P1, P0 →P2, Q, P3
P3, P2, P1, P0 →Q, P3
P2, P1, P0, (P2 ⊃P3) →Q, P3
P1, P0, (P1 ⊃P2), (P2 ⊃P3) →Q, P3
P0, (P0 ⊃P1), (P1 ⊃P2), (P2 ⊃P3) →Q, P3
with
Π2 = P0, (P1 ⊃P2), (P2 ⊃P3) →P0, Q, P3
and
Π3 = P1, P0, (P2 ⊃P3) →P1, Q, P3.
EXAMPLE 3.5.2
Consider the sequent Γ0 →∆0 where
Γ0 =< P0, (P0 ⊃P1), (P1 ⊃P2), ..., (Pi ⊃Pi+1), ... >,
and
∆0 =< Q > .
Note that the only diﬀerence is the absence of P3 in the conclusion. This
time, the search procedure does not stop. Indeed, the rightmost branch
of the tree is inﬁnite, since every sequent in it is of the form
Pn, Pn−1, ...P1, P0 →Q.
Let U be the union of all the propositions occurring as premises in the
sequents on the inﬁnite branch of the tree, and V be the union of all the
propositions occurring as conclusions in such sequents. We have
U = {(P0 ⊃P1), ..., (Pi ⊃Pi+1), ..., P0, P1, ..., Pi, ...},
and
V = {Q}.
The pair (U, V ) can be encoded as a single set if we preﬁx every proposi-
tion in U with the letter “T” (standing for true), and every proposition
in V with the letter “F” (standing for false). The resulting set
{T(P0 ⊃P1), ..., T(Pi ⊃Pi+1), ..., TP0, TP1, ..., TPi, ..., FQ}
is a set having some remarkable properties, and called a Hintikka set.
The crucial property of Hintikka sets is that they are always satisﬁable.
For instance, it is easy to see that the valuation such that v(Pi) = T for
all i ≥0 and v(Q) = F satisﬁes the above Hintikka set.
Roughly speaking, the new version of the search procedure is complete
because:

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
89
(1) If the input sequent is valid, a proof tree can be constructed from
the output tree (as in example 3.5.1);
(2) If the sequent is falsiﬁable, the output tree contains a path from
which a Hintikka set can be constructed (as in example 3.5.2).
Hence, a
counter example exists.
In order to prove rigorously properties (1) and (2), we will need some
auxiliary deﬁnitions and lemmas. First, we shall need the following result
about inﬁnite ﬁnite-branching trees known as K¨onig’s lemma.
3.5.3 K¨onig’s Lemma
Recall from Subsection 2.2.2 that a tree T is ﬁnite branching iﬀevery node
has ﬁnite outdegree (ﬁnite number of successors).
Lemma 3.5.1
(K¨onig’s lemma) If T is a ﬁnite-branching tree with inﬁnite
domain, then there is some inﬁnite path in T.
Proof : We show that an inﬁnite path can be deﬁned inductively. Let u0
be the root of the tree. Since the domain of T is inﬁnite and u0 has a ﬁnite
number of successors, one of the subtrees of u0 must be inﬁnite (otherwise,
T would be ﬁnite). Let u1 be the root of the leftmost inﬁnite subtree of u0.
Now, assume by induction that a path u0, ..., un has been deﬁned and that the
subtree T/un is inﬁnite. Since un has a ﬁnite number of successors and since
T/un is inﬁnite, using the same reasoning as above, un must have a successor
which is the root of an inﬁnite tree. Let un+1 be the leftmost such node. It
is clear that the above inductive construction yields an inﬁnite path in T (in
fact, the leftmost such path).
Remark: The above proof only shows the existence of an inﬁnite path.
In particular, since there is in general no eﬀective way of testing whether a
tree is inﬁnite, there is generally no algorithm to ﬁnd the above nodes.
In example 3.5.2, the two sets U and V play a crucial role since they yield
a falsifying valuation. The union of U and V is a set having certain remarkable
properties ﬁrst investigated by Hintikka and that we now describe. For this,
it is convenient to introduce the concept of a signed formula as in Smullyan,
1968.
3.5.4 Signed Formulae
Following Smullyan, we will deﬁne the concept of an a-formula and of a b-
formula, and describe their components. Using this device greatly reduces the
number of cases in the deﬁnition of a Hintikka set, as well as in some proofs.
Deﬁnition 3.5.3 A signed formula is any expression of the form TA or FA,
where A is an arbitrary proposition. Given any sequent (even inﬁnite) Γ →∆,

90
3/Propositional Logic
we deﬁne the signed set of formulae
{TA | A ∈Γ} ∪{FB | B ∈∆}.
Deﬁnition 3.5.4
type-a and type-b signed formulae and their components
are deﬁned in the following tables. If A is a signed formula of type a, it has
two components denoted by A1 and A2. Similarly, if B is a formula of type
b, it has two components denoted by B1 and B2.
Type-a formulae
A
A1
A2
T(X ∧Y )
TX
TY
F(X ∨Y )
FX
FY
F(X ⊃Y )
TX
FY
T(¬X)
FX
FX
F(¬X)
TX
TX
Type-b formulae
B
B1
B2
F(X ∧Y )
FX
FY
T(X ∨Y )
TX
TY
T(X ⊃Y )
FX
TY
Deﬁnition 3.5.5
A valuation v makes the signed formula TA true iﬀv
makes A true and v makes FA true iﬀv makes A false. A valuation v
satisﬁes a signed set S iﬀv makes every signed formula in S true.
Note that for any valuation, a signed formula A of type a is true if and
only if both A1 and A2 are true. Accordingly, we also refer to an a-formula
as a formula of conjunctive type. On the other hand, for any valuation, a
signed formula B of type b is true if and only if at least one of B1, B2 is
true. Accordingly, a b-formula is also called a formula of disjunctive type.
Deﬁnition 3.5.6
The conjugate of a signed formula is deﬁned as follows:
The conjugate of a formula TA is FA, and the conjugate of FA is TA.
3.5.5 Hintikka Sets
A Hintikka set is a set of signed formulae satisfying certain downward closure
conditions that ensure that such a set is satisﬁable.
Deﬁnition 3.5.7 A set S of signed formulae is a Hintikka set iﬀthe following
conditions hold:

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
91
(H1) No signed propositional letter and its conjugate are both in S.
(H2) If a signed a-formula A is in S then both A1 and A2 are in S.
(H3) If a signed b-formula B is in S then either B1 is in S or B2 is in S.
The following lemma shows that Hintikka sets arise when the search
procedure does not produce a closed tree (recall that a closed tree is ﬁnite).
Lemma 3.5.2 Whenever the tree T constructed by the search procedure is
not a closed tree, a Hintikka set can be extracted from T.
Proof : If T is not a closed tree, then either it is ﬁnite and some leaf is
not an axiom, or it is inﬁnite. If T is inﬁnite, by lemma 3.5.1, there is an
inﬁnite path in T. In the ﬁrst case, consider a path to some nonaxiom leaf,
and in the second consider an inﬁnite path. Let
S = {TA | A ∈U} ∪{FB | B ∈V }
be the set of signed formulae such that U is the union of all propositions
occurring in the antecedent of each sequent in the chosen path, and V is the
union of all propositions occurring in the succedent of each sequent in the
chosen path. S is a Hintikka set.
(1) H1 holds. Since every atomic formula occurring in a sequent occurs
in every path having this sequent as source, if S contains both TP and FP
for some propositional letter P, some sequent in the path is an axiom. This
contradicts the fact that either the path is ﬁnite and ends in a non-axiom, or
is an inﬁnite path.
(2) H2 and H3 hold. This is true because the deﬁnition of a-components
and b-components mirrors the inference rules.
Since all nonatomic propo-
sitions in a sequent Γ →∆on the chosen path are considered during the
expansion phase, and since every proposition in the input sequent is eventu-
ally considered (as head(L) or head(R)):
(i) For every proposition A in U, if A belongs to Γ →∆and TA is of
type a, A1 and A2 are added to the successor of Γ →∆during the expansion
step. More precisely, if A1 (or A2) is of the form TC1 (or TC2), C1 (C2) is
added to the premise of the successor of Γ →∆; if A1 (A2) is of the form
FC1 (FC2), C1 (C2) is added to the conclusion of the successor of Γ →∆. In
both cases, A1 and A2 belong to S.
(ii) If A belongs to Γ →∆and TA is of type b, A1 is added to the left
successor of Γ →∆, and A2 is added to the right successor of Γ →∆, during
the expansion step. As in (i), more precisely, if A1 (or A2) is of the form TC1
(TC2), C1 (C2) is added to the premise of the left successor (right successor)
of Γ →∆; if A1 (A2) is of the form FC1 (FC2), C1 (C2) is added to the
conclusion of the left successor (right successor) of Γ →∆. Hence, either B1
or B2 belongs to S.

92
3/Propositional Logic
Properties (i) and (ii) also apply to the set V . This proves that S is a
Hintikka set.
The following lemma establishes the fundamental property of Hintikka
sets.
Lemma 3.5.3
Every Hintikka set S is satisﬁable.
Proof : We deﬁne a valuation v satisfying S as follows: For every signed
propositional symbol TP in S let v(P) = T; for every signed propositional
symbol FP in S let v(P) = F; for every propositional symbol P such that
neither TP nor FP is in S, set arbitrarily v(P) = T. By clause (H1) of a
Hintikka set, v is well deﬁned. It remains to show that v makes every signed
formula TX or FX true (that is, in the ﬁrst case X true and in the second
case X false). This is shown by induction on the number of logical connectives
in X. Since every signed formula is either of type a or of type b, there are two
cases.
(1) If A of type a is in S, by (H2) both A1 and A2 are in S. But A1 and
A2 have fewer connectives than A and so, the induction hypothesis applies.
Hence, v makes both A1 and A2 true. This implies that v makes A true.
(2) If B of type b is in S, by (H3) either B1 or B2 is in S. Without loss
of generality assume that B1 is in S. Since B1 has fewer connectives than B,
the induction hypothesis applies. Hence, v makes B1 true. This implies that
v makes B true.
3.5.6 Extended Completeness of the Gentzen System G′
We are now ready to prove the generalization of theorem 3.4.1.
Theorem 3.5.1
Given a sequent Γ →∆, either
(1) Γ →∆is falsiﬁable and
(i) If Γ →∆is inﬁnite, then search runs forever building an inﬁnite
tree T, or
(ii) If Γ →∆is ﬁnite, then search produces a ﬁnite tree T with some
non-axiom leaf.
In both cases, a falsifying valuation can be obtained from some
path in the tree produced by procedure search; or
(2) Γ →∆is valid and search terminates with a closed tree T. In this case,
there exist ﬁnite subsequences C1, ..., Cm and D1, ..., Dn of Γ and ∆
respectively such that the sequent C1, ..., Cm →D1, ..., Dn is provable.
Proof : First, observe that if a subsequent of a sequent is valid, the
sequent itself is valid. Also, if Γ →∆is inﬁnite, search terminates if and only
if the tree is closed. This last statement holds because any node that is not an
axiom is not ﬁnished, since otherwise L and R would be empty, contradicting

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
93
the fact that Γ →∆is inﬁnite. Hence, at every step of the procedure search,
some node is unﬁnished, and since the procedure expand adds at least one
new node to the tree (when head(L) is added to Γ and head(R) is added to
∆), search builds an inﬁnite tree. Consequently, if Γ →∆is inﬁnite, either
search halts with a closed tree, or it builds an inﬁnite tree.
If search halts with a closed tree, let C1, ..., Cm be the initial subsequence
of propositions in Γ that were deleted from Γ to obtain L, and D1, ..., Dn be
the initial subsequence of propositions in ∆which were deleted from ∆to
obtain R. A proof tree for a the ﬁnite sequent C1, ..., Cm →D1, ..., Dn can
easily be obtained from T, using the technique illustrated in example 3.5.1.
First, starting from the root and proceeding bottom-up, for each node
Γ, head(L) →∆, head(R)
at depth k created at the end of a call to procedure expand, add head(L) after
the rightmost proposition in the premise of every sequent at depth less than
k, and add head(R) after the rightmost proposition in the conclusion of every
sequent at depth less than k, obtaining the tree T ′. Then, a proof tree T ′′
for C1, ..., Cm →D1, ..., Dn is constructed from T ′ by deleting all duplicate
nodes. The tree T ′′ is a proof tree because the same inference rules that have
been used in T are used in T ′′. A proof similar to that of theorem 3.4.1 shows
that C1, ..., Cm →D1, ..., Dn is valid and consequently that Γ →∆is valid.
Hence, if the search procedure halts with a closed tree, a subsequent of
Γ →∆is provable, which implies that Γ →∆is provable (and consequently
valid). Hence, if Γ →∆is falsiﬁable, either the search procedure halts with
a ﬁnite nonclosed tree if Γ →∆is ﬁnite, or else search must go on forever
if Γ →∆is inﬁnite.
If the tree is ﬁnite, some leaf is not an axiom, and
consider the path to this leaf. Otherwise, let T be the inﬁnite tree obtained
in the limit. This tree is well deﬁned since for every integer k, search will
produce the subtree of depth k of T. Since T is inﬁnite and ﬁnite branching,
by K¨onig’s lemma, there is an inﬁnite path u0, u1, ..., un,... in T. By lemma
3.5.2, the set
S = {TA | A ∈U} ∪{FB | B ∈V }
of signed formulae such that U is the union of all propositions occurring in
the antecedent of each sequent in the chosen path, and V is the union of all
propositions occurring in the succedent of each sequent in the chosen path, is
a Hintikka set. By lemma 3.5.3, S is satisﬁable. But any valuation satisfying
S falsiﬁes Γ →∆, and Γ →∆is falsiﬁable.
To summarize, if the search procedure halts with a closed tree, Γ →∆
is provable, and therefore valid. Otherwise Γ →∆is falsiﬁable.
Conversely, if Γ →∆is valid, search must halt with a closed tree,
since otherwise the above reasoning shows that a falsifying valuation can be
found.
But then, we have shown that Γ →∆is provable.
If Γ →∆is

94
3/Propositional Logic
falsiﬁable, search cannot halt with a closed tree, since otherwise Γ →∆
would be provable, and consequently valid. But then, we have shown that a
falsifying valuation can be found from the tree T. This concludes the proof
of the theorem.
We now derive some easy consequences of the main theorem. Since a
provable sequent is valid, the following is an obvious corollary.
Theorem 3.5.2
(Extended completeness theorem for G′) For every (possi-
bly inﬁnite) sequent Γ →∆, Γ →∆is valid if and only if Γ →∆is provable.
3.5.7 Compactness, Model Existence, Consistency
Recall that a proposition A is satisﬁable if some valuation makes it true.
Deﬁnition 3.5.8
A set Γ of propositions is satisﬁable iﬀsome valuation
makes all propositions in Γ true.
Theorem 3.5.3
(Compactness theorem for G′) For any (possibly inﬁnite)
set Γ of propositions, if every ﬁnite (nonempty) subset of Γ is satisﬁable then
Γ is satisﬁable.
Proof : Assume Γ is not satisﬁable. Viewing Γ as a sequence of proposi-
tions, it is clear that the sequent
Γ →
is valid, and by theorem 3.5.1 there is a ﬁnite subsequence A1, ..., Ap of Γ such
that
A1, ..., Ap →
is provable. But then, by lemma 3.4.3, A1, ..., Ap →is valid, which means that
A1, ..., Ap is not satisﬁable contrary to the hypothesis. Hence Γ is satisﬁable.
Deﬁnition 3.5.9
A set Γ of propositions is consistent if there exists some
proposition B such that the sequent Γ →B is not provable (that is, A1, ..., Am
→B is not provable for any ﬁnite subsequence A1, ..., Am of Γ). Otherwise,
we say that Γ is inconsistent.
Theorem 3.5.4
(Model existence theorem for G′) If a set Γ of propositions
is consistent then it is satisﬁable.
Proof : Assume Γ unsatisﬁable.
Hence, for every proposition B, the
sequent
Γ →B
is valid. By theorem 3.5.1, for every such B, there is a ﬁnite subsequence
A1, ..., Ap of Γ such that the sequent
A1, ..., Ap →B

3.5 Proof Theory for Inﬁnite Sequents: Extended Completeness of G′
95
is provable. But then, Γ is not consistent, contrary to the hypothesis.
The converse of theorem 3.5.4 is also true.
Lemma 3.5.4
(Consistency lemma for G′) If a set Γ of propositions is sat-
isﬁable then it is consistent.
Proof : Let v be a valuation such that
v |= A
for every proposition in Γ. Assume that Γ is inconsistent. Then,
Γ →B
is provable for every proposition B, and in particular, there is a ﬁnite subse-
quence A1, ..., Am of Γ such that
A1, ..., Am →P ∧¬P
is provable (for some propositional symbol P). By lemma 3.4.3, A1, ..., Am →
P ∧¬P is valid and, since the valuation v makes all propositions in Γ true, v
should make P ∧¬P true, which is impossible. Hence, Γ is consistent.
Note that if a set Γ of propositions is consistent, theorem 3.5.4 shows
that the sequent Γ →
is falsiﬁable. Hence, by theorem 3.5.1, a falsifying
valuation can be obtained (in fact, all falsifying valuations can be obtained
by considering all inﬁnite paths in the counter-example tree).
One may view the goal of procedure search as the construction of Hin-
tikka sets.
If this goal fails, the original sequent was valid and otherwise,
any Hintikka set yields a falsifying valuation. The decomposition of proposi-
tions into a-components or b-components is the basis of a variant of Gentzen
systems called the tableaux system. For details, see Smullyan, 1968.
3.5.8 Maximal Consistent Sets
We conclude this section by discussing brieﬂy the concept of maximal consis-
tent sets. This concept is important because it can be used to give another
proof of the completeness theorem (theorem 3.5.2).
Deﬁnition 3.5.10 A consistent set Γ of propositions is maximally consistent
(or a maximal consistent set) iﬀ, for every consistent set ∆, if Γ ⊆∆, then
Γ = ∆. Equivalently, every proper superset of Γ is inconsistent.
The importance of maximal consistent sets lies in the following lemma.
Lemma 3.5.5 Every consistent set Γ is a subset of some maximal consistent
set ∆.

96
3/Propositional Logic
Proof : If Γ is a consistent set, by theorem 3.5.4, it is satisﬁable. Let v
be a valuation satisfying Γ. Let ∆be the set
{A | v |= A}
of all propositions satisﬁed by v. Clearly, Γ is a subset of ∆. We claim that ∆
is a maximal consistent set. First, by lemma 3.5.4, ∆is consistent since it is
satisﬁed by v. It remains to show that it is maximally consistent. We proceed
by contradiction. Assume that there is a consistent set Λ such that ∆is a
proper subset of Λ. Since Λ is consistent, by theorem 3.5.4, it is satisﬁed by a
valuation v′. Since ∆is a proper subset of Λ, there is a proposition A which
is in Λ but not in ∆. Hence,
v ̸|= A,
since otherwise A would be in ∆. But then,
v |= ¬A,
and ¬A is in ∆. Since ∆is a subset of Λ, v′ satisﬁes every proposition in ∆,
and in particular
v′ |= ¬A.
But since v′ satisﬁes Λ, we also have
v′ |= A,
which is impossible. Hence, ∆is indeed maximally consistent.
The above lemma was shown using theorem 3.5.4, but it can be shown
more directly and without theorem 3.5.4.
Actually, theorem 3.5.4 can be
shown from lemma 3.5.5, and in turn, the completeness theorem can be shown
from theorem 3.5.4. Such an approach to the completeness theorem is more
traditional, but not as constructive, in the sense that it does not provide a
procedure for constructing a deduction tree.
There is also a close relationship between maximally consistent sets and
Hintikka sets.
Indeed, by reformulating Hintikka sets as unsigned sets of
propositions, it can be shown that every maximal consistent set is a Hintikka
set. However, the converse is not true. Hintikka sets are more general (and in
a sense more economical) than maximal consistent sets. For details, we refer
the reader to the problems.

PROBLEMS
97
PROBLEMS
3.5.1.
(i) Show that the inﬁnite sequent Γ →∆where
Γ =< P0, (P0 ⊃P1), (P1 ⊃P2), ..., (Pi ⊃Pi+1), ... >
and
∆=< (P1 ⊃Q) >
is falsiﬁable.
(ii) Prove that for every i > 0, the sequent Γ →∆′, where Γ is as
above and ∆′ =< (P0 ⊃Pi) > is provable.
3.5.2.
The cut rule is the following inference rule:
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
Let G′ + {cut} be the formal system obtained by adding the cut rule
to G′. The notion of a deduction tree is extended to allow the cut
rule as an inference. A proof in G′ is called a cut-free proof.
(i) Prove that for every valuation v, if v satisﬁes the premises of the
cut rule, then it satisﬁes its conclusion.
(ii) Prove that if a sequent is provable in the system G′ + {cut}, then
it is valid.
(iii) Prove that if a sequent is provable in G′ + {cut}, then it has a
cut-free proof.
3.5.3.
(i) Prove solely in terms of proofs in G′ +{cut} that a set Γ of propo-
sitions is inconsistent if and only if there is a proposition A such that
both Γ →A and Γ →¬A are provable in G′ +{cut}. (For inspiration
see Section 3.6.)
(ii) Prove solely in terms of proofs in G′ + {cut} that, Γ →A is not
provable in G′ + {cut} if and only if Γ ∪{¬A} is consistent. (For
inspiration see Section 3.6.)
Note: Properties (i) and (ii) also hold for the proof system G′, but
the author does not know of any proof not involving a proof-theoretic
version of Gentzen’s cut elimination theorem. The cut elimination
theorem states that any proof in the system G′ + {cut} can be trans-
formed to a proof in G′ (without cut). The completeness theorem for
G′ provides a semantic proof of the cut elimination theorem. How-
ever, in order to show (i) and (ii) without using semantic arguments,
it appears that one has to mimic Gentzen’s original proof. (See Szabo,
1969.)

98
3/Propositional Logic
∗3.5.4.
A set Γ of propositions is said to be complete if, for every proposition
A, either Γ →A or Γ →¬A is provable, but not both. Prove that
for any set Γ of propositions, the following are equivalent:
(i) The set
{A | ⊢Γ →A in G′}
is a maximal consistent set.
(ii) Γ is complete.
(iii) There is a single valuation v satisfying Γ.
(iv) There is a valuation v such that for all A,
Γ →A is provable (in G′) if and only if v |= A.
3.5.5.
Let Γ be a consistent set. Let A1, A2, ..., An,... be an enumeration
of all propositions in PROP. Deﬁne the sequence Γn inductively as
follows:
Γ0 = Γ,
Γn+1 =

Γn ∪{An+1}
if Γn ∪{An+1} is consistent;
Γn
otherwise.
Let
∆=

n≥0
Γn.
Prove the following:
(a) Each Γn is consistent.
(b) ∆is consistent.
(c) ∆is maximally consistent.
Note that this exercise provides another proof of lemma 3.5.5 for the
system G′ + {cut}, not using the completeness theorem.
3.5.6.
Prove that if a proposition A over the language using the logical con-
nectives {∨, ∧, ⊃, ¬} is a tautology, then A contains some occurrence
of either ¬ or ⊃.
∗3.5.7.
Given a proposition A, its immediate descendants A1 and A2 are
given by the following table:
Type-a formulae
A
A1
A2
(X ∧Y )
X
Y
¬(X ∨Y )
¬X
¬Y
¬(X ⊃Y )
X
¬Y
¬(¬X)
X
X

PROBLEMS
99
Type-b formulae
B
B1
B2
¬(X ∧Y )
¬X
¬Y
(X ∨Y )
X
Y
(X ⊃Y )
¬X
Y
Note that neither propositional letters nor negations of propositional
letters have immediate descendants.
Given a set S of propositions, let Des(S) be the set of immediate
descendants of propositions in S, and deﬁne Sn by induction as fol-
lows:
S0 = S;
Sn+1 = Des(Sn)
Let
S∗=

n≥0
Sn
be the union of all the Sn. Hintikka sets can also be deﬁned without
using signed formulae, in terms of immediate descendants:
A set S of propositions is a Hintikka set if the following conditions
hold:
(H1) No propositional letter and its negation are both in S.
(H2) If an a-formula A is in S then both A1 and A2 are in S.
(H3) If a b-formula B is in S then either B1 is in S or B2 is in S.
In this problem and some of the following problems, given any set S
of propositions and any propositions A1,...,An, the set S ∪{A1, ...An}
will also be denoted by {S, A1, ..., An}.
Assume that S is consistent.
(a) Using a modiﬁcation of the construction given in problem 3.5.5,
show that S can be extended to a maximal consistent subset U of S∗
(that is, to a consistent subset U of S∗containing S, such that U is
not a proper subset of any consistent subset of S∗).
(b) Prove that consistent sets satisfy the following properties:
C0: No set S containing a propositional letter and its negation is
consistent.
C1: If {S, A} is consistent, so is {S, A1, A2}, where A is a proposition
of type a.

100
3/Propositional Logic
C2: If {S, B} is consistent, then either {S, B1} or {S, B2} is consis-
tent, where B is a proposition of type b.
(c) Prove that U is a Hintikka set.
(d) Show that U is not necessarily a maximal consistent subset of
PROP, the set of all propositions.
∗3.5.8.
The purpose of this problem is to prove the compactness theorem
for propositional logic without using the completeness theorem. The
proof developed in the following questions is in a sense more construc-
tive than the proof given by using the completeness theorem, since if
we are given a set Γ such that every ﬁnite subset of Γ is satisﬁable,
we will actually construct a valuation that satisﬁes Γ. However, the
existence of ultraﬁlters requires Zorn’s Lemma, and so this proof is
not constructive in the recursion-theoretic sense.
For this problem, you may use the following result stated below and
known as Zorn’s lemma. For details, the reader should consult a text
on set Theory, such as Enderton, 1977; Suppes, 1972; or Kuratowski
and Mostowski, 1976.
We recall the following concepts from Subsection 2.1.9. A chain in a
poset (P, ≤) is a totally ordered subset of P. A chain C is bounded
if there exists an element b ∈P such that for all p ∈C, p ≤b. A
maximal element of P is some m ∈P such that for any m′ ∈P, if
m ≤m′ then m = m′.
Zorn’s lemma: Given a partially ordered set S, if every chain in S
is bounded, then S has a maximal element.
(1) Let E be a nonempty set, and F a class of subsets of E. We say
that F is a ﬁlter on E iﬀ:
1. E is in F;
2. if u and v are in F, then u ∩v is in F;
3. if u is in F and v is any subset of E, if u ⊆v, then v is also in F.
A ﬁlter F is a proper ﬁlter if ∅(the empty set) is not in F. A proper
ﬁlter F is maximal if, for any other proper ﬁlter D, if F is a subset
of D, then D = F.
A class C (even empty) of subsets of a nonempty set E has the ﬁnite
intersection property (f.i.p.) iﬀthe intersection of every ﬁnite number
of sets in C is nonempty. Let C be any class of subsets of a nonempty
set E. The ﬁlter generated by C is the intersection D of all ﬁlters over
E which include C.
Prove the following properties:
(i) The ﬁlter D generated by C is indeed a ﬁlter over E.

PROBLEMS
101
(ii) D is equal to the set of all subsets X of E such that either X = E,
or for some Y1, ..., Yn ∈C,
Y1 ∩... ∩Yn ⊆X.
(iii) D is a proper ﬁlter if and only if C has the ﬁnite intersection
property.
(2) A maximal proper ﬁlter is called an ultraﬁlter.
Prove that a nonempty collection U of sets with the ﬁnite intersection
property is an ultraﬁlter over E if and only if, for every subset X of
E,
X ∈U
if and only if
(E −X) /∈U.
Hint: Assume that (E −X) /∈U. Let D = U ∪{X}, and let F be
the ﬁlter generated by D (as in question 1). Show that F is a proper
ﬁlter including U. Hence, U = F and D is a subset of U, so that
X ∈U.
(3) Use Zorn’s lemma to show that if a class C of subsets of a nonempty
set E has the ﬁnite intersection property, then it is contained in some
ultraﬁlter.
Hint: Show that the union of a chain of proper ﬁlters is a proper ﬁlter
that bounds the chain.
∗3.5.9.
Let I be a nonempty set and V = {vi | i ∈I} be a set of valuations.
Let U be a proper ﬁlter on I. Deﬁne the valuation v such that for
each propositional symbol P ∈PS,
v(P) = T
iﬀ
{i | vi(P) = T} ∈U.
(Such a valuation v is called a reduced product).
(a) Show that
v(P) = F
iﬀ
{i | vi(P) = T} /∈U,
and
if
{i | vi(P) = T} = ∅
then
v(P) = F.
If U is an ultraﬁlter, show that for all propositions A,
v |= A
iﬀ
{i | vi |= A} ∈U.
Such a valuation v is called the ultraproduct of V with respect to U.
(b) Show that for any Horn formula A (see problem 3.3.18), whenever
U is a proper ﬁlter, if
{i | vi |= A} ∈U
then
v |= A.

102
3/Propositional Logic
As a consequence, show that for every Horn formula A,
if vi |= A for all i ∈I, then v |= A.
(c) Let I = {1, 2}. Give all the ﬁlters on I. Give an example showing
that there exists a proper ﬁlter U on {1, 2}, a set of valuations {v1, v2},
and a proposition A, such that v |= A, but {i | vi |= A} /∈U.
(d) Consider the proper ﬁlter U = {{1, 2}} on I = {1, 2}, and let
A = P1 ∨P2. Find two valuations v1 and v2 such that v1 |= A and
v2 |= A, but the reduced product v of v1 and v2 with respect to U
does not satisfy A. Conclude that not every proposition is logically
equivalent to a Horn formula.
∗3.5.10. (a) Let Γ be a set of propositions such that every ﬁnite subset of Γ
is satisﬁable. Let I be the set of all ﬁnite subsets of Γ, and for each
i ∈I, let vi be a valuation satisfying i. For each proposition A ∈Γ,
let
A∗= {i ∈I | A ∈i}.
Let
C = {A∗| A ∈Γ}.
Note that C has the ﬁnite intersection property since
{A1, ..., An} ∈A∗
1 ∩... ∩A∗
n.
By problem 3.5.8, let U be an ultraﬁlter including C, so that every
A∗is in U. If i ∈A∗, then A ∈i, and so
vi |= A.
Thus, for every A in Γ, A∗is a subset of {i ∈I | vi |= A}.
Show that each set {i ∈I | vi |= A} is in U.
(b) Show that the ultraproduct v (deﬁned in problem 3.5.9) of the set
of valuations {vi | i ∈I} with respect to the ultraﬁlter U satisﬁes Γ.
∗3.5.11. Recall the deﬁnition of a Horn formula given in problem 3.3.18. Given
a countable set {vi | i ≥0} of truth assignments, the product v of
{vi | i ≥0} is the truth assignment such that for every propositional
symbol Pj,
v(Pj) =
	 T
if vi(Pj) = T, for all vi,
F
otherwise.
(a) Show that if X is a set of propositional Horn formulae and every
truth assignment in {vi | i ≥0} satisﬁes X, then the product v
satisﬁes X.

PROBLEMS
103
(b) Let
X∗= {¬P | P is atomic and X ̸⊢P}.
Show that if X is a consistent set of basic Horn formulas, then X ∪X∗
is consistent.
Hint: Using question (a), show that there is a truth assignment v
satisfying X ∪X∗.
∗3.5.12. In this problem, we are using the deﬁnitions given in problem 3.5.7.
Given a set S, a property P about subsets of S (P is a subset of 2S)
is a property of ﬁnite character iﬀthe following hold:
Given any subset U of S, P holds for U if and only if P holds for all
ﬁnite subsets of U.
A property P about sets of propositions is a consistency property if
P is of ﬁnite character and the following hold:
C0: No set S containing a propositional letter and its negation satis-
ﬁes P.
C1: If {S, A} satisﬁes P, so does {S, A1, A2}, where A is a proposition
of type a.
C2: If {S, B} satisﬁes P, then either {S, B1} or {S, B2} satisﬁes P,
where B is a proposition of type b.
(a) Using Zorn’s lemma (see problem 3.5.7), show that for any set S,
for any property P about subsets of S, if P is of ﬁnite character, then
any subset U of S for which P holds is a subset of some maximal
subset of S for which P holds.
(b) Prove that if P is a consistency property and P satisﬁes a set U
of propositions, then U can be extended to a Hintikka set.
Hint: Use the technique described in problem 3.5.7.
(c) Prove that if P is a consistency property and P satisﬁes a set U
of propositions, then U is satisﬁable.
∗3.5.13. Using the deﬁnitions given in problem 3.5.7, show that a maximal
consistent set S is a Hintikka set satisfying the additional property:
M0 : For every proposition A,
A ∈S
if and only if
¬A /∈S.
∗3.5.14. In this problem, we also use the deﬁnitions of problem 3.5.7. A set S
of propositions is downward closed iﬀthe following conditions hold:
D1: For every proposition A of type a, if A ∈S, then both A1 and
A2 are in S.
D2: For every proposition B of type b, if B ∈S, then either B1 is in
S or B2 is in S.

104
3/Propositional Logic
A set S of propositions is upward closed iﬀ:
U1: For every proposition A of type a, if A1 and A2 are both in S,
then A is in S.
U2: For every proposition B of type b, if either B1 is in S or B2 is in
S, then B is in S.
(a) Prove that any downward closed set satisfying condition M0 (given
in problem 3.5.13) is a maximal consistent set.
(b) Prove that any upward closed set satisfying condition M0 is a
maximal consistent set.
Note: Conditions D1 and D2 are conditions H2 and H3 for Hin-
tikka sets. Furthermore, U1 and U2 state the converse of D1 and D2.
Hence, the above problem shows that a maximal consistent set is a
set satisfying condition M0 and the “if and only if” version of H2 and
H3. Consequently, this reproves that a maximal consistent set is a
Hintikka set.
In the next problems, some connections between logic and the theory
of boolean algebras are explored.
∗3.5.15. Recall from Section 3.3 that a boolean algebra is a structure A =<
A, +, ∗, ¬, 0, 1 >, where A is a nonempty set, + and ∗are binary
functions on A, ¬ is a unary function on A, and 0 and 1 are distinct
elements of A, such that the following axioms hold:
Associativity rules:
((A + B) + C) = (A + (B + C))
((A ∗B) ∗C) = (A ∗(B ∗C))
Commutativity rules:
(A + B) = (B + A)
(A ∗B) = (B ∗A)
Distributivity rules:
(A + (B ∗C)) = ((A + B) ∗(A + C))
(A ∗(B + C)) = ((A ∗B) + (A ∗C))
De Morgan’s rules:
¬(A + B) = (¬A ∗¬B)
¬(A ∗B) = (¬A + ¬B)
Idempotency rules:
(A + A) = A
(A ∗A) = A
Double negation rule:
¬¬A = A
Absorption rules:
(A + (A ∗B)) = A
(A ∗(A + B)) = A
Laws of zero and one:

PROBLEMS
105
(A + 0) = A
(A ∗0) = 0
(A + 1) = 1
(A ∗1) = A
(A + ¬A) = 1
(A ∗¬A) = 0
When dealing with boolean algebras, ¬A is also denoted as A. Given
a boolean algebra A, a partial ordering ≤is deﬁned on A as follows:
a ≤b
if and only if
a + b = b.
A ﬁlter D is a subset of A such that D is nonempty, for all x, y ∈D,
x ∗y ∈D, and for all z ∈A and x ∈D, if x ≤z, then z ∈D. A
proper ﬁlter is a ﬁlter such that 0 /∈D (equivalently, D ̸= A). (Note
that this is a generalization of the notion deﬁned in problem 3.5.8.)
An ideal is a nonempty subset I of A such that, for all x, y ∈I,
x + y ∈I, and for all z ∈A and x ∈I, x ∗z ∈I.
(a) Show that for any (proper) ﬁlter D, the set
{x | x ∈D}
is an ideal.
Given a ﬁlter D, the relation (D) deﬁned by
x(D)y
if and only if
x ∗y + x ∗y ∈D.
is a congruence relation on A. The set of equivalences classes modulo
(D) is a boolean algebra denoted by A/D, whose 1 is D, and whose
0 is {x | x ∈D}.
(b) Prove that x(D)y if and only if there is some z ∈D such that
x ∗z = y ∗z,
if and only if
x ∗y + x ∗y ∈{x | x ∈D},
if and only if
(x + y) ∗(y + x) ∈D.
Note: Intuitively speaking, (x + y) corresponds to the proposition
(x ⊃y) and (x + y) ∗(y + x) to (x ≡y).
A subset E of A has the ﬁnite intersection property iﬀfor any ﬁnite
number of elements x1, ..., xn ∈E,
x1 ∗... ∗xn ̸= 0.

106
3/Propositional Logic
(c) Prove that every subset E with the ﬁnite intersection property is
contained in a smallest proper ﬁlter. (See problem 3.5.8.)
A ﬁlter D is principal iﬀfor some a ̸= 0 in A, x ∈D if and only if
a ≤x. A proper ﬁlter is an ultraﬁlter iﬀit is maximal.
(d) Prove that any set with the ﬁnite intersection property can be
extended to an ultraﬁlter.
(e) If D is an ultraﬁlter, then
x + y ∈D
iﬀ
either x ∈D or y ∈D,
x ∈D
iﬀ
x /∈D.
Prove that D is an ultraﬁlter if and only if the quotient boolean alge-
bra A/D is isomorphic to the two-element boolean algebra BOOL.
∗3.5.16. Let ≃be the proof-theoretic version of the equivalence relation on
PROP deﬁned in problem 3.4.6, so that for any two propositions A
and B,
A ≃B if and only if ⊢(A ≡B) in G′.
(a) Show that the set B0 of equivalence classes modulo ≃is a boolean
algebra if we deﬁne the operations +, ∗and – on B0 as follows:
[A] + [B] = [A ∨B],
[A] ∗[B] = [A ∧B],
[A] = [¬A].
Also, let 0 = [⊥] and 1 = [⊤]. Observe that 0 ̸= 1. The algebra B0 is
called the Lindenbaum algebra of PROP.
Hint: Use problems 3.4.6 and 3.4.7.
(b) Prove that the following statements are equivalent:
(1) Every consistent set can be extended to a maximal consistent set.
(2) Every ﬁlter on B0 can be extended to an ultraﬁlter.
∗3.5.17. Let T be any subset of propositions in PROP. We say that T is
ﬁnitely axiomatizable if there is a ﬁnite set S of propositions such
that for every proposition A in PROP,
⊢T →A in G′ + {cut}
if and only if
⊢S →A in G′ + {cut}.
Let
DT = {[A] | ⊢T →A in G′ + {cut}},

PROBLEMS
107
where [A] denotes the equivalence class of A modulo ≃. Prove the
following statements:
(i) T is consistent iﬀDT is a proper ﬁlter on B0.
(ii) T is consistent and ﬁnitely axiomatizable iﬀDT is a principal
ﬁlter on B0.
(iii) T is complete iﬀDT is an ultraﬁlter on B0. (For the deﬁnition
of a complete set of propositions, See problem 3.5.4).
(iv) T is complete and ﬁnitely axiomatizable iﬀDT is a principal
ultraﬁlter on B0.
Given a subset D of B0, let
TD = {A ∈PROP | [A] ∈D}.
Show that the converses of (i) to (iv) each hold, with T replaced by
TD and DT by D.
Say that a set T of propositions is closed if, for every A ∈PROP,
⊢T →A implies that A ∈T. Show that there is a one-to-one cor-
respondence between complete closed extensions of T and ultraﬁlters
in B0/DT .
Note: In this problem the cut rule seems necessary to prove that DT
is a ﬁlter, speciﬁcally, that if ⊢T →A and ⊢T →(A ⊃B) in
G′ + {cut}, then ⊢T →B in G′ + {cut}. To prove this in G′, a form
of Gentzen’s cut elimination theorem seems necessary.
∗3.5.18. (a) Let A1 and A2 be two boolean algebras. A function h : A1 →A2
is a homomorphism if, for all x, y ∈A1,
h(x ∨1 y) = h(x) ∨2 h(y),
h(x ∧1 y) = h(x) ∧2 h(y) and
h(x) = h(x).
Show that h(0) = 0 and h(1) = 1.
(b) Given a boolean algebra A and a proper ﬁlter D, show that the
mapping hD : A →A/D that maps every element a of A to its
equivalence class [a] modulo (D) is a homomorphism.
(c) Let T be a consistent set of propositions. The equivalence relation
≃T on PROP is deﬁned as follows:
A ≃T B
if and only if
⊢T →(A ≡B) in G′ + {cut}.
Show that the set BT of equivalence classes modulo ≃T is a boolean
algebra if we deﬁne the operations +, ∗and – on BT as follows:

108
3/Propositional Logic
[A]T + [B]T = [A ∨B]T ,
[A]T ∗[B]T = [A ∧B]T ,
[A]T = [¬A]T .
([A]T denotes the equivalence class of A modulo ≃T .) Furthermore,
the element 1 is the equivalence class
{A | ⊢T →A in G′ + {cut}},
and the element 0 is the class
{A | ⊢T →¬A in G′ + {cut}}.
The boolean algebra BT is called the Lindenbaum algebra of T. Note
that the equivalence class [A]T of A modulo ≃T is the set
{B | ⊢T →(A ≡B) in G′ + {cut}}.
For any homomorphism h : BT →BOOL, let v : PS →BOOL be
deﬁned such that for every propositional letter P,
v(P) = h([P]T ).
Show that v is a valuation satisfying T such that v(A) = h([A]T ) for
all A ∈PROP.
(d) There is a correspondence between valuations satisfying T and
ultraﬁlters U in BT deﬁned as follows: For every ultraﬁlter U in
BT , the quotient algebra BT /U is isomorphic to the boolean algebra
BOOL (see problem 3.5.15(e)). By questions 3.5.18(a) and 3.5.18(b),
there is a valuation vU satisfying T induced by the homomorphism
from BT to BT /U. Conversely, if v is a valuation satisfying T, show
that
Uv = {[A]T | v(A) = T}
is an ultraﬁlter in BT .
(e) Prove the extended completeness theorem for G′ + {cut}.
Hint: Assume that T →A is valid, but that A is not provable from T.
Then, in the Lindenbaum algebra BT , [A]T ̸= 1, and so [¬A]T ̸= 0.
Using problem 3.5.15(d), there is an ultraﬁlter U in BT containing
[¬A]T . Since BT /U is isomorphic to BOOL, by questions 3.5.18(c)
and 3.5.18(d), there is a valuation v satisfying T such that
v(¬A) = h([¬A]T ),

3.6 More on Gentzen Systems: The Cut Rule
109
where h is the homormophism from BT to BT /U. Since [¬A]T is in
U,
h([¬A]T ) = T.
Hence, there is a valuation satisfying T such that v(A) = F. This
contradicts the validity of T →A.
3.5.19. Write a computer program (preferably in PASCAL or C) implement-
ing the extended search procedure of deﬁnition 3.5.1.
3.6 More on Gentzen Systems: The Cut Rule
The rules of the Gentzen system G′ given in deﬁnition 3.4.2 were chosen so
as to give ourselves as few choices as possible at each step upward in search-
ing systematically for a falsifying valuation. The use of other Gentzen-type
systems may aﬀord simpler proofs, especially working downward. One such
system is the system LK′ due to Gentzen. The system LK′ contains a rule
called the cut rule, which is important from a historical point of view, but
also from a mathematical point of view. Indeed, even though it is possible to
ﬁnd complete proof systems not using the cut rule, we will discover some un-
expected complications when we study ﬁrst-order logic with equality. Indeed,
the system for ﬁrst-order logic with equality not using the cut rule is not very
natural, and the cut rule cannot be dispensed with easily.
3.6.1 Using Auxiliary Lemmas in Proofs
There are also “pragmatic” reasons for considering the cut rule. The cut rule
is the following:
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
Notice that this rule formalizes the technique constantly used in practice
to use an auxiliary lemma in a proof. This is more easily visualized if we
assume that ∆is empty. Then, Γ →A is the auxiliary lemma, which can
be assumed to belong to a catalogue of already-proven results. Now, using A
as an assumption, if we can show that using other assumptions Λ, that Θ is
provable, we can conclude that Γ, Λ →Θ is provable. The conclusion does
not refer to A.
One might say that a proof using the cut rule is not as “direct” and
consequently, not as perspicuous as a proof not using the cut rule. On the
other hand, if we already have a vast catalogue of known results, and we can
use them to give short and “easy” proofs of other results, why force ourselves

110
3/Propositional Logic
not to use the convenience aﬀorded by the cut rule? We shall not try to answer
these questions of a philosophical nature. Let us just make a few remarks.
Let us call a proof not using the cut rule a cut-free proof. Cut-free proofs
are important in investigations regarding consistency results. The object of
such investigations is to establish constructively the consistency of mathe-
matical theories such as arithmetic or set theory, the ultimate goal being to
show that mathematics formalized as a logical theory is free of contradictions.
First-order logic is simple enough that Gentzen’s cut elimination theorem
holds constructively. This means that for every proof using the cut rule, an-
other proof not using the cut rule can eﬀectively be constructed. We shall
give a (nonconstructive) semantic proof of this result in this chapter, and a
constructive proof for a simpler system in Chapter 6. From a mathematical
point of view, this shows that the cut rule can be dispensed with. For richer
logics, such as second-order logic, the cut-elimination theorem also holds, but
not constructively, in the sense that the argument showing that there is a
method for converting a proof with cut to a proof without cut is not eﬀective.
Another interesting issue is to examine the relative complexity of proofs
with or without cut.
Proofs with cuts can be much shorter than cut-free
proofs. This will be shown in Chapter 6. However, from the point of view
of automatic theorem proving, cut-free proofs are easier to ﬁnd. For more on
cut-free proofs and the cut rule, the reader is referred to Takeuti, 1975, and
Pfenning’s paper in Shostack, 1984a.
We now present the system LK′.
3.6.2 The Gentzen System LK′
The system LK′ consists of structural rules, the cut rule, and of logical rules.
Deﬁnition 3.6.1 Gentzen system LK′. The letters Γ,∆,Λ,Θ stand for arbi-
trary (possibly empty) sequences of propositions and A,B for arbitrary propo-
sitions.
(1) Structural rules:
(i) Weakening:
Γ →∆
A, Γ →∆(left)
Γ →∆
Γ →∆, A (right)
A is called the weakening formula.
(ii) Contraction:
A, A, Γ →∆
A, Γ →∆
(left)
Γ →∆, A, A
Γ →∆, A
(right)

3.6 More on Gentzen Systems: The Cut Rule
111
(iii) Exchange:
Γ, A, B, ∆→Λ
Γ, B, A, ∆→Λ (left)
Γ →∆, A, B, Λ
Γ →∆, B, A, Λ (right)
(2) Cut rule:
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
(3) Logical rules:
A, Γ →∆
A ∧B, Γ →∆(∧: left)
and
B, Γ →∆
A ∧B, Γ →∆(∧: left)
Γ →∆, A
Γ →∆, B
Γ →∆, A ∧B
(∧: right)
A, Γ →∆
B, Γ →∆
A ∨B, Γ →∆
(∨: left)
Γ →∆, A
Γ →∆, A ∨B (∨: right)
and
Γ →∆, B
Γ →∆, A ∨B (∨: right)
Γ →∆, A
B, Λ →Θ
A ⊃B, Γ, Λ →∆, Θ
(⊃: left)
A, Γ →∆, B
Γ →∆, A ⊃B (⊃: right)
Γ →∆, A
¬A, Γ →∆(¬ : left)
A, Γ →∆
Γ →∆, ¬A (¬ : right)
In the rules above, the propositions A ∨B, A ∧B, A ⊃B and ¬A are
called the principal formulae and the propositions A, B the side formulae.
The axioms of the system LK′ are all sequents of the form
A →A.
Note that in view of the exchange rule, the order of propositions in
a sequent is really irrelevant, and the system LK′ could be deﬁned using
multisets as deﬁned in problem 2.1.8.
Proof trees are deﬁned inductively as in deﬁnition 3.4.5, but with the
rules of the system LK′ given in deﬁnition 3.6.1. If a sequent has a proof
in the system G′ we say that it is G′-provable and similarly, if it is provable
in the system LK′, we say that it is LK′-provable. The system obtained by
removing the cut rule from LK′ will be denoted as LK′ −{cut}. We also say
that a sequent is LK′-provable without a cut if it has a proof tree using the

112
3/Propositional Logic
rules of the system LK′ −{cut}. We now show that the systems G′ and LK′
are logically equivalent. We will in fact prove a stronger result, namely that
G′, LK′ −{cut} and LK′ are equivalent. First, we show that the system LK′
is sound.
Lemma 3.6.1
(Soundness of LK′) Every axiom of LK′ is valid. For every
rule of LK′, for every valuation v, if v makes all the premises of a rule true
then v makes the conclusion of the rule true. Every LK′-provable sequent is
valid.
Proof : The proof uses the induction principle for proofs and is straight-
forward.
Note that lemma 3.6.1 diﬀers from lemma 3.4.3 in the following point:
It is not true that if v makes the conclusion of a rule true then v makes all
premises of that rule true. This reveals a remarkable property of the system
G′. The system G′ is a “two way” system, in the sense that the rules can be
used either from top-down or from bottom-up. However, LK′ is a top-down
system. In order to ensure that the inferences are sound, the rules must be
used from top-down.
3.6.3 Logical Equivalence of G′, LK′, and LK′ −{cut}
The following theorem yields a semantic version of the cut elimination theo-
rem.
Theorem 3.6.1
Logical equivalence of G′, LK′, and LK′ −{cut}. There is
an algorithm to convert any LK′-proof of a sequent Γ →∆into a G′-proof.
There is an algorithm to convert any G′-proof of a sequent Γ →∆into a
proof using the rules of LK′ −{cut}.
Proof : If Γ →∆has an LK′-proof, by lemma 3.6.1, Γ →∆is valid.
By theorem 3.5.2, Γ →∆has a G′-proof given by the algorithm search.
Note that if Γ →∆is inﬁnite, then the search procedure gives a proof for a
ﬁnite subsequent of Γ →∆, but by deﬁnition 3.6.2, it is a proof of Γ →∆.
Conversely, using the induction principle for G′-proofs we show that every
G′-proof can be converted to an (LK′ −{cut})-proof. This argument also
applies to inﬁnite sequents, since a proof of an inﬁnite sequent is in fact a
proof of some ﬁnite subsequent of it.
First, every G′-axiom Γ →∆contains some common proposition A, and
by application of the weakening and the exchange rules, an (LK′ −{cut})-
proof of Γ →∆can be obtained from the axiom A →A. Next, we have
to show that every application of a G′-rule can be replaced by a sequence of
(LK′ −{cut})-rules. There are eight cases to consider. Note that the G′-rules
∧: right, ∨: left, ⊃: right, ⊃: left, ¬ : right and ¬ : left can easily be
simulated in LK′ −{cut} using the exchange, contraction, and corresponding
(LK′ −{cut})-rules. We show how the G′-rule ∧: left can be transformed to

3.6 More on Gentzen Systems: The Cut Rule
113
a sequence of (LK′ −{cut})-rules, leaving the transformation of the G′-rule
∨: right as an exercise. The following is an (LK′ −{cut})-derivation from
Γ, A, B, ∆→Λ to Γ, A ∧B, ∆→Λ.
Γ, A, B, ∆→Λ
(several exchanges)
A, B, Γ, ∆→Λ
(∧: left (A))
A ∧B, B, Γ, ∆→Λ
(exchange)
B, A ∧B, Γ, ∆→Λ
(∧: left (B))
A ∧B, A ∧B, Γ, ∆→Λ
(contraction)
A ∧B, Γ, ∆→Λ
(several exchanges)
Γ, A ∧B, ∆→Λ
3.6.4 Gentzen’s Hauptsatz for LK′ (Cut elimination the-
orem for LK′)
Theorem 3.6.1 has the following important corollary.
Corollary
(Gentzen’s Hauptsatz for LK′) A sequent is LK′-provable if and
only if it is LK′-provable without a cut.
Note that the search procedure together with the above procedure pro-
vides an algorithm to construct a cut-free LK′-proof from an LK′-proof with
cut. Gentzen proved the above result by a very diﬀerent method in which
an LK′-proof is (recursively) transformed into an LK′-proof without cut.
Gentzen’s proof is more structural and syntactical than ours, since we com-
pletely forget about the LK′-proof and start from scratch using the procedure
search. Also, Gentzen’s proof generalizes to the ﬁrst-order predicate calculus
LK, providing an algorithm for transforming any LK-proof with cut to an LK-
proof without cut. The search procedure will also provide a cut-free proof,
but the argument used in justifying the correctness of the search procedure
is not constructive. The nonconstructive step arises when we show that the
search procedure terminates for a valid sequent. Gentzen’s proof is diﬃcult
and can be found in either Takeuti, 1975; Kleene, 1952; or in Gentzen’s origi-
nal paper in Szabo, 1969. A constructive proof for a simpler system (sequents
of formulae in NNF) will also be given in Chapter 6.

114
3/Propositional Logic
3.6.5 Characterization of Consistency in LK′
The following lemma gives a characterization of consistency in the system
LK′.
Lemma 3.6.2 (1) A set Γ of propositions is inconsistent if and only if there
is some proposition A such that both Γ →A and Γ →¬A are LK′-provable.
(2) For any proposition A, the sequent Γ →A is not LK′-provable if
and only if Γ ∪{¬A} is consistent.
Proof : In this proof, we will abbreviate LK′-provable as provable.
(1) If Γ is inconsistent then Γ →B is provable for any proposition B,
showing that the second half of (1) holds. Conversely, assume that for some A,
both ⊢Γ →A and ⊢Γ →¬A in LK′, with proofs T1 and T2. The following
is a proof of Γ →B for any given B.
T1
Γ →A
(¬ : left)
¬A, Γ →
(¬ : right)
Γ →¬¬A
T2
Γ →¬A
(¬ : left)
¬¬A, Γ →
(weakening)
¬¬A, Γ →B
(cut (¬¬A))
Γ, Γ →B
(contractions and exchanges)
Γ →B
(2) Assume that Γ →A is not provable. If Γ ∪{¬A} was inconsistent,
then ¬A, Γ →A would be provable with proof T. The following is a proof of
Γ →A, contradicting the hypothesis.
T
¬A, Γ →A
(¬ : right)
Γ →A, ¬¬A
A →A
(¬ : right)
→A, ¬A
(¬ : left)
¬¬A →A
(cut (¬¬A))
Γ →A, A
(contraction)
Γ →A
Conversely, assume that Γ ∪{¬A} is consistent. If Γ →A is provable, a
fortiori Γ, ¬A →A is provable. But ¬A →¬A is also provable since it is an
axiom, and so Γ, ¬A →¬A is provable. By (1), Γ ∪{¬A} is inconsistent.
Remark: Recall that for an inﬁnite set of propositions Γ, Γ →A is
provable if ∆→A is provable for a ﬁnite subsequence ∆of Γ. Hence, the
above proofs should really be modiﬁed to refer to ﬁnite subsequences of Γ.
Using the exchange, weakening and contraction rules, we can ensure that the
antecedent in the conclusion of each proof is a subsequence of Γ. We leave
the details as an exercise.
Also, note that the above characterizations of

PROBLEMS
115
consistency (or inconsistency) in LK′ are purely syntactic (proof theoretic),
and that the cut rule was used in a crucial way.
PROBLEMS
3.6.1.
Give LK′-proof trees for the following tautologies:
A ⊃(B ⊃A)
(A ⊃B) ⊃((A ⊃(B ⊃C)) ⊃(A ⊃C))
A ⊃(B ⊃(A ∧B))
A ⊃(A ∨B)
B ⊃(A ∨B)
(A ⊃B) ⊃((A ⊃¬B) ⊃¬A)
(A ∧B) ⊃A
(A ∧B) ⊃B
(A ⊃C) ⊃((B ⊃C) ⊃((A ∨B) ⊃C))
¬¬A ⊃A
3.6.2.
Show that the cut rule is not a two-way rule, that is, if a valuation v
satisﬁes the conclusion of the cut rule, it does not necessarily satisfy
its premises. Find the other rules of LK′ that are not two-way rules.
∗3.6.3.
Recall that a set Γ of propositions is maximally consistent if Γ is
consistent and for any other set ∆, if Γ is a proper subset of ∆then
∆is inconsistent.
(a) Show that if Γ is maximally consistent, for any proposition A such
that Γ →A is provable in LK′ (with cut), A is in Γ.
(b) Show that
∆= {A | ⊢Γ →A in LK′ (with cut)}
is maximally consistent if and only if for every proposition A, either
⊢Γ →A or ⊢Γ →¬A in LK′, but not both.
(c) Show that Γ is maximally consistent iﬀthere is a single valuation
v satisfying Γ.
(d) Show that Γ is maximally consistent iﬀthere is a valuation v such
that v |= A if and only if A is in Γ.
3.6.4.
Using the technique of problem 3.5.5, prove in LK′ (+{cut}) that
every consistent set can be extended to a maximal consistent set.
∗3.6.5.
In this problem, we are adopting the deﬁnition of a Hintikka set given
in problem 3.5.6. Let ∆be a maximally consistent set. To cut down
on the number of cases, in this problem, assume that (A ⊃B) is an
abbreviation for (¬A ∨B), so that the set of connectives is {∧, ∨, ¬}.

116
3/Propositional Logic
(a) Show that ∆is a Hintikka set.
(b) Recall that in LK′, Γ →A is not provable if and only if Γ∪{¬A}
is consistent. Using problem 3.6.4, prove that if Γ →A is valid, then
it is provable in LK′ (with cut).
Remark: This provides another proof of the completeness of LK′
(with cut). Note that a proof tree for Γ →A is not produced (compare
with theorem 3.4.1).
3.6.6.
Prove that the extended completeness theorem and the model exis-
tence theorem are equivalent for LK′. (This is also true for LK′ −
{cut}, but apparently requires the cut elimination theorem).
3.6.7.
Implement a computer program (preferably in PASCAL or C) con-
verting an LK′-proof into a cut-free LK′-proof. Compare and inves-
tigate the relative lengths of proofs.
Notes and Suggestions for Further Reading
We have chosen Gentzen systems as the main vehicle for presenting proposi-
tional logic because of their algorithmic nature and their conceptual simplicity.
Our treatment is inspired from Kleene, 1967 and Kleene, 1952. For more on
Gentzen systems, the reader should consult Takeuti, 1975; Szabo, 1969; or
Smullyan, 1968.
We believe that the use of Hintikka sets improves the clarity of the proof
of the completeness theorem. For more details on Hintikka sets and related
concepts such as consistency properties, the reader is referred to Smullyan,
1968.
There are other proof systems for propositional logic. The Hilbert sys-
tem H discussed in problems 3.4.9 to 3.4.12 is from Kleene, 1967, as well as
the natural deduction system used in problems 3.4.11 and 3.4.12. For more on
natural deduction systems, the reader is referred to Van Dalen, 1980; Prawitz
1965; or Szabo, 1969. A variant of Gentzen systems called tableaux systems
is discussed at length in Smullyan, 1968.
The relationship between boolean algebra and logic was investigated by
Tarski, Lindenbaum, Rasiowa, and Sikorski. For more details, the reader is
referred to Chang and Keisler, 1973, or Bell and Slomson, 1974. Exercise
3.5.18 is adapted from Bell and Slomson, 1974.
The proof of Gentzen’s cut elimination theorem can be found in Kleene,
1952; Takeuti, 1975; and Szabo, 1969.

Chapter 4
Resolution In
Propositional Logic
4.1 Introduction
In Chapter 3, a procedure for showing whether or not a given proposition is
valid was given. This procedure, which uses a Gentzen system, yields a formal
proof in the Gentzen system when the input proposition is valid. In this chap-
ter, another method for deciding whether a proposition is valid is presented.
This method due to J. Robinson (Robinson, 1965) and called resolution, has
become quite popular in automatic theorem proving, because it is simple to
implement. The essence of the method is to prove the validity of a proposition
by establishing that the negation of this proposition is unsatisﬁable.
The main attractive feature of the resolution method is that it has a
single inference rule (the resolution rule). However, there is a price to pay:
The resolution method applies to a proposition in conjunctive normal form.
In this chapter, the resolution method will be presented as a variant of a
special kind of Gentzen-like system. A similar approach is followed in Robin-
son, 1969, but the technical details diﬀer. We shall justify the completeness
of the resolution method by showing that Gentzen proofs can be recursively
transformed into resolution proofs, and conversely. Such transformations are
interesting not only because they show constructively the equivalence of the
two proof systems, but because they also show a relationship between the
complexity of Gentzen proofs and resolution refutation proofs. Indeed, it will
be shown that every proof tree T in the system GCNF′ can be converted to
117

118
4/Resolution In Propositional Logic
a resolution refutation D whose number of steps is at most the number of
leaves of the proof tree T.
In the original edition of this book, it was claimed that the number of
axioms in a Gentzen proof tree was linearly related to the number of resolution
steps in the corresponding input resolution DAG, but the proof was wrong.
The essence of the connection between Gentzen proofs and resolution
proofs is the following: Given a proposition A, if B is a conjunctive normal
form of ¬A, A is valid iﬀB is unsatisﬁable, iﬀthe sequent B →is valid.
We will show how a Gentzen proof tree for B →can be transformed
into a resolution DAG for B (and conversely). The ﬁrst step is to design an
eﬃcient Gentzen system for proving sequents of the form B →, where B is
in conjunctive normal form.
4.2 A Special Gentzen System
The system GCNF ′ is obtained by restricting the system G′ to sequents of
the form A →, where A is a proposition in conjunctive normal form.
4.2.1 Deﬁnition of the System GCNF ′
Recall from deﬁnition 3.4.7 that a proposition A is in conjunctive normal
form iﬀit is a conjunction C1 ∧... ∧Cm of disjunctions Ci = Bi,1 ∨... ∨Bi,ni,
where each Bi,j is either a propositional symbol P or the negation ¬P of a
propositional symbol.
Deﬁnition 4.2.1
A literal is either a propositional symbol or its negation.
Given a literal L, its conjugate L is deﬁned as follows: If L = P then L = ¬P,
and if L = ¬P, then L = P. A proposition of the form
Ci = Bi,1 ∨... ∨Bi,ni,
where each Bi,j is a literal is called a clause.
By lemma 3.3.6, since both ∧and ∨are commutative, associative and
idempotent, it can be assumed that the clauses Ci are distinct, and each clause
can be viewed as the set of its distinct literals. Also, since the semantics of
sequents implies that a sequent
C1 ∧... ∧Cn →
is valid iﬀ
the sequent C1, ..., Cn →is valid,
it will be assumed in this section that we are dealing with sequents of the
form C1, ..., Cn →, where {C1, ..., Cn} is a set of clauses, and each clause
Ci is a set of literals. We will also view a sequent C1, ..., Cn →
as the set

4.2 A Special Gentzen System
119
of clauses {C1, ..., Cn}. For simplicity, the clause {L} consisting of a single
literal will often be denoted by L. If Γ and ∆denote sets of propositions, then
Γ, ∆denotes the set Γ ∪∆. Similarly, if Γ is a set of propositions and A is a
proposition, then Γ, A denotes the set Γ∪{A}. Consequently, Γ, A1, ..., Am →
denotes the sequent Γ ∪{A1, ..., Am} →.
It should be noted that when a sequent C1, ..., Cn →is viewed as the
set of clauses {C1, ..., Cn}, the comma is interpreted as the connective and
(∧), but that when we consider a single clause Ci = {L1, ..., Lm} as a set of
literals, the comma is interpreted as the connective or (∨).
For sequents of the above form, note that only the ∨: left rule and the
¬ : left rule are applicable. Indeed, since each proposition in each Ci is a
literal, for every application of the ¬ : left rule resulting in a literal of the
form ¬P in the antecedent of the conclusion of the rule, the propositional
letter P belongs to the right-hand side of the sequent which is the premise of
the rule. Since the ∨: left rule does not add propositions to the right-hand
side of sequents, only propositional letters can appear on the right-hand side
of sequents. Hence, only the ∨: left rule and the ¬ : left rule are applicable
to sequents of the form deﬁned above. But then, by redeﬁning the axioms to
be sequents of the form
Γ, P, ¬P →,
we obtain a proof system in which the only inference rule is ∨: left.
A further improvement is achieved by allowing several ∨: left rules to
be performed in a single step. If a sequent of the form
Γ, (A1 ∨B), ..., (Am ∨B) →
is provable, the proof may contain m distinct applications of the ∨: left rule
to (A1 ∨B),...,(Am ∨B), and the proof may contain 2m −1 copies of the proof
tree for the sequent Γ, B →. We can avoid such redundancies if we introduce
a rule of the form
Γ, A1, ..., Am →
Γ, B →
Γ, (A1 ∨B), ..., (Am ∨B) →
We obtain a proof system in which proofs are even more economical if
we introduce a rule of the form
Γ1, C1, ..., Ck →
Γ2, B →
Γ, (A1 ∨B), ..., (Am ∨B) →
where Γ1 and Γ2 are arbitrary subsets of Γ ∪{(A1 ∨B), ..., (Am ∨B)} (not
necessarily disjoint), and {C1, ..., Ck} is any nonempty subset of {A1,...,Am}.
In this fashion, we obtain a system similar to LK′ with implicit weakenings,

120
4/Resolution In Propositional Logic
in which every nontrivial sequent (see deﬁnition below) has a proof in which
the axioms are of the form P, ¬P →.
Deﬁnition 4.2.2
A sequent of the form C1, ..., Cn →is trivial if Ci = P
and Cj = ¬P for some letter P and some i, j, 1 ≤i, j ≤n. Otherwise, we say
that the sequent is nontrivial.
The system GCNF ′ is deﬁned as follows.
Deﬁnition 4.2.3 (The system GCNF ′). Let Γ, Γ1, Γ2 denote sets of propo-
sitions, A1,...,Am denote propositions, B denote a literal, and P denote a
propositional letter.
Axioms: All trivial sequents, that is, sequents of the form
Γ, P, ¬P →
Inference Rule: All instances of the rule
Γ1, C1, ..., Ck →
Γ2, B →
Γ, (A1 ∨B), ..., (Am ∨B) →
where Γ1 and Γ2 are arbitrary subsets of Γ∪{(A1∨B), ..., (Am∨B)} (possibly
empty and not necessarily disjoint), {C1, ..., Ck} is any nonempty subset of
{A1, ..., Am}, and B is a literal.
Remark: In the above rule, B is a literal and not an arbitrary proposi-
tion. This is not a restriction because the system GCNF ′ is complete. We
could allow arbitrary propositions, but this would complicate the transfor-
mation of a proof tree into a resolution refutation (see the problems). The
system obtained by restricting Γ1 and Γ2 to be subsets of Γ is also complete.
However, if such a rule was used, this would complicate the transformation of
a resolution refutation into a proof tree.
Deduction trees and proof trees are deﬁned in the obvious way as in
deﬁnition 3.4.5.
EXAMPLE 4.2.1
The following is a proof tree in the system GCNF ′.
¬S, S →
S, ¬S →
¬Q, Q →
¬Q, S, {¬S, Q} →
{P, ¬Q}, {¬S, ¬Q}, S, {¬S, Q} →
In the above proof, the ∨: left rule is applied to {P, ¬Q}, {¬S, ¬Q}.
The literal ¬Q, which occurs in both clauses, goes into the right premise
of the lowest inference, and from the set {P, ¬S}, only ¬S goes into

4.2 A Special Gentzen System
121
the left premise. The above proof in GCNF ′ is more concise than the
following proof in G′. This is because rules of GCNF ′ can apply to
several propositions in a single step, avoiding the duplication of subtrees.
T1
T2
{P, ¬Q}, {¬S, ¬Q}, S, {¬S, Q} →
where T1 is the tree
P, ¬S, S, {¬S, Q} →
P, ¬Q, S, ¬S →
P, ¬Q, S, Q →
P, ¬Q, S, {¬S, Q} →
P, {¬S, ¬Q}, S, {¬S, Q} →
and T2 is the tree
¬Q, ¬S, S, {¬S, Q} →
¬Q, S, ¬S →
¬Q, S, Q →
¬Q, S, {¬S, Q} →
¬Q, {¬S, ¬Q}, S, {¬S, Q} →
In order to prove the soundness and completeness of the System GCNF ′,
we need the following lemmas.
4.2.2 Soundness of the System GCNF ′
First, we prove the following lemma.
Lemma 4.2.1
Every axiom is valid. For every valuation v, if v satisﬁes
both premises of a rule of GCNF ′, then v satisﬁes the conclusion.
Proof : It is obvious that every axiom is valid. For the second part, it is
equivalent to show that if v falsiﬁes Γ, (A1 ∨B), ..., (Am ∨B) →, then either
v falsiﬁes Γ1, C1, ..., Ck →, or v falsiﬁes Γ2, B →.
But v falsiﬁes Γ, (A1 ∨
B), ..., (Am ∨B) →if v satisﬁes all propositions in Γ and v satisﬁes all of
(A1 ∨B),...,(Am ∨B). If v satisﬁes B, then v satisﬁes Γ and B, and so v
falsiﬁes Γ2, B →for every subset Γ2 of Γ,(A1 ∨B),...,(Am ∨B). If v does not
satisfy B, then it must satisfy all of A1, ..., Am. But then, v satisﬁes Γ1 and
C1, ..., Ck for any subset Γ1 of Γ,(A1 ∨B),...,(Am ∨B) and nonempty subset
{C1, ..., Ck} of {A1, ..., Am}. This concludes the proof.
Using induction on proof trees as in lemma 3.4.3, we obtain the following
corollary:
Corollary
(Soundness of GCNF ′) Every sequent provable in GCNF ′ is
valid.

122
4/Resolution In Propositional Logic
In order to prove the completeness of GCNF ′, we will also need the
following normal form lemma.
Lemma 4.2.2
Every proof in G′ for a sequent of the form C1, ..., Cm →,
where each Ci is a clause is equivalent to a G′-proof in which all instances of
the ¬ : left rule precede all instances of the ∨: left rule, in the sense that on
every path from a leaf to the root, all ¬ : left rules precede all ∨: left rules.
Proof : First, observe that every proof tree in G′ of the form
Subtree of type (1)
T1
Γ, A →P
T2
Γ, B →P
Γ, (A ∨B) →P
Γ, (A ∨B), ¬P →
can be transformed to a proof tree of the same depth of the form
Subtree of type (2)
T1
Γ, A →P
Γ, A, ¬P →
T2
Γ, B →P
Γ, B, ¬P →
Γ, (A ∨B), ¬P →
Next, we prove the lemma by induction on proof trees. Let T be a proof
tree. If T is a one-node tree, it is an axiom and the lemma holds trivially.
Otherwise, either the inference applied at the root is the ∨: left rule, or it
is the ¬ : left rule. If it is the ∨: left rule, T has two subtrees T1 and T2
with root S1 and S2, and by the induction hypothesis, there are proof trees T ′
1
and T ′
2 with root S1 and S2 satisfying the conditions of the lemma. The tree
obtained from T by replacing T1 by T ′
1 and T2 by T ′
2 satisﬁes the conditions
of the lemma. Otherwise, the inference applied at the root of T is the ¬ : left
rule. Let T1 be the subtree of T having Γ, (A ∨B) →P as its root. If T1 is
an axiom, the lemma holds trivially. If the rule applied at the root of T1 is
a ¬ : left rule, by the induction hypothesis, there is a tree T ′
1 with the same
depth as T1 satisfying the condition of the lemma. If T ′
1 contains only ¬ : left
rules, the lemma holds since for some letter Q, both Q and ¬Q must belong
to Γ, (A ∨B), ¬P →. Otherwise, the tree T ′ obtained by replacing T1 by T ′
1
in T is a tree of type (1), and depth(T ′) = depth(T). By the the remark at
the beginning of the proof, this tree can be replaced by a tree of type (2)
having the same depth as T ′ (and T). We conclude by applying the induction
hypothesis to the two subtrees of T ′. Finally, if the rule applied at the root

4.2 A Special Gentzen System
123
of the subtree T1 is a ∨: left rule, T is a tree of type (1). We conclude as in
the previous case.
4.2.3 Completeness of the System GCNF ′
Having this normal form, we can prove the completeness of GCNF ′.
Theorem 4.2.1
(Completeness of GCNF ′). If a sequent
C1, ..., Cm →
is valid, then it is provable in GCNF ′. Equivalently, if
C1 ∧... ∧Cm
is unsatisﬁable, the sequent
C1, ..., Cm →
is provable in GCNF ′.
Furthermore, every nontrivial valid sequent has a
proof in GCNF ′ in which the axioms are of the form P, ¬P →.
Proof : Since ∨is associative, commutative and idempotent, a clause
{L1, ..., Lk} is equivalent to the formula
(((...(L1 ∨L2) ∨...) ∨Lk−1) ∨Lk).
By the completeness of G′ (theorem 3.4.1), the sequent C1,...,Cm →has a
proof in which the ∨: left rules are instances of the rule of deﬁnition 4.2.3.
From lemma 4.2.2, the sequent C1, ..., Cm →has a G′-proof in which all the
¬ : left rules precede all the ∨: left rules. We prove that every nontrivial
valid sequent C1, ..., Cm →has a proof in GCNF ′ by induction on proof trees
in G′.
Let T be a G′-proof of C1, ..., Cm →
in G′. Since it is assumed that
C1, ..., Cm →is nontrivial, some Ci, say C1, is of the form (A ∨B), and the
bottom inference must be the ∨: left rule. So, T is of the form
T1
A, C2, ..., Cm →
T2
B, C2, ..., Cm →
(A ∨B), C2, ..., Cm →
If both T1 and T2 only contain applications of the ¬ : left rule, A must
be a literal and some Ci is its conjugate, since otherwise, C2, ..., Cm would be
trivial. Assume that Ci is the conjugate of A. Similarly, B is a literal, and
some Cj is its conjugate.

124
4/Resolution In Propositional Logic
Then, we have the following proof in GCNF ′:
A, A →
B, B →
(A ∨B), C2, ..., Cm →
In this proof, Γ1 = {A} = Ci, and Γ2 = {B} = Cj.
Otherwise, either A, C2, ..., Cm or B, C2, ..., Cm is nontrivial.
If both
are nontrivial, by the induction hypothesis, they have proofs S1 and S2 in
GCNF ′, and by one more application of the ∨: left rule, we obtain the
following proof in GCNF ′:
S1
A, C2, ..., Cm →
S2
B, C2, ..., Cm →
(A ∨B), C2, ..., Cm →
If A, C2, ..., Cm is trivial, then A must be a literal and some Ci is its
conjugate, since otherwise, C1, ..., Cm would be trivial. Assume that Ci = A.
Since B, C2, ..., Cm is nontrivial, by the induction hypothesis it has a proof S2
in GCNF ′. Then, the following is a proof of C1, ..., Cm →in GCNF ′:
A, A →
S2
B, C2, ..., Cm →
(A ∨B), C2, ..., Cm →
The case in which A, C2, ..., Cm is nontrivial and B, C2, ..., Cm is trivial
is symmetric. This concludes the proof of the theorem.
The completeness of the system GCNF ′ has been established by showing
that it can simulate the system G′. However, the system GCNF ′ is implicitly
more nondeterministic than the system G′. This is because for a given sequent
of the form
Γ, (A1 ∨B), ..., (Am ∨B) →,
one has the choice to pick the number of propositions (A1 ∨B),...,(Am ∨
B), and to pick the subsets Γ1, Γ2 and C1, ..., Ck.
A consequence of this
nondeterminism is that smaller proofs can be obtained, but that the process
of ﬁnding proofs is not easier in GCNF ′ than it is in G′.
PROBLEMS
4.2.1.
Show that the set of clauses
{{A, B, ¬C}, {A, B, C}, {A, ¬B}, {¬A}}

PROBLEMS
125
is unsatisﬁable.
4.2.2.
Show that the following sets of clauses are unsatisﬁable:
(a) {{A, ¬B, C}, {B, C}, {¬A, C}, {B, ¬C}, {¬B}}
(b) {{A, ¬B}, {A, C}, {¬B, C}, {¬A, B}, {B, ¬C}, {¬A, ¬C}}
4.2.3.
Which of the following sets of clauses are satisﬁable? Give satisfying
valuations for those that are satisﬁable, and otherwise, give a proof
tree in GCNF ′:
(a) {{A, B}, {¬A, ¬B}, {¬A, B}}
(b) {{¬A}, {A, ¬B}, {B}}
(c) {{A, B}, ⊥}
4.2.4.
Consider the following algorithm for converting a proposition A into
a proposition B in conjunctive normal form, such that A is satisﬁable
if and only if B is satisﬁable. First, express A in terms of the con-
nectives ∨, ∧and ¬. Then, let A1, ..., An be all the subformulae of
A, with An = A. Let P1, ..., Pn be distinct propositional letters. The
proposition B is the conjunction of Pn with the conjunctive normal
forms of the following propositions:
((Pi ∨Pj) ≡Pk)
whenever
Ak is (Ai ∨Aj)
((Pi ∧Pj) ≡Pk)
whenever
Ak is (Ai ∧Aj)
(¬Pi ≡Pk)
whenever
Ak is ¬Ai.
(a) Prove that A is satisﬁable if and only if B is satisﬁable, by showing
how to obtain a valuation satisfying B from a valuation satisfying A
and vice versa.
(b) Prove that the above algorithm runs in polynomial time in the
length of the proposition A.
Hint: Use problem 3.2.4.
4.2.5.
Prove that the system GCNF ′ is still complete if we require all leaf
nodes of a proof tree to contain only literals.
4.2.6.
Design a new system GDNF ′ with a single inference rule ∧: right,
for sequents of the form
→D1, ..., Dn,
where D1 ∨... ∨Dn is in disjunctive normal form. Prove the com-
pleteness of such a system.
4.2.7.
Prove that the system GCNF ′ is complete for inﬁnite sets of clauses.
4.2.8.
Write a computer program for testing whether a set of clauses is
unsatisﬁable, using the system GCNF ′.

126
4/Resolution In Propositional Logic
4.3 The Resolution Method for Propositional Logic
We will now present the resolution method, and show how a proof in the
system GCNF ′ can be transformed into a proof by resolution (and vice versa).
In the resolution method, it is convenient to introduce a notation for the clause
{⊥} consisting of the constant false (⊥). This clause is denoted by
and is
called the empty clause. Then, one attempts to establish the unsatisﬁability
of a set {C1, ..., Cm} of clauses by constructing a kind of tree whose root is
the empty clause. Such trees usually contain copies of identical subtrees, and
the resolution method represents them as collapsed trees. Technically, they
are represented by directed acyclic graphs (DAGs).
Proof trees in GCNF ′ can also be represented as DAGs and in this way,
more concise proofs can be obtained. We could describe the algorithms for
converting a proof in GCNF ′ into a proof by resolution and vice versa in
terms of DAGs, but this is not as clear and simple for the system GCNF ′
using DAGs as it is for the standard system GCNF ′ using trees. Hence, for
clarity and simplicity, we shall present our results in terms of proof trees and
resolution DAGs. Some remarks on proof DAGs will be made at the end of
this section.
4.3.1 Resolution DAGs
For our purposes, it is convenient to deﬁne DAGs as pairs (t, R) where t is a
tree and R is an equivalence relation on the set of tree addresses of t satisfying
certain conditions. Roughly speaking, R speciﬁes how common subtrees are
shared (collapsed into the same DAG).
EXAMPLE 4.3.1
f
↙
↘
h
f
↙
↘
↙
↘
f
f
↙
↘
↙
↘
b
b
b
b
The above tree t contains two copies of the subtree
f
↙
↘
b
b
which itself contains two copies of the one-node subtree b.

4.3 The Resolution Method for Propositional Logic
127
If we deﬁne the equivalence relation R on the domain
dom(t) = {e, 1, 2, 21, 22, 211, 212, 221, 222} as the least equivalence relation
containing the set of pairs
{(21, 22), (211, 221), (212, 222), (211, 212)},
the equivalence classes modulo R are the nodes of a graph representing the
tree t as a collapsed tree. This graph can be represented as follows:
EXAMPLE 4.3.2
f
h
f
f
b
The equivalence relation R is such that two equivalent tree addresses
must be the roots of identical subtrees. If two addresses u, v are equiva-
lent, then they must be independent (as deﬁned in Subsection 2.2.2), so
that there are no cycles in the graph.
The formal deﬁnition of a DAG is as follows.
Deﬁnition 4.3.1 A directed acyclic graph (for short, a DAG) is a pair (t, R),
where t is a tree labeled with symbols from some alphabet ∆, and R is an
equivalence relation on dom(t) satisfying the following conditions:
For any pair of tree addresses u, v ∈dom(t), if (u, v) ∈R, then either
u = v or u and v are independent (as deﬁned in Subsection 2.2.2) and, for all
i > 0, we have:
(1) ui ∈dom(t) iﬀvi ∈dom(t);
(2) If ui ∈dom(t), then (ui, vi) ∈R;
(3) t(u) = t(v).
The tree t is called the underlying tree of the DAG. The equivalence
classes modulo R are called the nodes of the DAG. Given any two equivalence
classes S and T, an edge from S to T is any pair ([u], [ui]) of equivalence classes
of tree addresses modulo R, such that u ∈S, and ui ∈T. By conditions (1)-
(3), the deﬁnition of an edge makes sense.
The concept of root, descendant, ancestor, and path are also well deﬁned
for DAGs: They are deﬁned on the underlying tree.

128
4/Resolution In Propositional Logic
This deﬁnition only allows connected rooted DAGs, but this is enough
for our purpose. Indeed, DAGs arising in the resolution method are sets of
DAGs as deﬁned above.
We now describe the resolution method.
4.3.2 Deﬁnition of the Resolution Method for Proposi-
tional Logic
The resolution method rests on the fact that the proposition
((A ∨P) ∧(B ∨¬P)) ≡((A ∨P) ∧(B ∨¬P) ∧(A ∨B))
(∗)
is a tautology. Indeed, since the above is a tautology, the set of clauses
{C1, ..., Cm, {A, P}, {B, ¬P}}
is logically equivalent to the set
{C1, ..., Cm, {A, P}, {B, ¬P}, {A, B}}
obtained by adding the clause {A, B}. Consequently, the set
{C1, ..., Cm, {A, P}, {B, ¬P}}
is unsatisﬁable if and only if the set
{C1, ..., Cm, {A, P}, {B, ¬P}, {A, B}}
is unsatisﬁable.
The clause {A, B} is a resolvent of the clauses {A, P} and {B, ¬P}. The
resolvent of the clauses {P} and {¬P} is the empty clause
. The process of
adding a resolvent of two clauses from a set S to S is called a resolution step.
The resolution method attempts to build a sequence of sets of clauses obtained
by successive resolution steps, and ending with a set containing the empty
clause. When this happens, we know that the original clause is unsatisﬁable,
since resolution steps preserve unsatisﬁability, and a set of clauses containing
the empty clause is obviously unsatisﬁable.
There are several ways of recording the resolution steps. A convenient
and space-eﬃcient way to do so is to represent a sequence of resolution steps
as a DAG. First, we show that the proposition (∗) deﬁned above is a tautology.
Lemma 4.3.1
The proposition
((A ∨P) ∧(B ∨¬P)) ≡((A ∨P) ∧(B ∨¬P) ∧(A ∨B))

4.3 The Resolution Method for Propositional Logic
129
is a tautology, even when either A or B is empty. (When A is empty, (A ∨P)
reduces to P and (A ∨B) to B, and similarly when B is empty. When both
A and B are empty, we have the tautology (P ∧¬P) ≡(P ∧¬P).)
Proof : We prove that
((A ∨P) ∧(B ∨¬P)) ⊃((A ∨P) ∧(B ∨¬P) ∧(A ∨B))
is valid and that
((A ∨P) ∧(B ∨¬P) ∧(A ∨B)) ⊃((A ∨P) ∧(B ∨¬P))
is valid. The validity of the second proposition is immediate. For the ﬁrst
one, it is suﬃcient to show that
((A ∨P) ∧(B ∨¬P)) ⊃(A ∨B)
is valid. We have the following proof tree (in G′):
A, (B ∨¬P) →A, B
P, B →A, B
P →P, A, B
P, ¬P →A, B
P, (B ∨¬P) →A, B
(A ∨P), (B ∨¬P) →A, B
(A ∨P), (B ∨¬P) →(A ∨B)
(A ∨P) ∧(B ∨¬P) →(A ∨B)
→((A ∨P) ∧(B ∨¬P)) ⊃(A ∨B)
Deﬁnition 4.3.2
Given two clauses C1 and C2, a clause C is a resolvent of
C1 and C2 iﬀ, for some literal L, L ∈C1, L ∈C2, and
C = (C1 −{L}) ∪(C2 −{L}).
In other words, a resolvent of two clauses is any clause obtained by strik-
ing out a literal and its conjugate, one from each, and merging the remaining
literals into a single clause.
EXAMPLE 4.3.3
The clauses {A, B} and {¬A, ¬B} have the two resolvents {A, ¬A} and
{B, ¬B}. The clauses {P} and {¬P} have the empty clause as their
resolvent.

130
4/Resolution In Propositional Logic
Observe that by lemma 4.3.1, any set S of clauses is logically equivalent
to the set S ∪{C} obtained by adding any resolvent of clauses in S. However,
it is not true that the set
(S ∪{C}) −{C1, C2}
obtained by adding a resolvent of two clauses C1 and C2, and deleting C1 and
C2 from S is equivalent to S. If S = {{P, Q}, {¬Q}}, then {P} is a resolvent
of {P, Q} and {¬Q}, but S is not equivalent to {{P}}. Indeed, the valuation
v which satisﬁes both P and Q satisﬁes {{P}} but does not satisfy S since it
does not satisfy {¬Q}.
We now deﬁne resolution DAGs. We believe that it is more appropri-
ate to call resolution DAGs having the empty clause at their root resolution
refutations rather than resolutions proofs, since they are used to show the
unsatisﬁability of a set of clauses.
Deﬁnition 4.3.3
Given a set S = {C1, ..., Cn} of clauses, a resolution DAG
for S is any ﬁnite set
D = {(t1, R1), ..., (tm, Rm)}
of distinct DAGs labeled with clauses and such that:
(1) The leaf nodes of each underlying tree ti are labeled with clauses in
S.
(2) For every DAG (ti, Ri), for every non-leaf node u in ti, u has exactly
two successors u1 and u2, and if u1 is labeled with a clause C1 and u2 is
labeled with a clause C2 (not necessarily distinct from C1), then u is labeled
with a resolvent of C1 and C2.
A resolution DAG is a resolution refutation iﬀit consists of a single
DAG (t, R) whose root is labeled with the empty clause. The nodes of a DAG
which are not leaves are also called resolution steps.
EXAMPLE 4.3.4
{P, Q}
{P, ¬Q}
{¬P, Q}
{¬P, ¬Q}
{P}
{Q}
{¬P}
The DAG of example 4.3.4 is a resolution refutation.

4.3 The Resolution Method for Propositional Logic
131
4.3.3 Soundness of the Resolution Method
The soundness of the resolution method is given by the following lemma.
Lemma 4.3.2
If a set S of clauses has a resolution refutation DAG, then S
is unsatisﬁable.
Proof : Let (t1, R1) be a resolution refutation for S. The set S′ of clauses
labeling the leaves of t1 is a subset of S.
First, we prove by induction on the number of nodes in a DAG (t, R)
that the set of clauses S′ labeling the leaves of t is equivalent to the set of
clauses labeling all nodes in t.
If t has one node, the property holds trivially. If t has more than one
node, let l and r be the left subtree and right subtree of t respectively. By the
induction hypothesis, the set S1 of clauses labeling the nodes of l is equivalent
to the set L1 of clauses labeling the leaves of l, and the set S2 of clauses labeling
the nodes of r is equivalent to the set L2 of clauses labeling the leaves of r.
Let C1 and C2 be the clauses labeling the root of l and r respectively. By
deﬁnition 4.3.3, the root of t is a resolvent R of C1 and C2. By lemma 4.3.1,
the set S1 ∪S2 is equivalent to the set S1 ∪S2 ∪{R}. But then, S1 ∪S2 ∪{R}
is equivalent to S′ = L1 ∪L2, since S1 ∪S2 is equivalent to L1 ∪L2. This
concludes the induction proof.
Applying the above property to t1, S′ is equivalent to the set of clauses
labeling all nodes in t1. Since the resolvent D labeling the root of t1 is the
empty clause, the set S′ of clauses labeling the leaves of t1 is unsatisﬁable,
which implies that S is unsatisﬁable since S′ is a subset of S.
4.3.4 Converting GCNF ′-proofs into Resolution Refuta-
tions and Completeness
The completeness of the resolution method is proved by showing how to trans-
form a proof in the system GCNF ′ into a resolution refutation.
Theorem 4.3.1
There is an algorithm for constructing a resolution refuta-
tion D from a proof tree T in the system GCNF ′. Furthermore, the number
of resolution steps in D (nonleaf nodes) is less than or equal to the number
of axioms in T.
Proof : We give a construction and prove its correctness by induction on
proof trees in GCNF ′. Let S = {C1, ..., Cm}. If the proof tree of the sequent
C1, ..., Cm →is a one-node tree labeled with an axiom, there must a literal
L such that some Ci = L and some Cj = L. Hence, we have the resolution
refutation consisting of the DAG

132
4/Resolution In Propositional Logic
Ci
Cj
This resolution DAG has one resolution step, so the base step of the
induction holds.
Otherwise, the proof tree is of the form
T1
Γ1, F1, ..., Fk →
T2
Γ2, B →
Γ, {A1, B}, ..., {An, B} →
where Γ1, Γ2 ⊆Γ∪{{A1, B}, ..., {An, B}}, {F1, ..., Fk} ⊆{A1, ...An}, and Γ∪
{{A1, B}, ..., {An, B}} = S. By the induction hypothesis, there is a resolution
refutation D1 obtained from the tree T1 with root Γ1, F1, ..., Fk →, and a
resolution refutation D2 obtained from the tree T2 with root Γ2, B →. The
leaves of D1 are labeled with clauses in Γ1 ∪{F1, ..., Fk}, and the leaves of D2
are labeled with clauses in Γ2 ∪{B}. Let {F ′
1, ..., F ′
p} be the subset of clauses
in {F1, ..., Fk} that label leaves of D1.
If {F ′
1, ..., F ′
p} ⊆S, then D1 is also resolution refutation for S. Similarly,
if the set of clauses labeling the leaves of D2 is a subset of S, D2 is a resolution
refutation for S. In both cases, by the induction hypothesis, the number of
resolution steps in D1 (resp. D2) is less than or equal to the number of leaves
of T1 (resp. T2), and so, it is less than the number of leaves of T.
Otherwise, let {F ′′
1 , ..., F ′′
q } ⊆{F1, ..., Fk} be the set of clauses not in S
labeling the leaves of D1. Also, B /∈S and B labels some leaf of D2, since oth-
erwise the clauses labeling the leaves of D2 would be in S. Let D′
1 be the reso-
lution DAG obtained from D1 by replacing F ′′
1 , ..., F ′′
q by {F ′′
1 , B}, ..., {F ′′
q , B},
and applying the same resolution steps in D′
1 as the resolution steps applied
in D1. We obtain a resolution DAG that may be a resolution refutation, or
in which the clause B is the label of the root node.
If D′
1 is a resolution refutation, we are done.
Otherwise, since B /∈S and D′
1 is not a resolution refutation, the root
of D′
1 is labeled with B and no leaf of D′
1 is labeled with B. By the induction
hypothesis, the number of resolution steps n1 in D1 (and D′
1) is less than or
equal to the number of axioms in T1, and the number n2 of resolution steps
in D2 is less than or equal to the number of axioms in T2. We construct a
resolution refutation for a subset of Γ, {A1, B}, ..., {An, B} by combining D′
1
and D2 as follows: Identify the root labeled with B in D′
1 with the leaf labeled
with B in D2, and identify all leaf nodes u, v with u ∈D′
1 and v ∈D2, iﬀu
and v are labeled with the same clause. (Since B /∈S, B cannot be such a
clause.) The number of resolution steps in D is n1 + n2, which is less than or

4.3 The Resolution Method for Propositional Logic
133
equal to the number of axioms in T. This concludes the construction and the
proof.
Theorem 4.3.2
(Completeness of the resolution method) Given a ﬁnite set
S of clauses, S is unsatisﬁable if and only if there is a resolution refutation
for S.
Proof : By lemma 4.3.2, if S has a resolution refutation, S is unsatisﬁ-
able. If S is unsatiﬁable, then the sequent S →is valid. Since the system
GCNF ′ is complete (theorem 4.2.1), there is a proof tree T for S →. Finally,
by theorem 4.3.1, a resolution refutation D is obtained from T.
EXAMPLE 4.3.5
Consider the following proof tree in GCNF ′:
P, ¬P →
S, ¬S →
P, S, {¬P, ¬S} →
¬Q, Q →
{P, ¬Q}, {S, ¬Q}, Q, {¬P, ¬S} →
The leaves P, ¬P →and S, ¬S →are mapped into the following DAGs:
D1
{P}
{¬P}
D2
{S}
{¬S}
Let D3 be the DAG obtained from D1 by adding ¬S to {¬P}:
D3
{P}
{¬P, ¬S}
{¬S}
We now combine D3 and D2 to obtain D4:
D4
{P}
{¬P, ¬S}
{S}
{¬S}

134
4/Resolution In Propositional Logic
The leaf Q, ¬Q →is mapped into the DAG D5:
D5
{Q}
{¬Q}
Let D6 be the DAG obtained from D4 by adding ¬Q to {P} and {S}:
D6
{P, ¬Q}
{¬P, ¬S}
{S, ¬Q}
{¬Q, ¬S}
{¬Q}
Finally, let D7 be obtained by combining D6 and D5:
D7
{P, ¬Q}
{¬P, ¬S}
{S, ¬Q}
{¬Q, ¬S}
{¬Q}
{Q}
The theorem actually provides an algorithm for constructing a resolution
refutation. Since the number of resolution steps in D is less than or equal to
the number of axioms in T, the number of nodes in the DAG D cannot be
substantially larger than the number of nodes in T. In fact, the DAG D has
at most two more nodes than the tree, as shown by the following lemma.
Lemma 4.3.3 If T is a proof tree in GCNF ′ and T has m nodes, the number
n of nodes in the DAG D given by theorem 4.3.1 is such that, r+1 ≤n ≤m+2,
where r is the number of resolution steps in D.
Proof : Assume that T has k leaves. Since this tree is binary branching,
it has m = 2k −1 nodes. The number r of resolution steps in D is less than
or equal to the number k of leaves in T. But the number n of nodes in the
DAG is at least r + 1 and at most 2r + 1, since every node has at most two
successors. Hence, n ≤2r + 1 ≤2k + 1 = m + 2.

4.3 The Resolution Method for Propositional Logic
135
In example 4.3.5, the number of resolution steps in the DAG is equal
to the number of leaves in the tree. The following example shows that the
number of resolution steps can be strictly smaller.
EXAMPLE 4.3.6
Consider the following proof tree T:
¬R, R →
¬P, P →
¬Q, Q →
¬Q, ¬P, {P, Q} →
¬R, ¬Q, {R, ¬P}, {P, Q} →
¬R, R →
¬R, ¬Q, {R, ¬P}, {P, Q, R} →
The result of recursively applying the algorithm to T yields the two
DAGs D1 and D2:
D1
{¬P}
{P}
D2
{¬Q}
{Q}
corresponding to the axioms ¬P, P →and ¬Q, Q →. Adding Q to P in
the DAG D1, we obtain the DAG D3:
D3
{¬P}
{P, Q}
{Q}
Identifying the root of DAG D3 with the leaf labeled Q of DAG D2, we
obtain the DAG D4 corresponding to the subtree
¬P, P →
¬Q, Q →
¬Q, ¬P, {P, Q} →
of T:
D4
{¬P}
{P, Q}
{¬Q}
{Q}

136
4/Resolution In Propositional Logic
Adding R to ¬P in D4, we obtain the DAG D5:
D5
{R, ¬P}
{P, Q}
{¬Q}
{R, Q}
{R}
The axiom ¬R, R →yields the DAG D6:
D6
{¬R}
{R}
and merging the root of D5 with the leaf labeled R in D6, we obtain the
DAG D7 corresponding to the left subtree
¬R, R →
¬P, P →
¬Q, Q →
¬Q, ¬P, {P, Q} →
¬R, ¬Q, {R, ¬P}, {P, Q} →
of the original proof tree:
D7
{R, ¬P}
{P, Q}
{¬Q}
{R, Q}
{R}
{¬R}
At this stage, since the right subtree of the proof tree T is the axiom
¬R, R →, we add R to {P, Q} in D7.
However, since R is also in
{R, ¬P}, the resulting DAG D8 is a resolution refutation:
D8
{R, ¬P}
{P, Q, R}
{¬Q}
{R, Q}
{R}
{¬R}

4.3 The Resolution Method for Propositional Logic
137
The DAG D8 has three resolution steps, and the original proof tree T
has four axioms. This is because the original proof tree is not optimal. If
we had applied the ∨: rule to {¬P, R} and {P, Q, R}, we would obtain a
proof tree with three axioms, which yields a DAG with three resolution
steps.
4.3.5 From Resolution Refutations to GCNF ′-proofs
We will now prove that every resolution refutation can be transformed into
a proof tree in GCNF ′. Unfortunately, contrary to what was claimed in the
original edition of this book, the size of the resulting tree is not necessarily
polynomial in the size of the DAG. This result only holds if the resolution
refutation is a tree.
Theorem 4.3.3 There is an algorithm which transforms any resolution refu-
tation D into a proof tree T in the system GCNF ′.
Proof : Let S = {C1, ..., Cm}.
We construct the tree and prove the
theorem by induction on the number of resolution steps in D. If D contains
a single resolution step, D is a DAG of the form
{P}
{¬P}
Hence, for some clauses Ci and Cj in the set of clauses {C1, ..., Cm},
Ci = {P} and Cj = {¬P}. Hence, the one-node tree labeled with the axiom
C1, ..., Cm →is a proof tree.
If D has more than one resolution step, it is a DAG (t, R) whose root
is labeled with the empty clause. The descendants n1 and n2 of the root r of
(t, R) are labeled with clauses {P} and {¬P} for some propositional letter P.
There are two cases: Either one of n1, n2 is a leaf, or neither is a leaf.
Case 1: Assume that n1 is a leaf, so that C1 = {P}, the other case being
similar. Let
{A1, ¬P}, ..., {An, ¬P}
be the terminal nodes of all paths from the node n2, such that every node in
each of these paths contains ¬P. Let D2 be the resolution DAG obtained by
deleting r and n1 (as well as the edges having these nodes as source) and by
deleting ¬P from every node in every path from n2 such that every node in
such a path contains ¬P. The root of D2 is labeled with the empty clause.
By the induction hypothesis, there is a proof tree T2 in GCNF ′ for

138
4/Resolution In Propositional Logic
Γ, A1, ..., An →,
where
Γ = {C2, ..., Cm} −{{A1, ¬P}, ..., {An, ¬P}}.
If {A1, ..., An} is a subset of Γ, then the tree obtained from T2 by re-
placing Γ by {C1, ..., Cm} is also a proof tree. By the induction hypothesis
the number of leaves k in T2 is less than or equal to the number of resolution
steps in D2, which is less than the number of resolution steps in D.
If {A1, ..., An} is not a subset of Γ, then, the following is a proof tree T
in GCNF ′ for C1, ..., Cm →.
T2
Γ, A1, ..., An →
P, ¬P →
P, Γ, {A1, ¬P}, ..., {An, ¬P} →
The number of axioms in T is k−1+1 = k, which is less than or equal to
the number of resolution steps in D. In the special case where Γ, A1, ..., An →
is an axiom, since C1, ..., Cm →is not a trivial sequent (the case of a trivial
sequent being covered by the base case of a DAG with a single resolution
step), there is some proposition in Γ, A1, ..., An which is the conjugate of one
of the Ai. Then, T2 is simply the axiom Ai, Ai →.
Case 2: Neither n1 nor n2 is a leaf. Let
{A1, P}, ..., {An, P}
be the set of terminal nodes of all paths from the node n1, such that every node
in each of these paths contains P. Let D1 be the resolution DAG consisting
of the descendant nodes of n1 (and n1 itself), and obtained by deleting P
from every node in every path from n1 such that every node in such a path
contains P. The root of D1 is labeled with the empty clause. By the induction
hypothesis, there is a proof tree T1 in GCNF ′ for Γ1, A1, ..., An →, where Γ1
is some subset of {C1, ...Cm}. By the induction hypothesis the number of
axioms in T1 is less than or equal to the number m1 of resolution steps in
D1. Similarly, let D2 be the DAG obtained by deleting all descendants of n1
that are not descendants of n2, and deleting all edges having these nodes as
source, including the edges from n1, so that n1 is now a leaf. By the induction
hypothesis, there is a proof tree T2 in GCNF ′ with root labeled with Γ2, P →,
where Γ2 is some subset of {C1, ..., Cm}.
If {A1, ..., An} is a subset of Γ1, then the tree T1 is also a proof tree
for {C1,...,Cm}. Similarly, if P belongs to Γ2, the tree T2 is a proof tree for
{C1,...,Cm}. Otherwise, we have the following proof tree T in GCNF ′:

4.3 The Resolution Method for Propositional Logic
139
T1
Γ1, A1, ..., An →
T2
Γ2, P →
Γ, {A1, P}, ..., {An, P} →
The special cases in which either Γ1, A1, ..., An →
or Γ2, P →
is an
axiom is handled as in case 1.
The details are left as an exercise.
This
completes the proof.
Remark. Although it is tempting to claim that the number of resolution
steps in D is m1 + m2, this is unfortunately false!
The problem is that
because a resolution refutation is a DAG, certain nodes can be shared.
EXAMPLE 4.3.7
Let D be the following resolution refutation:
{P, Q}
{P, ¬Q}
{¬P, Q}
{¬P, ¬Q}
{P}
{Q}
{¬P}
Let D1 be the set of descendants of {P}:
{P, Q}
{P, ¬Q}
{P}
Let D2 be the resolution refutation obtained by deleting P:
{Q}
{¬Q}
Let D3 be the DAG obtained from D by deleting the descendants of
{P} that are not descendants of {¬P}.
Note that since {P, Q} is a
descendant of {¬P}, it is retained. However, {P, ¬Q} is deleted.
{P}
{P, Q}
{¬P, Q}
{¬P, ¬Q}
{Q}
{¬P}

140
4/Resolution In Propositional Logic
Let D4 be the resolution refutation obtained by deleting ¬P from all
paths from {¬P} containing it:
{P, Q}
{¬P, Q}
{¬Q}
{Q}
Let D5 be the set of descendants of {Q}:
{P, Q}
{¬P, Q}
{Q}
Let D6 be the resolution refutation obtained from D5 by deleting Q:
{P}
{¬P}
The method of theorem 4.3.3 yields the following proof trees: The tree
T1:
Q, ¬Q →
corresponds to D2;
The tree T2:
Q, ¬Q →
P, ¬P →
{P, Q}, {¬P, Q}, ¬Q →
corresponds to D4;
The tree T3:
Q, ¬Q →
P, ¬P →
{P, Q}, {¬P, Q}, ¬Q →
P, ¬P →
P, {P, Q}, {¬P, Q}, {¬P, ¬Q} →
corresponds to D3;
The tree T4:

4.3 The Resolution Method for Propositional Logic
141
Q, ¬Q →
Q, ¬Q →
P, ¬P →
{P, Q}, {¬P, Q}, ¬Q →
P, ¬P →
P, {P, Q}, {¬P, Q}, {¬P, ¬Q} →
{P, Q}, {P, ¬Q}, {¬P, Q}, {¬P, ¬Q} →
corresponds to D. The number of axioms of T4 is four, which is the
number of resolution steps in D. The next example shows that it is
possible that the proof tree constructed from a resolution DAG has fewer
leaves than the number of resolution steps.
EXAMPLE 4.3.8
Consider the following resolution refutation D:
{P, Q}
{¬P, R}
{¬Q}
{¬R, P}
{¬P}
{Q, R}
{R}
{¬R}
Let D1 be the DAG consisting of the descendants of {R}:
{P, Q}
{¬P, R}
{¬Q}
{Q, R}
{R}
Let D2 be the DAG obtained from D1 by deleting R:
{P, Q}
{¬P}
{¬Q}
{Q}
Note that D2 is a resolution for a subset of the set of clauses
{{P, Q}, {¬P, R}, {¬Q}, {¬R, P}, {¬P}}.
The construction recursively applied to D2 yields the following proof
tree:

142
4/Resolution In Propositional Logic
P, ¬P →
Q, ¬Q →
{P, Q}, ¬P, ¬Q →
From the above tree, we obtain the following proof tree for the original
set of clauses:
P, ¬P →
Q, ¬Q →
{P, Q}, {¬P, R}, ¬Q, {¬R, P}, ¬P →
This last tree has two leaves, whereas the DAG D has four resolution
steps.
When the resolution DAG D is a tree, we still have the following lemma
showing that the Gentzen system GCNF′ is basically as eﬃcient as the reso-
lution method.
Lemma 4.3.4
If a resolution refutation D is a tree with m nodes, then the
proof tree T constructed by the method of theorem 4.3.3 has a number of
nodes n such that, n ≤2m −3.
Proof : Assume that D has r resolution steps. Since the tree T is binary
branching and has k ≤r leaves, it has n = 2k −1 nodes. But the number m
of nodes in the DAG D is such that, r + 1 ≤m ≤2r + 1. Hence, n ≤2m −3.
In the proof tree of example 4.3.7, there are two leaves labeled with
P, ¬P →, and two leaves labeled with Q, ¬Q →.
Hence, if this tree is
represented as a DAG, it will have ﬁve nodes instead of seven. The original
DAG has eight nodes. This suggests that the system GCNF′ using DAGs
instead of trees is just as eﬃcient as the resolution method. However, we do
not have a proof of this fact at this time.
PROBLEMS
4.3.1.
Show that the set of clauses
{{A, B, ¬C}, {A, B, C}, {A, ¬B}, {¬A}}
is unsatisﬁable using the resolution method.
4.3.2.
Show that the following sets of clauses are unsatisﬁable using the
resolution method:
(a) {{A, ¬B, C}, {B, C}, {¬A, C}, {B, ¬C}, {¬B}}

PROBLEMS
143
(b) {{A, ¬B}, {A, C}, {¬B, C}, {¬A, B}, {B, ¬C}, {¬A, ¬C}}
4.3.3.
Construct a proof tree in GCNF ′ for the set of clauses of problem
4.3.1, and convert it into a resolution DAG using the algorithm of
theorem 4.3.1.
4.3.4.
Construct proof trees in GCNF ′ for the sets of clauses of problem
4.3.2, and convert them into resolution DAGs using the algorithm of
theorem 4.3.1.
4.3.5.
Convert the resolution DAG for the clause of problem 4.3.1 into a
proof tree using the algorithm of theorem 4.3.3.
4.3.6.
Convert the resolution DAGs for the clause of problem 4.3.2 into proof
trees using the algorithm of theorem 4.3.3.
4.3.7.
Find all resolvents of the following pairs of clauses:
(a) {A, B}, {¬A, ¬B}
(b) {A, ¬B}, {B, C, D}
(c) {¬A, B, ¬C}, {B, C}
(d) {A, ¬A}, {A, ¬A}
4.3.8.
Construct a resolution refutation for the following set of clauses:
{{P, Q}, {¬P, Q}, {P, ¬Q}, {¬P, ¬Q}}.
Convert this resolution DAG into a proof tree using the algorithm of
theorem 4.3.3
4.3.9.
Another way of presenting the resolution method is as follows. Given
a (ﬁnite) set S of clauses, let
R(S) = S ∪{C | C is a resolvent of two clauses in S}.
Also, let
R0(S) = S,
Rn+1(S) = R(Rn(S)), n ≥0, and let
R∗(S) =

n≥0
Rn(S).
(a) Prove that S is unsatisﬁable if and only if R∗(S) is unsatisﬁable.
(b) Prove that if S is ﬁnite, there is some n ≥0 such that
R∗(S) = Rn(S).

144
4/Resolution In Propositional Logic
(c) Prove that there is a resolution refutation for S if and only if the
empty clause
is in R∗(S).
(d) Prove that S is unsatisﬁable if and only if
belongs to R∗(S).
4.3.10. Find R(S) for the following sets of clauses:
(a) {{A, ¬B}, {A, B}, {¬A}}
(b) {{A, B, C}, {¬B, ¬C}, {¬A, ¬C}}
(c) {{¬A, ¬B}, {B, C}, {¬C, A}}
(d) {{A, B, C}, {A}, {B}}
4.3.11. Prove that the resolution method is still complete if the resolution
rule is restricted to clauses that are not tautologies (that is, clauses
not containing both P and ¬P for some propositional letter P.)
4.3.12. We say that a clause C1 subsumes a clause C2 if C1 is a proper subset
of C2. In the version of the resolution method described in problem
4.3.8, let
R1(S) = R(S) −{C | C is subsumed by some clause in R(S)}.
Let
R0
1 = S,
Rn+1
1
(S) = R1(Rn
1 (S)) and
R∗
1(S) =

n≥0
{Rn
1 (S)}.
Prove that S is unsatisﬁable if and only if
belongs to R∗
1(S).
4.3.13. Prove that the resolution method is also complete for inﬁnite sets of
clauses.
4.3.14. Write a computer program implementing the resolution method.
Notes and Suggestions for Further Reading
The resolution method was discovered by J.A. Robinson (Robinson, 1965). An
earlier method for testing the unsatisﬁability of a set of clauses is the Davis-
Putnam procedure (Davis and Putnam, 1960). Improvements due to Prawitz
(Prawitz, 1960) and Davis (Davis, 1963) led to the resolution method. An
exposition of resolution based on Gentzen sequents has also been given by J.
Robinson (Robinson, 1969), but the technical details are diﬀerent. To the best
of our knowledge, the constructive method used in this chapter for proving the
completeness of the resolution method by transforming a Gentzen-like proof
is original.

Notes and Suggestions for Further Reading
145
With J.A. Robinson, this author believes that a Gentzen-sequent based
exposition of resolution is the best from a pedagogical point of view and for
theoretical understanding.
The resolution method and many of its reﬁnements are discussed in
Robinson, 1969; Loveland, 1978; and in Chang and Lee, 1973.

Chapter 5
First-Order Logic
5.1 INTRODUCTION
In propositional logic, it is not possible to express assertions about elements
of a structure. The weak expressive power of propositional logic accounts for
its relative mathematical simplicity, but it is a very severe limitation, and it
is desirable to have more expressive logics. First-order logic is a considerably
richer logic than propositional logic, but yet enjoys many nice mathemati-
cal properties. In particular, there are ﬁnitary proof systems complete with
respect to the semantics.
In ﬁrst-order logic, assertions about elements of structures can be ex-
pressed. Technically, this is achieved by allowing the propositional symbols
to have arguments ranging over elements of structures. For convenience, we
also allow symbols denoting functions and constants.
Our study of ﬁrst-order logic will parallel the study of propositional logic
conducted in Chapter 3. First, the syntax of ﬁrst-order logic will be deﬁned.
The syntax is given by an inductive deﬁnition. Next, the semantics of ﬁrst-
order logic will be given. For this, it will be necessary to deﬁne the notion of
a structure, which is essentially the concept of an algebra deﬁned in Section
2.4, and the notion of satisfaction. Given a structure M and a formula A, for
any assignment s of values in M to the variables (in A), we shall deﬁne the
satisfaction relation |=, so that
M |= A[s]
146

5.2 FIRST-ORDER LANGUAGES
147
expresses the fact that the assignment s satisﬁes the formula A in M.
The satisfaction relation |= is deﬁned recursively on the set of formulae.
Hence, it will be necessary to prove that the set of formulae is freely generated
by the atomic formulae.
A formula A is said to be valid in a structure M if
M |= A[s]
for every assignment s. A formula A is valid (or universally valid) if A is valid
in every structure M.
Next, we shall attempt to ﬁnd an algorithm for deciding whether a
formula is valid. Unfortunately, there is no such algorithm. However, it is
possible to ﬁnd a procedure that will construct a proof for a valid formula.
Contrary to the propositional case, this procedure may run forever if the input
formula is not valid. We will set up a proof system that is a generalization of
the Gentzen system of Section 3.4 and extend the search procedure to ﬁrst-
order formulae. Then, some fundamental theorems will be proved, including
the completeness theorem, compactness theorem, and model existence theo-
rem.
The two main concepts in this chapter are the search procedure to build
proof trees and Hintikka sets. The main theme is that the search procedure
is a Hintikka set constructor. If the search procedure fails to ﬁnd a counter
example, a proof is produced. Otherwise, the Hintikka set yields a counter
example.
This approach yields the completeness theorem in a very direct
fashion. Another theme that will emerge in this chapter is that the search
procedure is a very ineﬃcient proof procedure.
The purpose of the next
chapters is to try to ﬁnd more eﬃcient proof procedures.
5.2 FIRST-ORDER LANGUAGES
First, we deﬁne alphabets for ﬁrst-order languages.
5.2.1 Syntax
In contrast with propositional logic, ﬁrst-order languages have a ﬁxed part
consisting of logical connectives, variables, and auxiliary symbols, and a part
that depends on the intended application of the language, consisting of pred-
icate, function, and constant symbols, called the non logical part.
Deﬁnition 5.2.1
The alphabet of a ﬁrst-order language consists of the fol-
lowing sets of symbols:
Logical connectives: ∧(and), ∨(or), ¬ (not), ⊃(implication), ≡(equi-
valence), ⊥(falsehood); quantiﬁers: ∀(for all), ∃(there exists); the equality
symbol .=.

148
5/First-Order Logic
Variables: A countably inﬁnite set V = {x0, x1, x2, ...}.
Auxiliary symbols: “(” and “)”.
A set L of nonlogical symbols consisting of:
(i) Function symbols: A (countable, possibly empty) set FS of symbols
f0,f1,..., and a rank function r assigning a positive integer r(f) (called rank
or arity) to every function symbol f.
(ii) Constants: A (countable, possibly empty) set CS of symbols c0,c1,...,
each of rank zero.
(iii) Predicate symbols: A (countable, possibly empty) set PS of symbols
P0, P1,..., and a rank function r assigning a nonnegative integer r(P) (called
rank or arity) to each predicate symbol P.
It is assumed that the sets V, FS, CS and PS are disjoint. We will refer
to a ﬁrst-order language with set of nonlogical symbols L as the language L.
First-order languages obtained by omitting the equality symbol are referred
to as ﬁrst-order languages without equality. Note that predicate symbols of
rank zero are in fact propositional symbols. Note also that we are using the
symbol .= for equality in the object language in order to avoid confusion with
the symbol = used for equality in the meta language.
We now give inductive deﬁnitions for the sets of terms and formulae.
Note that a ﬁrst-order language is in fact a two-sorted ranked alphabet in the
sense of Subsection 2.5.1. The sorts are term and formula. The symbols in
V, CS and FS are of sort term, and the symbols in PS and ⊥are of sort
formula.
Deﬁnition 5.2.2
Given a ﬁrst-order language L, let Γ be the union of the
sets V, CS and FS. For every function symbol f of rank n > 0, let Cf be the
function Cf : (Γ∗)n →Γ∗such that, for all strings t1, ..., tn ∈Γ∗,
Cf(t1, ..., tn) = ft1...tn.
The set TERML of L-terms (for short, terms) is the inductive closure
of the union of the set V of variables and the set CS of constants under the
constructors Cf.
A more informal way of stating deﬁnition 5.2.2 is the following:
(i) Every constant and every variable is a term.
(ii) If t1, ..., tn are terms and f is a function symbol of rank n > 0, then
ft1...tn is a term.

5.2 FIRST-ORDER LANGUAGES
149
EXAMPLE 5.2.1
Let L be the following ﬁrst-order language for arithmetic where, CS =
{0}, FS = {S, +, ∗}, and PS = {<}. The symbol S has rank 1, and the
symbols +, ∗and < have rank 2. Then, the following are terms:
S0
+S0SS0
∗x1S + Sx1x2.
Deﬁnition 5.2.3
Given a ﬁrst-order language L, let ∆be the union of the
sets Γ and PS (where Γ is the union of the sets V, CS, FS). For every
predicate symbol P of rank n > 0, let CP be the function
CP : (Γ∗)n →∆∗
such that, for all strings t1, ..., tn ∈Γ∗,
CP (t1, ..., tn) = Pt1...tn.
Also, let C .= be the function C .= : (Γ∗)2 →∆∗such that, for all strings
t1, t2 ∈Γ∗,
C .=(t1, t2) = .= t1t2.
The set of L-atomic formulae (for short, atomic formulae) is the induc-
tive closure of the pair of sets TERML (of sort term) and {P | P ∈PS, r(P) =
0} ∪{⊥} (of sort formula), under the functions CP and C .=.
A less formal deﬁnition is the following:
(i) Every predicate symbol of rank 0 is an atomic formula, and so is ⊥.
(ii) If t1, ..., tn are terms and P is a predicate symbol of rank n > 0, then
Pt1...tn is an atomic formula, and so is .= t1t2.
Let Σ be the union of the sets V, CS, FS, PS, and {∧, ∨, ¬, ⊃, ≡,
∀, ∃, .=, (, ), ⊥}. The functions C∧, C∨, C⊃, C≡, C¬ are deﬁned (on Σ∗) as in
deﬁnition 3.2.2, and the functions Ai and Ei are deﬁned such that, for any
string A ∈Σ∗,
Ai(A) = ∀xiA, and Ei(A) = ∃xiA.
The set FORML of L-formulae (for short, formulae) is the inductive
closure of the set of atomic formulae under the functions C∧, C∨, C⊃, C≡,
C¬ and the functions Ai and Ei.
A more informal way to state deﬁnition 5.2.3 is to deﬁne a formula as
follows:

150
5/First-Order Logic
(i) Every atomic formula is a formula.
(ii) For any two formulae A and B, (A∧B), (A∨B), (A ⊃B), (A ≡B)
and ¬A are also formulae.
(iii) For any variable xi and any formula A, ∀xiA and ∃xiA are also
formulae.
We let the letters x, y, z subscripted or not range over variables. We
also omit parentheses whenever possible, as in the propositional calculus.
EXAMPLE 5.2.2
Using the ﬁrst-order language of example 5.2.1, the following are atomic
formulae:
< 0S0
.= ySx
The following are formulae:
∀x∀y(< xy ⊃∃z .= y + xz)
∀x∀y((< xy∨< yx)∨.= xy)
Next, we will show that terms and formulae are freely generated.
5.2.2 Free Generation of the Set of Terms
We deﬁne a function K such that for a symbol s (variable, constant or function
symbol), K(s) = 1−n, where n is the least number of terms that must follow
s to obtain a term (n is the “tail deﬁciency”).
K(x) = 1 −0 = 1, for a variable x;
K(c) = 1 −0 = 1, for a constant c;
K(f) = 1 −n, for a n-ary function symbol f.
We extend K to strings composed of variables, constants and function symbols
as follows: if w = w1...wm then
K(w) = K(w1) + ... + K(wm),
and the following property holds.
Lemma 5.2.1
For any term t, K(t) = 1.
Proof : We use the induction principle for the set of terms. The basis of
the induction holds trivially for the atoms. For a term ft1...tn where f is of
arity n > 0 and t1, ..., tn are terms, since by deﬁnition K(ft1...tn) = K(f) +

5.2 FIRST-ORDER LANGUAGES
151
K(t1) + ... + K(tn) and by the induction hypothesis K(t1) = ... = K(tn) = 1,
we have K(ft1...tn) = 1 −n + (1 + ... + 1) = 1.
Lemma 5.2.2 Every nonempty suﬃx of a term is a concatenation of one or
more terms.
Proof : We use the induction principle for terms. The basis is obvious
for the atoms. For a term ft1...tn, any proper suﬃx w must be of the form
stk+1...tn, where k ≤n, and s is a suﬃx of tk. By the induction hypothesis,
s is a concatenation of terms s1, ..., sm, and w = s1...smtk+1...tn, which is a
concatenation of terms.
Lemma 5.2.3
No proper preﬁx of a term is a term.
Proof : Assume a term t is divided into a proper preﬁx t1 and a (proper)
suﬃx t2. Then, 1 = K(t) = K(t1) + K(t2). By lemma 5.2.2, K(t2) ≥1.
Hence K(t1) ≤0 and t1 cannot be a term by lemma 5.2.1.
Theorem 5.2.1
The set of L-terms is freely generated from the variables
and constants as atoms and the functions Cf as operations.
Proof : First, f ̸= g clearly implies that Cf and Cg have disjoint ranges
and these ranges are disjoint from the set of variables and the set of constants.
Since the constructors increase the length of strings, condition (3) for free
generation holds. It remains to show that the restrictions of the functions Cf
to TERML are injective. If ft1...tm = fs1...sn, we must have t1...tm = s1...sn.
Then either t1 = s1, or t1 is a proper preﬁx of s1, or s1 is a proper preﬁx of t1.
In the ﬁrst case, s2...sm = t2...tn. The other two cases are ruled out by lemma
5.2.3 since both t1 and s1 are terms. By repeating the above reasoning, we
ﬁnd that m = n and ti = si, 1 ≤i ≤n. Hence the set of terms is freely
generated.
5.2.3 Free Generation of the Set of Formulae
To extend this argument to formulae, we deﬁne K on the other symbols with
the following idea in mind: K(s) should be 1−n, where n is the least number
of things (right parentheses, terms or formulae) required to go along with s
in order to form a formula (n is the “tail deﬁciency”).
K(“(”) = −1;
K(“)”) = 1;
K(∀) = −1;
K(∃) = −1;
K(∧) = −1;
K(∨) = −1;
K(⊃) = −1;

152
5/First-Order Logic
K(≡) = −1;
K(¬) = 0;
K( .=) = −1;
K(P) = 1 −n
for any n-ary predicate symbol P;
K(⊥) = 1.
We also extend K to strings as usual:
K(w1...wm) = K(w1) + ... + K(wm).
A lemma analogous to lemma 5.2.1 holds for formulae.
Lemma 5.2.4
For any formula A, K(A) = 1.
Proof : The proof uses the induction principle for formulae. Let Y be
the subset of the set of formulae A such that K(A) = 1. First, we show that
Y contains the atomic formulae. If A is of the form Pt1...tm where P has rank
m, since by lemma 5.2.1, K(ti) = 1, and by deﬁnition K(P) = 1−m, K(A) =
1−m+m = 1, as desired. If A is of the form .= t1t2, since K(t1) = K(t2) = 1
and K( .=) = −1, K(A) = −1+1+1 = 1. By deﬁnition K(⊥) = 1. Next, if A
is of the form ¬B, by the induction hypothesis, K(B) = 1. Since K(¬) = 0,
K(A) = 0 + 1 = 1. If A is of the form (B ∗C) where ∗∈{∧, ∨, ⊃, ≡}, by the
induction hypothesis, K(B) = 1 and K(C) = 1, and
K(A) = K(“(”) + K(B) + K(∗) + K(C) + K(“)”) = −1 + 1 + −1 + 1 + 1 = 1.
Finally if A is of the form ∀xiB (or ∃xiB), by the induction hypothesis
K(B) = 1, and K(A) = K(∀) + K(xi) + K(B) = −1 + 1 + 1 = 1. Hence Y is
closed under the connectives, which concludes the proof by induction.
Lemma 5.2.5
For any proper preﬁx w of a formula A, K(w) ≤0.
Proof : The proof uses the induction principle for formulae and is similar
to that of lemma 5.2.4. It is left as an exercise.
Lemma 5.2.6
No proper preﬁx of a formula is a formula.
Proof : Immediate from lemma 5.2.4 and lemma 5.2.5.
Theorem 5.2.2
The set of L-formulae is freely generated by the atomic
formulae as atoms and the functions CX (X a logical connective), Ai and Ei.
Proof : The proof is similar to that of theorem 3.2.1 and rests on the
fact that no proper preﬁx of a formula is a formula. The argument for atomic
formulae is similar to the arguments for terms. The propositional connectives
are handled as in the proof of theorem 3.2.1.
For the quantiﬁers, observe
that given two formulae, one in the range of a function Ai or Ei, the other

5.2 FIRST-ORDER LANGUAGES
153
in the range of another function, either the leftmost characters diﬀer (for C∧,
C∨, C⊃, C≡and C¬, “(” and “¬” are diﬀerent from “∀” and “∃”), or the
substrings consisting of the two leftmost symbols diﬀer (∀xi is diﬀerent from
∀xj for j ̸= i and diﬀerent from ∃xj for any xj, and similarly ∃xi is diﬀerent
from ∃xj for xj ̸= xi and diﬀerent from ∀xj for any xj).
The fact that the restrictions of the functions C∧, C∨, C⊃, C≡and C¬
to FORML are injective is shown as in lemma 3.2.1, using the property that
a proper preﬁx of a formula is not a formula (and a proper preﬁx of a term
is not a term). For the functions Ai, Ai(A) = Ai(B) iﬀ∀xiA = ∀xiB iﬀ
A = B, and similarly for Ei. Finally, the constructors increase the number of
connectives (or quantiﬁers).
Remarks:
(1) Instead of deﬁning terms and atomic formulae in preﬁx notation, one
can deﬁne them as follows (using parentheses):
The second clause of deﬁnition 5.2.2 is changed to: For every function
symbol f of arity n and any terms t1,...,tn, f(t1,...,tn) is a term. Also, atomic
formulae are deﬁned as follows: For every predicate symbol P of arity n and
any terms t1, ..., tn, P(t1, ..., tn) is an atomic formula; t1 .= t2 is an atomic
formula.
One can still show that the terms and formulae are freely generated. In
the sequel, we shall use the second notation when it is convenient. For simplic-
ity, we shall also frequently use = instead of .= and omit parentheses whenever
possible, using the conventions adopted for the propositional calculus.
(2) The sets TERML and FORML are the carriers of a two-sorted
algebra T(L, V) with sorts term and formula, in the sense of Subsection
2.5.2. The operations are the functions Cf for all function symbols, C .=, CP
for all predicate symbols, C∧, C∨, C⊃, C≡, C¬, Ai, Ei (i ≥0) for formulae,
and the symbols of arity 0 are the propositional symbols in PS, the constants
in CS, and ⊥. This two-sorted algebra T(L, V) is free on the set of variables
V (as deﬁned in Section 2.5), and is isomorphic to the tree algebra TL(V) (in
TL(V), the term Ai(A) is used instead of ∀xiA, and Ei(A) instead of ∃xiA).
5.2.4 Free and Bound Variables
In ﬁrst-order logic, variables may occur bound by quantiﬁers. Free and bound
occurrences of variables are deﬁned by recursion as follows.
Deﬁnition 5.2.4
Given a term t, the set FV (t) of free variables of t is
deﬁned by recursion as follows:
FV (xi) = {xi}, for a variable xi;
FV (c) = ∅, for a constant c;

154
5/First-Order Logic
FV (ft1...tn) = FV (t1) ∪... ∪FV (tn),
for a function symbol f of rank n.
For a formula A, the set FV (A) of free variables of A is deﬁned by:
FV (Pt1...tn) = FV (t1) ∪... ∪FV (tn),
for a predicate symbol P of rank n;
FV ( .= t1t2) = FV (t1) ∪FV (t2);
FV (¬A) = FV (A);
FV ((A ∗B)) = FV (A) ∪FV (B), where ∗∈{∧, ∨, ⊃, ≡};
FV (⊥) = ∅;
FV (∀xiA) = FV (A) −{xi};
FV (∃xiA) = FV (A) −{xi}.
A term t or a formula A is closed if, respectively FV (t) = ∅, or FV (A) =
∅. A closed formula is also called a sentence. A formula without quantiﬁers
is called open.
Deﬁnition 5.2.5
Given a formula A, the set BV (A) of bound variables in
A is given by:
BV (Pt1...tn) = ∅;
BV ( .= t1t2) = ∅;
BV (¬A) = BV (A);
BV ((A ∗B)) = BV (A) ∪BV (B), where ∗∈{∧, ∨, ⊃, ≡};
BV (⊥) = ∅;
BV (∀xiA) = BV (A) ∪{xi};
BV (∃xiA) = BV (A) ∪{xi}.
In a formula ∀xiA (or ∃xiA), we say that the variable xi is bound by the
quantiﬁer ∀(or ∃).
For a formula A, the intersection of FV (A) and BV (A) need not be
empty, that is, the same variable may have both a free and a bound occurrence
in A. As we shall see later, every formula is equivalent to a formula where the
free and bound variables are disjoint.
EXAMPLE 5.2.3
Let
A = (∀x(Rxy ⊃Px) ∧∀y(¬Rxy ∧∀xPx)).

5.2 FIRST-ORDER LANGUAGES
155
Then
FV (A) = {x, y}, BV (A) = {x, y}.
For
B = ∀x(Rxy ⊃Px),
we have
FV (B) = {y}, BV (B) = {x}.
For
C = ∀y(¬Rxy ∧∀xPx),
we have
FV (C) = {x} and BV (C) = {x, y}.
5.2.5 Substitutions
We will also need to deﬁne the substitution of a term for a free variable in a
term or a formula.
Deﬁnition 5.2.6
Let s and t be terms. The result of substituting t in s for
a variable x, denoted by s[t/x] is deﬁned recursively as follows:
y[t/x] = if y ̸= x then y else t, when s is a variable y;
c[t/x] = c, when s is a constant c;
ft1...tn[t/x] = ft1[t/x]...tn[t/x], when s is a term ft1...tn.
For a formula A, A[t/x] is deﬁned recursively as follows:
⊥[t/x] =⊥, when A is ⊥;
Pt1...tn[t/x] = Pt1[t/x]...tn[t/x], when A = Pt1...tn;
.= t1t2[t/x] = .= t1[t/x]t2[t/x], when A = .= t1t2;
(B ∗C)[t/x] = (B[t/x] ∗C[t/x]), when A = (B ∗C), ∗∈{∧, ∨, ⊃, ≡}.
(¬B)[t/x] = ¬B[t/x], when A = ¬B;
(∀yB)[t/x] = if x ̸= y then ∀yB[t/x] else ∀yB, when A = ∀yB;
(∃yB)[t/x] = if x ̸= y then ∃yB[t/x] else ∃yB, when A = ∃yB.
EXAMPLE 5.2.4
Let
A = ∀x(P(x) ⊃Q(x, f(y))), and let t = g(y).
We have
f(x)[t/x] = f(g(y)),
A[t/y] = ∀x(P(x) ⊃Q(x, f(g(y)))),

156
5/First-Order Logic
A[t/x] = ∀x(P(x) ⊃Q(x, f(y))) = A,
since x is a bound variable.
The above deﬁnition prevents substitution of a term for a bound variable.
When we give the semantics of the language, certain substitutions will not
behave properly in the sense that they can change the truth value in a wrong
way. These are substitutions in which some variable in the term t becomes
bound in A[t/x].
EXAMPLE 5.2.5
Let
A = ∃x(x < y)[x/y] = ∃x(x < x).
The sentence
∃x(x < x)
is false in an ordered structure, but
∃x(x < y)
may well be satisﬁed.
Deﬁnition 5.2.7
A term t is free for x in A if either:
(i) A is atomic or
(ii) A = (B ∗C) and t is free for x in B and C, ∗∈{∧, ∨, ⊃, ≡} or
(iii) A = ¬B and t is free for x in B or
(iv) A = ∀yB or A = ∃yB and either
x = y or
x ̸= y, y /∈FV (t) and t is free for x in B.
EXAMPLE 5.2.6
Let
A = ∀x(P(x) ⊃Q(x, f(y))).
Then g(y) is free for y in A, but g(x) is not free for y in A.
If
B = ∀x(P(x) ⊃∀zQ(z, f(y))),
then g(z) is not free for y in B, but g(z) is free for z in B (because the
substitution of g(z) for z will not take place).
From now on we will assume that all substitutions satisfy the conditions
of deﬁnition 5.2.7. Sometimes, if a formula A contains x as a free variable we
write A as A(x) and we abbreviate A[t/x] as A(t). In the next section, we
will turn to the semantics of ﬁrst-order logic.

PROBLEMS
157
PROBLEMS
5.2.1.
Prove lemma 5.2.6.
5.2.2.
Let w be a string consisting of variables, constants and function sym-
bols.
(a) Prove that if K(v) > 0 for every proper suﬃx v of w, then w is a
concatenation of K(w) terms.
(b) Prove that w is a term iﬀK(w) = 1 and K(v) > 0 for every
proper suﬃx v of w.
5.2.3.
Given a formula A and constants a and b, show that for any two
distinct variables xi and xj,
A[a/xi][b/xj] = A[b/xj][a/xi].
5.2.4.
Prove that for every formula A and for every constant c,
FV (A[c/x]) = FV (A) −{x}.
5.2.5.
Prove that for any formula A and term t, if y is not in FV (A), then
A[t/y] = A.
5.2.6.
Prove that for every formula A and every term t, if t is free for x in
A, x ̸= z and z is not in FV (t), for every constant d,
A[t/x][d/z] = A[d/z][t/x].
5.2.7.
Prove that for every formula A, if x and y are distinct variables, y is
free in A, and z is a variable not occurring in A, for every term t free
for x in A,
A[t/x][z/y] = A[z/y][t[z/y]/x],
and t[z/y] is free for x in A[z/y].
∗5.2.8.
The purpose of this problem is to generalize problem 3.2.6, that is, to
give a context-free grammar deﬁning terms and formulae. For this, it
is necessary to encode the variables, constants, and the function and
predicate symbols, as strings over a ﬁnite alphabet. Following Lewis
and Papadimitriou, 1981, each variable xn is encoded as xIn$, each
m-ary predicate symbol Pn is encoded as PIm$In$ (m, n ≥0), and
each m-ary function symbol fn is encoded as fIm$In$.
Then, the set of terms and formulae is the language L(G) deﬁned by
the following context-free grammar G = (V, Σ, R, S):
Σ = {P, I, f, x, $, ∧, ∨, ⊃, ≡, ¬, ∀, ∃, .=, ⊥},

158
5/First-Order Logic
V = Σ ∪{S, N, T, U, W},
R = {N →e,
N →NI,
W →xN$,
T →W,
T →fU,
U →IUT,
U →$N$,
S →PU,
S →⊥,
S →.= TT,
S →(S ∨S),
S →(S ∧S),
S →(S ⊃S),
S →(S ≡S),
S →¬S,
S →∀WS,
S →∃WS}
Prove that the grammar G is unambiguous.
Note: The above language is actually SLR(1). For details on parsing
techniques, consult Aho and Ullman, 1977.
5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
Given a ﬁrst-order language L, the semantics of formulae is obtained by inter-
preting the function, constant and predicate symbols in L and assigning values
to the free variables. For this, we need to deﬁne the concept of a structure.
5.3.1 First-Order Structures
A ﬁrst-order structure assigns a meaning to the symbols in L as explained
below.
Deﬁnition 5.3.1
Given a ﬁrst-order language L, an L-structure M (for
short, a structure) is a pair M = (M, I) where M is a nonempty set called
the domain (or carrier) of the structure and I is a function called the inter-
pretation function and which assigns functions and predicates over M to the
symbols in L as follows:

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
159
(i) For every function symbol f of rank n > 0, I(f) : M n →M is an
n-ary function.
(ii) For every constant c, I(c) is an element of M.
(iii) For every predicate symbol P of rank n ≥0, I(P) : M n →BOOL is
an n-ary predicate. In particular, predicate symbols of rank 0 are interpreted
as truth values (Hence, the interpretation function I restricted to predicate
symbols of rank zero is a valuation as in propositional logic.)
We will also use the following notation in which I is omitted: I(f) is
denoted as fM, I(c) as cM and I(P) as PM.
EXAMPLE 5.3.1
Let L be the language of arithmetic where, CS = {0} and FS =
{S, +, ∗}. The symbol S has rank 1, and the symbols +, ∗have rank 2.
The L-structure N is deﬁned such that its domain is the set N of nat-
ural numbers, and the constant and function symbols are interpreted as
follows: 0 is interpreted as zero, S as the function such that S(x) = x+1
for all x ∈N (the successor function), + is interpreted as addition and
∗as multiplication. The structure Q obtained by replacing N by the set
Q of rational numbers and the structure R obtained by replacing N by
the set R of real numbers and interpreting 0, S, + and ∗in the natural
way are also L-structures.
Remark: Note that a ﬁrst-order L-structure is in fact a two-sorted al-
gebra as deﬁned in Subsection 2.5.2 (with carrier BOOL of sort formula and
carrier M of sort term).
5.3.2 Semantics of Formulae
We now wish to deﬁne the semantics of formulae. Since a formula A may
contain free variables, its truth value will generally depend on the speciﬁc
assignment s of values from the domain M to the variables. Hence, we shall
deﬁne the semantics of a formula A as a function AM from the set of assign-
ments of values in M to the variables, to the set BOOL of truth values. First,
we need some deﬁnitions. For more details, see Section 10.3.2.
Deﬁnition 5.3.2
Given a ﬁrst-order language L and an L-structure M, an
assignment is any function s : V →M from the set of variables V to the
domain M. The set of all such functions is denoted by [V →M].
To deﬁne the meaning of a formula, we shall construct the function
AM recursively, using the fact that formulae are freely generated from the
atomic formulae, and using theorem 2.4.1. Since the meaning of a formula is
a function from [V →M] to BOOL, let us denote the set of all such functions
as [[V →M] →BOOL]. In order to apply theorem 2.4.1, it is necessary to
extend the connectives to the set of functions [[V →M] →BOOL] to make
this set into an algebra. For this, the next two deﬁnitions are needed.

160
5/First-Order Logic
Deﬁnition 5.3.3 Given any nonempty domain M, given any element a ∈M
and any assignment s : V →M, s[xi := a] denotes the new assignment
s′ : V →M such that
s′(y) = s(y) for y ̸= xi and s′(xi) = a.
For all i ≥0, deﬁne the function (Ai)M and (Ei)M from [[V →M] →
BOOL] to [[V →M] →BOOL] as follows: For every function f ∈[[V →
M] →BOOL], (Ai)M(f) is the function such that: For every assignment
s ∈[V →M],
(Ai)M(f)(s) = F
iﬀ
f(s[xi := a]) = F
for some a ∈M;
The function (Ei)M(f) is the function such that: For every assignment
s ∈[V →M],
(Ei)M(f)(s) = T
iﬀ
f(s[xi := a]) = T
for some a ∈M.
Note that (Ai)M(f)(s) = T iﬀthe function gs : M →BOOL such that
gs(a) = f(s[xi := a]) for all a ∈M is the constant function whose value is T,
and that (Ei)M(f)(s) = F iﬀthe function gs : M →BOOL deﬁned above is
the constant function whose value is F.
Deﬁnition 5.3.4
Given a nonempty domain M, the functions ∧M, ∨M,
⊃M and ≡M from [[V →M] →BOOL] × [[V →M] →BOOL] to [[V →
M] →BOOL], and the function ¬M from [[V →M] →BOOL] to [[V →
M] →BOOL] are deﬁned as follows: For every two functions f and g in
[[V →M] →BOOL], for all s in [V →M],
∧M(f, g)(s) = H∧(f(s), g(s));
∨M(f, g)(s) = H∨(f(s), g(s));
⊃M (f, g)(s) = H⊃(f(s), g(s));
≡M (f, g)(s) = H≡(f(s), g(s));
¬M(f)(s) = H¬(f(s)).
Using theorem 2.4.1, we can now deﬁne the meaning tM of a term t and
the meaning AM of a formula A. We begin with terms.
Deﬁnition 5.3.5
Given an L-structure M, the function
tM : [V →M] →M
deﬁned by a term t is the function such that for every assignment s ∈[V →
M], the value tM[s] is deﬁned recursively as follows:

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
161
xM[s] = s(x), for a variable x;
(i)
cM[s] = cM, for a constant c;
(ii)
(ft1...tn)M[s] = fM((t1)M[s], ..., (tn)M[s]).
(iii)
The recursive deﬁnition of the function AM : [V →M] →BOOL is
now given.
Deﬁnition 5.3.6
The function
AM : [V →M] →BOOL
is deﬁned recursively by the following clauses:
(1) For atomic formulae: AM is the function such that, for every assign-
ment s ∈[V →M],
(Pt1...tn)M[s] = PM((t1)M[s], ..., (tn)M[s]);
(i)
( .= t1t2)M[s] = if (t1)M[s] = (t2)M[s] then T else F;
(ii)
(⊥)M[s] = F.
(iii)
(2) For nonatomic formulae:
(i)
(A ∗B)M = ∗M(AM, BM),
where ∗∈{∧, ∨, ⊃, ≡} and ∗M is the corresponding function deﬁned in deﬁ-
nition 5.3.4;
(¬A)M = ¬M(AM);
(ii)
(∀xiA)M = (Ai)M(AM);
(iii)
(∃xiA)M = (Ei)M(AM).
(iv)
Note that by deﬁnitions 5.3.3, 5.3.4, 5.3.5, and 5.3.6, for every assign-
ment s ∈[V →M],
(∀xiA)M[s] = T iﬀAM[s[xi := m]] = T for all m ∈M,
and
(∃xiA)M[s] = T iﬀAM[s[xi := m]] = T for some m ∈M.
Hence, if M is inﬁnite, evaluating (∀xiA)M[s] (or (∃xiA)M[s]) requires
testing inﬁnitely many values (all the truth values AM[s[xi := m]], for m ∈

162
5/First-Order Logic
M). Hence, contrary to the propositional calculus, there does not appear to
be an algorithm for computing the truth value of (∀xiA)M[s] (or (∃xiA)M[s])
and in fact, no such algorithm exists in general.
5.3.3 Satisfaction, Validity, and Model
We can now deﬁne the notions of satisfaction, validity and model.
Deﬁnition 5.3.7
Let L be a ﬁrst-order language and M be an L-structure.
(i) Given a formula A and an assignment s, we say that M satisﬁes A
with s iﬀ
AM[s] = T.
This is also denoted by
M |= A[s].
(ii) A formula A is satisﬁable in M iﬀthere is some assignment s such
that
AM[s] = T;
A is satisﬁable iﬀthere is some M in which A is satisﬁable.
(iii) A formula A is valid in M (or true in M) iﬀ
AM[s] = T
for every assignment s.
This is denoted by
M |= A.
In this case, M is called a model of A. A formula A is valid (or universally
valid) iﬀit is valid in every structure M. This is denoted by
|= A.
(iv) Given a set Γ of formulae, Γ is satisﬁable iﬀthere exists a structure
M and an assignment s such that
M |= A[s]
for every formula
A ∈Γ;
A structure M is a model of Γ iﬀM is a model of every formula in Γ.
This is denoted by
M |= Γ.
The set Γ is valid iﬀM |= Γ for every structure M. This is denoted by
|= Γ.
(v) Given a set Γ of formulae and a formula B, B is a semantic conse-
quence of Γ, denoted by
Γ |= B

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
163
iﬀ, for every L-structure M, for every assignment s,
if
M |= A[s]
for every formula
A ∈Γ
then
M |= B[s].
EXAMPLE 5.3.2
Consider the language of arithmetic deﬁned in example 5.3.1. The fol-
lowing formulae are valid in the model N with domain the set of natural
numbers deﬁned in example 5.3.1. This set AP is known as the axioms
of Peano’s arithmetic:
∀x¬(S(x) .= 0)
∀x∀y(S(x) .= S(y) ⊃x .= y)
∀x(x + 0 .= x)
∀x∀y(x + S(y) .= S(x + y))
∀x(x ∗0 .= 0)
∀x∀y(x ∗S(y) .= x ∗y + x)
For every formula A with one free variable x,
(A(0) ∧∀x(A(x) ⊃A(S(x)))) ⊃∀yA(y)
This last axiom scheme is known as an induction axiom. The structure
N is a model of these formulae.
These formulae are not valid in all
structures. For example, the ﬁrst formula is not valid in the structure
whose domain has a single element.
As in propositional logic, formulae that are universally valid are partic-
ularly interesting. These formulae will be characterized in terms of a proof
system in the next section.
5.3.4 A More Convenient Semantics
It will be convenient in the sequel, especially in proofs involving Hintikka
sets, to have a slightly diﬀerent deﬁnition of the truth of a quantiﬁed formula.
The idea behind this deﬁnition is to augment the language L with a set of
constants naming the elements in the structure M, so that elements in M can
be treated as constants in formulae. Instead of deﬁning the truth value of
(∀xiA)M[s] using the modiﬁed assignments s[xi := m] so that
(∀xiA)M[s] = T iﬀAM[s[xi := m]] = T for all m ∈M,
we will deﬁne it using the modiﬁed formula A[m/xi], so that
(∀xiA)M[s] = T iﬀA[m/xi]M[s] = T for all m ∈M,
where m is a constant naming the element m ∈M. Instead of computing the
truth value of A in the modiﬁed assignment s[xi := m], we compute the truth

164
5/First-Order Logic
value of the modiﬁed formula A[m/xi] obtained by substituting the constant
m for the variable xi, in the assignment s itself.
Deﬁnition 5.3.8 Given a ﬁrst-order language L and an L-structure M, the
extended language L(M) is obtained by adjoining to the set CS of constants
in L a set
{m | m ∈M}
of new constants, one for each element of M. The interpretation function I of
the ﬁrst-order structure M is extended to the constants in the set {m | m ∈
M} by deﬁning I(m) = m. Hence, for any assignment s : V →M,
(m)M[s] = m.
The resulting alphabet is not necessarily countable and the set of formu-
lae may also not be countable. However, since a formula contains only ﬁnitely
many symbols, there is no problem with inductive deﬁnitions or proofs by
induction.
Next, we will prove a lemma that shows the equivalence of deﬁnition
5.3.6 and the more convenient deﬁnition of the truth of a quantiﬁed formula
sketched before deﬁnition 5.3.8. First, we need the following technical lemma.
Lemma 5.3.1
Given a ﬁrst-order language L and an L-structure M, the
following hold:
(1) For any term t, for any assignment s ∈[V →M], any element
m ∈M and any variable xi,
tM[s[xi := m]] = (t[m/xi])M[s].
(2) For any formula A, for any assignment s ∈[V →M], any element
m ∈M and any variable xi,
AM[s[xi := m]] = (A[m/xi])M[s].
Proof : This proof is a typical induction on the structure of terms and
formulae. Since we haven’t given a proof of this kind for ﬁrst-order formulae,
we will give it in full. This will allow us to omit similar proofs later. First,
we prove (1) by induction on terms.
If t is a variable xj ̸= xi, then
(xj)M[s[xi := m]] = s(xj)
and
(xj[m/xi])M[s] = (xj)M[s] = s(xj),

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
165
establishing (1).
If t is the variable xi, then
(xi)M[s[xi := m]] = m
and
(xi[m/xi])M[s] = (m)M[s] = m,
establishing (1).
If t is a constant c, then
(c)M[s[xi := m]] = cM = cM[s] = (c[m/xi])M[s],
establishing (1).
If t is a term ft1...tk, then
(ft1...tk)M[s[xi := m]] = fM((t1)M[s[xi := m]], ..., (tk)M[s[xi := m]]).
By the induction hypothesis, for every j, 1 ≤j ≤k,
(tj)M[s[xi := m]] = (tj[m/xi])M[s].
Hence,
(ft1...tk)M[s[xi := m]] = fM((t1)M[s[xi := m]], ..., (tk)M[s[xi := m]])
= fM((t1[m/xi])M[s], ..., (tk[m/xi])M[s])
= (ft1[m/xi]...tk[m/xi])M[s]
= (t[m/xi])M[s],
establishing (1).
This concludes the proof of (1). Next, we prove (2) by induction on
formulae.
If A is an atomic formula of the form Pt1...tk, then
(Pt1...tk)M[s[xi := m]] = PM((t1)M[s[xi := m]], ..., (tk)M[s[xi := m]]).
By part (1) of the lemma, for every j, 1 ≤j ≤k,
(tj)M[s[xi := m]] = (tj[m/xi])M[s].
Hence,
(Pt1...tk)M[s[xi := m]] = PM((t1)M[s[xi := m]], ..., (tk)M[s[xi := m]])
= PM((t1[m/xi])M[s], ..., (tk[m/xi])M[s])
= (Pt1[m/xi]...tk[m/xi])M[s]
= (A[m/xi])M[s],

166
5/First-Order Logic
establishing (2).
It is obvious that (2) holds for the constant ⊥.
If A is an atomic formula of the form .= t1t2, then
( .= t1t2)M[s[xi := m]] = T iﬀ(t1)M[s[xi := m]] = (t2)M[s[xi := m]].
By part (1) of the lemma, for j = 1, 2, we have
(tj)M[s[xi := m]] = (tj[m/xi])M[s].
Hence,
( .= t1t2)M[s[xi := m]] = T iﬀ
(t1)M[s[xi := m]] = (t2)M[s[xi := m]] iﬀ
(t1[m/xi])M[s] = (t2[m/xi])M[s] iﬀ
(( .= t1t2)[m/xi])M[s] = T,
establishing (2).
If A is a formula of the form (B ∗C) where ∗∈{∧, ∨, ⊃, ≡}, we have
(B ∗C)M[s[xi := m]] = ∗M(BM, CM)[s[xi := m]]
= H∗(BM[s[xi := m]], CM[s[xi := m]]).
By the induction hypothesis,
BM[s[xi := m]] = (B[m/xi])M[s] and
CM[s[xi := m]] = (C[m/xi])M[s].
Hence,
(B ∗C)M[s[xi := m]] = H∗(BM[s[xi := m]], CM[s[xi := m]])
= H∗(B[m/xi])M[s], C[m/xi])M[s]) = ((B ∗C)[m/xi])M[s],
establishing (2).
If A is of the form ¬B, then
(¬B)M[s[xi := m]] = ¬M(BM)[s[xi := m]] = H¬(BM[s[xi := m]]).
By the induction hypothesis,
BM[s[xi := m]] = (B[m/xi])M[s].

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
167
Hence,
(¬B)M[s[xi := m]] = H¬(BM[s[xi := m]])
= H¬(B[m/xi])M[s] = ((¬B)[m/xi])M[s],
establishing (2).
If A is of the form ∀xjB, there are two cases. If xi ̸= xj, then
(∀xjB)M[s[xi := m]] = T iﬀ
BM[s[xi := m][xj := a]] = T for all a ∈M.
By the induction hypothesis,
BM[s[xi := m][xj := a]] = (B[a/xj])M[s[xi := m]],
and by one more application of the induction hypothesis,
(B[a/xj])M[s[xi := m]] = (B[a/xj][m/xi])M[s].
By problem 5.2.3, since xi ̸= xj and m and a are constants,
B[a/xj][m/xi] = B[m/xi][a/xj].
Hence,
BM[s[xi := m][xj := a]] = T for all a ∈M iﬀ
(B[a/xj][m/xi])M[s] = T for all a ∈M iﬀ
((B[m/xi])[a/xj])M[s] = T for all a ∈M.
By the induction hypothesis,
((B[m/xi])[a/xj])M[s] = (B[m/xi])M[s[xj := a]].
Hence,
((B[m/xi])[a/xj])M[s] = T for all a ∈M iﬀ
(B[m/xi])M[s[xj := a]] = T for all a ∈M iﬀ
((∀xjB)[m/xi])M[s] = T,
establishing (2).
If xi = xj, then
s[xi := m][xj := a] = s[xi := a] and (∀xjB)[m/xi] = ∀xiB,

168
5/First-Order Logic
and so
(∀xjB)M[s[xi := m]] = T iﬀ
BM[s[xi := m][xj := a]] = T for all a ∈M iﬀ
BM[s[xi := a]] = T for all a ∈M iﬀ
(∀xiB)M[s] = T iﬀ
((∀xjB)[m/xi])M[s] = T,
establishing (2).
The case in which A is of the form ∃xjB is similar to the previous case
and is left as an exercise. This concludes the induction proof for (2).
We can now prove that the new deﬁnition of the truth of a quantiﬁed
formula is equivalent to the old one.
Lemma 5.3.2
For any formula B, for any assignment s ∈[V →M] and
any variable xi, the following hold:
(∀xiB)M[s] = T iﬀ(B[m/xi])M[s] = T for all m ∈M;
(1)
(∃xiB)M[s] = T iﬀ(B[m/xi])M[s] = T for some m ∈M;
(2)
Proof : Recall that
(∀xiB)M[s] = T iﬀBM[s[xi := m]] = T for all m ∈M.
By lemma 5.3.1,
BM[s[xi := m]] = (B[m/xi])M[s].
Hence,
(∀xiB)M[s] = T iﬀ
(B[m/xi])M[s] = T for all m ∈M,
proving (1).
Also recall that
(∃xiB)M[s] = T iﬀBM[s[xi := m]] = T for some m ∈M.
By lemma 5.3.1,
BM[s[xi := m]] = (B[m/xi])M[s].
Hence,
(∃xiB)M[s] = T iﬀ
(B[m/xi])M[s] = T for some m ∈M,

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
169
proving (2).
In view of lemma 5.3.2, the recursive clauses of the deﬁnition of satis-
faction can also be stated more informally as follows:
M |= (¬A)[s] iﬀM ̸|= A[s],
M |= (A ∧B)[s] iﬀM |= A[s] and M |= B[s],
M |= (A ∨B)[s] iﬀM |= A[s] or M |= B[s],
M |= (A ⊃B)[s] iﬀM ̸|= A[s] or M |= B[s],
M |= (A ≡B)[s] iﬀ(M |= A[s] iff M |= B[s]),
M |= (∀xi)A[s] iﬀM |= (A[a/xi])[s] for every a ∈M,
M |= (∃xi)A[s] iﬀM |= (A[a/xi])[s] for some a ∈M.
5.3.5 Free Variables and Semantics of Formulae
If A is a formula and the set FV (A) of variables free in A is {y1, ..., yn}, for
every assignment s, the truth value AM[s] only depends on the restriction of
s to {y1, ..., yn}. The following lemma makes the above statement precise.
Lemma 5.3.3
Given a formula A with set of free variables {y1, ..., yn}, for
any two assignments s1, s2 such that s1(yi) = s2(yi) for 1 ≤i ≤n,
AM[s1] = AM[s2].
Proof : The lemma is proved using the induction principle for terms and
formulae. First, let t be a term, and assume that s1 and s2 agree on FV (t).
If t = c (a constant), then
tM[s1] = cM = tM[s2].
If t = y (a variable), then
tM[s1] = s1(y) = s2(y) = tM[s2],
since FV (t) = {y} and s1 and s2 agree on FV (t).
If t = ft1...tn, then
tM[s1] = fM((t1)M[s1], ..., (tn)M[s1]).
Since every FV (ti) is a subset of FV (t), 1 ≤i ≤n, and s1 and s2 agree on
FV (t), by the induction hypothesis,
(ti)M[s1] = (ti)M[s2],

170
5/First-Order Logic
for all i, 1 ≤i ≤n. Hence,
tM[s1] = fM((t1)M[s1], ..., (tn)M[s1])
= fM((t1)M[s2], ..., (tn)M[s2]) = tM[s2].
Now, let A be a formula, and assume that s1 and s2 agree on FV (A).
If A = Pt1...tn, since FV (ti) is a subset of FV (A) and s1 and s2 agree
on FV (A), we have
(ti)M[s1] = (ti)M[s2],
1 ≤i ≤n. But then, we have
AM[s1] = PM((t1)M[s1], ..., (tn)M[s1])
= PM((t1)M[s2], ..., (tn)M[s2]) = AM[s2].
If A = .= t1t2, since FV (t1) and FV (t2) are subsets of FV (t) and s1
and s2 agree on FV (A),
(t1)M[s1] = (t1)M[s2] and (t2)M[s1] = (t2)M[s2],
and so
AM[s1] = AM[s2].
The cases in which A is of the form (B ∨C), or (B ∧C), or (B ⊃C), or
(A ≡B), or ¬B are easily handled by induction as in lemma 3.3.1 (or 5.3.1),
and the details are left as an exercise. Finally, we treat the case in which A
is of the form ∀xB (the case ∃xB is similar).
If A = ∀xB, then FV (A) = FV (B) −{x}. First, recall the following
property shown in problem 5.2.4: For every constant c,
FV (B[c/x]) = FV (B) −{x}.
Recall that from lemma 5.3.2,
AM[s1] = T iﬀ(B[a/x])M[s1] = T for every a ∈M.
Since s1 and s2 agree on FV (A) = FV (B)−{x} and FV (B[a/x]) = FV (B)−
{x}, by the induction hypothesis,
(B[a/x])M[s1] = (B[a/x])M[s2] for every a ∈M.
This shows that
(B[a/x])M[s1] = T iﬀ(B[a/x])M[s2] = T,

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
171
that is,
AM[s1] = AM[s2].
As a consequence, if A is a sentence (that is FV (A) = ∅) the truth value
of A in M is the same for all assignments. Hence for a sentence A, for every
structure M,
either M |= A or M |= ¬A.
5.3.6 Subformulae and Rectiﬁed Formulae
In preparation for the next section in which we present a Gentzen system that
is sound and complete with respect to the semantics, we need the following
deﬁnitions and lemmas.
Deﬁnition 5.3.9
Given a formula A, a formula X is a subformula of A if
either:
(i) A = Pt1...tn and X = A, or
(ii) A = .= t1t2 and X = A, or
(iii) A =⊥and X = A, or
(iv) A = (B ∗C) and X = A, or X is a subformula of B, or X is a
subformula of C, where ∗∈{∧, ∨, ⊃, ≡}, or
(v) A = ¬B and X = A, or X is a subformula of B, or
(vi) A = ∀xB and X = A, or X is a subformula of B[t/x] for any term
t free for x in B, or
(vii) A = ∃xB and X = A, or X is a subformula of B[t/x] for any term
t free for x in B.
Deﬁnition 5.3.10
A quantiﬁer ∀(or ∃) binds an occurrence of a variable x
in A iﬀA contains a subformula of the form ∀xB (or ∃xB).
A formula A is rectiﬁed if
(i) FV (A) and BV (A) are disjoint and
(ii) Distinct quantiﬁers in A bind occurrences of distinct variables.
The following lemma for renaming variables apart will be needed later.
Lemma 5.3.4
(i) For every formula A,
|= ∀xA ≡∀yA[y/x]
and
|= ∃xA ≡∃yA[y/x],

172
5/First-Order Logic
for every variable y free for x in A and not in FV (A) −{x}.
(ii) There is an algorithm such that, for any input formula A, the algo-
rithm produces a rectiﬁed formula A′ such that
|= A ≡A′
(that is, A′ is semantically equivalent to A).
Proof : We prove (i), leaving (ii) as an exercise. First, we show that for
every term t and every term r,
t[r/x] = t[y/x][r/y], if y is not in FV (t) −{x}.
The proof is by induction on the structure of terms and it is left as an exercise.
Now, let A be a formula, M be a structure, s an assignment, and a any
element in M. We show that
(A[a/x])M[s] = (A[y/x][a/y])M[s],
provided that y is free for x in A, and that y /∈FV (A) −{x}.
This proof is by induction on formulae. We only treat the case of quan-
tiﬁers, leaving the other cases as an exercise. We consider the case A = ∀zB,
the case ∃zB being similar. Recall that the following property was shown in
problem 5.2.5:
If y is not in FV (A), then A[t/y] = A for every term t. Also recall that
FV (A) = FV (B) −{z}.
If z = x, since y /∈FV (A) −{x} = FV (A) −{z} and z is not free in A, we
know that x and y are not free in A, and
A[a/x] = A, A[y/x] = A, and A[a/y] = A.
If z ̸= x, then
A[a/x] = ∀zB[a/x].
Since y is free for x in A, we know that y ̸= z, y is free for x in B, and
A[y/x][a/y] = ∀zB[y/x][a/y]
(a constant is always free for a substitution). Furthermore, since y /∈FV (A)−
{x}, FV (A) = FV (B) −{z} and z ̸= y, we have y /∈FV (B) −{x}. By the
induction hypothesis,
(B[a/x])M[s] = (B[y/x][a/y])M[s] for every a ∈M,

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
173
which proves that
(A[a/x])M[s] = (A[y/x][a/y])M[s] for every a ∈M.
(ii) This is proved using the induction principle for formulae and re-
peated applications of (i).
EXAMPLE 5.3.3
Let
A = (∀x(Rxy ⊃Px) ∧∀y(¬Rxy ∧∀xPx)).
A rectiﬁed formula equivalent to A is
B = (∀u(Ruy ⊃Pu) ∧∀v(¬Rxv ∧∀zPz)).
From now on, we will assume that we are dealing with rectiﬁed formulae.
5.3.7 Valid Formulae Obtained by Substitution in Tau-
tologies
As in propositional logic we will be particularly interested in those formulae
that are valid. An easy way to generate valid formulae is to substitute formulae
for the propositional symbols in a propositional formula.
Deﬁnition 5.3.11
Let L be a ﬁrst-order language. Let PS0 be the subset
of PS consisting of all predicate symbols of rank 0 (the propositional letters).
Consider the set PROPL of all L-formulae obtained as follows: PROPL con-
sists of all L-formulae A such that for some tautology B and some substitution
σ : PS0 →FORML
assigning an arbitrary formula to each propositional letter in PS0,
A = σ(B),
the result of performing the substitution σ on B.
EXAMPLE 5.3.4
Let
B = (P ≡Q) ≡((P ⊃Q) ∧(Q ⊃P)).
If σ(P) = ∀xC and σ(Q) = ∃y(C ∧D), then
A = (∀xC ≡∃y(C ∧D)) ≡((∀xC ⊃∃y(C ∧D)) ∧(∃y(C ∧D) ⊃∀xC))
is of the above form.

174
5/First-Order Logic
However, note that not all L-formulae can be obtained by substituting
formulae for propositional letters in tautologies. For example, the formulae
∀xC and ∃y(C ∧D) cannot be obtained in this fashion.
The main property of the formulae in PROPL is that they are valid.
Note that not all valid formulae can be obtained in this fashion. For example,
we will prove shortly that the formula ¬∀xA ≡∃x¬A is valid.
Lemma 5.3.5 Let A be a formula obtained by substitution into a tautology
as explained in deﬁnition 5.3.11. Then A is valid.
Proof : Let σ : PS0 →FORML be the substitution and B the propo-
sition such that σ(B) = A.
First, we prove by induction on propositions
that for every L-structure M and every assignment s, for every formula A in
PROPL, if v is the valuation deﬁned such that for every propositional letter
P,
v(P) = (σ(P))M[s],
then
v(B) = AM[s].
The base case B = P is obvious by deﬁnition of v. If B is of the form
(C ∗D) with ∗∈{∨, ∧, ⊃, ≡}, we have
v((C ∗D)) = H∗(v(C), v(D)),
σ((C ∗D)) = (σ(C) ∗σ(D)),
(σ((C ∗D)))M[s] = (σ(C) ∗σ(D))M[s]
= ∗M(σ(C)M, σ(D)M)[s]
= H∗(σ(C)M[s], σ(D)M[s]).
By the induction hypothesis,
v(C) = (σ(C))M[s] and v(D) = (σ(D))M[s].
Hence,
v((C ∗D)) = H∗(v(C), v(D))
= H∗((σ(C))M[s], (σ(D))M[s]) = (σ((C ∗D)))M[s],
as desired.
If B is of the form ¬C, then we have
v(¬C) = H¬(v(C)), σ(¬C) = ¬σ(C),
and
(σ(¬C))M[s] = (¬σ(C))M[s] = ¬M((σ(C))M)[s] = H¬((σ(C))M[s]).

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
175
By the induction hypothesis,
v(C) = (σ(C))M[s].
Hence,
v(¬C) = H¬(v(C)) = H¬((σ(C))M[s]) = (σ(¬C))M[s],
as desired.
This completes the induction proof.
To conclude the lemma,
observe that if B is a tautology,
v(B) = T.
Hence, by the above property, for every structure M and every assignment s,
there is a valuation v such that v(B) = AM[s], and so AM[s] = T.
5.3.8 Complete Sets of Connectives
As in the propositional case, the logical connectives are not independent. The
following lemma shows how they are related.
Lemma 5.3.6
The following formulae are valid for all formulae A,B.
(A ≡B) ≡((A ⊃B) ∧(B ⊃A))
(1)
(A ⊃B) ≡(¬A ∨B)
(2)
(A ∨B) ≡(¬A ⊃B)
(3)
(A ∨B) ≡¬(¬A ∧¬B)
(4)
(A ∧B) ≡¬(¬A ∨¬B)
(5)
¬A ≡(A ⊃⊥)
(6)
⊥≡(A ∧¬A)
(7)
¬∀xA ≡∃x¬A
(8)
¬∃xA ≡∀x¬A
(9)
∀xA ≡¬∃x¬A
(10)
∃xA ≡¬∀x¬A
(11)
Proof : The validity of (1) to (7) follows from lemma 5.3.5, since these
formulae are obtained by substitution in tautologies. To prove (8), recall that
for any formula B,
(∀xB)M[s] = T iﬀBM[s[x := m]] = T for all m ∈M,
and
(∃xB)M[s] = T iﬀBM[s[x := m]] = T for some m ∈M.

176
5/First-Order Logic
Furthermore,
(¬B)M[s] = T iﬀBM[s] = F.
Hence,
(¬∀xA)M[s] = T iﬀ
(∀xA)M[s] = F iﬀ
AM[s[x := m]] = F for some m ∈M iﬀ
(¬A)M[s[x := m]] = T for some m ∈M iﬀ
(∃x¬A)M[s] = T.
The proof of (9) is similar.
To prove (11), observe that
(¬∀x¬A)M[s] = T iﬀ
(∀x¬A)M[s] = F iﬀ
(¬A)M[s[x := m]] = F for some m ∈M, iﬀ
AM[s[x := m]] = T for some m ∈M, iﬀ
(∃xA)M[s] = T.
The proof of (10) is similar.
The above lemma shows that, as in the propositional case, we can restrict
our attention to any functionally complete set of connectives. But it also shows
that we can either dispense with ∃or with ∀, taking ∃xA as an abbreviation
for ¬∀x¬A, or ∀xA as an abbreviation for ¬∃x¬A.
In the rest of this text, we shall use mostly the set {∧, ∨, ¬, ⊃, ∀, ∃} even
though it is not minimal, because it is particularly convenient and natural.
Hence, (A ≡B) will be viewed as an abbreviation for ((A ⊃B) ∧(B ⊃A))
and ⊥as an abbreviation for (P ∧¬P).
5.3.9 Logical Equivalence and Boolean Algebras
As in the propositional case, we can also deﬁne the notion of logical equiva-
lence for formulae.
Deﬁnition 5.3.12
The relation ≃on FORML is deﬁned so that for any
two formulae A and B,
A ≃B
if and only if
|= (A ≡B).
We say that A and B are logically equivalent, or for short, equivalent.
It is immediate to show that ≃is an equivalence relation. The following
additional properties show that it is a congruence in the sense of Subsection
2.4.6 (in the Appendix).

5.3 SEMANTICS OF FIRST-ORDER LANGUAGES
177
Lemma 5.3.7
For all formulae A, A′, B, B′, the following properties hold:
If A ≃A′ and B ≃B′ then, for ∗∈{∧, ∨, ⊃, ≡},
(A ∗B) ≃(A′ ∗B′),
¬A ≃¬A′,
∀xA ≃∀xA′ and
∃xA ≃∃xA′.
Proof : First, we note that a formula (A ≡B) is valid iﬀfor every
structure M and every assignment s,
AM[s] = BM[s].
Then, the proof is similar to the proof of the propositional case (lemma 3.3.5)
for formulae of the form (A ∗B) or ¬A.
Since
(∀xA)M[s] = T iﬀAM[s[x := m]] = T for all m ∈M,
and A ≃A′ implies that
AM[s] = A′
M[s] for all M and s,
then
AM[s[x := m]] = T for all m ∈M iﬀ
A′
M[s[x := m]] = T for all m ∈M iﬀ
(∀xA′)M[s] = T.
Hence ∀xA and ∀xA′ are equivalent. The proof that if A ≃A′ then
∃xA ≃∃xA′ is similar.
In the rest of this section, it is assumed that the constant symbol ⊤is
added to the alphabet of deﬁnition 5.2.1, and that ⊤is interpreted as T. By
lemma 5.3.5 and lemma 3.3.6, the following identities hold.
Lemma 5.3.8
The following identities hold.
Associativity rules:
((A ∨B) ∨C) ≃(A ∨(B ∨C))
((A ∧B) ∧C) ≃(A ∧(B ∧C))
Commutativity rules:
(A ∨B) ≃(B ∨A)
(A ∧B) ≃(B ∧A)
Distributivity rules:
(A ∨(B ∧C)) ≃((A ∨B) ∧(A ∨C))

178
5/First-Order Logic
(A ∧(B ∨C)) ≃((A ∧B) ∨(A ∧C))
De Morgan’s rules:
¬(A ∨B) ≃(¬A ∧¬B)
¬(A ∧B) ≃(¬A ∨¬B)
Idempotency rules:
(A ∨A) ≃A
(A ∧A) ≃A
Double negation rule:
¬¬A ≃A
Absorption rules:
(A ∨(A ∧B)) ≃A
(A ∧(A ∨B)) ≃A
Laws of zero and one:
(A∨⊥) ≃A
(A∧⊥) ≃⊥
(A ∨⊤) ≃⊤
(A ∧⊤) ≃A
(A ∨¬A) ≃⊤(A ∧¬A) ≃⊥
Let us denote the equivalence class of a formula A modulo ≃as [A], and
the set of all such equivalence classes as BL. As in Chapter 3, we deﬁne the
operations +, ∗and ¬ on BL as follows:
[A] + [B] = [A ∨B],
[A] ∗[B] = [A ∧B],
¬[A] = [¬A].
Also, let 0 = [⊥] and 1 = [⊤].
By lemma 5.3.7, the above functions (and constants) are independent of
the choice of representatives in the equivalence classes, and the properties of
lemma 5.3.8 are identities valid on the set BL of equivalence classes modulo
≃. The structure BL is a boolean algebra called the Lindenbaum algebra of L.
This algebra is important for studying algebraic properties of formulae. Some
of these properties will be investigated in the problems of the next section.
Remark: Note that properties of the quantiﬁers and of equality are not
captured in the axioms of a boolean algebra. There is a generalization of the
notion of a boolean algebra, the cylindric algebra, due to Tarski. Cylindric
algebras have axioms for the existential quantiﬁer and for equality. However,
this topic is beyond the scope of this text. The interested reader is referred
to Henkin, Monk, and Tarski, 1971.

PROBLEMS
179
PROBLEMS
5.3.1.
Prove that the following formulae are valid:
∀xA ⊃A[t/x],
A[t/x] ⊃∃xA,
where t is free for x in A.
5.3.2.
Let x, y be any distinct variables.
Let A be any formula, C any
formula not containing the variable x free, and let E be any formula
such that x is free for y in E. Prove that the following formulae are
valid:
∀xC ≡C
∃xC ≡C
∀x∀yA ≡∀y∀xA
∃x∃yA ≡∃y∃xA
∀x∀yE ⊃∀xE[x/y]
∃xE[x/y] ⊃∃x∃yE
∀xA ⊃∃xA
∃x∀yA ⊃∀y∃xA
5.3.3.
Let A, B be any formulae, and C any formula not containing the
variable x free. Prove that the following formulae are valid:
¬∃xA ≡∀x¬A
¬∀xA ≡∃x¬A
∃xA ≡¬∀x¬A
∀xA ≡¬∃x¬A
∀xA ∧∀xB ≡∀x(A ∧B)
∃xA ∨∃xB ≡∃x(A ∨B)
C ∧∀xA ≡∀x(C ∧A)
C ∨∃xA ≡∃x(C ∨A)
C ∧∃xA ≡∃x(C ∧A)
C ∨∀xA ≡∀x(C ∨A)
∃x(A ∧B) ⊃∃xA ∧∃xB
∀xA ∨∀xB ⊃∀x(A ∨B)
5.3.4.
Let A, B be any formulae, and C any formula not containing the
variable x free. Prove that the following formulae are valid:
(C ⊃∀xA) ≡∀x(C ⊃A)
(C ⊃∃xA) ≡∃x(C ⊃A)
(∀xA ⊃C) ≡∃x(A ⊃C)
(∃xA ⊃C) ≡∀x(A ⊃C)
(∀xA ⊃∃xB) ≡∃x(A ⊃B)
(∃xA ⊃∀xB) ⊃∀x(A ⊃B)
5.3.5.
Prove that the following formulae are not valid:
A[t/x] ⊃∀xA,
∃xA ⊃A[t/x],
where t is free for x in A.
5.3.6.
Show that the following formulae are not valid:
∃xA ⊃∀xA
∀y∃xA ⊃∃x∀yA
∃xA ∧∃xB ⊃∃x(A ∧B)
∀x(A ∨B) ⊃∀xA ∨∀xB

180
5/First-Order Logic
5.3.7.
Prove that the formulae of problem 5.3.4 are not necessarily valid if
x is free in C.
5.3.8.
Let A be any formula and B any formula in which the variable x does
not occur free.
(a) Prove that
if |= (B ⊃A), then |= (B ⊃∀xA),
and
if |= (A ⊃B), then |= (∃xA ⊃B).
(b) Prove that the above may be false if x is free in B.
5.3.9.
Let L be a ﬁrst-order language, and M be an L-structure. For any
formulae A, B, prove that:
(a)
M |= A ⊃B implies that (M |= A implies M |= B),
but not vice versa.
(b)
A |= B implies that (|= A implies |= B),
but not vice versa.
5.3.10. Given a formula A with set of free variables FV (A) = {x1, ..., xn},
the universal closure A′ of A is the sentence ∀x1...∀xnA. Prove that
A is valid iﬀ∀x1...∀xnA is valid.
5.3.11. Let L be a ﬁrst-order language, Γ a set of formulae, and B some
formula. Let Γ′ be the set of universal closures of formulae in Γ.
Prove that
Γ |= B implies that Γ′ |= B,
but not vice versa.
5.3.12. Let L be the ﬁrst-order language with equality consisting of one
unary function symbol f.
Write formulae asserting that for every
L-structure M:
(a) f is injective
(b) f is surjective
(c) f is bijective
5.3.13. Let L be a ﬁrst-order language with equality.
Find a formula asserting that any L-structure M has at least n ele-
ments.

PROBLEMS
181
5.3.14. Prove that the following formulae are valid:
∀x∃y(x .= y)
A[t/x] ≡∀x((x .= t) ⊃A), if x /∈V ar(t) and t is free for x in A.
A[t/x] ≡∃x((x .= t) ∧A), if x /∈V ar(t) and t is free for x in A.
∗5.3.15. Let A and B be two L-structures. A surjective function h : A →B
is a homomorphism of A onto B if:
(i) For every n-ary function symbol f, for every (a1, ..., an) ∈An,
h(fA(a1, ..., an)) = fB(h(a1), ..., h(an));
(ii) For every constant symbol c, h(cA) = cB;
(iii) For every n-ary predicate symbol P, for every (a1, ..., an) ∈An,
if
PA(a1, ..., an) = T
then
PB(h(a1), ..., h(an)) = T.
Let t be a term containing the free variables {y1, ..., yn}, and A
be a formula with free variables {y1, ..., yn}. For any assignment s
whose restriction to {y1, ..., yn} is given by s(yi) = ai, the notation
t[a1, ..., an] is equivalent to t[s], and A[a1, ..., an] is equivalent to A[s].
(a) Prove that for any term t, if t contains the variables {y1, ..., yn},
for every (a1, ..., an) ∈An, if h is a homomorphism from A onto B,
then
h(tA[a1, ..., an]) = tB[h(a1), ..., h(an)],
even if h is not surjective.
A positive formula is a formula built up from atomic formulae (ex-
cluding ⊥), using only the connectives ∧, ∨and the quantiﬁers ∀,
∃.
(b) Prove that for any positive formula X, for every (a1, ..., an) ∈An,
if
A |= X[a1, ..., an]
then
B |= X[h(a1), ..., h(an)],
where h is a homomorphism from A onto B (we say that positive
formulae are preserved under homomorphisms).
A strong homomorphism between A and B is a homomorphism h :
A →B such that for any n-ary predicate P and any (a1, ..., an) ∈An,
PA(a1, ..., an) = T
iﬀ
PB(h(a1), ..., h(an)) = T.
An isomorphism is a bijective strong homomorphism.

182
5/First-Order Logic
(c) If L is a ﬁrst-order language without equality, prove that for any
formula X with free variables {y1, ..., yn}, for any (a1, ..., an) ∈An, if
h is a strong homomorphism of A onto B then
A |= X[a1, ..., an]
iﬀ
B |= X[h(a1), ..., h(an)].
(d) If L is a ﬁrst-order language with or without equality, prove that
for any formula X with free variables {y1, ..., yn}, for any (a1, ..., an) ∈
An, if h is an isomorphism between A and B then
A |= X[a1, ..., an]
iﬀ
B |= X[h(a1), ..., h(an)].
(e) Find two structures, a homomorphism h between them, and a
nonpositive formula X that is not preserved under h.
5.3.16. Let A be the sentence:
∀x¬R(x, x) ∧∀x∀y∀z(R(x, y) ∧R(y, z) ⊃R(x, z))∧
∀x∃yR(x, y).
Give an inﬁnite model for A and prove that A has no ﬁnite model.
∗5.3.17. The monadic predicate calculus is the language L with no equality
having only unary predicate symbols, and no function or constant
symbols. Let M be an L-structure having n distinct unary predicates
Q1,...,Qn on M. Deﬁne the relation ∼= on M as follows:
a ∼= b
iﬀ
Qi(a) = T
iﬀ
Qi(b) = T, for all i, 1 ≤i ≤n.
(a) Prove that ∼= is an equivalence relation having at most 2n equiv-
alence classes.
Let M/ ∼= be the structure having the set M/ ∼= of equivalence classes
modulo ∼= as its domain, and the unary predicates R1,...,Rn such that,
for every equivalence class x,
Ri(x) = T
iﬀ
Qi(x) = T.
(b) Prove that for every L-formula A containing only predicate sym-
bols in the set {P1, ..., Pn},
M |= A
iﬀ
M/ ∼=|= A,
and that
|= A
iﬀ
M |= A
for all L-structures M with at most 2n elements.

PROBLEMS
183
(c) Using part (b), outline an algorithm for deciding validity in the
monadic predicate calculus.
5.3.18. Give a detailed rectiﬁcation algorithm (see lemma 5.3.4(ii)).
5.3.19. Let A and B be two L-structures. The structure B is a substructure
of the structure A iﬀthe following conditions hold:
(i) The domain B is a subset of the domain A;
(ii) For every constant symbol c,
cB = cA;
(iii) For every function symbol f of rank n > 0, fB is the restriction
of fA to Bn, and for every predicate symbol P of rank n ≥0, PB is
the restriction of PA to Bn.
A universal sentence is a sentence of the form
∀x1...∀xmB,
where B is quantiﬁer free. Prove that if A is a model of a set Γ of
universal sentences, then B is also a model of Γ.
∗5.3.20. In this problem, some properties of reduced products are investigated.
We are considering ﬁrst-order languages with or without equality.
The notion of a reduced product depends on that of a ﬁlter, and the
reader is advised to consult problem 3.5.8 for the deﬁnition of ﬁlters
and ultraﬁlters.
Let I be a nonempty set which will be used as an index set, and let
D be a proper ﬁlter over I. Let (Ai)i∈I be an I-indexed family of
nonempty sets. The Cartesian product C of these sets, denoted by

i∈I
Ai
is the set of all I-indexed sequences
f : I →

i∈I
Ai
such that, for each i ∈I, f(i) ∈Ai. Such I-sequences will also be
denoted as < f(i) | i ∈I >.
The relation =D on C is deﬁned as follows:
For any two functions f, g ∈C,
f =D g
iﬀ
{i ∈I | f(i) = g(i)} ∈D.

184
5/First-Order Logic
In words, f and g are related iﬀthe set of indices on which they
“agree” belongs to the proper ﬁlter D.
(a) Prove that =D is an equivalence relation.
(b) Let L be a ﬁrst-order language (with or without equality). For
each i ∈I, let Ai be an L-structure. We deﬁne the reduced product
B of the (Ai)i∈I modulo D, as the L-structure deﬁned as follows:
(i) The domain of B the set of equivalence classes of

i∈I
Ai
modulo =D.
(ii) For every constant symbol c, c is interpreted in B as the equiva-
lence class
[< cAi | i ∈I >]D.
(iii) For every function symbol f of rank n > 0, f is interpreted as the
function such that, for any n equivalence classes G1 = [< g1(i) | i ∈
I >]D,...,Gn = [< gn(i) | i ∈I >]D,
fB(G1, ..., Gn) = [< fAi(g1(i), ..., gn(i)) | i ∈I >]D.
(iv) For every predicate symbol P of rank n ≥0, P is interpreted
as the predicate such that, for any n equivalence classes G1 = [<
g1(i) | i ∈I >]D,...,Gn = [< gn(i) | i ∈I >]D,
PB(G1, ..., Gn) = T
iﬀ
{i ∈I | PAi(g1(i), ..., gn(i)) = T} ∈D.
The reduced product B is also denoted by

D
(Ai)i∈I.
(c) Prove that =D is a congruence; that is, that deﬁnitions (ii) to
(iv) are independent of the representatives chosen in the equivalence
classes G1,...,Gn.
∗∗5.3.21. Let B be the reduced product

D
(Ai)i∈I
as deﬁned in problem 5.3.20. When D is an ultraﬁlter, B is called an
ultraproduct.

PROBLEMS
185
(a) Prove that for every term t with free variables {y1, ..., yn}, for any
n equivalence classes G1 = [< g1(i) | i ∈I >]D,...,Gn = [< gn(i) | i ∈
I >]D in B,
tB[G1, ..., Gn] = [< tAi[g1(i), ..., gn(i)] | ∈I >]D.
(b) Let D be an ultraﬁlter. Prove that for any formula A with free
variables FV (A) = {y1, ..., yn}, for any n equivalence classes G1 = [<
g1(i) | i ∈I >]D,...,Gn = [< gn(i) | i ∈I >]D in B,
B |= A[G1, ..., Gn]
iﬀ
{i ∈I | Ai |= A[g1(i), ..., gn(i)]} ∈D.
(c) Prove that if D is an ultraﬁlter, for every sentence A,
B |= A
iﬀ
{i ∈I | Ai |= A} ∈D.
Hint: Proceed by induction on formulae.
The fact that D is an
ultraﬁlter is needed in the case where A is of the form ¬B.
∗5.3.22. This problem generalizes problem 3.5.10 to ﬁrst-order logic. It pro-
vides a proof of the compactness theorem for ﬁrst-order languages of
any cardinality.
(a) Let Γ be a set of sentences such that every ﬁnite subset of Γ is
satisﬁable. Let I be the set of all ﬁnite subsets of Γ, and for each
i ∈I, let Ai be a structure satisfying i. For each sentence A ∈Γ, let
A∗= {i ∈I | A ∈i}.
Let
C = {A∗| A ∈Γ}.
Note that C has the ﬁnite intersection property since
{A1, ..., An} ∈A∗
1 ∩... ∩A∗
n.
By problem 3.5.8, there is an ultraﬁlter U including C, so that every
A∗is in U. If i ∈A∗, then A ∈i, and so
Ai |= A.
Thus, for every A in Γ, A∗is a subset of the set {i ∈I | Ai |= A}.
Show that
{i ∈I | Ai |= A} ∈U.
(b) Show that the ultraproduct B (deﬁned in problem 5.3.20) satisﬁes
Γ.

186
5/First-Order Logic
∗∗5.3.23. Given a ﬁrst-order language L, a literal is either an atomic formula
or the negation of an atomic formula.
A basic Horn formula is a
disjunction of literals, in which at most one literal is positive. The
class of Horn formulae is the least class of formulae containing the
basic Horn formulae, and such that:
(i) If A and B are Horn formulae, so is A ∧B;
(ii) If A is a Horn formula, so is ∀xA;
(iii) If A is a Horn formula, so is ∃xA.
A Horn sentence is a closed Horn formula.
Let A be a Horn formula, and let (Ai)i∈I be an I-indexed family of
structures satisfying A. Let D be a proper ﬁlter over D, and let B
be the reduced product of (Ai)i∈I modulo =D.
(a) Prove that for any n equivalence classes G1 = [< g1(i) | i ∈I >
]D,...,Gn = [< gn(i) | i ∈I >]D in B,
if
{i ∈I | Ai |= A[g1(i), ..., gn(i)]} ∈D
then
B |= A[G1, ..., Gn].
(b) Given a Horn sentence A,
if
Ai |= A
for all ∈I, then
B |= A.
(We say that Horn sentences are preserved under reduced products.)
(c) If we let the ﬁlter D be the family of sets {I}, the reduced product
B is called the direct product of (Ai)i∈I.
Show that the formula A below (which is not a Horn formula) is
preserved under direct products, but is not preserved under reduced
products:
A is the conjunction of the boolean algebra axioms plus the sentence
∃x∀y((x ̸= 0) ∧(x ∗y = y ⊃(y = x ∨y = 0))).
∗5.3.24. Show that in the proof of the compactness theorem given in problem
5.3.22, if Γ is a set of Horn formulae, it is not necessary to extend C
to an ultraﬁlter, but simply to take the ﬁlter generated by C.
∗5.3.25. Let L be a ﬁrst-order language. Two L-structures M1 and M2 are
elementary equivalent iﬀfor every sentence A,
M1 |= A
iﬀ
M2 |= A.
Prove that if there is an isomorphism between M1 and M2, then M1
and M2 are elementary equivalent.

PROBLEMS
187
Note: The converse of the above statement holds if one of the struc-
tures has a ﬁnite domain. However, there are inﬁnite structures that
are elementary equivalent but are not isomorphic. For example, if
the language L has a unique binary predicate <, the structure Q of
the rational numbers and the structure R of the real numbers (with
< interpreted as the strict order of each structure) are elementary
equivalent, but nonisomorphic since Q is countable but R is not.
5.4 Proof Theory of First-Order Languages
In this section, the Gentzen system G′ is extended to ﬁrst-order logic.
5.4.1 The Gentzen System G for Languages Without
Equality
We ﬁrst consider the case of a ﬁrst-order language L without equality.
In
addition to the rules for the logical connectives, we need four new rules for
the quantiﬁers. The rules presented below are designed to make the search for
a counter-example as simple as possible. In the following sections, sequents
are deﬁned as before but are composed of formulae instead of propositions.
Furthermore, it is assumed that the set of connectives is {∧, ∨, ⊃, ¬, ∀, ∃}. By
lemma 5.3.7, there is no loss of generality. Hence, when ≡is used, (A ≡B)
is considered as an abbreviation for ((A ⊃B) ∧(B ⊃A)).
Deﬁnition 5.4.1
(Gentzen system G for languages without equality) The
symbols Γ, ∆, Λ will be used to denote arbitrary sequences of formulae and A,
B to denote formulae. The rules of the sequent calculus G are the following:
Γ, A, B, ∆→Λ
Γ, A ∧B, ∆→Λ (∧: left)
Γ →∆, A, Λ
Γ →∆, B, Λ
Γ →∆, A ∧B, Λ
(∧: right)
Γ, A, ∆→Λ
Γ, B, ∆→Λ
Γ, A ∨B, ∆→Λ
(∨: left)
Γ →∆, A, B, Λ
Γ →∆, A ∨B, Λ (∨: right)
Γ, ∆→A, Λ
B, Γ, ∆→Λ
Γ, A ⊃B, ∆→Λ
(⊃: left)
A, Γ →B, ∆, Λ
Γ →∆, A ⊃B, Λ (⊃: right)
Γ, ∆→A, Λ
Γ, ¬A, ∆→Λ (¬ : left)
A, Γ →∆, Λ
Γ →∆, ¬A, Λ (¬ : right)
In the quantiﬁer rules below, x is any variable and y is any variable free
for x in A and not free in A, unless y = x (y /∈FV (A) −{x}). The term t is
any term free for x in A.

188
5/First-Order Logic
Γ, A[t/x], ∀xA, ∆→Λ
Γ, ∀xA, ∆→Λ
(∀: left)
Γ →∆, A[y/x], Λ
Γ →∆, ∀xA, Λ
(∀: right)
Γ, A[y/x], ∆→Λ
Γ, ∃xA, ∆→Λ
(∃: left)
Γ →∆, A[t/x], ∃xA, Λ
Γ →∆, ∃xA, Λ
(∃: right)
Note that in both the (∀: right)-rule and the (∃: left)-rule, the variable
y does not occur free in the lower sequent. In these rules, the variable y is
called the eigenvariable of the inference. The condition that the eigenvariable
does not occur free in the conclusion of the rule is called the eigenvariable
condition. The formula ∀xA (or ∃xA) is called the principal formula of the
inference, and the formula A[t/x] (or A[y/x]) the side formula of the inference.
The axioms of G are all sequents Γ →∆such that Γ and ∆contain a
common formula.
5.4.2 Deduction Trees for the System G
First, we deﬁne when a sequent is falsiﬁable or valid.
Deﬁnition 5.4.2
(i) A sequent A1, ..., Am →B1, ..., Bn is falsiﬁable iﬀfor
some structure M and some assignment s:
M |= (A1 ∧... ∧Am ∧¬B1 ∧... ∧¬Bn)[s].
Note that when m = 0 the condition reduces to
M |= (¬B1 ∧... ∧¬Bn)[s]
and when n = 0 the condition reduces to
M |= (A1 ∧... ∧Am)[s].
(ii) A sequent A1, ..., Am →B1, ..., Bn is valid iﬀfor every structure M
and every assignment s:
M |= (¬A1 ∨... ∨¬Am ∨B1 ∨... ∨Bn)[s].
This is denoted by
|= A1, ..., Am →B1, ..., Bn.
Note that a sequent is valid if and only if it is not falsiﬁable.
Deduction trees and proof trees for the system G are deﬁned as in deﬁ-
nition 3.4.5, but for formulae and with the rules given in deﬁnition 5.4.1.

5.4 Proof Theory of First-Order Languages
189
EXAMPLE 5.4.1
Let A = ∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z)), where P is a propositional
symbol and Q a unary predicate symbol.
P →P, ∃zQ(z)
Q(y1), P →Q(y1), ∃zQ(z)
Q(y1), P →∃zQ(z)
P, P ⊃Q(y1) →∃zQ(z)
P ⊃Q(y1) →P ⊃∃zQ(z)
∃x(P ⊃Q(x)) →P ⊃∃zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z))
The above is a proof tree for A. Note that P[y1/x] = P.
EXAMPLE 5.4.2
Let A = (∀xP(x) ∧∃yQ(y)) ⊃(P(f(v)) ∧∃zQ(z)), where P and Q are
unary predicate symbols, and f is a unary function symbol. The tree
below is a proof tree for A.
Π
P(f(v)), ∀xP(x), Q(u) →Q(u), ∃zQ(z)
P(f(v)), ∀xP(x), Q(u) →∃zQ(z)
P(f(v)), ∀xP(x), ∃yQ(y) →∃zQ(z)
P(f(v)), ∀xP(x), ∃yQ(y) →P(f(v)) ∧∃zQ(z)
∀xP(x), ∃yQ(y) →P(f(v)) ∧∃zQ(z)
∀xP(x) ∧∃yQ(y) →P(f(v)) ∧∃zQ(z)
→(∀xP(x) ∧∃yQ(y)) ⊃(P(f(v)) ∧∃zQ(z))
where Π = P(f(v)), ∀xP(x), ∃yQ(y) →P(f(v)).
5.4.3 Soundness of the System G
In order to establish the soundness of the system G, the following lemmas will
be needed.
Lemma 5.4.1
(i) For any two terms t and r,
(r[t/x])M[s] = (r[a/x])M[s], where a = tM[s].

190
5/First-Order Logic
(ii) If t is free for x in A, then
(A[t/x])M[s] = (A[a/x])M[s], where a = tM[s].
Proof : The lemma is shown using the induction principle for terms and
formulae as in lemma 5.3.1. The proof of (i) is left as an exercise. Since the
proof of (ii) is quite similar to the proof of lemma 5.3.1, we only treat one
case, leaving the others as an exercise.
Assume A = ∀zB. If z = x, then A[t/x] = A, and A[a/x] = A. If x ̸= z,
since t is free for x in A, z /∈FV (t), t is free for x in B,
A[t/x] = ∀zB[t/x] and A[a/x] = ∀zB[a/x].
By problem 5.2.6, the following property holds: If t is free for x in B,
x ̸= z and z is not in FV (t), for every constant d,
B[t/x][d/z] = B[d/z][t/x].
Then, for every c ∈M, by the induction hypothesis,
(B[t/x][c/z])M[s] = (B[c/z][t/x])M[s]
= (B[c/z][a/x])M[s] = (B[a/x][c/z])M[s],
with a = tM[s]. But this proves that
(∀zB[t/x])M[s] = (∀zB[a/x])M[s],
as desired.
Lemma 5.4.2 For any rule stated in deﬁnition 5.4.1, the conclusion is falsi-
ﬁable in some structure M if and only if one of the premises is falsiﬁable in the
structure M. Equivalently, the conclusion is valid if and only if all premises
are valid.
Proof : We prove the lemma for the rules ∀: right and ∀: left, leaving
the others as an exercise. In this proof, the following abbreviating conventions
will be used: If Γ (or ∆) is the antecedent of a sequent,
M |= Γ[s]
means that
M |= A[s]
for every formula A ∈Γ, and if ∆(or Λ) is the succedent of a sequent,
M ̸|= ∆[s]

5.4 Proof Theory of First-Order Languages
191
means that
M ̸|= B[s]
for every formula B ∈∆.
(i) Assume that Γ →∆, A[y/x], Λ is falsiﬁable. This means that there
is a structure M and an assignment s such that
M |= Γ[s], M ̸|= ∆[s],
M ̸|= (A[y/x])[s] and M ̸|= Λ[s].
Recall that
M |= (∀xA)[s]
iﬀ
M |= (A[a/x])[s] for every a ∈M.
Since y is free for x in A, by lemma 5.4.1, for a = s(y), we have
(A[y/x])M[s] = (A[a/x])M[s].
Hence,
M ̸|= (∀xA)[s],
which implies that Γ →∆, ∀xA, Λ is falsiﬁed by M and s.
Conversely, assume that M and s falsify Γ →∆, ∀xA, Λ. In particular,
there is some a ∈M such that
M ̸|= (A[a/x])[s].
Let s′ be the assignment s(y := a). Since y /∈FV (A) −{x} and y is free for
x in A, by lemma 5.3.3 and lemma 5.4.1, then
M ̸|= (A[y/x])[s′].
Since y does not appear free in Γ, ∆or Λ, by lemma 5.3.3, we also have
M |= Γ[s′], M ̸|= ∆[s′] and M ̸|= Λ[s′].
Hence, M and s′ falsify Γ →∆, A[y/x], Λ.
(ii) Assume that M and s falsify Γ, A[t/x], ∀xA, ∆→Λ. Then, it is
clear that M and s also falsify Γ, ∀xA, ∆→Λ.
Conversely, assume that M and s falsify Γ, ∀xA, ∆→Λ. Then,
M |= Γ[s], M |= (∀xA)[s],
M |= ∆[s] and M ̸|= Λ[s].

192
5/First-Order Logic
In particular,
M |= (A[a/x])[s] for every a ∈M.
By lemma 5.4.1, for a = tM[s], we have
(A[t/x])M[s] = (A[a/x])M[s],
and so
M |= (A[t/x])[s].
Hence, M and s also falsify Γ, A[t/x], ∀xA, ∆→Λ.
As a consequence, we obtain the soundness of the system G.
Lemma 5.4.3
(Soundness of G) Every sequent provable in G is valid.
Proof : Use the induction principle for G-proofs and the fact that the
axioms are obviously valid.
We shall now turn to the completeness of the system G. For that, we
shall modify the procedure expand. The additional complexity comes from
the quantiﬁer rules and the handling of terms, and we need to revise the
deﬁnition of a Hintikka set. We shall deﬁne the concept of a Hintikka set with
respect to a term algebra H deﬁned in the next section. First, we extend the
notation for signed formulae given in deﬁnition 3.5.3 to quantiﬁed formulae.
5.4.4 Signed Formulae and Term Algebras (no Equality
Symbol)
We begin with signed formulae.
Deﬁnition 5.4.3
Signed formulae of type a, type b, type c, and type d and
their components are deﬁned in the tables below.
Type-a formulae
A
A1
A2
T(X ∧Y )
TX
TY
F(X ∨Y )
FX
FY
F(X ⊃Y )
TX
FY
T(¬X)
FX
FX
F(¬X)
TX
TX
Type-b formulae
B
B1
B2
F(X ∧Y )
FX
FY
T(X ∨Y )
TX
TY
T(X ⊃Y )
FX
TY

5.4 Proof Theory of First-Order Languages
193
Type-c formulae
C
C1
C2
T∀xY
TY [t/x]
TY [t/x]
F∃xY
FY [t/x]
FY [t/x]
where t is free for x in Y
Type-d formulae
D
D1
D2
T∃xY
TY [t/x]
TY [t/x]
F∀xY
FY [t/x]
FY [t/x]
where t is free for x in Y
For a formula C of type c, we let C(t) denote TY [t/x] if C = T∀xY ,
FY [t/x] if C = F∃xY , and for a formula D of type d, we let D(t) denote
TY [t/x] if D = T∃xY , FY [t/x] if D = F∀xY .
We deﬁne satisfaction for signed formulae as follows.
Deﬁnition 5.4.4
Given a structure M, an assignment s, and a formula A,
M |= (TA)[s]
iﬀ
M |= A[s],
and
M |= (FA)[s]
iﬀ
M |= (¬A)[s]
(or equivalently, M ̸|= A[s]).
Lemma 5.4.4
For any structure M and any assignment s:
(i) For any formula C of type c,
M |= C[s]
iﬀ
M |= C(a)[s] for every a ∈M;
(ii) For any formula D of type d,
M |= D[s]
iﬀ
M |= D(a)[s] for at least one a ∈M.
Proof : The lemma follows immediately from lemma 5.3.1.
In view of lemma 5.4.4, formulae of type c are also called formulae of
universal type, and formulae of type d are called formulae of existential type.
In order to deﬁne Hintikka sets with respect to a term algebra H, some deﬁ-
nitions are needed.

194
5/First-Order Logic
Deﬁnition 5.4.5 Given a ﬁrst-order language L, a nonempty set H of terms
in TERML (with variables from V) is a term algebra iﬀ:
(i) For every n-ary function symbol f in L (n > 0), for any terms
t1, ..., tn ∈H, ft1...tn is also in H.
(ii) Every constant symbol c in L is also in H.
Note that this deﬁnition is consistent with the deﬁnition of an algebra
given in Section 2.4. Indeed, every function symbol and every constant in
L receives an interpretation.
A term algebra is simply an algebra whose
carrier H is a nonempty set of terms, and whose operations are the term
constructors. Note also that the terms in the carrier H may contain variables
from the countable set V. However, variables are not in L and are not treated
as constants. Also, observe that if L has at least one function symbol, by
condition (i) any term algebra on L is inﬁnite. Hence, a term algebra is ﬁnite
only if L does not contain any function symbols.
5.4.5 Reducts, Expansions
Given a set S of signed L-formulae, not all symbols in L need occur in formulae
in S. Hence, we shall deﬁne the reduct of L to S.
Deﬁnition 5.4.6 (i) Given two ﬁrst-order languages (with or without equal-
ity) L and L′, if L is a subset of L′ (that is, the sets of constant symbols,
function symbols, and predicate symbols of L are subsets of the corresponding
sets of L′), we say that L is a reduct of L′ and that L′ is an expansion of L.
(The set of variables is neither in L nor in L′ and is the given countable set
V.)
(ii) If L is a reduct of L′, an L-structure M = (M, I) is a reduct of an
L′-structure M′ = (M ′, I′) if M ′ = M and I is the restriction of I′ to L. M′
is called an expansion of M.
(iii) Given a set S of signed L-formulae, the reduct of L with respect to
S, denoted by LS, is the subset of L consisting of all constant, function, and
predicate symbols occurring in formulae in S.
Note: If L is a reduct of L′, any L-structure M can be expanded to an
L′-structure M′ (in many ways). Furthermore, for any L-formula A and any
assigment s,
M |= A[s]
if and only if
M′ |= A[s].
5.4.6 Hintikka Sets (Languages Without Equality)
The deﬁnition of a Hintikka set is generalized to ﬁrst-order languages without
equality as follows.

5.4 Proof Theory of First-Order Languages
195
Deﬁnition 5.4.7
A Hintikka set S (over a language L without equality)
with respect to a term algebra H (over the reduct LS) is a set of signed L-
formulae such that the following conditions hold for all signed formulae A, B,
C, D of type a, b, c, d:
H0: No atomic formula and its conjugate are both in S (TA or FA is
atomic iﬀA is).
H1: If a type-a formula A is in S, then both A1 and A2 are in S.
H2: If a type-b formula B is in S, then either B1 is in S or B2 is in S.
H3: If a type-c formula C is in S, then for every term t ∈H, C(t) is in
S (we require that t is free for x in C for every t ∈H).
H4: If a type-d formula D is in S, then for at least one term t ∈H, D(t)
is in S (we require that t is free for x in D for every t ∈H).
H5: Every variable x occurring free in some formula of S is in H.
Observe that condition H5 and the fact that H is a term algebra imply
that, for every term occurring in some formula in S, if that term is closed or
contains only variables free in S, then it is in H.
We can now prove the generalization of lemma 3.5.3 for ﬁrst-order logic
(without equality). From lemma 5.3.4, we can assume without loss of gener-
ality that the set of variables occurring free in formulae in S is disjoint from
the set of variables occurring bound in formulae in S.
Lemma 5.4.5
Every Hintikka set S (with respect to a term algebra H) is
satisﬁable in a structure HS with domain H.
Proof : The LS-structure HS is deﬁned as follows. The domain of HS
is H.
Every constant c in LS is interpreted as the term c;
Every function symbol f of rank n in LS is interpreted as the function
such that, for any terms t1, ..., tn ∈H,
fHS(t1, ..., tn) = ft1...tn.
For every predicate symbol P of rank n in LS, for any terms t1, ..., tn ∈
H,
PHS(t1, ..., tn) =
 F
if FPt1...tn ∈S,
T
if TPt1...tn ∈S,
or neither TPt1...tn nor FPt1...tn is in S.
By conditions H0 and the fact that H is an algebra, this deﬁnition is
proper. Let s be any assignment that is the identity on the variables belonging
to H.

196
5/First-Order Logic
We now prove using the induction principle for formulae that
HS |= X[s]
for every signed formula X ∈S. We will need the following claim that is
proved using the induction principle for terms. (For a proof of a more general
version of this claim, see claim 2 in lemma 5.6.1.)
Claim: For every term t (in H),
tHS[s] = t.
Assume that TPt1...tn is in S. By H5, the variables in t1,..., tn are in
H, and since H is a term algebra, t1,...,tn are in H. By deﬁnition of PHS and
the above claim,
(Pt1...tn)HS[s] = PHS((t1)HS[s], ..., (tn)HS[s])
= PHS(t1, ..., tn) = T.
Hence,
HS |= Pt1...tn[s].
Similarly, it is shown that if FPt1...tn is in S then
(Pt1...tn)HS[s] = F.
The propositional connectives are handled as in lemma 3.5.3.
If a signed formula C of type c is in S, C(t) is in S for every term t in
H by H3. Since C(t) contains one less quantiﬁer than C, by the induction
hypothesis,
HS |= C(t)[s] for every t ∈H.
By lemma 5.4.1, for any formula A, any term t free for x in A, any structure
M and any assignment v, we have
(A[t/x])M[v] = (A[a/x])M[v],
where a = tM[v]. Since
tHS[s] = t,
we have
(C[t/x])HS[s] = (C[t/x])HS[s].
Hence,
HS |= C(t)[s] for every t ∈H,
which implies that
HS |= C[s]

5.4 Proof Theory of First-Order Languages
197
by lemma 5.4.4.
If a signed formula D of type d is in S, D(t) is in S for some t in H by H4.
Since D(t) contains one less quantiﬁer than D, by the induction hypothesis,
HS |= D(t)[s].
As above, it can be shown that
HS |= D(t)[s] for some t ∈H,
which implies that
HS |= D[s]
(by lemma 5.4.4). Finally, using the note before deﬁnition 5.4.7, HS can be
expanded to an L-structure satisfying S.
The domain H of the structure HS is also called a Herbrand universe.
In order to prove the completeness of the system G (in case of a ﬁrst-
order language L without equality) we shall extend the methods used in sec-
tions 3.4 and 3.5 for the propositional calculus to ﬁrst-order logic. Given a
(possibly inﬁnite) sequent Γ →∆, our goal is to attempt to falsify it. For
this, we design a search procedure with the following properties:
(1) If the original sequent is valid, the search procedure stops after a
ﬁnite number of steps, yielding a proof tree.
(2) If the original sequent is falsiﬁable, the search procedure constructs
a possibly inﬁnite tree, and along some (possibly inﬁnite) path in the tree it
can be shown that a Hintikka set exists, which yields a counter example for
the sequent.
The problem is to modify the expand procedure to deal with quantiﬁer
rules and terms. Clauses H3 and H4 in the deﬁnition of a Hintikka set suggest
that a careful procedure must be designed in dealing with quantiﬁed formulae.
We ﬁrst treat the special case in which we are dealing with a ﬁrst-order
language without equality, without function symbols and with a ﬁnite sequent
Γ →∆.
5.4.7 Completeness: Special Case of Languages Without
Function Symbols and Without Equality
Given a sequent Γ →∆, using lemma 5.3.4, we can assume that the set of
all variables occurring free in some formula in the sequent is disjoint from
the set of all variables occurring bound in some formula in the sequent. This
condition ensures that terms occurring in formulae in the sequent are free for
susbtitutions. Even though it is not strictly necessary to assume that all the
formulae in the sequent are rectiﬁed, it is convenient to assume that they are.

198
5/First-Order Logic
First, it is convenient for proving the correctness of the search procedure
to give a slightly more general version of the quantiﬁer rules ∀: left and
∃: right.
Deﬁnition 5.4.8 The extended rules ∀: left and ∃: right are the following:
Γ, A[t1/x], ..., A[tk/x], ∀xA, ∆→Λ
Γ, ∀xA, ∆→Λ
(∀: left)
Γ →∆, A[t1/x], ..., A[tk/x], ∃xA, Λ
Γ →∆, ∃xA, Λ
(∃: right)
where t1,...,tk are any k terms (k ≥1) free for x in A.
It is clear that an inference using this new version of the ∀: left rule
(resp. ∃: right rule) can be simulated by k applications of the old ∀: left rule
(resp. ∃: right rule). Hence, there is no gain of generality. However, these
new rules may reduce the size of proof trees. Consequently, we will assume
from now on that the rules of deﬁnition 5.4.8 are used as the ∀: left and
∃: right rules of the Gentzen system G.
In order to fulﬁll conditions H3 and H4 we build lists of variables and
constants as follows.
Let TERM0 be a nonempty list of terms and variables deﬁned as follows.
If no free variables and no constants occur in any of the formulae in Γ →∆,
TERM0 =< y0 >,
where y0 is the ﬁrst variable in V not occurring in any formula in Γ →∆.
Otherwise,
TERM0 =< u0, ..., up >,
a list of all free variables and constants occurring in formulae in Γ →∆. Let
AV AIL0 =< y1, ..., yn, ... >
be a countably inﬁnite list disjoint from TERM0 and consisting of variables
not occurring in any formula in Γ →∆.
The terms in TERM0 and the variables in AV AIL0 will be used as the
t’s and y’s for our applications of quantiﬁer rules ∀: right, ∀: left, ∃: right
and ∃: left. Because the variables in TERM0 do not occur bound in Γ →∆
and the variables in AV AIL0 are new, the substitutions with results A[t/x]
and A[y/x] performed using the quantiﬁer rules will be free.
As the search for a counter example for Γ →∆progresses, we keep track
step by step of which of u0,...,up,y1,y2,y3,... have been thus far activated. The

5.4 Proof Theory of First-Order Languages
199
list of activated terms is kept in TERM0 and the list of available variables in
AV AIL0.
Every time a rule ∀: right or ∃: left is applied, as the variable y we use
the head of the list AV AIL0, we append y to the end of TERM0 and delete
y from the head of AV AIL0.
When a rule ∀: left or ∃: right is applied, we use as the terms t each
of the terms u0,...,uq in TERM0 that have not previously served as a term t
for that rule with the same principal formula.
To handle the ∀: left rule and the ∃: right rule correctly, it is necessary
to keep track of the formulae ∀xA (or ∃xA) for which the term ui was used
as a term for the rule ∀: left (or ∃: right) with principal formula ∀xA (or
∃xA). The ﬁrst reason is economy, but the second is more crucial:
If a sequent has the property that all formulae in it are either atomic or
of the form ∀xA (or ∃xA) such that all the terms u0,...,uq in TERM0 have
already been used as terms for the rule ∀: left (or ∃: right), and if this
sequent is not an axiom, then it will never become an axiom and we can stop
expanding it.
Hence, we structure TERM0 as a list of records where every record
< ui, FORM0(i) > contains two ﬁelds: ui is a term and FORM0(i) a list
of the formulae ∀xA (or ∃xA) for which ui was used as a term t for the rule
∀: left (or ∃: right) with principal formula ∀xA (or ∃xA). Initially, each
list FORM0(i) is the null list. The lists FORM0(i) are updated each time a
term t is used in a rule ∀: left or ∃: right. We also let t(TERM0) denote
the set of terms {ui | < ui, FORM0(i) > ∈TERM0}.
Finally, we need to take care of another technical detail: These lists
must be updated only at the end of a round, so that the same substitutions
are performed for all occurrences of a formula.
Hence, we create another
variable TERM1 local to the procedure search. During a round, TERM1 is
updated but TERM0 is not, and at the end of the round, TERM0 is set to
its updated version TERM1.
A leaf of the tree constructed by procedure search is ﬁnished iﬀeither:
(1) The sequent labeling it is an axiom, or
(2) The sequent contains only atoms or formulae ∀xA (or ∃xA) belonging
to all of the lists FORM0(i) for all < ui, FORM0(i) > in TERM0.
The search procedure is obtained by modifying the procedure given in
deﬁnition 3.4.6 by adding the initialization of TERM0 and AV AIL0.
Deﬁnition 5.4.9 The search procedure. The input to search is a one-node
tree labeled with a sequent Γ →∆. The output is a possibly inﬁnite tree T
called a systematic deduction tree.

200
5/First-Order Logic
Procedure Search
procedure search(Γ →∆: sequent; var T : tree);
begin
let T be the one-node tree labeled with Γ →∆;
Let TERM0 :=<< u0, nil >, ..., < up, nil >>
and let AVAIL0 :=< y1, y2, y3, ... >, with u0, ..., up
as explained after def. 5.4.8, with p = 1 and u0 = y0 when
Γ →∆contains no free variables and no constants.
while not all leaves of T are ﬁnished do
TERM1 := TERM0; T0 := T;
for each leaf node of T0
(in lexicographic order of tree addresses) do
if not finished(node) then
expand(node, T)
endif
endfor;
TERM0 := TERM1
endwhile;
if all leaves are closed
then
write (‘T is a proof of Γ →∆’)
else
write (‘Γ →∆is falsiﬁable’)
endif
end
Procedure Expand
procedure expand(node : tree-address; var T : tree);
begin
let A1, ..., Am →B1, ..., Bn be the label of node;
let S be the one-node tree labeled with
A1, ..., Am →B1, ..., Bn;
for i := 1 to m do
if nonatomic(Ai) then
grow-left(Ai, S)
endif
endfor;
for i := 1 to n do
if nonatomic(Bi) then
grow-right(Bi, S)
endif
endfor;
T := dosubstitution(T, node, S)
end

5.4 Proof Theory of First-Order Languages
201
Procedure Grow-Left
procedure grow-left(A : formula; var S : tree);
begin
case A of
B ∧C, B ∨C,
B ⊃C, ¬B
: extend every nonaxiom leaf of S using the
left rule corresponding to the main
propositional connective;
∀xB
: for every term uk ∈t(TERM0)
such that A is not in FORM0(k) do
extend every nonaxiom leaf of S by applying
the ∀: left rule using the term uk
as one of the terms of the rule. In TERM1
let FORM1(k) := append(FORM1(k), A)
endfor;
∃xB
: extend every nonaxiom leaf of S by applying
the ∃: left rule using y = head(AVAIL0)
as the new variable;
TERM1 := append(TERM1, < y, nil >);
AVAIL0 := tail(AVAIL0)
endcase
end
Procedure Grow-Right
procedure grow-right(A : formula; var S : tree);
begin
case A of
B ∧C, B ∨C,
B ⊃C, ¬B
: extend every nonaxiom leaf of S using the
right rule corresponding to the main
propositional connective;
∃xB
: for every term uk ∈t(TERM0)
such that A is not in FORM0(k) do
extend every nonaxiom leaf of S by applying
the ∃: right rule using the term uk
as one of the terms of the rule. In TERM1
let FORM1(k) := append(FORM1(k), A)
endfor;
∀xB
: extend every nonaxiom leaf of S by applying
the ∀: right rule using y = head(AVAIL0)
as the new variable;
TERM1 := append(TERM1, < y, nil >);
AVAIL0 := tail(AVAIL0)
endcase
end

202
5/First-Order Logic
EXAMPLE 5.4.3
Let
A = ∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z)),
where P is a propositional symbol and Q a unary predicate symbol. Let
us trace the construction of the proof tree constructed by the search
procedure.
Since A does not have any free variables, TERM0 =<<
y0, nil >>.
After the ﬁrst round, we have the following tree, and
TERM0 and AV AIL0 have not changed.
∃x(P ⊃Q(x)) →P ⊃∃zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z))
The ∃: left rule is applied using the head y1 of AV AIL0, and the
following tree is obtained:
P, P ⊃Q(y1) →∃zQ(z)
P ⊃Q(y1) →P ⊃∃zQ(z)
∃x(P ⊃Q(x)) →P ⊃∃zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z))
At the end of this round,
TERM0 =<< y0, nil >, < y1, nil >>,
and
AV AIL0 =< y2, y3, ... > .
During the next round, the ∃: right rule is applied to the formula
∃zQ(z) with the terms y0 and y1. The following tree is obtained:
P →P, ∃zQ(z)
Q(y1), P →Q(y0), Q(y1), ∃zQ(z)
Q(y1), P →∃zQ(z)
P, P ⊃Q(y1) →∃zQ(z)
P ⊃Q(y1) →P ⊃∃zQ(z)
∃x(P ⊃Q(x)) →P ⊃∃zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∃zQ(z))
At the end of the round,
TERM0 =<< y0, < ∃zQ(z) >>, < y1, < ∃zQ(z) >>> .

5.4 Proof Theory of First-Order Languages
203
This last tree is a proof tree for A.
EXAMPLE 5.4.4
Let
A = ∃x(P ⊃Q(x)) ⊃(P ⊃∀zQ(z)),
where P is a propositional letter and Q is a unary predicate symbol.
Initially, TERM0 =<< y0, nil >>. After the ﬁrst round, the following
tree is obtained and TERM0 and AV AIL0 are unchanged:
∃x(P ⊃Q(x)) →P ⊃∀zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∀zQ(z))
During the second round, the ∃: left rule is applied and the following
tree is obtained:
P, P ⊃Q(y1) →∀zQ(z)
P ⊃Q(y1) →P ⊃∀zQ(z)
∃x(P ⊃Q(x)) →P ⊃∀zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∀zQ(z))
At the end of this round,
TERM0 =<< y0, nil >, < y1, nil >>,
and
AV AIL0 =< y2, y3, ... > .
During the next round, the ∀: right rule is applied to the formula
∀zQ(z) with the new variable y2. The following tree is obtained:
P →P, ∀zQ(z)
Q(y1), P →Q(y2)
Q(y1), P →∀zQ(z)
P, P ⊃Q(y1) →∀zQ(z)
P ⊃Q(y1) →P ⊃∀zQ(z)
∃x(P ⊃Q(x)) →P ⊃∀zQ(z)
→∃x(P ⊃Q(x)) ⊃(P ⊃∀zQ(z))
At the end of this round,
TERM0 =<< y0, nil >, < y1, nil >, < y2, nil >>,

204
5/First-Order Logic
and
AV AIL0 =< y3, y4, ... > .
Since all formulae in the sequent Q(y1), P →Q(y2) are atomic and it
is not an axiom, this last tree yields a counter example with domain
{y0, y1, y2}, by making Q arbitrarily true for y0 and y1, false for y2,
and P true. Note that {y1, y2} is also the domain of a counter example.
The following theorem is a generalization of theorem 3.4.1 to ﬁnite se-
quents in ﬁrst-order languages without function symbols (and without equal-
ity). Recall that a closed tree is ﬁnite by deﬁnition.
Theorem 5.4.1
Let Γ →∆be an input sequent in which no free variable
occurs bound. (i) If Γ →∆is valid, then the procedure search halts with a
closed tree T which is a proof tree for Γ →∆.
(ii) If Γ →∆is falsiﬁable, then either search halts with a ﬁnite counter-
example tree T and Γ →∆can be falsiﬁed in a ﬁnite structure, or search
generates an inﬁnite tree T and Γ →∆can be falsiﬁed in a countably inﬁnite
structure.
Proof : First, assume that the sequent Γ →∆is falsiﬁable. If the tree T
was closed, by lemma 5.4.3, Γ →∆would be valid, a contradiction. Hence,
either T is ﬁnite and contains some path to a nonaxiom leaf, or T is inﬁnite
and by K¨onig’s lemma contains an inﬁnite path. In either case, we show as
in theorem 3.5.1 that a Hintikka set can be found along that path. Let U be
the union of all formulae occurring in the left-hand side of each sequent along
that path, and V the union of all formulae occurring in the right-hand side of
any such sequent. Let
S = {TA | A ∈U} ∪{FB | B ∈V }.
We prove the following claim:
Claim: S is a Hintikka set with respect to the term algebra consisting
of the set H of terms in t(TERM0).
Conditions H0, H1, and H2 are proved as in the propositional case (proof
of lemma 3.5.2), and we only have to check that t(TERM0) is a term algebra,
and conditions H3, H4, and H5. Since L does not contain function symbols,
t(TERM0) is trivially closed under the operations (there are none). Since the
constants occurring in the input sequent are put in t(TERM0), t(TERM0) is a
term algebra. Since t(TERM0) is initialized with the list of free variables and
constants occurring in the input sequent (or y0 if this list is empty), and since
every time a variable y is removed from the head of AV AIL0, < y, < nil >>
is added to TERM0, H5 holds. Every time a formula C of type c is expanded,
all substitution instances C(t) for all t in t(TERM0) that have not already
been used with C are added to the upper sequent. Hence, H3 is satisﬁed.
Every time a formula D of type d is expanded, y is added to t(TERM0) and

PROBLEMS
205
the substitution instance D(y) is added to the upper sequent. Hence, H4 is
satisﬁed, and the claim holds.
By lemma 5.4.5, some assignment s satisﬁes S in the structure HS. This
implies that Γ →∆is falsiﬁed by HS and s. Note that H must be inﬁnite if
the tree T is inﬁnite. Otherwise, since we start with a ﬁnite sequent, every
path would be ﬁnite and would end either with an axiom or a ﬁnished sequent.
If the sequent Γ →∆is valid the tree T must be ﬁnite and closed since
otherwise, the above argument shows that Γ →∆is falsiﬁable.
As a corollary, we obtain a version of G¨odel’s completeness theorem for
ﬁrst-order languages without function symbols or equality.
Corollary
If a sequent in which no variable occurs both free and bound
(over a ﬁrst-order language without function symbols or equality) is valid then
it is G-provable.
In Section 5.5, we shall modify the search procedure to handle function
symbols and possibly inﬁnite sequents. Finally in Section 5.6, we will adapt
the procedure to deal with languages with equality.
PROBLEMS
5.4.1.
Give proof trees for the following formulae:
∀xA ⊃A[t/x],
A[t/x] ⊃∃xA,
where t is free for x in A.
5.4.2.
Let x, y be any distinct variables.
Let A be any formula, C any
formula not containing the variable x free, and let E be any formula
such that x is free for y in E.
Give proof trees for the following
formulae:
∀xC ≡C
∃xC ≡C
∀x∀yA ≡∀y∀xA
∃x∃yA ≡∃y∃xA
∀x∀yE ⊃∀xE[x/y]
∃xE[x/y] ⊃∃x∃yE
∀xA ⊃∃xA
∃x∀yA ⊃∀y∃xA
5.4.3.
Let A, B be any formulae, and C any formula not containing the
variable x free. Give proof trees for the following formulae:
¬∃xA ≡∀x¬A
¬∀xA ≡∃x¬A
∃xA ≡¬∀x¬A
∀xA ≡¬∃x¬A
∀xA ∧∀xB ≡∀x(A ∧B)
∃xA ∨∃xB ≡∃x(A ∨B)
C ∧∀xA ≡∀x(C ∧A)
C ∨∃xA ≡∃x(C ∨A)
C ∧∃xA ≡∃x(C ∧A)
C ∨∀xA ≡∀x(C ∨A)
∃x(A ∧B) ⊃∃xA ∧∃xB
∀xA ∨∀xB ⊃∀x(A ∨B)

206
5/First-Order Logic
5.4.4.
Let A, B be any formulae, and C any formula not containing the
variable x free. Give proof trees for the following formulae:
(C ⊃∀xA) ≡∀x(C ⊃A)
(C ⊃∃xA) ≡∃x(C ⊃A)
(∀xA ⊃C) ≡∃x(A ⊃C)
(∃xA ⊃C) ≡∀x(A ⊃C)
(∀xA ⊃∃xB) ≡∃x(A ⊃B)
(∃xA ⊃∀xB) ⊃∀x(A ⊃B)
5.4.5.
Give a proof tree for the following formula:
¬∀x∃y(¬P(x) ∧P(y)).
5.4.6.
Show that the rules ∀: right and ∃: left are not necessarily sound if
y occurs free in the lower sequent.
∗5.4.7.
Let L be a ﬁrst-order language without function symbols and with-
out equality. A ﬁrst-order formula A is called simple if, for every
subformula of the form ∀xB or ∃xB, B is quantiﬁer free.
Prove that the search procedure always terminates for simple for-
mulae. Conclude that there is an algorithm for deciding validity of
simple formulae.
5.4.8.
Let Γ →A be a ﬁnite sequent, let x be any variable occurring free
in Γ →A, and let Γ[c/x] →A[c/x] be the result of substituting any
constant c for x in all formulae in Γ and in A.
Prove that if Γ[c/x] →A[c/x] is provable, then Γ[y/x] →A[y/x] is
provable for every variable y not occurring in Γ or in A. Conclude
that if x does not occur in Γ, if Γ →A[c/x] is provable, then Γ →∀xA
is provable.
∗5.4.9.
The language of a binary relation (simple directed graphs) consists of
one binary predicate symbol R. The axiom for simple directed graphs
is:
∀x∀y(R(x, y) ⊃¬R(y, x)).
This expresses that a simple directed graph has no cycles of length
≤2. Let T1 contain the above axiom together with the axiom
∀x∃yR(x, y)
asserting that every node has an outgoing edge.
(a) Find a model for T1 having three elements.
Let T2 be T1 plus the density axiom:
∀x∀y(R(x, y) ⊃∃z(R(x, z) ∧R(z, y))).
(b) Find a model for T2. Find a seven-node graph model of T2.

5.5 Completeness for Languages with Function Symbols and no Equality
207
5.5 Completeness for Languages with Function Symbols
and no Equality
In order to deal with (countably) inﬁnite sequents, we shall use the technique
for building a tree used in Section 3.5. First, we need to deal with function
symbols.
5.5.1 Organizing the Terms for Languages with Function
Symbols and no Equality
Function symbols are handled in the following way.
First, we can assume
without loss of generality that the set of variables V is the disjoint union of
two countably inﬁnite sets {x0, x1, x2, ...} and {y0, y1, y2, ...} and that only
variables in the ﬁrst set (the x’s) are used to build formulae. In this way,
{y0, y1, y2, ...} is an inﬁnite supply of new variables. Let Γ →∆be a possibly
inﬁnite sequent. As before, using lemma 5.3.4, we can assume that the set of
variables occurring free in Γ →∆is disjoint from the set of variables occurring
bound in Γ →∆.
Let L′ be the reduct of L consisting of the constant, function and predi-
cate symbols occurring in formulae in Γ →∆. If the set of constants and free
variables occurring in Γ →∆is nonempty, let
TERMS =< u0, u1, ..., uk, ... >
be an enumeration of all L′-terms constructed from the variables occurring
free in formulae in Γ →∆, and the constant and function symbols in L′.
Otherwise, let
TERMS =< y0, u1, ..., uk, ... >
be an enumeration of all the terms constructed from the variable y0 and the
function symbols in L′.
Let
TERM0 =<< head(TERMS), nil >>
and let
AV AIL0 = tail(TERMS).
For any i ≥1, let AV AILi be an enumeration of the set of all L′-terms actually
containing some occurrence of yi and constructed from the variables occurring
free in formulae in Γ →∆, the constant and function symbols occurring in
Γ →∆, and the variables y1,...,yi.
We assume that such an enumeration
begins with yi and is of the form
AV AILi = < yi, ui,1, ..., ui,j, .. > .

208
5/First-Order Logic
EXAMPLE 5.5.1
Assume that L′ contains a constant symbol a, and the function symbols
f of rank 2, and g of rank 1. Then, a possible enumeration of TERMS
is the following:
TERMS =< a, g(a), f(a, a), g(g(a)), g(f(a, a)), f(g(a), a),
f(a, g(a)), f(g(a), g(a)), ... > .
Hence, TERM0 =<< a, nil >>, and
AV AIL0 =< g(a), f(a, a), g(g(a)), g(f(a, a)),
f(g(a), a), f(a, g(a)), f(g(a), g(a)), ... > .
For i = 1, a possible enumeration of AV AIL1 is:
AV AIL1 =< y1, g(y1), f(a, y1), f(y1, a), f(y1, y1),
g(g(y1)), g(f(a, y1)), g(f(y1, a)), g(f(y1, y1)), ... > .
Each time a rule ∀: right or ∃: left is applied, we use as the variable y
the ﬁrst yi of y1,y2,... not yet activated, append yi to t(TERM0), and delete
yi from the list AV AILi. We use a counter NUMACT to record the number
of variables y1,y2,... thus far activated. Initially NUMACT = 0, and every
time a new yi is activated NUMACT is incremented by 1.
In a ∀: left or a ∃: right step, all the terms in t(TERM0) thus far
activated are available as terms t. Furthermore, at the end of every round,
the head of every available list AV AILi thus far activated (with 0 ≤i ≤
NUMACT) is appended to t(TERM0) and deleted from AV AILi. Thus,
along any path in the tree that does not close, once any term in an AV AILi
list is activated, every term in the list is eventually activated.
Note that if the sets FS and CS are eﬀective (recursive), recursive func-
tions enumerating the lists t(TERM0) and AV AILi (i ≥0) can be written
(these lists are in fact recursive).
The deﬁnition of a ﬁnished leaf is the following: A leaf of the tree is
ﬁnished if either
(1) It is an axiom, or
(2) L and R (as deﬁned in subsection 3.5.2) are empty, and the se-
quent only contains atoms or formulae ∀xA (or ∃xA) belonging to all lists
FORM0(i), for all < ui, FORM0(i) > in TERM0.

5.5 Completeness for Languages with Function Symbols and no Equality
209
5.5.2 The Search Procedure for Languages with Func-
tion Symbols and no Equality
The search procedure (and its subprograms) are revised as follows.
Deﬁnition 5.5.1
The search procedure.
Procedure Search
procedure search(Γ0 →∆0 : sequent; var T : tree);
begin
L := tail(Γ0); Γ := head(Γ0);
R := tail(∆0); ∆:= head(∆0);
let T be the one-node tree labeled with Γ →∆;
Let TERM0 and AVAILi, 0 ≤i,
be initialized as explained in section 5.5.1.
NUMACT := 0;
while not all leaves of T are ﬁnished do
TERM1 := TERM0; T0 := T;
for each leaf node of T0
(in lexicographic order of tree addresses) do
if not finished(node) then
expand(node, T)
endif
endfor;
TERM0 := TERM1; L := tail(L); R := tail(R);
for i := 0 to NUMACT do
TERM0 := append(TERM0, < head(AVAILi), nil >);
AVAILi := tail(AVAILi)
endfor
endwhile;
if all leaves are closed
then
write (‘T is a proof of Γ0 →∆0’)
else
write (‘Γ0 →∆0 is falsiﬁable’)
endif
end
The Procedures expand, grow-left, and grow-right appear on the next
two pages.

210
5/First-Order Logic
procedure expand(node : tree-address; var T : tree);
begin
let A1, ..., Am →B1, ..., Bn be the label of node;
let S be the one-node tree labeled with
A1, ..., Am →B1, ..., Bn;
for i := 1 to m do
if nonatomic(Ai) then
grow-left(Ai, S)
endif
endfor;
for i := 1 to n do
if nonatomic(Bi) then
grow-right(Bi, S)
endif
endfor;
for each leaf u of S do
let Γ →∆be the label of u;
Γ′ := Γ, head(L);
∆′ := ∆, head(R);
create a new node u1 labeled with Γ′ →∆′
endfor; T := dosubstitution(T, node, S)
end
procedure grow-left(A : formula; var S : tree);
begin
case A of
B ∧C, B ∨C,
B ⊃C, ¬B
: extend every nonaxiom leaf of S using the
left rule corresponding to the main
propositional connective;
∀xB
: for every term uk ∈t(TERM0)
such that A is not in FORM0(k) do
extend every nonaxiom leaf of S by applying
the ∀: left rule using the term uk
as one of the terms of the rule. In TERM1
let FORM1(k) := append(FORM1(k), A)
endfor;
∃xB
: NUMACT := NUMACT + 1;
extend every nonaxiom leaf of S by applying
the ∃: left rule using y = head(AVAILNUMACT )
as the new variable;
TERM1 := append(TERM1, < y, nil >);
AVAILNUMACT := tail(AVAILNUMACT )
endcase
end

5.5 Completeness for Languages with Function Symbols and no Equality
211
procedure grow-right(A : formula; var S : tree);
begin
case A of
B ∧C, B ∨C,
B ⊃C, ¬B
: extend every nonaxiom leaf of S using the
right rule corresponding to the main
propositional connective;
∃xB
: for every term uk ∈t(TERM0)
such that A is not in FORM0(k) do
extend every nonaxiom leaf of S by applying
the ∃: right rule using the term uk
as one of the terms of the rule. In TERM1
let FORM1(k) := append(FORM1(k), A)
endfor;
∀xB
: NUMACT := NUMACT + 1;
extend every nonaxiom leaf of S by applying
the ∀: right rule using y = head(AVAILNUMACT )
as the new variable;
TERM1 := append(TERM1, < y, nil >);
AVAILNUMACT := tail(AVAILNUMACT )
endcase
end
Let us give an example illustrating this new version of the search pro-
cedure.
EXAMPLE 5.5.2
Let
A = (∀xP(x) ∧∃yQ(y)) ⊃(P(f(v)) ∧∃zQ(z)),
where P and Q are unary predicate symbols, and f is a unary function
symbol. The variable v is free in A. Initially,
TERM0 =<< v, nil >>,
AV AIL0 =< f(v), f(f(v)), ..., f n(v), ... >,
and for i ≥1,
AV AILi =< yi, f(yi), ..., f n(yi), ... > .
After the ﬁrst round, we have the following tree:
∀xP(x) ∧∃yQ(y) →P(f(v)) ∧∃zQ(z)
→∀xP(x) ∧∃yQ(y) ⊃P(f(v)) ∧∃zQ(z)

212
5/First-Order Logic
At the end of this round,
TERM0 =<< v, nil >, < f(v), nil >>,
AV AIL0 =< f 2(v), ..., f n(v), ... >,
and for i ≥1, AV AILi is unchanged. After the second round, we have
the following tree:
∀xP(x), ∃yQ(y) →P(f(v))
∀xP(x), ∃yQ(y) →∃zQ(z)
∀xP(x), ∃yQ(y) →P(f(v)) ∧∃zQ(z)
∀xP(x) ∧∃yQ(y) →P(f(v)) ∧∃zQ(z)
→∀xP(x) ∧∃yQ(y) ⊃P(f(v)) ∧∃zQ(z)
At the end of this round,
TERM0 =<< v, nil >, < f(v), nil >, < f(f(v)), nil >>,
AV AIL0 =< f 3(v), ..., f n(v), ... >,
and for i ≥1, AV AILi is unchanged. After the third round, we have
the following tree:
T1
∀xP(x), ∃yQ(y) →P(f(v))
T2
∀xP(x), ∃yQ(y) →∃zQ(z)
∀xP(x), ∃yQ(y) →P(f(v)) ∧∃zQ(z)
∀xP(x) ∧∃yQ(y) →P(f(v)) ∧∃zQ(z)
→∀xP(x) ∧∃yQ(y) ⊃P(f(v)) ∧∃zQ(z)
where the tree T1 is the proof tree
P(v), P(f(v)), P(f(f(v))), ∀xP(x), ∃yQ(y) →P(f(v))
∀xP(x), ∃yQ(y) →P(f(v))
and the tree T2 is the tree
Γ, ∀xP(x), Q(y1) →Q(v), Q(f(v)), Q(f(f(v))), ∃zQ(z)
P(v), P(f(v)), P(f(f(v))), ∀xP(x), Q(y1) →∃zQ(z)
P(v), P(f(v)), P(f(f(v))), ∀xP(x), ∃yQ(y) →∃zQ(z)
∀xP(x), ∃yQ(y) →∃zQ(z)

5.5 Completeness for Languages with Function Symbols and no Equality
213
where
Γ = P(v), P(f(v)), P(f(f(v))).
At the end of this round,
TERM0 =<< v, < ∀xP(x), ∃zQ(z) >>, < f(v), < ∀xP(x), ∃zQ(z) >>,
< f(f(v)), < ∀xP(x), ∃zQ(z) >>, < y1, nil >, < f 3(v), nil >,
< f(y1), nil >>,
AV AIL0 =< f 4(v), ..., f n(v), ... >,
AV AIL1 =< f 2(y1), ..., f n(y1), ... >,
and for i > 1, AV AILi is unchanged. At the end of the fourth round,
T2 expands into the following proof tree:
Γ′, Q(y1) →Q(v), Q(f(v)), Q(f(f(v))), Q(y1), Q(f 3(v)), Q(f(y1)), ∃zQ(z)
Γ, ∀xP(x), Q(y1) →Q(v), Q(f(v)), Q(f(f(v))), ∃zQ(z)
P(v), P(f(v)), P(f(f(v))), ∀xP(x), Q(y1) →∃zQ(z)
P(v), P(f(v)), P(f(f(v))), ∀xP(x), ∃yQ(y) →∃zQ(z)
∀xP(x), ∃yQ(y) →∃zQ(z)
where
Γ′ = P(v), P(f(v)), P(f(f(v))), Q(y1), Q(f 3(v)), Q(f(y1)), ∀xP(x).
EXAMPLE 5.5.3
Let
A = (∃xP(x) ∧Q(a)) ⊃∀yP(f(y)).
Initially, TERM0 =<< a, nil >>, and
AV AIL0 =< f(a), f 2(a), ..., f n(a), ... > .
At the end of the ﬁrst round, the following tree is obtained:
∃xP(x) ∧Q(a) →∀yP(f(y))
→∃xP(x) ∧Q(a) ⊃∀yP(f(y))
At the end of the round,
TERM0 =<< a, nil >, < f(a), nil >>,
and
AV AIL0 =< f 2(a), ..., f n(a), ... > .

214
5/First-Order Logic
At the end of the second round, the following tree is obtained:
∃xP(x), Q(a) →P(f(y1))
∃xP(x) ∧Q(a) →∀yP(f(y))
→∃xP(x) ∧Q(a) ⊃∀yP(f(y))
At the end of this round,
TERM0 =<< a, nil >, < f(a), nil >, < y1, nil >, < f(y1), nil >,
< f 2(a), nil >>,
AV AIL0 =< f 3(a), ..., f n(a), ... >, and
AV AIL1 =< f 2(y1), ..., f n(y1), ... > .
At the end of the third round, the following tree is obtained:
P(y2), Q(a) →P(f(y1))
∃xP(x), Q(a) →P(f(y1))
∃xP(x) ∧Q(a) →∀yP(f(y))
→∃xP(x) ∧Q(a) ⊃∀yP(f(y))
This tree is a ﬁnished nonclosed tree, since all formulae in the top sequent
are atomic. A counter example is given by the structure having
H =< a, f(a), ..., f n(a), ..., y1, f(y1), ..., f n(y1), ...,
y2, f(y2), ..., f n(y2), ... >
as its domain, and by interpreting P as taking the value T for y2, F for
f(y1), and Q taking the value T for a.
5.5.3 Completeness of the System G (Languages With-
out Equality)
We can now prove the following fundamental theorem.
Theorem 5.5.1 Let Γ0 →∆0 be an input sequent in which no free variable
occurs bound. (i) If Γ0 →∆0 is valid, then the procedure search halts with
a closed tree T, from which a proof tree for a ﬁnite subsequent C1, ..., Cm →
D1, ..., Dn of Γ0 →∆0 can be constructed.
(ii) If Γ0 →∆0 is falsiﬁable, then there is a Hintikka set S such that
either
(a) Procedure search halts with a ﬁnite counter-example tree T and,
if we let
HS = t(TERM0) ∪{AV AILi | 0 ≤i ≤NUMACT},

5.5 Completeness for Languages with Function Symbols and no Equality
215
Γ0 →∆0 is falsiﬁable in a structure with countable domain HS; or
(b) Procedure search generates an inﬁnite tree T and, if we let HS =
t(TERM0), then Γ0 →∆0 is falsiﬁable in a structure with count-
able domain HS.
Proof : The proof combines techniques from the proof of theorem 5.4.1
and the proof of theorem 3.5.1. The diﬀerence with the proof of theorem 5.4.1
is that a closed tree T is not exactly a proof tree, and that it is necessary to
modify the tree T in order to obtain a proof tree.
Assume that search produces a closed tree, and let C1, ..., Cm be the
initial subsequence of formulae in Γ which were deleted from Γ to obtain L, and
D1, ..., Dn the initial subsequence of formulae in ∆which were deleted from
∆to obtain R. A proof tree for a the ﬁnite sequent C1, ..., Cm →D1, ..., Dn
can easily be obtained from T, using the following technique:
First, starting from the root and proceeding bottom-up, for each node
Γ, head(L) →∆, head(R) at depth k created at the end of a call to procedure
expand, add head(L) after the rightmost formula in the premise of every
sequent at depth less than k, and add head(R) after the rightmost formula
in the conclusion of every sequent at depth less than k, obtaining the tree T ′.
Then, a proof tree T ′′ for C1, ..., Cm →D1, ..., Dn is constructed from T ′ by
deleting all duplicate nodes. The tree T ′′ is a proof tree because the same
inference rules that have been used in T are used in T ′′.
If T is not a closed tree, along a ﬁnite or inﬁnite path we obtain a set
S of signed formulae as in the proof of theorem 5.4.1. We show the following
claim:
Claim: The set HS of terms deﬁned in clause (ii) of theorem 5.5.1 is a
term algebra (over the reduct L′), and S is a Hintikka set with respect to HS.
To prove the claim, as in theorem 5.4.1, we only need to prove that HS
is a term algebra, and that H3, H4, and H5 hold. We can assume that L′
contains function symbols since the other case has been covered by theorem
5.4.1. Since L′ contains function symbols, all the sets AV AILi (i ≥0) are
countably inﬁnite.
First, observe that every variable free in S is either a variable free in the
input sequent or one of the activated variables. Since
(i) t(TERM0) and AV AIL0 are initialized in such a way that the vari-
ables free in the input sequent belong to the union of t(TERM0) and AV AIL0,
(ii) Whenever a new variable yi is removed from the head of the list
AV AILi it is added to t(TERM0) and,
(iii) At the end of every round the head of every activated list (of the
form AV AILi, for 0 ≤i ≤NUMACT) is removed from that list and added

216
5/First-Order Logic
to t(TERM0), it follows from (i)-(iii) that all variables free in S are in
t(TERM0) ∪{AV AILi | 0 ≤i ≤NUMACT} if T is ﬁnite or in t(TERM0)
if T is inﬁnite, and condition H5 holds.
Observe that if T is ﬁnite, from (i), (ii), and (iii) it also follows that
t(TERM0) ∪{AV AILi | 0 ≤i ≤NUMACT} contains the set of all terms
built up from the variables free in the input sequent, the variables activated
during applications of ∃: left or ∀: right rules, and the constant and func-
tion symbols occurring in the input sequent. Hence, HS is closed under the
function symbols in L′, and it is a term algebra.
If T is inﬁnite, all the terms in all the activated lists AV AILi are even-
tually transferred to t(TERM0), and conditions (i) to (iii) also imply that
t(TERM0) contains the set of all terms built up from the variables free in
the input sequent, the variables activated during applications of ∃: left or
∀: right rules, and the constant and function symbols occurring in the input
sequent. Hence, HS = t(TERM0) is a term algebra.
If T is inﬁnite, condition H5 also follows from (i) to (iii).
Every time a formula C of type c is expanded, all substitution instances
C(t) for all t in t(TERM0) which have not already been used with C are
added to the upper sequent. Hence, H3 is satisﬁed. Every time a formula D
of type d is expanded, the new variable yi removed from AV AILi is added to
t(TERM0) and the substitution instance D(yi) is added to the upper sequent.
Hence, H4 is satisﬁed, and the claim holds.
Since S is a Hintikka set, by lemma 5.4.5, some assignment s satisﬁes S
in a structure HS with domain HS, and so, Γ0 →∆0 is falsiﬁed by HS and
s.
Corollary
(A version of G¨odel’s extended completeness theorem for G) A
sequent in which no variable occurs both free and bound (even inﬁnite) is
valid iﬀit is G-provable.
As a second corollary, we obtain the following useful result.
Corollary
There is an algorithm for deciding whether a ﬁnite sequent con-
sisting of quantiﬁer-free formulae is valid.
Proof : Observe that for quantiﬁer-free formulae, the search procedure
never uses the quantiﬁer rules. Hence, it behaves exactly as in the proposi-
tional case, and the result follows from theorem 3.4.1.
Unfortunately, this second corollary does not hold for all formulae. In-
deed, it can be shown that there is no algorithm for deciding whether any
given ﬁrst-order formula is valid.
This is known as Church’s theorem.
A
proof of Church’s theorem can be found in Enderton, 1972, or Kleene, 1952.
A particularly concise and elegant proof due to Floyd is also given in Manna,
1974.

5.5 Completeness for Languages with Function Symbols and no Equality
217
The completeness theorem only provides a semidecision procedure, in the
sense that if a formula is valid, this can be demonstrated in a ﬁnite number
of steps, but if it is falsiﬁable, the procedure may run forever.
Even though the search procedure provides a rather natural proof pro-
cedure which is theoretically complete, in practice it is extremely ineﬃcient
in terms of the number of steps and the amount of memory needed.
This is illustated by the very simple formulae of example 5.5.2 and ex-
ample 5.5.3 for which it is already laborious to apply the search procedure.
The main diﬃculty is the proper choice of terms in applications of ∀: left
and ∃: right rules. In particular, note that the ordering of the terms in the
lists t(TERM0) and AV AILi can have a drastic inﬂuence on the length of
proofs.
After having tried the search procedure on several examples, the fol-
lowing fact emerges: It is highly desirable to perform all the quantiﬁer rules
ﬁrst, in order to work as soon as possible on quantiﬁer-free formulae. Indeed,
by the second corollary to the completeness theorem, proving quantiﬁer-free
formulae is a purely mechanical process.
It will be shown in the next chapter that provided that the formulae in
the input sequent have a certain form, if such a sequent is provable, then it
has a proof in which all the quantiﬁer inferences are performed below all the
propositional inferences. This fact will be formally established by Gentzen’s
sharpened Hauptsatz, proved in Chapter 7. Hence, the process of ﬁnding a
proof can be viewed as a two-step procedure:
(1) In the ﬁrst step, one attempts to “guess” the right terms used in
∀: left and ∃: right inferences;
(2) In the second step, one checks that the quantiﬁer-free formula ob-
tained in step 1 is a tautology.
A rigorous justiﬁcation of this method will be given by Herbrand’s the-
orem, proved in Chapter 7. The resolution method for ﬁrst-order logic pre-
sented in Chapter 8 can be viewed as an improvement of the above method.
For the time being, we consider some applications of theorem 5.5.1. The
following theorems are easily obtained as consequences of the main theorem.
5.5.4 L¨owenheim-Skolem, Compactness, and Model Ex-
istence Theorems for Languages Without Equality
The following result known as L¨owenheim-Skolem’s theorem is often used in
model theory.
Theorem 5.5.2
(L¨owenheim-Skolem) If a set of formulae Γ is satisﬁable in
some structure M, then it is satisﬁable in a structure whose domain is at most
countably inﬁnite.

218
5/First-Order Logic
Proof : It is clear that Γ →is falsiﬁable in M. By theorem 5.5.1, the
search procedure yields a tree from which a Hintikka set S can be obtained.
But the domain H of HS is at most countably inﬁnite since it consists of
terms built from countable sets. Hence, HS is a countable structure in which
Γ is satisﬁable.
The other results obtained as consequences of theorem 5.5.1 are coun-
terparts of theorem 3.5.3, theorem 3.5.4, and lemma 3.5.4. The proofs are
similar and use structures instead of valuations.
Theorem 5.5.3
(Compactness theorem) For any (possibly countably inﬁ-
nite) set Γ of formulae, if every nonempty ﬁnite subset of Γ is satisﬁable then
Γ is satisﬁable.
Proof : Similar to that of theorem 3.5.3.
Recall that a set Γ of formulae is consistent if there exists some formula
A such that C1, ..., Cm →A is not G-provable for any C1, ..., Cm in Γ.
Theorem 5.5.4
(Model existence theorem) If a set Γ of formulae is consis-
tent then it is satisﬁable.
Proof : Similar to that of theorem 3.5.4.
Note that the search procedure will actually yield a structure HS for
each Hintikka set S arising along each nonclosed path in the tree T, in which
S and Γ are satisﬁable.
Lemma 5.5.1 (Consistency lemma) If a set Γ of formulae is satisﬁable then
it is consistent.
Proof : Similar to that of lemma 3.5.4.
5.5.5 Maximal Consistent Sets
The concept of a maximal consistent set introduced in deﬁnition 3.5.9 is gen-
eralized directly to sets of ﬁrst-order formulae:
A consistent set Γ of ﬁrst-order formulae (without equality) is maximally
consistent (or a maximal consistent set) iﬀ, for every consistent set ∆, if
Γ ⊆∆, then Γ = ∆. Equivalently, every proper superset of Γ is inconsistent.
Lemma 3.5.5 can be easily generalized to the ﬁrst-order case.
Lemma 5.5.2
Given a ﬁrst-order language (without equality), every con-
sistent set Γ is a subset of some maximal consistent set ∆.
Proof : Almost identical to that of lemma 3.5.5, but using structures
instead of valuations.

PROBLEMS
219
It is possible as indicated in Section 3.5 for propositional logic to prove
lemma 5.5.2 directly, and use lemma 5.5.2 to prove the model existence theo-
rem (theorem 5.5.4). From the model existence theorem, the extended G¨odel
completeness theorem can be shown (corollary to theorem 5.5.1). Using this
approach, it is necessary to use a device due to Henkin known as adding wit-
nesses. Roughly speaking, this is necessary to show that a Henkin maximal
consistent set is a Hintikka set with respect to a certain term algebra. The
addition of witnesses is necessary to ensure condition H4 of Hintikka sets.
Such an approach is explored in the problems and for details, we refer the
reader to Enderton, 1972; Chang and Keisler, 1973; or Van Dalen, 1980.
PROBLEMS
5.5.1.
Using the search procedure, ﬁnd counter examples for the formulae:
∃xA ∧∃xB ⊃∃x(A ∧B)
∀y∃xA ⊃∃x∀yA
∃xA ⊃∀xA
5.5.2.
Using the search procedure, prove that the following formula is valid:
¬∃y∀x(S(y, x) ≡¬S(x, x))
5.5.3.
Using the search procedure, prove that the following formulae are
valid:
∀xA ⊃A[t/x],
A[t/x] ⊃∃xA,
where t is free for x in A.
5.5.4.
This problem is a generalization of the Hilbert system H of problem
3.4.9 to ﬁrst-order logic.
For simplicity, we ﬁrst treat the case of
ﬁrst-order languages without equality. The Hilbert system H for ﬁrst-
order logic (without equality) over the language using the connectives,
∧, ∨, ⊃, ¬, ∀and ∃is deﬁned as follows:
The axioms are all formulae given below, where A, B, C denote
arbitrary formulae.
A ⊃(B ⊃A)
(A ⊃B) ⊃((A ⊃(B ⊃C)) ⊃(A ⊃C))

220
5/First-Order Logic
A ⊃(B ⊃(A ∧B))
A ⊃(A ∨B),
B ⊃(A ∨B)
(A ⊃B) ⊃((A ⊃¬B) ⊃¬A)
(A ∧B) ⊃A,
(A ∧B) ⊃B
(A ⊃C) ⊃((B ⊃C) ⊃((A ∨B) ⊃C))
¬¬A ⊃A
∀xA ⊃A[t/x]
A[t/x] ⊃∃xA
where in the last two axioms, t is free for x in A.
There are three inference rules:
(i) The rule modus ponens given by:
A
(A ⊃B)
B
(ii) The generalization rules given by:
(B ⊃A)
(B ⊃∀xA) ∀: rule
(A ⊃B)
(∃xA ⊃B) ∃: rule
where x does not occur free in B.
Let {A1, ..., Am} be any set of formulae. The concept of a deduction
tree of a formula B from the set {A1, ..., Am} in the system H is
deﬁned inductively as follows:
(i) Every one-node tree labeled with an axiom B or a formula B in
{A1, ..., Am} is a deduction tree of B from {A1, ..., Am}.
(ii) If T1 is a deduction tree of A from {A1, ..., Am} and T2 is a de-
duction tree of (A ⊃B) from {A1, ..., Am}, then the following tree is
a deduction tree of B from {A1, ..., Am}:
T1
A
T2
(A ⊃B)
B
(iii) If T1 is a deduction tree of (B ⊃A) from {A1, ..., Am} (or a
deduction tree of (A ⊃B) from {A1, ..., Am}), and x does not occur
free in any of the formulae in {A1, ..., Am}, then the following trees
are deduction trees of (B ⊃∀xA) from {A1, ..., Am} (or (∃xA ⊃B)
from {A1, ..., Am}).

PROBLEMS
221
T1
(B ⊃A)
(B ⊃∀xA)
T1
(A ⊃B)
(∃xA ⊃B)
A proof tree is a deduction tree whose leaves are labeled with axioms.
Given a set {A1, ..., Am} of formulae and a formula B, we use the
notation A1, ..., Am ⊢B to denote that there is a deduction tree of B
from {A1, ..., Am}. In particular, if the set {A1, ..., Am} is empty, the
tree is a proof tree and we write ⊢B.
Prove that the generalization rules are sound rules, in the sense that
if the premise is valid, then the conclusion is valid. Prove that the
system H is sound; that is, every provable formula is valid.
5.5.5.
(i) Show that if A1, ..., Am, A ⊢B is a deduction in H not using the
generalization rules, then A1, ..., Am ⊢(A ⊃B).
(ii) Check that the following is a deduction of C from A ⊃(B ⊃C)
and A ∧B:
A ∧B
A ∧B ⊃B
B
A ∧B
A ∧B ⊃A
A
A ⊃(B ⊃C)
B ⊃C
C
Conclude that A ⊃(B ⊃C) ⊢(A ∧B) ⊃C.
(iii) Check that the following is a deduction of C from (A ∧B) ⊃C,
A, and B:
B
A
A ⊃(B ⊃(A ∧B))
B ⊃(A ∧B)
A ∧B
(A ∧B) ⊃C
C
Conclude that (A ∧B) ⊃C ⊢A ⊃(B ⊃C).
∗5.5.6.
In this problem, we are also considering the proof system H of prob-
lem 5.5.4. The deduction theorem states that, for arbitrary formulae
A1,...,Am,A,B,
if A1, ..., Am, A ⊢B, then A1, ..., Am ⊢(A ⊃B).

222
5/First-Order Logic
Prove the deduction theorem.
Hint: Use induction on deduction trees. In the case of the general-
ization rules, use problem 5.5.5.
5.5.7.
Prove the following:
If A1, ..., Am ⊢Bi for every i, 1 ≤i ≤m and
B1, ..., Bm ⊢C, then A1, ..., Am ⊢C.
Hint: Use the deduction theorem.
5.5.8.
Prove that for any set Γ of formulae, and any two formulae B and C,
if ⊢C
then
Γ, C ⊢B
iﬀ
Γ ⊢B.
∗5.5.9.
In this problem, we are still considering the proof system H of problem
5.5.4. Prove that the following meta rules hold about deductions in
the system H:
For all formulae A, B, C and ﬁnite sequence Γ of formulae (possibly
empty), we have:
Introduction
Elimination
⊃
If Γ, A ⊢B,
A, (A ⊃B) ⊢B
then Γ ⊢(A ⊃B)
∧
A, B ⊢(A ∧B)
(A ∧B) ⊢A
(A ∧B) ⊢B
∨
A ⊢(A ∨B)
If Γ, A ⊢C and Γ, B ⊢C
then Γ, (A ∨B) ⊢C
¬
If Γ, A ⊢B and Γ, A ⊢¬B
¬¬A ⊢A
then Γ ⊢¬A
(double negation elimination)
(reductio ad absurdum)
A, ¬A ⊢B
(weak negation elimination)
∀
If Γ ⊢A then
∀xA ⊢A[t/x]
Γ ⊢∀xA
∃
A[t/x] ⊢∃xA
If Γ, A ⊢C then
Γ, ∃xA ⊢C
where t is free for x in A, x does not occur free in Γ, and x does not
occur free in C.
Hint: Use problem 5.5.7, problem 5.5.8, and the deduction theorem.
∗5.5.10. In this problem it is shown that the Hilbert system H is complete, by
proving that for every Gentzen proof T of a sequent →A, where A
is any formula, there is a proof in the system H.

PROBLEMS
223
(i) Prove that for arbitrary formulae A1, ..., Am,B1, ..., Bn,
(a) in H, for n > 0,
A1, ..., Am, ¬B1, ..., ¬Bn ⊢P ∧¬P
if and only if
A1, ..., Am, ¬B1, ..., ¬Bn−1 ⊢Bn, and
(b) in H, for m > 0,
A1, ..., Am, ¬B1, ..., ¬Bn ⊢P ∧¬P
if and only if
A2, ..., Am, ¬B1, ..., ¬Bn ⊢¬A1.
(ii) Prove that for any sequent A1, ..., Am →B1, ..., Bn,
if ⊢A1, ..., Am →B1, ..., Bn in the Gentzen system G then
A1, ..., Am, ¬B1, ..., ¬Bn ⊢(P ∧¬P)
is a deduction in the Hilbert system H.
Conclude that H is complete.
Hint: Use problem 5.5.9.
5.5.11. Recall that the cut rule is the rule
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
Let G + {cut} be the formal system obtained by adding the cut rule
to G. The notion of a deduction tree is extended to allow the cut rule
as an inference. A proof in G is called a cut-free proof.
(i) Prove that for every structure A, if A satisﬁes the premises of the
cut rule, then it satisﬁes its conclusion.
(ii) Prove that if a sequent is provable in the system G + {cut}, then
it is valid.
(iii) Prove that if a sequent is provable in G + {cut}, then it has a
cut-free proof.
5.5.12. (i) Prove solely in terms of proofs in G+{cut} that a set Γ of formulae
is inconsistent if and only if there is a formula A such that both Γ →A
and Γ →¬A are provable in G + {cut}.
(ii) Prove solely in terms of proofs in G + {cut} that, Γ →A is not
provable in G + {cut} if and only if Γ ∪{¬A} is consistent.
Note: Properties (i) and (ii) also hold for the proof system G, but
the author does not know of any proof not involving a proof-theoretic
version of Gentzen’s cut elimination theorem. The completeness the-
orem for G provides a semantic proof of the cut elimination theorem.
However, in order to show (i) and (ii) without using semantic argu-
ments, it appears that one has to mimic Gentzen’s original proof.
(See Szabo, 1969.)

224
5/First-Order Logic
5.5.13. A set Γ of sentences is said to be complete if, for every sentence A,
either
⊢Γ →A
or
⊢Γ →¬A,
but not both. Prove that for any set Γ of sentences, the following
are equivalent:
(i) The set {A | ⊢Γ →A in G} is a maximal consistent set.
(ii) Γ is complete.
(iii) Any two models of Γ are elementary equivalent.
(See problem 5.3.25 for the deﬁnition of elementary equivalence.)
5.5.14. Let Γ be a consistent set (of L-formulae). Let A1, A2, ..., An,... be an
enumeration of all L-formulae. Deﬁne the sequence Γn inductively as
follows:
Γ0 = Γ,
Γn+1 =

Γn ∪{An+1}
if Γn ∪{An+1} is consistent;
Γn
otherwise.
Let
∆=

n≥0
Γn.
Prove the following:
(a) Each Γn is consistent.
(b) ∆is consistent.
(c) ∆is maximally consistent.
∗5.5.15. Prove that the results of problem 3.5.16 hold for ﬁrst-order logic if
we change the word proposition to sentence.
∗5.5.16. Prove that the results of problem 3.5.17 hold for ﬁrst-order logic if
we change the word proposition to sentence and work in G + {cut}.
∗5.5.17. In this problem and the next four, Henkin’s version of the complete-
ness theorem is worked out.
The approach is to prove the model
existence theorem, and derive the completeness theorem as a con-
sequence.
To prove that every consistent set S is satisﬁable, ﬁrst
problem 5.5.14 is used to extend S to a maximal consistent set. How-
ever, such a maximal consistent set is not necessarily a Hintikka set,
because condition H4 may not be satisﬁed. To overcome this problem,
we use Henkin’s method, which consists of adding to S formulae of
the form ∃xB ⊃B[c/x], where c is a new constant called a witness of
∃xB. However, we have to iterate this process to obtain the desired
property.

PROBLEMS
225
Technically, we proceed as follows. Let L be a ﬁrst-order language
(without equality). Let L∗be the extension of L obtained by adding
the set of new constants
{cD | D is any sentence of the form ∃xB}.
The constant cD is called a witness of D. Let S be any set of L-
sentences, and let
S∗= S ∪{∃xB ⊃B[cD/x] | ∃xB ∈FORML,
cD is a witness of the sentence D = ∃xB}.
(Note that ∃xB is any arbitrary existential sentence which need not
belong to S).
(a) Prove that for every L-sentence A, if S∗→A is provable in
G + {cut}, then S →A is provable in G + {cut}.
Hint: Let Γ be a ﬁnite subset of S∗such that Γ →A is provable
(in G + {cut}). Assume that Γ contains some sentence of the form
∃xB ⊃B[cD/x], where D = ∃xB. Let ∆= Γ −{∃xB ⊃B[cD/x]}.
Note that cD does not occur in ∆, ∃xB, or A.
Using the result
of problem 5.4.8, show that ∆, ∃xB ⊃B[y/x] →A is provable (in
G + {cut}), where y is a new variable not occurring in Γ →A. Next,
show that →∃x(∃xB ⊃B) is provable (in G). Then, show (using a
cut) that ∆→A is provable (in G + {cut}). Conclude by induction
on the number of sentences of the form ∃xB ⊃B[cD/x] in Γ.
(b) A set S of L-sentences is a theory iﬀit is closed under provability,
that is, iﬀ
S = {A | ⊢S →A in G + {cut}, FV (A) = ∅}.
A theory S is a Henkin theory iﬀfor any sentence D of the form ∃xB
in FORML (not necessarily in S), there is a constant c in L such
that ∃xB ⊃B[c/x] is also in S.
Let S be any set of L-sentences and let T be the theory
T = {A | ⊢S →A in G + {cut}, FV (A) = ∅}.
Deﬁne the sequence of languages Ln and the sequence of theories Tn
as follows:
L0 = L; Ln+1 = (Ln)∗; and
T0 = T; Tn+1 = {A | ⊢(Tn)∗→A in G + {cut}, FV (A) = ∅}.

226
5/First-Order Logic
Let
LH =

n≥0
Ln,
and
T H =

n≥0
Tn.
Prove that T H is a Henkin theory over the language LH.
(c) Prove that for every L-sentence C,
if ⊢T H →C in G + {cut},
then ⊢T →C in G + {cut}.
We say that T H is conservative over T.
(d) Prove that T H is consistent iﬀT is.
∗5.5.18. Let T be a consistent set of L-sentences.
(a) Show that there exists a maximal consistent extension T ′ over LH
of T which is also a Henkin theory.
Hint: Let S = {A | ⊢T →A, FV (A) = ∅}. Show that any max-
imally consistent extension of SH is also a Henkin theory which is
maximally consistent.
(b) The deﬁnition of formulae of type a, b, c, d for unsigned formulae
and of their immediate descendants given in problem 3.5.7 is extended
to the ﬁrst-order case.
Formulae of types a, b, c, d and their immediate descendants are
deﬁned by the following table:
Type-a formulae
A
A1
A2
(X ∧Y )
X
Y
¬(X ∨Y )
¬X
¬Y
¬(X ⊃Y )
X
¬Y
¬¬X
X
X
Type-b formulae
B
B1
B2
¬(X ∧Y )
¬X
¬Y
(X ∨Y )
X
Y
(X ⊃Y )
¬X
Y

PROBLEMS
227
Type-c formulae
C
C1
C2
∀xY
Y [t/x]
Y [t/x]
¬∃xY
¬Y [t/x]
¬Y [t/x]
where t is free for x in Y
Type-d formulae
D
D1
D2
∃xY
Y [t/x]
Y [t/x]
¬∀xY
¬Y [t/x]
¬Y [t/x]
where t is free for x in Y
The deﬁnition of a Hintikka set is also adapted in the obvious way to
unsigned formulae. Also, in this problem and some of the following
problems, given any set S of formulae and any formulae A1,...,An,
the set S ∪{A1, ...An} will also be denoted by {S, A1, ..., An}.
Prove that a maximal consistent Henkin set T ′ of sentences is a Hin-
tikka set with respect to the term algebra H consisting of all closed
terms built up from the function and constant symbols in LH.
(c) Prove that every consistent set T of L-sentences is satisﬁable.
(d) Prove that a formula A with free variables {x1, ..., xn} is satisﬁable
iﬀA[c1/x1, ..., cn/xn] is satisﬁable, where c1,...,cn are new constants.
Using this fact, prove that every consistent set T of L-formulae is
satisﬁable.
∗5.5.19. Recall that from problem 5.5.12, in G + {cut}, Γ →A is not provable
if and only if Γ ∪{¬A} is consistent. Use the above fact and problem
5.5.18 to give an alternate proof of the extended completeness theorem
for G + {cut} (|= Γ →A
iﬀ
⊢Γ →A in G + {cut}).
∗∗5.5.20. Prove that the results of problem 5.5.17 hold for languages of any
cardinality. Using Zorn’s lemma, prove that the results of problem
5.5.18 hold for languages of any cardinality. Thus, prove that the
extended completeness theorem holds for languages of any cardinality.
∗5.5.21. For a language L of cardinality α, show that the cardinality of the
term algebra arising in problem 5.5.18 is at most α. Conclude that
any consistent set of L-sentences has a model of cardinality at most
α.
∗5.5.22. Let L be a countable ﬁrst-order language without equality. A prop-
erty P of sets of formulae is an analytic consistency property iﬀthe
following hold:

228
5/First-Order Logic
(i) P is of ﬁnite character (see problem 3.5.12).
(ii) For every set S of formulae for which P is true, the following
conditions hold:
A0: S contains no atomic formula and its negation.
A1: For every formula A of type a in S, P holds for {S, A1} and
{S, A2}.
A2: For every formula B of type b in S, either P holds for {S, B1} or
P holds for {S, B2}.
A3: For every formula C of type c in S, P holds for {S, C(t)} for
every term t.
A4: For every formula D of type d in S, for some constant c not
occurring in S, P holds for {S, D(c)}.
Prove that if P is an analytic consistency property and P holds for
S, then S is satisﬁable.
Hint: The search procedure can be adapted to work with sets of for-
mulae rather than sequents, by identifying each sequent Γ →∆with
the set of formulae Γ∪{¬B | B ∈∆}, and using the rules correspond-
ing to the deﬁnition of descendants given in problem 5.5.18. Using
the search procedure applied to the set S (that is, to the sequent
S →), show that at any stage of the construction of a deduction tree,
there is a path such that P holds for the union of S and the union
of the formulae along that path. Also, the constant c plays the same
role as a new variable.
∗5.5.23. Let L be a countable ﬁrst-order language without equality.
A set
S of formulae is truth-functionally inconsistent iﬀS is the result of
substituting ﬁrst-order formulae for the propositional letters in an
unsatisﬁable set of propositions (see deﬁnition 5.3.11). We say that a
formula A is truth-functionally valid iﬀit is obtained by substitution
of ﬁrst-order formulae into a tautology (see deﬁnition 5.3.11). We say
that a ﬁnite set S = {A1, ..., Am} truth-functionally implies B iﬀthe
formula (A1 ∧...∧Am) ⊃B is truth-functionally valid. A property P
of sets of formulae is a synthetic consistency property iﬀthe following
conditions hold:
(i) P is of ﬁnite character (see problem 3.5.12).
(ii) For every set S of formulae, the following conditions hold:
B0: If S is truth-functionally inconsistent, then P does not hold for
S;
B3: If P holds for S then for every formula C of type c in S, P holds
for {S, C(t)} for every term t.

PROBLEMS
229
B4: For every formula D of type d in S, for some constant c not
occurring in S, P holds for {S, D(c)}.
B5: For every formula X, if P does not hold for {S, X} or {S, ¬X},
then P does not hold for S. Equivalently, if P holds for S, then for
every formula X, either P holds for {S, X} or P holds for {S, ¬X}
(a) Prove that if P is a synthetic consistency property then the fol-
lowing condition holds:
B6: If P holds for S and a ﬁnite subset of S truth-functionally implies
X, then P holds for {S, X}.
(b) Prove that every synthetic consistency property is an analytic
consistency property.
(c) Prove that consistency within the Hilbert system H of problem
5.5.4 is a synthetic consistency property.
∗5.5.24. A set S of formulae is Henkin-complete if for every formula D = ∃xB
of type d, there is some constant c such that B(c) is also in S.
Prove that if P is a synthetic consistency property and S is a set of
formulae that is both Henkin-complete and a maximal set for which
P holds, then S is satisﬁable.
Hint: Show that S is a Hintikka set for the term algebra consisting
of all terms built up from function, constant symbols, and variables
occuring free in S.
∗5.5.25. Prove that if P is a synthetic consistency property, then every set S
of formulae for which P holds can be extended to a Henkin-complete
set which is a maximal set for which P holds.
Hint: Use the idea of problem 5.5.17.
Use the above property to prove the completeness of the Hilbert sys-
tem H.
∗5.5.26. Let L be a countable ﬁrst-order language without equality.
The
method of this problem that is due to Henkin (reported in Smullyan,
1968, Chapter 10, page 96) yields one of the most elegant proofs of
the model existence theorem.
Using this method, a set that is both Henkin-complete and maximal
consistent is obtained from a single construction.
Let A1, ..., An,...
be an enumeration of all L-formulae.
Let P be
a synthetic consistency property, and let S be a set of formulae for
which P holds. Construct the sequence Sn as follows:
S0 = S;

230
5/First-Order Logic
Sn+1 =

Sn ∪{An+1}
If P satisﬁes Sn ∪{An+1};
Sn ∪{¬An+1}
otherwise;
In addition, if An+1 is a formula ∃xD of type d, then add D(c) to
Sn+1 for some new constant c not in Sn. (It may be necessary to add
countably many new constants to L.)
(a) Prove that S′ = 
n≥0 Sn is Henkin-complete, and a maximal set
such that P holds for S′.
(b) Let P be an analytic consistency property.
Given a set S of
formulae, let Des(S) be the set of immediate descendants of formulae
in S as deﬁned in problem 5.5.18, and deﬁne Sn by induction as
follows:
S0 = S;
Sn+1 = Des(Sn).
Let S∗=

n≥0
Sn.
S∗is called the set of descendants of S. Use the construction of (a) to
prove that every set S of formulae for which P holds can be extended
to a Henkin-complete subset S′ of S∗which is a maximal subset of
S∗for which P holds.
Hint: Consider an enumeration of S∗and extend S to a subset of S∗,
which is Henkin-complete and maximal with respect to P.
Why does such a set generally fail to be the set of formulae satisﬁed
by some structure?
(c) Prove that if M is a subset of S∗which is Henkin-complete and is
a maximal subset of S∗for which P holds, then M is a Hintikka set
with respect to the term algebra H consisting of all terms built up
from function symbols, constants, and variables occurring free in M.
(d) Use (c) to prove that every set S of formulae for which P holds
is satisﬁable.
5.6 A Gentzen System for First-Order Languages With
Equality
Let L be a ﬁrst-order language with equality. The purpose of this section is
to generalize the results of Section 5.5 to ﬁrst-order languages with equality.
First, we need to generalize the concept of a Hintikka set and lemma 5.4.5 to
deal with languages with equality.

5.6 A Gentzen System for First-Order Languages With Equality
231
5.6.1 Hintikka Sets for Languages With Equality
The only modiﬁcation to the deﬁnition of a signed formula (deﬁnition 5.4.3)
is that we allow atomic formulae of the form (s .= t) as the formula A in TA
or FA.
Deﬁnition 5.6.1 A Hintikka S set with respect to a term algebra H (over the
reduct LS with equality) is a set of signed L-formulae such that the following
conditions hold for all signed formulae A,B,C,D of type a,b,c,d:
H0: No atomic formula and its conjugate are both in S (TA or FA is
atomic iﬀA is).
H1: If a type-a formula A is in S, then both A1 and A2 are in S.
H2: If a type-b formula B is in S, then either B1 is in S or B2 is in S.
H3: If a type-c formula C is in S, then for every t ∈H, C(t) is in S (we
require that t is free for x in C for every t ∈H).
H4: If a type-d formula D is in S, then for at least one term t ∈H, D(t)
is in S (we require that t is free for x in D for every t ∈H).
H5: Every variable x occurring free in some formula of S is in H.
H6(i): For every term t ∈H,
T(t .= t) ∈S.
(ii): For every n-ary function symbol f in LS and any terms s1, ..., sn,
t1, ..., tn ∈H,
T((s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn)) ∈S.
(iii): For every n-ary predicate symbol P in LS (including .=) and any
terms s1, ..., sn, t1, ..., tn ∈H
T(((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ⊃Pt1...tn) ∈S.
As in lemma 5.4.5, we can assume without loss of generality that the set
of variables occurring free in formulae in S is disjoint from the set of variables
occurring bound in formulae in S.
The alert reader will have noticed that conditions H6(i) to H6(iii) do not
state explicitly that .= is an equivalence relation. However, these conditions
will be used in the proof of lemma 5.6.1 to show that a certain relation on
terms is a congruence as deﬁned in Subsection 2.4.6. The generalization of
lemma 5.4.5 is as follows.

232
5/First-Order Logic
Lemma 5.6.1
Every Hintikka set S (over a language with equality) with
respect to a term algebra H is satisﬁable in a structure HS whose domain HS
is a quotient of H.
Proof : In order to deﬁne the LS-structure HS, we deﬁne the relation ∼=
on the set of all terms in H as follows:
s ∼= t
if and only if
T(s .= t) ∈S.
First, we prove that ∼= is an equivalence relation.
(1) By condition H6(i), ∼= is reﬂexive.
(2) Assume that T(s .= t) ∈S. By H6(iii),
T((s .= t) ∧(s .= s) ∧Pss ⊃Pts) ∈S
for any binary predicate symbol P, and in particular when P is .=. By H2,
either T(t .= s) ∈S or F((s .= t) ∧(s .= s) ∧(s .= s)) ∈S. In the second
case, using H2 again either F(s .= t) ∈S, or F(s .= s) ∈S. In each case we
reach a contradiction by H0 and H6(i). Hence, we must have T(t .= s) ∈S,
establishing that ∼= is symmetric.
(3) To prove transitivity we proceed as follows. Assume that T(r .= s)
and T(s .= t) are in S. By (2) above, T(s .= r) ∈S. By H6(iii), we have
T((s .= r) ∧(s .= t) ∧(s .= s) ⊃(r .= t)) ∈S.
(With P being identical to .=.) A reasoning similar to that used in (2) shows
that T(r .= t) ∈S, establishing transitivity.
Hence, ∼= is an equivalence relation on H.
We now deﬁne the structure HS as follows.
The domain HS of this
structure is the quotient of the set H modulo the equivalence relation ∼=, that
is, the set of all equivalence classes of terms in H modulo ∼= (see Subsection
2.4.7).
Given a term t ∈H, we let t denote its equivalence class.
The
interpretation function is deﬁned as follows:
Every constant c in LS is interpreted as the equivalence class c;
Every function symbol f of rank n in LS is interpreted as the function
such that, for any equivalence classes t1,...,tn,
fHS(t1, ..., tn) = ft1...tn;
For every predicate symbol P of rank n in LS, for any equivalence classes
t1,...,tn,
PHS(t1, ..., tn) =
 F
if FPt1...tn ∈S,
T
if TPt1...tn ∈S,
or neither TPt1...tn nor FPt1...tn is in S.

5.6 A Gentzen System for First-Order Languages With Equality
233
To deﬁne a quotient structure as in Subsection 2.4.7, we need to show
that the equivalence relation ∼= is a congruence (as deﬁned in Subsection 2.4.6)
so that the functions fHS and the predicates PHS are well deﬁned; that is,
that their deﬁnition is independent of the particular choice of representatives
t1, ..., tn in the equivalence classes of terms (in H). This is shown using condi-
tion H6(ii) for functions and condition H6(iii) for predicates. More speciﬁcally,
the following claim is proved:
Claim 1: For any terms s1, ..., sn, t1, ..., tn ∈H, if si = ti for i = 1, .., n,
then:
(i) For each n-ary function symbol f,
fs1...sn = ft1...tn.
(ii) For each n-ary predicate symbol P,
if
TPs1...sn ∈S
then
TPt1...tn ∈S,
and
if
FPs1...sn ∈S
then
FPt1...tn ∈S.
Proof of claim 1:
To prove (i), assume that T(si .= ti) ∈S, for all i, 1 ≤i ≤n. Since by
H6(ii),
T((s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn)) ∈S,
by condition H2 of a Hintikka set, either
T(fs1...sn .= ft1...tn) ∈S,
or
F((s1 .= t1) ∧... ∧(sn .= tn)) ∈S.
But if F((s1 .= t1) ∧... ∧(sn .= tn)) ∈S, by H2, some F(si .= ti) ∈S,
contradicting H0. Hence, T(fs1...sn .= ft1...tn) must be in S, which is just
the conclusion of (i) by the deﬁnition of the relation ∼=.
Next, we prove (ii). If TPs1...sn ∈S, since by H6(iii),
T((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn ⊃Pt1...tn) ∈S,
by condition H2, either
TPt1...tn ∈S,
or
F((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ∈S.
By condition H2, either
FPs1...sn ∈S,

234
5/First-Order Logic
or
F((s1 .= t1) ∧... ∧(sn .= tn)) ∈S.
In the ﬁrst case, H0 is violated, and in the second case, since we have assumed
that T(si .= ti) ∈S for all i, 1 ≤i ≤n, H0 is also violated as in the proof of
(i). Hence, TPt1...tn must be in S.
If FPs1...sn ∈S, since by H6(iii),
T((t1 .= s1) ∧... ∧(tn .= sn) ∧Pt1...tn ⊃Ps1...sn) ∈S,
either
TPs1...sn ∈S,
or
F((t1 .= s1) ∧... ∧(tn .= sn) ∧Pt1...tn) ∈S.
In the ﬁrst case, H0 is violated. In the second case, either
FPt1...tn ∈S,
or
F((t1 .= s1) ∧... ∧(tn .= sn)) ∈S.
However, if
F((t1 .= s1) ∧... ∧(tn .= sn)) ∈S,
since we have assumed that Tsi .= ti ∈S for all i, 1 ≤i ≤n, and we have
shown in (2) that whenever Tsi .= ti ∈S, then Tti .= si is also in S, H0 is
also contradicted. Hence, FPt1...tn must be in S. This concludes the proof
of claim 1.
Claim (1)(i) shows that ∼= is a congruence with respect to function sym-
bols. To show that ∼= is a congruence with respect to predicate symbols, we
show that:
If si ∼= ti for all i, 1 ≤i ≤n, then
TPs1...sn ∈S
iﬀ
TPt1...tn ∈S
and
FPs1...sn ∈S
iﬀ
FPt1...tn ∈S.
Proof : If TPs1...sn ∈S, by claim 1(ii), TPt1...tn ∈S. If TPs1...sn ̸∈S,
then either
FPs1...sn ∈S,
or
FPs1...sn ̸∈S.
If FPs1...sn ∈S, by claim 1(ii), FPt1...tn ∈S, and by H0, TPt1...tn ̸∈S.
If FPs1...sn ̸∈S, then if TPt1...tn was in S, then by claim 1(ii), TPs1...sn

5.6 A Gentzen System for First-Order Languages With Equality
235
would be in S, contrary to the assumption. The proof that FPs1...sn ∈S iﬀ
FPt1...tn ∈S is similar. This concludes the proof.
Having shown that the deﬁnition of the structure HS is proper, let s be
any assignment such that s(x) = x, for each variable x ∈H. It remains to
show that HS and s satisfy S. For this, we need the following claim which is
proved using the induction principle for terms:
Claim 2: For every term t (in H),
tHS[s] = t.
If t is a variable x, then xHS[s] = s(x) = x, and the claim holds.
If t is a constant c, then
cHS[s] = cHS = c
by the deﬁnition of the interpretation function.
If t is a term ft1...tk, then
(ft1...tk)HS[s] = fHS((t1)HS[s], ..., (tk)HS[s]).
By the induction hypothesis, for all i, 1 ≤i ≤k,
(ti)HS[s] = ti.
Hence,
(ft1...tk)HS[s] = fHS((t1)HS[s], ..., (tk)HS[s])
= fHS(t1, ..., tk).
But by the deﬁnition of fHS,
fHS(t1, ..., tk) = ft1...tk = t.
This concludes the induction and the proof of claim 2.
Claim 2 is now used to show that for any atomic formula TPt1...tn ∈S,
Pt1...tn[s] is satisﬁed in HS (and that for any FPt1...tn ∈S, Pt1...tn[s] is
not satisﬁed in HS). Indeed, if TPt1...tn ∈S, by H5 and since H is a term
algebra, t1, ..., tn ∈H. Then,
(Pt1...tn)HS[s] = PHS((t1)HS[s], ..., (tn)HS[s])
= (by Claim 2) PHS(t1, ..., tn) = T.
Hence,
HS |= (Pt1...tn)[s].

236
5/First-Order Logic
A similar proof applies when FPt1...tn ∈S. Also, if T(t1 .= t2) ∈S, by
deﬁnition of ∼= we have t1 ∼= t2, which is equivalent to t1 = t2, that is,
(t1)HS[s] = (t2)HS[s]. The rest of the argument proceeds using the induction
principle for formulae. The propositional connectives are handled as before.
Let us give the proof for a signed formula of type c. If a formula C of
type c is in S, then by H3, C(t) is in S for every term t ∈H. By the induction
hypothesis, we have
HS |= C(t)[s].
By lemma 5.4.1, for any formula A, any term t free for x in A, any structure
M and any assignment v, we have
(A[t/x])M[v] = (A[a/x])M[v],
where a = tM[v].
Since tHS[s] = t, we have
(C[t/x])HS[s] = (C[t/x])HS[s].
Hence,
HS |= C(t)[s]
iﬀ
HS |= C(t)[s],
and since t ∈HS, by lemma 5.4.4, this shows that
HS |= C[s].
As in lemma 5.4.5, HS is expanded to an L-structure in which S is satisﬁable.
This concludes the proof that every signed formula in S is satisﬁed in HS by
the assignment s.
5.6.2 The Gentzen System G= (Languages With Equal-
ity)
Let G= be the Gentzen system obtained from the Gentzen system G (deﬁned
in deﬁnition 5.4.1) by adding the following rules.
Deﬁnition 5.6.2
(Equality rules for G=) Let Γ, ∆, Λ denote arbitrary
sequences of formulae (possibly empty) and let t,s1,...,sn, t1,...,tn denote ar-
bitrary L-terms.
For each term t, we have the following rule:
Γ, t .= t →∆
Γ →∆
reflexivity

5.6 A Gentzen System for First-Order Languages With Equality
237
For each n-ary function symbol f and any terms s1, ..., sn, t1, ..., tn, we
have the following rule:
Γ, (s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn) →∆
Γ →∆
For each n-ary predicate symbol P (including .=) and any terms s1, ..., sn,
t1, ..., tn, we have the following rule:
Γ, ((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ⊃Pt1...tn →∆
Γ →∆
Note that the reason these formulae are added after the rightmost for-
mula of the premise of a sequent is that in the proof of theorem 5.6.1, this
simpliﬁes the reconstruction of a legal proof tree from a closed tree. In fact,
as in deﬁnition 5.4.8, we generalize the above rules to inferences of the form
Γ, A1, ..., Ak →∆
Γ →∆
where A1, ..., Ak are any formulae of the form either
t .= t,
(s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn),
or
((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ⊃Pt1...tn.
EXAMPLE 5.6.1
Let
A = ∀x(x .= f(y) ⊃P(x)) ⊃P(f(y)),
where f is a unary function symbol, and P is a unary predicate symbol.
The following tree is a proof tree for A.
T1
P(f(y)), ∀x(x .= f(y) ⊃P(x)) →P(f(y))
f(y) .= f(y) ⊃P(f(y)), ∀x(x .= f(y) ⊃P(x)) →P(f(y))
∀x(x .= f(y) ⊃P(x)) →P(f(y))
→∀x(x .= f(y) ⊃P(x)) ⊃P(f(y))
where T1 is the tree
∀x(x .= f(y) ⊃P(x)), f(y) .= f(y) →f(y) .= f(y), P(f(y))
∀x(x .= f(y) ⊃P(x)) →f(y) .= f(y), P(f(y))

238
5/First-Order Logic
EXAMPLE 5.6.2
Let
A = P(f(y)) ⊃∀x(x .= f(y) ⊃P(x)).
The following tree is a proof tree for A.
T1
P(z), P(f(y)), z .= f(y) →P(z)
P(f(y)), z .= f(y), (f(y) .= z ∧P(f(y))) ⊃P(z) →P(z)
P(f(y)), z .= f(y) →P(z)
P(f(y)) →z .= f(y) ⊃P(z)
P(f(y)) →∀x(x .= f(y) ⊃P(x))
→P(f(y)) ⊃∀x(x .= f(y) ⊃P(x))
where T1 is the tree
T2
P(f(y)), z .= f(y) →P(f(y)), P(z)
P(f(y)), z .= f(y) →f(y) .= z ∧P(f(y)), P(z)
T2 is the tree
T3
f(y) .= z, P(f(y)), z .= f(y) →f(y) .= z, P(z)
P(f(y)), z .= f(y), (z .= f(y) ∧z .= z ∧z .= z) ⊃f(y) .= z →f(y) .= z, P(z)
P(f(y)), z .= f(y) →f(y) .= z, P(z)
and T3 is the tree
S1
S2
S2
P(f(y)), z .= f(y), z .= z →z .= z ∧z .= z, f(y) .= z, P(z)
P(f(y)), z .= f(y) →z .= z ∧z .= z, f(y) .= z, P(z)
P(f(y)), z .= f(y) →z .= f(y) ∧z .= z ∧z .= z, f(y) .= z, P(z)
The sequent S1 is
P(f(y)), z .= f(y) →z .= f(y), f(y) .= z, P(z)
and the sequent S2 is
P(f(y)), z .= f(y), z .= z →z .= z, f(y) .= z, P(z).

5.6 A Gentzen System for First-Order Languages With Equality
239
5.6.3 Soundness of the System G=
First, the following lemma is easily shown.
Lemma 5.6.2 For any of the equality rules given in deﬁnition 5.6.2 (includ-
ing their generalization), the premise is falsiﬁable if and only if the conclusion
is falsiﬁable.
Proof : Straightforward, and left as an exercise.
Lemma 5.6.3
(Soundness of the System G=) If a sequent is G=-provable
then it is valid.
Proof : Use the induction principle for G=-proofs and lemma 5.6.2.
5.6.4 Search Procedure for Languages With Equality
To prove the completeness of the system G= we modify the procedure expand
in the following way.
Given a sequent Γ0 →∆0, let L′ be the reduct of
L consisting of all constant, function, and predicate symbols occurring in
formulae in Γ0 →∆0 (It is also assumed that the set of variables occurring
free in Γ0 →∆0 is disjoint from the set of variables occurring bound in
Γ0 →∆0, which is possible by lemma 5.3.4.)
Let EQ1,0 be an enumeration of all L′-formulae of the form
(t .= t),
EQ2,0 an enumeration of all L′-formulae of the form
(s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn) and
EQ3,0 an enumeration of all L′-formulae of the form
((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ⊃Pt1...tn,
where the terms t, s1, ..., sn,t1, ...tn are built from the constant and function
symbols in L′, and the set of variables free in formulae in Γ0 →∆0. Note
that these terms belong to the set TERMS, deﬁned in Subsection 5.5.1.
For i ≥1, we deﬁne the sets EQ1,i, EQ2,i and EQ3,i as above, except
that the terms t, s1, ..., sn,t1, ...tn belong to the lists AV AILi (at the start).
During a round, at the end of every expansion step, we shall add each formula
that is the head of the list EQj,i, where j = (RC mod 3) + 1 (RC is a round
counter initialized to 0 and incremented at the end of every round) for all
i = 0, ..., NUMACT, to the antecedent of every leaf sequent. At the end of
the round, each such formula is deleted from the head of the list EQj,i. In
this way, the set S of signed formulae along any (ﬁnite or inﬁnite) nonclosed
path in the tree will be a Hintikka set.

240
5/First-Order Logic
We also add to the procedure search, statements to initialize the lists
EQ1,i, EQ2,i and EQ3,i as explained above.
Note that if the sets FS, PS and CS are eﬀective (recursive), recursive
functions enumerating the lists EQ1,i, EQ2,i and EQ3,i can be written (these
lists are in fact recursive).
In the new version of the search procedure for sequents containing the
equality symbol, a leaf of the tree is finished iﬀit is an axiom. Hence, the
search procedure always builds an inﬁnite tree if the input sequent is not valid.
This is necessary to guarantee that conditions H6(i) to (iii) are satisﬁed.
Deﬁnition 5.6.3
Procedure search for a language with equality.
Procedure Expand for a Language with Equality
procedure expand(node : tree-address; var T : tree);
begin
let A1, ..., Am →B1, ..., Bn be the label of node;
let S be the one-node tree labeled with
A1, ..., Am →B1, ..., Bn;
for i := 1 to m do
if nonatomic(Ai) then
grow-left(Ai, S)
endif
endfor;
for i := 1 to n do
if nonatomic(Bi) then
grow-right(Bi, S)
endif
endfor;
for each leaf u of S do
let Γ →∆be the label of u;
Γ′ := Γ, head(L);
∆′ := ∆, head(R);
for i := 0 to NUMACT0 do
Γ′ := Γ′, head(EQj,i)
endfor;
create a new node u1 labeled with Γ′ →∆′
endfor;
T := dosubstitution(T, node, S)
end

5.6 A Gentzen System for First-Order Languages With Equality
241
Procedure Search for a Language with Equality
procedure search(Γ0 →∆0 : sequent; var T : tree);
begin
L := tail(Γ0); Γ := head(Γ0);
R := tail(∆0); ∆:= head(∆0);
let T be the one-node tree labeled with Γ →∆;
Let TERM0, EQ1,i, EQ2,i, EQ3,i and AVAILi, 0 ≤i,
be initialized as explained above.
NUMACT := 0; RC := 0;
while not all leaves of T are ﬁnished do
TERM1 := TERM0; T0 := T; NUMACT0 := NUMACT;
j := (RC mod 3) + 1;
for each leaf node of T0
(in lexicographic order of tree addresses) do
if not finished(node) then
expand(node, T)
endif
endfor;
TERM0 := TERM1; L := tail(L); R := tail(R);
for i := 0 to NUMACT0 do
EQj,i := tail(EQj,i)
endfor;
RC := RC + 1;
for i := 0 to NUMACT do
TERM0 := append(TERM0, < head(AVAILi), nil >);
AVAILi := tail(AVAILi)
endfor
endwhile;
if all leaves are closed
then
write (‘T is a proof of Γ0 →∆0’)
else
write (‘Γ0 →∆0 is falsiﬁable’)
endif
end
5.6.5 Completeness of the System G=
Using lemma 5.6.1, it is easy to generalize theorem 5.5.1 as follows.
Theorem 5.6.1 For every sequent Γ0 →∆0 containing the equality symbol
and in which no free variable occurs bound the following holds:
(i) If the input sequent Γ0 →∆0 is valid, then the procedure search

242
5/First-Order Logic
halts with a closed tree T, from which a proof tree for a ﬁnite subsequent
C1, ..., Cm →D1, ..., Dn of Γ0 →∆0 can be constructed.
(ii) If the input sequent Γ0 →∆0 is falsiﬁable, then search builds an
inﬁnite counter-example tree T and Γ0 →∆0 can be falsiﬁed in a ﬁnite or a
countably inﬁnite structure which is a quotient of t(TERM0).
Proof : It is similar to that of theorem 5.5.1 but uses lemma 5.6.1 instead
of lemma 5.4.5 in order to deal with equality. What is needed is the following
claim:
Claim: If S is the set of signed formulae along any nonclosed path in
the tree T generated by search, the set HS = t(TERM0) is a term algebra
(over the reduct L′) and S is a Hintikka set for HS.
The proof of the claim is essentially the same as the proof given for the-
orem 5.5.1. The only diﬀerence is that it is necessary to check that conditions
H6(i) to (iii) hold, but this follows immediately since all formulae in the lists
EQj,0 and EQj,i for all activated variables yi (i > 0) are eventually entered
in each inﬁnite path of the tree.
Corollary
(A version of G¨odel’s extended completeness theorem for G=)
Let L be a ﬁrst-order language with equality. A sequent in which no variable
occurs both free and bound (even inﬁnite) is valid iﬀit is G=-provable.
Again, as observed in Section 5.5 in the paragraph immediately after
the proof of theorem 5.5.1, although constructive and theoretically complete,
the search procedure is horribly ineﬃcient. But in the case of a language
with equality, things are even worse. Indeed, the management of the equality
axioms (the lists EQj,i) adds signiﬁcantly to the complexity of the bookkeep-
ing. In addition, it is no longer true that the search procedure always halts
for quantiﬁer-free sequents, as it does for languages without equality. This is
because the equality rules allow the introduction of new formulae.
It can be shown that there is an algorithm for deciding the validity
of quantiﬁer-free formulae with equality by adapting the congruence closure
method of Kozen, 1976, 1977. For a proof, see Chapter 10.
The diﬃculty is that the search procedure no longer preserves the prop-
erty known as the subformula property. For languages without equality, the
Gentzen rules are such that given any input sequent Γ →∆, the formulae
occurring in any deduction tree with root Γ →∆contain only subformulae
of the formulae in Γ →∆. These formulae are in fact “descendants” of the
formulae in Γ →∆, which are essentially “signed subformulae” of the formu-
lae in Γ →∆(for a precise deﬁnition, see problem 5.5.18). We also say that
such proofs are analytic. But the equality rules can introduce “brand new”
formulae that may be totally unrelated to the formulae in Γ →∆. Of course,
one could object that this problem arises because the rules of G= were badly
designed, and that this problem may not arise for a better system. However,

5.6 A Gentzen System for First-Order Languages With Equality
243
it is not easy to ﬁnd a complete proof system for ﬁrst-order logic with equal-
ity, that incorporates a weaker version of the subformula property. It will be
shown in the next chapter that there is a Gentzen system LKe, such that for
every sequent Γ →∆provable in G=, there is a proof in LKe in which the for-
mulae occurring in the proof are not too unrelated to the formulae in Γ →∆.
This is a consequence of the cut elimination theorem without essential cuts
for LKe. Nevertherless, theorem proving for languages with equality tends
to be harder than theorem proving for languages without equality. We shall
come back to this point later, in particular when we discuss the resolution
method.
Returning to the completeness theorem, the following classical results
apply to ﬁrst-order languages with equality and are immediate consequences
of theorem 5.6.1.
5.6.6 L¨owenheim-Skolem, Compactness, and Model Ex-
istence Theorems for Languages With Equality
For a language with equality, the structure given by L¨owenheim-Skolem the-
orem may be ﬁnite.
Theorem 5.6.2
(L¨owenheim-Skolem) Let L be a ﬁrst-order language with
equality. If a set of formulae Γ over L is satisﬁable in some structure M, then
it is satisﬁable in a structure whose domain is at most countably inﬁnite.
Proof : From theorem 5.6.1, Γ is satisﬁable in a quotient structure of HS.
Since, HS is countable, a quotient of HS is countable, but possibly ﬁnite.
Theorem 5.6.3
(Compactness theorem) Let L be a ﬁrst-order language
with equality. For any (possibly countably inﬁnite) set Γ of formulae over L,
if every nonempty ﬁnite subset of Γ is satisﬁable then Γ is satisﬁable.
Theorem 5.6.4
(Model existence theorem) Let L be a ﬁrst-order language
with equality. If a set Γ of formulae over L is consistent, then it is satisﬁable.
Note that if Γ is consistent, then by theorem 5.6.4 the sequent Γ →is
falsiﬁable. Hence the search procedure run on input Γ →will actually yield
a structure HS (with domain a quotient of t(TERM0)) for each Hintikka set
S arising along each nonclosed path in the tree T, in which S and Γ are
satisﬁable.
We also have the following lemma.
Lemma 5.6.4
(Consistency lemma) Let L be a ﬁrst-order language with
equality. If a set Γ of formulae is satisﬁable then it is consistent.

244
5/First-Order Logic
5.6.7 Maximal Consistent Sets
As in Subsection 5.5.5, a consistent set Γ of ﬁrst-order formulae (possibly
involving equality) is maximally consistent (or a maximal consistent set) iﬀ,
for every consistent set ∆, if Γ ⊆∆, then Γ = ∆. Lemma 5.5.2 can be easily
generalized to the ﬁrst-order languages with equality.
Lemma 5.6.5 Given a ﬁrst-order language (with or without equality), every
consistent set Γ is a subset of some maximal consistent set ∆.
Proof : Almost identical to that of lemma 3.5.5, but using structures
instead of valuations.
5.6.8 Application of the Compactness and L¨owenheim-
Skolem Theorems: Nonstandard Models of Arithmetic
The compactness and the L¨owenhein-Skolem theorems are important tools in
model theory. Indeed, they can be used for showing the existence of countable
models for certain interesting theories. As an illustration, we show that the
theory of example 5.3.2 known as Peano’s arithmetic has a countable model
nonisomorphic to N, the set of natural numbers. Such a model is called a
nonstandard model.
EXAMPLE 5.6.3
Let L be the language of arithmetic deﬁned in example 5.3.1. The fol-
lowing set AP of formulae is known as the axioms of Peano’s arithmetic:
∀x¬(S(x) .= 0)
∀x∀y(S(x) .= S(y) ⊃x .= y)
∀x(x + 0 .= x)
∀x∀y(x + S(y) .= S(x + y))
∀x(x ∗0 .= 0)
∀x∀y(x ∗S(y) .= x ∗y + x)
For every formula A with one free variable x,
(A(0) ∧∀x(A(x) ⊃A(S(x)))) ⊃∀yA(y)
Let L′ be the expansion of L obtained by adding the new constant c.
Let Γ be the union of AP and the following set I of formulae:
I = {¬(0 .= c), ¬(1 .= c), ..., ¬(n .= c), ...},
where n is an abbreviation for the term S(S(...(0))), with n occurrences
of S.
Observe that N can be made into a model of any ﬁnite subset of Γ.
Indeed, N is a model of AP , and to satisfy any ﬁnite subset X of I, it is

5.6 A Gentzen System for First-Order Languages With Equality
245
suﬃcient to interpret c as any integer strictly larger than the maximum
of the ﬁnite set {n | ¬(n .= c) ∈X}. Since every ﬁnite subset of Γ is
satisﬁable, by the compactness theorem, Γ is satisﬁable in some model.
By the L¨owenheim-Skolem theorem, Γ has a countable model, say M0.
Let M be the reduct of M0 to the language L. The model M still
contains the element a, which is the interpretation of c ∈L′, but since
c does not belong to L, the interpretation function of the structure M
does not have c in its domain.
It remains to show that M and N are not isomorphic. An isomorphism
h from M to N is a bijection h : M →N such that:
h(0M) = 0 and, for all x, y ∈M,
h(SM(x)) = S(h(x)),
h(x +M y) = x + y and
h(x ∗M y) = x ∗y.
Assume that there is an isomorphism h between M and N. Let k ∈N
be the natural number h(a) (with a = cM0 in M0). Then, for all n ∈N,
n ̸= k.
Indeed, since the formula ¬(n .= c) is valid in M0,
nM ̸= a in M,
and since h is injective, nM ̸= a implies h(nM) ̸= h(a), that is, n ̸= k.
But then, we would have k ̸= k, a contradiction.
It is not diﬃcult to see that in the model M, mM ̸= nM, whenever m ̸=
n. Hence, the natural numbers are represented in M, and satisfy Peano’s
axioms. However, there may be other elements in M. In particular, the
interpretation a of c ∈L′ belongs to M. Intuitively speaking, a is an
inﬁnite element of M.
A model of AP nonisomorphic to N such as M is called a nonstandard
model of Peano’s arithmetic.
For a detailed exposition of model theory including applications of the
compactness theorem, L¨owenheim-Skolem theorem, and other results, the
reader is referred to Chang and Keisler, 1973; Monk, 1976; or Bell and Slom-
son, 1969. The introductory chapter on model theory by J. Keisler in Barwise,
1977, is also highly recommended.

246
5/First-Order Logic
PROBLEMS
5.6.1.
Give proof tree for the following formulae:
∀x(x .= x)
∀x1...∀xn∀y1...∀yn((x1 .= y1 ∧... ∧xn .= yn) ⊃
(f(x1, ..., xn) .= f(y1, ..., yn)))
∀x1...∀xn∀y1...∀yn(((x1 .= y1 ∧... ∧xn .= yn) ∧P(x1, ..., xn)) ⊃
P(y1, ..., yn))
The above formulae are called the closed equality axioms.
5.6.2.
Let S = Γ →∆be a sequent, and let Se be the set of closed equality
axioms for all predicate and function symbols occurring in S. Prove
that the sequent Γ →∆is provable in G= iﬀthe sequent Se, Γ →∆
is provable in G= + {cut}.
5.6.3.
Prove that the following formula is provable:
f 3(a) .= a ∧f 5(a) .= a ⊃f(a) .= a
5.6.4.
Let ∗be a binary function symbol and 1 be a constant. Prove that
the following sequent is valid:
∀x(∗(x, 1) .= x), ∀x(∗(1, x) .= x),
∀x∀y∀z(∗(x, ∗(y, z)) .= ∗(∗(x, y), z)),
∀x(∗(x, x) .= 1) →∀x∀y(∗(x, y) .= ∗(y, x))
5.6.5.
Give proof trees for the following formulae:
∀x∃y(x .= y)
A[t/x] ≡∀x((x .= t) ⊃A), if x /∈V ar(t) and t is free for x in A.
A[t/x] ≡∃x((x .= t) ∧A), if x /∈V ar(t) and t is free for x in A.
5.6.6.
(a) Prove that the following formulae are valid:
x .= x
x .= y ⊃(x .= z ⊃y .= z)
x .= y ⊃(P(x1, ..., xi−1, x, xi+1, ..., xn) ⊃
P(x1, ..., xi−1, y, xi+1, ..., xn))
x .= y ⊃
(f(x1, ..., xi−1, x, xi+1, ..., xn) .= f(x1, ..., xi−1, y, xi+1, ..., xn))

PROBLEMS
247
Consider the extension H= of the Hilbert system H deﬁned in prob-
lem 5.5.4 obtained by adding the above formulae known as the open
equality axioms.
(b) Prove that
∀x∀y(x .= y ⊃y .= x)
and
∀x∀y∀z(x .= y ∧y .= z ⊃x .= z)
are provable in H=.
(c) Given a set S of formulae (over a language with equality), let Se
be the set of universal closures (deﬁned in problem 5.3.10) of equality
axioms for all function and predicate symbols occuring in S. Prove
that
S ⊢A in H=
iﬀ
S, Se ⊢A in H.
(d) Prove the completeness of the Hilbert system H=.
Hint: Use the result of problem 5.5.10 and, given a model for S ∪Se,
construct a model of S in which the predicate symbol .= is interpreted
as equality, using the quotient construction.
∗5.6.7.
In languages with equality, it is possible to eliminate function and
constant symbols as shown in this problem.
Let L be a language (with or without equality). A relational version
of L is a language L′ with equality satisfying the following properties:
(1) L′ has no function or constant symbols;
(2) There is an injection T : FS∪CS →PS′ called a translation such
that r(T(f)) = r(f) + 1;
(3) PS′ = PS ∪T(FS ∪CS), and T(FS ∪CS) and PS are disjoint.
We deﬁne the T-translate AT of an L-formula A as follows:
(1) If A is of the form (s .= x), where s is a term and x is a variable,
then
(i) If s is a variable y, then (y .= x)T = (y .= x);
(ii) If s is a constant c, then (c .= x)T = T(c)(x), where T(c) is the
unary predicate symbol associated with c;
(iii) If s is a term of the form fs1...sn, then
(s .= x)T = ∃y1...∃yn[(s1 .= y1)T ∧...∧(sn .= yn)T ∧T(f)(y1, ..., yn, x)],
where y1, ..., yn are new variables, and T(f) is the (n+1)-ary predicate
symbol associated with f;

248
5/First-Order Logic
(2) If A is of the form (s .= t), where t is not a variable, then
(s .= t)T = ∃y((s .= y)T ∧(t .= y)T ),
where y is a new variable.
(3) If A is of the form Ps1...sn, then
(i) If every s1, ..., sn is a variable, then (Ps1...sn)T = Ps1...sn;
(ii) If some si is not a variable, then
(Ps1...sn)T = ∃y1...∃yn[Py1...yn ∧(s1 .= y1)T ∧... ∧(sn .= yn)T ],
where y1, ..., yn are new variables.
(4) If A is not atomic, then
(¬B)T = ¬BT ,
(B ∨C)T = BT ∨CT ,
(B ∧C)T = BT ∧CT ,
(B ⊃C)T = (BT ⊃CT ),
(∀xB)T = ∀xBT ,
(∃xB)T = ∃xBT .
For every function or constant symbol f in L, the existence condition
for f is the following sentence in L′:
∀x1...∀xn∃yT(f)(x1, ..., xn, y);
The uniqueness condition for f is the following sentence in L′:
∀x1...∀xn∀y∀z(T(f)(x1, ..., xn, y) ∧T(f)(x1, ..., xn, z) ⊃y .= z).
The set of translation conditions is the set of all existence and unique-
ness conditions for all function and constant symbols in L.
Given an L-structure A, the relational version A′ of A has the same
domain as A, the same interpretation for the symbols in PS, and
interprets each symbol T(f) of rank n + 1 as the relation
{(x1, ..., xn, y) ∈An+1 | fA(x1, ..., xn) = y}.
Prove the following properties:
(a) For every formula A of L, if A does not contain function or con-
stant symbols, then AT = A.

PROBLEMS
249
(b) Let A be an L-structure, and A′ its relational version. Prove
that A′ is a model of the translation conditions. Prove that for every
L-formula A, for every assignment s,
A |= A[s]
iﬀ
A′ |= AT [s].
(c) For every L′-formula A, let A∗be the formula obtained by replac-
ing every atomic formula T(f)(x1, ..., xn, y) by (f(x1, ..., xn) .= y).
Prove that
A |= A∗[s]
iﬀ
A′ |= A[s].
(d) Prove that if B is an L′-structure which is a model of the trans-
lation conditions, then B = A′ for some L-structure A.
(e) Let Γ be a set of L-formulae, and A any L-formula. Prove that
Γ |= A
iﬀ
{BT | B ∈Γ} ∪{B | B is a translation condition} |= AT .
∗5.6.8.
Let L be a ﬁrst-order language with equality. A theory is a set Γ of
L-sentences such that for every L-sentence A, if Γ |= A, then A ∈Γ.
If L′ is an expansion of L and Γ′ is a theory over L′, we say that Γ′
is conservative over Γ, iﬀ
{A | A is an L-sentence in Γ′} = Γ.
Let Γ be a theory over L, L′ an expansion of L, and Γ′ a theory over
L′.
(i) If P is an n-ary predicate symbol in L′ but not in L, a possible
deﬁnition of P over Γ is an L-formula A whose set of free variables is
a subset of {x1, ..., xn}.
(ii) If f is an n-ary function symbol or a constant in L′ but not in L,
a possible deﬁnition of f over Γ is an L-formula A whose set of free
variables is a subset of {x1, ..., xn, y}, such that the following existence
and uniqueness conditions are in Γ:
∀x1...∀xn∃yA,
∀x1...∀xn∀y∀z(A(x1, ..., xn, y) ∧A(x1, ..., xn, z) ⊃y .= z).
(iii) We say that Γ′ over L′ is a deﬁnitional extension of Γ over L iﬀ
for every constant, function, or predicate symbol X in L′ but not in
L, there is a possible deﬁnition AX over Γ such that the condition
stated below holds.

250
5/First-Order Logic
For every possible deﬁnition AX, the sentence A′
X is deﬁned as fol-
lows:
For an n-ary predicate symbol P, let A′
P be the sentence
∀x1...∀xn(Px1...xn ≡AP );
For an n-ary function symbol or a constant f, let A′
f be the sentence
∀x1...∀xn∀y((fx1...xn .= y) ≡Af).
Then we require that
Γ′ = {B | B is an L′-formula such that,
Γ ∪{A′
X | X is a symbol in L′ not in L} |= B}.
(a) Prove that if Γ′ is a deﬁnitional extension of Γ, then for every
L′-formula A, there is an L-formula AT with the same free variables
as A, such that
Γ′ |= (A ≡AT ).
Hint: Use the technique of problem 5.6.7.
(b) Prove that Γ′ is conservative over Γ.
5.6.9.
Let Γ be a theory over a language L. A set of axioms ∆for Γ is any
set of L-sentences such that
Γ = {B | ∆|= B}.
Given an L-formula A with set of free variables {x1, ..., xn, y}, assume
that
Γ |= ∀x1...∀xn∃yA(x1, ..., xn, y).
Let L′ be the expansion of L obtained by adding the new n-ary func-
tion symbol f, and let Γ′ be the theory with set of axioms
Γ ∪{∀x1...∀xnA(x1, ..., xn, f(x1, ..., xn))}.
Prove that Γ′ is conservative over Γ.
∗5.6.10. Let L be a ﬁrst-order language with equality. From problem 5.6.1,
the closed equality axioms are provable in G=. Recall the concept of
a Henkin theory from problem 5.5.17.
(a) Prove that a maximally consistent (with respect to G=) Henkin
theory T ′ of sentences is a Hintikka set with respect to the term

PROBLEMS
251
algebra H consisting of all closed terms built up from the function
and constant symbols in LH.
(b) Prove that every consistent set T of L-sentences is satisﬁable.
(c) Prove that a formula A with free variables {x1, ..., xn} is satisﬁable
iﬀA[c1/x1, ..., cn/xn] is satisﬁable, where c1, ..., cn are new constants.
Using this fact, prove that every consistent set T of L-formulae is
satisﬁable.
5.6.11. Prove that in G= + {cut},
Γ →A is not provable in G= + {cut} iﬀ
Γ ∪{¬A} is consistent.
Use the above fact and problem 5.6.10 to give an alternate proof of
the extended completeness theorem for G= + {cut}.
∗5.6.12. Prove that the results of problem 5.6.10 hold for languages with equal-
ity of any cardinality. Using Zorn’s lemma, prove that the results of
problem 5.6.10 hold for languages of any cardinality. Thus, prove that
the extended completeness theorem holds for languages with equality
of any cardinality.
∗5.6.13. For a language L with equality of cardinality α, show that the car-
dinality of the term algebra arising in problem 5.6.10 is at most α.
Conclude that any consistent set of L-sentences has a model of car-
dinality at most α.
∗5.6.14. Let L be a countable ﬁrst-order language with equality. A property P
of sets of formulae is an analytic consistency property if the following
hold:
(i) P is of ﬁnite character (see problem 3.5.12).
(ii) For every set S of formulae for which P is true, the following
conditions hold:
A0: S contains no atomic formula and its negation.
A1: For every formula A of type a in S, P holds for {S, A1} and
{S, A2}.
A2: For every formula B of type b in S, P holds either for {S, B1} or
for {S, B2}.
A3: For every formula C of type c in S, P holds for {S, C(t)} for
every term t.
A4: For every formula D of type d in S, for some constant c not in
S, P holds for {S, D(c)}.
A5 (i): For every term t, if P holds for S then P holds for {S, (t .= t)}.

252
5/First-Order Logic
(ii): For each n-ary function symbol f, for any terms s1, ..., sn, t1,
..., tn, if P holds for S then P holds for
{S, (s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn)}.
(iii): For each n-ary predicate symbol Q (including .=), for any
terms s1, ..., sn, t1, ..., tn, if P holds for S then P holds for
{S, (s1 .= t1) ∧... ∧(sn .= tn) ∧Qs1...sn ⊃Qt1...tn}.
Prove that if P is an analytic consistency property and P holds for
S, then S is satisﬁable.
Hint: See problem 5.5.22.
∗5.6.15. Let L be a countable ﬁrst-order language with equality.
A set S
of formulae is truth-functionally inconsistent iﬀS is the result of
substituting ﬁrst-order formulae for the propositional letters in an
unsatisﬁable set of propositions (see deﬁnition 5.3.11). We say that a
formula A is truth-functionally valid iﬀit is obtained by substitution
of ﬁrst-order formulae into a tautology (see deﬁnition 5.3.11). We say
that a ﬁnite set S = {A1, ..., Am} truth-functionally implies B iﬀthe
formula
(A1 ∧... ∧Am) ⊃B
is truth-functionally valid.
A property P of sets of formulae is a
synthetic consistency property iﬀthe following conditions hold:
(i) P is of ﬁnite character (see problem 3.5.12).
(ii) For every set S of formulae, the following conditions hold:
B0: If S is truth-functionally inconsistent, then P does not hold for
S;
B3: If P holds for S then for every formula C of type c in S, P holds
for {S, C(t)} for every term t.
B4: For every formula D of type d in S, for some constant c not in
S, P holds for {S, D(c)}.
B5: For every formula X, if P does not hold for {S, X} or {S, ¬X},
then P does not hold for S. Equivalently, if P holds for S, then for
every formula X, either P holds for {S, X} or P holds for {S, ¬X}.
B6 (i): For every term t, if P holds for S then P holds for {S, (t .= t)}.
(ii): For each n-ary function symbol f, for any terms s1, ..., sn, t1,
..., tn, if P holds for S then P holds for
{S, (s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn)}.

PROBLEMS
253
(iii): For each n-ary predicate symbol Q (including .=), for any
terms s1, ..., sn, t1, ..., tn, if P holds for S then P holds for
{S, (s1 .= t1) ∧... ∧(sn .= tn) ∧Qs1...sn ⊃Qt1...tn}.
(a) Prove that if P is a synthetic consistency property then the fol-
lowing condition holds:
B7: If P holds for S and a ﬁnite subset of S truth-functionally implies
X, then P holds for {S, X}.
(b) Prove that every synthetic consistency property is an analytic
consistency property.
(c) Prove that consistency within the Hilbert system H= of problem
5.6.6 is a synthetic consistency property.
∗5.6.16. A set S of formulae is Henkin-complete iﬀfor every formula D = ∃xB
of type d, there is some constant c such that B(c) is also in S.
Prove that if P is a synthetic consistency property and S is a set of
formulae that is both Henkin-complete and a maximally set for which
P holds, then S is satisﬁable.
Hint: Show that S is a Hintikka set for the term algebra consisting
of all terms built up from function, constant symbols, and variables
occurring free in S.
∗5.6.17. Prove that if P is a synthetic consistency property, then every set S
of formulae for which P holds can be extended to a Henkin-complete
set which is a maximal set for which P holds.
Hint: Use the idea of problem 5.5.17.
Use the above property to prove the completeness of the Hilbert sys-
tem H=.
∗5.6.18. Let L be a countable ﬁrst-order language with equality. Prove that
the results of problem 5.5.26 are still valid.
∗5.6.19. Given a ﬁrst-order language L with equality, for any L-structure M,
for any ﬁnite or countably inﬁnite sequence X of elements in M (the
domain of the structure M), the language LX is the expansion of
L obtained by adding new distinct constants ({c1, ..., cn} if X =<
a1, ..., an >, {c1, ..., cn, cn+1, ...} if X is countably inﬁnite) to the set of
constants in L. The structure (M, X) is the expansion of M obtained
by interpreting each ci as ai.
An L-structure M is countably saturated if, for every ﬁnite sequence
X =< a1, ..., an > of elements in M, for every set Γ(x) of formulae
with at most one free variable x over the expanded language LX,

254
5/First-Order Logic
if every ﬁnite subset of Γ(x) is satisﬁable in (M, X) then Γ(x) is
satisﬁable in (M, X).
(1) Prove that every ﬁnite structure M is countably saturated.
Hint: Show that if the conclusion does not hold, a ﬁnite unsatisﬁable
subset of Γ(x) can be found.
(2) Two L-structures A and B are elementary equivalent if, for any
L-sentence D,
A |= D
if and only if
B |= D.
(a) Assume that A and B are elementary equivalent. Show that for
every formula E(x) with at most one free variable x, E(x) is satisﬁable
in A if and only if E(x) is satisﬁable in B.
(b) Let X =< a1, ..., an > and Y =< b1, ..., bn > be two ﬁnite se-
quences of elements in A and B respectively. Assume that (A, X)
and (B, Y ) are elementary equivalent and that A is countably satu-
rated.
Show that for any set Γ(x) of formulae with at most one free vari-
able x over the expansion LY such that every ﬁnite subset of Γ(x) is
satisﬁable in (B, Y ), Γ(x) is satisﬁable in (A, X). (Note that the lan-
guages LX and LY are identical. Hence, we will refer to this language
as LX.)
(c) Assume that A and B are elementary equivalent, with A count-
ably saturated. Let Y =< b1, ..., bn, ... > be a countable sequence of
elements in B.
Prove that there exists a countable sequence X =< a1, ..., an, ... > of
elements from A, such that (A, X) and (B, Y ) are elementary equiv-
alent.
Hint: Proceed in the following way: Deﬁne the sequence Xn =<
a1, ..., an > by induction so that (A, Xn) and (B, Yn) are elementary
equivalent (with Yn =< b1, ..., bn >) as follows: Let Γ(x) be the set
of formulae over LYn satisﬁed by bn+1 in (B, Yn).
Show that Γ(x) is maximally consistent. Using 2(b), show that Γ(x)
is satisﬁed by some an+1 in (A, Xn) and that it is the set of formulae
satisﬁed by an+1 in (A, Xn) (recall that Γ(x) is maximally consis-
tent). Use these properties to show that (A, Xn+1) and (B, Yn+1) are
elementary equivalent.
(d) If (A, X) and (B, Y ) are elementary equivalent as above, show
that
ai = aj
if and only if
bi = bj for all i, j ≥1.

Notes and Suggestions for Further Reading
255
(e) Use (d) to prove that if A and B are elementary equivalent and
A is ﬁnite, then B is isomorphic to A.
∗5.6.20. Write a computer program implementing the search procedure in the
case of a ﬁrst-order language with equality.
Notes and Suggestions for Further Reading
First-order logic is a rich subject that has been studied extensively.
We
have presented the basic model-theoretic and proof-theoretic concepts, us-
ing Gentzen systems because of their algorithmic nature and their concep-
tual simplicity. This treatment is inspired from Kleene, 1967. For more on
Gentzen systems, the reader should consult Robinson, 1979; Takeuti, 1975;
Szabo, 1969; or Smullyan, 1968.
We have focused our attention on a constructive proof of the complete-
ness theorem, using Gentzen systems and Hintikka sets. For more details on
Hintikka sets and related concepts such as consistency properties, the reader
is referred to Smullyan, 1968.
There are other proof systems for ﬁrst-order logic. Most texts adopt
Hilbert systems. Just to mention a few texts of varying degree of diﬃculty,
Hilbert systems are discussed in Kleene, 1952; Kleene, 1967; Enderton, 1972;
Shoenﬁeld, 1967; and Monk, 1976. A more elementary presentation of ﬁrst-
order logic tailored for computer scientists is found in Manna and Waldinger,
1985. Natural deduction systems are discussed in Kleene, 1967; Van Dalen,
1980; Prawitz, 1965; and Szabo, 1969. A variant of Gentzen systems called
the tableaux system is discussed at length in Smullyan, 1968. Type Theory
(higher-order logic “a la Church”), including the G¨odel incompleness theo-
rems, is discussed in Andrews 1986.
An illuminating comparison of various approaches to the completeness
theorem, including Henkin’s method, can be found in Smullyan, 1968.
Since we have chosen to emphasize results and techniques dealing with
the foundations of (automatic) theorem proving, model theory is only touched
upon lightly. However, this is a very important branch of logic. The reader
is referred to Chang and Keisler, 1973; Bell and Slomson, 1974; and Monk,
1976, for thorough and advanced expositions of model theory. The article
on fundamentals of model theory by Keisler, and the article by Eklof about
ultraproducts, both in Barwise, 1977, are also recommended. Barwise 1977
also contains many other interesting articles on other branches of logic.
We have not discussed the important incompleteness theorems of G¨odel.
The reader is referred to Kleene,1952; Kleene, 1967; Enderton, 1972; Shoen-
ﬁeld, 1967; or Monk, 1976. One should also consult the survey article by
Smorynski in Barwise, 1977.

Chapter 6
Gentzen’s Cut Elimination
Theorem And Applications
6.1 Introduction
The rules of the Gentzen system G (given in deﬁnition 5.4.1) were chosen
mainly to facilitate the design of the search procedure. The Gentzen system
LK′ given in Section 3.6 can be extended to a system for ﬁrst-order logic
called LK, which is more convenient for constructing proofs in working down-
ward, and is also useful for proof-theoretical investigations.
In particular,
the system LK will be used to prove three classical results, Craig’s theorem,
Beth’s theorem and Robinson’s theorem, and will be used in Chapter 7 to
derive a constructive version of Herbrand’s theorem.
The main result about LK is Gentzen’s cut elimination theorem. An
entirely proof-theoretic argument (involving only algorithmic proof transfor-
mation steps) of the cut elimination can be given, but it is technically rather
involved.
Rather than giving such a proof (which can be found in Szabo,
1969, or Kleene, 1952), we will adopt the following compromise: We give a
rather simple semantic proof of Gentzen’s cut elimination theorem (using the
completeness theorem) for LK, and we give a constructive proof of the cut
elimination for a Gentzen system G1nnf simpler than LK (by constructive,
we mean that an algorithm for converting a proof into a cut-free proof is ac-
tually given). The sequents of the system G1nnf are pairs of sets of formulae
in negation normal form. This system is inspired from Schwichtenberg (see
Barwise, 1977). The cut elimination theorem for the system G1nnf
=
which
256

6.2 Gentzen System LK for Languages Without Equality
257
includes axioms for equality is also given.
Three applications of the cut elimination theorem will also be given:
Craig’s interpolation theorem,
Beth’s deﬁnability theorem and
Robinson’s joint consistency theorem.
These are classical results of ﬁrst-order logic, and the proofs based on
cut elimination are constructive and elegant. Beth’s deﬁnability theorem also
illustrates the subtle interaction between syntax and semantics.
A new important theme emerges in this chapter, and will be further elab-
orated in Chapter 7. This is the notion of normal form for proofs. Gentzen’s
cut elimination theorem (for LK or LKe) shows that for every provable se-
quent Γ →∆, there is a proof in normal form, in the sense that in the system
LK, the proof does not have any cuts, and for the system LKe, it has only
atomic cuts. In the next chapter, it will be shown that this normal form can
be improved if the formulae in the sequent are of a certain type (prenex form
or negation normal form). This normal form guaranteed by Gentzen’s Sharp-
ened Hauptsatz is of fundamental importance, since it reduces provabilily in
ﬁrst-order logic to provability in propositional logic.
Related to the concept of normal form is the concept of proof transfor-
mation. Indeed, a constructive way of obtaining proofs in normal form is to
perform a sequence of proof transformations.
The concepts of normal form for proofs and of proof transformation are
essential. In fact, it turns out that the completeness results obtained in the
remaining chapters will be obtained via proof transformations.
First, we consider the case of a ﬁrst-order language without equality. In
this Chapter, it is assumed that no variable occurs both free and bound in
any sequent (or formula).
6.2 Gentzen System LK for Languages Without Equal-
ity
The system LK is obtained from LK′ by adding rules for quantiﬁed formulae.
6.2.1 Syntax of LK
The inference rules and axioms of LK are deﬁned as follows.
Deﬁnition 6.2.1
Gentzen system LK. The system LK consists of struc-
tural rules, the cut rule, and of logical rules. The letters Γ, ∆, Λ, Θ stand
for arbitrary (possibly empty) sequences of formulae and A, B for arbitrary
formulae.

258
6/Gentzen’s Cut Elimination Theorem And Applications
(1) Structural rules:
(i) Weakening:
Γ →∆
A, Γ →∆(left)
Γ →∆
Γ →∆, A (right)
A is called the weakening formula
(ii) Contraction:
A, A, Γ →∆
A, Γ →∆
(left)
Γ →∆, A, A
Γ →∆, A
(right)
(iii) Exchange:
Γ, A, B, ∆→Λ
Γ, B, A, ∆→Λ (left)
Γ →∆, A, B, Λ
Γ →∆, B, A, Λ (right)
(2) Cut rule:
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
(3) Logical rules:
A, Γ →∆
A ∧B, Γ →∆(∧: left)
and
B, Γ →∆
A ∧B, Γ →∆(∧: left)
Γ →∆, A
Γ →∆, B
Γ →∆, A ∧B
(∧: right)
A, Γ →∆
B, Γ →∆
A ∨B, Γ →∆
(∨: left)
Γ →∆, A
Γ →∆, A ∨B (∨: right)
and
Γ →∆, B
Γ →∆, A ∨B (∨: right)
Γ →∆, A
B, Λ →Θ
A ⊃B, Γ, Λ →∆, Θ
(⊃: left)
A, Γ →∆, B
Γ →∆, A ⊃B (⊃: right)

6.2 Gentzen System LK for Languages Without Equality
259
Γ →∆, A
¬A, Γ →∆(¬ : left)
A, Γ →∆
Γ →∆, ¬A (¬ : right)
In the rules above, A∨B, A∧B, A ⊃B, and ¬A are called the principal
formulae and A, B the side formulae of the inference.
In the quantiﬁer rules below, x is any variable and y is any variable free
for x in A and not free in A, unless y = x (y /∈FV (A) −{x}). The term t is
any term free for x in A.
A[t/x], Γ →∆
∀xA, Γ →∆
(∀: left)
Γ →∆, A[y/x]
Γ →∆, ∀xA
(∀: right)
A[y/x], Γ →∆
∃xA, Γ →∆
(∃: left)
Γ →∆, A[t/x]
Γ →∆, ∃xA
(∃: right)
Note that in both the (∀: right)-rule and the (∃: left)-rule, the variable
y does not occur free in the lower sequent. In these rules, the variable y is
called the eigenvariable of the inference. The condition that the eigenvariable
does not occur free in the conclusion of the rule is called the eigenvariable
condition. The formula ∀xA (or ∃xA) is called the principal formula of the
inference, and the formula A[t/x] (or A[y/x]) the side formula of the inference.
The axioms of the system LK are all sequents of the form A →A.
Note that since the system LK contains the exchange rules, the order of
the formulae in a sequent is really irrelevant. Hence, we can view a sequent
as a pair of multisets (as deﬁned in problem 2.1.8).
Proof trees and deduction tree are deﬁned inductively as in deﬁnition
3.4.5, but with the rules of the system LK given in deﬁnition 6.2.1.
If a
sequent has a proof in the system G we say that it is G-provable and similarly,
if it is provable in the system LK, we say that it is LK-provable. The system
obtained from LK by removing the cut rule is denoted by LK −{cut}. We
also say that a sequent is LK-provable without a cut if it has a proof tree
using the rules of the system LK −{cut}.
6.2.2 The Logical Equivalence of the Systems G, LK,
and LK −{cut}
We now show that the systems G and LK are logically equivalent. We will in
fact prove a stronger result, namely that G, LK−{cut} and LK are equivalent.
First, we show that the system LK is sound.

260
6/Gentzen’s Cut Elimination Theorem And Applications
Lemma 6.2.1
Every axiom of LK is valid.
For every rule of LK, if the
premises of the rule are valid, then the conclusion of the rule is valid. Every
LK-provable sequent is valid.
Proof : The proof uses the induction principle for proofs and is straight-
forward.
lemma 6.2.1 diﬀers from lemma 5.4.3 in the following point: It is not
necessarily true that if the conclusion of a rule is valid, then the premises of
that rule are valid.
Theorem 6.2.1
(Logical equivalence of G, LK, and LK −{cut}) There is
an algorithm to convert any LK-proof of a sequent Γ →∆into a G-proof.
There is an algorithm to convert any G-proof of a sequent Γ →∆into a proof
using the rules of LK −{cut}.
Proof : If Γ →∆has an LK-proof, by lemma 6.2.1, Γ →∆is valid.
By theorem 5.5.1, Γ →∆has a G-proof given by the procedure search.
Conversely, using the induction principle for G-proofs we show that every G-
proof can be converted to an (LK −{cut})-proof. The proof is similar to that
of theorem 3.6.1. Every G-axiom Γ →∆contains some common formula A,
and by application of the weakening and the exchange rules, an (LK −{cut})-
proof of Γ →∆can be obtained from the axiom A →A. Next, we have to
show that every application of a G-rule can be replaced by a sequence of
(LK −{cut})-rules. There are twelve cases to consider. Note that the G-
rules ∧: right, ∨: left, ⊃: right, ⊃: left, ¬ : right, ¬ : left, ∀: right and
∃: left can easily be simulated in LK−{cut} using the exchange, contraction,
and corresponding (LK −{cut})-rules. The rules ∨: right and ∧: left are
handled as in theorem 3.6.1. Finally, we show how the G-rule ∀: left can be
transformed into a sequence of (LK −{cut})-rules, leaving the ∃: right case
as an exercise.
Γ, A[t/x], ∀xA, ∆→Λ
(several exchanges)
A[t/x], ∀xA, Γ, ∆→Λ
(∀: left)
∀xA, ∀xA, Γ, ∆→Λ
(contraction)
∀xA, Γ, ∆→Λ
(several exchanges)
Γ, ∀xA, ∆→Λ
The above (LK −{cut})-derivation simulates the G-rule ∀: left. This
conclude the proof of the theorem.

PROBLEMS
261
Corollary
(Gentzen Hauptsatz for LK) A sequent is LK-provable if and
only if it is LK-provable without a cut.
Note that the search procedure together with the method indicated in
theorem 6.2.1 actually provides an algorithm to construct a cut-free LK-proof
from an LK-proof with cuts.
Gentzen proved the above result by a very
diﬀerent method in which an LK-proof is (recursively) transformed into an
LK-proof without cut. Gentzen’s proof is entirely constructive since it does
not use any semantic arguments and can be found either in Takeuti, 1975,
Kleene, 1952, or in Gentzen’s original paper in Szabo, 1969.
In the next section, we discuss the case of ﬁrst-order languages with
equality symbol.
PROBLEMS
6.2.1.
Give LK-proofs for the following formulae:
∀xA ⊃A[t/x],
A[t/x] ⊃∃xA,
where t is free for x in A.
6.2.2.
Let x, y be any distinct variables. Let A, B be any formulae, C,
D any formulae not containing the variable x free, and let E be any
formula such that x is free for y in E. Give proof trees for the following
formulae:
∀xC ≡C
∃xC ≡C
∀x∀yA ≡∀y∀xA
∃x∃yA ≡∃y∃xA
∀x∀yE ⊃∀xE[x/y]
∃xE[x/y] ⊃∃x∃yE
∀xA ⊃∃xA
∃x∀yA ⊃∀y∃xA
6.2.3.
First, give G-proofs for the formulae of problem 6.2.2, and then con-
vert them into LK-proofs using the method of theorem 6.2.1.
6.2.4.
Give an LK-proof for
(A ⊃(B ⊃C)) →(B ⊃(A ⊃C)).
Give an LK-proof for
(B ⊃(C ⊃A)), (A ⊃(B ⊃C)) →(B ⊃(A ≡C))
using the cut rule, and another LK-proof without cut.

262
6/Gentzen’s Cut Elimination Theorem And Applications
6.2.5.
Given a set S of formulae, let Des(S) be the set of immediate descen-
dants of formulae in S as deﬁned in problem 5.5.18, and deﬁne Sn by
induction as follows:
S0 = S;
Sn+1 = Des(Sn).
Let S∗=

n≥0
Sn.
S∗is called the set of descendants of S.
(a) Every sequent Γ →∆corresponds to the set of formulae Γ ∪
{¬B | B ∈∆}. Prove that for every deduction tree for a sequent
Γ0 →∆0, the union of the sets of formulae Γ ∪{¬B | B ∈∆} for
all sequents Γ →∆occurring in that tree is a subset of S∗, where
S = Γ0 ∪{¬B | B ∈∆0} (this is called the subformula property).
(b) Deduce from (a) that not all formulae are provable.
6.3 The Gentzen System LKe With Equality
We now generalize the system LK to include equality.
6.3.1 Syntax of LKe
The rules and axioms of LKe are deﬁned as follows.
Deﬁnition 6.3.1
(The Gentzen system LKe) The Gentzen system LKe is
obtained from the system LK by adding the following sequents known as
equality axioms:
Let t, s1, ..., sn,t1, ..., tn be arbitrary L-terms. For every term t, the se-
quent
→t .= t
is an axiom.
For every n-ary function symbol f,
s1 .= t1, s2 .= t2, ..., sn .= tn →fs1...sn .= ft1...tn
is an axiom.
For every n-ary predicate symbol P (including .=),
s1 .= t1, s2 .= t2, ..., sn .= tn, Ps1...sn →Pt1...tn
is an axiom.

6.3 The Gentzen System LKe With Equality
263
It is easily shown that these axioms are valid. In order to generalize
theorem 6.2.1 it is necessary to deﬁne the concept of an atomic cut.
Deﬁnition 6.3.2
If the cut formula of a cut in an LKe-proof is an atomic
formula, the cut is called atomic. Otherwise, it is called an essential cut.
theorem 6.2.1 can now be generalized, provided that we allow atomic
cuts. However, some technical lemmas including an exchange lemma will be
needed in the proof.
6.3.2 A Permutation Lemma for the System G=
The following lemma holds in G=.
Lemma 6.3.1 Given a G=-proof T, there is a G=-proof T ′ such that all leaf
sequents of T ′ contain either atomic formulae or quantiﬁed formulae (that is,
formulae of the form ∀xB or ∃xB).
Proof : Given a nonatomic formula A not of the form ∀xB nor ∃xB,
deﬁne its weight as the number of logical connectives ∧, ∨, ¬, ⊃in it. The
weight of an atomic formula and of a quantiﬁed formula is 0. The weight of a
sequent is the sum of the weights of the formulae in it. The weight of a proof
tree is the maximum of the weights of its leaf sequents. We prove the lemma
by induction on the weight of the proof tree T. If weight(T) = 0, the lemma
holds trivially. Otherwise, we show how the weight of each leaf sequent of T
whose weight is nonzero can be decreased. This part of the proof proceeds by
cases. We cover some of the cases, leaving the others as an exercise.
Let S be any leaf sequent of T whose weight is nonzero. Then, for some
formula A, S is of the form
Γ1, A, Γ2 →∆1, A, ∆2.
If A is the only formula in S whose weight is nonzero, we can extend the leaf
S as follows:
Case 1: A is of the form B ∧C.
Γ1, B, C, Γ2 →∆1, B, ∆2
Γ1, B, C, Γ2 →∆1, C, ∆2
Γ1, B, C, Γ2 →∆1, B ∧C, ∆2
Γ1, B ∧C, Γ2 →∆1, B ∧C, ∆2
The weights of the new leaf sequents are strictly smaller than the weight
of S.

264
6/Gentzen’s Cut Elimination Theorem And Applications
Case 2: A is of the form ¬B.
B, Γ1, Γ2 →B, ∆1, ∆2
Γ1, Γ2 →B, ∆1, ¬B, ∆2
Γ1, ¬B, Γ2 →∆1, ¬B, ∆2
The weight of the new leaf sequent is strictly smaller than the weight of
S.
Case 3: A is of the form B ⊃C.
B, Γ1, Γ2 →B, C, ∆1, ∆2
C, B, Γ1, Γ2 →C, ∆1, ∆2
B, Γ1, B ⊃C, Γ2 →C, ∆1, ∆2
Γ1, B ⊃C, Γ2 →∆1, B ⊃C, ∆2
The weights of the new leaf sequents are strictly smaller than the weight
of S.
We leave the case in which A is of the form (C ∨D) as an exercise.
If A is not the only formula in Γ1, A, Γ2 →∆1, A, ∆2 whose weight
is nonzero, if we apply the corresponding rule to that formula, we obtain a
sequent S1 or two sequents S1 and S2 still containing A on both sides of the
arrow, and whose weight is strictly smaller than the weight of S.
Now, if we apply the above transformations to all leaf sequents of T
whose weight is nonzero, since the weight of each new leaf sequent is strictly
smaller than the weight of some leaf sequent of T, we obtain a tree T ′ whose
weight is strictly smaller than the weight of T. We conclude by applying the
induction hypothesis to T ′. This completes the proof.
Lemma 6.3.2 (i) Given a G=-proof tree T for a sequent Γ →A∧B, ∆satis-
fying the conclusion of lemma 6.3.1, another G=-proof T ′ can be constructed
such that, depth(T) = depth(T ′), and the rule applied at the root of T ′ is the
∧: right rule applied to the occurrence of A ∧B to the right of →.
(ii) Given a G=−proof tree T of a sequent Γ, A ⊃B →∆satisfying
the conclusion of lemma 6.3.1, another G=-proof tree T ′ can be constructed
such that, depth(T) = depth(T ′), and the rule applied at the root of T ′ is the
⊃: left rule applied to the occurrence of A ⊃B to the left of →.
Proof : (i) Let S be the initial subtree of T obtained by deleting the
descendants of every node closest to the root of T, where the ∧: right rule is
applied to the formula A ∧B to the right of →in the sequent labeling that
node. Since the proof tree T satisﬁes the condition of lemma 6.3.1, the rule
∧: right is applied to each occurrence of A∧B on the right of →. Hence, the
tree T has the following shape:

6.3 The Gentzen System LKe With Equality
265
Tree T
S1
Γ1 →A, ∆1
T1
Γ1 →B, ∆1
Γ1 →A ∧B, ∆1
Sm
Γm →A, ∆m
Tm
Γm →B, ∆m
Γm →A ∧B, ∆m
S
Γ →A ∧B, ∆
where the tree with leaves Γ1 →A ∧B, ∆1,..., Γm →A ∧B, ∆m is the tree S,
and the subtrees S1, T1,..., Sm, Tm are proof trees. Let S′ be the tree obtained
from S by replacing every occurrence in S of the formula A ∧B to the right
of →by A, and S′′ the tree obtained from S by replacing every occurrence of
A ∧B on the right of →by B. Since no rule is applied to an occurrence of
A ∧B on the right of →in S, both S′ and S′′ are well deﬁned. The following
proof tree T ′ satisﬁes the conditions of the lemma:
Tree T ′
S1
Γ1 →A, ∆1
Sm
Γm →A, ∆m
T1
Γ1 →B, ∆1
Tm
Γm →B, ∆m
S′
S′′
Γ →A, ∆
Γ →B, ∆
Γ →A ∧B, ∆
It is clear that depth(T) = depth(T ′).
(ii) The proof is similar to that of (i). The proof tree T can be converted
to T ′ as shown:
Tree T
S1
Γ1 →A, ∆1
T1
B, Γ1 →∆1
Γ1, A ⊃B →∆1
Sm
Γm →A, ∆m
Tm
B, Γm →∆m
Γm, A ⊃B →∆m
S
Γ, A ⊃B →∆

266
6/Gentzen’s Cut Elimination Theorem And Applications
Tree T ′
S1
Γ1 →A, ∆1
Sm
Γm →A, ∆m
T1
B, Γ1 →∆1
Tm
B, Γm →∆m
S′
S′′
Γ →A, ∆
B, Γ →∆
Γ, A ⊃B →∆
Note that depth(T) = depth(T ′).
Lemma 6.3.3
Every G=-proof tree T can be converted to a proof tree T ′
of the same sequent such that the rule applied to every sequent of the form
Γ →A ∧B, ∆or Γ, C ⊃D →∆is either the ∧: right rule applied to
the occurrence of A ∧B to the right of →, or the ⊃: left rule applied to
the occurrence of C ⊃D to the left of →. Furthermore, if T satisﬁes the
conditions of lemma 6.3.1, then T ′ has the same depth as T.
Proof : First, using lemma 6.3.1, we can assume that T has been con-
verted to a proof tree such that in all leaf sequents, all formulae are either
atomic or quantiﬁed formulae. Then, since lemma 6.3.2 preserves the depth
of such proof trees, we conclude by induction on the depth of proof trees using
lemma 6.3.2. Since the transformations of lemma 6.3.2 are depth preserving,
the last clause of the lemma follows.
6.3.3 Logical equivalence of G=, LKe, and LKe Without
Essential Cuts: Gentzen’s Hauptsatz for LKe Without
Essential Cuts
The generalization of theorem 6.2.1 is the following.
Theorem 6.3.1
(Logical equivalence of G=, LKe, and LKe without essen-
tial cuts) There is an algorithm to convert any LKe-proof of a sequent Γ →∆
into a G=-proof. There is an algorithm to convert any G=-proof of a sequent
Γ →∆into an LKe-proof without essential cuts.
Proof : The proof is similar to that of theorem 6.2.1. The proof diﬀers
because atomic cuts cannot be eliminated. By lemma 6.3.1, we can assume
that the axioms of proof trees are either atomic formulae or quantiﬁed formu-
lae. We proceed by induction on G=-proof trees having axioms of this form.
The base case is unchanged and, for the induction step, only the equality rules
of deﬁnition 6.2.1 need to be considered, since the other rules are handled as
in theorem 6.2.1.

6.3 The Gentzen System LKe With Equality
267
(i) The rule
Γ, t .= t →∆
Γ →∆
is simulated in LKe using an atomic cut as follows:
→t .= t
Γ, t .= t →∆
exchanges
t .= t, Γ →∆
atomic cut
Γ →∆
(ii) The root of the G=-proof tree T is the conclusion of the rule
Γ, (s1 .= t1) ∧... ∧(sn .= tn) ⊃(fs1...sn .= ft1...tn) →∆
Γ →∆
If the premise of this rule is an axiom in G=, this sequent contains a
same formula A on both sides, and an LKe-proof without cuts can be obtained
from A →A using the exchange and weakening rules. Otherwise, using lemma
6.3.3, an LKe-derivation with only atomic cuts can be constructed as follows.
By lemma 6.3.3, the proof tree T is equivalent to a proof tree of same depth
having the following shape:
Tn−1
Γ →sn−1 .= tn−1, ∆
Tn
Γ →sn .= tn, ∆
Γ →sn−1 .= tn−1 ∧sn .= tn, ∆
...
T1
Γ →s1 .= t1, ∆Γ →s2 .= t2 ∧... ∧sn .= tn, ∆
Γ →s1 .= t1 ∧... ∧sn .= tn, ∆
T0
fs1...sn .= ft1...tn, Γ →∆
Γ, s1 .= t1 ∧... ∧sn .= tn ⊃fs1...sn .= ft1...tn →∆
Using the axiom
s1 .= t1, ..., sn .= tn →fs1...sn .= ft1...tn
and applying the induction hypothesis to the G=-trees T0, T1,..., Tn, an LKe-
proof without essential cuts can be constructed:

268
6/Gentzen’s Cut Elimination Theorem And Applications
T ′
n
Γ →∆, sn .= tn
s1 .= t1, ..., sn .= tn →fs1...sn .= ft1...tn
sn .= tn, ..., s1 .= t1 →fs1...sn .= ft1...tn
sn−1 .= tn−1, ..., s1 .= t1, Γ →fs1...sn .= ft1...tn, ∆
...
T ′
1
Γ →∆, s1 .= t1
s1 .= t1, Γ →fs1...sn .= ft1...tn, ∆
Γ →fs1...sn .= ft1...tn, ∆
We ﬁnish the proof with one more atomic cut:
Γ →fs1..., sn .= ft1...tn, ∆
Γ →∆, fs1..., sn .= ft1...tn
T ′
0
Γ, fs1...sn .= ft1...tn →∆
fs1...sn .= ft1...tn, Γ →∆
Γ →∆
The trees T ′
0, T ′
1, ..., T ′
n have been obtained using the induction hypothesis.
Note that applications of exchange and contraction rules are implicit in the
above proofs.
(iii) The root of the G=-proof tree T is labeled with the conclusion of
the rule:
Γ, ((s1 .= t1) ∧... ∧(sn .= tn) ∧Ps1...sn) ⊃Pt1...tn →∆
Γ →∆
This case is handled as case (ii). This concludes the proof of the theorem.
Corollary
(A version of Gentzen Hauptsatz for LKe) A sequent is LKe-
provable if and only if it is LKe-provable without essential cuts.
PROBLEMS
6.3.1.
The set of closed equality axioms is the set of closed formulae given
below:
∀x(x .= x)

6.4 Gentzen’s Hauptsatz for Sequents in NNF
269
∀x1...∀xn∀y1...∀yn((x1 .= y1 ∧... ∧xn .= yn) ⊃
(f(x1, ..., xn) .= f(y1, ..., yn)))
∀x1...∀xn∀y1...∀yn(((x1 .= y1 ∧... ∧xn .= yn) ∧P(x1, ..., xn)) ⊃
P(y1, ..., yn))
Prove that the closed equality axioms are LKe-provable.
6.3.2.
Prove that the axioms of deﬁnition 6.3.1 are valid.
6.3.4.
Finish the proof of the cases in lemma 6.3.1.
6.3.5.
Prove case (iii) in the proof of theorem 6.3.1.
6.4 Gentzen’s Hauptsatz for Sequents in NNF
Combining ideas from Smullyan and Schwichtenberg (Smullyan, 1968, Bar-
wise, 1977), we formulate a sequent calculus G1nnf in which sequents consist
of formulae in negation normal form. Such a system shares characteristics of
both G and LK but the main diﬀerence is that sequents consist of pairs of
sets rather than pairs of sequences. The main reason for using sets rather
than sequences is that the structural rules become unnecessary. As a con-
sequence, an induction argument simpler than Gentzen’s original argument
(Szabo, 1969) can be used in the proof of the cut elimination theorem. The
advantage of considering formulae in negation normal form is that fewer in-
ference rules need to be considered. Since every formula is equivalent to a
formula in negation normal form, there is actually no loss of generality.
6.4.1 Negation Normal Form
The deﬁnition of a formula in negation normal form given in deﬁnition 3.4.8
is extended to the ﬁrst-order case as follows.
Deﬁnition 6.4.1
The set of formulae in negation normal form (for short,
NNF) is the smallest set of formulae such that
(1) For every atomic formula A, A and ¬A are in NNF;
(2) If A and B are in NNF, then (A ∨B) and (A ∧B) are in NNF.
(3) If A is in NNF, then ∀xA and ∃xA are in NNF.
Lemma 6.4.1
Every formula is equivalent to another formula in NNF.
Proof : The proof proceeds by induction on formulae as in lemma 3.4.4.
Careful checking of the proof of lemma 3.4.4 reveals that for the propositional
connectives, only the case where A is of the form ¬∀xB or ¬∃xB needs to be
considered. If A is of the form ¬∀xB, by lemma 5.3.6(8) ¬∀xB is equivalent

270
6/Gentzen’s Cut Elimination Theorem And Applications
to ∃x¬B, and since ¬B has fewer connectives than ¬∀xB, by the induction
hypothesis ¬B has a NNF B′. By lemma 5.3.7, A is equivalent to ∃xB′, which
is in NNF. The case where A is of the form ¬∃xB is similar. Finally, if A
is of the form ∀xB or ∃xB, by the induction hypothesis B is equivalent to a
formula B′ is NNF, and by lemma 5.3.7 ∀xB is equivalent to ∀xB′ and ∃xB
is equivalent to ∃xB′.
EXAMPLE 6.4.1
Let
A = ∀x(P(x) ∨¬∃y(Q(y) ∧R(x, y))) ∨¬(P(y) ∧¬∀xP(x))
The NNF of ¬∃y(Q(y) ∧R(x, y)) is
∀y(¬Q(y) ∨¬R(x, y)).
The NNF of ¬(P(y) ∧¬∀xP(x)) is
(¬P(y) ∨∀xP(x)).
The NNF of A is
∀x(P(x) ∨∀y(¬Q(y) ∨¬R(x, y))) ∨(¬P(y) ∨∀xP(x)).
We now deﬁne a new Gentzen system in which sequents are pairs of sets
of formulae in NNF. First, we treat the case of languages without equality.
6.4.2 The Gentzen System G1nnf
The axioms and inference rules of G1nnf are deﬁned as follows.
Deﬁnition 6.4.2 The sequents of the system G1nnf are pairs Γ →∆, where
Γ and ∆are ﬁnite sets of formulae in NNF. Given two sets of formuae Γ and
∆, the expression Γ, ∆denotes the union of the sets Γ and ∆, and similarly,
if A is a formula, Γ, A denotes the set Γ ∪{A}. The inference rules are the
rules listed below:
(1) Cut rule:
Γ →∆, A
A, Λ →Θ
Γ, Λ →∆, Θ
A is called the cut formula of this inference.
(2) Propositional logical rules:
A, Γ →∆
A ∧B, Γ →∆(∧: left)
and
B, Γ →∆
A ∧B, Γ →∆(∧: left)

6.4 Gentzen’s Hauptsatz for Sequents in NNF
271
Γ →∆, A
Γ →∆, B
Γ →∆, A ∧B
(∧: right)
A, Γ →∆
B, Γ →∆
A ∨B, Γ →∆
(∨: left)
Γ →∆, A
Γ →∆, A ∨B (∨: right)
and
Γ →∆, B
Γ →∆, A ∨B (∨: right)
In the rules above, A ∨B and A ∧B are called the principal formulae
and A, B the side formulae of the inference.
(3) Quantiﬁer rules
In the quantiﬁer rules below, x is any variable and y is any variable free
for x in A and not free in A, unless y = x (y /∈FV (A) −{x}). The term t is
any term free for x in A.
A[t/x], Γ →∆
∀xA, Γ →∆
(∀: left)
Γ →∆, A[y/x]
Γ →∆, ∀xA
(∀: right)
A[y/x], Γ →∆
∃xA, Γ →∆
(∃: left)
Γ →∆, A[t/x]
Γ →∆, ∃xA
(∃: right)
In both the (∀: right)-rule and the (∃: left)-rule, the variable y does
not occur free in the lower sequent. In these rules, the variable y is called the
eigenvariable of the inference. The condition that the eigenvariable does not
occur free in the conclusion of the rule is called the eigenvariable condition.
The formula ∀xA (or ∃xA) is called the principal formula of the inference,
and the formula A[t/x] (or A[y/x]) the side formula of the inference.
The axioms of G1nnf are all sequents of the form
Γ, A →A, ∆,
Γ, ¬A →¬A, ∆,
Γ, A, ¬A →∆, or
Γ →∆, A, ¬A,
with A atomic.

272
6/Gentzen’s Cut Elimination Theorem And Applications
The notions of deduction trees and proof trees are deﬁned as usual, but
with the rules and axioms of G1nnf.
It is readily shown that the system
G1nnf is sound. We can also prove that G1nnf is complete for sequents in
NNF.
6.4.3 Completeness of G1nnf
The following lemma is shown using theorem 5.5.1.
Lemma 6.4.2
(Completeness of G1nnf) Every valid G1nnf-sequent has a
G1nnf-proof.
Proof : First, we check that the proof given in theorem 5.5.1 can be
adapted to hold for sequents consisting of sets rather than sequences. In order
to simulate the ∀: left rule and the ∃: right rule of G using the quantiﬁer
rules of G1nnf, we use the fact that in G1nnf, sequents consist of sets. Since
∀xB, Γ →∆and ∀xB, ∀xB, Γ →∆actually denote the same sequent, we can
apply the ∀: left rule (of G1nnf) to ∀xB, with the formulae in ∀xB, Γ and
∆as auxiliary formulae, obtaining:
∀xB, B[t/x], Γ →∆
∀xB, Γ →∆
The case of ∃: right is similar. We simulate the ∧: left rule of G and
the ∨: right of G as in the proof of theorem 3.6.1. For example, the following
derivation simulates ∧: left of G:
A, B, Γ →∆
∧: left applied to A
A ∧B, B, Γ →∆
∧: left applied to B
A ∧B, Γ →∆
We obtain proofs in which the conditions for declaring that a sequent is
an axiom are as follows: A sequent Γ →∆is an axiom iﬀany of the following
conditions holds:
(i) Γ and ∆have some formula A in common; or
(ii) Γ contains some atomic formula B and its negation ¬B; or
(iii) ∆contains some atomic formula B and its negation ¬B.
However, the formula A may not be a literal (that is, an atomic formula,
or the negation of an atomic formula). To make sure that in (i) A is a literal,
we use the method of lemma 6.3.1, complemented by the cases of quantiﬁed
formulae. We consider the case in which A = ∀xB is on the left of →, the
case A = ∃xB on the right of →being similar. Assume that the axiom is
Γ, ∀xB →∆, ∀xB. Then we have the following proof:

6.4 Gentzen’s Hauptsatz for Sequents in NNF
273
Γ, B[z/x] →∆, B[z/x]
Γ, ∀xB →∆, B[z/x]
Γ, ∀xB →∆, ∀xB
where z is a new variable.
The top sequent is an axiom, and B[z/x] has fewer connectives than
∀xB. We conclude by induction on the number of connectives in B[z/x]. The
details are left as an exercise.
6.4.4 The Cut Elimination Theorem for G1nnf
We now proceed with the proof of the cut elimination theorem for G1nnf.
The proof uses a method due to Schwichtenberg (adapted from Tait, Tait,
1968), which consists of a single induction on the cut-rank of a proof with
cut. This proof is simpler than Gentzen’s original, because the system G1nnf
does not have structural rules. This is the reason a simple induction on the
cut-rank works (as opposed to the the induction used in Gentzen’s original
proof, which uses a lexicographic ordering). Furthermore, the proof of the
theorem also yields an upper bound on the size of the resulting cut-free proof.
The key parameter of the proof, is the cut-rank of a G1nnf-proof. First, we
need to deﬁne the degree of a formula in NNF. Roughly speaking, the degree of
a formula in NNF is the depth of the tree representing that formula, ignoring
negations.
Deﬁnition 6.4.3
The degree |A| of a formula A in NNF is deﬁned induc-
tively as follows:
(i) If A is an atomic formula or the negation of an atomic formula, then
|A| = 0;
(ii) If A is either of the form (B ∨C) or (B ∧C), then
|A| = max(|B|, |C|) + 1;
(iii) If A is either of the form ∀xB or ∃xB, then
|A| = |B| + 1.
The cut-rank is deﬁned as follows.
Deﬁnition 6.4.4 Let T be a G1nnf-proof. The cut-rank c(T) of T is deﬁned
inductively as follows. If T is an axiom, then c(T) = 0. If T is not an axiom,
the last inference has either one or two premises. In the ﬁrst case, the premise

274
6/Gentzen’s Cut Elimination Theorem And Applications
of that inference is the root of a subtree T1. In the second case, the left premise
is the root of a subtree T1, and the right premise is the root of a subtree T2.
If the last inference is not a cut, then if it has a single premise,
c(T) = c(T1),
else
c(T) = max(c(T1), c(T2)).
If the last inference is a cut with cut formula A, then
c(T) = max(|A| + 1, c(T1), c(T2)).
Note that c(T) = 0 iﬀT is cut free. We will need a number of lemmas
to establish the cut elimination theorem for G1nnf. In some of these proofs,
it will be necessary to replace in a proof all free occurrences of variable y by a
new variable z not occurring in the proof. This substitution process is deﬁned
as follows.
Deﬁnition 6.4.5 Given a formula A, we say that the variable y is not bound
in the formula A iﬀy /∈BV (A). The variable y is not bound in the sequent
Γ →∆iﬀy is not bound in any formula in Γ or ∆. The variable y is not
bound in the deduction tree T iﬀit is not bound in any sequent occurring in
T. Given a formula A and two variables y, z, the formula A[z/y] is deﬁned
as in deﬁnition 5.2.6. For a sequent Γ →∆, the sequent (Γ →∆)[z/y] is
the sequent obtained by substituting z for y in all formulae in Γ →∆. For a
deduction tree T, a variable y not bound in T, and a variable z not occurring
in T, the deduction tree T[z/y] is the result of replacing every sequent Γ →∆
in T by (Γ →∆)[z/y].
This operation can be deﬁned more precisely by
induction on proof trees, the simple details being left to the reader.
A similar deﬁnition can be given for the result T[t/y] of substituting a
term t for a variable y not bound in T, provided that t is free for y in every
formula in which it is substituted, and that y and the variables in FV (t) are
distinct from all eigenvariables in T. In order to justify that T[z/y] are T[t/y]
are indeed proof trees when T is, the following technical lemma is needed.
Lemma 6.4.3
Let Γ →∆be a sequent and T an G1nnf-proof for Γ →∆.
Assume that y is any variable not bound in the proof tree T.
(i) For any variable z not occurring in T, the result T[z/y] of substituting
z for all occurrences of y in T is a proof tree for Γ[z/y] →∆[z/y].
(ii) If t is a term free for y in every formula in which it is substituted,
and y and the variables in FV (t) are distinct from all eigenvariables in T,
then T[t/y] is a proof tree for Γ[t/y] →∆[t/y].
Proof : We proceed by induction on proof trees. We only treat some key
cases, leaving the others as an exercise. We consider (i). If T is an axiom

6.4 Gentzen’s Hauptsatz for Sequents in NNF
275
Γ →∆, it is clear that (Γ →∆)[z/y] is also an axiom. The propositional
rules present no diﬃculty and are left to the reader. We consider two of the
quantiﬁer rules: ∀: left and ∀: right.
Case 1: The bottom inference is ∀: left:
T1
A[t/x], Γ →∆
∀xA, Γ →∆
where t is free for x in A.
By the induction hypothesis, T1[z/y] is a proof tree of A[t/x][z/y], Γ[z/y]
→∆[z/y]. Since we have assumed that y is not bound in the proof T, y ̸= x.
Hence, (∀xA)[z/y] = ∀xA[z/y]. By the result of problem 5.2.7,
A[t/x][z/y] = A[z/y][t[z/y]/x],
and since z does not occur in T, t[z/y] is free for x in A[z/y]. But then, T[z/y]
is the proof tree:
T1[z/y]
A[z/y][t[z/y]/x], Γ[z/y] →∆[z/y]
∀xA[z/y], Γ[z/y] →∆[z/y]
Case 2: The bottom inference is ∀: right:
T1
Γ →∆, A[w/x]
Γ →∆, ∀xA
where w is not free in Γ →∆, ∀xA.
There are two subcases.
Subcase 2.1: If y = w, since w does not occur free in Γ →∆, ∀xA, the
variable w is not free in ∀xA, Γ or ∆. By the induction hypothesis, T1[z/y]
is a proof tree for
(Γ →∆, A[y/x])[z/y] = Γ →∆, A[z/x].
Also, since z does not occur in T, z does not occur in Γ →∆, ∀xA, the
∀: right rule is applicable and T[z/y] is a proof tree.
Subcase 2.2: y ̸= w. By the induction hypothesis, T1[z/y] is a proof tree
for Γ[z/y] →∆[z/y], A[w/x][z/y]. By problem 5.2.7, we have
A[w/x][z/y] = A[z/y][w[z/y]/x],

276
6/Gentzen’s Cut Elimination Theorem And Applications
but since w ̸= y,
A[w/x][z/y] = A[z/y][w/x].
Also, since z does not occur in T, z ̸= w, and so w does not occur free in
Γ[z/y] →∆[z/y], ∀xA[z/y]. Hence, the ∀: right rule is applicable and T[z/y]
is a proof tree:
T1[z/y]
Γ[z/y] →∆[z/y], A[z/y][w/x]
Γ[z/y] →∆[z/y], ∀xA[z/y]
It should be noted that in the proof of (ii), the condition that y is distinct
from all eigenvariables in T rules out subcase 2.1.
Lemma 6.4.4
(Substitution lemma) Let T be a G1nnf-proof of a sequent
Γ →∆such that the variable x is not bound in T. For any term t free for
x in Γ →∆, a proof T ′(t) of Γ[t/x] →∆[t/x] can be constructed such that
T ′(t) and T have same depth, x and the variables in FV (t) are distinct from
all eigenvariables in T ′(t) and c(T) = c(T ′(t)).
Proof : By induction on proof trees using lemma 6.4.3. For a similar
proof, see lemma 7.3.1.
Lemma 6.4.5
(Weakening lemma) Given a G1nnf-proof T of a sequent
Γ →∆, for any formula A (in NNF), a proof T ′ of A, Γ →∆(resp. Γ →∆, A)
can be obtained such that T and T ′ have the same depth, all variables free in
A are distinct from all eigenvariables in T and c(T ′) = c(T).
Proof :
Straightforward induction on proof trees, similar to that of
lemma 6.4.3.
The proof T ′ given by the weakening lemma will also be denoted by
(T, A).
Lemma 6.4.6 (Inversion lemma) (i) If a sequent Γ →∆, A∧B has a G1nnf-
proof T (resp. A ∨B, Γ →∆has a G1nnf-proof T), a G1nnf-proof T1 of Γ →
∆, A and a G1nnf-proof T2 of Γ →∆, B can be constructed (resp. a G1nnf-
proof T1 of A, Γ →∆and a G1nnf-proof T2 of B, Γ →∆can be constructed)
such that, depth(T1), depth(T2) ≤depth(T) and c(T1), c(T2) ≤c(T).
(ii) If Γ →∆, ∀xB has a G1nnf-proof T (resp.
∃xB, Γ →∆has a
G1nnf-proof T), then for any variable y not bound in T and distinct from all
eigenvariables in T, a G1nnf-proof T1 of Γ →∆, B[y/x] can be constructed
(resp. a G1nnf-proof T1 of B[y/x], Γ →∆can be constructed), such that
depth(T1) ≤depth(T) and c(T1) ≤c(T).
Proof : The proofs of (i) and (ii) are similar, both by induction on proof
trees. We consider (ii), leaving (i) as an exercise.

6.4 Gentzen’s Hauptsatz for Sequents in NNF
277
If ∀xB belongs to ∆, the result follows by the weakening lemma (lemma
6.4.5), using the proof (T, B[y/x]) given by that lemma. If ∀xB does not
belong to ∆, there are two cases.
Case 1: ∀xB is not the principal formula of the last inference. There are
several subcases depending on that inference. Let us consider the ∧: right
rule, the other subcases being similar and left as an exercise. The proof has
the form
S1
Γ →∆, ∀xB, C
S2
Γ →∆, ∀xB, D
Γ →∆, ∀xB, C ∧D
By the induction hypothesis, for any variable y not bound in S1 or S2
and distinct from all eigenvariables in S1 or S2, we can ﬁnd proofs T1 for Γ →
∆, B[y/x], C and T2 for Γ →∆, B[y/x], D, such that depth(Ti) ≤depth(Si)
and c(Ti) ≤c(Si), for i = 1, 2. We conclude using the following proof:
T1
Γ →∆, B[y/x], C
T2
Γ →∆, B[y/x], D
Γ →∆, B[y/x], C ∧D
Case 2: ∀xB is the principal formula of the last inference. Using the
weakening lemma, we can make sure that the last inference is of the form
T1
Γ →∆, ∀xB, B[y/x]
Γ →∆, ∀xB
replacing the proof T by (T, ∀xB) if necessary. Using lemma 6.4.3, we can
also make sure that y is not bound in T1 (or T) and is distinct from all
eigenvariables in T1 (and T). Then, the induction hypothesis applies to the
variable y in the lower sequent of the proof T1, and we can ﬁnd a proof T ′
1
of Γ →∆, B[y/x] such that depth(T1) < depth(T) and c(T1) ≤c(T). Using
lemma 6.4.3 again, we actually have a proof of Γ →∆, B[z/x] for any variable
z not bound in T ′
1 and distinct from all eigenvariables in T ′
1, establishing (ii).
The proof of the other cases is similar.
We are now ready for the main lemma which, shows how cuts are elim-
inated.
Lemma 6.4.7
(Reduction lemma for G1nnf) Let T1 be a G1nnf-proof of
Γ →∆, A, and T2 a G1nnf-proof of A, Λ →Θ, and assume that
c(T1), c(T2) ≤|A|.

278
6/Gentzen’s Cut Elimination Theorem And Applications
A G1nnf-proof T of
Γ, Λ →∆, Θ
can be constructed, such that
depth(T) ≤depth(T1) + depth(T2)
and
c(T) ≤|A|.
Proof : We proceed by induction on depth(T1) + depth(T2).
Case 1: Either A is not the principal formula of the last inference of T1,
or A is not the principal formula of the last inference of T2. By symmetry,
we can assume the former. There are several subcases, depending on the last
inference. Let us consider the ∧: right rule, the other subcases being similar
and left as an exercise. The proof has the form
S1
Γ →∆′, C, A
S2
Γ →∆′, D, A
Γ →∆′, C ∧D, A
By the induction hypothesis, we can ﬁnd proofs T ′
1 for Γ, Λ →∆′, Θ, C
and T ′
2 for Γ, Λ →∆′, Θ, D, such that depth(T ′
i) < depth(T1)+depth(T2) and
c(T ′
i) ≤|A|, for i=1,2. The result follows by the inference
Γ, Λ →∆′, Θ, C
Γ, Λ →∆′, Θ, D
Γ, Λ →∆′, C ∧D, Θ
Case 2: A is the principal formula of the last inference of both T1 and
T2.
Case 2.1: A is a literal; that is, an atomic formula, or the negation of an
atomic formula. In this case, both Γ →∆, A and A, Λ →Θ are axioms. By
considering all possible cases, it can be veriﬁed that Γ, Λ →∆, Θ is an axiom.
For example, if A is atomic, ∆contains ¬A and Λ contains ¬A, Γ, Λ →∆, Θ
is an axiom.
Case 2.2: A is of the form (B ∨C). Using the weakening lemma, we can
make sure that the last inference of T1 is of the form
S0
Γ →∆, A, B
Γ →∆, A
or

6.4 Gentzen’s Hauptsatz for Sequents in NNF
279
S0
Γ →∆, A, C
Γ →∆, A
replacing T1 by (T1, A) if necessary. Consider the ﬁrst case, the other being
similar.
By the induction hypothesis, we can ﬁnd a proof T ′
1 for Γ, Λ →
∆, Θ, B, such that depth(T ′
1) < depth(T1) + depth(T2) and c(T ′
1) ≤|A|. By
the inversion lemma, we can ﬁnd a proof T ′
2 of B, Λ →Θ such that depth(T ′
2) ≤
depth(T2) and c(T ′
2) ≤|A|. The following proof T ′
T ′
1
Γ, Λ →∆, Θ, B
T ′
2
B, Λ →Θ
Γ, Λ →∆, Θ
is such that depth(T ′) ≤depth(T1) + depth(T2) and has cut-rank c(T ′) ≤|A|,
since |B| < |A|.
Case 2.3: A is of the form (B ∧C). This case is symmetric to case 2.2.
Case 2.4: A is of the form ∃xB. As in case 2.2, we can assume that A
belongs to the premise of the last inference in T1, so that T1 is of the form
S0
Γ →∆, A, B[t/x]
Γ →∆, A
By the induction hypothesis, we can ﬁnd a proof tree T ′
1 for Γ, Λ →
∆, Θ, B[t/x], such that depth(T ′
1) < depth(T1) + depth(T2) and c(T ′
1) ≤|A|.
By the inversion lemma, for any variable y not bound in T2 and distinct from
all eigenvariables in T2, there is a proof T ′
2 for B[y/x], Λ →Θ, such that
depth(T ′
2) ≤depth(T2) and c(T ′
2) ≤|A|. By the substitution lemma, we can
construct a proof T ′′
2 for B[t/x], Λ →Θ, also such that depth(T ′′
2 ) ≤depth(T2)
and c(T ′′
2 ) ≤|A|. Since |B[t/x]| < |A|, the proof obtained from T ′
1 and T ′′
2 by
applying a cut to B[t/x] has cut-rank ≤|A|.
Case 2.5: A is of the form ∀xB. This case is symmetric to case 2.4.
This concludes all the cases.
Finally, we can prove the cut elimination theorem.
The function exp(m, n, p) is deﬁned recursively as follows:
exp(m, 0, p) = p;
exp(m, n + 1, p) = mexp(m,n,p).

280
6/Gentzen’s Cut Elimination Theorem And Applications
This function grows extremely fast in the argument n. Indeed, exp(m, 1,
p) = mp, exp(m, 2, p) = mmp, and in general, exp(m, n, p) is an iterated stack
of exponentials of height n, topped with a p:
exp(m, n, p) = mmm···mp 
n
The Tait-Schwichtenberg’s version of the cut elimination theorem for G1nnf
follows.
Theorem 6.4.1
(Cut elimination theorem for G1nnf) Let T be a G1nnf-
proof with cut-rank c(T) of a sequent Γ →∆. A cut-free proof T ∗for Γ →∆
such that depth(T ∗) ≤exp(2, c(T), depth(T)) can be constructed.
Proof : We prove the following claim by induction on the depth of proof
trees.
Claim: Let T be a G1nnf-proof with cut-rank c(T) for a sequent Γ →∆.
If c(T) > 0 then we can construct a proof T ′ for Γ →∆, such that
c(T ′) < c(T)
and
depth(T ′) ≤2depth(T ).
Proof of Claim: If either the last inference of T is not a cut, or it is a
cut and c(T) > |A| + 1, where A is the cut formula of the last inference, we
can apply the induction hypothesis to the immediate subtrees T1 or T2 (or
T1) of T. We are left with the case in which the last inference is a cut and
c(T) = |A| + 1. The proof is of the form
T1
Γ →∆, A
T2
A, Γ →∆
Γ →∆
By the induction hypothesis, we can construct a proof T ′
1 for Γ →∆, A
and a proof T ′
2 for A, Γ →∆, such that c(T ′
i) ≤|A| and depth(T ′
i) ≤2depth(Ti),
for i = 1, 2. Applying the reduction lemma, we obtain a proof T ′ such that,
c(T ′) ≤|A| and depth(T ′) ≤depth(T ′
1) + depth(T ′
2). But
depth(T ′
1) + depth(T ′
2) ≤2depth(T1) + 2depth(T2) ≤
2max(depth(T1),depth(T2))+1 = 2depth(T ).
Hence, the claim holds for T ′.
The proof of the theorem follows by induction on c(T), and by the
deﬁnition of exp(2, m, n).
It is remarkable that theorem 6.4.1 provides an upper bound on the
depth of cut-free proofs obtained by converting a proof with cuts. Note that
the “blow up” in the size of the proof can be very large.

6.4 Gentzen’s Hauptsatz for Sequents in NNF
281
Cut-free proofs are “direct” (or analytic), in the sense that all inferences
are purely mechanical, and thus require no ingenuity. Proofs with cuts are
“indirect” (or nonanalytic), in the sense that the cut formula in a cut rule may
not be a subformula of any of the formulae in the conclusion of the inference.
Theorem 6.4.1 suggests that if some ingenuity is exercised in constructing
proofs with cuts, the size of a proof can be reduced signiﬁcantly. It gives a
measure of the complexity of proofs. Without being very rigorous, we can say
that theorem 6.4.1 suggests that there are theorems that have no easy proofs,
in the sense that if the steps are straightforward, the proof is very long, or else
if the proof is short, the cuts are very ingenious. Such an example is given in
Statman, 1979.
We now add equality axioms to the system G1nnf.
6.4.5 The System G1nnf
=
The axioms and inference rules of the system G1nnf
=
are deﬁned as follows.
Deﬁnition 6.4.6
The system G1nnf
=
is obtained by adding to G1nnf the
following sequents as axioms. All sequents of the form
(i) Γ →∆, t .= t;
(ii) For every n-ary function symbol f,
Γ, s1 .= t1, s2 .= t2, ..., sn .= tn →∆, fs1...sn .= ft1...tn
(iii) For every n-ary predicate symbol P (including .=),
Γ, s1 .= t1, s2 .= t2, ..., sn .= tn, Ps1...sn →∆, Pt1...tn
and all sequents obtained from the above sequents by applications of ¬ : left
and ¬ : right rules to the atomic formulae t .= t, si .= ti, fs1...sn .= ft1...tn,
Ps1...sn and Pt1...tn.
For example, →¬(s .= t), f(s) .= f(t) is an equality axiom. It is obvious
that these sequents are valid and that the system G1nnf
=
is sound.
Lemma 6.4.8
(Completeness of G1nnf
=
) For the system G1nnf
=
, every valid
sequent is provable.
Proof : The lemma can be proved using the completeness of LKe (the-
orem 6.3.1). First, we can show that in an LKe-proof, all weakenings can be
moved above all other inferences. Then, we can show how such a proof can
be simulated by a G1nnf
=
-proof. The details are rather straighforward and are
left as an exercise.

282
6/Gentzen’s Cut Elimination Theorem And Applications
6.4.6 The Cut Elimination Theorem for G1nnf
=
Gentzen’s cut elimination theorem also holds for G1nnf
=
if an inessential cut
is deﬁned as a cut in which the cut formula is a literal B or ¬B, where B is
equation s .= t (but not an atomic formula of the form Ps1...sn, where P is a
predicate symbol diﬀerent from .=). This has interesting applications, such as
the completeness of equational logic (see the problems). The proof requires
a modiﬁcation of lemma 6.4.6. The cut-rank of a proof is now deﬁned by
considering essential cuts only.
Lemma 6.4.9
(Reduction lemma for G1nnf
=
) Let T1 be a G1nnf
=
-proof of
Γ →∆, A, and T2 a G1nnf
=
-proof of A, Λ →Θ, and assume that c(T1), c(T2) ≤
|A|. Let m be the maximal rank of all predicate symbols P such that some
literal Pt1...tn or ¬Pt1...tn is a cut formula in either T1 or T2. A G1nnf
=
-proof
T of Γ, Λ →∆, Θ can be constructed such that
c(T) ≤|A|
and
depth(T) ≤depth(T1) + depth(T2) + m.
Proof : We proceed by induction on depth(T1) + depth(T2). The only
new case is the case in which A is a literal of the form Pt1...tn or ¬Pt1...tn,
and one of the two axioms Γ →∆, A and A, Λ →Θ is an equality axiom.
Assume that A = Pt1...tn, the other case being similar. If Γ →∆or Λ →Θ
is an axiom, then Γ, Λ →∆, Θ is an axiom. Otherwise, we have three cases.
Case 1: Γ →∆, A is an equality axiom, but A, Λ →Θ is not. Then,
either ¬A is in Λ or A is in Θ, and Γ →∆, A is obtained from some equality
axiom of the form
Γ′, s1 .= t1, s2 .= t2, ..., sn .= tn, Ps1...sn →∆′, Pt1...tn
by application of ¬ : rules. Since either Pt1...tn is in Θ or ¬Pt1...tn is in
Λ, the sequent Γ, Λ →∆, Θ is an equality axiom also obtained from some
sequent of the form
Γ′′, s1 .= t1, s2 .= t2, ..., sn .= tn, Ps1...sn →∆′′, Pt1...tn.
Case 2: Γ →∆, A is not an equality axiom, but A, Λ →Θ is. This is
similar to case 1.
Case 3: Both Γ →∆, A and A, Λ →Θ are equality axioms. In this case,
Γ →∆, A is obtained from some equality axiom of the form
Γ′, s1 .= t1, s2 .= t2, ..., sn .= tn, Ps1...sn →∆′, Pt1...tn
by application of ¬ : rules, and A, Λ →Θ is obtained from some equality
axiom of the form
Λ′, t1 .= r1, t2 .= r2, ..., tn .= rn, Pt1...tn →Θ′, Pr1...rn

6.4 Gentzen’s Hauptsatz for Sequents in NNF
283
by applications of ¬ : rules.
Then, Γ, Λ →∆, Θ is obtained by applying negation rules to the sequent
Γ′, Λ′, s1 .= t1, s2 .= t2, ..., sn .= tn, t1 .= r1, t2 .= r2, ..., tn .= rn, Ps1...sn
→∆′, Θ′, Pr1...rn.
A proof with only inessential cuts can be given using axioms derived by
applying ¬ : rules to the provable (with no essential cuts) sequents
si .= ti, ti .= ri →si .= ri,
for i = 1, ..., n, and the axiom
Γ′, Λ′, s1 .= r1, ..., sn .= rn, Ps1...sn →∆′, Θ′, Pr1...rn,
and n inessential cuts (the i-th cut with cut formula si .= ri or ¬si .= ri). The
depth of this proof is n, which is bounded by m. The rest of the proof is left
as an exercise.
We obtain the following cut elimination theorem.
Theorem 6.4.2
(Cut elimination theorem for G1nnf
=
) Let T be a G1nnf
=
-
proof with cut-rank c(T) for a sequent Γ →∆, and let m be deﬁned as in
lemma 6.4.9. A G1nnf
=
-proof T ∗for Γ →∆without essential cuts such that
depth(T ∗) ≤exp(m + 2, c(T), depth(T)) can be constructed.
Proof : The following claim can be shown by induction on the depth of
proof trees:
Claim: Let T be a G1nnf
=
-proof with cut-rank c(T) for a sequent Γ →∆.
If c(T) > 0 then we can construct a G1nnf
=
-proof T ′ for Γ →∆, such that
c(T ′) < c(T)
and
depth(T ′) ≤(m + 2)depth(T ).
The details are left as an exercise.
Note: The cut elimination theorem with inessential cuts (with atomic
cut formulae of the form s .= t) also holds for LKe. The interested reader is
refered to Takeuti, 1975.
As an application of theorem 6.3.1, we shall prove Craig’s interpolation
theorem in the next section. The signiﬁcance of a proof of Craig’s theorem
using theorem 6.3.1 is that an interpolant can actually be constructed. In turn,
Craig’s interpolation theorem implies two other important results: Beth’s
deﬁnability theorem, and Robinson’s joint consistency theorem.

284
6/Gentzen’s Cut Elimination Theorem And Applications
PROBLEMS
6.4.1.
Convert the following formulae to NNF:
(¬∀xP(x, y) ∨∀xR(x, y))
∀x(P(x) ⊃¬∃yR(x, y))
(¬∀x¬∀y¬∀zP(x, y) ∨¬∃x¬∃y(¬∃zQ(x, y, z) ⊃R(x, y)))
6.4.2.
Convert the following formulae to NNF:
(∃x∀yP(x, y) ∧∀y∃xP(y, x))
(¬(∀xP(x) ∨∃y¬Q(y)) ∨(∀zG(z) ∨∃w¬Q(w)))
(¬∀x(P(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w)))
6.4.3.
Write a computer program for converting a formula to NNF.
6.4.4.
Give the details of the proof of lemma 6.4.2.
6.4.5.
Finish the proof of the cases that have been left out in the proof of
lemma 6.4.3.
6.4.6.
Prove lemma 6.4.4.
6.4.7.
Prove lemma 6.4.5.
6.4.8.
Finish the proof of the cases that have been left out in the proof of
lemma 6.4.6.
6.4.9.
Finish the proof of the cases that have been left out in the proof of
lemma 6.4.7.
6.4.10. Prove that for any LKe-proof, all weakenings can be moved above all
other kinds of inferences. Use this result to prove lemma 6.4.8.
6.4.11. Finish the proof of the cases that have been left out in the proof of
lemma 6.4.9.
6.4.12. Give the details of the proof of theorem 6.4.2.
6.4.13. Given a set S of formulae, let Des(S) be the set of immediate descen-
dants of formulae in S as deﬁned in problem 5.5.18, and deﬁne Sn by
induction as follows:
S0 = S;
Sn+1 = Des(Sn).
Let S∗=

n≥0
Sn.
S∗is called the set of descendants of S.

PROBLEMS
285
(a) Prove that for every deduction tree without essential cuts for a
sequent Γ0 →∆0, the formulae in the sets of formulae Γ ∪{¬B | B ∈
∆} for all sequents Γ →∆occurring in that tree, belong to S∗or are
equations of the form s .= t, where S = Γ0 ∪{¬B | B ∈∆0}.
(b) Deduce from (a) that not all G1nnf
=
-formulae are provable.
6.4.14. Let L be a ﬁrst-order language with equality and with function sym-
bols and constant symbols, but no predicate symbols. Such a language
will be called equational.
Let e1, ..., em →e be a sequent where each ei is a closed formula of the
form ∀x1...∀xn(s .= t), called a universal equation, where {x1, ..., xn}
is the set of variables free in s .= t, and e is an atomic formula of
the form s .= t, called an equation. Using theorem 6.4.2, prove that if
e1, ..., en →e is G1nnf
=
-provable, then it is provable using only axioms
(including the equality axioms), the cut rule applied to equations (of
the form s .= t), weakenings, and the ∀: left rule.
∗6.4.15. Let L be an equational language as deﬁned in problem 6.4.14.
A
substitution function (for short, a substitution) is any function σ :
V →TERML assigning terms to the variables in V. By theorem
2.4.1, there is a unique homomorphism σ : TERML →TERML
extending σ and deﬁned recursively as follows:
For every variable x ∈V, σ(x) = s(x).
For every constant c, σ(c) = c.
For every term ft1...tn ∈TERML,
σ(ft1...tn) = fσ(t1)...σ(tn).
By abuse of language and notation, the function σ will also be called
a substitution, and will often be denoted by σ.
The subset X of V consisting of the variables such that s(x) ̸= x
is called the support of the substitution. In what follows, we will be
dealing with substitutions of ﬁnite support. If a substitution σ has
ﬁnite support {y1, ..., yn} and σ(yi) = si, for i = 1, .., n, for any term
t, the substitution instance σ(t) is also denoted as t[s1/y1, ..., sn/yn].
Let E =< e1, ..., en > be a sequence of equations, and E′ the sequence
of their universal closures. (Recall that for a formula A with set of free
variables FV (A) = {x1, ..., xn}, ∀x1...∀xnA is the universal closure
of A.)
We deﬁne the relation −→E on the set TERML of terms as follows.
For any two terms t1, t2,
t1 −→E t2

286
6/Gentzen’s Cut Elimination Theorem And Applications
iﬀthere is some term r, some equation s .= t ∈E, some substitution
σ with support FV (s) ∪FV (t), and some tree address u in r, such
that,
t1 = r[u ←σ(s)], and t2 = r[u ←σ(t)].
When t1 −→E t2, we say that t1 rewrites to t2. In words, t1 rewrites
to t2 iﬀt2 is obtained from t1 by ﬁnding a subterm σ(s) of t1 (called
a pattern) which is a substitution instance of the left hand side s of
an equation s .= t ∈E, and replacing this subterm by the subterm
σ(t) obtained by applying the same substitution σ to the right hand
side t of the equation.
Let ←→E be the relation deﬁned such that
t1 ←→E t2
iﬀ
either t1 −→E t2
or
t2 −→E t1,
and let
∗
←→E be the reﬂexive and transitive closure of ←→E.
Our goal is to prove that for every sequence E =< e1, ..., en > of equa-
tions and for every equation s .= t, if E′ is the sequence of universal
closures of equations in E, then
E′ →s .= t
is G1nnf
=
-provable iﬀ
s
∗
←→E t.
During the proof that proceeds by induction, we will have to consider
formulae which are universally quantiﬁed equations of the form
∀y1...∀ym(s .= t),
where {y1, ..., ym} is a subset of FV (s) ∪FV (t), including the case
m = 0 which corresponds to the quantiﬁer-free equation s .= t. Hence,
we will prove two facts.
For every sequence T consisting of (partially) universally quantiﬁed
equations from E,
(1)
If
T →s .= t
is G1nnf
=
-provable then
s
∗
←→E t.
(2)
If
s
∗
←→E t
then
E′ →s .= t
is G1nnf
=
-provable,
where E′ is the universal closure of E.
This is done as follows:
(a) Prove that if s
∗
←→E t, then the sequent E′ →s .= t is G1nnf
=
-
provable, where E′ is the universal closure of E.

6.5 Craig’s Interpolation Theorem
287
Hint: Use induction on the structure of the term r in the deﬁnition
of
∗
←→E .
(b) Given a set E of equations and any equation v .= w, let {E, v .= w}
denote the union of E and {v .= w}.
Prove that
(i) If si
∗
←→E ti, for 1 ≤i ≤n, then
f(t1, ..., tn)
∗
←→E f(s1, ..., sn).
(ii) If t1
∗
←→E t2, then for every substitution σ with support FV (t1)∪
FV (t2),
σ(t1)
∗
←→E σ(t2).
(iii) If v
∗
←→E w and s
∗
←→{E,v .=w} t, then s
∗
←→E t.
Hint: Show that every rewrite step involving v .= w as an equation
can be simulated using the steps in v
∗
←→E w.
(c) Prove that, for every sequence T consisting of (partially) uni-
versally quantiﬁed equations from E, for any equation s .= t, if the
sequent T →s .= t is G1nnf
=
-provable then s
∗
←→E t.
Hint: Proceed by induction on G1nnf
=
proofs without essential cuts.
One of the cases is that of an inessential cut. If the bottom inference
is a cut, it must be of the form
S1
T1 →v .= w
S2
v .= w, T2 →s .= t
T →s .= t
where T1 and T2 are subsets of T.
By the induction hypothesis,
v
∗
←→E w and s
∗
←→{E,v .=w} t. Conclude using (b).
(d) Prove that the sequent E′ →s .= t is G1nnf
=
-provable iﬀs
∗
←→E t.
The above problem shows the completeness of the rewrite rule method
for (universal) equational logic. This is an important current area of
research.
For more on this approach, see the article by Huet and
Oppen, in Book, 1980, and Huet, 1980.
6.5 Craig’s Interpolation Theorem
First, we deﬁne the concept of an interpolant.

288
6/Gentzen’s Cut Elimination Theorem And Applications
6.5.1 Interpolants
If a formula of the form A ⊃B is valid, then it is not obvious that there is
a formula I, called an interpolant of A ⊃B, such that A ⊃I and I ⊃B
are valid, and every predicate, constant, and free variable occurring in I also
occurs both in A and B. As a matter of fact, the existence of an interpolant
depends on the language. If we don’t allow either the constant ⊥or equality
( .=), the formula (P ⊃P) ⊃(Q ⊃Q) is valid, and yet, no formula I as above
can be found. If we allow equality and make the exception that if .= occurs
in I, it does not necessarily occur in both A and B, then I = ∀x(x .= x) does
the job. Alternatively, ¬ ⊥(true!) does the job.
Craig’s interpolation theorem gives a full answer to the problem of the
existence of an interpolant. An interesting application of Craig’s interpolation
theorem can be found in Oppen and Nelson’s method for combining decision
procedures. For details, see Nelson and Oppen, 1979, and Oppen, 1980b.
6.5.2 Craig’s Interpolation Theorem Without Equality
First, we consider a ﬁrst-order language without equality. For the next lemma,
we assume that the constant ⊥(for false) is in the language, but that ≡is
not. Let LK⊥be the extension of LK obtained by allowing the sequent ⊥→
as an axiom. It is easily checked that the cut elimination theorem also holds
for LK⊥(see the problems).
During the proof of the key lemma, it will be necessary to replace in a
proof all occurrences of a free variable y by a new variable z not occurring
in the proof. This substitution process is deﬁned as in deﬁnition 6.4.5, but
for LK (and LKe) rather than G1nnf. Given a deduction tree T such that
y does not occur bound in T, T[z/y] is the result of replacing every sequent
Γ →∆in T by (Γ →∆)[z/y]. Similarly, T[z/c] is the result of substituting a
new variable z for all occurrences of a constant c in a proof T. The following
technical lemma will be needed.
Lemma 6.5.1
Let Γ →∆be a sequent and T an LK-proof for Γ →∆. As-
sume that y is a variable not occurring bound in the proof T. For any variable
z not occurring in T, the result T[z/y] of substituting z for all occurrences of
y in T is a proof tree. Similarly, if c is a constant occurring in the proof tree
T, T[z/c] is a proof tree. The lemma also holds for LKe-proofs.
Proof : Similar to that of lemma 6.4.3.
The following lemma is the key to a constructive proof of the interpola-
tion theorem.
Lemma 6.5.2
Let L be a ﬁrst-order language without equality, without ≡,
and with constant ⊥. Given any provable sequent Γ →∆, let (Γ1, Γ2) and
(∆1, ∆2) be pairs of disjoint subsequences of Γ and ∆respectively, such that

6.5 Craig’s Interpolation Theorem
289
the union of Γ1 and Γ2 is Γ, and the union of ∆1 and ∆2 is ∆. Let us call
< Γ1, ∆1, Γ2, ∆2 > a partition of Γ →∆. Then there is a formula C of LK⊥
called an interpolant of < Γ1, ∆1, Γ2, ∆2 > having the following properties:
(i) Γ1 →∆1, C and C, Γ2 →∆2 are LK⊥-provable.
(ii) All predicate symbols (except for ⊥), constant symbols, and variables
free in C occur both in Γ1 ∪∆1 and Γ2 ∪∆2.
Proof : We proceed by induction on cut-free proof trees. We treat some
typical cases, leaving the others as an exercise.
(1) Γ →∆is an axiom. Hence, it is of the form A →A. There are four
cases. For < Γ1, ∆1, Γ2, ∆2 >=< A, A, ∅, ∅>, C =⊥; for < Γ1, ∆1, Γ2, ∆2 >
=< ∅, ∅, A, A >, C = ¬ ⊥; for < Γ1, ∆1, Γ2, ∆2 >=< A, ∅, ∅, A >, C = A; For
< Γ1, ∆1, Γ2, ∆2 >=< ∅, A, A, ∅>, C = ¬A.
(2) The root inference is ∧: right:
Γ →∆, A
Γ →∆, B
Γ →∆, A ∧B
Assume the partition is
< Γ1, (∆1, A ∧B), Γ2, ∆2 >,
the case
< Γ1, ∆1, Γ2, (∆2, A ∧B) >
being similar.
We have induced partitions < Γ1, (∆1, A), Γ2, ∆2 > and <
Γ1, (∆1, B), Γ2, ∆2 >. By the induction hypothesis, there are formulae C1
and C2 satisfying the conditions of the lemma. Since Γ1 →∆1, A, C1 and
Γ1 →∆1, B, C2 are provable,
Γ1 →∆1, A, C1 ∨C2
and
Γ1 →∆1, B, C1 ∨C2
are provable (using ∨: right), and
Γ1 →∆1, A ∧B, C1 ∨C2
is provable, by ∧: right. Since C1, Γ2 →∆2 and C2, Γ2 →∆2 are provable,
C1 ∨C2, Γ2 →∆2
is provable by ∨: left. Then, we can take C1 ∨C2 as an interpolant.

290
6/Gentzen’s Cut Elimination Theorem And Applications
(3) The root inference is ∀: right:
Γ →∆, A[y/x]
Γ →∆, ∀xA
where y does not occur free in the conclusion. Assume the partition is
< Γ1, (∆1, ∀xA), Γ2, ∆2 >,
the case
< Γ1, ∆1, Γ2, (∆2, ∀xA) >
being similar. By the induction hypothesis, there is an interpolant C such
that Γ1 →∆1, A[y/x], C and C, Γ2 →∆2 are provable. By condition (ii)
of the lemma, C cannot contain y, since otherwise y would be in Γ1 ∪∆1,
contradicting the fact that y does not occur free in Γ →∆, ∀xA. Hence,
Γ1 →∆1, ∀xA, C
is provable by ∀: right. Since by the induction hypothesis
C, Γ2 →∆2
is provable, we can take C as an interpolant.
(4) The root inference is ∀: left:
A[t/x], Γ →∆
∀xA, Γ →∆
where t is free for x in A.
This case is more complicated if t is not a variable. Indeed, one has to
be careful to ensure condition (ii) of the lemma. Assume that the partition is
< (∀xA, Γ1), ∆1, Γ2, ∆2 >, the case < Γ1, ∆1, (∀xA, Γ2), ∆2 > being similar.
By the induction hypothesis, there is a formula C such that
A[t/x], Γ1 →∆1, C
and
C, Γ2 →∆2
are provable. First, note that ∀xA, Γ1 →∆1, C is provable by ∀: left. If t
is a variable z and z does not occur free in ∀xA, Γ →∆, then by ∀: right,
∀xA, Γ1 →∆1, ∀zC is provable, and so is ∀zC, Γ2 →∆2, by ∀: left. Since
C contains predicate symbols, constants and free variables both occurring in
(A[z/x], Γ1) ∪∆1 and Γ2 ∪∆2, and z does not occur free in ∀xA, Γ →∆,
the formula ∀zC also satisﬁes condition (ii) of the lemma. If z occurs free in
(∀xA, Γ1) ∪∆1, then C can serve as an interpolant, since if it contains z, z
is both in (∀xA, Γ1) ∪∆1 and also in Γ2 ∪∆2 by the induction hypothesis.

6.5 Craig’s Interpolation Theorem
291
If t is not a variable, we proceed as follows: Let x1, ...xk and c1, ..., cm be
the variables and the constants in t which occur in C but do not occur in
(∀xA, Γ1) ∪∆1. In the proof of A[t/x], Γ1 →∆1, C, replace all occurrences of
x1, ...xk and c1, ..., cm in t and C by new variables z1, ..., zk+m (not occurring
in the proof). Let t′ be the result of the substitution in the term t, and C′ the
result of the substitution in the formula C. By lemma 5.6.1, A[t′/x], Γ1 →
∆1, C′ is provable.
But now, z1, ..., zk+m do not occur free in ∀xA, Γ1 →
∆1, ∀z1...∀zk+mC′, and so
∀xA, Γ1 →∆1, ∀z1...∀zk+mC′
is provable (by applications of ∀: right and ∀: left). But
∀z1...∀zk+mC′, Γ2 →∆2
is also provable (by applications of ∀: left). Hence,
∀z1...∀zk+mC′
can be used as an interpolant, since it also satisﬁes condition (ii) of the lemma.
Note that the method used in lemma 6.5.2 does not guarantee that the
function symbols occurring in C also occur both in Γ1 ∪∆1 and Γ2 ∪∆2.
We are now ready to prove Craig’s interpolation theorem for LK.
Theorem 6.5.1
(Craig’s interpolation theorem, without equality) Let L be
a ﬁrst-order language without equality and without ≡. Let A and B be two
L-formulae such that A ⊃B is LK-provable.
(a) If A and B contain some common predicate symbol, then there exists
a formula C called an interpolant of A ⊃B, such that all predicate, constant
symbols, and free variables in C occur in both A and B, and both A ⊃C and
C ⊃B are LK-provable.
(b) If A and B do not have any predicate symbol in common, either
A →is LK-provable or →B is LK-provable.
Proof : Using the cut rule, it is obvious that →A ⊃B is LK-provable
iﬀA →B is. Consider the partition in which Γ1 = A, ∆1 = ∅, Γ2 = ∅and
∆2 = B. By lemma 6.5.2, there is a formula C of LK⊥such that A →C
and C →B are LK⊥-provable, and all predicate, constant symbols, and free
variables in C occur in A and B. If A and B have some predicate symbol P
of rank n in common, let P ′ be the sentence
∀y1...∀yn(P(y1, ..., yn) ∧¬P(y1, ...yn)),
where y1, ..., yn are new variables. Let C′ be obtained by replacing all occur-
rences of ⊥in C by P ′. Since P ′ is logically equivalent to ⊥, it is not diﬃcult

292
6/Gentzen’s Cut Elimination Theorem And Applications
to obtain LK-proofs of A →C′ and of C′ →B. But then →A ⊃C′ and
→C′ ⊃B are LK-provable, and the formula C′ is the desired interpolant.
If A and B have no predicate symbols in common, the formula C given
by lemma 6.5.2 only contains ⊥as an atom. It is easily shown by induction on
the structure of C that either →C is LK⊥-provable or C →is LK⊥-provable.
But then, using the cut rule, we can show that either A →is LK⊥-provable,
or →B is LK⊥-provable. However, since neither A nor B contains ⊥and the
cut elimination theorem holds for LK⊥, a cut-free proof in LK⊥for either
A →or →B is in fact an LK-proof.
EXAMPLE 6.5.1
Given the formula
(P(a) ∧∀xQ(x)) ⊃(∀yS(y) ∨Q(b)),
the formula ∀xQ(x) is an interpolant.
6.5.3 Craig’s Interpolation Theorem With Equality
We now consider Craig’s interpolation theorem for ﬁrst-order languages with
equality. Because cuts cannot be completely eliminated in LKe-proofs, the
technique used in lemma 6.5.2 cannot be easily extended to the system LKe.
However, there is a way around, which is to show that a sequent S = Γ →∆
is LKe-provable if and only if the sequent Se, Γ →∆is LK-provable, where
Se is a certain sequence of closed formulae called the closed equality axioms
for Γ →∆.
Deﬁnition 6.5.1
Given a sequent S = Γ →∆, the set of closed equality
axioms for S is the set Se of closed formulae given below, for all predicate
and function symbols occuring in S:
∀x(x .= x)
∀x1...∀xn∀y1...∀yn(x1 .= y1 ∧... ∧xn .= yn ⊃f(x1, ..., xn) .= f(y1, ..., yn))
∀x1...∀xn∀y1...∀yn(x1 .= y1 ∧... ∧xn .= yn ∧P(x1, ..., xn) ⊃P(y1, ..., yn))
Lemma 6.5.3
A sequent S = Γ →∆is LKe-provable iﬀSe, Γ →∆is
LK-provable.
Proof : We sketch the proof, leaving the details as an exercise. First,
we show that if S is LKe-provable then Se, Γ →∆is LK-provable. For this,
we prove that for every equality axiom Γ′ →∆′ of LKe used in the proof
of S, the sequent Se, Γ′ →∆′ is LK-provable. We conclude by induction on
LKe-proof trees.

6.5 Craig’s Interpolation Theorem
293
Next, assume that Se, Γ →∆is LK-provable. First, we show that every
formula in Se is LKe-provable. We conclude by constructing an LKe-proof of
Γ →∆using cuts on formulae in Se.
We can now generalize lemma 6.5.2 to LKe as follows. Let LKe,⊥be
the system obtained by allowing ⊥as a constant and the sequent ⊥→as an
axiom.
Lemma 6.5.4
Let L be a ﬁrst-order language with equality and with con-
stant ⊥, but without ≡.
Given any LKe-provable sequent Γ →∆, let
(Γ1, Γ2) and (∆1, ∆2) be pairs of disjoint subsequences of Γ and ∆respec-
tively, such that the union of Γ1 and Γ2 is Γ, and the union of ∆1 and ∆2
is ∆. < Γ1, ∆1, Γ2, ∆2 > is called a partition of Γ →∆. There is a formula
C of LKe,⊥called an interpolant of < Γ1, ∆1, Γ2, ∆2 > having the following
properties:
(i) Γ1 →∆1, C and C, Γ2 →∆2 are LKe-provable.
(ii) All predicate symbols (except for .=), constant symbols, and variables
free in C occur both in Γ1 ∪∆1 and Γ2 ∪∆2.
Proof : By lemma 6.5.3, S = Γ →∆is LKe-provable iﬀSe, Γ →∆is
LK-provable. Let S1
e be the closed equality axioms corresponding to function
and predicate symbols in Γ1 ∪∆1, and S2
e the closed equality axioms corre-
sponding to function and predicate symbols in Γ2 ∪∆2. Clearly, Se = S1
e ∪S2
e.
Consider the partition < (S1
e, Γ1), ∆1, (S2
e, Γ2), ∆2 >. By lemma 6.5.2, there
is a formula C such that
S1
e, Γ1 →∆1, C
and
C, S2
e, Γ2 →∆2
are LK⊥-provable, and the predicate symbols, constant symbols and free vari-
ables in C occur both in S1
e ∪Γ1 ∪∆1 and S2
e ∪Γ2 ∪∆2. By deﬁnition of S1
e
and S2
e, the predicate symbols (other than .=) in S1
e are those in Γ1 ∪∆1, and
similarly for S2
e and Γ2 ∪∆2. Furthermore, all formulae in S1
e and S2
e being
closed, there are no free variables occurring in them. Hence, the predicate
symbols, constant and free variables in C occur both in Γ1 ∪∆1 and Γ2 ∪∆2.
Using lemma 6.5.3 again, Γ1 →∆1, C and C, Γ2 →∆2 are LKe,⊥-provable.
Now, we can replace all occurrence of ⊥in C by ¬∀z(z .= z), where z is a
new variable, obtaining a formula C′, and since ⊥is logically equivalent to
¬∀z(z .= z), it is easy to obtain LKe-proofs for Γ1 →∆1, C′ and C′, Γ2 →∆2.
Taking C′ as the interpolant this concludes the proof of the lemma.
Using lemma 6.5.4, Craig’s interpolation theorem is generalized to lan-
guages with equality.
Theorem 6.5.2
(Craig’s interpolation theorem, with equality) Let L be a
ﬁrst-order language with equality and without ≡. Let A and B be two L-
formulae such that A ⊃B is LKe-provable. Then there exists a formula C
called an interpolant of A ⊃B, such that all predicate symbols except .=,

294
6/Gentzen’s Cut Elimination Theorem And Applications
constant symbols, and free variables in C occur in both A and B, and both
A ⊃C and C ⊃B are LKe-provable.
Proof : As in theorem 6.5.1, A ⊃B is LKe-provable iﬀA →B is LKe-
provable. We apply lemma 6.5.4 to the partition in which Γ1 = A, ∆1 = ∅,
Γ2 = ∅and ∆2 = B. This time, because .= is available, the result is obtained
immediately from lemma 6.5.4.
Remark: As in theorem 6.5.2, our proof does not guarantee that the
function symbols occurring in the interpolant also occur in both A and B.
However, this can be achieved using a diﬀerent proof technique presented
in problem 5.6.7, which consists in replacing function symbols by predicate
symbols. Hence, in case of a ﬁrst-order language with equality, we obtain a
stronger version of Craig’s interpolation theorem, in which all predicate sym-
bols (diﬀerent from .=), function symbols, constant symbols, and free variables
occurring in the interpolant C of A ⊃B occur in both A and B.
In the next section, we give two applications of Craig’s interpolation
theorem.
PROBLEMS
6.5.1.
Finish the proof of lemma 6.5.1.
6.5.2.
Finish the proof of lemma 6.5.2.
6.5.3.
Give a proof for lemma 6.5.3.
6.5.4.
Verify that the cut elimination theorem holds for LK⊥, and for LKe,⊥
(without essential cuts).
6.5.5.
Provide the details in the proof of theorem 6.5.1 regarding the re-
placement of ⊥by ∀y1...∀yn(P(y1, ..., yn) ∧¬P(y1, ...yn)).
6.5.6.
Show that ∀x¬P(x) ∨Q(b) is an interpolant for
[(R ⊃∃xP(x)) ⊃Q(b)] ⊃[∀x((S ∧P(x)) ⊃(S ∧Q(b)))].
6.5.7.
Using the proof technique presented in problem 5.6.7, which con-
sists in replacing function symbols by predicate symbols, prove the
stronger version of Craig’s interpolation theorem, in which all predi-
cate symbols (diﬀerent from .=), function symbols, constant symbols,
and free variables occurring in the interpolant C of A ⊃B occur in
both A and B.

6.6 Beth’s Deﬁnability Theorem
295
6.6 Beth’s Deﬁnability Theorem
First, we consider what deﬁnability means.
6.6.1 Implicit and Explicit Deﬁnability
Let L be a ﬁrst-order language with or without equality. Let A1, ..., Am be
closed formulae containing exactly the distinct predicate symbols P1, ..., Pk, Q,
where Q is not .=, but some of the Pi can be .=. Assume that Q has rank
n > 0. We can view A1, ..., Am as the axioms of a theory. The question of
interest is whether Q is deﬁnable in terms of P1, ..., Pk. First, we need to
make precise what deﬁnable means. Let A(P1, ..., Pk, Q) be the conjunction
A1 ∧... ∧Am.
A ﬁrst plausible criterion for deﬁnability is that, for any two L-structures
A and B with the same domain M, and which assign the same interpretation
to the predicate symbols P1, ..., Pk,
if
A |= A(P1, ..., Pk, Q)
and
B |= A(P1, ..., Pk, Q),
then for every (a1, ..., an) ∈M n,
A |= Q(a1, ..., an)
iﬀ
B |= Q(a1, ..., an).
The idea is that given any two interpretations of the predicate symbols
Q, if these two interpretations make A(P1, ..., Pk, Q) true then they must agree
on Q. This is what is called implicit deﬁnability.
A seemingly stronger criterion for deﬁnability is the following:
Deﬁnition 6.6.1
Assume that there is an L-formula D(P1, ..., Pk) whose
set of free variables is a subset of {x1, ..., xn} and whose predicate symbols
are among P1, ..., Pk, and which contains at least one of the Pi. We say that
Q is deﬁned explicitly from P1, ..., Pk in the theory based on A1, ..., Am iﬀthe
following is provable (or equivalently valid, by the completeness theorem):
→A(P1, ..., Pk, Q) ⊃∀x1...∀xn(Q(x1, ..., xn) ≡D(P1, ..., Pk)),
where A(P1, ..., Pk, Q) is the conjunction A1 ∧... ∧Am.
We can modify the deﬁnition of implicit deﬁnability so that it refers to
a single structure as opposed to a pair of structures, by using a new copy Q′
of the predicate symbol Q, explained as follows.
Deﬁnition 6.6.2
Let P1, ..., Pk, Q, Q′ be distinct predicate symbols, and
let A(P1, ..., Pk, Q′) be the result of substituting Q′ for Q in A(P1, ..., Pk, Q).
We say that Q is deﬁned implicitly from P1, ..., Pk in the theory based on
A1, ..., Am iﬀthe following formula is valid (or equivalently provable):
A(P1, ..., Pk, Q) ∧A(P1, ..., Pk, Q′) ⊃
∀x1...∀xn(Q(x1, ..., xn) ≡Q′(x1, ..., xn)),

296
6/Gentzen’s Cut Elimination Theorem And Applications
where A(P1, ..., Pk, Q) is the conjunction A1 ∧... ∧Am.
EXAMPLE 6.6.1
Let L be the language with equality having 0 as a constant, S as a
unary function symbol, + as a binary function symbol, and < as a binary
predicate symbol. Let A( .=, <) be the conjunction of the following closed
formulae:
∀x¬(S(x) .= 0)
∀x∀y(S(x) .= S(y) ⊃x .= y)
∀x(x + 0 .= x)
∀x∀y(x + S(y) .= S(x + y))
∀x(0 < Sx)
The predicate symbol < is not deﬁnable implicitly from .= and A( .=, <).
Indeed, we can deﬁne two structures A and B with domain N, in which
0, S and + receive the same natural interpretation, but in the ﬁrst
structure, < is interpreted as the strict order on N, whereas in the
second, we interpret < as the predicate such that for all x, y ∈N, x < y
iﬀx = 0. Both A and B are models of A( .=, <), but < is interpreted
diﬀerently.
On the other hand, if we add to A( .=, <) the sentence
∀x∀y(x < y ≡∃z(y .= x + S(z))),
then D = ∃z(y .= x + S(z)) deﬁnes < explicitly.
6.6.2 Explicit Deﬁnability Implies Implicit Deﬁnability
It is natural to ask whether the concepts of implicit and explicit deﬁnability
are related. First, it is not diﬃcult to see that explicit deﬁnability implies
implicit deﬁnability. Indeed, we show that if
A(P1, ..., Pk, Q) ⊃∀x1...∀xn(Q(x1, ..., xn) ≡D(P1, ..., Pk))
is valid, then A(P1, ..., Pk, Q) deﬁnes Q implicitly. For if A(P1, ..., Pk, Q) ∧
A(P1, ..., Pk, Q′) is valid in any model A, then
∀x1...∀xn(Q(x1, ..., xn) ≡D(P1, ..., Pk))
and
∀x1...∀xn(Q′(x1, ..., xn) ≡D(P1, ..., Pk))
are valid in A, and this implies that
∀x1...∀xn(Q(x1, ..., xn) ≡Q′(x1, ..., xn))

6.6 Beth’s Deﬁnability Theorem
297
is valid in A.
The fact that explicit deﬁnability implies implicit deﬁnability yields a
method known as Padoa’s method, to show that a predicate Q is not deﬁnable
explicitly from P1, ..., Pk in the theory based on A1, ..., Am:
Find two structures with the same domain and assigning the same in-
terpretation to the predicate symbols P1, ..., Pk, but assigning diﬀerent inter-
pretations to Q.
However, it is not as obvious that implicit deﬁnability implies explicit
deﬁnability. But this is the case, as shown by Beth’s theorem. The proof
given below that uses Craig’s interpolation theorem even gives the deﬁning
formula D(P1, ..., Pk) constructively.
6.6.3 Beth’s Deﬁnability Theorem, Without Equality
First, we consider the case of a ﬁrst-order language without equality.
Theorem 6.6.1
(Beth’s deﬁnability theorem, without equality) Let L be a
ﬁrst-order language without equality. Let A(P1, ..., Pk, Q) be a closed formula
containing predicate symbols among the distinct predicate symbols P1, ..., Pk,
Q, where Q has rank n > 0. Assume that Q is deﬁned implicitly from P1, ..., Pk
by the sentence A(P1, ..., Pk, Q).
(a) If one of the predicate symbols Pi actually occurs in A(P1, ..., Pk, Q),
then there is a formula D(P1, ..., Pk) deﬁning Q explicitly from P1, ..., Pk.
(b) If none of the predicate symbols P1, ..., Pk occur in A(P1, ..., Pk),
then either
|= A(P1, ..., Pk, Q) ⊃∀x1...∀xnQ(x1, ..., xn)
or
|= A(P1, ..., Pk, Q) ⊃∀x1...∀xn¬Q(x1, ..., xn).
Proof : Assume that
A(P1, ..., Pk, Q) ∧A(P1, ..., Pk, Q′) ⊃
∀x1...∀xn(Q(x1, ..., xn) ≡Q′(x1, ..., xn))
is valid. Since A(P1, ..., Pk, Q) and A(P1, ..., Pk, Q′) are closed,
A(P1, ..., Pk, Q) ∧A(P1, ..., Pk, Q′) ⊃(Q(x1, ..., xn) ≡Q′(x1, ..., xn))
is also valid. This formula is of the form (A ∧B) ⊃(C ≡D). It is an easy
exercise to show that if (A ∧B) ⊃(C ≡D) is valid, then (A ∧C) ⊃(B ⊃D)
is also valid. Hence,
(1)
(A(P1, ..., Pk, Q) ∧Q(x1, ..., xn)) ⊃(A(P1, ..., Pk, Q′) ⊃Q′(x1, ..., xn))

298
6/Gentzen’s Cut Elimination Theorem And Applications
is valid.
(a) If some of the predicate symbols Pi occurs in A(P1, ..., Pk, Q), by
Craig’s theorem (theorem 6.5.1) applied to (1), there is a formula C containing
only predicate symbols, constant symbols, and free variables occurring in both
A(P1, ..., Pk, Q)∧Q(x1, ..., xn) and A(P1, ..., Pk, Q′) ⊃Q′(x1, ..., xn), and such
that the following are valid:
(2)
(A(P1, ..., Pk, Q) ∧Q(x1, ..., xn)) ⊃C,
and
(3)
C ⊃(A(P1, ..., Pk, Q′) ⊃Q′(x1, ..., xn)).
Since A(P1, ..., Pk, Q)∧Q(x1, ..., xn) does not contain Q′ and A(P1, ..., Pk, Q′)
⊃Q′(x1, ..., xn) does not contain Q, the formula C only contains predicate
symbols among P1, ..., Pk and free variables among x1, ..., xn.
By substituting Q for Q′ in (3), we also have the valid formula
(4)
C ⊃(A(P1, ..., Pk, Q) ⊃Q(x1, ..., xn)),
which implies the valid formula
(5)
A(P1, ..., Pk, Q) ⊃(C ⊃Q(x1, ..., xn)).
But (2) implies the validity of
(6)
A(P1, ..., Pk, Q) ⊃(Q(x1, ..., xn) ⊃C).
The validity of (5) and (6) implies that
A(P1, ..., Pk, Q) ⊃(C ≡Q(x1, ..., xn))
is valid, which in turns implies that C deﬁnes Q explicitly from P1, ..., Pk.
(b) If none of P1, ..., Pk occurs in A(P1, ..., Pk, Q), then by Craig’s theo-
rem (theorem 6.5.1), part (ii), either
(7)
¬(A(P1, ..., Pk, Q) ∧Q(x1, ..., xn))
is valid, or
(8)
A(P1, ..., Pk, Q′) ⊃Q′(x1, ..., xn)
is valid.
Using propositional logic, either
A(P1, ..., Pk, Q) ⊃¬Q(x1, ..., xn)

PROBLEMS
299
is valid, or
A(P1, ..., Pk, Q) ⊃Q(x1, ..., xn)
is valid, which implies part (b) of the theorem.
6.6.4 Beth’s Deﬁnability Theorem, With Equality
We now consider Beth’s deﬁnability theorem for a ﬁrst-order language with
equality. This time, we can deﬁne either a predicate symbol, or a function
symbol, or a constant.
Theorem 6.6.2
(Beth’s deﬁnability theorem, with equality) Let L be a
ﬁrst-order language with equality.
(a) Let A(P1, ..., Pk, Q) be a closed formula possibly containing equal-
ity and containing predicate symbols among the distinct predicate symbols
P1, ..., Pk, Q (diﬀerent from .=), where Q has rank n > 0. Assume that Q is
deﬁned implicitly from P1, ..., Pk by the sentence A(P1, ..., Pk, Q). Then there
is a formula D(P1, ..., Pk) deﬁning Q explicitly from P1, ..., Pk.
(b) Let A(P1, ..., Pk, f) be a closed formula possibly containing equal-
ity, and containing predicate symbols among the distinct predicate symbols
P1, ..., Pk (diﬀerent from .=), and containing the function or constant sym-
bol f of rank n ≥0. Assume that f is deﬁned implicitly from P1, ..., Pk by
the sentence A(P1, ..., Pk, f), which means that the following formula is valid,
where f ′ is a new copy of f:
A(P1, ..., Pk, f) ∧A(P1, ..., Pk, f ′) ⊃∀x1...∀xn(f(x1, ..., xn) .= f ′(x1, ..., xn)).
Then there is a formula D(P1, ..., Pk) whose set of free variables is among
x1, ..., xn and not containing f (or f ′) deﬁning f explicitly, in the sense that
the following formula is valid:
A(P1, ..., Pk, f) ⊃∀x1...∀xn∀y((f(x1, ..., xn) .= y) ≡D(P1, ..., Pk)).
Proof : The proof of (a) is similar to that of theorem 6.6.1(a), but using
theorem 6.5.2, which yields D(P1, ..., Pk) in all cases.
To prove (b), observe that f(x1, ..., xn) .= f ′(x1, ..., xn) is equivalent to
∀y((f(x1, ..., xn) .= y) ≡(f ′(x1, ..., xn) .= y)).
We conclude by applying the reasoning used in part (a) with f(x1, ..., xn) .= y
instead of Q(x1, ..., xn).
The last application of Craig’s interpolation theorem presented in the
next section is Robinson’s joint consistency theorem.

300
6/Gentzen’s Cut Elimination Theorem And Applications
PROBLEMS
6.6.1.
Show that in deﬁnition 6.6.2, the deﬁnability condition can be relaxed
to
A(P1, ..., Pk, Q) ∧A(P1, ..., Pk, Q′) ⊃
∀x1...∀xn(Q(x1, ..., xn) ⊃Q′(x1, ..., xn)).
6.6.2.
Give the details of the proof that explicit deﬁnability implies implicit
deﬁnability.
6.7 Robinson’s Joint Consistency Theorem
Let L be a ﬁrst-order language with or without equality, and let L1 and L2
be two expansions of L such that L = L1 ∩L2.
Also, let S1 be a set of
L1-sentences, S2 a set of L2-sentences, and let S = S1 ∩S2. If S1 and S2 are
both consistent, their union S1 ∪S2 is not necessarily consistent. Indeed, if
S is incomplete, that is, there is some L-sentence C such that neither S →C
nor S →¬C is provable, S1 could contain C and S2 could contain ¬C, and
S1 ∪S2 would be inconsistent. A concrete illustration of this phenomenon
can be given using G¨odel’s incompleteness theorem for Peano’s arithmetic,
which states that there is a sentence C of the language of arithmetic such
that neither AP →C nor AP →¬C is provable, where AP consists of the
axioms of Peano’s arithmetic (see example 5.6.3). (For a treatment of G¨odel’s
incompleteness theorems, see Enderton, 1972; Kleene, 1952; Shoenﬁeld, 1967;
or Monk, 1976.) Since Peano’s arithmetic is incomplete, then {AP , C} and
{AP , ¬C} are both consistent, but their union is inconsistent.
A. Robinson’s theorem shows that inconsistency does not arise if S is
complete. Actually, one has to be a little careful about the presence in the
language of function symbols or of the equality predicate .=.
Theorem 6.7.1
(Robinson’s joint consistency theorem) Let L be a ﬁrst-
order language either without function symbols and without equality, or with
equality (and possibly function symbols). Let L1 and L2 be two expansions
of L such that L = L1 ∩L2. Let S1 be a set of L1-sentences, S2 a set of L2-
sentences, and let S = S1 ∩S2. If S1 and S2 are consistent and S is complete,
that is, for every closed L-formula C, either ⊢S →C, or ⊢S →¬C, then
the union S1 ∪S2 of S1 and S2 is consistent.
Proof : Assume that S1 ∪S2 is inconsistent. Then |= S1 ∪S2 →. By the
completeness theorem (theorem 5.6.1), there is a ﬁnite subsequent of S1∪S2 →
that is provable.
Let A1, ..., Am, B1, ..., Bn →
be such a sequent, where
A1, ..., Am ∈S1, and B1, ..., Bn ∈S2. It is immediate that
⊢(A1 ∧... ∧Am) ⊃¬(B1 ∧... ∧Bn).

PROBLEMS
301
We apply Craig’s interpolation theorem (theorem 6.5.1) to this formula.
First, we consider the case where L does not contain function symbols
and does not contain equality. Then, if A1 ∧... ∧Am and ¬(B1 ∧... ∧Bn) do
not have any predicate symbol in common, either
⊢¬(A1 ∧... ∧Am), or
⊢¬(B1 ∧... ∧Bn).
In the ﬁrst case, the consistency of S1 is contradicted, in the second case, the
consistency if S2 is contradicted.
If (A1 ∧... ∧Am) and ¬(B1 ∧... ∧Bn) have some predicate in common,
then there is a formula C such that
(1)
⊢(A1 ∧... ∧Am) ⊃C,
and
(2)
⊢C ⊃¬(B1 ∧... ∧Bn),
and the predicate symbols, constant symbols and variables free in C are both
in A1 ∧... ∧Am and ¬(B1 ∧... ∧Bn). Since these formulae are closed, C is
also a closed formula, and since L = L1 ∩L2, C is a closed L-formula. Since
S is complete, either ⊢S →C or ⊢S →¬C. In the ﬁrst case, by (2)
⊢S →¬(B1 ∧... ∧Bn),
contradicting the consistency of S2. In the second case, by (1)
⊢S →¬(A1 ∧... ∧Am),
contradicting the consistency of S1.
If L is a language with equality, we need the strong form of Craig’s in-
terpolation theorem mentioned as a remark after theorem 6.5.2, which states
that the all predicate, function and constant symbols occurring in an inter-
polant C of A ⊃B occur in both A and B. The rest of the proof is as above.
Another slightly more general version of Robinson’s joint consistency
theorem is given in problem 6.7.1.
PROBLEMS
6.7.1.
Prove the following version of Robinson’s joint consistency theorem:

302
6/Gentzen’s Cut Elimination Theorem And Applications
Let L be a ﬁrst-order language either without function symbols and
without equality, or with equality (and possibly function symbols).
Let L1 and L2 be two expansions of L such that L = L1 ∩L2. Let
S1 be a set of L1-sentences, and let S2 a set of L2-sentences. Assume
that S1 and S2 are consistent. Then the union S1 ∪S2 of S1 and S2
is consistent iﬀfor every closed L-formula C, either S1 →C is not
provable, or S2 →¬C is not provable.
6.7.2.
Prove that the version of Robinson’s joint consistency theorem given
in problem 6.7.1 implies Craig’s interpolation theorem.
Hint: Let A ⊃B be a provable formula. Let S1 = {C | ⊢A →C},
and S2 = {C | ⊢¬B →C}. Then S1 ∪S2 is inconsistent.
Notes and Suggestions for Further Reading
Gentzen’s cut elimination theorem is one of the jewels of proof theory. Orig-
inally, Gentzen’s motivation was to provide constructive consistency proofs,
and the cut elimination theorem is one of the main tools.
Gentzen’s original proof can be found in Szabo, 1969, and other proofs
are in Kleene, 1952, and Takeuti, 1975. A very elegant proof can also be found
in Smullyan, 1968. The proof given in Section 6.4 is inspired by Schwichten-
berg and Tait (Barwise, 1977, Tait, 1968).
Craig himself used Gentzen systems for proving his interpolation theo-
rem. We have followed a method due to Maehara sketched in Takeuti, 1975,
similar to the method used in Kleene, 1967. There are model-theoretic proofs
of Craig’s theorem, Beth’s deﬁnability theorem, and Robinson’s joint consis-
tency theorem. The reader is referred to Chang and Keisler, 1973, or Shoen-
ﬁeld, 1967.
The reader interested in proof theory should also read the article by
Schwichtenberg in Barwise, 1977. For an interesting analysis of analytic versus
nonanalytic proofs, the reader is referred to the article by Frank Pfenning, in
Shostak, 1984a.

Chapter 7
Gentzen’s Sharpened
Hauptsatz; Herbrand’s
Theorem
7.1 Introduction
We have mentioned in Chapter 6 that the cut elimination theorem shows the
existence of normal forms for proofs, namely the fact that every LK-proof can
be transformed to a cut-free proof (or a proof without essential cuts in LKe).
In this chapter we shall use Gentzen’s cut elimination theorem (also
called Gentzen’s Hauptsatz) to prove a version of Herbrand’s theorem for
LK and LKe. A derivation of Herbrand’s theorem from the cut elimination
theorem has the advantage that it yields a constructive version of the result,
in the spirit of Herbrand’s original version (Herbrand, 1971). The proof given
in this chapter using Gentzen’s Hauptsatz is inspired from a method sketched
in Kleene, 1967.
Herbrand’s theorem is perhaps the most fundamental result of ﬁrst-
order logic because it shows how the provability of a formula of ﬁrst-order
logic reduces to the provability of a quantiﬁer-free formula (obtained from the
original formula by substitutions).
Before proceeding any further, we wish to emphasize that Herbrand’s
original theorem is concerned with provability, a proof-theoretic concept, and
not validity, a semantic concept.
This is an important point because another theorem known as Skolem-
Herbrand-G¨odel theorem is often improperly referred to as Herbrand’s theo-
303

304
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
rem in the literature, thus causing a confusion. The Skolem-Herbrand-G¨odel
theorem is similar in form to Herbrand’s theorem but deals with unsatisﬁa-
bility (or validity), a semantic concept.
The reason for the confusion is probably that, by G¨odel’s completeness
theorem, validity can be equated to provability. Hence, Herbrand’s original
theorem can also be stated for unsatisﬁability (or validity).
However, Herbrand’s original theorem is a deeper result, whose proof
is signiﬁcantly harder than the Skolem-Herbrand-G¨odel theorem, and Her-
brand’s theorem also yields more information than the latter. More on this
subject will be said at the end of Sections 7.5 and 7.6. For an illuminating
discussion, the reader should consult Goldfarb’s introduction to Herbrand,
1971.
In this chapter, we shall give in Section 7.5 a version of Herbrand’s
original theorem for prenex formulae, and in Section 7.6 a version of the
Skolem-Herbrand-G¨odel theorem for formulae in NNF (actually, half of this
theorem is more like Herbrand’s original theorem).
The Skolem-Herbrand-G¨odel theorem can be viewed as the theoretical
basis of most theorem proving methods, in particular the resolution method,
one of the best known techniques in automatic theorem proving.
In fact,
the completeness of the resolution method will be shown in Chapter 8 by
combining the Skolem-Herbrand-G¨odel theorem and theorem 4.3.2.
A constructive version of Herbrand’s theorem can be obtained relatively
easily from Gentzen’s sharpened Hauptsatz, which is obtained by analyzing
carefully cut-free proofs of formulae of a special type.
Gentzen’s sharpened Hauptsatz shows that provided the formulae in
the bottom sequent have a certain form, a proof can be reorganized to yield
another proof in normal form such that all the quantiﬁer inferences appear
below all the propositional inferences. The main obstacle to the permutation
of inferences is the possibility of having a quantiﬁer rule applied to the side
formula arising from a propositional rule as illustrated below:
EXAMPLE 7.1.1
P(a) →Q(f(g(a)))
Q(f(g(a))) →Q(f(g(a)))
∀xQ(f(x)) →Q(f(g(a)))
P(a) ∨∀xQ(f(x)) →Q(f(g(a)))
The obvious solution that consists in permuting the ∨: left rule and
the ∀: left rule does not work since a quantiﬁer rule is not allowed to
apply to a subformula like ∀xQ(f(x)) in P(a) ∨∀xQ(f(x)).
There are at least two ways of resolving this diﬃculty:

7.2 Prenex Normal Form
305
(1) Impose restrictions on formulae so that a quantiﬁer inference cannot
be applied to the side formula of a propositional inference.
(2) Allow more general quantiﬁer rules applying to subformulae.
The standard approach has been to enforce (1) by requiring the formulae
to be prenex formulae. A Prenex formula is a formula consisting of a (possibly
empty) string of quantiﬁed variables followed by a quantiﬁer-free formula, and
it is easily seen that (1) holds.
The second approach is perhaps not as well known, but is possible.
Smullyan has given quantiﬁer rules (page 122 in Chapter 14 of Smullyan, 1968)
that allow the permutation process to be performed for unrestricted formulae,
and a general version of the extended Hauptsatz is also given (theorem 2, page
123). These rules are binary branching and do not seem very convenient in
practice. We will not pursue this method here and refer the interested reader
to Smullyan, 1968.
We have also discovered that Andrews’s version of the Skolem-Herbrand-
G¨odel theorem (Andrews, 1981) suggests quantiﬁer rules such that (2) holds
for formulae in negation normal form. Such rules are quite simple, and since
there is a refutation method based on Andrews’s version of Skolem-Herbrand-
G¨odel’s theorem (the method of matings), we shall give a proof of the extended
Hauptsatz for such a system. Since every formula is equivalent to a prenex
formula and to a formula in negation normal form, there is no loss of gen-
erality in restricting our attention to such normal forms. In this Chapter, it
is assumed that no variable occurs both free and bound in any sequent (or
formula).
7.2 Prenex Normal Form
First, we deﬁne the concept of a prenex formula.
Deﬁnition 7.2.1 (Prenex form) A formula is a prenex formula (or in prenex
form) iﬀeither it contains no quantiﬁers, or it is of the form Q1x1...QnxnB,
where B is a formula with no quantiﬁers (quantiﬁer-free), x1, ..., xn are (not
necessarily distinct) variables, and Qi ∈{∀, ∃}, for i = 1, ..., n.
In order to show that every formula is equivalent to a prenex formula,
the following lemma will be used.
Lemma 7.2.1
The following formulae are valid:
∀x(A ⊃B) ≡(∃xA ⊃B)
(a)
∃x(A ⊃B) ≡(∀xA ⊃B)
where x does not occur free in B.
∀x(A ⊃B) ≡(A ⊃∀xB)
∃x(A ⊃B) ≡(A ⊃∃xB)

306
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
where x does not occur free in A.
¬∀xA ≡∃x¬A
(b)
¬∃xA ≡∀x¬A
(∀xA ∨B) ≡∀x(A ∨B)
(c)
(∀xA ∧B) ≡∀x(A ∧B)
(∃xA ∨B) ≡∃x(A ∨B)
(∃xA ∧B) ≡∃x(A ∧B)
where x does not occur free in B.
∀x(A ∧B) ≡∀xA ∧∀xB
(d)
∃x(A ∨B) ≡∃xA ∨∃xB
∀xA ≡∀yA[y/x]
(e)
∃xA ≡∃yA[y/x]
where y is free for x in A, and y does not occur free in A unless y = x
(y /∈FV (A) −{x}).
Proof : Some of these equivalences have already been shown. We prove
two new cases, leaving the others as an exercise. A convenient method is to
construct a proof tree (in G).
We give proofs for (∀xA ∧B) ≡∀x(A ∧B) and (∀xA ∨B) ≡∀x(A ∨B),
where x is not free in B. As usual, to prove X ≡Y , we prove X ⊃Y and
Y ⊃X.
(i) Proof of (∀xA ∧B) ⊃∀x(A ∧B):
A[y/x], B →A[y/x]
A[y/x], B →B
A[y/x], B →(A[y/x] ∧B)
∀xA, B →(A ∧B)[y/x]
for a new variable y
∀xA, B →∀x(A ∧B)
(∀xA ∧B) →∀x(A ∧B)
→(∀xA ∧B) ⊃∀x(A ∧B)
We have used the fact that since x is not free in B, B[y/x] = B.
(ii) Proof of ∀x(A ∧B) ⊃(∀xA ∧B)

7.2 Prenex Normal Form
307
A[y/x], B →A[y/x]
A[y/x] ∧B →A[y/x]
∀x(A ∧B) →A[y/x]
∀x(A ∧B) →∀xA
A[y/x], B →B
A[y/x] ∧B →B
∀x(A ∧B) →B
→∀x(A ∧B) ⊃(∀xA ∧B)
Again, y is a new variable and we used the fact that B[y/x] = B.
(iii) Proof of (∀xA ∨B) ⊃∀x(A ∨B):
A[y/x] →A[y/x], B
∀xA →A[y/x], B
B →A[y/x], B
(∀xA ∨B) →A[y/x], B
(∀xA ∨B) →A[y/x] ∨B
(∀xA ∨B) →∀x(A ∨B)
→(∀xA ∨B) ⊃∀x(A ∨B)
As in (ii) y is a new variable and we used the fact that B[y/x] = B.
(iv) Proof of ∀x(A ∨B) ⊃(∀xA ∨B):
A[y/x] →A[y/x], B
B →A[y/x], B
A[y/x] ∨B →A[y/x], B
∀x(A ∨B) →A[y/x], B
∀x(A ∨B) →∀xA, B
∀x(A ∨B) →(∀xA ∨B)
→∀x(A ∨B) ⊃(∀xA ∨B)
As in (iii) y is a new variable and we used the fact that B[y/x] = B.
Theorem 7.2.1
For every formula A, a prenex formula B can be con-
structed such that A ≡B is valid (A and B are equivalent).
Proof : To simplify the proof, we will assume that Q1x1...QnxnB denotes
the quantiﬁer-free formula B when n = 0.
The proof proceeds using the
induction principle applied to A.
The base case is trivial since an atomic
formula is quantiﬁer free. The induction step requires six cases. We cover
three of these cases, leaving the others as exercises.

308
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
(i) A = (B ∨C).
By the induction hypothesis, B is equivalent to
a prenex formula Q1x1...QmxmB′ and C is equivalent to a prenex formula
R1y1...RnynC′, where B′ and C′ are quantiﬁer free and Qi, Rj ∈{∀, ∃}. By
lemma 7.2.1(e), it can be assumed that the sets of variables {x1, ..., xm} and
{y1, ..., yn} are disjoint and that both sets are also disjoint from the union
of the sets FV (B′) and FV (C′) of free variables in B′ and C′ (by renaming
with new variables). Using lemma 7.2.1(c) (several times), we get the prenex
formula
Q1x1...QmxmR1y1...Rnyn(B′ ∨C′),
which, by lemma 5.3.7 is equivalent to A.
(ii) A = ¬B. By the induction hypothesis, B is equivalent to a prenex
formula Q1x1...QmxmB′ where B′ is quantiﬁer free. For each Qi, let
Ri =

∀
if Qi = ∃;
∃
if Qi = ∀.
Using lemma 7.2.1(b) (several times), by lemma 5.3.7 the prenex formula
R1x1...Rmxm¬B′
is equivalent to A.
(iii) A = ∀xB. By the induction hypothesis, B is equivalent to a prenex
formula Q1x1...QmxmB′. Hence, by lemma 5.3.7 A is equivalent to the prenex
formula ∀xQ1x1...QmxmB′.
EXAMPLE 7.2.1
Let
A = ∀x(P(x) ∨¬∃y(Q(y) ∧R(x, y))) ∨¬(P(y) ∧¬∀xP(x)).
The prenex form of
¬∃y(Q(y) ∧R(x, y))
is
∀y¬(Q(y) ∧R(x, y)).
The prenex form of
∀x(P(x) ∨¬∃y(Q(y) ∧R(x, y)))
is
∀x∀y(P(x) ∨¬(Q(y) ∧R(x, y))).
The prenex form of
¬∀xP(x)

7.2 Prenex Normal Form
309
is
∃x¬P(x).
The prenex form of
(P(y) ∧¬∀xP(x))
is
∃x(P(y) ∧¬P(x)).
The prenex form of
¬(P(y) ∧¬∀xP(x))
is
∀x¬(P(y) ∧¬P(x)).
The prenex form of A is
∀x∀v∀z((P(x) ∨¬(Q(v) ∧R(x, v))) ∨¬(P(y) ∧¬P(z))).
In a prenex formula Q1x1...QnxnB, B is sometimes called the matrix.
Since B is quantiﬁer free, it is equivalent to a formula in either conjunctive
or disjunctive normal form. The disjunctive normal form of a formula can be
obtained by using the search procedure as explained in theorem 3.4.2. The
following lemma, which is the dual of lemma 3.4.5, provides another method
for transforming a formula into disjunctive normal form.
Lemma 7.2.2
Every quantiﬁer-free formula A (containing the connectives
∨, ∧, ¬, ⊃) can be transformed into an equivalent formula in disjunctive
normal form, by application of the following identities:
(A ⊃B) ≡(¬A ∨B)
¬¬A ≡A
¬(A ∧B) ≡(¬A ∨¬B)
¬(A ∨B) ≡(¬A ∧¬B)
A ∧(B ∨C) ≡(A ∧B) ∨(A ∧C)
(B ∨C) ∧A ≡(B ∧A) ∨(C ∧A)
(A ∧B) ∧C ≡A ∧(B ∧C)
(A ∨B) ∨C ≡A ∨(B ∨C)
Proof : The proof is dual to that of lemma 3.4.5 and is left as an exercise.
In the next sections, we consider versions of the sharpened Hauptsatz.

310
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
PROBLEMS
7.2.1.
Convert the following formulae to prenex form:
(¬∀xP(x, y) ∨∀xR(x, y))
∀x(P(x) ⊃¬∃yR(x, y))
(¬∀x¬∀y¬∀zP(x, y) ∨¬∃x¬∃y(¬∃zQ(x, y, z) ⊃R(x, y)))
7.2.2.
Convert the following formulae to prenex form:
(∃x∀yP(x, y) ∧∀y∃xP(y, x))
(¬(∀xP(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w)))
(¬∀x(P(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w)))
7.2.3.
Finish the proof of lemma 7.2.1.
7.2.4.
Finish the proof of theorem 7.2.1.
7.2.5.
Prove lemma 7.2.2.
7.2.6.
Write a computer program for converting a formula to prenex form.
7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formu-
lae
First, we will need a certain normal form for proofs.
7.3.1 Pure Variable Proofs
Gentzen’s sharpened Hauptsatz for prenex formulae is obtained by observing
that in a cut-free proof of a prenex sequent (in LK or LKe), the quantiﬁer
rules can be interchanged with the propositional rules, in such a way that no
propositional rule appears below (closer to the root of the proof tree than)
a quantiﬁer rule. There is actually a more general permutability lemma due
to Kleene, but only part of this lemma is needed to obtain the sharpened
Hauptsatz.
During the course of the proof of the sharpened Hauptsatz, it will be
necessary to show that every provable sequent (in LK or LKe) has a proof
in which the variables introduced by ∀: right and ∃: left rules satisfy
certain conditions. Proofs satisfying these conditions are called “pure-variable
proofs.” To illustrate the necessity of these conditions, consider the following
example.

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
311
EXAMPLE 7.3.1
Γ →P(y), Q
∀: right
Γ →∀xP(x), Q
Γ →∀xP(x), R(y)
∧: right
Γ →∀xP(x), Q ∧R(y)
(To shorten proof trees, structural rules are often not shown.) The rule
∀: right occurs above the rule ∧: right. We wish to permute these two
rules; that is, construct a proof with the same conclusion and the same
premises, but with the ∀: right rule occurring below the ∧: right rule.
The following “proof” almost works, except that the variable y occurs
free in the conclusion of the ∀: right rule, violating the eigenvariable
condition.
Γ →P(y), Q
weakening and exchanges
Γ →P(y), ∀xP(x), Q
Γ →∀xP(x), R(y)
weakening and exchanges
Γ →P(y), ∀xP(x), R(y)
∧: right
Γ →P(y), ∀xP(x), Q ∧R(y)
∀: right
Γ →∀xP(x), ∀xP(x), Q ∧R(y)
contraction
Γ →∀xP(x), Q ∧R(y)
The problem can be eliminated by making sure that for every eigen-
variable y used in an application of the rule ∀: right or ∃: left, that
variable only occurs in the subtree rooted at the sequent constituting
the premise of the rule (and y does not occur both free and bound, but
this is already a condition required for constructing proof trees). If we
rename the topmost occurrence of y in the ﬁrst proof tree with the new
variable z, the problem is indeed eliminated, as shown.
EXAMPLE 7.3.2
Pure-variable proof tree
Γ →P(z), Q
∀: right
Γ →∀xP(x), Q
Γ →∀xP(x), R(y)
∧: right
Γ →∀xP(x), Q ∧R(y)
New legal proof tree with ∧: right above ∀: right

312
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Γ →P(z), Q
weakening and exchanges
Γ →P(z), ∀xP(x), Q
Γ →∀xP(x), R(y)
weakening and exchanges
Γ →P(z), ∀xP(x), R(y)
∧: right
Γ →P(z), ∀xP(x), Q ∧R(y)
∀: right
Γ →∀xP(x), ∀xP(x), Q ∧R(y)
contraction
Γ →∀xP(x), Q ∧R(y)
Deﬁnition 7.3.1
A proof tree (in LK, LKe, G or G=) is a pure-variable
proof tree iﬀ, for every variable y occurring as the eigenvariable in an appli-
cation of the rule ∀: right or ∃: left, y does not occur both free and bound
in any formula in the proof tree, and y only occurs in the subtree rooted at
the sequent constituting the premise of the rule.
Lemma 7.3.1 In LK or LKe (or G or G=), every proof of a sequent Γ →∆
can be converted to a pure-variable proof of the same sequent, simply by
replacing occurrences of variables with new variables.
Proof : First, recall that from lemma 6.5.1, given a sequent Γ →∆with
proof tree T, where the variable x does not occur bound in T, for any variable
y not occurring in T, the tree T[y/x] obtained by substituting y for every free
occurrence of x in T is a proof tree for the sequent Γ[y/x] →∆[y/x].
The rest of the proof proceeds by induction on the number k of inferences
of the form ∀: right or ∃: left in the proof tree T for the sequent Γ →∆. If
k = 0, the lemma is obvious. Otherwise, T is of the form
T1
...
Tn
S
where each tree Ti is of the form
Qi
Γi →∆i, Ai[yi/xi]
Γi →∆i, ∀xiAi
or
Qi
Ai[yi/xi], Γi →∆i
∃xiAi, Γi →Γi
where S does not contain any inferences of the form ∀: right or ∃: left,
the root of Qi is labeled with Γi →∆i, Ai[yi/xi] (or Ai[yi/xi], Γi →∆i),

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
313
and the leaves of S are either axioms or are labeled with Γi →∆i, ∀xiAi
(or ∃xiAi, Γi →Γi). Since each proof tree Qi contains fewer applications of
∀: right or ∃: left than T, by the induction hypothesis, there is a pure-
variable proof tree Q′
i for each Γi →∆i, Ai[yi/xi] (or Ai[yi/xi], Γi →∆i).
Note that no variable free in Γi →∆i, Ai[yi/xi] (or Ai[yi/xi], Γi →∆i),
including yi, is used as an eigenvariable in Q′
i. For every i = 1, ..., n, let Q′′
i
be the proof tree obtained from Q′
i by renaming all eigenvariables in Q′
i, so
that the set of eigenvariables in Q′′
i is disjoint from the set of eigenvariables
in Q′′
j for i ̸= j, and is also disjoint from the set of variables in S. Finally, for
i = 1, ..., n, if yi occurs in S (not below Γi →∆i, ∀xiAi or ∃xiAi, Γi →∆i,
since yi is an eigenvariable), let zi be a new variable (not in any of the Q′′
i ,
and such that zi ̸= zj whenever i ̸= j), and let T ′
i be the tree
Q′′
i [zi/yi]
Γi →∆i, Ai[zi/xi]
Γi →∆i, ∀xiAi
or
Q′′
i [zi/yi]
Ai[zi/xi], Γi →Γi
∃xiAi, Γi →Γi
obtained by substituting zi for each occurrence of yi in Q′′
i . Then, using the
above claim, one can verify that the following proof tree is a pure-variable
proof tree:
T ′
1
...
T ′
n
S
We will also need the following lemma, which is analogous to lemma 6.3.1.
Lemma 7.3.2
(i) Every sequent provable in LK is provable with a proof in
which the axioms are sequents A →A, where A is atomic.
(ii) Every sequent provable in LKe is provable with a proof in which the
axioms are either sequents A →A where A is atomic, or equality axioms.
Proof : The proof proceeds by induction on the weight of a proof as in
lemma 6.3.1, and is very similar to the proof given in that lemma, except that
LK (or LKe) inferences are used. The details are left as an exercise.

314
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.3.2 The Permutability Lemma
The following terminology will be used in proving the permutability lemma
used for establishing the sharpened Hauptsatz.
Deﬁnition 7.3.2
If in a proof tree T the conclusion of a one-premise rule
R1 is a descendant of some premise of a rule R2, and if any intermediate
rules occur between R1 and R2, then they are structural rules, we say that
R1 can be permuted with R2, iﬀa proof tree T ′ can be constructed from T as
explained below.
If T is of the form
T1
S3
R1
S′
1
ST
S1
T2
S2
R2
S0
T0
or of the form
T1
S3
T2
S2
R1
S′
1
ST
S1
R2
S0
T0
where S0, S1, S2, S3, S′
1 are sequents, T0, T1, T2 are trees, and the only other
rules in ST between R1 and R2 (if any) are structural rules, then T ′ is of the
form
T1
S3
T2
S2
Q
S0
T0

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
315
and in the tree Q, the conclusion of the rule R1 is not a descendant of any
premise of the rule R2, and the other rules used in Q besides R1 and R2 are
structural rules.
If T is of the form
T1
S2
R1
S′
1
ST
S1
R2
S0
T0
and the only rules in ST (if any) between R1 and R2 are structural rules, then
T ′ is of the form
T1
S2
Q
S0
T0
where in the tree Q, the conclusion of the rule R1 is not a descendant of the
premise of the rule R2, and the other rules used in Q besides R1 and R2 are
structural rules.
Lemma 7.3.3 In every pure-variable, cut-free proof T in LK (proof without
essential cuts in LKe) the following properties hold:
(i) Every quantiﬁer rule R1 can be permuted with a rule R2 which is
either a logical rule, or a structural rule, or the cut rule, provided that the
principal formula of the quantiﬁer rule is not a side formula of the lower rule
R2.
(ii) Every quantiﬁer rule, contraction or exchange rule R1 can be per-
muted with a weakening rule R2 whose premise is the conclusion of R1.
Proof : There are several cases to consider.
We only consider some
typical cases, leaving the others as exercises.
(i) R1 = ∀: right, R2 = ∧: right.
We have the following tree:

316
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Γ′ →∆′, A[y/x]
∀: right
Γ′ →∆′, ∀xA
structural rules ST
Γ →∆, ∀xA, Λ, B
Γ →∆, ∀xA, Λ, C
∧: right
Γ →∆, ∀xA, Λ, B ∧C
The permutation is achieved as follows:
Γ′ →∆′, A[y/x]
weakening and exchanges
Γ′ →A[y/x], ∆′, ∀xA
structural rules ST
Γ →A[y/x], ∆, ∀xA, Λ, B
Γ →∆, ∀xA, Λ, C
weakening and exchanges
Γ →A[y/x], ∆, ∀xA, Λ, C
∧: right
Γ →A[y/x], ∆, ∀xA, Λ, B ∧C
exchanges
Γ →∆, ∀xA, Λ, B ∧C, A[y/x]
∀: right
Γ →∆, ∀xA, Λ, B ∧C, ∀xA
exchanges, contraction, and exchanges
Γ →∆, ∀xA, Λ, B ∧C
Since the ﬁrst tree is part of a pure-variable proof, y only occurs above
the conclusion of the ∀: right rule in it, and so, y does not occur in the con-
clusion of the ∀: right rule in the second tree and the eigenvariable condition
is satisﬁed.
(ii) R1 = ∀: left, R2 = ∧: right.
We have the following tree:
A[t/x], Γ′ →∆′
∀: left
∀xA, Γ′ →∆′
structural rules ST
Γ, ∀xA, ∆→Λ, B
Γ, ∀xA, ∆→Λ, C
∧: right
Γ, ∀xA, ∆→Λ, B ∧C

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
317
The permutation is achieved as follows:
A[t/x], Γ′ →∆′
weakening and exchanges
∀xA, Γ′, A[t/x] →∆′
structural rules ST
Γ, ∀xA, ∆, A[t/x] →Λ, B
Γ, ∀xA, ∆→Λ, C
weakening and exchanges
Γ, ∀xA, ∆, A[t/x] →Λ, C
∧: right
Γ, ∀xA, ∆, A[t/x] →Λ, B ∧C
exchanges
A[t/x], Γ, ∀xA, ∆→Λ, B ∧C
∀: left
∀xA, Γ, ∀xA, ∆→Λ, B ∧C
exchanges, contraction, and exchanges
Γ, ∀xA, ∆→Λ, B ∧C
(iii) R1 = ∀: left, R2 = ∧: left.
We have the following tree:
A[t/x], Γ′ →∆′
∀: left
∀xA, Γ′ →∆′
structural rules ST
B, Γ, ∀xA, ∆→Λ
∧: left
B ∧C, Γ, ∀xA, ∆→Λ
The permutation is achieved as follows:

318
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
A[t/x], Γ′ →∆′
weakening and exchanges
∀xA, Γ′, A[t/x] →∆′
structural rules ST
B, Γ, ∀xA, ∆, A[t/x] →Λ
∧: left
B ∧C, Γ, ∀xA, ∆, A[t/x] →Λ
exchanges
A[t/x], B ∧C, Γ, ∀xA, ∆→Λ
∀: left
∀xA, B ∧C, Γ, ∀xA, ∆→Λ
exchanges, contraction, and exchanges
B ∧C, Γ, ∀xA, ∆→Λ
(iv) R1 = ∀: right, R2 = weakening right.
We have the following tree:
Γ →∆, A[y/x]
∀: right
Γ →∆, ∀xA
weakening right
Γ →∆, ∀xA, B
The permutation is performed as follows:
Γ →∆, A[y/x]
weakening right
Γ →∆, A[y/x], B
exchange
Γ →∆, B, A[y/x]
∀: right
Γ →∆, B, ∀xA
exchange
Γ →∆, ∀xA, B
Since the ﬁrst tree is part of a pure-variable proof, y only occurs above
the conclusion of the ∀: right rule in it, and so, y does not occur in the con-
clusion of the ∀: right rule in the second tree and the eigenvariable condition
is satisﬁed.
(v) R1 = exchange right, R2 = weakening right.
We have the following tree:

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
319
Γ →∆, A, B, Λ
exchange right
Γ →∆, B, A, Λ
weakening right
Γ →∆, B, A, Λ, C
The permutation is performed as follows:
Γ →∆, A, B, Λ
weakening right
Γ →∆, A, B, Λ, C
exchange right
Γ →∆, B, A, Λ, C
(vi) R1 = contraction right, R2 = weakening right.
We have the following tree:
Γ →∆, A, A
contraction right
Γ →∆, A
weakening right
Γ →∆, A, B
The permutation is performed as follows:
Γ →∆, A, A
weakening right
Γ →∆, A, A, B
exchanges
Γ →∆, B, A, A
contraction right
Γ →∆, B, A
exchange
Γ →∆, A, B
(vii) R1 = ∀: right, R2 = atomic cut.
We have the following tree:
Γ′ →∆′, A[y/x]
∀: right
Γ′ →∆′, ∀xA
structural rules ST
Γ →∆1, ∀xA, ∆2, B
B, Λ →Θ
atomic cut (B)
Γ, Λ →∆1, ∀xA, ∆2, Θ
Since B is atomic, it is diﬀerent from ∀xA.

320
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
The permutation is performed as follows:
Γ′ →∆′, A[y/x]
weakening and exchanges
Γ′ →A[y/x], ∆′, ∀xA
structural rules ST
Γ →A[y/x], ∆1, ∀xA, ∆2, B
B, Λ →Θ
atomic cut (B)
Γ, Λ →A[y/x], ∆1, ∀xA, ∆2, Θ
exchanges
Γ, Λ →∆1, ∀xA, ∆2, Θ, A[y/x]
∀: right
Γ, Λ →∆1, ∀xA, ∆2, Θ, ∀xA
exchanges, contraction, and exchanges
Γ, Λ →∆1, ∀xA, ∆2, Θ
Remark: It is easily shown that the rules ∀: right and ∃: left permute
with each other, permute with the rules ∀: left and ∃: right, and the
rules ∀: left and ∃: right permute with each other, provided that the main
formula of the the upper inference is not equal to the side formula of the lower
inference.
7.3.3 Gentzen’s Sharpened Hauptsatz
We now prove Gentzen’s sharpened Hauptsatz.
Theorem 7.3.1
(Gentzen’s sharpened Hauptsatz) Given a sequent Γ →∆
containing only prenex formulae, if Γ →∆is provable in LK (or LKe), then
there is a cut-free, pure-variable proof in LK (proof without essential cuts in
LKe) that contains a sequent Γ′ →∆′ (called the midsequent), which has the
following properties:
(1) Every formula in Γ′ →∆′ is quantiﬁer free.
(2) Every inference rule above Γ′ →∆′ is either structural, logical (not
a quantiﬁer rule), or an inessential cut.
(3) Every inference rule below Γ′ →∆′ is either a quantiﬁer rule, or a
contraction, or an exchange rule (but not a weakening).
Thus, the midsequent splits the proof tree into an upper part that con-
tains the propositional inferences, and a lower part that contains the quantiﬁer
inferences (and no weakening rules).

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
321
Proof : From theorem 6.3.1, lemma 7.3.1, and lemma 7.3.2, we can as-
sume that Γ →∆has a cut-free proof T in LK (without essential cuts in
LKe) that is a pure-variable proof, and such that the axioms are of the form
either A →A with A atomic, or an equality axiom (it is actually suﬃcient to
assume that A is quantiﬁer free).
For every quantiﬁer inference R in T, let m(R) be the number of propo-
sitional inferences and n(R) the number of weakening inferences on the path
from R to the root (we shall say that such inferences are below R). For a proof
T, m(T) is the sum of all m(R) and n(T) the sum of all n(R), for all quantiﬁer
inferences R in T. We shall show by induction on (m(T), n(T)) (using the
lexicographic ordering deﬁned in Subsection 2.1.10) that the theorem holds.
(i) m(T) = 0, n(T) = 0. Then, all propositional inferences and all weak-
ening inferences are above all quantiﬁer inferences. Let S0 be the premise of
the highest quantiﬁer inference (that is, such that no descendant of this infer-
ence is a quantiﬁer inference). If S0 only contains quantiﬁer-free formulae, S0
is the midsequent and we are done. Otherwise, since the proof is cut free (with-
out essential cuts in LKe) and since the axioms only contain quantiﬁer-free
formulae (actually, atomic formulae), prenex quantiﬁed formulae in S0 must
have been introduced by weakening rules. If T0 is the portion of the proof with
conclusion S0, by eliminating the weakening inferences from T0, we obtain a
(quantiﬁer-free) proof T1 of the sequent S1 obtained by deleting all quantifed
formulae from S0. For every quantiﬁed prenex formula A = Q1x1...QnxnC
occurring in S0, let B = C[z1/x1, ..., zn/xn] be the quantiﬁer-free formula ob-
tained from A by substituting new variables z1, ..., zn for x1, ..., xn in C (we
are assuming that these new variables are distinct from all the variables oc-
curring in T1 and distinct from the variables used for other such occurrences
of quantiﬁed formulae occurring in S0). Let Γ′ →∆′ be the sequent obtained
from S0 by replacing every occurrence of a quantiﬁed formula A by the corre-
sponding quantiﬁer-free formula B, as above. It is clear that Γ′ →∆′ can be
obtained from S1 by weakening inferences, and that S0 can be obtained from
Γ′ →∆′ by quantiﬁer inferences. But then, Γ′ →∆′ is the midsequent of the
proof obtained from T1 and the intermediate inferences from S1 to Γ′ →∆′
and from Γ′ →∆′ to S0. Since the weakening inferences only occur above
Γ′ →∆′, all the conditions of the theorem are satisﬁed.
(ii) Case 1: m(T) > 0. In this case, there is an occurrence R1 of a
quantiﬁer inference above an occurrence R2 of a propositional inference, and
intermediate inferences (if any) are structural. Since all formulae in the root
sequent are prenex, the side formula (or formulae) of R2 are quantiﬁer free.
Hence, lemma 7.3.3(i) applies, R1 can be permuted with R2, yielding a new
proof T ′ such that m(T ′) = m(T)−1. We conclude by applying the induction
hypothesis to T ′.
(ii) Case 2: m(T) = 0, n(T) > 0. In this case, all propositional infer-
ences are above all quantiﬁer inferences, but there is a quantiﬁer inference
R1 above a weakening inference R2, and intermediate inferences (if any) are

322
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
exchange or contraction rules. Using lemma 7.3.3(ii), R2 can be moved up
until R1 and R2 are permuted, yielding a new proof T ′ such that m(T ′) = 0
and n(T ′) = n(T) −1. We conclude by applying the induction hypothesis to
T ′.
Remark: In the case where m(T) > 0, even though m(T ′) = m(T) −1,
n(T ′) might be increased because the transformation may introduce extra
weakenings. However, n(T) is eventually reduced to 0, when m(T) is reduced
to 0. Also, note that the proof provides an algorithm for converting a pure-
variable cut-free proof (proof without essential cuts in LKe) into a proof
having the properties stated in the theorem.
An example of the transformations of lemma 7.3.3 also showing the
values of m(T) and n(T) is given below. In order to shorten proofs, some
contractions and exchanges will not be shown explicitly.
EXAMPLE 7.3.3
Consider the following proof tree T in which m(T) = 2 and n(T) = 0:
P(f(a)) →P(f(a))
P(f(a)) →P(f(a)), ∃yR(y)
P(f(a)) →∃xP(x), ∃yR(y)
R(g(a)) →R(g(a))
R(g(a)) →R(g(a)), ∃xP(x)
R(g(a)) →∃xP(x), ∃yR(y)
∨: left
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y)
We ﬁrst exchange the ∃: right inference in the right subtree with ∨: left
obtaining the following tree
T1
T2
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), R(g(a))
∃: right
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), ∃yR(y)
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y)
where T1 is the tree
P(f(a)) →P(f(a))
P(f(a)) →P(f(a)), ∃yR(y)
∃: right
P(f(a)) →∃xP(x), ∃yR(y)
weakening
P(f(a)) →∃xP(x), ∃yR(y), R(g(a))
and T2 is the tree

7.3 Gentzen’s Sharpened Hauptsatz for Prenex Formulae
323
R(g(a)) →R(g(a))
R(g(a)) →R(g(a)), ∃xP(x)
weakening, exchanges
R(g(a)) →∃xP(x), ∃yR(y), R(g(a))
We now have n(T) = 1 and m(T) = 1, since a weakening was introduced
below the ∃: right inference in the tree T1. We ﬁnally exchange the
∃: right inference in T1 with ∨: left. We obtain the following tree
T1
T2
∨: left
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), R(g(a)), P(f(a))
∃: right
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), R(g(a)), ∃xP(x)
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), R(g(a))
∃: right
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y), ∃yR(y)
P(f(a)) ∨R(g(a)) →∃xP(x), ∃yR(y)
where T1 is the tree
P(f(a)) →P(f(a))
P(f(a)) →P(f(a)), ∃xP(x)
P(f(a)) →P(f(a)), ∃xP(x), ∃yR(y)
P(f(a)) →∃xP(x), ∃yR(y), R(g(a)), P(f(a))
and T2 is the tree
R(g(a)) →R(g(a))
R(g(a)) →R(g(a))), ∃xP(x)
R(g(a)) →R(g(a)), ∃xP(x), ∃yR(y)
R(g(a)) →∃xP(x), ∃yR(y), R(g(a)), P(f(a))
We now have n(T) = 0 and m(T) = 0 and the proof is in normal form.
Another example of a proof in the normal form given by theorem 7.3.1
is shown below.

324
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
EXAMPLE 7.3.4
Consider the sequent
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
whose validity is equivalent to the validity of the formula
A = ∃x∀y¬P(x, y) ∨∃y1∀z¬Q(y1, z) ∨∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
The following proof satisﬁes the conditions of theorem 7.3.1:
P(x1, y) →P(x1, y)
P(x1, y), Q(y, z) →P(x1, y)
Q(y, z) →Q(y, z)
P(x1, y), Q(y, z) →Q(y, z)
P(x1, y), Q(y, z) →P(x1, y) ∧Q(y, z)
P(x1, y) →¬Q(y, z), P(x1, y) ∧Q(y, z)
→¬P(x1, y), ¬Q(y, z), P(x1, y) ∧Q(y, z)
→¬P(x1, y), ¬Q(y, z), ∃z1(P(x1, y) ∧Q(y, z1))
→¬P(x1, y), ¬Q(y, z), ∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
→¬P(x1, y), ∀z¬Q(y, z), ∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
→¬P(x1, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
→∀y¬P(x1, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
The midsequent is
→¬P(x1, y), ¬Q(y, z), P(x1, y) ∧Q(y, z).
Notice that the proof was constructed so that the ∀: right rule is always
applied as soon as possible. The reason for this is that we have the freedom
of choosing the terms involved in applications of the ∃: right rule (as long as
they are free for the substitutions), whereas the ∀: right rule requires that
the substituted term be a new variable. The above strategy has been chosen
to allow ourselves as much freedom as possible in the substitutions.
PROBLEMS
7.3.1.
Prove lemma 7.3.2.

7.4 The Sharpened Hauptsatz for Sequents in NNF
325
7.3.2.
Finish the proof of the cases in lemma 7.3.3.
7.3.3.
Prove the following facts stated as a remark at the end of the proof
of lemma 7.3.3: The rules ∀: right and ∃: left permute with each
other, permute with the rules ∀: left and ∃: right, and the rules
∀: left and ∃: right permute with each other, provided that the main
formula of the the upper inference is not equal to the side formula of
the lower inference.
7.3.4.
Give proofs in normal form for the prenex form of the following se-
quents:
(∃x∀yP(x, y) ∧∀y∃xP(y, x)) →
(¬(∀xP(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w))) →
(¬∀x(P(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w))) →
7.3.5.
Write a computer program converting a proof into a proof in normal
form as described in theorem 7.3.1.
7.3.6.
Design a search procedure for prenex sequents using the suggestions
made at the end of example 7.3.4.
7.4 The Sharpened Hauptsatz for Sequents in NNF
In this section, it is shown that the quantiﬁer rules of the system G1nnf pre-
sented in Section 6.4 can be extended to apply to certain subformulae, so that
the permutation lemma holds. As a consequence, the sharpened Hauptsatz
also holds for such a system. In this section, all formulae are rectiﬁed.
7.4.1 The System G2nnf
First, we show why we chose to deﬁne such rules for formulae in NNF and
not for arbitrary formulae.
EXAMPLE 7.4.1
Let
A = P(a) ∨¬(Q(b) ∧∀xQ(x)) →.
Since A is logically equivalent to P(a) ∨(¬Q(b) ∨∃x¬Q(x)), the correct
rule to apply to the subformula ∀xQ(x) is actually the ∃: left rule!
Hence, the choice of the rule applicable to a quantiﬁed subformula B
depends on the parity of the number of negative subformulae that have the
formula B has a subformula. This can be handled, but makes matters more
complicated. However, this problem does not arise with formulae in NNF
since only atoms can be negated.

326
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Since there is no loss of generality in considering formulae in NNF and
the quantiﬁer rules for subformulae of formulae in NNF are simpler than for
arbitrary formulae, the reason for considering formulae in NNF is clear.
The extended quantiﬁer rules apply to maximal quantiﬁed subformulae
of a formula. This concept is deﬁned rigorously as follows.
Deﬁnition 7.4.1
Given a formula A in NNF, the set QF(A) of maximal
quantiﬁed subformulae of A is deﬁned inductively as follows:
(i) If A is a literal, that is either an atomic formula B or the negation
¬B of an atomic formula, then QF(A) = ∅;
(ii) If A is of the form (B ∨C), then QF(A) = QF(B) ∪QF(C);
(iii) If A is of the form (B ∧C), then QF(A) = QF(B) ∪QF(C).
(iv) If A is of the form ∀xB, then QF(A) = {A}.
(v) If A is of the form ∃xB, then QF(A) = {A}.
EXAMPLE 7.4.2
Let
A = ∀uP(u) ∨(Q(b) ∧∀x(¬P(x) ∨∀yR(x, y))) ∨∃zP(z).
Then
QF(A) = {∀uP(u), ∀x(¬P(x) ∨∀yR(x, y)), ∃zP(z)}.
However, ∀yR(x, y) is not a maximal quantiﬁed subformula since it is a
subformula of ∀x(¬P(x) ∨∀yR(x, y)).
Since we are dealing with rectiﬁed formulae, all quantiﬁed subformulae
diﬀer by at least the outermost quantiﬁed variable, and therefore, they are all
distinct. This allows us to adopt a simple notation for the result of substituting
a formula for a quantiﬁed subformula.
Deﬁnition 7.4.2 Given a formula A in NNF whose set QF(A) is nonempty,
for any subformula B in QF(A), for any formula C, the formula A[C/B] is
deﬁned inductively as follows:
(i) If A = B, then A[C/B] = C;
(ii) If A is of the form (A1 ∨A2), B belongs either to A1 or to A2 but not
both (since subformulae in QF(A) are distinct), and assume that B belongs
to A1, the other case being similar. Then, A[C/B] = (A1[C/B] ∨A2);
(iii) If A is of the form (A1 ∧A2), as in (ii), assume that B belongs to
A1. Then, A[C/B] = (A1[C/B] ∧A2).

7.4 The Sharpened Hauptsatz for Sequents in NNF
327
EXAMPLE 7.4.3
Let
A = ∀uP(u) ∨(Q(b) ∧∀x(¬P(x) ∨∀yR(x, y))),
B = ∀x(¬P(x) ∨∀yR(x, y)),
and
C = (¬P(f(a)) ∨∀yR(f(a), y)).
Then,
A[C/B] = ∀uP(u) ∨(Q(b) ∧(¬P(f(a)) ∨∀yR(f(a), y))).
We now deﬁne the system G2nnf.
Deﬁnition 7.4.3
(Gentzen system G2nnf) The Gentzen system G2nnf is
the system obtained from G1nnf by adding the weakening rules, by replacing
the quantiﬁer rules by the quantiﬁer rules for subformulae listed below, and
using axioms of the form A →A, →A, ¬A, A, ¬A →, and ¬A →¬A, where
A is atomic. Let A be any formula in NNF containing quantiﬁers, let C be
any subformula in QF(A) of the form ∀xB, and D any subformula in QF(A)
of the form ∃xB. Let t be any term free for x in B.
A[B[t/x]/C], Γ →∆
A, Γ →∆
(∀: left)
The formula A[B[t/x]/C] is the side formula of the inference. Note that
for A = C = ∀xB, this rule is identical to the ∀: left rule of G1nnf.
Γ →∆, A[B[y/x]/C]
Γ →∆, A
(∀: right)
where y is not free in the lower sequent.
The formula A[B[y/x]/C] is the side formula of the inference. For A =
C = ∀xB, the rule is identical to the ∀: right rule of G1nnf.
A[B[y/x]/D], Γ →∆
A, Γ →∆
(∃: left)
where y is not free in the lower sequent.
The formula A[B[y/x]/D] is the side formula of the inference. For A =
D = ∃xB, the rule is identical to the ∃: left rule of G1nnf.
Γ →∆, A[B[t/x]/D]
Γ →∆, A
(∃: right)

328
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
The formula A[B[t/x]/D] is the side formula of the inference. For A =
D = ∃xB, the rule is identical to the ∃: right rule of G1nnf.
7.4.2 Soundness of the System G2nnf
The soundness of the quantiﬁer rules of G2nnf of deﬁnition 7.4.3 is shown in
the following lemma.
Lemma 7.4.1
The system G2nnf is sound.
Proof : We treat the case of the rules ∀: left and ∀: right, the case of
the rules ∃: left and ∃: right beeing similar. We need to prove that if the
premise of the rule is valid, then the conclusion is valid. Equivalently, this
can shown by proving that if the conclusion if falsiﬁable, then the premise is
falsiﬁable.
Case 1: ∀: left.
To show that if the conclusion is falsiﬁable then the premise is falsiﬁable
amounts to proving the following: For every structure A, for every assignment
s:
(i) A = C: If ∀xB is satisﬁed in A by s, then B[t/x] is satisﬁed in A by
s. This has been shown in lemma 5.4.2.
(ii) A ̸= C: If A is satisﬁed in A by s, then A[B[t/x]/C] is satisﬁed in
A by s. We proceed by induction on A.
If A is of the form (A1 ∨A2), we can assume without loss of generality
that C is in QF(A1). But (A1 ∨A2) is satisﬁed in A by s iﬀeither A1 is
satisﬁed in A by s or A2 is satisﬁed in A by s. Since
A[B[t/x]/C] = (A1[B[t/x]/C] ∨A2),
in the second case, (A1[B[t/x]/C]∨A2) is also satisﬁed in A by s. In the ﬁrst
case, by the induction hypothesis, since A1 is satisﬁed in A by s, A1[B[t/x]/C]
is also satisﬁed in A by s, and so (A1[B[t/x]/C] ∨A2) is satisﬁed in A by s,
as desired.
If A is of the form (A1 ∧A2), assume as in the previous case that C
occurs in A1. If A is satisﬁed in A by s, then both A1 and A2 are satisﬁed
in A by the same assignment s. By the induction hypothesis, A1[B[t/x]/C]
is satisﬁed in A by s, and so A[B[t/x]/C] is satisﬁed in A by s.
If A is equal to C, then we have to show that if ∀xB is satisﬁed in A by
s, then B[t/x] is satisﬁed in A by s, but this reduces to Case 1(i).
Case 2: ∀: right.
This time, showing that if the conclusion is falsiﬁable then the premise
is falsiﬁable amounts to proving the following: For every structure A, if A is

7.4 The Sharpened Hauptsatz for Sequents in NNF
329
falsiﬁable in A by s, then A[B[y/x]/D] is falsiﬁable in A by an assignment s′
of the form s[y := a], where a is some element in the domain of A, and y is
not free in A, Γ and ∆. There are two cases:
(i) A = D; If ∀xB is falsiﬁable in A by s, there is some element a in the
domain of A such that B[a/x] is falsiﬁed in A, and since y does not occur
free in the conclusion of the inference, by lemma 5.4.2, B[y/x] is falsiﬁable in
A by s[y := a].
(ii) A ̸= D; If A is falsiﬁable in A by s, then A[B[y/x]/D] is falsiﬁable
in A by an assignment s′ of the form s[y := a], where y does not occur free
in A, Γ, ∆.
We prove by induction on formulae that for every subformula A′ of A
containing some formula in QF(A) as a subformula, if A′ is falsiﬁed in A by s,
then A′[B[y/x]/D] is falsiﬁed in A by an assignment s′ of the form s[y := a],
where y is not free in A, Γ and ∆. Note that in the induction hypothesis, it
is necessary to state that y is not free in A, and not just A′.
If A′ is of the form (A1 ∨A2), we can assume without loss of generality
that D is in QF(A1). But (A1 ∨A2) is falsiﬁed in A by s iﬀA1 is falsiﬁed in
A by s and A2 is falsiﬁed in A by s. Since
A′[B[y/x]/D] = (A1[B[y/x]/D] ∨A2),
by the induction hypothesis A1[B[y/x]/D] is falsiﬁed in A by some assignment
of the form s[y := a] where y is not not free in A, Γ and ∆. But since A2 is
a subformula of A′ and thus of A, y is not free in A2 and by lemma 5.3.3, A2
is also falsiﬁed in A by s[y := a]. Then A′[B[y/x]/D] is falsiﬁed in A by the
assignment s[y := a].
If A′ is of the form (A1 ∧A2), assume as in the previous case that D
occurs in A1. If A′ is falsiﬁed in A by s, then either A1 is falsiﬁed in A by s
or A2 is falsiﬁed in A by s. In the second case (A1[B[y/x]/D]∧A2) is falsiﬁed
in A by s. In the ﬁrst case, since A1 is falsiﬁed in A by s, by the induction
hypothesis A1[B[y/x]/D] is falsiﬁed in A by some assignment s′ of the form
s[y := a]. Then (A1[B[y/x]/D] ∧A2) is falsiﬁed in A by s[y := a], as desired.
If A′ is equal to D, then we are back to case 1(i).
It should be emphasized that the condition that y is not free in A, Γ
and ∆, and not just D, Γ, ∆plays a crucial role in the proof. Note that the
system G1nnf is a subsystem of G2nnf.
We now prove a version of lemma 7.3.3 and a normal form theorem
analogous to theorem 7.3.1 for the system G2nnf.

330
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.4.3 A Gentzen-like Sharpened Hauptsatz for G2nnf
The deﬁnition of an inference R1 permuting with an inference R2 given in
deﬁnition 7.3.2 extends immediately to the new quantiﬁer rules, except that
the section ST of structural rules only contains weakenings. It is also easy to
see that lemmas 7.3.1 and 7.3.2 extend to the system G2nnf. This is left as
an exercise to the reader.
Consider a quantiﬁer rule of G2nnf, say ∀: left:
A[B[t/x]/C], Γ →∆
A, Γ →∆
If A belongs to Γ, letting Λ = Γ −{A}, the rule can be written as:
A, A[B[t/x]/C], Λ →∆
A, Λ →∆
where A does not belong to Λ. In this version, we say that the rule is the
∀: left rule with contraction. The ﬁrst version of the rule in which either
A ∈Γ or A does not belong to the upper sequent is called the rule without
contraction. The deﬁnition extends to the other quantiﬁer rules.
Lemma 7.4.2
In every pure-variable, cut-free proof T in G2nnf, the fol-
lowing holds: Every quantiﬁer rule R1 of G2nnf can be permuted with a
propositional rule R2 or a weakening.
Proof : The proof proceeds by cases. First, we prove that every quantiﬁer
rule of G1nnf without contraction can be permuted with a propositional rule
R2 or a weakening. It is not diﬃcult to see that the cases treated in lemma
7.3.3 are still valid with the rules of G1nnf.
This is because sequents are
pairs of sets, and contraction rules can be simulated. For example to perform
the permutation involving R1 = ∀: right and R2 = ∧: right, the following
deduction is used:
Γ′ →∆′, A[y/x]
Γ′ →A[y/x], ∆′, ∀xA
weakening rules
Γ →A[y/x], ∆, ∀xA, Λ, B
Γ →∆, ∀xA, Λ, C
weakening
Γ →A[y/x], ∆, ∀xA, Λ, C
∧: right
Γ →∆, ∀xA, Λ, B ∧C, A[y/x]
∀: right
Γ →∆, ∀xA, Λ, B ∧C
In the lowest inference, we took advantage of the fact that
Γ →∆, ∀xA, Λ, B ∧C, ∀xA

7.4 The Sharpened Hauptsatz for Sequents in NNF
331
denotes the same sequent as
Γ →∆, ∀xA, Λ, B ∧C,
since a sequent is a pair of sets.
The details are left as an exercise.
We
still have to consider the cases of quantiﬁer rules of G1nnf with contractions,
quantiﬁer rules of G2nnf applied to the side formula of a propositional rule,
and permutation with weakenings. We treat ∧: left and ∧: right, the other
cases being similar.
Case 1: ∧: left, ∀: left. There are two subcases:
1.1: A ∈Γ or A does not belong to the top sequent:
A[B[t/x]/C], Γ →∆
∀: left
A, Γ →∆
∧: left
A ∧E, Γ →∆
The permutation is achieved as follows:
A[B[t/x]/C], Γ →∆
∧: left
A[B[t/x]/C] ∧E, Γ →∆
∀: left
A ∧E, Γ →∆
1.2: A /∈Γ and A occurs in the top sequent.
A, A[B[t/x]/C], Γ →∆
∀: left
A, Γ →∆
∧: left
A ∧E, Γ →∆
The permutation is achieved as follows:
A, A[B[t/x]/C], Γ →∆
∧: left
A ∧E, A[B[t/x]/C], Γ →∆
∧: left
A ∧E, A[B[t/x]/C] ∧E, Γ →∆
∀: left
A ∧E, Γ →∆
Case 2: ∧: left, ∃: left.
2.1: A ∈Γ or A does not belong to the top sequent:

332
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
A[B[y/x]/D], Γ →∆
∃: left
A, Γ →∆
∧: left
A ∧E, Γ →∆
The variable y is a new variable occurring only above A[B[y/x]/D], Γ →
∆. The permutation is achieved by performing the substitution of B[y/x] for
D in A ∧E, yielding A[B[y/x]/D] ∧E. The inference is legitimate since, y
being a variable that only occurs above A[B[y/x]/D], Γ →∆in the previous
proof tree, y does not occur free in A, E, Γ and ∆.
The permutation is
achieved as follows:
A[B[y/x]/D], Γ →∆
∧: left
A[B[y/x]/D] ∧E, Γ →∆
∃: left
A ∧E, Γ →∆
2.2: A /∈Γ and A occurs in the top sequent.
A, A[B[y/x]/D], Γ →∆
∃: left
A, Γ →∆
∧: left
A ∧E, Γ →∆
The permutation is performed as follows:
A, A[B[y/x]/D], Γ →∆
∧: left
A ∧E, A[B[y/x]/D], Γ →∆
∧: left
A ∧E, A[B[y/x]/D] ∧E, Γ →∆
∃: left
A ∧E, Γ →∆
Case 3: ∧: right, ∀: right.
3.1: A ∈Γ or A does not occur in the top sequent:
Γ →∆, A[B[y/x]/C]
∀: right
Γ →∆, A
Γ →∆, E
∧: right
Γ →∆, A ∧E
The variable y is new and occurs only above Γ →∆, A[B[y/x]/C]. The
permutation is achieved as follows:

7.4 The Sharpened Hauptsatz for Sequents in NNF
333
Γ →∆, A[B[y/x]/C]
Γ →∆, E
∧: right
Γ →∆, A[B[y/x]/C] ∧E
∀: right
Γ →∆, A ∧E
The application of the ∀: right is legal because y does not occur free
in Γ →∆, A ∧E, since in the previous proof it occurs only above Γ →
∆, A[B[y/x]/C].
3.2: A /∈Γ and A occurs in the top sequent.
Γ →∆, A, A[B[y/x]/C]
∀: right
Γ →∆, A
Γ →∆, E
∧: right
Γ →∆, A ∧E
The permutation is performed as follows:
Γ →∆, A, A[B[y/x]/C]
Γ →∆, E
Γ →∆, A[B[y/x]/C], E
Γ →∆, A[B[y/x]/C], A ∧E
Γ →∆, E
Γ →∆, E, A ∧E
Γ →∆, A[B[y/x]/C] ∧E, A ∧E
∀: right
Γ →∆, A ∧E
Case 4: ∧: right, ∃right:
4.1: A ∈Γ or A does not occur in the top sequent:
Γ →∆, A[B[t/x]/D]
∃: right
Γ →∆, A
Γ →∆, E
∧: right
Γ →∆, A ∧E
The permutation is achieved as follows:
Γ →∆, A[B[t/x]/D]
Γ →∆, E
∧: right
Γ →∆, A[B[t/x]/D] ∧E
∃: right
Γ →∆, A ∧E
4.2: A /∈Γ and A occurs in the top sequent.

334
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Γ →∆, A, A[B[t/x]/D]
∃: right
Γ →∆, A
Γ →∆, E
∧: right
Γ →∆, A ∧E
The permutation is achieved as follows:
Γ →∆, A, A[B[t/x]/D]
Γ →∆, E
Γ →∆, A[B[t/x]/D], E
Γ →∆, A[B[t/x]/D], A ∧E
Γ →∆, E
Γ →∆, E, A ∧E
Γ →∆, A[B[t/x]/D] ∧E, A ∧E
∃: right
Γ →∆, A ∧E
The other cases are similar and left as an exercise.
We can now prove the following type of normal form theorem for the
systems G1nnf and G2nnf.
Theorem 7.4.1
(A sharpened Hauptsatz for G1nnf and G2nnf) Given a
sequent Γ →∆containing only formulae in NNF, if Γ →∆is provable in
G1nnf, then there is a cut-free, pure-variable proof in G2nnf that contains a
sequent Γ′ →∆′ (called the midsequent), which has the following properties:
(1) Every formula in Γ′ →∆′ is quantiﬁer free.
(2) Every inference rule above Γ′ →∆′ is either a weakening or propo-
sitional (not a quantiﬁer rule).
(3) Every inference rule below Γ′ →∆′ is a quantiﬁer rule.
Thus, in such a proof in normal form, the midsequent splits the proof
tree into an upper part that contains the propositional inferences (and weak-
enings), and a lower part that contains the quantiﬁer inferences.
Proof : First we apply theorem 6.4.1 to obtain a cut-free G1nnf-proof.
Next, we use induction as in theorem 7.3.1. However, there is a new diﬃculty:
Exchanges involving contractions introduce two propositional rules and some-
times a weakening above a quantiﬁer rule, and a simple induction on m(T)
does not work. This diﬃculty can be resolved as follows. Let us assign the
letter a to a quantiﬁer rule, and the letter b to a propositional rule or a weak-
ening. Consider the set of all strings obtained by tracing the branches of the
proof tree from the root, and concatenating the letters corresponding to the
quantiﬁer and other inferences in the proof tree. The fact that a propositional
inference (or a weakening) occurs below a quantiﬁer inference is indicated by
an occurrence of the substring ba. Note that every string obtained is of the
form
an0bm1an1bm2an2...bmlanlbml+1.

7.4 The Sharpened Hauptsatz for Sequents in NNF
335
By inspection of the exchanges given by lemma 7.4.2, note that a string
ba is replaced either by ab or abb or abbb. From this, it is easily seen that bma
is replaced by some string abk, where m ≤k ≤3m. Hence, bman is replaced
by some string anbmk, where 1 ≤k ≤3n.
To permute quantiﬁer rules and propositional rules, proceed as follows:
Consider all lowest occurrences of inferences of the form bm1an1, and perform
the exchanges using lemma 7.4.2. After this step is completed, each path
an0bm1an1bm2an2...bmlanlbml+1
is now of the form
an0an1bp1bm2an2...bmlanlbml+1.
Since the number of blocks of the form bmiani has decreased, if we repeat
the above process, we will eventually obtain a proof in which all quantiﬁer
inferences are below all propositional inferences and weakenings. When such
a proof is obtained, we pick the premise of the highest quantiﬁer inference.
If this sequent M only contains quantiﬁer-free formulae, it is the midsequent.
Otherwise, since the axioms only contain literals and no quantiﬁer rule is ap-
plied in the part of the proof above the sequent M, all quantiﬁed subformulae
in M are introduced by weakening. As in the proof of theorem 7.3.1, we can
consider the part of the proof above M, and form another proof of the sequent
M ′ obtained from M by deleting all quantiﬁed formulae. It is now possible
(as in theorem 7.3.1) to form a quantiﬁer-free sequent Γ′ →∆′ from M ′ which
is also provable, from which M is provable by applying the quantiﬁer rules of
G2nnf.
Remarks: (i) The theorem deals with the two systems G1nnf and G2nnf,
whereas theorem 7.3.1 deals with the single system LK. Theorem 7.4.1 shows
how to convert a G1nnf-proof with cut into a cut-free normal G2nnf-proof.
This is easier to prove than the extended Hauptsatz for G2nnf. Also, this is
suﬃcient to derive part of Herbrand’s theorem for G1nnf.
(ii) The resulting proof may have a depth exponential in the size of the
original proof.
As an illustation of theorem 7.4.1, consider the following example.
EXAMPLE 7.4.4
The following is a proof of the sequent
(P(a) ∨∀x(Q(x) ∨∀yR(y))) →P(a), Q(a), R(f(a)) ∧R(f(b))
which is not in normal form:
P(a) →P(a)
P(a) →P(a), Q(a)
P(a) →P(a), Q(a), R(f(a)) ∧R(f(b))
T1
(P(a) ∨∀x(Q(x) ∨∀yR(y))) →P(a), Q(a), R(f(a)) ∧R(f(b))

336
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
where T1 is the tree
Q(a) →Q(a)
Q(a) →Q(a), R(f(a)) ∧R(f(b))
T2
Q(a) ∨∀yR(y) →Q(a), R(f(a)) ∧R(f(b))
∀x(Q(x) ∨∀yR(y)) →Q(a), R(f(a)) ∧R(f(b))
∀x(Q(x) ∨∀yR(y)) →P(a), Q(a), R(f(a)) ∧R(f(b))
and T2 is the tree
R(f(a)) →R(f(a))
∀yR(y) →R(f(a))
R(f(b)) →R(f(b))
∀yR(y) →R(f(b))
∀yR(y) →R(f(a)) ∧R(f(b))
∀yR(y) →Q(a), R(f(a)) ∧R(f(b))
The proof below is in normal form:
propositional part
P(a) ∨(Q(a) ∨R(f(a))), P(a) ∨(Q(a) ∨R(f(b))) →∆
P(a) ∨(Q(a) ∨∀yR(y)), P(a) ∨(Q(a) ∨R(f(a))) →∆
P(a) ∨(Q(a) ∨∀yR(y)) →P(a), Q(a), R(f(a)) ∧R(f(b))
(P(a) ∨∀x(Q(x) ∨∀yR(y))) →P(a), Q(a), R(f(a)) ∧R(f(b))
where ∆= P(a), Q(a), R(f(a)) ∧R(f(b)). The midsequent is
P(a) ∨(Q(a) ∨R(f(a))), P(a) ∨(Q(a) ∨R(f(b))) →
P(a), Q(a), R(f(a)) ∧R(f(b)),
which is equivalent to
(P(a) ∨(Q(a)) ∨(R(f(a)) ∧R(f(b))) →
P(a), Q(a), R(f(a)) ∧R(f(b)).
We now consider the case of languages with equality.
7.4.4 The Gentzen System G2nnf
=
The system G2nnf
=
has the following axioms and inference rules.

PROBLEMS
337
Deﬁnition 7.4.4
The system G2nnf
=
is obtained from G1nnf
=
by adding the
quantiﬁer rules of deﬁnition 7.4.3 and the weakening rules. It is easy to see
that G2nnf
=
is sound.
The following version of lemma 7.4.2 holds for G2nnf
=
.
Lemma 7.4.3
For every pure-variable proof T in G2nnf
=
without essential
cuts, the following holds: Every quantiﬁer rule of G2nnf
=
can be permuted
with a propositional rule, an inessential cut, or a weakening.
Proof : The case not handled in lemma 7.4.2 is an exchange with an
inessential cut. This is handled as in lemma 7.3.3.
7.4.5 A Gentzen-like Sharpened Hauptsatz for G2nnf
=
We also have the following sharpened Hauptsatz for G2nnf
=
.
Theorem 7.4.2
(A sharpened Hauptsatz for G1nnf
=
and G2nnf
=
) Given a
sequent Γ →∆containing only formulae in NNF, if Γ →∆is provable in
G1nnf
=
, then there is a pure-variable proof in G2nnf
=
without essential cuts that
contains a sequent Γ′ →∆′ (called the midsequent), which has the following
properties:
(1) Every formula in Γ′ →∆′ is quantiﬁer free.
(2) Every inference rule above Γ′ →∆′ is either a weakening, a propo-
sitional rule, or an inessential cut (but not a quantiﬁer rule).
(3) Every inference rule below Γ′ →∆′ is a quantiﬁer rule.
Proof : The proof is essentially identical to that of theorem 7.4.1, but
using lemma 7.4.3 instead of lemma 7.4.2. The details are left as an exercise.
We shall see in Section 7.6 how theorems 7.4.1 and 7.4.2 can be used to
yield Andrews’s version of the Skolem-Herbrand-G¨odel theorem (See Andrews,
1981), and also half of a version of Herbrand’s original theorem.
PROBLEMS
7.4.1.
Finish the proof of lemma 7.4.1.
7.4.2.
Finish the proof of the cases left out in the proof of lemma 7.4.2
7.4.3.
Prove that lemma 7.3.1 and 7.3.2 extend to the system G2nnf.
7.4.4.
Fill in the details in the proof of theorem 7.4.1.
7.4.5.
Fill in the details in the proof of lemma 7.4.3.
7.4.6.
Fill in the details in the proof of theorem 7.4.2.

338
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.4.7.
Give a proof in normal form for the sequent
∀x(P(x) ∨∀yQ(y, f(x))) ∧(¬P(a) ∧(¬Q(a, f(a)) ∨¬Q(b, f(a))) →.
7.4.8.
It is tempting to formulate the ∀: left rule so that a formula of the
form ∀xA can be instantiated to the formula A[t1/x] ∧... ∧A[tk/x],
for any k terms t1, ..., tk free for x in A, and never apply contractions.
However, this does not work.
Indeed, the resulting system is not
complete. Consider the following sequent provided by Dale Miller:
∀x∃y(¬P(x) ∧P(y)) →.
The following is a proof of the above sequent using contractions:
P(u) →¬P(x), P(u)), P(v)
¬P(x), P(u), ¬P(u), P(v) →
¬P(x), P(u), (¬P(u) ∧P(v)) →
(¬P(x) ∧P(u)), (¬P(u) ∧P(v)) →
(¬P(x) ∧P(u)), ∃y(¬P(u) ∧P(y)) →
(¬P(x) ∧P(u)), ∀x∃y(¬P(x) ∧P(y)) →
∃y(¬P(x) ∧P(y)), ∀x∃y(¬P(x) ∧P(y)) →
∀x∃y(¬P(x) ∧P(y)), ∀x∃y(¬P(x) ∧P(y)) →
∀x∃y(¬P(x) ∧P(y)) →
Show that a derivation involving no contractions cannot lead to a
proof tree, due to the eigenvariable restriction.
7.5 Herbrand’s Theorem for Prenex Formulae
In this section, we shall derive a constructive version of Herbrand’s theorem
from Gentzen’s Hauptsatz, using a method inspired by Kleene (Kleene, 1967).
In presenting Herbrand’s theorem, it is convenient to assume that we
are dealing with sentences.
7.5.1 Preliminaries
The following lemma shows that there is no loss of generality in doing so.

7.5 Herbrand’s Theorem for Prenex Formulae
339
Lemma 7.5.1
Let A be a rectiﬁed formula, and let FV (A) = {y1, ..., yn}
be its set of free variables. The sequent →A is (LK or LKe) provable if and
only if the sequent →∀y1...∀ynA is provable.
Proof : We proceed by induction on the number of free variables. The
induction step consists in showing that if A is a rectiﬁed formula and has a
single free variable x, then →A is provable iﬀ→∀xA is provable.
We can appeal to the completeness theorem and show that A is valid iﬀ
∀xA is valid, which is straightforward. We can also give a the following proof
using the pure-variable lemma 7.3.1 and lemma 6.5.1.
Assume that →A is provable. Since A is rectiﬁed, the variable x is not
free in ∀xA, and the following inference is valid.
→A[x/x]
→∀xA
By putting the proof of A = A[x/x] on top of this inference, we have a
proof for ∀xA.
Conversely, assume that ∀xA is provable. There is a minor problem,
which is that the eigenvariable z used in the lowest inference in the proof of
∀xA is not necessarily x, even though x is not free in ∀xA. However, by lemma
7.3.1, there is a pure-variable proof of A in which all eigenvariables are new
and distinct. Hence, the variable x does not occur in the part of the proof
above →A[z/x]. Using lemma 6.5.1, we can substitute x for all occurrences
of z in this part of the proof, and we get a proof of A.
We will also use the following fact which is easily shown: Given a sequent
A1, ..., Am →B1, ..., Bn,
A1, ..., Am →B1, ..., Bn
is provable if and only if
→¬A1 ∨... ∨¬Am ∨B1 ∨... ∨Bn
is provable. Using this fact and lemma 7.5.1, a sequent consisting of formulae
is provable if and only if a sentence is provable.
Hence, we will assume without loss of generality that the sequent Γ to be
proved does not have sentences on the left of →, that the sentences occurring
on the righthand side of →are all distinct, and that they are rectiﬁed.
The main idea of Herbrand’s theorem is to encode (with some inessential
loss of information) the steps of the proof in which the ∀: rule is applied.
For this, some new function symbols called Herbrand functions or Skolem
functions are introduced.

340
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.5.2 Skolem Function and Constant Symbols
Skolem symbols are deﬁned as follows.
Deﬁnition 7.5.1
For every prenex formula A = Qnxn...Q1x1B occurring
in a sequent →Γ, for every occurrence Qi of a universal quantiﬁer in A then:
(i) If n > 0 and mi > 0, where mi is the number of existential quanti-
ﬁers in the string Qn...Qi+1, a new function symbol f A
i
having a number of
arguments equal to mi is created.
(ii) If mi = 0 or i = n, the constant symbol f A
i
is created.
Such symbols are called Skolem function symbols and Skolem constant
symbols (for short, Skolem functions).
The essence of Herbrand’s theorem can be illustrated using example
7.3.4.
EXAMPLE 7.5.1
For the sequent of example 7.3.4, the unary function symbol f is associ-
ated with ∀in ∃x∀y¬P(x, y), the unary function symbol g is associated
with ∀in ∃y1∀z¬Q(y1, z), and the constant a with ∀in ∀x1∃y2∃z1(P(x1,
y2) ∧Q(y2, z1)).
Now, we shall perform alterations to the proof in example 7.3.4. Moving
up from the bottom sequent, instead of performing the ∀: right rules, we
are going to perform certain substitutions. For the instance of ∀: right
applied to
∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1)),
substitute the Skolem constant a for all occurrences of x1 in the proof;
for the instance of ∀: right applied to
∀y¬P(a, y),
substitute f(a) for all occurrences of y in the proof; for the instance of
∀: right applied to
∀z¬Q(f(a), z),
substitute g(f(a)) for all occurrences of z in the proof. Note that the
resulting tree is no longer a legal proof tree because the applications of
∀: right rules have been spoiled. However, the part of the proof from
the new midsequent up is still a valid proof involving only propositional
(and structural) rules (extending our original language with the symbols
a, f, g).
The following deﬁnition will be useful for stating Herbrand’s theorem.
Deﬁnition 7.5.2
Given a prenex sequent →Γ, the functional form of →
Γ is obtained as follows: For each prenex formula A = Qnxn...Q1x1B in

7.5 Herbrand’s Theorem for Prenex Formulae
341
→Γ, for each occurrence of a variable xi bound by an occurrence Qi of a
universal quantiﬁer, the term f A
i (y1, ..., ym) is substituted for xi in B and
Qixi is deleted from A, where f A
i
is the Skolem function symbol associated
with Qi, and y1, ..., ym is the list of variables (in that order) bound by the
existential quantiﬁers occurring in the string Qn...Qi+1 (if m = 0 or i = n,
f A
i
is a constant symbol).
EXAMPLE 7.5.2
Again, referring to example 7.3.4, the sequent
→∃x¬P(x, f(x)), ∃y1¬Q(y1, g(y1)), ∃y2∃z1(P(a, y2) ∧Q(y2, z1))
is the functional form of our original sequent. Note that the provable
sequent
→¬P(a, f(a)), ¬Q(f(a), g(f(a))), P(a, f(a)) ∧Q(f(a), g(f(a)))
is obtained from the functional form of the original sequent by deleting
quantiﬁers and substituting the terms a, f(a), f(a) and g(f(a)) for x,
y1, y2, and z1 respectively. Example 7.5.2 illustrates part of Herbrand’s
theorem.
Informal statement of Herbrand’s theorem: If a prenex sequent →Γ is
provable, then a disjunction of quantiﬁer-free formulae constructible from Γ
is provable. Furthermore, this disjunction of quantiﬁer-free formulae consists
of formulae obtained by substituting ground terms (built up from the original
language extended with Skolem functions) for the bound variables of the sen-
tences occurring in the functional form of the original sequent. The converse
is also true, as we shall see shortly.
The key observation used in showing the converse of the above statement
is to notice that the terms a, f(a), g(f(a)) record the order in which the
∀: right rules were applied in the original proof. The more nested the symbol,
the earlier the rule was applied.
Warning: Many authors introduce Skolem function symbols to eliminate
existential quantiﬁers, and not universal quantiﬁers as we do. This may seem
confusing to readers who have seen this other deﬁnition of Skolem function
symbols. However, there is nothing wrong with our approach. The reason the
dual deﬁnition is also used (eliminating existential quantiﬁers using Skolem
function symbols), as in the resolution method, is that the dual approach
consists in showing that a formula A is valid by showing that B = ¬A is
unsatisﬁable. This is equivalent to showing that the sequent B →is valid, or
equivalently that →¬B (that is →A) is valid. Since in the dual approach,
the existential quantiﬁers in B are Skolemized, in ¬B = A, the universal
quantiﬁers are Skolemized. Since our approach consists in showing directly
that A is valid, and not that ¬A is unsatisﬁable, we have deﬁned Skolem
function symbols for that purpose, and this is why they are used to eliminate

342
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
universal quantiﬁers. What we have deﬁned in deﬁnition 7.5.2 is often called
in the literature the validity functional form, the dual form for eliminating
existential quantiﬁers being called the satisﬁability functional form (Herbrand,
1971). To avoid confusion, we shall call the ﬁrst (the validity functional form)
simply the functional form, and the second (the satisﬁability functional form)
the Skolem normal form (see deﬁnition 7.6.2).
7.5.3 Substitutions
Before stating and proving Herbrand’s theorem, we need to deﬁne substitution
functions.
Deﬁnition 7.5.3
A substitution function (for short, a substitution) is any
function
σ : V →TERML
assigning terms to the variables in V. By theorem 2.4.1, there is a unique
homomorphism
σ : TERML →TERML
extending σ and deﬁned recursively as follows:
For every variable x ∈V, σ(x) = σ(x).
For every constant c, σ(c) = σ(c).
For every term ft1...tn ∈TERML,
σ(ft1...tn) = fσ(t1)...σ(tn).
By abuse of language and notation, the function σ will also be called a
substitution, and will often be denoted by σ.
The subset X of V consisting of the variables such that σ(x) ̸= x is
called the support of the substitution. In what follows, we will be dealing
with substitutions of ﬁnite support. If a substitution σ has ﬁnite support
{y1, ..., yn} and σ(yi) = si, for i = 1, .., n, for any term t, σ(t) is also denoted
by t[s1/y1, ..., sn/yn].
Substitutions can be extended to formulae as follows. Let A be a for-
mula, and let σ be a substitution with ﬁnite support {x1, ..., xn}. Assume
that the variables in {x1, ..., xn} are free in A and do not occur bound in A,
and that each si (si = σ(xi)) is free for xi in A. The substitution instance
A[s1/x1, ..., sn/xn] is deﬁned recursively as follows:
If A is an atomic formula of the form Pt1...tm, then
A[s1/x1, ..., sn/xn] = Pt1[s1/x1, ..., sn/xn]...tm[s1/x1, ..., sn/xn].

7.5 Herbrand’s Theorem for Prenex Formulae
343
If A is an atomic formula of the form (t1 .= t2), then
A[s1/x1, ..., sn/xn] = (t1[s1/x1, ..., sn/xn] .= t2[s1/x1, ..., sn/xn]).
If A =⊥, then
A[s1/x1, ..., sn/xn] =⊥.
If A = ¬B, then
A[s1/x1, ..., sn/xn] = ¬B[s1/x1, ..., sn/xn].
If A = (B ∗C), where ∗∈{∨, ∧, ⊃, ≡}, then
A[s1/x1, ..., sn/xn] = (B[s1/x1, ..., sn/xn] ∗C[s1/x1, ..., sn/xn]).
If A = ∀yB (where y /∈{x1, ..., xn}), then
A[s1/x1, ..., sn/xn] = ∀yB[s1/x1, ..., sn/xn].
If A = ∃yB (where y /∈{x1, ..., xn}), then
A[s1/x1, ..., sn/xn] = ∃yB[s1/x1, ..., sn/xn].
Remark: A[s1/x1][s2/x2]...[sn/xn], the result of substituting s1 for x1,
... ,sn for xn (as deﬁned in deﬁnition 5.2.6) in that order, is usually diﬀerent
from A[s1/xs, ..., sn/xn]. This is because the terms s1,...,sn may contain some
of the variables in {x1, ..., xn}. However, if none of the variables in the support
of the substitution σ occurs in the terms s1,...,sn, it is easy to see that the
order in which the substitutions are performed is irrelevant, and in this case,
A[s1/x1][s2/x2]...[sn/xn] = A[s1/x1, ..., sn/xn].
In particular, this is the case when σ is a ground substitution, that is, when
the terms s1,...,sn do not contain variables.
Given a formula A and a substitution σ as above, the pair (A, σ) is called
a substitution pair. The substitution instance deﬁned by A and σ as above
is also denoted by σ(A). Notice that distinct substitution pairs can yield the
same substitution instance.
A minor technicality has to be taken care of before proving Herbrand’s
theorem. If the ﬁrst-order language does not have any constants, the theorem
fails. For example, the sequent →∃x∃y(P(x) ∨¬P(y)) has a proof whose
midsequent is →P(x) ∨¬P(x), but if the language has no constants, we
cannot ﬁnd a ground substitution instance of P(x) ∨¬P(x) that is valid. To

344
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
avoid this problem, we will assume that if any ﬁrst-order language L does not
have constants, the special constant # is added to it.
7.5.4 Herbrand’s Theorem for Prenex Formulae
Before stating and proving Herbrand’s theorem, recall that we can assume
without loss of generality that the sequent →Γ to be proved consists of sen-
tences, does not have sentences on the left of →, that the sentences occurring
on the righthand side of →are all distinct, and that distinct quantiﬁers bind
occurrences of distinct variables.
Theorem 7.5.1 (Herbrand’s theorem for prenex formulae) Given a sequent
→Γ such that all sentences in Γ are prenex, →Γ is provable (in LK or
LKe) if and only if there is some ﬁnite sequence < (B1, σ1), ..., (BN, σN) > of
substitution pairs such that the sequent →H consisting of the substitution
instances σ1(B1), ..., σN(BN) is provable, where each σi(Bi) is a quantiﬁer-
free substitution instance constructible from the sentences in Γ (H is called
a Herbrand disjunction). Furthermore, each quantiﬁer-free formula σi(Bi) in
the Herbrand disjunction H is a substitution instance Bi[t1/x1, ..., tk/xk] of
a quantiﬁer-free formula Bi, matrix of some sentence ∃x1...∃xkBi occurring
in the functional form of →Γ. The terms t1, ..., tk are ground terms over
the language consisting of the function and constant symbols in L occurring
in →Γ, and the Skolem function (and constant) symbols occurring in the
functional form of →Γ.
Proof : Using Gentzen’s sharpened Hauptsatz (theorem 7.3.1), we can
assume that we have a pure-variable cut-free proof in LK (proof without
essential cuts in LKe) with midsequent →Γ′.
We alter this proof in the
following way. Starting with the bottom sequent and moving up, for every
instance of the rule ∀: right applied to a formula
∀xiQi−1xi−1...Q1x1B[s1/y1, ..., sm/ym]
which has been obtained from a prenex sentence
A = Qnxn...Qi+1xi+1∀xiQi−1xi−1...Q1x1C
in Γ, substitute the term
f A
i (s1, ..., sm)
for all occurrences of xi in the proof (or Skolem constant f A
i if m = 0), where
f A
i
is the Skolem function symbol associated with xi in A, y1, ..., ym is the
list of all the variables (all distinct by our hypothesis on proofs) bound by
existential quantiﬁers in the string Qnxn...Qi+1xi+1, and s1,..., sm is the list
of terms that have been substituted for y1, ..., ym in previous steps. Since the
only inference rules used below the midsequent are quantiﬁer, contraction or
exchange rules, the resulting midsequent →H is indeed composed of substi-
tutions instances of matrices of sentences occurring in the functional form of

7.5 Herbrand’s Theorem for Prenex Formulae
345
→Γ, and since the modiﬁed part of the proof above the midsequent is still
a proof, the new midsequent →H is provable. If the midsequent contains
variables, substitute any constant for all of these variables (by a previous as-
sumption, the language has at least one constant, perhaps the special constant
#).
We now prove the converse of the theorem. We can assume without
loss of generality that the disjuncts are distinct, since if they were not, we
could suppress the duplications and still have a provable disjunct.
Let <
(B1, σ1), ..., (BN, σN) > be a sequence of distinct substitution pairs where
each Bi is the matrix of the functional form of some sentence in →Γ, let
H be the corresponding sequence of substitution instances and assume that
→H is provable. Notice that the conditions assumed before the statement
of theorem 7.5.1 guarantee that every substitution pair (B, σ) corresponds to
the unique pair (Qnxn...Q1x1C, σ), where ∃y1...∃ymB is the functional form
of some sentence Qnxn...Q1x1C in →Γ, and σ is a substitution with support
{y1, ..., ym}.
Given the prenex sentence A = Qnxn...Q1x1C in →Γ, its functional
form is a sentence of the form
∃y1...∃ymC[r1/z1, ..., rp/zp],
where the union of {y1, ..., ym} and {z1, ..., zp} is {x1, ..., xn}, and {z1, ..., zp}
is the set of variables which are universally quantiﬁed in A. Each term ri is
rooted with the Skolem function symbol f A
i . Consider the set HT composed
of all terms of the form
ri[s1/y1, ..., sm/ym]
occurring in the Herbrand disjunction →H, where ri is a Skolem term asso-
ciated with an occurrence of a universal quantiﬁer in some prenex sentence A
in Γ, and s1,...,sm are the terms deﬁning the substitution σ involved in the
substitution pair (C[r1/z1, ..., rp/zp], σ).
We deﬁne a partial order on the set HT as follows: For every term in
HT of the form f A
i (t1, ..., tm), every subterm of tj ∈HT (possibly tj itself,
1 ≤i ≤m) precedes f A
i (t1, ..., tm);
Every term in HT rooted with f A
i precedes any term in HT rooted with
f A
j if i > j.
In order to reconstruct a proof, we will have to eliminate the Skolem
symbols and perform ∀: right rules with new variables. For this, we set up
a bijection v between the set HT and a set of new variables not occurring
in Γ. For every term t occurring in the Herbrand disjunction H (not only
terms in HT), let t be the result of substituting the variable v(s) for every
maximal subterm s ∈HT of t (a maximal subterm in HT of t is a subterm
of t in HT, which is not a proper subterm of any other subterm in HT of t).
Let < (B′
1, σ′
1), ..., (B′
N, σ′
N) > be the list of substitution pairs obtained from

346
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
< (B1, σ1), ..., (BN, σN) > by replacing each term f Ai
j (y1, ..., ym) occurring in
Bi (where Ai is the sentence in Γ whose functional form is ∃y1...∃ymBi) by
v(σi(f Ai
j (y1, ..., ym))), and each term s involved in deﬁning the substitution σi
by s (B′
i is actually a substitution instance of the matrix of a prenex formula
in Γ, where the substitution is a bijection). Let →H′ be the resulting sequent
of substitution instances.
We are going to show that a deduction of →Γ can be constructed from
→H′ (really < (B′
1, σ′
1), ..., (B′
N, σ′
N) >) and the partially ordered set HT.
First, notice that since →H is provable in the propositional part of LK
(or LKe), →H′ is also provable since the above substitutions do not aﬀect
inferences used in the proof of →H. The following deﬁnition will be needed.
Deﬁnition 7.5.4
Given a substitution pair (A, σ), where A is a prenex
formula of the form Qnxn...Q1x1C (n ≥1) occurring in the sequent →Γ and
σ is a substitution with support the subset of variables in {x1, ..., xn} bound
by existential quantiﬁers, the σ-matrix of the functional form of A up to i
(0 ≤i ≤n) is deﬁned inductively as follows:
For i = 0, the σ-matrix of the functional form of A up to 0 is the substitu-
tion instance of C obtained by substituting the new variable v(σ(f A
j (y1, ..., ym)
)) for xj in C, for each occurrence of a variable xj bound by a universal quan-
tiﬁer in A.
For 0 ≤i ≤n −1:
(i) If Qi+1 = ∃and if the σ-matrix of the functional form of A up to
i is B, the σ-matrix of the functional form of A up to i + 1 is ∃xi+1B;
(ii) If Qi+1 = ∀, if the σ-matrix of the functional form of A up to i is
B[v(σ(f A
i+1(y1, ..., ym)))/xi+1], where the set of variables bound by existential
quantiﬁers in Qnxn...Qi+2xi+2 is {y1, ..., ym}, the σ-matrix of the functional
form of A up to i + 1 is ∀xi+1B.
Note that the σ-matrix of the functional form of A up to n is A itself.
Next, we deﬁne inductively a sequence Π of lists of substitution pairs
< (B1, σ1), ..., (Bp, σp) >, such that the tree of sequents →σ1(B1), ..., σp(Bp)
is a deduction of →Γ from →H′.
During the construction of Π, terms
will be deleted from HT, eventually emptying it. Each Bj in a pair (Bj, σj)
is the σ-matrix of the functional form up to some i of some sentence A =
Qnxn...Qixi...Q1x1C in →Γ, where σ is one of the original substitutions in
the list < (B′
1, σ′
1), ..., (B′
N, σ′
N) >. Each σj is a substitution whose support is
the set of variables in {xn, ..., xi+1} which are bound by existential quantiﬁers
in A.
The ﬁrst element of Π is
< (B′
1, σ′
1), ..., (B′
N, σ′
N) > .

7.5 Herbrand’s Theorem for Prenex Formulae
347
If the last element of the list Π constructed so far is < (B1, σ1), ..., (Bp, σp) >
and the corresponding sequent of substitution instances is →∆, the next list
of substitution pairs is determined as follows:
If ∆diﬀers from Γ and no formula Bj for some pair (Bj, σj) in the list
< (B1, σ1), ..., (Bp, σp) > is the σ-matrix of the functional form up to i of
some sentence
A = Qnxn...∃xi+1Qixi...Q1x1C
occurring in →Γ, select the leftmost formula
Bj = Qixi...Q1x1B[v(f A
i+1(s1, ..., sm))/xi+1]
which is the σ-matrix of the functional form up to i of some sentence
A = Qnxn...∀xi+1Qixi...Q1x1C
in →Γ, and for which the variable v(f A
i+1(s1, ..., sm)) corresponds to a maxi-
mal term in (the current) HT. Then, apply the ∀: right rule to v(f A
i+1(s1, ...,
sm)), obtaining the sequent →∆′ in which
σj(∀xi+1Qixi...Q1x1B)
replaces
σj(Qixi...Q1x1B[v(f A
i+1(s1, ..., sm))/xi+1]).
At the end of this step, delete f A
i+1(s1, ..., sm) from HT, and perform con-
tractions (and exchanges) if possible. The new list of substitution pairs is
obtained by ﬁrst replacing
(Bj, σj)
by
(∀xi+1Qixi...Q1x1B, σj),
and performing the contractions and exchanges speciﬁed above.
Otherwise, there is a formula
Bj = Qixi...Q1x1B
which is the σ-matrix of the functional form up to i of some sentence
A = Qnxn...∃xi+1Qixi...Q1x1C
in →Γ. Then, apply the ∃: right rule to the leftmost substitution instance
σj(Bj)
in →∆of such a formula. The conclusion of this inference is the sequent
→∆′ in which
σ′
j(∃xi+1Qixi...Q1x1B)

348
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
replaces
σj(Qixi...Q1x1B),
where σ′
j is the restriction of the substitution σj obtained by eliminating xi+1
from the support of σj (Hence, σ′
j(xi+1) = xi+1). The pair
(∃xi+1Qixi...Q1x1, σ′
j)
replaces the pair
(Bj, σj)
in the list of substitution pairs. After this step, perform contractions (and
exchanges) if possible. Note that in this step, no term is deleted from HT.
Repeat this process until a list of substitution pairs < (B1, σ1), ..., (Bp,
σp) > is obtained, such that every substitution σj has empty support and
→B1, ..., Bp is the sequent →Γ.
We claim that Π deﬁnes a deduction of →Γ from →H′. First, it is easy
to see that the sequence Π ends with the sequent →Γ, since we started with
substitution instances of matrices of functional forms of sentences in →Γ,
and since every step brings some formula in ∆“closer” to the corresponding
formula in Γ. We leave the details as an exercise.
To show that the eigenvariable condition is satisﬁed for every application
of the ∀: right rule, we show the following claim by induction on the number
of ∀: right steps in Π.
Claim: Just before any application of a ∀: right rule, the set of terms
of the form f A
i (s1, ..., sm) such that v(f A
i (s1, ..., sm)) occurs (free) in ∆is the
current set HT, and for every maximal term f A
i (s1, ..., sm) ∈HT, the variable
v(f A
i (s1, ..., sm)) occurs free in at most one formula in ∆of the form
Qi−1xi−1...Q1x1B[v(f A
i (s1, ..., sm))/xi].
Proof of claim: Just before the ﬁrst ∀: right step, since all the formulae
in →∆are of the form ∃xi−1...∃x1B, and since the formulae in →H are
substitution instances of matrices of sentences occurring in the functional
form of →Γ, it is clear that the set of terms of the form f A
i (s1, ..., sm) such
that v(f A
i (s1, ..., sm)) occurs in →∆is the initial set HT.
Since a term
f A
i (s1, ..., sm) is maximal in HT if and only if it corresponds to the rightmost
occurrence of a universal quantiﬁer in A, v(f A
i (s1, ..., sm)) occurs free at most
in a single formula
∃xi−1...∃x1B[v(f A
i (s1, ..., sm))/xi]
(substitution instance of the σ-matrix of the functional form up to i−1 of the
sentence A = Qnxn...∀xi∃xi−1...∃x1C in →Γ). Next, assuming the induction
hypothesis, let →∆1 be the sequent and HT1 the set of terms just before

7.5 Herbrand’s Theorem for Prenex Formulae
349
an application of a ∀: right step, and →∆2 be the sequent and HT2 the
set of terms just before the next ∀: right step.
Since the maximal term
f A
i (s1, ..., sm) is deleted from HT1 during the ∀: right step applied to →∆1,
and since the following steps until the next ∀: right step are ∃: right rules
which do not aﬀect HT1 −{f A
i (s1, ..., sm)},
HT2 = HT1 −{f A
i (s1, ..., sm)}.
Since a term f A
i (s1, ..., sm) is maximal in HT2 if and only if it corresponds
to the rightmost occurrence of a universal quantiﬁer in the preﬁx Qnxn...
Qi+1xi+1∀xi of the formula
A = Qnxn...Qi+1xi+1∀xiQi−1xi−1...Q1x1C
in →Γ, it must correspond to ∀xi.
If the variable v(f A
i (s1, ..., sm)) oc-
curs free in some other formula Rj−1xj−1...R1x1B′ in →∆2, since Rj = ∀,
v(f A
i (s1, ..., sm)) occurs within a term of the form f A′
j (s′
1, ..., s′
q), contradict-
ing the maximality of f A
i (s1, ..., sm), since f A′
j (s′
1, ..., s′
q) is also in HT2 (as
a result of the induction hypothesis). Therefore, v(f A
i (s1, ..., sm)) may only
occur free in the formula
Qi−1xi−1...Q1x1B[v(f A
i (s1, ..., sm))/xi]
in →∆2, substitution instance of the σ-matrix of the functional form up to
i −1 of the formula
A = Qnxn...∀xiQi−1xi−1...Q1x1C
in →Γ. Hence, the eigenvariable condition is satisﬁed. In a ∃: step, since
the variables occurring in the term s are distinct from the variables occurring
bound in the formulae in Γ, the term s is free for xi in the substitution, and
the inference is valid. Hence, Π yields a deduction of →Γ from →H′, which
can be extended to a proof of →Γ from axioms, since →H′ is provable. This
concludes the proof of Herbrand’s theorem.
The method for reconstructing a proof from a list of substitution pairs
is illustrated in the following example.
EXAMPLE 7.5.3
Consider the sequent →Γ given by:
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1)),
whose functional form is:
→∃x¬P(x, f(x)), ∃y1¬Q(y1, g(y1)), ∃y2∃z1(P(a, y2) ∧Q(y2, z1)).

350
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
The provable sequent →H given by
→¬P(a, f(a)), ¬Q(f(a), g(f(a))), P(a, f(a)) ∧Q(f(a), g(f(a)))
is obtained from the functional form of the original sequent by deleting
quantiﬁers and substituting the terms a, f(a), f(a) and g(f(a)) for x,
y1, y2, and z1 respectively. We have HT = {a, f(a), g(f(a))}, with the
ordering a < f(a), a < g(f(a)), f(a) < g(f(a)).
Deﬁne the bijection v′ such that v′(g(f(a))) = u, v′(f(a)) = v and
v′(a) = w. The result of replacing in →H the maximal terms in HT
by the variables given by v′ is the sequent →H′ given by
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u).
The formula P(w, v) ∧Q(v, u) is the σ-matrix of the functional form up
to 0 of the formula ∀w∃v∃u(P(w, v)∧Q(v, u)). Hence we have a ∃: right
step.
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, v) ∧Q(v, z1))
Similarly, ∃z1(P(w, v) ∧Q(v, z1)) is the σ-matrix of the functional form
up to 1 of ∀w∃v∃z1(P(w, v)∧Q(v, z1)). Hence, we have another ∃: right
step.
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
Now, only a ∀: right step can be applied. According to the algorithm,
we apply it to the leftmost formula for which the variable v′(t) cor-
responds to a maximal term t ∈HT.
This must be ¬Q(v, u), since
v′(g(f(a)) = u and g(f(a)) is the largest element of HT. We also delete
g(f(a)) from HT.
Note that it would be wrong to apply the ∀: right rule to any of the
other formulae, since both w and v would occur free in the conclusion
of that inference.
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∀z¬Q(v, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))

7.5 Herbrand’s Theorem for Prenex Formulae
351
Now, we can apply a ∃: right step to ∀z¬Q(v, z).
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∀z¬Q(v, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
At this point, a ∀: right step is the only possibility. Since the next
largest term in HT = {a, f(a)} is f(a), we apply it to ¬P(w, v).
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∀z¬Q(v, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∀y¬P(w, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
We can now apply a ∃: right step to ∀y¬P(w, y).
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∀z¬Q(v, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∀y¬P(w, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
Finally, since HT = {a} and only a ∀: right step is possible, a ∀: right
step is applied to ∃y2∃z1(P(w, y2) ∧Q(y2, z1)).

352
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
→¬P(w, v), ¬Q(v, u), P(w, v) ∧Q(v, u)
→¬P(w, v), ¬Q(v, u), ∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ¬Q(v, u), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∀z¬Q(v, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→¬P(w, v), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∀y¬P(w, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∃y2∃z1(P(w, y2) ∧Q(y2, z1))
→∃x∀y¬P(x, y), ∃y1∀z¬Q(y1, z), ∀x1∃y2∃z1(P(x1, y2) ∧Q(y2, z1))
This last derivation is a deduction of →Γ from →H′. Observe that
this proof is identical to the proof of example 7.3.4.
Remarks: (1) The diﬀerence between ﬁrst-order logic without equality
and ﬁrst-order logic with equality noted in the paragraph following the proof of
theorem 5.6.1 shows up again in Herbrand’s theorem. For a language without
equality, in view of the second corollary to theorem 5.5.1, the hard part in
ﬁnding a proof is to ﬁnd appropriate substitutions yielding a valid Herbrand
disjunction.
Indeed, as soon as such a quantiﬁer-free formula is obtained,
there is an algorithm for deciding whether it is provable (or valid). However,
in view of the remark following the corollary, Church’s theorem implies that
there is no algorithm for ﬁnding these appropriate substitutions.
For languages with equality, the situation is worse! Indeed, even if we
can ﬁnd appropriate substitutions yielding a quantiﬁer-free formula, we are
still facing the problem of ﬁnding an algorithm for deciding the provability (or
validity) of quantiﬁer-free formulae if equality is present. As we mentioned
in Chapter 5, there is such an algorithm presented in Chapter 10, but it is
nontrivial. Hence, it appears that automatic theorem proving in the presence
of equality is harder than automatic theorem proving without equality. This
phenomenon will show up again in the resolution method.
(2) Note that the last part of the proof of theorem 7.5.1 provides an
algorithm for constructing a proof of →Γ from the Herbrand disjunction H
(really, the list of substitution pairs) and its proof. Similarly, the ﬁrst part of
the proof provides an algorithm for constructing an Herbrand disjunction and
its proof, from a proof satisfying the conditions of Gentzen’s sharpened Haupt-
satz. Actually, since the proof of the sharpened Hauptsatz from Gentzen’s cut
elimination theorem is entirely constructive, a Herbrand disjunction and its
proof can be constructed from a pure-variable, cut-free proof. The only step
that has not been justiﬁed constructively in our presentation is the fact that a
provable sequent has a cut-free proof. This is because even though the search
procedure yields a cut-free proof of a provable sequent, the correctness and
termination of the search procedure for provable sequents is established by

PROBLEMS
353
semantic means involving a nonconstructive step: the existence of the possibly
inﬁnite counter-example tree (considering the case where the sequent is fal-
siﬁable). However, Gentzen gave a completely constructive (syntactic) proof
of the cut elimination theorem, and so, the version of Herbrand’s theorem
given in this section is actually entirely constructive, as is Herbrand’s original
version (Herbrand, 1971). See also lemma 7.6.2.
As mentioned at the beginning of this chapter, there is a theorem similar
in form to Herbrand’s theorem and known as the Skolem-Herbrand-G¨odel
theorem. Since a version of that theorem will be proved in Section 7.6, we
postpone a discussion of the relationship between the two theorems to the end
of Section 7.6.
PROBLEMS
7.5.1.
Prove the following fact: Given a sequent A1, ..., Am →B1, ..., Bn,
A1, ..., Am →B1, ..., Bn is provable (in LK) if and only if →¬A1 ∨
... ∨¬Am ∨B1 ∨... ∨Bn is provable (in LK).
7.5.2.
The method given in Section 7.2 for converting a formula to prenex
form used in conjunction with the Skolemization method of Section
7.5 tends to create Skolem functions with more arguments than nec-
essary.
(a) Prove that the following method for Skolemizing is correct:
Step 1: Eliminate redundant quantiﬁers; that is, quantiﬁers ∀x or ∃x
such that the input formula contains a subformula of the form ∀xB
or ∃xB in which x does not occur in B.
Step 2: Rectify the formula.
Step 3: Eliminate the connectives ⊃and ≡.
Step 4: Convert to NNF.
Step 5: Push quantiﬁers to the right. By this, we mean: Replace
∃x(A ∨B) by
	 A ∨∃xB
if x is not free in A,
∃xA ∨B
if x is not free in B.
∀x(A ∨B) by
	 A ∨∀xB
if x is not free in A ,
∀xA ∨B
if x is not free in B.
∃x(A ∧B) by
	 A ∧∃xB
if x is not free in A,
∃xA ∧B
if x is not free in B.
∀x(A ∧B) by
	 A ∧∀xB
if x is not free in A,
∀xA ∧B
if x is not free in B.
Step 6: Eliminate universal quantiﬁers using Skolem function and
constant symbols.

354
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Step 7: Move existential quantiﬁers to the left, using the inverse of
the transformation of step 5.
(b) Compare the ﬁrst method and the method of this problem for the
formula
∀x2∃y1∀x1∃y2(P(x1, y1) ∧Q(x2, y2)).
Note: Step 5 is the step that reduces the number of arguments of
Skolem functions.
7.5.3.
Prove that the following formulae are valid using Herbrand’s theorem:
¬(∃x∀yP(x, y) ∧∀y∃xP(y, x))
¬(¬(∀xP(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w)))
¬(¬∀x(P(x) ∨∃y¬Q(y)) ∨(∀zP(z) ∨∃w¬Q(w)))
7.5.4.
Give an example in which A[s1/x1][s2/x2]...[sn/xn], the result of sub-
stituting s1 for x1, ... ,sn for xn (as deﬁned in deﬁnition 5.2.6) in that
order, is diﬀerent from A[s1/xs, ..., sn/xn].
Show that if none of the variables in the support of the substitution
σ occurs in the terms s1,...,sn, the order in which the substitutions
are performed is irrelevant, and in this case,
A[s1/x1][s2/x2]...[sn/xn] = A[s1/x1, ..., sn/xn].
7.5.5.
Fill in the missing details in the proof of theorem 7.5.1.
7.5.6.
Consider the following formula given by
¬∃y∀z(P(z, y) ≡¬∃x(P(z, x) ∧P(x, z))).
(a) Prove that the above formula is equivalent to the following prenex
formula A:
∀y∃z∀u∃x[(P(z, y) ∧P(z, x) ∧P(x, z))∨
(¬P(z, y) ∧(¬P(z, u) ∨¬P(u, z)))].
(b) Show that A can be Skolemized to the formula
B = ∃z∃x[(P(z, a) ∧P(z, x) ∧P(x, z))∨
(¬P(z, a) ∧(¬P(z, f(z)) ∨¬P(f(z), z)))],
and that the formula C given by
[(P(a, a) ∧P(a, f(a)) ∧P(f(a), a))∨
(¬P(a, a) ∧(¬P(a, f(a)) ∨¬P(f(a), a)))]

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
355
is valid.
(c) Using the method of theorem 7.5.1, reconstruct a proof of A from
the valid Herbrand disjunction C.
7.5.7.
Consider a ﬁrst-order language without equality.
Show that Her-
brand’s theorem provides an algorithm for deciding the validity of
prenex sentences of the form
∀x1...∀xm∃y1...∃ynB.
7.5.8.
Write a computer program implementing the method given in the
proof of theorem 7.5.1 for reconstructing a proof from a Herbrand’s
disjunction.
7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in
NNF
In this section, we shall state a version of the Skolem-Herbrand-G¨odel theorem
for unsatisﬁability as opposed to validity.
7.6.1 Skolem-Herbrand-G¨odel’s Theorem in Unsatisﬁa-
bility Form
Using the results of Section 7.4, we shall derive a version of the Herbrand-
Skolem-G¨odel theorem for formulae in NNF due to Andrews (Andrews, 1981).
Actually, we shall prove more. We shall also give half of a version of Her-
brand’s theorem for sentences in NNF, the part which states that if a sequent
A →
is provable, then a quantiﬁer-free formula C whose negation ¬C is
provable can be eﬀectively constructed.
We believe that it is possible to give a constructive Herbrand-like ver-
sion of this theorem similar to theorem 7.5.1, but the technical details of the
proof of the converse of the theorem appear to be very involved. Hence we
shall use a mixed strategy: Part of the proof will be obtained constructively
from theorem 7.4.1, the other part by a semantic argument showing that a
sentence is satisﬁable iﬀits Skolem form is satisﬁable.
This last result is
also interesting in its own right, and can be used to prove other results, such
as the compactness theorem, and the L¨owenheim-Skolem theorem (see the
problems).
Since a formula A is valid iﬀ¬A is unsatisﬁable, any unsatisﬁability
version of the Skolem-Herbrand-G¨odel theorem yields a validity version of the
theorem, and vice versa. Since one of the most important applications of the
Skolem-Herbrand-G¨odel theorem is the completeness of refutation-oriented
procedures such as the resolution method (to be presented in Chapter 8) and

356
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
the method of matings (Andrews, 1981), it will be useful for the reader to see
a treatment of this theorem for unsatisﬁablity.
As discussed in Section 7.5, since our goal is now to prove that a formula
A is valid by showing that ¬A is unsatisﬁable, we are going to use the dual of
the method used in Section 7.5, that is, eliminate existential quantiﬁers using
Skolem functions. However, it is not quite as simple to deﬁne the conversion
of a formula in NNF to Skolem normal form (satisﬁability functional form) as
it is to convert a formula in prenex form into (validity) functional form. We
present an example ﬁrst.
EXAMPLE 7.6.1
Consider the formula
A = ∃x((P(x) ∨∃yR(x, y)) ⊃(∃zR(x, z) ∨P(a))).
The NNF of its negation is
B = ∀x((P(x) ∨∃yR(x, y)) ∧(∀z¬R(x, z) ∧¬P(a))).
The following is a (G2nnf) proof in normal form of the sequent B →:
P(a), ¬P(a) →
P(a), ¬R(a, y), ¬P(a) →
R(a, y), ¬R(a, y) →
R(a, y), ¬R(a, y), ¬P(a) →
(P(a) ∨R(a, y)), ¬R(a, y), ¬P(a) →
(P(a) ∨R(a, y)), (¬R(a, y) ∧¬P(a)) →
(P(a) ∨R(a, y)) ∧(¬R(a, y) ∧¬P(a)) →
(P(a) ∨R(a, y)) ∧(∀z¬R(a, z) ∧¬P(a)) →
(P(a) ∨∃yR(a, y)) ∧(∀z¬R(a, z) ∧¬P(a)) →
∀x((P(x) ∨∃yR(x, y)) ∧(∀z¬R(x, z) ∧¬P(a))) →
(In order to shorten the proof, the ∧: left rule of G was used rather
than the ∧: left rule of G2nnf. We leave it as an exercise to make the
necessary alterations to obtain a pure G2nnf-proof.) The midsequent is
(P(a) ∨R(a, y)) ∧(¬R(a, y) ∧¬P(a)) →.
The existential quantiﬁer can be eliminated by introducing the unary
Skolem function symbol f, and we have the following sequent:
(∗)
∀x((P(x) ∨R(x, f(x))) ∧(∀z¬R(x, z) ∧¬P(a))) →

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
357
If in the above proof we replace all occurrences of the eigenvariable y by
f(a), we obtain a proof of the sequent (∗) whose midsequent is:
(P(a) ∨R(a, f(a))) ∧(¬R(a, f(a)) ∧¬P(a)) →.
This illustates the Skolem-Herbrand-G¨odel’s theorem stated in unsatis-
ﬁability form: A formula B is unsatisﬁable iﬀsome special kind of quantiﬁer-
free substitution instance of B is unsatisﬁable.
Such instances are called compound instances by Andrews (Andrews,
1981). We shall now deﬁne precisely all the concepts mentioned in the above
example.
7.6.2 Skolem Normal Form
We begin with the notion of universal scope of a subformula.
Deﬁnition 7.6.1
Given a (rectiﬁed) formula A in NNF, the set US(A) of
pairs < B, L > where B is a subformula of A and L is a sequence of variables
is deﬁned inductively as follows:
US0 = {< A, <>>};
USk+1 = USk ∪{< C, L >, < D, L > | < B, L >∈USk,
B is of the form (C ∧D) or (C ∨D)}
∪{< C, L > | < ∃xC, L >∈USk}
∪{< C, < y1, ..., ym, x >> | < ∀xC, < y1, ..., ym >>∈USk}.
For every subformula B of A, the sequence L of variables such that
< B, L > belongs to US(A) =  USk is the universal scope of B.
In the process of introducing Skolem symbols to eliminate existential
quantiﬁers, we shall consider the subset of US consisting of all the pairs
< ∃xB, L >, where ∃xB is a subformula of A.
EXAMPLE 7.6.2
Let
A = ∀x(P(a) ∨∃y(Q(y) ∧∀z(P(y, z) ∨∃uQ(x, u)))) ∨∃wQ(a, w).
Then,
< ∃y(Q(y) ∧∀z(P(y, z) ∨∃uQ(x, u))), < x >>,
< ∃uQ(x, u), < x, z >>
and
< ∃wQ(a, w), <>>

358
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
deﬁne the universal scope of the subformulae of A of the form ∃xB. We
now deﬁne the process of Skolemization.
Deﬁnition 7.6.2
Given a rectiﬁed sentence A, the Skolem form SK(A)
of A (or Skolem normal form) is deﬁned recursively as follows using the set
US(A). Let A′ be any subformula of A:
(i) If A′ is either an atomic formula B or the negation ¬B of an atomic
formula B, then
SK(A′) = A′.
(ii) If A′ is of the form (B ∗C), where ∗∈{∨, ∧}, then
SK(A′) = (SK(B) ∗SK(C)).
(iii) If A′ is of the form ∀xB, then
SK(A′) = ∀xSK(B).
(iv) If A′ is of the form ∃xB, then if < y1, ..., ym > is the universal scope
of ∃xB (that is, the sequence of variables such that < ∃xB, < y1, ..., ym >>∈
US(A)) then
(a) If m > 0, create a new Skolem function symbol fA′ of rank m and
let
SK(A′) = SK(B[fA′(y1, ..., ym)/x]).
(b) If m = 0, create a new Skolem constant symbol fA′ and let
SK(A′) = SK(B[fA′/x]).
Observe that since the sentence A is rectiﬁed, all subformulae A′ are
distinct, and since the Skolem symbols are indexed by the subformulae A′,
they are also distinct.
EXAMPLE 7.6.3
Let
A = ∀x(P(a) ∨∃y(Q(y) ∧∀z(P(y, z) ∨∃uQ(x, u)))) ∨∃wQ(a, w),
as in example 7.6.2.
SK(∃wQ(a, w)) = Q(a, c),
SK(∃uQ(x, u)) = Q(x, f(x, z)),
SK(∃y(Q(y) ∧∀z(P(y, z) ∨∃uQ(x, u)))) =
(Q(g(x)) ∧∀z(P(g(x), z) ∨Q(x, f(x, z)))),
and
SK(A) =
∀x(P(a) ∨(Q(g(x)) ∧∀z(P(g(x), z) ∨Q(x, f(x, z))))) ∨Q(a, c).

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
359
The symbol c is a Skolem constant symbol, g is a unary Skolem function
symbol, and f is a binary Skolem function symbol.
7.6.3 Compound Instances
At the end of example 7.6.1, we mentioned that the midsequent of a proof in
normal form of a sequent B →where B is in Skolem normal form consists
of certain formulae called compound instances. The formal deﬁnition is given
below.
Deﬁnition 7.6.3
Let A be a rectiﬁed sentence and let B its Skolem form.
The set of compound instances (for short, c-instances) of B is deﬁned induc-
tively as follows:
(i) If B is either an atomic formula C or the negation ¬C of an atomic
formula, then B is its only c-instance;
(ii) If B is of the form (C ∗D), where ∗∈{∨, ∧}, for any c-instance H
of C and c-instance K of D, (H ∗K) is a c-instance of B;
(iii) If B is of the form ∀xC, for any k closed terms t1,...,tk, if Hi is a
c-instance of C[ti/x] for i = 1, ..., k, then H1 ∧... ∧Hk is a c-instance of B.
EXAMPLE 7.6.4
Let
B = ∀x(P(x) ∨∀yQ(y, f(x))) ∧(¬P(a) ∧(¬Q(a, f(a)) ∨¬Q(b, f(a)))).
Then,
(P(a)∨(Q(a, f(a))∧Q(b, f(a))))∧(¬P(a)∧(¬Q(a, f(a))∨¬Q(b, f(a))))
is a c-instance of B.
Note that c-instances are quantiﬁer free. The following lemma shows
that in a certain sense, c-instances are closed under conjunctions.
Lemma 7.6.1 Let A be a sentence in NNF and in Skolem normal form. For
any two c-instances K and L of A, a c-instance D such that D ⊃(K ∧L) is
provable can be constructed.
Proof : We proceed by induction on A.
(i) If A is either of the form B or ¬B for an atomic formula B, then
K = L = A and we let D = A.
(ii) If A is of the form (B ∗C), where ∗∈{∨, ∧}, then K is of the form
(B1 ∗C1) and L is of the form (B2 ∗C2), where B1 and B2 are c-instances
of B, and C1 and C2 are c-instances of C.
By the induction hypothesis,
there are c-instances D1 of B and D2 of C such that D1 ⊃(B1 ∗B2) and

360
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
D2 ⊃(C1 ∗C2) are provable. But then, D = (D1 ∗D2) is a c-instance of A
such that D ⊃(K ∗L) is provable.
(iii) If A is of the form ∀xC, then K has the form H1 ∧... ∧Hm, where
Hi is a c-instance of C[si/x] for i = 1, ..., m, and L has the form K1 ∧...∧Kn,
where Kj is a c-instance of C[tj/x] for j = 1, ..., n. It is clear that D = K ∧L
satisﬁes the lemma.
We are now ready for the constructive part of Herbrand’s theorem
7.6.4 Half of a Herbrand-like Theorem for Sentences in
NNF
In the rest of this section, it is assumed that ﬁrst-order languages have at least
one constant symbol. This can always be achieved by adjoining the special
symbol # as a constant. First, we prove the constructive half of Herbrand’s
theorem announced in the introduction to this section. This part asserts a
kind of completeness result: If a sequent A →is provable, then this can
be demonstrated constructively by providing a (propositional) proof of the
negation ¬C of a quantiﬁer-free formula C obtained from A.
Lemma 7.6.2 Let L be a ﬁrst-order language with or without equality. Let
A be an L-sentence in NNF, and let B be its Skolem normal form. If A →is
provable (in G1nnf or G1nnf
=
), then a (quantiﬁer-free) c-instance C of B can
be constructed such that ¬C is provable.
Proof : From theorems 7.4.1 and 7.4.2, if A →is provable, it has a proof
in normal form, in which all quantiﬁer rules are below all propositional rules.
Let us now perform the following alteration to the proof:
In a bottom-up fashion, starting from B →, whenever the ∃: left rule
is applied to a subformula A′ of the form ∃xB with eigenvariable z, if the
universal scope of ∃xB is < y1, ..., ym > and the terms t1,...,tm have been
substituted for y1,...,ym in previous ∀: right steps, substitute fA′(t1, ..., tm)
for all occurrence of z in the proof. If the midsequent of the resulting tree still
has variables, substitute any constant for all occurrences of these variables.
It is not diﬃcult to prove by induction on proof trees that the resulting
deduction is a proof of the sequent B →, where B is the Skolem form of A.
This is left as an (easy) exercise.
We can also prove that the midsequent consists of c-instances of B. For
this, we prove the following claim by induction on proof trees:
Claim: For every proof in normal form of a sequent Γ →consisting of
Skolem forms of sentences, the formulae in the midsequent are c-instances of
the sentences in Γ.
Proof of claim: We have three cases depending on the sentence B to
which the quantiﬁer rule is applied.

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
361
The result is trivial if B is either an atomic formula or the negation of
an atomic formula.
If B is of the form (C ∗D), where ∗∈{∧, ∨}, assume without loss of
generality that the ∀: left rule is applied to C. Hence, in the premise of the
rule, (C ∗D) is replaced by (C[F[t/x])/E] ∗D), where E is a maximal sub-
formula of C of the form ∀xF. By the induction hypothesis, the midsequent
consists of c-instances of (C[F[t/x])/E] ∗D) and of the other formulae in Γ.
However, one can easily show by induction on the formula C that a c-instance
of (C[F[t/x])/E]∗D) is also a c-instance of (C ∗D). This is left as an exercise
to the reader. Hence, the result holds.
If B is of the form ∀xC, then in the premise of the rule, B is replaced by
C[t/x] for some closed term t. By the induction hypothesis, the midsequent
consists of c-instances of C[t/x] and of the other formulae in Γ. By deﬁnition,
a c-instance of C[t/x] is a c-instance of B. Hence, the midsequent consists of
c-instances of the sentences in Γ.
If C1,...,Cm are the c-instances of B occurring in the midsequent, since
C1, ..., Cm →is provable, ¬(C1 ∧... ∧Cm) is provable.
Applying lemma
7.6.1 m −1 times, a c-instance C of B can be constructed such that C ⊃
(C1 ∧... ∧Cm) is provable. But then, ¬C is provable since ¬(C1 ∧... ∧Cm)
is provable.
Remark: In Andrews, 1981, a semantic proof of lemma 7.6.2 is given for
languages without equality. Our result applies to languages with equality as
well, and is constructive, because we are using the full strength of the normal
form theorems (theorems 7.4.1 and 7.4.2).
7.6.5 Skolem-Herbrand-G¨odel’s Theorem (Sentences in
NNF)
In order to prove the converse of the above lemma, we could as in the proof of
theorem 7.5.1 try to reconstruct a proof from an unsatisﬁable c-instance C of
B. This is a rather delicate process whose justiﬁcation is very tedious, and we
will follow a less constructive but simpler approach involving semantic argu-
ments. Hence, instead of proving the converse of the half of Herbrand’s theo-
rem shown in lemma 7.6.2, we shall prove a version of the Skolem-Herbrand-
G¨odel theorem.
The following fact will be used:
(∗) If the Skolem form B of a sentence A is unsatisﬁable then A is
unsatisﬁable.
Since it is easy to prove that if C is a c-instance of B then (B ⊃C) is
valid, if C is unsatisﬁable, then B must be unsatisﬁable, and by (∗) A is also
unsatisﬁable.
We shall actually prove not only (∗) but also its converse. This is more
than we need for the part of the proof of the Skolem-Herbrand-G¨odel theorem,

362
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
but since this result can be used to give a semantic proof of the Skolem-
Herbrand-G¨odel theorem (see the problems), it is interesting to prove it in
full.
Lemma 7.6.3 Let L be a ﬁrst-order language with or without equality. Let
A be a rectiﬁed L-sentence in NNF, and let B be its Skolem normal form.
The sentence A is satisﬁable iﬀits Skolem form B is satisﬁable.
Proof : Let C be any subformula of A. We will show that the following
properties hold:
(a) For every structure A such that all function, predicate, and constant
symbols in the Skolem form SK(C) of C receive an interpretation, for every
assignment s, if A |= SK(C)[s] then A |= C[s].
(b) For every structure A such that exactly all function, predicate, and
constant symbols in C receive an interpretation, for every assignment s (with
range A), if A |= C[s] then there is an expansion B of A such that B |=
SK(C)[s].
Recall from deﬁnition 5.4.6 that if B is an expansion of A, then A and
B have the same domain, and the interpretation function of A is a restriction
of the interpretation function of B.
The proof proceeds by induction on subformulae of A.
(i) If C is either of the form D or ¬D where D is atomic, SK(C) = C
and both (a) and (b) are trivial.
(ii) If C is of the form (D ∧E), then SK(C) = (SK(D) ∧SK(E)).
(a) If A |= SK(C)[s] then A |= SK(D)[s] and A |= SK(E)[s]. By the
induction hypothesis, A |= D[s] and A |= E[s]. But then, A |= C[s].
(b) Let A be a structure such that exactly all function, predicate, and
constant symbols in C receive an interpretation. Since A |= C[s], we have
A |= D[s] and A |= E[s]. By the induction hypothesis, there are expansions
B1 and B2 of A such that B1 |= SK(D)[s] and B2 |= SK(E)[s].
Since
B1 and B2 are both expansions of A, their interpretation functions agree on
all the predicate, function, and constant symbols occurring in both D and
E, and since the sets of Skolem symbols in D and E are disjoint (because
A is rectiﬁed), we can take the union of the two interpretation functions to
obtain an expansion B of A such that B |= (SK(D) ∧SK(E))[s], that is,
B |= SK(C)[s].
(iii) C is of the form (D ∨E). This case is similar to case (ii) and is left
as an exercise.
(iv) C is of the form ∀xD. Then SK(C) = ∀xSK(D).
(a) If A |= SK(C)[s], then A |= SK(D)[s[x := a]] for all a ∈A. By the
induction hypothesis, A |= D[s[x := a]] for all a ∈A, that is, A |= C[s].

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
363
(b) If A |= C[s], then A |= D[s[x := a]] for all a ∈A. By the induction
hypothesis, there is an expansion B of A such that for any assignment s′,
if A |= D[s′] then B |= SK(D)[s′]. But then, for all a ∈A, we have B |=
SK(D)[s[x := a]], that is, B |= SK(C)[s].
(v) C is of the form ∃xD. Then SK(C) = SK(D)[fC(y1, ..., ym)/x],
where < y1, ..., ym > is the universal scope of C in A, and fC is the Skolem
symbol associated with C.
(a) If A |= SK(C), then by lemma 5.4.1, letting
a = (fC(y1, ..., ym))A[s],
we have A |= (SK(D)[a/x])[s], which by lemma 5.3.1 is equivalent to A |=
(SK(D))[s[y := a]]. By the induction hypothesis, A |= D[s[y := a]], that is,
A |= (∃xD)[s].
(b) Assume that A |= C[s] for every s. Then, for some a ∈A, A |=
D[s[x := a]]. By the induction hypothesis, there is an expansion B of A such
that B |= (SK(D))[s[x := a]]. Observe that by the recursive deﬁnition of the
Skolem form of a formula, FV (SK(D)) = {x, y1, ..., ym}, where < y1, ..., ym >
is the universal scope of C in A. In order to expand B to a structure for
SK(C), we need to interpret fC in B. For any (a1, ..., am) ∈Am, let s be any
assignment such that s(yi) = ai, for i = 1, .., m. By the induction hypothesis,
there is some a ∈A such that
B |= (SK(D))[s[x := a]].
Deﬁne the value of fC(a1, ..., am) in B as any chosen a ∈A such that
(∗)
B |= (SK(D))[s[x := a]].
Since the only free variables in SK(D) are {x, y1, ..., ym}, by lemma 5.3.3, this
deﬁnition only depends on the values of y1, ..., ym. Given the interpretation
for fC given in (∗), for any assignment s, for a = (fC(y1, ..., ym))B[s], we have
B |= (SK(D))[s[x := a]]. By lemma 5.3.1 and lemma 5.4.1, we have
B |= (SK(D))[s[x := a]]
iﬀ
B |= (SK(D)[a/x])[s]
iﬀ
B |= (SK(D)[fC(y1, ..., ym)/x])[s].
Hence, B |= SK(C)[s], as desired.
We are now ready to prove the following version of the Skolem-Herbrand-
G¨odel theorem extending Andrews’s theorem (Andrews, 1981) to ﬁrst-order
languages with equality.

364
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
Theorem 7.6.1 (Skolem-Herbrand-G¨odel theorem, after Andrews) Let L be
a ﬁrst-order language with or without equality. Given any rectiﬁed sentence
A, if B is the Skolem form of A, then the following holds:
(a) A is unsatisﬁable if and only if some compound instance C of B is
unsatisﬁable.
(b) Given a (G1nnf or G1nnf
=
) proof of the sequent A →, a compound
instance C of B such that ¬C is provable can be eﬀectively constructed from
the proof of A →.
Proof : Part (b) is lemma 7.6.2. Part (a) is proved as follows.
If A is unsatisﬁable, then ¬A is valid and by the completeness theorem,
¬A is provable. Hence, A →is provable, and by lemma 7.6.2, a compound
instance C of B such that ¬C is provable can be eﬀectively constructed. By
soundness, ¬C is valid, and so C is unsatisﬁable.
Conversely, assume that some compound instance C of B is unsatisﬁable.
We prove that for any compound instance C of B, (B ⊃C) is valid. This is
shown by induction on B.
If B is either an atomic formula or the negation of an atomic formula,
C = B and (B ⊃C) is valid.
If B is of the form (D ∗E), where ∗∈{∨, ∧}, then C is of the form
(K ∗L) where K is a compound instance of D and L is a compound instance
of D. By the induction hypothesis, both (D ⊃K) and (E ⊃L) are valid.
But then, (D ∗E) ⊃(K ∗L) is valid.
If B is of the form ∀xD, then C is of the form H1 ∧... ∧Hk, where
Hi is a compound instance of D[ti/x], for some closed terms ti, i = 1, ..., k.
By the induction hypothesis, (D[ti/x] ⊃Hi) is valid for i = 1, ..., k. But
(∀xD ⊃D[t/x]) is valid for every closed term t (in fact for every term t free
for x in D), as shown in the proof of lemma 5.4.2. Therefore, (B ⊃Hi) is
valid for i = 1, ..., k, which implies that (B ⊃(H1 ∧... ∧Hk)) is valid.
This concludes the proof that (B ⊃C) is valid.
Now, since C is unsatisﬁable and (B ⊃C) is valid, B is also unsatisﬁable.
By lemma 7.6.3 (part (a)), if A is satisﬁable then B is satisﬁable. Since B is
unsatisﬁable, A must be unsatisﬁable.
If the sentence A in NNF is also prenex, observe that a compound in-
stance C of the Skolem form B of A is in fact a conjunction of ground sub-
stitution instances of the matrix of B. Hence, we obtain the following useful
corollary.
Corollary
If A is a prenex sentence in NNF, A is unsatisﬁable if and only
if a ﬁnite conjunction of ground substitution instances of the matrix of the
Skolem form B of A is unsatisﬁable.

7.6 Skolem-Herbrand-G¨odel’s Theorem for Formulae in NNF
365
Remarks: (1) The ﬁrst remark given at the end of the proof of theorem
7.5.1 regarding the diﬀerence between logic without equality and logic with
equality also applies here. There is an algorithm for deciding whether a c-
instance is unsatisﬁable if equality is absent, but in case equality is present,
such an algorithm is much less trivial. For details, see Chapter 10.
(2) The version of theorem 7.6.1(a) for languages without equality is due
to Andrews (Andrews, 1981). Andrews’s proof uses semantic arguments and
assumes the result of lemma 7.6.3. Instead of using lemma 7.6.2, assuming
that every compound instance of B is satisﬁable, Andrews constructs a model
of B using the compactness theorem. His proof is more concise than ours,
but there is more to the theorem, as revealed by part (b). Indeed, there is
a constructive aspect to this theorem reﬂected in the part of our proof using
lemma 7.6.2, which is not brought to light by Andrews’s semantic method.
Actually, we have not pushed the constructive approach as far as we
could, since we did not show how a proof of A →can be reconstructed from
a proof of a compound instance C →.
This last construction appears to
be feasible, but we have not worked out the technical details, which seem
very tedious.
Instead, we have proved a version of the Skolem-Herbrand-
G¨odel theorem using the easy semantic argument that consists of showing
that (B ⊃C) is valid for every compound instance of B, and part (a) of
lemma 7.6.3.
7.6.6 Comparison of Herbrand and Skolem-Herbrand-
G¨odel Theorems
We now wish to discuss brieﬂy the diﬀerences between Herbrand’s original
theorem and the Skolem-Herbrand-G¨odel theorem.
First, Herbrand’s theorem deals with provability whereas the Skolem-
Herbrand-G¨odel deals with unsatisﬁability (or validity). Herbrand’s theorem
is also a deeper result, whose proof is harder, but it yields more information.
Roughly speaking, Herbrand’s theorem asserts the following:
Herbrand’s original theorem:
(1) If a formula A is provable in a formal system QH deﬁned by Her-
brand, then a Herbrand disjunction H and its proof can be obtained construc-
tively via primitive recursive functions from the proof of A.
(2) From a Herbrand disjunction H and its proof, a proof of A in QH
can be obtained constructively via a primitive recursive function.
The concept of a primitive recursive function is covered in some detail
in Section 7.7, and in the following discussion, we shall content ourselves with
an informal deﬁnition. Roughly speaking, a primitive recursive function is a
function over the natural numbers whose rate of growth is reasonably well
behaved. The rate of growth of each primitive recursive function is uniformly
bounded, in the sense that no primitive recursive function can grow faster

366
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
than a certain given function (which itself is not primitive recursive). The
class of primitive recursive function contains some simple functions, called the
base functions, and is closed under two operations: composition and primitive
recursion (see Section 7.7). Primitive recursion is a certain constrained type
of recursion, and this is why the rate of growth of the primitive recursive
functions is not arbitrary.
The fact that in Herbrand’s theorem proofs can be obtained construc-
tively and with reasonable complexity via primitive recursive functions, is a
very essential part of the theorem. This last point is well illustrated by the
history of the theorem, discussed extensively by Goldfarb, in Herbrand, 1971.
Herbrand’s original version of the theorem was sometimes diﬃcult to
follow and its proof contained errors. As a matter of fact, Herbrand’s original
statement of the theorem did not refer to the concept of a primitive recursive
function. Denton and Dreben were able to repair the defective proofs, and
they realized the fact that the constructions are primitive recursive (see Note
G and and Note H, in Herbrand, 1971). In his thesis, Herbrand mistakenly
claimed simpler functions. It is also interesting to know that Herbrand did not
accept the concept of validity because it is an inﬁnitistic concept, and that this
is the reason he gave an argument that is entirely proof-theoretic. However,
Herbrand had an intuitive sense of the semantic contents of his theorem. As
mentioned by Goldfarb in his introduction to Herbrand, 1971:
“Herbrand intends the notion of expansion to furnish more, namely a
ﬁnitistic surrogate for the model-theoretic notion of inﬁnite satisﬁability.”
We close this discussion with a ﬁnal remark showing the central role
occupied by Herbrand’s theorem. First, observe that it is possible to prove
the Skolem-Herbrand-G¨odel theorem without appealing to the completeness
theorem (see problem 7.6.11). Then, the following hold:
(1) Herbrand’s theorem together with the Skolem-Herbrand-G¨odel the-
orem implies the completeness theorem.
(2) Herbrand’s theorem together with the completeness theorem implies
the Skolem-Herbrand-G¨odel theorem (see problem 7.6.15).
Of course, such proofs are a bit of an overkill, but we are merely illus-
trating the depth of Herbrand’s theorem.
The version of Herbrand’s theorem that we have presented in theorem
7.5.1 (and in the part in lemma 7.6.2) has the constructive nature of Her-
brand’s original theorem. What has not been shown is the primitive recursive
nature of the functions yielding on the one hand the Herbrand disjunction
H and its proof from the prenex sequent →Γ, and on the other hand the
proof of →Γ from a proof of the Herbrand disjunction H. However, we have
shown the recursive nature of these functions. In view of the above discussion
regarding Herbrand’s original version of the theorem, it would be surprising
if these functions were not primitive recursive.

PROBLEMS
367
For details on Herbrand’s original theorem, the interested reader is re-
ferred to Herbrand, 1971; Van Heijenoort, 1967; and Joyner’s Ph.D thesis
(Automatic theorem Proving and The Decision Problem, Ph.D thesis, W. H.
Joyner, Harvard University, 1974), which contains a Herbrand-like theorem
for the resolution method.
PROBLEMS
7.6.1.
Show that the Skolem form of the negation of the formula
A = ∃x∀y[(P(x) ≡P(y)) ⊃(∃xP(x) ≡∀yP(y))]
is the formula
C = ∀y[(¬P(c) ∨P(y)) ∧(¬P(y) ∨P(c))]∧
[(P(d) ∧¬P(e)) ∨(∀zP(z) ∧∀x¬P(x))].
Using Skolem-Herbrand-G¨odel’s theorem, prove that A is valid.
7.6.2.
Convert the negation of the following formula to Skolem form:
¬∃y∀z(P(z, y) ≡¬∃x(P(z, x) ∧P(x, z))).
7.6.3.
Convert the negation of the following formula to Skolem form:
∃x∃y∀z([P(x, y) ⊃(P(y, z) ∧P(z, z))]∧
[(P(x, y) ∧Q(x, y)) ⊃(Q(x, z) ∧Q(z, z))]).
7.6.4.
Write a computer program for converting a formulae in NNF to
SKolem normal form, incorporating the optimization suggested in
problem 7.5.2.
7.6.5.
Fill in the missing details in the proof of lemma 7.6.2.
7.6.6.
Prove the following fact: A c-instance of (C[F[t/x])/E] ∗D) is a c-
instance of (C ∗D).
7.6.7.
Use the Skolem-Herbrand-G¨odel theorem to show that the formula of
problem 7.6.2 is valid.
7.6.8.
Use the Skolem-Herbrand-G¨odel theorem to show that the formula of
problem 7.6.3 is valid.
7.6.9.
Use lemma 7.6.3 to prove that a set of sentences is satisﬁable iﬀthe
set of their Skolem forms is satisﬁable. (Assume that for any two
distinct sentences, the sets of Skolem symbols are disjoint.)

368
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.6.10. Let L be a ﬁrst-order language without equality. A free structure H
is an L-structure with domain the set HL of all closed L-terms, and
whose interpretation function satisﬁes the following property:
(i) For every function symbol f of rank n, for all t1,...,tn ∈H,
fH(t1, ..., tn) = ft1...tn, and
(ii) For every constant symbol c,
cH = c.
(a) Prove that the Skolem form B of a sentence A in NNF is satisﬁable
iﬀB is satisﬁable in a free structure.
(b) Prove that a set of sentences is satisﬁable iﬀit is satisﬁable in a
free structure. (Use problem 7.6.9.)
(c) Prove that (b) and (c) are false for languages with equality, but
that they are true if we replace free structure by quotient of a free
structure.
∗7.6.11. Let L be a ﬁrst-order language without equality. Given a set S of L-
sentences in NNF, let H be the free structure built up from the set of
function and constant symbols occurring in the Skolem forms of the
sentences in S. The Herbrand expansion E(C, H) of a quantiﬁer-free
formula C in NNF over the free universe H, is the set of all formulae
of the form C[t1/x1, ..., tm/xm], where {x1, ..., xm} is the set of free
variables in C, and t1,...,tm ∈H.
For each sentence A ∈S, let
E(B∗, H) be the Herbrand expansion of the quantiﬁer-free formula
B∗obtained by deleting the universal quantiﬁers in the Skolem form
B of A. The Herbrand expansion E(A, H) of the sentence A is equal
to E(B∗, H).
(a) Prove that S is satisﬁable iﬀthe union of all the expansions
E(B∗, H) deﬁned above is satisﬁable. (Use problem 7.6.10.)
(b) Using the compactness theorem for propositional logic, prove the
following version of the Skolem-Herbrand-G¨odel theorem:
A sentence A is unsatisﬁable iﬀsome ﬁnite conjunction of quantiﬁer-
free formulae in the Herbrand Expansion E(A, H) is unsatisﬁable.
(c) Use (a) to prove the L¨owenheim-Skolem theorem.
∗7.6.12. Let L be a ﬁrst-order language without equality. Let L′ be an expan-
sion of L obtained by adding function and constant symbols. Let H
be the set of all closed L-terms, and H′ the set of all closed L′-terms.
Prove that for any sentence A, if the Herbrand expansion E(A, H)
is satisﬁable, then E(B∗, H′) is also satisﬁable, where B∗is the

∗7.7 The Primitive Recursive Functions
369
quantiﬁer-free formula obtained by deleting the universal quantiﬁers
in the Skolem form B of A.
Hint: Deﬁne a function h : H′ →H as follows: Let t0 be any ﬁxed
term in H.
(i) h(t) = t0 if t is either a constant in H′ not occurring in B∗or a
term of the form f(t1, ..., tk) such that f does not occur in B∗.
(ii) h(t) = t if t is a constant occurring in B∗, or f(h(t1), .., h(tn)) if
t is of the form f(t1, ..., tn) and f occurs in B∗.
Assume that E(A, H) is satisﬁable in a free structure A. Expand A to
an L′-structure B using the following deﬁnition: For every predicate
symbol of rank n, for all t1,...,tn ∈H′,
B |= Pt1...tn
iﬀ
A |= Ph(t1)...h(tn).
Prove that E(B∗, H′) is satisﬁed in B.
7.6.13. Let L be a ﬁrst-order language without equality.
Prove the com-
pactness theorem using problems 7.6.11, 7.6.12, and the compactness
theorem for propositional logic.
7.6.14. State and prove a validity version of theorem 7.6.1.
7.6.15. Consider ﬁrst-order languages without equality. In this problem, as-
sume that Gentzen’s original proof of the cut elimination theorem is
used to avoid the completeness theorem in proving theorem 7.5.1.
(a) Prove that Herbrand’s theorem (theorem 7.5.1) and the Skolem-
Herbrand-G¨odel theorem proved in problem 7.6.11 (without the com-
pleteness theorem) yield the completeness theorem (for prenex for-
mulae).
(b) Prove that Herbrand’s theorem (theorem 7.5.1) and the complete-
ness theorem yield the Skolem-Herbrand-G¨odel theorem.
∗7.7 The Primitive Recursive Functions
First, we discuss informally the notion of computability.
7.7.1 The Concept of Computability
In the discussion at the end of Section 7.6, the concept of a primitive recur-
sive function was mentioned. In this section, we discuss this concept very
brieﬂy. For more details on recursive function theory and complexity theory,
the reader is referred to Lewis and Papadimitriou, 1981; Davis and Weyuker,

370
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
1983; Machtey and Young, 1978; or Rogers, 1967. For excellent surveys, we
recommend Enderton and Smorynski’s articles in Barwise, 1977.
At the end of the nineteenth century, classical mathematics was shaken
by paradoxes and inconsistencies. The famous mathematician Hilbert pro-
posed the following program in order to put mathematics on solid foundations:
Formalize mathematics completely, and exploit the ﬁnitist nature of proofs to
prove the consistency of the formalized theory (that is, the absence of a con-
tradiction) in the theory itself. In 1930, the famous logician Kurt G¨odel made
a major announcement; Hilbert’s consistency program could not be carried
out. Indeed, G¨odel had proved two incompleteness theorems that showed the
impossibility of Hilbert’s program. The second theorem roughly states that in
any consistent formal theory T containing arithmetic, the sentence asserting
the consistency of T is not provable in T.
To prove his theorems, G¨odel invented a technique now known as G¨odel-
numbering, in which syntactic objects such as formulae and proofs are encoded
as natural numbers. The functions used to perform such encodings are deﬁn-
able in arithmetic, and are in some intuitive sense computable. These func-
tions are the primitive recursive functions. To carry out Hilbert’s program, it
was also important to understand what is a computable function, since one
of the objectives of the program was to check proofs mechanically.
The concern for providing logical foundations for mathematics prompted
important and extensive research (initiated in the early thirties) on the topic
of computability and undecidability, by Herbrand, G¨odel, Church, Rosser,
Kleene, Turing, and Post, to name only the pioneers in the ﬁeld. Summarizing
more than 60 years of research in a few lines, the following important and
surprising facts (at least at the time of their ﬁnding) were discovered:
Several models of computations were proposed by diﬀerent researchers,
and were shown to be equivalent, in the sense that they all deﬁne the same
class of functions called the partial recursive functions.
Among these models are the Turing machine already discussed in Sub-
section 3.3.5, and the class of partial recursive functions (due to Herbrand,
Kleene, G¨odel).
The above led to what is usually known as the Church-Turing thesis,
which states that any “reasonable” deﬁnition of the concept of an eﬀectively
(or algorithmically) computable function is equivalent to the concept of a
partial recursive function.
The Church-Turing thesis cannot be proved because the notion of a rea-
sonable deﬁnition of computability is not clearly deﬁned, but most researchers
in the ﬁeld believe it.
The other important theme relevant to our considerations is that of the
complexity of computing a function. Indeed, there are computable functions
(such as Ackermann’s function, see Davis and Weyuker, 1983) that require

∗7.7 The Primitive Recursive Functions
371
so much time and space to be computed that they are computable only for
very small arguments (may be n = 0, 1, 2). For some of these functions, the
rate of growth is so “wild” that it is actually beyond imagination. For an
entertaining article on this topic, consult Smorynski, 1983.
The class of primitive recursive functions is a subclass of the computable
functions that, for all practical purposes, contains all the computable func-
tions that one would ever want to compute. It is generally agreed that if an
algorithm corresponds to a computable function that is not primitive recur-
sive, it is not a simple algorithm. Herbrand’s theorem says that the algorithms
that yields a Herbrand’s disjunction from a proof of the input formula and
its converse are primitive recursive functions. Hence, from a computational
point of view, the transformation given by Herbrand’s theorem are reasonably
simple, or at least not too bad.
The primitive recursive functions also play an important role in G¨odel’s
incompleteness results (see Enderton, 1972, or Monk, 1976).
7.7.2 Deﬁnition of the Primitive Recursive Functions
In the rest of this section, we are considering functions of the natural numbers.
In order to deﬁne the class of primitive recursive functions we need to deﬁne
two operations on functions:
(1) Composition;
(2) Primitive Recursion.
Deﬁnition 7.7.1
Given a function f of m > 0 arguments and m functions
g1,...,gm each of n > 0 arguments, the composition
f ◦(g1, .., gm)
of f and g1,...,gm is the function h of n arguments such that, for all x1, .., xn ∈
N,
h(x1, ..., xn) = f(g1(x1, ..., xn), ..., gm(x1, ..., xn)).
Primitive recusion is deﬁned as follows.
Deﬁnition 7.7.2 Given a function f of n arguments (n > 0), and a function
g of n+1 arguments, the function h of n+1 arguments is deﬁned by primitive
recursion from f and g iﬀthe following holds: For all x1, ..., xn, y ∈N,
h(x1, ..., xn, 0) = f(x1, ..., xn);
h(x1, ..., xn, y + 1) = g(y, h(x1, ..., xn, y), x1, ..., xn).
In the special case n = 0, let m be any given integer. Then
h(0) = m;
h(y + 1) = g(y, h(y)).

372
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
We also deﬁne the base functions.
Deﬁnition 7.7.3
The base functions are the following functions:
(i) The successor function S, such that for all x ∈N,
S(x) = x + 1;
(ii) The zero function Z, such that for all x ∈N,
Z(x) = 0;
(iii) The projections functions. For every n > 0, for every i, 1 ≤i ≤n,
for all x1, ..., xn ∈N,
P n
i (x1, ..., xn) = xi.
The class of primitive recursive functions is deﬁned inductively as fol-
lows.
Deﬁnition 7.7.4 The class of primitive recursive functions is the least class
of total functions over N containing the base functions and closed under com-
position and primitive recursion.
It can be shown that composition and primitive recursion preserve to-
tality, so the deﬁnition makes sense.
7.7.3 The Partial Recursive Functions
In order to deﬁne the partial recursive functions, we need one more operation,
the operation of minimization.
Deﬁnition 7.7.5
Given a function g of n + 1 arguments, the function f of
n > 0 arguments is deﬁned by minimization from g iﬀthe following holds:
For all x1, ..., xn ∈N:
(i) f(x1, ..., xn) is deﬁned iﬀthere is some y ∈N such that g(x1, ..., xn, z)
is deﬁned for all z ≤y and g(x1, ..., xn, y) = 0;
(ii) If f(x1, ..., xn) is deﬁned, then f(x1, ..., xn) is equal to the least y
satisfying (i). In other words,
f(x1, ..., xn) = y
iﬀ
g(x1, ..., xn, y) = 0
and
for all z < y, g(x1, ..., xn, z) is deﬁned and nonzero.
The condition that g(x1, ..., xn, z) is deﬁned for all z ≤y is essential to
the deﬁnition, since otherwise one could deﬁne noncomputable functions. The
function f is also denoted by
miny(g(x1, ..., xn, y) = 0)

∗7.7 The Primitive Recursive Functions
373
(with a small abuse of notation, since the variables x1, ..., xn should not be
present).
Deﬁnition 7.7.6
The class of partial recursive functions is the least class
of partial functions over N containing the base functions and closed under
composition, primitive recursion, and minimization.
A function is recursive iﬀit is a total partial recursive function.
Obviously, the class of primitive recursive functions is a subclass of the
class of recursive functions. Contrary to the other closure operations, if f is
obtained by minimization from a total function g, f is not necessarily a total
function. For example, if g is the function such that g(x, y) = x + y + 1,
miny(g(x, y) = 0) is the partial function undeﬁned everywhere.
It can be shown that there are (total) recursive functions that are not
primitive recursive. The following function kown as Ackermann’s function is
such as example: (See example 2.1.1, in Chapter 2.)
EXAMPLE 7.7.1
A(x, y) = if x = 0 then y + 1
else if y = 0 then A(x −1, 1)
else A(x −1, A(x, y −1))
A problem A (encoded as a set of natural numbers) is said to be decidable
iﬀthere is a (total) recursive function hA such that for all n ∈N,
n ∈A
iﬀ
hA(n) = 1, otherwise hA(n) = 0.
A problem A is partially decidable iﬀthere is a partial recursive function hA
such that for all n ∈N,
n ∈A
iﬀ
hA(n) = 1, otherwise either hA is undeﬁned or hA(n) = 0.
Church’s theorem states that the problem of deciding whether a ﬁrst-
order formula is valid is partially decidable, but is not decidable (see Enderton,
1972; Monk, 1976; Lewis and Papadimitriou, 1981).
We conclude with a short list of examples of primitive recursive func-
tions. One of the unpleasant properties of deﬁnition 7.7.4 is the rigid format
of primitive recursion, which forces one to use projections and composition to
permute or drop arguments. Since the purpose of this section is only to give
a superﬁcial idea of what the primitive recursive functions are, we will ignore
these details in the deﬁnitions given below. We leave as a (tedious) exercise to
the reader the task to rewrite the deﬁnitions below so that they ﬁt deﬁnition
7.7.4.

374
7/Gentzen’s Sharpened Hauptsatz; Herbrand’s Theorem
7.7.4 Some Primitive Recursive Functions
EXAMPLE 7.7.2
(a) Addition:
x + 0 = P 1
1 (x)
x + (y + 1) = S(x + y)
(b) Multiplication:
x ∗0 = Z(x)
x ∗(y + 1) = (x ∗y) + x
(c) Exponentiation:
exp(x, 0) = 1
exp(x, y + 1) = exp(x, y) ∗x
(d) Factorial:
fact(0) = 1
fact(y + 1) = fact(y) ∗S(y)
(e) Iterated exponentiation:
ex(0) = 0
ex(y + 1) = exp(2, ex(y))
(f) N-th prime number:
pr(x) = the x-th prime number.
PROBLEMS
7.7.1.
Prove that composition and primitive recursion applied to total func-
tions yield total functions.
7.7.2.
Prove that the functions given in example 7.7.2 are primitive recur-
sive.

Notes and Suggestions for Further Reading
375
Notes and Suggestions for Further Reading
Gentzen’s cut elimination theorem, Gentzen’s sharpened Hauptsatz, and Her-
brand’s theorem are perhaps the most fundamental proof-theoretic results of
ﬁrst-order logic.
Gentzen’s theorems show that there are normal forms for proofs, and re-
duce the provability of a ﬁrst-order sentence to the provability of a quantiﬁer-
free formula. Similarly, Herbrand’s theorem provides a deep characterization
of the notion of provability, and a reduction to the quantiﬁer-free case.
Interestingly, Herbrand’s proof (Herbrand, 1971) and Gentzen’s proofs
(Szabo, 1969) are signiﬁcantly diﬀerent. It is often felt that Herbrand’s argu-
ments are diﬃcult to follow, whereas Gentzen’s arguments are crystal clear.
Since Gentzen’s Hauptsatz requires formulae to be prenex, but Her-
brand’s original theorem holds for arbitrary formulae, it is often said that
Herbrand’s theorem is more general than Gentzen’s sharpened Hauptsatz.
However, using Kleene’s method (presented in Section 7.5) and the method
partially developed in Section 7.4, it appears that this is not the case.
For more on Gentzen systems, the reader is referred to Szabo, 1969;
Takeuti, 1975; Kleene, 1952; Smullyan, 1968; Prawitz, 1965; and Schwichten-
berg’s article in Barwise, 1977. Another interesting application of Herbrand’s
theorem is its use to ﬁnd decidable classes of formulae. The reader is referred
to Dreben and Goldfarb, 1979. A companion book by Lewis (Lewis, 1979)
deals with unsolvable classes of formulae.
We have not explored complexity issues related to Herbrand’s theorem,
or Gentzen’s theorems, but these are interesting. It is known that a proof of
a sentence can be transformed into a proof of a disjunction of quantiﬁer-free
(ground) instances of that sentence, and that the transformation is primitive
recursive, but how complex is the resulting proof?
A result of Statman (Statman, 1979) shows that a signiﬁcant increase
in the length of the proof can occur.
It is shown in Statman, 1979, that
for a certain set X of universally quantiﬁed equations, for each n, there is a
closed equation En such that En is provable from X in a proof of size linear
in n, but that for any set Y of ground instances of equations in X such that
Y →En is provable, Y has cardinality at least ex(n)/2, where ex(n) is the
iterated exponential function deﬁned at the end of Section 7.7. For related
considerations, the reader is referred to Statman’s article in Barwise, 1977.
Another interesting topic that we have not discussed is the possibility
of extending Herbrand’s theorem to higher-order logic. Such a generalization
is investigated in Miller, 1984.

Chapter 8
Resolution In
First-Order Logic
8.1 Introduction
In this chapter, the resolution method presented in Chapter 4 for propositional
logic is extended to ﬁrst-order logic without equality. The point of departure
is the Skolem-Herbrand-G¨odel theorem (theorem 7.6.1). Recall that this the-
orem says that a sentence A is unsatisﬁable iﬀsome compound instance C of
the Skolem form B of A is unsatisﬁable. This suggests the following procedure
for checking unsatisﬁability:
Enumerate the compound instances of B systematically one by one,
testing each time a new compound instance C is generated, whether C is
unsatisﬁable.
If we are considering a ﬁrst-order language without equality, there are
algorithms for testing whether a quantiﬁer-free formula is valid (for example,
the search procedure) and, if B is unsatisﬁable, this will be eventually discov-
ered. Indeed, the search procedure halts for every compound instance, and
for some compound instance C, ¬C will be found valid.
If the logic contains equality, the situation is more complex. This is
because the search procedure does not necessarily halt for quantiﬁer-free for-
mulae that are not valid. Hence, it is possible that the procedure for checking
unsatisﬁability will run forever even if B is unsatisﬁable, because the search
procedure can run forever for some compound instance that is not unsatisﬁ-
able. We can ﬁx the problem as follows:
376

8.1 Introduction
377
Interleave the generation of compound instances with the process of
checking whether a compound instance is unsatisﬁable, proceeding by rounds.
A round consists in running the search procedure a ﬁxed number of steps for
each compound instance being tested, and then generating a new compound
instance. The process is repeated with the new set of compound instances.
In this fashion, at the end of each round, we have made progress in checking
the unsatisﬁability of all the activated compound instances, but we have also
made progress in the number of compound instances being considered.
Needless to say, such a method is horribly ineﬃcient.
Actually, it is
possible to design an algorithm for testing the unsatisﬁability of a quantiﬁer-
free formula with equality by extending the congruence closure method of
Oppen and Nelson (Nelson and Oppen, 1980). This extension is presented in
Chapter 10.
In the case of a language without equality, any algorithm for deciding
the unsatisﬁability of a quantiﬁer-free formula can be used.
However, the
choice of such an algorithm is constrained by the need for eﬃciency. Several
methods have been proposed. The search procedure can be used, but this is
probably the least eﬃcient choice. If the compound instances C are in CNF,
the resolution method of Chapter 4 is a possible candidate. Another method
called the method of matings has also been proposed by Andrews (Andrews,
1981).
In this chapter, we are going to explore the method using resolution.
Such a method is called ground resolution, because it is applied to quantiﬁer-
free clauses with no variables.
From the point of view of eﬃciency, there is an undesirable feature,
which is the need for systematically generating compound instances. Unfor-
tunately, there is no hope that the process of ﬁnding a refutation can be purely
mechanical. Indeed, by Church’s theorem (mentioned in the remark after the
proof of theorem 5.5.1), there is no algorithm for deciding the unsatisﬁability
(validity) of a formula.
There is a way of avoiding the systematic generation of compound in-
stances due to J. A. Robinson (Robinson, 1965). The idea is not to generate
compound instances at all, but instead to generalize the resolution method so
that it applies directly to the clauses in B, as opposed to the (ground) clauses
in the compound instance C. The completeness of this method was shown by
Robinson. The method is to show that every ground refutation can be lifted
to a refutation operating on the original clauses, as opposed to the closed (or
ground) substitution instances. In order to perform this lifting operation the
process of uniﬁcation must be introduced. We shall deﬁne these concepts in
the following sections.
It is also possible to extend the resolution method to ﬁrst-order lan-
guages with equality using the paramodulation method due to Robinson and
Wos (Robinson and Wos, 1969, Loveland, 1978), but the completeness proof is

378
8/Resolution In First-Order Logic
rather delicate. Hence, we will restrict our attention to ﬁrst-order languages
without equality, and refer the interested reader to Loveland, 1978, for an
exposition of paramodulation.
As in Chapter 4, the resolution method for ﬁrst-order logic (without
equality) is applied to special conjunctions of formulae called clauses. Hence,
it is necessary to convert a sentence A into a sentence A′ in clause form, such
that A is unsatisﬁable iﬀA′ is unsatisﬁable. The conversion process is deﬁned
below.
8.2 Formulae in Clause Form
First, we deﬁne the notion of a formula in clause form.
Deﬁnition 8.2.1
As in the propositional case, a literal is either an atomic
formula B, or the negation ¬B of an atomic formula. Given a literal L, its
conjugate L is deﬁned such that, if L = B then L = ¬B, else if L = ¬B
then L = B. A sentence A is in clause form iﬀit is a conjunction of (prenex)
sentences of the form ∀x1...∀xmC, where C is a disjunction of literals, and the
sets of bound variables {x1, ..., xm} are disjoint for any two distinct clauses.
Each sentence ∀x1...∀xmC is called a clause. If a clause in A has no quantiﬁers
and does not contain any variables, we say that it is a ground clause.
For simplicity of notation, the universal quantiﬁers are usually omitted
in writing clauses.
Lemma 8.2.1 For every (rectiﬁed) sentence A, a sentence B′ in clause form
such that A is valid iﬀB′ is unsatisﬁable can be constructed.
Proof : Given a sentence A, ﬁrst B = ¬A is converted to B1 in NNF
using lemma 6.4.1. Then B1 is converted to B2 in Skolem normal form using
the method of deﬁnition 7.6.2. Next, by lemma 7.2.1, B2 is converted to B3 in
prenex form. Next, the matrix of B3 is converted to conjunctive normal form
using theorem 3.4.2, yielding B4. In this step, theorem 3.4.2 is applicable
because the matrix is quantiﬁer free. Finally, the quantiﬁers are distributed
over each conjunct using the valid formula ∀x(A ∧B) ≡∀xA ∧∀xB, and
renamed apart using lemma 5.3.4.
Let the resulting sentence be called B′. The resulting formula B′ is a
conjunction of clauses.
By lemma 6.4.1, B is unsatisﬁable iﬀB1 is.
By lemma 7.6.3, B1 is
unsatisﬁable iﬀB2 is. By lemma 7.2.1, B2 is unsatisﬁable iﬀB3 is. By theorem
3.4.2 and lemma 5.3.7, B3 is unsatisﬁable iﬀB4 is. Finally, by lemma 5.3.4
and lemma 5.3.7, B4 is unsatisﬁable iﬀB′ is. Hence, B is unsatisﬁable iﬀ
B′ is. Since A is valid iﬀB = ¬A is unsatisﬁable, then A is valid iﬀB′ is
unsatisﬁable.

8.3 Ground Resolution
379
EXAMPLE 8.2.1
Let
A = ¬∃y∀z(P(z, y) ≡¬∃x(P(z, x) ∧P(x, z))).
First, we negate A and eliminate ≡. We obtain the sentence
∃y∀z[(¬P(z, y) ∨¬∃x(P(z, x) ∧P(x, z)))∧
(∃x(P(z, x) ∧P(x, z)) ∨P(z, y))].
Next, we put in this formula in NNF:
∃y∀z[(¬P(z, y) ∨∀x(¬P(z, x) ∨¬P(x, z)))∧
(∃x(P(z, x) ∧P(x, z)) ∨P(z, y))].
Next, we eliminate existential quantiﬁers, by the introduction of Skolem
symbols:
∀z[(¬P(z, a) ∨∀x(¬P(z, x) ∨¬P(x, z)))∧
((P(z, f(z)) ∧P(f(z), z)) ∨P(z, a))].
We now put in prenex form:
∀z∀x[(¬P(z, a) ∨(¬P(z, x) ∨¬P(x, z)))∧
((P(z, f(z)) ∧P(f(z), z)) ∨P(z, a))].
We put in CNF by distributing ∧over ∨:
∀z∀x[(¬P(z, a) ∨¬P(z, x) ∨¬P(x, z))∧
(P(z, f(z)) ∨P(z, a)) ∧(P(f(z), z)) ∨P(z, a))].
Omitting universal quantiﬁers, we have the following three clauses:
C1 = (¬P(z1, a) ∨¬P(z1, x) ∨¬P(x, z1)),
C2 = (P(z2, f(z2)) ∨P(z2, a)) and
C3 = (P(f(z3), z3) ∨P(z3, a)).
We will now show that we can prove that B = ¬A is unsatisﬁable, by
instantiating C1, C2, C3 to ground clauses and use the resolution method of
Chapter 4.
8.3 Ground Resolution
The ground resolution method is the resolution method applied to sets of
ground clauses.

380
8/Resolution In First-Order Logic
EXAMPLE 8.3.1
Consider the following ground clauses obtained by substitution from C1,
C2 and C3:
G1 = (¬P(a, a)) (from C1, substituting a for x and z1)
G2 = (P(a, f(a)) ∨P(a, a)) (from C2, substituting a for z2)
G3 = (P(f(a), a)) ∨P(a, a)) (from C3, substituting a for z3).
G4 = (¬P(f(a), a) ∨¬P(a, f(a))) (from C1, substituting f(a)
for z1 and a for x).
The following is a refutation by (ground) resolution of the set of ground
clauses G1, G2, G3, G4.
G2
G1
G3
G4
{P(a, f(a))}
{P(f(a), a)}
{¬P(a, f(a))}
We have the following useful result.
Lemma 8.3.1
(Completeness of ground resolution) The ground resolution
method is complete for ground clauses.
Proof :
Observe that the systems G′ and GCNF ′ are complete for
quantiﬁer-free formulae of a ﬁrst-order language without equality.
Hence,
by theorem 4.3.1, the resolution method is also complete for sets of ground
clauses.
However, note that this is not the case for quantiﬁer-free formulae with
equality, due to the need for equality axioms and for inessential cuts, in order
to retain completeness.
Since we have shown that a conjunction of ground instances of the clauses
C1, C2, C3 of example 8.2.1 is unsatisﬁable, by the Skolem-Herbrand-G¨odel
theorem, the sentence A of example 8.2.1 is valid.
Summarizing the above, we have a method for ﬁnding whether a sen-
tence B is unsatisﬁable known as ground resolution. This method consists in
converting the sentence B into a set of clauses B′, instantiating these clauses
to ground clauses, and applying the ground resolution method.
By the completeness of resolution for propositional logic (theorem 4.3.1),
and the Skolem-Herbrand-G¨odel theorem (actually the corollary to theorem
7.6.1 suﬃces, since the clauses are in CNF, and so in NNF), this method is
complete.

8.4 Uniﬁcation and the Uniﬁcation Algorithm
381
However, we were lucky to ﬁnd so easily the ground clauses G1, G2, G3
and G4. In general, all one can do is enumerate ground instances one by one,
testing for the unsatisﬁabiliy of the current set of ground clauses each time.
This can be a very costly process, both in terms of time and space.
8.4 Uniﬁcation and the Uniﬁcation Algorithm
The fundamental concept that allows the lifting of the ground resolution
method to the ﬁrst-order case is that of a most general uniﬁer.
8.4.1 Uniﬁers and Most General Uniﬁers
We have already mentioned that Robinson has generalized ground resolution
to arbitrary clauses, so that the systematic generation of ground clauses is
unnecessary.
The new ingredient in this new form of resolution is that in forming the
resolvent, one is allowed to apply substitutions to the parent clauses.
For example, to obtain {P(a, f(a))} from
C1 = (¬P(z1, a) ∨¬P(z1, x) ∨¬P(x, z1))
and
C2 = (P(z2, f(z2)) ∨P(z2, a)),
ﬁrst we substitute a for z1, a for x, and a for z2, obtaining
G1 = (¬P(a, a))
and
G2 = (P(a, f(a)) ∨P(a, a)),
and then we resolve on the literal P(a, a).
Note that the two sets of literals {P(z1, a), P(z1, x), P(x, z1)} and {P(z2,
a)} obtained by dropping the negation sign in C1 have been “uniﬁed” by the
substitution (a/x, a/z1, a/z2).
In general, given two clauses B and C whose variables are disjoint, given
a substitution σ having as support the union of the sets of variables in B and
C, if σ(B) and σ(C) contain a literal Q and its conjugate, there must be a
subset {B1, ..., Bm} of the sets of literals of B, and a subset {C1, ..., Cn} of
the set of literals in C such that
σ(B1) = ... = σ(Bm) = σ(C1) = ... = σ(Cn).
We say that σ is a uniﬁer for the set of literals {B1, ..., Bm, C1, ..., Cn}. Robin-
son showed that there is an algorithm called the uniﬁcation algorithm, for
deciding whether a set of literals is uniﬁable, and if so, the algorithm yields
what is called a most general uniﬁer (Robinson, 1965). We will now explain
these concepts in detail.

382
8/Resolution In First-Order Logic
Deﬁnition 8.4.1
Given a substitution σ, let D(σ) = {x | σ(x) ̸= x} denote
the support of σ, and let I(σ) = 
x∈D(σ) FV (σ(x)). Given two substitutions
σ and θ, their composition denoted σ◦θ is the substitution σ◦θ (recall that θ is
the unique homomorphic extension of θ). It is easily shown that the substitu-
tion σ◦θ is the restriction of σ◦θ to V. If σ has support {x1, ..., xm} and σ(xi)
= si for i = 1, ..., m, we also denote the substitution σ by (s1/x1, ..., sm/xm).
The notions of a uniﬁer and a most general uniﬁer are deﬁned for ar-
bitrary trees over a ranked alphabet (see Subsection 2.2.6). Since terms and
atomic formulae have an obvious representation as trees (rigorously, since they
are freely generated, we could deﬁne a bijection recursively), it is perfectly
suitable to deal with trees, and in fact, this is intuitively more appealing due
to the graphical nature of trees.
Deﬁnition 8.4.2
Given a ranked alphabet Σ, given any set S = {t1, ..., tn}
of ﬁnite Σ-trees, we say that a substitution σ is a uniﬁer of S iﬀ
σ(t1) = ... = σ(tn).
We say that a substitution σ is a most general uniﬁer of S iﬀit is a uniﬁer of
S, the support of σ is a subset of the set of variables occurring in the set S,
and for any other uniﬁer σ′ of S, there is a substitution θ such that
σ′ = σ ◦θ.
The tree t = σ(t1) = ... = σ(tn) is called a most common instance of t1,...,tn.
EXAMPLE 8.4.1
(i) Let t1 = f(x, g(y)) and t2 = f(g(u), g(z)). The substitution (g(u)/x,
y/z) is a most general uniﬁer yielding the most common instance f(g(u),
g(y)).
(ii) However, t1 = f(x, g(y)) and t2 = f(g(u), h(z)) are not uniﬁable
since this requires g = h.
(iii) A slightly more devious case of non uniﬁability is the following:
Let t1 = f(x, g(x), x) and t2 = f(g(u), g(g(z)), z). To unify these two
trees, we must have x = g(u) = z. But we also need g(x) = g(g(z)),
that is, x = g(z). This implies z = g(z), which is impossible for ﬁnite
trees.
This last example suggest that unifying trees is similar to solving systems
of equations by variable elimination, and there is indeed such an analogy. This
analogy is explicated in Gorn, 1984. First, we show that we can reduce the
problem of unifying any set of trees to the problem of unifying two trees.
Lemma 8.4.1
Let t1,...,tm be any m trees, and let # be a symbol of rank
m not occurring in any of these trees. A substitution σ is a uniﬁer for the set
{t1, ..., tm} iﬀσ is a uniﬁer for the set {#(t1, ..., tm), #(t1, ..., t1)}.

8.4 Uniﬁcation and the Uniﬁcation Algorithm
383
Proof : Since a substitution σ is a homomorphism (see deﬁnition 7.5.3),
σ(#(t1, ..., tm)) = #(σ(t1), ..., σ(tm))
and
σ(#(t1, ..., t1)) = #(σ(t1), ..., σ(t1)).
Hence,
σ(#(t1, ..., tm)) = σ(#(t1, ..., t1))
iﬀ
#(σ(t1), ..., σ(tm)) = #(σ(t1), ..., σ(t1))
iﬀ
σ(t1) = σ(t1), σ(t2) = σ(t1), ..., σ(tm) = σ(t1)
iﬀ
σ(t1) = ... = σ(tm).
Before showing that if a set of trees is uniﬁable then it has a most general
uniﬁer, we note that most general uniﬁers are essentially unique when they
exist.
Lemma 8.4.2 holds even if the support of mgu’s is not a subset of
FV (S).
Lemma 8.4.2
If a set of trees S is uniﬁable and σ and θ are any two most
general uniﬁers for S, then there exists a substitution ρ such that θ = σ ◦ρ,
ρ is a bijection between I(σ) ∪(D(θ) −D(σ)) and I(θ) ∪(D(σ) −D(θ)), and
D(ρ) = I(σ) ∪(D(θ) −D(σ)) and ρ(x) is a variable for every x ∈D(ρ).
Proof : First, note that a bijective substitution must be a bijective re-
naming of variables.
Let f|A denote the restriction of a function f to A.
If ρ is bijective, there is a substitution ρ′ such that (ρ ◦ρ′)|D(ρ) = Id and
(ρ′ ◦ρ)|D(ρ′) = Id. But then, if ρ(x) is not a variable for some x in the sup-
port of ρ, ρ(x) is a constant or a tree t of depth ≥1. Since (ρ ◦ρ′)|D(ρ) = Id,
we have ρ′(t) = x. Since a substitution is a homomorphism, if t is a con-
stant c, ρ′(c) = c ̸= x, and otherwise ρ′(t) has depth at least 1, and so
ρ′(t) ̸= x. Hence, ρ(x) must be a variable for every x (and similarly for ρ′).
A reasoning similar to the above also shows that for any two substitutions
σ and ρ, if σ = σ ◦ρ, then ρ is the identity on I(σ). But then, if both σ
and θ are most general uniﬁers, there exist σ′ and θ′ such that θ = σ ◦θ′ and
σ = θ◦σ′. Thus, D(σ′) = I(θ)∪(D(σ)−D(θ)), D(θ′) = I(σ)∪(D(θ)−D(σ)),
θ = θ ◦(σ′ ◦θ′), and σ = σ ◦(θ′ ◦σ′). We claim that (σ′ ◦θ′)|D(σ′) = Id, and
(θ′ ◦σ′)|D(θ′) = Id. We prove that (σ′ ◦θ′)|D(σ′) = Id, the other case being
similar. For x ∈I(θ), σ′ ◦θ′(x) = x follows from above. For x ∈D(σ)−D(θ),
then x = θ(x) = θ′(σ(x)), and so, σ(x) = y, and θ′(y) = x, for some variable
y. Also, σ(x) = y = σ′(θ(x)) = σ′(x). Hence, σ′ ◦θ′(x) = x. Since D(θ′) and
D(σ′) are ﬁnite, θ′ is a bijection between D(θ′) and D(σ′). Letting ρ = θ′,
the lemma holds.
We shall now present a version of Robinson’s uniﬁcation algorithm.
8.4.2 The Uniﬁcation Algorithm
In view of lemma 8.4.1, we restrict our attention to pairs of trees. The main
idea of the uniﬁcation algorithm is to ﬁnd how two trees “disagree,” and try

384
8/Resolution In First-Order Logic
to force them to agree by substituting trees for variables, if possible. There
are two types of disagreements:
(1) Fatal disagreements, which are of two kinds:
(i) For some tree address u both in dom(t1) and dom(t2), the labels
t1(u) and t2(u) are not variables and t1(u) ̸= t2(u). This is illus-
trated by case (ii) in example 8.4.1;
(ii) For some tree address u in both dom(t1) and dom(t2), t1(u) is
a variable say x, and the subtree t2/u rooted at u in t2 is not
a variable and x occurs in t2/u (or the symmetric case in which
t2(u) is a variable and t1/u isn’t). This is illustrated in case (iii)
of example 8.4.1.
(2) Repairable disagreements: For some tree address u both in dom(t1) and
dom(t2), t1(u) is a variable and the subtree t2/u rooted at u in t2 does
not contain the variable t1(u).
In case (1), uniﬁcation is impossible (although if we allowed inﬁnite trees,
disagreements of type (1)(ii) could be ﬁxed; see Gorn, 1984). In case (2), we
force “local agreement” by substituting the subtree t2/u for all occurrences of
the variable x in both t1 and t2.
It is rather clear that we need a systematic method for ﬁnding disagree-
ments in trees. Depending on the representation chosen for trees, the method
will vary.
In most presentations of uniﬁcation, it is usually assumed that
trees are represented as parenthesized expressions, and that the two strings
are scanned from left to right until a disagreement is found. However, an
actual method for doing so is usually not given explicitly. We believe that in
order to give a clearer description of the uniﬁcation algorithm, it is better to
be more explicit about the method for ﬁnding disagreements, and that it is
also better not to be tied to any string representation of trees. Hence, we will
give a recursive algorithm inspired from J. A. Robinson’s original algorithm,
in which trees are deﬁned in terms of tree domains (as in Section 2.2), and the
disagreements are discovered by performing two parallel top-down traversals
of the trees t1 and t2.
The type of traversal that we shall be using is a recursive traversal in
which the root is visited ﬁrst, and then, from left to right, the subtrees of the
root are recursively visited (this kind of traversal is called a preorder traversal,
see Knuth, 1968, Vol. 1). We deﬁne some useful functions on trees. (The
reader is advised to review the deﬁnitions concerning trees given in Section
2.2.)
Deﬁnition 8.4.3
For any tree t, for any tree address u ∈dom(t):
leaf(u) = true iﬀu is a leaf;
variable(t(u)) = true iﬀt(u) is a variable;
left(u) = if leaf(u) then nil else u1;
right(ui) = if u(i + 1) ∈dom(t) then u(i + 1) else nil.

8.4 Uniﬁcation and the Uniﬁcation Algorithm
385
We also assume that we have a function dosubstitution(t, σ), where t is
a tree and σ is a substitution.
Deﬁnition 8.4.4
(A uniﬁcation algorithm) The formal parameters of the
algorithm uniﬁcation are the two input trees t1 and t2, an output ﬂag indicat-
ing whether the two trees are uniﬁable or not (unifiable), and a most general
uniﬁer (unifier) (if it exists).
The main program uniﬁcation calls the recursive procedure unify, which
performs the uniﬁcation recursively and needs procedure test-and-substitute
to repair disagreements found, as in case (2) discussed above. The variables
tree1 and tree2 denote trees (of type tree), and the variables node, newnode
are tree addresses (of type treereference). The variable unifier is used to
build a most general uniﬁer (if any), and the variable newpair is used to form
a new substitution component (of the form (t/x), where t is a tree and x
is a variable). The function compose is simply function composition, where
compose(unifier, newpair) is the result of composing unifier and newpair,
in this order. The variables tree1, tree2, and node are global variables to
the procedure uniﬁcation. Whenever a new disageement is resolved in test-
and-substitute, we also apply the substitution newpair to tree1 and tree2 to
remove the disagreement. This step is not really necessary, since at any time,
dosubstitution(t1, unifier) = tree1 and dosubstitution(t2, unifier) = tree2,
but it simpliﬁes the algorithm.
Procedure to Unify Two Trees t1 and t2
procedure unification(t1, t2 : tree; var unifiable : boolean;
var unifier : substitution);
var node : treereference; tree1, tree2 : tree;
procedure test-and-substitute(var node : treereference;
var tree1, tree2 : tree;
var unifier : substitution; var unifiable : boolean);
var newpair : substitution;
{This procedure tests whether the variable tree1(node) belongs
to the subtree of tree2 rooted at node. If it does, the
uniﬁcation fails. Otherwise, a new substitution newpair
consisting of the subtree tree2/node and the variable
tree1(node) is formed, the current unifier is composed with
newpair, and the new pair is added to the unifier.
To simplify the algorithm, we also apply newpair to tree1
and tree2 to remove the disagreement}
begin
{test whether the variable tree1(node) belongs to the
subtree tree2/node, known in the literature as “occur check”}

386
8/Resolution In First-Order Logic
if tree1(node) ∈tree2/node then
unifiable := false
else
{create a new substitution pair consisting of the
subtree tree2/node at address node, and the
variable tree1(node) at node in tree1}
newpair := ((tree2/node)/tree1(node));
{compose the current partial uniﬁer with
the new pair newpair}
unifier := compose(unifier, newpair);
{updates the two trees so that they now agree on
the subtrees at node}
tree1 := dosubstitution(tree1, newpair);
tree2 := dosubstitution(tree2, newpair)
endif
end test-and-substitute;
procedure unify(var node : treereference;
var unifiable : boolean; var unifier : substitution);
var newnode : treereference;
{Procedure unify recursively uniﬁes the subtree
of tree1 at node and the subtree of tree2 at node}
begin
if tree1(node) <> tree2(node) then
{the labels of tree1(node) and tree2(node) disagree}
if variable(tree1(node)) or variable(tree2(node)) then
{one of the two labels is a variable}
if variable(tree1(node)) then
test-and-substitute(node, tree1, tree2, unifier, unifiable)
else
test-and-substitute(node, tree2, tree1, unifier, unifiable)
endif
else
{the labels of tree1(node) and tree2(node)
disagree and are not variables}
unifiable := false
endif
endif;

8.4 Uniﬁcation and the Uniﬁcation Algorithm
387
{At this point, if unifiable = true, the labels at node
agree. We recursively unify the immediate subtrees of node
in tree1 and tree2 from left to right, if node is not a leaf}
if (left(node) <> nil) and unifiable then
newnode := left(node);
while (newnode <> nil) and unifiable do
unify(newnode, unifiable, unifier);
if unifiable then
newnode := right(newnode)
endif
endwhile
endif
end unify;
Body of Procedure Uniﬁcation
begin
tree1 := t1;
tree2 := t2;
unifiable := true;
unifier := nil;
{empty uniﬁcation}
node := e;
{start from the root}
unify(node, unifiable, unifier)
end unification
Note that if successful, the algorithm could also return the tree tree1
(or tree2), which is a most common form of t1 and t2. As presented, the
algorithm performs a single parallel traversal, but we also have the cost of the
occur check in test-and-substitute, and the cost of the substitutions. Let us
illustrate how the algorithm works.
EXAMPLE 8.4.2
Let t1 = f(x, f(x, y)) and t2 = f(g(y), f(g(a), z)), which are represented
as trees as follows:
Tree t1
f
↙
↘
x
f
↙
↘
x
y

388
8/Resolution In First-Order Logic
Tree t2
f
↙
↘
g
f
↓
↙
↘
y
g
z
↓
a
Initially, tree1 = t1, tree2 = t2 and node = e. The ﬁrst disagreement
is found for node = 1. We form newpair = (g(y)/x), and unifier =
newpair. After applying newpair to tree1 and tree2, we have:
Tree tree1
f
↙
↘
g
f
↓
↙
↘
y
g
y
↓
y
Tree tree2
f
↙
↘
g
f
↓
↙
↘
y
g
z
↓
a
The next disagreement is found for node = 211. We ﬁnd that newpair =
(a/y), and compose unifier = (g(y)/x) with newpair, obtaining (g(a)/
x, a/y). After applying newpair to tree1 and tree2, we have:
Tree tree1
f
↙
↘
g
f
↓
↙
↘
a
g
a
↓
a

8.4 Uniﬁcation and the Uniﬁcation Algorithm
389
Tree tree2
f
↙
↘
g
f
↓
↙
↘
a
g
z
↓
a
The last disagreement occurs for node = 22. We form newpair = (a/z),
and compose unifier with newpair, obtaining
unifier = (g(a)/x, a/y, a/z).
The algorithm stops successfully with the most general uniﬁer (g(a)/x,
a/y, a/z), and the trees are uniﬁed to the last value of tree1.
In order to prove the correctness of the uniﬁcation algorithm, the fol-
lowing lemma will be needed.
Lemma 8.4.3 Let # be any constant. Given any two trees f(s1, ..., sn) and
f(t1, ..., tn) the following properties hold:
(a) For any i, 1 ≤i ≤n, if σ is a most general uniﬁer for the trees
f(s1, ..., si−1, #, ..., #)
and
f(t1, ..., ti−1, #, ..., #),
then
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #)
are uniﬁable iﬀ
σ(f(s1, ..., si, #, ..., #))
and
σ(f(t1, ..., ti, #, ..., #))
are uniﬁable.
(b) For any i, 1 ≤i ≤n, if σ is a most general uniﬁer for the trees
f(s1, ..., si−1, #, ..., #) and f(t1, ..., ti−1, #, ..., #), and θ is a most general uni-
ﬁer for the trees σ(si) and σ(ti), then σ ◦θ is a most general uniﬁer for the
trees f(s1, ..., si, #, ..., #) and f(t1, ..., ti, #, ..., #).
Proof : (a) The case i = 1 is trivial. Clearly, if σ is a most general uni-
ﬁer for the trees f(s1, ..., si−1, #, ..., #) and f(t1, ..., ti−1, #, ..., #) and if the
trees σ(f(s1, ..., si, #, ..., #)) and σ(f(t1, ..., ti, #, ..., #)) are uniﬁable, then
f(s1, ..., si, #, ..., #) and f(t1, ..., ti, #, ..., #) are uniﬁable.
We now prove the other direction. Let θ be a uniﬁer for
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #).
Then,
θ(s1) = θ(t1), ..., θ(si) = θ(ti).

390
8/Resolution In First-Order Logic
Hence, θ is a uniﬁer for
f(s1, ..., si−1, #, ..., #)
and
f(t1, ..., ti−1, #, ..., #).
Since σ is a most general uniﬁer, there is some θ′ such that θ = σ ◦θ′. Then,
θ′(σ(f(s1, ..., si, #, ..., #))) = θ(f(s1, ..., si, #, ..., #))
= θ(f(t1, ..., ti, #, ..., #)) = θ′(σ(f(t1, ..., ti, #, ..., #))),
which shows that θ′ uniﬁes
σ(f(s1, ..., si, #, ..., #))
and
σ(f(t1, ..., ti, #, ..., #)).
(b) Again, the case i = 1 is trivial. Otherwise, clearly,
σ(s1) = σ(t1), ..., σ(si−1) = σ(ti−1) and θ(σ(si)) = θ(σ(ti))
implies that σ ◦θ is a uniﬁer of
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #).
If λ uniﬁes f(s1, ..., si, #, ..., #) and f(t1, ..., ti, #, ..., #), then
λ(s1) = λ(t1), ..., λ(si) = λ(ti).
Hence, λ uniﬁes
f(s1, ..., si−1, #, ..., #)
and
f(t1, ..., ti−1, #, ..., #).
Since σ is a most general uniﬁer of these two trees, there is some σ′ such that
λ = σ ◦σ′. But then, since λ(si) = λ(ti), we have σ′(σ(si)) = σ′(σ(ti)), and
since θ is a most general uniﬁer of σ(si) and σ(ti), there is some θ′ such that
σ′ = θ ◦θ′. Hence,
λ = σ ◦(θ ◦θ′) = (σ ◦θ) ◦θ′,
which proves that σ ◦θ is a most general uniﬁer of f(s1, ..., si, #, ..., #) and
f(t1, ..., ti, #, ..., #).
We will now prove the correctness of the uniﬁcation algorithm.
Theorem 8.4.1
(Correctness of the uniﬁcation algorithm) (i) Given any
two ﬁnite trees t1 and t2, the uniﬁcation algorithm always halts. It halts with
output unifiable = true iﬀt1 and t2 are uniﬁable.
(ii) If t1 and t2 are uniﬁable, then they have a most general uniﬁer and
the output of procedure unify is a most general uniﬁer.
Proof : Clearly, the procedure test-and-substitute always terminates, and
we only have to prove the termination of the unify procedure. The diﬃculty

8.4 Uniﬁcation and the Uniﬁcation Algorithm
391
in proving termination is that the trees tree1 and tree2 may grow. However,
this can only happen if test-and-substitute is called, and in that case, since
uniﬁable is not false iﬀthe variable x = tree1(node) does not belong to
t = tree2/node, after the substitution of t for all occurrences of x in both
tree1 and tree2, the variable x has been completely eliminated from both
tree1 and tree2.
This suggests to try a proof by induction over the well-
founded lexicographic ordering << deﬁned such that, for all pairs (m, t) and
(m′, t′), where m, m′ are natural numbers and t, t′ are ﬁnite trees,
(m, t) << (m′, t′)
iﬀeither m < m′,
or m = m′ and t is a proper subtree of t′.
We shall actually prove the input-output correctness assertion stated
below for the procedure unify.
Let s0 and t0 be two given ﬁnite trees, σ a substitution such that none
of the variables in the support of σ is in σ(s0) or σ(t0), u any tree address
in both dom(σ(s0)) and dom(σ(t0)), and let s = σ(s0)/u and t = σ(t0)/u.
Let tree10, tree20, node0, unifiable0 and unifier0 be the input values of
the variables tree1, tree2, unifiable, and unifier, and tree1′, tree2′, node′
unifiable′ and unifier′ be their output value (if any). Also, let m0 be the
sum of the number of variables in σ(s0) and σ(t0), and m′ the sum of the
number of variables in tree1′ and tree2′.
Correctness assertion:
If tree10 = σ(s0),
tree20 = σ(t0),
node0 = u,
unifiable0 = true
and
unifier0 = σ, then
the following holds:
(1) The procedure unify always terminates;
(2) unifiable′ = true iﬀs and t are uniﬁable and, if unifiable′ = true,
then unifier′ = σ ◦θ, where θ is a most general uniﬁer of s and t, tree1′ =
unifier′(s0), tree2′ = unifier′(t0), and no variable in the support of unifier′
occurs in tree1′ or tree2′.
(3) If tree1′ ̸= σ(s0) or tree2′ ̸= σ(t0) then m′ < m0, else m′ = m0.
Proof of assertion: We proceed by complete induction on (m, s), where
m is the sum of the number of variables in tree1 and tree2 and s is the subtree
tree1/node.
(i) Assume that s is a constant and t is not a variable, the case in
which t is a constant being similar. Then u is a leaf node in σ(s0). If t ̸= s,
the comparison of tree1(node) and tree2(node) fails, and unifiable is set to
false. The procedure terminates with failure. If s = t, since u is a leaf node
in σ(s0) and σ(t0), the procedure terminates with success, tree1′ = σ(s0),

392
8/Resolution In First-Order Logic
tree2′ = σ(t0), and unifier′ = σ. Hence the assertion holds with the identity
substitution for θ.
(ii) Assume that s is a variable say x, the case in which t is a variable
being similar. Then u is a leaf node in σ(s0). If t = s, this case reduces
to case (i). Otherwise, t ̸= x and the occur check is performed in test-and-
substitute. If x occurs in t, then unifiable is set to false, and the procedure
terminates. In this case, it is clear that x and t are not uniﬁable, and the
assertion holds. Otherwise, the substitution θ = (t/x) is created, unifier′ =
σ ◦θ, and tree1′ = θ(σ(s0)) = unifier′(s0), tree2′ = θ(σ(t0)) = unifier′(t0).
Clearly, θ is a most general uniﬁer of x and t, and since x does not occur in
t, since no variable in the support of σ occurs in σ(s0) or σ(t0), no variable
in the support of unifier′ occurs in tree1′ = θ(σ(s0)) or tree2′ = θ(σ(s0)).
Since the variable x does not occur in tree1′ and tree2′, (3) also holds. Hence,
the assertion holds.
(iii) Both s and t have depth ≥1. Assume that s = f(s1, ..., sm) and
t = f ′(t1, ..., tn). If f ̸= f ′, the test tree1(node) = tree2(node) fails, and
unify halts with failure. Clearly, s and t are not uniﬁable, and the claim
holds. Otherwise, s = f(s1, ..., sn) and t = f(t1, ..., tn).
We shall prove the following claim by induction:
Claim: (1) For every i, 1 ≤i ≤n + 1, the ﬁrst i −1 recursive calls
in the while loop in unify halt with success iﬀf(s1, ..., si−1, #, ..., #) and
f(t1, ..., ti−1, #, ..., #) are uniﬁable, and otherwise one of the calls halts with
failure;
(2) If the ﬁrst i −1 recursive calls halt with success, the input values at
the end of the (i −1)-th iteration are:
nodei = ui,
unifiablei = true,
unifieri = σ ◦θi−1,
where θi−1 is a most general uniﬁer for the trees f(s1, ..., si−1, #, ..., #) and
f(t1, ..., ti−1, #, ..., #), (with θ0 = Id, the identity substitution),
tree1i = unifieri(s0),
tree2i = unifieri(t0),
and no variable in the support of unifieri occurs in tree1i or tree2i.
(3) If tree1i ̸= tree10 or tree2i ̸= tree20, if mi is the sum of the number
of variables in tree1i and tree2i, then mi < m0.
Proof of claim: For i = 1, the claim holds because before entering the
while loop for the ﬁrst time,
tree11 = s0,
tree21 = t0,
node1 = u1,
unifier1 = σ,
unifiable1 = true.
Now, for the induction step. We only need to consider the case where
the ﬁrst i −1 recursive calls were successful. If we have tree1i = tree10 and

8.4 Uniﬁcation and the Uniﬁcation Algorithm
393
tree2i = tree20, then we can apply the induction hypothesis for the assertion
to the address ui, since tree10/ui is a proper subtree of tree10/u. Otherwise,
mi < m0, and we can also also apply the induction hypothesis for the assertion
to address ui. Note that
tree1i/u = θi−1(f(s1, ..., si, ..., sn))
and
tree2i/u = θi−1(f(t1, ..., ti, ..., sn)),
since
unifieri = σ ◦θi−1.
By lemma 8.4.3(a), since θi−1 is a most general uniﬁer for the trees
f(s1, ..., si−1, #, ..., #)
and
f(t1, ..., ti−1, #, ..., #),
then
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #)
are uniﬁable, iﬀ
θi−1(f(s1, ..., si, #, ..., #))
and
θi−1f((t1, ..., ti, #, ..., #))
are uniﬁable.
Hence, unify halts with success for this call for address ui, iﬀ
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #)
are uniﬁable.
Otherwise, unify halts with failure. This proves part (1) of the claim.
By part (2) of the assertion, the output value of the variable unifier is
of the form unifieri ◦λi, where λi is a most general uniﬁer for θi−1(si) and
θi−1(ti) (the subtrees at ui), and since θi−1 is a most general uniﬁer for
f(s1, ..., si−1, #, ..., #)
and
f(t1, ..., ti−1, #, ..., #),
λi is a most general uniﬁer for
θi−1(f(s1, ..., si, #, ..., #))
and
θi−1f((t1, ..., ti, #, ..., #)).
By lemma 8.4.3(b), θi−1 ◦λi is a most general uniﬁer for
f(s1, ..., si, #, ..., #)
and
f(t1, ..., ti, #, ..., #).
Letting
θi = θi−1 ◦λi,
it is easily seen that part (2) of the claim is satisﬁed. By part (3) of the
assertion, part (3) of the claim also holds.
This concludes the proof of the claim.
For i = n + 1, we see that all the recursive calls in the while loop halt
successfully iﬀs and t are uniﬁable, and if s and t are uniﬁable, when the loop
is exited, we have
unifiern+1 = σ ◦θn,

394
8/Resolution In First-Order Logic
where θn is a most general uniﬁer of s and t,
tree1n+1 = unifiern+1(s0),
tree2n+1 = unifiern+1(t0),
and part (3) of the assertion also holds.
This concludes the proof of the
assertion.
But now, we can apply the assertion to the input trees t1 and t2, with
u = e, and σ the identity substitution. The correctness assertion says that
unify always halts, and if it halts with success, the output variable unifier
is a most general uniﬁer for t1 and t2. This concludes the correctness proof.
The subject of uniﬁcation is the object of current research because fast
uniﬁcation is crucial for the eﬃciency of programming logic systems such
as PROLOG. Some fast uniﬁcation algorithms have been published such as
Paterson and Wegman, 1978; Martelli and Montanari, 1982; and Huet, 1976.
For a survey on uniﬁcation, see the article by Siekmann in Shostak, 1984a.
Huet, 1976, also contains a thorough study of uniﬁcation, including higher-
order uniﬁcation.
PROBLEMS
8.4.1.
Convert the following formulae to clause form:
∀y(∃x(P(y, x) ∨¬Q(y, x)) ∧∃x(¬P(x, y) ∨Q(x, y)))
∀x(∃yP(x, y) ∧¬Q(y, x)) ∨(∀y∃z(R(x, y, z) ∧¬Q(y, z)))
¬(∀x∃yP(x, y) ⊃(∀y∃z¬Q(x, z) ∧∀y¬∀zR(y, z)))
∀x∃y∀z(∃w(Q(x, w) ∨R(x, y)) ≡¬∃w¬∃u(Q(x, w) ∧¬R(x, u)))
8.4.2.
Apply the uniﬁcation algorithm to the following clauses:
{P(x, y), P(y, f(z))}
{P(a, y, f(y)), P(z, z, u)}
{P(x, g(x)), P(y, y)}
{P(x, g(x), y), P(z, u, g(u))}
{P(g(x), y), P(y, y), P(u, f(w))}
8.4.3.
Let S and T be two ﬁnite sets of terms such that the set of variables
occurring in S is disjoint from the set of variables occurring in T.
Prove that if S ∪T is uniﬁable, σS is a most general uniﬁer of S, σT
is a most general uniﬁer of T, and σS,T is a most general uniﬁer of
σS(S) ∪σT (T), then
σS ◦σT ◦σS,T

8.5 The Resolution Method for First-Order Logic
395
is a most general uniﬁer of S ∪T.
8.4.4.
Show that the most general uniﬁer of the following two trees contains
a tree with 2n−1 occurrences of the variable x1:
f(g(x1, x1), g(x2, x2), ..., g(xn−1, xn−1))
and
f(x2, x3, ..., xn)
∗8.4.5.
Deﬁne the relation ≤on terms as follows: Given any two terms t1,
t2,
t1 ≤t2
iﬀ
there is a substitution σ such that t2 = σ(t1).
Deﬁne the relation ∼= such that
t1 ∼= t2
iﬀ
t1 ≤t2 and t2 ≤t1.
(a) Prove that ≤is reﬂexive and transitive and that ∼= is an equiva-
lence relation.
(b) Prove that t1 ∼= t2 iﬀthere is a bijective renaming of variables ρ
such that t1 = ρ(t2). Show that the relation ≤induces a partial order-
ing on the set of equivalence classes of terms modulo the equivalence
relation ∼=.
(c) Prove that two terms have a least upper bound iﬀthey have a
most general uniﬁer (use a separating substitution, see Section 8.5).
(d) Prove that any two terms always have a greatest lower bound.
Remark: The structure of the set of equivalence classes of terms mod-
ulo ∼= under the partial ordering ≤has been studied extensively in
Huet, 1976. Huet has shown that this set is well founded, that every
subset has a greatest lower bound, and that every bounded subset
has a least upper bound.
8.5 The Resolution Method for First-Order Logic
Recall that we are considering ﬁrst-order languages without equality. Also,
recall that even though we usually omit quantiﬁers, clauses are universally
quantiﬁed sentences. We extend the deﬁnition of a resolvent given in deﬁnition
4.3.2 to arbitrary clauses using the notion of a most general uniﬁer.
8.5.1 Deﬁnition of the Method
First, we deﬁne the concept of a separating pair of substitutions.

396
8/Resolution In First-Order Logic
Deﬁnition 8.5.1
Given two clauses A and A′, a separating pair of substi-
tutions is a pair of substitutions ρ and ρ′ such that:
ρ has support FV (A), ρ′ has support FV (A′), for every variable x in A,
ρ(x) is a variable, for every variable y in A′, ρ′(y) is a variable, ρ and ρ′ are
bijections, and the range of ρ and the range of ρ′ are disjoint.
Given a set S of literals, we say that S is positive if all literals in S are
atomic formulae, and we say that S is negative if all literals in S are negations
of atomic formulae. If a set S is positive or negative, we say that the literals in
S are of the same sign. Given a set of literals S = {A1, ..., Am}, the conjugate
of S is deﬁned as the set
S = {A1, ..., Am}
of conjugates of literals in S. If S is a positive set of literals we let |S| = S,
and if S is a negative set of literals, we let |S| = S.
Deﬁnition 8.5.2
Given two clauses A and B, a clause C is a resolvent of
A and B iﬀthe following holds:
(i) There is a subset A′ = {A1, ..., Am} ⊆A of literals all of the same
sign, a subset B′ = {B1, ..., Bn} ⊆B of literals all of the opposite sign of the
set A′, and a separating pair of substitutions (ρ, ρ′) such that the set
|ρ(A′) ∪ρ′(B′)|
is uniﬁable;
(ii) For some most general uniﬁer σ of the set
|ρ(A′) ∪ρ′(B′)|,
we have
C = σ(ρ(A −A′) ∪ρ′(B −B′)).
EXAMPLE 8.5.1
Let
A = {¬P(z, a), ¬P(z, x), ¬P(x, z)}
and
B = {P(z, f(z)), P(z, a)}.
Let
A′ = {¬P(z, a), ¬P(z, x)}
and
B′ = {P(z, a)},
ρ = (z1/z),
ρ′ = (z2/z).
Then,
|ρ(A′) ∪ρ′(B′)| = {P(z1, a), P(z1, x), P(z2, a)}

8.5 The Resolution Method for First-Order Logic
397
is uniﬁable,
σ = (z1/z2, a/x)
is a most general uniﬁer, and
C = {¬P(a, z1), P(z1, f(z1))}
is a resolvent of A and B.
If we take A′ = A, B′ = {P(z, a)},
|ρ(A′) ∪ρ′(B′)| = {P(z1, a), P(z1, x), P(x, z1), P(z2, a)}
is also uniﬁable,
σ = (a/z1, a/z2, a/x)
is the most general uniﬁer, and
C = {P(a, f(a))}
is a resolvent.
Hence, two clauses may have several resolvents.
The generalization of deﬁnition 4.3.3 of a resolution DAG to the ﬁrst-
order case is now obvious.
Deﬁnition 8.5.3
Given a set S = {C1, ..., Cn} of ﬁrst-order clauses, a res-
olution DAG for S is any ﬁnite set
G = {(t1, R1), ..., (tm, Rm)}
of distinct DAGs labeled in the following way:
(1) The leaf nodes of each underlying tree ti are labeled with clauses in
S.
(2) For every DAG (ti, Ri), every nonleaf node u in ti is labeled with
some triple (C, (ρ, ρ′), σ), where C is a clause, (ρ, ρ′) is a separating pair of
substitutions, σ is a substitution and the following holds:
For every nonleaf node u in ti, u has exactly two successors u1 and u2,
and if u1 is labeled with a clause C1 and u2 is labeled with a clause C2 (not
necessarily distinct from C1), then u is labeled with the triple (C, (ρ, ρ′), σ),
where (ρ, ρ′) is a separating pair of substitutions for C1 and C2 and C is the
resolvent of C1 and C2 obtained with the most general uniﬁer σ.
A resolution DAG is a resolution refutation iﬀit consists of a single DAG
(t, R) whose root is labeled with the empty clause. The nodes of a DAG that
are not leaves are also called resolution steps.

398
8/Resolution In First-Order Logic
We will often use a simpliﬁed form of the above deﬁnition by dropping
(ρ, ρ′) and σ from the interior nodes, and consider that nodes are labeled with
clauses. This has the eﬀect that it is not always obvious how a resolvent is
obtained.
EXAMPLE 8.5.2
Consider the following clauses:
C1 = {¬P(z1, a), ¬P(z1, x), ¬P(x, z1)},
C2 = {P(z2, f(z2)), P(z2, a)}
and
C3 = {P(f(z3), z3), P(z3, a)}.
The following is a resolution refutation:
C2
C1
C3
({P(a, f(a))}, (Id, Id),
(a/z1, a/z2, a/x))
({P(f(a), a)}, (Id, Id),
(a/z1, a/x, a/z3))
({¬P(a, f(a))}, (Id, Id), (f(a)/z1, a/x))
( , (Id, Id), Id)
8.5.2 Soundness of the Resolution Method
In order to prove the soundness of the resolution method, we prove the fol-
lowing lemma, analogous to lemma 4.3.1.
Lemma 8.5.1
Given two clauses A and B, let C = σ(ρ(A −A′) ∪ρ′(B −
B′)) be any resolvent of A and B, for some subset A′ ⊆A of literals of A,
subset B′ ⊆B of literals of B, separating pair of substitutions (ρ, ρ′), with
ρ = (z1/x1, ..., zm/xm), ρ′ = (zm+1/y1, ..., zm+n/yn) and most general uniﬁer
σ = (t1/u1, ..., tk/uk), where {u1, ..., uk} is a subset of {z1, ..., zm+n}. Also,
let {v1, ..., vp} = FV (C). Then,
|= (∀x1...∀xmA ∧∀y1...∀ynB) ⊃∀v1...∀vpC.
Proof : We show that we can constuct a G-proof for
(∀x1...∀xmA ∧∀y1...∀ynB) →∀v1...∀vpC.

8.5 The Resolution Method for First-Order Logic
399
Note that {z1, ..., zm+n} −{u1, ..., uk} is a subset of {v1, ..., vp}. First, we
perform p ∀: right steps using p entirely new variables w1,...,wp. Let
σ′ = σ ◦(w1/v1, ..., wp/vp) = (t′
1/z1, ..., t′
m+n/zm+n),
be the substitution obtained by composing σ and the substitution replacing
each occurrence of the variable vi by the variable wi. Then, note that the
support of σ′ is disjoint from the set {w1, ..., wp}, which means that for every
tree t,
σ′(t) = t[t′
1/z1]...[t′
m+n/zm+n]
(the order being irrelevant). At this point, we have the sequent
(∀x1...∀xmA ∧∀y1...∀ynB) →σ′(ρ(A −A′)), σ′(ρ′(B −B′)).
Then apply the ∧: left rule, obtaining
∀x1...∀xmA, ∀y1...∀ynB →σ′(ρ(A −A′)), σ′(ρ′(B −B′)).
At this point, we apply m+n ∀: left rules as follows: If ρ(xi) is some variable
uj, do the substitution t′
j/xi, else ρ(xi) is some variable vj not in {u1, .., uk},
do the substitution wj/vj.
If ρ′(yi) is some variable uj, do the substitution t′
j/yi, else ρ′(yj) is some
variable vj not in {u1, ..., uk}, do the substitution wj/vj.
It is easy to verify that at the end of these steps, we have the sequent
(σ′(ρ(A −A′)), Q), (σ′(ρ′(B −B′)), Q) →σ′(ρ(A −A′)), σ′(ρ′(B −B′))
where Q = σ′(ρ(A′)) and Q = σ′(ρ′(B′)) are conjugate literals, because σ is
a most general uniﬁer of the set |ρ(A′) ∪ρ′(B′)|.
Hence, we have a quantiﬁer-free sequent of the form
(A1 ∨Q), (A2 ∨¬Q) →A1, A2,
and we conclude that this sequent is valid using the proof of lemma 4.3.1.
As a consequence, we obtain the soundness of the resolution method.
Lemma 8.5.2
(Soundness of resolution without equality) If a set of clauses
has a resolution refutation DAG, then S is unsatisﬁable.
Proof : The proof is identical to the proof of lemma 4.3.2, but using
lemma 8.5.1, as opposed to lemma 4.3.1.

400
8/Resolution In First-Order Logic
8.5.3 Completeness of the Resolution Method
In order to prove the completeness of the resolution method for ﬁrst-order
languages without equality, we shall prove the following lifting lemma.
Lemma 8.5.3
(Lifting lemma) Let A and B be two clauses, σ1 and σ2 two
substitutions such that σ1(A) and σ2(B) are ground, and assume that D is a
resolvent of the ground clauses σ1(A) and σ2(B). Then, there is a resolvent
C of A and B and a substitution θ such that D = θ(C).
Proof : First, let (ρ, ρ′) be a separating pair of substitutions for A and
B. Since ρ and ρ′ are bijections they have inverses ρ−1 and ρ
′−1. Let σ be
the substitution formed by the union of ρ−1 ◦σ1 and ρ
′−1 ◦σ2, which is well
deﬁned, since the supports of ρ−1 and ρ
′−1 are disjoint. It is clear that
σ(ρ(A)) = σ1(A)
and
σ(ρ′(B)) = σ2(B).
Hence, we can work with ρ(A) and ρ′(B), whose sets of variables are disjoint.
If D is a resolvent of the clauses σ1(A) and σ2(B), there is a ground literal Q
such that σ(ρ(A)) contains Q and σ(ρ′(B)) contains its conjugate. Assume
that Q is positive, the case in which Q is negative being similar. Then, there
must exist subsets A′ = {A1, ..., Am} of A and B′ = {¬B1, ..., ¬Bn} of B,
such that
σ(ρ(A1)) = ... = σ(ρ(Am)) = σ(ρ′(B1)) = ..., σ(ρ′(Bn)) = Q,
and σ is a uniﬁer of ρ(A′) ∪ρ′(B′). By theorem 8.4.1, there is a most general
uniﬁer λ and a substitution θ such that
σ = λ ◦θ.
Let C be the resolvent
C = λ(ρ(A −A′) ∪ρ′(B −B′)).
Clearly,
D = (σ(ρ(A)) −{Q}) ∪(σ(ρ′(B)) −{¬Q})
= (σ(ρ(A −A′)) ∪σ(ρ′(B −B′)))
= θ(λ(ρ(A −A′) ∪ρ′(B −B′))) = θ(C).
Using the above lemma, we can now prove the following lemma which
shows that resolution DAGs of ground instances of clauses can be lifted to
resolution DAGs using the original clauses.
Lemma 8.5.4
(Lifting lemma for resolution refutations) Let S be a ﬁnite
set of clauses, and Sg be a set of ground instances of S, so that every clause in

8.5 The Resolution Method for First-Order Logic
401
Sg is of the form σi(Ci) for some clause Ci in S and some ground substitution
σi.
For any resolution DAG Hg for Sg, there is a resolution DAG H for S,
such that the DAG Hg is a homomorphic image of the DAG H in the following
sense:
There is a function F : H →Hg from the set of nodes of H to the set of
nodes of Hg, such that, for every node u in H, if u1 and u2 are the immediate
descendants of u, then F(u1) and F(u2) are the immediate descendants of
F(u), and if the clause C (not necessarily in Sg) is the label of u, then F(u)
is labeled by the clause θ(C), where θ is some ground substitution.
Proof : We prove the lemma by induction on the underlying tree of Hg.
(i) If Hg has a single resolution step, we have clauses σ1(A), σ2(B) and
their resolvent D. By lemma 8.5.3, there exists a resolvent C of A and B
and a substitution θ such that θ(C) = D. Note that it is possible that A and
B are distinct, but σ1(A) and σ2(B) are not. In the ﬁrst case, we have the
following DAGs:
DAG
Hg
σ1(A) = σ2(B)
D
DAG
H
A
B
C
The homomorphism F is such that F(e) = e, F(1) = 1 and F(2) = 1.
In the second case, σ1(A) ̸= σ2(B), but we could have A = B. Whether
or not A = B, we create the following DAG H with three distinct nodes, so
that the homomorphism is well deﬁned:
DAG
Hg
σ1(A)
σ2(B)
D
DAG
H
A
B
C
The homomorphism F is the identity on nodes.
(ii) If Hg has more than one resolution step, it is of the form either
(ii)(a)
DAG
Hg
G1
G2
A′
B′
D

402
8/Resolution In First-Order Logic
where A′ and B′ are distinct, or of the form
(ii)(b)
DAG
Hg
G1
A′ = B′
D
if A′ = B′.
(a) In the ﬁrst case, by the induction hypothesis, there are DAGs H1
and H2 and homomorphisms F1 : H1 →G1 and F2 : H2 →G2, where H1 is
rooted with some formula A and H2 is rooted with some formula B, and for
some ground substitutions θ1 and θ2, we have, A′ = θ1(A) and B′ = θ2(B).
By lemma 8.5.3, there is a resolvent C of A and B and a substitution θ such
that θ(C) = D. We can construct H as the DAG obtained by making C as
the root, and even if A = B, by creating two distinct nodes 1 and 2, with 1
labeled A and 2 labeled B:
DAG
H
H1
H2
A
B
C
The homomorphism F : H →Hg is deﬁned such that F(e) = e, F(1) =
1, F(2) = 2, and it behaves like F1 on H1 and like F2 on H2. The root clause
C is mapped to θ(C) = D.
(b) In the second case, by the induction hypothesis, there is a DAG H1
rooted with some formula A and a homomorphism F1 : H1 →G1, and for
some ground substitution θ1, we have A′ = θ1(A). By lemma 8.5.3, there is
a resolvent C of A with itself, and a substitution θ such that θ(C) = D. It is
clear that we can form H so that C is a root node with two edges connected to
A, and F is the homomorphism such that F(e) = e, F(1) = 1, and F behaves
like F1 on H1.
DAG
H
H1
A
C
The clause C is mapped onto D = θ(C). This concludes the proof.

8.5 The Resolution Method for First-Order Logic
403
EXAMPLE 8.5.3
The following shows a lifting of the ground resolution of example 8.3.1
for the clauses:
C1 = {¬P(z1, a), ¬P(z1, x), ¬P(x, z1)}
C2 = {P(z2, f(z2)), P(z2, a)}
C3 = {P(f(z3), z3), P(z3, a)}.
Recall that the ground instances are
G1 = {¬P(a, a)}
G2 = {P(a, f(a)), P(a, a)}
G3 = {P(f(a), a), P(a, a)}
G4 = {¬P(f(a), a), ¬P(a, f(a))},
and the substitutions are
σ1 = (a/z2)
σ2 = (a/z1, a/x)
σ3 = (a/z3)
σ4 = (f(a)/z1, a/x).
Ground resolution-refutation Hg
for the set of ground clauses G1, G2, G3, G4
G2
G1
G3
G4
{P(a, f(a))}
{P(f(a), a)}
{¬P(a, f(a))}
Lifting H of the above resolution refutation
for the clauses C1, C2, C3
C2
C1
C3
C1
{P(a, f(a))}
{P(f(a), a)}
{¬P(a, f(a))}
The homomorphism is the identity on the nodes, and the substitutions
are, (a/z2) for node 11 labeled C2, (a/z1, a/x) for node 12 labeled C1,

404
8/Resolution In First-Order Logic
(a/z3) for node 212 labeled C3, and (f(a)/z1, a/x) for node 22 labeled
C1.
Note that this DAG is not as concise as the DAG of example 8.5.1. This
is because is has been designed so that there is a homomorphism from
H to Hg.
As a consequence of the lifting theorem, we obtain the completeness of
resolution.
Theorem 8.5.1
(Completeness of resolution, without equality) If a ﬁnite
set S of clauses is unsatisﬁable, then there is a resolution refutation for S.
Proof : By the Skolem-Herbrand-G¨odel theorem (theorem 7.6.1, or its
corollary), S is unsatisﬁable iﬀa conjunction Sg of ground substitution in-
stances of clauses in S is unsatisﬁable. By the completeness of ground reso-
lution (lemma 8.3.1), there is a ground resolution refutation Hg for Sg. By
lemma 8.5.4, this resolution refutation can be lifted to a resolution refutation
H for S. This concludes the proof.
Actually, we can also prove the following type of Herbrand theorem for
the resolution method, using the constructive nature of lemma 7.6.2.
Theorem 8.5.2
(A Herbrand-like theorem for resolution) Consider a ﬁrst-
order language without equality. Given any prenex sentence A whose matrix
is in CNF, if A →is LK-provable, then a resolution refutation of the clause
form of A can be obtained constructively.
Proof : By lemma 7.6.2, a compound instance C of the Skolem form B
of A can be obtained constructively. Observe that the Skolem form B of A is
in fact a clause form of A, since A is in CNF. But C is in fact a conjunction of
ground instances of the clauses in the clause form of A. Since ¬C is provable,
the search procedure will give a proof that can be converted to a GCNF ′-
proof.
Since theorem 4.3.1 is constructive, we obtain a ground resolution
refutation Hg.
By the lifting lemma 8.5.4, a resolution refutation H can
be constructively obtained for Sg. Hence, we have shown that a resolution
refutation for the clause form of A can be constructively obtained from an
LK-proof of A →.
It is likely that theorem 8.5.2 has a converse, but we do not have a proof
of such a result. A simpler result is to prove the converse of lemma 8.5.4,
the lifting theorem. This would provide another proof of the soundness of
resolution. It is indeed possible to show that given any resolution refutation H
of a set S of clauses, a resolution refutation Hg for a certain set Sg of ground
instances of S can be constructed.
However, the homomorphism property
does not hold directly, and one has to exercise care in the construction. The
interested reader should consult the problems.
It should be noted that a Herbrand-like theorem for the resolution
method and a certain Hilbert system has been proved by Joyner in his Ph.D

PROBLEMS
405
thesis (Joyner, 1974). However, these considerations are somewhat beyond
the scope of this text, and we will not pursue this matter any further.
PROBLEMS
8.5.1.
Give separating pairs of substitutions for the following clauses:
{P(x, y, f(z))}, {P(y, z, f(z))}
{P(x, y), P(y, z)}, {Q(y, z), P(z, f(y))}
{P(x, g(x))}, {P(x, g(x))}
8.5.2.
Find all resolvents of the following pairs of clauses:
{P(x, y), P(y, z)}, {¬P(u, f(u))}
{P(x, x), ¬R(x, f(x))}, {R(x, y), Q(y, z)}
{P(x, y), ¬P(x, x), Q(x, f(x), z)}, {¬Q(f(x), x, z), P(x, z)}
{P(x, f(x), z), P(u, w, w)}, {¬P(x, y, z), ¬P(z, z, z)}
8.5.3.
Establish the unsatisﬁability of each of the following formulae using
the resolution method.
(∀x∃yP(x, y) ∧∃x∀y¬P(x, y))
(∀x∃y∃z(L(x, y) ∧L(y, z) ∧Q(y) ∧R(z) ∧(P(z) ≡R(x)))∧
∀x∀y∀z((L(x, y) ∧L(y, z)) ⊃L(x, z)) ∧∃x∀y¬(P(y) ∧L(x, y)))
8.5.4.
Consider the following formulae asserting that a binary relation is
symmetric, transitive, and total:
S1 : ∀x∀y(P(x, y) ⊃P(y, x))
S2 : ∀x∀y∀z((P(x, y) ∧P(y, z)) ⊃P(x, z))
S3 : ∀x∃yP(x, y)
Prove by resolution that
S1 ∧S2 ∧S3 ⊃∀xP(x, x).
In other words, if P is symmetric, transitive and total, then P is
reﬂexive.
8.5.5.
Complete the details of the proof of lemma 8.5.1.

406
8/Resolution In First-Order Logic
∗8.5.6.
(a) Prove that given a resolution refutation H of a set S of clauses, a
resolution refutation Hg for a certain set Sg of ground instances of S
can be constructed.
Apply the above construction to the following refutation:
{¬P(a), Q(a)}
{P(x)}
{¬P(f(a)), ¬Q(a)}
{Q(a)}
{¬Q(a)}
(b) Using (a), give another proof of the soundness of the resolution
method.
∗8.5.7.
As in the propositional case, another way of presenting the resolution
method is as follows. Given a (ﬁnite) set S of clauses, let
R(S) = S ∪{C | C is a resolvent of two clauses in S}.
Also, let
R0(S) = S,
Rn+1(S) = R(Rn(S)), (n ≥0), and let
R∗(S) =

n≥0
Rn(S).
(a) Prove that S is unsatisﬁable if and only if R∗(S) is unsatisﬁable.
(b) Prove that if S is ﬁnite, there is some n ≥0 such that
R∗(S) = Rn(S).
(c) Prove that there is a resolution refutation for S if and only if the
empty clause
is in R∗(S).
(d) Prove that S is unsatisﬁable if and only if
belongs to R∗(S).
8.5.8.
Prove that the resolution method is still complete if the resolution
rule is restricted to clauses that are not tautologies (that is, clauses
not containing both A and ¬A for some atomic formula A).
∗8.5.9.
We say that a clause C1 subsumes a clause C2 if there is a substitution
σ such that σ(C1) is a subset of C2. In the version of the resolution
method described in problem 8.5.7, let
R1(S) = R(S) −{C | C is subsumed by some clause in R(S)}.
Let R0
1 = S,
Rn+1
1
(S) = R1(Rn
1 (S)) and
R∗
1(S) =

n≥0
Rn
1 (S).

8.6 A Glimpse at Paramodulation
407
Prove that S is unsatisﬁable if and only if
belongs to R∗
1(S).
8.5.10. The resolution method described in problem 8.5.7 can be modiﬁed
by introducing the concept of factoring. Given a clause C, if C′ is
any subset of C and C′ is uniﬁable, the clause σ(C) where σ is a
most general uniﬁer of C′ is a factor of C. The factoring rule is the
rule that allows any factor of a clause to be added to R(S). Consider
the simpliﬁcation of the resolution rule in which a resolvent of two
clauses A and B is obtained by resolving sets A′ and B′ consisting
of a single literal.
This restricted version of the resolution rule is
sometimes called binary resolution.
(a) Show that binary resolution together with the factoring rule is
complete.
(b) Show that the factoring rule can be restricted to sets C′ consisting
of a pair of literals.
(c) Show that binary resolution alone is not complete.
8.5.11. Prove that the resolution method is also complete for inﬁnite sets of
clauses.
8.5.12. Write a computer program implementing the resolution method.
8.6 A Glimpse at Paramodulation
As we have noted earlier, equality causes complications in automatic the-
orem proving.
Several methods for handling equality with the resolution
method have been proposed, including the paramodulation method (Robin-
son and Wos, 1969), and the E-resolution method (Morris, 1969; Anderson,
1970). Due to the lack of space, we will only deﬁne the paramodulation rule,
but we will not give a full treatment of this method.
In order to deﬁne the paramodulation rule, it is convenient to assume
that the factoring rule is added to the resolution method. Given a clause
A, if A′ is any subset of A and A′ is uniﬁable, the clause σ(A) where σ is a
most general uniﬁer of A′ is a factor of A. Using the factoring rule, it is easy
to see that the resolution rule can be simpliﬁed, so that a resolvent of two
clauses A and B is obtained by resolving sets A′ and B′ consisting of a single
literal. This restricted version of the resolution rule is sometimes called binary
resolution (this is a poor choice of terminology since both this restricted rule
and the general resolution rule take two clauses as arguments, but yet, it is
used in the literature!). It can be shown that binary resolution alone is not
complete, but it is easy to show that it is complete together with the factoring
rule (see problem 8.5.10).
The paramodulation rule is a rule that treats an equation s .= t as a (two
way) rewrite rule, and allows the replacement of a subterm r uniﬁable with

408
8/Resolution In First-Order Logic
s (or t) in an atomic formula Q, by the other side of the equation, modulo
substitution by a most general uniﬁer.
More precisely, let
A = ((s .= t) ∨C)
be a clause containing the equation s .= t, and
B = (Q ∨D)
be another clause containing some literal Q (of the form Pt1...tn or ¬Pt1...tn,
for some predicate symbol P of rank n, possibly the equality symbol .=, in
which case n = 2), and assume that for some tree address u in Q, the subterm
r = Q/u is uniﬁable with s (or that r is uniﬁable with t). If σ is a most
general uniﬁer of s and r, then the clause
σ(C ∨Q[u ←t] ∨D)
(or σ(C ∨Q[u ←s] ∨D), if r and t are uniﬁable) is a paramodulant of A and
B. (Recall from Subsection 2.2.5, that Q[u ←t] (or Q[u ←s]) is the result of
replacing the subtree at address u in Q by t (or s)).
EXAMPLE 8.6.1
Let
A = {f(x, h(y)) .= g(x, y), P(x)},
B = {Q(h(f(h(x), h(a))))}.
Then
{Q(h(g(h(z), h(a)))), P(h(z))}
is a paramodulant of A and B, in which the replacement is performed
in B at address 11.
EXAMPLE 8.6.2
Let
A = {f(g(x), x) .= h(a)},
B = {f(x, y) .= h(y)}.
Then,
{h(z) .= h(a)}
is a paramodulant of A and B, in which the replacement is performed
in A at address e.
It can be shown that the resolution method using the (binary) resolution
rule, the factoring rule, and the paramodulation rule, is complete for any ﬁnite
set S of clauses, provided that the reﬂexity axiom and the functional reﬂexivity
axioms are added to S. The reﬂexivity axiom is the clause
{x .= x},

Notes and Suggestions for Further Reading
409
and the functional reﬂexivity axioms are the clauses
{f(x1, ..., xn) .= f(x1, ..., xn)},
for each function symbol f occurring in S, of any rank n > 0.
The proof that this method is complete is more involved than the proof
for the case of a ﬁrst-language without equality, partly because the lifting
lemma does not extend directly. It can also be shown that paramodulation is
complete without the functional reﬂexivity axioms, but this is much harder.
For details, the reader is referred to Loveland, 1978.
Notes and Suggestions for Further Reading
The resolution method has been studied extensively, and there are many re-
ﬁnements of this method. Some of the reﬁnements are still complete for all
clauses (linear resolution, model elimination), others are more eﬃcient but
only complete for special kinds of clauses (unit or input resolution). For a
detailed exposition of these methods, the reader is referred to Loveland, 1978;
Robinson, 1979, and to the collection of original papers compiled in Siekmann
and Wrightson, 1983. One should also consult Boyer and Moore, 1979, for
advanced techniques in automatic theorem proving, induction in particular.
For a more introductory approach, the reader may consult Bundy, 1983, and
Kowalski, 1979.
The resolution method has also been extended to higher-order logic by
Andrews. The interested reader should consult Andrews, 1971; Pietrzykowski,
1973; and Huet, 1973.

Chapter 9
SLD-Resolution And
Logic Programming
(PROLOG)
9.1 Introduction
We have seen in Chapter 8 that the resolution method is a complete procedure
for showing unsatisﬁability. However, ﬁnding refutations by resolution can be
a very expensive process in the general case. If subclasses of formulae are
considered, more eﬃcient procedures for producing resolution refutations can
be found. This is the case for the class of Horn clauses. A Horn clause is a
disjunction of literals containing at most one positive literal. For sets of Horn
clauses, there is a variant of resolution called SLD-resolution, which enjoys
many nice properties.
SLD-resolution is a special case of a reﬁnement of
the resolution method due to Kowalski and Kuehner known as SL-resolution
(Kowalski and Kuehner, 1970), a variant of Model Elimination (Loveland,
1978), and applies to special kinds of Horn clauses called deﬁnite clauses. We
shall present SLD-resolution and show its completeness for Horn clauses.
SLD-resolution is also interesting because it is the main computation
procedure used in PROLOG. PROLOG is a programming language based on
logic, in which a computation is in fact a refutation. The idea to deﬁne a
program as a logic formula and view a refutation as a computation is a very
fruitful one, because it reduces the complexity of proving the correctness of
programs. In fact, it is often claimed that logic programs are obviously cor-
rect, because these programs “express” the assertions that they should satisfy.
However, this is not quite so, because the notion of correctness is relative, and
410

9.2 GCNF ′-Proofs in SLD-Form
411
one still needs to deﬁne the semantics of logic programs in some independent
fashion. This will be done in Subsection 9.5.4, using a model-theoretic seman-
tics. Then, the correctness of SLD-resolution (as a computation procedure)
with respect to the model-theoretic semantics will be proved.
In this chapter, as in Chapter 8, we begin by studying SLD-resolution
in the propositional case, and then use the lifting technique to extend the
results obtained in the propositional case to the ﬁrst-order case. Fortunately,
the lifting process goes very smoothly.
As in Chapter 4, in order to prove the completeness of SLD-resolution
for propositional Horn clauses, we ﬁrst show that Horn clauses have GCNF ′-
proofs of a certain kind, that we shall call GCNF ′-proofs in SLD-form. Then,
we show that every GCNF ′-proof in SLD-form can be mapped into a linear
SLD-refutation.
Hence, the completeness proof for SLD-resolution is con-
structive.
The arguments used for showing that every unsatisﬁable Horn clause
has a GCNF ′-proof in SLD-form are quite basic and combinatorial in nature.
Once again, the central concept is that of proof transformation.
We conclude this chapter by discussing the notion of logic program and
the idea of viewing SLD-resolution as a computation procedure.
We pro-
vide a rigorous semantics for logic programs, and show the correctness and
completeness of SLD-resolution with respect to this semantics.
The contents of Section 9.5 can be viewed as the theoretical foundations
of logic programming, and PROLOG in particular.
9.2 GCNF ′-Proofs in SLD-Form
First, we shall prove that every unsatisﬁable propositional Horn clause has
a GCNF ′-proof of a certain type, called a proof in SLD-form. In order to
explain the method for converting a GCNF ′-proof into an SLD-resolution
proof, it is convenient to consider the special case of sets of Horn clauses,
containing exactly one clause containing no positive literals (clause of the
form {¬P1, ..., ¬Pm}). Other Horn clauses will be called deﬁnite clauses.
9.2.1 The Case of Deﬁnite Clauses
These concepts are deﬁned as follows.
Deﬁnition 9.2.1
A Horn clause is a disjunction of literals containing at
most one positive literal. A Horn clause is a deﬁnite clause iﬀit contains a
(single) positive literal. Hence, a deﬁnite clause is either of the form
{Q},
or
{¬P1, ..., ¬Pm, Q}.

412
9/SLD-Resolution And Logic Programming (PROLOG)
A Horn clause of the form
{¬P1, ..., ¬Pm}
is called a negative clause or goal clause.
For simplicity of notation, a clause {Q} will also be denoted by Q. In
the rest of this section, we restrict our attention to sets S of clauses consisting
of deﬁnite clauses except for one goal clause. Our goal is to show that for
a set S consisting of deﬁnite clauses and of a single goal B, if S is GCNF ′-
provable, then there is a proof having the property that whenever a ∨: left
rule is applied to a deﬁnite clause {¬P1, ..., ¬Pm, Q}, the rule splits it into
{¬P1, ..., ¬Pm} and {Q}, the sequent containing {Q} is an axiom, and the
sequent containing {¬P1, ..., ¬Pm} does not contain ¬Q.
EXAMPLE 9.2.1
Consider the set S of Horn clauses with goal {¬P1, ¬P2} given by:
S = {{P3}, {P4}, {¬P1, ¬P2}, {¬P3, ¬P4, P1}, {¬P3, P2}}.
The following is a GCNF ′-proof:
P3, ¬P3 →
P4, ¬P4 →
P3, P4, {¬P3, ¬P4} →
¬P1, P1 →
¬P2, P2 →
P3, ¬P3 →
P3, ¬P2, {¬P3, P2} →
P3, {¬P1, ¬P2}, P1, {¬P3, P2} →
P3, P4, {¬P1, ¬P2}, {¬P3, ¬P4, P1}, {¬P3, P2} →
Another proof having the properties mentioned above is
¬P1, P1 →
P3, ¬P3 →P4, ¬P4 →
P3, P4, {¬P3, ¬P4} →
P3, P4, ¬P1, {¬P3, ¬P4, P1} →
¬P2, P2 →
P3, ¬P3 →
P3, ¬P2, {¬P3, P2} →
P3, P4, {¬P1, ¬P2}, {¬P3, ¬P4, P1}, {¬P3, P2} →
Observe that in the above proof, the ∨: left rule is ﬁrst applied to the
goal clause {¬P1, ¬P2}, and then it is applied to split each deﬁnite clause
{¬Q1, ..., ¬Qm, Q} into {¬Q1, ..., ¬Qm} and {Q}, in such a way that the se-
quent containing {Q} is the axiom Q, ¬Q →.
Note also that each clause
{¬Q1, ..., ¬Qm} resulting from splitting a deﬁnite clause as indicated above is
the only goal clause in the sequent containing it.

9.2 GCNF ′-Proofs in SLD-Form
413
9.2.2 GCNF ′-Proofs in SLD-Form
The above example suggests that if a set of deﬁnite clauses with goal B is
GCNF ′-provable, it has a proof obtained by following rules described below,
starting with a one-node tree containing the goal B = {¬P1, ..., ¬Pm}:
(1) If no leaf of the tree obtained so far contains a clause consisting of a
single negative literal ¬Q then,
As long as the tree is not a GCNF ′-proof tree, apply the ∨: left rule
to each goal clause B of the form {¬Q1, ..., ¬Qm} (m > 1) in order to form
m immediate descendants of B, else
(2) For every goal clause consisting of a single negative literal ¬Q, ﬁnd a
deﬁnite clause {¬P1, ..., ¬Pk, Q} (or Q when k = 0), and split {¬P1, ..., ¬Pk,
Q} using the ∨: left rule in order to get the axiom ¬Q, Q →in one node,
{¬P1, ..., ¬Pm} in the other, and drop ¬Q from that second node.
Go back to (1).
In is not clear that such a method works, and that in step (2), the
existence of a deﬁnite clause {¬P1, ..., ¬Pk, Q} such that Q cancels ¬Q is
guaranteed.
However, we are going to prove that this is always the case.
First, we deﬁne the type of proofs arising in the procedure described above.
Deﬁnition 9.2.2
Given a set S of clauses consisting of deﬁnite clauses and
of a single goal B, a GCNF ′-proof is in SLD-form iﬀthe conditions below
are satisﬁed:
For every node B in the tree that is not an axiom:
(1) If the set of clauses labeling that node does not contain any clause
consisting of a single negative literal ¬Q, then it contains a single goal clause
of the form {¬Q1, ..., ¬Qm} (m > 1), and the ∨: left rule is applied to this
goal clause in order to form m immediate descendants of B.
(2) If the set of clauses labeling that node contains some single negative
literal, for such a clause ¬Q, there is some deﬁnite clause
{¬P1, ..., ¬Pk, Q},
(k > 0), such that the ∨: left rule is applied to
{¬P1, ..., ¬Pk, Q}
in order to get the axiom ¬Q, Q →and a sequent containing the single goal
clause {¬P1, ..., ¬Pk}.
We are now going to prove that if a set of clauses consisting of deﬁnite
clauses and of a single goal clause if provable in GCNF ′, then it has a proof
in SLD-form. For this, we are going to perform proof transformations, and
use simple combinatorial properties.

414
9/SLD-Resolution And Logic Programming (PROLOG)
9.2.3 Completeness of Proofs in SLD-Form
First, we need to show that every GCNF ′-provable set of clauses has a proof
in which no weakenings takes place. This is deﬁned as follows.
Deﬁnition 9.2.3
A GCNF ′-proof is without weakenings iﬀevery applica-
tion of the ∨: left rule is of the form:
Γ, A1, ..., Am →
Γ, B →
Γ, (A1 ∨B), ..., (Am ∨B) →
We have the following normal form lemma.
Lemma 9.2.1
If a set S of clauses is GCNF ′-provable, then a GCNF ′-
proof without weakenings and in which all the axioms contain only literals
can be constructed.
Proof : Since G′ is complete, S →has a G′-proof T. By lemma 6.3.1
restricted to propositions, S →
has a G′-proof T ′ in which all axioms are
atomic.
Using lemma 4.2.2, S →has a G′-proof T ′′ in which all axioms
are atomic, and in which all applications of the ∨: left rule precede all
applications of the ¬ : left rule. The tree obtained from T ′′ by retaining
the portion of the proof tree that does not contain ¬ : left inferences is the
desired GCNF ′-proof.
The following permutation lemma is the key to the conversion to SLD-
form.
Lemma 9.2.2
Let S be a set of clauses that has a GCNF ′-proof T. Then,
for any clause C in S having more than one literal, for any partition of the
literals in C into two disjunctions A and B such C = (A ∨B), there is a
GCNF ′-proof T ′ in which the ∨: left rule is applied to (A ∨B) at the root.
Furthermore, if the proof T of S is without weakenings and all axioms contain
only literals, the proof T ′ has the same depth as T.
Proof : Observe that representing disjunctions of literals as unordered
sets of literals is really a convenience aﬀorded by the associativity, commuta-
tivity and idempotence of ∨, but that this convenience does not aﬀect the com-
pleteness of G′. Hence, no matter how C is split into a disjunction (A∨B), the
sequent Γ, (A∨B) →is G′-provable. By converting a G′-proof of Γ, (A∨B) →
given by lemma 9.2.1 into a GCNF ′-proof, we obtain a GCNF ′-proof with-
out weakenings, and in which the ∨: left rule is applied to A and B only
after it is applied to (A ∨B). If the ∨: left rule applied at the root does not
apply to (A ∨B), it must apply to some other disjunction (C ∨D). Such a
proof T must be of the following form:

9.2 GCNF ′-Proofs in SLD-Form
415
Tree T
Π1
Π2
Γ, (A ∨B), (C ∨D) →
where Π1 is the tree
T1
Γ1, A →
S1
Γ1, B →
Γ1, (A ∨B) →
Tm
Γm, A →
Sm
Γm, B →
Γm, (A ∨B) →
R
Γ, (A ∨B), C →
and where Π2 is the tree
T ′
1
∆1, A →
S′
1
∆1, B →
∆1, (A ∨B) →
T ′
n
∆n, A →
S′
n
∆n, B →
∆n, (A ∨B) →
S
Γ, (A ∨B), D →
In the above proof, we have indicated the nodes to which the ∨: left
rule is applied, nodes that must exist since all axioms consist of literals. The
inferences above Γ, (A ∨B), C and below applications of the ∨: left rule to
(A ∨B) are denoted by R, and the similar inferences above Γ, (A ∨B), D are
denoted by S. We can transform T into T ′ by applying the ∨: left rule at
the root as shown below:
Tree T ′
Π′
1
Π′
2
Γ, (A ∨B), (C ∨D) →

416
9/SLD-Resolution And Logic Programming (PROLOG)
where Π′
1 is the tree
T1
Γ1, A →
Tm
Γm, A →
T ′
1
∆1, A →
T ′
n
∆n, A →
R
S
Γ, A, C →
Γ, A, D →
Γ, A, (C ∨D) →
and where Π′
2 is the tree
S1
Γ1, B →
Sm
Γm, B →
S′
1
∆1, B →
S′
n
∆n, B →
R
S
Γ, B, C →
Γ, B, D →
Γ, B, (C ∨D) →
Clearly, depth(T ′) = depth(T).
Note that T ′ is obtained from T by permutation of inferences. We need
another crucial combinatorial property shown in the following lemma.
Lemma 9.2.3
Let S be an arbitrary set of clauses such that the subset
of clauses containing more than one literal is the nonempty set {C1, ..., Cn}
and the subset consisting of the one-literal clauses is J. Assume that S is
GCNF ′-provable, and that we have a proof T without weakenings such that
all axioms consist of literals. Then, every axiom is labeled with a set of literals
of the form {L1, ..., Ln} ∪J, where each literal Li is in Ci, i = 1, ..., n.
Proof : We proceed by induction on proof trees. Since S contains at
least one clause with at least two literals and the axioms only contain literals,
depth(T) ≥1. If T has depth 1, then there is exactly one application of the
∨: rule and the proof is of the following form:
J, L1 →
J, L2 →
J, (L1 ∨L2) →

9.2 GCNF ′-Proofs in SLD-Form
417
Clearly, the lemma holds.
If T is a tree of depth k + 1, it is of the following form,
T1
Γ, A →
T2
Γ, B →
Γ, (A ∨B) →
where we can assume without loss of generality that Cn = (A ∨B). By the
induction hypothesis, each axiom of T1 is labeled with a set of clauses of the
form {L1, ..., Ln} ∪J, where each literal Li is in Ci for i = 1, ..., n −1, and
either Ln = A if A consists of a single literal, or Ln belongs to A. Similarly,
each axiom of T2 is labeled with a set of clauses of the form {L1, ..., Ln} ∪J,
where each literal Li is in Ci for i = 1, ..., n−1, and either Ln = B if B consists
of a single literal, or Ln belongs to B. Since the union of A and B is Cn,
every axiom of T is labeled with a set of clauses of the form {L1, ..., Ln} ∪J,
where each literal Li is in Ci, i = 1, ..., n. Hence, the lemma holds.
As a consequence, we obtain the following useful corollary.
Lemma 9.2.4
Let S be a set of Horn clauses. If S is GCNF ′-provable,
then S contains at least one clause consisting of a single positive literal, and
at least one goal (negative) clause.
Proof : It S is an axiom, this is obvious. Otherwise, by lemma 9.2.3, if
S is GCNF ′-provable, then it has a proof T without weakenings such that
every axiom is labeled with a set of literals of the form {L1, ..., Ln}∪J, where
each literal Li is in Ci, i = 1, ..., n, and J is the set of clauses in S consisting
of a single literal.
If J does not contain any positive literals, since every
Horn clause Ci contains a negative literal say ¬Ai, the set {¬A1, ..., ¬An}∪J
contains only negative literals, and so cannot be an axiom. If every clause in
J is positive and every clause Ci contains some positive literal say Ai, then
{A1, ..., An} ∪J contains only positive literals and cannot be an axiom.
In order to prove the main theorem of this section, we will need to
show that the provability of a set of Horn clauses with several goals (negative
clauses) reduces to the case of a set of Horn clauses with a single goal.
Lemma 9.2.5
Let S be a set of Horn clauses consisting of a set J of single
positive literals, goal clauses N1,...,Nk, and deﬁnite clauses C1,...,Cm contain-
ing at least two literals.
If S is GCNF ′-provable, then there is some i, 1 ≤i ≤k, such that
J ∪{C1, ..., Cm} ∪{Ni}
is GCNF ′-provable. Furthermore, if T is a GCNF ′-proof of S without weak-
enings and such that the axioms contain only literals, J ∪{C1, ..., Cm} ∪{Ni}
has a proof of depth less than or equal to the depth of T.

418
9/SLD-Resolution And Logic Programming (PROLOG)
Proof : We proceed by induction on proof trees. Let T be a GCNF ′-
proof of S without weakenings and such that all axioms contain only literals.
Case 1: J ∪{C1, ..., Cm} ∪{N1, ..., Nk} is an axiom. Then, one of the
positive literals in J must be the conjugate of some negative clause Ni, and
the lemma holds.
Case 2: The bottom ∨: left rule is applied to one of the Ni. Without
loss of generality, we can assume that it is N1 = {¬Q1, ..., ¬Qj, ¬P}.
Letting C = C1, ..., Cm, the proof is of the form
T1
J, C, N2, ..., Nk, {¬Q1, ..., ¬Qj} →
T2
J, C, N2, ..., Nk, ¬P →
J, C, N1, ..., Nk →
Observe that the bottom sequents of T1 and T2 satisfy the conditions of
the induction hypothesis. There are two subcases. If both
J, C1, ..., Cm, {¬Q1, ..., ¬Qj} →
and
J, C1, ..., Cm, ¬P →
are provable, then
J, C1, ..., Cm, {¬Q1, ..., ¬Qj, ¬P} →
is provable by application of the ∨: rule, and the lemma holds. If
J, C1, ..., Cm, Ni →
is provable for some i, 2 ≤i ≤k, then the lemma also holds.
Case 3: The bottom ∨: rule is applied to one of the Ci. Without loss
of generality, we can assume that it is C1 = {¬Q1, ..., ¬Qj, P}. There are two
subcases:
Case 3.1: Letting N = N1, ..., Nk, the proof is of the form
T1
J, C2, ..., Cm, N, {¬Q1, ..., ¬Qj} →
T2
J, P, C2, ..., Cm, N →
J, C1, ..., Cm, N →
Again the induction hypothesis applies to both T1 and T2. If
J, C2, ..., Cm, {¬Q1, ..., ¬Qj} →
is provable and
J, P, C2, ..., Cm, Ni →
is provable

9.2 GCNF ′-Proofs in SLD-Form
419
for some i, 1 ≤i ≤k, then by the ∨: rule,
J, C1, ..., Cm, Ni →
is also provable, and the lemma holds. If
J, C2, ..., Cm, Ni →
is provable for some i, 1 ≤i ≤k, then
J, C1, ..., Cm, Ni →
is also provable (using weakening in the last ∨: rule).
Case 3.2: Letting N = N1, ..., Nk, the proof is of the form
T1
J, C2, ..., Cm, N, {¬Q2, ..., ¬Qj, P} →
T2
J, C2, ..., Cm, ¬Q1, N →
J, C1, ..., Cm, N →
Applying the induction hypothesis, either
J, C2, ..., Cm, Ni, {¬Q2, ..., ¬Qj, P}
is provable for some i, 1 ≤i ≤k, and
J, C2, ..., Cm, ¬Q1 →
is provable, and by the ∨: rule, J, C1, ..., Cm, Ni is provable and the lemma
holds. Otherwise,
J, C2, ..., Cm, Ni
is provable for some i, 1 ≤i ≤k, and so J, C1, ..., Cm, Ni is also provable
using weakening in the last ∨: rule. This concludes the proof.
We are now ready to prove the main theorem of this section.
Theorem 9.2.1
(Completeness of proofs in SLD-form) If a set S consisting
of deﬁnite clauses and of a single goal B = {¬P1, ..., ¬Pn} is GCNF ′-provable,
then it has a GCNF ′-proof in SLD-form.
Proof : Assume that S is not an axiom.
By lemma 9.2.1, there is a
GCNF ′-proof T without weakenings, and such that all axioms consist of
literals. We proceed by induction on the depth of proof trees. If depth(T) = 1,
the proof is already in SLD-form (this is the base case of lemma 9.2.3). If
depth(T) > 1, by n applications of lemma 9.2.2, we obtain a proof tree T ′
having the same depth as T, such that the i-th inference using the ∨: left
rule is applied to {¬Pi, ..., ¬Pn}. Hence, letting C = C1, ..., Cm, the tree T ′ is
of the form:

420
9/SLD-Resolution And Logic Programming (PROLOG)
Tn−1
J, C, ¬Pn−1 →
Tn
J, C, ¬Pn →
J, C, {¬Pn−1, ¬Pn} →
...
T1
J, C, ¬P1 →
T2
J, C, ¬P2 →
J, C, {¬P3, ..., ¬Pn} →
J, C, {¬P2, ..., ¬Pn} →
J, C, {¬P1, ..., ¬Pn} →
where J is the set of clauses consisting of a single positive literal, and each
clause Ci has more than one literal. For every subproof rooted with J, C1, ...,
Cm, ¬Pi →, by lemma 9.2.3, each axiom is labeled with a set of literals
{L1, ..., Lm} ∪{¬Pi} ∪J,
where each Lj is in Cj, 1 ≤j ≤m. In particular, since each clause Cj contains
a single positive literal Aj, for every i, 1 ≤i ≤n, {A1, ..., Am} ∪{¬Pi} ∪J
must be an axiom. Clearly, either some literal in J is of the form Pi, or there
is some deﬁnite clause C = {¬Q1, ..., ¬Qp, Aj} among C1,...,Cm, with positive
literal Aj = Pi. In the ﬁrst case, J, C1, ..., Cm, ¬Pi →is an axiom and the
tree Ti is not present. Otherwise, let C′ = {C1, ..., Cm} −{C}. Using lemma
9.2.2 again, we obtain a proof Ri of
J, C1, ..., Cm, ¬Pi →
(of depth equal to the previous one) such that the the ∨: left rule is applied
to C:
Pi, ¬Pi →
T ′
i
J, {¬Q1, ..., ¬Qp}, C′, ¬Pi →
J, {¬Q1, ..., ¬Qp, Pi}, C′, ¬Pi →
Note that
J, {¬Q1, ..., ¬Qp}, C′, ¬Pi →
has two goal clauses. By lemma 9.2.5, either
J, {¬Q1, ..., ¬Qp}, C′ →

PROBLEMS
421
has a proof Ui, or
J, C′, ¬Pi →
has a proof Vi, and the depth of each proof is no greater than the depth
of the proof Ri of J, {¬Q1, ..., ¬Qp, Pi}, C′, ¬Pi →. In the second case, by
performing a weakening in the last inference of Vi, we obtain a proof for
J, C1..., Cm, ¬Pi →
of smaller depth than the original, and the induction
hypothesis applies, yielding a proof in SLD-form for J, C1, ..., Cm, ¬Pi →. In
the ﬁrst case, ¬Pi is dropped and, by the induction hypothesis, we also have
a proof in SLD-form of the form:
Pi, ¬Pi →
T ′′
i
J, {¬Q1, ..., ¬Qp}, C′ →
J, {¬Q1, ..., ¬Qp, Pi}, C′, ¬Pi →
Hence, by combining these proofs in SLD-form, we obtain a proof in
SLD-form for S.
Combining theorem 9.2.1 and lemma 9.2.5, we also have the following
theorem.
Theorem 9.2.2
Let S be a set of Horn clauses, consisting of a set J of
single positive literals, goal clauses N1,...,Nk, and deﬁnite clauses C1,...,Cm
containing at least two literals. If S is GCNF ′-provable, then there is some
i, 1 ≤i ≤k, such that
J ∪{C1, ..., Cm} ∪{Ni}
has a GCNF ′-proof in SLD-form.
Proof : Obvious by theorem 9.2.1 and lemma 9.2.5.
In the next section, we shall show how proofs in SLD-form can be con-
verted into resolution refutations of a certain type.
PROBLEMS
9.2.1.
Give a GCNF ′-proof in SLD-form for each of the following sequents:
{¬P3, ¬P4, P5}, {¬P1, P2}, {¬P2, P1}, {¬P3, P4}, {P3},
{¬P1, ¬P2}, {¬P5, P2} →
{P1}, {P2}, {P3}, {P4}, {¬P1, ¬P2, P6}, {¬P3, ¬P4, P7},
{¬P6, ¬P7, P8}, {¬P8} →

422
9/SLD-Resolution And Logic Programming (PROLOG)
{¬P2, P3}, {¬P3, P4}, {¬P4, P5}, {P3}, {P1}, {P2}, {¬P1},
{¬P3, P6}, {¬P3, P7}, {¬P3, P8} →
9.2.2.
Complete the missing details in the proof of lemma 9.2.5.
9.2.3.
Write a computer program for building proof trees in SLD-form for
Horn clauses.
∗9.2.4.
Given a set S of Horn clauses, we deﬁne an H-tree for S as a tree
labeled with propositional letters and satisfying the following proper-
ties:
(i) The root of T is labeled with F (false);
(ii) The immediate descendants of F are nodes labeled with proposi-
tional letters P1,...,Pn such that {¬P1, ..., ¬Pn} is some goal clause in
S;
(iii) For every nonroot node in the tree labeled with some letter Q,
either the immediate descendants of that node are nodes labeled with
letters P1,...,Pk such that {¬P1, ..., ¬Pk, Q} is some clause in S, or
this node is a leaf if {Q} is a clause in S.
Prove that S is unsatisﬁable iﬀit has an H-tree.
9.3 SLD-Resolution in Propositional Logic
SLD-refutations for sets of Horn clauses can be viewed as linearizations of
GCNF ′-proofs in SLD-form.
9.3.1 SLD-Derivations and SLD-Refutations
First, we show how to linearize SLD-proofs.
Deﬁnition 9.3.1
The linearization procedure is a recursive algorithm that
converts a GCNF ′-proof in SLD-form into a sequence of negative clauses
according to the following rules:
(1) Every axiom ¬P, P →is converted to the sequence < {¬P},
>.
(2) For a sequent R →containing a goal clause N = {¬P1, ..., ¬Pn}, with
n > 1, if Ci is the sequence of clauses that is the linearization of the subtree
with root the i-th descendant of the sequent R →, construct the sequence
obtained as follows:
Concatenate the sequences C′
1,...,C′
n−1, Cn, where, for each i, 1 ≤i ≤
n −1, letting ni be the number of clauses in the sequence Ci, the sequence C′
i
has ni −1 clauses such that, for every j, 1 ≤j ≤ni −1, if the j-th clause of
Ci is
{B1, ..., Bm},

9.3 SLD-Resolution in Propositional Logic
423
then the j-th clause of C′
i is
{B1, ..., Bm, ¬Pi+1, ..., ¬Pn}.
(3) For every nonaxiom sequent Γ, ¬P →containing some negative lit-
eral ¬P, if the deﬁnite clause used in the inference is {¬P1, ..., ¬Pm, P}, letting
∆= Γ −{¬P1, ..., ¬Pm, P}, then if the sequence of clauses for the sequent
∆, {¬P1, ..., ¬Pm} →is C, form the sequence obtained by concatenating ¬P
and the sequence C.
Note that by (1), (2), and (3), in (2), the ﬁrst clause of each C′
i, (1 ≤
i ≤n −1), is
{¬Pi, ¬Pi+1, ..., ¬Pn},
and the ﬁrst clause of Cn is {¬Pn}.
The following example shows how such a linearization is done.
EXAMPLE 9.3.1
Recall the proof tree in SLD-form given in example 9.2.1:
¬P1, P1 →
P3, ¬P3 →P4, ¬P4 →
P3, P4, {¬P3, ¬P4} →
P3, P4, ¬P1, {¬P3, ¬P4, P1} →
¬P2, P2 →
P3, ¬P3 →
P3, ¬P2, {¬P3, P2} →
P3, P4, {¬P1, ¬P2}, {¬P3, ¬P4, P1}, {¬P3, P2} →
The sequence corresponding to the left subtree is
< {¬P1}, {¬P3, ¬P4}, {¬P4},
} >
and the sequence corresponding to the right subtree is
< {¬P2}, {¬P3},
>
Hence, the sequence corresponding to the proof tree is
< {¬P1, ¬P2}, {¬P3, ¬P4, ¬P2},
{¬P4, ¬P2}, {¬P2}, {¬P3},
> .
This last sequence is an SLD-refutation, as deﬁned below.
Deﬁnition 9.3.2
Let S be a set of Horn clauses consisting of a set D of
deﬁnite clauses and a set {G1, ..., Gq} of goals.
An SLD-derivation for S
is a sequence < N0, N1, ..., Np > of negative clauses satisfying the following
properties:

424
9/SLD-Resolution And Logic Programming (PROLOG)
(1) N0 = Gj, where Gj is one of the goals;
(2) For every Ni in the sequence, 0 ≤i < p, if
Ni = {¬A1, ..., ¬Ak−1, ¬Ak, ¬Ak+1, ..., ¬An},
then there is some deﬁnite clause
Ci = {¬B1, ..., ¬Bm, Ak}
in D such that, if m > 0, then
Ni+1 = {¬A1, ..., ¬Ak−1, ¬B1, ..., ¬Bm, ¬Ak+1, ..., ¬An}
else if m = 0 then
Ni+1 = {¬A1, ..., ¬Ak−1, ¬Ak+1, ..., ¬An}.
An SLD-derivation is an SLD-refutation iﬀNp =
. The SLD-resolution
method is the method in which a set of of Horn clauses is shown to be unsat-
isﬁable by ﬁnding an SLD-refutation.
Note that an SLD-derivation is a linear representation of a resolution
DAG of the following special form:
Cp
· · ·
Ci
· · ·
C2
C1
N0 = Gj
N1
N2
Ni
Np =
At each step, the clauses
{¬A1, ..., ¬Ak−1, ¬Ak, ¬Ak+1, ..., ¬An}
and
{¬B1, ..., ¬Bm, Ak}
are resolved, the literals Ak and ¬Ak being canceled. The literal Ak is called
the selected atom of Ni, and the clauses N0, C1, ..., Cp are the input clauses.

9.3 SLD-Resolution in Propositional Logic
425
Such a resolution method is a form of linear input resolution, because it
resolves the current clause Nk with some clause in the input set D.
By the soundness of the resolution method (lemma 4.3.2), the SLD-
resolution method is sound.
EXAMPLE 9.3.2
The sequence
< {¬P1, ¬P2}, {¬P3, ¬P4, ¬P2},
{¬P4, ¬P2}, {¬P2}, {¬P3},
>
of example 9.3.1 is an SLD-refutation.
9.3.2 Completeness of SLD-Resolution for Horn Clauses
In order to show that SLD-resolution is complete for Horn clauses, since by
theorem 9.2.2 every set of Horn clauses has a GCNF ′-proof in SLD-form, it is
suﬃcient to prove that the linearization algorithm of deﬁnition 9.3.1 converts
a proof in SLD-form to an SLD-refutation.
Lemma 9.3.1 (Correctness of the linearization process) Given any GCNF ′-
proof T in SLD-form, the linearization procedure outputs an SLD-refutation.
Proof : We proceed by induction on proofs. If T consists of an axiom,
then the set S of Horn clauses contains a goal ¬Q and a positive literal Q,
and we have the SLD-refutation < {¬Q},
>.
Otherwise, because it is in SLD-form, letting C = C1, ..., Cm, the tree T
has the following structure:
Tn−1
J, C, ¬Pn−1 →
Tn
J, C, ¬Pn →
J, C, {¬Pn−1, ¬Pn} →
...
T1
J, C, ¬P1 →
T2
J, C, ¬P2 →
J, C, {¬P3, ..., ¬Pn} →
J, C, {¬P2, ..., ¬Pn} →
J, C, {¬P1, ..., ¬Pn} →
Each tree Ti that is not an axiom is also in SLD-form and has the
following shape:

426
9/SLD-Resolution And Logic Programming (PROLOG)
Pi, ¬Pi →
T ′
i
J, {¬Q1, ..., ¬Qp}, C′ →
J, {¬Q1, ..., ¬Qp, Pi}, C′, ¬Pi →
where C′ = {C1, ..., Cm} −{C}, for some deﬁnite clause C = {¬Q1, ..., ¬Qp,
Pi}.
By the induction hypothesis, each tree T ′
i is converted to an SLD-
refutation
Yi =< {¬Q1, ..., ¬Qp}, N2, ..., Nq > .
By rule (3), the proof tree Ti is converted to the SLD-refutation Xi obtained
by concatenating {¬Pi} and Yi. But then,
Xi =< {¬Pi}, {¬Q1, ..., ¬Qp}, N2, ..., Nq >
is an SLD-refutation obtained by resolving {¬Pi} with {¬Q1, ..., ¬Qp, Pi}.
If Ti is an axiom then by rule (1) it is converted to < {¬Pi},
>, which
is an SLD-refutation.
Finally, rule (2) combines the SLD-refutations X1,...,Xn in such a way
that the resulting sequence is an SLD-refutation. Indeed, for every i,
1 ≤i ≤n −1, Xi becomes the SLD-derivation X′
i, where
X′
i =< {¬Pi, ¬Pi+1..., ¬Pn}, {¬Q1, ..., ¬Qp, ¬Pi+1..., ¬Pn},
N2 ∪{¬Pi+1..., ¬Pn}, ..., Nq−1 ∪{¬Pi+1..., ¬Pn} >,
and so the entire sequence X′
1,...,X′
n−1,Xn is an SLD-refutation starting from
the goal {¬P1, ..., ¬Pn}.
As a corollary, we have the completeness of SLD-resolution for Horn
clauses.
Theorem 9.3.1
(Completeness of SLD-resolution for Horn clauses) The
SLD-resolution method is complete for Horn clauses.
Furthermore, if the
ﬁrst negative clause is {¬P1, ..., ¬Pn}, for every literal ¬Pi in this goal, there
is an SLD-resolution whose ﬁrst selected atom is Pi.
Proof : Completeness is a consequence of lemma 9.3.1 and theorem 9.2.2.
It is easy to see that in the linearization procedure, the order in which the
subsequences are concatenated does not matter. This implies the second part
of the lemma.
Actually, since SLD-refutations are the result of linearizing proof trees
in SLD-form, it is easy to show that any atom Pi such that ¬Pi belongs to a
negative clause Nk in an SLD-refutation can be chosen as the selected atom.

PROBLEMS
427
By theorem 9.2.2, if a set S of Horn clauses with several goals N1, ..., Nk
is GCNF ′-provable, then there is some goal Ni such that S −{N1, ..., Ni−1,
Ni+1, ..., Nk} is GCNF ′-provable. This does not mean that there is a unique
such Ni, as shown by the following example.
EXAMPLE 9.3.2
Consider the set S of clauses:
{P}, {Q}, {¬S, R}, {¬R, ¬P}, {¬R, ¬Q}, {S}.
We have two SLD-refutations:
< {¬R, ¬P}, {¬R}, {¬S},
>
and
< {¬R, ¬Q}, {¬R}, {¬S},
> .
In the next section, we generalize SLD-resolution to ﬁrst-order languages
without equality, using the lifting technique of Section 8.5.
PROBLEMS
9.3.1.
Apply the linearization procedure to the proof trees in SLD-form
obtained in problem 9.2.1.
9.3.2.
Give diﬀerent SLD-resolution refutations for the following sets of
clauses:
{P1}, {P2}, {P3}, {P4}, {¬P1, ¬P2, P6}, {¬P3, ¬P4, P7},
{¬P6, ¬P7, P8}, {¬P8}.
{¬P2, P3}, {¬P3, P4}, {¬P4, P5}, {P3}, {P1}, {P2}, {¬P1},
{¬P3, P6}, {¬P3, P7}, {¬P3, P8}.
9.3.3.
Write a computer program implementing the linearization procedure.
9.4 SLD-Resolution in First-Order Logic
In this section we shall generalize SLD-resolution to ﬁrst-order languages with-
out equality. Fortunately, it is relatively painless to generalize results about

428
9/SLD-Resolution And Logic Programming (PROLOG)
propositional SLD-resolution to the ﬁrst-order case, using the lifting technique
of Section 8.5.
9.4.1 Deﬁnition of SLD-Refutations
Since the main application of SLD-resolution is to PROLOG, we shall also
revise our notation to conform to the PROLOG notation.
Deﬁnition 9.4.1
A Horn clause (in PROLOG notation) is one of the fol-
lowing expressions:
(i) B : −A1, ..., Am
(ii) B
(iii) : −A1, ..., Am
In the above, B, A1,...,Am are atomic formulae of the form Pt1...tk,
where P is a predicate symbol of rank k, and t1,...,tk are terms.
A clause of the form (i) or (ii) is called a deﬁnite clause, and a clause of
the form (iii) is called a goal clause (or negative clause).
The translation into the standard logic notation is the following:
The clause B : −A1, ..., Am corresponds to the formula
(¬A1 ∨... ∨¬Am ∨B);
The clause B corresponds to the atomic formula B;
The clause : −A1, ..., Am corresponds to the formula
(¬A1 ∨... ∨¬Am).
Actually, as in deﬁnition 8.2.1, it is assumed that a Horn clause is the
universal closure of a formula as above (that is, of the form ∀x1...∀xnC, where
FV (C) = {x1, ..., xn}). The universal quantiﬁers are dropped for simplicity
of notation, but it is important to remember that they are implicitly present.
The deﬁnition of SLD-derivations and SLD-refutations is extended by
combining deﬁnition 9.3.2 and the deﬁnition of a resolvent given in deﬁnition
8.5.2.
Deﬁnition 9.4.2
Let S be a set of Horn clauses consisting of a set D of
deﬁnite clauses and a set {G1, ..., Gq} of goals.
An SLD-derivation for S
is a sequence < N0, N1, ..., Np > of negative clauses satisfying the following
properties:
(1) N0 = Gj, where Gj is one of the goals;

9.4 SLD-Resolution in First-Order Logic
429
(2) For every Ni in the sequence, 0 ≤i < p, if
Ni =: −A1, ..., Ak−1, Ak, Ak+1, ..., An,
then there is some deﬁnite clause Ci = A : −B1, ..., Bm in D such that Ak
and A are uniﬁable, and for some most general uniﬁer σi of Ak and ρi(A),
where (Id, ρi) is a separating substitution pair, if m > 0, then
Ni+1 =: −σi(A1, ..., Ak−1, ρi(B1), ..., ρi(Bm), Ak+1, ..., An)
else if m = 0 then
Ni+1 =: −σi(A1, ..., Ak−1, Ak+1, ..., An).
An SLD-derivation is an SLD-refutation iﬀNp =
.
Note that an SLD-derivation is a linear representation of a resolution
DAG of the following special form:
Cp
· · ·
Ci
· · ·
C2
C1
N0 = Gj
N1
N2
Ni
Np =
σ1
σ2
σi
σp
At each step, the clauses
: −A1, ..., Ak−1, Ak, Ak+1, ..., An
and
A : −B1, ..., Bm
are resolved, the atoms Ak and ρi(A) being canceled, since they are uniﬁed
by the most general uniﬁer σi. The literal Ak is called the selected atom of
Ni, and the clauses N0, C1, ..., Cp are the input clauses.
When the derivation is a refutation, the substitution
σ = (ρ1 ◦σ1) ◦... ◦(ρp ◦σp)

430
9/SLD-Resolution And Logic Programming (PROLOG)
obtained by composing the substitutions occurring in the refutation is called
the result substitution or answer substitution. It is used in PROLOG to ex-
tract the output of an SLD-computation.
Since an SLD-derivation is a special kind of resolution DAG, (a linear
input resolution), its soundness is a consequence of lemma 8.5.2.
Lemma 9.4.1
(Soundness of SLD-resolution) If a set of Horn clauses has
an SLD-refutation, then it is unsatisﬁable.
Proof : Immediate from lemma 8.5.2.
Let us give an example of an SLD-refutation in the ﬁrst-order case.
EXAMPLE 9.4.1
Consider the following set of deﬁnite clauses, axiomatizing addition of
natural numbers:
C1 : add(X, 0, X).
C2 : add(X, succ(Y ), succ(Z)) : −add(X, Y, Z).
Consider the goal
B : −add(succ(0), V, succ(succ(0))).
We wish to show that the above set is unsatisﬁable. We have the fol-
lowing SLD-refutation:
Goal clause
Input clause
Substitution
: −add(succ(0), V, succ(succ(0)))
C2
: −add(succ(0), Y2, succ(0))
C1
σ1
σ2
where
σ1 = (succ(0)/X1, succ(0)/Z1, succ(Y2)/V ),
σ2 = (succ(0)/X2, 0/Y2)
The variables X1, Z1, Y2, X2 were introduced by separating substitu-
tions in computing resolvents. The result substitution is
(succ(0)/V, succ(0)/X1, succ(0)/Z1, succ(0)/X2).
The interesting component is succ(0)/V . Indeed, there is a computa-
tional interpretation of the unsatisﬁability of the set {C1, C2, B}. For
this, it is necessary to write quantiﬁers explicitly and remember that
goal clauses are negative. Observe that
∀XC1 ∧∀X∀Y ∀ZC2 ∧∀V B

9.4 SLD-Resolution in First-Order Logic
431
is unsatisﬁable, iﬀ
¬(∀XC1 ∧∀X∀Y ∀ZC2 ∧∀V B)
is valid, iﬀ
(∀XC1 ∧∀X∀Y ∀ZC2) ⊃∃V ¬B
is valid. But ∃V ¬B is actually
∃V add(succ(0), V, succ(0)).
Since (∀XC1 ∧∀X∀Y ∀ZC2) deﬁnes addition in the intuitive sense that
any X, Y , Z satisfying the above sentence are such that Z = X +Y , we
are trying to ﬁnd some V such that succ(0) + V = succ(succ(0)), or in
other words, compute the diﬀerence of succ(succ(0)) and succ(0), which
is indeed succ(0)!
This interpretation of a refutation showing that a set of Horn clauses is
unsatisﬁable as a computation of the answer to a query, such as
(∀XC1 ∧∀X∀Y ∀ZC2) ⊃∃V ¬B,
“ﬁnd some V satisfying ¬B and such that some conditional
axioms ∀XC1 and ∀X∀Y ∀ZC2 hold,”
is the essense of PROLOG. The set of clauses {C1, C2} can be viewed as a
logic program.
We will come back to the idea of refutations as computations in the next
section.
9.4.2 Completeness of SLD-Resolution for Horn Clauses
The completeness of SLD-resolution for Horn clauses is shown in the following
theorem.
Theorem 9.4.1
(Completeness of SLD-Resolution for Horn Clauses) Let L
be any ﬁrst-order language without equality. Given any ﬁnite set S of Horn
clauses, if S is unsatisﬁable, then there is an SLD-refutation with ﬁrst clause
some negative clause : −B1, ..., Bn in S.
Proof : We shall use the lifting technique provided by lemma 8.5.4. First,
by the Skolem-Herbrand-G¨odel theorem, if S is unsatisﬁable, there is a set Sg
of ground instances of clauses in S which is unsatisﬁable. Since substitution
instances of Horn clauses are Horn clauses, by theorem 9.3.1, there is an
SLD-refutation for Sg, starting from some negative clause in Sg. Finally, we
conclude by observing that if we apply the lifting technique of lemma 8.5.4,
we obtain an SLD-refutation. This is because we always resolve a negative
clause (Ni) against an input clause (Ci). Hence, the result is proved.

432
9/SLD-Resolution And Logic Programming (PROLOG)
From theorem 9.3.1, it is also true that if the ﬁrst negative clause is
: −B1, ..., Bn, for every atom Bi in this goal, there is an SLD-resolution whose
ﬁrst selected atom is Bi. As a matter of fact, this property holds for any clause
Ni in the refutation.
Even though SLD-resolution is complete for Horn clauses, there is still
the problem of choosing among many possible SLD-derivations. The above
shows that the choice of the selected atom is irrelevant. However, we still have
the problem of choosing a deﬁnite clause A : −B1, ..., Bm such that A uniﬁes
with one of the atoms in the current goal clause : −A1, ..., Ak−1, Ak, Ak+1, ...,
An.
Such problems are important and are the object of current research in
programming logic, but we do not have the space to address them here. The
interested reader is referred to Kowalski, 1979, or Campbell, 1983, for an
introduction to the methods and problems in programming logic.
In the next section, we discuss the use of SLD-resolution as a computa-
tion procedure for PROLOG.
PROBLEMS
9.4.1.
Prove using SLD-resolution that the following set of clauses is unsat-
isﬁable:
add(X, 0, X)
add(X, succ(Y ), succ(Z)) : −add(X, Y, Z)
: −add(succ(succ(0)), succ(succ(0)), U).
9.4.2.
Prove using SLD-resolution that the following set of clauses is unsat-
isﬁable:
add(X, 0, X)
add(X, succ(Y ), succ(Z)) : −add(X, Y, Z)
: −add(U, V, succ(succ(succ(0)))).
Find all possible SLD-refutations.
9.4.3.
Using SLD-resolution, show that the following set of Horn clauses is
unsatisﬁable:
hanoi(N, Output) : −move(a, b, c, N, Output).
move(A, B, C, succ(M), Output) : −move(A, C, B, M, Out1),
move(C, B, A, M, Out2),
append(Out1, cons(to(A, B), Out2), Output).
move(A, B, C, 0, nil).
append(cons(A, L1), L2, cons(A, L3)) : −append(L1, L2, L3).
append(nil, L1, L1).
: −hanoi(succ(succ(0)), Z)

9.5 SLD-Resolution, Logic Programming (PROLOG)
433
9.5 SLD-Resolution, Logic Programming (PROLOG)
We have seen in example 9.4.1 that an SLD-refutation for a set of Horn clauses
can be viewed as a computation. This illustrates an extremely interesting use
of logic as a programming language.
9.5.1 Refutations as Computations
In the past few years, Horn logic has been the basis of a new type of program-
ming language due to Colmerauer named PROLOG. It is not the purpose of
this book to give a complete treatment of PROLOG, and we refer the inter-
ested reader to Kowalski, 1979, or Clocksin and Mellish, 1981, for details. In
this section, we shall lay the foundations of the programming logic PROLOG.
It will be shown how SLD-resolution can be used as a computational proce-
dure to solve certain problems, and the correctness and completeness of this
approach will be proved.
In a logic programming language like PROLOG, one writes programs
as sets of assertions in the form of Horn clauses, or more accurately, deﬁnite
clauses, except for the goal. A set P of deﬁnite clauses is a logic program. As
we said in Section 9.4, it is assumed that distinct Horn clauses are universally
quantiﬁed.
Roughly speaking, a logic program consists of facts and assertions. Given
such a logic program, one is usually interested in extracting facts that are
consequences of the logic program P. Typically, one has a certain “query” (or
goal) G containing some free variables z1,...,zq, and one wants to ﬁnd term
instances t1, ..., tq for the variables z1, ..., zq, such that the formula
P ⊃G[t1/z1, ..., tq/zq]
is valid.
For simplicity, it will be assumed that the query is a positive atomic
formula G. More complicated formulae can be handled (anti-Horn clauses),
but we will consider this case later.
In PROLOG, a goal statement G is
denoted by ? −G.
From a logical point of view, the problem is to determine whether the
sentence
P ⊃(∃z1...∃zqG)
is valid.
From a computational point of view, the problem is to ﬁnd term values
t1,...,tq for the variables z1,...,zq that make the formula
P ⊃G[t1/z1, ..., tq/zq]
valid, and perhaps all such assignments.

434
9/SLD-Resolution And Logic Programming (PROLOG)
Remarkably, SLD-resolution can be used not only as a proof procedure,
but also a a computational procedure, because it returns a result substitution.
The reason is as follows:
The formula P ⊃(∃z1...∃zqG) is valid iﬀ
¬(P ⊃(∃z1...∃zqG)) is unsatisﬁable iﬀ
P ∧(∀z1...∀zq¬G) is unsatisﬁable.
But since G is an atomic formula, ¬G is a goal clause : −G, and P ∧
(∀z1...∀zq¬G) is a conjuction of Horn clauses!
Hence, SLD-resolution can be used to test for unsatisﬁability, and if
it succeeds, it returns a result substitution σ. The crucial fact is that the
components of the substitution σ corresponding to the variables z1,...,zq are
answers to the query G. However, this fact is not obvious. A proof will be
given in the next section. As a preliminary task, we give a rigorous deﬁnition
of the semantics of a logic program.
9.5.2 Model-Theoretic Semantics of Logic Programs
We begin by deﬁning what kind of formula can appear as a goal.
Deﬁnition 9.5.1
An anti-Horn clause is a formula of the form
∃x1...∃xmB,
where B is a conjunctions of literals L1 ∧... ∧Lp, with at most one negative
literal and FV (B) = {x1, ..., xm}.
A logic program is a pair (P, G), where the program P is a set of (uni-
versal) Horn clauses, and the query G is a disjunction
(G1 ∨... ∨Gn)
of anti-Horn clauses Gi = ∃y1...∃zmiBi.
It is also assumed that for all i ̸= j, 1 ≤i, j ≤n, the sets of variables
FV (Bi) and FV (Bj) are disjoint. The union {z1, ..., zq} of the sets of free
variables occurring in each Bi is called the set of output variables associated
with G.
Note that an anti-Horn clause is not a clause. However, the terminology
is justiﬁed by the fact that the negation of an anti-Horn clause is a (universal)
Horn clause, and that ¬G is equivalent to a conjunction of universal Horn
clauses.
Remark: This deﬁnition is more general than the usual deﬁnition used
in PROLOG. In (standard) PROLOG, P is a set of deﬁnite clauses (that is,

9.5 SLD-Resolution, Logic Programming (PROLOG)
435
P does not contain negative clauses), and G is a formula that is a conjunction
of atomic formulae. It is shown in the sequel that more general queries can
be handled, but that the semantics is a bit more subtle. Indeed, indeﬁnite
answers may arise.
EXAMPLE 9.5.1
The following is a logic program, where P consists of the following
clauses:
rocksinger(jackson).
teacher(jean).
teacher(susan).
rich(X) : −rocksinger(X).
: −teacher(X), rich(X).
The query is the following disjunction:
? −¬rocksinger(Y ) ∨rich(Z)
EXAMPLE 9.5.2
The following is the program of a logic program:
hanoi(N, Output) : −move(a, b, c, N, Output).
move(A, B, C, succ(M), Output) : −move(A, C, B, M, Out1),
move(C, B, A, M, Out2),
append(Out1, cons(to(A, B), Out2), Output).
move(A, B, C, 0, nil).
append(cons(A, L1), L2, cons(A, L3)) : −append(L1, L2, L3).
append(nil, L1, L1).
The query is:
? −hanoi(succ(succ(succ(0))), Output).
The above program is a logical version of the well known problem known
as the tower of Hanoi (see Clocksin and Mellish, 1981).
In order to give a rigorous deﬁnition of the semantics of a logic program,
it is convenient to deﬁne the concept of a free structure.
Recall that we
are only dealing with ﬁrst-order languages without equality, and that if the
language has no constants, the special constant # is added to it.
Deﬁnition 9.5.2
Given a ﬁrst-order language L without equality and with
at least one constant, a free structure (or Herbrand structure) H is an L-
structure with domain the set HL of all closed L-terms, and whose interpre-
tation function satisﬁes the following property:

436
9/SLD-Resolution And Logic Programming (PROLOG)
(i) For every function symbol f of rank n, for all t1,...,tn ∈HL,
fH(t1, ..., tn) = ft1...tn
and
(ii) For every constant symbol c,
cH = c.
The set of terms HL is called the Herbrand universe of L. For simplicity
of notation, the set HL is denoted as H when L is understood. The following
lemma shows that free structures are universal. This lemma is actually not
necessary for giving the semantics of Horn clauses, but it is of independent
interest.
Lemma 9.5.1
A sentence X in NNF containing only universal quantiﬁers
is satisﬁable in some model iﬀit is satisﬁable in some free structure.
Proof : Clearly, if X is satisﬁed in a free structure, it is satisﬁable in some
model. For the converse, assume that X has some model A. We show how a
free structure can be constructed from A. We deﬁne the function h : H →A
as follows:
For every constant c, h(c) = cA;
For every function symbol f of rank n > 0, for any n terms t1,...,tn ∈H,
h(ft1...tn) = fA(h(t1), ..., h(tn)).
Deﬁne the interpretation of the free structure H such that, for any pred-
icate symbol P of rank n, for any n terms t1,...,tn ∈H,
H |= P(t1, ..., tn)
iﬀ
A |= P(h(t1), ..., h(tn)).
(∗)
We now prove by induction on formulae that, for every assignment s : V →H,
if A |= X[s ◦h], then H |= X[s].
(i) If X is a literal, this amounts to the deﬁnition (∗).
(ii) If X is of the form (B ∧C), then A |= X[s ◦h] implies that
A |= B[s ◦h]
and
A |= C[s ◦h].
By the induction hypothesis,
H |= B[s]
and
H |= C[s],
that is, H |= X[s].

9.5 SLD-Resolution, Logic Programming (PROLOG)
437
(iii) If X is of the form (B ∨C), then A |= X[s ◦h] implies that
A |= B[s ◦h]
or
A |= C[s ◦h].
By the induction hypothesis,
H |= B[s]
or
H |= C[s],
that is, H |= X[s].
(iv) X is of the form ∃xB. This case is not possible since X does not
contain existential quantiﬁers.
(v) X is of the form ∀xB. If A |= X[s ◦h], then for every a ∈A,
A |= B[(s ◦h)[x := a]].
Now, since h : H →A, for every t ∈H, h(t) = a for some a ∈A, and so, for
every t in H,
A |= B[(s ◦h)[x := h(t)]], that is, A |= B[(s[x := t]) ◦h].
By the induction hypothesis, H |= B[s[x := t]] for all t ∈H, that is, H |= X[s].
It is obvious that lemma 9.5.1 also applies to sets of sentences. Also,
since a formula is unsatisﬁable iﬀit has no model, we have the following
corollary:
Corollary
Given a ﬁrst-order language without equality and with some
constant, a set of sentences in NNF and only containing universal quantiﬁers
is unsatisﬁable iﬀit is unsatisﬁable in every free (Herbrand) structure.
We now provide a rigorous semantics of logic programs.
Given a logic program (P, G), the question of interest is to determine
whether the formula P ⊃G is valid.
Actually, we really want more.
If
{z1, ..., zq} is the the set of output variables occurring in G, we would like to
ﬁnd some (or all) tuple(s) (t1, ..., tq) of ground terms such that
|= P ⊃(B1 ∨... ∨Bn)[t1/z1, ..., tq/zq].
As we shall see, such tuples do not always exist.
However, indeﬁnite (or
disjunctive) answers always exist, and if some conditions are imposed on P
and G, deﬁnite answers (tuples of ground terms) exist.
Assume that P ⊃G is valid. This is equivalent to ¬(P ⊃G) being
unsatisﬁable. But ¬(P ⊃G) is equivalent to P ∧¬G, which is equivalent
to a conjunction of universal Horn clauses. By the Skolem-Herbrand-G¨odel

438
9/SLD-Resolution And Logic Programming (PROLOG)
theorem (theorem 7.6.1), if {x1, ..., xm} is the set of all universally quantiﬁed
variables in P ∧¬G, there is some set
{(t1
1, ..., t1
m), ..., (tk
1, ..., tk
m)}
of m-tuples of ground terms such that the conjunction
(P ∧¬G)[t1
1/x1, ..., t1
m/xm] ∧... ∧(P ∧¬G)[tk
1/x1, ..., tk
m/xm]
is unsatisﬁable (for some k ≥1). From this, it is not diﬃcult to prove that
|= P ⊃G[t1
1/x1, ..., t1
m/xm] ∨... ∨G[tk
1/x1, ..., tk
m/xm].
However, we cannot claim that k = 1, as shown by the following example.
EXAMPLE 9.5.3
Let P = ¬Q(a) ∨¬Q(b), and G = ∃x¬Q(x). P ⊃G is valid, but there
is no term t such that
¬Q(a) ∨¬Q(b) ⊃¬Q(t)
is valid.
As a consequence, the answer to a query may be indeﬁnite, in the sense
that it is a disjunction of substitution instances of the goal. However, deﬁnite
answers can be ensured if certain restrictions are met.
Lemma 9.5.2
(Deﬁnite answer lemma) If P is a (ﬁnite) set of deﬁnite
clauses and G is a query of the form
∃z1...∃zq(B1 ∧... ∧Bl),
where each Bi is an atomic formula, if
|= P ⊃∃z1...∃zq(B1 ∧... ∧Bl),
then there is some tuple (t1, ..., tq) of ground terms such that
|= P ⊃(B1 ∧... ∧Bl)[t1/z1, ..., tq/zq].
Proof :
|= P ⊃∃z1...zq(B1 ∧... ∧Bl)
iﬀ
P ∧∀z1...∀zq(¬B1 ∨... ∨¬Bl)
is unsatisﬁable.
By the Skolem-Herbrand-G¨odel theorem, there is a set C of ground substitu-
tion instances of the clauses in P ∪{¬B1, ..., ¬Bl} that is unsatisﬁable. Since

9.5 SLD-Resolution, Logic Programming (PROLOG)
439
the only negative clauses in C come from {¬B1, ..., ¬Bl}, by lemma 9.2.5,
there is some substitution instance
(¬B1 ∨... ∨¬Bl)[t1/z1, ..., tq/zq]
such that
P ′ ∪{(¬B1 ∨... ∨¬Bl)[t1/z1, ..., tq/zq]}
is unsatisﬁable, where P ′ is the subset of C consisting of substitution instances
of clauses in P. But then, it is not diﬃcult to show that
|= P ⊃(B1 ∧... ∧Bl)[t1/z1, ..., tq/zq].
The result of lemma 9.5.2 justiﬁes the reason that in PROLOG only
programs consisting of deﬁnite clauses and queries consisting of conjunctions
of atomic formulae are considered. With such restrictions, deﬁnite answers
are guaranteed. The above discussion leads to the following deﬁnition.
Deﬁnition 9.5.3
Given a logic program (P, G) with query G = ∃z1...∃zqB
and with B = (B1 ∨... ∨Bn), the semantics (or meaning) of (P, G) is the set
M(P, G) =

{{(t1
1, ..., t1
q), ..., (tk
1, ..., tk
q)}, k ≥1, (tk
1, ..., tk
q) ∈Hq |
|= P ⊃B[t1
1/z1, ..., t1
q/zq] ∨... ∨B[tk
1/z1, ..., tk
q/zq]}
of sets q-tuples of terms in the Herbrand universe H that make the formula
P ⊃B[t1
1/z1, ..., t1
q/zq] ∨... ∨B[tk
1/z1, ..., tk
q/zq]
valid (in every free structure).
If P is a set of deﬁnite clauses and B is a conjunction of atomic formulae,
k = 1.
9.5.3 Correctness of SLD-Resolution as a Computation
Procedure
We now prove that for every SLD-refutation of the conjunction of clauses in
P ∧¬G, the components of the result substitution σ restricted to the output
variables belong to the semantics M(P, G) of (P, G). We prove the following
slightly more general lemma, which implies the fact mentioned above.
Lemma 9.5.3
Given a set P of Horn clauses, let R be an SLD-refutation

440
9/SLD-Resolution And Logic Programming (PROLOG)
Cp
· · ·
Ci
· · ·
C2
C1
N0 = Gj
N1
N2
Ni
Np =
σ1
σ2
σi
σp
with result substitution σ (not necessarily ground). Let θp = ρp ◦σp, and for
every i, 1 ≤i ≤p −1, let
θi = (ρi ◦σi) ◦θi+1.
(Note that σ = θ1, the result substitution.) The substitutions θi are also
called result substitutions.) Then the set of quantiﬁer-free clauses
{θ1(N0), θ1(C1), ..., θp(Cp)}
is unsatisﬁable (using the slight abuse of notation in which the matrix D of a
clause C = ∀x1...∀xkD is also denoted by C).
Proof : We proceed by induction on the length of the derivation.
(i) If p = 1, N0 must be a negative formula : −B and C1 a positive literal
A such that A and B are uniﬁable, and it is clear that {¬θ1(B), θ1(C1)} is
unsatisﬁable.
(ii) If p > 1, then by the induction hypothesis, taking N1 as the goal of
an SLD-refutation of length p −1 the set
{θ2(N1), θ2(C2), ..., θp(Cp)}
is unsatisﬁable. But N0 is some goal clause
: −A1, ..., Ak−1, Ak, Ak+1, ..., An,
and C1 is some deﬁnite clause
A : −B1, ..., Bm,
such that A and Ak are uniﬁable. Furthermore, the resolvent is given by
N1 =: −σ1(A1, ..., Ak−1, ρ1(B1), ..., ρ1(Bm), Ak+1, ..., An),

9.5 SLD-Resolution, Logic Programming (PROLOG)
441
where σ1 is a most general uniﬁer, and we know that
σ1(N0) ∧(ρ1 ◦σ1)(C1) ⊃N1
is valid (by lemma 8.5.1). Since ρ1 is a renaming substitution, it is the identity
on N0, and by the deﬁnition of θ1, we have
{θ2(σ1(N0)), θ2(ρ1 ◦σ1(C1)), θ2(C2), ..., θp(Cp)}
= {θ1(N0), θ1(C1), θ2(C2), ..., θp(Cp)}.
If {θ1(N0), θ1(C1), ..., θp(Cp)} was satisﬁable, since
σ1(N0) ∧(ρ1 ◦σ1)(C1) ⊃N1
is valid,
{θ2(N1), θ2(C2), ..., θp(Cp)}
would also be satisﬁable, a contradiction. Hence,
{θ1(N0), θ1(C1), ..., θp(Cp)}
is unsatisﬁable.
Theorem 9.5.1
(Correctness of SLD-resolution as a computational pro-
cedure) Let (P, G) be a logic program with query G = ∃z1...∃zqB, with
B = (B1 ∨... ∨Bn).
For every SLD-refutation R =< N0, N1, ..., Np >
for the set of Horn clauses in P ∧¬G, if R uses (as in lemma 9.5.3) the
list of deﬁnite clauses < C1, ..., Cp >, the list of result substitutions (not
necessarily ground) < θ1, ..., θp >, and if < ¬Ci1, ..., ¬Cik > is the subse-
quence of < N0, C1, ..., Cp > consisting of the clauses in {¬B1, ..., ¬Bn} (with
¬C0 = N0), then
|= P ⊃θi1(Ci1) ∨... ∨θik(Cik).
Proof : Let P ′ be the set of formulae obtained by deleting the universal
quantiﬁers from the clauses in P. By lemma 9.5.3, there is a sequence of
clauses < N0, C1, ..., Cp > from the set P ′ ∪{¬B1, ..., ¬Bn} such that
{θ1(N0), θ1(C1), ..., θp(Cp)}
is unsatisﬁable. But then, it is easy to construct a proof of
P ⊃θi1(Ci1) ∨... ∨θik(Cik)
(using ∀: right rules as in lemma 8.5.4), and this yields the result.
Note: The formulae Ci1,...,Cik are not necessarily distinct, but the sub-
stitutions θi1,...,θik might be.

442
9/SLD-Resolution And Logic Programming (PROLOG)
Corollary
Let (P, G) be a logic program such that P is a set of deﬁnite
clauses and G is a formula of the form ∃z1...∃zqB, where B is a conjunction of
atomic formulae. For every SLD-refutation of the set of Horn clauses P ∧¬G,
if σ is the result substitution and (t1/z1, ..., tq/zq) is any ground substitution
such that for every variable zi in the support of σ, ti is some ground instance
of σ(zi) and otherwise ti is any arbitrary term in H, then
|= P ⊃B[t1/z1, ..., tq/zq].
Proof : First, observe that ¬B must be the goal clause N0. Also, if some
output variable zi does not occur in the support of the output substitution
σ, this means that σ(zi) = zi. But then, it is immediate by lemma 9.5.3 that
the result of substituting arbitrary terms in H for these variables in
{θ1(N0), θ1(C1), ..., θp(Cp)}
is also unsatisﬁable.
Theorem 9.5.1 shows that SLD-resolution is a correct method for com-
puting elements of M(P, G), since every set {(t1
1, ..., t1
q), ..., (tk
1, ..., tk
q)} of tuples
of terms in H returned by an SLD-refutation (corresponding to the output
variables) makes
P ⊃B[t1
1/z1, ..., t1
q/zq] ∨... ∨B[tk
1/z1, ..., tk
q/zq]
valid.
Remark: Normally, we are interested in tuples of terms in H, because
we want the answers to be interpretable as deﬁnite elements of the Herbrand
universe. However, by lemma 9.5.3, indeﬁnite answers (sets of tuples of terms
containing variables) have to be considered. This is illustrated in the next
example.
EXAMPLE 9.5.4
Consider the logic program of example 9.5.1. The set of clauses corre-
sponding to P ∧¬G is the following:
rocksinger(jackson).
teacher(jean).
teacher(susan).
rich(X) : −rocksinger(X).
: −teacher(X), rich(X).
rocksinger(Y ).
: −rich(Z)

9.5 SLD-Resolution, Logic Programming (PROLOG)
443
Note the two negative clauses. There are four SLD-refutations, two with
goal : −teacher(X), rich(X), and two with goal : −rich(Z).
(i) SLD-refutation with output (jean/Y ):
Goal clause
Input clause
Substitution
: −teacher(X), rich(X)
teacher(jean)
: −rich(jean)
rich(X) : −rocksinger(X)
(jean/X)
: −rocksinger(jean)
rocksinger(Y )
(jean/X1)
(jean/Y1)
The result substitution is (jean/Y, jean/X). Also, Z is any element of
the Herbrand universe.
(ii) SLD-refutation with output (susan/Y ): Similar to the above.
(iii) SLD-refutation with output (jackson/Z):
Goal clause
Input clause
Substitution
: −rich(Z)
rich(X) : −rocksinger(X)
: −rocksinger(X1)
rocksinger(jackson)
(X1/Z)
(jackson/X1)
Y is any element of the Herbrand universe.
(iv) SLD-refutation with output (Y1/Y, Y1/Z):
Goal clause
Input clause
Substitution
: −rich(Z)
rich(X) : −rocksinger(X)
: −rocksinger(X1)
rocksinger(Y )
(X1/Z)
(Y1/X1)
In this last refutation, we have an indeﬁnite answer that says that for
any Y1 in the Herbrand universe, Y = Y1, Z = Y1 is an answer. This is
indeed correct, since the clause rich(X) : −rocksinger(X) is equivalent
to ¬rocksinger(X) ∨rich(X), and so
|= P ⊃(¬rocksinger(Y1) ∨rich(Y1)).
We now turn to the completeness of SLD-resolution as a computation
procedure.

444
9/SLD-Resolution And Logic Programming (PROLOG)
9.5.4 Completeness of SLD-Resolution as a Computa-
tional Procedure
The correctness of SLD-resolution as a computational procedure brings up im-
mediately the question of its completeness. For any set of tuples in M(P, G),
is there an SLD-refutation with that answer?
This is indeed the case, as
shown below. We state and prove the following theorem for the special case
of deﬁnite clauses, leaving the general case as an exercise.
Theorem 9.5.2 Let (P, G) be a logic program such that P is a set of deﬁnite
clauses and G is a goal of the form ∃z1...∃zqB, where B is a conjunction
B1 ∧... ∧Bn of atomic formulae. For every tuple (t1, ..., tq) ∈M(P, G), there
is an SLD-refutation with result substitution σ and a (ground) substitution η
such that the restriction of σ ◦η to z1,...,zq is (t1/z1, ..., tq/zq).
Proof : By deﬁnition, (t1, ..., tq) ∈M(P, G) iﬀ
|= P ⊃(B1 ∧... ∧Bn)[t1/z1, ..., tq/zq]
iﬀ
P ∧(¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq]
is unsatisﬁable.
By theorem 9.5.1, there is an SLD-refutation with output substitution θ1.
Since
(¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq]
is the only negative clause, by lemma 9.5.3, for some sequence of clauses
< C1, ..., Cp > such that the universal closure of each clause Ci is in P,
{θ1((¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq]), θ1(C1), ..., θp(Cp)}
is also unsatisﬁable.
If θ1 is not a ground substitution, we can substitute
ground terms for the variables and form other ground substitutions θ′
1,...,θ′
p
such that,
{θ′
1((¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq]), θ′
1(C1), ..., θ′
p(Cp)}
is still unsatisﬁable. Since the terms t1,...,tq are ground terms,
θ′
1((¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq]) = (¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq].
By theorem 9.3.1, there is a ground SLD-refutation Rg with sequence of input
clauses
< {(¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq], C′
1, ..., C′
r >
for
{(¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq], θ′
1(C1), ..., θ′
p(Cp)}.
By the lifting lemma (lemma 8.5.4), there is an SLD-refutation R with se-
quence of input clauses
< {¬B1, ..., ¬Bn}, C′′
1 , ..., C′′
r >
for
{{¬B1, ..., ¬Bn}, C1, ..., Cp},

PROBLEMS
445
such that for every pair of clauses N ′′
i in R and N ′
i in Rg, N ′
i = ηi(N ′′
i ), for
some ground substitution ηi. Let η = ηr, and let σ be the result substitution
of the SLD-refutation R. It can be shown that
(¬B1 ∨... ∨¬Bn)[t1/z1, ..., tq/zq] = (σ ◦η)(¬B1 ∨... ∨¬Bn),
which shows that (t1/z1, ..., tq/zq) is equal to the restriction of σ◦η to z1, ..., zq.
9.5.5 Limitations of PROLOG
Theorem 9.5.1 and theorem 9.5.2 show that SLD-Resolution is a correct and
complete procedure for computing the sets of tuples belonging to the meaning
of a logic program. From a theoretical point of view, this is very satisfactory.
However, from a practical point of view, there is still something missing. In-
deed, we still need a procedure for producing SLD-refutations, and if possible,
eﬃciently. It is possible to organize the set all SLD-refutations into a kind of
tree (the search space), and the problem is then reduced to a tree traversal.
If one wants to retain completeness, the kind of tree traversal chosen must be
a breadth-ﬁrst search, which can be very ineﬃcient. Most implementations of
PROLOG sacriﬁce completeness for eﬃciency, and adopt a depth-ﬁrst traver-
sal strategy.
Unfortunately, we do not have the space to consider these interesting
issues, but we refer the interested reader to Kowalski, 1979, and to Apt and
Van Emden, 1982, where the semantics of logic programming is investigated
in terms of ﬁxedpoints.
Another point worth noting is that not all ﬁrst-order formulae (in Skolem
form) can be expressed as Horn clauses. The main limitation is that negative
premises are not allowed, in the sense that a formula of the form
B : −A1, ..., Ai−1, ¬A, Ai+1, ..., An.
is not equivalent to any Horn clause (see problem 3.5.9).
This restriction can be somewhat overcome by the negation by failure
strategy, but one has to be careful in deﬁning the semantics of such programs
(see Kowalski, 1979, or Apt and Van Emden, 1982).
PROBLEMS
9.5.1.
(a) Give an SLD-resolution and the result substitution for the follow-
ing set of clauses:

446
9/SLD-Resolution And Logic Programming (PROLOG)
French(Jean).
French(Jacques).
British(Peter).
likewine(X, Y ) : −French(X), wine(Y ).
likewine(X, Bordeaux) : −British(X).
wine(Burgundy).
wine(Bordeaux).
: −likewine(U, V ).
(b) Derive all possible answers to the query likewine(U, V ).
9.5.2.
Give an SLD-resolution and the result substitution for the following
set of clauses:
append(cons(A, L1), L2, cons(A, L3)) : −append(L1, L2, L3).
append(nil, L1, L1).
: −append(cons(a, cons(b, nil)), cons(b, cons(c, nil)), Z)
9.5.3.
Give an SLD-resolution and the result substitution for the following
set of clauses:
hanoi(N, Output) : −move(a, b, c, N, Output).
move(A, B, C, succ(M), Output) : −move(A, C, B, M, Out1),
move(C, B, A, M, Out2),
append(Out1, cons(to(A, B), Out2), Output).
move(A, B, C, 0, nil).
append(cons(A, L1), L2, cons(A, L3)) : −append(L1, L2, L3).
append(nil, L1, L1).
: −hanoi(succ(succ(succ(0))), Z)
9.5.4.
Complete the proof of theorem 9.5.1 by ﬁlling in the missing details.
9.5.5.
State and prove a generalization of theorem 9.5.2 for the case of ar-
bitrary logic programs.
∗9.5.6
Given a set S of Horn clauses, an H-tree for S is a tree labeled with
substitution instances of atomic formulae in S deﬁned inductively as
follows:
(i) A tree whose root is labeled with F (false), and having n im-
mediate successors labeled with atomic formulae B1, ..., Bn, where
: −B1, ..., Bn is some goal clause in S, is an H-tree.
(ii) If T is an H-tree, for every leaf node u labeled with some atomic
formulae X that is not a substitution instance of some atomic formula

Notes and Suggestions for Further Reading
447
B in S (a deﬁnite clause consisting of a single atomic formula), if X
is uniﬁable with the lefthand side of any clause A : −B1, ..., Bk in
S, if σ is a most general uniﬁer of X and A, the tree T ′ obtained
by applying the substitution σ to all nodes in T and adding the k
(k > 0) immediate successors σ(B1),...,σ(Bk) to the node u labeled
with σ(X) = σ(A) is an H-tree (if k = 0, the tree T becomes the tree
T ′ obtained by applying the substitution σ to all nodes in T. In this
case, σ(X) is a substitution instance of an axiom.)
An H-tree for S is a proof tree iﬀall its leaves are labeled with substi-
tution instances of axioms in S (deﬁnite clauses consisting of a single
atomic formula).
Prove that S is unsatisﬁable iﬀthere is some H-tree for S which is a
proof tree.
Hint: Use problem 9.2.4 and adapt the lifting lemma.
∗9.5.7
Complete the proof of theorem 9.5.2 by proving that the substitution
ϕ = (t1/z1, . . . , tq/zq) is equal to the restriction of σ ◦η to z1, . . . , zq.
Hint: Let R =< N ′′
0 , . . . , N ′′
r > be the nonground SLD-refutation
obtained by lifting the ground SLD-refutation Rg =< N ′
0, . . . , N ′
r >,
and let < σ′′
1, . . . , σ′′
r > be the sequence of uniﬁers associated with
R. Note that σ = σ′′
1 ◦. . . ◦σ′′
r . Prove that there exists a sequence
< η0, . . . , ηr > of ground substitutions, such that, η0 = ϕ, and for
every i, 1 ≤i ≤r, ηi−1 = λi ◦ηi, where λi denotes the restriction of
σ′′
i to the support of ηi−1. Conclude that ϕ = λ1 ◦. . . ◦λr ◦ηr.
Notes and Suggestions for Further Reading
The method of SLD-resolution is a special case of the SL-resolution of Kowal-
ski and Kuehner (see Siekman and Wrightson, 1983), itself a derivative of
Model Elimination (Loveland, 1978).
To the best of our knowledge, the method used in Sections 9.2 and 9.3 for
proving the completeness of SLD-resolution for (propositional) Horn clauses
by linearizing a Gentzen proof in SLD-form to an SLD-refutation is original.
For an introduction to logic as a problem-solving tool, the reader is re-
ferred to Kowalski, 1979, or Bundy, 1983. Issues about the implementation
of PROLOG are discussed in Campbell, 1983. So far, there are only a few
articles and texts on the semantic foundations of PROLOG, including Kowal-
ski and Van Emden, 1976; Apt and Van Emden, 1982; and Lloyd, 1984. The
results of Section 9.5 for disjunctive goals appear to be original.

Chapter 10
Many-Sorted
First-Order Logic
10.1 Introduction
There are situtations in which it is desirable to express properties of structures
of diﬀerent types (or sorts). For instance, this is the case if one is interested in
axiomatizing data strutures found in computer science (integers, reals, char-
acters, stacks, queues, lists, etc). By adding to the formalism of ﬁrst-order
logic the notion of type (also called sort), one obtains a ﬂexible and convenient
logic called many-sorted ﬁrst-order logic, which enjoys the same properties as
ﬁrst-order logic.
In this chapter, we shall deﬁne and give the basic properties of many-
sorted ﬁrst-order logic. It turns out that the semantics of ﬁrst-order logic can
be given conveniently using the notion of a many-sorted algebra deﬁned in
Section 2.5, given in the appendix. Hence, the reader is advised to review the
appendix before reading this chapter.
At the end of this chapter, we give an algorithm for deciding whether a
quantiﬁer-free formula is valid, using the method of congruence closure due
to Kozen (Kozen, 1976, 1977).
Due to the lack of space, we can only give a brief presentation of many-
sorted ﬁrst-order logic, and most proofs are left as exercises. Fortunately, they
are all simple variations of proofs given in the ﬁrst-order case.
448

10.2 Syntax
449
10.2 Syntax
First, we deﬁne alphabets for many-sorted ﬁrst-order languages.
10.2.1 Many-Sorted First-Order Languages
In contrast to standard ﬁrst-order languages, in many-sorted ﬁrst-order lan-
guages, the arguments of function and predicate symbols may have diﬀerent
types (or sorts), and constant and function symbols also have some type (or
sort). Technically, this means that a many-sorted alphabet is a many-sorted
ranked alphabet as deﬁned in Subsection 2.5.1.
Deﬁnition 10.2.1
The alphabet of a many-sorted ﬁrst-order language con-
sists of the following sets of symbols:
A countable set S ∪{bool} of sorts (or types) containing the special sort
bool, and such that S is nonempty and does not contain bool.
Logical connectives: ∧(and), ∨(or), ⊃(implication), ≡(equivalence),
all of rank (bool.bool, bool), ¬ (not) of rank (bool, bool), ⊥(of rank (e, bool));
Quantiﬁers: For every sort s ∈S, ∀s (for all), ∃s (there exists), each of
rank (bool, bool);
For every sort s in S, the equality symbol .=s, of rank (ss, bool).
Variables: For every sort s ∈S, a countably inﬁnite set Vs = {x0, x1,
x2, ...}, each variable xi being of rank (e, s). The family of sets Vs is denoted
by V.
Auxiliary symbols: “(” and “)”.
An (S ∪{bool})-ranked alphabet L of nonlogical symbols consisting of:
(i) Function symbols: A (countable, possibly empty) set FS of symbols
f0, f1,..., and a rank function r : FS →S+ × S, assigning a pair r(f) = (u, s)
called rank to every function symbol f. The string u is called the arity of f,
and the symbol s the sort (or type) of f.
(ii) Constants: For every sort s ∈S, a (countable, possibly empty) set
CSs of symbols c0, c1,..., each of rank (e, s). The family of sets CSs is denoted
by CS.
(iii) Predicate symbols: A (countable, possibly empty) set PS of symbols
P0, P1,..., and a rank function r : PS →S∗× {bool}, assigning a pair r(P) =
(u, bool) called rank to each predicate symbol P. The string u is called the
arity of P. If u = e, P is a propositional letter.
It is assumed that the sets Vs, FS, CSs, and PS are disjoint for all
s ∈S. We will refer to a many-sorted ﬁrst-order language with set of nonlog-
ical symbols L as the language L. Many-sorted ﬁrst-order languages obtained

450
10/Many-Sorted First-Order Logic
by omitting the equality symbol are referred to as many-sorted ﬁrst-order
languages without equality.
Observe that a standard (one sorted) ﬁrst-order language corresponds
to the special case of a many-sorted ﬁrst-order language for which the set S
of sorts contains a single element.
We now give inductive deﬁnitions for the sets of many-sorted terms and
formulae.
Deﬁnition 10.2.2 Given a many-sorted ﬁrst-order language L, let Γ be the
union of the sets V, CS, FS, PS, and {⊥}, and let Γs be the subset of Γ+
consisting of all strings whose leftmost symbol is of sort s ∈S ∪{bool}.
For every function symbol f of rank (u1...un, s), let Cf be the function
Cf : Γu1 × ... × Γun →Γs such that, for all strings t1, ..., tn, with each ti of
sort ui,
Cf(t1, ..., tn) = ft1...tn.
For every predicate symbol P of arity u1...un, let CP be the function
CP : Γu1 × ... × Γun →Γbool such that, for all strings t1, ..., tn, each ti of sort
ui,
CP (t1, ..., tn) = Pt1...tn.
Also, let Cs.= be the function Cs.= : (Γs)2 →Γbool (of rank (ss, bool)) such
that, for all strings t1, t2 of sort s,
Cs.=(t1, t2) = .=s t1t2.
Finally, the functions C∧, C∨, C⊃, C≡, C¬ are deﬁned as in deﬁnition
3.2.2, with C∧, C∨, C⊃, C≡of rank (bool.bool, bool), C¬ of rank (bool, bool),
and the functions As
i, Es
i : Γbool →Γbool (of rank (bool, bool)) are deﬁned such
that, for any string A in Γbool,
As
i(A) = ∀sxiA, and Es
i (A) = ∃sxiA.
The (S ∪{bool})-indexed family (Γs)s∈(S∪{bool}) is made into a many-
sorted algebra also denoted by Γ as follows:
Each carrier of sort s ∈(S ∪{bool}) is the set of strings Γs;
Each constant c of sort s in CS is interpreted as the string c;
Each predicate symbol P of rank (e, bool) in PS (propositional symbol)
is interpreted as the string P;
The constant ⊥is interpreted as the string ⊥.
The operations are the functions Cf, CP , C∧, C∨, C⊃, C≡, C¬, Cs.=, As
i
and Es
i .
From Subsection 2.5.5, we know that there is a least subalgebra T(L, V)
containing the (S ∪{bool})-indexed family of sets V (with the component of
sort bool being empty).

10.2 Syntax
451
The set of terms TERM s
L of L-terms of sort s (for short, terms of sort
s) is the carrier of sort s of T(L, V), and the set FORML of L-formulae (for
short, formulae) is the carrier of sort bool of T(L, V).
A less formal way of stating deﬁnition 10.2.2 is the following. Terms and
atomic formulae are deﬁned as follows:
(i) Every constant and every variable of sort s is a term of sort s.
(ii) If t1, ..., tn are terms, each ti of sort ui, and f is a function symbol
of rank (u1...un, s), then ft1...tn is a term of sort s.
(iii) Every propositional letter is an atomic formula, and so is ⊥.
(iv) If t1, ..., tn are terms, each ti of sort ui, and P is a predicate symbol
of arity u1...un, then Pt1...tn is an atomic formula; If t1 and t2 are terms of
sort s, then .=s t1t2 is an atomic formula;
Formulae are deﬁned as follows:
(i) Every atomic formula is a formula.
(ii) For any two formulae A and B, (A∧B), (A∨B), (A ⊃B), (A ≡B)
and ¬A are also formulae.
(iii) For any variable xi of sort s and any formula A, ∀sxiA and ∃sxiA
are also formulae.
EXAMPLE 10.2.1
Let L be following many-sorted ﬁrst-order language for stacks, where
S = {stack, integer}, CSinteger = {0}, CSstack = {Λ}, FS = {Succ,
+, ∗, push, pop, top}, and PS = {<}.
The rank functions are given by:
r(Succ) = (integer, integer);
r(+) = r(∗) = (integer.integer, integer);
r(push) = (stack.integer, stack);
r(pop) = (stack, stack);
r(top) = (stack, integer);
r(<) = (integer.integer, bool).
Then, the following are terms:
Succ 0
top push Λ Succ 0
EXAMPLE 10.2.2
Using the ﬁrst-order language of example 10.2.1, the following are for-
mulae:

452
10/Many-Sorted First-Order Logic
< 0 Succ 0
∀integerx∀stacky .=stack pop push y x y .
10.2.2 Free Generation of Terms and Formulae
As in Subsections 5.2.2 and 5.2.3, it is possible to show that terms and for-
mulae are freely generated. This is left as an exercise for the reader.
Theorem 10.2.1
The many-sorted algebra T(L, V) is freely generated by
the family V.
As a consequence, the family TERML of many-sorted terms is freely
generated by the set of constants and variables as atoms and the functions
Cf, and the set of L-formulae is freely generated by the atomic formulae as
atoms and the functions CX (X a logical connective), As
i and Es
i .
Remarks:
(1) Instead of deﬁning terms and atomic formulae in preﬁx notation, one
can deﬁne them as follows (using parentheses):
The second clause of deﬁnition 10.2.2 is changed to: For every function
symbol f of rank (u1...un, s) and any terms t1, ..., tn, with each ti of sort ui,
f(t1, ..., tn) is a term of sort s. Also, atomic formulae are deﬁned as follows:
For every predicate symbol P of arity u1...un, for any terms t1, ..., tn,
with each ti of sort ui, P(t1, ..., tn) is an atomic formula; For any terms t1
and t2 of sort s, (t1 .=s t2) is an atomic formula.
We will also use the notation ∀x : sA and ∃x : sA, instead of ∀sxA and
∃sxA.
One can still show that the terms and formulae are freely generated.
In the sequel, we shall use either notation. For simplicity, we shall also fre-
quently use = instead of .=s and omit parentheses whenever possible, using
the conventions adopted in Chapter 5.
(2) The many-sorted algebra T(L, V) is free on the set of variables V
(as deﬁned in Section 2.5), and is isomorphic to the tree algebra TL(V) (in
TL(V), the term As
i(A) is used instead of ∀xi : sA, and Es
i (A) instead of
∃xi : sA).
10.2.3 Free and Bound Variables, Substitutions
The deﬁnitions of free and bound variables, substitution, and term free for a
variable in a formula given in Chapter 5 extend immediately to the many-
sorted case. The only diﬀerence is that in a substitution s[t/x] or A[t/x] of a
term t for a variable x, the sort of the term t must be equal to the sort of the
variable x.

10.3 Semantics of Many-Sorted First-Order Languages
453
PROBLEMS
10.2.1. Prove theorem 10.2.1.
10.2.2. Examine carefully how the deﬁnitions of Section 5.2 generalize to the
many-sorted case (free and bound variables, substitutions, etc.).
10.2.3. Give a context-free grammar describing many-sorted terms and many-
sorted formulae, as in problem 5.2.8.
10.2.4. Generalize the results of problems 5.2.3 to 5.2.7 to the many-sorted
case.
10.3 Semantics of Many-Sorted First-Order Languages
First, we need to deﬁne many-sorted ﬁrst-order structures.
10.3.1 Many-Sorted First-Order Structures
Given a many-sorted ﬁrst-order language L, the semantics of formulae is de-
ﬁned as in Section 5.3, but using the concept of a many-sorted algebra rather
than the concept of a (one-sorted) structure.
Recall from deﬁnition 10.2.1 that the nonlogical part L of a many-sorted
ﬁrst-order language is an (S ∪{bool})-sorted ranked alphabet.
Deﬁnition 10.3.1
Given a many-sorted ﬁrst-order language L, a many-
sorted L-structure M, (for short, a structure) is a many-sorted L-algebra as
deﬁned in Section 2.5.2, such that the carrier of sort bool is the set BOOL =
{T, F}.
Recall that the carrier of sort s is nonempty and is denoted by Ms.
Every function symbol f of rank (u1...un, s) is interpreted as a function fM :
M u →Ms, with M u = M u1 × ... × M un, u = u1...un, each constant c of sort
s is interpreted as an element cM in Ms, and each predicate P of arity u1...un
is interpreted as a function PM : M u →BOOL.
10.3.2 Semantics of Formulae
We now wish to deﬁne the semantics of formulae by generalizing the deﬁnition
given in Section 5.3.
Deﬁnition 10.3.2
Given a ﬁrst-order many-sorted language L and an L-
structure M, an assignment of sort s is any function vs : Vs →Ms from the
set of variables Vs to the domain Ms. The family of all such functions is de-
noted as [Vs →Ms]. An assignment v is any S-indexed family of assignments
of sort s. The set of all assignments is denoted by [V →M].

454
10/Many-Sorted First-Order Logic
As in Chapter 5, the meaning of a formula is deﬁned recursively using
theorem 2.5.1. In order to apply theorem 2.5.1, it is necessary to deﬁne a
many-sorted algebra M. For this, the next two deﬁnitions are needed.
Deﬁnition 10.3.3
For all i ≥0, for every sort s, the functions (As
i)M and
(Es
i )M (of rank (bool, bool)) from [[V →M] →BOOL] to [[V →M] →
BOOL] are deﬁned as follows: For every function f in [[V →M] →BOOL],
(As
i)M(f) is the function such that: For every assignment v in [V →M],
(As
i)M(f)(v) = F
iﬀ
f(v[xi := a]) = F for some a ∈Ms;
The function (Es
i )M(f) is the function such that: For every assignment
v in [V →M],
(Es
i )M(f)(v) = T
iﬀ
f(v[xi := a]) = T for some a ∈Ms.
Note that (As
i)M(f)(v) = T iﬀthe function gv : Ms →BOOL such
that gv(a) = f(v[xi := a]) for all a ∈Ms, is the constant function whose value
is T, and that (Es
i )M(f)(v) = F iﬀthe function gv : Ms →BOOL deﬁned
above is the constant function whose value is F.
Deﬁnition 10.3.4
Given any L-structure M, the many-sorted algebra M
is deﬁned as follows. For each sort s ∈S, the carrier Ms is the set [[V →
M] →Ms], and the carrier Mbool is the set [[V →M] →BOOL].
The
functions ∧M, ∨M, ⊃M and ≡M of rank (bool.bool, bool) from Mbool ×Mbool
to Mbool, and the function ¬M of rank (bool, bool) from Mbool to Mbool are
deﬁned as follows: For every two functions f, g in Mbool, for every assignment
v in [V →M],
∧M(f, g)(v) = H∧(f(v), g(v));
∨M(f, g)(v) = H∨(f(v), g(v));
⊃M (f, g)(v) = H⊃(f(v), g(v));
≡M (f, g)(v) = H≡(f(v), g(v));
¬M(f)(v) = H¬(f(v)).
For every function symbol f of rank (u, s), every predicate symbol P of
rank (u, bool), and every constant symbol c, the functions fM : Mu →Ms,
PM : Mu →Mbool, and the function cM ∈Ms are deﬁned as follows. For
any (g1, . . . , gn) ∈Mu, (n = |u|), for any assignment v in [V →M],
fM(g1, . . . , gn)(v) = fM(g1(v), . . . , gn(v));
PM(g1, . . . , gn)(v) = PM(g1(v), . . . , gn(v));
cM(v) = cM.
For every variable x of sort s, we also deﬁne xM ∈Ms as the function
such that for every assignment v, xM(v) = vs(x). Let ϕ : V →M be the
function deﬁned such that, ϕ(x) = xM. Since T(L, V) is freely generated
by V, by theorem 2.5.1, there is a unique homomorphism ϕ : T(L, V) →M
extending ϕ. We deﬁne the meaning tM of a term t as ϕ(t), and the meaning
AM of a formula A as ϕ(A). The explicit recursive deﬁnitions follow.

10.3 Semantics of Many-Sorted First-Order Languages
455
Deﬁnition 10.3.5
Given a many-sorted L-structure M and an assignment
v : V →M, the function tM : [V →M] →Ms deﬁned by a term t of sort s is
the function such that for every assignments v in [V →M], the value tM[v]
is deﬁned recursively as follows:
(i) xM[v] = vs(x), for a variable x of sort s;
(ii) cM[v] = cM, for a constant c;
(iii) (ft1...tn)M[v] = fM((t1)M[v], ..., (tn)M[v]).
The recursive deﬁnition of the function AM : [V →M] →BOOL is
now given.
Deﬁnition 10.3.6
The function AM : [V →M] →BOOL is deﬁned recur-
sively by the following clauses:
(1) For atomic formulae: AM is the function such that, for every assign-
ment v,
(Pt1...tn)M[v] = PM((t1)M[v], ..., (tn)M[v]);
( .=s t1t2)M[v] = if (t1)M[v] = (t2)M[v] then T else F;
(⊥)M[v] = F;
(2) For nonatomic formulae:
(A ∗B)M = ∗M(AM, BM), where ∗is in {∧, ∨, ⊃, ≡}, and ∗M is the
corresponding function deﬁned in deﬁnition 10.3.4;
(¬A)M = ¬M(AM);
(∀xi : sA)M = (As
i)M(AM);
(∃xi : sA)M = (Es
i )M(AM).
Note that by deﬁnitions 10.3.3, 10.3.4, 10.3.5, and 10.3.6, for every as-
signment v,
(∀xi : sA)M[v] = T
iﬀ
AM[v[xi := m]] = T, for all m ∈Ms, and
(∃xi : sA)M[v] = T
iﬀ
AM[v[xi := m]] = T, for some m ∈Ms.
The notions of satisfaction, validity, and model are deﬁned as in Sub-
section 5.3.3. As in Subsection 5.3.4, it is also possible to deﬁne the semantics
of formulae using modiﬁed formulae obtained by substitution.
10.3.3 An Alternate Semantics
The following result analogous to lemma 5.3.2 can be shown.
Lemma 10.3.1
For any formula B, for any assignment v in [V →M], and
any variable xi of sort s, the following hold:
(1) (∀xi : sB)M[v] = T iﬀ(B[m/xi])M[v] = T for all m ∈Ms;
(2) (∃xi : sB)M[v] = T iﬀ(B[m/xi])M[v] = T for some m ∈Ms.

456
10/Many-Sorted First-Order Logic
In view of lemma 10.3.1, the recursive clauses of the deﬁnition of satis-
faction can also be stated more informally as follows:
M |= (¬A)[v] iﬀnot M |= A[v],
M |= (A ∧B)[v] iﬀM |= A[v] and M |= B[v],
M |= (A ∨B)[v] iﬀM |= A[v] or M |= B[v],
M |= (A ⊃B)[v] iﬀnot M |= A[v] or M |= B[v],
M |= (A ≡B)[v] iﬀ(M |= A[v] iff M |= B[v]),
M |= (∀xi : sA)[v] iﬀM |= (A[a/xi])[v] for every a ∈Ms,
M |= (∃xi : sA)[v] iﬀM |= (A[a/xi])[v] for some a ∈Ms.
It is also easy to show that the semantics of a formula A only depends
on the set FV (A) of variables free in A.
10.3.4 Semantics and Free Variables
The following lemma holds.
Lemma 10.3.2
Given a formula A with set of free variables {y1, ..., yn},
for any two assignments s1, s2 such that s1(yi) = s2(yi), for 1 ≤i ≤n,
AM[s1] = AM[s2].
10.3.5 Subformulae and Rectiﬁed Formulae
Subformulae and rectiﬁed formulae are deﬁned as in Subsection 5.3.6, and
lemma 5.3.4 can be generalized easily. Similarly, the results of the rest of
Section 5.3 can be generalized to many-sorted logic. The details are left as
exercises.
PROBLEMS
10.3.1. Prove lemma 10.3.1.
10.3.2. Prove lemma 10.3.2.
10.3.3. Generalize lemma 5.3.4 to the many-sorted case.
10.3.4. Examine the generalization of the other results of Section 5.3 to the
many-sorted case.
10.4 Proof Theory of Many-Sorted Languages
The system G deﬁned in Section 5.4 is extended to the many-sorted case as
follows.

10.4 Proof Theory of Many-Sorted Languages
457
10.4.1 Gentzen System G for Many-Sorted Languages
Without Equality
We ﬁrst consider the case of a many-sorted ﬁrst-order language L without
equality.
Deﬁnition 10.4.1
(Gentzen system G for languages without equality) The
symbols Γ, ∆, Λ will be used to denote arbitrary sequences of formulae and
A, B to denote formulae. The rules of the sequent calculus G are the following:
Γ, A, B, ∆→Λ
Γ, A ∧B, ∆→Λ (∧: left)
Γ →∆, A, Λ
Γ →∆, B, Λ
Γ →∆, A ∧B, Λ
(∧: right)
Γ, A, ∆→Λ
Γ, B, ∆→Λ
Γ, A ∨B, ∆→Λ
(∨: left)
Γ →∆, A, B, Λ
Γ →∆, A ∨B, Λ (∨: right)
Γ, ∆→A, Λ
B, Γ, ∆→Λ
Γ, A ⊃B, ∆→Λ
(⊃: left)
A, Γ →B, ∆, Λ
Γ →∆, A ⊃B, Λ (⊃: right)
Γ, ∆→A, Λ
Γ, ¬A, ∆→Λ (¬ : left)
A, Γ →∆, Λ
Γ →∆, ¬A, Λ (¬ : right)
In the quantiﬁer rules below, x is any variable of sort s and y is any
variable of sort s free for x in A and not free in A, unless y = x (y /∈
FV (A) −{x}). The term t is any term of sort s free for x in A.
Γ, A[t/x], ∀x : sA, ∆→Λ
Γ, ∀x : sA, ∆→Λ
(∀: left)
Γ →∆, A[y/x], Λ
Γ →∆, ∀x : sA, Λ (∀: right)
Γ, A[y/x], ∆→Λ
Γ, ∃x : sA, ∆→Λ (∃: left)
Γ →∆, A[t/x], ∃x : sA, Λ
Γ →∆, ∃x : sA, Λ
(∃: right)
Note that in both the (∀: right)-rule and the (∃: left)-rule, the variable
y does not occur free in the lower sequent. In these rules, the variable y is
called the eigenvariable of the inference. The condition that the eigenvariable
does not occur free in the conclusion of the rule is called the eigenvariable
condition. The formula ∀x : sA (or ∃x : sA) is called the principal formula
of the inference, and the formula A[t/x] (or A[y/x]) the side formula of the
inference.
The axioms of G are all sequents Γ →∆such that Γ and ∆contain a
common formula.

458
10/Many-Sorted First-Order Logic
10.4.2 Deduction Trees for the System G
Deduction trees and proof trees are deﬁned as in Subsection 5.4.2.
10.4.3 Soundness of the System G
The soundness of the system G is obtained easily from the proofs given in
Subsection 5.4.3.
Lemma 10.4.1 (Soundness of G, many-sorted case) Every sequent provable
in G is valid.
10.4.4 Completeness of G
It is also possible to prove the completeness of the system G by adapting the
deﬁnitions and proofs given in Sections 5.4 and 5.5 to the many-sorted case.
For this it is necessary to modify the deﬁnition of a Hintikka set so that it
applies to a many-sorted algebra. We only state the result, leaving the proof
as a sequence of problems.
Theorem 10.4.1 (G¨odel’s extended completeness theorem for G) A sequent
(even inﬁnite) is valid iﬀit is G-provable.
10.5 Many-Sorted First-Order Logic With Equality
The equality rules for many-sorted languages with equality are deﬁned as
follows.
10.5.1 Gentzen System G= for Languages With Equality
Let G= be the Gentzen system obtained from the Gentzen system G (deﬁned
in deﬁnition 10.4.1) by adding the following rules.
Deﬁnition 10.5.1
(Equality rules for G=) Let Γ, ∆, Λ denote arbitrary se-
quences of formulae (possibly empty) and let t, s1, ..., sn, t1, ..., tn denote ar-
bitrary L-terms. For every sort s, for every term t of sort s:
Γ, t .=s t →∆
Γ →∆
For each function symbol f of rank (u1...un, s) and any terms s1, ..., sn,
t1, ..., tn such that si and ti are of sort ui:
Γ, (s1 .=u1 t1) ∧... ∧(sn .=un tn) ⊃(fs1...sn .=s ft1...tn) →∆
Γ →∆

10.5 Many-Sorted First-Order Logic With Equality
459
For each predicate symbol P (including .=s) of arity u1...un and any
terms s1, ..., sn, t1, ..., tn such that si and ti are of sort ui:
Γ, ((s1 .=u1 t1) ∧... ∧(sn .=un tn) ∧Ps1...sn) ⊃Pt1...tn →∆
Γ →∆
10.5.2 Soundness of the System G=
The following lemma is easily shown.
Lemma 10.5.1
If a sequent is G=-provable then it is valid.
10.5.3 Completeness of the System G=
It is not diﬃcult to adapt the proofs of Section 5.6 to obtain the following
completeness theorem.
Theorem 10.5.1
(G¨odel’s extended completeness theorem for G=) Let L
be a many-sorted ﬁrst-order language with equality. A sequent (even inﬁnite)
is valid iﬀit is G=-provable.
10.5.4 Reduction of Many-Sorted Logic to One-Sorted
Logic
Although many-sorted ﬁrst-order logic is very convenient, it is not an essential
extension of standard one-sorted ﬁrst-order logic, in the sense that there is a
translation of many-sorted logic into one-sorted logic. Such a translation is
described in Enderton, 1972, and the reader is referred to it for details. The
essential idea to convert a many-sorted language L into a one-sorted language
L′ is to add domain predicate symbols Ds, one for each sort, and to modify
quantiﬁed formulae recursively as follows:
Every formula A of the form ∀x : sB (or ∃x : sB) is converted to the
formula A′ = ∀x(Ds(x) ⊃B′) (∃x : sB is converted to ∃x(Ds(x)∧B′)), where
B′ is the result of converting B.
We also deﬁne the set MS to be the set of all one-sorted formulae of the
form:
(1) ∃xDs(x), for every sort s, and
(2) ∀x1...∀xn(Ds1(x1) ∧... ∧Dsn(xn)
⊃
Ds(f(x1, ..., xn))), for every
function symbol f of rank (s1...sn, s).
Then, the following lemma can be shown.

460
10/Many-Sorted First-Order Logic
Lemma 10.5.2 Given a many-sorted ﬁrst-order language L, for every set T
of many-sorted formulae in L and for every many-sorted formula A in L,
T ⊢A
iﬀ
T ′ ∪MS ⊢A′,
in the translated one-sorted language L′,
where T ′ is the set of formulae in T translated into one-sorted formulae, and
A′ is the one-sorted translation of A.
Lemma 10.5.2 can be used to transfer results from one-sorted logic to
many-sorted logic. In particular, the compactness theorem, model existence
theorem, and L¨owenheim-Skolem theorem hold in many-sorted ﬁrst-order
logic.
PROBLEMS
10.4.1. Prove lemma 10.4.1.
10.4.2. Deﬁne many-sorted Hintikka sets for languages without equality, and
prove lemma 5.4.5 for the many-sorted case.
10.4.3. Prove the completeness theorem (theorem 10.4.1) for many-sorted
logic without equality.
10.5.1. Prove lemma 10.5.1.
10.5.2. Generalize the other theorems of Section 5.5 to the many-sorted case
(compactness, model existence, L¨owenheim-Skolem).
10.5.3. Deﬁne many-sorted Hintikka sets for languages with equality, and
prove lemma 5.6.1 for the many-sorted case.
10.5.3. Prove the completeness theorem (theorem 10.5.1) for many-sorted
logic with equality.
10.5.4. Generalize the other theorems of Section 5.6 to the many-sorted case
(compactness, model existence, L¨owenheim-Skolem).
10.5.5. Prove lemma 10.5.2.
10.6 Decision Procedures Based on Congruence Closure
In this section, we show that there is an algorithm for deciding whether a
quantiﬁer-free formula without predicate symbols in a many-sorted language
with equality is valid, using a method due to Kozen (Kozen, 1976, 1977) and
Nelson and Oppen (Nelson and Oppen, 1980). Then, we show how this algo-
rithm can easily be extended to deal with arbitrary quantiﬁer-free formulae
in a many-sorted language with equality.

10.6 Decision Procedures Based on Congruence Closure
461
10.6.1 Decision Procedure for Quantiﬁer-free Formulae
Without Predicate Symbols
First, we state the following lemma whose proof is left as an exercise.
Lemma 10.6.1
Given any quantiﬁer-free formula A in a many-sorted ﬁrst-
order language L, a formula B in disjunctive normal form such that A ≡B
is valid can be constructed.
Using lemma 10.6.1, observe that a quantiﬁer-free formula A is valid iﬀ
the disjunctive normal form B of ¬A is unsatisﬁable. But a formula B =
C1 ∨... ∨Cn in disjunctive normal form is unsatisﬁable iﬀevery disjunct Ci
is unsatisﬁable. Hence, in order to have an algorithm for deciding validity
of quantiﬁer-free formulae, it is enough to have an algorithm for deciding
whether a conjunction of literals is unsatisﬁable.
If the language L has no equality symbols, the problem is trivial since
a conjunction of literals is unsatisﬁable iﬀit contains some atomic formula
Pt1...tn and its negation.
Otherwise, we follow a method due to Kozen
(Kozen, 1976, 1977) and Nelson and Oppen (Nelson and Oppen, 1980). First,
we assume that the language L does not have predicate symbols. Then, every
conjunct C consists of equations and of negations of equations:
t1 .=s1 u1 ∧... ∧tm .=sm um ∧¬r1 .=s′
1 v1 ∧... ∧¬rn .=s′
n vn
We give a method inspired from Kozen and Nelson and Oppen for de-
ciding whether a conjunction C as above is unsatisﬁable. For this, we deﬁne
the concept of a congruence on a graph.
10.6.2 Congruence Closure on a Graph
First, we deﬁne the notion of a labeled graph. We are considering graphs in
which for every node u, the set of immediate successors is ordered. Also, every
node is labeled with a function symbol from a many-sorted ranked alphabet,
and the labeling satisﬁes a condition similar to the condition imposed on
many-sorted terms.
Deﬁnition 10.6.1 A ﬁnite labeled graph G is a quadruple (V, Σ, Λ, δ), where:
V is an S-indexed family of ﬁnite sets Vs of nodes (or vertices);
Σ is an S-sorted ranked alphabet;
Λ : V
→Σ is a labeling function assigning a symbol Λ(v) in Σ to every
node v in V ;
δ : {(v, i) | v ∈V, 1 ≤i ≤n, r(Λ(v)) = (u1...un, s)} →V ,
is a function called the successor function.

462
10/Many-Sorted First-Order Logic
The functions Λ and δ also satisfy the following properties: For every
node v in Vs, the rank of the symbol Λ(v) labeling v is of the form (u1...un, s),
and for every node δ(v, i), the sort of the symbol Λ(δ(v, i)) labeling δ(v, i) is
equal to ui.
For every node v, δ(v, i) is called the i-th successor of v, and is also
denoted as v[i]. Note that for a node v such that r(Λ(v)) = (e, s), δ is not
deﬁned. Such a node is called a terminal node, or leaf . Given a node u, the
set Pu of predecessors of u is the set {v ∈V | δ(v, i) = u, for some i}. A
node u such that Pu = ∅is called an initial node, or root. A pair (v, i) as in
the deﬁnition of δ is also called an edge.
Next, we deﬁne a certain kind of equivalence relation on a graph called
a congruence. A congruence is an equivalence relation closed under a certain
form of implication.
Deﬁnition 10.6.2
Given a (ﬁnite) graph G = (V, Σ, Λ, δ), an S-indexed
family R of relations Rs over Vs is a G-congruence (for short, a congruence)
iﬀ:
(1) Each Rs is an equivalence relation;
(2) For every pair (u, v) of nodes in V 2, if Λ(u) = Λ(v) and r(Λ(u)) =
(e, s) then uRsv, else if Λ(u) = Λ(v), r(Λ(u)) = (s1...sn, s), and for every i,
1 ≤i ≤n, u[i]Rsiv[i], then uRsv.
In particular, note that any two terminal nodes labeled with the same
symbol of arity e are congruent.
Graphically, if u and v are two nodes labeled with the same symbol f
of rank (s1...sn, s), if u[1], ..., u[n] are the successors of u and v[1], ..., v[n] are
the successors of v,
u
u[1]
u[n]
· · ·
v
v[1]
v[n]
· · ·
if u[i] and v[i] are equivalent for all i, 1 ≤i ≤n, then u and v are also
equivalent. Hence, we have a kind of backward closure.
We will prove shortly that given any ﬁnite graph and S-indexed family R
of relations on G, there is a smallest congruence on G containing R, called the
congruence closure of R. First, we show how the congruence closure concept
can be used to give an algorithm for deciding unsatisﬁability.
10.6.3 The Graph Associated With a Conjunction
The key to the algorithm for deciding unsatisﬁability is the computation of a
certain congruence over a ﬁnite graph induced by C and deﬁned below.

10.6 Decision Procedures Based on Congruence Closure
463
Deﬁnition 10.6.3
Given a conjunction C of the form
t1 .=s1 u1 ∧... ∧tm .=sm um ∧¬r1 .=s′
1 v1 ∧... ∧¬rn .=s′
n vn,
let TERM(C) be the set of all subterms of terms occurring in the conjunction
C, including the terms ti, ui, rj, vj themselves. Let S(C) be the set of sorts
of all terms in TERM(C). For every sort s in S(C), let TERM(C)s be the
set of all terms of sort s in TERM(C). Note that by deﬁnition, each set
TERM(C)s is nonempty. Let Σ be the S(C)-ranked alphabet consisting of
all constants, function symbols, and variables occurring in TERM(C). The
graph G(C) = (TERM(C), Σ, Λ, δ) is deﬁned as follows:
For every node t in TERM(C), if t is either a variable or a constant
then Λ(t) = t, else t is of the form fy1...yk and Λ(t) = f;
For every node t in TERM(C), if t is of the form fy1...yk, then for every
i, 1 ≤i ≤k, δ(t, i) = yi, else t is a constant or a variable and it is a terminal
node of G(C).
Finally, let E = {(t1, u1), ..., (tm, um)} be the set of pairs of terms oc-
curring in the positive (nonnegated) equations in the conjunction C.
EXAMPLE 10.6.1
Consider the alphabet in which S = {i, s}, Σ = {f, g, a, b, c}, with r(f) =
(is, s), r(g) = (si, s), r(a) = i, r(b) = r(c) = s. Let C be the conjunction
f(a, b) .= c ∧¬g(f(a, b), a) .= g(c, a).
Then, TERM(C)i = {a}, TERM(C)s = {b, c, f(a, b), g(c, a), g(f(a, b),
a)}, and E = {(f(a, b), c)}. The graph G(C) is the following:
g
a
f
g
b
c
The key to the decision procedure is that the least congruence on G(C)
containing E exists, and that there is an algorithm for computing it. Indeed,
assume that this least congruence
∗
←→E containing E (called the congruence
closure of E) exists and has been computed. Then, we have the following
result.
Lemma 10.6.2
Given a conjunction C of the form
t1 .=s1 u1 ∧... ∧tm .=sm um ∧¬r1 .=s′
1 v1 ∧... ∧¬rn .=s′
n vn

464
10/Many-Sorted First-Order Logic
of equations and negations of equations, if
∗
←→E
is the congruence closure
on G(C) of the relation E = {(t1, u1), ..., (tm, um)}, then
C
is unsatisﬁable iﬀfor some j, 1 ≤j ≤n,
rj
∗
←→E vj.
Proof : Let E = {t1 .=s1 u1, · · · , tm .=sm um}. First, we prove that the
S(C)-indexed family R of relations Rs on TERM(C) deﬁned such that
tRsu
iﬀ
E |= t .=s u,
is a congruence on G(C) containing E. It is obvious that
E |= ti .=si ui
for every (ti, ui) in E, 1 ≤i ≤m, and so
tiRsiui.
For every two subterms of the form fy1...yk and fz1...zk such that f is of
rank (w1...wk, s), if for every i, 1 ≤i ≤k,
E |= yi .=wi zi
then by the deﬁnition of the semantics of equality,
E |= fy1...yk .=s fz1...zk.
Hence, R is a congruence on G(C) containing E. Since
∗
←→E
is the least
congruence on G(C) containing E,
if
rj
∗
←→E vj,
then
E |= rj .=s′
j vj.
But then, since any model satisfying C satisﬁes E, both rj .=s′
j vj and ¬rj .=s′
j
vj would be satisﬁed, a contradiction. We conclude that if for some j, 1 ≤
j ≤n,
rj
∗
←→E vj,
then C is unsatisﬁable. Conversely, assuming that there is no j, 1 ≤j ≤n,
such that
rj
∗
←→E vj,
we shall construct a model M of C. First, we make the S(C)-indexed family
TERM(C) into a many-sorted Σ-algebra C as follows:
For each sort s in S(C), each constant or variable t of sort s is interpreted
as the term t itself.

10.6 Decision Procedures Based on Congruence Closure
465
For every function symbol f in Σ of rank (w1...wk, s), for every k terms
y1, ..., yk in TERM(C), each yi being of sort wi, 1 ≤i ≤k,
fC(y1, ..., yk) =















fy1...yk
if fy1...yk ∈TERM(C)s;
fz1...zk
if fy1...yk /∈TERM(C)s and there are terms
z1, ..., zk such that, yi
∗
←→E zi, and
fz1...zk ∈TERM(C)s;
t0
otherwise, where t0 is some arbitrary term
chosen in TERM(C)s.
Next, we prove that
∗
←→E
is a congruence on C. Indeed, for ev-
ery function symbol f in Σ of rank (w1...wk, s), for every k pairs of terms
(y1, z1), ..., (yk, zk), with yi, zi of sort wi, 1 ≤i ≤k, if yi
∗
←→E zi, then:
(1) If fy1...yk and fz1...zk are both in TERM(C), then
fC(y1, ..., yk) = fy1...yk,
and
fC(z1, ..., zk) = fz1...zk,
and since
∗
←→E
is a congruence on G(C), we have fy1...yk
∗
←→E fz1...zk.
Hence, fC(y1, ..., yk)
∗
←→E fC(z1, ..., zk);
(2) fy1...yk /∈TERM(C), or fz1...zk /∈TERM(C), but there are
some terms z′
1, . . . , z′
k ∈TERM(C), such that, yi
∗
←→E z′
i and fz′
1 . . . z′
k ∈
TERM(C). Since yi
∗
←→E zi, there are also terms z′′
1 , . . . , z′′
k ∈TERM(C)
such that, zi
∗
←→E z′′
i and fz′′
1 . . . z′′
k ∈TERM(C). Then,
fC(y1, . . . , yk) = fz′
1 . . . z′
k
and
fC(z1, . . . , zk) = fz′′
1 . . . z′′
k.
Since yi
∗
←→E zi, we have, z′
i
∗
←→E z′′
i , and so, fz′
1 . . . z′
k
∗
←→E fz′′
1 . . . z′′
k,
that is,
fC(y1, . . . , yk)
∗
←→E fC(z1, . . . , zk).
(3) If neither fy1...yk nor fz1...zk is in TERM(C) and (2) does not
hold, then
fC(y1, ..., yk) = fC(z1, ..., zk) = t0
for some chosen term t0 in TERM(C), and we conclude using the reﬂexivity
of
∗
←→E .
Let M be the quotient of the algebra C by the congruence
∗
←→E , as
deﬁned in Subsection 2.5.8. Let v be the assignment deﬁned as follows: For
every variable x occurring in some term in TERM(C), v(x) is the congruence
class [x] of x.
We claim that for every term t in TERM(C),
tM[v] = [t],

466
10/Many-Sorted First-Order Logic
the congruence class of t. This is easily shown by induction and is left as an
exercise. But then, C is satisﬁed by v in M. Indeed, for every (ti, ui) in E,
[ti] = [ui],
and so
M |= (ti .=si ui)[v],
and for every j, 1 ≤j ≤n, since we have assumed that the congruence classes
[rj] and [vj] are unequal,
M |= (¬rj .=s′
j vj)[v].
The above lemma provides a decision procedure for quantiﬁer-free for-
mulae without predicate symbols. It only remains to prove that
∗
←→E exists
and to give an algorithm for computing it. First, we give an example.
EXAMPLE 10.6.2
Recall the conjunction C of example 10.6.1:
f(a, b) .= c ∧¬g(f(a, b), a) .= g(c, a).
Then, TERM(C)i = {a}, TERM(C)s = {b, c, f(a, b), g(c, a), g(f(a, b),
a)}, and it is not diﬃcult to verify that the congruence closure of the
relation E = {(f(a, b), c)} has the following congruence classes: {a}, {b},
{f(a, b), c} and {g(f(a, b), a), g(c, a)}. A possible candidate for the al-
gebra C is given by the following table:
f
g
(a, b)
f(a, b)
(b, a)
b
(c, a)
g(c, a)
(a, c)
b
(f(a, b), a)
g(f(a, b), a)
(a, f(a, b))
b
(a, g(c, a))
b
(g(c, a), a)
b
(a, g(f(a, b), a))
b
(g(f(a, b), a), a)
b
By lemma 10.6.2, C is unsatisﬁable.

10.6 Decision Procedures Based on Congruence Closure
467
10.6.4 Existence of the Congruence Closure
We now prove that the congruence closure of a relation R on a ﬁnite graph G
exists.
Lemma 10.6.3
(Existence of the congruence closure) Given a ﬁnite graph
G = (V, Σ, Λ, δ) and a relation R on V , there is a smallest congruence
∗
←→R
on G containing R.
Proof : We deﬁne a sequence Ri of S-indexed families of relations induc-
tively as follows: For every sort s ∈S,
R0
s = Rs∪{(u, v) ∈V 2 | Λ(u) = Λ(v), and r(Λ(u)) = (e, s)}∪{(u, u) | u ∈V };
For every sort s ∈S,
Ri+1
s
= Ri
s ∪{(v, u) ∈V 2 | (u, v) ∈Ri
s}
∪{(u, w) ∈V 2 | ∃v ∈V, (u, v) ∈Ri
s and (v, w) ∈Ri
s}
∪{(u, v) ∈V 2 | Λ(u) = Λ(v), r(Λ(u)) = (s1...sn, s),
and u[j]Ri
sjv[j], 1 ≤j ≤n}.
Let
(
∗
←→R)s =

Ri
s.
It is easily shown by induction that every congruence on G containing
R contains every Ri, and that
∗
←→R is a congruence on G. Hence,
∗
←→R is
the least congruence on G containing R.
Since the graph G is ﬁnite, there must exist some integer i such that
Ri = Ri+1. Hence, the congruence closure of R is also computable. We shall
give later a better algorithm due to Nelson and Oppen. But ﬁrst, we show
how the method of Subsection 10.6.3 can be used to give an algorithm for
deciding the unsatisﬁability of a conjunction of literals over a many-sorted
language with equality (and function, constant, and predicate symbols).
10.6.5 Decision Procedure for Quantiﬁer-free Formulae
The crucial observation that allows us to adapt the congruence closure method
is the following:
For any atomic formula Pt1...tk, if ⊤represents the logical constant
always interpreted as T, then Pt1...tk is logically equivalent to (Pt1...tk ≡⊤),
in the sense that
Pt1...tk ≡(Pt1...tk ≡⊤)
is valid.

468
10/Many-Sorted First-Order Logic
But then, this means that ≡behaves semantically exactly as the identity
relation on BOOL. Hence, we can treat ≡as the equality symbol .=bool of sort
bool, and interpret it as the identity on BOOL.
Hence, every conjunction C of literals is equivalent to a conjunction C′ of
equations and negations of equations, such that every atomic formula Pt1...tk
in C is replaced by the equation Pt1...tk ≡⊤, and every formula ¬Pt1...tk in
C is replaced by ¬(Pt1...tk ≡⊤).
Then, as in Subsection 10.6.3, we can build the graph G(C′) associated
with C′, treating ⊤as a constant of sort bool, and treating every predicate
symbol P of arity u1...uk as a function symbol of rank (u1...uk, bool). We then
have the following lemma generalizing lemma 10.6.2.
Lemma 10.6.4
Let C′ be the conjunction obtained from a conjunction of
literals obtained by changing literals of the form Pt1...tk or ¬Pt1...tk into
equations as explained above. If C′ is of the form
t1 .=s1 u1 ∧... ∧tm .=sm um ∧¬r1 .=s′
1 v1 ∧... ∧¬rn .=s′
n vn
and if
∗
←→E
is the congruence closure on G(C) of the relation E = {(t1,
u1), ..., (tm, um)}, then
C
is unsatisﬁable iﬀfor some j, 1 ≤j ≤n,
rj
∗
←→E vj.
Proof : We deﬁne E = {t1 .=s1 u1, · · · , tm .=sm um} and TERM(C) as in
Subsection 10.6.3, except that S(C) also contains the sort bool, and we have
a set of terms of sort bool. First, we prove that the S(C)-indexed family R of
relations Rs on TERM(C) deﬁned such that
tRsu
iﬀ
E |= t .=s u,
is a congruence on G(C) containing E. For function symbols of sort s ̸= bool,
the proof is identical to the proof of lemma 10.6.2. For every two subterms
of the form Py1...yk and Pz1...zk such that P is of rank (w1...wk, bool), if for
every i, 1 ≤i ≤k,
E |= yi .=wi zi,
then by the deﬁnition of the semantics of equality symbols,
E |= Py1...yk ≡Pz1...zk.
Hence, R is a congruence on G(C) containing E. Now there are two cases.
(i) If rj
∗
←→E vj and the sort of rj and vj is not bool, we conclude as in
the proof of lemma 10.6.2.
(ii) Otherwise, rj is some atomic formula Py1...yk and vj is the constant
⊤. In that case,
E |= rj.

10.6 Decision Procedures Based on Congruence Closure
469
But then, since any model satisfying C satisﬁes E, both rj and ¬rj would be
satisﬁed, a contradiction. We conclude that if for some j, 1 ≤j ≤n,
rj
∗
←→E vj,
then C is unsatisﬁable. Conversely, assuming that there is no j, 1 ≤j ≤n,
such that
rj
∗
←→E vj,
we shall construct a model M of C. First, we make the S(C)-indexed family
TERM(C) into a many-sorted Σ-algebra C as in the proof of lemma 10.6.2.
The new case is the case of symbols of sort bool. For every predicate symbol
P of rank (w1...wk, bool), for every k terms y1, ..., yk ∈TERM(C), each yi
being of sort wi, 1 ≤i ≤k,
PC(y1, ..., yk) =











T
if Py1...yk ∈TERM(C) and Py1...yk
∗
←→E ⊤;
T
if Py1...yk /∈TERM(C), there are terms z1, ..., zk
such that, yi
∗
←→E zi, Pz1...zk ∈TERM(C),
and Pz1...zk
∗
←→E ⊤;
F
otherwise.
Next, we prove that
∗
←→E is a congruence on C. The case of symbols of
sort ̸= bool has already been treated in the proof of lemma 10.6.2, and we only
need to consider the case of predicate symbols. For every predicate symbol
P of rank (w1...wk, bool), for every k pairs of terms (y1, z1), ..., (yk, zk), with
yi, zi of sort wi, 1 ≤i ≤k, if yi
∗
←→E zi, then:
(1) Py1...yk ∈TERM(C) and Pz1...zk ∈TERM(C). Since yi
∗
←→E
zi and
∗
←→E
is a graph congruence, we have Py1...yk
∗
←→E Pz1...zk.
Hence, Py1...yk
∗
←→E ⊤iﬀPz1...zk
∗
←→E ⊤, that is, PC(y1, ..., yk) = T
iﬀPC(z1, ..., zk) = T.
(2) Py1...yk /∈TERM(C) or Pz1...zk /∈TERM(C). In this case, if
PC(y1, ..., yk) = T, this means that there are terms z′
1, . . . , z′
k ∈TERM(C)
such that, yi
∗
←→E z′
i, Pz′
1 . . . z′
k ∈TERM(C), and Pz′
1 . . . z′
k
∗
←→E ⊤. Since
yi
∗
←→E zi, we also have zi
∗
←→E z′
i. Since Pz′
1 . . . z′
k ∈TERM(C), and
Pz′
1 . . . z′
k
∗
←→E ⊤, we have, PC(z1, . . . , zk) = T. The same argument shows
that if PC(z1, ..., zk) = T, then PC(y1, ..., yk) = T. Hence, we have shown
that PC(y1, ..., yk) = T iﬀPC(z1, ..., zk) = T.
This concludes the proof that
∗
←→E
is a congruence. Let M be the
quotient of the algebra C by the congruence
∗
←→E , as deﬁned in Subsec-
tion 2.5.8. Let v be the assignment deﬁned as follows: For every variable x
occurring in some term in TERM(C), v(x) is the congruence class [x] of x.
As in the proof of lemma 10.6.2, it is easily shown that for every term t
in TERM(C),
tM[v] = [t],

470
10/Many-Sorted First-Order Logic
the congruence class of t. But then, C is satisﬁed by v in M. The case of
equations not involving predicate symbols in treated as in the proof of lemma
10.6.2.
Clearly, for every equation Py1...yk ≡⊤in C′, by the deﬁnition
of M, M |= Py1...yk[v].
Since we have assumed that for every negation
¬(Py1...yk ≡⊤) in C′, it is not the case that Py1...yk
∗
←→E ⊤, by the
deﬁnition of M, M |= ¬Py1...yk[v]. This concludes the proof.
The above lemma provides a decision procedure for quantiﬁer-free for-
mulae.
EXAMPLE 10.6.3
Consider the following conjunction C over a (one-sorted) ﬁrst-order lan-
guage:
f(f(f(a))) .= a ∧f(f(f(f(f(a))))) .= a ∧P(a) ∧¬P(f(a)),
where f is a unary function symbol and P a unary predicate. First, C
is converted to C′:
f(f(f(a))) .= a ∧f(f(f(f(f(a))))) .= a ∧P(a) ≡⊤∧¬P(f(a)) ≡⊤.
The graph G(C) corresponding to C′ is the following:
V 6
V 5
V 4
V 3
V 2
V 1
a
⊤
V 7
P
P
V 9
V 8
f
f
f
f
f
Initially, R = {(V 1, V 4), (V 1, V 6), (V 8, V 7)}, and let R′ be its con-
gruence closure.
Since (V 1, V 4) is in R, (V 2, V 5) is in R′.
Since
(V 2, V 5) is in R′, (V 3, V 6) is in R′. Since both (V 1, V 6) and (V 3, V 6)
are in R′, (V 1, V 3) is in R′. Hence, (V 2, V 4) is in R′. So the nodes
V 1, V 2, V 3, V 4, V 5, V 6 are all equivalent under R′.
But then, since
(V 1, V 2) is in R′, (V 8, V 9) is in R′, and since (V 8, V 7) is in R, (V 9, V 7)
is in R′. Consequently, P(f(a)) is equivalent to ⊤, and C′ is unsatisﬁ-
able.

10.6 Decision Procedures Based on Congruence Closure
471
10.6.6 Computing the Congruence Closure
We conclude this section by giving an algorithm due to Nelson and Oppen
(Nelson and Oppen, 1980) for computing the congruence closure R′ of a rela-
tion R on a graph G. This algorithm uses a procedure MERGE that, given
a graph G, a congruence R on G, and a pair (u, v) of nodes, computes the
congruence closure of R ∪{(u, v)}. An equivalence relation is represented by
its corresponding partition; that is, the set of its equivalence classes. Two
procedures for operating on partitions are assumed to be available: UNION
and FIND.
UNION(u, v) combines the equivalence classes of nodes u and v into a
single class. FIND(u) returns a unique name associated with the equivalence
class of node u.
The recursive procedure MERGE(R, u, v) makes use of the function
CONGRUENT(R, u, v) that determines whether two nodes u, v are congru-
ent.
Deﬁnition 10.6.4
(Oppen and Nelson’s congruence closure algorithm)
Function CONGRUENT
function CONGRUENT(R′: congruence; u, v: node): boolean;
var flag: boolean; i, n: integer;
begin
if Λ(u) = Λ(v) then
let n = |w| where r(Λ(u)) = (w, s);
flag := true;
for i := 1 to n do
if FIND(u[i]) <> FIND(v[i]) then
flag := false
endif
endfor;
CONGRUENT := flag
else
CONGRUENT := false
endif
end
Procedure MERGE
procedure MERGE(var R′: congruence; u, v: node);
var X, Y : set-of-nodes; x, y: node;
begin
if FIND(u) <> FIND(v) then
X := the union of the sets Px of predecessors of all
nodes x in [u], the equivalence class of u;

472
10/Many-Sorted First-Order Logic
Y := the union of the sets Py of predecessors of all
nodes y in [v], the equivalence class of v;
UNION(u, v);
for each pair (x, y) such that x ∈X and y ∈Y do
if FIND(x) <> FIND(y) and CONGRUENT(R′, x, y)
then
MERGE(R′, x, y)
endif
endfor
endif
end
In order to compute the congruence closure R′ of a relation R = {(u1,
v1), ..., (un, vn)} on a graph G, we use the following algorithm, which computes
R′ incrementally. It is assumed that R′ is a global variable.
program closure;
var R′: relation;
function CONGRUENT(R′: congruence; u, v: node): boolean;
procedure MERGE(var R′: congruence; u, v: node);
begin
input(G, R);
R′ := Id; {the identity relation on the set V of nodes}
for each (ui, vi) in R do
MERGE(R′, ui, vi)
endfor
end
To prove the correctness of the above algorithm, we need the following
lemma.
Lemma 10.6.5
(Correctness of the congruence closure algorithm) Given a
ﬁnite graph G, a congruence R on G, and any pair (u, v) of nodes in G, let
R1 be the least equivalence relation containing R ∪{(u, v)}, and R3 be the
congruence closure of R1 ∪R2, where
R2 = {(x, y) ∈X × Y | CONGRUENT(R1, x, y) = true},
with
X =the union of the sets Px of predecessors of all nodes x in [u], the
equivalence class of u (modulo R1), and
Y =the union of the sets Py of predecessors of all nodes y in [v], the
equivalence class of v (modulo R1).
Then the relation R3 is the congruence closure of R ∪{(u, v)}.

10.6 Decision Procedures Based on Congruence Closure
473
Proof : First, since R3 is the congruence closure of R1 ∪R2 and R1
contains R ∪{(u, v)}, R3 is a congruence containing R ∪{(u, v)}. To show
that it is the smallest one, observe that for any congruence R′ containing
R ∪{(u, v)}, by the deﬁnition of a congruence, R′ has to contain all pairs
in R2, as well as all pairs in R and (u, v). Hence, any such congruence R′
contains R3.
Using lemma 10.6.5, it is easy to justify that if MERGE(R, u, v) termi-
nates, then it computes the congruence closure of R ∪{(u, v)}. The only fact
that remains to be checked is that the procedure terminates. But note that
MERGE(R′, u, v) calls UNION(u, v) iﬀu and v are not already equivalent,
before calling MERGE(R′, x, y) recursively iﬀx and y are not equivalent.
Hence, every time MERGE is called recursively, the number of inequivalent
nodes decreases, which guarantees termination. Then, the correctness of the
algorithm closure is straigtforward.
The complexity of the procedure MERGE has been analyzed in Nelson
and Oppen, 1980. Nelson and Oppen showed that if G has m edges and G
has no isolated nodes, in which case the number of nodes n is O(m), then
the algorithm can be implemented to run in O(m2)-time. Downey, Sethi, and
Tarjan (Downey, 1980) have also studied the problem of congruence closure,
and have given a faster algorithm running in O(mlog(m))-time.
It should also be noted that there is a dual version of the congruence
closure problem, the uniﬁcation closure problem, and also a symmetric version
of the problem, which have been investigated in Oppen, 1980a, and Downey,
1980. Both have applications to decision problems for certain classes of for-
mulae. For instance, in Oppen, 1980a, the symmetric congruence closure is
used to give a decision procedure for the theory of recursively deﬁned data
structures, and in Nelson and Oppen, 1980, the congruence closure is used to
give a decision problem for the theory of list structures. Other applications
of the concept of congruence closure are also found in Oppen, 1980b; Nelson
and Oppen, 1979; and Shostak, 1984b.
The dual concept of the congruence closure is deﬁned as follows. An
equivalence relation R on a graph G is a uniﬁcation closure iﬀ, for every pair
(u, v) of nodes in V 2, whenever uRsv then:
(1) Either Λ(u) = Λ(v), or one of Λ(u), Λ(v) is a variable;
(2) If Λ(u) = Λ(v) and r(Λ(u)) = (s1...sn, s), then for every i, 1 ≤i ≤n,
u[i]Rsiv[i].
Graphically, if u and v are two nodes labeled with the same symbol f
of rank (s1...sn, s), if u[1], ..., u[n] are the successors of u and v[1], ..., v[n] are
the successors of v,

474
10/Many-Sorted First-Order Logic
u
u[1]
u[n]
· · ·
v
v[1]
v[n]
· · ·
if u and v are equivalent then u[i] and v[i] are equivalent for all i, 1 ≤i ≤n.
Hence, we have a kind of forward closure. In contrast with the congruence
closure, the least uniﬁcation closure containing a given relation R does not
necessarily exist, because condition (1) may fail (see the problems).
The uniﬁcation closure problem has applications to the uniﬁcation of
trees, and to the equivalence of deterministic ﬁnite automata.
There is a
linear-time uniﬁcation closure algorithm due to Paterson and Wegman (Pa-
terson and Wegman, 1978) when a certain acyclicity condition is satisﬁed,
and in the general case, there is an O(mα(m))-time algorithm, where α is a
functional inverse of Ackermann’s function. For details, the reader is referred
to Downey, 1980. An O(mα(m))-time uniﬁcation algorithm is also given in
Huet, 1976.
PROBLEMS
10.6.1. Prove lemma 10.6.1.
10.6.2. Give the details of the proof of lemma 10.6.3.
10.6.3. Use the method of Subsection 10.6.3 to show that the following for-
mula is valid:
x .= y ⊃f(x, y) .= f(y, x)
10.6.4. Use the method of Subsection 10.6.3 to show that the following for-
mula is valid:
f(f(f(a))) .= a ∧f(f(a)) .= a ⊃f(a) .= a
10.6.5. Use the method of Subsection 10.6.5 to show that the following for-
mula is valid:
x .= y ∧g(f(x, y)) .= h(f(x, y)) ∧P(g(f(x, y)) ⊃P(h(f(y, x)))
10.6.6. Use the method of Subsection 10.6.5 to show that the following for-
mula is not valid:
x .= y ∧g(z) .= h(z) ∧P(g(f(x, y))) ⊃P(h(f(y, x)))
∗10.6.7. An equivalence relation R on a graph G is a uniﬁcation closure iﬀ,
for every pair (u, v) of nodes in V 2, whenever uRsv then:

PROBLEMS
475
(1) Either Λ(u) = Λ(v), or one of Λ(u), Λ(v) is a variable;
(2) If Λ(u) = Λ(v) and r(Λ(u)) = (s1...sn, s), then for every i, 1 ≤
i ≤n, u[i]Rsiv[i].
(a) Given an arbitrary relation R0 on a graph G, give an example
showing that the smallest uniﬁcation closure containing R0 does not
necessarily exists, because condition (1) may fail.
(b) Using the idea of Subsection 10.6.4, show that there is an algo-
rithm for deciding whether the smallest uniﬁcation closure containing
a relation R0 on G exists, and if so, for computing it.
∗10.6.8. In order to test whether two trees t1 and t2 are uniﬁable, we can
compute the uniﬁcation closure of the relation {t1, t2} on the graph
G(t1, t2) constructed from t1 and t2 as follows:
(i) The set set of nodes of G(t1, t2) is the set of all subterms of t1 and
t2.
(ii) Every subterm that is either a constant or a variable is a terminal
node labeled with that symbol.
(iii) For every subterm of the form fy1...yk, the label is f, and there
is an edge from fy1...yk to yi, for each i, 1 ≤i ≤k.
Let R be the least uniﬁcation closure containing the relation {t1, t2}
on the graph G(t1, t2), if it exits. A new graph G(t1, t2)/R can be
constructed as follows:
(i) The nodes of G(t1, t2)/R are the equivalence classes of R.
(ii) There is an edge from a class C to a class C′ iﬀthere is an edge
in G(t1, t2) from some node y in class C to some node z in class C′.
Prove that t1 and t2 are uniﬁable iﬀthe uniﬁcation closure R exists
and the graph G(t1, t2)/R is acyclic. When t1 and t2 are uniﬁable,
let ⟨C1, . . . , Cn⟩be the sequence of all equivalence classes containing
some variable, ordered such that if there is a path from Ci to Cj,
then i < j. For each i, 1 ≤i ≤n, if Ci contains some nonvariable
term, let ti be any such term, else let ti be any variable in Ci. Let
σi = [ti/z1, . . . , ti/zk], where {z1, . . . , zk} is the set of variables in Ci,
and let σ = σ1 ◦. . . ◦σn. Show that σ is a most general uniﬁer.
(A cycle in a graph is a sequence v1, ..., vk, of nodes such that v1 = vk,
and there is an edge from vi to vi+1, 1 ≤i ≤k −1. A graph is acyclic
iﬀit does not have any cycle.)
∗10.6.9. Let C be a quantiﬁer-free formula of the form
(s1 .= t1) ∧... ∧(sn .= tn) ⊃(s .= t).

476
10/Many-Sorted First-Order Logic
Let COM(C) be the conjunction of all formulae of the form
∀x∀y(f(x, y) .= f(y, x)),
for every binary function symbol f in C.
Show that the decision procedure provided by the congruence closure
algorithm can be adpated to decide whether formulae of the form
COM(C) ⊃C are valid.
Hint: Make the following modiﬁcation in building the graph G(C):
For every term ft1t2, create a node labeled ft1t2 having two ordered
successors:
A terminal node labeled f, and a node labeled with v(ft1t2), such
that v(ft1t2) has two unordered successors labeled with t1 and t2.
Also modify the deﬁnition of a congruence, so that for any two nodes
u and v labeled with v(ft1t2), where f is a binary function symbol,
if either
(i) u[1] is congruent to v[1] and u[2] is congruent to v[2], or
(ii) u[1] is congruent to v[2] and u[2] is congruent to v[1],
then u and v are congruent.
Notes and Suggestions for Further Reading
Many-sorted ﬁrst-order logic is now used quite extensively in computer sci-
ence. Its main uses are to the deﬁnition of abstract data types and to pro-
gramming with rewrite rules. Brief presentations of many-sorted ﬁrst-order
logic can be found in Enderton, 1972, and Monk, 1976. A pioneering paper
appears to be Wang, 1952.
The literature on abstact data types and rewrite rules is now extensive.
A good introduction to abstract data types can be found in the survey paper
by Goguen,Thatcher,Wagner, and Wright, in Yeh, 1978. For an introduction
to rewrite rules, the reader is referred to the survey paper by Huet and Oppen,
in Book, 1980, and to Huet, 1980.
Congruence closure algorithms were ﬁrst discovered by Kozen (Kozen,
1976, 1977), and independently a few years later by Nelson and Oppen (Nelson
and Oppen, 1980, Oppen, 1980a), and Downey, Sethi, and Tarjan (Downey,
1980). The extension given in Subsection 10.6.4 appears to be new. Problems
10.6.7 and 10.6.8 are inspired from Paterson and Wegman, 1978, where a
linear-time algorithm is given, and problem 10.6.9 is inspired from Downey,
1980, where fast algorithms are given.

Appendix
2.4 Algebras
We have already encountered the concept of an algebra in example 2.3.1 and
in the section containing theorem 2.3.1. An algebra is simply a pair < A, F >
consisting of a nonempty set A together with a collection F of functions also
called operations, each function in F being of the form f : An →A, for some
natural number n > 0. When working with algebras, one is often dealing with
algebras having a common structure, in the sense that for any two algebras
< A, F > and < B, G >, there is a function d : F →G from the set of
functions F to the set of functions G. A more convenient way to indicate
common structure is to deﬁne in advance the set Σ of function symbols as a
ranked alphabet used to name operators used in these algebras. Then, given
an algebra < A, F >, each function name f receives an interpretation I(f)
which is a function in F. In other words, the set F of functions is deﬁned by
an interpretation I : Σ →F assigning a function of rank n to every function
symbol of rank n in Σ.
Given any two algebras < A, I : Σ →F > and
< B, J : Σ →G >, the mapping d from F to G indicating common structure
is the function such that d(I(f)) = J(f), for every function name f in Σ.
This leads to the following formal deﬁnition.
2.4.1 Deﬁnition of an Algebra
Given a ranked alphabet Σ, a Σ-algebra A is a pair < A, I > where A is a
477

478
Appendix
nonempty set called the carrier, and I is an interpretation function assigning
functions to the function symbols as follows:
(i) Each symbol f in Σ of rank n > 0 is interpreted as a function I(f) :
An →A;
(ii) Each constant c in Σ is interpreted as an element I(c) in A.
The following abbreviations will also be used: I(f) will be denoted as
fA and I(c) as cA.
Roughly speaking, the ranked alphabet describes the syntax, and the
interpretation function I describes the semantics.
EXAMPLE 2.4.1
Let A be any (nonempty) set and let 2A denote the power-set of A, that
is, the set of all subsets of A. Let Σ = {0, 1, −, +, ∗}, where 0,1 are
constants, that is of rank 0, −has rank 1, and + and ∗have rank 2. If
we deﬁne the interpretation function I such that I(0) = ∅(the empty
set), I(1) = A, I(−) = set complementation, I(+) = set union, and
I(∗) = set intersection, we have the algebra BA =< 2A, I >. Such an
algebra is a boolean algebra of sets in the universe A.
EXAMPLE 2.4.2
Let Σ be a ranked alphabet. Recall that the set CTΣ denotes the set of all
ﬁnite or inﬁnite Σ-trees. Every function symbol f of rank n > 0 deﬁnes
the function f : CT n
Σ →CTΣ as follows: for all t1, t2, ..., tn ∈CTΣ,
f(t1, t2, ..., tn) is the tree denoted by ft1t2...tn and whose graph is the
set of pairs
{(e, f)} ∪
i=n

i=1
{(iu, ti(u)) | u ∈dom(ti)}.
The tree ft1...tn is the tree with f at the root and ti as the subtree
at address i. Let I be the interpretation function such that for every
f ∈Σn, n > 0, I(f) = f. The pair (CTΣ, I) is a Σ-algebra. Similarly,
(TΣ, J) is a Σ-algebra for the interpretation J such that for every f ∈Σn,
n > 0, J(f) is the restriction of f to ﬁnite trees.
For simplicity of
notation, the algebra (CTΣ, I) is denoted by CTΣ and (TΣ, J) by TΣ.
The notion of a function preserving algebraic structure is deﬁned below
2.4.2 Homomorphisms
Given two Σ-algebras A and B, a function h : A →B is a homomorphism iﬀ:
(i) For every function symbol f of rank n > 0, for every (a1, ..., an) ∈An,
h(fA(a1, ..., an)) = fB(h(a1), ..., h(an));

2.4 Algebras
479
(ii) For every constant c, h(cA) = cB.
If we deﬁne A0 = {e} and the function hn : An →Bn by h(a1, ..., an) =
(h(a1), ..., h(an)), then the fact that the function h is a homomorphism is
expressed by the commutativity of the following diagram:
An
fA
−→
A
hn
h
Bn
−→
fB
B
We say that a homomorphism h : A →B is an isomorphism iﬀthere is
a homomorphism g : B →A such that h ◦g = IA and g ◦h = IB. Note that
if h is an isomorphism, then it is a bijection.
Inductive sets correspond to subalgebras.
This concept is deﬁned as
follows.
2.4.3 Subalgebras
An algebra B =< B, J > is a subalgebra of an algebra A =< A, I > (with
the same ranked alphabet Σ) iﬀ:
(1) B is a subset of A;
(2) For every constant c in Σ, cB = cA, and for every function symbol
f of rank n > 0, fB : Bn →B is the restriction of fA : An →A.
The fact that B is an algebra implies that B is closed under the op-
erations; that is, for every function symbol f of rank n > 0 in Σ, for all
b1, ..., bn ∈B, fB(b1, ..., bn) ∈B. Now, inductive closures correspond to least
subalgebras.
2.4.4 Least Subalgebra Generated by a Subset
Given an algebra A and a subset X of A, the inductive closure of X in A is
the least subalgebra of A containing X. It is also called the least subalgebra
of A generated by X. This algebra can be deﬁned as in lemma 2.3.1. Let
([X]i)i≥0 be the sequence of subsets of A deﬁned by induction as follows:
[X]0 = X ∪{cA | c is a constant in Σ};
[X]i+1 = [X]i ∪{fA(a1, ..., an) | a1, ..., an ∈[X]i, f ∈Σn, n ≥1}.
Let
[X] =

i≥0
[X]i.
One can verify easily that [X] is closed under the operations of A. Hence,
[X] together with the restriction of the operators to [X] is a subalgebra of A.

480
Appendix
Let [X] denote this subalgebra. The following lemma is easily proved (using
a proof similar to that of lemma 2.3.1).
Lemma 2.4.1
Given any algebra A and any subset X of A, [X] is the least
subalgebra of A containing X.
Important note: The carrier of an algebra is always nonempty. To avoid
having the carrier [X] empty, we will make the assumption that [X]0 ̸= ∅
(either X is nonempty or there are constant symbols).
Finally, the notion of free generation is generalized as follows.
2.4.5 Subalgebras Freely Generated by a Set X
We say that the algebra [X] is freely generated by X in A iﬀthe following
conditions hold:
(1) For every f (not a constant) in Σ, the restriction of the function
fA : Am →A to [X]m is injective.
(2) For every fA : Am →A, gA : An →A with f, g ∈Σ, fA([X]m) is
disjoint from gA([X]n) whenever f ̸= g (and cA ̸= dA for constants c ̸= d).
(3) For every fA : An →A with f ∈Σ and every (x1, ..., xn) ∈[X]n,
fA(x1, ..., xn) /∈X (and cA /∈X, for a constant c).
As in lemma 2.3.3, it can be shown that for every (x1, ..., xn) ∈[X]n
i −
[X]n
i−1, fA(x1, ..., xn) /∈[X]i, (i ≥0, with [X]−1 = ∅).
We have the following version of theorem 2.3.1.
Theorem 2.4.1 (Unique homomorphic extension theorem) Let A and B be
two Σ-algebras, X a subset of A, let [X] be the least subalgebra of A con-
taining X, and assume that [X] is freely generated by X. For every function
h : X →B, there is a unique homomorphism h : [X] →B such that:
(1) For all x ∈X, h(x) = h(x);
For every function symbol f of rank n > 0, for all (x1, ..., xn) ∈[X]n,
(2) h(fA(x1, ..., xn)) = fB(h(x1), ...,h(xn)), and h(cA) = cB, for each
constant c.
This is also expressed by the following diagram.
X
−→
[X]
h ↘
h
B
The following lemma shows the importance of the algebra of ﬁnite trees.

2.4 Algebras
481
Lemma 2.4.2
For every ranked alphabet Σ, for every set X, if X ∩Σ = ∅
and X ∪Σ0 ̸= ∅(where Σ0 denotes the set of constants in Σ), then the Σ-
algebra TΣ(X) of ﬁnite trees over the ranked alphabet Σ ∪X obtained by
adjoining the set X to Σ0 is freely generated by X.
Proof : First, it is easy to show from the deﬁnitions that for every tree
t such that depth(t) > 0,
t = f(t/1, ..., t/n)
and
depth(t/i) < depth(t), 1 ≤i ≤n,
where t(e) = f (the label of the root of t), n = r(f) (the rank of f) and
t/i is the “i-th subtree” of t, 1 ≤i ≤n.
Using this property, we prove
by induction on the depth of trees that every tree in TΣ(X) belongs to the
inductive closure of X. Since a tree of depth 0 is a one-node tree labeled with
either a constant or an element of X, the base case of the induction holds.
Assume by induction that every tree of depth at most k belongs to Xk, the
k-th stage of the inductive closure of X. Since every tree of depth k + 1 can
be written as t = f(t/1, ..., t/n) where every subtree t/i has depth at most k,
the induction hypothesis applies. Hence, each t/i belongs to Xk. But then
f(t/1, ..., t/n) = t belongs to Xk+1. This concludes the induction showing
that TΣ(X) is a subset of X+. For every f ∈Σn, n > 0, by the deﬁnition of
f, if t1,...,tn are ﬁnite trees, f(t1, ..., tn) is a ﬁnite tree (because its domain is
a ﬁnite union of ﬁnite domains). Hence, every Xk is a set of ﬁnite trees, and
thus a subset of TΣ(X). But then X+ is a subset of TΣ(X) and TΣ(X) = X+.
Note also that for any two trees t and t′ (even inﬁnite), t = t′ if and only if,
either
(1) t = f(t1, ..., tm) = t′, for some unique f ∈Σm, (m > 0), and some
unique trees t1,...,tm ∈CTΣ(X), or
(2) t = a = t′, for some unique a ∈X ∪Σ0.
Then, it is clear that each function f is injective, that range(f) and
range(g) are disjoint whenever f ̸= g, and that for every f of rank n > 0,
f(t1, ..., tn) is never a one-node tree labeled with a constant. Hence, conditions
(1),(2),(3) for free generation are satisﬁed.
In Chapter 5, when proving the completeness theorem for ﬁrst-order
logic, it is necessary to deﬁne the quotient of an algebra by a type of equiv-
alence relation.
Actually, in order to deﬁne an algebraic structure on the
set of equivalence classes, we need a stronger concept known as a congruence
relation.
2.4.6 Congruences
Given a Σ-algebra A, a congruence ∼= on A is an equivalence relation on the
carrier A satisfying the following conditions: For every function symbol f of
rank n > 0 in Σ, for all x1, ..., xn, y1, ..., yn ∈A,

482
Appendix
if xi ∼= yi for all i, 1 ≤i ≤n, then
fA(x1, ..., xn) ∼= fA(y1, ..., yn).
The equivalence class of x modulo ∼= is denoted by [x]∼
=, or more simply
by [x] or x.
Given any function symbol f of rank n > 0 in Σ and any n subsets
B1, ..., Bn of A, we deﬁne
fA(B1, ..., Bn) = {fA(b1, ..., bn) | bi ∈Bi, 1 ≤i ≤n}.
If ∼= is a congruence on A, for any a1, ..., an ∈A, fA([a1], ..., [an]) is a
subset of some unique equivalence class which is in fact [fA(a1, ..., an)]. Hence,
we can deﬁne a structure of Σ-algebra on the set A/ ∼= of equivalence classes.
2.4.7 Quotient Algebras
Given a Σ-algebra A and a congruence ∼= on A, the quotient algebra A/ ∼= has
the set A/ ∼= of equivalence classes modulo ∼= as its carrier, and its operations
are deﬁned as follows: For every function symbol f of rank n > 0 in Σ, for all
[a1], ..., [an] ∈A/ ∼=,
fA/∼
=([a1], ..., [an]) = [fA(a1, ..., an)],
and for every constant c,
cA/∼
= = [c].
One can easily verify that the function h∼
= : A →A/ ∼= such that
h∼
=(x) = [x] is a homomorphism from A to A/ ∼=.
EXAMPLE 2.4.3
Let Σ = {0, 1, −, +, ∗} be the ranked alphabet of example 2.4.1. Let A be
any nonempty set and let TΣ(2A) be the tree algebra freely generated
by 2A. By lemma 2.4.2 and theorem 2.4.1, the identity function Id :
2A →2A extends to a unique homomorphism h : TΣ(2A) →BA, where
the range of the function h is the boolean algebra BA (and not just
its carrier 2A as in the function Id). Let ∼= be the relation deﬁned on
TΣ(2A) such that:
t1 ∼= t2
if and only if
h(t1) = h(t2).
For example, if A = {a, b, c, d}, for the trees t1 and t2 given by the terms
t1 = +({a}, ∗({b, c, d}, {a, b, c})) and t2 = +({a, b}, {c}), we have
h(t1) = h(t2) = {a, b, c}.

2.5 Many-Sorted Algebras
483
It can be veriﬁed that ∼= is a congruence, and that the quotient algebra
TΣ(2A)/ ∼= is isomorphic to BA.
2.5 Many-Sorted Algebras
For many computer science applications, and for the deﬁnition of data types
in particular, it is convenient to generalize algebras by allowing domains and
operations of diﬀerent types (also called sorts). A convenient way to do so
is to introduce the concept of a many-sorted algebra. In Chapter 10, a gen-
eralization of ﬁrst-order logic known as many-sorted ﬁrst-order logic is also
presented. Since the semantics of this logic is based on many-sorted algebras,
we present in this section some basic material on many-sorted algebra.
Let S be a set of sorts (or types). Typically, S consists of types in a
programming language (such as integer, real, boolean, character, etc.).
2.5.1 S-Ranked Alphabets
An S-ranked alphabet is pair (Σ, r) consisting of a set Σ together with a func-
tion r : Σ →S∗× S assigning a rank (u, s) to each symbol f in Σ.
The string u in S∗is the arity of f and s is the sort (or type) of f.
If u = s1...sn, (n ≥1), a symbol f of rank (u, s) is to be interpreted as an
operation taking arguments, the i-th argument being of type si and yielding
a result of type s. A symbol of rank (e, s) (when u is the empty string ) is
called a constant of sort s. For simplicity, a ranked alphabet (Σ, r) is often
denoted by Σ.
2.5.2 Deﬁnition of a Many-Sorted Algebra
Given an S-ranked alphabet Σ, a many-sorted Σ-algebra A is a pair < A, I >,
where A = (As)s∈S is an S-indexed family of nonempty sets, each As being
called a carrier of sort s, and I is an interpretation function assigning func-
tions to the function symbols as follows:
(i) Each symbol f of rank (u, s) where u = s1...sn is interpreted as a
function I(f) : As1 × ... × Asn →As;
(ii) Each constant c of sort s is interpreted as an element I(c) in As.
The following abbreviations will also be used: I(f) will be denoted as
fA and I(c) as cA; If u = s1...sn, we let Au = As1 × ... × Asn and Ae = {e}.
(Since there is a bijection between A and A × {e}, A and A × {e} will be
identiﬁed.) Given an S-indexed family h = (hs)s∈S of functions hs : As →Bs,
the function hu : Au →Bu is deﬁned so that, for all (a1, ..., an) ∈Au,
hu(a1, ..., an) = (hs1(a1), ..., hsn(an)).

484
Appendix
EXAMPLE 2.5.1
The algebra of stacks of natural numbers is deﬁned as follows.
Let
S = {int, stack}, Σ = {Λ, ERROR, POP, PUSH, TOP}, where Λ is
a constant of sort stack, ERROR is a constant of sort int, PUSH is
a function symbol of rank (int.stack,stack), POP a function symbol
of rank (stack,stack) and TOP a function symbol of rank (stack,int).
The carrier of sort int is N ∪{error}, and the carrier of sort stack, the
set of functions of the form X : [n] →N ∪{error}, where n ∈N. When
n = 0, the unique function from the empty set ([0]) is denoted by Λ and
is called the empty stack. The constant ERROR is interpreted as the
element error. Given any stack X : [n] →N ∪{error} and any element
a ∈N ∪{error}, PUSH(a, X) is the stack X′ : [n + 1] →N ∪{error}
such that X′(k) = X(k) for all k, 1 ≤k ≤n, and X′(n + 1) = a;
TOP(X) is the top element X(n) of X if n > 0, error if n = 0; POP(X)
is the empty stack Λ if either X is the empty stack or n = 1, or the stack
X′ : [n−1] →N∪{error} such that X′(k) = X(k) for all k, 1 ≤k ≤n−1
if n > 1.
Note: This formalization of a stack is not perfectly faithful because
TOP(X) = error does not necessarily imply that X = Λ. However, it is good
enough as an example.
2.5.3 Homomorphisms
Given two many-sorted Σ-algebras A and B, an S-indexed family h = (hs)s∈S
of functions hs : As →Bs is a homomorphism iﬀ:
(i) For every function symbol f or rank (u, s) with u = s1...sn, for every
(a1, ..., an) ∈Au,
hs(fA(a1, ..., an)) = fB(hs1(a1), ..., hsn(an));
(ii) For every sort s, for every constant c of sort s, hs(cA) = cB.
These conditions can be represented by the following commutative dia-
gram.
Au
fA
−→
As
hu
h
Bu
−→
fB
Bs
2.5.4 Subalgebras
An algebra B =< B, J > is a subalgebra of an algebra A =< A, I > (with
the same ranked alphabet Σ) iﬀ:
(1) Each Bs is a subset of As, for every sort s ∈S;

2.5 Many-Sorted Algebras
485
(2) For every sort s ∈S, for every constant c of sort s, cB = cA, and for
every function symbol f of rank (u, s) with u = s1...sn, fB : Bu →Bs is the
restriction of fA : Au →As.
The fact that B is an algebra implies that B is closed under the opera-
tions; that is, for every function symbol f of rank (u, s) with u = s1...sn, for
every (b1, ..., bn) ∈Bu, fA(b1, ..., bn) is in Bs, and for every constant c of sort
s, cA is in Bs.
2.5.5 Least Subalgebras
Given a Σ-algebra A, let X = (Xs)s∈S be an S-indexed family with every Xs
a subset of As. As in the one-sorted case, the least subalgebra of A containing
X can be characterized by a bottom-up deﬁnition. We deﬁne this algebra [X]
as follows.
The sequence of S-indexed families of sets ([X]i), (i ≥0) is deﬁned by
induction: For every sort s,
[Xs]0 = Xs ∪{cA | c a constant of sort s},
[Xs]i+1 = [Xs]i ∪{fA(x1, ..., xn) | r(f) = (u, s), (x1, ..., xn) ∈([X]i)u},
with u = s1...sn, n ≥1.
The carrier of sort s of the algebra [X] is

i≥0
[Xs]i.
Important note: The carriers of an algebra are always nonempty. To
avoid having any carrier [Xs] empty, we will make the assumption that for
every sort s, [Xs]0 ̸= ∅. This will be satisﬁed if either Xs is nonempty or there
are constants of sort s. A more general condition can be given. Call a sort s
nonvoid if either there is some constant of sort s, or there is some function
symbol f of rank (s1...sn, s) such that s1, ..., sn are all non-void (n ≥1).
Then, [X]s is nonempty if and only if either Xs ̸= ∅or s is non-void.
We also have the following induction principle.
Induction Principle for Least Subalgebras
If [X] is the least subalgebra of A containing X, for every subfamily Y
of [X], if Y contains X and is closed under the operations of A (and contains
{cA | c is a constant}) then Y = [X].
2.5.6 Freely Generated Subalgebras
We say that [X] is freely generated by X in A iﬀthe following conditions hold:

486
Appendix
(1) For every f (not a constant) in Σ, the restriction of the function
fA : Au →As to [X]u is injective.
(2) For every fA : Au →As, gA : Av →As′ with f, g ∈Σ, fA([X]u) is
disjoint from gA([X]v) whenever f ̸= g (and cA ̸= dA for constants c ̸= d).
(3) For every fA : Au →As with f ∈Σ and every (x1, ..., xn) ∈[X]u,
fA(x1, ..., xn) /∈Xs (and cA /∈Xs, for a constant c of sort s).
As in lemma 2.3.3 , it can be shown that for every (x1, ..., xn) in [X]u
i −
[X]u
i−1, fA(x1, ..., xn) /∈[Xs]i, (i ≥0, with [Xs]−1 = ∅).
We have the following generalization of theorem 2.4.1.
Theorem 2.5.1
(Unique homomorphic extension theorem) Let A and B
be two many-sorted Σ-algebras, X an S-indexed family of subsets of A, let
[X] be the least subalgebra of A containing X, and assume that [X] is freely
generated by X. For every S-indexed family h : X →B of functions hs :
Xs →Bs, there is a unique homomorphism h : [X] →B such that:
(1) For all x ∈Xs, hs(x) = hs(x) for every sort s;
For every function symbol f of rank (u, s), with u = s1...sn, for all
(x1, ..., xn) ∈[X]u,
(2) hs(fA(x1, ..., xn)) = fB(hs1(x1), ...,hsn(xn)), and hs(cA) = cB for a
constant c of sort s.
X
−→
[X]
h ↘
h
B
2.5.7 Congruences
Given a Σ-algebra A, a congruence ∼= on A is an S-indexed family (∼=s)s∈S
of relations, each ∼=s being an equivalence relation on the carrier As, and
satisfying the following conditions: For every function symbol f of rank (u, s),
with u = s1...sn, for all (x1, ..., xn) and (y1, ..., yn) in Au,
if xi ∼=si yi for all i, 1 ≤i ≤n, then
fA(x1, ..., xn) ∼=s fA(y1, ..., yn).
The equivalence class of x modulo ∼=s is denoted by [x]∼
=s, or more simply
by [x]s or xs.
Given any function symbol f of rank (u, s), with u = s1...sn, and any n
subsets B1, ..., Bn such that Bi is a subset of Asi, we deﬁne
fA(B1, ..., Bn) = {fA(b1, ..., bn) | (b1, ..., bn) ∈Au}.

2.5 Many-Sorted Algebras
487
If ∼= is a congruence on A, for any (a1, ..., an) ∈Au, fA([a1]s1, ..., [an]sn)
is a subset of some unique equivalence class which is in fact [fA(a1, ..., an)]s.
Hence, we can deﬁne a structure of Σ-algebra on the S-indexed family A/ ∼=
of sets of equivalence classes.
2.5.8 Quotient Algebras
Given a Σ-algebra A and a congruence ∼= on A, the quotient algebra A/ ∼=
has the S-indexed family A/ ∼= of sets of equivalence classes modulo ∼=s as its
carriers, and its operations are deﬁned as follows: For every function symbol
f of rank (u, s), with u = s1...sn, for all ([a1]s1, ..., [an]sn) ∈(A/ ∼=)u,
fA/∼
=([a1]s1, ..., [an]sn) = [fA(a1, ..., an)]s,
and for every constant c of sort s,
cA/∼
=s = [c]s.
The S-indexed family h∼
= of functions h∼
=s : As →A/ ∼=s such that
h∼
=s(x) = [x]∼
=s is a homomorphism from A to A/ ∼=.
Finally, many-sorted trees are deﬁned as follows.
2.5.9 Many-Sorted Trees
Given a many-sorted alphabet Σ (with set S of sorts), a Σ-tree of sort s is
any function t : D →Σ where D is a tree domain denoted by dom(t) and t
satisﬁes the following conditions:
1) The root of t is labeled with a symbol t(e) in Σ of sort s.
2) For every node u ∈dom(t), if {i | ui ∈dom(t)} = [n], then if n > 0,
for each ui, i ∈[n], if t(ui) is a symbol of sort vi, then t(u) has rank (v, s′),
with v = v1...vn, else if n = 0, then t(u) has rank (e, s′), for some s′ ∈S.
The set of all ﬁnite trees of sort s is denoted by T s
Σ, and the set of all
ﬁnite trees by TΣ. Given an S-indexed family X = (Xs)s∈S, we can form the
sets of trees T s
Σ(Xs) obtained by adjoining each set Xs to the set of constants
of sort s. TΣ(X) is a Σ-algebra, and lemma 2.4.2 generalizes as follows.
Lemma 2.5.1
For every many-sorted Σ-algebra A and S-indexed family
X = (Xs)s∈S, if X ∩Σ = ∅, then the Σ-algebra TΣ(X) of ﬁnite trees over the
ranked alphabet obtained by adjoining each set Xs to the set Σe,s of constants
of sort s is freely generated by X.
EXAMPLE 2.5.2
Referring to example 2.5.1, let Σ be the ranked alphabet of the algebra
A of stacks.
Let TΣ(N) be the algebra freely generated by the pair

488
Appendix
of sets (∅, N).
The identity function on (∅, N) extends to a unique
homormophism h from TΣ(N) to A. Deﬁne the relations ∼=int and ∼=stack
on TΣ(N) as follows: For all t1, t2 of sort stack,
t1 ∼=int t2 iﬀh(t1) = h(t2),
and for all t1, t2 of sort int,
t1 ∼=stack t2 iﬀh(t1) = h(t2).
One can check that ∼= is a congruence, and that TΣ(N)/ ∼= is isomorphic
to A. One can also check that the following holds for all trees X of sort
stack and all trees a of sort int:
POP(PUSH(a, X)) ∼=stack X,
POP(Λ) ∼=stack Λ,
TOP(PUSH(a, X)) ∼=int a,
TOP(Λ) ∼=int ERROR.
The reader is referred to Cohn, 1981, or Gratzer, 1979, for a complete
exposition of universal algebra. For more details on many-sorted algebras,
the reader is referred to the article by Goguen,Thatcher,Wagner and Wright
in Yeh, 1978, or the survey article by Huet and Oppen, in Book, 1980.
PROBLEMS
2.4.1.
Let A and B two Σ-algebras and X a subset of A. Assume that A
is the least subalgebra generated by X. Show that if h1 and h2 are
any two homomorphisms from A to B such that h1 and h2 agree on
X (that is, h1(x) = h2(x) for all x ∈X), then h1 = h2.
2.4.2.
Let h : A →B be a homomorphism of Σ-algebras.
(a) Given any subalgebra X of A, prove that h(X) is a subalgebra of
B (denoted by h(X)).
(b) Given any subalgebra Y of B, prove that h−1(Y ) is a subalgebra
of A (denoted by h−1(Y)).
2.4.3.
Let h : A →B be a homomorphism of Σ-algebras. Let ∼= be the
relation deﬁned on A such that, for all x, y ∈A,
x ∼= y
if and only if
h(x) = h(y).
Prove that ∼= is a congruence on A, and that h(A) is isomorphic to
A/ ∼=.

PROBLEMS
489
2.4.4.
Prove that for every Σ-algebra A, there is some tree algebra TΣ(X)
freely generated by some set X and some congruence ∼= on TΣ(X)
such that TΣ(X)/ ∼= is isomorphic to A.
2.4.5.
Let A be a Σ-algebra, X a subset of A, and assume that [X] = A,
that is, X generates A.
Prove that if for every Σ-algebra B and function h : X →B there
is a unique homomorphism h : A →B extending h, then A is freely
generated by X.
∗2.4.6.
Given a Σ-algebra A and any relation R on A, prove that there is a
least congruence ∼= containing R.
2.5.1.
Do problem 2.4.1 for many-sorted algebras.
2.5.2.
Do problem 2.4.2 for many-sorted algebras.
2.5.3.
Do problem 2.4.3 for many-sorted algebras.
2.5.4.
Do problem 2.4.4 for many-sorted algebras.
2.5.5.
Do problem 2.4.5 for many-sorted algebras.
∗2.5.6.
Do problem 2.4.6 for many-sorted algebras.
∗2.5.7.
Referring to example 2.5.2, prove that the quotient algebra TΣ(N)/ ∼=
is isomorphic to the stack algebra A.
∗2.5.8.
Prove that the least congruence containing the relation R deﬁned
below is the congruence ∼= of problem 2.5.7. The relation R is deﬁned
such that, for all trees X of sort stack and all trees a of sort int:
POP(PUSH(a, X)) Rstack X,
POP(Λ) Rstack Λ,
TOP(PUSH(a, X)) Rint a,
TOP(Λ) Rint ERROR.
This problem shows that the stack algebra is isomorphic to the quo-
tient of the tree algebra TΣ(N) by the least congruence ∼= containing
the above relation.

REFERENCES
Aho, Alfred V., and Ullman, Jeﬀrey D. 1979. Principles of Compiler Design. Read-
ing, MA: Addison Wesley.
Anderson, R. 1970.
Completeness Results for E-resolution.
Proceedings AFIPS
1970, Spring Joint Computer Conference, Vol. 36. Montvale, NJ: AFIPS Press,
653-656.
Andrews, Peter B. 1970. Resolution in Type Theory. Journal of Symbolic Logic
36(3), 414-432.
Andrews, Peter B. 1981. Theorem Proving via General Matings. J.ACM 28(2),
193-214.
Andrews, Peter B. 1986. An Introduction to Mathematical Logic and Type Theory:
To Truth through Proof . New York: Academic Press.
Apt, Krzysztof R., and VanEmden, M. H. 1982. Contributions to the Theory of
Logic Programming. J.ACM 29(3), 841-862.
Barwise, Jon. 1977. Handbook of Mathematical Logic. Studies in Logic, Vol. 90.
New York: Elsevier North-Holland.
Bell, J. L., and Slomson, A. B. 1974.
Models and Ultraproducts.
Amsterdam:
Elsevier North-Holland.
Birkhoﬀ, Garrett. 1973. Lattice Theory. American Mathematical Society Collo-
quium Publications, Vol. 25.
Book, Ronald. 1980. Formal Language Theory. New York: Academic Press.
Boyer, R. S., and Moore, J. S. 1979. A Computational Logic. New York: Academic
Press.
490

491
Bundy, Alan. 1983. The Computer Modelling of Mathematical Reasoning. New
York: Academic Press.
Campbell, J. A. 1984. Implementations of PROLOG. Chichester, England: Ellis &
Horwood.
Chang, C. C., and Keisler, H. J. 1973. Model Theory. Studies in Logic, Vol. 73.
Amsterdam: Elsevier North-Holland.
Chang, C., and Lee, R. C. 1973. Symbolic Logic and Mechanical Theorem Proving.
New York: Academic Press.
Clocksin, William F., and Mellish, Christopher S. 1981. Programming in PROLOG.
New York: Springer Verlag.
Cohn, Paul M. 1981. Universal Algebra. Hingham, MA: Reidel Publishing Com-
pany.
Cook, Stephen A. 1971.
The Complexity of Theorem-Proving Procedures.
Pro-
ceedings of the Third Annual ACM Symposium on the Theory of Computing, pp.
151-158, Association for Computing Machinery, New York.
Cook, Stephen A., and Reckhow, Robert A. 1979. The Relative Eﬃciency of Propo-
sitional Proof Systems. Journal of Symbolic Logic 44(1), 33-50.
Davis, Martin D. 1963. Eliminating the Irrelevant from Mechanical Proofs. Proceed-
ings of a Symposium on Applied Mathematics, Vol. XV, Providence, RI, 15-30.
Davis, Martin D., and Putnam, H. 1960. A Computing Procedure for Quantiﬁcation
Theory. J.ACM 7, 210-215.
Davis, Martin D. and Weyuker, Elaine J. 1983.
Computability, Complexity and
Languages. New York: Academic Press.
Dowling, William F., and Gallier, Jean H. 1984. Linear-time Algorithms for Testing
the Satisﬁability of Propositional Horn Formulae. Journal of Logic Programming
3, 267-284.
Downey, Peter J., Sethi, Ravi, and Tarjan, Endre R. 1980. Variations on the Com-
mon Subexpressions Problem. J.ACM 27(4), 758-771.
Dreben, B., and Goldfarb, W.D. 1979.
The Decision Problem.
Reading, MA:
Addison-Wesley.
Enderton, Herbert B. 1972.
A Mathematical Introduction to Logic.
New York:
Academic Press.
Enderton, Herbert B. 1977. Elements of Set Theory. New York: Academic Press.
Garey, M. R., and Johnson, D. S. 1979. Computers and Intractability: A Guide to
the Theory of NP-Completeness. New York: Freeman.
Gorn, Saul. 1965. Explicit Deﬁnitions and Linguistic Dominoes. In John Hart and
Satoru Takasu, eds., Systems and Computer Science. Toronto, Canada: Univer-
sity of Toronto Press.
Gorn, Saul. 1984. Data Representation and Lexical Calculi. Information Processing
and Management 20(1-2), 151-174.

492
REFERENCES
Gratzer, G. 1979. Universal Algebra. New York: Springer Verlag.
Halmos, Paul R. 1974. Lectures on Boolean Algebras. New York: Springer Verlag.
Henkin, L., Monk, J. D., and Tarski, A. 1971. Cylindric Algebras. Studies in Logic,
Vol. 64, New York: Elsevier North-Holland.
Herbrand, Jacques.
1971.
Logical Writings.
Hingham, MA: Reidel Publishing
Company.
Huet, G´erard. 1973. A Mechanization of Type Theory. Proceedings of the Third
International Joint Conference on Artiﬁcial Intelligence, Stanford, CA, 139-146.
Huet, G´erard. 1976. R´esolution d’Equations dans les Languages d’Ordre 1,2,...ω.
Doctoral Thesis, Universit´e de Paris VII.
Huet, G´erard. 1980. Conﬂuent Reductions: Abstract Properties and Applications
to Term Rewriting Systems. J.ACM 27(4), 797-821.
Joyner, William H. 1974. Automatic Theorem Proving and The Decision Problem.
Ph.D thesis, Harvard University, Cambridge, MA.
Kleene, Stephen C. 1952. Introduction to Metamathematics. Amsterdam: Elsevier
North-Holland.
Kleene, Stephen C. 1967. Mathematical Logic. New York: Wiley Interscience.
Knuth, Donald. 1968. The Art of Computer Programming, Vol. 1: Fundamental
Algorithms. Reading, MA: Addison Wesley.
Kowalski, Robert A. 1979. Logic for Problem Solving. New York: Elsevier North-
Holland.
Kowalski, Robert A., and Kuehner, D. 1970. Linear Resolution with Selection Func-
tion. Artiﬁcial Intelligence 2, 227-260.
Kowalski, Robert A., and Van Emdem, M. H. 1976. The Semantics of Predicate
Logic as a Programming Language. J.ACM 23(4), 733-742.
Kozen, Dexter. 1976. Complexity of Finitely Presented Algebras. Technical Report
TR 76-294, Computer Science Department, Cornell University, Ithaca, New York.
Kozen, Dexter. 1977. Complexity of Finitely Presented Algebras. Ph.D. Thesis,
Computer Science Department, Cornell University, Ithaca, New York.
Kuratowski, K., and Mostowski, A. 1976. Set Theory. Studies in Logic, Vol. 86.
New York: Elsevier North-Holland.
Levy, A. 1979. Basic Set Theory. Berlin, Heidelberg, New York: Springer-Verlag,
Lewis, H. R. 1979. Unsolvable Classes of Quantiﬁcational Formulas. Reading, MA:
Addison-Wesley.
Lewis, H. R., and Papadimitriou, C. H. 1981. Elements of the Theory of Computa-
tion. Englewood Cliﬀs, NY: Prentice-Hall.
Lloyd, J. W. 1984. Foundations of Logic Programming. New York: Springer-Verlag.
Loveland, Donald W. 1979. Automated Theorem Proving: A Logical Basis. New
York: Elsevier North-Holland.

493
Machtey, Michael, and Young, Paul. 1978. An Introduction to the General Theory
of Algorithms. New York: Elsevier, North-Holland.
Manna, Zohar. 1974. Mathematical Theory of Computation. New York: McGraw-
Hill.
Manna, Zohar, and Waldinger, Richard.
1985.
The Logical Basis for Computer
Programming. Reading, MA: Addison Wesley.
Martelli, A. and Montanari, U. 1982. An Eﬃcient Uniﬁcation Algorithm. TOPLAS
4(2), 258-283.
Miller, Dale. 1984. Herbrand’s Theorem in Higher-Order Logic. Technical Report,
Department of Computer and Information Science, University of Pennsylvania,
Philadelphia, PA 19104.
Monk, J. D. 1976. Mathematical Logic. G.T.M Vol. 37. New York: Springer-Verlag.
Morris, J. B. 1969. E-resolution: Extension of Resolution to Include the Equality
Relation. Proceedings International Joint Conference on Artiﬁcial Intelligence,
Washington D.C.
Nelson, G., and Oppen, D. C. 1979. Simpliﬁcation by Cooperating Decision Proce-
dures. TOPLAS 1(2), 245-257.
Nelson G., and Oppen, D. C. 1980. Fast Decision Procedures Based on Congruence
Closure. J. ACM 27(2), 356-364.
Oppen, D. C. 1980a. Reasoning About Recursively Deﬁned Data Structures. J.
ACM 27(3), 403-411.
Oppen, D. C. 1980b. Complexity, Convexity and Combinations of Theories. Theo-
retical Computer Science 12(3), 291-302.
Paterson, M. S., and Wegman, M. N. 1978. Linear Uniﬁcation. Journal of Computer
and System Sciences 16(2), 158-167.
Pietrzykowski, Tomasz. 1973. A Complete Mechanization of Second-Order Type
Theory. J.ACM 20, 333-364.
Prawitz, Dag. 1960. An Improved Proof Procedure. Theoria 26, 102-139.
Prawitz, Dag. 1965. Natural Deduction. Stockholm: Almquist & Wiskell.
Robinson, J. A. 1965. A Machine Oriented Logic Based on the Resolution Principle.
J.ACM 12(1), 23-41.
Robinson, J. A. 1979.
Logic: Form and Function.
New York: Elsevier North-
Holland.
Robinson, G. A., and Wos, L. 1969. Paramodulation and Theorem-Proving in First-
order Logic with Equality. Machine Intelligence, Vol. 4, 135-150.
Rogers, H. R. 1967. Theory of Recursive Functions and Eﬀective Computability.
New York: McGraw-Hill.
Schoenﬁeld, J. R. 1967. Mathematical Logic. Reading, MA: Addison Wesley.
Shostak, Robert. E. 1984a. 7th International Conference on Automated Deduction.
Lecture Notes in Computer Science, Vol. 170. New York: Springer Verlag.

494
REFERENCES
Shostak, Robert. E. 1984b. Deciding Combinations of Theories. J.ACM 31(1),
1-12.
Siekmann J., and Wrightson, G. 1983.
Automation of Reasoning.
New York:
Springer Verlag.
Smorynski, C. 1983. “Big” News From Archimedes to Friedman. Notices of the
American Mathematical Society 30(3).
Smullyan, R. M. 1968. First-order Logic. Berlin, Heidelberg, New York: Springer-
Verlag.
Statman, R. 1979. Lower Bounds on Herbrand’s Theorem. Proceedings of the Amer-
ican Mathematical Society 75(1), 104-107.
Suppes, P. 1972. Axiomatic Set Theory. New York: Dover.
Szabo, M. E. 1970. The Collected Papers of Gerhard Gentzen. Studies in Logic.
New York: Elsevier North-Holland.
Tait, W. W. 1968.
Normal Derivability in Classical Logic.
In The Syntax and
Semantics of Inﬁnitary Languages, J. Barwise, ed., Lecture Notes in Mathematics.
New York: Springer Verlag.
Takeuti, G. 1975. Proof Theory. Studies in Logic, Vol. 81. Amsterdam: Elsevier
North-Holland.
Tarski, A., Mostowski, A., and Robinson, R. M. 1971. Undecidable Theories. Stud-
ies in Logic. Amsterdam: Elsevier North-Holland.
Van Dalen, D. 1980. Logic and Structure. Berlin, Heidelberg, New York: Springer-
Verlag.
Van Heijenoort, Jean.
1967.
From Frege to G¨odel.
Cambridge, MA: Harvard
University Press.
Wang, Hao. 1952. Logic of Many-Sorted Theories. Journal of Symbolic Logic 17(2),
105-116.
Yeh, Raymond.
1978.
Current Trends in Programming Methodology, Vol.
IV.
Englewood Cliﬀs, NJ: Prentice Hall.

INDEX OF SYMBOLS
(Γ →∆)[z/y], 274
(A[m/xi])M[s], 164
(As
i)M, 454
(Ai)M, 160
(Ai)i∈I, 7
(Es
i )M, 454
(Ei)M, 160
(s1/x1, ..., sm/xm), 382
(T, A), 276
(M, X), 253
<>, 82
=D, 183
[[V →M] →BOOL], 159, 454
[n], 7
[x]R, 7
[V →M], 159, 453
[Vs →Ms], 453
[X], 480
←→E, 286
∗
←→E , 286, 463
∼=, 481, 486
≡, 32, 147, 449
≡M, 160, 454
∃, 147
∃: left, 188, 259, 271, 327, 457
∃: right, 188, 259, 271, 327, 457
∃s, 449
∃x : sA, 452
CS, 148, 449
F, 39
FS, 148, 449
L, 148, 449
M, 158
PS, 32, 148, 449
T, 39
V, 148, 449
∀, 147
∀: left, 188, 259, 271, 327, 457
∀: right, 188, 259, 271, 327, 457
∀s, 449
∀x : sA, 452
Γ |= A, 42
Γ |= B, 163
Γ →∆, 62
∧, 32, 147, 449
∧: left, 63, 111, 187, 258, 270, 457
∧: right, 63, 111, 187, 258, 271, 457
∧M, 160, 454
∨, 32, 147, 449
∨: left, 63, 111, 187, 258, 271, 457
∨: right, 63, 111, 187, 258, 271, 457
∨M, 160, 454
495

496
INDEX OF SYMBOLS
h, 22
v(A), 40
.=, 147
.=s, 449
|=, 42
|= Γ, 162
|= Γ →∆, 85
|= A, 42, 162
|= A1, ..., Am →B1, ..., Bn, 65, 188
¬, 32, 147, 449
¬ : left, 63, 111, 187, 259, 457
¬ : right, 63, 111, 187, 259, 457
¬M, 160, 454
⊕, 45, 58
xR, 7
⊥, 32, 147, 288, 293, 449

D(Ai)i∈I, 184

i∈I Ai, 183
≃, 48, 106, 176
≃T , 107
, 126
⊃, 32, 147, 449
⊃: left, 63, 111, 187, 258, 457
⊃: right, 63, 111, 187, 258, 457
⊃M, 160, 454
⊤, 49, 177
⊢, 66
⊢Γ →∆, 66, 85
A/ ∼=, 482
A/R, 7
A[C/B], 326
A[s1/x1, ..., sn/xn], 342
A[t/x], 155
A ≃B, 48
As
i, 450
A1, ..., Am, ... →B1, ..., Bn, ..., 82
AM, 160, 455
AM[s], 161
AM[v], 455
Ai, 149
atomic(A), 83
AV AIL0, 198
AV AILi, 207
BOOL, 39, 449
BV (A), 154
C(t), 193
Cs.=, 450
C≡, 32, 149, 450
cA, 478, 483
cM, 159, 453
C∧, 32, 149, 450
C∨, 32, 149, 450
C .=, 149
C¬, 32, 149, 450
C⊃, 32, 149, 450
Cf, 148, 450
CP , 149, 450
CONGRUENT, 471
CTΣ, 17
D(t), 193
Ds, 459
Des(S), 99
dom(R), 5
dosubstitution, 70, 385
E(B∗, H), 368
E(C, H), 368
Es
i , 450
Ei, 149
EQ1,0, 239
EQ1,i, 239
EQ2,0, 239
EQ2,i, 239
EQ3,0, 239
EQ3,i, 239
ex, 374, 375
exp(m, n, p), 279
f ◦g, 6
f(X), 6
f : A →B, 5
f −1(Y ), 6
fA, 478, 483
fM, 159, 453
f A
i , 340
FIND, 471
FORM0(i), 199
FORML, 149, 451
FV (A), 154
FV (t), 153
G(C), 463
graph(f), 5
hu, 483
H≡, 39
H∧, 39
H∨, 39
H¬, 39
H⊃, 39
HA, 45

INDEX OF SYMBOLS
497
Hnand, 56
Hnor, 56
HX, 39
HT, 345
IA, 6
left, 384
LK′ −{cut}, 111
LK −{cut}, 259
M(P, G), 439
M u, 453
MS, 459
NUMACT, 208
PM, 159, 453
Pu, 462
PROP, 32
PROPL, 173
QF(A), 326
R ◦S, 5
R∗, 8
R+, 8
R−1, 6
range(R), 5
RC, 239
right, 384
S(C), 463
s[t/x], 155
s[xi := a], 160
Se, 292
SK(A), 358
t(TERM0), 199
T(L, V), 450
t/u, 15
t[s1/y1, ..., sn/yn], 285, 342
T[t/y], 274
T[z/c], 288
T[z/y], 274, 288
t1[u ←t2], 15
tM, 160, 454
tM[s], 160
tM[v], 454
TΣ, 17
TERM(C), 463
TERM(C)s, 463
TERM s
L, 451
TERM0, 198, 207
TERM1, 199
TERML, 148
TERMS, 207, 239
UNION, 471
US(A), 357
v : PS →BOOL, 39
v |= A, 41
v ̸|= A, 41
v[i], 462
variable, 384
X+, 19
X+, 18, 19
A/ ∼=, 482, 487
B0, 106
BL, 178
BP ROP , 50
BT , 107
CSs, 449
HS, 195, 232
LS, 194
LX, 253
M |= Γ, 162
M |= A, 162
M |= A[s], 162
Vs, 449

INDEX OF
DEFINITIONS
Corollary, 72, 113, 121, 205, 216, 242,
260, 268, 364, 437, 441
Deﬁnition 3.2.1, 32
Deﬁnition 3.2.2, 32
Deﬁnition 3.3.1, 39
Deﬁnition 3.3.2, 39
Deﬁnition 3.3.3, 39
Deﬁnition 3.3.4, 42
Deﬁnition 3.3.5, 45
Deﬁnition 3.3.6, 48
Deﬁnition 3.4.1, 62
Deﬁnition 3.4.2, 63
Deﬁnition 3.4.3, 64
Deﬁnition 3.4.4, 65
Deﬁnition 3.4.5, 65
Deﬁnition 3.4.6, 68
Deﬁnition 3.4.7, 73
Deﬁnition 3.4.8, 74
Deﬁnition 3.5.1, 83
Deﬁnition 3.5.2, 85
Deﬁnition 3.5.3, 89
Deﬁnition 3.5.4, 90
Deﬁnition 3.5.5, 90
Deﬁnition 3.5.6, 90
Deﬁnition 3.5.7, 90
Deﬁnition 3.5.8, 94
Deﬁnition 3.5.9, 94
Deﬁnition 3.5.10, 95
Deﬁnition 3.6.1, 110
Deﬁnition 4.2.1, 118
Deﬁnition 4.2.2, 120
Deﬁnition 4.2.3, 120
Deﬁnition 4.3.1, 127
Deﬁnition 4.3.2, 129
Deﬁnition 4.3.3, 130
Deﬁnition 5.2.1, 147
Deﬁnition 5.2.2, 148
Deﬁnition 5.2.3, 149
Deﬁnition 5.2.4, 153
Deﬁnition 5.2.5, 154
Deﬁnition 5.2.6, 155
Deﬁnition 5.2.7, 156
Deﬁnition 5.3.1, 158
Deﬁnition 5.3.2, 159
Deﬁnition 5.3.3, 159
Deﬁnition 5.3.4, 160
Deﬁnition 5.3.5, 160
Deﬁnition 5.3.6, 161
Deﬁnition 5.3.7, 162
Deﬁnition 5.3.8, 164
Deﬁnition 5.3.9, 171
498

INDEX OF DEFINITIONS
499
Deﬁnition 5.3.10, 171
Deﬁnition 5.3.11, 173
Deﬁnition 5.3.12, 176
Deﬁnition 5.4.1, 187
Deﬁnition 5.4.2, 188
Deﬁnition 5.4.3, 192
Deﬁnition 5.4.4, 193
Deﬁnition 5.4.5, 193
Deﬁnition 5.4.6, 194
Deﬁnition 5.4.7, 194
Deﬁnition 5.4.8, 198
Deﬁnition 5.4.9, 199
Deﬁnition 5.5.1, 209
Deﬁnition 5.6.1, 231
Deﬁnition 5.6.2, 236
Deﬁnition 5.6.3, 240
Deﬁnition 6.2.1, 257
Deﬁnition 6.3.1, 262
Deﬁnition 6.3.2, 263
Deﬁnition 6.4.1, 269
Deﬁnition 6.4.2, 270
Deﬁnition 6.4.3, 273
Deﬁnition 6.4.4, 273
Deﬁnition 6.4.5, 274
Deﬁnition 6.4.6, 281
Deﬁnition 6.5.1, 292
Deﬁnition 6.6.1, 295
Deﬁnition 6.6.2, 295
Deﬁnition 7.2.1, 305
Deﬁnition 7.3.1, 312
Deﬁnition 7.3.2, 314
Deﬁnition 7.4.1, 326
Deﬁnition 7.4.2, 326
Deﬁnition 7.4.3, 327
Deﬁnition 7.4.4, 336
Deﬁnition 7.5.1, 340
Deﬁnition 7.5.2, 340
Deﬁnition 7.5.3, 342
Deﬁnition 7.5.4, 346
Deﬁnition 7.6.1, 357
Deﬁnition 7.6.2, 358
Deﬁnition 7.6.3, 359
Deﬁnition 7.7.1, 371
Deﬁnition 7.7.2, 371
Deﬁnition 7.7.3, 372
Deﬁnition 7.7.4, 372
Deﬁnition 7.7.5, 372
Deﬁnition 7.7.6, 373
Deﬁnition 8.2.1, 378
Deﬁnition 8.4.1, 382
Deﬁnition 8.4.2, 382
Deﬁnition 8.4.3, 384
Deﬁnition 8.4.4, 385
Deﬁnition 8.5.1, 395
Deﬁnition 8.5.2, 396
Deﬁnition 8.5.3, 397
Deﬁnition 9.2.1, 411
Deﬁnition 9.2.2, 413
Deﬁnition 9.2.3, 414
Deﬁnition 9.3.1, 422
Deﬁnition 9.3.2, 423
Deﬁnition 9.4.1, 428
Deﬁnition 9.4.2, 428
Deﬁnition 9.5.1, 434
Deﬁnition 9.5.2, 435
Deﬁnition 9.5.3, 439
Deﬁnition 10.2.1, 449
Deﬁnition 10.2.2, 450
Deﬁnition 10.3.1, 453
Deﬁnition 10.3.2, 453
Deﬁnition 10.3.3, 454
Deﬁnition 10.3.4, 454
Deﬁnition 10.3.5, 455
Deﬁnition 10.3.6, 455
Deﬁnition 10.4.1, 457
Deﬁnition 10.5.1, 458
Deﬁnition 10.6.1, 461
Deﬁnition 10.6.2, 462
Deﬁnition 10.6.3, 462
Deﬁnition 10.6.4, 471
Lemma 2.1.1, 9
Lemma 2.1.2, 10
Lemma 2.1.3, 11
Lemma 2.1.4, 12
Lemma 2.3.1, 19
Lemma 2.3.2, 19
Lemma 2.3.3, 21
Lemma 2.4.1, 480
Lemma 2.4.2, 480
Lemma 2.5.1, 487
Lemma 3.2.1, 33
Lemma 3.3.1, 40
Lemma 3.3.2, 43
Lemma 3.3.3, 46
Lemma 3.3.4, 46
Lemma 3.3.5, 48

500
INDEX OF DEFINITIONS
Lemma 3.3.6, 49
Lemma 3.4.1, 65
Lemma 3.4.2, 65
Lemma 3.4.3, 67
Lemma 3.4.4, 75
Lemma 3.4.5, 75
Lemma 3.5.1, 89
Lemma 3.5.2, 91
Lemma 3.5.3, 92
Lemma 3.5.4, 95
Lemma 3.5.5, 96
Lemma 3.6.1, 112
Lemma 3.6.2, 114
Lemma 4.2.1, 121
Lemma 4.2.2, 122
Lemma 4.3.1, 128
Lemma 4.3.2, 131
Lemma 4.3.3, 134
Lemma 4.3.4, 142
Lemma 5.2.1, 150
Lemma 5.2.2, 151
Lemma 5.2.3, 151
Lemma 5.2.4, 152
Lemma 5.2.5, 152
Lemma 5.2.6, 152
Lemma 5.3.1, 164
Lemma 5.3.2, 168
Lemma 5.3.3, 169
Lemma 5.3.4, 171
Lemma 5.3.5, 174
Lemma 5.3.6, 175
Lemma 5.3.7, 177
Lemma 5.3.8, 177
Lemma 5.4.1, 189
Lemma 5.4.2, 190
Lemma 5.4.3, 192
Lemma 5.4.4, 193
Lemma 5.4.5, 195
Lemma 5.5.1, 218
Lemma 5.5.2, 218
Lemma 5.6.1, 231
Lemma 5.6.2, 239
Lemma 5.6.3, 239
Lemma 5.6.4, 243
Lemma 5.6.5, 244
Lemma 6.2.1, 259
Lemma 6.3.1, 263
Lemma 6.3.2, 264
Lemma 6.3.3, 266
Lemma 6.4.1, 269
Lemma 6.4.2, 272
Lemma 6.4.3, 274
Lemma 6.4.4, 276
Lemma 6.4.5, 276
Lemma 6.4.6, 276
Lemma 6.4.7, 277
Lemma 6.4.8, 281
Lemma 6.4.9, 282
Lemma 6.5.1, 288
Lemma 6.5.2, 288
Lemma 6.5.3, 292
Lemma 6.5.4, 293
Lemma 7.2.1, 305
Lemma 7.2.2, 309
Lemma 7.3.1, 312
Lemma 7.3.2, 313
Lemma 7.3.3, 315
Lemma 7.4.1, 328
Lemma 7.4.2, 330
Lemma 7.4.3, 337
Lemma 7.5.1, 338
Lemma 7.6.1, 359
Lemma 7.6.2, 360
Lemma 7.6.3, 362
Lemma 8.2.1, 378
Lemma 8.3.1, 380
Lemma 8.4.1, 382
Lemma 8.4.2, 383
Lemma 8.4.3, 389
Lemma 8.5.1, 398
Lemma 8.5.2, 399
Lemma 8.5.3, 400
Lemma 8.5.4, 400
Lemma 9.2.1, 414
Lemma 9.2.2, 414
Lemma 9.2.3, 416
Lemma 9.2.4, 417
Lemma 9.2.5, 417
Lemma 9.3.1, 425
Lemma 9.4.1, 430
Lemma 9.5.1, 436
Lemma 9.5.2, 438
Lemma 9.5.3, 439
Lemma 10.3.1, 455
Lemma 10.3.2, 456
Lemma 10.4.1, 458
Lemma 10.5.1, 459

INDEX OF DEFINITIONS
501
Lemma 10.5.2, 459
Lemma 10.6.1, 461
Lemma 10.6.2, 463
Lemma 10.6.3, 467
Lemma 10.6.4, 468
Lemma 10.6.5, 472
Theorem 2.1.1, 9
Theorem 2.3.1, 22
Theorem 2.4.1, 480
Theorem 2.5.1, 486
Theorem 3.2.1, 34
Theorem 3.3.1, 47
Theorem 3.4.1, 71
Theorem 3.4.2, 73
Theorem 3.5.1, 92
Theorem 3.5.2, 94
Theorem 3.5.3, 94
Theorem 3.5.4, 94
Theorem 3.6.1, 112
Theorem 4.2.1, 123
Theorem 4.3.1, 131
Theorem 4.3.2, 133
Theorem 4.3.3, 137
Theorem 5.2.1, 151
Theorem 5.2.2, 152
Theorem 5.4.1, 204
Theorem 5.5.1, 214
Theorem 5.5.2, 217
Theorem 5.5.3, 218
Theorem 5.5.4, 218
Theorem 5.6.1, 241
Theorem 5.6.2, 243
Theorem 5.6.3, 243
Theorem 5.6.4, 243
Theorem 6.2.1, 260
Theorem 6.3.1, 266
Theorem 6.4.1, 280
Theorem 6.4.2, 283
Theorem 6.5.1, 291
Theorem 6.5.2, 293
Theorem 6.6.1, 297
Theorem 6.6.2, 299
Theorem 6.7.1, 300
Theorem 7.2.1, 307
Theorem 7.3.1, 320
Theorem 7.4.1, 334
Theorem 7.4.2, 337
Theorem 7.5.1, 344
Theorem 7.6.1, 363
Theorem 8.4.1, 390
Theorem 8.5.1, 404
Theorem 8.5.2, 404
Theorem 9.2.1, 419
Theorem 9.2.2, 421
Theorem 9.3.1, 426
Theorem 9.4.1, 431
Theorem 9.5.1, 441
Theorem 9.5.2, 444
Theorem 10.2.1, 452
Theorem 10.4.1, 458
Theorem 10.5.1, 459

SUBJECT INDEX
Accepted
deterministically in polynomial time,
52
nondeterministically in polynomial
time, 52
Accepting
computation, 52
state, 51
Ackermann’s function, 11, 373
a-formula, 90
Alphabet, 12
of a ﬁrst-order language, 147
of a many-sorted ﬁrst-order language,
449
for propositional formulae, 32
Analytic consistency property, 227, 251
Ancestor, 15
And, 32, 147
Answer substitution, 430
Antecedent, 62
Anti-Horn clause, 434
Antisymmetric, 8
Arithmetic expression, 17
Arity, 16, 148, 449, 483
Assignment, 159
of sort s, 453
Atomic
cut, 263
formulae, 149
proposition, 68
Atomically closed, 70
Atoms, 17
Atp, 2
Automatic theorem proving, 1, 2, 117
Auxiliary symbols, 32, 148, 449
Axiom, 29
of G, 188
of G, many-sorted case, 457
of G′, 65
of G1nnf, 271
of G1nnf
=
, 281
of GCNF ′, 120
of LK′, 111
of LK, 259
Backtracking, 51
Base
cases, 10
functions, 372
Basic Horn formula, 59, 186
Beth’s deﬁnability theorem
with equality, 299
502

SUBJECT INDEX
503
without equality, 297
b-formula, 90
Bijective, 6
Binary
relation, 5
resolution, 407
Binds, 171
Boolean algebra, 50, 104, 178
Bound variable, 154
Bounded, 100
Branch, 15
Canonical function, 7
Cardinality, 7
Carrier, 158, 478
of sort s, 483
Cartesian product, 4
c-formula, 192
Chain, 8, 15, 100
Characterization of Consistency in LK′,
114
Church, 2, 30
Church’s theorem, 216, 373
Church-Turing thesis, 370
c-instance, 359
Clause, 118, 378
form, 378
Closed
under complementation, 53
equality axioms, 246, 268
equality axioms for S, 292
formula, 154
under the functions in F, 19
leaf, 61, 83
under the operations, 479, 485
term, 154
tree, 85
Closure conditions, 17
CNF, 73
Compactness theorem
for ﬁrst-order languages of any cardi-
nality, 185
for G, 218
for G′, 94
for G=, 243
Complete, 30
induction, 9
set of propositions, 98
set of sentences, 224
sets of connectives, 175
Completeness, 2
of G′, 72
of G1nnf, 272
of G1nnf
=
, 281
of GCNF ′, 123
of ground resolution, 380
of LK′ (with cut), 116
of proofs in SLD-form, 419
of resolution, without equality, 404
of SLD-resolution for Horn clauses,
426, 431
of the resolution method, proposition-
al case , 133
Complexity, 71
theory, 369
Components of a signed formula, 90, 192
Composition, 5, 371
Compound instance, 357, 359
Computability, 369
Concatenation, 13
Conclusion, 63, 66
Congruence, 233, 462, 481, 486
closure of E, 463
on a graph, 461
Conjugate
of a literal, 118, 378
of a set of literals, 396
of a signed formula, 90
Conjunctive normal form, 73, 118
Conservative over, 226, 249
Consistency
in G, 218
in G′, 94
lemma for G, 218
lemma for G′, 95
lemma for G=, 243
in LK′, 114
properties, 255
property, 103
Consistent set of propositions, 94
Constant of sort s, 483
Constants, 16, 148, 449
Contraction, 110, 258
Converse, 6
Converting GCNF ′-proofs into Resolu-
tion Refutations, 131

504
SUBJECT INDEX
Correctness
of the congruence closure algorithm,
472
of the linearization process, 425
of SLD-resolution as a computational
procedure, 441
of the uniﬁcation algorithm, 390
Countable
sequence, 7
set, 7
Countably
inﬁnite, 7
saturated structure, 253
Counter-example tree, 66
Craig’s interpolation theorem
with equality, 293
without equality, 291
Cut
elimination theorem, 97, 110
elimination theorem for G1nnf, 280
elimination theorem for G1nnf
=
, 283
elimination theorem for LK′, 113
formula, 97, 109, 111, 223, 258, 270
rule, 97, 109, 110, 111, 223, 257, 258,
270
Cut-free, 97
proof, 110
Cut-rank
in G1nnf, 273
in G1nnf
=
, 282
Cycle, 475
Cylindric algebra, 178
DAG, 127
Decidable, 373
Decision procedure
for quantiﬁer-free formulae, 470
for quantiﬁer-free formulae without
predicate symbols, 466
Deduction
rule, 29
system, 29
theorem, 80, 221
tree, for G, 188
tree, for G′, 62, 66
tree, for G1nnf, 272
tree, for GCNF ′, 120
tree, for LK, 259
tree, many-sorted case, 458
Deﬁnable, 56
Deﬁned inductively, 17
Deﬁnite
answer, 438, 442
answer lemma, 438
clause, 411, 428
Deﬁnitional extension, 249
Degree of a formula in NNF, 273
Denumerable set, 7
Depth, 15
d-formula, 192
Direct
image, 6
product, 186
Directed acyclic graph, 127
Disjunctive normal form, 57, 73
DNF, 73
Domain, 5, 158
Dominates, 15
Downward closed, 103
Dual, 57
Edge, 462
of a DAG, 127
Eigenvariable, 188, 259, 271, 457
condition, 188, 259, 271, 457
Elementary
equivalence, 224, 254
equivalent structures, 186
Empty
clause, 126
string, 13
Equality
axioms of LKe, 262
rules for G=, 236, 458
symbol, 147
Equation, 282, 285
Equational
language, 285
logic, 287
Equivalence, 32, 147
class of x modulo R, 7
relation, 7
E-resolution method, 407
Essential cut, 263
Exchange, 111, 258
Exclusive OR, 45, 58

SUBJECT INDEX
505
Existence
condition for f, 248
of the congruence closure, 467
Expansion
of a language, 194
of a structure, 194
step, 70
Explicit deﬁnability, 295
Extended
completeness theorem for G′, 94
completeness theorem for G + {cut},
227
language L(M), 164
quantiﬁer rules, 198
rule ∃: right, 198
rule ∀: left, 198
Extension, 12
Extra formula, 63
Factor, 407
Factoring, 407
rule, 407
False, 32
Falsehood, 147
Falsiﬁable
propositional ﬁnite sequent, 64
sequent, ﬁrst-order case, 188
sequent, propositional case, 85
Falsiﬁes, 41, 85
Family, 7
Filter, 100, 105, 183
generated by, 100
Finished leaf
ﬁrst-order case, no equality, 208
propositional ﬁnite case, 68
propositional inﬁnite case, 83
special case, 199
Finite
intersection property, 100, 105
labeled graph, 461
path, 15
sequence, 7
set, 7
tree, 14
Finite-branching, 14, 89
Finitely axiomatizable, 106
First-order
languages without equality, 148
languages with equality, 230
logic, 1, 146
For all, 147
Formula, 1
ﬁrst-order case, 149
many-sorted, 451
Free
and Bound Variables, 153
structure, 368, 435
variable, 153
Freely generated
by X, 480, 485
by X and F, 21
From Resolution Refutations to
GCNF ′-proofs, 137
Function
CONGRUENT, 471
K, for formulae, 151
K, for terms, 150
symbols, 148, 449
Functional, 5
form, 342
form of a sequent, 340
reﬂexivity axioms, 409
Functionally complete set, 45
G-congruence, 462
Generalization rules, 220
Gentzen, 60
cut elimination theorem for LK, 256
Hauptsatz for LK, 261
Hauptsatz for LK′, 113
Hauptsatz for LKe, 268
sharpened Hauptsatz, 304, 320
system G for languages without equal-
ity, 187, 457
system G′, 63
system G1nnf, 270
system G2nnf, 327
system G2nnf
=
, 337
system G=, 236
system LK, 257
system LK′, 110
system LKe, 262
system LKe,⊥, 293
system LK⊥, 288
Goal clause, 412, 428
G¨odel, 2

506
SUBJECT INDEX
G¨odel (continued)
extended completeness theorem for
G, 216, 458
extended completeness theorem for
G=, 242, 459
incompleteness theorem, 300, 370
Goldfarb, 366
G-provable, 259
G′-provable sequent, 111
Graph, 5
Greatest
element, 8
lower bound, 8
Ground clause, 378
Ground
resolution, 377
resolution method, 379
substitution, 343
Head, 13, 82
Height, 15
Henkin, 219
theory, 225, 250
method, 224
Henkin-complete, 229, 253
Herbrand
disjunction, 344
expansion, 368
like theorem for resolution, 404
structure, 435
theorem, 303, 365
theorem for prenex formulae, 344
universe, 197, 436
Hilbert, 370
system, 79, 116, 219, 247, 255
Hintikka set
ﬁrst-order language with equality,
231
ﬁrst-order language without equality,
195
many-sorted case, 458
propositional case, 90
propositional unsigned case, 99
Homomorphism, 23, 107, 478, 484
of structures, 181
Horn clause , 410, 411
in PROLOG notation, 428
formula, 59, 101, 186
H-tree, 422, 446
Ideal, 105
Identity relation, 6
I-indexed sequence, 7
Image, 6
Immediate
descendants, 98, 226, 262, 284
successor, 14
Implication, 32, 147
Implicit deﬁnability, 295
Incomplete, 300
Inconsistency in G′, 94
Inconsistent, 94
Indeﬁnite answer, 438, 442
Independent, 15, 59, 127
Index set, 7
Induction
hypothesis, 9
principle, 19
step, 10
Inductive
closure, 19, 31
deﬁnition, 17
on, 19
Inessential cut, 282
Inference rule, 29
of G, 187
of G′, 63
of G1nnf, 270
of G1nnf
=
, 281
of G2nnf, 327
of G2nnf
=
, 337
of GCNF ′, 120
of LK, 258
of LK′, 110
Logically equivalent, 48, 176
L¨owenheim-Skolem theorem
for G, 217
for G=, 243
Lower bound, 8
Many-sorted
algebra, 453
ﬁrst-order logic, 1, 448
L-structure, 453

SUBJECT INDEX
507
Σ-algebra, 483
Matrix of a formula, 309
Maximal, 8, 100
consistency, in G=, 244
consistent set, 96, 115, 224
consistent set, in G, 218
quantiﬁed subformulae, 326
Maximally consistent, 96, 115
Meaning, 1
of a formula, 160
of a term, 160
Meta-language, 33
Meta-variables, 33
Midsequent, 320, 334, 337
Minimal, 8
Minimization, 372
Model, 455
of A, ﬁrst-order case, 162
of computation, 3
existence theorem for G, 218
existence theorem for G′, 94
existence theorem for G=, 243
of a set of formulae, 162
theory, 1, 29, 245, 255
Theoretic Semantics, 434
Modus ponens, 79, 220
Monadic predicate calculus, 182
Monotonic, 24
Most common instance, 382
Most general uniﬁer, 382
Multiset, 25, 111, 259
Natural
deduction system, 116, 255
numbers, 7
Negation
by failure, 445
normal form, 74, 269
Negative
clause, 412, 428
set of literals, 396
Nelson and Oppen, 460
NNF, 74
Node, 14, 461
of a DAG, 127
Nonatomic proposition, 68
Nondeterministic, 51
Nonlogical symbols, 148, 449
Nonstandard model, 244, 245
Nontrivial sequent, 120
Nonvoid sort, 485
Normal form, 3
lemma for GCNF ′, 122
Not, 32, 147
bound in the deduction tree, 274
NP, 51
NP-complete, 53
NP-hard, 53
Null
path, 15
string, 13
Object language, 33
Occur check, 387
One to one, 6
Onto, 6
Open
equality axioms, 247
formula, 154
Operations, 18
Oppen and Nelson’s congruence closure
algorithm, 471
Or, 32, 147
Outdegree, 14
Output variables, 434
P, 51
Padoa’s method, 297
Paramodulant, 408
Paramodulation, 377
method, 407
rule, 407
Partial
function, 5
order, 8
recursive functions, 370, 373
Partially
decidable, 373
ordered set, 8
Partition, 7, 289
Peano’s arithmetic, 163, 244, 300
Permutability lemma for LK, 314
Polynomial-time reducibility, 53
Polynomially reducible, 53
Poset, 8

508
SUBJECT INDEX
Positive
formula, 181
set of literals, 396
Possible deﬁnition
of f, 249
of P, 249
Powers, 7
Power-set, 478
Precedence, 35
Predecessor, 15
Predicate symbols, 148, 449
Preﬁx, 13
notation, 36
Premise, 63
Prenex
form, 305
formula, 305
Preorder traversal, 384
Preserved under
homomorphisms, 181
reduced products, 186
Primitive
recursion, 371
recursive function, 365, 371, 372
Principal
ﬁlter, 106
formula, 63, 111, 188, 259, 271, 457
Procedure
expand, ﬁrst-order case, no equality,
209
expand, language with equality, 240
expand, propositional ﬁnite case, 69
expand, propositional inﬁnite case, 84
expand, special case, 200
grow-left, ﬁrst-order case, no equality,
209
grow-left, special case, 201
grow-right, ﬁrst-order case, no equal-
ity, 211
grow-right, special case, 201
MERGE, 471
search, ﬁrst-order case, no equality,
209
search, language with equality, 241
search, propositional ﬁnite case, 69
search, propositional inﬁnite case, 83
search, special case, 199
test-and-substitute, 385
uniﬁcation, 385
unify, 386
Program, 434
Projections functions, 372
PROLOG, 3, 410
Proof, 2
in SLD-form, 413
theory, 2, 29
tree, for G, 188
tree, for G′, 66
tree, for G1nnf, 272
tree, for GCNF ′, 120
tree, for LK, 259
tree, for LK′, 111
tree, many-sorted case, 458
without weakening, 414
Proper, 13
ﬁlter, 100, 105, 183
Property of ﬁnite character, 103, 228,
251
Proposition symbols, 32
Propositional
formulae, 32
logical rules, 270
Propositions, 1, 32
Provability, 1
Provable
in G′, 66
inﬁnite sequent, 85
Pure-variable proof tree, 312
Quantiﬁer rules for subformulae, 327
Quantiﬁers, 147, 449
Query, 434
Quotient, 7, 232
algebra, 482, 487
Ramiﬁcation, 14
Range, 5
Rank, 16, 148, 449, 483
function, 16, 148, 449
Ranked alphabet, 16
Rectiﬁed formula, 171, 456
Recursive
function, 373
function theory, 369
Recursively, 20
Reduced, 59

SUBJECT INDEX
509
product, 101, 183, 184
Reduct
of a language, 194
of a signed set, 194
of a structure, 194
Reduction lemma
for G1nnf, 277
lemma for G1nnf
=
, 282
Reﬂexive, 7
and transitive closure, 8
Reﬂexivity
axiom, 408
rule, 236
Rejecting
computation, 52
state, 51
Relational version, 248
of a language with equality, 247
Resolution, 117
DAG, ﬁrst-order case, 397
DAG, propositional case, 130
method, 3
method for ﬁrst-order logic without
equality, 395
method for propositional logic, 128
refutation, ﬁrst-order case, 397
refutation, propositional case, 130
steps, 130, 397
Resolvent
ﬁrst-order case, 396
propositional case, 129
Restriction, 12
Result substitution
σ, 430
θi, 440
Rewrite rule, 407
Robinson, J. A., 117, 377
Robinson’s joint consistency theorem,
300
Root, 14, 462
Round, 70
counter, 239
Rule
R1 can be permuted with R2, 314
with contraction, 330
without contraction, 330
SAT, 50
Satisfaction, 455
for signed formulae, 193
Satisﬁability
functional form, 342
problem, 42
Satisﬁable, 42
formula, ﬁrst-order case, 162
in M, ﬁrst-order case, 162
sequent, propositional case, 85
set of formulae, 162
set of propositions, 94
Satisﬁes, 41
a signed set, 90
Schwichtenberg, 273
Scope, 15
Searching for a counter example, 60
Selected atom, 424, 429
Semantic consequence
ﬁrst-order case, 162
propositional case, 42
Semantics, 1, 30
of formulae, 158
of logic programs, 437, 439
Sentence, 154
Separating pair of substitutions, 396
Sequence, 7
Sequent, 62
Set
of predecessors, 462
of sorts, 449, 483
of states, 51
of types, 449, 483
Sharpened Hauptsatz
for G1nnf and G2nnf, 334
for G1nnf
=
and G2nnf
=
, 337
Side formula, 63, 111, 188, 259, 271, 327,
457
Σ-algebra, 477
σ-matrix of the functional form of A up
to i, 346
Σ-tree, 16
of sort s, 487
Signature, 16
Signed formula , 89, 192
of conjunctive type, 90
of disjunctive type, 90
of existential type, 193

510
SUBJECT INDEX
Signed formula (continued)
ﬁrst-order language with equality,
231
of type a, 90
of type b, 90
of type c, 192
of type d, 192
of universal type, 193
Signed set of formulae, 90
Simple formula, 206
Simply stratiﬁed alphabet, 16
Singulary, 56
Skolem
constant symbol, 340, 358
form, 358
function symbol, 340, 358
normal form, 342, 358
Skolem-Herbrand-G¨odel theorem, 303,
365
after Andrews, 364
SLD-derivation, 423, 428
SLD-refutation, 424, 429
SLD-resolution, 410
method, 424
Smorynski, 371
Smullyan, 89, 95, 229
Son, 14
Sort, 448, 449, 483
Sound, 2, 30
Soundness
of the resolution method, proposition-
al case, 131
of resolution without equality, 399
of SLD-resolution, 430
of the system G, 192
of the system G′, 67
of the system G, many-sorted case,
458
of the system GCNF ′, 121
of the system G1nnf
=
, 281
of the system G2nnf
=
, 337
of the system G2nnf, 328
of the system G=, 239
of the system LK, 260
of the system LK′, 112
Source, 15
S-ranked alphabet, 483
Stacks of natural numbers, 484
Strict order, 9
String, 12
Structural
induction, 9
rules, 110, 257
Structure (ﬁrst-order), 158
Subalgebra, 479, 484
Subformula, 171, 456
property, 262
Substitution, 57
function, 285, 342
instance, 342
lemma, 276
pair, 343
in tautologies, 173
of a term, 155
Substring, 13
Substructure, 183
Subsumes, 144, 406
Subtree rooted at u, 15
Succedent, 62
Successor function, 372, 461
Suﬃx, 13
Support of the substitution, 285, 342
Surjective, 6
Symmetric, 7
Syntax, 30
rules, 28
Synthetic consistency property, 228, 252
System GCNF ′, 120
Systematic deduction tree, 68, 199
Tableaux system, 95, 116, 255
Tail, 13, 82
Tait, 273
Target, 15
Tarski, 1, 178
TAUT, 50
Tautology, 31, 42
problem, 42
Term
algebra, 194
constructors, 194
ﬁrst-order case, 148
free for x in A, 156
of sort s, 451
Terminal node, 462
Theory, 225, 249

SUBJECT INDEX
511
There exists, 147
To the left, 15
Total function, 5
order, 8
Transitive, 7
closure, 8
Translation, 247
conditions, 248
Tree, 14
address, 14
domain, 13
replacement, 15
Trivial sequent, 120
Truth, 1
assignment, 39
function, 39, 45
functionally implies, 228, 252
functionally inconsistent, 228, 252
functionally valid, 228, 252
table, 39
table method, 44
value, 1, 39, 40
T-translate, 247
Turing machine, 51, 370
Type, 448, 449, 483
Ultraﬁlter, 101, 106, 184
Ultraproduct, 101, 184, 185
Uncountable, 7
Underlying tree, 127
Uniﬁcation, 377
algorithm, 385
closure, 473, 474
closure problem, 473
Uniﬁer, 381
of S, 382
Unique homomorphic extension, 23
theorem, 22, 480, 486
Uniqueness condition for f, 248
Universal
algebra, 488
closure, 180, 285
equation, 285
scope of a formula, 357
Unsatisﬁable, 42
Upper bound, 8
Upward closed, 104
Valid
proposition, 42
formula, ﬁrst-order case, 162
in M, ﬁrst-order case, 162
propositional ﬁnite sequent, 64
sequent, ﬁrst-order case, 188
sequent, propositional case, 85
set of formulae, 162
Validity, 455
functional form, 342
Valuation, 39
Variables, 148, 449
Weakening, 110, 258
formula, 110
lemma, 276
Well-founded, 9
Witness, 224
Zero function, 372
Zorn’s lemma, 9, 100, 227, 251

